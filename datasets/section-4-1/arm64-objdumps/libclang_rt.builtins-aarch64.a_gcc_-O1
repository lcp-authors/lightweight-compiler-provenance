In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_gcc_-O1:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x3, [sp]
   c:	ldr	x1, [sp, #8]
  10:	str	q1, [sp]
  14:	ldr	x5, [sp]
  18:	ldr	x2, [sp, #8]
  1c:	and	x4, x1, #0x7fffffffffffffff
  20:	mov	w0, #0x1                   	// #1
  24:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
  28:	cmp	x4, x6
  2c:	b.hi	38 <__cmptf2+0x38>  // b.pmore
  30:	b.eq	ac <__cmptf2+0xac>  // b.none
  34:	mov	w0, #0x0                   	// #0
  38:	and	x6, x2, #0x7fffffffffffffff
  3c:	mov	w4, #0x1                   	// #1
  40:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
  44:	cmp	x6, x7
  48:	b.hi	54 <__cmptf2+0x54>  // b.pmore
  4c:	b.eq	b4 <__cmptf2+0xb4>  // b.none
  50:	mov	w4, #0x0                   	// #0
  54:	orr	w4, w0, w4
  58:	mov	w0, #0x1                   	// #1
  5c:	tst	w4, #0xff
  60:	b.ne	a4 <__cmptf2+0xa4>  // b.any
  64:	orr	x4, x3, x5
  68:	orr	x0, x1, x2
  6c:	and	x0, x0, #0x7fffffffffffffff
  70:	orr	x4, x4, x0
  74:	mov	w0, #0x0                   	// #0
  78:	cbz	x4, a4 <__cmptf2+0xa4>
  7c:	tst	x1, x2
  80:	b.mi	cc <__cmptf2+0xcc>  // b.first
  84:	cmp	x2, x1
  88:	b.gt	c4 <__cmptf2+0xc4>
  8c:	b.eq	bc <__cmptf2+0xbc>  // b.none
  90:	eor	x3, x3, x5
  94:	eor	x1, x1, x2
  98:	orr	x1, x3, x1
  9c:	cmp	x1, #0x0
  a0:	cset	w0, ne  // ne = any
  a4:	add	sp, sp, #0x10
  a8:	ret
  ac:	cbnz	x3, 38 <__cmptf2+0x38>
  b0:	b	34 <__cmptf2+0x34>
  b4:	cbnz	x5, 54 <__cmptf2+0x54>
  b8:	b	50 <__cmptf2+0x50>
  bc:	cmp	x5, x3
  c0:	b.ls	90 <__cmptf2+0x90>  // b.plast
  c4:	mov	w0, #0xffffffff            	// #-1
  c8:	b	a4 <__cmptf2+0xa4>
  cc:	cmp	x1, x2
  d0:	b.gt	f8 <__cmptf2+0xf8>
  d4:	b.eq	f0 <__cmptf2+0xf0>  // b.none
  d8:	eor	x3, x3, x5
  dc:	eor	x1, x1, x2
  e0:	orr	x1, x3, x1
  e4:	cmp	x1, #0x0
  e8:	cset	w0, ne  // ne = any
  ec:	b	a4 <__cmptf2+0xa4>
  f0:	cmp	x3, x5
  f4:	b.ls	d8 <__cmptf2+0xd8>  // b.plast
  f8:	mov	w0, #0xffffffff            	// #-1
  fc:	b	a4 <__cmptf2+0xa4>

0000000000000100 <__getf2>:
 100:	sub	sp, sp, #0x10
 104:	str	q0, [sp]
 108:	ldr	x3, [sp]
 10c:	ldr	x1, [sp, #8]
 110:	str	q1, [sp]
 114:	ldr	x5, [sp]
 118:	ldr	x2, [sp, #8]
 11c:	and	x4, x1, #0x7fffffffffffffff
 120:	mov	w0, #0x1                   	// #1
 124:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
 128:	cmp	x4, x6
 12c:	b.hi	138 <__getf2+0x38>  // b.pmore
 130:	b.eq	1ac <__getf2+0xac>  // b.none
 134:	mov	w0, #0x0                   	// #0
 138:	and	x6, x2, #0x7fffffffffffffff
 13c:	mov	w4, #0x1                   	// #1
 140:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 144:	cmp	x6, x7
 148:	b.hi	154 <__getf2+0x54>  // b.pmore
 14c:	b.eq	1b4 <__getf2+0xb4>  // b.none
 150:	mov	w4, #0x0                   	// #0
 154:	orr	w4, w0, w4
 158:	mov	w0, #0xffffffff            	// #-1
 15c:	tst	w4, #0xff
 160:	b.ne	1a4 <__getf2+0xa4>  // b.any
 164:	orr	x4, x3, x5
 168:	orr	x0, x1, x2
 16c:	and	x0, x0, #0x7fffffffffffffff
 170:	orr	x4, x4, x0
 174:	mov	w0, #0x0                   	// #0
 178:	cbz	x4, 1a4 <__getf2+0xa4>
 17c:	tst	x1, x2
 180:	b.mi	1cc <__getf2+0xcc>  // b.first
 184:	cmp	x2, x1
 188:	b.gt	1c4 <__getf2+0xc4>
 18c:	b.eq	1bc <__getf2+0xbc>  // b.none
 190:	eor	x3, x3, x5
 194:	eor	x1, x1, x2
 198:	orr	x1, x3, x1
 19c:	cmp	x1, #0x0
 1a0:	cset	w0, ne  // ne = any
 1a4:	add	sp, sp, #0x10
 1a8:	ret
 1ac:	cbnz	x3, 138 <__getf2+0x38>
 1b0:	b	134 <__getf2+0x34>
 1b4:	cbnz	x5, 154 <__getf2+0x54>
 1b8:	b	150 <__getf2+0x50>
 1bc:	cmp	x5, x3
 1c0:	b.ls	190 <__getf2+0x90>  // b.plast
 1c4:	mov	w0, #0xffffffff            	// #-1
 1c8:	b	1a4 <__getf2+0xa4>
 1cc:	cmp	x1, x2
 1d0:	b.gt	1f8 <__getf2+0xf8>
 1d4:	b.eq	1f0 <__getf2+0xf0>  // b.none
 1d8:	eor	x3, x3, x5
 1dc:	eor	x1, x1, x2
 1e0:	orr	x1, x3, x1
 1e4:	cmp	x1, #0x0
 1e8:	cset	w0, ne  // ne = any
 1ec:	b	1a4 <__getf2+0xa4>
 1f0:	cmp	x3, x5
 1f4:	b.ls	1d8 <__getf2+0xd8>  // b.plast
 1f8:	mov	w0, #0xffffffff            	// #-1
 1fc:	b	1a4 <__getf2+0xa4>

0000000000000200 <__unordtf2>:
 200:	sub	sp, sp, #0x10
 204:	str	q0, [sp]
 208:	ldr	x0, [sp]
 20c:	ldr	x2, [sp, #8]
 210:	mov	x4, x0
 214:	str	q1, [sp]
 218:	ldr	x0, [sp]
 21c:	ldr	x1, [sp, #8]
 220:	mov	x3, x0
 224:	and	x2, x2, #0x7fffffffffffffff
 228:	mov	w0, #0x1                   	// #1
 22c:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
 230:	cmp	x2, x5
 234:	b.hi	240 <__unordtf2+0x40>  // b.pmore
 238:	b.eq	26c <__unordtf2+0x6c>  // b.none
 23c:	mov	w0, #0x0                   	// #0
 240:	and	x1, x1, #0x7fffffffffffffff
 244:	mov	w2, #0x1                   	// #1
 248:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 24c:	cmp	x1, x4
 250:	b.hi	25c <__unordtf2+0x5c>  // b.pmore
 254:	b.eq	274 <__unordtf2+0x74>  // b.none
 258:	mov	w2, #0x0                   	// #0
 25c:	orr	w0, w0, w2
 260:	and	w0, w0, #0x1
 264:	add	sp, sp, #0x10
 268:	ret
 26c:	cbnz	x4, 240 <__unordtf2+0x40>
 270:	b	23c <__unordtf2+0x3c>
 274:	cbnz	x3, 25c <__unordtf2+0x5c>
 278:	b	258 <__unordtf2+0x58>

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	fmov	x0, d0
   4:	and	x1, x0, #0x7fffffffffffffff
   8:	and	x0, x0, #0x8000000000000000
   c:	mov	x2, #0xfff0000000000000    	// #-4503599627370496
  10:	add	x2, x1, x2
  14:	mov	x3, #0x7fdfffffffffffff    	// #9214364837600034815
  18:	cmp	x2, x3
  1c:	b.hi	3c <__extenddftf2+0x3c>  // b.pmore
  20:	lsl	x2, x1, #60
  24:	mov	x3, #0x3c00000000000000    	// #4323455642275676160
  28:	add	x1, x3, x1, lsr #4
  2c:	orr	x5, x0, x1
  30:	fmov	d0, x2
  34:	fmov	v0.d[1], x5
  38:	ret
  3c:	mov	x2, #0x7fefffffffffffff    	// #9218868437227405311
  40:	cmp	x1, x2
  44:	b.ls	58 <__extenddftf2+0x58>  // b.plast
  48:	ubfx	x3, x1, #4, #48
  4c:	lsl	x2, x1, #60
  50:	orr	x1, x3, #0x7fff000000000000
  54:	b	2c <__extenddftf2+0x2c>
  58:	cbz	x1, 9c <__extenddftf2+0x9c>
  5c:	clz	x4, x1
  60:	add	w2, w4, #0x31
  64:	subs	w5, w4, #0xf
  68:	lsl	x3, x1, x5
  6c:	lsr	x6, x1, #1
  70:	mov	w7, #0x3f                  	// #63
  74:	sub	w7, w7, w2
  78:	lsr	x6, x6, x7
  7c:	lsl	x2, x1, x2
  80:	csel	x1, x3, x6, pl  // pl = nfrst
  84:	eor	x1, x1, #0x1000000000000
  88:	mov	w3, #0x3c0c                	// #15372
  8c:	sub	w3, w3, w4
  90:	csel	x2, xzr, x2, pl  // pl = nfrst
  94:	orr	x1, x1, x3, lsl #48
  98:	b	2c <__extenddftf2+0x2c>
  9c:	mov	x2, #0x0                   	// #0
  a0:	mov	x1, #0x0                   	// #0
  a4:	b	2c <__extenddftf2+0x2c>

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	fmov	w0, s0
   4:	and	w1, w0, #0x7fffffff
   8:	and	w0, w0, #0x80000000
   c:	sub	w3, w1, #0x800, lsl #12
  10:	mov	w2, #0x7effffff            	// #2130706431
  14:	cmp	w3, w2
  18:	b.hi	40 <__extendsftf2+0x40>  // b.pmore
  1c:	ubfiz	x1, x1, #25, #31
  20:	mov	x4, #0x0                   	// #0
  24:	mov	x2, #0x3f80000000000000    	// #4575657221408423936
  28:	add	x1, x1, x2
  2c:	mov	w0, w0
  30:	orr	x3, x1, x0, lsl #32
  34:	fmov	d0, x4
  38:	fmov	v0.d[1], x3
  3c:	ret
  40:	mov	w2, #0x7f7fffff            	// #2139095039
  44:	cmp	w1, w2
  48:	b.ls	5c <__extendsftf2+0x5c>  // b.plast
  4c:	ubfiz	x1, x1, #25, #23
  50:	mov	x4, #0x0                   	// #0
  54:	orr	x1, x1, #0x7fff000000000000
  58:	b	2c <__extendsftf2+0x2c>
  5c:	cbz	w1, 88 <__extendsftf2+0x88>
  60:	clz	w3, w1
  64:	mov	w1, w1
  68:	add	w2, w3, #0x11
  6c:	lsl	x1, x1, x2
  70:	eor	x1, x1, #0x1000000000000
  74:	mov	w2, #0x3f89                	// #16265
  78:	sub	w2, w2, w3
  7c:	mov	x4, #0x0                   	// #0
  80:	orr	x1, x1, x2, lsl #48
  84:	b	2c <__extendsftf2+0x2c>
  88:	mov	x4, #0x0                   	// #0
  8c:	mov	x1, #0x0                   	// #0
  90:	b	2c <__extendsftf2+0x2c>

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x1, [sp, #8]
  10:	mov	x2, x0
  14:	mov	x0, #0x1                   	// #1
  18:	cmp	x1, #0x0
  1c:	cneg	x3, x0, lt  // lt = tstop
  20:	ubfx	x5, x1, #48, #15
  24:	sub	w4, w5, #0x3, lsl #12
  28:	sub	w4, w4, #0xfff
  2c:	mov	x0, #0x0                   	// #0
  30:	tbnz	w4, #31, 8c <__fixtfdi+0x8c>
  34:	cmp	w4, #0x3f
  38:	b.hi	80 <__fixtfdi+0x80>  // b.pmore
  3c:	and	x1, x1, #0xffffffffffff
  40:	orr	x1, x1, #0x1000000000000
  44:	cmp	w4, #0x6f
  48:	b.gt	94 <__fixtfdi+0x94>
  4c:	mov	w0, #0x70                  	// #112
  50:	sub	w0, w0, w4
  54:	subs	w5, w0, #0x40
  58:	lsr	x6, x1, x5
  5c:	lsl	x1, x1, #1
  60:	mov	w4, #0x3f                  	// #63
  64:	sub	w4, w4, w0
  68:	lsl	x1, x1, x4
  6c:	lsr	x0, x2, x0
  70:	orr	x0, x1, x0
  74:	csel	x0, x6, x0, pl  // pl = nfrst
  78:	mul	x0, x0, x3
  7c:	b	8c <__fixtfdi+0x8c>
  80:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  84:	cmp	x3, #0x1
  88:	cinv	x0, x0, eq  // eq = none
  8c:	add	sp, sp, #0x10
  90:	ret
  94:	sub	w0, w5, #0x4, lsl #12
  98:	sub	w0, w0, #0x6f
  9c:	lsl	x0, x2, x0
  a0:	mul	x0, x0, x3
  a4:	b	8c <__fixtfdi+0x8c>

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x1, [sp, #8]
  10:	mov	x2, x0
  14:	mov	w0, #0x1                   	// #1
  18:	cmp	x1, #0x0
  1c:	cneg	w3, w0, lt  // lt = tstop
  20:	ubfx	x5, x1, #48, #15
  24:	sub	w4, w5, #0x3, lsl #12
  28:	sub	w4, w4, #0xfff
  2c:	mov	w0, #0x0                   	// #0
  30:	tbnz	w4, #31, 8c <__fixtfsi+0x8c>
  34:	cmp	w4, #0x1f
  38:	b.hi	80 <__fixtfsi+0x80>  // b.pmore
  3c:	and	x1, x1, #0xffffffffffff
  40:	orr	x1, x1, #0x1000000000000
  44:	cmp	w4, #0x6f
  48:	b.gt	94 <__fixtfsi+0x94>
  4c:	mov	w0, #0x70                  	// #112
  50:	sub	w0, w0, w4
  54:	subs	w5, w0, #0x40
  58:	lsr	x6, x1, x5
  5c:	lsl	x1, x1, #1
  60:	mov	w4, #0x3f                  	// #63
  64:	sub	w4, w4, w0
  68:	lsl	x1, x1, x4
  6c:	lsr	x0, x2, x0
  70:	orr	x0, x1, x0
  74:	csel	x0, x6, x0, pl  // pl = nfrst
  78:	mul	w0, w0, w3
  7c:	b	8c <__fixtfsi+0x8c>
  80:	mov	w0, #0x80000000            	// #-2147483648
  84:	cmp	w3, #0x1
  88:	cinv	w0, w0, eq  // eq = none
  8c:	add	sp, sp, #0x10
  90:	ret
  94:	sub	w0, w5, #0x4, lsl #12
  98:	sub	w0, w0, #0x6f
  9c:	lsl	w0, w2, w0
  a0:	mul	w0, w0, w3
  a4:	b	8c <__fixtfsi+0x8c>

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x1, [sp]
   c:	ldr	x2, [sp, #8]
  10:	tbnz	x2, #63, 8c <__fixtfti+0x8c>
  14:	mov	x0, #0x1                   	// #1
  18:	mov	x3, #0x0                   	// #0
  1c:	ubfx	x5, x2, #48, #15
  20:	sub	w4, w5, #0x3, lsl #12
  24:	sub	w4, w4, #0xfff
  28:	tbnz	w4, #31, 11c <__fixtfti+0x11c>
  2c:	cmp	w4, #0x7f
  30:	b.hi	98 <__fixtfti+0x98>  // b.pmore
  34:	and	x2, x2, #0xffffffffffff
  38:	orr	x2, x2, #0x1000000000000
  3c:	cmp	w4, #0x6f
  40:	b.gt	cc <__fixtfti+0xcc>
  44:	mov	w5, #0x70                  	// #112
  48:	sub	w4, w5, w4
  4c:	subs	w5, w4, #0x40
  50:	lsr	x8, x2, x5
  54:	lsl	x6, x2, #1
  58:	mov	w7, #0x3f                  	// #63
  5c:	sub	w7, w7, w4
  60:	lsl	x6, x6, x7
  64:	lsr	x1, x1, x4
  68:	orr	x1, x6, x1
  6c:	lsr	x2, x2, x4
  70:	csel	x1, x8, x1, pl  // pl = nfrst
  74:	csel	x2, xzr, x2, pl  // pl = nfrst
  78:	umulh	x4, x1, x0
  7c:	madd	x4, x2, x0, x4
  80:	mul	x0, x1, x0
  84:	madd	x1, x1, x3, x4
  88:	b	124 <__fixtfti+0x124>
  8c:	mov	x0, #0xffffffffffffffff    	// #-1
  90:	mov	x3, x0
  94:	b	1c <__fixtfti+0x1c>
  98:	cmp	x0, #0x1
  9c:	b.eq	b4 <__fixtfti+0xb4>  // b.none
  a0:	adrp	x0, 0 <__fixtfti>
  a4:	add	x1, x0, #0x0
  a8:	ldr	x0, [x0]
  ac:	ldr	x1, [x1, #8]
  b0:	b	124 <__fixtfti+0x124>
  b4:	cbnz	x3, a0 <__fixtfti+0xa0>
  b8:	adrp	x0, 0 <__fixtfti>
  bc:	add	x1, x0, #0x0
  c0:	ldr	x0, [x0]
  c4:	ldr	x1, [x1, #8]
  c8:	b	124 <__fixtfti+0x124>
  cc:	sub	w4, w5, #0x4, lsl #12
  d0:	sub	w4, w4, #0x6f
  d4:	sub	w5, w5, #0x4, lsl #12
  d8:	sub	w5, w5, #0xaf
  dc:	lsl	x8, x1, x5
  e0:	lsr	x6, x1, #1
  e4:	mov	w7, #0x3f                  	// #63
  e8:	sub	w7, w7, w4
  ec:	lsr	x6, x6, x7
  f0:	lsl	x2, x2, x4
  f4:	orr	x2, x6, x2
  f8:	lsl	x1, x1, x4
  fc:	cmp	w5, #0x0
 100:	csel	x2, x8, x2, ge  // ge = tcont
 104:	csel	x1, xzr, x1, ge  // ge = tcont
 108:	umulh	x4, x1, x0
 10c:	madd	x4, x2, x0, x4
 110:	mul	x0, x1, x0
 114:	madd	x1, x1, x3, x4
 118:	b	124 <__fixtfti+0x124>
 11c:	mov	x0, #0x0                   	// #0
 120:	mov	x1, #0x0                   	// #0
 124:	add	sp, sp, #0x10
 128:	ret

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x1, [sp, #8]
  10:	mov	x2, x0
  14:	mov	x0, #0x0                   	// #0
  18:	tbnz	x1, #63, 74 <__fixunstfdi+0x74>
  1c:	lsr	x4, x1, #48
  20:	sub	w3, w4, #0x3, lsl #12
  24:	sub	w3, w3, #0xfff
  28:	and	x1, x1, #0xffffffffffff
  2c:	orr	x1, x1, #0x1000000000000
  30:	tbnz	w3, #31, 8c <__fixunstfdi+0x8c>
  34:	mov	x0, #0xffffffffffffffff    	// #-1
  38:	cmp	w3, #0x3f
  3c:	b.hi	74 <__fixunstfdi+0x74>  // b.pmore
  40:	cmp	w3, #0x6f
  44:	b.gt	7c <__fixunstfdi+0x7c>
  48:	mov	w0, #0x70                  	// #112
  4c:	sub	w0, w0, w3
  50:	subs	w5, w0, #0x40
  54:	lsr	x3, x1, x5
  58:	lsl	x1, x1, #1
  5c:	mov	w4, #0x3f                  	// #63
  60:	sub	w4, w4, w0
  64:	lsl	x1, x1, x4
  68:	lsr	x0, x2, x0
  6c:	orr	x0, x1, x0
  70:	csel	x0, x3, x0, pl  // pl = nfrst
  74:	add	sp, sp, #0x10
  78:	ret
  7c:	sub	w0, w4, #0x4, lsl #12
  80:	sub	w0, w0, #0x6f
  84:	lsl	x0, x2, x0
  88:	b	74 <__fixunstfdi+0x74>
  8c:	mov	x0, #0x0                   	// #0
  90:	b	74 <__fixunstfdi+0x74>

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x1, [sp, #8]
  10:	mov	x2, x0
  14:	mov	w0, #0x0                   	// #0
  18:	tbnz	x1, #63, 74 <__fixunstfsi+0x74>
  1c:	lsr	x4, x1, #48
  20:	sub	w3, w4, #0x3, lsl #12
  24:	sub	w3, w3, #0xfff
  28:	and	x1, x1, #0xffffffffffff
  2c:	orr	x1, x1, #0x1000000000000
  30:	tbnz	w3, #31, 8c <__fixunstfsi+0x8c>
  34:	mov	w0, #0xffffffff            	// #-1
  38:	cmp	w3, #0x1f
  3c:	b.hi	74 <__fixunstfsi+0x74>  // b.pmore
  40:	cmp	w3, #0x6f
  44:	b.gt	7c <__fixunstfsi+0x7c>
  48:	mov	w0, #0x70                  	// #112
  4c:	sub	w0, w0, w3
  50:	subs	w5, w0, #0x40
  54:	lsr	x3, x1, x5
  58:	lsl	x1, x1, #1
  5c:	mov	w4, #0x3f                  	// #63
  60:	sub	w4, w4, w0
  64:	lsl	x1, x1, x4
  68:	lsr	x0, x2, x0
  6c:	orr	x0, x1, x0
  70:	csel	x0, x3, x0, pl  // pl = nfrst
  74:	add	sp, sp, #0x10
  78:	ret
  7c:	sub	w0, w4, #0x4, lsl #12
  80:	sub	w0, w0, #0x6f
  84:	lsl	w0, w2, w0
  88:	b	74 <__fixunstfsi+0x74>
  8c:	mov	w0, #0x0                   	// #0
  90:	b	74 <__fixunstfsi+0x74>

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x1, [sp, #8]
  10:	tbnz	x1, #63, b4 <__fixunstfti+0xb4>
  14:	lsr	x2, x1, #48
  18:	sub	w3, w2, #0x3, lsl #12
  1c:	sub	w3, w3, #0xfff
  20:	and	x1, x1, #0xffffffffffff
  24:	orr	x1, x1, #0x1000000000000
  28:	tbnz	w3, #31, c4 <__fixunstfti+0xc4>
  2c:	cmp	w3, #0x7f
  30:	b.hi	d0 <__fixunstfti+0xd0>  // b.pmore
  34:	cmp	w3, #0x6f
  38:	b.gt	74 <__fixunstfti+0x74>
  3c:	mov	w2, #0x70                  	// #112
  40:	sub	w3, w2, w3
  44:	subs	w2, w3, #0x40
  48:	lsr	x6, x1, x2
  4c:	lsl	x4, x1, #1
  50:	mov	w5, #0x3f                  	// #63
  54:	sub	w5, w5, w3
  58:	lsl	x4, x4, x5
  5c:	lsr	x0, x0, x3
  60:	orr	x0, x4, x0
  64:	lsr	x1, x1, x3
  68:	csel	x0, x6, x0, pl  // pl = nfrst
  6c:	csel	x1, xzr, x1, pl  // pl = nfrst
  70:	b	bc <__fixunstfti+0xbc>
  74:	sub	w3, w2, #0x4, lsl #12
  78:	sub	w3, w3, #0x6f
  7c:	sub	w2, w2, #0x4, lsl #12
  80:	sub	w2, w2, #0xaf
  84:	lsl	x6, x0, x2
  88:	lsr	x4, x0, #1
  8c:	mov	w5, #0x3f                  	// #63
  90:	sub	w5, w5, w3
  94:	lsr	x4, x4, x5
  98:	lsl	x1, x1, x3
  9c:	orr	x1, x4, x1
  a0:	lsl	x0, x0, x3
  a4:	cmp	w2, #0x0
  a8:	csel	x1, x6, x1, ge  // ge = tcont
  ac:	csel	x0, xzr, x0, ge  // ge = tcont
  b0:	b	bc <__fixunstfti+0xbc>
  b4:	mov	x0, #0x0                   	// #0
  b8:	mov	x1, #0x0                   	// #0
  bc:	add	sp, sp, #0x10
  c0:	ret
  c4:	mov	x0, #0x0                   	// #0
  c8:	mov	x1, #0x0                   	// #0
  cc:	b	bc <__fixunstfti+0xbc>
  d0:	mov	x0, #0xffffffffffffffff    	// #-1
  d4:	mov	x1, x0
  d8:	b	bc <__fixunstfti+0xbc>

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	x0, 64 <__floatditf+0x64>
   8:	mov	x1, x0
   c:	mov	x8, #0x0                   	// #0
  10:	mov	x7, #0x0                   	// #0
  14:	tbnz	x0, #63, 68 <__floatditf+0x68>
  18:	clz	x3, x1
  1c:	add	w6, w3, #0x31
  20:	subs	w4, w3, #0xf
  24:	lsl	x2, x1, x4
  28:	lsr	x0, x1, #1
  2c:	mov	w5, #0x3f                  	// #63
  30:	sub	w5, w5, w6
  34:	lsr	x0, x0, x5
  38:	lsl	x1, x1, x6
  3c:	csel	x2, x2, x0, pl  // pl = nfrst
  40:	eor	x2, x2, #0x1000000000000
  44:	mov	w0, #0x403e                	// #16446
  48:	sub	w0, w0, w3
  4c:	csel	x1, xzr, x1, pl  // pl = nfrst
  50:	add	x0, x2, x0, lsl #48
  54:	orr	x2, x1, x8
  58:	orr	x3, x0, x7
  5c:	fmov	d0, x2
  60:	fmov	v0.d[1], x3
  64:	ret
  68:	neg	x1, x0
  6c:	adrp	x0, 0 <__floatditf>
  70:	add	x2, x0, #0x0
  74:	ldr	x8, [x0]
  78:	ldr	x7, [x2, #8]
  7c:	b	18 <__floatditf+0x18>

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	w0, 44 <__floatsitf+0x44>
   8:	mov	w1, w0
   c:	mov	x4, #0x0                   	// #0
  10:	mov	x3, #0x0                   	// #0
  14:	tbnz	w0, #31, 48 <__floatsitf+0x48>
  18:	clz	w0, w1
  1c:	mov	w1, w1
  20:	add	w2, w0, #0x11
  24:	lsl	x1, x1, x2
  28:	eor	x1, x1, #0x1000000000000
  2c:	mov	w2, #0x401e                	// #16414
  30:	sub	w2, w2, w0
  34:	add	x2, x1, x2, lsl #48
  38:	orr	x1, x2, x3
  3c:	fmov	d0, x4
  40:	fmov	v0.d[1], x1
  44:	ret
  48:	neg	w1, w0
  4c:	adrp	x0, 0 <__floatsitf>
  50:	add	x2, x0, #0x0
  54:	ldr	x4, [x0]
  58:	ldr	x3, [x2, #8]
  5c:	b	18 <__floatsitf+0x18>

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	orr	x2, x0, x1
   4:	movi	v0.2d, #0x0
   8:	cbnz	x2, 10 <__floattitf+0x10>
   c:	ret
  10:	stp	x29, x30, [sp, #-64]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	stp	x21, x22, [sp, #32]
  20:	str	x23, [sp, #48]
  24:	asr	x21, x1, #63
  28:	eor	x0, x0, x21
  2c:	eor	x1, x1, x21
  30:	subs	x20, x0, x21
  34:	sbc	x19, x1, x21
  38:	mov	x23, x20
  3c:	mov	x22, x19
  40:	mov	x0, x20
  44:	mov	x1, x19
  48:	bl	0 <__clzti2>
  4c:	mov	w2, #0x80                  	// #128
  50:	sub	w2, w2, w0
  54:	sub	w5, w2, #0x1
  58:	cmp	w2, #0x71
  5c:	b.le	120 <__floattitf+0x120>
  60:	cmp	w2, #0x72
  64:	b.eq	ec <__floattitf+0xec>  // b.none
  68:	cmp	w2, #0x73
  6c:	b.eq	f4 <__floattitf+0xf4>  // b.none
  70:	add	w6, w0, #0x73
  74:	adds	w0, w0, #0x33
  78:	mov	x3, #0xffffffffffffffff    	// #-1
  7c:	lsr	x8, x3, x0
  80:	mov	w4, #0x3f                  	// #63
  84:	sub	w1, w4, w6
  88:	mov	x7, #0xfffffffffffffffe    	// #-2
  8c:	lsl	x7, x7, x1
  90:	lsr	x1, x3, x6
  94:	orr	x1, x7, x1
  98:	lsr	x3, x3, x6
  9c:	csel	x1, x8, x1, pl  // pl = nfrst
  a0:	csel	x3, xzr, x3, pl  // pl = nfrst
  a4:	and	x1, x1, x20
  a8:	and	x3, x3, x19
  ac:	orr	x1, x1, x3
  b0:	cmp	x1, #0x0
  b4:	cset	x23, ne  // ne = any
  b8:	sub	w22, w2, #0x73
  bc:	subs	w0, w2, #0xb3
  c0:	lsr	x3, x19, x0
  c4:	lsl	x1, x19, #1
  c8:	sub	w4, w4, w22
  cc:	lsl	x4, x1, x4
  d0:	lsr	x20, x20, x22
  d4:	orr	x20, x4, x20
  d8:	lsr	x19, x19, x22
  dc:	csel	x20, x3, x20, pl  // pl = nfrst
  e0:	orr	x23, x23, x20
  e4:	csel	x22, xzr, x19, pl  // pl = nfrst
  e8:	b	f4 <__floattitf+0xf4>
  ec:	lsl	x23, x20, #1
  f0:	extr	x22, x19, x20, #63
  f4:	ubfx	x20, x23, #2, #1
  f8:	orr	x20, x20, x23
  fc:	adds	x3, x20, #0x1
 100:	cinc	x19, x22, cs  // cs = hs, nlast
 104:	extr	x20, x19, x3, #2
 108:	asr	x1, x19, #2
 10c:	tbz	x19, #51, 154 <__floattitf+0x154>
 110:	extr	x20, x19, x3, #3
 114:	asr	x1, x19, #3
 118:	mov	w5, w2
 11c:	b	154 <__floattitf+0x154>
 120:	mov	w1, #0x71                  	// #113
 124:	sub	w2, w1, w2
 128:	subs	w3, w2, #0x40
 12c:	lsl	x0, x20, x3
 130:	lsr	x4, x20, #1
 134:	mov	w1, #0x3f                  	// #63
 138:	sub	w1, w1, w2
 13c:	lsr	x4, x4, x1
 140:	lsl	x1, x19, x2
 144:	orr	x1, x4, x1
 148:	lsl	x20, x20, x2
 14c:	csel	x1, x0, x1, pl  // pl = nfrst
 150:	csel	x20, xzr, x20, pl  // pl = nfrst
 154:	and	x1, x1, #0xffffffffffff
 158:	and	x0, x21, #0x8000000000000000
 15c:	orr	x1, x1, x0
 160:	add	w0, w5, #0x3, lsl #12
 164:	add	w0, w0, #0xfff
 168:	orr	x3, x1, x0, lsl #48
 16c:	fmov	d0, x20
 170:	fmov	v0.d[1], x3
 174:	ldp	x19, x20, [sp, #16]
 178:	ldp	x21, x22, [sp, #32]
 17c:	ldr	x23, [sp, #48]
 180:	ldp	x29, x30, [sp], #64
 184:	ret

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	x0, 4c <__floatunditf+0x4c>
   8:	clz	x2, x0
   c:	add	w6, w2, #0x31
  10:	subs	w3, w2, #0xf
  14:	lsl	x1, x0, x3
  18:	lsr	x4, x0, #1
  1c:	mov	w5, #0x3f                  	// #63
  20:	sub	w5, w5, w6
  24:	lsr	x4, x4, x5
  28:	lsl	x0, x0, x6
  2c:	csel	x1, x1, x4, pl  // pl = nfrst
  30:	eor	x1, x1, #0x1000000000000
  34:	mov	w4, #0x403e                	// #16446
  38:	sub	w4, w4, w2
  3c:	csel	x2, xzr, x0, pl  // pl = nfrst
  40:	add	x3, x1, x4, lsl #48
  44:	fmov	d0, x2
  48:	fmov	v0.d[1], x3
  4c:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	w0, 34 <__floatunsitf+0x34>
   8:	clz	w2, w0
   c:	mov	w0, w0
  10:	add	w1, w2, #0x11
  14:	lsl	x0, x0, x1
  18:	eor	x0, x0, #0x1000000000000
  1c:	mov	w1, #0x401e                	// #16414
  20:	sub	w1, w1, w2
  24:	mov	x2, #0x0                   	// #0
  28:	add	x3, x0, x1, lsl #48
  2c:	fmov	d0, x2
  30:	fmov	v0.d[1], x3
  34:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x0
  10:	orr	x0, x0, x1
  14:	movi	v0.2d, #0x0
  18:	cbnz	x0, 28 <__floatuntitf+0x28>
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret
  28:	mov	x20, x1
  2c:	mov	x0, x19
  30:	bl	0 <__clzti2>
  34:	mov	w2, #0x80                  	// #128
  38:	sub	w2, w2, w0
  3c:	sub	w5, w2, #0x1
  40:	cmp	w2, #0x71
  44:	b.le	110 <__floatuntitf+0x110>
  48:	cmp	w2, #0x72
  4c:	b.eq	d8 <__floatuntitf+0xd8>  // b.none
  50:	cmp	w2, #0x73
  54:	b.eq	e4 <__floatuntitf+0xe4>  // b.none
  58:	sub	w4, w2, #0x73
  5c:	subs	w1, w2, #0xb3
  60:	lsr	x8, x20, x1
  64:	lsl	x7, x20, #1
  68:	mov	w3, #0x3f                  	// #63
  6c:	sub	w6, w3, w4
  70:	lsl	x7, x7, x6
  74:	lsr	x6, x19, x4
  78:	orr	x6, x7, x6
  7c:	lsr	x4, x20, x4
  80:	csel	x6, x8, x6, pl  // pl = nfrst
  84:	csel	x4, xzr, x4, pl  // pl = nfrst
  88:	add	w7, w0, #0x73
  8c:	adds	w0, w0, #0x33
  90:	mov	x1, #0xffffffffffffffff    	// #-1
  94:	lsr	x9, x1, x0
  98:	sub	w3, w3, w7
  9c:	mov	x8, #0xfffffffffffffffe    	// #-2
  a0:	lsl	x8, x8, x3
  a4:	lsr	x3, x1, x7
  a8:	orr	x3, x8, x3
  ac:	lsr	x1, x1, x7
  b0:	csel	x3, x9, x3, pl  // pl = nfrst
  b4:	csel	x1, xzr, x1, pl  // pl = nfrst
  b8:	and	x19, x3, x19
  bc:	and	x1, x1, x20
  c0:	orr	x1, x19, x1
  c4:	cmp	x1, #0x0
  c8:	cset	x19, ne  // ne = any
  cc:	orr	x19, x6, x19
  d0:	mov	x20, x4
  d4:	b	e4 <__floatuntitf+0xe4>
  d8:	extr	x1, x20, x19, #63
  dc:	mov	x20, x1
  e0:	lsl	x19, x19, #1
  e4:	ubfx	x1, x19, #2, #1
  e8:	orr	x19, x1, x19
  ec:	adds	x3, x19, #0x1
  f0:	cinc	x20, x20, cs  // cs = hs, nlast
  f4:	extr	x19, x20, x3, #2
  f8:	lsr	x1, x20, #2
  fc:	tbz	x20, #51, 144 <__floatuntitf+0x144>
 100:	extr	x19, x20, x3, #3
 104:	lsr	x1, x20, #3
 108:	mov	w5, w2
 10c:	b	144 <__floatuntitf+0x144>
 110:	mov	w1, #0x71                  	// #113
 114:	sub	w2, w1, w2
 118:	subs	w3, w2, #0x40
 11c:	lsl	x0, x19, x3
 120:	lsr	x4, x19, #1
 124:	mov	w1, #0x3f                  	// #63
 128:	sub	w1, w1, w2
 12c:	lsr	x4, x4, x1
 130:	lsl	x1, x20, x2
 134:	orr	x1, x4, x1
 138:	lsl	x19, x19, x2
 13c:	csel	x1, x0, x1, pl  // pl = nfrst
 140:	csel	x19, xzr, x19, pl  // pl = nfrst
 144:	add	w0, w5, #0x3, lsl #12
 148:	add	w0, w0, #0xfff
 14c:	bfi	x1, x0, #48, #16
 150:	fmov	d0, x19
 154:	fmov	v0.d[1], x1
 158:	b	1c <__floatuntitf+0x1c>

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-224]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	stp	x25, x26, [sp, #64]
  18:	stp	x27, x28, [sp, #80]
  1c:	str	q0, [sp, #96]
  20:	ldr	x19, [sp, #96]
  24:	ldr	x24, [sp, #104]
  28:	str	q1, [sp, #96]
  2c:	ldr	x20, [sp, #96]
  30:	ldr	x23, [sp, #104]
  34:	str	q2, [sp, #96]
  38:	ldr	x21, [sp, #96]
  3c:	ldr	x26, [sp, #104]
  40:	str	q3, [sp, #96]
  44:	ldr	x22, [sp, #96]
  48:	ldr	x25, [sp, #104]
  4c:	str	x21, [sp, #96]
  50:	str	x26, [sp, #104]
  54:	ldr	q1, [sp, #96]
  58:	str	x19, [sp, #96]
  5c:	str	x24, [sp, #104]
  60:	ldr	q0, [sp, #96]
  64:	bl	0 <__multf3>
  68:	str	q0, [sp, #96]
  6c:	ldr	x0, [sp, #96]
  70:	ldr	x27, [sp, #104]
  74:	str	x0, [sp, #144]
  78:	str	x22, [sp, #96]
  7c:	str	x25, [sp, #104]
  80:	ldr	q1, [sp, #96]
  84:	str	x20, [sp, #96]
  88:	str	x23, [sp, #104]
  8c:	ldr	q0, [sp, #96]
  90:	bl	0 <__multf3>
  94:	str	q0, [sp, #96]
  98:	ldr	x1, [sp, #96]
  9c:	ldr	x0, [sp, #104]
  a0:	mov	x28, x1
  a4:	str	x1, [sp, #192]
  a8:	str	x0, [sp, #152]
  ac:	str	x22, [sp, #96]
  b0:	str	x25, [sp, #104]
  b4:	ldr	q1, [sp, #96]
  b8:	str	x19, [sp, #96]
  bc:	str	x24, [sp, #104]
  c0:	ldr	q0, [sp, #96]
  c4:	bl	0 <__multf3>
  c8:	str	q0, [sp, #96]
  cc:	ldr	x1, [sp, #96]
  d0:	ldr	x0, [sp, #104]
  d4:	str	x1, [sp, #160]
  d8:	str	x0, [sp, #168]
  dc:	str	x20, [sp, #96]
  e0:	str	x23, [sp, #104]
  e4:	ldr	q1, [sp, #96]
  e8:	str	x21, [sp, #96]
  ec:	str	x26, [sp, #104]
  f0:	ldr	q0, [sp, #96]
  f4:	bl	0 <__multf3>
  f8:	str	q0, [sp, #96]
  fc:	ldr	x1, [sp, #96]
 100:	ldr	x0, [sp, #104]
 104:	str	x1, [sp, #176]
 108:	str	x0, [sp, #184]
 10c:	str	x28, [sp, #96]
 110:	ldr	x2, [sp, #152]
 114:	str	x2, [sp, #104]
 118:	ldr	q1, [sp, #96]
 11c:	ldr	x2, [sp, #144]
 120:	str	x2, [sp, #96]
 124:	str	x27, [sp, #104]
 128:	ldr	q0, [sp, #96]
 12c:	bl	0 <__subtf3>
 130:	str	q0, [sp, #112]
 134:	ldr	x2, [sp, #176]
 138:	str	x2, [sp, #96]
 13c:	ldr	x2, [sp, #184]
 140:	str	x2, [sp, #104]
 144:	ldr	q1, [sp, #96]
 148:	ldr	x1, [sp, #160]
 14c:	str	x1, [sp, #96]
 150:	ldr	x0, [sp, #168]
 154:	str	x0, [sp, #104]
 158:	ldr	q0, [sp, #96]
 15c:	bl	0 <__addtf3>
 160:	mov	v1.16b, v0.16b
 164:	ldr	q2, [sp, #112]
 168:	str	q2, [sp, #96]
 16c:	str	q0, [sp, #128]
 170:	bl	0 <__unordtf2>
 174:	cmp	w0, #0x0
 178:	cset	w28, ne  // ne = any
 17c:	ldr	q0, [sp, #112]
 180:	mov	v1.16b, v0.16b
 184:	bl	0 <__unordtf2>
 188:	cmp	w0, #0x0
 18c:	cset	w0, ne  // ne = any
 190:	tst	w0, w28
 194:	b.ne	1bc <__multc3+0x1bc>  // b.any
 198:	ldr	q0, [sp, #96]
 19c:	ldr	q1, [sp, #128]
 1a0:	ldp	x19, x20, [sp, #16]
 1a4:	ldp	x21, x22, [sp, #32]
 1a8:	ldp	x23, x24, [sp, #48]
 1ac:	ldp	x25, x26, [sp, #64]
 1b0:	ldp	x27, x28, [sp, #80]
 1b4:	ldp	x29, x30, [sp], #224
 1b8:	ret
 1bc:	str	x19, [sp, #208]
 1c0:	and	x0, x24, #0x7fffffffffffffff
 1c4:	mov	w28, #0x1                   	// #1
 1c8:	adrp	x1, 0 <__multc3>
 1cc:	add	x1, x1, #0x0
 1d0:	ldr	q1, [x1]
 1d4:	str	x19, [sp, #112]
 1d8:	str	x0, [sp, #200]
 1dc:	str	x0, [sp, #120]
 1e0:	ldr	q0, [sp, #112]
 1e4:	bl	0 <__unordtf2>
 1e8:	cbnz	w0, 214 <__multc3+0x214>
 1ec:	adrp	x1, 0 <__multc3>
 1f0:	add	x1, x1, #0x0
 1f4:	ldr	q1, [x1]
 1f8:	str	x19, [sp, #112]
 1fc:	ldr	x0, [sp, #200]
 200:	str	x0, [sp, #120]
 204:	ldr	q0, [sp, #112]
 208:	bl	0 <__letf2>
 20c:	cmp	w0, #0x0
 210:	csel	w28, w28, wzr, le
 214:	eor	w28, w28, #0x1
 218:	and	w28, w28, #0xff
 21c:	adrp	x0, 0 <__multc3>
 220:	add	x0, x0, #0x0
 224:	ldr	q1, [x0]
 228:	ldr	x0, [sp, #208]
 22c:	str	x0, [sp, #112]
 230:	ldr	x1, [sp, #200]
 234:	str	x1, [sp, #120]
 238:	ldr	q0, [sp, #112]
 23c:	bl	0 <__unordtf2>
 240:	cbnz	w0, 540 <__multc3+0x540>
 244:	adrp	x2, 0 <__multc3>
 248:	add	x2, x2, #0x0
 24c:	ldr	q1, [x2]
 250:	ldr	x0, [sp, #208]
 254:	str	x0, [sp, #112]
 258:	ldr	x1, [sp, #200]
 25c:	str	x1, [sp, #120]
 260:	ldr	q0, [sp, #112]
 264:	bl	0 <__letf2>
 268:	cmp	w0, #0x0
 26c:	b.le	540 <__multc3+0x540>
 270:	mov	w0, w28
 274:	bl	0 <__floatsitf>
 278:	str	q0, [sp, #112]
 27c:	ldr	x19, [sp, #112]
 280:	ldr	x0, [sp, #120]
 284:	bfxil	x24, x0, #0, #63
 288:	and	x1, x23, #0x7fffffffffffffff
 28c:	mov	w28, #0x1                   	// #1
 290:	adrp	x0, 0 <__multc3>
 294:	add	x0, x0, #0x0
 298:	ldr	q1, [x0]
 29c:	str	x20, [sp, #112]
 2a0:	str	x1, [sp, #200]
 2a4:	str	x1, [sp, #120]
 2a8:	ldr	q0, [sp, #112]
 2ac:	bl	0 <__unordtf2>
 2b0:	cbnz	w0, 2dc <__multc3+0x2dc>
 2b4:	adrp	x0, 0 <__multc3>
 2b8:	add	x0, x0, #0x0
 2bc:	ldr	q1, [x0]
 2c0:	str	x20, [sp, #112]
 2c4:	ldr	x1, [sp, #200]
 2c8:	str	x1, [sp, #120]
 2cc:	ldr	q0, [sp, #112]
 2d0:	bl	0 <__letf2>
 2d4:	cmp	w0, #0x0
 2d8:	csel	w28, w28, wzr, le
 2dc:	eor	w0, w28, #0x1
 2e0:	and	w0, w0, #0x1
 2e4:	bl	0 <__floatsitf>
 2e8:	str	q0, [sp, #112]
 2ec:	ldr	x20, [sp, #112]
 2f0:	ldr	x0, [sp, #120]
 2f4:	bfxil	x23, x0, #0, #63
 2f8:	str	x21, [sp, #112]
 2fc:	str	x26, [sp, #120]
 300:	ldr	q1, [sp, #112]
 304:	ldr	q0, [sp, #112]
 308:	bl	0 <__unordtf2>
 30c:	cbnz	w0, 598 <__multc3+0x598>
 310:	str	x22, [sp, #112]
 314:	str	x25, [sp, #120]
 318:	ldr	q1, [sp, #112]
 31c:	ldr	q0, [sp, #112]
 320:	bl	0 <__unordtf2>
 324:	cbnz	w0, 5b0 <__multc3+0x5b0>
 328:	mov	w0, #0x1                   	// #1
 32c:	str	w0, [sp, #208]
 330:	str	x21, [sp, #200]
 334:	and	x28, x26, #0x7fffffffffffffff
 338:	adrp	x0, 0 <__multc3>
 33c:	add	x0, x0, #0x0
 340:	ldr	q1, [x0]
 344:	str	x21, [sp, #112]
 348:	str	x28, [sp, #120]
 34c:	ldr	q0, [sp, #112]
 350:	bl	0 <__unordtf2>
 354:	cbnz	w0, 5d0 <__multc3+0x5d0>
 358:	adrp	x0, 0 <__multc3>
 35c:	add	x0, x0, #0x0
 360:	ldr	q1, [x0]
 364:	str	x21, [sp, #112]
 368:	str	x28, [sp, #120]
 36c:	ldr	q0, [sp, #112]
 370:	bl	0 <__letf2>
 374:	cmp	w0, #0x0
 378:	b.le	5d0 <__multc3+0x5d0>
 37c:	str	x19, [sp, #96]
 380:	str	x24, [sp, #104]
 384:	ldr	q1, [sp, #96]
 388:	ldr	q0, [sp, #96]
 38c:	bl	0 <__unordtf2>
 390:	cbnz	w0, 6ec <__multc3+0x6ec>
 394:	str	x20, [sp, #96]
 398:	str	x23, [sp, #104]
 39c:	ldr	q1, [sp, #96]
 3a0:	ldr	q0, [sp, #96]
 3a4:	bl	0 <__unordtf2>
 3a8:	cbnz	w0, 704 <__multc3+0x704>
 3ac:	mov	w21, #0x1                   	// #1
 3b0:	adrp	x0, 0 <__multc3>
 3b4:	add	x0, x0, #0x0
 3b8:	ldr	q1, [x0]
 3bc:	ldr	x27, [sp, #200]
 3c0:	str	x27, [sp, #96]
 3c4:	str	x28, [sp, #104]
 3c8:	ldr	q0, [sp, #96]
 3cc:	bl	0 <__unordtf2>
 3d0:	cbnz	w0, 3f8 <__multc3+0x3f8>
 3d4:	adrp	x0, 0 <__multc3>
 3d8:	add	x0, x0, #0x0
 3dc:	ldr	q1, [x0]
 3e0:	str	x27, [sp, #96]
 3e4:	str	x28, [sp, #104]
 3e8:	ldr	q0, [sp, #96]
 3ec:	bl	0 <__letf2>
 3f0:	cmp	w0, #0x0
 3f4:	csel	w21, w21, wzr, le
 3f8:	eor	w0, w21, #0x1
 3fc:	and	w0, w0, #0x1
 400:	bl	0 <__floatsitf>
 404:	str	q0, [sp, #96]
 408:	ldr	x21, [sp, #96]
 40c:	ldr	x0, [sp, #104]
 410:	bfxil	x26, x0, #0, #63
 414:	and	x28, x25, #0x7fffffffffffffff
 418:	mov	w27, #0x1                   	// #1
 41c:	adrp	x0, 0 <__multc3>
 420:	add	x0, x0, #0x0
 424:	ldr	q1, [x0]
 428:	str	x22, [sp, #96]
 42c:	str	x28, [sp, #104]
 430:	ldr	q0, [sp, #96]
 434:	bl	0 <__unordtf2>
 438:	cbnz	w0, 460 <__multc3+0x460>
 43c:	adrp	x0, 0 <__multc3>
 440:	add	x0, x0, #0x0
 444:	ldr	q1, [x0]
 448:	str	x22, [sp, #96]
 44c:	str	x28, [sp, #104]
 450:	ldr	q0, [sp, #96]
 454:	bl	0 <__letf2>
 458:	cmp	w0, #0x0
 45c:	csel	w27, w27, wzr, le
 460:	eor	w0, w27, #0x1
 464:	and	w0, w0, #0x1
 468:	bl	0 <__floatsitf>
 46c:	str	q0, [sp, #96]
 470:	ldr	x22, [sp, #96]
 474:	ldr	x0, [sp, #104]
 478:	bfxil	x25, x0, #0, #63
 47c:	str	x21, [sp, #96]
 480:	str	x26, [sp, #104]
 484:	ldr	q1, [sp, #96]
 488:	str	x19, [sp, #96]
 48c:	str	x24, [sp, #104]
 490:	ldr	q0, [sp, #96]
 494:	bl	0 <__multf3>
 498:	str	q0, [sp, #112]
 49c:	str	x22, [sp, #96]
 4a0:	str	x25, [sp, #104]
 4a4:	ldr	q1, [sp, #96]
 4a8:	str	x20, [sp, #96]
 4ac:	str	x23, [sp, #104]
 4b0:	ldr	q0, [sp, #96]
 4b4:	bl	0 <__multf3>
 4b8:	mov	v1.16b, v0.16b
 4bc:	ldr	q0, [sp, #112]
 4c0:	bl	0 <__subtf3>
 4c4:	adrp	x0, 0 <__multc3>
 4c8:	add	x0, x0, #0x0
 4cc:	ldr	q1, [x0]
 4d0:	bl	0 <__multf3>
 4d4:	str	q0, [sp, #112]
 4d8:	str	x22, [sp, #96]
 4dc:	str	x25, [sp, #104]
 4e0:	ldr	q1, [sp, #96]
 4e4:	str	x19, [sp, #96]
 4e8:	str	x24, [sp, #104]
 4ec:	ldr	q0, [sp, #96]
 4f0:	bl	0 <__multf3>
 4f4:	str	q0, [sp, #128]
 4f8:	str	x21, [sp, #96]
 4fc:	str	x26, [sp, #104]
 500:	ldr	q1, [sp, #96]
 504:	str	x20, [sp, #96]
 508:	str	x23, [sp, #104]
 50c:	ldr	q0, [sp, #96]
 510:	bl	0 <__multf3>
 514:	mov	v1.16b, v0.16b
 518:	ldr	q0, [sp, #128]
 51c:	bl	0 <__addtf3>
 520:	adrp	x0, 0 <__multc3>
 524:	add	x0, x0, #0x0
 528:	ldr	q1, [x0]
 52c:	bl	0 <__multf3>
 530:	ldr	q1, [sp, #112]
 534:	str	q1, [sp, #96]
 538:	str	q0, [sp, #128]
 53c:	b	198 <__multc3+0x198>
 540:	and	x0, x23, #0x7fffffffffffffff
 544:	adrp	x1, 0 <__multc3>
 548:	add	x1, x1, #0x0
 54c:	ldr	q1, [x1]
 550:	str	x20, [sp, #112]
 554:	str	x0, [sp, #200]
 558:	str	x0, [sp, #120]
 55c:	ldr	q0, [sp, #112]
 560:	bl	0 <__unordtf2>
 564:	cbnz	w0, 590 <__multc3+0x590>
 568:	adrp	x1, 0 <__multc3>
 56c:	add	x1, x1, #0x0
 570:	ldr	q1, [x1]
 574:	str	x20, [sp, #112]
 578:	ldr	x0, [sp, #200]
 57c:	str	x0, [sp, #120]
 580:	ldr	q0, [sp, #112]
 584:	bl	0 <__letf2>
 588:	cmp	w0, #0x0
 58c:	b.gt	270 <__multc3+0x270>
 590:	str	wzr, [sp, #208]
 594:	b	330 <__multc3+0x330>
 598:	mov	x21, #0x0                   	// #0
 59c:	mov	x0, #0x0                   	// #0
 5a0:	tbz	x26, #63, 5a8 <__multc3+0x5a8>
 5a4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 5a8:	mov	x26, x0
 5ac:	b	310 <__multc3+0x310>
 5b0:	mov	x22, #0x0                   	// #0
 5b4:	mov	x0, #0x0                   	// #0
 5b8:	tbz	x25, #63, 5c0 <__multc3+0x5c0>
 5bc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 5c0:	mov	x25, x0
 5c4:	mov	w0, #0x1                   	// #1
 5c8:	str	w0, [sp, #208]
 5cc:	b	330 <__multc3+0x330>
 5d0:	and	x0, x25, #0x7fffffffffffffff
 5d4:	adrp	x1, 0 <__multc3>
 5d8:	add	x1, x1, #0x0
 5dc:	ldr	q1, [x1]
 5e0:	str	x22, [sp, #112]
 5e4:	str	x0, [sp, #216]
 5e8:	str	x0, [sp, #120]
 5ec:	ldr	q0, [sp, #112]
 5f0:	bl	0 <__unordtf2>
 5f4:	cbnz	w0, 620 <__multc3+0x620>
 5f8:	adrp	x1, 0 <__multc3>
 5fc:	add	x1, x1, #0x0
 600:	ldr	q1, [x1]
 604:	str	x22, [sp, #112]
 608:	ldr	x0, [sp, #216]
 60c:	str	x0, [sp, #120]
 610:	ldr	q0, [sp, #112]
 614:	bl	0 <__letf2>
 618:	cmp	w0, #0x0
 61c:	b.gt	37c <__multc3+0x37c>
 620:	ldr	w0, [sp, #208]
 624:	cbnz	w0, 47c <__multc3+0x47c>
 628:	and	x27, x27, #0x7fffffffffffffff
 62c:	adrp	x0, 0 <__multc3>
 630:	add	x0, x0, #0x0
 634:	ldr	q1, [x0]
 638:	ldr	x28, [sp, #144]
 63c:	str	x28, [sp, #112]
 640:	str	x27, [sp, #120]
 644:	ldr	q0, [sp, #112]
 648:	bl	0 <__unordtf2>
 64c:	cbnz	w0, 71c <__multc3+0x71c>
 650:	adrp	x0, 0 <__multc3>
 654:	add	x0, x0, #0x0
 658:	ldr	q1, [x0]
 65c:	str	x28, [sp, #112]
 660:	str	x27, [sp, #120]
 664:	ldr	q0, [sp, #112]
 668:	bl	0 <__letf2>
 66c:	cmp	w0, #0x0
 670:	b.le	71c <__multc3+0x71c>
 674:	str	x19, [sp, #96]
 678:	str	x24, [sp, #104]
 67c:	ldr	q1, [sp, #96]
 680:	ldr	q0, [sp, #96]
 684:	bl	0 <__unordtf2>
 688:	cbnz	w0, 810 <__multc3+0x810>
 68c:	str	x20, [sp, #96]
 690:	str	x23, [sp, #104]
 694:	ldr	q1, [sp, #96]
 698:	ldr	q0, [sp, #96]
 69c:	bl	0 <__unordtf2>
 6a0:	cbnz	w0, 828 <__multc3+0x828>
 6a4:	str	x21, [sp, #96]
 6a8:	str	x26, [sp, #104]
 6ac:	ldr	q1, [sp, #96]
 6b0:	ldr	q0, [sp, #96]
 6b4:	bl	0 <__unordtf2>
 6b8:	cbnz	w0, 840 <__multc3+0x840>
 6bc:	str	x22, [sp, #96]
 6c0:	str	x25, [sp, #104]
 6c4:	ldr	q1, [sp, #96]
 6c8:	ldr	q0, [sp, #96]
 6cc:	bl	0 <__unordtf2>
 6d0:	cbz	w0, 47c <__multc3+0x47c>
 6d4:	mov	x22, #0x0                   	// #0
 6d8:	mov	x0, #0x0                   	// #0
 6dc:	tbz	x25, #63, 6e4 <__multc3+0x6e4>
 6e0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6e4:	mov	x25, x0
 6e8:	b	47c <__multc3+0x47c>
 6ec:	mov	x19, #0x0                   	// #0
 6f0:	mov	x0, #0x0                   	// #0
 6f4:	tbz	x24, #63, 6fc <__multc3+0x6fc>
 6f8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6fc:	mov	x24, x0
 700:	b	394 <__multc3+0x394>
 704:	mov	x20, #0x0                   	// #0
 708:	mov	x0, #0x0                   	// #0
 70c:	tbz	x23, #63, 714 <__multc3+0x714>
 710:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 714:	mov	x23, x0
 718:	b	3ac <__multc3+0x3ac>
 71c:	ldr	x0, [sp, #152]
 720:	and	x27, x0, #0x7fffffffffffffff
 724:	adrp	x0, 0 <__multc3>
 728:	add	x0, x0, #0x0
 72c:	ldr	q1, [x0]
 730:	ldr	x28, [sp, #192]
 734:	str	x28, [sp, #112]
 738:	str	x27, [sp, #120]
 73c:	ldr	q0, [sp, #112]
 740:	bl	0 <__unordtf2>
 744:	cbnz	w0, 76c <__multc3+0x76c>
 748:	adrp	x0, 0 <__multc3>
 74c:	add	x0, x0, #0x0
 750:	ldr	q1, [x0]
 754:	str	x28, [sp, #112]
 758:	str	x27, [sp, #120]
 75c:	ldr	q0, [sp, #112]
 760:	bl	0 <__letf2>
 764:	cmp	w0, #0x0
 768:	b.gt	674 <__multc3+0x674>
 76c:	ldr	x0, [sp, #168]
 770:	and	x27, x0, #0x7fffffffffffffff
 774:	adrp	x0, 0 <__multc3>
 778:	add	x0, x0, #0x0
 77c:	ldr	q1, [x0]
 780:	ldr	x28, [sp, #160]
 784:	str	x28, [sp, #112]
 788:	str	x27, [sp, #120]
 78c:	ldr	q0, [sp, #112]
 790:	bl	0 <__unordtf2>
 794:	cbnz	w0, 7bc <__multc3+0x7bc>
 798:	adrp	x0, 0 <__multc3>
 79c:	add	x0, x0, #0x0
 7a0:	ldr	q1, [x0]
 7a4:	str	x28, [sp, #112]
 7a8:	str	x27, [sp, #120]
 7ac:	ldr	q0, [sp, #112]
 7b0:	bl	0 <__letf2>
 7b4:	cmp	w0, #0x0
 7b8:	b.gt	674 <__multc3+0x674>
 7bc:	ldr	x0, [sp, #184]
 7c0:	and	x27, x0, #0x7fffffffffffffff
 7c4:	adrp	x0, 0 <__multc3>
 7c8:	add	x0, x0, #0x0
 7cc:	ldr	q1, [x0]
 7d0:	ldr	x28, [sp, #176]
 7d4:	str	x28, [sp, #112]
 7d8:	str	x27, [sp, #120]
 7dc:	ldr	q0, [sp, #112]
 7e0:	bl	0 <__unordtf2>
 7e4:	cbnz	w0, 198 <__multc3+0x198>
 7e8:	adrp	x0, 0 <__multc3>
 7ec:	add	x0, x0, #0x0
 7f0:	ldr	q1, [x0]
 7f4:	str	x28, [sp, #112]
 7f8:	str	x27, [sp, #120]
 7fc:	ldr	q0, [sp, #112]
 800:	bl	0 <__letf2>
 804:	cmp	w0, #0x0
 808:	b.gt	674 <__multc3+0x674>
 80c:	b	198 <__multc3+0x198>
 810:	mov	x19, #0x0                   	// #0
 814:	mov	x0, #0x0                   	// #0
 818:	tbz	x24, #63, 820 <__multc3+0x820>
 81c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 820:	mov	x24, x0
 824:	b	68c <__multc3+0x68c>
 828:	mov	x20, #0x0                   	// #0
 82c:	mov	x0, #0x0                   	// #0
 830:	tbz	x23, #63, 838 <__multc3+0x838>
 834:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 838:	mov	x23, x0
 83c:	b	6a4 <__multc3+0x6a4>
 840:	mov	x21, #0x0                   	// #0
 844:	mov	x0, #0x0                   	// #0
 848:	tbz	x26, #63, 850 <__multc3+0x850>
 84c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 850:	mov	x26, x0
 854:	b	6bc <__multc3+0x6bc>

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x0, [sp]
   c:	ldr	x2, [sp, #8]
  10:	mov	x3, x0
  14:	and	x0, x2, #0x7fffffffffffffff
  18:	mov	x4, #0xbc01000000000000    	// #-4899634919602388992
  1c:	add	x4, x0, x4
  20:	mov	x1, #0xc3ff000000000000    	// #-4323737117252386816
  24:	add	x1, x0, x1
  28:	cmp	x4, x1
  2c:	b.hi	5c <__trunctfdf2+0x5c>  // b.pmore
  30:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  34:	cmp	x0, x1
  38:	b.hi	4c <__trunctfdf2+0x4c>  // b.pmore
  3c:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  40:	cmp	x0, x1
  44:	b.ne	c4 <__trunctfdf2+0xc4>  // b.any
  48:	cbz	x3, c4 <__trunctfdf2+0xc4>
  4c:	extr	x0, x0, x3, #60
  50:	and	x0, x0, #0x7ffffffffffff
  54:	orr	x4, x0, #0x7ff8000000000000
  58:	b	80 <__trunctfdf2+0x80>
  5c:	extr	x0, x0, x3, #60
  60:	and	x3, x3, #0xfffffffffffffff
  64:	mov	x4, #0x0                   	// #0
  68:	mov	x1, #0x800000000000000     	// #576460752303423488
  6c:	cmp	x3, x1
  70:	b.ls	94 <__trunctfdf2+0x94>  // b.plast
  74:	mov	x4, #0x1                   	// #1
  78:	movk	x4, #0x4000, lsl #48
  7c:	add	x4, x0, x4
  80:	and	x2, x2, #0x8000000000000000
  84:	orr	x0, x4, x2
  88:	fmov	d0, x0
  8c:	add	sp, sp, #0x10
  90:	ret
  94:	mov	x1, #0x800000000000000     	// #576460752303423488
  98:	cmp	x3, x1
  9c:	b.eq	ac <__trunctfdf2+0xac>  // b.none
  a0:	mov	x4, #0x4000000000000000    	// #4611686018427387904
  a4:	add	x4, x0, x4
  a8:	b	80 <__trunctfdf2+0x80>
  ac:	cbnz	x4, a0 <__trunctfdf2+0xa0>
  b0:	mov	x1, #0x1                   	// #1
  b4:	movk	x1, #0x4000, lsl #48
  b8:	add	x0, x0, x1
  bc:	and	x4, x0, #0xfffffffffffffffe
  c0:	b	80 <__trunctfdf2+0x80>
  c4:	mov	x4, #0x7ff0000000000000    	// #9218868437227405312
  c8:	mov	x1, #0x43feffffffffffff    	// #4899634919602388991
  cc:	cmp	x0, x1
  d0:	b.hi	80 <__trunctfdf2+0x80>  // b.pmore
  d4:	lsr	x1, x0, #48
  d8:	mov	w4, #0x3c01                	// #15361
  dc:	sub	w5, w4, w1
  e0:	mov	x4, #0x0                   	// #0
  e4:	cmp	w5, #0x70
  e8:	b.gt	80 <__trunctfdf2+0x80>
  ec:	and	x4, x2, #0xffffffffffff
  f0:	orr	x4, x4, #0x1000000000000
  f4:	sub	w0, w1, #0x3, lsl #12
  f8:	sub	w0, w0, #0xb81
  fc:	sub	w1, w1, #0x3, lsl #12
 100:	sub	w1, w1, #0xbc1
 104:	lsl	x9, x3, x1
 108:	lsr	x8, x3, #1
 10c:	mov	w7, #0x3f                  	// #63
 110:	sub	w6, w7, w0
 114:	lsr	x8, x8, x6
 118:	lsl	x6, x4, x0
 11c:	orr	x6, x8, x6
 120:	lsl	x0, x3, x0
 124:	cmp	w1, #0x0
 128:	csel	x6, x9, x6, ge  // ge = tcont
 12c:	csel	x0, xzr, x0, ge  // ge = tcont
 130:	orr	x0, x0, x6
 134:	cmp	x0, #0x0
 138:	cset	x1, ne  // ne = any
 13c:	subs	w6, w5, #0x40
 140:	lsr	x8, x4, x6
 144:	lsl	x0, x4, #1
 148:	sub	w7, w7, w5
 14c:	lsl	x7, x0, x7
 150:	lsr	x0, x3, x5
 154:	orr	x0, x7, x0
 158:	lsr	x4, x4, x5
 15c:	csel	x0, x8, x0, pl  // pl = nfrst
 160:	csel	x4, xzr, x4, pl  // pl = nfrst
 164:	orr	x1, x1, x0
 168:	extr	x4, x4, x0, #60
 16c:	and	x0, x1, #0xfffffffffffffff
 170:	mov	x1, #0x800000000000000     	// #576460752303423488
 174:	cmp	x0, x1
 178:	b.ls	184 <__trunctfdf2+0x184>  // b.plast
 17c:	add	x4, x4, #0x1
 180:	b	80 <__trunctfdf2+0x80>
 184:	mov	x1, #0x800000000000000     	// #576460752303423488
 188:	cmp	x0, x1
 18c:	b.ne	80 <__trunctfdf2+0x80>  // b.any
 190:	add	x0, x4, #0x1
 194:	and	x0, x0, #0xfffffffffffffffe
 198:	mov	x1, #0x0                   	// #0
 19c:	cmp	x1, #0x0
 1a0:	csel	x4, x0, x4, eq  // eq = none
 1a4:	b	80 <__trunctfdf2+0x80>

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldr	x1, [sp]
   c:	ldr	x0, [sp, #8]
  10:	mov	x4, x1
  14:	and	x1, x0, #0x7fffffffffffffff
  18:	mov	x3, #0xbf81000000000000    	// #-4647433340469641216
  1c:	add	x3, x1, x3
  20:	mov	x2, #0xc07f000000000000    	// #-4575938696385134592
  24:	add	x2, x1, x2
  28:	cmp	x3, x2
  2c:	b.hi	58 <__trunctfsf2+0x58>  // b.pmore
  30:	mov	x2, #0x7fff000000000000    	// #9223090561878065152
  34:	cmp	x1, x2
  38:	b.hi	4c <__trunctfsf2+0x4c>  // b.pmore
  3c:	mov	x2, #0x7fff000000000000    	// #9223090561878065152
  40:	cmp	x1, x2
  44:	b.ne	c8 <__trunctfsf2+0xc8>  // b.any
  48:	cbz	x4, c8 <__trunctfsf2+0xc8>
  4c:	ubfx	x1, x1, #25, #22
  50:	orr	w3, w1, #0x7fc00000
  54:	b	80 <__trunctfsf2+0x80>
  58:	lsr	x1, x1, #25
  5c:	and	x2, x0, #0x1ffffff
  60:	mov	x3, #0x1000000             	// #16777216
  64:	cmp	x2, x3
  68:	b.hi	74 <__trunctfsf2+0x74>  // b.pmore
  6c:	b.ne	98 <__trunctfsf2+0x98>  // b.any
  70:	cbz	x4, 98 <__trunctfsf2+0x98>
  74:	mov	w3, #0x1                   	// #1
  78:	movk	w3, #0x4000, lsl #16
  7c:	add	w3, w3, w1
  80:	lsr	x0, x0, #32
  84:	and	x0, x0, #0x80000000
  88:	orr	w0, w3, w0
  8c:	fmov	s0, w0
  90:	add	sp, sp, #0x10
  94:	ret
  98:	cbz	x4, a8 <__trunctfsf2+0xa8>
  9c:	mov	w3, #0x40000000            	// #1073741824
  a0:	add	w3, w3, w1
  a4:	b	80 <__trunctfsf2+0x80>
  a8:	mov	x3, #0x1000000             	// #16777216
  ac:	cmp	x2, x3
  b0:	b.ne	9c <__trunctfsf2+0x9c>  // b.any
  b4:	mov	w3, #0x1                   	// #1
  b8:	movk	w3, #0x4000, lsl #16
  bc:	add	w1, w3, w1
  c0:	and	w3, w1, #0xfffffffe
  c4:	b	80 <__trunctfsf2+0x80>
  c8:	mov	w3, #0x7f800000            	// #2139095040
  cc:	mov	x2, #0x407effffffffffff    	// #4647433340469641215
  d0:	cmp	x1, x2
  d4:	b.hi	80 <__trunctfsf2+0x80>  // b.pmore
  d8:	lsr	x2, x1, #48
  dc:	mov	w5, #0x3f81                	// #16257
  e0:	sub	w5, w5, w2
  e4:	mov	w3, #0x0                   	// #0
  e8:	cmp	w5, #0x70
  ec:	b.gt	80 <__trunctfsf2+0x80>
  f0:	and	x3, x0, #0xffffffffffff
  f4:	orr	x3, x3, #0x1000000000000
  f8:	sub	w6, w2, #0x3, lsl #12
  fc:	sub	w6, w6, #0xf01
 100:	sub	w1, w2, #0x3, lsl #12
 104:	sub	w1, w1, #0xf41
 108:	lsl	x9, x4, x1
 10c:	lsr	x2, x4, #1
 110:	mov	w8, #0x3f                  	// #63
 114:	sub	w7, w8, w6
 118:	lsr	x2, x2, x7
 11c:	lsl	x7, x3, x6
 120:	orr	x7, x2, x7
 124:	lsl	x2, x4, x6
 128:	cmp	w1, #0x0
 12c:	csel	x7, x9, x7, ge  // ge = tcont
 130:	csel	x2, xzr, x2, ge  // ge = tcont
 134:	orr	x2, x2, x7
 138:	cmp	x2, #0x0
 13c:	cset	x7, ne  // ne = any
 140:	subs	w6, w5, #0x40
 144:	lsr	x9, x3, x6
 148:	lsl	x2, x3, #1
 14c:	sub	w8, w8, w5
 150:	lsl	x8, x2, x8
 154:	lsr	x1, x4, x5
 158:	orr	x1, x8, x1
 15c:	lsr	x2, x3, x5
 160:	csel	x1, x9, x1, pl  // pl = nfrst
 164:	csel	x2, xzr, x2, pl  // pl = nfrst
 168:	orr	x1, x7, x1
 16c:	lsr	x4, x2, #25
 170:	mov	w3, w4
 174:	and	x2, x2, #0x1ffffff
 178:	mov	x5, #0x1000000             	// #16777216
 17c:	cmp	x2, x5
 180:	b.hi	18c <__trunctfsf2+0x18c>  // b.pmore
 184:	b.ne	194 <__trunctfsf2+0x194>  // b.any
 188:	cbz	x1, 194 <__trunctfsf2+0x194>
 18c:	add	w3, w4, #0x1
 190:	b	80 <__trunctfsf2+0x80>
 194:	cbnz	x1, 80 <__trunctfsf2+0x80>
 198:	add	w1, w4, #0x1
 19c:	and	w1, w1, #0xfffffffe
 1a0:	mov	x4, #0x1000000             	// #16777216
 1a4:	cmp	x2, x4
 1a8:	csel	w3, w1, w3, eq  // eq = none
 1ac:	b	80 <__trunctfsf2+0x80>

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.eq	1c <__absvdi2+0x1c>  // b.none
   c:	asr	x1, x0, #63
  10:	eor	x0, x0, x1
  14:	sub	x0, x0, x1
  18:	ret
  1c:	stp	x29, x30, [sp, #-16]!
  20:	mov	x29, sp
  24:	adrp	x2, 0 <__absvdi2>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	adrp	x0, 0 <__absvdi2>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.eq	1c <__absvsi2+0x1c>  // b.none
   c:	asr	w1, w0, #31
  10:	eor	w0, w0, w1
  14:	sub	w0, w0, w1
  18:	ret
  1c:	stp	x29, x30, [sp, #-16]!
  20:	mov	x29, sp
  24:	adrp	x2, 0 <__absvsi2>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	adrp	x0, 0 <__absvsi2>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	cbz	x0, 1c <__absvti2+0x1c>
   4:	asr	x2, x1, #63
   8:	eor	x0, x0, x2
   c:	eor	x1, x1, x2
  10:	subs	x0, x0, x2
  14:	sbc	x1, x1, x2
  18:	ret
  1c:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  20:	cmp	x1, x2
  24:	b.ne	4 <__absvti2+0x4>  // b.any
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	adrp	x2, 0 <__absvti2>
  34:	add	x2, x2, #0x0
  38:	mov	w1, #0x18                  	// #24
  3c:	adrp	x0, 0 <__absvti2>
  40:	add	x0, x0, #0x0
  44:	bl	0 <__compilerrt_abort_impl>

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	fmov	d2, d0
  10:	fmov	x19, d0
  14:	fmov	x0, d1
  18:	and	x1, x19, #0x7fffffffffffffff
  1c:	and	x2, x0, #0x7fffffffffffffff
  20:	sub	x4, x1, #0x1
  24:	mov	x3, #0x7fefffffffffffff    	// #9218868437227405311
  28:	cmp	x4, x3
  2c:	b.cs	130 <__adddf3+0x130>  // b.hs, b.nlast
  30:	sub	x4, x2, #0x1
  34:	cmp	x4, x3
  38:	b.cs	25c <__adddf3+0x25c>  // b.hs, b.nlast
  3c:	cmp	x1, x2
  40:	csel	x3, x19, x0, cs  // cs = hs, nlast
  44:	csel	x0, x0, x19, cs  // cs = hs, nlast
  48:	mov	x19, x3
  4c:	ubfx	x2, x3, #52, #11
  50:	ubfx	x4, x0, #52, #11
  54:	and	x1, x3, #0xfffffffffffff
  58:	and	x3, x0, #0xfffffffffffff
  5c:	cbnz	w2, 74 <__adddf3+0x74>
  60:	clz	x2, x1
  64:	sub	w20, w2, #0xb
  68:	lsl	x1, x1, x20
  6c:	mov	w2, #0x1                   	// #1
  70:	sub	w2, w2, w20
  74:	cbnz	w4, 8c <__adddf3+0x8c>
  78:	clz	x4, x3
  7c:	sub	w5, w4, #0xb
  80:	lsl	x3, x3, x5
  84:	mov	w4, #0x1                   	// #1
  88:	sub	w4, w4, w5
  8c:	and	x20, x19, #0x8000000000000000
  90:	eor	x0, x19, x0
  94:	lsl	x1, x1, #3
  98:	orr	x1, x1, #0x80000000000000
  9c:	lsl	x3, x3, #3
  a0:	orr	x3, x3, #0x80000000000000
  a4:	subs	w4, w2, w4
  a8:	b.eq	cc <__adddf3+0xcc>  // b.none
  ac:	cmp	w4, #0x3f
  b0:	b.hi	24c <__adddf3+0x24c>  // b.pmore
  b4:	neg	w5, w4
  b8:	lsl	x5, x3, x5
  bc:	cmp	x5, #0x0
  c0:	cset	x5, ne  // ne = any
  c4:	lsr	x3, x3, x4
  c8:	orr	x3, x5, x3
  cc:	tbnz	x0, #63, 19c <__adddf3+0x19c>
  d0:	add	x0, x1, x3
  d4:	tbz	x0, #56, e4 <__adddf3+0xe4>
  d8:	and	x1, x0, #0x1
  dc:	orr	x0, x1, x0, lsr #1
  e0:	add	w2, w2, #0x1
  e4:	cmp	w2, #0x7fe
  e8:	b.gt	1cc <__adddf3+0x1cc>
  ec:	str	x21, [sp, #32]
  f0:	cmp	w2, #0x0
  f4:	b.le	1d8 <__adddf3+0x1d8>
  f8:	and	w21, w0, #0x7
  fc:	ubfx	x0, x0, #3, #52
 100:	orr	x0, x0, x2, lsl #52
 104:	orr	x20, x20, x0
 108:	bl	0 <__fe_getround>
 10c:	cmp	w0, #0x1
 110:	b.eq	224 <__adddf3+0x224>  // b.none
 114:	cmp	w0, #0x2
 118:	b.eq	238 <__adddf3+0x238>  // b.none
 11c:	cbz	w0, 200 <__adddf3+0x200>
 120:	cbnz	w21, 20c <__adddf3+0x20c>
 124:	fmov	d0, x20
 128:	ldr	x21, [sp, #32]
 12c:	b	144 <__adddf3+0x144>
 130:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
 134:	cmp	x1, x3
 138:	b.ls	150 <__adddf3+0x150>  // b.plast
 13c:	orr	x0, x19, #0x8000000000000
 140:	fmov	d0, x0
 144:	ldp	x19, x20, [sp, #16]
 148:	ldp	x29, x30, [sp], #48
 14c:	ret
 150:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
 154:	cmp	x2, x3
 158:	b.hi	184 <__adddf3+0x184>  // b.pmore
 15c:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
 160:	cmp	x1, x3
 164:	b.ne	268 <__adddf3+0x268>  // b.any
 168:	eor	x0, x19, x0
 16c:	mov	x1, #0x7ff8000000000000    	// #9221120237041090560
 170:	fmov	d0, x1
 174:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 178:	cmp	x0, x1
 17c:	fcsel	d0, d2, d0, ne  // ne = any
 180:	b	144 <__adddf3+0x144>
 184:	orr	x0, x0, #0x8000000000000
 188:	fmov	d0, x0
 18c:	b	144 <__adddf3+0x144>
 190:	fmov	d0, d2
 194:	cbnz	x2, 3c <__adddf3+0x3c>
 198:	b	144 <__adddf3+0x144>
 19c:	movi	d0, #0x0
 1a0:	subs	x0, x1, x3
 1a4:	b.eq	144 <__adddf3+0x144>  // b.none
 1a8:	mov	x1, #0x7fffffffffffff      	// #36028797018963967
 1ac:	cmp	x0, x1
 1b0:	b.hi	e4 <__adddf3+0xe4>  // b.pmore
 1b4:	str	x21, [sp, #32]
 1b8:	clz	x1, x0
 1bc:	sub	w1, w1, #0x8
 1c0:	lsl	x0, x0, x1
 1c4:	sub	w2, w2, w1
 1c8:	b	f0 <__adddf3+0xf0>
 1cc:	orr	x0, x20, #0x7ff0000000000000
 1d0:	fmov	d0, x0
 1d4:	b	144 <__adddf3+0x144>
 1d8:	mov	w1, #0x1                   	// #1
 1dc:	sub	w2, w1, w2
 1e0:	neg	w1, w2
 1e4:	lsl	x1, x0, x1
 1e8:	cmp	x1, #0x0
 1ec:	cset	x1, ne  // ne = any
 1f0:	lsr	x0, x0, x2
 1f4:	orr	x0, x1, x0
 1f8:	mov	w2, #0x0                   	// #0
 1fc:	b	f8 <__adddf3+0xf8>
 200:	cmp	w21, #0x4
 204:	b.le	214 <__adddf3+0x214>
 208:	add	x20, x20, #0x1
 20c:	bl	0 <__fe_raise_inexact>
 210:	b	124 <__adddf3+0x124>
 214:	b.ne	120 <__adddf3+0x120>  // b.any
 218:	add	x20, x20, #0x1
 21c:	and	x20, x20, #0xfffffffffffffffe
 220:	b	20c <__adddf3+0x20c>
 224:	cmp	x19, #0x0
 228:	ccmp	w21, #0x0, #0x4, lt  // lt = tstop
 22c:	b.eq	120 <__adddf3+0x120>  // b.none
 230:	add	x20, x20, #0x1
 234:	b	20c <__adddf3+0x20c>
 238:	cmp	x19, #0x0
 23c:	ccmp	w21, #0x0, #0x4, ge  // ge = tcont
 240:	b.eq	120 <__adddf3+0x120>  // b.none
 244:	add	x20, x20, #0x1
 248:	b	20c <__adddf3+0x20c>
 24c:	mov	x3, #0x1                   	// #1
 250:	tbz	x0, #63, d0 <__adddf3+0xd0>
 254:	sub	x0, x1, #0x1
 258:	b	1a8 <__adddf3+0x1a8>
 25c:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
 260:	cmp	x2, x3
 264:	b.hi	184 <__adddf3+0x184>  // b.pmore
 268:	fmov	d0, d1
 26c:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
 270:	cmp	x2, x3
 274:	b.eq	144 <__adddf3+0x144>  // b.none
 278:	cbnz	x1, 190 <__adddf3+0x190>
 27c:	and	x0, x19, x0
 280:	fmov	d0, x0
 284:	cmp	x2, #0x0
 288:	fcsel	d0, d0, d1, eq  // eq = none
 28c:	b	144 <__adddf3+0x144>

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	fmov	s2, s0
  10:	fmov	w19, s0
  14:	fmov	w0, s1
  18:	and	w1, w19, #0x7fffffff
  1c:	and	w2, w0, #0x7fffffff
  20:	sub	w4, w1, #0x1
  24:	mov	w3, #0x7f7fffff            	// #2139095039
  28:	cmp	w4, w3
  2c:	b.cs	130 <__addsf3+0x130>  // b.hs, b.nlast
  30:	sub	w4, w2, #0x1
  34:	cmp	w4, w3
  38:	b.cs	25c <__addsf3+0x25c>  // b.hs, b.nlast
  3c:	cmp	w1, w2
  40:	csel	w3, w19, w0, cs  // cs = hs, nlast
  44:	csel	w0, w0, w19, cs  // cs = hs, nlast
  48:	mov	w19, w3
  4c:	ubfx	x2, x19, #23, #8
  50:	ubfx	x4, x0, #23, #8
  54:	and	w1, w3, #0x7fffff
  58:	and	w3, w0, #0x7fffff
  5c:	cbnz	w2, 74 <__addsf3+0x74>
  60:	clz	w2, w1
  64:	sub	w20, w2, #0x8
  68:	lsl	w1, w1, w20
  6c:	mov	w2, #0x1                   	// #1
  70:	sub	w2, w2, w20
  74:	cbnz	w4, 8c <__addsf3+0x8c>
  78:	clz	w4, w3
  7c:	sub	w5, w4, #0x8
  80:	lsl	w3, w3, w5
  84:	mov	w4, #0x1                   	// #1
  88:	sub	w4, w4, w5
  8c:	and	w20, w19, #0x80000000
  90:	eor	w0, w19, w0
  94:	lsl	w1, w1, #3
  98:	orr	w1, w1, #0x4000000
  9c:	lsl	w3, w3, #3
  a0:	orr	w3, w3, #0x4000000
  a4:	subs	w4, w2, w4
  a8:	b.eq	cc <__addsf3+0xcc>  // b.none
  ac:	cmp	w4, #0x1f
  b0:	b.hi	24c <__addsf3+0x24c>  // b.pmore
  b4:	neg	w5, w4
  b8:	lsl	w5, w3, w5
  bc:	cmp	w5, #0x0
  c0:	cset	w5, ne  // ne = any
  c4:	lsr	w3, w3, w4
  c8:	orr	w3, w5, w3
  cc:	tbnz	w0, #31, 19c <__addsf3+0x19c>
  d0:	add	w0, w1, w3
  d4:	tbz	w0, #27, e4 <__addsf3+0xe4>
  d8:	and	w1, w0, #0x1
  dc:	orr	w0, w1, w0, lsr #1
  e0:	add	w2, w2, #0x1
  e4:	cmp	w2, #0xfe
  e8:	b.gt	1cc <__addsf3+0x1cc>
  ec:	str	x21, [sp, #32]
  f0:	cmp	w2, #0x0
  f4:	b.le	1d8 <__addsf3+0x1d8>
  f8:	and	w21, w0, #0x7
  fc:	ubfx	x0, x0, #3, #23
 100:	orr	w0, w0, w2, lsl #23
 104:	orr	w20, w20, w0
 108:	bl	0 <__fe_getround>
 10c:	cmp	w0, #0x1
 110:	b.eq	224 <__addsf3+0x224>  // b.none
 114:	cmp	w0, #0x2
 118:	b.eq	238 <__addsf3+0x238>  // b.none
 11c:	cbz	w0, 200 <__addsf3+0x200>
 120:	cbnz	w21, 20c <__addsf3+0x20c>
 124:	fmov	s0, w20
 128:	ldr	x21, [sp, #32]
 12c:	b	144 <__addsf3+0x144>
 130:	mov	w3, #0x7f800000            	// #2139095040
 134:	cmp	w1, w3
 138:	b.ls	150 <__addsf3+0x150>  // b.plast
 13c:	orr	w0, w19, #0x400000
 140:	fmov	s0, w0
 144:	ldp	x19, x20, [sp, #16]
 148:	ldp	x29, x30, [sp], #48
 14c:	ret
 150:	mov	w3, #0x7f800000            	// #2139095040
 154:	cmp	w2, w3
 158:	b.hi	184 <__addsf3+0x184>  // b.pmore
 15c:	mov	w3, #0x7f800000            	// #2139095040
 160:	cmp	w1, w3
 164:	b.ne	268 <__addsf3+0x268>  // b.any
 168:	eor	w0, w19, w0
 16c:	mov	w1, #0x7fc00000            	// #2143289344
 170:	fmov	s0, w1
 174:	mov	w1, #0x80000000            	// #-2147483648
 178:	cmp	w0, w1
 17c:	fcsel	s0, s2, s0, ne  // ne = any
 180:	b	144 <__addsf3+0x144>
 184:	orr	w0, w0, #0x400000
 188:	fmov	s0, w0
 18c:	b	144 <__addsf3+0x144>
 190:	fmov	s0, s2
 194:	cbnz	w2, 3c <__addsf3+0x3c>
 198:	b	144 <__addsf3+0x144>
 19c:	movi	v0.2s, #0x0
 1a0:	subs	w0, w1, w3
 1a4:	b.eq	144 <__addsf3+0x144>  // b.none
 1a8:	mov	w1, #0x3ffffff             	// #67108863
 1ac:	cmp	w0, w1
 1b0:	b.hi	e4 <__addsf3+0xe4>  // b.pmore
 1b4:	str	x21, [sp, #32]
 1b8:	clz	w1, w0
 1bc:	sub	w1, w1, #0x5
 1c0:	lsl	w0, w0, w1
 1c4:	sub	w2, w2, w1
 1c8:	b	f0 <__addsf3+0xf0>
 1cc:	orr	w0, w20, #0x7f800000
 1d0:	fmov	s0, w0
 1d4:	b	144 <__addsf3+0x144>
 1d8:	mov	w1, #0x1                   	// #1
 1dc:	sub	w2, w1, w2
 1e0:	neg	w1, w2
 1e4:	lsl	w1, w0, w1
 1e8:	cmp	w1, #0x0
 1ec:	cset	w1, ne  // ne = any
 1f0:	lsr	w0, w0, w2
 1f4:	orr	w0, w1, w0
 1f8:	mov	w2, #0x0                   	// #0
 1fc:	b	f8 <__addsf3+0xf8>
 200:	cmp	w21, #0x4
 204:	b.le	214 <__addsf3+0x214>
 208:	add	w20, w20, #0x1
 20c:	bl	0 <__fe_raise_inexact>
 210:	b	124 <__addsf3+0x124>
 214:	b.ne	120 <__addsf3+0x120>  // b.any
 218:	add	w20, w20, #0x1
 21c:	and	w20, w20, #0xfffffffe
 220:	b	20c <__addsf3+0x20c>
 224:	cmp	w19, #0x0
 228:	ccmp	w21, #0x0, #0x4, lt  // lt = tstop
 22c:	b.eq	120 <__addsf3+0x120>  // b.none
 230:	add	w20, w20, #0x1
 234:	b	20c <__addsf3+0x20c>
 238:	cmp	w19, #0x0
 23c:	ccmp	w21, #0x0, #0x4, ge  // ge = tcont
 240:	b.eq	120 <__addsf3+0x120>  // b.none
 244:	add	w20, w20, #0x1
 248:	b	20c <__addsf3+0x20c>
 24c:	mov	w3, #0x1                   	// #1
 250:	tbz	w0, #31, d0 <__addsf3+0xd0>
 254:	sub	w0, w1, #0x1
 258:	b	1a8 <__addsf3+0x1a8>
 25c:	mov	w3, #0x7f800000            	// #2139095040
 260:	cmp	w2, w3
 264:	b.hi	184 <__addsf3+0x184>  // b.pmore
 268:	fmov	s0, s1
 26c:	mov	w3, #0x7f800000            	// #2139095040
 270:	cmp	w2, w3
 274:	b.eq	144 <__addsf3+0x144>  // b.none
 278:	cbnz	w1, 190 <__addsf3+0x190>
 27c:	and	w0, w19, w0
 280:	fmov	s0, w0
 284:	cmp	w2, #0x0
 288:	fcsel	s0, s0, s1, eq  // eq = none
 28c:	b	144 <__addsf3+0x144>

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	q0, [sp, #48]
  10:	ldr	x2, [sp, #48]
  14:	ldr	x1, [sp, #56]
  18:	str	q1, [sp, #48]
  1c:	ldr	x12, [sp, #48]
  20:	ldr	x13, [sp, #56]
  24:	mov	x0, x2
  28:	mov	x19, x1
  2c:	mov	x10, x12
  30:	mov	x7, x13
  34:	mov	x3, #0xffffffffffffffff    	// #-1
  38:	mov	x11, x2
  3c:	and	x6, x1, #0x7fffffffffffffff
  40:	mov	x8, x12
  44:	and	x9, x13, #0x7fffffffffffffff
  48:	adds	x3, x2, x3
  4c:	sbc	x4, x6, xzr
  50:	mov	x5, #0x7ffeffffffffffff    	// #9223090561878065151
  54:	cmp	x4, x5
  58:	b.hi	294 <__addtf3+0x294>  // b.pmore
  5c:	b.ne	68 <__addtf3+0x68>  // b.any
  60:	cmn	x3, #0x2
  64:	b.hi	294 <__addtf3+0x294>  // b.pmore
  68:	mov	x3, #0xffffffffffffffff    	// #-1
  6c:	adds	x3, x8, x3
  70:	sbc	x4, x9, xzr
  74:	mov	x5, #0x7ffeffffffffffff    	// #9223090561878065151
  78:	cmp	x4, x5
  7c:	b.hi	4fc <__addtf3+0x4fc>  // b.pmore
  80:	b.ne	8c <__addtf3+0x8c>  // b.any
  84:	cmn	x3, #0x2
  88:	b.hi	4fc <__addtf3+0x4fc>  // b.pmore
  8c:	cmp	x9, x6
  90:	b.hi	a0 <__addtf3+0xa0>  // b.pmore
  94:	b.ne	b8 <__addtf3+0xb8>  // b.any
  98:	cmp	x8, x11
  9c:	b.ls	b8 <__addtf3+0xb8>  // b.plast
  a0:	mov	x2, x0
  a4:	mov	x1, x19
  a8:	mov	x0, x10
  ac:	mov	x19, x7
  b0:	mov	x10, x2
  b4:	mov	x7, x1
  b8:	ubfx	x6, x19, #48, #15
  bc:	ubfx	x4, x7, #48, #15
  c0:	and	x3, x19, #0xffffffffffff
  c4:	and	x1, x7, #0xffffffffffff
  c8:	cbnz	w6, 11c <__addtf3+0x11c>
  cc:	cmp	x3, #0x0
  d0:	csel	x2, x3, x0, ne  // ne = any
  d4:	mov	x5, #0x40                  	// #64
  d8:	csel	x5, x5, xzr, eq  // eq = none
  dc:	clz	x2, x2
  e0:	add	w5, w5, w2
  e4:	sub	w6, w5, #0xf
  e8:	subs	w5, w5, #0x4f
  ec:	lsl	x9, x0, x5
  f0:	lsr	x2, x0, #1
  f4:	mov	w8, #0x3f                  	// #63
  f8:	sub	w8, w8, w6
  fc:	lsr	x2, x2, x8
 100:	lsl	x3, x3, x6
 104:	orr	x3, x2, x3
 108:	lsl	x0, x0, x6
 10c:	csel	x3, x9, x3, pl  // pl = nfrst
 110:	csel	x0, xzr, x0, pl  // pl = nfrst
 114:	mov	w2, #0x1                   	// #1
 118:	sub	w6, w2, w6
 11c:	cbnz	w4, 170 <__addtf3+0x170>
 120:	cmp	x1, #0x0
 124:	csel	x2, x1, x10, ne  // ne = any
 128:	mov	x5, #0x40                  	// #64
 12c:	csel	x5, x5, xzr, eq  // eq = none
 130:	clz	x2, x2
 134:	add	w5, w5, w2
 138:	sub	w4, w5, #0xf
 13c:	subs	w5, w5, #0x4f
 140:	lsl	x9, x10, x5
 144:	lsr	x2, x10, #1
 148:	mov	w8, #0x3f                  	// #63
 14c:	sub	w8, w8, w4
 150:	lsr	x2, x2, x8
 154:	lsl	x1, x1, x4
 158:	orr	x1, x2, x1
 15c:	lsl	x10, x10, x4
 160:	csel	x1, x9, x1, pl  // pl = nfrst
 164:	csel	x10, xzr, x10, pl  // pl = nfrst
 168:	mov	w2, #0x1                   	// #1
 16c:	sub	w4, w2, w4
 170:	and	x8, x19, #0x8000000000000000
 174:	eor	x9, x19, x7
 178:	extr	x3, x3, x0, #61
 17c:	lsl	x0, x0, #3
 180:	orr	x3, x3, #0x8000000000000
 184:	extr	x1, x1, x10, #61
 188:	lsl	x2, x10, #3
 18c:	mov	x5, x2
 190:	orr	x1, x1, #0x8000000000000
 194:	subs	w4, w6, w4
 198:	b.eq	210 <__addtf3+0x210>  // b.none
 19c:	cmp	w4, #0x7f
 1a0:	b.hi	4ec <__addtf3+0x4ec>  // b.pmore
 1a4:	mov	w7, #0x80                  	// #128
 1a8:	sub	w7, w7, w4
 1ac:	subs	w11, w7, #0x40
 1b0:	lsl	x13, x2, x11
 1b4:	lsr	x12, x2, #1
 1b8:	mov	w10, #0x3f                  	// #63
 1bc:	sub	w5, w10, w7
 1c0:	lsr	x12, x12, x5
 1c4:	lsl	x5, x1, x7
 1c8:	orr	x5, x12, x5
 1cc:	lsl	x7, x2, x7
 1d0:	csel	x5, x13, x5, pl  // pl = nfrst
 1d4:	csel	x7, xzr, x7, pl  // pl = nfrst
 1d8:	orr	x7, x7, x5
 1dc:	cmp	x7, #0x0
 1e0:	cset	x5, ne  // ne = any
 1e4:	subs	w7, w4, #0x40
 1e8:	lsr	x12, x1, x7
 1ec:	lsl	x11, x1, #1
 1f0:	sub	w10, w10, w4
 1f4:	lsl	x10, x11, x10
 1f8:	lsr	x2, x2, x4
 1fc:	orr	x2, x10, x2
 200:	lsr	x1, x1, x4
 204:	csel	x2, x12, x2, pl  // pl = nfrst
 208:	orr	x5, x5, x2
 20c:	csel	x1, xzr, x1, pl  // pl = nfrst
 210:	tbnz	x9, #63, 344 <__addtf3+0x344>
 214:	adds	x2, x0, x5
 218:	adc	x1, x3, x1
 21c:	mov	x4, x2
 220:	mov	x3, x1
 224:	tbz	x1, #52, 23c <__addtf3+0x23c>
 228:	and	x4, x2, #0x1
 22c:	extr	x2, x1, x2, #1
 230:	orr	x4, x4, x2
 234:	lsr	x3, x1, #1
 238:	add	w6, w6, #0x1
 23c:	mov	w0, #0x7ffe                	// #32766
 240:	cmp	w6, w0
 244:	b.gt	3c0 <__addtf3+0x3c0>
 248:	stp	x21, x22, [sp, #32]
 24c:	cmp	w6, #0x0
 250:	b.le	3d0 <__addtf3+0x3d0>
 254:	and	w22, w4, #0x7
 258:	ubfx	x0, x3, #3, #48
 25c:	orr	x6, x0, x6, lsl #48
 260:	extr	x20, x3, x4, #3
 264:	orr	x21, x8, x6
 268:	bl	0 <__fe_getround>
 26c:	cmp	w0, #0x1
 270:	b.eq	484 <__addtf3+0x484>  // b.none
 274:	cmp	w0, #0x2
 278:	b.eq	4ac <__addtf3+0x4ac>  // b.none
 27c:	cbz	w0, 44c <__addtf3+0x44c>
 280:	cbnz	w22, 464 <__addtf3+0x464>
 284:	mov	x12, x20
 288:	mov	x1, x21
 28c:	ldp	x21, x22, [sp, #32]
 290:	b	2ac <__addtf3+0x2ac>
 294:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 298:	cmp	x6, x3
 29c:	b.ls	2c4 <__addtf3+0x2c4>  // b.plast
 2a0:	orr	x3, x19, #0x800000000000
 2a4:	mov	x12, x0
 2a8:	mov	x1, x3
 2ac:	str	x12, [sp, #48]
 2b0:	str	x1, [sp, #56]
 2b4:	ldr	q0, [sp, #48]
 2b8:	ldp	x19, x20, [sp, #16]
 2bc:	ldp	x29, x30, [sp], #64
 2c0:	ret
 2c4:	b.ne	2cc <__addtf3+0x2cc>  // b.any
 2c8:	cbnz	x11, 2a0 <__addtf3+0x2a0>
 2cc:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 2d0:	cmp	x9, x3
 2d4:	b.hi	53c <__addtf3+0x53c>  // b.pmore
 2d8:	b.eq	300 <__addtf3+0x300>  // b.none
 2dc:	cbnz	x11, 50c <__addtf3+0x50c>
 2e0:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 2e4:	cmp	x6, x3
 2e8:	b.ne	50c <__addtf3+0x50c>  // b.any
 2ec:	eor	x19, x19, x7
 2f0:	cmp	x0, x10
 2f4:	b.eq	308 <__addtf3+0x308>  // b.none
 2f8:	mov	x12, x2
 2fc:	b	2ac <__addtf3+0x2ac>
 300:	cbz	x8, 2dc <__addtf3+0x2dc>
 304:	b	53c <__addtf3+0x53c>
 308:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 30c:	cmp	x19, x0
 310:	b.ne	2f8 <__addtf3+0x2f8>  // b.any
 314:	mov	x12, #0x0                   	// #0
 318:	mov	x1, #0x7fff800000000000    	// #9223231299366420480
 31c:	b	2ac <__addtf3+0x2ac>
 320:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 324:	cmp	x9, x3
 328:	b.ne	510 <__addtf3+0x510>  // b.any
 32c:	mov	x1, x13
 330:	b	2ac <__addtf3+0x2ac>
 334:	orr	x3, x8, x9
 338:	mov	x12, x2
 33c:	cbnz	x3, 8c <__addtf3+0x8c>
 340:	b	2ac <__addtf3+0x2ac>
 344:	subs	x2, x0, x5
 348:	sbc	x1, x3, x1
 34c:	mov	x4, x2
 350:	mov	x3, x1
 354:	orr	x5, x2, x1
 358:	mov	x12, #0x0                   	// #0
 35c:	mov	x1, #0x0                   	// #0
 360:	cbz	x5, 2ac <__addtf3+0x2ac>
 364:	mov	x0, #0x7ffffffffffff       	// #2251799813685247
 368:	cmp	x3, x0
 36c:	b.hi	23c <__addtf3+0x23c>  // b.pmore
 370:	cmp	x3, #0x0
 374:	csel	x1, x3, x4, ne  // ne = any
 378:	mov	x0, #0x40                  	// #64
 37c:	csel	x0, x0, xzr, eq  // eq = none
 380:	clz	x1, x1
 384:	add	w0, w0, w1
 388:	sub	w1, w0, #0xc
 38c:	subs	w0, w0, #0x4c
 390:	lsl	x7, x4, x0
 394:	lsr	x2, x4, #1
 398:	mov	w5, #0x3f                  	// #63
 39c:	sub	w5, w5, w1
 3a0:	lsr	x2, x2, x5
 3a4:	lsl	x3, x3, x1
 3a8:	orr	x3, x2, x3
 3ac:	lsl	x2, x4, x1
 3b0:	csel	x3, x7, x3, pl  // pl = nfrst
 3b4:	csel	x4, xzr, x2, pl  // pl = nfrst
 3b8:	sub	w6, w6, w1
 3bc:	b	23c <__addtf3+0x23c>
 3c0:	mov	x0, #0x0                   	// #0
 3c4:	orr	x1, x8, #0x7fff000000000000
 3c8:	mov	x12, x0
 3cc:	b	2ac <__addtf3+0x2ac>
 3d0:	mov	w0, #0x1                   	// #1
 3d4:	sub	w6, w0, w6
 3d8:	mov	w0, #0x80                  	// #128
 3dc:	sub	w0, w0, w6
 3e0:	subs	w5, w0, #0x40
 3e4:	lsl	x9, x4, x5
 3e8:	lsr	x7, x4, #1
 3ec:	mov	w2, #0x3f                  	// #63
 3f0:	sub	w1, w2, w0
 3f4:	lsr	x7, x7, x1
 3f8:	lsl	x1, x3, x0
 3fc:	orr	x1, x7, x1
 400:	lsl	x0, x4, x0
 404:	csel	x1, x9, x1, pl  // pl = nfrst
 408:	csel	x0, xzr, x0, pl  // pl = nfrst
 40c:	orr	x0, x0, x1
 410:	cmp	x0, #0x0
 414:	cset	x5, ne  // ne = any
 418:	subs	w0, w6, #0x40
 41c:	lsr	x7, x3, x0
 420:	lsl	x1, x3, #1
 424:	sub	w2, w2, w6
 428:	lsl	x1, x1, x2
 42c:	lsr	x2, x4, x6
 430:	orr	x2, x1, x2
 434:	lsr	x3, x3, x6
 438:	csel	x2, x7, x2, pl  // pl = nfrst
 43c:	orr	x4, x5, x2
 440:	csel	x3, xzr, x3, pl  // pl = nfrst
 444:	mov	w6, #0x0                   	// #0
 448:	b	254 <__addtf3+0x254>
 44c:	cmp	w22, #0x4
 450:	b.le	46c <__addtf3+0x46c>
 454:	adds	x2, x20, #0x1
 458:	cinc	x0, x21, cs  // cs = hs, nlast
 45c:	mov	x20, x2
 460:	mov	x21, x0
 464:	bl	0 <__fe_raise_inexact>
 468:	b	284 <__addtf3+0x284>
 46c:	b.ne	280 <__addtf3+0x280>  // b.any
 470:	adds	x1, x20, #0x1
 474:	cinc	x0, x21, cs  // cs = hs, nlast
 478:	and	x20, x1, #0xfffffffffffffffe
 47c:	mov	x21, x0
 480:	b	464 <__addtf3+0x464>
 484:	lsr	x19, x19, #63
 488:	cmp	w22, #0x0
 48c:	cset	w0, ne  // ne = any
 490:	tst	w0, w19
 494:	b.eq	280 <__addtf3+0x280>  // b.none
 498:	adds	x2, x20, #0x1
 49c:	cinc	x0, x21, cs  // cs = hs, nlast
 4a0:	mov	x20, x2
 4a4:	mov	x21, x0
 4a8:	b	464 <__addtf3+0x464>
 4ac:	mvn	x19, x19
 4b0:	lsr	x19, x19, #63
 4b4:	cmp	w22, #0x0
 4b8:	cset	w0, ne  // ne = any
 4bc:	tst	w0, w19
 4c0:	b.eq	280 <__addtf3+0x280>  // b.none
 4c4:	adds	x2, x20, #0x1
 4c8:	cinc	x0, x21, cs  // cs = hs, nlast
 4cc:	mov	x20, x2
 4d0:	mov	x21, x0
 4d4:	b	464 <__addtf3+0x464>
 4d8:	mov	x1, #0xffffffffffffffff    	// #-1
 4dc:	adds	x2, x0, x1
 4e0:	mov	x4, x2
 4e4:	sbc	x3, x3, xzr
 4e8:	b	364 <__addtf3+0x364>
 4ec:	tbnz	x9, #63, 4d8 <__addtf3+0x4d8>
 4f0:	mov	x5, #0x1                   	// #1
 4f4:	mov	x1, #0x0                   	// #0
 4f8:	b	214 <__addtf3+0x214>
 4fc:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 500:	cmp	x9, x3
 504:	b.hi	53c <__addtf3+0x53c>  // b.pmore
 508:	b.eq	538 <__addtf3+0x538>  // b.none
 50c:	cbz	x8, 320 <__addtf3+0x320>
 510:	orr	x3, x11, x6
 514:	cbnz	x3, 334 <__addtf3+0x334>
 518:	orr	x8, x8, x9
 51c:	mov	x1, x13
 520:	cbnz	x8, 2ac <__addtf3+0x2ac>
 524:	and	x2, x0, x10
 528:	and	x3, x19, x7
 52c:	mov	x12, x2
 530:	mov	x1, x3
 534:	b	2ac <__addtf3+0x2ac>
 538:	cbz	x8, 50c <__addtf3+0x50c>
 53c:	orr	x1, x7, #0x800000000000
 540:	mov	x12, x10
 544:	b	2ac <__addtf3+0x2ac>

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x2, x0
   c:	add	x0, x0, x1
  10:	tbnz	x1, #63, 3c <__addvdi3+0x3c>
  14:	cmp	x2, x0
  18:	b.gt	24 <__addvdi3+0x24>
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	adrp	x2, 0 <__addvdi3>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	adrp	x0, 0 <__addvdi3>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>
  3c:	cmp	x2, x0
  40:	b.gt	1c <__addvdi3+0x1c>
  44:	adrp	x2, 0 <__addvdi3>
  48:	add	x2, x2, #0x0
  4c:	mov	w1, #0x1a                  	// #26
  50:	adrp	x0, 0 <__addvdi3>
  54:	add	x0, x0, #0x0
  58:	bl	0 <__compilerrt_abort_impl>

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w2, w0
   c:	add	w0, w0, w1
  10:	tbnz	w1, #31, 3c <__addvsi3+0x3c>
  14:	cmp	w2, w0
  18:	b.gt	24 <__addvsi3+0x24>
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	adrp	x2, 0 <__addvsi3>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	adrp	x0, 0 <__addvsi3>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>
  3c:	cmp	w2, w0
  40:	b.gt	1c <__addvsi3+0x1c>
  44:	adrp	x2, 0 <__addvsi3>
  48:	add	x2, x2, #0x0
  4c:	mov	w1, #0x1a                  	// #26
  50:	adrp	x0, 0 <__addvsi3>
  54:	add	x0, x0, #0x0
  58:	bl	0 <__compilerrt_abort_impl>

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x4, x0
   c:	mov	x5, x1
  10:	adds	x0, x0, x2
  14:	adc	x1, x1, x3
  18:	tbnz	x3, #63, 50 <__addvti3+0x50>
  1c:	cmp	x5, x1
  20:	b.gt	38 <__addvti3+0x38>
  24:	b.eq	30 <__addvti3+0x30>  // b.none
  28:	ldp	x29, x30, [sp], #16
  2c:	ret
  30:	cmp	x4, x0
  34:	b.ls	28 <__addvti3+0x28>  // b.plast
  38:	adrp	x2, 0 <__addvti3>
  3c:	add	x2, x2, #0x0
  40:	mov	w1, #0x19                  	// #25
  44:	adrp	x0, 0 <__addvti3>
  48:	add	x0, x0, #0x0
  4c:	bl	0 <__compilerrt_abort_impl>
  50:	cmp	x5, x1
  54:	b.gt	28 <__addvti3+0x28>
  58:	b.ne	64 <__addvti3+0x64>  // b.any
  5c:	cmp	x4, x0
  60:	b.hi	28 <__addvti3+0x28>  // b.pmore
  64:	adrp	x2, 0 <__addvti3>
  68:	add	x2, x2, #0x0
  6c:	mov	w1, #0x1c                  	// #28
  70:	adrp	x0, 0 <__addvti3>
  74:	add	x0, x0, #0x0
  78:	bl	0 <__compilerrt_abort_impl>

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 1c <__ashldi3+0x1c>
   8:	mov	x0, #0x0                   	// #0
   c:	sub	w1, w1, #0x20
  10:	lsl	w2, w2, w1
  14:	bfi	x0, x2, #32, #32
  18:	ret
  1c:	cbz	w1, 18 <__ashldi3+0x18>
  20:	lsl	w3, w0, w1
  24:	mov	x0, #0x0                   	// #0
  28:	bfxil	x0, x3, #0, #32
  2c:	neg	w4, w1
  30:	lsr	w4, w2, w4
  34:	asr	x3, x2, #32
  38:	lsl	w2, w3, w1
  3c:	orr	w2, w4, w2
  40:	bfi	x0, x2, #32, #32
  44:	b	18 <__ashldi3+0x18>

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbz	w2, #6, 18 <__ashlti3+0x18>
   4:	mov	x4, #0x0                   	// #0
   8:	sub	w1, w2, #0x40
   c:	lsl	x1, x0, x1
  10:	mov	x0, x4
  14:	ret
  18:	cbz	w2, 14 <__ashlti3+0x14>
  1c:	lsl	x4, x0, x2
  20:	neg	w3, w2
  24:	lsr	x3, x0, x3
  28:	lsl	x1, x1, x2
  2c:	orr	x1, x3, x1
  30:	b	10 <__ashlti3+0x10>

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 28 <__ashrdi3+0x28>
   8:	asr	x3, x0, #32
   c:	asr	x2, x0, #63
  10:	mov	x0, #0x0                   	// #0
  14:	bfi	x0, x2, #32, #32
  18:	sub	w1, w1, #0x20
  1c:	asr	w1, w3, w1
  20:	bfxil	x0, x1, #0, #32
  24:	ret
  28:	cbz	w1, 24 <__ashrdi3+0x24>
  2c:	asr	x3, x0, #32
  30:	asr	w4, w3, w1
  34:	mov	x0, #0x0                   	// #0
  38:	bfi	x0, x4, #32, #32
  3c:	neg	w4, w1
  40:	lsl	w3, w3, w4
  44:	lsr	w1, w2, w1
  48:	orr	w1, w3, w1
  4c:	bfxil	x0, x1, #0, #32
  50:	b	24 <__ashrdi3+0x24>

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	tbz	w2, #6, 18 <__ashrti3+0x18>
   4:	asr	x4, x1, #63
   8:	sub	w2, w2, #0x40
   c:	asr	x0, x1, x2
  10:	mov	x1, x4
  14:	ret
  18:	cbz	w2, 14 <__ashrti3+0x14>
  1c:	asr	x4, x1, x2
  20:	neg	w3, w2
  24:	lsl	x3, x1, x3
  28:	lsr	x0, x0, x2
  2c:	orr	x0, x3, x0
  30:	b	10 <__ashrti3+0x10>

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	lsr	x2, x0, #40
   4:	and	x2, x2, #0xff00
   8:	lsr	x1, x0, #24
   c:	and	x1, x1, #0xff0000
  10:	orr	x2, x2, x1
  14:	lsl	x1, x0, #56
  18:	orr	x1, x1, x0, lsr #56
  1c:	orr	x2, x2, x1
  20:	lsr	x1, x0, #8
  24:	and	x1, x1, #0xff000000
  28:	lsl	x3, x0, #8
  2c:	and	x3, x3, #0xff00000000
  30:	orr	x1, x1, x3
  34:	orr	x2, x2, x1
  38:	lsl	x3, x0, #24
  3c:	and	x3, x3, #0xff0000000000
  40:	lsl	x1, x0, #40
  44:	and	x1, x1, #0xff000000000000
  48:	orr	x0, x3, x1
  4c:	orr	x0, x2, x0
  50:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	lsr	w1, w0, #8
   4:	and	w1, w1, #0xff00
   8:	lsl	w2, w0, #8
   c:	and	w2, w2, #0xff0000
  10:	orr	w1, w1, w2
  14:	lsl	w2, w0, #24
  18:	orr	w0, w2, w0, lsr #24
  1c:	orr	w0, w1, w0
  20:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	asr	x2, x0, #32
   4:	cmp	w2, #0x0
   8:	csetm	w1, eq  // eq = none
   c:	csel	w0, w0, w2, eq  // eq = none
  10:	clz	w0, w0
  14:	and	w1, w1, #0x20
  18:	add	w0, w1, w0
  1c:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	tst	w0, #0xffff0000
   4:	cset	w4, eq  // eq = none
   8:	lsl	w4, w4, #4
   c:	mov	w1, #0x10                  	// #16
  10:	sub	w1, w1, w4
  14:	lsr	w1, w0, w1
  18:	tst	w1, #0xff00
  1c:	cset	w2, eq  // eq = none
  20:	lsl	w2, w2, #3
  24:	mov	w0, #0x8                   	// #8
  28:	sub	w0, w0, w2
  2c:	lsr	w1, w1, w0
  30:	add	w4, w2, w4
  34:	tst	w1, #0xf0
  38:	cset	w2, eq  // eq = none
  3c:	lsl	w5, w2, #2
  40:	mov	w0, #0x4                   	// #4
  44:	sub	w0, w0, w5
  48:	lsr	w1, w1, w0
  4c:	tst	w1, #0xc
  50:	cset	w3, eq  // eq = none
  54:	lsl	w2, w3, #1
  58:	mov	w3, #0x2                   	// #2
  5c:	sub	w0, w3, w2
  60:	lsr	w1, w1, w0
  64:	eor	x0, x1, #0x2
  68:	sbfx	w0, w0, #1, #1
  6c:	sub	w1, w3, w1
  70:	and	w0, w0, w1
  74:	add	w2, w2, w5
  78:	add	w2, w2, w4
  7c:	add	w0, w0, w2
  80:	ret

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csetm	w2, eq  // eq = none
   8:	csel	x1, x1, x0, ne  // ne = any
   c:	clz	x1, x1
  10:	and	w0, w2, #0x40
  14:	add	w0, w0, w1
  18:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	mov	x2, x0
   4:	asr	x4, x0, #32
   8:	asr	x3, x1, #32
   c:	mov	w0, #0x0                   	// #0
  10:	cmp	w4, w3
  14:	b.lt	34 <__cmpdi2+0x34>  // b.tstop
  18:	mov	w0, #0x2                   	// #2
  1c:	b.gt	34 <__cmpdi2+0x34>
  20:	mov	w0, #0x0                   	// #0
  24:	cmp	w2, w1
  28:	b.cc	34 <__cmpdi2+0x34>  // b.lo, b.ul, b.last
  2c:	cset	w0, hi  // hi = pmore
  30:	add	w0, w0, #0x1
  34:	ret

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	mov	x4, x0
   4:	mov	w0, #0x0                   	// #0
   8:	cmp	x1, x3
   c:	b.lt	2c <__cmpti2+0x2c>  // b.tstop
  10:	mov	w0, #0x2                   	// #2
  14:	b.gt	2c <__cmpti2+0x2c>
  18:	mov	w0, #0x0                   	// #0
  1c:	cmp	x4, x2
  20:	b.cc	2c <__cmpti2+0x2c>  // b.lo, b.ul, b.last
  24:	cset	w0, hi  // hi = pmore
  28:	add	w0, w0, #0x1
  2c:	ret

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	fmov	x2, d0
   4:	fmov	x1, d1
   8:	and	x0, x2, #0x7fffffffffffffff
   c:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
  10:	and	x4, x1, #0x7fffffffffffffff
  14:	cmp	x0, x3
  18:	mov	w0, #0x1                   	// #1
  1c:	ccmp	x4, x3, #0x2, ls  // ls = plast
  20:	b.hi	4c <__cmpdf2+0x4c>  // b.pmore
  24:	orr	x3, x2, x1
  28:	mov	w0, #0x0                   	// #0
  2c:	tst	x3, #0x7fffffffffffffff
  30:	b.eq	4c <__cmpdf2+0x4c>  // b.none
  34:	tst	x2, x1
  38:	b.mi	50 <__cmpdf2+0x50>  // b.first
  3c:	mov	w0, #0xffffffff            	// #-1
  40:	cmp	x2, x1
  44:	b.lt	4c <__cmpdf2+0x4c>  // b.tstop
  48:	cset	w0, ne  // ne = any
  4c:	ret
  50:	mov	w0, #0xffffffff            	// #-1
  54:	cmp	x2, x1
  58:	b.gt	4c <__cmpdf2+0x4c>
  5c:	cset	w0, ne  // ne = any
  60:	b	4c <__cmpdf2+0x4c>

0000000000000064 <__gedf2>:
  64:	fmov	x2, d0
  68:	fmov	x1, d1
  6c:	and	x0, x2, #0x7fffffffffffffff
  70:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
  74:	and	x4, x1, #0x7fffffffffffffff
  78:	cmp	x0, x3
  7c:	mov	w0, #0xffffffff            	// #-1
  80:	ccmp	x4, x3, #0x2, ls  // ls = plast
  84:	b.hi	b0 <__gedf2+0x4c>  // b.pmore
  88:	orr	x3, x2, x1
  8c:	mov	w0, #0x0                   	// #0
  90:	tst	x3, #0x7fffffffffffffff
  94:	b.eq	b0 <__gedf2+0x4c>  // b.none
  98:	tst	x2, x1
  9c:	b.mi	b4 <__gedf2+0x50>  // b.first
  a0:	mov	w0, #0xffffffff            	// #-1
  a4:	cmp	x2, x1
  a8:	b.lt	b0 <__gedf2+0x4c>  // b.tstop
  ac:	cset	w0, ne  // ne = any
  b0:	ret
  b4:	mov	w0, #0xffffffff            	// #-1
  b8:	cmp	x2, x1
  bc:	b.gt	b0 <__gedf2+0x4c>
  c0:	cset	w0, ne  // ne = any
  c4:	b	b0 <__gedf2+0x4c>

00000000000000c8 <__unorddf2>:
  c8:	fmov	x1, d0
  cc:	fmov	x0, d1
  d0:	and	x1, x1, #0x7fffffffffffffff
  d4:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  d8:	and	x0, x0, #0x7fffffffffffffff
  dc:	cmp	x1, x2
  e0:	ccmp	x0, x2, #0x2, ls  // ls = plast
  e4:	cset	w0, hi  // hi = pmore
  e8:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	fmov	w2, s0
   4:	fmov	w1, s1
   8:	and	w0, w2, #0x7fffffff
   c:	mov	w3, #0x7f800000            	// #2139095040
  10:	and	w4, w1, #0x7fffffff
  14:	cmp	w0, w3
  18:	mov	w0, #0x1                   	// #1
  1c:	ccmp	w4, w3, #0x2, ls  // ls = plast
  20:	b.hi	4c <__cmpsf2+0x4c>  // b.pmore
  24:	orr	w3, w2, w1
  28:	mov	w0, #0x0                   	// #0
  2c:	tst	x3, #0x7fffffff
  30:	b.eq	4c <__cmpsf2+0x4c>  // b.none
  34:	tst	w2, w1
  38:	b.mi	50 <__cmpsf2+0x50>  // b.first
  3c:	mov	w0, #0xffffffff            	// #-1
  40:	cmp	w2, w1
  44:	b.lt	4c <__cmpsf2+0x4c>  // b.tstop
  48:	cset	w0, ne  // ne = any
  4c:	ret
  50:	mov	w0, #0xffffffff            	// #-1
  54:	cmp	w2, w1
  58:	b.gt	4c <__cmpsf2+0x4c>
  5c:	cset	w0, ne  // ne = any
  60:	b	4c <__cmpsf2+0x4c>

0000000000000064 <__gesf2>:
  64:	fmov	w2, s0
  68:	fmov	w1, s1
  6c:	and	w0, w2, #0x7fffffff
  70:	mov	w3, #0x7f800000            	// #2139095040
  74:	and	w4, w1, #0x7fffffff
  78:	cmp	w0, w3
  7c:	mov	w0, #0xffffffff            	// #-1
  80:	ccmp	w4, w3, #0x2, ls  // ls = plast
  84:	b.hi	b0 <__gesf2+0x4c>  // b.pmore
  88:	orr	w3, w2, w1
  8c:	mov	w0, #0x0                   	// #0
  90:	tst	x3, #0x7fffffff
  94:	b.eq	b0 <__gesf2+0x4c>  // b.none
  98:	tst	w2, w1
  9c:	b.mi	b4 <__gesf2+0x50>  // b.first
  a0:	mov	w0, #0xffffffff            	// #-1
  a4:	cmp	w2, w1
  a8:	b.lt	b0 <__gesf2+0x4c>  // b.tstop
  ac:	cset	w0, ne  // ne = any
  b0:	ret
  b4:	mov	w0, #0xffffffff            	// #-1
  b8:	cmp	w2, w1
  bc:	b.gt	b0 <__gesf2+0x4c>
  c0:	cset	w0, ne  // ne = any
  c4:	b	b0 <__gesf2+0x4c>

00000000000000c8 <__unordsf2>:
  c8:	fmov	w1, s0
  cc:	fmov	w0, s1
  d0:	and	w1, w1, #0x7fffffff
  d4:	mov	w2, #0x7f800000            	// #2139095040
  d8:	and	w0, w0, #0x7fffffff
  dc:	cmp	w1, w2
  e0:	ccmp	w0, w2, #0x2, ls  // ls = plast
  e4:	cset	w0, hi  // hi = pmore
  e8:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	cmp	w0, #0x0
   4:	csetm	w2, eq  // eq = none
   8:	asr	x1, x0, #32
   c:	csel	w1, w1, w0, eq  // eq = none
  10:	rbit	w1, w1
  14:	clz	w1, w1
  18:	and	w0, w2, #0x20
  1c:	add	w0, w0, w1
  20:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	cset	w4, eq  // eq = none
   8:	lsl	w4, w4, #4
   c:	lsr	w1, w0, w4
  10:	tst	w1, #0xff
  14:	cset	w2, eq  // eq = none
  18:	lsl	w2, w2, #3
  1c:	lsr	w1, w1, w2
  20:	add	w4, w2, w4
  24:	tst	x1, #0xf
  28:	cset	w2, eq  // eq = none
  2c:	lsl	w5, w2, #2
  30:	lsr	w1, w1, w5
  34:	tst	x1, #0x3
  38:	cset	w3, eq  // eq = none
  3c:	lsl	w2, w3, #1
  40:	lsr	w1, w1, w2
  44:	ubfx	x3, x1, #1, #1
  48:	mov	w0, #0x2                   	// #2
  4c:	sub	w0, w0, w3
  50:	eor	w1, w1, #0x1
  54:	sbfx	x1, x1, #0, #1
  58:	and	w0, w0, w1
  5c:	add	w2, w2, w5
  60:	add	w2, w2, w4
  64:	add	w0, w0, w2
  68:	ret

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csetm	w2, eq  // eq = none
   8:	csel	x0, x0, x1, ne  // ne = any
   c:	rbit	x0, x0
  10:	clz	x0, x0
  14:	and	w2, w2, #0x40
  18:	add	w0, w2, w0
  1c:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	stp	x29, x30, [sp, #-96]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	stp	d8, d9, [sp, #48]
  14:	stp	d10, d11, [sp, #64]
  18:	stp	d12, d13, [sp, #80]
  1c:	str	d14, [sp, #40]
  20:	fmov	d13, d0
  24:	fmov	d14, d1
  28:	fmov	d9, d2
  2c:	fmov	d10, d3
  30:	fabs	d8, d2
  34:	fabs	d0, d3
  38:	fmaxnm	d8, d8, d0
  3c:	fmov	x0, d8
  40:	ubfx	x1, x0, #52, #11
  44:	cmp	w1, #0x7ff
  48:	b.eq	64 <__divdc3+0x64>  // b.none
  4c:	fcmp	d8, #0.0
  50:	b.eq	168 <__divdc3+0x168>  // b.none
  54:	cbz	w1, 144 <__divdc3+0x144>
  58:	sub	w1, w1, #0x3ff
  5c:	scvtf	d8, w1
  60:	b	74 <__divdc3+0x74>
  64:	cmp	x0, #0x0
  68:	fneg	d0, d8
  6c:	fccmp	d8, d8, #0x0, lt  // lt = tstop
  70:	fcsel	d8, d0, d8, eq  // eq = none
  74:	fabs	d1, d8
  78:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  7c:	fmov	d0, x0
  80:	fcmp	d1, d0
  84:	cset	w21, le
  88:	fcmp	d8, d8
  8c:	cset	w0, vc
  90:	mov	w19, #0x0                   	// #0
  94:	ands	w20, w21, w0
  98:	b.eq	c8 <__divdc3+0xc8>  // b.none
  9c:	fcvtzs	w19, d8
  a0:	neg	w21, w19
  a4:	mov	w0, w21
  a8:	fmov	d0, d9
  ac:	bl	0 <scalbn>
  b0:	fmov	d9, d0
  b4:	mov	w0, w21
  b8:	fmov	d0, d10
  bc:	bl	0 <scalbn>
  c0:	fmov	d10, d0
  c4:	mov	w21, w20
  c8:	fmul	d11, d9, d9
  cc:	fmul	d0, d10, d10
  d0:	fadd	d11, d11, d0
  d4:	neg	w19, w19
  d8:	fmul	d0, d9, d13
  dc:	fmul	d1, d10, d14
  e0:	fadd	d0, d0, d1
  e4:	mov	w0, w19
  e8:	fdiv	d0, d0, d11
  ec:	bl	0 <scalbn>
  f0:	fmov	d12, d0
  f4:	fmul	d0, d9, d14
  f8:	fmul	d1, d10, d13
  fc:	fsub	d0, d0, d1
 100:	mov	w0, w19
 104:	fdiv	d0, d0, d11
 108:	bl	0 <scalbn>
 10c:	fmov	d2, d0
 110:	fmov	d0, d12
 114:	fmov	d1, d2
 118:	fcmp	d2, d2
 11c:	fccmp	d12, d12, #0x0, vs
 120:	b.vs	17c <__divdc3+0x17c>
 124:	ldp	x19, x20, [sp, #16]
 128:	ldr	x21, [sp, #32]
 12c:	ldp	d8, d9, [sp, #48]
 130:	ldp	d10, d11, [sp, #64]
 134:	ldp	d12, d13, [sp, #80]
 138:	ldr	d14, [sp, #40]
 13c:	ldp	x29, x30, [sp], #96
 140:	ret
 144:	and	x0, x0, #0x7fffffffffffffff
 148:	clz	x1, x0
 14c:	sub	w1, w1, #0xb
 150:	lsl	x0, x0, x1
 154:	ubfx	x0, x0, #52, #11
 158:	sub	w0, w0, #0x3ff
 15c:	sub	w0, w0, w1
 160:	scvtf	d8, w0
 164:	b	74 <__divdc3+0x74>
 168:	adrp	x0, 0 <__divdc3>
 16c:	ldr	d8, [x0]
 170:	mov	w21, #0x0                   	// #0
 174:	mov	w19, #0x0                   	// #0
 178:	b	c8 <__divdc3+0xc8>
 17c:	fcmp	d11, #0.0
 180:	b.ne	1b0 <__divdc3+0x1b0>  // b.any
 184:	fcmp	d13, d13
 188:	fccmp	d14, d14, #0x0, vs
 18c:	b.vs	1b0 <__divdc3+0x1b0>
 190:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 194:	fmov	d1, x0
 198:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 19c:	fmov	d0, x0
 1a0:	bit	v1.8b, v9.8b, v0.8b
 1a4:	fmul	d0, d1, d13
 1a8:	fmul	d1, d1, d14
 1ac:	b	124 <__divdc3+0x124>
 1b0:	fabs	d2, d13
 1b4:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1b8:	fmov	d3, x0
 1bc:	fcmp	d2, d3
 1c0:	b.gt	1d0 <__divdc3+0x1d0>
 1c4:	fabs	d4, d14
 1c8:	fcmp	d4, d3
 1cc:	b.le	1e8 <__divdc3+0x1e8>
 1d0:	fabs	d4, d9
 1d4:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1d8:	fmov	d3, x0
 1dc:	fcmp	d4, d3
 1e0:	fccmp	d9, d9, #0x1, le
 1e4:	b.vc	280 <__divdc3+0x280>
 1e8:	cbnz	w21, 124 <__divdc3+0x124>
 1ec:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1f0:	fmov	d3, x0
 1f4:	fcmpe	d8, #0.0
 1f8:	fccmp	d2, d3, #0x0, gt
 1fc:	fccmp	d13, d13, #0x1, le
 200:	b.vs	124 <__divdc3+0x124>
 204:	fabs	d3, d14
 208:	fmov	d2, x0
 20c:	fcmp	d3, d2
 210:	fccmp	d14, d14, #0x1, le
 214:	b.vs	124 <__divdc3+0x124>
 218:	fabs	d1, d9
 21c:	fcmp	d1, d2
 220:	movi	d0, #0x0
 224:	fmov	d1, #1.000000000000000000e+00
 228:	fcsel	d0, d0, d1, le
 22c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 230:	fmov	d1, x0
 234:	bif	v9.8b, v0.8b, v1.8b
 238:	fabs	d1, d10
 23c:	fcmp	d1, d2
 240:	movi	d0, #0x0
 244:	fmov	d1, #1.000000000000000000e+00
 248:	fcsel	d0, d0, d1, le
 24c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 250:	fmov	d1, x0
 254:	bif	v10.8b, v0.8b, v1.8b
 258:	fmul	d0, d13, d9
 25c:	fmul	d1, d14, d10
 260:	fadd	d0, d0, d1
 264:	movi	d1, #0x0
 268:	fmul	d14, d14, d9
 26c:	fmul	d13, d13, d10
 270:	fsub	d13, d14, d13
 274:	fmul	d0, d0, d1
 278:	fmul	d1, d13, d1
 27c:	b	124 <__divdc3+0x124>
 280:	fabs	d4, d10
 284:	fcmp	d4, d3
 288:	fccmp	d10, d10, #0x1, le
 28c:	b.vs	1e8 <__divdc3+0x1e8>
 290:	fcmp	d2, d3
 294:	movi	d0, #0x0
 298:	fmov	d1, #1.000000000000000000e+00
 29c:	fcsel	d0, d0, d1, le
 2a0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2a4:	fmov	d1, x0
 2a8:	bif	v13.8b, v0.8b, v1.8b
 2ac:	fabs	d1, d14
 2b0:	fcmp	d1, d3
 2b4:	movi	d0, #0x0
 2b8:	fmov	d1, #1.000000000000000000e+00
 2bc:	fcsel	d0, d0, d1, le
 2c0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2c4:	fmov	d1, x0
 2c8:	bif	v14.8b, v0.8b, v1.8b
 2cc:	fmul	d0, d9, d13
 2d0:	fmul	d1, d10, d14
 2d4:	fadd	d0, d0, d1
 2d8:	adrp	x0, 0 <__divdc3>
 2dc:	ldr	d2, [x0]
 2e0:	fmul	d1, d9, d14
 2e4:	fmul	d13, d10, d13
 2e8:	fsub	d1, d1, d13
 2ec:	fmul	d0, d0, d2
 2f0:	fmul	d1, d1, d2
 2f4:	b	124 <__divdc3+0x124>

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	fmov	x1, d0
   4:	ubfx	x3, x1, #52, #11
   8:	fmov	x2, d1
   c:	ubfx	x8, x2, #52, #11
  10:	eor	x4, x2, x1
  14:	and	x4, x4, #0x8000000000000000
  18:	and	x0, x1, #0xfffffffffffff
  1c:	and	x6, x2, #0xfffffffffffff
  20:	sub	w5, w3, #0x1
  24:	cmp	w5, #0x7fd
  28:	b.hi	38 <__divdf3+0x38>  // b.pmore
  2c:	sub	w5, w8, #0x1
  30:	cmp	w5, #0x7fd
  34:	b.ls	114 <__divdf3+0x114>  // b.plast
  38:	and	x5, x1, #0x7fffffffffffffff
  3c:	mov	x7, #0x7ff0000000000000    	// #9218868437227405312
  40:	cmp	x5, x7
  44:	b.hi	90 <__divdf3+0x90>  // b.pmore
  48:	and	x7, x2, #0x7fffffffffffffff
  4c:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  50:	cmp	x7, x9
  54:	b.hi	9c <__divdf3+0x9c>  // b.pmore
  58:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  5c:	cmp	x5, x9
  60:	b.eq	a8 <__divdf3+0xa8>  // b.none
  64:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  68:	fmov	d0, x4
  6c:	cmp	x7, x9
  70:	b.eq	98 <__divdf3+0x98>  // b.none
  74:	cbnz	x5, c4 <__divdf3+0xc4>
  78:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  7c:	fmov	d0, x0
  80:	cmp	x7, #0x0
  84:	fmov	d1, x4
  88:	fcsel	d0, d1, d0, ne  // ne = any
  8c:	b	98 <__divdf3+0x98>
  90:	orr	x0, x1, #0x8000000000000
  94:	fmov	d0, x0
  98:	ret
  9c:	orr	x0, x2, #0x8000000000000
  a0:	fmov	d0, x0
  a4:	b	98 <__divdf3+0x98>
  a8:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  ac:	fmov	d1, x0
  b0:	orr	x0, x4, x9
  b4:	fmov	d0, x0
  b8:	cmp	x7, x9
  bc:	fcsel	d0, d1, d0, eq  // eq = none
  c0:	b	98 <__divdf3+0x98>
  c4:	cbz	x7, 108 <__divdf3+0x108>
  c8:	mov	w5, #0x0                   	// #0
  cc:	tst	x1, #0x7ff0000000000000
  d0:	b.ne	e8 <__divdf3+0xe8>  // b.any
  d4:	clz	x1, x0
  d8:	sub	w1, w1, #0xb
  dc:	lsl	x0, x0, x1
  e0:	mov	w5, #0x1                   	// #1
  e4:	sub	w5, w5, w1
  e8:	tst	x2, #0x7ff0000000000000
  ec:	b.ne	118 <__divdf3+0x118>  // b.any
  f0:	clz	x2, x6
  f4:	sub	w2, w2, #0xb
  f8:	lsl	x6, x6, x2
  fc:	add	w2, w5, w2
 100:	sub	w5, w2, #0x1
 104:	b	118 <__divdf3+0x118>
 108:	orr	x0, x4, #0x7ff0000000000000
 10c:	fmov	d0, x0
 110:	b	98 <__divdf3+0x98>
 114:	mov	w5, #0x0                   	// #0
 118:	orr	x7, x0, #0x10000000000000
 11c:	orr	x6, x6, #0x10000000000000
 120:	sub	w2, w3, w8
 124:	add	w2, w2, w5
 128:	lsr	x8, x6, #21
 12c:	mov	w5, #0xf333                	// #62259
 130:	movk	w5, #0x7504, lsl #16
 134:	sub	w3, w5, w8
 138:	and	x8, x8, #0xffffffff
 13c:	mul	x5, x3, x8
 140:	lsr	x5, x5, #32
 144:	neg	w5, w5
 148:	mul	x5, x5, x3
 14c:	ubfx	x5, x5, #31, #32
 150:	mul	x3, x5, x8
 154:	lsr	x3, x3, #32
 158:	neg	w3, w3
 15c:	mul	x3, x3, x5
 160:	ubfx	x5, x3, #31, #32
 164:	mul	x3, x8, x5
 168:	lsr	x3, x3, #32
 16c:	neg	w3, w3
 170:	mul	x3, x3, x5
 174:	lsr	x3, x3, #31
 178:	sub	w3, w3, #0x1
 17c:	lsl	w1, w6, #11
 180:	mul	x1, x1, x3
 184:	mul	x8, x3, x8
 188:	add	x1, x8, x1, lsr #32
 18c:	neg	x5, x1
 190:	neg	w1, w1
 194:	mul	x1, x1, x3
 198:	lsr	x5, x5, #32
 19c:	mul	x3, x5, x3
 1a0:	sub	x3, x3, #0x2
 1a4:	add	x1, x3, x1, lsr #32
 1a8:	lsl	w0, w0, #2
 1ac:	and	x9, x1, #0xffffffff
 1b0:	lsr	x1, x1, #32
 1b4:	mul	x3, x0, x1
 1b8:	ubfx	x8, x7, #30, #32
 1bc:	mul	x5, x9, x8
 1c0:	mul	x0, x0, x9
 1c4:	and	x9, x3, #0xffffffff
 1c8:	add	x0, x9, x0, lsr #32
 1cc:	add	x0, x0, w5, uxtw
 1d0:	mul	x1, x1, x8
 1d4:	add	x0, x1, x0, lsr #32
 1d8:	lsr	x5, x5, #32
 1dc:	add	x3, x5, x3, lsr #32
 1e0:	add	x0, x0, x3
 1e4:	mov	x1, #0x1fffffffffffff      	// #9007199254740991
 1e8:	cmp	x0, x1
 1ec:	b.hi	234 <__divdf3+0x234>  // b.pmore
 1f0:	lsl	x7, x7, #53
 1f4:	msub	x7, x6, x0, x7
 1f8:	sub	w2, w2, #0x1
 1fc:	add	w2, w2, #0x3ff
 200:	cmp	w2, #0x7fe
 204:	b.gt	244 <__divdf3+0x244>
 208:	cmp	w2, #0x0
 20c:	b.gt	258 <__divdf3+0x258>
 210:	cbnz	w2, 250 <__divdf3+0x250>
 214:	and	x0, x0, #0xfffffffffffff
 218:	cmp	x6, x7, lsl #1
 21c:	cinc	x0, x0, cc  // cc = lo, ul, last
 220:	tst	x0, #0xfff0000000000000
 224:	b.eq	250 <__divdf3+0x250>  // b.none
 228:	orr	x0, x4, #0x10000000000000
 22c:	fmov	d0, x0
 230:	b	98 <__divdf3+0x98>
 234:	lsr	x0, x0, #1
 238:	lsl	x7, x7, #52
 23c:	msub	x7, x6, x0, x7
 240:	b	1fc <__divdf3+0x1fc>
 244:	orr	x0, x4, #0x7ff0000000000000
 248:	fmov	d0, x0
 24c:	b	98 <__divdf3+0x98>
 250:	fmov	d0, x4
 254:	b	98 <__divdf3+0x98>
 258:	bfi	x0, x2, #52, #12
 25c:	cmp	x6, x7, lsl #1
 260:	cinc	x0, x0, cc  // cc = lo, ul, last
 264:	orr	x0, x0, x4
 268:	fmov	d0, x0
 26c:	b	98 <__divdf3+0x98>

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	asr	x3, x0, #63
  10:	asr	x4, x1, #63
  14:	eor	x0, x0, x0, asr #63
  18:	eor	x1, x1, x1, asr #63
  1c:	eor	x19, x3, x4
  20:	mov	x2, #0x0                   	// #0
  24:	sub	x1, x1, x4
  28:	sub	x0, x0, x3
  2c:	bl	0 <__udivmoddi4>
  30:	eor	x0, x0, x19
  34:	sub	x0, x0, x19
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	mov	x21, x0
  14:	mov	x19, x1
  18:	mov	x20, x2
  1c:	bl	0 <__divdi3>
  20:	msub	x19, x19, x0, x21
  24:	str	x19, [x20]
  28:	ldp	x19, x20, [sp, #16]
  2c:	ldr	x21, [sp, #32]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	mov	w21, w0
  14:	mov	w19, w1
  18:	mov	x20, x2
  1c:	bl	0 <__divsi3>
  20:	msub	w19, w19, w0, w21
  24:	str	w19, [x20]
  28:	ldp	x19, x20, [sp, #16]
  2c:	ldr	x21, [sp, #32]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	stp	x29, x30, [sp, #-96]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	d8, d9, [sp, #32]
  10:	stp	d10, d11, [sp, #48]
  14:	stp	d12, d13, [sp, #64]
  18:	str	d14, [sp, #80]
  1c:	fmov	s13, s0
  20:	fmov	s14, s1
  24:	fmov	s9, s2
  28:	fmov	s10, s3
  2c:	fabs	s8, s2
  30:	fabs	s0, s3
  34:	fmaxnm	s8, s8, s0
  38:	fmov	w0, s8
  3c:	ubfx	x1, x0, #23, #8
  40:	cmp	w1, #0xff
  44:	b.eq	60 <__divsc3+0x60>  // b.none
  48:	fcmp	s8, #0.0
  4c:	b.eq	168 <__divsc3+0x168>  // b.none
  50:	cbz	w1, 144 <__divsc3+0x144>
  54:	sub	w1, w1, #0x7f
  58:	scvtf	s8, w1
  5c:	b	70 <__divsc3+0x70>
  60:	cmp	w0, #0x0
  64:	fneg	s0, s8
  68:	fccmp	s8, s8, #0x0, lt  // lt = tstop
  6c:	fcsel	s8, s0, s8, eq  // eq = none
  70:	fabs	s1, s8
  74:	mov	w0, #0x7f7fffff            	// #2139095039
  78:	fmov	s0, w0
  7c:	fcmp	s1, s0
  80:	cset	w20, le
  84:	fcmp	s8, s8
  88:	cset	w0, vc
  8c:	fmov	s12, wzr
  90:	ands	w19, w20, w0
  94:	b.eq	c8 <__divsc3+0xc8>  // b.none
  98:	fcvtzs	s12, s8
  9c:	fmov	w0, s12
  a0:	neg	w20, w0
  a4:	mov	w0, w20
  a8:	fmov	s0, s9
  ac:	bl	0 <scalbnf>
  b0:	fmov	s9, s0
  b4:	mov	w0, w20
  b8:	fmov	s0, s10
  bc:	bl	0 <scalbnf>
  c0:	fmov	s10, s0
  c4:	mov	w20, w19
  c8:	fmul	s11, s9, s9
  cc:	fmul	s0, s10, s10
  d0:	fadd	s11, s11, s0
  d4:	fmov	w0, s12
  d8:	neg	w19, w0
  dc:	fmul	s0, s9, s13
  e0:	fmul	s1, s10, s14
  e4:	fadd	s0, s0, s1
  e8:	mov	w0, w19
  ec:	fdiv	s0, s0, s11
  f0:	bl	0 <scalbnf>
  f4:	fmov	s12, s0
  f8:	fmul	s0, s9, s14
  fc:	fmul	s1, s10, s13
 100:	fsub	s0, s0, s1
 104:	mov	w0, w19
 108:	fdiv	s0, s0, s11
 10c:	bl	0 <scalbnf>
 110:	fmov	s2, s0
 114:	fmov	s0, s12
 118:	fmov	s1, s2
 11c:	fcmp	s2, s2
 120:	fccmp	s12, s12, #0x0, vs
 124:	b.vs	178 <__divsc3+0x178>
 128:	ldp	x19, x20, [sp, #16]
 12c:	ldp	d8, d9, [sp, #32]
 130:	ldp	d10, d11, [sp, #48]
 134:	ldp	d12, d13, [sp, #64]
 138:	ldr	d14, [sp, #80]
 13c:	ldp	x29, x30, [sp], #96
 140:	ret
 144:	and	w0, w0, #0x7fffffff
 148:	clz	w1, w0
 14c:	sub	w1, w1, #0x8
 150:	lsl	w0, w0, w1
 154:	ubfx	x0, x0, #23, #8
 158:	sub	w0, w0, #0x7f
 15c:	sub	w0, w0, w1
 160:	scvtf	s8, w0
 164:	b	70 <__divsc3+0x70>
 168:	mvni	v8.2s, #0x7f, msl #16
 16c:	mov	w20, #0x0                   	// #0
 170:	fmov	s12, wzr
 174:	b	c8 <__divsc3+0xc8>
 178:	fcmp	s11, #0.0
 17c:	b.ne	1a8 <__divsc3+0x1a8>  // b.any
 180:	fcmp	s13, s13
 184:	fccmp	s14, s14, #0x0, vs
 188:	b.vs	1a8 <__divsc3+0x1a8>
 18c:	mov	w0, #0x7f800000            	// #2139095040
 190:	fmov	s1, w0
 194:	movi	v0.2s, #0x80, lsl #24
 198:	bit	v1.8b, v9.8b, v0.8b
 19c:	fmul	s0, s1, s13
 1a0:	fmul	s1, s1, s14
 1a4:	b	128 <__divsc3+0x128>
 1a8:	fabs	s3, s13
 1ac:	adrp	x0, 0 <__divsc3>
 1b0:	ldr	s2, [x0]
 1b4:	fcmp	s3, s2
 1b8:	cset	w0, le
 1bc:	b.gt	1d4 <__divsc3+0x1d4>
 1c0:	fabs	s3, s14
 1c4:	mov	w1, #0x7f7fffff            	// #2139095039
 1c8:	fmov	s2, w1
 1cc:	fcmp	s3, s2
 1d0:	b.le	1ec <__divsc3+0x1ec>
 1d4:	fabs	s3, s9
 1d8:	mov	w1, #0x7f7fffff            	// #2139095039
 1dc:	fmov	s2, w1
 1e0:	fcmp	s3, s2
 1e4:	fccmp	s9, s9, #0x1, le
 1e8:	b.vc	274 <__divsc3+0x274>
 1ec:	cbnz	w20, 128 <__divsc3+0x128>
 1f0:	fcmpe	s8, #0.0
 1f4:	ccmp	w0, #0x0, #0x4, gt
 1f8:	fccmp	s13, s13, #0x1, ne  // ne = any
 1fc:	b.vs	128 <__divsc3+0x128>
 200:	fabs	s3, s14
 204:	mov	w0, #0x7f7fffff            	// #2139095039
 208:	fmov	s2, w0
 20c:	fcmp	s3, s2
 210:	fccmp	s14, s14, #0x1, le
 214:	b.vs	128 <__divsc3+0x128>
 218:	fabs	s0, s9
 21c:	adrp	x0, 0 <__divsc3>
 220:	ldr	s2, [x0]
 224:	fcmp	s0, s2
 228:	cset	w0, gt
 22c:	scvtf	s0, w0
 230:	movi	v1.2s, #0x80, lsl #24
 234:	bif	v9.8b, v0.8b, v1.8b
 238:	fabs	s0, s10
 23c:	fcmp	s0, s2
 240:	cset	w0, gt
 244:	scvtf	s0, w0
 248:	bif	v10.8b, v0.8b, v1.8b
 24c:	fmul	s0, s13, s9
 250:	fmul	s1, s14, s10
 254:	fadd	s0, s0, s1
 258:	movi	v1.2s, #0x0
 25c:	fmul	s14, s14, s9
 260:	fmul	s13, s13, s10
 264:	fsub	s13, s14, s13
 268:	fmul	s0, s0, s1
 26c:	fmul	s1, s13, s1
 270:	b	128 <__divsc3+0x128>
 274:	fabs	s3, s10
 278:	fcmp	s3, s2
 27c:	fccmp	s10, s10, #0x1, le
 280:	b.vs	1ec <__divsc3+0x1ec>
 284:	eor	w0, w0, #0x1
 288:	scvtf	s0, w0
 28c:	movi	v1.2s, #0x80, lsl #24
 290:	bif	v13.8b, v0.8b, v1.8b
 294:	fabs	s2, s14
 298:	fmov	s0, w1
 29c:	fcmp	s2, s0
 2a0:	cset	w0, gt
 2a4:	scvtf	s0, w0
 2a8:	bif	v14.8b, v0.8b, v1.8b
 2ac:	fmul	s0, s9, s13
 2b0:	fmul	s1, s10, s14
 2b4:	fadd	s0, s0, s1
 2b8:	adrp	x0, 0 <__divsc3>
 2bc:	ldr	s2, [x0]
 2c0:	fmul	s1, s9, s14
 2c4:	fmul	s13, s10, s13
 2c8:	fsub	s1, s1, s13
 2cc:	fmul	s0, s0, s2
 2d0:	fmul	s1, s1, s2
 2d4:	b	128 <__divsc3+0x128>

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	fmov	w0, s0
   4:	ubfx	x2, x0, #23, #8
   8:	fmov	w1, s1
   c:	ubfx	x7, x1, #23, #8
  10:	eor	w3, w1, w0
  14:	and	w4, w3, #0x80000000
  18:	and	w5, w0, #0x7fffff
  1c:	and	w6, w1, #0x7fffff
  20:	sub	w3, w2, #0x1
  24:	cmp	w3, #0xfd
  28:	b.hi	38 <__divsf3+0x38>  // b.pmore
  2c:	sub	w3, w7, #0x1
  30:	cmp	w3, #0xfd
  34:	b.ls	114 <__divsf3+0x114>  // b.plast
  38:	and	w3, w0, #0x7fffffff
  3c:	mov	w8, #0x7f800000            	// #2139095040
  40:	cmp	w3, w8
  44:	b.hi	90 <__divsf3+0x90>  // b.pmore
  48:	and	w8, w1, #0x7fffffff
  4c:	mov	w9, #0x7f800000            	// #2139095040
  50:	cmp	w8, w9
  54:	b.hi	9c <__divsf3+0x9c>  // b.pmore
  58:	mov	w9, #0x7f800000            	// #2139095040
  5c:	cmp	w3, w9
  60:	b.eq	a8 <__divsf3+0xa8>  // b.none
  64:	mov	w9, #0x7f800000            	// #2139095040
  68:	fmov	s0, w4
  6c:	cmp	w8, w9
  70:	b.eq	98 <__divsf3+0x98>  // b.none
  74:	cbnz	w3, c4 <__divsf3+0xc4>
  78:	mov	w0, #0x7fc00000            	// #2143289344
  7c:	fmov	s0, w0
  80:	cmp	w8, #0x0
  84:	fmov	s1, w4
  88:	fcsel	s0, s1, s0, ne  // ne = any
  8c:	b	98 <__divsf3+0x98>
  90:	orr	w0, w0, #0x400000
  94:	fmov	s0, w0
  98:	ret
  9c:	orr	w0, w1, #0x400000
  a0:	fmov	s0, w0
  a4:	b	98 <__divsf3+0x98>
  a8:	mov	w0, #0x7fc00000            	// #2143289344
  ac:	fmov	s1, w0
  b0:	orr	w0, w4, w9
  b4:	fmov	s0, w0
  b8:	cmp	w8, w9
  bc:	fcsel	s0, s1, s0, eq  // eq = none
  c0:	b	98 <__divsf3+0x98>
  c4:	cbz	w8, 108 <__divsf3+0x108>
  c8:	mov	w3, #0x0                   	// #0
  cc:	tst	w0, #0x7f800000
  d0:	b.ne	e8 <__divsf3+0xe8>  // b.any
  d4:	clz	w0, w5
  d8:	sub	w0, w0, #0x8
  dc:	lsl	w5, w5, w0
  e0:	mov	w3, #0x1                   	// #1
  e4:	sub	w3, w3, w0
  e8:	tst	w1, #0x7f800000
  ec:	b.ne	118 <__divsf3+0x118>  // b.any
  f0:	clz	w0, w6
  f4:	sub	w0, w0, #0x8
  f8:	lsl	w6, w6, w0
  fc:	add	w3, w3, w0
 100:	sub	w3, w3, #0x1
 104:	b	118 <__divsf3+0x118>
 108:	orr	w0, w4, #0x7f800000
 10c:	fmov	s0, w0
 110:	b	98 <__divsf3+0x98>
 114:	mov	w3, #0x0                   	// #0
 118:	orr	w5, w5, #0x800000
 11c:	orr	w6, w6, #0x800000
 120:	sub	w2, w2, w7
 124:	add	w3, w2, w3
 128:	lsl	w1, w6, #8
 12c:	mov	w2, #0xf333                	// #62259
 130:	movk	w2, #0x7504, lsl #16
 134:	sub	w0, w2, w1
 138:	mov	w1, w1
 13c:	mul	x2, x0, x1
 140:	lsr	x2, x2, #32
 144:	neg	w2, w2
 148:	mul	x2, x2, x0
 14c:	ubfx	x2, x2, #31, #32
 150:	mul	x0, x1, x2
 154:	lsr	x0, x0, #32
 158:	neg	w0, w0
 15c:	mul	x0, x0, x2
 160:	ubfx	x2, x0, #31, #32
 164:	mul	x0, x1, x2
 168:	lsr	x0, x0, #32
 16c:	neg	w0, w0
 170:	mul	x0, x0, x2
 174:	lsr	x0, x0, #31
 178:	sub	w0, w0, #0x2
 17c:	lsl	w1, w5, #1
 180:	mul	x0, x0, x1
 184:	lsr	x0, x0, #32
 188:	mov	w7, w0
 18c:	mov	w1, #0xffffff              	// #16777215
 190:	cmp	w0, w1
 194:	b.hi	1dc <__divsf3+0x1dc>  // b.pmore
 198:	lsl	w1, w5, #24
 19c:	msub	w0, w6, w0, w1
 1a0:	sub	w3, w3, #0x1
 1a4:	add	w2, w3, #0x7f
 1a8:	cmp	w2, #0xfe
 1ac:	b.gt	1ec <__divsf3+0x1ec>
 1b0:	cmp	w2, #0x0
 1b4:	b.gt	200 <__divsf3+0x200>
 1b8:	cbnz	w2, 1f8 <__divsf3+0x1f8>
 1bc:	and	w7, w7, #0x7fffff
 1c0:	cmp	w6, w0, lsl #1
 1c4:	cinc	w7, w7, cc  // cc = lo, ul, last
 1c8:	tst	w7, #0xff800000
 1cc:	b.eq	1f8 <__divsf3+0x1f8>  // b.none
 1d0:	orr	w0, w4, #0x800000
 1d4:	fmov	s0, w0
 1d8:	b	98 <__divsf3+0x98>
 1dc:	lsr	w7, w0, #1
 1e0:	lsl	w0, w5, #23
 1e4:	msub	w0, w6, w7, w0
 1e8:	b	1a4 <__divsf3+0x1a4>
 1ec:	orr	w0, w4, #0x7f800000
 1f0:	fmov	s0, w0
 1f4:	b	98 <__divsf3+0x98>
 1f8:	fmov	s0, w4
 1fc:	b	98 <__divsf3+0x98>
 200:	bfi	w7, w2, #23, #9
 204:	cmp	w6, w0, lsl #1
 208:	cinc	w7, w7, cc  // cc = lo, ul, last
 20c:	orr	w0, w7, w4
 210:	fmov	s0, w0
 214:	b	98 <__divsf3+0x98>

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	asr	w4, w0, #31
   4:	asr	w3, w1, #31
   8:	eor	w0, w0, w0, asr #31
   c:	eor	w1, w1, w1, asr #31
  10:	eor	w2, w4, w3
  14:	sub	w0, w0, w4
  18:	sub	w1, w1, w3
  1c:	udiv	w0, w0, w1
  20:	eor	w0, w0, w2
  24:	sub	w0, w0, w2
  28:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-208]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	stp	x25, x26, [sp, #64]
  18:	stp	x27, x28, [sp, #80]
  1c:	str	q0, [sp, #96]
  20:	ldr	x0, [sp, #96]
  24:	ldr	x26, [sp, #104]
  28:	str	x0, [sp, #184]
  2c:	str	q1, [sp, #96]
  30:	ldr	x1, [sp, #96]
  34:	ldr	x0, [sp, #104]
  38:	str	x1, [sp, #112]
  3c:	str	x0, [sp, #128]
  40:	str	q2, [sp, #96]
  44:	ldr	x20, [sp, #96]
  48:	ldr	x19, [sp, #104]
  4c:	str	q3, [sp, #96]
  50:	ldr	x22, [sp, #96]
  54:	ldr	x21, [sp, #104]
  58:	and	x0, x21, #0x7fffffffffffffff
  5c:	str	x0, [sp, #104]
  60:	ldr	q1, [sp, #96]
  64:	str	x20, [sp, #96]
  68:	and	x0, x19, #0x7fffffffffffffff
  6c:	str	x0, [sp, #104]
  70:	ldr	q0, [sp, #96]
  74:	bl	0 <fmaxl>
  78:	str	q0, [sp, #96]
  7c:	ldr	x23, [sp, #96]
  80:	ldr	x24, [sp, #104]
  84:	ubfx	x27, x24, #48, #15
  88:	mov	w0, #0x7fff                	// #32767
  8c:	cmp	w27, w0
  90:	b.eq	2fc <__divtc3+0x2fc>  // b.none
  94:	movi	v1.2d, #0x0
  98:	str	x23, [sp, #96]
  9c:	str	x24, [sp, #104]
  a0:	ldr	q0, [sp, #96]
  a4:	bl	0 <__eqtf2>
  a8:	cbz	w0, 394 <__divtc3+0x394>
  ac:	cbz	w27, 32c <__divtc3+0x32c>
  b0:	sub	w0, w27, #0x3, lsl #12
  b4:	sub	w0, w0, #0xfff
  b8:	bl	0 <__floatsitf>
  bc:	str	q0, [sp, #96]
  c0:	ldr	x23, [sp, #96]
  c4:	ldr	x24, [sp, #104]
  c8:	and	x25, x24, #0x7fffffffffffffff
  cc:	mov	w27, #0x1                   	// #1
  d0:	adrp	x0, 0 <__divtc3>
  d4:	add	x0, x0, #0x0
  d8:	ldr	q1, [x0]
  dc:	str	x23, [sp, #96]
  e0:	str	x25, [sp, #104]
  e4:	ldr	q0, [sp, #96]
  e8:	bl	0 <__unordtf2>
  ec:	cbnz	w0, 114 <__divtc3+0x114>
  f0:	adrp	x0, 0 <__divtc3>
  f4:	add	x0, x0, #0x0
  f8:	ldr	q1, [x0]
  fc:	str	x23, [sp, #96]
 100:	str	x25, [sp, #104]
 104:	ldr	q0, [sp, #96]
 108:	bl	0 <__letf2>
 10c:	cmp	w0, #0x0
 110:	csel	w27, w27, wzr, le
 114:	and	w27, w27, #0xff
 118:	str	x23, [sp, #96]
 11c:	str	x24, [sp, #104]
 120:	ldr	q1, [sp, #96]
 124:	ldr	q0, [sp, #96]
 128:	bl	0 <__unordtf2>
 12c:	cmp	w0, #0x0
 130:	csel	w28, w27, wzr, eq  // eq = none
 134:	mov	w25, #0x0                   	// #0
 138:	cbz	w28, 198 <__divtc3+0x198>
 13c:	str	x23, [sp, #96]
 140:	str	x24, [sp, #104]
 144:	ldr	q0, [sp, #96]
 148:	bl	0 <__fixtfsi>
 14c:	mov	w25, w0
 150:	neg	w27, w0
 154:	mov	w0, w27
 158:	str	x20, [sp, #96]
 15c:	str	x19, [sp, #104]
 160:	ldr	q0, [sp, #96]
 164:	bl	0 <scalbnl>
 168:	str	q0, [sp, #96]
 16c:	ldr	x20, [sp, #96]
 170:	ldr	x19, [sp, #104]
 174:	mov	w0, w27
 178:	str	x22, [sp, #96]
 17c:	str	x21, [sp, #104]
 180:	ldr	q0, [sp, #96]
 184:	bl	0 <scalbnl>
 188:	str	q0, [sp, #96]
 18c:	ldr	x22, [sp, #96]
 190:	ldr	x21, [sp, #104]
 194:	mov	w27, w28
 198:	str	x20, [sp, #96]
 19c:	str	x19, [sp, #104]
 1a0:	ldr	q1, [sp, #96]
 1a4:	ldr	q0, [sp, #96]
 1a8:	bl	0 <__multf3>
 1ac:	str	q0, [sp, #144]
 1b0:	str	x22, [sp, #96]
 1b4:	str	x21, [sp, #104]
 1b8:	ldr	q1, [sp, #96]
 1bc:	ldr	q0, [sp, #96]
 1c0:	bl	0 <__multf3>
 1c4:	mov	v1.16b, v0.16b
 1c8:	ldr	q0, [sp, #144]
 1cc:	bl	0 <__addtf3>
 1d0:	str	q0, [sp, #144]
 1d4:	neg	w25, w25
 1d8:	ldr	x28, [sp, #184]
 1dc:	str	x28, [sp, #96]
 1e0:	str	x26, [sp, #104]
 1e4:	ldr	q1, [sp, #96]
 1e8:	str	x20, [sp, #96]
 1ec:	str	x19, [sp, #104]
 1f0:	ldr	q0, [sp, #96]
 1f4:	bl	0 <__multf3>
 1f8:	str	q0, [sp, #160]
 1fc:	ldr	x0, [sp, #112]
 200:	str	x0, [sp, #96]
 204:	ldr	x1, [sp, #128]
 208:	str	x1, [sp, #104]
 20c:	ldr	q1, [sp, #96]
 210:	str	x22, [sp, #96]
 214:	str	x21, [sp, #104]
 218:	ldr	q0, [sp, #96]
 21c:	bl	0 <__multf3>
 220:	mov	v1.16b, v0.16b
 224:	ldr	q0, [sp, #160]
 228:	bl	0 <__addtf3>
 22c:	ldr	q1, [sp, #144]
 230:	bl	0 <__divtf3>
 234:	mov	w0, w25
 238:	bl	0 <scalbnl>
 23c:	str	q0, [sp, #160]
 240:	ldr	x0, [sp, #112]
 244:	str	x0, [sp, #96]
 248:	ldr	x1, [sp, #128]
 24c:	str	x1, [sp, #104]
 250:	ldr	q1, [sp, #96]
 254:	str	x20, [sp, #96]
 258:	str	x19, [sp, #104]
 25c:	ldr	q0, [sp, #96]
 260:	bl	0 <__multf3>
 264:	str	q0, [sp, #192]
 268:	str	x28, [sp, #96]
 26c:	str	x26, [sp, #104]
 270:	ldr	q1, [sp, #96]
 274:	str	x22, [sp, #96]
 278:	str	x21, [sp, #104]
 27c:	ldr	q0, [sp, #96]
 280:	bl	0 <__multf3>
 284:	mov	v1.16b, v0.16b
 288:	ldr	q0, [sp, #192]
 28c:	bl	0 <__subtf3>
 290:	ldr	q1, [sp, #144]
 294:	bl	0 <__divtf3>
 298:	mov	w0, w25
 29c:	bl	0 <scalbnl>
 2a0:	mov	v1.16b, v0.16b
 2a4:	ldr	q2, [sp, #160]
 2a8:	str	q2, [sp, #96]
 2ac:	str	q0, [sp, #192]
 2b0:	bl	0 <__unordtf2>
 2b4:	cmp	w0, #0x0
 2b8:	cset	w25, ne  // ne = any
 2bc:	ldr	q0, [sp, #160]
 2c0:	mov	v1.16b, v0.16b
 2c4:	bl	0 <__unordtf2>
 2c8:	cmp	w0, #0x0
 2cc:	cset	w0, ne  // ne = any
 2d0:	tst	w0, w25
 2d4:	b.ne	3a8 <__divtc3+0x3a8>  // b.any
 2d8:	ldr	q0, [sp, #96]
 2dc:	ldr	q1, [sp, #192]
 2e0:	ldp	x19, x20, [sp, #16]
 2e4:	ldp	x21, x22, [sp, #32]
 2e8:	ldp	x23, x24, [sp, #48]
 2ec:	ldp	x25, x26, [sp, #64]
 2f0:	ldp	x27, x28, [sp, #80]
 2f4:	ldp	x29, x30, [sp], #208
 2f8:	ret
 2fc:	mvn	x25, x24
 300:	lsr	x25, x25, #63
 304:	ldr	q1, [sp, #96]
 308:	ldr	q0, [sp, #96]
 30c:	bl	0 <__netf2>
 310:	cmp	w0, #0x0
 314:	cset	w0, ne  // ne = any
 318:	orr	w25, w0, w25
 31c:	cbnz	w25, c8 <__divtc3+0xc8>
 320:	eor	x1, x24, #0x8000000000000000
 324:	mov	x24, x1
 328:	b	c8 <__divtc3+0xc8>
 32c:	ands	x25, x24, #0x7fffffffffffffff
 330:	csel	x0, x25, x23, ne  // ne = any
 334:	cmp	x25, #0x0
 338:	mov	x1, #0x40                  	// #64
 33c:	csel	x1, x1, xzr, eq  // eq = none
 340:	clz	x0, x0
 344:	add	w1, w1, w0
 348:	sub	w0, w1, #0xf
 34c:	subs	w1, w1, #0x4f
 350:	lsl	x3, x23, x1
 354:	lsr	x28, x23, #1
 358:	mov	w2, #0x3f                  	// #63
 35c:	sub	w2, w2, w0
 360:	lsr	x28, x28, x2
 364:	lsl	x25, x25, x0
 368:	orr	x25, x28, x25
 36c:	csel	x25, x3, x25, pl  // pl = nfrst
 370:	ubfx	x25, x25, #48, #15
 374:	sub	w25, w25, #0x3, lsl #12
 378:	sub	w25, w25, #0xfff
 37c:	sub	w0, w25, w0
 380:	bl	0 <__floatsitf>
 384:	str	q0, [sp, #96]
 388:	ldr	x23, [sp, #96]
 38c:	ldr	x24, [sp, #104]
 390:	b	c8 <__divtc3+0xc8>
 394:	mov	x23, #0x0                   	// #0
 398:	mov	x24, #0xffff000000000000    	// #-281474976710656
 39c:	mov	w27, #0x0                   	// #0
 3a0:	mov	w25, #0x0                   	// #0
 3a4:	b	198 <__divtc3+0x198>
 3a8:	movi	v1.2d, #0x0
 3ac:	ldr	q0, [sp, #144]
 3b0:	bl	0 <__eqtf2>
 3b4:	cbnz	w0, 474 <__divtc3+0x474>
 3b8:	ldr	x0, [sp, #184]
 3bc:	str	x0, [sp, #144]
 3c0:	str	x26, [sp, #152]
 3c4:	ldr	q1, [sp, #144]
 3c8:	ldr	q0, [sp, #144]
 3cc:	bl	0 <__unordtf2>
 3d0:	cmp	w0, #0x0
 3d4:	cset	w25, eq  // eq = none
 3d8:	ldr	x0, [sp, #112]
 3dc:	str	x0, [sp, #144]
 3e0:	ldr	x1, [sp, #128]
 3e4:	str	x1, [sp, #152]
 3e8:	ldr	q1, [sp, #144]
 3ec:	ldr	q0, [sp, #144]
 3f0:	bl	0 <__unordtf2>
 3f4:	cmp	w0, #0x0
 3f8:	cset	w0, eq  // eq = none
 3fc:	orr	w25, w25, w0
 400:	tst	w25, #0xff
 404:	b.eq	474 <__divtc3+0x474>  // b.none
 408:	adrp	x0, 0 <__divtc3>
 40c:	add	x0, x0, #0x0
 410:	ldr	q0, [x0]
 414:	str	q0, [sp, #144]
 418:	tbz	x19, #63, 42c <__divtc3+0x42c>
 41c:	adrp	x0, 0 <__divtc3>
 420:	add	x0, x0, #0x0
 424:	ldr	q0, [x0]
 428:	str	q0, [sp, #144]
 42c:	ldr	x0, [sp, #184]
 430:	str	x0, [sp, #96]
 434:	str	x26, [sp, #104]
 438:	ldr	q1, [sp, #96]
 43c:	ldr	q0, [sp, #144]
 440:	bl	0 <__multf3>
 444:	str	q0, [sp, #160]
 448:	ldr	x0, [sp, #112]
 44c:	str	x0, [sp, #96]
 450:	ldr	x0, [sp, #128]
 454:	str	x0, [sp, #104]
 458:	ldr	q1, [sp, #96]
 45c:	ldr	q0, [sp, #144]
 460:	bl	0 <__multf3>
 464:	ldr	q1, [sp, #160]
 468:	str	q1, [sp, #96]
 46c:	str	q0, [sp, #192]
 470:	b	2d8 <__divtc3+0x2d8>
 474:	and	x28, x26, #0x7fffffffffffffff
 478:	adrp	x0, 0 <__divtc3>
 47c:	add	x0, x0, #0x0
 480:	ldr	q1, [x0]
 484:	ldr	x25, [sp, #184]
 488:	str	x25, [sp, #144]
 48c:	str	x28, [sp, #152]
 490:	ldr	q0, [sp, #144]
 494:	bl	0 <__unordtf2>
 498:	cbnz	w0, 7c0 <__divtc3+0x7c0>
 49c:	adrp	x0, 0 <__divtc3>
 4a0:	add	x0, x0, #0x0
 4a4:	ldr	q1, [x0]
 4a8:	str	x25, [sp, #144]
 4ac:	str	x28, [sp, #152]
 4b0:	ldr	q0, [sp, #144]
 4b4:	bl	0 <__letf2>
 4b8:	cmp	w0, #0x0
 4bc:	b.le	7c0 <__divtc3+0x7c0>
 4c0:	and	x1, x19, #0x7fffffffffffffff
 4c4:	mov	w25, #0x1                   	// #1
 4c8:	adrp	x0, 0 <__divtc3>
 4cc:	add	x0, x0, #0x0
 4d0:	ldr	q1, [x0]
 4d4:	str	x20, [sp, #144]
 4d8:	str	x1, [sp, #160]
 4dc:	str	x1, [sp, #152]
 4e0:	ldr	q0, [sp, #144]
 4e4:	bl	0 <__unordtf2>
 4e8:	cbnz	w0, 514 <__divtc3+0x514>
 4ec:	adrp	x0, 0 <__divtc3>
 4f0:	add	x0, x0, #0x0
 4f4:	ldr	q1, [x0]
 4f8:	str	x20, [sp, #144]
 4fc:	ldr	x1, [sp, #160]
 500:	str	x1, [sp, #152]
 504:	ldr	q0, [sp, #144]
 508:	bl	0 <__letf2>
 50c:	cmp	w0, #0x0
 510:	csel	w25, w25, wzr, le
 514:	str	x20, [sp, #144]
 518:	str	x19, [sp, #152]
 51c:	ldr	q1, [sp, #144]
 520:	ldr	q0, [sp, #144]
 524:	bl	0 <__unordtf2>
 528:	cmp	w0, #0x0
 52c:	cset	w0, eq  // eq = none
 530:	tst	w0, w25
 534:	b.ne	818 <__divtc3+0x818>  // b.any
 538:	cbnz	w27, 2d8 <__divtc3+0x2d8>
 53c:	mov	w25, #0x1                   	// #1
 540:	adrp	x0, 0 <__divtc3>
 544:	add	x0, x0, #0x0
 548:	ldr	q1, [x0]
 54c:	ldr	x27, [sp, #184]
 550:	str	x27, [sp, #144]
 554:	str	x28, [sp, #152]
 558:	ldr	q0, [sp, #144]
 55c:	bl	0 <__unordtf2>
 560:	cbnz	w0, 588 <__divtc3+0x588>
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	ldr	q1, [x0]
 570:	str	x27, [sp, #144]
 574:	str	x28, [sp, #152]
 578:	ldr	q0, [sp, #144]
 57c:	bl	0 <__letf2>
 580:	cmp	w0, #0x0
 584:	csel	w25, w25, wzr, le
 588:	movi	v1.2d, #0x0
 58c:	str	x23, [sp, #144]
 590:	str	x24, [sp, #152]
 594:	ldr	q0, [sp, #144]
 598:	bl	0 <__gttf2>
 59c:	cmp	w0, #0x0
 5a0:	cset	w0, gt
 5a4:	and	w25, w25, w0
 5a8:	ldr	x0, [sp, #184]
 5ac:	str	x0, [sp, #144]
 5b0:	str	x26, [sp, #152]
 5b4:	ldr	q1, [sp, #144]
 5b8:	ldr	q0, [sp, #144]
 5bc:	bl	0 <__unordtf2>
 5c0:	cmp	w0, #0x0
 5c4:	cset	w0, eq  // eq = none
 5c8:	tst	w0, w25
 5cc:	b.eq	2d8 <__divtc3+0x2d8>  // b.none
 5d0:	ldr	x0, [sp, #128]
 5d4:	and	x24, x0, #0x7fffffffffffffff
 5d8:	mov	w23, #0x1                   	// #1
 5dc:	adrp	x0, 0 <__divtc3>
 5e0:	add	x0, x0, #0x0
 5e4:	ldr	q1, [x0]
 5e8:	ldr	x25, [sp, #112]
 5ec:	str	x25, [sp, #144]
 5f0:	str	x24, [sp, #152]
 5f4:	ldr	q0, [sp, #144]
 5f8:	bl	0 <__unordtf2>
 5fc:	cbnz	w0, 624 <__divtc3+0x624>
 600:	adrp	x0, 0 <__divtc3>
 604:	add	x0, x0, #0x0
 608:	ldr	q1, [x0]
 60c:	str	x25, [sp, #144]
 610:	str	x24, [sp, #152]
 614:	ldr	q0, [sp, #144]
 618:	bl	0 <__letf2>
 61c:	cmp	w0, #0x0
 620:	csel	w23, w23, wzr, le
 624:	ldr	x0, [sp, #112]
 628:	str	x0, [sp, #144]
 62c:	ldr	x1, [sp, #128]
 630:	str	x1, [sp, #152]
 634:	ldr	q1, [sp, #144]
 638:	ldr	q0, [sp, #144]
 63c:	bl	0 <__unordtf2>
 640:	cmp	w0, #0x0
 644:	cset	w0, eq  // eq = none
 648:	tst	w0, w23
 64c:	b.eq	2d8 <__divtc3+0x2d8>  // b.none
 650:	and	x23, x19, #0x7fffffffffffffff
 654:	adrp	x0, 0 <__divtc3>
 658:	add	x0, x0, #0x0
 65c:	ldr	q1, [x0]
 660:	str	x20, [sp, #96]
 664:	str	x23, [sp, #104]
 668:	ldr	q0, [sp, #96]
 66c:	bl	0 <__unordtf2>
 670:	cbnz	w0, a28 <__divtc3+0xa28>
 674:	adrp	x0, 0 <__divtc3>
 678:	add	x0, x0, #0x0
 67c:	ldr	q1, [x0]
 680:	str	x20, [sp, #96]
 684:	str	x23, [sp, #104]
 688:	ldr	q0, [sp, #96]
 68c:	bl	0 <__letf2>
 690:	cmp	w0, #0x0
 694:	b.le	a28 <__divtc3+0xa28>
 698:	mov	x20, #0x0                   	// #0
 69c:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 6a0:	and	x19, x19, #0x8000000000000000
 6a4:	orr	x19, x0, x19
 6a8:	and	x23, x21, #0x7fffffffffffffff
 6ac:	adrp	x0, 0 <__divtc3>
 6b0:	add	x0, x0, #0x0
 6b4:	ldr	q1, [x0]
 6b8:	str	x22, [sp, #96]
 6bc:	str	x23, [sp, #104]
 6c0:	ldr	q0, [sp, #96]
 6c4:	bl	0 <__unordtf2>
 6c8:	cbnz	w0, a34 <__divtc3+0xa34>
 6cc:	adrp	x0, 0 <__divtc3>
 6d0:	add	x0, x0, #0x0
 6d4:	ldr	q1, [x0]
 6d8:	str	x22, [sp, #96]
 6dc:	str	x23, [sp, #104]
 6e0:	ldr	q0, [sp, #96]
 6e4:	bl	0 <__letf2>
 6e8:	cmp	w0, #0x0
 6ec:	b.le	a34 <__divtc3+0xa34>
 6f0:	mov	x22, #0x0                   	// #0
 6f4:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 6f8:	and	x21, x21, #0x8000000000000000
 6fc:	orr	x21, x0, x21
 700:	str	x20, [sp, #96]
 704:	str	x19, [sp, #104]
 708:	ldr	q1, [sp, #96]
 70c:	ldr	x23, [sp, #184]
 710:	str	x23, [sp, #96]
 714:	str	x26, [sp, #104]
 718:	ldr	q0, [sp, #96]
 71c:	bl	0 <__multf3>
 720:	str	q0, [sp, #144]
 724:	str	x22, [sp, #96]
 728:	str	x21, [sp, #104]
 72c:	ldr	q1, [sp, #96]
 730:	ldr	x24, [sp, #112]
 734:	str	x24, [sp, #96]
 738:	ldr	x25, [sp, #128]
 73c:	str	x25, [sp, #104]
 740:	ldr	q0, [sp, #96]
 744:	bl	0 <__multf3>
 748:	mov	v1.16b, v0.16b
 74c:	ldr	q0, [sp, #144]
 750:	bl	0 <__addtf3>
 754:	movi	v1.2d, #0x0
 758:	bl	0 <__multf3>
 75c:	str	q0, [sp, #112]
 760:	str	x20, [sp, #96]
 764:	str	x19, [sp, #104]
 768:	ldr	q1, [sp, #96]
 76c:	str	x24, [sp, #96]
 770:	str	x25, [sp, #104]
 774:	ldr	q0, [sp, #96]
 778:	bl	0 <__multf3>
 77c:	str	q0, [sp, #128]
 780:	str	x22, [sp, #96]
 784:	str	x21, [sp, #104]
 788:	ldr	q1, [sp, #96]
 78c:	str	x23, [sp, #96]
 790:	str	x26, [sp, #104]
 794:	ldr	q0, [sp, #96]
 798:	bl	0 <__multf3>
 79c:	mov	v1.16b, v0.16b
 7a0:	ldr	q0, [sp, #128]
 7a4:	bl	0 <__subtf3>
 7a8:	movi	v1.2d, #0x0
 7ac:	bl	0 <__multf3>
 7b0:	ldr	q1, [sp, #112]
 7b4:	str	q1, [sp, #96]
 7b8:	str	q0, [sp, #192]
 7bc:	b	2d8 <__divtc3+0x2d8>
 7c0:	ldr	x0, [sp, #128]
 7c4:	and	x25, x0, #0x7fffffffffffffff
 7c8:	adrp	x0, 0 <__divtc3>
 7cc:	add	x0, x0, #0x0
 7d0:	ldr	q1, [x0]
 7d4:	ldr	x0, [sp, #112]
 7d8:	str	x0, [sp, #144]
 7dc:	str	x25, [sp, #152]
 7e0:	ldr	q0, [sp, #144]
 7e4:	bl	0 <__unordtf2>
 7e8:	cbnz	w0, 538 <__divtc3+0x538>
 7ec:	adrp	x1, 0 <__divtc3>
 7f0:	add	x1, x1, #0x0
 7f4:	ldr	q1, [x1]
 7f8:	ldr	x0, [sp, #112]
 7fc:	str	x0, [sp, #144]
 800:	str	x25, [sp, #152]
 804:	ldr	q0, [sp, #144]
 808:	bl	0 <__letf2>
 80c:	cmp	w0, #0x0
 810:	b.gt	4c0 <__divtc3+0x4c0>
 814:	b	538 <__divtc3+0x538>
 818:	and	x1, x21, #0x7fffffffffffffff
 81c:	mov	w25, #0x1                   	// #1
 820:	adrp	x0, 0 <__divtc3>
 824:	add	x0, x0, #0x0
 828:	ldr	q1, [x0]
 82c:	str	x22, [sp, #144]
 830:	str	x1, [sp, #160]
 834:	str	x1, [sp, #152]
 838:	ldr	q0, [sp, #144]
 83c:	bl	0 <__unordtf2>
 840:	cbnz	w0, 86c <__divtc3+0x86c>
 844:	adrp	x0, 0 <__divtc3>
 848:	add	x0, x0, #0x0
 84c:	ldr	q1, [x0]
 850:	str	x22, [sp, #144]
 854:	ldr	x1, [sp, #160]
 858:	str	x1, [sp, #152]
 85c:	ldr	q0, [sp, #144]
 860:	bl	0 <__letf2>
 864:	cmp	w0, #0x0
 868:	csel	w25, w25, wzr, le
 86c:	str	x22, [sp, #144]
 870:	str	x21, [sp, #152]
 874:	ldr	q1, [sp, #144]
 878:	ldr	q0, [sp, #144]
 87c:	bl	0 <__unordtf2>
 880:	cmp	w0, #0x0
 884:	cset	w0, eq  // eq = none
 888:	tst	w0, w25
 88c:	b.eq	538 <__divtc3+0x538>  // b.none
 890:	adrp	x0, 0 <__divtc3>
 894:	add	x0, x0, #0x0
 898:	ldr	q1, [x0]
 89c:	ldr	x23, [sp, #184]
 8a0:	str	x23, [sp, #96]
 8a4:	str	x28, [sp, #104]
 8a8:	ldr	q0, [sp, #96]
 8ac:	bl	0 <__unordtf2>
 8b0:	cbnz	w0, a10 <__divtc3+0xa10>
 8b4:	adrp	x0, 0 <__divtc3>
 8b8:	add	x0, x0, #0x0
 8bc:	ldr	q1, [x0]
 8c0:	str	x23, [sp, #96]
 8c4:	str	x28, [sp, #104]
 8c8:	ldr	q0, [sp, #96]
 8cc:	bl	0 <__letf2>
 8d0:	cmp	w0, #0x0
 8d4:	b.le	a10 <__divtc3+0xa10>
 8d8:	mov	x24, #0x0                   	// #0
 8dc:	mov	x23, #0x3fff000000000000    	// #4611404543450677248
 8e0:	and	x26, x26, #0x8000000000000000
 8e4:	orr	x23, x23, x26
 8e8:	ldr	x0, [sp, #128]
 8ec:	and	x25, x0, #0x7fffffffffffffff
 8f0:	adrp	x0, 0 <__divtc3>
 8f4:	add	x0, x0, #0x0
 8f8:	ldr	q1, [x0]
 8fc:	ldr	x26, [sp, #112]
 900:	str	x26, [sp, #96]
 904:	str	x25, [sp, #104]
 908:	ldr	q0, [sp, #96]
 90c:	bl	0 <__unordtf2>
 910:	cbnz	w0, a1c <__divtc3+0xa1c>
 914:	adrp	x0, 0 <__divtc3>
 918:	add	x0, x0, #0x0
 91c:	ldr	q1, [x0]
 920:	str	x26, [sp, #96]
 924:	str	x25, [sp, #104]
 928:	ldr	q0, [sp, #96]
 92c:	bl	0 <__letf2>
 930:	cmp	w0, #0x0
 934:	b.le	a1c <__divtc3+0xa1c>
 938:	mov	x26, #0x0                   	// #0
 93c:	mov	x25, #0x3fff000000000000    	// #4611404543450677248
 940:	ldr	x0, [sp, #128]
 944:	and	x0, x0, #0x8000000000000000
 948:	orr	x25, x25, x0
 94c:	str	x24, [sp, #96]
 950:	str	x23, [sp, #104]
 954:	ldr	q1, [sp, #96]
 958:	str	x20, [sp, #96]
 95c:	str	x19, [sp, #104]
 960:	ldr	q0, [sp, #96]
 964:	bl	0 <__multf3>
 968:	str	q0, [sp, #112]
 96c:	str	x26, [sp, #96]
 970:	str	x25, [sp, #104]
 974:	ldr	q1, [sp, #96]
 978:	str	x22, [sp, #96]
 97c:	str	x21, [sp, #104]
 980:	ldr	q0, [sp, #96]
 984:	bl	0 <__multf3>
 988:	mov	v1.16b, v0.16b
 98c:	ldr	q0, [sp, #112]
 990:	bl	0 <__addtf3>
 994:	adrp	x0, 0 <__divtc3>
 998:	add	x0, x0, #0x0
 99c:	ldr	q1, [x0]
 9a0:	bl	0 <__multf3>
 9a4:	str	q0, [sp, #112]
 9a8:	str	x26, [sp, #96]
 9ac:	str	x25, [sp, #104]
 9b0:	ldr	q1, [sp, #96]
 9b4:	str	x20, [sp, #96]
 9b8:	str	x19, [sp, #104]
 9bc:	ldr	q0, [sp, #96]
 9c0:	bl	0 <__multf3>
 9c4:	str	q0, [sp, #128]
 9c8:	str	x24, [sp, #96]
 9cc:	str	x23, [sp, #104]
 9d0:	ldr	q1, [sp, #96]
 9d4:	str	x22, [sp, #96]
 9d8:	str	x21, [sp, #104]
 9dc:	ldr	q0, [sp, #96]
 9e0:	bl	0 <__multf3>
 9e4:	mov	v1.16b, v0.16b
 9e8:	ldr	q0, [sp, #128]
 9ec:	bl	0 <__subtf3>
 9f0:	adrp	x0, 0 <__divtc3>
 9f4:	add	x0, x0, #0x0
 9f8:	ldr	q1, [x0]
 9fc:	bl	0 <__multf3>
 a00:	ldr	q1, [sp, #112]
 a04:	str	q1, [sp, #96]
 a08:	str	q0, [sp, #192]
 a0c:	b	2d8 <__divtc3+0x2d8>
 a10:	mov	x24, #0x0                   	// #0
 a14:	mov	x23, #0x0                   	// #0
 a18:	b	8e0 <__divtc3+0x8e0>
 a1c:	mov	x26, #0x0                   	// #0
 a20:	mov	x25, #0x0                   	// #0
 a24:	b	940 <__divtc3+0x940>
 a28:	mov	x20, #0x0                   	// #0
 a2c:	mov	x0, #0x0                   	// #0
 a30:	b	6a0 <__divtc3+0x6a0>
 a34:	mov	x22, #0x0                   	// #0
 a38:	mov	x0, #0x0                   	// #0
 a3c:	b	6f8 <__divtc3+0x6f8>

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	asr	x5, x1, #63
  10:	asr	x4, x3, #63
  14:	eor	x0, x0, x5
  18:	eor	x1, x1, x5
  1c:	eor	x2, x2, x4
  20:	eor	x3, x3, x4
  24:	eor	x19, x5, x4
  28:	subs	x2, x2, x4
  2c:	sbc	x3, x3, x4
  30:	subs	x0, x0, x5
  34:	mov	x4, #0x0                   	// #0
  38:	sbc	x1, x1, x5
  3c:	bl	0 <__udivmodti4>
  40:	eor	x0, x0, x19
  44:	eor	x1, x1, x19
  48:	subs	x0, x0, x19
  4c:	sbc	x1, x1, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #32
  58:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #32]
   c:	ldr	x7, [sp, #32]
  10:	ldr	x0, [sp, #40]
  14:	str	q1, [sp, #32]
  18:	ldr	x5, [sp, #32]
  1c:	ldr	x1, [sp, #40]
  20:	mov	x12, x7
  24:	ubfx	x6, x0, #48, #15
  28:	mov	x13, x5
  2c:	mov	x11, x1
  30:	ubfx	x14, x1, #48, #15
  34:	eor	x1, x1, x0
  38:	mov	x2, #0x0                   	// #0
  3c:	and	x3, x1, #0x8000000000000000
  40:	and	x10, x0, #0xffffffffffff
  44:	and	x8, x11, #0xffffffffffff
  48:	sub	w9, w6, #0x1
  4c:	mov	w4, #0x7ffd                	// #32765
  50:	cmp	w9, w4
  54:	b.hi	68 <__divtf3+0x68>  // b.pmore
  58:	sub	w4, w14, #0x1
  5c:	mov	w1, #0x7ffd                	// #32765
  60:	cmp	w4, w1
  64:	b.ls	204 <__divtf3+0x204>  // b.plast
  68:	and	x4, x0, #0x7fffffffffffffff
  6c:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  70:	cmp	x4, x1
  74:	b.hi	b8 <__divtf3+0xb8>  // b.pmore
  78:	b.eq	b4 <__divtf3+0xb4>  // b.none
  7c:	and	x9, x11, #0x7fffffffffffffff
  80:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  84:	cmp	x9, x1
  88:	b.hi	d0 <__divtf3+0xd0>  // b.pmore
  8c:	b.eq	cc <__divtf3+0xcc>  // b.none
  90:	cbz	x12, e0 <__divtf3+0xe0>
  94:	cbz	x13, 118 <__divtf3+0x118>
  98:	orr	x4, x12, x4
  9c:	cbnz	x4, 130 <__divtf3+0x130>
  a0:	orr	x5, x13, x9
  a4:	cbz	x5, 688 <__divtf3+0x688>
  a8:	fmov	d0, x2
  ac:	fmov	v0.d[1], x3
  b0:	b	c4 <__divtf3+0xc4>
  b4:	cbz	x12, 7c <__divtf3+0x7c>
  b8:	orr	x3, x0, #0x800000000000
  bc:	fmov	d0, x12
  c0:	fmov	v0.d[1], x3
  c4:	ldp	x29, x30, [sp], #48
  c8:	ret
  cc:	cbz	x13, 90 <__divtf3+0x90>
  d0:	orr	x1, x11, #0x800000000000
  d4:	fmov	d0, x13
  d8:	fmov	v0.d[1], x1
  dc:	b	c4 <__divtf3+0xc4>
  e0:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  e4:	cmp	x4, x1
  e8:	b.ne	94 <__divtf3+0x94>  // b.any
  ec:	cbz	x13, 100 <__divtf3+0x100>
  f0:	orr	x1, x3, #0x7fff000000000000
  f4:	fmov	d0, x2
  f8:	fmov	v0.d[1], x1
  fc:	b	c4 <__divtf3+0xc4>
 100:	cmp	x9, x1
 104:	b.ne	f0 <__divtf3+0xf0>  // b.any
 108:	adrp	x0, 0 <__divtf3>
 10c:	add	x0, x0, #0x0
 110:	ldr	q0, [x0]
 114:	b	c4 <__divtf3+0xc4>
 118:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
 11c:	cmp	x9, x1
 120:	b.ne	98 <__divtf3+0x98>  // b.any
 124:	fmov	d0, x2
 128:	fmov	v0.d[1], x3
 12c:	b	c4 <__divtf3+0xc4>
 130:	orr	x9, x13, x9
 134:	cbz	x9, 1f4 <__divtf3+0x1f4>
 138:	str	x19, [sp, #16]
 13c:	mov	w1, #0x0                   	// #0
 140:	tst	x0, #0x7fff000000000000
 144:	b.ne	198 <__divtf3+0x198>  // b.any
 148:	cmp	x10, #0x0
 14c:	csel	x1, x10, x12, ne  // ne = any
 150:	mov	x0, #0x40                  	// #64
 154:	csel	x0, x0, xzr, eq  // eq = none
 158:	clz	x1, x1
 15c:	add	w0, w0, w1
 160:	sub	w1, w0, #0xf
 164:	subs	w0, w0, #0x4f
 168:	lsl	x9, x12, x0
 16c:	lsr	x4, x12, #1
 170:	mov	w7, #0x3f                  	// #63
 174:	sub	w7, w7, w1
 178:	lsr	x4, x4, x7
 17c:	lsl	x10, x10, x1
 180:	orr	x10, x4, x10
 184:	lsl	x7, x12, x1
 188:	csel	x10, x9, x10, pl  // pl = nfrst
 18c:	csel	x7, xzr, x7, pl  // pl = nfrst
 190:	mov	w0, #0x1                   	// #1
 194:	sub	w1, w0, w1
 198:	tst	x11, #0x7fff000000000000
 19c:	b.ne	20c <__divtf3+0x20c>  // b.any
 1a0:	cmp	x8, #0x0
 1a4:	csel	x4, x8, x13, ne  // ne = any
 1a8:	mov	x0, #0x40                  	// #64
 1ac:	csel	x0, x0, xzr, eq  // eq = none
 1b0:	clz	x4, x4
 1b4:	add	w0, w0, w4
 1b8:	sub	w4, w0, #0xf
 1bc:	subs	w0, w0, #0x4f
 1c0:	lsl	x11, x13, x0
 1c4:	lsr	x5, x13, #1
 1c8:	mov	w9, #0x3f                  	// #63
 1cc:	sub	w9, w9, w4
 1d0:	lsr	x5, x5, x9
 1d4:	lsl	x8, x8, x4
 1d8:	orr	x8, x5, x8
 1dc:	lsl	x5, x13, x4
 1e0:	csel	x8, x11, x8, pl  // pl = nfrst
 1e4:	csel	x5, xzr, x5, pl  // pl = nfrst
 1e8:	add	w1, w1, w4
 1ec:	sub	w1, w1, #0x1
 1f0:	b	20c <__divtf3+0x20c>
 1f4:	orr	x1, x3, #0x7fff000000000000
 1f8:	fmov	d0, x2
 1fc:	fmov	v0.d[1], x1
 200:	b	c4 <__divtf3+0xc4>
 204:	str	x19, [sp, #16]
 208:	mov	w1, #0x0                   	// #0
 20c:	orr	x10, x10, #0x1000000000000
 210:	mov	x9, x5
 214:	orr	x8, x8, #0x1000000000000
 218:	sub	w6, w6, w14
 21c:	add	w6, w6, w1
 220:	extr	x14, x8, x5, #49
 224:	mov	x4, #0x6484                	// #25732
 228:	movk	x4, #0xf9de, lsl #16
 22c:	movk	x4, #0xf333, lsl #32
 230:	movk	x4, #0x7504, lsl #48
 234:	sub	x11, x4, x14
 238:	umulh	x0, x11, x14
 23c:	neg	x4, x0
 240:	mneg	x0, x0, x11
 244:	umulh	x4, x4, x11
 248:	extr	x4, x4, x0, #63
 24c:	umulh	x1, x4, x14
 250:	neg	x0, x1
 254:	mneg	x1, x1, x4
 258:	umulh	x0, x0, x4
 25c:	extr	x4, x0, x1, #63
 260:	umulh	x12, x14, x4
 264:	neg	x0, x12
 268:	mneg	x12, x12, x4
 26c:	umulh	x0, x0, x4
 270:	extr	x0, x0, x12, #63
 274:	umulh	x1, x0, x14
 278:	neg	x12, x1
 27c:	mneg	x1, x1, x0
 280:	umulh	x12, x12, x0
 284:	extr	x0, x12, x1, #63
 288:	umulh	x1, x0, x14
 28c:	neg	x12, x1
 290:	mneg	x1, x1, x0
 294:	umulh	x12, x12, x0
 298:	extr	x12, x12, x1, #63
 29c:	sub	x12, x12, #0x1
 2a0:	lsr	x13, x14, #32
 2a4:	and	x17, x14, #0xffffffff
 2a8:	lsr	x14, x12, #32
 2ac:	and	x12, x12, #0xffffffff
 2b0:	mul	x1, x17, x14
 2b4:	mul	x4, x13, x12
 2b8:	adds	x4, x1, x4
 2bc:	cset	x18, cs  // cs = hs, nlast
 2c0:	ubfx	x1, x5, #17, #32
 2c4:	lsl	w15, w5, #15
 2c8:	mul	x11, x15, x14
 2cc:	mul	x0, x1, x12
 2d0:	adds	x11, x11, x0
 2d4:	cset	x0, cs  // cs = hs, nlast
 2d8:	lsl	x16, x11, #32
 2dc:	mul	x15, x15, x12
 2e0:	adds	x15, x16, x15
 2e4:	extr	x0, x0, x11, #32
 2e8:	madd	x0, x1, x14, x0
 2ec:	extr	x1, x18, x4, #32
 2f0:	madd	x13, x13, x14, x1
 2f4:	cinc	x0, x0, cs  // cs = hs, nlast
 2f8:	lsl	x1, x4, #32
 2fc:	mul	x4, x17, x12
 300:	adds	x1, x1, x4
 304:	cset	x4, cs  // cs = hs, nlast
 308:	adds	x0, x0, x1
 30c:	adc	x1, x13, x4
 310:	negs	x0, x0
 314:	ngc	x1, x1
 318:	lsr	x11, x1, #32
 31c:	and	x15, x1, #0xffffffff
 320:	mul	x1, x15, x14
 324:	mul	x4, x11, x12
 328:	adds	x1, x1, x4
 32c:	cset	x17, cs  // cs = hs, nlast
 330:	lsr	x4, x0, #32
 334:	and	x0, x0, #0xffffffff
 338:	mul	x13, x0, x14
 33c:	mul	x16, x4, x12
 340:	adds	x13, x13, x16
 344:	cset	x18, cs  // cs = hs, nlast
 348:	lsl	x16, x13, #32
 34c:	mul	x0, x0, x12
 350:	adds	x0, x16, x0
 354:	extr	x13, x18, x13, #32
 358:	madd	x4, x4, x14, x13
 35c:	cinc	x4, x4, cs  // cs = hs, nlast
 360:	extr	x0, x17, x1, #32
 364:	madd	x11, x11, x14, x0
 368:	lsl	x0, x1, #32
 36c:	mul	x1, x15, x12
 370:	mov	x12, #0xfffffffffffffffe    	// #-2
 374:	adds	x1, x1, x12
 378:	csetm	x12, cc  // cc = lo, ul, last
 37c:	adds	x0, x0, x1
 380:	adc	x11, x11, x12
 384:	adds	x4, x4, x0
 388:	cinc	x11, x11, cs  // cs = hs, nlast
 38c:	extr	x15, x10, x7, #62
 390:	ubfx	x1, x10, #30, #32
 394:	lsr	x12, x11, #32
 398:	and	x11, x11, #0xffffffff
 39c:	lsr	x16, x4, #32
 3a0:	and	x4, x4, #0xffffffff
 3a4:	and	x15, x15, #0xffffffff
 3a8:	ubfx	x0, x7, #30, #32
 3ac:	lsl	w10, w7, #2
 3b0:	mov	x18, x0
 3b4:	mul	x13, x16, x0
 3b8:	mul	x0, x11, x10
 3bc:	adds	x13, x13, x0
 3c0:	cset	x14, cs  // cs = hs, nlast
 3c4:	mul	x19, x4, x15
 3c8:	adds	x19, x13, x19
 3cc:	cinc	x14, x14, cs  // cs = hs, nlast
 3d0:	mul	x13, x1, x4
 3d4:	mul	x0, x12, x10
 3d8:	adds	x13, x13, x0
 3dc:	cset	x17, cs  // cs = hs, nlast
 3e0:	mul	x30, x16, x15
 3e4:	mul	x0, x11, x18
 3e8:	adds	x30, x30, x0
 3ec:	cset	x0, cs  // cs = hs, nlast
 3f0:	adds	x13, x13, x30
 3f4:	adc	x17, x17, x0
 3f8:	mul	x4, x4, x18
 3fc:	mul	x10, x16, x10
 400:	adds	x10, x4, x10
 404:	cset	x4, cs  // cs = hs, nlast
 408:	extr	x4, x4, x10, #32
 40c:	adds	x4, x4, x19
 410:	cset	x0, cs  // cs = hs, nlast
 414:	lsl	x10, x13, #32
 418:	adds	x4, x4, x10
 41c:	cinc	x0, x0, cs  // cs = hs, nlast
 420:	mul	x4, x1, x11
 424:	mul	x10, x12, x15
 428:	adds	x4, x4, x10
 42c:	cset	x19, cs  // cs = hs, nlast
 430:	extr	x19, x19, x4, #32
 434:	lsl	x4, x4, #32
 438:	adds	x4, x0, x4
 43c:	cinc	x19, x19, cs  // cs = hs, nlast
 440:	mul	x10, x1, x12
 444:	mul	x0, x12, x18
 448:	mul	x1, x1, x16
 44c:	mul	x11, x11, x15
 450:	adds	x1, x1, x11
 454:	cset	x11, cs  // cs = hs, nlast
 458:	adds	x0, x0, x1
 45c:	adc	x1, x10, x11
 460:	extr	x13, x17, x13, #32
 464:	adds	x14, x14, x13
 468:	cset	x10, cs  // cs = hs, nlast
 46c:	adds	x0, x0, x14
 470:	adc	x1, x1, x10
 474:	adds	x0, x4, x0
 478:	adc	x19, x19, x1
 47c:	mov	x13, x0
 480:	mov	x4, x19
 484:	mov	x1, #0x1ffffffffffff       	// #562949953421311
 488:	cmp	x19, x1
 48c:	b.hi	57c <__divtf3+0x57c>  // b.pmore
 490:	and	x10, x8, #0xffffffff
 494:	lsr	x14, x5, #32
 498:	and	x11, x5, #0xffffffff
 49c:	and	x17, x19, #0xffffffff
 4a0:	lsr	x16, x0, #32
 4a4:	and	x0, x0, #0xffffffff
 4a8:	mul	x12, x11, x16
 4ac:	mov	x18, #0x0                   	// #0
 4b0:	mul	x1, x14, x0
 4b4:	adds	x12, x12, x1
 4b8:	cset	x15, cs  // cs = hs, nlast
 4bc:	lsl	x5, x7, #49
 4c0:	mul	x1, x11, x0
 4c4:	subs	x1, x18, x1
 4c8:	sbc	x5, x5, xzr
 4cc:	lsr	x19, x19, #32
 4d0:	lsr	x7, x8, #32
 4d4:	mul	x7, x0, x7
 4d8:	madd	x19, x11, x19, x7
 4dc:	mul	x7, x10, x16
 4e0:	madd	x7, x14, x17, x7
 4e4:	adds	x19, x19, x7
 4e8:	mul	x14, x14, x16
 4ec:	madd	x11, x11, x17, x14
 4f0:	madd	x0, x10, x0, x11
 4f4:	extr	x7, x15, x12, #32
 4f8:	adds	x0, x0, x7
 4fc:	add	x0, x0, x19, lsl #32
 500:	lsl	x7, x12, #32
 504:	subs	x1, x1, x7
 508:	sbc	x5, x5, x0
 50c:	sub	w6, w6, #0x1
 510:	add	w6, w6, #0x3, lsl #12
 514:	add	w6, w6, #0xfff
 518:	mov	w0, #0x7ffe                	// #32766
 51c:	cmp	w6, w0
 520:	b.gt	608 <__divtf3+0x608>
 524:	cmp	w6, #0x0
 528:	b.gt	638 <__divtf3+0x638>
 52c:	cbnz	w6, 628 <__divtf3+0x628>
 530:	and	x4, x4, #0xffffffffffff
 534:	extr	x5, x5, x1, #63
 538:	lsl	x1, x1, #1
 53c:	mov	x0, #0x1                   	// #1
 540:	mov	x6, #0x0                   	// #0
 544:	cmp	x5, x8
 548:	b.hi	558 <__divtf3+0x558>  // b.pmore
 54c:	b.eq	61c <__divtf3+0x61c>  // b.none
 550:	mov	x0, #0x0                   	// #0
 554:	mov	x6, #0x0                   	// #0
 558:	adds	x0, x0, x13
 55c:	adc	x4, x4, x6
 560:	tst	x4, #0xffff000000000000
 564:	b.eq	628 <__divtf3+0x628>  // b.none
 568:	orr	x1, x3, #0x1000000000000
 56c:	fmov	d0, x2
 570:	fmov	v0.d[1], x1
 574:	ldr	x19, [sp, #16]
 578:	b	c4 <__divtf3+0xc4>
 57c:	and	x10, x8, #0xffffffff
 580:	lsr	x13, x5, #32
 584:	and	x4, x5, #0xffffffff
 588:	ubfx	x17, x19, #1, #32
 58c:	extr	x5, x19, x0, #33
 590:	and	x5, x5, #0xffffffff
 594:	ubfx	x15, x0, #1, #32
 598:	mul	x12, x4, x5
 59c:	mov	x11, #0x0                   	// #0
 5a0:	mul	x1, x13, x15
 5a4:	adds	x12, x12, x1
 5a8:	cset	x14, cs  // cs = hs, nlast
 5ac:	lsl	x7, x7, #48
 5b0:	mul	x1, x4, x15
 5b4:	subs	x1, x11, x1
 5b8:	sbc	x7, x7, xzr
 5bc:	lsr	x11, x19, #33
 5c0:	lsr	x16, x8, #32
 5c4:	mul	x16, x15, x16
 5c8:	madd	x11, x4, x11, x16
 5cc:	mul	x16, x10, x5
 5d0:	madd	x16, x13, x17, x16
 5d4:	adds	x11, x11, x16
 5d8:	mul	x5, x13, x5
 5dc:	madd	x5, x4, x17, x5
 5e0:	madd	x5, x10, x15, x5
 5e4:	extr	x4, x14, x12, #32
 5e8:	adds	x5, x5, x4
 5ec:	add	x5, x5, x11, lsl #32
 5f0:	lsl	x4, x12, #32
 5f4:	subs	x1, x1, x4
 5f8:	sbc	x5, x7, x5
 5fc:	extr	x13, x19, x0, #1
 600:	lsr	x4, x19, #1
 604:	b	510 <__divtf3+0x510>
 608:	orr	x1, x3, #0x7fff000000000000
 60c:	fmov	d0, x2
 610:	fmov	v0.d[1], x1
 614:	ldr	x19, [sp, #16]
 618:	b	c4 <__divtf3+0xc4>
 61c:	cmp	x1, x9
 620:	b.hi	558 <__divtf3+0x558>  // b.pmore
 624:	b	550 <__divtf3+0x550>
 628:	fmov	d0, x2
 62c:	fmov	v0.d[1], x3
 630:	ldr	x19, [sp, #16]
 634:	b	c4 <__divtf3+0xc4>
 638:	bfi	x4, x6, #48, #16
 63c:	extr	x5, x5, x1, #63
 640:	lsl	x1, x1, #1
 644:	mov	x0, #0x1                   	// #1
 648:	mov	x6, #0x0                   	// #0
 64c:	cmp	x8, x5
 650:	b.hi	660 <__divtf3+0x660>  // b.pmore
 654:	b.ne	668 <__divtf3+0x668>  // b.any
 658:	cmp	x9, x1
 65c:	b.ls	668 <__divtf3+0x668>  // b.plast
 660:	mov	x0, #0x0                   	// #0
 664:	mov	x6, #0x0                   	// #0
 668:	adds	x13, x0, x13
 66c:	adc	x4, x4, x6
 670:	orr	x0, x13, x2
 674:	orr	x1, x4, x3
 678:	fmov	d0, x0
 67c:	fmov	v0.d[1], x1
 680:	ldr	x19, [sp, #16]
 684:	b	c4 <__divtf3+0xc4>
 688:	adrp	x0, 0 <__divtf3>
 68c:	add	x0, x0, #0x0
 690:	ldr	q0, [x0]
 694:	b	c4 <__divtf3+0xc4>

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	fmov	w0, s0
   4:	and	w1, w0, #0x7fffffff
   8:	and	w0, w0, #0x80000000
   c:	sub	w3, w1, #0x800, lsl #12
  10:	mov	w2, #0x7effffff            	// #2130706431
  14:	cmp	w3, w2
  18:	b.hi	38 <__extendsfdf2+0x38>  // b.pmore
  1c:	ubfiz	x1, x1, #29, #31
  20:	mov	x2, #0x3800000000000000    	// #4035225266123964416
  24:	add	x2, x1, x2
  28:	mov	w0, w0
  2c:	orr	x0, x2, x0, lsl #32
  30:	fmov	d0, x0
  34:	ret
  38:	mov	w2, #0x7f7fffff            	// #2139095039
  3c:	cmp	w1, w2
  40:	b.ls	50 <__extendsfdf2+0x50>  // b.plast
  44:	ubfiz	x1, x1, #29, #23
  48:	orr	x2, x1, #0x7ff0000000000000
  4c:	b	28 <__extendsfdf2+0x28>
  50:	mov	x2, #0x0                   	// #0
  54:	cbz	w1, 28 <__extendsfdf2+0x28>
  58:	clz	w3, w1
  5c:	mov	w1, w1
  60:	add	w2, w3, #0x15
  64:	lsl	x1, x1, x2
  68:	eor	x1, x1, #0x10000000000000
  6c:	mov	w2, #0x389                 	// #905
  70:	sub	w2, w2, w3
  74:	orr	x2, x1, x2, lsl #52
  78:	b	28 <__extendsfdf2+0x28>

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	and	w1, w0, #0x7fff
   4:	and	w0, w0, #0x8000
   8:	sub	w3, w1, #0x400
   c:	mov	w2, #0x77ff                	// #30719
  10:	cmp	w2, w3, uxth
  14:	b.cc	2c <__extendhfsf2+0x2c>  // b.lo, b.ul, b.last
  18:	mov	w2, #0x38000000            	// #939524096
  1c:	add	w2, w2, w1, lsl #13
  20:	orr	w1, w2, w0, lsl #16
  24:	fmov	s0, w1
  28:	ret
  2c:	mov	w2, #0x7bff                	// #31743
  30:	cmp	w1, w2
  34:	b.ls	44 <__extendhfsf2+0x44>  // b.plast
  38:	ubfiz	w1, w1, #13, #10
  3c:	orr	w2, w1, #0x7f800000
  40:	b	20 <__extendhfsf2+0x20>
  44:	mov	w2, #0x0                   	// #0
  48:	cbz	w1, 20 <__extendhfsf2+0x20>
  4c:	clz	w3, w1
  50:	sub	w2, w3, #0x8
  54:	lsl	w2, w1, w2
  58:	eor	w2, w2, #0x800000
  5c:	mov	w1, #0x86                  	// #134
  60:	sub	w1, w1, w3
  64:	orr	w2, w2, w1, lsl #23
  68:	b	20 <__extendhfsf2+0x20>

000000000000006c <__gnu_h2f_ieee>:
  6c:	stp	x29, x30, [sp, #-16]!
  70:	mov	x29, sp
  74:	bl	0 <__extendhfsf2>
  78:	ldp	x29, x30, [sp], #16
  7c:	ret

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbnz	w0, 20 <__ffsdi2+0x20>
   4:	asr	x0, x0, #32
   8:	rbit	w1, w0
   c:	clz	w1, w1
  10:	add	w1, w1, #0x21
  14:	cmp	w0, #0x0
  18:	csel	w0, w0, w1, eq  // eq = none
  1c:	ret
  20:	rbit	w0, w0
  24:	clz	w0, w0
  28:	add	w0, w0, #0x1
  2c:	b	1c <__ffsdi2+0x1c>

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	rbit	w1, w0
   4:	clz	w1, w1
   8:	cmp	w0, #0x0
   c:	csinc	w0, w0, w1, eq  // eq = none
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbnz	x0, 1c <__ffsti2+0x1c>
   4:	rbit	x0, x1
   8:	clz	x0, x0
   c:	add	w0, w0, #0x41
  10:	cmp	x1, #0x0
  14:	csel	w0, w0, wzr, ne  // ne = any
  18:	ret
  1c:	rbit	x0, x0
  20:	clz	x0, x0
  24:	add	w0, w0, #0x1
  28:	b	18 <__ffsti2+0x18>

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	fcmpe	d0, #0.0
   c:	b.mi	1c <__fixdfdi+0x1c>  // b.first
  10:	bl	0 <__fixunsdfdi>
  14:	ldp	x29, x30, [sp], #16
  18:	ret
  1c:	fneg	d0, d0
  20:	bl	0 <__fixunsdfdi>
  24:	neg	x0, x0
  28:	b	14 <__fixdfdi+0x14>

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	fmov	x1, d0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	x1, #0x0
   c:	cneg	w2, w0, lt  // lt = tstop
  10:	ubfx	x3, x1, #52, #11
  14:	mov	w0, #0x0                   	// #0
  18:	subs	w4, w3, #0x3ff
  1c:	b.mi	58 <__fixdfsi+0x58>  // b.first
  20:	cmp	w4, #0x1f
  24:	b.hi	4c <__fixdfsi+0x4c>  // b.pmore
  28:	and	x1, x1, #0xfffffffffffff
  2c:	orr	x1, x1, #0x10000000000000
  30:	cmp	w4, #0x33
  34:	b.gt	5c <__fixdfsi+0x5c>
  38:	mov	w0, #0x34                  	// #52
  3c:	sub	w4, w0, w4
  40:	lsr	x1, x1, x4
  44:	mul	w0, w1, w2
  48:	b	58 <__fixdfsi+0x58>
  4c:	mov	w1, #0x80000000            	// #-2147483648
  50:	cmp	w2, #0x1
  54:	cinv	w0, w1, eq  // eq = none
  58:	ret
  5c:	sub	w3, w3, #0x433
  60:	lsl	w1, w1, w3
  64:	mul	w0, w1, w2
  68:	b	58 <__fixdfsi+0x58>

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fmov	x2, d0
   4:	tbnz	x2, #63, 50 <__fixdfti+0x50>
   8:	mov	x0, #0x1                   	// #1
   c:	mov	x1, #0x0                   	// #0
  10:	ubfx	x3, x2, #52, #11
  14:	subs	w4, w3, #0x3ff
  18:	b.mi	cc <__fixdfti+0xcc>  // b.first
  1c:	cmp	w4, #0x7f
  20:	b.hi	5c <__fixdfti+0x5c>  // b.pmore
  24:	and	x2, x2, #0xfffffffffffff
  28:	orr	x2, x2, #0x10000000000000
  2c:	cmp	w4, #0x33
  30:	b.gt	90 <__fixdfti+0x90>
  34:	mov	w3, #0x34                  	// #52
  38:	sub	w4, w3, w4
  3c:	lsr	x2, x2, x4
  40:	umulh	x3, x2, x0
  44:	mul	x0, x2, x0
  48:	madd	x1, x2, x1, x3
  4c:	b	d4 <__fixdfti+0xd4>
  50:	mov	x0, #0xffffffffffffffff    	// #-1
  54:	mov	x1, x0
  58:	b	10 <__fixdfti+0x10>
  5c:	cmp	x0, #0x1
  60:	b.eq	78 <__fixdfti+0x78>  // b.none
  64:	adrp	x0, 0 <__fixdfti>
  68:	add	x1, x0, #0x0
  6c:	ldr	x0, [x0]
  70:	ldr	x1, [x1, #8]
  74:	b	d4 <__fixdfti+0xd4>
  78:	cbnz	x1, 64 <__fixdfti+0x64>
  7c:	adrp	x0, 0 <__fixdfti>
  80:	add	x1, x0, #0x0
  84:	ldr	x0, [x0]
  88:	ldr	x1, [x1, #8]
  8c:	b	d4 <__fixdfti+0xd4>
  90:	sub	w7, w3, #0x433
  94:	subs	w3, w3, #0x473
  98:	lsl	x4, x2, x3
  9c:	lsr	x5, x2, #1
  a0:	mov	w6, #0x3f                  	// #63
  a4:	sub	w6, w6, w7
  a8:	lsr	x5, x5, x6
  ac:	lsl	x2, x2, x7
  b0:	csel	x4, x4, x5, pl  // pl = nfrst
  b4:	csel	x2, xzr, x2, pl  // pl = nfrst
  b8:	umulh	x3, x2, x0
  bc:	madd	x3, x4, x0, x3
  c0:	mul	x0, x2, x0
  c4:	madd	x1, x2, x1, x3
  c8:	b	d4 <__fixdfti+0xd4>
  cc:	mov	x0, #0x0                   	// #0
  d0:	mov	x1, #0x0                   	// #0
  d4:	ret

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	fcmpe	s0, #0.0
   c:	b.mi	1c <__fixsfdi+0x1c>  // b.first
  10:	bl	0 <__fixunssfdi>
  14:	ldp	x29, x30, [sp], #16
  18:	ret
  1c:	fneg	s0, s0
  20:	bl	0 <__fixunssfdi>
  24:	neg	x0, x0
  28:	b	14 <__fixsfdi+0x14>

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	fmov	w1, s0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	w1, #0x0
   c:	cneg	w2, w0, lt  // lt = tstop
  10:	ubfx	x3, x1, #23, #8
  14:	mov	w0, #0x0                   	// #0
  18:	subs	w4, w3, #0x7f
  1c:	b.mi	58 <__fixsfsi+0x58>  // b.first
  20:	cmp	w4, #0x1f
  24:	b.hi	4c <__fixsfsi+0x4c>  // b.pmore
  28:	and	w1, w1, #0x7fffff
  2c:	orr	w1, w1, #0x800000
  30:	cmp	w4, #0x16
  34:	b.gt	5c <__fixsfsi+0x5c>
  38:	mov	w0, #0x17                  	// #23
  3c:	sub	w4, w0, w4
  40:	lsr	w1, w1, w4
  44:	mul	w0, w1, w2
  48:	b	58 <__fixsfsi+0x58>
  4c:	mov	w1, #0x80000000            	// #-2147483648
  50:	cmp	w2, #0x1
  54:	cinv	w0, w1, eq  // eq = none
  58:	ret
  5c:	sub	w3, w3, #0x96
  60:	lsl	w1, w1, w3
  64:	mul	w0, w1, w2
  68:	b	58 <__fixsfsi+0x58>

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fmov	w2, s0
   4:	tbnz	w2, #31, 50 <__fixsfti+0x50>
   8:	mov	x0, #0x1                   	// #1
   c:	mov	x1, #0x0                   	// #0
  10:	ubfx	x4, x2, #23, #8
  14:	subs	w3, w4, #0x7f
  18:	b.mi	d0 <__fixsfti+0xd0>  // b.first
  1c:	cmp	w3, #0x7f
  20:	b.hi	5c <__fixsfti+0x5c>  // b.pmore
  24:	and	w2, w2, #0x7fffff
  28:	orr	w2, w2, #0x800000
  2c:	cmp	w3, #0x16
  30:	b.gt	90 <__fixsfti+0x90>
  34:	mov	w4, #0x17                  	// #23
  38:	sub	w3, w4, w3
  3c:	lsr	w3, w2, w3
  40:	umulh	x2, x3, x0
  44:	mul	x0, x3, x0
  48:	madd	x1, x3, x1, x2
  4c:	b	d8 <__fixsfti+0xd8>
  50:	mov	x0, #0xffffffffffffffff    	// #-1
  54:	mov	x1, x0
  58:	b	10 <__fixsfti+0x10>
  5c:	cmp	x0, #0x1
  60:	b.eq	78 <__fixsfti+0x78>  // b.none
  64:	adrp	x0, 0 <__fixsfti>
  68:	add	x1, x0, #0x0
  6c:	ldr	x0, [x0]
  70:	ldr	x1, [x1, #8]
  74:	b	d8 <__fixsfti+0xd8>
  78:	cbnz	x1, 64 <__fixsfti+0x64>
  7c:	adrp	x0, 0 <__fixsfti>
  80:	add	x1, x0, #0x0
  84:	ldr	x0, [x0]
  88:	ldr	x1, [x1, #8]
  8c:	b	d8 <__fixsfti+0xd8>
  90:	mov	w2, w2
  94:	sub	w7, w4, #0x96
  98:	subs	w4, w4, #0xd6
  9c:	lsl	x5, x2, x4
  a0:	lsr	x3, x2, #1
  a4:	mov	w6, #0x3f                  	// #63
  a8:	sub	w6, w6, w7
  ac:	lsr	x3, x3, x6
  b0:	lsl	x2, x2, x7
  b4:	csel	x5, x5, x3, pl  // pl = nfrst
  b8:	csel	x2, xzr, x2, pl  // pl = nfrst
  bc:	umulh	x3, x2, x0
  c0:	madd	x3, x5, x0, x3
  c4:	mul	x0, x2, x0
  c8:	madd	x1, x2, x1, x3
  cc:	b	d8 <__fixsfti+0xd8>
  d0:	mov	x0, #0x0                   	// #0
  d4:	mov	x1, #0x0                   	// #0
  d8:	ret

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	mov	x0, #0x0                   	// #0
   4:	fcmpe	d0, #0.0
   8:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   c:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
  10:	fmov	d1, x0
  14:	fmul	d1, d0, d1
  18:	fcvtzu	w1, d1
  1c:	ucvtf	d1, w1
  20:	mov	x0, #0x41f0000000000000    	// #4751297606875873280
  24:	fmov	d2, x0
  28:	fmul	d1, d1, d2
  2c:	fsub	d0, d0, d1
  30:	fcvtzu	w0, d0
  34:	orr	x0, x0, x1, lsl #32
  38:	ret

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	fmov	x1, d0
   4:	mov	w0, #0x0                   	// #0
   8:	tbnz	x1, #63, 40 <__fixunsdfsi+0x40>
   c:	lsr	x2, x1, #52
  10:	and	x1, x1, #0xfffffffffffff
  14:	orr	x1, x1, #0x10000000000000
  18:	subs	w3, w2, #0x3ff
  1c:	b.mi	50 <__fixunsdfsi+0x50>  // b.first
  20:	mov	w0, #0xffffffff            	// #-1
  24:	cmp	w3, #0x1f
  28:	b.hi	40 <__fixunsdfsi+0x40>  // b.pmore
  2c:	cmp	w3, #0x33
  30:	b.gt	44 <__fixunsdfsi+0x44>
  34:	mov	w0, #0x34                  	// #52
  38:	sub	w0, w0, w3
  3c:	lsr	x0, x1, x0
  40:	ret
  44:	sub	w0, w2, #0x433
  48:	lsl	w0, w1, w0
  4c:	b	40 <__fixunsdfsi+0x40>
  50:	mov	w0, #0x0                   	// #0
  54:	b	40 <__fixunsdfsi+0x40>

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	fmov	x0, d0
   4:	tbnz	x0, #63, 6c <__fixunsdfti+0x6c>
   8:	lsr	x1, x0, #52
   c:	and	x0, x0, #0xfffffffffffff
  10:	orr	x0, x0, #0x10000000000000
  14:	subs	w2, w1, #0x3ff
  18:	b.mi	78 <__fixunsdfti+0x78>  // b.first
  1c:	cmp	w2, #0x7f
  20:	b.hi	84 <__fixunsdfti+0x84>  // b.pmore
  24:	cmp	w2, #0x33
  28:	b.gt	40 <__fixunsdfti+0x40>
  2c:	mov	w1, #0x34                  	// #52
  30:	sub	w2, w1, w2
  34:	lsr	x0, x0, x2
  38:	mov	x1, #0x0                   	// #0
  3c:	b	74 <__fixunsdfti+0x74>
  40:	sub	w5, w1, #0x433
  44:	subs	w2, w1, #0x473
  48:	lsl	x1, x0, x2
  4c:	lsr	x3, x0, #1
  50:	mov	w4, #0x3f                  	// #63
  54:	sub	w4, w4, w5
  58:	lsr	x3, x3, x4
  5c:	lsl	x0, x0, x5
  60:	csel	x1, x1, x3, pl  // pl = nfrst
  64:	csel	x0, xzr, x0, pl  // pl = nfrst
  68:	b	74 <__fixunsdfti+0x74>
  6c:	mov	x0, #0x0                   	// #0
  70:	mov	x1, #0x0                   	// #0
  74:	ret
  78:	mov	x0, #0x0                   	// #0
  7c:	mov	x1, #0x0                   	// #0
  80:	b	74 <__fixunsdfti+0x74>
  84:	mov	x0, #0xffffffffffffffff    	// #-1
  88:	mov	x1, x0
  8c:	b	74 <__fixunsdfti+0x74>

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	mov	x0, #0x0                   	// #0
   4:	fcmpe	s0, #0.0
   8:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   c:	fcvt	d0, s0
  10:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
  14:	fmov	d1, x0
  18:	fmul	d1, d0, d1
  1c:	fcvtzu	w1, d1
  20:	ucvtf	d1, w1
  24:	mov	x0, #0x41f0000000000000    	// #4751297606875873280
  28:	fmov	d2, x0
  2c:	fmul	d1, d1, d2
  30:	fsub	d0, d0, d1
  34:	fcvtzu	w0, d0
  38:	orr	x0, x0, x1, lsl #32
  3c:	ret

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	fmov	w1, s0
   4:	mov	w0, #0x0                   	// #0
   8:	tbnz	w1, #31, 40 <__fixunssfsi+0x40>
   c:	lsr	w2, w1, #23
  10:	and	w1, w1, #0x7fffff
  14:	orr	w1, w1, #0x800000
  18:	subs	w3, w2, #0x7f
  1c:	b.mi	50 <__fixunssfsi+0x50>  // b.first
  20:	mov	w0, #0xffffffff            	// #-1
  24:	cmp	w3, #0x1f
  28:	b.hi	40 <__fixunssfsi+0x40>  // b.pmore
  2c:	cmp	w3, #0x16
  30:	b.gt	44 <__fixunssfsi+0x44>
  34:	mov	w0, #0x17                  	// #23
  38:	sub	w0, w0, w3
  3c:	lsr	w0, w1, w0
  40:	ret
  44:	sub	w0, w2, #0x96
  48:	lsl	w0, w1, w0
  4c:	b	40 <__fixunssfsi+0x40>
  50:	mov	w0, #0x0                   	// #0
  54:	b	40 <__fixunssfsi+0x40>

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fmov	w0, s0
   4:	tbnz	w0, #31, 70 <__fixunssfti+0x70>
   8:	lsr	w1, w0, #23
   c:	and	w0, w0, #0x7fffff
  10:	orr	w0, w0, #0x800000
  14:	subs	w2, w1, #0x7f
  18:	b.mi	7c <__fixunssfti+0x7c>  // b.first
  1c:	cmp	w2, #0x7f
  20:	b.hi	88 <__fixunssfti+0x88>  // b.pmore
  24:	cmp	w2, #0x16
  28:	b.gt	40 <__fixunssfti+0x40>
  2c:	mov	w1, #0x17                  	// #23
  30:	sub	w2, w1, w2
  34:	lsr	w0, w0, w2
  38:	mov	x1, #0x0                   	// #0
  3c:	b	78 <__fixunssfti+0x78>
  40:	mov	w0, w0
  44:	sub	w5, w1, #0x96
  48:	subs	w2, w1, #0xd6
  4c:	lsl	x1, x0, x2
  50:	lsr	x3, x0, #1
  54:	mov	w4, #0x3f                  	// #63
  58:	sub	w4, w4, w5
  5c:	lsr	x3, x3, x4
  60:	lsl	x0, x0, x5
  64:	csel	x1, x1, x3, pl  // pl = nfrst
  68:	csel	x0, xzr, x0, pl  // pl = nfrst
  6c:	b	78 <__fixunssfti+0x78>
  70:	mov	x0, #0x0                   	// #0
  74:	mov	x1, #0x0                   	// #0
  78:	ret
  7c:	mov	x0, #0x0                   	// #0
  80:	mov	x1, #0x0                   	// #0
  84:	b	78 <__fixunssfti+0x78>
  88:	mov	x0, #0xffffffffffffffff    	// #-1
  8c:	mov	x1, x0
  90:	b	78 <__fixunssfti+0x78>

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	asr	x1, x0, #32
   4:	scvtf	d1, w1
   8:	mov	x1, #0x41f0000000000000    	// #4751297606875873280
   c:	fmov	d0, x1
  10:	fmul	d1, d1, d0
  14:	mov	x1, #0x4330000000000000    	// #4841369599423283200
  18:	fmov	d0, x1
  1c:	fsub	d1, d1, d0
  20:	and	x0, x0, #0xffffffff
  24:	orr	x0, x0, x1
  28:	fmov	d0, x0
  2c:	fadd	d0, d1, d0
  30:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	x0, ac <__floatdisf+0xac>
   8:	asr	x3, x0, #63
   c:	eor	x0, x0, x3
  10:	sub	x0, x0, x3
  14:	mov	x5, x0
  18:	clz	x1, x0
  1c:	mov	w4, #0x40                  	// #64
  20:	sub	w4, w4, w1
  24:	sub	w2, w4, #0x1
  28:	cmp	w4, #0x18
  2c:	b.le	88 <__floatdisf+0x88>
  30:	cmp	w4, #0x19
  34:	b.eq	64 <__floatdisf+0x64>  // b.none
  38:	cmp	w4, #0x1a
  3c:	b.eq	68 <__floatdisf+0x68>  // b.none
  40:	add	w1, w1, #0x1a
  44:	mov	x0, #0xffffffffffffffff    	// #-1
  48:	lsr	x1, x0, x1
  4c:	tst	x1, x5
  50:	cset	x1, ne  // ne = any
  54:	sub	w0, w4, #0x1a
  58:	lsr	x0, x5, x0
  5c:	orr	x0, x1, x0
  60:	b	68 <__floatdisf+0x68>
  64:	lsl	x0, x0, #1
  68:	ubfx	x1, x0, #2, #1
  6c:	orr	x0, x1, x0
  70:	add	x1, x0, #0x1
  74:	asr	x0, x1, #2
  78:	tbz	w1, #26, 94 <__floatdisf+0x94>
  7c:	asr	x0, x1, #3
  80:	mov	w2, w4
  84:	b	94 <__floatdisf+0x94>
  88:	mov	w1, #0x18                  	// #24
  8c:	sub	w4, w1, w4
  90:	lsl	x0, x0, x4
  94:	and	w3, w3, #0x80000000
  98:	and	w0, w0, #0x7fffff
  9c:	orr	w0, w3, w0
  a0:	add	w2, w2, #0x7f
  a4:	orr	w0, w0, w2, lsl #23
  a8:	fmov	s0, w0
  ac:	ret

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	movi	d0, #0x0
   4:	cbz	w0, 38 <__floatsidf+0x38>
   8:	mov	x3, #0x0                   	// #0
   c:	tbnz	w0, #31, 3c <__floatsidf+0x3c>
  10:	clz	w2, w0
  14:	mov	w0, w0
  18:	add	w1, w2, #0x15
  1c:	lsl	x0, x0, x1
  20:	eor	x0, x0, #0x10000000000000
  24:	mov	w1, #0x41e                 	// #1054
  28:	sub	w1, w1, w2
  2c:	add	x0, x0, x1, lsl #52
  30:	orr	x0, x0, x3
  34:	fmov	d0, x0
  38:	ret
  3c:	neg	w0, w0
  40:	mov	x3, #0x8000000000000000    	// #-9223372036854775808
  44:	b	10 <__floatsidf+0x10>

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	w0, 44 <__floatsisf+0x44>
   8:	mov	w4, #0x0                   	// #0
   c:	tbnz	w0, #31, 48 <__floatsisf+0x48>
  10:	clz	w2, w0
  14:	mov	w1, #0x1f                  	// #31
  18:	sub	w1, w1, w2
  1c:	cmp	w1, #0x17
  20:	b.gt	54 <__floatsisf+0x54>
  24:	sub	w1, w2, #0x8
  28:	lsl	w0, w0, w1
  2c:	eor	w1, w0, #0x800000
  30:	mov	w0, #0x9e                  	// #158
  34:	sub	w0, w0, w2
  38:	add	w0, w1, w0, lsl #23
  3c:	orr	w0, w0, w4
  40:	fmov	s0, w0
  44:	ret
  48:	neg	w0, w0
  4c:	mov	w4, #0x80000000            	// #-2147483648
  50:	b	10 <__floatsisf+0x10>
  54:	mov	w3, #0x8                   	// #8
  58:	sub	w3, w3, w2
  5c:	lsr	w1, w0, w3
  60:	eor	w1, w1, #0x800000
  64:	neg	w3, w3
  68:	lsl	w0, w0, w3
  6c:	mov	w3, #0x80000000            	// #-2147483648
  70:	cmp	w0, w3
  74:	b.ls	80 <__floatsisf+0x80>  // b.plast
  78:	add	w1, w1, #0x1
  7c:	b	30 <__floatsisf+0x30>
  80:	add	w3, w1, #0x1
  84:	and	w3, w3, #0xfffffffe
  88:	mov	w5, #0x80000000            	// #-2147483648
  8c:	cmp	w0, w5
  90:	csel	w1, w3, w1, eq  // eq = none
  94:	b	30 <__floatsisf+0x30>

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x2, x0, x1
   4:	movi	d0, #0x0
   8:	cbnz	x2, 10 <__floattidf+0x10>
   c:	ret
  10:	stp	x29, x30, [sp, #-64]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	stp	x21, x22, [sp, #32]
  20:	str	x23, [sp, #48]
  24:	asr	x21, x1, #63
  28:	eor	x0, x0, x21
  2c:	eor	x1, x1, x21
  30:	subs	x20, x0, x21
  34:	sbc	x19, x1, x21
  38:	mov	x23, x20
  3c:	mov	x22, x19
  40:	mov	x0, x20
  44:	mov	x1, x19
  48:	bl	0 <__clzti2>
  4c:	mov	w3, #0x80                  	// #128
  50:	sub	w3, w3, w0
  54:	sub	w2, w3, #0x1
  58:	cmp	w3, #0x35
  5c:	b.le	118 <__floattidf+0x118>
  60:	cmp	w3, #0x36
  64:	b.eq	ec <__floattidf+0xec>  // b.none
  68:	cmp	w3, #0x37
  6c:	b.eq	f4 <__floattidf+0xf4>  // b.none
  70:	add	w6, w0, #0x37
  74:	subs	w0, w0, #0x9
  78:	mov	x4, #0xffffffffffffffff    	// #-1
  7c:	lsr	x8, x4, x0
  80:	mov	w5, #0x3f                  	// #63
  84:	sub	w1, w5, w6
  88:	mov	x7, #0xfffffffffffffffe    	// #-2
  8c:	lsl	x7, x7, x1
  90:	lsr	x1, x4, x6
  94:	orr	x1, x7, x1
  98:	lsr	x4, x4, x6
  9c:	csel	x1, x8, x1, pl  // pl = nfrst
  a0:	csel	x4, xzr, x4, pl  // pl = nfrst
  a4:	and	x1, x1, x20
  a8:	and	x4, x4, x19
  ac:	orr	x1, x1, x4
  b0:	cmp	x1, #0x0
  b4:	cset	x1, ne  // ne = any
  b8:	sub	w22, w3, #0x37
  bc:	subs	w4, w3, #0x77
  c0:	lsr	x6, x19, x4
  c4:	lsl	x0, x19, #1
  c8:	sub	w5, w5, w22
  cc:	lsl	x5, x0, x5
  d0:	lsr	x0, x20, x22
  d4:	orr	x0, x5, x0
  d8:	lsr	x19, x19, x22
  dc:	csel	x0, x6, x0, pl  // pl = nfrst
  e0:	orr	x23, x1, x0
  e4:	csel	x22, xzr, x19, pl  // pl = nfrst
  e8:	b	f4 <__floattidf+0xf4>
  ec:	lsl	x23, x20, #1
  f0:	extr	x22, x19, x20, #63
  f4:	ubfx	x1, x23, #2, #1
  f8:	orr	x1, x1, x23
  fc:	adds	x1, x1, #0x1
 100:	cinc	x22, x22, cs  // cs = hs, nlast
 104:	extr	x0, x22, x1, #2
 108:	tbz	x1, #55, 12c <__floattidf+0x12c>
 10c:	extr	x0, x22, x1, #3
 110:	mov	w2, w3
 114:	b	12c <__floattidf+0x12c>
 118:	mov	w0, #0x35                  	// #53
 11c:	sub	w0, w0, w3
 120:	subs	w1, w0, #0x40
 124:	lsl	x0, x20, x0
 128:	csel	x0, xzr, x0, pl  // pl = nfrst
 12c:	add	w2, w2, #0x3ff
 130:	ubfx	x1, x0, #32, #20
 134:	orr	w2, w1, w2, lsl #20
 138:	and	w21, w21, #0x80000000
 13c:	orr	w21, w2, w21
 140:	bfi	x0, x21, #32, #32
 144:	fmov	d0, x0
 148:	ldp	x19, x20, [sp, #16]
 14c:	ldp	x21, x22, [sp, #32]
 150:	ldr	x23, [sp, #48]
 154:	ldp	x29, x30, [sp], #64
 158:	ret

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x2, x0, x1
   4:	movi	v0.2s, #0x0
   8:	cbnz	x2, 10 <__floattisf+0x10>
   c:	ret
  10:	stp	x29, x30, [sp, #-64]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	stp	x21, x22, [sp, #32]
  20:	str	x23, [sp, #48]
  24:	asr	x21, x1, #63
  28:	eor	x0, x0, x21
  2c:	eor	x1, x1, x21
  30:	subs	x20, x0, x21
  34:	sbc	x19, x1, x21
  38:	mov	x23, x20
  3c:	mov	x22, x19
  40:	mov	x0, x20
  44:	mov	x1, x19
  48:	bl	0 <__clzti2>
  4c:	mov	w3, #0x80                  	// #128
  50:	sub	w3, w3, w0
  54:	sub	w2, w3, #0x1
  58:	cmp	w3, #0x18
  5c:	b.le	118 <__floattisf+0x118>
  60:	cmp	w3, #0x19
  64:	b.eq	ec <__floattisf+0xec>  // b.none
  68:	cmp	w3, #0x1a
  6c:	b.eq	f4 <__floattisf+0xf4>  // b.none
  70:	add	w6, w0, #0x1a
  74:	subs	w0, w0, #0x26
  78:	mov	x4, #0xffffffffffffffff    	// #-1
  7c:	lsr	x8, x4, x0
  80:	mov	w5, #0x3f                  	// #63
  84:	sub	w1, w5, w6
  88:	mov	x7, #0xfffffffffffffffe    	// #-2
  8c:	lsl	x7, x7, x1
  90:	lsr	x1, x4, x6
  94:	orr	x1, x7, x1
  98:	lsr	x4, x4, x6
  9c:	csel	x1, x8, x1, pl  // pl = nfrst
  a0:	csel	x4, xzr, x4, pl  // pl = nfrst
  a4:	and	x1, x1, x20
  a8:	and	x4, x4, x19
  ac:	orr	x1, x1, x4
  b0:	cmp	x1, #0x0
  b4:	cset	x1, ne  // ne = any
  b8:	sub	w22, w3, #0x1a
  bc:	subs	w0, w3, #0x5a
  c0:	lsr	x6, x19, x0
  c4:	lsl	x4, x19, #1
  c8:	sub	w5, w5, w22
  cc:	lsl	x5, x4, x5
  d0:	lsr	x20, x20, x22
  d4:	orr	x20, x5, x20
  d8:	lsr	x19, x19, x22
  dc:	csel	x20, x6, x20, pl  // pl = nfrst
  e0:	orr	x23, x1, x20
  e4:	csel	x22, xzr, x19, pl  // pl = nfrst
  e8:	b	f4 <__floattisf+0xf4>
  ec:	lsl	x23, x20, #1
  f0:	extr	x22, x19, x20, #63
  f4:	ubfx	x1, x23, #2, #1
  f8:	orr	x1, x1, x23
  fc:	adds	x1, x1, #0x1
 100:	cinc	x22, x22, cs  // cs = hs, nlast
 104:	extr	x0, x22, x1, #2
 108:	tbz	w1, #26, 12c <__floattisf+0x12c>
 10c:	extr	x0, x22, x1, #3
 110:	mov	w2, w3
 114:	b	12c <__floattisf+0x12c>
 118:	mov	w0, #0x18                  	// #24
 11c:	sub	w3, w0, w3
 120:	lsl	x0, x20, x3
 124:	cmp	w3, #0x40
 128:	csel	x0, xzr, x0, pl  // pl = nfrst
 12c:	and	w1, w21, #0x80000000
 130:	and	w20, w0, #0x7fffff
 134:	orr	w20, w1, w20
 138:	add	w2, w2, #0x7f
 13c:	orr	w0, w20, w2, lsl #23
 140:	fmov	s0, w0
 144:	ldp	x19, x20, [sp, #16]
 148:	ldp	x21, x22, [sp, #32]
 14c:	ldr	x23, [sp, #48]
 150:	ldp	x29, x30, [sp], #64
 154:	ret

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	mov	x1, #0x4530000000000000    	// #4985484787499139072
   4:	orr	x1, x1, x0, lsr #32
   8:	mov	x2, #0x100000              	// #1048576
   c:	movk	x2, #0x4530, lsl #48
  10:	fmov	d1, x2
  14:	fmov	d0, x1
  18:	fsub	d1, d0, d1
  1c:	and	x0, x0, #0xffffffff
  20:	mov	x1, #0x4330000000000000    	// #4841369599423283200
  24:	orr	x0, x0, x1
  28:	fmov	d0, x0
  2c:	fadd	d0, d1, d0
  30:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	x0, 90 <__floatundisf+0x90>
   8:	clz	x4, x0
   c:	mov	w1, #0x40                  	// #64
  10:	sub	w2, w1, w4
  14:	sub	w3, w2, #0x1
  18:	cmp	w2, #0x18
  1c:	b.le	78 <__floatundisf+0x78>
  20:	cmp	w2, #0x19
  24:	b.eq	54 <__floatundisf+0x54>  // b.none
  28:	cmp	w2, #0x1a
  2c:	b.eq	58 <__floatundisf+0x58>  // b.none
  30:	sub	w1, w2, #0x1a
  34:	lsr	x1, x0, x1
  38:	add	w4, w4, #0x1a
  3c:	mov	x5, #0xffffffffffffffff    	// #-1
  40:	lsr	x4, x5, x4
  44:	tst	x4, x0
  48:	cset	x0, ne  // ne = any
  4c:	orr	x0, x1, x0
  50:	b	58 <__floatundisf+0x58>
  54:	lsl	x0, x0, #1
  58:	ubfx	x1, x0, #2, #1
  5c:	orr	x0, x1, x0
  60:	add	x1, x0, #0x1
  64:	lsr	x0, x1, #2
  68:	tbz	w1, #26, 84 <__floatundisf+0x84>
  6c:	lsr	x0, x1, #3
  70:	mov	w3, w2
  74:	b	84 <__floatundisf+0x84>
  78:	mov	w1, #0x18                  	// #24
  7c:	sub	w1, w1, w2
  80:	lsl	x0, x0, x1
  84:	add	w3, w3, #0x7f
  88:	bfi	w0, w3, #23, #9
  8c:	fmov	s0, w0
  90:	ret

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	movi	d0, #0x0
   4:	cbz	w0, 2c <__floatunsidf+0x2c>
   8:	clz	w2, w0
   c:	mov	w0, w0
  10:	add	w1, w2, #0x15
  14:	lsl	x0, x0, x1
  18:	eor	x0, x0, #0x10000000000000
  1c:	mov	w1, #0x41e                 	// #1054
  20:	sub	w1, w1, w2
  24:	add	x0, x0, x1, lsl #52
  28:	fmov	d0, x0
  2c:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	w0, 38 <__floatunsisf+0x38>
   8:	clz	w3, w0
   c:	mov	w1, #0x1f                  	// #31
  10:	sub	w1, w1, w3
  14:	cmp	w1, #0x17
  18:	b.gt	3c <__floatunsisf+0x3c>
  1c:	sub	w1, w3, #0x8
  20:	lsl	w0, w0, w1
  24:	eor	w2, w0, #0x800000
  28:	mov	w1, #0x9e                  	// #158
  2c:	sub	w1, w1, w3
  30:	add	w0, w2, w1, lsl #23
  34:	fmov	s0, w0
  38:	ret
  3c:	mov	w1, #0x8                   	// #8
  40:	sub	w1, w1, w3
  44:	lsr	w2, w0, w1
  48:	eor	w2, w2, #0x800000
  4c:	neg	w1, w1
  50:	lsl	w0, w0, w1
  54:	mov	w1, #0x80000000            	// #-2147483648
  58:	cmp	w0, w1
  5c:	b.ls	68 <__floatunsisf+0x68>  // b.plast
  60:	add	w2, w2, #0x1
  64:	b	28 <__floatunsisf+0x28>
  68:	add	w1, w2, #0x1
  6c:	and	w1, w1, #0xfffffffe
  70:	mov	w4, #0x80000000            	// #-2147483648
  74:	cmp	w0, w4
  78:	csel	w2, w1, w2, eq  // eq = none
  7c:	b	28 <__floatunsisf+0x28>

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x20, x0
  10:	orr	x0, x0, x1
  14:	movi	d0, #0x0
  18:	cbnz	x0, 28 <__floatuntidf+0x28>
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret
  28:	mov	x19, x1
  2c:	mov	x0, x20
  30:	bl	0 <__clzti2>
  34:	mov	w1, #0x80                  	// #128
  38:	sub	w2, w1, w0
  3c:	sub	w3, w2, #0x1
  40:	cmp	w2, #0x35
  44:	b.le	104 <__floatuntidf+0x104>
  48:	cmp	w2, #0x36
  4c:	b.eq	d8 <__floatuntidf+0xd8>  // b.none
  50:	cmp	w2, #0x37
  54:	b.eq	e0 <__floatuntidf+0xe0>  // b.none
  58:	sub	w5, w2, #0x37
  5c:	subs	w4, w2, #0x77
  60:	lsr	x8, x19, x4
  64:	lsl	x7, x19, #1
  68:	mov	w1, #0x3f                  	// #63
  6c:	sub	w6, w1, w5
  70:	lsl	x7, x7, x6
  74:	lsr	x6, x20, x5
  78:	orr	x6, x7, x6
  7c:	lsr	x5, x19, x5
  80:	csel	x6, x8, x6, pl  // pl = nfrst
  84:	csel	x5, xzr, x5, pl  // pl = nfrst
  88:	add	w7, w0, #0x37
  8c:	subs	w0, w0, #0x9
  90:	mov	x4, #0xffffffffffffffff    	// #-1
  94:	lsr	x9, x4, x0
  98:	sub	w1, w1, w7
  9c:	mov	x8, #0xfffffffffffffffe    	// #-2
  a0:	lsl	x8, x8, x1
  a4:	lsr	x1, x4, x7
  a8:	orr	x1, x8, x1
  ac:	lsr	x4, x4, x7
  b0:	csel	x1, x9, x1, pl  // pl = nfrst
  b4:	csel	x4, xzr, x4, pl  // pl = nfrst
  b8:	and	x0, x1, x20
  bc:	and	x19, x4, x19
  c0:	orr	x0, x0, x19
  c4:	cmp	x0, #0x0
  c8:	cset	x0, ne  // ne = any
  cc:	orr	x20, x6, x0
  d0:	mov	x19, x5
  d4:	b	e0 <__floatuntidf+0xe0>
  d8:	extr	x19, x19, x20, #63
  dc:	lsl	x20, x20, #1
  e0:	ubfx	x0, x20, #2, #1
  e4:	orr	x20, x0, x20
  e8:	adds	x20, x20, #0x1
  ec:	cinc	x19, x19, cs  // cs = hs, nlast
  f0:	extr	x0, x19, x20, #2
  f4:	tbz	x20, #55, 118 <__floatuntidf+0x118>
  f8:	extr	x0, x19, x20, #3
  fc:	mov	w3, w2
 100:	b	118 <__floatuntidf+0x118>
 104:	mov	w1, #0x35                  	// #53
 108:	sub	w1, w1, w2
 10c:	subs	w2, w1, #0x40
 110:	lsl	x0, x20, x1
 114:	csel	x0, xzr, x0, pl  // pl = nfrst
 118:	add	w1, w3, #0x3ff
 11c:	ubfx	x2, x0, #32, #20
 120:	orr	w1, w2, w1, lsl #20
 124:	bfi	x0, x1, #32, #32
 128:	fmov	d0, x0
 12c:	b	1c <__floatuntidf+0x1c>

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x1
  10:	orr	x1, x0, x1
  14:	movi	v0.2s, #0x0
  18:	cbnz	x1, 28 <__floatuntisf+0x28>
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret
  28:	mov	x20, x0
  2c:	mov	x1, x19
  30:	bl	0 <__clzti2>
  34:	mov	w1, w0
  38:	mov	w2, #0x80                  	// #128
  3c:	sub	w2, w2, w0
  40:	sub	w5, w2, #0x1
  44:	cmp	w2, #0x18
  48:	b.le	108 <__floatuntisf+0x108>
  4c:	cmp	w2, #0x19
  50:	b.eq	dc <__floatuntisf+0xdc>  // b.none
  54:	cmp	w2, #0x1a
  58:	b.eq	e4 <__floatuntisf+0xe4>  // b.none
  5c:	sub	w4, w2, #0x1a
  60:	subs	w3, w2, #0x5a
  64:	lsr	x8, x19, x3
  68:	lsl	x7, x19, #1
  6c:	mov	w0, #0x3f                  	// #63
  70:	sub	w6, w0, w4
  74:	lsl	x7, x7, x6
  78:	lsr	x6, x20, x4
  7c:	orr	x6, x7, x6
  80:	lsr	x4, x19, x4
  84:	csel	x6, x8, x6, pl  // pl = nfrst
  88:	csel	x4, xzr, x4, pl  // pl = nfrst
  8c:	add	w7, w1, #0x1a
  90:	subs	w1, w1, #0x26
  94:	mov	x3, #0xffffffffffffffff    	// #-1
  98:	lsr	x9, x3, x1
  9c:	sub	w0, w0, w7
  a0:	mov	x8, #0xfffffffffffffffe    	// #-2
  a4:	lsl	x8, x8, x0
  a8:	lsr	x0, x3, x7
  ac:	orr	x0, x8, x0
  b0:	lsr	x3, x3, x7
  b4:	csel	x0, x9, x0, pl  // pl = nfrst
  b8:	csel	x3, xzr, x3, pl  // pl = nfrst
  bc:	and	x0, x0, x20
  c0:	and	x19, x3, x19
  c4:	orr	x0, x0, x19
  c8:	cmp	x0, #0x0
  cc:	cset	x0, ne  // ne = any
  d0:	orr	x20, x6, x0
  d4:	mov	x19, x4
  d8:	b	e4 <__floatuntisf+0xe4>
  dc:	extr	x19, x19, x20, #63
  e0:	lsl	x20, x20, #1
  e4:	ubfx	x4, x20, #2, #1
  e8:	orr	x0, x4, x20
  ec:	adds	x3, x0, #0x1
  f0:	cinc	x19, x19, cs  // cs = hs, nlast
  f4:	extr	x0, x19, x3, #2
  f8:	tbz	w3, #26, 11c <__floatuntisf+0x11c>
  fc:	extr	x0, x19, x3, #3
 100:	mov	w5, w2
 104:	b	11c <__floatuntisf+0x11c>
 108:	mov	w0, #0x18                  	// #24
 10c:	sub	w2, w0, w2
 110:	lsl	x0, x20, x2
 114:	cmp	w2, #0x40
 118:	csel	x0, xzr, x0, pl  // pl = nfrst
 11c:	add	w1, w5, #0x7f
 120:	bfi	w0, w1, #23, #9
 124:	fmov	s0, w0
 128:	b	1c <__floatuntisf+0x1c>

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 20 <__lshrdi3+0x20>
   8:	mov	x0, #0x0                   	// #0
   c:	lsr	x2, x2, #32
  10:	sub	w1, w1, #0x20
  14:	lsr	w2, w2, w1
  18:	bfxil	x0, x2, #0, #32
  1c:	ret
  20:	cbz	w1, 1c <__lshrdi3+0x1c>
  24:	lsr	x3, x0, #32
  28:	lsr	w4, w3, w1
  2c:	mov	x0, #0x0                   	// #0
  30:	bfi	x0, x4, #32, #32
  34:	neg	w4, w1
  38:	lsl	w3, w3, w4
  3c:	lsr	w2, w2, w1
  40:	orr	w2, w3, w2
  44:	bfxil	x0, x2, #0, #32
  48:	b	1c <__lshrdi3+0x1c>

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbz	w2, #6, 18 <__lshrti3+0x18>
   4:	mov	x4, #0x0                   	// #0
   8:	sub	w2, w2, #0x40
   c:	lsr	x0, x1, x2
  10:	mov	x1, x4
  14:	ret
  18:	cbz	w2, 14 <__lshrti3+0x14>
  1c:	lsr	x4, x1, x2
  20:	neg	w3, w2
  24:	lsl	x3, x1, x3
  28:	lsr	x0, x0, x2
  2c:	orr	x0, x3, x0
  30:	b	10 <__lshrti3+0x10>

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	eor	x3, x1, x1, asr #63
  10:	asr	x19, x0, #63
  14:	eor	x0, x0, x0, asr #63
  18:	add	x2, sp, #0x28
  1c:	sub	x1, x3, x1, asr #63
  20:	sub	x0, x0, x19
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x0, [sp, #40]
  2c:	eor	x0, x19, x0
  30:	sub	x0, x0, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #48
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w20, w0
  10:	mov	w19, w1
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	asr	x4, x3, #63
  10:	eor	x2, x2, x4
  14:	eor	x3, x3, x4
  18:	asr	x19, x1, #63
  1c:	eor	x0, x0, x19
  20:	eor	x1, x1, x19
  24:	subs	x2, x2, x4
  28:	sbc	x3, x3, x4
  2c:	subs	x0, x0, x19
  30:	add	x4, sp, #0x20
  34:	sbc	x1, x1, x19
  38:	bl	0 <__udivmodti4>
  3c:	ldr	x0, [sp, #32]
  40:	eor	x0, x0, x19
  44:	ldr	x1, [sp, #40]
  48:	eor	x1, x1, x19
  4c:	subs	x0, x0, x19
  50:	sbc	x1, x1, x19
  54:	ldr	x19, [sp, #16]
  58:	ldp	x29, x30, [sp], #48
  5c:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmov	d19, d0
   4:	fmov	d18, d1
   8:	fmul	d6, d0, d2
   c:	fmul	d7, d1, d3
  10:	fmul	d17, d0, d3
  14:	fmul	d16, d2, d1
  18:	fsub	d4, d6, d7
  1c:	fadd	d5, d17, d16
  20:	fmov	d0, d4
  24:	fmov	d1, d5
  28:	fcmp	d5, d5
  2c:	fccmp	d4, d4, #0x0, vs
  30:	b.vs	38 <__muldc3+0x38>
  34:	ret
  38:	fabs	d5, d19
  3c:	adrp	x0, 0 <__muldc3>
  40:	ldr	d4, [x0]
  44:	fcmp	d5, d4
  48:	cset	w0, gt
  4c:	b.gt	68 <__muldc3+0x68>
  50:	fabs	d5, d18
  54:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  58:	fmov	d4, x1
  5c:	mov	w1, #0x0                   	// #0
  60:	fcmp	d5, d4
  64:	b.le	a8 <__muldc3+0xa8>
  68:	scvtf	d4, w0
  6c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  70:	fmov	d5, x0
  74:	bif	v19.8b, v4.8b, v5.8b
  78:	fabs	d20, d18
  7c:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  80:	fmov	d4, x0
  84:	fcmp	d20, d4
  88:	cset	w0, gt
  8c:	scvtf	d4, w0
  90:	bif	v18.8b, v4.8b, v5.8b
  94:	fcmp	d2, d2
  98:	b.vs	138 <__muldc3+0x138>
  9c:	mov	w1, #0x1                   	// #1
  a0:	fcmp	d3, d3
  a4:	b.vs	144 <__muldc3+0x144>
  a8:	fabs	d4, d2
  ac:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  b0:	fmov	d5, x0
  b4:	fcmp	d4, d5
  b8:	b.gt	c8 <__muldc3+0xc8>
  bc:	fabs	d20, d3
  c0:	fcmp	d20, d5
  c4:	b.le	180 <__muldc3+0x180>
  c8:	fcmp	d19, d19
  cc:	b.vs	158 <__muldc3+0x158>
  d0:	fcmp	d18, d18
  d4:	b.vs	16c <__muldc3+0x16c>
  d8:	adrp	x0, 0 <__muldc3>
  dc:	ldr	d5, [x0]
  e0:	fcmp	d4, d5
  e4:	cset	w0, gt
  e8:	scvtf	d0, w0
  ec:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  f0:	fmov	d1, x0
  f4:	bif	v2.8b, v0.8b, v1.8b
  f8:	fabs	d0, d3
  fc:	fcmp	d0, d5
 100:	cset	w0, gt
 104:	scvtf	d0, w0
 108:	bif	v3.8b, v0.8b, v1.8b
 10c:	fmul	d0, d19, d2
 110:	fmul	d1, d18, d3
 114:	fsub	d0, d0, d1
 118:	adrp	x0, 0 <__muldc3>
 11c:	ldr	d4, [x0]
 120:	fmul	d1, d19, d3
 124:	fmul	d2, d18, d2
 128:	fadd	d1, d1, d2
 12c:	fmul	d0, d0, d4
 130:	fmul	d1, d1, d4
 134:	b	34 <__muldc3+0x34>
 138:	movi	d4, #0x0
 13c:	bif	v2.8b, v4.8b, v5.8b
 140:	b	9c <__muldc3+0x9c>
 144:	movi	d4, #0x0
 148:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 14c:	fmov	d5, x0
 150:	bif	v3.8b, v4.8b, v5.8b
 154:	b	a8 <__muldc3+0xa8>
 158:	movi	d0, #0x0
 15c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 160:	fmov	d1, x0
 164:	bif	v19.8b, v0.8b, v1.8b
 168:	b	d0 <__muldc3+0xd0>
 16c:	movi	d0, #0x0
 170:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 174:	fmov	d1, x0
 178:	bif	v18.8b, v0.8b, v1.8b
 17c:	b	d8 <__muldc3+0xd8>
 180:	cbnz	w1, 10c <__muldc3+0x10c>
 184:	fabs	d6, d6
 188:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 18c:	fmov	d4, x0
 190:	fcmp	d6, d4
 194:	b.gt	1bc <__muldc3+0x1bc>
 198:	fabs	d7, d7
 19c:	fcmp	d7, d4
 1a0:	b.gt	1bc <__muldc3+0x1bc>
 1a4:	fabs	d17, d17
 1a8:	fcmp	d17, d4
 1ac:	b.gt	1bc <__muldc3+0x1bc>
 1b0:	fabs	d16, d16
 1b4:	fcmp	d16, d4
 1b8:	b.le	34 <__muldc3+0x34>
 1bc:	fcmp	d19, d19
 1c0:	b.vs	1f0 <__muldc3+0x1f0>
 1c4:	fcmp	d18, d18
 1c8:	b.vs	204 <__muldc3+0x204>
 1cc:	fcmp	d2, d2
 1d0:	b.vs	218 <__muldc3+0x218>
 1d4:	fcmp	d3, d3
 1d8:	b.vc	10c <__muldc3+0x10c>
 1dc:	movi	d0, #0x0
 1e0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1e4:	fmov	d1, x0
 1e8:	bif	v3.8b, v0.8b, v1.8b
 1ec:	b	10c <__muldc3+0x10c>
 1f0:	movi	d0, #0x0
 1f4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1f8:	fmov	d1, x0
 1fc:	bif	v19.8b, v0.8b, v1.8b
 200:	b	1c4 <__muldc3+0x1c4>
 204:	movi	d0, #0x0
 208:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 20c:	fmov	d1, x0
 210:	bif	v18.8b, v0.8b, v1.8b
 214:	b	1cc <__muldc3+0x1cc>
 218:	movi	d0, #0x0
 21c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 220:	fmov	d1, x0
 224:	bif	v2.8b, v0.8b, v1.8b
 228:	b	1d4 <__muldc3+0x1d4>

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	fmov	x0, d0
   4:	ubfx	x3, x0, #52, #11
   8:	fmov	x1, d1
   c:	ubfx	x8, x1, #52, #11
  10:	eor	x4, x0, x1
  14:	and	x4, x4, #0x8000000000000000
  18:	and	x5, x0, #0xfffffffffffff
  1c:	and	x2, x1, #0xfffffffffffff
  20:	sub	w6, w3, #0x1
  24:	cmp	w6, #0x7fd
  28:	b.hi	38 <__muldf3+0x38>  // b.pmore
  2c:	sub	w6, w8, #0x1
  30:	cmp	w6, #0x7fd
  34:	b.ls	10c <__muldf3+0x10c>  // b.plast
  38:	and	x6, x0, #0x7fffffffffffffff
  3c:	mov	x7, #0x7ff0000000000000    	// #9218868437227405312
  40:	cmp	x6, x7
  44:	b.hi	bc <__muldf3+0xbc>  // b.pmore
  48:	and	x7, x1, #0x7fffffffffffffff
  4c:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  50:	cmp	x7, x9
  54:	b.hi	c8 <__muldf3+0xc8>  // b.pmore
  58:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  5c:	cmp	x6, x9
  60:	b.eq	d4 <__muldf3+0xd4>  // b.none
  64:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  68:	cmp	x7, x9
  6c:	b.eq	f0 <__muldf3+0xf0>  // b.none
  70:	fmov	d0, x4
  74:	cbz	x6, c4 <__muldf3+0xc4>
  78:	cbz	x7, c4 <__muldf3+0xc4>
  7c:	mov	w9, #0x0                   	// #0
  80:	tst	x0, #0x7ff0000000000000
  84:	b.ne	9c <__muldf3+0x9c>  // b.any
  88:	clz	x0, x5
  8c:	sub	w0, w0, #0xb
  90:	lsl	x5, x5, x0
  94:	mov	w9, #0x1                   	// #1
  98:	sub	w9, w9, w0
  9c:	tst	x1, #0x7ff0000000000000
  a0:	b.ne	110 <__muldf3+0x110>  // b.any
  a4:	clz	x0, x2
  a8:	sub	w0, w0, #0xb
  ac:	lsl	x2, x2, x0
  b0:	sub	w9, w9, w0
  b4:	add	w9, w9, #0x1
  b8:	b	110 <__muldf3+0x110>
  bc:	orr	x0, x0, #0x8000000000000
  c0:	fmov	d0, x0
  c4:	ret
  c8:	orr	x0, x1, #0x8000000000000
  cc:	fmov	d0, x0
  d0:	b	c4 <__muldf3+0xc4>
  d4:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  d8:	fmov	d1, x0
  dc:	orr	x0, x4, x9
  e0:	fmov	d0, x0
  e4:	cmp	x7, #0x0
  e8:	fcsel	d0, d1, d0, eq  // eq = none
  ec:	b	c4 <__muldf3+0xc4>
  f0:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  f4:	fmov	d1, x0
  f8:	orr	x0, x4, x9
  fc:	fmov	d0, x0
 100:	cmp	x6, #0x0
 104:	fcsel	d0, d1, d0, eq  // eq = none
 108:	b	c4 <__muldf3+0xc4>
 10c:	mov	w9, #0x0                   	// #0
 110:	and	x6, x5, #0xffffffff
 114:	lsl	w7, w2, #11
 118:	mul	x1, x6, x7
 11c:	ubfx	x2, x2, #21, #32
 120:	orr	x2, x2, #0x80000000
 124:	mul	x6, x6, x2
 128:	lsr	x5, x5, #32
 12c:	orr	x0, x5, #0x100000
 130:	mul	x7, x0, x7
 134:	and	x5, x7, #0xffffffff
 138:	add	x5, x5, x1, lsr #32
 13c:	add	x5, x5, w6, uxtw
 140:	and	x1, x1, #0xffffffff
 144:	add	x1, x1, x5, lsl #32
 148:	lsr	x7, x7, #32
 14c:	madd	x2, x2, x0, x7
 150:	lsr	x5, x5, #32
 154:	add	x5, x5, x6, lsr #32
 158:	add	x0, x2, x5
 15c:	add	w3, w3, w8
 160:	add	w3, w3, w9
 164:	tbz	x0, #52, 19c <__muldf3+0x19c>
 168:	sub	w3, w3, #0x3fe
 16c:	cmp	w3, #0x7fe
 170:	b.gt	1ac <__muldf3+0x1ac>
 174:	cmp	w3, #0x0
 178:	b.le	1b8 <__muldf3+0x1b8>
 17c:	bfi	x0, x3, #52, #12
 180:	orr	x0, x4, x0
 184:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
 188:	cmp	x1, x2
 18c:	b.ls	1f4 <__muldf3+0x1f4>  // b.plast
 190:	add	x0, x0, #0x1
 194:	fmov	d0, x0
 198:	b	c4 <__muldf3+0xc4>
 19c:	sub	w3, w3, #0x3ff
 1a0:	extr	x0, x0, x1, #63
 1a4:	lsl	x1, x1, #1
 1a8:	b	16c <__muldf3+0x16c>
 1ac:	orr	x0, x4, #0x7ff0000000000000
 1b0:	fmov	d0, x0
 1b4:	b	c4 <__muldf3+0xc4>
 1b8:	mov	w2, #0x1                   	// #1
 1bc:	sub	w2, w2, w3
 1c0:	fmov	d0, x4
 1c4:	cmp	w2, #0x3f
 1c8:	b.hi	c4 <__muldf3+0xc4>  // b.pmore
 1cc:	add	w3, w3, #0x3f
 1d0:	lsl	x5, x0, x3
 1d4:	lsr	x6, x1, x2
 1d8:	orr	x5, x5, x6
 1dc:	lsl	x3, x1, x3
 1e0:	cmp	x3, #0x0
 1e4:	cset	x1, ne  // ne = any
 1e8:	orr	x1, x5, x1
 1ec:	lsr	x0, x0, x2
 1f0:	b	180 <__muldf3+0x180>
 1f4:	add	x2, x0, #0x1
 1f8:	and	x2, x2, #0xfffffffffffffffe
 1fc:	mov	x3, #0x8000000000000000    	// #-9223372036854775808
 200:	cmp	x1, x3
 204:	csel	x0, x2, x0, eq  // eq = none
 208:	b	194 <__muldf3+0x194>

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	mov	x4, x0
   4:	and	w2, w0, #0xffff
   8:	and	w3, w1, #0xffff
   c:	mul	w0, w2, w3
  10:	lsr	w5, w4, #16
  14:	mul	w3, w3, w5
  18:	add	w3, w3, w0, lsr #16
  1c:	lsr	w7, w1, #16
  20:	mul	w2, w2, w7
  24:	add	w2, w2, w3, uxth
  28:	lsl	w6, w2, #16
  2c:	add	w6, w6, w0, uxth
  30:	mov	x0, #0x0                   	// #0
  34:	bfxil	x0, x6, #0, #32
  38:	mul	w5, w5, w7
  3c:	add	w3, w5, w3, lsr #16
  40:	add	w2, w3, w2, lsr #16
  44:	bfi	x0, x2, #32, #32
  48:	asr	x2, x1, #32
  4c:	asr	x3, x0, #32
  50:	madd	w2, w4, w2, w3
  54:	asr	x4, x4, #32
  58:	madd	w1, w1, w4, w2
  5c:	bfi	x0, x1, #32, #32
  60:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x3, x0
   4:	str	wzr, [x2]
   8:	mul	x0, x0, x1
   c:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  10:	cmp	x3, x4
  14:	b.eq	70 <__mulodi4+0x70>  // b.none
  18:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  1c:	cmp	x1, x4
  20:	b.eq	84 <__mulodi4+0x84>  // b.none
  24:	asr	x5, x3, #63
  28:	eor	x3, x3, x5
  2c:	sub	x3, x3, x5
  30:	asr	x4, x1, #63
  34:	eor	x1, x1, x4
  38:	sub	x6, x1, x4
  3c:	cmp	x3, #0x1
  40:	ccmp	x6, #0x1, #0x4, gt
  44:	b.le	6c <__mulodi4+0x6c>
  48:	cmp	x5, x4
  4c:	b.eq	98 <__mulodi4+0x98>  // b.none
  50:	sub	x1, x4, x1
  54:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  58:	sdiv	x1, x4, x1
  5c:	cmp	x1, x3
  60:	b.ge	6c <__mulodi4+0x6c>  // b.tcont
  64:	mov	w1, #0x1                   	// #1
  68:	str	w1, [x2]
  6c:	ret
  70:	cmp	x1, #0x1
  74:	b.ls	6c <__mulodi4+0x6c>  // b.plast
  78:	mov	w1, #0x1                   	// #1
  7c:	str	w1, [x2]
  80:	b	6c <__mulodi4+0x6c>
  84:	cmp	x3, #0x1
  88:	b.ls	6c <__mulodi4+0x6c>  // b.plast
  8c:	mov	w1, #0x1                   	// #1
  90:	str	w1, [x2]
  94:	b	6c <__mulodi4+0x6c>
  98:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  9c:	sdiv	x6, x1, x6
  a0:	cmp	x6, x3
  a4:	b.ge	6c <__mulodi4+0x6c>  // b.tcont
  a8:	mov	w1, #0x1                   	// #1
  ac:	str	w1, [x2]
  b0:	b	6c <__mulodi4+0x6c>

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w3, w0
   4:	str	wzr, [x2]
   8:	mul	w0, w0, w1
   c:	mov	w4, #0x80000000            	// #-2147483648
  10:	cmp	w3, w4
  14:	b.eq	70 <__mulosi4+0x70>  // b.none
  18:	mov	w4, #0x80000000            	// #-2147483648
  1c:	cmp	w1, w4
  20:	b.eq	84 <__mulosi4+0x84>  // b.none
  24:	asr	w5, w3, #31
  28:	eor	w3, w3, w5
  2c:	sub	w3, w3, w5
  30:	asr	w4, w1, #31
  34:	eor	w1, w1, w4
  38:	sub	w6, w1, w4
  3c:	cmp	w3, #0x1
  40:	ccmp	w6, #0x1, #0x4, gt
  44:	b.le	6c <__mulosi4+0x6c>
  48:	cmp	w5, w4
  4c:	b.eq	98 <__mulosi4+0x98>  // b.none
  50:	sub	w1, w4, w1
  54:	mov	w4, #0x80000000            	// #-2147483648
  58:	sdiv	w1, w4, w1
  5c:	cmp	w1, w3
  60:	b.ge	6c <__mulosi4+0x6c>  // b.tcont
  64:	mov	w1, #0x1                   	// #1
  68:	str	w1, [x2]
  6c:	ret
  70:	cmp	w1, #0x1
  74:	b.ls	6c <__mulosi4+0x6c>  // b.plast
  78:	mov	w1, #0x1                   	// #1
  7c:	str	w1, [x2]
  80:	b	6c <__mulosi4+0x6c>
  84:	cmp	w3, #0x1
  88:	b.ls	6c <__mulosi4+0x6c>  // b.plast
  8c:	mov	w1, #0x1                   	// #1
  90:	str	w1, [x2]
  94:	b	6c <__mulosi4+0x6c>
  98:	mov	w1, #0x7fffffff            	// #2147483647
  9c:	sdiv	w6, w1, w6
  a0:	cmp	w6, w3
  a4:	b.ge	6c <__mulosi4+0x6c>  // b.tcont
  a8:	mov	w1, #0x1                   	// #1
  ac:	str	w1, [x2]
  b0:	b	6c <__mulosi4+0x6c>

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	mov	x21, x4
  14:	str	wzr, [x4]
  18:	mul	x22, x0, x2
  1c:	umulh	x20, x0, x2
  20:	madd	x20, x1, x2, x20
  24:	madd	x20, x0, x3, x20
  28:	cbz	x0, c8 <__muloti4+0xc8>
  2c:	cbz	x2, f4 <__muloti4+0xf4>
  30:	str	x23, [sp, #48]
  34:	asr	x5, x1, #63
  38:	eor	x0, x0, x5
  3c:	eor	x1, x1, x5
  40:	subs	x19, x0, x5
  44:	sbc	x23, x1, x5
  48:	asr	x4, x3, #63
  4c:	eor	x0, x2, x4
  50:	eor	x1, x3, x4
  54:	subs	x2, x0, x4
  58:	sbc	x3, x1, x4
  5c:	mov	w6, #0x1                   	// #1
  60:	cmp	x23, #0x0
  64:	b.le	120 <__muloti4+0x120>
  68:	mov	w6, #0x0                   	// #0
  6c:	mov	w7, #0x1                   	// #1
  70:	cmp	x3, #0x0
  74:	b.le	130 <__muloti4+0x130>
  78:	mov	w7, #0x0                   	// #0
  7c:	orr	w6, w6, w7
  80:	tst	w6, #0xff
  84:	b.ne	174 <__muloti4+0x174>  // b.any
  88:	cmp	x5, x4
  8c:	b.eq	140 <__muloti4+0x140>  // b.none
  90:	subs	x2, x4, x0
  94:	sbc	x3, x4, x1
  98:	mov	x0, #0x0                   	// #0
  9c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  a0:	bl	0 <__divti3>
  a4:	cmp	x23, x1
  a8:	b.gt	b8 <__muloti4+0xb8>
  ac:	b.ne	1a0 <__muloti4+0x1a0>  // b.any
  b0:	cmp	x19, x0
  b4:	b.ls	1a8 <__muloti4+0x1a8>  // b.plast
  b8:	mov	w0, #0x1                   	// #1
  bc:	str	w0, [x21]
  c0:	ldr	x23, [sp, #48]
  c4:	b	178 <__muloti4+0x178>
  c8:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  cc:	cmp	x1, x4
  d0:	b.ne	2c <__muloti4+0x2c>  // b.any
  d4:	cbz	x3, e4 <__muloti4+0xe4>
  d8:	mov	w0, #0x1                   	// #1
  dc:	str	w0, [x21]
  e0:	b	178 <__muloti4+0x178>
  e4:	cbnz	x3, 178 <__muloti4+0x178>
  e8:	cmp	x2, #0x1
  ec:	b.hi	d8 <__muloti4+0xd8>  // b.pmore
  f0:	b	178 <__muloti4+0x178>
  f4:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  f8:	cmp	x3, x4
  fc:	b.ne	30 <__muloti4+0x30>  // b.any
 100:	cbz	x1, 110 <__muloti4+0x110>
 104:	mov	w0, #0x1                   	// #1
 108:	str	w0, [x21]
 10c:	b	178 <__muloti4+0x178>
 110:	cbnz	x1, 178 <__muloti4+0x178>
 114:	cmp	x0, #0x1
 118:	b.hi	104 <__muloti4+0x104>  // b.pmore
 11c:	b	178 <__muloti4+0x178>
 120:	cbnz	x23, 6c <__muloti4+0x6c>
 124:	cmp	x19, #0x1
 128:	b.ls	6c <__muloti4+0x6c>  // b.plast
 12c:	b	68 <__muloti4+0x68>
 130:	cbnz	x3, 7c <__muloti4+0x7c>
 134:	cmp	x2, #0x1
 138:	b.ls	7c <__muloti4+0x7c>  // b.plast
 13c:	b	78 <__muloti4+0x78>
 140:	b.ne	90 <__muloti4+0x90>  // b.any
 144:	mov	x0, #0xffffffffffffffff    	// #-1
 148:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 14c:	bl	0 <__divti3>
 150:	cmp	x23, x1
 154:	b.gt	164 <__muloti4+0x164>
 158:	b.ne	190 <__muloti4+0x190>  // b.any
 15c:	cmp	x19, x0
 160:	b.ls	198 <__muloti4+0x198>  // b.plast
 164:	mov	w0, #0x1                   	// #1
 168:	str	w0, [x21]
 16c:	ldr	x23, [sp, #48]
 170:	b	178 <__muloti4+0x178>
 174:	ldr	x23, [sp, #48]
 178:	mov	x0, x22
 17c:	mov	x1, x20
 180:	ldp	x19, x20, [sp, #16]
 184:	ldp	x21, x22, [sp, #32]
 188:	ldp	x29, x30, [sp], #64
 18c:	ret
 190:	ldr	x23, [sp, #48]
 194:	b	178 <__muloti4+0x178>
 198:	ldr	x23, [sp, #48]
 19c:	b	178 <__muloti4+0x178>
 1a0:	ldr	x23, [sp, #48]
 1a4:	b	178 <__muloti4+0x178>
 1a8:	ldr	x23, [sp, #48]
 1ac:	b	178 <__muloti4+0x178>

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmov	s19, s0
   4:	fmov	s18, s1
   8:	fmul	s6, s0, s2
   c:	fmul	s7, s1, s3
  10:	fmul	s17, s0, s3
  14:	fmul	s16, s2, s1
  18:	fsub	s4, s6, s7
  1c:	fadd	s5, s17, s16
  20:	fmov	s0, s4
  24:	fmov	s1, s5
  28:	fcmp	s5, s5
  2c:	fccmp	s4, s4, #0x0, vs
  30:	b.vs	38 <__mulsc3+0x38>
  34:	ret
  38:	fabs	s5, s19
  3c:	adrp	x0, 0 <__mulsc3>
  40:	ldr	s4, [x0]
  44:	fcmp	s5, s4
  48:	cset	w0, gt
  4c:	b.gt	68 <__mulsc3+0x68>
  50:	fabs	s5, s18
  54:	mov	w1, #0x7f7fffff            	// #2139095039
  58:	fmov	s4, w1
  5c:	mov	w1, #0x0                   	// #0
  60:	fcmp	s5, s4
  64:	b.le	a4 <__mulsc3+0xa4>
  68:	scvtf	s4, w0
  6c:	movi	v5.2s, #0x80, lsl #24
  70:	bif	v19.8b, v4.8b, v5.8b
  74:	fabs	s20, s18
  78:	mov	w0, #0x7f7fffff            	// #2139095039
  7c:	fmov	s4, w0
  80:	fcmp	s20, s4
  84:	cset	w0, gt
  88:	scvtf	s4, w0
  8c:	bif	v18.8b, v4.8b, v5.8b
  90:	fcmp	s2, s2
  94:	b.vs	130 <__mulsc3+0x130>
  98:	mov	w1, #0x1                   	// #1
  9c:	fcmp	s3, s3
  a0:	b.vs	13c <__mulsc3+0x13c>
  a4:	fabs	s4, s2
  a8:	mov	w0, #0x7f7fffff            	// #2139095039
  ac:	fmov	s5, w0
  b0:	fcmp	s4, s5
  b4:	b.gt	c4 <__mulsc3+0xc4>
  b8:	fabs	s20, s3
  bc:	fcmp	s20, s5
  c0:	b.le	16c <__mulsc3+0x16c>
  c4:	fcmp	s19, s19
  c8:	b.vs	14c <__mulsc3+0x14c>
  cc:	fcmp	s18, s18
  d0:	b.vs	15c <__mulsc3+0x15c>
  d4:	adrp	x0, 0 <__mulsc3>
  d8:	ldr	s5, [x0]
  dc:	fcmp	s4, s5
  e0:	cset	w0, gt
  e4:	scvtf	s0, w0
  e8:	movi	v1.2s, #0x80, lsl #24
  ec:	bif	v2.8b, v0.8b, v1.8b
  f0:	fabs	s0, s3
  f4:	fcmp	s0, s5
  f8:	cset	w0, gt
  fc:	scvtf	s0, w0
 100:	bif	v3.8b, v0.8b, v1.8b
 104:	fmul	s0, s19, s2
 108:	fmul	s1, s18, s3
 10c:	fsub	s0, s0, s1
 110:	adrp	x0, 0 <__mulsc3>
 114:	ldr	s4, [x0]
 118:	fmul	s1, s19, s3
 11c:	fmul	s2, s18, s2
 120:	fadd	s1, s1, s2
 124:	fmul	s0, s0, s4
 128:	fmul	s1, s1, s4
 12c:	b	34 <__mulsc3+0x34>
 130:	movi	v4.2s, #0x0
 134:	bif	v2.8b, v4.8b, v5.8b
 138:	b	98 <__mulsc3+0x98>
 13c:	movi	v4.2s, #0x0
 140:	movi	v5.2s, #0x80, lsl #24
 144:	bif	v3.8b, v4.8b, v5.8b
 148:	b	a4 <__mulsc3+0xa4>
 14c:	movi	v0.2s, #0x0
 150:	movi	v1.2s, #0x80, lsl #24
 154:	bif	v19.8b, v0.8b, v1.8b
 158:	b	cc <__mulsc3+0xcc>
 15c:	movi	v0.2s, #0x0
 160:	movi	v1.2s, #0x80, lsl #24
 164:	bif	v18.8b, v0.8b, v1.8b
 168:	b	d4 <__mulsc3+0xd4>
 16c:	cbnz	w1, 104 <__mulsc3+0x104>
 170:	fabs	s6, s6
 174:	mov	w0, #0x7f7fffff            	// #2139095039
 178:	fmov	s4, w0
 17c:	fcmp	s6, s4
 180:	b.gt	1a8 <__mulsc3+0x1a8>
 184:	fabs	s7, s7
 188:	fcmp	s7, s4
 18c:	b.gt	1a8 <__mulsc3+0x1a8>
 190:	fabs	s17, s17
 194:	fcmp	s17, s4
 198:	b.gt	1a8 <__mulsc3+0x1a8>
 19c:	fabs	s16, s16
 1a0:	fcmp	s16, s4
 1a4:	b.le	34 <__mulsc3+0x34>
 1a8:	fcmp	s19, s19
 1ac:	b.vs	1d8 <__mulsc3+0x1d8>
 1b0:	fcmp	s18, s18
 1b4:	b.vs	1e8 <__mulsc3+0x1e8>
 1b8:	fcmp	s2, s2
 1bc:	b.vs	1f8 <__mulsc3+0x1f8>
 1c0:	fcmp	s3, s3
 1c4:	b.vc	104 <__mulsc3+0x104>
 1c8:	movi	v0.2s, #0x0
 1cc:	movi	v1.2s, #0x80, lsl #24
 1d0:	bif	v3.8b, v0.8b, v1.8b
 1d4:	b	104 <__mulsc3+0x104>
 1d8:	movi	v0.2s, #0x0
 1dc:	movi	v1.2s, #0x80, lsl #24
 1e0:	bif	v19.8b, v0.8b, v1.8b
 1e4:	b	1b0 <__mulsc3+0x1b0>
 1e8:	movi	v0.2s, #0x0
 1ec:	movi	v1.2s, #0x80, lsl #24
 1f0:	bif	v18.8b, v0.8b, v1.8b
 1f4:	b	1b8 <__mulsc3+0x1b8>
 1f8:	movi	v0.2s, #0x0
 1fc:	movi	v1.2s, #0x80, lsl #24
 200:	bif	v2.8b, v0.8b, v1.8b
 204:	b	1c0 <__mulsc3+0x1c0>

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	fmov	w0, s0
   4:	ubfx	x2, x0, #23, #8
   8:	fmov	w1, s1
   c:	ubfx	x7, x1, #23, #8
  10:	eor	w3, w0, w1
  14:	and	w6, w3, #0x80000000
  18:	and	w5, w0, #0x7fffff
  1c:	and	w3, w1, #0x7fffff
  20:	sub	w4, w2, #0x1
  24:	cmp	w4, #0xfd
  28:	b.hi	38 <__mulsf3+0x38>  // b.pmore
  2c:	sub	w4, w7, #0x1
  30:	cmp	w4, #0xfd
  34:	b.ls	10c <__mulsf3+0x10c>  // b.plast
  38:	and	w4, w0, #0x7fffffff
  3c:	mov	w8, #0x7f800000            	// #2139095040
  40:	cmp	w4, w8
  44:	b.hi	bc <__mulsf3+0xbc>  // b.pmore
  48:	and	w8, w1, #0x7fffffff
  4c:	mov	w9, #0x7f800000            	// #2139095040
  50:	cmp	w8, w9
  54:	b.hi	c8 <__mulsf3+0xc8>  // b.pmore
  58:	mov	w9, #0x7f800000            	// #2139095040
  5c:	cmp	w4, w9
  60:	b.eq	d4 <__mulsf3+0xd4>  // b.none
  64:	mov	w9, #0x7f800000            	// #2139095040
  68:	cmp	w8, w9
  6c:	b.eq	f0 <__mulsf3+0xf0>  // b.none
  70:	fmov	s0, w6
  74:	cbz	w4, c4 <__mulsf3+0xc4>
  78:	cbz	w8, c4 <__mulsf3+0xc4>
  7c:	mov	w8, #0x0                   	// #0
  80:	tst	w0, #0x7f800000
  84:	b.ne	9c <__mulsf3+0x9c>  // b.any
  88:	clz	w0, w5
  8c:	sub	w0, w0, #0x8
  90:	lsl	w5, w5, w0
  94:	mov	w8, #0x1                   	// #1
  98:	sub	w8, w8, w0
  9c:	tst	w1, #0x7f800000
  a0:	b.ne	110 <__mulsf3+0x110>  // b.any
  a4:	clz	w0, w3
  a8:	sub	w0, w0, #0x8
  ac:	lsl	w3, w3, w0
  b0:	sub	w8, w8, w0
  b4:	add	w8, w8, #0x1
  b8:	b	110 <__mulsf3+0x110>
  bc:	orr	w0, w0, #0x400000
  c0:	fmov	s0, w0
  c4:	ret
  c8:	orr	w0, w1, #0x400000
  cc:	fmov	s0, w0
  d0:	b	c4 <__mulsf3+0xc4>
  d4:	mov	w0, #0x7fc00000            	// #2143289344
  d8:	fmov	s1, w0
  dc:	orr	w0, w6, w9
  e0:	fmov	s0, w0
  e4:	cmp	w8, #0x0
  e8:	fcsel	s0, s1, s0, eq  // eq = none
  ec:	b	c4 <__mulsf3+0xc4>
  f0:	mov	w0, #0x7fc00000            	// #2143289344
  f4:	fmov	s1, w0
  f8:	orr	w0, w6, w9
  fc:	fmov	s0, w0
 100:	cmp	w4, #0x0
 104:	fcsel	s0, s1, s0, eq  // eq = none
 108:	b	c4 <__mulsf3+0xc4>
 10c:	mov	w8, #0x0                   	// #0
 110:	lsl	w3, w3, #8
 114:	orr	x3, x3, #0x80000000
 118:	orr	w5, w5, #0x800000
 11c:	mul	x3, x3, x5
 120:	lsr	x1, x3, #32
 124:	mov	w0, w1
 128:	mov	w4, w3
 12c:	add	w2, w2, w7
 130:	add	w2, w2, w8
 134:	tbz	w1, #23, 16c <__mulsf3+0x16c>
 138:	sub	w2, w2, #0x7e
 13c:	cmp	w2, #0xfe
 140:	b.gt	17c <__mulsf3+0x17c>
 144:	cmp	w2, #0x0
 148:	b.le	188 <__mulsf3+0x188>
 14c:	bfi	w0, w2, #23, #9
 150:	orr	w0, w6, w0
 154:	mov	w1, #0x80000000            	// #-2147483648
 158:	cmp	w4, w1
 15c:	b.ls	1c4 <__mulsf3+0x1c4>  // b.plast
 160:	add	w0, w0, #0x1
 164:	fmov	s0, w0
 168:	b	c4 <__mulsf3+0xc4>
 16c:	sub	w2, w2, #0x7f
 170:	extr	w0, w1, w3, #31
 174:	lsl	w4, w3, #1
 178:	b	13c <__mulsf3+0x13c>
 17c:	orr	w0, w6, #0x7f800000
 180:	fmov	s0, w0
 184:	b	c4 <__mulsf3+0xc4>
 188:	mov	w1, #0x1                   	// #1
 18c:	sub	w1, w1, w2
 190:	fmov	s0, w6
 194:	cmp	w1, #0x1f
 198:	b.hi	c4 <__mulsf3+0xc4>  // b.pmore
 19c:	add	w2, w2, #0x1f
 1a0:	lsl	w3, w0, w2
 1a4:	lsr	w5, w4, w1
 1a8:	orr	w3, w3, w5
 1ac:	lsl	w2, w4, w2
 1b0:	cmp	w2, #0x0
 1b4:	cset	w4, ne  // ne = any
 1b8:	orr	w4, w3, w4
 1bc:	lsr	w0, w0, w1
 1c0:	b	150 <__mulsf3+0x150>
 1c4:	add	w1, w0, #0x1
 1c8:	and	w1, w1, #0xfffffffe
 1cc:	mov	w2, #0x80000000            	// #-2147483648
 1d0:	cmp	w4, w2
 1d4:	csel	w0, w1, w0, eq  // eq = none
 1d8:	b	164 <__mulsf3+0x164>

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x4, x0, #0xffffffff
   4:	and	x5, x2, #0xffffffff
   8:	mul	x7, x4, x5
   c:	lsr	x6, x0, #32
  10:	mul	x5, x5, x6
  14:	add	x5, x5, x7, lsr #32
  18:	lsr	x8, x2, #32
  1c:	mul	x4, x4, x8
  20:	add	x4, x4, w5, uxtw
  24:	lsl	x9, x4, #32
  28:	mul	x6, x6, x8
  2c:	add	x5, x6, x5, lsr #32
  30:	add	x4, x5, x4, lsr #32
  34:	madd	x4, x0, x3, x4
  38:	add	x0, x9, w7, uxtw
  3c:	madd	x1, x2, x1, x4
  40:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #32]
   c:	ldr	x4, [sp, #32]
  10:	ldr	x0, [sp, #40]
  14:	str	q1, [sp, #32]
  18:	ldr	x5, [sp, #32]
  1c:	ldr	x2, [sp, #40]
  20:	mov	x14, x4
  24:	ubfx	x7, x0, #48, #15
  28:	mov	x16, x5
  2c:	mov	x12, x2
  30:	ubfx	x8, x2, #48, #15
  34:	eor	x6, x0, x2
  38:	mov	x2, #0x0                   	// #0
  3c:	and	x3, x6, #0x8000000000000000
  40:	mov	x13, x4
  44:	and	x6, x0, #0xffffffffffff
  48:	mov	x11, x5
  4c:	and	x17, x12, #0xffffffffffff
  50:	sub	w10, w7, #0x1
  54:	mov	w9, #0x7ffd                	// #32765
  58:	cmp	w10, w9
  5c:	b.hi	6c <__multf3+0x6c>  // b.pmore
  60:	sub	w4, w8, #0x1
  64:	cmp	w4, w9
  68:	b.ls	1fc <__multf3+0x1fc>  // b.plast
  6c:	and	x4, x0, #0x7fffffffffffffff
  70:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
  74:	cmp	x4, x5
  78:	b.hi	174 <__multf3+0x174>  // b.pmore
  7c:	cmp	x4, x5
  80:	b.eq	170 <__multf3+0x170>  // b.none
  84:	and	x5, x12, #0x7fffffffffffffff
  88:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
  8c:	cmp	x5, x1
  90:	b.hi	18c <__multf3+0x18c>  // b.pmore
  94:	b.eq	188 <__multf3+0x188>  // b.none
  98:	cbz	x14, 19c <__multf3+0x19c>
  9c:	cbz	x16, 1c0 <__multf3+0x1c0>
  a0:	orr	x4, x14, x4
  a4:	cbz	x4, 1e4 <__multf3+0x1e4>
  a8:	orr	x9, x16, x5
  ac:	cbz	x9, 1f0 <__multf3+0x1f0>
  b0:	stp	x19, x20, [sp, #16]
  b4:	mov	w9, #0x0                   	// #0
  b8:	tst	x0, #0x7fff000000000000
  bc:	b.ne	110 <__multf3+0x110>  // b.any
  c0:	cmp	x6, #0x0
  c4:	csel	x0, x6, x14, ne  // ne = any
  c8:	mov	x1, #0x40                  	// #64
  cc:	csel	x1, x1, xzr, eq  // eq = none
  d0:	clz	x0, x0
  d4:	add	w1, w1, w0
  d8:	sub	w4, w1, #0xf
  dc:	subs	w1, w1, #0x4f
  e0:	lsl	x9, x14, x1
  e4:	lsr	x0, x14, #1
  e8:	mov	w5, #0x3f                  	// #63
  ec:	sub	w5, w5, w4
  f0:	lsr	x0, x0, x5
  f4:	lsl	x6, x6, x4
  f8:	orr	x6, x0, x6
  fc:	lsl	x0, x14, x4
 100:	csel	x6, x9, x6, pl  // pl = nfrst
 104:	csel	x13, xzr, x0, pl  // pl = nfrst
 108:	mov	w1, #0x1                   	// #1
 10c:	sub	w9, w1, w4
 110:	tst	x12, #0x7fff000000000000
 114:	b.ne	204 <__multf3+0x204>  // b.any
 118:	cmp	x17, #0x0
 11c:	csel	x0, x17, x16, ne  // ne = any
 120:	mov	x4, #0x40                  	// #64
 124:	csel	x4, x4, xzr, eq  // eq = none
 128:	clz	x0, x0
 12c:	add	w4, w4, w0
 130:	sub	w5, w4, #0xf
 134:	subs	w4, w4, #0x4f
 138:	lsl	x10, x16, x4
 13c:	lsr	x1, x16, #1
 140:	mov	w0, #0x3f                  	// #63
 144:	sub	w0, w0, w5
 148:	lsr	x1, x1, x0
 14c:	lsl	x0, x17, x5
 150:	orr	x0, x1, x0
 154:	lsl	x1, x16, x5
 158:	csel	x0, x10, x0, pl  // pl = nfrst
 15c:	csel	x11, xzr, x1, pl  // pl = nfrst
 160:	mov	x17, x0
 164:	sub	w1, w9, w5
 168:	add	w9, w1, #0x1
 16c:	b	204 <__multf3+0x204>
 170:	cbz	x14, 84 <__multf3+0x84>
 174:	orr	x3, x0, #0x800000000000
 178:	fmov	d0, x14
 17c:	fmov	v0.d[1], x3
 180:	ldp	x29, x30, [sp], #48
 184:	ret
 188:	cbz	x16, 98 <__multf3+0x98>
 18c:	orr	x1, x12, #0x800000000000
 190:	fmov	d0, x16
 194:	fmov	v0.d[1], x1
 198:	b	180 <__multf3+0x180>
 19c:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
 1a0:	cmp	x4, x1
 1a4:	b.ne	9c <__multf3+0x9c>  // b.any
 1a8:	orr	x5, x16, x5
 1ac:	cbz	x5, 4b4 <__multf3+0x4b4>
 1b0:	orr	x1, x3, #0x7fff000000000000
 1b4:	fmov	d0, x2
 1b8:	fmov	v0.d[1], x1
 1bc:	b	180 <__multf3+0x180>
 1c0:	mov	x1, #0x7fff000000000000    	// #9223090561878065152
 1c4:	cmp	x5, x1
 1c8:	b.ne	a0 <__multf3+0xa0>  // b.any
 1cc:	orr	x4, x14, x4
 1d0:	cbz	x4, 4c4 <__multf3+0x4c4>
 1d4:	orr	x1, x3, #0x7fff000000000000
 1d8:	fmov	d0, x2
 1dc:	fmov	v0.d[1], x1
 1e0:	b	180 <__multf3+0x180>
 1e4:	fmov	d0, x2
 1e8:	fmov	v0.d[1], x3
 1ec:	b	180 <__multf3+0x180>
 1f0:	fmov	d0, x2
 1f4:	fmov	v0.d[1], x3
 1f8:	b	180 <__multf3+0x180>
 1fc:	stp	x19, x20, [sp, #16]
 200:	mov	w9, #0x0                   	// #0
 204:	orr	x6, x6, #0x1000000000000
 208:	extr	x0, x17, x11, #49
 20c:	orr	x0, x0, #0x8000000000000000
 210:	lsr	x1, x6, #32
 214:	lsr	x14, x0, #32
 218:	and	x10, x0, #0xffffffff
 21c:	ubfx	x4, x11, #17, #32
 220:	lsl	w11, w11, #15
 224:	and	x15, x6, #0xffffffff
 228:	lsr	x0, x13, #32
 22c:	and	x13, x13, #0xffffffff
 230:	mul	x6, x11, x0
 234:	mul	x19, x4, x13
 238:	adds	x19, x6, x19
 23c:	cset	x5, cs  // cs = hs, nlast
 240:	mul	x6, x4, x0
 244:	mul	x12, x10, x13
 248:	adds	x6, x6, x12
 24c:	cset	x17, cs  // cs = hs, nlast
 250:	mul	x12, x11, x15
 254:	adds	x6, x6, x12
 258:	cinc	x17, x17, cs  // cs = hs, nlast
 25c:	mul	x12, x1, x11
 260:	mul	x16, x14, x13
 264:	adds	x12, x12, x16
 268:	cset	x20, cs  // cs = hs, nlast
 26c:	mul	x16, x4, x15
 270:	mul	x18, x10, x0
 274:	adds	x16, x16, x18
 278:	cset	x30, cs  // cs = hs, nlast
 27c:	adds	x18, x12, x16
 280:	adc	x30, x20, x30
 284:	extr	x5, x5, x19, #32
 288:	adds	x5, x5, x6
 28c:	cset	x16, cs  // cs = hs, nlast
 290:	lsl	x6, x18, #32
 294:	adds	x5, x5, x6
 298:	cinc	x16, x16, cs  // cs = hs, nlast
 29c:	lsl	x6, x19, #32
 2a0:	mul	x11, x11, x13
 2a4:	adds	x13, x6, x11
 2a8:	cinc	x5, x5, cs  // cs = hs, nlast
 2ac:	mov	x12, x13
 2b0:	mov	x11, x5
 2b4:	mul	x6, x1, x14
 2b8:	mul	x0, x14, x0
 2bc:	mul	x4, x1, x4
 2c0:	mul	x19, x10, x15
 2c4:	adds	x4, x4, x19
 2c8:	cset	x19, cs  // cs = hs, nlast
 2cc:	adds	x0, x0, x4
 2d0:	adc	x4, x6, x19
 2d4:	extr	x18, x30, x18, #32
 2d8:	adds	x17, x17, x18
 2dc:	cset	x6, cs  // cs = hs, nlast
 2e0:	adds	x0, x0, x17
 2e4:	adc	x4, x4, x6
 2e8:	mul	x1, x1, x10
 2ec:	mul	x6, x14, x15
 2f0:	adds	x1, x1, x6
 2f4:	cset	x6, cs  // cs = hs, nlast
 2f8:	extr	x6, x6, x1, #32
 2fc:	lsl	x1, x1, #32
 300:	adds	x1, x1, x16
 304:	cinc	x6, x6, cs  // cs = hs, nlast
 308:	adds	x0, x0, x1
 30c:	adc	x4, x4, x6
 310:	mov	x6, x0
 314:	mov	x10, x4
 318:	add	w1, w7, w8
 31c:	add	w1, w1, w9
 320:	tbz	x4, #48, 384 <__multf3+0x384>
 324:	sub	w1, w1, #0x3, lsl #12
 328:	sub	w1, w1, #0xffe
 32c:	mov	w0, #0x7ffe                	// #32766
 330:	cmp	w1, w0
 334:	b.gt	3a0 <__multf3+0x3a0>
 338:	cmp	w1, #0x0
 33c:	b.le	3b4 <__multf3+0x3b4>
 340:	bfi	x10, x1, #48, #16
 344:	orr	x4, x2, x6
 348:	orr	x5, x3, x10
 34c:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
 350:	cmp	x11, x2
 354:	b.hi	360 <__multf3+0x360>  // b.pmore
 358:	b.ne	490 <__multf3+0x490>  // b.any
 35c:	cbz	x12, 490 <__multf3+0x490>
 360:	adds	x0, x4, #0x1
 364:	mov	x3, x0
 368:	cinc	x2, x5, cs  // cs = hs, nlast
 36c:	mov	x4, x3
 370:	mov	x5, x2
 374:	fmov	d0, x4
 378:	fmov	v0.d[1], x5
 37c:	ldp	x19, x20, [sp, #16]
 380:	b	180 <__multf3+0x180>
 384:	sub	w1, w1, #0x3, lsl #12
 388:	sub	w1, w1, #0xfff
 38c:	extr	x6, x0, x5, #63
 390:	extr	x10, x4, x0, #63
 394:	lsl	x12, x13, #1
 398:	extr	x11, x5, x13, #63
 39c:	b	32c <__multf3+0x32c>
 3a0:	orr	x1, x3, #0x7fff000000000000
 3a4:	fmov	d0, x2
 3a8:	fmov	v0.d[1], x1
 3ac:	ldp	x19, x20, [sp, #16]
 3b0:	b	180 <__multf3+0x180>
 3b4:	mov	w0, #0x1                   	// #1
 3b8:	sub	w0, w0, w1
 3bc:	cmp	w0, #0x7f
 3c0:	b.ls	3d4 <__multf3+0x3d4>  // b.plast
 3c4:	fmov	d0, x2
 3c8:	fmov	v0.d[1], x3
 3cc:	ldp	x19, x20, [sp, #16]
 3d0:	b	180 <__multf3+0x180>
 3d4:	add	w4, w1, #0x7f
 3d8:	adds	w1, w1, #0x3f
 3dc:	lsl	x13, x6, x1
 3e0:	lsr	x5, x6, #1
 3e4:	mov	w7, #0x3f                  	// #63
 3e8:	sub	w15, w7, w4
 3ec:	lsr	x5, x5, x15
 3f0:	lsl	x8, x10, x4
 3f4:	orr	x8, x5, x8
 3f8:	lsl	x9, x6, x4
 3fc:	csel	x8, x13, x8, pl  // pl = nfrst
 400:	csel	x9, xzr, x9, pl  // pl = nfrst
 404:	subs	w5, w0, #0x40
 408:	lsr	x16, x11, x5
 40c:	lsl	x14, x11, #1
 410:	sub	w7, w7, w0
 414:	lsl	x14, x14, x7
 418:	lsr	x13, x12, x0
 41c:	orr	x13, x14, x13
 420:	lsr	x14, x11, x0
 424:	csel	x13, x16, x13, pl  // pl = nfrst
 428:	csel	x14, xzr, x14, pl  // pl = nfrst
 42c:	orr	x9, x9, x13
 430:	lsl	x16, x12, x1
 434:	lsr	x13, x12, #1
 438:	lsr	x13, x13, x15
 43c:	lsl	x11, x11, x4
 440:	orr	x11, x13, x11
 444:	lsl	x12, x12, x4
 448:	cmp	w1, #0x0
 44c:	csel	x11, x16, x11, ge  // ge = tcont
 450:	csel	x12, xzr, x12, ge  // ge = tcont
 454:	orr	x12, x12, x11
 458:	cmp	x12, #0x0
 45c:	cset	x12, ne  // ne = any
 460:	orr	x12, x9, x12
 464:	orr	x11, x8, x14
 468:	lsr	x4, x10, x5
 46c:	lsl	x1, x10, #1
 470:	lsl	x7, x1, x7
 474:	lsr	x6, x6, x0
 478:	orr	x6, x7, x6
 47c:	lsr	x10, x10, x0
 480:	cmp	w5, #0x0
 484:	csel	x6, x4, x6, ge  // ge = tcont
 488:	csel	x10, xzr, x10, ge  // ge = tcont
 48c:	b	344 <__multf3+0x344>
 490:	cbnz	x12, 374 <__multf3+0x374>
 494:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
 498:	cmp	x11, x2
 49c:	b.ne	374 <__multf3+0x374>  // b.any
 4a0:	adds	x3, x4, #0x1
 4a4:	cinc	x2, x5, cs  // cs = hs, nlast
 4a8:	and	x4, x3, #0xfffffffffffffffe
 4ac:	mov	x5, x2
 4b0:	b	374 <__multf3+0x374>
 4b4:	adrp	x0, 0 <__multf3>
 4b8:	add	x0, x0, #0x0
 4bc:	ldr	q0, [x0]
 4c0:	b	180 <__multf3+0x180>
 4c4:	adrp	x0, 0 <__multf3>
 4c8:	add	x0, x0, #0x0
 4cc:	ldr	q0, [x0]
 4d0:	b	180 <__multf3+0x180>

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
   c:	cmp	x0, x2
  10:	b.eq	68 <__mulvdi3+0x68>  // b.none
  14:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  18:	cmp	x1, x2
  1c:	b.eq	94 <__mulvdi3+0x94>  // b.none
  20:	asr	x4, x0, #63
  24:	eor	x3, x0, x4
  28:	sub	x3, x3, x4
  2c:	asr	x2, x1, #63
  30:	eor	x5, x1, x2
  34:	sub	x6, x5, x2
  38:	cmp	x3, #0x1
  3c:	ccmp	x6, #0x1, #0x4, gt
  40:	b.le	bc <__mulvdi3+0xbc>
  44:	cmp	x4, x2
  48:	b.eq	c4 <__mulvdi3+0xc4>  // b.none
  4c:	sub	x2, x2, x5
  50:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  54:	sdiv	x2, x4, x2
  58:	cmp	x2, x3
  5c:	b.lt	ec <__mulvdi3+0xec>  // b.tstop
  60:	mul	x0, x0, x1
  64:	b	74 <__mulvdi3+0x74>
  68:	cmp	x1, #0x1
  6c:	b.hi	7c <__mulvdi3+0x7c>  // b.pmore
  70:	lsl	x0, x1, #63
  74:	ldp	x29, x30, [sp], #16
  78:	ret
  7c:	adrp	x2, 0 <__mulvdi3>
  80:	add	x2, x2, #0x0
  84:	mov	w1, #0x1a                  	// #26
  88:	adrp	x0, 0 <__mulvdi3>
  8c:	add	x0, x0, #0x0
  90:	bl	0 <__compilerrt_abort_impl>
  94:	cmp	x0, #0x1
  98:	b.hi	a4 <__mulvdi3+0xa4>  // b.pmore
  9c:	lsl	x0, x0, #63
  a0:	b	74 <__mulvdi3+0x74>
  a4:	adrp	x2, 0 <__mulvdi3>
  a8:	add	x2, x2, #0x0
  ac:	mov	w1, #0x1f                  	// #31
  b0:	adrp	x0, 0 <__mulvdi3>
  b4:	add	x0, x0, #0x0
  b8:	bl	0 <__compilerrt_abort_impl>
  bc:	mul	x0, x0, x1
  c0:	b	74 <__mulvdi3+0x74>
  c4:	mov	x2, #0x7fffffffffffffff    	// #9223372036854775807
  c8:	sdiv	x6, x2, x6
  cc:	cmp	x6, x3
  d0:	b.ge	60 <__mulvdi3+0x60>  // b.tcont
  d4:	adrp	x2, 0 <__mulvdi3>
  d8:	add	x2, x2, #0x0
  dc:	mov	w1, #0x29                  	// #41
  e0:	adrp	x0, 0 <__mulvdi3>
  e4:	add	x0, x0, #0x0
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x2, 0 <__mulvdi3>
  f0:	add	x2, x2, #0x0
  f4:	mov	w1, #0x2c                  	// #44
  f8:	adrp	x0, 0 <__mulvdi3>
  fc:	add	x0, x0, #0x0
 100:	bl	0 <__compilerrt_abort_impl>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w2, #0x80000000            	// #-2147483648
   c:	cmp	w0, w2
  10:	b.eq	68 <__mulvsi3+0x68>  // b.none
  14:	mov	w2, #0x80000000            	// #-2147483648
  18:	cmp	w1, w2
  1c:	b.eq	94 <__mulvsi3+0x94>  // b.none
  20:	asr	w4, w0, #31
  24:	eor	w3, w0, w4
  28:	sub	w3, w3, w4
  2c:	asr	w2, w1, #31
  30:	eor	w5, w1, w2
  34:	sub	w6, w5, w2
  38:	cmp	w3, #0x1
  3c:	ccmp	w6, #0x1, #0x4, gt
  40:	b.le	bc <__mulvsi3+0xbc>
  44:	cmp	w4, w2
  48:	b.eq	c4 <__mulvsi3+0xc4>  // b.none
  4c:	sub	w2, w2, w5
  50:	mov	w4, #0x80000000            	// #-2147483648
  54:	sdiv	w2, w4, w2
  58:	cmp	w2, w3
  5c:	b.lt	ec <__mulvsi3+0xec>  // b.tstop
  60:	mul	w0, w0, w1
  64:	b	74 <__mulvsi3+0x74>
  68:	cmp	w1, #0x1
  6c:	b.hi	7c <__mulvsi3+0x7c>  // b.pmore
  70:	lsl	w0, w1, #31
  74:	ldp	x29, x30, [sp], #16
  78:	ret
  7c:	adrp	x2, 0 <__mulvsi3>
  80:	add	x2, x2, #0x0
  84:	mov	w1, #0x1a                  	// #26
  88:	adrp	x0, 0 <__mulvsi3>
  8c:	add	x0, x0, #0x0
  90:	bl	0 <__compilerrt_abort_impl>
  94:	cmp	w0, #0x1
  98:	b.hi	a4 <__mulvsi3+0xa4>  // b.pmore
  9c:	lsl	w0, w0, #31
  a0:	b	74 <__mulvsi3+0x74>
  a4:	adrp	x2, 0 <__mulvsi3>
  a8:	add	x2, x2, #0x0
  ac:	mov	w1, #0x1f                  	// #31
  b0:	adrp	x0, 0 <__mulvsi3>
  b4:	add	x0, x0, #0x0
  b8:	bl	0 <__compilerrt_abort_impl>
  bc:	mul	w0, w0, w1
  c0:	b	74 <__mulvsi3+0x74>
  c4:	mov	w2, #0x7fffffff            	// #2147483647
  c8:	sdiv	w6, w2, w6
  cc:	cmp	w6, w3
  d0:	b.ge	60 <__mulvsi3+0x60>  // b.tcont
  d4:	adrp	x2, 0 <__mulvsi3>
  d8:	add	x2, x2, #0x0
  dc:	mov	w1, #0x29                  	// #41
  e0:	adrp	x0, 0 <__mulvsi3>
  e4:	add	x0, x0, #0x0
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x2, 0 <__mulvsi3>
  f0:	add	x2, x2, #0x0
  f4:	mov	w1, #0x2c                  	// #44
  f8:	adrp	x0, 0 <__mulvsi3>
  fc:	add	x0, x0, #0x0
 100:	bl	0 <__compilerrt_abort_impl>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	mov	x19, x0
  14:	mov	x22, x1
  18:	mov	x20, x2
  1c:	mov	x21, x3
  20:	cbz	x0, c0 <__mulvti3+0xc0>
  24:	cbz	x20, 110 <__mulvti3+0x110>
  28:	stp	x23, x24, [sp, #48]
  2c:	asr	x1, x22, #63
  30:	eor	x24, x19, x1
  34:	eor	x23, x22, x1
  38:	subs	x24, x24, x1
  3c:	sbc	x23, x23, x1
  40:	asr	x0, x21, #63
  44:	eor	x4, x20, x0
  48:	eor	x7, x21, x0
  4c:	subs	x2, x4, x0
  50:	sbc	x3, x7, x0
  54:	mov	w5, #0x1                   	// #1
  58:	cmp	x23, #0x0
  5c:	b.le	154 <__mulvti3+0x154>
  60:	mov	w5, #0x0                   	// #0
  64:	mov	w6, #0x1                   	// #1
  68:	cmp	x3, #0x0
  6c:	b.le	164 <__mulvti3+0x164>
  70:	mov	w6, #0x0                   	// #0
  74:	orr	w5, w5, w6
  78:	tst	w5, #0xff
  7c:	b.ne	174 <__mulvti3+0x174>  // b.any
  80:	cmp	x1, x0
  84:	b.eq	18c <__mulvti3+0x18c>  // b.none
  88:	subs	x2, x0, x4
  8c:	sbc	x3, x0, x7
  90:	mov	x0, #0x0                   	// #0
  94:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  98:	bl	0 <__divti3>
  9c:	cmp	x23, x1
  a0:	b.gt	1d0 <__mulvti3+0x1d0>
  a4:	b.eq	1c8 <__mulvti3+0x1c8>  // b.none
  a8:	umulh	x1, x19, x20
  ac:	madd	x1, x22, x20, x1
  b0:	mul	x0, x19, x20
  b4:	madd	x1, x19, x21, x1
  b8:	ldp	x23, x24, [sp, #48]
  bc:	b	dc <__mulvti3+0xdc>
  c0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  c4:	cmp	x1, x0
  c8:	b.ne	24 <__mulvti3+0x24>  // b.any
  cc:	cbnz	x3, f4 <__mulvti3+0xf4>
  d0:	cbz	x3, ec <__mulvti3+0xec>
  d4:	lsl	x1, x20, #63
  d8:	mov	x0, #0x0                   	// #0
  dc:	ldp	x19, x20, [sp, #16]
  e0:	ldp	x21, x22, [sp, #32]
  e4:	ldp	x29, x30, [sp], #64
  e8:	ret
  ec:	cmp	x2, #0x1
  f0:	b.ls	d4 <__mulvti3+0xd4>  // b.plast
  f4:	stp	x23, x24, [sp, #48]
  f8:	adrp	x2, 0 <__mulvti3>
  fc:	add	x2, x2, #0x0
 100:	mov	w1, #0x1c                  	// #28
 104:	adrp	x0, 0 <__mulvti3>
 108:	add	x0, x0, #0x0
 10c:	bl	0 <__compilerrt_abort_impl>
 110:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 114:	cmp	x21, x0
 118:	b.ne	28 <__mulvti3+0x28>  // b.any
 11c:	cbnz	x22, 138 <__mulvti3+0x138>
 120:	cbz	x22, 130 <__mulvti3+0x130>
 124:	lsl	x1, x19, #63
 128:	mov	x0, #0x0                   	// #0
 12c:	b	dc <__mulvti3+0xdc>
 130:	cmp	x19, #0x1
 134:	b.ls	124 <__mulvti3+0x124>  // b.plast
 138:	stp	x23, x24, [sp, #48]
 13c:	adrp	x2, 0 <__mulvti3>
 140:	add	x2, x2, #0x0
 144:	mov	w1, #0x21                  	// #33
 148:	adrp	x0, 0 <__mulvti3>
 14c:	add	x0, x0, #0x0
 150:	bl	0 <__compilerrt_abort_impl>
 154:	cbnz	x23, 64 <__mulvti3+0x64>
 158:	cmp	x24, #0x1
 15c:	b.ls	64 <__mulvti3+0x64>  // b.plast
 160:	b	60 <__mulvti3+0x60>
 164:	cbnz	x3, 74 <__mulvti3+0x74>
 168:	cmp	x2, #0x1
 16c:	b.ls	74 <__mulvti3+0x74>  // b.plast
 170:	b	70 <__mulvti3+0x70>
 174:	umulh	x1, x19, x20
 178:	madd	x1, x22, x20, x1
 17c:	mul	x0, x19, x20
 180:	madd	x1, x19, x21, x1
 184:	ldp	x23, x24, [sp, #48]
 188:	b	dc <__mulvti3+0xdc>
 18c:	b.ne	88 <__mulvti3+0x88>  // b.any
 190:	mov	x0, #0xffffffffffffffff    	// #-1
 194:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 198:	bl	0 <__divti3>
 19c:	cmp	x23, x1
 1a0:	b.gt	1b0 <__mulvti3+0x1b0>
 1a4:	b.ne	a8 <__mulvti3+0xa8>  // b.any
 1a8:	cmp	x24, x0
 1ac:	b.ls	a8 <__mulvti3+0xa8>  // b.plast
 1b0:	adrp	x2, 0 <__mulvti3>
 1b4:	add	x2, x2, #0x0
 1b8:	mov	w1, #0x2b                  	// #43
 1bc:	adrp	x0, 0 <__mulvti3>
 1c0:	add	x0, x0, #0x0
 1c4:	bl	0 <__compilerrt_abort_impl>
 1c8:	cmp	x24, x0
 1cc:	b.ls	a8 <__mulvti3+0xa8>  // b.plast
 1d0:	adrp	x2, 0 <__mulvti3>
 1d4:	add	x2, x2, #0x0
 1d8:	mov	w1, #0x2e                  	// #46
 1dc:	adrp	x0, 0 <__mulvti3>
 1e0:	add	x0, x0, #0x0
 1e4:	bl	0 <__compilerrt_abort_impl>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	fmov	x0, d0
   4:	eor	x0, x0, #0x8000000000000000
   8:	fmov	d0, x0
   c:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	fmov	w0, s0
   4:	eor	w0, w0, #0x80000000
   8:	fmov	s0, w0
   c:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngc	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.eq	14 <__negvdi2+0x14>  // b.none
   c:	neg	x0, x0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x2, 0 <__negvdi2>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	adrp	x0, 0 <__negvdi2>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.eq	14 <__negvsi2+0x14>  // b.none
   c:	neg	w0, w0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x2, 0 <__negvsi2>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	adrp	x0, 0 <__negvsi2>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	cbz	x0, 10 <__negvti2+0x10>
   4:	negs	x0, x0
   8:	ngc	x1, x1
   c:	ret
  10:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  14:	cmp	x1, x2
  18:	b.ne	4 <__negvti2+0x4>  // b.any
  1c:	stp	x29, x30, [sp, #-16]!
  20:	mov	x29, sp
  24:	adrp	x2, 0 <__negvti2>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x18                  	// #24
  30:	adrp	x0, 0 <__negvti2>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	lsr	x1, x0, #32
   c:	eor	w0, w1, w0
  10:	bl	0 <__paritysi2>
  14:	ldp	x29, x30, [sp], #16
  18:	ret

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w0, w0, w0, lsr #16
   4:	eor	w0, w0, w0, lsr #8
   8:	eor	w0, w0, w0, lsr #4
   c:	and	w0, w0, #0xf
  10:	mov	w1, #0x6996                	// #27030
  14:	asr	w0, w1, w0
  18:	and	w0, w0, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	eor	x0, x1, x0
   c:	bl	0 <__paritydi2>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	and	x1, x1, #0x5555555555555555
   8:	sub	x1, x0, x1
   c:	lsr	x0, x1, #2
  10:	and	x0, x0, #0x3333333333333333
  14:	and	x1, x1, #0x3333333333333333
  18:	add	x0, x0, x1
  1c:	add	x0, x0, x0, lsr #4
  20:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  24:	lsr	x1, x0, #32
  28:	add	w0, w1, w0
  2c:	add	w0, w0, w0, lsr #16
  30:	add	w0, w0, w0, lsr #8
  34:	and	w0, w0, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w1, w0, #1
   4:	and	w1, w1, #0x55555555
   8:	sub	w1, w0, w1
   c:	lsr	w0, w1, #2
  10:	and	w0, w0, #0x33333333
  14:	and	w1, w1, #0x33333333
  18:	add	w0, w0, w1
  1c:	add	w0, w0, w0, lsr #4
  20:	and	w0, w0, #0xf0f0f0f
  24:	add	w0, w0, w0, lsr #16
  28:	add	w0, w0, w0, lsr #8
  2c:	and	w0, w0, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	mov	x3, x0
   4:	extr	x0, x1, x0, #1
   8:	lsr	x2, x1, #1
   c:	and	x0, x0, #0x5555555555555555
  10:	and	x2, x2, #0x5555555555555555
  14:	subs	x3, x3, x0
  18:	sbc	x1, x1, x2
  1c:	extr	x2, x1, x3, #2
  20:	lsr	x0, x1, #2
  24:	and	x2, x2, #0x3333333333333333
  28:	and	x0, x0, #0x3333333333333333
  2c:	and	x3, x3, #0x3333333333333333
  30:	and	x1, x1, #0x3333333333333333
  34:	adds	x2, x2, x3
  38:	adc	x0, x0, x1
  3c:	extr	x1, x0, x2, #4
  40:	lsr	x3, x0, #4
  44:	adds	x2, x1, x2
  48:	adc	x0, x0, x3
  4c:	and	x2, x2, #0xf0f0f0f0f0f0f0f
  50:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  54:	add	x0, x0, x2
  58:	lsr	x1, x0, #32
  5c:	add	w0, w1, w0
  60:	add	w0, w0, w0, lsr #16
  64:	add	w0, w0, w0, lsr #8
  68:	and	w0, w0, #0xff
  6c:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	fmov	d1, d0
   4:	mov	w1, w0
   8:	fmov	d0, #1.000000000000000000e+00
   c:	b	28 <__powidf2+0x28>
  10:	fmul	d0, d0, d1
  14:	add	w2, w1, w1, lsr #31
  18:	asr	w1, w2, #1
  1c:	cmp	wzr, w2, asr #1
  20:	b.eq	30 <__powidf2+0x30>  // b.none
  24:	fmul	d1, d1, d1
  28:	tbz	w1, #0, 14 <__powidf2+0x14>
  2c:	b	10 <__powidf2+0x10>
  30:	tbnz	w0, #31, 38 <__powidf2+0x38>
  34:	ret
  38:	fmov	d1, #1.000000000000000000e+00
  3c:	fdiv	d0, d1, d0
  40:	b	34 <__powidf2+0x34>

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	fmov	s1, s0
   4:	mov	w1, w0
   8:	fmov	s0, #1.000000000000000000e+00
   c:	b	28 <__powisf2+0x28>
  10:	fmul	s0, s0, s1
  14:	add	w2, w1, w1, lsr #31
  18:	asr	w1, w2, #1
  1c:	cmp	wzr, w2, asr #1
  20:	b.eq	30 <__powisf2+0x30>  // b.none
  24:	fmul	s1, s1, s1
  28:	tbz	w1, #0, 14 <__powisf2+0x14>
  2c:	b	10 <__powisf2+0x10>
  30:	tbnz	w0, #31, 38 <__powisf2+0x38>
  34:	ret
  38:	fmov	s1, #1.000000000000000000e+00
  3c:	fdiv	s0, s1, s0
  40:	b	34 <__powisf2+0x34>

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
   c:	fmov	d2, x0
  10:	eor	v1.8b, v1.8b, v2.8b
  14:	bl	0 <__adddf3>
  18:	ldp	x29, x30, [sp], #16
  1c:	ret

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	movi	v2.2s, #0x80, lsl #24
   c:	eor	v1.8b, v1.8b, v2.8b
  10:	bl	0 <__addsf3>
  14:	ldp	x29, x30, [sp], #16
  18:	ret

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x2, x0
   c:	sub	x0, x0, x1
  10:	tbnz	x1, #63, 3c <__subvdi3+0x3c>
  14:	cmp	x2, x0
  18:	b.lt	24 <__subvdi3+0x24>  // b.tstop
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	adrp	x2, 0 <__subvdi3>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	adrp	x0, 0 <__subvdi3>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>
  3c:	cmp	x2, x0
  40:	b.lt	1c <__subvdi3+0x1c>  // b.tstop
  44:	adrp	x2, 0 <__subvdi3>
  48:	add	x2, x2, #0x0
  4c:	mov	w1, #0x1a                  	// #26
  50:	adrp	x0, 0 <__subvdi3>
  54:	add	x0, x0, #0x0
  58:	bl	0 <__compilerrt_abort_impl>

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w2, w0
   c:	sub	w0, w0, w1
  10:	tbnz	w1, #31, 3c <__subvsi3+0x3c>
  14:	cmp	w2, w0
  18:	b.lt	24 <__subvsi3+0x24>  // b.tstop
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	adrp	x2, 0 <__subvsi3>
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	adrp	x0, 0 <__subvsi3>
  34:	add	x0, x0, #0x0
  38:	bl	0 <__compilerrt_abort_impl>
  3c:	cmp	w2, w0
  40:	b.lt	1c <__subvsi3+0x1c>  // b.tstop
  44:	adrp	x2, 0 <__subvsi3>
  48:	add	x2, x2, #0x0
  4c:	mov	w1, #0x1a                  	// #26
  50:	adrp	x0, 0 <__subvsi3>
  54:	add	x0, x0, #0x0
  58:	bl	0 <__compilerrt_abort_impl>

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x5, x0
   c:	mov	x4, x1
  10:	subs	x0, x0, x2
  14:	sbc	x1, x1, x3
  18:	tbnz	x3, #63, 50 <__subvti3+0x50>
  1c:	cmp	x1, x4
  20:	b.gt	38 <__subvti3+0x38>
  24:	b.eq	30 <__subvti3+0x30>  // b.none
  28:	ldp	x29, x30, [sp], #16
  2c:	ret
  30:	cmp	x0, x5
  34:	b.ls	28 <__subvti3+0x28>  // b.plast
  38:	adrp	x2, 0 <__subvti3>
  3c:	add	x2, x2, #0x0
  40:	mov	w1, #0x19                  	// #25
  44:	adrp	x0, 0 <__subvti3>
  48:	add	x0, x0, #0x0
  4c:	bl	0 <__compilerrt_abort_impl>
  50:	cmp	x1, x4
  54:	b.gt	28 <__subvti3+0x28>
  58:	b.ne	64 <__subvti3+0x64>  // b.any
  5c:	cmp	x0, x5
  60:	b.hi	28 <__subvti3+0x28>  // b.pmore
  64:	adrp	x2, 0 <__subvti3>
  68:	add	x2, x2, #0x0
  6c:	mov	w1, #0x1c                  	// #28
  70:	adrp	x0, 0 <__subvti3>
  74:	add	x0, x0, #0x0
  78:	bl	0 <__compilerrt_abort_impl>

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q1, [sp, #16]
   c:	ldr	x3, [sp, #16]
  10:	ldr	x2, [sp, #24]
  14:	eor	x1, x2, #0x8000000000000000
  18:	fmov	d1, x3
  1c:	fmov	v1.d[1], x1
  20:	bl	0 <__addtf3>
  24:	ldp	x29, x30, [sp], #32
  28:	ret

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	fmov	x1, d0
   4:	and	x2, x1, #0x7fffffffffffffff
   8:	mov	x3, #0xc0f0000000000000    	// #-4544132024016830464
   c:	add	x3, x2, x3
  10:	mov	x0, #0xbf10000000000000    	// #-4679240012837945344
  14:	add	x0, x2, x0
  18:	cmp	x3, x0
  1c:	b.cs	74 <__truncdfhf2+0x74>  // b.hs, b.nlast
  20:	ubfx	x2, x2, #42, #16
  24:	and	x3, x1, #0x3ffffffffff
  28:	mov	x0, #0x20000000000         	// #2199023255552
  2c:	cmp	x3, x0
  30:	b.ls	50 <__truncdfhf2+0x50>  // b.plast
  34:	add	w2, w2, #0x4, lsl #12
  38:	add	w2, w2, #0x1
  3c:	and	w0, w2, #0xffff
  40:	lsr	x1, x1, #48
  44:	and	x1, x1, #0x8000
  48:	orr	w0, w0, w1
  4c:	ret
  50:	add	w0, w2, #0x4, lsl #12
  54:	and	w0, w0, #0xffff
  58:	add	w2, w2, #0x4, lsl #12
  5c:	add	w2, w2, #0x1
  60:	and	w2, w2, #0xfffe
  64:	mov	x4, #0x20000000000         	// #2199023255552
  68:	cmp	x3, x4
  6c:	csel	w0, w2, w0, eq  // eq = none
  70:	b	40 <__truncdfhf2+0x40>
  74:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  78:	cmp	x2, x0
  7c:	b.ls	8c <__truncdfhf2+0x8c>  // b.plast
  80:	ubfx	x2, x2, #42, #9
  84:	orr	w0, w2, #0x7e00
  88:	b	40 <__truncdfhf2+0x40>
  8c:	mov	w0, #0x7c00                	// #31744
  90:	mov	x3, #0x40efffffffffffff    	// #4679240012837945343
  94:	cmp	x2, x3
  98:	b.hi	40 <__truncdfhf2+0x40>  // b.pmore
  9c:	lsr	x3, x2, #52
  a0:	mov	w2, #0x3f1                 	// #1009
  a4:	sub	w4, w2, w3
  a8:	mov	w0, #0x0                   	// #0
  ac:	cmp	w4, #0x34
  b0:	b.gt	40 <__truncdfhf2+0x40>
  b4:	and	x0, x1, #0xfffffffffffff
  b8:	orr	x0, x0, #0x10000000000000
  bc:	sub	w2, w3, #0x3b1
  c0:	lsl	x2, x0, x2
  c4:	cmp	x2, #0x0
  c8:	cset	x3, ne  // ne = any
  cc:	lsr	x2, x0, x4
  d0:	orr	x3, x3, x2
  d4:	ubfx	x0, x2, #42, #16
  d8:	and	x2, x3, #0x3ffffffffff
  dc:	mov	x3, #0x20000000000         	// #2199023255552
  e0:	cmp	x2, x3
  e4:	b.ls	f4 <__truncdfhf2+0xf4>  // b.plast
  e8:	add	w2, w0, #0x1
  ec:	and	w0, w2, #0xffff
  f0:	b	40 <__truncdfhf2+0x40>
  f4:	add	w3, w0, #0x1
  f8:	and	w3, w3, #0xfffe
  fc:	mov	x4, #0x20000000000         	// #2199023255552
 100:	cmp	x2, x4
 104:	csel	w0, w3, w0, eq  // eq = none
 108:	b	40 <__truncdfhf2+0x40>

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	fmov	x0, d0
   4:	and	x1, x0, #0x7fffffffffffffff
   8:	mov	x3, #0xc7f0000000000000    	// #-4039728865751334912
   c:	add	x3, x1, x3
  10:	mov	x2, #0xb810000000000000    	// #-5183643171103440896
  14:	add	x2, x1, x2
  18:	cmp	x3, x2
  1c:	b.cs	78 <__truncdfsf2+0x78>  // b.hs, b.nlast
  20:	lsr	x1, x1, #29
  24:	and	x2, x0, #0x1fffffff
  28:	mov	x3, #0x10000000            	// #268435456
  2c:	cmp	x2, x3
  30:	b.ls	54 <__truncdfsf2+0x54>  // b.plast
  34:	mov	w3, #0x1                   	// #1
  38:	movk	w3, #0x4000, lsl #16
  3c:	add	w3, w3, w1
  40:	lsr	x0, x0, #32
  44:	and	x0, x0, #0x80000000
  48:	orr	w0, w3, w0
  4c:	fmov	s0, w0
  50:	ret
  54:	mov	w3, #0x40000000            	// #1073741824
  58:	add	w4, w3, w1
  5c:	add	w3, w3, #0x1
  60:	add	w1, w3, w1
  64:	and	w1, w1, #0xfffffffe
  68:	mov	x3, #0x10000000            	// #268435456
  6c:	cmp	x2, x3
  70:	csel	w3, w1, w4, eq  // eq = none
  74:	b	40 <__truncdfsf2+0x40>
  78:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  7c:	cmp	x1, x2
  80:	b.ls	90 <__truncdfsf2+0x90>  // b.plast
  84:	ubfx	x1, x1, #29, #22
  88:	orr	w3, w1, #0x7fc00000
  8c:	b	40 <__truncdfsf2+0x40>
  90:	mov	w3, #0x7f800000            	// #2139095040
  94:	mov	x2, #0x47efffffffffffff    	// #5183643171103440895
  98:	cmp	x1, x2
  9c:	b.hi	40 <__truncdfsf2+0x40>  // b.pmore
  a0:	lsr	x2, x1, #52
  a4:	mov	w1, #0x381                 	// #897
  a8:	sub	w4, w1, w2
  ac:	mov	w3, #0x0                   	// #0
  b0:	cmp	w4, #0x34
  b4:	b.gt	40 <__truncdfsf2+0x40>
  b8:	and	x1, x0, #0xfffffffffffff
  bc:	orr	x3, x1, #0x10000000000000
  c0:	sub	w1, w2, #0x341
  c4:	lsl	x1, x3, x1
  c8:	cmp	x1, #0x0
  cc:	cset	x2, ne  // ne = any
  d0:	lsr	x1, x3, x4
  d4:	orr	x2, x2, x1
  d8:	lsr	x1, x1, #29
  dc:	mov	w3, w1
  e0:	and	x2, x2, #0x1fffffff
  e4:	mov	x4, #0x10000000            	// #268435456
  e8:	cmp	x2, x4
  ec:	b.ls	f8 <__truncdfsf2+0xf8>  // b.plast
  f0:	add	w3, w1, #0x1
  f4:	b	40 <__truncdfsf2+0x40>
  f8:	add	w1, w1, #0x1
  fc:	and	w1, w1, #0xfffffffe
 100:	mov	x4, #0x10000000            	// #268435456
 104:	cmp	x2, x4
 108:	csel	w3, w1, w3, eq  // eq = none
 10c:	b	40 <__truncdfsf2+0x40>

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	fmov	w1, s0
   4:	and	w2, w1, #0x7fffffff
   8:	mov	w3, #0xc7800000            	// #-947912704
   c:	add	w3, w2, w3
  10:	mov	w0, #0xb8800000            	// #-1199570944
  14:	add	w0, w2, w0
  18:	cmp	w3, w0
  1c:	b.cs	68 <__truncsfhf2+0x68>  // b.hs, b.nlast
  20:	ubfx	x2, x2, #13, #16
  24:	and	w3, w1, #0x1fff
  28:	cmp	w3, #0x1, lsl #12
  2c:	b.ls	4c <__truncsfhf2+0x4c>  // b.plast
  30:	add	w2, w2, #0x4, lsl #12
  34:	add	w2, w2, #0x1
  38:	and	w0, w2, #0xffff
  3c:	lsr	w1, w1, #16
  40:	and	w1, w1, #0x8000
  44:	orr	w0, w0, w1
  48:	ret
  4c:	add	w0, w2, #0x4, lsl #12
  50:	and	w0, w0, #0xffff
  54:	add	w2, w2, #0x4, lsl #12
  58:	add	w2, w2, #0x1
  5c:	and	w2, w2, #0xfffe
  60:	csel	w0, w2, w0, eq  // eq = none
  64:	b	3c <__truncsfhf2+0x3c>
  68:	mov	w0, #0x7f800000            	// #2139095040
  6c:	cmp	w2, w0
  70:	b.ls	80 <__truncsfhf2+0x80>  // b.plast
  74:	ubfx	x2, x2, #13, #9
  78:	orr	w0, w2, #0x7e00
  7c:	b	3c <__truncsfhf2+0x3c>
  80:	mov	w0, #0x7c00                	// #31744
  84:	mov	w3, #0x477fffff            	// #1199570943
  88:	cmp	w2, w3
  8c:	b.hi	3c <__truncsfhf2+0x3c>  // b.pmore
  90:	lsr	w3, w2, #23
  94:	mov	w2, #0x71                  	// #113
  98:	sub	w4, w2, w3
  9c:	mov	w0, #0x0                   	// #0
  a0:	cmp	w4, #0x17
  a4:	b.gt	3c <__truncsfhf2+0x3c>
  a8:	and	w0, w1, #0x7fffff
  ac:	orr	w0, w0, #0x800000
  b0:	sub	w2, w3, #0x51
  b4:	lsl	w2, w0, w2
  b8:	cmp	w2, #0x0
  bc:	cset	w3, ne  // ne = any
  c0:	lsr	w2, w0, w4
  c4:	orr	w3, w3, w2
  c8:	ubfx	x0, x2, #13, #16
  cc:	and	w2, w3, #0x1fff
  d0:	cmp	w2, #0x1, lsl #12
  d4:	b.ls	e4 <__truncsfhf2+0xe4>  // b.plast
  d8:	add	w2, w0, #0x1
  dc:	and	w0, w2, #0xffff
  e0:	b	3c <__truncsfhf2+0x3c>
  e4:	add	w3, w0, #0x1
  e8:	and	w3, w3, #0xfffe
  ec:	csel	w0, w3, w0, eq  // eq = none
  f0:	b	3c <__truncsfhf2+0x3c>

00000000000000f4 <__gnu_f2h_ieee>:
  f4:	stp	x29, x30, [sp, #-16]!
  f8:	mov	x29, sp
  fc:	bl	0 <__truncsfhf2>
 100:	ldp	x29, x30, [sp], #16
 104:	ret

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	mov	x2, x0
   4:	lsr	x4, x0, #32
   8:	lsr	x3, x1, #32
   c:	mov	w0, #0x0                   	// #0
  10:	cmp	w4, w3
  14:	b.cc	34 <__ucmpdi2+0x34>  // b.lo, b.ul, b.last
  18:	mov	w0, #0x2                   	// #2
  1c:	b.hi	34 <__ucmpdi2+0x34>  // b.pmore
  20:	mov	w0, #0x0                   	// #0
  24:	cmp	w2, w1
  28:	b.cc	34 <__ucmpdi2+0x34>  // b.lo, b.ul, b.last
  2c:	cset	w0, hi  // hi = pmore
  30:	add	w0, w0, #0x1
  34:	ret

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	mov	x4, x0
   4:	mov	w0, #0x0                   	// #0
   8:	cmp	x1, x3
   c:	b.cc	2c <__ucmpti2+0x2c>  // b.lo, b.ul, b.last
  10:	mov	w0, #0x2                   	// #2
  14:	b.hi	2c <__ucmpti2+0x2c>  // b.pmore
  18:	mov	w0, #0x0                   	// #0
  1c:	cmp	x4, x2
  20:	b.cc	2c <__ucmpti2+0x2c>  // b.lo, b.ul, b.last
  24:	cset	w0, hi  // hi = pmore
  28:	add	w0, w0, #0x1
  2c:	ret

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x2, #0x0                   	// #0
   c:	bl	0 <__udivmoddi4>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	mov	x4, x0
   4:	lsr	x5, x0, #32
   8:	cmp	xzr, x0, lsr #32
   c:	b.ne	44 <__udivmoddi4+0x44>  // b.any
  10:	cmp	xzr, x1, lsr #32
  14:	b.ne	30 <__udivmoddi4+0x30>  // b.any
  18:	cbz	x2, 28 <__udivmoddi4+0x28>
  1c:	udiv	w0, w0, w1
  20:	msub	w0, w0, w1, w4
  24:	str	x0, [x2]
  28:	udiv	w0, w4, w1
  2c:	ret
  30:	mov	x0, #0x0                   	// #0
  34:	cbz	x2, 2c <__udivmoddi4+0x2c>
  38:	and	x4, x4, #0xffffffff
  3c:	str	x4, [x2]
  40:	b	2c <__udivmoddi4+0x2c>
  44:	cbnz	w1, 118 <__udivmoddi4+0x118>
  48:	lsr	x0, x1, #32
  4c:	cmp	xzr, x1, lsr #32
  50:	b.eq	b0 <__udivmoddi4+0xb0>  // b.none
  54:	cbz	w4, c4 <__udivmoddi4+0xc4>
  58:	sub	w6, w0, #0x1
  5c:	tst	w6, w0
  60:	b.eq	e0 <__udivmoddi4+0xe0>  // b.none
  64:	clz	w0, w0
  68:	clz	w3, w5
  6c:	sub	w0, w0, w3
  70:	cmp	w0, #0x1e
  74:	b.hi	108 <__udivmoddi4+0x108>  // b.pmore
  78:	add	w6, w0, #0x1
  7c:	mov	x9, #0x0                   	// #0
  80:	mov	w0, #0x20                  	// #32
  84:	sub	w0, w0, w6
  88:	lsl	w3, w4, w0
  8c:	bfi	x9, x3, #32, #32
  90:	lsr	w7, w5, w6
  94:	mov	x3, #0x0                   	// #0
  98:	bfi	x3, x7, #32, #32
  9c:	lsl	w0, w5, w0
  a0:	lsr	w4, w4, w6
  a4:	orr	w0, w0, w4
  a8:	bfxil	x3, x0, #0, #32
  ac:	b	264 <__udivmoddi4+0x264>
  b0:	cbz	x2, b8 <__udivmoddi4+0xb8>
  b4:	str	x5, [x2]
  b8:	mov	w0, #0x0                   	// #0
  bc:	udiv	w0, w5, w0
  c0:	b	2c <__udivmoddi4+0x2c>
  c4:	cbz	x2, d8 <__udivmoddi4+0xd8>
  c8:	udiv	w3, w5, w0
  cc:	msub	w3, w3, w0, w5
  d0:	lsl	x3, x3, #32
  d4:	str	x3, [x2]
  d8:	udiv	w0, w5, w0
  dc:	b	2c <__udivmoddi4+0x2c>
  e0:	cbz	x2, f8 <__udivmoddi4+0xf8>
  e4:	mov	x3, #0x0                   	// #0
  e8:	bfxil	x3, x4, #0, #32
  ec:	and	w6, w6, w5
  f0:	bfi	x3, x6, #32, #32
  f4:	str	x3, [x2]
  f8:	rbit	w0, w0
  fc:	clz	w0, w0
 100:	lsr	w0, w5, w0
 104:	b	2c <__udivmoddi4+0x2c>
 108:	mov	x0, #0x0                   	// #0
 10c:	cbz	x2, 2c <__udivmoddi4+0x2c>
 110:	str	x4, [x2]
 114:	b	2c <__udivmoddi4+0x2c>
 118:	lsr	x0, x1, #32
 11c:	cmp	xzr, x1, lsr #32
 120:	b.ne	218 <__udivmoddi4+0x218>  // b.any
 124:	sub	w0, w1, #0x1
 128:	tst	w0, w1
 12c:	b.ne	178 <__udivmoddi4+0x178>  // b.any
 130:	cbz	x2, 13c <__udivmoddi4+0x13c>
 134:	and	w0, w0, w4
 138:	str	x0, [x2]
 13c:	mov	x0, x4
 140:	cmp	w1, #0x1
 144:	b.eq	2c <__udivmoddi4+0x2c>  // b.none
 148:	rbit	w1, w1
 14c:	clz	w1, w1
 150:	lsr	w0, w5, w1
 154:	mov	x9, #0x0                   	// #0
 158:	bfi	x9, x0, #32, #32
 15c:	neg	w0, w1
 160:	lsl	w0, w5, w0
 164:	lsr	w1, w4, w1
 168:	orr	w0, w0, w1
 16c:	bfxil	x9, x0, #0, #32
 170:	mov	x0, x9
 174:	b	2c <__udivmoddi4+0x2c>
 178:	clz	w0, w1
 17c:	clz	w3, w5
 180:	sub	w0, w0, w3
 184:	add	w6, w0, #0x21
 188:	cmp	w6, #0x20
 18c:	b.eq	1cc <__udivmoddi4+0x1cc>  // b.none
 190:	cmp	w6, #0x1f
 194:	b.hi	1e0 <__udivmoddi4+0x1e0>  // b.pmore
 198:	mov	x9, #0x0                   	// #0
 19c:	mov	w0, #0x20                  	// #32
 1a0:	sub	w0, w0, w6
 1a4:	lsl	w3, w4, w0
 1a8:	bfi	x9, x3, #32, #32
 1ac:	lsr	w7, w5, w6
 1b0:	mov	x3, #0x0                   	// #0
 1b4:	bfi	x3, x7, #32, #32
 1b8:	lsl	w0, w5, w0
 1bc:	lsr	w4, w4, w6
 1c0:	orr	w0, w0, w4
 1c4:	bfxil	x3, x0, #0, #32
 1c8:	b	264 <__udivmoddi4+0x264>
 1cc:	mov	x9, #0x0                   	// #0
 1d0:	bfi	x9, x4, #32, #32
 1d4:	mov	x3, #0x0                   	// #0
 1d8:	bfxil	x3, x5, #0, #32
 1dc:	b	264 <__udivmoddi4+0x264>
 1e0:	mov	w3, #0x40                  	// #64
 1e4:	sub	w3, w3, w6
 1e8:	lsl	w7, w4, w3
 1ec:	mov	x9, #0x0                   	// #0
 1f0:	bfxil	x9, x7, #0, #32
 1f4:	add	w0, w0, #0x1
 1f8:	lsl	w3, w5, w3
 1fc:	lsr	w4, w4, w0
 200:	orr	w3, w3, w4
 204:	bfi	x9, x3, #32, #32
 208:	mov	x3, #0x0                   	// #0
 20c:	lsr	w0, w5, w0
 210:	bfxil	x3, x0, #0, #32
 214:	b	264 <__udivmoddi4+0x264>
 218:	clz	w0, w0
 21c:	clz	w3, w5
 220:	sub	w0, w0, w3
 224:	cmp	w0, #0x1f
 228:	b.hi	2c8 <__udivmoddi4+0x2c8>  // b.pmore
 22c:	add	w6, w0, #0x1
 230:	mov	x9, #0x0                   	// #0
 234:	b.eq	2d8 <__udivmoddi4+0x2d8>  // b.none
 238:	mov	w0, #0x20                  	// #32
 23c:	sub	w0, w0, w6
 240:	lsl	w3, w4, w0
 244:	bfi	x9, x3, #32, #32
 248:	lsr	w7, w5, w6
 24c:	mov	x3, #0x0                   	// #0
 250:	bfi	x3, x7, #32, #32
 254:	lsl	w5, w5, w0
 258:	lsr	w4, w4, w6
 25c:	orr	w4, w5, w4
 260:	bfxil	x3, x4, #0, #32
 264:	lsr	x8, x9, #32
 268:	mov	w7, w9
 26c:	mov	w0, #0x0                   	// #0
 270:	mov	w4, w3
 274:	lsr	x5, x3, #32
 278:	extr	w5, w5, w3, #31
 27c:	bfi	x3, x5, #32, #32
 280:	extr	w4, w4, w8, #31
 284:	bfxil	x3, x4, #0, #32
 288:	extr	w8, w8, w7, #31
 28c:	orr	w7, w0, w7, lsl #1
 290:	mvn	x4, x3
 294:	add	x4, x4, x1
 298:	asr	x4, x4, #63
 29c:	and	w0, w4, #0x1
 2a0:	and	x4, x4, x1
 2a4:	sub	x3, x3, x4
 2a8:	subs	w6, w6, #0x1
 2ac:	b.ne	270 <__udivmoddi4+0x270>  // b.any
 2b0:	bfi	x9, x8, #32, #32
 2b4:	bfxil	x9, x7, #0, #32
 2b8:	bfi	x0, x9, #1, #63
 2bc:	cbz	x2, 2c <__udivmoddi4+0x2c>
 2c0:	str	x3, [x2]
 2c4:	b	2c <__udivmoddi4+0x2c>
 2c8:	mov	x0, #0x0                   	// #0
 2cc:	cbz	x2, 2c <__udivmoddi4+0x2c>
 2d0:	str	x4, [x2]
 2d4:	b	2c <__udivmoddi4+0x2c>
 2d8:	bfi	x9, x4, #32, #32
 2dc:	mov	x3, #0x0                   	// #0
 2e0:	bfxil	x3, x5, #0, #32
 2e4:	b	264 <__udivmoddi4+0x264>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	mov	w21, w0
  14:	mov	w20, w1
  18:	mov	x19, x2
  1c:	bl	0 <__udivsi3>
  20:	msub	w1, w0, w20, w21
  24:	str	w1, [x19]
  28:	ldp	x19, x20, [sp, #16]
  2c:	ldr	x21, [sp, #32]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	cbnz	x1, 40 <__udivmodti4+0x40>
   4:	cbnz	x3, 28 <__udivmodti4+0x28>
   8:	cbz	x4, 1c <__udivmodti4+0x1c>
   c:	udiv	x1, x0, x2
  10:	msub	x1, x1, x2, x0
  14:	str	x1, [x4]
  18:	str	xzr, [x4, #8]
  1c:	udiv	x0, x0, x2
  20:	mov	x1, #0x0                   	// #0
  24:	ret
  28:	cbz	x4, 2a4 <__udivmodti4+0x2a4>
  2c:	str	x0, [x4]
  30:	str	xzr, [x4, #8]
  34:	mov	x0, #0x0                   	// #0
  38:	mov	x1, #0x0                   	// #0
  3c:	b	24 <__udivmodti4+0x24>
  40:	cbnz	x2, 10c <__udivmodti4+0x10c>
  44:	cbz	x3, 94 <__udivmodti4+0x94>
  48:	cbz	x0, b0 <__udivmodti4+0xb0>
  4c:	sub	x5, x3, #0x1
  50:	tst	x5, x3
  54:	b.eq	d0 <__udivmodti4+0xd0>  // b.none
  58:	clz	x11, x3
  5c:	clz	x5, x1
  60:	sub	w11, w11, w5
  64:	cmp	w11, #0x3e
  68:	b.hi	f4 <__udivmodti4+0xf4>  // b.pmore
  6c:	add	w11, w11, #0x1
  70:	mov	x12, #0x0                   	// #0
  74:	mov	w5, #0x40                  	// #64
  78:	sub	w6, w5, w11
  7c:	lsl	x9, x0, x6
  80:	lsr	x10, x1, x11
  84:	lsl	x6, x1, x6
  88:	lsr	x0, x0, x11
  8c:	orr	x7, x6, x0
  90:	b	210 <__udivmodti4+0x210>
  94:	cbz	x4, a0 <__udivmodti4+0xa0>
  98:	str	x1, [x4]
  9c:	str	xzr, [x4, #8]
  a0:	mov	x0, #0x0                   	// #0
  a4:	udiv	x0, x1, x0
  a8:	mov	x1, #0x0                   	// #0
  ac:	b	24 <__udivmodti4+0x24>
  b0:	cbz	x4, c4 <__udivmodti4+0xc4>
  b4:	udiv	x0, x1, x3
  b8:	msub	x0, x0, x3, x1
  bc:	str	xzr, [x4]
  c0:	str	x0, [x4, #8]
  c4:	udiv	x0, x1, x3
  c8:	mov	x1, #0x0                   	// #0
  cc:	b	24 <__udivmodti4+0x24>
  d0:	cbz	x4, e0 <__udivmodti4+0xe0>
  d4:	and	x5, x5, x1
  d8:	str	x0, [x4]
  dc:	str	x5, [x4, #8]
  e0:	rbit	x0, x3
  e4:	clz	x0, x0
  e8:	lsr	x0, x1, x0
  ec:	mov	x1, #0x0                   	// #0
  f0:	b	24 <__udivmodti4+0x24>
  f4:	cbz	x4, 2b0 <__udivmodti4+0x2b0>
  f8:	str	x0, [x4]
  fc:	str	x1, [x4, #8]
 100:	mov	x0, #0x0                   	// #0
 104:	mov	x1, #0x0                   	// #0
 108:	b	24 <__udivmodti4+0x24>
 10c:	cbnz	x3, 1d4 <__udivmodti4+0x1d4>
 110:	sub	x5, x2, #0x1
 114:	tst	x5, x2
 118:	b.ne	154 <__udivmodti4+0x154>  // b.any
 11c:	cbz	x4, 12c <__udivmodti4+0x12c>
 120:	and	x5, x5, x0
 124:	str	x5, [x4]
 128:	str	xzr, [x4, #8]
 12c:	cmp	x2, #0x1
 130:	b.eq	24 <__udivmodti4+0x24>  // b.none
 134:	rbit	x2, x2
 138:	clz	x2, x2
 13c:	neg	w3, w2
 140:	lsl	x3, x1, x3
 144:	lsr	x0, x0, x2
 148:	orr	x0, x3, x0
 14c:	lsr	x1, x1, x2
 150:	b	24 <__udivmodti4+0x24>
 154:	clz	x5, x2
 158:	clz	x6, x1
 15c:	sub	w5, w5, w6
 160:	add	w11, w5, #0x41
 164:	cmp	w11, #0x40
 168:	b.eq	198 <__udivmodti4+0x198>  // b.none
 16c:	cmp	w11, #0x3f
 170:	b.hi	1ac <__udivmodti4+0x1ac>  // b.pmore
 174:	mov	x12, #0x0                   	// #0
 178:	mov	w5, #0x40                  	// #64
 17c:	sub	w8, w5, w11
 180:	lsl	x9, x0, x8
 184:	lsr	x10, x1, x11
 188:	lsl	x7, x1, x8
 18c:	lsr	x0, x0, x11
 190:	orr	x7, x7, x0
 194:	b	210 <__udivmodti4+0x210>
 198:	mov	x12, #0x0                   	// #0
 19c:	mov	x9, x0
 1a0:	mov	x10, #0x0                   	// #0
 1a4:	mov	x7, x1
 1a8:	b	210 <__udivmodti4+0x210>
 1ac:	mov	w9, #0x80                  	// #128
 1b0:	sub	w9, w9, w11
 1b4:	lsl	x12, x0, x9
 1b8:	add	w8, w5, #0x1
 1bc:	lsl	x9, x1, x9
 1c0:	lsr	x0, x0, x8
 1c4:	orr	x9, x9, x0
 1c8:	mov	x10, #0x0                   	// #0
 1cc:	lsr	x7, x1, x8
 1d0:	b	210 <__udivmodti4+0x210>
 1d4:	clz	x5, x3
 1d8:	clz	x6, x1
 1dc:	sub	w5, w5, w6
 1e0:	cmp	w5, #0x3f
 1e4:	b.hi	27c <__udivmodti4+0x27c>  // b.pmore
 1e8:	add	w11, w5, #0x1
 1ec:	mov	x12, #0x0                   	// #0
 1f0:	b.eq	294 <__udivmodti4+0x294>  // b.none
 1f4:	lsr	x10, x1, x11
 1f8:	mov	w9, #0x40                  	// #64
 1fc:	sub	w9, w9, w11
 200:	lsl	x7, x1, x9
 204:	lsr	x1, x0, x11
 208:	orr	x7, x7, x1
 20c:	lsl	x9, x0, x9
 210:	mov	x1, x9
 214:	mov	x8, x12
 218:	mov	w0, #0x0                   	// #0
 21c:	extr	x10, x10, x7, #63
 220:	extr	x7, x7, x1, #63
 224:	extr	x1, x1, x8, #63
 228:	bfi	x0, x8, #1, #63
 22c:	mov	x8, x0
 230:	mvn	x0, x7
 234:	mvn	x6, x10
 238:	adds	x0, x0, x2
 23c:	adc	x6, x3, x6
 240:	asr	x5, x6, #63
 244:	and	w0, w5, #0x1
 248:	and	x9, x5, x2
 24c:	and	x6, x5, x3
 250:	subs	x7, x7, x9
 254:	sbc	x5, x10, x6
 258:	mov	x10, x5
 25c:	subs	w11, w11, #0x1
 260:	b.ne	21c <__udivmodti4+0x21c>  // b.any
 264:	bfi	x0, x8, #1, #63
 268:	extr	x1, x1, x8, #63
 26c:	cbz	x4, 24 <__udivmodti4+0x24>
 270:	str	x7, [x4]
 274:	str	x5, [x4, #8]
 278:	b	24 <__udivmodti4+0x24>
 27c:	cbz	x4, 2bc <__udivmodti4+0x2bc>
 280:	str	x0, [x4]
 284:	str	x1, [x4, #8]
 288:	mov	x0, #0x0                   	// #0
 28c:	mov	x1, #0x0                   	// #0
 290:	b	24 <__udivmodti4+0x24>
 294:	mov	x9, x0
 298:	mov	x10, #0x0                   	// #0
 29c:	mov	x7, x1
 2a0:	b	210 <__udivmodti4+0x210>
 2a4:	mov	x0, #0x0                   	// #0
 2a8:	mov	x1, #0x0                   	// #0
 2ac:	b	24 <__udivmodti4+0x24>
 2b0:	mov	x0, #0x0                   	// #0
 2b4:	mov	x1, #0x0                   	// #0
 2b8:	b	24 <__udivmodti4+0x24>
 2bc:	mov	x0, #0x0                   	// #0
 2c0:	mov	x1, #0x0                   	// #0
 2c4:	b	24 <__udivmodti4+0x24>

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	mov	w2, w0
   4:	cmp	w0, #0x0
   8:	ccmp	w1, #0x0, #0x4, ne  // ne = any
   c:	mov	w0, #0x0                   	// #0
  10:	b.ne	18 <__udivsi3+0x18>  // b.any
  14:	ret
  18:	clz	w3, w1
  1c:	clz	w0, w2
  20:	sub	w3, w3, w0
  24:	mov	w0, #0x0                   	// #0
  28:	cmp	w3, #0x1f
  2c:	b.hi	14 <__udivsi3+0x14>  // b.pmore
  30:	mov	w0, w2
  34:	b.eq	14 <__udivsi3+0x14>  // b.none
  38:	add	w3, w3, #0x1
  3c:	neg	w0, w3
  40:	lsl	w0, w2, w0
  44:	lsr	w2, w2, w3
  48:	mov	w5, #0x0                   	// #0
  4c:	sub	w6, w1, #0x1
  50:	extr	w2, w2, w0, #31
  54:	orr	w0, w5, w0, lsl #1
  58:	sub	w4, w6, w2
  5c:	lsr	w5, w4, #31
  60:	and	w4, w1, w4, asr #31
  64:	sub	w2, w2, w4
  68:	subs	w3, w3, #0x1
  6c:	b.ne	50 <__udivsi3+0x50>  // b.any
  70:	orr	w0, w5, w0, lsl #1
  74:	b	14 <__udivsi3+0x14>

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x4, #0x0                   	// #0
   c:	bl	0 <__udivmodti4>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x2, sp, #0x18
   c:	bl	0 <__udivmoddi4>
  10:	ldr	x0, [sp, #24]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w20, w0
  10:	mov	w19, w1
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x4, sp, #0x10
   c:	bl	0 <__udivmodti4>
  10:	ldp	x0, x1, [sp, #16]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <emutls_init>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	adrp	x1, 0 <emutls_init>
   c:	add	x1, x1, #0x0
  10:	adrp	x0, 0 <emutls_init>
  14:	add	x0, x0, #0x0
  18:	bl	0 <pthread_key_create>
  1c:	cbnz	w0, 28 <emutls_init+0x28>
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	bl	0 <abort>

000000000000002c <emutls_key_destructor>:
  2c:	stp	x29, x30, [sp, #-48]!
  30:	mov	x29, sp
  34:	stp	x19, x20, [sp, #16]
  38:	mov	x20, x0
  3c:	ldr	x19, [x0]
  40:	cbz	x19, 60 <emutls_key_destructor+0x34>
  44:	sub	x19, x19, #0x1
  48:	str	x19, [x0]
  4c:	mov	x1, x0
  50:	adrp	x0, 0 <emutls_init>
  54:	ldr	w0, [x0]
  58:	bl	0 <pthread_setspecific>
  5c:	b	a4 <emutls_key_destructor+0x78>
  60:	ldr	x0, [x0, #8]
  64:	cbz	x0, 9c <emutls_key_destructor+0x70>
  68:	str	x21, [sp, #32]
  6c:	add	x21, x20, #0x10
  70:	b	8c <emutls_key_destructor+0x60>
  74:	ldur	x0, [x0, #-8]
  78:	bl	0 <free>
  7c:	add	x19, x19, #0x1
  80:	ldr	x0, [x20, #8]
  84:	cmp	x19, x0
  88:	b.cs	98 <emutls_key_destructor+0x6c>  // b.hs, b.nlast
  8c:	ldr	x0, [x21, x19, lsl #3]
  90:	cbnz	x0, 74 <emutls_key_destructor+0x48>
  94:	b	7c <emutls_key_destructor+0x50>
  98:	ldr	x21, [sp, #32]
  9c:	mov	x0, x20
  a0:	bl	0 <free>
  a4:	ldp	x19, x20, [sp, #16]
  a8:	ldp	x29, x30, [sp], #48
  ac:	ret

00000000000000b0 <__emutls_get_address>:
  b0:	stp	x29, x30, [sp, #-64]!
  b4:	mov	x29, sp
  b8:	stp	x19, x20, [sp, #16]
  bc:	stp	x21, x22, [sp, #32]
  c0:	mov	x22, x0
  c4:	add	x20, x0, #0x10
  c8:	ldar	x21, [x20]
  cc:	cbz	x21, 110 <__emutls_get_address+0x60>
  d0:	adrp	x0, 0 <emutls_init>
  d4:	ldr	w0, [x0]
  d8:	bl	0 <pthread_getspecific>
  dc:	mov	x19, x0
  e0:	cbz	x0, 164 <__emutls_get_address+0xb4>
  e4:	ldr	x20, [x0, #8]
  e8:	cmp	x20, x21
  ec:	b.cc	1ac <__emutls_get_address+0xfc>  // b.lo, b.ul, b.last
  f0:	add	x19, x19, x21, lsl #3
  f4:	ldr	x1, [x19, #8]
  f8:	cbz	x1, 200 <__emutls_get_address+0x150>
  fc:	ldr	x0, [x19, #8]
 100:	ldp	x19, x20, [sp, #16]
 104:	ldp	x21, x22, [sp, #32]
 108:	ldp	x29, x30, [sp], #64
 10c:	ret
 110:	adrp	x19, 0 <emutls_init>
 114:	add	x19, x19, #0x0
 118:	adrp	x1, 0 <emutls_init>
 11c:	add	x1, x1, #0x0
 120:	add	x0, x19, #0x4
 124:	bl	0 <pthread_once>
 128:	add	x0, x19, #0x8
 12c:	bl	0 <pthread_mutex_lock>
 130:	ldr	x21, [x22, #16]
 134:	cbz	x21, 14c <__emutls_get_address+0x9c>
 138:	adrp	x0, 0 <emutls_init>
 13c:	add	x0, x0, #0x0
 140:	add	x0, x0, #0x8
 144:	bl	0 <pthread_mutex_unlock>
 148:	b	d0 <__emutls_get_address+0x20>
 14c:	mov	x0, x19
 150:	ldr	x19, [x19, #56]
 154:	add	x21, x19, #0x1
 158:	str	x21, [x0, #56]
 15c:	stlr	x21, [x20]
 160:	b	138 <__emutls_get_address+0x88>
 164:	add	x0, x21, #0x11
 168:	and	x0, x0, #0xfffffffffffffff0
 16c:	sub	x20, x0, #0x2
 170:	lsl	x0, x0, #3
 174:	bl	0 <malloc>
 178:	mov	x19, x0
 17c:	cbz	x0, 284 <__emutls_get_address+0x1d4>
 180:	lsl	x2, x20, #3
 184:	mov	w1, #0x0                   	// #0
 188:	add	x0, x0, #0x10
 18c:	bl	0 <memset>
 190:	str	xzr, [x19]
 194:	str	x20, [x19, #8]
 198:	mov	x1, x19
 19c:	adrp	x0, 0 <emutls_init>
 1a0:	ldr	w0, [x0]
 1a4:	bl	0 <pthread_setspecific>
 1a8:	b	f0 <__emutls_get_address+0x40>
 1ac:	str	x23, [sp, #48]
 1b0:	add	x1, x21, #0x11
 1b4:	and	x1, x1, #0xfffffffffffffff0
 1b8:	sub	x23, x1, #0x2
 1bc:	lsl	x1, x1, #3
 1c0:	bl	0 <realloc>
 1c4:	mov	x19, x0
 1c8:	cbz	x0, 28c <__emutls_get_address+0x1dc>
 1cc:	sub	x2, x23, x20
 1d0:	add	x0, x0, #0x10
 1d4:	lsl	x2, x2, #3
 1d8:	mov	w1, #0x0                   	// #0
 1dc:	add	x0, x0, x20, lsl #3
 1e0:	bl	0 <memset>
 1e4:	str	x23, [x19, #8]
 1e8:	mov	x1, x19
 1ec:	adrp	x0, 0 <emutls_init>
 1f0:	ldr	w0, [x0]
 1f4:	bl	0 <pthread_setspecific>
 1f8:	ldr	x23, [sp, #48]
 1fc:	b	f0 <__emutls_get_address+0x40>
 200:	ldr	x21, [x22]
 204:	ldr	x20, [x22, #8]
 208:	cmp	x20, #0x8
 20c:	mov	x0, #0x8                   	// #8
 210:	csel	x20, x20, x0, cs  // cs = hs, nlast
 214:	sub	x0, x20, #0x1
 218:	tst	x0, x20
 21c:	b.ne	260 <__emutls_get_address+0x1b0>  // b.any
 220:	add	x0, x21, #0x7
 224:	add	x0, x0, x20
 228:	bl	0 <malloc>
 22c:	cbz	x0, 268 <__emutls_get_address+0x1b8>
 230:	add	x1, x20, #0x7
 234:	add	x1, x0, x1
 238:	neg	x20, x20
 23c:	and	x20, x1, x20
 240:	stur	x0, [x20, #-8]
 244:	ldr	x1, [x22, #24]
 248:	cbz	x1, 270 <__emutls_get_address+0x1c0>
 24c:	mov	x2, x21
 250:	mov	x0, x20
 254:	bl	0 <memcpy>
 258:	str	x20, [x19, #8]
 25c:	b	fc <__emutls_get_address+0x4c>
 260:	str	x23, [sp, #48]
 264:	bl	0 <abort>
 268:	str	x23, [sp, #48]
 26c:	bl	0 <abort>
 270:	mov	x2, x21
 274:	mov	w1, #0x0                   	// #0
 278:	mov	x0, x20
 27c:	bl	0 <memset>
 280:	b	258 <__emutls_get_address+0x1a8>
 284:	str	x23, [sp, #48]
 288:	bl	0 <abort>
 28c:	bl	0 <abort>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	mov	x1, x0
  1c:	neg	x2, x0
  20:	and	x0, x2, x19
  24:	add	x19, x19, #0x30
  28:	add	x1, x19, x1
  2c:	and	x1, x1, x2
  30:	mov	w2, #0x7                   	// #7
  34:	sub	x1, x1, x0
  38:	bl	0 <mprotect>
  3c:	ldr	x19, [sp, #16]
  40:	ldp	x29, x30, [sp], #32
  44:	ret

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	adrp	x19, 0 <stderr>
  10:	ldr	x19, [x19]
  14:	mov	x4, x3
  18:	mov	x3, x2
  1c:	mov	x2, x1
  20:	mov	x1, x0
  24:	ldr	x0, [x19]
  28:	bl	0 <fprintf>
  2c:	ldr	x0, [x19]
  30:	bl	0 <fflush>
  34:	adrp	x2, 0 <__eprintf>
  38:	add	x2, x2, #0x0
  3c:	mov	w1, #0x1a                  	// #26
  40:	adrp	x0, 0 <__eprintf>
  44:	add	x0, x0, #0x0
  48:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <readULEB128>:
   0:	mov	x5, x0
   4:	ldr	x4, [x0]
   8:	mov	x2, #0x0                   	// #0
   c:	mov	x0, #0x0                   	// #0
  10:	ldrb	w3, [x4], #1
  14:	and	w1, w3, #0x7f
  18:	lsl	w1, w1, w2
  1c:	sxtw	x1, w1
  20:	orr	x0, x0, x1
  24:	add	x2, x2, #0x7
  28:	tbnz	w3, #7, 10 <readULEB128+0x10>
  2c:	str	x4, [x5]
  30:	ret

0000000000000034 <readEncodedPointer>:
  34:	stp	x29, x30, [sp, #-48]!
  38:	mov	x29, sp
  3c:	stp	x19, x20, [sp, #16]
  40:	and	w20, w1, #0xff
  44:	ldr	x2, [x0]
  48:	str	x2, [sp, #40]
  4c:	cmp	w20, #0xff
  50:	b.eq	164 <readEncodedPointer+0x130>  // b.none
  54:	mov	x19, x0
  58:	and	w1, w20, #0xf
  5c:	cmp	w1, #0x4
  60:	b.eq	114 <readEncodedPointer+0xe0>  // b.none
  64:	b.hi	a0 <readEncodedPointer+0x6c>  // b.pmore
  68:	cmp	w1, #0x2
  6c:	b.eq	108 <readEncodedPointer+0xd4>  // b.none
  70:	b.hi	8c <readEncodedPointer+0x58>  // b.pmore
  74:	cbz	w1, d0 <readEncodedPointer+0x9c>
  78:	cmp	w1, #0x1
  7c:	b.ne	12c <readEncodedPointer+0xf8>  // b.any
  80:	add	x0, sp, #0x28
  84:	bl	0 <readULEB128>
  88:	b	d8 <readEncodedPointer+0xa4>
  8c:	cmp	w1, #0x3
  90:	b.ne	12c <readEncodedPointer+0xf8>  // b.any
  94:	ldr	w0, [x2], #4
  98:	str	x2, [sp, #40]
  9c:	b	d8 <readEncodedPointer+0xa4>
  a0:	cmp	w1, #0xb
  a4:	b.eq	120 <readEncodedPointer+0xec>  // b.none
  a8:	cmp	w1, #0xc
  ac:	b.ne	bc <readEncodedPointer+0x88>  // b.any
  b0:	ldr	x0, [x2], #8
  b4:	str	x2, [sp, #40]
  b8:	b	d8 <readEncodedPointer+0xa4>
  bc:	cmp	w1, #0xa
  c0:	b.ne	12c <readEncodedPointer+0xf8>  // b.any
  c4:	ldrsh	x0, [x2], #2
  c8:	str	x2, [sp, #40]
  cc:	b	d8 <readEncodedPointer+0xa4>
  d0:	ldr	x0, [x2], #8
  d4:	str	x2, [sp, #40]
  d8:	ands	w1, w20, #0x70
  dc:	b.eq	f0 <readEncodedPointer+0xbc>  // b.none
  e0:	cmp	w1, #0x10
  e4:	b.ne	144 <readEncodedPointer+0x110>  // b.any
  e8:	ldr	x1, [x19]
  ec:	add	x0, x1, x0
  f0:	tbnz	w20, #7, 15c <readEncodedPointer+0x128>
  f4:	ldr	x1, [sp, #40]
  f8:	str	x1, [x19]
  fc:	ldp	x19, x20, [sp, #16]
 100:	ldp	x29, x30, [sp], #48
 104:	ret
 108:	ldrh	w0, [x2], #2
 10c:	str	x2, [sp, #40]
 110:	b	d8 <readEncodedPointer+0xa4>
 114:	ldr	x0, [x2], #8
 118:	str	x2, [sp, #40]
 11c:	b	d8 <readEncodedPointer+0xa4>
 120:	ldrsw	x0, [x2], #4
 124:	str	x2, [sp, #40]
 128:	b	d8 <readEncodedPointer+0xa4>
 12c:	adrp	x2, 0 <readULEB128>
 130:	add	x2, x2, #0x0
 134:	mov	w1, #0x68                  	// #104
 138:	adrp	x0, 0 <readULEB128>
 13c:	add	x0, x0, #0x0
 140:	bl	0 <__compilerrt_abort_impl>
 144:	adrp	x2, 0 <readULEB128>
 148:	add	x2, x2, #0x0
 14c:	mov	w1, #0x7a                  	// #122
 150:	adrp	x0, 0 <readULEB128>
 154:	add	x0, x0, #0x0
 158:	bl	0 <__compilerrt_abort_impl>
 15c:	ldr	x0, [x0]
 160:	b	f4 <readEncodedPointer+0xc0>
 164:	mov	x0, #0x0                   	// #0
 168:	b	fc <readEncodedPointer+0xc8>

000000000000016c <__gcc_personality_v0>:
 16c:	mov	w0, #0x8                   	// #8
 170:	tbz	w1, #0, 178 <__gcc_personality_v0+0xc>
 174:	ret
 178:	stp	x29, x30, [sp, #-112]!
 17c:	mov	x29, sp
 180:	stp	x25, x26, [sp, #64]
 184:	stp	x27, x28, [sp, #80]
 188:	mov	x27, x3
 18c:	mov	x26, x4
 190:	mov	x0, x4
 194:	bl	0 <_Unwind_GetLanguageSpecificData>
 198:	mov	x1, x0
 19c:	str	x0, [sp, #104]
 1a0:	mov	w0, #0x8                   	// #8
 1a4:	cbz	x1, 2fc <__gcc_personality_v0+0x190>
 1a8:	stp	x23, x24, [sp, #48]
 1ac:	mov	x0, x26
 1b0:	bl	0 <_Unwind_GetIP>
 1b4:	mov	x24, x0
 1b8:	mov	x0, x26
 1bc:	bl	0 <_Unwind_GetRegionStart>
 1c0:	mov	x28, x0
 1c4:	sub	x24, x24, #0x1
 1c8:	sub	x24, x24, x0
 1cc:	ldr	x0, [sp, #104]
 1d0:	add	x1, x0, #0x1
 1d4:	str	x1, [sp, #104]
 1d8:	ldrb	w1, [x0]
 1dc:	cmp	w1, #0xff
 1e0:	b.ne	238 <__gcc_personality_v0+0xcc>  // b.any
 1e4:	ldr	x0, [sp, #104]
 1e8:	add	x1, x0, #0x1
 1ec:	str	x1, [sp, #104]
 1f0:	ldrb	w0, [x0]
 1f4:	cmp	w0, #0xff
 1f8:	b.ne	244 <__gcc_personality_v0+0xd8>  // b.any
 1fc:	ldr	x0, [sp, #104]
 200:	add	x1, x0, #0x1
 204:	str	x1, [sp, #104]
 208:	ldrb	w23, [x0]
 20c:	add	x0, sp, #0x68
 210:	bl	0 <readULEB128>
 214:	ldr	x1, [sp, #104]
 218:	add	x25, x1, w0, uxtw
 21c:	str	x1, [sp, #96]
 220:	cmp	x1, x25
 224:	b.cs	30c <__gcc_personality_v0+0x1a0>  // b.hs, b.nlast
 228:	stp	x19, x20, [sp, #16]
 22c:	stp	x21, x22, [sp, #32]
 230:	add	x20, sp, #0x60
 234:	b	25c <__gcc_personality_v0+0xf0>
 238:	add	x0, sp, #0x68
 23c:	bl	34 <readEncodedPointer>
 240:	b	1e4 <__gcc_personality_v0+0x78>
 244:	add	x0, sp, #0x68
 248:	bl	0 <readULEB128>
 24c:	b	1fc <__gcc_personality_v0+0x90>
 250:	ldr	x0, [sp, #96]
 254:	cmp	x0, x25
 258:	b.cs	2ec <__gcc_personality_v0+0x180>  // b.hs, b.nlast
 25c:	mov	w1, w23
 260:	mov	x0, x20
 264:	bl	34 <readEncodedPointer>
 268:	mov	x19, x0
 26c:	mov	w1, w23
 270:	mov	x0, x20
 274:	bl	34 <readEncodedPointer>
 278:	mov	x22, x0
 27c:	mov	w1, w23
 280:	mov	x0, x20
 284:	bl	34 <readEncodedPointer>
 288:	mov	x21, x0
 28c:	mov	x0, x20
 290:	bl	0 <readULEB128>
 294:	cmp	x21, #0x0
 298:	ccmp	x24, x19, #0x0, ne  // ne = any
 29c:	b.cc	250 <__gcc_personality_v0+0xe4>  // b.lo, b.ul, b.last
 2a0:	add	x19, x19, x22
 2a4:	cmp	x19, x24
 2a8:	b.ls	250 <__gcc_personality_v0+0xe4>  // b.plast
 2ac:	mov	x2, x27
 2b0:	mov	w1, #0x0                   	// #0
 2b4:	mov	x0, x26
 2b8:	bl	0 <_Unwind_SetGR>
 2bc:	mov	x2, #0x0                   	// #0
 2c0:	mov	w1, #0x1                   	// #1
 2c4:	mov	x0, x26
 2c8:	bl	0 <_Unwind_SetGR>
 2cc:	add	x1, x28, x21
 2d0:	mov	x0, x26
 2d4:	bl	0 <_Unwind_SetIP>
 2d8:	mov	w0, #0x7                   	// #7
 2dc:	ldp	x19, x20, [sp, #16]
 2e0:	ldp	x21, x22, [sp, #32]
 2e4:	ldp	x23, x24, [sp, #48]
 2e8:	b	2fc <__gcc_personality_v0+0x190>
 2ec:	mov	w0, #0x8                   	// #8
 2f0:	ldp	x19, x20, [sp, #16]
 2f4:	ldp	x21, x22, [sp, #32]
 2f8:	ldp	x23, x24, [sp, #48]
 2fc:	ldp	x25, x26, [sp, #64]
 300:	ldp	x27, x28, [sp, #80]
 304:	ldp	x29, x30, [sp], #112
 308:	ret
 30c:	mov	w0, #0x8                   	// #8
 310:	ldp	x23, x24, [sp, #48]
 314:	b	2fc <__gcc_personality_v0+0x190>

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	mov	x3, x1
   4:	adrp	x2, 0 <__clear_cache>
   8:	ldr	x2, [x2]
   c:	cbnz	x2, 1c <__clear_cache+0x1c>
  10:	mrs	x4, ctr_el0
  14:	adrp	x2, 0 <__clear_cache>
  18:	str	x4, [x2]
  1c:	adrp	x2, 0 <__clear_cache>
  20:	ldr	x4, [x2]
  24:	tbnz	w4, #28, 58 <__clear_cache+0x58>
  28:	ubfx	w2, w4, #16, #4
  2c:	mov	w4, #0x4                   	// #4
  30:	lsl	w4, w4, w2
  34:	sxtw	x4, w4
  38:	neg	x2, x4
  3c:	and	x2, x0, x2
  40:	cmp	x1, x2
  44:	b.ls	58 <__clear_cache+0x58>  // b.plast
  48:	dc	cvau, x2
  4c:	add	x2, x2, x4
  50:	cmp	x3, x2
  54:	b.hi	48 <__clear_cache+0x48>  // b.pmore
  58:	dsb	ish
  5c:	adrp	x2, 0 <__clear_cache>
  60:	ldr	x2, [x2]
  64:	tbnz	w2, #29, 98 <__clear_cache+0x98>
  68:	and	w4, w2, #0xf
  6c:	mov	w2, #0x4                   	// #4
  70:	lsl	w2, w2, w4
  74:	sxtw	x2, w2
  78:	neg	x4, x2
  7c:	and	x0, x0, x4
  80:	cmp	x1, x0
  84:	b.ls	98 <__clear_cache+0x98>  // b.plast
  88:	ic	ivau, x0
  8c:	add	x0, x0, x2
  90:	cmp	x3, x0
  94:	b.hi	88 <__clear_cache+0x88>  // b.pmore
  98:	isb
  9c:	ret

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x1, fpcr
   4:	ubfx	x1, x1, #22, #2
   8:	mov	w0, #0x1                   	// #1
   c:	cmp	x1, #0x2
  10:	b.eq	2c <__fe_getround+0x2c>  // b.none
  14:	mov	w0, #0x3                   	// #3
  18:	cmp	x1, #0x3
  1c:	b.eq	2c <__fe_getround+0x2c>  // b.none
  20:	cmp	x1, #0x1
  24:	cset	w0, eq  // eq = none
  28:	lsl	w0, w0, #1
  2c:	ret

0000000000000030 <__fe_raise_inexact>:
  30:	mrs	x0, fpsr
  34:	orr	x0, x0, #0x10
  38:	msr	fpsr, x0
  3c:	mov	w0, #0x0                   	// #0
  40:	ret
