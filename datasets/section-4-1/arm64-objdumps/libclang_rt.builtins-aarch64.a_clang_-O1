In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_clang_-O1:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	str	q1, [sp]
  14:	bl	e0 <toRep>
  18:	ldr	q0, [sp]
  1c:	mov	x20, x0
  20:	mov	x19, x1
  24:	bl	e0 <toRep>
  28:	and	x9, x19, #0x7fffffffffffffff
  2c:	cmp	x20, #0x0
  30:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
  34:	cset	w8, ne  // ne = any
  38:	cmp	x9, x11
  3c:	cset	w10, hi  // hi = pmore
  40:	csel	w10, w8, w10, eq  // eq = none
  44:	mov	w8, #0x1                   	// #1
  48:	tbnz	w10, #0, cc <__cmptf2+0xcc>
  4c:	and	x10, x1, #0x7fffffffffffffff
  50:	cmp	x0, #0x0
  54:	cset	w12, ne  // ne = any
  58:	cmp	x10, x11
  5c:	cset	w11, hi  // hi = pmore
  60:	csel	w11, w12, w11, eq  // eq = none
  64:	tbnz	w11, #0, cc <__cmptf2+0xcc>
  68:	orr	x8, x0, x20
  6c:	orr	x9, x10, x9
  70:	orr	x8, x8, x9
  74:	cbz	x8, cc <__cmptf2+0xcc>
  78:	tst	x1, x19
  7c:	b.lt	a0 <__cmptf2+0xa0>  // b.tstop
  80:	cmp	x20, x0
  84:	cset	w8, cc  // cc = lo, ul, last
  88:	cmp	x19, x1
  8c:	cset	w9, lt  // lt = tstop
  90:	csel	w8, w8, w9, eq  // eq = none
  94:	tbz	w8, #0, b8 <__cmptf2+0xb8>
  98:	mov	w8, #0xffffffff            	// #-1
  9c:	b	cc <__cmptf2+0xcc>
  a0:	cmp	x20, x0
  a4:	cset	w8, hi  // hi = pmore
  a8:	cmp	x19, x1
  ac:	cset	w9, gt
  b0:	csel	w8, w8, w9, eq  // eq = none
  b4:	tbnz	w8, #0, 98 <__cmptf2+0x98>
  b8:	eor	x8, x20, x0
  bc:	eor	x9, x19, x1
  c0:	orr	x8, x8, x9
  c4:	cmp	x8, #0x0
  c8:	cset	w8, ne  // ne = any
  cc:	ldp	x20, x19, [sp, #32]
  d0:	ldp	x29, x30, [sp, #16]
  d4:	mov	w0, w8
  d8:	add	sp, sp, #0x30
  dc:	ret

00000000000000e0 <toRep>:
  e0:	str	q0, [sp, #-16]!
  e4:	ldp	x0, x1, [sp], #16
  e8:	ret

00000000000000ec <__getf2>:
  ec:	sub	sp, sp, #0x30
  f0:	stp	x29, x30, [sp, #16]
  f4:	stp	x20, x19, [sp, #32]
  f8:	add	x29, sp, #0x10
  fc:	str	q1, [sp]
 100:	bl	e0 <toRep>
 104:	ldr	q0, [sp]
 108:	mov	x20, x0
 10c:	mov	x19, x1
 110:	bl	e0 <toRep>
 114:	and	x9, x19, #0x7fffffffffffffff
 118:	cmp	x20, #0x0
 11c:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
 120:	cset	w8, ne  // ne = any
 124:	cmp	x9, x11
 128:	cset	w10, hi  // hi = pmore
 12c:	csel	w10, w8, w10, eq  // eq = none
 130:	mov	w8, #0xffffffff            	// #-1
 134:	tbnz	w10, #0, 1b8 <__getf2+0xcc>
 138:	and	x10, x1, #0x7fffffffffffffff
 13c:	cmp	x0, #0x0
 140:	cset	w12, ne  // ne = any
 144:	cmp	x10, x11
 148:	cset	w11, hi  // hi = pmore
 14c:	csel	w11, w12, w11, eq  // eq = none
 150:	tbnz	w11, #0, 1b8 <__getf2+0xcc>
 154:	orr	x8, x0, x20
 158:	orr	x9, x10, x9
 15c:	orr	x8, x8, x9
 160:	cbz	x8, 1b8 <__getf2+0xcc>
 164:	tst	x1, x19
 168:	b.lt	18c <__getf2+0xa0>  // b.tstop
 16c:	cmp	x20, x0
 170:	cset	w8, cc  // cc = lo, ul, last
 174:	cmp	x19, x1
 178:	cset	w9, lt  // lt = tstop
 17c:	csel	w8, w8, w9, eq  // eq = none
 180:	tbz	w8, #0, 1a4 <__getf2+0xb8>
 184:	mov	w8, #0xffffffff            	// #-1
 188:	b	1b8 <__getf2+0xcc>
 18c:	cmp	x20, x0
 190:	cset	w8, hi  // hi = pmore
 194:	cmp	x19, x1
 198:	cset	w9, gt
 19c:	csel	w8, w8, w9, eq  // eq = none
 1a0:	tbnz	w8, #0, 184 <__getf2+0x98>
 1a4:	eor	x8, x20, x0
 1a8:	eor	x9, x19, x1
 1ac:	orr	x8, x8, x9
 1b0:	cmp	x8, #0x0
 1b4:	cset	w8, ne  // ne = any
 1b8:	ldp	x20, x19, [sp, #32]
 1bc:	ldp	x29, x30, [sp, #16]
 1c0:	mov	w0, w8
 1c4:	add	sp, sp, #0x30
 1c8:	ret

00000000000001cc <__unordtf2>:
 1cc:	sub	sp, sp, #0x30
 1d0:	stp	x29, x30, [sp, #16]
 1d4:	stp	x20, x19, [sp, #32]
 1d8:	add	x29, sp, #0x10
 1dc:	str	q1, [sp]
 1e0:	bl	e0 <toRep>
 1e4:	ldr	q0, [sp]
 1e8:	mov	x19, x0
 1ec:	and	x20, x1, #0x7fffffffffffffff
 1f0:	bl	e0 <toRep>
 1f4:	cmp	x19, #0x0
 1f8:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
 1fc:	cset	w10, ne  // ne = any
 200:	cmp	x20, x9
 204:	cset	w11, hi  // hi = pmore
 208:	and	x8, x1, #0x7fffffffffffffff
 20c:	csel	w10, w10, w11, eq  // eq = none
 210:	cmp	x0, #0x0
 214:	cset	w11, ne  // ne = any
 218:	cmp	x8, x9
 21c:	ldp	x20, x19, [sp, #32]
 220:	ldp	x29, x30, [sp, #16]
 224:	cset	w8, hi  // hi = pmore
 228:	csel	w8, w11, w8, eq  // eq = none
 22c:	orr	w0, w10, w8
 230:	add	sp, sp, #0x30
 234:	ret

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__extendXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__extendXfYf2__>:
  14:	stp	x29, x30, [sp, #-48]!
  18:	str	x21, [sp, #16]
  1c:	stp	x20, x19, [sp, #32]
  20:	mov	x29, sp
  24:	bl	f0 <srcToRep>
  28:	and	x19, x0, #0x7fffffffffffffff
  2c:	mov	x9, #0xfff0000000000000    	// #-4503599627370496
  30:	add	x9, x19, x9
  34:	lsr	x9, x9, #53
  38:	cmp	x9, #0x3fe
  3c:	and	x21, x0, #0x8000000000000000
  40:	b.hi	54 <__extendXfYf2__+0x40>  // b.pmore
  44:	mov	x8, #0x3c00000000000000    	// #4323455642275676160
  48:	lsl	x0, x19, #60
  4c:	add	x8, x8, x19, lsr #4
  50:	b	d8 <__extendXfYf2__+0xc4>
  54:	lsr	x9, x19, #52
  58:	cmp	x9, #0x7ff
  5c:	b.cc	74 <__extendXfYf2__+0x60>  // b.lo, b.ul, b.last
  60:	mov	x8, x0
  64:	lsr	x8, x8, #4
  68:	lsl	x0, x0, #60
  6c:	orr	x8, x8, #0x7fff000000000000
  70:	b	d8 <__extendXfYf2__+0xc4>
  74:	cbz	x19, d0 <__extendXfYf2__+0xbc>
  78:	mov	x0, x19
  7c:	bl	f8 <src_rep_t_clz>
  80:	mov	w20, w0
  84:	mov	x0, #0x10000000000000      	// #4503599627370496
  88:	bl	f8 <src_rep_t_clz>
  8c:	sub	w8, w20, w0
  90:	add	w10, w8, #0x3c
  94:	neg	x11, x10
  98:	cmp	x10, #0x0
  9c:	lsl	x12, x19, x10
  a0:	lsr	x11, x19, x11
  a4:	lsl	x13, x19, x10
  a8:	sub	x10, x10, #0x40
  ac:	csel	x11, xzr, x11, eq  // eq = none
  b0:	cmp	x10, #0x0
  b4:	mov	w9, #0x3c01                	// #15361
  b8:	csel	x10, x13, x11, ge  // ge = tcont
  bc:	eor	x10, x10, #0x1000000000000
  c0:	sub	w8, w9, w8
  c4:	csel	x0, xzr, x12, ge  // ge = tcont
  c8:	orr	x8, x10, x8, lsl #48
  cc:	b	d8 <__extendXfYf2__+0xc4>
  d0:	mov	x0, xzr
  d4:	mov	x8, xzr
  d8:	orr	x1, x8, x21
  dc:	bl	100 <dstFromRep>
  e0:	ldp	x20, x19, [sp, #32]
  e4:	ldr	x21, [sp, #16]
  e8:	ldp	x29, x30, [sp], #48
  ec:	ret

00000000000000f0 <srcToRep>:
  f0:	fmov	x0, d0
  f4:	ret

00000000000000f8 <src_rep_t_clz>:
  f8:	clz	x0, x0
  fc:	ret

0000000000000100 <dstFromRep>:
 100:	stp	x0, x1, [sp, #-16]!
 104:	ldr	q0, [sp], #16
 108:	ret

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__extendXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__extendXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	cc <srcToRep>
  20:	and	w10, w0, #0x7fffffff
  24:	sub	w9, w10, #0x800, lsl #12
  28:	lsr	w9, w9, #24
  2c:	cmp	w9, #0x7e
  30:	and	w9, w0, #0x80000000
  34:	b.hi	48 <__extendXfYf2__+0x34>  // b.pmore
  38:	mov	x8, #0x3f80000000000000    	// #4575657221408423936
  3c:	mov	x0, xzr
  40:	add	x8, x8, x10, lsl #25
  44:	b	bc <__extendXfYf2__+0xa8>
  48:	lsr	w11, w10, #23
  4c:	cmp	w11, #0xff
  50:	b.cc	6c <__extendXfYf2__+0x58>  // b.lo, b.ul, b.last
  54:	mov	w8, w0
  58:	mov	w8, w8
  5c:	lsl	x8, x8, #25
  60:	mov	x0, xzr
  64:	orr	x8, x8, #0x7fff000000000000
  68:	b	bc <__extendXfYf2__+0xa8>
  6c:	cbz	w10, b4 <__extendXfYf2__+0xa0>
  70:	clz	w8, w10
  74:	add	w12, w8, #0x51
  78:	neg	x13, x12
  7c:	cmp	x12, #0x0
  80:	lsl	x14, x10, x12
  84:	lsr	x13, x10, x13
  88:	lsl	x10, x10, x12
  8c:	sub	x12, x12, #0x40
  90:	csel	x13, xzr, x13, eq  // eq = none
  94:	cmp	x12, #0x0
  98:	mov	w11, #0x3f89                	// #16265
  9c:	csel	x12, x14, x13, ge  // ge = tcont
  a0:	csel	x0, xzr, x10, ge  // ge = tcont
  a4:	eor	x10, x12, #0x1000000000000
  a8:	sub	w8, w11, w8
  ac:	orr	x8, x10, x8, lsl #48
  b0:	b	bc <__extendXfYf2__+0xa8>
  b4:	mov	x0, xzr
  b8:	mov	x8, xzr
  bc:	orr	x1, x8, x9, lsl #32
  c0:	bl	d4 <dstFromRep>
  c4:	ldp	x29, x30, [sp], #16
  c8:	ret

00000000000000cc <srcToRep>:
  cc:	fmov	w0, s0
  d0:	ret

00000000000000d4 <dstFromRep>:
  d4:	stp	x0, x1, [sp, #-16]!
  d8:	ldr	q0, [sp], #16
  dc:	ret

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	a8 <toRep>
  20:	ubfx	x8, x1, #48, #15
  24:	mov	w9, #0x3fff                	// #16383
  28:	cmp	w8, w9
  2c:	b.cs	3c <__fixint+0x28>  // b.hs, b.nlast
  30:	mov	x0, xzr
  34:	ldp	x29, x30, [sp], #16
  38:	ret
  3c:	mov	w9, #0xffffc001            	// #-16383
  40:	add	w9, w8, w9
  44:	cmp	w9, #0x40
  48:	b.cc	60 <__fixint+0x4c>  // b.lo, b.ul, b.last
  4c:	cmp	x1, #0x0
  50:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  54:	cinv	x0, x8, ge  // ge = tcont
  58:	ldp	x29, x30, [sp], #16
  5c:	ret
  60:	mov	w10, #0x406f                	// #16495
  64:	mov	x9, #0x1000000000000       	// #281474976710656
  68:	sub	w8, w10, w8
  6c:	bfxil	x9, x1, #0, #48
  70:	neg	x10, x8
  74:	cmp	x8, #0x0
  78:	lsl	x10, x9, x10
  7c:	lsr	x11, x0, x8
  80:	lsr	x9, x9, x8
  84:	sub	x8, x8, #0x40
  88:	csel	x10, xzr, x10, eq  // eq = none
  8c:	cmp	x8, #0x0
  90:	orr	x8, x11, x10
  94:	csel	x8, x9, x8, ge  // ge = tcont
  98:	cmp	x1, #0x0
  9c:	cneg	x0, x8, lt  // lt = tstop
  a0:	ldp	x29, x30, [sp], #16
  a4:	ret

00000000000000a8 <toRep>:
  a8:	str	q0, [sp, #-16]!
  ac:	ldp	x0, x1, [sp], #16
  b0:	ret

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	a8 <toRep>
  20:	ubfx	x8, x1, #48, #15
  24:	mov	w9, #0x3fff                	// #16383
  28:	cmp	w8, w9
  2c:	b.cs	3c <__fixint+0x28>  // b.hs, b.nlast
  30:	mov	w0, wzr
  34:	ldp	x29, x30, [sp], #16
  38:	ret
  3c:	mov	w9, #0xffffc001            	// #-16383
  40:	add	w9, w8, w9
  44:	cmp	w9, #0x20
  48:	b.cc	60 <__fixint+0x4c>  // b.lo, b.ul, b.last
  4c:	cmp	x1, #0x0
  50:	mov	w8, #0x80000000            	// #-2147483648
  54:	cinv	w0, w8, ge  // ge = tcont
  58:	ldp	x29, x30, [sp], #16
  5c:	ret
  60:	mov	w10, #0x406f                	// #16495
  64:	mov	x9, #0x1000000000000       	// #281474976710656
  68:	sub	w8, w10, w8
  6c:	bfxil	x9, x1, #0, #48
  70:	neg	x10, x8
  74:	cmp	x8, #0x0
  78:	lsl	x10, x9, x10
  7c:	lsr	x11, x0, x8
  80:	lsr	x9, x9, x8
  84:	sub	x8, x8, #0x40
  88:	csel	x10, xzr, x10, eq  // eq = none
  8c:	cmp	x8, #0x0
  90:	orr	x8, x11, x10
  94:	csel	x8, x9, x8, ge  // ge = tcont
  98:	cmp	x1, #0x0
  9c:	cneg	w0, w8, lt  // lt = tstop
  a0:	ldp	x29, x30, [sp], #16
  a4:	ret

00000000000000a8 <toRep>:
  a8:	str	q0, [sp, #-16]!
  ac:	ldp	x0, x1, [sp], #16
  b0:	ret

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	130 <toRep>
  20:	cmp	x1, #0x0
  24:	mov	w8, #0x1                   	// #1
  28:	ubfx	x11, x1, #48, #15
  2c:	mov	w10, #0x3fff                	// #16383
  30:	csetm	x9, lt  // lt = tstop
  34:	cneg	x8, x8, lt  // lt = tstop
  38:	cmp	w11, w10
  3c:	b.cs	50 <__fixint+0x3c>  // b.hs, b.nlast
  40:	mov	x0, xzr
  44:	mov	x1, xzr
  48:	ldp	x29, x30, [sp], #16
  4c:	ret
  50:	mov	w10, #0xffffc001            	// #-16383
  54:	add	w10, w11, w10
  58:	cmp	w10, #0x80
  5c:	b.cc	7c <__fixint+0x68>  // b.lo, b.ul, b.last
  60:	mov	x8, #0xffffffffffffffff    	// #-1
  64:	cmp	x1, #0x0
  68:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  6c:	eor	x0, x8, x1, asr #63
  70:	cinv	x1, x9, lt  // lt = tstop
  74:	ldp	x29, x30, [sp], #16
  78:	ret
  7c:	mov	x10, #0x1000000000000       	// #281474976710656
  80:	mov	w12, #0x406e                	// #16494
  84:	cmp	w11, w12
  88:	bfxil	x10, x1, #0, #48
  8c:	b.hi	e0 <__fixint+0xcc>  // b.pmore
  90:	mov	w12, #0x406f                	// #16495
  94:	sub	w11, w12, w11
  98:	neg	x12, x11
  9c:	cmp	x11, #0x0
  a0:	lsr	x13, x10, x11
  a4:	sub	x14, x11, #0x40
  a8:	lsr	x15, x0, x11
  ac:	lsr	x11, x10, x11
  b0:	lsl	x10, x10, x12
  b4:	csel	x10, xzr, x10, eq  // eq = none
  b8:	cmp	x14, #0x0
  bc:	orr	x10, x15, x10
  c0:	csel	x10, x11, x10, ge  // ge = tcont
  c4:	umulh	x11, x10, x8
  c8:	csel	x12, xzr, x13, ge  // ge = tcont
  cc:	madd	x9, x10, x9, x11
  d0:	madd	x1, x12, x8, x9
  d4:	mul	x0, x10, x8
  d8:	ldp	x29, x30, [sp], #16
  dc:	ret
  e0:	mov	w12, #0xffffbf91            	// #-16495
  e4:	add	w11, w11, w12
  e8:	neg	x12, x11
  ec:	cmp	x11, #0x0
  f0:	lsl	x10, x10, x11
  f4:	lsl	x13, x0, x11
  f8:	lsr	x12, x0, x12
  fc:	lsl	x14, x0, x11
 100:	sub	x11, x11, #0x40
 104:	csel	x12, xzr, x12, eq  // eq = none
 108:	cmp	x11, #0x0
 10c:	csel	x11, xzr, x14, ge  // ge = tcont
 110:	orr	x10, x12, x10
 114:	umulh	x12, x11, x8
 118:	csel	x10, x13, x10, ge  // ge = tcont
 11c:	madd	x9, x11, x9, x12
 120:	madd	x1, x10, x8, x9
 124:	mul	x0, x11, x8
 128:	ldp	x29, x30, [sp], #16
 12c:	ret

0000000000000130 <toRep>:
 130:	str	q0, [sp, #-16]!
 134:	ldp	x0, x1, [sp], #16
 138:	ret

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	94 <toRep>
  20:	mov	x8, xzr
  24:	tbnz	x1, #63, 88 <__fixuint+0x74>
  28:	ubfx	x9, x1, #48, #15
  2c:	mov	w10, #0x3fff                	// #16383
  30:	cmp	w9, w10
  34:	b.cc	88 <__fixuint+0x74>  // b.lo, b.ul, b.last
  38:	mov	w8, #0xffffc001            	// #-16383
  3c:	add	w8, w9, w8
  40:	cmp	w8, #0x3f
  44:	b.ls	50 <__fixuint+0x3c>  // b.plast
  48:	mov	x8, #0xffffffffffffffff    	// #-1
  4c:	b	88 <__fixuint+0x74>
  50:	mov	w10, #0x406f                	// #16495
  54:	mov	x8, #0x1000000000000       	// #281474976710656
  58:	sub	w9, w10, w9
  5c:	bfxil	x8, x1, #0, #48
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsr	x12, x8, x9
  6c:	lsl	x8, x8, x10
  70:	lsr	x11, x0, x9
  74:	sub	x9, x9, #0x40
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	orr	x8, x11, x8
  80:	cmp	x9, #0x0
  84:	csel	x8, x12, x8, ge  // ge = tcont
  88:	mov	x0, x8
  8c:	ldp	x29, x30, [sp], #16
  90:	ret

0000000000000094 <toRep>:
  94:	str	q0, [sp, #-16]!
  98:	ldp	x0, x1, [sp], #16
  9c:	ret

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	94 <toRep>
  20:	mov	w8, wzr
  24:	tbnz	x1, #63, 88 <__fixuint+0x74>
  28:	ubfx	x9, x1, #48, #15
  2c:	mov	w10, #0x3fff                	// #16383
  30:	cmp	w9, w10
  34:	b.cc	88 <__fixuint+0x74>  // b.lo, b.ul, b.last
  38:	mov	w8, #0xffffc001            	// #-16383
  3c:	add	w8, w9, w8
  40:	cmp	w8, #0x1f
  44:	b.ls	50 <__fixuint+0x3c>  // b.plast
  48:	mov	w8, #0xffffffff            	// #-1
  4c:	b	88 <__fixuint+0x74>
  50:	mov	w10, #0x406f                	// #16495
  54:	mov	x8, #0x1000000000000       	// #281474976710656
  58:	sub	w9, w10, w9
  5c:	bfxil	x8, x1, #0, #48
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsr	x12, x8, x9
  6c:	lsl	x8, x8, x10
  70:	lsr	x11, x0, x9
  74:	sub	x9, x9, #0x40
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	orr	x8, x11, x8
  80:	cmp	x9, #0x0
  84:	csel	x8, x12, x8, ge  // ge = tcont
  88:	mov	w0, w8
  8c:	ldp	x29, x30, [sp], #16
  90:	ret

0000000000000094 <toRep>:
  94:	str	q0, [sp, #-16]!
  98:	ldp	x0, x1, [sp], #16
  9c:	ret

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	f8 <toRep>
  20:	mov	x8, xzr
  24:	tbnz	x1, #63, 58 <__fixuint+0x44>
  28:	ubfx	x10, x1, #48, #15
  2c:	mov	w9, #0x3fff                	// #16383
  30:	cmp	w10, w9
  34:	mov	x9, x8
  38:	b.cc	e8 <__fixuint+0xd4>  // b.lo, b.ul, b.last
  3c:	mov	w8, #0xffffc001            	// #-16383
  40:	add	w8, w10, w8
  44:	cmp	w8, #0x7f
  48:	b.ls	60 <__fixuint+0x4c>  // b.plast
  4c:	mov	x8, #0xffffffffffffffff    	// #-1
  50:	mov	x9, #0xffffffffffffffff    	// #-1
  54:	b	e8 <__fixuint+0xd4>
  58:	mov	x9, x8
  5c:	b	e8 <__fixuint+0xd4>
  60:	mov	x8, #0x1000000000000       	// #281474976710656
  64:	mov	w9, #0x406e                	// #16494
  68:	cmp	w10, w9
  6c:	bfxil	x8, x1, #0, #48
  70:	b.hi	b0 <__fixuint+0x9c>  // b.pmore
  74:	mov	w9, #0x406f                	// #16495
  78:	sub	w10, w9, w10
  7c:	neg	x9, x10
  80:	cmp	x10, #0x0
  84:	sub	x12, x10, #0x40
  88:	lsl	x9, x8, x9
  8c:	lsr	x11, x8, x10
  90:	lsr	x13, x0, x10
  94:	csel	x14, xzr, x9, eq  // eq = none
  98:	cmp	x12, #0x0
  9c:	csel	x9, xzr, x11, ge  // ge = tcont
  a0:	orr	x11, x13, x14
  a4:	lsr	x8, x8, x10
  a8:	csel	x8, x8, x11, ge  // ge = tcont
  ac:	b	e8 <__fixuint+0xd4>
  b0:	mov	w9, #0xffffbf91            	// #-16495
  b4:	add	w9, w10, w9
  b8:	neg	x10, x9
  bc:	cmp	x9, #0x0
  c0:	lsl	x13, x8, x9
  c4:	lsr	x8, x0, x10
  c8:	sub	x12, x9, #0x40
  cc:	csel	x10, xzr, x8, eq  // eq = none
  d0:	lsl	x11, x0, x9
  d4:	cmp	x12, #0x0
  d8:	orr	x10, x10, x13
  dc:	lsl	x9, x0, x9
  e0:	csel	x8, xzr, x11, ge  // ge = tcont
  e4:	csel	x9, x9, x10, ge  // ge = tcont
  e8:	mov	x0, x8
  ec:	mov	x1, x9
  f0:	ldp	x29, x30, [sp], #16
  f4:	ret

00000000000000f8 <toRep>:
  f8:	str	q0, [sp, #-16]!
  fc:	ldp	x0, x1, [sp], #16
 100:	ret

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	cbz	x0, 58 <__floatditf+0x58>
   4:	cmp	x0, #0x0
   8:	cneg	x10, x0, mi  // mi = first
   c:	clz	x11, x10
  10:	add	w12, w11, #0x31
  14:	neg	x13, x12
  18:	cmp	x12, #0x0
  1c:	lsl	x14, x10, x12
  20:	lsr	x13, x10, x13
  24:	lsl	x10, x10, x12
  28:	sub	x12, x12, #0x40
  2c:	csel	x13, xzr, x13, eq  // eq = none
  30:	cmp	x12, #0x0
  34:	mov	w9, #0x403e                	// #16446
  38:	csel	x12, x14, x13, ge  // ge = tcont
  3c:	and	x8, x0, #0x8000000000000000
  40:	csel	x0, xzr, x10, ge  // ge = tcont
  44:	eor	x10, x12, #0x1000000000000
  48:	sub	w9, w9, w11
  4c:	add	x9, x10, x9, lsl #48
  50:	orr	x1, x9, x8
  54:	b	5c <__floatditf+0x5c>
  58:	mov	x1, xzr
  5c:	stp	x29, x30, [sp, #-16]!
  60:	mov	x29, sp
  64:	bl	70 <fromRep>
  68:	ldp	x29, x30, [sp], #16
  6c:	ret

0000000000000070 <fromRep>:
  70:	stp	x0, x1, [sp, #-16]!
  74:	ldr	q0, [sp], #16
  78:	ret

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	cbz	w0, 58 <__floatsitf+0x58>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	clz	w11, w10
  10:	add	w12, w11, #0x51
  14:	neg	x13, x12
  18:	cmp	x12, #0x0
  1c:	lsl	x14, x10, x12
  20:	lsr	x13, x10, x13
  24:	lsl	x10, x10, x12
  28:	sub	x12, x12, #0x40
  2c:	csel	x13, xzr, x13, eq  // eq = none
  30:	cmp	x12, #0x0
  34:	mov	w9, #0x401e                	// #16414
  38:	csel	x12, x14, x13, ge  // ge = tcont
  3c:	and	w8, w0, #0x80000000
  40:	csel	x0, xzr, x10, ge  // ge = tcont
  44:	eor	x10, x12, #0x1000000000000
  48:	sub	w9, w9, w11
  4c:	add	x9, x10, x9, lsl #48
  50:	orr	x1, x9, x8, lsl #32
  54:	b	60 <__floatsitf+0x60>
  58:	mov	x0, xzr
  5c:	mov	x1, xzr
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	74 <fromRep>
  6c:	ldp	x29, x30, [sp], #16
  70:	ret

0000000000000074 <fromRep>:
  74:	stp	x0, x1, [sp, #-16]!
  78:	ldr	q0, [sp], #16
  7c:	ret

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	sub	sp, sp, #0x40
   4:	stp	x29, x30, [sp, #16]
   8:	str	x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	add	x29, sp, #0x10
  14:	orr	x8, x0, x1
  18:	cbz	x8, 70 <__floattitf+0x70>
  1c:	asr	x21, x1, #63
  20:	eor	x9, x21, x0
  24:	eor	x8, x21, x1
  28:	subs	x19, x9, x21
  2c:	sbcs	x20, x8, x21
  30:	mov	x0, x19
  34:	mov	x1, x20
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x72
  4c:	sub	w8, w8, w0
  50:	b.lt	7c <__floattitf+0x7c>  // b.tstop
  54:	cmp	w9, #0x73
  58:	b.eq	130 <__floattitf+0x130>  // b.none
  5c:	cmp	w9, #0x72
  60:	b.ne	b4 <__floattitf+0xb4>  // b.any
  64:	extr	x20, x20, x19, #63
  68:	lsl	x19, x19, #1
  6c:	b	130 <__floattitf+0x130>
  70:	adrp	x8, 0 <__floattitf>
  74:	ldr	q0, [x8]
  78:	b	184 <__floattitf+0x184>
  7c:	sub	w9, w0, #0xf
  80:	neg	x10, x9
  84:	cmp	x9, #0x0
  88:	lsr	x10, x19, x10
  8c:	lsl	x11, x20, x9
  90:	sub	x13, x9, #0x40
  94:	csel	x10, xzr, x10, eq  // eq = none
  98:	lsl	x12, x19, x9
  9c:	lsl	x9, x19, x9
  a0:	cmp	x13, #0x0
  a4:	orr	x10, x10, x11
  a8:	csel	x10, x12, x10, ge  // ge = tcont
  ac:	csel	x11, xzr, x9, ge  // ge = tcont
  b0:	b	168 <__floattitf+0x168>
  b4:	mov	w10, #0xd                   	// #13
  b8:	sub	w10, w10, w0
  bc:	neg	x13, x10
  c0:	cmp	x10, #0x0
  c4:	sub	x14, x10, #0x40
  c8:	lsl	x13, x20, x13
  cc:	add	w11, w0, #0x73
  d0:	csel	x13, xzr, x13, eq  // eq = none
  d4:	cmp	x14, #0x0
  d8:	lsr	x14, x19, x10
  dc:	neg	x12, x11
  e0:	orr	x13, x14, x13
  e4:	lsr	x14, x20, x10
  e8:	lsr	x10, x20, x10
  ec:	lsl	x15, x20, x11
  f0:	csel	x10, x10, x13, ge  // ge = tcont
  f4:	lsr	x12, x19, x12
  f8:	csel	x20, xzr, x14, ge  // ge = tcont
  fc:	cmp	x11, #0x0
 100:	lsl	x13, x19, x11
 104:	lsl	x16, x19, x11
 108:	sub	x11, x11, #0x40
 10c:	csel	x12, xzr, x12, eq  // eq = none
 110:	cmp	x11, #0x0
 114:	orr	x11, x12, x15
 118:	csel	x11, x13, x11, ge  // ge = tcont
 11c:	csel	x12, xzr, x16, ge  // ge = tcont
 120:	orr	x11, x12, x11
 124:	cmp	x11, #0x0
 128:	cset	w11, ne  // ne = any
 12c:	orr	x19, x10, x11
 130:	ubfx	x10, x19, #2, #1
 134:	orr	x10, x10, x19
 138:	adds	x10, x10, #0x1
 13c:	adcs	x12, x20, xzr
 140:	mov	w11, #0x2                   	// #2
 144:	tst	x12, #0x8000000000000
 148:	cinc	x11, x11, ne  // ne = any
 14c:	lsl	x13, x12, #1
 150:	eor	x15, x11, #0x3f
 154:	lsr	x14, x10, x11
 158:	asr	x10, x12, x11
 15c:	lsl	x11, x13, x15
 160:	orr	x11, x14, x11
 164:	csel	w8, w8, w9, eq  // eq = none
 168:	mov	w12, #0x3fff                	// #16383
 16c:	and	x9, x21, #0x8000000000000000
 170:	add	w8, w8, w12
 174:	orr	x8, x9, x8, lsl #48
 178:	bfxil	x8, x10, #0, #48
 17c:	stp	x11, x8, [sp]
 180:	ldr	q0, [sp]
 184:	ldp	x20, x19, [sp, #48]
 188:	ldr	x21, [sp, #32]
 18c:	ldp	x29, x30, [sp, #16]
 190:	add	sp, sp, #0x40
 194:	ret

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	cbz	x0, 50 <__floatunditf+0x50>
   4:	clz	x8, x0
   8:	mov	w9, #0x70                  	// #112
   c:	eor	w8, w8, #0x3f
  10:	sub	w9, w9, w8
  14:	neg	x11, x9
  18:	cmp	x9, #0x0
  1c:	lsl	x12, x0, x9
  20:	lsr	x11, x0, x11
  24:	lsl	x13, x0, x9
  28:	sub	x9, x9, #0x40
  2c:	csel	x11, xzr, x11, eq  // eq = none
  30:	cmp	x9, #0x0
  34:	mov	w10, #0x3fff                	// #16383
  38:	csel	x9, x13, x11, ge  // ge = tcont
  3c:	eor	x9, x9, #0x1000000000000
  40:	add	w8, w8, w10
  44:	csel	x0, xzr, x12, ge  // ge = tcont
  48:	add	x1, x9, x8, lsl #48
  4c:	b	54 <__floatunditf+0x54>
  50:	mov	x1, xzr
  54:	stp	x29, x30, [sp, #-16]!
  58:	mov	x29, sp
  5c:	bl	68 <fromRep>
  60:	ldp	x29, x30, [sp], #16
  64:	ret

0000000000000068 <fromRep>:
  68:	stp	x0, x1, [sp, #-16]!
  6c:	ldr	q0, [sp], #16
  70:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	cbz	w0, 54 <__floatunsitf+0x54>
   4:	clz	w8, w0
   8:	mov	w9, #0x70                  	// #112
   c:	eor	w8, w8, #0x1f
  10:	sub	w9, w9, w8
  14:	mov	w10, w0
  18:	neg	x12, x9
  1c:	cmp	x9, #0x0
  20:	lsl	x13, x10, x9
  24:	lsr	x12, x10, x12
  28:	lsl	x10, x10, x9
  2c:	sub	x9, x9, #0x40
  30:	csel	x12, xzr, x12, eq  // eq = none
  34:	cmp	x9, #0x0
  38:	mov	w11, #0x3fff                	// #16383
  3c:	csel	x9, x10, x12, ge  // ge = tcont
  40:	eor	x9, x9, #0x1000000000000
  44:	add	w8, w8, w11
  48:	csel	x0, xzr, x13, ge  // ge = tcont
  4c:	add	x1, x9, x8, lsl #48
  50:	b	5c <__floatunsitf+0x5c>
  54:	mov	x0, xzr
  58:	mov	x1, xzr
  5c:	stp	x29, x30, [sp, #-16]!
  60:	mov	x29, sp
  64:	bl	70 <fromRep>
  68:	ldp	x29, x30, [sp], #16
  6c:	ret

0000000000000070 <fromRep>:
  70:	stp	x0, x1, [sp, #-16]!
  74:	ldr	q0, [sp], #16
  78:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	orr	x8, x0, x1
  14:	cbz	x8, 58 <__floatuntitf+0x58>
  18:	mov	x20, x1
  1c:	mov	x19, x0
  20:	bl	0 <__clzti2>
  24:	mov	w8, #0x80                  	// #128
  28:	sub	w9, w8, w0
  2c:	mov	w8, #0x7f                  	// #127
  30:	cmp	w9, #0x72
  34:	sub	w8, w8, w0
  38:	b.lt	64 <__floatuntitf+0x64>  // b.tstop
  3c:	cmp	w9, #0x73
  40:	b.eq	118 <__floatuntitf+0x118>  // b.none
  44:	cmp	w9, #0x72
  48:	b.ne	9c <__floatuntitf+0x9c>  // b.any
  4c:	extr	x20, x20, x19, #63
  50:	lsl	x19, x19, #1
  54:	b	118 <__floatuntitf+0x118>
  58:	adrp	x8, 0 <__floatuntitf>
  5c:	ldr	q0, [x8]
  60:	b	164 <__floatuntitf+0x164>
  64:	sub	w9, w0, #0xf
  68:	neg	x10, x9
  6c:	cmp	x9, #0x0
  70:	lsr	x10, x19, x10
  74:	lsl	x11, x20, x9
  78:	sub	x13, x9, #0x40
  7c:	csel	x10, xzr, x10, eq  // eq = none
  80:	lsl	x12, x19, x9
  84:	lsl	x9, x19, x9
  88:	cmp	x13, #0x0
  8c:	orr	x10, x10, x11
  90:	csel	x10, x12, x10, ge  // ge = tcont
  94:	csel	x11, xzr, x9, ge  // ge = tcont
  98:	b	150 <__floatuntitf+0x150>
  9c:	mov	w10, #0xd                   	// #13
  a0:	sub	w10, w10, w0
  a4:	neg	x13, x10
  a8:	cmp	x10, #0x0
  ac:	sub	x14, x10, #0x40
  b0:	lsl	x13, x20, x13
  b4:	add	w11, w0, #0x73
  b8:	csel	x13, xzr, x13, eq  // eq = none
  bc:	cmp	x14, #0x0
  c0:	lsr	x14, x19, x10
  c4:	neg	x12, x11
  c8:	orr	x13, x14, x13
  cc:	lsr	x14, x20, x10
  d0:	lsr	x10, x20, x10
  d4:	lsl	x15, x20, x11
  d8:	csel	x10, x10, x13, ge  // ge = tcont
  dc:	lsr	x12, x19, x12
  e0:	csel	x20, xzr, x14, ge  // ge = tcont
  e4:	cmp	x11, #0x0
  e8:	lsl	x13, x19, x11
  ec:	lsl	x16, x19, x11
  f0:	sub	x11, x11, #0x40
  f4:	csel	x12, xzr, x12, eq  // eq = none
  f8:	cmp	x11, #0x0
  fc:	orr	x11, x12, x15
 100:	csel	x11, x13, x11, ge  // ge = tcont
 104:	csel	x12, xzr, x16, ge  // ge = tcont
 108:	orr	x11, x12, x11
 10c:	cmp	x11, #0x0
 110:	cset	w11, ne  // ne = any
 114:	orr	x19, x10, x11
 118:	ubfx	x10, x19, #2, #1
 11c:	orr	x10, x10, x19
 120:	adds	x10, x10, #0x1
 124:	adcs	x12, x20, xzr
 128:	mov	w11, #0x2                   	// #2
 12c:	tst	x12, #0x8000000000000
 130:	cinc	x11, x11, ne  // ne = any
 134:	lsl	x13, x12, #1
 138:	eor	x15, x11, #0x3f
 13c:	lsr	x14, x10, x11
 140:	lsr	x10, x12, x11
 144:	lsl	x11, x13, x15
 148:	orr	x11, x14, x11
 14c:	csel	w8, w8, w9, eq  // eq = none
 150:	mov	w9, #0x3fff                	// #16383
 154:	add	w8, w8, w9
 158:	bfi	x10, x8, #48, #16
 15c:	stp	x11, x10, [sp]
 160:	ldr	q0, [sp]
 164:	ldp	x20, x19, [sp, #32]
 168:	ldp	x29, x30, [sp, #16]
 16c:	add	sp, sp, #0x30
 170:	ret

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x28, x19, [sp, #16]
   8:	mov	x29, sp
   c:	sub	sp, sp, #0x2d0
  10:	stp	q3, q1, [sp, #176]
  14:	mov	v1.16b, v2.16b
  18:	stp	q0, q2, [sp, #128]
  1c:	bl	0 <__multf3>
  20:	str	q0, [sp, #80]
  24:	ldp	q1, q0, [sp, #176]
  28:	bl	0 <__multf3>
  2c:	str	q0, [sp, #64]
  30:	ldr	q0, [sp, #128]
  34:	ldr	q1, [sp, #176]
  38:	bl	0 <__multf3>
  3c:	str	q0, [sp, #48]
  40:	ldr	q0, [sp, #192]
  44:	ldr	q1, [sp, #144]
  48:	bl	0 <__multf3>
  4c:	str	q0, [sp, #32]
  50:	ldp	q1, q0, [sp, #64]
  54:	bl	0 <__subtf3>
  58:	str	q0, [sp, #112]
  5c:	ldp	q0, q1, [sp, #32]
  60:	bl	0 <__addtf3>
  64:	str	q0, [sp, #96]
  68:	ldr	q0, [sp, #112]
  6c:	mov	v1.16b, v0.16b
  70:	bl	0 <__unordtf2>
  74:	cbz	w0, ec <__multc3+0xec>
  78:	ldr	q0, [sp, #96]
  7c:	mov	v1.16b, v0.16b
  80:	bl	0 <__unordtf2>
  84:	cbz	w0, ec <__multc3+0xec>
  88:	ldr	q0, [sp, #128]
  8c:	stur	q0, [x29, #-32]
  90:	ldurb	w8, [x29, #-17]
  94:	and	w8, w8, #0x7f
  98:	sturb	w8, [x29, #-17]
  9c:	ldr	q0, [sp, #192]
  a0:	stur	q0, [x29, #-16]
  a4:	ldurb	w8, [x29, #-1]
  a8:	and	w8, w8, #0x7f
  ac:	sturb	w8, [x29, #-1]
  b0:	ldp	q2, q0, [x29, #-32]
  b4:	adrp	x8, 0 <__multc3>
  b8:	ldr	q1, [x8]
  bc:	str	q0, [sp, #16]
  c0:	mov	v0.16b, v2.16b
  c4:	str	q2, [sp]
  c8:	str	q1, [sp, #160]
  cc:	bl	0 <__eqtf2>
  d0:	cbz	w0, f4 <__multc3+0xf4>
  d4:	ldr	q0, [sp, #16]
  d8:	ldr	q1, [sp, #160]
  dc:	bl	0 <__netf2>
  e0:	cbz	w0, f4 <__multc3+0xf4>
  e4:	mov	w19, wzr
  e8:	b	1f8 <__multc3+0x1f8>
  ec:	ldr	q1, [sp, #96]
  f0:	b	560 <__multc3+0x560>
  f4:	ldr	q0, [sp, #16]
  f8:	ldr	q1, [sp, #160]
  fc:	bl	0 <__eqtf2>
 100:	ldr	q0, [sp]
 104:	ldr	q1, [sp, #160]
 108:	cmp	w0, #0x0
 10c:	cset	w19, eq  // eq = none
 110:	bl	0 <__eqtf2>
 114:	cmp	w0, #0x0
 118:	adrp	x8, 0 <__multc3>
 11c:	add	x8, x8, #0x0
 120:	cset	w9, eq  // eq = none
 124:	ldr	q0, [x8, w9, uxtw #4]
 128:	ldr	q1, [sp, #128]
 12c:	stp	q0, q1, [x29, #-64]
 130:	ldurb	w9, [x29, #-33]
 134:	ldurb	w10, [x29, #-49]
 138:	ldr	q0, [x8, w19, uxtw #4]
 13c:	bfxil	w9, w10, #0, #7
 140:	sturb	w9, [x29, #-49]
 144:	ldur	q1, [x29, #-64]
 148:	adrp	x10, 0 <__multc3>
 14c:	str	q1, [sp, #128]
 150:	ldr	q1, [sp, #192]
 154:	stp	q0, q1, [x29, #-96]
 158:	ldurb	w8, [x29, #-65]
 15c:	ldurb	w9, [x29, #-81]
 160:	ldr	q1, [x10]
 164:	bfxil	w8, w9, #0, #7
 168:	sturb	w8, [x29, #-81]
 16c:	ldur	q0, [x29, #-96]
 170:	str	q0, [sp, #192]
 174:	ldr	q0, [sp, #144]
 178:	stur	q0, [x29, #-144]
 17c:	str	q1, [sp, #16]
 180:	stur	q1, [x29, #-160]
 184:	ldurb	w8, [x29, #-129]
 188:	ldurb	w9, [x29, #-145]
 18c:	bfxil	w8, w9, #0, #7
 190:	sturb	w8, [x29, #-145]
 194:	ldur	q1, [x29, #-160]
 198:	str	q1, [sp]
 19c:	mov	v1.16b, v0.16b
 1a0:	bl	0 <__unordtf2>
 1a4:	cmp	w0, #0x0
 1a8:	b.eq	1b4 <__multc3+0x1b4>  // b.none
 1ac:	ldr	q0, [sp]
 1b0:	str	q0, [sp, #144]
 1b4:	ldr	q0, [sp, #176]
 1b8:	stur	q0, [x29, #-112]
 1bc:	ldr	q1, [sp, #16]
 1c0:	ldurb	w8, [x29, #-97]
 1c4:	stur	q1, [x29, #-128]
 1c8:	ldurb	w9, [x29, #-113]
 1cc:	bfxil	w8, w9, #0, #7
 1d0:	sturb	w8, [x29, #-113]
 1d4:	ldur	q1, [x29, #-128]
 1d8:	str	q1, [sp, #16]
 1dc:	mov	v1.16b, v0.16b
 1e0:	bl	0 <__unordtf2>
 1e4:	cmp	w0, #0x0
 1e8:	b.eq	1f4 <__multc3+0x1f4>  // b.none
 1ec:	ldr	q0, [sp, #16]
 1f0:	str	q0, [sp, #176]
 1f4:	mov	w19, #0x1                   	// #1
 1f8:	ldp	q0, q1, [sp, #144]
 1fc:	stur	q0, [x29, #-192]
 200:	ldurb	w8, [x29, #-177]
 204:	and	w8, w8, #0x7f
 208:	sturb	w8, [x29, #-177]
 20c:	ldur	q0, [x29, #-192]
 210:	str	q0, [sp, #16]
 214:	ldr	q0, [sp, #176]
 218:	stur	q0, [x29, #-176]
 21c:	ldurb	w8, [x29, #-161]
 220:	and	w8, w8, #0x7f
 224:	sturb	w8, [x29, #-161]
 228:	ldur	q0, [x29, #-176]
 22c:	str	q0, [sp]
 230:	bl	0 <__eqtf2>
 234:	cbz	w0, 248 <__multc3+0x248>
 238:	ldr	q0, [sp, #16]
 23c:	ldr	q1, [sp, #160]
 240:	bl	0 <__netf2>
 244:	cbnz	w0, 344 <__multc3+0x344>
 248:	ldr	q0, [sp]
 24c:	ldr	q1, [sp, #160]
 250:	bl	0 <__eqtf2>
 254:	ldr	q0, [sp, #16]
 258:	ldr	q1, [sp, #160]
 25c:	cmp	w0, #0x0
 260:	cset	w19, eq  // eq = none
 264:	bl	0 <__eqtf2>
 268:	cmp	w0, #0x0
 26c:	adrp	x8, 0 <__multc3>
 270:	add	x8, x8, #0x0
 274:	cset	w9, eq  // eq = none
 278:	ldr	q0, [x8, w9, uxtw #4]
 27c:	ldr	q1, [sp, #144]
 280:	stp	q0, q1, [x29, #-224]
 284:	ldurb	w9, [x29, #-193]
 288:	ldurb	w10, [x29, #-209]
 28c:	ldr	q0, [x8, w19, uxtw #4]
 290:	bfxil	w9, w10, #0, #7
 294:	sturb	w9, [x29, #-209]
 298:	ldur	q1, [x29, #-224]
 29c:	adrp	x10, 0 <__multc3>
 2a0:	str	q1, [sp, #144]
 2a4:	ldr	q1, [sp, #176]
 2a8:	stp	q0, q1, [x29, #-256]
 2ac:	ldurb	w8, [x29, #-225]
 2b0:	ldurb	w9, [x29, #-241]
 2b4:	ldr	q1, [x10]
 2b8:	bfxil	w8, w9, #0, #7
 2bc:	sturb	w8, [x29, #-241]
 2c0:	ldur	q0, [x29, #-256]
 2c4:	str	q1, [sp, #16]
 2c8:	str	q0, [sp, #176]
 2cc:	ldr	q0, [sp, #128]
 2d0:	stp	q1, q0, [sp, #400]
 2d4:	ldrb	w8, [sp, #431]
 2d8:	ldrb	w9, [sp, #415]
 2dc:	bfxil	w8, w9, #0, #7
 2e0:	strb	w8, [sp, #415]
 2e4:	ldr	q1, [sp, #400]
 2e8:	str	q1, [sp]
 2ec:	mov	v1.16b, v0.16b
 2f0:	bl	0 <__unordtf2>
 2f4:	cmp	w0, #0x0
 2f8:	b.eq	304 <__multc3+0x304>  // b.none
 2fc:	ldr	q0, [sp]
 300:	str	q0, [sp, #128]
 304:	ldr	q0, [sp, #192]
 308:	ldr	q1, [sp, #16]
 30c:	stp	q1, q0, [sp, #432]
 310:	ldrb	w8, [sp, #463]
 314:	ldrb	w9, [sp, #447]
 318:	bfxil	w8, w9, #0, #7
 31c:	strb	w8, [sp, #447]
 320:	ldr	q1, [sp, #432]
 324:	str	q1, [sp, #16]
 328:	mov	v1.16b, v0.16b
 32c:	bl	0 <__unordtf2>
 330:	cmp	w0, #0x0
 334:	b.eq	340 <__multc3+0x340>  // b.none
 338:	ldr	q0, [sp, #16]
 33c:	str	q0, [sp, #192]
 340:	mov	w19, #0x1                   	// #1
 344:	ldr	q2, [sp, #160]
 348:	ldr	q1, [sp, #96]
 34c:	cbnz	w19, 500 <__multc3+0x500>
 350:	ldr	q0, [sp, #80]
 354:	mov	v1.16b, v2.16b
 358:	str	q0, [sp, #336]
 35c:	ldrb	w8, [sp, #351]
 360:	and	w8, w8, #0x7f
 364:	strb	w8, [sp, #351]
 368:	ldr	q0, [sp, #336]
 36c:	str	q0, [sp, #16]
 370:	ldr	q0, [sp, #64]
 374:	str	q0, [sp, #352]
 378:	ldrb	w8, [sp, #367]
 37c:	and	w8, w8, #0x7f
 380:	strb	w8, [sp, #367]
 384:	ldr	q0, [sp, #352]
 388:	str	q0, [sp, #64]
 38c:	ldr	q0, [sp, #48]
 390:	str	q0, [sp, #368]
 394:	ldrb	w8, [sp, #383]
 398:	and	w8, w8, #0x7f
 39c:	strb	w8, [sp, #383]
 3a0:	ldr	q0, [sp, #368]
 3a4:	str	q0, [sp, #80]
 3a8:	ldr	q0, [sp, #32]
 3ac:	str	q0, [sp, #384]
 3b0:	ldrb	w8, [sp, #399]
 3b4:	and	w8, w8, #0x7f
 3b8:	strb	w8, [sp, #399]
 3bc:	ldr	q0, [sp, #384]
 3c0:	bl	0 <__eqtf2>
 3c4:	cbz	w0, 400 <__multc3+0x400>
 3c8:	ldr	q0, [sp, #80]
 3cc:	ldr	q1, [sp, #160]
 3d0:	bl	0 <__eqtf2>
 3d4:	cbz	w0, 400 <__multc3+0x400>
 3d8:	ldr	q0, [sp, #16]
 3dc:	ldr	q1, [sp, #160]
 3e0:	bl	0 <__eqtf2>
 3e4:	cbz	w0, 400 <__multc3+0x400>
 3e8:	ldr	q0, [sp, #64]
 3ec:	ldr	q1, [sp, #160]
 3f0:	bl	0 <__netf2>
 3f4:	cbz	w0, 400 <__multc3+0x400>
 3f8:	mov	w19, wzr
 3fc:	b	4fc <__multc3+0x4fc>
 400:	adrp	x8, 0 <__multc3>
 404:	ldr	q1, [x8]
 408:	ldr	q0, [sp, #128]
 40c:	str	q1, [sp, #80]
 410:	stp	q1, q0, [sp, #208]
 414:	ldrb	w8, [sp, #239]
 418:	ldrb	w9, [sp, #223]
 41c:	bfxil	w8, w9, #0, #7
 420:	strb	w8, [sp, #223]
 424:	ldr	q1, [sp, #208]
 428:	str	q1, [sp, #64]
 42c:	mov	v1.16b, v0.16b
 430:	bl	0 <__unordtf2>
 434:	cmp	w0, #0x0
 438:	b.eq	444 <__multc3+0x444>  // b.none
 43c:	ldr	q0, [sp, #64]
 440:	str	q0, [sp, #128]
 444:	ldr	q0, [sp, #192]
 448:	ldr	q1, [sp, #80]
 44c:	stp	q1, q0, [sp, #240]
 450:	ldrb	w8, [sp, #271]
 454:	ldrb	w9, [sp, #255]
 458:	bfxil	w8, w9, #0, #7
 45c:	strb	w8, [sp, #255]
 460:	ldr	q1, [sp, #240]
 464:	str	q1, [sp, #64]
 468:	mov	v1.16b, v0.16b
 46c:	bl	0 <__unordtf2>
 470:	cmp	w0, #0x0
 474:	b.eq	480 <__multc3+0x480>  // b.none
 478:	ldr	q0, [sp, #64]
 47c:	str	q0, [sp, #192]
 480:	ldr	q0, [sp, #144]
 484:	ldr	q1, [sp, #80]
 488:	stp	q1, q0, [sp, #272]
 48c:	ldrb	w8, [sp, #303]
 490:	ldrb	w9, [sp, #287]
 494:	bfxil	w8, w9, #0, #7
 498:	strb	w8, [sp, #287]
 49c:	ldr	q1, [sp, #272]
 4a0:	str	q1, [sp, #64]
 4a4:	mov	v1.16b, v0.16b
 4a8:	bl	0 <__unordtf2>
 4ac:	cmp	w0, #0x0
 4b0:	b.eq	4bc <__multc3+0x4bc>  // b.none
 4b4:	ldr	q0, [sp, #64]
 4b8:	str	q0, [sp, #144]
 4bc:	ldr	q0, [sp, #176]
 4c0:	ldr	q1, [sp, #80]
 4c4:	stp	q1, q0, [sp, #304]
 4c8:	ldrb	w8, [sp, #335]
 4cc:	ldrb	w9, [sp, #319]
 4d0:	bfxil	w8, w9, #0, #7
 4d4:	strb	w8, [sp, #319]
 4d8:	ldr	q1, [sp, #304]
 4dc:	str	q1, [sp, #80]
 4e0:	mov	v1.16b, v0.16b
 4e4:	bl	0 <__unordtf2>
 4e8:	cmp	w0, #0x0
 4ec:	b.eq	4f8 <__multc3+0x4f8>  // b.none
 4f0:	ldr	q0, [sp, #80]
 4f4:	str	q0, [sp, #176]
 4f8:	mov	w19, #0x1                   	// #1
 4fc:	ldr	q1, [sp, #96]
 500:	cbz	w19, 560 <__multc3+0x560>
 504:	ldp	q1, q0, [sp, #128]
 508:	bl	0 <__multf3>
 50c:	str	q0, [sp, #112]
 510:	ldp	q0, q1, [sp, #176]
 514:	bl	0 <__multf3>
 518:	mov	v1.16b, v0.16b
 51c:	ldr	q0, [sp, #112]
 520:	bl	0 <__subtf3>
 524:	ldr	q1, [sp, #160]
 528:	bl	0 <__multf3>
 52c:	str	q0, [sp, #112]
 530:	ldr	q0, [sp, #176]
 534:	ldr	q1, [sp, #128]
 538:	bl	0 <__multf3>
 53c:	str	q0, [sp, #176]
 540:	ldr	q0, [sp, #144]
 544:	ldr	q1, [sp, #192]
 548:	bl	0 <__multf3>
 54c:	ldr	q1, [sp, #176]
 550:	bl	0 <__addtf3>
 554:	ldr	q1, [sp, #160]
 558:	bl	0 <__multf3>
 55c:	mov	v1.16b, v0.16b
 560:	ldr	q0, [sp, #112]
 564:	add	sp, sp, #0x2d0
 568:	ldp	x28, x19, [sp, #16]
 56c:	ldp	x29, x30, [sp], #32
 570:	ret

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__truncXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__truncXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	19c <srcToRep>
  20:	and	x8, x1, #0x7fffffffffffffff
  24:	mov	x9, #0xc3ff000000000000    	// #-4323737117252386816
  28:	mov	x10, #0xbc01000000000000    	// #-4899634919602388992
  2c:	add	x9, x8, x9
  30:	add	x10, x8, x10
  34:	cmp	x9, x10
  38:	b.cs	64 <__truncXfYf2__+0x50>  // b.hs, b.nlast
  3c:	mov	x8, #0x1                   	// #1
  40:	and	x9, x0, #0xfffffffffffffff
  44:	movk	x8, #0x800, lsl #48
  48:	cmp	x9, x8
  4c:	extr	x8, x1, x0, #60
  50:	b.cc	90 <__truncXfYf2__+0x7c>  // b.lo, b.ul, b.last
  54:	mov	x9, #0x1                   	// #1
  58:	movk	x9, #0x4000, lsl #48
  5c:	add	x8, x8, x9
  60:	b	188 <__truncXfYf2__+0x174>
  64:	cmp	x0, #0x0
  68:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
  6c:	cset	w10, eq  // eq = none
  70:	cmp	x8, x9
  74:	cset	w9, cc  // cc = lo, ul, last
  78:	csel	w9, w10, w9, eq  // eq = none
  7c:	tbnz	w9, #0, ac <__truncXfYf2__+0x98>
  80:	extr	x9, x1, x0, #60
  84:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
  88:	bfxil	x8, x9, #0, #51
  8c:	b	188 <__truncXfYf2__+0x174>
  90:	mov	x10, #0x4000000000000000    	// #4611686018427387904
  94:	eor	x9, x9, #0x800000000000000
  98:	add	x8, x8, x10
  9c:	cbnz	x9, 188 <__truncXfYf2__+0x174>
  a0:	and	x9, x8, #0x1
  a4:	add	x8, x9, x8
  a8:	b	188 <__truncXfYf2__+0x174>
  ac:	mov	x9, #0x43feffffffffffff    	// #4899634919602388991
  b0:	cmp	x8, x9
  b4:	b.ls	c0 <__truncXfYf2__+0xac>  // b.plast
  b8:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  bc:	b	188 <__truncXfYf2__+0x174>
  c0:	lsr	x8, x8, #48
  c4:	mov	w9, #0x3b91                	// #15249
  c8:	cmp	w8, w9
  cc:	b.cs	d8 <__truncXfYf2__+0xc4>  // b.hs, b.nlast
  d0:	mov	x8, xzr
  d4:	b	188 <__truncXfYf2__+0x174>
  d8:	mov	w10, #0x3c01                	// #15361
  dc:	mov	w11, #0xffffc47f            	// #-15233
  e0:	sub	w10, w10, w8
  e4:	add	w8, w8, w11
  e8:	mov	x9, #0x1000000000000       	// #281474976710656
  ec:	neg	x11, x8
  f0:	bfxil	x9, x1, #0, #48
  f4:	cmp	x8, #0x0
  f8:	sub	x13, x8, #0x40
  fc:	lsr	x11, x0, x11
 100:	csel	x11, xzr, x11, eq  // eq = none
 104:	cmp	x13, #0x0
 108:	lsl	x13, x9, x8
 10c:	lsl	x12, x0, x8
 110:	lsl	x8, x0, x8
 114:	orr	x11, x11, x13
 118:	csel	x8, x8, x11, ge  // ge = tcont
 11c:	csel	x12, xzr, x12, ge  // ge = tcont
 120:	orr	x8, x12, x8
 124:	neg	x13, x10
 128:	cmp	x8, #0x0
 12c:	lsl	x13, x9, x13
 130:	cset	w8, ne  // ne = any
 134:	cmp	x10, #0x0
 138:	lsr	x11, x9, x10
 13c:	sub	x12, x10, #0x40
 140:	lsr	x14, x0, x10
 144:	lsr	x9, x9, x10
 148:	csel	x10, xzr, x13, eq  // eq = none
 14c:	cmp	x12, #0x0
 150:	orr	x10, x14, x10
 154:	csel	x10, x9, x10, ge  // ge = tcont
 158:	and	x9, x10, #0xfffffffffffffff
 15c:	orr	x9, x9, x8
 160:	mov	x8, #0x1                   	// #1
 164:	csel	x11, xzr, x11, ge  // ge = tcont
 168:	movk	x8, #0x800, lsl #48
 16c:	cmp	x9, x8
 170:	extr	x8, x11, x10, #60
 174:	b.cc	180 <__truncXfYf2__+0x16c>  // b.lo, b.ul, b.last
 178:	add	x8, x8, #0x1
 17c:	b	188 <__truncXfYf2__+0x174>
 180:	eor	x9, x9, #0x800000000000000
 184:	cbz	x9, a0 <__truncXfYf2__+0x8c>
 188:	and	x9, x1, #0x8000000000000000
 18c:	orr	x0, x8, x9
 190:	bl	1a8 <dstFromRep>
 194:	ldp	x29, x30, [sp], #16
 198:	ret

000000000000019c <srcToRep>:
 19c:	str	q0, [sp, #-16]!
 1a0:	ldp	x0, x1, [sp], #16
 1a4:	ret

00000000000001a8 <dstFromRep>:
 1a8:	fmov	d0, x0
 1ac:	ret

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__truncXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__truncXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	1cc <srcToRep>
  20:	and	x8, x1, #0x7fffffffffffffff
  24:	mov	x9, #0xc07f000000000000    	// #-4575938696385134592
  28:	mov	x10, #0xbf81000000000000    	// #-4647433340469641216
  2c:	add	x9, x8, x9
  30:	add	x10, x8, x10
  34:	cmp	x9, x10
  38:	b.cs	78 <__truncXfYf2__+0x64>  // b.hs, b.nlast
  3c:	cmp	x0, #0x0
  40:	ubfx	x9, x1, #24, #1
  44:	and	x8, x1, #0x1ffffff
  48:	mov	w10, #0x1000000             	// #16777216
  4c:	cset	w11, eq  // eq = none
  50:	cmp	x9, #0x0
  54:	cset	w9, eq  // eq = none
  58:	cmp	x8, x10
  5c:	csel	w10, w11, w9, eq  // eq = none
  60:	lsr	x9, x1, #25
  64:	tbnz	w10, #0, a0 <__truncXfYf2__+0x8c>
  68:	mov	w8, #0x1                   	// #1
  6c:	movk	w8, #0x4000, lsl #16
  70:	add	w8, w9, w8
  74:	b	1b4 <__truncXfYf2__+0x1a0>
  78:	cmp	x0, #0x0
  7c:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
  80:	cset	w10, eq  // eq = none
  84:	cmp	x8, x9
  88:	cset	w9, cc  // cc = lo, ul, last
  8c:	csel	w9, w10, w9, eq  // eq = none
  90:	tbnz	w9, #0, c0 <__truncXfYf2__+0xac>
  94:	ubfx	x8, x1, #25, #22
  98:	orr	w8, w8, #0x7fc00000
  9c:	b	1b4 <__truncXfYf2__+0x1a0>
  a0:	mov	w10, #0x40000000            	// #1073741824
  a4:	eor	x8, x8, #0x1000000
  a8:	orr	x11, x0, x8
  ac:	add	w8, w9, w10
  b0:	cbnz	x11, 1b4 <__truncXfYf2__+0x1a0>
  b4:	and	w9, w8, #0x1
  b8:	add	w8, w9, w8
  bc:	b	1b4 <__truncXfYf2__+0x1a0>
  c0:	mov	x9, #0x407effffffffffff    	// #4647433340469641215
  c4:	cmp	x8, x9
  c8:	b.ls	d4 <__truncXfYf2__+0xc0>  // b.plast
  cc:	mov	w8, #0x7f800000            	// #2139095040
  d0:	b	1b4 <__truncXfYf2__+0x1a0>
  d4:	lsr	x8, x8, #48
  d8:	mov	w9, #0x3f11                	// #16145
  dc:	cmp	w8, w9
  e0:	b.cs	ec <__truncXfYf2__+0xd8>  // b.hs, b.nlast
  e4:	mov	w8, wzr
  e8:	b	1b4 <__truncXfYf2__+0x1a0>
  ec:	mov	w10, #0x3f81                	// #16257
  f0:	mov	w11, #0xffffc0ff            	// #-16129
  f4:	sub	w10, w10, w8
  f8:	add	w8, w8, w11
  fc:	mov	x9, #0x1000000000000       	// #281474976710656
 100:	neg	x11, x8
 104:	bfxil	x9, x1, #0, #48
 108:	cmp	x8, #0x0
 10c:	sub	x13, x8, #0x40
 110:	lsr	x11, x0, x11
 114:	csel	x11, xzr, x11, eq  // eq = none
 118:	cmp	x13, #0x0
 11c:	lsl	x13, x9, x8
 120:	orr	x11, x11, x13
 124:	lsl	x13, x0, x8
 128:	lsl	x8, x0, x8
 12c:	csel	x8, x8, x11, ge  // ge = tcont
 130:	csel	x13, xzr, x13, ge  // ge = tcont
 134:	orr	x8, x13, x8
 138:	neg	x11, x10
 13c:	cmp	x8, #0x0
 140:	lsl	x11, x9, x11
 144:	cset	w8, ne  // ne = any
 148:	cmp	x10, #0x0
 14c:	lsr	x14, x0, x10
 150:	lsr	x13, x9, x10
 154:	lsr	x9, x9, x10
 158:	sub	x10, x10, #0x40
 15c:	csel	x11, xzr, x11, eq  // eq = none
 160:	cmp	x10, #0x0
 164:	orr	x10, x14, x11
 168:	csel	x10, x13, x10, ge  // ge = tcont
 16c:	csel	x11, xzr, x9, ge  // ge = tcont
 170:	orr	x10, x10, x8
 174:	ubfx	x13, x11, #24, #1
 178:	cmp	x10, #0x0
 17c:	mov	w12, #0x1000000             	// #16777216
 180:	and	x9, x11, #0x1ffffff
 184:	cset	w8, eq  // eq = none
 188:	cmp	x13, #0x0
 18c:	cset	w13, eq  // eq = none
 190:	cmp	x9, x12
 194:	csel	w12, w8, w13, eq  // eq = none
 198:	lsr	x8, x11, #25
 19c:	tbnz	w12, #0, 1a8 <__truncXfYf2__+0x194>
 1a0:	add	w8, w8, #0x1
 1a4:	b	1b4 <__truncXfYf2__+0x1a0>
 1a8:	eor	x9, x9, #0x1000000
 1ac:	orr	x9, x10, x9
 1b0:	cbz	x9, b4 <__truncXfYf2__+0xa0>
 1b4:	lsr	x9, x1, #32
 1b8:	and	w9, w9, #0x80000000
 1bc:	orr	w0, w8, w9
 1c0:	bl	1d8 <dstFromRep>
 1c4:	ldp	x29, x30, [sp], #16
 1c8:	ret

00000000000001cc <srcToRep>:
 1cc:	str	q0, [sp, #-16]!
 1d0:	ldp	x0, x1, [sp], #16
 1d4:	ret

00000000000001d8 <dstFromRep>:
 1d8:	fmov	s0, w0
 1dc:	ret

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	18 <__absvdi2+0x18>  // b.none
   c:	cmp	x0, #0x0
  10:	cneg	x0, x0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvdi2>
  24:	adrp	x2, 0 <__absvdi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	18 <__absvsi2+0x18>  // b.none
   c:	cmp	w0, #0x0
  10:	cneg	w0, w0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvsi2>
  24:	adrp	x2, 0 <__absvsi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 24 <__absvti2+0x24>
   c:	negs	x8, x0
  10:	ngcs	x9, x1
  14:	cmp	x1, #0x0
  18:	csel	x0, x8, x0, lt  // lt = tstop
  1c:	csel	x1, x9, x1, lt  // lt = tstop
  20:	ret
  24:	stp	x29, x30, [sp, #-16]!
  28:	mov	x29, sp
  2c:	adrp	x0, 0 <__absvti2>
  30:	adrp	x2, 0 <__absvti2>
  34:	add	x0, x0, #0x0
  38:	add	x2, x2, #0x0
  3c:	mov	w1, #0x18                  	// #24
  40:	bl	0 <__compilerrt_abort_impl>

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__addXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__addXf3__>:
  14:	sub	sp, sp, #0x50
  18:	stp	d9, d8, [sp, #16]
  1c:	stp	x29, x30, [sp, #32]
  20:	stp	x22, x21, [sp, #48]
  24:	stp	x20, x19, [sp, #64]
  28:	add	x29, sp, #0x10
  2c:	mov	v9.16b, v1.16b
  30:	mov	v8.16b, v0.16b
  34:	bl	2cc <toRep>
  38:	mov	v0.16b, v9.16b
  3c:	mov	x19, x0
  40:	bl	2cc <toRep>
  44:	and	x8, x19, #0x7fffffffffffffff
  48:	mov	x10, #0xfffffffffffffffe    	// #-2
  4c:	sub	x9, x8, #0x1
  50:	movk	x10, #0x7fef, lsl #48
  54:	cmp	x9, x10
  58:	and	x9, x0, #0x7fffffffffffffff
  5c:	b.hi	e0 <__addXf3__+0xcc>  // b.pmore
  60:	sub	x11, x9, #0x1
  64:	cmp	x11, x10
  68:	b.hi	e0 <__addXf3__+0xcc>  // b.pmore
  6c:	cmp	x9, x8
  70:	csel	x21, x19, x0, hi  // hi = pmore
  74:	csel	x22, x0, x19, hi  // hi = pmore
  78:	ubfx	x19, x22, #52, #11
  7c:	ubfx	x20, x21, #52, #11
  80:	and	x8, x22, #0xfffffffffffff
  84:	and	x9, x21, #0xfffffffffffff
  88:	stp	x9, x8, [sp]
  8c:	cbz	w19, 108 <__addXf3__+0xf4>
  90:	cbz	w20, 118 <__addXf3__+0x104>
  94:	ldp	x10, x9, [sp]
  98:	eor	x8, x22, x21
  9c:	subs	w11, w19, w20
  a0:	lsl	x9, x9, #3
  a4:	lsl	x10, x10, #3
  a8:	orr	x9, x9, #0x80000000000000
  ac:	orr	x10, x10, #0x80000000000000
  b0:	stp	x10, x9, [sp]
  b4:	b.eq	130 <__addXf3__+0x11c>  // b.none
  b8:	cmp	w11, #0x3f
  bc:	b.hi	128 <__addXf3__+0x114>  // b.pmore
  c0:	mov	w11, w11
  c4:	neg	x12, x11
  c8:	lsl	x12, x10, x12
  cc:	cmp	x12, #0x0
  d0:	cset	w12, ne  // ne = any
  d4:	lsr	x10, x10, x11
  d8:	orr	x10, x10, x12
  dc:	b	12c <__addXf3__+0x118>
  e0:	mov	x10, #0x1                   	// #1
  e4:	movk	x10, #0x7ff0, lsl #48
  e8:	cmp	x8, x10
  ec:	b.cc	f8 <__addXf3__+0xe4>  // b.lo, b.ul, b.last
  f0:	orr	x0, x19, #0x8000000000000
  f4:	b	28c <__addXf3__+0x278>
  f8:	cmp	x9, x10
  fc:	b.cc	220 <__addXf3__+0x20c>  // b.lo, b.ul, b.last
 100:	orr	x0, x0, #0x8000000000000
 104:	b	28c <__addXf3__+0x278>
 108:	add	x0, sp, #0x8
 10c:	bl	2dc <normalize>
 110:	mov	w19, w0
 114:	cbnz	w20, 94 <__addXf3__+0x80>
 118:	mov	x0, sp
 11c:	bl	2dc <normalize>
 120:	mov	w20, w0
 124:	b	94 <__addXf3__+0x80>
 128:	mov	w10, #0x1                   	// #1
 12c:	str	x10, [sp]
 130:	ldr	x10, [sp]
 134:	tbnz	x8, #63, 158 <__addXf3__+0x144>
 138:	add	x8, x10, x9
 13c:	str	x8, [sp, #8]
 140:	tbz	x8, #56, 190 <__addXf3__+0x17c>
 144:	and	x9, x8, #0x1
 148:	orr	x8, x9, x8, lsr #1
 14c:	add	w19, w19, #0x1
 150:	str	x8, [sp, #8]
 154:	b	190 <__addXf3__+0x17c>
 158:	subs	x20, x9, x10
 15c:	str	x20, [sp, #8]
 160:	b.eq	244 <__addXf3__+0x230>  // b.none
 164:	lsr	x8, x20, #55
 168:	cbnz	x8, 190 <__addXf3__+0x17c>
 16c:	mov	x0, x20
 170:	bl	32c <rep_clz>
 174:	mov	w21, w0
 178:	mov	x0, #0x80000000000000      	// #36028797018963968
 17c:	bl	32c <rep_clz>
 180:	sub	w8, w21, w0
 184:	lsl	x9, x20, x8
 188:	str	x9, [sp, #8]
 18c:	sub	w19, w19, w8
 190:	cmp	w19, #0x7ff
 194:	and	x21, x22, #0x8000000000000000
 198:	b.lt	1a4 <__addXf3__+0x190>  // b.tstop
 19c:	orr	x19, x21, #0x7ff0000000000000
 1a0:	b	288 <__addXf3__+0x274>
 1a4:	cmp	w19, #0x0
 1a8:	b.gt	1dc <__addXf3__+0x1c8>
 1ac:	mov	w8, #0x1                   	// #1
 1b0:	ldr	x9, [sp, #8]
 1b4:	sub	w8, w8, w19
 1b8:	sxtw	x10, w8
 1bc:	neg	x10, x10
 1c0:	lsl	x10, x9, x10
 1c4:	cmp	x10, #0x0
 1c8:	cset	w10, ne  // ne = any
 1cc:	lsr	x8, x9, x8
 1d0:	orr	x8, x8, x10
 1d4:	mov	w19, wzr
 1d8:	str	x8, [sp, #8]
 1dc:	ldr	x8, [sp, #8]
 1e0:	orr	x9, x21, x19, lsl #52
 1e4:	and	w20, w8, #0x7
 1e8:	ubfx	x8, x8, #3, #52
 1ec:	orr	x19, x9, x8
 1f0:	bl	0 <__fe_getround>
 1f4:	cmp	w0, #0x2
 1f8:	b.eq	25c <__addXf3__+0x248>  // b.none
 1fc:	cmp	w0, #0x1
 200:	b.eq	268 <__addXf3__+0x254>  // b.none
 204:	cbnz	w0, 280 <__addXf3__+0x26c>
 208:	cmp	w20, #0x4
 20c:	cinc	x19, x19, hi  // hi = pmore
 210:	b.ne	280 <__addXf3__+0x26c>  // b.any
 214:	and	x8, x19, #0x1
 218:	add	x19, x8, x19
 21c:	b	280 <__addXf3__+0x26c>
 220:	mov	x10, #0x7ff0000000000000    	// #9218868437227405312
 224:	cmp	x8, x10
 228:	b.ne	24c <__addXf3__+0x238>  // b.any
 22c:	eor	x8, x0, x19
 230:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
 234:	cmp	x8, x9
 238:	b.ne	294 <__addXf3__+0x280>  // b.any
 23c:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 240:	b	28c <__addXf3__+0x278>
 244:	mov	x19, xzr
 248:	b	288 <__addXf3__+0x274>
 24c:	cmp	x9, x10
 250:	b.ne	2b0 <__addXf3__+0x29c>  // b.any
 254:	mov	v8.16b, v9.16b
 258:	b	294 <__addXf3__+0x280>
 25c:	cmp	x21, #0x0
 260:	cset	w8, eq  // eq = none
 264:	b	270 <__addXf3__+0x25c>
 268:	cmp	x21, #0x0
 26c:	cset	w8, ne  // ne = any
 270:	cmp	w20, #0x0
 274:	cset	w9, ne  // ne = any
 278:	and	w8, w8, w9
 27c:	add	x19, x19, x8
 280:	cbz	w20, 288 <__addXf3__+0x274>
 284:	bl	0 <__fe_raise_inexact>
 288:	mov	x0, x19
 28c:	bl	2d4 <fromRep>
 290:	mov	v8.16b, v0.16b
 294:	mov	v0.16b, v8.16b
 298:	ldp	x20, x19, [sp, #64]
 29c:	ldp	x22, x21, [sp, #48]
 2a0:	ldp	x29, x30, [sp, #32]
 2a4:	ldp	d9, d8, [sp, #16]
 2a8:	add	sp, sp, #0x50
 2ac:	ret
 2b0:	cbz	x8, 2bc <__addXf3__+0x2a8>
 2b4:	cbnz	x9, 6c <__addXf3__+0x58>
 2b8:	b	294 <__addXf3__+0x280>
 2bc:	mov	v8.16b, v9.16b
 2c0:	cbnz	x9, 294 <__addXf3__+0x280>
 2c4:	and	x0, x0, x19
 2c8:	b	28c <__addXf3__+0x278>

00000000000002cc <toRep>:
 2cc:	fmov	x0, d0
 2d0:	ret

00000000000002d4 <fromRep>:
 2d4:	fmov	d0, x0
 2d8:	ret

00000000000002dc <normalize>:
 2dc:	stp	x29, x30, [sp, #-48]!
 2e0:	str	x21, [sp, #16]
 2e4:	stp	x20, x19, [sp, #32]
 2e8:	mov	x29, sp
 2ec:	ldr	x20, [x0]
 2f0:	mov	x19, x0
 2f4:	mov	x0, x20
 2f8:	bl	32c <rep_clz>
 2fc:	mov	w21, w0
 300:	mov	x0, #0x10000000000000      	// #4503599627370496
 304:	bl	32c <rep_clz>
 308:	sub	w8, w21, w0
 30c:	lsl	x10, x20, x8
 310:	str	x10, [x19]
 314:	ldp	x20, x19, [sp, #32]
 318:	ldr	x21, [sp, #16]
 31c:	mov	w9, #0x1                   	// #1
 320:	sub	w0, w9, w8
 324:	ldp	x29, x30, [sp], #48
 328:	ret

000000000000032c <rep_clz>:
 32c:	clz	x0, x0
 330:	ret

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__addXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__addXf3__>:
  14:	sub	sp, sp, #0x50
  18:	stp	d9, d8, [sp, #16]
  1c:	stp	x29, x30, [sp, #32]
  20:	stp	x22, x21, [sp, #48]
  24:	stp	x20, x19, [sp, #64]
  28:	add	x29, sp, #0x10
  2c:	mov	v9.16b, v1.16b
  30:	mov	v8.16b, v0.16b
  34:	bl	2d0 <toRep>
  38:	mov	v0.16b, v9.16b
  3c:	mov	w19, w0
  40:	bl	2d0 <toRep>
  44:	and	w8, w19, #0x7fffffff
  48:	mov	w10, #0xfffe                	// #65534
  4c:	sub	w9, w8, #0x1
  50:	movk	w10, #0x7f7f, lsl #16
  54:	cmp	w9, w10
  58:	and	w9, w0, #0x7fffffff
  5c:	b.hi	e8 <__addXf3__+0xd4>  // b.pmore
  60:	sub	w11, w9, #0x1
  64:	cmp	w11, w10
  68:	b.hi	e8 <__addXf3__+0xd4>  // b.pmore
  6c:	cmp	w9, w8
  70:	csel	w21, w19, w0, hi  // hi = pmore
  74:	csel	w22, w0, w19, hi  // hi = pmore
  78:	ubfx	w19, w22, #23, #8
  7c:	ubfx	w20, w21, #23, #8
  80:	and	w8, w22, #0x7fffff
  84:	and	w9, w21, #0x7fffff
  88:	stur	w8, [x29, #-4]
  8c:	str	w9, [sp, #8]
  90:	cbz	w19, 110 <__addXf3__+0xfc>
  94:	cbz	w20, 120 <__addXf3__+0x10c>
  98:	ldur	w9, [x29, #-4]
  9c:	ldr	w11, [sp, #8]
  a0:	eor	w8, w22, w21
  a4:	subs	w10, w19, w20
  a8:	lsl	w9, w9, #3
  ac:	lsl	w11, w11, #3
  b0:	orr	w9, w9, #0x4000000
  b4:	orr	w11, w11, #0x4000000
  b8:	stur	w9, [x29, #-4]
  bc:	str	w11, [sp, #8]
  c0:	b.eq	138 <__addXf3__+0x124>  // b.none
  c4:	cmp	w10, #0x1f
  c8:	b.hi	130 <__addXf3__+0x11c>  // b.pmore
  cc:	neg	w12, w10
  d0:	lsl	w12, w11, w12
  d4:	cmp	w12, #0x0
  d8:	cset	w12, ne  // ne = any
  dc:	lsr	w10, w11, w10
  e0:	orr	w10, w10, w12
  e4:	b	134 <__addXf3__+0x120>
  e8:	mov	w10, #0x1                   	// #1
  ec:	movk	w10, #0x7f80, lsl #16
  f0:	cmp	w8, w10
  f4:	b.cc	100 <__addXf3__+0xec>  // b.lo, b.ul, b.last
  f8:	orr	w0, w19, #0x400000
  fc:	b	290 <__addXf3__+0x27c>
 100:	cmp	w9, w10
 104:	b.cc	224 <__addXf3__+0x210>  // b.lo, b.ul, b.last
 108:	orr	w0, w0, #0x400000
 10c:	b	290 <__addXf3__+0x27c>
 110:	sub	x0, x29, #0x4
 114:	bl	2e0 <normalize>
 118:	mov	w19, w0
 11c:	cbnz	w20, 98 <__addXf3__+0x84>
 120:	add	x0, sp, #0x8
 124:	bl	2e0 <normalize>
 128:	mov	w20, w0
 12c:	b	98 <__addXf3__+0x84>
 130:	mov	w10, #0x1                   	// #1
 134:	str	w10, [sp, #8]
 138:	ldr	w10, [sp, #8]
 13c:	tbnz	w8, #31, 160 <__addXf3__+0x14c>
 140:	add	w8, w10, w9
 144:	stur	w8, [x29, #-4]
 148:	tbz	w8, #27, 198 <__addXf3__+0x184>
 14c:	and	w9, w8, #0x1
 150:	orr	w8, w9, w8, lsr #1
 154:	add	w19, w19, #0x1
 158:	stur	w8, [x29, #-4]
 15c:	b	198 <__addXf3__+0x184>
 160:	subs	w20, w9, w10
 164:	stur	w20, [x29, #-4]
 168:	b.eq	248 <__addXf3__+0x234>  // b.none
 16c:	lsr	w8, w20, #26
 170:	cbnz	w8, 198 <__addXf3__+0x184>
 174:	mov	w0, w20
 178:	bl	330 <rep_clz>
 17c:	mov	w21, w0
 180:	mov	w0, #0x4000000             	// #67108864
 184:	bl	330 <rep_clz>
 188:	sub	w8, w21, w0
 18c:	lsl	w9, w20, w8
 190:	stur	w9, [x29, #-4]
 194:	sub	w19, w19, w8
 198:	cmp	w19, #0xff
 19c:	and	w21, w22, #0x80000000
 1a0:	b.lt	1ac <__addXf3__+0x198>  // b.tstop
 1a4:	orr	w19, w21, #0x7f800000
 1a8:	b	28c <__addXf3__+0x278>
 1ac:	cmp	w19, #0x0
 1b0:	b.gt	1e0 <__addXf3__+0x1cc>
 1b4:	ldur	w8, [x29, #-4]
 1b8:	add	w10, w19, #0x1f
 1bc:	mov	w9, #0x1                   	// #1
 1c0:	sub	w9, w9, w19
 1c4:	lsl	w10, w8, w10
 1c8:	cmp	w10, #0x0
 1cc:	cset	w10, ne  // ne = any
 1d0:	lsr	w8, w8, w9
 1d4:	orr	w8, w8, w10
 1d8:	mov	w19, wzr
 1dc:	stur	w8, [x29, #-4]
 1e0:	ldur	w8, [x29, #-4]
 1e4:	orr	w9, w21, w19, lsl #23
 1e8:	and	w20, w8, #0x7
 1ec:	ubfx	w8, w8, #3, #23
 1f0:	orr	w19, w9, w8
 1f4:	bl	0 <__fe_getround>
 1f8:	cmp	w0, #0x2
 1fc:	b.eq	260 <__addXf3__+0x24c>  // b.none
 200:	cmp	w0, #0x1
 204:	b.eq	26c <__addXf3__+0x258>  // b.none
 208:	cbnz	w0, 284 <__addXf3__+0x270>
 20c:	cmp	w20, #0x4
 210:	cinc	w19, w19, hi  // hi = pmore
 214:	b.ne	284 <__addXf3__+0x270>  // b.any
 218:	and	w8, w19, #0x1
 21c:	add	w19, w8, w19
 220:	b	284 <__addXf3__+0x270>
 224:	mov	w10, #0x7f800000            	// #2139095040
 228:	cmp	w8, w10
 22c:	b.ne	250 <__addXf3__+0x23c>  // b.any
 230:	eor	w8, w0, w19
 234:	mov	w9, #0x80000000            	// #-2147483648
 238:	cmp	w8, w9
 23c:	b.ne	298 <__addXf3__+0x284>  // b.any
 240:	mov	w0, #0x7fc00000            	// #2143289344
 244:	b	290 <__addXf3__+0x27c>
 248:	mov	w19, wzr
 24c:	b	28c <__addXf3__+0x278>
 250:	cmp	w9, w10
 254:	b.ne	2b4 <__addXf3__+0x2a0>  // b.any
 258:	mov	v8.16b, v9.16b
 25c:	b	298 <__addXf3__+0x284>
 260:	cmp	w21, #0x0
 264:	cset	w8, eq  // eq = none
 268:	b	274 <__addXf3__+0x260>
 26c:	cmp	w21, #0x0
 270:	cset	w8, ne  // ne = any
 274:	cmp	w20, #0x0
 278:	cset	w9, ne  // ne = any
 27c:	and	w8, w8, w9
 280:	add	w19, w19, w8
 284:	cbz	w20, 28c <__addXf3__+0x278>
 288:	bl	0 <__fe_raise_inexact>
 28c:	mov	w0, w19
 290:	bl	2d8 <fromRep>
 294:	mov	v8.16b, v0.16b
 298:	mov	v0.16b, v8.16b
 29c:	ldp	x20, x19, [sp, #64]
 2a0:	ldp	x22, x21, [sp, #48]
 2a4:	ldp	x29, x30, [sp, #32]
 2a8:	ldp	d9, d8, [sp, #16]
 2ac:	add	sp, sp, #0x50
 2b0:	ret
 2b4:	cbz	w8, 2c0 <__addXf3__+0x2ac>
 2b8:	cbnz	w9, 6c <__addXf3__+0x58>
 2bc:	b	298 <__addXf3__+0x284>
 2c0:	mov	v8.16b, v9.16b
 2c4:	cbnz	w9, 298 <__addXf3__+0x284>
 2c8:	and	w0, w0, w19
 2cc:	b	290 <__addXf3__+0x27c>

00000000000002d0 <toRep>:
 2d0:	fmov	w0, s0
 2d4:	ret

00000000000002d8 <fromRep>:
 2d8:	fmov	s0, w0
 2dc:	ret

00000000000002e0 <normalize>:
 2e0:	stp	x29, x30, [sp, #-48]!
 2e4:	str	x21, [sp, #16]
 2e8:	stp	x20, x19, [sp, #32]
 2ec:	mov	x29, sp
 2f0:	ldr	w20, [x0]
 2f4:	mov	x19, x0
 2f8:	mov	w0, w20
 2fc:	bl	330 <rep_clz>
 300:	mov	w21, w0
 304:	mov	w0, #0x800000              	// #8388608
 308:	bl	330 <rep_clz>
 30c:	sub	w8, w21, w0
 310:	lsl	w10, w20, w8
 314:	str	w10, [x19]
 318:	ldp	x20, x19, [sp, #32]
 31c:	ldr	x21, [sp, #16]
 320:	mov	w9, #0x1                   	// #1
 324:	sub	w0, w9, w8
 328:	ldp	x29, x30, [sp], #48
 32c:	ret

0000000000000330 <rep_clz>:
 330:	clz	w0, w0
 334:	ret

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__addXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__addXf3__>:
  14:	sub	sp, sp, #0x80
  18:	stp	x29, x30, [sp, #64]
  1c:	str	x23, [sp, #80]
  20:	stp	x22, x21, [sp, #96]
  24:	stp	x20, x19, [sp, #112]
  28:	add	x29, sp, #0x40
  2c:	stp	q0, q1, [sp]
  30:	bl	488 <toRep>
  34:	ldr	q0, [sp, #16]
  38:	mov	x19, x0
  3c:	mov	x20, x1
  40:	bl	488 <toRep>
  44:	and	x9, x20, #0x7fffffffffffffff
  48:	subs	x11, x19, #0x1
  4c:	mov	x13, #0xffffffffffffffff    	// #-1
  50:	adcs	x12, x9, x13
  54:	mov	x10, #0x7ffeffffffffffff    	// #9223090561878065151
  58:	cmn	x11, #0x1
  5c:	ldr	q0, [sp]
  60:	cset	w11, eq  // eq = none
  64:	cmp	x12, x10
  68:	cset	w12, hi  // hi = pmore
  6c:	and	x8, x1, #0x7fffffffffffffff
  70:	csel	w14, w11, w12, eq  // eq = none
  74:	subs	x12, x0, #0x1
  78:	adcs	x11, x8, x13
  7c:	tbnz	w14, #0, 198 <__addXf3__+0x184>
  80:	cmn	x12, #0x1
  84:	cset	w12, ne  // ne = any
  88:	cmp	x11, x10
  8c:	cset	w10, cc  // cc = lo, ul, last
  90:	csel	w10, w12, w10, eq  // eq = none
  94:	tbz	w10, #0, 198 <__addXf3__+0x184>
  98:	cmp	x0, x19
  9c:	cset	w10, hi  // hi = pmore
  a0:	cmp	x8, x9
  a4:	cset	w8, hi  // hi = pmore
  a8:	csel	w8, w10, w8, eq  // eq = none
  ac:	cmp	w8, #0x0
  b0:	csel	x21, x20, x1, ne  // ne = any
  b4:	csel	x23, x1, x20, ne  // ne = any
  b8:	csel	x8, x19, x0, ne  // ne = any
  bc:	csel	x9, x0, x19, ne  // ne = any
  c0:	ubfx	x19, x23, #48, #15
  c4:	ubfx	x20, x21, #48, #15
  c8:	and	x10, x23, #0xffffffffffff
  cc:	and	x11, x21, #0xffffffffffff
  d0:	stp	x9, x10, [x29, #-16]
  d4:	stp	x8, x11, [sp, #32]
  d8:	cbz	w19, 1e0 <__addXf3__+0x1cc>
  dc:	cbz	w20, 1f0 <__addXf3__+0x1dc>
  e0:	ldp	x9, x10, [x29, #-16]
  e4:	ldp	x11, x12, [sp, #32]
  e8:	eor	x8, x23, x21
  ec:	subs	w13, w19, w20
  f0:	extr	x10, x10, x9, #61
  f4:	extr	x12, x12, x11, #61
  f8:	lsl	x9, x9, #3
  fc:	lsl	x11, x11, #3
 100:	orr	x10, x10, #0x8000000000000
 104:	orr	x12, x12, #0x8000000000000
 108:	stp	x9, x10, [x29, #-16]
 10c:	stp	x11, x12, [sp, #32]
 110:	b.eq	20c <__addXf3__+0x1f8>  // b.none
 114:	cmp	w13, #0x7f
 118:	b.hi	200 <__addXf3__+0x1ec>  // b.pmore
 11c:	mov	w14, w13
 120:	mov	w15, #0x80                  	// #128
 124:	subs	x15, x15, x14
 128:	neg	x15, x15
 12c:	mov	w16, #0x40                  	// #64
 130:	neg	x18, x14
 134:	lsr	x15, x11, x15
 138:	lsr	x17, x11, x13
 13c:	sub	x16, x16, x14
 140:	lsl	x11, x11, x18
 144:	lsl	x18, x12, x18
 148:	csel	x15, xzr, x15, eq  // eq = none
 14c:	cmp	x16, #0x0
 150:	orr	x15, x15, x18
 154:	csel	x15, x11, x15, ge  // ge = tcont
 158:	csel	x11, xzr, x11, ge  // ge = tcont
 15c:	orr	x11, x11, x15
 160:	cmp	x11, #0x0
 164:	cset	w11, ne  // ne = any
 168:	cmp	x14, #0x0
 16c:	lsr	x13, x12, x13
 170:	lsr	x12, x12, x14
 174:	sub	x16, x14, #0x40
 178:	csel	x14, xzr, x18, eq  // eq = none
 17c:	cmp	x16, #0x0
 180:	orr	x14, x17, x14
 184:	csel	x13, xzr, x13, ge  // ge = tcont
 188:	csel	x12, x12, x14, ge  // ge = tcont
 18c:	orr	x11, x12, x11
 190:	str	x13, [sp, #40]
 194:	b	208 <__addXf3__+0x1f4>
 198:	cmp	x19, #0x0
 19c:	mov	x10, #0x7fff000000000000    	// #9223090561878065152
 1a0:	cset	w11, eq  // eq = none
 1a4:	cmp	x9, x10
 1a8:	cset	w12, cc  // cc = lo, ul, last
 1ac:	csel	w11, w11, w12, eq  // eq = none
 1b0:	tbnz	w11, #0, 1c0 <__addXf3__+0x1ac>
 1b4:	orr	x1, x20, #0x800000000000
 1b8:	mov	x0, x19
 1bc:	b	440 <__addXf3__+0x42c>
 1c0:	cmp	x0, #0x0
 1c4:	cset	w11, eq  // eq = none
 1c8:	cmp	x8, x10
 1cc:	cset	w10, cc  // cc = lo, ul, last
 1d0:	csel	w10, w11, w10, eq  // eq = none
 1d4:	tbnz	w10, #0, 3bc <__addXf3__+0x3a8>
 1d8:	orr	x1, x1, #0x800000000000
 1dc:	b	440 <__addXf3__+0x42c>
 1e0:	sub	x0, x29, #0x10
 1e4:	bl	4a0 <normalize>
 1e8:	mov	w19, w0
 1ec:	cbnz	w20, e0 <__addXf3__+0xcc>
 1f0:	add	x0, sp, #0x20
 1f4:	bl	4a0 <normalize>
 1f8:	mov	w20, w0
 1fc:	b	e0 <__addXf3__+0xcc>
 200:	mov	w11, #0x1                   	// #1
 204:	str	xzr, [sp, #40]
 208:	str	x11, [sp, #32]
 20c:	ldp	x12, x11, [sp, #32]
 210:	tbnz	x8, #63, 240 <__addXf3__+0x22c>
 214:	adds	x8, x12, x9
 218:	adcs	x9, x11, x10
 21c:	stp	x8, x9, [x29, #-16]
 220:	tbz	x9, #52, 2b4 <__addXf3__+0x2a0>
 224:	and	x10, x8, #0x1
 228:	extr	x8, x9, x8, #1
 22c:	lsr	x9, x9, #1
 230:	orr	x8, x8, x10
 234:	add	w19, w19, #0x1
 238:	stp	x8, x9, [x29, #-16]
 23c:	b	2b4 <__addXf3__+0x2a0>
 240:	subs	x20, x9, x12
 244:	sbcs	x21, x10, x11
 248:	orr	x8, x20, x21
 24c:	stp	x20, x21, [x29, #-16]
 250:	cbz	x8, 3e8 <__addXf3__+0x3d4>
 254:	lsr	x8, x21, #51
 258:	cbnz	x8, 2b4 <__addXf3__+0x2a0>
 25c:	mov	x0, x20
 260:	mov	x1, x21
 264:	bl	524 <rep_clz>
 268:	mov	w22, w0
 26c:	mov	x1, #0x8000000000000       	// #2251799813685248
 270:	mov	x0, xzr
 274:	bl	524 <rep_clz>
 278:	sub	w8, w22, w0
 27c:	neg	x9, x8
 280:	cmp	x8, #0x0
 284:	lsr	x9, x20, x9
 288:	lsl	x10, x21, x8
 28c:	sub	x11, x8, #0x40
 290:	csel	x9, xzr, x9, eq  // eq = none
 294:	cmp	x11, #0x0
 298:	lsl	x11, x20, x8
 29c:	lsl	x12, x20, x8
 2a0:	orr	x9, x9, x10
 2a4:	csel	x10, xzr, x12, ge  // ge = tcont
 2a8:	csel	x9, x11, x9, ge  // ge = tcont
 2ac:	stp	x10, x9, [x29, #-16]
 2b0:	sub	w19, w19, w8
 2b4:	mov	w8, #0x7fff                	// #32767
 2b8:	cmp	w19, w8
 2bc:	and	x22, x23, #0x8000000000000000
 2c0:	b.lt	2d0 <__addXf3__+0x2bc>  // b.tstop
 2c4:	mov	x20, xzr
 2c8:	orr	x19, x22, #0x7fff000000000000
 2cc:	b	438 <__addXf3__+0x424>
 2d0:	cmp	w19, #0x0
 2d4:	b.gt	368 <__addXf3__+0x354>
 2d8:	mov	w8, #0x1                   	// #1
 2dc:	ldp	x10, x9, [x29, #-16]
 2e0:	sub	w8, w8, w19
 2e4:	mov	w11, #0x80                  	// #128
 2e8:	sxtw	x13, w8
 2ec:	subs	x11, x11, x13
 2f0:	mov	w12, #0x40                  	// #64
 2f4:	neg	x11, x11
 2f8:	sub	x12, x12, x13
 2fc:	neg	x13, x13
 300:	lsr	x11, x10, x11
 304:	csel	x11, xzr, x11, eq  // eq = none
 308:	cmp	x12, #0x0
 30c:	lsl	x12, x10, x13
 310:	lsl	x13, x9, x13
 314:	orr	x11, x11, x13
 318:	csel	x11, x12, x11, ge  // ge = tcont
 31c:	csel	x12, xzr, x12, ge  // ge = tcont
 320:	orr	x11, x12, x11
 324:	neg	x14, x8
 328:	cmp	x11, #0x0
 32c:	lsl	x14, x9, x14
 330:	cset	w11, ne  // ne = any
 334:	cmp	x8, #0x0
 338:	sub	x15, x8, #0x40
 33c:	lsr	x13, x9, x8
 340:	lsr	x10, x10, x8
 344:	lsr	x9, x9, x8
 348:	csel	x8, xzr, x14, eq  // eq = none
 34c:	cmp	x15, #0x0
 350:	orr	x8, x10, x8
 354:	csel	x8, x9, x8, ge  // ge = tcont
 358:	csel	x12, xzr, x13, ge  // ge = tcont
 35c:	orr	x8, x8, x11
 360:	mov	w19, wzr
 364:	stp	x8, x12, [x29, #-16]
 368:	ldp	x8, x9, [x29, #-16]
 36c:	and	w21, w8, #0x7
 370:	extr	x20, x9, x8, #3
 374:	ubfx	x8, x9, #3, #48
 378:	orr	x9, x22, x19, lsl #48
 37c:	orr	x19, x9, x8
 380:	bl	0 <__fe_getround>
 384:	cmp	w0, #0x2
 388:	b.eq	408 <__addXf3__+0x3f4>  // b.none
 38c:	cmp	w0, #0x1
 390:	b.eq	414 <__addXf3__+0x400>  // b.none
 394:	cbnz	w0, 430 <__addXf3__+0x41c>
 398:	cmp	w21, #0x4
 39c:	cset	w8, hi  // hi = pmore
 3a0:	adds	x20, x20, x8
 3a4:	adcs	x19, x19, xzr
 3a8:	cmp	w21, #0x4
 3ac:	b.ne	430 <__addXf3__+0x41c>  // b.any
 3b0:	and	x8, x20, #0x1
 3b4:	adds	x20, x8, x20
 3b8:	b	42c <__addXf3__+0x418>
 3bc:	eor	x10, x9, #0x7fff000000000000
 3c0:	orr	x10, x19, x10
 3c4:	cbnz	x10, 3f4 <__addXf3__+0x3e0>
 3c8:	eor	x9, x1, x20
 3cc:	eor	x8, x0, x19
 3d0:	eor	x9, x9, #0x8000000000000000
 3d4:	orr	x8, x8, x9
 3d8:	cbnz	x8, 444 <__addXf3__+0x430>
 3dc:	mov	x1, #0x7fff800000000000    	// #9223231299366420480
 3e0:	mov	x0, xzr
 3e4:	b	440 <__addXf3__+0x42c>
 3e8:	mov	x20, xzr
 3ec:	mov	x19, xzr
 3f0:	b	438 <__addXf3__+0x424>
 3f4:	eor	x10, x8, #0x7fff000000000000
 3f8:	orr	x10, x0, x10
 3fc:	cbnz	x10, 45c <__addXf3__+0x448>
 400:	ldr	q0, [sp, #16]
 404:	b	444 <__addXf3__+0x430>
 408:	cmp	x22, #0x0
 40c:	cset	w8, eq  // eq = none
 410:	b	41c <__addXf3__+0x408>
 414:	cmp	x22, #0x0
 418:	cset	w8, ne  // ne = any
 41c:	cmp	w21, #0x0
 420:	cset	w9, ne  // ne = any
 424:	and	w8, w8, w9
 428:	adds	x20, x20, x8
 42c:	adcs	x19, x19, xzr
 430:	cbz	w21, 438 <__addXf3__+0x424>
 434:	bl	0 <__fe_raise_inexact>
 438:	mov	x0, x20
 43c:	mov	x1, x19
 440:	bl	494 <fromRep>
 444:	ldp	x20, x19, [sp, #112]
 448:	ldp	x22, x21, [sp, #96]
 44c:	ldr	x23, [sp, #80]
 450:	ldp	x29, x30, [sp, #64]
 454:	add	sp, sp, #0x80
 458:	ret
 45c:	orr	x10, x19, x9
 460:	cbz	x10, 470 <__addXf3__+0x45c>
 464:	orr	x10, x0, x8
 468:	cbnz	x10, 98 <__addXf3__+0x84>
 46c:	b	444 <__addXf3__+0x430>
 470:	ldr	q0, [sp, #16]
 474:	orr	x8, x0, x8
 478:	cbnz	x8, 444 <__addXf3__+0x430>
 47c:	and	x0, x0, x19
 480:	and	x1, x1, x20
 484:	b	440 <__addXf3__+0x42c>

0000000000000488 <toRep>:
 488:	str	q0, [sp, #-16]!
 48c:	ldp	x0, x1, [sp], #16
 490:	ret

0000000000000494 <fromRep>:
 494:	stp	x0, x1, [sp, #-16]!
 498:	ldr	q0, [sp], #16
 49c:	ret

00000000000004a0 <normalize>:
 4a0:	stp	x29, x30, [sp, #-48]!
 4a4:	stp	x22, x21, [sp, #16]
 4a8:	stp	x20, x19, [sp, #32]
 4ac:	mov	x29, sp
 4b0:	ldp	x21, x20, [x0]
 4b4:	mov	x19, x0
 4b8:	mov	x0, x21
 4bc:	mov	x1, x20
 4c0:	bl	524 <rep_clz>
 4c4:	mov	w22, w0
 4c8:	mov	x1, #0x1000000000000       	// #281474976710656
 4cc:	mov	x0, xzr
 4d0:	bl	524 <rep_clz>
 4d4:	sub	w8, w22, w0
 4d8:	neg	x10, x8
 4dc:	mov	w9, #0x1                   	// #1
 4e0:	cmp	x8, #0x0
 4e4:	lsr	x10, x21, x10
 4e8:	lsl	x11, x20, x8
 4ec:	lsl	x12, x21, x8
 4f0:	lsl	x13, x21, x8
 4f4:	sub	w0, w9, w8
 4f8:	sub	x8, x8, #0x40
 4fc:	csel	x9, xzr, x10, eq  // eq = none
 500:	cmp	x8, #0x0
 504:	orr	x8, x9, x11
 508:	csel	x9, xzr, x13, ge  // ge = tcont
 50c:	csel	x8, x12, x8, ge  // ge = tcont
 510:	stp	x9, x8, [x19]
 514:	ldp	x20, x19, [sp, #32]
 518:	ldp	x22, x21, [sp, #16]
 51c:	ldp	x29, x30, [sp], #48
 520:	ret

0000000000000524 <rep_clz>:
 524:	cmp	x1, #0x0
 528:	csel	x9, x0, x1, eq  // eq = none
 52c:	cset	w8, eq  // eq = none
 530:	clz	x9, x9
 534:	add	w0, w9, w8, lsl #6
 538:	ret

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	add	x0, x1, x0
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__addvdi3+0x34>
  18:	b.ge	38 <__addvdi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvdi3>
  20:	adrp	x2, 0 <__addvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvdi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvdi3>
  44:	adrp	x2, 0 <__addvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	add	w0, w1, w0
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__addvsi3+0x34>
  18:	b.ge	38 <__addvsi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvsi3>
  20:	adrp	x2, 0 <__addvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvsi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvsi3>
  44:	adrp	x2, 0 <__addvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	adds	x0, x2, x0
  10:	mov	x8, x1
  14:	adcs	x1, x3, x1
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__addvti3+0x4c>
  20:	cset	w9, cs  // cs = hs, nlast
  24:	cmp	x1, x8
  28:	cset	w8, ge  // ge = tcont
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__addvti3+0x60>
  34:	adrp	x0, 0 <__addvti3>
  38:	adrp	x2, 0 <__addvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, cc  // cc = lo, ul, last
  50:	cmp	x1, x8
  54:	cset	w8, lt  // lt = tstop
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__addvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__addvti3>
  6c:	adrp	x2, 0 <__addvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	tbnz	w1, #5, 24 <__ashldi3+0x24>
   4:	cbz	w1, 30 <__ashldi3+0x30>
   8:	lsr	x8, x0, #32
   c:	neg	w9, w1
  10:	lsr	w9, w0, w9
  14:	lsl	w8, w8, w1
  18:	lsl	w0, w0, w1
  1c:	orr	w8, w8, w9
  20:	b	2c <__ashldi3+0x2c>
  24:	lsl	w8, w0, w1
  28:	mov	x0, xzr
  2c:	bfi	x0, x8, #32, #32
  30:	ret

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbnz	w2, #6, 24 <__ashlti3+0x24>
   4:	cbz	w2, 34 <__ashlti3+0x34>
   8:	neg	w9, w2
   c:	lsr	x9, x0, x9
  10:	lsl	x10, x1, x2
  14:	mov	x8, xzr
  18:	lsl	x0, x0, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__ashlti3+0x30>
  24:	mov	x8, xzr
  28:	lsl	x9, x0, x2
  2c:	mov	x0, xzr
  30:	orr	x1, x9, x8
  34:	ret

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	lsr	x9, x0, #32
   4:	tbnz	w1, #5, 24 <__ashrdi3+0x24>
   8:	cbz	w1, 34 <__ashrdi3+0x34>
   c:	neg	w10, w1
  10:	asr	w8, w9, w1
  14:	lsl	w9, w9, w10
  18:	lsr	w10, w0, w1
  1c:	orr	w9, w9, w10
  20:	b	2c <__ashrdi3+0x2c>
  24:	asr	w8, w9, #31
  28:	asr	w9, w9, w1
  2c:	mov	w0, w9
  30:	bfi	x0, x8, #32, #32
  34:	ret

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	mov	x8, x1
   4:	tbnz	w2, #6, 24 <__ashrti3+0x24>
   8:	cbz	w2, 30 <__ashrti3+0x30>
   c:	neg	w9, w2
  10:	asr	x1, x8, x2
  14:	lsl	x8, x8, x9
  18:	lsr	x9, x0, x2
  1c:	orr	x0, x8, x9
  20:	ret
  24:	asr	x1, x8, #63
  28:	asr	x0, x8, x2
  2c:	ret
  30:	mov	x1, x8
  34:	ret

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w8, #0x0
   8:	csel	w8, w0, w8, eq  // eq = none
   c:	cset	w9, eq  // eq = none
  10:	clz	w8, w8
  14:	add	w0, w8, w9, lsl #5
  18:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	cmp	w0, #0x10, lsl #12
   4:	cset	w11, cc  // cc = lo, ul, last
   8:	mov	w8, #0x10                  	// #16
   c:	lsl	w11, w11, #4
  10:	sub	w8, w8, w11
  14:	lsr	w8, w0, w8
  18:	tst	w8, #0xff00
  1c:	cset	w12, eq  // eq = none
  20:	mov	w9, #0x8                   	// #8
  24:	bfi	w11, w12, #3, #1
  28:	lsl	w12, w12, #3
  2c:	sub	w9, w9, w12
  30:	lsr	w8, w8, w9
  34:	tst	w8, #0xf0
  38:	cset	w9, eq  // eq = none
  3c:	mov	w10, #0x4                   	// #4
  40:	bfi	w11, w9, #2, #1
  44:	lsl	w9, w9, #2
  48:	sub	w9, w10, w9
  4c:	lsr	w8, w8, w9
  50:	tst	w8, #0xc
  54:	cset	w9, eq  // eq = none
  58:	mov	w12, #0x2                   	// #2
  5c:	bfi	w11, w9, #1, #1
  60:	lsl	w9, w9, #1
  64:	sub	w9, w12, w9
  68:	mov	w10, #0x1                   	// #1
  6c:	lsr	w8, w8, w9
  70:	sub	w9, w12, w8
  74:	bic	w8, w10, w8, lsr #1
  78:	neg	w8, w8
  7c:	and	w8, w9, w8
  80:	add	w0, w8, w11
  84:	ret

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csel	x9, x0, x1, eq  // eq = none
   8:	cset	w8, eq  // eq = none
   c:	clz	x9, x9
  10:	add	w0, w9, w8, lsl #6
  14:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.ge	18 <__cmpdi2+0x18>  // b.tcont
  10:	mov	w0, wzr
  14:	ret
  18:	b.le	24 <__cmpdi2+0x24>
  1c:	mov	w0, #0x2                   	// #2
  20:	ret
  24:	cmp	w0, w1
  28:	b.cs	34 <__cmpdi2+0x34>  // b.hs, b.nlast
  2c:	mov	w0, wzr
  30:	ret
  34:	mov	w8, #0x1                   	// #1
  38:	cinc	w0, w8, hi  // hi = pmore
  3c:	ret

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	cmp	x1, x3
   4:	b.ge	10 <__cmpti2+0x10>  // b.tcont
   8:	mov	w0, wzr
   c:	ret
  10:	b.le	1c <__cmpti2+0x1c>
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	x0, x2
  20:	b.cs	2c <__cmpti2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	str	d8, [sp, #-32]!
   4:	stp	x29, x30, [sp, #8]
   8:	str	x19, [sp, #24]
   c:	mov	x29, sp
  10:	mov	v8.16b, v1.16b
  14:	bl	8c <toRep>
  18:	mov	v0.16b, v8.16b
  1c:	mov	x19, x0
  20:	bl	8c <toRep>
  24:	and	x9, x19, #0x7fffffffffffffff
  28:	mov	x11, #0x7ff0000000000000    	// #9218868437227405312
  2c:	mov	x8, x0
  30:	cmp	x9, x11
  34:	mov	w0, #0x1                   	// #1
  38:	b.hi	7c <__cmpdf2+0x7c>  // b.pmore
  3c:	and	x10, x8, #0x7fffffffffffffff
  40:	cmp	x10, x11
  44:	b.hi	7c <__cmpdf2+0x7c>  // b.pmore
  48:	orr	x9, x10, x9
  4c:	cbz	x9, 68 <__cmpdf2+0x68>
  50:	tst	x8, x19
  54:	b.lt	70 <__cmpdf2+0x70>  // b.tstop
  58:	cmp	x19, x8
  5c:	b.ge	78 <__cmpdf2+0x78>  // b.tcont
  60:	mov	w0, #0xffffffff            	// #-1
  64:	b	7c <__cmpdf2+0x7c>
  68:	mov	w0, wzr
  6c:	b	7c <__cmpdf2+0x7c>
  70:	cmp	x19, x8
  74:	b.gt	60 <__cmpdf2+0x60>
  78:	cset	w0, ne  // ne = any
  7c:	ldr	x19, [sp, #24]
  80:	ldp	x29, x30, [sp, #8]
  84:	ldr	d8, [sp], #32
  88:	ret

000000000000008c <toRep>:
  8c:	fmov	x0, d0
  90:	ret

0000000000000094 <__gedf2>:
  94:	str	d8, [sp, #-32]!
  98:	stp	x29, x30, [sp, #8]
  9c:	str	x19, [sp, #24]
  a0:	mov	x29, sp
  a4:	mov	v8.16b, v1.16b
  a8:	bl	8c <toRep>
  ac:	mov	v0.16b, v8.16b
  b0:	mov	x19, x0
  b4:	bl	8c <toRep>
  b8:	and	x9, x19, #0x7fffffffffffffff
  bc:	mov	x11, #0x7ff0000000000000    	// #9218868437227405312
  c0:	mov	x8, x0
  c4:	cmp	x9, x11
  c8:	mov	w0, #0xffffffff            	// #-1
  cc:	b.hi	110 <__gedf2+0x7c>  // b.pmore
  d0:	and	x10, x8, #0x7fffffffffffffff
  d4:	cmp	x10, x11
  d8:	b.hi	110 <__gedf2+0x7c>  // b.pmore
  dc:	orr	x9, x10, x9
  e0:	cbz	x9, fc <__gedf2+0x68>
  e4:	tst	x8, x19
  e8:	b.lt	104 <__gedf2+0x70>  // b.tstop
  ec:	cmp	x19, x8
  f0:	b.ge	10c <__gedf2+0x78>  // b.tcont
  f4:	mov	w0, #0xffffffff            	// #-1
  f8:	b	110 <__gedf2+0x7c>
  fc:	mov	w0, wzr
 100:	b	110 <__gedf2+0x7c>
 104:	cmp	x19, x8
 108:	b.gt	f4 <__gedf2+0x60>
 10c:	cset	w0, ne  // ne = any
 110:	ldr	x19, [sp, #24]
 114:	ldp	x29, x30, [sp, #8]
 118:	ldr	d8, [sp], #32
 11c:	ret

0000000000000120 <__unorddf2>:
 120:	str	d8, [sp, #-32]!
 124:	stp	x29, x30, [sp, #8]
 128:	str	x19, [sp, #24]
 12c:	mov	x29, sp
 130:	mov	v8.16b, v1.16b
 134:	bl	8c <toRep>
 138:	mov	v0.16b, v8.16b
 13c:	and	x19, x0, #0x7fffffffffffffff
 140:	bl	8c <toRep>
 144:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
 148:	and	x8, x0, #0x7fffffffffffffff
 14c:	cmp	x19, x9
 150:	ldr	x19, [sp, #24]
 154:	ldp	x29, x30, [sp, #8]
 158:	cset	w10, hi  // hi = pmore
 15c:	cmp	x8, x9
 160:	cset	w8, hi  // hi = pmore
 164:	orr	w0, w10, w8
 168:	ldr	d8, [sp], #32
 16c:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	str	d8, [sp, #-32]!
   4:	stp	x29, x30, [sp, #8]
   8:	str	x19, [sp, #24]
   c:	mov	x29, sp
  10:	mov	v8.16b, v1.16b
  14:	bl	8c <toRep>
  18:	mov	v0.16b, v8.16b
  1c:	mov	w19, w0
  20:	bl	8c <toRep>
  24:	and	w9, w19, #0x7fffffff
  28:	mov	w11, #0x7f800000            	// #2139095040
  2c:	mov	w8, w0
  30:	cmp	w9, w11
  34:	mov	w0, #0x1                   	// #1
  38:	b.hi	7c <__cmpsf2+0x7c>  // b.pmore
  3c:	and	w10, w8, #0x7fffffff
  40:	cmp	w10, w11
  44:	b.hi	7c <__cmpsf2+0x7c>  // b.pmore
  48:	orr	w9, w10, w9
  4c:	cbz	w9, 68 <__cmpsf2+0x68>
  50:	tst	w8, w19
  54:	b.lt	70 <__cmpsf2+0x70>  // b.tstop
  58:	cmp	w19, w8
  5c:	b.ge	78 <__cmpsf2+0x78>  // b.tcont
  60:	mov	w0, #0xffffffff            	// #-1
  64:	b	7c <__cmpsf2+0x7c>
  68:	mov	w0, wzr
  6c:	b	7c <__cmpsf2+0x7c>
  70:	cmp	w19, w8
  74:	b.gt	60 <__cmpsf2+0x60>
  78:	cset	w0, ne  // ne = any
  7c:	ldr	x19, [sp, #24]
  80:	ldp	x29, x30, [sp, #8]
  84:	ldr	d8, [sp], #32
  88:	ret

000000000000008c <toRep>:
  8c:	fmov	w0, s0
  90:	ret

0000000000000094 <__gesf2>:
  94:	str	d8, [sp, #-32]!
  98:	stp	x29, x30, [sp, #8]
  9c:	str	x19, [sp, #24]
  a0:	mov	x29, sp
  a4:	mov	v8.16b, v1.16b
  a8:	bl	8c <toRep>
  ac:	mov	v0.16b, v8.16b
  b0:	mov	w19, w0
  b4:	bl	8c <toRep>
  b8:	and	w9, w19, #0x7fffffff
  bc:	mov	w11, #0x7f800000            	// #2139095040
  c0:	mov	w8, w0
  c4:	cmp	w9, w11
  c8:	mov	w0, #0xffffffff            	// #-1
  cc:	b.hi	110 <__gesf2+0x7c>  // b.pmore
  d0:	and	w10, w8, #0x7fffffff
  d4:	cmp	w10, w11
  d8:	b.hi	110 <__gesf2+0x7c>  // b.pmore
  dc:	orr	w9, w10, w9
  e0:	cbz	w9, fc <__gesf2+0x68>
  e4:	tst	w8, w19
  e8:	b.lt	104 <__gesf2+0x70>  // b.tstop
  ec:	cmp	w19, w8
  f0:	b.ge	10c <__gesf2+0x78>  // b.tcont
  f4:	mov	w0, #0xffffffff            	// #-1
  f8:	b	110 <__gesf2+0x7c>
  fc:	mov	w0, wzr
 100:	b	110 <__gesf2+0x7c>
 104:	cmp	w19, w8
 108:	b.gt	f4 <__gesf2+0x60>
 10c:	cset	w0, ne  // ne = any
 110:	ldr	x19, [sp, #24]
 114:	ldp	x29, x30, [sp, #8]
 118:	ldr	d8, [sp], #32
 11c:	ret

0000000000000120 <__unordsf2>:
 120:	str	d8, [sp, #-32]!
 124:	stp	x29, x30, [sp, #8]
 128:	str	x19, [sp, #24]
 12c:	mov	x29, sp
 130:	mov	v8.16b, v1.16b
 134:	bl	8c <toRep>
 138:	mov	v0.16b, v8.16b
 13c:	and	w19, w0, #0x7fffffff
 140:	bl	8c <toRep>
 144:	mov	w9, #0x7f800000            	// #2139095040
 148:	and	w8, w0, #0x7fffffff
 14c:	cmp	w19, w9
 150:	ldr	x19, [sp, #24]
 154:	ldp	x29, x30, [sp, #8]
 158:	cset	w10, hi  // hi = pmore
 15c:	cmp	w8, w9
 160:	cset	w8, hi  // hi = pmore
 164:	orr	w0, w10, w8
 168:	ldr	d8, [sp], #32
 16c:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w0, #0x0
   8:	csel	w8, w8, w0, eq  // eq = none
   c:	rbit	w8, w8
  10:	cset	w9, eq  // eq = none
  14:	clz	w8, w8
  18:	add	w0, w8, w9, lsl #5
  1c:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	cset	w9, eq  // eq = none
   8:	lsl	w9, w9, #4
   c:	lsr	w10, w0, w9
  10:	tst	w10, #0xff
  14:	cset	w11, eq  // eq = none
  18:	bfi	w9, w11, #3, #1
  1c:	lsl	w11, w11, #3
  20:	lsr	w10, w10, w11
  24:	tst	w10, #0xf
  28:	cset	w11, eq  // eq = none
  2c:	bfi	w9, w11, #2, #1
  30:	lsl	w11, w11, #2
  34:	lsr	w10, w10, w11
  38:	tst	w10, #0x3
  3c:	cset	w11, eq  // eq = none
  40:	bfi	w9, w11, #1, #1
  44:	lsl	w11, w11, #1
  48:	lsr	w10, w10, w11
  4c:	mov	w8, #0x2                   	// #2
  50:	mvn	w11, w10
  54:	ubfx	w10, w10, #1, #1
  58:	sub	w8, w8, w10
  5c:	and	w10, w11, #0x1
  60:	neg	w10, w10
  64:	and	w8, w8, w10
  68:	add	w0, w8, w9
  6c:	ret

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csel	x9, x1, x0, eq  // eq = none
   8:	rbit	x9, x9
   c:	cset	w8, eq  // eq = none
  10:	clz	x9, x9
  14:	add	w0, w9, w8, lsl #6
  18:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	stp	q1, q0, [sp]
  1c:	fabs	d0, d2
  20:	fabs	d1, d3
  24:	fmaxnm	d0, d0, d1
  28:	stur	q2, [x29, #-16]
  2c:	str	q3, [sp, #32]
  30:	bl	2b0 <__compiler_rt_logb>
  34:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  38:	mov	v8.16b, v0.16b
  3c:	fabs	d10, d0
  40:	fmov	d0, x8
  44:	fcmp	d10, d0
  48:	mov	w19, wzr
  4c:	b.eq	80 <__divdc3+0x80>  // b.none
  50:	b.vs	80 <__divdc3+0x80>
  54:	b	58 <__divdc3+0x58>
  58:	fcvtzs	w8, d8
  5c:	neg	w19, w8
  60:	ldur	q0, [x29, #-16]
  64:	mov	w0, w19
  68:	bl	0 <scalbn>
  6c:	stur	q0, [x29, #-16]
  70:	ldr	q0, [sp, #32]
  74:	mov	w0, w19
  78:	bl	0 <scalbn>
  7c:	str	q0, [sp, #32]
  80:	ldur	q3, [x29, #-16]
  84:	ldp	q2, q4, [sp, #16]
  88:	mov	w0, w19
  8c:	fmul	d0, d3, d3
  90:	fmul	d2, d3, d2
  94:	ldr	q3, [sp]
  98:	fmul	d1, d4, d4
  9c:	fadd	d11, d0, d1
  a0:	fmul	d3, d4, d3
  a4:	fadd	d0, d2, d3
  a8:	fdiv	d0, d0, d11
  ac:	bl	0 <scalbn>
  b0:	mov	v9.16b, v0.16b
  b4:	ldur	q0, [x29, #-16]
  b8:	ldr	q1, [sp]
  bc:	mov	w0, w19
  c0:	fmul	d0, d0, d1
  c4:	ldp	q1, q2, [sp, #16]
  c8:	fmul	d1, d2, d1
  cc:	fsub	d0, d0, d1
  d0:	fdiv	d0, d0, d11
  d4:	bl	0 <scalbn>
  d8:	fcmp	d9, d9
  dc:	mov	v1.16b, v0.16b
  e0:	b.vc	294 <__divdc3+0x294>
  e4:	fcmp	d1, d1
  e8:	b.vc	294 <__divdc3+0x294>
  ec:	fcmp	d11, #0.0
  f0:	b.ne	10c <__divdc3+0x10c>  // b.any
  f4:	ldr	q0, [sp, #16]
  f8:	fcmp	d0, d0
  fc:	b.vc	26c <__divdc3+0x26c>
 100:	ldr	q0, [sp]
 104:	fcmp	d0, d0
 108:	b.vc	26c <__divdc3+0x26c>
 10c:	ldp	q0, q7, [sp, #16]
 110:	ldur	q6, [x29, #-16]
 114:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 118:	fabs	d3, d0
 11c:	ldr	q0, [sp]
 120:	fabs	d2, d6
 124:	fabs	d4, d0
 128:	fmov	d0, x8
 12c:	fcmp	d3, d0
 130:	cset	w8, eq  // eq = none
 134:	fcmp	d4, d0
 138:	cset	w9, eq  // eq = none
 13c:	fcmp	d2, d0
 140:	fabs	d0, d7
 144:	b.eq	1cc <__divdc3+0x1cc>  // b.none
 148:	b.vs	1cc <__divdc3+0x1cc>
 14c:	b	150 <__divdc3+0x150>
 150:	orr	w8, w8, w9
 154:	cbz	w8, 1cc <__divdc3+0x1cc>
 158:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 15c:	fmov	d5, x8
 160:	fcmp	d0, d5
 164:	b.eq	1cc <__divdc3+0x1cc>  // b.none
 168:	b.vs	1cc <__divdc3+0x1cc>
 16c:	b	170 <__divdc3+0x170>
 170:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 174:	fmov	d5, x8
 178:	fmov	d0, xzr
 17c:	fmov	d1, #1.000000000000000000e+00
 180:	fcmp	d3, d5
 184:	fcsel	d3, d1, d0, eq  // eq = none
 188:	fcmp	d4, d5
 18c:	ldr	q4, [sp, #16]
 190:	fcsel	d0, d1, d0, eq  // eq = none
 194:	ldr	q1, [sp]
 198:	movi	v2.2d, #0x0
 19c:	fneg	v2.2d, v2.2d
 1a0:	bit	v3.16b, v4.16b, v2.16b
 1a4:	bit	v0.16b, v1.16b, v2.16b
 1a8:	fmul	d1, d3, d6
 1ac:	fmul	d2, d3, d7
 1b0:	fmul	d3, d0, d7
 1b4:	fmul	d0, d0, d6
 1b8:	fadd	d1, d1, d3
 1bc:	fsub	d0, d0, d2
 1c0:	fmul	d9, d1, d5
 1c4:	fmul	d1, d0, d5
 1c8:	b	294 <__divdc3+0x294>
 1cc:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1d0:	fmov	d5, x8
 1d4:	fcmp	d4, d5
 1d8:	b.eq	294 <__divdc3+0x294>  // b.none
 1dc:	b.vs	294 <__divdc3+0x294>
 1e0:	b	1e4 <__divdc3+0x1e4>
 1e4:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1e8:	fmov	d4, x8
 1ec:	fcmp	d3, d4
 1f0:	b.eq	294 <__divdc3+0x294>  // b.none
 1f4:	b.vs	294 <__divdc3+0x294>
 1f8:	b	1fc <__divdc3+0x1fc>
 1fc:	fcmp	d8, #0.0
 200:	b.le	294 <__divdc3+0x294>
 204:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 208:	fmov	d3, x8
 20c:	fcmp	d10, d3
 210:	b.ne	294 <__divdc3+0x294>  // b.any
 214:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 218:	fmov	d5, x8
 21c:	fmov	d1, xzr
 220:	fmov	d3, #1.000000000000000000e+00
 224:	movi	v4.2d, #0x0
 228:	fcmp	d2, d5
 22c:	fneg	v2.2d, v4.2d
 230:	fcsel	d4, d3, d1, eq  // eq = none
 234:	fcmp	d0, d5
 238:	bit	v4.16b, v6.16b, v2.16b
 23c:	ldp	q5, q6, [sp]
 240:	fcsel	d0, d3, d1, eq  // eq = none
 244:	bit	v0.16b, v7.16b, v2.16b
 248:	fmul	d2, d4, d6
 24c:	fmul	d3, d4, d5
 250:	fmul	d4, d0, d5
 254:	fmul	d0, d0, d6
 258:	fadd	d2, d2, d4
 25c:	fsub	d0, d3, d0
 260:	fmul	d9, d2, d1
 264:	fmul	d1, d0, d1
 268:	b	294 <__divdc3+0x294>
 26c:	ldur	q2, [x29, #-16]
 270:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 274:	movi	v0.2d, #0x0
 278:	fmov	d1, x8
 27c:	fneg	v0.2d, v0.2d
 280:	bit	v1.16b, v2.16b, v0.16b
 284:	ldr	q0, [sp, #16]
 288:	fmul	d9, d1, d0
 28c:	ldr	q0, [sp]
 290:	fmul	d1, d1, d0
 294:	mov	v0.16b, v9.16b
 298:	ldr	x19, [sp, #112]
 29c:	ldp	x29, x30, [sp, #96]
 2a0:	ldp	d9, d8, [sp, #80]
 2a4:	ldp	d11, d10, [sp, #64]
 2a8:	add	sp, sp, #0x80
 2ac:	ret

00000000000002b0 <__compiler_rt_logb>:
 2b0:	stp	x29, x30, [sp, #-16]!
 2b4:	mov	x29, sp
 2b8:	bl	2c4 <__compiler_rt_logbX>
 2bc:	ldp	x29, x30, [sp], #16
 2c0:	ret

00000000000002c4 <__compiler_rt_logbX>:
 2c4:	str	d8, [sp, #-32]!
 2c8:	stp	x29, x30, [sp, #16]
 2cc:	mov	x29, sp
 2d0:	mov	v8.16b, v0.16b
 2d4:	bl	34c <toRep>
 2d8:	ubfx	x8, x0, #52, #11
 2dc:	cmp	w8, #0x7ff
 2e0:	str	x0, [x29, #8]
 2e4:	b.ne	2fc <__compiler_rt_logbX+0x38>  // b.any
 2e8:	cmn	x0, #0x1
 2ec:	fccmp	d8, d8, #0x1, le
 2f0:	fneg	d0, d8
 2f4:	fcsel	d0, d8, d0, vs
 2f8:	b	340 <__compiler_rt_logbX+0x7c>
 2fc:	fcmp	d8, #0.0
 300:	b.ne	310 <__compiler_rt_logbX+0x4c>  // b.any
 304:	mov	x8, #0xfff0000000000000    	// #-4503599627370496
 308:	fmov	d0, x8
 30c:	b	340 <__compiler_rt_logbX+0x7c>
 310:	cbz	w8, 31c <__compiler_rt_logbX+0x58>
 314:	sub	w8, w8, #0x3ff
 318:	b	33c <__compiler_rt_logbX+0x78>
 31c:	and	x8, x0, #0x7fffffffffffffff
 320:	add	x0, x29, #0x8
 324:	str	x8, [x29, #8]
 328:	bl	354 <normalize>
 32c:	ldr	x8, [x29, #8]
 330:	ubfx	x8, x8, #52, #11
 334:	add	w8, w0, w8
 338:	sub	w8, w8, #0x400
 33c:	scvtf	d0, w8
 340:	ldp	x29, x30, [sp, #16]
 344:	ldr	d8, [sp], #32
 348:	ret

000000000000034c <toRep>:
 34c:	fmov	x0, d0
 350:	ret

0000000000000354 <normalize>:
 354:	stp	x29, x30, [sp, #-48]!
 358:	str	x21, [sp, #16]
 35c:	stp	x20, x19, [sp, #32]
 360:	mov	x29, sp
 364:	ldr	x20, [x0]
 368:	mov	x19, x0
 36c:	mov	x0, x20
 370:	bl	3a4 <rep_clz>
 374:	mov	w21, w0
 378:	mov	x0, #0x10000000000000      	// #4503599627370496
 37c:	bl	3a4 <rep_clz>
 380:	sub	w8, w21, w0
 384:	lsl	x10, x20, x8
 388:	str	x10, [x19]
 38c:	ldp	x20, x19, [sp, #32]
 390:	ldr	x21, [sp, #16]
 394:	mov	w9, #0x1                   	// #1
 398:	sub	w0, w9, w8
 39c:	ldp	x29, x30, [sp], #48
 3a0:	ret

00000000000003a4 <rep_clz>:
 3a4:	clz	x0, x0
 3a8:	ret

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	sub	sp, sp, #0x60
   4:	str	d8, [sp, #32]
   8:	stp	x29, x30, [sp, #40]
   c:	str	x23, [sp, #56]
  10:	stp	x22, x21, [sp, #64]
  14:	stp	x20, x19, [sp, #80]
  18:	add	x29, sp, #0x20
  1c:	mov	v8.16b, v1.16b
  20:	bl	2a8 <toRep>
  24:	mov	v0.16b, v8.16b
  28:	mov	x20, x0
  2c:	ubfx	x21, x0, #52, #11
  30:	bl	2a8 <toRep>
  34:	eor	x8, x0, x20
  38:	sub	w11, w21, #0x1
  3c:	ubfx	x22, x0, #52, #11
  40:	and	x9, x20, #0xfffffffffffff
  44:	and	x10, x0, #0xfffffffffffff
  48:	and	x19, x8, #0x8000000000000000
  4c:	cmp	w11, #0x7fd
  50:	stur	x9, [x29, #-8]
  54:	str	x10, [sp, #16]
  58:	b.hi	70 <__divdf3+0x70>  // b.pmore
  5c:	sub	w8, w22, #0x1
  60:	cmp	w8, #0x7fe
  64:	b.cs	70 <__divdf3+0x70>  // b.hs, b.nlast
  68:	mov	w20, wzr
  6c:	b	104 <__divdf3+0x104>
  70:	mov	x9, #0x1                   	// #1
  74:	and	x8, x20, #0x7fffffffffffffff
  78:	movk	x9, #0x7ff0, lsl #48
  7c:	cmp	x8, x9
  80:	b.cc	8c <__divdf3+0x8c>  // b.lo, b.ul, b.last
  84:	orr	x0, x20, #0x8000000000000
  88:	b	f0 <__divdf3+0xf0>
  8c:	and	x23, x0, #0x7fffffffffffffff
  90:	cmp	x23, x9
  94:	b.cc	a0 <__divdf3+0xa0>  // b.lo, b.ul, b.last
  98:	orr	x0, x0, #0x8000000000000
  9c:	b	f0 <__divdf3+0xf0>
  a0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  a4:	cmp	x8, x9
  a8:	b.ne	bc <__divdf3+0xbc>  // b.any
  ac:	cmp	x23, x9
  b0:	b.ne	ec <__divdf3+0xec>  // b.any
  b4:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  b8:	b	f0 <__divdf3+0xf0>
  bc:	cmp	x23, x9
  c0:	b.ne	cc <__divdf3+0xcc>  // b.any
  c4:	mov	x0, x19
  c8:	b	f0 <__divdf3+0xf0>
  cc:	cbz	x8, 27c <__divdf3+0x27c>
  d0:	cbz	x23, ec <__divdf3+0xec>
  d4:	lsr	x8, x8, #52
  d8:	cbnz	x8, 284 <__divdf3+0x284>
  dc:	sub	x0, x29, #0x8
  e0:	bl	2b8 <normalize>
  e4:	mov	w20, w0
  e8:	b	288 <__divdf3+0x288>
  ec:	orr	x0, x19, #0x7ff0000000000000
  f0:	bl	2b0 <fromRep>
  f4:	mov	v8.16b, v0.16b
  f8:	mov	w8, wzr
  fc:	mov	w20, wzr
 100:	cbz	w8, 244 <__divdf3+0x244>
 104:	ldr	x8, [sp, #16]
 108:	sub	w9, w21, w22
 10c:	mov	w10, #0xf333                	// #62259
 110:	add	w21, w20, w9
 114:	orr	x20, x8, #0x10000000000000
 118:	movk	w10, #0x7504, lsl #16
 11c:	lsr	x11, x20, #21
 120:	ubfx	x12, x20, #21, #32
 124:	sub	w10, w10, w11
 128:	mul	x11, x10, x12
 12c:	lsr	x11, x11, #32
 130:	neg	w11, w11
 134:	umull	x10, w11, w10
 138:	ubfx	x10, x10, #31, #32
 13c:	mul	x11, x10, x12
 140:	lsr	x11, x11, #32
 144:	neg	w11, w11
 148:	mul	x10, x11, x10
 14c:	ubfx	x10, x10, #31, #32
 150:	mul	x11, x10, x12
 154:	lsr	x11, x11, #32
 158:	neg	w11, w11
 15c:	mul	x10, x11, x10
 160:	lsr	x10, x10, #31
 164:	lsl	w8, w8, #11
 168:	sub	w10, w10, #0x1
 16c:	mul	x11, x10, x12
 170:	umull	x8, w10, w8
 174:	ldur	x23, [x29, #-8]
 178:	add	x8, x11, x8, lsr #32
 17c:	neg	x8, x8
 180:	lsr	x11, x8, #32
 184:	and	x8, x8, #0xffffffff
 188:	mul	x11, x11, x10
 18c:	mul	x8, x8, x10
 190:	orr	x9, x23, #0x10000000000000
 194:	add	x8, x11, x8, lsr #32
 198:	sub	x1, x8, #0x2
 19c:	lsl	x0, x9, #2
 1a0:	add	x2, sp, #0x8
 1a4:	mov	x3, sp
 1a8:	stur	x9, [x29, #-8]
 1ac:	str	x20, [sp, #16]
 1b0:	bl	308 <wideMultiply>
 1b4:	ldr	x8, [sp, #8]
 1b8:	lsr	x9, x8, #53
 1bc:	cbnz	x9, 204 <__divdf3+0x204>
 1c0:	lsl	x9, x23, #53
 1c4:	msub	x8, x8, x20, x9
 1c8:	sub	w21, w21, #0x1
 1cc:	cmp	w21, #0x400
 1d0:	b.ge	21c <__divdf3+0x21c>  // b.tcont
 1d4:	cmn	w21, #0x3ff
 1d8:	add	w9, w21, #0x3ff
 1dc:	b.gt	224 <__divdf3+0x224>
 1e0:	cbnz	w9, 238 <__divdf3+0x238>
 1e4:	ldr	x9, [sp, #8]
 1e8:	cmp	x20, x8, lsl #1
 1ec:	and	x8, x9, #0xfffffffffffff
 1f0:	cinc	x8, x8, cc  // cc = lo, ul, last
 1f4:	tbnz	x8, #52, 264 <__divdf3+0x264>
 1f8:	mov	w8, #0x1                   	// #1
 1fc:	tbnz	w8, #0, 238 <__divdf3+0x238>
 200:	b	244 <__divdf3+0x244>
 204:	lsr	x8, x8, #1
 208:	lsl	x9, x23, #52
 20c:	str	x8, [sp, #8]
 210:	msub	x8, x8, x20, x9
 214:	cmp	w21, #0x400
 218:	b.lt	1d4 <__divdf3+0x1d4>  // b.tstop
 21c:	orr	x19, x19, #0x7ff0000000000000
 220:	b	238 <__divdf3+0x238>
 224:	ldr	x10, [sp, #8]
 228:	cmp	x20, x8, lsl #1
 22c:	bfi	x10, x9, #52, #12
 230:	cinc	x8, x10, cc  // cc = lo, ul, last
 234:	orr	x19, x8, x19
 238:	mov	x0, x19
 23c:	bl	2b0 <fromRep>
 240:	mov	v8.16b, v0.16b
 244:	mov	v0.16b, v8.16b
 248:	ldp	x20, x19, [sp, #80]
 24c:	ldp	x22, x21, [sp, #64]
 250:	ldr	x23, [sp, #56]
 254:	ldp	x29, x30, [sp, #40]
 258:	ldr	d8, [sp, #32]
 25c:	add	sp, sp, #0x60
 260:	ret
 264:	orr	x0, x8, x19
 268:	bl	2b0 <fromRep>
 26c:	mov	v8.16b, v0.16b
 270:	mov	w8, wzr
 274:	tbnz	wzr, #0, 238 <__divdf3+0x238>
 278:	b	244 <__divdf3+0x244>
 27c:	cbnz	x23, c4 <__divdf3+0xc4>
 280:	b	b4 <__divdf3+0xb4>
 284:	mov	w20, wzr
 288:	lsr	x8, x23, #52
 28c:	cbnz	x8, 29c <__divdf3+0x29c>
 290:	add	x0, sp, #0x10
 294:	bl	2b8 <normalize>
 298:	sub	w20, w20, w0
 29c:	mov	w8, #0x1                   	// #1
 2a0:	cbnz	w8, 104 <__divdf3+0x104>
 2a4:	b	244 <__divdf3+0x244>

00000000000002a8 <toRep>:
 2a8:	fmov	x0, d0
 2ac:	ret

00000000000002b0 <fromRep>:
 2b0:	fmov	d0, x0
 2b4:	ret

00000000000002b8 <normalize>:
 2b8:	stp	x29, x30, [sp, #-48]!
 2bc:	str	x21, [sp, #16]
 2c0:	stp	x20, x19, [sp, #32]
 2c4:	mov	x29, sp
 2c8:	ldr	x20, [x0]
 2cc:	mov	x19, x0
 2d0:	mov	x0, x20
 2d4:	bl	350 <rep_clz>
 2d8:	mov	w21, w0
 2dc:	mov	x0, #0x10000000000000      	// #4503599627370496
 2e0:	bl	350 <rep_clz>
 2e4:	sub	w8, w21, w0
 2e8:	lsl	x10, x20, x8
 2ec:	str	x10, [x19]
 2f0:	ldp	x20, x19, [sp, #32]
 2f4:	ldr	x21, [sp, #16]
 2f8:	mov	w9, #0x1                   	// #1
 2fc:	sub	w0, w9, w8
 300:	ldp	x29, x30, [sp], #48
 304:	ret

0000000000000308 <wideMultiply>:
 308:	and	x8, x0, #0xffffffff
 30c:	and	x9, x1, #0xffffffff
 310:	lsr	x10, x1, #32
 314:	lsr	x11, x0, #32
 318:	mul	x12, x9, x8
 31c:	mul	x8, x10, x8
 320:	mul	x9, x9, x11
 324:	mul	x10, x10, x11
 328:	lsr	x11, x12, #32
 32c:	add	x10, x10, x9, lsr #32
 330:	add	x11, x11, w8, uxtw
 334:	add	x8, x10, x8, lsr #32
 338:	add	x9, x11, w9, uxtw
 33c:	bfi	x12, x9, #32, #32
 340:	add	x8, x8, x9, lsr #32
 344:	str	x12, [x3]
 348:	str	x8, [x2]
 34c:	ret

0000000000000350 <rep_clz>:
 350:	clz	x0, x0
 354:	ret

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x8, x0, #63
  10:	asr	x9, x1, #63
  14:	eor	x10, x8, x0
  18:	eor	x11, x9, x1
  1c:	sub	x0, x10, x8
  20:	sub	x1, x11, x9
  24:	mov	x2, xzr
  28:	eor	x19, x9, x8
  2c:	bl	0 <__udivmoddi4>
  30:	eor	x8, x0, x19
  34:	sub	x0, x8, x19
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	x20, x1
  18:	mov	x21, x0
  1c:	bl	0 <__divdi3>
  20:	msub	x8, x0, x20, x21
  24:	str	x8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__divsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	str	d1, [sp]
  1c:	str	d0, [sp, #16]
  20:	fabs	s0, s2
  24:	fabs	s1, s3
  28:	fmaxnm	s0, s0, s1
  2c:	stur	q2, [x29, #-16]
  30:	str	q3, [sp, #32]
  34:	bl	2a8 <__compiler_rt_logbf>
  38:	mov	w8, #0x7f800000            	// #2139095040
  3c:	mov	v8.16b, v0.16b
  40:	fabs	s10, s0
  44:	fmov	s0, w8
  48:	fcmp	s10, s0
  4c:	mov	w19, wzr
  50:	b.eq	84 <__divsc3+0x84>  // b.none
  54:	b.vs	84 <__divsc3+0x84>
  58:	b	5c <__divsc3+0x5c>
  5c:	fcvtzs	w8, s8
  60:	neg	w19, w8
  64:	ldur	q0, [x29, #-16]
  68:	mov	w0, w19
  6c:	bl	0 <scalbnf>
  70:	stur	d0, [x29, #-16]
  74:	ldr	q0, [sp, #32]
  78:	mov	w0, w19
  7c:	bl	0 <scalbnf>
  80:	str	d0, [sp, #32]
  84:	ldur	q3, [x29, #-16]
  88:	ldp	q2, q4, [sp, #16]
  8c:	mov	w0, w19
  90:	fmul	s0, s3, s3
  94:	fmul	s2, s3, s2
  98:	ldr	q3, [sp]
  9c:	fmul	s1, s4, s4
  a0:	fadd	s11, s0, s1
  a4:	fmul	s3, s4, s3
  a8:	fadd	s0, s2, s3
  ac:	fdiv	s0, s0, s11
  b0:	bl	0 <scalbnf>
  b4:	mov	v9.16b, v0.16b
  b8:	ldur	q0, [x29, #-16]
  bc:	ldr	q1, [sp]
  c0:	mov	w0, w19
  c4:	fmul	s0, s0, s1
  c8:	ldp	q1, q2, [sp, #16]
  cc:	fmul	s1, s2, s1
  d0:	fsub	s0, s0, s1
  d4:	fdiv	s0, s0, s11
  d8:	bl	0 <scalbnf>
  dc:	fcmp	s9, s9
  e0:	mov	v1.16b, v0.16b
  e4:	b.vc	28c <__divsc3+0x28c>
  e8:	fcmp	s1, s1
  ec:	b.vc	28c <__divsc3+0x28c>
  f0:	fcmp	s11, #0.0
  f4:	b.ne	110 <__divsc3+0x110>  // b.any
  f8:	ldr	q0, [sp, #16]
  fc:	fcmp	s0, s0
 100:	b.vc	268 <__divsc3+0x268>
 104:	ldr	q0, [sp]
 108:	fcmp	s0, s0
 10c:	b.vc	268 <__divsc3+0x268>
 110:	ldp	q0, q7, [sp, #16]
 114:	ldur	q6, [x29, #-16]
 118:	mov	w8, #0x7f800000            	// #2139095040
 11c:	fmov	s2, w8
 120:	fabs	s3, s0
 124:	ldr	q0, [sp]
 128:	fcmp	s3, s2
 12c:	cset	w8, eq  // eq = none
 130:	fabs	s4, s0
 134:	fabs	s0, s6
 138:	fcmp	s4, s2
 13c:	cset	w9, eq  // eq = none
 140:	fcmp	s0, s2
 144:	fabs	s2, s7
 148:	b.eq	1cc <__divsc3+0x1cc>  // b.none
 14c:	b.vs	1cc <__divsc3+0x1cc>
 150:	b	154 <__divsc3+0x154>
 154:	orr	w8, w8, w9
 158:	cbz	w8, 1cc <__divsc3+0x1cc>
 15c:	mov	w8, #0x7f800000            	// #2139095040
 160:	fmov	s5, w8
 164:	fcmp	s2, s5
 168:	b.eq	1cc <__divsc3+0x1cc>  // b.none
 16c:	b.vs	1cc <__divsc3+0x1cc>
 170:	b	174 <__divsc3+0x174>
 174:	mov	w8, #0x7f800000            	// #2139095040
 178:	fmov	s5, w8
 17c:	fmov	s0, wzr
 180:	fmov	s1, #1.000000000000000000e+00
 184:	fcmp	s3, s5
 188:	fcsel	s3, s1, s0, eq  // eq = none
 18c:	fcmp	s4, s5
 190:	ldr	q4, [sp, #16]
 194:	fcsel	s0, s1, s0, eq  // eq = none
 198:	ldr	q1, [sp]
 19c:	movi	v2.4s, #0x80, lsl #24
 1a0:	bit	v3.16b, v4.16b, v2.16b
 1a4:	bit	v0.16b, v1.16b, v2.16b
 1a8:	fmul	s1, s3, s6
 1ac:	fmul	s2, s3, s7
 1b0:	fmul	s3, s0, s7
 1b4:	fmul	s0, s0, s6
 1b8:	fadd	s1, s1, s3
 1bc:	fsub	s0, s0, s2
 1c0:	fmul	s9, s1, s5
 1c4:	fmul	s1, s0, s5
 1c8:	b	28c <__divsc3+0x28c>
 1cc:	mov	w8, #0x7f800000            	// #2139095040
 1d0:	fmov	s5, w8
 1d4:	fcmp	s4, s5
 1d8:	b.eq	28c <__divsc3+0x28c>  // b.none
 1dc:	b.vs	28c <__divsc3+0x28c>
 1e0:	b	1e4 <__divsc3+0x1e4>
 1e4:	mov	w8, #0x7f800000            	// #2139095040
 1e8:	fmov	s4, w8
 1ec:	fcmp	s3, s4
 1f0:	b.eq	28c <__divsc3+0x28c>  // b.none
 1f4:	b.vs	28c <__divsc3+0x28c>
 1f8:	b	1fc <__divsc3+0x1fc>
 1fc:	fcmp	s8, #0.0
 200:	b.le	28c <__divsc3+0x28c>
 204:	mov	w8, #0x7f800000            	// #2139095040
 208:	fmov	s3, w8
 20c:	fcmp	s10, s3
 210:	b.ne	28c <__divsc3+0x28c>  // b.any
 214:	mov	w8, #0x7f800000            	// #2139095040
 218:	fmov	s5, w8
 21c:	fmov	s1, wzr
 220:	fmov	s3, #1.000000000000000000e+00
 224:	fcmp	s0, s5
 228:	fcsel	s0, s3, s1, eq  // eq = none
 22c:	fcmp	s2, s5
 230:	movi	v4.4s, #0x80, lsl #24
 234:	fcsel	s2, s3, s1, eq  // eq = none
 238:	bit	v0.16b, v6.16b, v4.16b
 23c:	bit	v2.16b, v7.16b, v4.16b
 240:	ldp	q4, q5, [sp]
 244:	fmul	s3, s0, s5
 248:	fmul	s0, s0, s4
 24c:	fmul	s4, s2, s4
 250:	fmul	s2, s2, s5
 254:	fadd	s3, s3, s4
 258:	fsub	s0, s0, s2
 25c:	fmul	s9, s3, s1
 260:	fmul	s1, s0, s1
 264:	b	28c <__divsc3+0x28c>
 268:	ldur	q2, [x29, #-16]
 26c:	mov	w8, #0x7f800000            	// #2139095040
 270:	movi	v0.4s, #0x80, lsl #24
 274:	fmov	s1, w8
 278:	bit	v1.16b, v2.16b, v0.16b
 27c:	ldr	q0, [sp, #16]
 280:	fmul	s9, s1, s0
 284:	ldr	q0, [sp]
 288:	fmul	s1, s1, s0
 28c:	mov	v0.16b, v9.16b
 290:	ldr	x19, [sp, #112]
 294:	ldp	x29, x30, [sp, #96]
 298:	ldp	d9, d8, [sp, #80]
 29c:	ldp	d11, d10, [sp, #64]
 2a0:	add	sp, sp, #0x80
 2a4:	ret

00000000000002a8 <__compiler_rt_logbf>:
 2a8:	stp	x29, x30, [sp, #-16]!
 2ac:	mov	x29, sp
 2b0:	bl	2bc <__compiler_rt_logbX>
 2b4:	ldp	x29, x30, [sp], #16
 2b8:	ret

00000000000002bc <__compiler_rt_logbX>:
 2bc:	str	d8, [sp, #-32]!
 2c0:	stp	x29, x30, [sp, #16]
 2c4:	mov	x29, sp
 2c8:	mov	v8.16b, v0.16b
 2cc:	bl	344 <toRep>
 2d0:	ubfx	w8, w0, #23, #8
 2d4:	cmp	w8, #0xff
 2d8:	str	w0, [x29, #12]
 2dc:	b.ne	2f4 <__compiler_rt_logbX+0x38>  // b.any
 2e0:	cmn	w0, #0x1
 2e4:	fccmp	s8, s8, #0x1, le
 2e8:	fneg	s0, s8
 2ec:	fcsel	s0, s8, s0, vs
 2f0:	b	338 <__compiler_rt_logbX+0x7c>
 2f4:	fcmp	s8, #0.0
 2f8:	b.ne	308 <__compiler_rt_logbX+0x4c>  // b.any
 2fc:	mov	w8, #0xff800000            	// #-8388608
 300:	fmov	s0, w8
 304:	b	338 <__compiler_rt_logbX+0x7c>
 308:	cbz	w8, 314 <__compiler_rt_logbX+0x58>
 30c:	sub	w8, w8, #0x7f
 310:	b	334 <__compiler_rt_logbX+0x78>
 314:	and	w8, w0, #0x7fffffff
 318:	add	x0, x29, #0xc
 31c:	str	w8, [x29, #12]
 320:	bl	34c <normalize>
 324:	ldr	w8, [x29, #12]
 328:	lsr	w8, w8, #23
 32c:	add	w8, w0, w8, uxtb
 330:	sub	w8, w8, #0x80
 334:	scvtf	s0, w8
 338:	ldp	x29, x30, [sp, #16]
 33c:	ldr	d8, [sp], #32
 340:	ret

0000000000000344 <toRep>:
 344:	fmov	w0, s0
 348:	ret

000000000000034c <normalize>:
 34c:	stp	x29, x30, [sp, #-48]!
 350:	str	x21, [sp, #16]
 354:	stp	x20, x19, [sp, #32]
 358:	mov	x29, sp
 35c:	ldr	w20, [x0]
 360:	mov	x19, x0
 364:	mov	w0, w20
 368:	bl	39c <rep_clz>
 36c:	mov	w21, w0
 370:	mov	w0, #0x800000              	// #8388608
 374:	bl	39c <rep_clz>
 378:	sub	w8, w21, w0
 37c:	lsl	w10, w20, w8
 380:	str	w10, [x19]
 384:	ldp	x20, x19, [sp, #32]
 388:	ldr	x21, [sp, #16]
 38c:	mov	w9, #0x1                   	// #1
 390:	sub	w0, w9, w8
 394:	ldp	x29, x30, [sp], #48
 398:	ret

000000000000039c <rep_clz>:
 39c:	clz	w0, w0
 3a0:	ret

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	sub	sp, sp, #0x50
   4:	str	d8, [sp, #16]
   8:	stp	x29, x30, [sp, #24]
   c:	str	x23, [sp, #40]
  10:	stp	x22, x21, [sp, #48]
  14:	stp	x20, x19, [sp, #64]
  18:	add	x29, sp, #0x10
  1c:	mov	v8.16b, v1.16b
  20:	bl	250 <toRep>
  24:	mov	v0.16b, v8.16b
  28:	mov	w20, w0
  2c:	ubfx	w21, w0, #23, #8
  30:	bl	250 <toRep>
  34:	eor	w8, w0, w20
  38:	sub	w11, w21, #0x1
  3c:	ubfx	w22, w0, #23, #8
  40:	and	w9, w20, #0x7fffff
  44:	and	w10, w0, #0x7fffff
  48:	and	w19, w8, #0x80000000
  4c:	cmp	w11, #0xfd
  50:	stur	w9, [x29, #-4]
  54:	str	w10, [sp, #8]
  58:	b.hi	70 <__divsf3+0x70>  // b.pmore
  5c:	sub	w8, w22, #0x1
  60:	cmp	w8, #0xfe
  64:	b.cs	70 <__divsf3+0x70>  // b.hs, b.nlast
  68:	mov	w20, wzr
  6c:	b	100 <__divsf3+0x100>
  70:	mov	w9, #0x1                   	// #1
  74:	and	w8, w20, #0x7fffffff
  78:	movk	w9, #0x7f80, lsl #16
  7c:	cmp	w8, w9
  80:	b.cc	8c <__divsf3+0x8c>  // b.lo, b.ul, b.last
  84:	orr	w0, w20, #0x400000
  88:	b	f0 <__divsf3+0xf0>
  8c:	and	w23, w0, #0x7fffffff
  90:	cmp	w23, w9
  94:	b.cc	a0 <__divsf3+0xa0>  // b.lo, b.ul, b.last
  98:	orr	w0, w0, #0x400000
  9c:	b	f0 <__divsf3+0xf0>
  a0:	mov	w9, #0x7f800000            	// #2139095040
  a4:	cmp	w8, w9
  a8:	b.ne	bc <__divsf3+0xbc>  // b.any
  ac:	cmp	w23, w9
  b0:	b.ne	ec <__divsf3+0xec>  // b.any
  b4:	mov	w0, #0x7fc00000            	// #2143289344
  b8:	b	f0 <__divsf3+0xf0>
  bc:	cmp	w23, w9
  c0:	b.ne	cc <__divsf3+0xcc>  // b.any
  c4:	mov	w0, w19
  c8:	b	f0 <__divsf3+0xf0>
  cc:	cbz	w8, 224 <__divsf3+0x224>
  d0:	cbz	w23, ec <__divsf3+0xec>
  d4:	lsr	w8, w8, #23
  d8:	cbnz	w8, 22c <__divsf3+0x22c>
  dc:	sub	x0, x29, #0x4
  e0:	bl	260 <normalize>
  e4:	mov	w20, w0
  e8:	b	230 <__divsf3+0x230>
  ec:	orr	w0, w19, #0x7f800000
  f0:	bl	258 <fromRep>
  f4:	mov	w8, wzr
  f8:	mov	w20, wzr
  fc:	cbz	w8, 208 <__divsf3+0x208>
 100:	ldr	w8, [sp, #8]
 104:	mov	w10, #0xf333                	// #62259
 108:	movk	w10, #0x7504, lsl #16
 10c:	ldur	w9, [x29, #-4]
 110:	orr	w8, w8, #0x800000
 114:	lsl	w12, w8, #8
 118:	sub	w10, w10, w12
 11c:	umull	x13, w10, w12
 120:	lsr	x13, x13, #32
 124:	neg	w13, w13
 128:	umull	x10, w13, w10
 12c:	ubfx	x10, x10, #31, #32
 130:	mul	x13, x10, x12
 134:	lsr	x13, x13, #32
 138:	neg	w13, w13
 13c:	mul	x10, x13, x10
 140:	ubfx	x10, x10, #31, #32
 144:	mul	x12, x10, x12
 148:	lsr	x12, x12, #32
 14c:	neg	w12, w12
 150:	mul	x10, x12, x10
 154:	orr	w13, w9, #0x800000
 158:	lsr	x10, x10, #31
 15c:	sub	w10, w10, #0x2
 160:	lsl	w12, w13, #1
 164:	sub	w11, w21, w22
 168:	umull	x10, w10, w12
 16c:	lsr	x12, x10, #56
 170:	add	w11, w20, w11
 174:	stur	w13, [x29, #-4]
 178:	str	w8, [sp, #8]
 17c:	cbnz	w12, 1c0 <__divsf3+0x1c0>
 180:	lsr	x10, x10, #32
 184:	lsl	w9, w9, #24
 188:	msub	w9, w8, w10, w9
 18c:	sub	w11, w11, #0x1
 190:	cmp	w11, #0x80
 194:	b.ge	1d4 <__divsf3+0x1d4>  // b.tcont
 198:	cmn	w11, #0x7f
 19c:	add	w11, w11, #0x7f
 1a0:	b.gt	1dc <__divsf3+0x1dc>
 1a4:	cbnz	w11, 200 <__divsf3+0x200>
 1a8:	cmp	w8, w9, lsl #1
 1ac:	and	w8, w10, #0x7fffff
 1b0:	cinc	w8, w8, cc  // cc = lo, ul, last
 1b4:	tbnz	w8, #23, 1f0 <__divsf3+0x1f0>
 1b8:	mov	w8, #0x1                   	// #1
 1bc:	b	1fc <__divsf3+0x1fc>
 1c0:	lsr	x10, x10, #33
 1c4:	lsl	w9, w9, #23
 1c8:	msub	w9, w8, w10, w9
 1cc:	cmp	w11, #0x80
 1d0:	b.lt	198 <__divsf3+0x198>  // b.tstop
 1d4:	orr	w0, w19, #0x7f800000
 1d8:	b	204 <__divsf3+0x204>
 1dc:	cmp	w8, w9, lsl #1
 1e0:	bfi	w10, w11, #23, #9
 1e4:	cinc	w8, w10, cc  // cc = lo, ul, last
 1e8:	orr	w0, w8, w19
 1ec:	b	204 <__divsf3+0x204>
 1f0:	orr	w0, w8, w19
 1f4:	bl	258 <fromRep>
 1f8:	mov	w8, wzr
 1fc:	cbz	w8, 208 <__divsf3+0x208>
 200:	mov	w0, w19
 204:	bl	258 <fromRep>
 208:	ldp	x20, x19, [sp, #64]
 20c:	ldp	x22, x21, [sp, #48]
 210:	ldr	x23, [sp, #40]
 214:	ldp	x29, x30, [sp, #24]
 218:	ldr	d8, [sp, #16]
 21c:	add	sp, sp, #0x50
 220:	ret
 224:	cbnz	w23, c4 <__divsf3+0xc4>
 228:	b	b4 <__divsf3+0xb4>
 22c:	mov	w20, wzr
 230:	lsr	w8, w23, #23
 234:	cbnz	w8, 244 <__divsf3+0x244>
 238:	add	x0, sp, #0x8
 23c:	bl	260 <normalize>
 240:	sub	w20, w20, w0
 244:	mov	w8, #0x1                   	// #1
 248:	cbnz	w8, 100 <__divsf3+0x100>
 24c:	b	208 <__divsf3+0x208>

0000000000000250 <toRep>:
 250:	fmov	w0, s0
 254:	ret

0000000000000258 <fromRep>:
 258:	fmov	s0, w0
 25c:	ret

0000000000000260 <normalize>:
 260:	stp	x29, x30, [sp, #-48]!
 264:	str	x21, [sp, #16]
 268:	stp	x20, x19, [sp, #32]
 26c:	mov	x29, sp
 270:	ldr	w20, [x0]
 274:	mov	x19, x0
 278:	mov	w0, w20
 27c:	bl	2b0 <rep_clz>
 280:	mov	w21, w0
 284:	mov	w0, #0x800000              	// #8388608
 288:	bl	2b0 <rep_clz>
 28c:	sub	w8, w21, w0
 290:	lsl	w10, w20, w8
 294:	str	w10, [x19]
 298:	ldp	x20, x19, [sp, #32]
 29c:	ldr	x21, [sp, #16]
 2a0:	mov	w9, #0x1                   	// #1
 2a4:	sub	w0, w9, w8
 2a8:	ldp	x29, x30, [sp], #48
 2ac:	ret

00000000000002b0 <rep_clz>:
 2b0:	clz	w0, w0
 2b4:	ret

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	asr	w8, w0, #31
   4:	asr	w9, w1, #31
   8:	eor	w10, w8, w0
   c:	eor	w11, w9, w1
  10:	sub	w10, w10, w8
  14:	sub	w11, w11, w9
  18:	eor	w8, w9, w8
  1c:	udiv	w9, w10, w11
  20:	eor	w9, w9, w8
  24:	sub	w0, w9, w8
  28:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x28, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	sub	sp, sp, #0x1f0
  18:	str	q1, [sp, #144]
  1c:	str	q2, [sp, #208]
  20:	stur	q2, [x29, #-48]
  24:	ldurb	w8, [x29, #-33]
  28:	and	w8, w8, #0x7f
  2c:	sturb	w8, [x29, #-33]
  30:	ldur	q2, [x29, #-48]
  34:	str	q3, [sp, #192]
  38:	stur	q3, [x29, #-32]
  3c:	ldurb	w8, [x29, #-17]
  40:	and	w8, w8, #0x7f
  44:	sturb	w8, [x29, #-17]
  48:	ldur	q1, [x29, #-32]
  4c:	str	q0, [sp, #160]
  50:	mov	v0.16b, v2.16b
  54:	bl	0 <fmaxl>
  58:	bl	5a8 <__compiler_rt_logbl>
  5c:	str	q0, [sp, #64]
  60:	stur	q0, [x29, #-16]
  64:	ldurb	w8, [x29, #-1]
  68:	and	w8, w8, #0x7f
  6c:	sturb	w8, [x29, #-1]
  70:	adrp	x8, 0 <__divtc3>
  74:	ldur	q0, [x29, #-16]
  78:	ldr	q1, [x8]
  7c:	str	q0, [sp, #80]
  80:	str	q1, [sp, #176]
  84:	bl	0 <__eqtf2>
  88:	ldr	q0, [sp, #80]
  8c:	ldr	q1, [sp, #176]
  90:	cmp	w0, #0x0
  94:	cset	w19, eq  // eq = none
  98:	bl	0 <__unordtf2>
  9c:	cmp	w0, #0x0
  a0:	cset	w8, ne  // ne = any
  a4:	orr	w8, w8, w19
  a8:	cbnz	w8, dc <__divtc3+0xdc>
  ac:	ldr	q0, [sp, #64]
  b0:	bl	0 <__fixtfsi>
  b4:	ldr	q0, [sp, #208]
  b8:	neg	w19, w0
  bc:	mov	w0, w19
  c0:	bl	0 <scalbnl>
  c4:	str	q0, [sp, #208]
  c8:	ldr	q0, [sp, #192]
  cc:	mov	w0, w19
  d0:	bl	0 <scalbnl>
  d4:	str	q0, [sp, #192]
  d8:	b	e0 <__divtc3+0xe0>
  dc:	mov	w19, wzr
  e0:	ldr	q0, [sp, #208]
  e4:	mov	v1.16b, v0.16b
  e8:	bl	0 <__multf3>
  ec:	str	q0, [sp, #128]
  f0:	ldr	q0, [sp, #192]
  f4:	mov	v1.16b, v0.16b
  f8:	bl	0 <__multf3>
  fc:	mov	v1.16b, v0.16b
 100:	ldr	q0, [sp, #128]
 104:	bl	0 <__addtf3>
 108:	str	q0, [sp, #112]
 10c:	ldr	q0, [sp, #208]
 110:	ldr	q1, [sp, #160]
 114:	bl	0 <__multf3>
 118:	str	q0, [sp, #128]
 11c:	ldr	q0, [sp, #192]
 120:	ldr	q1, [sp, #144]
 124:	bl	0 <__multf3>
 128:	mov	v1.16b, v0.16b
 12c:	ldr	q0, [sp, #128]
 130:	bl	0 <__addtf3>
 134:	ldr	q1, [sp, #112]
 138:	bl	0 <__divtf3>
 13c:	mov	w0, w19
 140:	bl	0 <scalbnl>
 144:	str	q0, [sp, #128]
 148:	ldr	q0, [sp, #208]
 14c:	ldr	q1, [sp, #144]
 150:	bl	0 <__multf3>
 154:	str	q0, [sp, #96]
 158:	ldr	q0, [sp, #192]
 15c:	ldr	q1, [sp, #160]
 160:	bl	0 <__multf3>
 164:	mov	v1.16b, v0.16b
 168:	ldr	q0, [sp, #96]
 16c:	bl	0 <__subtf3>
 170:	ldr	q1, [sp, #112]
 174:	bl	0 <__divtf3>
 178:	mov	w0, w19
 17c:	bl	0 <scalbnl>
 180:	str	q0, [sp, #96]
 184:	ldr	q0, [sp, #128]
 188:	mov	v1.16b, v0.16b
 18c:	bl	0 <__unordtf2>
 190:	cbz	w0, 53c <__divtc3+0x53c>
 194:	ldr	q0, [sp, #96]
 198:	mov	v1.16b, v0.16b
 19c:	bl	0 <__unordtf2>
 1a0:	cbz	w0, 53c <__divtc3+0x53c>
 1a4:	adrp	x19, 0 <__divtc3>
 1a8:	ldr	q1, [x19]
 1ac:	ldr	q0, [sp, #112]
 1b0:	str	q1, [sp, #48]
 1b4:	bl	0 <__netf2>
 1b8:	cbnz	w0, 1dc <__divtc3+0x1dc>
 1bc:	ldr	q0, [sp, #160]
 1c0:	mov	v1.16b, v0.16b
 1c4:	bl	0 <__unordtf2>
 1c8:	cbz	w0, 548 <__divtc3+0x548>
 1cc:	ldr	q0, [sp, #144]
 1d0:	mov	v1.16b, v0.16b
 1d4:	bl	0 <__unordtf2>
 1d8:	cbz	w0, 548 <__divtc3+0x548>
 1dc:	ldr	q0, [sp, #160]
 1e0:	stur	q0, [x29, #-144]
 1e4:	ldurb	w8, [x29, #-129]
 1e8:	and	w8, w8, #0x7f
 1ec:	sturb	w8, [x29, #-129]
 1f0:	ldur	q0, [x29, #-144]
 1f4:	ldr	q1, [sp, #176]
 1f8:	str	q0, [sp]
 1fc:	bl	0 <__eqtf2>
 200:	ldr	q0, [sp, #144]
 204:	cmp	w0, #0x0
 208:	cset	w20, eq  // eq = none
 20c:	stur	q0, [x29, #-128]
 210:	ldurb	w8, [x29, #-113]
 214:	and	w8, w8, #0x7f
 218:	sturb	w8, [x29, #-113]
 21c:	ldur	q0, [x29, #-128]
 220:	ldr	q1, [sp, #176]
 224:	str	q0, [sp, #32]
 228:	bl	0 <__eqtf2>
 22c:	ldr	q0, [sp, #208]
 230:	cmp	w0, #0x0
 234:	cset	w21, eq  // eq = none
 238:	stur	q0, [x29, #-112]
 23c:	ldurb	w8, [x29, #-97]
 240:	and	w8, w8, #0x7f
 244:	sturb	w8, [x29, #-97]
 248:	ldur	q1, [x29, #-112]
 24c:	ldr	q0, [sp, #192]
 250:	str	q1, [sp, #16]
 254:	stur	q0, [x29, #-96]
 258:	ldurb	w8, [x29, #-81]
 25c:	and	w8, w8, #0x7f
 260:	sturb	w8, [x29, #-81]
 264:	ldur	q0, [x29, #-96]
 268:	str	q0, [sp, #112]
 26c:	mov	v0.16b, v1.16b
 270:	ldr	q1, [sp, #176]
 274:	bl	0 <__eqtf2>
 278:	ldr	q0, [sp, #16]
 27c:	ldr	q1, [sp, #176]
 280:	cmp	w0, #0x0
 284:	cset	w22, eq  // eq = none
 288:	bl	0 <__unordtf2>
 28c:	cmp	w0, #0x0
 290:	cset	w8, ne  // ne = any
 294:	orr	w8, w8, w22
 298:	cbnz	w8, 3c4 <__divtc3+0x3c4>
 29c:	orr	w8, w20, w21
 2a0:	cbz	w8, 3c4 <__divtc3+0x3c4>
 2a4:	ldr	q0, [sp, #112]
 2a8:	ldr	q1, [sp, #176]
 2ac:	bl	0 <__eqtf2>
 2b0:	ldr	q0, [sp, #112]
 2b4:	ldr	q1, [sp, #176]
 2b8:	cmp	w0, #0x0
 2bc:	cset	w20, eq  // eq = none
 2c0:	bl	0 <__unordtf2>
 2c4:	cmp	w0, #0x0
 2c8:	cset	w8, ne  // ne = any
 2cc:	orr	w8, w8, w20
 2d0:	cbnz	w8, 3c4 <__divtc3+0x3c4>
 2d4:	ldr	q0, [sp]
 2d8:	ldr	q1, [sp, #176]
 2dc:	bl	0 <__eqtf2>
 2e0:	adrp	x8, 0 <__divtc3>
 2e4:	ldr	q1, [x8]
 2e8:	ldr	q0, [sp, #48]
 2ec:	cmp	w0, #0x0
 2f0:	b.ne	2f8 <__divtc3+0x2f8>  // b.any
 2f4:	mov	v0.16b, v1.16b
 2f8:	str	q1, [sp, #128]
 2fc:	ldr	q1, [sp, #160]
 300:	stp	q0, q1, [x29, #-176]
 304:	ldurb	w8, [x29, #-145]
 308:	ldurb	w9, [x29, #-161]
 30c:	bfxil	w8, w9, #0, #7
 310:	sturb	w8, [x29, #-161]
 314:	ldur	q0, [x29, #-176]
 318:	ldr	q1, [sp, #176]
 31c:	str	q0, [sp, #160]
 320:	ldr	q0, [sp, #32]
 324:	bl	0 <__eqtf2>
 328:	cmp	w0, #0x0
 32c:	b.ne	338 <__divtc3+0x338>  // b.any
 330:	ldr	q0, [sp, #128]
 334:	str	q0, [sp, #48]
 338:	ldr	q0, [sp, #144]
 33c:	stur	q0, [x29, #-192]
 340:	ldr	q0, [sp, #48]
 344:	ldurb	w8, [x29, #-177]
 348:	stur	q0, [x29, #-208]
 34c:	ldurb	w9, [x29, #-193]
 350:	bfxil	w8, w9, #0, #7
 354:	sturb	w8, [x29, #-193]
 358:	ldur	q0, [x29, #-208]
 35c:	ldr	q1, [sp, #208]
 360:	str	q0, [sp, #144]
 364:	ldr	q0, [sp, #160]
 368:	bl	0 <__multf3>
 36c:	str	q0, [sp, #128]
 370:	ldr	q0, [sp, #144]
 374:	ldr	q1, [sp, #192]
 378:	bl	0 <__multf3>
 37c:	mov	v1.16b, v0.16b
 380:	ldr	q0, [sp, #128]
 384:	bl	0 <__addtf3>
 388:	ldr	q1, [sp, #176]
 38c:	bl	0 <__multf3>
 390:	str	q0, [sp, #128]
 394:	ldr	q0, [sp, #144]
 398:	ldr	q1, [sp, #208]
 39c:	bl	0 <__multf3>
 3a0:	str	q0, [sp, #208]
 3a4:	ldr	q0, [sp, #160]
 3a8:	ldr	q1, [sp, #192]
 3ac:	bl	0 <__multf3>
 3b0:	mov	v1.16b, v0.16b
 3b4:	ldr	q0, [sp, #208]
 3b8:	bl	0 <__subtf3>
 3bc:	ldr	q1, [sp, #176]
 3c0:	b	584 <__divtc3+0x584>
 3c4:	ldr	q0, [sp, #32]
 3c8:	ldr	q1, [sp, #176]
 3cc:	bl	0 <__eqtf2>
 3d0:	ldr	q0, [sp, #32]
 3d4:	ldr	q1, [sp, #176]
 3d8:	cmp	w0, #0x0
 3dc:	cset	w20, eq  // eq = none
 3e0:	bl	0 <__unordtf2>
 3e4:	cmp	w0, #0x0
 3e8:	cset	w8, ne  // ne = any
 3ec:	orr	w8, w8, w20
 3f0:	cbnz	w8, 53c <__divtc3+0x53c>
 3f4:	ldr	q0, [sp]
 3f8:	ldr	q1, [sp, #176]
 3fc:	bl	0 <__eqtf2>
 400:	ldr	q0, [sp]
 404:	ldr	q1, [sp, #176]
 408:	cmp	w0, #0x0
 40c:	cset	w20, eq  // eq = none
 410:	bl	0 <__unordtf2>
 414:	cmp	w0, #0x0
 418:	cset	w8, ne  // ne = any
 41c:	orr	w8, w8, w20
 420:	cbnz	w8, 53c <__divtc3+0x53c>
 424:	ldr	q1, [x19]
 428:	ldr	q0, [sp, #64]
 42c:	str	q1, [sp, #64]
 430:	bl	0 <__gttf2>
 434:	cmp	w0, #0x0
 438:	b.le	53c <__divtc3+0x53c>
 43c:	ldr	q0, [sp, #80]
 440:	ldr	q1, [sp, #176]
 444:	bl	0 <__netf2>
 448:	ldr	q0, [sp, #128]
 44c:	ldr	q1, [sp, #96]
 450:	cbnz	w0, 590 <__divtc3+0x590>
 454:	ldr	q0, [sp, #16]
 458:	ldr	q1, [sp, #176]
 45c:	bl	0 <__eqtf2>
 460:	adrp	x8, 0 <__divtc3>
 464:	ldr	q1, [x8]
 468:	ldr	q0, [sp, #64]
 46c:	cmp	w0, #0x0
 470:	b.ne	478 <__divtc3+0x478>  // b.any
 474:	mov	v0.16b, v1.16b
 478:	str	q1, [sp, #128]
 47c:	ldr	q1, [sp, #208]
 480:	stp	q0, q1, [sp, #224]
 484:	ldrb	w8, [sp, #255]
 488:	ldrb	w9, [sp, #239]
 48c:	ldr	q1, [sp, #176]
 490:	bfxil	w8, w9, #0, #7
 494:	strb	w8, [sp, #239]
 498:	ldr	q0, [sp, #224]
 49c:	str	q0, [sp, #208]
 4a0:	ldr	q0, [sp, #112]
 4a4:	bl	0 <__eqtf2>
 4a8:	ldr	q0, [sp, #64]
 4ac:	cmp	w0, #0x0
 4b0:	b.ne	4b8 <__divtc3+0x4b8>  // b.any
 4b4:	ldr	q0, [sp, #128]
 4b8:	ldr	q1, [sp, #192]
 4bc:	stp	q0, q1, [x29, #-240]
 4c0:	ldurb	w8, [x29, #-209]
 4c4:	ldurb	w9, [x29, #-225]
 4c8:	bfxil	w8, w9, #0, #7
 4cc:	sturb	w8, [x29, #-225]
 4d0:	ldur	q0, [x29, #-240]
 4d4:	ldr	q1, [sp, #160]
 4d8:	str	q0, [sp, #192]
 4dc:	ldr	q0, [sp, #208]
 4e0:	bl	0 <__multf3>
 4e4:	str	q0, [sp, #176]
 4e8:	ldr	q0, [sp, #192]
 4ec:	ldr	q1, [sp, #144]
 4f0:	bl	0 <__multf3>
 4f4:	mov	v1.16b, v0.16b
 4f8:	ldr	q0, [sp, #176]
 4fc:	bl	0 <__addtf3>
 500:	ldr	q1, [sp, #64]
 504:	bl	0 <__multf3>
 508:	str	q0, [sp, #128]
 50c:	ldr	q0, [sp, #208]
 510:	ldr	q1, [sp, #144]
 514:	bl	0 <__multf3>
 518:	str	q0, [sp, #208]
 51c:	ldr	q0, [sp, #192]
 520:	ldr	q1, [sp, #160]
 524:	bl	0 <__multf3>
 528:	mov	v1.16b, v0.16b
 52c:	ldr	q0, [sp, #208]
 530:	bl	0 <__subtf3>
 534:	ldr	q1, [sp, #64]
 538:	b	584 <__divtc3+0x584>
 53c:	ldr	q0, [sp, #128]
 540:	ldr	q1, [sp, #96]
 544:	b	590 <__divtc3+0x590>
 548:	ldr	q0, [sp, #208]
 54c:	stur	q0, [x29, #-64]
 550:	ldr	q0, [sp, #176]
 554:	ldurb	w8, [x29, #-49]
 558:	stur	q0, [x29, #-80]
 55c:	ldurb	w9, [x29, #-65]
 560:	bfxil	w8, w9, #0, #7
 564:	sturb	w8, [x29, #-65]
 568:	ldur	q0, [x29, #-80]
 56c:	ldr	q1, [sp, #160]
 570:	str	q0, [sp, #208]
 574:	bl	0 <__multf3>
 578:	str	q0, [sp, #128]
 57c:	ldr	q0, [sp, #208]
 580:	ldr	q1, [sp, #144]
 584:	bl	0 <__multf3>
 588:	mov	v1.16b, v0.16b
 58c:	ldr	q0, [sp, #128]
 590:	add	sp, sp, #0x1f0
 594:	ldp	x20, x19, [sp, #48]
 598:	ldp	x22, x21, [sp, #32]
 59c:	ldr	x28, [sp, #16]
 5a0:	ldp	x29, x30, [sp], #64
 5a4:	ret

00000000000005a8 <__compiler_rt_logbl>:
 5a8:	stp	x29, x30, [sp, #-16]!
 5ac:	mov	x29, sp
 5b0:	bl	5bc <__compiler_rt_logbX>
 5b4:	ldp	x29, x30, [sp], #16
 5b8:	ret

00000000000005bc <__compiler_rt_logbX>:
 5bc:	sub	sp, sp, #0x50
 5c0:	stp	x29, x30, [sp, #32]
 5c4:	str	x21, [sp, #48]
 5c8:	stp	x20, x19, [sp, #64]
 5cc:	add	x29, sp, #0x20
 5d0:	str	q0, [sp]
 5d4:	bl	69c <toRep>
 5d8:	ubfx	x21, x1, #48, #15
 5dc:	mov	w8, #0x7fff                	// #32767
 5e0:	mov	x20, x1
 5e4:	cmp	w21, w8
 5e8:	stp	x0, x1, [sp, #16]
 5ec:	b.ne	630 <__compiler_rt_logbX+0x74>  // b.any
 5f0:	ldr	q0, [sp]
 5f4:	cmp	x20, #0x0
 5f8:	cset	w19, ge  // ge = tcont
 5fc:	mov	v1.16b, v0.16b
 600:	bl	0 <__unordtf2>
 604:	cmp	w0, #0x0
 608:	cset	w8, ne  // ne = any
 60c:	orr	w19, w8, w19
 610:	adrp	x8, 0 <__divtc3>
 614:	ldr	q0, [x8]
 618:	ldr	q1, [sp]
 61c:	bl	0 <__subtf3>
 620:	cmp	w19, #0x0
 624:	b.eq	688 <__compiler_rt_logbX+0xcc>  // b.none
 628:	ldr	q0, [sp]
 62c:	b	688 <__compiler_rt_logbX+0xcc>
 630:	adrp	x8, 0 <__divtc3>
 634:	ldr	q1, [x8]
 638:	ldr	q0, [sp]
 63c:	mov	x19, x0
 640:	bl	0 <__eqtf2>
 644:	cbnz	w0, 654 <__compiler_rt_logbX+0x98>
 648:	adrp	x8, 0 <__divtc3>
 64c:	ldr	q0, [x8]
 650:	b	688 <__compiler_rt_logbX+0xcc>
 654:	cbz	w21, 664 <__compiler_rt_logbX+0xa8>
 658:	mov	w8, #0xffffc001            	// #-16383
 65c:	add	w0, w21, w8
 660:	b	684 <__compiler_rt_logbX+0xc8>
 664:	and	x8, x20, #0x7fffffffffffffff
 668:	add	x0, sp, #0x10
 66c:	stp	x19, x8, [sp, #16]
 670:	bl	6a8 <normalize>
 674:	ldrh	w8, [sp, #30]
 678:	and	w8, w8, #0x7fff
 67c:	add	w8, w0, w8
 680:	sub	w0, w8, #0x4, lsl #12
 684:	bl	0 <__floatsitf>
 688:	ldp	x20, x19, [sp, #64]
 68c:	ldr	x21, [sp, #48]
 690:	ldp	x29, x30, [sp, #32]
 694:	add	sp, sp, #0x50
 698:	ret

000000000000069c <toRep>:
 69c:	str	q0, [sp, #-16]!
 6a0:	ldp	x0, x1, [sp], #16
 6a4:	ret

00000000000006a8 <normalize>:
 6a8:	stp	x29, x30, [sp, #-48]!
 6ac:	stp	x22, x21, [sp, #16]
 6b0:	stp	x20, x19, [sp, #32]
 6b4:	mov	x29, sp
 6b8:	ldp	x21, x20, [x0]
 6bc:	mov	x19, x0
 6c0:	mov	x0, x21
 6c4:	mov	x1, x20
 6c8:	bl	72c <rep_clz>
 6cc:	mov	w22, w0
 6d0:	mov	x1, #0x1000000000000       	// #281474976710656
 6d4:	mov	x0, xzr
 6d8:	bl	72c <rep_clz>
 6dc:	sub	w8, w22, w0
 6e0:	neg	x10, x8
 6e4:	mov	w9, #0x1                   	// #1
 6e8:	cmp	x8, #0x0
 6ec:	lsr	x10, x21, x10
 6f0:	lsl	x11, x20, x8
 6f4:	lsl	x12, x21, x8
 6f8:	lsl	x13, x21, x8
 6fc:	sub	w0, w9, w8
 700:	sub	x8, x8, #0x40
 704:	csel	x9, xzr, x10, eq  // eq = none
 708:	cmp	x8, #0x0
 70c:	orr	x8, x9, x11
 710:	csel	x9, xzr, x13, ge  // ge = tcont
 714:	csel	x8, x12, x8, ge  // ge = tcont
 718:	stp	x9, x8, [x19]
 71c:	ldp	x20, x19, [sp, #32]
 720:	ldp	x22, x21, [sp, #16]
 724:	ldp	x29, x30, [sp], #48
 728:	ret

000000000000072c <rep_clz>:
 72c:	cmp	x1, #0x0
 730:	csel	x9, x0, x1, eq  // eq = none
 734:	cset	w8, eq  // eq = none
 738:	clz	x9, x9
 73c:	add	w0, w9, w8, lsl #6
 740:	ret

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x8, x1, #63
  10:	eor	x11, x8, x0
  14:	asr	x9, x3, #63
  18:	eor	x10, x8, x1
  1c:	subs	x0, x11, x8
  20:	eor	x13, x9, x2
  24:	sbcs	x1, x10, x8
  28:	eor	x12, x9, x3
  2c:	subs	x2, x13, x9
  30:	sbcs	x3, x12, x9
  34:	mov	x4, xzr
  38:	eor	x19, x9, x8
  3c:	bl	0 <__udivmodti4>
  40:	eor	x9, x0, x19
  44:	eor	x8, x1, x19
  48:	subs	x0, x9, x19
  4c:	sbcs	x1, x8, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #32
  58:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	sub	sp, sp, #0x100
   4:	stp	x29, x30, [sp, #176]
   8:	stp	x26, x25, [sp, #192]
   c:	stp	x24, x23, [sp, #208]
  10:	stp	x22, x21, [sp, #224]
  14:	stp	x20, x19, [sp, #240]
  18:	add	x29, sp, #0xb0
  1c:	str	q1, [sp]
  20:	bl	400 <toRep>
  24:	ldr	q0, [sp]
  28:	mov	x20, x0
  2c:	mov	x21, x1
  30:	ubfx	x22, x1, #48, #15
  34:	bl	400 <toRep>
  38:	eor	x8, x1, x21
  3c:	sub	w11, w22, #0x1
  40:	mov	w12, #0x7ffd                	// #32765
  44:	ubfx	x23, x1, #48, #15
  48:	and	x9, x21, #0xffffffffffff
  4c:	and	x10, x1, #0xffffffffffff
  50:	and	x19, x8, #0x8000000000000000
  54:	cmp	w11, w12
  58:	stp	x20, x9, [x29, #-16]
  5c:	stp	x0, x10, [x29, #-32]
  60:	b.hi	7c <__divtf3+0x7c>  // b.pmore
  64:	sub	w8, w23, #0x1
  68:	mov	w9, #0x7ffe                	// #32766
  6c:	cmp	w8, w9
  70:	b.cs	7c <__divtf3+0x7c>  // b.hs, b.nlast
  74:	mov	w20, wzr
  78:	b	144 <__divtf3+0x144>
  7c:	and	x8, x21, #0x7fffffffffffffff
  80:	cmp	x20, #0x0
  84:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
  88:	cset	w10, eq  // eq = none
  8c:	cmp	x8, x9
  90:	cset	w11, cc  // cc = lo, ul, last
  94:	csel	w10, w10, w11, eq  // eq = none
  98:	tbnz	w10, #0, a8 <__divtf3+0xa8>
  9c:	orr	x1, x21, #0x800000000000
  a0:	mov	x0, x20
  a4:	b	134 <__divtf3+0x134>
  a8:	and	x21, x1, #0x7fffffffffffffff
  ac:	cmp	x0, #0x0
  b0:	cset	w10, eq  // eq = none
  b4:	cmp	x21, x9
  b8:	cset	w9, cc  // cc = lo, ul, last
  bc:	csel	w9, w10, w9, eq  // eq = none
  c0:	tbnz	w9, #0, cc <__divtf3+0xcc>
  c4:	orr	x1, x1, #0x800000000000
  c8:	b	134 <__divtf3+0x134>
  cc:	eor	x9, x8, #0x7fff000000000000
  d0:	orr	x9, x20, x9
  d4:	cbnz	x9, ec <__divtf3+0xec>
  d8:	eor	x8, x21, #0x7fff000000000000
  dc:	orr	x8, x0, x8
  e0:	cbnz	x8, 12c <__divtf3+0x12c>
  e4:	mov	x1, #0x7fff800000000000    	// #9223231299366420480
  e8:	b	130 <__divtf3+0x130>
  ec:	eor	x9, x21, #0x7fff000000000000
  f0:	orr	x9, x0, x9
  f4:	cbnz	x9, 104 <__divtf3+0x104>
  f8:	mov	x0, xzr
  fc:	mov	x1, x19
 100:	b	134 <__divtf3+0x134>
 104:	orr	x9, x20, x8
 108:	cbz	x9, 3d0 <__divtf3+0x3d0>
 10c:	orr	x9, x0, x21
 110:	cbz	x9, 12c <__divtf3+0x12c>
 114:	lsr	x8, x8, #48
 118:	cbnz	x8, 3dc <__divtf3+0x3dc>
 11c:	sub	x0, x29, #0x10
 120:	bl	418 <normalize>
 124:	mov	w20, w0
 128:	b	3e0 <__divtf3+0x3e0>
 12c:	orr	x1, x19, #0x7fff000000000000
 130:	mov	x0, xzr
 134:	bl	40c <fromRep>
 138:	mov	w8, wzr
 13c:	mov	w20, wzr
 140:	cbz	w8, 3a0 <__divtf3+0x3a0>
 144:	ldp	x21, x9, [x29, #-32]
 148:	ldp	x25, x8, [x29, #-16]
 14c:	mov	x11, #0x6484                	// #25732
 150:	sub	w10, w22, w23
 154:	movk	x11, #0xf9de, lsl #16
 158:	movk	x11, #0xf333, lsl #32
 15c:	add	w24, w20, w10
 160:	orr	x20, x9, #0x1000000000000
 164:	movk	x11, #0x7504, lsl #48
 168:	extr	x2, x20, x21, #49
 16c:	orr	x26, x8, #0x1000000000000
 170:	sub	x8, x11, x2
 174:	umulh	x9, x2, x8
 178:	neg	x10, x9
 17c:	mneg	x9, x9, x8
 180:	umulh	x8, x10, x8
 184:	extr	x8, x8, x9, #63
 188:	umulh	x9, x8, x2
 18c:	neg	x10, x9
 190:	mneg	x9, x9, x8
 194:	umulh	x8, x8, x10
 198:	extr	x8, x8, x9, #63
 19c:	umulh	x9, x8, x2
 1a0:	neg	x10, x9
 1a4:	mneg	x9, x9, x8
 1a8:	umulh	x8, x8, x10
 1ac:	extr	x8, x8, x9, #63
 1b0:	umulh	x9, x8, x2
 1b4:	neg	x10, x9
 1b8:	mneg	x9, x9, x8
 1bc:	umulh	x8, x8, x10
 1c0:	extr	x8, x8, x9, #63
 1c4:	umulh	x9, x8, x2
 1c8:	neg	x10, x9
 1cc:	mneg	x9, x9, x8
 1d0:	umulh	x8, x8, x10
 1d4:	extr	x8, x8, x9, #63
 1d8:	sub	x22, x8, #0x1
 1dc:	add	x4, sp, #0x40
 1e0:	sub	x5, x29, #0x30
 1e4:	mov	x0, x22
 1e8:	mov	x1, xzr
 1ec:	mov	x3, xzr
 1f0:	str	q0, [sp]
 1f4:	stur	x26, [x29, #-8]
 1f8:	stur	x20, [x29, #-24]
 1fc:	lsl	x23, x21, #15
 200:	bl	49c <wideMultiply>
 204:	add	x4, sp, #0x40
 208:	sub	x5, x29, #0x40
 20c:	mov	x0, x22
 210:	mov	x1, xzr
 214:	mov	x2, x23
 218:	mov	x3, xzr
 21c:	bl	49c <wideMultiply>
 220:	ldp	x9, x8, [x29, #-56]
 224:	ldur	x10, [x29, #-40]
 228:	add	x4, sp, #0x40
 22c:	sub	x5, x29, #0x50
 230:	adds	x8, x8, x9
 234:	adcs	x9, x10, xzr
 238:	negs	x23, x8
 23c:	ngcs	x2, x9
 240:	mov	x0, x22
 244:	mov	x1, xzr
 248:	mov	x3, xzr
 24c:	bl	49c <wideMultiply>
 250:	add	x4, sp, #0x40
 254:	add	x5, sp, #0x50
 258:	mov	x0, x22
 25c:	mov	x1, xzr
 260:	mov	x2, x23
 264:	mov	x3, xzr
 268:	bl	49c <wideMultiply>
 26c:	ldp	x8, x10, [x29, #-80]
 270:	ldr	x9, [sp, #88]
 274:	mov	x11, #0xffffffffffffffff    	// #-1
 278:	extr	x1, x26, x25, #62
 27c:	lsl	x0, x25, #2
 280:	adds	x8, x8, x9
 284:	adcs	x9, x10, xzr
 288:	subs	x2, x8, #0x2
 28c:	adcs	x3, x9, x11
 290:	add	x4, sp, #0x30
 294:	add	x5, sp, #0x20
 298:	bl	49c <wideMultiply>
 29c:	ldp	x0, x1, [sp, #48]
 2a0:	lsr	x8, x1, #49
 2a4:	cbnz	x8, 2cc <__divtf3+0x2cc>
 2a8:	add	x4, sp, #0x40
 2ac:	add	x5, sp, #0x10
 2b0:	mov	x2, x21
 2b4:	mov	x3, x20
 2b8:	bl	49c <wideMultiply>
 2bc:	ldp	x8, x9, [sp, #16]
 2c0:	lsl	x10, x25, #49
 2c4:	sub	w24, w24, #0x1
 2c8:	b	2f4 <__divtf3+0x2f4>
 2cc:	extr	x0, x1, x0, #1
 2d0:	lsr	x1, x1, #1
 2d4:	add	x4, sp, #0x40
 2d8:	add	x5, sp, #0x10
 2dc:	mov	x2, x21
 2e0:	mov	x3, x20
 2e4:	stp	x0, x1, [sp, #48]
 2e8:	bl	49c <wideMultiply>
 2ec:	ldp	x8, x9, [sp, #16]
 2f0:	lsl	x10, x25, #48
 2f4:	negs	x8, x8
 2f8:	sbcs	x9, x10, x9
 2fc:	ldr	q0, [sp]
 300:	cmp	w24, #0x4, lsl #12
 304:	b.lt	314 <__divtf3+0x314>  // b.tstop
 308:	mov	x22, xzr
 30c:	orr	x19, x19, #0x7fff000000000000
 310:	b	394 <__divtf3+0x394>
 314:	mov	w10, #0x3fff                	// #16383
 318:	mov	w11, #0xffffc001            	// #-16383
 31c:	cmp	w24, w11
 320:	add	w10, w24, w10
 324:	b.gt	368 <__divtf3+0x368>
 328:	mov	x22, xzr
 32c:	cbnz	w10, 394 <__divtf3+0x394>
 330:	extr	x9, x9, x8, #63
 334:	cmp	x21, x8, lsl #1
 338:	ldp	x8, x10, [sp, #48]
 33c:	cset	w11, cc  // cc = lo, ul, last
 340:	cmp	x9, x20
 344:	cset	w9, hi  // hi = pmore
 348:	csel	w9, w11, w9, eq  // eq = none
 34c:	and	x10, x10, #0xffffffffffff
 350:	adds	x0, x8, x9
 354:	adcs	x8, x10, xzr
 358:	tbnz	x8, #48, 3bc <__divtf3+0x3bc>
 35c:	mov	w8, #0x1                   	// #1
 360:	tbnz	w8, #0, 394 <__divtf3+0x394>
 364:	b	3a0 <__divtf3+0x3a0>
 368:	extr	x9, x9, x8, #63
 36c:	cmp	x21, x8, lsl #1
 370:	ldp	x8, x11, [sp, #48]
 374:	cset	w12, ls  // ls = plast
 378:	cmp	x9, x20
 37c:	cset	w9, cs  // cs = hs, nlast
 380:	csel	w9, w12, w9, eq  // eq = none
 384:	bfi	x11, x10, #48, #16
 388:	adds	x22, x8, x9
 38c:	adcs	x8, x11, xzr
 390:	orr	x19, x8, x19
 394:	mov	x0, x22
 398:	mov	x1, x19
 39c:	bl	40c <fromRep>
 3a0:	ldp	x20, x19, [sp, #240]
 3a4:	ldp	x22, x21, [sp, #224]
 3a8:	ldp	x24, x23, [sp, #208]
 3ac:	ldp	x26, x25, [sp, #192]
 3b0:	ldp	x29, x30, [sp, #176]
 3b4:	add	sp, sp, #0x100
 3b8:	ret
 3bc:	orr	x1, x8, x19
 3c0:	bl	40c <fromRep>
 3c4:	mov	w8, wzr
 3c8:	tbnz	wzr, #0, 394 <__divtf3+0x394>
 3cc:	b	3a0 <__divtf3+0x3a0>
 3d0:	orr	x8, x0, x21
 3d4:	cbnz	x8, f8 <__divtf3+0xf8>
 3d8:	b	e4 <__divtf3+0xe4>
 3dc:	mov	w20, wzr
 3e0:	lsr	x8, x21, #48
 3e4:	cbnz	x8, 3f4 <__divtf3+0x3f4>
 3e8:	sub	x0, x29, #0x20
 3ec:	bl	418 <normalize>
 3f0:	sub	w20, w20, w0
 3f4:	mov	w8, #0x1                   	// #1
 3f8:	cbnz	w8, 144 <__divtf3+0x144>
 3fc:	b	3a0 <__divtf3+0x3a0>

0000000000000400 <toRep>:
 400:	str	q0, [sp, #-16]!
 404:	ldp	x0, x1, [sp], #16
 408:	ret

000000000000040c <fromRep>:
 40c:	stp	x0, x1, [sp, #-16]!
 410:	ldr	q0, [sp], #16
 414:	ret

0000000000000418 <normalize>:
 418:	stp	x29, x30, [sp, #-48]!
 41c:	stp	x22, x21, [sp, #16]
 420:	stp	x20, x19, [sp, #32]
 424:	mov	x29, sp
 428:	ldp	x21, x20, [x0]
 42c:	mov	x19, x0
 430:	mov	x0, x21
 434:	mov	x1, x20
 438:	bl	5a0 <rep_clz>
 43c:	mov	w22, w0
 440:	mov	x1, #0x1000000000000       	// #281474976710656
 444:	mov	x0, xzr
 448:	bl	5a0 <rep_clz>
 44c:	sub	w8, w22, w0
 450:	neg	x10, x8
 454:	mov	w9, #0x1                   	// #1
 458:	cmp	x8, #0x0
 45c:	lsr	x10, x21, x10
 460:	lsl	x11, x20, x8
 464:	lsl	x12, x21, x8
 468:	lsl	x13, x21, x8
 46c:	sub	w0, w9, w8
 470:	sub	x8, x8, #0x40
 474:	csel	x9, xzr, x10, eq  // eq = none
 478:	cmp	x8, #0x0
 47c:	orr	x8, x9, x11
 480:	csel	x9, xzr, x13, ge  // ge = tcont
 484:	csel	x8, x12, x8, ge  // ge = tcont
 488:	stp	x9, x8, [x19]
 48c:	ldp	x20, x19, [sp, #32]
 490:	ldp	x22, x21, [sp, #16]
 494:	ldp	x29, x30, [sp], #48
 498:	ret

000000000000049c <wideMultiply>:
 49c:	str	x19, [sp, #-16]!
 4a0:	lsr	x8, x1, #32
 4a4:	lsr	x9, x3, #32
 4a8:	and	x10, x3, #0xffffffff
 4ac:	lsr	x11, x2, #32
 4b0:	and	x12, x2, #0xffffffff
 4b4:	and	x13, x1, #0xffffffff
 4b8:	lsr	x14, x0, #32
 4bc:	and	x15, x0, #0xffffffff
 4c0:	mul	x17, x11, x8
 4c4:	mul	x2, x11, x13
 4c8:	mul	x3, x14, x9
 4cc:	mul	x6, x10, x14
 4d0:	mul	x7, x11, x14
 4d4:	mul	x14, x14, x12
 4d8:	mul	x11, x11, x15
 4dc:	adds	x11, x11, x14
 4e0:	mul	x0, x13, x9
 4e4:	mul	x1, x10, x13
 4e8:	mul	x13, x13, x12
 4ec:	adcs	x14, xzr, xzr
 4f0:	adds	x13, x7, x13
 4f4:	mul	x16, x10, x8
 4f8:	mul	x18, x12, x8
 4fc:	mul	x19, x15, x9
 500:	mul	x10, x10, x15
 504:	mul	x12, x12, x15
 508:	adcs	x15, xzr, xzr
 50c:	adds	x10, x13, x10
 510:	adcs	x13, x15, xzr
 514:	adds	x15, x19, x18
 518:	adcs	x18, xzr, xzr
 51c:	adds	x15, x15, x2
 520:	adcs	x18, x18, xzr
 524:	adds	x15, x15, x6
 528:	adcs	x18, x18, xzr
 52c:	adds	x16, x16, x0
 530:	adcs	x0, xzr, xzr
 534:	extr	x14, x14, x11, #32
 538:	adds	x11, x12, x11, lsl #32
 53c:	extr	x12, x0, x16, #32
 540:	adcs	x0, xzr, xzr
 544:	adds	x10, x10, x14
 548:	adcs	x14, xzr, xzr
 54c:	adds	x10, x10, x15, lsl #32
 550:	adcs	x14, x14, xzr
 554:	add	x10, x0, x10
 558:	stp	x11, x10, [x5]
 55c:	adds	x10, x1, x3
 560:	adcs	x11, xzr, xzr
 564:	adds	x10, x10, x17
 568:	adcs	x11, x11, xzr
 56c:	madd	x8, x9, x8, x11
 570:	adds	x9, x10, x16, lsl #32
 574:	adcs	x8, x8, x12
 578:	adds	x9, x9, x13
 57c:	extr	x18, x18, x15, #32
 580:	adcs	x8, x8, xzr
 584:	adds	x9, x9, x18
 588:	adcs	x8, x8, xzr
 58c:	adds	x9, x9, x14
 590:	adcs	x8, x8, xzr
 594:	stp	x9, x8, [x4]
 598:	ldr	x19, [sp], #16
 59c:	ret

00000000000005a0 <rep_clz>:
 5a0:	cmp	x1, #0x0
 5a4:	csel	x9, x0, x1, eq  // eq = none
 5a8:	cset	w8, eq  // eq = none
 5ac:	clz	x9, x9
 5b0:	add	w0, w9, w8, lsl #6
 5b4:	ret

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__extendXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__extendXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	98 <srcToRep>
  20:	and	w9, w0, #0x7fffffff
  24:	sub	w8, w9, #0x800, lsl #12
  28:	lsr	w8, w8, #24
  2c:	cmp	w8, #0x7e
  30:	and	w8, w0, #0x80000000
  34:	b.hi	44 <__extendXfYf2__+0x30>  // b.pmore
  38:	mov	x10, #0x3800000000000000    	// #4035225266123964416
  3c:	add	x9, x10, x9, lsl #29
  40:	b	88 <__extendXfYf2__+0x74>
  44:	lsr	w10, w9, #23
  48:	cmp	w10, #0xff
  4c:	b.cc	60 <__extendXfYf2__+0x4c>  // b.lo, b.ul, b.last
  50:	mov	w9, w0
  54:	lsl	x9, x9, #29
  58:	orr	x9, x9, #0x7ff0000000000000
  5c:	b	88 <__extendXfYf2__+0x74>
  60:	cbz	w9, 84 <__extendXfYf2__+0x70>
  64:	clz	w10, w9
  68:	add	w12, w10, #0x15
  6c:	mov	w11, #0x389                 	// #905
  70:	lsl	x9, x9, x12
  74:	eor	x9, x9, #0x10000000000000
  78:	sub	w10, w11, w10
  7c:	orr	x9, x9, x10, lsl #52
  80:	b	88 <__extendXfYf2__+0x74>
  84:	mov	x9, xzr
  88:	orr	x0, x9, x8, lsl #32
  8c:	bl	a0 <dstFromRep>
  90:	ldp	x29, x30, [sp], #16
  94:	ret

0000000000000098 <srcToRep>:
  98:	fmov	w0, s0
  9c:	ret

00000000000000a0 <dstFromRep>:
  a0:	fmov	d0, x0
  a4:	ret

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__extendXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__extendXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	a0 <srcToRep>
  20:	and	w9, w0, #0x7fff
  24:	sub	w8, w9, #0x400
  28:	ubfx	w8, w8, #11, #5
  2c:	cmp	w8, #0xe
  30:	and	w8, w0, #0xffff8000
  34:	b.hi	44 <__extendXfYf2__+0x30>  // b.pmore
  38:	mov	w10, #0x38000000            	// #939524096
  3c:	add	w9, w10, w9, lsl #13
  40:	b	7c <__extendXfYf2__+0x68>
  44:	lsr	w10, w9, #10
  48:	cmp	w10, #0x1f
  4c:	b.cc	5c <__extendXfYf2__+0x48>  // b.lo, b.ul, b.last
  50:	lsl	w9, w9, #13
  54:	orr	w9, w9, #0x7f800000
  58:	b	7c <__extendXfYf2__+0x68>
  5c:	cbz	w9, 7c <__extendXfYf2__+0x68>
  60:	clz	w10, w9
  64:	sub	w12, w10, #0x8
  68:	mov	w11, #0x43000000            	// #1124073472
  6c:	lsl	w9, w9, w12
  70:	eor	w9, w9, #0x800000
  74:	sub	w10, w11, w10, lsl #23
  78:	orr	w9, w9, w10
  7c:	orr	w0, w9, w8, lsl #16
  80:	bl	a4 <dstFromRep>
  84:	ldp	x29, x30, [sp], #16
  88:	ret

000000000000008c <__gnu_h2f_ieee>:
  8c:	stp	x29, x30, [sp, #-16]!
  90:	mov	x29, sp
  94:	bl	0 <__extendhfsf2>
  98:	ldp	x29, x30, [sp], #16
  9c:	ret

00000000000000a0 <srcToRep>:
  a0:	ret

00000000000000a4 <dstFromRep>:
  a4:	fmov	s0, w0
  a8:	ret

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbz	w0, 14 <__ffsdi2+0x14>
   4:	rbit	w8, w0
   8:	clz	w8, w8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	lsr	x8, x0, #32
  18:	cbz	w8, 2c <__ffsdi2+0x2c>
  1c:	rbit	w8, w8
  20:	clz	w8, w8
  24:	add	w0, w8, #0x21
  28:	ret
  2c:	mov	w0, wzr
  30:	ret

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	rbit	w8, w0
   4:	clz	w8, w8
   8:	cmp	w0, #0x0
   c:	csinc	w0, wzr, w8, eq  // eq = none
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 14 <__ffsti2+0x14>
   4:	rbit	x8, x0
   8:	clz	x8, x8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	cbz	x1, 28 <__ffsti2+0x28>
  18:	rbit	x8, x1
  1c:	clz	x8, x8
  20:	add	w0, w8, #0x41
  24:	ret
  28:	mov	w0, wzr
  2c:	ret

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	fcmp	d0, #0.0
   c:	b.pl	24 <__fixdfdi+0x24>  // b.nfrst
  10:	fneg	d0, d0
  14:	bl	0 <__fixunsdfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	bl	0 <__fixunsdfdi>
  28:	ldp	x29, x30, [sp], #16
  2c:	ret

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	78 <toRep>
  20:	ubfx	x8, x0, #52, #11
  24:	subs	w9, w8, #0x3ff
  28:	b.cs	38 <__fixint+0x24>  // b.hs, b.nlast
  2c:	mov	w0, wzr
  30:	ldp	x29, x30, [sp], #16
  34:	ret
  38:	cmp	w9, #0x20
  3c:	b.cc	54 <__fixint+0x40>  // b.lo, b.ul, b.last
  40:	cmp	x0, #0x0
  44:	mov	w8, #0x80000000            	// #-2147483648
  48:	cinv	w0, w8, ge  // ge = tcont
  4c:	ldp	x29, x30, [sp], #16
  50:	ret
  54:	mov	x9, #0x10000000000000      	// #4503599627370496
  58:	mov	w10, #0x433                 	// #1075
  5c:	bfxil	x9, x0, #0, #52
  60:	sub	w8, w10, w8
  64:	lsr	x8, x9, x8
  68:	cmp	x0, #0x0
  6c:	cneg	w0, w8, lt  // lt = tstop
  70:	ldp	x29, x30, [sp], #16
  74:	ret

0000000000000078 <toRep>:
  78:	fmov	x0, d0
  7c:	ret

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	e0 <toRep>
  20:	cmp	x0, #0x0
  24:	mov	x8, #0xffffffffffffffff    	// #-1
  28:	ubfx	x11, x0, #52, #11
  2c:	cneg	x8, x8, ge  // ge = tcont
  30:	subs	w9, w11, #0x3ff
  34:	b.cs	48 <__fixint+0x34>  // b.hs, b.nlast
  38:	mov	x0, xzr
  3c:	mov	x1, xzr
  40:	ldp	x29, x30, [sp], #16
  44:	ret
  48:	cmp	w9, #0x80
  4c:	b.cc	68 <__fixint+0x54>  // b.lo, b.ul, b.last
  50:	cmp	x0, #0x0
  54:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  58:	cinv	x1, x8, ge  // ge = tcont
  5c:	csetm	x0, ge  // ge = tcont
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	mov	x10, #0x10000000000000      	// #4503599627370496
  6c:	asr	x9, x0, #63
  70:	cmp	w11, #0x432
  74:	bfxil	x10, x0, #0, #52
  78:	b.hi	9c <__fixint+0x88>  // b.pmore
  7c:	mov	w12, #0x433                 	// #1075
  80:	sub	w11, w12, w11
  84:	lsr	x10, x10, x11
  88:	umulh	x11, x8, x10
  8c:	madd	x1, x9, x10, x11
  90:	mul	x0, x8, x10
  94:	ldp	x29, x30, [sp], #16
  98:	ret
  9c:	sub	w11, w11, #0x433
  a0:	neg	x12, x11
  a4:	cmp	x11, #0x0
  a8:	lsl	x13, x10, x11
  ac:	sub	x14, x11, #0x40
  b0:	lsl	x11, x10, x11
  b4:	lsr	x10, x10, x12
  b8:	csel	x10, xzr, x10, eq  // eq = none
  bc:	cmp	x14, #0x0
  c0:	csel	x11, xzr, x11, ge  // ge = tcont
  c4:	umulh	x12, x11, x8
  c8:	csel	x10, x13, x10, ge  // ge = tcont
  cc:	madd	x9, x11, x9, x12
  d0:	madd	x1, x10, x8, x9
  d4:	mul	x0, x11, x8
  d8:	ldp	x29, x30, [sp], #16
  dc:	ret

00000000000000e0 <toRep>:
  e0:	fmov	x0, d0
  e4:	ret

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	fcmp	s0, #0.0
   c:	b.pl	24 <__fixsfdi+0x24>  // b.nfrst
  10:	fneg	s0, s0
  14:	bl	0 <__fixunssfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	bl	0 <__fixunssfdi>
  28:	ldp	x29, x30, [sp], #16
  2c:	ret

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	94 <toRep>
  20:	cmp	w0, #0x0
  24:	mov	w8, #0x1                   	// #1
  28:	ubfx	w9, w0, #23, #8
  2c:	cneg	w8, w8, lt  // lt = tstop
  30:	subs	w10, w9, #0x7f
  34:	b.cs	44 <__fixint+0x30>  // b.hs, b.nlast
  38:	mov	w0, wzr
  3c:	ldp	x29, x30, [sp], #16
  40:	ret
  44:	cmp	w10, #0x20
  48:	b.cc	60 <__fixint+0x4c>  // b.lo, b.ul, b.last
  4c:	cmp	w0, #0x0
  50:	mov	w8, #0x7fffffff            	// #2147483647
  54:	cinv	w0, w8, lt  // lt = tstop
  58:	ldp	x29, x30, [sp], #16
  5c:	ret
  60:	mov	w10, #0x800000              	// #8388608
  64:	cmp	w9, #0x95
  68:	bfxil	w10, w0, #0, #23
  6c:	b.hi	80 <__fixint+0x6c>  // b.pmore
  70:	mov	w11, #0x96                  	// #150
  74:	sub	w9, w11, w9
  78:	lsr	w9, w10, w9
  7c:	b	88 <__fixint+0x74>
  80:	sub	w9, w9, #0x96
  84:	lsl	w9, w10, w9
  88:	mul	w0, w9, w8
  8c:	ldp	x29, x30, [sp], #16
  90:	ret

0000000000000094 <toRep>:
  94:	fmov	w0, s0
  98:	ret

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	e0 <toRep>
  20:	cmp	w0, #0x0
  24:	mov	w8, #0x1                   	// #1
  28:	ubfx	w9, w0, #23, #8
  2c:	cneg	w10, w8, lt  // lt = tstop
  30:	subs	w8, w9, #0x7f
  34:	b.cs	48 <__fixint+0x34>  // b.hs, b.nlast
  38:	mov	x0, xzr
  3c:	mov	x1, xzr
  40:	ldp	x29, x30, [sp], #16
  44:	ret
  48:	cmp	w8, #0x80
  4c:	b.cc	68 <__fixint+0x54>  // b.lo, b.ul, b.last
  50:	cmp	w0, #0x0
  54:	mov	x8, #0x7fffffffffffffff    	// #9223372036854775807
  58:	cinv	x1, x8, lt  // lt = tstop
  5c:	csetm	x0, ge  // ge = tcont
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	mov	w8, #0x800000              	// #8388608
  6c:	cmp	w9, #0x95
  70:	bfxil	w8, w0, #0, #23
  74:	sxtw	x10, w10
  78:	b.hi	98 <__fixint+0x84>  // b.pmore
  7c:	mov	w11, #0x96                  	// #150
  80:	sub	w9, w11, w9
  84:	lsr	w8, w8, w9
  88:	smulh	x1, x8, x10
  8c:	mul	x0, x8, x10
  90:	ldp	x29, x30, [sp], #16
  94:	ret
  98:	sub	w9, w9, #0x96
  9c:	neg	x12, x9
  a0:	cmp	x9, #0x0
  a4:	lsl	x13, x8, x9
  a8:	sub	x14, x9, #0x40
  ac:	lsl	x9, x8, x9
  b0:	lsr	x8, x8, x12
  b4:	csel	x8, xzr, x8, eq  // eq = none
  b8:	cmp	x14, #0x0
  bc:	csel	x9, xzr, x9, ge  // ge = tcont
  c0:	asr	x11, x10, #63
  c4:	umulh	x12, x9, x10
  c8:	csel	x8, x13, x8, ge  // ge = tcont
  cc:	madd	x11, x9, x11, x12
  d0:	madd	x1, x8, x10, x11
  d4:	mul	x0, x9, x10
  d8:	ldp	x29, x30, [sp], #16
  dc:	ret

00000000000000e0 <toRep>:
  e0:	fmov	w0, s0
  e4:	ret

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	fcmp	d0, #0.0
   4:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fmov	d1, x8
  10:	fmul	d1, d0, d1
  14:	fcvtzu	w8, d1
  18:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  1c:	ucvtf	d1, w8
  20:	fmov	d2, x9
  24:	fmul	d1, d1, d2
  28:	fadd	d0, d0, d1
  2c:	fcvtzu	w0, d0
  30:	bfi	x0, x8, #32, #32
  34:	ret
  38:	mov	x0, xzr
  3c:	ret

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	68 <toRep>
  20:	mov	x8, x0
  24:	mov	w0, wzr
  28:	tbnz	x8, #63, 44 <__fixuint+0x30>
  2c:	ubfx	x9, x8, #52, #11
  30:	subs	w10, w9, #0x3ff
  34:	b.cc	44 <__fixuint+0x30>  // b.lo, b.ul, b.last
  38:	cmp	w10, #0x1f
  3c:	b.ls	4c <__fixuint+0x38>  // b.plast
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ldp	x29, x30, [sp], #16
  48:	ret
  4c:	mov	x10, #0x10000000000000      	// #4503599627370496
  50:	mov	w11, #0x433                 	// #1075
  54:	bfxil	x10, x8, #0, #52
  58:	sub	w8, w11, w9
  5c:	lsr	x0, x10, x8
  60:	ldp	x29, x30, [sp], #16
  64:	ret

0000000000000068 <toRep>:
  68:	fmov	x0, d0
  6c:	ret

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	bc <toRep>
  20:	mov	x8, x0
  24:	mov	x0, xzr
  28:	tbnz	x8, #63, 54 <__fixuint+0x40>
  2c:	ubfx	x10, x8, #52, #11
  30:	subs	w9, w10, #0x3ff
  34:	mov	x1, x0
  38:	b.cc	b4 <__fixuint+0xa0>  // b.lo, b.ul, b.last
  3c:	cmp	w9, #0x7f
  40:	b.ls	60 <__fixuint+0x4c>  // b.plast
  44:	mov	x0, #0xffffffffffffffff    	// #-1
  48:	mov	x1, #0xffffffffffffffff    	// #-1
  4c:	ldp	x29, x30, [sp], #16
  50:	ret
  54:	mov	x1, x0
  58:	ldp	x29, x30, [sp], #16
  5c:	ret
  60:	mov	x9, #0x10000000000000      	// #4503599627370496
  64:	cmp	w10, #0x432
  68:	bfxil	x9, x8, #0, #52
  6c:	b.hi	88 <__fixuint+0x74>  // b.pmore
  70:	mov	w8, #0x433                 	// #1075
  74:	sub	w8, w8, w10
  78:	mov	x1, xzr
  7c:	lsr	x0, x9, x8
  80:	ldp	x29, x30, [sp], #16
  84:	ret
  88:	sub	w8, w10, #0x433
  8c:	neg	x10, x8
  90:	cmp	x8, #0x0
  94:	lsl	x11, x9, x8
  98:	sub	x12, x8, #0x40
  9c:	lsl	x8, x9, x8
  a0:	lsr	x9, x9, x10
  a4:	csel	x9, xzr, x9, eq  // eq = none
  a8:	cmp	x12, #0x0
  ac:	csel	x1, x11, x9, ge  // ge = tcont
  b0:	csel	x0, xzr, x8, ge  // ge = tcont
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret

00000000000000bc <toRep>:
  bc:	fmov	x0, d0
  c0:	ret

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	fcmp	s0, #0.0
   4:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fcvt	d0, s0
  10:	fmov	d1, x8
  14:	fmul	d1, d0, d1
  18:	fcvtzu	w8, d1
  1c:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  20:	ucvtf	d1, w8
  24:	fmov	d2, x9
  28:	fmul	d1, d1, d2
  2c:	fadd	d0, d0, d1
  30:	fcvtzu	w0, d0
  34:	bfi	x0, x8, #32, #32
  38:	ret
  3c:	mov	x0, xzr
  40:	ret

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	80 <toRep>
  20:	mov	w8, w0
  24:	mov	w0, wzr
  28:	tbnz	w8, #31, 78 <__fixuint+0x64>
  2c:	ubfx	w9, w8, #23, #8
  30:	subs	w10, w9, #0x7f
  34:	b.cc	78 <__fixuint+0x64>  // b.lo, b.ul, b.last
  38:	cmp	w10, #0x1f
  3c:	b.ls	4c <__fixuint+0x38>  // b.plast
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ldp	x29, x30, [sp], #16
  48:	ret
  4c:	mov	w10, #0x800000              	// #8388608
  50:	cmp	w9, #0x95
  54:	bfxil	w10, w8, #0, #23
  58:	b.hi	70 <__fixuint+0x5c>  // b.pmore
  5c:	mov	w8, #0x96                  	// #150
  60:	sub	w8, w8, w9
  64:	lsr	w0, w10, w8
  68:	ldp	x29, x30, [sp], #16
  6c:	ret
  70:	sub	w8, w9, #0x96
  74:	lsl	w0, w10, w8
  78:	ldp	x29, x30, [sp], #16
  7c:	ret

0000000000000080 <toRep>:
  80:	fmov	w0, s0
  84:	ret

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__fixuint>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__fixuint>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	bc <toRep>
  20:	mov	w8, w0
  24:	mov	x0, xzr
  28:	tbnz	w8, #31, 54 <__fixuint+0x40>
  2c:	ubfx	w10, w8, #23, #8
  30:	subs	w9, w10, #0x7f
  34:	mov	x1, x0
  38:	b.cc	b4 <__fixuint+0xa0>  // b.lo, b.ul, b.last
  3c:	cmp	w9, #0x7f
  40:	b.ls	60 <__fixuint+0x4c>  // b.plast
  44:	mov	x0, #0xffffffffffffffff    	// #-1
  48:	mov	x1, #0xffffffffffffffff    	// #-1
  4c:	ldp	x29, x30, [sp], #16
  50:	ret
  54:	mov	x1, x0
  58:	ldp	x29, x30, [sp], #16
  5c:	ret
  60:	mov	w9, #0x800000              	// #8388608
  64:	cmp	w10, #0x95
  68:	bfxil	w9, w8, #0, #23
  6c:	b.hi	88 <__fixuint+0x74>  // b.pmore
  70:	mov	w8, #0x96                  	// #150
  74:	sub	w8, w8, w10
  78:	mov	x1, xzr
  7c:	lsr	w0, w9, w8
  80:	ldp	x29, x30, [sp], #16
  84:	ret
  88:	sub	w8, w10, #0x96
  8c:	neg	x10, x8
  90:	cmp	x8, #0x0
  94:	lsl	x11, x9, x8
  98:	sub	x12, x8, #0x40
  9c:	lsl	x8, x9, x8
  a0:	lsr	x9, x9, x10
  a4:	csel	x9, xzr, x9, eq  // eq = none
  a8:	cmp	x12, #0x0
  ac:	csel	x1, x11, x9, ge  // ge = tcont
  b0:	csel	x0, xzr, x8, ge  // ge = tcont
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret

00000000000000bc <toRep>:
  bc:	fmov	w0, s0
  c0:	ret

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	lsr	x8, x0, #32
   4:	mov	x9, #0x41f0000000000000    	// #4751297606875873280
   8:	mov	x10, #0x4330000000000000    	// #4841369599423283200
   c:	mov	x11, #0xc330000000000000    	// #-4382002437431492608
  10:	scvtf	d0, w8
  14:	fmov	d1, x9
  18:	fmul	d0, d0, d1
  1c:	bfxil	x10, x0, #0, #32
  20:	fmov	d1, x11
  24:	fadd	d0, d0, d1
  28:	fmov	d1, x10
  2c:	fadd	d0, d0, d1
  30:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	cbz	x0, 44 <__floatdisf+0x44>
   4:	asr	x8, x0, #63
   8:	eor	x9, x8, x0
   c:	sub	x9, x9, x8
  10:	clz	x11, x9
  14:	mov	w8, #0x3f                  	// #63
  18:	cmp	w11, #0x27
  1c:	sub	w8, w8, w11
  20:	b.hi	4c <__floatdisf+0x4c>  // b.pmore
  24:	mov	w10, #0x40                  	// #64
  28:	sub	w10, w10, w11
  2c:	cmp	w10, #0x1a
  30:	b.eq	74 <__floatdisf+0x74>  // b.none
  34:	cmp	w10, #0x19
  38:	b.ne	58 <__floatdisf+0x58>  // b.any
  3c:	lsl	x9, x9, #1
  40:	b	74 <__floatdisf+0x74>
  44:	fmov	s0, wzr
  48:	ret
  4c:	sub	w10, w11, #0x28
  50:	lsl	x9, x9, x10
  54:	b	94 <__floatdisf+0x94>
  58:	mov	w12, #0x26                  	// #38
  5c:	lsl	x13, x9, x11
  60:	sub	w11, w12, w11
  64:	tst	x13, #0x3fffffffff
  68:	lsr	x9, x9, x11
  6c:	cset	w11, ne  // ne = any
  70:	orr	x9, x9, x11
  74:	ubfx	x11, x9, #2, #1
  78:	orr	x9, x11, x9
  7c:	add	x9, x9, #0x1
  80:	mov	w12, #0x2                   	// #2
  84:	tst	x9, #0x4000000
  88:	cinc	x11, x12, ne  // ne = any
  8c:	asr	x9, x9, x11
  90:	csel	w8, w8, w10, eq  // eq = none
  94:	lsr	x10, x0, #32
  98:	mov	w11, #0x3f800000            	// #1065353216
  9c:	and	w10, w10, #0x80000000
  a0:	add	w8, w11, w8, lsl #23
  a4:	bfxil	w10, w9, #0, #23
  a8:	orr	w8, w10, w8
  ac:	fmov	s0, w8
  b0:	ret

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	cbz	w0, 34 <__floatsidf+0x34>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	clz	w11, w10
  10:	add	w12, w11, #0x15
  14:	mov	w9, #0x41e                 	// #1054
  18:	lsl	x10, x10, x12
  1c:	eor	x10, x10, #0x10000000000000
  20:	sub	w9, w9, w11
  24:	and	w8, w0, #0x80000000
  28:	add	x9, x10, x9, lsl #52
  2c:	orr	x0, x9, x8, lsl #32
  30:	b	38 <__floatsidf+0x38>
  34:	mov	x0, xzr
  38:	stp	x29, x30, [sp, #-16]!
  3c:	mov	x29, sp
  40:	bl	4c <fromRep>
  44:	ldp	x29, x30, [sp], #16
  48:	ret

000000000000004c <fromRep>:
  4c:	fmov	d0, x0
  50:	ret

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	cbz	w0, 6c <__floatsisf+0x6c>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	and	w8, w0, #0x80000000
  10:	lsr	w11, w10, #24
  14:	clz	w9, w10
  18:	cbnz	w11, 2c <__floatsisf+0x2c>
  1c:	sub	w11, w9, #0x8
  20:	lsl	w10, w10, w11
  24:	eor	w10, w10, #0x800000
  28:	b	5c <__floatsisf+0x5c>
  2c:	mov	w11, #0x8                   	// #8
  30:	add	w12, w9, #0x18
  34:	sub	w11, w11, w9
  38:	mov	w13, #0x80000000            	// #-2147483648
  3c:	lsl	w12, w10, w12
  40:	lsr	w10, w10, w11
  44:	eor	w10, w10, #0x800000
  48:	cmp	w12, w13
  4c:	cinc	w10, w10, hi  // hi = pmore
  50:	and	w11, w10, #0x1
  54:	csel	w11, w11, wzr, eq  // eq = none
  58:	add	w10, w11, w10
  5c:	sub	w9, w10, w9, lsl #23
  60:	mov	w10, #0x4f000000            	// #1325400064
  64:	add	w9, w9, w10
  68:	orr	w0, w9, w8
  6c:	stp	x29, x30, [sp, #-16]!
  70:	mov	x29, sp
  74:	bl	80 <fromRep>
  78:	ldp	x29, x30, [sp], #16
  7c:	ret

0000000000000080 <fromRep>:
  80:	fmov	s0, w0
  84:	ret

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattidf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	asr	x8, x1, #63
  1c:	eor	x10, x8, x0
  20:	eor	x9, x8, x1
  24:	subs	x20, x10, x8
  28:	sbcs	x21, x9, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x36
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattidf+0x78>  // b.tstop
  54:	cmp	w9, #0x37
  58:	b.eq	10c <__floattidf+0x10c>  // b.none
  5c:	cmp	w9, #0x36
  60:	b.ne	90 <__floattidf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattidf+0x10c>
  70:	fmov	d0, xzr
  74:	ret
  78:	sub	w9, w0, #0x4b
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	124 <__floattidf+0x124>
  90:	mov	w10, #0x49                  	// #73
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x37
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	x10, #55, 12c <__floattidf+0x12c>
 120:	extr	x10, x11, x10, #2
 124:	lsr	x11, x10, #32
 128:	b	138 <__floattidf+0x138>
 12c:	extr	x10, x11, x10, #3
 130:	lsr	x11, x10, #32
 134:	mov	w8, w9
 138:	lsr	x9, x19, #32
 13c:	mov	w12, #0x3ff00000            	// #1072693248
 140:	and	w9, w9, #0x80000000
 144:	add	w8, w12, w8, lsl #20
 148:	bfxil	w9, w11, #0, #20
 14c:	ldp	x20, x19, [sp, #32]
 150:	ldr	x21, [sp, #16]
 154:	orr	w8, w9, w8
 158:	bfi	x10, x8, #32, #32
 15c:	fmov	d0, x10
 160:	ldp	x29, x30, [sp], #48
 164:	ret

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattisf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	asr	x8, x1, #63
  1c:	eor	x10, x8, x0
  20:	eor	x9, x8, x1
  24:	subs	x20, x10, x8
  28:	sbcs	x21, x9, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x19
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattisf+0x78>  // b.tstop
  54:	cmp	w9, #0x1a
  58:	b.eq	10c <__floattisf+0x10c>  // b.none
  5c:	cmp	w9, #0x19
  60:	b.ne	90 <__floattisf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattisf+0x10c>
  70:	fmov	s0, wzr
  74:	ret
  78:	sub	w9, w0, #0x68
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	134 <__floattisf+0x134>
  90:	mov	w10, #0x66                  	// #102
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x1a
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	w10, #26, 128 <__floattisf+0x128>
 120:	lsr	x10, x10, #2
 124:	b	134 <__floattisf+0x134>
 128:	extr	x8, x11, x10, #2
 12c:	lsr	x10, x8, #1
 130:	mov	w8, w9
 134:	lsr	x9, x19, #32
 138:	mov	w11, #0x3f800000            	// #1065353216
 13c:	and	w9, w9, #0x80000000
 140:	ldp	x20, x19, [sp, #32]
 144:	ldr	x21, [sp, #16]
 148:	add	w8, w11, w8, lsl #23
 14c:	bfxil	w9, w10, #0, #23
 150:	orr	w8, w9, w8
 154:	fmov	s0, w8
 158:	ldp	x29, x30, [sp], #48
 15c:	ret

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	mov	x8, #0x4530000000000000    	// #4985484787499139072
   4:	mov	x10, #0x100000              	// #1048576
   8:	mov	x9, #0x4330000000000000    	// #4841369599423283200
   c:	movk	x10, #0xc530, lsl #48
  10:	bfxil	x8, x0, #32, #32
  14:	bfxil	x9, x0, #0, #32
  18:	fmov	d0, x10
  1c:	fmov	d1, x8
  20:	fadd	d0, d1, d0
  24:	fmov	d1, x9
  28:	fadd	d0, d0, d1
  2c:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	cbz	x0, 34 <__floatundisf+0x34>
   4:	clz	x10, x0
   8:	cmp	w10, #0x27
   c:	eor	w8, w10, #0x3f
  10:	b.hi	3c <__floatundisf+0x3c>  // b.pmore
  14:	mov	w9, #0x40                  	// #64
  18:	sub	w9, w9, w10
  1c:	cmp	w9, #0x1a
  20:	b.eq	64 <__floatundisf+0x64>  // b.none
  24:	cmp	w9, #0x19
  28:	b.ne	48 <__floatundisf+0x48>  // b.any
  2c:	lsl	x0, x0, #1
  30:	b	64 <__floatundisf+0x64>
  34:	fmov	s0, wzr
  38:	ret
  3c:	sub	w9, w10, #0x28
  40:	lsl	x10, x0, x9
  44:	b	84 <__floatundisf+0x84>
  48:	mov	w11, #0x26                  	// #38
  4c:	lsl	x12, x0, x10
  50:	sub	w10, w11, w10
  54:	tst	x12, #0x3fffffffff
  58:	lsr	x10, x0, x10
  5c:	cset	w11, ne  // ne = any
  60:	orr	x0, x10, x11
  64:	ubfx	x10, x0, #2, #1
  68:	orr	x10, x10, x0
  6c:	add	x10, x10, #0x1
  70:	mov	w11, #0x2                   	// #2
  74:	tst	x10, #0x4000000
  78:	cinc	x11, x11, ne  // ne = any
  7c:	lsr	x10, x10, x11
  80:	csel	w8, w8, w9, eq  // eq = none
  84:	bfi	w10, w8, #23, #9
  88:	mov	w8, #0x3f800000            	// #1065353216
  8c:	add	w8, w10, w8
  90:	fmov	s0, w8
  94:	ret

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	cbz	w0, 2c <__floatunsidf+0x2c>
   4:	clz	w8, w0
   8:	mov	w9, #0x34                  	// #52
   c:	eor	w8, w8, #0x1f
  10:	mov	w10, w0
  14:	sub	w9, w9, w8
  18:	lsl	x9, x10, x9
  1c:	eor	x9, x9, #0x10000000000000
  20:	add	w8, w8, #0x3ff
  24:	add	x0, x9, x8, lsl #52
  28:	b	30 <__floatunsidf+0x30>
  2c:	mov	x0, xzr
  30:	stp	x29, x30, [sp, #-16]!
  34:	mov	x29, sp
  38:	bl	44 <fromRep>
  3c:	ldp	x29, x30, [sp], #16
  40:	ret

0000000000000044 <fromRep>:
  44:	fmov	d0, x0
  48:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	cbz	w0, 60 <__floatunsisf+0x60>
   4:	clz	w8, w0
   8:	eor	w8, w8, #0x1f
   c:	subs	w9, w8, #0x17
  10:	b.hi	28 <__floatunsisf+0x28>  // b.pmore
  14:	mov	w9, #0x17                  	// #23
  18:	sub	w9, w9, w8
  1c:	lsl	w9, w0, w9
  20:	eor	w9, w9, #0x800000
  24:	b	54 <__floatunsisf+0x54>
  28:	mov	w10, #0x37                  	// #55
  2c:	sub	w10, w10, w8
  30:	lsr	w9, w0, w9
  34:	lsl	w10, w0, w10
  38:	mov	w11, #0x80000000            	// #-2147483648
  3c:	eor	w9, w9, #0x800000
  40:	cmp	w10, w11
  44:	cinc	w9, w9, hi  // hi = pmore
  48:	and	w10, w9, #0x1
  4c:	csel	w10, w10, wzr, eq  // eq = none
  50:	add	w9, w10, w9
  54:	add	w8, w9, w8, lsl #23
  58:	mov	w9, #0x3f800000            	// #1065353216
  5c:	add	w0, w8, w9
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	74 <fromRep>
  6c:	ldp	x29, x30, [sp], #16
  70:	ret

0000000000000074 <fromRep>:
  74:	fmov	s0, w0
  78:	ret

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 54 <__floatuntidf+0x54>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x36
  30:	sub	w8, w8, w0
  34:	b.lt	5c <__floatuntidf+0x5c>  // b.tstop
  38:	cmp	w9, #0x37
  3c:	b.eq	f4 <__floatuntidf+0xf4>  // b.none
  40:	cmp	w9, #0x36
  44:	b.ne	78 <__floatuntidf+0x78>  // b.any
  48:	extr	x20, x20, x19, #63
  4c:	lsl	x19, x19, #1
  50:	b	f4 <__floatuntidf+0xf4>
  54:	fmov	d0, xzr
  58:	ret
  5c:	sub	w9, w0, #0x4b
  60:	lsl	x10, x19, x9
  64:	sub	x9, x9, #0x40
  68:	cmp	x9, #0x0
  6c:	csel	x10, xzr, x10, ge  // ge = tcont
  70:	lsr	x11, x10, #32
  74:	b	128 <__floatuntidf+0x128>
  78:	mov	w10, #0x49                  	// #73
  7c:	sub	w10, w10, w0
  80:	neg	x13, x10
  84:	cmp	x10, #0x0
  88:	sub	x14, x10, #0x40
  8c:	lsl	x13, x20, x13
  90:	add	w11, w0, #0x37
  94:	csel	x13, xzr, x13, eq  // eq = none
  98:	cmp	x14, #0x0
  9c:	lsr	x14, x19, x10
  a0:	neg	x12, x11
  a4:	orr	x13, x14, x13
  a8:	lsr	x14, x20, x10
  ac:	lsr	x10, x20, x10
  b0:	lsl	x15, x20, x11
  b4:	csel	x10, x10, x13, ge  // ge = tcont
  b8:	lsr	x12, x19, x12
  bc:	csel	x20, xzr, x14, ge  // ge = tcont
  c0:	cmp	x11, #0x0
  c4:	lsl	x13, x19, x11
  c8:	lsl	x16, x19, x11
  cc:	sub	x11, x11, #0x40
  d0:	csel	x12, xzr, x12, eq  // eq = none
  d4:	cmp	x11, #0x0
  d8:	orr	x11, x12, x15
  dc:	csel	x11, x13, x11, ge  // ge = tcont
  e0:	csel	x12, xzr, x16, ge  // ge = tcont
  e4:	orr	x11, x12, x11
  e8:	cmp	x11, #0x0
  ec:	cset	w11, ne  // ne = any
  f0:	orr	x19, x10, x11
  f4:	ubfx	x10, x19, #2, #1
  f8:	orr	x10, x10, x19
  fc:	adds	x11, x10, #0x1
 100:	adcs	x12, x20, xzr
 104:	tbnz	x11, #55, 118 <__floatuntidf+0x118>
 108:	extr	x10, x12, x11, #2
 10c:	lsr	x11, x11, #34
 110:	bfi	w11, w12, #30, #2
 114:	b	128 <__floatuntidf+0x128>
 118:	extr	x10, x12, x11, #3
 11c:	lsr	x11, x11, #35
 120:	bfi	w11, w12, #29, #3
 124:	mov	w8, w9
 128:	bfi	w11, w8, #20, #12
 12c:	mov	w8, #0x3ff00000            	// #1072693248
 130:	ldp	x20, x19, [sp, #16]
 134:	add	w8, w11, w8
 138:	bfi	x10, x8, #32, #32
 13c:	fmov	d0, x10
 140:	ldp	x29, x30, [sp], #32
 144:	ret

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 50 <__floatuntisf+0x50>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x19
  30:	sub	w8, w8, w0
  34:	b.lt	58 <__floatuntisf+0x58>  // b.tstop
  38:	cmp	w9, #0x1a
  3c:	b.eq	e4 <__floatuntisf+0xe4>  // b.none
  40:	cmp	w9, #0x19
  44:	b.ne	70 <__floatuntisf+0x70>  // b.any
  48:	lsl	x19, x19, #1
  4c:	b	e4 <__floatuntisf+0xe4>
  50:	fmov	s0, wzr
  54:	ret
  58:	sub	w9, w0, #0x68
  5c:	lsl	x10, x19, x9
  60:	sub	x9, x9, #0x40
  64:	cmp	x9, #0x0
  68:	csel	x10, xzr, x10, ge  // ge = tcont
  6c:	b	104 <__floatuntisf+0x104>
  70:	mov	w10, #0x66                  	// #102
  74:	sub	w10, w10, w0
  78:	neg	x12, x10
  7c:	cmp	x10, #0x0
  80:	sub	x13, x10, #0x40
  84:	lsl	x12, x20, x12
  88:	add	w11, w0, #0x1a
  8c:	csel	x12, xzr, x12, eq  // eq = none
  90:	cmp	x13, #0x0
  94:	lsr	x13, x19, x10
  98:	orr	x12, x13, x12
  9c:	neg	x13, x11
  a0:	lsr	x10, x20, x10
  a4:	csel	x10, x10, x12, ge  // ge = tcont
  a8:	lsr	x13, x19, x13
  ac:	cmp	x11, #0x0
  b0:	lsl	x14, x20, x11
  b4:	lsl	x12, x19, x11
  b8:	lsl	x15, x19, x11
  bc:	sub	x11, x11, #0x40
  c0:	csel	x13, xzr, x13, eq  // eq = none
  c4:	cmp	x11, #0x0
  c8:	orr	x11, x13, x14
  cc:	csel	x11, x12, x11, ge  // ge = tcont
  d0:	csel	x12, xzr, x15, ge  // ge = tcont
  d4:	orr	x11, x12, x11
  d8:	cmp	x11, #0x0
  dc:	cset	w11, ne  // ne = any
  e0:	orr	x19, x10, x11
  e4:	ubfx	x10, x19, #2, #1
  e8:	orr	x10, x10, x19
  ec:	adds	x10, x10, #0x1
  f0:	tbnz	w10, #26, fc <__floatuntisf+0xfc>
  f4:	lsr	x10, x10, #2
  f8:	b	104 <__floatuntisf+0x104>
  fc:	lsr	x10, x10, #3
 100:	mov	w8, w9
 104:	ldp	x20, x19, [sp, #16]
 108:	bfi	w10, w8, #23, #9
 10c:	mov	w8, #0x3f800000            	// #1065353216
 110:	add	w8, w10, w8
 114:	fmov	s0, w8
 118:	ldp	x29, x30, [sp], #32
 11c:	ret

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	lsr	x8, x0, #32
   4:	tbnz	w1, #5, 28 <__lshrdi3+0x28>
   8:	cbz	w1, 38 <__lshrdi3+0x38>
   c:	neg	w10, w1
  10:	lsr	w9, w8, w1
  14:	lsr	w11, w0, w1
  18:	lsl	w8, w8, w10
  1c:	orr	w8, w8, w11
  20:	lsl	x9, x9, #32
  24:	b	30 <__lshrdi3+0x30>
  28:	mov	x9, xzr
  2c:	lsr	w8, w8, w1
  30:	mov	w8, w8
  34:	orr	x0, x9, x8
  38:	ret

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbnz	w2, #6, 24 <__lshrti3+0x24>
   4:	cbz	w2, 34 <__lshrti3+0x34>
   8:	neg	w9, w2
   c:	lsl	x9, x1, x9
  10:	lsr	x10, x0, x2
  14:	mov	x8, xzr
  18:	lsr	x1, x1, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__lshrti3+0x30>
  24:	mov	x8, xzr
  28:	lsr	x9, x1, x2
  2c:	mov	x1, xzr
  30:	orr	x0, x8, x9
  34:	ret

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x19, x0, #63
  10:	cmp	x1, #0x0
  14:	eor	x8, x19, x0
  18:	cneg	x1, x1, mi  // mi = first
  1c:	sub	x0, x8, x19
  20:	add	x2, x29, #0x18
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x8, [x29, #24]
  2c:	eor	x8, x8, x19
  30:	sub	x0, x8, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #32
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	str	x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	negs	x8, x2
  14:	asr	x19, x1, #63
  18:	ngcs	x9, x3
  1c:	eor	x11, x19, x0
  20:	cmp	x3, #0x0
  24:	eor	x10, x19, x1
  28:	csel	x2, x8, x2, lt  // lt = tstop
  2c:	csel	x3, x9, x3, lt  // lt = tstop
  30:	subs	x0, x11, x19
  34:	sbcs	x1, x10, x19
  38:	mov	x4, sp
  3c:	bl	0 <__udivmodti4>
  40:	ldp	x9, x8, [sp]
  44:	ldp	x29, x30, [sp, #16]
  48:	eor	x9, x9, x19
  4c:	eor	x8, x8, x19
  50:	subs	x0, x9, x19
  54:	sbcs	x1, x8, x19
  58:	ldr	x19, [sp, #32]
  5c:	add	sp, sp, #0x30
  60:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d7, d0, d2
   4:	fmul	d6, d1, d3
   8:	mov	v5.16b, v0.16b
   c:	fmul	d16, d0, d3
  10:	fmul	d17, d1, d2
  14:	fsub	d0, d7, d6
  18:	mov	v4.16b, v1.16b
  1c:	fcmp	d0, d0
  20:	fadd	d1, d17, d16
  24:	b.vc	110 <__muldc3+0x110>
  28:	fcmp	d1, d1
  2c:	b.vc	110 <__muldc3+0x110>
  30:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  34:	fabs	d20, d5
  38:	fmov	d18, x8
  3c:	fabs	d19, d4
  40:	fcmp	d20, d18
  44:	fmov	d18, #1.000000000000000000e+00
  48:	b.eq	64 <__muldc3+0x64>  // b.none
  4c:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  50:	fmov	d21, x8
  54:	fcmp	d19, d21
  58:	b.eq	64 <__muldc3+0x64>  // b.none
  5c:	mov	w8, wzr
  60:	b	b8 <__muldc3+0xb8>
  64:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  68:	fmov	d23, x8
  6c:	fmov	d21, xzr
  70:	movi	v22.2d, #0x0
  74:	fcmp	d20, d23
  78:	fcsel	d20, d18, d21, eq  // eq = none
  7c:	fcmp	d19, d23
  80:	fmov	d19, xzr
  84:	fneg	v22.2d, v22.2d
  88:	bit	v19.16b, v2.16b, v22.16b
  8c:	bit	v20.16b, v5.16b, v22.16b
  90:	fcsel	d5, d18, d21, eq  // eq = none
  94:	fcmp	d2, d2
  98:	bit	v21.16b, v3.16b, v22.16b
  9c:	bit	v5.16b, v4.16b, v22.16b
  a0:	fcsel	d2, d19, d2, vs
  a4:	fcmp	d3, d3
  a8:	fcsel	d3, d21, d3, vs
  ac:	mov	w8, #0x1                   	// #1
  b0:	mov	v4.16b, v5.16b
  b4:	mov	v5.16b, v20.16b
  b8:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  bc:	fabs	d19, d3
  c0:	fmov	d20, x9
  c4:	fcmp	d19, d20
  c8:	fabs	d20, d2
  cc:	b.eq	114 <__muldc3+0x114>  // b.none
  d0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  d4:	fmov	d21, x9
  d8:	fcmp	d20, d21
  dc:	b.eq	114 <__muldc3+0x114>  // b.none
  e0:	cbz	w8, 16c <__muldc3+0x16c>
  e4:	cbz	w8, 110 <__muldc3+0x110>
  e8:	fmul	d0, d2, d5
  ec:	fmul	d1, d3, d4
  f0:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  f4:	fmul	d3, d3, d5
  f8:	fmul	d2, d2, d4
  fc:	fsub	d0, d0, d1
 100:	fmov	d1, x8
 104:	fadd	d2, d2, d3
 108:	fmul	d0, d0, d1
 10c:	fmul	d1, d2, d1
 110:	ret
 114:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 118:	fmov	d23, x8
 11c:	fmov	d21, xzr
 120:	movi	v22.2d, #0x0
 124:	fcmp	d20, d23
 128:	fcsel	d20, d18, d21, eq  // eq = none
 12c:	fcmp	d19, d23
 130:	fmov	d19, xzr
 134:	fneg	v22.2d, v22.2d
 138:	bit	v19.16b, v5.16b, v22.16b
 13c:	bit	v20.16b, v2.16b, v22.16b
 140:	fcsel	d2, d18, d21, eq  // eq = none
 144:	fcmp	d5, d5
 148:	bit	v21.16b, v4.16b, v22.16b
 14c:	bit	v2.16b, v3.16b, v22.16b
 150:	fcsel	d5, d19, d5, vs
 154:	fcmp	d4, d4
 158:	fcsel	d4, d21, d4, vs
 15c:	mov	w8, #0x1                   	// #1
 160:	mov	v3.16b, v2.16b
 164:	mov	v2.16b, v20.16b
 168:	cbnz	w8, e4 <__muldc3+0xe4>
 16c:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 170:	fabs	d17, d17
 174:	fmov	d18, x8
 178:	fcmp	d17, d18
 17c:	b.eq	1c8 <__muldc3+0x1c8>  // b.none
 180:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 184:	fabs	d16, d16
 188:	fmov	d17, x8
 18c:	fcmp	d16, d17
 190:	b.eq	1c8 <__muldc3+0x1c8>  // b.none
 194:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 198:	fabs	d7, d7
 19c:	fmov	d16, x8
 1a0:	fcmp	d7, d16
 1a4:	b.eq	1c8 <__muldc3+0x1c8>  // b.none
 1a8:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1ac:	fabs	d6, d6
 1b0:	fmov	d7, x8
 1b4:	fcmp	d6, d7
 1b8:	b.eq	1c8 <__muldc3+0x1c8>  // b.none
 1bc:	mov	w8, wzr
 1c0:	cbnz	w8, e8 <__muldc3+0xe8>
 1c4:	b	110 <__muldc3+0x110>
 1c8:	movi	v7.2d, #0x0
 1cc:	fmov	d16, xzr
 1d0:	fneg	v7.2d, v7.2d
 1d4:	fmov	d17, xzr
 1d8:	bit	v16.16b, v5.16b, v7.16b
 1dc:	fcmp	d5, d5
 1e0:	fmov	d18, xzr
 1e4:	bit	v17.16b, v4.16b, v7.16b
 1e8:	fcsel	d5, d16, d5, vs
 1ec:	fcmp	d4, d4
 1f0:	fmov	d6, xzr
 1f4:	bit	v18.16b, v2.16b, v7.16b
 1f8:	fcsel	d4, d17, d4, vs
 1fc:	fcmp	d2, d2
 200:	bit	v6.16b, v3.16b, v7.16b
 204:	fcsel	d2, d18, d2, vs
 208:	fcmp	d3, d3
 20c:	fcsel	d3, d6, d3, vs
 210:	mov	w8, #0x1                   	// #1
 214:	cbnz	w8, e8 <__muldc3+0xe8>
 218:	b	110 <__muldc3+0x110>

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__mulXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__mulXf3__>:
  14:	sub	sp, sp, #0x60
  18:	str	d8, [sp, #32]
  1c:	stp	x29, x30, [sp, #40]
  20:	str	x23, [sp, #56]
  24:	stp	x22, x21, [sp, #64]
  28:	stp	x20, x19, [sp, #80]
  2c:	add	x29, sp, #0x20
  30:	mov	v8.16b, v1.16b
  34:	bl	254 <toRep>
  38:	mov	v0.16b, v8.16b
  3c:	mov	x20, x0
  40:	ubfx	x21, x0, #52, #11
  44:	bl	254 <toRep>
  48:	eor	x8, x0, x20
  4c:	sub	w11, w21, #0x1
  50:	ubfx	x22, x0, #52, #11
  54:	and	x9, x20, #0xfffffffffffff
  58:	and	x10, x0, #0xfffffffffffff
  5c:	and	x19, x8, #0x8000000000000000
  60:	cmp	w11, #0x7fd
  64:	stur	x9, [x29, #-8]
  68:	str	x10, [sp, #16]
  6c:	b.hi	84 <__mulXf3__+0x70>  // b.pmore
  70:	sub	w8, w22, #0x1
  74:	cmp	w8, #0x7fe
  78:	b.cs	84 <__mulXf3__+0x70>  // b.hs, b.nlast
  7c:	mov	w20, wzr
  80:	b	118 <__mulXf3__+0x104>
  84:	mov	x9, #0x1                   	// #1
  88:	and	x8, x20, #0x7fffffffffffffff
  8c:	movk	x9, #0x7ff0, lsl #48
  90:	cmp	x8, x9
  94:	b.cc	a0 <__mulXf3__+0x8c>  // b.lo, b.ul, b.last
  98:	orr	x0, x20, #0x8000000000000
  9c:	b	104 <__mulXf3__+0xf0>
  a0:	and	x23, x0, #0x7fffffffffffffff
  a4:	cmp	x23, x9
  a8:	b.cc	b4 <__mulXf3__+0xa0>  // b.lo, b.ul, b.last
  ac:	orr	x0, x0, #0x8000000000000
  b0:	b	104 <__mulXf3__+0xf0>
  b4:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  b8:	cmp	x8, x9
  bc:	b.ne	cc <__mulXf3__+0xb8>  // b.any
  c0:	cbz	x23, d8 <__mulXf3__+0xc4>
  c4:	orr	x0, x19, #0x7ff0000000000000
  c8:	b	104 <__mulXf3__+0xf0>
  cc:	cmp	x23, x9
  d0:	b.ne	e0 <__mulXf3__+0xcc>  // b.any
  d4:	cbnz	x8, c4 <__mulXf3__+0xb0>
  d8:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
  dc:	b	104 <__mulXf3__+0xf0>
  e0:	cbz	x8, 100 <__mulXf3__+0xec>
  e4:	cbz	x23, 100 <__mulXf3__+0xec>
  e8:	lsr	x8, x8, #52
  ec:	cbnz	x8, 230 <__mulXf3__+0x21c>
  f0:	sub	x0, x29, #0x8
  f4:	bl	264 <normalize>
  f8:	mov	w20, w0
  fc:	b	234 <__mulXf3__+0x220>
 100:	mov	x0, x19
 104:	bl	25c <fromRep>
 108:	mov	v8.16b, v0.16b
 10c:	mov	w20, wzr
 110:	mov	w8, wzr
 114:	cbz	w8, 1d8 <__mulXf3__+0x1c4>
 118:	ldur	x8, [x29, #-8]
 11c:	ldr	x9, [sp, #16]
 120:	add	x2, sp, #0x8
 124:	mov	x3, sp
 128:	orr	x0, x8, #0x10000000000000
 12c:	orr	x8, x9, #0x10000000000000
 130:	lsl	x1, x8, #11
 134:	stur	x0, [x29, #-8]
 138:	str	x8, [sp, #16]
 13c:	bl	2b4 <wideMultiply>
 140:	ldrb	w8, [sp, #14]
 144:	add	w9, w21, w22
 148:	add	w9, w9, w20
 14c:	sub	w20, w9, #0x3ff
 150:	tbnz	w8, #4, 1c0 <__mulXf3__+0x1ac>
 154:	add	x0, sp, #0x8
 158:	mov	x1, sp
 15c:	bl	2fc <wideLeftShift>
 160:	cmp	w20, #0x7ff
 164:	b.ge	1cc <__mulXf3__+0x1b8>  // b.tcont
 168:	cmp	w20, #0x0
 16c:	b.le	1f8 <__mulXf3__+0x1e4>
 170:	ldr	x8, [sp, #8]
 174:	bfi	x8, x20, #52, #12
 178:	str	x8, [sp, #8]
 17c:	ldp	x8, x9, [sp]
 180:	mov	x10, #0x8000000000000001    	// #-9223372036854775807
 184:	orr	x9, x9, x19
 188:	cmp	x8, x10
 18c:	str	x9, [sp, #8]
 190:	b.cc	19c <__mulXf3__+0x188>  // b.lo, b.ul, b.last
 194:	add	x9, x9, #0x1
 198:	str	x9, [sp, #8]
 19c:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
 1a0:	cmp	x8, x9
 1a4:	b.ne	1b8 <__mulXf3__+0x1a4>  // b.any
 1a8:	ldr	x8, [sp, #8]
 1ac:	and	x9, x8, #0x1
 1b0:	add	x8, x9, x8
 1b4:	str	x8, [sp, #8]
 1b8:	ldr	x0, [sp, #8]
 1bc:	b	1d0 <__mulXf3__+0x1bc>
 1c0:	add	w20, w20, #0x1
 1c4:	cmp	w20, #0x7ff
 1c8:	b.lt	168 <__mulXf3__+0x154>  // b.tstop
 1cc:	orr	x0, x19, #0x7ff0000000000000
 1d0:	bl	25c <fromRep>
 1d4:	mov	v8.16b, v0.16b
 1d8:	mov	v0.16b, v8.16b
 1dc:	ldp	x20, x19, [sp, #80]
 1e0:	ldp	x22, x21, [sp, #64]
 1e4:	ldr	x23, [sp, #56]
 1e8:	ldp	x29, x30, [sp, #40]
 1ec:	ldr	d8, [sp, #32]
 1f0:	add	sp, sp, #0x60
 1f4:	ret
 1f8:	mov	w8, #0x1                   	// #1
 1fc:	sub	w2, w8, w20
 200:	cmp	w2, #0x40
 204:	b.cc	218 <__mulXf3__+0x204>  // b.lo, b.ul, b.last
 208:	mov	x0, x19
 20c:	bl	25c <fromRep>
 210:	tbnz	wzr, #0, 17c <__mulXf3__+0x168>
 214:	b	1d4 <__mulXf3__+0x1c0>
 218:	add	x0, sp, #0x8
 21c:	mov	x1, sp
 220:	bl	31c <wideRightShiftWithSticky>
 224:	mov	w8, #0x1                   	// #1
 228:	tbnz	w8, #0, 17c <__mulXf3__+0x168>
 22c:	b	1d8 <__mulXf3__+0x1c4>
 230:	mov	w20, wzr
 234:	lsr	x8, x23, #52
 238:	cbnz	x8, 248 <__mulXf3__+0x234>
 23c:	add	x0, sp, #0x10
 240:	bl	264 <normalize>
 244:	add	w20, w0, w20
 248:	mov	w8, #0x1                   	// #1
 24c:	cbnz	w8, 118 <__mulXf3__+0x104>
 250:	b	1d8 <__mulXf3__+0x1c4>

0000000000000254 <toRep>:
 254:	fmov	x0, d0
 258:	ret

000000000000025c <fromRep>:
 25c:	fmov	d0, x0
 260:	ret

0000000000000264 <normalize>:
 264:	stp	x29, x30, [sp, #-48]!
 268:	str	x21, [sp, #16]
 26c:	stp	x20, x19, [sp, #32]
 270:	mov	x29, sp
 274:	ldr	x20, [x0]
 278:	mov	x19, x0
 27c:	mov	x0, x20
 280:	bl	3b8 <rep_clz>
 284:	mov	w21, w0
 288:	mov	x0, #0x10000000000000      	// #4503599627370496
 28c:	bl	3b8 <rep_clz>
 290:	sub	w8, w21, w0
 294:	lsl	x10, x20, x8
 298:	str	x10, [x19]
 29c:	ldp	x20, x19, [sp, #32]
 2a0:	ldr	x21, [sp, #16]
 2a4:	mov	w9, #0x1                   	// #1
 2a8:	sub	w0, w9, w8
 2ac:	ldp	x29, x30, [sp], #48
 2b0:	ret

00000000000002b4 <wideMultiply>:
 2b4:	and	x8, x0, #0xffffffff
 2b8:	and	x9, x1, #0xffffffff
 2bc:	lsr	x10, x1, #32
 2c0:	lsr	x11, x0, #32
 2c4:	mul	x12, x9, x8
 2c8:	mul	x8, x10, x8
 2cc:	mul	x9, x9, x11
 2d0:	mul	x10, x10, x11
 2d4:	lsr	x11, x12, #32
 2d8:	add	x10, x10, x9, lsr #32
 2dc:	add	x11, x11, w8, uxtw
 2e0:	add	x8, x10, x8, lsr #32
 2e4:	add	x9, x11, w9, uxtw
 2e8:	bfi	x12, x9, #32, #32
 2ec:	add	x8, x8, x9, lsr #32
 2f0:	str	x12, [x3]
 2f4:	str	x8, [x2]
 2f8:	ret

00000000000002fc <wideLeftShift>:
 2fc:	ldr	x8, [x0]
 300:	ldr	x9, [x1]
 304:	extr	x8, x8, x9, #63
 308:	str	x8, [x0]
 30c:	ldr	x8, [x1]
 310:	lsl	x8, x8, #1
 314:	str	x8, [x1]
 318:	ret

000000000000031c <wideRightShiftWithSticky>:
 31c:	cmp	w2, #0x3f
 320:	mov	w8, w2
 324:	b.hi	364 <wideRightShiftWithSticky+0x48>  // b.pmore
 328:	ldr	x9, [x1]
 32c:	ldr	x10, [x0]
 330:	neg	x11, x8
 334:	lsl	x12, x9, x11
 338:	lsl	x10, x10, x11
 33c:	lsr	x9, x9, x8
 340:	cmp	x12, #0x0
 344:	cset	w11, ne  // ne = any
 348:	orr	x9, x10, x9
 34c:	orr	x9, x9, x11
 350:	str	x9, [x1]
 354:	ldr	x9, [x0]
 358:	lsr	x9, x9, x8
 35c:	str	x9, [x0]
 360:	ret
 364:	ldr	x10, [x0]
 368:	cmp	w2, #0x7f
 36c:	b.hi	398 <wideRightShiftWithSticky+0x7c>  // b.pmore
 370:	ldr	x11, [x1]
 374:	neg	x12, x8
 378:	lsl	x12, x10, x12
 37c:	lsr	x8, x10, x8
 380:	orr	x11, x11, x12
 384:	cmp	x11, #0x0
 388:	cset	w11, ne  // ne = any
 38c:	mov	x9, xzr
 390:	orr	x8, x8, x11
 394:	b	3ac <wideRightShiftWithSticky+0x90>
 398:	ldr	x8, [x1]
 39c:	mov	x9, xzr
 3a0:	orr	x8, x8, x10
 3a4:	cmp	x8, #0x0
 3a8:	cset	w8, ne  // ne = any
 3ac:	str	x8, [x1]
 3b0:	str	x9, [x0]
 3b4:	ret

00000000000003b8 <rep_clz>:
 3b8:	clz	x0, x0
 3bc:	ret

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	stp	x22, x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x1
  14:	mov	x20, x0
  18:	lsr	x21, x0, #32
  1c:	lsr	x22, x1, #32
  20:	bl	40 <__muldsi3>
  24:	mul	w8, w21, w19
  28:	madd	w8, w22, w20, w8
  2c:	ldp	x20, x19, [sp, #32]
  30:	ldp	x22, x21, [sp, #16]
  34:	add	x0, x0, x8, lsl #32
  38:	ldp	x29, x30, [sp], #48
  3c:	ret

0000000000000040 <__muldsi3>:
  40:	and	w8, w0, #0xffff
  44:	and	w9, w1, #0xffff
  48:	lsr	w10, w0, #16
  4c:	lsr	w11, w1, #16
  50:	mul	w0, w9, w8
  54:	mul	w9, w9, w10
  58:	mul	w8, w11, w8
  5c:	mul	w10, w11, w10
  60:	add	w9, w9, w0, lsr #16
  64:	add	w8, w8, w9, uxth
  68:	add	w9, w10, w9, lsr #16
  6c:	bfi	w0, w8, #16, #16
  70:	add	w8, w9, w8, lsr #16
  74:	bfi	x0, x8, #32, #32
  78:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   4:	mov	x8, x0
   8:	cmp	x0, x9
   c:	mul	x0, x1, x0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulodi4+0x24>  // b.any
  18:	cmp	x1, #0x2
  1c:	b.cs	90 <__mulodi4+0x90>  // b.hs, b.nlast
  20:	ret
  24:	cmp	x1, x9
  28:	b.ne	38 <__mulodi4+0x38>  // b.any
  2c:	cmp	x8, #0x2
  30:	b.cc	20 <__mulodi4+0x20>  // b.lo, b.ul, b.last
  34:	b	90 <__mulodi4+0x90>
  38:	asr	x9, x8, #63
  3c:	eor	x8, x9, x8
  40:	sub	x8, x8, x9
  44:	cmp	x8, #0x2
  48:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  4c:	asr	x11, x1, #63
  50:	eor	x10, x11, x1
  54:	sub	x10, x10, x11
  58:	cmp	x10, #0x2
  5c:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  60:	cmp	x9, x11
  64:	b.ne	7c <__mulodi4+0x7c>  // b.any
  68:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  6c:	sdiv	x9, x9, x10
  70:	cmp	x8, x9
  74:	b.le	20 <__mulodi4+0x20>
  78:	b	90 <__mulodi4+0x90>
  7c:	neg	x9, x10
  80:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  84:	sdiv	x9, x10, x9
  88:	cmp	x8, x9
  8c:	b.le	20 <__mulodi4+0x20>
  90:	mov	w8, #0x1                   	// #1
  94:	str	w8, [x2]
  98:	ret

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w9, #0x80000000            	// #-2147483648
   4:	mov	w8, w0
   8:	cmp	w0, w9
   c:	mul	w0, w1, w0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulosi4+0x24>  // b.any
  18:	cmp	w1, #0x2
  1c:	b.cs	90 <__mulosi4+0x90>  // b.hs, b.nlast
  20:	ret
  24:	cmp	w1, w9
  28:	b.ne	38 <__mulosi4+0x38>  // b.any
  2c:	cmp	w8, #0x2
  30:	b.cc	20 <__mulosi4+0x20>  // b.lo, b.ul, b.last
  34:	b	90 <__mulosi4+0x90>
  38:	asr	w9, w8, #31
  3c:	eor	w8, w9, w8
  40:	sub	w8, w8, w9
  44:	cmp	w8, #0x2
  48:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  4c:	asr	w11, w1, #31
  50:	eor	w10, w11, w1
  54:	sub	w10, w10, w11
  58:	cmp	w10, #0x2
  5c:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  60:	cmp	w9, w11
  64:	b.ne	7c <__mulosi4+0x7c>  // b.any
  68:	mov	w9, #0x7fffffff            	// #2147483647
  6c:	sdiv	w9, w9, w10
  70:	cmp	w8, w9
  74:	b.le	20 <__mulosi4+0x20>
  78:	b	90 <__mulosi4+0x90>
  7c:	neg	w9, w10
  80:	mov	w10, #0x80000000            	// #-2147483648
  84:	sdiv	w9, w10, w9
  88:	cmp	w8, w9
  8c:	b.le	20 <__mulosi4+0x20>
  90:	mov	w8, #0x1                   	// #1
  94:	str	w8, [x2]
  98:	ret

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	umulh	x8, x2, x0
  18:	eor	x9, x1, #0x8000000000000000
  1c:	madd	x8, x2, x1, x8
  20:	mov	x19, x4
  24:	orr	x9, x0, x9
  28:	madd	x20, x3, x0, x8
  2c:	mul	x21, x2, x0
  30:	str	wzr, [x4]
  34:	cbnz	x9, 48 <__muloti4+0x48>
  38:	cmp	x2, #0x2
  3c:	cset	w8, cc  // cc = lo, ul, last
  40:	cmp	x3, #0x0
  44:	b	60 <__muloti4+0x60>
  48:	eor	x8, x3, #0x8000000000000000
  4c:	orr	x8, x2, x8
  50:	cbnz	x8, 8c <__muloti4+0x8c>
  54:	cmp	x0, #0x2
  58:	cset	w8, cc  // cc = lo, ul, last
  5c:	cmp	x1, #0x0
  60:	csel	w8, w8, wzr, eq  // eq = none
  64:	tbnz	w8, #0, 70 <__muloti4+0x70>
  68:	mov	w8, #0x1                   	// #1
  6c:	str	w8, [x19]
  70:	mov	x0, x21
  74:	mov	x1, x20
  78:	ldp	x20, x19, [sp, #48]
  7c:	ldp	x22, x21, [sp, #32]
  80:	ldr	x23, [sp, #16]
  84:	ldp	x29, x30, [sp], #64
  88:	ret
  8c:	asr	x8, x1, #63
  90:	eor	x11, x8, x0
  94:	asr	x9, x3, #63
  98:	eor	x10, x8, x1
  9c:	subs	x23, x11, x8
  a0:	eor	x13, x9, x2
  a4:	sbcs	x22, x10, x8
  a8:	eor	x12, x9, x3
  ac:	subs	x2, x13, x9
  b0:	sbcs	x3, x12, x9
  b4:	cmp	x23, #0x2
  b8:	cset	w10, cc  // cc = lo, ul, last
  bc:	cmp	x22, #0x0
  c0:	cset	w11, lt  // lt = tstop
  c4:	csel	w10, w10, w11, eq  // eq = none
  c8:	tbnz	w10, #0, 70 <__muloti4+0x70>
  cc:	cmp	x2, #0x2
  d0:	cset	w10, cc  // cc = lo, ul, last
  d4:	cmp	x3, #0x0
  d8:	cset	w11, lt  // lt = tstop
  dc:	csel	w10, w10, w11, eq  // eq = none
  e0:	tbnz	w10, #0, 70 <__muloti4+0x70>
  e4:	eor	x8, x8, x9
  e8:	orr	x8, x8, x8
  ec:	cbnz	x8, fc <__muloti4+0xfc>
  f0:	mov	x0, #0xffffffffffffffff    	// #-1
  f4:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  f8:	b	10c <__muloti4+0x10c>
  fc:	negs	x2, x2
 100:	ngcs	x3, x3
 104:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 108:	mov	x0, xzr
 10c:	bl	0 <__divti3>
 110:	cmp	x23, x0
 114:	cset	w8, ls  // ls = plast
 118:	cmp	x22, x1
 11c:	cset	w9, le
 120:	csel	w8, w8, w9, eq  // eq = none
 124:	tbz	w8, #0, 68 <__muloti4+0x68>
 128:	b	70 <__muloti4+0x70>

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s7, s0, s2
   4:	fmul	s6, s1, s3
   8:	mov	v5.16b, v0.16b
   c:	fmul	s16, s0, s3
  10:	fmul	s17, s1, s2
  14:	fsub	s0, s7, s6
  18:	mov	v4.16b, v1.16b
  1c:	fcmp	s0, s0
  20:	fadd	s1, s17, s16
  24:	b.vc	10c <__mulsc3+0x10c>
  28:	fcmp	s1, s1
  2c:	b.vc	10c <__mulsc3+0x10c>
  30:	mov	w8, #0x7f800000            	// #2139095040
  34:	fabs	s20, s5
  38:	fmov	s18, w8
  3c:	fabs	s19, s4
  40:	fcmp	s20, s18
  44:	fmov	s18, #1.000000000000000000e+00
  48:	b.eq	64 <__mulsc3+0x64>  // b.none
  4c:	mov	w8, #0x7f800000            	// #2139095040
  50:	fmov	s21, w8
  54:	fcmp	s19, s21
  58:	b.eq	64 <__mulsc3+0x64>  // b.none
  5c:	mov	w8, wzr
  60:	b	b4 <__mulsc3+0xb4>
  64:	mov	w8, #0x7f800000            	// #2139095040
  68:	fmov	s23, w8
  6c:	fmov	s21, wzr
  70:	fcmp	s20, s23
  74:	movi	v22.4s, #0x80, lsl #24
  78:	fcsel	s20, s18, s21, eq  // eq = none
  7c:	fcmp	s19, s23
  80:	fmov	s19, wzr
  84:	bit	v19.16b, v2.16b, v22.16b
  88:	bit	v20.16b, v5.16b, v22.16b
  8c:	fcsel	s5, s18, s21, eq  // eq = none
  90:	fcmp	s2, s2
  94:	bit	v21.16b, v3.16b, v22.16b
  98:	bit	v5.16b, v4.16b, v22.16b
  9c:	fcsel	s2, s19, s2, vs
  a0:	fcmp	s3, s3
  a4:	fcsel	s3, s21, s3, vs
  a8:	mov	w8, #0x1                   	// #1
  ac:	mov	v4.16b, v5.16b
  b0:	mov	v5.16b, v20.16b
  b4:	mov	w9, #0x7f800000            	// #2139095040
  b8:	fabs	s19, s3
  bc:	fmov	s20, w9
  c0:	fcmp	s19, s20
  c4:	fabs	s20, s2
  c8:	b.eq	110 <__mulsc3+0x110>  // b.none
  cc:	mov	w9, #0x7f800000            	// #2139095040
  d0:	fmov	s21, w9
  d4:	fcmp	s20, s21
  d8:	b.eq	110 <__mulsc3+0x110>  // b.none
  dc:	cbz	w8, 164 <__mulsc3+0x164>
  e0:	cbz	w8, 10c <__mulsc3+0x10c>
  e4:	fmul	s0, s2, s5
  e8:	fmul	s1, s3, s4
  ec:	mov	w8, #0x7f800000            	// #2139095040
  f0:	fmul	s3, s3, s5
  f4:	fmul	s2, s2, s4
  f8:	fsub	s0, s0, s1
  fc:	fmov	s1, w8
 100:	fadd	s2, s2, s3
 104:	fmul	s0, s0, s1
 108:	fmul	s1, s2, s1
 10c:	ret
 110:	mov	w8, #0x7f800000            	// #2139095040
 114:	fmov	s23, w8
 118:	fmov	s21, wzr
 11c:	fcmp	s20, s23
 120:	movi	v22.4s, #0x80, lsl #24
 124:	fcsel	s20, s18, s21, eq  // eq = none
 128:	fcmp	s19, s23
 12c:	fmov	s19, wzr
 130:	bit	v19.16b, v5.16b, v22.16b
 134:	bit	v20.16b, v2.16b, v22.16b
 138:	fcsel	s2, s18, s21, eq  // eq = none
 13c:	fcmp	s5, s5
 140:	bit	v21.16b, v4.16b, v22.16b
 144:	bit	v2.16b, v3.16b, v22.16b
 148:	fcsel	s5, s19, s5, vs
 14c:	fcmp	s4, s4
 150:	fcsel	s4, s21, s4, vs
 154:	mov	w8, #0x1                   	// #1
 158:	mov	v3.16b, v2.16b
 15c:	mov	v2.16b, v20.16b
 160:	cbnz	w8, e0 <__mulsc3+0xe0>
 164:	mov	w8, #0x7f800000            	// #2139095040
 168:	fabs	s17, s17
 16c:	fmov	s18, w8
 170:	fcmp	s17, s18
 174:	b.eq	1c0 <__mulsc3+0x1c0>  // b.none
 178:	mov	w8, #0x7f800000            	// #2139095040
 17c:	fabs	s16, s16
 180:	fmov	s17, w8
 184:	fcmp	s16, s17
 188:	b.eq	1c0 <__mulsc3+0x1c0>  // b.none
 18c:	mov	w8, #0x7f800000            	// #2139095040
 190:	fabs	s7, s7
 194:	fmov	s16, w8
 198:	fcmp	s7, s16
 19c:	b.eq	1c0 <__mulsc3+0x1c0>  // b.none
 1a0:	mov	w8, #0x7f800000            	// #2139095040
 1a4:	fabs	s6, s6
 1a8:	fmov	s7, w8
 1ac:	fcmp	s6, s7
 1b0:	b.eq	1c0 <__mulsc3+0x1c0>  // b.none
 1b4:	mov	w8, wzr
 1b8:	cbnz	w8, e4 <__mulsc3+0xe4>
 1bc:	b	10c <__mulsc3+0x10c>
 1c0:	movi	v7.4s, #0x80, lsl #24
 1c4:	fmov	s16, wzr
 1c8:	fmov	s17, wzr
 1cc:	bit	v16.16b, v5.16b, v7.16b
 1d0:	fcmp	s5, s5
 1d4:	fmov	s18, wzr
 1d8:	bit	v17.16b, v4.16b, v7.16b
 1dc:	fcsel	s5, s16, s5, vs
 1e0:	fcmp	s4, s4
 1e4:	fmov	s6, wzr
 1e8:	bit	v18.16b, v2.16b, v7.16b
 1ec:	fcsel	s4, s17, s4, vs
 1f0:	fcmp	s2, s2
 1f4:	bit	v6.16b, v3.16b, v7.16b
 1f8:	fcsel	s2, s18, s2, vs
 1fc:	fcmp	s3, s3
 200:	fcsel	s3, s6, s3, vs
 204:	mov	w8, #0x1                   	// #1
 208:	cbnz	w8, e4 <__mulsc3+0xe4>
 20c:	b	10c <__mulsc3+0x10c>

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__mulXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__mulXf3__>:
  14:	sub	sp, sp, #0x50
  18:	str	d8, [sp, #16]
  1c:	stp	x29, x30, [sp, #24]
  20:	str	x23, [sp, #40]
  24:	stp	x22, x21, [sp, #48]
  28:	stp	x20, x19, [sp, #64]
  2c:	add	x29, sp, #0x10
  30:	mov	v8.16b, v1.16b
  34:	bl	254 <toRep>
  38:	mov	v0.16b, v8.16b
  3c:	mov	w20, w0
  40:	ubfx	w21, w0, #23, #8
  44:	bl	254 <toRep>
  48:	eor	w8, w0, w20
  4c:	sub	w11, w21, #0x1
  50:	ubfx	w22, w0, #23, #8
  54:	and	w9, w20, #0x7fffff
  58:	and	w10, w0, #0x7fffff
  5c:	and	w19, w8, #0x80000000
  60:	cmp	w11, #0xfd
  64:	stur	w9, [x29, #-4]
  68:	str	w10, [sp, #8]
  6c:	b.hi	84 <__mulXf3__+0x70>  // b.pmore
  70:	sub	w8, w22, #0x1
  74:	cmp	w8, #0xfe
  78:	b.cs	84 <__mulXf3__+0x70>  // b.hs, b.nlast
  7c:	mov	w20, wzr
  80:	b	118 <__mulXf3__+0x104>
  84:	mov	w9, #0x1                   	// #1
  88:	and	w8, w20, #0x7fffffff
  8c:	movk	w9, #0x7f80, lsl #16
  90:	cmp	w8, w9
  94:	b.cc	a0 <__mulXf3__+0x8c>  // b.lo, b.ul, b.last
  98:	orr	w0, w20, #0x400000
  9c:	b	104 <__mulXf3__+0xf0>
  a0:	and	w23, w0, #0x7fffffff
  a4:	cmp	w23, w9
  a8:	b.cc	b4 <__mulXf3__+0xa0>  // b.lo, b.ul, b.last
  ac:	orr	w0, w0, #0x400000
  b0:	b	104 <__mulXf3__+0xf0>
  b4:	mov	w9, #0x7f800000            	// #2139095040
  b8:	cmp	w8, w9
  bc:	b.ne	cc <__mulXf3__+0xb8>  // b.any
  c0:	cbz	w23, d8 <__mulXf3__+0xc4>
  c4:	orr	w0, w19, #0x7f800000
  c8:	b	104 <__mulXf3__+0xf0>
  cc:	cmp	w23, w9
  d0:	b.ne	e0 <__mulXf3__+0xcc>  // b.any
  d4:	cbnz	w8, c4 <__mulXf3__+0xb0>
  d8:	mov	w0, #0x7fc00000            	// #2143289344
  dc:	b	104 <__mulXf3__+0xf0>
  e0:	cbz	w8, 100 <__mulXf3__+0xec>
  e4:	cbz	w23, 100 <__mulXf3__+0xec>
  e8:	lsr	w8, w8, #23
  ec:	cbnz	w8, 230 <__mulXf3__+0x21c>
  f0:	sub	x0, x29, #0x4
  f4:	bl	264 <normalize>
  f8:	mov	w20, w0
  fc:	b	234 <__mulXf3__+0x220>
 100:	mov	w0, w19
 104:	bl	25c <fromRep>
 108:	mov	v8.16b, v0.16b
 10c:	mov	w20, wzr
 110:	mov	w8, wzr
 114:	cbz	w8, 1d8 <__mulXf3__+0x1c4>
 118:	ldur	w8, [x29, #-4]
 11c:	ldr	w9, [sp, #8]
 120:	add	x2, sp, #0x4
 124:	mov	x3, sp
 128:	orr	w0, w8, #0x800000
 12c:	orr	w8, w9, #0x800000
 130:	lsl	w1, w8, #8
 134:	stur	w0, [x29, #-4]
 138:	str	w8, [sp, #8]
 13c:	bl	2b4 <wideMultiply>
 140:	ldrb	w8, [sp, #6]
 144:	add	w9, w21, w22
 148:	add	w9, w9, w20
 14c:	sub	w20, w9, #0x7f
 150:	tbnz	w8, #7, 1c0 <__mulXf3__+0x1ac>
 154:	add	x0, sp, #0x4
 158:	mov	x1, sp
 15c:	bl	2c8 <wideLeftShift>
 160:	cmp	w20, #0xff
 164:	b.ge	1cc <__mulXf3__+0x1b8>  // b.tcont
 168:	cmp	w20, #0x0
 16c:	b.le	1f8 <__mulXf3__+0x1e4>
 170:	ldr	w8, [sp, #4]
 174:	bfi	w8, w20, #23, #9
 178:	str	w8, [sp, #4]
 17c:	ldp	w8, w9, [sp]
 180:	mov	w10, #0x80000001            	// #-2147483647
 184:	orr	w9, w9, w19
 188:	cmp	w8, w10
 18c:	str	w9, [sp, #4]
 190:	b.cc	19c <__mulXf3__+0x188>  // b.lo, b.ul, b.last
 194:	add	w9, w9, #0x1
 198:	str	w9, [sp, #4]
 19c:	mov	w9, #0x80000000            	// #-2147483648
 1a0:	cmp	w8, w9
 1a4:	b.ne	1b8 <__mulXf3__+0x1a4>  // b.any
 1a8:	ldr	w8, [sp, #4]
 1ac:	and	w9, w8, #0x1
 1b0:	add	w8, w9, w8
 1b4:	str	w8, [sp, #4]
 1b8:	ldr	w0, [sp, #4]
 1bc:	b	1d0 <__mulXf3__+0x1bc>
 1c0:	add	w20, w20, #0x1
 1c4:	cmp	w20, #0xff
 1c8:	b.lt	168 <__mulXf3__+0x154>  // b.tstop
 1cc:	orr	w0, w19, #0x7f800000
 1d0:	bl	25c <fromRep>
 1d4:	mov	v8.16b, v0.16b
 1d8:	mov	v0.16b, v8.16b
 1dc:	ldp	x20, x19, [sp, #64]
 1e0:	ldp	x22, x21, [sp, #48]
 1e4:	ldr	x23, [sp, #40]
 1e8:	ldp	x29, x30, [sp, #24]
 1ec:	ldr	d8, [sp, #16]
 1f0:	add	sp, sp, #0x50
 1f4:	ret
 1f8:	mov	w8, #0x1                   	// #1
 1fc:	sub	w2, w8, w20
 200:	cmp	w2, #0x20
 204:	b.cc	218 <__mulXf3__+0x204>  // b.lo, b.ul, b.last
 208:	mov	w0, w19
 20c:	bl	25c <fromRep>
 210:	tbnz	wzr, #0, 17c <__mulXf3__+0x168>
 214:	b	1d4 <__mulXf3__+0x1c0>
 218:	add	x0, sp, #0x4
 21c:	mov	x1, sp
 220:	bl	2e8 <wideRightShiftWithSticky>
 224:	mov	w8, #0x1                   	// #1
 228:	tbnz	w8, #0, 17c <__mulXf3__+0x168>
 22c:	b	1d8 <__mulXf3__+0x1c4>
 230:	mov	w20, wzr
 234:	lsr	w8, w23, #23
 238:	cbnz	w8, 248 <__mulXf3__+0x234>
 23c:	add	x0, sp, #0x8
 240:	bl	264 <normalize>
 244:	add	w20, w0, w20
 248:	mov	w8, #0x1                   	// #1
 24c:	cbnz	w8, 118 <__mulXf3__+0x104>
 250:	b	1d8 <__mulXf3__+0x1c4>

0000000000000254 <toRep>:
 254:	fmov	w0, s0
 258:	ret

000000000000025c <fromRep>:
 25c:	fmov	s0, w0
 260:	ret

0000000000000264 <normalize>:
 264:	stp	x29, x30, [sp, #-48]!
 268:	str	x21, [sp, #16]
 26c:	stp	x20, x19, [sp, #32]
 270:	mov	x29, sp
 274:	ldr	w20, [x0]
 278:	mov	x19, x0
 27c:	mov	w0, w20
 280:	bl	380 <rep_clz>
 284:	mov	w21, w0
 288:	mov	w0, #0x800000              	// #8388608
 28c:	bl	380 <rep_clz>
 290:	sub	w8, w21, w0
 294:	lsl	w10, w20, w8
 298:	str	w10, [x19]
 29c:	ldp	x20, x19, [sp, #32]
 2a0:	ldr	x21, [sp, #16]
 2a4:	mov	w9, #0x1                   	// #1
 2a8:	sub	w0, w9, w8
 2ac:	ldp	x29, x30, [sp], #48
 2b0:	ret

00000000000002b4 <wideMultiply>:
 2b4:	umull	x8, w1, w0
 2b8:	lsr	x9, x8, #32
 2bc:	str	w9, [x2]
 2c0:	str	w8, [x3]
 2c4:	ret

00000000000002c8 <wideLeftShift>:
 2c8:	ldr	w8, [x0]
 2cc:	ldr	w9, [x1]
 2d0:	extr	w8, w8, w9, #31
 2d4:	str	w8, [x0]
 2d8:	ldr	w8, [x1]
 2dc:	lsl	w8, w8, #1
 2e0:	str	w8, [x1]
 2e4:	ret

00000000000002e8 <wideRightShiftWithSticky>:
 2e8:	cmp	w2, #0x1f
 2ec:	b.hi	32c <wideRightShiftWithSticky+0x44>  // b.pmore
 2f0:	ldr	w8, [x1]
 2f4:	ldr	w9, [x0]
 2f8:	neg	w10, w2
 2fc:	lsl	w11, w8, w10
 300:	lsl	w9, w9, w10
 304:	lsr	w8, w8, w2
 308:	cmp	w11, #0x0
 30c:	cset	w10, ne  // ne = any
 310:	orr	w8, w9, w8
 314:	orr	w8, w8, w10
 318:	str	w8, [x1]
 31c:	ldr	w8, [x0]
 320:	lsr	w8, w8, w2
 324:	str	w8, [x0]
 328:	ret
 32c:	ldr	w9, [x0]
 330:	cmp	w2, #0x3f
 334:	b.hi	360 <wideRightShiftWithSticky+0x78>  // b.pmore
 338:	ldr	w10, [x1]
 33c:	neg	w11, w2
 340:	lsl	w11, w9, w11
 344:	lsr	w9, w9, w2
 348:	orr	w10, w10, w11
 34c:	cmp	w10, #0x0
 350:	cset	w10, ne  // ne = any
 354:	mov	w8, wzr
 358:	orr	w9, w9, w10
 35c:	b	374 <wideRightShiftWithSticky+0x8c>
 360:	ldr	w10, [x1]
 364:	mov	w8, wzr
 368:	orr	w9, w10, w9
 36c:	cmp	w9, #0x0
 370:	cset	w9, ne  // ne = any
 374:	str	w9, [x1]
 378:	str	w8, [x0]
 37c:	ret

0000000000000380 <rep_clz>:
 380:	clz	w0, w0
 384:	ret

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	stp	x22, x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x21, x1
  14:	mov	x1, x2
  18:	mov	x19, x3
  1c:	mov	x20, x2
  20:	mov	x22, x0
  24:	bl	44 <__mulddi3>
  28:	mul	x8, x21, x20
  2c:	madd	x8, x19, x22, x8
  30:	ldp	x20, x19, [sp, #32]
  34:	ldp	x22, x21, [sp, #16]
  38:	add	x1, x8, x1
  3c:	ldp	x29, x30, [sp], #48
  40:	ret

0000000000000044 <__mulddi3>:
  44:	and	x8, x0, #0xffffffff
  48:	and	x9, x1, #0xffffffff
  4c:	lsr	x10, x0, #32
  50:	lsr	x11, x1, #32
  54:	mul	x0, x9, x8
  58:	mul	x9, x9, x10
  5c:	mul	x8, x11, x8
  60:	mul	x10, x11, x10
  64:	add	x9, x9, x0, lsr #32
  68:	add	x8, x8, w9, uxtw
  6c:	add	x9, x10, x9, lsr #32
  70:	bfi	x0, x8, #32, #32
  74:	add	x1, x9, x8, lsr #32
  78:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__mulXf3__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__mulXf3__>:
  14:	sub	sp, sp, #0x90
  18:	stp	x29, x30, [sp, #80]
  1c:	str	x23, [sp, #96]
  20:	stp	x22, x21, [sp, #112]
  24:	stp	x20, x19, [sp, #128]
  28:	add	x29, sp, #0x50
  2c:	str	q1, [sp]
  30:	bl	2c4 <toRep>
  34:	ldr	q0, [sp]
  38:	mov	x20, x0
  3c:	mov	x21, x1
  40:	ubfx	x22, x1, #48, #15
  44:	bl	2c4 <toRep>
  48:	eor	x8, x1, x21
  4c:	sub	w11, w22, #0x1
  50:	mov	w12, #0x7ffd                	// #32765
  54:	ubfx	x23, x1, #48, #15
  58:	and	x9, x21, #0xffffffffffff
  5c:	and	x10, x1, #0xffffffffffff
  60:	and	x19, x8, #0x8000000000000000
  64:	cmp	w11, w12
  68:	stp	x20, x9, [x29, #-16]
  6c:	stp	x0, x10, [x29, #-32]
  70:	b.hi	8c <__mulXf3__+0x78>  // b.pmore
  74:	sub	w8, w23, #0x1
  78:	mov	w9, #0x7ffe                	// #32766
  7c:	cmp	w8, w9
  80:	b.cs	8c <__mulXf3__+0x78>  // b.hs, b.nlast
  84:	mov	w20, wzr
  88:	b	15c <__mulXf3__+0x148>
  8c:	and	x8, x21, #0x7fffffffffffffff
  90:	cmp	x20, #0x0
  94:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
  98:	cset	w10, eq  // eq = none
  9c:	cmp	x8, x9
  a0:	cset	w11, cc  // cc = lo, ul, last
  a4:	csel	w10, w10, w11, eq  // eq = none
  a8:	tbnz	w10, #0, b8 <__mulXf3__+0xa4>
  ac:	orr	x1, x21, #0x800000000000
  b0:	mov	x0, x20
  b4:	b	14c <__mulXf3__+0x138>
  b8:	and	x21, x1, #0x7fffffffffffffff
  bc:	cmp	x0, #0x0
  c0:	cset	w10, eq  // eq = none
  c4:	cmp	x21, x9
  c8:	cset	w9, cc  // cc = lo, ul, last
  cc:	csel	w9, w10, w9, eq  // eq = none
  d0:	tbnz	w9, #0, dc <__mulXf3__+0xc8>
  d4:	orr	x1, x1, #0x800000000000
  d8:	b	14c <__mulXf3__+0x138>
  dc:	eor	x9, x8, #0x7fff000000000000
  e0:	orr	x9, x20, x9
  e4:	cbnz	x9, fc <__mulXf3__+0xe8>
  e8:	orr	x8, x0, x21
  ec:	cbz	x8, 110 <__mulXf3__+0xfc>
  f0:	orr	x1, x19, #0x7fff000000000000
  f4:	mov	x0, xzr
  f8:	b	14c <__mulXf3__+0x138>
  fc:	eor	x9, x21, #0x7fff000000000000
 100:	orr	x9, x0, x9
 104:	cbnz	x9, 11c <__mulXf3__+0x108>
 108:	orr	x8, x20, x8
 10c:	cbnz	x8, f0 <__mulXf3__+0xdc>
 110:	mov	x1, #0x7fff800000000000    	// #9223231299366420480
 114:	mov	x0, xzr
 118:	b	14c <__mulXf3__+0x138>
 11c:	orr	x9, x20, x8
 120:	cbz	x9, 144 <__mulXf3__+0x130>
 124:	orr	x9, x0, x21
 128:	cbz	x9, 144 <__mulXf3__+0x130>
 12c:	lsr	x8, x8, #48
 130:	cbnz	x8, 2a0 <__mulXf3__+0x28c>
 134:	sub	x0, x29, #0x10
 138:	bl	2dc <normalize>
 13c:	mov	w20, w0
 140:	b	2a4 <__mulXf3__+0x290>
 144:	mov	x0, xzr
 148:	mov	x1, x19
 14c:	bl	2d0 <fromRep>
 150:	mov	w20, wzr
 154:	mov	w8, wzr
 158:	cbz	w8, 248 <__mulXf3__+0x234>
 15c:	ldp	x0, x8, [x29, #-16]
 160:	ldp	x10, x9, [x29, #-32]
 164:	add	x4, sp, #0x20
 168:	add	x5, sp, #0x10
 16c:	orr	x1, x8, #0x1000000000000
 170:	orr	x8, x9, #0x1000000000000
 174:	extr	x3, x8, x10, #49
 178:	lsl	x2, x10, #15
 17c:	str	q0, [sp]
 180:	stur	x1, [x29, #-8]
 184:	stur	x8, [x29, #-24]
 188:	bl	360 <wideMultiply>
 18c:	ldrb	w8, [sp, #46]
 190:	add	w9, w22, w23
 194:	add	w9, w9, w20
 198:	mov	w10, #0xffffc001            	// #-16383
 19c:	add	w20, w9, w10
 1a0:	tbnz	w8, #0, 1b4 <__mulXf3__+0x1a0>
 1a4:	add	x0, sp, #0x20
 1a8:	add	x1, sp, #0x10
 1ac:	bl	464 <wideLeftShift>
 1b0:	b	1b8 <__mulXf3__+0x1a4>
 1b4:	add	w20, w20, #0x1
 1b8:	mov	w8, #0x7fff                	// #32767
 1bc:	cmp	w20, w8
 1c0:	b.lt	1d0 <__mulXf3__+0x1bc>  // b.tstop
 1c4:	mov	x0, xzr
 1c8:	orr	x1, x19, #0x7fff000000000000
 1cc:	b	244 <__mulXf3__+0x230>
 1d0:	cmp	w20, #0x0
 1d4:	b.le	260 <__mulXf3__+0x24c>
 1d8:	ldr	x8, [sp, #40]
 1dc:	bfi	x8, x20, #48, #16
 1e0:	str	x8, [sp, #40]
 1e4:	ldp	x9, x8, [sp, #16]
 1e8:	ldp	x10, x11, [sp, #32]
 1ec:	mov	x12, #0x8000000000000000    	// #-9223372036854775808
 1f0:	cmp	x8, #0x0
 1f4:	cset	w13, ge  // ge = tcont
 1f8:	cmp	x9, #0x0
 1fc:	cset	w14, eq  // eq = none
 200:	cmp	x8, x12
 204:	orr	x11, x11, x19
 208:	csel	w12, w14, w13, eq  // eq = none
 20c:	str	x11, [sp, #40]
 210:	tbnz	w12, #0, 220 <__mulXf3__+0x20c>
 214:	adds	x10, x10, #0x1
 218:	adcs	x11, x11, xzr
 21c:	stp	x10, x11, [sp, #32]
 220:	eor	x8, x8, #0x8000000000000000
 224:	orr	x8, x9, x8
 228:	cbnz	x8, 240 <__mulXf3__+0x22c>
 22c:	ldp	x8, x9, [sp, #32]
 230:	and	x10, x8, #0x1
 234:	adds	x8, x10, x8
 238:	adcs	x9, x9, xzr
 23c:	stp	x8, x9, [sp, #32]
 240:	ldp	x0, x1, [sp, #32]
 244:	bl	2d0 <fromRep>
 248:	ldp	x20, x19, [sp, #128]
 24c:	ldp	x22, x21, [sp, #112]
 250:	ldr	x23, [sp, #96]
 254:	ldp	x29, x30, [sp, #80]
 258:	add	sp, sp, #0x90
 25c:	ret
 260:	mov	w8, #0x1                   	// #1
 264:	sub	w2, w8, w20
 268:	cmp	w2, #0x80
 26c:	b.cc	284 <__mulXf3__+0x270>  // b.lo, b.ul, b.last
 270:	mov	x0, xzr
 274:	mov	x1, x19
 278:	bl	2d0 <fromRep>
 27c:	tbnz	wzr, #0, 1e4 <__mulXf3__+0x1d0>
 280:	b	248 <__mulXf3__+0x234>
 284:	add	x0, sp, #0x20
 288:	add	x1, sp, #0x10
 28c:	bl	48c <wideRightShiftWithSticky>
 290:	ldr	q0, [sp]
 294:	mov	w8, #0x1                   	// #1
 298:	tbnz	w8, #0, 1e4 <__mulXf3__+0x1d0>
 29c:	b	248 <__mulXf3__+0x234>
 2a0:	mov	w20, wzr
 2a4:	lsr	x8, x21, #48
 2a8:	cbnz	x8, 2b8 <__mulXf3__+0x2a4>
 2ac:	sub	x0, x29, #0x20
 2b0:	bl	2dc <normalize>
 2b4:	add	w20, w0, w20
 2b8:	mov	w8, #0x1                   	// #1
 2bc:	cbnz	w8, 15c <__mulXf3__+0x148>
 2c0:	b	248 <__mulXf3__+0x234>

00000000000002c4 <toRep>:
 2c4:	str	q0, [sp, #-16]!
 2c8:	ldp	x0, x1, [sp], #16
 2cc:	ret

00000000000002d0 <fromRep>:
 2d0:	stp	x0, x1, [sp, #-16]!
 2d4:	ldr	q0, [sp], #16
 2d8:	ret

00000000000002dc <normalize>:
 2dc:	stp	x29, x30, [sp, #-48]!
 2e0:	stp	x22, x21, [sp, #16]
 2e4:	stp	x20, x19, [sp, #32]
 2e8:	mov	x29, sp
 2ec:	ldp	x21, x20, [x0]
 2f0:	mov	x19, x0
 2f4:	mov	x0, x21
 2f8:	mov	x1, x20
 2fc:	bl	640 <rep_clz>
 300:	mov	w22, w0
 304:	mov	x1, #0x1000000000000       	// #281474976710656
 308:	mov	x0, xzr
 30c:	bl	640 <rep_clz>
 310:	sub	w8, w22, w0
 314:	neg	x10, x8
 318:	mov	w9, #0x1                   	// #1
 31c:	cmp	x8, #0x0
 320:	lsr	x10, x21, x10
 324:	lsl	x11, x20, x8
 328:	lsl	x12, x21, x8
 32c:	lsl	x13, x21, x8
 330:	sub	w0, w9, w8
 334:	sub	x8, x8, #0x40
 338:	csel	x9, xzr, x10, eq  // eq = none
 33c:	cmp	x8, #0x0
 340:	orr	x8, x9, x11
 344:	csel	x9, xzr, x13, ge  // ge = tcont
 348:	csel	x8, x12, x8, ge  // ge = tcont
 34c:	stp	x9, x8, [x19]
 350:	ldp	x20, x19, [sp, #32]
 354:	ldp	x22, x21, [sp, #16]
 358:	ldp	x29, x30, [sp], #48
 35c:	ret

0000000000000360 <wideMultiply>:
 360:	str	x19, [sp, #-16]!
 364:	lsr	x8, x1, #32
 368:	lsr	x9, x3, #32
 36c:	and	x10, x3, #0xffffffff
 370:	lsr	x11, x2, #32
 374:	and	x12, x2, #0xffffffff
 378:	and	x13, x1, #0xffffffff
 37c:	lsr	x14, x0, #32
 380:	and	x15, x0, #0xffffffff
 384:	mul	x17, x11, x8
 388:	mul	x2, x11, x13
 38c:	mul	x3, x14, x9
 390:	mul	x6, x10, x14
 394:	mul	x7, x11, x14
 398:	mul	x14, x14, x12
 39c:	mul	x11, x11, x15
 3a0:	adds	x11, x11, x14
 3a4:	mul	x0, x13, x9
 3a8:	mul	x1, x10, x13
 3ac:	mul	x13, x13, x12
 3b0:	adcs	x14, xzr, xzr
 3b4:	adds	x13, x7, x13
 3b8:	mul	x16, x10, x8
 3bc:	mul	x18, x12, x8
 3c0:	mul	x19, x15, x9
 3c4:	mul	x10, x10, x15
 3c8:	mul	x12, x12, x15
 3cc:	adcs	x15, xzr, xzr
 3d0:	adds	x10, x13, x10
 3d4:	adcs	x13, x15, xzr
 3d8:	adds	x15, x19, x18
 3dc:	adcs	x18, xzr, xzr
 3e0:	adds	x15, x15, x2
 3e4:	adcs	x18, x18, xzr
 3e8:	adds	x15, x15, x6
 3ec:	adcs	x18, x18, xzr
 3f0:	adds	x16, x16, x0
 3f4:	adcs	x0, xzr, xzr
 3f8:	extr	x14, x14, x11, #32
 3fc:	adds	x11, x12, x11, lsl #32
 400:	extr	x12, x0, x16, #32
 404:	adcs	x0, xzr, xzr
 408:	adds	x10, x10, x14
 40c:	adcs	x14, xzr, xzr
 410:	adds	x10, x10, x15, lsl #32
 414:	adcs	x14, x14, xzr
 418:	add	x10, x0, x10
 41c:	stp	x11, x10, [x5]
 420:	adds	x10, x1, x3
 424:	adcs	x11, xzr, xzr
 428:	adds	x10, x10, x17
 42c:	adcs	x11, x11, xzr
 430:	madd	x8, x9, x8, x11
 434:	adds	x9, x10, x16, lsl #32
 438:	adcs	x8, x8, x12
 43c:	adds	x9, x9, x13
 440:	extr	x18, x18, x15, #32
 444:	adcs	x8, x8, xzr
 448:	adds	x9, x9, x18
 44c:	adcs	x8, x8, xzr
 450:	adds	x9, x9, x14
 454:	adcs	x8, x8, xzr
 458:	stp	x9, x8, [x4]
 45c:	ldr	x19, [sp], #16
 460:	ret

0000000000000464 <wideLeftShift>:
 464:	ldp	x8, x9, [x0]
 468:	ldr	x10, [x1, #8]
 46c:	extr	x9, x9, x8, #63
 470:	extr	x8, x8, x10, #63
 474:	stp	x8, x9, [x0]
 478:	ldp	x8, x9, [x1]
 47c:	extr	x9, x9, x8, #63
 480:	lsl	x8, x8, #1
 484:	stp	x8, x9, [x1]
 488:	ret

000000000000048c <wideRightShiftWithSticky>:
 48c:	cmp	w2, #0x7f
 490:	mov	w8, w2
 494:	b.hi	57c <wideRightShiftWithSticky+0xf0>  // b.pmore
 498:	ldp	x14, x15, [x0]
 49c:	mov	w11, #0x40                  	// #64
 4a0:	ldp	x12, x13, [x1]
 4a4:	mov	w9, #0x80                  	// #128
 4a8:	neg	x10, x8
 4ac:	sub	x11, x11, x8
 4b0:	sub	x9, x9, x8
 4b4:	cmp	x11, #0x0
 4b8:	lsl	x17, x14, x10
 4bc:	csel	x18, xzr, x17, ge  // ge = tcont
 4c0:	cmp	x9, #0x0
 4c4:	neg	x9, x9
 4c8:	lsr	x14, x14, x9
 4cc:	lsr	x9, x12, x9
 4d0:	csel	x9, xzr, x9, eq  // eq = none
 4d4:	csel	x14, xzr, x14, eq  // eq = none
 4d8:	cmp	x11, #0x0
 4dc:	lsl	x11, x13, x10
 4e0:	orr	x9, x9, x11
 4e4:	lsl	x15, x15, x10
 4e8:	lsl	x10, x12, x10
 4ec:	csel	x9, x10, x9, ge  // ge = tcont
 4f0:	csel	x10, xzr, x10, ge  // ge = tcont
 4f4:	orr	x14, x14, x15
 4f8:	orr	x9, x10, x9
 4fc:	neg	x16, x8
 500:	csel	x14, x17, x14, ge  // ge = tcont
 504:	cmp	x9, #0x0
 508:	lsl	x15, x13, x16
 50c:	cset	w9, ne  // ne = any
 510:	cmp	x8, #0x0
 514:	sub	x11, x8, #0x40
 518:	lsr	x12, x12, x2
 51c:	csel	x10, xzr, x15, eq  // eq = none
 520:	lsr	x17, x13, x8
 524:	cmp	x11, #0x0
 528:	orr	x10, x12, x10
 52c:	lsr	x13, x13, x2
 530:	csel	x10, x17, x10, ge  // ge = tcont
 534:	csel	x12, xzr, x13, ge  // ge = tcont
 538:	orr	x10, x18, x10
 53c:	orr	x12, x14, x12
 540:	orr	x9, x10, x9
 544:	stp	x9, x12, [x1]
 548:	ldp	x10, x9, [x0]
 54c:	cmp	x8, #0x0
 550:	lsl	x12, x9, x16
 554:	lsr	x10, x10, x2
 558:	lsr	x8, x9, x8
 55c:	lsr	x13, x9, x2
 560:	csel	x9, xzr, x12, eq  // eq = none
 564:	cmp	x11, #0x0
 568:	orr	x9, x10, x9
 56c:	csel	x9, x8, x9, ge  // ge = tcont
 570:	csel	x10, xzr, x13, ge  // ge = tcont
 574:	stp	x9, x10, [x0]
 578:	ret
 57c:	ldp	x12, x11, [x0]
 580:	cmp	w2, #0xff
 584:	b.hi	610 <wideRightShiftWithSticky+0x184>  // b.pmore
 588:	mov	w13, #0x100                 	// #256
 58c:	subs	x13, x13, x8
 590:	mov	w15, #0xc0                  	// #192
 594:	neg	x13, x13
 598:	neg	x14, x8
 59c:	ldp	x17, x16, [x1]
 5a0:	sub	x15, x15, x8
 5a4:	lsr	x13, x12, x13
 5a8:	csel	x13, xzr, x13, eq  // eq = none
 5ac:	cmp	x15, #0x0
 5b0:	lsl	x15, x11, x14
 5b4:	lsl	x14, x12, x14
 5b8:	orr	x13, x13, x15
 5bc:	csel	x13, x14, x13, ge  // ge = tcont
 5c0:	csel	x14, xzr, x14, ge  // ge = tcont
 5c4:	orr	x14, x17, x14
 5c8:	orr	x13, x16, x13
 5cc:	orr	x13, x14, x13
 5d0:	cmp	x13, #0x0
 5d4:	cset	w13, ne  // ne = any
 5d8:	cmp	x8, #0x80
 5dc:	lsr	x12, x12, x8
 5e0:	lsr	x11, x11, x8
 5e4:	sub	x14, x8, #0xc0
 5e8:	csel	x8, xzr, x15, eq  // eq = none
 5ec:	cmp	x14, #0x0
 5f0:	orr	x8, x12, x8
 5f4:	csel	x12, xzr, x11, ge  // ge = tcont
 5f8:	csel	x8, x11, x8, ge  // ge = tcont
 5fc:	mov	x9, xzr
 600:	mov	x10, xzr
 604:	orr	x8, x8, x13
 608:	str	x12, [x1, #8]
 60c:	b	634 <wideRightShiftWithSticky+0x1a8>
 610:	ldp	x13, x8, [x1]
 614:	mov	x9, xzr
 618:	mov	x10, xzr
 61c:	str	xzr, [x1, #8]
 620:	orr	x8, x8, x11
 624:	orr	x11, x13, x12
 628:	orr	x8, x11, x8
 62c:	cmp	x8, #0x0
 630:	cset	w8, ne  // ne = any
 634:	str	x8, [x1]
 638:	stp	x9, x10, [x0]
 63c:	ret

0000000000000640 <rep_clz>:
 640:	cmp	x1, #0x0
 644:	csel	x9, x0, x1, eq  // eq = none
 648:	cset	w8, eq  // eq = none
 64c:	clz	x9, x9
 650:	add	w0, w9, w8, lsl #6
 654:	ret

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   c:	cmp	x0, x8
  10:	b.ne	28 <__mulvdi3+0x28>  // b.any
  14:	cmp	x1, #0x1
  18:	b.hi	bc <__mulvdi3+0xbc>  // b.pmore
  1c:	lsl	x0, x1, #63
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	cmp	x1, x8
  2c:	b.ne	44 <__mulvdi3+0x44>  // b.any
  30:	cmp	x0, #0x1
  34:	b.hi	d4 <__mulvdi3+0xd4>  // b.pmore
  38:	lsl	x0, x0, #63
  3c:	ldp	x29, x30, [sp], #16
  40:	ret
  44:	asr	x9, x0, #63
  48:	eor	x8, x9, x0
  4c:	sub	x8, x8, x9
  50:	cmp	x8, #0x2
  54:	b.lt	b0 <__mulvdi3+0xb0>  // b.tstop
  58:	asr	x11, x1, #63
  5c:	eor	x10, x11, x1
  60:	sub	x10, x10, x11
  64:	cmp	x10, #0x2
  68:	b.lt	b0 <__mulvdi3+0xb0>  // b.tstop
  6c:	cmp	x9, x11
  70:	b.ne	9c <__mulvdi3+0x9c>  // b.any
  74:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  78:	sdiv	x9, x9, x10
  7c:	cmp	x8, x9
  80:	b.le	b0 <__mulvdi3+0xb0>
  84:	adrp	x0, 0 <__mulvdi3>
  88:	adrp	x2, 0 <__mulvdi3>
  8c:	add	x0, x0, #0x0
  90:	add	x2, x2, #0x0
  94:	mov	w1, #0x29                  	// #41
  98:	bl	0 <__compilerrt_abort_impl>
  9c:	neg	x9, x10
  a0:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  a4:	sdiv	x9, x10, x9
  a8:	cmp	x8, x9
  ac:	b.gt	ec <__mulvdi3+0xec>
  b0:	mul	x0, x1, x0
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret
  bc:	adrp	x0, 0 <__mulvdi3>
  c0:	adrp	x2, 0 <__mulvdi3>
  c4:	add	x0, x0, #0x0
  c8:	add	x2, x2, #0x0
  cc:	mov	w1, #0x1a                  	// #26
  d0:	bl	0 <__compilerrt_abort_impl>
  d4:	adrp	x0, 0 <__mulvdi3>
  d8:	adrp	x2, 0 <__mulvdi3>
  dc:	add	x0, x0, #0x0
  e0:	add	x2, x2, #0x0
  e4:	mov	w1, #0x1f                  	// #31
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x0, 0 <__mulvdi3>
  f0:	adrp	x2, 0 <__mulvdi3>
  f4:	add	x0, x0, #0x0
  f8:	add	x2, x2, #0x0
  fc:	mov	w1, #0x2c                  	// #44
 100:	bl	0 <__compilerrt_abort_impl>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, #0x80000000            	// #-2147483648
   c:	cmp	w0, w8
  10:	b.ne	28 <__mulvsi3+0x28>  // b.any
  14:	cmp	w1, #0x1
  18:	b.hi	bc <__mulvsi3+0xbc>  // b.pmore
  1c:	lsl	w0, w1, #31
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	cmp	w1, w8
  2c:	b.ne	44 <__mulvsi3+0x44>  // b.any
  30:	cmp	w0, #0x1
  34:	b.hi	d4 <__mulvsi3+0xd4>  // b.pmore
  38:	lsl	w0, w0, #31
  3c:	ldp	x29, x30, [sp], #16
  40:	ret
  44:	asr	w9, w0, #31
  48:	eor	w8, w9, w0
  4c:	sub	w8, w8, w9
  50:	cmp	w8, #0x2
  54:	b.lt	b0 <__mulvsi3+0xb0>  // b.tstop
  58:	asr	w11, w1, #31
  5c:	eor	w10, w11, w1
  60:	sub	w10, w10, w11
  64:	cmp	w10, #0x2
  68:	b.lt	b0 <__mulvsi3+0xb0>  // b.tstop
  6c:	cmp	w9, w11
  70:	b.ne	9c <__mulvsi3+0x9c>  // b.any
  74:	mov	w9, #0x7fffffff            	// #2147483647
  78:	sdiv	w9, w9, w10
  7c:	cmp	w8, w9
  80:	b.le	b0 <__mulvsi3+0xb0>
  84:	adrp	x0, 0 <__mulvsi3>
  88:	adrp	x2, 0 <__mulvsi3>
  8c:	add	x0, x0, #0x0
  90:	add	x2, x2, #0x0
  94:	mov	w1, #0x29                  	// #41
  98:	bl	0 <__compilerrt_abort_impl>
  9c:	neg	w9, w10
  a0:	mov	w10, #0x80000000            	// #-2147483648
  a4:	sdiv	w9, w10, w9
  a8:	cmp	w8, w9
  ac:	b.gt	ec <__mulvsi3+0xec>
  b0:	mul	w0, w1, w0
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret
  bc:	adrp	x0, 0 <__mulvsi3>
  c0:	adrp	x2, 0 <__mulvsi3>
  c4:	add	x0, x0, #0x0
  c8:	add	x2, x2, #0x0
  cc:	mov	w1, #0x1a                  	// #26
  d0:	bl	0 <__compilerrt_abort_impl>
  d4:	adrp	x0, 0 <__mulvsi3>
  d8:	adrp	x2, 0 <__mulvsi3>
  dc:	add	x0, x0, #0x0
  e0:	add	x2, x2, #0x0
  e4:	mov	w1, #0x1f                  	// #31
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x0, 0 <__mulvsi3>
  f0:	adrp	x2, 0 <__mulvsi3>
  f4:	add	x0, x0, #0x0
  f8:	add	x2, x2, #0x0
  fc:	mov	w1, #0x2c                  	// #44
 100:	bl	0 <__compilerrt_abort_impl>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	stp	x24, x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	eor	x8, x1, #0x8000000000000000
  18:	mov	x21, x3
  1c:	orr	x8, x0, x8
  20:	mov	x20, x2
  24:	cbnz	x8, 4c <__mulvti3+0x4c>
  28:	cmp	x20, #0x1
  2c:	cset	w8, hi  // hi = pmore
  30:	cmp	x21, #0x0
  34:	cset	w9, ne  // ne = any
  38:	csel	w8, w8, w9, eq  // eq = none
  3c:	tbnz	w8, #0, 174 <__mulvti3+0x174>
  40:	mov	x0, xzr
  44:	lsl	x1, x20, #63
  48:	b	160 <__mulvti3+0x160>
  4c:	eor	x8, x21, #0x8000000000000000
  50:	mov	x22, x1
  54:	mov	x19, x0
  58:	orr	x8, x20, x8
  5c:	cbnz	x8, 84 <__mulvti3+0x84>
  60:	cmp	x19, #0x1
  64:	cset	w8, hi  // hi = pmore
  68:	cmp	x22, #0x0
  6c:	cset	w9, ne  // ne = any
  70:	csel	w8, w8, w9, eq  // eq = none
  74:	tbnz	w8, #0, 18c <__mulvti3+0x18c>
  78:	mov	x0, xzr
  7c:	lsl	x1, x19, #63
  80:	b	160 <__mulvti3+0x160>
  84:	asr	x8, x22, #63
  88:	eor	x11, x8, x19
  8c:	asr	x9, x21, #63
  90:	eor	x10, x8, x22
  94:	subs	x24, x11, x8
  98:	eor	x13, x9, x20
  9c:	sbcs	x23, x10, x8
  a0:	eor	x12, x9, x21
  a4:	subs	x2, x13, x9
  a8:	sbcs	x3, x12, x9
  ac:	cmp	x24, #0x2
  b0:	cset	w10, cc  // cc = lo, ul, last
  b4:	cmp	x23, #0x0
  b8:	cset	w11, lt  // lt = tstop
  bc:	csel	w10, w10, w11, eq  // eq = none
  c0:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  c4:	cmp	x2, #0x2
  c8:	cset	w10, cc  // cc = lo, ul, last
  cc:	cmp	x3, #0x0
  d0:	cset	w11, lt  // lt = tstop
  d4:	csel	w10, w10, w11, eq  // eq = none
  d8:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  dc:	eor	x8, x8, x9
  e0:	orr	x8, x8, x8
  e4:	cbnz	x8, 124 <__mulvti3+0x124>
  e8:	mov	x0, #0xffffffffffffffff    	// #-1
  ec:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  f0:	bl	0 <__divti3>
  f4:	cmp	x24, x0
  f8:	cset	w8, ls  // ls = plast
  fc:	cmp	x23, x1
 100:	cset	w9, le
 104:	csel	w8, w8, w9, eq  // eq = none
 108:	tbnz	w8, #0, 150 <__mulvti3+0x150>
 10c:	adrp	x0, 0 <__mulvti3>
 110:	adrp	x2, 0 <__mulvti3>
 114:	add	x0, x0, #0x0
 118:	add	x2, x2, #0x0
 11c:	mov	w1, #0x2b                  	// #43
 120:	bl	0 <__compilerrt_abort_impl>
 124:	negs	x2, x2
 128:	ngcs	x3, x3
 12c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 130:	mov	x0, xzr
 134:	bl	0 <__divti3>
 138:	cmp	x24, x0
 13c:	cset	w8, ls  // ls = plast
 140:	cmp	x23, x1
 144:	cset	w9, le
 148:	csel	w8, w8, w9, eq  // eq = none
 14c:	tbz	w8, #0, 1a4 <__mulvti3+0x1a4>
 150:	umulh	x8, x20, x19
 154:	madd	x8, x20, x22, x8
 158:	madd	x1, x21, x19, x8
 15c:	mul	x0, x20, x19
 160:	ldp	x20, x19, [sp, #48]
 164:	ldp	x22, x21, [sp, #32]
 168:	ldp	x24, x23, [sp, #16]
 16c:	ldp	x29, x30, [sp], #64
 170:	ret
 174:	adrp	x0, 0 <__mulvti3>
 178:	adrp	x2, 0 <__mulvti3>
 17c:	add	x0, x0, #0x0
 180:	add	x2, x2, #0x0
 184:	mov	w1, #0x1c                  	// #28
 188:	bl	0 <__compilerrt_abort_impl>
 18c:	adrp	x0, 0 <__mulvti3>
 190:	adrp	x2, 0 <__mulvti3>
 194:	add	x0, x0, #0x0
 198:	add	x2, x2, #0x0
 19c:	mov	w1, #0x21                  	// #33
 1a0:	bl	0 <__compilerrt_abort_impl>
 1a4:	adrp	x0, 0 <__mulvti3>
 1a8:	adrp	x2, 0 <__mulvti3>
 1ac:	add	x0, x0, #0x0
 1b0:	add	x2, x2, #0x0
 1b4:	mov	w1, #0x2e                  	// #46
 1b8:	bl	0 <__compilerrt_abort_impl>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	24 <toRep>
   c:	eor	x0, x0, #0x8000000000000000
  10:	bl	1c <fromRep>
  14:	ldp	x29, x30, [sp], #16
  18:	ret

000000000000001c <fromRep>:
  1c:	fmov	d0, x0
  20:	ret

0000000000000024 <toRep>:
  24:	fmov	x0, d0
  28:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	24 <toRep>
   c:	eor	w0, w0, #0x80000000
  10:	bl	1c <fromRep>
  14:	ldp	x29, x30, [sp], #16
  18:	ret

000000000000001c <fromRep>:
  1c:	fmov	s0, w0
  20:	ret

0000000000000024 <toRep>:
  24:	fmov	w0, s0
  28:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngcs	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	14 <__negvdi2+0x14>  // b.none
   c:	neg	x0, x0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvdi2>
  20:	adrp	x2, 0 <__negvdi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	14 <__negvsi2+0x14>  // b.none
   c:	neg	w0, w0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvsi2>
  20:	adrp	x2, 0 <__negvsi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 18 <__negvti2+0x18>
   c:	negs	x0, x0
  10:	ngcs	x1, x1
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__negvti2>
  24:	adrp	x2, 0 <__negvti2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x18                  	// #24
  34:	bl	0 <__compilerrt_abort_impl>

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	lsr	x8, x0, #32
   c:	eor	w0, w8, w0
  10:	bl	0 <__paritysi2>
  14:	ldp	x29, x30, [sp], #16
  18:	ret

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w8, w0, w0, lsr #16
   4:	eor	w8, w8, w8, lsr #8
   8:	eor	w8, w8, w8, lsr #4
   c:	and	w8, w8, #0xf
  10:	mov	w9, #0x6996                	// #27030
  14:	lsr	w8, w9, w8
  18:	and	w0, w8, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	eor	x0, x1, x0
   c:	bl	0 <__paritydi2>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x8, x0, #1
   4:	and	x8, x8, #0x5555555555555555
   8:	sub	x8, x0, x8
   c:	lsr	x9, x8, #2
  10:	and	x9, x9, #0x3333333333333333
  14:	and	x8, x8, #0x3333333333333333
  18:	add	x8, x9, x8
  1c:	add	x8, x8, x8, lsr #4
  20:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  24:	lsr	x9, x8, #32
  28:	add	w8, w9, w8
  2c:	add	w8, w8, w8, lsr #16
  30:	add	w8, w8, w8, lsr #8
  34:	and	w0, w8, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w8, w0, #1
   4:	and	w8, w8, #0x55555555
   8:	sub	w8, w0, w8
   c:	lsr	w9, w8, #2
  10:	and	w9, w9, #0x33333333
  14:	and	w8, w8, #0x33333333
  18:	add	w8, w9, w8
  1c:	add	w8, w8, w8, lsr #4
  20:	and	w8, w8, #0xf0f0f0f
  24:	add	w8, w8, w8, lsr #16
  28:	add	w8, w8, w8, lsr #8
  2c:	and	w0, w8, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x8, x0, #1
   4:	lsr	x9, x1, #1
   8:	and	x8, x8, #0x5555555555555555
   c:	and	x9, x9, #0x5555555555555555
  10:	subs	x8, x0, x8
  14:	sbcs	x9, x1, x9
  18:	lsr	x10, x8, #2
  1c:	lsr	x11, x9, #2
  20:	and	x8, x8, #0x3333333333333333
  24:	and	x10, x10, #0x3333333333333333
  28:	and	x9, x9, #0x3333333333333333
  2c:	and	x11, x11, #0x3333333333333333
  30:	add	x8, x10, x8
  34:	add	x9, x11, x9
  38:	add	x8, x8, x8, lsr #4
  3c:	add	x9, x9, x9, lsr #4
  40:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  44:	and	x9, x9, #0xf0f0f0f0f0f0f0f
  48:	add	x8, x9, x8
  4c:	lsr	x9, x8, #32
  50:	add	w8, w9, w8
  54:	add	w8, w8, w8, lsr #16
  58:	add	w8, w8, w8, lsr #8
  5c:	and	w0, w8, #0xff
  60:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	tst	w0, #0x1
   4:	fmov	d1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	d2, d1, d0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	44 <__powidf2+0x44>  // b.lo, b.ul, b.last
  18:	mov	w8, w0
  1c:	cmp	w8, #0x0
  20:	cinc	w8, w8, lt  // lt = tstop
  24:	fmul	d0, d0, d0
  28:	asr	w8, w8, #1
  2c:	fmul	d3, d0, d2
  30:	tst	w8, #0x1
  34:	add	w9, w8, #0x1
  38:	fcsel	d2, d2, d3, eq  // eq = none
  3c:	cmp	w9, #0x2
  40:	b.hi	1c <__powidf2+0x1c>  // b.pmore
  44:	fdiv	d0, d1, d2
  48:	cmp	w0, #0x0
  4c:	fcsel	d0, d0, d2, lt  // lt = tstop
  50:	ret

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	tst	w0, #0x1
   4:	fmov	s1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	s2, s1, s0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	44 <__powisf2+0x44>  // b.lo, b.ul, b.last
  18:	mov	w8, w0
  1c:	cmp	w8, #0x0
  20:	cinc	w8, w8, lt  // lt = tstop
  24:	fmul	s0, s0, s0
  28:	asr	w8, w8, #1
  2c:	fmul	s3, s0, s2
  30:	tst	w8, #0x1
  34:	add	w9, w8, #0x1
  38:	fcsel	s2, s2, s3, eq  // eq = none
  3c:	cmp	w9, #0x2
  40:	b.hi	1c <__powisf2+0x1c>  // b.pmore
  44:	fdiv	s0, s1, s2
  48:	cmp	w0, #0x0
  4c:	fcsel	s0, s0, s2, lt  // lt = tstop
  50:	ret

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	str	d8, [sp, #-32]!
   4:	stp	x29, x30, [sp, #16]
   8:	mov	x29, sp
   c:	mov	v8.16b, v0.16b
  10:	mov	v0.16b, v1.16b
  14:	bl	40 <toRep>
  18:	eor	x0, x0, #0x8000000000000000
  1c:	bl	38 <fromRep>
  20:	mov	v1.16b, v0.16b
  24:	mov	v0.16b, v8.16b
  28:	bl	0 <__adddf3>
  2c:	ldp	x29, x30, [sp, #16]
  30:	ldr	d8, [sp], #32
  34:	ret

0000000000000038 <fromRep>:
  38:	fmov	d0, x0
  3c:	ret

0000000000000040 <toRep>:
  40:	fmov	x0, d0
  44:	ret

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	str	d8, [sp, #-32]!
   4:	stp	x29, x30, [sp, #16]
   8:	mov	x29, sp
   c:	mov	v8.16b, v0.16b
  10:	mov	v0.16b, v1.16b
  14:	bl	40 <toRep>
  18:	eor	w0, w0, #0x80000000
  1c:	bl	38 <fromRep>
  20:	mov	v1.16b, v0.16b
  24:	mov	v0.16b, v8.16b
  28:	bl	0 <__addsf3>
  2c:	ldp	x29, x30, [sp, #16]
  30:	ldr	d8, [sp], #32
  34:	ret

0000000000000038 <fromRep>:
  38:	fmov	s0, w0
  3c:	ret

0000000000000040 <toRep>:
  40:	fmov	w0, s0
  44:	ret

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	sub	x0, x0, x1
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__subvdi3+0x34>
  18:	b.le	38 <__subvdi3+0x38>
  1c:	adrp	x0, 0 <__subvdi3>
  20:	adrp	x2, 0 <__subvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvdi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvdi3>
  44:	adrp	x2, 0 <__subvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	sub	w0, w0, w1
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__subvsi3+0x34>
  18:	b.le	38 <__subvsi3+0x38>
  1c:	adrp	x0, 0 <__subvsi3>
  20:	adrp	x2, 0 <__subvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvsi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvsi3>
  44:	adrp	x2, 0 <__subvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	subs	x0, x0, x2
  10:	mov	x8, x1
  14:	sbcs	x1, x1, x3
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__subvti3+0x4c>
  20:	cset	w9, ls  // ls = plast
  24:	cmp	x1, x8
  28:	cset	w8, le
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__subvti3+0x60>
  34:	adrp	x0, 0 <__subvti3>
  38:	adrp	x2, 0 <__subvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, hi  // hi = pmore
  50:	cmp	x1, x8
  54:	cset	w8, gt
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__subvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__subvti3>
  6c:	adrp	x2, 0 <__subvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	str	q0, [sp]
  10:	mov	v0.16b, v1.16b
  14:	bl	44 <toRep>
  18:	eor	x1, x1, #0x8000000000000000
  1c:	bl	38 <fromRep>
  20:	mov	v1.16b, v0.16b
  24:	ldr	q0, [sp]
  28:	bl	0 <__addtf3>
  2c:	ldp	x29, x30, [sp, #16]
  30:	add	sp, sp, #0x20
  34:	ret

0000000000000038 <fromRep>:
  38:	stp	x0, x1, [sp, #-16]!
  3c:	ldr	q0, [sp], #16
  40:	ret

0000000000000044 <toRep>:
  44:	str	q0, [sp, #-16]!
  48:	ldp	x0, x1, [sp], #16
  4c:	ret

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__truncXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__truncXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	130 <srcToRep>
  20:	and	x8, x0, #0x7fffffffffffffff
  24:	mov	x9, #0xc0f0000000000000    	// #-4544132024016830464
  28:	mov	x10, #0xbf10000000000000    	// #-4679240012837945344
  2c:	add	x9, x8, x9
  30:	add	x10, x8, x10
  34:	cmp	x9, x10
  38:	b.cs	60 <__truncXfYf2__+0x4c>  // b.hs, b.nlast
  3c:	mov	x8, #0x1                   	// #1
  40:	and	x9, x0, #0x3ffffffffff
  44:	movk	x8, #0x200, lsl #32
  48:	cmp	x9, x8
  4c:	lsr	x8, x0, #42
  50:	b.cc	7c <__truncXfYf2__+0x68>  // b.lo, b.ul, b.last
  54:	mov	w9, #0x4001                	// #16385
  58:	add	w8, w8, w9
  5c:	b	118 <__truncXfYf2__+0x104>
  60:	mov	x9, #0x1                   	// #1
  64:	movk	x9, #0x7ff0, lsl #48
  68:	cmp	x8, x9
  6c:	b.cc	98 <__truncXfYf2__+0x84>  // b.lo, b.ul, b.last
  70:	ubfx	x8, x0, #42, #9
  74:	orr	w8, w8, #0x7e00
  78:	b	118 <__truncXfYf2__+0x104>
  7c:	mov	x10, #0x20000000000         	// #2199023255552
  80:	cmp	x9, x10
  84:	add	w8, w8, #0x4, lsl #12
  88:	b.ne	118 <__truncXfYf2__+0x104>  // b.any
  8c:	and	w9, w8, #0x1
  90:	add	w8, w9, w8
  94:	b	118 <__truncXfYf2__+0x104>
  98:	lsr	x8, x8, #52
  9c:	cmp	x8, #0x40e
  a0:	b.ls	ac <__truncXfYf2__+0x98>  // b.plast
  a4:	mov	w8, #0x7c00                	// #31744
  a8:	b	118 <__truncXfYf2__+0x104>
  ac:	cmp	w8, #0x3bd
  b0:	b.cs	bc <__truncXfYf2__+0xa8>  // b.hs, b.nlast
  b4:	mov	w8, wzr
  b8:	b	118 <__truncXfYf2__+0x104>
  bc:	mov	x9, #0x10000000000000      	// #4503599627370496
  c0:	mov	w10, #0x3f1                 	// #1009
  c4:	sub	w11, w8, #0x3b1
  c8:	bfxil	x9, x0, #0, #52
  cc:	sub	w8, w10, w8
  d0:	lsl	x10, x9, x11
  d4:	lsr	x9, x9, x8
  d8:	cmp	x10, #0x0
  dc:	and	x8, x9, #0x3ffffffffff
  e0:	cset	w10, ne  // ne = any
  e4:	orr	x10, x8, x10
  e8:	mov	x8, #0x1                   	// #1
  ec:	movk	x8, #0x200, lsl #32
  f0:	cmp	x10, x8
  f4:	lsr	x8, x9, #42
  f8:	b.cc	104 <__truncXfYf2__+0xf0>  // b.lo, b.ul, b.last
  fc:	add	w8, w8, #0x1
 100:	b	118 <__truncXfYf2__+0x104>
 104:	mov	x11, #0x20000000000         	// #2199023255552
 108:	cmp	x10, x11
 10c:	b.ne	118 <__truncXfYf2__+0x104>  // b.any
 110:	ubfx	x9, x9, #42, #1
 114:	add	w8, w9, w8
 118:	lsr	x9, x0, #48
 11c:	and	w9, w9, #0x8000
 120:	orr	w0, w8, w9
 124:	bl	138 <dstFromRep>
 128:	ldp	x29, x30, [sp], #16
 12c:	ret

0000000000000130 <srcToRep>:
 130:	fmov	x0, d0
 134:	ret

0000000000000138 <dstFromRep>:
 138:	ret

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__truncXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__truncXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	130 <srcToRep>
  20:	and	x8, x0, #0x7fffffffffffffff
  24:	mov	x9, #0xc7f0000000000000    	// #-4039728865751334912
  28:	mov	x10, #0xb810000000000000    	// #-5183643171103440896
  2c:	add	x9, x8, x9
  30:	add	x10, x8, x10
  34:	cmp	x9, x10
  38:	b.cs	64 <__truncXfYf2__+0x50>  // b.hs, b.nlast
  3c:	mov	w9, #0x1                   	// #1
  40:	and	x8, x0, #0x1fffffff
  44:	movk	w9, #0x1000, lsl #16
  48:	cmp	x8, x9
  4c:	lsr	x9, x0, #29
  50:	b.cc	80 <__truncXfYf2__+0x6c>  // b.lo, b.ul, b.last
  54:	mov	w8, #0x1                   	// #1
  58:	movk	w8, #0x4000, lsl #16
  5c:	add	w8, w9, w8
  60:	b	118 <__truncXfYf2__+0x104>
  64:	mov	x9, #0x1                   	// #1
  68:	movk	x9, #0x7ff0, lsl #48
  6c:	cmp	x8, x9
  70:	b.cc	a0 <__truncXfYf2__+0x8c>  // b.lo, b.ul, b.last
  74:	ubfx	x8, x0, #29, #22
  78:	orr	w8, w8, #0x7fc00000
  7c:	b	118 <__truncXfYf2__+0x104>
  80:	mov	w10, #0x40000000            	// #1073741824
  84:	mov	w11, #0x10000000            	// #268435456
  88:	cmp	x8, x11
  8c:	add	w8, w9, w10
  90:	b.ne	118 <__truncXfYf2__+0x104>  // b.any
  94:	and	w9, w8, #0x1
  98:	add	w8, w9, w8
  9c:	b	118 <__truncXfYf2__+0x104>
  a0:	lsr	x8, x8, #52
  a4:	cmp	x8, #0x47e
  a8:	b.ls	b4 <__truncXfYf2__+0xa0>  // b.plast
  ac:	mov	w8, #0x7f800000            	// #2139095040
  b0:	b	118 <__truncXfYf2__+0x104>
  b4:	cmp	w8, #0x34d
  b8:	b.cs	c4 <__truncXfYf2__+0xb0>  // b.hs, b.nlast
  bc:	mov	w8, wzr
  c0:	b	118 <__truncXfYf2__+0x104>
  c4:	mov	x9, #0x10000000000000      	// #4503599627370496
  c8:	mov	w10, #0x381                 	// #897
  cc:	sub	w11, w8, #0x341
  d0:	bfxil	x9, x0, #0, #52
  d4:	sub	w8, w10, w8
  d8:	lsl	x10, x9, x11
  dc:	lsr	x8, x9, x8
  e0:	cmp	x10, #0x0
  e4:	and	x9, x8, #0x1fffffff
  e8:	cset	w10, ne  // ne = any
  ec:	orr	x9, x9, x10
  f0:	mov	w10, #0x1                   	// #1
  f4:	movk	w10, #0x1000, lsl #16
  f8:	cmp	x9, x10
  fc:	lsr	x8, x8, #29
 100:	b.cc	10c <__truncXfYf2__+0xf8>  // b.lo, b.ul, b.last
 104:	add	w8, w8, #0x1
 108:	b	118 <__truncXfYf2__+0x104>
 10c:	mov	w10, #0x10000000            	// #268435456
 110:	cmp	x9, x10
 114:	b.eq	94 <__truncXfYf2__+0x80>  // b.none
 118:	lsr	x9, x0, #32
 11c:	and	w9, w9, #0x80000000
 120:	orr	w0, w8, w9
 124:	bl	138 <dstFromRep>
 128:	ldp	x29, x30, [sp], #16
 12c:	ret

0000000000000130 <srcToRep>:
 130:	fmov	x0, d0
 134:	ret

0000000000000138 <dstFromRep>:
 138:	fmov	s0, w0
 13c:	ret

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	14 <__truncXfYf2__>
   c:	ldp	x29, x30, [sp], #16
  10:	ret

0000000000000014 <__truncXfYf2__>:
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	130 <srcToRep>
  20:	and	w8, w0, #0x7fffffff
  24:	mov	w9, #0xc7800000            	// #-947912704
  28:	mov	w10, #0xb8800000            	// #-1199570944
  2c:	add	w9, w8, w9
  30:	add	w10, w8, w10
  34:	cmp	w9, w10
  38:	b.cs	64 <__truncXfYf2__+0x50>  // b.hs, b.nlast
  3c:	ubfx	w9, w0, #13, #16
  40:	and	w8, w0, #0x1fff
  44:	cmp	w8, #0x1, lsl #12
  48:	sub	w8, w9, #0x1c, lsl #12
  4c:	b.ls	80 <__truncXfYf2__+0x6c>  // b.plast
  50:	mov	w8, #0x4000                	// #16384
  54:	movk	w8, #0xfffe, lsl #16
  58:	add	w8, w8, w9
  5c:	add	w8, w8, #0x1
  60:	b	104 <__truncXfYf2__+0xf0>
  64:	mov	w9, #0x1                   	// #1
  68:	movk	w9, #0x7f80, lsl #16
  6c:	cmp	w8, w9
  70:	b.cc	90 <__truncXfYf2__+0x7c>  // b.lo, b.ul, b.last
  74:	mov	w8, #0x7e00                	// #32256
  78:	bfxil	w8, w0, #13, #9
  7c:	b	104 <__truncXfYf2__+0xf0>
  80:	b.ne	104 <__truncXfYf2__+0xf0>  // b.any
  84:	and	w9, w8, #0x1
  88:	add	w8, w9, w8, uxth
  8c:	b	104 <__truncXfYf2__+0xf0>
  90:	lsr	w9, w8, #23
  94:	cmp	w9, #0x8e
  98:	b.ls	a4 <__truncXfYf2__+0x90>  // b.plast
  9c:	mov	w8, #0x7c00                	// #31744
  a0:	b	104 <__truncXfYf2__+0xf0>
  a4:	lsr	w8, w8, #24
  a8:	cmp	w8, #0x2d
  ac:	b.cs	b8 <__truncXfYf2__+0xa4>  // b.hs, b.nlast
  b0:	mov	w8, wzr
  b4:	b	104 <__truncXfYf2__+0xf0>
  b8:	mov	w8, #0x800000              	// #8388608
  bc:	mov	w10, #0x71                  	// #113
  c0:	sub	w11, w9, #0x51
  c4:	bfxil	w8, w0, #0, #23
  c8:	sub	w9, w10, w9
  cc:	lsl	w10, w8, w11
  d0:	lsr	w9, w8, w9
  d4:	cmp	w10, #0x0
  d8:	cset	w8, ne  // ne = any
  dc:	and	w10, w9, #0x1fff
  e0:	orr	w8, w10, w8
  e4:	cmp	w8, #0x1, lsl #12
  e8:	lsr	w8, w9, #13
  ec:	b.ls	f8 <__truncXfYf2__+0xe4>  // b.plast
  f0:	add	w8, w8, #0x1
  f4:	b	104 <__truncXfYf2__+0xf0>
  f8:	b.ne	104 <__truncXfYf2__+0xf0>  // b.any
  fc:	ubfx	w9, w9, #13, #1
 100:	add	w8, w8, w9
 104:	lsr	w9, w0, #16
 108:	and	w9, w9, #0x8000
 10c:	orr	w0, w8, w9
 110:	bl	138 <dstFromRep>
 114:	ldp	x29, x30, [sp], #16
 118:	ret

000000000000011c <__gnu_f2h_ieee>:
 11c:	stp	x29, x30, [sp, #-16]!
 120:	mov	x29, sp
 124:	bl	0 <__truncsfhf2>
 128:	ldp	x29, x30, [sp], #16
 12c:	ret

0000000000000130 <srcToRep>:
 130:	fmov	w0, s0
 134:	ret

0000000000000138 <dstFromRep>:
 138:	ret

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.cs	18 <__ucmpdi2+0x18>  // b.hs, b.nlast
  10:	mov	w0, wzr
  14:	ret
  18:	b.ls	24 <__ucmpdi2+0x24>  // b.plast
  1c:	mov	w0, #0x2                   	// #2
  20:	ret
  24:	cmp	w0, w1
  28:	b.cs	34 <__ucmpdi2+0x34>  // b.hs, b.nlast
  2c:	mov	w0, wzr
  30:	ret
  34:	mov	w8, #0x1                   	// #1
  38:	cinc	w0, w8, hi  // hi = pmore
  3c:	ret

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	cmp	x1, x3
   4:	b.cs	10 <__ucmpti2+0x10>  // b.hs, b.nlast
   8:	mov	w0, wzr
   c:	ret
  10:	b.ls	1c <__ucmpti2+0x1c>  // b.plast
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	x0, x2
  20:	b.cs	2c <__ucmpti2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x2, xzr
   c:	bl	0 <__udivmoddi4>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cbz	w8, 38 <__udivmoddi4+0x38>
   c:	cbz	w1, 50 <__udivmoddi4+0x50>
  10:	cbz	w9, 8c <__udivmoddi4+0x8c>
  14:	clz	w9, w9
  18:	clz	w10, w8
  1c:	sub	w12, w9, w10
  20:	cmp	w12, #0x20
  24:	b.cc	f0 <__udivmoddi4+0xf0>  // b.lo, b.ul, b.last
  28:	cbz	x2, 200 <__udivmoddi4+0x200>
  2c:	str	x0, [x2]
  30:	mov	x0, xzr
  34:	ret
  38:	cbz	w9, d8 <__udivmoddi4+0xd8>
  3c:	cbz	x2, 200 <__udivmoddi4+0x200>
  40:	and	x8, x0, #0xffffffff
  44:	mov	x0, xzr
  48:	str	x8, [x2]
  4c:	ret
  50:	cbz	w9, 12c <__udivmoddi4+0x12c>
  54:	cbz	w0, 1d0 <__udivmoddi4+0x1d0>
  58:	sub	w10, w9, #0x1
  5c:	tst	w9, w10
  60:	b.ne	1e8 <__udivmoddi4+0x1e8>  // b.any
  64:	cbz	x2, 7c <__udivmoddi4+0x7c>
  68:	mov	w10, #0xffffffff            	// #-1
  6c:	add	x10, x9, x10
  70:	and	w10, w10, w8
  74:	bfi	x0, x10, #32, #32
  78:	str	x0, [x2]
  7c:	rbit	w9, w9
  80:	clz	w9, w9
  84:	lsr	w0, w8, w9
  88:	ret
  8c:	sub	w9, w1, #0x1
  90:	tst	w1, w9
  94:	b.ne	13c <__udivmoddi4+0x13c>  // b.any
  98:	cbz	x2, ac <__udivmoddi4+0xac>
  9c:	mov	w9, #0xffffffff            	// #-1
  a0:	add	x9, x1, x9
  a4:	and	w9, w0, w9
  a8:	str	x9, [x2]
  ac:	cmp	w1, #0x1
  b0:	b.eq	1cc <__udivmoddi4+0x1cc>  // b.none
  b4:	rbit	w9, w1
  b8:	clz	w9, w9
  bc:	neg	w11, w9
  c0:	lsr	w10, w8, w9
  c4:	lsl	w8, w8, w11
  c8:	lsr	w9, w0, w9
  cc:	orr	w0, w8, w9
  d0:	bfi	x0, x10, #32, #32
  d4:	ret
  d8:	udiv	w8, w0, w1
  dc:	cbz	x2, e8 <__udivmoddi4+0xe8>
  e0:	msub	w9, w8, w1, w0
  e4:	str	x9, [x2]
  e8:	mov	x0, x8
  ec:	ret
  f0:	add	w10, w12, #0x1
  f4:	cmp	w10, #0x20
  f8:	b.eq	154 <__udivmoddi4+0x154>  // b.none
  fc:	mov	w13, #0x1f                  	// #31
 100:	sub	w12, w13, w12
 104:	lsr	w11, w8, w10
 108:	lsr	w14, w0, w10
 10c:	lsl	w13, w0, w12
 110:	lsl	w8, w8, w12
 114:	mov	w9, wzr
 118:	orr	w8, w8, w14
 11c:	mov	w0, w13
 120:	cbnz	w10, 160 <__udivmoddi4+0x160>
 124:	mov	x12, xzr
 128:	b	19c <__udivmoddi4+0x19c>
 12c:	mov	x0, xzr
 130:	cbz	x2, 1cc <__udivmoddi4+0x1cc>
 134:	str	xzr, [x2]
 138:	ret
 13c:	clz	w9, w1
 140:	clz	w10, w8
 144:	sub	w9, w9, w10
 148:	add	w10, w9, #0x21
 14c:	cmp	w10, #0x20
 150:	b.ne	208 <__udivmoddi4+0x208>  // b.any
 154:	mov	w9, wzr
 158:	mov	w11, wzr
 15c:	cbz	w10, 124 <__udivmoddi4+0x124>
 160:	mov	w12, wzr
 164:	extr	w11, w11, w8, #31
 168:	extr	w8, w8, w0, #31
 16c:	bfi	x8, x11, #32, #32
 170:	mvn	x11, x8
 174:	add	x11, x11, x1
 178:	asr	x11, x11, #63
 17c:	extr	w0, w0, w9, #31
 180:	orr	w9, w12, w9, lsl #1
 184:	and	w12, w11, #0x1
 188:	and	x11, x11, x1
 18c:	sub	x8, x8, x11
 190:	subs	w10, w10, #0x1
 194:	lsr	x11, x8, #32
 198:	b.ne	164 <__udivmoddi4+0x164>  // b.any
 19c:	mov	w10, w9
 1a0:	lsl	x10, x10, #1
 1a4:	lsl	x9, x0, #33
 1a8:	and	x13, x10, #0x100000000
 1ac:	cbz	x2, 1bc <__udivmoddi4+0x1bc>
 1b0:	mov	w8, w8
 1b4:	bfi	x8, x11, #32, #32
 1b8:	str	x8, [x2]
 1bc:	and	x8, x10, #0xfffffffe
 1c0:	orr	x9, x13, x9
 1c4:	orr	x8, x9, x8
 1c8:	orr	x0, x8, x12
 1cc:	ret
 1d0:	udiv	w0, w8, w9
 1d4:	cbz	x2, 1cc <__udivmoddi4+0x1cc>
 1d8:	msub	w8, w0, w9, w8
 1dc:	lsl	x8, x8, #32
 1e0:	str	x8, [x2]
 1e4:	ret
 1e8:	clz	w9, w9
 1ec:	clz	w10, w8
 1f0:	sub	w11, w9, w10
 1f4:	cmp	w11, #0x1f
 1f8:	b.cc	21c <__udivmoddi4+0x21c>  // b.lo, b.ul, b.last
 1fc:	cbnz	x2, 2c <__udivmoddi4+0x2c>
 200:	mov	x0, xzr
 204:	ret
 208:	cmp	w10, #0x1f
 20c:	b.hi	24c <__udivmoddi4+0x24c>  // b.pmore
 210:	mov	w9, wzr
 214:	neg	w12, w10
 218:	b	22c <__udivmoddi4+0x22c>
 21c:	mov	w12, #0x1f                  	// #31
 220:	mov	w9, wzr
 224:	add	w10, w11, #0x1
 228:	sub	w12, w12, w11
 22c:	lsr	w11, w8, w10
 230:	lsr	w13, w0, w10
 234:	lsl	w14, w0, w12
 238:	lsl	w8, w8, w12
 23c:	orr	w8, w8, w13
 240:	mov	w0, w14
 244:	cbnz	w10, 160 <__udivmoddi4+0x160>
 248:	b	124 <__udivmoddi4+0x124>
 24c:	neg	w12, w10
 250:	lsr	w13, w0, w10
 254:	lsl	w9, w0, w12
 258:	lsl	w12, w8, w12
 25c:	mov	w11, wzr
 260:	orr	w0, w12, w13
 264:	lsr	w8, w8, w10
 268:	cbnz	w10, 160 <__udivmoddi4+0x160>
 26c:	b	124 <__udivmoddi4+0x124>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__udivsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	cbz	x1, 2c <__udivmodti4+0x2c>
   4:	cbz	x2, 44 <__udivmodti4+0x44>
   8:	cbz	x3, 78 <__udivmodti4+0x78>
   c:	clz	x8, x3
  10:	clz	x9, x1
  14:	sub	w11, w8, w9
  18:	cmp	w11, #0x40
  1c:	b.cc	d4 <__udivmodti4+0xd4>  // b.lo, b.ul, b.last
  20:	cbz	x4, 178 <__udivmodti4+0x178>
  24:	stp	x0, x1, [x4]
  28:	b	178 <__udivmodti4+0x178>
  2c:	cbz	x3, b8 <__udivmodti4+0xb8>
  30:	cbz	x4, 178 <__udivmodti4+0x178>
  34:	mov	x1, xzr
  38:	stp	x0, xzr, [x4]
  3c:	mov	x0, xzr
  40:	ret
  44:	cbz	x3, 108 <__udivmodti4+0x108>
  48:	cbz	x0, 148 <__udivmodti4+0x148>
  4c:	sub	x8, x3, #0x1
  50:	tst	x3, x8
  54:	b.ne	160 <__udivmodti4+0x160>  // b.any
  58:	cbz	x4, 64 <__udivmodti4+0x64>
  5c:	and	x8, x8, x1
  60:	stp	x0, x8, [x4]
  64:	rbit	x8, x3
  68:	clz	x8, x8
  6c:	lsr	x0, x1, x8
  70:	mov	x1, xzr
  74:	ret
  78:	sub	x8, x2, #0x1
  7c:	tst	x2, x8
  80:	b.ne	11c <__udivmodti4+0x11c>  // b.any
  84:	cbz	x4, 90 <__udivmodti4+0x90>
  88:	and	x8, x8, x0
  8c:	stp	x8, xzr, [x4]
  90:	cmp	x2, #0x1
  94:	b.eq	258 <__udivmodti4+0x258>  // b.none
  98:	rbit	x8, x2
  9c:	clz	x8, x8
  a0:	neg	x9, x8
  a4:	lsl	x9, x1, x9
  a8:	lsr	x1, x1, x8
  ac:	lsr	x8, x0, x8
  b0:	orr	x0, x9, x8
  b4:	ret
  b8:	udiv	x8, x0, x2
  bc:	cbz	x4, c8 <__udivmodti4+0xc8>
  c0:	msub	x9, x8, x2, x0
  c4:	stp	x9, xzr, [x4]
  c8:	mov	x1, xzr
  cc:	mov	x0, x8
  d0:	ret
  d4:	add	w9, w11, #0x1
  d8:	cmp	w9, #0x40
  dc:	b.eq	134 <__udivmodti4+0x134>  // b.none
  e0:	mov	w12, #0x3f                  	// #63
  e4:	sub	w11, w12, w11
  e8:	lsr	x13, x0, x9
  ec:	lsl	x12, x1, x11
  f0:	mov	x8, xzr
  f4:	lsr	x10, x1, x9
  f8:	orr	x1, x12, x13
  fc:	lsl	x0, x0, x11
 100:	cbnz	w9, 1c0 <__udivmodti4+0x1c0>
 104:	b	22c <__udivmodti4+0x22c>
 108:	cbz	x4, 178 <__udivmodti4+0x178>
 10c:	mov	x0, xzr
 110:	mov	x1, xzr
 114:	stp	xzr, xzr, [x4]
 118:	ret
 11c:	clz	x8, x2
 120:	clz	x9, x1
 124:	sub	w8, w8, w9
 128:	add	w9, w8, #0x41
 12c:	cmp	w9, #0x40
 130:	b.ne	184 <__udivmodti4+0x184>  // b.any
 134:	mov	x8, xzr
 138:	mov	x10, xzr
 13c:	mov	w9, #0x40                  	// #64
 140:	cbnz	w9, 1c0 <__udivmodti4+0x1c0>
 144:	b	22c <__udivmodti4+0x22c>
 148:	udiv	x0, x1, x3
 14c:	cbz	x4, 158 <__udivmodti4+0x158>
 150:	msub	x8, x0, x3, x1
 154:	stp	xzr, x8, [x4]
 158:	mov	x1, xzr
 15c:	ret
 160:	clz	x8, x3
 164:	clz	x9, x1
 168:	sub	w10, w8, w9
 16c:	cmp	w10, #0x3f
 170:	b.cc	198 <__udivmodti4+0x198>  // b.lo, b.ul, b.last
 174:	cbnz	x4, 24 <__udivmodti4+0x24>
 178:	mov	x0, xzr
 17c:	mov	x1, xzr
 180:	ret
 184:	cmp	w9, #0x3f
 188:	b.hi	20c <__udivmodti4+0x20c>  // b.pmore
 18c:	mov	x8, xzr
 190:	neg	w11, w9
 194:	b	1a8 <__udivmodti4+0x1a8>
 198:	mov	w11, #0x3f                  	// #63
 19c:	mov	x8, xzr
 1a0:	add	w9, w10, #0x1
 1a4:	sub	w11, w11, w10
 1a8:	lsr	x12, x0, x9
 1ac:	lsl	x0, x0, x11
 1b0:	lsl	x11, x1, x11
 1b4:	lsr	x10, x1, x9
 1b8:	orr	x1, x11, x12
 1bc:	cbz	w9, 22c <__udivmodti4+0x22c>
 1c0:	mov	w11, wzr
 1c4:	extr	x12, x1, x0, #63
 1c8:	extr	x10, x10, x1, #63
 1cc:	mov	w11, w11
 1d0:	mvn	x13, x12
 1d4:	extr	x0, x0, x8, #63
 1d8:	orr	x8, x11, x8, lsl #1
 1dc:	mvn	x11, x10
 1e0:	cmn	x13, x2
 1e4:	adcs	x11, x11, x3
 1e8:	asr	x11, x11, #63
 1ec:	and	x14, x11, x2
 1f0:	and	x13, x11, x3
 1f4:	subs	x1, x12, x14
 1f8:	sbcs	x10, x10, x13
 1fc:	subs	w9, w9, #0x1
 200:	and	w11, w11, #0x1
 204:	b.ne	1c4 <__udivmodti4+0x1c4>  // b.any
 208:	b	230 <__udivmodti4+0x230>
 20c:	neg	w11, w9
 210:	lsr	x12, x0, x9
 214:	lsl	x8, x0, x11
 218:	lsl	x11, x1, x11
 21c:	mov	x10, xzr
 220:	orr	x0, x11, x12
 224:	lsr	x1, x1, x9
 228:	cbnz	w9, 1c0 <__udivmodti4+0x1c0>
 22c:	mov	x11, xzr
 230:	mov	x9, xzr
 234:	lsl	x12, x0, #1
 238:	lsr	x13, x8, #63
 23c:	lsl	x8, x8, #1
 240:	cbz	x4, 248 <__udivmodti4+0x248>
 244:	stp	x1, x10, [x4]
 248:	and	x8, x8, #0xfffffffffffffffe
 24c:	orr	x10, x13, x12
 250:	orr	x0, x8, x11
 254:	orr	x1, x10, x9
 258:	ret

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	mov	w8, w0
   4:	mov	w0, wzr
   8:	cbz	w8, 88 <__udivsi3+0x88>
   c:	cbz	w1, 88 <__udivsi3+0x88>
  10:	clz	w11, w1
  14:	clz	w12, w8
  18:	sub	w9, w11, w12
  1c:	cmp	w9, #0x1f
  20:	b.ls	2c <__udivsi3+0x2c>  // b.plast
  24:	mov	w0, wzr
  28:	ret
  2c:	b.ne	38 <__udivsi3+0x38>  // b.any
  30:	mov	w0, w8
  34:	ret
  38:	mov	w13, #0x1f                  	// #31
  3c:	adds	w10, w9, #0x1
  40:	sub	w9, w13, w9
  44:	lsl	w9, w8, w9
  48:	mov	w0, wzr
  4c:	b.cs	84 <__udivsi3+0x84>  // b.hs, b.nlast
  50:	lsr	w10, w8, w10
  54:	mvn	w8, w11
  58:	add	w8, w8, w12
  5c:	extr	w10, w10, w9, #31
  60:	mvn	w11, w10
  64:	add	w11, w11, w1
  68:	asr	w11, w11, #31
  6c:	orr	w9, w0, w9, lsl #1
  70:	and	w0, w11, #0x1
  74:	and	w11, w11, w1
  78:	adds	w8, w8, #0x1
  7c:	sub	w10, w10, w11
  80:	b.cc	5c <__udivsi3+0x5c>  // b.lo, b.ul, b.last
  84:	bfi	w0, w9, #1, #31
  88:	ret

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x4, xzr
   c:	bl	0 <__udivmodti4>
  10:	ldp	x29, x30, [sp], #16
  14:	ret

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	add	x2, sp, #0x8
  10:	bl	0 <__udivmoddi4>
  14:	ldr	x0, [sp, #8]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	mov	x4, sp
  10:	bl	0 <__udivmodti4>
  14:	ldp	x0, x1, [sp]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__emutls_get_address>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	x19, x0
  10:	bl	44 <emutls_get_index>
  14:	mov	x20, x0
  18:	bl	9c <emutls_get_address_array>
  1c:	add	x20, x0, x20, lsl #3
  20:	ldr	x8, [x20, #8]!
  24:	cbnz	x8, 34 <__emutls_get_address+0x34>
  28:	mov	x0, x19
  2c:	bl	158 <emutls_allocate_object>
  30:	str	x0, [x20]
  34:	ldr	x0, [x20]
  38:	ldp	x20, x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

0000000000000044 <emutls_get_index>:
  44:	stp	x29, x30, [sp, #-32]!
  48:	stp	x20, x19, [sp, #16]
  4c:	mov	x29, sp
  50:	add	x8, x0, #0x10
  54:	ldar	x20, [x8]
  58:	cbnz	x20, 8c <emutls_get_index+0x48>
  5c:	mov	x19, x0
  60:	bl	1c0 <emutls_init_once>
  64:	bl	2f0 <emutls_lock>
  68:	ldr	x20, [x19, #16]
  6c:	cbnz	x20, 88 <emutls_get_index+0x44>
  70:	adrp	x8, 0 <__emutls_get_address>
  74:	ldr	x9, [x8]
  78:	add	x10, x19, #0x10
  7c:	add	x20, x9, #0x1
  80:	str	x20, [x8]
  84:	stlr	x20, [x10]
  88:	bl	30c <emutls_unlock>
  8c:	mov	x0, x20
  90:	ldp	x20, x19, [sp, #16]
  94:	ldp	x29, x30, [sp], #32
  98:	ret

000000000000009c <emutls_get_address_array>:
  9c:	stp	x29, x30, [sp, #-48]!
  a0:	str	x21, [sp, #16]
  a4:	stp	x20, x19, [sp, #32]
  a8:	mov	x29, sp
  ac:	mov	x20, x0
  b0:	bl	364 <emutls_getspecific>
  b4:	cbz	x0, 108 <emutls_get_address_array+0x6c>
  b8:	ldr	x21, [x0, #8]
  bc:	mov	x19, x0
  c0:	cmp	x21, x20
  c4:	b.cs	144 <emutls_get_address_array+0xa8>  // b.hs, b.nlast
  c8:	mov	x0, x20
  cc:	bl	328 <emutls_new_data_array_size>
  d0:	mov	x20, x0
  d4:	bl	338 <emutls_asize>
  d8:	mov	x1, x0
  dc:	mov	x0, x19
  e0:	bl	0 <realloc>
  e4:	mov	x19, x0
  e8:	cbz	x0, 138 <emutls_get_address_array+0x9c>
  ec:	add	x8, x19, x21, lsl #3
  f0:	sub	x9, x20, x21
  f4:	add	x0, x8, #0x10
  f8:	lsl	x2, x9, #3
  fc:	mov	w1, wzr
 100:	bl	0 <memset>
 104:	b	138 <emutls_get_address_array+0x9c>
 108:	mov	x0, x20
 10c:	bl	328 <emutls_new_data_array_size>
 110:	mov	x20, x0
 114:	bl	338 <emutls_asize>
 118:	bl	0 <malloc>
 11c:	mov	x19, x0
 120:	cbz	x0, 138 <emutls_get_address_array+0x9c>
 124:	add	x0, x19, #0x10
 128:	lsl	x2, x20, #3
 12c:	mov	w1, wzr
 130:	bl	0 <memset>
 134:	str	xzr, [x19]
 138:	mov	x0, x19
 13c:	mov	x1, x20
 140:	bl	344 <emutls_check_array_set_size>
 144:	mov	x0, x19
 148:	ldp	x20, x19, [sp, #32]
 14c:	ldr	x21, [sp, #16]
 150:	ldp	x29, x30, [sp], #48
 154:	ret

0000000000000158 <emutls_allocate_object>:
 158:	stp	x29, x30, [sp, #-32]!
 15c:	stp	x20, x19, [sp, #16]
 160:	mov	x29, sp
 164:	ldr	x8, [x0, #8]
 168:	mov	w9, #0x8                   	// #8
 16c:	mov	x20, x0
 170:	cmp	x8, #0x8
 174:	csel	x0, x8, x9, hi  // hi = pmore
 178:	sub	x8, x0, #0x1
 17c:	tst	x0, x8
 180:	b.ne	1bc <emutls_allocate_object+0x64>  // b.any
 184:	ldr	x19, [x20]
 188:	mov	x1, x19
 18c:	bl	380 <emutls_memalign_alloc>
 190:	ldr	x1, [x20, #24]
 194:	mov	x20, x0
 198:	mov	x2, x19
 19c:	cbz	x1, 1a8 <emutls_allocate_object+0x50>
 1a0:	bl	0 <memcpy>
 1a4:	b	1ac <emutls_allocate_object+0x54>
 1a8:	bl	0 <memset>
 1ac:	mov	x0, x20
 1b0:	ldp	x20, x19, [sp, #16]
 1b4:	ldp	x29, x30, [sp], #32
 1b8:	ret
 1bc:	bl	0 <abort>

00000000000001c0 <emutls_init_once>:
 1c0:	stp	x29, x30, [sp, #-16]!
 1c4:	mov	x29, sp
 1c8:	adrp	x0, 0 <__emutls_get_address>
 1cc:	adrp	x1, 0 <__emutls_get_address>
 1d0:	add	x0, x0, #0x0
 1d4:	add	x1, x1, #0x0
 1d8:	bl	0 <pthread_once>
 1dc:	ldp	x29, x30, [sp], #16
 1e0:	ret

00000000000001e4 <emutls_init>:
 1e4:	stp	x29, x30, [sp, #-16]!
 1e8:	mov	x29, sp
 1ec:	adrp	x0, 0 <__emutls_get_address>
 1f0:	adrp	x1, 0 <__emutls_get_address>
 1f4:	add	x0, x0, #0x0
 1f8:	add	x1, x1, #0x0
 1fc:	bl	0 <pthread_key_create>
 200:	cbnz	w0, 20c <emutls_init+0x28>
 204:	ldp	x29, x30, [sp], #16
 208:	ret
 20c:	bl	0 <abort>

0000000000000210 <emutls_key_destructor>:
 210:	stp	x29, x30, [sp, #-32]!
 214:	str	x19, [sp, #16]
 218:	mov	x29, sp
 21c:	ldr	x8, [x0]
 220:	mov	x19, x0
 224:	cbz	x8, 23c <emutls_key_destructor+0x2c>
 228:	sub	x8, x8, #0x1
 22c:	mov	x0, x19
 230:	str	x8, [x19]
 234:	bl	258 <emutls_setspecific>
 238:	b	24c <emutls_key_destructor+0x3c>
 23c:	mov	x0, x19
 240:	bl	27c <emutls_shutdown>
 244:	mov	x0, x19
 248:	bl	0 <free>
 24c:	ldr	x19, [sp, #16]
 250:	ldp	x29, x30, [sp], #32
 254:	ret

0000000000000258 <emutls_setspecific>:
 258:	stp	x29, x30, [sp, #-16]!
 25c:	mov	x29, sp
 260:	adrp	x8, 0 <__emutls_get_address>
 264:	ldr	w8, [x8]
 268:	mov	x1, x0
 26c:	mov	w0, w8
 270:	bl	0 <pthread_setspecific>
 274:	ldp	x29, x30, [sp], #16
 278:	ret

000000000000027c <emutls_shutdown>:
 27c:	stp	x29, x30, [sp, #-48]!
 280:	str	x21, [sp, #16]
 284:	stp	x20, x19, [sp, #32]
 288:	mov	x29, sp
 28c:	cbz	x0, 2c8 <emutls_shutdown+0x4c>
 290:	ldr	x8, [x0, #8]
 294:	mov	x19, x0
 298:	cbz	x8, 2c8 <emutls_shutdown+0x4c>
 29c:	mov	x20, xzr
 2a0:	add	x21, x19, #0x10
 2a4:	b	2b8 <emutls_shutdown+0x3c>
 2a8:	ldr	x8, [x19, #8]
 2ac:	add	x20, x20, #0x1
 2b0:	cmp	x20, x8
 2b4:	b.cs	2c8 <emutls_shutdown+0x4c>  // b.hs, b.nlast
 2b8:	ldr	x0, [x21, x20, lsl #3]
 2bc:	cbz	x0, 2a8 <emutls_shutdown+0x2c>
 2c0:	bl	2d8 <emutls_memalign_free>
 2c4:	b	2a8 <emutls_shutdown+0x2c>
 2c8:	ldp	x20, x19, [sp, #32]
 2cc:	ldr	x21, [sp, #16]
 2d0:	ldp	x29, x30, [sp], #48
 2d4:	ret

00000000000002d8 <emutls_memalign_free>:
 2d8:	stp	x29, x30, [sp, #-16]!
 2dc:	mov	x29, sp
 2e0:	ldur	x0, [x0, #-8]
 2e4:	bl	0 <free>
 2e8:	ldp	x29, x30, [sp], #16
 2ec:	ret

00000000000002f0 <emutls_lock>:
 2f0:	stp	x29, x30, [sp, #-16]!
 2f4:	mov	x29, sp
 2f8:	adrp	x0, 0 <__emutls_get_address>
 2fc:	add	x0, x0, #0x0
 300:	bl	0 <pthread_mutex_lock>
 304:	ldp	x29, x30, [sp], #16
 308:	ret

000000000000030c <emutls_unlock>:
 30c:	stp	x29, x30, [sp, #-16]!
 310:	mov	x29, sp
 314:	adrp	x0, 0 <__emutls_get_address>
 318:	add	x0, x0, #0x0
 31c:	bl	0 <pthread_mutex_unlock>
 320:	ldp	x29, x30, [sp], #16
 324:	ret

0000000000000328 <emutls_new_data_array_size>:
 328:	add	x8, x0, #0x11
 32c:	and	x8, x8, #0xfffffffffffffff0
 330:	sub	x0, x8, #0x2
 334:	ret

0000000000000338 <emutls_asize>:
 338:	lsl	x8, x0, #3
 33c:	add	x0, x8, #0x10
 340:	ret

0000000000000344 <emutls_check_array_set_size>:
 344:	stp	x29, x30, [sp, #-16]!
 348:	mov	x29, sp
 34c:	cbz	x0, 360 <emutls_check_array_set_size+0x1c>
 350:	str	x1, [x0, #8]
 354:	bl	258 <emutls_setspecific>
 358:	ldp	x29, x30, [sp], #16
 35c:	ret
 360:	bl	0 <abort>

0000000000000364 <emutls_getspecific>:
 364:	stp	x29, x30, [sp, #-16]!
 368:	mov	x29, sp
 36c:	adrp	x8, 0 <__emutls_get_address>
 370:	ldr	w0, [x8]
 374:	bl	0 <pthread_getspecific>
 378:	ldp	x29, x30, [sp], #16
 37c:	ret

0000000000000380 <emutls_memalign_alloc>:
 380:	stp	x29, x30, [sp, #-32]!
 384:	stp	x20, x19, [sp, #16]
 388:	mov	x29, sp
 38c:	add	x20, x0, #0x7
 390:	mov	x19, x0
 394:	add	x0, x20, x1
 398:	bl	0 <malloc>
 39c:	cbz	x0, 3c0 <emutls_memalign_alloc+0x40>
 3a0:	add	x8, x0, x20
 3a4:	neg	x9, x19
 3a8:	and	x8, x8, x9
 3ac:	stur	x0, [x8, #-8]
 3b0:	ldp	x20, x19, [sp, #16]
 3b4:	mov	x0, x8
 3b8:	ldp	x29, x30, [sp], #32
 3bc:	ret
 3c0:	bl	0 <abort>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	add	x9, x19, x0
  1c:	neg	x8, x0
  20:	add	x9, x9, #0x30
  24:	and	x0, x8, x19
  28:	and	x8, x9, x8
  2c:	sub	x1, x8, x0
  30:	mov	w2, #0x7                   	// #7
  34:	bl	0 <mprotect>
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	adrp	x19, 0 <stderr>
  10:	ldr	x19, [x19]
  14:	mov	x4, x3
  18:	mov	x3, x2
  1c:	mov	x2, x1
  20:	ldr	x8, [x19]
  24:	mov	x1, x0
  28:	mov	x0, x8
  2c:	bl	0 <fprintf>
  30:	ldr	x0, [x19]
  34:	bl	0 <fflush>
  38:	adrp	x0, 0 <__eprintf>
  3c:	adrp	x2, 0 <__eprintf>
  40:	add	x0, x0, #0x0
  44:	add	x2, x2, #0x0
  48:	mov	w1, #0x1a                  	// #26
  4c:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_personality_v0>:
   0:	tbnz	w1, #0, 174 <__gcc_personality_v0+0x174>
   4:	sub	sp, sp, #0x70
   8:	stp	x29, x30, [sp, #16]
   c:	stp	x28, x27, [sp, #32]
  10:	stp	x26, x25, [sp, #48]
  14:	stp	x24, x23, [sp, #64]
  18:	stp	x22, x21, [sp, #80]
  1c:	stp	x20, x19, [sp, #96]
  20:	add	x29, sp, #0x10
  24:	mov	x0, x4
  28:	mov	x19, x4
  2c:	mov	x20, x3
  30:	bl	0 <_Unwind_GetLanguageSpecificData>
  34:	str	x0, [sp, #8]
  38:	cbz	x0, 16c <__gcc_personality_v0+0x16c>
  3c:	mov	x21, x0
  40:	mov	x0, x19
  44:	bl	0 <_Unwind_GetIP>
  48:	mov	x23, x0
  4c:	mov	x0, x19
  50:	bl	0 <_Unwind_GetRegionStart>
  54:	add	x8, x21, #0x1
  58:	str	x8, [sp, #8]
  5c:	ldrb	w1, [x21]
  60:	mov	x21, x0
  64:	cmp	w1, #0xff
  68:	b.eq	74 <__gcc_personality_v0+0x74>  // b.none
  6c:	add	x0, sp, #0x8
  70:	bl	1a0 <readEncodedPointer>
  74:	ldr	x8, [sp, #8]
  78:	add	x9, x8, #0x1
  7c:	str	x9, [sp, #8]
  80:	ldrb	w8, [x8]
  84:	cmp	w8, #0xff
  88:	b.eq	94 <__gcc_personality_v0+0x94>  // b.none
  8c:	add	x0, sp, #0x8
  90:	bl	298 <readULEB128>
  94:	ldr	x8, [sp, #8]
  98:	add	x0, sp, #0x8
  9c:	add	x9, x8, #0x1
  a0:	str	x9, [sp, #8]
  a4:	ldrb	w22, [x8]
  a8:	bl	298 <readULEB128>
  ac:	ldr	x8, [sp, #8]
  b0:	ands	x9, x0, #0xffffffff
  b4:	str	x8, [sp]
  b8:	b.eq	16c <__gcc_personality_v0+0x16c>  // b.none
  bc:	mvn	x10, x21
  c0:	add	x26, x23, x10
  c4:	add	x27, x8, x9
  c8:	mov	x0, sp
  cc:	mov	w1, w22
  d0:	bl	1a0 <readEncodedPointer>
  d4:	mov	x24, x0
  d8:	mov	x0, sp
  dc:	mov	w1, w22
  e0:	bl	1a0 <readEncodedPointer>
  e4:	mov	x25, x0
  e8:	mov	x0, sp
  ec:	mov	w1, w22
  f0:	bl	1a0 <readEncodedPointer>
  f4:	mov	x23, x0
  f8:	mov	x0, sp
  fc:	bl	298 <readULEB128>
 100:	cbz	x23, 150 <__gcc_personality_v0+0x150>
 104:	cmp	x24, x26
 108:	mov	w28, wzr
 10c:	b.hi	154 <__gcc_personality_v0+0x154>  // b.pmore
 110:	add	x8, x25, x24
 114:	cmp	x26, x8
 118:	b.cs	154 <__gcc_personality_v0+0x154>  // b.hs, b.nlast
 11c:	mov	x0, x19
 120:	mov	w1, wzr
 124:	mov	x2, x20
 128:	bl	0 <_Unwind_SetGR>
 12c:	mov	w1, #0x1                   	// #1
 130:	mov	x0, x19
 134:	mov	x2, xzr
 138:	mov	w28, #0x1                   	// #1
 13c:	bl	0 <_Unwind_SetGR>
 140:	add	x1, x23, x21
 144:	mov	x0, x19
 148:	bl	0 <_Unwind_SetIP>
 14c:	b	154 <__gcc_personality_v0+0x154>
 150:	mov	w28, #0x2                   	// #2
 154:	orr	w8, w28, #0x2
 158:	cmp	w8, #0x2
 15c:	b.ne	17c <__gcc_personality_v0+0x17c>  // b.any
 160:	ldr	x8, [sp]
 164:	cmp	x8, x27
 168:	b.cc	c8 <__gcc_personality_v0+0xc8>  // b.lo, b.ul, b.last
 16c:	mov	w0, #0x8                   	// #8
 170:	b	180 <__gcc_personality_v0+0x180>
 174:	mov	w0, #0x8                   	// #8
 178:	ret
 17c:	mov	w0, #0x7                   	// #7
 180:	ldp	x20, x19, [sp, #96]
 184:	ldp	x22, x21, [sp, #80]
 188:	ldp	x24, x23, [sp, #64]
 18c:	ldp	x26, x25, [sp, #48]
 190:	ldp	x28, x27, [sp, #32]
 194:	ldp	x29, x30, [sp, #16]
 198:	add	sp, sp, #0x70
 19c:	ret

00000000000001a0 <readEncodedPointer>:
 1a0:	sub	sp, sp, #0x30
 1a4:	stp	x29, x30, [sp, #16]
 1a8:	stp	x20, x19, [sp, #32]
 1ac:	add	x29, sp, #0x10
 1b0:	ldr	x8, [x0]
 1b4:	and	w20, w1, #0xff
 1b8:	cmp	w20, #0xff
 1bc:	str	x8, [sp, #8]
 1c0:	b.eq	1f4 <readEncodedPointer+0x54>  // b.none
 1c4:	and	w9, w20, #0xf
 1c8:	cmp	w9, #0xc
 1cc:	b.hi	268 <readEncodedPointer+0xc8>  // b.pmore
 1d0:	adrp	x10, 0 <__gcc_personality_v0>
 1d4:	add	x10, x10, #0x0
 1d8:	adr	x11, 1ec <readEncodedPointer+0x4c>
 1dc:	ldrb	w12, [x10, x9]
 1e0:	add	x11, x11, x12, lsl #2
 1e4:	mov	x19, x0
 1e8:	br	x11
 1ec:	ldr	x0, [x8], #8
 1f0:	b	22c <readEncodedPointer+0x8c>
 1f4:	mov	x0, xzr
 1f8:	b	258 <readEncodedPointer+0xb8>
 1fc:	add	x0, sp, #0x8
 200:	bl	298 <readULEB128>
 204:	ubfx	w8, w20, #4, #3
 208:	cbnz	w8, 238 <readEncodedPointer+0x98>
 20c:	b	248 <readEncodedPointer+0xa8>
 210:	ldrsw	x0, [x8], #4
 214:	b	22c <readEncodedPointer+0x8c>
 218:	ldr	w0, [x8], #4
 21c:	b	22c <readEncodedPointer+0x8c>
 220:	ldrsh	x0, [x8], #2
 224:	b	22c <readEncodedPointer+0x8c>
 228:	ldrh	w0, [x8], #2
 22c:	str	x8, [sp, #8]
 230:	ubfx	w8, w20, #4, #3
 234:	cbz	w8, 248 <readEncodedPointer+0xa8>
 238:	cmp	w8, #0x1
 23c:	b.ne	280 <readEncodedPointer+0xe0>  // b.any
 240:	ldr	x8, [x19]
 244:	add	x0, x8, x0
 248:	tbz	w20, #7, 250 <readEncodedPointer+0xb0>
 24c:	ldr	x0, [x0]
 250:	ldr	x8, [sp, #8]
 254:	str	x8, [x19]
 258:	ldp	x20, x19, [sp, #32]
 25c:	ldp	x29, x30, [sp, #16]
 260:	add	sp, sp, #0x30
 264:	ret
 268:	adrp	x0, 0 <__gcc_personality_v0>
 26c:	adrp	x2, 0 <__gcc_personality_v0>
 270:	add	x0, x0, #0x0
 274:	add	x2, x2, #0x0
 278:	mov	w1, #0x68                  	// #104
 27c:	bl	0 <__compilerrt_abort_impl>
 280:	adrp	x0, 0 <__gcc_personality_v0>
 284:	adrp	x2, 0 <__gcc_personality_v0>
 288:	add	x0, x0, #0x0
 28c:	add	x2, x2, #0x0
 290:	mov	w1, #0x7a                  	// #122
 294:	bl	0 <__compilerrt_abort_impl>

0000000000000298 <readULEB128>:
 298:	ldr	x9, [x0]
 29c:	mov	w10, wzr
 2a0:	mov	x8, xzr
 2a4:	ldrb	w11, [x9], #1
 2a8:	and	w12, w11, #0x7f
 2ac:	lsl	w12, w12, w10
 2b0:	sxtw	x12, w12
 2b4:	orr	x8, x8, x12
 2b8:	add	w10, w10, #0x7
 2bc:	tbnz	w11, #7, 2a4 <readULEB128+0xc>
 2c0:	str	x9, [x0]
 2c4:	mov	x0, x8
 2c8:	ret

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	adrp	x8, 0 <__clear_cache>
   4:	ldr	x9, [x8]
   8:	cbz	x9, 18 <__clear_cache+0x18>
   c:	ldr	x9, [x8]
  10:	tbz	w9, #28, 28 <__clear_cache+0x28>
  14:	b	54 <__clear_cache+0x54>
  18:	mrs	x9, ctr_el0
  1c:	str	x9, [x8]
  20:	ldr	x9, [x8]
  24:	tbnz	w9, #28, 54 <__clear_cache+0x54>
  28:	ubfx	w9, w9, #16, #4
  2c:	mov	w10, #0x4                   	// #4
  30:	lsl	w9, w10, w9
  34:	neg	x10, x9
  38:	and	x10, x10, x0
  3c:	cmp	x10, x1
  40:	b.cs	54 <__clear_cache+0x54>  // b.hs, b.nlast
  44:	dc	cvau, x10
  48:	add	x10, x10, x9
  4c:	cmp	x10, x1
  50:	b.cc	44 <__clear_cache+0x44>  // b.lo, b.ul, b.last
  54:	dsb	ish
  58:	ldr	x8, [x8]
  5c:	tbnz	w8, #29, 8c <__clear_cache+0x8c>
  60:	and	w8, w8, #0xf
  64:	mov	w9, #0x4                   	// #4
  68:	lsl	w8, w9, w8
  6c:	neg	x9, x8
  70:	and	x9, x9, x0
  74:	cmp	x9, x1
  78:	b.cs	8c <__clear_cache+0x8c>  // b.hs, b.nlast
  7c:	ic	ivau, x9
  80:	add	x9, x9, x8
  84:	cmp	x9, x1
  88:	b.cc	7c <__clear_cache+0x7c>  // b.lo, b.ul, b.last
  8c:	isb
  90:	ret

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x8, fpcr
   4:	ubfx	x8, x8, #22, #2
   8:	sub	x8, x8, #0x1
   c:	cmp	x8, #0x2
  10:	b.hi	24 <__fe_getround+0x24>  // b.pmore
  14:	adrp	x9, 0 <__fe_getround>
  18:	add	x9, x9, #0x0
  1c:	ldr	w0, [x9, x8, lsl #2]
  20:	ret
  24:	mov	w0, wzr
  28:	ret

000000000000002c <__fe_raise_inexact>:
  2c:	mrs	x8, fpsr
  30:	mov	w0, wzr
  34:	orr	x8, x8, #0x10
  38:	msr	fpsr, x8
  3c:	ret
