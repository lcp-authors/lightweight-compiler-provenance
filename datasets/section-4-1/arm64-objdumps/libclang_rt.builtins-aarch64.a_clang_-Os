In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_clang_-Os:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	stp	q1, q0, [sp, #-32]!
   4:	ldp	x9, x8, [sp, #16]
   8:	mov	x13, #0x7fff000000000000    	// #9223090561878065152
   c:	mov	w0, #0x1                   	// #1
  10:	and	x12, x8, #0x7fffffffffffffff
  14:	cmp	x9, #0x0
  18:	cset	w10, ne  // ne = any
  1c:	cmp	x12, x13
  20:	cset	w11, hi  // hi = pmore
  24:	csel	w10, w10, w11, eq  // eq = none
  28:	tbnz	w10, #0, b4 <__cmptf2+0xb4>
  2c:	ldp	x11, x10, [sp]
  30:	and	x14, x10, #0x7fffffffffffffff
  34:	cmp	x11, #0x0
  38:	cset	w15, ne  // ne = any
  3c:	cmp	x14, x13
  40:	cset	w13, hi  // hi = pmore
  44:	csel	w13, w15, w13, eq  // eq = none
  48:	tbnz	w13, #0, b4 <__cmptf2+0xb4>
  4c:	orr	x13, x11, x9
  50:	orr	x12, x14, x12
  54:	orr	x12, x13, x12
  58:	cbz	x12, 78 <__cmptf2+0x78>
  5c:	tst	x10, x8
  60:	b.lt	80 <__cmptf2+0x80>  // b.tstop
  64:	cmp	x9, x11
  68:	cset	w12, cc  // cc = lo, ul, last
  6c:	cmp	x8, x10
  70:	cset	w13, lt  // lt = tstop
  74:	b	90 <__cmptf2+0x90>
  78:	mov	w0, wzr
  7c:	b	b4 <__cmptf2+0xb4>
  80:	cmp	x9, x11
  84:	cset	w12, hi  // hi = pmore
  88:	cmp	x8, x10
  8c:	cset	w13, gt
  90:	csel	w12, w12, w13, eq  // eq = none
  94:	tbz	w12, #0, a0 <__cmptf2+0xa0>
  98:	mov	w0, #0xffffffff            	// #-1
  9c:	b	b4 <__cmptf2+0xb4>
  a0:	eor	x9, x9, x11
  a4:	eor	x8, x8, x10
  a8:	orr	x8, x9, x8
  ac:	cmp	x8, #0x0
  b0:	cset	w0, ne  // ne = any
  b4:	add	sp, sp, #0x20
  b8:	ret

00000000000000bc <__getf2>:
  bc:	stp	q1, q0, [sp, #-32]!
  c0:	ldp	x9, x8, [sp, #16]
  c4:	mov	x13, #0x7fff000000000000    	// #9223090561878065152
  c8:	mov	w0, #0xffffffff            	// #-1
  cc:	and	x12, x8, #0x7fffffffffffffff
  d0:	cmp	x9, #0x0
  d4:	cset	w10, ne  // ne = any
  d8:	cmp	x12, x13
  dc:	cset	w11, hi  // hi = pmore
  e0:	csel	w10, w10, w11, eq  // eq = none
  e4:	tbnz	w10, #0, 170 <__getf2+0xb4>
  e8:	ldp	x11, x10, [sp]
  ec:	and	x14, x10, #0x7fffffffffffffff
  f0:	cmp	x11, #0x0
  f4:	cset	w15, ne  // ne = any
  f8:	cmp	x14, x13
  fc:	cset	w13, hi  // hi = pmore
 100:	csel	w13, w15, w13, eq  // eq = none
 104:	tbnz	w13, #0, 170 <__getf2+0xb4>
 108:	orr	x13, x11, x9
 10c:	orr	x12, x14, x12
 110:	orr	x12, x13, x12
 114:	cbz	x12, 134 <__getf2+0x78>
 118:	tst	x10, x8
 11c:	b.lt	13c <__getf2+0x80>  // b.tstop
 120:	cmp	x9, x11
 124:	cset	w12, cc  // cc = lo, ul, last
 128:	cmp	x8, x10
 12c:	cset	w13, lt  // lt = tstop
 130:	b	14c <__getf2+0x90>
 134:	mov	w0, wzr
 138:	b	170 <__getf2+0xb4>
 13c:	cmp	x9, x11
 140:	cset	w12, hi  // hi = pmore
 144:	cmp	x8, x10
 148:	cset	w13, gt
 14c:	csel	w12, w12, w13, eq  // eq = none
 150:	tbz	w12, #0, 15c <__getf2+0xa0>
 154:	mov	w0, #0xffffffff            	// #-1
 158:	b	170 <__getf2+0xb4>
 15c:	eor	x9, x9, x11
 160:	eor	x8, x8, x10
 164:	orr	x8, x9, x8
 168:	cmp	x8, #0x0
 16c:	cset	w0, ne  // ne = any
 170:	add	sp, sp, #0x20
 174:	ret

0000000000000178 <__unordtf2>:
 178:	stp	q1, q0, [sp, #-32]!
 17c:	ldp	x8, x9, [sp, #16]
 180:	ldp	x10, x11, [sp], #32
 184:	mov	x12, #0x7fff000000000000    	// #9223090561878065152
 188:	and	x9, x9, #0x7fffffffffffffff
 18c:	cmp	x8, #0x0
 190:	and	x8, x11, #0x7fffffffffffffff
 194:	cset	w11, ne  // ne = any
 198:	cmp	x9, x12
 19c:	cset	w9, hi  // hi = pmore
 1a0:	csel	w9, w11, w9, eq  // eq = none
 1a4:	cmp	x10, #0x0
 1a8:	cset	w10, ne  // ne = any
 1ac:	cmp	x8, x12
 1b0:	cset	w8, hi  // hi = pmore
 1b4:	csel	w8, w10, w8, eq  // eq = none
 1b8:	orr	w0, w9, w8
 1bc:	ret

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	fmov	x11, d0
   4:	and	x9, x11, #0x7fffffffffffffff
   8:	mov	x8, #0xfff0000000000000    	// #-4503599627370496
   c:	add	x8, x9, x8
  10:	lsr	x8, x8, #53
  14:	cmp	x8, #0x3fe
  18:	and	x8, x11, #0x8000000000000000
  1c:	b.hi	30 <__extenddftf2+0x30>  // b.pmore
  20:	mov	x11, #0x3c00000000000000    	// #4323455642275676160
  24:	lsl	x10, x9, #60
  28:	add	x9, x11, x9, lsr #4
  2c:	b	98 <__extenddftf2+0x98>
  30:	lsr	x10, x9, #52
  34:	cmp	x10, #0x7ff
  38:	b.cc	4c <__extenddftf2+0x4c>  // b.lo, b.ul, b.last
  3c:	lsr	x9, x11, #4
  40:	lsl	x10, x11, #60
  44:	orr	x9, x9, #0x7fff000000000000
  48:	b	98 <__extenddftf2+0x98>
  4c:	cbz	x9, 94 <__extenddftf2+0x94>
  50:	clz	x11, x9
  54:	add	w10, w11, #0x31
  58:	neg	x13, x10
  5c:	cmp	x10, #0x0
  60:	lsl	x14, x9, x10
  64:	lsr	x13, x9, x13
  68:	lsl	x9, x9, x10
  6c:	sub	x10, x10, #0x40
  70:	csel	x13, xzr, x13, eq  // eq = none
  74:	cmp	x10, #0x0
  78:	mov	w12, #0x3c0c                	// #15372
  7c:	csel	x13, x14, x13, ge  // ge = tcont
  80:	csel	x10, xzr, x9, ge  // ge = tcont
  84:	eor	x9, x13, #0x1000000000000
  88:	sub	w11, w12, w11
  8c:	orr	x9, x9, x11, lsl #48
  90:	b	98 <__extenddftf2+0x98>
  94:	mov	x10, xzr
  98:	orr	x8, x9, x8
  9c:	stp	x10, x8, [sp, #-16]!
  a0:	ldr	q0, [sp], #16
  a4:	ret

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	fmov	w11, s0
   4:	and	w9, w11, #0x7fffffff
   8:	sub	w8, w9, #0x800, lsl #12
   c:	lsr	w8, w8, #24
  10:	cmp	w8, #0x7e
  14:	and	w8, w11, #0x80000000
  18:	b.hi	2c <__extendsftf2+0x2c>  // b.pmore
  1c:	mov	x11, #0x3f80000000000000    	// #4575657221408423936
  20:	mov	x10, xzr
  24:	add	x9, x11, x9, lsl #25
  28:	b	9c <__extendsftf2+0x9c>
  2c:	lsr	w10, w9, #23
  30:	cmp	w10, #0xff
  34:	b.cc	4c <__extendsftf2+0x4c>  // b.lo, b.ul, b.last
  38:	mov	w9, w11
  3c:	lsl	x9, x9, #25
  40:	mov	x10, xzr
  44:	orr	x9, x9, #0x7fff000000000000
  48:	b	9c <__extendsftf2+0x9c>
  4c:	cbz	w9, 94 <__extendsftf2+0x94>
  50:	clz	w11, w9
  54:	add	w10, w11, #0x51
  58:	neg	x13, x10
  5c:	cmp	x10, #0x0
  60:	lsl	x14, x9, x10
  64:	lsr	x13, x9, x13
  68:	lsl	x9, x9, x10
  6c:	sub	x10, x10, #0x40
  70:	csel	x13, xzr, x13, eq  // eq = none
  74:	cmp	x10, #0x0
  78:	mov	w12, #0x3f89                	// #16265
  7c:	csel	x13, x14, x13, ge  // ge = tcont
  80:	csel	x10, xzr, x9, ge  // ge = tcont
  84:	eor	x9, x13, #0x1000000000000
  88:	sub	w11, w12, w11
  8c:	orr	x9, x9, x11, lsl #48
  90:	b	9c <__extendsftf2+0x9c>
  94:	mov	x10, xzr
  98:	mov	x9, xzr
  9c:	orr	x8, x9, x8, lsl #32
  a0:	stp	x10, x8, [sp, #-16]!
  a4:	ldr	q0, [sp], #16
  a8:	ret

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w10, #0x3fff                	// #16383
   c:	ubfx	x9, x8, #48, #15
  10:	cmp	w9, w10
  14:	b.cs	20 <__fixtfdi+0x20>  // b.hs, b.nlast
  18:	mov	x0, xzr
  1c:	b	84 <__fixtfdi+0x84>
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x40
  2c:	b.cc	40 <__fixtfdi+0x40>  // b.lo, b.ul, b.last
  30:	cmp	x8, #0x0
  34:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  38:	cinv	x0, x8, ge  // ge = tcont
  3c:	b	84 <__fixtfdi+0x84>
  40:	ldr	x10, [sp]
  44:	mov	w12, #0x406f                	// #16495
  48:	mov	x11, #0x1000000000000       	// #281474976710656
  4c:	sub	w9, w12, w9
  50:	bfxil	x11, x8, #0, #48
  54:	neg	x12, x9
  58:	cmp	x9, #0x0
  5c:	lsl	x12, x11, x12
  60:	lsr	x11, x11, x9
  64:	lsr	x10, x10, x9
  68:	sub	x9, x9, #0x40
  6c:	csel	x12, xzr, x12, eq  // eq = none
  70:	cmp	x9, #0x0
  74:	orr	x9, x10, x12
  78:	csel	x9, x11, x9, ge  // ge = tcont
  7c:	cmp	x8, #0x0
  80:	cneg	x0, x9, lt  // lt = tstop
  84:	add	sp, sp, #0x10
  88:	ret

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w10, #0x3fff                	// #16383
   c:	ubfx	x9, x8, #48, #15
  10:	cmp	w9, w10
  14:	b.cs	20 <__fixtfsi+0x20>  // b.hs, b.nlast
  18:	mov	w0, wzr
  1c:	b	84 <__fixtfsi+0x84>
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x20
  2c:	b.cc	40 <__fixtfsi+0x40>  // b.lo, b.ul, b.last
  30:	cmp	x8, #0x0
  34:	mov	w8, #0x80000000            	// #-2147483648
  38:	cinv	w0, w8, ge  // ge = tcont
  3c:	b	84 <__fixtfsi+0x84>
  40:	ldr	x10, [sp]
  44:	mov	w12, #0x406f                	// #16495
  48:	mov	x11, #0x1000000000000       	// #281474976710656
  4c:	sub	w9, w12, w9
  50:	bfxil	x11, x8, #0, #48
  54:	neg	x12, x9
  58:	cmp	x9, #0x0
  5c:	lsl	x12, x11, x12
  60:	lsr	x11, x11, x9
  64:	lsr	x10, x10, x9
  68:	sub	x9, x9, #0x40
  6c:	csel	x12, xzr, x12, eq  // eq = none
  70:	cmp	x9, #0x0
  74:	orr	x9, x10, x12
  78:	csel	x9, x11, x9, ge  // ge = tcont
  7c:	cmp	x8, #0x0
  80:	cneg	w0, w9, lt  // lt = tstop
  84:	add	sp, sp, #0x10
  88:	ret

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x12, [sp, #8]
   8:	mov	w8, #0x1                   	// #1
   c:	mov	w10, #0x3fff                	// #16383
  10:	cmp	x12, #0x0
  14:	ubfx	x13, x12, #48, #15
  18:	csetm	x9, lt  // lt = tstop
  1c:	cneg	x8, x8, lt  // lt = tstop
  20:	cmp	w13, w10
  24:	b.cs	34 <__fixtfti+0x34>  // b.hs, b.nlast
  28:	mov	x0, xzr
  2c:	mov	x1, xzr
  30:	b	104 <__fixtfti+0x104>
  34:	mov	w10, #0xffffc001            	// #-16383
  38:	add	w10, w13, w10
  3c:	cmp	w10, #0x80
  40:	b.cc	5c <__fixtfti+0x5c>  // b.lo, b.ul, b.last
  44:	mov	x8, #0xffffffffffffffff    	// #-1
  48:	cmp	x12, #0x0
  4c:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  50:	eor	x0, x8, x12, asr #63
  54:	cinv	x1, x9, lt  // lt = tstop
  58:	b	104 <__fixtfti+0x104>
  5c:	ldr	x10, [sp]
  60:	mov	x11, #0x1000000000000       	// #281474976710656
  64:	mov	w14, #0x406e                	// #16494
  68:	cmp	w13, w14
  6c:	bfxil	x11, x12, #0, #48
  70:	b.hi	bc <__fixtfti+0xbc>  // b.pmore
  74:	mov	w12, #0x406f                	// #16495
  78:	sub	w12, w12, w13
  7c:	neg	x13, x12
  80:	cmp	x12, #0x0
  84:	lsr	x14, x11, x12
  88:	sub	x15, x12, #0x40
  8c:	lsr	x10, x10, x12
  90:	lsr	x12, x11, x12
  94:	lsl	x11, x11, x13
  98:	csel	x11, xzr, x11, eq  // eq = none
  9c:	cmp	x15, #0x0
  a0:	orr	x10, x10, x11
  a4:	csel	x10, x12, x10, ge  // ge = tcont
  a8:	umulh	x11, x10, x8
  ac:	csel	x13, xzr, x14, ge  // ge = tcont
  b0:	madd	x9, x10, x9, x11
  b4:	madd	x1, x13, x8, x9
  b8:	b	100 <__fixtfti+0x100>
  bc:	mov	w12, #0xffffbf91            	// #-16495
  c0:	add	w12, w13, w12
  c4:	neg	x13, x12
  c8:	cmp	x12, #0x0
  cc:	lsl	x11, x11, x12
  d0:	lsl	x14, x10, x12
  d4:	lsr	x13, x10, x13
  d8:	lsl	x10, x10, x12
  dc:	sub	x12, x12, #0x40
  e0:	csel	x13, xzr, x13, eq  // eq = none
  e4:	cmp	x12, #0x0
  e8:	csel	x10, xzr, x10, ge  // ge = tcont
  ec:	orr	x11, x13, x11
  f0:	umulh	x12, x10, x8
  f4:	csel	x11, x14, x11, ge  // ge = tcont
  f8:	madd	x9, x10, x9, x12
  fc:	madd	x1, x11, x8, x9
 100:	mul	x0, x10, x8
 104:	add	sp, sp, #0x10
 108:	ret

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	x0, xzr
   c:	tbnz	x8, #63, 74 <__fixunstfdi+0x74>
  10:	ubfx	x9, x8, #48, #15
  14:	mov	w10, #0x3fff                	// #16383
  18:	cmp	w9, w10
  1c:	b.cc	74 <__fixunstfdi+0x74>  // b.lo, b.ul, b.last
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x3f
  2c:	b.ls	38 <__fixunstfdi+0x38>  // b.plast
  30:	mov	x0, #0xffffffffffffffff    	// #-1
  34:	b	74 <__fixunstfdi+0x74>
  38:	ldr	x10, [sp]
  3c:	mov	x11, #0x1000000000000       	// #281474976710656
  40:	mov	w12, #0x406f                	// #16495
  44:	bfxil	x11, x8, #0, #48
  48:	sub	w8, w12, w9
  4c:	neg	x9, x8
  50:	cmp	x8, #0x0
  54:	lsl	x9, x11, x9
  58:	lsr	x12, x11, x8
  5c:	sub	x13, x8, #0x40
  60:	csel	x9, xzr, x9, eq  // eq = none
  64:	lsr	x8, x10, x8
  68:	orr	x8, x8, x9
  6c:	cmp	x13, #0x0
  70:	csel	x0, x12, x8, ge  // ge = tcont
  74:	add	sp, sp, #0x10
  78:	ret

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w0, wzr
   c:	tbnz	x8, #63, 74 <__fixunstfsi+0x74>
  10:	ubfx	x9, x8, #48, #15
  14:	mov	w10, #0x3fff                	// #16383
  18:	cmp	w9, w10
  1c:	b.cc	74 <__fixunstfsi+0x74>  // b.lo, b.ul, b.last
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x1f
  2c:	b.ls	38 <__fixunstfsi+0x38>  // b.plast
  30:	mov	w0, #0xffffffff            	// #-1
  34:	b	74 <__fixunstfsi+0x74>
  38:	ldr	x10, [sp]
  3c:	mov	x11, #0x1000000000000       	// #281474976710656
  40:	mov	w12, #0x406f                	// #16495
  44:	bfxil	x11, x8, #0, #48
  48:	sub	w8, w12, w9
  4c:	neg	x9, x8
  50:	cmp	x8, #0x0
  54:	lsl	x9, x11, x9
  58:	lsr	x12, x11, x8
  5c:	sub	x13, x8, #0x40
  60:	csel	x9, xzr, x9, eq  // eq = none
  64:	lsr	x8, x10, x8
  68:	orr	x8, x8, x9
  6c:	cmp	x13, #0x0
  70:	csel	x0, x12, x8, ge  // ge = tcont
  74:	add	sp, sp, #0x10
  78:	ret

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x11, [sp, #8]
   8:	mov	x0, xzr
   c:	tbnz	x11, #63, 40 <__fixunstfti+0x40>
  10:	ubfx	x10, x11, #48, #15
  14:	mov	w8, #0x3fff                	// #16383
  18:	cmp	w10, w8
  1c:	mov	x1, x0
  20:	b.cc	d4 <__fixunstfti+0xd4>  // b.lo, b.ul, b.last
  24:	mov	w8, #0xffffc001            	// #-16383
  28:	add	w8, w10, w8
  2c:	cmp	w8, #0x7f
  30:	b.ls	48 <__fixunstfti+0x48>  // b.plast
  34:	mov	x0, #0xffffffffffffffff    	// #-1
  38:	mov	x1, #0xffffffffffffffff    	// #-1
  3c:	b	d4 <__fixunstfti+0xd4>
  40:	mov	x1, x0
  44:	b	d4 <__fixunstfti+0xd4>
  48:	ldr	x8, [sp]
  4c:	mov	x9, #0x1000000000000       	// #281474976710656
  50:	mov	w12, #0x406e                	// #16494
  54:	cmp	w10, w12
  58:	bfxil	x9, x11, #0, #48
  5c:	b.hi	9c <__fixunstfti+0x9c>  // b.pmore
  60:	mov	w11, #0x406f                	// #16495
  64:	sub	w10, w11, w10
  68:	neg	x11, x10
  6c:	cmp	x10, #0x0
  70:	lsl	x11, x9, x11
  74:	sub	x13, x10, #0x40
  78:	lsr	x8, x8, x10
  7c:	csel	x11, xzr, x11, eq  // eq = none
  80:	lsr	x12, x9, x10
  84:	cmp	x13, #0x0
  88:	orr	x8, x8, x11
  8c:	lsr	x9, x9, x10
  90:	csel	x1, xzr, x12, ge  // ge = tcont
  94:	csel	x0, x9, x8, ge  // ge = tcont
  98:	b	d4 <__fixunstfti+0xd4>
  9c:	mov	w11, #0xffffbf91            	// #-16495
  a0:	add	w10, w10, w11
  a4:	neg	x11, x10
  a8:	cmp	x10, #0x0
  ac:	lsr	x11, x8, x11
  b0:	sub	x13, x10, #0x40
  b4:	lsl	x9, x9, x10
  b8:	csel	x11, xzr, x11, eq  // eq = none
  bc:	lsl	x12, x8, x10
  c0:	cmp	x13, #0x0
  c4:	orr	x9, x11, x9
  c8:	lsl	x8, x8, x10
  cc:	csel	x0, xzr, x12, ge  // ge = tcont
  d0:	csel	x1, x8, x9, ge  // ge = tcont
  d4:	add	sp, sp, #0x10
  d8:	ret

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	x0, 64 <__floatditf+0x64>
   8:	cmp	x0, #0x0
   c:	cneg	x10, x0, mi  // mi = first
  10:	clz	x11, x10
  14:	mov	w9, #0x403e                	// #16446
  18:	add	w12, w11, #0x31
  1c:	sub	w9, w9, w11
  20:	neg	x11, x12
  24:	cmp	x12, #0x0
  28:	lsl	x13, x10, x12
  2c:	sub	x14, x12, #0x40
  30:	lsl	x12, x10, x12
  34:	lsr	x10, x10, x11
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x14, #0x0
  40:	csel	x10, x13, x10, ge  // ge = tcont
  44:	eor	x10, x10, #0x1000000000000
  48:	and	x8, x0, #0x8000000000000000
  4c:	add	x9, x10, x9, lsl #48
  50:	csel	x11, xzr, x12, ge  // ge = tcont
  54:	orr	x8, x9, x8
  58:	stp	x11, x8, [sp]
  5c:	ldr	q0, [sp]
  60:	b	6c <__floatditf+0x6c>
  64:	adrp	x8, 0 <__floatditf>
  68:	ldr	q0, [x8]
  6c:	add	sp, sp, #0x10
  70:	ret

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	w0, 64 <__floatsitf+0x64>
   8:	cmp	w0, #0x0
   c:	cneg	w10, w0, mi  // mi = first
  10:	clz	w11, w10
  14:	mov	w9, #0x401e                	// #16414
  18:	add	w12, w11, #0x51
  1c:	sub	w9, w9, w11
  20:	neg	x11, x12
  24:	cmp	x12, #0x0
  28:	lsl	x13, x10, x12
  2c:	sub	x14, x12, #0x40
  30:	lsl	x12, x10, x12
  34:	lsr	x10, x10, x11
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x14, #0x0
  40:	csel	x10, x13, x10, ge  // ge = tcont
  44:	eor	x10, x10, #0x1000000000000
  48:	and	w8, w0, #0x80000000
  4c:	add	x9, x10, x9, lsl #48
  50:	csel	x11, xzr, x12, ge  // ge = tcont
  54:	orr	x8, x9, x8, lsl #32
  58:	stp	x11, x8, [sp]
  5c:	ldr	q0, [sp]
  60:	b	6c <__floatsitf+0x6c>
  64:	adrp	x8, 0 <__floatsitf>
  68:	ldr	q0, [x8]
  6c:	add	sp, sp, #0x10
  70:	ret

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	sub	sp, sp, #0x40
   4:	stp	x29, x30, [sp, #16]
   8:	str	x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	add	x29, sp, #0x10
  14:	orr	x8, x0, x1
  18:	cbz	x8, 70 <__floattitf+0x70>
  1c:	eor	x8, x0, x1, asr #63
  20:	asr	x21, x1, #63
  24:	eor	x9, x1, x1, asr #63
  28:	subs	x19, x8, x1, asr #63
  2c:	sbcs	x20, x9, x21
  30:	mov	x0, x19
  34:	mov	x1, x20
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x72
  4c:	sub	w8, w8, w0
  50:	b.lt	7c <__floattitf+0x7c>  // b.tstop
  54:	cmp	w9, #0x73
  58:	b.eq	130 <__floattitf+0x130>  // b.none
  5c:	cmp	w9, #0x72
  60:	b.ne	b4 <__floattitf+0xb4>  // b.any
  64:	extr	x20, x20, x19, #63
  68:	lsl	x19, x19, #1
  6c:	b	130 <__floattitf+0x130>
  70:	adrp	x8, 0 <__floattitf>
  74:	ldr	q0, [x8]
  78:	b	184 <__floattitf+0x184>
  7c:	sub	w9, w0, #0xf
  80:	neg	x10, x9
  84:	cmp	x9, #0x0
  88:	lsr	x10, x19, x10
  8c:	lsl	x11, x20, x9
  90:	sub	x13, x9, #0x40
  94:	csel	x10, xzr, x10, eq  // eq = none
  98:	lsl	x12, x19, x9
  9c:	lsl	x9, x19, x9
  a0:	cmp	x13, #0x0
  a4:	orr	x10, x10, x11
  a8:	csel	x10, x12, x10, ge  // ge = tcont
  ac:	csel	x11, xzr, x9, ge  // ge = tcont
  b0:	b	168 <__floattitf+0x168>
  b4:	mov	w10, #0xd                   	// #13
  b8:	sub	w10, w10, w0
  bc:	neg	x13, x10
  c0:	cmp	x10, #0x0
  c4:	sub	x14, x10, #0x40
  c8:	lsl	x13, x20, x13
  cc:	add	w11, w0, #0x73
  d0:	csel	x13, xzr, x13, eq  // eq = none
  d4:	cmp	x14, #0x0
  d8:	lsr	x14, x19, x10
  dc:	neg	x12, x11
  e0:	orr	x13, x14, x13
  e4:	lsr	x14, x20, x10
  e8:	lsr	x10, x20, x10
  ec:	lsl	x15, x20, x11
  f0:	csel	x10, x10, x13, ge  // ge = tcont
  f4:	lsr	x12, x19, x12
  f8:	csel	x20, xzr, x14, ge  // ge = tcont
  fc:	cmp	x11, #0x0
 100:	lsl	x13, x19, x11
 104:	lsl	x16, x19, x11
 108:	sub	x11, x11, #0x40
 10c:	csel	x12, xzr, x12, eq  // eq = none
 110:	cmp	x11, #0x0
 114:	orr	x11, x12, x15
 118:	csel	x11, x13, x11, ge  // ge = tcont
 11c:	csel	x12, xzr, x16, ge  // ge = tcont
 120:	orr	x11, x12, x11
 124:	cmp	x11, #0x0
 128:	cset	w11, ne  // ne = any
 12c:	orr	x19, x10, x11
 130:	ubfx	x10, x19, #2, #1
 134:	orr	x10, x10, x19
 138:	adds	x10, x10, #0x1
 13c:	adcs	x12, x20, xzr
 140:	mov	w11, #0x2                   	// #2
 144:	tst	x12, #0x8000000000000
 148:	cinc	x11, x11, ne  // ne = any
 14c:	lsl	x13, x12, #1
 150:	eor	x15, x11, #0x3f
 154:	lsr	x14, x10, x11
 158:	asr	x10, x12, x11
 15c:	lsl	x11, x13, x15
 160:	orr	x11, x14, x11
 164:	csel	w8, w8, w9, eq  // eq = none
 168:	mov	w12, #0x3fff                	// #16383
 16c:	and	x9, x21, #0x8000000000000000
 170:	add	w8, w8, w12
 174:	orr	x8, x9, x8, lsl #48
 178:	bfxil	x8, x10, #0, #48
 17c:	stp	x11, x8, [sp]
 180:	ldr	q0, [sp]
 184:	ldp	x20, x19, [sp, #48]
 188:	ldr	x21, [sp, #32]
 18c:	ldp	x29, x30, [sp, #16]
 190:	add	sp, sp, #0x40
 194:	ret

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	x0, 5c <__floatunditf+0x5c>
   8:	clz	x8, x0
   c:	mov	w9, #0x70                  	// #112
  10:	eor	w8, w8, #0x3f
  14:	mov	w10, #0x3fff                	// #16383
  18:	sub	w9, w9, w8
  1c:	add	w8, w8, w10
  20:	neg	x10, x9
  24:	cmp	x9, #0x0
  28:	sub	x12, x9, #0x40
  2c:	lsr	x10, x0, x10
  30:	lsl	x11, x0, x9
  34:	lsl	x9, x0, x9
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x12, #0x0
  40:	csel	x9, x9, x10, ge  // ge = tcont
  44:	eor	x9, x9, #0x1000000000000
  48:	csel	x11, xzr, x11, ge  // ge = tcont
  4c:	add	x8, x9, x8, lsl #48
  50:	stp	x11, x8, [sp]
  54:	ldr	q0, [sp]
  58:	b	64 <__floatunditf+0x64>
  5c:	adrp	x8, 0 <__floatunditf>
  60:	ldr	q0, [x8]
  64:	add	sp, sp, #0x10
  68:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	w0, 60 <__floatunsitf+0x60>
   8:	clz	w8, w0
   c:	mov	w9, #0x70                  	// #112
  10:	eor	w8, w8, #0x1f
  14:	mov	w11, #0x3fff                	// #16383
  18:	sub	w9, w9, w8
  1c:	mov	w10, w0
  20:	add	w8, w8, w11
  24:	neg	x11, x9
  28:	cmp	x9, #0x0
  2c:	lsl	x12, x10, x9
  30:	sub	x13, x9, #0x40
  34:	lsl	x9, x10, x9
  38:	lsr	x10, x10, x11
  3c:	csel	x10, xzr, x10, eq  // eq = none
  40:	cmp	x13, #0x0
  44:	csel	x9, x9, x10, ge  // ge = tcont
  48:	eor	x9, x9, #0x1000000000000
  4c:	csel	x11, xzr, x12, ge  // ge = tcont
  50:	add	x8, x9, x8, lsl #48
  54:	stp	x11, x8, [sp]
  58:	ldr	q0, [sp]
  5c:	b	68 <__floatunsitf+0x68>
  60:	adrp	x8, 0 <__floatunsitf>
  64:	ldr	q0, [x8]
  68:	add	sp, sp, #0x10
  6c:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	orr	x8, x0, x1
  14:	cbz	x8, 58 <__floatuntitf+0x58>
  18:	mov	x20, x1
  1c:	mov	x19, x0
  20:	bl	0 <__clzti2>
  24:	mov	w8, #0x80                  	// #128
  28:	sub	w9, w8, w0
  2c:	mov	w8, #0x7f                  	// #127
  30:	cmp	w9, #0x72
  34:	sub	w8, w8, w0
  38:	b.lt	64 <__floatuntitf+0x64>  // b.tstop
  3c:	cmp	w9, #0x73
  40:	b.eq	118 <__floatuntitf+0x118>  // b.none
  44:	cmp	w9, #0x72
  48:	b.ne	9c <__floatuntitf+0x9c>  // b.any
  4c:	extr	x20, x20, x19, #63
  50:	lsl	x19, x19, #1
  54:	b	118 <__floatuntitf+0x118>
  58:	adrp	x8, 0 <__floatuntitf>
  5c:	ldr	q0, [x8]
  60:	b	164 <__floatuntitf+0x164>
  64:	sub	w9, w0, #0xf
  68:	neg	x10, x9
  6c:	cmp	x9, #0x0
  70:	lsr	x10, x19, x10
  74:	lsl	x11, x20, x9
  78:	sub	x13, x9, #0x40
  7c:	csel	x10, xzr, x10, eq  // eq = none
  80:	lsl	x12, x19, x9
  84:	lsl	x9, x19, x9
  88:	cmp	x13, #0x0
  8c:	orr	x10, x10, x11
  90:	csel	x10, x12, x10, ge  // ge = tcont
  94:	csel	x11, xzr, x9, ge  // ge = tcont
  98:	b	150 <__floatuntitf+0x150>
  9c:	mov	w10, #0xd                   	// #13
  a0:	sub	w10, w10, w0
  a4:	neg	x13, x10
  a8:	cmp	x10, #0x0
  ac:	sub	x14, x10, #0x40
  b0:	lsl	x13, x20, x13
  b4:	add	w11, w0, #0x73
  b8:	csel	x13, xzr, x13, eq  // eq = none
  bc:	cmp	x14, #0x0
  c0:	lsr	x14, x19, x10
  c4:	neg	x12, x11
  c8:	orr	x13, x14, x13
  cc:	lsr	x14, x20, x10
  d0:	lsr	x10, x20, x10
  d4:	lsl	x15, x20, x11
  d8:	csel	x10, x10, x13, ge  // ge = tcont
  dc:	lsr	x12, x19, x12
  e0:	csel	x20, xzr, x14, ge  // ge = tcont
  e4:	cmp	x11, #0x0
  e8:	lsl	x13, x19, x11
  ec:	lsl	x16, x19, x11
  f0:	sub	x11, x11, #0x40
  f4:	csel	x12, xzr, x12, eq  // eq = none
  f8:	cmp	x11, #0x0
  fc:	orr	x11, x12, x15
 100:	csel	x11, x13, x11, ge  // ge = tcont
 104:	csel	x12, xzr, x16, ge  // ge = tcont
 108:	orr	x11, x12, x11
 10c:	cmp	x11, #0x0
 110:	cset	w11, ne  // ne = any
 114:	orr	x19, x10, x11
 118:	ubfx	x10, x19, #2, #1
 11c:	orr	x10, x10, x19
 120:	adds	x10, x10, #0x1
 124:	adcs	x12, x20, xzr
 128:	mov	w11, #0x2                   	// #2
 12c:	tst	x12, #0x8000000000000
 130:	cinc	x11, x11, ne  // ne = any
 134:	lsl	x13, x12, #1
 138:	eor	x15, x11, #0x3f
 13c:	lsr	x14, x10, x11
 140:	lsr	x10, x12, x11
 144:	lsl	x11, x13, x15
 148:	orr	x11, x14, x11
 14c:	csel	w8, w8, w9, eq  // eq = none
 150:	mov	w9, #0x3fff                	// #16383
 154:	add	w8, w8, w9
 158:	bfi	x10, x8, #48, #16
 15c:	stp	x11, x10, [sp]
 160:	ldr	q0, [sp]
 164:	ldp	x20, x19, [sp, #32]
 168:	ldp	x29, x30, [sp, #16]
 16c:	add	sp, sp, #0x30
 170:	ret

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x28, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	sub	sp, sp, #0x2d0
  14:	str	q1, [sp, #192]
  18:	mov	v1.16b, v2.16b
  1c:	stp	q2, q3, [sp, #160]
  20:	str	q0, [sp, #144]
  24:	bl	0 <__multf3>
  28:	str	q0, [sp, #80]
  2c:	ldp	q1, q0, [sp, #176]
  30:	bl	0 <__multf3>
  34:	str	q0, [sp, #64]
  38:	ldr	q0, [sp, #144]
  3c:	ldr	q1, [sp, #176]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #48]
  48:	ldr	q0, [sp, #192]
  4c:	ldr	q1, [sp, #160]
  50:	bl	0 <__multf3>
  54:	str	q0, [sp, #32]
  58:	ldp	q1, q0, [sp, #64]
  5c:	bl	0 <__subtf3>
  60:	str	q0, [sp, #112]
  64:	ldp	q0, q1, [sp, #32]
  68:	bl	0 <__addtf3>
  6c:	str	q0, [sp, #96]
  70:	ldr	q0, [sp, #112]
  74:	mov	v1.16b, v0.16b
  78:	bl	0 <__unordtf2>
  7c:	cbz	w0, fc <__multc3+0xfc>
  80:	ldr	q0, [sp, #96]
  84:	mov	v1.16b, v0.16b
  88:	bl	0 <__unordtf2>
  8c:	cbz	w0, fc <__multc3+0xfc>
  90:	ldr	q0, [sp, #144]
  94:	stur	q0, [x29, #-32]
  98:	ldurb	w8, [x29, #-17]
  9c:	and	w8, w8, #0x7f
  a0:	sturb	w8, [x29, #-17]
  a4:	ldr	q0, [sp, #192]
  a8:	stur	q0, [x29, #-16]
  ac:	ldurb	w8, [x29, #-1]
  b0:	and	w8, w8, #0x7f
  b4:	sturb	w8, [x29, #-1]
  b8:	ldp	q2, q0, [x29, #-32]
  bc:	adrp	x8, 0 <__multc3>
  c0:	ldr	q1, [x8]
  c4:	str	q0, [sp, #16]
  c8:	mov	v0.16b, v2.16b
  cc:	str	q1, [sp, #128]
  d0:	str	q2, [sp]
  d4:	bl	0 <__eqtf2>
  d8:	adrp	x19, 0 <__multc3>
  dc:	add	x19, x19, #0x0
  e0:	cbz	w0, 104 <__multc3+0x104>
  e4:	ldr	q0, [sp, #16]
  e8:	ldr	q1, [sp, #128]
  ec:	bl	0 <__netf2>
  f0:	cbz	w0, 104 <__multc3+0x104>
  f4:	mov	w20, wzr
  f8:	b	200 <__multc3+0x200>
  fc:	ldr	q1, [sp, #96]
 100:	b	54c <__multc3+0x54c>
 104:	ldr	q0, [sp, #16]
 108:	ldr	q1, [sp, #128]
 10c:	bl	0 <__eqtf2>
 110:	ldr	q0, [sp]
 114:	ldr	q1, [sp, #128]
 118:	cmp	w0, #0x0
 11c:	cset	w20, eq  // eq = none
 120:	bl	0 <__eqtf2>
 124:	cmp	w0, #0x0
 128:	cset	w8, eq  // eq = none
 12c:	ldr	q0, [x19, w8, uxtw #4]
 130:	ldr	q1, [sp, #144]
 134:	adrp	x10, 0 <__multc3>
 138:	stp	q0, q1, [x29, #-64]
 13c:	ldurb	w8, [x29, #-33]
 140:	ldurb	w9, [x29, #-49]
 144:	ldr	q0, [x19, w20, uxtw #4]
 148:	bfxil	w8, w9, #0, #7
 14c:	sturb	w8, [x29, #-49]
 150:	ldur	q1, [x29, #-64]
 154:	str	q1, [sp, #144]
 158:	ldr	q1, [sp, #192]
 15c:	stp	q0, q1, [x29, #-96]
 160:	ldurb	w8, [x29, #-65]
 164:	ldurb	w9, [x29, #-81]
 168:	ldr	q1, [x10]
 16c:	bfxil	w8, w9, #0, #7
 170:	sturb	w8, [x29, #-81]
 174:	ldur	q0, [x29, #-96]
 178:	str	q0, [sp, #192]
 17c:	ldr	q0, [sp, #160]
 180:	stur	q0, [x29, #-144]
 184:	str	q1, [sp, #16]
 188:	stur	q1, [x29, #-160]
 18c:	ldurb	w8, [x29, #-129]
 190:	ldurb	w9, [x29, #-145]
 194:	bfxil	w8, w9, #0, #7
 198:	sturb	w8, [x29, #-145]
 19c:	ldur	q1, [x29, #-160]
 1a0:	str	q1, [sp]
 1a4:	mov	v1.16b, v0.16b
 1a8:	bl	0 <__unordtf2>
 1ac:	cmp	w0, #0x0
 1b0:	b.eq	1bc <__multc3+0x1bc>  // b.none
 1b4:	ldr	q0, [sp]
 1b8:	str	q0, [sp, #160]
 1bc:	ldr	q0, [sp, #176]
 1c0:	stur	q0, [x29, #-112]
 1c4:	ldr	q1, [sp, #16]
 1c8:	ldurb	w8, [x29, #-97]
 1cc:	stur	q1, [x29, #-128]
 1d0:	ldurb	w9, [x29, #-113]
 1d4:	bfxil	w8, w9, #0, #7
 1d8:	sturb	w8, [x29, #-113]
 1dc:	ldur	q1, [x29, #-128]
 1e0:	str	q1, [sp, #16]
 1e4:	mov	v1.16b, v0.16b
 1e8:	bl	0 <__unordtf2>
 1ec:	cmp	w0, #0x0
 1f0:	b.eq	1fc <__multc3+0x1fc>  // b.none
 1f4:	ldr	q0, [sp, #16]
 1f8:	str	q0, [sp, #176]
 1fc:	mov	w20, #0x1                   	// #1
 200:	ldr	q0, [sp, #160]
 204:	ldr	q1, [sp, #128]
 208:	stur	q0, [x29, #-192]
 20c:	ldurb	w8, [x29, #-177]
 210:	and	w8, w8, #0x7f
 214:	sturb	w8, [x29, #-177]
 218:	ldur	q0, [x29, #-192]
 21c:	str	q0, [sp, #16]
 220:	ldr	q0, [sp, #176]
 224:	stur	q0, [x29, #-176]
 228:	ldurb	w8, [x29, #-161]
 22c:	and	w8, w8, #0x7f
 230:	sturb	w8, [x29, #-161]
 234:	ldur	q0, [x29, #-176]
 238:	str	q0, [sp]
 23c:	bl	0 <__eqtf2>
 240:	cbz	w0, 400 <__multc3+0x400>
 244:	ldr	q0, [sp, #16]
 248:	ldr	q1, [sp, #128]
 24c:	bl	0 <__netf2>
 250:	cbz	w0, 400 <__multc3+0x400>
 254:	cbnz	w20, 4f0 <__multc3+0x4f0>
 258:	ldr	q0, [sp, #80]
 25c:	stur	q0, [x29, #-256]
 260:	ldurb	w8, [x29, #-241]
 264:	and	w8, w8, #0x7f
 268:	sturb	w8, [x29, #-241]
 26c:	ldur	q0, [x29, #-256]
 270:	str	q0, [sp, #16]
 274:	ldr	q0, [sp, #64]
 278:	stur	q0, [x29, #-240]
 27c:	ldurb	w8, [x29, #-225]
 280:	and	w8, w8, #0x7f
 284:	sturb	w8, [x29, #-225]
 288:	ldur	q0, [x29, #-240]
 28c:	str	q0, [sp, #64]
 290:	ldr	q0, [sp, #48]
 294:	stur	q0, [x29, #-224]
 298:	ldurb	w8, [x29, #-209]
 29c:	and	w8, w8, #0x7f
 2a0:	sturb	w8, [x29, #-209]
 2a4:	ldur	q0, [x29, #-224]
 2a8:	str	q0, [sp, #80]
 2ac:	ldr	q0, [sp, #32]
 2b0:	stur	q0, [x29, #-208]
 2b4:	ldurb	w8, [x29, #-193]
 2b8:	and	w8, w8, #0x7f
 2bc:	sturb	w8, [x29, #-193]
 2c0:	ldur	q0, [x29, #-208]
 2c4:	ldr	q1, [sp, #128]
 2c8:	bl	0 <__eqtf2>
 2cc:	cbz	w0, 304 <__multc3+0x304>
 2d0:	ldr	q0, [sp, #80]
 2d4:	ldr	q1, [sp, #128]
 2d8:	bl	0 <__eqtf2>
 2dc:	cbz	w0, 304 <__multc3+0x304>
 2e0:	ldr	q0, [sp, #16]
 2e4:	ldr	q1, [sp, #128]
 2e8:	bl	0 <__eqtf2>
 2ec:	cbz	w0, 304 <__multc3+0x304>
 2f0:	ldr	q0, [sp, #64]
 2f4:	ldr	q1, [sp, #128]
 2f8:	bl	0 <__netf2>
 2fc:	ldr	q1, [sp, #96]
 300:	cbnz	w0, 54c <__multc3+0x54c>
 304:	adrp	x8, 0 <__multc3>
 308:	ldr	q1, [x8]
 30c:	ldr	q0, [sp, #144]
 310:	str	q1, [sp, #112]
 314:	stp	q1, q0, [sp, #336]
 318:	ldrb	w8, [sp, #367]
 31c:	ldrb	w9, [sp, #351]
 320:	bfxil	w8, w9, #0, #7
 324:	strb	w8, [sp, #351]
 328:	ldr	q1, [sp, #336]
 32c:	str	q1, [sp, #96]
 330:	mov	v1.16b, v0.16b
 334:	bl	0 <__unordtf2>
 338:	cmp	w0, #0x0
 33c:	b.eq	348 <__multc3+0x348>  // b.none
 340:	ldr	q0, [sp, #96]
 344:	str	q0, [sp, #144]
 348:	ldr	q0, [sp, #192]
 34c:	ldr	q1, [sp, #112]
 350:	stp	q1, q0, [sp, #368]
 354:	ldrb	w8, [sp, #399]
 358:	ldrb	w9, [sp, #383]
 35c:	bfxil	w8, w9, #0, #7
 360:	strb	w8, [sp, #383]
 364:	ldr	q1, [sp, #368]
 368:	str	q1, [sp, #96]
 36c:	mov	v1.16b, v0.16b
 370:	bl	0 <__unordtf2>
 374:	cmp	w0, #0x0
 378:	b.eq	384 <__multc3+0x384>  // b.none
 37c:	ldr	q0, [sp, #96]
 380:	str	q0, [sp, #192]
 384:	ldr	q0, [sp, #160]
 388:	ldr	q1, [sp, #112]
 38c:	stp	q1, q0, [sp, #400]
 390:	ldrb	w8, [sp, #431]
 394:	ldrb	w9, [sp, #415]
 398:	bfxil	w8, w9, #0, #7
 39c:	strb	w8, [sp, #415]
 3a0:	ldr	q1, [sp, #400]
 3a4:	str	q1, [sp, #96]
 3a8:	mov	v1.16b, v0.16b
 3ac:	bl	0 <__unordtf2>
 3b0:	cmp	w0, #0x0
 3b4:	b.eq	3c0 <__multc3+0x3c0>  // b.none
 3b8:	ldr	q0, [sp, #96]
 3bc:	str	q0, [sp, #160]
 3c0:	ldr	q0, [sp, #176]
 3c4:	ldr	q1, [sp, #112]
 3c8:	stp	q1, q0, [sp, #432]
 3cc:	ldrb	w8, [sp, #463]
 3d0:	ldrb	w9, [sp, #447]
 3d4:	bfxil	w8, w9, #0, #7
 3d8:	strb	w8, [sp, #447]
 3dc:	ldr	q1, [sp, #432]
 3e0:	str	q1, [sp, #112]
 3e4:	mov	v1.16b, v0.16b
 3e8:	bl	0 <__unordtf2>
 3ec:	cmp	w0, #0x0
 3f0:	b.eq	4f0 <__multc3+0x4f0>  // b.none
 3f4:	ldr	q0, [sp, #112]
 3f8:	str	q0, [sp, #176]
 3fc:	b	4f0 <__multc3+0x4f0>
 400:	ldr	q0, [sp]
 404:	ldr	q1, [sp, #128]
 408:	bl	0 <__eqtf2>
 40c:	ldr	q0, [sp, #16]
 410:	ldr	q1, [sp, #128]
 414:	cmp	w0, #0x0
 418:	cset	w20, eq  // eq = none
 41c:	bl	0 <__eqtf2>
 420:	cmp	w0, #0x0
 424:	cset	w8, eq  // eq = none
 428:	ldr	q0, [x19, w8, uxtw #4]
 42c:	ldr	q1, [sp, #160]
 430:	adrp	x10, 0 <__multc3>
 434:	stp	q0, q1, [sp, #304]
 438:	ldrb	w8, [sp, #335]
 43c:	ldrb	w9, [sp, #319]
 440:	ldr	q0, [x19, w20, uxtw #4]
 444:	bfxil	w8, w9, #0, #7
 448:	strb	w8, [sp, #319]
 44c:	ldr	q1, [sp, #304]
 450:	str	q1, [sp, #160]
 454:	ldr	q1, [sp, #176]
 458:	stp	q0, q1, [sp, #272]
 45c:	ldrb	w8, [sp, #303]
 460:	ldrb	w9, [sp, #287]
 464:	ldr	q1, [x10]
 468:	bfxil	w8, w9, #0, #7
 46c:	strb	w8, [sp, #287]
 470:	ldr	q0, [sp, #272]
 474:	str	q1, [sp, #112]
 478:	str	q0, [sp, #176]
 47c:	ldr	q0, [sp, #144]
 480:	stp	q1, q0, [sp, #208]
 484:	ldrb	w8, [sp, #239]
 488:	ldrb	w9, [sp, #223]
 48c:	bfxil	w8, w9, #0, #7
 490:	strb	w8, [sp, #223]
 494:	ldr	q1, [sp, #208]
 498:	str	q1, [sp, #96]
 49c:	mov	v1.16b, v0.16b
 4a0:	bl	0 <__unordtf2>
 4a4:	cmp	w0, #0x0
 4a8:	b.eq	4b4 <__multc3+0x4b4>  // b.none
 4ac:	ldr	q0, [sp, #96]
 4b0:	str	q0, [sp, #144]
 4b4:	ldr	q0, [sp, #192]
 4b8:	ldr	q1, [sp, #112]
 4bc:	stp	q1, q0, [sp, #240]
 4c0:	ldrb	w8, [sp, #271]
 4c4:	ldrb	w9, [sp, #255]
 4c8:	bfxil	w8, w9, #0, #7
 4cc:	strb	w8, [sp, #255]
 4d0:	ldr	q1, [sp, #240]
 4d4:	str	q1, [sp, #112]
 4d8:	mov	v1.16b, v0.16b
 4dc:	bl	0 <__unordtf2>
 4e0:	cmp	w0, #0x0
 4e4:	b.eq	4f0 <__multc3+0x4f0>  // b.none
 4e8:	ldr	q0, [sp, #112]
 4ec:	str	q0, [sp, #192]
 4f0:	ldp	q1, q0, [sp, #144]
 4f4:	bl	0 <__multf3>
 4f8:	str	q0, [sp, #112]
 4fc:	ldp	q0, q1, [sp, #176]
 500:	bl	0 <__multf3>
 504:	mov	v1.16b, v0.16b
 508:	ldr	q0, [sp, #112]
 50c:	bl	0 <__subtf3>
 510:	ldr	q1, [sp, #128]
 514:	bl	0 <__multf3>
 518:	str	q0, [sp, #112]
 51c:	ldr	q0, [sp, #176]
 520:	ldr	q1, [sp, #144]
 524:	bl	0 <__multf3>
 528:	str	q0, [sp, #176]
 52c:	ldr	q0, [sp, #160]
 530:	ldr	q1, [sp, #192]
 534:	bl	0 <__multf3>
 538:	ldr	q1, [sp, #176]
 53c:	bl	0 <__addtf3>
 540:	ldr	q1, [sp, #128]
 544:	bl	0 <__multf3>
 548:	mov	v1.16b, v0.16b
 54c:	ldr	q0, [sp, #112]
 550:	add	sp, sp, #0x2d0
 554:	ldp	x20, x19, [sp, #32]
 558:	ldr	x28, [sp, #16]
 55c:	ldp	x29, x30, [sp], #48
 560:	ret

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	str	q0, [sp, #-16]!
   4:	ldp	x9, x8, [sp]
   8:	mov	x11, #0xc3ff000000000000    	// #-4323737117252386816
   c:	mov	x12, #0xbc01000000000000    	// #-4899634919602388992
  10:	and	x10, x8, #0x7fffffffffffffff
  14:	add	x11, x10, x11
  18:	add	x12, x10, x12
  1c:	cmp	x11, x12
  20:	b.cs	4c <__trunctfdf2+0x4c>  // b.hs, b.nlast
  24:	mov	x11, #0x1                   	// #1
  28:	and	x10, x9, #0xfffffffffffffff
  2c:	movk	x11, #0x800, lsl #48
  30:	cmp	x10, x11
  34:	extr	x9, x8, x9, #60
  38:	b.cc	78 <__trunctfdf2+0x78>  // b.lo, b.ul, b.last
  3c:	mov	x10, #0x1                   	// #1
  40:	movk	x10, #0x4000, lsl #48
  44:	add	x9, x9, x10
  48:	b	170 <__trunctfdf2+0x170>
  4c:	cmp	x9, #0x0
  50:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
  54:	cset	w12, eq  // eq = none
  58:	cmp	x10, x11
  5c:	cset	w11, cc  // cc = lo, ul, last
  60:	csel	w11, w12, w11, eq  // eq = none
  64:	tbnz	w11, #0, 94 <__trunctfdf2+0x94>
  68:	extr	x10, x8, x9, #60
  6c:	mov	x9, #0x7ff8000000000000    	// #9221120237041090560
  70:	bfxil	x9, x10, #0, #51
  74:	b	170 <__trunctfdf2+0x170>
  78:	mov	x11, #0x4000000000000000    	// #4611686018427387904
  7c:	eor	x10, x10, #0x800000000000000
  80:	add	x9, x9, x11
  84:	cbnz	x10, 170 <__trunctfdf2+0x170>
  88:	and	x10, x9, #0x1
  8c:	add	x9, x10, x9
  90:	b	170 <__trunctfdf2+0x170>
  94:	mov	x11, #0x43feffffffffffff    	// #4899634919602388991
  98:	cmp	x10, x11
  9c:	b.ls	a8 <__trunctfdf2+0xa8>  // b.plast
  a0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  a4:	b	170 <__trunctfdf2+0x170>
  a8:	lsr	x10, x10, #48
  ac:	mov	w11, #0x3b91                	// #15249
  b0:	cmp	w10, w11
  b4:	b.cs	c0 <__trunctfdf2+0xc0>  // b.hs, b.nlast
  b8:	mov	x9, xzr
  bc:	b	170 <__trunctfdf2+0x170>
  c0:	mov	w12, #0x3c01                	// #15361
  c4:	mov	w13, #0xffffc47f            	// #-15233
  c8:	sub	w12, w12, w10
  cc:	add	w10, w10, w13
  d0:	mov	x11, #0x1000000000000       	// #281474976710656
  d4:	neg	x13, x10
  d8:	bfxil	x11, x8, #0, #48
  dc:	cmp	x10, #0x0
  e0:	sub	x15, x10, #0x40
  e4:	lsr	x13, x9, x13
  e8:	csel	x13, xzr, x13, eq  // eq = none
  ec:	cmp	x15, #0x0
  f0:	lsl	x15, x11, x10
  f4:	lsl	x14, x9, x10
  f8:	lsl	x10, x9, x10
  fc:	orr	x13, x13, x15
 100:	csel	x10, x10, x13, ge  // ge = tcont
 104:	csel	x14, xzr, x14, ge  // ge = tcont
 108:	orr	x10, x14, x10
 10c:	neg	x15, x12
 110:	cmp	x10, #0x0
 114:	lsl	x15, x11, x15
 118:	cset	w10, ne  // ne = any
 11c:	cmp	x12, #0x0
 120:	lsr	x13, x11, x12
 124:	sub	x14, x12, #0x40
 128:	lsr	x9, x9, x12
 12c:	lsr	x11, x11, x12
 130:	csel	x12, xzr, x15, eq  // eq = none
 134:	cmp	x14, #0x0
 138:	orr	x9, x9, x12
 13c:	csel	x9, x11, x9, ge  // ge = tcont
 140:	and	x11, x9, #0xfffffffffffffff
 144:	orr	x10, x11, x10
 148:	mov	x11, #0x1                   	// #1
 14c:	csel	x13, xzr, x13, ge  // ge = tcont
 150:	movk	x11, #0x800, lsl #48
 154:	cmp	x10, x11
 158:	extr	x9, x13, x9, #60
 15c:	b.cc	168 <__trunctfdf2+0x168>  // b.lo, b.ul, b.last
 160:	add	x9, x9, #0x1
 164:	b	170 <__trunctfdf2+0x170>
 168:	eor	x10, x10, #0x800000000000000
 16c:	cbz	x10, 88 <__trunctfdf2+0x88>
 170:	and	x8, x8, #0x8000000000000000
 174:	orr	x8, x9, x8
 178:	fmov	d0, x8
 17c:	add	sp, sp, #0x10
 180:	ret

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	str	q0, [sp, #-16]!
   4:	ldp	x9, x8, [sp]
   8:	mov	x11, #0xc07f000000000000    	// #-4575938696385134592
   c:	mov	x12, #0xbf81000000000000    	// #-4647433340469641216
  10:	and	x10, x8, #0x7fffffffffffffff
  14:	add	x11, x10, x11
  18:	add	x12, x10, x12
  1c:	cmp	x11, x12
  20:	b.cs	60 <__trunctfsf2+0x60>  // b.hs, b.nlast
  24:	cmp	x9, #0x0
  28:	ubfx	x11, x8, #24, #1
  2c:	and	x10, x8, #0x1ffffff
  30:	mov	w12, #0x1000000             	// #16777216
  34:	cset	w13, eq  // eq = none
  38:	cmp	x11, #0x0
  3c:	cset	w11, eq  // eq = none
  40:	cmp	x10, x12
  44:	csel	w12, w13, w11, eq  // eq = none
  48:	lsr	x11, x8, #25
  4c:	tbnz	w12, #0, 88 <__trunctfsf2+0x88>
  50:	mov	w9, #0x1                   	// #1
  54:	movk	w9, #0x4000, lsl #16
  58:	add	w9, w11, w9
  5c:	b	19c <__trunctfsf2+0x19c>
  60:	cmp	x9, #0x0
  64:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
  68:	cset	w12, eq  // eq = none
  6c:	cmp	x10, x11
  70:	cset	w11, cc  // cc = lo, ul, last
  74:	csel	w11, w12, w11, eq  // eq = none
  78:	tbnz	w11, #0, a8 <__trunctfsf2+0xa8>
  7c:	ubfx	x9, x8, #25, #22
  80:	orr	w9, w9, #0x7fc00000
  84:	b	19c <__trunctfsf2+0x19c>
  88:	mov	w12, #0x40000000            	// #1073741824
  8c:	eor	x10, x10, #0x1000000
  90:	orr	x10, x9, x10
  94:	add	w9, w11, w12
  98:	cbnz	x10, 19c <__trunctfsf2+0x19c>
  9c:	and	w10, w9, #0x1
  a0:	add	w9, w10, w9
  a4:	b	19c <__trunctfsf2+0x19c>
  a8:	mov	x11, #0x407effffffffffff    	// #4647433340469641215
  ac:	cmp	x10, x11
  b0:	b.ls	bc <__trunctfsf2+0xbc>  // b.plast
  b4:	mov	w9, #0x7f800000            	// #2139095040
  b8:	b	19c <__trunctfsf2+0x19c>
  bc:	lsr	x10, x10, #48
  c0:	mov	w11, #0x3f11                	// #16145
  c4:	cmp	w10, w11
  c8:	b.cs	d4 <__trunctfsf2+0xd4>  // b.hs, b.nlast
  cc:	mov	w9, wzr
  d0:	b	19c <__trunctfsf2+0x19c>
  d4:	mov	w12, #0x3f81                	// #16257
  d8:	mov	w13, #0xffffc0ff            	// #-16129
  dc:	sub	w12, w12, w10
  e0:	add	w10, w10, w13
  e4:	mov	x11, #0x1000000000000       	// #281474976710656
  e8:	neg	x13, x10
  ec:	bfxil	x11, x8, #0, #48
  f0:	cmp	x10, #0x0
  f4:	sub	x15, x10, #0x40
  f8:	lsr	x13, x9, x13
  fc:	csel	x13, xzr, x13, eq  // eq = none
 100:	cmp	x15, #0x0
 104:	lsl	x15, x11, x10
 108:	orr	x13, x13, x15
 10c:	lsl	x15, x9, x10
 110:	lsl	x10, x9, x10
 114:	csel	x10, x10, x13, ge  // ge = tcont
 118:	csel	x15, xzr, x15, ge  // ge = tcont
 11c:	orr	x10, x15, x10
 120:	neg	x13, x12
 124:	cmp	x10, #0x0
 128:	lsl	x13, x11, x13
 12c:	cset	w16, ne  // ne = any
 130:	cmp	x12, #0x0
 134:	lsr	x9, x9, x12
 138:	lsr	x15, x11, x12
 13c:	lsr	x11, x11, x12
 140:	sub	x10, x12, #0x40
 144:	csel	x12, xzr, x13, eq  // eq = none
 148:	cmp	x10, #0x0
 14c:	orr	x9, x9, x12
 150:	csel	x9, x15, x9, ge  // ge = tcont
 154:	csel	x12, xzr, x11, ge  // ge = tcont
 158:	orr	x11, x9, x16
 15c:	ubfx	x13, x12, #24, #1
 160:	cmp	x11, #0x0
 164:	mov	w14, #0x1000000             	// #16777216
 168:	and	x10, x12, #0x1ffffff
 16c:	cset	w9, eq  // eq = none
 170:	cmp	x13, #0x0
 174:	cset	w13, eq  // eq = none
 178:	cmp	x10, x14
 17c:	csel	w13, w9, w13, eq  // eq = none
 180:	lsr	x9, x12, #25
 184:	tbnz	w13, #0, 190 <__trunctfsf2+0x190>
 188:	add	w9, w9, #0x1
 18c:	b	19c <__trunctfsf2+0x19c>
 190:	eor	x10, x10, #0x1000000
 194:	orr	x10, x11, x10
 198:	cbz	x10, 9c <__trunctfsf2+0x9c>
 19c:	lsr	x8, x8, #32
 1a0:	and	w8, w8, #0x80000000
 1a4:	orr	w8, w9, w8
 1a8:	fmov	s0, w8
 1ac:	add	sp, sp, #0x10
 1b0:	ret

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	18 <__absvdi2+0x18>  // b.none
   c:	cmp	x0, #0x0
  10:	cneg	x0, x0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvdi2>
  24:	adrp	x2, 0 <__absvdi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	18 <__absvsi2+0x18>  // b.none
   c:	cmp	w0, #0x0
  10:	cneg	w0, w0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvsi2>
  24:	adrp	x2, 0 <__absvsi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 24 <__absvti2+0x24>
   c:	negs	x8, x0
  10:	ngcs	x9, x1
  14:	cmp	x1, #0x0
  18:	csel	x0, x8, x0, lt  // lt = tstop
  1c:	csel	x1, x9, x1, lt  // lt = tstop
  20:	ret
  24:	stp	x29, x30, [sp, #-16]!
  28:	mov	x29, sp
  2c:	adrp	x0, 0 <__absvti2>
  30:	adrp	x2, 0 <__absvti2>
  34:	add	x0, x0, #0x0
  38:	add	x2, x2, #0x0
  3c:	mov	w1, #0x18                  	// #24
  40:	bl	0 <__compilerrt_abort_impl>

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	fmov	x8, d0
  14:	and	x10, x8, #0x7fffffffffffffff
  18:	mov	x12, #0xfffffffffffffffe    	// #-2
  1c:	fmov	x9, d1
  20:	sub	x11, x10, #0x1
  24:	movk	x12, #0x7fef, lsl #48
  28:	cmp	x11, x12
  2c:	and	x11, x9, #0x7fffffffffffffff
  30:	b.hi	a0 <__adddf3+0xa0>  // b.pmore
  34:	sub	x13, x11, #0x1
  38:	cmp	x13, x12
  3c:	b.hi	a0 <__adddf3+0xa0>  // b.pmore
  40:	cmp	x11, x10
  44:	csel	x10, x8, x9, hi  // hi = pmore
  48:	csel	x9, x9, x8, hi  // hi = pmore
  4c:	ubfx	x8, x9, #52, #11
  50:	ubfx	x13, x10, #52, #11
  54:	and	x11, x9, #0xfffffffffffff
  58:	and	x12, x10, #0xfffffffffffff
  5c:	cbz	w8, c8 <__adddf3+0xc8>
  60:	cbz	w13, e4 <__adddf3+0xe4>
  64:	lsl	x12, x12, #3
  68:	eor	x10, x9, x10
  6c:	lsl	x11, x11, #3
  70:	subs	w13, w8, w13
  74:	orr	x12, x12, #0x80000000000000
  78:	b.eq	104 <__adddf3+0x104>  // b.none
  7c:	cmp	w13, #0x3f
  80:	b.hi	100 <__adddf3+0x100>  // b.pmore
  84:	neg	x14, x13
  88:	lsl	x14, x12, x14
  8c:	cmp	x14, #0x0
  90:	cset	w14, ne  // ne = any
  94:	lsr	x12, x12, x13
  98:	orr	x12, x12, x14
  9c:	b	104 <__adddf3+0x104>
  a0:	mov	x12, #0x1                   	// #1
  a4:	movk	x12, #0x7ff0, lsl #48
  a8:	cmp	x10, x12
  ac:	b.cc	b8 <__adddf3+0xb8>  // b.lo, b.ul, b.last
  b0:	orr	x8, x8, #0x8000000000000
  b4:	b	154 <__adddf3+0x154>
  b8:	cmp	x11, x12
  bc:	b.cc	1cc <__adddf3+0x1cc>  // b.lo, b.ul, b.last
  c0:	orr	x8, x9, #0x8000000000000
  c4:	b	154 <__adddf3+0x154>
  c8:	clz	x8, x11
  cc:	mov	w14, #0xfffffff5            	// #-11
  d0:	mov	w15, #0xc                   	// #12
  d4:	add	w14, w8, w14
  d8:	lsl	x11, x11, x14
  dc:	sub	w8, w15, w8
  e0:	cbnz	w13, 64 <__adddf3+0x64>
  e4:	clz	x13, x12
  e8:	mov	w14, #0xfffffff5            	// #-11
  ec:	mov	w15, #0xc                   	// #12
  f0:	add	w14, w13, w14
  f4:	lsl	x12, x12, x14
  f8:	sub	w13, w15, w13
  fc:	b	64 <__adddf3+0x64>
 100:	mov	w12, #0x1                   	// #1
 104:	orr	x11, x11, #0x80000000000000
 108:	tbnz	x10, #63, 124 <__adddf3+0x124>
 10c:	add	x10, x12, x11
 110:	tbz	x10, #56, 144 <__adddf3+0x144>
 114:	and	x11, x10, #0x1
 118:	orr	x10, x11, x10, lsr #1
 11c:	add	w8, w8, #0x1
 120:	b	144 <__adddf3+0x144>
 124:	subs	x10, x11, x12
 128:	b.eq	1f4 <__adddf3+0x1f4>  // b.none
 12c:	lsr	x11, x10, #55
 130:	cbnz	x11, 144 <__adddf3+0x144>
 134:	clz	x11, x10
 138:	sub	w11, w11, #0x8
 13c:	lsl	x10, x10, x11
 140:	sub	w8, w8, w11
 144:	cmp	w8, #0x7ff
 148:	and	x21, x9, #0x8000000000000000
 14c:	b.lt	15c <__adddf3+0x15c>  // b.tstop
 150:	orr	x8, x21, #0x7ff0000000000000
 154:	fmov	d1, x8
 158:	b	244 <__adddf3+0x244>
 15c:	cmp	w8, #0x0
 160:	b.gt	18c <__adddf3+0x18c>
 164:	mov	w9, #0x1                   	// #1
 168:	sub	w9, w9, w8
 16c:	sxtw	x11, w9
 170:	neg	x11, x11
 174:	lsl	x11, x10, x11
 178:	cmp	x11, #0x0
 17c:	cset	w11, ne  // ne = any
 180:	lsr	x9, x10, x9
 184:	mov	w8, wzr
 188:	orr	x10, x9, x11
 18c:	ubfx	x9, x10, #3, #52
 190:	orr	x9, x9, x21
 194:	and	w20, w10, #0x7
 198:	orr	x19, x9, x8, lsl #52
 19c:	bl	0 <__fe_getround>
 1a0:	cmp	w0, #0x2
 1a4:	b.eq	214 <__adddf3+0x214>  // b.none
 1a8:	cmp	w0, #0x1
 1ac:	b.eq	220 <__adddf3+0x220>  // b.none
 1b0:	cbnz	w0, 238 <__adddf3+0x238>
 1b4:	cmp	w20, #0x4
 1b8:	cinc	x19, x19, hi  // hi = pmore
 1bc:	b.ne	238 <__adddf3+0x238>  // b.any
 1c0:	and	x8, x19, #0x1
 1c4:	add	x19, x8, x19
 1c8:	b	23c <__adddf3+0x23c>
 1cc:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
 1d0:	cmp	x10, x12
 1d4:	b.ne	1fc <__adddf3+0x1fc>  // b.any
 1d8:	eor	x8, x9, x8
 1dc:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
 1e0:	mov	x10, #0x7ff8000000000000    	// #9221120237041090560
 1e4:	cmp	x8, x9
 1e8:	fmov	d1, x10
 1ec:	fcsel	d1, d1, d0, eq  // eq = none
 1f0:	b	244 <__adddf3+0x244>
 1f4:	fmov	d1, xzr
 1f8:	b	244 <__adddf3+0x244>
 1fc:	cmp	x11, x12
 200:	b.eq	244 <__adddf3+0x244>  // b.none
 204:	cbz	x10, 258 <__adddf3+0x258>
 208:	mov	v1.16b, v0.16b
 20c:	cbnz	x11, 40 <__adddf3+0x40>
 210:	b	244 <__adddf3+0x244>
 214:	cmp	x21, #0x0
 218:	cset	w8, eq  // eq = none
 21c:	b	228 <__adddf3+0x228>
 220:	cmp	x21, #0x0
 224:	cset	w8, ne  // ne = any
 228:	cmp	w20, #0x0
 22c:	cset	w9, ne  // ne = any
 230:	and	w8, w8, w9
 234:	add	x19, x19, x8
 238:	cbz	w20, 240 <__adddf3+0x240>
 23c:	bl	0 <__fe_raise_inexact>
 240:	fmov	d1, x19
 244:	ldp	x20, x19, [sp, #32]
 248:	ldr	x21, [sp, #16]
 24c:	mov	v0.16b, v1.16b
 250:	ldp	x29, x30, [sp], #48
 254:	ret
 258:	cbnz	x11, 244 <__adddf3+0x244>
 25c:	and	x8, x9, x8
 260:	b	154 <__adddf3+0x154>

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	fmov	w8, s0
  14:	and	w10, w8, #0x7fffffff
  18:	mov	w12, #0xfffe                	// #65534
  1c:	fmov	w9, s1
  20:	sub	w11, w10, #0x1
  24:	movk	w12, #0x7f7f, lsl #16
  28:	cmp	w11, w12
  2c:	and	w11, w9, #0x7fffffff
  30:	b.hi	d8 <__addsf3+0xd8>  // b.pmore
  34:	sub	w13, w11, #0x1
  38:	cmp	w13, w12
  3c:	b.hi	d8 <__addsf3+0xd8>  // b.pmore
  40:	cmp	w11, w10
  44:	csel	w13, w8, w9, hi  // hi = pmore
  48:	csel	w9, w9, w8, hi  // hi = pmore
  4c:	and	w15, w9, #0x7fffff
  50:	mov	w11, #0x9                   	// #9
  54:	mov	w12, #0xff                  	// #255
  58:	clz	w17, w15
  5c:	ubfx	w8, w9, #23, #8
  60:	and	w16, w13, #0x7fffff
  64:	tst	w12, w9, lsr #23
  68:	sub	w10, w11, w17
  6c:	sub	w17, w17, #0x8
  70:	csel	w8, w10, w8, eq  // eq = none
  74:	csel	w17, w17, wzr, eq  // eq = none
  78:	tst	w12, w13, lsr #23
  7c:	clz	w12, w16
  80:	ubfx	w14, w13, #23, #8
  84:	eor	w10, w9, w13
  88:	sub	w13, w12, #0x8
  8c:	csel	w13, w13, wzr, eq  // eq = none
  90:	sub	w11, w11, w12
  94:	lsl	w13, w16, w13
  98:	lsl	w12, w15, w17
  9c:	csel	w11, w11, w14, eq  // eq = none
  a0:	lsl	w14, w13, #3
  a4:	lsl	w12, w12, #3
  a8:	subs	w13, w8, w11
  ac:	orr	w11, w14, #0x4000000
  b0:	b.eq	104 <__addsf3+0x104>  // b.none
  b4:	cmp	w13, #0x1f
  b8:	b.hi	100 <__addsf3+0x100>  // b.pmore
  bc:	neg	w14, w13
  c0:	lsl	w14, w11, w14
  c4:	cmp	w14, #0x0
  c8:	cset	w14, ne  // ne = any
  cc:	lsr	w11, w11, w13
  d0:	orr	w11, w11, w14
  d4:	b	104 <__addsf3+0x104>
  d8:	mov	w12, #0x1                   	// #1
  dc:	movk	w12, #0x7f80, lsl #16
  e0:	cmp	w10, w12
  e4:	b.cc	f0 <__addsf3+0xf0>  // b.lo, b.ul, b.last
  e8:	orr	w8, w8, #0x400000
  ec:	b	154 <__addsf3+0x154>
  f0:	cmp	w11, w12
  f4:	b.cc	1c8 <__addsf3+0x1c8>  // b.lo, b.ul, b.last
  f8:	orr	w8, w9, #0x400000
  fc:	b	154 <__addsf3+0x154>
 100:	mov	w11, #0x1                   	// #1
 104:	orr	w12, w12, #0x4000000
 108:	tbnz	w10, #31, 124 <__addsf3+0x124>
 10c:	add	w10, w11, w12
 110:	tbz	w10, #27, 144 <__addsf3+0x144>
 114:	and	w11, w10, #0x1
 118:	orr	w10, w11, w10, lsr #1
 11c:	add	w8, w8, #0x1
 120:	b	144 <__addsf3+0x144>
 124:	subs	w10, w12, w11
 128:	b.eq	1f0 <__addsf3+0x1f0>  // b.none
 12c:	lsr	w11, w10, #26
 130:	cbnz	w11, 144 <__addsf3+0x144>
 134:	clz	w11, w10
 138:	sub	w11, w11, #0x5
 13c:	lsl	w10, w10, w11
 140:	sub	w8, w8, w11
 144:	cmp	w8, #0xff
 148:	and	w21, w9, #0x80000000
 14c:	b.lt	15c <__addsf3+0x15c>  // b.tstop
 150:	orr	w8, w21, #0x7f800000
 154:	fmov	s1, w8
 158:	b	240 <__addsf3+0x240>
 15c:	cmp	w8, #0x0
 160:	b.gt	188 <__addsf3+0x188>
 164:	mov	w9, #0x1                   	// #1
 168:	add	w11, w8, #0x1f
 16c:	sub	w8, w9, w8
 170:	lsl	w9, w10, w11
 174:	cmp	w9, #0x0
 178:	cset	w9, ne  // ne = any
 17c:	lsr	w8, w10, w8
 180:	orr	w10, w8, w9
 184:	mov	w8, wzr
 188:	ubfx	w9, w10, #3, #23
 18c:	orr	w8, w21, w8, lsl #23
 190:	and	w20, w10, #0x7
 194:	orr	w19, w8, w9
 198:	bl	0 <__fe_getround>
 19c:	cmp	w0, #0x2
 1a0:	b.eq	210 <__addsf3+0x210>  // b.none
 1a4:	cmp	w0, #0x1
 1a8:	b.eq	21c <__addsf3+0x21c>  // b.none
 1ac:	cbnz	w0, 234 <__addsf3+0x234>
 1b0:	cmp	w20, #0x4
 1b4:	cinc	w19, w19, hi  // hi = pmore
 1b8:	b.ne	234 <__addsf3+0x234>  // b.any
 1bc:	and	w8, w19, #0x1
 1c0:	add	w19, w8, w19
 1c4:	b	238 <__addsf3+0x238>
 1c8:	mov	w12, #0x7f800000            	// #2139095040
 1cc:	cmp	w10, w12
 1d0:	b.ne	1f8 <__addsf3+0x1f8>  // b.any
 1d4:	eor	w8, w9, w8
 1d8:	mov	w9, #0x80000000            	// #-2147483648
 1dc:	mov	w10, #0x7fc00000            	// #2143289344
 1e0:	cmp	w8, w9
 1e4:	fmov	s1, w10
 1e8:	fcsel	s1, s1, s0, eq  // eq = none
 1ec:	b	240 <__addsf3+0x240>
 1f0:	fmov	s1, wzr
 1f4:	b	240 <__addsf3+0x240>
 1f8:	cmp	w11, w12
 1fc:	b.eq	240 <__addsf3+0x240>  // b.none
 200:	cbz	w10, 254 <__addsf3+0x254>
 204:	mov	v1.16b, v0.16b
 208:	cbnz	w11, 40 <__addsf3+0x40>
 20c:	b	240 <__addsf3+0x240>
 210:	cmp	w21, #0x0
 214:	cset	w8, eq  // eq = none
 218:	b	224 <__addsf3+0x224>
 21c:	cmp	w21, #0x0
 220:	cset	w8, ne  // ne = any
 224:	cmp	w20, #0x0
 228:	cset	w9, ne  // ne = any
 22c:	and	w8, w8, w9
 230:	add	w19, w19, w8
 234:	cbz	w20, 23c <__addsf3+0x23c>
 238:	bl	0 <__fe_raise_inexact>
 23c:	fmov	s1, w19
 240:	ldp	x20, x19, [sp, #32]
 244:	ldr	x21, [sp, #16]
 248:	mov	v0.16b, v1.16b
 24c:	ldp	x29, x30, [sp], #48
 250:	ret
 254:	cbnz	w11, 240 <__addsf3+0x240>
 258:	and	w8, w9, w8
 25c:	b	154 <__addsf3+0x154>

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	sub	sp, sp, #0xa0
   4:	stp	x29, x30, [sp, #112]
   8:	stp	x22, x21, [sp, #128]
   c:	stp	x20, x19, [sp, #144]
  10:	add	x29, sp, #0x70
  14:	stp	q1, q0, [x29, #-32]
  18:	ldp	x10, x8, [x29, #-16]
  1c:	mov	x15, #0xffffffffffffffff    	// #-1
  20:	ldp	x13, x9, [x29, #-32]
  24:	mov	x14, #0x7ffeffffffffffff    	// #9223090561878065151
  28:	and	x12, x8, #0x7fffffffffffffff
  2c:	subs	x16, x10, #0x1
  30:	adcs	x17, x12, x15
  34:	cmn	x16, #0x1
  38:	cset	w16, eq  // eq = none
  3c:	cmp	x17, x14
  40:	cset	w17, hi  // hi = pmore
  44:	and	x11, x9, #0x7fffffffffffffff
  48:	csel	w17, w16, w17, eq  // eq = none
  4c:	subs	x16, x13, #0x1
  50:	adcs	x15, x11, x15
  54:	tbnz	w17, #0, 1f0 <__addtf3+0x1f0>
  58:	cmn	x16, #0x1
  5c:	cset	w16, ne  // ne = any
  60:	cmp	x15, x14
  64:	cset	w14, cc  // cc = lo, ul, last
  68:	csel	w14, w16, w14, eq  // eq = none
  6c:	tbz	w14, #0, 1f0 <__addtf3+0x1f0>
  70:	cmp	x13, x10
  74:	cset	w14, hi  // hi = pmore
  78:	cmp	x11, x12
  7c:	cset	w11, hi  // hi = pmore
  80:	csel	w11, w14, w11, eq  // eq = none
  84:	cmp	w11, #0x0
  88:	csel	x11, x8, x9, ne  // ne = any
  8c:	csel	x9, x9, x8, ne  // ne = any
  90:	csel	x12, x10, x13, ne  // ne = any
  94:	csel	x10, x13, x10, ne  // ne = any
  98:	ubfx	x8, x9, #48, #15
  9c:	ubfx	x14, x11, #48, #15
  a0:	and	x13, x9, #0xffffffffffff
  a4:	cbnz	w8, fc <__addtf3+0xfc>
  a8:	cmp	x13, #0x0
  ac:	csel	x16, x10, x13, eq  // eq = none
  b0:	cset	w15, eq  // eq = none
  b4:	clz	x16, x16
  b8:	add	w15, w16, w15, lsl #6
  bc:	sub	w16, w15, #0xf
  c0:	neg	x17, x16
  c4:	cmp	x16, #0x0
  c8:	lsl	x18, x10, x16
  cc:	lsl	x13, x13, x16
  d0:	lsr	x17, x10, x17
  d4:	lsl	x10, x10, x16
  d8:	sub	x16, x16, #0x40
  dc:	csel	x17, xzr, x17, eq  // eq = none
  e0:	cmp	x16, #0x0
  e4:	mov	w8, #0x10                  	// #16
  e8:	csel	x16, xzr, x18, ge  // ge = tcont
  ec:	orr	x13, x17, x13
  f0:	csel	x13, x10, x13, ge  // ge = tcont
  f4:	sub	w8, w8, w15
  f8:	mov	x10, x16
  fc:	and	x15, x11, #0xffffffffffff
 100:	cbnz	w14, 158 <__addtf3+0x158>
 104:	cmp	x15, #0x0
 108:	csel	x17, x12, x15, eq  // eq = none
 10c:	cset	w16, eq  // eq = none
 110:	clz	x17, x17
 114:	add	w16, w17, w16, lsl #6
 118:	sub	w17, w16, #0xf
 11c:	neg	x18, x17
 120:	cmp	x17, #0x0
 124:	lsl	x0, x12, x17
 128:	lsl	x15, x15, x17
 12c:	lsr	x18, x12, x18
 130:	lsl	x12, x12, x17
 134:	sub	x17, x17, #0x40
 138:	csel	x18, xzr, x18, eq  // eq = none
 13c:	cmp	x17, #0x0
 140:	mov	w14, #0x10                  	// #16
 144:	csel	x17, xzr, x0, ge  // ge = tcont
 148:	orr	x15, x18, x15
 14c:	csel	x15, x12, x15, ge  // ge = tcont
 150:	sub	w14, w14, w16
 154:	mov	x12, x17
 158:	extr	x16, x15, x12, #61
 15c:	eor	x11, x9, x11
 160:	extr	x13, x13, x10, #61
 164:	lsl	x15, x12, #3
 168:	subs	w14, w8, w14
 16c:	orr	x12, x16, #0x8000000000000
 170:	b.eq	24c <__addtf3+0x24c>  // b.none
 174:	cmp	w14, #0x7f
 178:	b.hi	244 <__addtf3+0x244>  // b.pmore
 17c:	mov	w16, #0x80                  	// #128
 180:	subs	x16, x16, x14
 184:	neg	x16, x16
 188:	neg	x17, x14
 18c:	mov	w18, #0x40                  	// #64
 190:	lsr	x16, x15, x16
 194:	lsr	x0, x15, x14
 198:	sub	x18, x18, x14
 19c:	csel	x16, xzr, x16, eq  // eq = none
 1a0:	lsl	x15, x15, x17
 1a4:	lsl	x17, x12, x17
 1a8:	cmp	x18, #0x0
 1ac:	orr	x16, x16, x17
 1b0:	csel	x16, x15, x16, ge  // ge = tcont
 1b4:	csel	x15, xzr, x15, ge  // ge = tcont
 1b8:	orr	x15, x15, x16
 1bc:	cmp	x15, #0x0
 1c0:	cset	w15, ne  // ne = any
 1c4:	cmp	x14, #0x0
 1c8:	lsr	x18, x12, x14
 1cc:	sub	x16, x14, #0x40
 1d0:	lsr	x12, x12, x14
 1d4:	csel	x14, xzr, x17, eq  // eq = none
 1d8:	cmp	x16, #0x0
 1dc:	orr	x14, x0, x14
 1e0:	csel	x14, x18, x14, ge  // ge = tcont
 1e4:	csel	x12, xzr, x12, ge  // ge = tcont
 1e8:	orr	x15, x14, x15
 1ec:	b	24c <__addtf3+0x24c>
 1f0:	cmp	x10, #0x0
 1f4:	mov	x14, #0x7fff000000000000    	// #9223090561878065152
 1f8:	cset	w15, eq  // eq = none
 1fc:	cmp	x12, x14
 200:	cset	w16, cc  // cc = lo, ul, last
 204:	csel	w15, w15, w16, eq  // eq = none
 208:	tbnz	w15, #0, 21c <__addtf3+0x21c>
 20c:	orr	x8, x8, #0x800000000000
 210:	stp	x10, x8, [sp]
 214:	ldr	q1, [sp]
 218:	b	48c <__addtf3+0x48c>
 21c:	cmp	x13, #0x0
 220:	cset	w15, eq  // eq = none
 224:	cmp	x11, x14
 228:	cset	w14, cc  // cc = lo, ul, last
 22c:	csel	w14, w15, w14, eq  // eq = none
 230:	tbnz	w14, #0, 3f0 <__addtf3+0x3f0>
 234:	orr	x8, x9, #0x800000000000
 238:	stp	x13, x8, [sp, #16]
 23c:	ldr	q1, [sp, #16]
 240:	b	48c <__addtf3+0x48c>
 244:	mov	x12, xzr
 248:	mov	w15, #0x1                   	// #1
 24c:	lsl	x14, x10, #3
 250:	orr	x10, x13, #0x8000000000000
 254:	tbnz	x11, #63, 27c <__addtf3+0x27c>
 258:	adds	x11, x15, x14
 25c:	adcs	x10, x12, x10
 260:	tbz	x10, #52, 2e4 <__addtf3+0x2e4>
 264:	and	x12, x11, #0x1
 268:	extr	x11, x10, x11, #1
 26c:	lsr	x10, x10, #1
 270:	orr	x11, x11, x12
 274:	add	w8, w8, #0x1
 278:	b	2e4 <__addtf3+0x2e4>
 27c:	subs	x11, x14, x15
 280:	sbcs	x10, x10, x12
 284:	orr	x12, x11, x10
 288:	cbz	x12, 424 <__addtf3+0x424>
 28c:	lsr	x12, x10, #51
 290:	cbnz	x12, 2e4 <__addtf3+0x2e4>
 294:	cmp	x10, #0x0
 298:	csel	x13, x11, x10, eq  // eq = none
 29c:	cset	w12, eq  // eq = none
 2a0:	clz	x13, x13
 2a4:	add	w12, w13, w12, lsl #6
 2a8:	sub	w12, w12, #0xc
 2ac:	neg	x13, x12
 2b0:	cmp	x12, #0x0
 2b4:	sub	x15, x12, #0x40
 2b8:	lsr	x13, x11, x13
 2bc:	lsl	x14, x11, x12
 2c0:	lsl	x10, x10, x12
 2c4:	csel	x13, xzr, x13, eq  // eq = none
 2c8:	cmp	x15, #0x0
 2cc:	lsl	x11, x11, x12
 2d0:	csel	x14, xzr, x14, ge  // ge = tcont
 2d4:	orr	x10, x13, x10
 2d8:	csel	x10, x11, x10, ge  // ge = tcont
 2dc:	sub	w8, w8, w12
 2e0:	mov	x11, x14
 2e4:	mov	w12, #0x7fff                	// #32767
 2e8:	cmp	w8, w12
 2ec:	and	x21, x9, #0x8000000000000000
 2f0:	b.lt	304 <__addtf3+0x304>  // b.tstop
 2f4:	orr	x8, x21, #0x7fff000000000000
 2f8:	stp	xzr, x8, [sp, #48]
 2fc:	ldr	q1, [sp, #48]
 300:	b	48c <__addtf3+0x48c>
 304:	cmp	w8, #0x0
 308:	b.gt	39c <__addtf3+0x39c>
 30c:	mov	w9, #0x1                   	// #1
 310:	mov	w12, #0x80                  	// #128
 314:	sub	w8, w9, w8
 318:	sub	x12, x12, w8, sxtw
 31c:	mov	w13, #0x40                  	// #64
 320:	sxtw	x9, w8
 324:	cmp	x12, #0x0
 328:	neg	x12, x12
 32c:	sub	x13, x13, w8, sxtw
 330:	neg	x9, x9
 334:	lsr	x12, x11, x12
 338:	csel	x12, xzr, x12, eq  // eq = none
 33c:	cmp	x13, #0x0
 340:	lsl	x13, x10, x9
 344:	orr	x12, x12, x13
 348:	lsl	x9, x11, x9
 34c:	csel	x12, x9, x12, ge  // ge = tcont
 350:	csel	x9, xzr, x9, ge  // ge = tcont
 354:	orr	x9, x9, x12
 358:	neg	x14, x8
 35c:	cmp	x9, #0x0
 360:	lsl	x14, x10, x14
 364:	cset	w9, ne  // ne = any
 368:	cmp	x8, #0x0
 36c:	lsr	x13, x10, x8
 370:	sub	x12, x8, #0x40
 374:	lsr	x11, x11, x8
 378:	lsr	x10, x10, x8
 37c:	csel	x8, xzr, x14, eq  // eq = none
 380:	cmp	x12, #0x0
 384:	orr	x8, x11, x8
 388:	csel	x12, xzr, x13, ge  // ge = tcont
 38c:	csel	x8, x10, x8, ge  // ge = tcont
 390:	orr	x11, x8, x9
 394:	mov	x10, x12
 398:	mov	w8, wzr
 39c:	ubfx	x9, x10, #3, #48
 3a0:	orr	x9, x9, x21
 3a4:	and	w22, w11, #0x7
 3a8:	extr	x19, x10, x11, #3
 3ac:	orr	x20, x9, x8, lsl #48
 3b0:	bl	0 <__fe_getround>
 3b4:	cmp	w0, #0x2
 3b8:	b.eq	454 <__addtf3+0x454>  // b.none
 3bc:	cmp	w0, #0x1
 3c0:	b.eq	460 <__addtf3+0x460>  // b.none
 3c4:	cbnz	w0, 47c <__addtf3+0x47c>
 3c8:	cmp	w22, #0x4
 3cc:	cset	w8, hi  // hi = pmore
 3d0:	adds	x19, x19, x8
 3d4:	adcs	x20, x20, xzr
 3d8:	cmp	w22, #0x4
 3dc:	b.ne	47c <__addtf3+0x47c>  // b.any
 3e0:	and	x8, x19, #0x1
 3e4:	adds	x19, x8, x19
 3e8:	adcs	x20, x20, xzr
 3ec:	b	480 <__addtf3+0x480>
 3f0:	eor	x14, x12, #0x7fff000000000000
 3f4:	orr	x14, x10, x14
 3f8:	cbnz	x14, 430 <__addtf3+0x430>
 3fc:	eor	x8, x9, x8
 400:	eor	x10, x13, x10
 404:	eor	x8, x8, #0x8000000000000000
 408:	orr	x8, x10, x8
 40c:	cmp	x8, #0x0
 410:	b.ne	41c <__addtf3+0x41c>  // b.any
 414:	adrp	x8, 0 <__addtf3>
 418:	ldr	q0, [x8]
 41c:	mov	v1.16b, v0.16b
 420:	b	48c <__addtf3+0x48c>
 424:	adrp	x8, 0 <__addtf3>
 428:	ldr	q1, [x8]
 42c:	b	48c <__addtf3+0x48c>
 430:	eor	x14, x11, #0x7fff000000000000
 434:	orr	x14, x13, x14
 438:	cbz	x14, 48c <__addtf3+0x48c>
 43c:	orr	x14, x10, x12
 440:	cbz	x14, 4a4 <__addtf3+0x4a4>
 444:	orr	x14, x13, x11
 448:	mov	v1.16b, v0.16b
 44c:	cbnz	x14, 70 <__addtf3+0x70>
 450:	b	48c <__addtf3+0x48c>
 454:	cmp	x21, #0x0
 458:	cset	w8, eq  // eq = none
 45c:	b	468 <__addtf3+0x468>
 460:	cmp	x21, #0x0
 464:	cset	w8, ne  // ne = any
 468:	cmp	w22, #0x0
 46c:	cset	w9, ne  // ne = any
 470:	and	w8, w8, w9
 474:	adds	x19, x19, x8
 478:	adcs	x20, x20, xzr
 47c:	cbz	w22, 484 <__addtf3+0x484>
 480:	bl	0 <__fe_raise_inexact>
 484:	stp	x19, x20, [x29, #-48]
 488:	ldur	q1, [x29, #-48]
 48c:	ldp	x20, x19, [sp, #144]
 490:	ldp	x22, x21, [sp, #128]
 494:	ldp	x29, x30, [sp, #112]
 498:	mov	v0.16b, v1.16b
 49c:	add	sp, sp, #0xa0
 4a0:	ret
 4a4:	orr	x11, x13, x11
 4a8:	cbnz	x11, 48c <__addtf3+0x48c>
 4ac:	and	x10, x13, x10
 4b0:	and	x8, x9, x8
 4b4:	stp	x10, x8, [sp, #32]
 4b8:	ldr	q1, [sp, #32]
 4bc:	b	48c <__addtf3+0x48c>

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	add	x0, x1, x0
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__addvdi3+0x34>
  18:	b.ge	38 <__addvdi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvdi3>
  20:	adrp	x2, 0 <__addvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvdi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvdi3>
  44:	adrp	x2, 0 <__addvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	add	w0, w1, w0
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__addvsi3+0x34>
  18:	b.ge	38 <__addvsi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvsi3>
  20:	adrp	x2, 0 <__addvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvsi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvsi3>
  44:	adrp	x2, 0 <__addvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	adds	x0, x2, x0
  10:	mov	x8, x1
  14:	adcs	x1, x3, x1
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__addvti3+0x4c>
  20:	cset	w9, cs  // cs = hs, nlast
  24:	cmp	x1, x8
  28:	cset	w8, ge  // ge = tcont
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__addvti3+0x60>
  34:	adrp	x0, 0 <__addvti3>
  38:	adrp	x2, 0 <__addvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, cc  // cc = lo, ul, last
  50:	cmp	x1, x8
  54:	cset	w8, lt  // lt = tstop
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__addvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__addvti3>
  6c:	adrp	x2, 0 <__addvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	tbnz	w1, #5, 24 <__ashldi3+0x24>
   4:	cbz	w1, 30 <__ashldi3+0x30>
   8:	lsr	x8, x0, #32
   c:	neg	w9, w1
  10:	lsr	w9, w0, w9
  14:	lsl	w8, w8, w1
  18:	lsl	w0, w0, w1
  1c:	orr	w8, w8, w9
  20:	b	2c <__ashldi3+0x2c>
  24:	lsl	w8, w0, w1
  28:	mov	x0, xzr
  2c:	bfi	x0, x8, #32, #32
  30:	ret

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbnz	w2, #6, 24 <__ashlti3+0x24>
   4:	cbz	w2, 34 <__ashlti3+0x34>
   8:	neg	w9, w2
   c:	lsr	x9, x0, x9
  10:	lsl	x10, x1, x2
  14:	mov	x8, xzr
  18:	lsl	x0, x0, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__ashlti3+0x30>
  24:	mov	x8, xzr
  28:	lsl	x9, x0, x2
  2c:	mov	x0, xzr
  30:	orr	x1, x9, x8
  34:	ret

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	lsr	x9, x0, #32
   4:	tbnz	w1, #5, 24 <__ashrdi3+0x24>
   8:	cbz	w1, 34 <__ashrdi3+0x34>
   c:	neg	w10, w1
  10:	asr	w8, w9, w1
  14:	lsl	w9, w9, w10
  18:	lsr	w10, w0, w1
  1c:	orr	w9, w9, w10
  20:	b	2c <__ashrdi3+0x2c>
  24:	asr	w8, w9, #31
  28:	asr	w9, w9, w1
  2c:	mov	w0, w9
  30:	bfi	x0, x8, #32, #32
  34:	ret

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	mov	x8, x1
   4:	tbnz	w2, #6, 24 <__ashrti3+0x24>
   8:	cbz	w2, 30 <__ashrti3+0x30>
   c:	neg	w9, w2
  10:	asr	x1, x8, x2
  14:	lsl	x8, x8, x9
  18:	lsr	x9, x0, x2
  1c:	orr	x0, x8, x9
  20:	ret
  24:	asr	x1, x8, #63
  28:	asr	x0, x8, x2
  2c:	ret
  30:	mov	x1, x8
  34:	ret

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w8, #0x0
   8:	csel	w8, w0, w8, eq  // eq = none
   c:	cset	w9, eq  // eq = none
  10:	clz	w8, w8
  14:	add	w0, w8, w9, lsl #5
  18:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	cmp	w0, #0x10, lsl #12
   4:	mov	w8, #0x10                  	// #16
   8:	cset	w11, cc  // cc = lo, ul, last
   c:	sub	w8, w8, w11, lsl #4
  10:	lsr	w8, w0, w8
  14:	tst	w8, #0xff00
  18:	mov	w9, #0x8                   	// #8
  1c:	cset	w12, eq  // eq = none
  20:	sub	w9, w9, w12, lsl #3
  24:	lsr	w8, w8, w9
  28:	tst	w8, #0xf0
  2c:	mov	w10, #0x4                   	// #4
  30:	cset	w9, eq  // eq = none
  34:	sub	w10, w10, w9, lsl #2
  38:	lsl	w11, w11, #4
  3c:	lsr	w8, w8, w10
  40:	bfi	w11, w12, #3, #1
  44:	tst	w8, #0xc
  48:	mov	w12, #0x2                   	// #2
  4c:	bfi	w11, w9, #2, #1
  50:	cset	w10, eq  // eq = none
  54:	bfi	w11, w10, #1, #1
  58:	sub	w10, w12, w10, lsl #1
  5c:	mov	w9, #0x1                   	// #1
  60:	lsr	w8, w8, w10
  64:	sub	w10, w12, w8
  68:	bic	w8, w9, w8, lsr #1
  6c:	neg	w8, w8
  70:	and	w8, w10, w8
  74:	add	w0, w8, w11
  78:	ret

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csel	x9, x0, x1, eq  // eq = none
   8:	cset	w8, eq  // eq = none
   c:	clz	x9, x9
  10:	add	w0, w9, w8, lsl #6
  14:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.lt	24 <__cmpdi2+0x24>  // b.tstop
  10:	b.le	1c <__cmpdi2+0x1c>
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	w0, w1
  20:	b.cs	2c <__cmpdi2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	cmp	x1, x3
   4:	b.lt	1c <__cmpti2+0x1c>  // b.tstop
   8:	b.le	14 <__cmpti2+0x14>
   c:	mov	w0, #0x2                   	// #2
  10:	ret
  14:	cmp	x0, x2
  18:	b.cs	24 <__cmpti2+0x24>  // b.hs, b.nlast
  1c:	mov	w0, wzr
  20:	ret
  24:	mov	w8, #0x1                   	// #1
  28:	cinc	w0, w8, hi  // hi = pmore
  2c:	ret

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	fmov	x8, d0
   4:	and	x10, x8, #0x7fffffffffffffff
   8:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
   c:	cmp	x10, x12
  10:	mov	w0, #0x1                   	// #1
  14:	b.hi	5c <__cmpdf2+0x5c>  // b.pmore
  18:	fmov	x9, d1
  1c:	and	x11, x9, #0x7fffffffffffffff
  20:	cmp	x11, x12
  24:	b.hi	5c <__cmpdf2+0x5c>  // b.pmore
  28:	orr	x10, x11, x10
  2c:	cbz	x10, 48 <__cmpdf2+0x48>
  30:	tst	x9, x8
  34:	b.lt	50 <__cmpdf2+0x50>  // b.tstop
  38:	cmp	x8, x9
  3c:	b.ge	58 <__cmpdf2+0x58>  // b.tcont
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ret
  48:	mov	w0, wzr
  4c:	ret
  50:	cmp	x8, x9
  54:	b.gt	40 <__cmpdf2+0x40>
  58:	cset	w0, ne  // ne = any
  5c:	ret

0000000000000060 <__gedf2>:
  60:	fmov	x8, d0
  64:	and	x10, x8, #0x7fffffffffffffff
  68:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
  6c:	cmp	x10, x12
  70:	mov	w0, #0xffffffff            	// #-1
  74:	b.hi	bc <__gedf2+0x5c>  // b.pmore
  78:	fmov	x9, d1
  7c:	and	x11, x9, #0x7fffffffffffffff
  80:	cmp	x11, x12
  84:	b.hi	bc <__gedf2+0x5c>  // b.pmore
  88:	orr	x10, x11, x10
  8c:	cbz	x10, a8 <__gedf2+0x48>
  90:	tst	x9, x8
  94:	b.lt	b0 <__gedf2+0x50>  // b.tstop
  98:	cmp	x8, x9
  9c:	b.ge	b8 <__gedf2+0x58>  // b.tcont
  a0:	mov	w0, #0xffffffff            	// #-1
  a4:	ret
  a8:	mov	w0, wzr
  ac:	ret
  b0:	cmp	x8, x9
  b4:	b.gt	a0 <__gedf2+0x40>
  b8:	cset	w0, ne  // ne = any
  bc:	ret

00000000000000c0 <__unorddf2>:
  c0:	fmov	x8, d0
  c4:	and	x8, x8, #0x7fffffffffffffff
  c8:	fmov	x9, d1
  cc:	mov	x10, #0x7ff0000000000000    	// #9218868437227405312
  d0:	and	x9, x9, #0x7fffffffffffffff
  d4:	cmp	x8, x10
  d8:	cset	w8, hi  // hi = pmore
  dc:	cmp	x9, x10
  e0:	cset	w9, hi  // hi = pmore
  e4:	orr	w0, w8, w9
  e8:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	fmov	w8, s0
   4:	and	w10, w8, #0x7fffffff
   8:	mov	w12, #0x7f800000            	// #2139095040
   c:	cmp	w10, w12
  10:	mov	w0, #0x1                   	// #1
  14:	b.hi	5c <__cmpsf2+0x5c>  // b.pmore
  18:	fmov	w9, s1
  1c:	and	w11, w9, #0x7fffffff
  20:	cmp	w11, w12
  24:	b.hi	5c <__cmpsf2+0x5c>  // b.pmore
  28:	orr	w10, w11, w10
  2c:	cbz	w10, 48 <__cmpsf2+0x48>
  30:	tst	w9, w8
  34:	b.lt	50 <__cmpsf2+0x50>  // b.tstop
  38:	cmp	w8, w9
  3c:	b.ge	58 <__cmpsf2+0x58>  // b.tcont
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ret
  48:	mov	w0, wzr
  4c:	ret
  50:	cmp	w8, w9
  54:	b.gt	40 <__cmpsf2+0x40>
  58:	cset	w0, ne  // ne = any
  5c:	ret

0000000000000060 <__gesf2>:
  60:	fmov	w8, s0
  64:	and	w10, w8, #0x7fffffff
  68:	mov	w12, #0x7f800000            	// #2139095040
  6c:	cmp	w10, w12
  70:	mov	w0, #0xffffffff            	// #-1
  74:	b.hi	bc <__gesf2+0x5c>  // b.pmore
  78:	fmov	w9, s1
  7c:	and	w11, w9, #0x7fffffff
  80:	cmp	w11, w12
  84:	b.hi	bc <__gesf2+0x5c>  // b.pmore
  88:	orr	w10, w11, w10
  8c:	cbz	w10, a8 <__gesf2+0x48>
  90:	tst	w9, w8
  94:	b.lt	b0 <__gesf2+0x50>  // b.tstop
  98:	cmp	w8, w9
  9c:	b.ge	b8 <__gesf2+0x58>  // b.tcont
  a0:	mov	w0, #0xffffffff            	// #-1
  a4:	ret
  a8:	mov	w0, wzr
  ac:	ret
  b0:	cmp	w8, w9
  b4:	b.gt	a0 <__gesf2+0x40>
  b8:	cset	w0, ne  // ne = any
  bc:	ret

00000000000000c0 <__unordsf2>:
  c0:	fmov	w8, s0
  c4:	and	w8, w8, #0x7fffffff
  c8:	fmov	w9, s1
  cc:	mov	w10, #0x7f800000            	// #2139095040
  d0:	and	w9, w9, #0x7fffffff
  d4:	cmp	w8, w10
  d8:	cset	w8, hi  // hi = pmore
  dc:	cmp	w9, w10
  e0:	cset	w9, hi  // hi = pmore
  e4:	orr	w0, w8, w9
  e8:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w0, #0x0
   8:	csel	w8, w8, w0, eq  // eq = none
   c:	rbit	w8, w8
  10:	cset	w9, eq  // eq = none
  14:	clz	w8, w8
  18:	add	w0, w8, w9, lsl #5
  1c:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	cset	w9, eq  // eq = none
   8:	lsl	w9, w9, #4
   c:	lsr	w10, w0, w9
  10:	tst	w10, #0xff
  14:	cset	w11, eq  // eq = none
  18:	bfi	w9, w11, #3, #1
  1c:	lsl	w11, w11, #3
  20:	lsr	w10, w10, w11
  24:	tst	w10, #0xf
  28:	cset	w11, eq  // eq = none
  2c:	bfi	w9, w11, #2, #1
  30:	lsl	w11, w11, #2
  34:	lsr	w10, w10, w11
  38:	tst	w10, #0x3
  3c:	cset	w11, eq  // eq = none
  40:	bfi	w9, w11, #1, #1
  44:	lsl	w11, w11, #1
  48:	lsr	w10, w10, w11
  4c:	mov	w8, #0x2                   	// #2
  50:	mvn	w11, w10
  54:	ubfx	w10, w10, #1, #1
  58:	sub	w8, w8, w10
  5c:	and	w10, w11, #0x1
  60:	neg	w10, w10
  64:	and	w8, w8, w10
  68:	add	w0, w8, w9
  6c:	ret

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csel	x9, x1, x0, eq  // eq = none
   8:	rbit	x9, x9
   c:	cset	w8, eq  // eq = none
  10:	clz	x9, x9
  14:	add	w0, w9, w8, lsl #6
  18:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	str	q1, [sp, #32]
  1c:	fabs	d1, d2
  20:	fabs	d4, d3
  24:	fmaxnm	d1, d1, d4
  28:	fmov	x8, d1
  2c:	ubfx	x9, x8, #52, #11
  30:	cmp	w9, #0x7ff
  34:	stur	q0, [x29, #-16]
  38:	b.ne	50 <__divdc3+0x50>  // b.any
  3c:	cmn	x8, #0x1
  40:	fccmp	d1, d1, #0x1, le
  44:	fneg	d0, d1
  48:	fcsel	d9, d1, d0, vs
  4c:	b	90 <__divdc3+0x90>
  50:	fcmp	d1, #0.0
  54:	b.ne	64 <__divdc3+0x64>  // b.any
  58:	mov	x8, #0xfff0000000000000    	// #-4503599627370496
  5c:	fmov	d9, x8
  60:	b	90 <__divdc3+0x90>
  64:	cbz	w9, 70 <__divdc3+0x70>
  68:	sub	w8, w9, #0x3ff
  6c:	b	8c <__divdc3+0x8c>
  70:	and	x8, x8, #0x7fffffffffffffff
  74:	clz	x9, x8
  78:	sub	w10, w9, #0xb
  7c:	lsl	x8, x8, x10
  80:	ubfx	x8, x8, #52, #11
  84:	sub	w8, w8, w9
  88:	sub	w8, w8, #0x3f4
  8c:	scvtf	d9, w8
  90:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  94:	fabs	d10, d9
  98:	fmov	d0, x8
  9c:	fcmp	d10, d0
  a0:	mov	w19, wzr
  a4:	b.eq	e0 <__divdc3+0xe0>  // b.none
  a8:	b.vs	e0 <__divdc3+0xe0>
  ac:	b	b0 <__divdc3+0xb0>
  b0:	fcvtzs	w8, d9
  b4:	neg	w19, w8
  b8:	mov	v0.16b, v2.16b
  bc:	mov	w0, w19
  c0:	str	q3, [sp]
  c4:	bl	0 <scalbn>
  c8:	str	q0, [sp, #16]
  cc:	ldr	q0, [sp]
  d0:	mov	w0, w19
  d4:	bl	0 <scalbn>
  d8:	ldr	q2, [sp, #16]
  dc:	mov	v3.16b, v0.16b
  e0:	ldur	q4, [x29, #-16]
  e4:	ldr	q5, [sp, #32]
  e8:	fmul	d0, d2, d2
  ec:	fmul	d1, d3, d3
  f0:	fmul	d4, d2, d4
  f4:	fmul	d5, d3, d5
  f8:	fadd	d11, d0, d1
  fc:	fadd	d0, d4, d5
 100:	fdiv	d0, d0, d11
 104:	mov	w0, w19
 108:	stp	q3, q2, [sp]
 10c:	bl	0 <scalbn>
 110:	mov	v8.16b, v0.16b
 114:	ldp	q1, q0, [sp, #16]
 118:	ldr	q2, [sp]
 11c:	mov	w0, w19
 120:	fmul	d0, d1, d0
 124:	ldur	q1, [x29, #-16]
 128:	fmul	d1, d2, d1
 12c:	fsub	d0, d0, d1
 130:	fdiv	d0, d0, d11
 134:	bl	0 <scalbn>
 138:	fcmp	d8, d8
 13c:	mov	v1.16b, v0.16b
 140:	b.vc	2f8 <__divdc3+0x2f8>
 144:	fcmp	d1, d1
 148:	b.vc	2f8 <__divdc3+0x2f8>
 14c:	fcmp	d11, #0.0
 150:	b.ne	16c <__divdc3+0x16c>  // b.any
 154:	ldur	q0, [x29, #-16]
 158:	fcmp	d0, d0
 15c:	b.vc	2d0 <__divdc3+0x2d0>
 160:	ldr	q0, [sp, #32]
 164:	fcmp	d0, d0
 168:	b.vc	2d0 <__divdc3+0x2d0>
 16c:	ldur	q0, [x29, #-16]
 170:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 174:	ldr	q7, [sp]
 178:	fabs	d3, d0
 17c:	ldp	q6, q0, [sp, #16]
 180:	fabs	d4, d0
 184:	fmov	d0, x8
 188:	fcmp	d3, d0
 18c:	fabs	d2, d6
 190:	cset	w8, eq  // eq = none
 194:	fcmp	d4, d0
 198:	cset	w9, eq  // eq = none
 19c:	fcmp	d2, d0
 1a0:	fabs	d0, d7
 1a4:	b.eq	22c <__divdc3+0x22c>  // b.none
 1a8:	b.vs	22c <__divdc3+0x22c>
 1ac:	b	1b0 <__divdc3+0x1b0>
 1b0:	orr	w8, w8, w9
 1b4:	cbz	w8, 22c <__divdc3+0x22c>
 1b8:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1bc:	fmov	d5, x8
 1c0:	fcmp	d0, d5
 1c4:	b.eq	22c <__divdc3+0x22c>  // b.none
 1c8:	b.vs	22c <__divdc3+0x22c>
 1cc:	b	1d0 <__divdc3+0x1d0>
 1d0:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1d4:	fmov	d5, x8
 1d8:	fmov	d0, xzr
 1dc:	fmov	d1, #1.000000000000000000e+00
 1e0:	fcmp	d3, d5
 1e4:	fcsel	d3, d1, d0, eq  // eq = none
 1e8:	fcmp	d4, d5
 1ec:	ldur	q4, [x29, #-16]
 1f0:	fcsel	d0, d1, d0, eq  // eq = none
 1f4:	ldr	q1, [sp, #32]
 1f8:	movi	v2.2d, #0x0
 1fc:	fneg	v2.2d, v2.2d
 200:	bit	v3.16b, v4.16b, v2.16b
 204:	bit	v0.16b, v1.16b, v2.16b
 208:	fmul	d1, d3, d6
 20c:	fmul	d2, d3, d7
 210:	fmul	d3, d0, d7
 214:	fmul	d0, d0, d6
 218:	fadd	d1, d1, d3
 21c:	fsub	d0, d0, d2
 220:	fmul	d8, d1, d5
 224:	fmul	d1, d0, d5
 228:	b	2f8 <__divdc3+0x2f8>
 22c:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 230:	fmov	d5, x8
 234:	fcmp	d4, d5
 238:	b.eq	2f8 <__divdc3+0x2f8>  // b.none
 23c:	b.vs	2f8 <__divdc3+0x2f8>
 240:	b	244 <__divdc3+0x244>
 244:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 248:	fmov	d4, x8
 24c:	fcmp	d3, d4
 250:	b.eq	2f8 <__divdc3+0x2f8>  // b.none
 254:	b.vs	2f8 <__divdc3+0x2f8>
 258:	b	25c <__divdc3+0x25c>
 25c:	fcmp	d9, #0.0
 260:	b.le	2f8 <__divdc3+0x2f8>
 264:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 268:	fmov	d3, x8
 26c:	fcmp	d10, d3
 270:	b.ne	2f8 <__divdc3+0x2f8>  // b.any
 274:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 278:	fmov	d5, x8
 27c:	fmov	d1, xzr
 280:	fmov	d3, #1.000000000000000000e+00
 284:	movi	v4.2d, #0x0
 288:	fcmp	d2, d5
 28c:	fneg	v2.2d, v4.2d
 290:	fcsel	d4, d3, d1, eq  // eq = none
 294:	fcmp	d0, d5
 298:	bit	v4.16b, v6.16b, v2.16b
 29c:	ldur	q6, [x29, #-16]
 2a0:	ldr	q5, [sp, #32]
 2a4:	fcsel	d0, d3, d1, eq  // eq = none
 2a8:	bit	v0.16b, v7.16b, v2.16b
 2ac:	fmul	d2, d4, d6
 2b0:	fmul	d3, d4, d5
 2b4:	fmul	d4, d0, d5
 2b8:	fmul	d0, d0, d6
 2bc:	fadd	d2, d2, d4
 2c0:	fsub	d0, d3, d0
 2c4:	fmul	d8, d2, d1
 2c8:	fmul	d1, d0, d1
 2cc:	b	2f8 <__divdc3+0x2f8>
 2d0:	ldr	q2, [sp, #16]
 2d4:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 2d8:	movi	v0.2d, #0x0
 2dc:	fmov	d1, x8
 2e0:	fneg	v0.2d, v0.2d
 2e4:	bit	v1.16b, v2.16b, v0.16b
 2e8:	ldur	q0, [x29, #-16]
 2ec:	fmul	d8, d1, d0
 2f0:	ldr	q0, [sp, #32]
 2f4:	fmul	d1, d1, d0
 2f8:	mov	v0.16b, v8.16b
 2fc:	ldr	x19, [sp, #112]
 300:	ldp	x29, x30, [sp, #96]
 304:	ldp	d9, d8, [sp, #80]
 308:	ldp	d11, d10, [sp, #64]
 30c:	add	sp, sp, #0x80
 310:	ret

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	fmov	x13, d0
   4:	ubfx	x12, x13, #52, #11
   8:	fmov	x11, d1
   c:	eor	x8, x11, x13
  10:	sub	w10, w12, #0x1
  14:	ubfx	x14, x11, #52, #11
  18:	and	x9, x13, #0xfffffffffffff
  1c:	and	x8, x8, #0x8000000000000000
  20:	cmp	w10, #0x7fd
  24:	and	x10, x11, #0xfffffffffffff
  28:	b.hi	120 <__divdf3+0x120>  // b.pmore
  2c:	sub	w15, w14, #0x1
  30:	cmp	w15, #0x7fe
  34:	b.cs	120 <__divdf3+0x120>  // b.hs, b.nlast
  38:	mov	w11, wzr
  3c:	orr	x13, x10, #0x10000000000000
  40:	sub	w12, w12, w14
  44:	mov	w14, #0xf333                	// #62259
  48:	movk	w14, #0x7504, lsl #16
  4c:	lsr	x16, x13, #21
  50:	ubfx	x17, x13, #21, #32
  54:	sub	w14, w14, w16
  58:	mul	x16, x14, x17
  5c:	lsr	x16, x16, #32
  60:	neg	w16, w16
  64:	umull	x14, w16, w14
  68:	ubfx	x14, x14, #31, #32
  6c:	mul	x16, x14, x17
  70:	lsr	x16, x16, #32
  74:	neg	w16, w16
  78:	mul	x14, x16, x14
  7c:	ubfx	x14, x14, #31, #32
  80:	mul	x16, x14, x17
  84:	lsr	x16, x16, #32
  88:	neg	w16, w16
  8c:	mul	x14, x16, x14
  90:	lsr	x14, x14, #31
  94:	lsl	w10, w10, #11
  98:	sub	w14, w14, #0x1
  9c:	mul	x17, x14, x17
  a0:	umull	x10, w14, w10
  a4:	add	x10, x17, x10, lsr #32
  a8:	neg	x10, x10
  ac:	lsr	x17, x10, #32
  b0:	and	x10, x10, #0xffffffff
  b4:	mul	x17, x17, x14
  b8:	mul	x10, x10, x14
  bc:	add	x10, x17, x10, lsr #32
  c0:	lsr	x16, x9, #30
  c4:	sub	x10, x10, #0x2
  c8:	lsl	w15, w9, #2
  cc:	and	x16, x16, #0xffffffff
  d0:	and	x14, x10, #0xffffffff
  d4:	lsr	x10, x10, #32
  d8:	orr	x16, x16, #0x400000
  dc:	mul	x17, x14, x15
  e0:	mul	x15, x10, x15
  e4:	mul	x14, x14, x16
  e8:	mul	x10, x10, x16
  ec:	and	x16, x15, #0xfffffffc
  f0:	add	x16, x16, x17, lsr #32
  f4:	add	x10, x10, x14, lsr #32
  f8:	add	x14, x16, w14, uxtw
  fc:	add	x10, x10, x15, lsr #32
 100:	add	x10, x10, x14, lsr #32
 104:	lsr	x14, x10, #53
 108:	add	w11, w11, w12
 10c:	cbnz	x14, 13c <__divdf3+0x13c>
 110:	lsl	x9, x9, #53
 114:	msub	x9, x10, x13, x9
 118:	sub	w11, w11, #0x1
 11c:	b	148 <__divdf3+0x148>
 120:	mov	x16, #0x1                   	// #1
 124:	and	x15, x13, #0x7fffffffffffffff
 128:	movk	x16, #0x7ff0, lsl #48
 12c:	cmp	x15, x16
 130:	b.cc	17c <__divdf3+0x17c>  // b.lo, b.ul, b.last
 134:	orr	x8, x13, #0x8000000000000
 138:	b	1a0 <__divdf3+0x1a0>
 13c:	lsr	x10, x10, #1
 140:	lsl	x9, x9, #52
 144:	msub	x9, x10, x13, x9
 148:	cmp	w11, #0x400
 14c:	b.lt	158 <__divdf3+0x158>  // b.tstop
 150:	orr	x8, x8, #0x7ff0000000000000
 154:	b	1a0 <__divdf3+0x1a0>
 158:	cmn	w11, #0x3ff
 15c:	add	w11, w11, #0x3ff
 160:	b.gt	190 <__divdf3+0x190>
 164:	cbnz	w11, 1a0 <__divdf3+0x1a0>
 168:	cmp	x13, x9, lsl #1
 16c:	and	x9, x10, #0xfffffffffffff
 170:	cinc	x9, x9, cc  // cc = lo, ul, last
 174:	tbnz	x9, #52, 19c <__divdf3+0x19c>
 178:	b	1a0 <__divdf3+0x1a0>
 17c:	and	x13, x11, #0x7fffffffffffffff
 180:	cmp	x13, x16
 184:	b.cc	1a8 <__divdf3+0x1a8>  // b.lo, b.ul, b.last
 188:	orr	x8, x11, #0x8000000000000
 18c:	b	1a0 <__divdf3+0x1a0>
 190:	cmp	x13, x9, lsl #1
 194:	bfi	x10, x11, #52, #12
 198:	cinc	x9, x10, cc  // cc = lo, ul, last
 19c:	orr	x8, x9, x8
 1a0:	fmov	d0, x8
 1a4:	ret
 1a8:	mov	x11, #0x7ff0000000000000    	// #9218868437227405312
 1ac:	cmp	x15, x11
 1b0:	b.ne	1c4 <__divdf3+0x1c4>  // b.any
 1b4:	cmp	x13, x11
 1b8:	b.ne	150 <__divdf3+0x150>  // b.any
 1bc:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 1c0:	b	1a0 <__divdf3+0x1a0>
 1c4:	cmp	x13, x11
 1c8:	b.eq	1a0 <__divdf3+0x1a0>  // b.none
 1cc:	cbz	x15, 1f8 <__divdf3+0x1f8>
 1d0:	cbz	x13, 150 <__divdf3+0x150>
 1d4:	lsr	x11, x15, #52
 1d8:	cbnz	x11, 210 <__divdf3+0x210>
 1dc:	clz	x11, x9
 1e0:	mov	w15, #0xfffffff5            	// #-11
 1e4:	mov	w16, #0xc                   	// #12
 1e8:	add	w15, w11, w15
 1ec:	lsl	x9, x9, x15
 1f0:	sub	w11, w16, w11
 1f4:	b	214 <__divdf3+0x214>
 1f8:	fmov	d0, x8
 1fc:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 200:	cmp	x13, #0x0
 204:	fmov	d1, x8
 208:	fcsel	d0, d0, d1, ne  // ne = any
 20c:	ret
 210:	mov	w11, wzr
 214:	lsr	x13, x13, #52
 218:	cbnz	x13, 3c <__divdf3+0x3c>
 21c:	clz	x13, x10
 220:	mov	w15, #0xfffffff5            	// #-11
 224:	add	w15, w13, w15
 228:	add	w11, w13, w11
 22c:	lsl	x10, x10, x15
 230:	sub	w11, w11, #0xc
 234:	b	3c <__divdf3+0x3c>

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	eor	x8, x0, x0, asr #63
  10:	asr	x9, x1, #63
  14:	eor	x10, x1, x1, asr #63
  18:	sub	x8, x8, x0, asr #63
  1c:	sub	x1, x10, x1, asr #63
  20:	eor	x19, x9, x0, asr #63
  24:	mov	x0, x8
  28:	mov	x2, xzr
  2c:	bl	0 <__udivmoddi4>
  30:	eor	x8, x0, x19
  34:	sub	x0, x8, x19
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	x20, x1
  18:	mov	x21, x0
  1c:	bl	0 <__divdi3>
  20:	msub	x8, x0, x20, x21
  24:	str	x8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__divsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	str	d1, [sp, #32]
  1c:	fabs	s1, s2
  20:	fabs	s4, s3
  24:	fmaxnm	s1, s1, s4
  28:	fmov	w8, s1
  2c:	ubfx	w9, w8, #23, #8
  30:	cmp	w9, #0xff
  34:	stur	d0, [x29, #-16]
  38:	b.ne	50 <__divsc3+0x50>  // b.any
  3c:	cmn	w8, #0x1
  40:	fccmp	s1, s1, #0x1, le
  44:	fneg	s0, s1
  48:	fcsel	s9, s1, s0, vs
  4c:	b	90 <__divsc3+0x90>
  50:	fcmp	s1, #0.0
  54:	b.ne	64 <__divsc3+0x64>  // b.any
  58:	mov	w8, #0xff800000            	// #-8388608
  5c:	fmov	s9, w8
  60:	b	90 <__divsc3+0x90>
  64:	cbz	w9, 70 <__divsc3+0x70>
  68:	sub	w8, w9, #0x7f
  6c:	b	8c <__divsc3+0x8c>
  70:	and	w8, w8, #0x7fffffff
  74:	clz	w9, w8
  78:	sub	w10, w9, #0x8
  7c:	lsl	w8, w8, w10
  80:	ubfx	w8, w8, #23, #8
  84:	sub	w8, w8, w9
  88:	sub	w8, w8, #0x77
  8c:	scvtf	s9, w8
  90:	mov	w8, #0x7f800000            	// #2139095040
  94:	fabs	s10, s9
  98:	fmov	s0, w8
  9c:	fcmp	s10, s0
  a0:	mov	w19, wzr
  a4:	b.eq	e0 <__divsc3+0xe0>  // b.none
  a8:	b.vs	e0 <__divsc3+0xe0>
  ac:	b	b0 <__divsc3+0xb0>
  b0:	fcvtzs	w8, s9
  b4:	neg	w19, w8
  b8:	mov	v0.16b, v2.16b
  bc:	mov	w0, w19
  c0:	str	q3, [sp]
  c4:	bl	0 <scalbnf>
  c8:	str	d0, [sp, #16]
  cc:	ldr	q0, [sp]
  d0:	mov	w0, w19
  d4:	bl	0 <scalbnf>
  d8:	ldr	q2, [sp, #16]
  dc:	mov	v3.16b, v0.16b
  e0:	ldur	q4, [x29, #-16]
  e4:	ldr	q5, [sp, #32]
  e8:	fmul	s0, s2, s2
  ec:	fmul	s1, s3, s3
  f0:	fmul	s4, s2, s4
  f4:	fmul	s5, s3, s5
  f8:	fadd	s11, s0, s1
  fc:	fadd	s0, s4, s5
 100:	fdiv	s0, s0, s11
 104:	mov	w0, w19
 108:	stp	q3, q2, [sp]
 10c:	bl	0 <scalbnf>
 110:	mov	v8.16b, v0.16b
 114:	ldp	q1, q0, [sp, #16]
 118:	ldr	q2, [sp]
 11c:	mov	w0, w19
 120:	fmul	s0, s1, s0
 124:	ldur	q1, [x29, #-16]
 128:	fmul	s1, s2, s1
 12c:	fsub	s0, s0, s1
 130:	fdiv	s0, s0, s11
 134:	bl	0 <scalbnf>
 138:	fcmp	s8, s8
 13c:	mov	v1.16b, v0.16b
 140:	b.vc	2ec <__divsc3+0x2ec>
 144:	fcmp	s1, s1
 148:	b.vc	2ec <__divsc3+0x2ec>
 14c:	fcmp	s11, #0.0
 150:	b.ne	16c <__divsc3+0x16c>  // b.any
 154:	ldur	q0, [x29, #-16]
 158:	fcmp	s0, s0
 15c:	b.vc	2c8 <__divsc3+0x2c8>
 160:	ldr	q0, [sp, #32]
 164:	fcmp	s0, s0
 168:	b.vc	2c8 <__divsc3+0x2c8>
 16c:	ldur	q0, [x29, #-16]
 170:	mov	w8, #0x7f800000            	// #2139095040
 174:	ldr	q7, [sp]
 178:	fmov	s2, w8
 17c:	fabs	s3, s0
 180:	ldp	q6, q0, [sp, #16]
 184:	fcmp	s3, s2
 188:	cset	w8, eq  // eq = none
 18c:	fabs	s4, s0
 190:	fabs	s0, s6
 194:	fcmp	s4, s2
 198:	cset	w9, eq  // eq = none
 19c:	fcmp	s0, s2
 1a0:	fabs	s2, s7
 1a4:	b.eq	228 <__divsc3+0x228>  // b.none
 1a8:	b.vs	228 <__divsc3+0x228>
 1ac:	b	1b0 <__divsc3+0x1b0>
 1b0:	orr	w8, w8, w9
 1b4:	cbz	w8, 228 <__divsc3+0x228>
 1b8:	mov	w8, #0x7f800000            	// #2139095040
 1bc:	fmov	s5, w8
 1c0:	fcmp	s2, s5
 1c4:	b.eq	228 <__divsc3+0x228>  // b.none
 1c8:	b.vs	228 <__divsc3+0x228>
 1cc:	b	1d0 <__divsc3+0x1d0>
 1d0:	mov	w8, #0x7f800000            	// #2139095040
 1d4:	fmov	s5, w8
 1d8:	fmov	s0, wzr
 1dc:	fmov	s1, #1.000000000000000000e+00
 1e0:	fcmp	s3, s5
 1e4:	fcsel	s3, s1, s0, eq  // eq = none
 1e8:	fcmp	s4, s5
 1ec:	ldur	q4, [x29, #-16]
 1f0:	fcsel	s0, s1, s0, eq  // eq = none
 1f4:	ldr	q1, [sp, #32]
 1f8:	movi	v2.4s, #0x80, lsl #24
 1fc:	bit	v3.16b, v4.16b, v2.16b
 200:	bit	v0.16b, v1.16b, v2.16b
 204:	fmul	s1, s3, s6
 208:	fmul	s2, s3, s7
 20c:	fmul	s3, s0, s7
 210:	fmul	s0, s0, s6
 214:	fadd	s1, s1, s3
 218:	fsub	s0, s0, s2
 21c:	fmul	s8, s1, s5
 220:	fmul	s1, s0, s5
 224:	b	2ec <__divsc3+0x2ec>
 228:	mov	w8, #0x7f800000            	// #2139095040
 22c:	fmov	s5, w8
 230:	fcmp	s4, s5
 234:	b.eq	2ec <__divsc3+0x2ec>  // b.none
 238:	b.vs	2ec <__divsc3+0x2ec>
 23c:	b	240 <__divsc3+0x240>
 240:	mov	w8, #0x7f800000            	// #2139095040
 244:	fmov	s4, w8
 248:	fcmp	s3, s4
 24c:	b.eq	2ec <__divsc3+0x2ec>  // b.none
 250:	b.vs	2ec <__divsc3+0x2ec>
 254:	b	258 <__divsc3+0x258>
 258:	fcmp	s9, #0.0
 25c:	b.le	2ec <__divsc3+0x2ec>
 260:	mov	w8, #0x7f800000            	// #2139095040
 264:	fmov	s3, w8
 268:	fcmp	s10, s3
 26c:	b.ne	2ec <__divsc3+0x2ec>  // b.any
 270:	mov	w8, #0x7f800000            	// #2139095040
 274:	fmov	s5, w8
 278:	fmov	s1, wzr
 27c:	fmov	s3, #1.000000000000000000e+00
 280:	fcmp	s0, s5
 284:	fcsel	s0, s3, s1, eq  // eq = none
 288:	fcmp	s2, s5
 28c:	movi	v4.4s, #0x80, lsl #24
 290:	fcsel	s2, s3, s1, eq  // eq = none
 294:	bit	v0.16b, v6.16b, v4.16b
 298:	bit	v2.16b, v7.16b, v4.16b
 29c:	ldur	q5, [x29, #-16]
 2a0:	ldr	q4, [sp, #32]
 2a4:	fmul	s3, s0, s5
 2a8:	fmul	s0, s0, s4
 2ac:	fmul	s4, s2, s4
 2b0:	fmul	s2, s2, s5
 2b4:	fadd	s3, s3, s4
 2b8:	fsub	s0, s0, s2
 2bc:	fmul	s8, s3, s1
 2c0:	fmul	s1, s0, s1
 2c4:	b	2ec <__divsc3+0x2ec>
 2c8:	ldr	q2, [sp, #16]
 2cc:	mov	w8, #0x7f800000            	// #2139095040
 2d0:	movi	v0.4s, #0x80, lsl #24
 2d4:	fmov	s1, w8
 2d8:	bit	v1.16b, v2.16b, v0.16b
 2dc:	ldur	q0, [x29, #-16]
 2e0:	fmul	s8, s1, s0
 2e4:	ldr	q0, [sp, #32]
 2e8:	fmul	s1, s1, s0
 2ec:	mov	v0.16b, v8.16b
 2f0:	ldr	x19, [sp, #112]
 2f4:	ldp	x29, x30, [sp, #96]
 2f8:	ldp	d9, d8, [sp, #80]
 2fc:	ldp	d11, d10, [sp, #64]
 300:	add	sp, sp, #0x80
 304:	ret

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	fmov	w13, s0
   4:	ubfx	w10, w13, #23, #8
   8:	fmov	w15, s1
   c:	eor	w8, w15, w13
  10:	sub	w12, w10, #0x1
  14:	ubfx	w11, w15, #23, #8
  18:	and	w9, w13, #0x7fffff
  1c:	and	w8, w8, #0x80000000
  20:	cmp	w12, #0xfd
  24:	and	w12, w15, #0x7fffff
  28:	b.hi	c0 <__divsf3+0xc0>  // b.pmore
  2c:	sub	w14, w11, #0x1
  30:	cmp	w14, #0xfe
  34:	b.cs	c0 <__divsf3+0xc0>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	sub	w11, w10, w11
  40:	mov	w10, #0xf333                	// #62259
  44:	orr	w12, w12, #0x800000
  48:	movk	w10, #0x7504, lsl #16
  4c:	lsl	w14, w12, #8
  50:	sub	w10, w10, w12, lsl #8
  54:	umull	x15, w10, w14
  58:	lsr	x15, x15, #32
  5c:	neg	w15, w15
  60:	umull	x10, w15, w10
  64:	ubfx	x10, x10, #31, #32
  68:	mul	x15, x10, x14
  6c:	lsr	x15, x15, #32
  70:	neg	w15, w15
  74:	mul	x10, x15, x10
  78:	ubfx	x10, x10, #31, #32
  7c:	mul	x14, x10, x14
  80:	lsr	x14, x14, #32
  84:	neg	w14, w14
  88:	mul	x10, x14, x10
  8c:	lsl	w14, w9, #1
  90:	lsr	x10, x10, #31
  94:	sub	w10, w10, #0x2
  98:	orr	w14, w14, #0x1000000
  9c:	umull	x10, w10, w14
  a0:	lsr	x14, x10, #56
  a4:	add	w11, w13, w11
  a8:	cbnz	w14, dc <__divsf3+0xdc>
  ac:	lsr	x10, x10, #32
  b0:	lsl	w9, w9, #24
  b4:	msub	w9, w12, w10, w9
  b8:	sub	w11, w11, #0x1
  bc:	b	e8 <__divsf3+0xe8>
  c0:	mov	w16, #0x1                   	// #1
  c4:	and	w14, w13, #0x7fffffff
  c8:	movk	w16, #0x7f80, lsl #16
  cc:	cmp	w14, w16
  d0:	b.cc	11c <__divsf3+0x11c>  // b.lo, b.ul, b.last
  d4:	orr	w8, w13, #0x400000
  d8:	b	140 <__divsf3+0x140>
  dc:	lsr	x10, x10, #33
  e0:	lsl	w9, w9, #23
  e4:	msub	w9, w12, w10, w9
  e8:	cmp	w11, #0x80
  ec:	b.lt	f8 <__divsf3+0xf8>  // b.tstop
  f0:	orr	w8, w8, #0x7f800000
  f4:	b	140 <__divsf3+0x140>
  f8:	cmn	w11, #0x7f
  fc:	add	w11, w11, #0x7f
 100:	b.gt	130 <__divsf3+0x130>
 104:	cbnz	w11, 140 <__divsf3+0x140>
 108:	cmp	w12, w9, lsl #1
 10c:	and	w9, w10, #0x7fffff
 110:	cinc	w9, w9, cc  // cc = lo, ul, last
 114:	tbnz	w9, #23, 13c <__divsf3+0x13c>
 118:	b	140 <__divsf3+0x140>
 11c:	and	w13, w15, #0x7fffffff
 120:	cmp	w13, w16
 124:	b.cc	148 <__divsf3+0x148>  // b.lo, b.ul, b.last
 128:	orr	w8, w15, #0x400000
 12c:	b	140 <__divsf3+0x140>
 130:	cmp	w12, w9, lsl #1
 134:	bfi	w10, w11, #23, #9
 138:	cinc	w9, w10, cc  // cc = lo, ul, last
 13c:	orr	w8, w9, w8
 140:	fmov	s0, w8
 144:	ret
 148:	mov	w15, #0x7f800000            	// #2139095040
 14c:	cmp	w14, w15
 150:	b.ne	164 <__divsf3+0x164>  // b.any
 154:	cmp	w13, w15
 158:	b.ne	f0 <__divsf3+0xf0>  // b.any
 15c:	mov	w8, #0x7fc00000            	// #2143289344
 160:	b	140 <__divsf3+0x140>
 164:	cmp	w13, w15
 168:	b.eq	140 <__divsf3+0x140>  // b.none
 16c:	cbz	w14, 1b4 <__divsf3+0x1b4>
 170:	cbz	w13, f0 <__divsf3+0xf0>
 174:	clz	w15, w9
 178:	mov	w16, #0x9                   	// #9
 17c:	cmp	w14, #0x800, lsl #12
 180:	lsr	w14, w13, #23
 184:	sub	w13, w15, #0x8
 188:	sub	w15, w16, w15
 18c:	csel	w13, w13, wzr, cc  // cc = lo, ul, last
 190:	lsl	w9, w9, w13
 194:	csel	w13, w15, wzr, cc  // cc = lo, ul, last
 198:	cbnz	w14, 3c <__divsf3+0x3c>
 19c:	clz	w14, w12
 1a0:	sub	w15, w14, #0x8
 1a4:	add	w13, w14, w13
 1a8:	lsl	w12, w12, w15
 1ac:	sub	w13, w13, #0x9
 1b0:	b	3c <__divsf3+0x3c>
 1b4:	fmov	s0, w8
 1b8:	mov	w8, #0x7fc00000            	// #2143289344
 1bc:	cmp	w13, #0x0
 1c0:	fmov	s1, w8
 1c4:	fcsel	s0, s0, s1, ne  // ne = any
 1c8:	ret

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	eor	w9, w0, w0, asr #31
   4:	eor	w10, w1, w1, asr #31
   8:	asr	w8, w1, #31
   c:	sub	w9, w9, w0, asr #31
  10:	sub	w10, w10, w1, asr #31
  14:	eor	w8, w8, w0, asr #31
  18:	udiv	w9, w9, w10
  1c:	eor	w9, w9, w8
  20:	sub	w0, w9, w8
  24:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x28, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	sub	sp, sp, #0x200
  18:	str	q1, [sp, #144]
  1c:	str	q2, [sp, #208]
  20:	stur	q2, [x29, #-48]
  24:	ldurb	w8, [x29, #-33]
  28:	and	w8, w8, #0x7f
  2c:	sturb	w8, [x29, #-33]
  30:	ldur	q2, [x29, #-48]
  34:	str	q3, [sp, #192]
  38:	stur	q3, [x29, #-32]
  3c:	ldurb	w8, [x29, #-17]
  40:	and	w8, w8, #0x7f
  44:	sturb	w8, [x29, #-17]
  48:	ldur	q1, [x29, #-32]
  4c:	str	q0, [sp, #160]
  50:	mov	v0.16b, v2.16b
  54:	bl	0 <fmaxl>
  58:	stur	q0, [x29, #-16]
  5c:	ldur	x21, [x29, #-8]
  60:	mov	w8, #0x7fff                	// #32767
  64:	adrp	x20, 0 <__divtc3>
  68:	ubfx	x22, x21, #48, #15
  6c:	cmp	w22, w8
  70:	b.ne	b4 <__divtc3+0xb4>  // b.any
  74:	cmp	x21, #0x0
  78:	mov	v1.16b, v0.16b
  7c:	cset	w19, ge  // ge = tcont
  80:	str	q0, [sp, #176]
  84:	bl	0 <__unordtf2>
  88:	cmp	w0, #0x0
  8c:	cset	w8, ne  // ne = any
  90:	orr	w19, w8, w19
  94:	adrp	x8, 0 <__divtc3>
  98:	ldr	q0, [x8]
  9c:	ldr	q1, [sp, #176]
  a0:	bl	0 <__subtf3>
  a4:	cmp	w19, #0x0
  a8:	b.eq	138 <__divtc3+0x138>  // b.none
  ac:	ldr	q0, [sp, #176]
  b0:	b	138 <__divtc3+0x138>
  b4:	ldr	q1, [x20]
  b8:	ldur	x19, [x29, #-16]
  bc:	bl	0 <__eqtf2>
  c0:	cbnz	w0, d0 <__divtc3+0xd0>
  c4:	adrp	x8, 0 <__divtc3>
  c8:	ldr	q0, [x8]
  cc:	b	138 <__divtc3+0x138>
  d0:	cbz	w22, e0 <__divtc3+0xe0>
  d4:	mov	w8, #0xffffc001            	// #-16383
  d8:	add	w0, w22, w8
  dc:	b	134 <__divtc3+0x134>
  e0:	and	x8, x21, #0x7fffffffffffffff
  e4:	tst	x21, #0x7fffffffffffffff
  e8:	csel	x10, x19, x8, eq  // eq = none
  ec:	cset	w9, eq  // eq = none
  f0:	clz	x10, x10
  f4:	add	w9, w10, w9, lsl #6
  f8:	sub	w10, w9, #0xf
  fc:	neg	x11, x10
 100:	cmp	x10, #0x0
 104:	lsr	x11, x19, x11
 108:	lsl	x8, x8, x10
 10c:	lsl	x12, x19, x10
 110:	sub	x10, x10, #0x40
 114:	csel	x11, xzr, x11, eq  // eq = none
 118:	cmp	x10, #0x0
 11c:	orr	x8, x11, x8
 120:	csel	x8, x12, x8, ge  // ge = tcont
 124:	ubfx	x8, x8, #48, #15
 128:	sub	w8, w8, w9
 12c:	mov	w9, #0xffffc010            	// #-16368
 130:	add	w0, w8, w9
 134:	bl	0 <__floatsitf>
 138:	str	q0, [sp, #64]
 13c:	stur	q0, [x29, #-64]
 140:	ldurb	w8, [x29, #-49]
 144:	and	w8, w8, #0x7f
 148:	sturb	w8, [x29, #-49]
 14c:	adrp	x8, 0 <__divtc3>
 150:	ldur	q0, [x29, #-64]
 154:	ldr	q1, [x8]
 158:	str	q0, [sp, #80]
 15c:	str	q1, [sp, #176]
 160:	bl	0 <__eqtf2>
 164:	ldr	q0, [sp, #80]
 168:	ldr	q1, [sp, #176]
 16c:	cmp	w0, #0x0
 170:	cset	w19, eq  // eq = none
 174:	bl	0 <__unordtf2>
 178:	cmp	w0, #0x0
 17c:	cset	w8, ne  // ne = any
 180:	orr	w8, w8, w19
 184:	cbnz	w8, 1b8 <__divtc3+0x1b8>
 188:	ldr	q0, [sp, #64]
 18c:	bl	0 <__fixtfsi>
 190:	ldr	q0, [sp, #208]
 194:	neg	w19, w0
 198:	mov	w0, w19
 19c:	bl	0 <scalbnl>
 1a0:	str	q0, [sp, #208]
 1a4:	ldr	q0, [sp, #192]
 1a8:	mov	w0, w19
 1ac:	bl	0 <scalbnl>
 1b0:	str	q0, [sp, #192]
 1b4:	b	1bc <__divtc3+0x1bc>
 1b8:	mov	w19, wzr
 1bc:	ldr	q0, [sp, #208]
 1c0:	mov	v1.16b, v0.16b
 1c4:	bl	0 <__multf3>
 1c8:	str	q0, [sp, #128]
 1cc:	ldr	q0, [sp, #192]
 1d0:	mov	v1.16b, v0.16b
 1d4:	bl	0 <__multf3>
 1d8:	mov	v1.16b, v0.16b
 1dc:	ldr	q0, [sp, #128]
 1e0:	bl	0 <__addtf3>
 1e4:	str	q0, [sp, #112]
 1e8:	ldr	q0, [sp, #208]
 1ec:	ldr	q1, [sp, #160]
 1f0:	bl	0 <__multf3>
 1f4:	str	q0, [sp, #128]
 1f8:	ldr	q0, [sp, #192]
 1fc:	ldr	q1, [sp, #144]
 200:	bl	0 <__multf3>
 204:	mov	v1.16b, v0.16b
 208:	ldr	q0, [sp, #128]
 20c:	bl	0 <__addtf3>
 210:	ldr	q1, [sp, #112]
 214:	bl	0 <__divtf3>
 218:	mov	w0, w19
 21c:	bl	0 <scalbnl>
 220:	str	q0, [sp, #128]
 224:	ldr	q0, [sp, #208]
 228:	ldr	q1, [sp, #144]
 22c:	bl	0 <__multf3>
 230:	str	q0, [sp, #96]
 234:	ldr	q0, [sp, #192]
 238:	ldr	q1, [sp, #160]
 23c:	bl	0 <__multf3>
 240:	mov	v1.16b, v0.16b
 244:	ldr	q0, [sp, #96]
 248:	bl	0 <__subtf3>
 24c:	ldr	q1, [sp, #112]
 250:	bl	0 <__divtf3>
 254:	mov	w0, w19
 258:	bl	0 <scalbnl>
 25c:	str	q0, [sp, #96]
 260:	ldr	q0, [sp, #128]
 264:	mov	v1.16b, v0.16b
 268:	bl	0 <__unordtf2>
 26c:	cbz	w0, 618 <__divtc3+0x618>
 270:	ldr	q0, [sp, #96]
 274:	mov	v1.16b, v0.16b
 278:	bl	0 <__unordtf2>
 27c:	cbz	w0, 618 <__divtc3+0x618>
 280:	ldr	q1, [x20]
 284:	ldr	q0, [sp, #112]
 288:	str	q1, [sp, #48]
 28c:	bl	0 <__netf2>
 290:	cbnz	w0, 2b4 <__divtc3+0x2b4>
 294:	ldr	q0, [sp, #160]
 298:	mov	v1.16b, v0.16b
 29c:	bl	0 <__unordtf2>
 2a0:	cbz	w0, 624 <__divtc3+0x624>
 2a4:	ldr	q0, [sp, #144]
 2a8:	mov	v1.16b, v0.16b
 2ac:	bl	0 <__unordtf2>
 2b0:	cbz	w0, 624 <__divtc3+0x624>
 2b4:	ldr	q0, [sp, #160]
 2b8:	stur	q0, [x29, #-160]
 2bc:	ldurb	w8, [x29, #-145]
 2c0:	and	w8, w8, #0x7f
 2c4:	sturb	w8, [x29, #-145]
 2c8:	ldur	q0, [x29, #-160]
 2cc:	ldr	q1, [sp, #176]
 2d0:	str	q0, [sp]
 2d4:	bl	0 <__eqtf2>
 2d8:	ldr	q0, [sp, #144]
 2dc:	cmp	w0, #0x0
 2e0:	cset	w19, eq  // eq = none
 2e4:	stur	q0, [x29, #-144]
 2e8:	ldurb	w8, [x29, #-129]
 2ec:	and	w8, w8, #0x7f
 2f0:	sturb	w8, [x29, #-129]
 2f4:	ldur	q0, [x29, #-144]
 2f8:	ldr	q1, [sp, #176]
 2fc:	str	q0, [sp, #32]
 300:	bl	0 <__eqtf2>
 304:	ldr	q0, [sp, #208]
 308:	cmp	w0, #0x0
 30c:	cset	w21, eq  // eq = none
 310:	stur	q0, [x29, #-128]
 314:	ldurb	w8, [x29, #-113]
 318:	and	w8, w8, #0x7f
 31c:	sturb	w8, [x29, #-113]
 320:	ldur	q1, [x29, #-128]
 324:	ldr	q0, [sp, #192]
 328:	str	q1, [sp, #16]
 32c:	stur	q0, [x29, #-112]
 330:	ldurb	w8, [x29, #-97]
 334:	and	w8, w8, #0x7f
 338:	sturb	w8, [x29, #-97]
 33c:	ldur	q0, [x29, #-112]
 340:	str	q0, [sp, #112]
 344:	mov	v0.16b, v1.16b
 348:	ldr	q1, [sp, #176]
 34c:	bl	0 <__eqtf2>
 350:	ldr	q0, [sp, #16]
 354:	ldr	q1, [sp, #176]
 358:	cmp	w0, #0x0
 35c:	cset	w22, eq  // eq = none
 360:	bl	0 <__unordtf2>
 364:	cmp	w0, #0x0
 368:	cset	w8, ne  // ne = any
 36c:	orr	w8, w8, w22
 370:	cbnz	w8, 49c <__divtc3+0x49c>
 374:	orr	w8, w19, w21
 378:	cbz	w8, 49c <__divtc3+0x49c>
 37c:	ldr	q0, [sp, #112]
 380:	ldr	q1, [sp, #176]
 384:	bl	0 <__eqtf2>
 388:	ldr	q0, [sp, #112]
 38c:	ldr	q1, [sp, #176]
 390:	cmp	w0, #0x0
 394:	cset	w19, eq  // eq = none
 398:	bl	0 <__unordtf2>
 39c:	cmp	w0, #0x0
 3a0:	cset	w8, ne  // ne = any
 3a4:	orr	w8, w8, w19
 3a8:	cbnz	w8, 49c <__divtc3+0x49c>
 3ac:	ldr	q0, [sp]
 3b0:	ldr	q1, [sp, #176]
 3b4:	bl	0 <__eqtf2>
 3b8:	adrp	x8, 0 <__divtc3>
 3bc:	ldr	q1, [x8]
 3c0:	ldr	q0, [sp, #48]
 3c4:	cmp	w0, #0x0
 3c8:	b.ne	3d0 <__divtc3+0x3d0>  // b.any
 3cc:	mov	v0.16b, v1.16b
 3d0:	str	q1, [sp, #128]
 3d4:	ldr	q1, [sp, #160]
 3d8:	stp	q0, q1, [x29, #-192]
 3dc:	ldurb	w8, [x29, #-161]
 3e0:	ldurb	w9, [x29, #-177]
 3e4:	bfxil	w8, w9, #0, #7
 3e8:	sturb	w8, [x29, #-177]
 3ec:	ldur	q0, [x29, #-192]
 3f0:	ldr	q1, [sp, #176]
 3f4:	str	q0, [sp, #160]
 3f8:	ldr	q0, [sp, #32]
 3fc:	bl	0 <__eqtf2>
 400:	cmp	w0, #0x0
 404:	b.ne	410 <__divtc3+0x410>  // b.any
 408:	ldr	q0, [sp, #128]
 40c:	str	q0, [sp, #48]
 410:	ldr	q0, [sp, #144]
 414:	stur	q0, [x29, #-208]
 418:	ldr	q0, [sp, #48]
 41c:	ldurb	w8, [x29, #-193]
 420:	stur	q0, [x29, #-224]
 424:	ldurb	w9, [x29, #-209]
 428:	bfxil	w8, w9, #0, #7
 42c:	sturb	w8, [x29, #-209]
 430:	ldur	q0, [x29, #-224]
 434:	ldr	q1, [sp, #208]
 438:	str	q0, [sp, #144]
 43c:	ldr	q0, [sp, #160]
 440:	bl	0 <__multf3>
 444:	str	q0, [sp, #128]
 448:	ldr	q0, [sp, #144]
 44c:	ldr	q1, [sp, #192]
 450:	bl	0 <__multf3>
 454:	mov	v1.16b, v0.16b
 458:	ldr	q0, [sp, #128]
 45c:	bl	0 <__addtf3>
 460:	ldr	q1, [sp, #176]
 464:	bl	0 <__multf3>
 468:	str	q0, [sp, #128]
 46c:	ldr	q0, [sp, #144]
 470:	ldr	q1, [sp, #208]
 474:	bl	0 <__multf3>
 478:	str	q0, [sp, #208]
 47c:	ldr	q0, [sp, #160]
 480:	ldr	q1, [sp, #192]
 484:	bl	0 <__multf3>
 488:	mov	v1.16b, v0.16b
 48c:	ldr	q0, [sp, #208]
 490:	bl	0 <__subtf3>
 494:	ldr	q1, [sp, #176]
 498:	b	660 <__divtc3+0x660>
 49c:	ldr	q0, [sp, #32]
 4a0:	ldr	q1, [sp, #176]
 4a4:	bl	0 <__eqtf2>
 4a8:	ldr	q0, [sp, #32]
 4ac:	ldr	q1, [sp, #176]
 4b0:	cmp	w0, #0x0
 4b4:	cset	w19, eq  // eq = none
 4b8:	bl	0 <__unordtf2>
 4bc:	cmp	w0, #0x0
 4c0:	cset	w8, ne  // ne = any
 4c4:	orr	w8, w8, w19
 4c8:	cbnz	w8, 618 <__divtc3+0x618>
 4cc:	ldr	q0, [sp]
 4d0:	ldr	q1, [sp, #176]
 4d4:	bl	0 <__eqtf2>
 4d8:	ldr	q0, [sp]
 4dc:	ldr	q1, [sp, #176]
 4e0:	cmp	w0, #0x0
 4e4:	cset	w19, eq  // eq = none
 4e8:	bl	0 <__unordtf2>
 4ec:	cmp	w0, #0x0
 4f0:	cset	w8, ne  // ne = any
 4f4:	orr	w8, w8, w19
 4f8:	cbnz	w8, 618 <__divtc3+0x618>
 4fc:	ldr	q1, [x20]
 500:	ldr	q0, [sp, #64]
 504:	str	q1, [sp, #64]
 508:	bl	0 <__gttf2>
 50c:	cmp	w0, #0x0
 510:	b.le	618 <__divtc3+0x618>
 514:	ldr	q0, [sp, #80]
 518:	ldr	q1, [sp, #176]
 51c:	bl	0 <__netf2>
 520:	ldr	q0, [sp, #128]
 524:	ldr	q1, [sp, #96]
 528:	cbnz	w0, 66c <__divtc3+0x66c>
 52c:	ldr	q0, [sp, #16]
 530:	ldr	q1, [sp, #176]
 534:	bl	0 <__eqtf2>
 538:	adrp	x8, 0 <__divtc3>
 53c:	ldr	q1, [x8]
 540:	ldr	q0, [sp, #64]
 544:	cmp	w0, #0x0
 548:	b.ne	550 <__divtc3+0x550>  // b.any
 54c:	mov	v0.16b, v1.16b
 550:	str	q1, [sp, #128]
 554:	ldr	q1, [sp, #208]
 558:	stp	q0, q1, [sp, #224]
 55c:	ldrb	w8, [sp, #255]
 560:	ldrb	w9, [sp, #239]
 564:	ldr	q1, [sp, #176]
 568:	bfxil	w8, w9, #0, #7
 56c:	strb	w8, [sp, #239]
 570:	ldr	q0, [sp, #224]
 574:	str	q0, [sp, #208]
 578:	ldr	q0, [sp, #112]
 57c:	bl	0 <__eqtf2>
 580:	ldr	q0, [sp, #64]
 584:	cmp	w0, #0x0
 588:	b.ne	590 <__divtc3+0x590>  // b.any
 58c:	ldr	q0, [sp, #128]
 590:	ldr	q1, [sp, #192]
 594:	stur	q1, [x29, #-240]
 598:	str	q0, [sp, #256]
 59c:	ldurb	w8, [x29, #-225]
 5a0:	ldrb	w9, [sp, #271]
 5a4:	ldr	q1, [sp, #160]
 5a8:	bfxil	w8, w9, #0, #7
 5ac:	strb	w8, [sp, #271]
 5b0:	ldr	q0, [sp, #256]
 5b4:	str	q0, [sp, #192]
 5b8:	ldr	q0, [sp, #208]
 5bc:	bl	0 <__multf3>
 5c0:	str	q0, [sp, #176]
 5c4:	ldr	q0, [sp, #192]
 5c8:	ldr	q1, [sp, #144]
 5cc:	bl	0 <__multf3>
 5d0:	mov	v1.16b, v0.16b
 5d4:	ldr	q0, [sp, #176]
 5d8:	bl	0 <__addtf3>
 5dc:	ldr	q1, [sp, #64]
 5e0:	bl	0 <__multf3>
 5e4:	str	q0, [sp, #128]
 5e8:	ldr	q0, [sp, #208]
 5ec:	ldr	q1, [sp, #144]
 5f0:	bl	0 <__multf3>
 5f4:	str	q0, [sp, #208]
 5f8:	ldr	q0, [sp, #192]
 5fc:	ldr	q1, [sp, #160]
 600:	bl	0 <__multf3>
 604:	mov	v1.16b, v0.16b
 608:	ldr	q0, [sp, #208]
 60c:	bl	0 <__subtf3>
 610:	ldr	q1, [sp, #64]
 614:	b	660 <__divtc3+0x660>
 618:	ldr	q0, [sp, #128]
 61c:	ldr	q1, [sp, #96]
 620:	b	66c <__divtc3+0x66c>
 624:	ldr	q0, [sp, #208]
 628:	stur	q0, [x29, #-80]
 62c:	ldr	q0, [sp, #176]
 630:	ldurb	w8, [x29, #-65]
 634:	stur	q0, [x29, #-96]
 638:	ldurb	w9, [x29, #-81]
 63c:	bfxil	w8, w9, #0, #7
 640:	sturb	w8, [x29, #-81]
 644:	ldur	q0, [x29, #-96]
 648:	ldr	q1, [sp, #160]
 64c:	str	q0, [sp, #208]
 650:	bl	0 <__multf3>
 654:	str	q0, [sp, #128]
 658:	ldr	q0, [sp, #208]
 65c:	ldr	q1, [sp, #144]
 660:	bl	0 <__multf3>
 664:	mov	v1.16b, v0.16b
 668:	ldr	q0, [sp, #128]
 66c:	add	sp, sp, #0x200
 670:	ldp	x20, x19, [sp, #48]
 674:	ldp	x22, x21, [sp, #32]
 678:	ldr	x28, [sp, #16]
 67c:	ldp	x29, x30, [sp], #64
 680:	ret

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	eor	x10, x0, x1, asr #63
  10:	asr	x8, x1, #63
  14:	eor	x11, x1, x1, asr #63
  18:	subs	x0, x10, x1, asr #63
  1c:	eor	x12, x2, x3, asr #63
  20:	sbcs	x8, x11, x8
  24:	asr	x9, x3, #63
  28:	eor	x13, x3, x3, asr #63
  2c:	subs	x2, x12, x3, asr #63
  30:	sbcs	x3, x13, x9
  34:	eor	x19, x9, x1, asr #63
  38:	mov	x1, x8
  3c:	mov	x4, xzr
  40:	bl	0 <__udivmodti4>
  44:	eor	x9, x0, x19
  48:	eor	x8, x1, x19
  4c:	subs	x0, x9, x19
  50:	sbcs	x1, x8, x19
  54:	ldr	x19, [sp, #16]
  58:	ldp	x29, x30, [sp], #32
  5c:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	sub	sp, sp, #0xe0
   4:	stp	x22, x21, [sp, #192]
   8:	stp	x20, x19, [sp, #208]
   c:	stp	q1, q0, [sp, #160]
  10:	ldp	x11, x12, [sp, #176]
  14:	ldp	x9, x17, [sp, #160]
  18:	mov	w10, #0x7ffd                	// #32765
  1c:	ubfx	x15, x12, #48, #15
  20:	eor	x8, x17, x12
  24:	sub	w14, w15, #0x1
  28:	and	x13, x12, #0xffffffffffff
  2c:	ubfx	x16, x17, #48, #15
  30:	and	x8, x8, #0x8000000000000000
  34:	cmp	w14, w10
  38:	and	x10, x17, #0xffffffffffff
  3c:	b.hi	344 <__divtf3+0x344>  // b.pmore
  40:	sub	w14, w16, #0x1
  44:	mov	w18, #0x7ffe                	// #32766
  48:	cmp	w14, w18
  4c:	b.cs	344 <__divtf3+0x344>  // b.hs, b.nlast
  50:	mov	w14, wzr
  54:	sub	w15, w15, w16
  58:	mov	x16, #0x6484                	// #25732
  5c:	movk	x16, #0xf9de, lsl #16
  60:	orr	x12, x10, #0x1000000000000
  64:	movk	x16, #0xf333, lsl #32
  68:	movk	x16, #0x7504, lsl #48
  6c:	lsr	x1, x13, #30
  70:	extr	x4, x12, x9, #49
  74:	extr	x13, x13, x11, #62
  78:	and	x1, x1, #0xffffffff
  7c:	sub	x16, x16, x4
  80:	and	x5, x13, #0xffffffff
  84:	orr	x13, x1, #0x40000
  88:	umulh	x1, x4, x16
  8c:	neg	x19, x1
  90:	mneg	x1, x1, x16
  94:	umulh	x16, x19, x16
  98:	extr	x16, x16, x1, #63
  9c:	umulh	x1, x16, x4
  a0:	neg	x19, x1
  a4:	mneg	x1, x1, x16
  a8:	umulh	x16, x16, x19
  ac:	extr	x16, x16, x1, #63
  b0:	umulh	x1, x16, x4
  b4:	neg	x19, x1
  b8:	mneg	x1, x1, x16
  bc:	umulh	x16, x16, x19
  c0:	extr	x16, x16, x1, #63
  c4:	umulh	x1, x16, x4
  c8:	neg	x19, x1
  cc:	mneg	x1, x1, x16
  d0:	umulh	x16, x16, x19
  d4:	extr	x16, x16, x1, #63
  d8:	umulh	x1, x16, x4
  dc:	lsr	x6, x4, #32
  e0:	and	x7, x4, #0xffffffff
  e4:	neg	x4, x1
  e8:	mneg	x1, x1, x16
  ec:	umulh	x16, x16, x4
  f0:	extr	x16, x16, x1, #63
  f4:	sub	x16, x16, #0x1
  f8:	lsr	x1, x16, #32
  fc:	and	x16, x16, #0xffffffff
 100:	mul	x4, x1, x7
 104:	mul	x19, x16, x6
 108:	adds	x4, x19, x4
 10c:	mul	x7, x16, x7
 110:	adcs	x19, xzr, xzr
 114:	lsl	w17, w9, #15
 118:	ubfx	x18, x9, #17, #32
 11c:	adds	x7, x7, x4, lsl #32
 120:	mul	x20, x1, x17
 124:	mul	x21, x16, x18
 128:	extr	x4, x19, x4, #32
 12c:	adcs	x19, xzr, xzr
 130:	madd	x4, x1, x6, x4
 134:	adds	x6, x21, x20
 138:	mul	x17, x16, x17
 13c:	add	x4, x19, x4
 140:	adcs	x19, xzr, xzr
 144:	cmn	x17, x6, lsl #32
 148:	extr	x17, x19, x6, #32
 14c:	adcs	x6, xzr, xzr
 150:	madd	x17, x1, x18, x17
 154:	add	x17, x6, x17
 158:	adds	x17, x7, x17
 15c:	adcs	x18, x4, xzr
 160:	negs	x17, x17
 164:	ngcs	x18, x18
 168:	lsr	x6, x18, #32
 16c:	and	x18, x18, #0xffffffff
 170:	lsr	x4, x17, #32
 174:	and	x17, x17, #0xffffffff
 178:	mul	x20, x18, x1
 17c:	mul	x21, x16, x6
 180:	mul	x7, x17, x1
 184:	mul	x19, x4, x16
 188:	mul	x17, x17, x16
 18c:	mul	x16, x18, x16
 190:	adds	x18, x20, x21
 194:	adcs	x20, xzr, xzr
 198:	adds	x7, x19, x7
 19c:	adcs	x19, xzr, xzr
 1a0:	cmn	x17, x7, lsl #32
 1a4:	extr	x17, x19, x7, #32
 1a8:	adcs	x7, xzr, xzr
 1ac:	extr	x20, x20, x18, #32
 1b0:	madd	x17, x4, x1, x17
 1b4:	adds	x16, x16, x18, lsl #32
 1b8:	madd	x6, x1, x6, x20
 1bc:	add	x17, x7, x17
 1c0:	adcs	x18, xzr, xzr
 1c4:	add	x18, x18, x6
 1c8:	adds	x16, x16, x17
 1cc:	adcs	x17, x18, xzr
 1d0:	subs	x16, x16, #0x2
 1d4:	mov	x0, #0xffffffffffffffff    	// #-1
 1d8:	ubfx	x2, x11, #30, #32
 1dc:	lsl	w3, w11, #2
 1e0:	lsr	x18, x16, #32
 1e4:	and	x16, x16, #0xffffffff
 1e8:	adcs	x17, x17, x0
 1ec:	mul	x1, x18, x13
 1f0:	mul	x4, x16, x13
 1f4:	mul	x6, x18, x5
 1f8:	mul	x7, x16, x5
 1fc:	mul	x19, x18, x2
 200:	mul	x16, x16, x2
 204:	mul	x18, x18, x3
 208:	adds	x16, x18, x16
 20c:	adcs	x18, xzr, xzr
 210:	lsr	x0, x17, #32
 214:	and	x17, x17, #0xffffffff
 218:	adds	x7, x19, x7
 21c:	mul	x20, x17, x13
 220:	mul	x21, x5, x0
 224:	mul	x5, x17, x5
 228:	mul	x22, x2, x0
 22c:	mul	x2, x17, x2
 230:	mul	x17, x17, x3
 234:	extr	x16, x18, x16, #32
 238:	adcs	x18, xzr, xzr
 23c:	adds	x17, x7, x17
 240:	mul	x3, x3, x0
 244:	adcs	x18, x18, xzr
 248:	adds	x3, x3, x4
 24c:	adcs	x4, xzr, xzr
 250:	adds	x3, x3, x6
 254:	adcs	x4, x4, xzr
 258:	adds	x2, x3, x2
 25c:	adcs	x3, x4, xzr
 260:	adds	x4, x20, x21
 264:	adcs	x6, xzr, xzr
 268:	adds	x16, x17, x16
 26c:	extr	x17, x6, x4, #32
 270:	adcs	x6, xzr, xzr
 274:	cmn	x16, x2, lsl #32
 278:	adcs	x16, x6, xzr
 27c:	extr	x3, x3, x2, #32
 280:	adds	x2, x5, x22
 284:	adcs	x5, xzr, xzr
 288:	adds	x1, x2, x1
 28c:	adcs	x2, x5, xzr
 290:	madd	x13, x13, x0, x2
 294:	adds	x0, x1, x4, lsl #32
 298:	adcs	x13, x13, x17
 29c:	adds	x17, x0, x18
 2a0:	adcs	x13, x13, xzr
 2a4:	adds	x17, x17, x3
 2a8:	adcs	x18, x13, xzr
 2ac:	adds	x13, x17, x16
 2b0:	adcs	x16, x18, xzr
 2b4:	lsr	x17, x16, #49
 2b8:	add	w14, w14, w15
 2bc:	cbnz	x17, 374 <__divtf3+0x374>
 2c0:	extr	x3, x16, x13, #32
 2c4:	lsr	x15, x16, #32
 2c8:	and	x18, x10, #0xffffffff
 2cc:	lsr	x0, x9, #32
 2d0:	and	x1, x9, #0xffffffff
 2d4:	and	x5, x13, #0xffffffff
 2d8:	and	x3, x3, #0xffffffff
 2dc:	lsr	x17, x12, #32
 2e0:	and	x2, x16, #0xffffffff
 2e4:	mul	w15, w9, w15
 2e8:	mul	x18, x5, x18
 2ec:	mul	x6, x5, x0
 2f0:	mul	x7, x3, x1
 2f4:	madd	w15, w13, w17, w15
 2f8:	madd	x17, x2, x1, x18
 2fc:	adds	x18, x7, x6
 300:	lsr	x4, x13, #32
 304:	mul	x5, x5, x1
 308:	madd	w15, w16, w0, w15
 30c:	madd	x17, x3, x0, x17
 310:	adcs	x0, xzr, xzr
 314:	lsl	x11, x11, #49
 318:	madd	w10, w4, w10, w15
 31c:	extr	x15, x0, x18, #32
 320:	negs	x0, x5
 324:	add	x15, x17, x15
 328:	sbcs	x11, x11, xzr
 32c:	add	x15, x15, x10, lsl #32
 330:	subs	x10, x0, x18, lsl #32
 334:	sbcs	x11, x11, xzr
 338:	sub	x11, x11, x15
 33c:	sub	w14, w14, #0x1
 340:	b	3f8 <__divtf3+0x3f8>
 344:	and	x14, x12, #0x7fffffffffffffff
 348:	cmp	x11, #0x0
 34c:	mov	x18, #0x7fff000000000000    	// #9223090561878065152
 350:	cset	w0, eq  // eq = none
 354:	cmp	x14, x18
 358:	cset	w1, cc  // cc = lo, ul, last
 35c:	csel	w0, w0, w1, eq  // eq = none
 360:	tbnz	w0, #0, 45c <__divtf3+0x45c>
 364:	orr	x8, x12, #0x800000000000
 368:	stp	x11, x8, [sp]
 36c:	ldr	q0, [sp]
 370:	b	5ac <__divtf3+0x5ac>
 374:	lsr	x15, x13, #1
 378:	extr	x17, x16, x13, #33
 37c:	extr	x13, x16, x13, #1
 380:	lsr	x18, x16, #33
 384:	lsr	x3, x9, #32
 388:	and	x4, x9, #0xffffffff
 38c:	and	x5, x17, #0xffffffff
 390:	and	x6, x13, #0xffffffff
 394:	ubfx	x0, x16, #1, #32
 398:	lsr	x1, x12, #32
 39c:	mul	w18, w9, w18
 3a0:	mul	x7, x5, x4
 3a4:	mul	x19, x6, x3
 3a8:	lsr	x16, x16, #1
 3ac:	mul	x0, x0, x4
 3b0:	madd	w15, w15, w1, w18
 3b4:	adds	x18, x19, x7
 3b8:	and	x2, x10, #0xffffffff
 3bc:	mul	x4, x6, x4
 3c0:	madd	x0, x5, x3, x0
 3c4:	madd	w15, w16, w3, w15
 3c8:	adcs	x1, xzr, xzr
 3cc:	lsl	x11, x11, #48
 3d0:	madd	x0, x6, x2, x0
 3d4:	madd	w10, w17, w10, w15
 3d8:	extr	x15, x1, x18, #32
 3dc:	negs	x17, x4
 3e0:	add	x15, x0, x15
 3e4:	sbcs	x11, x11, xzr
 3e8:	add	x15, x15, x10, lsl #32
 3ec:	subs	x10, x17, x18, lsl #32
 3f0:	sbcs	x11, x11, xzr
 3f4:	sub	x11, x11, x15
 3f8:	cmp	w14, #0x4, lsl #12
 3fc:	b.lt	410 <__divtf3+0x410>  // b.tstop
 400:	orr	x8, x8, #0x7fff000000000000
 404:	stp	xzr, x8, [sp, #80]
 408:	ldr	q0, [sp, #80]
 40c:	b	5ac <__divtf3+0x5ac>
 410:	mov	w15, #0x3fff                	// #16383
 414:	mov	w17, #0xffffc001            	// #-16383
 418:	cmp	w14, w17
 41c:	add	w14, w14, w15
 420:	b.gt	488 <__divtf3+0x488>
 424:	cbnz	w14, 450 <__divtf3+0x450>
 428:	extr	x11, x11, x10, #63
 42c:	cmp	x9, x10, lsl #1
 430:	cset	w9, cc  // cc = lo, ul, last
 434:	cmp	x11, x12
 438:	cset	w10, hi  // hi = pmore
 43c:	csel	w9, w9, w10, eq  // eq = none
 440:	and	x10, x16, #0xffffffffffff
 444:	adds	x9, x13, x9
 448:	adcs	x10, x10, xzr
 44c:	tbnz	x10, #48, 578 <__divtf3+0x578>
 450:	stp	xzr, x8, [sp, #96]
 454:	ldr	q0, [sp, #96]
 458:	b	5ac <__divtf3+0x5ac>
 45c:	and	x12, x17, #0x7fffffffffffffff
 460:	cmp	x9, #0x0
 464:	cset	w0, eq  // eq = none
 468:	cmp	x12, x18
 46c:	cset	w18, cc  // cc = lo, ul, last
 470:	csel	w18, w0, w18, eq  // eq = none
 474:	tbnz	w18, #0, 4bc <__divtf3+0x4bc>
 478:	orr	x8, x17, #0x800000000000
 47c:	stp	x9, x8, [sp, #16]
 480:	ldr	q0, [sp, #16]
 484:	b	5ac <__divtf3+0x5ac>
 488:	extr	x11, x11, x10, #63
 48c:	cmp	x9, x10, lsl #1
 490:	cset	w9, ls  // ls = plast
 494:	cmp	x11, x12
 498:	cset	w10, cs  // cs = hs, nlast
 49c:	csel	w9, w9, w10, eq  // eq = none
 4a0:	bfi	x16, x14, #48, #16
 4a4:	adds	x9, x13, x9
 4a8:	adcs	x10, x16, xzr
 4ac:	orr	x8, x10, x8
 4b0:	stp	x9, x8, [sp, #128]
 4b4:	ldr	q0, [sp, #128]
 4b8:	b	5ac <__divtf3+0x5ac>
 4bc:	eor	x17, x14, #0x7fff000000000000
 4c0:	orr	x17, x11, x17
 4c4:	cbnz	x17, 4e0 <__divtf3+0x4e0>
 4c8:	eor	x10, x12, #0x7fff000000000000
 4cc:	orr	x9, x9, x10
 4d0:	cbnz	x9, 4f8 <__divtf3+0x4f8>
 4d4:	adrp	x8, 0 <__divtf3>
 4d8:	ldr	q0, [x8]
 4dc:	b	5ac <__divtf3+0x5ac>
 4e0:	eor	x17, x12, #0x7fff000000000000
 4e4:	orr	x17, x9, x17
 4e8:	cbnz	x17, 508 <__divtf3+0x508>
 4ec:	stp	xzr, x8, [sp, #48]
 4f0:	ldr	q0, [sp, #48]
 4f4:	b	5ac <__divtf3+0x5ac>
 4f8:	orr	x8, x8, #0x7fff000000000000
 4fc:	stp	xzr, x8, [sp, #32]
 500:	ldr	q0, [sp, #32]
 504:	b	5ac <__divtf3+0x5ac>
 508:	orr	x17, x11, x14
 50c:	cbz	x17, 588 <__divtf3+0x588>
 510:	orr	x17, x9, x12
 514:	cbz	x17, 5a0 <__divtf3+0x5a0>
 518:	lsr	x14, x14, #48
 51c:	cbnz	x14, 5bc <__divtf3+0x5bc>
 520:	cmp	x13, #0x0
 524:	csel	x18, x11, x13, eq  // eq = none
 528:	cset	w17, eq  // eq = none
 52c:	clz	x18, x18
 530:	add	w17, w18, w17, lsl #6
 534:	sub	w18, w17, #0xf
 538:	neg	x0, x18
 53c:	cmp	x18, #0x0
 540:	lsl	x1, x11, x18
 544:	lsl	x13, x13, x18
 548:	lsr	x0, x11, x0
 54c:	lsl	x11, x11, x18
 550:	sub	x18, x18, #0x40
 554:	csel	x0, xzr, x0, eq  // eq = none
 558:	cmp	x18, #0x0
 55c:	mov	w14, #0x10                  	// #16
 560:	csel	x18, xzr, x1, ge  // ge = tcont
 564:	orr	x13, x0, x13
 568:	csel	x13, x11, x13, ge  // ge = tcont
 56c:	sub	w14, w14, w17
 570:	mov	x11, x18
 574:	b	5c0 <__divtf3+0x5c0>
 578:	orr	x8, x10, x8
 57c:	stp	x9, x8, [sp, #112]
 580:	ldr	q0, [sp, #112]
 584:	b	5ac <__divtf3+0x5ac>
 588:	orr	x9, x9, x12
 58c:	cmp	x9, #0x0
 590:	stp	xzr, x8, [sp, #64]
 594:	b.eq	4d4 <__divtf3+0x4d4>  // b.none
 598:	ldr	q0, [sp, #64]
 59c:	b	5ac <__divtf3+0x5ac>
 5a0:	orr	x8, x8, #0x7fff000000000000
 5a4:	stp	xzr, x8, [sp, #144]
 5a8:	ldr	q0, [sp, #144]
 5ac:	ldp	x20, x19, [sp, #208]
 5b0:	ldp	x22, x21, [sp, #192]
 5b4:	add	sp, sp, #0xe0
 5b8:	ret
 5bc:	mov	w14, wzr
 5c0:	lsr	x12, x12, #48
 5c4:	cbnz	x12, 54 <__divtf3+0x54>
 5c8:	cmp	x10, #0x0
 5cc:	csel	x17, x9, x10, eq  // eq = none
 5d0:	cset	w12, eq  // eq = none
 5d4:	clz	x17, x17
 5d8:	add	w12, w17, w12, lsl #6
 5dc:	sub	w17, w12, #0xf
 5e0:	add	w12, w12, w14
 5e4:	neg	x14, x17
 5e8:	cmp	x17, #0x0
 5ec:	lsl	x18, x9, x17
 5f0:	sub	x0, x17, #0x40
 5f4:	lsl	x10, x10, x17
 5f8:	lsl	x17, x9, x17
 5fc:	lsr	x9, x9, x14
 600:	csel	x9, xzr, x9, eq  // eq = none
 604:	cmp	x0, #0x0
 608:	csel	x18, xzr, x18, ge  // ge = tcont
 60c:	orr	x9, x9, x10
 610:	csel	x10, x17, x9, ge  // ge = tcont
 614:	sub	w14, w12, #0x10
 618:	mov	x9, x18
 61c:	b	54 <__divtf3+0x54>

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	fmov	w10, s0
   4:	and	w9, w10, #0x7fffffff
   8:	sub	w8, w9, #0x800, lsl #12
   c:	lsr	w8, w8, #24
  10:	cmp	w8, #0x7e
  14:	and	w8, w10, #0x80000000
  18:	b.hi	28 <__extendsfdf2+0x28>  // b.pmore
  1c:	mov	x10, #0x3800000000000000    	// #4035225266123964416
  20:	add	x9, x10, x9, lsl #29
  24:	b	6c <__extendsfdf2+0x6c>
  28:	lsr	w11, w9, #23
  2c:	cmp	w11, #0xff
  30:	b.cc	44 <__extendsfdf2+0x44>  // b.lo, b.ul, b.last
  34:	mov	w9, w10
  38:	lsl	x9, x9, #29
  3c:	orr	x9, x9, #0x7ff0000000000000
  40:	b	6c <__extendsfdf2+0x6c>
  44:	cbz	w9, 68 <__extendsfdf2+0x68>
  48:	clz	w10, w9
  4c:	add	w12, w10, #0x15
  50:	mov	w11, #0x389                 	// #905
  54:	lsl	x9, x9, x12
  58:	eor	x9, x9, #0x10000000000000
  5c:	sub	w10, w11, w10
  60:	orr	x9, x9, x10, lsl #52
  64:	b	6c <__extendsfdf2+0x6c>
  68:	mov	x9, xzr
  6c:	orr	x8, x9, x8, lsl #32
  70:	fmov	d0, x8
  74:	ret

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	and	w9, w0, #0x7fff
   4:	sub	w8, w9, #0x400
   8:	ubfx	w8, w8, #11, #5
   c:	cmp	w8, #0xe
  10:	and	w8, w0, #0xffff8000
  14:	b.hi	24 <__extendhfsf2+0x24>  // b.pmore
  18:	mov	w10, #0x38000000            	// #939524096
  1c:	add	w9, w10, w9, lsl #13
  20:	b	5c <__extendhfsf2+0x5c>
  24:	lsr	w10, w9, #10
  28:	cmp	w10, #0x1f
  2c:	b.cc	3c <__extendhfsf2+0x3c>  // b.lo, b.ul, b.last
  30:	lsl	w9, w9, #13
  34:	orr	w9, w9, #0x7f800000
  38:	b	5c <__extendhfsf2+0x5c>
  3c:	cbz	w9, 5c <__extendhfsf2+0x5c>
  40:	clz	w10, w9
  44:	sub	w12, w10, #0x8
  48:	mov	w11, #0x43000000            	// #1124073472
  4c:	lsl	w9, w9, w12
  50:	eor	w9, w9, #0x800000
  54:	sub	w10, w11, w10, lsl #23
  58:	orr	w9, w9, w10
  5c:	orr	w8, w9, w8, lsl #16
  60:	fmov	s0, w8
  64:	ret

0000000000000068 <__gnu_h2f_ieee>:
  68:	b	0 <__extendhfsf2>

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbz	w0, 14 <__ffsdi2+0x14>
   4:	rbit	w8, w0
   8:	clz	w8, w8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	lsr	x8, x0, #32
  18:	cbz	w8, 2c <__ffsdi2+0x2c>
  1c:	rbit	w8, w8
  20:	clz	w8, w8
  24:	add	w0, w8, #0x21
  28:	ret
  2c:	mov	w0, wzr
  30:	ret

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	rbit	w8, w0
   4:	clz	w8, w8
   8:	cmp	w0, #0x0
   c:	csinc	w0, wzr, w8, eq  // eq = none
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 14 <__ffsti2+0x14>
   4:	rbit	x8, x0
   8:	clz	x8, x8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	cbz	x1, 28 <__ffsti2+0x28>
  18:	rbit	x8, x1
  1c:	clz	x8, x8
  20:	add	w0, w8, #0x41
  24:	ret
  28:	mov	w0, wzr
  2c:	ret

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	fcmp	d0, #0.0
   4:	b.pl	24 <__fixdfdi+0x24>  // b.nfrst
   8:	stp	x29, x30, [sp, #-16]!
   c:	mov	x29, sp
  10:	fneg	d0, d0
  14:	bl	0 <__fixunsdfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunsdfdi>

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	fmov	x8, d0
   4:	ubfx	x9, x8, #52, #11
   8:	subs	w10, w9, #0x3ff
   c:	b.cs	18 <__fixdfsi+0x18>  // b.hs, b.nlast
  10:	mov	w0, wzr
  14:	ret
  18:	cmp	w10, #0x20
  1c:	b.cc	30 <__fixdfsi+0x30>  // b.lo, b.ul, b.last
  20:	cmp	x8, #0x0
  24:	mov	w8, #0x80000000            	// #-2147483648
  28:	cinv	w0, w8, ge  // ge = tcont
  2c:	ret
  30:	mov	x10, #0x10000000000000      	// #4503599627370496
  34:	mov	w11, #0x433                 	// #1075
  38:	bfxil	x10, x8, #0, #52
  3c:	sub	w9, w11, w9
  40:	lsr	x9, x10, x9
  44:	cmp	x8, #0x0
  48:	cneg	w0, w9, lt  // lt = tstop
  4c:	ret

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fmov	x12, d0
   4:	cmp	x12, #0x0
   8:	mov	x8, #0xffffffffffffffff    	// #-1
   c:	ubfx	x11, x12, #52, #11
  10:	cneg	x8, x8, ge  // ge = tcont
  14:	subs	w9, w11, #0x3ff
  18:	b.cs	28 <__fixdfti+0x28>  // b.hs, b.nlast
  1c:	mov	x0, xzr
  20:	mov	x1, xzr
  24:	ret
  28:	cmp	w9, #0x80
  2c:	b.cc	44 <__fixdfti+0x44>  // b.lo, b.ul, b.last
  30:	cmp	x12, #0x0
  34:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  38:	cinv	x1, x8, ge  // ge = tcont
  3c:	csetm	x0, ge  // ge = tcont
  40:	ret
  44:	mov	x10, #0x10000000000000      	// #4503599627370496
  48:	asr	x9, x12, #63
  4c:	cmp	w11, #0x432
  50:	bfxil	x10, x12, #0, #52
  54:	b.hi	74 <__fixdfti+0x74>  // b.pmore
  58:	mov	w12, #0x433                 	// #1075
  5c:	sub	w11, w12, w11
  60:	lsr	x10, x10, x11
  64:	umulh	x11, x8, x10
  68:	madd	x1, x9, x10, x11
  6c:	mul	x0, x8, x10
  70:	ret
  74:	sub	w11, w11, #0x433
  78:	neg	x12, x11
  7c:	cmp	x11, #0x0
  80:	lsl	x13, x10, x11
  84:	sub	x14, x11, #0x40
  88:	lsl	x11, x10, x11
  8c:	lsr	x10, x10, x12
  90:	csel	x10, xzr, x10, eq  // eq = none
  94:	cmp	x14, #0x0
  98:	csel	x11, xzr, x11, ge  // ge = tcont
  9c:	umulh	x12, x11, x8
  a0:	csel	x10, x13, x10, ge  // ge = tcont
  a4:	madd	x9, x11, x9, x12
  a8:	madd	x1, x10, x8, x9
  ac:	mul	x0, x11, x8
  b0:	ret

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	fcmp	s0, #0.0
   4:	b.pl	24 <__fixsfdi+0x24>  // b.nfrst
   8:	stp	x29, x30, [sp, #-16]!
   c:	mov	x29, sp
  10:	fneg	s0, s0
  14:	bl	0 <__fixunssfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunssfdi>

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	fmov	w10, s0
   4:	cmp	w10, #0x0
   8:	mov	w8, #0x1                   	// #1
   c:	ubfx	w9, w10, #23, #8
  10:	cneg	w8, w8, lt  // lt = tstop
  14:	subs	w11, w9, #0x7f
  18:	b.cs	24 <__fixsfsi+0x24>  // b.hs, b.nlast
  1c:	mov	w0, wzr
  20:	ret
  24:	cmp	w11, #0x20
  28:	b.cc	3c <__fixsfsi+0x3c>  // b.lo, b.ul, b.last
  2c:	cmp	w10, #0x0
  30:	mov	w8, #0x7fffffff            	// #2147483647
  34:	cinv	w0, w8, lt  // lt = tstop
  38:	ret
  3c:	mov	w11, #0x800000              	// #8388608
  40:	cmp	w9, #0x95
  44:	bfxil	w11, w10, #0, #23
  48:	b.hi	5c <__fixsfsi+0x5c>  // b.pmore
  4c:	mov	w10, #0x96                  	// #150
  50:	sub	w9, w10, w9
  54:	lsr	w9, w11, w9
  58:	b	64 <__fixsfsi+0x64>
  5c:	sub	w9, w9, #0x96
  60:	lsl	w9, w11, w9
  64:	mul	w0, w9, w8
  68:	ret

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fmov	w11, s0
   4:	cmp	w11, #0x0
   8:	mov	w8, #0x1                   	// #1
   c:	ubfx	w9, w11, #23, #8
  10:	cneg	w10, w8, lt  // lt = tstop
  14:	subs	w8, w9, #0x7f
  18:	b.cs	28 <__fixsfti+0x28>  // b.hs, b.nlast
  1c:	mov	x0, xzr
  20:	mov	x1, xzr
  24:	ret
  28:	cmp	w8, #0x80
  2c:	b.cc	44 <__fixsfti+0x44>  // b.lo, b.ul, b.last
  30:	cmp	w11, #0x0
  34:	mov	x8, #0x7fffffffffffffff    	// #9223372036854775807
  38:	cinv	x1, x8, lt  // lt = tstop
  3c:	csetm	x0, ge  // ge = tcont
  40:	ret
  44:	mov	w8, #0x800000              	// #8388608
  48:	cmp	w9, #0x95
  4c:	bfxil	w8, w11, #0, #23
  50:	sxtw	x10, w10
  54:	b.hi	70 <__fixsfti+0x70>  // b.pmore
  58:	mov	w11, #0x96                  	// #150
  5c:	sub	w9, w11, w9
  60:	lsr	w8, w8, w9
  64:	smulh	x1, x8, x10
  68:	mul	x0, x8, x10
  6c:	ret
  70:	sub	w9, w9, #0x96
  74:	neg	x12, x9
  78:	cmp	x9, #0x0
  7c:	lsl	x13, x8, x9
  80:	sub	x14, x9, #0x40
  84:	lsl	x9, x8, x9
  88:	lsr	x8, x8, x12
  8c:	csel	x8, xzr, x8, eq  // eq = none
  90:	cmp	x14, #0x0
  94:	csel	x9, xzr, x9, ge  // ge = tcont
  98:	asr	x11, x10, #63
  9c:	umulh	x12, x9, x10
  a0:	csel	x8, x13, x8, ge  // ge = tcont
  a4:	madd	x11, x9, x11, x12
  a8:	madd	x1, x8, x10, x11
  ac:	mul	x0, x9, x10
  b0:	ret

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	fcmp	d0, #0.0
   4:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fmov	d1, x8
  10:	fmul	d1, d0, d1
  14:	fcvtzu	w8, d1
  18:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  1c:	ucvtf	d1, w8
  20:	fmov	d2, x9
  24:	fmul	d1, d1, d2
  28:	fadd	d0, d0, d1
  2c:	fcvtzu	w0, d0
  30:	bfi	x0, x8, #32, #32
  34:	ret
  38:	mov	x0, xzr
  3c:	ret

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	mov	w0, wzr
   4:	fmov	x8, d0
   8:	tbnz	x8, #63, 24 <__fixunsdfsi+0x24>
   c:	ubfx	x9, x8, #52, #11
  10:	subs	w10, w9, #0x3ff
  14:	b.cc	24 <__fixunsdfsi+0x24>  // b.lo, b.ul, b.last
  18:	cmp	w10, #0x1f
  1c:	b.ls	28 <__fixunsdfsi+0x28>  // b.plast
  20:	mov	w0, #0xffffffff            	// #-1
  24:	ret
  28:	mov	x10, #0x10000000000000      	// #4503599627370496
  2c:	mov	w11, #0x433                 	// #1075
  30:	bfxil	x10, x8, #0, #52
  34:	sub	w8, w11, w9
  38:	lsr	x0, x10, x8
  3c:	ret

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	mov	x0, xzr
   4:	fmov	x10, d0
   8:	tbnz	x10, #63, 30 <__fixunsdfti+0x30>
   c:	ubfx	x9, x10, #52, #11
  10:	subs	w8, w9, #0x3ff
  14:	mov	x1, x0
  18:	b.cc	88 <__fixunsdfti+0x88>  // b.lo, b.ul, b.last
  1c:	cmp	w8, #0x7f
  20:	b.ls	38 <__fixunsdfti+0x38>  // b.plast
  24:	mov	x0, #0xffffffffffffffff    	// #-1
  28:	mov	x1, #0xffffffffffffffff    	// #-1
  2c:	ret
  30:	mov	x1, x0
  34:	ret
  38:	mov	x8, #0x10000000000000      	// #4503599627370496
  3c:	cmp	w9, #0x432
  40:	bfxil	x8, x10, #0, #52
  44:	b.hi	5c <__fixunsdfti+0x5c>  // b.pmore
  48:	mov	w10, #0x433                 	// #1075
  4c:	sub	w9, w10, w9
  50:	mov	x1, xzr
  54:	lsr	x0, x8, x9
  58:	ret
  5c:	sub	w9, w9, #0x433
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsl	x11, x8, x9
  6c:	sub	x12, x9, #0x40
  70:	lsl	x9, x8, x9
  74:	lsr	x8, x8, x10
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	cmp	x12, #0x0
  80:	csel	x1, x11, x8, ge  // ge = tcont
  84:	csel	x0, xzr, x9, ge  // ge = tcont
  88:	ret

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	fcmp	s0, #0.0
   4:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fcvt	d0, s0
  10:	fmov	d1, x8
  14:	fmul	d1, d0, d1
  18:	fcvtzu	w8, d1
  1c:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  20:	ucvtf	d1, w8
  24:	fmov	d2, x9
  28:	fmul	d1, d1, d2
  2c:	fadd	d0, d0, d1
  30:	fcvtzu	w0, d0
  34:	bfi	x0, x8, #32, #32
  38:	ret
  3c:	mov	x0, xzr
  40:	ret

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	mov	w0, wzr
   4:	fmov	w9, s0
   8:	tbnz	w9, #31, 50 <__fixunssfsi+0x50>
   c:	ubfx	w8, w9, #23, #8
  10:	subs	w10, w8, #0x7f
  14:	b.cc	50 <__fixunssfsi+0x50>  // b.lo, b.ul, b.last
  18:	cmp	w10, #0x1f
  1c:	b.ls	28 <__fixunssfsi+0x28>  // b.plast
  20:	mov	w0, #0xffffffff            	// #-1
  24:	ret
  28:	mov	w10, #0x800000              	// #8388608
  2c:	cmp	w8, #0x95
  30:	bfxil	w10, w9, #0, #23
  34:	b.hi	48 <__fixunssfsi+0x48>  // b.pmore
  38:	mov	w9, #0x96                  	// #150
  3c:	sub	w8, w9, w8
  40:	lsr	w0, w10, w8
  44:	ret
  48:	sub	w8, w8, #0x96
  4c:	lsl	w0, w10, w8
  50:	ret

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	mov	x0, xzr
   4:	fmov	w10, s0
   8:	tbnz	w10, #31, 30 <__fixunssfti+0x30>
   c:	ubfx	w9, w10, #23, #8
  10:	subs	w8, w9, #0x7f
  14:	mov	x1, x0
  18:	b.cc	88 <__fixunssfti+0x88>  // b.lo, b.ul, b.last
  1c:	cmp	w8, #0x7f
  20:	b.ls	38 <__fixunssfti+0x38>  // b.plast
  24:	mov	x0, #0xffffffffffffffff    	// #-1
  28:	mov	x1, #0xffffffffffffffff    	// #-1
  2c:	ret
  30:	mov	x1, x0
  34:	ret
  38:	mov	w8, #0x800000              	// #8388608
  3c:	cmp	w9, #0x95
  40:	bfxil	w8, w10, #0, #23
  44:	b.hi	5c <__fixunssfti+0x5c>  // b.pmore
  48:	mov	w10, #0x96                  	// #150
  4c:	sub	w9, w10, w9
  50:	mov	x1, xzr
  54:	lsr	w0, w8, w9
  58:	ret
  5c:	sub	w9, w9, #0x96
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsl	x11, x8, x9
  6c:	sub	x12, x9, #0x40
  70:	lsl	x9, x8, x9
  74:	lsr	x8, x8, x10
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	cmp	x12, #0x0
  80:	csel	x1, x11, x8, ge  // ge = tcont
  84:	csel	x0, xzr, x9, ge  // ge = tcont
  88:	ret

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	lsr	x8, x0, #32
   4:	mov	x9, #0x41f0000000000000    	// #4751297606875873280
   8:	mov	x10, #0x4330000000000000    	// #4841369599423283200
   c:	mov	x11, #0xc330000000000000    	// #-4382002437431492608
  10:	scvtf	d0, w8
  14:	fmov	d1, x9
  18:	fmul	d0, d0, d1
  1c:	bfxil	x10, x0, #0, #32
  20:	fmov	d1, x11
  24:	fadd	d0, d0, d1
  28:	fmov	d1, x10
  2c:	fadd	d0, d0, d1
  30:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	cbz	x0, 40 <__floatdisf+0x40>
   4:	eor	x8, x0, x0, asr #63
   8:	sub	x9, x8, x0, asr #63
   c:	clz	x11, x9
  10:	mov	w8, #0x3f                  	// #63
  14:	cmp	w11, #0x27
  18:	sub	w8, w8, w11
  1c:	b.hi	48 <__floatdisf+0x48>  // b.pmore
  20:	mov	w10, #0x40                  	// #64
  24:	sub	w10, w10, w11
  28:	cmp	w10, #0x1a
  2c:	b.eq	70 <__floatdisf+0x70>  // b.none
  30:	cmp	w10, #0x19
  34:	b.ne	54 <__floatdisf+0x54>  // b.any
  38:	lsl	x9, x9, #1
  3c:	b	70 <__floatdisf+0x70>
  40:	fmov	s0, wzr
  44:	ret
  48:	sub	w10, w11, #0x28
  4c:	lsl	x9, x9, x10
  50:	b	90 <__floatdisf+0x90>
  54:	mov	w12, #0x26                  	// #38
  58:	lsl	x13, x9, x11
  5c:	sub	w11, w12, w11
  60:	tst	x13, #0x3fffffffff
  64:	lsr	x9, x9, x11
  68:	cset	w11, ne  // ne = any
  6c:	orr	x9, x9, x11
  70:	ubfx	x11, x9, #2, #1
  74:	orr	x9, x11, x9
  78:	add	x9, x9, #0x1
  7c:	mov	w12, #0x2                   	// #2
  80:	tst	x9, #0x4000000
  84:	cinc	x11, x12, ne  // ne = any
  88:	asr	x9, x9, x11
  8c:	csel	w8, w8, w10, eq  // eq = none
  90:	lsr	x10, x0, #32
  94:	mov	w11, #0x3f800000            	// #1065353216
  98:	and	w10, w10, #0x80000000
  9c:	add	w8, w11, w8, lsl #23
  a0:	bfxil	w10, w9, #0, #23
  a4:	orr	w8, w10, w8
  a8:	fmov	s0, w8
  ac:	ret

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	cbz	w0, 38 <__floatsidf+0x38>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	clz	w11, w10
  10:	add	w12, w11, #0x15
  14:	mov	w9, #0x41e                 	// #1054
  18:	lsl	x10, x10, x12
  1c:	eor	x10, x10, #0x10000000000000
  20:	sub	w9, w9, w11
  24:	and	w8, w0, #0x80000000
  28:	add	x9, x10, x9, lsl #52
  2c:	orr	x8, x9, x8, lsl #32
  30:	fmov	d0, x8
  34:	ret
  38:	fmov	d0, xzr
  3c:	ret

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	cbz	w0, 2c <__floatsisf+0x2c>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	and	w8, w0, #0x80000000
  10:	lsr	w11, w10, #24
  14:	clz	w9, w10
  18:	cbnz	w11, 34 <__floatsisf+0x34>
  1c:	sub	w11, w9, #0x8
  20:	lsl	w10, w10, w11
  24:	eor	w10, w10, #0x800000
  28:	b	64 <__floatsisf+0x64>
  2c:	fmov	s0, wzr
  30:	ret
  34:	mov	w11, #0x8                   	// #8
  38:	add	w12, w9, #0x18
  3c:	sub	w11, w11, w9
  40:	mov	w13, #0x80000000            	// #-2147483648
  44:	lsl	w12, w10, w12
  48:	lsr	w10, w10, w11
  4c:	eor	w10, w10, #0x800000
  50:	cmp	w12, w13
  54:	cinc	w10, w10, hi  // hi = pmore
  58:	and	w11, w10, #0x1
  5c:	csel	w11, w11, wzr, eq  // eq = none
  60:	add	w10, w11, w10
  64:	sub	w9, w10, w9, lsl #23
  68:	mov	w10, #0x4f000000            	// #1325400064
  6c:	add	w9, w9, w10
  70:	orr	w8, w9, w8
  74:	fmov	s0, w8
  78:	ret

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattidf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	eor	x9, x0, x1, asr #63
  1c:	asr	x8, x1, #63
  20:	eor	x10, x1, x1, asr #63
  24:	subs	x20, x9, x1, asr #63
  28:	sbcs	x21, x10, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x36
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattidf+0x78>  // b.tstop
  54:	cmp	w9, #0x37
  58:	b.eq	10c <__floattidf+0x10c>  // b.none
  5c:	cmp	w9, #0x36
  60:	b.ne	90 <__floattidf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattidf+0x10c>
  70:	fmov	d0, xzr
  74:	ret
  78:	sub	w9, w0, #0x4b
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	124 <__floattidf+0x124>
  90:	mov	w10, #0x49                  	// #73
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x37
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	x10, #55, 12c <__floattidf+0x12c>
 120:	extr	x10, x11, x10, #2
 124:	lsr	x11, x10, #32
 128:	b	138 <__floattidf+0x138>
 12c:	extr	x10, x11, x10, #3
 130:	lsr	x11, x10, #32
 134:	mov	w8, w9
 138:	lsr	x9, x19, #32
 13c:	mov	w12, #0x3ff00000            	// #1072693248
 140:	and	w9, w9, #0x80000000
 144:	add	w8, w12, w8, lsl #20
 148:	bfxil	w9, w11, #0, #20
 14c:	ldp	x20, x19, [sp, #32]
 150:	ldr	x21, [sp, #16]
 154:	orr	w8, w9, w8
 158:	bfi	x10, x8, #32, #32
 15c:	fmov	d0, x10
 160:	ldp	x29, x30, [sp], #48
 164:	ret

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattisf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	eor	x9, x0, x1, asr #63
  1c:	asr	x8, x1, #63
  20:	eor	x10, x1, x1, asr #63
  24:	subs	x20, x9, x1, asr #63
  28:	sbcs	x21, x10, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x19
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattisf+0x78>  // b.tstop
  54:	cmp	w9, #0x1a
  58:	b.eq	10c <__floattisf+0x10c>  // b.none
  5c:	cmp	w9, #0x19
  60:	b.ne	90 <__floattisf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattisf+0x10c>
  70:	fmov	s0, wzr
  74:	ret
  78:	sub	w9, w0, #0x68
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	134 <__floattisf+0x134>
  90:	mov	w10, #0x66                  	// #102
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x1a
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	w10, #26, 128 <__floattisf+0x128>
 120:	lsr	x10, x10, #2
 124:	b	134 <__floattisf+0x134>
 128:	extr	x8, x11, x10, #2
 12c:	lsr	x10, x8, #1
 130:	mov	w8, w9
 134:	lsr	x9, x19, #32
 138:	mov	w11, #0x3f800000            	// #1065353216
 13c:	and	w9, w9, #0x80000000
 140:	ldp	x20, x19, [sp, #32]
 144:	ldr	x21, [sp, #16]
 148:	add	w8, w11, w8, lsl #23
 14c:	bfxil	w9, w10, #0, #23
 150:	orr	w8, w9, w8
 154:	fmov	s0, w8
 158:	ldp	x29, x30, [sp], #48
 15c:	ret

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	adrp	x9, 0 <__floatundidf>
   4:	ldr	d0, [x9]
   8:	mov	x8, #0x4530000000000000    	// #4985484787499139072
   c:	bfxil	x8, x0, #32, #32
  10:	mov	x9, #0x4330000000000000    	// #4841369599423283200
  14:	bfxil	x9, x0, #0, #32
  18:	fmov	d1, x8
  1c:	fadd	d0, d1, d0
  20:	fmov	d1, x9
  24:	fadd	d0, d0, d1
  28:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	cbz	x0, 34 <__floatundisf+0x34>
   4:	clz	x10, x0
   8:	cmp	w10, #0x27
   c:	eor	w8, w10, #0x3f
  10:	b.hi	3c <__floatundisf+0x3c>  // b.pmore
  14:	mov	w9, #0x40                  	// #64
  18:	sub	w9, w9, w10
  1c:	cmp	w9, #0x1a
  20:	b.eq	64 <__floatundisf+0x64>  // b.none
  24:	cmp	w9, #0x19
  28:	b.ne	48 <__floatundisf+0x48>  // b.any
  2c:	lsl	x0, x0, #1
  30:	b	64 <__floatundisf+0x64>
  34:	fmov	s0, wzr
  38:	ret
  3c:	sub	w9, w10, #0x28
  40:	lsl	x10, x0, x9
  44:	b	84 <__floatundisf+0x84>
  48:	mov	w11, #0x26                  	// #38
  4c:	lsl	x12, x0, x10
  50:	sub	w10, w11, w10
  54:	tst	x12, #0x3fffffffff
  58:	lsr	x10, x0, x10
  5c:	cset	w11, ne  // ne = any
  60:	orr	x0, x10, x11
  64:	ubfx	x10, x0, #2, #1
  68:	orr	x10, x10, x0
  6c:	add	x10, x10, #0x1
  70:	mov	w11, #0x2                   	// #2
  74:	tst	x10, #0x4000000
  78:	cinc	x11, x11, ne  // ne = any
  7c:	lsr	x10, x10, x11
  80:	csel	w8, w8, w9, eq  // eq = none
  84:	bfi	w10, w8, #23, #9
  88:	mov	w8, #0x3f800000            	// #1065353216
  8c:	add	w8, w10, w8
  90:	fmov	s0, w8
  94:	ret

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	cbz	w0, 30 <__floatunsidf+0x30>
   4:	clz	w8, w0
   8:	mov	w9, #0x34                  	// #52
   c:	eor	w8, w8, #0x1f
  10:	mov	w10, w0
  14:	sub	w9, w9, w8
  18:	lsl	x9, x10, x9
  1c:	eor	x9, x9, #0x10000000000000
  20:	add	w8, w8, #0x3ff
  24:	add	x8, x9, x8, lsl #52
  28:	fmov	d0, x8
  2c:	ret
  30:	fmov	d0, xzr
  34:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	cbz	w0, 28 <__floatunsisf+0x28>
   4:	clz	w8, w0
   8:	eor	w8, w8, #0x1f
   c:	subs	w9, w8, #0x17
  10:	b.hi	30 <__floatunsisf+0x30>  // b.pmore
  14:	mov	w9, #0x17                  	// #23
  18:	sub	w9, w9, w8
  1c:	lsl	w9, w0, w9
  20:	eor	w9, w9, #0x800000
  24:	b	5c <__floatunsisf+0x5c>
  28:	fmov	s0, wzr
  2c:	ret
  30:	mov	w10, #0x37                  	// #55
  34:	sub	w10, w10, w8
  38:	lsr	w9, w0, w9
  3c:	lsl	w10, w0, w10
  40:	mov	w11, #0x80000000            	// #-2147483648
  44:	eor	w9, w9, #0x800000
  48:	cmp	w10, w11
  4c:	cinc	w9, w9, hi  // hi = pmore
  50:	and	w10, w9, #0x1
  54:	csel	w10, w10, wzr, eq  // eq = none
  58:	add	w9, w10, w9
  5c:	add	w8, w9, w8, lsl #23
  60:	mov	w9, #0x3f800000            	// #1065353216
  64:	add	w8, w8, w9
  68:	fmov	s0, w8
  6c:	ret

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 54 <__floatuntidf+0x54>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x36
  30:	sub	w8, w8, w0
  34:	b.lt	5c <__floatuntidf+0x5c>  // b.tstop
  38:	cmp	w9, #0x37
  3c:	b.eq	f4 <__floatuntidf+0xf4>  // b.none
  40:	cmp	w9, #0x36
  44:	b.ne	78 <__floatuntidf+0x78>  // b.any
  48:	extr	x20, x20, x19, #63
  4c:	lsl	x19, x19, #1
  50:	b	f4 <__floatuntidf+0xf4>
  54:	fmov	d0, xzr
  58:	ret
  5c:	sub	w9, w0, #0x4b
  60:	lsl	x10, x19, x9
  64:	sub	x9, x9, #0x40
  68:	cmp	x9, #0x0
  6c:	csel	x10, xzr, x10, ge  // ge = tcont
  70:	lsr	x11, x10, #32
  74:	b	128 <__floatuntidf+0x128>
  78:	mov	w10, #0x49                  	// #73
  7c:	sub	w10, w10, w0
  80:	neg	x13, x10
  84:	cmp	x10, #0x0
  88:	sub	x14, x10, #0x40
  8c:	lsl	x13, x20, x13
  90:	add	w11, w0, #0x37
  94:	csel	x13, xzr, x13, eq  // eq = none
  98:	cmp	x14, #0x0
  9c:	lsr	x14, x19, x10
  a0:	neg	x12, x11
  a4:	orr	x13, x14, x13
  a8:	lsr	x14, x20, x10
  ac:	lsr	x10, x20, x10
  b0:	lsl	x15, x20, x11
  b4:	csel	x10, x10, x13, ge  // ge = tcont
  b8:	lsr	x12, x19, x12
  bc:	csel	x20, xzr, x14, ge  // ge = tcont
  c0:	cmp	x11, #0x0
  c4:	lsl	x13, x19, x11
  c8:	lsl	x16, x19, x11
  cc:	sub	x11, x11, #0x40
  d0:	csel	x12, xzr, x12, eq  // eq = none
  d4:	cmp	x11, #0x0
  d8:	orr	x11, x12, x15
  dc:	csel	x11, x13, x11, ge  // ge = tcont
  e0:	csel	x12, xzr, x16, ge  // ge = tcont
  e4:	orr	x11, x12, x11
  e8:	cmp	x11, #0x0
  ec:	cset	w11, ne  // ne = any
  f0:	orr	x19, x10, x11
  f4:	ubfx	x10, x19, #2, #1
  f8:	orr	x10, x10, x19
  fc:	adds	x11, x10, #0x1
 100:	adcs	x12, x20, xzr
 104:	tbnz	x11, #55, 118 <__floatuntidf+0x118>
 108:	extr	x10, x12, x11, #2
 10c:	lsr	x11, x11, #34
 110:	bfi	w11, w12, #30, #2
 114:	b	128 <__floatuntidf+0x128>
 118:	extr	x10, x12, x11, #3
 11c:	lsr	x11, x11, #35
 120:	bfi	w11, w12, #29, #3
 124:	mov	w8, w9
 128:	bfi	w11, w8, #20, #12
 12c:	mov	w8, #0x3ff00000            	// #1072693248
 130:	ldp	x20, x19, [sp, #16]
 134:	add	w8, w11, w8
 138:	bfi	x10, x8, #32, #32
 13c:	fmov	d0, x10
 140:	ldp	x29, x30, [sp], #32
 144:	ret

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 50 <__floatuntisf+0x50>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x19
  30:	sub	w8, w8, w0
  34:	b.lt	58 <__floatuntisf+0x58>  // b.tstop
  38:	cmp	w9, #0x1a
  3c:	b.eq	e4 <__floatuntisf+0xe4>  // b.none
  40:	cmp	w9, #0x19
  44:	b.ne	70 <__floatuntisf+0x70>  // b.any
  48:	lsl	x19, x19, #1
  4c:	b	e4 <__floatuntisf+0xe4>
  50:	fmov	s0, wzr
  54:	ret
  58:	sub	w9, w0, #0x68
  5c:	lsl	x10, x19, x9
  60:	sub	x9, x9, #0x40
  64:	cmp	x9, #0x0
  68:	csel	x10, xzr, x10, ge  // ge = tcont
  6c:	b	104 <__floatuntisf+0x104>
  70:	mov	w10, #0x66                  	// #102
  74:	sub	w10, w10, w0
  78:	neg	x12, x10
  7c:	cmp	x10, #0x0
  80:	sub	x13, x10, #0x40
  84:	lsl	x12, x20, x12
  88:	add	w11, w0, #0x1a
  8c:	csel	x12, xzr, x12, eq  // eq = none
  90:	cmp	x13, #0x0
  94:	lsr	x13, x19, x10
  98:	orr	x12, x13, x12
  9c:	neg	x13, x11
  a0:	lsr	x10, x20, x10
  a4:	csel	x10, x10, x12, ge  // ge = tcont
  a8:	lsr	x13, x19, x13
  ac:	cmp	x11, #0x0
  b0:	lsl	x14, x20, x11
  b4:	lsl	x12, x19, x11
  b8:	lsl	x15, x19, x11
  bc:	sub	x11, x11, #0x40
  c0:	csel	x13, xzr, x13, eq  // eq = none
  c4:	cmp	x11, #0x0
  c8:	orr	x11, x13, x14
  cc:	csel	x11, x12, x11, ge  // ge = tcont
  d0:	csel	x12, xzr, x15, ge  // ge = tcont
  d4:	orr	x11, x12, x11
  d8:	cmp	x11, #0x0
  dc:	cset	w11, ne  // ne = any
  e0:	orr	x19, x10, x11
  e4:	ubfx	x10, x19, #2, #1
  e8:	orr	x10, x10, x19
  ec:	adds	x10, x10, #0x1
  f0:	tbnz	w10, #26, fc <__floatuntisf+0xfc>
  f4:	lsr	x10, x10, #2
  f8:	b	104 <__floatuntisf+0x104>
  fc:	lsr	x10, x10, #3
 100:	mov	w8, w9
 104:	ldp	x20, x19, [sp, #16]
 108:	bfi	w10, w8, #23, #9
 10c:	mov	w8, #0x3f800000            	// #1065353216
 110:	add	w8, w10, w8
 114:	fmov	s0, w8
 118:	ldp	x29, x30, [sp], #32
 11c:	ret

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	lsr	x8, x0, #32
   4:	tbnz	w1, #5, 28 <__lshrdi3+0x28>
   8:	cbz	w1, 38 <__lshrdi3+0x38>
   c:	neg	w10, w1
  10:	lsr	w9, w8, w1
  14:	lsr	w11, w0, w1
  18:	lsl	w8, w8, w10
  1c:	orr	w8, w8, w11
  20:	lsl	x9, x9, #32
  24:	b	30 <__lshrdi3+0x30>
  28:	mov	x9, xzr
  2c:	lsr	w8, w8, w1
  30:	mov	w8, w8
  34:	orr	x0, x9, x8
  38:	ret

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbnz	w2, #6, 24 <__lshrti3+0x24>
   4:	cbz	w2, 34 <__lshrti3+0x34>
   8:	neg	w9, w2
   c:	lsl	x9, x1, x9
  10:	lsr	x10, x0, x2
  14:	mov	x8, xzr
  18:	lsr	x1, x1, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__lshrti3+0x30>
  24:	mov	x8, xzr
  28:	lsr	x9, x1, x2
  2c:	mov	x1, xzr
  30:	orr	x0, x8, x9
  34:	ret

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	cmp	x1, #0x0
  10:	eor	x8, x0, x0, asr #63
  14:	mov	x19, x0
  18:	cneg	x1, x1, mi  // mi = first
  1c:	sub	x0, x8, x0, asr #63
  20:	add	x2, x29, #0x18
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x8, [x29, #24]
  2c:	eor	x8, x8, x19, asr #63
  30:	sub	x0, x8, x19, asr #63
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #32
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	negs	x8, x2
  14:	ngcs	x11, x3
  18:	eor	x9, x0, x1, asr #63
  1c:	cmp	x3, #0x0
  20:	asr	x20, x1, #63
  24:	eor	x10, x1, x1, asr #63
  28:	csel	x2, x8, x2, lt  // lt = tstop
  2c:	csel	x3, x11, x3, lt  // lt = tstop
  30:	subs	x0, x9, x1, asr #63
  34:	mov	x19, x1
  38:	sbcs	x1, x10, x20
  3c:	mov	x4, sp
  40:	bl	0 <__udivmodti4>
  44:	ldp	x9, x8, [sp]
  48:	ldp	x29, x30, [sp, #16]
  4c:	eor	x9, x9, x19, asr #63
  50:	eor	x8, x8, x19, asr #63
  54:	subs	x0, x9, x19, asr #63
  58:	sbcs	x1, x8, x20
  5c:	ldp	x20, x19, [sp, #32]
  60:	add	sp, sp, #0x30
  64:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d7, d0, d2
   4:	fmul	d6, d1, d3
   8:	fmul	d16, d0, d3
   c:	fmul	d17, d1, d2
  10:	fsub	d4, d7, d6
  14:	fcmp	d4, d4
  18:	fadd	d5, d17, d16
  1c:	b.vc	1f4 <__muldc3+0x1f4>
  20:	fcmp	d5, d5
  24:	b.vc	1f4 <__muldc3+0x1f4>
  28:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  2c:	fabs	d19, d0
  30:	fmov	d18, x8
  34:	fcmp	d19, d18
  38:	fabs	d18, d1
  3c:	b.eq	58 <__muldc3+0x58>  // b.none
  40:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  44:	fmov	d20, x8
  48:	fcmp	d18, d20
  4c:	b.eq	58 <__muldc3+0x58>  // b.none
  50:	mov	w8, wzr
  54:	b	b0 <__muldc3+0xb0>
  58:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  5c:	fmov	d23, x8
  60:	fmov	d20, xzr
  64:	fmov	d21, #1.000000000000000000e+00
  68:	movi	v22.2d, #0x0
  6c:	fcmp	d19, d23
  70:	fcsel	d19, d21, d20, eq  // eq = none
  74:	fcmp	d18, d23
  78:	fmov	d18, xzr
  7c:	fneg	v22.2d, v22.2d
  80:	bit	v18.16b, v2.16b, v22.16b
  84:	bit	v19.16b, v0.16b, v22.16b
  88:	fcsel	d0, d21, d20, eq  // eq = none
  8c:	fcmp	d2, d2
  90:	bit	v20.16b, v3.16b, v22.16b
  94:	bit	v0.16b, v1.16b, v22.16b
  98:	fcsel	d2, d18, d2, vs
  9c:	fcmp	d3, d3
  a0:	fcsel	d3, d20, d3, vs
  a4:	mov	w8, #0x1                   	// #1
  a8:	mov	v1.16b, v0.16b
  ac:	mov	v0.16b, v19.16b
  b0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  b4:	fabs	d18, d3
  b8:	fmov	d19, x9
  bc:	fcmp	d18, d19
  c0:	fabs	d19, d2
  c4:	b.eq	178 <__muldc3+0x178>  // b.none
  c8:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  cc:	fmov	d20, x9
  d0:	fcmp	d19, d20
  d4:	b.eq	178 <__muldc3+0x178>  // b.none
  d8:	cbnz	w8, 1cc <__muldc3+0x1cc>
  dc:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  e0:	fabs	d17, d17
  e4:	fmov	d18, x8
  e8:	fcmp	d17, d18
  ec:	b.eq	12c <__muldc3+0x12c>  // b.none
  f0:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  f4:	fabs	d16, d16
  f8:	fmov	d17, x8
  fc:	fcmp	d16, d17
 100:	b.eq	12c <__muldc3+0x12c>  // b.none
 104:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 108:	fabs	d7, d7
 10c:	fmov	d16, x8
 110:	fcmp	d7, d16
 114:	b.eq	12c <__muldc3+0x12c>  // b.none
 118:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 11c:	fabs	d6, d6
 120:	fmov	d7, x8
 124:	fcmp	d6, d7
 128:	b.ne	1f4 <__muldc3+0x1f4>  // b.any
 12c:	movi	v5.2d, #0x0
 130:	fmov	d6, xzr
 134:	fneg	v5.2d, v5.2d
 138:	fmov	d7, xzr
 13c:	bit	v6.16b, v0.16b, v5.16b
 140:	fcmp	d0, d0
 144:	fmov	d16, xzr
 148:	bit	v7.16b, v1.16b, v5.16b
 14c:	fcsel	d0, d6, d0, vs
 150:	fcmp	d1, d1
 154:	fmov	d4, xzr
 158:	bit	v16.16b, v2.16b, v5.16b
 15c:	fcsel	d1, d7, d1, vs
 160:	fcmp	d2, d2
 164:	bit	v4.16b, v3.16b, v5.16b
 168:	fcsel	d2, d16, d2, vs
 16c:	fcmp	d3, d3
 170:	fcsel	d3, d4, d3, vs
 174:	b	1cc <__muldc3+0x1cc>
 178:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 17c:	fmov	d7, x8
 180:	fmov	d4, xzr
 184:	fmov	d5, #1.000000000000000000e+00
 188:	movi	v6.2d, #0x0
 18c:	fcmp	d19, d7
 190:	fcsel	d16, d5, d4, eq  // eq = none
 194:	fcmp	d18, d7
 198:	fmov	d7, xzr
 19c:	fneg	v6.2d, v6.2d
 1a0:	bit	v7.16b, v0.16b, v6.16b
 1a4:	bit	v16.16b, v2.16b, v6.16b
 1a8:	fcsel	d2, d5, d4, eq  // eq = none
 1ac:	fcmp	d0, d0
 1b0:	bit	v4.16b, v1.16b, v6.16b
 1b4:	bit	v2.16b, v3.16b, v6.16b
 1b8:	fcsel	d0, d7, d0, vs
 1bc:	fcmp	d1, d1
 1c0:	fcsel	d1, d4, d1, vs
 1c4:	mov	v3.16b, v2.16b
 1c8:	mov	v2.16b, v16.16b
 1cc:	fmul	d4, d2, d0
 1d0:	fmul	d5, d3, d1
 1d4:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1d8:	fmul	d0, d3, d0
 1dc:	fmul	d1, d2, d1
 1e0:	fsub	d2, d4, d5
 1e4:	fmov	d3, x8
 1e8:	fadd	d0, d1, d0
 1ec:	fmul	d4, d2, d3
 1f0:	fmul	d5, d0, d3
 1f4:	mov	v0.16b, v4.16b
 1f8:	mov	v1.16b, v5.16b
 1fc:	ret

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	fmov	x14, d0
   4:	ubfx	x9, x14, #52, #11
   8:	fmov	x13, d1
   c:	eor	x8, x13, x14
  10:	sub	w11, w9, #0x1
  14:	ubfx	x10, x13, #52, #11
  18:	and	x12, x14, #0xfffffffffffff
  1c:	and	x8, x8, #0x8000000000000000
  20:	cmp	w11, #0x7fd
  24:	and	x11, x13, #0xfffffffffffff
  28:	b.hi	a8 <__muldf3+0xa8>  // b.pmore
  2c:	sub	w15, w10, #0x1
  30:	cmp	w15, #0x7fe
  34:	b.cs	a8 <__muldf3+0xa8>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	and	x15, x12, #0xffffffff
  40:	lsr	x12, x12, #32
  44:	lsl	w14, w11, #11
  48:	mov	w16, #0x80000000            	// #-2147483648
  4c:	add	w17, w9, w10
  50:	orr	x10, x12, #0x100000
  54:	bfxil	x16, x11, #21, #31
  58:	mul	x12, x14, x10
  5c:	mul	x9, x14, x15
  60:	mul	x11, x16, x15
  64:	mul	x14, x16, x10
  68:	and	x15, x12, #0xfffff800
  6c:	and	x10, x9, #0xfffff800
  70:	add	x12, x14, x12, lsr #32
  74:	add	x9, x15, x9, lsr #32
  78:	add	x14, x12, x11, lsr #32
  7c:	add	x12, x9, w11, uxtw
  80:	add	w11, w17, w13
  84:	bfi	x10, x12, #32, #32
  88:	add	x9, x14, x12, lsr #32
  8c:	sub	w11, w11, #0x3ff
  90:	tbnz	x9, #52, c4 <__muldf3+0xc4>
  94:	lsr	x12, x12, #31
  98:	bfi	x12, x9, #1, #63
  9c:	lsl	x10, x10, #1
  a0:	mov	x9, x12
  a4:	b	c8 <__muldf3+0xc8>
  a8:	mov	x16, #0x1                   	// #1
  ac:	and	x15, x14, #0x7fffffffffffffff
  b0:	movk	x16, #0x7ff0, lsl #48
  b4:	cmp	x15, x16
  b8:	b.cc	e8 <__muldf3+0xe8>  // b.lo, b.ul, b.last
  bc:	orr	x8, x14, #0x8000000000000
  c0:	b	1b4 <__muldf3+0x1b4>
  c4:	add	w11, w11, #0x1
  c8:	cmp	w11, #0x7ff
  cc:	b.lt	d8 <__muldf3+0xd8>  // b.tstop
  d0:	orr	x8, x8, #0x7ff0000000000000
  d4:	b	1b4 <__muldf3+0x1b4>
  d8:	cmp	w11, #0x0
  dc:	b.le	110 <__muldf3+0x110>
  e0:	bfi	x9, x11, #52, #12
  e4:	b	144 <__muldf3+0x144>
  e8:	and	x14, x13, #0x7fffffffffffffff
  ec:	cmp	x14, x16
  f0:	b.cc	fc <__muldf3+0xfc>  // b.lo, b.ul, b.last
  f4:	orr	x8, x13, #0x8000000000000
  f8:	b	1b4 <__muldf3+0x1b4>
  fc:	mov	x13, #0x7ff0000000000000    	// #9218868437227405312
 100:	cmp	x15, x13
 104:	b.ne	174 <__muldf3+0x174>  // b.any
 108:	cbnz	x14, d0 <__muldf3+0xd0>
 10c:	b	180 <__muldf3+0x180>
 110:	mov	w12, #0x1                   	// #1
 114:	sub	w11, w12, w11
 118:	cmp	w11, #0x3f
 11c:	b.hi	1b4 <__muldf3+0x1b4>  // b.pmore
 120:	neg	x12, x11
 124:	lsr	x13, x10, x11
 128:	lsl	x10, x10, x12
 12c:	lsl	x12, x9, x12
 130:	cmp	x10, #0x0
 134:	orr	x10, x12, x13
 138:	cset	w12, ne  // ne = any
 13c:	orr	x10, x10, x12
 140:	lsr	x9, x9, x11
 144:	mov	x11, #0x8000000000000001    	// #-9223372036854775807
 148:	cmp	x10, x11
 14c:	orr	x8, x9, x8
 150:	b.cc	15c <__muldf3+0x15c>  // b.lo, b.ul, b.last
 154:	add	x8, x8, #0x1
 158:	b	1b4 <__muldf3+0x1b4>
 15c:	mov	x11, #0x8000000000000000    	// #-9223372036854775808
 160:	cmp	x10, x11
 164:	b.ne	1b4 <__muldf3+0x1b4>  // b.any
 168:	and	x9, x9, #0x1
 16c:	add	x8, x8, x9
 170:	b	1b4 <__muldf3+0x1b4>
 174:	cmp	x14, x13
 178:	b.ne	188 <__muldf3+0x188>  // b.any
 17c:	cbnz	x15, d0 <__muldf3+0xd0>
 180:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 184:	b	1b4 <__muldf3+0x1b4>
 188:	cbz	x15, 1b4 <__muldf3+0x1b4>
 18c:	cbz	x14, 1b4 <__muldf3+0x1b4>
 190:	lsr	x13, x15, #52
 194:	cbnz	x13, 1bc <__muldf3+0x1bc>
 198:	clz	x13, x12
 19c:	mov	w15, #0xfffffff5            	// #-11
 1a0:	mov	w16, #0xc                   	// #12
 1a4:	add	w15, w13, w15
 1a8:	lsl	x12, x12, x15
 1ac:	sub	w13, w16, w13
 1b0:	b	1c0 <__muldf3+0x1c0>
 1b4:	fmov	d0, x8
 1b8:	ret
 1bc:	mov	w13, wzr
 1c0:	lsr	x14, x14, #52
 1c4:	cbnz	x14, 3c <__muldf3+0x3c>
 1c8:	clz	x14, x11
 1cc:	mov	w15, #0xfffffff5            	// #-11
 1d0:	add	w15, w14, w15
 1d4:	sub	w13, w13, w14
 1d8:	lsl	x11, x11, x15
 1dc:	add	w13, w13, #0xc
 1e0:	b	3c <__muldf3+0x3c>

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	and	w10, w0, #0xffff
   4:	and	w11, w1, #0xffff
   8:	ubfx	x12, x0, #16, #16
   c:	ubfx	x13, x1, #16, #16
  10:	mul	w14, w11, w10
  14:	mul	w11, w11, w12
  18:	mul	w10, w13, w10
  1c:	mul	w12, w13, w12
  20:	add	w11, w11, w14, lsr #16
  24:	lsr	x8, x0, #32
  28:	add	w10, w10, w11, uxth
  2c:	add	w11, w12, w11, lsr #16
  30:	lsr	x9, x1, #32
  34:	mul	w8, w8, w1
  38:	bfi	w14, w10, #16, #16
  3c:	add	w10, w11, w10, lsr #16
  40:	bfi	x14, x10, #32, #32
  44:	madd	w8, w9, w0, w8
  48:	add	x0, x14, x8, lsl #32
  4c:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   4:	mov	x8, x0
   8:	cmp	x0, x9
   c:	mul	x0, x1, x0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulodi4+0x24>  // b.any
  18:	cmp	x1, #0x2
  1c:	b.cs	88 <__mulodi4+0x88>  // b.hs, b.nlast
  20:	ret
  24:	cmp	x1, x9
  28:	b.ne	38 <__mulodi4+0x38>  // b.any
  2c:	cmp	x8, #0x2
  30:	b.cc	20 <__mulodi4+0x20>  // b.lo, b.ul, b.last
  34:	b	88 <__mulodi4+0x88>
  38:	eor	x9, x8, x8, asr #63
  3c:	sub	x9, x9, x8, asr #63
  40:	cmp	x9, #0x2
  44:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  48:	eor	x10, x1, x1, asr #63
  4c:	sub	x10, x10, x1, asr #63
  50:	cmp	x10, #0x2
  54:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  58:	asr	x8, x8, #63
  5c:	asr	x11, x1, #63
  60:	cmp	x8, x11
  64:	b.ne	74 <__mulodi4+0x74>  // b.any
  68:	mov	x8, #0x7fffffffffffffff    	// #9223372036854775807
  6c:	udiv	x8, x8, x10
  70:	b	80 <__mulodi4+0x80>
  74:	neg	x8, x10
  78:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  7c:	sdiv	x8, x10, x8
  80:	cmp	x9, x8
  84:	b.le	20 <__mulodi4+0x20>
  88:	mov	w8, #0x1                   	// #1
  8c:	str	w8, [x2]
  90:	ret

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w9, #0x80000000            	// #-2147483648
   4:	mov	w8, w0
   8:	cmp	w0, w9
   c:	mul	w0, w1, w0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulosi4+0x24>  // b.any
  18:	cmp	w1, #0x2
  1c:	b.cs	88 <__mulosi4+0x88>  // b.hs, b.nlast
  20:	ret
  24:	cmp	w1, w9
  28:	b.ne	38 <__mulosi4+0x38>  // b.any
  2c:	cmp	w8, #0x2
  30:	b.cc	20 <__mulosi4+0x20>  // b.lo, b.ul, b.last
  34:	b	88 <__mulosi4+0x88>
  38:	eor	w9, w8, w8, asr #31
  3c:	sub	w9, w9, w8, asr #31
  40:	cmp	w9, #0x2
  44:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  48:	eor	w10, w1, w1, asr #31
  4c:	sub	w10, w10, w1, asr #31
  50:	cmp	w10, #0x2
  54:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  58:	asr	w8, w8, #31
  5c:	asr	w11, w1, #31
  60:	cmp	w8, w11
  64:	b.ne	74 <__mulosi4+0x74>  // b.any
  68:	mov	w8, #0x7fffffff            	// #2147483647
  6c:	udiv	w8, w8, w10
  70:	b	80 <__mulosi4+0x80>
  74:	neg	w8, w10
  78:	mov	w10, #0x80000000            	// #-2147483648
  7c:	sdiv	w8, w10, w8
  80:	cmp	w9, w8
  84:	b.le	20 <__mulosi4+0x20>
  88:	mov	w8, #0x1                   	// #1
  8c:	str	w8, [x2]
  90:	ret

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	umulh	x8, x2, x0
  18:	eor	x9, x1, #0x8000000000000000
  1c:	madd	x8, x2, x1, x8
  20:	mov	x19, x4
  24:	orr	x9, x0, x9
  28:	madd	x20, x3, x0, x8
  2c:	mul	x21, x2, x0
  30:	str	wzr, [x4]
  34:	cbnz	x9, 48 <__muloti4+0x48>
  38:	cmp	x2, #0x2
  3c:	cset	w8, cc  // cc = lo, ul, last
  40:	cmp	x3, #0x0
  44:	b	60 <__muloti4+0x60>
  48:	eor	x8, x3, #0x8000000000000000
  4c:	orr	x8, x2, x8
  50:	cbnz	x8, 8c <__muloti4+0x8c>
  54:	cmp	x0, #0x2
  58:	cset	w8, cc  // cc = lo, ul, last
  5c:	cmp	x1, #0x0
  60:	csel	w8, w8, wzr, eq  // eq = none
  64:	tbnz	w8, #0, 70 <__muloti4+0x70>
  68:	mov	w8, #0x1                   	// #1
  6c:	str	w8, [x19]
  70:	mov	x0, x21
  74:	mov	x1, x20
  78:	ldp	x20, x19, [sp, #48]
  7c:	ldp	x22, x21, [sp, #32]
  80:	ldr	x23, [sp, #16]
  84:	ldp	x29, x30, [sp], #64
  88:	ret
  8c:	eor	x10, x0, x1, asr #63
  90:	asr	x8, x1, #63
  94:	eor	x11, x1, x1, asr #63
  98:	subs	x23, x10, x1, asr #63
  9c:	eor	x12, x2, x3, asr #63
  a0:	sbcs	x22, x11, x8
  a4:	asr	x9, x3, #63
  a8:	eor	x13, x3, x3, asr #63
  ac:	subs	x2, x12, x3, asr #63
  b0:	sbcs	x3, x13, x9
  b4:	cmp	x23, #0x2
  b8:	cset	w10, cc  // cc = lo, ul, last
  bc:	cmp	x22, #0x0
  c0:	cset	w11, lt  // lt = tstop
  c4:	csel	w10, w10, w11, eq  // eq = none
  c8:	tbnz	w10, #0, 70 <__muloti4+0x70>
  cc:	cmp	x2, #0x2
  d0:	cset	w10, cc  // cc = lo, ul, last
  d4:	cmp	x3, #0x0
  d8:	cset	w11, lt  // lt = tstop
  dc:	csel	w10, w10, w11, eq  // eq = none
  e0:	tbnz	w10, #0, 70 <__muloti4+0x70>
  e4:	eor	x8, x8, x9
  e8:	orr	x8, x8, x8
  ec:	cbnz	x8, 100 <__muloti4+0x100>
  f0:	mov	x0, #0xffffffffffffffff    	// #-1
  f4:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  f8:	bl	0 <__udivti3>
  fc:	b	114 <__muloti4+0x114>
 100:	negs	x2, x2
 104:	ngcs	x3, x3
 108:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 10c:	mov	x0, xzr
 110:	bl	0 <__divti3>
 114:	cmp	x23, x0
 118:	cset	w8, ls  // ls = plast
 11c:	cmp	x22, x1
 120:	cset	w9, le
 124:	csel	w8, w8, w9, eq  // eq = none
 128:	tbz	w8, #0, 68 <__muloti4+0x68>
 12c:	b	70 <__muloti4+0x70>

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s7, s0, s2
   4:	fmul	s6, s1, s3
   8:	fmul	s16, s0, s3
   c:	fmul	s17, s1, s2
  10:	fsub	s4, s7, s6
  14:	fcmp	s4, s4
  18:	fadd	s5, s17, s16
  1c:	b.vc	1e8 <__mulsc3+0x1e8>
  20:	fcmp	s5, s5
  24:	b.vc	1e8 <__mulsc3+0x1e8>
  28:	mov	w8, #0x7f800000            	// #2139095040
  2c:	fabs	s19, s0
  30:	fmov	s18, w8
  34:	fcmp	s19, s18
  38:	fabs	s18, s1
  3c:	b.eq	58 <__mulsc3+0x58>  // b.none
  40:	mov	w8, #0x7f800000            	// #2139095040
  44:	fmov	s20, w8
  48:	fcmp	s18, s20
  4c:	b.eq	58 <__mulsc3+0x58>  // b.none
  50:	mov	w8, wzr
  54:	b	ac <__mulsc3+0xac>
  58:	mov	w8, #0x7f800000            	// #2139095040
  5c:	fmov	s23, w8
  60:	fmov	s20, wzr
  64:	fmov	s21, #1.000000000000000000e+00
  68:	fcmp	s19, s23
  6c:	movi	v22.4s, #0x80, lsl #24
  70:	fcsel	s19, s21, s20, eq  // eq = none
  74:	fcmp	s18, s23
  78:	fmov	s18, wzr
  7c:	bit	v18.16b, v2.16b, v22.16b
  80:	bit	v19.16b, v0.16b, v22.16b
  84:	fcsel	s0, s21, s20, eq  // eq = none
  88:	fcmp	s2, s2
  8c:	bit	v20.16b, v3.16b, v22.16b
  90:	bit	v0.16b, v1.16b, v22.16b
  94:	fcsel	s2, s18, s2, vs
  98:	fcmp	s3, s3
  9c:	fcsel	s3, s20, s3, vs
  a0:	mov	w8, #0x1                   	// #1
  a4:	mov	v1.16b, v0.16b
  a8:	mov	v0.16b, v19.16b
  ac:	mov	w9, #0x7f800000            	// #2139095040
  b0:	fabs	s18, s3
  b4:	fmov	s19, w9
  b8:	fcmp	s18, s19
  bc:	fabs	s19, s2
  c0:	b.eq	170 <__mulsc3+0x170>  // b.none
  c4:	mov	w9, #0x7f800000            	// #2139095040
  c8:	fmov	s20, w9
  cc:	fcmp	s19, s20
  d0:	b.eq	170 <__mulsc3+0x170>  // b.none
  d4:	cbnz	w8, 1c0 <__mulsc3+0x1c0>
  d8:	mov	w8, #0x7f800000            	// #2139095040
  dc:	fabs	s17, s17
  e0:	fmov	s18, w8
  e4:	fcmp	s17, s18
  e8:	b.eq	128 <__mulsc3+0x128>  // b.none
  ec:	mov	w8, #0x7f800000            	// #2139095040
  f0:	fabs	s16, s16
  f4:	fmov	s17, w8
  f8:	fcmp	s16, s17
  fc:	b.eq	128 <__mulsc3+0x128>  // b.none
 100:	mov	w8, #0x7f800000            	// #2139095040
 104:	fabs	s7, s7
 108:	fmov	s16, w8
 10c:	fcmp	s7, s16
 110:	b.eq	128 <__mulsc3+0x128>  // b.none
 114:	mov	w8, #0x7f800000            	// #2139095040
 118:	fabs	s6, s6
 11c:	fmov	s7, w8
 120:	fcmp	s6, s7
 124:	b.ne	1e8 <__mulsc3+0x1e8>  // b.any
 128:	movi	v5.4s, #0x80, lsl #24
 12c:	fmov	s6, wzr
 130:	fmov	s7, wzr
 134:	bit	v6.16b, v0.16b, v5.16b
 138:	fcmp	s0, s0
 13c:	fmov	s16, wzr
 140:	bit	v7.16b, v1.16b, v5.16b
 144:	fcsel	s0, s6, s0, vs
 148:	fcmp	s1, s1
 14c:	fmov	s4, wzr
 150:	bit	v16.16b, v2.16b, v5.16b
 154:	fcsel	s1, s7, s1, vs
 158:	fcmp	s2, s2
 15c:	bit	v4.16b, v3.16b, v5.16b
 160:	fcsel	s2, s16, s2, vs
 164:	fcmp	s3, s3
 168:	fcsel	s3, s4, s3, vs
 16c:	b	1c0 <__mulsc3+0x1c0>
 170:	mov	w8, #0x7f800000            	// #2139095040
 174:	fmov	s7, w8
 178:	fmov	s4, wzr
 17c:	fmov	s5, #1.000000000000000000e+00
 180:	fcmp	s19, s7
 184:	movi	v6.4s, #0x80, lsl #24
 188:	fcsel	s16, s5, s4, eq  // eq = none
 18c:	fcmp	s18, s7
 190:	fmov	s7, wzr
 194:	bit	v7.16b, v0.16b, v6.16b
 198:	bit	v16.16b, v2.16b, v6.16b
 19c:	fcsel	s2, s5, s4, eq  // eq = none
 1a0:	fcmp	s0, s0
 1a4:	bit	v4.16b, v1.16b, v6.16b
 1a8:	bit	v2.16b, v3.16b, v6.16b
 1ac:	fcsel	s0, s7, s0, vs
 1b0:	fcmp	s1, s1
 1b4:	fcsel	s1, s4, s1, vs
 1b8:	mov	v3.16b, v2.16b
 1bc:	mov	v2.16b, v16.16b
 1c0:	fmul	s4, s2, s0
 1c4:	fmul	s5, s3, s1
 1c8:	mov	w8, #0x7f800000            	// #2139095040
 1cc:	fmul	s0, s3, s0
 1d0:	fmul	s1, s2, s1
 1d4:	fsub	s2, s4, s5
 1d8:	fmov	s3, w8
 1dc:	fadd	s0, s1, s0
 1e0:	fmul	s4, s2, s3
 1e4:	fmul	s5, s0, s3
 1e8:	mov	v0.16b, v4.16b
 1ec:	mov	v1.16b, v5.16b
 1f0:	ret

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	fmov	w13, s0
   4:	ubfx	w9, w13, #23, #8
   8:	fmov	w15, s1
   c:	eor	w8, w15, w13
  10:	sub	w12, w9, #0x1
  14:	ubfx	w10, w15, #23, #8
  18:	and	w11, w13, #0x7fffff
  1c:	and	w8, w8, #0x80000000
  20:	cmp	w12, #0xfd
  24:	and	w12, w15, #0x7fffff
  28:	b.hi	6c <__mulsf3+0x6c>  // b.pmore
  2c:	sub	w14, w10, #0x1
  30:	cmp	w14, #0xfe
  34:	b.cs	6c <__mulsf3+0x6c>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	lsl	w12, w12, #8
  40:	orr	w11, w11, #0x800000
  44:	add	w14, w9, w10
  48:	orr	w9, w12, #0x80000000
  4c:	umull	x10, w9, w11
  50:	add	w11, w14, w13
  54:	lsr	x9, x10, #32
  58:	sub	w11, w11, #0x7f
  5c:	tbnz	w9, #23, 88 <__mulsf3+0x88>
  60:	extr	w9, w9, w10, #31
  64:	lsl	w10, w10, #1
  68:	b	8c <__mulsf3+0x8c>
  6c:	mov	w16, #0x1                   	// #1
  70:	and	w14, w13, #0x7fffffff
  74:	movk	w16, #0x7f80, lsl #16
  78:	cmp	w14, w16
  7c:	b.cc	ac <__mulsf3+0xac>  // b.lo, b.ul, b.last
  80:	orr	w8, w13, #0x400000
  84:	b	194 <__mulsf3+0x194>
  88:	add	w11, w11, #0x1
  8c:	cmp	w11, #0xff
  90:	b.lt	9c <__mulsf3+0x9c>  // b.tstop
  94:	orr	w8, w8, #0x7f800000
  98:	b	194 <__mulsf3+0x194>
  9c:	cmp	w11, #0x0
  a0:	b.le	d4 <__mulsf3+0xd4>
  a4:	bfi	w9, w11, #23, #9
  a8:	b	108 <__mulsf3+0x108>
  ac:	and	w13, w15, #0x7fffffff
  b0:	cmp	w13, w16
  b4:	b.cc	c0 <__mulsf3+0xc0>  // b.lo, b.ul, b.last
  b8:	orr	w8, w15, #0x400000
  bc:	b	194 <__mulsf3+0x194>
  c0:	mov	w15, #0x7f800000            	// #2139095040
  c4:	cmp	w14, w15
  c8:	b.ne	138 <__mulsf3+0x138>  // b.any
  cc:	cbnz	w13, 94 <__mulsf3+0x94>
  d0:	b	144 <__mulsf3+0x144>
  d4:	mov	w12, #0x1                   	// #1
  d8:	sub	w12, w12, w11
  dc:	cmp	w12, #0x1f
  e0:	b.hi	194 <__mulsf3+0x194>  // b.pmore
  e4:	add	w11, w11, #0x1f
  e8:	lsr	w13, w10, w12
  ec:	lsl	w10, w10, w11
  f0:	lsl	w11, w9, w11
  f4:	cmp	w10, #0x0
  f8:	orr	w10, w11, w13
  fc:	cset	w11, ne  // ne = any
 100:	orr	w10, w10, w11
 104:	lsr	w9, w9, w12
 108:	mov	w11, #0x80000001            	// #-2147483647
 10c:	cmp	w10, w11
 110:	orr	w8, w9, w8
 114:	b.cc	120 <__mulsf3+0x120>  // b.lo, b.ul, b.last
 118:	add	w8, w8, #0x1
 11c:	b	194 <__mulsf3+0x194>
 120:	mov	w11, #0x80000000            	// #-2147483648
 124:	cmp	w10, w11
 128:	b.ne	194 <__mulsf3+0x194>  // b.any
 12c:	and	w9, w9, #0x1
 130:	add	w8, w8, w9
 134:	b	194 <__mulsf3+0x194>
 138:	cmp	w13, w15
 13c:	b.ne	14c <__mulsf3+0x14c>  // b.any
 140:	cbnz	w14, 94 <__mulsf3+0x94>
 144:	mov	w8, #0x7fc00000            	// #2143289344
 148:	b	194 <__mulsf3+0x194>
 14c:	cbz	w14, 194 <__mulsf3+0x194>
 150:	cbz	w13, 194 <__mulsf3+0x194>
 154:	clz	w15, w11
 158:	mov	w16, #0x9                   	// #9
 15c:	cmp	w14, #0x800, lsl #12
 160:	lsr	w14, w13, #23
 164:	sub	w13, w15, #0x8
 168:	sub	w15, w16, w15
 16c:	csel	w13, w13, wzr, cc  // cc = lo, ul, last
 170:	lsl	w11, w11, w13
 174:	csel	w13, w15, wzr, cc  // cc = lo, ul, last
 178:	cbnz	w14, 3c <__mulsf3+0x3c>
 17c:	clz	w14, w12
 180:	sub	w15, w14, #0x8
 184:	sub	w13, w13, w14
 188:	lsl	w12, w12, w15
 18c:	add	w13, w13, #0x9
 190:	b	3c <__mulsf3+0x3c>
 194:	fmov	s0, w8
 198:	ret

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x9, x0, #0xffffffff
   4:	and	x10, x2, #0xffffffff
   8:	lsr	x11, x0, #32
   c:	lsr	x12, x2, #32
  10:	mul	x13, x1, x2
  14:	mul	x8, x10, x9
  18:	mul	x10, x10, x11
  1c:	mul	x9, x12, x9
  20:	madd	x13, x3, x0, x13
  24:	add	x10, x10, x8, lsr #32
  28:	madd	x11, x12, x11, x13
  2c:	add	x9, x9, w10, uxtw
  30:	add	x10, x11, x10, lsr #32
  34:	bfi	x8, x9, #32, #32
  38:	add	x1, x10, x9, lsr #32
  3c:	mov	x0, x8
  40:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	sub	sp, sp, #0xc0
   4:	str	x19, [sp, #176]
   8:	stp	q1, q0, [sp, #144]
   c:	ldp	x12, x2, [sp, #160]
  10:	ldp	x14, x3, [sp, #144]
  14:	mov	w11, #0x7ffd                	// #32765
  18:	ubfx	x9, x2, #48, #15
  1c:	eor	x8, x3, x2
  20:	and	x1, x3, #0xffffffffffff
  24:	sub	w16, w9, #0x1
  28:	and	x13, x2, #0xffffffffffff
  2c:	extr	x15, x2, x12, #32
  30:	ubfx	x0, x2, #32, #16
  34:	ubfx	x10, x3, #48, #15
  38:	extr	x17, x3, x14, #17
  3c:	and	x8, x8, #0x8000000000000000
  40:	extr	x18, x1, x14, #49
  44:	cmp	w16, w11
  48:	ubfx	x11, x3, #17, #31
  4c:	b.hi	180 <__multf3+0x180>  // b.pmore
  50:	sub	w16, w10, #0x1
  54:	mov	w4, #0x7ffe                	// #32766
  58:	cmp	w16, w4
  5c:	b.cs	180 <__multf3+0x180>  // b.hs, b.nlast
  60:	mov	w16, wzr
  64:	orr	x0, x0, #0x10000
  68:	mov	w1, #0x80000000            	// #-2147483648
  6c:	and	x17, x17, #0xffffffff
  70:	lsl	w14, w14, #15
  74:	and	x13, x13, #0xffffffff
  78:	and	x15, x15, #0xffffffff
  7c:	and	x12, x12, #0xffffffff
  80:	and	x18, x18, #0xffffffff
  84:	add	w9, w9, w10
  88:	bfxil	x1, x11, #0, #31
  8c:	mul	x2, x17, x0
  90:	mul	x4, x17, x13
  94:	mul	x7, x17, x15
  98:	mul	x19, x14, x15
  9c:	mul	x17, x17, x12
  a0:	mul	x3, x18, x13
  a4:	mul	x5, x14, x13
  a8:	mul	x6, x18, x15
  ac:	add	w16, w9, w16
  b0:	mul	x9, x1, x13
  b4:	mul	x13, x1, x15
  b8:	adds	x15, x17, x19
  bc:	adcs	x17, xzr, xzr
  c0:	adds	x5, x7, x5
  c4:	mul	x10, x18, x0
  c8:	mul	x18, x18, x12
  cc:	adcs	x7, xzr, xzr
  d0:	adds	x18, x5, x18
  d4:	mul	x11, x14, x0
  d8:	adcs	x5, x7, xzr
  dc:	adds	x11, x4, x11
  e0:	adcs	x4, xzr, xzr
  e4:	adds	x11, x11, x6
  e8:	mul	x14, x14, x12
  ec:	mul	x12, x1, x12
  f0:	adcs	x4, x4, xzr
  f4:	adds	x11, x11, x12
  f8:	adcs	x12, x4, xzr
  fc:	adds	x9, x9, x10
 100:	adcs	x4, xzr, xzr
 104:	adds	x10, x14, x15, lsl #32
 108:	extr	x17, x17, x15, #32
 10c:	adcs	x15, xzr, xzr
 110:	adds	x17, x18, x17
 114:	adcs	x18, xzr, xzr
 118:	extr	x12, x12, x11, #32
 11c:	adds	x11, x17, x11, lsl #32
 120:	adcs	x17, x18, xzr
 124:	add	x11, x15, x11
 128:	adds	x15, x3, x2
 12c:	adcs	x18, xzr, xzr
 130:	adds	x13, x15, x13
 134:	adcs	x15, x18, xzr
 138:	adds	x13, x13, x5
 13c:	adcs	x15, x15, xzr
 140:	extr	x14, x4, x9, #32
 144:	madd	x15, x1, x0, x15
 148:	adds	x9, x13, x9, lsl #32
 14c:	adcs	x13, x15, x14
 150:	adds	x9, x9, x12
 154:	adcs	x12, x13, xzr
 158:	adds	x9, x9, x17
 15c:	mov	w13, #0xffffc001            	// #-16383
 160:	adcs	x12, x12, xzr
 164:	add	w13, w16, w13
 168:	tbnz	x12, #48, 1b0 <__multf3+0x1b0>
 16c:	extr	x12, x12, x9, #63
 170:	extr	x9, x9, x11, #63
 174:	extr	x11, x11, x10, #63
 178:	lsl	x10, x10, #1
 17c:	b	1b4 <__multf3+0x1b4>
 180:	and	x16, x2, #0x7fffffffffffffff
 184:	cmp	x12, #0x0
 188:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 18c:	cset	w5, eq  // eq = none
 190:	cmp	x16, x4
 194:	cset	w6, cc  // cc = lo, ul, last
 198:	csel	w5, w5, w6, eq  // eq = none
 19c:	tbnz	w5, #0, 1e0 <__multf3+0x1e0>
 1a0:	orr	x8, x2, #0x800000000000
 1a4:	stp	x12, x8, [sp]
 1a8:	ldr	q0, [sp]
 1ac:	b	374 <__multf3+0x374>
 1b0:	add	w13, w13, #0x1
 1b4:	mov	w14, #0x7fff                	// #32767
 1b8:	cmp	w13, w14
 1bc:	b.lt	1d0 <__multf3+0x1d0>  // b.tstop
 1c0:	orr	x8, x8, #0x7fff000000000000
 1c4:	stp	xzr, x8, [sp, #80]
 1c8:	ldr	q0, [sp, #80]
 1cc:	b	374 <__multf3+0x374>
 1d0:	cmp	w13, #0x0
 1d4:	b.le	230 <__multf3+0x230>
 1d8:	bfi	x12, x13, #48, #16
 1dc:	b	328 <__multf3+0x328>
 1e0:	and	x2, x3, #0x7fffffffffffffff
 1e4:	cmp	x14, #0x0
 1e8:	cset	w5, eq  // eq = none
 1ec:	cmp	x2, x4
 1f0:	cset	w4, cc  // cc = lo, ul, last
 1f4:	csel	w4, w5, w4, eq  // eq = none
 1f8:	tbnz	w4, #0, 20c <__multf3+0x20c>
 1fc:	orr	x8, x3, #0x800000000000
 200:	stp	x14, x8, [sp, #16]
 204:	ldr	q0, [sp, #16]
 208:	b	374 <__multf3+0x374>
 20c:	eor	x3, x16, #0x7fff000000000000
 210:	orr	x3, x12, x3
 214:	cbnz	x3, 24c <__multf3+0x24c>
 218:	orr	x9, x14, x2
 21c:	cbz	x9, 380 <__multf3+0x380>
 220:	orr	x8, x8, #0x7fff000000000000
 224:	stp	xzr, x8, [sp, #32]
 228:	ldr	q0, [sp, #32]
 22c:	b	374 <__multf3+0x374>
 230:	mov	w14, #0x1                   	// #1
 234:	sub	w13, w14, w13
 238:	cmp	w13, #0x7f
 23c:	b.ls	270 <__multf3+0x270>  // b.plast
 240:	stp	xzr, x8, [sp, #96]
 244:	ldr	q0, [sp, #96]
 248:	b	374 <__multf3+0x374>
 24c:	eor	x3, x2, #0x7fff000000000000
 250:	orr	x3, x14, x3
 254:	cbnz	x3, 38c <__multf3+0x38c>
 258:	orr	x9, x12, x16
 25c:	cbz	x9, 380 <__multf3+0x380>
 260:	orr	x8, x8, #0x7fff000000000000
 264:	stp	xzr, x8, [sp, #48]
 268:	ldr	q0, [sp, #48]
 26c:	b	374 <__multf3+0x374>
 270:	mov	w14, #0x80                  	// #128
 274:	mov	w16, #0x40                  	// #64
 278:	neg	x15, x13
 27c:	sub	x14, x14, x13
 280:	sub	x16, x16, x13
 284:	lsl	x0, x9, x15
 288:	neg	x1, x14
 28c:	cmp	x16, #0x0
 290:	csel	x2, xzr, x0, ge  // ge = tcont
 294:	cmp	x14, #0x0
 298:	lsr	x14, x10, x1
 29c:	lsr	x1, x9, x1
 2a0:	lsr	x18, x10, x13
 2a4:	csel	x14, xzr, x14, eq  // eq = none
 2a8:	csel	x1, xzr, x1, eq  // eq = none
 2ac:	cmp	x16, #0x0
 2b0:	lsl	x16, x12, x15
 2b4:	lsl	x10, x10, x15
 2b8:	lsl	x15, x11, x15
 2bc:	orr	x14, x14, x15
 2c0:	csel	x14, x10, x14, ge  // ge = tcont
 2c4:	csel	x10, xzr, x10, ge  // ge = tcont
 2c8:	orr	x1, x1, x16
 2cc:	orr	x10, x10, x14
 2d0:	csel	x0, x0, x1, ge  // ge = tcont
 2d4:	cmp	x10, #0x0
 2d8:	cset	w10, ne  // ne = any
 2dc:	cmp	x13, #0x0
 2e0:	sub	x17, x13, #0x40
 2e4:	lsr	x1, x11, x13
 2e8:	lsr	x11, x11, x13
 2ec:	lsr	x9, x9, x13
 2f0:	lsr	x14, x12, x13
 2f4:	lsr	x12, x12, x13
 2f8:	csel	x13, xzr, x15, eq  // eq = none
 2fc:	csel	x15, xzr, x16, eq  // eq = none
 300:	cmp	x17, #0x0
 304:	orr	x13, x18, x13
 308:	csel	x13, x1, x13, ge  // ge = tcont
 30c:	csel	x11, xzr, x11, ge  // ge = tcont
 310:	orr	x9, x9, x15
 314:	orr	x13, x2, x13
 318:	orr	x11, x0, x11
 31c:	csel	x9, x14, x9, ge  // ge = tcont
 320:	orr	x10, x13, x10
 324:	csel	x12, xzr, x12, ge  // ge = tcont
 328:	cmp	x10, #0x0
 32c:	mov	x13, #0x8000000000000000    	// #-9223372036854775808
 330:	cset	w14, eq  // eq = none
 334:	cmp	x11, #0x0
 338:	cset	w15, ge  // ge = tcont
 33c:	cmp	x11, x13
 340:	csel	w13, w14, w15, eq  // eq = none
 344:	orr	x8, x12, x8
 348:	tbnz	w13, #0, 354 <__multf3+0x354>
 34c:	adds	x9, x9, #0x1
 350:	b	368 <__multf3+0x368>
 354:	eor	x11, x11, #0x8000000000000000
 358:	orr	x10, x10, x11
 35c:	cbnz	x10, 36c <__multf3+0x36c>
 360:	and	x10, x9, #0x1
 364:	adds	x9, x9, x10
 368:	adcs	x8, x8, xzr
 36c:	stp	x9, x8, [sp, #112]
 370:	ldr	q0, [sp, #112]
 374:	ldr	x19, [sp, #176]
 378:	add	sp, sp, #0xc0
 37c:	ret
 380:	adrp	x8, 0 <__multf3>
 384:	ldr	q0, [x8]
 388:	b	374 <__multf3+0x374>
 38c:	orr	x3, x12, x16
 390:	cbz	x3, 404 <__multf3+0x404>
 394:	orr	x3, x14, x2
 398:	cbz	x3, 410 <__multf3+0x410>
 39c:	lsr	x16, x16, #48
 3a0:	cbnz	x16, 41c <__multf3+0x41c>
 3a4:	cmp	x13, #0x0
 3a8:	csel	x0, x12, x13, eq  // eq = none
 3ac:	cset	w16, eq  // eq = none
 3b0:	clz	x0, x0
 3b4:	add	w16, w0, w16, lsl #6
 3b8:	mov	w15, #0x10                  	// #16
 3bc:	sub	w0, w16, #0xf
 3c0:	sub	w16, w15, w16
 3c4:	neg	x15, x0
 3c8:	cmp	x0, #0x0
 3cc:	lsl	x3, x12, x0
 3d0:	sub	x4, x0, #0x40
 3d4:	lsl	x13, x13, x0
 3d8:	lsl	x0, x12, x0
 3dc:	lsr	x12, x12, x15
 3e0:	csel	x12, xzr, x12, eq  // eq = none
 3e4:	cmp	x4, #0x0
 3e8:	orr	x12, x12, x13
 3ec:	csel	x3, xzr, x3, ge  // ge = tcont
 3f0:	csel	x13, x0, x12, ge  // ge = tcont
 3f4:	extr	x15, x13, x3, #32
 3f8:	lsr	x0, x13, #32
 3fc:	mov	x12, x3
 400:	b	420 <__multf3+0x420>
 404:	stp	xzr, x8, [sp, #128]
 408:	ldr	q0, [sp, #128]
 40c:	b	374 <__multf3+0x374>
 410:	stp	xzr, x8, [sp, #64]
 414:	ldr	q0, [sp, #64]
 418:	b	374 <__multf3+0x374>
 41c:	mov	w16, wzr
 420:	lsr	x2, x2, #48
 424:	cbnz	x2, 64 <__multf3+0x64>
 428:	cmp	x1, #0x0
 42c:	csel	x17, x14, x1, eq  // eq = none
 430:	cset	w11, eq  // eq = none
 434:	clz	x17, x17
 438:	add	w11, w17, w11, lsl #6
 43c:	sub	w17, w11, #0xf
 440:	sub	w11, w16, w11
 444:	neg	x18, x17
 448:	cmp	x17, #0x0
 44c:	add	w16, w11, #0x10
 450:	lsr	x11, x14, x18
 454:	sub	x3, x17, #0x40
 458:	lsl	x1, x1, x17
 45c:	csel	x11, xzr, x11, eq  // eq = none
 460:	lsl	x2, x14, x17
 464:	lsl	x17, x14, x17
 468:	cmp	x3, #0x0
 46c:	orr	x11, x11, x1
 470:	csel	x14, xzr, x2, ge  // ge = tcont
 474:	csel	x18, x17, x11, ge  // ge = tcont
 478:	extr	x17, x18, x14, #17
 47c:	lsr	x11, x18, #17
 480:	extr	x18, x18, x14, #49
 484:	b	64 <__multf3+0x64>

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   c:	cmp	x0, x8
  10:	b.ne	24 <__mulvdi3+0x24>  // b.any
  14:	cmp	x1, #0x1
  18:	b.hi	b4 <__mulvdi3+0xb4>  // b.pmore
  1c:	lsl	x0, x1, #63
  20:	b	ac <__mulvdi3+0xac>
  24:	cmp	x1, x8
  28:	b.ne	3c <__mulvdi3+0x3c>  // b.any
  2c:	cmp	x0, #0x1
  30:	b.hi	cc <__mulvdi3+0xcc>  // b.pmore
  34:	lsl	x0, x0, #63
  38:	b	ac <__mulvdi3+0xac>
  3c:	eor	x8, x0, x0, asr #63
  40:	sub	x8, x8, x0, asr #63
  44:	cmp	x8, #0x2
  48:	b.lt	a8 <__mulvdi3+0xa8>  // b.tstop
  4c:	eor	x9, x1, x1, asr #63
  50:	sub	x9, x9, x1, asr #63
  54:	cmp	x9, #0x2
  58:	b.lt	a8 <__mulvdi3+0xa8>  // b.tstop
  5c:	asr	x10, x0, #63
  60:	asr	x11, x1, #63
  64:	cmp	x10, x11
  68:	b.ne	94 <__mulvdi3+0x94>  // b.any
  6c:	mov	x10, #0x7fffffffffffffff    	// #9223372036854775807
  70:	udiv	x9, x10, x9
  74:	cmp	x8, x9
  78:	b.le	a8 <__mulvdi3+0xa8>
  7c:	adrp	x0, 0 <__mulvdi3>
  80:	adrp	x2, 0 <__mulvdi3>
  84:	add	x0, x0, #0x0
  88:	add	x2, x2, #0x0
  8c:	mov	w1, #0x29                  	// #41
  90:	bl	0 <__compilerrt_abort_impl>
  94:	neg	x9, x9
  98:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  9c:	sdiv	x9, x10, x9
  a0:	cmp	x8, x9
  a4:	b.gt	e4 <__mulvdi3+0xe4>
  a8:	mul	x0, x1, x0
  ac:	ldp	x29, x30, [sp], #16
  b0:	ret
  b4:	adrp	x0, 0 <__mulvdi3>
  b8:	adrp	x2, 0 <__mulvdi3>
  bc:	add	x0, x0, #0x0
  c0:	add	x2, x2, #0x0
  c4:	mov	w1, #0x1a                  	// #26
  c8:	bl	0 <__compilerrt_abort_impl>
  cc:	adrp	x0, 0 <__mulvdi3>
  d0:	adrp	x2, 0 <__mulvdi3>
  d4:	add	x0, x0, #0x0
  d8:	add	x2, x2, #0x0
  dc:	mov	w1, #0x1f                  	// #31
  e0:	bl	0 <__compilerrt_abort_impl>
  e4:	adrp	x0, 0 <__mulvdi3>
  e8:	adrp	x2, 0 <__mulvdi3>
  ec:	add	x0, x0, #0x0
  f0:	add	x2, x2, #0x0
  f4:	mov	w1, #0x2c                  	// #44
  f8:	bl	0 <__compilerrt_abort_impl>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, #0x80000000            	// #-2147483648
   c:	cmp	w0, w8
  10:	b.ne	24 <__mulvsi3+0x24>  // b.any
  14:	cmp	w1, #0x1
  18:	b.hi	b4 <__mulvsi3+0xb4>  // b.pmore
  1c:	lsl	w0, w1, #31
  20:	b	ac <__mulvsi3+0xac>
  24:	cmp	w1, w8
  28:	b.ne	3c <__mulvsi3+0x3c>  // b.any
  2c:	cmp	w0, #0x1
  30:	b.hi	cc <__mulvsi3+0xcc>  // b.pmore
  34:	lsl	w0, w0, #31
  38:	b	ac <__mulvsi3+0xac>
  3c:	eor	w8, w0, w0, asr #31
  40:	sub	w8, w8, w0, asr #31
  44:	cmp	w8, #0x2
  48:	b.lt	a8 <__mulvsi3+0xa8>  // b.tstop
  4c:	eor	w9, w1, w1, asr #31
  50:	sub	w9, w9, w1, asr #31
  54:	cmp	w9, #0x2
  58:	b.lt	a8 <__mulvsi3+0xa8>  // b.tstop
  5c:	asr	w10, w0, #31
  60:	asr	w11, w1, #31
  64:	cmp	w10, w11
  68:	b.ne	94 <__mulvsi3+0x94>  // b.any
  6c:	mov	w10, #0x7fffffff            	// #2147483647
  70:	udiv	w9, w10, w9
  74:	cmp	w8, w9
  78:	b.le	a8 <__mulvsi3+0xa8>
  7c:	adrp	x0, 0 <__mulvsi3>
  80:	adrp	x2, 0 <__mulvsi3>
  84:	add	x0, x0, #0x0
  88:	add	x2, x2, #0x0
  8c:	mov	w1, #0x29                  	// #41
  90:	bl	0 <__compilerrt_abort_impl>
  94:	neg	w9, w9
  98:	mov	w10, #0x80000000            	// #-2147483648
  9c:	sdiv	w9, w10, w9
  a0:	cmp	w8, w9
  a4:	b.gt	e4 <__mulvsi3+0xe4>
  a8:	mul	w0, w1, w0
  ac:	ldp	x29, x30, [sp], #16
  b0:	ret
  b4:	adrp	x0, 0 <__mulvsi3>
  b8:	adrp	x2, 0 <__mulvsi3>
  bc:	add	x0, x0, #0x0
  c0:	add	x2, x2, #0x0
  c4:	mov	w1, #0x1a                  	// #26
  c8:	bl	0 <__compilerrt_abort_impl>
  cc:	adrp	x0, 0 <__mulvsi3>
  d0:	adrp	x2, 0 <__mulvsi3>
  d4:	add	x0, x0, #0x0
  d8:	add	x2, x2, #0x0
  dc:	mov	w1, #0x1f                  	// #31
  e0:	bl	0 <__compilerrt_abort_impl>
  e4:	adrp	x0, 0 <__mulvsi3>
  e8:	adrp	x2, 0 <__mulvsi3>
  ec:	add	x0, x0, #0x0
  f0:	add	x2, x2, #0x0
  f4:	mov	w1, #0x2c                  	// #44
  f8:	bl	0 <__compilerrt_abort_impl>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	stp	x24, x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	eor	x8, x1, #0x8000000000000000
  18:	mov	x21, x3
  1c:	orr	x8, x0, x8
  20:	mov	x20, x2
  24:	cbnz	x8, 4c <__mulvti3+0x4c>
  28:	cmp	x20, #0x1
  2c:	cset	w8, hi  // hi = pmore
  30:	cmp	x21, #0x0
  34:	cset	w9, ne  // ne = any
  38:	csel	w8, w8, w9, eq  // eq = none
  3c:	tbnz	w8, #0, 174 <__mulvti3+0x174>
  40:	mov	x0, xzr
  44:	lsl	x1, x20, #63
  48:	b	160 <__mulvti3+0x160>
  4c:	eor	x8, x21, #0x8000000000000000
  50:	mov	x22, x1
  54:	mov	x19, x0
  58:	orr	x8, x20, x8
  5c:	cbnz	x8, 84 <__mulvti3+0x84>
  60:	cmp	x19, #0x1
  64:	cset	w8, hi  // hi = pmore
  68:	cmp	x22, #0x0
  6c:	cset	w9, ne  // ne = any
  70:	csel	w8, w8, w9, eq  // eq = none
  74:	tbnz	w8, #0, 18c <__mulvti3+0x18c>
  78:	mov	x0, xzr
  7c:	lsl	x1, x19, #63
  80:	b	160 <__mulvti3+0x160>
  84:	eor	x10, x19, x22, asr #63
  88:	asr	x8, x22, #63
  8c:	eor	x11, x22, x22, asr #63
  90:	subs	x24, x10, x22, asr #63
  94:	eor	x12, x20, x21, asr #63
  98:	sbcs	x23, x11, x8
  9c:	asr	x9, x21, #63
  a0:	eor	x13, x21, x21, asr #63
  a4:	subs	x2, x12, x21, asr #63
  a8:	sbcs	x3, x13, x9
  ac:	cmp	x24, #0x2
  b0:	cset	w10, cc  // cc = lo, ul, last
  b4:	cmp	x23, #0x0
  b8:	cset	w11, lt  // lt = tstop
  bc:	csel	w10, w10, w11, eq  // eq = none
  c0:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  c4:	cmp	x2, #0x2
  c8:	cset	w10, cc  // cc = lo, ul, last
  cc:	cmp	x3, #0x0
  d0:	cset	w11, lt  // lt = tstop
  d4:	csel	w10, w10, w11, eq  // eq = none
  d8:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  dc:	eor	x8, x8, x9
  e0:	orr	x8, x8, x8
  e4:	cbnz	x8, 124 <__mulvti3+0x124>
  e8:	mov	x0, #0xffffffffffffffff    	// #-1
  ec:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  f0:	bl	0 <__udivti3>
  f4:	cmp	x24, x0
  f8:	cset	w8, ls  // ls = plast
  fc:	cmp	x23, x1
 100:	cset	w9, le
 104:	csel	w8, w8, w9, eq  // eq = none
 108:	tbnz	w8, #0, 150 <__mulvti3+0x150>
 10c:	adrp	x0, 0 <__mulvti3>
 110:	adrp	x2, 0 <__mulvti3>
 114:	add	x0, x0, #0x0
 118:	add	x2, x2, #0x0
 11c:	mov	w1, #0x2b                  	// #43
 120:	bl	0 <__compilerrt_abort_impl>
 124:	negs	x2, x2
 128:	ngcs	x3, x3
 12c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 130:	mov	x0, xzr
 134:	bl	0 <__divti3>
 138:	cmp	x24, x0
 13c:	cset	w8, ls  // ls = plast
 140:	cmp	x23, x1
 144:	cset	w9, le
 148:	csel	w8, w8, w9, eq  // eq = none
 14c:	tbz	w8, #0, 1a4 <__mulvti3+0x1a4>
 150:	umulh	x8, x20, x19
 154:	madd	x8, x20, x22, x8
 158:	madd	x1, x21, x19, x8
 15c:	mul	x0, x20, x19
 160:	ldp	x20, x19, [sp, #48]
 164:	ldp	x22, x21, [sp, #32]
 168:	ldp	x24, x23, [sp, #16]
 16c:	ldp	x29, x30, [sp], #64
 170:	ret
 174:	adrp	x0, 0 <__mulvti3>
 178:	adrp	x2, 0 <__mulvti3>
 17c:	add	x0, x0, #0x0
 180:	add	x2, x2, #0x0
 184:	mov	w1, #0x1c                  	// #28
 188:	bl	0 <__compilerrt_abort_impl>
 18c:	adrp	x0, 0 <__mulvti3>
 190:	adrp	x2, 0 <__mulvti3>
 194:	add	x0, x0, #0x0
 198:	add	x2, x2, #0x0
 19c:	mov	w1, #0x21                  	// #33
 1a0:	bl	0 <__compilerrt_abort_impl>
 1a4:	adrp	x0, 0 <__mulvti3>
 1a8:	adrp	x2, 0 <__mulvti3>
 1ac:	add	x0, x0, #0x0
 1b0:	add	x2, x2, #0x0
 1b4:	mov	w1, #0x2e                  	// #46
 1b8:	bl	0 <__compilerrt_abort_impl>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	fneg	d0, d0
   4:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	fneg	s0, s0
   4:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngcs	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	14 <__negvdi2+0x14>  // b.none
   c:	neg	x0, x0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvdi2>
  20:	adrp	x2, 0 <__negvdi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	14 <__negvsi2+0x14>  // b.none
   c:	neg	w0, w0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvsi2>
  20:	adrp	x2, 0 <__negvsi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 18 <__negvti2+0x18>
   c:	negs	x0, x0
  10:	ngcs	x1, x1
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__negvti2>
  24:	adrp	x2, 0 <__negvti2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x18                  	// #24
  34:	bl	0 <__compilerrt_abort_impl>

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	lsr	x8, x0, #32
   4:	eor	w0, w8, w0
   8:	b	0 <__paritysi2>

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w8, w0, w0, lsr #16
   4:	eor	w8, w8, w8, lsr #8
   8:	eor	w8, w8, w8, lsr #4
   c:	and	w8, w8, #0xf
  10:	mov	w9, #0x6996                	// #27030
  14:	lsr	w8, w9, w8
  18:	and	w0, w8, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x0, x1, x0
   4:	b	0 <__paritydi2>

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x8, x0, #1
   4:	and	x8, x8, #0x5555555555555555
   8:	sub	x8, x0, x8
   c:	lsr	x9, x8, #2
  10:	and	x9, x9, #0x3333333333333333
  14:	and	x8, x8, #0x3333333333333333
  18:	add	x8, x9, x8
  1c:	add	x8, x8, x8, lsr #4
  20:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  24:	lsr	x9, x8, #32
  28:	add	w8, w9, w8
  2c:	add	w8, w8, w8, lsr #16
  30:	add	w8, w8, w8, lsr #8
  34:	and	w0, w8, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w8, w0, #1
   4:	and	w8, w8, #0x55555555
   8:	sub	w8, w0, w8
   c:	lsr	w9, w8, #2
  10:	and	w9, w9, #0x33333333
  14:	and	w8, w8, #0x33333333
  18:	add	w8, w9, w8
  1c:	add	w8, w8, w8, lsr #4
  20:	and	w8, w8, #0xf0f0f0f
  24:	add	w8, w8, w8, lsr #16
  28:	add	w8, w8, w8, lsr #8
  2c:	and	w0, w8, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x8, x0, #1
   4:	lsr	x9, x1, #1
   8:	and	x8, x8, #0x5555555555555555
   c:	and	x9, x9, #0x5555555555555555
  10:	subs	x8, x0, x8
  14:	sbcs	x9, x1, x9
  18:	lsr	x10, x8, #2
  1c:	lsr	x11, x9, #2
  20:	and	x8, x8, #0x3333333333333333
  24:	and	x10, x10, #0x3333333333333333
  28:	and	x9, x9, #0x3333333333333333
  2c:	and	x11, x11, #0x3333333333333333
  30:	add	x8, x10, x8
  34:	add	x9, x11, x9
  38:	add	x8, x8, x8, lsr #4
  3c:	add	x9, x9, x9, lsr #4
  40:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  44:	and	x9, x9, #0xf0f0f0f0f0f0f0f
  48:	add	x8, x9, x8
  4c:	lsr	x9, x8, #32
  50:	add	w8, w9, w8
  54:	add	w8, w8, w8, lsr #16
  58:	add	w8, w8, w8, lsr #8
  5c:	and	w0, w8, #0xff
  60:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	tst	w0, #0x1
   4:	fmov	d1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	d2, d1, d0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	48 <__powidf2+0x48>  // b.lo, b.ul, b.last
  18:	mov	w8, #0x1                   	// #1
  1c:	mov	w9, w0
  20:	cmp	w9, #0x0
  24:	cinc	w10, w9, lt  // lt = tstop
  28:	fmul	d0, d0, d0
  2c:	asr	w9, w10, #1
  30:	fmul	d3, d0, d2
  34:	tst	w8, w10, asr #1
  38:	add	w10, w9, #0x1
  3c:	fcsel	d2, d2, d3, eq  // eq = none
  40:	cmp	w10, #0x2
  44:	b.hi	20 <__powidf2+0x20>  // b.pmore
  48:	fdiv	d0, d1, d2
  4c:	cmp	w0, #0x0
  50:	fcsel	d0, d0, d2, lt  // lt = tstop
  54:	ret

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	tst	w0, #0x1
   4:	fmov	s1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	s2, s1, s0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	48 <__powisf2+0x48>  // b.lo, b.ul, b.last
  18:	mov	w8, #0x1                   	// #1
  1c:	mov	w9, w0
  20:	cmp	w9, #0x0
  24:	cinc	w10, w9, lt  // lt = tstop
  28:	fmul	s0, s0, s0
  2c:	asr	w9, w10, #1
  30:	fmul	s3, s0, s2
  34:	tst	w8, w10, asr #1
  38:	add	w10, w9, #0x1
  3c:	fcsel	s2, s2, s3, eq  // eq = none
  40:	cmp	w10, #0x2
  44:	b.hi	20 <__powisf2+0x20>  // b.pmore
  48:	fdiv	s0, s1, s2
  4c:	cmp	w0, #0x0
  50:	fcsel	s0, s0, s2, lt  // lt = tstop
  54:	ret

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	fneg	d1, d1
   4:	b	0 <__adddf3>

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	fneg	s1, s1
   4:	b	0 <__addsf3>

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	sub	x0, x0, x1
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__subvdi3+0x34>
  18:	b.le	38 <__subvdi3+0x38>
  1c:	adrp	x0, 0 <__subvdi3>
  20:	adrp	x2, 0 <__subvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvdi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvdi3>
  44:	adrp	x2, 0 <__subvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	sub	w0, w0, w1
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__subvsi3+0x34>
  18:	b.le	38 <__subvsi3+0x38>
  1c:	adrp	x0, 0 <__subvsi3>
  20:	adrp	x2, 0 <__subvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvsi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvsi3>
  44:	adrp	x2, 0 <__subvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	subs	x0, x0, x2
  10:	mov	x8, x1
  14:	sbcs	x1, x1, x3
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__subvti3+0x4c>
  20:	cset	w9, ls  // ls = plast
  24:	cmp	x1, x8
  28:	cset	w8, le
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__subvti3+0x60>
  34:	adrp	x0, 0 <__subvti3>
  38:	adrp	x2, 0 <__subvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, hi  // hi = pmore
  50:	cmp	x1, x8
  54:	cset	w8, gt
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__subvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__subvti3>
  6c:	adrp	x2, 0 <__subvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	sub	sp, sp, #0x20
   4:	str	q1, [sp, #16]
   8:	ldp	x9, x8, [sp, #16]
   c:	eor	x8, x8, #0x8000000000000000
  10:	stp	x9, x8, [sp]
  14:	ldr	q1, [sp], #32
  18:	b	0 <__addtf3>

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	fmov	x8, d0
   4:	and	x9, x8, #0x7fffffffffffffff
   8:	mov	x10, #0xc0f0000000000000    	// #-4544132024016830464
   c:	mov	x11, #0xbf10000000000000    	// #-4679240012837945344
  10:	add	x10, x9, x10
  14:	add	x11, x9, x11
  18:	cmp	x10, x11
  1c:	b.cs	44 <__truncdfhf2+0x44>  // b.hs, b.nlast
  20:	mov	x9, #0x1                   	// #1
  24:	and	x10, x8, #0x3ffffffffff
  28:	movk	x9, #0x200, lsl #32
  2c:	cmp	x10, x9
  30:	lsr	x9, x8, #42
  34:	b.cc	60 <__truncdfhf2+0x60>  // b.lo, b.ul, b.last
  38:	mov	w10, #0x4001                	// #16385
  3c:	add	w9, w9, w10
  40:	b	fc <__truncdfhf2+0xfc>
  44:	mov	x10, #0x1                   	// #1
  48:	movk	x10, #0x7ff0, lsl #48
  4c:	cmp	x9, x10
  50:	b.cc	7c <__truncdfhf2+0x7c>  // b.lo, b.ul, b.last
  54:	ubfx	x9, x8, #42, #9
  58:	orr	w9, w9, #0x7e00
  5c:	b	fc <__truncdfhf2+0xfc>
  60:	mov	x11, #0x20000000000         	// #2199023255552
  64:	cmp	x10, x11
  68:	add	w9, w9, #0x4, lsl #12
  6c:	b.ne	fc <__truncdfhf2+0xfc>  // b.any
  70:	and	w10, w9, #0x1
  74:	add	w9, w10, w9
  78:	b	fc <__truncdfhf2+0xfc>
  7c:	lsr	x9, x9, #52
  80:	cmp	x9, #0x40e
  84:	b.ls	90 <__truncdfhf2+0x90>  // b.plast
  88:	mov	w9, #0x7c00                	// #31744
  8c:	b	fc <__truncdfhf2+0xfc>
  90:	cmp	w9, #0x3bd
  94:	b.cs	a0 <__truncdfhf2+0xa0>  // b.hs, b.nlast
  98:	mov	w9, wzr
  9c:	b	fc <__truncdfhf2+0xfc>
  a0:	mov	x10, #0x10000000000000      	// #4503599627370496
  a4:	mov	w11, #0x3f1                 	// #1009
  a8:	sub	w12, w9, #0x3b1
  ac:	bfxil	x10, x8, #0, #52
  b0:	sub	w9, w11, w9
  b4:	lsl	x11, x10, x12
  b8:	lsr	x10, x10, x9
  bc:	cmp	x11, #0x0
  c0:	and	x9, x10, #0x3ffffffffff
  c4:	cset	w11, ne  // ne = any
  c8:	orr	x11, x9, x11
  cc:	mov	x9, #0x1                   	// #1
  d0:	movk	x9, #0x200, lsl #32
  d4:	cmp	x11, x9
  d8:	lsr	x9, x10, #42
  dc:	b.cc	e8 <__truncdfhf2+0xe8>  // b.lo, b.ul, b.last
  e0:	add	w9, w9, #0x1
  e4:	b	fc <__truncdfhf2+0xfc>
  e8:	mov	x12, #0x20000000000         	// #2199023255552
  ec:	cmp	x11, x12
  f0:	b.ne	fc <__truncdfhf2+0xfc>  // b.any
  f4:	ubfx	x10, x10, #42, #1
  f8:	add	w9, w10, w9
  fc:	lsr	x8, x8, #48
 100:	and	w8, w8, #0x8000
 104:	orr	w0, w9, w8
 108:	ret

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	fmov	x8, d0
   4:	and	x9, x8, #0x7fffffffffffffff
   8:	mov	x10, #0xc7f0000000000000    	// #-4039728865751334912
   c:	mov	x11, #0xb810000000000000    	// #-5183643171103440896
  10:	add	x10, x9, x10
  14:	add	x11, x9, x11
  18:	cmp	x10, x11
  1c:	b.cs	48 <__truncdfsf2+0x48>  // b.hs, b.nlast
  20:	mov	w10, #0x1                   	// #1
  24:	and	x9, x8, #0x1fffffff
  28:	movk	w10, #0x1000, lsl #16
  2c:	cmp	x9, x10
  30:	lsr	x10, x8, #29
  34:	b.cc	64 <__truncdfsf2+0x64>  // b.lo, b.ul, b.last
  38:	mov	w9, #0x1                   	// #1
  3c:	movk	w9, #0x4000, lsl #16
  40:	add	w9, w10, w9
  44:	b	fc <__truncdfsf2+0xfc>
  48:	mov	x10, #0x1                   	// #1
  4c:	movk	x10, #0x7ff0, lsl #48
  50:	cmp	x9, x10
  54:	b.cc	84 <__truncdfsf2+0x84>  // b.lo, b.ul, b.last
  58:	ubfx	x9, x8, #29, #22
  5c:	orr	w9, w9, #0x7fc00000
  60:	b	fc <__truncdfsf2+0xfc>
  64:	mov	w11, #0x40000000            	// #1073741824
  68:	mov	w12, #0x10000000            	// #268435456
  6c:	cmp	x9, x12
  70:	add	w9, w10, w11
  74:	b.ne	fc <__truncdfsf2+0xfc>  // b.any
  78:	and	w10, w9, #0x1
  7c:	add	w9, w10, w9
  80:	b	fc <__truncdfsf2+0xfc>
  84:	lsr	x9, x9, #52
  88:	cmp	x9, #0x47e
  8c:	b.ls	98 <__truncdfsf2+0x98>  // b.plast
  90:	mov	w9, #0x7f800000            	// #2139095040
  94:	b	fc <__truncdfsf2+0xfc>
  98:	cmp	w9, #0x34d
  9c:	b.cs	a8 <__truncdfsf2+0xa8>  // b.hs, b.nlast
  a0:	mov	w9, wzr
  a4:	b	fc <__truncdfsf2+0xfc>
  a8:	mov	x10, #0x10000000000000      	// #4503599627370496
  ac:	mov	w11, #0x381                 	// #897
  b0:	sub	w12, w9, #0x341
  b4:	bfxil	x10, x8, #0, #52
  b8:	sub	w9, w11, w9
  bc:	lsl	x11, x10, x12
  c0:	lsr	x9, x10, x9
  c4:	cmp	x11, #0x0
  c8:	and	x10, x9, #0x1fffffff
  cc:	cset	w11, ne  // ne = any
  d0:	orr	x10, x10, x11
  d4:	mov	w11, #0x1                   	// #1
  d8:	movk	w11, #0x1000, lsl #16
  dc:	cmp	x10, x11
  e0:	lsr	x9, x9, #29
  e4:	b.cc	f0 <__truncdfsf2+0xf0>  // b.lo, b.ul, b.last
  e8:	add	w9, w9, #0x1
  ec:	b	fc <__truncdfsf2+0xfc>
  f0:	mov	w11, #0x10000000            	// #268435456
  f4:	cmp	x10, x11
  f8:	b.eq	78 <__truncdfsf2+0x78>  // b.none
  fc:	lsr	x8, x8, #32
 100:	and	w8, w8, #0x80000000
 104:	orr	w8, w9, w8
 108:	fmov	s0, w8
 10c:	ret

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	fmov	w8, s0
   4:	and	w9, w8, #0x7fffffff
   8:	mov	w10, #0xc7800000            	// #-947912704
   c:	mov	w11, #0xb8800000            	// #-1199570944
  10:	add	w10, w9, w10
  14:	add	w11, w9, w11
  18:	cmp	w10, w11
  1c:	b.cs	48 <__truncsfhf2+0x48>  // b.hs, b.nlast
  20:	ubfx	w10, w8, #13, #16
  24:	and	w9, w8, #0x1fff
  28:	cmp	w9, #0x1, lsl #12
  2c:	sub	w9, w10, #0x1c, lsl #12
  30:	b.ls	64 <__truncsfhf2+0x64>  // b.plast
  34:	mov	w9, #0x4000                	// #16384
  38:	movk	w9, #0xfffe, lsl #16
  3c:	add	w9, w9, w10
  40:	add	w9, w9, #0x1
  44:	b	e0 <__truncsfhf2+0xe0>
  48:	mov	w10, #0x1                   	// #1
  4c:	movk	w10, #0x7f80, lsl #16
  50:	cmp	w9, w10
  54:	b.cc	74 <__truncsfhf2+0x74>  // b.lo, b.ul, b.last
  58:	mov	w9, #0x7e00                	// #32256
  5c:	bfxil	w9, w8, #13, #9
  60:	b	e0 <__truncsfhf2+0xe0>
  64:	b.ne	e0 <__truncsfhf2+0xe0>  // b.any
  68:	and	w10, w9, #0x1
  6c:	add	w9, w10, w9, uxth
  70:	b	e0 <__truncsfhf2+0xe0>
  74:	lsr	w10, w9, #23
  78:	cmp	w10, #0x8e
  7c:	b.ls	88 <__truncsfhf2+0x88>  // b.plast
  80:	mov	w9, #0x7c00                	// #31744
  84:	b	e0 <__truncsfhf2+0xe0>
  88:	lsr	w9, w9, #24
  8c:	cmp	w9, #0x2d
  90:	b.cs	9c <__truncsfhf2+0x9c>  // b.hs, b.nlast
  94:	mov	w9, wzr
  98:	b	e0 <__truncsfhf2+0xe0>
  9c:	mov	w9, #0x800000              	// #8388608
  a0:	mov	w11, #0x71                  	// #113
  a4:	sub	w12, w10, #0x51
  a8:	bfxil	w9, w8, #0, #23
  ac:	sub	w10, w11, w10
  b0:	lsl	w11, w9, w12
  b4:	lsr	w10, w9, w10
  b8:	cmp	w11, #0x0
  bc:	cset	w9, ne  // ne = any
  c0:	and	w11, w10, #0x1fff
  c4:	orr	w9, w11, w9
  c8:	cmp	w9, #0x1, lsl #12
  cc:	lsr	w9, w10, #13
  d0:	b.hi	40 <__truncsfhf2+0x40>  // b.pmore
  d4:	b.ne	e0 <__truncsfhf2+0xe0>  // b.any
  d8:	ubfx	w9, w10, #13, #1
  dc:	add	w9, w9, w10, lsr #13
  e0:	lsr	w8, w8, #16
  e4:	and	w8, w8, #0x8000
  e8:	orr	w0, w9, w8
  ec:	ret

00000000000000f0 <__gnu_f2h_ieee>:
  f0:	b	0 <__truncsfhf2>

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.cc	24 <__ucmpdi2+0x24>  // b.lo, b.ul, b.last
  10:	b.ls	1c <__ucmpdi2+0x1c>  // b.plast
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	w0, w1
  20:	b.cs	2c <__ucmpdi2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	cmp	x1, x3
   4:	b.cc	1c <__ucmpti2+0x1c>  // b.lo, b.ul, b.last
   8:	b.ls	14 <__ucmpti2+0x14>  // b.plast
   c:	mov	w0, #0x2                   	// #2
  10:	ret
  14:	cmp	x0, x2
  18:	b.cs	24 <__ucmpti2+0x24>  // b.hs, b.nlast
  1c:	mov	w0, wzr
  20:	ret
  24:	mov	w8, #0x1                   	// #1
  28:	cinc	w0, w8, hi  // hi = pmore
  2c:	ret

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	mov	x2, xzr
   4:	b	0 <__udivmoddi4>

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cbz	w8, 34 <__udivmoddi4+0x34>
   c:	cbz	w1, 4c <__udivmoddi4+0x4c>
  10:	cbz	w9, 88 <__udivmoddi4+0x88>
  14:	clz	w9, w9
  18:	clz	w10, w8
  1c:	sub	w12, w9, w10
  20:	cmp	w12, #0x20
  24:	b.cc	ec <__udivmoddi4+0xec>  // b.lo, b.ul, b.last
  28:	cbz	x2, 130 <__udivmoddi4+0x130>
  2c:	str	x0, [x2]
  30:	b	130 <__udivmoddi4+0x130>
  34:	cbz	w9, d4 <__udivmoddi4+0xd4>
  38:	cbz	x2, 130 <__udivmoddi4+0x130>
  3c:	and	x8, x0, #0xffffffff
  40:	mov	x0, xzr
  44:	str	x8, [x2]
  48:	ret
  4c:	cbz	w9, 120 <__udivmoddi4+0x120>
  50:	cbz	w0, 1cc <__udivmoddi4+0x1cc>
  54:	sub	w10, w9, #0x1
  58:	tst	w9, w10
  5c:	b.ne	1e0 <__udivmoddi4+0x1e0>  // b.any
  60:	cbz	x2, 78 <__udivmoddi4+0x78>
  64:	mov	w10, #0xffffffff            	// #-1
  68:	add	x10, x9, x10
  6c:	and	w10, w10, w8
  70:	bfi	x0, x10, #32, #32
  74:	str	x0, [x2]
  78:	rbit	w9, w9
  7c:	clz	w9, w9
  80:	lsr	w0, w8, w9
  84:	ret
  88:	sub	w9, w1, #0x1
  8c:	tst	w1, w9
  90:	b.ne	138 <__udivmoddi4+0x138>  // b.any
  94:	cbz	x2, a8 <__udivmoddi4+0xa8>
  98:	mov	w9, #0xffffffff            	// #-1
  9c:	add	x9, x1, x9
  a0:	and	w9, w0, w9
  a4:	str	x9, [x2]
  a8:	cmp	w1, #0x1
  ac:	b.eq	1c8 <__udivmoddi4+0x1c8>  // b.none
  b0:	rbit	w9, w1
  b4:	clz	w9, w9
  b8:	neg	w11, w9
  bc:	lsr	w10, w8, w9
  c0:	lsl	w8, w8, w11
  c4:	lsr	w9, w0, w9
  c8:	orr	w0, w8, w9
  cc:	bfi	x0, x10, #32, #32
  d0:	ret
  d4:	udiv	w8, w0, w1
  d8:	cbz	x2, e4 <__udivmoddi4+0xe4>
  dc:	msub	w9, w8, w1, w0
  e0:	str	x9, [x2]
  e4:	mov	x0, x8
  e8:	ret
  ec:	add	w9, w12, #0x1
  f0:	cmp	w9, #0x20
  f4:	b.eq	150 <__udivmoddi4+0x150>  // b.none
  f8:	mov	w13, #0x1f                  	// #31
  fc:	sub	w12, w13, w12
 100:	lsr	w10, w8, w9
 104:	lsr	w14, w0, w9
 108:	lsl	w13, w0, w12
 10c:	lsl	w8, w8, w12
 110:	mov	w11, wzr
 114:	orr	w8, w8, w14
 118:	mov	w0, w13
 11c:	b	158 <__udivmoddi4+0x158>
 120:	cbz	x2, 130 <__udivmoddi4+0x130>
 124:	mov	x0, xzr
 128:	str	xzr, [x2]
 12c:	ret
 130:	mov	x0, xzr
 134:	ret
 138:	clz	w9, w1
 13c:	clz	w10, w8
 140:	sub	w9, w9, w10
 144:	add	w9, w9, #0x21
 148:	cmp	w9, #0x20
 14c:	b.ne	220 <__udivmoddi4+0x220>  // b.any
 150:	mov	w10, wzr
 154:	mov	w11, wzr
 158:	mov	w13, wzr
 15c:	mov	w12, w0
 160:	extr	w10, w10, w8, #31
 164:	extr	w8, w8, w12, #31
 168:	bfi	x8, x10, #32, #32
 16c:	mvn	x10, x8
 170:	add	x10, x10, x1
 174:	extr	w12, w12, w11, #31
 178:	orr	w11, w13, w11, lsl #1
 17c:	asr	x13, x10, #63
 180:	and	x10, x1, x10, asr #63
 184:	sub	x8, x8, x10
 188:	and	w13, w13, #0x1
 18c:	subs	w9, w9, #0x1
 190:	lsr	x10, x8, #32
 194:	b.ne	160 <__udivmoddi4+0x160>  // b.any
 198:	mov	w9, w11
 19c:	lsl	x9, x9, #1
 1a0:	lsl	x11, x12, #33
 1a4:	and	x12, x9, #0x100000000
 1a8:	cbz	x2, 1b8 <__udivmoddi4+0x1b8>
 1ac:	mov	w8, w8
 1b0:	bfi	x8, x10, #32, #32
 1b4:	str	x8, [x2]
 1b8:	and	x8, x9, #0xfffffffe
 1bc:	orr	x9, x12, x11
 1c0:	orr	x8, x9, x8
 1c4:	orr	x0, x8, x13
 1c8:	ret
 1cc:	udiv	w0, w8, w9
 1d0:	cbz	x2, 1c8 <__udivmoddi4+0x1c8>
 1d4:	msub	w8, w0, w9, w8
 1d8:	lsl	x8, x8, #32
 1dc:	b	44 <__udivmoddi4+0x44>
 1e0:	clz	w9, w9
 1e4:	clz	w10, w8
 1e8:	sub	w10, w9, w10
 1ec:	cmp	w10, #0x1f
 1f0:	b.cs	28 <__udivmoddi4+0x28>  // b.hs, b.nlast
 1f4:	mov	w12, #0x1f                  	// #31
 1f8:	add	w9, w10, #0x1
 1fc:	sub	w12, w12, w10
 200:	lsr	w10, w8, w9
 204:	lsr	w13, w0, w9
 208:	lsl	w14, w0, w12
 20c:	lsl	w8, w8, w12
 210:	mov	w11, wzr
 214:	orr	w8, w8, w13
 218:	mov	w0, w14
 21c:	b	158 <__udivmoddi4+0x158>
 220:	b.cs	24c <__udivmoddi4+0x24c>  // b.hs, b.nlast
 224:	neg	w13, w9
 228:	lsr	w10, w8, w9
 22c:	lsl	w12, w0, w13
 230:	lsl	w8, w8, w13
 234:	lsr	w13, w0, w9
 238:	orr	w8, w8, w13
 23c:	cbz	w9, 26c <__udivmoddi4+0x26c>
 240:	mov	w11, wzr
 244:	mov	w0, w12
 248:	b	158 <__udivmoddi4+0x158>
 24c:	neg	w12, w9
 250:	lsr	w13, w0, w9
 254:	lsl	w11, w0, w12
 258:	lsl	w12, w8, w12
 25c:	mov	w10, wzr
 260:	orr	w0, w12, w13
 264:	lsr	w8, w8, w9
 268:	b	158 <__udivmoddi4+0x158>
 26c:	mov	x9, xzr
 270:	mov	x13, xzr
 274:	b	1a0 <__udivmoddi4+0x1a0>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__udivsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	cbz	x1, 2c <__udivmodti4+0x2c>
   4:	cbz	x2, 44 <__udivmodti4+0x44>
   8:	cbz	x3, 74 <__udivmodti4+0x74>
   c:	clz	x8, x3
  10:	clz	x9, x1
  14:	sub	w11, w8, w9
  18:	cmp	w11, #0x40
  1c:	b.cc	d0 <__udivmodti4+0xd0>  // b.lo, b.ul, b.last
  20:	cbz	x4, 114 <__udivmodti4+0x114>
  24:	stp	x0, x1, [x4]
  28:	b	114 <__udivmodti4+0x114>
  2c:	cbz	x3, b4 <__udivmodti4+0xb4>
  30:	cbz	x4, 114 <__udivmodti4+0x114>
  34:	mov	x1, xzr
  38:	stp	x0, xzr, [x4]
  3c:	mov	x0, xzr
  40:	ret
  44:	cbz	x3, 100 <__udivmodti4+0x100>
  48:	cbz	x0, 1c0 <__udivmodti4+0x1c0>
  4c:	sub	x8, x3, #0x1
  50:	tst	x3, x8
  54:	b.ne	1d4 <__udivmodti4+0x1d4>  // b.any
  58:	cbz	x4, 64 <__udivmodti4+0x64>
  5c:	and	x8, x8, x1
  60:	stp	x0, x8, [x4]
  64:	rbit	x8, x3
  68:	clz	x8, x8
  6c:	lsr	x0, x1, x8
  70:	b	118 <__udivmodti4+0x118>
  74:	sub	x8, x2, #0x1
  78:	tst	x2, x8
  7c:	b.ne	120 <__udivmodti4+0x120>  // b.any
  80:	cbz	x4, 8c <__udivmodti4+0x8c>
  84:	and	x8, x8, x0
  88:	stp	x8, xzr, [x4]
  8c:	cmp	x2, #0x1
  90:	b.eq	1bc <__udivmodti4+0x1bc>  // b.none
  94:	rbit	x8, x2
  98:	clz	x8, x8
  9c:	neg	x9, x8
  a0:	lsl	x9, x1, x9
  a4:	lsr	x1, x1, x8
  a8:	lsr	x8, x0, x8
  ac:	orr	x0, x9, x8
  b0:	ret
  b4:	udiv	x8, x0, x2
  b8:	cbz	x4, c4 <__udivmodti4+0xc4>
  bc:	msub	x9, x8, x2, x0
  c0:	stp	x9, xzr, [x4]
  c4:	mov	x1, xzr
  c8:	mov	x0, x8
  cc:	ret
  d0:	add	w8, w11, #0x1
  d4:	cmp	w8, #0x40
  d8:	b.eq	138 <__udivmodti4+0x138>  // b.none
  dc:	mov	w12, #0x3f                  	// #63
  e0:	sub	w11, w12, w11
  e4:	lsr	x13, x0, x8
  e8:	lsl	x12, x1, x11
  ec:	mov	x10, xzr
  f0:	lsr	x9, x1, x8
  f4:	orr	x1, x12, x13
  f8:	lsl	x0, x0, x11
  fc:	b	144 <__udivmodti4+0x144>
 100:	cbz	x4, 114 <__udivmodti4+0x114>
 104:	mov	x0, xzr
 108:	mov	x1, xzr
 10c:	stp	xzr, xzr, [x4]
 110:	ret
 114:	mov	x0, xzr
 118:	mov	x1, xzr
 11c:	ret
 120:	clz	x8, x2
 124:	clz	x9, x1
 128:	sub	w8, w8, w9
 12c:	add	w8, w8, #0x41
 130:	cmp	w8, #0x40
 134:	b.ne	210 <__udivmodti4+0x210>  // b.any
 138:	mov	x9, xzr
 13c:	mov	x10, xzr
 140:	mov	w8, #0x40                  	// #64
 144:	mov	w12, wzr
 148:	mov	x11, x0
 14c:	extr	x13, x1, x11, #63
 150:	extr	x9, x9, x1, #63
 154:	mov	w12, w12
 158:	mvn	x14, x13
 15c:	extr	x11, x11, x10, #63
 160:	orr	x10, x12, x10, lsl #1
 164:	mvn	x12, x9
 168:	cmn	x14, x2
 16c:	adcs	x12, x12, x3
 170:	asr	x14, x12, #63
 174:	and	x15, x3, x12, asr #63
 178:	and	x12, x2, x12, asr #63
 17c:	subs	x1, x13, x12
 180:	sbcs	x9, x9, x15
 184:	subs	w8, w8, #0x1
 188:	and	w12, w14, #0x1
 18c:	b.ne	14c <__udivmodti4+0x14c>  // b.any
 190:	mov	x8, xzr
 194:	lsr	x13, x10, #63
 198:	lsl	x10, x10, #1
 19c:	lsl	x11, x11, #1
 1a0:	and	x13, x13, #0x1
 1a4:	cbz	x4, 1ac <__udivmodti4+0x1ac>
 1a8:	stp	x1, x9, [x4]
 1ac:	and	x9, x10, #0xfffffffffffffffe
 1b0:	orr	x10, x13, x11
 1b4:	orr	x0, x9, x12
 1b8:	orr	x1, x10, x8
 1bc:	ret
 1c0:	udiv	x0, x1, x3
 1c4:	cbz	x4, 118 <__udivmodti4+0x118>
 1c8:	msub	x8, x0, x3, x1
 1cc:	stp	xzr, x8, [x4]
 1d0:	b	118 <__udivmodti4+0x118>
 1d4:	clz	x8, x3
 1d8:	clz	x9, x1
 1dc:	sub	w9, w8, w9
 1e0:	cmp	w9, #0x3f
 1e4:	b.cs	20 <__udivmodti4+0x20>  // b.hs, b.nlast
 1e8:	mov	w11, #0x3f                  	// #63
 1ec:	add	w8, w9, #0x1
 1f0:	sub	w11, w11, w9
 1f4:	lsr	x12, x0, x8
 1f8:	lsl	x0, x0, x11
 1fc:	lsl	x11, x1, x11
 200:	mov	x10, xzr
 204:	lsr	x9, x1, x8
 208:	orr	x1, x11, x12
 20c:	b	144 <__udivmodti4+0x144>
 210:	b.cs	23c <__udivmodti4+0x23c>  // b.hs, b.nlast
 214:	neg	w10, w8
 218:	lsl	x11, x0, x10
 21c:	lsl	x10, x1, x10
 220:	lsr	x12, x0, x8
 224:	lsr	x9, x1, x8
 228:	orr	x1, x10, x12
 22c:	mov	x10, xzr
 230:	cbz	w8, 25c <__udivmodti4+0x25c>
 234:	mov	x0, x11
 238:	b	144 <__udivmodti4+0x144>
 23c:	neg	w11, w8
 240:	lsr	x12, x0, x8
 244:	lsl	x10, x0, x11
 248:	lsl	x11, x1, x11
 24c:	mov	x9, xzr
 250:	orr	x0, x11, x12
 254:	lsr	x1, x1, x8
 258:	b	144 <__udivmodti4+0x144>
 25c:	mov	x13, xzr
 260:	mov	x12, xzr
 264:	mov	x8, xzr
 268:	b	19c <__udivmodti4+0x19c>

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	mov	w8, w0
   4:	mov	w0, wzr
   8:	cbz	w8, 84 <__udivsi3+0x84>
   c:	cbz	w1, 84 <__udivsi3+0x84>
  10:	clz	w10, w1
  14:	clz	w9, w8
  18:	sub	w11, w10, w9
  1c:	cmp	w11, #0x1f
  20:	b.ls	2c <__udivsi3+0x2c>  // b.plast
  24:	mov	w0, wzr
  28:	ret
  2c:	b.ne	38 <__udivsi3+0x38>  // b.any
  30:	mov	w0, w8
  34:	ret
  38:	mov	w13, #0x1f                  	// #31
  3c:	add	w12, w11, #0x1
  40:	mvn	w14, w10
  44:	sub	w11, w13, w11
  48:	mov	w0, wzr
  4c:	lsr	w10, w8, w12
  50:	lsl	w8, w8, w11
  54:	add	w9, w14, w9
  58:	extr	w10, w10, w8, #31
  5c:	mvn	w11, w10
  60:	add	w11, w11, w1
  64:	asr	w12, w11, #31
  68:	and	w11, w1, w11, asr #31
  6c:	orr	w8, w0, w8, lsl #1
  70:	adds	w9, w9, #0x1
  74:	and	w0, w12, #0x1
  78:	sub	w10, w10, w11
  7c:	b.cc	58 <__udivsi3+0x58>  // b.lo, b.ul, b.last
  80:	bfi	w0, w8, #1, #31
  84:	ret

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, xzr
   4:	b	0 <__udivmodti4>

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	add	x2, sp, #0x8
  10:	bl	0 <__udivmoddi4>
  14:	ldr	x0, [sp, #8]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	mov	x4, sp
  10:	bl	0 <__udivmodti4>
  14:	ldp	x0, x1, [sp]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__emutls_get_address>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	stp	x24, x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	add	x8, x0, #0x10
  18:	ldar	x22, [x8]
  1c:	mov	x19, x0
  20:	cbnz	x22, 70 <__emutls_get_address+0x70>
  24:	adrp	x0, 0 <__emutls_get_address>
  28:	adrp	x1, 0 <__emutls_get_address>
  2c:	add	x0, x0, #0x0
  30:	add	x1, x1, #0x0
  34:	bl	0 <pthread_once>
  38:	adrp	x0, 0 <__emutls_get_address>
  3c:	add	x0, x0, #0x0
  40:	bl	0 <pthread_mutex_lock>
  44:	ldr	x22, [x19, #16]
  48:	cbnz	x22, 64 <__emutls_get_address+0x64>
  4c:	adrp	x8, 0 <__emutls_get_address>
  50:	ldr	x9, [x8]
  54:	add	x10, x19, #0x10
  58:	add	x22, x9, #0x1
  5c:	str	x22, [x8]
  60:	stlr	x22, [x10]
  64:	adrp	x0, 0 <__emutls_get_address>
  68:	add	x0, x0, #0x0
  6c:	bl	0 <pthread_mutex_unlock>
  70:	adrp	x23, 0 <__emutls_get_address>
  74:	ldr	w0, [x23]
  78:	bl	0 <pthread_getspecific>
  7c:	cbz	x0, d0 <__emutls_get_address+0xd0>
  80:	ldr	x21, [x0, #8]
  84:	mov	x20, x0
  88:	cmp	x21, x22
  8c:	b.cs	114 <__emutls_get_address+0x114>  // b.hs, b.nlast
  90:	add	x8, x22, #0x11
  94:	and	x8, x8, #0xfffffffffffffff0
  98:	sub	x24, x8, #0x2
  9c:	lsl	x8, x24, #3
  a0:	add	x1, x8, #0x10
  a4:	mov	x0, x20
  a8:	bl	0 <realloc>
  ac:	cbz	x0, 1a4 <__emutls_get_address+0x1a4>
  b0:	add	x8, x0, x21, lsl #3
  b4:	sub	x9, x24, x21
  b8:	mov	x20, x0
  bc:	add	x0, x8, #0x10
  c0:	lsl	x2, x9, #3
  c4:	mov	w1, wzr
  c8:	bl	0 <memset>
  cc:	b	104 <__emutls_get_address+0x104>
  d0:	add	x8, x22, #0x11
  d4:	and	x8, x8, #0xfffffffffffffff0
  d8:	sub	x24, x8, #0x2
  dc:	lsl	x21, x24, #3
  e0:	add	x0, x21, #0x10
  e4:	bl	0 <malloc>
  e8:	cbz	x0, 1a4 <__emutls_get_address+0x1a4>
  ec:	mov	x20, x0
  f0:	add	x0, x0, #0x10
  f4:	mov	w1, wzr
  f8:	mov	x2, x21
  fc:	bl	0 <memset>
 100:	str	xzr, [x20]
 104:	ldr	w0, [x23]
 108:	mov	x1, x20
 10c:	str	x24, [x20, #8]
 110:	bl	0 <pthread_setspecific>
 114:	sub	x8, x22, #0x1
 118:	add	x22, x20, x8, lsl #3
 11c:	ldr	x20, [x22, #16]!
 120:	cbnz	x20, 18c <__emutls_get_address+0x18c>
 124:	ldr	x8, [x19, #8]
 128:	mov	w9, #0x8                   	// #8
 12c:	cmp	x8, #0x8
 130:	csel	x20, x8, x9, hi  // hi = pmore
 134:	sub	x8, x20, #0x1
 138:	tst	x20, x8
 13c:	b.ne	1a4 <__emutls_get_address+0x1a4>  // b.any
 140:	ldr	x21, [x19]
 144:	add	x23, x20, #0x7
 148:	add	x0, x23, x21
 14c:	bl	0 <malloc>
 150:	cbz	x0, 1a4 <__emutls_get_address+0x1a4>
 154:	add	x8, x0, x23
 158:	neg	x9, x20
 15c:	and	x20, x8, x9
 160:	stur	x0, [x20, #-8]
 164:	ldr	x1, [x19, #24]
 168:	cbz	x1, 17c <__emutls_get_address+0x17c>
 16c:	mov	x0, x20
 170:	mov	x2, x21
 174:	bl	0 <memcpy>
 178:	b	188 <__emutls_get_address+0x188>
 17c:	mov	x0, x20
 180:	mov	x2, x21
 184:	bl	0 <memset>
 188:	str	x20, [x22]
 18c:	mov	x0, x20
 190:	ldp	x20, x19, [sp, #48]
 194:	ldp	x22, x21, [sp, #32]
 198:	ldp	x24, x23, [sp, #16]
 19c:	ldp	x29, x30, [sp], #64
 1a0:	ret
 1a4:	bl	0 <abort>

00000000000001a8 <emutls_init>:
 1a8:	stp	x29, x30, [sp, #-16]!
 1ac:	mov	x29, sp
 1b0:	adrp	x0, 0 <__emutls_get_address>
 1b4:	adrp	x1, 0 <__emutls_get_address>
 1b8:	add	x0, x0, #0x0
 1bc:	add	x1, x1, #0x0
 1c0:	bl	0 <pthread_key_create>
 1c4:	cbnz	w0, 1d0 <emutls_init+0x28>
 1c8:	ldp	x29, x30, [sp], #16
 1cc:	ret
 1d0:	bl	0 <abort>

00000000000001d4 <emutls_key_destructor>:
 1d4:	stp	x29, x30, [sp, #-48]!
 1d8:	str	x21, [sp, #16]
 1dc:	stp	x20, x19, [sp, #32]
 1e0:	mov	x29, sp
 1e4:	ldr	x8, [x0]
 1e8:	mov	x19, x0
 1ec:	cbz	x8, 214 <emutls_key_destructor+0x40>
 1f0:	sub	x8, x8, #0x1
 1f4:	str	x8, [x19]
 1f8:	adrp	x8, 0 <__emutls_get_address>
 1fc:	ldr	w0, [x8]
 200:	mov	x1, x19
 204:	ldp	x20, x19, [sp, #32]
 208:	ldr	x21, [sp, #16]
 20c:	ldp	x29, x30, [sp], #48
 210:	b	0 <pthread_setspecific>
 214:	ldr	x8, [x19, #8]
 218:	cbz	x8, 244 <emutls_key_destructor+0x70>
 21c:	mov	x20, xzr
 220:	add	x21, x19, #0x10
 224:	ldr	x9, [x21, x20, lsl #3]
 228:	cbz	x9, 238 <emutls_key_destructor+0x64>
 22c:	ldur	x0, [x9, #-8]
 230:	bl	0 <free>
 234:	ldr	x8, [x19, #8]
 238:	add	x20, x20, #0x1
 23c:	cmp	x20, x8
 240:	b.cc	224 <emutls_key_destructor+0x50>  // b.lo, b.ul, b.last
 244:	mov	x0, x19
 248:	ldp	x20, x19, [sp, #32]
 24c:	ldr	x21, [sp, #16]
 250:	ldp	x29, x30, [sp], #48
 254:	b	0 <free>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	neg	x8, x0
  1c:	add	x9, x19, x0
  20:	and	x0, x8, x19
  24:	ldr	x19, [sp, #16]
  28:	add	x9, x9, #0x30
  2c:	and	x8, x9, x8
  30:	sub	x1, x8, x0
  34:	mov	w2, #0x7                   	// #7
  38:	ldp	x29, x30, [sp], #32
  3c:	b	0 <mprotect>

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	adrp	x19, 0 <stderr>
  10:	ldr	x19, [x19]
  14:	mov	x4, x3
  18:	mov	x3, x2
  1c:	mov	x2, x1
  20:	ldr	x8, [x19]
  24:	mov	x1, x0
  28:	mov	x0, x8
  2c:	bl	0 <fprintf>
  30:	ldr	x0, [x19]
  34:	bl	0 <fflush>
  38:	adrp	x0, 0 <__eprintf>
  3c:	adrp	x2, 0 <__eprintf>
  40:	add	x0, x0, #0x0
  44:	add	x2, x2, #0x0
  48:	mov	w1, #0x1a                  	// #26
  4c:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_personality_v0>:
   0:	tbnz	w1, #0, 168 <__gcc_personality_v0+0x168>
   4:	sub	sp, sp, #0x70
   8:	stp	x29, x30, [sp, #16]
   c:	str	x27, [sp, #32]
  10:	stp	x26, x25, [sp, #48]
  14:	stp	x24, x23, [sp, #64]
  18:	stp	x22, x21, [sp, #80]
  1c:	stp	x20, x19, [sp, #96]
  20:	add	x29, sp, #0x10
  24:	mov	x0, x4
  28:	mov	x19, x4
  2c:	mov	x20, x3
  30:	bl	0 <_Unwind_GetLanguageSpecificData>
  34:	str	x0, [x29, #24]
  38:	cbz	x0, 144 <__gcc_personality_v0+0x144>
  3c:	mov	x21, x0
  40:	mov	x0, x19
  44:	bl	0 <_Unwind_GetIP>
  48:	mov	x23, x0
  4c:	mov	x0, x19
  50:	bl	0 <_Unwind_GetRegionStart>
  54:	add	x8, x21, #0x1
  58:	str	x8, [x29, #24]
  5c:	ldrb	w1, [x21]
  60:	mov	x21, x0
  64:	cmp	w1, #0xff
  68:	b.eq	78 <__gcc_personality_v0+0x78>  // b.none
  6c:	add	x0, x29, #0x18
  70:	bl	1a4 <readEncodedPointer>
  74:	ldr	x8, [x29, #24]
  78:	add	x11, x8, #0x1
  7c:	str	x11, [x29, #24]
  80:	ldrb	w8, [x8]
  84:	mvn	x12, x21
  88:	cmp	w8, #0xff
  8c:	b.eq	9c <__gcc_personality_v0+0x9c>  // b.none
  90:	ldrsb	w8, [x11], #1
  94:	tbnz	w8, #31, 90 <__gcc_personality_v0+0x90>
  98:	str	x11, [x29, #24]
  9c:	add	x9, x11, #0x1
  a0:	str	x9, [x29, #24]
  a4:	ldrb	w22, [x11]
  a8:	mov	w10, wzr
  ac:	mov	x8, xzr
  b0:	add	x26, x23, x12
  b4:	ldrb	w11, [x9], #1
  b8:	and	w12, w11, #0x7f
  bc:	lsl	w12, w12, w10
  c0:	sxtw	x12, w12
  c4:	orr	x8, x8, x12
  c8:	add	w10, w10, #0x7
  cc:	tbnz	w11, #7, b4 <__gcc_personality_v0+0xb4>
  d0:	tst	x8, #0xffffffff
  d4:	str	x9, [x29, #24]
  d8:	str	x9, [sp, #8]
  dc:	b.eq	144 <__gcc_personality_v0+0x144>  // b.none
  e0:	add	x27, x9, w8, uxtw
  e4:	add	x0, sp, #0x8
  e8:	mov	w1, w22
  ec:	bl	1a4 <readEncodedPointer>
  f0:	mov	x24, x0
  f4:	add	x0, sp, #0x8
  f8:	mov	w1, w22
  fc:	bl	1a4 <readEncodedPointer>
 100:	mov	x25, x0
 104:	add	x0, sp, #0x8
 108:	mov	w1, w22
 10c:	bl	1a4 <readEncodedPointer>
 110:	ldr	x8, [sp, #8]
 114:	mov	x23, x0
 118:	ldrsb	w9, [x8], #1
 11c:	tbnz	w9, #31, 118 <__gcc_personality_v0+0x118>
 120:	str	x8, [sp, #8]
 124:	cbz	x23, 13c <__gcc_personality_v0+0x13c>
 128:	cmp	x24, x26
 12c:	b.hi	13c <__gcc_personality_v0+0x13c>  // b.pmore
 130:	add	x9, x25, x24
 134:	cmp	x26, x9
 138:	b.cc	170 <__gcc_personality_v0+0x170>  // b.lo, b.ul, b.last
 13c:	cmp	x8, x27
 140:	b.cc	e4 <__gcc_personality_v0+0xe4>  // b.lo, b.ul, b.last
 144:	mov	w0, #0x8                   	// #8
 148:	ldp	x20, x19, [sp, #96]
 14c:	ldp	x22, x21, [sp, #80]
 150:	ldp	x24, x23, [sp, #64]
 154:	ldp	x26, x25, [sp, #48]
 158:	ldr	x27, [sp, #32]
 15c:	ldp	x29, x30, [sp, #16]
 160:	add	sp, sp, #0x70
 164:	ret
 168:	mov	w0, #0x8                   	// #8
 16c:	ret
 170:	mov	x0, x19
 174:	mov	w1, wzr
 178:	mov	x2, x20
 17c:	bl	0 <_Unwind_SetGR>
 180:	mov	w1, #0x1                   	// #1
 184:	mov	x0, x19
 188:	mov	x2, xzr
 18c:	bl	0 <_Unwind_SetGR>
 190:	add	x1, x23, x21
 194:	mov	x0, x19
 198:	bl	0 <_Unwind_SetIP>
 19c:	mov	w0, #0x7                   	// #7
 1a0:	b	148 <__gcc_personality_v0+0x148>

00000000000001a4 <readEncodedPointer>:
 1a4:	stp	x29, x30, [sp, #-16]!
 1a8:	mov	x29, sp
 1ac:	and	w9, w1, #0xff
 1b0:	cmp	w9, #0xff
 1b4:	b.eq	1ec <readEncodedPointer+0x48>  // b.none
 1b8:	and	w8, w9, #0xf
 1bc:	cmp	w8, #0xc
 1c0:	b.hi	278 <readEncodedPointer+0xd4>  // b.pmore
 1c4:	ldr	x10, [x0]
 1c8:	adrp	x11, 0 <__gcc_personality_v0>
 1cc:	add	x11, x11, #0x0
 1d0:	adr	x12, 1e0 <readEncodedPointer+0x3c>
 1d4:	ldrb	w13, [x11, x8]
 1d8:	add	x12, x12, x13, lsl #2
 1dc:	br	x12
 1e0:	mov	x11, x10
 1e4:	ldr	x8, [x11], #8
 1e8:	b	24c <readEncodedPointer+0xa8>
 1ec:	mov	x8, xzr
 1f0:	b	26c <readEncodedPointer+0xc8>
 1f4:	mov	w12, wzr
 1f8:	mov	x8, xzr
 1fc:	mov	x11, x10
 200:	ldrb	w13, [x11], #1
 204:	and	w14, w13, #0x7f
 208:	lsl	w14, w14, w12
 20c:	sxtw	x14, w14
 210:	orr	x8, x8, x14
 214:	add	w12, w12, #0x7
 218:	tbnz	w13, #7, 200 <readEncodedPointer+0x5c>
 21c:	b	24c <readEncodedPointer+0xa8>
 220:	mov	x11, x10
 224:	ldrsw	x8, [x11], #4
 228:	b	24c <readEncodedPointer+0xa8>
 22c:	mov	x11, x10
 230:	ldr	w8, [x11], #4
 234:	b	24c <readEncodedPointer+0xa8>
 238:	mov	x11, x10
 23c:	ldrsh	x8, [x11], #2
 240:	b	24c <readEncodedPointer+0xa8>
 244:	mov	x11, x10
 248:	ldrh	w8, [x11], #2
 24c:	ubfx	w12, w9, #4, #3
 250:	cbz	w12, 260 <readEncodedPointer+0xbc>
 254:	cmp	w12, #0x1
 258:	b.ne	290 <readEncodedPointer+0xec>  // b.any
 25c:	add	x8, x8, x10
 260:	tbz	w9, #7, 268 <readEncodedPointer+0xc4>
 264:	ldr	x8, [x8]
 268:	str	x11, [x0]
 26c:	mov	x0, x8
 270:	ldp	x29, x30, [sp], #16
 274:	ret
 278:	adrp	x0, 0 <__gcc_personality_v0>
 27c:	adrp	x2, 0 <__gcc_personality_v0>
 280:	add	x0, x0, #0x0
 284:	add	x2, x2, #0x0
 288:	mov	w1, #0x68                  	// #104
 28c:	bl	0 <__compilerrt_abort_impl>
 290:	adrp	x0, 0 <__gcc_personality_v0>
 294:	adrp	x2, 0 <__gcc_personality_v0>
 298:	add	x0, x0, #0x0
 29c:	add	x2, x2, #0x0
 2a0:	mov	w1, #0x7a                  	// #122
 2a4:	bl	0 <__compilerrt_abort_impl>

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	adrp	x8, 0 <__clear_cache>
   4:	ldr	x9, [x8]
   8:	cbz	x9, 14 <__clear_cache+0x14>
   c:	tbz	w9, #28, 20 <__clear_cache+0x20>
  10:	b	48 <__clear_cache+0x48>
  14:	mrs	x9, ctr_el0
  18:	str	x9, [x8]
  1c:	tbnz	w9, #28, 48 <__clear_cache+0x48>
  20:	ubfx	w9, w9, #16, #4
  24:	mov	w10, #0x4                   	// #4
  28:	lsl	w9, w10, w9
  2c:	neg	x10, x9
  30:	and	x10, x10, x0
  34:	cmp	x10, x1
  38:	b.cs	48 <__clear_cache+0x48>  // b.hs, b.nlast
  3c:	dc	cvau, x10
  40:	add	x10, x10, x9
  44:	b	34 <__clear_cache+0x34>
  48:	dsb	ish
  4c:	ldr	x8, [x8]
  50:	tbnz	w8, #29, 7c <__clear_cache+0x7c>
  54:	and	w8, w8, #0xf
  58:	mov	w9, #0x4                   	// #4
  5c:	lsl	w8, w9, w8
  60:	neg	x9, x8
  64:	and	x9, x9, x0
  68:	cmp	x9, x1
  6c:	b.cs	7c <__clear_cache+0x7c>  // b.hs, b.nlast
  70:	ic	ivau, x9
  74:	add	x9, x9, x8
  78:	b	68 <__clear_cache+0x68>
  7c:	isb
  80:	ret

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x8, fpcr
   4:	ubfx	x8, x8, #22, #2
   8:	sub	x8, x8, #0x1
   c:	cmp	x8, #0x2
  10:	b.hi	24 <__fe_getround+0x24>  // b.pmore
  14:	adrp	x9, 0 <__fe_getround>
  18:	add	x9, x9, #0x0
  1c:	ldr	w0, [x9, x8, lsl #2]
  20:	ret
  24:	mov	w0, wzr
  28:	ret

000000000000002c <__fe_raise_inexact>:
  2c:	mrs	x8, fpsr
  30:	mov	w0, wzr
  34:	orr	x8, x8, #0x10
  38:	msr	fpsr, x8
  3c:	ret
