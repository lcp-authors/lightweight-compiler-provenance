In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgcc.a_gcc_-Os:

_muldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x4, x0, #0xffffffff
   4:	lsr	x5, x0, #32
   8:	and	x6, x2, #0xffffffff
   c:	lsr	x8, x2, #32
  10:	mul	x7, x4, x6
  14:	mul	x6, x5, x6
  18:	madd	x4, x4, x8, x6
  1c:	mul	x5, x5, x8
  20:	add	x4, x4, x7, lsr #32
  24:	cmp	x6, x4
  28:	b.ls	34 <__multi3+0x34>  // b.plast
  2c:	mov	x6, #0x100000000           	// #4294967296
  30:	add	x5, x5, x6
  34:	add	x5, x5, x4, lsr #32
  38:	and	x7, x7, #0xffffffff
  3c:	madd	x5, x0, x3, x5
  40:	add	x0, x7, x4, lsl #32
  44:	madd	x1, x2, x1, x5
  48:	ret

_negdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	cmp	x0, #0x0
   4:	neg	x1, x1
   8:	cset	x2, ne  // ne = any
   c:	neg	x0, x0
  10:	sub	x1, x1, x2
  14:	ret

_lshrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	cbz	x2, 24 <__lshrti3+0x24>
   4:	mov	x3, #0x40                  	// #64
   8:	sub	x3, x3, x2
   c:	cmp	x3, #0x0
  10:	b.gt	28 <__lshrti3+0x28>
  14:	neg	w0, w3
  18:	mov	x4, #0x0                   	// #0
  1c:	lsr	x0, x1, x0
  20:	mov	x1, x4
  24:	ret
  28:	lsr	x4, x1, x2
  2c:	lsr	x0, x0, x2
  30:	lsl	x1, x1, x3
  34:	orr	x0, x0, x1
  38:	b	20 <__lshrti3+0x20>

_ashldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	cbz	x2, 24 <__ashlti3+0x24>
   4:	mov	x3, #0x40                  	// #64
   8:	sub	x3, x3, x2
   c:	cmp	x3, #0x0
  10:	b.gt	28 <__ashlti3+0x28>
  14:	neg	w1, w3
  18:	mov	x4, #0x0                   	// #0
  1c:	lsl	x1, x0, x1
  20:	mov	x0, x4
  24:	ret
  28:	lsl	x4, x0, x2
  2c:	lsl	x1, x1, x2
  30:	lsr	x0, x0, x3
  34:	orr	x1, x1, x0
  38:	b	20 <__ashlti3+0x20>

_ashrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	cbz	x2, 24 <__ashrti3+0x24>
   4:	mov	x3, #0x40                  	// #64
   8:	sub	x3, x3, x2
   c:	cmp	x3, #0x0
  10:	b.gt	28 <__ashrti3+0x28>
  14:	neg	w0, w3
  18:	asr	x4, x1, #63
  1c:	asr	x0, x1, x0
  20:	mov	x1, x4
  24:	ret
  28:	asr	x4, x1, x2
  2c:	lsr	x0, x0, x2
  30:	lsl	x1, x1, x3
  34:	orr	x0, x0, x1
  38:	b	20 <__ashrti3+0x20>

_cmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	cmp	x1, x3
   4:	b.lt	20 <__cmpti2+0x20>  // b.tstop
   8:	b.gt	28 <__cmpti2+0x28>
   c:	cmp	x0, x2
  10:	cset	w1, hi  // hi = pmore
  14:	add	w1, w1, #0x1
  18:	csel	w0, w1, wzr, cs  // cs = hs, nlast
  1c:	ret
  20:	mov	w0, #0x0                   	// #0
  24:	b	1c <__cmpti2+0x1c>
  28:	mov	w0, #0x2                   	// #2
  2c:	b	1c <__cmpti2+0x1c>

_ucmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	cmp	x1, x3
   4:	b.cc	20 <__ucmpti2+0x20>  // b.lo, b.ul, b.last
   8:	b.hi	28 <__ucmpti2+0x28>  // b.pmore
   c:	cmp	x0, x2
  10:	cset	w1, hi  // hi = pmore
  14:	add	w1, w1, #0x1
  18:	csel	w0, w1, wzr, cs  // cs = hs, nlast
  1c:	ret
  20:	mov	w0, #0x0                   	// #0
  24:	b	1c <__ucmpti2+0x1c>
  28:	mov	w0, #0x2                   	// #2
  2c:	b	1c <__ucmpti2+0x1c>

_clear_cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	b	0 <__aarch64_sync_cache_range>

_trampoline.o:     file format elf64-littleaarch64


__main.o:     file format elf64-littleaarch64


_absvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	tbz	x0, #63, 18 <__absvdi2+0x18>
   4:	negs	x0, x0
   8:	b.pl	18 <__absvdi2+0x18>  // b.nfrst
   c:	stp	x29, x30, [sp, #-16]!
  10:	mov	x29, sp
  14:	bl	0 <abort>
  18:	ret

000000000000001c <__absvsi2>:
  1c:	tbz	w0, #31, 34 <__absvsi2+0x18>
  20:	negs	w0, w0
  24:	b.pl	34 <__absvsi2+0x18>  // b.nfrst
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	ret

_absvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	tbz	x1, #63, 1c <__absvti2+0x1c>
   4:	negs	x0, x0
   8:	ngc	x1, x1
   c:	tbz	x1, #63, 1c <__absvti2+0x1c>
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <abort>
  1c:	ret

_addvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	mov	x2, x0
   4:	add	x0, x0, x1
   8:	tbnz	x1, #63, 24 <__addvdi3+0x24>
   c:	cmp	x2, x0
  10:	cset	w1, gt
  14:	cbz	w1, 30 <__addvdi3+0x30>
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	bl	0 <abort>
  24:	cmp	x2, x0
  28:	cset	w1, lt  // lt = tstop
  2c:	b	14 <__addvdi3+0x14>
  30:	ret

0000000000000034 <__addvsi3>:
  34:	mov	w2, w0
  38:	add	w0, w0, w1
  3c:	tbnz	w1, #31, 58 <__addvsi3+0x24>
  40:	cmp	w2, w0
  44:	cset	w1, gt
  48:	cbz	w1, 64 <__addvsi3+0x30>
  4c:	stp	x29, x30, [sp, #-16]!
  50:	mov	x29, sp
  54:	bl	0 <abort>
  58:	cmp	w2, w0
  5c:	cset	w1, lt  // lt = tstop
  60:	b	48 <__addvsi3+0x14>
  64:	ret

_addvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	mov	x4, x0
   4:	adds	x0, x0, x2
   8:	mov	x5, x1
   c:	adc	x1, x1, x3
  10:	tbnz	x3, #63, 40 <__addvti3+0x40>
  14:	cmp	x5, x1
  18:	mov	w3, #0x1                   	// #1
  1c:	b.gt	30 <__addvti3+0x30>
  20:	b.ne	2c <__addvti3+0x2c>  // b.any
  24:	cmp	x4, x0
  28:	b.hi	30 <__addvti3+0x30>  // b.pmore
  2c:	mov	w3, #0x0                   	// #0
  30:	cbz	w3, 58 <__addvti3+0x58>
  34:	stp	x29, x30, [sp, #-16]!
  38:	mov	x29, sp
  3c:	bl	0 <abort>
  40:	cmp	x1, x5
  44:	mov	w3, #0x1                   	// #1
  48:	b.gt	30 <__addvti3+0x30>
  4c:	b.ne	2c <__addvti3+0x2c>  // b.any
  50:	cmp	x0, x4
  54:	b	28 <__addvti3+0x28>
  58:	ret

_subvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	mov	x2, x0
   4:	sub	x0, x0, x1
   8:	tbnz	x1, #63, 24 <__subvdi3+0x24>
   c:	cmp	x2, x0
  10:	cset	w1, lt  // lt = tstop
  14:	cbz	w1, 30 <__subvdi3+0x30>
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	bl	0 <abort>
  24:	cmp	x2, x0
  28:	cset	w1, gt
  2c:	b	14 <__subvdi3+0x14>
  30:	ret

0000000000000034 <__subvsi3>:
  34:	mov	w2, w0
  38:	sub	w0, w0, w1
  3c:	tbnz	w1, #31, 58 <__subvsi3+0x24>
  40:	cmp	w2, w0
  44:	cset	w1, lt  // lt = tstop
  48:	cbz	w1, 64 <__subvsi3+0x30>
  4c:	stp	x29, x30, [sp, #-16]!
  50:	mov	x29, sp
  54:	bl	0 <abort>
  58:	cmp	w2, w0
  5c:	cset	w1, gt
  60:	b	48 <__subvsi3+0x14>
  64:	ret

_subvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	mov	x4, x0
   4:	subs	x0, x0, x2
   8:	mov	x5, x1
   c:	sbc	x1, x1, x3
  10:	tbnz	x3, #63, 40 <__subvti3+0x40>
  14:	cmp	x1, x5
  18:	mov	w3, #0x1                   	// #1
  1c:	b.gt	30 <__subvti3+0x30>
  20:	b.ne	2c <__subvti3+0x2c>  // b.any
  24:	cmp	x0, x4
  28:	b.hi	30 <__subvti3+0x30>  // b.pmore
  2c:	mov	w3, #0x0                   	// #0
  30:	cbz	w3, 58 <__subvti3+0x58>
  34:	stp	x29, x30, [sp, #-16]!
  38:	mov	x29, sp
  3c:	bl	0 <abort>
  40:	cmp	x5, x1
  44:	mov	w3, #0x1                   	// #1
  48:	b.gt	30 <__subvti3+0x30>
  4c:	b.ne	2c <__subvti3+0x2c>  // b.any
  50:	cmp	x4, x0
  54:	b	28 <__subvti3+0x28>
  58:	ret

_mulvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	mov	x2, x0
   4:	mul	x0, x0, x1
   8:	smulh	x2, x2, x1
   c:	cmp	x2, x0, asr #63
  10:	b.eq	20 <__mulvdi3+0x20>  // b.none
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	0 <abort>
  20:	ret

0000000000000024 <__mulvsi3>:
  24:	smull	x0, w0, w1
  28:	asr	x1, x0, #32
  2c:	cmp	w1, w0, asr #31
  30:	b.eq	40 <__mulvsi3+0x1c>  // b.none
  34:	stp	x29, x30, [sp, #-16]!
  38:	mov	x29, sp
  3c:	bl	0 <abort>
  40:	ret

_mulvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	mov	x4, x1
   4:	mov	x5, x0
   8:	cmp	x4, x0, asr #63
   c:	asr	x1, x2, #63
  10:	mul	x0, x0, x2
  14:	b.ne	60 <__mulvti3+0x60>  // b.any
  18:	cmp	x1, x3
  1c:	b.ne	28 <__mulvti3+0x28>  // b.any
  20:	smulh	x1, x5, x2
  24:	ret
  28:	mul	x1, x3, x5
  2c:	umulh	x7, x2, x5
  30:	umulh	x6, x3, x5
  34:	mov	x4, x1
  38:	tbz	x3, #63, 40 <__mulvti3+0x40>
  3c:	sub	x6, x6, x5
  40:	tbz	x5, #63, 4c <__mulvti3+0x4c>
  44:	subs	x4, x1, x2
  48:	sbc	x6, x6, x3
  4c:	adds	x1, x4, x7
  50:	cinc	x6, x6, cs  // cs = hs, nlast
  54:	cmp	x6, x1, asr #63
  58:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  5c:	ret
  60:	cmp	x1, x3
  64:	umulh	x1, x2, x5
  68:	b.ne	a0 <__mulvti3+0xa0>  // b.any
  6c:	mul	x7, x4, x2
  70:	umulh	x3, x4, x2
  74:	mov	x6, x7
  78:	tbz	x4, #63, 80 <__mulvti3+0x80>
  7c:	sub	x3, x3, x2
  80:	tbz	x2, #63, 8c <__mulvti3+0x8c>
  84:	subs	x6, x7, x5
  88:	sbc	x3, x3, x4
  8c:	adds	x1, x6, x1
  90:	cinc	x3, x3, cs  // cs = hs, nlast
  94:	cmp	x3, x1, asr #63
  98:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  9c:	ret
  a0:	tbnz	x4, #63, d8 <__mulvti3+0xd8>
  a4:	tbnz	x3, #63, c0 <__mulvti3+0xc0>
  a8:	orr	x3, x3, x4
  ac:	cbnz	x3, b4 <__mulvti3+0xb4>
  b0:	tbz	x1, #63, 114 <__mulvti3+0x114>
  b4:	stp	x29, x30, [sp, #-16]!
  b8:	mov	x29, sp
  bc:	bl	0 <abort>
  c0:	cbnz	x4, b4 <__mulvti3+0xb4>
  c4:	cmn	x3, #0x1
  c8:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  cc:	subs	x1, x1, x5
  d0:	b.pl	b4 <__mulvti3+0xb4>  // b.nfrst
  d4:	ret
  d8:	tbnz	x3, #63, f4 <__mulvti3+0xf4>
  dc:	cmn	x4, #0x1
  e0:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  e4:	cbnz	x3, b4 <__mulvti3+0xb4>
  e8:	subs	x1, x1, x2
  ec:	b.pl	b4 <__mulvti3+0xb4>  // b.nfrst
  f0:	ret
  f4:	and	x3, x3, x4
  f8:	cmn	x3, #0x1
  fc:	b.ne	b4 <__mulvti3+0xb4>  // b.any
 100:	orr	x3, x2, x5
 104:	cbz	x3, b4 <__mulvti3+0xb4>
 108:	sub	x1, x1, x5
 10c:	subs	x1, x1, x2
 110:	b.mi	b4 <__mulvti3+0xb4>  // b.first
 114:	ret

_negvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, x0
   4:	neg	x0, x0
   8:	tbnz	x1, #63, 24 <__negvdi2+0x24>
   c:	cmp	x0, #0x0
  10:	cset	w1, gt
  14:	cbz	w1, 30 <__negvdi2+0x30>
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	bl	0 <abort>
  24:	lsr	x1, x0, #63
  28:	and	w1, w1, #0xff
  2c:	b	14 <__negvdi2+0x14>
  30:	ret

0000000000000034 <__negvsi2>:
  34:	mov	w1, w0
  38:	neg	w0, w0
  3c:	tbnz	w1, #31, 58 <__negvsi2+0x24>
  40:	cmp	w0, #0x0
  44:	cset	w1, gt
  48:	cbz	w1, 60 <__negvsi2+0x2c>
  4c:	stp	x29, x30, [sp, #-16]!
  50:	mov	x29, sp
  54:	bl	0 <abort>
  58:	lsr	w1, w0, #31
  5c:	b	48 <__negvsi2+0x14>
  60:	ret

_negvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	negs	x0, x0
   4:	mov	x2, x1
   8:	ngc	x1, x1
   c:	tbnz	x2, #63, 38 <__negvti2+0x38>
  10:	cmp	x1, #0x0
  14:	mov	w2, #0x1                   	// #1
  18:	b.gt	28 <__negvti2+0x28>
  1c:	b.ne	24 <__negvti2+0x24>  // b.any
  20:	cbnz	x0, 28 <__negvti2+0x28>
  24:	mov	w2, #0x0                   	// #0
  28:	cbz	w2, 44 <__negvti2+0x44>
  2c:	stp	x29, x30, [sp, #-16]!
  30:	mov	x29, sp
  34:	bl	0 <abort>
  38:	lsr	x2, x1, #63
  3c:	and	w2, w2, #0xff
  40:	b	28 <__negvti2+0x28>
  44:	ret

_ctors.o:     file format elf64-littleaarch64


_ffssi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	rbit	x1, x0
   4:	cmp	x0, #0x0
   8:	clz	x1, x1
   c:	csinc	w0, wzr, w1, eq  // eq = none
  10:	ret

_ffsdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 1c <__ffsti2+0x1c>
   4:	mov	x1, #0x0                   	// #0
   8:	rbit	x0, x0
   c:	clz	x0, x0
  10:	add	w0, w1, w0
  14:	add	w0, w0, #0x1
  18:	ret
  1c:	cbz	x1, 2c <__ffsti2+0x2c>
  20:	mov	x0, x1
  24:	mov	x1, #0x40                  	// #64
  28:	b	8 <__ffsti2+0x8>
  2c:	mov	w0, #0x0                   	// #0
  30:	b	18 <__ffsti2+0x18>

_clz.o:     file format elf64-littleaarch64


_clzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	clz	x0, x0
   4:	ret

_clzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cbz	x1, 14 <__clzti2+0x14>
   4:	mov	x0, #0x0                   	// #0
   8:	clz	x1, x1
   c:	add	w0, w0, w1
  10:	ret
  14:	mov	x1, x0
  18:	mov	x0, #0x40                  	// #64
  1c:	b	8 <__clzti2+0x8>

_ctzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	rbit	x0, x0
   4:	clz	x0, x0
   8:	ret

_ctzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cbz	x0, 1c <__ctzti2+0x1c>
   4:	mov	x1, x0
   8:	mov	x0, #0x0                   	// #0
   c:	rbit	x1, x1
  10:	clz	x1, x1
  14:	add	w0, w0, w1
  18:	ret
  1c:	mov	x0, #0x40                  	// #64
  20:	b	c <__ctzti2+0xc>

_popcount_tab.o:     file format elf64-littleaarch64


_popcountsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	and	x1, x1, #0x5555555555555555
   8:	sub	x0, x0, x1
   c:	and	x1, x0, #0x3333333333333333
  10:	lsr	x0, x0, #2
  14:	and	x0, x0, #0x3333333333333333
  18:	add	x0, x1, x0
  1c:	mov	x1, #0x101010101010101     	// #72340172838076673
  20:	add	x0, x0, x0, lsr #4
  24:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  28:	mul	x0, x0, x1
  2c:	lsr	x0, x0, #56
  30:	ret

_popcountdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x2, x0, #1
   4:	and	x2, x2, #0x5555555555555555
   8:	sub	x0, x0, x2
   c:	lsr	x2, x1, #1
  10:	and	x2, x2, #0x5555555555555555
  14:	sub	x2, x1, x2
  18:	and	x1, x0, #0x3333333333333333
  1c:	lsr	x0, x0, #2
  20:	and	x0, x0, #0x3333333333333333
  24:	add	x0, x1, x0
  28:	and	x1, x2, #0x3333333333333333
  2c:	lsr	x2, x2, #2
  30:	and	x2, x2, #0x3333333333333333
  34:	add	x0, x0, x0, lsr #4
  38:	add	x1, x1, x2
  3c:	and	x2, x0, #0xf0f0f0f0f0f0f0f
  40:	add	x0, x1, x1, lsr #4
  44:	mov	x1, #0x101010101010101     	// #72340172838076673
  48:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  4c:	add	x0, x0, x2
  50:	mul	x0, x0, x1
  54:	lsr	x0, x0, #56
  58:	ret

_paritysi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	eor	x0, x0, x0, lsr #32
   4:	mov	w1, #0x6996                	// #27030
   8:	eor	x0, x0, x0, lsr #16
   c:	eor	x0, x0, x0, lsr #8
  10:	eor	x0, x0, x0, lsr #4
  14:	and	x0, x0, #0xf
  18:	asr	w0, w1, w0
  1c:	and	w0, w0, #0x1
  20:	ret

_paritydi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x0, x0, x1
   4:	mov	w1, #0x6996                	// #27030
   8:	eor	x0, x0, x0, lsr #32
   c:	eor	x0, x0, x0, lsr #16
  10:	eor	x0, x0, x0, lsr #8
  14:	eor	x0, x0, x0, lsr #4
  18:	and	x0, x0, #0xf
  1c:	asr	w0, w1, w0
  20:	and	w0, w0, #0x1
  24:	ret

_powisf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	cmp	w0, #0x0
   4:	fmov	s1, #1.000000000000000000e+00
   8:	cneg	w1, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	s2, s0
  14:	fcsel	s0, s0, s1, ne  // ne = any
  18:	lsr	w1, w1, #1
  1c:	cbnz	w1, 30 <__powisf2+0x30>
  20:	tbz	w0, #31, 2c <__powisf2+0x2c>
  24:	fmov	s1, #1.000000000000000000e+00
  28:	fdiv	s0, s1, s0
  2c:	ret
  30:	fmul	s2, s2, s2
  34:	tbz	w1, #0, 18 <__powisf2+0x18>
  38:	fmul	s0, s0, s2
  3c:	b	18 <__powisf2+0x18>

_powidf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	cmp	w0, #0x0
   4:	fmov	d1, #1.000000000000000000e+00
   8:	cneg	w1, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	d2, d0
  14:	fcsel	d0, d0, d1, ne  // ne = any
  18:	lsr	w1, w1, #1
  1c:	cbnz	w1, 30 <__powidf2+0x30>
  20:	tbz	w0, #31, 2c <__powidf2+0x2c>
  24:	fmov	d1, #1.000000000000000000e+00
  28:	fdiv	d0, d1, d0
  2c:	ret
  30:	fmul	d2, d2, d2
  34:	tbz	w1, #0, 18 <__powidf2+0x18>
  38:	fmul	d0, d0, d2
  3c:	b	18 <__powidf2+0x18>

_powixf2.o:     file format elf64-littleaarch64


_powitf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powitf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	cmp	w0, #0x0
   8:	mov	v1.16b, v0.16b
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	w19, w0
  18:	cneg	w20, w0, lt  // lt = tstop
  1c:	tbnz	w19, #0, 60 <__powitf2+0x60>
  20:	adrp	x0, 0 <__powitf2>
  24:	add	x0, x0, #0x0
  28:	ldr	q2, [x0]
  2c:	lsr	w20, w20, #1
  30:	cbnz	w20, 68 <__powitf2+0x68>
  34:	tbz	w19, #31, 50 <__powitf2+0x50>
  38:	adrp	x0, 0 <__powitf2>
  3c:	add	x0, x0, #0x0
  40:	mov	v1.16b, v2.16b
  44:	ldr	q0, [x0]
  48:	bl	0 <__divtf3>
  4c:	mov	v2.16b, v0.16b
  50:	mov	v0.16b, v2.16b
  54:	ldp	x19, x20, [sp, #16]
  58:	ldp	x29, x30, [sp], #48
  5c:	ret
  60:	mov	v2.16b, v0.16b
  64:	b	2c <__powitf2+0x2c>
  68:	mov	v0.16b, v1.16b
  6c:	str	q2, [sp, #32]
  70:	bl	0 <__multf3>
  74:	mov	v1.16b, v0.16b
  78:	ldr	q2, [sp, #32]
  7c:	tbz	w20, #0, 2c <__powitf2+0x2c>
  80:	str	q0, [sp, #32]
  84:	mov	v0.16b, v2.16b
  88:	bl	0 <__multf3>
  8c:	mov	v2.16b, v0.16b
  90:	ldr	q1, [sp, #32]
  94:	b	2c <__powitf2+0x2c>

_mulhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulhc3>:
   0:	fcvt	s19, h0
   4:	fcvt	s5, h1
   8:	fcvt	s22, h2
   c:	fcvt	s21, h3
  10:	mov	v4.h[0], v0.h[0]
  14:	mov	v6.h[0], v1.h[0]
  18:	fmul	s18, s19, s22
  1c:	fmul	s17, s5, s21
  20:	fmul	s16, s19, s21
  24:	fmul	s7, s22, s5
  28:	fcvt	h18, s18
  2c:	fcvt	h17, s17
  30:	fcvt	h16, s16
  34:	fcvt	h7, s7
  38:	fcvt	s18, h18
  3c:	fcvt	s17, h17
  40:	fcvt	s16, h16
  44:	fcvt	s7, h7
  48:	fsub	s0, s18, s17
  4c:	fadd	s1, s16, s7
  50:	fcvt	h0, s0
  54:	fcvt	h1, s1
  58:	fcvt	s20, h0
  5c:	fcmp	s20, s20
  60:	fcvt	s20, h1
  64:	cset	w0, vs
  68:	fcmp	s20, s20
  6c:	cset	w1, vs
  70:	ands	w0, w0, w1
  74:	b.eq	1f8 <__mulhc3+0x1f8>  // b.none
  78:	fabs	s20, s19
  7c:	adrp	x1, 0 <__mulhc3>
  80:	fabs	s19, s5
  84:	ldr	s5, [x1]
  88:	fcvt	h20, s20
  8c:	fcvt	h19, s19
  90:	fcvt	s20, h20
  94:	fcvt	s19, h19
  98:	fcmp	s20, s5
  9c:	b.gt	a8 <__mulhc3+0xa8>
  a0:	fcmp	s19, s5
  a4:	b.le	1fc <__mulhc3+0x1fc>
  a8:	fcmp	s20, s5
  ac:	umov	w2, v4.h[0]
  b0:	cset	w1, gt
  b4:	fcmp	s19, s5
  b8:	scvtf	d20, w1
  bc:	fcvt	h4, d20
  c0:	umov	w1, v4.h[0]
  c4:	bfxil	w2, w1, #0, #15
  c8:	cset	w1, gt
  cc:	fcmp	s22, s22
  d0:	scvtf	d19, w1
  d4:	dup	v4.4h, w2
  d8:	umov	w2, v6.h[0]
  dc:	fcvt	h6, d19
  e0:	umov	w1, v6.h[0]
  e4:	bfxil	w2, w1, #0, #15
  e8:	dup	v6.4h, w2
  ec:	b.vc	100 <__mulhc3+0x100>
  f0:	umov	w1, v2.h[0]
  f4:	movi	v2.4h, #0x0
  f8:	tbz	w1, #15, 100 <__mulhc3+0x100>
  fc:	movi	v2.4h, #0x80, lsl #8
 100:	fcmp	s21, s21
 104:	b.vc	118 <__mulhc3+0x118>
 108:	umov	w1, v3.h[0]
 10c:	movi	v3.4h, #0x0
 110:	tbz	w1, #15, 118 <__mulhc3+0x118>
 114:	movi	v3.4h, #0x80, lsl #8
 118:	fcvt	s22, h2
 11c:	fcvt	s21, h3
 120:	fabs	s20, s22
 124:	fabs	s19, s21
 128:	fcvt	h20, s20
 12c:	fcvt	h19, s19
 130:	fcvt	s20, h20
 134:	fcvt	s19, h19
 138:	fcmp	s20, s5
 13c:	b.gt	148 <__mulhc3+0x148>
 140:	fcmp	s19, s5
 144:	b.le	204 <__mulhc3+0x204>
 148:	fcmp	s20, s5
 14c:	umov	w1, v2.h[0]
 150:	cset	w0, gt
 154:	fcmp	s19, s5
 158:	scvtf	d0, w0
 15c:	fcvt	h0, d0
 160:	umov	w0, v0.h[0]
 164:	bfxil	w1, w0, #0, #15
 168:	cset	w0, gt
 16c:	scvtf	d0, w0
 170:	dup	v2.4h, w1
 174:	umov	w1, v3.h[0]
 178:	fcvt	h0, d0
 17c:	umov	w0, v0.h[0]
 180:	fcvt	s0, h4
 184:	fcmp	s0, s0
 188:	bfxil	w1, w0, #0, #15
 18c:	dup	v3.4h, w1
 190:	b.vc	1a4 <__mulhc3+0x1a4>
 194:	umov	w0, v4.h[0]
 198:	movi	v4.4h, #0x0
 19c:	tbz	w0, #15, 1a4 <__mulhc3+0x1a4>
 1a0:	movi	v4.4h, #0x80, lsl #8
 1a4:	fcvt	s0, h6
 1a8:	fcmp	s0, s0
 1ac:	b.vc	1c0 <__mulhc3+0x1c0>
 1b0:	umov	w0, v6.h[0]
 1b4:	movi	v6.4h, #0x0
 1b8:	tbz	w0, #15, 1c0 <__mulhc3+0x1c0>
 1bc:	movi	v6.4h, #0x80, lsl #8
 1c0:	fcvt	s1, h6
 1c4:	fcvt	s3, h3
 1c8:	fcvt	s4, h4
 1cc:	fcvt	s2, h2
 1d0:	adrp	x0, 0 <__mulhc3>
 1d4:	fmul	s0, s1, s3
 1d8:	ldr	s5, [x0]
 1dc:	fnmsub	s0, s4, s2, s0
 1e0:	fmul	s2, s2, s1
 1e4:	fmadd	s1, s4, s3, s2
 1e8:	fmul	s0, s0, s5
 1ec:	fmul	s1, s1, s5
 1f0:	fcvt	h0, s0
 1f4:	fcvt	h1, s1
 1f8:	ret
 1fc:	mov	w0, #0x0                   	// #0
 200:	b	118 <__mulhc3+0x118>
 204:	cbnz	w0, 1c0 <__mulhc3+0x1c0>
 208:	fabs	s18, s18
 20c:	fcvt	h18, s18
 210:	fcvt	s18, h18
 214:	fcmp	s18, s5
 218:	b.gt	258 <__mulhc3+0x258>
 21c:	fabs	s17, s17
 220:	fcvt	h17, s17
 224:	fcvt	s17, h17
 228:	fcmp	s17, s5
 22c:	b.gt	258 <__mulhc3+0x258>
 230:	fabs	s16, s16
 234:	fcvt	h16, s16
 238:	fcvt	s16, h16
 23c:	fcmp	s16, s5
 240:	b.gt	258 <__mulhc3+0x258>
 244:	fabs	s7, s7
 248:	fcvt	h7, s7
 24c:	fcvt	s7, h7
 250:	fcmp	s7, s5
 254:	b.le	1f8 <__mulhc3+0x1f8>
 258:	fcvt	s0, h4
 25c:	fcmp	s0, s0
 260:	b.vc	274 <__mulhc3+0x274>
 264:	umov	w0, v4.h[0]
 268:	movi	v4.4h, #0x0
 26c:	tbz	w0, #15, 274 <__mulhc3+0x274>
 270:	movi	v4.4h, #0x80, lsl #8
 274:	fcvt	s0, h6
 278:	fcmp	s0, s0
 27c:	b.vc	290 <__mulhc3+0x290>
 280:	umov	w0, v6.h[0]
 284:	movi	v6.4h, #0x0
 288:	tbz	w0, #15, 290 <__mulhc3+0x290>
 28c:	movi	v6.4h, #0x80, lsl #8
 290:	fcmp	s22, s22
 294:	b.vc	2a8 <__mulhc3+0x2a8>
 298:	umov	w0, v2.h[0]
 29c:	movi	v2.4h, #0x0
 2a0:	tbz	w0, #15, 2a8 <__mulhc3+0x2a8>
 2a4:	movi	v2.4h, #0x80, lsl #8
 2a8:	fcmp	s21, s21
 2ac:	b.vc	1c0 <__mulhc3+0x1c0>
 2b0:	umov	w0, v3.h[0]
 2b4:	movi	v3.4h, #0x0
 2b8:	tbz	w0, #15, 1c0 <__mulhc3+0x1c0>
 2bc:	movi	v3.4h, #0x80, lsl #8
 2c0:	b	1c0 <__mulhc3+0x1c0>

_mulsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s17, s1, s3
   4:	fmul	s18, s0, s2
   8:	fmul	s16, s0, s3
   c:	fmov	s5, s0
  10:	fmul	s7, s2, s1
  14:	fmov	s4, s1
  18:	fsub	s0, s18, s17
  1c:	fadd	s1, s16, s7
  20:	fcmp	s0, s0
  24:	cset	w0, vs
  28:	fcmp	s1, s1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	120 <__mulsc3+0x120>  // b.none
  38:	adrp	x1, 0 <__mulsc3>
  3c:	fabs	s19, s5
  40:	fabs	s21, s4
  44:	ldr	s6, [x1]
  48:	fcmp	s19, s6
  4c:	b.gt	58 <__mulsc3+0x58>
  50:	fcmp	s21, s6
  54:	b.le	124 <__mulsc3+0x124>
  58:	fcmp	s19, s6
  5c:	movi	v19.2s, #0x80, lsl #24
  60:	cset	w1, gt
  64:	fcmp	s21, s6
  68:	scvtf	s20, w1
  6c:	cset	w1, gt
  70:	fcmp	s2, s2
  74:	bif	v5.8b, v20.8b, v19.8b
  78:	scvtf	s20, w1
  7c:	bif	v4.8b, v20.8b, v19.8b
  80:	b.vc	8c <__mulsc3+0x8c>
  84:	movi	v20.2s, #0x0
  88:	bif	v2.8b, v20.8b, v19.8b
  8c:	fcmp	s3, s3
  90:	b.vc	a0 <__mulsc3+0xa0>
  94:	movi	v19.2s, #0x0
  98:	movi	v20.2s, #0x80, lsl #24
  9c:	bif	v3.8b, v19.8b, v20.8b
  a0:	fabs	s20, s2
  a4:	fabs	s19, s3
  a8:	fcmp	s20, s6
  ac:	b.gt	b8 <__mulsc3+0xb8>
  b0:	fcmp	s19, s6
  b4:	b.le	12c <__mulsc3+0x12c>
  b8:	fcmp	s20, s6
  bc:	movi	v0.2s, #0x80, lsl #24
  c0:	cset	w0, gt
  c4:	fcmp	s19, s6
  c8:	scvtf	s1, w0
  cc:	cset	w0, gt
  d0:	fcmp	s5, s5
  d4:	bif	v2.8b, v1.8b, v0.8b
  d8:	scvtf	s1, w0
  dc:	bif	v3.8b, v1.8b, v0.8b
  e0:	b.vc	ec <__mulsc3+0xec>
  e4:	movi	v1.2s, #0x0
  e8:	bif	v5.8b, v1.8b, v0.8b
  ec:	fcmp	s4, s4
  f0:	b.vc	100 <__mulsc3+0x100>
  f4:	movi	v0.2s, #0x0
  f8:	movi	v1.2s, #0x80, lsl #24
  fc:	bif	v4.8b, v0.8b, v1.8b
 100:	fmul	s0, s4, s3
 104:	fmul	s4, s4, s2
 108:	fnmsub	s0, s5, s2, s0
 10c:	fmadd	s1, s5, s3, s4
 110:	adrp	x0, 0 <__mulsc3>
 114:	ldr	s6, [x0]
 118:	fmul	s0, s0, s6
 11c:	fmul	s1, s1, s6
 120:	ret
 124:	mov	w0, #0x0                   	// #0
 128:	b	a0 <__mulsc3+0xa0>
 12c:	cbnz	w0, 100 <__mulsc3+0x100>
 130:	fabs	s18, s18
 134:	fcmp	s18, s6
 138:	b.gt	160 <__mulsc3+0x160>
 13c:	fabs	s17, s17
 140:	fcmp	s17, s6
 144:	b.gt	160 <__mulsc3+0x160>
 148:	fabs	s16, s16
 14c:	fcmp	s16, s6
 150:	b.gt	160 <__mulsc3+0x160>
 154:	fabs	s7, s7
 158:	fcmp	s7, s6
 15c:	b.le	120 <__mulsc3+0x120>
 160:	fcmp	s5, s5
 164:	b.vc	174 <__mulsc3+0x174>
 168:	movi	v0.2s, #0x0
 16c:	movi	v1.2s, #0x80, lsl #24
 170:	bif	v5.8b, v0.8b, v1.8b
 174:	fcmp	s4, s4
 178:	b.vc	188 <__mulsc3+0x188>
 17c:	movi	v0.2s, #0x0
 180:	movi	v1.2s, #0x80, lsl #24
 184:	bif	v4.8b, v0.8b, v1.8b
 188:	fcmp	s2, s2
 18c:	b.vc	19c <__mulsc3+0x19c>
 190:	movi	v0.2s, #0x0
 194:	movi	v1.2s, #0x80, lsl #24
 198:	bif	v2.8b, v0.8b, v1.8b
 19c:	fcmp	s3, s3
 1a0:	b.vc	100 <__mulsc3+0x100>
 1a4:	movi	v0.2s, #0x0
 1a8:	movi	v1.2s, #0x80, lsl #24
 1ac:	bif	v3.8b, v0.8b, v1.8b
 1b0:	b	100 <__mulsc3+0x100>

_muldc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d17, d1, d3
   4:	fmul	d18, d0, d2
   8:	fmul	d16, d0, d3
   c:	fmov	d5, d0
  10:	fmul	d7, d2, d1
  14:	fmov	d4, d1
  18:	fsub	d0, d18, d17
  1c:	fadd	d1, d16, d7
  20:	fcmp	d0, d0
  24:	cset	w0, vs
  28:	fcmp	d1, d1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	130 <__muldc3+0x130>  // b.none
  38:	adrp	x1, 0 <__muldc3>
  3c:	fabs	d19, d5
  40:	fabs	d21, d4
  44:	ldr	d6, [x1]
  48:	fcmp	d19, d6
  4c:	b.gt	58 <__muldc3+0x58>
  50:	fcmp	d21, d6
  54:	b.le	134 <__muldc3+0x134>
  58:	fcmp	d19, d6
  5c:	cset	w1, gt
  60:	fcmp	d21, d6
  64:	scvtf	d20, w1
  68:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  6c:	fmov	d19, x1
  70:	cset	w1, gt
  74:	fcmp	d2, d2
  78:	bif	v5.8b, v20.8b, v19.8b
  7c:	scvtf	d20, w1
  80:	bif	v4.8b, v20.8b, v19.8b
  84:	b.vc	90 <__muldc3+0x90>
  88:	movi	d20, #0x0
  8c:	bif	v2.8b, v20.8b, v19.8b
  90:	fcmp	d3, d3
  94:	b.vc	a8 <__muldc3+0xa8>
  98:	movi	d19, #0x0
  9c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  a0:	fmov	d20, x1
  a4:	bif	v3.8b, v19.8b, v20.8b
  a8:	fabs	d20, d2
  ac:	fabs	d19, d3
  b0:	fcmp	d20, d6
  b4:	b.gt	c0 <__muldc3+0xc0>
  b8:	fcmp	d19, d6
  bc:	b.le	13c <__muldc3+0x13c>
  c0:	fcmp	d20, d6
  c4:	cset	w0, gt
  c8:	fcmp	d19, d6
  cc:	scvtf	d1, w0
  d0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  d4:	fmov	d0, x0
  d8:	cset	w0, gt
  dc:	fcmp	d5, d5
  e0:	bif	v2.8b, v1.8b, v0.8b
  e4:	scvtf	d1, w0
  e8:	bif	v3.8b, v1.8b, v0.8b
  ec:	b.vc	f8 <__muldc3+0xf8>
  f0:	movi	d1, #0x0
  f4:	bif	v5.8b, v1.8b, v0.8b
  f8:	fcmp	d4, d4
  fc:	b.vc	110 <__muldc3+0x110>
 100:	movi	d0, #0x0
 104:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 108:	fmov	d1, x0
 10c:	bif	v4.8b, v0.8b, v1.8b
 110:	fmul	d0, d4, d3
 114:	fmul	d4, d4, d2
 118:	fnmsub	d0, d5, d2, d0
 11c:	fmadd	d1, d5, d3, d4
 120:	adrp	x0, 0 <__muldc3>
 124:	ldr	d6, [x0]
 128:	fmul	d0, d0, d6
 12c:	fmul	d1, d1, d6
 130:	ret
 134:	mov	w0, #0x0                   	// #0
 138:	b	a8 <__muldc3+0xa8>
 13c:	cbnz	w0, 110 <__muldc3+0x110>
 140:	fabs	d18, d18
 144:	fcmp	d18, d6
 148:	b.gt	170 <__muldc3+0x170>
 14c:	fabs	d17, d17
 150:	fcmp	d17, d6
 154:	b.gt	170 <__muldc3+0x170>
 158:	fabs	d16, d16
 15c:	fcmp	d16, d6
 160:	b.gt	170 <__muldc3+0x170>
 164:	fabs	d7, d7
 168:	fcmp	d7, d6
 16c:	b.le	130 <__muldc3+0x130>
 170:	fcmp	d5, d5
 174:	b.vc	188 <__muldc3+0x188>
 178:	movi	d0, #0x0
 17c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 180:	fmov	d1, x0
 184:	bif	v5.8b, v0.8b, v1.8b
 188:	fcmp	d4, d4
 18c:	b.vc	1a0 <__muldc3+0x1a0>
 190:	movi	d0, #0x0
 194:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 198:	fmov	d1, x0
 19c:	bif	v4.8b, v0.8b, v1.8b
 1a0:	fcmp	d2, d2
 1a4:	b.vc	1b8 <__muldc3+0x1b8>
 1a8:	movi	d0, #0x0
 1ac:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1b0:	fmov	d1, x0
 1b4:	bif	v2.8b, v0.8b, v1.8b
 1b8:	fcmp	d3, d3
 1bc:	b.vc	110 <__muldc3+0x110>
 1c0:	movi	d0, #0x0
 1c4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1c8:	fmov	d1, x0
 1cc:	bif	v3.8b, v0.8b, v1.8b
 1d0:	b	110 <__muldc3+0x110>

_mulxc3.o:     file format elf64-littleaarch64


_multc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-224]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #96]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x25, x26, [sp, #64]
  14:	ldp	x21, x25, [sp, #96]
  18:	str	q1, [sp, #96]
  1c:	stp	x19, x20, [sp, #16]
  20:	stp	x23, x24, [sp, #48]
  24:	ldp	x19, x23, [sp, #96]
  28:	str	q2, [sp, #96]
  2c:	ldp	x20, x24, [sp, #96]
  30:	str	q3, [sp, #96]
  34:	ldp	x22, x26, [sp, #96]
  38:	stp	x20, x24, [sp, #96]
  3c:	ldr	q1, [sp, #96]
  40:	stp	x21, x25, [sp, #96]
  44:	stp	x27, x28, [sp, #80]
  48:	ldr	q0, [sp, #96]
  4c:	bl	0 <__multf3>
  50:	str	q0, [sp, #96]
  54:	ldp	x1, x0, [sp, #96]
  58:	stp	x22, x26, [sp, #96]
  5c:	ldr	q1, [sp, #96]
  60:	stp	x19, x23, [sp, #96]
  64:	ldr	q0, [sp, #96]
  68:	str	x1, [sp, #128]
  6c:	str	x0, [sp, #184]
  70:	bl	0 <__multf3>
  74:	str	q0, [sp, #96]
  78:	ldp	x1, x0, [sp, #96]
  7c:	stp	x22, x26, [sp, #96]
  80:	ldr	q1, [sp, #96]
  84:	stp	x21, x25, [sp, #96]
  88:	ldr	q0, [sp, #96]
  8c:	str	x1, [sp, #144]
  90:	str	x0, [sp, #192]
  94:	bl	0 <__multf3>
  98:	str	q0, [sp, #96]
  9c:	ldp	x1, x0, [sp, #96]
  a0:	stp	x19, x23, [sp, #96]
  a4:	ldr	q1, [sp, #96]
  a8:	stp	x20, x24, [sp, #96]
  ac:	ldr	q0, [sp, #96]
  b0:	str	x1, [sp, #152]
  b4:	str	x0, [sp, #200]
  b8:	bl	0 <__multf3>
  bc:	str	q0, [sp, #96]
  c0:	ldp	x1, x0, [sp, #96]
  c4:	str	x0, [sp, #208]
  c8:	ldr	x0, [sp, #144]
  cc:	str	x0, [sp, #96]
  d0:	ldr	x0, [sp, #192]
  d4:	str	x0, [sp, #104]
  d8:	ldr	x0, [sp, #128]
  dc:	str	x1, [sp, #176]
  e0:	ldr	q1, [sp, #96]
  e4:	str	x0, [sp, #96]
  e8:	ldr	x0, [sp, #184]
  ec:	str	x0, [sp, #104]
  f0:	ldr	q0, [sp, #96]
  f4:	bl	0 <__subtf3>
  f8:	str	q0, [sp, #96]
  fc:	ldr	x0, [sp, #176]
 100:	str	x0, [sp, #112]
 104:	ldr	x0, [sp, #208]
 108:	str	x0, [sp, #120]
 10c:	ldr	x0, [sp, #152]
 110:	ldr	q1, [sp, #112]
 114:	str	x0, [sp, #112]
 118:	ldr	x0, [sp, #200]
 11c:	str	x0, [sp, #120]
 120:	ldr	q0, [sp, #112]
 124:	bl	0 <__addtf3>
 128:	str	q0, [sp, #112]
 12c:	ldr	q1, [sp, #96]
 130:	mov	v0.16b, v1.16b
 134:	bl	0 <__unordtf2>
 138:	cmp	w0, #0x0
 13c:	ldr	q1, [sp, #112]
 140:	cset	w27, ne  // ne = any
 144:	mov	v0.16b, v1.16b
 148:	bl	0 <__unordtf2>
 14c:	cmp	w0, #0x0
 150:	cset	w0, ne  // ne = any
 154:	ands	w0, w0, w27
 158:	str	w0, [sp, #216]
 15c:	b.eq	510 <__multc3+0x510>  // b.none
 160:	adrp	x0, 0 <__multc3>
 164:	add	x0, x0, #0x0
 168:	and	x28, x25, #0x7fffffffffffffff
 16c:	stp	x21, x28, [sp, #160]
 170:	and	x27, x23, #0x7fffffffffffffff
 174:	ldr	q1, [x0]
 178:	ldr	q0, [sp, #160]
 17c:	bl	0 <__unordtf2>
 180:	cbnz	w0, 1a4 <__multc3+0x1a4>
 184:	adrp	x0, 0 <__multc3>
 188:	add	x0, x0, #0x0
 18c:	stp	x21, x28, [sp, #160]
 190:	ldr	q1, [x0]
 194:	ldr	q0, [sp, #160]
 198:	bl	0 <__letf2>
 19c:	cmp	w0, #0x0
 1a0:	b.gt	1e0 <__multc3+0x1e0>
 1a4:	adrp	x0, 0 <__multc3>
 1a8:	add	x0, x0, #0x0
 1ac:	stp	x19, x27, [sp, #160]
 1b0:	ldr	q1, [x0]
 1b4:	ldr	q0, [sp, #160]
 1b8:	bl	0 <__unordtf2>
 1bc:	cbnz	w0, 534 <__multc3+0x534>
 1c0:	adrp	x0, 0 <__multc3>
 1c4:	add	x0, x0, #0x0
 1c8:	stp	x19, x27, [sp, #160]
 1cc:	ldr	q1, [x0]
 1d0:	ldr	q0, [sp, #160]
 1d4:	bl	0 <__letf2>
 1d8:	cmp	w0, #0x0
 1dc:	b.le	534 <__multc3+0x534>
 1e0:	adrp	x0, 0 <__multc3>
 1e4:	add	x0, x0, #0x0
 1e8:	stp	x21, x28, [sp, #160]
 1ec:	mov	w1, #0x1                   	// #1
 1f0:	ldr	q1, [x0]
 1f4:	ldr	q0, [sp, #160]
 1f8:	str	w1, [sp, #220]
 1fc:	bl	0 <__unordtf2>
 200:	ldr	w1, [sp, #220]
 204:	cbnz	w0, 228 <__multc3+0x228>
 208:	adrp	x0, 0 <__multc3>
 20c:	add	x0, x0, #0x0
 210:	stp	x21, x28, [sp, #160]
 214:	ldr	q1, [x0]
 218:	ldr	q0, [sp, #160]
 21c:	bl	0 <__letf2>
 220:	cmp	w0, #0x0
 224:	cset	w1, le
 228:	eor	w0, w1, #0x1
 22c:	mov	w28, #0x1                   	// #1
 230:	and	w0, w0, #0x1
 234:	bl	0 <__floatsitf>
 238:	str	q0, [sp, #160]
 23c:	ldp	x21, x0, [sp, #160]
 240:	stp	x19, x27, [sp, #160]
 244:	ldr	q0, [sp, #160]
 248:	bfxil	x25, x0, #0, #63
 24c:	adrp	x0, 0 <__multc3>
 250:	add	x0, x0, #0x0
 254:	ldr	q1, [x0]
 258:	bl	0 <__unordtf2>
 25c:	cbnz	w0, 280 <__multc3+0x280>
 260:	adrp	x0, 0 <__multc3>
 264:	add	x0, x0, #0x0
 268:	stp	x19, x27, [sp, #160]
 26c:	ldr	q1, [x0]
 270:	ldr	q0, [sp, #160]
 274:	bl	0 <__letf2>
 278:	cmp	w0, #0x0
 27c:	cset	w28, le
 280:	eor	w0, w28, #0x1
 284:	and	w0, w0, #0x1
 288:	bl	0 <__floatsitf>
 28c:	str	q0, [sp, #160]
 290:	ldp	x19, x0, [sp, #160]
 294:	stp	x20, x24, [sp, #160]
 298:	ldr	q1, [sp, #160]
 29c:	bfxil	x23, x0, #0, #63
 2a0:	mov	v0.16b, v1.16b
 2a4:	bl	0 <__unordtf2>
 2a8:	cbz	w0, 2c0 <__multc3+0x2c0>
 2ac:	mov	x20, #0x0                   	// #0
 2b0:	mov	x0, #0x0                   	// #0
 2b4:	tbz	x24, #63, 2bc <__multc3+0x2bc>
 2b8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2bc:	mov	x24, x0
 2c0:	stp	x22, x26, [sp, #160]
 2c4:	ldr	q1, [sp, #160]
 2c8:	mov	v0.16b, v1.16b
 2cc:	bl	0 <__unordtf2>
 2d0:	cbz	w0, 2e8 <__multc3+0x2e8>
 2d4:	mov	x22, #0x0                   	// #0
 2d8:	mov	x0, #0x0                   	// #0
 2dc:	tbz	x26, #63, 2e4 <__multc3+0x2e4>
 2e0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2e4:	mov	x26, x0
 2e8:	adrp	x0, 0 <__multc3>
 2ec:	add	x0, x0, #0x0
 2f0:	and	x28, x24, #0x7fffffffffffffff
 2f4:	stp	x20, x28, [sp, #160]
 2f8:	and	x27, x26, #0x7fffffffffffffff
 2fc:	ldr	q1, [x0]
 300:	ldr	q0, [sp, #160]
 304:	bl	0 <__unordtf2>
 308:	cbnz	w0, 32c <__multc3+0x32c>
 30c:	adrp	x0, 0 <__multc3>
 310:	add	x0, x0, #0x0
 314:	stp	x20, x28, [sp, #160]
 318:	ldr	q1, [x0]
 31c:	ldr	q0, [sp, #160]
 320:	bl	0 <__letf2>
 324:	cmp	w0, #0x0
 328:	b.gt	368 <__multc3+0x368>
 32c:	adrp	x0, 0 <__multc3>
 330:	add	x0, x0, #0x0
 334:	stp	x22, x27, [sp, #160]
 338:	ldr	q1, [x0]
 33c:	ldr	q0, [sp, #160]
 340:	bl	0 <__unordtf2>
 344:	cbnz	w0, 53c <__multc3+0x53c>
 348:	adrp	x0, 0 <__multc3>
 34c:	add	x0, x0, #0x0
 350:	stp	x22, x27, [sp, #160]
 354:	ldr	q1, [x0]
 358:	ldr	q0, [sp, #160]
 35c:	bl	0 <__letf2>
 360:	cmp	w0, #0x0
 364:	b.le	53c <__multc3+0x53c>
 368:	adrp	x0, 0 <__multc3>
 36c:	add	x0, x0, #0x0
 370:	stp	x20, x28, [sp, #96]
 374:	mov	w1, #0x1                   	// #1
 378:	ldr	q1, [x0]
 37c:	ldr	q0, [sp, #96]
 380:	str	w1, [sp, #112]
 384:	bl	0 <__unordtf2>
 388:	ldr	w1, [sp, #112]
 38c:	cbnz	w0, 3b0 <__multc3+0x3b0>
 390:	adrp	x0, 0 <__multc3>
 394:	add	x0, x0, #0x0
 398:	stp	x20, x28, [sp, #96]
 39c:	ldr	q1, [x0]
 3a0:	ldr	q0, [sp, #96]
 3a4:	bl	0 <__letf2>
 3a8:	cmp	w0, #0x0
 3ac:	cset	w1, le
 3b0:	eor	w0, w1, #0x1
 3b4:	mov	w28, #0x1                   	// #1
 3b8:	and	w0, w0, #0x1
 3bc:	bl	0 <__floatsitf>
 3c0:	str	q0, [sp, #96]
 3c4:	ldp	x20, x0, [sp, #96]
 3c8:	stp	x22, x27, [sp, #96]
 3cc:	ldr	q0, [sp, #96]
 3d0:	bfxil	x24, x0, #0, #63
 3d4:	adrp	x0, 0 <__multc3>
 3d8:	add	x0, x0, #0x0
 3dc:	ldr	q1, [x0]
 3e0:	bl	0 <__unordtf2>
 3e4:	cbnz	w0, 408 <__multc3+0x408>
 3e8:	adrp	x0, 0 <__multc3>
 3ec:	add	x0, x0, #0x0
 3f0:	stp	x22, x27, [sp, #96]
 3f4:	ldr	q1, [x0]
 3f8:	ldr	q0, [sp, #96]
 3fc:	bl	0 <__letf2>
 400:	cmp	w0, #0x0
 404:	cset	w28, le
 408:	eor	w0, w28, #0x1
 40c:	and	w0, w0, #0x1
 410:	bl	0 <__floatsitf>
 414:	str	q0, [sp, #96]
 418:	ldp	x22, x0, [sp, #96]
 41c:	stp	x21, x25, [sp, #96]
 420:	ldr	q1, [sp, #96]
 424:	bfxil	x26, x0, #0, #63
 428:	mov	v0.16b, v1.16b
 42c:	bl	0 <__unordtf2>
 430:	cbz	w0, 448 <__multc3+0x448>
 434:	mov	x21, #0x0                   	// #0
 438:	mov	x0, #0x0                   	// #0
 43c:	tbz	x25, #63, 444 <__multc3+0x444>
 440:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 444:	mov	x25, x0
 448:	stp	x19, x23, [sp, #96]
 44c:	ldr	q1, [sp, #96]
 450:	mov	v0.16b, v1.16b
 454:	bl	0 <__unordtf2>
 458:	cbz	w0, 470 <__multc3+0x470>
 45c:	mov	x19, #0x0                   	// #0
 460:	mov	x0, #0x0                   	// #0
 464:	tbz	x23, #63, 46c <__multc3+0x46c>
 468:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 46c:	mov	x23, x0
 470:	stp	x20, x24, [sp, #96]
 474:	ldr	q1, [sp, #96]
 478:	stp	x21, x25, [sp, #96]
 47c:	ldr	q0, [sp, #96]
 480:	bl	0 <__multf3>
 484:	stp	x22, x26, [sp, #96]
 488:	ldr	q1, [sp, #96]
 48c:	stp	x19, x23, [sp, #96]
 490:	str	q0, [sp, #112]
 494:	ldr	q0, [sp, #96]
 498:	bl	0 <__multf3>
 49c:	mov	v1.16b, v0.16b
 4a0:	ldr	q2, [sp, #112]
 4a4:	mov	v0.16b, v2.16b
 4a8:	bl	0 <__subtf3>
 4ac:	adrp	x0, 0 <__multc3>
 4b0:	add	x0, x0, #0x0
 4b4:	ldr	q1, [x0]
 4b8:	bl	0 <__multf3>
 4bc:	stp	x22, x26, [sp, #112]
 4c0:	ldr	q1, [sp, #112]
 4c4:	stp	x21, x25, [sp, #112]
 4c8:	str	q0, [sp, #96]
 4cc:	ldr	q0, [sp, #112]
 4d0:	bl	0 <__multf3>
 4d4:	stp	x20, x24, [sp, #112]
 4d8:	ldr	q1, [sp, #112]
 4dc:	stp	x19, x23, [sp, #112]
 4e0:	str	q0, [sp, #128]
 4e4:	ldr	q0, [sp, #112]
 4e8:	bl	0 <__multf3>
 4ec:	mov	v1.16b, v0.16b
 4f0:	ldr	q2, [sp, #128]
 4f4:	mov	v0.16b, v2.16b
 4f8:	bl	0 <__addtf3>
 4fc:	adrp	x0, 0 <__multc3>
 500:	add	x0, x0, #0x0
 504:	ldr	q1, [x0]
 508:	bl	0 <__multf3>
 50c:	str	q0, [sp, #112]
 510:	ldp	x19, x20, [sp, #16]
 514:	ldp	x21, x22, [sp, #32]
 518:	ldp	x23, x24, [sp, #48]
 51c:	ldp	x25, x26, [sp, #64]
 520:	ldp	x27, x28, [sp, #80]
 524:	ldr	q0, [sp, #96]
 528:	ldr	q1, [sp, #112]
 52c:	ldp	x29, x30, [sp], #224
 530:	ret
 534:	str	wzr, [sp, #216]
 538:	b	2e8 <__multc3+0x2e8>
 53c:	ldr	w0, [sp, #216]
 540:	cbnz	w0, 470 <__multc3+0x470>
 544:	ldr	x0, [sp, #184]
 548:	and	x27, x0, #0x7fffffffffffffff
 54c:	adrp	x0, 0 <__multc3>
 550:	add	x0, x0, #0x0
 554:	ldr	q1, [x0]
 558:	ldr	x0, [sp, #128]
 55c:	stp	x0, x27, [sp, #160]
 560:	ldr	q0, [sp, #160]
 564:	bl	0 <__unordtf2>
 568:	cbnz	w0, 590 <__multc3+0x590>
 56c:	adrp	x0, 0 <__multc3>
 570:	add	x0, x0, #0x0
 574:	ldr	q1, [x0]
 578:	ldr	x0, [sp, #128]
 57c:	stp	x0, x27, [sp, #160]
 580:	ldr	q0, [sp, #160]
 584:	bl	0 <__letf2>
 588:	cmp	w0, #0x0
 58c:	b.gt	674 <__multc3+0x674>
 590:	ldr	x0, [sp, #192]
 594:	and	x27, x0, #0x7fffffffffffffff
 598:	adrp	x0, 0 <__multc3>
 59c:	add	x0, x0, #0x0
 5a0:	ldr	q1, [x0]
 5a4:	ldr	x0, [sp, #144]
 5a8:	stp	x0, x27, [sp, #128]
 5ac:	ldr	q0, [sp, #128]
 5b0:	bl	0 <__unordtf2>
 5b4:	cbnz	w0, 5dc <__multc3+0x5dc>
 5b8:	adrp	x0, 0 <__multc3>
 5bc:	add	x0, x0, #0x0
 5c0:	ldr	q1, [x0]
 5c4:	ldr	x0, [sp, #144]
 5c8:	stp	x0, x27, [sp, #128]
 5cc:	ldr	q0, [sp, #128]
 5d0:	bl	0 <__letf2>
 5d4:	cmp	w0, #0x0
 5d8:	b.gt	674 <__multc3+0x674>
 5dc:	ldr	x0, [sp, #200]
 5e0:	and	x27, x0, #0x7fffffffffffffff
 5e4:	adrp	x0, 0 <__multc3>
 5e8:	add	x0, x0, #0x0
 5ec:	ldr	q1, [x0]
 5f0:	ldr	x0, [sp, #152]
 5f4:	stp	x0, x27, [sp, #128]
 5f8:	ldr	q0, [sp, #128]
 5fc:	bl	0 <__unordtf2>
 600:	cbnz	w0, 628 <__multc3+0x628>
 604:	adrp	x0, 0 <__multc3>
 608:	add	x0, x0, #0x0
 60c:	ldr	q1, [x0]
 610:	ldr	x0, [sp, #152]
 614:	stp	x0, x27, [sp, #128]
 618:	ldr	q0, [sp, #128]
 61c:	bl	0 <__letf2>
 620:	cmp	w0, #0x0
 624:	b.gt	674 <__multc3+0x674>
 628:	ldr	x0, [sp, #208]
 62c:	and	x27, x0, #0x7fffffffffffffff
 630:	adrp	x0, 0 <__multc3>
 634:	add	x0, x0, #0x0
 638:	ldr	q1, [x0]
 63c:	ldr	x0, [sp, #176]
 640:	stp	x0, x27, [sp, #128]
 644:	ldr	q0, [sp, #128]
 648:	bl	0 <__unordtf2>
 64c:	cbnz	w0, 510 <__multc3+0x510>
 650:	adrp	x0, 0 <__multc3>
 654:	add	x0, x0, #0x0
 658:	ldr	q1, [x0]
 65c:	ldr	x0, [sp, #176]
 660:	stp	x0, x27, [sp, #128]
 664:	ldr	q0, [sp, #128]
 668:	bl	0 <__letf2>
 66c:	cmp	w0, #0x0
 670:	b.le	510 <__multc3+0x510>
 674:	stp	x21, x25, [sp, #96]
 678:	ldr	q1, [sp, #96]
 67c:	mov	v0.16b, v1.16b
 680:	bl	0 <__unordtf2>
 684:	cbz	w0, 69c <__multc3+0x69c>
 688:	mov	x21, #0x0                   	// #0
 68c:	mov	x0, #0x0                   	// #0
 690:	tbz	x25, #63, 698 <__multc3+0x698>
 694:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 698:	mov	x25, x0
 69c:	stp	x19, x23, [sp, #96]
 6a0:	ldr	q1, [sp, #96]
 6a4:	mov	v0.16b, v1.16b
 6a8:	bl	0 <__unordtf2>
 6ac:	cbz	w0, 6c4 <__multc3+0x6c4>
 6b0:	mov	x19, #0x0                   	// #0
 6b4:	mov	x0, #0x0                   	// #0
 6b8:	tbz	x23, #63, 6c0 <__multc3+0x6c0>
 6bc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6c0:	mov	x23, x0
 6c4:	stp	x20, x24, [sp, #96]
 6c8:	ldr	q1, [sp, #96]
 6cc:	mov	v0.16b, v1.16b
 6d0:	bl	0 <__unordtf2>
 6d4:	cbz	w0, 6ec <__multc3+0x6ec>
 6d8:	mov	x20, #0x0                   	// #0
 6dc:	mov	x0, #0x0                   	// #0
 6e0:	tbz	x24, #63, 6e8 <__multc3+0x6e8>
 6e4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6e8:	mov	x24, x0
 6ec:	stp	x22, x26, [sp, #96]
 6f0:	ldr	q1, [sp, #96]
 6f4:	mov	v0.16b, v1.16b
 6f8:	bl	0 <__unordtf2>
 6fc:	cbz	w0, 470 <__multc3+0x470>
 700:	mov	x22, #0x0                   	// #0
 704:	mov	x0, #0x0                   	// #0
 708:	tbz	x26, #63, 710 <__multc3+0x710>
 70c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 710:	mov	x26, x0
 714:	b	470 <__multc3+0x470>

_divhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divhc3>:
   0:	fcvt	s19, h2
   4:	fcvt	s20, h3
   8:	mov	v5.h[0], v0.h[0]
   c:	mov	v7.h[0], v1.h[0]
  10:	fcvt	s6, h1
  14:	fcvt	s16, h0
  18:	fabs	s18, s19
  1c:	fabs	s17, s20
  20:	fcvt	h18, s18
  24:	fcvt	h17, s17
  28:	fcvt	s18, h18
  2c:	fcvt	s17, h17
  30:	fcmpe	s18, s17
  34:	b.pl	d8 <__divhc3+0xd8>  // b.nfrst
  38:	fdiv	s4, s19, s20
  3c:	fcvt	h4, s4
  40:	fcvt	s4, h4
  44:	fmadd	s1, s4, s19, s20
  48:	fmadd	s0, s4, s16, s6
  4c:	fnmsub	s4, s4, s6, s16
  50:	fcvt	h1, s1
  54:	fcvt	s1, h1
  58:	fdiv	s0, s0, s1
  5c:	fcvt	h0, s0
  60:	fdiv	s4, s4, s1
  64:	fcvt	h1, s4
  68:	fcvt	s4, h0
  6c:	fcmp	s4, s4
  70:	fcvt	s4, h1
  74:	cset	w1, vs
  78:	fcmp	s4, s4
  7c:	cset	w0, vs
  80:	tst	w1, w0
  84:	b.eq	d4 <__divhc3+0xd4>  // b.none
  88:	fcmp	s19, #0.0
  8c:	b.ne	104 <__divhc3+0x104>  // b.any
  90:	fcmp	s20, #0.0
  94:	b.ne	104 <__divhc3+0x104>  // b.any
  98:	fcmp	s16, s16
  9c:	cset	w0, vc
  a0:	fcmp	s6, s6
  a4:	cset	w1, vc
  a8:	orr	w0, w0, w1
  ac:	cbz	w0, 104 <__divhc3+0x104>
  b0:	umov	w0, v2.h[0]
  b4:	movi	v1.4h, #0x7c, lsl #8
  b8:	tbz	w0, #15, c0 <__divhc3+0xc0>
  bc:	movi	v1.4h, #0xfc, lsl #8
  c0:	fcvt	s1, h1
  c4:	fmul	s0, s1, s16
  c8:	fmul	s1, s1, s6
  cc:	fcvt	h0, s0
  d0:	fcvt	h1, s1
  d4:	ret
  d8:	fdiv	s4, s20, s19
  dc:	fcvt	h4, s4
  e0:	fcvt	s4, h4
  e4:	fmadd	s1, s4, s20, s19
  e8:	fmadd	s0, s4, s6, s16
  ec:	fmsub	s4, s4, s16, s6
  f0:	fcvt	h1, s1
  f4:	fcvt	s1, h1
  f8:	fdiv	s0, s0, s1
  fc:	fcvt	h0, s0
 100:	b	60 <__divhc3+0x60>
 104:	fabs	s21, s16
 108:	adrp	x0, 0 <__divhc3>
 10c:	ldr	s4, [x0]
 110:	fcvt	h21, s21
 114:	fcvt	s21, h21
 118:	fcmp	s21, s4
 11c:	b.gt	134 <__divhc3+0x134>
 120:	fabs	s22, s6
 124:	fcvt	h22, s22
 128:	fcvt	s22, h22
 12c:	fcmp	s22, s4
 130:	b.le	1c0 <__divhc3+0x1c0>
 134:	fcmp	s18, s4
 138:	b.hi	1c0 <__divhc3+0x1c0>  // b.pmore
 13c:	fcmp	s17, s4
 140:	b.hi	1c0 <__divhc3+0x1c0>  // b.pmore
 144:	fcmp	s21, s4
 148:	fabs	s1, s6
 14c:	cset	w0, gt
 150:	fcvt	h1, s1
 154:	scvtf	d0, w0
 158:	umov	w0, v5.h[0]
 15c:	fcvt	s1, h1
 160:	fcvt	h0, d0
 164:	fcmp	s1, s4
 168:	umov	w1, v0.h[0]
 16c:	bfxil	w0, w1, #0, #15
 170:	dup	v2.4h, w0
 174:	cset	w0, gt
 178:	scvtf	d0, w0
 17c:	umov	w0, v7.h[0]
 180:	fcvt	s2, h2
 184:	fcvt	h0, d0
 188:	umov	w1, v0.h[0]
 18c:	bfxil	w0, w1, #0, #15
 190:	dup	v1.4h, w0
 194:	adrp	x0, 0 <__divhc3>
 198:	ldr	s3, [x0]
 19c:	fcvt	s1, h1
 1a0:	fmul	s0, s1, s20
 1a4:	fmadd	s0, s2, s19, s0
 1a8:	fmul	s2, s2, s20
 1ac:	fnmsub	s1, s1, s19, s2
 1b0:	fmul	s0, s0, s3
 1b4:	fmul	s1, s1, s3
 1b8:	fcvt	h0, s0
 1bc:	b	d0 <__divhc3+0xd0>
 1c0:	fcmp	s18, s4
 1c4:	b.gt	1d0 <__divhc3+0x1d0>
 1c8:	fcmp	s17, s4
 1cc:	b.le	d4 <__divhc3+0xd4>
 1d0:	fcmp	s21, s4
 1d4:	b.hi	d4 <__divhc3+0xd4>  // b.pmore
 1d8:	fabs	s5, s6
 1dc:	fcvt	h5, s5
 1e0:	fcvt	s5, h5
 1e4:	fcmp	s5, s4
 1e8:	b.hi	d4 <__divhc3+0xd4>  // b.pmore
 1ec:	fcmp	s18, s4
 1f0:	cset	w0, gt
 1f4:	fcmp	s17, s4
 1f8:	scvtf	d0, w0
 1fc:	umov	w0, v2.h[0]
 200:	fcvt	h0, d0
 204:	umov	w1, v0.h[0]
 208:	bfxil	w0, w1, #0, #15
 20c:	dup	v1.4h, w0
 210:	cset	w0, gt
 214:	scvtf	d0, w0
 218:	umov	w0, v3.h[0]
 21c:	movi	d3, #0x0
 220:	fcvt	s1, h1
 224:	fcvt	h0, d0
 228:	umov	w1, v0.h[0]
 22c:	bfxil	w0, w1, #0, #15
 230:	dup	v2.4h, w0
 234:	fcvt	s2, h2
 238:	fmul	s0, s2, s6
 23c:	fmul	s2, s2, s16
 240:	fmadd	s0, s1, s16, s0
 244:	fnmsub	s1, s1, s6, s2
 248:	fcvt	d0, s0
 24c:	fcvt	d1, s1
 250:	fmul	d0, d0, d3
 254:	fmul	d1, d1, d3
 258:	fcvt	h0, d0
 25c:	fcvt	h1, d1
 260:	b	d4 <__divhc3+0xd4>

_divsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	fabs	s19, s2
   4:	fabs	s18, s3
   8:	fmov	s6, s0
   c:	fmov	s7, s1
  10:	fcmpe	s19, s18
  14:	b.pl	74 <__divsc3+0x74>  // b.nfrst
  18:	fdiv	s4, s2, s3
  1c:	fmadd	s5, s0, s4, s7
  20:	fmadd	s1, s2, s4, s3
  24:	fdiv	s0, s5, s1
  28:	fnmsub	s5, s7, s4, s6
  2c:	fdiv	s1, s5, s1
  30:	fcmp	s0, s0
  34:	fccmp	s1, s1, #0x0, vs
  38:	b.vc	70 <__divsc3+0x70>
  3c:	movi	v17.2s, #0x0
  40:	fcmp	s2, s17
  44:	fccmp	s3, s17, #0x0, eq  // eq = none
  48:	b.ne	8c <__divsc3+0x8c>  // b.any
  4c:	fcmp	s6, s6
  50:	fccmp	s7, s7, #0x0, vs
  54:	b.vs	8c <__divsc3+0x8c>
  58:	movi	v1.2s, #0x80, lsl #24
  5c:	mov	w0, #0x7f800000            	// #2139095040
  60:	fmov	s3, w0
  64:	bif	v2.8b, v3.8b, v1.8b
  68:	fmul	s0, s2, s6
  6c:	fmul	s1, s2, s7
  70:	ret
  74:	fdiv	s4, s3, s2
  78:	fmadd	s5, s4, s7, s0
  7c:	fmadd	s1, s3, s4, s2
  80:	fdiv	s0, s5, s1
  84:	fmsub	s5, s4, s6, s7
  88:	b	2c <__divsc3+0x2c>
  8c:	adrp	x0, 0 <__divsc3>
  90:	fabs	s4, s6
  94:	ldr	s16, [x0]
  98:	fcmp	s4, s16
  9c:	b.gt	ac <__divsc3+0xac>
  a0:	fabs	s5, s7
  a4:	fcmp	s5, s16
  a8:	b.le	10c <__divsc3+0x10c>
  ac:	fcmp	s19, s16
  b0:	b.hi	10c <__divsc3+0x10c>  // b.pmore
  b4:	fcmp	s18, s16
  b8:	b.hi	10c <__divsc3+0x10c>  // b.pmore
  bc:	fcmp	s4, s16
  c0:	fabs	s1, s7
  c4:	movi	v0.2s, #0x80, lsl #24
  c8:	cset	w0, gt
  cc:	fcmp	s1, s16
  d0:	scvtf	s5, w0
  d4:	cset	w0, gt
  d8:	scvtf	s1, w0
  dc:	fmov	s4, s5
  e0:	adrp	x0, 0 <__divsc3>
  e4:	ldr	s5, [x0]
  e8:	bit	v1.8b, v7.8b, v0.8b
  ec:	bit	v4.8b, v6.8b, v0.8b
  f0:	fmul	s0, s3, s1
  f4:	fmadd	s0, s2, s4, s0
  f8:	fmul	s4, s3, s4
  fc:	fnmsub	s1, s2, s1, s4
 100:	fmul	s0, s0, s5
 104:	fmul	s1, s1, s5
 108:	b	70 <__divsc3+0x70>
 10c:	fcmp	s19, s16
 110:	b.gt	11c <__divsc3+0x11c>
 114:	fcmp	s18, s16
 118:	b.le	70 <__divsc3+0x70>
 11c:	fcmp	s4, s16
 120:	b.hi	70 <__divsc3+0x70>  // b.pmore
 124:	fabs	s4, s7
 128:	fcmp	s4, s16
 12c:	b.hi	70 <__divsc3+0x70>  // b.pmore
 130:	fcmp	s19, s16
 134:	movi	v0.2s, #0x80, lsl #24
 138:	cset	w0, gt
 13c:	fcmp	s18, s16
 140:	scvtf	s1, w0
 144:	cset	w0, gt
 148:	scvtf	s4, w0
 14c:	bit	v1.8b, v2.8b, v0.8b
 150:	bit	v4.8b, v3.8b, v0.8b
 154:	fmul	s5, s7, s4
 158:	fmul	s4, s6, s4
 15c:	fmadd	s5, s6, s1, s5
 160:	fnmsub	s1, s7, s1, s4
 164:	fmul	s0, s5, s17
 168:	fmul	s1, s1, s17
 16c:	b	70 <__divsc3+0x70>

_divdc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	fabs	d19, d2
   4:	fabs	d18, d3
   8:	fmov	d6, d0
   c:	fmov	d7, d1
  10:	fcmpe	d19, d18
  14:	b.pl	78 <__divdc3+0x78>  // b.nfrst
  18:	fdiv	d4, d2, d3
  1c:	fmadd	d5, d0, d4, d7
  20:	fmadd	d1, d2, d4, d3
  24:	fdiv	d0, d5, d1
  28:	fnmsub	d5, d7, d4, d6
  2c:	fdiv	d1, d5, d1
  30:	fcmp	d0, d0
  34:	fccmp	d1, d1, #0x0, vs
  38:	b.vc	74 <__divdc3+0x74>
  3c:	movi	d17, #0x0
  40:	fcmp	d2, d17
  44:	fccmp	d3, d17, #0x0, eq  // eq = none
  48:	b.ne	90 <__divdc3+0x90>  // b.any
  4c:	fcmp	d6, d6
  50:	fccmp	d7, d7, #0x0, vs
  54:	b.vs	90 <__divdc3+0x90>
  58:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  5c:	fmov	d3, x0
  60:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  64:	fmov	d1, x0
  68:	bif	v2.8b, v3.8b, v1.8b
  6c:	fmul	d0, d2, d6
  70:	fmul	d1, d2, d7
  74:	ret
  78:	fdiv	d4, d3, d2
  7c:	fmadd	d5, d4, d7, d0
  80:	fmadd	d1, d3, d4, d2
  84:	fdiv	d0, d5, d1
  88:	fmsub	d5, d4, d6, d7
  8c:	b	2c <__divdc3+0x2c>
  90:	adrp	x0, 0 <__divdc3>
  94:	fabs	d4, d6
  98:	ldr	d16, [x0]
  9c:	fcmp	d4, d16
  a0:	b.gt	b0 <__divdc3+0xb0>
  a4:	fabs	d5, d7
  a8:	fcmp	d5, d16
  ac:	b.le	114 <__divdc3+0x114>
  b0:	fcmp	d19, d16
  b4:	b.hi	114 <__divdc3+0x114>  // b.pmore
  b8:	fcmp	d18, d16
  bc:	b.hi	114 <__divdc3+0x114>  // b.pmore
  c0:	fcmp	d4, d16
  c4:	fabs	d1, d7
  c8:	cset	w0, gt
  cc:	fcmp	d1, d16
  d0:	scvtf	d5, w0
  d4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  d8:	fmov	d0, x0
  dc:	cset	w0, gt
  e0:	scvtf	d1, w0
  e4:	fmov	d4, d5
  e8:	adrp	x0, 0 <__divdc3>
  ec:	ldr	d5, [x0]
  f0:	bit	v1.8b, v7.8b, v0.8b
  f4:	bit	v4.8b, v6.8b, v0.8b
  f8:	fmul	d0, d3, d1
  fc:	fmadd	d0, d2, d4, d0
 100:	fmul	d4, d3, d4
 104:	fnmsub	d1, d2, d1, d4
 108:	fmul	d0, d0, d5
 10c:	fmul	d1, d1, d5
 110:	b	74 <__divdc3+0x74>
 114:	fcmp	d19, d16
 118:	b.gt	124 <__divdc3+0x124>
 11c:	fcmp	d18, d16
 120:	b.le	74 <__divdc3+0x74>
 124:	fcmp	d4, d16
 128:	b.hi	74 <__divdc3+0x74>  // b.pmore
 12c:	fabs	d4, d7
 130:	fcmp	d4, d16
 134:	b.hi	74 <__divdc3+0x74>  // b.pmore
 138:	fcmp	d19, d16
 13c:	cset	w0, gt
 140:	fcmp	d18, d16
 144:	scvtf	d1, w0
 148:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 14c:	fmov	d0, x0
 150:	cset	w0, gt
 154:	scvtf	d4, w0
 158:	bit	v1.8b, v2.8b, v0.8b
 15c:	bit	v4.8b, v3.8b, v0.8b
 160:	fmul	d5, d7, d4
 164:	fmul	d4, d6, d4
 168:	fmadd	d5, d6, d1, d5
 16c:	fnmsub	d1, d7, d1, d4
 170:	fmul	d0, d5, d17
 174:	fmul	d1, d1, d17
 178:	b	74 <__divdc3+0x74>

_divxc3.o:     file format elf64-littleaarch64


_divtc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-160]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #96]
   c:	stp	x19, x20, [sp, #16]
  10:	stp	x21, x22, [sp, #32]
  14:	ldp	x20, x22, [sp, #96]
  18:	str	q1, [sp, #96]
  1c:	ldp	x21, x19, [sp, #96]
  20:	str	q2, [sp, #96]
  24:	stp	x23, x24, [sp, #48]
  28:	stp	x25, x26, [sp, #64]
  2c:	ldp	x24, x25, [sp, #96]
  30:	str	q3, [sp, #96]
  34:	ldp	x23, x26, [sp, #96]
  38:	stp	x27, x28, [sp, #80]
  3c:	and	x28, x25, #0x7fffffffffffffff
  40:	and	x27, x26, #0x7fffffffffffffff
  44:	str	x27, [sp, #104]
  48:	ldr	q1, [sp, #96]
  4c:	stp	x24, x28, [sp, #96]
  50:	ldr	q0, [sp, #96]
  54:	bl	0 <__lttf2>
  58:	tbz	w0, #31, 21c <__divtc3+0x21c>
  5c:	stp	x23, x26, [sp, #96]
  60:	ldr	q1, [sp, #96]
  64:	stp	x24, x25, [sp, #96]
  68:	ldr	q0, [sp, #96]
  6c:	bl	0 <__divtf3>
  70:	stp	x24, x25, [sp, #96]
  74:	mov	v1.16b, v0.16b
  78:	str	q0, [sp, #112]
  7c:	ldr	q0, [sp, #96]
  80:	bl	0 <__multf3>
  84:	stp	x23, x26, [sp, #96]
  88:	ldr	q1, [sp, #96]
  8c:	bl	0 <__addtf3>
  90:	stp	x20, x22, [sp, #96]
  94:	ldr	q2, [sp, #112]
  98:	str	q0, [sp, #128]
  9c:	ldr	q0, [sp, #96]
  a0:	mov	v1.16b, v2.16b
  a4:	bl	0 <__multf3>
  a8:	stp	x21, x19, [sp, #96]
  ac:	ldr	q1, [sp, #96]
  b0:	bl	0 <__addtf3>
  b4:	ldr	q4, [sp, #128]
  b8:	mov	v1.16b, v4.16b
  bc:	bl	0 <__divtf3>
  c0:	ldr	q2, [sp, #112]
  c4:	stp	x21, x19, [sp, #112]
  c8:	str	q0, [sp, #96]
  cc:	mov	v1.16b, v2.16b
  d0:	ldr	q0, [sp, #112]
  d4:	bl	0 <__multf3>
  d8:	stp	x20, x22, [sp, #112]
  dc:	ldr	q1, [sp, #112]
  e0:	bl	0 <__subtf3>
  e4:	ldr	q4, [sp, #128]
  e8:	mov	v1.16b, v4.16b
  ec:	bl	0 <__divtf3>
  f0:	ldr	q1, [sp, #96]
  f4:	str	q0, [sp, #112]
  f8:	mov	v0.16b, v1.16b
  fc:	bl	0 <__unordtf2>
 100:	cmp	w0, #0x0
 104:	ldr	q1, [sp, #112]
 108:	cset	w1, ne  // ne = any
 10c:	str	w1, [sp, #128]
 110:	mov	v0.16b, v1.16b
 114:	bl	0 <__unordtf2>
 118:	cmp	w0, #0x0
 11c:	ldr	w1, [sp, #128]
 120:	cset	w0, ne  // ne = any
 124:	tst	w0, w1
 128:	b.eq	1f8 <__divtc3+0x1f8>  // b.none
 12c:	movi	v1.2d, #0x0
 130:	stp	x24, x25, [sp, #128]
 134:	ldr	q0, [sp, #128]
 138:	bl	0 <__eqtf2>
 13c:	stp	x23, x26, [sp, #128]
 140:	cmp	w0, #0x0
 144:	movi	v1.2d, #0x0
 148:	cset	w1, eq  // eq = none
 14c:	ldr	q0, [sp, #128]
 150:	str	w1, [sp, #144]
 154:	bl	0 <__eqtf2>
 158:	cmp	w0, #0x0
 15c:	ldr	w1, [sp, #144]
 160:	cset	w0, eq  // eq = none
 164:	tst	w0, w1
 168:	b.eq	2b0 <__divtc3+0x2b0>  // b.none
 16c:	stp	x20, x22, [sp, #128]
 170:	ldr	q1, [sp, #128]
 174:	mov	v0.16b, v1.16b
 178:	bl	0 <__unordtf2>
 17c:	stp	x21, x19, [sp, #128]
 180:	cmp	w0, #0x0
 184:	cset	w1, eq  // eq = none
 188:	ldr	q1, [sp, #128]
 18c:	str	w1, [sp, #144]
 190:	mov	v0.16b, v1.16b
 194:	bl	0 <__unordtf2>
 198:	cmp	w0, #0x0
 19c:	ldr	w1, [sp, #144]
 1a0:	cset	w0, eq  // eq = none
 1a4:	orr	w1, w1, w0
 1a8:	tbz	w1, #0, 2b0 <__divtc3+0x2b0>
 1ac:	adrp	x0, 0 <__divtc3>
 1b0:	add	x0, x0, #0x0
 1b4:	ldr	q2, [x0]
 1b8:	tbz	x25, #63, 1c8 <__divtc3+0x1c8>
 1bc:	adrp	x0, 0 <__divtc3>
 1c0:	add	x0, x0, #0x0
 1c4:	ldr	q2, [x0]
 1c8:	stp	x20, x22, [sp, #96]
 1cc:	mov	v0.16b, v2.16b
 1d0:	ldr	q1, [sp, #96]
 1d4:	str	q2, [sp, #128]
 1d8:	bl	0 <__multf3>
 1dc:	stp	x21, x19, [sp, #112]
 1e0:	ldr	q2, [sp, #128]
 1e4:	str	q0, [sp, #96]
 1e8:	ldr	q1, [sp, #112]
 1ec:	mov	v0.16b, v2.16b
 1f0:	bl	0 <__multf3>
 1f4:	str	q0, [sp, #112]
 1f8:	ldp	x19, x20, [sp, #16]
 1fc:	ldp	x21, x22, [sp, #32]
 200:	ldp	x23, x24, [sp, #48]
 204:	ldp	x25, x26, [sp, #64]
 208:	ldp	x27, x28, [sp, #80]
 20c:	ldr	q0, [sp, #96]
 210:	ldr	q1, [sp, #112]
 214:	ldp	x29, x30, [sp], #160
 218:	ret
 21c:	stp	x24, x25, [sp, #96]
 220:	ldr	q1, [sp, #96]
 224:	stp	x23, x26, [sp, #96]
 228:	ldr	q0, [sp, #96]
 22c:	bl	0 <__divtf3>
 230:	stp	x23, x26, [sp, #96]
 234:	mov	v1.16b, v0.16b
 238:	str	q0, [sp, #128]
 23c:	ldr	q0, [sp, #96]
 240:	bl	0 <__multf3>
 244:	stp	x24, x25, [sp, #96]
 248:	ldr	q1, [sp, #96]
 24c:	bl	0 <__addtf3>
 250:	stp	x21, x19, [sp, #96]
 254:	ldr	q2, [sp, #128]
 258:	ldr	q1, [sp, #96]
 25c:	str	q0, [sp, #112]
 260:	mov	v0.16b, v2.16b
 264:	str	q2, [sp, #144]
 268:	bl	0 <__multf3>
 26c:	stp	x20, x22, [sp, #96]
 270:	ldr	q1, [sp, #96]
 274:	bl	0 <__addtf3>
 278:	ldr	q4, [sp, #112]
 27c:	mov	v1.16b, v4.16b
 280:	str	q4, [sp, #128]
 284:	bl	0 <__divtf3>
 288:	stp	x20, x22, [sp, #112]
 28c:	ldr	q2, [sp, #144]
 290:	ldr	q1, [sp, #112]
 294:	str	q0, [sp, #96]
 298:	mov	v0.16b, v2.16b
 29c:	bl	0 <__multf3>
 2a0:	stp	x21, x19, [sp, #112]
 2a4:	mov	v1.16b, v0.16b
 2a8:	ldr	q0, [sp, #112]
 2ac:	b	e0 <__divtc3+0xe0>
 2b0:	adrp	x0, 0 <__divtc3>
 2b4:	add	x0, x0, #0x0
 2b8:	ldr	q1, [x0]
 2bc:	and	x0, x22, #0x7fffffffffffffff
 2c0:	stp	x20, x0, [sp, #128]
 2c4:	ldr	q0, [sp, #128]
 2c8:	bl	0 <__unordtf2>
 2cc:	cbnz	w0, 2f4 <__divtc3+0x2f4>
 2d0:	adrp	x0, 0 <__divtc3>
 2d4:	add	x0, x0, #0x0
 2d8:	ldr	q1, [x0]
 2dc:	and	x0, x22, #0x7fffffffffffffff
 2e0:	stp	x20, x0, [sp, #128]
 2e4:	ldr	q0, [sp, #128]
 2e8:	bl	0 <__letf2>
 2ec:	cmp	w0, #0x0
 2f0:	b.gt	33c <__divtc3+0x33c>
 2f4:	adrp	x0, 0 <__divtc3>
 2f8:	add	x0, x0, #0x0
 2fc:	and	x1, x19, #0x7fffffffffffffff
 300:	stp	x21, x1, [sp, #128]
 304:	ldr	q1, [x0]
 308:	ldr	q0, [sp, #128]
 30c:	str	x1, [sp, #144]
 310:	bl	0 <__unordtf2>
 314:	cbnz	w0, 50c <__divtc3+0x50c>
 318:	adrp	x0, 0 <__divtc3>
 31c:	add	x0, x0, #0x0
 320:	ldr	x1, [sp, #144]
 324:	stp	x21, x1, [sp, #128]
 328:	ldr	q1, [x0]
 32c:	ldr	q0, [sp, #128]
 330:	bl	0 <__letf2>
 334:	cmp	w0, #0x0
 338:	b.le	50c <__divtc3+0x50c>
 33c:	adrp	x0, 0 <__divtc3>
 340:	add	x0, x0, #0x0
 344:	stp	x24, x28, [sp, #128]
 348:	ldr	q1, [x0]
 34c:	ldr	q0, [sp, #128]
 350:	bl	0 <__unordtf2>
 354:	cbnz	w0, 50c <__divtc3+0x50c>
 358:	adrp	x0, 0 <__divtc3>
 35c:	add	x0, x0, #0x0
 360:	stp	x24, x28, [sp, #128]
 364:	ldr	q1, [x0]
 368:	ldr	q0, [sp, #128]
 36c:	bl	0 <__gttf2>
 370:	cmp	w0, #0x0
 374:	b.gt	50c <__divtc3+0x50c>
 378:	adrp	x0, 0 <__divtc3>
 37c:	add	x0, x0, #0x0
 380:	stp	x23, x27, [sp, #128]
 384:	ldr	q1, [x0]
 388:	ldr	q0, [sp, #128]
 38c:	bl	0 <__unordtf2>
 390:	cbnz	w0, 50c <__divtc3+0x50c>
 394:	adrp	x0, 0 <__divtc3>
 398:	add	x0, x0, #0x0
 39c:	stp	x23, x27, [sp, #128]
 3a0:	ldr	q1, [x0]
 3a4:	ldr	q0, [sp, #128]
 3a8:	bl	0 <__gttf2>
 3ac:	cmp	w0, #0x0
 3b0:	b.gt	50c <__divtc3+0x50c>
 3b4:	adrp	x0, 0 <__divtc3>
 3b8:	add	x0, x0, #0x0
 3bc:	mov	w27, #0x1                   	// #1
 3c0:	ldr	q1, [x0]
 3c4:	and	x0, x22, #0x7fffffffffffffff
 3c8:	stp	x20, x0, [sp, #96]
 3cc:	ldr	q0, [sp, #96]
 3d0:	bl	0 <__unordtf2>
 3d4:	cbnz	w0, 3fc <__divtc3+0x3fc>
 3d8:	adrp	x0, 0 <__divtc3>
 3dc:	add	x0, x0, #0x0
 3e0:	ldr	q1, [x0]
 3e4:	and	x0, x22, #0x7fffffffffffffff
 3e8:	stp	x20, x0, [sp, #96]
 3ec:	ldr	q0, [sp, #96]
 3f0:	bl	0 <__letf2>
 3f4:	cmp	w0, #0x0
 3f8:	cset	w27, le
 3fc:	eor	w0, w27, #0x1
 400:	and	x28, x19, #0x7fffffffffffffff
 404:	and	w0, w0, #0x1
 408:	bl	0 <__floatsitf>
 40c:	str	q0, [sp, #96]
 410:	mov	w27, #0x1                   	// #1
 414:	ldp	x20, x0, [sp, #96]
 418:	stp	x21, x28, [sp, #96]
 41c:	ldr	q0, [sp, #96]
 420:	bfxil	x22, x0, #0, #63
 424:	adrp	x0, 0 <__divtc3>
 428:	add	x0, x0, #0x0
 42c:	ldr	q1, [x0]
 430:	bl	0 <__unordtf2>
 434:	cbnz	w0, 458 <__divtc3+0x458>
 438:	adrp	x0, 0 <__divtc3>
 43c:	add	x0, x0, #0x0
 440:	stp	x21, x28, [sp, #96]
 444:	ldr	q1, [x0]
 448:	ldr	q0, [sp, #96]
 44c:	bl	0 <__letf2>
 450:	cmp	w0, #0x0
 454:	cset	w27, le
 458:	eor	w0, w27, #0x1
 45c:	and	w0, w0, #0x1
 460:	bl	0 <__floatsitf>
 464:	str	q0, [sp, #96]
 468:	ldp	x21, x0, [sp, #96]
 46c:	stp	x20, x22, [sp, #96]
 470:	ldr	q1, [sp, #96]
 474:	stp	x24, x25, [sp, #96]
 478:	ldr	q0, [sp, #96]
 47c:	bfxil	x19, x0, #0, #63
 480:	bl	0 <__multf3>
 484:	stp	x21, x19, [sp, #96]
 488:	ldr	q1, [sp, #96]
 48c:	stp	x23, x26, [sp, #96]
 490:	str	q0, [sp, #112]
 494:	ldr	q0, [sp, #96]
 498:	bl	0 <__multf3>
 49c:	mov	v1.16b, v0.16b
 4a0:	ldr	q2, [sp, #112]
 4a4:	mov	v0.16b, v2.16b
 4a8:	bl	0 <__addtf3>
 4ac:	adrp	x0, 0 <__divtc3>
 4b0:	add	x0, x0, #0x0
 4b4:	ldr	q1, [x0]
 4b8:	bl	0 <__multf3>
 4bc:	stp	x21, x19, [sp, #112]
 4c0:	ldr	q1, [sp, #112]
 4c4:	stp	x24, x25, [sp, #112]
 4c8:	str	q0, [sp, #96]
 4cc:	ldr	q0, [sp, #112]
 4d0:	bl	0 <__multf3>
 4d4:	stp	x20, x22, [sp, #112]
 4d8:	ldr	q1, [sp, #112]
 4dc:	stp	x23, x26, [sp, #112]
 4e0:	str	q0, [sp, #128]
 4e4:	ldr	q0, [sp, #112]
 4e8:	bl	0 <__multf3>
 4ec:	mov	v1.16b, v0.16b
 4f0:	ldr	q2, [sp, #128]
 4f4:	mov	v0.16b, v2.16b
 4f8:	bl	0 <__subtf3>
 4fc:	adrp	x0, 0 <__divtc3>
 500:	add	x0, x0, #0x0
 504:	ldr	q1, [x0]
 508:	b	1f0 <__divtc3+0x1f0>
 50c:	adrp	x0, 0 <__divtc3>
 510:	add	x0, x0, #0x0
 514:	stp	x24, x28, [sp, #128]
 518:	ldr	q1, [x0]
 51c:	ldr	q0, [sp, #128]
 520:	bl	0 <__unordtf2>
 524:	cbnz	w0, 548 <__divtc3+0x548>
 528:	adrp	x0, 0 <__divtc3>
 52c:	add	x0, x0, #0x0
 530:	stp	x24, x28, [sp, #128]
 534:	ldr	q1, [x0]
 538:	ldr	q0, [sp, #128]
 53c:	bl	0 <__letf2>
 540:	cmp	w0, #0x0
 544:	b.gt	584 <__divtc3+0x584>
 548:	adrp	x0, 0 <__divtc3>
 54c:	add	x0, x0, #0x0
 550:	stp	x23, x27, [sp, #128]
 554:	ldr	q1, [x0]
 558:	ldr	q0, [sp, #128]
 55c:	bl	0 <__unordtf2>
 560:	cbnz	w0, 1f8 <__divtc3+0x1f8>
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	stp	x23, x27, [sp, #128]
 570:	ldr	q1, [x0]
 574:	ldr	q0, [sp, #128]
 578:	bl	0 <__letf2>
 57c:	cmp	w0, #0x0
 580:	b.le	1f8 <__divtc3+0x1f8>
 584:	adrp	x0, 0 <__divtc3>
 588:	add	x0, x0, #0x0
 58c:	ldr	q1, [x0]
 590:	and	x0, x22, #0x7fffffffffffffff
 594:	stp	x20, x0, [sp, #128]
 598:	ldr	q0, [sp, #128]
 59c:	bl	0 <__unordtf2>
 5a0:	cbnz	w0, 1f8 <__divtc3+0x1f8>
 5a4:	adrp	x0, 0 <__divtc3>
 5a8:	add	x0, x0, #0x0
 5ac:	ldr	q1, [x0]
 5b0:	and	x0, x22, #0x7fffffffffffffff
 5b4:	stp	x20, x0, [sp, #128]
 5b8:	ldr	q0, [sp, #128]
 5bc:	bl	0 <__gttf2>
 5c0:	cmp	w0, #0x0
 5c4:	b.gt	1f8 <__divtc3+0x1f8>
 5c8:	adrp	x0, 0 <__divtc3>
 5cc:	add	x0, x0, #0x0
 5d0:	and	x1, x19, #0x7fffffffffffffff
 5d4:	stp	x21, x1, [sp, #128]
 5d8:	ldr	q1, [x0]
 5dc:	ldr	q0, [sp, #128]
 5e0:	str	x1, [sp, #144]
 5e4:	bl	0 <__unordtf2>
 5e8:	cbnz	w0, 1f8 <__divtc3+0x1f8>
 5ec:	adrp	x0, 0 <__divtc3>
 5f0:	add	x0, x0, #0x0
 5f4:	ldr	x1, [sp, #144]
 5f8:	stp	x21, x1, [sp, #128]
 5fc:	ldr	q1, [x0]
 600:	ldr	q0, [sp, #128]
 604:	bl	0 <__gttf2>
 608:	cmp	w0, #0x0
 60c:	b.gt	1f8 <__divtc3+0x1f8>
 610:	adrp	x0, 0 <__divtc3>
 614:	add	x0, x0, #0x0
 618:	stp	x24, x28, [sp, #96]
 61c:	mov	w1, #0x1                   	// #1
 620:	ldr	q1, [x0]
 624:	ldr	q0, [sp, #96]
 628:	str	w1, [sp, #112]
 62c:	bl	0 <__unordtf2>
 630:	ldr	w1, [sp, #112]
 634:	cbnz	w0, 658 <__divtc3+0x658>
 638:	adrp	x0, 0 <__divtc3>
 63c:	add	x0, x0, #0x0
 640:	stp	x24, x28, [sp, #96]
 644:	ldr	q1, [x0]
 648:	ldr	q0, [sp, #96]
 64c:	bl	0 <__letf2>
 650:	cmp	w0, #0x0
 654:	cset	w1, le
 658:	eor	w0, w1, #0x1
 65c:	mov	w28, #0x1                   	// #1
 660:	and	w0, w0, #0x1
 664:	bl	0 <__floatsitf>
 668:	str	q0, [sp, #96]
 66c:	ldp	x24, x0, [sp, #96]
 670:	stp	x23, x27, [sp, #96]
 674:	ldr	q0, [sp, #96]
 678:	bfxil	x25, x0, #0, #63
 67c:	adrp	x0, 0 <__divtc3>
 680:	add	x0, x0, #0x0
 684:	ldr	q1, [x0]
 688:	bl	0 <__unordtf2>
 68c:	cbnz	w0, 6b0 <__divtc3+0x6b0>
 690:	adrp	x0, 0 <__divtc3>
 694:	add	x0, x0, #0x0
 698:	stp	x23, x27, [sp, #96]
 69c:	ldr	q1, [x0]
 6a0:	ldr	q0, [sp, #96]
 6a4:	bl	0 <__letf2>
 6a8:	cmp	w0, #0x0
 6ac:	cset	w28, le
 6b0:	eor	w0, w28, #0x1
 6b4:	and	w0, w0, #0x1
 6b8:	bl	0 <__floatsitf>
 6bc:	str	q0, [sp, #96]
 6c0:	ldp	x23, x0, [sp, #96]
 6c4:	stp	x24, x25, [sp, #96]
 6c8:	ldr	q1, [sp, #96]
 6cc:	stp	x20, x22, [sp, #96]
 6d0:	ldr	q0, [sp, #96]
 6d4:	bfxil	x26, x0, #0, #63
 6d8:	bl	0 <__multf3>
 6dc:	stp	x23, x26, [sp, #96]
 6e0:	ldr	q1, [sp, #96]
 6e4:	stp	x21, x19, [sp, #96]
 6e8:	str	q0, [sp, #112]
 6ec:	ldr	q0, [sp, #96]
 6f0:	bl	0 <__multf3>
 6f4:	mov	v1.16b, v0.16b
 6f8:	ldr	q2, [sp, #112]
 6fc:	mov	v0.16b, v2.16b
 700:	bl	0 <__addtf3>
 704:	movi	v1.2d, #0x0
 708:	bl	0 <__multf3>
 70c:	stp	x24, x25, [sp, #112]
 710:	ldr	q1, [sp, #112]
 714:	stp	x21, x19, [sp, #112]
 718:	str	q0, [sp, #96]
 71c:	ldr	q0, [sp, #112]
 720:	bl	0 <__multf3>
 724:	stp	x23, x26, [sp, #112]
 728:	ldr	q1, [sp, #112]
 72c:	stp	x20, x22, [sp, #112]
 730:	str	q0, [sp, #128]
 734:	ldr	q0, [sp, #112]
 738:	bl	0 <__multf3>
 73c:	mov	v1.16b, v0.16b
 740:	ldr	q2, [sp, #128]
 744:	mov	v0.16b, v2.16b
 748:	bl	0 <__subtf3>
 74c:	movi	v1.2d, #0x0
 750:	b	1f0 <__divtc3+0x1f0>

_bswapsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

_bswapdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

_clrsbsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbdi2>:
   0:	tbz	x0, #63, 8 <__clrsbdi2+0x8>
   4:	mvn	x0, x0
   8:	clz	x1, x0
   c:	cmp	x0, #0x0
  10:	sub	w1, w1, #0x1
  14:	mov	w0, #0x3f                  	// #63
  18:	csel	w0, w1, w0, ne  // ne = any
  1c:	ret

_clrsbdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbti2>:
   0:	mov	x2, x0
   4:	mov	x0, x1
   8:	cbnz	x1, 20 <__clrsbti2+0x20>
   c:	mov	x0, x2
  10:	mov	x1, #0x40                  	// #64
  14:	cbnz	x0, 38 <__clrsbti2+0x38>
  18:	mov	x0, x1
  1c:	b	3c <__clrsbti2+0x3c>
  20:	cmn	x1, #0x1
  24:	b.ne	30 <__clrsbti2+0x30>  // b.any
  28:	mvn	x0, x2
  2c:	b	10 <__clrsbti2+0x10>
  30:	tbnz	x1, #63, 48 <__clrsbti2+0x48>
  34:	mov	x1, #0x0                   	// #0
  38:	clz	x0, x0
  3c:	add	w0, w0, w1
  40:	sub	w0, w0, #0x1
  44:	ret
  48:	mvn	x0, x1
  4c:	b	34 <__clrsbti2+0x34>

_fixunssfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	movi	v1.2s, #0x5f, lsl #24
   4:	fcmpe	s0, s1
   8:	b.lt	20 <__fixunssfdi+0x20>  // b.tstop
   c:	fsub	s0, s0, s1
  10:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  14:	fcvtzs	x0, s0
  18:	add	x0, x0, x1
  1c:	ret
  20:	fcvtzs	x0, s0
  24:	b	1c <__fixunssfdi+0x1c>

_fixunsdfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	adrp	x0, 0 <__fixunsdfdi>
   4:	ldr	d1, [x0]
   8:	fcmpe	d0, d1
   c:	b.lt	24 <__fixunsdfdi+0x24>  // b.tstop
  10:	fsub	d0, d0, d1
  14:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  18:	fcvtzs	x0, d0
  1c:	add	x0, x0, x1
  20:	ret
  24:	fcvtzs	x0, d0
  28:	b	20 <__fixunsdfdi+0x20>

_fixunsxfsi.o:     file format elf64-littleaarch64


_fixsfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fcmpe	s0, #0.0
   4:	b.pl	28 <__fixsfti+0x28>  // b.nfrst
   8:	fneg	s0, s0
   c:	stp	x29, x30, [sp, #-16]!
  10:	mov	x29, sp
  14:	bl	0 <__fixunssfti>
  18:	negs	x0, x0
  1c:	ngc	x1, x1
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	b	0 <__fixunssfti>

_fixdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fcmpe	d0, #0.0
   4:	b.pl	28 <__fixdfti+0x28>  // b.nfrst
   8:	fneg	d0, d0
   c:	stp	x29, x30, [sp, #-16]!
  10:	mov	x29, sp
  14:	bl	0 <__fixunsdfti>
  18:	negs	x0, x0
  1c:	ngc	x1, x1
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	b	0 <__fixunsdfti>

_fixxfdi.o:     file format elf64-littleaarch64


_fixunssfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fcvt	d0, s0
   4:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   8:	fmov	d1, x0
   c:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
  10:	fmul	d1, d0, d1
  14:	fcvtzu	x1, d1
  18:	fmov	d1, x0
  1c:	ucvtf	d2, x1
  20:	fmsub	d0, d2, d1, d0
  24:	fcvtzu	x0, d0
  28:	ret

_fixunsdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   4:	fmov	d1, x0
   8:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
   c:	fmul	d1, d0, d1
  10:	fcvtzu	x1, d1
  14:	fmov	d1, x0
  18:	ucvtf	d2, x1
  1c:	fmsub	d0, d2, d1, d0
  20:	fcvtzu	x0, d0
  24:	ret

_fixunsxfdi.o:     file format elf64-littleaarch64


_floatdisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x3, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x2, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x3
  28:	b.hi	38 <__floattisf+0x38>  // b.pmore
  2c:	b.ne	48 <__floattisf+0x48>  // b.any
  30:	cmn	x2, #0x2
  34:	b.ls	48 <__floattisf+0x48>  // b.plast
  38:	tst	x19, #0x7fff
  3c:	b.eq	48 <__floattisf+0x48>  // b.none
  40:	and	x19, x19, #0xffffffffffff8000
  44:	orr	x19, x19, #0x8000
  48:	bl	0 <__floatditf>
  4c:	adrp	x0, 0 <__floattisf>
  50:	add	x0, x0, #0x0
  54:	ldr	q1, [x0]
  58:	bl	0 <__multf3>
  5c:	str	q0, [sp, #32]
  60:	mov	x0, x19
  64:	bl	0 <__floatunditf>
  68:	ldr	q1, [sp, #32]
  6c:	bl	0 <__addtf3>
  70:	bl	0 <__trunctfsf2>
  74:	ldr	x19, [sp, #16]
  78:	ldp	x29, x30, [sp], #48
  7c:	ret

_floatdidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x3, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x2, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x3
  28:	b.hi	38 <__floattidf+0x38>  // b.pmore
  2c:	b.ne	48 <__floattidf+0x48>  // b.any
  30:	cmn	x2, #0x2
  34:	b.ls	48 <__floattidf+0x48>  // b.plast
  38:	tst	x19, #0x7fff
  3c:	b.eq	48 <__floattidf+0x48>  // b.none
  40:	and	x19, x19, #0xffffffffffff8000
  44:	orr	x19, x19, #0x8000
  48:	bl	0 <__floatditf>
  4c:	adrp	x0, 0 <__floattidf>
  50:	add	x0, x0, #0x0
  54:	ldr	q1, [x0]
  58:	bl	0 <__multf3>
  5c:	str	q0, [sp, #32]
  60:	mov	x0, x19
  64:	bl	0 <__floatunditf>
  68:	ldr	q1, [sp, #32]
  6c:	bl	0 <__addtf3>
  70:	bl	0 <__trunctfdf2>
  74:	ldr	x19, [sp, #16]
  78:	ldp	x29, x30, [sp], #48
  7c:	ret

_floatdixf.o:     file format elf64-littleaarch64


_floatundisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntisf+0x30>  // b.plast
  20:	tst	x19, #0x7fff
  24:	b.eq	30 <__floatuntisf+0x30>  // b.none
  28:	and	x19, x19, #0xffffffffffff8000
  2c:	orr	x19, x19, #0x8000
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntisf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfsf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntidf+0x30>  // b.plast
  20:	tst	x19, #0x7fff
  24:	b.eq	30 <__floatuntidf+0x30>  // b.none
  28:	and	x19, x19, #0xffffffffffff8000
  2c:	orr	x19, x19, #0x8000
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntidf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfdf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundixf.o:     file format elf64-littleaarch64


_eprintf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x4, x3
   8:	mov	w3, w2
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	adrp	x19, 0 <stderr>
  18:	mov	x2, x1
  1c:	mov	x1, x0
  20:	ldr	x19, [x19]
  24:	ldr	x0, [x19]
  28:	bl	0 <fprintf>
  2c:	ldr	x0, [x19]
  30:	bl	0 <fflush>
  34:	bl	0 <abort>

__gcc_bcmp.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_bcmp>:
   0:	sub	x1, x1, #0x1
   4:	mov	x4, #0x0                   	// #0
   8:	cmp	x2, x4
   c:	b.ne	18 <__gcc_bcmp+0x18>  // b.any
  10:	mov	w0, #0x0                   	// #0
  14:	b	30 <__gcc_bcmp+0x30>
  18:	ldrb	w3, [x0, x4]
  1c:	add	x4, x4, #0x1
  20:	ldrb	w5, [x1, x4]
  24:	cmp	w3, w5
  28:	b.eq	8 <__gcc_bcmp+0x8>  // b.none
  2c:	sub	w0, w3, w5
  30:	ret

_divdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	mov	x8, x0
   4:	tbz	x1, #63, ec <__divti3+0xec>
   8:	negs	x8, x0
   c:	mov	x7, #0xffffffffffffffff    	// #-1
  10:	ngc	x1, x1
  14:	tbz	x3, #63, 24 <__divti3+0x24>
  18:	negs	x2, x2
  1c:	mvn	x7, x7
  20:	ngc	x3, x3
  24:	mov	x4, x2
  28:	mov	x6, x3
  2c:	mov	x9, x8
  30:	mov	x5, x1
  34:	cbnz	x3, 260 <__divti3+0x260>
  38:	cmp	x2, x1
  3c:	b.ls	104 <__divti3+0x104>  // b.plast
  40:	clz	x0, x2
  44:	cbz	x0, 60 <__divti3+0x60>
  48:	neg	w3, w0
  4c:	lsl	x5, x1, x0
  50:	lsl	x4, x2, x0
  54:	lsr	x3, x8, x3
  58:	orr	x5, x3, x5
  5c:	lsl	x9, x8, x0
  60:	lsr	x1, x4, #32
  64:	and	x8, x4, #0xffffffff
  68:	udiv	x2, x5, x1
  6c:	msub	x5, x2, x1, x5
  70:	mul	x10, x8, x2
  74:	extr	x3, x5, x9, #32
  78:	cmp	x10, x3
  7c:	b.ls	f4 <__divti3+0xf4>  // b.plast
  80:	sub	x0, x2, #0x1
  84:	adds	x3, x4, x3
  88:	b.cs	9c <__divti3+0x9c>  // b.hs, b.nlast
  8c:	cmp	x10, x3
  90:	b.ls	9c <__divti3+0x9c>  // b.plast
  94:	sub	x0, x2, #0x2
  98:	add	x3, x3, x4
  9c:	sub	x3, x3, x10
  a0:	and	x9, x9, #0xffffffff
  a4:	udiv	x5, x3, x1
  a8:	msub	x3, x5, x1, x3
  ac:	mul	x8, x8, x5
  b0:	orr	x3, x9, x3, lsl #32
  b4:	cmp	x8, x3
  b8:	b.ls	fc <__divti3+0xfc>  // b.plast
  bc:	sub	x1, x5, #0x1
  c0:	adds	x3, x4, x3
  c4:	b.cs	d4 <__divti3+0xd4>  // b.hs, b.nlast
  c8:	cmp	x8, x3
  cc:	b.ls	d4 <__divti3+0xd4>  // b.plast
  d0:	sub	x1, x5, #0x2
  d4:	orr	x0, x1, x0, lsl #32
  d8:	mov	x1, x6
  dc:	cbz	x7, e8 <__divti3+0xe8>
  e0:	negs	x0, x0
  e4:	ngc	x1, x6
  e8:	ret
  ec:	mov	x7, #0x0                   	// #0
  f0:	b	14 <__divti3+0x14>
  f4:	mov	x0, x2
  f8:	b	9c <__divti3+0x9c>
  fc:	mov	x1, x5
 100:	b	d4 <__divti3+0xd4>
 104:	cbnz	x2, 110 <__divti3+0x110>
 108:	mov	x4, #0x1                   	// #1
 10c:	udiv	x4, x4, x3
 110:	clz	x2, x4
 114:	cbnz	x2, 19c <__divti3+0x19c>
 118:	sub	x1, x1, x4
 11c:	mov	x6, #0x1                   	// #1
 120:	lsr	x3, x4, #32
 124:	and	x5, x4, #0xffffffff
 128:	udiv	x8, x1, x3
 12c:	msub	x1, x8, x3, x1
 130:	mul	x10, x5, x8
 134:	extr	x2, x1, x9, #32
 138:	cmp	x10, x2
 13c:	b.ls	250 <__divti3+0x250>  // b.plast
 140:	sub	x0, x8, #0x1
 144:	adds	x2, x4, x2
 148:	b.cs	15c <__divti3+0x15c>  // b.hs, b.nlast
 14c:	cmp	x10, x2
 150:	b.ls	15c <__divti3+0x15c>  // b.plast
 154:	sub	x0, x8, #0x2
 158:	add	x2, x2, x4
 15c:	sub	x2, x2, x10
 160:	and	x9, x9, #0xffffffff
 164:	udiv	x1, x2, x3
 168:	msub	x2, x1, x3, x2
 16c:	mul	x3, x5, x1
 170:	orr	x2, x9, x2, lsl #32
 174:	cmp	x3, x2
 178:	b.ls	258 <__divti3+0x258>  // b.plast
 17c:	sub	x5, x1, #0x1
 180:	adds	x4, x4, x2
 184:	b.cs	194 <__divti3+0x194>  // b.hs, b.nlast
 188:	cmp	x3, x4
 18c:	b.ls	194 <__divti3+0x194>  // b.plast
 190:	sub	x5, x1, #0x2
 194:	orr	x0, x5, x0, lsl #32
 198:	b	d8 <__divti3+0xd8>
 19c:	mov	x0, #0x40                  	// #64
 1a0:	sub	x0, x0, x2
 1a4:	lsl	x4, x4, x2
 1a8:	lsr	x5, x1, x0
 1ac:	lsr	x0, x8, x0
 1b0:	lsl	x1, x1, x2
 1b4:	orr	x1, x0, x1
 1b8:	lsr	x0, x4, #32
 1bc:	lsl	x9, x8, x2
 1c0:	and	x2, x4, #0xffffffff
 1c4:	udiv	x3, x5, x0
 1c8:	msub	x5, x3, x0, x5
 1cc:	mul	x8, x2, x3
 1d0:	extr	x5, x5, x1, #32
 1d4:	cmp	x8, x5
 1d8:	b.ls	240 <__divti3+0x240>  // b.plast
 1dc:	sub	x6, x3, #0x1
 1e0:	adds	x5, x4, x5
 1e4:	b.cs	1f8 <__divti3+0x1f8>  // b.hs, b.nlast
 1e8:	cmp	x8, x5
 1ec:	b.ls	1f8 <__divti3+0x1f8>  // b.plast
 1f0:	sub	x6, x3, #0x2
 1f4:	add	x5, x5, x4
 1f8:	sub	x5, x5, x8
 1fc:	and	x1, x1, #0xffffffff
 200:	udiv	x3, x5, x0
 204:	msub	x0, x3, x0, x5
 208:	mul	x2, x2, x3
 20c:	orr	x1, x1, x0, lsl #32
 210:	cmp	x2, x1
 214:	b.ls	248 <__divti3+0x248>  // b.plast
 218:	sub	x0, x3, #0x1
 21c:	adds	x1, x4, x1
 220:	b.cs	234 <__divti3+0x234>  // b.hs, b.nlast
 224:	cmp	x2, x1
 228:	b.ls	234 <__divti3+0x234>  // b.plast
 22c:	sub	x0, x3, #0x2
 230:	add	x1, x1, x4
 234:	sub	x1, x1, x2
 238:	orr	x6, x0, x6, lsl #32
 23c:	b	120 <__divti3+0x120>
 240:	mov	x6, x3
 244:	b	1f8 <__divti3+0x1f8>
 248:	mov	x0, x3
 24c:	b	234 <__divti3+0x234>
 250:	mov	x0, x8
 254:	b	15c <__divti3+0x15c>
 258:	mov	x5, x1
 25c:	b	194 <__divti3+0x194>
 260:	cmp	x3, x1
 264:	b.hi	398 <__divti3+0x398>  // b.pmore
 268:	clz	x10, x3
 26c:	cbnz	x10, 280 <__divti3+0x280>
 270:	ccmp	x2, x8, #0x0, cs  // cs = hs, nlast
 274:	cset	x0, ls  // ls = plast
 278:	mov	x6, #0x0                   	// #0
 27c:	b	d8 <__divti3+0xd8>
 280:	mov	x9, #0x40                  	// #64
 284:	sub	x9, x9, x10
 288:	lsl	x3, x3, x10
 28c:	lsr	x6, x2, x9
 290:	orr	x6, x6, x3
 294:	lsr	x5, x1, x9
 298:	and	x0, x6, #0xffffffff
 29c:	lsr	x4, x6, #32
 2a0:	lsr	x9, x8, x9
 2a4:	lsl	x1, x1, x10
 2a8:	orr	x1, x9, x1
 2ac:	lsl	x2, x2, x10
 2b0:	udiv	x9, x5, x4
 2b4:	msub	x5, x9, x4, x5
 2b8:	mul	x11, x0, x9
 2bc:	extr	x5, x5, x1, #32
 2c0:	cmp	x11, x5
 2c4:	b.ls	388 <__divti3+0x388>  // b.plast
 2c8:	sub	x3, x9, #0x1
 2cc:	adds	x5, x6, x5
 2d0:	b.cs	2e4 <__divti3+0x2e4>  // b.hs, b.nlast
 2d4:	cmp	x11, x5
 2d8:	b.ls	2e4 <__divti3+0x2e4>  // b.plast
 2dc:	sub	x3, x9, #0x2
 2e0:	add	x5, x5, x6
 2e4:	sub	x5, x5, x11
 2e8:	and	x1, x1, #0xffffffff
 2ec:	udiv	x9, x5, x4
 2f0:	msub	x4, x9, x4, x5
 2f4:	mul	x0, x0, x9
 2f8:	orr	x1, x1, x4, lsl #32
 2fc:	cmp	x0, x1
 300:	b.ls	390 <__divti3+0x390>  // b.plast
 304:	sub	x4, x9, #0x1
 308:	adds	x1, x6, x1
 30c:	b.cs	320 <__divti3+0x320>  // b.hs, b.nlast
 310:	cmp	x0, x1
 314:	b.ls	320 <__divti3+0x320>  // b.plast
 318:	sub	x4, x9, #0x2
 31c:	add	x1, x1, x6
 320:	sub	x1, x1, x0
 324:	orr	x0, x4, x3, lsl #32
 328:	and	x5, x2, #0xffffffff
 32c:	mov	w4, w4
 330:	lsr	x3, x0, #32
 334:	lsr	x2, x2, #32
 338:	mul	x6, x4, x5
 33c:	mul	x5, x3, x5
 340:	madd	x4, x4, x2, x5
 344:	mul	x3, x3, x2
 348:	add	x4, x4, x6, lsr #32
 34c:	cmp	x5, x4
 350:	b.ls	35c <__divti3+0x35c>  // b.plast
 354:	mov	x2, #0x100000000           	// #4294967296
 358:	add	x3, x3, x2
 35c:	add	x3, x3, x4, lsr #32
 360:	cmp	x1, x3
 364:	b.cc	380 <__divti3+0x380>  // b.lo, b.ul, b.last
 368:	and	x6, x6, #0xffffffff
 36c:	lsl	x8, x8, x10
 370:	add	x4, x6, x4, lsl #32
 374:	cmp	x8, x4
 378:	ccmp	x1, x3, #0x0, cc  // cc = lo, ul, last
 37c:	b.ne	278 <__divti3+0x278>  // b.any
 380:	sub	x0, x0, #0x1
 384:	b	278 <__divti3+0x278>
 388:	mov	x3, x9
 38c:	b	2e4 <__divti3+0x2e4>
 390:	mov	x4, x9
 394:	b	320 <__divti3+0x320>
 398:	mov	x6, #0x0                   	// #0
 39c:	mov	x0, #0x0                   	// #0
 3a0:	b	d8 <__divti3+0xd8>

_moddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	tbz	x1, #63, e0 <__modti3+0xe0>
   4:	negs	x0, x0
   8:	mov	x7, #0xffffffffffffffff    	// #-1
   c:	ngc	x1, x1
  10:	tbz	x3, #63, 1c <__modti3+0x1c>
  14:	negs	x2, x2
  18:	ngc	x3, x3
  1c:	mov	x6, x2
  20:	mov	x5, x0
  24:	mov	x4, x1
  28:	cbnz	x3, 204 <__modti3+0x204>
  2c:	cmp	x2, x1
  30:	b.ls	e8 <__modti3+0xe8>  // b.plast
  34:	clz	x0, x2
  38:	mov	x8, x0
  3c:	cbz	x0, 58 <__modti3+0x58>
  40:	lsl	x4, x1, x0
  44:	neg	w1, w0
  48:	lsl	x6, x2, x0
  4c:	lsr	x1, x5, x1
  50:	orr	x4, x1, x4
  54:	lsl	x5, x5, x0
  58:	lsr	x1, x6, #32
  5c:	and	x0, x6, #0xffffffff
  60:	udiv	x2, x4, x1
  64:	msub	x4, x2, x1, x4
  68:	mul	x2, x0, x2
  6c:	extr	x4, x4, x5, #32
  70:	cmp	x2, x4
  74:	b.ls	8c <__modti3+0x8c>  // b.plast
  78:	adds	x4, x6, x4
  7c:	b.cs	8c <__modti3+0x8c>  // b.hs, b.nlast
  80:	cmp	x2, x4
  84:	b.ls	8c <__modti3+0x8c>  // b.plast
  88:	add	x4, x4, x6
  8c:	sub	x4, x4, x2
  90:	and	x5, x5, #0xffffffff
  94:	udiv	x2, x4, x1
  98:	msub	x4, x2, x1, x4
  9c:	mul	x0, x0, x2
  a0:	orr	x4, x5, x4, lsl #32
  a4:	cmp	x0, x4
  a8:	b.ls	c0 <__modti3+0xc0>  // b.plast
  ac:	adds	x4, x6, x4
  b0:	b.cs	c0 <__modti3+0xc0>  // b.hs, b.nlast
  b4:	cmp	x0, x4
  b8:	b.ls	c0 <__modti3+0xc0>  // b.plast
  bc:	add	x4, x4, x6
  c0:	sub	x5, x4, x0
  c4:	lsr	x0, x5, x8
  c8:	mov	x1, #0x0                   	// #0
  cc:	cbz	x7, dc <__modti3+0xdc>
  d0:	negs	x5, x0
  d4:	ngc	x1, x1
  d8:	mov	x0, x5
  dc:	ret
  e0:	mov	x7, #0x0                   	// #0
  e4:	b	10 <__modti3+0x10>
  e8:	cbnz	x2, f4 <__modti3+0xf4>
  ec:	mov	x6, #0x1                   	// #1
  f0:	udiv	x6, x6, x3
  f4:	clz	x3, x6
  f8:	mov	x8, x3
  fc:	cbnz	x3, 174 <__modti3+0x174>
 100:	sub	x1, x1, x6
 104:	lsr	x2, x6, #32
 108:	and	x0, x6, #0xffffffff
 10c:	udiv	x3, x1, x2
 110:	msub	x1, x3, x2, x1
 114:	mul	x3, x0, x3
 118:	extr	x1, x1, x5, #32
 11c:	cmp	x3, x1
 120:	b.ls	138 <__modti3+0x138>  // b.plast
 124:	adds	x1, x6, x1
 128:	b.cs	138 <__modti3+0x138>  // b.hs, b.nlast
 12c:	cmp	x3, x1
 130:	b.ls	138 <__modti3+0x138>  // b.plast
 134:	add	x1, x1, x6
 138:	sub	x1, x1, x3
 13c:	and	x5, x5, #0xffffffff
 140:	udiv	x4, x1, x2
 144:	msub	x1, x4, x2, x1
 148:	mul	x0, x0, x4
 14c:	orr	x5, x5, x1, lsl #32
 150:	cmp	x0, x5
 154:	b.ls	16c <__modti3+0x16c>  // b.plast
 158:	adds	x5, x6, x5
 15c:	b.cs	16c <__modti3+0x16c>  // b.hs, b.nlast
 160:	cmp	x0, x5
 164:	b.ls	16c <__modti3+0x16c>  // b.plast
 168:	add	x5, x5, x6
 16c:	sub	x5, x5, x0
 170:	b	c4 <__modti3+0xc4>
 174:	mov	x2, #0x40                  	// #64
 178:	sub	x2, x2, x3
 17c:	lsl	x6, x6, x3
 180:	and	x4, x6, #0xffffffff
 184:	lsr	x9, x1, x2
 188:	lsr	x2, x0, x2
 18c:	lsl	x1, x1, x3
 190:	orr	x1, x2, x1
 194:	lsr	x2, x6, #32
 198:	lsl	x5, x0, x3
 19c:	udiv	x3, x9, x2
 1a0:	msub	x9, x3, x2, x9
 1a4:	mul	x0, x4, x3
 1a8:	extr	x3, x9, x1, #32
 1ac:	cmp	x0, x3
 1b0:	b.ls	1c8 <__modti3+0x1c8>  // b.plast
 1b4:	adds	x3, x6, x3
 1b8:	b.cs	1c8 <__modti3+0x1c8>  // b.hs, b.nlast
 1bc:	cmp	x0, x3
 1c0:	b.ls	1c8 <__modti3+0x1c8>  // b.plast
 1c4:	add	x3, x3, x6
 1c8:	sub	x3, x3, x0
 1cc:	and	x1, x1, #0xffffffff
 1d0:	udiv	x0, x3, x2
 1d4:	msub	x2, x0, x2, x3
 1d8:	mul	x4, x4, x0
 1dc:	orr	x1, x1, x2, lsl #32
 1e0:	cmp	x4, x1
 1e4:	b.ls	1fc <__modti3+0x1fc>  // b.plast
 1e8:	adds	x1, x6, x1
 1ec:	b.cs	1fc <__modti3+0x1fc>  // b.hs, b.nlast
 1f0:	cmp	x4, x1
 1f4:	b.ls	1fc <__modti3+0x1fc>  // b.plast
 1f8:	add	x1, x1, x6
 1fc:	sub	x1, x1, x4
 200:	b	104 <__modti3+0x104>
 204:	cmp	x3, x1
 208:	b.hi	cc <__modti3+0xcc>  // b.pmore
 20c:	clz	x10, x3
 210:	cbnz	x10, 234 <__modti3+0x234>
 214:	cmp	x3, x1
 218:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 21c:	b.hi	228 <__modti3+0x228>  // b.pmore
 220:	subs	x5, x0, x2
 224:	sbc	x4, x1, x3
 228:	mov	x0, x5
 22c:	mov	x1, x4
 230:	b	cc <__modti3+0xcc>
 234:	mov	x6, #0x40                  	// #64
 238:	sub	x6, x6, x10
 23c:	lsl	x3, x3, x10
 240:	lsr	x11, x2, x6
 244:	orr	x11, x11, x3
 248:	lsr	x8, x1, x6
 24c:	and	x3, x11, #0xffffffff
 250:	lsr	x5, x11, #32
 254:	lsl	x1, x1, x10
 258:	lsr	x4, x0, x6
 25c:	orr	x4, x4, x1
 260:	lsl	x2, x2, x10
 264:	udiv	x1, x8, x5
 268:	lsl	x0, x0, x10
 26c:	msub	x8, x1, x5, x8
 270:	mul	x12, x3, x1
 274:	extr	x8, x8, x4, #32
 278:	cmp	x12, x8
 27c:	b.ls	35c <__modti3+0x35c>  // b.plast
 280:	sub	x9, x1, #0x1
 284:	adds	x8, x11, x8
 288:	b.cs	29c <__modti3+0x29c>  // b.hs, b.nlast
 28c:	cmp	x12, x8
 290:	b.ls	29c <__modti3+0x29c>  // b.plast
 294:	sub	x9, x1, #0x2
 298:	add	x8, x8, x11
 29c:	sub	x8, x8, x12
 2a0:	udiv	x12, x8, x5
 2a4:	msub	x5, x12, x5, x8
 2a8:	mul	x1, x3, x12
 2ac:	and	x3, x4, #0xffffffff
 2b0:	orr	x3, x3, x5, lsl #32
 2b4:	cmp	x1, x3
 2b8:	b.ls	364 <__modti3+0x364>  // b.plast
 2bc:	sub	x8, x12, #0x1
 2c0:	adds	x3, x11, x3
 2c4:	b.cs	2d8 <__modti3+0x2d8>  // b.hs, b.nlast
 2c8:	cmp	x1, x3
 2cc:	b.ls	2d8 <__modti3+0x2d8>  // b.plast
 2d0:	sub	x8, x12, #0x2
 2d4:	add	x3, x3, x11
 2d8:	sub	x3, x3, x1
 2dc:	orr	x1, x8, x9, lsl #32
 2e0:	mov	w8, w8
 2e4:	and	x9, x2, #0xffffffff
 2e8:	lsr	x1, x1, #32
 2ec:	lsr	x5, x2, #32
 2f0:	mul	x4, x8, x9
 2f4:	mul	x9, x1, x9
 2f8:	madd	x8, x8, x5, x9
 2fc:	mul	x1, x1, x5
 300:	add	x8, x8, x4, lsr #32
 304:	cmp	x9, x8
 308:	b.ls	314 <__modti3+0x314>  // b.plast
 30c:	mov	x5, #0x100000000           	// #4294967296
 310:	add	x1, x1, x5
 314:	add	x1, x1, x8, lsr #32
 318:	and	x4, x4, #0xffffffff
 31c:	add	x8, x4, x8, lsl #32
 320:	cmp	x3, x1
 324:	b.cc	330 <__modti3+0x330>  // b.lo, b.ul, b.last
 328:	ccmp	x0, x8, #0x2, eq  // eq = none
 32c:	b.cs	33c <__modti3+0x33c>  // b.hs, b.nlast
 330:	subs	x8, x8, x2
 334:	cinc	x2, x11, cc  // cc = lo, ul, last
 338:	sub	x1, x1, x2
 33c:	subs	x4, x0, x8
 340:	cmp	x0, x8
 344:	sbc	x1, x3, x1
 348:	lsr	x0, x4, x10
 34c:	lsl	x5, x1, x6
 350:	orr	x0, x5, x0
 354:	lsr	x1, x1, x10
 358:	b	cc <__modti3+0xcc>
 35c:	mov	x9, x1
 360:	b	29c <__modti3+0x29c>
 364:	mov	x8, x12
 368:	b	2d8 <__modti3+0x2d8>

_divmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodti4>:
   0:	tbz	x1, #63, 114 <__divmodti4+0x114>
   4:	negs	x5, x0
   8:	mov	x8, #0xffffffffffffffff    	// #-1
   c:	ngc	x6, x1
  10:	mov	x0, x5
  14:	mov	x1, x6
  18:	tbz	x3, #63, 11c <__divmodti4+0x11c>
  1c:	negs	x2, x2
  20:	mvn	x9, x8
  24:	ngc	x3, x3
  28:	mov	x7, x2
  2c:	mov	x10, x3
  30:	mov	x5, x0
  34:	mov	x6, x1
  38:	cbnz	x3, 29c <__divmodti4+0x29c>
  3c:	cmp	x2, x1
  40:	b.ls	134 <__divmodti4+0x134>  // b.plast
  44:	clz	x0, x2
  48:	mov	x11, x0
  4c:	cbz	x0, 68 <__divmodti4+0x68>
  50:	lsl	x6, x1, x0
  54:	neg	w1, w0
  58:	lsl	x7, x2, x0
  5c:	lsr	x1, x5, x1
  60:	orr	x6, x1, x6
  64:	lsl	x5, x5, x0
  68:	lsr	x2, x7, #32
  6c:	and	x1, x7, #0xffffffff
  70:	udiv	x3, x6, x2
  74:	msub	x6, x3, x2, x6
  78:	mul	x0, x1, x3
  7c:	extr	x6, x6, x5, #32
  80:	cmp	x0, x6
  84:	b.ls	124 <__divmodti4+0x124>  // b.plast
  88:	sub	x12, x3, #0x1
  8c:	adds	x6, x7, x6
  90:	b.cs	a4 <__divmodti4+0xa4>  // b.hs, b.nlast
  94:	cmp	x0, x6
  98:	b.ls	a4 <__divmodti4+0xa4>  // b.plast
  9c:	sub	x12, x3, #0x2
  a0:	add	x6, x6, x7
  a4:	sub	x6, x6, x0
  a8:	udiv	x3, x6, x2
  ac:	msub	x2, x3, x2, x6
  b0:	and	x6, x5, #0xffffffff
  b4:	mul	x1, x1, x3
  b8:	orr	x6, x6, x2, lsl #32
  bc:	cmp	x1, x6
  c0:	b.ls	12c <__divmodti4+0x12c>  // b.plast
  c4:	sub	x0, x3, #0x1
  c8:	adds	x6, x7, x6
  cc:	b.cs	e0 <__divmodti4+0xe0>  // b.hs, b.nlast
  d0:	cmp	x1, x6
  d4:	b.ls	e0 <__divmodti4+0xe0>  // b.plast
  d8:	sub	x0, x3, #0x2
  dc:	add	x6, x6, x7
  e0:	sub	x5, x6, x1
  e4:	orr	x0, x0, x12, lsl #32
  e8:	lsr	x5, x5, x11
  ec:	mov	x6, #0x0                   	// #0
  f0:	mov	x1, x10
  f4:	cbz	x9, 100 <__divmodti4+0x100>
  f8:	negs	x0, x0
  fc:	ngc	x1, x10
 100:	cbz	x8, 10c <__divmodti4+0x10c>
 104:	negs	x5, x5
 108:	ngc	x6, x6
 10c:	stp	x5, x6, [x4]
 110:	ret
 114:	mov	x8, #0x0                   	// #0
 118:	b	18 <__divmodti4+0x18>
 11c:	mov	x9, x8
 120:	b	28 <__divmodti4+0x28>
 124:	mov	x12, x3
 128:	b	a4 <__divmodti4+0xa4>
 12c:	mov	x0, x3
 130:	b	e0 <__divmodti4+0xe0>
 134:	cbnz	x2, 140 <__divmodti4+0x140>
 138:	mov	x7, #0x1                   	// #1
 13c:	udiv	x7, x7, x3
 140:	clz	x2, x7
 144:	mov	x11, x2
 148:	cbnz	x2, 1d8 <__divmodti4+0x1d8>
 14c:	sub	x6, x1, x7
 150:	mov	x10, #0x1                   	// #1
 154:	lsr	x3, x7, #32
 158:	and	x1, x7, #0xffffffff
 15c:	udiv	x12, x6, x3
 160:	msub	x6, x12, x3, x6
 164:	mul	x0, x1, x12
 168:	extr	x2, x6, x5, #32
 16c:	cmp	x0, x2
 170:	b.ls	28c <__divmodti4+0x28c>  // b.plast
 174:	sub	x13, x12, #0x1
 178:	adds	x2, x7, x2
 17c:	b.cs	190 <__divmodti4+0x190>  // b.hs, b.nlast
 180:	cmp	x0, x2
 184:	b.ls	190 <__divmodti4+0x190>  // b.plast
 188:	sub	x13, x12, #0x2
 18c:	add	x2, x2, x7
 190:	sub	x2, x2, x0
 194:	and	x5, x5, #0xffffffff
 198:	udiv	x12, x2, x3
 19c:	msub	x2, x12, x3, x2
 1a0:	mul	x6, x1, x12
 1a4:	orr	x5, x5, x2, lsl #32
 1a8:	cmp	x6, x5
 1ac:	b.ls	294 <__divmodti4+0x294>  // b.plast
 1b0:	sub	x0, x12, #0x1
 1b4:	adds	x5, x7, x5
 1b8:	b.cs	1cc <__divmodti4+0x1cc>  // b.hs, b.nlast
 1bc:	cmp	x6, x5
 1c0:	b.ls	1cc <__divmodti4+0x1cc>  // b.plast
 1c4:	sub	x0, x12, #0x2
 1c8:	add	x5, x5, x7
 1cc:	sub	x5, x5, x6
 1d0:	orr	x0, x0, x13, lsl #32
 1d4:	b	e8 <__divmodti4+0xe8>
 1d8:	lsl	x7, x7, x2
 1dc:	mov	x12, #0x40                  	// #64
 1e0:	sub	x12, x12, x2
 1e4:	lsr	x3, x7, #32
 1e8:	lsl	x5, x0, x2
 1ec:	lsr	x10, x1, x12
 1f0:	lsr	x12, x0, x12
 1f4:	udiv	x0, x10, x3
 1f8:	lsl	x6, x1, x2
 1fc:	orr	x12, x12, x6
 200:	and	x6, x7, #0xffffffff
 204:	msub	x10, x0, x3, x10
 208:	mul	x1, x6, x0
 20c:	extr	x10, x10, x12, #32
 210:	cmp	x1, x10
 214:	b.ls	27c <__divmodti4+0x27c>  // b.plast
 218:	sub	x13, x0, #0x1
 21c:	adds	x10, x7, x10
 220:	b.cs	234 <__divmodti4+0x234>  // b.hs, b.nlast
 224:	cmp	x1, x10
 228:	b.ls	234 <__divmodti4+0x234>  // b.plast
 22c:	sub	x13, x0, #0x2
 230:	add	x10, x10, x7
 234:	sub	x10, x10, x1
 238:	and	x12, x12, #0xffffffff
 23c:	udiv	x0, x10, x3
 240:	msub	x3, x0, x3, x10
 244:	mul	x2, x6, x0
 248:	orr	x6, x12, x3, lsl #32
 24c:	cmp	x2, x6
 250:	b.ls	284 <__divmodti4+0x284>  // b.plast
 254:	sub	x1, x0, #0x1
 258:	adds	x6, x7, x6
 25c:	b.cs	270 <__divmodti4+0x270>  // b.hs, b.nlast
 260:	cmp	x2, x6
 264:	b.ls	270 <__divmodti4+0x270>  // b.plast
 268:	sub	x1, x0, #0x2
 26c:	add	x6, x6, x7
 270:	sub	x6, x6, x2
 274:	orr	x10, x1, x13, lsl #32
 278:	b	154 <__divmodti4+0x154>
 27c:	mov	x13, x0
 280:	b	234 <__divmodti4+0x234>
 284:	mov	x1, x0
 288:	b	270 <__divmodti4+0x270>
 28c:	mov	x13, x12
 290:	b	190 <__divmodti4+0x190>
 294:	mov	x0, x12
 298:	b	1cc <__divmodti4+0x1cc>
 29c:	cmp	x3, x1
 2a0:	b.ls	2b0 <__divmodti4+0x2b0>  // b.plast
 2a4:	mov	x10, #0x0                   	// #0
 2a8:	mov	x0, #0x0                   	// #0
 2ac:	b	f0 <__divmodti4+0xf0>
 2b0:	clz	x11, x3
 2b4:	cbnz	x11, 2e0 <__divmodti4+0x2e0>
 2b8:	cmp	x3, x1
 2bc:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 2c0:	b.hi	2d8 <__divmodti4+0x2d8>  // b.pmore
 2c4:	subs	x5, x0, x2
 2c8:	mov	x0, #0x1                   	// #1
 2cc:	sbc	x6, x1, x3
 2d0:	mov	x10, #0x0                   	// #0
 2d4:	b	f0 <__divmodti4+0xf0>
 2d8:	mov	x0, #0x0                   	// #0
 2dc:	b	2d0 <__divmodti4+0x2d0>
 2e0:	mov	x7, #0x40                  	// #64
 2e4:	sub	x7, x7, x11
 2e8:	lsl	x3, x3, x11
 2ec:	lsr	x12, x2, x7
 2f0:	orr	x12, x12, x3
 2f4:	lsr	x10, x1, x7
 2f8:	lsr	x3, x12, #32
 2fc:	lsr	x6, x0, x7
 300:	lsl	x1, x1, x11
 304:	orr	x1, x6, x1
 308:	and	x6, x12, #0xffffffff
 30c:	lsl	x2, x2, x11
 310:	udiv	x13, x10, x3
 314:	lsl	x5, x0, x11
 318:	msub	x10, x13, x3, x10
 31c:	mul	x14, x6, x13
 320:	extr	x10, x10, x1, #32
 324:	cmp	x14, x10
 328:	b.ls	40c <__divmodti4+0x40c>  // b.plast
 32c:	sub	x0, x13, #0x1
 330:	adds	x10, x12, x10
 334:	b.cs	348 <__divmodti4+0x348>  // b.hs, b.nlast
 338:	cmp	x14, x10
 33c:	b.ls	348 <__divmodti4+0x348>  // b.plast
 340:	sub	x0, x13, #0x2
 344:	add	x10, x10, x12
 348:	sub	x10, x10, x14
 34c:	and	x1, x1, #0xffffffff
 350:	udiv	x13, x10, x3
 354:	msub	x3, x13, x3, x10
 358:	mul	x6, x6, x13
 35c:	orr	x1, x1, x3, lsl #32
 360:	cmp	x6, x1
 364:	b.ls	414 <__divmodti4+0x414>  // b.plast
 368:	sub	x3, x13, #0x1
 36c:	adds	x1, x12, x1
 370:	b.cs	384 <__divmodti4+0x384>  // b.hs, b.nlast
 374:	cmp	x6, x1
 378:	b.ls	384 <__divmodti4+0x384>  // b.plast
 37c:	sub	x3, x13, #0x2
 380:	add	x1, x1, x12
 384:	orr	x0, x3, x0, lsl #32
 388:	and	x10, x2, #0xffffffff
 38c:	mov	w3, w3
 390:	sub	x1, x1, x6
 394:	lsr	x6, x0, #32
 398:	lsr	x14, x2, #32
 39c:	mul	x13, x3, x10
 3a0:	mul	x10, x6, x10
 3a4:	madd	x3, x3, x14, x10
 3a8:	mul	x6, x6, x14
 3ac:	add	x3, x3, x13, lsr #32
 3b0:	cmp	x10, x3
 3b4:	b.ls	3c0 <__divmodti4+0x3c0>  // b.plast
 3b8:	mov	x10, #0x100000000           	// #4294967296
 3bc:	add	x6, x6, x10
 3c0:	add	x6, x6, x3, lsr #32
 3c4:	and	x13, x13, #0xffffffff
 3c8:	add	x3, x13, x3, lsl #32
 3cc:	cmp	x1, x6
 3d0:	b.cc	3dc <__divmodti4+0x3dc>  // b.lo, b.ul, b.last
 3d4:	ccmp	x5, x3, #0x2, eq  // eq = none
 3d8:	b.cs	3ec <__divmodti4+0x3ec>  // b.hs, b.nlast
 3dc:	subs	x3, x3, x2
 3e0:	sub	x0, x0, #0x1
 3e4:	cinc	x2, x12, cc  // cc = lo, ul, last
 3e8:	sub	x6, x6, x2
 3ec:	subs	x2, x5, x3
 3f0:	cmp	x5, x3
 3f4:	sbc	x6, x1, x6
 3f8:	lsr	x1, x2, x11
 3fc:	lsl	x5, x6, x7
 400:	orr	x5, x5, x1
 404:	lsr	x6, x6, x11
 408:	b	2d0 <__divmodti4+0x2d0>
 40c:	mov	x0, x13
 410:	b	348 <__divmodti4+0x348>
 414:	mov	x3, x13
 418:	b	384 <__divmodti4+0x384>

_udivdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, x1
   4:	mov	x8, x0
   8:	mov	x5, x2
   c:	mov	x1, x3
  10:	mov	x9, x0
  14:	mov	x6, x4
  18:	cbnz	x3, 22c <__udivti3+0x22c>
  1c:	cmp	x2, x4
  20:	b.ls	d0 <__udivti3+0xd0>  // b.plast
  24:	clz	x0, x2
  28:	cbz	x0, 44 <__udivti3+0x44>
  2c:	lsl	x5, x2, x0
  30:	neg	w2, w0
  34:	lsl	x6, x4, x0
  38:	lsr	x2, x8, x2
  3c:	orr	x6, x2, x6
  40:	lsl	x9, x8, x0
  44:	lsr	x3, x5, #32
  48:	and	x7, x5, #0xffffffff
  4c:	udiv	x4, x6, x3
  50:	msub	x6, x4, x3, x6
  54:	mul	x8, x7, x4
  58:	extr	x2, x6, x9, #32
  5c:	cmp	x8, x2
  60:	b.ls	c0 <__udivti3+0xc0>  // b.plast
  64:	sub	x0, x4, #0x1
  68:	adds	x2, x5, x2
  6c:	b.cs	80 <__udivti3+0x80>  // b.hs, b.nlast
  70:	cmp	x8, x2
  74:	b.ls	80 <__udivti3+0x80>  // b.plast
  78:	sub	x0, x4, #0x2
  7c:	add	x2, x2, x5
  80:	sub	x2, x2, x8
  84:	and	x9, x9, #0xffffffff
  88:	udiv	x6, x2, x3
  8c:	msub	x2, x6, x3, x2
  90:	mul	x7, x7, x6
  94:	orr	x2, x9, x2, lsl #32
  98:	cmp	x7, x2
  9c:	b.ls	c8 <__udivti3+0xc8>  // b.plast
  a0:	sub	x3, x6, #0x1
  a4:	adds	x2, x5, x2
  a8:	b.cs	b8 <__udivti3+0xb8>  // b.hs, b.nlast
  ac:	cmp	x7, x2
  b0:	b.ls	b8 <__udivti3+0xb8>  // b.plast
  b4:	sub	x3, x6, #0x2
  b8:	orr	x0, x3, x0, lsl #32
  bc:	ret
  c0:	mov	x0, x4
  c4:	b	80 <__udivti3+0x80>
  c8:	mov	x3, x6
  cc:	b	b8 <__udivti3+0xb8>
  d0:	cbnz	x2, dc <__udivti3+0xdc>
  d4:	mov	x0, #0x1                   	// #1
  d8:	udiv	x5, x0, x2
  dc:	clz	x1, x5
  e0:	cbnz	x1, 168 <__udivti3+0x168>
  e4:	sub	x4, x4, x5
  e8:	mov	x1, #0x1                   	// #1
  ec:	lsr	x2, x5, #32
  f0:	and	x6, x5, #0xffffffff
  f4:	udiv	x3, x4, x2
  f8:	msub	x4, x3, x2, x4
  fc:	mul	x7, x6, x3
 100:	extr	x4, x4, x9, #32
 104:	cmp	x7, x4
 108:	b.ls	21c <__udivti3+0x21c>  // b.plast
 10c:	sub	x0, x3, #0x1
 110:	adds	x4, x5, x4
 114:	b.cs	128 <__udivti3+0x128>  // b.hs, b.nlast
 118:	cmp	x7, x4
 11c:	b.ls	128 <__udivti3+0x128>  // b.plast
 120:	sub	x0, x3, #0x2
 124:	add	x4, x4, x5
 128:	sub	x4, x4, x7
 12c:	and	x9, x9, #0xffffffff
 130:	udiv	x3, x4, x2
 134:	msub	x4, x3, x2, x4
 138:	mul	x2, x6, x3
 13c:	orr	x4, x9, x4, lsl #32
 140:	cmp	x2, x4
 144:	b.ls	224 <__udivti3+0x224>  // b.plast
 148:	sub	x6, x3, #0x1
 14c:	adds	x5, x5, x4
 150:	b.cs	160 <__udivti3+0x160>  // b.hs, b.nlast
 154:	cmp	x2, x5
 158:	b.ls	160 <__udivti3+0x160>  // b.plast
 15c:	sub	x6, x3, #0x2
 160:	orr	x0, x6, x0, lsl #32
 164:	b	bc <__udivti3+0xbc>
 168:	mov	x0, #0x40                  	// #64
 16c:	sub	x0, x0, x1
 170:	lsl	x5, x5, x1
 174:	and	x2, x5, #0xffffffff
 178:	lsr	x3, x4, x0
 17c:	lsr	x0, x8, x0
 180:	lsl	x4, x4, x1
 184:	orr	x4, x0, x4
 188:	lsr	x0, x5, #32
 18c:	lsl	x9, x8, x1
 190:	udiv	x6, x3, x0
 194:	msub	x3, x6, x0, x3
 198:	mul	x1, x2, x6
 19c:	extr	x3, x3, x4, #32
 1a0:	cmp	x1, x3
 1a4:	b.ls	20c <__udivti3+0x20c>  // b.plast
 1a8:	sub	x7, x6, #0x1
 1ac:	adds	x3, x5, x3
 1b0:	b.cs	1c4 <__udivti3+0x1c4>  // b.hs, b.nlast
 1b4:	cmp	x1, x3
 1b8:	b.ls	1c4 <__udivti3+0x1c4>  // b.plast
 1bc:	sub	x7, x6, #0x2
 1c0:	add	x3, x3, x5
 1c4:	sub	x3, x3, x1
 1c8:	and	x4, x4, #0xffffffff
 1cc:	udiv	x6, x3, x0
 1d0:	msub	x0, x6, x0, x3
 1d4:	mul	x2, x2, x6
 1d8:	orr	x4, x4, x0, lsl #32
 1dc:	cmp	x2, x4
 1e0:	b.ls	214 <__udivti3+0x214>  // b.plast
 1e4:	sub	x1, x6, #0x1
 1e8:	adds	x4, x5, x4
 1ec:	b.cs	200 <__udivti3+0x200>  // b.hs, b.nlast
 1f0:	cmp	x2, x4
 1f4:	b.ls	200 <__udivti3+0x200>  // b.plast
 1f8:	sub	x1, x6, #0x2
 1fc:	add	x4, x4, x5
 200:	sub	x4, x4, x2
 204:	orr	x1, x1, x7, lsl #32
 208:	b	ec <__udivti3+0xec>
 20c:	mov	x7, x6
 210:	b	1c4 <__udivti3+0x1c4>
 214:	mov	x1, x6
 218:	b	200 <__udivti3+0x200>
 21c:	mov	x0, x3
 220:	b	128 <__udivti3+0x128>
 224:	mov	x6, x3
 228:	b	160 <__udivti3+0x160>
 22c:	cmp	x3, x4
 230:	b.hi	364 <__udivti3+0x364>  // b.pmore
 234:	clz	x9, x3
 238:	cbnz	x9, 24c <__udivti3+0x24c>
 23c:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 240:	cset	x0, ls  // ls = plast
 244:	mov	x1, #0x0                   	// #0
 248:	b	bc <__udivti3+0xbc>
 24c:	mov	x5, #0x40                  	// #64
 250:	sub	x5, x5, x9
 254:	lsl	x3, x3, x9
 258:	lsr	x7, x2, x5
 25c:	orr	x7, x7, x3
 260:	lsr	x6, x4, x5
 264:	lsr	x5, x0, x5
 268:	and	x0, x7, #0xffffffff
 26c:	lsl	x4, x4, x9
 270:	orr	x4, x5, x4
 274:	lsr	x5, x7, #32
 278:	lsl	x10, x2, x9
 27c:	udiv	x1, x6, x5
 280:	msub	x6, x1, x5, x6
 284:	mul	x2, x0, x1
 288:	extr	x6, x6, x4, #32
 28c:	cmp	x2, x6
 290:	b.ls	354 <__udivti3+0x354>  // b.plast
 294:	sub	x3, x1, #0x1
 298:	adds	x6, x7, x6
 29c:	b.cs	2b0 <__udivti3+0x2b0>  // b.hs, b.nlast
 2a0:	cmp	x2, x6
 2a4:	b.ls	2b0 <__udivti3+0x2b0>  // b.plast
 2a8:	sub	x3, x1, #0x2
 2ac:	add	x6, x6, x7
 2b0:	sub	x6, x6, x2
 2b4:	and	x4, x4, #0xffffffff
 2b8:	udiv	x2, x6, x5
 2bc:	msub	x5, x2, x5, x6
 2c0:	mul	x0, x0, x2
 2c4:	orr	x4, x4, x5, lsl #32
 2c8:	cmp	x0, x4
 2cc:	b.ls	35c <__udivti3+0x35c>  // b.plast
 2d0:	sub	x1, x2, #0x1
 2d4:	adds	x4, x7, x4
 2d8:	b.cs	2ec <__udivti3+0x2ec>  // b.hs, b.nlast
 2dc:	cmp	x0, x4
 2e0:	b.ls	2ec <__udivti3+0x2ec>  // b.plast
 2e4:	sub	x1, x2, #0x2
 2e8:	add	x4, x4, x7
 2ec:	sub	x4, x4, x0
 2f0:	orr	x0, x1, x3, lsl #32
 2f4:	mov	w1, w1
 2f8:	and	x3, x10, #0xffffffff
 2fc:	lsr	x2, x0, #32
 300:	lsr	x5, x10, #32
 304:	mul	x6, x1, x3
 308:	mul	x3, x2, x3
 30c:	madd	x1, x1, x5, x3
 310:	mul	x2, x2, x5
 314:	add	x1, x1, x6, lsr #32
 318:	cmp	x3, x1
 31c:	b.ls	328 <__udivti3+0x328>  // b.plast
 320:	mov	x3, #0x100000000           	// #4294967296
 324:	add	x2, x2, x3
 328:	add	x2, x2, x1, lsr #32
 32c:	cmp	x4, x2
 330:	b.cc	34c <__udivti3+0x34c>  // b.lo, b.ul, b.last
 334:	and	x6, x6, #0xffffffff
 338:	lsl	x8, x8, x9
 33c:	add	x1, x6, x1, lsl #32
 340:	cmp	x8, x1
 344:	ccmp	x4, x2, #0x0, cc  // cc = lo, ul, last
 348:	b.ne	244 <__udivti3+0x244>  // b.any
 34c:	sub	x0, x0, #0x1
 350:	b	244 <__udivti3+0x244>
 354:	mov	x3, x1
 358:	b	2b0 <__udivti3+0x2b0>
 35c:	mov	x1, x2
 360:	b	2ec <__udivti3+0x2ec>
 364:	mov	x1, #0x0                   	// #0
 368:	mov	x0, #0x0                   	// #0
 36c:	b	bc <__udivti3+0xbc>

_umoddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	mov	x6, x2
   4:	mov	x5, x0
   8:	mov	x4, x1
   c:	cbnz	x3, 1d0 <__umodti3+0x1d0>
  10:	cmp	x2, x1
  14:	b.ls	b4 <__umodti3+0xb4>  // b.plast
  18:	clz	x0, x2
  1c:	mov	x8, x0
  20:	cbz	x0, 3c <__umodti3+0x3c>
  24:	lsl	x4, x1, x0
  28:	neg	w1, w0
  2c:	lsl	x6, x2, x0
  30:	lsr	x1, x5, x1
  34:	orr	x4, x1, x4
  38:	lsl	x5, x5, x0
  3c:	lsr	x1, x6, #32
  40:	and	x0, x6, #0xffffffff
  44:	udiv	x2, x4, x1
  48:	msub	x4, x2, x1, x4
  4c:	mul	x2, x0, x2
  50:	extr	x4, x4, x5, #32
  54:	cmp	x2, x4
  58:	b.ls	70 <__umodti3+0x70>  // b.plast
  5c:	adds	x4, x6, x4
  60:	b.cs	70 <__umodti3+0x70>  // b.hs, b.nlast
  64:	cmp	x2, x4
  68:	b.ls	70 <__umodti3+0x70>  // b.plast
  6c:	add	x4, x4, x6
  70:	sub	x4, x4, x2
  74:	and	x5, x5, #0xffffffff
  78:	udiv	x2, x4, x1
  7c:	msub	x4, x2, x1, x4
  80:	mul	x0, x0, x2
  84:	orr	x4, x5, x4, lsl #32
  88:	cmp	x0, x4
  8c:	b.ls	a4 <__umodti3+0xa4>  // b.plast
  90:	adds	x4, x6, x4
  94:	b.cs	a4 <__umodti3+0xa4>  // b.hs, b.nlast
  98:	cmp	x0, x4
  9c:	b.ls	a4 <__umodti3+0xa4>  // b.plast
  a0:	add	x4, x4, x6
  a4:	sub	x5, x4, x0
  a8:	lsr	x0, x5, x8
  ac:	mov	x1, #0x0                   	// #0
  b0:	ret
  b4:	cbnz	x2, c0 <__umodti3+0xc0>
  b8:	mov	x2, #0x1                   	// #1
  bc:	udiv	x6, x2, x6
  c0:	clz	x3, x6
  c4:	mov	x8, x3
  c8:	cbnz	x3, 140 <__umodti3+0x140>
  cc:	sub	x1, x1, x6
  d0:	lsr	x2, x6, #32
  d4:	and	x0, x6, #0xffffffff
  d8:	udiv	x3, x1, x2
  dc:	msub	x1, x3, x2, x1
  e0:	mul	x3, x0, x3
  e4:	extr	x1, x1, x5, #32
  e8:	cmp	x3, x1
  ec:	b.ls	104 <__umodti3+0x104>  // b.plast
  f0:	adds	x1, x6, x1
  f4:	b.cs	104 <__umodti3+0x104>  // b.hs, b.nlast
  f8:	cmp	x3, x1
  fc:	b.ls	104 <__umodti3+0x104>  // b.plast
 100:	add	x1, x1, x6
 104:	sub	x1, x1, x3
 108:	and	x5, x5, #0xffffffff
 10c:	udiv	x4, x1, x2
 110:	msub	x1, x4, x2, x1
 114:	mul	x0, x0, x4
 118:	orr	x5, x5, x1, lsl #32
 11c:	cmp	x0, x5
 120:	b.ls	138 <__umodti3+0x138>  // b.plast
 124:	adds	x5, x6, x5
 128:	b.cs	138 <__umodti3+0x138>  // b.hs, b.nlast
 12c:	cmp	x0, x5
 130:	b.ls	138 <__umodti3+0x138>  // b.plast
 134:	add	x5, x5, x6
 138:	sub	x5, x5, x0
 13c:	b	a8 <__umodti3+0xa8>
 140:	mov	x2, #0x40                  	// #64
 144:	sub	x2, x2, x3
 148:	lsl	x6, x6, x3
 14c:	and	x4, x6, #0xffffffff
 150:	lsr	x7, x1, x2
 154:	lsr	x2, x0, x2
 158:	lsl	x1, x1, x3
 15c:	orr	x1, x2, x1
 160:	lsr	x2, x6, #32
 164:	lsl	x5, x0, x3
 168:	udiv	x3, x7, x2
 16c:	msub	x7, x3, x2, x7
 170:	mul	x0, x4, x3
 174:	extr	x3, x7, x1, #32
 178:	cmp	x0, x3
 17c:	b.ls	194 <__umodti3+0x194>  // b.plast
 180:	adds	x3, x6, x3
 184:	b.cs	194 <__umodti3+0x194>  // b.hs, b.nlast
 188:	cmp	x0, x3
 18c:	b.ls	194 <__umodti3+0x194>  // b.plast
 190:	add	x3, x3, x6
 194:	sub	x3, x3, x0
 198:	and	x1, x1, #0xffffffff
 19c:	udiv	x0, x3, x2
 1a0:	msub	x2, x0, x2, x3
 1a4:	mul	x4, x4, x0
 1a8:	orr	x1, x1, x2, lsl #32
 1ac:	cmp	x4, x1
 1b0:	b.ls	1c8 <__umodti3+0x1c8>  // b.plast
 1b4:	adds	x1, x6, x1
 1b8:	b.cs	1c8 <__umodti3+0x1c8>  // b.hs, b.nlast
 1bc:	cmp	x4, x1
 1c0:	b.ls	1c8 <__umodti3+0x1c8>  // b.plast
 1c4:	add	x1, x1, x6
 1c8:	sub	x1, x1, x4
 1cc:	b	d0 <__umodti3+0xd0>
 1d0:	cmp	x3, x1
 1d4:	b.hi	b0 <__umodti3+0xb0>  // b.pmore
 1d8:	clz	x10, x3
 1dc:	cbnz	x10, 200 <__umodti3+0x200>
 1e0:	cmp	x3, x1
 1e4:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 1e8:	b.hi	1f4 <__umodti3+0x1f4>  // b.pmore
 1ec:	subs	x5, x0, x2
 1f0:	sbc	x4, x1, x3
 1f4:	mov	x0, x5
 1f8:	mov	x1, x4
 1fc:	b	b0 <__umodti3+0xb0>
 200:	mov	x6, #0x40                  	// #64
 204:	sub	x6, x6, x10
 208:	lsl	x3, x3, x10
 20c:	lsr	x9, x2, x6
 210:	orr	x9, x9, x3
 214:	lsr	x7, x1, x6
 218:	and	x3, x9, #0xffffffff
 21c:	lsr	x5, x9, #32
 220:	lsl	x1, x1, x10
 224:	lsr	x4, x0, x6
 228:	orr	x4, x4, x1
 22c:	lsl	x2, x2, x10
 230:	udiv	x1, x7, x5
 234:	lsl	x0, x0, x10
 238:	msub	x7, x1, x5, x7
 23c:	mul	x11, x3, x1
 240:	extr	x7, x7, x4, #32
 244:	cmp	x11, x7
 248:	b.ls	328 <__umodti3+0x328>  // b.plast
 24c:	sub	x8, x1, #0x1
 250:	adds	x7, x9, x7
 254:	b.cs	268 <__umodti3+0x268>  // b.hs, b.nlast
 258:	cmp	x11, x7
 25c:	b.ls	268 <__umodti3+0x268>  // b.plast
 260:	sub	x8, x1, #0x2
 264:	add	x7, x7, x9
 268:	sub	x7, x7, x11
 26c:	udiv	x11, x7, x5
 270:	msub	x5, x11, x5, x7
 274:	mul	x1, x3, x11
 278:	and	x3, x4, #0xffffffff
 27c:	orr	x3, x3, x5, lsl #32
 280:	cmp	x1, x3
 284:	b.ls	330 <__umodti3+0x330>  // b.plast
 288:	sub	x7, x11, #0x1
 28c:	adds	x3, x9, x3
 290:	b.cs	2a4 <__umodti3+0x2a4>  // b.hs, b.nlast
 294:	cmp	x1, x3
 298:	b.ls	2a4 <__umodti3+0x2a4>  // b.plast
 29c:	sub	x7, x11, #0x2
 2a0:	add	x3, x3, x9
 2a4:	sub	x3, x3, x1
 2a8:	orr	x1, x7, x8, lsl #32
 2ac:	and	x4, x2, #0xffffffff
 2b0:	mov	w7, w7
 2b4:	lsr	x1, x1, #32
 2b8:	lsr	x8, x2, #32
 2bc:	mul	x5, x7, x4
 2c0:	mul	x4, x1, x4
 2c4:	madd	x7, x7, x8, x4
 2c8:	mul	x1, x1, x8
 2cc:	add	x7, x7, x5, lsr #32
 2d0:	cmp	x4, x7
 2d4:	b.ls	2e0 <__umodti3+0x2e0>  // b.plast
 2d8:	mov	x4, #0x100000000           	// #4294967296
 2dc:	add	x1, x1, x4
 2e0:	add	x1, x1, x7, lsr #32
 2e4:	and	x5, x5, #0xffffffff
 2e8:	add	x7, x5, x7, lsl #32
 2ec:	cmp	x3, x1
 2f0:	b.cc	2fc <__umodti3+0x2fc>  // b.lo, b.ul, b.last
 2f4:	ccmp	x0, x7, #0x2, eq  // eq = none
 2f8:	b.cs	308 <__umodti3+0x308>  // b.hs, b.nlast
 2fc:	subs	x7, x7, x2
 300:	cinc	x9, x9, cc  // cc = lo, ul, last
 304:	sub	x1, x1, x9
 308:	subs	x4, x0, x7
 30c:	cmp	x0, x7
 310:	sbc	x1, x3, x1
 314:	lsr	x0, x4, x10
 318:	lsl	x5, x1, x6
 31c:	orr	x0, x5, x0
 320:	lsr	x1, x1, x10
 324:	b	b0 <__umodti3+0xb0>
 328:	mov	x8, x1
 32c:	b	268 <__umodti3+0x268>
 330:	mov	x7, x11
 334:	b	2a4 <__umodti3+0x2a4>

_udivmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	mov	x5, x1
   4:	mov	x6, x2
   8:	mov	x1, x3
   c:	mov	x8, x0
  10:	mov	x7, x5
  14:	cbnz	x3, 210 <__udivmodti4+0x210>
  18:	cmp	x2, x5
  1c:	b.ls	dc <__udivmodti4+0xdc>  // b.plast
  20:	clz	x0, x2
  24:	mov	x3, x0
  28:	cbz	x0, 44 <__udivmodti4+0x44>
  2c:	lsl	x6, x2, x0
  30:	neg	w2, w0
  34:	lsl	x7, x5, x0
  38:	lsr	x2, x8, x2
  3c:	orr	x7, x2, x7
  40:	lsl	x8, x8, x0
  44:	lsr	x2, x6, #32
  48:	and	x5, x6, #0xffffffff
  4c:	udiv	x9, x7, x2
  50:	msub	x7, x9, x2, x7
  54:	mul	x0, x5, x9
  58:	extr	x7, x7, x8, #32
  5c:	cmp	x0, x7
  60:	b.ls	d4 <__udivmodti4+0xd4>  // b.plast
  64:	sub	x10, x9, #0x1
  68:	adds	x7, x6, x7
  6c:	b.cs	80 <__udivmodti4+0x80>  // b.hs, b.nlast
  70:	cmp	x0, x7
  74:	b.ls	80 <__udivmodti4+0x80>  // b.plast
  78:	sub	x10, x9, #0x2
  7c:	add	x7, x7, x6
  80:	sub	x7, x7, x0
  84:	udiv	x9, x7, x2
  88:	msub	x2, x9, x2, x7
  8c:	mul	x7, x5, x9
  90:	and	x5, x8, #0xffffffff
  94:	orr	x5, x5, x2, lsl #32
  98:	cmp	x7, x5
  9c:	b.ls	208 <__udivmodti4+0x208>  // b.plast
  a0:	sub	x0, x9, #0x1
  a4:	adds	x5, x6, x5
  a8:	b.cs	bc <__udivmodti4+0xbc>  // b.hs, b.nlast
  ac:	cmp	x7, x5
  b0:	b.ls	bc <__udivmodti4+0xbc>  // b.plast
  b4:	sub	x0, x9, #0x2
  b8:	add	x5, x5, x6
  bc:	sub	x5, x5, x7
  c0:	orr	x0, x0, x10, lsl #32
  c4:	cbz	x4, d0 <__udivmodti4+0xd0>
  c8:	lsr	x5, x5, x3
  cc:	stp	x5, xzr, [x4]
  d0:	ret
  d4:	mov	x10, x9
  d8:	b	80 <__udivmodti4+0x80>
  dc:	cbnz	x2, e8 <__udivmodti4+0xe8>
  e0:	mov	x1, #0x1                   	// #1
  e4:	udiv	x6, x1, x2
  e8:	clz	x7, x6
  ec:	mov	x3, x7
  f0:	cbnz	x7, 14c <__udivmodti4+0x14c>
  f4:	sub	x5, x5, x6
  f8:	mov	x1, #0x1                   	// #1
  fc:	lsr	x2, x6, #32
 100:	and	x7, x6, #0xffffffff
 104:	udiv	x9, x5, x2
 108:	msub	x5, x9, x2, x5
 10c:	mul	x0, x7, x9
 110:	extr	x5, x5, x8, #32
 114:	cmp	x0, x5
 118:	b.ls	200 <__udivmodti4+0x200>  // b.plast
 11c:	sub	x10, x9, #0x1
 120:	adds	x5, x6, x5
 124:	b.cs	138 <__udivmodti4+0x138>  // b.hs, b.nlast
 128:	cmp	x0, x5
 12c:	b.ls	138 <__udivmodti4+0x138>  // b.plast
 130:	sub	x10, x9, #0x2
 134:	add	x5, x5, x6
 138:	sub	x5, x5, x0
 13c:	udiv	x9, x5, x2
 140:	msub	x2, x9, x2, x5
 144:	mul	x7, x7, x9
 148:	b	90 <__udivmodti4+0x90>
 14c:	mov	x1, #0x40                  	// #64
 150:	sub	x1, x1, x7
 154:	lsl	x6, x6, x7
 158:	lsr	x2, x5, x1
 15c:	lsl	x8, x0, x7
 160:	lsr	x1, x0, x1
 164:	lsr	x0, x6, #32
 168:	lsl	x5, x5, x7
 16c:	and	x7, x6, #0xffffffff
 170:	orr	x5, x1, x5
 174:	udiv	x9, x2, x0
 178:	msub	x2, x9, x0, x2
 17c:	mul	x1, x7, x9
 180:	extr	x2, x2, x5, #32
 184:	cmp	x1, x2
 188:	b.ls	1f0 <__udivmodti4+0x1f0>  // b.plast
 18c:	sub	x10, x9, #0x1
 190:	adds	x2, x6, x2
 194:	b.cs	1a8 <__udivmodti4+0x1a8>  // b.hs, b.nlast
 198:	cmp	x1, x2
 19c:	b.ls	1a8 <__udivmodti4+0x1a8>  // b.plast
 1a0:	sub	x10, x9, #0x2
 1a4:	add	x2, x2, x6
 1a8:	sub	x2, x2, x1
 1ac:	and	x5, x5, #0xffffffff
 1b0:	udiv	x9, x2, x0
 1b4:	msub	x0, x9, x0, x2
 1b8:	mul	x2, x7, x9
 1bc:	orr	x5, x5, x0, lsl #32
 1c0:	cmp	x2, x5
 1c4:	b.ls	1f8 <__udivmodti4+0x1f8>  // b.plast
 1c8:	sub	x1, x9, #0x1
 1cc:	adds	x5, x6, x5
 1d0:	b.cs	1e4 <__udivmodti4+0x1e4>  // b.hs, b.nlast
 1d4:	cmp	x2, x5
 1d8:	b.ls	1e4 <__udivmodti4+0x1e4>  // b.plast
 1dc:	sub	x1, x9, #0x2
 1e0:	add	x5, x5, x6
 1e4:	sub	x5, x5, x2
 1e8:	orr	x1, x1, x10, lsl #32
 1ec:	b	fc <__udivmodti4+0xfc>
 1f0:	mov	x10, x9
 1f4:	b	1a8 <__udivmodti4+0x1a8>
 1f8:	mov	x1, x9
 1fc:	b	1e4 <__udivmodti4+0x1e4>
 200:	mov	x10, x9
 204:	b	138 <__udivmodti4+0x138>
 208:	mov	x0, x9
 20c:	b	bc <__udivmodti4+0xbc>
 210:	cmp	x3, x5
 214:	b.ls	22c <__udivmodti4+0x22c>  // b.plast
 218:	cbz	x4, 220 <__udivmodti4+0x220>
 21c:	stp	x0, x5, [x4]
 220:	mov	x1, #0x0                   	// #0
 224:	mov	x0, #0x0                   	// #0
 228:	b	d0 <__udivmodti4+0xd0>
 22c:	clz	x9, x3
 230:	cbnz	x9, 264 <__udivmodti4+0x264>
 234:	cmp	x3, x5
 238:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 23c:	b.hi	25c <__udivmodti4+0x25c>  // b.pmore
 240:	subs	x8, x0, x2
 244:	mov	x0, #0x1                   	// #1
 248:	sbc	x7, x5, x3
 24c:	cbz	x4, 254 <__udivmodti4+0x254>
 250:	stp	x8, x7, [x4]
 254:	mov	x1, #0x0                   	// #0
 258:	b	d0 <__udivmodti4+0xd0>
 25c:	mov	x0, #0x0                   	// #0
 260:	b	24c <__udivmodti4+0x24c>
 264:	mov	x1, #0x40                  	// #64
 268:	sub	x10, x1, x9
 26c:	lsl	x3, x3, x9
 270:	lsr	x7, x2, x10
 274:	orr	x7, x7, x3
 278:	lsr	x6, x5, x10
 27c:	lsr	x3, x7, #32
 280:	lsl	x5, x5, x9
 284:	lsr	x1, x0, x10
 288:	orr	x1, x1, x5
 28c:	and	x5, x7, #0xffffffff
 290:	lsl	x2, x2, x9
 294:	udiv	x11, x6, x3
 298:	lsl	x8, x0, x9
 29c:	msub	x6, x11, x3, x6
 2a0:	mul	x12, x5, x11
 2a4:	extr	x6, x6, x1, #32
 2a8:	cmp	x12, x6
 2ac:	b.ls	398 <__udivmodti4+0x398>  // b.plast
 2b0:	sub	x0, x11, #0x1
 2b4:	adds	x6, x7, x6
 2b8:	b.cs	2cc <__udivmodti4+0x2cc>  // b.hs, b.nlast
 2bc:	cmp	x12, x6
 2c0:	b.ls	2cc <__udivmodti4+0x2cc>  // b.plast
 2c4:	sub	x0, x11, #0x2
 2c8:	add	x6, x6, x7
 2cc:	sub	x6, x6, x12
 2d0:	udiv	x11, x6, x3
 2d4:	msub	x3, x11, x3, x6
 2d8:	mul	x6, x5, x11
 2dc:	and	x5, x1, #0xffffffff
 2e0:	orr	x5, x5, x3, lsl #32
 2e4:	cmp	x6, x5
 2e8:	b.ls	3a0 <__udivmodti4+0x3a0>  // b.plast
 2ec:	sub	x1, x11, #0x1
 2f0:	adds	x5, x7, x5
 2f4:	b.cs	308 <__udivmodti4+0x308>  // b.hs, b.nlast
 2f8:	cmp	x6, x5
 2fc:	b.ls	308 <__udivmodti4+0x308>  // b.plast
 300:	sub	x1, x11, #0x2
 304:	add	x5, x5, x7
 308:	orr	x0, x1, x0, lsl #32
 30c:	sub	x5, x5, x6
 310:	mov	w1, w1
 314:	and	x6, x2, #0xffffffff
 318:	lsr	x3, x0, #32
 31c:	lsr	x12, x2, #32
 320:	mul	x11, x1, x6
 324:	mul	x6, x3, x6
 328:	madd	x1, x1, x12, x6
 32c:	mul	x3, x3, x12
 330:	add	x1, x1, x11, lsr #32
 334:	cmp	x6, x1
 338:	b.ls	344 <__udivmodti4+0x344>  // b.plast
 33c:	mov	x6, #0x100000000           	// #4294967296
 340:	add	x3, x3, x6
 344:	add	x3, x3, x1, lsr #32
 348:	and	x11, x11, #0xffffffff
 34c:	add	x1, x11, x1, lsl #32
 350:	cmp	x5, x3
 354:	b.cc	360 <__udivmodti4+0x360>  // b.lo, b.ul, b.last
 358:	ccmp	x8, x1, #0x2, eq  // eq = none
 35c:	b.cs	370 <__udivmodti4+0x370>  // b.hs, b.nlast
 360:	subs	x1, x1, x2
 364:	sub	x0, x0, #0x1
 368:	cinc	x7, x7, cc  // cc = lo, ul, last
 36c:	sub	x3, x3, x7
 370:	cbz	x4, 254 <__udivmodti4+0x254>
 374:	subs	x2, x8, x1
 378:	cmp	x8, x1
 37c:	sbc	x5, x5, x3
 380:	lsr	x2, x2, x9
 384:	lsl	x1, x5, x10
 388:	orr	x1, x1, x2
 38c:	lsr	x5, x5, x9
 390:	stp	x1, x5, [x4]
 394:	b	254 <__udivmodti4+0x254>
 398:	mov	x0, x11
 39c:	b	2cc <__udivmodti4+0x2cc>
 3a0:	mov	x1, x11
 3a4:	b	308 <__udivmodti4+0x308>

_udiv_w_sdiv.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udiv_w_sdiv>:
   0:	mov	x0, #0x0                   	// #0
   4:	ret

sync-cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__aarch64_sync_cache_range>:
   0:	adrp	x2, 0 <__aarch64_sync_cache_range>
   4:	ldr	w3, [x2]
   8:	cbnz	w3, 14 <__aarch64_sync_cache_range+0x14>
   c:	mrs	x3, ctr_el0
  10:	str	w3, [x2]
  14:	ldr	w4, [x2]
  18:	mov	w2, #0x4                   	// #4
  1c:	and	w3, w4, #0xf
  20:	ubfx	x4, x4, #16, #4
  24:	lsl	w3, w2, w3
  28:	lsl	w2, w2, w4
  2c:	sub	w4, w2, #0x1
  30:	bic	x4, x0, x4
  34:	sxtw	x2, w2
  38:	cmp	x4, x1
  3c:	b.cc	64 <__aarch64_sync_cache_range+0x64>  // b.lo, b.ul, b.last
  40:	dsb	ish
  44:	sub	w2, w3, #0x1
  48:	sxtw	x3, w3
  4c:	bic	x0, x0, x2
  50:	cmp	x0, x1
  54:	b.cc	70 <__aarch64_sync_cache_range+0x70>  // b.lo, b.ul, b.last
  58:	dsb	ish
  5c:	isb
  60:	ret
  64:	dc	cvau, x4
  68:	add	x4, x4, x2
  6c:	b	38 <__aarch64_sync_cache_range+0x38>
  70:	ic	ivau, x0
  74:	add	x0, x0, x3
  78:	b	50 <__aarch64_sync_cache_range+0x50>

sfp-exceptions.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__sfp_handle_exceptions>:
   0:	tbz	w0, #0, 10 <__sfp_handle_exceptions+0x10>
   4:	movi	v1.2s, #0x0
   8:	fdiv	s0, s1, s1
   c:	mrs	x1, fpsr
  10:	tbz	w0, #1, 24 <__sfp_handle_exceptions+0x24>
  14:	fmov	s1, #1.000000000000000000e+00
  18:	movi	v2.2s, #0x0
  1c:	fdiv	s0, s1, s2
  20:	mrs	x1, fpsr
  24:	tbz	w0, #2, 44 <__sfp_handle_exceptions+0x44>
  28:	mov	w1, #0x7f7fffff            	// #2139095039
  2c:	fmov	s1, w1
  30:	mov	w1, #0xc5ae                	// #50606
  34:	movk	w1, #0x749d, lsl #16
  38:	fmov	s2, w1
  3c:	fadd	s0, s1, s2
  40:	mrs	x1, fpsr
  44:	tbz	w0, #3, 54 <__sfp_handle_exceptions+0x54>
  48:	movi	v1.2s, #0x80, lsl #16
  4c:	fmul	s0, s1, s1
  50:	mrs	x1, fpsr
  54:	tbz	w0, #4, 6c <__sfp_handle_exceptions+0x6c>
  58:	mov	w0, #0x7f7fffff            	// #2139095039
  5c:	fmov	s2, #1.000000000000000000e+00
  60:	fmov	s1, w0
  64:	fsub	s0, s1, s2
  68:	mrs	x0, fpsr
  6c:	ret

addtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x9, x5, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x1, x2, [sp, #16]
  18:	mrs	x10, fpcr
  1c:	ubfx	x0, x5, #48, #15
  20:	ubfx	x12, x2, #48, #15
  24:	ubfiz	x3, x5, #3, #48
  28:	lsr	x11, x2, #63
  2c:	mov	x4, x0
  30:	ubfiz	x2, x2, #3, #48
  34:	sub	w0, w0, w12
  38:	lsr	x8, x5, #63
  3c:	mov	x14, x1
  40:	orr	x2, x2, x1, lsr #61
  44:	cmp	x11, x5, lsr #63
  48:	orr	x3, x3, x9, lsr #61
  4c:	lsl	x7, x9, #3
  50:	mov	x6, x12
  54:	lsl	x1, x1, #3
  58:	mov	w5, w0
  5c:	b.ne	464 <__addtf3+0x464>  // b.any
  60:	cmp	w0, #0x0
  64:	b.le	1a4 <__addtf3+0x1a4>
  68:	mov	x9, #0x7fff                	// #32767
  6c:	cbnz	x12, 10c <__addtf3+0x10c>
  70:	orr	x5, x2, x1
  74:	cbnz	x5, a0 <__addtf3+0xa0>
  78:	cmp	x4, x9
  7c:	b.ne	850 <__addtf3+0x850>  // b.any
  80:	orr	x1, x3, x7
  84:	cbz	x1, 8d4 <__addtf3+0x8d4>
  88:	lsr	x0, x3, #50
  8c:	mov	x2, x3
  90:	eor	x0, x0, #0x1
  94:	mov	x1, x7
  98:	and	w0, w0, #0x1
  9c:	b	2e8 <__addtf3+0x2e8>
  a0:	subs	w5, w0, #0x1
  a4:	b.ne	e4 <__addtf3+0xe4>  // b.any
  a8:	adds	x1, x7, x1
  ac:	mov	x7, x1
  b0:	adc	x3, x3, x2
  b4:	tbz	x3, #51, 850 <__addtf3+0x850>
  b8:	add	x4, x4, #0x1
  bc:	mov	x0, #0x7fff                	// #32767
  c0:	cmp	x4, x0
  c4:	b.eq	428 <__addtf3+0x428>  // b.none
  c8:	and	x2, x3, #0xfff7ffffffffffff
  cc:	and	x1, x7, #0x1
  d0:	orr	x1, x1, x7, lsr #1
  d4:	orr	x1, x1, x3, lsl #63
  d8:	lsr	x2, x2, #1
  dc:	mov	w0, #0x0                   	// #0
  e0:	b	1d8 <__addtf3+0x1d8>
  e4:	cmp	x4, x9
  e8:	b.ne	118 <__addtf3+0x118>  // b.any
  ec:	orr	x1, x3, x7
  f0:	cbz	x1, 8d4 <__addtf3+0x8d4>
  f4:	lsr	x0, x3, #50
  f8:	mov	x2, x3
  fc:	eor	x0, x0, #0x1
 100:	mov	x1, x7
 104:	and	w0, w0, #0x1
 108:	b	1d8 <__addtf3+0x1d8>
 10c:	cmp	x4, x9
 110:	b.eq	ec <__addtf3+0xec>  // b.none
 114:	orr	x2, x2, #0x8000000000000
 118:	cmp	w5, #0x74
 11c:	b.gt	194 <__addtf3+0x194>
 120:	cmp	w5, #0x3f
 124:	b.gt	160 <__addtf3+0x160>
 128:	mov	w6, #0x40                  	// #64
 12c:	sub	w6, w6, w5
 130:	lsr	x9, x1, x5
 134:	lsl	x1, x1, x6
 138:	cmp	x1, #0x0
 13c:	lsl	x0, x2, x6
 140:	cset	x1, ne  // ne = any
 144:	orr	x0, x0, x9
 148:	lsr	x2, x2, x5
 14c:	orr	x0, x0, x1
 150:	adds	x1, x0, x7
 154:	mov	x7, x1
 158:	adc	x3, x2, x3
 15c:	b	b4 <__addtf3+0xb4>
 160:	sub	w0, w5, #0x40
 164:	mov	w6, #0x80                  	// #128
 168:	sub	w6, w6, w5
 16c:	cmp	w5, #0x40
 170:	lsr	x0, x2, x0
 174:	lsl	x2, x2, x6
 178:	csel	x2, x2, xzr, ne  // ne = any
 17c:	orr	x1, x2, x1
 180:	cmp	x1, #0x0
 184:	cset	x1, ne  // ne = any
 188:	orr	x0, x0, x1
 18c:	mov	x2, #0x0                   	// #0
 190:	b	150 <__addtf3+0x150>
 194:	orr	x1, x2, x1
 198:	cmp	x1, #0x0
 19c:	cset	x0, ne  // ne = any
 1a0:	b	18c <__addtf3+0x18c>
 1a4:	b.eq	2a8 <__addtf3+0x2a8>  // b.none
 1a8:	mov	x5, #0x7fff                	// #32767
 1ac:	cbnz	x4, 250 <__addtf3+0x250>
 1b0:	orr	x4, x3, x7
 1b4:	cbnz	x4, 1e0 <__addtf3+0x1e0>
 1b8:	cmp	x12, x5
 1bc:	b.ne	860 <__addtf3+0x860>  // b.any
 1c0:	orr	x0, x2, x1
 1c4:	cbz	x0, 8a8 <__addtf3+0x8a8>
 1c8:	lsr	x0, x2, #50
 1cc:	mov	x4, x6
 1d0:	eor	x0, x0, #0x1
 1d4:	and	w0, w0, #0x1
 1d8:	mov	w5, #0x0                   	// #0
 1dc:	b	2e8 <__addtf3+0x2e8>
 1e0:	cmn	w0, #0x1
 1e4:	b.ne	1fc <__addtf3+0x1fc>  // b.any
 1e8:	adds	x1, x7, x1
 1ec:	mov	x7, x1
 1f0:	adc	x3, x3, x2
 1f4:	mov	x4, x6
 1f8:	b	b4 <__addtf3+0xb4>
 1fc:	cmp	x12, x5
 200:	b.eq	1c0 <__addtf3+0x1c0>  // b.none
 204:	mvn	w0, w0
 208:	cmp	w0, #0x74
 20c:	b.gt	298 <__addtf3+0x298>
 210:	cmp	w0, #0x3f
 214:	b.gt	264 <__addtf3+0x264>
 218:	mov	w5, #0x40                  	// #64
 21c:	sub	w5, w5, w0
 220:	lsr	x9, x7, x0
 224:	lsl	x7, x7, x5
 228:	cmp	x7, #0x0
 22c:	cset	x7, ne  // ne = any
 230:	lsl	x4, x3, x5
 234:	orr	x4, x4, x9
 238:	lsr	x0, x3, x0
 23c:	orr	x7, x4, x7
 240:	adds	x1, x7, x1
 244:	mov	x7, x1
 248:	adc	x3, x0, x2
 24c:	b	1f4 <__addtf3+0x1f4>
 250:	cmp	x12, x5
 254:	b.eq	1c0 <__addtf3+0x1c0>  // b.none
 258:	neg	w0, w0
 25c:	orr	x3, x3, #0x8000000000000
 260:	b	208 <__addtf3+0x208>
 264:	sub	w4, w0, #0x40
 268:	mov	w5, #0x80                  	// #128
 26c:	sub	w5, w5, w0
 270:	cmp	w0, #0x40
 274:	lsr	x4, x3, x4
 278:	lsl	x3, x3, x5
 27c:	csel	x3, x3, xzr, ne  // ne = any
 280:	orr	x3, x3, x7
 284:	cmp	x3, #0x0
 288:	cset	x7, ne  // ne = any
 28c:	orr	x7, x4, x7
 290:	mov	x0, #0x0                   	// #0
 294:	b	240 <__addtf3+0x240>
 298:	orr	x3, x3, x7
 29c:	cmp	x3, #0x0
 2a0:	cset	x7, ne  // ne = any
 2a4:	b	290 <__addtf3+0x290>
 2a8:	add	x0, x4, #0x1
 2ac:	mov	x13, #0x7fff                	// #32767
 2b0:	tst	x0, #0x7ffe
 2b4:	b.ne	3c4 <__addtf3+0x3c4>  // b.any
 2b8:	orr	x12, x3, x7
 2bc:	cbnz	x4, 328 <__addtf3+0x328>
 2c0:	cbz	x12, 884 <__addtf3+0x884>
 2c4:	orr	x0, x2, x1
 2c8:	cbz	x0, 670 <__addtf3+0x670>
 2cc:	adds	x1, x7, x1
 2d0:	mov	x7, x1
 2d4:	adc	x3, x3, x2
 2d8:	tbz	x3, #51, 670 <__addtf3+0x670>
 2dc:	and	x2, x3, #0xfff7ffffffffffff
 2e0:	mov	w0, #0x0                   	// #0
 2e4:	mov	x4, #0x1                   	// #1
 2e8:	tst	x1, #0x7
 2ec:	b.eq	904 <__addtf3+0x904>  // b.none
 2f0:	and	x3, x10, #0xc00000
 2f4:	orr	w0, w0, #0x10
 2f8:	cmp	x3, #0x400, lsl #12
 2fc:	b.eq	8f0 <__addtf3+0x8f0>  // b.none
 300:	cmp	x3, #0x800, lsl #12
 304:	b.eq	8fc <__addtf3+0x8fc>  // b.none
 308:	cbnz	x3, 320 <__addtf3+0x320>
 30c:	and	x3, x1, #0xf
 310:	cmp	x3, #0x4
 314:	b.eq	320 <__addtf3+0x320>  // b.none
 318:	adds	x1, x1, #0x4
 31c:	cinc	x2, x2, cs  // cs = hs, nlast
 320:	cbz	w5, 914 <__addtf3+0x914>
 324:	b	910 <__addtf3+0x910>
 328:	cmp	x4, x13
 32c:	b.ne	390 <__addtf3+0x390>  // b.any
 330:	cbz	x12, 9f4 <__addtf3+0x9f4>
 334:	lsr	x0, x3, #50
 338:	cmp	x6, x4
 33c:	eor	x0, x0, #0x1
 340:	and	w0, w0, #0x1
 344:	b.ne	3b0 <__addtf3+0x3b0>  // b.any
 348:	orr	x4, x2, x1
 34c:	cbz	x4, 9ec <__addtf3+0x9ec>
 350:	tst	x2, #0x4000000000000
 354:	csinc	w0, w0, wzr, ne  // ne = any
 358:	cbz	x12, 3a8 <__addtf3+0x3a8>
 35c:	and	x9, x9, #0x1fffffffffffffff
 360:	lsr	x1, x3, #3
 364:	orr	x9, x9, x3, lsl #61
 368:	tbz	x3, #50, 384 <__addtf3+0x384>
 36c:	lsr	x3, x2, #3
 370:	tbnz	x2, #50, 384 <__addtf3+0x384>
 374:	and	x1, x14, #0x1fffffffffffffff
 378:	mov	x8, x11
 37c:	orr	x9, x1, x2, lsl #61
 380:	mov	x1, x3
 384:	extr	x2, x1, x9, #61
 388:	lsl	x1, x9, #3
 38c:	b	3a8 <__addtf3+0x3a8>
 390:	cmp	x6, x13
 394:	b.ne	3a0 <__addtf3+0x3a0>  // b.any
 398:	mov	w0, #0x0                   	// #0
 39c:	b	348 <__addtf3+0x348>
 3a0:	mov	w0, #0x0                   	// #0
 3a4:	cbnz	x12, 3b0 <__addtf3+0x3b0>
 3a8:	mov	x4, #0x7fff                	// #32767
 3ac:	b	2e8 <__addtf3+0x2e8>
 3b0:	orr	x1, x2, x1
 3b4:	cbnz	x1, 35c <__addtf3+0x35c>
 3b8:	mov	x2, x3
 3bc:	mov	x1, x7
 3c0:	b	3a8 <__addtf3+0x3a8>
 3c4:	cmp	x0, x13
 3c8:	b.eq	3e8 <__addtf3+0x3e8>  // b.none
 3cc:	adds	x1, x7, x1
 3d0:	mov	x4, x0
 3d4:	adc	x2, x3, x2
 3d8:	extr	x1, x2, x1, #1
 3dc:	lsr	x2, x2, #1
 3e0:	mov	w0, #0x0                   	// #0
 3e4:	b	2e8 <__addtf3+0x2e8>
 3e8:	ands	x1, x10, #0xc00000
 3ec:	b.eq	8b8 <__addtf3+0x8b8>  // b.none
 3f0:	cmp	x1, #0x400, lsl #12
 3f4:	b.ne	40c <__addtf3+0x40c>  // b.any
 3f8:	cbnz	x8, 418 <__addtf3+0x418>
 3fc:	mov	x4, x0
 400:	mov	x2, #0x0                   	// #0
 404:	mov	x1, #0x0                   	// #0
 408:	b	8c0 <__addtf3+0x8c0>
 40c:	cmp	x1, #0x800, lsl #12
 410:	b.ne	418 <__addtf3+0x418>  // b.any
 414:	cbnz	x8, 3fc <__addtf3+0x3fc>
 418:	mov	x2, #0xffffffffffffffff    	// #-1
 41c:	mov	x4, #0x7ffe                	// #32766
 420:	mov	x1, x2
 424:	b	8c0 <__addtf3+0x8c0>
 428:	ands	x1, x10, #0xc00000
 42c:	b.eq	8c8 <__addtf3+0x8c8>  // b.none
 430:	cmp	x1, #0x400, lsl #12
 434:	b.ne	448 <__addtf3+0x448>  // b.any
 438:	cbnz	x8, 454 <__addtf3+0x454>
 43c:	mov	x2, #0x0                   	// #0
 440:	mov	x1, #0x0                   	// #0
 444:	b	8cc <__addtf3+0x8cc>
 448:	cmp	x1, #0x800, lsl #12
 44c:	b.ne	454 <__addtf3+0x454>  // b.any
 450:	cbnz	x8, 43c <__addtf3+0x43c>
 454:	mov	x2, #0xffffffffffffffff    	// #-1
 458:	mov	x4, #0x7ffe                	// #32766
 45c:	mov	x1, x2
 460:	b	8cc <__addtf3+0x8cc>
 464:	cmp	w0, #0x0
 468:	b.le	53c <__addtf3+0x53c>
 46c:	mov	x9, #0x7fff                	// #32767
 470:	cbnz	x12, 4e8 <__addtf3+0x4e8>
 474:	orr	x5, x2, x1
 478:	cbz	x5, 78 <__addtf3+0x78>
 47c:	subs	w5, w0, #0x1
 480:	b.ne	49c <__addtf3+0x49c>  // b.any
 484:	subs	x7, x7, x1
 488:	sbc	x3, x3, x2
 48c:	tbz	x3, #51, 850 <__addtf3+0x850>
 490:	and	x5, x3, #0x7ffffffffffff
 494:	mov	x6, x7
 498:	b	780 <__addtf3+0x780>
 49c:	cmp	x4, x9
 4a0:	b.eq	ec <__addtf3+0xec>  // b.none
 4a4:	cmp	w5, #0x74
 4a8:	b.gt	52c <__addtf3+0x52c>
 4ac:	cmp	w5, #0x3f
 4b0:	b.gt	4f8 <__addtf3+0x4f8>
 4b4:	mov	w6, #0x40                  	// #64
 4b8:	sub	w6, w6, w5
 4bc:	lsr	x9, x1, x5
 4c0:	lsl	x1, x1, x6
 4c4:	cmp	x1, #0x0
 4c8:	lsl	x0, x2, x6
 4cc:	cset	x1, ne  // ne = any
 4d0:	orr	x0, x0, x9
 4d4:	lsr	x2, x2, x5
 4d8:	orr	x0, x0, x1
 4dc:	subs	x7, x7, x0
 4e0:	sbc	x3, x3, x2
 4e4:	b	48c <__addtf3+0x48c>
 4e8:	cmp	x4, x9
 4ec:	b.eq	ec <__addtf3+0xec>  // b.none
 4f0:	orr	x2, x2, #0x8000000000000
 4f4:	b	4a4 <__addtf3+0x4a4>
 4f8:	sub	w0, w5, #0x40
 4fc:	mov	w6, #0x80                  	// #128
 500:	sub	w6, w6, w5
 504:	cmp	w5, #0x40
 508:	lsr	x0, x2, x0
 50c:	lsl	x2, x2, x6
 510:	csel	x2, x2, xzr, ne  // ne = any
 514:	orr	x1, x2, x1
 518:	cmp	x1, #0x0
 51c:	cset	x1, ne  // ne = any
 520:	orr	x0, x0, x1
 524:	mov	x2, #0x0                   	// #0
 528:	b	4dc <__addtf3+0x4dc>
 52c:	orr	x1, x2, x1
 530:	cmp	x1, #0x0
 534:	cset	x0, ne  // ne = any
 538:	b	524 <__addtf3+0x524>
 53c:	b.eq	63c <__addtf3+0x63c>  // b.none
 540:	mov	x5, #0x7fff                	// #32767
 544:	cbnz	x4, 5e4 <__addtf3+0x5e4>
 548:	orr	x4, x3, x7
 54c:	cbnz	x4, 578 <__addtf3+0x578>
 550:	cmp	x12, x5
 554:	b.ne	870 <__addtf3+0x870>  // b.any
 558:	orr	x0, x2, x1
 55c:	cbz	x0, 8dc <__addtf3+0x8dc>
 560:	lsr	x0, x2, #50
 564:	mov	x4, x6
 568:	eor	x0, x0, #0x1
 56c:	mov	x8, x11
 570:	and	w0, w0, #0x1
 574:	b	1d8 <__addtf3+0x1d8>
 578:	cmn	w0, #0x1
 57c:	b.ne	594 <__addtf3+0x594>  // b.any
 580:	subs	x7, x1, x7
 584:	sbc	x3, x2, x3
 588:	mov	x4, x6
 58c:	mov	x8, x11
 590:	b	48c <__addtf3+0x48c>
 594:	cmp	x12, x5
 598:	b.eq	558 <__addtf3+0x558>  // b.none
 59c:	mvn	w0, w0
 5a0:	cmp	w0, #0x74
 5a4:	b.gt	62c <__addtf3+0x62c>
 5a8:	cmp	w0, #0x3f
 5ac:	b.gt	5f8 <__addtf3+0x5f8>
 5b0:	mov	w5, #0x40                  	// #64
 5b4:	sub	w5, w5, w0
 5b8:	lsr	x8, x7, x0
 5bc:	lsl	x7, x7, x5
 5c0:	cmp	x7, #0x0
 5c4:	lsl	x4, x3, x5
 5c8:	cset	x5, ne  // ne = any
 5cc:	orr	x4, x4, x8
 5d0:	lsr	x0, x3, x0
 5d4:	orr	x4, x4, x5
 5d8:	subs	x7, x1, x4
 5dc:	sbc	x3, x2, x0
 5e0:	b	588 <__addtf3+0x588>
 5e4:	cmp	x12, x5
 5e8:	b.eq	558 <__addtf3+0x558>  // b.none
 5ec:	neg	w0, w0
 5f0:	orr	x3, x3, #0x8000000000000
 5f4:	b	5a0 <__addtf3+0x5a0>
 5f8:	sub	w4, w0, #0x40
 5fc:	mov	w5, #0x80                  	// #128
 600:	sub	w5, w5, w0
 604:	cmp	w0, #0x40
 608:	lsr	x4, x3, x4
 60c:	lsl	x3, x3, x5
 610:	csel	x3, x3, xzr, ne  // ne = any
 614:	orr	x3, x3, x7
 618:	cmp	x3, #0x0
 61c:	cset	x0, ne  // ne = any
 620:	orr	x4, x4, x0
 624:	mov	x0, #0x0                   	// #0
 628:	b	5d8 <__addtf3+0x5d8>
 62c:	orr	x3, x3, x7
 630:	cmp	x3, #0x0
 634:	cset	x4, ne  // ne = any
 638:	b	624 <__addtf3+0x624>
 63c:	add	x0, x4, #0x1
 640:	tst	x0, #0x7ffe
 644:	b.ne	760 <__addtf3+0x760>  // b.any
 648:	orr	x12, x3, x7
 64c:	orr	x13, x2, x1
 650:	cbnz	x4, 6cc <__addtf3+0x6cc>
 654:	cbnz	x12, 68c <__addtf3+0x68c>
 658:	cbnz	x13, 890 <__addtf3+0x890>
 65c:	and	x0, x10, #0xc00000
 660:	mov	x3, #0x0                   	// #0
 664:	cmp	x0, #0x800, lsl #12
 668:	mov	x7, #0x0                   	// #0
 66c:	cset	x8, eq  // eq = none
 670:	orr	x0, x7, x3
 674:	mov	x2, x3
 678:	cmp	x0, #0x0
 67c:	mov	x1, x7
 680:	cset	w5, ne  // ne = any
 684:	mov	x4, #0x0                   	// #0
 688:	b	3e0 <__addtf3+0x3e0>
 68c:	cbz	x13, 670 <__addtf3+0x670>
 690:	subs	x4, x7, x1
 694:	cmp	x7, x1
 698:	sbc	x0, x3, x2
 69c:	tbz	x0, #51, 6b0 <__addtf3+0x6b0>
 6a0:	subs	x7, x1, x7
 6a4:	sbc	x3, x2, x3
 6a8:	mov	x8, x11
 6ac:	b	670 <__addtf3+0x670>
 6b0:	orr	x7, x4, x0
 6b4:	cbnz	x7, 89c <__addtf3+0x89c>
 6b8:	and	x0, x10, #0xc00000
 6bc:	cmp	x0, #0x800, lsl #12
 6c0:	cset	x8, eq  // eq = none
 6c4:	mov	x3, #0x0                   	// #0
 6c8:	b	670 <__addtf3+0x670>
 6cc:	mov	x0, #0x7fff                	// #32767
 6d0:	cmp	x4, x0
 6d4:	b.ne	734 <__addtf3+0x734>  // b.any
 6d8:	cbz	x12, 9dc <__addtf3+0x9dc>
 6dc:	lsr	x0, x3, #50
 6e0:	cmp	x6, x4
 6e4:	eor	x0, x0, #0x1
 6e8:	and	w0, w0, #0x1
 6ec:	b.ne	758 <__addtf3+0x758>  // b.any
 6f0:	cbz	x13, a04 <__addtf3+0xa04>
 6f4:	tst	x2, #0x4000000000000
 6f8:	csinc	w0, w0, wzr, ne  // ne = any
 6fc:	cbz	x12, 74c <__addtf3+0x74c>
 700:	and	x9, x9, #0x1fffffffffffffff
 704:	lsr	x4, x3, #3
 708:	orr	x1, x9, x3, lsl #61
 70c:	tbz	x3, #50, 728 <__addtf3+0x728>
 710:	lsr	x3, x2, #3
 714:	tbnz	x2, #50, 728 <__addtf3+0x728>
 718:	and	x1, x14, #0x1fffffffffffffff
 71c:	mov	x4, x3
 720:	orr	x1, x1, x2, lsl #61
 724:	mov	x8, x11
 728:	extr	x2, x4, x1, #61
 72c:	lsl	x1, x1, #3
 730:	b	3a8 <__addtf3+0x3a8>
 734:	cmp	x6, x0
 738:	b.ne	744 <__addtf3+0x744>  // b.any
 73c:	mov	w0, #0x0                   	// #0
 740:	b	6f0 <__addtf3+0x6f0>
 744:	mov	w0, #0x0                   	// #0
 748:	cbnz	x12, 758 <__addtf3+0x758>
 74c:	cbz	x13, a08 <__addtf3+0xa08>
 750:	mov	x8, x11
 754:	b	3a8 <__addtf3+0x3a8>
 758:	cbnz	x13, 700 <__addtf3+0x700>
 75c:	b	3b8 <__addtf3+0x3b8>
 760:	subs	x0, x7, x1
 764:	cmp	x7, x1
 768:	mov	x6, x0
 76c:	sbc	x5, x3, x2
 770:	tbz	x5, #51, 7fc <__addtf3+0x7fc>
 774:	subs	x6, x1, x7
 778:	mov	x8, x11
 77c:	sbc	x5, x2, x3
 780:	clz	x0, x6
 784:	cmp	x5, #0x0
 788:	add	w0, w0, #0x40
 78c:	clz	x3, x5
 790:	csel	w3, w0, w3, eq  // eq = none
 794:	sub	w0, w3, #0xc
 798:	cmp	w0, #0x3f
 79c:	b.gt	808 <__addtf3+0x808>
 7a0:	neg	w3, w0
 7a4:	lsl	x5, x5, x0
 7a8:	lsl	x7, x6, x0
 7ac:	lsr	x3, x6, x3
 7b0:	orr	x3, x3, x5
 7b4:	sxtw	x1, w0
 7b8:	cmp	x4, w0, sxtw
 7bc:	b.gt	848 <__addtf3+0x848>
 7c0:	sub	w4, w0, w4
 7c4:	add	w2, w4, #0x1
 7c8:	cmp	w2, #0x3f
 7cc:	b.gt	818 <__addtf3+0x818>
 7d0:	mov	w1, #0x40                  	// #64
 7d4:	sub	w1, w1, w2
 7d8:	lsr	x4, x7, x2
 7dc:	lsl	x0, x3, x1
 7e0:	orr	x0, x0, x4
 7e4:	lsl	x1, x7, x1
 7e8:	cmp	x1, #0x0
 7ec:	cset	x1, ne  // ne = any
 7f0:	lsr	x3, x3, x2
 7f4:	orr	x7, x0, x1
 7f8:	b	670 <__addtf3+0x670>
 7fc:	orr	x7, x0, x5
 800:	cbnz	x7, 780 <__addtf3+0x780>
 804:	b	6b8 <__addtf3+0x6b8>
 808:	sub	w3, w3, #0x4c
 80c:	mov	x7, #0x0                   	// #0
 810:	lsl	x3, x6, x3
 814:	b	7b4 <__addtf3+0x7b4>
 818:	sub	w4, w4, #0x3f
 81c:	mov	w0, #0x80                  	// #128
 820:	sub	w0, w0, w2
 824:	cmp	w2, #0x40
 828:	lsr	x4, x3, x4
 82c:	lsl	x3, x3, x0
 830:	csel	x3, x3, xzr, ne  // ne = any
 834:	orr	x3, x7, x3
 838:	cmp	x3, #0x0
 83c:	cset	x3, ne  // ne = any
 840:	orr	x7, x4, x3
 844:	b	6c4 <__addtf3+0x6c4>
 848:	sub	x4, x4, x1
 84c:	and	x3, x3, #0xfff7ffffffffffff
 850:	cbz	x4, 670 <__addtf3+0x670>
 854:	mov	x2, x3
 858:	mov	x1, x7
 85c:	b	dc <__addtf3+0xdc>
 860:	mov	x3, x2
 864:	mov	x7, x1
 868:	mov	x4, x12
 86c:	b	850 <__addtf3+0x850>
 870:	mov	x3, x2
 874:	mov	x7, x1
 878:	mov	x4, x12
 87c:	mov	x8, x11
 880:	b	850 <__addtf3+0x850>
 884:	mov	x3, x2
 888:	mov	x7, x1
 88c:	b	670 <__addtf3+0x670>
 890:	mov	x3, x2
 894:	mov	x7, x1
 898:	b	6a8 <__addtf3+0x6a8>
 89c:	mov	x3, x0
 8a0:	mov	x7, x4
 8a4:	b	670 <__addtf3+0x670>
 8a8:	mov	x4, x6
 8ac:	mov	x2, #0x0                   	// #0
 8b0:	mov	x1, #0x0                   	// #0
 8b4:	b	dc <__addtf3+0xdc>
 8b8:	mov	x4, x0
 8bc:	mov	x2, #0x0                   	// #0
 8c0:	mov	w0, #0x14                  	// #20
 8c4:	b	2e8 <__addtf3+0x2e8>
 8c8:	mov	x2, #0x0                   	// #0
 8cc:	mov	w0, #0x14                  	// #20
 8d0:	b	1d8 <__addtf3+0x1d8>
 8d4:	mov	x2, #0x0                   	// #0
 8d8:	b	dc <__addtf3+0xdc>
 8dc:	mov	x4, x6
 8e0:	mov	x8, x11
 8e4:	mov	x2, #0x0                   	// #0
 8e8:	mov	x1, #0x0                   	// #0
 8ec:	b	dc <__addtf3+0xdc>
 8f0:	cbnz	x8, 320 <__addtf3+0x320>
 8f4:	adds	x1, x1, #0x8
 8f8:	b	31c <__addtf3+0x31c>
 8fc:	cbz	x8, 320 <__addtf3+0x320>
 900:	b	8f4 <__addtf3+0x8f4>
 904:	cbz	w5, 914 <__addtf3+0x914>
 908:	tbnz	w0, #4, 910 <__addtf3+0x910>
 90c:	tbz	w10, #11, 914 <__addtf3+0x914>
 910:	orr	w0, w0, #0x8
 914:	tbz	x2, #51, 92c <__addtf3+0x92c>
 918:	add	x4, x4, #0x1
 91c:	mov	x3, #0x7fff                	// #32767
 920:	cmp	x4, x3
 924:	b.eq	984 <__addtf3+0x984>  // b.none
 928:	and	x2, x2, #0xfff7ffffffffffff
 92c:	mov	x3, #0x7fff                	// #32767
 930:	extr	x1, x2, x1, #3
 934:	cmp	x4, x3
 938:	lsr	x2, x2, #3
 93c:	b.ne	950 <__addtf3+0x950>  // b.any
 940:	orr	x3, x1, x2
 944:	orr	x2, x2, #0x800000000000
 948:	cmp	x3, #0x0
 94c:	csel	x2, x2, xzr, ne  // ne = any
 950:	and	x4, x4, #0x7fff
 954:	mov	x7, #0x0                   	// #0
 958:	bfxil	x7, x2, #0, #48
 95c:	orr	w8, w4, w8, lsl #15
 960:	fmov	d0, x1
 964:	bfi	x7, x8, #48, #16
 968:	fmov	v0.d[1], x7
 96c:	cbz	w0, 97c <__addtf3+0x97c>
 970:	str	q0, [sp, #16]
 974:	bl	0 <__sfp_handle_exceptions>
 978:	ldr	q0, [sp, #16]
 97c:	ldp	x29, x30, [sp], #32
 980:	ret
 984:	ands	x1, x10, #0xc00000
 988:	b.eq	9a4 <__addtf3+0x9a4>  // b.none
 98c:	cmp	x1, #0x400, lsl #12
 990:	b.ne	9b4 <__addtf3+0x9b4>  // b.any
 994:	cmp	x8, #0x0
 998:	mov	x2, #0x7ffe                	// #32766
 99c:	csetm	x1, ne  // ne = any
 9a0:	csel	x4, x4, x2, eq  // eq = none
 9a4:	mov	w2, #0x14                  	// #20
 9a8:	orr	w0, w0, w2
 9ac:	mov	x2, x1
 9b0:	b	92c <__addtf3+0x92c>
 9b4:	cmp	x1, #0x800, lsl #12
 9b8:	b.ne	9d0 <__addtf3+0x9d0>  // b.any
 9bc:	cmp	x8, #0x0
 9c0:	mov	x2, #0x7ffe                	// #32766
 9c4:	csetm	x1, eq  // eq = none
 9c8:	csel	x4, x4, x2, ne  // ne = any
 9cc:	b	9a4 <__addtf3+0x9a4>
 9d0:	mov	x1, #0xffffffffffffffff    	// #-1
 9d4:	mov	x4, #0x7ffe                	// #32766
 9d8:	b	9a4 <__addtf3+0x9a4>
 9dc:	cmp	x6, x4
 9e0:	b.eq	73c <__addtf3+0x73c>  // b.none
 9e4:	mov	w0, #0x0                   	// #0
 9e8:	b	74c <__addtf3+0x74c>
 9ec:	cbz	x12, 3a8 <__addtf3+0x3a8>
 9f0:	b	3b8 <__addtf3+0x3b8>
 9f4:	cmp	x6, x4
 9f8:	b.eq	398 <__addtf3+0x398>  // b.none
 9fc:	mov	w0, #0x0                   	// #0
 a00:	b	3a8 <__addtf3+0x3a8>
 a04:	cbnz	x12, 3b8 <__addtf3+0x3b8>
 a08:	mov	x8, #0x0                   	// #0
 a0c:	mov	x2, #0x7ffffffffffff       	// #2251799813685247
 a10:	mov	x1, #0xfffffffffffffff8    	// #-8
 a14:	mov	x4, #0x7fff                	// #32767
 a18:	mov	w0, #0x1                   	// #1
 a1c:	b	2e8 <__addtf3+0x2e8>

divtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x1, x10, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x8, x9, [sp, #16]
  18:	mrs	x6, fpcr
  1c:	ubfx	x4, x10, #0, #48
  20:	ubfx	x11, x10, #48, #15
  24:	lsr	x10, x10, #63
  28:	and	w5, w10, #0xff
  2c:	cbz	w11, 60 <__divtf3+0x60>
  30:	mov	w2, #0x7fff                	// #32767
  34:	cmp	w11, w2
  38:	b.eq	c0 <__divtf3+0xc0>  // b.none
  3c:	and	x11, x11, #0xffff
  40:	extr	x2, x4, x1, #61
  44:	mov	x12, #0xffffffffffffc001    	// #-16383
  48:	orr	x2, x2, #0x8000000000000
  4c:	lsl	x1, x1, #3
  50:	add	x11, x11, x12
  54:	mov	x14, #0x0                   	// #0
  58:	mov	w0, #0x0                   	// #0
  5c:	b	dc <__divtf3+0xdc>
  60:	orr	x2, x4, x1
  64:	cbz	x2, 154 <__divtf3+0x154>
  68:	clz	x2, x1
  6c:	cmp	x4, #0x0
  70:	add	x2, x2, #0x40
  74:	clz	x11, x4
  78:	csel	x0, x2, x11, eq  // eq = none
  7c:	sub	x2, x0, #0xf
  80:	cmp	x2, #0x3c
  84:	b.gt	b0 <__divtf3+0xb0>
  88:	add	w7, w2, #0x3
  8c:	mov	w3, #0x3d                  	// #61
  90:	sub	w2, w3, w2
  94:	lsl	x4, x4, x7
  98:	lsr	x2, x1, x2
  9c:	orr	x2, x2, x4
  a0:	lsl	x1, x1, x7
  a4:	mov	x11, #0xffffffffffffc011    	// #-16367
  a8:	sub	x11, x11, x0
  ac:	b	54 <__divtf3+0x54>
  b0:	sub	w2, w2, #0x3d
  b4:	lsl	x2, x1, x2
  b8:	mov	x1, #0x0                   	// #0
  bc:	b	a4 <__divtf3+0xa4>
  c0:	orr	x2, x4, x1
  c4:	cbz	x2, 164 <__divtf3+0x164>
  c8:	lsr	x0, x4, #47
  cc:	mov	x2, x4
  d0:	eor	w0, w0, #0x1
  d4:	mov	x11, #0x7fff                	// #32767
  d8:	mov	x14, #0x3                   	// #3
  dc:	lsr	x13, x9, #63
  e0:	ubfx	x7, x9, #0, #48
  e4:	ubfx	x4, x9, #48, #15
  e8:	and	w3, w13, #0xff
  ec:	cbz	w4, 174 <__divtf3+0x174>
  f0:	mov	w9, #0x7fff                	// #32767
  f4:	cmp	w4, w9
  f8:	b.eq	1d4 <__divtf3+0x1d4>  // b.none
  fc:	and	x4, x4, #0xffff
 100:	extr	x7, x7, x8, #61
 104:	mov	x9, #0xffffffffffffc001    	// #-16383
 108:	orr	x7, x7, #0x8000000000000
 10c:	lsl	x8, x8, #3
 110:	add	x4, x4, x9
 114:	mov	x12, #0x0                   	// #0
 118:	eor	w5, w5, w3
 11c:	orr	x3, x12, x14, lsl #2
 120:	sub	x3, x3, #0x1
 124:	and	x5, x5, #0xff
 128:	sub	x9, x11, x4
 12c:	cmp	x3, #0xe
 130:	b.hi	218 <__divtf3+0x218>  // b.pmore
 134:	cmp	w3, #0xe
 138:	b.hi	218 <__divtf3+0x218>  // b.pmore
 13c:	adrp	x4, 0 <__divtf3>
 140:	add	x4, x4, #0x0
 144:	ldrh	w3, [x4, w3, uxtw #1]
 148:	adr	x4, 154 <__divtf3+0x154>
 14c:	add	x3, x4, w3, sxth #2
 150:	br	x3
 154:	mov	x1, #0x0                   	// #0
 158:	mov	x11, #0x0                   	// #0
 15c:	mov	x14, #0x1                   	// #1
 160:	b	58 <__divtf3+0x58>
 164:	mov	x1, #0x0                   	// #0
 168:	mov	x11, #0x7fff                	// #32767
 16c:	mov	x14, #0x2                   	// #2
 170:	b	58 <__divtf3+0x58>
 174:	orr	x4, x7, x8
 178:	cbz	x4, 1f0 <__divtf3+0x1f0>
 17c:	clz	x12, x8
 180:	cmp	x7, #0x0
 184:	add	x12, x12, #0x40
 188:	clz	x9, x7
 18c:	csel	x9, x12, x9, eq  // eq = none
 190:	sub	x12, x9, #0xf
 194:	cmp	x12, #0x3c
 198:	b.gt	1c4 <__divtf3+0x1c4>
 19c:	add	w16, w12, #0x3
 1a0:	mov	w15, #0x3d                  	// #61
 1a4:	sub	w12, w15, w12
 1a8:	lsl	x7, x7, x16
 1ac:	lsr	x12, x8, x12
 1b0:	orr	x7, x12, x7
 1b4:	lsl	x8, x8, x16
 1b8:	mov	x4, #0xffffffffffffc011    	// #-16367
 1bc:	sub	x4, x4, x9
 1c0:	b	114 <__divtf3+0x114>
 1c4:	sub	w7, w12, #0x3d
 1c8:	lsl	x7, x8, x7
 1cc:	mov	x8, #0x0                   	// #0
 1d0:	b	1b8 <__divtf3+0x1b8>
 1d4:	orr	x4, x7, x8
 1d8:	cbz	x4, 204 <__divtf3+0x204>
 1dc:	tst	x7, #0x800000000000
 1e0:	mov	x4, #0x7fff                	// #32767
 1e4:	csinc	w0, w0, wzr, ne  // ne = any
 1e8:	mov	x12, #0x3                   	// #3
 1ec:	b	118 <__divtf3+0x118>
 1f0:	mov	x7, #0x0                   	// #0
 1f4:	mov	x8, #0x0                   	// #0
 1f8:	mov	x4, #0x0                   	// #0
 1fc:	mov	x12, #0x1                   	// #1
 200:	b	118 <__divtf3+0x118>
 204:	mov	x7, #0x0                   	// #0
 208:	mov	x8, #0x0                   	// #0
 20c:	mov	x4, #0x7fff                	// #32767
 210:	mov	x12, #0x2                   	// #2
 214:	b	118 <__divtf3+0x118>
 218:	cmp	x7, x2
 21c:	b.cc	228 <__divtf3+0x228>  // b.lo, b.ul, b.last
 220:	ccmp	x8, x1, #0x2, eq  // eq = none
 224:	b.hi	4c0 <__divtf3+0x4c0>  // b.pmore
 228:	lsl	x14, x1, #63
 22c:	extr	x1, x2, x1, #1
 230:	lsr	x2, x2, #1
 234:	extr	x3, x7, x8, #52
 238:	ubfx	x7, x7, #20, #32
 23c:	and	x13, x3, #0xffffffff
 240:	lsl	x8, x8, #12
 244:	udiv	x10, x2, x7
 248:	msub	x2, x10, x7, x2
 24c:	mul	x11, x13, x10
 250:	extr	x2, x2, x1, #32
 254:	cmp	x11, x2
 258:	b.ls	4cc <__divtf3+0x4cc>  // b.plast
 25c:	sub	x4, x10, #0x1
 260:	adds	x2, x3, x2
 264:	b.cs	278 <__divtf3+0x278>  // b.hs, b.nlast
 268:	cmp	x11, x2
 26c:	b.ls	278 <__divtf3+0x278>  // b.plast
 270:	sub	x4, x10, #0x2
 274:	add	x2, x2, x3
 278:	sub	x2, x2, x11
 27c:	and	x1, x1, #0xffffffff
 280:	udiv	x10, x2, x7
 284:	msub	x2, x10, x7, x2
 288:	mul	x11, x13, x10
 28c:	orr	x1, x1, x2, lsl #32
 290:	cmp	x11, x1
 294:	b.ls	4d4 <__divtf3+0x4d4>  // b.plast
 298:	sub	x2, x10, #0x1
 29c:	adds	x1, x3, x1
 2a0:	b.cs	2b4 <__divtf3+0x2b4>  // b.hs, b.nlast
 2a4:	cmp	x11, x1
 2a8:	b.ls	2b4 <__divtf3+0x2b4>  // b.plast
 2ac:	sub	x2, x10, #0x2
 2b0:	add	x1, x1, x3
 2b4:	orr	x4, x2, x4, lsl #32
 2b8:	and	x15, x8, #0xffffffff
 2bc:	lsr	x10, x8, #32
 2c0:	sub	x1, x1, x11
 2c4:	lsr	x12, x4, #32
 2c8:	and	x11, x4, #0xffffffff
 2cc:	mul	x16, x12, x15
 2d0:	mul	x2, x11, x15
 2d4:	madd	x11, x10, x11, x16
 2d8:	mul	x12, x12, x10
 2dc:	add	x11, x11, x2, lsr #32
 2e0:	cmp	x16, x11
 2e4:	b.ls	2f0 <__divtf3+0x2f0>  // b.plast
 2e8:	mov	x16, #0x100000000           	// #4294967296
 2ec:	add	x12, x12, x16
 2f0:	add	x12, x12, x11, lsr #32
 2f4:	and	x2, x2, #0xffffffff
 2f8:	add	x11, x2, x11, lsl #32
 2fc:	cmp	x1, x12
 300:	b.cc	30c <__divtf3+0x30c>  // b.lo, b.ul, b.last
 304:	ccmp	x14, x11, #0x2, eq  // eq = none
 308:	b.cs	4dc <__divtf3+0x4dc>  // b.hs, b.nlast
 30c:	adds	x16, x14, x8
 310:	sub	x2, x4, #0x1
 314:	adc	x1, x1, x3
 318:	cset	x17, cs  // cs = hs, nlast
 31c:	mov	x14, x16
 320:	cmp	x3, x1
 324:	b.cc	334 <__divtf3+0x334>  // b.lo, b.ul, b.last
 328:	cmp	x17, #0x0
 32c:	ccmp	x3, x1, #0x0, eq  // eq = none
 330:	b.ne	350 <__divtf3+0x350>  // b.any
 334:	cmp	x12, x1
 338:	b.hi	344 <__divtf3+0x344>  // b.pmore
 33c:	ccmp	x11, x16, #0x0, eq  // eq = none
 340:	b.ls	350 <__divtf3+0x350>  // b.plast
 344:	adds	x14, x8, x16
 348:	sub	x2, x4, #0x2
 34c:	adc	x1, x1, x3
 350:	subs	x16, x14, x11
 354:	cmp	x14, x11
 358:	sbc	x1, x1, x12
 35c:	cmp	x3, x1
 360:	b.eq	570 <__divtf3+0x570>  // b.none
 364:	udiv	x12, x1, x7
 368:	msub	x1, x12, x7, x1
 36c:	mul	x4, x13, x12
 370:	extr	x1, x1, x16, #32
 374:	cmp	x4, x1
 378:	b.ls	4e4 <__divtf3+0x4e4>  // b.plast
 37c:	sub	x11, x12, #0x1
 380:	adds	x1, x3, x1
 384:	b.cs	398 <__divtf3+0x398>  // b.hs, b.nlast
 388:	cmp	x4, x1
 38c:	b.ls	398 <__divtf3+0x398>  // b.plast
 390:	sub	x11, x12, #0x2
 394:	add	x1, x1, x3
 398:	sub	x1, x1, x4
 39c:	and	x16, x16, #0xffffffff
 3a0:	udiv	x12, x1, x7
 3a4:	msub	x4, x12, x7, x1
 3a8:	mul	x13, x13, x12
 3ac:	orr	x4, x16, x4, lsl #32
 3b0:	cmp	x13, x4
 3b4:	b.ls	4ec <__divtf3+0x4ec>  // b.plast
 3b8:	sub	x1, x12, #0x1
 3bc:	adds	x4, x3, x4
 3c0:	b.cs	3d4 <__divtf3+0x3d4>  // b.hs, b.nlast
 3c4:	cmp	x13, x4
 3c8:	b.ls	3d4 <__divtf3+0x3d4>  // b.plast
 3cc:	sub	x1, x12, #0x2
 3d0:	add	x4, x4, x3
 3d4:	orr	x11, x1, x11, lsl #32
 3d8:	sub	x4, x4, x13
 3dc:	and	x12, x11, #0xffffffff
 3e0:	lsr	x7, x11, #32
 3e4:	mul	x1, x15, x12
 3e8:	mul	x15, x7, x15
 3ec:	mul	x7, x10, x7
 3f0:	madd	x10, x10, x12, x15
 3f4:	add	x10, x10, x1, lsr #32
 3f8:	cmp	x15, x10
 3fc:	b.ls	408 <__divtf3+0x408>  // b.plast
 400:	mov	x12, #0x100000000           	// #4294967296
 404:	add	x7, x7, x12
 408:	add	x7, x7, x10, lsr #32
 40c:	and	x1, x1, #0xffffffff
 410:	add	x10, x1, x10, lsl #32
 414:	cmp	x4, x7
 418:	b.cc	428 <__divtf3+0x428>  // b.lo, b.ul, b.last
 41c:	cmp	x10, #0x0
 420:	ccmp	x4, x7, #0x0, ne  // ne = any
 424:	b.ne	4f4 <__divtf3+0x4f4>  // b.any
 428:	adds	x12, x3, x4
 42c:	sub	x1, x11, #0x1
 430:	mov	x4, x12
 434:	b.cs	460 <__divtf3+0x460>  // b.hs, b.nlast
 438:	cmp	x12, x7
 43c:	b.cc	448 <__divtf3+0x448>  // b.lo, b.ul, b.last
 440:	ccmp	x8, x10, #0x2, eq  // eq = none
 444:	b.cs	460 <__divtf3+0x460>  // b.hs, b.nlast
 448:	sub	x1, x11, #0x2
 44c:	lsl	x11, x8, #1
 450:	cmp	x8, x11
 454:	mov	x8, x11
 458:	cinc	x4, x3, hi  // hi = pmore
 45c:	add	x4, x12, x4
 460:	cmp	x4, x7
 464:	mov	x3, x1
 468:	ccmp	x8, x10, #0x0, eq  // eq = none
 46c:	orr	x1, x1, #0x1
 470:	csel	x1, x1, x3, ne  // ne = any
 474:	mov	x3, #0x3fff                	// #16383
 478:	add	x3, x9, x3
 47c:	cmp	x3, #0x0
 480:	b.le	63c <__divtf3+0x63c>
 484:	tst	x1, #0x7
 488:	b.eq	588 <__divtf3+0x588>  // b.none
 48c:	and	x4, x6, #0xc00000
 490:	orr	w0, w0, #0x10
 494:	cmp	x4, #0x400, lsl #12
 498:	b.eq	578 <__divtf3+0x578>  // b.none
 49c:	cmp	x4, #0x800, lsl #12
 4a0:	b.eq	584 <__divtf3+0x584>  // b.none
 4a4:	cbnz	x4, 588 <__divtf3+0x588>
 4a8:	and	x4, x1, #0xf
 4ac:	cmp	x4, #0x4
 4b0:	b.eq	588 <__divtf3+0x588>  // b.none
 4b4:	adds	x1, x1, #0x4
 4b8:	cinc	x2, x2, cs  // cs = hs, nlast
 4bc:	b	588 <__divtf3+0x588>
 4c0:	sub	x9, x9, #0x1
 4c4:	mov	x14, #0x0                   	// #0
 4c8:	b	234 <__divtf3+0x234>
 4cc:	mov	x4, x10
 4d0:	b	278 <__divtf3+0x278>
 4d4:	mov	x2, x10
 4d8:	b	2b4 <__divtf3+0x2b4>
 4dc:	mov	x2, x4
 4e0:	b	350 <__divtf3+0x350>
 4e4:	mov	x11, x12
 4e8:	b	398 <__divtf3+0x398>
 4ec:	mov	x1, x12
 4f0:	b	3d4 <__divtf3+0x3d4>
 4f4:	mov	x1, x11
 4f8:	mov	x8, #0x0                   	// #0
 4fc:	b	460 <__divtf3+0x460>
 500:	tbz	x2, #47, 514 <__divtf3+0x514>
 504:	tbnz	x7, #47, 514 <__divtf3+0x514>
 508:	mov	x2, x7
 50c:	mov	x1, x8
 510:	mov	x10, x13
 514:	orr	x2, x2, #0x800000000000
 518:	mov	x5, x10
 51c:	mov	x3, #0x7fff                	// #32767
 520:	b	5a8 <__divtf3+0x5a8>
 524:	orr	w0, w0, #0x2
 528:	mov	x2, #0x0                   	// #0
 52c:	mov	x1, #0x0                   	// #0
 530:	b	51c <__divtf3+0x51c>
 534:	mov	x5, x10
 538:	mov	x12, x14
 53c:	cmp	x12, #0x1
 540:	b.eq	7a4 <__divtf3+0x7a4>  // b.none
 544:	cbz	x12, 474 <__divtf3+0x474>
 548:	cmp	x12, #0x2
 54c:	b.eq	528 <__divtf3+0x528>  // b.none
 550:	cmp	x12, #0x3
 554:	b.ne	474 <__divtf3+0x474>  // b.any
 558:	mov	x10, x5
 55c:	b	514 <__divtf3+0x514>
 560:	mov	x2, x7
 564:	mov	x1, x8
 568:	mov	x5, x13
 56c:	b	53c <__divtf3+0x53c>
 570:	mov	x1, #0xffffffffffffffff    	// #-1
 574:	b	474 <__divtf3+0x474>
 578:	cbnz	x5, 588 <__divtf3+0x588>
 57c:	adds	x1, x1, #0x8
 580:	b	4b8 <__divtf3+0x4b8>
 584:	cbnz	x5, 57c <__divtf3+0x57c>
 588:	tbz	x2, #52, 594 <__divtf3+0x594>
 58c:	and	x2, x2, #0xffefffffffffffff
 590:	add	x3, x9, #0x4, lsl #12
 594:	mov	x4, #0x7ffe                	// #32766
 598:	cmp	x3, x4
 59c:	b.gt	5dc <__divtf3+0x5dc>
 5a0:	extr	x1, x2, x1, #3
 5a4:	lsr	x2, x2, #3
 5a8:	and	x3, x3, #0x7fff
 5ac:	mov	x7, #0x0                   	// #0
 5b0:	bfxil	x7, x2, #0, #48
 5b4:	orr	w3, w3, w5, lsl #15
 5b8:	fmov	d0, x1
 5bc:	bfi	x7, x3, #48, #16
 5c0:	fmov	v0.d[1], x7
 5c4:	cbz	w0, 5d4 <__divtf3+0x5d4>
 5c8:	str	q0, [sp, #16]
 5cc:	bl	0 <__sfp_handle_exceptions>
 5d0:	ldr	q0, [sp, #16]
 5d4:	ldp	x29, x30, [sp], #32
 5d8:	ret
 5dc:	and	x1, x6, #0xc00000
 5e0:	cmp	x1, #0x400, lsl #12
 5e4:	b.eq	608 <__divtf3+0x608>  // b.none
 5e8:	cmp	x1, #0x800, lsl #12
 5ec:	b.eq	61c <__divtf3+0x61c>  // b.none
 5f0:	cbnz	x1, 630 <__divtf3+0x630>
 5f4:	mov	x3, #0x7fff                	// #32767
 5f8:	mov	w2, #0x14                  	// #20
 5fc:	orr	w0, w0, w2
 600:	mov	x2, x1
 604:	b	5a8 <__divtf3+0x5a8>
 608:	cmp	x5, #0x0
 60c:	mov	x2, #0x7fff                	// #32767
 610:	csetm	x1, ne  // ne = any
 614:	csel	x3, x2, x4, eq  // eq = none
 618:	b	5f8 <__divtf3+0x5f8>
 61c:	cmp	x5, #0x0
 620:	mov	x2, #0x7fff                	// #32767
 624:	csetm	x1, eq  // eq = none
 628:	csel	x3, x2, x4, ne  // ne = any
 62c:	b	5f8 <__divtf3+0x5f8>
 630:	mov	x3, x4
 634:	mov	x1, #0xffffffffffffffff    	// #-1
 638:	b	5f8 <__divtf3+0x5f8>
 63c:	mov	x4, #0x1                   	// #1
 640:	sub	x3, x4, x3
 644:	cmp	x3, #0x74
 648:	b.gt	730 <__divtf3+0x730>
 64c:	cmp	x3, #0x3f
 650:	b.gt	6b8 <__divtf3+0x6b8>
 654:	mov	w7, #0x40                  	// #64
 658:	sub	w7, w7, w3
 65c:	lsr	x8, x1, x3
 660:	lsl	x1, x1, x7
 664:	cmp	x1, #0x0
 668:	lsl	x4, x2, x7
 66c:	cset	x1, ne  // ne = any
 670:	orr	x4, x4, x8
 674:	lsr	x2, x2, x3
 678:	orr	x1, x4, x1
 67c:	tst	x1, #0x7
 680:	b.eq	6fc <__divtf3+0x6fc>  // b.none
 684:	and	x3, x6, #0xc00000
 688:	orr	w0, w0, #0x10
 68c:	cmp	x3, #0x400, lsl #12
 690:	b.eq	6ec <__divtf3+0x6ec>  // b.none
 694:	cmp	x3, #0x800, lsl #12
 698:	b.eq	6f8 <__divtf3+0x6f8>  // b.none
 69c:	cbnz	x3, 6fc <__divtf3+0x6fc>
 6a0:	and	x3, x1, #0xf
 6a4:	cmp	x3, #0x4
 6a8:	b.eq	6fc <__divtf3+0x6fc>  // b.none
 6ac:	adds	x1, x1, #0x4
 6b0:	cinc	x2, x2, cs  // cs = hs, nlast
 6b4:	b	6fc <__divtf3+0x6fc>
 6b8:	sub	w4, w3, #0x40
 6bc:	mov	w7, #0x80                  	// #128
 6c0:	sub	w7, w7, w3
 6c4:	cmp	x3, #0x40
 6c8:	lsr	x4, x2, x4
 6cc:	lsl	x2, x2, x7
 6d0:	csel	x2, x2, xzr, ne  // ne = any
 6d4:	orr	x1, x2, x1
 6d8:	mov	x2, #0x0                   	// #0
 6dc:	cmp	x1, #0x0
 6e0:	cset	x1, ne  // ne = any
 6e4:	orr	x1, x4, x1
 6e8:	b	67c <__divtf3+0x67c>
 6ec:	cbnz	x5, 6fc <__divtf3+0x6fc>
 6f0:	adds	x1, x1, #0x8
 6f4:	b	6b0 <__divtf3+0x6b0>
 6f8:	cbnz	x5, 6f0 <__divtf3+0x6f0>
 6fc:	tbz	x2, #51, 718 <__divtf3+0x718>
 700:	orr	w0, w0, #0x10
 704:	mov	x2, #0x0                   	// #0
 708:	mov	x1, #0x0                   	// #0
 70c:	mov	x3, #0x1                   	// #1
 710:	orr	w0, w0, #0x8
 714:	b	5a8 <__divtf3+0x5a8>
 718:	mov	x3, #0x0                   	// #0
 71c:	extr	x1, x2, x1, #3
 720:	lsr	x2, x2, #3
 724:	tbnz	w0, #4, 710 <__divtf3+0x710>
 728:	tbz	w6, #11, 5a8 <__divtf3+0x5a8>
 72c:	b	710 <__divtf3+0x710>
 730:	orr	x1, x1, x2
 734:	cbz	x1, 760 <__divtf3+0x760>
 738:	and	x6, x6, #0xc00000
 73c:	orr	w0, w0, #0x10
 740:	cmp	x6, #0x400, lsl #12
 744:	b.eq	770 <__divtf3+0x770>  // b.none
 748:	cmp	x6, #0x800, lsl #12
 74c:	b.eq	780 <__divtf3+0x780>  // b.none
 750:	cmp	x6, #0x0
 754:	mov	x1, #0x5                   	// #5
 758:	csel	x4, x4, x1, ne  // ne = any
 75c:	lsr	x1, x4, #3
 760:	orr	w0, w0, #0x8
 764:	mov	x2, #0x0                   	// #0
 768:	mov	x3, #0x0                   	// #0
 76c:	b	5a8 <__divtf3+0x5a8>
 770:	cmp	x5, #0x0
 774:	mov	x1, #0x9                   	// #9
 778:	csel	x4, x1, x4, eq  // eq = none
 77c:	b	75c <__divtf3+0x75c>
 780:	cmp	x5, #0x0
 784:	mov	x1, #0x9                   	// #9
 788:	csel	x4, x1, x4, ne  // ne = any
 78c:	b	75c <__divtf3+0x75c>
 790:	mov	x2, #0xffffffffffff        	// #281474976710655
 794:	mov	x1, #0xffffffffffffffff    	// #-1
 798:	mov	w0, #0x1                   	// #1
 79c:	mov	x10, #0x0                   	// #0
 7a0:	b	514 <__divtf3+0x514>
 7a4:	mov	x2, #0x0                   	// #0
 7a8:	mov	x1, #0x0                   	// #0
 7ac:	b	768 <__divtf3+0x768>

eqtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eqtf2>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x8, x0, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x9, x1, [sp, #16]
  18:	mrs	x2, fpcr
  1c:	ubfx	x3, x0, #48, #15
  20:	ubfx	x2, x0, #0, #48
  24:	ubfx	x7, x1, #0, #48
  28:	lsr	x0, x0, #63
  2c:	ubfx	x5, x1, #48, #15
  30:	lsr	x1, x1, #63
  34:	mov	x10, #0x7fff                	// #32767
  38:	mov	x4, x8
  3c:	and	w0, w0, #0xff
  40:	mov	x6, x9
  44:	and	w1, w1, #0xff
  48:	cmp	x3, x10
  4c:	b.ne	64 <__eqtf2+0x64>  // b.any
  50:	orr	x10, x2, x8
  54:	cbnz	x10, d0 <__eqtf2+0xd0>
  58:	cmp	x5, x3
  5c:	b.ne	f8 <__eqtf2+0xf8>  // b.any
  60:	b	6c <__eqtf2+0x6c>
  64:	cmp	x5, x10
  68:	b.ne	74 <__eqtf2+0x74>  // b.any
  6c:	orr	x10, x7, x6
  70:	cbnz	x10, bc <__eqtf2+0xbc>
  74:	cmp	x3, x5
  78:	b.ne	f8 <__eqtf2+0xf8>  // b.any
  7c:	cmp	x2, x7
  80:	ccmp	x8, x9, #0x0, eq  // eq = none
  84:	b.ne	f8 <__eqtf2+0xf8>  // b.any
  88:	cmp	w0, w1
  8c:	b.eq	b4 <__eqtf2+0xb4>  // b.none
  90:	cbnz	x3, f8 <__eqtf2+0xf8>
  94:	orr	x2, x2, x4
  98:	cmp	x2, #0x0
  9c:	cset	w0, ne  // ne = any
  a0:	ldp	x29, x30, [sp], #32
  a4:	ret
  a8:	mov	w0, #0x1                   	// #1
  ac:	bl	0 <__sfp_handle_exceptions>
  b0:	b	f8 <__eqtf2+0xf8>
  b4:	mov	w0, #0x0                   	// #0
  b8:	b	a0 <__eqtf2+0xa0>
  bc:	mov	x0, #0x7fff                	// #32767
  c0:	cmp	x3, x0
  c4:	b.ne	f0 <__eqtf2+0xf0>  // b.any
  c8:	orr	x4, x2, x4
  cc:	cbz	x4, f0 <__eqtf2+0xf0>
  d0:	tst	x2, #0x800000000000
  d4:	b.eq	a8 <__eqtf2+0xa8>  // b.none
  d8:	mov	x1, #0x7fff                	// #32767
  dc:	mov	w0, #0x1                   	// #1
  e0:	cmp	x5, x1
  e4:	b.ne	a0 <__eqtf2+0xa0>  // b.any
  e8:	orr	x6, x7, x6
  ec:	cbz	x6, a0 <__eqtf2+0xa0>
  f0:	tst	x7, #0x800000000000
  f4:	b.eq	a8 <__eqtf2+0xa8>  // b.none
  f8:	mov	w0, #0x1                   	// #1
  fc:	b	a0 <__eqtf2+0xa0>

getf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__getf2>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x8, x1, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x7, x0, [sp, #16]
  18:	mrs	x2, fpcr
  1c:	ubfx	x10, x1, #48, #15
  20:	lsr	x2, x0, #63
  24:	ubfx	x4, x1, #0, #48
  28:	lsr	x1, x1, #63
  2c:	ubfx	x3, x0, #0, #48
  30:	ubfx	x9, x0, #48, #15
  34:	mov	x5, x8
  38:	mov	x0, #0x7fff                	// #32767
  3c:	and	w1, w1, #0xff
  40:	mov	x6, x7
  44:	and	w2, w2, #0xff
  48:	cmp	x10, x0
  4c:	b.ne	58 <__getf2+0x58>  // b.any
  50:	orr	x11, x4, x8
  54:	cbnz	x11, 128 <__getf2+0x128>
  58:	cmp	x9, x0
  5c:	b.ne	68 <__getf2+0x68>  // b.any
  60:	orr	x0, x3, x6
  64:	cbnz	x0, 128 <__getf2+0x128>
  68:	cbnz	x10, a8 <__getf2+0xa8>
  6c:	orr	x5, x4, x5
  70:	cmp	x5, #0x0
  74:	cset	w5, eq  // eq = none
  78:	cbnz	x9, b0 <__getf2+0xb0>
  7c:	orr	x6, x3, x6
  80:	cmp	x6, #0x0
  84:	cset	w0, eq  // eq = none
  88:	csel	w6, w5, wzr, eq  // eq = none
  8c:	cbnz	w6, 138 <__getf2+0x138>
  90:	and	x2, x2, #0xff
  94:	cbz	w5, b8 <__getf2+0xb8>
  98:	cmp	x2, #0x0
  9c:	csinv	w0, w5, wzr, ne  // ne = any
  a0:	ldp	x29, x30, [sp], #32
  a4:	ret
  a8:	mov	w5, #0x0                   	// #0
  ac:	b	78 <__getf2+0x78>
  b0:	mov	w0, #0x0                   	// #0
  b4:	b	90 <__getf2+0x90>
  b8:	and	x1, x1, #0xff
  bc:	cbz	w0, cc <__getf2+0xcc>
  c0:	cmp	x1, #0x0
  c4:	csinv	w0, w0, wzr, eq  // eq = none
  c8:	b	a0 <__getf2+0xa0>
  cc:	cmp	x1, x2
  d0:	b.eq	e4 <__getf2+0xe4>  // b.none
  d4:	mov	w0, #0xffffffff            	// #-1
  d8:	cmp	x1, #0x0
  dc:	cneg	w0, w0, eq  // eq = none
  e0:	b	a0 <__getf2+0xa0>
  e4:	cmp	x10, x9
  e8:	b.gt	d4 <__getf2+0xd4>
  ec:	b.ge	f8 <__getf2+0xf8>  // b.tcont
  f0:	mov	w0, #0x1                   	// #1
  f4:	b	d8 <__getf2+0xd8>
  f8:	cmp	x4, x3
  fc:	b.hi	d4 <__getf2+0xd4>  // b.pmore
 100:	cset	w2, eq  // eq = none
 104:	cmp	w2, #0x0
 108:	ccmp	x8, x7, #0x0, ne  // ne = any
 10c:	b.hi	d4 <__getf2+0xd4>  // b.pmore
 110:	cmp	x4, x3
 114:	b.cc	f0 <__getf2+0xf0>  // b.lo, b.ul, b.last
 118:	cmp	w2, #0x0
 11c:	ccmp	x8, x7, #0x2, ne  // ne = any
 120:	b.cs	a0 <__getf2+0xa0>  // b.hs, b.nlast
 124:	b	f0 <__getf2+0xf0>
 128:	mov	w0, #0x1                   	// #1
 12c:	bl	0 <__sfp_handle_exceptions>
 130:	mov	w0, #0xfffffffe            	// #-2
 134:	b	a0 <__getf2+0xa0>
 138:	mov	w0, #0x0                   	// #0
 13c:	b	a0 <__getf2+0xa0>

letf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__letf2>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x8, x1, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x7, x0, [sp, #16]
  18:	mrs	x2, fpcr
  1c:	ubfx	x10, x1, #48, #15
  20:	lsr	x2, x0, #63
  24:	ubfx	x4, x1, #0, #48
  28:	lsr	x1, x1, #63
  2c:	ubfx	x3, x0, #0, #48
  30:	ubfx	x9, x0, #48, #15
  34:	mov	x5, x8
  38:	mov	x0, #0x7fff                	// #32767
  3c:	and	w1, w1, #0xff
  40:	mov	x6, x7
  44:	and	w2, w2, #0xff
  48:	cmp	x10, x0
  4c:	b.ne	58 <__letf2+0x58>  // b.any
  50:	orr	x11, x4, x8
  54:	cbnz	x11, 128 <__letf2+0x128>
  58:	cmp	x9, x0
  5c:	b.ne	68 <__letf2+0x68>  // b.any
  60:	orr	x0, x3, x6
  64:	cbnz	x0, 128 <__letf2+0x128>
  68:	cbnz	x10, a8 <__letf2+0xa8>
  6c:	orr	x5, x4, x5
  70:	cmp	x5, #0x0
  74:	cset	w5, eq  // eq = none
  78:	cbnz	x9, b0 <__letf2+0xb0>
  7c:	orr	x6, x3, x6
  80:	cmp	x6, #0x0
  84:	cset	w0, eq  // eq = none
  88:	csel	w6, w5, wzr, eq  // eq = none
  8c:	cbnz	w6, 138 <__letf2+0x138>
  90:	and	x2, x2, #0xff
  94:	cbz	w5, b8 <__letf2+0xb8>
  98:	cmp	x2, #0x0
  9c:	csinv	w0, w5, wzr, ne  // ne = any
  a0:	ldp	x29, x30, [sp], #32
  a4:	ret
  a8:	mov	w5, #0x0                   	// #0
  ac:	b	78 <__letf2+0x78>
  b0:	mov	w0, #0x0                   	// #0
  b4:	b	90 <__letf2+0x90>
  b8:	and	x1, x1, #0xff
  bc:	cbz	w0, cc <__letf2+0xcc>
  c0:	cmp	x1, #0x0
  c4:	csinv	w0, w0, wzr, eq  // eq = none
  c8:	b	a0 <__letf2+0xa0>
  cc:	cmp	x1, x2
  d0:	b.eq	e4 <__letf2+0xe4>  // b.none
  d4:	mov	w0, #0xffffffff            	// #-1
  d8:	cmp	x1, #0x0
  dc:	cneg	w0, w0, eq  // eq = none
  e0:	b	a0 <__letf2+0xa0>
  e4:	cmp	x10, x9
  e8:	b.gt	d4 <__letf2+0xd4>
  ec:	b.ge	f8 <__letf2+0xf8>  // b.tcont
  f0:	mov	w0, #0x1                   	// #1
  f4:	b	d8 <__letf2+0xd8>
  f8:	cmp	x4, x3
  fc:	b.hi	d4 <__letf2+0xd4>  // b.pmore
 100:	cset	w2, eq  // eq = none
 104:	cmp	w2, #0x0
 108:	ccmp	x8, x7, #0x0, ne  // ne = any
 10c:	b.hi	d4 <__letf2+0xd4>  // b.pmore
 110:	cmp	x4, x3
 114:	b.cc	f0 <__letf2+0xf0>  // b.lo, b.ul, b.last
 118:	cmp	w2, #0x0
 11c:	ccmp	x8, x7, #0x2, ne  // ne = any
 120:	b.cs	a0 <__letf2+0xa0>  // b.hs, b.nlast
 124:	b	f0 <__letf2+0xf0>
 128:	mov	w0, #0x1                   	// #1
 12c:	bl	0 <__sfp_handle_exceptions>
 130:	mov	w0, #0x2                   	// #2
 134:	b	a0 <__letf2+0xa0>
 138:	mov	w0, #0x0                   	// #0
 13c:	b	a0 <__letf2+0xa0>

multf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x8, x4, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x1, x9, [sp, #16]
  18:	mrs	x6, fpcr
  1c:	ubfx	x11, x4, #0, #48
  20:	ubfx	x10, x4, #48, #15
  24:	lsr	x4, x4, #63
  28:	and	w5, w4, #0xff
  2c:	cbz	w10, 60 <__multf3+0x60>
  30:	mov	w2, #0x7fff                	// #32767
  34:	cmp	w10, w2
  38:	b.eq	c0 <__multf3+0xc0>  // b.none
  3c:	and	x10, x10, #0xffff
  40:	extr	x11, x11, x8, #61
  44:	mov	x12, #0xffffffffffffc001    	// #-16383
  48:	orr	x11, x11, #0x8000000000000
  4c:	lsl	x2, x8, #3
  50:	add	x10, x10, x12
  54:	mov	x15, #0x0                   	// #0
  58:	mov	w0, #0x0                   	// #0
  5c:	b	e0 <__multf3+0xe0>
  60:	orr	x2, x11, x8
  64:	cbz	x2, 160 <__multf3+0x160>
  68:	clz	x2, x8
  6c:	cmp	x11, #0x0
  70:	clz	x0, x11
  74:	add	x2, x2, #0x40
  78:	csel	x0, x2, x0, eq  // eq = none
  7c:	sub	x7, x0, #0xf
  80:	cmp	x7, #0x3c
  84:	b.gt	b0 <__multf3+0xb0>
  88:	add	w2, w7, #0x3
  8c:	mov	w10, #0x3d                  	// #61
  90:	sub	w7, w10, w7
  94:	lsl	x11, x11, x2
  98:	lsr	x7, x8, x7
  9c:	orr	x11, x7, x11
  a0:	lsl	x2, x8, x2
  a4:	mov	x10, #0xffffffffffffc011    	// #-16367
  a8:	sub	x10, x10, x0
  ac:	b	54 <__multf3+0x54>
  b0:	sub	w11, w7, #0x3d
  b4:	mov	x2, #0x0                   	// #0
  b8:	lsl	x11, x8, x11
  bc:	b	a4 <__multf3+0xa4>
  c0:	orr	x2, x11, x8
  c4:	cbz	x2, 170 <__multf3+0x170>
  c8:	lsr	x0, x11, #47
  cc:	mov	x2, x8
  d0:	eor	x0, x0, #0x1
  d4:	mov	x10, #0x7fff                	// #32767
  d8:	and	w0, w0, #0x1
  dc:	mov	x15, #0x3                   	// #3
  e0:	lsr	x8, x9, #63
  e4:	ubfx	x12, x9, #0, #48
  e8:	ubfx	x7, x9, #48, #15
  ec:	and	w3, w8, #0xff
  f0:	mov	x16, x8
  f4:	cbz	w7, 180 <__multf3+0x180>
  f8:	mov	w8, #0x7fff                	// #32767
  fc:	cmp	w7, w8
 100:	b.eq	1e0 <__multf3+0x1e0>  // b.none
 104:	and	x7, x7, #0xffff
 108:	extr	x12, x12, x1, #61
 10c:	mov	x8, #0xffffffffffffc001    	// #-16383
 110:	orr	x12, x12, #0x8000000000000
 114:	lsl	x1, x1, #3
 118:	add	x7, x7, x8
 11c:	mov	x8, #0x0                   	// #0
 120:	eor	w5, w5, w3
 124:	orr	x3, x8, x15, lsl #2
 128:	add	x10, x7, x10
 12c:	sub	x3, x3, #0x1
 130:	and	x5, x5, #0xff
 134:	add	x9, x10, #0x1
 138:	cmp	x3, #0xe
 13c:	b.hi	254 <__multf3+0x254>  // b.pmore
 140:	cmp	w3, #0xe
 144:	b.hi	254 <__multf3+0x254>  // b.pmore
 148:	adrp	x7, 0 <__multf3>
 14c:	add	x7, x7, #0x0
 150:	ldrh	w3, [x7, w3, uxtw #1]
 154:	adr	x7, 160 <__multf3+0x160>
 158:	add	x3, x7, w3, sxth #2
 15c:	br	x3
 160:	mov	x11, #0x0                   	// #0
 164:	mov	x10, #0x0                   	// #0
 168:	mov	x15, #0x1                   	// #1
 16c:	b	58 <__multf3+0x58>
 170:	mov	x11, #0x0                   	// #0
 174:	mov	x10, #0x7fff                	// #32767
 178:	mov	x15, #0x2                   	// #2
 17c:	b	58 <__multf3+0x58>
 180:	orr	x7, x12, x1
 184:	cbz	x7, 1fc <__multf3+0x1fc>
 188:	clz	x13, x1
 18c:	cmp	x12, #0x0
 190:	add	x13, x13, #0x40
 194:	clz	x7, x12
 198:	csel	x9, x13, x7, eq  // eq = none
 19c:	sub	x13, x9, #0xf
 1a0:	cmp	x13, #0x3c
 1a4:	b.gt	1d0 <__multf3+0x1d0>
 1a8:	add	w7, w13, #0x3
 1ac:	mov	w14, #0x3d                  	// #61
 1b0:	sub	w13, w14, w13
 1b4:	lsl	x12, x12, x7
 1b8:	lsr	x13, x1, x13
 1bc:	orr	x12, x13, x12
 1c0:	lsl	x1, x1, x7
 1c4:	mov	x7, #0xffffffffffffc011    	// #-16367
 1c8:	sub	x7, x7, x9
 1cc:	b	11c <__multf3+0x11c>
 1d0:	sub	w12, w13, #0x3d
 1d4:	lsl	x12, x1, x12
 1d8:	mov	x1, #0x0                   	// #0
 1dc:	b	1c4 <__multf3+0x1c4>
 1e0:	orr	x7, x12, x1
 1e4:	cbz	x7, 210 <__multf3+0x210>
 1e8:	tst	x12, #0x800000000000
 1ec:	mov	x7, #0x7fff                	// #32767
 1f0:	csinc	w0, w0, wzr, ne  // ne = any
 1f4:	mov	x8, #0x3                   	// #3
 1f8:	b	120 <__multf3+0x120>
 1fc:	mov	x12, #0x0                   	// #0
 200:	mov	x1, #0x0                   	// #0
 204:	mov	x7, #0x0                   	// #0
 208:	mov	x8, #0x1                   	// #1
 20c:	b	120 <__multf3+0x120>
 210:	mov	x12, #0x0                   	// #0
 214:	mov	x1, #0x0                   	// #0
 218:	mov	x7, #0x7fff                	// #32767
 21c:	mov	x8, #0x2                   	// #2
 220:	b	120 <__multf3+0x120>
 224:	mov	x12, x11
 228:	mov	x1, x2
 22c:	mov	x8, x15
 230:	cmp	x8, #0x2
 234:	b.eq	680 <__multf3+0x680>  // b.none
 238:	cmp	x8, #0x3
 23c:	b.eq	670 <__multf3+0x670>  // b.none
 240:	cmp	x8, #0x1
 244:	b.ne	3b4 <__multf3+0x3b4>  // b.any
 248:	mov	x1, #0x0                   	// #0
 24c:	mov	x2, #0x0                   	// #0
 250:	b	634 <__multf3+0x634>
 254:	lsr	x14, x2, #32
 258:	and	x15, x1, #0xffffffff
 25c:	lsr	x4, x1, #32
 260:	and	x2, x2, #0xffffffff
 264:	mul	x1, x14, x15
 268:	mul	x3, x15, x2
 26c:	madd	x13, x4, x2, x1
 270:	mul	x17, x14, x4
 274:	add	x13, x13, x3, lsr #32
 278:	cmp	x1, x13
 27c:	b.ls	288 <__multf3+0x288>  // b.plast
 280:	mov	x1, #0x100000000           	// #4294967296
 284:	add	x17, x17, x1
 288:	and	x3, x3, #0xffffffff
 28c:	and	x1, x12, #0xffffffff
 290:	lsr	x7, x13, #32
 294:	add	x13, x3, x13, lsl #32
 298:	lsr	x3, x12, #32
 29c:	mul	x12, x14, x1
 2a0:	mul	x18, x2, x1
 2a4:	madd	x2, x3, x2, x12
 2a8:	mul	x14, x14, x3
 2ac:	add	x8, x2, x18, lsr #32
 2b0:	cmp	x12, x8
 2b4:	b.ls	2c0 <__multf3+0x2c0>  // b.plast
 2b8:	mov	x2, #0x100000000           	// #4294967296
 2bc:	add	x14, x14, x2
 2c0:	lsr	x16, x11, #32
 2c4:	and	x11, x11, #0xffffffff
 2c8:	and	x18, x18, #0xffffffff
 2cc:	add	x14, x14, x8, lsr #32
 2d0:	add	x18, x18, x8, lsl #32
 2d4:	mul	x8, x15, x11
 2d8:	add	x7, x7, x18
 2dc:	mul	x15, x16, x15
 2e0:	mul	x2, x4, x16
 2e4:	madd	x4, x4, x11, x15
 2e8:	add	x4, x4, x8, lsr #32
 2ec:	cmp	x15, x4
 2f0:	b.ls	2fc <__multf3+0x2fc>  // b.plast
 2f4:	mov	x12, #0x100000000           	// #4294967296
 2f8:	add	x2, x2, x12
 2fc:	mul	x12, x16, x1
 300:	and	x8, x8, #0xffffffff
 304:	mul	x16, x3, x16
 308:	add	x15, x2, x4, lsr #32
 30c:	madd	x3, x3, x11, x12
 310:	add	x8, x8, x4, lsl #32
 314:	mul	x4, x11, x1
 318:	add	x3, x3, x4, lsr #32
 31c:	cmp	x12, x3
 320:	b.ls	32c <__multf3+0x32c>  // b.plast
 324:	mov	x1, #0x100000000           	// #4294967296
 328:	add	x16, x16, x1
 32c:	and	x2, x4, #0xffffffff
 330:	add	x7, x17, x7
 334:	add	x2, x2, x3, lsl #32
 338:	lsr	x3, x3, #32
 33c:	adds	x2, x2, x14
 340:	cset	x4, cs  // cs = hs, nlast
 344:	cmp	x7, x18
 348:	cset	x1, cc  // cc = lo, ul, last
 34c:	adds	x2, x2, x1
 350:	cset	x1, cs  // cs = hs, nlast
 354:	cmp	x4, #0x0
 358:	ccmp	x1, #0x0, #0x0, eq  // eq = none
 35c:	cinc	x3, x3, ne  // ne = any
 360:	adds	x7, x7, x8
 364:	cset	x1, cs  // cs = hs, nlast
 368:	adds	x2, x2, x15
 36c:	cset	x4, cs  // cs = hs, nlast
 370:	adds	x2, x2, x1
 374:	cset	x1, cs  // cs = hs, nlast
 378:	cmp	x4, #0x0
 37c:	ccmp	x1, #0x0, #0x0, eq  // eq = none
 380:	orr	x13, x13, x7, lsl #13
 384:	cinc	x1, x16, ne  // ne = any
 388:	cmp	x13, #0x0
 38c:	add	x3, x1, x3
 390:	cset	x1, ne  // ne = any
 394:	orr	x7, x1, x7, lsr #51
 398:	orr	x1, x7, x2, lsl #13
 39c:	extr	x12, x3, x2, #51
 3a0:	tbz	x3, #39, 43c <__multf3+0x43c>
 3a4:	and	x2, x1, #0x1
 3a8:	orr	x1, x2, x1, lsr #1
 3ac:	orr	x1, x1, x12, lsl #63
 3b0:	lsr	x12, x12, #1
 3b4:	mov	x2, #0x3fff                	// #16383
 3b8:	add	x3, x9, x2
 3bc:	cmp	x3, #0x0
 3c0:	b.le	508 <__multf3+0x508>
 3c4:	tst	x1, #0x7
 3c8:	b.eq	454 <__multf3+0x454>  // b.none
 3cc:	and	x2, x6, #0xc00000
 3d0:	orr	w0, w0, #0x10
 3d4:	cmp	x2, #0x400, lsl #12
 3d8:	b.eq	444 <__multf3+0x444>  // b.none
 3dc:	cmp	x2, #0x800, lsl #12
 3e0:	b.eq	450 <__multf3+0x450>  // b.none
 3e4:	cbnz	x2, 454 <__multf3+0x454>
 3e8:	and	x2, x1, #0xf
 3ec:	cmp	x2, #0x4
 3f0:	b.eq	454 <__multf3+0x454>  // b.none
 3f4:	adds	x1, x1, #0x4
 3f8:	cinc	x12, x12, cs  // cs = hs, nlast
 3fc:	b	454 <__multf3+0x454>
 400:	tbz	x11, #47, 414 <__multf3+0x414>
 404:	tbnz	x12, #47, 414 <__multf3+0x414>
 408:	mov	x11, x12
 40c:	mov	x2, x1
 410:	mov	x4, x16
 414:	orr	x1, x11, #0x800000000000
 418:	mov	x5, x4
 41c:	mov	x3, #0x7fff                	// #32767
 420:	b	474 <__multf3+0x474>
 424:	mov	x12, x11
 428:	mov	x1, x2
 42c:	mov	x5, x4
 430:	b	22c <__multf3+0x22c>
 434:	mov	x5, x16
 438:	b	230 <__multf3+0x230>
 43c:	mov	x9, x10
 440:	b	3b4 <__multf3+0x3b4>
 444:	cbnz	x5, 454 <__multf3+0x454>
 448:	adds	x1, x1, #0x8
 44c:	b	3f8 <__multf3+0x3f8>
 450:	cbnz	x5, 448 <__multf3+0x448>
 454:	tbz	x12, #52, 460 <__multf3+0x460>
 458:	and	x12, x12, #0xffefffffffffffff
 45c:	add	x3, x9, #0x4, lsl #12
 460:	mov	x4, #0x7ffe                	// #32766
 464:	cmp	x3, x4
 468:	b.gt	4a8 <__multf3+0x4a8>
 46c:	extr	x2, x12, x1, #3
 470:	lsr	x1, x12, #3
 474:	and	x3, x3, #0x7fff
 478:	mov	x7, #0x0                   	// #0
 47c:	bfxil	x7, x1, #0, #48
 480:	orr	w3, w3, w5, lsl #15
 484:	fmov	d0, x2
 488:	bfi	x7, x3, #48, #16
 48c:	fmov	v0.d[1], x7
 490:	cbz	w0, 4a0 <__multf3+0x4a0>
 494:	str	q0, [sp, #16]
 498:	bl	0 <__sfp_handle_exceptions>
 49c:	ldr	q0, [sp, #16]
 4a0:	ldp	x29, x30, [sp], #32
 4a4:	ret
 4a8:	and	x2, x6, #0xc00000
 4ac:	cmp	x2, #0x400, lsl #12
 4b0:	b.eq	4d4 <__multf3+0x4d4>  // b.none
 4b4:	cmp	x2, #0x800, lsl #12
 4b8:	b.eq	4e8 <__multf3+0x4e8>  // b.none
 4bc:	cbnz	x2, 4fc <__multf3+0x4fc>
 4c0:	mov	x3, #0x7fff                	// #32767
 4c4:	mov	w1, #0x14                  	// #20
 4c8:	orr	w0, w0, w1
 4cc:	mov	x1, x2
 4d0:	b	474 <__multf3+0x474>
 4d4:	cmp	x5, #0x0
 4d8:	mov	x3, #0x7fff                	// #32767
 4dc:	csetm	x2, ne  // ne = any
 4e0:	csel	x3, x3, x4, eq  // eq = none
 4e4:	b	4c4 <__multf3+0x4c4>
 4e8:	cmp	x5, #0x0
 4ec:	mov	x3, #0x7fff                	// #32767
 4f0:	csetm	x2, eq  // eq = none
 4f4:	csel	x3, x3, x4, ne  // ne = any
 4f8:	b	4c4 <__multf3+0x4c4>
 4fc:	mov	x3, x4
 500:	mov	x2, #0xffffffffffffffff    	// #-1
 504:	b	4c4 <__multf3+0x4c4>
 508:	mov	x4, #0x1                   	// #1
 50c:	sub	x3, x4, x3
 510:	cmp	x3, #0x74
 514:	b.gt	5fc <__multf3+0x5fc>
 518:	cmp	x3, #0x3f
 51c:	b.gt	584 <__multf3+0x584>
 520:	mov	w4, #0x40                  	// #64
 524:	sub	w4, w4, w3
 528:	lsr	x7, x1, x3
 52c:	lsl	x1, x1, x4
 530:	cmp	x1, #0x0
 534:	cset	x1, ne  // ne = any
 538:	lsl	x2, x12, x4
 53c:	orr	x2, x2, x7
 540:	orr	x2, x2, x1
 544:	lsr	x1, x12, x3
 548:	tst	x2, #0x7
 54c:	b.eq	5c8 <__multf3+0x5c8>  // b.none
 550:	and	x3, x6, #0xc00000
 554:	orr	w0, w0, #0x10
 558:	cmp	x3, #0x400, lsl #12
 55c:	b.eq	5b8 <__multf3+0x5b8>  // b.none
 560:	cmp	x3, #0x800, lsl #12
 564:	b.eq	5c4 <__multf3+0x5c4>  // b.none
 568:	cbnz	x3, 5c8 <__multf3+0x5c8>
 56c:	and	x3, x2, #0xf
 570:	cmp	x3, #0x4
 574:	b.eq	5c8 <__multf3+0x5c8>  // b.none
 578:	adds	x2, x2, #0x4
 57c:	cinc	x1, x1, cs  // cs = hs, nlast
 580:	b	5c8 <__multf3+0x5c8>
 584:	sub	w2, w3, #0x40
 588:	mov	w4, #0x80                  	// #128
 58c:	sub	w4, w4, w3
 590:	cmp	x3, #0x40
 594:	lsr	x2, x12, x2
 598:	lsl	x12, x12, x4
 59c:	csel	x12, x12, xzr, ne  // ne = any
 5a0:	orr	x1, x12, x1
 5a4:	cmp	x1, #0x0
 5a8:	cset	x1, ne  // ne = any
 5ac:	orr	x2, x2, x1
 5b0:	mov	x1, #0x0                   	// #0
 5b4:	b	548 <__multf3+0x548>
 5b8:	cbnz	x5, 5c8 <__multf3+0x5c8>
 5bc:	adds	x2, x2, #0x8
 5c0:	b	57c <__multf3+0x57c>
 5c4:	cbnz	x5, 5bc <__multf3+0x5bc>
 5c8:	tbz	x1, #51, 5e4 <__multf3+0x5e4>
 5cc:	orr	w0, w0, #0x10
 5d0:	mov	x1, #0x0                   	// #0
 5d4:	mov	x2, #0x0                   	// #0
 5d8:	mov	x3, #0x1                   	// #1
 5dc:	orr	w0, w0, #0x8
 5e0:	b	474 <__multf3+0x474>
 5e4:	mov	x3, #0x0                   	// #0
 5e8:	extr	x2, x1, x2, #3
 5ec:	lsr	x1, x1, #3
 5f0:	tbnz	w0, #4, 5dc <__multf3+0x5dc>
 5f4:	tbz	w6, #11, 474 <__multf3+0x474>
 5f8:	b	5dc <__multf3+0x5dc>
 5fc:	orr	x2, x1, x12
 600:	cbz	x2, 62c <__multf3+0x62c>
 604:	and	x6, x6, #0xc00000
 608:	orr	w0, w0, #0x10
 60c:	cmp	x6, #0x400, lsl #12
 610:	b.eq	63c <__multf3+0x63c>  // b.none
 614:	cmp	x6, #0x800, lsl #12
 618:	b.eq	64c <__multf3+0x64c>  // b.none
 61c:	cmp	x6, #0x0
 620:	mov	x2, #0x5                   	// #5
 624:	csel	x4, x4, x2, ne  // ne = any
 628:	lsr	x2, x4, #3
 62c:	orr	w0, w0, #0x8
 630:	mov	x1, #0x0                   	// #0
 634:	mov	x3, #0x0                   	// #0
 638:	b	474 <__multf3+0x474>
 63c:	cmp	x5, #0x0
 640:	mov	x2, #0x9                   	// #9
 644:	csel	x4, x2, x4, eq  // eq = none
 648:	b	628 <__multf3+0x628>
 64c:	cmp	x5, #0x0
 650:	mov	x2, #0x9                   	// #9
 654:	csel	x4, x2, x4, ne  // ne = any
 658:	b	628 <__multf3+0x628>
 65c:	mov	x11, #0xffffffffffff        	// #281474976710655
 660:	mov	x2, #0xffffffffffffffff    	// #-1
 664:	mov	w0, #0x1                   	// #1
 668:	mov	x4, #0x0                   	// #0
 66c:	b	414 <__multf3+0x414>
 670:	mov	x11, x12
 674:	mov	x2, x1
 678:	mov	x4, x5
 67c:	b	414 <__multf3+0x414>
 680:	mov	x1, #0x0                   	// #0
 684:	mov	x2, #0x0                   	// #0
 688:	b	41c <__multf3+0x41c>

negtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negtf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x1, #0x0                   	// #0
   8:	str	q0, [sp]
   c:	ldp	x4, x2, [sp]
  10:	add	sp, sp, #0x10
  14:	lsr	x3, x2, #48
  18:	bfxil	x1, x2, #0, #48
  1c:	eor	w3, w3, #0x8000
  20:	fmov	d0, x4
  24:	bfi	x1, x3, #48, #16
  28:	fmov	v0.d[1], x1
  2c:	ret

subtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x7, x3, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x1, x0, [sp, #16]
  18:	mrs	x8, fpcr
  1c:	ubfx	x4, x0, #48, #15
  20:	ubfx	x12, x3, #48, #15
  24:	lsr	x6, x0, #63
  28:	lsr	x5, x3, #63
  2c:	ubfiz	x0, x0, #3, #48
  30:	ubfiz	x3, x3, #3, #48
  34:	orr	x2, x0, x1, lsr #61
  38:	mov	x14, x1
  3c:	mov	x0, #0x7fff                	// #32767
  40:	mov	x10, x12
  44:	orr	x3, x3, x7, lsr #61
  48:	lsl	x9, x7, #3
  4c:	mov	x11, x4
  50:	and	w6, w6, #0xff
  54:	lsl	x1, x1, #3
  58:	cmp	x4, x0
  5c:	b.ne	a8 <__subtf3+0xa8>  // b.any
  60:	orr	x0, x2, x1
  64:	cbz	x0, a8 <__subtf3+0xa8>
  68:	and	x6, x6, #0xff
  6c:	sub	w4, w12, w4
  70:	cmp	x6, x5
  74:	b.ne	478 <__subtf3+0x478>  // b.any
  78:	cmp	w4, #0x0
  7c:	b.le	1b4 <__subtf3+0x1b4>
  80:	mov	x0, #0x7fff                	// #32767
  84:	cbnz	x11, 11c <__subtf3+0x11c>
  88:	orr	x6, x2, x1
  8c:	cbnz	x6, b0 <__subtf3+0xb0>
  90:	cmp	x10, x0
  94:	b.eq	fc <__subtf3+0xfc>  // b.none
  98:	cbz	x10, 684 <__subtf3+0x684>
  9c:	mov	x2, x3
  a0:	mov	x1, x9
  a4:	b	ec <__subtf3+0xec>
  a8:	eor	w6, w6, #0x1
  ac:	b	68 <__subtf3+0x68>
  b0:	subs	w4, w4, #0x1
  b4:	b.ne	f4 <__subtf3+0xf4>  // b.any
  b8:	adds	x0, x9, x1
  bc:	mov	x9, x0
  c0:	adc	x3, x3, x2
  c4:	tbz	x3, #51, 98 <__subtf3+0x98>
  c8:	add	x10, x10, #0x1
  cc:	mov	x0, #0x7fff                	// #32767
  d0:	cmp	x10, x0
  d4:	b.eq	43c <__subtf3+0x43c>  // b.none
  d8:	and	x2, x3, #0xfff7ffffffffffff
  dc:	and	x0, x9, #0x1
  e0:	orr	x0, x0, x9, lsr #1
  e4:	orr	x1, x0, x3, lsl #63
  e8:	lsr	x2, x2, #1
  ec:	mov	w0, #0x0                   	// #0
  f0:	b	1e8 <__subtf3+0x1e8>
  f4:	cmp	x10, x0
  f8:	b.ne	128 <__subtf3+0x128>  // b.any
  fc:	orr	x1, x3, x9
 100:	cbz	x1, 8dc <__subtf3+0x8dc>
 104:	lsr	x0, x3, #50
 108:	mov	x2, x3
 10c:	eor	x0, x0, #0x1
 110:	mov	x1, x9
 114:	and	w0, w0, #0x1
 118:	b	1e8 <__subtf3+0x1e8>
 11c:	cmp	x10, x0
 120:	b.eq	fc <__subtf3+0xfc>  // b.none
 124:	orr	x2, x2, #0x8000000000000
 128:	cmp	w4, #0x74
 12c:	b.gt	1a4 <__subtf3+0x1a4>
 130:	cmp	w4, #0x3f
 134:	b.gt	170 <__subtf3+0x170>
 138:	mov	w0, #0x40                  	// #64
 13c:	sub	w0, w0, w4
 140:	lsr	x6, x1, x4
 144:	lsl	x1, x1, x0
 148:	cmp	x1, #0x0
 14c:	lsl	x7, x2, x0
 150:	orr	x7, x7, x6
 154:	cset	x6, ne  // ne = any
 158:	lsr	x2, x2, x4
 15c:	orr	x6, x7, x6
 160:	adds	x0, x6, x9
 164:	mov	x9, x0
 168:	adc	x3, x2, x3
 16c:	b	c4 <__subtf3+0xc4>
 170:	sub	w7, w4, #0x40
 174:	mov	w0, #0x80                  	// #128
 178:	sub	w0, w0, w4
 17c:	cmp	w4, #0x40
 180:	lsr	x7, x2, x7
 184:	lsl	x2, x2, x0
 188:	csel	x2, x2, xzr, ne  // ne = any
 18c:	orr	x1, x2, x1
 190:	cmp	x1, #0x0
 194:	cset	x6, ne  // ne = any
 198:	orr	x6, x7, x6
 19c:	mov	x2, #0x0                   	// #0
 1a0:	b	160 <__subtf3+0x160>
 1a4:	orr	x1, x2, x1
 1a8:	cmp	x1, #0x0
 1ac:	cset	x6, ne  // ne = any
 1b0:	b	19c <__subtf3+0x19c>
 1b4:	b.eq	2b8 <__subtf3+0x2b8>  // b.none
 1b8:	mov	x0, #0x7fff                	// #32767
 1bc:	cbnz	x10, 260 <__subtf3+0x260>
 1c0:	orr	x6, x3, x9
 1c4:	cbnz	x6, 1f0 <__subtf3+0x1f0>
 1c8:	cmp	x11, x0
 1cc:	b.ne	868 <__subtf3+0x868>  // b.any
 1d0:	orr	x0, x2, x1
 1d4:	cbz	x0, 8b0 <__subtf3+0x8b0>
 1d8:	lsr	x0, x2, #50
 1dc:	mov	x10, x11
 1e0:	eor	x0, x0, #0x1
 1e4:	and	w0, w0, #0x1
 1e8:	mov	w4, #0x0                   	// #0
 1ec:	b	2fc <__subtf3+0x2fc>
 1f0:	cmn	w4, #0x1
 1f4:	b.ne	20c <__subtf3+0x20c>  // b.any
 1f8:	adds	x0, x9, x1
 1fc:	mov	x9, x0
 200:	adc	x3, x3, x2
 204:	mov	x10, x11
 208:	b	c4 <__subtf3+0xc4>
 20c:	cmp	x11, x0
 210:	b.eq	1d0 <__subtf3+0x1d0>  // b.none
 214:	mvn	w4, w4
 218:	cmp	w4, #0x74
 21c:	b.gt	2a8 <__subtf3+0x2a8>
 220:	cmp	w4, #0x3f
 224:	b.gt	274 <__subtf3+0x274>
 228:	mov	w0, #0x40                  	// #64
 22c:	sub	w0, w0, w4
 230:	lsr	x7, x9, x4
 234:	lsl	x6, x3, x0
 238:	orr	x6, x6, x7
 23c:	lsl	x0, x9, x0
 240:	cmp	x0, #0x0
 244:	cset	x0, ne  // ne = any
 248:	lsr	x4, x3, x4
 24c:	orr	x0, x6, x0
 250:	adds	x3, x0, x1
 254:	mov	x9, x3
 258:	adc	x3, x4, x2
 25c:	b	204 <__subtf3+0x204>
 260:	cmp	x11, x0
 264:	b.eq	1d0 <__subtf3+0x1d0>  // b.none
 268:	neg	w4, w4
 26c:	orr	x3, x3, #0x8000000000000
 270:	b	218 <__subtf3+0x218>
 274:	sub	w6, w4, #0x40
 278:	mov	w0, #0x80                  	// #128
 27c:	sub	w0, w0, w4
 280:	cmp	w4, #0x40
 284:	lsr	x6, x3, x6
 288:	lsl	x3, x3, x0
 28c:	csel	x3, x3, xzr, ne  // ne = any
 290:	orr	x3, x3, x9
 294:	cmp	x3, #0x0
 298:	cset	x0, ne  // ne = any
 29c:	orr	x0, x6, x0
 2a0:	mov	x4, #0x0                   	// #0
 2a4:	b	250 <__subtf3+0x250>
 2a8:	orr	x3, x3, x9
 2ac:	cmp	x3, #0x0
 2b0:	cset	x0, ne  // ne = any
 2b4:	b	2a0 <__subtf3+0x2a0>
 2b8:	add	x12, x10, #0x1
 2bc:	mov	x0, #0x7fff                	// #32767
 2c0:	tst	x12, #0x7ffe
 2c4:	b.ne	3d8 <__subtf3+0x3d8>  // b.any
 2c8:	orr	x12, x3, x9
 2cc:	cbnz	x10, 33c <__subtf3+0x33c>
 2d0:	cbz	x12, 88c <__subtf3+0x88c>
 2d4:	orr	x0, x2, x1
 2d8:	cbz	x0, 684 <__subtf3+0x684>
 2dc:	adds	x0, x9, x1
 2e0:	mov	x9, x0
 2e4:	adc	x3, x3, x2
 2e8:	tbz	x3, #51, 684 <__subtf3+0x684>
 2ec:	and	x2, x3, #0xfff7ffffffffffff
 2f0:	mov	x1, x0
 2f4:	mov	x10, #0x1                   	// #1
 2f8:	mov	w0, #0x0                   	// #0
 2fc:	tst	x1, #0x7
 300:	b.eq	90c <__subtf3+0x90c>  // b.none
 304:	and	x3, x8, #0xc00000
 308:	orr	w0, w0, #0x10
 30c:	cmp	x3, #0x400, lsl #12
 310:	b.eq	8f8 <__subtf3+0x8f8>  // b.none
 314:	cmp	x3, #0x800, lsl #12
 318:	b.eq	904 <__subtf3+0x904>  // b.none
 31c:	cbnz	x3, 334 <__subtf3+0x334>
 320:	and	x3, x1, #0xf
 324:	cmp	x3, #0x4
 328:	b.eq	334 <__subtf3+0x334>  // b.none
 32c:	adds	x1, x1, #0x4
 330:	cinc	x2, x2, cs  // cs = hs, nlast
 334:	cbz	w4, 91c <__subtf3+0x91c>
 338:	b	918 <__subtf3+0x918>
 33c:	cmp	x10, x0
 340:	b.ne	3a4 <__subtf3+0x3a4>  // b.any
 344:	cbz	x12, 9fc <__subtf3+0x9fc>
 348:	lsr	x0, x3, #50
 34c:	cmp	x11, x10
 350:	eor	x0, x0, #0x1
 354:	and	w0, w0, #0x1
 358:	b.ne	3c4 <__subtf3+0x3c4>  // b.any
 35c:	orr	x10, x2, x1
 360:	cbz	x10, 9f4 <__subtf3+0x9f4>
 364:	tst	x2, #0x4000000000000
 368:	csinc	w0, w0, wzr, ne  // ne = any
 36c:	cbz	x12, 3bc <__subtf3+0x3bc>
 370:	and	x7, x7, #0x1fffffffffffffff
 374:	lsr	x1, x3, #3
 378:	orr	x7, x7, x3, lsl #61
 37c:	tbz	x3, #50, 398 <__subtf3+0x398>
 380:	lsr	x3, x2, #3
 384:	tbnz	x2, #50, 398 <__subtf3+0x398>
 388:	and	x1, x14, #0x1fffffffffffffff
 38c:	mov	x5, x6
 390:	orr	x7, x1, x2, lsl #61
 394:	mov	x1, x3
 398:	extr	x2, x1, x7, #61
 39c:	lsl	x1, x7, #3
 3a0:	b	3bc <__subtf3+0x3bc>
 3a4:	cmp	x11, x0
 3a8:	b.ne	3b4 <__subtf3+0x3b4>  // b.any
 3ac:	mov	w0, #0x0                   	// #0
 3b0:	b	35c <__subtf3+0x35c>
 3b4:	mov	w0, #0x0                   	// #0
 3b8:	cbnz	x12, 3c4 <__subtf3+0x3c4>
 3bc:	mov	x10, #0x7fff                	// #32767
 3c0:	b	2fc <__subtf3+0x2fc>
 3c4:	orr	x1, x2, x1
 3c8:	cbnz	x1, 370 <__subtf3+0x370>
 3cc:	mov	x2, x3
 3d0:	mov	x1, x9
 3d4:	b	3bc <__subtf3+0x3bc>
 3d8:	cmp	x12, x0
 3dc:	b.eq	3fc <__subtf3+0x3fc>  // b.none
 3e0:	adds	x1, x9, x1
 3e4:	mov	x10, x12
 3e8:	adc	x2, x3, x2
 3ec:	extr	x1, x2, x1, #1
 3f0:	lsr	x2, x2, #1
 3f4:	mov	w0, #0x0                   	// #0
 3f8:	b	2fc <__subtf3+0x2fc>
 3fc:	ands	x1, x8, #0xc00000
 400:	b.eq	8c0 <__subtf3+0x8c0>  // b.none
 404:	cmp	x1, #0x400, lsl #12
 408:	b.ne	420 <__subtf3+0x420>  // b.any
 40c:	cbnz	x5, 42c <__subtf3+0x42c>
 410:	mov	x10, x12
 414:	mov	x2, #0x0                   	// #0
 418:	mov	x1, #0x0                   	// #0
 41c:	b	8c8 <__subtf3+0x8c8>
 420:	cmp	x1, #0x800, lsl #12
 424:	b.ne	42c <__subtf3+0x42c>  // b.any
 428:	cbnz	x5, 410 <__subtf3+0x410>
 42c:	mov	x2, #0xffffffffffffffff    	// #-1
 430:	mov	x10, #0x7ffe                	// #32766
 434:	mov	x1, x2
 438:	b	8c8 <__subtf3+0x8c8>
 43c:	ands	x1, x8, #0xc00000
 440:	b.eq	8d0 <__subtf3+0x8d0>  // b.none
 444:	cmp	x1, #0x400, lsl #12
 448:	b.ne	45c <__subtf3+0x45c>  // b.any
 44c:	cbnz	x5, 468 <__subtf3+0x468>
 450:	mov	x2, #0x0                   	// #0
 454:	mov	x1, #0x0                   	// #0
 458:	b	8d4 <__subtf3+0x8d4>
 45c:	cmp	x1, #0x800, lsl #12
 460:	b.ne	468 <__subtf3+0x468>  // b.any
 464:	cbnz	x5, 450 <__subtf3+0x450>
 468:	mov	x2, #0xffffffffffffffff    	// #-1
 46c:	mov	x10, #0x7ffe                	// #32766
 470:	mov	x1, x2
 474:	b	8d4 <__subtf3+0x8d4>
 478:	cmp	w4, #0x0
 47c:	b.le	550 <__subtf3+0x550>
 480:	mov	x0, #0x7fff                	// #32767
 484:	cbnz	x11, 4fc <__subtf3+0x4fc>
 488:	orr	x6, x2, x1
 48c:	cbz	x6, 90 <__subtf3+0x90>
 490:	subs	w4, w4, #0x1
 494:	b.ne	4b0 <__subtf3+0x4b0>  // b.any
 498:	subs	x9, x9, x1
 49c:	sbc	x3, x3, x2
 4a0:	tbz	x3, #51, 98 <__subtf3+0x98>
 4a4:	and	x7, x3, #0x7ffffffffffff
 4a8:	mov	x11, x9
 4ac:	b	794 <__subtf3+0x794>
 4b0:	cmp	x10, x0
 4b4:	b.eq	fc <__subtf3+0xfc>  // b.none
 4b8:	cmp	w4, #0x74
 4bc:	b.gt	540 <__subtf3+0x540>
 4c0:	cmp	w4, #0x3f
 4c4:	b.gt	50c <__subtf3+0x50c>
 4c8:	mov	w6, #0x40                  	// #64
 4cc:	sub	w6, w6, w4
 4d0:	lsr	x7, x1, x4
 4d4:	lsl	x1, x1, x6
 4d8:	cmp	x1, #0x0
 4dc:	lsl	x0, x2, x6
 4e0:	cset	x1, ne  // ne = any
 4e4:	orr	x0, x0, x7
 4e8:	lsr	x2, x2, x4
 4ec:	orr	x0, x0, x1
 4f0:	subs	x9, x9, x0
 4f4:	sbc	x3, x3, x2
 4f8:	b	4a0 <__subtf3+0x4a0>
 4fc:	cmp	x10, x0
 500:	b.eq	fc <__subtf3+0xfc>  // b.none
 504:	orr	x2, x2, #0x8000000000000
 508:	b	4b8 <__subtf3+0x4b8>
 50c:	sub	w0, w4, #0x40
 510:	mov	w6, #0x80                  	// #128
 514:	sub	w6, w6, w4
 518:	cmp	w4, #0x40
 51c:	lsr	x0, x2, x0
 520:	lsl	x2, x2, x6
 524:	csel	x2, x2, xzr, ne  // ne = any
 528:	orr	x1, x2, x1
 52c:	cmp	x1, #0x0
 530:	cset	x1, ne  // ne = any
 534:	orr	x0, x0, x1
 538:	mov	x2, #0x0                   	// #0
 53c:	b	4f0 <__subtf3+0x4f0>
 540:	orr	x1, x2, x1
 544:	cmp	x1, #0x0
 548:	cset	x0, ne  // ne = any
 54c:	b	538 <__subtf3+0x538>
 550:	b.eq	650 <__subtf3+0x650>  // b.none
 554:	mov	x0, #0x7fff                	// #32767
 558:	cbnz	x10, 5f8 <__subtf3+0x5f8>
 55c:	orr	x5, x3, x9
 560:	cbnz	x5, 58c <__subtf3+0x58c>
 564:	cmp	x11, x0
 568:	b.ne	878 <__subtf3+0x878>  // b.any
 56c:	orr	x0, x2, x1
 570:	cbz	x0, 8e4 <__subtf3+0x8e4>
 574:	lsr	x0, x2, #50
 578:	mov	x10, x11
 57c:	eor	x0, x0, #0x1
 580:	mov	x5, x6
 584:	and	w0, w0, #0x1
 588:	b	1e8 <__subtf3+0x1e8>
 58c:	cmn	w4, #0x1
 590:	b.ne	5a8 <__subtf3+0x5a8>  // b.any
 594:	subs	x9, x1, x9
 598:	sbc	x3, x2, x3
 59c:	mov	x10, x11
 5a0:	mov	x5, x6
 5a4:	b	4a0 <__subtf3+0x4a0>
 5a8:	cmp	x11, x0
 5ac:	b.eq	56c <__subtf3+0x56c>  // b.none
 5b0:	mvn	w4, w4
 5b4:	cmp	w4, #0x74
 5b8:	b.gt	640 <__subtf3+0x640>
 5bc:	cmp	w4, #0x3f
 5c0:	b.gt	60c <__subtf3+0x60c>
 5c4:	mov	w0, #0x40                  	// #64
 5c8:	sub	w0, w0, w4
 5cc:	lsr	x7, x9, x4
 5d0:	lsl	x5, x3, x0
 5d4:	orr	x5, x5, x7
 5d8:	lsl	x0, x9, x0
 5dc:	cmp	x0, #0x0
 5e0:	cset	x0, ne  // ne = any
 5e4:	lsr	x4, x3, x4
 5e8:	orr	x0, x5, x0
 5ec:	subs	x9, x1, x0
 5f0:	sbc	x3, x2, x4
 5f4:	b	59c <__subtf3+0x59c>
 5f8:	cmp	x11, x0
 5fc:	b.eq	56c <__subtf3+0x56c>  // b.none
 600:	neg	w4, w4
 604:	orr	x3, x3, #0x8000000000000
 608:	b	5b4 <__subtf3+0x5b4>
 60c:	sub	w0, w4, #0x40
 610:	cmp	w4, #0x40
 614:	lsr	x5, x3, x0
 618:	mov	w0, #0x80                  	// #128
 61c:	sub	w0, w0, w4
 620:	lsl	x3, x3, x0
 624:	csel	x3, x3, xzr, ne  // ne = any
 628:	orr	x3, x3, x9
 62c:	cmp	x3, #0x0
 630:	cset	x0, ne  // ne = any
 634:	orr	x0, x5, x0
 638:	mov	x4, #0x0                   	// #0
 63c:	b	5ec <__subtf3+0x5ec>
 640:	orr	x3, x3, x9
 644:	cmp	x3, #0x0
 648:	cset	x0, ne  // ne = any
 64c:	b	638 <__subtf3+0x638>
 650:	add	x0, x10, #0x1
 654:	tst	x0, #0x7ffe
 658:	b.ne	774 <__subtf3+0x774>  // b.any
 65c:	orr	x12, x3, x9
 660:	orr	x13, x2, x1
 664:	cbnz	x10, 6e0 <__subtf3+0x6e0>
 668:	cbnz	x12, 6a0 <__subtf3+0x6a0>
 66c:	cbnz	x13, 898 <__subtf3+0x898>
 670:	and	x0, x8, #0xc00000
 674:	mov	x3, #0x0                   	// #0
 678:	cmp	x0, #0x800, lsl #12
 67c:	mov	x9, #0x0                   	// #0
 680:	cset	x5, eq  // eq = none
 684:	orr	x0, x9, x3
 688:	mov	x2, x3
 68c:	cmp	x0, #0x0
 690:	mov	x1, x9
 694:	cset	w4, ne  // ne = any
 698:	mov	x10, #0x0                   	// #0
 69c:	b	3f4 <__subtf3+0x3f4>
 6a0:	cbz	x13, 684 <__subtf3+0x684>
 6a4:	subs	x4, x9, x1
 6a8:	cmp	x9, x1
 6ac:	sbc	x0, x3, x2
 6b0:	tbz	x0, #51, 6c4 <__subtf3+0x6c4>
 6b4:	subs	x9, x1, x9
 6b8:	sbc	x3, x2, x3
 6bc:	mov	x5, x6
 6c0:	b	684 <__subtf3+0x684>
 6c4:	orr	x9, x4, x0
 6c8:	cbnz	x9, 8a4 <__subtf3+0x8a4>
 6cc:	and	x0, x8, #0xc00000
 6d0:	cmp	x0, #0x800, lsl #12
 6d4:	cset	x5, eq  // eq = none
 6d8:	mov	x3, #0x0                   	// #0
 6dc:	b	684 <__subtf3+0x684>
 6e0:	mov	x0, #0x7fff                	// #32767
 6e4:	cmp	x10, x0
 6e8:	b.ne	748 <__subtf3+0x748>  // b.any
 6ec:	cbz	x12, 9e4 <__subtf3+0x9e4>
 6f0:	lsr	x0, x3, #50
 6f4:	cmp	x11, x10
 6f8:	eor	x0, x0, #0x1
 6fc:	and	w0, w0, #0x1
 700:	b.ne	76c <__subtf3+0x76c>  // b.any
 704:	cbz	x13, a0c <__subtf3+0xa0c>
 708:	tst	x2, #0x4000000000000
 70c:	csinc	w0, w0, wzr, ne  // ne = any
 710:	cbz	x12, 760 <__subtf3+0x760>
 714:	and	x7, x7, #0x1fffffffffffffff
 718:	orr	x1, x7, x3, lsl #61
 71c:	lsr	x7, x3, #3
 720:	tbz	x3, #50, 73c <__subtf3+0x73c>
 724:	lsr	x3, x2, #3
 728:	tbnz	x2, #50, 73c <__subtf3+0x73c>
 72c:	and	x1, x14, #0x1fffffffffffffff
 730:	mov	x7, x3
 734:	orr	x1, x1, x2, lsl #61
 738:	mov	x5, x6
 73c:	extr	x2, x7, x1, #61
 740:	lsl	x1, x1, #3
 744:	b	3bc <__subtf3+0x3bc>
 748:	cmp	x11, x0
 74c:	b.ne	758 <__subtf3+0x758>  // b.any
 750:	mov	w0, #0x0                   	// #0
 754:	b	704 <__subtf3+0x704>
 758:	mov	w0, #0x0                   	// #0
 75c:	cbnz	x12, 76c <__subtf3+0x76c>
 760:	cbz	x13, a10 <__subtf3+0xa10>
 764:	mov	x5, x6
 768:	b	3bc <__subtf3+0x3bc>
 76c:	cbnz	x13, 714 <__subtf3+0x714>
 770:	b	3cc <__subtf3+0x3cc>
 774:	subs	x0, x9, x1
 778:	cmp	x9, x1
 77c:	mov	x11, x0
 780:	sbc	x7, x3, x2
 784:	tbz	x7, #51, 810 <__subtf3+0x810>
 788:	subs	x11, x1, x9
 78c:	mov	x5, x6
 790:	sbc	x7, x2, x3
 794:	clz	x0, x11
 798:	cmp	x7, #0x0
 79c:	clz	x3, x7
 7a0:	add	w0, w0, #0x40
 7a4:	csel	w3, w0, w3, eq  // eq = none
 7a8:	sub	w4, w3, #0xc
 7ac:	cmp	w4, #0x3f
 7b0:	b.gt	81c <__subtf3+0x81c>
 7b4:	neg	w3, w4
 7b8:	lsl	x7, x7, x4
 7bc:	lsl	x9, x11, x4
 7c0:	lsr	x3, x11, x3
 7c4:	orr	x3, x3, x7
 7c8:	sxtw	x0, w4
 7cc:	cmp	x10, w4, sxtw
 7d0:	b.gt	85c <__subtf3+0x85c>
 7d4:	sub	w4, w4, w10
 7d8:	add	w2, w4, #0x1
 7dc:	cmp	w2, #0x3f
 7e0:	b.gt	82c <__subtf3+0x82c>
 7e4:	mov	w0, #0x40                  	// #64
 7e8:	sub	w0, w0, w2
 7ec:	lsr	x1, x9, x2
 7f0:	lsl	x4, x3, x0
 7f4:	orr	x4, x4, x1
 7f8:	lsl	x0, x9, x0
 7fc:	cmp	x0, #0x0
 800:	cset	x0, ne  // ne = any
 804:	lsr	x3, x3, x2
 808:	orr	x9, x4, x0
 80c:	b	684 <__subtf3+0x684>
 810:	orr	x9, x0, x7
 814:	cbnz	x9, 794 <__subtf3+0x794>
 818:	b	6cc <__subtf3+0x6cc>
 81c:	sub	w3, w3, #0x4c
 820:	mov	x9, #0x0                   	// #0
 824:	lsl	x3, x11, x3
 828:	b	7c8 <__subtf3+0x7c8>
 82c:	sub	w4, w4, #0x3f
 830:	mov	w0, #0x80                  	// #128
 834:	sub	w0, w0, w2
 838:	cmp	w2, #0x40
 83c:	lsr	x4, x3, x4
 840:	lsl	x3, x3, x0
 844:	csel	x3, x3, xzr, ne  // ne = any
 848:	orr	x3, x9, x3
 84c:	cmp	x3, #0x0
 850:	cset	x3, ne  // ne = any
 854:	orr	x9, x4, x3
 858:	b	6d8 <__subtf3+0x6d8>
 85c:	sub	x10, x10, x0
 860:	and	x3, x3, #0xfff7ffffffffffff
 864:	b	98 <__subtf3+0x98>
 868:	mov	x3, x2
 86c:	mov	x9, x1
 870:	mov	x10, x11
 874:	b	98 <__subtf3+0x98>
 878:	mov	x3, x2
 87c:	mov	x9, x1
 880:	mov	x10, x11
 884:	mov	x5, x6
 888:	b	98 <__subtf3+0x98>
 88c:	mov	x3, x2
 890:	mov	x9, x1
 894:	b	684 <__subtf3+0x684>
 898:	mov	x3, x2
 89c:	mov	x9, x1
 8a0:	b	6bc <__subtf3+0x6bc>
 8a4:	mov	x3, x0
 8a8:	mov	x9, x4
 8ac:	b	684 <__subtf3+0x684>
 8b0:	mov	x10, x11
 8b4:	mov	x2, #0x0                   	// #0
 8b8:	mov	x1, #0x0                   	// #0
 8bc:	b	ec <__subtf3+0xec>
 8c0:	mov	x10, x12
 8c4:	mov	x2, #0x0                   	// #0
 8c8:	mov	w0, #0x14                  	// #20
 8cc:	b	2fc <__subtf3+0x2fc>
 8d0:	mov	x2, #0x0                   	// #0
 8d4:	mov	w0, #0x14                  	// #20
 8d8:	b	1e8 <__subtf3+0x1e8>
 8dc:	mov	x2, #0x0                   	// #0
 8e0:	b	ec <__subtf3+0xec>
 8e4:	mov	x10, x11
 8e8:	mov	x5, x6
 8ec:	mov	x2, #0x0                   	// #0
 8f0:	mov	x1, #0x0                   	// #0
 8f4:	b	ec <__subtf3+0xec>
 8f8:	cbnz	x5, 334 <__subtf3+0x334>
 8fc:	adds	x1, x1, #0x8
 900:	b	330 <__subtf3+0x330>
 904:	cbz	x5, 334 <__subtf3+0x334>
 908:	b	8fc <__subtf3+0x8fc>
 90c:	cbz	w4, 91c <__subtf3+0x91c>
 910:	tbnz	w0, #4, 918 <__subtf3+0x918>
 914:	tbz	w8, #11, 91c <__subtf3+0x91c>
 918:	orr	w0, w0, #0x8
 91c:	tbz	x2, #51, 934 <__subtf3+0x934>
 920:	add	x10, x10, #0x1
 924:	mov	x3, #0x7fff                	// #32767
 928:	cmp	x10, x3
 92c:	b.eq	98c <__subtf3+0x98c>  // b.none
 930:	and	x2, x2, #0xfff7ffffffffffff
 934:	mov	x3, #0x7fff                	// #32767
 938:	extr	x1, x2, x1, #3
 93c:	cmp	x10, x3
 940:	lsr	x2, x2, #3
 944:	b.ne	958 <__subtf3+0x958>  // b.any
 948:	orr	x3, x1, x2
 94c:	orr	x2, x2, #0x800000000000
 950:	cmp	x3, #0x0
 954:	csel	x2, x2, xzr, ne  // ne = any
 958:	and	x4, x10, #0x7fff
 95c:	mov	x7, #0x0                   	// #0
 960:	bfxil	x7, x2, #0, #48
 964:	orr	w5, w4, w5, lsl #15
 968:	fmov	d0, x1
 96c:	bfi	x7, x5, #48, #16
 970:	fmov	v0.d[1], x7
 974:	cbz	w0, 984 <__subtf3+0x984>
 978:	str	q0, [sp, #16]
 97c:	bl	0 <__sfp_handle_exceptions>
 980:	ldr	q0, [sp, #16]
 984:	ldp	x29, x30, [sp], #32
 988:	ret
 98c:	ands	x1, x8, #0xc00000
 990:	b.eq	9ac <__subtf3+0x9ac>  // b.none
 994:	cmp	x1, #0x400, lsl #12
 998:	b.ne	9bc <__subtf3+0x9bc>  // b.any
 99c:	cmp	x5, #0x0
 9a0:	mov	x2, #0x7ffe                	// #32766
 9a4:	csetm	x1, ne  // ne = any
 9a8:	csel	x10, x10, x2, eq  // eq = none
 9ac:	mov	w2, #0x14                  	// #20
 9b0:	orr	w0, w0, w2
 9b4:	mov	x2, x1
 9b8:	b	934 <__subtf3+0x934>
 9bc:	cmp	x1, #0x800, lsl #12
 9c0:	b.ne	9d8 <__subtf3+0x9d8>  // b.any
 9c4:	cmp	x5, #0x0
 9c8:	mov	x2, #0x7ffe                	// #32766
 9cc:	csetm	x1, eq  // eq = none
 9d0:	csel	x10, x10, x2, ne  // ne = any
 9d4:	b	9ac <__subtf3+0x9ac>
 9d8:	mov	x1, #0xffffffffffffffff    	// #-1
 9dc:	mov	x10, #0x7ffe                	// #32766
 9e0:	b	9ac <__subtf3+0x9ac>
 9e4:	cmp	x11, x10
 9e8:	b.eq	750 <__subtf3+0x750>  // b.none
 9ec:	mov	w0, #0x0                   	// #0
 9f0:	b	760 <__subtf3+0x760>
 9f4:	cbz	x12, 3bc <__subtf3+0x3bc>
 9f8:	b	3cc <__subtf3+0x3cc>
 9fc:	cmp	x11, x10
 a00:	b.eq	3ac <__subtf3+0x3ac>  // b.none
 a04:	mov	w0, #0x0                   	// #0
 a08:	b	3bc <__subtf3+0x3bc>
 a0c:	cbnz	x12, 3cc <__subtf3+0x3cc>
 a10:	mov	x5, #0x0                   	// #0
 a14:	mov	x2, #0x7ffffffffffff       	// #2251799813685247
 a18:	mov	x1, #0xfffffffffffffff8    	// #-8
 a1c:	mov	x10, #0x7fff                	// #32767
 a20:	mov	w0, #0x1                   	// #1
 a24:	b	2fc <__subtf3+0x2fc>

unordtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__unordtf2>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	ldp	x2, x0, [sp, #16]
  10:	str	q1, [sp, #16]
  14:	ldp	x3, x1, [sp, #16]
  18:	mrs	x4, fpcr
  1c:	ubfx	x5, x0, #0, #48
  20:	ubfx	x0, x0, #48, #15
  24:	ubfx	x6, x1, #0, #48
  28:	mov	x4, #0x7fff                	// #32767
  2c:	ubfx	x1, x1, #48, #15
  30:	cmp	x0, x4
  34:	b.ne	40 <__unordtf2+0x40>  // b.any
  38:	orr	x7, x2, x5
  3c:	cbnz	x7, 6c <__unordtf2+0x6c>
  40:	cmp	x1, x4
  44:	b.ne	50 <__unordtf2+0x50>  // b.any
  48:	orr	x4, x6, x3
  4c:	cbnz	x4, 5c <__unordtf2+0x5c>
  50:	mov	w0, #0x0                   	// #0
  54:	ldp	x29, x30, [sp], #32
  58:	ret
  5c:	cmp	x0, x1
  60:	b.ne	90 <__unordtf2+0x90>  // b.any
  64:	orr	x2, x2, x5
  68:	cbz	x2, 84 <__unordtf2+0x84>
  6c:	tst	x5, #0x800000000000
  70:	b.eq	98 <__unordtf2+0x98>  // b.none
  74:	mov	x2, #0x7fff                	// #32767
  78:	mov	w0, #0x1                   	// #1
  7c:	cmp	x1, x2
  80:	b.ne	54 <__unordtf2+0x54>  // b.any
  84:	orr	x3, x6, x3
  88:	mov	w0, #0x1                   	// #1
  8c:	cbz	x3, 54 <__unordtf2+0x54>
  90:	tst	x6, #0x800000000000
  94:	b.ne	a0 <__unordtf2+0xa0>  // b.any
  98:	mov	w0, #0x1                   	// #1
  9c:	bl	0 <__sfp_handle_exceptions>
  a0:	mov	w0, #0x1                   	// #1
  a4:	b	54 <__unordtf2+0x54>

fixtfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x1, x0, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	mov	x3, x1
  20:	mov	x5, #0x3ffe                	// #16382
  24:	ubfx	x1, x0, #0, #48
  28:	mov	x2, x4
  2c:	cmp	x4, x5
  30:	b.gt	54 <__fixtfsi+0x54>
  34:	cbnz	x4, c4 <__fixtfsi+0xc4>
  38:	orr	x1, x1, x3
  3c:	mov	w19, #0x0                   	// #0
  40:	cbnz	x1, bc <__fixtfsi+0xbc>
  44:	mov	w0, w19
  48:	ldr	x19, [sp, #16]
  4c:	ldp	x29, x30, [sp], #48
  50:	ret
  54:	lsr	x0, x0, #63
  58:	mov	x5, #0x401d                	// #16413
  5c:	and	w0, w0, #0xff
  60:	cmp	x4, x5
  64:	b.le	90 <__fixtfsi+0x90>
  68:	mov	x4, #0x401e                	// #16414
  6c:	cmp	x2, x4
  70:	mov	w19, #0x7fffffff            	// #2147483647
  74:	add	w19, w0, w19
  78:	csel	w0, w0, wzr, eq  // eq = none
  7c:	cbz	w0, cc <__fixtfsi+0xcc>
  80:	cmp	xzr, x1, lsr #17
  84:	b.ne	cc <__fixtfsi+0xcc>  // b.any
  88:	orr	x1, x3, x1, lsl #47
  8c:	b	40 <__fixtfsi+0x40>
  90:	orr	x1, x1, #0x1000000000000
  94:	mov	w2, #0xffffc011            	// #-16367
  98:	mov	w19, #0x402f                	// #16431
  9c:	add	w2, w4, w2
  a0:	sub	w19, w19, w4
  a4:	cmp	w0, #0x0
  a8:	lsl	x2, x1, x2
  ac:	orr	x2, x2, x3
  b0:	lsr	x19, x1, x19
  b4:	cneg	w19, w19, ne  // ne = any
  b8:	cbz	x2, 44 <__fixtfsi+0x44>
  bc:	mov	w0, #0x10                  	// #16
  c0:	b	d0 <__fixtfsi+0xd0>
  c4:	mov	w19, #0x0                   	// #0
  c8:	b	bc <__fixtfsi+0xbc>
  cc:	mov	w0, #0x1                   	// #1
  d0:	bl	0 <__sfp_handle_exceptions>
  d4:	b	44 <__fixtfsi+0x44>

fixunstfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldr	x0, [sp, #32]
  14:	ldr	x19, [sp, #40]
  18:	mrs	x1, fpcr
  1c:	ubfx	x2, x19, #48, #15
  20:	mov	x1, x0
  24:	mov	x4, #0x3ffe                	// #16382
  28:	ubfx	x0, x19, #0, #48
  2c:	cmp	x2, x4
  30:	b.gt	54 <__fixunstfsi+0x54>
  34:	cbnz	x2, b4 <__fixunstfsi+0xb4>
  38:	orr	x0, x1, x0
  3c:	cbnz	x0, b4 <__fixunstfsi+0xb4>
  40:	mov	w19, #0x0                   	// #0
  44:	mov	w0, w19
  48:	ldr	x19, [sp, #16]
  4c:	ldp	x29, x30, [sp], #48
  50:	ret
  54:	lsr	x19, x19, #63
  58:	mov	x4, #0x401f                	// #16415
  5c:	and	w19, w19, #0xff
  60:	mov	x6, #0x401e                	// #16414
  64:	ands	x5, x19, #0xff
  68:	csel	x4, x4, x6, eq  // eq = none
  6c:	cmp	x4, x2
  70:	b.le	a4 <__fixunstfsi+0xa4>
  74:	cbnz	x5, c0 <__fixunstfsi+0xc0>
  78:	orr	x0, x0, #0x1000000000000
  7c:	mov	w19, #0x402f                	// #16431
  80:	mov	w3, #0xffffc011            	// #-16367
  84:	sub	w19, w19, w2
  88:	add	w2, w2, w3
  8c:	lsr	x19, x0, x19
  90:	lsl	x0, x0, x2
  94:	orr	x0, x0, x1
  98:	cbz	x0, 44 <__fixunstfsi+0x44>
  9c:	mov	w0, #0x10                  	// #16
  a0:	b	ac <__fixunstfsi+0xac>
  a4:	sub	w19, w19, #0x1
  a8:	mov	w0, #0x1                   	// #1
  ac:	bl	0 <__sfp_handle_exceptions>
  b0:	b	44 <__fixunstfsi+0x44>
  b4:	mov	w0, #0x10                  	// #16
  b8:	mov	w19, #0x0                   	// #0
  bc:	b	ac <__fixunstfsi+0xac>
  c0:	mov	w0, #0x1                   	// #1
  c4:	b	b8 <__fixunstfsi+0xb8>

floatsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	cmp	w0, #0x0
   4:	cbz	w0, 4c <__floatsitf+0x4c>
   8:	lsr	w1, w0, #31
   c:	cneg	w0, w0, lt  // lt = tstop
  10:	clz	x3, x0
  14:	mov	w2, #0x403e                	// #16446
  18:	sub	w2, w2, w3
  1c:	mov	w3, #0x402f                	// #16431
  20:	sxtw	x4, w2
  24:	sub	w2, w3, w2
  28:	lsl	x0, x0, x2
  2c:	mov	x3, #0x0                   	// #0
  30:	orr	w1, w4, w1, lsl #15
  34:	bfxil	x3, x0, #0, #48
  38:	mov	x2, #0x0                   	// #0
  3c:	fmov	d0, x2
  40:	bfi	x3, x1, #48, #16
  44:	fmov	v0.d[1], x3
  48:	ret
  4c:	mov	x0, #0x0                   	// #0
  50:	mov	x4, #0x0                   	// #0
  54:	mov	x1, #0x0                   	// #0
  58:	b	2c <__floatsitf+0x2c>

floatunsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	cbz	w0, 40 <__floatunsitf+0x40>
   4:	mov	w0, w0
   8:	mov	w1, #0x403e                	// #16446
   c:	clz	x2, x0
  10:	sub	w1, w1, w2
  14:	mov	w2, #0x402f                	// #16431
  18:	sxtw	x4, w1
  1c:	sub	w1, w2, w1
  20:	lsl	x0, x0, x1
  24:	mov	x3, #0x0                   	// #0
  28:	mov	x2, #0x0                   	// #0
  2c:	bfxil	x3, x0, #0, #48
  30:	fmov	d0, x2
  34:	bfi	x3, x4, #48, #16
  38:	fmov	v0.d[1], x3
  3c:	ret
  40:	mov	x0, #0x0                   	// #0
  44:	mov	x4, #0x0                   	// #0
  48:	b	24 <__floatunsitf+0x24>

fixtfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x2, x1, [sp, #32]
  14:	mrs	x0, fpcr
  18:	ubfx	x4, x1, #48, #15
  1c:	mov	x5, #0x3ffe                	// #16382
  20:	ubfx	x0, x1, #0, #48
  24:	mov	x3, x4
  28:	cmp	x4, x5
  2c:	b.gt	4c <__fixtfdi+0x4c>
  30:	cbnz	x4, 10c <__fixtfdi+0x10c>
  34:	orr	x19, x0, x2
  38:	cbnz	x19, 10c <__fixtfdi+0x10c>
  3c:	mov	x0, x19
  40:	ldr	x19, [sp, #16]
  44:	ldp	x29, x30, [sp], #48
  48:	ret
  4c:	lsr	x1, x1, #63
  50:	mov	x5, #0x403d                	// #16445
  54:	and	w1, w1, #0xff
  58:	cmp	x4, x5
  5c:	b.le	94 <__fixtfdi+0x94>
  60:	and	x19, x1, #0xff
  64:	mov	x4, #0x7fffffffffffffff    	// #9223372036854775807
  68:	add	x19, x19, x4
  6c:	mov	x4, #0x403e                	// #16446
  70:	cmp	x3, x4
  74:	csel	w1, w1, wzr, eq  // eq = none
  78:	cbz	w1, 114 <__fixtfdi+0x114>
  7c:	extr	x0, x0, x2, #49
  80:	cbnz	x0, 114 <__fixtfdi+0x114>
  84:	cmp	xzr, x2, lsl #15
  88:	b.eq	3c <__fixtfdi+0x3c>  // b.none
  8c:	mov	w0, #0x10                  	// #16
  90:	b	118 <__fixtfdi+0x118>
  94:	mov	x5, #0x406f                	// #16495
  98:	sub	x3, x5, x4
  9c:	orr	x0, x0, #0x1000000000000
  a0:	cmp	x3, #0x3f
  a4:	b.gt	dc <__fixtfdi+0xdc>
  a8:	mov	w6, #0xffffbfd1            	// #-16431
  ac:	add	w19, w4, w6
  b0:	sub	w4, w5, w4
  b4:	lsl	x3, x2, x19
  b8:	cmp	x3, #0x0
  bc:	lsr	x2, x2, x4
  c0:	cset	w3, ne  // ne = any
  c4:	lsl	x19, x0, x19
  c8:	orr	x19, x2, x19
  cc:	cbz	w1, d4 <__fixtfdi+0xd4>
  d0:	neg	x19, x19
  d4:	cbz	w3, 3c <__fixtfdi+0x3c>
  d8:	b	8c <__fixtfdi+0x8c>
  dc:	mov	w5, #0xffffc011            	// #-16367
  e0:	add	w19, w4, w5
  e4:	cmp	x3, #0x40
  e8:	lsl	x19, x0, x19
  ec:	csel	x19, x19, xzr, ne  // ne = any
  f0:	orr	x2, x19, x2
  f4:	mov	w19, #0x402f                	// #16431
  f8:	sub	w19, w19, w4
  fc:	cmp	x2, #0x0
 100:	cset	w3, ne  // ne = any
 104:	lsr	x19, x0, x19
 108:	b	cc <__fixtfdi+0xcc>
 10c:	mov	x19, #0x0                   	// #0
 110:	b	8c <__fixtfdi+0x8c>
 114:	mov	w0, #0x1                   	// #1
 118:	bl	0 <__sfp_handle_exceptions>
 11c:	b	3c <__fixtfdi+0x3c>

fixunstfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldr	x19, [sp, #32]
  14:	ldr	x0, [sp, #40]
  18:	mrs	x1, fpcr
  1c:	ubfx	x3, x0, #48, #15
  20:	mov	x1, x19
  24:	mov	x4, #0x3ffe                	// #16382
  28:	ubfx	x19, x0, #0, #48
  2c:	cmp	x3, x4
  30:	b.gt	50 <__fixunstfdi+0x50>
  34:	cbnz	x3, f8 <__fixunstfdi+0xf8>
  38:	orr	x19, x1, x19
  3c:	cbnz	x19, f8 <__fixunstfdi+0xf8>
  40:	mov	x0, x19
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	lsr	x0, x0, #63
  54:	mov	x4, #0x403f                	// #16447
  58:	and	w0, w0, #0xff
  5c:	and	x5, x0, #0xff
  60:	sub	x4, x4, x5
  64:	cmp	x4, x3
  68:	b.le	b4 <__fixunstfdi+0xb4>
  6c:	cbnz	x5, 104 <__fixunstfdi+0x104>
  70:	mov	x4, #0x406f                	// #16495
  74:	sub	x2, x4, x3
  78:	orr	x0, x19, #0x1000000000000
  7c:	cmp	x2, #0x3f
  80:	b.gt	c8 <__fixunstfdi+0xc8>
  84:	mov	w5, #0xffffbfd1            	// #-16431
  88:	add	w5, w3, w5
  8c:	sub	w19, w4, w3
  90:	lsl	x2, x1, x5
  94:	cmp	x2, #0x0
  98:	lsr	x1, x1, x19
  9c:	cset	w2, ne  // ne = any
  a0:	lsl	x19, x0, x5
  a4:	orr	x19, x1, x19
  a8:	cbz	w2, 40 <__fixunstfdi+0x40>
  ac:	mov	w0, #0x10                  	// #16
  b0:	b	c0 <__fixunstfdi+0xc0>
  b4:	eor	w0, w0, #0x1
  b8:	sbfx	x19, x0, #0, #1
  bc:	mov	w0, #0x1                   	// #1
  c0:	bl	0 <__sfp_handle_exceptions>
  c4:	b	40 <__fixunstfdi+0x40>
  c8:	mov	w4, #0xffffc011            	// #-16367
  cc:	add	w19, w3, w4
  d0:	cmp	x2, #0x40
  d4:	lsl	x19, x0, x19
  d8:	csel	x19, x19, xzr, ne  // ne = any
  dc:	orr	x19, x19, x1
  e0:	cmp	x19, #0x0
  e4:	mov	w19, #0x402f                	// #16431
  e8:	sub	w19, w19, w3
  ec:	cset	w2, ne  // ne = any
  f0:	lsr	x19, x0, x19
  f4:	b	a8 <__fixunstfdi+0xa8>
  f8:	mov	w0, #0x10                  	// #16
  fc:	mov	x19, #0x0                   	// #0
 100:	b	c0 <__fixunstfdi+0xc0>
 104:	mov	w0, #0x1                   	// #1
 108:	b	fc <__fixunstfdi+0xfc>

floatditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	cmp	x0, #0x0
   4:	cbz	x0, 7c <__floatditf+0x7c>
   8:	lsr	x4, x0, #63
   c:	cneg	x0, x0, lt  // lt = tstop
  10:	clz	x1, x0
  14:	mov	w2, #0x403e                	// #16446
  18:	sub	w3, w2, w1
  1c:	mov	x2, #0x406f                	// #16495
  20:	sub	x1, x2, w3, sxtw
  24:	sxtw	x6, w3
  28:	cmp	x1, #0x3f
  2c:	b.gt	64 <__floatditf+0x64>
  30:	sub	w2, w2, w3
  34:	mov	w1, #0xffffbfd1            	// #-16431
  38:	add	w1, w3, w1
  3c:	lsl	x2, x0, x2
  40:	lsr	x1, x0, x1
  44:	mov	x0, x4
  48:	mov	x5, #0x0                   	// #0
  4c:	orr	w0, w6, w0, lsl #15
  50:	bfxil	x5, x1, #0, #48
  54:	fmov	d0, x2
  58:	bfi	x5, x0, #48, #16
  5c:	fmov	v0.d[1], x5
  60:	ret
  64:	mov	w1, #0x402f                	// #16431
  68:	sub	w1, w1, w3
  6c:	lsl	x1, x0, x1
  70:	mov	x0, x4
  74:	mov	x2, #0x0                   	// #0
  78:	b	48 <__floatditf+0x48>
  7c:	mov	x6, #0x0                   	// #0
  80:	mov	x1, #0x0                   	// #0
  84:	b	74 <__floatditf+0x74>

floatunditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	cbz	x0, 64 <__floatunditf+0x64>
   4:	clz	x2, x0
   8:	mov	w1, #0x403e                	// #16446
   c:	sub	w2, w1, w2
  10:	mov	x3, #0x406f                	// #16495
  14:	sub	x1, x3, w2, sxtw
  18:	sxtw	x4, w2
  1c:	cmp	x1, #0x3f
  20:	b.gt	50 <__floatunditf+0x50>
  24:	mov	w1, #0xffffbfd1            	// #-16431
  28:	add	w1, w2, w1
  2c:	sub	w2, w3, w2
  30:	lsr	x1, x0, x1
  34:	lsl	x0, x0, x2
  38:	mov	x3, #0x0                   	// #0
  3c:	fmov	d0, x0
  40:	bfxil	x3, x1, #0, #48
  44:	bfi	x3, x4, #48, #16
  48:	fmov	v0.d[1], x3
  4c:	ret
  50:	mov	w1, #0x402f                	// #16431
  54:	sub	w1, w1, w2
  58:	lsl	x1, x0, x1
  5c:	mov	x0, #0x0                   	// #0
  60:	b	38 <__floatunditf+0x38>
  64:	mov	x1, #0x0                   	// #0
  68:	mov	x4, #0x0                   	// #0
  6c:	b	38 <__floatunditf+0x38>

fixtfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	str	q0, [sp, #48]
  14:	ldp	x0, x4, [sp, #48]
  18:	mrs	x1, fpcr
  1c:	ubfx	x19, x4, #48, #15
  20:	mov	x2, #0x3ffe                	// #16382
  24:	ubfx	x1, x4, #0, #48
  28:	mov	x5, x19
  2c:	cmp	x19, x2
  30:	b.gt	60 <__fixtfti+0x60>
  34:	cbnz	x19, 16c <__fixtfti+0x16c>
  38:	orr	x1, x0, x1
  3c:	mov	x19, #0x0                   	// #0
  40:	mov	x20, #0x0                   	// #0
  44:	cbnz	x1, 12c <__fixtfti+0x12c>
  48:	mov	x0, x19
  4c:	mov	x1, x20
  50:	ldp	x19, x20, [sp, #16]
  54:	ldr	x21, [sp, #32]
  58:	ldp	x29, x30, [sp], #64
  5c:	ret
  60:	lsr	x4, x4, #63
  64:	mov	x2, #0x407d                	// #16509
  68:	and	w4, w4, #0xff
  6c:	cmp	x19, x2
  70:	and	x21, x4, #0xff
  74:	b.le	b4 <__fixtfti+0xb4>
  78:	adrp	x6, 0 <__fixtfti>
  7c:	mov	x2, #0x1                   	// #1
  80:	sub	x19, x2, x21
  84:	ldr	x20, [x6]
  88:	asr	x3, x19, #63
  8c:	negs	x19, x19
  90:	sbc	x20, x20, x3
  94:	mov	x3, #0x407e                	// #16510
  98:	cmp	x5, x3
  9c:	csel	w4, w4, wzr, eq  // eq = none
  a0:	cbz	w4, ac <__fixtfti+0xac>
  a4:	orr	x1, x0, x1
  a8:	cbz	x1, 48 <__fixtfti+0x48>
  ac:	mov	w0, w2
  b0:	b	130 <__fixtfti+0x130>
  b4:	mov	x2, #0x406e                	// #16494
  b8:	orr	x1, x1, #0x1000000000000
  bc:	cmp	x19, x2
  c0:	b.le	e4 <__fixtfti+0xe4>
  c4:	mov	w4, #0xffffbf91            	// #-16495
  c8:	add	w2, w19, w4
  cc:	bl	0 <__ashlti3>
  d0:	mov	x19, x0
  d4:	mov	x20, x1
  d8:	cbz	x21, 48 <__fixtfti+0x48>
  dc:	mov	w4, #0x0                   	// #0
  e0:	b	120 <__fixtfti+0x120>
  e4:	mov	x20, #0x406f                	// #16495
  e8:	sub	x5, x20, x19
  ec:	cmp	x5, #0x3f
  f0:	b.gt	138 <__fixtfti+0x138>
  f4:	mov	w3, #0xffffbfd1            	// #-16431
  f8:	add	w3, w19, w3
  fc:	sub	w20, w20, w19
 100:	lsl	x2, x0, x3
 104:	cmp	x2, #0x0
 108:	lsr	x19, x0, x20
 10c:	cset	w4, ne  // ne = any
 110:	lsl	x3, x1, x3
 114:	orr	x19, x3, x19
 118:	lsr	x20, x1, x20
 11c:	cbz	x21, 128 <__fixtfti+0x128>
 120:	negs	x19, x19
 124:	ngc	x20, x20
 128:	cbz	w4, 48 <__fixtfti+0x48>
 12c:	mov	w0, #0x10                  	// #16
 130:	bl	0 <__sfp_handle_exceptions>
 134:	b	48 <__fixtfti+0x48>
 138:	mov	w2, #0xffffc011            	// #-16367
 13c:	add	w4, w19, w2
 140:	cmp	x5, #0x40
 144:	mov	w3, #0x402f                	// #16431
 148:	lsl	x4, x1, x4
 14c:	csel	x4, x4, xzr, ne  // ne = any
 150:	orr	x0, x4, x0
 154:	sub	w19, w3, w19
 158:	cmp	x0, #0x0
 15c:	mov	x20, #0x0                   	// #0
 160:	cset	w4, ne  // ne = any
 164:	lsr	x19, x1, x19
 168:	b	11c <__fixtfti+0x11c>
 16c:	mov	x19, #0x0                   	// #0
 170:	mov	x20, #0x0                   	// #0
 174:	b	12c <__fixtfti+0x12c>

fixunstfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x4, [sp, #32]
  14:	mrs	x1, fpcr
  18:	ubfx	x2, x4, #48, #15
  1c:	mov	x3, #0x3ffe                	// #16382
  20:	ubfx	x1, x4, #0, #48
  24:	cmp	x2, x3
  28:	b.gt	54 <__fixunstfti+0x54>
  2c:	cbnz	x2, 130 <__fixunstfti+0x130>
  30:	orr	x1, x0, x1
  34:	cbnz	x1, 130 <__fixunstfti+0x130>
  38:	mov	x20, #0x0                   	// #0
  3c:	mov	x19, #0x0                   	// #0
  40:	mov	x0, x20
  44:	mov	x1, x19
  48:	ldp	x19, x20, [sp, #16]
  4c:	ldp	x29, x30, [sp], #48
  50:	ret
  54:	lsr	x4, x4, #63
  58:	mov	x3, #0x407f                	// #16511
  5c:	and	w4, w4, #0xff
  60:	and	x20, x4, #0xff
  64:	sub	x3, x3, x20
  68:	cmp	x3, x2
  6c:	b.le	a0 <__fixunstfti+0xa0>
  70:	cbnz	x20, 140 <__fixunstfti+0x140>
  74:	mov	x3, #0x406e                	// #16494
  78:	orr	x1, x1, #0x1000000000000
  7c:	cmp	x2, x3
  80:	b.le	b8 <__fixunstfti+0xb8>
  84:	mov	w5, #0xffffbf91            	// #-16495
  88:	adds	x0, x0, x20
  8c:	add	w2, w2, w5
  90:	bl	0 <__ashlti3>
  94:	mov	x20, x0
  98:	mov	x19, x1
  9c:	b	40 <__fixunstfti+0x40>
  a0:	eor	w4, w4, #0x1
  a4:	mov	w0, #0x1                   	// #1
  a8:	sbfx	x20, x4, #0, #1
  ac:	mov	x19, x20
  b0:	bl	0 <__sfp_handle_exceptions>
  b4:	b	40 <__fixunstfti+0x40>
  b8:	mov	x20, #0x406f                	// #16495
  bc:	sub	x5, x20, x2
  c0:	cmp	x5, #0x3f
  c4:	b.gt	fc <__fixunstfti+0xfc>
  c8:	mov	w4, #0xffffbfd1            	// #-16431
  cc:	add	w3, w2, w4
  d0:	sub	w19, w20, w2
  d4:	lsl	x4, x0, x3
  d8:	cmp	x4, #0x0
  dc:	lsr	x0, x0, x19
  e0:	cset	w4, ne  // ne = any
  e4:	lsl	x20, x1, x3
  e8:	orr	x20, x20, x0
  ec:	lsr	x19, x1, x19
  f0:	cbz	w4, 40 <__fixunstfti+0x40>
  f4:	mov	w0, #0x10                  	// #16
  f8:	b	b0 <__fixunstfti+0xb0>
  fc:	mov	w3, #0xffffc011            	// #-16367
 100:	add	w4, w2, w3
 104:	cmp	x5, #0x40
 108:	mov	w20, #0x402f                	// #16431
 10c:	lsl	x4, x1, x4
 110:	csel	x4, x4, xzr, ne  // ne = any
 114:	orr	x0, x4, x0
 118:	sub	w20, w20, w2
 11c:	cmp	x0, #0x0
 120:	mov	x19, #0x0                   	// #0
 124:	cset	w4, ne  // ne = any
 128:	lsr	x20, x1, x20
 12c:	b	f0 <__fixunstfti+0xf0>
 130:	mov	w0, #0x10                  	// #16
 134:	mov	x20, #0x0                   	// #0
 138:	mov	x19, #0x0                   	// #0
 13c:	b	b0 <__fixunstfti+0xb0>
 140:	mov	w0, #0x1                   	// #1
 144:	b	134 <__fixunstfti+0x134>

floattitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	stp	x29, x30, [sp, #-112]!
   4:	mov	x2, x0
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	stp	x21, x22, [sp, #32]
  14:	stp	x23, x24, [sp, #48]
  18:	stp	x25, x26, [sp, #64]
  1c:	str	x27, [sp, #80]
  20:	mrs	x25, fpcr
  24:	orr	x0, x0, x1
  28:	cbz	x0, 1f8 <__floattitf+0x1f8>
  2c:	mov	x22, x1
  30:	mov	x19, x2
  34:	mov	x21, x1
  38:	lsr	x24, x1, #63
  3c:	tbz	x1, #63, 4c <__floattitf+0x4c>
  40:	negs	x2, x2
  44:	ngc	x21, x1
  48:	mov	x19, x2
  4c:	clz	x20, x19
  50:	cmp	x21, #0x0
  54:	add	w20, w20, #0x40
  58:	clz	x0, x21
  5c:	csel	w0, w20, w0, eq  // eq = none
  60:	mov	w3, #0x407e                	// #16510
  64:	sub	w20, w3, w0
  68:	mov	x4, x21
  6c:	mov	w0, #0x406f                	// #16495
  70:	cmp	w20, w0
  74:	sxtw	x23, w20
  78:	b.gt	cc <__floattitf+0xcc>
  7c:	mov	x1, #0x406f                	// #16495
  80:	mov	x2, x19
  84:	subs	x3, x1, x23
  88:	b.eq	20c <__floattitf+0x20c>  // b.none
  8c:	cmp	x3, #0x3f
  90:	b.gt	b8 <__floattitf+0xb8>
  94:	sub	w0, w0, w20
  98:	mov	w3, #0xffffbfd1            	// #-16431
  9c:	add	w20, w20, w3
  a0:	lsl	x3, x21, x0
  a4:	lsr	x20, x19, x20
  a8:	orr	x4, x20, x3
  ac:	lsl	x2, x19, x0
  b0:	mov	w0, #0x0                   	// #0
  b4:	b	1b4 <__floattitf+0x1b4>
  b8:	mov	w3, #0x402f                	// #16431
  bc:	sub	w3, w3, w20
  c0:	mov	x2, #0x0                   	// #0
  c4:	lsl	x4, x19, x3
  c8:	b	b0 <__floattitf+0xb0>
  cc:	mov	w2, #0x4072                	// #16498
  d0:	cmp	w20, w2
  d4:	b.le	118 <__floattitf+0x118>
  d8:	sub	w2, w20, w2
  dc:	mov	x0, x19
  e0:	mov	x1, x21
  e4:	bl	0 <__lshrti3>
  e8:	mov	w2, #0x40f2                	// #16626
  ec:	mov	x27, x0
  f0:	mov	x26, x1
  f4:	sub	w2, w2, w20
  f8:	mov	x0, x19
  fc:	mov	x1, x21
 100:	bl	0 <__ashlti3>
 104:	orr	x0, x0, x1
 108:	cmp	x0, #0x0
 10c:	mov	x21, x26
 110:	cset	x2, ne  // ne = any
 114:	orr	x19, x2, x27
 118:	mov	x0, #0x4072                	// #16498
 11c:	sub	x1, x0, x23
 120:	mov	x2, x19
 124:	cmp	x1, #0x0
 128:	b.le	148 <__floattitf+0x148>
 12c:	sub	w0, w0, w20
 130:	mov	w1, #0xffffbfce            	// #-16434
 134:	add	w3, w20, w1
 138:	lsl	x21, x21, x0
 13c:	lsr	x3, x19, x3
 140:	orr	x21, x3, x21
 144:	lsl	x2, x19, x0
 148:	and	x3, x21, #0xfff7ffffffffffff
 14c:	tst	x2, #0x7
 150:	b.eq	19c <__floattitf+0x19c>  // b.none
 154:	and	x0, x25, #0xc00000
 158:	cmp	x0, #0x400, lsl #12
 15c:	b.eq	184 <__floattitf+0x184>  // b.none
 160:	cmp	x0, #0x800, lsl #12
 164:	b.eq	190 <__floattitf+0x190>  // b.none
 168:	cbnz	x0, 194 <__floattitf+0x194>
 16c:	and	x0, x2, #0xf
 170:	cmp	x0, #0x4
 174:	b.eq	194 <__floattitf+0x194>  // b.none
 178:	adds	x2, x2, #0x4
 17c:	cinc	x3, x3, cs  // cs = hs, nlast
 180:	b	194 <__floattitf+0x194>
 184:	tbnz	x22, #63, 194 <__floattitf+0x194>
 188:	adds	x2, x2, #0x8
 18c:	b	17c <__floattitf+0x17c>
 190:	tbnz	x22, #63, 188 <__floattitf+0x188>
 194:	mov	w0, #0x10                  	// #16
 198:	b	1a0 <__floattitf+0x1a0>
 19c:	mov	w0, #0x0                   	// #0
 1a0:	tbz	x3, #51, 1ac <__floattitf+0x1ac>
 1a4:	and	x3, x3, #0xfff7ffffffffffff
 1a8:	add	x23, x23, #0x1
 1ac:	lsr	x4, x3, #3
 1b0:	extr	x2, x3, x2, #3
 1b4:	mov	x7, #0x0                   	// #0
 1b8:	orr	w23, w23, w24, lsl #15
 1bc:	bfxil	x7, x4, #0, #48
 1c0:	fmov	d0, x2
 1c4:	bfi	x7, x23, #48, #16
 1c8:	fmov	v0.d[1], x7
 1cc:	cbz	w0, 1dc <__floattitf+0x1dc>
 1d0:	str	q0, [sp, #96]
 1d4:	bl	0 <__sfp_handle_exceptions>
 1d8:	ldr	q0, [sp, #96]
 1dc:	ldp	x19, x20, [sp, #16]
 1e0:	ldp	x21, x22, [sp, #32]
 1e4:	ldp	x23, x24, [sp, #48]
 1e8:	ldp	x25, x26, [sp, #64]
 1ec:	ldr	x27, [sp, #80]
 1f0:	ldp	x29, x30, [sp], #112
 1f4:	ret
 1f8:	mov	x4, #0x0                   	// #0
 1fc:	mov	x2, #0x0                   	// #0
 200:	mov	x23, #0x0                   	// #0
 204:	mov	x24, #0x0                   	// #0
 208:	b	b0 <__floattitf+0xb0>
 20c:	mov	x23, x1
 210:	b	b0 <__floattitf+0xb0>

floatuntitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	stp	x29, x30, [sp, #-96]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	str	x25, [sp, #64]
  18:	mrs	x23, fpcr
  1c:	orr	x2, x0, x1
  20:	cbz	x2, 1b4 <__floatuntitf+0x1b4>
  24:	clz	x20, x0
  28:	cmp	x1, #0x0
  2c:	add	w20, w20, #0x40
  30:	clz	x2, x1
  34:	csel	w2, w20, w2, eq  // eq = none
  38:	mov	w20, #0x407e                	// #16510
  3c:	sub	w20, w20, w2
  40:	mov	w4, #0x406f                	// #16495
  44:	mov	x19, x0
  48:	mov	x21, x1
  4c:	mov	x3, x1
  50:	sxtw	x22, w20
  54:	cmp	w20, w4
  58:	b.gt	ac <__floatuntitf+0xac>
  5c:	mov	x2, x0
  60:	mov	x0, #0x406f                	// #16495
  64:	subs	x1, x0, x22
  68:	b.eq	1c4 <__floatuntitf+0x1c4>  // b.none
  6c:	cmp	x1, #0x3f
  70:	b.gt	98 <__floatuntitf+0x98>
  74:	sub	w4, w4, w20
  78:	mov	w5, #0xffffbfd1            	// #-16431
  7c:	add	w20, w20, w5
  80:	lsl	x1, x21, x4
  84:	lsr	x20, x19, x20
  88:	orr	x3, x20, x1
  8c:	lsl	x2, x19, x4
  90:	mov	w0, #0x0                   	// #0
  94:	b	178 <__floatuntitf+0x178>
  98:	mov	w1, #0x402f                	// #16431
  9c:	sub	w1, w1, w20
  a0:	mov	x2, #0x0                   	// #0
  a4:	lsl	x3, x19, x1
  a8:	b	90 <__floatuntitf+0x90>
  ac:	mov	w24, #0x4072                	// #16498
  b0:	cmp	w20, w24
  b4:	b.le	e8 <__floatuntitf+0xe8>
  b8:	mov	w2, #0x40f2                	// #16626
  bc:	sub	w2, w2, w20
  c0:	bl	0 <__ashlti3>
  c4:	orr	x0, x0, x1
  c8:	cmp	x0, #0x0
  cc:	mov	x1, x21
  d0:	cset	x25, ne  // ne = any
  d4:	mov	x0, x19
  d8:	sub	w2, w20, w24
  dc:	bl	0 <__lshrti3>
  e0:	orr	x19, x25, x0
  e4:	mov	x21, x1
  e8:	mov	x0, #0x4072                	// #16498
  ec:	sub	x3, x0, x22
  f0:	mov	x2, x19
  f4:	mov	x1, x21
  f8:	cmp	x3, #0x0
  fc:	b.le	11c <__floatuntitf+0x11c>
 100:	sub	w0, w0, w20
 104:	mov	w3, #0xffffbfce            	// #-16434
 108:	lsl	x1, x21, x0
 10c:	add	w21, w20, w3
 110:	lsl	x2, x19, x0
 114:	lsr	x21, x19, x21
 118:	orr	x1, x21, x1
 11c:	and	x1, x1, #0xfff7ffffffffffff
 120:	tst	x2, #0x7
 124:	b.eq	160 <__floatuntitf+0x160>  // b.none
 128:	ands	x0, x23, #0xc00000
 12c:	b.eq	140 <__floatuntitf+0x140>  // b.none
 130:	cmp	x0, #0x400, lsl #12
 134:	b.eq	158 <__floatuntitf+0x158>  // b.none
 138:	mov	w0, #0x10                  	// #16
 13c:	b	164 <__floatuntitf+0x164>
 140:	and	x0, x2, #0xf
 144:	cmp	x0, #0x4
 148:	b.eq	138 <__floatuntitf+0x138>  // b.none
 14c:	adds	x2, x2, #0x4
 150:	cinc	x1, x1, cs  // cs = hs, nlast
 154:	b	138 <__floatuntitf+0x138>
 158:	adds	x2, x2, #0x8
 15c:	b	150 <__floatuntitf+0x150>
 160:	mov	w0, #0x0                   	// #0
 164:	tbz	x1, #51, 170 <__floatuntitf+0x170>
 168:	and	x1, x1, #0xfff7ffffffffffff
 16c:	add	x22, x22, #0x1
 170:	lsr	x3, x1, #3
 174:	extr	x2, x1, x2, #3
 178:	mov	x5, #0x0                   	// #0
 17c:	fmov	d0, x2
 180:	bfxil	x5, x3, #0, #48
 184:	bfi	x5, x22, #48, #16
 188:	fmov	v0.d[1], x5
 18c:	cbz	w0, 19c <__floatuntitf+0x19c>
 190:	str	q0, [sp, #80]
 194:	bl	0 <__sfp_handle_exceptions>
 198:	ldr	q0, [sp, #80]
 19c:	ldp	x19, x20, [sp, #16]
 1a0:	ldp	x21, x22, [sp, #32]
 1a4:	ldp	x23, x24, [sp, #48]
 1a8:	ldr	x25, [sp, #64]
 1ac:	ldp	x29, x30, [sp], #96
 1b0:	ret
 1b4:	mov	x3, #0x0                   	// #0
 1b8:	mov	x2, #0x0                   	// #0
 1bc:	mov	x22, #0x0                   	// #0
 1c0:	b	90 <__floatuntitf+0x90>
 1c4:	mov	x22, x0
 1c8:	b	90 <__floatuntitf+0x90>

extendsftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	w0, s0
   8:	ubfx	x5, x0, #23, #8
   c:	ubfx	x3, x0, #0, #23
  10:	add	x1, x5, #0x1
  14:	and	x2, x0, #0x7fffff
  18:	ands	x1, x1, #0xfe
  1c:	lsr	w0, w0, #31
  20:	b.eq	38 <__extendsftf2+0x38>  // b.none
  24:	mov	x1, #0x3f80                	// #16256
  28:	lsl	x2, x2, #25
  2c:	add	x1, x5, x1
  30:	mov	w3, #0x0                   	// #0
  34:	b	74 <__extendsftf2+0x74>
  38:	cbnz	w5, 5c <__extendsftf2+0x5c>
  3c:	cbz	x2, 30 <__extendsftf2+0x30>
  40:	clz	x3, x2
  44:	sub	w1, w3, #0xf
  48:	lsl	x2, x2, x1
  4c:	mov	w1, #0x3fa9                	// #16297
  50:	sub	w1, w1, w3
  54:	sxtw	x1, w1
  58:	b	30 <__extendsftf2+0x30>
  5c:	cbz	x2, b4 <__extendsftf2+0xb4>
  60:	lsr	w3, w3, #22
  64:	lsl	x2, x2, #25
  68:	eor	w3, w3, #0x1
  6c:	orr	x2, x2, #0x800000000000
  70:	mov	x1, #0x7fff                	// #32767
  74:	mov	x5, #0x0                   	// #0
  78:	orr	w0, w1, w0, lsl #15
  7c:	bfxil	x5, x2, #0, #48
  80:	mov	x4, #0x0                   	// #0
  84:	fmov	d0, x4
  88:	bfi	x5, x0, #48, #16
  8c:	fmov	v0.d[1], x5
  90:	cbz	w3, bc <__extendsftf2+0xbc>
  94:	stp	x29, x30, [sp, #-32]!
  98:	mov	w0, #0x1                   	// #1
  9c:	mov	x29, sp
  a0:	str	q0, [sp, #16]
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	ldr	q0, [sp, #16]
  ac:	ldp	x29, x30, [sp], #32
  b0:	ret
  b4:	mov	x1, #0x7fff                	// #32767
  b8:	b	30 <__extendsftf2+0x30>
  bc:	ret

extenddftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	x0, d0
   8:	ubfx	x3, x0, #52, #11
   c:	ubfx	x1, x0, #0, #52
  10:	lsr	x0, x0, #63
  14:	and	w2, w0, #0xff
  18:	add	x0, x3, #0x1
  1c:	tst	x0, #0x7fe
  20:	b.eq	3c <__extenddftf2+0x3c>  // b.none
  24:	mov	x0, #0x3c00                	// #15360
  28:	add	x3, x3, x0
  2c:	lsr	x0, x1, #4
  30:	lsl	x1, x1, #60
  34:	mov	w4, #0x0                   	// #0
  38:	b	a4 <__extenddftf2+0xa4>
  3c:	cbnz	x3, 84 <__extenddftf2+0x84>
  40:	cbz	x1, e0 <__extenddftf2+0xe0>
  44:	clz	x4, x1
  48:	cmp	w4, #0xe
  4c:	b.gt	74 <__extenddftf2+0x74>
  50:	add	w3, w4, #0x31
  54:	mov	w0, #0xf                   	// #15
  58:	sub	w0, w0, w4
  5c:	lsr	x0, x1, x0
  60:	lsl	x1, x1, x3
  64:	mov	w3, #0x3c0c                	// #15372
  68:	sub	w3, w3, w4
  6c:	sxtw	x3, w3
  70:	b	34 <__extenddftf2+0x34>
  74:	sub	w0, w4, #0xf
  78:	lsl	x0, x1, x0
  7c:	mov	x1, #0x0                   	// #0
  80:	b	64 <__extenddftf2+0x64>
  84:	cbz	x1, e8 <__extenddftf2+0xe8>
  88:	lsr	x4, x1, #51
  8c:	lsr	x0, x1, #4
  90:	eor	x4, x4, #0x1
  94:	orr	x0, x0, #0x800000000000
  98:	and	w4, w4, #0x1
  9c:	lsl	x1, x1, #60
  a0:	mov	x3, #0x7fff                	// #32767
  a4:	mov	x7, #0x0                   	// #0
  a8:	fmov	d0, x1
  ac:	bfxil	x7, x0, #0, #48
  b0:	orr	w0, w3, w2, lsl #15
  b4:	bfi	x7, x0, #48, #16
  b8:	fmov	v0.d[1], x7
  bc:	cbz	w4, f4 <__extenddftf2+0xf4>
  c0:	stp	x29, x30, [sp, #-32]!
  c4:	mov	w0, #0x1                   	// #1
  c8:	mov	x29, sp
  cc:	str	q0, [sp, #16]
  d0:	bl	0 <__sfp_handle_exceptions>
  d4:	ldr	q0, [sp, #16]
  d8:	ldp	x29, x30, [sp], #32
  dc:	ret
  e0:	mov	x0, #0x0                   	// #0
  e4:	b	34 <__extenddftf2+0x34>
  e8:	mov	x0, #0x0                   	// #0
  ec:	mov	x3, #0x7fff                	// #32767
  f0:	b	34 <__extenddftf2+0x34>
  f4:	ret

extendhftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhftf2>:
   0:	mrs	x0, fpcr
   4:	umov	w0, v0.h[0]
   8:	mov	w3, #0x0                   	// #0
   c:	bfxil	w3, w0, #0, #16
  10:	and	w2, w3, #0x3ff
  14:	and	x1, x3, #0x3ff
  18:	ubfx	x0, x3, #10, #5
  1c:	ubfx	x3, x3, #15, #1
  20:	add	x4, x0, #0x1
  24:	tst	x4, #0x1e
  28:	b.eq	40 <__extendhftf2+0x40>  // b.none
  2c:	mov	x2, #0x3ff0                	// #16368
  30:	lsl	x1, x1, #38
  34:	add	x0, x0, x2
  38:	mov	w2, #0x0                   	// #0
  3c:	b	7c <__extendhftf2+0x7c>
  40:	cbnz	x0, 64 <__extendhftf2+0x64>
  44:	cbz	x1, 38 <__extendhftf2+0x38>
  48:	clz	x2, x1
  4c:	sub	w0, w2, #0xf
  50:	lsl	x1, x1, x0
  54:	mov	w0, #0x4026                	// #16422
  58:	sub	w0, w0, w2
  5c:	sxtw	x0, w0
  60:	b	38 <__extendhftf2+0x38>
  64:	cbz	x1, bc <__extendhftf2+0xbc>
  68:	lsr	w2, w2, #9
  6c:	lsl	x1, x1, #38
  70:	eor	w2, w2, #0x1
  74:	orr	x1, x1, #0x800000000000
  78:	mov	x0, #0x7fff                	// #32767
  7c:	mov	x5, #0x0                   	// #0
  80:	orr	w0, w0, w3, lsl #15
  84:	bfxil	x5, x1, #0, #48
  88:	mov	x4, #0x0                   	// #0
  8c:	fmov	d0, x4
  90:	bfi	x5, x0, #48, #16
  94:	fmov	v0.d[1], x5
  98:	cbz	w2, c4 <__extendhftf2+0xc4>
  9c:	stp	x29, x30, [sp, #-32]!
  a0:	mov	w0, #0x1                   	// #1
  a4:	mov	x29, sp
  a8:	str	q0, [sp, #16]
  ac:	bl	0 <__sfp_handle_exceptions>
  b0:	ldr	q0, [sp, #16]
  b4:	ldp	x29, x30, [sp], #32
  b8:	ret
  bc:	mov	x0, #0x7fff                	// #32767
  c0:	b	38 <__extendhftf2+0x38>
  c4:	ret

trunctfsf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x1, x3, [sp, #32]
  14:	mrs	x4, fpcr
  18:	ubfx	x2, x3, #48, #15
  1c:	lsr	x19, x3, #63
  20:	add	x0, x2, #0x1
  24:	ubfiz	x3, x3, #3, #48
  28:	orr	x3, x3, x1, lsr #61
  2c:	and	w19, w19, #0xff
  30:	lsl	x1, x1, #3
  34:	tst	x0, #0x7ffe
  38:	b.eq	f0 <__trunctfsf2+0xf0>  // b.none
  3c:	mov	x0, #0xffffffffffffc080    	// #-16256
  40:	add	x2, x2, x0
  44:	cmp	x2, #0xfe
  48:	b.le	98 <__trunctfsf2+0x98>
  4c:	ands	x1, x4, #0xc00000
  50:	b.eq	178 <__trunctfsf2+0x178>  // b.none
  54:	cmp	x1, #0x400, lsl #12
  58:	b.ne	78 <__trunctfsf2+0x78>  // b.any
  5c:	cmp	w19, #0x0
  60:	mov	x2, #0xff                  	// #255
  64:	mov	x0, #0xfe                  	// #254
  68:	csetm	x1, ne  // ne = any
  6c:	csel	x2, x2, x0, eq  // eq = none
  70:	mov	w0, #0x14                  	// #20
  74:	b	12c <__trunctfsf2+0x12c>
  78:	cmp	x1, #0x800, lsl #12
  7c:	b.ne	180 <__trunctfsf2+0x180>  // b.any
  80:	cmp	w19, #0x0
  84:	mov	x2, #0xff                  	// #255
  88:	mov	x0, #0xfe                  	// #254
  8c:	csetm	x1, eq  // eq = none
  90:	csel	x2, x2, x0, ne  // ne = any
  94:	b	70 <__trunctfsf2+0x70>
  98:	cmp	x2, #0x0
  9c:	b.gt	d8 <__trunctfsf2+0xd8>
  a0:	cmn	x2, #0x17
  a4:	b.lt	18c <__trunctfsf2+0x18c>  // b.tstop
  a8:	orr	x3, x3, #0x8000000000000
  ac:	add	w0, w2, #0x26
  b0:	lsl	x0, x3, x0
  b4:	orr	x1, x0, x1
  b8:	cmp	x1, #0x0
  bc:	mov	w0, #0x1a                  	// #26
  c0:	sub	w2, w0, w2
  c4:	cset	x1, ne  // ne = any
  c8:	lsr	x3, x3, x2
  cc:	orr	x1, x3, x1
  d0:	mov	x2, #0x0                   	// #0
  d4:	b	e8 <__trunctfsf2+0xe8>
  d8:	orr	x1, x1, x3, lsl #39
  dc:	cmp	x1, #0x0
  e0:	cset	x1, ne  // ne = any
  e4:	orr	x1, x1, x3, lsr #25
  e8:	mov	w0, #0x0                   	// #0
  ec:	b	12c <__trunctfsf2+0x12c>
  f0:	orr	x1, x3, x1
  f4:	cbnz	x2, 104 <__trunctfsf2+0x104>
  f8:	cmp	x1, #0x0
  fc:	cset	x1, ne  // ne = any
 100:	b	e8 <__trunctfsf2+0xe8>
 104:	cbz	x1, 194 <__trunctfsf2+0x194>
 108:	mov	x1, #0x7fff                	// #32767
 10c:	lsr	x0, x3, #50
 110:	cmp	x2, x1
 114:	lsr	x1, x3, #25
 118:	eor	w0, w0, #0x1
 11c:	and	x1, x1, #0xfffffffffffffff8
 120:	csel	w0, w0, wzr, eq  // eq = none
 124:	orr	x1, x1, #0x2000000
 128:	mov	x2, #0xff                  	// #255
 12c:	cmp	x2, #0x0
 130:	cset	w3, eq  // eq = none
 134:	cmp	x1, #0x0
 138:	csel	w3, w3, wzr, ne  // ne = any
 13c:	tst	x1, #0x7
 140:	b.eq	1b0 <__trunctfsf2+0x1b0>  // b.none
 144:	and	x5, x4, #0xc00000
 148:	orr	w0, w0, #0x10
 14c:	cmp	x5, #0x400, lsl #12
 150:	b.eq	19c <__trunctfsf2+0x19c>  // b.none
 154:	cmp	x5, #0x800, lsl #12
 158:	b.eq	1a8 <__trunctfsf2+0x1a8>  // b.none
 15c:	cbnz	x5, 170 <__trunctfsf2+0x170>
 160:	and	x5, x1, #0xf
 164:	cmp	x5, #0x4
 168:	b.eq	170 <__trunctfsf2+0x170>  // b.none
 16c:	add	x1, x1, #0x4
 170:	cbz	w3, 1c0 <__trunctfsf2+0x1c0>
 174:	b	1bc <__trunctfsf2+0x1bc>
 178:	mov	x2, #0xff                  	// #255
 17c:	b	70 <__trunctfsf2+0x70>
 180:	mov	x1, #0xffffffffffffffff    	// #-1
 184:	mov	x2, #0xfe                  	// #254
 188:	b	70 <__trunctfsf2+0x70>
 18c:	mov	x1, #0x1                   	// #1
 190:	b	d0 <__trunctfsf2+0xd0>
 194:	mov	x2, #0xff                  	// #255
 198:	b	e8 <__trunctfsf2+0xe8>
 19c:	cbnz	w19, 170 <__trunctfsf2+0x170>
 1a0:	add	x1, x1, #0x8
 1a4:	b	170 <__trunctfsf2+0x170>
 1a8:	cbz	w19, 170 <__trunctfsf2+0x170>
 1ac:	b	1a0 <__trunctfsf2+0x1a0>
 1b0:	cbz	w3, 1c0 <__trunctfsf2+0x1c0>
 1b4:	tbnz	w0, #4, 1bc <__trunctfsf2+0x1bc>
 1b8:	tbz	w4, #11, 1c0 <__trunctfsf2+0x1c0>
 1bc:	orr	w0, w0, #0x8
 1c0:	tbz	w1, #26, 1d4 <__trunctfsf2+0x1d4>
 1c4:	add	x2, x2, #0x1
 1c8:	cmp	x2, #0xff
 1cc:	b.eq	214 <__trunctfsf2+0x214>  // b.none
 1d0:	and	x1, x1, #0xfffffffffbffffff
 1d4:	lsr	x1, x1, #3
 1d8:	cmp	x2, #0xff
 1dc:	ccmp	x1, #0x0, #0x4, eq  // eq = none
 1e0:	mov	x3, x1
 1e4:	orr	x1, x1, #0x400000
 1e8:	ubfiz	w2, w2, #23, #8
 1ec:	csel	x1, x1, x3, ne  // ne = any
 1f0:	and	x1, x1, #0x7fffff
 1f4:	orr	w1, w2, w1
 1f8:	orr	w19, w1, w19, lsl #31
 1fc:	cbz	w0, 204 <__trunctfsf2+0x204>
 200:	bl	0 <__sfp_handle_exceptions>
 204:	fmov	s0, w19
 208:	ldr	x19, [sp, #16]
 20c:	ldp	x29, x30, [sp], #48
 210:	ret
 214:	ands	x1, x4, #0xc00000
 218:	b.eq	234 <__trunctfsf2+0x234>  // b.none
 21c:	cmp	x1, #0x400, lsl #12
 220:	b.ne	240 <__trunctfsf2+0x240>  // b.any
 224:	cmp	w19, #0x0
 228:	mov	x3, #0xfe                  	// #254
 22c:	csetm	x1, ne  // ne = any
 230:	csel	x2, x2, x3, eq  // eq = none
 234:	mov	w3, #0x14                  	// #20
 238:	orr	w0, w0, w3
 23c:	b	1d4 <__trunctfsf2+0x1d4>
 240:	cmp	x1, #0x800, lsl #12
 244:	b.ne	25c <__trunctfsf2+0x25c>  // b.any
 248:	cmp	w19, #0x0
 24c:	mov	x3, #0xfe                  	// #254
 250:	csetm	x1, eq  // eq = none
 254:	csel	x2, x2, x3, ne  // ne = any
 258:	b	234 <__trunctfsf2+0x234>
 25c:	mov	x1, #0xffffffffffffffff    	// #-1
 260:	mov	x2, #0xfe                  	// #254
 264:	b	234 <__trunctfsf2+0x234>

trunctfdf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x3, [sp, #32]
  14:	mrs	x6, fpcr
  18:	ubfx	x2, x3, #48, #15
  1c:	lsr	x4, x3, #63
  20:	add	x1, x2, #0x1
  24:	ubfiz	x3, x3, #3, #48
  28:	and	w4, w4, #0xff
  2c:	orr	x3, x3, x0, lsr #61
  30:	lsl	x5, x0, #3
  34:	tst	x1, #0x7ffe
  38:	b.eq	130 <__trunctfdf2+0x130>  // b.none
  3c:	mov	x1, #0xffffffffffffc400    	// #-15360
  40:	add	x2, x2, x1
  44:	cmp	x2, #0x7fe
  48:	b.le	98 <__trunctfdf2+0x98>
  4c:	ands	x1, x6, #0xc00000
  50:	b.eq	1b8 <__trunctfdf2+0x1b8>  // b.none
  54:	cmp	x1, #0x400, lsl #12
  58:	b.ne	78 <__trunctfdf2+0x78>  // b.any
  5c:	cmp	w4, #0x0
  60:	mov	x2, #0x7ff                 	// #2047
  64:	mov	x0, #0x7fe                 	// #2046
  68:	csetm	x1, ne  // ne = any
  6c:	csel	x2, x2, x0, eq  // eq = none
  70:	mov	w0, #0x14                  	// #20
  74:	b	16c <__trunctfdf2+0x16c>
  78:	cmp	x1, #0x800, lsl #12
  7c:	b.ne	1c0 <__trunctfdf2+0x1c0>  // b.any
  80:	cmp	w4, #0x0
  84:	mov	x2, #0x7ff                 	// #2047
  88:	mov	x0, #0x7fe                 	// #2046
  8c:	csetm	x1, eq  // eq = none
  90:	csel	x2, x2, x0, ne  // ne = any
  94:	b	70 <__trunctfdf2+0x70>
  98:	cmp	x2, #0x0
  9c:	b.gt	118 <__trunctfdf2+0x118>
  a0:	cmn	x2, #0x34
  a4:	b.lt	1cc <__trunctfdf2+0x1cc>  // b.tstop
  a8:	mov	x0, #0x3d                  	// #61
  ac:	sub	x8, x0, x2
  b0:	orr	x3, x3, #0x8000000000000
  b4:	cmp	x8, #0x3f
  b8:	b.gt	e8 <__trunctfdf2+0xe8>
  bc:	add	w7, w2, #0x3
  c0:	sub	w1, w0, w2
  c4:	lsr	x1, x5, x1
  c8:	lsl	x5, x5, x7
  cc:	cmp	x5, #0x0
  d0:	cset	x0, ne  // ne = any
  d4:	lsl	x3, x3, x7
  d8:	orr	x1, x1, x0
  dc:	orr	x1, x3, x1
  e0:	mov	x2, #0x0                   	// #0
  e4:	b	128 <__trunctfdf2+0x128>
  e8:	add	w0, w2, #0x43
  ec:	mov	w1, #0xfffffffd            	// #-3
  f0:	sub	w1, w1, w2
  f4:	cmp	x8, #0x40
  f8:	lsr	x1, x3, x1
  fc:	lsl	x3, x3, x0
 100:	csel	x3, x3, xzr, ne  // ne = any
 104:	orr	x3, x3, x5
 108:	cmp	x3, #0x0
 10c:	cset	x0, ne  // ne = any
 110:	orr	x1, x1, x0
 114:	b	e0 <__trunctfdf2+0xe0>
 118:	cmp	xzr, x0, lsl #7
 11c:	cset	x1, ne  // ne = any
 120:	orr	x1, x1, x5, lsr #60
 124:	orr	x1, x1, x3, lsl #4
 128:	mov	w0, #0x0                   	// #0
 12c:	b	16c <__trunctfdf2+0x16c>
 130:	orr	x1, x3, x5
 134:	cbnz	x2, 144 <__trunctfdf2+0x144>
 138:	cmp	x1, #0x0
 13c:	cset	x1, ne  // ne = any
 140:	b	128 <__trunctfdf2+0x128>
 144:	cbz	x1, 1d4 <__trunctfdf2+0x1d4>
 148:	mov	x1, #0x7fff                	// #32767
 14c:	lsr	x0, x3, #50
 150:	cmp	x2, x1
 154:	extr	x1, x3, x5, #60
 158:	eor	w0, w0, #0x1
 15c:	and	x1, x1, #0xfffffffffffffff8
 160:	csel	w0, w0, wzr, eq  // eq = none
 164:	orr	x1, x1, #0x40000000000000
 168:	mov	x2, #0x7ff                 	// #2047
 16c:	cmp	x2, #0x0
 170:	cset	w3, eq  // eq = none
 174:	cmp	x1, #0x0
 178:	csel	w3, w3, wzr, ne  // ne = any
 17c:	tst	x1, #0x7
 180:	b.eq	1f0 <__trunctfdf2+0x1f0>  // b.none
 184:	and	x5, x6, #0xc00000
 188:	orr	w0, w0, #0x10
 18c:	cmp	x5, #0x400, lsl #12
 190:	b.eq	1dc <__trunctfdf2+0x1dc>  // b.none
 194:	cmp	x5, #0x800, lsl #12
 198:	b.eq	1e8 <__trunctfdf2+0x1e8>  // b.none
 19c:	cbnz	x5, 1b0 <__trunctfdf2+0x1b0>
 1a0:	and	x5, x1, #0xf
 1a4:	cmp	x5, #0x4
 1a8:	b.eq	1b0 <__trunctfdf2+0x1b0>  // b.none
 1ac:	add	x1, x1, #0x4
 1b0:	cbz	w3, 200 <__trunctfdf2+0x200>
 1b4:	b	1fc <__trunctfdf2+0x1fc>
 1b8:	mov	x2, #0x7ff                 	// #2047
 1bc:	b	70 <__trunctfdf2+0x70>
 1c0:	mov	x1, #0xffffffffffffffff    	// #-1
 1c4:	mov	x2, #0x7fe                 	// #2046
 1c8:	b	70 <__trunctfdf2+0x70>
 1cc:	mov	x1, #0x1                   	// #1
 1d0:	b	e0 <__trunctfdf2+0xe0>
 1d4:	mov	x2, #0x7ff                 	// #2047
 1d8:	b	128 <__trunctfdf2+0x128>
 1dc:	cbnz	w4, 1b0 <__trunctfdf2+0x1b0>
 1e0:	add	x1, x1, #0x8
 1e4:	b	1b0 <__trunctfdf2+0x1b0>
 1e8:	cbz	w4, 1b0 <__trunctfdf2+0x1b0>
 1ec:	b	1e0 <__trunctfdf2+0x1e0>
 1f0:	cbz	w3, 200 <__trunctfdf2+0x200>
 1f4:	tbnz	w0, #4, 1fc <__trunctfdf2+0x1fc>
 1f8:	tbz	w6, #11, 200 <__trunctfdf2+0x200>
 1fc:	orr	w0, w0, #0x8
 200:	tbz	x1, #55, 214 <__trunctfdf2+0x214>
 204:	add	x2, x2, #0x1
 208:	cmp	x2, #0x7ff
 20c:	b.eq	258 <__trunctfdf2+0x258>  // b.none
 210:	and	x1, x1, #0xff7fffffffffffff
 214:	lsr	x1, x1, #3
 218:	cmp	x2, #0x7ff
 21c:	ccmp	x1, #0x0, #0x4, eq  // eq = none
 220:	mov	x3, x1
 224:	orr	x1, x1, #0x8000000000000
 228:	ubfiz	x2, x2, #52, #11
 22c:	csel	x1, x1, x3, ne  // ne = any
 230:	and	x4, x4, #0xff
 234:	and	x1, x1, #0xfffffffffffff
 238:	orr	x1, x2, x1
 23c:	orr	x19, x1, x4, lsl #63
 240:	cbz	w0, 248 <__trunctfdf2+0x248>
 244:	bl	0 <__sfp_handle_exceptions>
 248:	fmov	d0, x19
 24c:	ldr	x19, [sp, #16]
 250:	ldp	x29, x30, [sp], #48
 254:	ret
 258:	ands	x1, x6, #0xc00000
 25c:	b.eq	278 <__trunctfdf2+0x278>  // b.none
 260:	cmp	x1, #0x400, lsl #12
 264:	b.ne	284 <__trunctfdf2+0x284>  // b.any
 268:	cmp	w4, #0x0
 26c:	mov	x3, #0x7fe                 	// #2046
 270:	csetm	x1, ne  // ne = any
 274:	csel	x2, x2, x3, eq  // eq = none
 278:	mov	w3, #0x14                  	// #20
 27c:	orr	w0, w0, w3
 280:	b	214 <__trunctfdf2+0x214>
 284:	cmp	x1, #0x800, lsl #12
 288:	b.ne	2a0 <__trunctfdf2+0x2a0>  // b.any
 28c:	cmp	w4, #0x0
 290:	mov	x3, #0x7fe                 	// #2046
 294:	csetm	x1, eq  // eq = none
 298:	csel	x2, x2, x3, ne  // ne = any
 29c:	b	278 <__trunctfdf2+0x278>
 2a0:	mov	x1, #0xffffffffffffffff    	// #-1
 2a4:	mov	x2, #0x7fe                 	// #2046
 2a8:	b	278 <__trunctfdf2+0x278>

trunctfhf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfhf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x1, x3, [sp, #32]
  14:	mrs	x4, fpcr
  18:	ubfx	x2, x3, #48, #15
  1c:	lsr	x19, x3, #63
  20:	add	x0, x2, #0x1
  24:	ubfiz	x3, x3, #3, #48
  28:	orr	x3, x3, x1, lsr #61
  2c:	and	w19, w19, #0xff
  30:	lsl	x1, x1, #3
  34:	tst	x0, #0x7ffe
  38:	b.eq	f0 <__trunctfhf2+0xf0>  // b.none
  3c:	mov	x0, #0xffffffffffffc010    	// #-16368
  40:	add	x2, x2, x0
  44:	cmp	x2, #0x1e
  48:	b.le	98 <__trunctfhf2+0x98>
  4c:	ands	x1, x4, #0xc00000
  50:	b.eq	178 <__trunctfhf2+0x178>  // b.none
  54:	cmp	x1, #0x400, lsl #12
  58:	b.ne	78 <__trunctfhf2+0x78>  // b.any
  5c:	cmp	w19, #0x0
  60:	mov	x2, #0x1f                  	// #31
  64:	mov	x0, #0x1e                  	// #30
  68:	csetm	x1, ne  // ne = any
  6c:	csel	x2, x2, x0, eq  // eq = none
  70:	mov	w0, #0x14                  	// #20
  74:	b	12c <__trunctfhf2+0x12c>
  78:	cmp	x1, #0x800, lsl #12
  7c:	b.ne	180 <__trunctfhf2+0x180>  // b.any
  80:	cmp	w19, #0x0
  84:	mov	x2, #0x1f                  	// #31
  88:	mov	x0, #0x1e                  	// #30
  8c:	csetm	x1, eq  // eq = none
  90:	csel	x2, x2, x0, ne  // ne = any
  94:	b	70 <__trunctfhf2+0x70>
  98:	cmp	x2, #0x0
  9c:	b.gt	d8 <__trunctfhf2+0xd8>
  a0:	cmn	x2, #0xa
  a4:	b.lt	18c <__trunctfhf2+0x18c>  // b.tstop
  a8:	orr	x3, x3, #0x8000000000000
  ac:	add	w0, w2, #0x19
  b0:	lsl	x0, x3, x0
  b4:	orr	x1, x0, x1
  b8:	cmp	x1, #0x0
  bc:	mov	w0, #0x27                  	// #39
  c0:	sub	w2, w0, w2
  c4:	cset	x1, ne  // ne = any
  c8:	lsr	x3, x3, x2
  cc:	orr	x1, x3, x1
  d0:	mov	x2, #0x0                   	// #0
  d4:	b	e8 <__trunctfhf2+0xe8>
  d8:	orr	x1, x1, x3, lsl #26
  dc:	cmp	x1, #0x0
  e0:	cset	x1, ne  // ne = any
  e4:	orr	x1, x1, x3, lsr #38
  e8:	mov	w0, #0x0                   	// #0
  ec:	b	12c <__trunctfhf2+0x12c>
  f0:	orr	x1, x3, x1
  f4:	cbnz	x2, 104 <__trunctfhf2+0x104>
  f8:	cmp	x1, #0x0
  fc:	cset	x1, ne  // ne = any
 100:	b	e8 <__trunctfhf2+0xe8>
 104:	cbz	x1, 194 <__trunctfhf2+0x194>
 108:	mov	x1, #0x7fff                	// #32767
 10c:	lsr	x0, x3, #50
 110:	cmp	x2, x1
 114:	lsr	x1, x3, #38
 118:	eor	w0, w0, #0x1
 11c:	and	x1, x1, #0xfffffffffffffff8
 120:	csel	w0, w0, wzr, eq  // eq = none
 124:	orr	x1, x1, #0x1000
 128:	mov	x2, #0x1f                  	// #31
 12c:	cmp	x2, #0x0
 130:	cset	w3, eq  // eq = none
 134:	cmp	x1, #0x0
 138:	csel	w3, w3, wzr, ne  // ne = any
 13c:	tst	x1, #0x7
 140:	b.eq	1b0 <__trunctfhf2+0x1b0>  // b.none
 144:	and	x5, x4, #0xc00000
 148:	orr	w0, w0, #0x10
 14c:	cmp	x5, #0x400, lsl #12
 150:	b.eq	19c <__trunctfhf2+0x19c>  // b.none
 154:	cmp	x5, #0x800, lsl #12
 158:	b.eq	1a8 <__trunctfhf2+0x1a8>  // b.none
 15c:	cbnz	x5, 170 <__trunctfhf2+0x170>
 160:	and	x5, x1, #0xf
 164:	cmp	x5, #0x4
 168:	b.eq	170 <__trunctfhf2+0x170>  // b.none
 16c:	add	x1, x1, #0x4
 170:	cbz	w3, 1c0 <__trunctfhf2+0x1c0>
 174:	b	1bc <__trunctfhf2+0x1bc>
 178:	mov	x2, #0x1f                  	// #31
 17c:	b	70 <__trunctfhf2+0x70>
 180:	mov	x1, #0xffffffffffffffff    	// #-1
 184:	mov	x2, #0x1e                  	// #30
 188:	b	70 <__trunctfhf2+0x70>
 18c:	mov	x1, #0x1                   	// #1
 190:	b	d0 <__trunctfhf2+0xd0>
 194:	mov	x2, #0x1f                  	// #31
 198:	b	e8 <__trunctfhf2+0xe8>
 19c:	cbnz	w19, 170 <__trunctfhf2+0x170>
 1a0:	add	x1, x1, #0x8
 1a4:	b	170 <__trunctfhf2+0x170>
 1a8:	cbz	w19, 170 <__trunctfhf2+0x170>
 1ac:	b	1a0 <__trunctfhf2+0x1a0>
 1b0:	cbz	w3, 1c0 <__trunctfhf2+0x1c0>
 1b4:	tbnz	w0, #4, 1bc <__trunctfhf2+0x1bc>
 1b8:	tbz	w4, #11, 1c0 <__trunctfhf2+0x1c0>
 1bc:	orr	w0, w0, #0x8
 1c0:	tbz	w1, #13, 1d4 <__trunctfhf2+0x1d4>
 1c4:	add	x2, x2, #0x1
 1c8:	cmp	x2, #0x1f
 1cc:	b.eq	218 <__trunctfhf2+0x218>  // b.none
 1d0:	and	x1, x1, #0xffffffffffffdfff
 1d4:	lsr	x1, x1, #3
 1d8:	cmp	x1, #0x0
 1dc:	mov	x3, x1
 1e0:	ccmp	x2, #0x1f, #0x0, ne  // ne = any
 1e4:	orr	x1, x1, #0x200
 1e8:	csel	x1, x1, x3, eq  // eq = none
 1ec:	and	x2, x2, #0x1f
 1f0:	and	x1, x1, #0x3ff
 1f4:	orr	w1, w1, w2, lsl #10
 1f8:	orr	w19, w1, w19, lsl #15
 1fc:	sxth	x19, w19
 200:	cbz	w0, 208 <__trunctfhf2+0x208>
 204:	bl	0 <__sfp_handle_exceptions>
 208:	dup	v0.4h, w19
 20c:	ldr	x19, [sp, #16]
 210:	ldp	x29, x30, [sp], #48
 214:	ret
 218:	ands	x1, x4, #0xc00000
 21c:	b.eq	238 <__trunctfhf2+0x238>  // b.none
 220:	cmp	x1, #0x400, lsl #12
 224:	b.ne	244 <__trunctfhf2+0x244>  // b.any
 228:	cmp	w19, #0x0
 22c:	mov	x3, #0x1e                  	// #30
 230:	csetm	x1, ne  // ne = any
 234:	csel	x2, x2, x3, eq  // eq = none
 238:	mov	w3, #0x14                  	// #20
 23c:	orr	w0, w0, w3
 240:	b	1d4 <__trunctfhf2+0x1d4>
 244:	cmp	x1, #0x800, lsl #12
 248:	b.ne	260 <__trunctfhf2+0x260>  // b.any
 24c:	cmp	w19, #0x0
 250:	mov	x3, #0x1e                  	// #30
 254:	csetm	x1, eq  // eq = none
 258:	csel	x2, x2, x3, ne  // ne = any
 25c:	b	238 <__trunctfhf2+0x238>
 260:	mov	x1, #0xffffffffffffffff    	// #-1
 264:	mov	x2, #0x1e                  	// #30
 268:	b	238 <__trunctfhf2+0x238>

fixhfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixhfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	x21, [sp, #32]
  10:	mrs	x0, fpcr
  14:	umov	w0, v0.h[0]
  18:	mov	w1, #0x0                   	// #0
  1c:	bfxil	w1, w0, #0, #16
  20:	and	x0, x1, #0x3ff
  24:	ubfx	x2, x1, #10, #5
  28:	ubfx	x1, x1, #15, #1
  2c:	cmp	x2, #0xe
  30:	b.gt	5c <__fixhfti+0x5c>
  34:	mov	x19, #0x0                   	// #0
  38:	mov	x20, #0x0                   	// #0
  3c:	cbnz	x2, e8 <__fixhfti+0xe8>
  40:	cbnz	x0, e8 <__fixhfti+0xe8>
  44:	mov	x0, x19
  48:	mov	x1, x20
  4c:	ldp	x19, x20, [sp, #16]
  50:	ldr	x21, [sp, #32]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	and	x21, x1, #0xff
  60:	cmp	x2, #0x1f
  64:	b.ne	8c <__fixhfti+0x8c>  // b.any
  68:	adrp	x2, 0 <__fixhfti>
  6c:	mov	x0, #0x1                   	// #1
  70:	sub	x19, x0, x21
  74:	ldr	x20, [x2]
  78:	asr	x1, x19, #63
  7c:	negs	x19, x19
  80:	sbc	x20, x20, x1
  84:	bl	0 <__sfp_handle_exceptions>
  88:	b	44 <__fixhfti+0x44>
  8c:	orr	x0, x0, #0x400
  90:	cmp	x2, #0x18
  94:	b.le	b8 <__fixhfti+0xb8>
  98:	sub	w2, w2, #0x19
  9c:	mov	x1, #0x0                   	// #0
  a0:	bl	0 <__ashlti3>
  a4:	mov	x19, x0
  a8:	mov	x20, x1
  ac:	cbz	x21, 44 <__fixhfti+0x44>
  b0:	mov	w1, #0x0                   	// #0
  b4:	b	dc <__fixhfti+0xdc>
  b8:	add	w1, w2, #0x27
  bc:	mov	w19, #0x19                  	// #25
  c0:	sub	w19, w19, w2
  c4:	mov	x20, #0x0                   	// #0
  c8:	lsl	x1, x0, x1
  cc:	cmp	x1, #0x0
  d0:	cset	w1, ne  // ne = any
  d4:	lsr	x19, x0, x19
  d8:	cbz	x21, e4 <__fixhfti+0xe4>
  dc:	negs	x19, x19
  e0:	ngc	x20, x20
  e4:	cbz	w1, 44 <__fixhfti+0x44>
  e8:	mov	w0, #0x10                  	// #16
  ec:	b	84 <__fixhfti+0x84>

fixunshfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunshfti>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mrs	x0, fpcr
  10:	umov	w0, v0.h[0]
  14:	mov	w19, #0x0                   	// #0
  18:	bfxil	w19, w0, #0, #16
  1c:	and	x0, x19, #0x3ff
  20:	ubfx	x2, x19, #10, #5
  24:	ubfx	x19, x19, #15, #1
  28:	cmp	x2, #0xe
  2c:	b.gt	54 <__fixunshfti+0x54>
  30:	cbnz	x2, c4 <__fixunshfti+0xc4>
  34:	cbnz	x0, c4 <__fixunshfti+0xc4>
  38:	mov	x20, #0x0                   	// #0
  3c:	mov	x19, #0x0                   	// #0
  40:	mov	x0, x20
  44:	mov	x1, x19
  48:	ldp	x19, x20, [sp, #16]
  4c:	ldp	x29, x30, [sp], #32
  50:	ret
  54:	cmp	x2, #0x1e
  58:	cset	w3, gt
  5c:	orr	w3, w19, w3
  60:	cbz	w3, 7c <__fixunshfti+0x7c>
  64:	eor	w19, w19, #0x1
  68:	mov	w0, #0x1                   	// #1
  6c:	sbfx	x20, x19, #0, #1
  70:	mov	x19, x20
  74:	bl	0 <__sfp_handle_exceptions>
  78:	b	40 <__fixunshfti+0x40>
  7c:	orr	x0, x0, #0x400
  80:	cmp	x2, #0x18
  84:	b.le	a0 <__fixunshfti+0xa0>
  88:	sub	w2, w2, #0x19
  8c:	mov	x1, #0x0                   	// #0
  90:	bl	0 <__ashlti3>
  94:	mov	x20, x0
  98:	mov	x19, x1
  9c:	b	40 <__fixunshfti+0x40>
  a0:	mov	w20, #0x19                  	// #25
  a4:	sub	w20, w20, w2
  a8:	add	w2, w2, #0x27
  ac:	mov	x19, #0x0                   	// #0
  b0:	lsr	x20, x0, x20
  b4:	lsl	x0, x0, x2
  b8:	cbz	x0, 40 <__fixunshfti+0x40>
  bc:	mov	w0, #0x10                  	// #16
  c0:	b	74 <__fixunshfti+0x74>
  c4:	mov	w0, #0x10                  	// #16
  c8:	mov	x20, #0x0                   	// #0
  cc:	mov	x19, #0x0                   	// #0
  d0:	b	74 <__fixunshfti+0x74>

floattihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattihf>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x0
  10:	stp	x21, x22, [sp, #32]
  14:	stp	x23, x24, [sp, #48]
  18:	stp	x25, x26, [sp, #64]
  1c:	mrs	x25, fpcr
  20:	orr	x0, x0, x1
  24:	cbz	x0, 24c <__floattihf+0x24c>
  28:	mov	x21, x1
  2c:	mov	x23, x1
  30:	lsr	x24, x1, #63
  34:	tbz	x1, #63, 40 <__floattihf+0x40>
  38:	negs	x19, x19
  3c:	ngc	x23, x1
  40:	clz	x1, x19
  44:	clz	x22, x23
  48:	cmp	x23, #0x0
  4c:	add	w1, w1, #0x40
  50:	csel	w0, w1, w22, eq  // eq = none
  54:	mov	w22, #0x8e                  	// #142
  58:	sub	w22, w22, w0
  5c:	cmp	w22, #0x1e
  60:	b.le	b0 <__floattihf+0xb0>
  64:	ands	x2, x25, #0xc00000
  68:	b.eq	1d0 <__floattihf+0x1d0>  // b.none
  6c:	cmp	x2, #0x400, lsl #12
  70:	b.ne	90 <__floattihf+0x90>  // b.any
  74:	cmp	x21, #0x0
  78:	mov	x20, #0x1f                  	// #31
  7c:	mov	x0, #0x1e                  	// #30
  80:	csetm	x2, lt  // lt = tstop
  84:	csel	x20, x20, x0, ge  // ge = tcont
  88:	mov	w0, #0x14                  	// #20
  8c:	b	138 <__floattihf+0x138>
  90:	cmp	x2, #0x800, lsl #12
  94:	b.ne	1d8 <__floattihf+0x1d8>  // b.any
  98:	cmp	x21, #0x0
  9c:	mov	x20, #0x1f                  	// #31
  a0:	mov	x0, #0x1e                  	// #30
  a4:	csetm	x2, ge  // ge = tcont
  a8:	csel	x20, x20, x0, lt  // lt = tstop
  ac:	b	88 <__floattihf+0x88>
  b0:	sxtw	x20, w22
  b4:	cmp	w22, #0x19
  b8:	b.gt	d8 <__floattihf+0xd8>
  bc:	cmp	x20, #0x19
  c0:	b.eq	d0 <__floattihf+0xd0>  // b.none
  c4:	mov	w0, #0x19                  	// #25
  c8:	sub	w22, w0, w22
  cc:	lsl	x19, x19, x22
  d0:	mov	w0, #0x0                   	// #0
  d4:	b	198 <__floattihf+0x198>
  d8:	cmp	x20, #0x1c
  dc:	b.le	118 <__floattihf+0x118>
  e0:	sub	w2, w22, #0x1c
  e4:	mov	x0, x19
  e8:	mov	x1, x23
  ec:	bl	0 <__lshrti3>
  f0:	mov	x1, x23
  f4:	mov	x26, x0
  f8:	mov	w2, #0x9c                  	// #156
  fc:	mov	x0, x19
 100:	sub	w2, w2, w22
 104:	bl	0 <__ashlti3>
 108:	orr	x0, x0, x1
 10c:	cmp	x0, #0x0
 110:	cset	x19, ne  // ne = any
 114:	orr	x19, x19, x26
 118:	mov	x0, #0x1c                  	// #28
 11c:	sub	x1, x0, x20
 120:	cmp	x1, #0x0
 124:	b.le	130 <__floattihf+0x130>
 128:	sub	w22, w0, w22
 12c:	lsl	x19, x19, x22
 130:	and	x2, x19, #0xffffffffffffdfff
 134:	mov	w0, #0x0                   	// #0
 138:	tst	x2, #0x7
 13c:	b.eq	16c <__floattihf+0x16c>  // b.none
 140:	and	x1, x25, #0xc00000
 144:	orr	w0, w0, #0x10
 148:	cmp	x1, #0x400, lsl #12
 14c:	b.eq	1e4 <__floattihf+0x1e4>  // b.none
 150:	cmp	x1, #0x800, lsl #12
 154:	b.eq	1f0 <__floattihf+0x1f0>  // b.none
 158:	cbnz	x1, 16c <__floattihf+0x16c>
 15c:	and	x1, x2, #0xf
 160:	cmp	x1, #0x4
 164:	b.eq	16c <__floattihf+0x16c>  // b.none
 168:	add	x2, x2, #0x4
 16c:	tbz	w2, #13, 180 <__floattihf+0x180>
 170:	add	x20, x20, #0x1
 174:	cmp	x20, #0x1f
 178:	b.eq	1f8 <__floattihf+0x1f8>  // b.none
 17c:	and	x2, x2, #0xffffffffffffdfff
 180:	lsr	x19, x2, #3
 184:	cmp	x19, #0x0
 188:	mov	x1, x19
 18c:	ccmp	x20, #0x1f, #0x0, ne  // ne = any
 190:	orr	x19, x19, #0x200
 194:	csel	x19, x19, x1, eq  // eq = none
 198:	and	x20, x20, #0x1f
 19c:	and	x19, x19, #0x3ff
 1a0:	orr	w19, w19, w20, lsl #10
 1a4:	orr	w19, w19, w24, lsl #15
 1a8:	sxth	x19, w19
 1ac:	cbz	w0, 1b4 <__floattihf+0x1b4>
 1b0:	bl	0 <__sfp_handle_exceptions>
 1b4:	dup	v0.4h, w19
 1b8:	ldp	x19, x20, [sp, #16]
 1bc:	ldp	x21, x22, [sp, #32]
 1c0:	ldp	x23, x24, [sp, #48]
 1c4:	ldp	x25, x26, [sp, #64]
 1c8:	ldp	x29, x30, [sp], #80
 1cc:	ret
 1d0:	mov	x20, #0x1f                  	// #31
 1d4:	b	88 <__floattihf+0x88>
 1d8:	mov	x2, #0xffffffffffffffff    	// #-1
 1dc:	mov	x20, #0x1e                  	// #30
 1e0:	b	88 <__floattihf+0x88>
 1e4:	tbnz	x21, #63, 16c <__floattihf+0x16c>
 1e8:	add	x2, x2, #0x8
 1ec:	b	16c <__floattihf+0x16c>
 1f0:	tbz	x21, #63, 16c <__floattihf+0x16c>
 1f4:	b	1e8 <__floattihf+0x1e8>
 1f8:	ands	x2, x25, #0xc00000
 1fc:	b.eq	238 <__floattihf+0x238>  // b.none
 200:	cmp	x2, #0x400, lsl #12
 204:	b.ne	220 <__floattihf+0x220>  // b.any
 208:	cmp	x21, #0x0
 20c:	mov	x1, #0x1e                  	// #30
 210:	csetm	x2, lt  // lt = tstop
 214:	csel	x20, x20, x1, ge  // ge = tcont
 218:	mov	w0, #0x14                  	// #20
 21c:	b	180 <__floattihf+0x180>
 220:	cmp	x2, #0x800, lsl #12
 224:	b.ne	240 <__floattihf+0x240>  // b.any
 228:	cmp	x21, #0x0
 22c:	mov	x0, #0x1e                  	// #30
 230:	csetm	x2, ge  // ge = tcont
 234:	csel	x20, x20, x0, lt  // lt = tstop
 238:	mov	w0, #0x14                  	// #20
 23c:	b	180 <__floattihf+0x180>
 240:	mov	x2, #0xffffffffffffffff    	// #-1
 244:	mov	x20, #0x1e                  	// #30
 248:	b	238 <__floattihf+0x238>
 24c:	mov	x19, #0x0                   	// #0
 250:	mov	x20, #0x0                   	// #0
 254:	mov	x24, #0x0                   	// #0
 258:	b	d0 <__floattihf+0xd0>

floatuntihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntihf>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	mrs	x22, fpcr
  18:	orr	x2, x0, x1
  1c:	cbz	x2, 1a4 <__floatuntihf+0x1a4>
  20:	clz	x3, x0
  24:	clz	x21, x1
  28:	cmp	x1, #0x0
  2c:	add	w3, w3, #0x40
  30:	csel	w2, w3, w21, eq  // eq = none
  34:	mov	w21, #0x8e                  	// #142
  38:	sub	w21, w21, w2
  3c:	mov	x19, x0
  40:	mov	x23, x1
  44:	cmp	w21, #0x1e
  48:	b.le	6c <__floatuntihf+0x6c>
  4c:	ands	x1, x22, #0xc00000
  50:	b.eq	168 <__floatuntihf+0x168>  // b.none
  54:	cmp	x1, #0x400, lsl #12
  58:	b.eq	164 <__floatuntihf+0x164>  // b.none
  5c:	mov	x1, #0xffffffffffffffff    	// #-1
  60:	mov	x20, #0x1e                  	// #30
  64:	mov	w0, #0x14                  	// #20
  68:	b	f0 <__floatuntihf+0xf0>
  6c:	sxtw	x20, w21
  70:	cmp	w21, #0x19
  74:	b.gt	98 <__floatuntihf+0x98>
  78:	mov	x1, x0
  7c:	cmp	x20, #0x19
  80:	b.eq	90 <__floatuntihf+0x90>  // b.none
  84:	mov	w0, #0x19                  	// #25
  88:	sub	w21, w0, w21
  8c:	lsl	x1, x19, x21
  90:	mov	w0, #0x0                   	// #0
  94:	b	138 <__floatuntihf+0x138>
  98:	cmp	x20, #0x1c
  9c:	b.le	cc <__floatuntihf+0xcc>
  a0:	mov	w2, #0x9c                  	// #156
  a4:	sub	w2, w2, w21
  a8:	bl	0 <__ashlti3>
  ac:	orr	x0, x0, x1
  b0:	cmp	x0, #0x0
  b4:	sub	w2, w21, #0x1c
  b8:	cset	x24, ne  // ne = any
  bc:	mov	x0, x19
  c0:	mov	x1, x23
  c4:	bl	0 <__lshrti3>
  c8:	orr	x19, x24, x0
  cc:	mov	x0, #0x1c                  	// #28
  d0:	sub	x2, x0, x20
  d4:	mov	x1, x19
  d8:	cmp	x2, #0x0
  dc:	b.le	e8 <__floatuntihf+0xe8>
  e0:	sub	w21, w0, w21
  e4:	lsl	x1, x19, x21
  e8:	and	x1, x1, #0xffffffffffffdfff
  ec:	mov	w0, #0x0                   	// #0
  f0:	tst	x1, #0x7
  f4:	b.eq	10c <__floatuntihf+0x10c>  // b.none
  f8:	orr	w0, w0, #0x10
  fc:	ands	x2, x22, #0xc00000
 100:	b.eq	170 <__floatuntihf+0x170>  // b.none
 104:	cmp	x2, #0x400, lsl #12
 108:	b.eq	184 <__floatuntihf+0x184>  // b.none
 10c:	tbz	w1, #13, 120 <__floatuntihf+0x120>
 110:	add	x20, x20, #0x1
 114:	cmp	x20, #0x1f
 118:	b.eq	18c <__floatuntihf+0x18c>  // b.none
 11c:	and	x1, x1, #0xffffffffffffdfff
 120:	lsr	x1, x1, #3
 124:	cmp	x1, #0x0
 128:	mov	x2, x1
 12c:	ccmp	x20, #0x1f, #0x0, ne  // ne = any
 130:	orr	x1, x1, #0x200
 134:	csel	x1, x1, x2, eq  // eq = none
 138:	and	x20, x20, #0x1f
 13c:	and	x1, x1, #0x3ff
 140:	orr	x20, x1, x20, lsl #10
 144:	cbz	w0, 14c <__floatuntihf+0x14c>
 148:	bl	0 <__sfp_handle_exceptions>
 14c:	dup	v0.4h, w20
 150:	ldp	x19, x20, [sp, #16]
 154:	ldp	x21, x22, [sp, #32]
 158:	ldp	x23, x24, [sp, #48]
 15c:	ldp	x29, x30, [sp], #64
 160:	ret
 164:	mov	x1, #0x0                   	// #0
 168:	mov	x20, #0x1f                  	// #31
 16c:	b	64 <__floatuntihf+0x64>
 170:	and	x2, x1, #0xf
 174:	cmp	x2, #0x4
 178:	b.eq	10c <__floatuntihf+0x10c>  // b.none
 17c:	add	x1, x1, #0x4
 180:	b	10c <__floatuntihf+0x10c>
 184:	add	x1, x1, #0x8
 188:	b	10c <__floatuntihf+0x10c>
 18c:	and	x1, x22, #0x800000
 190:	tbz	w22, #23, 19c <__floatuntihf+0x19c>
 194:	mov	x1, #0xffffffffffffffff    	// #-1
 198:	mov	x20, #0x1e                  	// #30
 19c:	mov	w0, #0x14                  	// #20
 1a0:	b	120 <__floatuntihf+0x120>
 1a4:	mov	x1, #0x0                   	// #0
 1a8:	mov	x20, #0x0                   	// #0
 1ac:	b	90 <__floatuntihf+0x90>

enable-execute-stack.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	ret
