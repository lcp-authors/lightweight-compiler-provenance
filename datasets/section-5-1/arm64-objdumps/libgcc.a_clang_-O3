In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgcc.a_clang_-O3:

_muldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	mov	x5, x0
   4:	and	x0, x2, #0xffffffff
   8:	and	x7, x5, #0xffffffff
   c:	lsr	x9, x2, #32
  10:	lsr	x6, x5, #32
  14:	mov	x8, #0x100000000           	// #4294967296
  18:	mul	x4, x7, x0
  1c:	mul	x0, x6, x0
  20:	madd	x7, x7, x9, x0
  24:	and	x10, x4, #0xffffffff
  28:	mul	x6, x6, x9
  2c:	add	x4, x7, x4, lsr #32
  30:	add	x8, x6, x8
  34:	cmp	x0, x4
  38:	csel	x6, x8, x6, hi  // hi = pmore
  3c:	add	x0, x10, x4, lsl #32
  40:	add	x4, x6, x4, lsr #32
  44:	madd	x4, x5, x3, x4
  48:	madd	x1, x2, x1, x4
  4c:	ret

_negdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	cmp	x0, #0x0
   4:	neg	x1, x1
   8:	cset	x2, ne  // ne = any
   c:	neg	x0, x0
  10:	sub	x1, x1, x2
  14:	ret

_lshrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	mov	x4, x1
   4:	cbz	x2, 28 <__lshrti3+0x28>
   8:	mov	x3, #0x40                  	// #64
   c:	sub	x3, x3, x2
  10:	cmp	x3, #0x0
  14:	b.le	2c <__lshrti3+0x2c>
  18:	lsl	x3, x1, x3
  1c:	lsr	x0, x0, x2
  20:	orr	x0, x0, x3
  24:	lsr	x1, x1, x2
  28:	ret
  2c:	neg	w0, w3
  30:	mov	x1, #0x0                   	// #0
  34:	lsr	x0, x4, x0
  38:	ret

_ashldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	mov	x4, x0
   4:	cbz	x2, 28 <__ashlti3+0x28>
   8:	mov	x3, #0x40                  	// #64
   c:	sub	x3, x3, x2
  10:	cmp	x3, #0x0
  14:	b.le	2c <__ashlti3+0x2c>
  18:	lsr	x3, x0, x3
  1c:	lsl	x1, x1, x2
  20:	orr	x1, x1, x3
  24:	lsl	x0, x0, x2
  28:	ret
  2c:	neg	w1, w3
  30:	mov	x0, #0x0                   	// #0
  34:	lsl	x1, x4, x1
  38:	ret

_ashrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	mov	x3, x1
   4:	cbz	x2, 28 <__ashrti3+0x28>
   8:	mov	x1, #0x40                  	// #64
   c:	sub	x1, x1, x2
  10:	cmp	x1, #0x0
  14:	b.le	2c <__ashrti3+0x2c>
  18:	lsl	x1, x3, x1
  1c:	lsr	x0, x0, x2
  20:	orr	x0, x0, x1
  24:	asr	x1, x3, x2
  28:	ret
  2c:	neg	w0, w1
  30:	asr	x1, x3, #63
  34:	asr	x0, x3, x0
  38:	ret

_cmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.lt	2c <__cmpti2+0x2c>  // b.tstop
  10:	mov	w0, #0x2                   	// #2
  14:	b.gt	2c <__cmpti2+0x2c>
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

_ucmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.cc	2c <__ucmpti2+0x2c>  // b.lo, b.ul, b.last
  10:	mov	w0, #0x2                   	// #2
  14:	b.hi	2c <__ucmpti2+0x2c>  // b.pmore
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

_clear_cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	b	0 <__aarch64_sync_cache_range>

_trampoline.o:     file format elf64-littleaarch64


__main.o:     file format elf64-littleaarch64


_absvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	tbnz	x0, #63, 8 <__absvdi2+0x8>
   4:	ret
   8:	negs	x0, x0
   c:	b.pl	4 <__absvdi2+0x4>  // b.nfrst
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <abort>
  1c:	nop

0000000000000020 <__absvsi2>:
  20:	tbnz	w0, #31, 28 <__absvsi2+0x8>
  24:	ret
  28:	negs	w0, w0
  2c:	b.pl	24 <__absvsi2+0x4>  // b.nfrst
  30:	stp	x29, x30, [sp, #-16]!
  34:	mov	x29, sp
  38:	bl	0 <abort>

_absvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	tbnz	x1, #63, 8 <__absvti2+0x8>
   4:	ret
   8:	negs	x0, x0
   c:	ngc	x1, x1
  10:	tbz	x1, #63, 4 <__absvti2+0x4>
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	0 <abort>

_addvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	mov	x2, x0
   4:	add	x0, x0, x1
   8:	tbnz	x1, #63, 1c <__addvdi3+0x1c>
   c:	cmp	x2, x0
  10:	cset	w1, gt
  14:	cbnz	w1, 28 <__addvdi3+0x28>
  18:	ret
  1c:	cmp	x2, x0
  20:	cset	w1, lt  // lt = tstop
  24:	b	14 <__addvdi3+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__addvsi3>:
  38:	mov	w2, w0
  3c:	add	w0, w0, w1
  40:	tbnz	w1, #31, 54 <__addvsi3+0x1c>
  44:	cmp	w2, w0
  48:	cset	w1, gt
  4c:	cbnz	w1, 60 <__addvsi3+0x28>
  50:	ret
  54:	cmp	w2, w0
  58:	cset	w1, lt  // lt = tstop
  5c:	b	4c <__addvsi3+0x14>
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	0 <abort>

_addvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	mov	x4, x0
   4:	adds	x0, x0, x2
   8:	mov	x2, x1
   c:	adc	x1, x1, x3
  10:	tbnz	x3, #63, 34 <__addvti3+0x34>
  14:	cmp	x2, x1
  18:	mov	w2, #0x1                   	// #1
  1c:	b.le	28 <__addvti3+0x28>
  20:	cbnz	w2, 64 <__addvti3+0x64>
  24:	ret
  28:	b.eq	54 <__addvti3+0x54>  // b.none
  2c:	mov	w2, #0x0                   	// #0
  30:	b	20 <__addvti3+0x20>
  34:	cmp	x1, x2
  38:	mov	w2, #0x1                   	// #1
  3c:	b.gt	20 <__addvti3+0x20>
  40:	b.ne	2c <__addvti3+0x2c>  // b.any
  44:	cmp	x0, x4
  48:	b.hi	20 <__addvti3+0x20>  // b.pmore
  4c:	mov	w2, #0x0                   	// #0
  50:	b	20 <__addvti3+0x20>
  54:	cmp	x4, x0
  58:	b.hi	20 <__addvti3+0x20>  // b.pmore
  5c:	mov	w2, #0x0                   	// #0
  60:	b	20 <__addvti3+0x20>
  64:	stp	x29, x30, [sp, #-16]!
  68:	mov	x29, sp
  6c:	bl	0 <abort>

_subvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	mov	x2, x0
   4:	sub	x0, x0, x1
   8:	tbnz	x1, #63, 1c <__subvdi3+0x1c>
   c:	cmp	x2, x0
  10:	cset	w1, lt  // lt = tstop
  14:	cbnz	w1, 28 <__subvdi3+0x28>
  18:	ret
  1c:	cmp	x2, x0
  20:	cset	w1, gt
  24:	b	14 <__subvdi3+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__subvsi3>:
  38:	mov	w2, w0
  3c:	sub	w0, w0, w1
  40:	tbnz	w1, #31, 54 <__subvsi3+0x1c>
  44:	cmp	w2, w0
  48:	cset	w1, lt  // lt = tstop
  4c:	cbnz	w1, 60 <__subvsi3+0x28>
  50:	ret
  54:	cmp	w2, w0
  58:	cset	w1, gt
  5c:	b	4c <__subvsi3+0x14>
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	0 <abort>

_subvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	mov	x4, x0
   4:	subs	x0, x0, x2
   8:	mov	x2, x1
   c:	sbc	x1, x1, x3
  10:	tbnz	x3, #63, 34 <__subvti3+0x34>
  14:	cmp	x1, x2
  18:	mov	w2, #0x1                   	// #1
  1c:	b.le	28 <__subvti3+0x28>
  20:	cbnz	w2, 64 <__subvti3+0x64>
  24:	ret
  28:	b.eq	54 <__subvti3+0x54>  // b.none
  2c:	mov	w2, #0x0                   	// #0
  30:	b	20 <__subvti3+0x20>
  34:	cmp	x2, x1
  38:	mov	w2, #0x1                   	// #1
  3c:	b.gt	20 <__subvti3+0x20>
  40:	b.ne	2c <__subvti3+0x2c>  // b.any
  44:	cmp	x4, x0
  48:	b.hi	20 <__subvti3+0x20>  // b.pmore
  4c:	mov	w2, #0x0                   	// #0
  50:	b	20 <__subvti3+0x20>
  54:	cmp	x0, x4
  58:	b.hi	20 <__subvti3+0x20>  // b.pmore
  5c:	mov	w2, #0x0                   	// #0
  60:	b	20 <__subvti3+0x20>
  64:	stp	x29, x30, [sp, #-16]!
  68:	mov	x29, sp
  6c:	bl	0 <abort>

_mulvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	mov	x2, x0
   4:	mul	x0, x0, x1
   8:	smulh	x2, x2, x1
   c:	cmp	x2, x0, asr #63
  10:	b.ne	18 <__mulvdi3+0x18>  // b.any
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	bl	0 <abort>
  24:	nop

0000000000000028 <__mulvsi3>:
  28:	smull	x0, w0, w1
  2c:	asr	x1, x0, #32
  30:	cmp	w1, w0, asr #31
  34:	b.ne	3c <__mulvsi3+0x14>  // b.any
  38:	ret
  3c:	stp	x29, x30, [sp, #-16]!
  40:	mov	x29, sp
  44:	bl	0 <abort>

_mulvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	cmp	x1, x0, asr #63
   4:	asr	x4, x2, #63
   8:	b.ne	20 <__mulvti3+0x20>  // b.any
   c:	cmp	x4, x3
  10:	b.ne	64 <__mulvti3+0x64>  // b.any
  14:	smulh	x1, x0, x2
  18:	mul	x0, x0, x2
  1c:	ret
  20:	cmp	x4, x3
  24:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  28:	mul	x6, x1, x2
  2c:	umulh	x4, x1, x2
  30:	mul	x7, x0, x2
  34:	mov	x5, x6
  38:	umulh	x3, x0, x2
  3c:	tbnz	x1, #63, ac <__mulvti3+0xac>
  40:	tbz	x2, #63, 4c <__mulvti3+0x4c>
  44:	subs	x5, x6, x0
  48:	sbc	x4, x4, x1
  4c:	adds	x1, x3, x5
  50:	cinc	x4, x4, cs  // cs = hs, nlast
  54:	cmp	x4, x1, asr #63
  58:	b.ne	98 <__mulvti3+0x98>  // b.any
  5c:	mov	x0, x7
  60:	ret
  64:	mul	x6, x3, x0
  68:	umulh	x4, x3, x0
  6c:	mul	x7, x2, x0
  70:	mov	x5, x6
  74:	umulh	x1, x2, x0
  78:	tbnz	x3, #63, a4 <__mulvti3+0xa4>
  7c:	tbz	x0, #63, 88 <__mulvti3+0x88>
  80:	subs	x5, x6, x2
  84:	sbc	x4, x4, x3
  88:	adds	x1, x1, x5
  8c:	cinc	x4, x4, cs  // cs = hs, nlast
  90:	cmp	x4, x1, asr #63
  94:	b.eq	5c <__mulvti3+0x5c>  // b.none
  98:	stp	x29, x30, [sp, #-16]!
  9c:	mov	x29, sp
  a0:	bl	0 <abort>
  a4:	sub	x4, x4, x0
  a8:	b	7c <__mulvti3+0x7c>
  ac:	sub	x4, x4, x2
  b0:	b	40 <__mulvti3+0x40>
  b4:	tbnz	x1, #63, d4 <__mulvti3+0xd4>
  b8:	tbnz	x3, #63, 100 <__mulvti3+0x100>
  bc:	orr	x4, x3, x1
  c0:	cbnz	x4, 98 <__mulvti3+0x98>
  c4:	umulh	x1, x0, x2
  c8:	mul	x0, x0, x2
  cc:	tbnz	x1, #63, 98 <__mulvti3+0x98>
  d0:	ret
  d4:	cmp	x3, #0x0
  d8:	b.lt	124 <__mulvti3+0x124>  // b.tstop
  dc:	ccmn	x1, #0x1, #0x0, eq  // eq = none
  e0:	b.ne	98 <__mulvti3+0x98>  // b.any
  e4:	umulh	x3, x0, x2
  e8:	mul	x1, x0, x2
  ec:	subs	x3, x3, x2
  f0:	b.pl	98 <__mulvti3+0x98>  // b.nfrst
  f4:	mov	x0, x1
  f8:	mov	x1, x3
  fc:	ret
 100:	cmp	x1, #0x0
 104:	ccmn	x3, #0x1, #0x0, eq  // eq = none
 108:	b.ne	98 <__mulvti3+0x98>  // b.any
 10c:	umulh	x1, x0, x2
 110:	mul	x2, x0, x2
 114:	subs	x1, x1, x0
 118:	b.pl	98 <__mulvti3+0x98>  // b.nfrst
 11c:	mov	x0, x2
 120:	ret
 124:	and	x4, x3, x1
 128:	cmn	x4, #0x1
 12c:	b.ne	98 <__mulvti3+0x98>  // b.any
 130:	orr	x1, x2, x0
 134:	cbz	x1, 98 <__mulvti3+0x98>
 138:	umulh	x4, x0, x2
 13c:	mul	x3, x0, x2
 140:	sub	x1, x4, x0
 144:	subs	x1, x1, x2
 148:	b.mi	98 <__mulvti3+0x98>  // b.first
 14c:	mov	x0, x3
 150:	ret

_negvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, x0
   4:	neg	x0, x0
   8:	tbnz	x1, #63, 1c <__negvdi2+0x1c>
   c:	cmp	x0, #0x0
  10:	cset	w1, gt
  14:	cbnz	w1, 28 <__negvdi2+0x28>
  18:	ret
  1c:	lsr	x1, x0, #63
  20:	and	w1, w1, #0xff
  24:	b	14 <__negvdi2+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__negvsi2>:
  38:	mov	w1, w0
  3c:	neg	w0, w0
  40:	lsr	w2, w0, #31
  44:	tbnz	w1, #31, 50 <__negvsi2+0x18>
  48:	cmp	w0, #0x0
  4c:	cset	w2, gt
  50:	cbnz	w2, 58 <__negvsi2+0x20>
  54:	ret
  58:	stp	x29, x30, [sp, #-16]!
  5c:	mov	x29, sp
  60:	bl	0 <abort>

_negvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	negs	x0, x0
   4:	mov	x2, x1
   8:	ngc	x1, x1
   c:	tbnz	x2, #63, 2c <__negvti2+0x2c>
  10:	asr	x2, x1, #63
  14:	subs	x3, x2, x0
  18:	sbc	x2, x2, x1
  1c:	lsr	x2, x2, #63
  20:	and	w2, w2, #0xff
  24:	cbnz	w2, 38 <__negvti2+0x38>
  28:	ret
  2c:	lsr	x2, x1, #63
  30:	and	w2, w2, #0xff
  34:	b	24 <__negvti2+0x24>
  38:	stp	x29, x30, [sp, #-16]!
  3c:	mov	x29, sp
  40:	bl	0 <abort>

_ctors.o:     file format elf64-littleaarch64


_ffssi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	rbit	x1, x0
   4:	cmp	x0, #0x0
   8:	clz	x1, x1
   c:	csinc	w0, wzr, w1, eq  // eq = none
  10:	ret

_ffsdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 1c <__ffsti2+0x1c>
   4:	mov	w1, #0x0                   	// #0
   8:	rbit	x0, x0
   c:	add	w1, w1, #0x1
  10:	clz	x0, x0
  14:	add	w0, w1, w0
  18:	ret
  1c:	mov	w0, #0x0                   	// #0
  20:	cbz	x1, 18 <__ffsti2+0x18>
  24:	mov	x0, x1
  28:	mov	w1, #0x40                  	// #64
  2c:	b	8 <__ffsti2+0x8>

_clz.o:     file format elf64-littleaarch64


_clzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	clz	x0, x0
   4:	ret

_clzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cbz	x1, 18 <__clzti2+0x18>
   4:	mov	x0, x1
   8:	mov	w1, #0x0                   	// #0
   c:	clz	x0, x0
  10:	add	w0, w1, w0
  14:	ret
  18:	clz	x0, x0
  1c:	mov	w1, #0x40                  	// #64
  20:	add	w0, w1, w0
  24:	ret

_ctzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	rbit	x0, x0
   4:	clz	x0, x0
   8:	ret

_ctzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cbz	x0, 1c <__ctzti2+0x1c>
   4:	mov	x1, x0
   8:	mov	w0, #0x0                   	// #0
   c:	rbit	x1, x1
  10:	clz	x1, x1
  14:	add	w0, w0, w1
  18:	ret
  1c:	rbit	x1, x1
  20:	mov	w0, #0x40                  	// #64
  24:	clz	x1, x1
  28:	add	w0, w0, w1
  2c:	ret

_popcount_tab.o:     file format elf64-littleaarch64


_popcountsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	mov	x3, #0x101010101010101     	// #72340172838076673
   8:	and	x1, x1, #0x5555555555555555
   c:	sub	x1, x0, x1
  10:	and	x2, x1, #0x3333333333333333
  14:	lsr	x1, x1, #2
  18:	and	x1, x1, #0x3333333333333333
  1c:	add	x1, x2, x1
  20:	add	x1, x1, x1, lsr #4
  24:	and	x0, x1, #0xf0f0f0f0f0f0f0f
  28:	mul	x0, x0, x3
  2c:	lsr	x0, x0, #56
  30:	ret

_popcountdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x3, x0, #1
   4:	lsr	x2, x1, #1
   8:	and	x3, x3, #0x5555555555555555
   c:	and	x2, x2, #0x5555555555555555
  10:	sub	x0, x0, x3
  14:	sub	x2, x1, x2
  18:	and	x3, x0, #0x3333333333333333
  1c:	and	x1, x2, #0x3333333333333333
  20:	lsr	x0, x0, #2
  24:	lsr	x2, x2, #2
  28:	and	x2, x2, #0x3333333333333333
  2c:	and	x0, x0, #0x3333333333333333
  30:	add	x0, x3, x0
  34:	add	x1, x1, x2
  38:	mov	x4, #0x101010101010101     	// #72340172838076673
  3c:	add	x0, x0, x0, lsr #4
  40:	add	x1, x1, x1, lsr #4
  44:	and	x2, x0, #0xf0f0f0f0f0f0f0f
  48:	and	x0, x1, #0xf0f0f0f0f0f0f0f
  4c:	add	x0, x0, x2
  50:	mul	x0, x0, x4
  54:	lsr	x0, x0, #56
  58:	ret

_paritysi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	eor	x1, x0, x0, lsr #32
   4:	mov	w2, #0x6996                	// #27030
   8:	eor	x1, x1, x1, lsr #16
   c:	eor	x1, x1, x1, lsr #8
  10:	eor	x1, x1, x1, lsr #4
  14:	and	x1, x1, #0xf
  18:	asr	w1, w2, w1
  1c:	and	w0, w1, #0x1
  20:	ret

_paritydi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x1, x0, x1
   4:	mov	w0, #0x6996                	// #27030
   8:	eor	x1, x1, x1, lsr #32
   c:	eor	x1, x1, x1, lsr #16
  10:	eor	x1, x1, x1, lsr #8
  14:	eor	x1, x1, x1, lsr #4
  18:	and	x1, x1, #0xf
  1c:	asr	w0, w0, w1
  20:	and	w0, w0, #0x1
  24:	ret

_powisf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	cmp	w0, #0x0
   4:	fmov	s2, #1.000000000000000000e+00
   8:	cneg	w2, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	s1, s0
  14:	lsr	w1, w2, #1
  18:	fcsel	s0, s0, s2, ne  // ne = any
  1c:	cmp	wzr, w2, lsr #1
  20:	b.eq	3c <__powisf2+0x3c>  // b.none
  24:	nop
  28:	fmul	s1, s1, s1
  2c:	tbz	w1, #0, 34 <__powisf2+0x34>
  30:	fmul	s0, s0, s1
  34:	lsr	w1, w1, #1
  38:	cbnz	w1, 28 <__powisf2+0x28>
  3c:	tbnz	w0, #31, 44 <__powisf2+0x44>
  40:	ret
  44:	fmov	s1, #1.000000000000000000e+00
  48:	fdiv	s0, s1, s0
  4c:	ret

_powidf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	cmp	w0, #0x0
   4:	fmov	d2, #1.000000000000000000e+00
   8:	cneg	w2, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	d1, d0
  14:	lsr	w1, w2, #1
  18:	fcsel	d0, d0, d2, ne  // ne = any
  1c:	cmp	wzr, w2, lsr #1
  20:	b.eq	3c <__powidf2+0x3c>  // b.none
  24:	nop
  28:	fmul	d1, d1, d1
  2c:	tbz	w1, #0, 34 <__powidf2+0x34>
  30:	fmul	d0, d0, d1
  34:	lsr	w1, w1, #1
  38:	cbnz	w1, 28 <__powidf2+0x28>
  3c:	tbnz	w0, #31, 44 <__powidf2+0x44>
  40:	ret
  44:	fmov	d1, #1.000000000000000000e+00
  48:	fdiv	d0, d1, d0
  4c:	ret

_powixf2.o:     file format elf64-littleaarch64


_powitf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powitf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	cmp	w0, #0x0
   8:	mov	v2.16b, v0.16b
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	v4.16b, v0.16b
  18:	mov	w20, w0
  1c:	cneg	w19, w0, lt  // lt = tstop
  20:	tbnz	w20, #0, 30 <__powitf2+0x30>
  24:	adrp	x0, 0 <__powitf2>
  28:	add	x0, x0, #0x0
  2c:	ldr	q4, [x0]
  30:	cmp	wzr, w19, lsr #1
  34:	lsr	w19, w19, #1
  38:	b.eq	7c <__powitf2+0x7c>  // b.none
  3c:	nop
  40:	mov	v1.16b, v2.16b
  44:	mov	v0.16b, v2.16b
  48:	str	q4, [sp, #32]
  4c:	bl	0 <__multf3>
  50:	ldr	q4, [sp, #32]
  54:	mov	v2.16b, v0.16b
  58:	mov	v1.16b, v2.16b
  5c:	mov	v0.16b, v4.16b
  60:	tbz	w19, #0, 74 <__powitf2+0x74>
  64:	str	q2, [sp, #32]
  68:	bl	0 <__multf3>
  6c:	mov	v4.16b, v0.16b
  70:	ldr	q2, [sp, #32]
  74:	lsr	w19, w19, #1
  78:	cbnz	w19, 40 <__powitf2+0x40>
  7c:	tbz	w20, #31, 98 <__powitf2+0x98>
  80:	adrp	x0, 0 <__powitf2>
  84:	add	x0, x0, #0x0
  88:	mov	v1.16b, v4.16b
  8c:	ldr	q0, [x0]
  90:	bl	0 <__divtf3>
  94:	mov	v4.16b, v0.16b
  98:	mov	v0.16b, v4.16b
  9c:	ldp	x19, x20, [sp, #16]
  a0:	ldp	x29, x30, [sp], #48
  a4:	ret

_mulhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulhc3>:
   0:	fcvt	s5, h0
   4:	fcvt	s4, h1
   8:	fcvt	s19, h2
   c:	fcvt	s20, h3
  10:	mov	v22.h[0], v0.h[0]
  14:	mov	v21.h[0], v1.h[0]
  18:	fmul	s17, s5, s19
  1c:	fmul	s16, s4, s20
  20:	fmul	s7, s5, s20
  24:	fmul	s6, s19, s4
  28:	fcvt	h17, s17
  2c:	fcvt	h16, s16
  30:	fcvt	h7, s7
  34:	fcvt	h6, s6
  38:	fcvt	s17, h17
  3c:	fcvt	s16, h16
  40:	fcvt	s23, h7
  44:	fcvt	s24, h6
  48:	fsub	s0, s17, s16
  4c:	fadd	s1, s23, s24
  50:	fcvt	h0, s0
  54:	fcvt	h1, s1
  58:	fcvt	s7, h0
  5c:	fcvt	s6, h1
  60:	fcmp	s7, s7
  64:	cset	w0, vs
  68:	fcmp	s6, s6
  6c:	cset	w1, vs
  70:	ands	w0, w0, w1
  74:	b.eq	1d0 <__mulhc3+0x1d0>  // b.none
  78:	fabs	s18, s5
  7c:	fabs	s4, s4
  80:	mov	w1, #0xe000                	// #57344
  84:	movk	w1, #0x477f, lsl #16
  88:	fmov	s5, w1
  8c:	fcvt	h18, s18
  90:	fcvt	h4, s4
  94:	fcvt	s18, h18
  98:	fcvt	s4, h4
  9c:	fcmp	s18, s5
  a0:	b.le	1d4 <__mulhc3+0x1d4>
  a4:	mov	w1, #0xe000                	// #57344
  a8:	umov	w4, v22.h[0]
  ac:	movk	w1, #0x477f, lsl #16
  b0:	fmov	s5, w1
  b4:	umov	w3, v21.h[0]
  b8:	fcmp	s18, s5
  bc:	cset	w1, gt
  c0:	fcmp	s4, s5
  c4:	scvtf	d5, w1
  c8:	cset	w1, gt
  cc:	fcmp	s19, s19
  d0:	scvtf	d4, w1
  d4:	fcvt	h5, d5
  d8:	fcvt	h4, d4
  dc:	umov	w2, v5.h[0]
  e0:	umov	w1, v4.h[0]
  e4:	bfxil	w4, w2, #0, #15
  e8:	mov	w2, w3
  ec:	dup	v22.4h, w4
  f0:	bfxil	w2, w1, #0, #15
  f4:	dup	v21.4h, w2
  f8:	b.vs	2b8 <__mulhc3+0x2b8>
  fc:	fcmp	s20, s20
 100:	b.vs	2d0 <__mulhc3+0x2d0>
 104:	fabs	s6, s19
 108:	fabs	s7, s20
 10c:	mov	w1, #0xe000                	// #57344
 110:	movk	w1, #0x477f, lsl #16
 114:	fmov	s4, w1
 118:	fcvt	h6, s6
 11c:	fcvt	h7, s7
 120:	fcvt	s6, h6
 124:	fcvt	s7, h7
 128:	fcmp	s6, s4
 12c:	b.gt	138 <__mulhc3+0x138>
 130:	fcmp	s7, s4
 134:	b.le	1e4 <__mulhc3+0x1e4>
 138:	mov	w0, #0xe000                	// #57344
 13c:	umov	w3, v2.h[0]
 140:	movk	w0, #0x477f, lsl #16
 144:	fmov	s0, w0
 148:	umov	w2, v3.h[0]
 14c:	fcvt	s5, h22
 150:	fcmp	s6, s0
 154:	cset	w0, gt
 158:	fcmp	s7, s0
 15c:	scvtf	d1, w0
 160:	cset	w0, gt
 164:	fcmp	s5, s5
 168:	scvtf	d0, w0
 16c:	fcvt	h1, d1
 170:	fcvt	h0, d0
 174:	umov	w1, v1.h[0]
 178:	umov	w0, v0.h[0]
 17c:	bfxil	w3, w1, #0, #15
 180:	mov	w1, w2
 184:	dup	v2.4h, w3
 188:	bfxil	w1, w0, #0, #15
 18c:	dup	v3.4h, w1
 190:	b.vs	280 <__mulhc3+0x280>
 194:	fcvt	s4, h21
 198:	fcmp	s4, s4
 19c:	b.vs	298 <__mulhc3+0x298>
 1a0:	fcvt	s19, h2
 1a4:	fcvt	s20, h3
 1a8:	fmul	s0, s4, s20
 1ac:	fmul	s1, s19, s4
 1b0:	fnmsub	s0, s5, s19, s0
 1b4:	fmadd	s1, s5, s20, s1
 1b8:	mov	w0, #0x7f800000            	// #2139095040
 1bc:	fmov	s4, w0
 1c0:	fmul	s0, s0, s4
 1c4:	fmul	s1, s1, s4
 1c8:	fcvt	h0, s0
 1cc:	fcvt	h1, s1
 1d0:	ret
 1d4:	fcmp	s4, s5
 1d8:	b.gt	a4 <__mulhc3+0xa4>
 1dc:	mov	w0, #0x0                   	// #0
 1e0:	b	104 <__mulhc3+0x104>
 1e4:	cbnz	w0, 330 <__mulhc3+0x330>
 1e8:	fabs	s5, s17
 1ec:	fcvt	h5, s5
 1f0:	fcvt	s5, h5
 1f4:	fcmp	s5, s4
 1f8:	b.gt	228 <__mulhc3+0x228>
 1fc:	fabs	s16, s16
 200:	fcvt	h16, s16
 204:	fcvt	s16, h16
 208:	fcmp	s16, s4
 20c:	b.gt	228 <__mulhc3+0x228>
 210:	fabs	s5, s23
 214:	fcvt	h5, s5
 218:	fcvt	s5, h5
 21c:	fcmp	s5, s4
 220:	b.le	268 <__mulhc3+0x268>
 224:	nop
 228:	fcvt	s5, h22
 22c:	fcmp	s5, s5
 230:	b.vs	318 <__mulhc3+0x318>
 234:	fcvt	s4, h21
 238:	fcmp	s4, s4
 23c:	b.vs	300 <__mulhc3+0x300>
 240:	fcmp	s19, s19
 244:	b.vs	2e8 <__mulhc3+0x2e8>
 248:	fcmp	s20, s20
 24c:	b.vc	1a8 <__mulhc3+0x1a8>
 250:	umov	w0, v3.h[0]
 254:	movi	v1.4h, #0x0
 258:	tbz	w0, #15, 260 <__mulhc3+0x260>
 25c:	movi	v1.4h, #0x80, lsl #8
 260:	fcvt	s20, h1
 264:	b	1a8 <__mulhc3+0x1a8>
 268:	fabs	s5, s24
 26c:	fcvt	h5, s5
 270:	fcvt	s5, h5
 274:	fcmp	s5, s4
 278:	b.gt	228 <__mulhc3+0x228>
 27c:	ret
 280:	umov	w0, v22.h[0]
 284:	movi	v1.4h, #0x0
 288:	tbz	w0, #15, 290 <__mulhc3+0x290>
 28c:	movi	v1.4h, #0x80, lsl #8
 290:	fcvt	s5, h1
 294:	b	194 <__mulhc3+0x194>
 298:	umov	w0, v21.h[0]
 29c:	movi	v4.4h, #0x0
 2a0:	tbz	w0, #15, 2a8 <__mulhc3+0x2a8>
 2a4:	movi	v4.4h, #0x80, lsl #8
 2a8:	fcvt	s19, h2
 2ac:	fcvt	s4, h4
 2b0:	fcvt	s20, h3
 2b4:	b	1a8 <__mulhc3+0x1a8>
 2b8:	umov	w1, v2.h[0]
 2bc:	movi	v2.4h, #0x0
 2c0:	tbz	w1, #15, 2c8 <__mulhc3+0x2c8>
 2c4:	movi	v2.4h, #0x80, lsl #8
 2c8:	fcvt	s19, h2
 2cc:	b	fc <__mulhc3+0xfc>
 2d0:	umov	w1, v3.h[0]
 2d4:	movi	v3.4h, #0x0
 2d8:	tbz	w1, #15, 2e0 <__mulhc3+0x2e0>
 2dc:	movi	v3.4h, #0x80, lsl #8
 2e0:	fcvt	s20, h3
 2e4:	b	104 <__mulhc3+0x104>
 2e8:	umov	w0, v2.h[0]
 2ec:	movi	v19.4h, #0x0
 2f0:	tbz	w0, #15, 2f8 <__mulhc3+0x2f8>
 2f4:	movi	v19.4h, #0x80, lsl #8
 2f8:	fcvt	s19, h19
 2fc:	b	248 <__mulhc3+0x248>
 300:	umov	w0, v21.h[0]
 304:	movi	v0.4h, #0x0
 308:	tbz	w0, #15, 310 <__mulhc3+0x310>
 30c:	movi	v0.4h, #0x80, lsl #8
 310:	fcvt	s4, h0
 314:	b	240 <__mulhc3+0x240>
 318:	umov	w0, v22.h[0]
 31c:	movi	v1.4h, #0x0
 320:	tbz	w0, #15, 328 <__mulhc3+0x328>
 324:	movi	v1.4h, #0x80, lsl #8
 328:	fcvt	s5, h1
 32c:	b	234 <__mulhc3+0x234>
 330:	fcvt	s5, h22
 334:	fcvt	s4, h21
 338:	b	1a8 <__mulhc3+0x1a8>

_mulsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s7, s1, s3
   4:	fmul	s5, s0, s2
   8:	fmul	s16, s0, s3
   c:	fmov	s18, s0
  10:	fmul	s17, s2, s1
  14:	fmov	s4, s1
  18:	fsub	s0, s5, s7
  1c:	fadd	s1, s16, s17
  20:	fcmp	s0, s0
  24:	cset	w0, vs
  28:	fcmp	s1, s1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	164 <__mulsc3+0x164>  // b.none
  38:	fabs	s19, s18
  3c:	mov	w1, #0x7f7fffff            	// #2139095039
  40:	fmov	s6, w1
  44:	fabs	s21, s4
  48:	fcmp	s19, s6
  4c:	b.le	168 <__mulsc3+0x168>
  50:	mov	w1, #0x7f7fffff            	// #2139095039
  54:	fmov	s6, w1
  58:	movi	v20.2s, #0x80, lsl #24
  5c:	fcmp	s19, s6
  60:	cset	w1, gt
  64:	fcmp	s21, s6
  68:	scvtf	s19, w1
  6c:	cset	w1, gt
  70:	fcmp	s2, s2
  74:	scvtf	s6, w1
  78:	bif	v18.8b, v19.8b, v20.8b
  7c:	bif	v4.8b, v6.8b, v20.8b
  80:	b.vs	1a4 <__mulsc3+0x1a4>
  84:	fcmp	s3, s3
  88:	b.vs	1b0 <__mulsc3+0x1b0>
  8c:	fabs	s19, s2
  90:	mov	w1, #0x7f7fffff            	// #2139095039
  94:	fmov	s6, w1
  98:	fabs	s20, s3
  9c:	fcmp	s19, s6
  a0:	b.gt	108 <__mulsc3+0x108>
  a4:	fcmp	s20, s6
  a8:	b.gt	108 <__mulsc3+0x108>
  ac:	cbnz	w0, 144 <__mulsc3+0x144>
  b0:	fabs	s5, s5
  b4:	fcmp	s5, s6
  b8:	b.gt	d8 <__mulsc3+0xd8>
  bc:	fabs	s7, s7
  c0:	fcmp	s7, s6
  c4:	b.gt	d8 <__mulsc3+0xd8>
  c8:	fabs	s16, s16
  cc:	fcmp	s16, s6
  d0:	b.le	178 <__mulsc3+0x178>
  d4:	nop
  d8:	fcmp	s18, s18
  dc:	b.vs	1e0 <__mulsc3+0x1e0>
  e0:	fcmp	s4, s4
  e4:	b.vs	1d0 <__mulsc3+0x1d0>
  e8:	fcmp	s2, s2
  ec:	b.vs	1c0 <__mulsc3+0x1c0>
  f0:	fcmp	s3, s3
  f4:	b.vc	144 <__mulsc3+0x144>
  f8:	movi	v0.2s, #0x0
  fc:	movi	v1.2s, #0x80, lsl #24
 100:	bif	v3.8b, v0.8b, v1.8b
 104:	b	144 <__mulsc3+0x144>
 108:	mov	w0, #0x7f7fffff            	// #2139095039
 10c:	fmov	s0, w0
 110:	movi	v5.2s, #0x80, lsl #24
 114:	fcmp	s19, s0
 118:	cset	w0, gt
 11c:	fcmp	s20, s0
 120:	scvtf	s1, w0
 124:	cset	w0, gt
 128:	fcmp	s18, s18
 12c:	scvtf	s0, w0
 130:	bif	v2.8b, v1.8b, v5.8b
 134:	bif	v3.8b, v0.8b, v5.8b
 138:	b.vs	188 <__mulsc3+0x188>
 13c:	fcmp	s4, s4
 140:	b.vs	194 <__mulsc3+0x194>
 144:	fmul	s5, s4, s3
 148:	fmul	s4, s4, s2
 14c:	fmadd	s3, s18, s3, s4
 150:	fnmsub	s2, s18, s2, s5
 154:	mov	w0, #0x7f800000            	// #2139095040
 158:	fmov	s1, w0
 15c:	fmul	s0, s2, s1
 160:	fmul	s1, s3, s1
 164:	ret
 168:	fcmp	s21, s6
 16c:	b.gt	50 <__mulsc3+0x50>
 170:	mov	w0, #0x0                   	// #0
 174:	b	8c <__mulsc3+0x8c>
 178:	fabs	s17, s17
 17c:	fcmp	s17, s6
 180:	b.gt	d8 <__mulsc3+0xd8>
 184:	ret
 188:	movi	v0.2s, #0x0
 18c:	bif	v18.8b, v0.8b, v5.8b
 190:	b	13c <__mulsc3+0x13c>
 194:	movi	v0.2s, #0x0
 198:	movi	v1.2s, #0x80, lsl #24
 19c:	bif	v4.8b, v0.8b, v1.8b
 1a0:	b	144 <__mulsc3+0x144>
 1a4:	movi	v6.2s, #0x0
 1a8:	bif	v2.8b, v6.8b, v20.8b
 1ac:	b	84 <__mulsc3+0x84>
 1b0:	movi	v6.2s, #0x0
 1b4:	movi	v19.2s, #0x80, lsl #24
 1b8:	bif	v3.8b, v6.8b, v19.8b
 1bc:	b	8c <__mulsc3+0x8c>
 1c0:	movi	v0.2s, #0x0
 1c4:	movi	v1.2s, #0x80, lsl #24
 1c8:	bif	v2.8b, v0.8b, v1.8b
 1cc:	b	f0 <__mulsc3+0xf0>
 1d0:	movi	v0.2s, #0x0
 1d4:	movi	v1.2s, #0x80, lsl #24
 1d8:	bif	v4.8b, v0.8b, v1.8b
 1dc:	b	e8 <__mulsc3+0xe8>
 1e0:	movi	v0.2s, #0x0
 1e4:	movi	v1.2s, #0x80, lsl #24
 1e8:	bif	v18.8b, v0.8b, v1.8b
 1ec:	b	e0 <__mulsc3+0xe0>

_muldc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d7, d1, d3
   4:	fmul	d5, d0, d2
   8:	fmul	d16, d0, d3
   c:	fmov	d18, d0
  10:	fmul	d17, d2, d1
  14:	fmov	d4, d1
  18:	fsub	d0, d5, d7
  1c:	fadd	d1, d16, d17
  20:	fcmp	d0, d0
  24:	cset	w0, vs
  28:	fcmp	d1, d1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	16c <__muldc3+0x16c>  // b.none
  38:	fabs	d19, d18
  3c:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  40:	fmov	d6, x1
  44:	fabs	d21, d4
  48:	fcmp	d19, d6
  4c:	b.le	170 <__muldc3+0x170>
  50:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  54:	fmov	d6, x1
  58:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  5c:	fmov	d20, x1
  60:	fcmp	d19, d6
  64:	cset	w1, gt
  68:	fcmp	d21, d6
  6c:	scvtf	d19, w1
  70:	cset	w1, gt
  74:	fcmp	d2, d2
  78:	scvtf	d6, w1
  7c:	bif	v18.8b, v19.8b, v20.8b
  80:	bif	v4.8b, v6.8b, v20.8b
  84:	b.vs	1b0 <__muldc3+0x1b0>
  88:	fcmp	d3, d3
  8c:	b.vs	1bc <__muldc3+0x1bc>
  90:	fabs	d19, d2
  94:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  98:	fmov	d6, x1
  9c:	fabs	d20, d3
  a0:	fcmp	d19, d6
  a4:	b.gt	10c <__muldc3+0x10c>
  a8:	fcmp	d20, d6
  ac:	b.gt	10c <__muldc3+0x10c>
  b0:	cbnz	w0, 14c <__muldc3+0x14c>
  b4:	fabs	d5, d5
  b8:	fcmp	d5, d6
  bc:	b.gt	d8 <__muldc3+0xd8>
  c0:	fabs	d7, d7
  c4:	fcmp	d7, d6
  c8:	b.gt	d8 <__muldc3+0xd8>
  cc:	fabs	d16, d16
  d0:	fcmp	d16, d6
  d4:	b.le	180 <__muldc3+0x180>
  d8:	fcmp	d18, d18
  dc:	b.vs	1f8 <__muldc3+0x1f8>
  e0:	fcmp	d4, d4
  e4:	b.vs	1e4 <__muldc3+0x1e4>
  e8:	fcmp	d2, d2
  ec:	b.vs	1d0 <__muldc3+0x1d0>
  f0:	fcmp	d3, d3
  f4:	b.vc	14c <__muldc3+0x14c>
  f8:	movi	d0, #0x0
  fc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 100:	fmov	d1, x0
 104:	bif	v3.8b, v0.8b, v1.8b
 108:	b	14c <__muldc3+0x14c>
 10c:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 110:	fmov	d0, x0
 114:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 118:	fmov	d5, x0
 11c:	fcmp	d19, d0
 120:	cset	w0, gt
 124:	fcmp	d20, d0
 128:	scvtf	d1, w0
 12c:	cset	w0, gt
 130:	fcmp	d18, d18
 134:	scvtf	d0, w0
 138:	bif	v2.8b, v1.8b, v5.8b
 13c:	bif	v3.8b, v0.8b, v5.8b
 140:	b.vs	190 <__muldc3+0x190>
 144:	fcmp	d4, d4
 148:	b.vs	19c <__muldc3+0x19c>
 14c:	fmul	d5, d4, d3
 150:	fmul	d4, d4, d2
 154:	fmadd	d3, d18, d3, d4
 158:	fnmsub	d2, d18, d2, d5
 15c:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 160:	fmov	d1, x0
 164:	fmul	d0, d2, d1
 168:	fmul	d1, d3, d1
 16c:	ret
 170:	fcmp	d21, d6
 174:	b.gt	50 <__muldc3+0x50>
 178:	mov	w0, #0x0                   	// #0
 17c:	b	90 <__muldc3+0x90>
 180:	fabs	d17, d17
 184:	fcmp	d17, d6
 188:	b.gt	d8 <__muldc3+0xd8>
 18c:	ret
 190:	movi	d0, #0x0
 194:	bif	v18.8b, v0.8b, v5.8b
 198:	b	144 <__muldc3+0x144>
 19c:	movi	d0, #0x0
 1a0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1a4:	fmov	d1, x0
 1a8:	bif	v4.8b, v0.8b, v1.8b
 1ac:	b	14c <__muldc3+0x14c>
 1b0:	movi	d6, #0x0
 1b4:	bif	v2.8b, v6.8b, v20.8b
 1b8:	b	88 <__muldc3+0x88>
 1bc:	movi	d6, #0x0
 1c0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 1c4:	fmov	d19, x1
 1c8:	bif	v3.8b, v6.8b, v19.8b
 1cc:	b	90 <__muldc3+0x90>
 1d0:	movi	d0, #0x0
 1d4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1d8:	fmov	d1, x0
 1dc:	bif	v2.8b, v0.8b, v1.8b
 1e0:	b	f0 <__muldc3+0xf0>
 1e4:	movi	d0, #0x0
 1e8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1ec:	fmov	d1, x0
 1f0:	bif	v4.8b, v0.8b, v1.8b
 1f4:	b	e8 <__muldc3+0xe8>
 1f8:	movi	d0, #0x0
 1fc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 200:	fmov	d1, x0
 204:	bif	v18.8b, v0.8b, v1.8b
 208:	b	e0 <__muldc3+0xe0>

_mulxc3.o:     file format elf64-littleaarch64


_multc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-256]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x23, x24, [sp, #48]
  10:	stp	x25, x26, [sp, #64]
  14:	str	q0, [sp, #96]
  18:	str	q2, [sp, #112]
  1c:	ldp	x20, x25, [sp, #96]
  20:	stp	x21, x22, [sp, #32]
  24:	ldp	x19, x24, [sp, #112]
  28:	stp	x19, x24, [sp, #96]
  2c:	stp	x20, x25, [sp, #112]
  30:	stp	x27, x28, [sp, #80]
  34:	ldr	q0, [sp, #112]
  38:	str	q1, [sp, #128]
  3c:	str	q3, [sp, #144]
  40:	ldp	x21, x23, [sp, #128]
  44:	ldp	x22, x26, [sp, #144]
  48:	ldr	q1, [sp, #96]
  4c:	bl	0 <__multf3>
  50:	stp	x22, x26, [sp, #96]
  54:	stp	x21, x23, [sp, #112]
  58:	ldr	q1, [sp, #96]
  5c:	str	q0, [sp, #128]
  60:	ldr	q0, [sp, #112]
  64:	ldp	x28, x27, [sp, #128]
  68:	bl	0 <__multf3>
  6c:	str	q0, [sp, #128]
  70:	ldp	x1, x0, [sp, #128]
  74:	stp	x22, x26, [sp, #96]
  78:	stp	x20, x25, [sp, #112]
  7c:	ldr	q1, [sp, #96]
  80:	ldr	q0, [sp, #112]
  84:	str	x1, [sp, #144]
  88:	str	x0, [sp, #168]
  8c:	bl	0 <__multf3>
  90:	str	q0, [sp, #128]
  94:	ldp	x1, x0, [sp, #128]
  98:	stp	x21, x23, [sp, #96]
  9c:	stp	x19, x24, [sp, #112]
  a0:	ldr	q1, [sp, #96]
  a4:	ldr	q0, [sp, #112]
  a8:	stp	x1, x0, [sp, #176]
  ac:	bl	0 <__multf3>
  b0:	str	q0, [sp, #128]
  b4:	ldp	x0, x2, [sp, #136]
  b8:	str	x2, [sp, #96]
  bc:	ldr	x2, [sp, #168]
  c0:	str	x2, [sp, #104]
  c4:	stp	x28, x27, [sp, #112]
  c8:	ldr	q1, [sp, #96]
  cc:	ldr	q0, [sp, #112]
  d0:	str	x0, [sp, #224]
  d4:	bl	0 <__subtf3>
  d8:	ldr	x1, [sp, #176]
  dc:	str	x1, [sp, #112]
  e0:	ldr	x2, [sp, #128]
  e4:	str	x2, [sp, #96]
  e8:	ldr	x1, [sp, #224]
  ec:	str	x1, [sp, #104]
  f0:	ldr	x0, [sp, #184]
  f4:	ldr	q1, [sp, #96]
  f8:	str	x0, [sp, #120]
  fc:	str	q0, [sp, #96]
 100:	ldr	q0, [sp, #112]
 104:	bl	0 <__addtf3>
 108:	str	q0, [sp, #112]
 10c:	ldr	q1, [sp, #96]
 110:	mov	v0.16b, v1.16b
 114:	bl	0 <__unordtf2>
 118:	cmp	w0, #0x0
 11c:	ldr	q0, [sp, #112]
 120:	cset	w1, ne  // ne = any
 124:	str	w1, [sp, #192]
 128:	mov	v1.16b, v0.16b
 12c:	bl	0 <__unordtf2>
 130:	cmp	w0, #0x0
 134:	ldr	w1, [sp, #192]
 138:	cset	w0, ne  // ne = any
 13c:	ands	w0, w0, w1
 140:	str	w0, [sp, #236]
 144:	b.eq	470 <__multc3+0x470>  // b.none
 148:	adrp	x1, 0 <__multc3>
 14c:	add	x1, x1, #0x0
 150:	and	x0, x25, #0x7fffffffffffffff
 154:	stp	x20, x0, [sp, #192]
 158:	ldr	q1, [x1]
 15c:	and	x1, x23, #0x7fffffffffffffff
 160:	ldr	q0, [sp, #192]
 164:	str	x1, [sp, #192]
 168:	str	x0, [sp, #208]
 16c:	bl	0 <__unordtf2>
 170:	cbnz	w0, 198 <__multc3+0x198>
 174:	adrp	x1, 0 <__multc3>
 178:	add	x1, x1, #0x0
 17c:	ldr	x0, [sp, #208]
 180:	stp	x20, x0, [sp, #192]
 184:	ldr	q1, [x1]
 188:	ldr	q0, [sp, #192]
 18c:	bl	0 <__letf2>
 190:	cmp	w0, #0x0
 194:	b.gt	1dc <__multc3+0x1dc>
 198:	adrp	x0, 0 <__multc3>
 19c:	add	x0, x0, #0x0
 1a0:	ldr	q1, [x0]
 1a4:	and	x0, x23, #0x7fffffffffffffff
 1a8:	stp	x21, x0, [sp, #192]
 1ac:	ldr	q0, [sp, #192]
 1b0:	bl	0 <__unordtf2>
 1b4:	cbnz	w0, 678 <__multc3+0x678>
 1b8:	adrp	x1, 0 <__multc3>
 1bc:	add	x1, x1, #0x0
 1c0:	and	x0, x23, #0x7fffffffffffffff
 1c4:	stp	x21, x0, [sp, #192]
 1c8:	ldr	q1, [x1]
 1cc:	ldr	q0, [sp, #192]
 1d0:	bl	0 <__letf2>
 1d4:	cmp	w0, #0x0
 1d8:	b.le	678 <__multc3+0x678>
 1dc:	adrp	x0, 0 <__multc3>
 1e0:	add	x0, x0, #0x0
 1e4:	mov	w1, #0x1                   	// #1
 1e8:	str	w1, [sp, #240]
 1ec:	ldr	q1, [x0]
 1f0:	ldr	x0, [sp, #208]
 1f4:	stp	x20, x0, [sp, #192]
 1f8:	ldr	q0, [sp, #192]
 1fc:	bl	0 <__unordtf2>
 200:	ldr	w1, [sp, #240]
 204:	cbz	w0, 504 <__multc3+0x504>
 208:	eor	w0, w1, #0x1
 20c:	mov	w1, #0x1                   	// #1
 210:	and	w0, w0, #0x1
 214:	str	w1, [sp, #240]
 218:	bl	0 <__floatsitf>
 21c:	str	q0, [sp, #208]
 220:	ldp	x20, x0, [sp, #208]
 224:	and	x1, x23, #0x7fffffffffffffff
 228:	stp	x21, x1, [sp, #192]
 22c:	ldr	q0, [sp, #192]
 230:	bfxil	x25, x0, #0, #63
 234:	adrp	x0, 0 <__multc3>
 238:	add	x0, x0, #0x0
 23c:	ldr	q1, [x0]
 240:	bl	0 <__unordtf2>
 244:	ldr	w1, [sp, #240]
 248:	cbz	w0, 4dc <__multc3+0x4dc>
 24c:	eor	w0, w1, #0x1
 250:	and	w0, w0, #0x1
 254:	bl	0 <__floatsitf>
 258:	str	q0, [sp, #240]
 25c:	ldp	x21, x0, [sp, #240]
 260:	stp	x19, x24, [sp, #192]
 264:	stp	x19, x24, [sp, #208]
 268:	ldr	q1, [sp, #192]
 26c:	ldr	q0, [sp, #208]
 270:	bfxil	x23, x0, #0, #63
 274:	bl	0 <__unordtf2>
 278:	cbnz	w0, 714 <__multc3+0x714>
 27c:	stp	x22, x26, [sp, #192]
 280:	stp	x22, x26, [sp, #208]
 284:	ldr	q1, [sp, #192]
 288:	ldr	q0, [sp, #208]
 28c:	bl	0 <__unordtf2>
 290:	cbnz	w0, 6fc <__multc3+0x6fc>
 294:	adrp	x1, 0 <__multc3>
 298:	add	x1, x1, #0x0
 29c:	and	x0, x24, #0x7fffffffffffffff
 2a0:	stp	x19, x0, [sp, #192]
 2a4:	ldr	q1, [x1]
 2a8:	and	x1, x26, #0x7fffffffffffffff
 2ac:	ldr	q0, [sp, #192]
 2b0:	str	x1, [sp, #192]
 2b4:	str	x0, [sp, #208]
 2b8:	bl	0 <__unordtf2>
 2bc:	cbnz	w0, 2e4 <__multc3+0x2e4>
 2c0:	adrp	x1, 0 <__multc3>
 2c4:	add	x1, x1, #0x0
 2c8:	ldr	x0, [sp, #208]
 2cc:	stp	x19, x0, [sp, #192]
 2d0:	ldr	q1, [x1]
 2d4:	ldr	q0, [sp, #192]
 2d8:	bl	0 <__letf2>
 2dc:	cmp	w0, #0x0
 2e0:	b.gt	328 <__multc3+0x328>
 2e4:	adrp	x0, 0 <__multc3>
 2e8:	add	x0, x0, #0x0
 2ec:	ldr	q1, [x0]
 2f0:	and	x0, x26, #0x7fffffffffffffff
 2f4:	stp	x22, x0, [sp, #192]
 2f8:	ldr	q0, [sp, #192]
 2fc:	bl	0 <__unordtf2>
 300:	cbnz	w0, 52c <__multc3+0x52c>
 304:	adrp	x1, 0 <__multc3>
 308:	add	x1, x1, #0x0
 30c:	and	x0, x26, #0x7fffffffffffffff
 310:	stp	x22, x0, [sp, #192]
 314:	ldr	q1, [x1]
 318:	ldr	q0, [sp, #192]
 31c:	bl	0 <__letf2>
 320:	cmp	w0, #0x0
 324:	b.le	52c <__multc3+0x52c>
 328:	adrp	x0, 0 <__multc3>
 32c:	add	x0, x0, #0x0
 330:	ldr	x28, [sp, #208]
 334:	stp	x19, x28, [sp, #96]
 338:	ldr	q1, [x0]
 33c:	ldr	q0, [sp, #96]
 340:	mov	w27, #0x1                   	// #1
 344:	bl	0 <__unordtf2>
 348:	cbz	w0, 4b8 <__multc3+0x4b8>
 34c:	eor	w0, w27, #0x1
 350:	and	x28, x26, #0x7fffffffffffffff
 354:	and	w0, w0, #0x1
 358:	bl	0 <__floatsitf>
 35c:	str	q0, [sp, #112]
 360:	mov	w27, #0x1                   	// #1
 364:	ldp	x19, x0, [sp, #112]
 368:	stp	x22, x28, [sp, #96]
 36c:	ldr	q0, [sp, #96]
 370:	bfxil	x24, x0, #0, #63
 374:	adrp	x0, 0 <__multc3>
 378:	add	x0, x0, #0x0
 37c:	ldr	q1, [x0]
 380:	bl	0 <__unordtf2>
 384:	cbz	w0, 494 <__multc3+0x494>
 388:	eor	w0, w27, #0x1
 38c:	and	w0, w0, #0x1
 390:	bl	0 <__floatsitf>
 394:	str	q0, [sp, #128]
 398:	ldp	x22, x0, [sp, #128]
 39c:	stp	x20, x25, [sp, #96]
 3a0:	stp	x20, x25, [sp, #112]
 3a4:	ldr	q1, [sp, #96]
 3a8:	ldr	q0, [sp, #112]
 3ac:	bfxil	x26, x0, #0, #63
 3b0:	bl	0 <__unordtf2>
 3b4:	cbnz	w0, 6cc <__multc3+0x6cc>
 3b8:	stp	x21, x23, [sp, #96]
 3bc:	stp	x21, x23, [sp, #112]
 3c0:	ldr	q1, [sp, #96]
 3c4:	ldr	q0, [sp, #112]
 3c8:	bl	0 <__unordtf2>
 3cc:	cbnz	w0, 6e4 <__multc3+0x6e4>
 3d0:	stp	x19, x24, [sp, #96]
 3d4:	stp	x20, x25, [sp, #112]
 3d8:	ldr	q1, [sp, #96]
 3dc:	ldr	q0, [sp, #112]
 3e0:	bl	0 <__multf3>
 3e4:	stp	x22, x26, [sp, #96]
 3e8:	stp	x21, x23, [sp, #112]
 3ec:	ldr	q1, [sp, #96]
 3f0:	str	q0, [sp, #96]
 3f4:	ldr	q0, [sp, #112]
 3f8:	bl	0 <__multf3>
 3fc:	mov	v1.16b, v0.16b
 400:	ldr	q2, [sp, #96]
 404:	mov	v0.16b, v2.16b
 408:	bl	0 <__subtf3>
 40c:	adrp	x0, 0 <__multc3>
 410:	add	x0, x0, #0x0
 414:	ldr	q1, [x0]
 418:	bl	0 <__multf3>
 41c:	stp	x22, x26, [sp, #96]
 420:	stp	x20, x25, [sp, #112]
 424:	ldr	q1, [sp, #96]
 428:	str	q0, [sp, #96]
 42c:	ldr	q0, [sp, #112]
 430:	bl	0 <__multf3>
 434:	stp	x19, x24, [sp, #112]
 438:	stp	x21, x23, [sp, #128]
 43c:	ldr	q1, [sp, #112]
 440:	str	q0, [sp, #112]
 444:	ldr	q0, [sp, #128]
 448:	bl	0 <__multf3>
 44c:	mov	v1.16b, v0.16b
 450:	ldr	q2, [sp, #112]
 454:	mov	v0.16b, v2.16b
 458:	bl	0 <__addtf3>
 45c:	adrp	x0, 0 <__multc3>
 460:	add	x0, x0, #0x0
 464:	ldr	q1, [x0]
 468:	bl	0 <__multf3>
 46c:	str	q0, [sp, #112]
 470:	ldp	x19, x20, [sp, #16]
 474:	ldp	x21, x22, [sp, #32]
 478:	ldp	x23, x24, [sp, #48]
 47c:	ldp	x25, x26, [sp, #64]
 480:	ldp	x27, x28, [sp, #80]
 484:	ldr	q0, [sp, #96]
 488:	ldr	q1, [sp, #112]
 48c:	ldp	x29, x30, [sp], #256
 490:	ret
 494:	adrp	x0, 0 <__multc3>
 498:	add	x0, x0, #0x0
 49c:	stp	x22, x28, [sp, #96]
 4a0:	ldr	q1, [x0]
 4a4:	ldr	q0, [sp, #96]
 4a8:	bl	0 <__letf2>
 4ac:	cmp	w0, #0x0
 4b0:	cset	w27, le
 4b4:	b	388 <__multc3+0x388>
 4b8:	adrp	x0, 0 <__multc3>
 4bc:	add	x0, x0, #0x0
 4c0:	stp	x19, x28, [sp, #96]
 4c4:	ldr	q1, [x0]
 4c8:	ldr	q0, [sp, #96]
 4cc:	bl	0 <__letf2>
 4d0:	cmp	w0, #0x0
 4d4:	cset	w27, le
 4d8:	b	34c <__multc3+0x34c>
 4dc:	adrp	x0, 0 <__multc3>
 4e0:	add	x0, x0, #0x0
 4e4:	and	x1, x23, #0x7fffffffffffffff
 4e8:	stp	x21, x1, [sp, #192]
 4ec:	ldr	q1, [x0]
 4f0:	ldr	q0, [sp, #192]
 4f4:	bl	0 <__letf2>
 4f8:	cmp	w0, #0x0
 4fc:	cset	w1, le
 500:	b	24c <__multc3+0x24c>
 504:	adrp	x1, 0 <__multc3>
 508:	add	x1, x1, #0x0
 50c:	ldr	x0, [sp, #208]
 510:	stp	x20, x0, [sp, #192]
 514:	ldr	q1, [x1]
 518:	ldr	q0, [sp, #192]
 51c:	bl	0 <__letf2>
 520:	cmp	w0, #0x0
 524:	cset	w1, le
 528:	b	208 <__multc3+0x208>
 52c:	ldr	w0, [sp, #236]
 530:	cbnz	w0, 3d0 <__multc3+0x3d0>
 534:	adrp	x0, 0 <__multc3>
 538:	add	x0, x0, #0x0
 53c:	and	x27, x27, #0x7fffffffffffffff
 540:	stp	x28, x27, [sp, #192]
 544:	ldr	q1, [x0]
 548:	ldr	q0, [sp, #192]
 54c:	bl	0 <__unordtf2>
 550:	cbnz	w0, 574 <__multc3+0x574>
 554:	adrp	x0, 0 <__multc3>
 558:	add	x0, x0, #0x0
 55c:	stp	x28, x27, [sp, #192]
 560:	ldr	q1, [x0]
 564:	ldr	q0, [sp, #192]
 568:	bl	0 <__letf2>
 56c:	cmp	w0, #0x0
 570:	b.gt	600 <__multc3+0x600>
 574:	ldr	x0, [sp, #168]
 578:	ldr	x28, [sp, #144]
 57c:	and	x27, x0, #0x7fffffffffffffff
 580:	adrp	x0, 0 <__multc3>
 584:	add	x0, x0, #0x0
 588:	stp	x28, x27, [sp, #192]
 58c:	ldr	q1, [x0]
 590:	ldr	q0, [sp, #192]
 594:	bl	0 <__unordtf2>
 598:	cbnz	w0, 5bc <__multc3+0x5bc>
 59c:	adrp	x0, 0 <__multc3>
 5a0:	add	x0, x0, #0x0
 5a4:	stp	x28, x27, [sp, #144]
 5a8:	ldr	q1, [x0]
 5ac:	ldr	q0, [sp, #144]
 5b0:	bl	0 <__letf2>
 5b4:	cmp	w0, #0x0
 5b8:	b.gt	600 <__multc3+0x600>
 5bc:	ldp	x28, x0, [sp, #176]
 5c0:	and	x27, x0, #0x7fffffffffffffff
 5c4:	adrp	x0, 0 <__multc3>
 5c8:	add	x0, x0, #0x0
 5cc:	stp	x28, x27, [sp, #144]
 5d0:	ldr	q0, [sp, #144]
 5d4:	ldr	q1, [x0]
 5d8:	bl	0 <__unordtf2>
 5dc:	cbnz	w0, 680 <__multc3+0x680>
 5e0:	adrp	x0, 0 <__multc3>
 5e4:	add	x0, x0, #0x0
 5e8:	stp	x28, x27, [sp, #144]
 5ec:	ldr	q1, [x0]
 5f0:	ldr	q0, [sp, #144]
 5f4:	bl	0 <__letf2>
 5f8:	cmp	w0, #0x0
 5fc:	b.le	680 <__multc3+0x680>
 600:	stp	x20, x25, [sp, #96]
 604:	stp	x20, x25, [sp, #112]
 608:	ldr	q1, [sp, #96]
 60c:	ldr	q0, [sp, #112]
 610:	bl	0 <__unordtf2>
 614:	cbnz	w0, 75c <__multc3+0x75c>
 618:	stp	x21, x23, [sp, #96]
 61c:	stp	x21, x23, [sp, #112]
 620:	ldr	q1, [sp, #96]
 624:	ldr	q0, [sp, #112]
 628:	bl	0 <__unordtf2>
 62c:	cbnz	w0, 744 <__multc3+0x744>
 630:	stp	x19, x24, [sp, #96]
 634:	stp	x19, x24, [sp, #112]
 638:	ldr	q1, [sp, #96]
 63c:	ldr	q0, [sp, #112]
 640:	bl	0 <__unordtf2>
 644:	cbnz	w0, 72c <__multc3+0x72c>
 648:	stp	x22, x26, [sp, #96]
 64c:	stp	x22, x26, [sp, #112]
 650:	ldr	q1, [sp, #96]
 654:	ldr	q0, [sp, #112]
 658:	bl	0 <__unordtf2>
 65c:	cbz	w0, 3d0 <__multc3+0x3d0>
 660:	mov	x22, #0x0                   	// #0
 664:	mov	x0, #0x0                   	// #0
 668:	tbz	x26, #63, 670 <__multc3+0x670>
 66c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 670:	mov	x26, x0
 674:	b	3d0 <__multc3+0x3d0>
 678:	str	wzr, [sp, #236]
 67c:	b	294 <__multc3+0x294>
 680:	ldr	x0, [sp, #224]
 684:	ldr	x28, [sp, #128]
 688:	and	x27, x0, #0x7fffffffffffffff
 68c:	adrp	x0, 0 <__multc3>
 690:	add	x0, x0, #0x0
 694:	stp	x28, x27, [sp, #144]
 698:	ldr	q1, [x0]
 69c:	ldr	q0, [sp, #144]
 6a0:	bl	0 <__unordtf2>
 6a4:	cbnz	w0, 470 <__multc3+0x470>
 6a8:	adrp	x0, 0 <__multc3>
 6ac:	add	x0, x0, #0x0
 6b0:	stp	x28, x27, [sp, #128]
 6b4:	ldr	q1, [x0]
 6b8:	ldr	q0, [sp, #128]
 6bc:	bl	0 <__letf2>
 6c0:	cmp	w0, #0x0
 6c4:	b.gt	600 <__multc3+0x600>
 6c8:	b	470 <__multc3+0x470>
 6cc:	mov	x20, #0x0                   	// #0
 6d0:	mov	x0, #0x0                   	// #0
 6d4:	tbz	x25, #63, 6dc <__multc3+0x6dc>
 6d8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6dc:	mov	x25, x0
 6e0:	b	3b8 <__multc3+0x3b8>
 6e4:	mov	x21, #0x0                   	// #0
 6e8:	mov	x0, #0x0                   	// #0
 6ec:	tbz	x23, #63, 6f4 <__multc3+0x6f4>
 6f0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6f4:	mov	x23, x0
 6f8:	b	3d0 <__multc3+0x3d0>
 6fc:	mov	x22, #0x0                   	// #0
 700:	mov	x0, #0x0                   	// #0
 704:	tbz	x26, #63, 70c <__multc3+0x70c>
 708:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 70c:	mov	x26, x0
 710:	b	294 <__multc3+0x294>
 714:	mov	x19, #0x0                   	// #0
 718:	mov	x0, #0x0                   	// #0
 71c:	tbz	x24, #63, 724 <__multc3+0x724>
 720:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 724:	mov	x24, x0
 728:	b	27c <__multc3+0x27c>
 72c:	mov	x19, #0x0                   	// #0
 730:	mov	x0, #0x0                   	// #0
 734:	tbz	x24, #63, 73c <__multc3+0x73c>
 738:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 73c:	mov	x24, x0
 740:	b	648 <__multc3+0x648>
 744:	mov	x21, #0x0                   	// #0
 748:	mov	x0, #0x0                   	// #0
 74c:	tbz	x23, #63, 754 <__multc3+0x754>
 750:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 754:	mov	x23, x0
 758:	b	630 <__multc3+0x630>
 75c:	mov	x20, #0x0                   	// #0
 760:	mov	x0, #0x0                   	// #0
 764:	tbz	x25, #63, 76c <__multc3+0x76c>
 768:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 76c:	mov	x25, x0
 770:	b	618 <__multc3+0x618>

_divhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divhc3>:
   0:	fcvt	s19, h2
   4:	fcvt	s16, h3
   8:	mov	v17.h[0], v0.h[0]
   c:	mov	v18.h[0], v1.h[0]
  10:	fcvt	s20, h1
  14:	fcvt	s21, h0
  18:	fabs	s5, s19
  1c:	fabs	s6, s16
  20:	fcvt	h5, s5
  24:	fcvt	h6, s6
  28:	fcvt	s5, h5
  2c:	fcvt	s6, h6
  30:	fcmpe	s5, s6
  34:	b.pl	178 <__divhc3+0x178>  // b.nfrst
  38:	fdiv	s4, s19, s16
  3c:	fcvt	h4, s4
  40:	fcvt	s4, h4
  44:	fmadd	s1, s4, s19, s16
  48:	fmadd	s0, s4, s21, s20
  4c:	fnmsub	s4, s4, s20, s21
  50:	fcvt	h1, s1
  54:	fcvt	s1, h1
  58:	fdiv	s0, s0, s1
  5c:	fdiv	s4, s4, s1
  60:	fcvt	h0, s0
  64:	fcvt	h1, s4
  68:	fcvt	s7, h0
  6c:	fcvt	s4, h1
  70:	fcmp	s7, s7
  74:	cset	w1, vs
  78:	fcmp	s4, s4
  7c:	cset	w0, vs
  80:	tst	w1, w0
  84:	b.eq	174 <__divhc3+0x174>  // b.none
  88:	fcmp	s19, #0.0
  8c:	b.eq	194 <__divhc3+0x194>  // b.none
  90:	fabs	s4, s21
  94:	mov	w0, #0xe000                	// #57344
  98:	movk	w0, #0x477f, lsl #16
  9c:	fmov	s7, w0
  a0:	fcvt	h4, s4
  a4:	fcvt	s4, h4
  a8:	fcmp	s4, s7
  ac:	b.gt	1d8 <__divhc3+0x1d8>
  b0:	fabs	s22, s20
  b4:	fcvt	h22, s22
  b8:	fcvt	s22, h22
  bc:	fcmp	s22, s7
  c0:	b.gt	1d8 <__divhc3+0x1d8>
  c4:	mov	w0, #0xe000                	// #57344
  c8:	movk	w0, #0x477f, lsl #16
  cc:	fmov	s7, w0
  d0:	fcmp	s5, s7
  d4:	cset	w0, le
  d8:	fcmp	s6, s7
  dc:	cset	w1, le
  e0:	cmp	w0, #0x0
  e4:	ccmp	w1, #0x0, #0x4, ne  // ne = any
  e8:	b.ne	174 <__divhc3+0x174>  // b.any
  ec:	fcmp	s4, s7
  f0:	b.hi	174 <__divhc3+0x174>  // b.pmore
  f4:	fabs	s4, s20
  f8:	fcvt	h4, s4
  fc:	fcvt	s4, h4
 100:	fcmp	s4, s7
 104:	b.hi	174 <__divhc3+0x174>  // b.pmore
 108:	eor	w3, w1, #0x1
 10c:	eor	w2, w0, #0x1
 110:	umov	w1, v2.h[0]
 114:	umov	w0, v3.h[0]
 118:	scvtf	d2, w3
 11c:	scvtf	d0, w2
 120:	movi	d4, #0x0
 124:	fcvt	h1, d2
 128:	fcvt	h0, d0
 12c:	umov	w3, v1.h[0]
 130:	umov	w2, v0.h[0]
 134:	bfxil	w0, w3, #0, #15
 138:	bfxil	w1, w2, #0, #15
 13c:	dup	v2.4h, w0
 140:	dup	v1.4h, w1
 144:	fcvt	s2, h2
 148:	fcvt	s3, h1
 14c:	fmul	s0, s2, s20
 150:	fmul	s1, s2, s21
 154:	fmadd	s0, s3, s21, s0
 158:	fnmsub	s1, s3, s20, s1
 15c:	fcvt	d0, s0
 160:	fcvt	d1, s1
 164:	fmul	d0, d0, d4
 168:	fmul	d1, d1, d4
 16c:	fcvt	h0, d0
 170:	fcvt	h1, d1
 174:	ret
 178:	fdiv	s4, s16, s19
 17c:	fcvt	h4, s4
 180:	fcvt	s4, h4
 184:	fmadd	s1, s4, s16, s19
 188:	fmadd	s0, s4, s20, s21
 18c:	fmsub	s4, s4, s21, s20
 190:	b	50 <__divhc3+0x50>
 194:	fcmp	s16, #0.0
 198:	b.ne	90 <__divhc3+0x90>  // b.any
 19c:	fcmp	s21, s21
 1a0:	cset	w0, vc
 1a4:	fcmp	s20, s20
 1a8:	cset	w1, vc
 1ac:	orr	w0, w0, w1
 1b0:	cbz	w0, 90 <__divhc3+0x90>
 1b4:	umov	w0, v2.h[0]
 1b8:	movi	v1.4h, #0x7c, lsl #8
 1bc:	tbnz	w0, #15, 274 <__divhc3+0x274>
 1c0:	fcvt	s1, h1
 1c4:	fmul	s0, s1, s21
 1c8:	fmul	s1, s1, s20
 1cc:	fcvt	h0, s0
 1d0:	fcvt	h1, s1
 1d4:	ret
 1d8:	mov	w0, #0xe000                	// #57344
 1dc:	movk	w0, #0x477f, lsl #16
 1e0:	fmov	s7, w0
 1e4:	fcmp	s5, s7
 1e8:	b.hi	c4 <__divhc3+0xc4>  // b.pmore
 1ec:	fcmp	s6, s7
 1f0:	b.hi	c4 <__divhc3+0xc4>  // b.pmore
 1f4:	fcmp	s4, s7
 1f8:	mov	w2, #0x7f800000            	// #2139095040
 1fc:	fmov	s4, w2
 200:	fabs	s1, s20
 204:	umov	w0, v17.h[0]
 208:	umov	w1, v18.h[0]
 20c:	cset	w2, gt
 210:	fcvt	h1, s1
 214:	scvtf	d2, w2
 218:	fcvt	s1, h1
 21c:	fcvt	h0, d2
 220:	fcmp	s1, s7
 224:	umov	w2, v0.h[0]
 228:	bfxil	w0, w2, #0, #15
 22c:	dup	v0.4h, w0
 230:	cset	w0, gt
 234:	scvtf	d1, w0
 238:	fcvt	s2, h0
 23c:	fcvt	h0, d1
 240:	fmul	s1, s2, s16
 244:	umov	w0, v0.h[0]
 248:	bfxil	w1, w0, #0, #15
 24c:	dup	v3.4h, w1
 250:	fcvt	s3, h3
 254:	fmul	s0, s3, s16
 258:	fnmsub	s1, s3, s19, s1
 25c:	fmadd	s0, s2, s19, s0
 260:	fmul	s1, s1, s4
 264:	fmul	s0, s0, s4
 268:	fcvt	h1, s1
 26c:	fcvt	h0, s0
 270:	ret
 274:	movi	v1.4h, #0xfc, lsl #8
 278:	b	1c0 <__divhc3+0x1c0>

_divsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	fabs	s6, s2
   4:	fabs	s7, s3
   8:	fmov	s16, s0
   c:	fmov	s17, s1
  10:	fcmpe	s6, s7
  14:	b.pl	74 <__divsc3+0x74>  // b.nfrst
  18:	fdiv	s4, s2, s3
  1c:	fmadd	s1, s2, s4, s3
  20:	fmadd	s5, s0, s4, s17
  24:	fnmsub	s4, s17, s4, s0
  28:	fdiv	s0, s5, s1
  2c:	fdiv	s1, s4, s1
  30:	fcmp	s0, s0
  34:	fccmp	s1, s1, #0x0, vs
  38:	b.vc	70 <__divsc3+0x70>
  3c:	fcmp	s2, #0.0
  40:	movi	v4.2s, #0x0
  44:	fccmp	s3, s4, #0x0, eq  // eq = none
  48:	b.ne	90 <__divsc3+0x90>  // b.any
  4c:	fcmp	s16, s16
  50:	fccmp	s17, s17, #0x0, vs
  54:	b.vs	90 <__divsc3+0x90>
  58:	movi	v1.2s, #0x80, lsl #24
  5c:	mov	w0, #0x7f800000            	// #2139095040
  60:	fmov	s3, w0
  64:	bif	v2.8b, v3.8b, v1.8b
  68:	fmul	s0, s2, s16
  6c:	fmul	s1, s2, s17
  70:	ret
  74:	fdiv	s4, s3, s2
  78:	fmadd	s1, s3, s4, s2
  7c:	fmadd	s5, s4, s17, s0
  80:	fmsub	s4, s4, s0, s17
  84:	fdiv	s0, s5, s1
  88:	fdiv	s1, s4, s1
  8c:	b	30 <__divsc3+0x30>
  90:	fabs	s5, s16
  94:	mov	w0, #0x7f7fffff            	// #2139095039
  98:	fmov	s4, w0
  9c:	fcmp	s5, s4
  a0:	b.gt	124 <__divsc3+0x124>
  a4:	fabs	s18, s17
  a8:	fcmp	s18, s4
  ac:	b.gt	124 <__divsc3+0x124>
  b0:	mov	w0, #0x7f7fffff            	// #2139095039
  b4:	fmov	s4, w0
  b8:	fcmp	s6, s4
  bc:	cset	w1, le
  c0:	fcmp	s7, s4
  c4:	cset	w0, le
  c8:	cmp	w1, #0x0
  cc:	ccmp	w0, #0x0, #0x4, ne  // ne = any
  d0:	b.ne	70 <__divsc3+0x70>  // b.any
  d4:	fcmp	s5, s4
  d8:	b.hi	70 <__divsc3+0x70>  // b.pmore
  dc:	fabs	s5, s17
  e0:	fcmp	s5, s4
  e4:	b.hi	70 <__divsc3+0x70>  // b.pmore
  e8:	eor	w0, w0, #0x1
  ec:	eor	w1, w1, #0x1
  f0:	movi	v5.2s, #0x80, lsl #24
  f4:	movi	v4.2s, #0x0
  f8:	scvtf	s0, w0
  fc:	scvtf	s1, w1
 100:	bit	v0.8b, v3.8b, v5.8b
 104:	bif	v2.8b, v1.8b, v5.8b
 108:	fmul	s1, s16, s0
 10c:	fmul	s5, s17, s0
 110:	fmadd	s0, s16, s2, s5
 114:	fnmsub	s1, s17, s2, s1
 118:	fmul	s0, s0, s4
 11c:	fmul	s1, s1, s4
 120:	ret
 124:	mov	w0, #0x7f7fffff            	// #2139095039
 128:	fmov	s4, w0
 12c:	fcmp	s6, s4
 130:	b.hi	b0 <__divsc3+0xb0>  // b.pmore
 134:	fcmp	s7, s4
 138:	b.hi	b0 <__divsc3+0xb0>  // b.pmore
 13c:	fcmp	s5, s4
 140:	fabs	s0, s17
 144:	mov	w0, #0x7f800000            	// #2139095040
 148:	fmov	s5, w0
 14c:	movi	v6.2s, #0x80, lsl #24
 150:	cset	w0, gt
 154:	fcmp	s0, s4
 158:	scvtf	s0, w0
 15c:	cset	w0, gt
 160:	scvtf	s1, w0
 164:	bif	v16.8b, v0.8b, v6.8b
 168:	bit	v1.8b, v17.8b, v6.8b
 16c:	fmul	s4, s3, s16
 170:	fmul	s0, s3, s1
 174:	fnmsub	s3, s2, s1, s4
 178:	fmadd	s2, s2, s16, s0
 17c:	fmul	s1, s3, s5
 180:	fmul	s0, s2, s5
 184:	ret

_divdc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	fabs	d6, d2
   4:	fabs	d7, d3
   8:	fmov	d16, d0
   c:	fmov	d17, d1
  10:	fcmpe	d6, d7
  14:	b.pl	78 <__divdc3+0x78>  // b.nfrst
  18:	fdiv	d4, d2, d3
  1c:	fmadd	d1, d2, d4, d3
  20:	fmadd	d5, d0, d4, d17
  24:	fnmsub	d4, d17, d4, d0
  28:	fdiv	d0, d5, d1
  2c:	fdiv	d1, d4, d1
  30:	fcmp	d0, d0
  34:	fccmp	d1, d1, #0x0, vs
  38:	b.vc	74 <__divdc3+0x74>
  3c:	fcmp	d2, #0.0
  40:	movi	d4, #0x0
  44:	fccmp	d3, d4, #0x0, eq  // eq = none
  48:	b.ne	94 <__divdc3+0x94>  // b.any
  4c:	fcmp	d16, d16
  50:	fccmp	d17, d17, #0x0, vs
  54:	b.vs	94 <__divdc3+0x94>
  58:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  5c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  60:	fmov	d3, x0
  64:	fmov	d1, x1
  68:	bif	v2.8b, v3.8b, v1.8b
  6c:	fmul	d0, d2, d16
  70:	fmul	d1, d2, d17
  74:	ret
  78:	fdiv	d4, d3, d2
  7c:	fmadd	d1, d3, d4, d2
  80:	fmadd	d5, d4, d17, d0
  84:	fmsub	d4, d4, d0, d17
  88:	fdiv	d0, d5, d1
  8c:	fdiv	d1, d4, d1
  90:	b	30 <__divdc3+0x30>
  94:	fabs	d5, d16
  98:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  9c:	fmov	d4, x0
  a0:	fcmp	d5, d4
  a4:	b.gt	12c <__divdc3+0x12c>
  a8:	fabs	d18, d17
  ac:	fcmp	d18, d4
  b0:	b.gt	12c <__divdc3+0x12c>
  b4:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  b8:	fmov	d4, x0
  bc:	fcmp	d6, d4
  c0:	cset	w1, le
  c4:	fcmp	d7, d4
  c8:	cset	w0, le
  cc:	cmp	w1, #0x0
  d0:	ccmp	w0, #0x0, #0x4, ne  // ne = any
  d4:	b.ne	74 <__divdc3+0x74>  // b.any
  d8:	fcmp	d5, d4
  dc:	b.hi	74 <__divdc3+0x74>  // b.pmore
  e0:	fabs	d5, d17
  e4:	fcmp	d5, d4
  e8:	b.hi	74 <__divdc3+0x74>  // b.pmore
  ec:	eor	w0, w0, #0x1
  f0:	eor	w1, w1, #0x1
  f4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  f8:	fmov	d5, x2
  fc:	scvtf	d0, w0
 100:	scvtf	d1, w1
 104:	movi	d4, #0x0
 108:	bit	v0.8b, v3.8b, v5.8b
 10c:	bif	v2.8b, v1.8b, v5.8b
 110:	fmul	d1, d16, d0
 114:	fmul	d5, d17, d0
 118:	fmadd	d0, d16, d2, d5
 11c:	fnmsub	d1, d17, d2, d1
 120:	fmul	d0, d0, d4
 124:	fmul	d1, d1, d4
 128:	ret
 12c:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 130:	fmov	d4, x0
 134:	fcmp	d6, d4
 138:	b.hi	b4 <__divdc3+0xb4>  // b.pmore
 13c:	fcmp	d7, d4
 140:	b.hi	b4 <__divdc3+0xb4>  // b.pmore
 144:	fcmp	d5, d4
 148:	fabs	d0, d17
 14c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 150:	fmov	d6, x0
 154:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 158:	fmov	d5, x0
 15c:	cset	w0, gt
 160:	fcmp	d0, d4
 164:	scvtf	d0, w0
 168:	cset	w0, gt
 16c:	scvtf	d1, w0
 170:	bif	v16.8b, v0.8b, v6.8b
 174:	bit	v1.8b, v17.8b, v6.8b
 178:	fmul	d4, d3, d16
 17c:	fmul	d0, d3, d1
 180:	fnmsub	d3, d2, d1, d4
 184:	fmadd	d2, d2, d16, d0
 188:	fmul	d1, d3, d5
 18c:	fmul	d0, d2, d5
 190:	ret

_divxc3.o:     file format elf64-littleaarch64


_divtc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-176]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	str	q2, [sp, #96]
  18:	str	q3, [sp, #112]
  1c:	ldp	x19, x22, [sp, #96]
  20:	stp	x27, x28, [sp, #80]
  24:	ldp	x20, x23, [sp, #112]
  28:	stp	x25, x26, [sp, #64]
  2c:	and	x27, x22, #0x7fffffffffffffff
  30:	stp	x19, x27, [sp, #112]
  34:	str	q0, [sp, #128]
  38:	and	x28, x23, #0x7fffffffffffffff
  3c:	stp	x20, x28, [sp, #96]
  40:	ldr	q0, [sp, #112]
  44:	str	q1, [sp, #144]
  48:	ldp	x21, x25, [sp, #128]
  4c:	ldp	x24, x26, [sp, #144]
  50:	ldr	q1, [sp, #96]
  54:	bl	0 <__lttf2>
  58:	tbz	w0, #31, 370 <__divtc3+0x370>
  5c:	stp	x20, x23, [sp, #96]
  60:	stp	x19, x22, [sp, #112]
  64:	ldr	q1, [sp, #96]
  68:	ldr	q0, [sp, #112]
  6c:	bl	0 <__divtf3>
  70:	stp	x19, x22, [sp, #96]
  74:	mov	v1.16b, v0.16b
  78:	str	q0, [sp, #112]
  7c:	ldr	q0, [sp, #96]
  80:	bl	0 <__multf3>
  84:	stp	x20, x23, [sp, #96]
  88:	ldr	q1, [sp, #96]
  8c:	bl	0 <__addtf3>
  90:	stp	x21, x25, [sp, #96]
  94:	ldr	q2, [sp, #112]
  98:	str	q0, [sp, #112]
  9c:	ldr	q0, [sp, #96]
  a0:	mov	v1.16b, v2.16b
  a4:	str	q2, [sp, #144]
  a8:	bl	0 <__multf3>
  ac:	stp	x24, x26, [sp, #96]
  b0:	ldr	q1, [sp, #96]
  b4:	bl	0 <__addtf3>
  b8:	ldr	q4, [sp, #112]
  bc:	mov	v1.16b, v4.16b
  c0:	str	q4, [sp, #128]
  c4:	bl	0 <__divtf3>
  c8:	stp	x24, x26, [sp, #112]
  cc:	ldr	q2, [sp, #144]
  d0:	str	q0, [sp, #96]
  d4:	ldr	q0, [sp, #112]
  d8:	mov	v1.16b, v2.16b
  dc:	bl	0 <__multf3>
  e0:	stp	x21, x25, [sp, #112]
  e4:	ldr	q1, [sp, #112]
  e8:	bl	0 <__subtf3>
  ec:	ldr	q4, [sp, #128]
  f0:	mov	v1.16b, v4.16b
  f4:	bl	0 <__divtf3>
  f8:	str	q0, [sp, #112]
  fc:	ldr	q0, [sp, #96]
 100:	mov	v1.16b, v0.16b
 104:	bl	0 <__unordtf2>
 108:	cmp	w0, #0x0
 10c:	ldr	q0, [sp, #112]
 110:	cset	w1, ne  // ne = any
 114:	str	w1, [sp, #128]
 118:	mov	v1.16b, v0.16b
 11c:	bl	0 <__unordtf2>
 120:	cmp	w0, #0x0
 124:	ldr	w1, [sp, #128]
 128:	cset	w0, ne  // ne = any
 12c:	tst	w0, w1
 130:	b.eq	34c <__divtc3+0x34c>  // b.none
 134:	movi	v1.2d, #0x0
 138:	stp	x19, x22, [sp, #128]
 13c:	ldr	q0, [sp, #128]
 140:	bl	0 <__eqtf2>
 144:	stp	x20, x23, [sp, #128]
 148:	cmp	w0, #0x0
 14c:	movi	v1.2d, #0x0
 150:	cset	w1, eq  // eq = none
 154:	ldr	q0, [sp, #128]
 158:	str	w1, [sp, #144]
 15c:	bl	0 <__eqtf2>
 160:	cmp	w0, #0x0
 164:	ldr	w1, [sp, #144]
 168:	cset	w0, eq  // eq = none
 16c:	tst	w0, w1
 170:	b.ne	408 <__divtc3+0x408>  // b.any
 174:	adrp	x1, 0 <__divtc3>
 178:	add	x1, x1, #0x0
 17c:	and	x0, x25, #0x7fffffffffffffff
 180:	stp	x21, x0, [sp, #128]
 184:	ldr	q1, [x1]
 188:	ldr	q0, [sp, #128]
 18c:	str	x0, [sp, #144]
 190:	bl	0 <__unordtf2>
 194:	cbnz	w0, 4a0 <__divtc3+0x4a0>
 198:	adrp	x1, 0 <__divtc3>
 19c:	add	x1, x1, #0x0
 1a0:	ldr	x0, [sp, #144]
 1a4:	stp	x21, x0, [sp, #128]
 1a8:	ldr	q1, [x1]
 1ac:	ldr	q0, [sp, #128]
 1b0:	bl	0 <__letf2>
 1b4:	cmp	w0, #0x0
 1b8:	b.le	4a0 <__divtc3+0x4a0>
 1bc:	adrp	x0, 0 <__divtc3>
 1c0:	add	x0, x0, #0x0
 1c4:	stp	x19, x27, [sp, #128]
 1c8:	ldr	q1, [x0]
 1cc:	ldr	q0, [sp, #128]
 1d0:	bl	0 <__unordtf2>
 1d4:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 1d8:	adrp	x0, 0 <__divtc3>
 1dc:	add	x0, x0, #0x0
 1e0:	stp	x19, x27, [sp, #128]
 1e4:	ldr	q1, [x0]
 1e8:	ldr	q0, [sp, #128]
 1ec:	bl	0 <__gttf2>
 1f0:	cmp	w0, #0x0
 1f4:	b.gt	4e8 <__divtc3+0x4e8>
 1f8:	adrp	x0, 0 <__divtc3>
 1fc:	add	x0, x0, #0x0
 200:	stp	x20, x28, [sp, #128]
 204:	ldr	q1, [x0]
 208:	ldr	q0, [sp, #128]
 20c:	bl	0 <__unordtf2>
 210:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 214:	adrp	x0, 0 <__divtc3>
 218:	add	x0, x0, #0x0
 21c:	stp	x20, x28, [sp, #128]
 220:	ldr	q1, [x0]
 224:	ldr	q0, [sp, #128]
 228:	bl	0 <__gttf2>
 22c:	cmp	w0, #0x0
 230:	b.gt	4e8 <__divtc3+0x4e8>
 234:	adrp	x0, 0 <__divtc3>
 238:	add	x0, x0, #0x0
 23c:	ldr	x28, [sp, #144]
 240:	stp	x21, x28, [sp, #96]
 244:	ldr	q1, [x0]
 248:	ldr	q0, [sp, #96]
 24c:	mov	w27, #0x1                   	// #1
 250:	bl	0 <__unordtf2>
 254:	cbz	w0, 6ec <__divtc3+0x6ec>
 258:	eor	w0, w27, #0x1
 25c:	and	x27, x26, #0x7fffffffffffffff
 260:	and	w0, w0, #0x1
 264:	bl	0 <__floatsitf>
 268:	str	q0, [sp, #112]
 26c:	mov	w28, #0x1                   	// #1
 270:	ldp	x21, x0, [sp, #112]
 274:	stp	x24, x27, [sp, #96]
 278:	ldr	q0, [sp, #96]
 27c:	bfxil	x25, x0, #0, #63
 280:	adrp	x0, 0 <__divtc3>
 284:	add	x0, x0, #0x0
 288:	ldr	q1, [x0]
 28c:	bl	0 <__unordtf2>
 290:	cbz	w0, 6c8 <__divtc3+0x6c8>
 294:	eor	w0, w28, #0x1
 298:	and	w0, w0, #0x1
 29c:	bl	0 <__floatsitf>
 2a0:	str	q0, [sp, #128]
 2a4:	ldp	x24, x0, [sp, #128]
 2a8:	stp	x21, x25, [sp, #96]
 2ac:	stp	x19, x22, [sp, #112]
 2b0:	ldr	q1, [sp, #96]
 2b4:	ldr	q0, [sp, #112]
 2b8:	bfxil	x26, x0, #0, #63
 2bc:	bl	0 <__multf3>
 2c0:	stp	x24, x26, [sp, #96]
 2c4:	stp	x20, x23, [sp, #112]
 2c8:	ldr	q1, [sp, #96]
 2cc:	str	q0, [sp, #96]
 2d0:	ldr	q0, [sp, #112]
 2d4:	bl	0 <__multf3>
 2d8:	mov	v1.16b, v0.16b
 2dc:	ldr	q2, [sp, #96]
 2e0:	mov	v0.16b, v2.16b
 2e4:	bl	0 <__addtf3>
 2e8:	adrp	x0, 0 <__divtc3>
 2ec:	add	x0, x0, #0x0
 2f0:	ldr	q1, [x0]
 2f4:	bl	0 <__multf3>
 2f8:	stp	x24, x26, [sp, #96]
 2fc:	stp	x19, x22, [sp, #112]
 300:	ldr	q1, [sp, #96]
 304:	str	q0, [sp, #96]
 308:	ldr	q0, [sp, #112]
 30c:	bl	0 <__multf3>
 310:	stp	x21, x25, [sp, #112]
 314:	stp	x20, x23, [sp, #128]
 318:	ldr	q1, [sp, #112]
 31c:	str	q0, [sp, #112]
 320:	ldr	q0, [sp, #128]
 324:	bl	0 <__multf3>
 328:	mov	v1.16b, v0.16b
 32c:	ldr	q2, [sp, #112]
 330:	mov	v0.16b, v2.16b
 334:	bl	0 <__subtf3>
 338:	adrp	x0, 0 <__divtc3>
 33c:	add	x0, x0, #0x0
 340:	ldr	q1, [x0]
 344:	bl	0 <__multf3>
 348:	str	q0, [sp, #112]
 34c:	ldp	x19, x20, [sp, #16]
 350:	ldp	x21, x22, [sp, #32]
 354:	ldp	x23, x24, [sp, #48]
 358:	ldp	x25, x26, [sp, #64]
 35c:	ldp	x27, x28, [sp, #80]
 360:	ldr	q0, [sp, #96]
 364:	ldr	q1, [sp, #112]
 368:	ldp	x29, x30, [sp], #176
 36c:	ret
 370:	stp	x19, x22, [sp, #96]
 374:	stp	x20, x23, [sp, #112]
 378:	ldr	q1, [sp, #96]
 37c:	ldr	q0, [sp, #112]
 380:	bl	0 <__divtf3>
 384:	stp	x20, x23, [sp, #96]
 388:	mov	v1.16b, v0.16b
 38c:	str	q0, [sp, #128]
 390:	ldr	q0, [sp, #96]
 394:	bl	0 <__multf3>
 398:	stp	x19, x22, [sp, #96]
 39c:	ldr	q1, [sp, #96]
 3a0:	bl	0 <__addtf3>
 3a4:	stp	x24, x26, [sp, #96]
 3a8:	ldr	q2, [sp, #128]
 3ac:	ldr	q1, [sp, #96]
 3b0:	str	q0, [sp, #112]
 3b4:	mov	v0.16b, v2.16b
 3b8:	str	q2, [sp, #144]
 3bc:	bl	0 <__multf3>
 3c0:	stp	x21, x25, [sp, #96]
 3c4:	ldr	q1, [sp, #96]
 3c8:	bl	0 <__addtf3>
 3cc:	ldr	q4, [sp, #112]
 3d0:	mov	v1.16b, v4.16b
 3d4:	str	q4, [sp, #128]
 3d8:	bl	0 <__divtf3>
 3dc:	stp	x21, x25, [sp, #112]
 3e0:	mov	v6.16b, v0.16b
 3e4:	ldr	q1, [sp, #112]
 3e8:	ldr	q2, [sp, #144]
 3ec:	str	q6, [sp, #96]
 3f0:	mov	v0.16b, v2.16b
 3f4:	bl	0 <__multf3>
 3f8:	stp	x24, x26, [sp, #112]
 3fc:	mov	v1.16b, v0.16b
 400:	ldr	q0, [sp, #112]
 404:	b	e8 <__divtc3+0xe8>
 408:	stp	x21, x25, [sp, #128]
 40c:	stp	x21, x25, [sp, #144]
 410:	ldr	q1, [sp, #128]
 414:	ldr	q0, [sp, #144]
 418:	bl	0 <__unordtf2>
 41c:	stp	x24, x26, [sp, #128]
 420:	cmp	w0, #0x0
 424:	stp	x24, x26, [sp, #144]
 428:	cset	w1, eq  // eq = none
 42c:	ldr	q1, [sp, #128]
 430:	ldr	q0, [sp, #144]
 434:	str	w1, [sp, #168]
 438:	bl	0 <__unordtf2>
 43c:	cmp	w0, #0x0
 440:	ldr	w1, [sp, #168]
 444:	cset	w0, eq  // eq = none
 448:	orr	w1, w1, w0
 44c:	tbz	w1, #0, 174 <__divtc3+0x174>
 450:	adrp	x0, 0 <__divtc3>
 454:	add	x0, x0, #0x0
 458:	ldr	q2, [x0]
 45c:	tbz	x22, #63, 46c <__divtc3+0x46c>
 460:	adrp	x0, 0 <__divtc3>
 464:	add	x0, x0, #0x0
 468:	ldr	q2, [x0]
 46c:	stp	x21, x25, [sp, #96]
 470:	mov	v0.16b, v2.16b
 474:	ldr	q1, [sp, #96]
 478:	str	q2, [sp, #128]
 47c:	bl	0 <__multf3>
 480:	stp	x24, x26, [sp, #112]
 484:	ldr	q2, [sp, #128]
 488:	ldr	q1, [sp, #112]
 48c:	str	q0, [sp, #96]
 490:	mov	v0.16b, v2.16b
 494:	bl	0 <__multf3>
 498:	str	q0, [sp, #112]
 49c:	b	34c <__divtc3+0x34c>
 4a0:	adrp	x0, 0 <__divtc3>
 4a4:	add	x0, x0, #0x0
 4a8:	and	x1, x26, #0x7fffffffffffffff
 4ac:	stp	x24, x1, [sp, #128]
 4b0:	ldr	q1, [x0]
 4b4:	ldr	q0, [sp, #128]
 4b8:	str	x1, [sp, #168]
 4bc:	bl	0 <__unordtf2>
 4c0:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 4c4:	adrp	x0, 0 <__divtc3>
 4c8:	add	x0, x0, #0x0
 4cc:	ldr	x1, [sp, #168]
 4d0:	stp	x24, x1, [sp, #128]
 4d4:	ldr	q1, [x0]
 4d8:	ldr	q0, [sp, #128]
 4dc:	bl	0 <__letf2>
 4e0:	cmp	w0, #0x0
 4e4:	b.gt	1bc <__divtc3+0x1bc>
 4e8:	adrp	x0, 0 <__divtc3>
 4ec:	add	x0, x0, #0x0
 4f0:	stp	x19, x27, [sp, #128]
 4f4:	mov	w1, #0x1                   	// #1
 4f8:	ldr	q1, [x0]
 4fc:	ldr	q0, [sp, #128]
 500:	str	w1, [sp, #168]
 504:	bl	0 <__unordtf2>
 508:	ldr	w1, [sp, #168]
 50c:	cbz	w0, 6a4 <__divtc3+0x6a4>
 510:	adrp	x0, 0 <__divtc3>
 514:	add	x0, x0, #0x0
 518:	stp	x20, x28, [sp, #128]
 51c:	and	w19, w1, #0xff
 520:	mov	w27, #0x1                   	// #1
 524:	ldr	q1, [x0]
 528:	ldr	q0, [sp, #128]
 52c:	bl	0 <__unordtf2>
 530:	cbz	w0, 680 <__divtc3+0x680>
 534:	and	w27, w27, #0xff
 538:	cmp	w19, #0x0
 53c:	ccmp	w27, #0x0, #0x4, ne  // ne = any
 540:	b.ne	34c <__divtc3+0x34c>  // b.any
 544:	adrp	x0, 0 <__divtc3>
 548:	add	x0, x0, #0x0
 54c:	ldr	x20, [sp, #144]
 550:	stp	x21, x20, [sp, #128]
 554:	ldr	q1, [x0]
 558:	ldr	q0, [sp, #128]
 55c:	bl	0 <__unordtf2>
 560:	cbnz	w0, 34c <__divtc3+0x34c>
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	stp	x21, x20, [sp, #128]
 570:	ldr	q1, [x0]
 574:	ldr	q0, [sp, #128]
 578:	bl	0 <__gttf2>
 57c:	cmp	w0, #0x0
 580:	b.gt	34c <__divtc3+0x34c>
 584:	adrp	x0, 0 <__divtc3>
 588:	add	x0, x0, #0x0
 58c:	and	x20, x26, #0x7fffffffffffffff
 590:	stp	x24, x20, [sp, #128]
 594:	ldr	q1, [x0]
 598:	ldr	q0, [sp, #128]
 59c:	bl	0 <__unordtf2>
 5a0:	cbnz	w0, 34c <__divtc3+0x34c>
 5a4:	adrp	x0, 0 <__divtc3>
 5a8:	add	x0, x0, #0x0
 5ac:	stp	x24, x20, [sp, #128]
 5b0:	ldr	q1, [x0]
 5b4:	ldr	q0, [sp, #128]
 5b8:	bl	0 <__gttf2>
 5bc:	cmp	w0, #0x0
 5c0:	b.gt	34c <__divtc3+0x34c>
 5c4:	eor	w0, w19, #0x1
 5c8:	bl	0 <__floatsitf>
 5cc:	str	q0, [sp, #96]
 5d0:	eor	w0, w27, #0x1
 5d4:	ldp	x20, x1, [sp, #96]
 5d8:	bfxil	x22, x1, #0, #63
 5dc:	bl	0 <__floatsitf>
 5e0:	str	q0, [sp, #128]
 5e4:	ldp	x19, x0, [sp, #128]
 5e8:	stp	x20, x22, [sp, #96]
 5ec:	stp	x21, x25, [sp, #112]
 5f0:	ldr	q1, [sp, #96]
 5f4:	ldr	q0, [sp, #112]
 5f8:	bfxil	x23, x0, #0, #63
 5fc:	bl	0 <__multf3>
 600:	stp	x19, x23, [sp, #96]
 604:	stp	x24, x26, [sp, #112]
 608:	ldr	q1, [sp, #96]
 60c:	str	q0, [sp, #96]
 610:	ldr	q0, [sp, #112]
 614:	bl	0 <__multf3>
 618:	mov	v1.16b, v0.16b
 61c:	ldr	q2, [sp, #96]
 620:	mov	v0.16b, v2.16b
 624:	bl	0 <__addtf3>
 628:	movi	v1.2d, #0x0
 62c:	bl	0 <__multf3>
 630:	stp	x20, x22, [sp, #96]
 634:	stp	x24, x26, [sp, #112]
 638:	ldr	q1, [sp, #96]
 63c:	str	q0, [sp, #96]
 640:	ldr	q0, [sp, #112]
 644:	bl	0 <__multf3>
 648:	stp	x19, x23, [sp, #112]
 64c:	stp	x21, x25, [sp, #128]
 650:	ldr	q1, [sp, #112]
 654:	str	q0, [sp, #112]
 658:	ldr	q0, [sp, #128]
 65c:	bl	0 <__multf3>
 660:	mov	v1.16b, v0.16b
 664:	ldr	q2, [sp, #112]
 668:	mov	v0.16b, v2.16b
 66c:	bl	0 <__subtf3>
 670:	movi	v1.2d, #0x0
 674:	bl	0 <__multf3>
 678:	str	q0, [sp, #112]
 67c:	b	34c <__divtc3+0x34c>
 680:	adrp	x0, 0 <__divtc3>
 684:	add	x0, x0, #0x0
 688:	stp	x20, x28, [sp, #128]
 68c:	ldr	q1, [x0]
 690:	ldr	q0, [sp, #128]
 694:	bl	0 <__letf2>
 698:	cmp	w0, #0x0
 69c:	cset	w27, le
 6a0:	b	534 <__divtc3+0x534>
 6a4:	adrp	x0, 0 <__divtc3>
 6a8:	add	x0, x0, #0x0
 6ac:	stp	x19, x27, [sp, #128]
 6b0:	ldr	q1, [x0]
 6b4:	ldr	q0, [sp, #128]
 6b8:	bl	0 <__letf2>
 6bc:	cmp	w0, #0x0
 6c0:	cset	w1, le
 6c4:	b	510 <__divtc3+0x510>
 6c8:	adrp	x0, 0 <__divtc3>
 6cc:	add	x0, x0, #0x0
 6d0:	stp	x24, x27, [sp, #96]
 6d4:	ldr	q1, [x0]
 6d8:	ldr	q0, [sp, #96]
 6dc:	bl	0 <__letf2>
 6e0:	cmp	w0, #0x0
 6e4:	cset	w28, le
 6e8:	b	294 <__divtc3+0x294>
 6ec:	adrp	x0, 0 <__divtc3>
 6f0:	add	x0, x0, #0x0
 6f4:	stp	x21, x28, [sp, #96]
 6f8:	ldr	q1, [x0]
 6fc:	ldr	q0, [sp, #96]
 700:	bl	0 <__letf2>
 704:	cmp	w0, #0x0
 708:	cset	w27, le
 70c:	b	258 <__divtc3+0x258>

_bswapsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

_bswapdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

_clrsbsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbdi2>:
   0:	eor	x1, x0, x0, asr #63
   4:	mov	w2, #0x3f                  	// #63
   8:	clz	x0, x1
   c:	cmp	x1, #0x0
  10:	sub	w0, w0, #0x1
  14:	csel	w0, w0, w2, ne  // ne = any
  18:	ret

_clrsbdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbti2>:
   0:	cbnz	x1, 14 <__clrsbti2+0x14>
   4:	mov	x1, x0
   8:	mov	w0, #0x7f                  	// #127
   c:	cbnz	x1, 40 <__clrsbti2+0x40>
  10:	ret
  14:	cmn	x1, #0x1
  18:	b.eq	34 <__clrsbti2+0x34>  // b.none
  1c:	tbnz	x1, #63, 48 <__clrsbti2+0x48>
  20:	mov	w0, #0x0                   	// #0
  24:	clz	x1, x1
  28:	sub	w0, w0, #0x1
  2c:	add	w0, w0, w1
  30:	ret
  34:	mvn	x1, x0
  38:	mov	w0, #0x7f                  	// #127
  3c:	cbz	x1, 10 <__clrsbti2+0x10>
  40:	mov	w0, #0x40                  	// #64
  44:	b	24 <__clrsbti2+0x24>
  48:	mvn	x1, x1
  4c:	b	20 <__clrsbti2+0x20>

_fixunssfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	movi	v1.2s, #0x5f, lsl #24
   4:	fcmpe	s0, s1
   8:	b.ge	14 <__fixunssfdi+0x14>  // b.tcont
   c:	fcvtzs	x0, s0
  10:	ret
  14:	fsub	s0, s0, s1
  18:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  1c:	fcvtzs	x0, s0
  20:	add	x0, x0, x1
  24:	ret

_fixunsdfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	mov	x0, #0x43e0000000000000    	// #4890909195324358656
   4:	fmov	d1, x0
   8:	fcmpe	d0, d1
   c:	b.ge	18 <__fixunsdfdi+0x18>  // b.tcont
  10:	fcvtzs	x0, d0
  14:	ret
  18:	fsub	d0, d0, d1
  1c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  20:	fcvtzs	x0, d0
  24:	add	x0, x0, x1
  28:	ret

_fixunsxfsi.o:     file format elf64-littleaarch64


_fixsfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fcmpe	s0, #0.0
   4:	b.mi	c <__fixsfti+0xc>  // b.first
   8:	b	0 <__fixunssfti>
   c:	fneg	s0, s0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunssfti>
  1c:	negs	x0, x0
  20:	ngc	x1, x1
  24:	ldp	x29, x30, [sp], #16
  28:	ret

_fixdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fcmpe	d0, #0.0
   4:	b.mi	c <__fixdfti+0xc>  // b.first
   8:	b	0 <__fixunsdfti>
   c:	fneg	d0, d0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunsdfti>
  1c:	negs	x0, x0
  20:	ngc	x1, x1
  24:	ldp	x29, x30, [sp], #16
  28:	ret

_fixxfdi.o:     file format elf64-littleaarch64


_fixunssfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fcvt	d0, s0
   4:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   8:	fmov	d1, x0
   c:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
  10:	fmov	d2, x0
  14:	fmul	d1, d0, d1
  18:	fcvtzu	x1, d1
  1c:	ucvtf	d1, x1
  20:	fmsub	d0, d1, d2, d0
  24:	fcvtzu	x0, d0
  28:	ret

_fixunsdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   4:	fmov	d1, x0
   8:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
   c:	fmov	d2, x0
  10:	fmul	d1, d0, d1
  14:	fcvtzu	x1, d1
  18:	ucvtf	d1, x1
  1c:	fmsub	d0, d1, d2, d0
  20:	fcvtzu	x0, d0
  24:	ret

_fixunsxfdi.o:     file format elf64-littleaarch64


_floatdisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x2, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x3, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x2
  28:	b.hi	70 <__floattisf+0x70>  // b.pmore
  2c:	b.eq	68 <__floattisf+0x68>  // b.none
  30:	bl	0 <__floatditf>
  34:	adrp	x0, 0 <__floattisf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfsf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	cmn	x3, #0x2
  6c:	b.ls	30 <__floattisf+0x30>  // b.plast
  70:	and	x1, x19, #0xffffffffffff8000
  74:	tst	x19, #0x7fff
  78:	orr	x1, x1, #0x8000
  7c:	csel	x19, x1, x19, ne  // ne = any
  80:	b	30 <__floattisf+0x30>

_floatdidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x2, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x3, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x2
  28:	b.hi	70 <__floattidf+0x70>  // b.pmore
  2c:	b.eq	68 <__floattidf+0x68>  // b.none
  30:	bl	0 <__floatditf>
  34:	adrp	x0, 0 <__floattidf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfdf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	cmn	x3, #0x2
  6c:	b.ls	30 <__floattidf+0x30>  // b.plast
  70:	and	x1, x19, #0xffffffffffff8000
  74:	tst	x19, #0x7fff
  78:	orr	x1, x1, #0x8000
  7c:	csel	x19, x1, x19, ne  // ne = any
  80:	b	30 <__floattidf+0x30>

_floatdixf.o:     file format elf64-littleaarch64


_floatundisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntisf+0x30>  // b.plast
  20:	and	x1, x19, #0xffffffffffff8000
  24:	tst	x19, #0x7fff
  28:	orr	x1, x1, #0x8000
  2c:	csel	x19, x1, x19, ne  // ne = any
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntisf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfsf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntidf+0x30>  // b.plast
  20:	and	x1, x19, #0xffffffffffff8000
  24:	tst	x19, #0x7fff
  28:	orr	x1, x1, #0x8000
  2c:	csel	x19, x1, x19, ne  // ne = any
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntidf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfdf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundixf.o:     file format elf64-littleaarch64


_eprintf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x4, x1
   8:	mov	x1, x0
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	adrp	x19, 0 <stderr>
  18:	mov	w5, w2
  1c:	mov	x2, x4
  20:	ldr	x19, [x19]
  24:	mov	x4, x3
  28:	mov	w3, w5
  2c:	ldr	x0, [x19]
  30:	bl	0 <fprintf>
  34:	ldr	x0, [x19]
  38:	bl	0 <fflush>
  3c:	bl	0 <abort>

__gcc_bcmp.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_bcmp>:
   0:	cbz	x2, 30 <__gcc_bcmp+0x30>
   4:	mov	x3, #0x0                   	// #0
   8:	b	14 <__gcc_bcmp+0x14>
   c:	cmp	x3, x2
  10:	b.eq	30 <__gcc_bcmp+0x30>  // b.none
  14:	ldrb	w4, [x0, x3]
  18:	ldrb	w5, [x1, x3]
  1c:	add	x3, x3, #0x1
  20:	cmp	w4, w5
  24:	b.eq	c <__gcc_bcmp+0xc>  // b.none
  28:	sub	w0, w4, w5
  2c:	ret
  30:	mov	w0, #0x0                   	// #0
  34:	ret

_divdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	mov	x4, #0xffffffffffffffff    	// #-1
   4:	mov	x6, #0x0                   	// #0
   8:	tbnz	x1, #63, e8 <__divti3+0xe8>
   c:	tbz	x3, #63, 1c <__divti3+0x1c>
  10:	negs	x2, x2
  14:	mov	x6, x4
  18:	ngc	x3, x3
  1c:	mov	x4, x2
  20:	mov	x7, x0
  24:	mov	x5, x1
  28:	cbnz	x3, d4 <__divti3+0xd4>
  2c:	cmp	x2, x1
  30:	b.ls	1fc <__divti3+0x1fc>  // b.plast
  34:	clz	x0, x2
  38:	cbz	x0, 54 <__divti3+0x54>
  3c:	neg	w1, w0
  40:	lsl	x3, x5, x0
  44:	lsl	x4, x2, x0
  48:	lsr	x5, x7, x1
  4c:	orr	x5, x5, x3
  50:	lsl	x7, x7, x0
  54:	lsr	x2, x4, #32
  58:	and	x8, x4, #0xffffffff
  5c:	udiv	x3, x5, x2
  60:	mov	x0, x3
  64:	msub	x3, x3, x2, x5
  68:	mul	x5, x8, x0
  6c:	extr	x1, x3, x7, #32
  70:	cmp	x5, x1
  74:	b.ls	88 <__divti3+0x88>  // b.plast
  78:	adds	x1, x4, x1
  7c:	ccmp	x5, x1, #0x0, cc  // cc = lo, ul, last
  80:	b.hi	370 <__divti3+0x370>  // b.pmore
  84:	sub	x0, x0, #0x1
  88:	sub	x1, x1, x5
  8c:	udiv	x5, x1, x2
  90:	msub	x1, x5, x2, x1
  94:	mov	x2, x7
  98:	mul	x8, x8, x5
  9c:	mov	x3, x5
  a0:	bfi	x2, x1, #32, #32
  a4:	cmp	x8, x2
  a8:	b.ls	bc <__divti3+0xbc>  // b.plast
  ac:	adds	x2, x4, x2
  b0:	ccmp	x8, x2, #0x0, cc  // cc = lo, ul, last
  b4:	cinc	x3, x5, ls  // ls = plast
  b8:	sub	x3, x3, #0x2
  bc:	orr	x0, x3, x0, lsl #32
  c0:	mov	x1, #0x0                   	// #0
  c4:	cbz	x6, d0 <__divti3+0xd0>
  c8:	negs	x0, x0
  cc:	ngc	x1, x1
  d0:	ret
  d4:	cmp	x3, x1
  d8:	b.ls	fc <__divti3+0xfc>  // b.plast
  dc:	mov	x1, #0x0                   	// #0
  e0:	mov	x0, #0x0                   	// #0
  e4:	b	c4 <__divti3+0xc4>
  e8:	negs	x0, x0
  ec:	mov	x4, #0x0                   	// #0
  f0:	ngc	x1, x1
  f4:	mov	x6, #0xffffffffffffffff    	// #-1
  f8:	b	c <__divti3+0xc>
  fc:	clz	x8, x3
 100:	cbz	x8, 318 <__divti3+0x318>
 104:	mov	x4, #0x40                  	// #64
 108:	sub	x4, x4, x8
 10c:	lsl	x3, x3, x8
 110:	lsr	x5, x2, x4
 114:	orr	x3, x5, x3
 118:	lsr	x7, x1, x4
 11c:	and	x9, x3, #0xffffffff
 120:	lsr	x11, x3, #32
 124:	lsl	x1, x1, x8
 128:	lsr	x4, x0, x4
 12c:	orr	x4, x4, x1
 130:	lsl	x2, x2, x8
 134:	udiv	x5, x7, x11
 138:	mov	x1, x5
 13c:	msub	x5, x5, x11, x7
 140:	mul	x7, x9, x1
 144:	extr	x5, x5, x4, #32
 148:	cmp	x7, x5
 14c:	b.ls	160 <__divti3+0x160>  // b.plast
 150:	adds	x5, x3, x5
 154:	ccmp	x7, x5, #0x0, cc  // cc = lo, ul, last
 158:	b.hi	358 <__divti3+0x358>  // b.pmore
 15c:	sub	x1, x1, #0x1
 160:	sub	x7, x5, x7
 164:	udiv	x10, x7, x11
 168:	msub	x7, x10, x11, x7
 16c:	mov	x5, x10
 170:	mul	x9, x9, x10
 174:	bfi	x4, x7, #32, #32
 178:	cmp	x9, x4
 17c:	b.ls	190 <__divti3+0x190>  // b.plast
 180:	adds	x4, x3, x4
 184:	ccmp	x9, x4, #0x0, cc  // cc = lo, ul, last
 188:	b.hi	340 <__divti3+0x340>  // b.pmore
 18c:	sub	x5, x10, #0x1
 190:	orr	x1, x5, x1, lsl #32
 194:	and	x10, x2, #0xffffffff
 198:	mov	w5, w5
 19c:	lsr	x2, x2, #32
 1a0:	lsr	x7, x1, #32
 1a4:	sub	x4, x4, x9
 1a8:	mov	x11, #0x100000000           	// #4294967296
 1ac:	mul	x12, x5, x10
 1b0:	mul	x9, x7, x10
 1b4:	madd	x5, x5, x2, x9
 1b8:	mul	x2, x7, x2
 1bc:	add	x3, x5, x12, lsr #32
 1c0:	add	x5, x2, x11
 1c4:	cmp	x9, x3
 1c8:	csel	x2, x5, x2, hi  // hi = pmore
 1cc:	add	x2, x2, x3, lsr #32
 1d0:	cmp	x4, x2
 1d4:	b.cc	1f0 <__divti3+0x1f0>  // b.lo, b.ul, b.last
 1d8:	and	x12, x12, #0xffffffff
 1dc:	lsl	x0, x0, x8
 1e0:	add	x3, x12, x3, lsl #32
 1e4:	cmp	x0, x3
 1e8:	ccmp	x4, x2, #0x0, cc  // cc = lo, ul, last
 1ec:	b.ne	328 <__divti3+0x328>  // b.any
 1f0:	sub	x0, x1, #0x1
 1f4:	mov	x1, #0x0                   	// #0
 1f8:	b	c4 <__divti3+0xc4>
 1fc:	cbnz	x2, 208 <__divti3+0x208>
 200:	mov	x2, #0x1                   	// #1
 204:	udiv	x4, x2, x3
 208:	clz	x8, x4
 20c:	cbnz	x8, 288 <__divti3+0x288>
 210:	lsr	x11, x4, #32
 214:	and	x10, x4, #0xffffffff
 218:	sub	x3, x1, x4
 21c:	mov	x1, #0x1                   	// #1
 220:	udiv	x5, x3, x11
 224:	mov	x0, x5
 228:	msub	x5, x5, x11, x3
 22c:	mul	x2, x0, x10
 230:	extr	x3, x5, x7, #32
 234:	cmp	x2, x3
 238:	b.ls	24c <__divti3+0x24c>  // b.plast
 23c:	adds	x3, x4, x3
 240:	ccmp	x2, x3, #0x0, cc  // cc = lo, ul, last
 244:	b.hi	364 <__divti3+0x364>  // b.pmore
 248:	sub	x0, x0, #0x1
 24c:	sub	x3, x3, x2
 250:	mov	x2, x7
 254:	udiv	x9, x3, x11
 258:	msub	x3, x9, x11, x3
 25c:	mov	x8, x9
 260:	mul	x5, x9, x10
 264:	bfi	x2, x3, #32, #32
 268:	cmp	x5, x2
 26c:	b.ls	280 <__divti3+0x280>  // b.plast
 270:	adds	x2, x4, x2
 274:	ccmp	x5, x2, #0x0, cc  // cc = lo, ul, last
 278:	cinc	x8, x9, ls  // ls = plast
 27c:	sub	x8, x8, #0x2
 280:	orr	x0, x8, x0, lsl #32
 284:	b	c4 <__divti3+0xc4>
 288:	lsl	x4, x4, x8
 28c:	mov	x2, #0x40                  	// #64
 290:	sub	x2, x2, x8
 294:	lsr	x11, x4, #32
 298:	lsl	x3, x1, x8
 29c:	and	x10, x4, #0xffffffff
 2a0:	lsr	x1, x1, x2
 2a4:	lsr	x5, x0, x2
 2a8:	orr	x5, x5, x3
 2ac:	udiv	x2, x1, x11
 2b0:	lsl	x7, x0, x8
 2b4:	msub	x1, x2, x11, x1
 2b8:	mov	x0, x2
 2bc:	mul	x2, x10, x2
 2c0:	extr	x1, x1, x5, #32
 2c4:	cmp	x2, x1
 2c8:	b.ls	2dc <__divti3+0x2dc>  // b.plast
 2cc:	adds	x1, x4, x1
 2d0:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 2d4:	b.hi	334 <__divti3+0x334>  // b.pmore
 2d8:	sub	x0, x0, #0x1
 2dc:	sub	x1, x1, x2
 2e0:	udiv	x2, x1, x11
 2e4:	msub	x1, x2, x11, x1
 2e8:	mov	x8, x2
 2ec:	mul	x3, x10, x2
 2f0:	bfi	x5, x1, #32, #32
 2f4:	cmp	x3, x5
 2f8:	b.ls	30c <__divti3+0x30c>  // b.plast
 2fc:	adds	x5, x4, x5
 300:	ccmp	x3, x5, #0x0, cc  // cc = lo, ul, last
 304:	b.hi	34c <__divti3+0x34c>  // b.pmore
 308:	sub	x8, x2, #0x1
 30c:	sub	x3, x5, x3
 310:	orr	x1, x8, x0, lsl #32
 314:	b	220 <__divti3+0x220>
 318:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 31c:	mov	x1, #0x0                   	// #0
 320:	cset	x0, ls  // ls = plast
 324:	b	c4 <__divti3+0xc4>
 328:	mov	x0, x1
 32c:	mov	x1, #0x0                   	// #0
 330:	b	c4 <__divti3+0xc4>
 334:	sub	x0, x0, #0x2
 338:	add	x1, x1, x4
 33c:	b	2dc <__divti3+0x2dc>
 340:	sub	x5, x10, #0x2
 344:	add	x4, x4, x3
 348:	b	190 <__divti3+0x190>
 34c:	sub	x8, x2, #0x2
 350:	add	x5, x5, x4
 354:	b	30c <__divti3+0x30c>
 358:	sub	x1, x1, #0x2
 35c:	add	x5, x5, x3
 360:	b	160 <__divti3+0x160>
 364:	sub	x0, x0, #0x2
 368:	add	x3, x3, x4
 36c:	b	24c <__divti3+0x24c>
 370:	sub	x0, x0, #0x2
 374:	add	x1, x1, x4
 378:	b	88 <__divti3+0x88>

_moddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	mov	x6, #0x0                   	// #0
   4:	tbnz	x1, #63, e8 <__modti3+0xe8>
   8:	tbz	x3, #63, 14 <__modti3+0x14>
   c:	negs	x2, x2
  10:	ngc	x3, x3
  14:	mov	x4, x2
  18:	mov	x10, x0
  1c:	mov	x5, x1
  20:	cbnz	x3, bc <__modti3+0xbc>
  24:	cmp	x2, x1
  28:	b.ls	f8 <__modti3+0xf8>  // b.plast
  2c:	clz	x0, x2
  30:	mov	x7, x0
  34:	cbz	x0, 50 <__modti3+0x50>
  38:	neg	w1, w0
  3c:	lsl	x5, x5, x0
  40:	lsl	x4, x2, x0
  44:	lsr	x1, x10, x1
  48:	orr	x5, x1, x5
  4c:	lsl	x10, x10, x0
  50:	lsr	x2, x4, #32
  54:	and	x0, x4, #0xffffffff
  58:	udiv	x1, x5, x2
  5c:	msub	x5, x1, x2, x5
  60:	mul	x1, x0, x1
  64:	extr	x5, x5, x10, #32
  68:	cmp	x1, x5
  6c:	b.ls	80 <__modti3+0x80>  // b.plast
  70:	adds	x5, x4, x5
  74:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
  78:	add	x3, x5, x4
  7c:	csel	x5, x3, x5, hi  // hi = pmore
  80:	sub	x5, x5, x1
  84:	udiv	x1, x5, x2
  88:	msub	x5, x1, x2, x5
  8c:	mul	x0, x0, x1
  90:	mov	x1, x10
  94:	bfi	x1, x5, #32, #32
  98:	cmp	x0, x1
  9c:	b.hi	200 <__modti3+0x200>  // b.pmore
  a0:	sub	x0, x1, x0
  a4:	lsr	x0, x0, x7
  a8:	mov	x1, #0x0                   	// #0
  ac:	cbz	x6, b8 <__modti3+0xb8>
  b0:	negs	x0, x0
  b4:	ngc	x1, x1
  b8:	ret
  bc:	cmp	x3, x1
  c0:	mov	x4, x0
  c4:	b.hi	ac <__modti3+0xac>  // b.pmore
  c8:	clz	x8, x3
  cc:	cbnz	x8, 214 <__modti3+0x214>
  d0:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
  d4:	b.hi	e0 <__modti3+0xe0>  // b.pmore
  d8:	subs	x4, x0, x2
  dc:	sbc	x1, x1, x3
  e0:	mov	x0, x4
  e4:	b	ac <__modti3+0xac>
  e8:	negs	x0, x0
  ec:	mov	x6, #0xffffffffffffffff    	// #-1
  f0:	ngc	x1, x1
  f4:	b	8 <__modti3+0x8>
  f8:	cbnz	x2, 104 <__modti3+0x104>
  fc:	mov	x4, #0x1                   	// #1
 100:	udiv	x4, x4, x3
 104:	clz	x11, x4
 108:	mov	x7, x11
 10c:	cbnz	x11, 17c <__modti3+0x17c>
 110:	sub	x5, x1, x4
 114:	lsr	x12, x4, #32
 118:	and	x13, x4, #0xffffffff
 11c:	udiv	x8, x5, x12
 120:	msub	x5, x8, x12, x5
 124:	mul	x8, x8, x13
 128:	extr	x3, x5, x10, #32
 12c:	cmp	x8, x3
 130:	b.ls	144 <__modti3+0x144>  // b.plast
 134:	adds	x5, x4, x3
 138:	ccmp	x8, x5, #0x0, cc  // cc = lo, ul, last
 13c:	add	x3, x5, x4
 140:	csel	x3, x3, x5, hi  // hi = pmore
 144:	sub	x3, x3, x8
 148:	mov	x1, x10
 14c:	udiv	x2, x3, x12
 150:	msub	x3, x2, x12, x3
 154:	mul	x2, x2, x13
 158:	bfi	x1, x3, #32, #32
 15c:	cmp	x2, x1
 160:	b.ls	174 <__modti3+0x174>  // b.plast
 164:	adds	x1, x4, x1
 168:	add	x4, x1, x4
 16c:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 170:	csel	x1, x4, x1, hi  // hi = pmore
 174:	sub	x0, x1, x2
 178:	b	a4 <__modti3+0xa4>
 17c:	lsl	x4, x4, x11
 180:	mov	x5, #0x40                  	// #64
 184:	sub	x5, x5, x11
 188:	lsr	x12, x4, #32
 18c:	and	x13, x4, #0xffffffff
 190:	lsl	x2, x1, x11
 194:	lsr	x9, x1, x5
 198:	lsr	x5, x0, x5
 19c:	orr	x5, x5, x2
 1a0:	udiv	x8, x9, x12
 1a4:	lsl	x10, x0, x11
 1a8:	msub	x9, x8, x12, x9
 1ac:	mul	x0, x13, x8
 1b0:	extr	x8, x9, x5, #32
 1b4:	cmp	x0, x8
 1b8:	b.ls	1cc <__modti3+0x1cc>  // b.plast
 1bc:	adds	x8, x4, x8
 1c0:	ccmp	x0, x8, #0x0, cc  // cc = lo, ul, last
 1c4:	add	x1, x8, x4
 1c8:	csel	x8, x1, x8, hi  // hi = pmore
 1cc:	sub	x8, x8, x0
 1d0:	udiv	x1, x8, x12
 1d4:	msub	x8, x1, x12, x8
 1d8:	mul	x1, x13, x1
 1dc:	bfi	x5, x8, #32, #32
 1e0:	cmp	x1, x5
 1e4:	b.ls	1f8 <__modti3+0x1f8>  // b.plast
 1e8:	adds	x5, x4, x5
 1ec:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
 1f0:	add	x0, x5, x4
 1f4:	csel	x5, x0, x5, hi  // hi = pmore
 1f8:	sub	x5, x5, x1
 1fc:	b	11c <__modti3+0x11c>
 200:	adds	x1, x4, x1
 204:	add	x4, x1, x4
 208:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 20c:	csel	x1, x4, x1, hi  // hi = pmore
 210:	b	a0 <__modti3+0xa0>
 214:	mov	x10, #0x40                  	// #64
 218:	sub	x10, x10, x8
 21c:	lsl	x3, x3, x8
 220:	lsr	x9, x2, x10
 224:	orr	x9, x9, x3
 228:	lsr	x7, x1, x10
 22c:	and	x11, x9, #0xffffffff
 230:	lsr	x12, x9, #32
 234:	lsl	x4, x2, x8
 238:	lsr	x3, x0, x10
 23c:	lsl	x1, x1, x8
 240:	orr	x3, x3, x1
 244:	udiv	x2, x7, x12
 248:	lsl	x13, x0, x8
 24c:	mov	x5, x2
 250:	msub	x2, x2, x12, x7
 254:	mul	x0, x11, x5
 258:	extr	x2, x2, x3, #32
 25c:	cmp	x0, x2
 260:	b.ls	274 <__modti3+0x274>  // b.plast
 264:	adds	x2, x9, x2
 268:	ccmp	x0, x2, #0x0, cc  // cc = lo, ul, last
 26c:	b.hi	338 <__modti3+0x338>  // b.pmore
 270:	sub	x5, x5, #0x1
 274:	sub	x2, x2, x0
 278:	udiv	x0, x2, x12
 27c:	msub	x2, x0, x12, x2
 280:	mov	x7, x0
 284:	mul	x1, x11, x0
 288:	bfi	x3, x2, #32, #32
 28c:	mov	x2, x3
 290:	cmp	x1, x3
 294:	b.ls	2a8 <__modti3+0x2a8>  // b.plast
 298:	adds	x2, x9, x3
 29c:	ccmp	x1, x2, #0x0, cc  // cc = lo, ul, last
 2a0:	b.hi	32c <__modti3+0x32c>  // b.pmore
 2a4:	sub	x7, x0, #0x1
 2a8:	orr	x5, x7, x5, lsl #32
 2ac:	and	x12, x4, #0xffffffff
 2b0:	mov	w3, w7
 2b4:	lsr	x0, x4, #32
 2b8:	lsr	x5, x5, #32
 2bc:	sub	x1, x2, x1
 2c0:	mov	x14, #0x100000000           	// #4294967296
 2c4:	mul	x7, x3, x12
 2c8:	mul	x12, x5, x12
 2cc:	madd	x3, x3, x0, x12
 2d0:	and	x11, x7, #0xffffffff
 2d4:	mul	x2, x5, x0
 2d8:	add	x5, x3, x7, lsr #32
 2dc:	add	x0, x2, x14
 2e0:	cmp	x12, x5
 2e4:	csel	x2, x0, x2, hi  // hi = pmore
 2e8:	add	x7, x11, x5, lsl #32
 2ec:	add	x2, x2, x5, lsr #32
 2f0:	cmp	x1, x2
 2f4:	b.cc	300 <__modti3+0x300>  // b.lo, b.ul, b.last
 2f8:	ccmp	x13, x7, #0x2, eq  // eq = none
 2fc:	b.cs	30c <__modti3+0x30c>  // b.hs, b.nlast
 300:	subs	x7, x7, x4
 304:	cinc	x3, x9, cc  // cc = lo, ul, last
 308:	sub	x2, x2, x3
 30c:	subs	x0, x13, x7
 310:	cmp	x13, x7
 314:	sbc	x1, x1, x2
 318:	lsr	x0, x0, x8
 31c:	lsl	x10, x1, x10
 320:	orr	x0, x10, x0
 324:	lsr	x1, x1, x8
 328:	b	ac <__modti3+0xac>
 32c:	sub	x7, x0, #0x2
 330:	add	x2, x2, x9
 334:	b	2a8 <__modti3+0x2a8>
 338:	sub	x5, x5, #0x2
 33c:	add	x2, x2, x9
 340:	b	274 <__modti3+0x274>

_divmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodti4>:
   0:	mov	x6, #0xffffffffffffffff    	// #-1
   4:	mov	x5, #0x0                   	// #0
   8:	tbnz	x1, #63, 10c <__divmodti4+0x10c>
   c:	tbnz	x3, #63, 120 <__divmodti4+0x120>
  10:	mov	x6, x5
  14:	mov	x7, x2
  18:	mov	x11, x0
  1c:	mov	x8, x1
  20:	cbnz	x3, ec <__divmodti4+0xec>
  24:	cmp	x2, x1
  28:	b.ls	15c <__divmodti4+0x15c>  // b.plast
  2c:	clz	x0, x2
  30:	mov	x10, x0
  34:	cbz	x0, 50 <__divmodti4+0x50>
  38:	neg	w1, w0
  3c:	lsl	x3, x8, x0
  40:	lsl	x7, x2, x0
  44:	lsr	x8, x11, x1
  48:	orr	x8, x8, x3
  4c:	lsl	x11, x11, x0
  50:	lsr	x9, x7, #32
  54:	and	x2, x7, #0xffffffff
  58:	udiv	x3, x8, x9
  5c:	mov	x0, x3
  60:	msub	x3, x3, x9, x8
  64:	mul	x1, x2, x0
  68:	extr	x3, x3, x11, #32
  6c:	cmp	x1, x3
  70:	b.ls	84 <__divmodti4+0x84>  // b.plast
  74:	adds	x3, x7, x3
  78:	ccmp	x1, x3, #0x0, cc  // cc = lo, ul, last
  7c:	b.hi	3e8 <__divmodti4+0x3e8>  // b.pmore
  80:	sub	x0, x0, #0x1
  84:	sub	x3, x3, x1
  88:	udiv	x1, x3, x9
  8c:	msub	x3, x1, x9, x3
  90:	mov	x8, x1
  94:	mul	x2, x2, x1
  98:	mov	x1, x11
  9c:	bfi	x1, x3, #32, #32
  a0:	cmp	x2, x1
  a4:	b.ls	b8 <__divmodti4+0xb8>  // b.plast
  a8:	adds	x1, x7, x1
  ac:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
  b0:	b.hi	3f4 <__divmodti4+0x3f4>  // b.pmore
  b4:	sub	x8, x8, #0x1
  b8:	orr	x0, x8, x0, lsl #32
  bc:	sub	x2, x1, x2
  c0:	mov	x1, #0x0                   	// #0
  c4:	lsr	x2, x2, x10
  c8:	mov	x3, #0x0                   	// #0
  cc:	cbz	x6, d8 <__divmodti4+0xd8>
  d0:	negs	x0, x0
  d4:	ngc	x1, x1
  d8:	cbz	x5, e4 <__divmodti4+0xe4>
  dc:	negs	x2, x2
  e0:	ngc	x3, x3
  e4:	stp	x2, x3, [x4]
  e8:	ret
  ec:	cmp	x3, x1
  f0:	mov	x7, x0
  f4:	b.ls	12c <__divmodti4+0x12c>  // b.plast
  f8:	mov	x2, x0
  fc:	mov	x3, x1
 100:	mov	x0, #0x0                   	// #0
 104:	mov	x1, #0x0                   	// #0
 108:	b	cc <__divmodti4+0xcc>
 10c:	negs	x0, x0
 110:	mov	x6, #0x0                   	// #0
 114:	ngc	x1, x1
 118:	mov	x5, #0xffffffffffffffff    	// #-1
 11c:	tbz	x3, #63, 10 <__divmodti4+0x10>
 120:	negs	x2, x2
 124:	ngc	x3, x3
 128:	b	14 <__divmodti4+0x14>
 12c:	clz	x8, x3
 130:	cbnz	x8, 280 <__divmodti4+0x280>
 134:	ccmp	x2, x11, #0x0, cs  // cs = hs, nlast
 138:	mov	x0, #0x0                   	// #0
 13c:	b.hi	14c <__divmodti4+0x14c>  // b.pmore
 140:	subs	x7, x11, x2
 144:	mov	x0, #0x1                   	// #1
 148:	sbc	x1, x1, x3
 14c:	mov	x3, x1
 150:	mov	x2, x7
 154:	mov	x1, #0x0                   	// #0
 158:	b	cc <__divmodti4+0xcc>
 15c:	cbnz	x2, 168 <__divmodti4+0x168>
 160:	mov	x7, #0x1                   	// #1
 164:	udiv	x7, x7, x3
 168:	clz	x3, x7
 16c:	mov	x10, x3
 170:	cbnz	x3, 1f0 <__divmodti4+0x1f0>
 174:	lsr	x12, x7, #32
 178:	and	x2, x7, #0xffffffff
 17c:	sub	x9, x1, x7
 180:	mov	x1, #0x1                   	// #1
 184:	udiv	x8, x9, x12
 188:	mov	x0, x8
 18c:	msub	x8, x8, x12, x9
 190:	mul	x3, x0, x2
 194:	extr	x8, x8, x11, #32
 198:	cmp	x3, x8
 19c:	b.ls	1b0 <__divmodti4+0x1b0>  // b.plast
 1a0:	adds	x8, x7, x8
 1a4:	ccmp	x3, x8, #0x0, cc  // cc = lo, ul, last
 1a8:	b.hi	3dc <__divmodti4+0x3dc>  // b.pmore
 1ac:	sub	x0, x0, #0x1
 1b0:	sub	x8, x8, x3
 1b4:	udiv	x9, x8, x12
 1b8:	msub	x8, x9, x12, x8
 1bc:	mov	x13, x9
 1c0:	mul	x3, x9, x2
 1c4:	mov	x2, x11
 1c8:	bfi	x2, x8, #32, #32
 1cc:	cmp	x3, x2
 1d0:	b.ls	1e4 <__divmodti4+0x1e4>  // b.plast
 1d4:	adds	x2, x7, x2
 1d8:	ccmp	x3, x2, #0x0, cc  // cc = lo, ul, last
 1dc:	b.hi	3d0 <__divmodti4+0x3d0>  // b.pmore
 1e0:	sub	x13, x9, #0x1
 1e4:	sub	x2, x2, x3
 1e8:	orr	x0, x13, x0, lsl #32
 1ec:	b	c4 <__divmodti4+0xc4>
 1f0:	lsl	x7, x7, x3
 1f4:	mov	x9, #0x40                  	// #64
 1f8:	sub	x9, x9, x3
 1fc:	lsr	x12, x7, #32
 200:	lsl	x11, x1, x3
 204:	and	x2, x7, #0xffffffff
 208:	lsr	x13, x1, x9
 20c:	lsr	x8, x0, x9
 210:	orr	x8, x8, x11
 214:	udiv	x9, x13, x12
 218:	lsl	x11, x0, x3
 21c:	mov	x1, x9
 220:	msub	x9, x9, x12, x13
 224:	mul	x3, x2, x1
 228:	extr	x0, x9, x8, #32
 22c:	cmp	x3, x0
 230:	b.ls	244 <__divmodti4+0x244>  // b.plast
 234:	adds	x0, x7, x0
 238:	ccmp	x3, x0, #0x0, cc  // cc = lo, ul, last
 23c:	b.hi	3ac <__divmodti4+0x3ac>  // b.pmore
 240:	sub	x1, x1, #0x1
 244:	sub	x0, x0, x3
 248:	udiv	x3, x0, x12
 24c:	msub	x0, x3, x12, x0
 250:	mov	x13, x3
 254:	mul	x9, x2, x3
 258:	bfi	x8, x0, #32, #32
 25c:	cmp	x9, x8
 260:	b.ls	274 <__divmodti4+0x274>  // b.plast
 264:	adds	x8, x7, x8
 268:	ccmp	x9, x8, #0x0, cc  // cc = lo, ul, last
 26c:	b.hi	3c4 <__divmodti4+0x3c4>  // b.pmore
 270:	sub	x13, x3, #0x1
 274:	sub	x9, x8, x9
 278:	orr	x1, x13, x1, lsl #32
 27c:	b	184 <__divmodti4+0x184>
 280:	mov	x9, #0x40                  	// #64
 284:	sub	x9, x9, x8
 288:	lsl	x3, x3, x8
 28c:	lsr	x10, x2, x9
 290:	orr	x10, x10, x3
 294:	lsr	x11, x1, x9
 298:	and	x3, x10, #0xffffffff
 29c:	lsr	x14, x10, #32
 2a0:	lsr	x7, x0, x9
 2a4:	lsl	x1, x1, x8
 2a8:	orr	x1, x7, x1
 2ac:	lsl	x13, x0, x8
 2b0:	udiv	x7, x11, x14
 2b4:	lsl	x2, x2, x8
 2b8:	mov	x0, x7
 2bc:	msub	x7, x7, x14, x11
 2c0:	mul	x11, x3, x0
 2c4:	extr	x7, x7, x1, #32
 2c8:	cmp	x11, x7
 2cc:	b.ls	2e0 <__divmodti4+0x2e0>  // b.plast
 2d0:	adds	x7, x10, x7
 2d4:	ccmp	x11, x7, #0x0, cc  // cc = lo, ul, last
 2d8:	b.hi	3a0 <__divmodti4+0x3a0>  // b.pmore
 2dc:	sub	x0, x0, #0x1
 2e0:	sub	x7, x7, x11
 2e4:	udiv	x12, x7, x14
 2e8:	msub	x7, x12, x14, x7
 2ec:	mov	x11, x12
 2f0:	mul	x3, x3, x12
 2f4:	bfi	x1, x7, #32, #32
 2f8:	cmp	x3, x1
 2fc:	b.ls	310 <__divmodti4+0x310>  // b.plast
 300:	adds	x1, x10, x1
 304:	ccmp	x3, x1, #0x0, cc  // cc = lo, ul, last
 308:	b.hi	3b8 <__divmodti4+0x3b8>  // b.pmore
 30c:	sub	x11, x12, #0x1
 310:	orr	x0, x11, x0, lsl #32
 314:	and	x15, x2, #0xffffffff
 318:	mov	w11, w11
 31c:	lsr	x12, x2, #32
 320:	lsr	x14, x0, #32
 324:	sub	x3, x1, x3
 328:	mov	x16, #0x100000000           	// #4294967296
 32c:	mul	x7, x11, x15
 330:	mul	x15, x14, x15
 334:	madd	x11, x11, x12, x15
 338:	and	x1, x7, #0xffffffff
 33c:	mul	x12, x14, x12
 340:	add	x7, x11, x7, lsr #32
 344:	add	x11, x12, x16
 348:	cmp	x15, x7
 34c:	csel	x12, x11, x12, hi  // hi = pmore
 350:	add	x1, x1, x7, lsl #32
 354:	add	x7, x12, x7, lsr #32
 358:	cmp	x3, x7
 35c:	b.cc	368 <__divmodti4+0x368>  // b.lo, b.ul, b.last
 360:	ccmp	x13, x1, #0x2, eq  // eq = none
 364:	b.cs	37c <__divmodti4+0x37c>  // b.hs, b.nlast
 368:	cmp	x1, x2
 36c:	sub	x0, x0, #0x1
 370:	cinc	x10, x10, cc  // cc = lo, ul, last
 374:	sub	x1, x1, x2
 378:	sub	x7, x7, x10
 37c:	subs	x2, x13, x1
 380:	cmp	x13, x1
 384:	mov	x1, #0x0                   	// #0
 388:	sbc	x3, x3, x7
 38c:	lsr	x2, x2, x8
 390:	lsl	x9, x3, x9
 394:	orr	x2, x9, x2
 398:	lsr	x3, x3, x8
 39c:	b	cc <__divmodti4+0xcc>
 3a0:	sub	x0, x0, #0x2
 3a4:	add	x7, x7, x10
 3a8:	b	2e0 <__divmodti4+0x2e0>
 3ac:	sub	x1, x1, #0x2
 3b0:	add	x0, x0, x7
 3b4:	b	244 <__divmodti4+0x244>
 3b8:	sub	x11, x12, #0x2
 3bc:	add	x1, x1, x10
 3c0:	b	310 <__divmodti4+0x310>
 3c4:	sub	x13, x3, #0x2
 3c8:	add	x8, x8, x7
 3cc:	b	274 <__divmodti4+0x274>
 3d0:	sub	x13, x9, #0x2
 3d4:	add	x2, x2, x7
 3d8:	b	1e4 <__divmodti4+0x1e4>
 3dc:	sub	x0, x0, #0x2
 3e0:	add	x8, x8, x7
 3e4:	b	1b0 <__divmodti4+0x1b0>
 3e8:	sub	x0, x0, #0x2
 3ec:	add	x3, x3, x7
 3f0:	b	84 <__divmodti4+0x84>
 3f4:	sub	x8, x8, #0x2
 3f8:	add	x1, x1, x7
 3fc:	b	b8 <__divmodti4+0xb8>

_udivdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, x2
   4:	mov	x5, x0
   8:	mov	x6, x1
   c:	cbnz	x3, a8 <__udivti3+0xa8>
  10:	cmp	x2, x1
  14:	b.ls	1bc <__udivti3+0x1bc>  // b.plast
  18:	clz	x0, x2
  1c:	cbz	x0, 38 <__udivti3+0x38>
  20:	neg	w1, w0
  24:	lsl	x2, x6, x0
  28:	lsl	x4, x4, x0
  2c:	lsr	x6, x5, x1
  30:	orr	x6, x6, x2
  34:	lsl	x5, x5, x0
  38:	lsr	x3, x4, #32
  3c:	and	x7, x4, #0xffffffff
  40:	udiv	x2, x6, x3
  44:	mov	x0, x2
  48:	msub	x2, x2, x3, x6
  4c:	mul	x6, x7, x0
  50:	extr	x1, x2, x5, #32
  54:	cmp	x6, x1
  58:	b.ls	6c <__udivti3+0x6c>  // b.plast
  5c:	adds	x1, x4, x1
  60:	ccmp	x6, x1, #0x0, cc  // cc = lo, ul, last
  64:	b.hi	32c <__udivti3+0x32c>  // b.pmore
  68:	sub	x0, x0, #0x1
  6c:	sub	x1, x1, x6
  70:	udiv	x6, x1, x3
  74:	msub	x1, x6, x3, x1
  78:	mov	x2, x6
  7c:	mul	x7, x7, x6
  80:	bfi	x5, x1, #32, #32
  84:	cmp	x7, x5
  88:	b.ls	9c <__udivti3+0x9c>  // b.plast
  8c:	adds	x5, x4, x5
  90:	ccmp	x7, x5, #0x0, cc  // cc = lo, ul, last
  94:	cinc	x2, x6, ls  // ls = plast
  98:	sub	x2, x2, #0x2
  9c:	orr	x0, x2, x0, lsl #32
  a0:	mov	x1, #0x0                   	// #0
  a4:	ret
  a8:	cmp	x3, x1
  ac:	b.ls	bc <__udivti3+0xbc>  // b.plast
  b0:	mov	x1, #0x0                   	// #0
  b4:	mov	x0, #0x0                   	// #0
  b8:	ret
  bc:	clz	x7, x3
  c0:	cbz	x7, 2d4 <__udivti3+0x2d4>
  c4:	mov	x4, #0x40                  	// #64
  c8:	sub	x4, x4, x7
  cc:	lsl	x3, x3, x7
  d0:	lsr	x5, x2, x4
  d4:	orr	x3, x5, x3
  d8:	lsr	x6, x1, x4
  dc:	and	x8, x3, #0xffffffff
  e0:	lsr	x10, x3, #32
  e4:	lsr	x4, x0, x4
  e8:	lsl	x1, x1, x7
  ec:	orr	x1, x4, x1
  f0:	lsl	x2, x2, x7
  f4:	udiv	x5, x6, x10
  f8:	mov	x4, x5
  fc:	msub	x5, x5, x10, x6
 100:	mul	x6, x8, x4
 104:	extr	x5, x5, x1, #32
 108:	cmp	x6, x5
 10c:	b.ls	120 <__udivti3+0x120>  // b.plast
 110:	adds	x5, x3, x5
 114:	ccmp	x6, x5, #0x0, cc  // cc = lo, ul, last
 118:	b.hi	314 <__udivti3+0x314>  // b.pmore
 11c:	sub	x4, x4, #0x1
 120:	sub	x6, x5, x6
 124:	udiv	x9, x6, x10
 128:	msub	x6, x9, x10, x6
 12c:	mov	x5, x9
 130:	mul	x8, x8, x9
 134:	bfi	x1, x6, #32, #32
 138:	cmp	x8, x1
 13c:	b.ls	150 <__udivti3+0x150>  // b.plast
 140:	adds	x1, x3, x1
 144:	ccmp	x8, x1, #0x0, cc  // cc = lo, ul, last
 148:	b.hi	2fc <__udivti3+0x2fc>  // b.pmore
 14c:	sub	x5, x9, #0x1
 150:	orr	x4, x5, x4, lsl #32
 154:	and	x9, x2, #0xffffffff
 158:	mov	w5, w5
 15c:	lsr	x2, x2, #32
 160:	lsr	x6, x4, #32
 164:	sub	x1, x1, x8
 168:	mov	x10, #0x100000000           	// #4294967296
 16c:	mul	x11, x5, x9
 170:	mul	x8, x6, x9
 174:	madd	x5, x5, x2, x8
 178:	mul	x2, x6, x2
 17c:	add	x3, x5, x11, lsr #32
 180:	add	x5, x2, x10
 184:	cmp	x8, x3
 188:	csel	x2, x5, x2, hi  // hi = pmore
 18c:	add	x2, x2, x3, lsr #32
 190:	cmp	x1, x2
 194:	b.cc	1b0 <__udivti3+0x1b0>  // b.lo, b.ul, b.last
 198:	and	x11, x11, #0xffffffff
 19c:	lsl	x0, x0, x7
 1a0:	add	x3, x11, x3, lsl #32
 1a4:	cmp	x0, x3
 1a8:	ccmp	x1, x2, #0x0, cc  // cc = lo, ul, last
 1ac:	b.ne	2e4 <__udivti3+0x2e4>  // b.any
 1b0:	sub	x0, x4, #0x1
 1b4:	mov	x1, #0x0                   	// #0
 1b8:	ret
 1bc:	cbnz	x2, 1c8 <__udivti3+0x1c8>
 1c0:	mov	x2, #0x1                   	// #1
 1c4:	udiv	x4, x2, x4
 1c8:	clz	x8, x4
 1cc:	cbnz	x8, 244 <__udivti3+0x244>
 1d0:	lsr	x6, x4, #32
 1d4:	and	x7, x4, #0xffffffff
 1d8:	sub	x2, x1, x4
 1dc:	mov	x1, #0x1                   	// #1
 1e0:	udiv	x3, x2, x6
 1e4:	mov	x0, x3
 1e8:	msub	x3, x3, x6, x2
 1ec:	mul	x8, x0, x7
 1f0:	extr	x2, x3, x5, #32
 1f4:	cmp	x8, x2
 1f8:	b.ls	20c <__udivti3+0x20c>  // b.plast
 1fc:	adds	x2, x4, x2
 200:	ccmp	x8, x2, #0x0, cc  // cc = lo, ul, last
 204:	b.hi	320 <__udivti3+0x320>  // b.pmore
 208:	sub	x0, x0, #0x1
 20c:	sub	x2, x2, x8
 210:	udiv	x8, x2, x6
 214:	msub	x2, x8, x6, x2
 218:	mov	x3, x8
 21c:	mul	x7, x8, x7
 220:	bfi	x5, x2, #32, #32
 224:	cmp	x7, x5
 228:	b.ls	23c <__udivti3+0x23c>  // b.plast
 22c:	adds	x4, x4, x5
 230:	ccmp	x7, x4, #0x0, cc  // cc = lo, ul, last
 234:	cinc	x3, x8, ls  // ls = plast
 238:	sub	x3, x3, #0x2
 23c:	orr	x0, x3, x0, lsl #32
 240:	ret
 244:	lsl	x4, x4, x8
 248:	mov	x3, #0x40                  	// #64
 24c:	sub	x3, x3, x8
 250:	lsr	x6, x4, #32
 254:	lsl	x2, x1, x8
 258:	and	x7, x4, #0xffffffff
 25c:	lsr	x1, x1, x3
 260:	lsr	x3, x0, x3
 264:	orr	x3, x3, x2
 268:	udiv	x2, x1, x6
 26c:	lsl	x5, x0, x8
 270:	msub	x1, x2, x6, x1
 274:	mov	x0, x2
 278:	mul	x2, x7, x2
 27c:	extr	x1, x1, x3, #32
 280:	cmp	x2, x1
 284:	b.ls	298 <__udivti3+0x298>  // b.plast
 288:	adds	x1, x4, x1
 28c:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 290:	b.hi	2f0 <__udivti3+0x2f0>  // b.pmore
 294:	sub	x0, x0, #0x1
 298:	sub	x1, x1, x2
 29c:	udiv	x2, x1, x6
 2a0:	msub	x1, x2, x6, x1
 2a4:	mov	x8, x2
 2a8:	mul	x2, x7, x2
 2ac:	bfi	x3, x1, #32, #32
 2b0:	cmp	x2, x3
 2b4:	b.ls	2c8 <__udivti3+0x2c8>  // b.plast
 2b8:	adds	x3, x4, x3
 2bc:	ccmp	x2, x3, #0x0, cc  // cc = lo, ul, last
 2c0:	b.hi	308 <__udivti3+0x308>  // b.pmore
 2c4:	sub	x8, x8, #0x1
 2c8:	sub	x2, x3, x2
 2cc:	orr	x1, x8, x0, lsl #32
 2d0:	b	1e0 <__udivti3+0x1e0>
 2d4:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 2d8:	mov	x1, #0x0                   	// #0
 2dc:	cset	x0, ls  // ls = plast
 2e0:	ret
 2e4:	mov	x0, x4
 2e8:	mov	x1, #0x0                   	// #0
 2ec:	ret
 2f0:	sub	x0, x0, #0x2
 2f4:	add	x1, x1, x4
 2f8:	b	298 <__udivti3+0x298>
 2fc:	sub	x5, x9, #0x2
 300:	add	x1, x1, x3
 304:	b	150 <__udivti3+0x150>
 308:	sub	x8, x8, #0x2
 30c:	add	x3, x3, x4
 310:	b	2c8 <__udivti3+0x2c8>
 314:	sub	x4, x4, #0x2
 318:	add	x5, x5, x3
 31c:	b	120 <__udivti3+0x120>
 320:	sub	x0, x0, #0x2
 324:	add	x2, x2, x4
 328:	b	20c <__udivti3+0x20c>
 32c:	sub	x0, x0, #0x2
 330:	add	x1, x1, x4
 334:	b	6c <__udivti3+0x6c>

_umoddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	mov	x4, x2
   4:	mov	x7, x0
   8:	mov	x6, x1
   c:	mov	x5, x1
  10:	cbnz	x3, a0 <__umodti3+0xa0>
  14:	cmp	x2, x1
  18:	b.ls	d4 <__umodti3+0xd4>  // b.plast
  1c:	clz	x0, x2
  20:	mov	x6, x0
  24:	cbz	x0, 40 <__umodti3+0x40>
  28:	neg	w1, w0
  2c:	lsl	x5, x5, x0
  30:	lsl	x4, x2, x0
  34:	lsr	x1, x7, x1
  38:	orr	x5, x1, x5
  3c:	lsl	x7, x7, x0
  40:	lsr	x2, x4, #32
  44:	and	x0, x4, #0xffffffff
  48:	udiv	x1, x5, x2
  4c:	msub	x5, x1, x2, x5
  50:	mul	x1, x0, x1
  54:	extr	x5, x5, x7, #32
  58:	cmp	x1, x5
  5c:	b.ls	70 <__umodti3+0x70>  // b.plast
  60:	adds	x5, x4, x5
  64:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
  68:	add	x3, x5, x4
  6c:	csel	x5, x3, x5, hi  // hi = pmore
  70:	sub	x5, x5, x1
  74:	udiv	x1, x5, x2
  78:	msub	x5, x1, x2, x5
  7c:	mul	x0, x0, x1
  80:	mov	x1, x7
  84:	bfi	x1, x5, #32, #32
  88:	cmp	x0, x1
  8c:	b.hi	1dc <__umodti3+0x1dc>  // b.pmore
  90:	sub	x0, x1, x0
  94:	lsr	x0, x0, x6
  98:	mov	x1, #0x0                   	// #0
  9c:	ret
  a0:	cmp	x3, x1
  a4:	mov	x4, x0
  a8:	b.hi	9c <__umodti3+0x9c>  // b.pmore
  ac:	clz	x7, x3
  b0:	cbnz	x7, 1f0 <__umodti3+0x1f0>
  b4:	cmp	x3, x1
  b8:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
  bc:	b.hi	c8 <__umodti3+0xc8>  // b.pmore
  c0:	subs	x4, x0, x2
  c4:	sbc	x6, x1, x3
  c8:	mov	x0, x4
  cc:	mov	x1, x6
  d0:	ret
  d4:	cbnz	x2, e0 <__umodti3+0xe0>
  d8:	mov	x2, #0x1                   	// #1
  dc:	udiv	x4, x2, x4
  e0:	clz	x3, x4
  e4:	mov	x6, x3
  e8:	cbnz	x3, 158 <__umodti3+0x158>
  ec:	sub	x5, x1, x4
  f0:	lsr	x8, x4, #32
  f4:	and	x10, x4, #0xffffffff
  f8:	udiv	x3, x5, x8
  fc:	msub	x5, x3, x8, x5
 100:	mul	x9, x3, x10
 104:	extr	x3, x5, x7, #32
 108:	cmp	x9, x3
 10c:	b.ls	120 <__umodti3+0x120>  // b.plast
 110:	adds	x5, x4, x3
 114:	ccmp	x9, x5, #0x0, cc  // cc = lo, ul, last
 118:	add	x3, x5, x4
 11c:	csel	x3, x3, x5, hi  // hi = pmore
 120:	sub	x3, x3, x9
 124:	mov	x1, x7
 128:	udiv	x2, x3, x8
 12c:	msub	x3, x2, x8, x3
 130:	mul	x2, x2, x10
 134:	bfi	x1, x3, #32, #32
 138:	cmp	x2, x1
 13c:	b.ls	150 <__umodti3+0x150>  // b.plast
 140:	adds	x0, x4, x1
 144:	add	x4, x0, x4
 148:	ccmp	x2, x0, #0x0, cc  // cc = lo, ul, last
 14c:	csel	x1, x4, x0, hi  // hi = pmore
 150:	sub	x0, x1, x2
 154:	b	94 <__umodti3+0x94>
 158:	lsl	x4, x4, x3
 15c:	mov	x5, #0x40                  	// #64
 160:	sub	x5, x5, x3
 164:	lsr	x8, x4, #32
 168:	lsl	x2, x1, x3
 16c:	and	x10, x4, #0xffffffff
 170:	lsr	x1, x1, x5
 174:	lsr	x5, x0, x5
 178:	orr	x5, x5, x2
 17c:	udiv	x2, x1, x8
 180:	lsl	x7, x0, x3
 184:	msub	x1, x2, x8, x1
 188:	mul	x0, x10, x2
 18c:	extr	x2, x1, x5, #32
 190:	cmp	x0, x2
 194:	b.ls	1a8 <__umodti3+0x1a8>  // b.plast
 198:	adds	x2, x4, x2
 19c:	ccmp	x0, x2, #0x0, cc  // cc = lo, ul, last
 1a0:	add	x1, x2, x4
 1a4:	csel	x2, x1, x2, hi  // hi = pmore
 1a8:	sub	x2, x2, x0
 1ac:	udiv	x1, x2, x8
 1b0:	msub	x2, x1, x8, x2
 1b4:	mul	x1, x10, x1
 1b8:	bfi	x5, x2, #32, #32
 1bc:	cmp	x1, x5
 1c0:	b.ls	1d4 <__umodti3+0x1d4>  // b.plast
 1c4:	adds	x5, x4, x5
 1c8:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
 1cc:	add	x0, x5, x4
 1d0:	csel	x5, x0, x5, hi  // hi = pmore
 1d4:	sub	x5, x5, x1
 1d8:	b	f8 <__umodti3+0xf8>
 1dc:	adds	x1, x4, x1
 1e0:	add	x4, x1, x4
 1e4:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 1e8:	csel	x1, x4, x1, hi  // hi = pmore
 1ec:	b	90 <__umodti3+0x90>
 1f0:	mov	x8, #0x40                  	// #64
 1f4:	sub	x8, x8, x7
 1f8:	lsl	x3, x3, x7
 1fc:	lsr	x4, x2, x8
 200:	orr	x3, x4, x3
 204:	lsr	x6, x1, x8
 208:	and	x9, x3, #0xffffffff
 20c:	lsr	x10, x3, #32
 210:	lsr	x4, x0, x8
 214:	lsl	x1, x1, x7
 218:	orr	x1, x4, x1
 21c:	lsl	x12, x0, x7
 220:	udiv	x4, x6, x10
 224:	lsl	x2, x2, x7
 228:	mov	x5, x4
 22c:	msub	x4, x4, x10, x6
 230:	mul	x0, x9, x5
 234:	extr	x4, x4, x1, #32
 238:	cmp	x0, x4
 23c:	b.ls	250 <__umodti3+0x250>  // b.plast
 240:	adds	x4, x3, x4
 244:	ccmp	x0, x4, #0x0, cc  // cc = lo, ul, last
 248:	b.hi	310 <__umodti3+0x310>  // b.pmore
 24c:	sub	x5, x5, #0x1
 250:	sub	x4, x4, x0
 254:	udiv	x0, x4, x10
 258:	msub	x4, x0, x10, x4
 25c:	mov	x6, x0
 260:	mul	x0, x9, x0
 264:	bfi	x1, x4, #32, #32
 268:	cmp	x0, x1
 26c:	b.ls	280 <__umodti3+0x280>  // b.plast
 270:	adds	x1, x3, x1
 274:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 278:	b.hi	304 <__umodti3+0x304>  // b.pmore
 27c:	sub	x6, x6, #0x1
 280:	orr	x5, x6, x5, lsl #32
 284:	and	x11, x2, #0xffffffff
 288:	mov	w9, w6
 28c:	lsr	x4, x2, #32
 290:	lsr	x5, x5, #32
 294:	sub	x1, x1, x0
 298:	mov	x13, #0x100000000           	// #4294967296
 29c:	mul	x6, x9, x11
 2a0:	mul	x11, x5, x11
 2a4:	madd	x9, x9, x4, x11
 2a8:	and	x10, x6, #0xffffffff
 2ac:	mul	x4, x5, x4
 2b0:	add	x5, x9, x6, lsr #32
 2b4:	add	x0, x4, x13
 2b8:	cmp	x11, x5
 2bc:	csel	x4, x0, x4, hi  // hi = pmore
 2c0:	add	x6, x10, x5, lsl #32
 2c4:	add	x4, x4, x5, lsr #32
 2c8:	cmp	x1, x4
 2cc:	b.cc	2d8 <__umodti3+0x2d8>  // b.lo, b.ul, b.last
 2d0:	ccmp	x12, x6, #0x2, eq  // eq = none
 2d4:	b.cs	2e4 <__umodti3+0x2e4>  // b.hs, b.nlast
 2d8:	subs	x6, x6, x2
 2dc:	cinc	x3, x3, cc  // cc = lo, ul, last
 2e0:	sub	x4, x4, x3
 2e4:	subs	x0, x12, x6
 2e8:	cmp	x12, x6
 2ec:	sbc	x1, x1, x4
 2f0:	lsr	x0, x0, x7
 2f4:	lsl	x8, x1, x8
 2f8:	orr	x0, x8, x0
 2fc:	lsr	x1, x1, x7
 300:	ret
 304:	sub	x6, x6, #0x2
 308:	add	x1, x1, x3
 30c:	b	280 <__umodti3+0x280>
 310:	sub	x5, x5, #0x2
 314:	add	x4, x4, x3
 318:	b	250 <__umodti3+0x250>

_udivmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	mov	x9, x0
   4:	mov	x6, x1
   8:	mov	x7, x2
   c:	mov	x5, x0
  10:	mov	x8, x1
  14:	cbnz	x3, 154 <__udivmodti4+0x154>
  18:	cmp	x2, x1
  1c:	b.ls	c4 <__udivmodti4+0xc4>  // b.plast
  20:	clz	x0, x2
  24:	mov	x3, x0
  28:	cbz	x0, 44 <__udivmodti4+0x44>
  2c:	neg	w2, w0
  30:	lsl	x1, x1, x0
  34:	lsl	x7, x7, x0
  38:	lsr	x8, x9, x2
  3c:	orr	x8, x8, x1
  40:	lsl	x5, x9, x0
  44:	lsr	x9, x7, #32
  48:	and	x2, x7, #0xffffffff
  4c:	udiv	x1, x8, x9
  50:	mov	x0, x1
  54:	msub	x1, x1, x9, x8
  58:	mul	x6, x2, x0
  5c:	extr	x1, x1, x5, #32
  60:	cmp	x6, x1
  64:	b.ls	78 <__udivmodti4+0x78>  // b.plast
  68:	adds	x1, x7, x1
  6c:	ccmp	x6, x1, #0x0, cc  // cc = lo, ul, last
  70:	b.hi	3a8 <__udivmodti4+0x3a8>  // b.pmore
  74:	sub	x0, x0, #0x1
  78:	sub	x1, x1, x6
  7c:	udiv	x6, x1, x9
  80:	msub	x1, x6, x9, x1
  84:	mov	x8, x6
  88:	mul	x2, x2, x6
  8c:	bfi	x5, x1, #32, #32
  90:	cmp	x2, x5
  94:	b.ls	a8 <__udivmodti4+0xa8>  // b.plast
  98:	adds	x5, x7, x5
  9c:	ccmp	x2, x5, #0x0, cc  // cc = lo, ul, last
  a0:	b.hi	3b4 <__udivmodti4+0x3b4>  // b.pmore
  a4:	sub	x8, x6, #0x1
  a8:	sub	x5, x5, x2
  ac:	orr	x0, x8, x0, lsl #32
  b0:	mov	x1, #0x0                   	// #0
  b4:	cbz	x4, c0 <__udivmodti4+0xc0>
  b8:	lsr	x5, x5, x3
  bc:	stp	x5, xzr, [x4]
  c0:	ret
  c4:	cbnz	x2, d0 <__udivmodti4+0xd0>
  c8:	mov	x0, #0x1                   	// #1
  cc:	udiv	x7, x0, x2
  d0:	clz	x11, x7
  d4:	mov	x3, x11
  d8:	cbnz	x11, 1a4 <__udivmodti4+0x1a4>
  dc:	sub	x2, x6, x7
  e0:	lsr	x10, x7, #32
  e4:	and	x8, x7, #0xffffffff
  e8:	mov	x1, #0x1                   	// #1
  ec:	udiv	x6, x2, x10
  f0:	mov	x0, x6
  f4:	msub	x6, x6, x10, x2
  f8:	mul	x2, x0, x8
  fc:	extr	x6, x6, x5, #32
 100:	cmp	x2, x6
 104:	b.ls	118 <__udivmodti4+0x118>  // b.plast
 108:	adds	x6, x7, x6
 10c:	ccmp	x2, x6, #0x0, cc  // cc = lo, ul, last
 110:	b.hi	39c <__udivmodti4+0x39c>  // b.pmore
 114:	sub	x0, x0, #0x1
 118:	sub	x6, x6, x2
 11c:	udiv	x2, x6, x10
 120:	msub	x6, x2, x10, x6
 124:	mov	x9, x2
 128:	mul	x8, x2, x8
 12c:	bfi	x5, x6, #32, #32
 130:	cmp	x8, x5
 134:	b.ls	148 <__udivmodti4+0x148>  // b.plast
 138:	adds	x5, x7, x5
 13c:	ccmp	x8, x5, #0x0, cc  // cc = lo, ul, last
 140:	b.hi	390 <__udivmodti4+0x390>  // b.pmore
 144:	sub	x9, x2, #0x1
 148:	sub	x5, x5, x8
 14c:	orr	x0, x9, x0, lsl #32
 150:	b	b4 <__udivmodti4+0xb4>
 154:	cmp	x3, x1
 158:	b.ls	170 <__udivmodti4+0x170>  // b.plast
 15c:	mov	x1, #0x0                   	// #0
 160:	mov	x0, #0x0                   	// #0
 164:	cbz	x4, c0 <__udivmodti4+0xc0>
 168:	stp	x9, x6, [x4]
 16c:	ret
 170:	clz	x7, x3
 174:	cbnz	x7, 234 <__udivmodti4+0x234>
 178:	cmp	x3, x1
 17c:	mov	x0, #0x0                   	// #0
 180:	ccmp	x2, x9, #0x0, cs  // cs = hs, nlast
 184:	b.hi	194 <__udivmodti4+0x194>  // b.pmore
 188:	subs	x5, x9, x2
 18c:	mov	x0, #0x1                   	// #1
 190:	sbc	x8, x1, x3
 194:	mov	x1, #0x0                   	// #0
 198:	cbz	x4, c0 <__udivmodti4+0xc0>
 19c:	stp	x5, x8, [x4]
 1a0:	ret
 1a4:	lsl	x7, x7, x11
 1a8:	mov	x2, #0x40                  	// #64
 1ac:	sub	x2, x2, x11
 1b0:	lsr	x10, x7, #32
 1b4:	lsl	x0, x6, x11
 1b8:	and	x8, x7, #0xffffffff
 1bc:	lsr	x1, x6, x2
 1c0:	lsr	x2, x9, x2
 1c4:	orr	x6, x2, x0
 1c8:	udiv	x0, x1, x10
 1cc:	lsl	x5, x9, x11
 1d0:	mov	x9, x0
 1d4:	msub	x0, x0, x10, x1
 1d8:	mul	x1, x8, x9
 1dc:	extr	x0, x0, x6, #32
 1e0:	cmp	x1, x0
 1e4:	b.ls	1f8 <__udivmodti4+0x1f8>  // b.plast
 1e8:	adds	x0, x7, x0
 1ec:	ccmp	x1, x0, #0x0, cc  // cc = lo, ul, last
 1f0:	b.hi	36c <__udivmodti4+0x36c>  // b.pmore
 1f4:	sub	x9, x9, #0x1
 1f8:	sub	x0, x0, x1
 1fc:	udiv	x2, x0, x10
 200:	msub	x0, x2, x10, x0
 204:	mov	x1, x2
 208:	mul	x2, x8, x2
 20c:	bfi	x6, x0, #32, #32
 210:	cmp	x2, x6
 214:	b.ls	228 <__udivmodti4+0x228>  // b.plast
 218:	adds	x6, x7, x6
 21c:	ccmp	x2, x6, #0x0, cc  // cc = lo, ul, last
 220:	b.hi	384 <__udivmodti4+0x384>  // b.pmore
 224:	sub	x1, x1, #0x1
 228:	sub	x2, x6, x2
 22c:	orr	x1, x1, x9, lsl #32
 230:	b	ec <__udivmodti4+0xec>
 234:	mov	x8, #0x40                  	// #64
 238:	sub	x8, x8, x7
 23c:	lsl	x3, x3, x7
 240:	lsr	x0, x2, x8
 244:	orr	x3, x0, x3
 248:	lsr	x5, x1, x8
 24c:	and	x13, x3, #0xffffffff
 250:	lsr	x11, x3, #32
 254:	lsl	x6, x1, x7
 258:	lsr	x1, x9, x8
 25c:	orr	x6, x1, x6
 260:	lsl	x2, x2, x7
 264:	udiv	x1, x5, x11
 268:	lsl	x9, x9, x7
 26c:	mov	x0, x1
 270:	msub	x1, x1, x11, x5
 274:	mul	x5, x13, x0
 278:	extr	x1, x1, x6, #32
 27c:	cmp	x5, x1
 280:	b.ls	294 <__udivmodti4+0x294>  // b.plast
 284:	adds	x1, x3, x1
 288:	ccmp	x5, x1, #0x0, cc  // cc = lo, ul, last
 28c:	b.hi	360 <__udivmodti4+0x360>  // b.pmore
 290:	sub	x0, x0, #0x1
 294:	sub	x1, x1, x5
 298:	udiv	x10, x1, x11
 29c:	msub	x1, x10, x11, x1
 2a0:	mov	x5, x10
 2a4:	mul	x13, x13, x10
 2a8:	bfi	x6, x1, #32, #32
 2ac:	cmp	x13, x6
 2b0:	b.ls	2c4 <__udivmodti4+0x2c4>  // b.plast
 2b4:	adds	x6, x3, x6
 2b8:	ccmp	x13, x6, #0x0, cc  // cc = lo, ul, last
 2bc:	b.hi	378 <__udivmodti4+0x378>  // b.pmore
 2c0:	sub	x5, x10, #0x1
 2c4:	orr	x0, x5, x0, lsl #32
 2c8:	and	x12, x2, #0xffffffff
 2cc:	mov	w1, w5
 2d0:	lsr	x10, x2, #32
 2d4:	lsr	x11, x0, #32
 2d8:	sub	x6, x6, x13
 2dc:	mov	x14, #0x100000000           	// #4294967296
 2e0:	mul	x5, x1, x12
 2e4:	mul	x12, x11, x12
 2e8:	madd	x1, x1, x10, x12
 2ec:	and	x13, x5, #0xffffffff
 2f0:	mul	x10, x11, x10
 2f4:	add	x5, x1, x5, lsr #32
 2f8:	add	x1, x10, x14
 2fc:	cmp	x12, x5
 300:	csel	x10, x1, x10, hi  // hi = pmore
 304:	add	x11, x13, x5, lsl #32
 308:	add	x5, x10, x5, lsr #32
 30c:	cmp	x6, x5
 310:	b.cc	348 <__udivmodti4+0x348>  // b.lo, b.ul, b.last
 314:	ccmp	x9, x11, #0x2, eq  // eq = none
 318:	b.cc	348 <__udivmodti4+0x348>  // b.lo, b.ul, b.last
 31c:	mov	x1, #0x0                   	// #0
 320:	cbz	x4, c0 <__udivmodti4+0xc0>
 324:	subs	x2, x9, x11
 328:	cmp	x9, x11
 32c:	sbc	x6, x6, x5
 330:	lsr	x2, x2, x7
 334:	lsl	x8, x6, x8
 338:	orr	x8, x8, x2
 33c:	lsr	x6, x6, x7
 340:	stp	x8, x6, [x4]
 344:	ret
 348:	cmp	x11, x2
 34c:	sub	x0, x0, #0x1
 350:	cinc	x3, x3, cc  // cc = lo, ul, last
 354:	sub	x11, x11, x2
 358:	sub	x5, x5, x3
 35c:	b	31c <__udivmodti4+0x31c>
 360:	sub	x0, x0, #0x2
 364:	add	x1, x1, x3
 368:	b	294 <__udivmodti4+0x294>
 36c:	sub	x9, x9, #0x2
 370:	add	x0, x0, x7
 374:	b	1f8 <__udivmodti4+0x1f8>
 378:	sub	x5, x10, #0x2
 37c:	add	x6, x6, x3
 380:	b	2c4 <__udivmodti4+0x2c4>
 384:	sub	x1, x1, #0x2
 388:	add	x6, x6, x7
 38c:	b	228 <__udivmodti4+0x228>
 390:	sub	x9, x2, #0x2
 394:	add	x5, x5, x7
 398:	b	148 <__udivmodti4+0x148>
 39c:	sub	x0, x0, #0x2
 3a0:	add	x6, x6, x7
 3a4:	b	118 <__udivmodti4+0x118>
 3a8:	sub	x0, x0, #0x2
 3ac:	add	x1, x1, x7
 3b0:	b	78 <__udivmodti4+0x78>
 3b4:	sub	x8, x6, #0x2
 3b8:	add	x5, x5, x7
 3bc:	b	a8 <__udivmodti4+0xa8>

_udiv_w_sdiv.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udiv_w_sdiv>:
   0:	mov	x0, #0x0                   	// #0
   4:	ret

sync-cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__aarch64_sync_cache_range>:
   0:	adrp	x3, 0 <__aarch64_sync_cache_range>
   4:	ldr	w2, [x3]
   8:	cbnz	w2, 18 <__aarch64_sync_cache_range+0x18>
   c:	mrs	x4, ctr_el0
  10:	mov	w2, w4
  14:	str	w4, [x3]
  18:	ubfx	x4, x2, #16, #4
  1c:	mov	w3, #0x4                   	// #4
  20:	and	w5, w2, #0xf
  24:	lsl	w4, w3, w4
  28:	sub	w2, w4, #0x1
  2c:	bic	x2, x0, x2
  30:	sxtw	x4, w4
  34:	cmp	x2, x1
  38:	lsl	w3, w3, w5
  3c:	b.cs	50 <__aarch64_sync_cache_range+0x50>  // b.hs, b.nlast
  40:	dc	cvau, x2
  44:	add	x2, x2, x4
  48:	cmp	x1, x2
  4c:	b.hi	40 <__aarch64_sync_cache_range+0x40>  // b.pmore
  50:	dsb	ish
  54:	sub	w2, w3, #0x1
  58:	sxtw	x3, w3
  5c:	bic	x0, x0, x2
  60:	cmp	x1, x0
  64:	b.ls	78 <__aarch64_sync_cache_range+0x78>  // b.plast
  68:	ic	ivau, x0
  6c:	add	x0, x0, x3
  70:	cmp	x1, x0
  74:	b.hi	68 <__aarch64_sync_cache_range+0x68>  // b.pmore
  78:	dsb	ish
  7c:	isb
  80:	ret

sfp-exceptions.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__sfp_handle_exceptions>:
   0:	tbz	w0, #0, 10 <__sfp_handle_exceptions+0x10>
   4:	movi	v1.2s, #0x0
   8:	fdiv	s0, s1, s1
   c:	mrs	x1, fpsr
  10:	tbz	w0, #1, 24 <__sfp_handle_exceptions+0x24>
  14:	fmov	s1, #1.000000000000000000e+00
  18:	movi	v2.2s, #0x0
  1c:	fdiv	s0, s1, s2
  20:	mrs	x1, fpsr
  24:	tbz	w0, #2, 44 <__sfp_handle_exceptions+0x44>
  28:	mov	w2, #0xc5ae                	// #50606
  2c:	mov	w1, #0x7f7fffff            	// #2139095039
  30:	movk	w2, #0x749d, lsl #16
  34:	fmov	s1, w1
  38:	fmov	s2, w2
  3c:	fadd	s0, s1, s2
  40:	mrs	x1, fpsr
  44:	tbz	w0, #3, 54 <__sfp_handle_exceptions+0x54>
  48:	movi	v1.2s, #0x80, lsl #16
  4c:	fmul	s0, s1, s1
  50:	mrs	x1, fpsr
  54:	tbz	w0, #4, 6c <__sfp_handle_exceptions+0x6c>
  58:	mov	w0, #0x7f7fffff            	// #2139095039
  5c:	fmov	s2, #1.000000000000000000e+00
  60:	fmov	s1, w0
  64:	fsub	s0, s1, s2
  68:	mrs	x0, fpsr
  6c:	ret

addtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x4, x1, [sp, #16]
  14:	ldp	x0, x3, [sp, #32]
  18:	mrs	x15, fpcr
  1c:	mov	x11, x0
  20:	ubfiz	x0, x3, #3, #48
  24:	lsr	x6, x1, #63
  28:	lsr	x5, x3, #63
  2c:	ubfiz	x2, x1, #3, #48
  30:	orr	x9, x0, x11, lsr #61
  34:	ubfx	x7, x1, #48, #15
  38:	ubfx	x0, x3, #48, #15
  3c:	mov	x12, x6
  40:	and	w10, w6, #0xff
  44:	mov	x16, x6
  48:	cmp	x6, x5
  4c:	orr	x2, x2, x4, lsr #61
  50:	and	w6, w5, #0xff
  54:	mov	x1, x7
  58:	lsl	x8, x4, #3
  5c:	mov	x3, x0
  60:	lsl	x13, x11, #3
  64:	b.eq	210 <__addtf3+0x210>  // b.none
  68:	sub	w0, w7, w0
  6c:	cmp	w0, #0x0
  70:	b.le	1bc <__addtf3+0x1bc>
  74:	cbz	x3, 270 <__addtf3+0x270>
  78:	orr	x9, x9, #0x8000000000000
  7c:	mov	x3, #0x7fff                	// #32767
  80:	cmp	x1, x3
  84:	b.eq	474 <__addtf3+0x474>  // b.none
  88:	cmp	w0, #0x74
  8c:	b.gt	4f8 <__addtf3+0x4f8>
  90:	cmp	w0, #0x3f
  94:	b.gt	658 <__addtf3+0x658>
  98:	mov	w3, #0x40                  	// #64
  9c:	sub	w3, w3, w0
  a0:	lsr	x5, x13, x0
  a4:	lsl	x13, x13, x3
  a8:	cmp	x13, #0x0
  ac:	cset	x4, ne  // ne = any
  b0:	lsl	x3, x9, x3
  b4:	orr	x3, x3, x5
  b8:	lsr	x0, x9, x0
  bc:	orr	x3, x3, x4
  c0:	sub	x2, x2, x0
  c4:	subs	x8, x8, x3
  c8:	sbc	x2, x2, xzr
  cc:	and	x3, x2, #0x7ffffffffffff
  d0:	tbz	x2, #51, 2b0 <__addtf3+0x2b0>
  d4:	cbz	x3, 4dc <__addtf3+0x4dc>
  d8:	clz	x0, x3
  dc:	sub	w0, w0, #0xc
  e0:	neg	w2, w0
  e4:	lsl	x4, x3, x0
  e8:	lsl	x3, x8, x0
  ec:	lsr	x8, x8, x2
  f0:	orr	x2, x8, x4
  f4:	cmp	x1, w0, sxtw
  f8:	sxtw	x4, w0
  fc:	b.gt	4bc <__addtf3+0x4bc>
 100:	sub	w1, w0, w1
 104:	add	w0, w1, #0x1
 108:	cmp	w0, #0x3f
 10c:	b.gt	620 <__addtf3+0x620>
 110:	mov	w1, #0x40                  	// #64
 114:	sub	w1, w1, w0
 118:	lsr	x4, x3, x0
 11c:	lsl	x3, x3, x1
 120:	cmp	x3, #0x0
 124:	lsl	x8, x2, x1
 128:	cset	x1, ne  // ne = any
 12c:	orr	x8, x8, x4
 130:	lsr	x2, x2, x0
 134:	orr	x8, x8, x1
 138:	orr	x5, x8, x2
 13c:	cbz	x5, 2c4 <__addtf3+0x2c4>
 140:	and	x3, x8, #0x7
 144:	mov	x1, #0x0                   	// #0
 148:	mov	w7, #0x1                   	// #1
 14c:	cbz	x3, 508 <__addtf3+0x508>
 150:	and	x3, x15, #0xc00000
 154:	cmp	x3, #0x400, lsl #12
 158:	b.eq	44c <__addtf3+0x44c>  // b.none
 15c:	cmp	x3, #0x800, lsl #12
 160:	b.eq	42c <__addtf3+0x42c>  // b.none
 164:	cbz	x3, 458 <__addtf3+0x458>
 168:	and	x3, x2, #0x8000000000000
 16c:	mov	w0, #0x10                  	// #16
 170:	cbz	w7, 178 <__addtf3+0x178>
 174:	orr	w0, w0, #0x8
 178:	cbz	x3, 490 <__addtf3+0x490>
 17c:	add	x1, x1, #0x1
 180:	mov	x3, #0x7fff                	// #32767
 184:	cmp	x1, x3
 188:	b.eq	374 <__addtf3+0x374>  // b.none
 18c:	ubfx	x5, x2, #3, #48
 190:	extr	x8, x2, x8, #3
 194:	and	w1, w1, #0x7fff
 198:	mov	x7, #0x0                   	// #0
 19c:	orr	w1, w1, w10, lsl #15
 1a0:	bfxil	x7, x5, #0, #48
 1a4:	fmov	d0, x8
 1a8:	bfi	x7, x1, #48, #16
 1ac:	fmov	v0.d[1], x7
 1b0:	cbnz	w0, 3d0 <__addtf3+0x3d0>
 1b4:	ldp	x29, x30, [sp], #48
 1b8:	ret
 1bc:	mov	x14, x5
 1c0:	b.eq	2dc <__addtf3+0x2dc>  // b.none
 1c4:	cbnz	x7, 560 <__addtf3+0x560>
 1c8:	orr	x1, x2, x8
 1cc:	cbz	x1, 28c <__addtf3+0x28c>
 1d0:	cmn	w0, #0x1
 1d4:	b.eq	97c <__addtf3+0x97c>  // b.none
 1d8:	mov	x1, #0x7fff                	// #32767
 1dc:	mvn	w0, w0
 1e0:	cmp	x3, x1
 1e4:	b.ne	574 <__addtf3+0x574>  // b.any
 1e8:	orr	x0, x9, x13
 1ec:	cbnz	x0, 8d4 <__addtf3+0x8d4>
 1f0:	mov	x16, x14
 1f4:	nop
 1f8:	mov	x6, #0x0                   	// #0
 1fc:	fmov	d0, x6
 200:	lsl	x16, x16, #63
 204:	orr	x7, x16, #0x7fff000000000000
 208:	fmov	v0.d[1], x7
 20c:	b	1b4 <__addtf3+0x1b4>
 210:	sub	w7, w7, w0
 214:	cmp	w7, #0x0
 218:	b.le	3e4 <__addtf3+0x3e4>
 21c:	cbz	x0, 324 <__addtf3+0x324>
 220:	orr	x9, x9, #0x8000000000000
 224:	mov	x0, #0x7fff                	// #32767
 228:	cmp	x1, x0
 22c:	b.eq	474 <__addtf3+0x474>  // b.none
 230:	cmp	w7, #0x74
 234:	b.gt	608 <__addtf3+0x608>
 238:	cmp	w7, #0x3f
 23c:	b.gt	708 <__addtf3+0x708>
 240:	mov	w0, #0x40                  	// #64
 244:	sub	w0, w0, w7
 248:	lsr	x5, x13, x7
 24c:	lsl	x13, x13, x0
 250:	cmp	x13, #0x0
 254:	lsl	x3, x9, x0
 258:	cset	x4, ne  // ne = any
 25c:	orr	x3, x3, x5
 260:	lsr	x0, x9, x7
 264:	orr	x3, x3, x4
 268:	add	x2, x2, x0
 26c:	b	614 <__addtf3+0x614>
 270:	orr	x3, x9, x13
 274:	cbz	x3, 5e4 <__addtf3+0x5e4>
 278:	subs	w0, w0, #0x1
 27c:	b.ne	7c <__addtf3+0x7c>  // b.any
 280:	subs	x8, x8, x13
 284:	sbc	x2, x2, x9
 288:	b	cc <__addtf3+0xcc>
 28c:	mov	x0, #0x7fff                	// #32767
 290:	cmp	x3, x0
 294:	b.eq	9c8 <__addtf3+0x9c8>  // b.none
 298:	mov	w10, w6
 29c:	mov	x2, x9
 2a0:	mov	x8, x13
 2a4:	mov	x1, x3
 2a8:	mov	x12, x5
 2ac:	nop
 2b0:	orr	x5, x8, x2
 2b4:	and	x3, x8, #0x7
 2b8:	mov	w7, #0x0                   	// #0
 2bc:	cbnz	x1, 14c <__addtf3+0x14c>
 2c0:	cbnz	x5, 140 <__addtf3+0x140>
 2c4:	mov	x8, #0x0                   	// #0
 2c8:	mov	x1, #0x0                   	// #0
 2cc:	mov	w0, #0x0                   	// #0
 2d0:	and	x5, x5, #0xffffffffffff
 2d4:	and	w1, w1, #0x7fff
 2d8:	b	198 <__addtf3+0x198>
 2dc:	add	x5, x7, #0x1
 2e0:	tst	x5, #0x7ffe
 2e4:	b.ne	5b4 <__addtf3+0x5b4>  // b.any
 2e8:	orr	x7, x2, x8
 2ec:	orr	x5, x9, x13
 2f0:	cbnz	x1, 774 <__addtf3+0x774>
 2f4:	cbz	x7, 81c <__addtf3+0x81c>
 2f8:	cbz	x5, 830 <__addtf3+0x830>
 2fc:	subs	x4, x8, x13
 300:	cmp	x8, x13
 304:	sbc	x3, x2, x9
 308:	tbz	x3, #51, 9fc <__addtf3+0x9fc>
 30c:	subs	x8, x13, x8
 310:	mov	w10, w6
 314:	sbc	x2, x9, x2
 318:	mov	x12, x14
 31c:	orr	x5, x8, x2
 320:	b	13c <__addtf3+0x13c>
 324:	orr	x0, x9, x13
 328:	cbz	x0, 7fc <__addtf3+0x7fc>
 32c:	subs	w7, w7, #0x1
 330:	b.ne	224 <__addtf3+0x224>  // b.any
 334:	adds	x8, x8, x13
 338:	adc	x2, x9, x2
 33c:	nop
 340:	tbz	x2, #51, 2b0 <__addtf3+0x2b0>
 344:	add	x1, x1, #0x1
 348:	mov	x0, #0x7fff                	// #32767
 34c:	cmp	x1, x0
 350:	b.eq	83c <__addtf3+0x83c>  // b.none
 354:	and	x0, x8, #0x1
 358:	and	x3, x2, #0xfff7ffffffffffff
 35c:	orr	x8, x0, x8, lsr #1
 360:	mov	w7, #0x0                   	// #0
 364:	orr	x8, x8, x2, lsl #63
 368:	lsr	x2, x3, #1
 36c:	and	x3, x8, #0x7
 370:	b	14c <__addtf3+0x14c>
 374:	and	x3, x15, #0xc00000
 378:	cbz	x3, 3b0 <__addtf3+0x3b0>
 37c:	cmp	x3, #0x400, lsl #12
 380:	b.eq	3a8 <__addtf3+0x3a8>  // b.none
 384:	cmp	x3, #0x800, lsl #12
 388:	csel	w12, w12, wzr, eq  // eq = none
 38c:	cbnz	w12, 3b0 <__addtf3+0x3b0>
 390:	mov	w1, #0x14                  	// #20
 394:	mov	x8, #0xffffffffffffffff    	// #-1
 398:	orr	w0, w0, w1
 39c:	mov	x5, #0x1fffffffffffffff    	// #2305843009213693951
 3a0:	mov	x1, #0x7ffe                	// #32766
 3a4:	b	2d0 <__addtf3+0x2d0>
 3a8:	cbnz	x12, 390 <__addtf3+0x390>
 3ac:	nop
 3b0:	mov	w1, #0x14                  	// #20
 3b4:	and	x16, x10, #0xff
 3b8:	orr	w0, w0, w1
 3bc:	mov	x6, #0x0                   	// #0
 3c0:	fmov	d0, x6
 3c4:	lsl	x16, x16, #63
 3c8:	orr	x7, x16, #0x7fff000000000000
 3cc:	fmov	v0.d[1], x7
 3d0:	str	q0, [sp, #16]
 3d4:	bl	0 <__sfp_handle_exceptions>
 3d8:	ldr	q0, [sp, #16]
 3dc:	ldp	x29, x30, [sp], #48
 3e0:	ret
 3e4:	b.eq	524 <__addtf3+0x524>  // b.none
 3e8:	cbnz	x1, 6a8 <__addtf3+0x6a8>
 3ec:	orr	x0, x2, x8
 3f0:	cbz	x0, 914 <__addtf3+0x914>
 3f4:	cmn	w7, #0x1
 3f8:	b.eq	a74 <__addtf3+0xa74>  // b.none
 3fc:	mov	x0, #0x7fff                	// #32767
 400:	mvn	w7, w7
 404:	cmp	x3, x0
 408:	b.ne	6bc <__addtf3+0x6bc>  // b.any
 40c:	orr	x0, x9, x13
 410:	cbz	x0, 1f8 <__addtf3+0x1f8>
 414:	lsr	x7, x9, #50
 418:	mov	x8, x13
 41c:	eor	x7, x7, #0x1
 420:	mov	x2, x9
 424:	and	w7, w7, #0x1
 428:	b	488 <__addtf3+0x488>
 42c:	mov	w0, #0x10                  	// #16
 430:	cbz	x12, 43c <__addtf3+0x43c>
 434:	adds	x8, x8, #0x8
 438:	cinc	x2, x2, cs  // cs = hs, nlast
 43c:	and	x3, x2, #0x8000000000000
 440:	cbz	w7, 178 <__addtf3+0x178>
 444:	orr	w0, w0, #0x8
 448:	b	178 <__addtf3+0x178>
 44c:	mov	w0, #0x10                  	// #16
 450:	cbnz	x12, 43c <__addtf3+0x43c>
 454:	b	434 <__addtf3+0x434>
 458:	and	x3, x8, #0xf
 45c:	mov	w0, #0x10                  	// #16
 460:	cmp	x3, #0x4
 464:	b.eq	43c <__addtf3+0x43c>  // b.none
 468:	adds	x8, x8, #0x4
 46c:	cinc	x2, x2, cs  // cs = hs, nlast
 470:	b	43c <__addtf3+0x43c>
 474:	orr	x0, x2, x8
 478:	cbz	x0, 1f8 <__addtf3+0x1f8>
 47c:	lsr	x7, x2, #50
 480:	eor	x7, x7, #0x1
 484:	and	w7, w7, #0x1
 488:	mov	w0, w7
 48c:	mov	x1, #0x7fff                	// #32767
 490:	lsr	x5, x2, #3
 494:	extr	x8, x2, x8, #3
 498:	mov	x2, #0x7fff                	// #32767
 49c:	cmp	x1, x2
 4a0:	b.ne	2d0 <__addtf3+0x2d0>  // b.any
 4a4:	orr	x1, x5, x8
 4a8:	cbz	x1, b14 <__addtf3+0xb14>
 4ac:	orr	x5, x5, #0x800000000000
 4b0:	mov	w1, #0x7fff                	// #32767
 4b4:	and	x5, x5, #0xffffffffffff
 4b8:	b	198 <__addtf3+0x198>
 4bc:	mov	x8, x3
 4c0:	and	x2, x2, #0xfff7ffffffffffff
 4c4:	sub	x1, x1, x4
 4c8:	orr	x5, x8, x2
 4cc:	and	x3, x8, #0x7
 4d0:	mov	w7, #0x0                   	// #0
 4d4:	cbz	x1, 2c0 <__addtf3+0x2c0>
 4d8:	b	14c <__addtf3+0x14c>
 4dc:	clz	x2, x8
 4e0:	add	w0, w2, #0x34
 4e4:	cmp	w0, #0x3f
 4e8:	b.le	e0 <__addtf3+0xe0>
 4ec:	sub	w2, w2, #0xc
 4f0:	lsl	x2, x8, x2
 4f4:	b	f4 <__addtf3+0xf4>
 4f8:	orr	x0, x9, x13
 4fc:	cmp	x0, #0x0
 500:	cset	x3, ne  // ne = any
 504:	b	c4 <__addtf3+0xc4>
 508:	and	x3, x2, #0x8000000000000
 50c:	mov	w0, #0x0                   	// #0
 510:	cbz	w7, 178 <__addtf3+0x178>
 514:	mov	w0, #0x0                   	// #0
 518:	tbz	w15, #11, 178 <__addtf3+0x178>
 51c:	orr	w0, w0, #0x8
 520:	b	178 <__addtf3+0x178>
 524:	add	x0, x1, #0x1
 528:	tst	x0, #0x7ffe
 52c:	b.ne	734 <__addtf3+0x734>  // b.any
 530:	orr	x14, x2, x8
 534:	cbnz	x1, 8f0 <__addtf3+0x8f0>
 538:	orr	x5, x9, x13
 53c:	cbz	x14, 944 <__addtf3+0x944>
 540:	cbz	x5, 830 <__addtf3+0x830>
 544:	adds	x8, x8, x13
 548:	adc	x2, x9, x2
 54c:	tbz	x2, #51, 31c <__addtf3+0x31c>
 550:	and	x2, x2, #0xfff7ffffffffffff
 554:	and	x3, x8, #0x7
 558:	mov	x1, #0x1                   	// #1
 55c:	b	14c <__addtf3+0x14c>
 560:	mov	x1, #0x7fff                	// #32767
 564:	neg	w0, w0
 568:	orr	x2, x2, #0x8000000000000
 56c:	cmp	x3, x1
 570:	b.eq	1e8 <__addtf3+0x1e8>  // b.none
 574:	cmp	w0, #0x74
 578:	b.gt	684 <__addtf3+0x684>
 57c:	cmp	w0, #0x3f
 580:	b.gt	8a0 <__addtf3+0x8a0>
 584:	mov	w1, #0x40                  	// #64
 588:	sub	w1, w1, w0
 58c:	lsr	x4, x8, x0
 590:	lsl	x8, x8, x1
 594:	cmp	x8, #0x0
 598:	lsl	x8, x2, x1
 59c:	cset	x1, ne  // ne = any
 5a0:	orr	x8, x8, x4
 5a4:	lsr	x0, x2, x0
 5a8:	orr	x8, x8, x1
 5ac:	sub	x9, x9, x0
 5b0:	b	690 <__addtf3+0x690>
 5b4:	subs	x4, x8, x13
 5b8:	cmp	x8, x13
 5bc:	sbc	x3, x2, x9
 5c0:	tbnz	x3, #51, 75c <__addtf3+0x75c>
 5c4:	orr	x5, x4, x3
 5c8:	cbnz	x5, 888 <__addtf3+0x888>
 5cc:	and	x15, x15, #0xc00000
 5d0:	mov	x8, #0x0                   	// #0
 5d4:	cmp	x15, #0x800, lsl #12
 5d8:	mov	x1, #0x0                   	// #0
 5dc:	cset	w10, eq  // eq = none
 5e0:	b	2d0 <__addtf3+0x2d0>
 5e4:	mov	x0, #0x7fff                	// #32767
 5e8:	cmp	x7, x0
 5ec:	b.ne	2b0 <__addtf3+0x2b0>  // b.any
 5f0:	orr	x0, x2, x8
 5f4:	cbnz	x0, 47c <__addtf3+0x47c>
 5f8:	mov	x8, #0x0                   	// #0
 5fc:	mov	x5, #0x0                   	// #0
 600:	mov	w0, #0x0                   	// #0
 604:	b	4a4 <__addtf3+0x4a4>
 608:	orr	x0, x9, x13
 60c:	cmp	x0, #0x0
 610:	cset	x3, ne  // ne = any
 614:	adds	x8, x3, x8
 618:	cinc	x2, x2, cs  // cs = hs, nlast
 61c:	b	340 <__addtf3+0x340>
 620:	mov	w4, #0x80                  	// #128
 624:	sub	w4, w4, w0
 628:	cmp	w0, #0x40
 62c:	sub	w8, w1, #0x3f
 630:	lsl	x0, x2, x4
 634:	orr	x0, x3, x0
 638:	csel	x3, x0, x3, ne  // ne = any
 63c:	lsr	x8, x2, x8
 640:	cmp	x3, #0x0
 644:	mov	x2, #0x0                   	// #0
 648:	cset	x0, ne  // ne = any
 64c:	orr	x8, x0, x8
 650:	mov	x5, x8
 654:	b	13c <__addtf3+0x13c>
 658:	mov	w4, #0x80                  	// #128
 65c:	sub	w4, w4, w0
 660:	subs	w0, w0, #0x40
 664:	lsl	x4, x9, x4
 668:	orr	x4, x13, x4
 66c:	csel	x13, x4, x13, ne  // ne = any
 670:	lsr	x0, x9, x0
 674:	cmp	x13, #0x0
 678:	cset	x3, ne  // ne = any
 67c:	orr	x3, x3, x0
 680:	b	c4 <__addtf3+0xc4>
 684:	orr	x2, x2, x8
 688:	cmp	x2, #0x0
 68c:	cset	x8, ne  // ne = any
 690:	subs	x8, x13, x8
 694:	mov	w10, w6
 698:	sbc	x2, x9, xzr
 69c:	mov	x1, x3
 6a0:	mov	x12, x14
 6a4:	b	cc <__addtf3+0xcc>
 6a8:	mov	x0, #0x7fff                	// #32767
 6ac:	neg	w7, w7
 6b0:	orr	x2, x2, #0x8000000000000
 6b4:	cmp	x3, x0
 6b8:	b.eq	40c <__addtf3+0x40c>  // b.none
 6bc:	cmp	w7, #0x74
 6c0:	b.gt	890 <__addtf3+0x890>
 6c4:	cmp	w7, #0x3f
 6c8:	b.gt	950 <__addtf3+0x950>
 6cc:	mov	w1, #0x40                  	// #64
 6d0:	sub	w1, w1, w7
 6d4:	lsr	x4, x8, x7
 6d8:	lsl	x8, x8, x1
 6dc:	cmp	x8, #0x0
 6e0:	cset	x0, ne  // ne = any
 6e4:	lsl	x8, x2, x1
 6e8:	orr	x8, x8, x4
 6ec:	lsr	x7, x2, x7
 6f0:	orr	x8, x8, x0
 6f4:	add	x9, x9, x7
 6f8:	adds	x8, x8, x13
 6fc:	mov	x1, x3
 700:	cinc	x2, x9, cs  // cs = hs, nlast
 704:	b	340 <__addtf3+0x340>
 708:	mov	w3, #0x80                  	// #128
 70c:	sub	w3, w3, w7
 710:	subs	w0, w7, #0x40
 714:	lsl	x3, x9, x3
 718:	orr	x3, x13, x3
 71c:	csel	x13, x3, x13, ne  // ne = any
 720:	lsr	x0, x9, x0
 724:	cmp	x13, #0x0
 728:	cset	x3, ne  // ne = any
 72c:	orr	x3, x3, x0
 730:	b	614 <__addtf3+0x614>
 734:	mov	x1, #0x7fff                	// #32767
 738:	cmp	x0, x1
 73c:	b.eq	998 <__addtf3+0x998>  // b.none
 740:	adds	x8, x8, x13
 744:	mov	x1, x0
 748:	adc	x2, x9, x2
 74c:	ubfx	x3, x8, #1, #3
 750:	extr	x8, x2, x8, #1
 754:	lsr	x2, x2, #1
 758:	b	14c <__addtf3+0x14c>
 75c:	cmp	x13, x8
 760:	mov	w10, w6
 764:	sbc	x3, x9, x2
 768:	sub	x8, x13, x8
 76c:	mov	x12, x14
 770:	b	d4 <__addtf3+0xd4>
 774:	mov	x12, #0x7fff                	// #32767
 778:	cmp	x1, x12
 77c:	b.eq	7a8 <__addtf3+0x7a8>  // b.none
 780:	cmp	x3, x12
 784:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 788:	cbnz	x7, 7c0 <__addtf3+0x7c0>
 78c:	mov	w7, w0
 790:	cbnz	x5, ab8 <__addtf3+0xab8>
 794:	mov	x8, #0xffffffffffffffff    	// #-1
 798:	mov	x5, #0xffffffffffff        	// #281474976710655
 79c:	mov	w0, #0x1                   	// #1
 7a0:	mov	w10, #0x0                   	// #0
 7a4:	b	4ac <__addtf3+0x4ac>
 7a8:	cbz	x7, ad4 <__addtf3+0xad4>
 7ac:	lsr	x0, x2, #50
 7b0:	cmp	x3, x1
 7b4:	eor	x0, x0, #0x1
 7b8:	and	w0, w0, #0x1
 7bc:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 7c0:	cbz	x5, 9f4 <__addtf3+0x9f4>
 7c4:	bfi	x4, x2, #61, #3
 7c8:	lsr	x5, x2, #3
 7cc:	mov	x8, x4
 7d0:	tbz	x2, #50, 7ec <__addtf3+0x7ec>
 7d4:	lsr	x1, x9, #3
 7d8:	tbnz	x9, #50, 7ec <__addtf3+0x7ec>
 7dc:	mov	x8, x11
 7e0:	mov	w10, w6
 7e4:	bfi	x8, x9, #61, #3
 7e8:	mov	x5, x1
 7ec:	extr	x5, x5, x8, #61
 7f0:	bfi	x8, x5, #61, #3
 7f4:	lsr	x5, x5, #3
 7f8:	b	4a4 <__addtf3+0x4a4>
 7fc:	mov	x0, #0x7fff                	// #32767
 800:	cmp	x1, x0
 804:	b.ne	2b0 <__addtf3+0x2b0>  // b.any
 808:	orr	x0, x2, x8
 80c:	cbz	x0, 5f8 <__addtf3+0x5f8>
 810:	lsr	x7, x2, #50
 814:	eor	w7, w7, #0x1
 818:	b	488 <__addtf3+0x488>
 81c:	cbz	x5, 930 <__addtf3+0x930>
 820:	mov	w10, w6
 824:	mov	x2, x9
 828:	mov	x8, x13
 82c:	mov	x12, x14
 830:	mov	x1, #0x0                   	// #0
 834:	mov	x3, #0x0                   	// #0
 838:	b	514 <__addtf3+0x514>
 83c:	ands	x3, x15, #0xc00000
 840:	b.eq	8cc <__addtf3+0x8cc>  // b.none
 844:	cmp	x3, #0x400, lsl #12
 848:	eor	w0, w10, #0x1
 84c:	cset	w1, eq  // eq = none
 850:	tst	w1, w0
 854:	b.ne	af0 <__addtf3+0xaf0>  // b.any
 858:	cmp	x3, #0x800, lsl #12
 85c:	b.eq	a9c <__addtf3+0xa9c>  // b.none
 860:	cmp	x3, #0x400, lsl #12
 864:	mov	w0, #0x14                  	// #20
 868:	b.ne	378 <__addtf3+0x378>  // b.any
 86c:	mov	x2, #0xffffffffffffffff    	// #-1
 870:	mov	x1, #0x7ffe                	// #32766
 874:	mov	x8, x2
 878:	mov	w7, #0x0                   	// #0
 87c:	mov	w0, #0x14                  	// #20
 880:	cbnz	x12, 43c <__addtf3+0x43c>
 884:	b	434 <__addtf3+0x434>
 888:	mov	x8, x4
 88c:	b	d4 <__addtf3+0xd4>
 890:	orr	x2, x2, x8
 894:	cmp	x2, #0x0
 898:	cset	x8, ne  // ne = any
 89c:	b	6f8 <__addtf3+0x6f8>
 8a0:	mov	w1, #0x80                  	// #128
 8a4:	sub	w1, w1, w0
 8a8:	subs	w0, w0, #0x40
 8ac:	lsl	x1, x2, x1
 8b0:	orr	x1, x8, x1
 8b4:	csel	x8, x1, x8, ne  // ne = any
 8b8:	lsr	x2, x2, x0
 8bc:	cmp	x8, #0x0
 8c0:	cset	x8, ne  // ne = any
 8c4:	orr	x8, x8, x2
 8c8:	b	690 <__addtf3+0x690>
 8cc:	mov	w0, #0x14                  	// #20
 8d0:	b	3bc <__addtf3+0x3bc>
 8d4:	lsr	x7, x9, #50
 8d8:	mov	w10, w6
 8dc:	eor	x7, x7, #0x1
 8e0:	mov	x8, x13
 8e4:	and	w7, w7, #0x1
 8e8:	mov	x2, x9
 8ec:	b	488 <__addtf3+0x488>
 8f0:	mov	x0, #0x7fff                	// #32767
 8f4:	cmp	x1, x0
 8f8:	b.eq	a18 <__addtf3+0xa18>  // b.none
 8fc:	cmp	x3, x0
 900:	b.eq	a8c <__addtf3+0xa8c>  // b.none
 904:	cbnz	x14, a30 <__addtf3+0xa30>
 908:	mov	x2, x9
 90c:	mov	x8, x13
 910:	b	488 <__addtf3+0x488>
 914:	mov	x0, #0x7fff                	// #32767
 918:	cmp	x3, x0
 91c:	b.eq	ac8 <__addtf3+0xac8>  // b.none
 920:	mov	x2, x9
 924:	mov	x8, x13
 928:	mov	x1, x3
 92c:	b	2b0 <__addtf3+0x2b0>
 930:	and	x15, x15, #0xc00000
 934:	mov	x8, #0x0                   	// #0
 938:	cmp	x15, #0x800, lsl #12
 93c:	cset	w10, eq  // eq = none
 940:	b	2d0 <__addtf3+0x2d0>
 944:	mov	x2, x9
 948:	mov	x8, x13
 94c:	b	13c <__addtf3+0x13c>
 950:	mov	w0, #0x80                  	// #128
 954:	sub	w0, w0, w7
 958:	subs	w7, w7, #0x40
 95c:	lsl	x0, x2, x0
 960:	orr	x0, x8, x0
 964:	csel	x8, x0, x8, ne  // ne = any
 968:	lsr	x2, x2, x7
 96c:	cmp	x8, #0x0
 970:	cset	x8, ne  // ne = any
 974:	orr	x8, x8, x2
 978:	b	6f8 <__addtf3+0x6f8>
 97c:	cmp	x13, x8
 980:	mov	w10, w6
 984:	sbc	x2, x9, x2
 988:	sub	x8, x13, x8
 98c:	mov	x1, x3
 990:	mov	x12, x5
 994:	b	cc <__addtf3+0xcc>
 998:	ands	x3, x15, #0xc00000
 99c:	b.eq	8cc <__addtf3+0x8cc>  // b.none
 9a0:	cmp	x3, #0x400, lsl #12
 9a4:	eor	w0, w10, #0x1
 9a8:	csel	w0, w0, wzr, eq  // eq = none
 9ac:	cbnz	w0, af0 <__addtf3+0xaf0>
 9b0:	cmp	x3, #0x800, lsl #12
 9b4:	b.ne	860 <__addtf3+0x860>  // b.any
 9b8:	cbz	x12, aa0 <__addtf3+0xaa0>
 9bc:	mov	w0, #0x14                  	// #20
 9c0:	mov	x16, #0x1                   	// #1
 9c4:	b	3bc <__addtf3+0x3bc>
 9c8:	orr	x0, x9, x13
 9cc:	cbnz	x0, 8d4 <__addtf3+0x8d4>
 9d0:	mov	w10, w6
 9d4:	b	5f8 <__addtf3+0x5f8>
 9d8:	cbz	x5, ae4 <__addtf3+0xae4>
 9dc:	tst	x9, #0x4000000000000
 9e0:	csinc	w0, w0, wzr, ne  // ne = any
 9e4:	cbnz	x7, 7c4 <__addtf3+0x7c4>
 9e8:	mov	w10, w6
 9ec:	mov	x2, x9
 9f0:	mov	x8, x13
 9f4:	mov	w7, w0
 9f8:	b	488 <__addtf3+0x488>
 9fc:	orr	x5, x4, x3
 a00:	cbz	x5, 930 <__addtf3+0x930>
 a04:	mov	x2, x3
 a08:	mov	x8, x4
 a0c:	and	x3, x4, #0x7
 a10:	mov	w7, #0x1                   	// #1
 a14:	b	14c <__addtf3+0x14c>
 a18:	cbz	x14, a84 <__addtf3+0xa84>
 a1c:	lsr	x7, x2, #50
 a20:	cmp	x3, x1
 a24:	eor	x7, x7, #0x1
 a28:	and	w7, w7, #0x1
 a2c:	b.eq	afc <__addtf3+0xafc>  // b.none
 a30:	orr	x13, x9, x13
 a34:	cbz	x13, 488 <__addtf3+0x488>
 a38:	bfi	x4, x2, #61, #3
 a3c:	lsr	x5, x2, #3
 a40:	mov	x8, x4
 a44:	tbz	x2, #50, a60 <__addtf3+0xa60>
 a48:	lsr	x0, x9, #3
 a4c:	tbnz	x9, #50, a60 <__addtf3+0xa60>
 a50:	and	x8, x11, #0x1fffffffffffffff
 a54:	mov	w10, w6
 a58:	orr	x8, x8, x9, lsl #61
 a5c:	mov	x5, x0
 a60:	mov	w0, w7
 a64:	extr	x5, x5, x8, #61
 a68:	bfi	x8, x5, #61, #3
 a6c:	lsr	x5, x5, #3
 a70:	b	4a4 <__addtf3+0x4a4>
 a74:	adds	x8, x8, x13
 a78:	mov	x1, x3
 a7c:	adc	x2, x9, x2
 a80:	b	340 <__addtf3+0x340>
 a84:	cmp	x3, x1
 a88:	b.ne	908 <__addtf3+0x908>  // b.any
 a8c:	orr	x0, x9, x13
 a90:	cbnz	x0, b04 <__addtf3+0xb04>
 a94:	cbz	x14, 5f8 <__addtf3+0x5f8>
 a98:	b	488 <__addtf3+0x488>
 a9c:	cbnz	x16, 9bc <__addtf3+0x9bc>
 aa0:	mov	x2, #0xffffffffffffffff    	// #-1
 aa4:	mov	w10, #0x0                   	// #0
 aa8:	mov	x8, x2
 aac:	mov	x1, #0x7ffe                	// #32766
 ab0:	mov	w0, #0x14                  	// #20
 ab4:	b	17c <__addtf3+0x17c>
 ab8:	mov	w10, w6
 abc:	mov	x2, x9
 ac0:	mov	x8, x13
 ac4:	b	488 <__addtf3+0x488>
 ac8:	orr	x0, x9, x13
 acc:	cbz	x0, 5f8 <__addtf3+0x5f8>
 ad0:	b	414 <__addtf3+0x414>
 ad4:	cmp	x3, x1
 ad8:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 adc:	mov	w7, #0x0                   	// #0
 ae0:	b	790 <__addtf3+0x790>
 ae4:	cbnz	x7, 9f4 <__addtf3+0x9f4>
 ae8:	mov	w7, w0
 aec:	b	790 <__addtf3+0x790>
 af0:	mov	w0, #0x14                  	// #20
 af4:	mov	x16, #0x0                   	// #0
 af8:	b	3bc <__addtf3+0x3bc>
 afc:	orr	x0, x9, x13
 b00:	cbz	x0, 488 <__addtf3+0x488>
 b04:	tst	x9, #0x4000000000000
 b08:	csinc	w7, w7, wzr, ne  // ne = any
 b0c:	cbnz	x14, a38 <__addtf3+0xa38>
 b10:	b	908 <__addtf3+0x908>
 b14:	mov	x8, #0x0                   	// #0
 b18:	mov	w1, #0x7fff                	// #32767
 b1c:	mov	x5, #0x0                   	// #0
 b20:	b	198 <__addtf3+0x198>

divtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x2, x0, [sp, #16]
  14:	ldp	x5, x3, [sp, #32]
  18:	mrs	x11, fpcr
  1c:	lsr	x1, x0, #63
  20:	ubfx	x6, x0, #0, #48
  24:	and	w13, w1, #0xff
  28:	mov	x9, x1
  2c:	ubfx	x7, x0, #48, #15
  30:	cbz	w7, 448 <__divtf3+0x448>
  34:	mov	w1, #0x7fff                	// #32767
  38:	cmp	w7, w1
  3c:	b.eq	488 <__divtf3+0x488>  // b.none
  40:	and	x7, x7, #0xffff
  44:	extr	x6, x6, x2, #61
  48:	mov	x15, #0xffffffffffffc001    	// #-16383
  4c:	orr	x4, x6, #0x8000000000000
  50:	add	x7, x7, x15
  54:	lsl	x2, x2, #3
  58:	mov	x14, #0x2                   	// #2
  5c:	mov	x12, #0x1                   	// #1
  60:	mov	x1, #0x3                   	// #3
  64:	mov	x16, #0x0                   	// #0
  68:	mov	x17, #0x0                   	// #0
  6c:	mov	w0, #0x0                   	// #0
  70:	lsr	x8, x3, #63
  74:	ubfx	x6, x3, #0, #48
  78:	and	w15, w8, #0xff
  7c:	ubfx	x10, x3, #48, #15
  80:	cbz	w10, 400 <__divtf3+0x400>
  84:	mov	w12, #0x7fff                	// #32767
  88:	cmp	w10, w12
  8c:	b.eq	3cc <__divtf3+0x3cc>  // b.none
  90:	and	x10, x10, #0xffff
  94:	extr	x6, x6, x5, #61
  98:	mov	x14, #0xffffffffffffc001    	// #-16383
  9c:	add	x10, x10, x14
  a0:	orr	x6, x6, #0x8000000000000
  a4:	sub	x7, x7, x10
  a8:	lsl	x5, x5, #3
  ac:	mov	x1, x16
  b0:	mov	x3, #0x0                   	// #0
  b4:	eor	w10, w13, w15
  b8:	cmp	x1, #0x9
  bc:	and	x12, x10, #0xff
  c0:	mov	x14, x12
  c4:	b.gt	18c <__divtf3+0x18c>
  c8:	cmp	x1, #0x7
  cc:	b.gt	544 <__divtf3+0x544>
  d0:	cmp	x1, #0x3
  d4:	b.eq	f0 <__divtf3+0xf0>  // b.none
  d8:	b.le	1b4 <__divtf3+0x1b4>
  dc:	cmp	x1, #0x5
  e0:	b.eq	19c <__divtf3+0x19c>  // b.none
  e4:	b.le	1e4 <__divtf3+0x1e4>
  e8:	cmp	x1, #0x6
  ec:	b.eq	158 <__divtf3+0x158>  // b.none
  f0:	cmp	x3, #0x1
  f4:	b.eq	154 <__divtf3+0x154>  // b.none
  f8:	cbz	x3, 10c <__divtf3+0x10c>
  fc:	cmp	x3, #0x2
 100:	b.eq	540 <__divtf3+0x540>  // b.none
 104:	cmp	x3, #0x3
 108:	b.eq	738 <__divtf3+0x738>  // b.none
 10c:	mov	x1, #0x3fff                	// #16383
 110:	mov	w10, w15
 114:	mov	x14, x8
 118:	add	x3, x7, x1
 11c:	cmp	x3, #0x0
 120:	b.le	610 <__divtf3+0x610>
 124:	tst	x5, #0x7
 128:	b.ne	570 <__divtf3+0x570>  // b.any
 12c:	tbz	x6, #52, 138 <__divtf3+0x138>
 130:	and	x6, x6, #0xffefffffffffffff
 134:	add	x3, x7, #0x4, lsl #12
 138:	mov	x1, #0x7ffe                	// #32766
 13c:	cmp	x3, x1
 140:	b.gt	6f4 <__divtf3+0x6f4>
 144:	and	w1, w3, #0x7fff
 148:	extr	x2, x6, x5, #3
 14c:	ubfx	x6, x6, #3, #48
 150:	b	164 <__divtf3+0x164>
 154:	mov	w10, w15
 158:	mov	w1, #0x0                   	// #0
 15c:	mov	x6, #0x0                   	// #0
 160:	mov	x2, #0x0                   	// #0
 164:	mov	x5, #0x0                   	// #0
 168:	orr	w1, w1, w10, lsl #15
 16c:	bfxil	x5, x6, #0, #48
 170:	fmov	d0, x2
 174:	bfi	x5, x1, #48, #16
 178:	fmov	v0.d[1], x5
 17c:	cbnz	w0, 1d4 <__divtf3+0x1d4>
 180:	ldp	x29, x30, [sp], #48
 184:	ret
 188:	mov	x3, #0x3                   	// #3
 18c:	cmp	x1, #0xb
 190:	b.gt	4b4 <__divtf3+0x4b4>
 194:	cmp	x1, #0xa
 198:	b.ne	f0 <__divtf3+0xf0>  // b.any
 19c:	mov	w10, #0x0                   	// #0
 1a0:	mov	x6, #0xffffffffffff        	// #281474976710655
 1a4:	mov	x2, #0xffffffffffffffff    	// #-1
 1a8:	mov	w0, #0x1                   	// #1
 1ac:	mov	w1, #0x7fff                	// #32767
 1b0:	b	164 <__divtf3+0x164>
 1b4:	cmp	x1, #0x1
 1b8:	b.ne	3c0 <__divtf3+0x3c0>  // b.any
 1bc:	mov	x4, #0x0                   	// #0
 1c0:	fmov	d0, x4
 1c4:	lsl	x12, x12, #63
 1c8:	orr	w0, w0, #0x2
 1cc:	orr	x5, x12, #0x7fff000000000000
 1d0:	fmov	v0.d[1], x5
 1d4:	str	q0, [sp, #16]
 1d8:	bl	0 <__sfp_handle_exceptions>
 1dc:	ldr	q0, [sp, #16]
 1e0:	b	180 <__divtf3+0x180>
 1e4:	cmp	x1, #0x4
 1e8:	b.eq	158 <__divtf3+0x158>  // b.none
 1ec:	cmp	x4, x6
 1f0:	b.ls	554 <__divtf3+0x554>  // b.plast
 1f4:	lsr	x3, x4, #1
 1f8:	extr	x8, x4, x2, #1
 1fc:	lsl	x2, x2, #63
 200:	ubfx	x13, x6, #20, #32
 204:	extr	x9, x6, x5, #52
 208:	lsl	x12, x5, #12
 20c:	and	x15, x9, #0xffffffff
 210:	udiv	x5, x3, x13
 214:	msub	x3, x5, x13, x3
 218:	mul	x1, x15, x5
 21c:	extr	x3, x3, x8, #32
 220:	cmp	x1, x3
 224:	b.ls	238 <__divtf3+0x238>  // b.plast
 228:	adds	x3, x9, x3
 22c:	ccmp	x1, x3, #0x0, cc  // cc = lo, ul, last
 230:	b.hi	810 <__divtf3+0x810>  // b.pmore
 234:	sub	x5, x5, #0x1
 238:	sub	x3, x3, x1
 23c:	mov	x4, x8
 240:	udiv	x1, x3, x13
 244:	msub	x3, x1, x13, x3
 248:	mul	x6, x15, x1
 24c:	bfi	x4, x3, #32, #32
 250:	cmp	x6, x4
 254:	b.ls	268 <__divtf3+0x268>  // b.plast
 258:	adds	x4, x9, x4
 25c:	ccmp	x6, x4, #0x0, cc  // cc = lo, ul, last
 260:	b.hi	81c <__divtf3+0x81c>  // b.pmore
 264:	sub	x1, x1, #0x1
 268:	orr	x8, x1, x5, lsl #32
 26c:	and	x17, x12, #0xffffffff
 270:	and	x1, x8, #0xffffffff
 274:	lsr	x16, x12, #32
 278:	lsr	x5, x8, #32
 27c:	sub	x4, x4, x6
 280:	mov	x18, #0x100000000           	// #4294967296
 284:	mul	x3, x1, x17
 288:	mul	x30, x5, x17
 28c:	madd	x6, x16, x1, x30
 290:	and	x1, x3, #0xffffffff
 294:	mul	x5, x5, x16
 298:	add	x3, x6, x3, lsr #32
 29c:	add	x6, x5, x18
 2a0:	cmp	x30, x3
 2a4:	csel	x5, x6, x5, hi  // hi = pmore
 2a8:	add	x1, x1, x3, lsl #32
 2ac:	add	x5, x5, x3, lsr #32
 2b0:	cmp	x4, x5
 2b4:	b.cc	5dc <__divtf3+0x5dc>  // b.lo, b.ul, b.last
 2b8:	ccmp	x2, x1, #0x2, eq  // eq = none
 2bc:	mov	x6, x8
 2c0:	b.cc	5dc <__divtf3+0x5dc>  // b.lo, b.ul, b.last
 2c4:	subs	x8, x2, x1
 2c8:	mov	x3, #0x3fff                	// #16383
 2cc:	cmp	x2, x1
 2d0:	add	x3, x7, x3
 2d4:	sbc	x4, x4, x5
 2d8:	cmp	x9, x4
 2dc:	b.eq	828 <__divtf3+0x828>  // b.none
 2e0:	udiv	x5, x4, x13
 2e4:	msub	x4, x5, x13, x4
 2e8:	mul	x2, x15, x5
 2ec:	extr	x1, x4, x8, #32
 2f0:	cmp	x2, x1
 2f4:	b.ls	308 <__divtf3+0x308>  // b.plast
 2f8:	adds	x1, x9, x1
 2fc:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 300:	b.hi	8e0 <__divtf3+0x8e0>  // b.pmore
 304:	sub	x5, x5, #0x1
 308:	sub	x1, x1, x2
 30c:	udiv	x2, x1, x13
 310:	msub	x1, x2, x13, x1
 314:	mul	x15, x15, x2
 318:	bfi	x8, x1, #32, #32
 31c:	mov	x1, x8
 320:	cmp	x15, x8
 324:	b.ls	338 <__divtf3+0x338>  // b.plast
 328:	adds	x1, x9, x8
 32c:	ccmp	x15, x1, #0x0, cc  // cc = lo, ul, last
 330:	b.hi	8ec <__divtf3+0x8ec>  // b.pmore
 334:	sub	x2, x2, #0x1
 338:	orr	x5, x2, x5, lsl #32
 33c:	sub	x1, x1, x15
 340:	and	x4, x5, #0xffffffff
 344:	mov	x13, #0x100000000           	// #4294967296
 348:	lsr	x15, x5, #32
 34c:	mul	x2, x17, x4
 350:	mul	x17, x15, x17
 354:	madd	x4, x16, x4, x17
 358:	and	x8, x2, #0xffffffff
 35c:	mul	x16, x16, x15
 360:	add	x2, x4, x2, lsr #32
 364:	add	x4, x16, x13
 368:	cmp	x17, x2
 36c:	csel	x16, x4, x16, hi  // hi = pmore
 370:	add	x4, x8, x2, lsl #32
 374:	add	x16, x16, x2, lsr #32
 378:	cmp	x1, x16
 37c:	b.cs	760 <__divtf3+0x760>  // b.hs, b.nlast
 380:	adds	x2, x9, x1
 384:	sub	x8, x5, #0x1
 388:	mov	x1, x2
 38c:	b.cs	3a0 <__divtf3+0x3a0>  // b.hs, b.nlast
 390:	cmp	x2, x16
 394:	b.cc	860 <__divtf3+0x860>  // b.lo, b.ul, b.last
 398:	ccmp	x12, x4, #0x2, eq  // eq = none
 39c:	b.cc	860 <__divtf3+0x860>  // b.lo, b.ul, b.last
 3a0:	cmp	x12, x4
 3a4:	mov	x5, x8
 3a8:	cset	w2, ne  // ne = any
 3ac:	cmp	w2, #0x0
 3b0:	orr	x2, x5, #0x1
 3b4:	ccmp	x1, x16, #0x0, eq  // eq = none
 3b8:	csel	x5, x2, x5, ne  // ne = any
 3bc:	b	11c <__divtf3+0x11c>
 3c0:	cmp	x1, #0x2
 3c4:	b.eq	158 <__divtf3+0x158>  // b.none
 3c8:	b	1ec <__divtf3+0x1ec>
 3cc:	mov	x10, #0xffffffffffff8001    	// #-32767
 3d0:	orr	x3, x6, x5
 3d4:	add	x7, x7, x10
 3d8:	cbz	x3, 52c <__divtf3+0x52c>
 3dc:	eor	w10, w13, w15
 3e0:	ands	x3, x6, #0x800000000000
 3e4:	and	x12, x10, #0xff
 3e8:	csinc	w0, w0, wzr, ne  // ne = any
 3ec:	mov	x14, x12
 3f0:	cmp	x1, #0x9
 3f4:	b.gt	5a4 <__divtf3+0x5a4>
 3f8:	mov	x3, #0x3                   	// #3
 3fc:	b	c8 <__divtf3+0xc8>
 400:	orr	x1, x6, x5
 404:	cbz	x1, 518 <__divtf3+0x518>
 408:	cbz	x6, 6ac <__divtf3+0x6ac>
 40c:	clz	x1, x6
 410:	sub	x3, x1, #0xf
 414:	add	w12, w3, #0x3
 418:	mov	w10, #0x3d                  	// #61
 41c:	sub	w3, w10, w3
 420:	lsl	x6, x6, x12
 424:	lsr	x3, x5, x3
 428:	orr	x6, x3, x6
 42c:	lsl	x5, x5, x12
 430:	add	x7, x1, x7
 434:	mov	x12, #0x3fef                	// #16367
 438:	mov	x1, x16
 43c:	add	x7, x7, x12
 440:	mov	x3, #0x0                   	// #0
 444:	b	b4 <__divtf3+0xb4>
 448:	orr	x4, x6, x2
 44c:	cbz	x4, 4f4 <__divtf3+0x4f4>
 450:	cbz	x6, 6d0 <__divtf3+0x6d0>
 454:	clz	x0, x6
 458:	sub	x4, x0, #0xf
 45c:	add	w7, w4, #0x3
 460:	mov	w1, #0x3d                  	// #61
 464:	sub	w4, w1, w4
 468:	lsl	x6, x6, x7
 46c:	lsr	x4, x2, x4
 470:	orr	x4, x4, x6
 474:	lsl	x2, x2, x7
 478:	mov	x7, #0xffffffffffffc011    	// #-16367
 47c:	mov	x14, #0x2                   	// #2
 480:	sub	x7, x7, x0
 484:	b	5c <__divtf3+0x5c>
 488:	orr	x4, x6, x2
 48c:	cbnz	x4, 4cc <__divtf3+0x4cc>
 490:	mov	x2, #0x0                   	// #0
 494:	mov	x14, #0xa                   	// #10
 498:	mov	x12, #0x9                   	// #9
 49c:	mov	x1, #0xb                   	// #11
 4a0:	mov	x16, #0x8                   	// #8
 4a4:	mov	x7, #0x7fff                	// #32767
 4a8:	mov	x17, #0x2                   	// #2
 4ac:	mov	w0, #0x0                   	// #0
 4b0:	b	70 <__divtf3+0x70>
 4b4:	mov	w15, w13
 4b8:	mov	x6, x4
 4bc:	mov	x5, x2
 4c0:	mov	x8, x9
 4c4:	mov	x3, x17
 4c8:	b	f0 <__divtf3+0xf0>
 4cc:	lsr	x0, x6, #47
 4d0:	mov	x4, x6
 4d4:	eor	w0, w0, #0x1
 4d8:	mov	x14, #0xe                   	// #14
 4dc:	mov	x12, #0xd                   	// #13
 4e0:	mov	x1, #0xf                   	// #15
 4e4:	mov	x16, #0xc                   	// #12
 4e8:	mov	x7, #0x7fff                	// #32767
 4ec:	mov	x17, #0x3                   	// #3
 4f0:	b	70 <__divtf3+0x70>
 4f4:	mov	x2, #0x0                   	// #0
 4f8:	mov	x14, #0x6                   	// #6
 4fc:	mov	x12, #0x5                   	// #5
 500:	mov	x1, #0x7                   	// #7
 504:	mov	x16, #0x4                   	// #4
 508:	mov	x7, #0x0                   	// #0
 50c:	mov	x17, #0x1                   	// #1
 510:	mov	w0, #0x0                   	// #0
 514:	b	70 <__divtf3+0x70>
 518:	mov	x1, x12
 51c:	mov	x6, #0x0                   	// #0
 520:	mov	x5, #0x0                   	// #0
 524:	mov	x3, #0x1                   	// #1
 528:	b	b4 <__divtf3+0xb4>
 52c:	mov	x1, x14
 530:	mov	x6, #0x0                   	// #0
 534:	mov	x5, #0x0                   	// #0
 538:	mov	x3, #0x2                   	// #2
 53c:	b	b4 <__divtf3+0xb4>
 540:	mov	w10, w15
 544:	mov	w1, #0x7fff                	// #32767
 548:	mov	x6, #0x0                   	// #0
 54c:	mov	x2, #0x0                   	// #0
 550:	b	164 <__divtf3+0x164>
 554:	ccmp	x5, x2, #0x2, eq  // eq = none
 558:	b.ls	1f4 <__divtf3+0x1f4>  // b.plast
 55c:	mov	x8, x2
 560:	sub	x7, x7, #0x1
 564:	mov	x3, x4
 568:	mov	x2, #0x0                   	// #0
 56c:	b	200 <__divtf3+0x200>
 570:	and	x1, x11, #0xc00000
 574:	orr	w0, w0, #0x10
 578:	cmp	x1, #0x400, lsl #12
 57c:	b.eq	8ac <__divtf3+0x8ac>  // b.none
 580:	cmp	x1, #0x800, lsl #12
 584:	b.eq	7dc <__divtf3+0x7dc>  // b.none
 588:	cbnz	x1, 12c <__divtf3+0x12c>
 58c:	and	x1, x5, #0xf
 590:	cmp	x1, #0x4
 594:	b.eq	12c <__divtf3+0x12c>  // b.none
 598:	adds	x5, x5, #0x4
 59c:	cinc	x6, x6, cs  // cs = hs, nlast
 5a0:	b	12c <__divtf3+0x12c>
 5a4:	cmp	x1, #0xf
 5a8:	b.ne	188 <__divtf3+0x188>  // b.any
 5ac:	tbz	x4, #47, 5c8 <__divtf3+0x5c8>
 5b0:	cbnz	x3, 5c8 <__divtf3+0x5c8>
 5b4:	orr	x6, x6, #0x800000000000
 5b8:	mov	w10, w15
 5bc:	mov	x2, x5
 5c0:	mov	w1, #0x7fff                	// #32767
 5c4:	b	164 <__divtf3+0x164>
 5c8:	orr	x6, x4, #0x800000000000
 5cc:	mov	w10, w13
 5d0:	and	x6, x6, #0xffffffffffff
 5d4:	mov	w1, #0x7fff                	// #32767
 5d8:	b	164 <__divtf3+0x164>
 5dc:	adds	x3, x2, x12
 5e0:	sub	x6, x8, #0x1
 5e4:	adc	x4, x4, x9
 5e8:	cset	x18, cs  // cs = hs, nlast
 5ec:	mov	x2, x3
 5f0:	cmp	x9, x4
 5f4:	b.cs	750 <__divtf3+0x750>  // b.hs, b.nlast
 5f8:	cmp	x5, x4
 5fc:	b.ls	778 <__divtf3+0x778>  // b.plast
 600:	adds	x2, x12, x3
 604:	sub	x6, x8, #0x2
 608:	adc	x4, x4, x9
 60c:	b	2c4 <__divtf3+0x2c4>
 610:	mov	x1, #0x1                   	// #1
 614:	sub	x1, x1, x3
 618:	cmp	x1, #0x74
 61c:	b.le	638 <__divtf3+0x638>
 620:	orr	x2, x5, x6
 624:	cbnz	x2, 844 <__divtf3+0x844>
 628:	orr	w0, w0, #0x8
 62c:	mov	w1, #0x0                   	// #0
 630:	mov	x6, #0x0                   	// #0
 634:	b	71c <__divtf3+0x71c>
 638:	cmp	x1, #0x3f
 63c:	b.le	784 <__divtf3+0x784>
 640:	mov	w2, #0x80                  	// #128
 644:	sub	w2, w2, w1
 648:	cmp	x1, #0x40
 64c:	sub	w1, w1, #0x40
 650:	lsl	x2, x6, x2
 654:	orr	x2, x5, x2
 658:	csel	x5, x2, x5, ne  // ne = any
 65c:	lsr	x6, x6, x1
 660:	cmp	x5, #0x0
 664:	cset	x2, ne  // ne = any
 668:	orr	x2, x2, x6
 66c:	ands	x6, x2, #0x7
 670:	b.eq	7b8 <__divtf3+0x7b8>  // b.none
 674:	mov	x6, #0x0                   	// #0
 678:	and	x11, x11, #0xc00000
 67c:	orr	w0, w0, #0x10
 680:	cmp	x11, #0x400, lsl #12
 684:	b.eq	8f8 <__divtf3+0x8f8>  // b.none
 688:	cmp	x11, #0x800, lsl #12
 68c:	b.eq	918 <__divtf3+0x918>  // b.none
 690:	cbz	x11, 880 <__divtf3+0x880>
 694:	tbnz	x6, #51, 898 <__divtf3+0x898>
 698:	orr	w0, w0, #0x8
 69c:	extr	x2, x6, x2, #3
 6a0:	mov	w1, #0x0                   	// #0
 6a4:	ubfx	x6, x6, #3, #48
 6a8:	b	71c <__divtf3+0x71c>
 6ac:	clz	x1, x5
 6b0:	add	x3, x1, #0x31
 6b4:	add	x1, x1, #0x40
 6b8:	cmp	x3, #0x3c
 6bc:	b.le	414 <__divtf3+0x414>
 6c0:	sub	w6, w3, #0x3d
 6c4:	lsl	x6, x5, x6
 6c8:	mov	x5, #0x0                   	// #0
 6cc:	b	430 <__divtf3+0x430>
 6d0:	clz	x7, x2
 6d4:	add	x4, x7, #0x31
 6d8:	add	x0, x7, #0x40
 6dc:	cmp	x4, #0x3c
 6e0:	b.le	45c <__divtf3+0x45c>
 6e4:	sub	w4, w4, #0x3d
 6e8:	lsl	x4, x2, x4
 6ec:	mov	x2, #0x0                   	// #0
 6f0:	b	478 <__divtf3+0x478>
 6f4:	and	x2, x11, #0xc00000
 6f8:	cmp	x2, #0x400, lsl #12
 6fc:	b.eq	8c4 <__divtf3+0x8c4>  // b.none
 700:	cmp	x2, #0x800, lsl #12
 704:	b.eq	7f4 <__divtf3+0x7f4>  // b.none
 708:	cbz	x2, 7d0 <__divtf3+0x7d0>
 70c:	mov	x6, #0xffffffffffff        	// #281474976710655
 710:	mov	x2, #0xffffffffffffffff    	// #-1
 714:	mov	w3, #0x14                  	// #20
 718:	orr	w0, w0, w3
 71c:	mov	x5, #0x0                   	// #0
 720:	orr	w1, w1, w10, lsl #15
 724:	bfxil	x5, x6, #0, #48
 728:	fmov	d0, x2
 72c:	bfi	x5, x1, #48, #16
 730:	fmov	v0.d[1], x5
 734:	b	1d4 <__divtf3+0x1d4>
 738:	orr	x6, x6, #0x800000000000
 73c:	mov	w10, w15
 740:	and	x6, x6, #0xffffffffffff
 744:	mov	x2, x5
 748:	mov	w1, #0x7fff                	// #32767
 74c:	b	164 <__divtf3+0x164>
 750:	cmp	x18, #0x0
 754:	ccmp	x9, x4, #0x0, eq  // eq = none
 758:	b.ne	2c4 <__divtf3+0x2c4>  // b.any
 75c:	b	5f8 <__divtf3+0x5f8>
 760:	cmp	x4, #0x0
 764:	cset	w2, ne  // ne = any
 768:	cmp	w2, #0x0
 76c:	ccmp	x1, x16, #0x0, ne  // ne = any
 770:	b.ne	3ac <__divtf3+0x3ac>  // b.any
 774:	b	380 <__divtf3+0x380>
 778:	ccmp	x1, x3, #0x0, eq  // eq = none
 77c:	b.ls	2c4 <__divtf3+0x2c4>  // b.plast
 780:	b	600 <__divtf3+0x600>
 784:	mov	w2, #0x40                  	// #64
 788:	sub	w2, w2, w1
 78c:	lsr	x4, x5, x1
 790:	lsl	x5, x5, x2
 794:	cmp	x5, #0x0
 798:	cset	x3, ne  // ne = any
 79c:	lsl	x2, x6, x2
 7a0:	orr	x2, x2, x4
 7a4:	lsr	x6, x6, x1
 7a8:	orr	x2, x2, x3
 7ac:	tst	x2, #0x7
 7b0:	b.ne	678 <__divtf3+0x678>  // b.any
 7b4:	tbnz	x6, #51, 924 <__divtf3+0x924>
 7b8:	mov	w1, #0x0                   	// #0
 7bc:	extr	x2, x6, x2, #3
 7c0:	ubfx	x6, x6, #3, #48
 7c4:	tbz	w11, #11, 164 <__divtf3+0x164>
 7c8:	orr	w0, w0, #0x8
 7cc:	b	71c <__divtf3+0x71c>
 7d0:	mov	w1, #0x7fff                	// #32767
 7d4:	mov	x6, #0x0                   	// #0
 7d8:	b	714 <__divtf3+0x714>
 7dc:	mov	w10, #0x0                   	// #0
 7e0:	cbz	x14, 12c <__divtf3+0x12c>
 7e4:	adds	x5, x5, #0x8
 7e8:	mov	w10, #0x1                   	// #1
 7ec:	cinc	x6, x6, cs  // cs = hs, nlast
 7f0:	b	12c <__divtf3+0x12c>
 7f4:	cmp	x14, #0x0
 7f8:	mov	w2, #0x7fff                	// #32767
 7fc:	mov	x6, #0xffffffffffff        	// #281474976710655
 800:	csel	w1, w1, w2, eq  // eq = none
 804:	csel	x6, x6, xzr, eq  // eq = none
 808:	csetm	x2, eq  // eq = none
 80c:	b	714 <__divtf3+0x714>
 810:	sub	x5, x5, #0x2
 814:	add	x3, x3, x9
 818:	b	238 <__divtf3+0x238>
 81c:	sub	x1, x1, #0x2
 820:	add	x4, x4, x9
 824:	b	268 <__divtf3+0x268>
 828:	cmp	x3, #0x0
 82c:	mov	x5, #0xffffffffffffffff    	// #-1
 830:	b.gt	570 <__divtf3+0x570>
 834:	mov	x1, #0x1                   	// #1
 838:	sub	x1, x1, x3
 83c:	cmp	x1, #0x74
 840:	b.le	638 <__divtf3+0x638>
 844:	and	x11, x11, #0xc00000
 848:	orr	w0, w0, #0x10
 84c:	cmp	x11, #0x400, lsl #12
 850:	b.eq	90c <__divtf3+0x90c>  // b.none
 854:	cmp	x11, #0x800, lsl #12
 858:	csel	x2, x14, xzr, eq  // eq = none
 85c:	b	628 <__divtf3+0x628>
 860:	lsl	x8, x12, #1
 864:	sub	x5, x5, #0x2
 868:	cmp	x12, x8
 86c:	cinc	x1, x9, hi  // hi = pmore
 870:	cmp	x4, x8
 874:	add	x1, x2, x1
 878:	cset	w2, ne  // ne = any
 87c:	b	3ac <__divtf3+0x3ac>
 880:	and	x1, x2, #0xf
 884:	cmp	x1, #0x4
 888:	b.eq	894 <__divtf3+0x894>  // b.none
 88c:	adds	x2, x2, #0x4
 890:	cinc	x6, x6, cs  // cs = hs, nlast
 894:	tbz	x6, #51, 698 <__divtf3+0x698>
 898:	orr	w0, w0, #0x8
 89c:	mov	w1, #0x1                   	// #1
 8a0:	mov	x6, #0x0                   	// #0
 8a4:	mov	x2, #0x0                   	// #0
 8a8:	b	71c <__divtf3+0x71c>
 8ac:	mov	w10, #0x1                   	// #1
 8b0:	cbnz	x14, 12c <__divtf3+0x12c>
 8b4:	adds	x5, x5, #0x8
 8b8:	mov	w10, #0x0                   	// #0
 8bc:	cinc	x6, x6, cs  // cs = hs, nlast
 8c0:	b	12c <__divtf3+0x12c>
 8c4:	cmp	x14, #0x0
 8c8:	mov	w2, #0x7fff                	// #32767
 8cc:	mov	x6, #0xffffffffffff        	// #281474976710655
 8d0:	csel	w1, w1, w2, ne  // ne = any
 8d4:	csel	x6, x6, xzr, ne  // ne = any
 8d8:	csetm	x2, ne  // ne = any
 8dc:	b	714 <__divtf3+0x714>
 8e0:	sub	x5, x5, #0x2
 8e4:	add	x1, x1, x9
 8e8:	b	308 <__divtf3+0x308>
 8ec:	sub	x2, x2, #0x2
 8f0:	add	x1, x1, x9
 8f4:	b	338 <__divtf3+0x338>
 8f8:	cbnz	x14, 894 <__divtf3+0x894>
 8fc:	adds	x2, x2, #0x8
 900:	cinc	x6, x6, cs  // cs = hs, nlast
 904:	tbnz	x6, #51, 898 <__divtf3+0x898>
 908:	b	698 <__divtf3+0x698>
 90c:	mov	x2, #0x1                   	// #1
 910:	sub	x2, x2, x14
 914:	b	628 <__divtf3+0x628>
 918:	cbnz	x14, 8fc <__divtf3+0x8fc>
 91c:	tbnz	x6, #51, 898 <__divtf3+0x898>
 920:	b	698 <__divtf3+0x698>
 924:	orr	w0, w0, #0x10
 928:	b	898 <__divtf3+0x898>

eqtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eqtf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x6, x1, [sp, #16]
  14:	ldp	x7, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	lsr	x2, x1, #63
  24:	lsr	x3, x0, #63
  28:	ubfx	x9, x0, #0, #48
  2c:	mov	x5, #0x7fff                	// #32767
  30:	mov	x10, x6
  34:	cmp	x4, x5
  38:	and	w2, w2, #0xff
  3c:	ubfx	x1, x1, #0, #48
  40:	and	w3, w3, #0xff
  44:	ubfx	x0, x0, #48, #15
  48:	b.eq	7c <__eqtf2+0x7c>  // b.none
  4c:	cmp	x0, x5
  50:	b.eq	68 <__eqtf2+0x68>  // b.none
  54:	cmp	x4, x0
  58:	mov	w0, #0x1                   	// #1
  5c:	b.eq	94 <__eqtf2+0x94>  // b.none
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	orr	x8, x9, x7
  6c:	cbnz	x8, f8 <__eqtf2+0xf8>
  70:	mov	w0, #0x1                   	// #1
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x5, x1, x6
  80:	cbnz	x5, c8 <__eqtf2+0xc8>
  84:	cmp	x0, x4
  88:	b.ne	70 <__eqtf2+0x70>  // b.any
  8c:	orr	x8, x9, x7
  90:	cbnz	x8, f8 <__eqtf2+0xf8>
  94:	cmp	x1, x9
  98:	mov	w0, #0x1                   	// #1
  9c:	ccmp	x6, x7, #0x0, eq  // eq = none
  a0:	b.ne	60 <__eqtf2+0x60>  // b.any
  a4:	cmp	w2, w3
  a8:	mov	w0, #0x0                   	// #0
  ac:	b.eq	60 <__eqtf2+0x60>  // b.none
  b0:	mov	w0, #0x1                   	// #1
  b4:	cbnz	x4, 60 <__eqtf2+0x60>
  b8:	orr	x1, x1, x10
  bc:	cmp	x1, #0x0
  c0:	cset	w0, ne  // ne = any
  c4:	b	60 <__eqtf2+0x60>
  c8:	tst	x1, #0x800000000000
  cc:	b.ne	e4 <__eqtf2+0xe4>  // b.any
  d0:	mov	w0, #0x1                   	// #1
  d4:	bl	0 <__sfp_handle_exceptions>
  d8:	mov	w0, #0x1                   	// #1
  dc:	ldp	x29, x30, [sp], #48
  e0:	ret
  e4:	cmp	x0, x4
  e8:	mov	w0, #0x1                   	// #1
  ec:	b.ne	60 <__eqtf2+0x60>  // b.any
  f0:	orr	x8, x9, x7
  f4:	cbz	x8, 60 <__eqtf2+0x60>
  f8:	tst	x9, #0x800000000000
  fc:	b.eq	d0 <__eqtf2+0xd0>  // b.none
 100:	b	70 <__eqtf2+0x70>

getf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__getf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x8, x1, [sp, #16]
  14:	ldp	x9, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	ubfx	x10, x1, #0, #48
  24:	lsr	x2, x1, #63
  28:	mov	x7, #0x7fff                	// #32767
  2c:	mov	x5, x8
  30:	cmp	x4, x7
  34:	ubfx	x11, x0, #0, #48
  38:	ubfx	x6, x0, #48, #15
  3c:	lsr	x1, x0, #63
  40:	mov	x3, x9
  44:	b.eq	80 <__getf2+0x80>  // b.none
  48:	cmp	x6, x7
  4c:	b.eq	90 <__getf2+0x90>  // b.none
  50:	cbnz	x4, b8 <__getf2+0xb8>
  54:	orr	x5, x10, x8
  58:	cmp	x5, #0x0
  5c:	cset	w0, eq  // eq = none
  60:	cbnz	x6, 6c <__getf2+0x6c>
  64:	orr	x3, x11, x9
  68:	cbz	x3, d8 <__getf2+0xd8>
  6c:	cbz	w0, 9c <__getf2+0x9c>
  70:	cmp	x1, #0x0
  74:	csinv	w0, w0, wzr, ne  // ne = any
  78:	ldp	x29, x30, [sp], #48
  7c:	ret
  80:	orr	x0, x10, x8
  84:	cbnz	x0, e4 <__getf2+0xe4>
  88:	cmp	x6, x4
  8c:	b.ne	b8 <__getf2+0xb8>  // b.any
  90:	orr	x3, x11, x3
  94:	cbnz	x3, e4 <__getf2+0xe4>
  98:	cbz	x4, c8 <__getf2+0xc8>
  9c:	cmp	x2, x1
  a0:	b.eq	f8 <__getf2+0xf8>  // b.none
  a4:	cmp	x2, #0x0
  a8:	mov	w0, #0xffffffff            	// #-1
  ac:	cneg	w0, w0, eq  // eq = none
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	cbnz	x6, 9c <__getf2+0x9c>
  bc:	orr	x3, x11, x3
  c0:	cbnz	x3, 9c <__getf2+0x9c>
  c4:	b	a4 <__getf2+0xa4>
  c8:	orr	x5, x10, x5
  cc:	cmp	x5, #0x0
  d0:	cset	w0, eq  // eq = none
  d4:	b	6c <__getf2+0x6c>
  d8:	mov	w0, #0x0                   	// #0
  dc:	cbz	x5, 78 <__getf2+0x78>
  e0:	b	a4 <__getf2+0xa4>
  e4:	mov	w0, #0x1                   	// #1
  e8:	bl	0 <__sfp_handle_exceptions>
  ec:	mov	w0, #0xfffffffe            	// #-2
  f0:	ldp	x29, x30, [sp], #48
  f4:	ret
  f8:	cmp	x4, x6
  fc:	b.gt	a4 <__getf2+0xa4>
 100:	b.lt	138 <__getf2+0x138>  // b.tstop
 104:	cmp	x10, x11
 108:	b.hi	a4 <__getf2+0xa4>  // b.pmore
 10c:	cset	w0, eq  // eq = none
 110:	cmp	w0, #0x0
 114:	ccmp	x8, x9, #0x0, ne  // ne = any
 118:	b.hi	a4 <__getf2+0xa4>  // b.pmore
 11c:	cmp	x10, x11
 120:	b.cc	138 <__getf2+0x138>  // b.lo, b.ul, b.last
 124:	cmp	w0, #0x0
 128:	mov	w0, #0x0                   	// #0
 12c:	ccmp	x8, x9, #0x2, ne  // ne = any
 130:	b.cs	78 <__getf2+0x78>  // b.hs, b.nlast
 134:	nop
 138:	cmp	x2, #0x0
 13c:	mov	w0, #0x1                   	// #1
 140:	cneg	w0, w0, eq  // eq = none
 144:	b	78 <__getf2+0x78>

letf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__letf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x8, x1, [sp, #16]
  14:	ldp	x9, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	ubfx	x10, x1, #0, #48
  24:	lsr	x2, x1, #63
  28:	mov	x5, #0x7fff                	// #32767
  2c:	mov	x6, x8
  30:	cmp	x4, x5
  34:	ubfx	x11, x0, #0, #48
  38:	ubfx	x7, x0, #48, #15
  3c:	lsr	x1, x0, #63
  40:	mov	x3, x9
  44:	b.eq	7c <__letf2+0x7c>  // b.none
  48:	cmp	x7, x5
  4c:	b.eq	8c <__letf2+0x8c>  // b.none
  50:	cbnz	x4, b8 <__letf2+0xb8>
  54:	orr	x6, x10, x8
  58:	cmp	x6, #0x0
  5c:	cset	w0, eq  // eq = none
  60:	cbnz	x7, a4 <__letf2+0xa4>
  64:	orr	x3, x11, x9
  68:	cbnz	x3, a4 <__letf2+0xa4>
  6c:	mov	w0, #0x0                   	// #0
  70:	cbnz	x6, cc <__letf2+0xcc>
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x0, x10, x8
  80:	cbnz	x0, e0 <__letf2+0xe0>
  84:	cmp	x7, x4
  88:	b.ne	b8 <__letf2+0xb8>  // b.any
  8c:	orr	x3, x11, x3
  90:	cbnz	x3, e0 <__letf2+0xe0>
  94:	cbnz	x4, c4 <__letf2+0xc4>
  98:	orr	x6, x10, x6
  9c:	cmp	x6, #0x0
  a0:	cset	w0, eq  // eq = none
  a4:	cbz	w0, c4 <__letf2+0xc4>
  a8:	cmp	x1, #0x0
  ac:	csinv	w0, w0, wzr, ne  // ne = any
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	cbnz	x7, c4 <__letf2+0xc4>
  bc:	orr	x3, x11, x3
  c0:	cbz	x3, cc <__letf2+0xcc>
  c4:	cmp	x2, x1
  c8:	b.eq	f4 <__letf2+0xf4>  // b.none
  cc:	cmp	x2, #0x0
  d0:	mov	w0, #0xffffffff            	// #-1
  d4:	cneg	w0, w0, eq  // eq = none
  d8:	ldp	x29, x30, [sp], #48
  dc:	ret
  e0:	mov	w0, #0x1                   	// #1
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	mov	w0, #0x2                   	// #2
  ec:	ldp	x29, x30, [sp], #48
  f0:	ret
  f4:	cmp	x4, x7
  f8:	b.gt	cc <__letf2+0xcc>
  fc:	b.lt	130 <__letf2+0x130>  // b.tstop
 100:	cmp	x10, x11
 104:	b.hi	cc <__letf2+0xcc>  // b.pmore
 108:	cset	w0, eq  // eq = none
 10c:	cmp	w0, #0x0
 110:	ccmp	x8, x9, #0x0, ne  // ne = any
 114:	b.hi	cc <__letf2+0xcc>  // b.pmore
 118:	cmp	x10, x11
 11c:	b.cc	130 <__letf2+0x130>  // b.lo, b.ul, b.last
 120:	cmp	w0, #0x0
 124:	mov	w0, #0x0                   	// #0
 128:	ccmp	x8, x9, #0x2, ne  // ne = any
 12c:	b.cs	74 <__letf2+0x74>  // b.hs, b.nlast
 130:	cmp	x2, #0x0
 134:	mov	w0, #0x1                   	// #1
 138:	cneg	w0, w0, eq  // eq = none
 13c:	b	74 <__letf2+0x74>

multf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #48]
   c:	str	q1, [sp, #64]
  10:	ldp	x1, x0, [sp, #48]
  14:	ldp	x3, x2, [sp, #64]
  18:	mrs	x12, fpcr
  1c:	lsr	x4, x0, #63
  20:	ubfx	x8, x0, #0, #48
  24:	and	w16, w4, #0xff
  28:	mov	x14, x4
  2c:	ubfx	x10, x0, #48, #15
  30:	cbz	w10, 3b4 <__multf3+0x3b4>
  34:	mov	w4, #0x7fff                	// #32767
  38:	cmp	w10, w4
  3c:	b.eq	3f4 <__multf3+0x3f4>  // b.none
  40:	and	x10, x10, #0xffff
  44:	extr	x4, x8, x1, #61
  48:	mov	x5, #0xffffffffffffc001    	// #-16383
  4c:	orr	x8, x4, #0x8000000000000
  50:	add	x10, x10, x5
  54:	lsl	x7, x1, #3
  58:	mov	x11, #0x2                   	// #2
  5c:	mov	x9, #0x1                   	// #1
  60:	mov	x6, #0x3                   	// #3
  64:	mov	x1, #0x0                   	// #0
  68:	mov	x17, #0x0                   	// #0
  6c:	mov	w0, #0x0                   	// #0
  70:	lsr	x5, x2, #63
  74:	ubfx	x4, x2, #0, #48
  78:	and	w15, w5, #0xff
  7c:	mov	x13, x5
  80:	ubfx	x5, x2, #48, #15
  84:	cbz	w5, 438 <__multf3+0x438>
  88:	mov	w9, #0x7fff                	// #32767
  8c:	cmp	w5, w9
  90:	b.eq	120 <__multf3+0x120>  // b.none
  94:	and	x5, x5, #0xffff
  98:	extr	x2, x4, x3, #61
  9c:	mov	x4, #0xffffffffffffc001    	// #-16383
  a0:	add	x5, x5, x4
  a4:	add	x10, x10, x5
  a8:	orr	x4, x2, #0x8000000000000
  ac:	lsl	x5, x3, #3
  b0:	mov	x6, #0x0                   	// #0
  b4:	eor	w3, w16, w15
  b8:	cmp	x1, #0xa
  bc:	and	w11, w3, #0xff
  c0:	and	x9, x3, #0xff
  c4:	add	x18, x10, #0x1
  c8:	b.gt	3a0 <__multf3+0x3a0>
  cc:	cmp	x1, #0x2
  d0:	b.gt	160 <__multf3+0x160>
  d4:	sub	x1, x1, #0x1
  d8:	cmp	x1, #0x1
  dc:	b.hi	1c0 <__multf3+0x1c0>  // b.pmore
  e0:	cmp	x6, #0x2
  e4:	b.eq	47c <__multf3+0x47c>  // b.none
  e8:	cmp	x6, #0x1
  ec:	b.ne	320 <__multf3+0x320>  // b.any
  f0:	mov	w1, #0x0                   	// #0
  f4:	mov	x4, #0x0                   	// #0
  f8:	mov	x7, #0x0                   	// #0
  fc:	mov	x3, #0x0                   	// #0
 100:	orr	w1, w1, w11, lsl #15
 104:	bfxil	x3, x4, #0, #48
 108:	fmov	d0, x7
 10c:	bfi	x3, x1, #48, #16
 110:	fmov	v0.d[1], x3
 114:	cbnz	w0, 628 <__multf3+0x628>
 118:	ldp	x29, x30, [sp], #80
 11c:	ret
 120:	mov	x2, #0x7fff                	// #32767
 124:	orr	x5, x4, x3
 128:	add	x2, x10, x2
 12c:	cbz	x5, 48c <__multf3+0x48c>
 130:	ands	x1, x4, #0x800000000000
 134:	eor	w9, w16, w15
 138:	csinc	w0, w0, wzr, ne  // ne = any
 13c:	and	w11, w9, #0xff
 140:	add	x18, x10, #0x8, lsl #12
 144:	cmp	x6, #0xa
 148:	and	x9, x9, #0xff
 14c:	b.gt	584 <__multf3+0x584>
 150:	mov	x10, x2
 154:	mov	x5, x3
 158:	mov	x1, x6
 15c:	mov	x6, #0x3                   	// #3
 160:	mov	x2, #0x1                   	// #1
 164:	mov	x3, #0x530                 	// #1328
 168:	lsl	x1, x2, x1
 16c:	tst	x1, x3
 170:	b.ne	394 <__multf3+0x394>  // b.any
 174:	mov	x3, #0x240                 	// #576
 178:	tst	x1, x3
 17c:	b.ne	37c <__multf3+0x37c>  // b.any
 180:	mov	x2, #0x88                  	// #136
 184:	tst	x1, x2
 188:	b.eq	1c0 <__multf3+0x1c0>  // b.none
 18c:	mov	x8, x4
 190:	mov	x7, x5
 194:	mov	x17, x6
 198:	cmp	x17, #0x2
 19c:	b.eq	7d4 <__multf3+0x7d4>  // b.none
 1a0:	mov	x6, x17
 1a4:	mov	w11, w15
 1a8:	cmp	x17, #0x3
 1ac:	mov	x4, x8
 1b0:	mov	x5, x7
 1b4:	mov	x9, x13
 1b8:	b.ne	e8 <__multf3+0xe8>  // b.any
 1bc:	b	5cc <__multf3+0x5cc>
 1c0:	lsr	x13, x7, #32
 1c4:	and	x6, x5, #0xffffffff
 1c8:	and	x17, x4, #0xffffffff
 1cc:	and	x7, x7, #0xffffffff
 1d0:	stp	x21, x22, [sp, #32]
 1d4:	lsr	x22, x5, #32
 1d8:	lsr	x2, x4, #32
 1dc:	stp	x19, x20, [sp, #16]
 1e0:	mul	x19, x13, x6
 1e4:	lsr	x4, x8, #32
 1e8:	mul	x1, x13, x17
 1ec:	and	x3, x8, #0xffffffff
 1f0:	madd	x5, x22, x7, x19
 1f4:	mov	x14, #0x100000000           	// #4294967296
 1f8:	mul	x15, x6, x7
 1fc:	mul	x16, x7, x17
 200:	madd	x7, x2, x7, x1
 204:	and	x30, x15, #0xffffffff
 208:	mul	x21, x4, x6
 20c:	add	x15, x5, x15, lsr #32
 210:	mul	x20, x4, x17
 214:	cmp	x19, x15
 218:	mul	x5, x13, x22
 21c:	add	x30, x30, x15, lsl #32
 220:	mul	x19, x13, x2
 224:	add	x13, x7, x16, lsr #32
 228:	mul	x6, x6, x3
 22c:	add	x8, x5, x14
 230:	mul	x17, x3, x17
 234:	csel	x5, x8, x5, hi  // hi = pmore
 238:	madd	x7, x22, x3, x21
 23c:	cmp	x1, x13
 240:	madd	x3, x2, x3, x20
 244:	and	x16, x16, #0xffffffff
 248:	mul	x8, x22, x4
 24c:	add	x16, x16, x13, lsl #32
 250:	add	x7, x7, x6, lsr #32
 254:	mul	x2, x2, x4
 258:	add	x3, x3, x17, lsr #32
 25c:	add	x4, x19, x14
 260:	csel	x19, x4, x19, hi  // hi = pmore
 264:	and	x1, x17, #0xffffffff
 268:	cmp	x21, x7
 26c:	add	x4, x8, x14
 270:	csel	x8, x4, x8, hi  // hi = pmore
 274:	add	x1, x1, x3, lsl #32
 278:	cmp	x20, x3
 27c:	add	x15, x16, x15, lsr #32
 280:	add	x13, x19, x13, lsr #32
 284:	add	x14, x2, x14
 288:	add	x15, x5, x15
 28c:	csel	x2, x14, x2, hi  // hi = pmore
 290:	adds	x1, x1, x13
 294:	and	x6, x6, #0xffffffff
 298:	cset	x5, cs  // cs = hs, nlast
 29c:	cmp	x15, x16
 2a0:	cset	x4, cc  // cc = lo, ul, last
 2a4:	add	x6, x6, x7, lsl #32
 2a8:	adds	x1, x1, x4
 2ac:	lsr	x3, x3, #32
 2b0:	cset	x4, cs  // cs = hs, nlast
 2b4:	cmp	x5, #0x0
 2b8:	ccmp	x4, #0x0, #0x0, eq  // eq = none
 2bc:	add	x7, x8, x7, lsr #32
 2c0:	cinc	x3, x3, ne  // ne = any
 2c4:	adds	x5, x15, x6
 2c8:	cset	x4, cs  // cs = hs, nlast
 2cc:	adds	x1, x1, x7
 2d0:	cset	x6, cs  // cs = hs, nlast
 2d4:	adds	x1, x1, x4
 2d8:	cset	x4, cs  // cs = hs, nlast
 2dc:	cmp	x6, #0x0
 2e0:	ccmp	x4, #0x0, #0x0, eq  // eq = none
 2e4:	orr	x30, x30, x5, lsl #13
 2e8:	cinc	x2, x2, ne  // ne = any
 2ec:	cmp	x30, #0x0
 2f0:	add	x2, x2, x3
 2f4:	cset	x3, ne  // ne = any
 2f8:	orr	x5, x3, x5, lsr #51
 2fc:	orr	x5, x5, x1, lsl #13
 300:	extr	x4, x2, x1, #51
 304:	tbz	x2, #39, 6b0 <__multf3+0x6b0>
 308:	ldp	x19, x20, [sp, #16]
 30c:	and	x1, x5, #0x1
 310:	ldp	x21, x22, [sp, #32]
 314:	orr	x5, x1, x5, lsr #1
 318:	orr	x5, x5, x4, lsl #63
 31c:	lsr	x4, x4, #1
 320:	mov	x1, #0x3fff                	// #16383
 324:	add	x2, x18, x1
 328:	cmp	x2, #0x0
 32c:	b.le	500 <__multf3+0x500>
 330:	tst	x5, #0x7
 334:	b.eq	354 <__multf3+0x354>  // b.none
 338:	and	x1, x12, #0xc00000
 33c:	orr	w0, w0, #0x10
 340:	cmp	x1, #0x400, lsl #12
 344:	b.eq	788 <__multf3+0x788>  // b.none
 348:	cmp	x1, #0x800, lsl #12
 34c:	b.eq	718 <__multf3+0x718>  // b.none
 350:	cbz	x1, 744 <__multf3+0x744>
 354:	tbz	x4, #52, 360 <__multf3+0x360>
 358:	and	x4, x4, #0xffefffffffffffff
 35c:	add	x2, x18, #0x4, lsl #12
 360:	mov	x1, #0x7ffe                	// #32766
 364:	cmp	x2, x1
 368:	b.gt	684 <__multf3+0x684>
 36c:	and	w1, w2, #0x7fff
 370:	extr	x7, x4, x5, #3
 374:	ubfx	x4, x4, #3, #48
 378:	b	fc <__multf3+0xfc>
 37c:	mov	w0, w2
 380:	mov	w11, #0x0                   	// #0
 384:	mov	x4, #0xffffffffffff        	// #281474976710655
 388:	mov	x7, #0xffffffffffffffff    	// #-1
 38c:	mov	w1, #0x7fff                	// #32767
 390:	b	fc <__multf3+0xfc>
 394:	mov	w15, w11
 398:	mov	x13, x9
 39c:	b	198 <__multf3+0x198>
 3a0:	cmp	x1, #0xb
 3a4:	b.eq	18c <__multf3+0x18c>  // b.none
 3a8:	mov	w15, w16
 3ac:	mov	x13, x14
 3b0:	b	198 <__multf3+0x198>
 3b4:	orr	x7, x8, x1
 3b8:	cbz	x7, 4dc <__multf3+0x4dc>
 3bc:	cbz	x8, 63c <__multf3+0x63c>
 3c0:	clz	x0, x8
 3c4:	sub	x4, x0, #0xf
 3c8:	add	w7, w4, #0x3
 3cc:	mov	w5, #0x3d                  	// #61
 3d0:	sub	w5, w5, w4
 3d4:	lsl	x4, x8, x7
 3d8:	lsr	x5, x1, x5
 3dc:	orr	x8, x5, x4
 3e0:	lsl	x7, x1, x7
 3e4:	mov	x10, #0xffffffffffffc011    	// #-16367
 3e8:	mov	x11, #0x2                   	// #2
 3ec:	sub	x10, x10, x0
 3f0:	b	5c <__multf3+0x5c>
 3f4:	orr	x7, x8, x1
 3f8:	cbnz	x7, 4b0 <__multf3+0x4b0>
 3fc:	lsr	x5, x2, #63
 400:	ubfx	x4, x2, #0, #48
 404:	and	w15, w5, #0xff
 408:	mov	x13, x5
 40c:	mov	x8, #0x0                   	// #0
 410:	ubfx	x5, x2, #48, #15
 414:	mov	x11, #0xa                   	// #10
 418:	mov	x9, #0x9                   	// #9
 41c:	mov	x6, #0xb                   	// #11
 420:	mov	x1, #0x8                   	// #8
 424:	mov	x10, #0x7fff                	// #32767
 428:	mov	x17, #0x2                   	// #2
 42c:	mov	w0, #0x0                   	// #0
 430:	cbnz	w5, 88 <__multf3+0x88>
 434:	nop
 438:	orr	x5, x4, x3
 43c:	cbz	x5, 4a0 <__multf3+0x4a0>
 440:	cbz	x4, 660 <__multf3+0x660>
 444:	clz	x6, x4
 448:	sub	x2, x6, #0xf
 44c:	add	w5, w2, #0x3
 450:	mov	w9, #0x3d                  	// #61
 454:	sub	w9, w9, w2
 458:	lsl	x2, x4, x5
 45c:	lsr	x9, x3, x9
 460:	orr	x4, x9, x2
 464:	lsl	x5, x3, x5
 468:	sub	x10, x10, x6
 46c:	mov	x3, #0xffffffffffffc011    	// #-16367
 470:	mov	x6, #0x0                   	// #0
 474:	add	x10, x10, x3
 478:	b	b4 <__multf3+0xb4>
 47c:	mov	w1, #0x7fff                	// #32767
 480:	mov	x4, #0x0                   	// #0
 484:	mov	x7, #0x0                   	// #0
 488:	b	fc <__multf3+0xfc>
 48c:	mov	x1, x11
 490:	mov	x10, x2
 494:	mov	x4, #0x0                   	// #0
 498:	mov	x6, #0x2                   	// #2
 49c:	b	b4 <__multf3+0xb4>
 4a0:	mov	x1, x9
 4a4:	mov	x4, #0x0                   	// #0
 4a8:	mov	x6, #0x1                   	// #1
 4ac:	b	b4 <__multf3+0xb4>
 4b0:	lsr	x0, x8, #47
 4b4:	mov	x7, x1
 4b8:	eor	x0, x0, #0x1
 4bc:	mov	x11, #0xe                   	// #14
 4c0:	and	w0, w0, #0x1
 4c4:	mov	x9, #0xd                   	// #13
 4c8:	mov	x6, #0xf                   	// #15
 4cc:	mov	x1, #0xc                   	// #12
 4d0:	mov	x10, #0x7fff                	// #32767
 4d4:	mov	x17, #0x3                   	// #3
 4d8:	b	70 <__multf3+0x70>
 4dc:	mov	x8, #0x0                   	// #0
 4e0:	mov	x11, #0x6                   	// #6
 4e4:	mov	x9, #0x5                   	// #5
 4e8:	mov	x6, #0x7                   	// #7
 4ec:	mov	x1, #0x4                   	// #4
 4f0:	mov	x10, #0x0                   	// #0
 4f4:	mov	x17, #0x1                   	// #1
 4f8:	mov	w0, #0x0                   	// #0
 4fc:	b	70 <__multf3+0x70>
 500:	mov	x1, #0x1                   	// #1
 504:	sub	x2, x1, x2
 508:	cmp	x2, #0x74
 50c:	b.gt	5e0 <__multf3+0x5e0>
 510:	cmp	x2, #0x3f
 514:	b.le	6c0 <__multf3+0x6c0>
 518:	mov	w1, #0x80                  	// #128
 51c:	sub	w1, w1, w2
 520:	cmp	x2, #0x40
 524:	sub	w2, w2, #0x40
 528:	lsl	x1, x4, x1
 52c:	orr	x1, x5, x1
 530:	csel	x5, x1, x5, ne  // ne = any
 534:	lsr	x2, x4, x2
 538:	cmp	x5, #0x0
 53c:	cset	x7, ne  // ne = any
 540:	orr	x7, x7, x2
 544:	ands	x4, x7, #0x7
 548:	b.eq	6f4 <__multf3+0x6f4>  // b.none
 54c:	mov	x4, #0x0                   	// #0
 550:	and	x12, x12, #0xc00000
 554:	orr	w0, w0, #0x10
 558:	cmp	x12, #0x400, lsl #12
 55c:	b.eq	7c0 <__multf3+0x7c0>  // b.none
 560:	cmp	x12, #0x800, lsl #12
 564:	b.eq	7ac <__multf3+0x7ac>  // b.none
 568:	cbz	x12, 75c <__multf3+0x75c>
 56c:	tbnz	x4, #51, 774 <__multf3+0x774>
 570:	orr	w0, w0, #0x8
 574:	extr	x7, x4, x7, #3
 578:	mov	w1, #0x0                   	// #0
 57c:	ubfx	x4, x4, #3, #48
 580:	b	610 <__multf3+0x610>
 584:	cmp	x6, #0xf
 588:	b.ne	5bc <__multf3+0x5bc>  // b.any
 58c:	tbz	x8, #47, 5a8 <__multf3+0x5a8>
 590:	cbnz	x1, 5a8 <__multf3+0x5a8>
 594:	orr	x4, x4, #0x800000000000
 598:	mov	w11, w15
 59c:	mov	x7, x3
 5a0:	mov	w1, #0x7fff                	// #32767
 5a4:	b	fc <__multf3+0xfc>
 5a8:	orr	x4, x8, #0x800000000000
 5ac:	mov	w11, w16
 5b0:	and	x4, x4, #0xffffffffffff
 5b4:	mov	w1, #0x7fff                	// #32767
 5b8:	b	fc <__multf3+0xfc>
 5bc:	cmp	x6, #0xb
 5c0:	b.ne	3a8 <__multf3+0x3a8>  // b.any
 5c4:	mov	w11, w15
 5c8:	mov	x5, x3
 5cc:	orr	x4, x4, #0x800000000000
 5d0:	mov	x7, x5
 5d4:	and	x4, x4, #0xffffffffffff
 5d8:	mov	w1, #0x7fff                	// #32767
 5dc:	b	fc <__multf3+0xfc>
 5e0:	orr	x7, x5, x4
 5e4:	cbz	x7, 604 <__multf3+0x604>
 5e8:	and	x12, x12, #0xc00000
 5ec:	orr	w0, w0, #0x10
 5f0:	cmp	x12, #0x400, lsl #12
 5f4:	sub	x7, x1, x9
 5f8:	b.eq	604 <__multf3+0x604>  // b.none
 5fc:	cmp	x12, #0x800, lsl #12
 600:	csel	x7, x9, xzr, eq  // eq = none
 604:	orr	w0, w0, #0x8
 608:	mov	w1, #0x0                   	// #0
 60c:	mov	x4, #0x0                   	// #0
 610:	mov	x3, #0x0                   	// #0
 614:	fmov	d0, x7
 618:	bfxil	x3, x4, #0, #48
 61c:	bfi	x3, x1, #48, #15
 620:	bfi	x3, x11, #63, #1
 624:	fmov	v0.d[1], x3
 628:	str	q0, [sp, #48]
 62c:	bl	0 <__sfp_handle_exceptions>
 630:	ldr	q0, [sp, #48]
 634:	ldp	x29, x30, [sp], #80
 638:	ret
 63c:	clz	x10, x1
 640:	add	x4, x10, #0x31
 644:	add	x0, x10, #0x40
 648:	cmp	x4, #0x3c
 64c:	b.le	3c8 <__multf3+0x3c8>
 650:	sub	w4, w4, #0x3d
 654:	mov	x7, #0x0                   	// #0
 658:	lsl	x8, x1, x4
 65c:	b	3e4 <__multf3+0x3e4>
 660:	clz	x6, x3
 664:	add	x2, x6, #0x31
 668:	add	x6, x6, #0x40
 66c:	cmp	x2, #0x3c
 670:	b.le	44c <__multf3+0x44c>
 674:	sub	w2, w2, #0x3d
 678:	mov	x5, #0x0                   	// #0
 67c:	lsl	x4, x3, x2
 680:	b	468 <__multf3+0x468>
 684:	and	x7, x12, #0xc00000
 688:	cmp	x7, #0x400, lsl #12
 68c:	b.eq	790 <__multf3+0x790>  // b.none
 690:	cmp	x7, #0x800, lsl #12
 694:	b.eq	728 <__multf3+0x728>  // b.none
 698:	cbz	x7, 70c <__multf3+0x70c>
 69c:	mov	x4, #0xffffffffffff        	// #281474976710655
 6a0:	mov	x7, #0xffffffffffffffff    	// #-1
 6a4:	mov	w2, #0x14                  	// #20
 6a8:	orr	w0, w0, w2
 6ac:	b	610 <__multf3+0x610>
 6b0:	mov	x18, x10
 6b4:	ldp	x19, x20, [sp, #16]
 6b8:	ldp	x21, x22, [sp, #32]
 6bc:	b	320 <__multf3+0x320>
 6c0:	mov	w1, #0x40                  	// #64
 6c4:	sub	w1, w1, w2
 6c8:	lsr	x3, x5, x2
 6cc:	lsl	x5, x5, x1
 6d0:	cmp	x5, #0x0
 6d4:	lsl	x7, x4, x1
 6d8:	cset	x1, ne  // ne = any
 6dc:	orr	x7, x7, x3
 6e0:	lsr	x4, x4, x2
 6e4:	orr	x7, x7, x1
 6e8:	tst	x7, #0x7
 6ec:	b.ne	550 <__multf3+0x550>  // b.any
 6f0:	tbnz	x4, #51, 7cc <__multf3+0x7cc>
 6f4:	mov	w1, #0x0                   	// #0
 6f8:	extr	x7, x4, x7, #3
 6fc:	ubfx	x4, x4, #3, #48
 700:	tbz	w12, #11, fc <__multf3+0xfc>
 704:	orr	w0, w0, #0x8
 708:	b	610 <__multf3+0x610>
 70c:	mov	w1, #0x7fff                	// #32767
 710:	mov	x4, #0x0                   	// #0
 714:	b	6a4 <__multf3+0x6a4>
 718:	cbz	x9, 354 <__multf3+0x354>
 71c:	adds	x5, x5, #0x8
 720:	cinc	x4, x4, cs  // cs = hs, nlast
 724:	b	354 <__multf3+0x354>
 728:	cmp	x9, #0x0
 72c:	mov	w2, #0x7fff                	// #32767
 730:	mov	x4, #0xffffffffffff        	// #281474976710655
 734:	csel	w1, w1, w2, eq  // eq = none
 738:	csel	x4, x4, xzr, eq  // eq = none
 73c:	csetm	x7, eq  // eq = none
 740:	b	6a4 <__multf3+0x6a4>
 744:	and	x1, x5, #0xf
 748:	cmp	x1, #0x4
 74c:	b.eq	354 <__multf3+0x354>  // b.none
 750:	adds	x5, x5, #0x4
 754:	cinc	x4, x4, cs  // cs = hs, nlast
 758:	b	354 <__multf3+0x354>
 75c:	and	x1, x7, #0xf
 760:	cmp	x1, #0x4
 764:	b.eq	770 <__multf3+0x770>  // b.none
 768:	adds	x7, x7, #0x4
 76c:	cinc	x4, x4, cs  // cs = hs, nlast
 770:	tbz	x4, #51, 570 <__multf3+0x570>
 774:	orr	w0, w0, #0x8
 778:	mov	w1, #0x1                   	// #1
 77c:	mov	x4, #0x0                   	// #0
 780:	mov	x7, #0x0                   	// #0
 784:	b	610 <__multf3+0x610>
 788:	cbnz	x9, 354 <__multf3+0x354>
 78c:	b	71c <__multf3+0x71c>
 790:	cmp	x9, #0x0
 794:	mov	w2, #0x7fff                	// #32767
 798:	mov	x4, #0xffffffffffff        	// #281474976710655
 79c:	csel	w1, w1, w2, ne  // ne = any
 7a0:	csel	x4, x4, xzr, ne  // ne = any
 7a4:	csetm	x7, ne  // ne = any
 7a8:	b	6a4 <__multf3+0x6a4>
 7ac:	cbz	x9, 770 <__multf3+0x770>
 7b0:	adds	x7, x7, #0x8
 7b4:	cinc	x4, x4, cs  // cs = hs, nlast
 7b8:	tbnz	x4, #51, 774 <__multf3+0x774>
 7bc:	b	570 <__multf3+0x570>
 7c0:	cbz	x9, 7b0 <__multf3+0x7b0>
 7c4:	tbnz	x4, #51, 774 <__multf3+0x774>
 7c8:	b	570 <__multf3+0x570>
 7cc:	orr	w0, w0, #0x10
 7d0:	b	774 <__multf3+0x774>
 7d4:	mov	w11, w15
 7d8:	mov	w1, #0x7fff                	// #32767
 7dc:	mov	x4, #0x0                   	// #0
 7e0:	mov	x7, #0x0                   	// #0
 7e4:	b	fc <__multf3+0xfc>

negtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negtf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x1, #0x0                   	// #0
   8:	str	q0, [sp]
   c:	ldp	x3, x2, [sp]
  10:	add	sp, sp, #0x10
  14:	mov	x0, x3
  18:	lsr	x3, x2, #48
  1c:	bfxil	x1, x2, #0, #48
  20:	fmov	d0, x0
  24:	eor	w2, w3, #0x8000
  28:	bfi	x1, x2, #48, #16
  2c:	fmov	v0.d[1], x1
  30:	ret

subtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x6, x1, [sp, #16]
  14:	ldp	x0, x2, [sp, #32]
  18:	mrs	x13, fpcr
  1c:	mov	x9, x0
  20:	ubfx	x0, x2, #48, #15
  24:	lsr	x4, x1, #63
  28:	ubfx	x7, x1, #48, #15
  2c:	ubfiz	x3, x1, #3, #48
  30:	mov	x12, x0
  34:	lsr	x5, x2, #63
  38:	ubfiz	x2, x2, #3, #48
  3c:	mov	x11, x4
  40:	and	w8, w4, #0xff
  44:	mov	x14, x4
  48:	sub	w0, w7, w0
  4c:	mov	x1, x7
  50:	orr	x3, x3, x6, lsr #61
  54:	mov	x7, #0x7fff                	// #32767
  58:	and	w5, w5, #0xff
  5c:	cmp	x12, x7
  60:	orr	x2, x2, x9, lsr #61
  64:	lsl	x4, x6, #3
  68:	lsl	x10, x9, #3
  6c:	b.eq	1dc <__subtf3+0x1dc>  // b.none
  70:	eor	w5, w5, #0x1
  74:	and	x15, x5, #0xff
  78:	cmp	x11, w5, uxtb
  7c:	b.eq	248 <__subtf3+0x248>  // b.none
  80:	cmp	w0, #0x0
  84:	b.le	1f8 <__subtf3+0x1f8>
  88:	cbnz	x12, 33c <__subtf3+0x33c>
  8c:	orr	x5, x2, x10
  90:	cbz	x5, 52c <__subtf3+0x52c>
  94:	subs	w0, w0, #0x1
  98:	b.eq	820 <__subtf3+0x820>  // b.none
  9c:	mov	x5, #0x7fff                	// #32767
  a0:	cmp	x1, x5
  a4:	b.eq	500 <__subtf3+0x500>  // b.none
  a8:	cmp	w0, #0x74
  ac:	b.gt	51c <__subtf3+0x51c>
  b0:	cmp	w0, #0x3f
  b4:	b.gt	680 <__subtf3+0x680>
  b8:	mov	w5, #0x40                  	// #64
  bc:	sub	w5, w5, w0
  c0:	lsr	x7, x10, x0
  c4:	lsl	x10, x10, x5
  c8:	cmp	x10, #0x0
  cc:	lsl	x5, x2, x5
  d0:	cset	x6, ne  // ne = any
  d4:	orr	x5, x5, x7
  d8:	lsr	x2, x2, x0
  dc:	orr	x5, x5, x6
  e0:	sub	x3, x3, x2
  e4:	subs	x4, x4, x5
  e8:	sbc	x3, x3, xzr
  ec:	and	x6, x3, #0x7ffffffffffff
  f0:	tbz	x3, #51, 2c8 <__subtf3+0x2c8>
  f4:	cbz	x6, 4e4 <__subtf3+0x4e4>
  f8:	clz	x0, x6
  fc:	sub	w0, w0, #0xc
 100:	neg	w3, w0
 104:	lsl	x2, x6, x0
 108:	lsl	x6, x4, x0
 10c:	lsr	x4, x4, x3
 110:	orr	x3, x4, x2
 114:	cmp	x1, w0, sxtw
 118:	sxtw	x2, w0
 11c:	b.gt	4c4 <__subtf3+0x4c4>
 120:	sub	w1, w0, w1
 124:	add	w0, w1, #0x1
 128:	cmp	w0, #0x3f
 12c:	b.gt	648 <__subtf3+0x648>
 130:	mov	w1, #0x40                  	// #64
 134:	sub	w1, w1, w0
 138:	lsr	x2, x6, x0
 13c:	lsl	x6, x6, x1
 140:	cmp	x6, #0x0
 144:	lsl	x4, x3, x1
 148:	cset	x1, ne  // ne = any
 14c:	orr	x4, x4, x2
 150:	lsr	x3, x3, x0
 154:	orr	x4, x4, x1
 158:	orr	x7, x4, x3
 15c:	cbz	x7, 2dc <__subtf3+0x2dc>
 160:	and	x0, x4, #0x7
 164:	mov	x1, #0x0                   	// #0
 168:	mov	w5, #0x1                   	// #1
 16c:	cbz	x0, 550 <__subtf3+0x550>
 170:	and	x2, x13, #0xc00000
 174:	cmp	x2, #0x400, lsl #12
 178:	b.eq	49c <__subtf3+0x49c>  // b.none
 17c:	cmp	x2, #0x800, lsl #12
 180:	b.eq	47c <__subtf3+0x47c>  // b.none
 184:	cbz	x2, 4a8 <__subtf3+0x4a8>
 188:	and	x2, x3, #0x8000000000000
 18c:	mov	w0, #0x10                  	// #16
 190:	cbz	w5, 198 <__subtf3+0x198>
 194:	orr	w0, w0, #0x8
 198:	cbz	x2, 450 <__subtf3+0x450>
 19c:	add	x1, x1, #0x1
 1a0:	mov	x2, #0x7fff                	// #32767
 1a4:	cmp	x1, x2
 1a8:	b.eq	394 <__subtf3+0x394>  // b.none
 1ac:	ubfx	x7, x3, #3, #48
 1b0:	extr	x6, x3, x4, #3
 1b4:	and	w1, w1, #0x7fff
 1b8:	mov	x3, #0x0                   	// #0
 1bc:	orr	w1, w1, w8, lsl #15
 1c0:	bfxil	x3, x7, #0, #48
 1c4:	fmov	d0, x6
 1c8:	bfi	x3, x1, #48, #16
 1cc:	fmov	v0.d[1], x3
 1d0:	cbnz	w0, 3f0 <__subtf3+0x3f0>
 1d4:	ldp	x29, x30, [sp], #48
 1d8:	ret
 1dc:	orr	x7, x2, x10
 1e0:	cbz	x7, 70 <__subtf3+0x70>
 1e4:	cmp	x11, w5, uxtb
 1e8:	and	x15, x5, #0xff
 1ec:	b.eq	404 <__subtf3+0x404>  // b.none
 1f0:	cmp	w0, #0x0
 1f4:	b.gt	33c <__subtf3+0x33c>
 1f8:	cbz	w0, 2f4 <__subtf3+0x2f4>
 1fc:	cbnz	x1, 5dc <__subtf3+0x5dc>
 200:	orr	x1, x3, x4
 204:	cbz	x1, 2a4 <__subtf3+0x2a4>
 208:	cmn	w0, #0x1
 20c:	b.eq	994 <__subtf3+0x994>  // b.none
 210:	mov	x1, #0x7fff                	// #32767
 214:	mvn	w0, w0
 218:	cmp	x12, x1
 21c:	b.ne	5f0 <__subtf3+0x5f0>  // b.any
 220:	orr	x0, x2, x10
 224:	cbnz	x0, 8e4 <__subtf3+0x8e4>
 228:	and	x11, x5, #0xff
 22c:	nop
 230:	mov	x2, #0x0                   	// #0
 234:	fmov	d0, x2
 238:	lsl	x0, x11, #63
 23c:	orr	x3, x0, #0x7fff000000000000
 240:	fmov	v0.d[1], x3
 244:	b	1d4 <__subtf3+0x1d4>
 248:	cmp	w0, #0x0
 24c:	b.le	404 <__subtf3+0x404>
 250:	cbz	x12, 344 <__subtf3+0x344>
 254:	orr	x2, x2, #0x8000000000000
 258:	mov	x5, #0x7fff                	// #32767
 25c:	cmp	x1, x5
 260:	b.eq	500 <__subtf3+0x500>  // b.none
 264:	cmp	w0, #0x74
 268:	b.gt	630 <__subtf3+0x630>
 26c:	cmp	w0, #0x3f
 270:	b.gt	730 <__subtf3+0x730>
 274:	mov	w5, #0x40                  	// #64
 278:	sub	w5, w5, w0
 27c:	lsr	x7, x10, x0
 280:	lsl	x10, x10, x5
 284:	cmp	x10, #0x0
 288:	lsl	x5, x2, x5
 28c:	cset	x6, ne  // ne = any
 290:	orr	x5, x5, x7
 294:	lsr	x2, x2, x0
 298:	orr	x0, x5, x6
 29c:	add	x3, x3, x2
 2a0:	b	63c <__subtf3+0x63c>
 2a4:	mov	x0, #0x7fff                	// #32767
 2a8:	cmp	x12, x0
 2ac:	b.eq	9b0 <__subtf3+0x9b0>  // b.none
 2b0:	mov	w8, w5
 2b4:	mov	x3, x2
 2b8:	mov	x4, x10
 2bc:	mov	x1, x12
 2c0:	mov	x14, x15
 2c4:	nop
 2c8:	orr	x7, x4, x3
 2cc:	and	x0, x4, #0x7
 2d0:	mov	w5, #0x0                   	// #0
 2d4:	cbnz	x1, 16c <__subtf3+0x16c>
 2d8:	cbnz	x7, 160 <__subtf3+0x160>
 2dc:	mov	x6, #0x0                   	// #0
 2e0:	mov	x1, #0x0                   	// #0
 2e4:	mov	w0, #0x0                   	// #0
 2e8:	and	x7, x7, #0xffffffffffff
 2ec:	and	w1, w1, #0x7fff
 2f0:	b	1b8 <__subtf3+0x1b8>
 2f4:	add	x7, x1, #0x1
 2f8:	tst	x7, #0x7ffe
 2fc:	b.ne	5ac <__subtf3+0x5ac>  // b.any
 300:	orr	x11, x3, x4
 304:	orr	x7, x2, x10
 308:	cbnz	x1, 788 <__subtf3+0x788>
 30c:	cbz	x11, 878 <__subtf3+0x878>
 310:	cbz	x7, 88c <__subtf3+0x88c>
 314:	subs	x9, x4, x10
 318:	cmp	x4, x10
 31c:	sbc	x6, x3, x2
 320:	tbz	x6, #51, 9e0 <__subtf3+0x9e0>
 324:	subs	x4, x10, x4
 328:	mov	w8, w5
 32c:	sbc	x3, x2, x3
 330:	mov	x14, x15
 334:	orr	x7, x4, x3
 338:	b	15c <__subtf3+0x15c>
 33c:	orr	x2, x2, #0x8000000000000
 340:	b	9c <__subtf3+0x9c>
 344:	orr	x5, x2, x10
 348:	cbz	x5, 52c <__subtf3+0x52c>
 34c:	subs	w0, w0, #0x1
 350:	b.ne	258 <__subtf3+0x258>  // b.any
 354:	adds	x4, x4, x10
 358:	adc	x3, x2, x3
 35c:	nop
 360:	tbz	x3, #51, 2c8 <__subtf3+0x2c8>
 364:	add	x1, x1, #0x1
 368:	mov	x0, #0x7fff                	// #32767
 36c:	cmp	x1, x0
 370:	b.eq	82c <__subtf3+0x82c>  // b.none
 374:	and	x0, x4, #0x1
 378:	and	x2, x3, #0xfff7ffffffffffff
 37c:	orr	x4, x0, x4, lsr #1
 380:	mov	w5, #0x0                   	// #0
 384:	orr	x4, x4, x3, lsl #63
 388:	lsr	x3, x2, #1
 38c:	and	x0, x4, #0x7
 390:	b	16c <__subtf3+0x16c>
 394:	and	x2, x13, #0xc00000
 398:	cbz	x2, 3d0 <__subtf3+0x3d0>
 39c:	cmp	x2, #0x400, lsl #12
 3a0:	b.eq	3cc <__subtf3+0x3cc>  // b.none
 3a4:	cmp	x2, #0x800, lsl #12
 3a8:	and	w14, w14, #0x1
 3ac:	csel	w14, w14, wzr, eq  // eq = none
 3b0:	cbnz	w14, 3d0 <__subtf3+0x3d0>
 3b4:	mov	w1, #0x14                  	// #20
 3b8:	mov	x6, #0xffffffffffffffff    	// #-1
 3bc:	orr	w0, w0, w1
 3c0:	mov	x7, #0x1fffffffffffffff    	// #2305843009213693951
 3c4:	mov	x1, #0x7ffe                	// #32766
 3c8:	b	2e8 <__subtf3+0x2e8>
 3cc:	cbnz	x14, 3b4 <__subtf3+0x3b4>
 3d0:	mov	w1, #0x14                  	// #20
 3d4:	and	x11, x8, #0xff
 3d8:	orr	w0, w0, w1
 3dc:	mov	x2, #0x0                   	// #0
 3e0:	fmov	d0, x2
 3e4:	lsl	x11, x11, #63
 3e8:	orr	x3, x11, #0x7fff000000000000
 3ec:	fmov	v0.d[1], x3
 3f0:	str	q0, [sp, #16]
 3f4:	bl	0 <__sfp_handle_exceptions>
 3f8:	ldr	q0, [sp, #16]
 3fc:	ldp	x29, x30, [sp], #48
 400:	ret
 404:	cbz	w0, 56c <__subtf3+0x56c>
 408:	cbnz	x1, 6d0 <__subtf3+0x6d0>
 40c:	orr	x1, x3, x4
 410:	cbz	x1, 92c <__subtf3+0x92c>
 414:	cmn	w0, #0x1
 418:	b.eq	a60 <__subtf3+0xa60>  // b.none
 41c:	mov	x1, #0x7fff                	// #32767
 420:	mvn	w0, w0
 424:	cmp	x12, x1
 428:	b.ne	6e4 <__subtf3+0x6e4>  // b.any
 42c:	orr	x0, x2, x10
 430:	cbz	x0, 230 <__subtf3+0x230>
 434:	lsr	x0, x2, #50
 438:	mov	x4, x10
 43c:	eor	x0, x0, #0x1
 440:	mov	x3, x2
 444:	and	w0, w0, #0x1
 448:	mov	x1, #0x7fff                	// #32767
 44c:	nop
 450:	mov	x2, #0x7fff                	// #32767
 454:	extr	x6, x3, x4, #3
 458:	lsr	x7, x3, #3
 45c:	cmp	x1, x2
 460:	b.ne	2e8 <__subtf3+0x2e8>  // b.any
 464:	orr	x1, x7, x6
 468:	cbz	x1, ae4 <__subtf3+0xae4>
 46c:	orr	x7, x7, #0x800000000000
 470:	mov	w1, #0x7fff                	// #32767
 474:	and	x7, x7, #0xffffffffffff
 478:	b	1b8 <__subtf3+0x1b8>
 47c:	mov	w0, #0x10                  	// #16
 480:	cbz	x14, 48c <__subtf3+0x48c>
 484:	adds	x4, x4, #0x8
 488:	cinc	x3, x3, cs  // cs = hs, nlast
 48c:	and	x2, x3, #0x8000000000000
 490:	cbz	w5, 198 <__subtf3+0x198>
 494:	orr	w0, w0, #0x8
 498:	b	198 <__subtf3+0x198>
 49c:	mov	w0, #0x10                  	// #16
 4a0:	cbnz	x14, 48c <__subtf3+0x48c>
 4a4:	b	484 <__subtf3+0x484>
 4a8:	and	x2, x4, #0xf
 4ac:	mov	w0, #0x10                  	// #16
 4b0:	cmp	x2, #0x4
 4b4:	b.eq	48c <__subtf3+0x48c>  // b.none
 4b8:	adds	x4, x4, #0x4
 4bc:	cinc	x3, x3, cs  // cs = hs, nlast
 4c0:	b	48c <__subtf3+0x48c>
 4c4:	mov	x4, x6
 4c8:	and	x3, x3, #0xfff7ffffffffffff
 4cc:	sub	x1, x1, x2
 4d0:	orr	x7, x4, x3
 4d4:	and	x0, x4, #0x7
 4d8:	mov	w5, #0x0                   	// #0
 4dc:	cbz	x1, 2d8 <__subtf3+0x2d8>
 4e0:	b	16c <__subtf3+0x16c>
 4e4:	clz	x3, x4
 4e8:	add	w0, w3, #0x34
 4ec:	cmp	w0, #0x3f
 4f0:	b.le	100 <__subtf3+0x100>
 4f4:	sub	w3, w3, #0xc
 4f8:	lsl	x3, x4, x3
 4fc:	b	114 <__subtf3+0x114>
 500:	orr	x0, x3, x4
 504:	cbz	x0, 230 <__subtf3+0x230>
 508:	lsr	x0, x3, #50
 50c:	mov	x1, #0x7fff                	// #32767
 510:	eor	x0, x0, #0x1
 514:	and	w0, w0, #0x1
 518:	b	450 <__subtf3+0x450>
 51c:	orr	x2, x2, x10
 520:	cmp	x2, #0x0
 524:	cset	x5, ne  // ne = any
 528:	b	e4 <__subtf3+0xe4>
 52c:	mov	x0, #0x7fff                	// #32767
 530:	cmp	x1, x0
 534:	b.ne	2c8 <__subtf3+0x2c8>  // b.any
 538:	orr	x0, x3, x4
 53c:	cbnz	x0, 508 <__subtf3+0x508>
 540:	mov	x6, #0x0                   	// #0
 544:	mov	x7, #0x0                   	// #0
 548:	mov	w0, #0x0                   	// #0
 54c:	b	464 <__subtf3+0x464>
 550:	and	x2, x3, #0x8000000000000
 554:	mov	w0, #0x0                   	// #0
 558:	cbz	w5, 198 <__subtf3+0x198>
 55c:	mov	w0, #0x0                   	// #0
 560:	tbz	w13, #11, 198 <__subtf3+0x198>
 564:	orr	w0, w0, #0x8
 568:	b	198 <__subtf3+0x198>
 56c:	add	x7, x1, #0x1
 570:	tst	x7, #0x7ffe
 574:	b.ne	75c <__subtf3+0x75c>  // b.any
 578:	orr	x11, x3, x4
 57c:	cbnz	x1, 904 <__subtf3+0x904>
 580:	orr	x7, x2, x10
 584:	cbz	x11, 95c <__subtf3+0x95c>
 588:	cbz	x7, 88c <__subtf3+0x88c>
 58c:	adds	x4, x4, x10
 590:	adc	x3, x2, x3
 594:	tbz	x3, #51, 334 <__subtf3+0x334>
 598:	and	x3, x3, #0xfff7ffffffffffff
 59c:	and	x0, x4, #0x7
 5a0:	mov	w5, #0x0                   	// #0
 5a4:	mov	x1, #0x1                   	// #1
 5a8:	b	16c <__subtf3+0x16c>
 5ac:	subs	x9, x4, x10
 5b0:	cmp	x4, x10
 5b4:	sbc	x6, x3, x2
 5b8:	tbnz	x6, #51, 7b8 <__subtf3+0x7b8>
 5bc:	orr	x7, x9, x6
 5c0:	cbnz	x7, 898 <__subtf3+0x898>
 5c4:	and	x13, x13, #0xc00000
 5c8:	mov	x6, #0x0                   	// #0
 5cc:	cmp	x13, #0x800, lsl #12
 5d0:	mov	x1, #0x0                   	// #0
 5d4:	cset	w8, eq  // eq = none
 5d8:	b	2e8 <__subtf3+0x2e8>
 5dc:	mov	x1, #0x7fff                	// #32767
 5e0:	neg	w0, w0
 5e4:	orr	x3, x3, #0x8000000000000
 5e8:	cmp	x12, x1
 5ec:	b.eq	220 <__subtf3+0x220>  // b.none
 5f0:	cmp	w0, #0x74
 5f4:	b.gt	6ac <__subtf3+0x6ac>
 5f8:	cmp	w0, #0x3f
 5fc:	b.gt	8b0 <__subtf3+0x8b0>
 600:	mov	w1, #0x40                  	// #64
 604:	sub	w1, w1, w0
 608:	lsr	x6, x4, x0
 60c:	lsl	x4, x4, x1
 610:	cmp	x4, #0x0
 614:	lsl	x4, x3, x1
 618:	cset	x1, ne  // ne = any
 61c:	orr	x4, x4, x6
 620:	lsr	x0, x3, x0
 624:	orr	x4, x4, x1
 628:	sub	x2, x2, x0
 62c:	b	6b8 <__subtf3+0x6b8>
 630:	orr	x2, x2, x10
 634:	cmp	x2, #0x0
 638:	cset	x0, ne  // ne = any
 63c:	adds	x4, x0, x4
 640:	cinc	x3, x3, cs  // cs = hs, nlast
 644:	b	360 <__subtf3+0x360>
 648:	mov	w2, #0x80                  	// #128
 64c:	sub	w2, w2, w0
 650:	cmp	w0, #0x40
 654:	sub	w4, w1, #0x3f
 658:	lsl	x0, x3, x2
 65c:	orr	x0, x6, x0
 660:	csel	x6, x0, x6, ne  // ne = any
 664:	lsr	x4, x3, x4
 668:	cmp	x6, #0x0
 66c:	mov	x3, #0x0                   	// #0
 670:	cset	x0, ne  // ne = any
 674:	orr	x4, x0, x4
 678:	mov	x7, x4
 67c:	b	15c <__subtf3+0x15c>
 680:	mov	w6, #0x80                  	// #128
 684:	sub	w6, w6, w0
 688:	subs	w0, w0, #0x40
 68c:	lsl	x6, x2, x6
 690:	orr	x6, x10, x6
 694:	csel	x10, x6, x10, ne  // ne = any
 698:	lsr	x2, x2, x0
 69c:	cmp	x10, #0x0
 6a0:	cset	x5, ne  // ne = any
 6a4:	orr	x5, x5, x2
 6a8:	b	e4 <__subtf3+0xe4>
 6ac:	orr	x3, x3, x4
 6b0:	cmp	x3, #0x0
 6b4:	cset	x4, ne  // ne = any
 6b8:	subs	x4, x10, x4
 6bc:	mov	w8, w5
 6c0:	sbc	x3, x2, xzr
 6c4:	mov	x1, x12
 6c8:	mov	x14, x15
 6cc:	b	ec <__subtf3+0xec>
 6d0:	mov	x1, #0x7fff                	// #32767
 6d4:	neg	w0, w0
 6d8:	orr	x3, x3, #0x8000000000000
 6dc:	cmp	x12, x1
 6e0:	b.eq	42c <__subtf3+0x42c>  // b.none
 6e4:	cmp	w0, #0x74
 6e8:	b.gt	8a0 <__subtf3+0x8a0>
 6ec:	cmp	w0, #0x3f
 6f0:	b.gt	968 <__subtf3+0x968>
 6f4:	mov	w1, #0x40                  	// #64
 6f8:	sub	w1, w1, w0
 6fc:	lsr	x5, x4, x0
 700:	lsl	x4, x4, x1
 704:	cmp	x4, #0x0
 708:	lsl	x4, x3, x1
 70c:	cset	x1, ne  // ne = any
 710:	orr	x4, x4, x5
 714:	lsr	x0, x3, x0
 718:	orr	x4, x4, x1
 71c:	add	x2, x2, x0
 720:	adds	x4, x4, x10
 724:	mov	x1, x12
 728:	cinc	x3, x2, cs  // cs = hs, nlast
 72c:	b	360 <__subtf3+0x360>
 730:	mov	w5, #0x80                  	// #128
 734:	sub	w5, w5, w0
 738:	subs	w0, w0, #0x40
 73c:	lsl	x5, x2, x5
 740:	orr	x5, x10, x5
 744:	csel	x10, x5, x10, ne  // ne = any
 748:	lsr	x2, x2, x0
 74c:	cmp	x10, #0x0
 750:	cset	x0, ne  // ne = any
 754:	orr	x0, x0, x2
 758:	b	63c <__subtf3+0x63c>
 75c:	mov	x0, #0x7fff                	// #32767
 760:	cmp	x7, x0
 764:	b.eq	82c <__subtf3+0x82c>  // b.none
 768:	adds	x4, x4, x10
 76c:	mov	x1, x7
 770:	adc	x3, x2, x3
 774:	mov	w5, #0x0                   	// #0
 778:	ubfx	x0, x4, #1, #3
 77c:	extr	x4, x3, x4, #1
 780:	lsr	x3, x3, #1
 784:	b	16c <__subtf3+0x16c>
 788:	mov	x13, #0x7fff                	// #32767
 78c:	cmp	x1, x13
 790:	b.eq	7d0 <__subtf3+0x7d0>  // b.none
 794:	cmp	x12, x13
 798:	b.eq	9c8 <__subtf3+0x9c8>  // b.none
 79c:	cbnz	x11, 7e8 <__subtf3+0x7e8>
 7a0:	cbnz	x7, 9d8 <__subtf3+0x9d8>
 7a4:	mov	w8, #0x0                   	// #0
 7a8:	mov	x6, #0xffffffffffffffff    	// #-1
 7ac:	mov	x7, #0xffffffffffff        	// #281474976710655
 7b0:	mov	w0, #0x1                   	// #1
 7b4:	b	46c <__subtf3+0x46c>
 7b8:	cmp	x10, x4
 7bc:	mov	w8, w5
 7c0:	sbc	x6, x2, x3
 7c4:	sub	x4, x10, x4
 7c8:	mov	x14, x15
 7cc:	b	f4 <__subtf3+0xf4>
 7d0:	cbz	x11, 9c0 <__subtf3+0x9c0>
 7d4:	lsr	x0, x3, #50
 7d8:	cmp	x12, x1
 7dc:	eor	x0, x0, #0x1
 7e0:	and	w0, w0, #0x1
 7e4:	b.eq	9c8 <__subtf3+0x9c8>  // b.none
 7e8:	cbz	x7, 448 <__subtf3+0x448>
 7ec:	bfi	x6, x3, #61, #3
 7f0:	lsr	x7, x3, #3
 7f4:	tbz	x3, #50, 810 <__subtf3+0x810>
 7f8:	lsr	x1, x2, #3
 7fc:	tbnz	x2, #50, 810 <__subtf3+0x810>
 800:	mov	x6, x9
 804:	mov	w8, w5
 808:	bfi	x6, x2, #61, #3
 80c:	mov	x7, x1
 810:	extr	x7, x7, x6, #61
 814:	bfi	x6, x7, #61, #3
 818:	lsr	x7, x7, #3
 81c:	b	464 <__subtf3+0x464>
 820:	subs	x4, x4, x10
 824:	sbc	x3, x3, x2
 828:	b	ec <__subtf3+0xec>
 82c:	ands	x2, x13, #0xc00000
 830:	b.eq	8dc <__subtf3+0x8dc>  // b.none
 834:	cmp	x2, #0x400, lsl #12
 838:	eor	w0, w8, #0x1
 83c:	cset	w1, eq  // eq = none
 840:	tst	w1, w0
 844:	b.ne	a9c <__subtf3+0xa9c>  // b.any
 848:	cmp	x2, #0x800, lsl #12
 84c:	b.eq	9fc <__subtf3+0x9fc>  // b.none
 850:	cmp	x2, #0x400, lsl #12
 854:	mov	w0, #0x14                  	// #20
 858:	b.ne	398 <__subtf3+0x398>  // b.any
 85c:	mov	x3, #0xffffffffffffffff    	// #-1
 860:	mov	x1, #0x7ffe                	// #32766
 864:	mov	x4, x3
 868:	mov	w5, #0x0                   	// #0
 86c:	mov	w0, #0x14                  	// #20
 870:	cbnz	x14, 48c <__subtf3+0x48c>
 874:	b	484 <__subtf3+0x484>
 878:	cbz	x7, 948 <__subtf3+0x948>
 87c:	mov	w8, w5
 880:	mov	x3, x2
 884:	mov	x4, x10
 888:	mov	x14, x15
 88c:	mov	x1, #0x0                   	// #0
 890:	mov	x2, #0x0                   	// #0
 894:	b	55c <__subtf3+0x55c>
 898:	mov	x4, x9
 89c:	b	f4 <__subtf3+0xf4>
 8a0:	orr	x3, x3, x4
 8a4:	cmp	x3, #0x0
 8a8:	cset	x4, ne  // ne = any
 8ac:	b	720 <__subtf3+0x720>
 8b0:	mov	w1, #0x80                  	// #128
 8b4:	sub	w1, w1, w0
 8b8:	subs	w0, w0, #0x40
 8bc:	lsl	x1, x3, x1
 8c0:	orr	x1, x4, x1
 8c4:	csel	x4, x1, x4, ne  // ne = any
 8c8:	lsr	x3, x3, x0
 8cc:	cmp	x4, #0x0
 8d0:	cset	x4, ne  // ne = any
 8d4:	orr	x4, x4, x3
 8d8:	b	6b8 <__subtf3+0x6b8>
 8dc:	mov	w0, #0x14                  	// #20
 8e0:	b	3dc <__subtf3+0x3dc>
 8e4:	lsr	x0, x2, #50
 8e8:	mov	w8, w5
 8ec:	eor	x0, x0, #0x1
 8f0:	mov	x4, x10
 8f4:	and	w0, w0, #0x1
 8f8:	mov	x3, x2
 8fc:	mov	x1, #0x7fff                	// #32767
 900:	b	450 <__subtf3+0x450>
 904:	mov	x7, #0x7fff                	// #32767
 908:	cmp	x1, x7
 90c:	b.eq	a18 <__subtf3+0xa18>  // b.none
 910:	cmp	x12, x7
 914:	b.eq	a88 <__subtf3+0xa88>  // b.none
 918:	cbnz	x11, a30 <__subtf3+0xa30>
 91c:	mov	x3, x2
 920:	mov	x4, x10
 924:	mov	x1, #0x7fff                	// #32767
 928:	b	450 <__subtf3+0x450>
 92c:	mov	x0, #0x7fff                	// #32767
 930:	cmp	x12, x0
 934:	b.eq	a70 <__subtf3+0xa70>  // b.none
 938:	mov	x3, x2
 93c:	mov	x4, x10
 940:	mov	x1, x12
 944:	b	2c8 <__subtf3+0x2c8>
 948:	and	x13, x13, #0xc00000
 94c:	mov	x6, #0x0                   	// #0
 950:	cmp	x13, #0x800, lsl #12
 954:	cset	w8, eq  // eq = none
 958:	b	2e8 <__subtf3+0x2e8>
 95c:	mov	x3, x2
 960:	mov	x4, x10
 964:	b	15c <__subtf3+0x15c>
 968:	mov	w1, #0x80                  	// #128
 96c:	sub	w1, w1, w0
 970:	subs	w0, w0, #0x40
 974:	lsl	x1, x3, x1
 978:	orr	x1, x4, x1
 97c:	csel	x4, x1, x4, ne  // ne = any
 980:	lsr	x3, x3, x0
 984:	cmp	x4, #0x0
 988:	cset	x4, ne  // ne = any
 98c:	orr	x4, x4, x3
 990:	b	720 <__subtf3+0x720>
 994:	cmp	x10, x4
 998:	mov	w8, w5
 99c:	sbc	x3, x2, x3
 9a0:	sub	x4, x10, x4
 9a4:	mov	x1, x12
 9a8:	mov	x14, x15
 9ac:	b	ec <__subtf3+0xec>
 9b0:	orr	x0, x2, x10
 9b4:	cbnz	x0, 8e4 <__subtf3+0x8e4>
 9b8:	mov	w8, w5
 9bc:	b	540 <__subtf3+0x540>
 9c0:	cmp	x12, x1
 9c4:	b.ne	7a0 <__subtf3+0x7a0>  // b.any
 9c8:	cbz	x7, a7c <__subtf3+0xa7c>
 9cc:	tst	x2, #0x4000000000000
 9d0:	csinc	w0, w0, wzr, ne  // ne = any
 9d4:	cbnz	x11, 7ec <__subtf3+0x7ec>
 9d8:	mov	w8, w5
 9dc:	b	91c <__subtf3+0x91c>
 9e0:	orr	x7, x9, x6
 9e4:	cbz	x7, 948 <__subtf3+0x948>
 9e8:	mov	x3, x6
 9ec:	and	x0, x9, #0x7
 9f0:	mov	x4, x9
 9f4:	mov	w5, #0x1                   	// #1
 9f8:	b	16c <__subtf3+0x16c>
 9fc:	cbnz	x11, aa8 <__subtf3+0xaa8>
 a00:	mov	x3, #0xffffffffffffffff    	// #-1
 a04:	mov	w8, #0x0                   	// #0
 a08:	mov	x4, x3
 a0c:	mov	x1, #0x7ffe                	// #32766
 a10:	mov	w0, #0x14                  	// #20
 a14:	b	19c <__subtf3+0x19c>
 a18:	cbz	x11, ab4 <__subtf3+0xab4>
 a1c:	lsr	x0, x3, #50
 a20:	cmp	x12, x1
 a24:	eor	x0, x0, #0x1
 a28:	and	w0, w0, #0x1
 a2c:	b.eq	ad4 <__subtf3+0xad4>  // b.none
 a30:	orr	x10, x2, x10
 a34:	cbz	x10, 448 <__subtf3+0x448>
 a38:	bfi	x6, x3, #61, #3
 a3c:	lsr	x7, x3, #3
 a40:	tbz	x3, #50, 810 <__subtf3+0x810>
 a44:	lsr	x1, x2, #3
 a48:	tbnz	x2, #50, 810 <__subtf3+0x810>
 a4c:	and	x6, x9, #0x1fffffffffffffff
 a50:	mov	w8, w5
 a54:	orr	x6, x6, x2, lsl #61
 a58:	mov	x7, x1
 a5c:	b	810 <__subtf3+0x810>
 a60:	adds	x4, x4, x10
 a64:	mov	x1, x12
 a68:	adc	x3, x2, x3
 a6c:	b	360 <__subtf3+0x360>
 a70:	orr	x0, x2, x10
 a74:	cbz	x0, 540 <__subtf3+0x540>
 a78:	b	434 <__subtf3+0x434>
 a7c:	cbz	x11, 7a4 <__subtf3+0x7a4>
 a80:	mov	x1, #0x7fff                	// #32767
 a84:	b	450 <__subtf3+0x450>
 a88:	orr	x1, x2, x10
 a8c:	cbnz	x1, ac4 <__subtf3+0xac4>
 a90:	cbz	x11, 540 <__subtf3+0x540>
 a94:	mov	x1, #0x7fff                	// #32767
 a98:	b	450 <__subtf3+0x450>
 a9c:	mov	w0, #0x14                  	// #20
 aa0:	mov	x11, #0x0                   	// #0
 aa4:	b	3dc <__subtf3+0x3dc>
 aa8:	mov	w0, #0x14                  	// #20
 aac:	mov	x11, #0x1                   	// #1
 ab0:	b	3dc <__subtf3+0x3dc>
 ab4:	cmp	x12, x1
 ab8:	b.ne	91c <__subtf3+0x91c>  // b.any
 abc:	orr	x1, x2, x10
 ac0:	cbz	x1, 540 <__subtf3+0x540>
 ac4:	tst	x2, #0x4000000000000
 ac8:	csinc	w0, w0, wzr, ne  // ne = any
 acc:	cbnz	x11, a38 <__subtf3+0xa38>
 ad0:	b	91c <__subtf3+0x91c>
 ad4:	orr	x1, x2, x10
 ad8:	cbnz	x1, ac4 <__subtf3+0xac4>
 adc:	mov	x1, #0x7fff                	// #32767
 ae0:	b	450 <__subtf3+0x450>
 ae4:	mov	x6, #0x0                   	// #0
 ae8:	mov	w1, #0x7fff                	// #32767
 aec:	mov	x7, #0x0                   	// #0
 af0:	b	1b8 <__subtf3+0x1b8>

unordtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__unordtf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x3, x1, [sp, #16]
  14:	ldp	x2, x0, [sp, #32]
  18:	mrs	x4, fpcr
  1c:	ubfx	x5, x1, #48, #15
  20:	mov	x4, x3
  24:	mov	x3, x2
  28:	ubfx	x2, x1, #0, #48
  2c:	mov	x1, #0x7fff                	// #32767
  30:	ubfx	x7, x0, #0, #48
  34:	cmp	x5, x1
  38:	ubfx	x1, x0, #48, #15
  3c:	b.eq	58 <__unordtf2+0x58>  // b.none
  40:	mov	x6, #0x7fff                	// #32767
  44:	mov	w0, #0x0                   	// #0
  48:	cmp	x1, x6
  4c:	b.eq	7c <__unordtf2+0x7c>  // b.none
  50:	ldp	x29, x30, [sp], #48
  54:	ret
  58:	orr	x0, x4, x2
  5c:	cbz	x0, 40 <__unordtf2+0x40>
  60:	tst	x2, #0x800000000000
  64:	b.ne	9c <__unordtf2+0x9c>  // b.any
  68:	mov	w0, #0x1                   	// #1
  6c:	bl	0 <__sfp_handle_exceptions>
  70:	mov	w0, #0x1                   	// #1
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x3, x7, x3
  80:	cbz	x3, 50 <__unordtf2+0x50>
  84:	cmp	x5, x1
  88:	b.eq	b0 <__unordtf2+0xb0>  // b.none
  8c:	tst	x7, #0x800000000000
  90:	mov	w0, #0x1                   	// #1
  94:	b.ne	50 <__unordtf2+0x50>  // b.any
  98:	b	68 <__unordtf2+0x68>
  9c:	cmp	x1, x5
  a0:	mov	w0, #0x1                   	// #1
  a4:	b.ne	50 <__unordtf2+0x50>  // b.any
  a8:	orr	x3, x7, x3
  ac:	b	c0 <__unordtf2+0xc0>
  b0:	orr	x4, x4, x2
  b4:	cbz	x4, 8c <__unordtf2+0x8c>
  b8:	tst	x2, #0x800000000000
  bc:	b.eq	68 <__unordtf2+0x68>  // b.none
  c0:	mov	w0, #0x1                   	// #1
  c4:	cbz	x3, 50 <__unordtf2+0x50>
  c8:	b	8c <__unordtf2+0x8c>

fixtfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	mrs	x2, fpcr
  18:	mov	x3, x0
  1c:	ubfx	x4, x1, #48, #15
  20:	mov	x0, #0x3ffe                	// #16382
  24:	cmp	x4, x0
  28:	ubfx	x0, x1, #0, #48
  2c:	b.gt	58 <__fixtfsi+0x58>
  30:	cbnz	x4, d4 <__fixtfsi+0xd4>
  34:	orr	x0, x0, x3
  38:	mov	w19, #0x0                   	// #0
  3c:	cbz	x0, 48 <__fixtfsi+0x48>
  40:	mov	w0, #0x10                  	// #16
  44:	bl	0 <__sfp_handle_exceptions>
  48:	mov	w0, w19
  4c:	ldr	x19, [sp, #16]
  50:	ldp	x29, x30, [sp], #48
  54:	ret
  58:	lsr	x19, x1, #63
  5c:	mov	x5, #0x401d                	// #16413
  60:	and	w2, w19, #0xff
  64:	cmp	x4, x5
  68:	b.le	9c <__fixtfsi+0x9c>
  6c:	mov	x1, #0x401e                	// #16414
  70:	cmp	x4, x1
  74:	mov	w19, #0x7fffffff            	// #2147483647
  78:	csel	w1, w2, wzr, eq  // eq = none
  7c:	add	w19, w2, w19
  80:	cbz	w1, e4 <__fixtfsi+0xe4>
  84:	cmp	xzr, x0, lsr #17
  88:	b.ne	e4 <__fixtfsi+0xe4>  // b.any
  8c:	orr	x0, x3, x0, lsl #47
  90:	cbz	x0, 48 <__fixtfsi+0x48>
  94:	mov	w0, #0x10                  	// #16
  98:	b	44 <__fixtfsi+0x44>
  9c:	mov	x1, x4
  a0:	orr	x0, x0, #0x1000000000000
  a4:	mov	w19, #0x402f                	// #16431
  a8:	mov	w4, #0xffffc011            	// #-16367
  ac:	add	w4, w1, w4
  b0:	sub	w1, w19, w1
  b4:	cmp	w2, #0x0
  b8:	lsr	x19, x0, x1
  bc:	cneg	w19, w19, ne  // ne = any
  c0:	lsl	x0, x0, x4
  c4:	orr	x0, x0, x3
  c8:	cbz	x0, 48 <__fixtfsi+0x48>
  cc:	mov	w0, #0x10                  	// #16
  d0:	b	44 <__fixtfsi+0x44>
  d4:	mov	w19, #0x0                   	// #0
  d8:	mov	w0, #0x10                  	// #16
  dc:	bl	0 <__sfp_handle_exceptions>
  e0:	b	48 <__fixtfsi+0x48>
  e4:	mov	w0, #0x1                   	// #1
  e8:	bl	0 <__sfp_handle_exceptions>
  ec:	b	48 <__fixtfsi+0x48>

fixunstfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x3, x1, #48, #15
  1c:	mov	x4, #0x3ffe                	// #16382
  20:	mov	x2, x0
  24:	cmp	x3, x4
  28:	ubfx	x19, x1, #0, #48
  2c:	b.gt	6c <__fixunstfsi+0x6c>
  30:	cbnz	x3, 50 <__fixunstfsi+0x50>
  34:	orr	x0, x0, x19
  38:	mov	w19, #0x0                   	// #0
  3c:	cbnz	x0, 50 <__fixunstfsi+0x50>
  40:	mov	w0, w19
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	mov	w0, #0x10                  	// #16
  54:	mov	w19, #0x0                   	// #0
  58:	bl	0 <__sfp_handle_exceptions>
  5c:	mov	w0, w19
  60:	ldr	x19, [sp, #16]
  64:	ldp	x29, x30, [sp], #48
  68:	ret
  6c:	lsr	x0, x1, #63
  70:	mov	x4, #0x401f                	// #16415
  74:	and	w0, w0, #0xff
  78:	mov	x5, #0x401e                	// #16414
  7c:	ands	x6, x0, #0xff
  80:	csel	x4, x4, x5, eq  // eq = none
  84:	cmp	x4, x3
  88:	b.le	a0 <__fixunstfsi+0xa0>
  8c:	cbz	x6, b0 <__fixunstfsi+0xb0>
  90:	mov	w0, #0x1                   	// #1
  94:	mov	w19, #0x0                   	// #0
  98:	bl	0 <__sfp_handle_exceptions>
  9c:	b	5c <__fixunstfsi+0x5c>
  a0:	sub	w19, w0, #0x1
  a4:	mov	w0, #0x1                   	// #1
  a8:	bl	0 <__sfp_handle_exceptions>
  ac:	b	5c <__fixunstfsi+0x5c>
  b0:	mov	x1, x3
  b4:	orr	x0, x19, #0x1000000000000
  b8:	mov	w3, #0xffffc011            	// #-16367
  bc:	mov	w19, #0x402f                	// #16431
  c0:	add	w3, w1, w3
  c4:	sub	w1, w19, w1
  c8:	lsl	x3, x0, x3
  cc:	orr	x2, x3, x2
  d0:	lsr	x19, x0, x1
  d4:	mov	w0, #0x10                  	// #16
  d8:	cbnz	x2, 58 <__fixunstfsi+0x58>
  dc:	b	40 <__fixunstfsi+0x40>

floatsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	cmp	w0, #0x0
   4:	cbz	w0, 50 <__floatsitf+0x50>
   8:	cneg	w1, w0, lt  // lt = tstop
   c:	mov	w4, #0x403e                	// #16446
  10:	clz	x3, x1
  14:	mov	w2, #0x402f                	// #16431
  18:	sub	w4, w4, w3
  1c:	lsr	w0, w0, #31
  20:	sub	w2, w2, w4
  24:	mov	x3, #0x0                   	// #0
  28:	and	w4, w4, #0x7fff
  2c:	lsl	x1, x1, x2
  30:	and	x1, x1, #0xffffffffffff
  34:	orr	w0, w4, w0, lsl #15
  38:	mov	x2, #0x0                   	// #0
  3c:	bfxil	x3, x1, #0, #48
  40:	fmov	d0, x2
  44:	bfi	x3, x0, #48, #16
  48:	fmov	v0.d[1], x3
  4c:	ret
  50:	mov	w4, #0x0                   	// #0
  54:	mov	x1, #0x0                   	// #0
  58:	mov	w0, #0x0                   	// #0
  5c:	mov	x3, #0x0                   	// #0
  60:	orr	w0, w4, w0, lsl #15
  64:	bfxil	x3, x1, #0, #48
  68:	mov	x2, #0x0                   	// #0
  6c:	fmov	d0, x2
  70:	bfi	x3, x0, #48, #16
  74:	fmov	v0.d[1], x3
  78:	ret

floatunsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	cbz	w0, 44 <__floatunsitf+0x44>
   4:	mov	w0, w0
   8:	mov	w1, #0x403e                	// #16446
   c:	clz	x3, x0
  10:	mov	w2, #0x402f                	// #16431
  14:	sub	w1, w1, w3
  18:	mov	x3, #0x0                   	// #0
  1c:	sub	w2, w2, w1
  20:	and	w1, w1, #0x7fff
  24:	lsl	x0, x0, x2
  28:	and	x0, x0, #0xffffffffffff
  2c:	mov	x2, #0x0                   	// #0
  30:	fmov	d0, x2
  34:	bfxil	x3, x0, #0, #48
  38:	bfi	x3, x1, #48, #16
  3c:	fmov	v0.d[1], x3
  40:	ret
  44:	mov	x0, #0x0                   	// #0
  48:	mov	x3, #0x0                   	// #0
  4c:	bfxil	x3, x0, #0, #48
  50:	mov	x2, #0x0                   	// #0
  54:	fmov	d0, x2
  58:	mov	w1, #0x0                   	// #0
  5c:	bfi	x3, x1, #48, #16
  60:	fmov	v0.d[1], x3
  64:	ret

fixtfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x2, x0, [sp, #32]
  14:	mrs	x1, fpcr
  18:	ubfx	x19, x0, #48, #15
  1c:	mov	x3, #0x3ffe                	// #16382
  20:	ubfx	x4, x0, #0, #48
  24:	cmp	x19, x3
  28:	b.gt	50 <__fixtfdi+0x50>
  2c:	cbnz	x19, f0 <__fixtfdi+0xf0>
  30:	orr	x2, x4, x2
  34:	cbz	x2, 40 <__fixtfdi+0x40>
  38:	mov	w0, #0x10                  	// #16
  3c:	bl	0 <__sfp_handle_exceptions>
  40:	mov	x0, x19
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	lsr	x1, x0, #63
  54:	and	w3, w1, #0xff
  58:	mov	x1, #0x403d                	// #16445
  5c:	cmp	x19, x1
  60:	b.le	98 <__fixtfdi+0x98>
  64:	mov	x0, #0x403e                	// #16446
  68:	and	x5, x3, #0xff
  6c:	cmp	x19, x0
  70:	mov	x0, #0x7fffffffffffffff    	// #9223372036854775807
  74:	csel	w1, w3, wzr, eq  // eq = none
  78:	add	x19, x5, x0
  7c:	cbz	w1, 100 <__fixtfdi+0x100>
  80:	extr	x4, x4, x2, #49
  84:	cbnz	x4, 100 <__fixtfdi+0x100>
  88:	cmp	xzr, x2, lsl #15
  8c:	b.eq	40 <__fixtfdi+0x40>  // b.none
  90:	mov	w0, #0x10                  	// #16
  94:	b	3c <__fixtfdi+0x3c>
  98:	mov	x5, #0x406f                	// #16495
  9c:	sub	x1, x5, x19
  a0:	mov	x0, x19
  a4:	cmp	x1, #0x3f
  a8:	orr	x19, x4, #0x1000000000000
  ac:	b.le	10c <__fixtfdi+0x10c>
  b0:	mov	w4, #0xffffc011            	// #-16367
  b4:	add	w4, w0, w4
  b8:	cmp	x1, #0x40
  bc:	mov	w1, #0x402f                	// #16431
  c0:	sub	w0, w1, w0
  c4:	lsl	x1, x19, x4
  c8:	orr	x1, x2, x1
  cc:	csel	x2, x1, x2, ne  // ne = any
  d0:	lsr	x19, x19, x0
  d4:	cmp	x2, #0x0
  d8:	cset	w0, ne  // ne = any
  dc:	cmp	w3, #0x0
  e0:	cneg	x19, x19, ne  // ne = any
  e4:	cbz	w0, 40 <__fixtfdi+0x40>
  e8:	mov	w0, #0x10                  	// #16
  ec:	b	3c <__fixtfdi+0x3c>
  f0:	mov	x19, #0x0                   	// #0
  f4:	mov	w0, #0x10                  	// #16
  f8:	bl	0 <__sfp_handle_exceptions>
  fc:	b	40 <__fixtfdi+0x40>
 100:	mov	w0, #0x1                   	// #1
 104:	bl	0 <__sfp_handle_exceptions>
 108:	b	40 <__fixtfdi+0x40>
 10c:	mov	w6, #0xffffbfd1            	// #-16431
 110:	add	w4, w0, w6
 114:	sub	w0, w5, w0
 118:	lsl	x1, x2, x4
 11c:	cmp	x1, #0x0
 120:	lsr	x2, x2, x0
 124:	cset	w0, ne  // ne = any
 128:	lsl	x19, x19, x4
 12c:	orr	x19, x2, x19
 130:	b	dc <__fixtfdi+0xdc>

fixunstfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldr	x19, [sp, #32]
  14:	ldr	x1, [sp, #40]
  18:	mrs	x0, fpcr
  1c:	ubfx	x3, x1, #48, #15
  20:	mov	x2, x19
  24:	mov	x4, #0x3ffe                	// #16382
  28:	ubfx	x19, x1, #0, #48
  2c:	cmp	x3, x4
  30:	b.gt	5c <__fixunstfdi+0x5c>
  34:	cbnz	x3, 40 <__fixunstfdi+0x40>
  38:	orr	x19, x2, x19
  3c:	cbz	x19, 4c <__fixunstfdi+0x4c>
  40:	mov	w0, #0x10                  	// #16
  44:	mov	x19, #0x0                   	// #0
  48:	bl	0 <__sfp_handle_exceptions>
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	lsr	x0, x1, #63
  60:	mov	x4, #0x403f                	// #16447
  64:	and	w0, w0, #0xff
  68:	and	x5, x0, #0xff
  6c:	sub	x4, x4, x5
  70:	cmp	x4, x3
  74:	b.le	c8 <__fixunstfdi+0xc8>
  78:	cbnz	x5, dc <__fixunstfdi+0xdc>
  7c:	mov	x1, x3
  80:	mov	x0, #0x406f                	// #16495
  84:	sub	x3, x0, x3
  88:	orr	x4, x19, #0x1000000000000
  8c:	cmp	x3, #0x3f
  90:	b.gt	ec <__fixunstfdi+0xec>
  94:	mov	w3, #0xffffbfd1            	// #-16431
  98:	add	w3, w1, w3
  9c:	sub	w1, w0, w1
  a0:	lsl	x0, x2, x3
  a4:	cmp	x0, #0x0
  a8:	lsr	x19, x2, x1
  ac:	cset	w0, ne  // ne = any
  b0:	lsl	x4, x4, x3
  b4:	orr	x19, x19, x4
  b8:	cbz	w0, 4c <__fixunstfdi+0x4c>
  bc:	mov	w0, #0x10                  	// #16
  c0:	bl	0 <__sfp_handle_exceptions>
  c4:	b	4c <__fixunstfdi+0x4c>
  c8:	eor	w19, w0, #0x1
  cc:	mov	w0, #0x1                   	// #1
  d0:	sbfx	x19, x19, #0, #1
  d4:	bl	0 <__sfp_handle_exceptions>
  d8:	b	4c <__fixunstfdi+0x4c>
  dc:	mov	w0, #0x1                   	// #1
  e0:	mov	x19, #0x0                   	// #0
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	b	4c <__fixunstfdi+0x4c>
  ec:	mov	w0, #0xffffc011            	// #-16367
  f0:	add	w5, w1, w0
  f4:	mov	w0, #0x402f                	// #16431
  f8:	cmp	x3, #0x40
  fc:	sub	w1, w0, w1
 100:	lsl	x0, x4, x5
 104:	orr	x0, x2, x0
 108:	csel	x2, x0, x2, ne  // ne = any
 10c:	lsr	x19, x4, x1
 110:	cmp	x2, #0x0
 114:	cset	w0, ne  // ne = any
 118:	b	b8 <__fixunstfdi+0xb8>

floatditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	cmp	x0, #0x0
   4:	cbz	x0, 68 <__floatditf+0x68>
   8:	cneg	x1, x0, lt  // lt = tstop
   c:	mov	w2, #0x403e                	// #16446
  10:	clz	x3, x1
  14:	mov	x4, #0x406f                	// #16495
  18:	sub	w2, w2, w3
  1c:	lsr	x0, x0, #63
  20:	and	w0, w0, #0xff
  24:	and	w5, w2, #0x7fff
  28:	sub	x3, x4, w2, sxtw
  2c:	cmp	x3, #0x3f
  30:	b.gt	94 <__floatditf+0x94>
  34:	sub	w4, w4, w2
  38:	mov	w3, #0xffffbfd1            	// #-16431
  3c:	add	w2, w2, w3
  40:	mov	x3, #0x0                   	// #0
  44:	lsl	x4, x1, x4
  48:	orr	w0, w5, w0, lsl #15
  4c:	lsr	x1, x1, x2
  50:	and	x1, x1, #0xffffffffffff
  54:	fmov	d0, x4
  58:	bfxil	x3, x1, #0, #48
  5c:	bfi	x3, x0, #48, #16
  60:	fmov	v0.d[1], x3
  64:	ret
  68:	mov	w5, #0x0                   	// #0
  6c:	mov	x1, #0x0                   	// #0
  70:	mov	w0, #0x0                   	// #0
  74:	mov	x3, #0x0                   	// #0
  78:	orr	w0, w5, w0, lsl #15
  7c:	bfxil	x3, x1, #0, #48
  80:	mov	x4, #0x0                   	// #0
  84:	fmov	d0, x4
  88:	bfi	x3, x0, #48, #16
  8c:	fmov	v0.d[1], x3
  90:	ret
  94:	mov	w3, #0x402f                	// #16431
  98:	sub	w2, w3, w2
  9c:	mov	x3, #0x0                   	// #0
  a0:	orr	w0, w5, w0, lsl #15
  a4:	lsl	x1, x1, x2
  a8:	and	x1, x1, #0xffffffffffff
  ac:	mov	x4, #0x0                   	// #0
  b0:	fmov	d0, x4
  b4:	bfxil	x3, x1, #0, #48
  b8:	bfi	x3, x0, #48, #16
  bc:	fmov	v0.d[1], x3
  c0:	ret

floatunditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	cbz	x0, 54 <__floatunditf+0x54>
   4:	clz	x2, x0
   8:	mov	w1, #0x403e                	// #16446
   c:	sub	w1, w1, w2
  10:	mov	x2, #0x406f                	// #16495
  14:	and	w4, w1, #0x7fff
  18:	sub	x3, x2, w1, sxtw
  1c:	cmp	x3, #0x3f
  20:	b.gt	74 <__floatunditf+0x74>
  24:	sub	w2, w2, w1
  28:	mov	w3, #0xffffbfd1            	// #-16431
  2c:	add	w1, w1, w3
  30:	mov	x3, #0x0                   	// #0
  34:	lsr	x1, x0, x1
  38:	and	x1, x1, #0xffffffffffff
  3c:	lsl	x0, x0, x2
  40:	fmov	d0, x0
  44:	bfxil	x3, x1, #0, #48
  48:	bfi	x3, x4, #48, #16
  4c:	fmov	v0.d[1], x3
  50:	ret
  54:	mov	x1, #0x0                   	// #0
  58:	mov	x3, #0x0                   	// #0
  5c:	bfxil	x3, x1, #0, #48
  60:	fmov	d0, x0
  64:	mov	w4, #0x0                   	// #0
  68:	bfi	x3, x4, #48, #16
  6c:	fmov	v0.d[1], x3
  70:	ret
  74:	mov	w2, #0x402f                	// #16431
  78:	sub	w1, w2, w1
  7c:	mov	x3, #0x0                   	// #0
  80:	lsl	x1, x0, x1
  84:	and	x1, x1, #0xffffffffffff
  88:	mov	x0, #0x0                   	// #0
  8c:	fmov	d0, x0
  90:	bfxil	x3, x1, #0, #48
  94:	bfi	x3, x4, #48, #16
  98:	fmov	v0.d[1], x3
  9c:	ret

fixtfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x3, x0, [sp, #32]
  14:	mrs	x1, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	mov	x6, x3
  20:	mov	x5, #0x3ffe                	// #16382
  24:	ubfx	x3, x0, #0, #48
  28:	cmp	x4, x5
  2c:	b.gt	5c <__fixtfti+0x5c>
  30:	cbz	x4, 11c <__fixtfti+0x11c>
  34:	mov	x19, #0x0                   	// #0
  38:	mov	x1, #0x0                   	// #0
  3c:	mov	w0, #0x10                  	// #16
  40:	str	x1, [sp, #32]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	x1, [sp, #32]
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	lsr	x2, x0, #63
  60:	mov	x1, #0x407d                	// #16509
  64:	and	w2, w2, #0xff
  68:	cmp	x4, x1
  6c:	and	x7, x2, #0xff
  70:	b.le	ac <__fixtfti+0xac>
  74:	adrp	x1, 0 <__fixtfti>
  78:	mov	x0, #0x1                   	// #1
  7c:	sub	x19, x0, x7
  80:	mov	x5, #0x407e                	// #16510
  84:	ldr	x1, [x1]
  88:	asr	x7, x19, #63
  8c:	negs	x19, x19
  90:	sbc	x1, x1, x7
  94:	cmp	x4, x5
  98:	csel	w2, w2, wzr, eq  // eq = none
  9c:	cbz	w2, 40 <__fixtfti+0x40>
  a0:	orr	x3, x6, x3
  a4:	cbnz	x3, 40 <__fixtfti+0x40>
  a8:	b	4c <__fixtfti+0x4c>
  ac:	mov	x1, #0x406e                	// #16494
  b0:	mov	x0, x4
  b4:	orr	x5, x3, #0x1000000000000
  b8:	cmp	x4, x1
  bc:	b.gt	134 <__fixtfti+0x134>
  c0:	mov	x3, #0x406f                	// #16495
  c4:	sub	x4, x3, x4
  c8:	cmp	x4, #0x3f
  cc:	b.gt	184 <__fixtfti+0x184>
  d0:	mov	w2, #0xffffbfd1            	// #-16431
  d4:	add	w2, w0, w2
  d8:	sub	w0, w3, w0
  dc:	mov	x19, #0x0                   	// #0
  e0:	lsl	x1, x6, x2
  e4:	cmp	x1, #0x0
  e8:	lsr	x3, x6, x0
  ec:	cset	w4, ne  // ne = any
  f0:	lsl	x2, x5, x2
  f4:	orr	x1, x2, x3
  f8:	lsr	x0, x5, x0
  fc:	adds	x19, x1, x19
 100:	cinc	x1, x0, cs  // cs = hs, nlast
 104:	cbz	x7, 110 <__fixtfti+0x110>
 108:	negs	x19, x19
 10c:	ngc	x1, x1
 110:	cbz	w4, 4c <__fixtfti+0x4c>
 114:	mov	w0, #0x10                  	// #16
 118:	b	40 <__fixtfti+0x40>
 11c:	orr	x3, x6, x3
 120:	mov	x19, #0x0                   	// #0
 124:	mov	x1, #0x0                   	// #0
 128:	cbz	x3, 4c <__fixtfti+0x4c>
 12c:	mov	w0, #0x10                  	// #16
 130:	b	40 <__fixtfti+0x40>
 134:	mov	w3, #0xffffbf91            	// #-16495
 138:	add	w2, w4, w3
 13c:	mov	w3, #0x3f                  	// #63
 140:	sub	w4, w3, w2
 144:	lsr	x3, x6, #1
 148:	mov	x19, x6
 14c:	mov	w6, #0xffffbf51            	// #-16559
 150:	add	w0, w0, w6
 154:	cmp	w0, #0x0
 158:	lsr	x3, x3, x4
 15c:	lsl	x1, x5, x2
 160:	orr	x1, x3, x1
 164:	lsl	x3, x19, x0
 168:	csel	x1, x3, x1, ge  // ge = tcont
 16c:	lsl	x19, x19, x2
 170:	csel	x19, xzr, x19, ge  // ge = tcont
 174:	cbz	x7, 4c <__fixtfti+0x4c>
 178:	negs	x19, x19
 17c:	ngc	x1, x1
 180:	b	4c <__fixtfti+0x4c>
 184:	mov	w1, #0xffffc011            	// #-16367
 188:	add	w2, w0, w1
 18c:	mov	w1, #0x402f                	// #16431
 190:	cmp	x4, #0x40
 194:	sub	w0, w1, w0
 198:	lsl	x1, x5, x2
 19c:	orr	x1, x6, x1
 1a0:	mov	x19, #0x0                   	// #0
 1a4:	csel	x6, x1, x6, ne  // ne = any
 1a8:	lsr	x1, x5, x0
 1ac:	cmp	x6, #0x0
 1b0:	mov	x0, #0x0                   	// #0
 1b4:	cset	w4, ne  // ne = any
 1b8:	b	fc <__fixtfti+0xfc>

fixunstfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x1, x0, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	mov	x2, #0x3ffe                	// #16382
  20:	mov	x3, x1
  24:	cmp	x4, x2
  28:	ubfx	x5, x0, #0, #48
  2c:	b.gt	78 <__fixunstfti+0x78>
  30:	cbz	x4, 5c <__fixunstfti+0x5c>
  34:	mov	w0, #0x10                  	// #16
  38:	mov	x19, #0x0                   	// #0
  3c:	mov	x1, #0x0                   	// #0
  40:	str	x1, [sp, #32]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	x1, [sp, #32]
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	orr	x1, x1, x5
  60:	mov	x19, #0x0                   	// #0
  64:	cbnz	x1, 34 <__fixunstfti+0x34>
  68:	mov	x0, x19
  6c:	ldr	x19, [sp, #16]
  70:	ldp	x29, x30, [sp], #48
  74:	ret
  78:	lsr	x1, x0, #63
  7c:	mov	x2, #0x407f                	// #16511
  80:	and	w1, w1, #0xff
  84:	and	x6, x1, #0xff
  88:	sub	x2, x2, x6
  8c:	cmp	x2, x4
  90:	b.le	f0 <__fixunstfti+0xf0>
  94:	cbnz	x6, 104 <__fixunstfti+0x104>
  98:	mov	x1, #0x406e                	// #16494
  9c:	mov	x0, x4
  a0:	orr	x5, x5, #0x1000000000000
  a4:	cmp	x4, x1
  a8:	b.le	114 <__fixunstfti+0x114>
  ac:	adds	x3, x3, x6
  b0:	mov	w2, #0x3f                  	// #63
  b4:	mov	w6, #0xffffbf91            	// #-16495
  b8:	add	w19, w4, w6
  bc:	sub	w4, w2, w19
  c0:	lsr	x2, x3, #1
  c4:	mov	w7, #0xffffbf51            	// #-16559
  c8:	add	w0, w0, w7
  cc:	cmp	w0, #0x0
  d0:	lsl	x1, x5, x19
  d4:	lsr	x2, x2, x4
  d8:	orr	x1, x2, x1
  dc:	lsl	x19, x3, x19
  e0:	csel	x19, xzr, x19, ge  // ge = tcont
  e4:	lsl	x2, x3, x0
  e8:	csel	x1, x2, x1, ge  // ge = tcont
  ec:	b	4c <__fixunstfti+0x4c>
  f0:	eor	w1, w1, #0x1
  f4:	mov	w0, #0x1                   	// #1
  f8:	sbfx	x19, x1, #0, #1
  fc:	mov	x1, x19
 100:	b	40 <__fixunstfti+0x40>
 104:	mov	w0, #0x1                   	// #1
 108:	mov	x19, #0x0                   	// #0
 10c:	mov	x1, #0x0                   	// #0
 110:	b	40 <__fixunstfti+0x40>
 114:	mov	x2, #0x406f                	// #16495
 118:	sub	x4, x2, x4
 11c:	cmp	x4, #0x3f
 120:	b.le	170 <__fixunstfti+0x170>
 124:	mov	w1, #0xffffc011            	// #-16367
 128:	add	w1, w0, w1
 12c:	cmp	x4, #0x40
 130:	mov	w2, #0x402f                	// #16431
 134:	lsl	x1, x5, x1
 138:	orr	x1, x3, x1
 13c:	csel	x3, x1, x3, ne  // ne = any
 140:	sub	w0, w2, w0
 144:	cmp	x3, #0x0
 148:	mov	x19, #0x0                   	// #0
 14c:	cset	w2, ne  // ne = any
 150:	lsr	x0, x5, x0
 154:	mov	x1, #0x0                   	// #0
 158:	adds	x3, x0, x19
 15c:	mov	w0, #0x10                  	// #16
 160:	mov	x19, x3
 164:	cinc	x1, x1, cs  // cs = hs, nlast
 168:	cbnz	w2, 40 <__fixunstfti+0x40>
 16c:	b	4c <__fixunstfti+0x4c>
 170:	mov	w4, #0xffffbfd1            	// #-16431
 174:	add	w4, w0, w4
 178:	sub	w1, w2, w0
 17c:	mov	x19, #0x0                   	// #0
 180:	lsl	x0, x3, x4
 184:	cmp	x0, #0x0
 188:	lsl	x4, x5, x4
 18c:	cset	w2, ne  // ne = any
 190:	lsr	x0, x3, x1
 194:	orr	x0, x4, x0
 198:	lsr	x1, x5, x1
 19c:	b	158 <__fixunstfti+0x158>

floattitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	mrs	x9, fpcr
   4:	orr	x2, x0, x1
   8:	cbz	x2, c8 <__floattitf+0xc8>
   c:	lsr	x4, x1, #63
  10:	mov	x2, x1
  14:	and	w8, w4, #0xff
  18:	tbnz	x1, #63, f4 <__floattitf+0xf4>
  1c:	mov	x3, x2
  20:	cbz	x2, 104 <__floattitf+0x104>
  24:	clz	x4, x2
  28:	mov	w5, #0x407e                	// #16510
  2c:	sub	w5, w5, w4
  30:	mov	w4, #0x406f                	// #16495
  34:	cmp	w5, w4
  38:	sxtw	x7, w5
  3c:	b.le	254 <__floattitf+0x254>
  40:	mov	w2, #0x4072                	// #16498
  44:	cmp	w5, w2
  48:	b.gt	150 <__floattitf+0x150>
  4c:	mov	x4, #0x4072                	// #16498
  50:	cmp	x7, x4
  54:	b.eq	74 <__floattitf+0x74>  // b.none
  58:	sub	w2, w2, w5
  5c:	mov	w4, #0xffffbfce            	// #-16434
  60:	add	w5, w5, w4
  64:	lsl	x3, x3, x2
  68:	lsr	x5, x0, x5
  6c:	orr	x3, x5, x3
  70:	lsl	x0, x0, x2
  74:	and	x5, x3, #0xfff7ffffffffffff
  78:	tst	x0, #0x7
  7c:	mov	w4, #0x0                   	// #0
  80:	b.eq	a0 <__floattitf+0xa0>  // b.none
  84:	and	x2, x9, #0xc00000
  88:	mov	w4, #0x10                  	// #16
  8c:	cmp	x2, #0x400, lsl #12
  90:	b.eq	20c <__floattitf+0x20c>  // b.none
  94:	cmp	x2, #0x800, lsl #12
  98:	b.eq	24c <__floattitf+0x24c>  // b.none
  9c:	cbz	x2, 230 <__floattitf+0x230>
  a0:	lsr	x1, x5, #3
  a4:	mov	x3, #0x0                   	// #0
  a8:	extr	x2, x5, x0, #3
  ac:	bfxil	x3, x1, #0, #48
  b0:	fmov	d0, x2
  b4:	bfi	x3, x7, #48, #15
  b8:	bfi	x3, x8, #63, #1
  bc:	fmov	v0.d[1], x3
  c0:	cbnz	w4, 1ec <__floattitf+0x1ec>
  c4:	ret
  c8:	mov	w8, #0x0                   	// #0
  cc:	mov	w4, #0x0                   	// #0
  d0:	mov	x1, #0x0                   	// #0
  d4:	mov	x6, #0x0                   	// #0
  d8:	mov	x3, #0x0                   	// #0
  dc:	orr	w4, w4, w8, lsl #15
  e0:	bfxil	x3, x1, #0, #48
  e4:	fmov	d0, x6
  e8:	bfi	x3, x4, #48, #16
  ec:	fmov	v0.d[1], x3
  f0:	ret
  f4:	negs	x0, x0
  f8:	ngc	x2, x1
  fc:	mov	x3, x2
 100:	cbnz	x2, 24 <__floattitf+0x24>
 104:	clz	x1, x0
 108:	mov	w3, #0x403e                	// #16446
 10c:	sub	w1, w3, w1
 110:	mov	x3, #0x406f                	// #16495
 114:	mov	w5, w1
 118:	and	w4, w1, #0x7fff
 11c:	sub	x3, x3, w1, sxtw
 120:	cmp	x3, #0x3f
 124:	b.gt	1d4 <__floattitf+0x1d4>
 128:	mov	w6, #0x406f                	// #16495
 12c:	mov	w13, #0xffffbfd1            	// #-16431
 130:	add	w1, w5, w13
 134:	sub	w5, w6, w5
 138:	lsr	x1, x0, x1
 13c:	lsl	x2, x2, x5
 140:	orr	x2, x1, x2
 144:	and	x1, x2, #0xffffffffffff
 148:	lsl	x6, x0, x5
 14c:	b	d8 <__floattitf+0xd8>
 150:	sub	w6, w5, w2
 154:	mov	w10, #0x3f                  	// #63
 158:	lsl	x2, x3, #1
 15c:	sub	w11, w10, w6
 160:	mov	w4, #0x40f2                	// #16626
 164:	mov	w12, #0xffffbf4e            	// #-16562
 168:	add	w12, w5, w12
 16c:	sub	w5, w4, w5
 170:	sub	w10, w10, w5
 174:	lsl	x4, x2, x11
 178:	lsr	x11, x0, #1
 17c:	cmp	w12, #0x0
 180:	lsr	x2, x0, x6
 184:	orr	x2, x4, x2
 188:	sub	w4, w5, #0x40
 18c:	lsr	x13, x3, x12
 190:	lsr	x6, x3, x6
 194:	csel	x2, x13, x2, ge  // ge = tcont
 198:	csel	x6, xzr, x6, ge  // ge = tcont
 19c:	lsr	x10, x11, x10
 1a0:	cmp	w4, #0x0
 1a4:	lsl	x3, x3, x5
 1a8:	orr	x3, x10, x3
 1ac:	lsl	x10, x0, x4
 1b0:	lsl	x0, x0, x5
 1b4:	csel	x3, x10, x3, ge  // ge = tcont
 1b8:	csel	x0, xzr, x0, ge  // ge = tcont
 1bc:	orr	x0, x0, x3
 1c0:	mov	x3, x6
 1c4:	cmp	x0, #0x0
 1c8:	cset	x0, ne  // ne = any
 1cc:	orr	x0, x2, x0
 1d0:	b	74 <__floattitf+0x74>
 1d4:	mov	w2, #0x402f                	// #16431
 1d8:	sub	w2, w2, w1
 1dc:	mov	x6, #0x0                   	// #0
 1e0:	lsl	x1, x0, x2
 1e4:	and	x1, x1, #0xffffffffffff
 1e8:	b	d8 <__floattitf+0xd8>
 1ec:	stp	x29, x30, [sp, #-32]!
 1f0:	mov	w0, w4
 1f4:	mov	x29, sp
 1f8:	str	q0, [sp, #16]
 1fc:	bl	0 <__sfp_handle_exceptions>
 200:	ldr	q0, [sp, #16]
 204:	ldp	x29, x30, [sp], #32
 208:	ret
 20c:	tbnz	x1, #63, a0 <__floattitf+0xa0>
 210:	adds	x0, x0, #0x8
 214:	cinc	x5, x5, cs  // cs = hs, nlast
 218:	and	x1, x5, #0x8000000000000
 21c:	cbz	x1, 288 <__floattitf+0x288>
 220:	and	x5, x5, #0xfff7ffffffffffff
 224:	add	x7, x7, #0x1
 228:	mov	w4, #0x10                  	// #16
 22c:	b	a0 <__floattitf+0xa0>
 230:	and	x1, x0, #0xf
 234:	cmp	x1, #0x4
 238:	b.eq	a0 <__floattitf+0xa0>  // b.none
 23c:	adds	x0, x0, #0x4
 240:	cinc	x5, x5, cs  // cs = hs, nlast
 244:	and	x1, x5, #0x8000000000000
 248:	b	21c <__floattitf+0x21c>
 24c:	tbz	x1, #63, a0 <__floattitf+0xa0>
 250:	b	210 <__floattitf+0x210>
 254:	mov	x1, #0x406f                	// #16495
 258:	mov	x6, x0
 25c:	cmp	x7, x1
 260:	b.ne	290 <__floattitf+0x290>  // b.any
 264:	and	x1, x2, #0xffffffffffff
 268:	mov	w4, w7
 26c:	mov	x3, #0x0                   	// #0
 270:	orr	w4, w4, w8, lsl #15
 274:	bfxil	x3, x1, #0, #48
 278:	fmov	d0, x6
 27c:	bfi	x3, x4, #48, #16
 280:	fmov	v0.d[1], x3
 284:	ret
 288:	mov	w4, #0x10                  	// #16
 28c:	b	a0 <__floattitf+0xa0>
 290:	and	w4, w5, #0x7fff
 294:	b	128 <__floattitf+0x128>

floatuntitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	mrs	x6, fpcr
   4:	orr	x2, x0, x1
   8:	cbz	x2, cc <__floatuntitf+0xcc>
   c:	mov	x3, x1
  10:	cbz	x1, f0 <__floatuntitf+0xf0>
  14:	clz	x4, x1
  18:	mov	w2, #0x407e                	// #16510
  1c:	sub	w2, w2, w4
  20:	mov	w4, #0x406f                	// #16495
  24:	cmp	w2, w4
  28:	sxtw	x4, w2
  2c:	b.le	22c <__floatuntitf+0x22c>
  30:	mov	w1, #0x4072                	// #16498
  34:	cmp	w2, w1
  38:	b.gt	13c <__floatuntitf+0x13c>
  3c:	mov	x5, x0
  40:	mov	x0, #0x4072                	// #16498
  44:	cmp	x4, x0
  48:	b.eq	68 <__floatuntitf+0x68>  // b.none
  4c:	sub	w1, w1, w2
  50:	mov	w0, #0xffffbfce            	// #-16434
  54:	add	w2, w2, w0
  58:	lsl	x3, x3, x1
  5c:	lsr	x2, x5, x2
  60:	orr	x3, x2, x3
  64:	lsl	x5, x5, x1
  68:	and	x1, x3, #0xfff7ffffffffffff
  6c:	tst	x5, #0x7
  70:	mov	w0, #0x0                   	// #0
  74:	b.eq	a8 <__floatuntitf+0xa8>  // b.none
  78:	ands	x6, x6, #0xc00000
  7c:	b.eq	208 <__floatuntitf+0x208>  // b.none
  80:	cmp	x6, #0x400, lsl #12
  84:	b.ne	200 <__floatuntitf+0x200>  // b.any
  88:	adds	x0, x5, #0x8
  8c:	cinc	x1, x1, cs  // cs = hs, nlast
  90:	mov	x5, x0
  94:	and	x0, x1, #0x8000000000000
  98:	cbz	x0, 200 <__floatuntitf+0x200>
  9c:	and	x1, x1, #0xfff7ffffffffffff
  a0:	add	x4, x4, #0x1
  a4:	mov	w0, #0x10                  	// #16
  a8:	lsr	x6, x1, #3
  ac:	mov	x3, #0x0                   	// #0
  b0:	extr	x2, x1, x5, #3
  b4:	bfxil	x3, x6, #0, #48
  b8:	fmov	d0, x2
  bc:	bfi	x3, x4, #48, #16
  c0:	fmov	v0.d[1], x3
  c4:	cbnz	w0, 1e4 <__floatuntitf+0x1e4>
  c8:	ret
  cc:	mov	w4, #0x0                   	// #0
  d0:	mov	x1, #0x0                   	// #0
  d4:	mov	x5, #0x0                   	// #0
  d8:	mov	x3, #0x0                   	// #0
  dc:	fmov	d0, x5
  e0:	bfxil	x3, x1, #0, #48
  e4:	bfi	x3, x4, #48, #16
  e8:	fmov	v0.d[1], x3
  ec:	ret
  f0:	clz	x3, x0
  f4:	mov	w2, #0x403e                	// #16446
  f8:	sub	w3, w2, w3
  fc:	mov	x5, #0x406f                	// #16495
 100:	mov	w2, w3
 104:	and	w4, w3, #0x7fff
 108:	sub	x5, x5, w3, sxtw
 10c:	cmp	x5, #0x3f
 110:	b.gt	1b8 <__floatuntitf+0x1b8>
 114:	mov	w5, #0x406f                	// #16495
 118:	mov	w10, #0xffffbfd1            	// #-16431
 11c:	add	w3, w2, w10
 120:	sub	w2, w5, w2
 124:	lsr	x3, x0, x3
 128:	lsl	x1, x1, x2
 12c:	orr	x1, x3, x1
 130:	and	x1, x1, #0xffffffffffff
 134:	lsl	x5, x0, x2
 138:	b	d8 <__floatuntitf+0xd8>
 13c:	mov	w5, #0x40f2                	// #16626
 140:	sub	w5, w5, w2
 144:	lsr	x10, x0, #1
 148:	mov	w8, #0x3f                  	// #63
 14c:	sub	w11, w8, w5
 150:	subs	w9, w5, #0x40
 154:	lsl	x7, x3, x5
 158:	sub	w1, w2, w1
 15c:	lsr	x10, x10, x11
 160:	orr	x7, x10, x7
 164:	lsl	x5, x0, x5
 168:	csel	x5, xzr, x5, pl  // pl = nfrst
 16c:	lsl	x10, x0, x9
 170:	csel	x7, x10, x7, pl  // pl = nfrst
 174:	orr	x5, x5, x7
 178:	sub	w8, w8, w1
 17c:	lsl	x7, x3, #1
 180:	cmp	x5, #0x0
 184:	mov	w9, #0xffffbf4e            	// #-16562
 188:	add	w2, w2, w9
 18c:	lsr	x5, x0, x1
 190:	cset	x0, ne  // ne = any
 194:	lsl	x8, x7, x8
 198:	cmp	w2, #0x0
 19c:	orr	x5, x8, x5
 1a0:	lsr	x7, x3, x2
 1a4:	csel	x5, x7, x5, ge  // ge = tcont
 1a8:	lsr	x2, x3, x1
 1ac:	orr	x5, x0, x5
 1b0:	csel	x3, x2, xzr, lt  // lt = tstop
 1b4:	b	68 <__floatuntitf+0x68>
 1b8:	mov	w1, #0x402f                	// #16431
 1bc:	sub	w1, w1, w3
 1c0:	mov	x5, #0x0                   	// #0
 1c4:	lsl	x1, x0, x1
 1c8:	and	x1, x1, #0xffffffffffff
 1cc:	mov	x3, #0x0                   	// #0
 1d0:	fmov	d0, x5
 1d4:	bfxil	x3, x1, #0, #48
 1d8:	bfi	x3, x4, #48, #16
 1dc:	fmov	v0.d[1], x3
 1e0:	ret
 1e4:	stp	x29, x30, [sp, #-32]!
 1e8:	mov	x29, sp
 1ec:	str	q0, [sp, #16]
 1f0:	bl	0 <__sfp_handle_exceptions>
 1f4:	ldr	q0, [sp, #16]
 1f8:	ldp	x29, x30, [sp], #32
 1fc:	ret
 200:	mov	w0, #0x10                  	// #16
 204:	b	a8 <__floatuntitf+0xa8>
 208:	and	x2, x5, #0xf
 20c:	mov	w0, #0x10                  	// #16
 210:	cmp	x2, #0x4
 214:	b.eq	a8 <__floatuntitf+0xa8>  // b.none
 218:	adds	x0, x5, #0x4
 21c:	cinc	x1, x1, cs  // cs = hs, nlast
 220:	mov	x5, x0
 224:	and	x0, x1, #0x8000000000000
 228:	b	98 <__floatuntitf+0x98>
 22c:	mov	x3, #0x406f                	// #16495
 230:	mov	x5, x0
 234:	cmp	x4, x3
 238:	b.eq	1c8 <__floatuntitf+0x1c8>  // b.none
 23c:	and	w4, w2, #0x7fff
 240:	b	114 <__floatuntitf+0x114>

extendsftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	w0, s0
   8:	ubfx	x2, x0, #23, #8
   c:	and	x1, x0, #0x7fffff
  10:	add	x3, x2, #0x1
  14:	ubfx	x5, x0, #0, #23
  18:	tst	x3, #0xfe
  1c:	lsr	w0, w0, #31
  20:	b.eq	50 <__extendsftf2+0x50>  // b.none
  24:	lsl	x1, x1, #25
  28:	mov	w3, #0x3f80                	// #16256
  2c:	add	w4, w2, w3
  30:	mov	x3, #0x0                   	// #0
  34:	bfxil	x3, x1, #0, #48
  38:	mov	x2, #0x0                   	// #0
  3c:	fmov	d0, x2
  40:	bfi	x3, x4, #48, #15
  44:	bfi	x3, x0, #63, #1
  48:	fmov	v0.d[1], x3
  4c:	ret
  50:	cbnz	w2, 80 <__extendsftf2+0x80>
  54:	cbnz	x1, cc <__extendsftf2+0xcc>
  58:	mov	x1, #0x0                   	// #0
  5c:	mov	x3, #0x0                   	// #0
  60:	bfxil	x3, x1, #0, #48
  64:	mov	w4, #0x0                   	// #0
  68:	mov	x2, #0x0                   	// #0
  6c:	fmov	d0, x2
  70:	bfi	x3, x4, #48, #15
  74:	bfi	x3, x0, #63, #1
  78:	fmov	v0.d[1], x3
  7c:	ret
  80:	cbz	x1, 108 <__extendsftf2+0x108>
  84:	lsl	x1, x1, #25
  88:	mov	x3, #0x0                   	// #0
  8c:	orr	x1, x1, #0x800000000000
  90:	mov	x2, #0x0                   	// #0
  94:	fmov	d0, x2
  98:	bfxil	x3, x1, #0, #48
  9c:	orr	x3, x3, #0x7fff000000000000
  a0:	bfi	x3, x0, #63, #1
  a4:	fmov	v0.d[1], x3
  a8:	tbnz	w5, #22, 130 <__extendsftf2+0x130>
  ac:	stp	x29, x30, [sp, #-32]!
  b0:	mov	w0, #0x1                   	// #1
  b4:	mov	x29, sp
  b8:	str	q0, [sp, #16]
  bc:	bl	0 <__sfp_handle_exceptions>
  c0:	ldr	q0, [sp, #16]
  c4:	ldp	x29, x30, [sp], #32
  c8:	ret
  cc:	clz	x4, x1
  d0:	mov	w2, #0x3fa9                	// #16297
  d4:	sub	w3, w4, #0xf
  d8:	sub	w2, w2, w4
  dc:	and	w4, w2, #0x7fff
  e0:	mov	x2, #0x0                   	// #0
  e4:	lsl	x1, x1, x3
  e8:	and	x1, x1, #0xffffffffffff
  ec:	mov	x3, #0x0                   	// #0
  f0:	fmov	d0, x2
  f4:	bfxil	x3, x1, #0, #48
  f8:	bfi	x3, x4, #48, #15
  fc:	bfi	x3, x0, #63, #1
 100:	fmov	v0.d[1], x3
 104:	ret
 108:	mov	x1, #0x0                   	// #0
 10c:	mov	x3, #0x0                   	// #0
 110:	bfxil	x3, x1, #0, #48
 114:	mov	w4, #0x7fff                	// #32767
 118:	mov	x2, #0x0                   	// #0
 11c:	fmov	d0, x2
 120:	bfi	x3, x4, #48, #15
 124:	bfi	x3, x0, #63, #1
 128:	fmov	v0.d[1], x3
 12c:	ret
 130:	ret

extenddftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	x0, d0
   8:	ubfx	x1, x0, #52, #11
   c:	lsr	x4, x0, #63
  10:	add	x2, x1, #0x1
  14:	and	w4, w4, #0xff
  18:	tst	x2, #0x7fe
  1c:	ubfx	x0, x0, #0, #52
  20:	b.eq	54 <__extenddftf2+0x54>  // b.none
  24:	lsr	x5, x0, #4
  28:	mov	x3, #0x0                   	// #0
  2c:	and	x5, x5, #0xffffffffffff
  30:	mov	w2, #0x3c00                	// #15360
  34:	add	w1, w1, w2
  38:	lsl	x0, x0, #60
  3c:	bfxil	x3, x5, #0, #48
  40:	fmov	d0, x0
  44:	bfi	x3, x1, #48, #15
  48:	bfi	x3, x4, #63, #1
  4c:	fmov	v0.d[1], x3
  50:	ret
  54:	cbnz	x1, a8 <__extenddftf2+0xa8>
  58:	cbz	x0, f4 <__extenddftf2+0xf4>
  5c:	clz	x2, x0
  60:	cmp	w2, #0xe
  64:	b.gt	140 <__extenddftf2+0x140>
  68:	add	w1, w2, #0x31
  6c:	mov	w5, #0xf                   	// #15
  70:	sub	w5, w5, w2
  74:	lsr	x5, x0, x5
  78:	lsl	x0, x0, x1
  7c:	and	x5, x5, #0xffffffffffff
  80:	mov	w1, #0x3c0c                	// #15372
  84:	mov	x3, #0x0                   	// #0
  88:	sub	w1, w1, w2
  8c:	and	w1, w1, #0x7fff
  90:	bfxil	x3, x5, #0, #48
  94:	fmov	d0, x0
  98:	bfi	x3, x1, #48, #15
  9c:	bfi	x3, x4, #63, #1
  a0:	fmov	v0.d[1], x3
  a4:	ret
  a8:	cbz	x0, 118 <__extenddftf2+0x118>
  ac:	lsr	x1, x0, #4
  b0:	mov	x3, #0x0                   	// #0
  b4:	orr	x1, x1, #0x800000000000
  b8:	lsl	x2, x0, #60
  bc:	fmov	d0, x2
  c0:	bfxil	x3, x1, #0, #48
  c4:	orr	x3, x3, #0x7fff000000000000
  c8:	bfi	x3, x4, #63, #1
  cc:	fmov	v0.d[1], x3
  d0:	tbnz	x0, #51, 13c <__extenddftf2+0x13c>
  d4:	stp	x29, x30, [sp, #-32]!
  d8:	mov	w0, #0x1                   	// #1
  dc:	mov	x29, sp
  e0:	str	q0, [sp, #16]
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	ldr	q0, [sp, #16]
  ec:	ldp	x29, x30, [sp], #32
  f0:	ret
  f4:	mov	x5, #0x0                   	// #0
  f8:	mov	x3, #0x0                   	// #0
  fc:	bfxil	x3, x5, #0, #48
 100:	mov	w1, #0x0                   	// #0
 104:	fmov	d0, x0
 108:	bfi	x3, x1, #48, #15
 10c:	bfi	x3, x4, #63, #1
 110:	fmov	v0.d[1], x3
 114:	ret
 118:	mov	x5, #0x0                   	// #0
 11c:	mov	x3, #0x0                   	// #0
 120:	bfxil	x3, x5, #0, #48
 124:	mov	w1, #0x7fff                	// #32767
 128:	fmov	d0, x0
 12c:	bfi	x3, x1, #48, #15
 130:	bfi	x3, x4, #63, #1
 134:	fmov	v0.d[1], x3
 138:	ret
 13c:	ret
 140:	sub	w5, w2, #0xf
 144:	lsl	x5, x0, x5
 148:	mov	x0, #0x0                   	// #0
 14c:	b	7c <__extenddftf2+0x7c>

extendhftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhftf2>:
   0:	mrs	x0, fpcr
   4:	umov	w1, v0.h[0]
   8:	mov	w0, #0x0                   	// #0
   c:	bfxil	w0, w1, #0, #16
  10:	and	w4, w0, #0x3ff
  14:	and	x2, x0, #0x3ff
  18:	ubfx	x1, x0, #10, #5
  1c:	ubfx	x0, x0, #15, #1
  20:	add	x3, x1, #0x1
  24:	tst	x3, #0x1e
  28:	b.eq	58 <__extendhftf2+0x58>  // b.none
  2c:	lsl	x4, x2, #38
  30:	mov	x3, #0x0                   	// #0
  34:	mov	w2, #0x3ff0                	// #16368
  38:	add	w1, w1, w2
  3c:	bfxil	x3, x4, #0, #48
  40:	mov	x2, #0x0                   	// #0
  44:	fmov	d0, x2
  48:	bfi	x3, x1, #48, #15
  4c:	bfi	x3, x0, #63, #1
  50:	fmov	v0.d[1], x3
  54:	ret
  58:	cbnz	x1, 88 <__extendhftf2+0x88>
  5c:	cbnz	x2, d4 <__extendhftf2+0xd4>
  60:	mov	x4, #0x0                   	// #0
  64:	mov	x3, #0x0                   	// #0
  68:	bfxil	x3, x4, #0, #48
  6c:	mov	w1, #0x0                   	// #0
  70:	mov	x2, #0x0                   	// #0
  74:	fmov	d0, x2
  78:	bfi	x3, x1, #48, #15
  7c:	bfi	x3, x0, #63, #1
  80:	fmov	v0.d[1], x3
  84:	ret
  88:	cbz	x2, 110 <__extendhftf2+0x110>
  8c:	lsl	x1, x2, #38
  90:	mov	x3, #0x0                   	// #0
  94:	orr	x1, x1, #0x800000000000
  98:	mov	x2, #0x0                   	// #0
  9c:	fmov	d0, x2
  a0:	bfxil	x3, x1, #0, #48
  a4:	orr	x3, x3, #0x7fff000000000000
  a8:	bfi	x3, x0, #63, #1
  ac:	fmov	v0.d[1], x3
  b0:	tbnz	w4, #9, 138 <__extendhftf2+0x138>
  b4:	stp	x29, x30, [sp, #-32]!
  b8:	mov	w0, #0x1                   	// #1
  bc:	mov	x29, sp
  c0:	str	q0, [sp, #16]
  c4:	bl	0 <__sfp_handle_exceptions>
  c8:	ldr	q0, [sp, #16]
  cc:	ldp	x29, x30, [sp], #32
  d0:	ret
  d4:	clz	x3, x2
  d8:	mov	w1, #0x4026                	// #16422
  dc:	sub	w4, w3, #0xf
  e0:	sub	w1, w1, w3
  e4:	mov	x3, #0x0                   	// #0
  e8:	and	w1, w1, #0x7fff
  ec:	lsl	x4, x2, x4
  f0:	and	x4, x4, #0xffffffffffff
  f4:	mov	x2, #0x0                   	// #0
  f8:	fmov	d0, x2
  fc:	bfxil	x3, x4, #0, #48
 100:	bfi	x3, x1, #48, #15
 104:	bfi	x3, x0, #63, #1
 108:	fmov	v0.d[1], x3
 10c:	ret
 110:	mov	x4, #0x0                   	// #0
 114:	mov	x3, #0x0                   	// #0
 118:	bfxil	x3, x4, #0, #48
 11c:	mov	w1, #0x7fff                	// #32767
 120:	mov	x2, #0x0                   	// #0
 124:	fmov	d0, x2
 128:	bfi	x3, x1, #48, #15
 12c:	bfi	x3, x0, #63, #1
 130:	fmov	v0.d[1], x3
 134:	ret
 138:	ret

trunctfsf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x5, x0, [sp, #32]
  14:	mrs	x3, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	lsr	x6, x0, #63
  20:	add	x1, x4, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	orr	x0, x0, x5, lsr #61
  2c:	tst	x1, #0x7ffe
  30:	and	w6, w6, #0xff
  34:	lsl	x5, x5, #3
  38:	b.eq	b8 <__trunctfsf2+0xb8>  // b.none
  3c:	mov	x1, #0xffffffffffffc080    	// #-16256
  40:	add	x4, x4, x1
  44:	cmp	x4, #0xfe
  48:	b.le	114 <__trunctfsf2+0x114>
  4c:	ands	x0, x3, #0xc00000
  50:	b.eq	1ac <__trunctfsf2+0x1ac>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	288 <__trunctfsf2+0x288>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w7, w6, wzr, eq  // eq = none
  64:	cbnz	w7, 1ac <__trunctfsf2+0x1ac>
  68:	mov	x4, #0xfe                  	// #254
  6c:	mov	x1, #0xffffffffffffffff    	// #-1
  70:	mov	w0, #0x14                  	// #20
  74:	b.ne	154 <__trunctfsf2+0x154>  // b.any
  78:	cmp	w6, #0x0
  7c:	add	x2, x1, #0x8
  80:	csel	x1, x2, x1, ne  // ne = any
  84:	and	x2, x1, #0x4000000
  88:	cbnz	w7, 15c <__trunctfsf2+0x15c>
  8c:	cbnz	x2, 164 <__trunctfsf2+0x164>
  90:	lsr	x3, x1, #3
  94:	and	w2, w4, #0xff
  98:	mov	w1, w3
  9c:	bfi	w1, w2, #23, #9
  a0:	orr	w19, w1, w6, lsl #31
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	fmov	s0, w19
  ac:	ldr	x19, [sp, #16]
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	orr	x5, x0, x5
  bc:	cbnz	x4, d4 <__trunctfsf2+0xd4>
  c0:	cbnz	x5, 184 <__trunctfsf2+0x184>
  c4:	mov	w4, #0x0                   	// #0
  c8:	mov	w0, #0x0                   	// #0
  cc:	mov	x1, #0x0                   	// #0
  d0:	b	f8 <__trunctfsf2+0xf8>
  d4:	cbz	x5, 1bc <__trunctfsf2+0x1bc>
  d8:	lsr	x2, x0, #50
  dc:	lsr	x1, x0, #28
  e0:	eor	w0, w2, #0x1
  e4:	mov	x2, #0x7fff                	// #32767
  e8:	cmp	x4, x2
  ec:	orr	x1, x1, #0x400000
  f0:	csel	w0, w0, wzr, eq  // eq = none
  f4:	mov	w4, #0xff                  	// #255
  f8:	bfi	w1, w4, #23, #9
  fc:	orr	w19, w1, w6, lsl #31
 100:	cbnz	w0, a4 <__trunctfsf2+0xa4>
 104:	fmov	s0, w19
 108:	ldr	x19, [sp, #16]
 10c:	ldp	x29, x30, [sp], #48
 110:	ret
 114:	cmp	x4, #0x0
 118:	b.le	1cc <__trunctfsf2+0x1cc>
 11c:	orr	x5, x5, x0, lsl #39
 120:	mov	w7, #0x0                   	// #0
 124:	cmp	x5, #0x0
 128:	cset	x1, ne  // ne = any
 12c:	orr	x1, x1, x0, lsr #25
 130:	tst	x1, #0x7
 134:	b.eq	264 <__trunctfsf2+0x264>  // b.none
 138:	and	x2, x3, #0xc00000
 13c:	cmp	x2, #0x400, lsl #12
 140:	b.eq	19c <__trunctfsf2+0x19c>  // b.none
 144:	cmp	x2, #0x800, lsl #12
 148:	mov	w0, #0x10                  	// #16
 14c:	b.eq	78 <__trunctfsf2+0x78>  // b.none
 150:	cbz	x2, 274 <__trunctfsf2+0x274>
 154:	and	x2, x1, #0x4000000
 158:	cbz	w7, 160 <__trunctfsf2+0x160>
 15c:	orr	w0, w0, #0x8
 160:	cbz	x2, 268 <__trunctfsf2+0x268>
 164:	cmp	x4, #0xfe
 168:	add	x4, x4, #0x1
 16c:	b.eq	224 <__trunctfsf2+0x224>  // b.none
 170:	ubfx	x2, x1, #3, #23
 174:	orr	w4, w2, w4, lsl #23
 178:	orr	w19, w4, w6, lsl #31
 17c:	bl	0 <__sfp_handle_exceptions>
 180:	b	a8 <__trunctfsf2+0xa8>
 184:	and	x2, x3, #0xc00000
 188:	mov	w7, #0x1                   	// #1
 18c:	cmp	x2, #0x400, lsl #12
 190:	mov	x4, #0x0                   	// #0
 194:	mov	x1, #0x1                   	// #1
 198:	b.ne	144 <__trunctfsf2+0x144>  // b.any
 19c:	cbnz	w6, 1a4 <__trunctfsf2+0x1a4>
 1a0:	add	x1, x1, #0x8
 1a4:	mov	w0, #0x10                  	// #16
 1a8:	b	84 <__trunctfsf2+0x84>
 1ac:	mov	w4, #0xff                  	// #255
 1b0:	mov	w0, #0x14                  	// #20
 1b4:	mov	x1, #0x0                   	// #0
 1b8:	b	f8 <__trunctfsf2+0xf8>
 1bc:	mov	w4, #0xff                  	// #255
 1c0:	mov	w0, #0x0                   	// #0
 1c4:	mov	x1, #0x0                   	// #0
 1c8:	b	f8 <__trunctfsf2+0xf8>
 1cc:	cmn	x4, #0x17
 1d0:	b.lt	184 <__trunctfsf2+0x184>  // b.tstop
 1d4:	orr	x0, x0, #0x8000000000000
 1d8:	add	w2, w4, #0x26
 1dc:	mov	w1, #0x1a                  	// #26
 1e0:	sub	w1, w1, w4
 1e4:	lsl	x2, x0, x2
 1e8:	orr	x5, x2, x5
 1ec:	cmp	x5, #0x0
 1f0:	lsr	x0, x0, x1
 1f4:	cset	x1, ne  // ne = any
 1f8:	orr	x1, x0, x1
 1fc:	ands	x2, x1, #0x7
 200:	b.eq	210 <__trunctfsf2+0x210>  // b.none
 204:	mov	w7, #0x1                   	// #1
 208:	mov	x4, #0x0                   	// #0
 20c:	b	138 <__trunctfsf2+0x138>
 210:	tbz	w3, #11, 260 <__trunctfsf2+0x260>
 214:	mov	w0, #0x0                   	// #0
 218:	mov	x4, #0x0                   	// #0
 21c:	orr	w0, w0, #0x8
 220:	b	160 <__trunctfsf2+0x160>
 224:	mov	w2, w4
 228:	ands	x3, x3, #0xc00000
 22c:	b.eq	254 <__trunctfsf2+0x254>  // b.none
 230:	cmp	x3, #0x400, lsl #12
 234:	b.eq	2a0 <__trunctfsf2+0x2a0>  // b.none
 238:	cmp	x3, #0x800, lsl #12
 23c:	mov	w3, #0xfe                  	// #254
 240:	csel	w1, w6, wzr, eq  // eq = none
 244:	mov	x4, #0x1fffffffffffffff    	// #2305843009213693951
 248:	cmp	w1, #0x0
 24c:	csel	w2, w2, w3, ne  // ne = any
 250:	csel	x3, xzr, x4, ne  // ne = any
 254:	mov	w1, #0x14                  	// #20
 258:	orr	w0, w0, w1
 25c:	b	98 <__trunctfsf2+0x98>
 260:	mov	x4, #0x0                   	// #0
 264:	mov	w0, #0x0                   	// #0
 268:	and	w4, w4, #0xff
 26c:	lsr	x1, x1, #3
 270:	b	f8 <__trunctfsf2+0xf8>
 274:	and	x2, x1, #0xf
 278:	cmp	x2, #0x4
 27c:	add	x2, x1, #0x4
 280:	csel	x1, x2, x1, ne  // ne = any
 284:	b	84 <__trunctfsf2+0x84>
 288:	cbz	w6, 1ac <__trunctfsf2+0x1ac>
 28c:	mov	x4, #0xfe                  	// #254
 290:	mov	w0, #0x14                  	// #20
 294:	mov	w7, #0x0                   	// #0
 298:	mov	x1, #0xffffffffffffffff    	// #-1
 29c:	b	84 <__trunctfsf2+0x84>
 2a0:	cmp	w6, #0x0
 2a4:	mov	w1, #0xfe                  	// #254
 2a8:	csel	w2, w4, w1, eq  // eq = none
 2ac:	mov	x4, #0x1fffffffffffffff    	// #2305843009213693951
 2b0:	csel	x3, xzr, x4, eq  // eq = none
 2b4:	b	254 <__trunctfsf2+0x254>

trunctfdf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x3, x0, [sp, #32]
  14:	mrs	x6, fpcr
  18:	ubfx	x2, x0, #48, #15
  1c:	lsr	x4, x0, #63
  20:	add	x1, x2, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	tst	x1, #0x7ffe
  2c:	and	w4, w4, #0xff
  30:	orr	x0, x0, x3, lsr #61
  34:	lsl	x5, x3, #3
  38:	b.eq	b8 <__trunctfdf2+0xb8>  // b.none
  3c:	mov	x1, #0xffffffffffffc400    	// #-15360
  40:	add	x2, x2, x1
  44:	cmp	x2, #0x7fe
  48:	b.le	120 <__trunctfdf2+0x120>
  4c:	ands	x0, x6, #0xc00000
  50:	b.eq	1b8 <__trunctfdf2+0x1b8>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	2d8 <__trunctfdf2+0x2d8>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w7, w4, wzr, eq  // eq = none
  64:	cbnz	w7, 1b8 <__trunctfdf2+0x1b8>
  68:	mov	w0, #0x14                  	// #20
  6c:	mov	x1, #0xffffffffffffffff    	// #-1
  70:	mov	x2, #0x7fe                 	// #2046
  74:	b.ne	164 <__trunctfdf2+0x164>  // b.any
  78:	cmp	w4, #0x0
  7c:	add	x3, x1, #0x8
  80:	csel	x1, x3, x1, ne  // ne = any
  84:	and	x3, x1, #0x80000000000000
  88:	cbnz	w7, 16c <__trunctfdf2+0x16c>
  8c:	cbnz	x3, 174 <__trunctfdf2+0x174>
  90:	lsr	x1, x1, #3
  94:	and	w3, w2, #0x7ff
  98:	and	x4, x4, #0xff
  9c:	bfi	x1, x3, #52, #12
  a0:	orr	x19, x1, x4, lsl #63
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	fmov	d0, x19
  ac:	ldr	x19, [sp, #16]
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	orr	x1, x0, x5
  bc:	cbnz	x2, d4 <__trunctfdf2+0xd4>
  c0:	cbnz	x1, 190 <__trunctfdf2+0x190>
  c4:	mov	w2, #0x0                   	// #0
  c8:	mov	w0, #0x0                   	// #0
  cc:	mov	x1, #0x0                   	// #0
  d0:	b	100 <__trunctfdf2+0x100>
  d4:	cbz	x1, 1c8 <__trunctfdf2+0x1c8>
  d8:	mov	x3, #0x7fff                	// #32767
  dc:	extr	x1, x0, x5, #60
  e0:	lsr	x0, x0, #50
  e4:	cmp	x2, x3
  e8:	lsr	x1, x1, #3
  ec:	eor	w0, w0, #0x1
  f0:	orr	x1, x1, #0x8000000000000
  f4:	csel	w0, w0, wzr, eq  // eq = none
  f8:	mov	w2, #0x7ff                 	// #2047
  fc:	nop
 100:	and	x4, x4, #0xff
 104:	bfi	x1, x2, #52, #12
 108:	orr	x19, x1, x4, lsl #63
 10c:	cbnz	w0, a4 <__trunctfdf2+0xa4>
 110:	fmov	d0, x19
 114:	ldr	x19, [sp, #16]
 118:	ldp	x29, x30, [sp], #48
 11c:	ret
 120:	cmp	x2, #0x0
 124:	b.le	1d8 <__trunctfdf2+0x1d8>
 128:	cmp	xzr, x3, lsl #7
 12c:	mov	w7, #0x0                   	// #0
 130:	cset	x1, ne  // ne = any
 134:	orr	x5, x1, x5, lsr #60
 138:	orr	x1, x5, x0, lsl #4
 13c:	mov	w0, #0x0                   	// #0
 140:	tst	x5, #0x7
 144:	b.eq	290 <__trunctfdf2+0x290>  // b.none
 148:	and	x3, x6, #0xc00000
 14c:	cmp	x3, #0x400, lsl #12
 150:	b.eq	1a8 <__trunctfdf2+0x1a8>  // b.none
 154:	cmp	x3, #0x800, lsl #12
 158:	mov	w0, #0x10                  	// #16
 15c:	b.eq	78 <__trunctfdf2+0x78>  // b.none
 160:	cbz	x3, 29c <__trunctfdf2+0x29c>
 164:	and	x3, x1, #0x80000000000000
 168:	cbz	w7, 170 <__trunctfdf2+0x170>
 16c:	orr	w0, w0, #0x8
 170:	cbz	x3, 290 <__trunctfdf2+0x290>
 174:	cmp	x2, #0x7fe
 178:	add	x2, x2, #0x1
 17c:	b.eq	238 <__trunctfdf2+0x238>  // b.none
 180:	mov	x3, #0x1fefffffffffffff    	// #2301339409586323455
 184:	and	w2, w2, #0x7ff
 188:	and	x1, x3, x1, lsr #3
 18c:	b	100 <__trunctfdf2+0x100>
 190:	and	x3, x6, #0xc00000
 194:	mov	w7, #0x1                   	// #1
 198:	cmp	x3, #0x400, lsl #12
 19c:	mov	x2, #0x0                   	// #0
 1a0:	mov	x1, #0x1                   	// #1
 1a4:	b.ne	154 <__trunctfdf2+0x154>  // b.any
 1a8:	cbnz	w4, 1b0 <__trunctfdf2+0x1b0>
 1ac:	add	x1, x1, #0x8
 1b0:	mov	w0, #0x10                  	// #16
 1b4:	b	84 <__trunctfdf2+0x84>
 1b8:	mov	w2, #0x7ff                 	// #2047
 1bc:	mov	w0, #0x14                  	// #20
 1c0:	mov	x1, #0x0                   	// #0
 1c4:	b	100 <__trunctfdf2+0x100>
 1c8:	mov	w2, #0x7ff                 	// #2047
 1cc:	mov	w0, #0x0                   	// #0
 1d0:	mov	x1, #0x0                   	// #0
 1d4:	b	100 <__trunctfdf2+0x100>
 1d8:	cmn	x2, #0x34
 1dc:	b.lt	190 <__trunctfdf2+0x190>  // b.tstop
 1e0:	mov	x3, #0x3d                  	// #61
 1e4:	sub	x7, x3, x2
 1e8:	orr	x0, x0, #0x8000000000000
 1ec:	cmp	x7, #0x3f
 1f0:	b.le	2b0 <__trunctfdf2+0x2b0>
 1f4:	add	w1, w2, #0x43
 1f8:	cmp	x7, #0x40
 1fc:	mov	w3, #0xfffffffd            	// #-3
 200:	sub	w2, w3, w2
 204:	lsl	x1, x0, x1
 208:	orr	x1, x5, x1
 20c:	csel	x5, x1, x5, ne  // ne = any
 210:	lsr	x0, x0, x2
 214:	cmp	x5, #0x0
 218:	cset	x1, ne  // ne = any
 21c:	orr	x1, x1, x0
 220:	cmp	x1, #0x0
 224:	cset	w7, ne  // ne = any
 228:	tst	x1, #0x7
 22c:	b.eq	274 <__trunctfdf2+0x274>  // b.none
 230:	mov	x2, #0x0                   	// #0
 234:	b	148 <__trunctfdf2+0x148>
 238:	mov	w3, w2
 23c:	ands	x1, x6, #0xc00000
 240:	b.eq	268 <__trunctfdf2+0x268>  // b.none
 244:	cmp	x1, #0x400, lsl #12
 248:	b.eq	2f0 <__trunctfdf2+0x2f0>  // b.none
 24c:	cmp	x1, #0x800, lsl #12
 250:	mov	w5, #0x7fe                 	// #2046
 254:	csel	w1, w4, wzr, eq  // eq = none
 258:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 25c:	cmp	w1, #0x0
 260:	csel	w3, w3, w5, ne  // ne = any
 264:	csel	x1, xzr, x2, ne  // ne = any
 268:	mov	w2, #0x14                  	// #20
 26c:	orr	w0, w0, w2
 270:	b	98 <__trunctfdf2+0x98>
 274:	and	x3, x1, #0x80000000000000
 278:	cbnz	x1, 308 <__trunctfdf2+0x308>
 27c:	nop
 280:	mov	w0, #0x0                   	// #0
 284:	mov	x2, #0x1                   	// #1
 288:	cbnz	x3, 180 <__trunctfdf2+0x180>
 28c:	mov	x2, #0x0                   	// #0
 290:	and	w2, w2, #0x7ff
 294:	lsr	x1, x1, #3
 298:	b	100 <__trunctfdf2+0x100>
 29c:	and	x3, x1, #0xf
 2a0:	cmp	x3, #0x4
 2a4:	add	x3, x1, #0x4
 2a8:	csel	x1, x3, x1, ne  // ne = any
 2ac:	b	84 <__trunctfdf2+0x84>
 2b0:	add	w1, w2, #0x3
 2b4:	sub	w2, w3, w2
 2b8:	lsl	x3, x5, x1
 2bc:	cmp	x3, #0x0
 2c0:	cset	x3, ne  // ne = any
 2c4:	lsr	x2, x5, x2
 2c8:	orr	x2, x2, x3
 2cc:	lsl	x0, x0, x1
 2d0:	orr	x1, x0, x2
 2d4:	b	220 <__trunctfdf2+0x220>
 2d8:	cbz	w4, 1b8 <__trunctfdf2+0x1b8>
 2dc:	mov	w7, #0x0                   	// #0
 2e0:	mov	w0, #0x14                  	// #20
 2e4:	mov	x2, #0x7fe                 	// #2046
 2e8:	mov	x1, #0xffffffffffffffff    	// #-1
 2ec:	b	84 <__trunctfdf2+0x84>
 2f0:	cmp	w4, #0x0
 2f4:	mov	w1, #0x7fe                 	// #2046
 2f8:	csel	w3, w2, w1, eq  // eq = none
 2fc:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 300:	csel	x1, xzr, x2, eq  // eq = none
 304:	b	268 <__trunctfdf2+0x268>
 308:	tbz	w6, #11, 280 <__trunctfdf2+0x280>
 30c:	mov	w0, #0x0                   	// #0
 310:	mov	x2, #0x0                   	// #0
 314:	orr	w0, w0, #0x8
 318:	b	170 <__trunctfdf2+0x170>

trunctfhf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfhf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x3, x0, [sp, #32]
  14:	mrs	x6, fpcr
  18:	ubfx	x1, x0, #48, #15
  1c:	lsr	x19, x0, #63
  20:	add	x4, x1, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	tst	x4, #0x7ffe
  2c:	orr	x0, x0, x3, lsr #61
  30:	and	w4, w19, #0xff
  34:	lsl	x3, x3, #3
  38:	b.eq	b8 <__trunctfhf2+0xb8>  // b.none
  3c:	mov	x2, #0xffffffffffffc010    	// #-16368
  40:	add	x1, x1, x2
  44:	cmp	x1, #0x1e
  48:	b.le	118 <__trunctfhf2+0x118>
  4c:	ands	x0, x6, #0xc00000
  50:	b.eq	1b4 <__trunctfhf2+0x1b4>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	290 <__trunctfhf2+0x290>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w5, w4, wzr, eq  // eq = none
  64:	cbnz	w5, 1b4 <__trunctfhf2+0x1b4>
  68:	mov	x1, #0x1e                  	// #30
  6c:	mov	x2, #0xffffffffffffffff    	// #-1
  70:	mov	w0, #0x14                  	// #20
  74:	b.ne	158 <__trunctfhf2+0x158>  // b.any
  78:	cmp	w4, #0x0
  7c:	add	x3, x2, #0x8
  80:	csel	x2, x3, x2, ne  // ne = any
  84:	and	x3, x2, #0x2000
  88:	cbnz	w5, 160 <__trunctfhf2+0x160>
  8c:	cbnz	x3, 168 <__trunctfhf2+0x168>
  90:	lsr	x2, x2, #3
  94:	and	w1, w1, #0x1f
  98:	bfi	w2, w1, #10, #22
  9c:	orr	w19, w2, w4, lsl #15
  a0:	sxth	x19, w19
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	dup	v0.4h, w19
  ac:	ldr	x19, [sp, #16]
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	orr	x3, x0, x3
  bc:	cbnz	x1, d4 <__trunctfhf2+0xd4>
  c0:	cbnz	x3, 18c <__trunctfhf2+0x18c>
  c4:	mov	w1, #0x0                   	// #0
  c8:	mov	w0, #0x0                   	// #0
  cc:	mov	x19, #0x0                   	// #0
  d0:	b	f8 <__trunctfhf2+0xf8>
  d4:	cbz	x3, 1c4 <__trunctfhf2+0x1c4>
  d8:	lsr	x2, x0, #50
  dc:	mov	x3, #0x7fff                	// #32767
  e0:	lsr	x0, x0, #41
  e4:	cmp	x1, x3
  e8:	eor	w2, w2, #0x1
  ec:	orr	x19, x0, #0x200
  f0:	csel	w0, w2, wzr, eq  // eq = none
  f4:	mov	w1, #0x1f                  	// #31
  f8:	bfi	w19, w1, #10, #22
  fc:	orr	w19, w19, w4, lsl #15
 100:	sxth	x19, w19
 104:	cbnz	w0, a4 <__trunctfhf2+0xa4>
 108:	dup	v0.4h, w19
 10c:	ldr	x19, [sp, #16]
 110:	ldp	x29, x30, [sp], #48
 114:	ret
 118:	cmp	x1, #0x0
 11c:	b.le	1d4 <__trunctfhf2+0x1d4>
 120:	orr	x3, x3, x0, lsl #26
 124:	mov	w5, #0x0                   	// #0
 128:	cmp	x3, #0x0
 12c:	cset	x19, ne  // ne = any
 130:	orr	x2, x19, x0, lsr #38
 134:	tst	x2, #0x7
 138:	b.eq	268 <__trunctfhf2+0x268>  // b.none
 13c:	and	x3, x6, #0xc00000
 140:	cmp	x3, #0x400, lsl #12
 144:	b.eq	1a4 <__trunctfhf2+0x1a4>  // b.none
 148:	cmp	x3, #0x800, lsl #12
 14c:	mov	w0, #0x10                  	// #16
 150:	b.eq	78 <__trunctfhf2+0x78>  // b.none
 154:	cbz	x3, 278 <__trunctfhf2+0x278>
 158:	and	x3, x2, #0x2000
 15c:	cbz	w5, 164 <__trunctfhf2+0x164>
 160:	orr	w0, w0, #0x8
 164:	cbz	x3, 26c <__trunctfhf2+0x26c>
 168:	cmp	x1, #0x1e
 16c:	add	x1, x1, #0x1
 170:	b.eq	22c <__trunctfhf2+0x22c>  // b.none
 174:	ubfx	x19, x2, #3, #10
 178:	orr	w1, w19, w1, lsl #10
 17c:	orr	w19, w1, w4, lsl #15
 180:	sxth	x19, w19
 184:	bl	0 <__sfp_handle_exceptions>
 188:	b	a8 <__trunctfhf2+0xa8>
 18c:	and	x3, x6, #0xc00000
 190:	mov	w5, #0x1                   	// #1
 194:	cmp	x3, #0x400, lsl #12
 198:	mov	x1, #0x0                   	// #0
 19c:	mov	x2, #0x1                   	// #1
 1a0:	b.ne	148 <__trunctfhf2+0x148>  // b.any
 1a4:	cbnz	w4, 1ac <__trunctfhf2+0x1ac>
 1a8:	add	x2, x2, #0x8
 1ac:	mov	w0, #0x10                  	// #16
 1b0:	b	84 <__trunctfhf2+0x84>
 1b4:	mov	w1, #0x1f                  	// #31
 1b8:	mov	w0, #0x14                  	// #20
 1bc:	mov	x19, #0x0                   	// #0
 1c0:	b	f8 <__trunctfhf2+0xf8>
 1c4:	mov	w1, #0x1f                  	// #31
 1c8:	mov	w0, #0x0                   	// #0
 1cc:	mov	x19, #0x0                   	// #0
 1d0:	b	f8 <__trunctfhf2+0xf8>
 1d4:	cmn	x1, #0xa
 1d8:	b.lt	18c <__trunctfhf2+0x18c>  // b.tstop
 1dc:	orr	x0, x0, #0x8000000000000
 1e0:	add	w5, w1, #0x19
 1e4:	mov	w2, #0x27                  	// #39
 1e8:	sub	w1, w2, w1
 1ec:	lsl	x5, x0, x5
 1f0:	orr	x3, x5, x3
 1f4:	cmp	x3, #0x0
 1f8:	lsr	x0, x0, x1
 1fc:	cset	x19, ne  // ne = any
 200:	orr	x2, x0, x19
 204:	ands	x3, x2, #0x7
 208:	b.eq	218 <__trunctfhf2+0x218>  // b.none
 20c:	mov	w5, #0x1                   	// #1
 210:	mov	x1, #0x0                   	// #0
 214:	b	13c <__trunctfhf2+0x13c>
 218:	tbz	w6, #11, 264 <__trunctfhf2+0x264>
 21c:	mov	w0, #0x0                   	// #0
 220:	mov	x1, #0x0                   	// #0
 224:	orr	w0, w0, #0x8
 228:	b	164 <__trunctfhf2+0x164>
 22c:	ands	x2, x6, #0xc00000
 230:	b.eq	258 <__trunctfhf2+0x258>  // b.none
 234:	cmp	x2, #0x400, lsl #12
 238:	b.eq	2a8 <__trunctfhf2+0x2a8>  // b.none
 23c:	cmp	x2, #0x800, lsl #12
 240:	mov	w5, #0x1e                  	// #30
 244:	csel	w3, w4, wzr, eq  // eq = none
 248:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 24c:	cmp	w3, #0x0
 250:	csel	w1, w1, w5, ne  // ne = any
 254:	csel	x2, xzr, x2, ne  // ne = any
 258:	mov	w3, #0x14                  	// #20
 25c:	orr	w0, w0, w3
 260:	b	98 <__trunctfhf2+0x98>
 264:	mov	x1, #0x0                   	// #0
 268:	mov	w0, #0x0                   	// #0
 26c:	and	w1, w1, #0x1f
 270:	lsr	x19, x2, #3
 274:	b	f8 <__trunctfhf2+0xf8>
 278:	and	x3, x2, #0xf
 27c:	mov	x19, x2
 280:	cmp	x3, #0x4
 284:	add	x2, x2, #0x4
 288:	csel	x2, x2, x19, ne  // ne = any
 28c:	b	84 <__trunctfhf2+0x84>
 290:	cbz	w4, 1b4 <__trunctfhf2+0x1b4>
 294:	mov	x1, #0x1e                  	// #30
 298:	mov	w0, #0x14                  	// #20
 29c:	mov	w5, #0x0                   	// #0
 2a0:	mov	x2, #0xffffffffffffffff    	// #-1
 2a4:	b	84 <__trunctfhf2+0x84>
 2a8:	cmp	w4, #0x0
 2ac:	mov	w2, #0x1e                  	// #30
 2b0:	csel	w1, w1, w2, eq  // eq = none
 2b4:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 2b8:	csel	x2, xzr, x2, eq  // eq = none
 2bc:	b	258 <__trunctfhf2+0x258>

fixhfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixhfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mrs	x0, fpcr
  10:	umov	w0, v0.h[0]
  14:	mov	w19, #0x0                   	// #0
  18:	bfxil	w19, w0, #0, #16
  1c:	ubfx	x1, x19, #10, #5
  20:	ubfx	x0, x19, #0, #10
  24:	cmp	x1, #0xe
  28:	ubfx	x19, x19, #15, #1
  2c:	b.gt	64 <__fixhfti+0x64>
  30:	and	x0, x0, #0xffff
  34:	mov	x19, #0x0                   	// #0
  38:	orr	x0, x0, x1
  3c:	mov	x1, #0x0                   	// #0
  40:	cbz	x0, 54 <__fixhfti+0x54>
  44:	mov	w0, #0x10                  	// #16
  48:	str	x1, [sp, #40]
  4c:	bl	0 <__sfp_handle_exceptions>
  50:	ldr	x1, [sp, #40]
  54:	mov	x0, x19
  58:	ldr	x19, [sp, #16]
  5c:	ldp	x29, x30, [sp], #48
  60:	ret
  64:	and	x2, x19, #0xff
  68:	cmp	x1, #0x1f
  6c:	b.eq	b8 <__fixhfti+0xb8>  // b.none
  70:	and	x0, x0, #0xffff
  74:	cmp	x1, #0x18
  78:	orr	x0, x0, #0x400
  7c:	b.le	d8 <__fixhfti+0xd8>
  80:	sub	w19, w1, #0x19
  84:	subs	w3, w1, #0x59
  88:	lsr	x4, x0, #1
  8c:	mov	w5, #0x3f                  	// #63
  90:	sub	w5, w5, w19
  94:	lsl	x1, x0, x3
  98:	lsl	x19, x0, x19
  9c:	csel	x19, xzr, x19, pl  // pl = nfrst
  a0:	lsr	x4, x4, x5
  a4:	csel	x1, x1, x4, pl  // pl = nfrst
  a8:	cbz	x2, 54 <__fixhfti+0x54>
  ac:	negs	x19, x19
  b0:	ngc	x1, x1
  b4:	b	54 <__fixhfti+0x54>
  b8:	adrp	x1, 0 <__fixhfti>
  bc:	mov	x0, #0x1                   	// #1
  c0:	sub	x19, x0, x2
  c4:	ldr	x1, [x1]
  c8:	asr	x2, x19, #63
  cc:	negs	x19, x19
  d0:	sbc	x1, x1, x2
  d4:	b	48 <__fixhfti+0x48>
  d8:	add	w4, w1, #0x27
  dc:	mov	w19, #0x19                  	// #25
  e0:	sub	w19, w19, w1
  e4:	mov	x1, #0x0                   	// #0
  e8:	lsl	x4, x0, x4
  ec:	lsr	x19, x0, x19
  f0:	cbz	x2, fc <__fixhfti+0xfc>
  f4:	negs	x19, x19
  f8:	csetm	x1, cc  // cc = lo, ul, last
  fc:	cbnz	x4, 44 <__fixhfti+0x44>
 100:	b	54 <__fixhfti+0x54>

fixunshfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunshfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mrs	x0, fpcr
  10:	umov	w0, v0.h[0]
  14:	mov	w1, #0x0                   	// #0
  18:	bfxil	w1, w0, #0, #16
  1c:	and	x19, x1, #0x3ff
  20:	ubfx	x0, x1, #10, #5
  24:	ubfx	x1, x1, #15, #1
  28:	cmp	x0, #0xe
  2c:	b.gt	64 <__fixunshfti+0x64>
  30:	orr	x19, x0, x19
  34:	mov	x1, #0x0                   	// #0
  38:	cbz	x19, 54 <__fixunshfti+0x54>
  3c:	mov	w0, #0x10                  	// #16
  40:	mov	x19, #0x0                   	// #0
  44:	mov	x1, #0x0                   	// #0
  48:	str	x1, [sp, #40]
  4c:	bl	0 <__sfp_handle_exceptions>
  50:	ldr	x1, [sp, #40]
  54:	mov	x0, x19
  58:	ldr	x19, [sp, #16]
  5c:	ldp	x29, x30, [sp], #48
  60:	ret
  64:	cmp	x0, #0x1e
  68:	cset	w3, gt
  6c:	orr	w3, w1, w3
  70:	cbz	w3, 88 <__fixunshfti+0x88>
  74:	eor	w1, w1, #0x1
  78:	mov	w0, #0x1                   	// #1
  7c:	sbfx	x19, x1, #0, #1
  80:	mov	x1, x19
  84:	b	48 <__fixunshfti+0x48>
  88:	mov	x2, x0
  8c:	orr	x19, x19, #0x400
  90:	cmp	x0, #0x18
  94:	b.gt	bc <__fixunshfti+0xbc>
  98:	mov	w1, #0x19                  	// #25
  9c:	add	w0, w0, #0x27
  a0:	sub	w2, w1, w2
  a4:	mov	x1, #0x0                   	// #0
  a8:	lsl	x0, x19, x0
  ac:	lsr	x19, x19, x2
  b0:	cbz	x0, 54 <__fixunshfti+0x54>
  b4:	mov	w0, #0x10                  	// #16
  b8:	b	48 <__fixunshfti+0x48>
  bc:	sub	w4, w0, #0x19
  c0:	subs	w2, w0, #0x59
  c4:	mov	w3, #0x3f                  	// #63
  c8:	lsr	x0, x19, #1
  cc:	sub	w3, w3, w4
  d0:	lsl	x1, x19, x2
  d4:	lsl	x19, x19, x4
  d8:	csel	x19, xzr, x19, pl  // pl = nfrst
  dc:	lsr	x0, x0, x3
  e0:	csel	x1, x1, x0, pl  // pl = nfrst
  e4:	mov	x0, x19
  e8:	ldr	x19, [sp, #16]
  ec:	ldp	x29, x30, [sp], #48
  f0:	ret

floattihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattihf>:
   0:	mrs	x5, fpcr
   4:	orr	x2, x0, x1
   8:	cbnz	x2, 14 <__floattihf+0x14>
   c:	fmov	d0, xzr
  10:	ret
  14:	lsr	x2, x1, #63
  18:	and	w6, w2, #0xff
  1c:	mov	x2, x1
  20:	tbnz	x1, #63, 68 <__floattihf+0x68>
  24:	cbz	x2, 74 <__floattihf+0x74>
  28:	and	x3, x5, #0xc00000
  2c:	mov	x2, x3
  30:	cbnz	x3, 170 <__floattihf+0x170>
  34:	mov	w4, #0x1f                  	// #31
  38:	mov	w0, #0x14                  	// #20
  3c:	bfi	w2, w4, #10, #22
  40:	orr	w2, w2, w6, lsl #15
  44:	sxth	x1, w2
  48:	fmov	d0, x1
  4c:	stp	x29, x30, [sp, #-32]!
  50:	mov	x29, sp
  54:	str	s0, [sp, #28]
  58:	bl	0 <__sfp_handle_exceptions>
  5c:	ldr	s0, [sp, #28]
  60:	ldp	x29, x30, [sp], #32
  64:	ret
  68:	negs	x0, x0
  6c:	ngc	x2, x1
  70:	cbnz	x2, 28 <__floattihf+0x28>
  74:	clz	x3, x0
  78:	mov	w4, #0x4e                  	// #78
  7c:	sub	w3, w4, w3
  80:	mov	x2, x0
  84:	cmp	w3, #0x1e
  88:	sxtw	x4, w3
  8c:	b.gt	28 <__floattihf+0x28>
  90:	cmp	w3, #0x19
  94:	b.le	14c <__floattihf+0x14c>
  98:	cmp	x4, #0x1c
  9c:	b.le	1dc <__floattihf+0x1dc>
  a0:	sub	w0, w3, #0x1c
  a4:	mov	w7, #0x9c                  	// #156
  a8:	sub	w7, w7, w3
  ac:	cmp	w3, #0x5c
  b0:	sub	w8, w7, #0x40
  b4:	lsr	x9, x2, #1
  b8:	mov	w10, #0x3f                  	// #63
  bc:	sub	w10, w10, w7
  c0:	lsr	x0, x2, x0
  c4:	csel	x0, xzr, x0, pl  // pl = nfrst
  c8:	cmp	w8, #0x0
  cc:	lsl	x3, x2, x8
  d0:	lsr	x9, x9, x10
  d4:	csel	x3, x3, x9, ge  // ge = tcont
  d8:	lsl	x2, x2, x7
  dc:	csel	x2, xzr, x2, ge  // ge = tcont
  e0:	orr	x2, x2, x3
  e4:	cmp	x2, #0x0
  e8:	cset	x2, ne  // ne = any
  ec:	orr	x2, x0, x2
  f0:	tst	x2, #0x7
  f4:	and	x2, x2, #0xffffffffffffdfff
  f8:	b.eq	26c <__floattihf+0x26c>  // b.none
  fc:	and	x3, x5, #0xc00000
 100:	mov	w0, #0x10                  	// #16
 104:	cmp	x3, #0x400, lsl #12
 108:	b.eq	274 <__floattihf+0x274>  // b.none
 10c:	cmp	x3, #0x800, lsl #12
 110:	b.eq	2cc <__floattihf+0x2cc>  // b.none
 114:	cbz	x3, 2a4 <__floattihf+0x2a4>
 118:	lsr	x2, x2, #3
 11c:	and	w1, w4, #0x1f
 120:	cmp	x2, #0x0
 124:	ccmp	x4, #0x1f, #0x0, ne  // ne = any
 128:	b.ne	134 <__floattihf+0x134>  // b.any
 12c:	orr	x2, x2, #0x200
 130:	mov	w1, #0x1f                  	// #31
 134:	bfi	w2, w1, #10, #22
 138:	orr	w2, w2, w6, lsl #15
 13c:	sxth	x1, w2
 140:	fmov	d0, x1
 144:	cbnz	w0, 4c <__floattihf+0x4c>
 148:	ret
 14c:	cmp	x4, #0x19
 150:	b.ne	214 <__floattihf+0x214>  // b.any
 154:	and	x0, x0, #0x3ff
 158:	mov	w1, #0x6400                	// #25600
 15c:	orr	w0, w1, w0
 160:	orr	w0, w0, w6, lsl #15
 164:	sxth	x0, w0
 168:	fmov	d0, x0
 16c:	ret
 170:	mvn	x0, x1
 174:	cmp	x3, #0x400, lsl #12
 178:	cset	w4, eq  // eq = none
 17c:	lsr	x2, x0, #63
 180:	tst	w4, w2
 184:	b.ne	2bc <__floattihf+0x2bc>  // b.any
 188:	cmp	x3, #0x800, lsl #12
 18c:	ccmp	w6, #0x0, #0x4, eq  // eq = none
 190:	b.ne	2bc <__floattihf+0x2bc>  // b.any
 194:	cmp	x3, #0x400, lsl #12
 198:	b.eq	258 <__floattihf+0x258>  // b.none
 19c:	cmp	x3, #0x800, lsl #12
 1a0:	b.eq	238 <__floattihf+0x238>  // b.none
 1a4:	cmp	x3, #0x400, lsl #12
 1a8:	lsr	x0, x0, #63
 1ac:	cset	w1, eq  // eq = none
 1b0:	tst	w1, w0
 1b4:	b.ne	240 <__floattihf+0x240>  // b.any
 1b8:	cmp	x3, #0x800, lsl #12
 1bc:	ccmp	w6, #0x0, #0x4, eq  // eq = none
 1c0:	b.ne	2d4 <__floattihf+0x2d4>  // b.any
 1c4:	mov	w1, #0x7bff                	// #31743
 1c8:	mov	w0, #0x14                  	// #20
 1cc:	orr	w1, w1, w6, lsl #15
 1d0:	sxth	x1, w1
 1d4:	fmov	d0, x1
 1d8:	b	4c <__floattihf+0x4c>
 1dc:	b.eq	f0 <__floattihf+0xf0>  // b.none
 1e0:	mov	w2, #0x1c                  	// #28
 1e4:	sub	w2, w2, w3
 1e8:	lsl	x0, x0, x2
 1ec:	and	x2, x0, #0xffffffffffffdfff
 1f0:	tst	x0, #0x7
 1f4:	b.ne	fc <__floattihf+0xfc>  // b.any
 1f8:	and	w3, w3, #0x1f
 1fc:	ubfx	x2, x2, #3, #10
 200:	orr	w2, w2, w3, lsl #10
 204:	orr	w1, w2, w6, lsl #15
 208:	sxth	x0, w1
 20c:	fmov	d0, x0
 210:	ret
 214:	mov	w1, #0x19                  	// #25
 218:	sub	w1, w1, w3
 21c:	and	w3, w3, #0x1f
 220:	lsl	x0, x0, x1
 224:	bfi	w0, w3, #10, #22
 228:	orr	w0, w0, w6, lsl #15
 22c:	sxth	x0, w0
 230:	fmov	d0, x0
 234:	ret
 238:	tbnz	x1, #63, 25c <__floattihf+0x25c>
 23c:	cbnz	x3, 2e4 <__floattihf+0x2e4>
 240:	ubfiz	w1, w6, #15, #1
 244:	mov	w0, #0x14                  	// #20
 248:	orr	w1, w1, #0x7c00
 24c:	sxth	x1, w1
 250:	fmov	d0, x1
 254:	b	4c <__floattihf+0x4c>
 258:	tbnz	x1, #63, 23c <__floattihf+0x23c>
 25c:	mov	x2, #0xffffffffffffffff    	// #-1
 260:	mov	x4, #0x1e                  	// #30
 264:	mov	w0, #0x14                  	// #20
 268:	b	278 <__floattihf+0x278>
 26c:	mov	w0, #0x0                   	// #0
 270:	b	118 <__floattihf+0x118>
 274:	tbnz	x1, #63, 118 <__floattihf+0x118>
 278:	add	x2, x2, #0x8
 27c:	and	x5, x2, #0x2000
 280:	cbz	x5, 118 <__floattihf+0x118>
 284:	cmp	x4, #0x1e
 288:	add	x4, x4, #0x1
 28c:	b.eq	23c <__floattihf+0x23c>  // b.none
 290:	mov	x1, #0xfffffffffffffbff    	// #-1025
 294:	and	w4, w4, #0x1f
 298:	movk	x1, #0x1fff, lsl #48
 29c:	and	x2, x1, x2, lsr #3
 2a0:	b	3c <__floattihf+0x3c>
 2a4:	and	x5, x2, #0xf
 2a8:	cmp	x5, #0x4
 2ac:	b.eq	118 <__floattihf+0x118>  // b.none
 2b0:	add	x2, x2, #0x4
 2b4:	and	x5, x2, #0x2000
 2b8:	b	280 <__floattihf+0x280>
 2bc:	mov	w4, #0x1f                  	// #31
 2c0:	mov	w0, #0x14                  	// #20
 2c4:	mov	x2, #0x0                   	// #0
 2c8:	b	3c <__floattihf+0x3c>
 2cc:	tbnz	x1, #63, 278 <__floattihf+0x278>
 2d0:	b	118 <__floattihf+0x118>
 2d4:	mov	x0, #0xfffffffffffffc00    	// #-1024
 2d8:	fmov	d0, x0
 2dc:	mov	w0, #0x14                  	// #20
 2e0:	b	4c <__floattihf+0x4c>
 2e4:	mvn	x0, x1
 2e8:	b	1a4 <__floattihf+0x1a4>

floatuntihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntihf>:
   0:	mrs	x5, fpcr
   4:	orr	x2, x0, x1
   8:	cbnz	x2, 14 <__floatuntihf+0x14>
   c:	fmov	d0, xzr
  10:	ret
  14:	cbz	x1, 54 <__floatuntihf+0x54>
  18:	and	x2, x5, #0x800000
  1c:	mov	w3, #0x1f                  	// #31
  20:	mov	x1, x2
  24:	mov	w0, #0x14                  	// #20
  28:	cbnz	x2, 184 <__floatuntihf+0x184>
  2c:	bfi	w1, w3, #10, #22
  30:	and	x1, x1, #0x7fff
  34:	fmov	d0, x1
  38:	stp	x29, x30, [sp, #-32]!
  3c:	mov	x29, sp
  40:	str	s0, [sp, #28]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	s0, [sp, #28]
  4c:	ldp	x29, x30, [sp], #32
  50:	ret
  54:	clz	x2, x0
  58:	mov	w1, #0x4e                  	// #78
  5c:	sub	w2, w1, w2
  60:	mov	x1, x0
  64:	cmp	w2, #0x1e
  68:	sxtw	x3, w2
  6c:	b.gt	18 <__floatuntihf+0x18>
  70:	cmp	w2, #0x19
  74:	b.le	118 <__floatuntihf+0x118>
  78:	cmp	x3, #0x1c
  7c:	b.le	134 <__floatuntihf+0x134>
  80:	mov	w4, #0x9c                  	// #156
  84:	sub	w4, w4, w2
  88:	subs	w0, w4, #0x40
  8c:	lsr	x7, x1, #1
  90:	mov	w8, #0x3f                  	// #63
  94:	sub	w8, w8, w4
  98:	lsl	x6, x1, x0
  9c:	sub	w0, w2, #0x1c
  a0:	lsl	x4, x1, x4
  a4:	csel	x4, xzr, x4, pl  // pl = nfrst
  a8:	lsr	x7, x7, x8
  ac:	csel	x6, x6, x7, pl  // pl = nfrst
  b0:	orr	x4, x4, x6
  b4:	lsr	x1, x1, x0
  b8:	cmp	x4, #0x0
  bc:	cset	x0, ne  // ne = any
  c0:	cmp	w2, #0x5c
  c4:	csel	x1, xzr, x1, pl  // pl = nfrst
  c8:	orr	x1, x0, x1
  cc:	tst	x1, #0x7
  d0:	and	x1, x1, #0xffffffffffffdfff
  d4:	b.eq	224 <__floatuntihf+0x224>  // b.none
  d8:	ands	x0, x5, #0xc00000
  dc:	b.eq	208 <__floatuntihf+0x208>  // b.none
  e0:	cmp	x0, #0x400, lsl #12
  e4:	b.ne	1cc <__floatuntihf+0x1cc>  // b.any
  e8:	add	x1, x1, #0x8
  ec:	and	x0, x1, #0x2000
  f0:	cbz	x0, 1cc <__floatuntihf+0x1cc>
  f4:	cmp	x3, #0x1e
  f8:	add	x3, x3, #0x1
  fc:	b.eq	1a8 <__floatuntihf+0x1a8>  // b.none
 100:	mov	x0, #0xfffffffffffffbff    	// #-1025
 104:	and	w3, w3, #0x1f
 108:	movk	x0, #0x1fff, lsl #48
 10c:	and	x1, x0, x1, lsr #3
 110:	mov	w0, #0x10                  	// #16
 114:	b	2c <__floatuntihf+0x2c>
 118:	cmp	x3, #0x19
 11c:	b.ne	164 <__floatuntihf+0x164>  // b.any
 120:	and	x1, x0, #0x3ff
 124:	mov	w0, #0x6400                	// #25600
 128:	orr	w0, w0, w1
 12c:	fmov	s0, w0
 130:	ret
 134:	b.eq	cc <__floatuntihf+0xcc>  // b.none
 138:	mov	w1, #0x1c                  	// #28
 13c:	sub	w1, w1, w2
 140:	lsl	x0, x0, x1
 144:	and	x1, x0, #0xffffffffffffdfff
 148:	tst	x0, #0x7
 14c:	b.ne	d8 <__floatuntihf+0xd8>  // b.any
 150:	ubfx	x1, x1, #3, #10
 154:	ubfiz	x2, x2, #10, #5
 158:	orr	x0, x2, x1
 15c:	fmov	d0, x0
 160:	ret
 164:	mov	w1, #0x19                  	// #25
 168:	sub	w1, w1, w2
 16c:	and	w2, w2, #0x1f
 170:	lsl	x0, x0, x1
 174:	bfi	w0, w2, #10, #22
 178:	and	x0, x0, #0x7fff
 17c:	fmov	d0, x0
 180:	ret
 184:	ands	x0, x5, #0xc00000
 188:	b.eq	200 <__floatuntihf+0x200>  // b.none
 18c:	cmp	x0, #0x400, lsl #12
 190:	b.ne	1ac <__floatuntihf+0x1ac>  // b.any
 194:	mov	x1, #0x7                   	// #7
 198:	lsr	x1, x1, #3
 19c:	mov	w2, #0x1e                  	// #30
 1a0:	mov	w0, #0x14                  	// #20
 1a4:	b	1ec <__floatuntihf+0x1ec>
 1a8:	and	x2, x5, #0x800000
 1ac:	mov	x0, #0x7c00                	// #31744
 1b0:	fmov	d0, x0
 1b4:	mov	w0, #0x14                  	// #20
 1b8:	cbz	x2, 38 <__floatuntihf+0x38>
 1bc:	mov	x0, #0x7bff                	// #31743
 1c0:	fmov	d0, x0
 1c4:	mov	w0, #0x14                  	// #20
 1c8:	b	38 <__floatuntihf+0x38>
 1cc:	mov	w0, #0x10                  	// #16
 1d0:	lsr	x1, x1, #3
 1d4:	and	w2, w2, #0x1f
 1d8:	cmp	x1, #0x0
 1dc:	ccmp	x3, #0x1f, #0x0, ne  // ne = any
 1e0:	b.ne	1ec <__floatuntihf+0x1ec>  // b.any
 1e4:	orr	x1, x1, #0x200
 1e8:	mov	w2, #0x1f                  	// #31
 1ec:	bfi	w1, w2, #10, #22
 1f0:	and	x1, x1, #0x7fff
 1f4:	fmov	d0, x1
 1f8:	cbnz	w0, 38 <__floatuntihf+0x38>
 1fc:	ret
 200:	mov	x1, #0x3                   	// #3
 204:	b	198 <__floatuntihf+0x198>
 208:	and	x4, x1, #0xf
 20c:	mov	w0, #0x10                  	// #16
 210:	cmp	x4, #0x4
 214:	b.eq	1d0 <__floatuntihf+0x1d0>  // b.none
 218:	add	x1, x1, #0x4
 21c:	and	x0, x1, #0x2000
 220:	b	f0 <__floatuntihf+0xf0>
 224:	mov	w0, #0x0                   	// #0
 228:	b	1d0 <__floatuntihf+0x1d0>

enable-execute-stack.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	ret
