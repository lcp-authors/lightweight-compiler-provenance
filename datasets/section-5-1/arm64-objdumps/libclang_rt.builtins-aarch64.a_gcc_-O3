In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_gcc_-O3:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	sub	sp, sp, #0x20
   4:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
   8:	str	q0, [sp]
   c:	ldp	x1, x2, [sp]
  10:	str	q1, [sp, #16]
  14:	ldp	x0, x3, [sp, #16]
  18:	and	x5, x2, #0x7fffffffffffffff
  1c:	cmp	x5, x4
  20:	mov	w4, #0x1                   	// #1
  24:	mov	x5, x0
  28:	b.hi	34 <__cmptf2+0x34>  // b.pmore
  2c:	b.eq	b0 <__cmptf2+0xb0>  // b.none
  30:	mov	w4, #0x0                   	// #0
  34:	and	x7, x3, #0x7fffffffffffffff
  38:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
  3c:	mov	w0, #0x1                   	// #1
  40:	cmp	x7, x6
  44:	b.hi	50 <__cmptf2+0x50>  // b.pmore
  48:	b.eq	a4 <__cmptf2+0xa4>  // b.none
  4c:	mov	w0, #0x0                   	// #0
  50:	orr	w4, w4, w0
  54:	mov	w0, #0x1                   	// #1
  58:	tbnz	w4, #0, 9c <__cmptf2+0x9c>
  5c:	orr	x6, x2, x3
  60:	orr	x4, x1, x5
  64:	and	x6, x6, #0x7fffffffffffffff
  68:	mov	w0, #0x0                   	// #0
  6c:	orr	x4, x4, x6
  70:	cbz	x4, 9c <__cmptf2+0x9c>
  74:	tst	x2, x3
  78:	b.mi	d4 <__cmptf2+0xd4>  // b.first
  7c:	cmp	x3, x2
  80:	b.gt	c8 <__cmptf2+0xc8>
  84:	b.eq	fc <__cmptf2+0xfc>  // b.none
  88:	eor	x1, x1, x5
  8c:	eor	x2, x2, x3
  90:	orr	x1, x1, x2
  94:	cmp	x1, #0x0
  98:	cset	w0, ne  // ne = any
  9c:	add	sp, sp, #0x20
  a0:	ret
  a4:	cbnz	x5, 50 <__cmptf2+0x50>
  a8:	mov	w0, #0x0                   	// #0
  ac:	b	50 <__cmptf2+0x50>
  b0:	cbnz	x1, 34 <__cmptf2+0x34>
  b4:	mov	w4, #0x0                   	// #0
  b8:	b	34 <__cmptf2+0x34>
  bc:	cmp	x1, x5
  c0:	b.ls	e0 <__cmptf2+0xe0>  // b.plast
  c4:	nop
  c8:	mov	w0, #0xffffffff            	// #-1
  cc:	add	sp, sp, #0x20
  d0:	ret
  d4:	cmp	x2, x3
  d8:	b.gt	c8 <__cmptf2+0xc8>
  dc:	b.eq	bc <__cmptf2+0xbc>  // b.none
  e0:	eor	x0, x1, x5
  e4:	eor	x1, x2, x3
  e8:	orr	x1, x0, x1
  ec:	cmp	x1, #0x0
  f0:	add	sp, sp, #0x20
  f4:	cset	w0, ne  // ne = any
  f8:	ret
  fc:	cmp	x5, x1
 100:	b.ls	88 <__cmptf2+0x88>  // b.plast
 104:	mov	w0, #0xffffffff            	// #-1
 108:	b	cc <__cmptf2+0xcc>
 10c:	nop

0000000000000110 <__getf2>:
 110:	sub	sp, sp, #0x20
 114:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 118:	str	q0, [sp]
 11c:	ldp	x1, x2, [sp]
 120:	str	q1, [sp, #16]
 124:	ldp	x0, x3, [sp, #16]
 128:	and	x5, x2, #0x7fffffffffffffff
 12c:	cmp	x5, x4
 130:	mov	w4, #0x1                   	// #1
 134:	mov	x5, x0
 138:	b.hi	144 <__getf2+0x34>  // b.pmore
 13c:	b.eq	1c0 <__getf2+0xb0>  // b.none
 140:	mov	w4, #0x0                   	// #0
 144:	and	x7, x3, #0x7fffffffffffffff
 148:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
 14c:	mov	w0, #0x1                   	// #1
 150:	cmp	x7, x6
 154:	b.hi	160 <__getf2+0x50>  // b.pmore
 158:	b.eq	1b4 <__getf2+0xa4>  // b.none
 15c:	mov	w0, #0x0                   	// #0
 160:	orr	w4, w4, w0
 164:	mov	w0, #0xffffffff            	// #-1
 168:	tbnz	w4, #0, 1ac <__getf2+0x9c>
 16c:	orr	x6, x2, x3
 170:	orr	x4, x1, x5
 174:	and	x6, x6, #0x7fffffffffffffff
 178:	mov	w0, #0x0                   	// #0
 17c:	orr	x4, x4, x6
 180:	cbz	x4, 1ac <__getf2+0x9c>
 184:	tst	x2, x3
 188:	b.mi	1e4 <__getf2+0xd4>  // b.first
 18c:	cmp	x3, x2
 190:	b.gt	1d8 <__getf2+0xc8>
 194:	b.eq	20c <__getf2+0xfc>  // b.none
 198:	eor	x1, x1, x5
 19c:	eor	x2, x2, x3
 1a0:	orr	x1, x1, x2
 1a4:	cmp	x1, #0x0
 1a8:	cset	w0, ne  // ne = any
 1ac:	add	sp, sp, #0x20
 1b0:	ret
 1b4:	cbnz	x5, 160 <__getf2+0x50>
 1b8:	mov	w0, #0x0                   	// #0
 1bc:	b	160 <__getf2+0x50>
 1c0:	cbnz	x1, 144 <__getf2+0x34>
 1c4:	mov	w4, #0x0                   	// #0
 1c8:	b	144 <__getf2+0x34>
 1cc:	cmp	x1, x5
 1d0:	b.ls	1f0 <__getf2+0xe0>  // b.plast
 1d4:	nop
 1d8:	mov	w0, #0xffffffff            	// #-1
 1dc:	add	sp, sp, #0x20
 1e0:	ret
 1e4:	cmp	x2, x3
 1e8:	b.gt	1d8 <__getf2+0xc8>
 1ec:	b.eq	1cc <__getf2+0xbc>  // b.none
 1f0:	eor	x0, x1, x5
 1f4:	eor	x1, x2, x3
 1f8:	orr	x1, x0, x1
 1fc:	cmp	x1, #0x0
 200:	add	sp, sp, #0x20
 204:	cset	w0, ne  // ne = any
 208:	ret
 20c:	cmp	x5, x1
 210:	b.ls	198 <__getf2+0x88>  // b.plast
 214:	mov	w0, #0xffffffff            	// #-1
 218:	b	1dc <__getf2+0xcc>
 21c:	nop

0000000000000220 <__unordtf2>:
 220:	sub	sp, sp, #0x20
 224:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 228:	str	q0, [sp]
 22c:	ldp	x0, x2, [sp]
 230:	str	q1, [sp, #16]
 234:	ldp	x3, x1, [sp, #16]
 238:	and	x2, x2, #0x7fffffffffffffff
 23c:	cmp	x2, x4
 240:	mov	w2, #0x1                   	// #1
 244:	b.hi	250 <__unordtf2+0x30>  // b.pmore
 248:	b.eq	27c <__unordtf2+0x5c>  // b.none
 24c:	mov	w2, #0x0                   	// #0
 250:	and	x1, x1, #0x7fffffffffffffff
 254:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 258:	mov	w0, #0x1                   	// #1
 25c:	cmp	x1, x4
 260:	b.hi	26c <__unordtf2+0x4c>  // b.pmore
 264:	b.eq	288 <__unordtf2+0x68>  // b.none
 268:	mov	w0, #0x0                   	// #0
 26c:	orr	w0, w2, w0
 270:	and	w0, w0, #0x1
 274:	add	sp, sp, #0x20
 278:	ret
 27c:	cbnz	x0, 250 <__unordtf2+0x30>
 280:	mov	w2, #0x0                   	// #0
 284:	b	250 <__unordtf2+0x30>
 288:	cbnz	x3, 26c <__unordtf2+0x4c>
 28c:	b	268 <__unordtf2+0x48>

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	fmov	x0, d0
   4:	mov	x2, #0xfff0000000000000    	// #-4503599627370496
   8:	mov	x3, #0x7fdfffffffffffff    	// #9214364837600034815
   c:	and	x1, x0, #0x7fffffffffffffff
  10:	and	x0, x0, #0x8000000000000000
  14:	add	x2, x1, x2
  18:	cmp	x2, x3
  1c:	b.hi	3c <__extenddftf2+0x3c>  // b.pmore
  20:	lsl	x4, x1, #60
  24:	mov	x2, #0x3c00000000000000    	// #4323455642275676160
  28:	fmov	d0, x4
  2c:	add	x1, x2, x1, lsr #4
  30:	orr	x3, x0, x1
  34:	fmov	v0.d[1], x3
  38:	ret
  3c:	mov	x2, #0x7fefffffffffffff    	// #9218868437227405311
  40:	cmp	x1, x2
  44:	b.ls	64 <__extenddftf2+0x64>  // b.plast
  48:	lsl	x4, x1, #60
  4c:	ubfx	x2, x1, #4, #48
  50:	fmov	d0, x4
  54:	orr	x1, x2, #0x7fff000000000000
  58:	orr	x3, x0, x1
  5c:	fmov	v0.d[1], x3
  60:	ret
  64:	cbz	x1, b4 <__extenddftf2+0xb4>
  68:	clz	x7, x1
  6c:	lsr	x2, x1, #1
  70:	add	w4, w7, #0x31
  74:	subs	w5, w7, #0xf
  78:	mov	w6, #0x3f                  	// #63
  7c:	sub	w6, w6, w4
  80:	mov	w3, #0x3c0c                	// #15372
  84:	sub	w3, w3, w7
  88:	lsr	x6, x2, x6
  8c:	lsl	x2, x1, x5
  90:	csel	x2, x2, x6, pl  // pl = nfrst
  94:	eor	x2, x2, #0x1000000000000
  98:	lsl	x4, x1, x4
  9c:	csel	x4, x4, xzr, mi  // mi = first
  a0:	fmov	d0, x4
  a4:	orr	x1, x2, x3, lsl #48
  a8:	orr	x3, x0, x1
  ac:	fmov	v0.d[1], x3
  b0:	ret
  b4:	mov	x4, #0x0                   	// #0
  b8:	fmov	d0, x4
  bc:	mov	x1, #0x0                   	// #0
  c0:	orr	x3, x0, x1
  c4:	fmov	v0.d[1], x3
  c8:	ret

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	fmov	w0, s0
   4:	mov	w2, #0x7effffff            	// #2130706431
   8:	and	w1, w0, #0x7fffffff
   c:	and	w0, w0, #0x80000000
  10:	sub	w3, w1, #0x800, lsl #12
  14:	cmp	w3, w2
  18:	b.hi	3c <__extendsftf2+0x3c>  // b.pmore
  1c:	mov	x4, #0x0                   	// #0
  20:	ubfiz	x1, x1, #25, #31
  24:	fmov	d0, x4
  28:	mov	x2, #0x3f80000000000000    	// #4575657221408423936
  2c:	add	x1, x1, x2
  30:	orr	x3, x1, x0, lsl #32
  34:	fmov	v0.d[1], x3
  38:	ret
  3c:	mov	w2, #0x7f7fffff            	// #2139095039
  40:	cmp	w1, w2
  44:	b.ls	64 <__extendsftf2+0x64>  // b.plast
  48:	mov	x4, #0x0                   	// #0
  4c:	ubfiz	x1, x1, #25, #23
  50:	fmov	d0, x4
  54:	orr	x1, x1, #0x7fff000000000000
  58:	orr	x3, x1, x0, lsl #32
  5c:	fmov	v0.d[1], x3
  60:	ret
  64:	cbz	w1, 98 <__extendsftf2+0x98>
  68:	clz	w3, w1
  6c:	mov	w2, #0x3f89                	// #16265
  70:	add	w5, w3, #0x11
  74:	sub	w2, w2, w3
  78:	mov	x4, #0x0                   	// #0
  7c:	fmov	d0, x4
  80:	lsl	x1, x1, x5
  84:	eor	x1, x1, #0x1000000000000
  88:	orr	x1, x1, x2, lsl #48
  8c:	orr	x3, x1, x0, lsl #32
  90:	fmov	v0.d[1], x3
  94:	ret
  98:	mov	x4, #0x0                   	// #0
  9c:	fmov	d0, x4
  a0:	mov	x1, #0x0                   	// #0
  a4:	orr	x3, x1, x0, lsl #32
  a8:	fmov	v0.d[1], x3
  ac:	ret

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldp	x0, x1, [sp]
   c:	mov	x2, x0
  10:	tbnz	x1, #63, 94 <__fixtfdi+0x94>
  14:	mov	x5, #0x1                   	// #1
  18:	mov	x6, x5
  1c:	ubfx	x4, x1, #48, #15
  20:	mov	w3, #0xffffc001            	// #-16383
  24:	mov	x0, #0x0                   	// #0
  28:	add	w3, w4, w3
  2c:	tbnz	w3, #31, 78 <__fixtfdi+0x78>
  30:	cmp	w3, #0x3f
  34:	b.hi	80 <__fixtfdi+0x80>  // b.pmore
  38:	and	x1, x1, #0xffffffffffff
  3c:	cmp	w3, #0x6f
  40:	orr	x1, x1, #0x1000000000000
  44:	b.gt	a0 <__fixtfdi+0xa0>
  48:	mov	w0, #0x70                  	// #112
  4c:	sub	w0, w0, w3
  50:	mov	w4, #0x3f                  	// #63
  54:	lsl	x3, x1, #1
  58:	sub	w4, w4, w0
  5c:	subs	w6, w0, #0x40
  60:	lsr	x0, x2, x0
  64:	lsl	x2, x3, x4
  68:	orr	x0, x2, x0
  6c:	lsr	x1, x1, x6
  70:	csel	x0, x1, x0, pl  // pl = nfrst
  74:	mul	x0, x5, x0
  78:	add	sp, sp, #0x10
  7c:	ret
  80:	cmp	x6, #0x1
  84:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  88:	add	sp, sp, #0x10
  8c:	cinv	x0, x0, eq  // eq = none
  90:	ret
  94:	mov	x5, #0xffffffffffffffff    	// #-1
  98:	mov	x6, x5
  9c:	b	1c <__fixtfdi+0x1c>
  a0:	mov	w0, #0xffffbf91            	// #-16495
  a4:	add	w0, w4, w0
  a8:	add	sp, sp, #0x10
  ac:	lsl	x0, x2, x0
  b0:	mul	x0, x0, x6
  b4:	ret

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldp	x0, x1, [sp]
   c:	mov	x2, x0
  10:	tbnz	x1, #63, 94 <__fixtfsi+0x94>
  14:	mov	w5, #0x1                   	// #1
  18:	mov	w6, w5
  1c:	ubfx	x4, x1, #48, #15
  20:	mov	w3, #0xffffc001            	// #-16383
  24:	mov	w0, #0x0                   	// #0
  28:	add	w3, w4, w3
  2c:	tbnz	w3, #31, 78 <__fixtfsi+0x78>
  30:	cmp	w3, #0x1f
  34:	b.hi	80 <__fixtfsi+0x80>  // b.pmore
  38:	and	x1, x1, #0xffffffffffff
  3c:	cmp	w3, #0x6f
  40:	orr	x1, x1, #0x1000000000000
  44:	b.gt	a0 <__fixtfsi+0xa0>
  48:	mov	w0, #0x70                  	// #112
  4c:	sub	w0, w0, w3
  50:	mov	w4, #0x3f                  	// #63
  54:	lsl	x3, x1, #1
  58:	sub	w4, w4, w0
  5c:	subs	w6, w0, #0x40
  60:	lsr	x0, x2, x0
  64:	lsl	x2, x3, x4
  68:	orr	x0, x2, x0
  6c:	lsr	x1, x1, x6
  70:	csel	x0, x1, x0, pl  // pl = nfrst
  74:	mul	w0, w5, w0
  78:	add	sp, sp, #0x10
  7c:	ret
  80:	cmp	w6, #0x1
  84:	mov	w0, #0x80000000            	// #-2147483648
  88:	add	sp, sp, #0x10
  8c:	cinv	w0, w0, eq  // eq = none
  90:	ret
  94:	mov	w5, #0xffffffff            	// #-1
  98:	mov	w6, w5
  9c:	b	1c <__fixtfsi+0x1c>
  a0:	mov	w0, #0xffffbf91            	// #-16495
  a4:	add	w0, w4, w0
  a8:	add	sp, sp, #0x10
  ac:	lsl	w0, w2, w0
  b0:	mul	w0, w0, w6
  b4:	ret

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldp	x0, x2, [sp]
   c:	tbnz	x2, #63, b0 <__fixtfti+0xb0>
  10:	ubfx	x5, x2, #48, #15
  14:	mov	x3, #0x1                   	// #1
  18:	mov	w9, #0xffffc001            	// #-16383
  1c:	mov	x6, x3
  20:	add	w4, w5, w9
  24:	mov	x7, #0x0                   	// #0
  28:	mov	x1, #0x0                   	// #0
  2c:	tbnz	w4, #31, d0 <__fixtfti+0xd0>
  30:	cmp	w4, #0x7f
  34:	b.hi	94 <__fixtfti+0x94>  // b.pmore
  38:	and	x2, x2, #0xffffffffffff
  3c:	cmp	w4, #0x6f
  40:	orr	x2, x2, #0x1000000000000
  44:	b.gt	e0 <__fixtfti+0xe0>
  48:	mov	w5, #0x70                  	// #112
  4c:	sub	w4, w5, w4
  50:	lsl	x6, x2, #1
  54:	mov	w8, #0x3f                  	// #63
  58:	sub	w8, w8, w4
  5c:	subs	w5, w4, #0x40
  60:	lsr	x1, x0, x4
  64:	lsl	x0, x6, x8
  68:	orr	x1, x0, x1
  6c:	lsr	x0, x2, x5
  70:	csel	x1, x0, x1, pl  // pl = nfrst
  74:	lsr	x2, x2, x4
  78:	csel	x2, xzr, x2, pl  // pl = nfrst
  7c:	umulh	x4, x1, x3
  80:	madd	x4, x2, x3, x4
  84:	mul	x0, x1, x3
  88:	madd	x1, x1, x7, x4
  8c:	add	sp, sp, #0x10
  90:	ret
  94:	cmp	x6, #0x1
  98:	b.eq	134 <__fixtfti+0x134>  // b.none
  9c:	adrp	x1, 10 <__fixtfti+0x10>
  a0:	mov	x0, #0x0                   	// #0
  a4:	ldr	x1, [x1]
  a8:	add	sp, sp, #0x10
  ac:	ret
  b0:	ubfx	x5, x2, #48, #15
  b4:	mov	x3, #0xffffffffffffffff    	// #-1
  b8:	mov	w9, #0xffffc001            	// #-16383
  bc:	mov	x7, x3
  c0:	mov	x6, x3
  c4:	mov	x1, x3
  c8:	add	w4, w5, w9
  cc:	tbz	w4, #31, 30 <__fixtfti+0x30>
  d0:	mov	x0, #0x0                   	// #0
  d4:	mov	x1, #0x0                   	// #0
  d8:	add	sp, sp, #0x10
  dc:	ret
  e0:	mov	w3, #0xffffbf91            	// #-16495
  e4:	add	w3, w5, w3
  e8:	lsr	x4, x0, #1
  ec:	mov	w7, #0x3f                  	// #63
  f0:	sub	w7, w7, w3
  f4:	mov	w8, #0xffffbf51            	// #-16559
  f8:	add	w5, w5, w8
  fc:	lsl	x2, x2, x3
 100:	cmp	w5, #0x0
 104:	lsr	x4, x4, x7
 108:	orr	x2, x4, x2
 10c:	lsl	x4, x0, x5
 110:	lsl	x0, x0, x3
 114:	csel	x0, xzr, x0, ge  // ge = tcont
 118:	csel	x2, x4, x2, ge  // ge = tcont
 11c:	add	sp, sp, #0x10
 120:	umulh	x3, x0, x6
 124:	madd	x3, x2, x6, x3
 128:	madd	x1, x0, x1, x3
 12c:	mul	x0, x0, x6
 130:	ret
 134:	cbnz	x1, 9c <__fixtfti+0x9c>
 138:	adrp	x1, 0 <__fixtfti>
 13c:	mov	x0, #0xffffffffffffffff    	// #-1
 140:	ldr	x1, [x1]
 144:	b	8c <__fixtfti+0x8c>

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	sub	sp, sp, #0x10
   4:	mov	x0, #0x0                   	// #0
   8:	str	q0, [sp]
   c:	ldp	x2, x1, [sp]
  10:	tbnz	x1, #63, 6c <__fixunstfdi+0x6c>
  14:	lsr	x4, x1, #48
  18:	and	x1, x1, #0xffffffffffff
  1c:	mov	w3, #0xffffc001            	// #-16383
  20:	orr	x1, x1, #0x1000000000000
  24:	add	w3, w4, w3
  28:	tbnz	w3, #31, 6c <__fixunstfdi+0x6c>
  2c:	cmp	w3, #0x3f
  30:	mov	x0, #0xffffffffffffffff    	// #-1
  34:	b.hi	6c <__fixunstfdi+0x6c>  // b.pmore
  38:	cmp	w3, #0x6f
  3c:	b.gt	74 <__fixunstfdi+0x74>
  40:	mov	w0, #0x70                  	// #112
  44:	sub	w3, w0, w3
  48:	lsl	x4, x1, #1
  4c:	mov	w5, #0x3f                  	// #63
  50:	sub	w5, w5, w3
  54:	subs	w0, w3, #0x40
  58:	lsr	x2, x2, x3
  5c:	lsl	x3, x4, x5
  60:	orr	x2, x3, x2
  64:	lsr	x1, x1, x0
  68:	csel	x0, x1, x2, pl  // pl = nfrst
  6c:	add	sp, sp, #0x10
  70:	ret
  74:	mov	w0, #0xffffbf91            	// #-16495
  78:	add	w1, w4, w0
  7c:	add	sp, sp, #0x10
  80:	lsl	x0, x2, x1
  84:	ret

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	sub	sp, sp, #0x10
   4:	mov	w0, #0x0                   	// #0
   8:	str	q0, [sp]
   c:	ldp	x2, x1, [sp]
  10:	tbnz	x1, #63, 6c <__fixunstfsi+0x6c>
  14:	lsr	x4, x1, #48
  18:	and	x1, x1, #0xffffffffffff
  1c:	mov	w3, #0xffffc001            	// #-16383
  20:	orr	x1, x1, #0x1000000000000
  24:	add	w3, w4, w3
  28:	tbnz	w3, #31, 6c <__fixunstfsi+0x6c>
  2c:	cmp	w3, #0x1f
  30:	mov	w0, #0xffffffff            	// #-1
  34:	b.hi	6c <__fixunstfsi+0x6c>  // b.pmore
  38:	cmp	w3, #0x6f
  3c:	b.gt	74 <__fixunstfsi+0x74>
  40:	mov	w0, #0x70                  	// #112
  44:	sub	w3, w0, w3
  48:	lsl	x4, x1, #1
  4c:	mov	w5, #0x3f                  	// #63
  50:	sub	w5, w5, w3
  54:	subs	w0, w3, #0x40
  58:	lsr	x2, x2, x3
  5c:	lsl	x3, x4, x5
  60:	orr	x2, x3, x2
  64:	lsr	x1, x1, x0
  68:	csel	x0, x1, x2, pl  // pl = nfrst
  6c:	add	sp, sp, #0x10
  70:	ret
  74:	mov	w0, #0xffffbf91            	// #-16495
  78:	add	w1, w4, w0
  7c:	add	sp, sp, #0x10
  80:	lsl	w0, w2, w1
  84:	ret

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	sub	sp, sp, #0x10
   4:	str	q0, [sp]
   8:	ldp	x0, x1, [sp]
   c:	tbnz	x1, #63, b8 <__fixunstfti+0xb8>
  10:	lsr	x2, x1, #48
  14:	and	x1, x1, #0xffffffffffff
  18:	mov	w7, #0xffffc001            	// #-16383
  1c:	orr	x1, x1, #0x1000000000000
  20:	add	w3, w2, w7
  24:	tbnz	w3, #31, b8 <__fixunstfti+0xb8>
  28:	cmp	w3, #0x7f
  2c:	b.hi	c8 <__fixunstfti+0xc8>  // b.pmore
  30:	cmp	w3, #0x6f
  34:	b.le	7c <__fixunstfti+0x7c>
  38:	mov	w3, #0xffffbf91            	// #-16495
  3c:	add	w3, w2, w3
  40:	lsr	x4, x0, #1
  44:	mov	w5, #0x3f                  	// #63
  48:	sub	w5, w5, w3
  4c:	mov	w6, #0xffffbf51            	// #-16559
  50:	add	w2, w2, w6
  54:	lsl	x1, x1, x3
  58:	cmp	w2, #0x0
  5c:	lsr	x4, x4, x5
  60:	orr	x1, x4, x1
  64:	lsl	x4, x0, x2
  68:	add	sp, sp, #0x10
  6c:	csel	x1, x4, x1, ge  // ge = tcont
  70:	lsl	x0, x0, x3
  74:	csel	x0, xzr, x0, ge  // ge = tcont
  78:	ret
  7c:	mov	w2, #0x70                  	// #112
  80:	sub	w3, w2, w3
  84:	lsl	x4, x1, #1
  88:	mov	w5, #0x3f                  	// #63
  8c:	sub	w5, w5, w3
  90:	subs	w2, w3, #0x40
  94:	lsr	x0, x0, x3
  98:	lsl	x4, x4, x5
  9c:	orr	x0, x4, x0
  a0:	add	sp, sp, #0x10
  a4:	lsr	x4, x1, x2
  a8:	lsr	x1, x1, x3
  ac:	csel	x0, x4, x0, pl  // pl = nfrst
  b0:	csel	x1, xzr, x1, pl  // pl = nfrst
  b4:	ret
  b8:	mov	x0, #0x0                   	// #0
  bc:	mov	x1, #0x0                   	// #0
  c0:	add	sp, sp, #0x10
  c4:	ret
  c8:	mov	x0, #0xffffffffffffffff    	// #-1
  cc:	mov	x1, x0
  d0:	add	sp, sp, #0x10
  d4:	ret

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	x0, 58 <__floatditf+0x58>
   8:	mov	x7, #0x0                   	// #0
   c:	tbnz	x0, #63, 5c <__floatditf+0x5c>
  10:	clz	x3, x0
  14:	lsr	x4, x0, #1
  18:	add	w6, w3, #0x31
  1c:	subs	w5, w3, #0xf
  20:	mov	w2, #0x3f                  	// #63
  24:	sub	w2, w2, w6
  28:	mov	w1, #0x403e                	// #16446
  2c:	sub	w1, w1, w3
  30:	lsr	x2, x4, x2
  34:	lsl	x4, x0, x5
  38:	csel	x4, x4, x2, pl  // pl = nfrst
  3c:	eor	x4, x4, #0x1000000000000
  40:	lsl	x0, x0, x6
  44:	csel	x2, x0, xzr, mi  // mi = first
  48:	fmov	d0, x2
  4c:	add	x1, x4, x1, lsl #48
  50:	orr	x3, x1, x7
  54:	fmov	v0.d[1], x3
  58:	ret
  5c:	adrp	x1, 0 <__floatditf>
  60:	neg	x0, x0
  64:	ldr	x7, [x1]
  68:	b	10 <__floatditf+0x10>

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	w0, 40 <__floatsitf+0x40>
   8:	mov	x4, #0x0                   	// #0
   c:	tbnz	w0, #31, 44 <__floatsitf+0x44>
  10:	clz	w2, w0
  14:	mov	w0, w0
  18:	add	w3, w2, #0x11
  1c:	mov	w1, #0x401e                	// #16414
  20:	sub	w1, w1, w2
  24:	mov	x2, #0x0                   	// #0
  28:	lsl	x0, x0, x3
  2c:	fmov	d0, x2
  30:	eor	x0, x0, #0x1000000000000
  34:	add	x0, x0, x1, lsl #48
  38:	orr	x3, x0, x4
  3c:	fmov	v0.d[1], x3
  40:	ret
  44:	adrp	x1, 0 <__floatsitf>
  48:	neg	w0, w0
  4c:	ldr	x4, [x1]
  50:	b	10 <__floatsitf+0x10>

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floattitf+0x10>
   8:	movi	v0.2d, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-64]!
  14:	mov	x29, sp
  18:	stp	x21, x22, [sp, #32]
  1c:	asr	x21, x1, #63
  20:	eor	x0, x0, x21
  24:	stp	x19, x20, [sp, #16]
  28:	subs	x20, x0, x21
  2c:	eor	x1, x1, x21
  30:	sbc	x19, x1, x21
  34:	mov	x0, x20
  38:	mov	x1, x19
  3c:	str	x23, [sp, #48]
  40:	bl	0 <__clzti2>
  44:	mov	x22, x20
  48:	mov	w2, #0x80                  	// #128
  4c:	sub	w2, w2, w0
  50:	mov	x23, x19
  54:	cmp	w2, #0x71
  58:	sub	w6, w2, #0x1
  5c:	b.le	13c <__floattitf+0x13c>
  60:	cmp	w2, #0x72
  64:	b.eq	184 <__floattitf+0x184>  // b.none
  68:	cmp	w2, #0x73
  6c:	b.eq	ec <__floattitf+0xec>  // b.none
  70:	add	w5, w0, #0x73
  74:	mov	w4, #0x3f                  	// #63
  78:	sub	w1, w4, w5
  7c:	adds	w0, w0, #0x33
  80:	mov	x3, #0xffffffffffffffff    	// #-1
  84:	mov	x7, #0xfffffffffffffffe    	// #-2
  88:	lsl	x7, x7, x1
  8c:	lsr	x1, x3, x5
  90:	orr	x1, x7, x1
  94:	lsr	x7, x3, x0
  98:	csel	x1, x7, x1, pl  // pl = nfrst
  9c:	lsr	x0, x3, x5
  a0:	csel	x0, xzr, x0, pl  // pl = nfrst
  a4:	sub	w3, w2, #0x73
  a8:	and	x1, x1, x20
  ac:	and	x0, x0, x19
  b0:	sub	w4, w4, w3
  b4:	orr	x0, x1, x0
  b8:	lsl	x1, x19, #1
  bc:	cmp	x0, #0x0
  c0:	sub	w0, w2, #0xb3
  c4:	cset	x5, ne  // ne = any
  c8:	lsl	x4, x1, x4
  cc:	cmp	w0, #0x0
  d0:	lsr	x22, x20, x3
  d4:	orr	x22, x4, x22
  d8:	lsr	x1, x19, x0
  dc:	csel	x22, x1, x22, ge  // ge = tcont
  e0:	lsr	x1, x19, x3
  e4:	orr	x22, x5, x22
  e8:	csel	x23, x1, xzr, lt  // lt = tstop
  ec:	ubfx	x0, x22, #2, #1
  f0:	orr	x22, x0, x22
  f4:	adds	x22, x22, #0x1
  f8:	cinc	x1, x23, cs  // cs = hs, nlast
  fc:	asr	x0, x1, #2
 100:	extr	x20, x1, x22, #2
 104:	tbnz	x1, #51, 174 <__floattitf+0x174>
 108:	and	x21, x21, #0x8000000000000000
 10c:	fmov	d0, x20
 110:	and	x0, x0, #0xffffffffffff
 114:	mov	w1, #0x3fff                	// #16383
 118:	add	w1, w6, w1
 11c:	orr	x0, x0, x21
 120:	ldp	x19, x20, [sp, #16]
 124:	orr	x3, x0, x1, lsl #48
 128:	fmov	v0.d[1], x3
 12c:	ldp	x21, x22, [sp, #32]
 130:	ldr	x23, [sp, #48]
 134:	ldp	x29, x30, [sp], #64
 138:	ret
 13c:	mov	w0, #0x71                  	// #113
 140:	sub	w2, w0, w2
 144:	lsr	x4, x20, #1
 148:	mov	w5, #0x3f                  	// #63
 14c:	sub	w5, w5, w2
 150:	subs	w3, w2, #0x40
 154:	lsl	x0, x19, x2
 158:	lsr	x4, x4, x5
 15c:	orr	x0, x4, x0
 160:	lsl	x1, x20, x3
 164:	csel	x0, x1, x0, pl  // pl = nfrst
 168:	lsl	x20, x20, x2
 16c:	csel	x20, xzr, x20, pl  // pl = nfrst
 170:	b	108 <__floattitf+0x108>
 174:	asr	x0, x1, #3
 178:	extr	x20, x1, x22, #3
 17c:	mov	w6, w2
 180:	b	108 <__floattitf+0x108>
 184:	lsl	x22, x20, #1
 188:	extr	x1, x19, x20, #63
 18c:	mov	x23, x1
 190:	b	ec <__floattitf+0xec>

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	x0, 4c <__floatunditf+0x4c>
   8:	clz	x7, x0
   c:	lsr	x1, x0, #1
  10:	add	w8, w7, #0x31
  14:	subs	w5, w7, #0xf
  18:	mov	w6, #0x3f                  	// #63
  1c:	sub	w6, w6, w8
  20:	mov	w4, #0x403e                	// #16446
  24:	sub	w4, w4, w7
  28:	lsr	x6, x1, x6
  2c:	lsl	x1, x0, x5
  30:	csel	x1, x1, x6, pl  // pl = nfrst
  34:	lsl	x0, x0, x8
  38:	csel	x2, x0, xzr, mi  // mi = first
  3c:	fmov	d0, x2
  40:	eor	x1, x1, #0x1000000000000
  44:	add	x3, x1, x4, lsl #48
  48:	fmov	v0.d[1], x3
  4c:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	movi	v0.2d, #0x0
   4:	cbz	w0, 34 <__floatunsitf+0x34>
   8:	clz	w2, w0
   c:	mov	w0, w0
  10:	add	w4, w2, #0x11
  14:	mov	w1, #0x401e                	// #16414
  18:	sub	w1, w1, w2
  1c:	mov	x2, #0x0                   	// #0
  20:	fmov	d0, x2
  24:	lsl	x0, x0, x4
  28:	eor	x0, x0, #0x1000000000000
  2c:	add	x3, x0, x1, lsl #48
  30:	fmov	v0.d[1], x3
  34:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floatuntitf+0x10>
   8:	movi	v0.2d, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-32]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	mov	x19, x0
  20:	mov	x20, x1
  24:	bl	0 <__clzti2>
  28:	mov	w2, #0x80                  	// #128
  2c:	sub	w2, w2, w0
  30:	cmp	w2, #0x71
  34:	sub	w6, w2, #0x1
  38:	b.le	108 <__floatuntitf+0x108>
  3c:	cmp	w2, #0x72
  40:	b.eq	188 <__floatuntitf+0x188>  // b.none
  44:	cmp	w2, #0x73
  48:	b.eq	cc <__floatuntitf+0xcc>  // b.none
  4c:	sub	w4, w2, #0x73
  50:	lsl	x1, x20, #1
  54:	mov	w7, #0x3f                  	// #63
  58:	sub	w5, w7, w4
  5c:	subs	w3, w2, #0xb3
  60:	add	w8, w0, #0x73
  64:	lsl	x1, x1, x5
  68:	sub	w7, w7, w8
  6c:	lsr	x5, x19, x4
  70:	orr	x5, x1, x5
  74:	lsr	x1, x20, x3
  78:	add	w0, w0, #0x33
  7c:	csel	x5, x1, x5, pl  // pl = nfrst
  80:	mov	x9, #0xfffffffffffffffe    	// #-2
  84:	mov	x1, #0xffffffffffffffff    	// #-1
  88:	lsr	x4, x20, x4
  8c:	lsl	x7, x9, x7
  90:	csel	x4, xzr, x4, pl  // pl = nfrst
  94:	lsr	x3, x1, x8
  98:	cmp	w0, #0x0
  9c:	orr	x3, x7, x3
  a0:	lsr	x7, x1, x0
  a4:	lsr	x1, x1, x8
  a8:	csel	x3, x7, x3, ge  // ge = tcont
  ac:	csel	x1, xzr, x1, ge  // ge = tcont
  b0:	and	x19, x3, x19
  b4:	and	x1, x1, x20
  b8:	mov	x20, x4
  bc:	orr	x1, x19, x1
  c0:	cmp	x1, #0x0
  c4:	cset	x19, ne  // ne = any
  c8:	orr	x19, x5, x19
  cc:	ubfx	x3, x19, #2, #1
  d0:	orr	x19, x3, x19
  d4:	adds	x3, x19, #0x1
  d8:	cinc	x20, x20, cs  // cs = hs, nlast
  dc:	lsr	x1, x20, #2
  e0:	extr	x19, x20, x3, #2
  e4:	tbnz	x20, #51, 15c <__floatuntitf+0x15c>
  e8:	fmov	d0, x19
  ec:	mov	w0, #0x3fff                	// #16383
  f0:	add	w0, w6, w0
  f4:	ldp	x19, x20, [sp, #16]
  f8:	bfi	x1, x0, #48, #16
  fc:	ldp	x29, x30, [sp], #32
 100:	fmov	v0.d[1], x1
 104:	ret
 108:	mov	w1, #0x71                  	// #113
 10c:	sub	w2, w1, w2
 110:	subs	w3, w2, #0x40
 114:	lsr	x4, x19, #1
 118:	mov	w5, #0x3f                  	// #63
 11c:	sub	w5, w5, w2
 120:	lsl	x0, x19, x3
 124:	lsl	x1, x20, x2
 128:	lsr	x4, x4, x5
 12c:	orr	x1, x4, x1
 130:	csel	x1, x0, x1, pl  // pl = nfrst
 134:	lsl	x19, x19, x2
 138:	mov	w0, #0x3fff                	// #16383
 13c:	csel	x19, xzr, x19, pl  // pl = nfrst
 140:	add	w0, w6, w0
 144:	fmov	d0, x19
 148:	ldp	x19, x20, [sp, #16]
 14c:	bfi	x1, x0, #48, #16
 150:	ldp	x29, x30, [sp], #32
 154:	fmov	v0.d[1], x1
 158:	ret
 15c:	mov	w6, w2
 160:	extr	x19, x20, x3, #3
 164:	mov	w0, #0x3fff                	// #16383
 168:	fmov	d0, x19
 16c:	add	w0, w6, w0
 170:	lsr	x1, x20, #3
 174:	ldp	x19, x20, [sp, #16]
 178:	bfi	x1, x0, #48, #16
 17c:	ldp	x29, x30, [sp], #32
 180:	fmov	v0.d[1], x1
 184:	ret
 188:	extr	x1, x20, x19, #63
 18c:	mov	x20, x1
 190:	lsl	x19, x19, #1
 194:	b	cc <__floatuntitf+0xcc>

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-256]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x23, x24, [sp, #48]
  10:	stp	x25, x26, [sp, #64]
  14:	str	q0, [sp, #96]
  18:	str	q2, [sp, #112]
  1c:	ldp	x20, x25, [sp, #96]
  20:	stp	x21, x22, [sp, #32]
  24:	ldp	x19, x24, [sp, #112]
  28:	stp	x19, x24, [sp, #96]
  2c:	stp	x20, x25, [sp, #112]
  30:	stp	x27, x28, [sp, #80]
  34:	ldr	q0, [sp, #112]
  38:	str	q1, [sp, #128]
  3c:	str	q3, [sp, #144]
  40:	ldp	x21, x23, [sp, #128]
  44:	ldp	x22, x26, [sp, #144]
  48:	ldr	q1, [sp, #96]
  4c:	bl	0 <__multf3>
  50:	stp	x22, x26, [sp, #96]
  54:	stp	x21, x23, [sp, #112]
  58:	ldr	q1, [sp, #96]
  5c:	str	q0, [sp, #128]
  60:	ldr	q0, [sp, #112]
  64:	ldp	x28, x27, [sp, #128]
  68:	bl	0 <__multf3>
  6c:	str	q0, [sp, #128]
  70:	ldp	x1, x0, [sp, #128]
  74:	stp	x22, x26, [sp, #96]
  78:	stp	x20, x25, [sp, #112]
  7c:	ldr	q1, [sp, #96]
  80:	ldr	q0, [sp, #112]
  84:	str	x1, [sp, #144]
  88:	str	x0, [sp, #168]
  8c:	bl	0 <__multf3>
  90:	str	q0, [sp, #128]
  94:	ldp	x1, x0, [sp, #128]
  98:	stp	x21, x23, [sp, #96]
  9c:	stp	x19, x24, [sp, #112]
  a0:	ldr	q1, [sp, #96]
  a4:	ldr	q0, [sp, #112]
  a8:	stp	x1, x0, [sp, #176]
  ac:	bl	0 <__multf3>
  b0:	str	q0, [sp, #128]
  b4:	ldp	x0, x2, [sp, #136]
  b8:	str	x2, [sp, #96]
  bc:	ldr	x2, [sp, #168]
  c0:	str	x2, [sp, #104]
  c4:	stp	x28, x27, [sp, #112]
  c8:	ldr	q1, [sp, #96]
  cc:	ldr	q0, [sp, #112]
  d0:	str	x0, [sp, #240]
  d4:	bl	0 <__subtf3>
  d8:	ldr	x1, [sp, #176]
  dc:	str	x1, [sp, #112]
  e0:	ldr	x2, [sp, #128]
  e4:	str	x2, [sp, #96]
  e8:	ldr	x1, [sp, #240]
  ec:	str	x1, [sp, #104]
  f0:	ldr	x0, [sp, #184]
  f4:	ldr	q1, [sp, #96]
  f8:	str	x0, [sp, #120]
  fc:	str	q0, [sp, #96]
 100:	ldr	q0, [sp, #112]
 104:	bl	0 <__addtf3>
 108:	mov	v1.16b, v0.16b
 10c:	str	q0, [sp, #192]
 110:	bl	0 <__unordtf2>
 114:	cmp	w0, #0x0
 118:	ldr	q2, [sp, #96]
 11c:	cset	w1, ne  // ne = any
 120:	str	w1, [sp, #96]
 124:	mov	v1.16b, v2.16b
 128:	mov	v0.16b, v2.16b
 12c:	str	q2, [sp, #112]
 130:	bl	0 <__unordtf2>
 134:	ldr	w1, [sp, #96]
 138:	cmp	w0, #0x0
 13c:	cset	w0, ne  // ne = any
 140:	ldr	q2, [sp, #112]
 144:	tst	w0, w1
 148:	ldr	q4, [sp, #192]
 14c:	b.ne	174 <__multc3+0x174>  // b.any
 150:	mov	v0.16b, v2.16b
 154:	mov	v1.16b, v4.16b
 158:	ldp	x19, x20, [sp, #16]
 15c:	ldp	x21, x22, [sp, #32]
 160:	ldp	x23, x24, [sp, #48]
 164:	ldp	x25, x26, [sp, #64]
 168:	ldp	x27, x28, [sp, #80]
 16c:	ldp	x29, x30, [sp], #256
 170:	ret
 174:	adrp	x1, 0 <__multc3>
 178:	add	x1, x1, #0x0
 17c:	and	x0, x25, #0x7fffffffffffffff
 180:	stp	x20, x0, [sp, #96]
 184:	ldr	q1, [x1]
 188:	mov	w1, #0x1                   	// #1
 18c:	ldr	q0, [sp, #96]
 190:	str	w1, [sp, #96]
 194:	str	x0, [sp, #112]
 198:	str	q2, [sp, #192]
 19c:	str	q4, [sp, #208]
 1a0:	bl	0 <__unordtf2>
 1a4:	ldr	w1, [sp, #96]
 1a8:	ldr	q2, [sp, #192]
 1ac:	ldr	q4, [sp, #208]
 1b0:	cbnz	w0, 1e0 <__multc3+0x1e0>
 1b4:	adrp	x1, 0 <__multc3>
 1b8:	add	x1, x1, #0x0
 1bc:	ldr	x0, [sp, #112]
 1c0:	stp	x20, x0, [sp, #96]
 1c4:	ldr	q1, [x1]
 1c8:	ldr	q0, [sp, #96]
 1cc:	bl	0 <__letf2>
 1d0:	cmp	w0, #0x0
 1d4:	cset	w1, le
 1d8:	ldr	q2, [sp, #192]
 1dc:	ldr	q4, [sp, #208]
 1e0:	ldr	x0, [sp, #112]
 1e4:	stp	x20, x0, [sp, #96]
 1e8:	adrp	x0, 0 <__multc3>
 1ec:	add	x0, x0, #0x0
 1f0:	eor	w1, w1, #0x1
 1f4:	and	w1, w1, #0xff
 1f8:	str	w1, [sp, #224]
 1fc:	and	x1, x23, #0x7fffffffffffffff
 200:	ldr	q0, [sp, #96]
 204:	ldr	q1, [x0]
 208:	str	x1, [sp, #96]
 20c:	str	q2, [sp, #192]
 210:	str	q4, [sp, #208]
 214:	bl	0 <__unordtf2>
 218:	ldr	q2, [sp, #192]
 21c:	ldr	q4, [sp, #208]
 220:	cbnz	w0, 250 <__multc3+0x250>
 224:	adrp	x1, 0 <__multc3>
 228:	add	x1, x1, #0x0
 22c:	ldr	x0, [sp, #112]
 230:	stp	x20, x0, [sp, #96]
 234:	ldr	q1, [x1]
 238:	ldr	q0, [sp, #96]
 23c:	bl	0 <__letf2>
 240:	cmp	w0, #0x0
 244:	ldr	q2, [sp, #192]
 248:	ldr	q4, [sp, #208]
 24c:	b.gt	2ac <__multc3+0x2ac>
 250:	adrp	x0, 0 <__multc3>
 254:	add	x0, x0, #0x0
 258:	str	q2, [sp, #112]
 25c:	str	q4, [sp, #192]
 260:	ldr	q1, [x0]
 264:	and	x0, x23, #0x7fffffffffffffff
 268:	stp	x21, x0, [sp, #96]
 26c:	ldr	q0, [sp, #96]
 270:	bl	0 <__unordtf2>
 274:	ldr	q2, [sp, #112]
 278:	ldr	q4, [sp, #192]
 27c:	cbnz	w0, 764 <__multc3+0x764>
 280:	adrp	x1, 0 <__multc3>
 284:	add	x1, x1, #0x0
 288:	and	x0, x23, #0x7fffffffffffffff
 28c:	stp	x21, x0, [sp, #96]
 290:	ldr	q1, [x1]
 294:	ldr	q0, [sp, #96]
 298:	bl	0 <__letf2>
 29c:	cmp	w0, #0x0
 2a0:	ldr	q2, [sp, #112]
 2a4:	ldr	q4, [sp, #192]
 2a8:	b.le	764 <__multc3+0x764>
 2ac:	ldr	w0, [sp, #224]
 2b0:	str	q2, [sp, #192]
 2b4:	str	q4, [sp, #208]
 2b8:	bl	0 <__floatsitf>
 2bc:	str	q0, [sp, #112]
 2c0:	ldp	x20, x0, [sp, #112]
 2c4:	and	x1, x23, #0x7fffffffffffffff
 2c8:	stp	x21, x1, [sp, #96]
 2cc:	mov	w1, #0x1                   	// #1
 2d0:	ldr	q0, [sp, #96]
 2d4:	str	w1, [sp, #96]
 2d8:	bfxil	x25, x0, #0, #63
 2dc:	adrp	x0, 0 <__multc3>
 2e0:	add	x0, x0, #0x0
 2e4:	ldr	q1, [x0]
 2e8:	bl	0 <__unordtf2>
 2ec:	ldr	w1, [sp, #96]
 2f0:	ldr	q2, [sp, #192]
 2f4:	ldr	q4, [sp, #208]
 2f8:	cbnz	w0, 330 <__multc3+0x330>
 2fc:	adrp	x0, 0 <__multc3>
 300:	add	x0, x0, #0x0
 304:	and	x1, x23, #0x7fffffffffffffff
 308:	stp	x21, x1, [sp, #96]
 30c:	ldr	q1, [x0]
 310:	ldr	q0, [sp, #96]
 314:	str	q2, [sp, #112]
 318:	str	q4, [sp, #192]
 31c:	bl	0 <__letf2>
 320:	cmp	w0, #0x0
 324:	cset	w1, le
 328:	ldr	q2, [sp, #112]
 32c:	ldr	q4, [sp, #192]
 330:	eor	w0, w1, #0x1
 334:	str	q2, [sp, #208]
 338:	and	w0, w0, #0x1
 33c:	str	q4, [sp, #224]
 340:	bl	0 <__floatsitf>
 344:	str	q0, [sp, #192]
 348:	ldp	x21, x0, [sp, #192]
 34c:	stp	x19, x24, [sp, #96]
 350:	stp	x19, x24, [sp, #112]
 354:	ldr	q1, [sp, #96]
 358:	ldr	q0, [sp, #112]
 35c:	bfxil	x23, x0, #0, #63
 360:	bl	0 <__unordtf2>
 364:	ldr	q2, [sp, #208]
 368:	ldr	q4, [sp, #224]
 36c:	cbnz	w0, 83c <__multc3+0x83c>
 370:	stp	x22, x26, [sp, #96]
 374:	stp	x22, x26, [sp, #112]
 378:	ldr	q1, [sp, #96]
 37c:	ldr	q0, [sp, #112]
 380:	str	q2, [sp, #192]
 384:	str	q4, [sp, #208]
 388:	bl	0 <__unordtf2>
 38c:	ldr	q2, [sp, #192]
 390:	ldr	q4, [sp, #208]
 394:	cbnz	w0, 854 <__multc3+0x854>
 398:	mov	w0, #0x1                   	// #1
 39c:	str	w0, [sp, #252]
 3a0:	adrp	x1, 0 <__multc3>
 3a4:	add	x1, x1, #0x0
 3a8:	and	x0, x24, #0x7fffffffffffffff
 3ac:	stp	x19, x0, [sp, #96]
 3b0:	ldr	q1, [x1]
 3b4:	mov	w1, #0x1                   	// #1
 3b8:	ldr	q0, [sp, #96]
 3bc:	str	w1, [sp, #96]
 3c0:	str	x0, [sp, #112]
 3c4:	str	q2, [sp, #192]
 3c8:	str	q4, [sp, #208]
 3cc:	bl	0 <__unordtf2>
 3d0:	ldr	w1, [sp, #96]
 3d4:	ldr	q2, [sp, #192]
 3d8:	ldr	q4, [sp, #208]
 3dc:	cbnz	w0, 40c <__multc3+0x40c>
 3e0:	adrp	x1, 0 <__multc3>
 3e4:	add	x1, x1, #0x0
 3e8:	ldr	x0, [sp, #112]
 3ec:	stp	x19, x0, [sp, #96]
 3f0:	ldr	q1, [x1]
 3f4:	ldr	q0, [sp, #96]
 3f8:	bl	0 <__letf2>
 3fc:	cmp	w0, #0x0
 400:	cset	w1, le
 404:	ldr	q2, [sp, #192]
 408:	ldr	q4, [sp, #208]
 40c:	ldr	x0, [sp, #112]
 410:	stp	x19, x0, [sp, #96]
 414:	adrp	x0, 0 <__multc3>
 418:	add	x0, x0, #0x0
 41c:	eor	w1, w1, #0x1
 420:	and	w1, w1, #0xff
 424:	str	w1, [sp, #224]
 428:	and	x1, x26, #0x7fffffffffffffff
 42c:	ldr	q0, [sp, #96]
 430:	ldr	q1, [x0]
 434:	str	x1, [sp, #96]
 438:	str	q2, [sp, #192]
 43c:	str	q4, [sp, #208]
 440:	bl	0 <__unordtf2>
 444:	ldr	q2, [sp, #192]
 448:	ldr	q4, [sp, #208]
 44c:	cbnz	w0, 47c <__multc3+0x47c>
 450:	adrp	x1, 0 <__multc3>
 454:	add	x1, x1, #0x0
 458:	ldr	x0, [sp, #112]
 45c:	stp	x19, x0, [sp, #96]
 460:	ldr	q1, [x1]
 464:	ldr	q0, [sp, #96]
 468:	bl	0 <__letf2>
 46c:	cmp	w0, #0x0
 470:	ldr	q2, [sp, #192]
 474:	ldr	q4, [sp, #208]
 478:	b.gt	76c <__multc3+0x76c>
 47c:	adrp	x0, 0 <__multc3>
 480:	add	x0, x0, #0x0
 484:	str	q2, [sp, #112]
 488:	str	q4, [sp, #192]
 48c:	ldr	q1, [x0]
 490:	and	x0, x26, #0x7fffffffffffffff
 494:	stp	x22, x0, [sp, #96]
 498:	ldr	q0, [sp, #96]
 49c:	bl	0 <__unordtf2>
 4a0:	ldr	q2, [sp, #112]
 4a4:	ldr	q4, [sp, #192]
 4a8:	cbnz	w0, 4d8 <__multc3+0x4d8>
 4ac:	adrp	x1, 0 <__multc3>
 4b0:	add	x1, x1, #0x0
 4b4:	and	x0, x26, #0x7fffffffffffffff
 4b8:	stp	x22, x0, [sp, #96]
 4bc:	ldr	q1, [x1]
 4c0:	ldr	q0, [sp, #96]
 4c4:	bl	0 <__letf2>
 4c8:	cmp	w0, #0x0
 4cc:	ldr	q2, [sp, #112]
 4d0:	ldr	q4, [sp, #192]
 4d4:	b.gt	76c <__multc3+0x76c>
 4d8:	ldr	w0, [sp, #252]
 4dc:	str	q2, [sp, #112]
 4e0:	str	q4, [sp, #192]
 4e4:	cbnz	w0, 6bc <__multc3+0x6bc>
 4e8:	adrp	x0, 0 <__multc3>
 4ec:	add	x0, x0, #0x0
 4f0:	and	x27, x27, #0x7fffffffffffffff
 4f4:	stp	x28, x27, [sp, #96]
 4f8:	ldr	q1, [x0]
 4fc:	ldr	q0, [sp, #96]
 500:	bl	0 <__unordtf2>
 504:	ldr	q2, [sp, #112]
 508:	ldr	q4, [sp, #192]
 50c:	cbnz	w0, 538 <__multc3+0x538>
 510:	adrp	x0, 0 <__multc3>
 514:	add	x0, x0, #0x0
 518:	stp	x28, x27, [sp, #96]
 51c:	ldr	q1, [x0]
 520:	ldr	q0, [sp, #96]
 524:	bl	0 <__letf2>
 528:	cmp	w0, #0x0
 52c:	ldr	q2, [sp, #112]
 530:	ldr	q4, [sp, #192]
 534:	b.gt	65c <__multc3+0x65c>
 538:	ldr	x0, [sp, #168]
 53c:	str	q2, [sp, #112]
 540:	ldr	x28, [sp, #144]
 544:	and	x27, x0, #0x7fffffffffffffff
 548:	adrp	x0, 0 <__multc3>
 54c:	add	x0, x0, #0x0
 550:	stp	x28, x27, [sp, #96]
 554:	ldr	q1, [x0]
 558:	ldr	q0, [sp, #96]
 55c:	str	q4, [sp, #192]
 560:	bl	0 <__unordtf2>
 564:	ldr	q2, [sp, #112]
 568:	ldr	q4, [sp, #192]
 56c:	cbnz	w0, 59c <__multc3+0x59c>
 570:	adrp	x0, 0 <__multc3>
 574:	add	x0, x0, #0x0
 578:	stp	x28, x27, [sp, #96]
 57c:	ldr	q1, [x0]
 580:	ldr	q0, [sp, #96]
 584:	str	q4, [sp, #144]
 588:	bl	0 <__letf2>
 58c:	cmp	w0, #0x0
 590:	ldr	q2, [sp, #112]
 594:	ldr	q4, [sp, #144]
 598:	b.gt	65c <__multc3+0x65c>
 59c:	ldp	x28, x0, [sp, #176]
 5a0:	str	q2, [sp, #112]
 5a4:	str	q4, [sp, #144]
 5a8:	and	x27, x0, #0x7fffffffffffffff
 5ac:	adrp	x0, 0 <__multc3>
 5b0:	add	x0, x0, #0x0
 5b4:	stp	x28, x27, [sp, #96]
 5b8:	ldr	q0, [sp, #96]
 5bc:	ldr	q1, [x0]
 5c0:	bl	0 <__unordtf2>
 5c4:	ldr	q2, [sp, #112]
 5c8:	ldr	q4, [sp, #144]
 5cc:	cbnz	w0, 5f8 <__multc3+0x5f8>
 5d0:	adrp	x0, 0 <__multc3>
 5d4:	add	x0, x0, #0x0
 5d8:	stp	x28, x27, [sp, #96]
 5dc:	ldr	q1, [x0]
 5e0:	ldr	q0, [sp, #96]
 5e4:	bl	0 <__letf2>
 5e8:	cmp	w0, #0x0
 5ec:	ldr	q2, [sp, #112]
 5f0:	ldr	q4, [sp, #144]
 5f4:	b.gt	65c <__multc3+0x65c>
 5f8:	ldr	x0, [sp, #240]
 5fc:	str	q2, [sp, #112]
 600:	ldr	x28, [sp, #128]
 604:	and	x27, x0, #0x7fffffffffffffff
 608:	adrp	x0, 0 <__multc3>
 60c:	add	x0, x0, #0x0
 610:	stp	x28, x27, [sp, #96]
 614:	ldr	q1, [x0]
 618:	ldr	q0, [sp, #96]
 61c:	str	q4, [sp, #144]
 620:	bl	0 <__unordtf2>
 624:	ldr	q2, [sp, #112]
 628:	ldr	q4, [sp, #144]
 62c:	cbnz	w0, 150 <__multc3+0x150>
 630:	adrp	x0, 0 <__multc3>
 634:	add	x0, x0, #0x0
 638:	stp	x28, x27, [sp, #96]
 63c:	ldr	q1, [x0]
 640:	ldr	q0, [sp, #96]
 644:	str	q4, [sp, #128]
 648:	bl	0 <__letf2>
 64c:	cmp	w0, #0x0
 650:	ldr	q2, [sp, #112]
 654:	ldr	q4, [sp, #128]
 658:	b.le	150 <__multc3+0x150>
 65c:	stp	x20, x25, [sp, #96]
 660:	stp	x20, x25, [sp, #112]
 664:	ldr	q1, [sp, #96]
 668:	ldr	q0, [sp, #112]
 66c:	bl	0 <__unordtf2>
 670:	cbnz	w0, 8bc <__multc3+0x8bc>
 674:	stp	x21, x23, [sp, #96]
 678:	stp	x21, x23, [sp, #112]
 67c:	ldr	q1, [sp, #96]
 680:	ldr	q0, [sp, #112]
 684:	bl	0 <__unordtf2>
 688:	cbnz	w0, 8a4 <__multc3+0x8a4>
 68c:	stp	x19, x24, [sp, #96]
 690:	stp	x19, x24, [sp, #112]
 694:	ldr	q1, [sp, #96]
 698:	ldr	q0, [sp, #112]
 69c:	bl	0 <__unordtf2>
 6a0:	cbnz	w0, 88c <__multc3+0x88c>
 6a4:	stp	x22, x26, [sp, #96]
 6a8:	stp	x22, x26, [sp, #112]
 6ac:	ldr	q1, [sp, #96]
 6b0:	ldr	q0, [sp, #112]
 6b4:	bl	0 <__unordtf2>
 6b8:	cbnz	w0, 874 <__multc3+0x874>
 6bc:	stp	x19, x24, [sp, #96]
 6c0:	stp	x20, x25, [sp, #112]
 6c4:	ldr	q1, [sp, #96]
 6c8:	ldr	q0, [sp, #112]
 6cc:	bl	0 <__multf3>
 6d0:	stp	x22, x26, [sp, #96]
 6d4:	stp	x21, x23, [sp, #112]
 6d8:	ldr	q1, [sp, #96]
 6dc:	str	q0, [sp, #96]
 6e0:	ldr	q0, [sp, #112]
 6e4:	bl	0 <__multf3>
 6e8:	mov	v1.16b, v0.16b
 6ec:	ldr	q2, [sp, #96]
 6f0:	mov	v0.16b, v2.16b
 6f4:	bl	0 <__subtf3>
 6f8:	adrp	x0, 0 <__multc3>
 6fc:	add	x0, x0, #0x0
 700:	ldr	q1, [x0]
 704:	bl	0 <__multf3>
 708:	stp	x22, x26, [sp, #96]
 70c:	stp	x20, x25, [sp, #112]
 710:	ldr	q1, [sp, #96]
 714:	str	q0, [sp, #128]
 718:	ldr	q0, [sp, #112]
 71c:	bl	0 <__multf3>
 720:	stp	x19, x24, [sp, #96]
 724:	stp	x21, x23, [sp, #112]
 728:	ldr	q1, [sp, #96]
 72c:	str	q0, [sp, #96]
 730:	ldr	q0, [sp, #112]
 734:	bl	0 <__multf3>
 738:	mov	v1.16b, v0.16b
 73c:	ldr	q4, [sp, #96]
 740:	mov	v0.16b, v4.16b
 744:	bl	0 <__addtf3>
 748:	adrp	x0, 0 <__multc3>
 74c:	add	x0, x0, #0x0
 750:	ldr	q1, [x0]
 754:	bl	0 <__multf3>
 758:	mov	v4.16b, v0.16b
 75c:	ldr	q2, [sp, #128]
 760:	b	150 <__multc3+0x150>
 764:	str	wzr, [sp, #252]
 768:	b	3a0 <__multc3+0x3a0>
 76c:	ldr	w0, [sp, #224]
 770:	and	x28, x26, #0x7fffffffffffffff
 774:	mov	w27, #0x1                   	// #1
 778:	bl	0 <__floatsitf>
 77c:	str	q0, [sp, #112]
 780:	ldp	x19, x0, [sp, #112]
 784:	stp	x22, x28, [sp, #96]
 788:	ldr	q0, [sp, #96]
 78c:	bfxil	x24, x0, #0, #63
 790:	adrp	x0, 0 <__multc3>
 794:	add	x0, x0, #0x0
 798:	ldr	q1, [x0]
 79c:	bl	0 <__unordtf2>
 7a0:	cbnz	w0, 7c4 <__multc3+0x7c4>
 7a4:	adrp	x0, 0 <__multc3>
 7a8:	add	x0, x0, #0x0
 7ac:	stp	x22, x28, [sp, #96]
 7b0:	ldr	q1, [x0]
 7b4:	ldr	q0, [sp, #96]
 7b8:	bl	0 <__letf2>
 7bc:	cmp	w0, #0x0
 7c0:	cset	w27, le
 7c4:	eor	w0, w27, #0x1
 7c8:	and	w0, w0, #0x1
 7cc:	bl	0 <__floatsitf>
 7d0:	str	q0, [sp, #128]
 7d4:	ldp	x22, x0, [sp, #128]
 7d8:	stp	x20, x25, [sp, #96]
 7dc:	stp	x20, x25, [sp, #112]
 7e0:	ldr	q1, [sp, #96]
 7e4:	ldr	q0, [sp, #112]
 7e8:	bfxil	x26, x0, #0, #63
 7ec:	bl	0 <__unordtf2>
 7f0:	cbnz	w0, 824 <__multc3+0x824>
 7f4:	stp	x21, x23, [sp, #96]
 7f8:	stp	x21, x23, [sp, #112]
 7fc:	ldr	q1, [sp, #96]
 800:	ldr	q0, [sp, #112]
 804:	bl	0 <__unordtf2>
 808:	cbz	w0, 6bc <__multc3+0x6bc>
 80c:	mov	x21, #0x0                   	// #0
 810:	mov	x0, #0x0                   	// #0
 814:	tbz	x23, #63, 81c <__multc3+0x81c>
 818:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 81c:	mov	x23, x0
 820:	b	6bc <__multc3+0x6bc>
 824:	mov	x20, #0x0                   	// #0
 828:	mov	x0, #0x0                   	// #0
 82c:	tbz	x25, #63, 834 <__multc3+0x834>
 830:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 834:	mov	x25, x0
 838:	b	7f4 <__multc3+0x7f4>
 83c:	mov	x19, #0x0                   	// #0
 840:	mov	x0, #0x0                   	// #0
 844:	tbz	x24, #63, 84c <__multc3+0x84c>
 848:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 84c:	mov	x24, x0
 850:	b	370 <__multc3+0x370>
 854:	mov	x22, #0x0                   	// #0
 858:	mov	x0, #0x0                   	// #0
 85c:	tbz	x26, #63, 864 <__multc3+0x864>
 860:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 864:	mov	x26, x0
 868:	mov	w0, #0x1                   	// #1
 86c:	str	w0, [sp, #252]
 870:	b	3a0 <__multc3+0x3a0>
 874:	mov	x22, #0x0                   	// #0
 878:	mov	x0, #0x0                   	// #0
 87c:	tbz	x26, #63, 884 <__multc3+0x884>
 880:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 884:	mov	x26, x0
 888:	b	6bc <__multc3+0x6bc>
 88c:	mov	x19, #0x0                   	// #0
 890:	mov	x0, #0x0                   	// #0
 894:	tbz	x24, #63, 89c <__multc3+0x89c>
 898:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 89c:	mov	x24, x0
 8a0:	b	6a4 <__multc3+0x6a4>
 8a4:	mov	x21, #0x0                   	// #0
 8a8:	mov	x0, #0x0                   	// #0
 8ac:	tbz	x23, #63, 8b4 <__multc3+0x8b4>
 8b0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 8b4:	mov	x23, x0
 8b8:	b	68c <__multc3+0x68c>
 8bc:	mov	x20, #0x0                   	// #0
 8c0:	mov	x0, #0x0                   	// #0
 8c4:	tbz	x25, #63, 8cc <__multc3+0x8cc>
 8c8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 8cc:	mov	x25, x0
 8d0:	b	674 <__multc3+0x674>

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x4, #0xbc01000000000000    	// #-4899634919602388992
   8:	mov	x2, #0xc3ff000000000000    	// #-4323737117252386816
   c:	str	q0, [sp]
  10:	ldp	x0, x3, [sp]
  14:	and	x1, x3, #0x7fffffffffffffff
  18:	add	x4, x1, x4
  1c:	add	x2, x1, x2
  20:	cmp	x4, x2
  24:	b.hi	54 <__trunctfdf2+0x54>  // b.pmore
  28:	mov	x2, #0x7fff000000000000    	// #9223090561878065152
  2c:	cmp	x1, x2
  30:	b.ls	98 <__trunctfdf2+0x98>  // b.plast
  34:	extr	x0, x1, x0, #60
  38:	and	x0, x0, #0x7ffffffffffff
  3c:	orr	x2, x0, #0x7ff8000000000000
  40:	and	x3, x3, #0x8000000000000000
  44:	orr	x0, x2, x3
  48:	add	sp, sp, #0x10
  4c:	fmov	d0, x0
  50:	ret
  54:	and	x2, x0, #0xfffffffffffffff
  58:	extr	x0, x1, x0, #60
  5c:	mov	x1, #0x800000000000000     	// #576460752303423488
  60:	cmp	x2, x1
  64:	mov	x2, #0x1                   	// #1
  68:	movk	x2, #0x4000, lsl #48
  6c:	add	x2, x0, x2
  70:	b.hi	40 <__trunctfdf2+0x40>  // b.pmore
  74:	mov	x1, #0x4000000000000000    	// #4611686018427387904
  78:	and	x2, x2, #0xfffffffffffffffe
  7c:	add	x0, x0, x1
  80:	and	x3, x3, #0x8000000000000000
  84:	csel	x2, x0, x2, ne  // ne = any
  88:	orr	x0, x2, x3
  8c:	add	sp, sp, #0x10
  90:	fmov	d0, x0
  94:	ret
  98:	b.eq	16c <__trunctfdf2+0x16c>  // b.none
  9c:	mov	x4, #0x43feffffffffffff    	// #4899634919602388991
  a0:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  a4:	cmp	x1, x4
  a8:	b.hi	40 <__trunctfdf2+0x40>  // b.pmore
  ac:	lsr	x1, x1, #48
  b0:	mov	w2, #0x3c01                	// #15361
  b4:	sub	w4, w2, w1
  b8:	mov	x2, #0x0                   	// #0
  bc:	cmp	w4, #0x70
  c0:	b.gt	40 <__trunctfdf2+0x40>
  c4:	and	x2, x3, #0xffffffffffff
  c8:	mov	w5, #0xffffc47f            	// #-15233
  cc:	add	w5, w1, w5
  d0:	orr	x2, x2, #0x1000000000000
  d4:	lsr	x8, x0, #1
  d8:	mov	w7, #0x3f                  	// #63
  dc:	sub	w9, w7, w5
  e0:	mov	w6, #0xffffc43f            	// #-15297
  e4:	add	w1, w1, w6
  e8:	lsl	x6, x2, x5
  ec:	cmp	w1, #0x0
  f0:	lsr	x8, x8, x9
  f4:	orr	x6, x8, x6
  f8:	lsl	x8, x0, x1
  fc:	lsl	x1, x0, x5
 100:	csel	x6, x8, x6, ge  // ge = tcont
 104:	csel	x1, xzr, x1, ge  // ge = tcont
 108:	sub	w7, w7, w4
 10c:	orr	x1, x1, x6
 110:	lsl	x6, x2, #1
 114:	sub	w5, w4, #0x40
 118:	cmp	x1, #0x0
 11c:	cset	x1, ne  // ne = any
 120:	lsr	x0, x0, x4
 124:	cmp	w5, #0x0
 128:	lsl	x7, x6, x7
 12c:	orr	x0, x7, x0
 130:	lsr	x6, x2, x5
 134:	lsr	x2, x2, x4
 138:	csel	x0, x6, x0, ge  // ge = tcont
 13c:	csel	x2, xzr, x2, ge  // ge = tcont
 140:	orr	x1, x1, x0
 144:	and	x1, x1, #0xfffffffffffffff
 148:	mov	x4, #0x800000000000000     	// #576460752303423488
 14c:	cmp	x1, x4
 150:	extr	x2, x2, x0, #60
 154:	add	x0, x2, #0x1
 158:	add	x5, x2, #0x1
 15c:	and	x0, x0, #0xfffffffffffffffe
 160:	csel	x0, x0, x2, eq  // eq = none
 164:	csel	x2, x5, x0, hi  // hi = pmore
 168:	b	40 <__trunctfdf2+0x40>
 16c:	cbz	x0, 9c <__trunctfdf2+0x9c>
 170:	b	34 <__trunctfdf2+0x34>

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x5, #0xbf81000000000000    	// #-4647433340469641216
   8:	mov	x3, #0xc07f000000000000    	// #-4575938696385134592
   c:	str	q0, [sp]
  10:	ldp	x2, x0, [sp]
  14:	and	x1, x0, #0x7fffffffffffffff
  18:	mov	x4, x2
  1c:	add	x5, x1, x5
  20:	add	x2, x1, x3
  24:	cmp	x5, x2
  28:	b.hi	58 <__trunctfsf2+0x58>  // b.pmore
  2c:	mov	x2, #0x7fff000000000000    	// #9223090561878065152
  30:	cmp	x1, x2
  34:	b.ls	b8 <__trunctfsf2+0xb8>  // b.plast
  38:	ubfx	x1, x1, #25, #22
  3c:	lsr	x0, x0, #32
  40:	orr	w3, w1, #0x7fc00000
  44:	and	x0, x0, #0x80000000
  48:	orr	w0, w3, w0
  4c:	fmov	s0, w0
  50:	add	sp, sp, #0x10
  54:	ret
  58:	and	x2, x0, #0x1ffffff
  5c:	mov	x3, #0x1000000             	// #16777216
  60:	lsr	x1, x1, #25
  64:	cmp	x2, x3
  68:	b.ls	90 <__trunctfsf2+0x90>  // b.plast
  6c:	mov	w3, #0x1                   	// #1
  70:	movk	w3, #0x4000, lsl #16
  74:	add	w3, w3, w1
  78:	lsr	x0, x0, #32
  7c:	and	x0, x0, #0x80000000
  80:	add	sp, sp, #0x10
  84:	orr	w0, w3, w0
  88:	fmov	s0, w0
  8c:	ret
  90:	b.eq	1a4 <__trunctfsf2+0x1a4>  // b.none
  94:	cbz	x4, 1a8 <__trunctfsf2+0x1a8>
  98:	lsr	x0, x0, #32
  9c:	mov	w3, #0x40000000            	// #1073741824
  a0:	and	x0, x0, #0x80000000
  a4:	add	w3, w3, w1
  a8:	orr	w0, w3, w0
  ac:	fmov	s0, w0
  b0:	add	sp, sp, #0x10
  b4:	ret
  b8:	b.eq	19c <__trunctfsf2+0x19c>  // b.none
  bc:	mov	x2, #0x407effffffffffff    	// #4647433340469641215
  c0:	mov	w3, #0x7f800000            	// #2139095040
  c4:	cmp	x1, x2
  c8:	b.hi	78 <__trunctfsf2+0x78>  // b.pmore
  cc:	lsr	x2, x1, #48
  d0:	mov	w1, #0x3f81                	// #16257
  d4:	sub	w5, w1, w2
  d8:	mov	w3, #0x0                   	// #0
  dc:	cmp	w5, #0x70
  e0:	b.gt	78 <__trunctfsf2+0x78>
  e4:	and	x1, x0, #0xffffffffffff
  e8:	mov	w3, #0xffffc0ff            	// #-16129
  ec:	add	w6, w2, w3
  f0:	lsr	x8, x4, #1
  f4:	orr	x3, x1, #0x1000000000000
  f8:	mov	w7, #0x3f                  	// #63
  fc:	sub	w9, w7, w6
 100:	mov	w10, #0xffffc0bf            	// #-16193
 104:	add	w1, w2, w10
 108:	lsl	x2, x3, x6
 10c:	cmp	w1, #0x0
 110:	lsr	x8, x8, x9
 114:	orr	x2, x8, x2
 118:	lsl	x8, x4, x1
 11c:	lsl	x1, x4, x6
 120:	csel	x2, x8, x2, ge  // ge = tcont
 124:	csel	x1, xzr, x1, ge  // ge = tcont
 128:	lsl	x6, x3, #1
 12c:	orr	x1, x1, x2
 130:	sub	w7, w7, w5
 134:	cmp	x1, #0x0
 138:	sub	w1, w5, #0x40
 13c:	lsr	x2, x4, x5
 140:	cset	x4, ne  // ne = any
 144:	cmp	w1, #0x0
 148:	lsl	x7, x6, x7
 14c:	lsr	x6, x3, x1
 150:	orr	x2, x7, x2
 154:	lsr	x1, x3, x5
 158:	csel	x1, xzr, x1, ge  // ge = tcont
 15c:	csel	x2, x6, x2, ge  // ge = tcont
 160:	mov	x3, #0x1000000             	// #16777216
 164:	orr	x4, x4, x2
 168:	and	x2, x1, #0x1ffffff
 16c:	lsr	x1, x1, #25
 170:	cmp	x2, x3
 174:	mov	w3, w1
 178:	b.hi	1c8 <__trunctfsf2+0x1c8>  // b.pmore
 17c:	b.ne	1d0 <__trunctfsf2+0x1d0>  // b.any
 180:	cbnz	x4, 1c8 <__trunctfsf2+0x1c8>
 184:	add	w1, w1, #0x1
 188:	mov	x4, #0x1000000             	// #16777216
 18c:	and	w1, w1, #0xfffffffe
 190:	cmp	x2, x4
 194:	csel	w3, w1, w3, eq  // eq = none
 198:	b	78 <__trunctfsf2+0x78>
 19c:	cbz	x4, bc <__trunctfsf2+0xbc>
 1a0:	b	38 <__trunctfsf2+0x38>
 1a4:	cbnz	x4, 6c <__trunctfsf2+0x6c>
 1a8:	mov	x3, #0x1000000             	// #16777216
 1ac:	cmp	x2, x3
 1b0:	b.ne	98 <__trunctfsf2+0x98>  // b.any
 1b4:	mov	w3, #0x1                   	// #1
 1b8:	movk	w3, #0x4000, lsl #16
 1bc:	add	w1, w3, w1
 1c0:	and	w3, w1, #0xfffffffe
 1c4:	b	78 <__trunctfsf2+0x78>
 1c8:	add	w3, w1, #0x1
 1cc:	b	78 <__trunctfsf2+0x78>
 1d0:	cbnz	x4, 78 <__trunctfsf2+0x78>
 1d4:	b	184 <__trunctfsf2+0x184>

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.eq	1c <__absvdi2+0x1c>  // b.none
   c:	asr	x1, x0, #63
  10:	eor	x0, x0, x1
  14:	sub	x0, x0, x1
  18:	ret
  1c:	stp	x29, x30, [sp, #-16]!
  20:	adrp	x2, 0 <__absvdi2>
  24:	adrp	x0, 0 <__absvdi2>
  28:	mov	x29, sp
  2c:	add	x2, x2, #0x0
  30:	add	x0, x0, #0x0
  34:	mov	w1, #0x16                  	// #22
  38:	bl	0 <__compilerrt_abort_impl>

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.eq	1c <__absvsi2+0x1c>  // b.none
   c:	asr	w1, w0, #31
  10:	eor	w0, w0, w1
  14:	sub	w0, w0, w1
  18:	ret
  1c:	stp	x29, x30, [sp, #-16]!
  20:	adrp	x2, 0 <__absvsi2>
  24:	adrp	x0, 0 <__absvsi2>
  28:	mov	x29, sp
  2c:	add	x2, x2, #0x0
  30:	add	x0, x0, #0x0
  34:	mov	w1, #0x16                  	// #22
  38:	bl	0 <__compilerrt_abort_impl>

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	cbz	x0, 1c <__absvti2+0x1c>
   4:	asr	x2, x1, #63
   8:	eor	x0, x0, x2
   c:	eor	x1, x1, x2
  10:	subs	x0, x0, x2
  14:	sbc	x1, x1, x2
  18:	ret
  1c:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  20:	cmp	x1, x2
  24:	b.ne	4 <__absvti2+0x4>  // b.any
  28:	stp	x29, x30, [sp, #-16]!
  2c:	adrp	x2, 0 <__absvti2>
  30:	adrp	x0, 0 <__absvti2>
  34:	mov	x29, sp
  38:	add	x2, x2, #0x0
  3c:	add	x0, x0, #0x0
  40:	mov	w1, #0x18                  	// #24
  44:	bl	0 <__compilerrt_abort_impl>

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	fmov	x1, d0
   4:	fmov	x0, d1
   8:	mov	x4, #0x7fefffffffffffff    	// #9218868437227405311
   c:	fmov	d2, d0
  10:	and	x2, x1, #0x7fffffffffffffff
  14:	and	x3, x0, #0x7fffffffffffffff
  18:	sub	x5, x2, #0x1
  1c:	cmp	x5, x4
  20:	b.cs	48 <__adddf3+0x48>  // b.hs, b.nlast
  24:	sub	x5, x3, #0x1
  28:	cmp	x5, x4
  2c:	b.cc	68 <__adddf3+0x68>  // b.lo, b.ul, b.last
  30:	mov	x4, #0x7ff0000000000000    	// #9218868437227405312
  34:	cmp	x3, x4
  38:	b.hi	1c8 <__adddf3+0x1c8>  // b.pmore
  3c:	fmov	d0, d1
  40:	b.ne	60 <__adddf3+0x60>  // b.any
  44:	ret
  48:	mov	x4, #0x7ff0000000000000    	// #9218868437227405312
  4c:	cmp	x2, x4
  50:	b.ls	170 <__adddf3+0x170>  // b.plast
  54:	orr	x0, x1, #0x8000000000000
  58:	fmov	d0, x0
  5c:	ret
  60:	fmov	d0, d2
  64:	cbz	x3, 1e0 <__adddf3+0x1e0>
  68:	stp	x29, x30, [sp, #-48]!
  6c:	cmp	x2, x3
  70:	mov	x29, sp
  74:	stp	x19, x20, [sp, #16]
  78:	str	x21, [sp, #32]
  7c:	csel	x21, x1, x0, cs  // cs = hs, nlast
  80:	csel	x0, x0, x1, cs  // cs = hs, nlast
  84:	and	x1, x21, #0xfffffffffffff
  88:	ubfx	x3, x21, #52, #11
  8c:	and	x2, x0, #0xfffffffffffff
  90:	ubfx	x4, x0, #52, #11
  94:	cbnz	w3, ac <__adddf3+0xac>
  98:	clz	x5, x1
  9c:	mov	w3, #0x1                   	// #1
  a0:	sub	w5, w5, #0xb
  a4:	sub	w3, w3, w5
  a8:	lsl	x1, x1, x5
  ac:	cbnz	w4, c4 <__adddf3+0xc4>
  b0:	clz	x5, x2
  b4:	mov	w4, #0x1                   	// #1
  b8:	sub	w5, w5, #0xb
  bc:	sub	w4, w4, w5
  c0:	lsl	x2, x2, x5
  c4:	lsl	x1, x1, #3
  c8:	lsl	x2, x2, #3
  cc:	orr	x1, x1, #0x80000000000000
  d0:	orr	x2, x2, #0x80000000000000
  d4:	eor	x0, x21, x0
  d8:	subs	w4, w3, w4
  dc:	and	x19, x21, #0x8000000000000000
  e0:	b.eq	104 <__adddf3+0x104>  // b.none
  e4:	cmp	w4, #0x3f
  e8:	b.hi	1e4 <__adddf3+0x1e4>  // b.pmore
  ec:	neg	w5, w4
  f0:	lsr	x4, x2, x4
  f4:	lsl	x2, x2, x5
  f8:	cmp	x2, #0x0
  fc:	cset	x2, ne  // ne = any
 100:	orr	x2, x2, x4
 104:	tbnz	x0, #63, 19c <__adddf3+0x19c>
 108:	add	x0, x1, x2
 10c:	tbz	x0, #56, 11c <__adddf3+0x11c>
 110:	and	x1, x0, #0x1
 114:	add	w3, w3, #0x1
 118:	orr	x0, x1, x0, lsr #1
 11c:	cmp	w3, #0x7fe
 120:	b.gt	1d4 <__adddf3+0x1d4>
 124:	cmp	w3, #0x0
 128:	b.le	1f4 <__adddf3+0x1f4>
 12c:	lsl	x3, x3, #52
 130:	orr	x3, x19, x3
 134:	ubfx	x19, x0, #3, #52
 138:	orr	x19, x3, x19
 13c:	and	w20, w0, #0x7
 140:	bl	0 <__fe_getround>
 144:	cmp	w0, #0x1
 148:	b.eq	270 <__adddf3+0x270>  // b.none
 14c:	cmp	w0, #0x2
 150:	b.eq	258 <__adddf3+0x258>  // b.none
 154:	cbz	w0, 21c <__adddf3+0x21c>
 158:	cbnz	w20, 230 <__adddf3+0x230>
 15c:	fmov	d0, x19
 160:	ldp	x19, x20, [sp, #16]
 164:	ldr	x21, [sp, #32]
 168:	ldp	x29, x30, [sp], #48
 16c:	ret
 170:	cmp	x3, x4
 174:	b.hi	1c8 <__adddf3+0x1c8>  // b.pmore
 178:	cmp	x2, x4
 17c:	b.ne	238 <__adddf3+0x238>  // b.any
 180:	eor	x0, x1, x0
 184:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 188:	cmp	x0, x1
 18c:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 190:	fmov	d1, x0
 194:	fcsel	d0, d0, d1, ne  // ne = any
 198:	ret
 19c:	movi	d0, #0x0
 1a0:	subs	x0, x1, x2
 1a4:	b.eq	160 <__adddf3+0x160>  // b.none
 1a8:	mov	x1, #0x7fffffffffffff      	// #36028797018963967
 1ac:	cmp	x0, x1
 1b0:	b.hi	11c <__adddf3+0x11c>  // b.pmore
 1b4:	clz	x1, x0
 1b8:	sub	w1, w1, #0x8
 1bc:	sub	w3, w3, w1
 1c0:	lsl	x0, x0, x1
 1c4:	b	124 <__adddf3+0x124>
 1c8:	orr	x0, x0, #0x8000000000000
 1cc:	fmov	d0, x0
 1d0:	ret
 1d4:	orr	x0, x19, #0x7ff0000000000000
 1d8:	fmov	d0, x0
 1dc:	b	160 <__adddf3+0x160>
 1e0:	ret
 1e4:	mov	x2, #0x1                   	// #1
 1e8:	tbz	x0, #63, 108 <__adddf3+0x108>
 1ec:	sub	x0, x1, #0x1
 1f0:	b	1a8 <__adddf3+0x1a8>
 1f4:	mov	w1, #0x1                   	// #1
 1f8:	sub	w1, w1, w3
 1fc:	neg	w2, w1
 200:	mov	x3, #0x0                   	// #0
 204:	lsr	x1, x0, x1
 208:	lsl	x0, x0, x2
 20c:	cmp	x0, #0x0
 210:	cset	x0, ne  // ne = any
 214:	orr	x0, x0, x1
 218:	b	130 <__adddf3+0x130>
 21c:	cmp	w20, #0x4
 220:	b.gt	264 <__adddf3+0x264>
 224:	b.ne	158 <__adddf3+0x158>  // b.any
 228:	add	x19, x19, #0x1
 22c:	and	x19, x19, #0xfffffffffffffffe
 230:	bl	0 <__fe_raise_inexact>
 234:	b	15c <__adddf3+0x15c>
 238:	cmp	x3, x4
 23c:	b.eq	284 <__adddf3+0x284>  // b.none
 240:	cbnz	x2, 60 <__adddf3+0x60>
 244:	cmp	x3, #0x0
 248:	and	x0, x1, x0
 24c:	fmov	d0, x0
 250:	fcsel	d0, d1, d0, ne  // ne = any
 254:	ret
 258:	cmp	x21, #0x0
 25c:	ccmp	w20, #0x0, #0x4, ge  // ge = tcont
 260:	b.eq	158 <__adddf3+0x158>  // b.none
 264:	add	x19, x19, #0x1
 268:	bl	0 <__fe_raise_inexact>
 26c:	b	15c <__adddf3+0x15c>
 270:	cmp	x21, #0x0
 274:	ccmp	w20, #0x0, #0x4, lt  // lt = tstop
 278:	b.eq	158 <__adddf3+0x158>  // b.none
 27c:	add	x19, x19, #0x1
 280:	b	268 <__adddf3+0x268>
 284:	fmov	d0, d1
 288:	ret

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	fmov	w1, s0
   4:	fmov	w0, s1
   8:	mov	w4, #0x7f7fffff            	// #2139095039
   c:	fmov	s2, s0
  10:	and	w2, w1, #0x7fffffff
  14:	and	w3, w0, #0x7fffffff
  18:	sub	w5, w2, #0x1
  1c:	cmp	w5, w4
  20:	b.cs	48 <__addsf3+0x48>  // b.hs, b.nlast
  24:	sub	w5, w3, #0x1
  28:	cmp	w5, w4
  2c:	b.cc	68 <__addsf3+0x68>  // b.lo, b.ul, b.last
  30:	mov	w4, #0x7f800000            	// #2139095040
  34:	cmp	w3, w4
  38:	b.hi	1c8 <__addsf3+0x1c8>  // b.pmore
  3c:	fmov	s0, s1
  40:	b.ne	60 <__addsf3+0x60>  // b.any
  44:	ret
  48:	mov	w4, #0x7f800000            	// #2139095040
  4c:	cmp	w2, w4
  50:	b.ls	170 <__addsf3+0x170>  // b.plast
  54:	orr	w0, w1, #0x400000
  58:	fmov	s0, w0
  5c:	ret
  60:	fmov	s0, s2
  64:	cbz	w3, 1e0 <__addsf3+0x1e0>
  68:	stp	x29, x30, [sp, #-48]!
  6c:	cmp	w2, w3
  70:	mov	x29, sp
  74:	stp	x19, x20, [sp, #16]
  78:	str	x21, [sp, #32]
  7c:	csel	w21, w1, w0, cs  // cs = hs, nlast
  80:	csel	w0, w0, w1, cs  // cs = hs, nlast
  84:	and	w1, w21, #0x7fffff
  88:	ubfx	x3, x21, #23, #8
  8c:	and	w2, w0, #0x7fffff
  90:	ubfx	x4, x0, #23, #8
  94:	cbnz	w3, ac <__addsf3+0xac>
  98:	clz	w5, w1
  9c:	mov	w3, #0x1                   	// #1
  a0:	sub	w5, w5, #0x8
  a4:	sub	w3, w3, w5
  a8:	lsl	w1, w1, w5
  ac:	cbnz	w4, c4 <__addsf3+0xc4>
  b0:	clz	w5, w2
  b4:	mov	w4, #0x1                   	// #1
  b8:	sub	w5, w5, #0x8
  bc:	sub	w4, w4, w5
  c0:	lsl	w2, w2, w5
  c4:	lsl	w1, w1, #3
  c8:	lsl	w2, w2, #3
  cc:	orr	w1, w1, #0x4000000
  d0:	orr	w2, w2, #0x4000000
  d4:	eor	w0, w21, w0
  d8:	subs	w4, w3, w4
  dc:	and	w19, w21, #0x80000000
  e0:	b.eq	104 <__addsf3+0x104>  // b.none
  e4:	cmp	w4, #0x1f
  e8:	b.hi	1e4 <__addsf3+0x1e4>  // b.pmore
  ec:	neg	w5, w4
  f0:	lsr	w4, w2, w4
  f4:	lsl	w2, w2, w5
  f8:	cmp	w2, #0x0
  fc:	cset	w2, ne  // ne = any
 100:	orr	w2, w2, w4
 104:	tbnz	w0, #31, 19c <__addsf3+0x19c>
 108:	add	w0, w1, w2
 10c:	tbz	w0, #27, 11c <__addsf3+0x11c>
 110:	and	w1, w0, #0x1
 114:	add	w3, w3, #0x1
 118:	orr	w0, w1, w0, lsr #1
 11c:	cmp	w3, #0xfe
 120:	b.gt	1d4 <__addsf3+0x1d4>
 124:	cmp	w3, #0x0
 128:	b.le	1f4 <__addsf3+0x1f4>
 12c:	lsl	w3, w3, #23
 130:	orr	w3, w19, w3
 134:	ubfx	x19, x0, #3, #23
 138:	orr	w19, w3, w19
 13c:	and	w20, w0, #0x7
 140:	bl	0 <__fe_getround>
 144:	cmp	w0, #0x1
 148:	b.eq	270 <__addsf3+0x270>  // b.none
 14c:	cmp	w0, #0x2
 150:	b.eq	258 <__addsf3+0x258>  // b.none
 154:	cbz	w0, 21c <__addsf3+0x21c>
 158:	cbnz	w20, 230 <__addsf3+0x230>
 15c:	fmov	s0, w19
 160:	ldp	x19, x20, [sp, #16]
 164:	ldr	x21, [sp, #32]
 168:	ldp	x29, x30, [sp], #48
 16c:	ret
 170:	cmp	w3, w4
 174:	b.hi	1c8 <__addsf3+0x1c8>  // b.pmore
 178:	cmp	w2, w4
 17c:	b.ne	238 <__addsf3+0x238>  // b.any
 180:	eor	w0, w1, w0
 184:	mov	w1, #0x80000000            	// #-2147483648
 188:	cmp	w0, w1
 18c:	mov	w0, #0x7fc00000            	// #2143289344
 190:	fmov	s1, w0
 194:	fcsel	s0, s0, s1, ne  // ne = any
 198:	ret
 19c:	movi	v0.2s, #0x0
 1a0:	subs	w0, w1, w2
 1a4:	b.eq	160 <__addsf3+0x160>  // b.none
 1a8:	mov	w1, #0x3ffffff             	// #67108863
 1ac:	cmp	w0, w1
 1b0:	b.hi	11c <__addsf3+0x11c>  // b.pmore
 1b4:	clz	w1, w0
 1b8:	sub	w1, w1, #0x5
 1bc:	sub	w3, w3, w1
 1c0:	lsl	w0, w0, w1
 1c4:	b	124 <__addsf3+0x124>
 1c8:	orr	w0, w0, #0x400000
 1cc:	fmov	s0, w0
 1d0:	ret
 1d4:	orr	w0, w19, #0x7f800000
 1d8:	fmov	s0, w0
 1dc:	b	160 <__addsf3+0x160>
 1e0:	ret
 1e4:	mov	w2, #0x1                   	// #1
 1e8:	tbz	w0, #31, 108 <__addsf3+0x108>
 1ec:	sub	w0, w1, #0x1
 1f0:	b	1a8 <__addsf3+0x1a8>
 1f4:	mov	w1, #0x1                   	// #1
 1f8:	sub	w1, w1, w3
 1fc:	neg	w2, w1
 200:	mov	w3, #0x0                   	// #0
 204:	lsr	w1, w0, w1
 208:	lsl	w0, w0, w2
 20c:	cmp	w0, #0x0
 210:	cset	w0, ne  // ne = any
 214:	orr	w0, w0, w1
 218:	b	130 <__addsf3+0x130>
 21c:	cmp	w20, #0x4
 220:	b.gt	264 <__addsf3+0x264>
 224:	b.ne	158 <__addsf3+0x158>  // b.any
 228:	add	w19, w19, #0x1
 22c:	and	w19, w19, #0xfffffffe
 230:	bl	0 <__fe_raise_inexact>
 234:	b	15c <__addsf3+0x15c>
 238:	cmp	w3, w4
 23c:	b.eq	284 <__addsf3+0x284>  // b.none
 240:	cbnz	w2, 60 <__addsf3+0x60>
 244:	cmp	w3, #0x0
 248:	and	w0, w1, w0
 24c:	fmov	s0, w0
 250:	fcsel	s0, s1, s0, ne  // ne = any
 254:	ret
 258:	cmp	w21, #0x0
 25c:	ccmp	w20, #0x0, #0x4, ge  // ge = tcont
 260:	b.eq	158 <__addsf3+0x158>  // b.none
 264:	add	w19, w19, #0x1
 268:	bl	0 <__fe_raise_inexact>
 26c:	b	15c <__addsf3+0x15c>
 270:	cmp	w21, #0x0
 274:	ccmp	w20, #0x0, #0x4, lt  // lt = tstop
 278:	b.eq	158 <__addsf3+0x158>  // b.none
 27c:	add	w19, w19, #0x1
 280:	b	268 <__addsf3+0x268>
 284:	fmov	s0, s1
 288:	ret

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x7, #0x7ffeffffffffffff    	// #9223090561878065151
   8:	mov	x29, sp
   c:	str	q0, [sp, #48]
  10:	ldp	x3, x2, [sp, #48]
  14:	str	q1, [sp, #64]
  18:	ldp	x1, x6, [sp, #64]
  1c:	stp	x19, x20, [sp, #16]
  20:	subs	x10, x3, #0x1
  24:	and	x4, x2, #0x7fffffffffffffff
  28:	sbc	x8, x4, xzr
  2c:	mov	x0, x3
  30:	mov	x19, x2
  34:	mov	x9, x6
  38:	and	x5, x6, #0x7fffffffffffffff
  3c:	cmp	x8, x7
  40:	mov	x11, x1
  44:	b.hi	294 <__addtf3+0x294>  // b.pmore
  48:	b.eq	28c <__addtf3+0x28c>  // b.none
  4c:	subs	x10, x11, #0x1
  50:	mov	x7, #0x7ffeffffffffffff    	// #9223090561878065151
  54:	sbc	x8, x5, xzr
  58:	cmp	x8, x7
  5c:	b.ls	2c0 <__addtf3+0x2c0>  // b.plast
  60:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
  64:	cmp	x5, x7
  68:	b.hi	31c <__addtf3+0x31c>  // b.pmore
  6c:	b.eq	3c4 <__addtf3+0x3c4>  // b.none
  70:	cbz	x11, 3c8 <__addtf3+0x3c8>
  74:	mov	x1, x3
  78:	orr	x3, x11, x5
  7c:	cbz	x3, 2ac <__addtf3+0x2ac>
  80:	cmp	x5, x4
  84:	b.ls	308 <__addtf3+0x308>  // b.plast
  88:	mov	x2, x0
  8c:	mov	x1, x19
  90:	mov	x0, x11
  94:	mov	x19, x9
  98:	mov	x11, x2
  9c:	mov	x9, x1
  a0:	ubfx	x6, x19, #48, #15
  a4:	ubfx	x2, x9, #48, #15
  a8:	and	x3, x19, #0xffffffffffff
  ac:	mov	x1, x11
  b0:	and	x4, x9, #0xffffffffffff
  b4:	cbnz	w6, 108 <__addtf3+0x108>
  b8:	cmp	x3, #0x0
  bc:	mov	w5, #0x40                  	// #64
  c0:	csel	x6, x3, x0, ne  // ne = any
  c4:	csel	w5, wzr, w5, ne  // ne = any
  c8:	clz	x6, x6
  cc:	lsr	x7, x0, #1
  d0:	add	w5, w5, w6
  d4:	mov	w8, #0x3f                  	// #63
  d8:	sub	w6, w5, #0xf
  dc:	subs	w5, w5, #0x4f
  e0:	sub	w8, w8, w6
  e4:	lsl	x3, x3, x6
  e8:	lsr	x7, x7, x8
  ec:	orr	x3, x7, x3
  f0:	lsl	x7, x0, x5
  f4:	mov	w5, #0x1                   	// #1
  f8:	lsl	x0, x0, x6
  fc:	csel	x3, x7, x3, pl  // pl = nfrst
 100:	csel	x0, xzr, x0, pl  // pl = nfrst
 104:	sub	w6, w5, w6
 108:	cbnz	w2, 15c <__addtf3+0x15c>
 10c:	cmp	x4, #0x0
 110:	mov	w1, #0x40                  	// #64
 114:	csel	x2, x4, x11, ne  // ne = any
 118:	csel	w1, wzr, w1, ne  // ne = any
 11c:	clz	x2, x2
 120:	lsr	x5, x11, #1
 124:	add	w1, w1, w2
 128:	mov	w7, #0x3f                  	// #63
 12c:	sub	w2, w1, #0xf
 130:	subs	w1, w1, #0x4f
 134:	sub	w7, w7, w2
 138:	lsl	x4, x4, x2
 13c:	lsr	x5, x5, x7
 140:	orr	x4, x5, x4
 144:	lsl	x5, x11, x1
 148:	csel	x4, x5, x4, pl  // pl = nfrst
 14c:	lsl	x1, x11, x2
 150:	mov	w5, #0x1                   	// #1
 154:	csel	x1, xzr, x1, pl  // pl = nfrst
 158:	sub	w2, w5, w2
 15c:	extr	x3, x3, x0, #61
 160:	orr	x3, x3, #0x8000000000000
 164:	extr	x4, x4, x1, #61
 168:	orr	x4, x4, #0x8000000000000
 16c:	eor	x9, x19, x9
 170:	lsl	x0, x0, #3
 174:	lsl	x1, x1, #3
 178:	subs	w2, w6, w2
 17c:	and	x7, x19, #0x8000000000000000
 180:	b.eq	1fc <__addtf3+0x1fc>  // b.none
 184:	cmp	w2, #0x7f
 188:	b.hi	3dc <__addtf3+0x3dc>  // b.pmore
 18c:	mov	w5, #0x80                  	// #128
 190:	sub	w5, w5, w2
 194:	lsr	x12, x1, #1
 198:	mov	w10, #0x3f                  	// #63
 19c:	sub	w13, w10, w5
 1a0:	subs	w11, w5, #0x40
 1a4:	lsl	x8, x4, x5
 1a8:	sub	w10, w10, w2
 1ac:	lsr	x12, x12, x13
 1b0:	orr	x8, x12, x8
 1b4:	lsl	x5, x1, x5
 1b8:	csel	x5, xzr, x5, pl  // pl = nfrst
 1bc:	lsl	x12, x1, x11
 1c0:	csel	x8, x12, x8, pl  // pl = nfrst
 1c4:	orr	x5, x5, x8
 1c8:	lsl	x8, x4, #1
 1cc:	cmp	x5, #0x0
 1d0:	sub	w5, w2, #0x40
 1d4:	cset	x11, ne  // ne = any
 1d8:	lsr	x1, x1, x2
 1dc:	cmp	w5, #0x0
 1e0:	lsl	x10, x8, x10
 1e4:	orr	x1, x10, x1
 1e8:	lsr	x8, x4, x5
 1ec:	csel	x1, x8, x1, ge  // ge = tcont
 1f0:	lsr	x2, x4, x2
 1f4:	orr	x1, x11, x1
 1f8:	csel	x4, x2, xzr, lt  // lt = tstop
 1fc:	tbnz	x9, #63, 33c <__addtf3+0x33c>
 200:	adds	x0, x0, x1
 204:	adc	x3, x3, x4
 208:	tbz	x3, #52, 220 <__addtf3+0x220>
 20c:	add	w6, w6, #0x1
 210:	extr	x1, x3, x0, #1
 214:	and	x0, x0, #0x1
 218:	lsr	x3, x3, #1
 21c:	orr	x0, x1, x0
 220:	mov	w1, #0x7ffe                	// #32766
 224:	cmp	w6, w1
 228:	b.gt	3b0 <__addtf3+0x3b0>
 22c:	stp	x21, x22, [sp, #32]
 230:	cmp	w6, #0x0
 234:	b.le	3ec <__addtf3+0x3ec>
 238:	lsl	x6, x6, #48
 23c:	orr	x6, x7, x6
 240:	ubfx	x1, x3, #3, #48
 244:	and	w22, w0, #0x7
 248:	extr	x20, x3, x0, #3
 24c:	orr	x21, x6, x1
 250:	bl	0 <__fe_getround>
 254:	cmp	w0, #0x1
 258:	b.eq	510 <__addtf3+0x510>  // b.none
 25c:	cmp	w0, #0x2
 260:	b.eq	4e0 <__addtf3+0x4e0>  // b.none
 264:	cbz	w0, 46c <__addtf3+0x46c>
 268:	cbnz	w22, 488 <__addtf3+0x488>
 26c:	mov	x1, x20
 270:	mov	x2, x21
 274:	stp	x1, x2, [sp, #48]
 278:	ldp	x19, x20, [sp, #16]
 27c:	ldp	x21, x22, [sp, #32]
 280:	ldr	q0, [sp, #48]
 284:	ldp	x29, x30, [sp], #80
 288:	ret
 28c:	cmn	x10, #0x2
 290:	b.ls	4c <__addtf3+0x4c>  // b.plast
 294:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 298:	cmp	x4, x7
 29c:	b.ls	2d0 <__addtf3+0x2d0>  // b.plast
 2a0:	orr	x3, x19, #0x800000000000
 2a4:	mov	x1, x0
 2a8:	mov	x2, x3
 2ac:	stp	x1, x2, [sp, #48]
 2b0:	ldp	x19, x20, [sp, #16]
 2b4:	ldr	q0, [sp, #48]
 2b8:	ldp	x29, x30, [sp], #80
 2bc:	ret
 2c0:	b.ne	80 <__addtf3+0x80>  // b.any
 2c4:	cmn	x10, #0x2
 2c8:	b.hi	60 <__addtf3+0x60>  // b.pmore
 2cc:	b	80 <__addtf3+0x80>
 2d0:	b.ne	2d8 <__addtf3+0x2d8>  // b.any
 2d4:	cbnz	x0, 2a0 <__addtf3+0x2a0>
 2d8:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 2dc:	cmp	x5, x7
 2e0:	b.hi	31c <__addtf3+0x31c>  // b.pmore
 2e4:	b.eq	318 <__addtf3+0x318>  // b.none
 2e8:	cbnz	x0, 490 <__addtf3+0x490>
 2ec:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 2f0:	cmp	x4, x7
 2f4:	b.ne	490 <__addtf3+0x490>  // b.any
 2f8:	eor	x19, x19, x9
 2fc:	cbz	x11, 4bc <__addtf3+0x4bc>
 300:	mov	x1, x3
 304:	b	2ac <__addtf3+0x2ac>
 308:	b.ne	a0 <__addtf3+0xa0>  // b.any
 30c:	cmp	x11, x0
 310:	b.ls	a0 <__addtf3+0xa0>  // b.plast
 314:	b	88 <__addtf3+0x88>
 318:	cbz	x11, 2e8 <__addtf3+0x2e8>
 31c:	orr	x3, x9, #0x800000000000
 320:	mov	x1, x11
 324:	mov	x2, x3
 328:	stp	x1, x2, [sp, #48]
 32c:	ldp	x19, x20, [sp, #16]
 330:	ldr	q0, [sp, #48]
 334:	ldp	x29, x30, [sp], #80
 338:	ret
 33c:	subs	x0, x0, x1
 340:	mov	x2, #0x0                   	// #0
 344:	sbc	x3, x3, x4
 348:	mov	x1, #0x0                   	// #0
 34c:	orr	x4, x0, x3
 350:	cbz	x4, 2ac <__addtf3+0x2ac>
 354:	mov	x1, #0x7ffffffffffff       	// #2251799813685247
 358:	cmp	x3, x1
 35c:	b.hi	220 <__addtf3+0x220>  // b.pmore
 360:	cmp	x3, #0x0
 364:	mov	w1, #0x40                  	// #64
 368:	csel	x2, x3, x0, ne  // ne = any
 36c:	csel	w1, wzr, w1, ne  // ne = any
 370:	clz	x2, x2
 374:	mov	w5, #0x3f                  	// #63
 378:	add	w1, w1, w2
 37c:	lsr	x2, x0, #1
 380:	sub	w4, w1, #0xc
 384:	subs	w1, w1, #0x4c
 388:	sub	w5, w5, w4
 38c:	sub	w6, w6, w4
 390:	lsl	x3, x3, x4
 394:	lsr	x2, x2, x5
 398:	orr	x3, x2, x3
 39c:	lsl	x2, x0, x1
 3a0:	csel	x3, x2, x3, pl  // pl = nfrst
 3a4:	lsl	x0, x0, x4
 3a8:	csel	x0, xzr, x0, pl  // pl = nfrst
 3ac:	b	220 <__addtf3+0x220>
 3b0:	mov	x2, #0x0                   	// #0
 3b4:	orr	x3, x7, #0x7fff000000000000
 3b8:	mov	x1, x2
 3bc:	mov	x2, x3
 3c0:	b	2ac <__addtf3+0x2ac>
 3c4:	cbnz	x11, 31c <__addtf3+0x31c>
 3c8:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 3cc:	cmp	x5, x7
 3d0:	b.ne	74 <__addtf3+0x74>  // b.any
 3d4:	mov	x2, x6
 3d8:	b	2ac <__addtf3+0x2ac>
 3dc:	tbnz	x9, #63, 4d4 <__addtf3+0x4d4>
 3e0:	mov	x1, #0x1                   	// #1
 3e4:	mov	x4, #0x0                   	// #0
 3e8:	b	200 <__addtf3+0x200>
 3ec:	mov	w1, #0x1                   	// #1
 3f0:	sub	w6, w1, w6
 3f4:	mov	w1, #0x80                  	// #128
 3f8:	sub	w1, w1, w6
 3fc:	lsr	x8, x0, #1
 400:	mov	w4, #0x3f                  	// #63
 404:	sub	w9, w4, w1
 408:	subs	w5, w1, #0x40
 40c:	lsl	x2, x3, x1
 410:	sub	w4, w4, w6
 414:	lsr	x8, x8, x9
 418:	orr	x2, x8, x2
 41c:	lsl	x1, x0, x1
 420:	csel	x1, xzr, x1, pl  // pl = nfrst
 424:	lsl	x8, x0, x5
 428:	csel	x2, x8, x2, pl  // pl = nfrst
 42c:	orr	x1, x1, x2
 430:	lsl	x2, x3, #1
 434:	cmp	x1, #0x0
 438:	sub	w1, w6, #0x40
 43c:	cset	x5, ne  // ne = any
 440:	lsr	x0, x0, x6
 444:	cmp	w1, #0x0
 448:	lsl	x4, x2, x4
 44c:	orr	x0, x4, x0
 450:	lsr	x2, x3, x1
 454:	csel	x0, x2, x0, ge  // ge = tcont
 458:	lsr	x3, x3, x6
 45c:	orr	x0, x5, x0
 460:	csel	x3, x3, xzr, lt  // lt = tstop
 464:	mov	x6, #0x0                   	// #0
 468:	b	23c <__addtf3+0x23c>
 46c:	cmp	w22, #0x4
 470:	b.gt	4f8 <__addtf3+0x4f8>
 474:	b.ne	268 <__addtf3+0x268>  // b.any
 478:	adds	x1, x20, #0x1
 47c:	cinc	x0, x21, cs  // cs = hs, nlast
 480:	and	x20, x1, #0xfffffffffffffffe
 484:	mov	x21, x0
 488:	bl	0 <__fe_raise_inexact>
 48c:	b	26c <__addtf3+0x26c>
 490:	cbz	x11, 528 <__addtf3+0x528>
 494:	orr	x7, x0, x4
 498:	cbnz	x7, 74 <__addtf3+0x74>
 49c:	orr	x5, x11, x5
 4a0:	mov	x2, x6
 4a4:	cbnz	x5, 2ac <__addtf3+0x2ac>
 4a8:	and	x2, x0, x11
 4ac:	and	x3, x19, x9
 4b0:	mov	x1, x2
 4b4:	mov	x2, x3
 4b8:	b	2ac <__addtf3+0x2ac>
 4bc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 4c0:	cmp	x19, x0
 4c4:	b.ne	300 <__addtf3+0x300>  // b.any
 4c8:	mov	x1, #0x0                   	// #0
 4cc:	mov	x2, #0x7fff800000000000    	// #9223231299366420480
 4d0:	b	2ac <__addtf3+0x2ac>
 4d4:	subs	x0, x0, #0x1
 4d8:	sbc	x3, x3, xzr
 4dc:	b	354 <__addtf3+0x354>
 4e0:	mvn	x19, x19
 4e4:	cmp	w22, #0x0
 4e8:	cset	w0, ne  // ne = any
 4ec:	lsr	x19, x19, #63
 4f0:	tst	w0, w19
 4f4:	b.eq	268 <__addtf3+0x268>  // b.none
 4f8:	adds	x0, x20, #0x1
 4fc:	cinc	x1, x21, cs  // cs = hs, nlast
 500:	mov	x20, x0
 504:	mov	x21, x1
 508:	bl	0 <__fe_raise_inexact>
 50c:	b	26c <__addtf3+0x26c>
 510:	cmp	w22, #0x0
 514:	lsr	x19, x19, #63
 518:	cset	w0, ne  // ne = any
 51c:	tst	w0, w19
 520:	b.eq	268 <__addtf3+0x268>  // b.none
 524:	b	4f8 <__addtf3+0x4f8>
 528:	mov	x7, #0x7fff000000000000    	// #9223090561878065152
 52c:	cmp	x5, x7
 530:	b.ne	494 <__addtf3+0x494>  // b.any
 534:	mov	x2, x6
 538:	b	2ac <__addtf3+0x2ac>

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x2, x0
   8:	add	x0, x0, x1
   c:	mov	x29, sp
  10:	tbnz	x1, #63, 24 <__addvdi3+0x24>
  14:	cmp	x2, x0
  18:	b.gt	44 <__addvdi3+0x44>
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	cmp	x2, x0
  28:	b.gt	1c <__addvdi3+0x1c>
  2c:	adrp	x2, 0 <__addvdi3>
  30:	adrp	x0, 0 <__addvdi3>
  34:	add	x2, x2, #0x0
  38:	add	x0, x0, #0x0
  3c:	mov	w1, #0x1a                  	// #26
  40:	bl	0 <__compilerrt_abort_impl>
  44:	adrp	x2, 0 <__addvdi3>
  48:	adrp	x0, 0 <__addvdi3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x17                  	// #23
  58:	bl	0 <__compilerrt_abort_impl>

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w2, w0
   8:	add	w0, w0, w1
   c:	mov	x29, sp
  10:	tbnz	w1, #31, 24 <__addvsi3+0x24>
  14:	cmp	w2, w0
  18:	b.gt	44 <__addvsi3+0x44>
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	cmp	w2, w0
  28:	b.gt	1c <__addvsi3+0x1c>
  2c:	adrp	x2, 0 <__addvsi3>
  30:	adrp	x0, 0 <__addvsi3>
  34:	add	x2, x2, #0x0
  38:	add	x0, x0, #0x0
  3c:	mov	w1, #0x1a                  	// #26
  40:	bl	0 <__compilerrt_abort_impl>
  44:	adrp	x2, 0 <__addvsi3>
  48:	adrp	x0, 0 <__addvsi3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x17                  	// #23
  58:	bl	0 <__compilerrt_abort_impl>

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x4, x0
   8:	adds	x0, x0, x2
   c:	mov	x29, sp
  10:	mov	x2, x1
  14:	adc	x1, x1, x3
  18:	tbnz	x3, #63, 30 <__addvti3+0x30>
  1c:	cmp	x2, x1
  20:	b.gt	64 <__addvti3+0x64>
  24:	b.eq	5c <__addvti3+0x5c>  // b.none
  28:	ldp	x29, x30, [sp], #16
  2c:	ret
  30:	cmp	x2, x1
  34:	b.gt	28 <__addvti3+0x28>
  38:	b.ne	44 <__addvti3+0x44>  // b.any
  3c:	cmp	x4, x0
  40:	b.hi	28 <__addvti3+0x28>  // b.pmore
  44:	adrp	x2, 0 <__addvti3>
  48:	adrp	x0, 0 <__addvti3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x1c                  	// #28
  58:	bl	0 <__compilerrt_abort_impl>
  5c:	cmp	x4, x0
  60:	b.ls	28 <__addvti3+0x28>  // b.plast
  64:	adrp	x2, 0 <__addvti3>
  68:	adrp	x0, 0 <__addvti3>
  6c:	add	x2, x2, #0x0
  70:	add	x0, x0, #0x0
  74:	mov	w1, #0x19                  	// #25
  78:	bl	0 <__compilerrt_abort_impl>

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 1c <__ashldi3+0x1c>
   8:	sub	w1, w1, #0x20
   c:	mov	x0, #0x0                   	// #0
  10:	lsl	w2, w2, w1
  14:	bfi	x0, x2, #32, #32
  18:	ret
  1c:	cbz	w1, 18 <__ashldi3+0x18>
  20:	neg	w5, w1
  24:	asr	x3, x0, #32
  28:	lsl	w4, w0, w1
  2c:	mov	x0, #0x0                   	// #0
  30:	bfxil	x0, x4, #0, #32
  34:	lsr	w2, w2, w5
  38:	lsl	w1, w3, w1
  3c:	orr	w2, w2, w1
  40:	bfi	x0, x2, #32, #32
  44:	ret

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbz	w2, #6, 18 <__ashlti3+0x18>
   4:	sub	w2, w2, #0x40
   8:	mov	x4, #0x0                   	// #0
   c:	lsl	x1, x0, x2
  10:	mov	x0, x4
  14:	ret
  18:	cbz	w2, 14 <__ashlti3+0x14>
  1c:	neg	w5, w2
  20:	lsl	x3, x1, x2
  24:	lsl	x4, x0, x2
  28:	lsr	x1, x0, x5
  2c:	mov	x0, x4
  30:	orr	x1, x1, x3
  34:	b	14 <__ashlti3+0x14>

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 28 <__ashrdi3+0x28>
   8:	asr	x3, x0, #63
   c:	asr	x2, x0, #32
  10:	sub	w1, w1, #0x20
  14:	mov	x0, #0x0                   	// #0
  18:	bfi	x0, x3, #32, #32
  1c:	asr	w2, w2, w1
  20:	bfxil	x0, x2, #0, #32
  24:	ret
  28:	cbz	w1, 24 <__ashrdi3+0x24>
  2c:	asr	x3, x0, #32
  30:	neg	w4, w1
  34:	lsr	w2, w2, w1
  38:	mov	x0, #0x0                   	// #0
  3c:	asr	w1, w3, w1
  40:	bfi	x0, x1, #32, #32
  44:	lsl	w3, w3, w4
  48:	orr	w2, w3, w2
  4c:	bfxil	x0, x2, #0, #32
  50:	ret

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	tbz	w2, #6, 18 <__ashrti3+0x18>
   4:	sub	w2, w2, #0x40
   8:	asr	x4, x1, #63
   c:	asr	x0, x1, x2
  10:	mov	x1, x4
  14:	ret
  18:	cbz	w2, 14 <__ashrti3+0x14>
  1c:	neg	w5, w2
  20:	lsr	x3, x0, x2
  24:	asr	x4, x1, x2
  28:	lsl	x0, x1, x5
  2c:	mov	x1, x4
  30:	orr	x0, x0, x3
  34:	b	14 <__ashrti3+0x14>

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	asr	x1, x0, #32
   4:	cmp	w1, #0x0
   8:	csel	w0, w0, w1, eq  // eq = none
   c:	csetm	w1, eq  // eq = none
  10:	and	w1, w1, #0x20
  14:	clz	w0, w0
  18:	add	w0, w1, w0
  1c:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	tst	w0, #0xffff0000
   4:	b.eq	74 <__clzsi2+0x74>  // b.none
   8:	lsr	w0, w0, #16
   c:	mov	w2, #0x8                   	// #8
  10:	mov	w1, #0x0                   	// #0
  14:	tst	w0, #0xff00
  18:	b.eq	24 <__clzsi2+0x24>  // b.none
  1c:	lsr	w0, w0, #8
  20:	mov	w2, w1
  24:	tst	w0, #0xf0
  28:	b.eq	68 <__clzsi2+0x68>  // b.none
  2c:	lsr	w0, w0, #4
  30:	mov	w4, #0x2                   	// #2
  34:	mov	w1, #0x0                   	// #0
  38:	tst	w0, #0xc
  3c:	b.eq	48 <__clzsi2+0x48>  // b.none
  40:	lsr	w0, w0, #2
  44:	mov	w4, w1
  48:	eor	x1, x0, #0x2
  4c:	mov	w3, #0x2                   	// #2
  50:	sub	w0, w3, w0
  54:	add	w2, w2, w4
  58:	sbfx	w1, w1, #1, #1
  5c:	and	w0, w1, w0
  60:	add	w0, w0, w2
  64:	ret
  68:	mov	w4, #0x6                   	// #6
  6c:	mov	w1, #0x4                   	// #4
  70:	b	38 <__clzsi2+0x38>
  74:	mov	w2, #0x18                  	// #24
  78:	mov	w1, #0x10                  	// #16
  7c:	b	14 <__clzsi2+0x14>

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csel	x1, x1, x0, ne  // ne = any
   8:	csetm	w0, eq  // eq = none
   c:	clz	x1, x1
  10:	and	w0, w0, #0x40
  14:	add	w0, w0, w1
  18:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	asr	x3, x0, #32
   4:	mov	x2, x0
   8:	asr	x0, x1, #32
   c:	cmp	w3, w0
  10:	b.lt	34 <__cmpdi2+0x34>  // b.tstop
  14:	mov	w0, #0x2                   	// #2
  18:	b.gt	30 <__cmpdi2+0x30>
  1c:	cmp	w2, w1
  20:	mov	w3, #0x0                   	// #0
  24:	cset	w0, hi  // hi = pmore
  28:	add	w0, w0, #0x1
  2c:	csel	w0, w0, w3, cs  // cs = hs, nlast
  30:	ret
  34:	mov	w0, #0x0                   	// #0
  38:	ret

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.lt	2c <__cmpti2+0x2c>  // b.tstop
  10:	mov	w0, #0x2                   	// #2
  14:	b.gt	2c <__cmpti2+0x2c>
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	fmov	x2, d0
   4:	fmov	x1, d1
   8:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
   c:	mov	w0, #0x1                   	// #1
  10:	and	x5, x2, #0x7fffffffffffffff
  14:	and	x4, x1, #0x7fffffffffffffff
  18:	cmp	x5, x3
  1c:	ccmp	x4, x3, #0x2, ls  // ls = plast
  20:	b.hi	4c <__cmpdf2+0x4c>  // b.pmore
  24:	orr	x3, x2, x1
  28:	mov	w0, #0x0                   	// #0
  2c:	tst	x3, #0x7fffffffffffffff
  30:	b.eq	4c <__cmpdf2+0x4c>  // b.none
  34:	tst	x2, x1
  38:	b.mi	50 <__cmpdf2+0x50>  // b.first
  3c:	cmp	x2, x1
  40:	mov	w3, #0xffffffff            	// #-1
  44:	cset	w0, ne  // ne = any
  48:	csel	w0, w0, w3, ge  // ge = tcont
  4c:	ret
  50:	cmp	x2, x1
  54:	mov	w3, #0xffffffff            	// #-1
  58:	cset	w0, ne  // ne = any
  5c:	csel	w0, w0, w3, le
  60:	ret
  64:	nop

0000000000000068 <__gedf2>:
  68:	fmov	x2, d0
  6c:	fmov	x1, d1
  70:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
  74:	mov	w0, #0xffffffff            	// #-1
  78:	and	x5, x2, #0x7fffffffffffffff
  7c:	and	x4, x1, #0x7fffffffffffffff
  80:	cmp	x5, x3
  84:	ccmp	x4, x3, #0x2, ls  // ls = plast
  88:	b.hi	b4 <__gedf2+0x4c>  // b.pmore
  8c:	orr	x3, x2, x1
  90:	mov	w0, #0x0                   	// #0
  94:	tst	x3, #0x7fffffffffffffff
  98:	b.eq	b4 <__gedf2+0x4c>  // b.none
  9c:	tst	x2, x1
  a0:	b.mi	b8 <__gedf2+0x50>  // b.first
  a4:	cmp	x2, x1
  a8:	mov	w3, #0xffffffff            	// #-1
  ac:	cset	w0, ne  // ne = any
  b0:	csel	w0, w0, w3, ge  // ge = tcont
  b4:	ret
  b8:	cmp	x2, x1
  bc:	mov	w3, #0xffffffff            	// #-1
  c0:	cset	w0, ne  // ne = any
  c4:	csel	w0, w0, w3, le
  c8:	ret
  cc:	nop

00000000000000d0 <__unorddf2>:
  d0:	fmov	x1, d0
  d4:	fmov	x0, d1
  d8:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  dc:	and	x1, x1, #0x7fffffffffffffff
  e0:	and	x0, x0, #0x7fffffffffffffff
  e4:	cmp	x1, x2
  e8:	ccmp	x0, x2, #0x2, ls  // ls = plast
  ec:	cset	w0, hi  // hi = pmore
  f0:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	fmov	w2, s0
   4:	fmov	w1, s1
   8:	mov	w3, #0x7f800000            	// #2139095040
   c:	mov	w0, #0x1                   	// #1
  10:	and	w5, w2, #0x7fffffff
  14:	and	w4, w1, #0x7fffffff
  18:	cmp	w5, w3
  1c:	ccmp	w4, w3, #0x2, ls  // ls = plast
  20:	b.hi	4c <__cmpsf2+0x4c>  // b.pmore
  24:	orr	w3, w2, w1
  28:	mov	w0, #0x0                   	// #0
  2c:	tst	x3, #0x7fffffff
  30:	b.eq	4c <__cmpsf2+0x4c>  // b.none
  34:	tst	w2, w1
  38:	b.mi	50 <__cmpsf2+0x50>  // b.first
  3c:	cmp	w2, w1
  40:	mov	w3, #0xffffffff            	// #-1
  44:	cset	w0, ne  // ne = any
  48:	csel	w0, w0, w3, ge  // ge = tcont
  4c:	ret
  50:	cmp	w2, w1
  54:	mov	w3, #0xffffffff            	// #-1
  58:	cset	w0, ne  // ne = any
  5c:	csel	w0, w0, w3, le
  60:	ret
  64:	nop

0000000000000068 <__gesf2>:
  68:	fmov	w2, s0
  6c:	fmov	w1, s1
  70:	mov	w3, #0x7f800000            	// #2139095040
  74:	mov	w0, #0xffffffff            	// #-1
  78:	and	w5, w2, #0x7fffffff
  7c:	and	w4, w1, #0x7fffffff
  80:	cmp	w5, w3
  84:	ccmp	w4, w3, #0x2, ls  // ls = plast
  88:	b.hi	b4 <__gesf2+0x4c>  // b.pmore
  8c:	orr	w3, w2, w1
  90:	mov	w0, #0x0                   	// #0
  94:	tst	x3, #0x7fffffff
  98:	b.eq	b4 <__gesf2+0x4c>  // b.none
  9c:	tst	w2, w1
  a0:	b.mi	b8 <__gesf2+0x50>  // b.first
  a4:	cmp	w2, w1
  a8:	mov	w3, #0xffffffff            	// #-1
  ac:	cset	w0, ne  // ne = any
  b0:	csel	w0, w0, w3, ge  // ge = tcont
  b4:	ret
  b8:	cmp	w2, w1
  bc:	mov	w3, #0xffffffff            	// #-1
  c0:	cset	w0, ne  // ne = any
  c4:	csel	w0, w0, w3, le
  c8:	ret
  cc:	nop

00000000000000d0 <__unordsf2>:
  d0:	fmov	w1, s0
  d4:	fmov	w0, s1
  d8:	mov	w2, #0x7f800000            	// #2139095040
  dc:	and	w1, w1, #0x7fffffff
  e0:	and	w0, w0, #0x7fffffff
  e4:	cmp	w1, w2
  e8:	ccmp	w0, w2, #0x2, ls  // ls = plast
  ec:	cset	w0, hi  // hi = pmore
  f0:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	cmp	w0, #0x0
   4:	asr	x1, x0, #32
   8:	csel	w0, w1, w0, eq  // eq = none
   c:	csetm	w2, eq  // eq = none
  10:	rbit	w0, w0
  14:	and	w2, w2, #0x20
  18:	clz	w0, w0
  1c:	add	w0, w2, w0
  20:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	b.ne	78 <__ctzsi2+0x78>  // b.any
   8:	lsr	w0, w0, #16
   c:	mov	w1, #0x18                  	// #24
  10:	mov	w2, #0x10                  	// #16
  14:	tst	w0, #0xff
  18:	b.ne	24 <__ctzsi2+0x24>  // b.any
  1c:	lsr	w0, w0, #8
  20:	mov	w2, w1
  24:	tst	x0, #0xf
  28:	b.ne	6c <__ctzsi2+0x6c>  // b.any
  2c:	lsr	w0, w0, #4
  30:	mov	w1, #0x6                   	// #6
  34:	mov	w4, #0x4                   	// #4
  38:	tst	x0, #0x3
  3c:	b.ne	48 <__ctzsi2+0x48>  // b.any
  40:	lsr	w0, w0, #2
  44:	mov	w4, w1
  48:	mvn	w3, w0
  4c:	mov	w1, #0x2                   	// #2
  50:	ubfx	x0, x0, #1, #1
  54:	add	w2, w2, w4
  58:	sub	w0, w1, w0
  5c:	sbfx	x1, x3, #0, #1
  60:	and	w0, w0, w1
  64:	add	w0, w0, w2
  68:	ret
  6c:	mov	w1, #0x2                   	// #2
  70:	mov	w4, #0x0                   	// #0
  74:	b	38 <__ctzsi2+0x38>
  78:	mov	w1, #0x8                   	// #8
  7c:	mov	w2, #0x0                   	// #0
  80:	b	14 <__ctzsi2+0x14>

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csel	x0, x0, x1, ne  // ne = any
   8:	csetm	w2, eq  // eq = none
   c:	rbit	x0, x0
  10:	and	w2, w2, #0x40
  14:	clz	x0, x0
  18:	add	w0, w2, w0
  1c:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	stp	x29, x30, [sp, #-96]!
   4:	mov	x29, sp
   8:	stp	d10, d11, [sp, #48]
   c:	fabs	d10, d2
  10:	stp	d12, d13, [sp, #64]
  14:	fmov	d13, d0
  18:	fabs	d0, d3
  1c:	str	x19, [sp, #16]
  20:	stp	d8, d9, [sp, #32]
  24:	fmov	d8, d2
  28:	fmov	d9, d3
  2c:	fmaxnm	d10, d10, d0
  30:	stp	d14, d15, [sp, #80]
  34:	fmov	d14, d1
  38:	fmov	x0, d10
  3c:	ubfx	x1, x0, #52, #11
  40:	cmp	w1, #0x7ff
  44:	b.eq	12c <__divdc3+0x12c>  // b.none
  48:	fcmp	d10, #0.0
  4c:	b.eq	114 <__divdc3+0x114>  // b.none
  50:	cbz	w1, 148 <__divdc3+0x148>
  54:	sub	w1, w1, #0x3ff
  58:	mov	w2, #0x1                   	// #1
  5c:	scvtf	d11, w1
  60:	fabs	d10, d11
  64:	cmp	w2, #0x0
  68:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  6c:	fmov	d0, x0
  70:	mov	w19, #0x0                   	// #0
  74:	fccmp	d10, d0, #0x0, ne  // ne = any
  78:	b.gt	a4 <__divdc3+0xa4>
  7c:	fcvtzs	w19, d11
  80:	fmov	d0, d8
  84:	neg	w19, w19
  88:	mov	w0, w19
  8c:	bl	0 <scalbn>
  90:	fmov	d8, d0
  94:	fmov	d0, d9
  98:	mov	w0, w19
  9c:	bl	0 <scalbn>
  a0:	fmov	d9, d0
  a4:	fmul	d1, d9, d9
  a8:	fmul	d2, d9, d14
  ac:	fmul	d12, d8, d8
  b0:	fmul	d0, d8, d13
  b4:	mov	w0, w19
  b8:	fadd	d12, d12, d1
  bc:	fadd	d0, d0, d2
  c0:	fdiv	d0, d0, d12
  c4:	bl	0 <scalbn>
  c8:	fmul	d1, d8, d14
  cc:	fmul	d2, d9, d13
  d0:	fmov	d15, d0
  d4:	mov	w0, w19
  d8:	fsub	d0, d1, d2
  dc:	fdiv	d0, d0, d12
  e0:	bl	0 <scalbn>
  e4:	fcmp	d0, d0
  e8:	fmov	d1, d0
  ec:	fccmp	d15, d15, #0x0, vs
  f0:	b.vs	184 <__divdc3+0x184>
  f4:	fmov	d0, d15
  f8:	ldr	x19, [sp, #16]
  fc:	ldp	d8, d9, [sp, #32]
 100:	ldp	d10, d11, [sp, #48]
 104:	ldp	d12, d13, [sp, #64]
 108:	ldp	d14, d15, [sp, #80]
 10c:	ldp	x29, x30, [sp], #96
 110:	ret
 114:	mov	x1, #0xfff0000000000000    	// #-4503599627370496
 118:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 11c:	fmov	d11, x1
 120:	fmov	d10, x0
 124:	mov	w19, #0x0                   	// #0
 128:	b	a4 <__divdc3+0xa4>
 12c:	cmp	x0, #0x0
 130:	fccmp	d10, d10, #0x0, lt  // lt = tstop
 134:	b.eq	174 <__divdc3+0x174>  // b.none
 138:	fcmp	d10, d10
 13c:	fmov	d11, d10
 140:	cset	w2, vc
 144:	b	64 <__divdc3+0x64>
 148:	and	x0, x0, #0x7fffffffffffffff
 14c:	mov	w2, #0x1                   	// #1
 150:	clz	x1, x0
 154:	sub	w1, w1, #0xb
 158:	lsl	x0, x0, x1
 15c:	ubfx	x0, x0, #52, #11
 160:	sub	w0, w0, #0x3ff
 164:	sub	w0, w0, w1
 168:	scvtf	d11, w0
 16c:	fabs	d10, d11
 170:	b	64 <__divdc3+0x64>
 174:	fcmp	d10, d10
 178:	fneg	d11, d10
 17c:	cset	w2, vc
 180:	b	64 <__divdc3+0x64>
 184:	fcmp	d12, #0.0
 188:	b.ne	1b8 <__divdc3+0x1b8>  // b.any
 18c:	fcmp	d13, d13
 190:	fccmp	d14, d14, #0x0, vs
 194:	b.vs	1b8 <__divdc3+0x1b8>
 198:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 19c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 1a0:	fmov	d1, x0
 1a4:	fmov	d0, x1
 1a8:	bit	v1.8b, v8.8b, v0.8b
 1ac:	fmul	d15, d1, d13
 1b0:	fmul	d1, d1, d14
 1b4:	b	f4 <__divdc3+0xf4>
 1b8:	fabs	d4, d13
 1bc:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1c0:	fmov	d0, x0
 1c4:	fcmp	d4, d0
 1c8:	b.gt	1d8 <__divdc3+0x1d8>
 1cc:	fabs	d3, d14
 1d0:	fcmp	d3, d0
 1d4:	b.le	1f0 <__divdc3+0x1f0>
 1d8:	fabs	d0, d8
 1dc:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1e0:	fmov	d3, x0
 1e4:	fcmp	d0, d3
 1e8:	fccmp	d8, d8, #0x1, le
 1ec:	b.vc	274 <__divdc3+0x274>
 1f0:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 1f4:	fmov	d3, x0
 1f8:	fcmp	d10, d3
 1fc:	b.le	f4 <__divdc3+0xf4>
 200:	fcmpe	d11, #0.0
 204:	fccmp	d4, d3, #0x0, gt
 208:	fccmp	d13, d13, #0x1, le
 20c:	b.vs	f4 <__divdc3+0xf4>
 210:	fabs	d0, d14
 214:	fcmp	d0, d3
 218:	fccmp	d14, d14, #0x1, le
 21c:	b.vs	f4 <__divdc3+0xf4>
 220:	fabs	d0, d8
 224:	movi	d4, #0x0
 228:	fabs	d5, d9
 22c:	fmov	d1, #1.000000000000000000e+00
 230:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 234:	fmov	d2, x0
 238:	fcmp	d0, d3
 23c:	fcsel	d0, d4, d1, le
 240:	fcmp	d5, d3
 244:	fcsel	d1, d4, d1, le
 248:	bif	v8.8b, v0.8b, v2.8b
 24c:	bif	v9.8b, v1.8b, v2.8b
 250:	fmul	d2, d13, d8
 254:	fmul	d1, d14, d8
 258:	fmul	d13, d13, d9
 25c:	fmul	d14, d14, d9
 260:	fsub	d1, d1, d13
 264:	fadd	d2, d2, d14
 268:	fmul	d1, d1, d4
 26c:	fmul	d15, d2, d4
 270:	b	f4 <__divdc3+0xf4>
 274:	fabs	d0, d9
 278:	fcmp	d0, d3
 27c:	fccmp	d9, d9, #0x1, le
 280:	b.vs	1f0 <__divdc3+0x1f0>
 284:	fcmp	d4, d3
 288:	movi	d0, #0x0
 28c:	fabs	d6, d14
 290:	fmov	d2, #1.000000000000000000e+00
 294:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 298:	fmov	d5, x0
 29c:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 2a0:	fmov	d4, x0
 2a4:	fcsel	d1, d0, d2, le
 2a8:	fcmp	d6, d3
 2ac:	bif	v13.8b, v1.8b, v5.8b
 2b0:	fcsel	d0, d0, d2, le
 2b4:	fmul	d2, d8, d13
 2b8:	fmul	d13, d9, d13
 2bc:	bif	v14.8b, v0.8b, v5.8b
 2c0:	fmul	d9, d9, d14
 2c4:	fmul	d1, d8, d14
 2c8:	fadd	d2, d2, d9
 2cc:	fsub	d1, d1, d13
 2d0:	fmul	d15, d2, d4
 2d4:	fmul	d1, d1, d4
 2d8:	b	f4 <__divdc3+0xf4>

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	fmov	x0, d0
   4:	fmov	x1, d1
   8:	ubfx	x7, x0, #52, #11
   c:	eor	x4, x1, x0
  10:	sub	w2, w7, #0x1
  14:	and	x3, x0, #0xfffffffffffff
  18:	and	x4, x4, #0x8000000000000000
  1c:	ubfx	x8, x1, #52, #11
  20:	cmp	w2, #0x7fd
  24:	and	x5, x1, #0xfffffffffffff
  28:	b.hi	15c <__divdf3+0x15c>  // b.pmore
  2c:	sub	w2, w8, #0x1
  30:	cmp	w2, #0x7fd
  34:	b.hi	15c <__divdf3+0x15c>  // b.pmore
  38:	mov	w12, #0x0                   	// #0
  3c:	orr	x5, x5, #0x10000000000000
  40:	mov	w9, #0xf333                	// #62259
  44:	movk	w9, #0x7504, lsl #16
  48:	sub	w1, w7, w8
  4c:	lsr	x11, x5, #21
  50:	lsl	w0, w5, #11
  54:	and	x10, x11, #0xffffffff
  58:	sub	w2, w9, w11
  5c:	sub	w11, w9, w11
  60:	orr	x6, x3, #0x10000000000000
  64:	lsl	w3, w3, #2
  68:	mov	x7, #0x1fffffffffffff      	// #9007199254740991
  6c:	mul	x2, x2, x10
  70:	ubfx	x9, x6, #30, #32
  74:	add	w1, w1, w12
  78:	lsr	x2, x2, #32
  7c:	neg	w2, w2
  80:	umull	x2, w2, w11
  84:	ubfx	x2, x2, #31, #32
  88:	mul	x8, x2, x10
  8c:	lsr	x8, x8, #32
  90:	neg	w8, w8
  94:	mul	x2, x8, x2
  98:	ubfx	x8, x2, #31, #32
  9c:	mul	x2, x10, x8
  a0:	lsr	x2, x2, #32
  a4:	neg	w2, w2
  a8:	mul	x2, x2, x8
  ac:	lsr	x2, x2, #31
  b0:	sub	w8, w2, #0x1
  b4:	mul	x10, x8, x10
  b8:	umull	x0, w0, w8
  bc:	add	x0, x10, x0, lsr #32
  c0:	neg	x2, x0
  c4:	neg	w0, w0
  c8:	lsr	x2, x2, #32
  cc:	mul	x0, x0, x8
  d0:	mul	x2, x2, x8
  d4:	sub	x2, x2, #0x2
  d8:	add	x0, x2, x0, lsr #32
  dc:	and	x8, x0, #0xffffffff
  e0:	lsr	x0, x0, #32
  e4:	mul	x2, x3, x8
  e8:	mul	x3, x3, x0
  ec:	mul	x8, x8, x9
  f0:	and	x10, x3, #0xffffffff
  f4:	mul	x0, x0, x9
  f8:	add	x2, x10, x2, lsr #32
  fc:	add	x2, x2, w8, uxtw
 100:	lsr	x8, x8, #32
 104:	add	x3, x8, x3, lsr #32
 108:	add	x0, x0, x2, lsr #32
 10c:	add	x0, x0, x3
 110:	cmp	x0, x7
 114:	b.ls	1b0 <__divdf3+0x1b0>  // b.plast
 118:	lsr	x0, x0, #1
 11c:	lsl	x6, x6, #52
 120:	add	w1, w1, #0x3ff
 124:	cmp	w1, #0x7fe
 128:	msub	x6, x5, x0, x6
 12c:	b.gt	1c8 <__divdf3+0x1c8>
 130:	cmp	w1, #0x0
 134:	b.gt	204 <__divdf3+0x204>
 138:	b.ne	1d4 <__divdf3+0x1d4>  // b.any
 13c:	cmp	x5, x6, lsl #1
 140:	and	x0, x0, #0xfffffffffffff
 144:	cinc	x0, x0, cc  // cc = lo, ul, last
 148:	tst	x0, #0x10000000000000
 14c:	b.eq	1d4 <__divdf3+0x1d4>  // b.none
 150:	orr	x0, x4, #0x10000000000000
 154:	fmov	d0, x0
 158:	ret
 15c:	and	x6, x0, #0x7fffffffffffffff
 160:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
 164:	cmp	x6, x2
 168:	b.hi	1a4 <__divdf3+0x1a4>  // b.pmore
 16c:	and	x9, x1, #0x7fffffffffffffff
 170:	cmp	x9, x2
 174:	b.hi	1dc <__divdf3+0x1dc>  // b.pmore
 178:	cmp	x6, x2
 17c:	b.eq	1e8 <__divdf3+0x1e8>  // b.none
 180:	cmp	x9, x2
 184:	b.eq	1d4 <__divdf3+0x1d4>  // b.none
 188:	cbnz	x6, 21c <__divdf3+0x21c>
 18c:	cmp	x9, #0x0
 190:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 194:	fmov	d1, x4
 198:	fmov	d0, x0
 19c:	fcsel	d0, d1, d0, ne  // ne = any
 1a0:	ret
 1a4:	orr	x0, x0, #0x8000000000000
 1a8:	fmov	d0, x0
 1ac:	ret
 1b0:	lsl	x6, x6, #53
 1b4:	sub	w1, w1, #0x1
 1b8:	add	w1, w1, #0x3ff
 1bc:	cmp	w1, #0x7fe
 1c0:	msub	x6, x5, x0, x6
 1c4:	b.le	130 <__divdf3+0x130>
 1c8:	orr	x0, x4, #0x7ff0000000000000
 1cc:	fmov	d0, x0
 1d0:	ret
 1d4:	fmov	d0, x4
 1d8:	ret
 1dc:	orr	x0, x1, #0x8000000000000
 1e0:	fmov	d0, x0
 1e4:	ret
 1e8:	orr	x0, x4, #0x7ff0000000000000
 1ec:	cmp	x9, x6
 1f0:	fmov	d1, x0
 1f4:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 1f8:	fmov	d0, x0
 1fc:	fcsel	d0, d0, d1, eq  // eq = none
 200:	ret
 204:	bfi	x0, x1, #52, #12
 208:	cmp	x5, x6, lsl #1
 20c:	cinc	x0, x0, cc  // cc = lo, ul, last
 210:	orr	x0, x0, x4
 214:	fmov	d0, x0
 218:	ret
 21c:	cbz	x9, 1c8 <__divdf3+0x1c8>
 220:	tst	x0, #0x7ff0000000000000
 224:	mov	w12, #0x0                   	// #0
 228:	b.ne	240 <__divdf3+0x240>  // b.any
 22c:	clz	x0, x3
 230:	mov	w12, #0x1                   	// #1
 234:	sub	w0, w0, #0xb
 238:	sub	w12, w12, w0
 23c:	lsl	x3, x3, x0
 240:	tst	x1, #0x7ff0000000000000
 244:	b.ne	3c <__divdf3+0x3c>  // b.any
 248:	clz	x0, x5
 24c:	sub	w0, w0, #0xb
 250:	add	w12, w12, w0
 254:	sub	w12, w12, #0x1
 258:	lsl	x5, x5, x0
 25c:	b	3c <__divdf3+0x3c>

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	asr	x3, x1, #63
   8:	eor	x1, x1, x1, asr #63
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	asr	x19, x0, #63
  18:	eor	x0, x0, x0, asr #63
  1c:	sub	x1, x1, x3
  20:	sub	x0, x0, x19
  24:	eor	x19, x19, x3
  28:	mov	x2, #0x0                   	// #0
  2c:	bl	0 <__udivmoddi4>
  30:	eor	x0, x0, x19
  34:	sub	x0, x0, x19
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x1
  10:	mov	x20, x2
  14:	str	x21, [sp, #32]
  18:	mov	x21, x0
  1c:	bl	0 <__divdi3>
  20:	msub	x19, x19, x0, x21
  24:	ldr	x21, [sp, #32]
  28:	str	x19, [x20]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	x20, x2
  14:	str	x21, [sp, #32]
  18:	mov	w21, w0
  1c:	bl	0 <__divsi3>
  20:	msub	w19, w19, w0, w21
  24:	ldr	x21, [sp, #32]
  28:	str	w19, [x20]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	stp	x29, x30, [sp, #-96]!
   4:	mov	x29, sp
   8:	stp	d10, d11, [sp, #48]
   c:	fmov	s11, s1
  10:	fabs	s1, s3
  14:	stp	d12, d13, [sp, #64]
  18:	fmov	s12, s0
  1c:	fabs	s0, s2
  20:	stp	d14, d15, [sp, #80]
  24:	fmov	s13, s2
  28:	fmov	s14, s3
  2c:	str	x19, [sp, #16]
  30:	fmaxnm	s15, s0, s1
  34:	stp	d8, d9, [sp, #32]
  38:	fmov	w0, s15
  3c:	ubfx	x1, x0, #23, #8
  40:	cmp	w1, #0xff
  44:	b.eq	128 <__divsc3+0x128>  // b.none
  48:	fcmp	s15, #0.0
  4c:	b.eq	114 <__divsc3+0x114>  // b.none
  50:	cbz	w1, 13c <__divsc3+0x13c>
  54:	sub	w1, w1, #0x7f
  58:	scvtf	s8, w1
  5c:	fabs	s15, s8
  60:	mov	w0, #0x7f7fffff            	// #2139095039
  64:	fmov	s0, w0
  68:	mov	w19, #0x0                   	// #0
  6c:	fcmp	s15, s0
  70:	fccmp	s8, s8, #0x1, le
  74:	b.vs	a4 <__divsc3+0xa4>
  78:	fcvtzs	s1, s8
  7c:	fmov	s0, s13
  80:	fmov	w0, s1
  84:	neg	w19, w0
  88:	mov	w0, w19
  8c:	bl	0 <scalbnf>
  90:	fmov	s13, s0
  94:	fmov	s0, s14
  98:	mov	w0, w19
  9c:	bl	0 <scalbnf>
  a0:	fmov	s14, s0
  a4:	fmul	s0, s14, s14
  a8:	fmul	s10, s13, s13
  ac:	fmul	s5, s13, s12
  b0:	fmul	s4, s14, s11
  b4:	mov	w0, w19
  b8:	fadd	s10, s10, s0
  bc:	fadd	s4, s5, s4
  c0:	fdiv	s0, s4, s10
  c4:	bl	0 <scalbnf>
  c8:	fmul	s1, s14, s12
  cc:	fmul	s4, s13, s11
  d0:	fmov	s9, s0
  d4:	mov	w0, w19
  d8:	fsub	s4, s4, s1
  dc:	fdiv	s0, s4, s10
  e0:	bl	0 <scalbnf>
  e4:	fcmp	s0, s0
  e8:	fmov	s1, s0
  ec:	fccmp	s9, s9, #0x0, vs
  f0:	b.vs	164 <__divsc3+0x164>
  f4:	fmov	s0, s9
  f8:	ldr	x19, [sp, #16]
  fc:	ldp	d8, d9, [sp, #32]
 100:	ldp	d10, d11, [sp, #48]
 104:	ldp	d12, d13, [sp, #64]
 108:	ldp	d14, d15, [sp, #80]
 10c:	ldp	x29, x30, [sp], #96
 110:	ret
 114:	mov	w0, #0x7f800000            	// #2139095040
 118:	mvni	v8.2s, #0x7f, msl #16
 11c:	mov	w19, #0x0                   	// #0
 120:	fmov	s15, w0
 124:	b	a4 <__divsc3+0xa4>
 128:	cmp	w0, #0x0
 12c:	fneg	s8, s15
 130:	fccmp	s15, s15, #0x0, lt  // lt = tstop
 134:	fcsel	s8, s8, s15, eq  // eq = none
 138:	b	60 <__divsc3+0x60>
 13c:	and	w0, w0, #0x7fffffff
 140:	clz	w1, w0
 144:	sub	w1, w1, #0x8
 148:	lsl	w0, w0, w1
 14c:	ubfx	x0, x0, #23, #8
 150:	sub	w0, w0, #0x7f
 154:	sub	w0, w0, w1
 158:	scvtf	s8, w0
 15c:	fabs	s15, s8
 160:	b	60 <__divsc3+0x60>
 164:	fcmp	s10, #0.0
 168:	b.ne	194 <__divsc3+0x194>  // b.any
 16c:	fcmp	s12, s12
 170:	fccmp	s11, s11, #0x0, vs
 174:	b.vs	194 <__divsc3+0x194>
 178:	movi	v1.2s, #0x80, lsl #24
 17c:	mov	w0, #0x7f800000            	// #2139095040
 180:	fmov	s3, w0
 184:	bsl	v1.8b, v13.8b, v3.8b
 188:	fmul	s9, s1, s12
 18c:	fmul	s1, s1, s11
 190:	b	f4 <__divsc3+0xf4>
 194:	fabs	s2, s12
 198:	mov	w0, #0x7f7fffff            	// #2139095039
 19c:	fmov	s0, w0
 1a0:	fcmp	s2, s0
 1a4:	cset	w1, le
 1a8:	b.gt	1b8 <__divsc3+0x1b8>
 1ac:	fabs	s2, s11
 1b0:	fcmp	s2, s0
 1b4:	b.le	1d0 <__divsc3+0x1d0>
 1b8:	fabs	s2, s13
 1bc:	mov	w0, #0x7f7fffff            	// #2139095039
 1c0:	fmov	s0, w0
 1c4:	fcmp	s2, s0
 1c8:	fccmp	s13, s13, #0x1, le
 1cc:	b.vc	258 <__divsc3+0x258>
 1d0:	mov	w0, #0x7f7fffff            	// #2139095039
 1d4:	fmov	s0, w0
 1d8:	fcmp	s15, s0
 1dc:	b.le	f4 <__divsc3+0xf4>
 1e0:	fcmpe	s8, #0.0
 1e4:	ccmp	w1, #0x0, #0x4, gt
 1e8:	fccmp	s12, s12, #0x1, ne  // ne = any
 1ec:	b.vs	f4 <__divsc3+0xf4>
 1f0:	fabs	s2, s11
 1f4:	fcmp	s2, s0
 1f8:	fccmp	s11, s11, #0x1, le
 1fc:	b.vs	f4 <__divsc3+0xf4>
 200:	fabs	s4, s13
 204:	fabs	s1, s14
 208:	movi	v3.2s, #0x80, lsl #24
 20c:	movi	v2.2s, #0x0
 210:	fcmp	s4, s0
 214:	cset	w0, gt
 218:	fcmp	s1, s0
 21c:	scvtf	s4, w0
 220:	cset	w0, gt
 224:	fmov	s1, s4
 228:	scvtf	s0, w0
 22c:	bit	v1.8b, v13.8b, v3.8b
 230:	bif	v14.8b, v0.8b, v3.8b
 234:	fmul	s4, s12, s1
 238:	fmul	s1, s11, s1
 23c:	fmul	s12, s12, s14
 240:	fmul	s11, s11, s14
 244:	fsub	s1, s1, s12
 248:	fadd	s4, s4, s11
 24c:	fmul	s1, s1, s2
 250:	fmul	s9, s4, s2
 254:	b	f4 <__divsc3+0xf4>
 258:	fabs	s2, s14
 25c:	fcmp	s2, s0
 260:	fccmp	s14, s14, #0x1, le
 264:	b.vs	1d0 <__divsc3+0x1d0>
 268:	fabs	s4, s11
 26c:	mov	w0, #0x7f800000            	// #2139095040
 270:	eor	w1, w1, #0x1
 274:	fmov	s2, w0
 278:	movi	v3.2s, #0x80, lsl #24
 27c:	scvtf	s1, w1
 280:	fcmp	s4, s0
 284:	bif	v12.8b, v1.8b, v3.8b
 288:	cset	w0, gt
 28c:	scvtf	s5, w0
 290:	fmul	s4, s13, s12
 294:	fmul	s12, s14, s12
 298:	bif	v11.8b, v5.8b, v3.8b
 29c:	fmul	s14, s14, s11
 2a0:	fmul	s1, s13, s11
 2a4:	fadd	s4, s4, s14
 2a8:	fsub	s1, s1, s12
 2ac:	fmul	s9, s4, s2
 2b0:	fmul	s1, s1, s2
 2b4:	b	f4 <__divsc3+0xf4>

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	fmov	w0, s0
   4:	fmov	w1, s1
   8:	ubfx	x3, x0, #23, #8
   c:	eor	w2, w1, w0
  10:	and	w6, w2, #0x80000000
  14:	sub	w2, w3, #0x1
  18:	and	w8, w0, #0x7fffff
  1c:	ubfx	x4, x1, #23, #8
  20:	cmp	w2, #0xfd
  24:	and	w9, w1, #0x7fffff
  28:	b.hi	f8 <__divsf3+0xf8>  // b.pmore
  2c:	sub	w2, w4, #0x1
  30:	cmp	w2, #0xfd
  34:	b.hi	f8 <__divsf3+0xf8>  // b.pmore
  38:	mov	w0, #0x0                   	// #0
  3c:	orr	w5, w9, #0x800000
  40:	sub	w3, w3, w4
  44:	mov	w7, #0xf333                	// #62259
  48:	add	w4, w3, w0
  4c:	lsl	w1, w5, #8
  50:	movk	w7, #0x7504, lsl #16
  54:	sub	w3, w7, w1
  58:	orr	w8, w8, #0x800000
  5c:	mov	x7, #0xffffff              	// #16777215
  60:	lsl	w9, w8, #1
  64:	umull	x2, w3, w1
  68:	lsr	x2, x2, #32
  6c:	neg	w2, w2
  70:	umull	x2, w2, w3
  74:	ubfx	x2, x2, #31, #32
  78:	mul	x3, x1, x2
  7c:	lsr	x3, x3, #32
  80:	neg	w3, w3
  84:	mul	x2, x3, x2
  88:	ubfx	x2, x2, #31, #32
  8c:	mul	x0, x1, x2
  90:	lsr	x0, x0, #32
  94:	neg	w0, w0
  98:	mul	x0, x0, x2
  9c:	lsr	x0, x0, #31
  a0:	sub	w0, w0, #0x2
  a4:	umull	x0, w0, w9
  a8:	lsr	x0, x0, #32
  ac:	cmp	x0, x7
  b0:	mov	w2, w0
  b4:	b.ls	14c <__divsf3+0x14c>  // b.plast
  b8:	lsr	w2, w0, #1
  bc:	lsl	w0, w8, #23
  c0:	add	w4, w4, #0x7f
  c4:	cmp	w4, #0xfe
  c8:	msub	w0, w5, w2, w0
  cc:	b.gt	164 <__divsf3+0x164>
  d0:	cmp	w4, #0x0
  d4:	b.gt	1a0 <__divsf3+0x1a0>
  d8:	b.ne	170 <__divsf3+0x170>  // b.any
  dc:	cmp	w5, w0, lsl #1
  e0:	and	w2, w2, #0x7fffff
  e4:	cinc	w2, w2, cc  // cc = lo, ul, last
  e8:	tbz	w2, #23, 170 <__divsf3+0x170>
  ec:	orr	w0, w6, #0x800000
  f0:	fmov	s0, w0
  f4:	ret
  f8:	and	w5, w0, #0x7fffffff
  fc:	mov	w2, #0x7f800000            	// #2139095040
 100:	cmp	w5, w2
 104:	b.hi	140 <__divsf3+0x140>  // b.pmore
 108:	and	w7, w1, #0x7fffffff
 10c:	cmp	w7, w2
 110:	b.hi	178 <__divsf3+0x178>  // b.pmore
 114:	cmp	w5, w2
 118:	b.eq	184 <__divsf3+0x184>  // b.none
 11c:	cmp	w7, w2
 120:	b.eq	170 <__divsf3+0x170>  // b.none
 124:	cbnz	w5, 1b8 <__divsf3+0x1b8>
 128:	cmp	w7, #0x0
 12c:	mov	w0, #0x7fc00000            	// #2143289344
 130:	fmov	s1, w6
 134:	fmov	s0, w0
 138:	fcsel	s0, s1, s0, ne  // ne = any
 13c:	ret
 140:	orr	w0, w0, #0x400000
 144:	fmov	s0, w0
 148:	ret
 14c:	lsl	w1, w8, #24
 150:	sub	w4, w4, #0x1
 154:	add	w4, w4, #0x7f
 158:	cmp	w4, #0xfe
 15c:	msub	w0, w5, w0, w1
 160:	b.le	d0 <__divsf3+0xd0>
 164:	orr	w0, w6, #0x7f800000
 168:	fmov	s0, w0
 16c:	ret
 170:	fmov	s0, w6
 174:	ret
 178:	orr	w0, w1, #0x400000
 17c:	fmov	s0, w0
 180:	ret
 184:	orr	w0, w6, #0x7f800000
 188:	cmp	w7, w5
 18c:	fmov	s1, w0
 190:	mov	w0, #0x7fc00000            	// #2143289344
 194:	fmov	s0, w0
 198:	fcsel	s0, s0, s1, eq  // eq = none
 19c:	ret
 1a0:	cmp	w5, w0, lsl #1
 1a4:	bfi	w2, w4, #23, #9
 1a8:	cinc	w2, w2, cc  // cc = lo, ul, last
 1ac:	orr	w0, w2, w6
 1b0:	fmov	s0, w0
 1b4:	ret
 1b8:	cbz	w7, 164 <__divsf3+0x164>
 1bc:	tst	w0, #0x7f800000
 1c0:	mov	w0, #0x0                   	// #0
 1c4:	b.ne	1dc <__divsf3+0x1dc>  // b.any
 1c8:	clz	w2, w8
 1cc:	mov	w0, #0x1                   	// #1
 1d0:	sub	w2, w2, #0x8
 1d4:	sub	w0, w0, w2
 1d8:	lsl	w8, w8, w2
 1dc:	tst	w1, #0x7f800000
 1e0:	b.ne	3c <__divsf3+0x3c>  // b.any
 1e4:	clz	w1, w9
 1e8:	sub	w1, w1, #0x8
 1ec:	add	w0, w0, w1
 1f0:	sub	w0, w0, #0x1
 1f4:	lsl	w9, w9, w1
 1f8:	b	3c <__divsf3+0x3c>

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	asr	w3, w1, #31
   4:	asr	w2, w0, #31
   8:	eor	w1, w1, w1, asr #31
   c:	eor	w0, w0, w0, asr #31
  10:	sub	w0, w0, w2
  14:	sub	w1, w1, w3
  18:	eor	w2, w2, w3
  1c:	udiv	w0, w0, w1
  20:	eor	w0, w0, w2
  24:	sub	w0, w0, w2
  28:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-208]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	str	q2, [sp, #96]
  10:	ldp	x20, x19, [sp, #96]
  14:	stp	x21, x22, [sp, #32]
  18:	str	q3, [sp, #112]
  1c:	ldp	x22, x21, [sp, #112]
  20:	str	x22, [sp, #96]
  24:	and	x0, x19, #0x7fffffffffffffff
  28:	stp	x20, x0, [sp, #112]
  2c:	stp	x23, x24, [sp, #48]
  30:	and	x0, x21, #0x7fffffffffffffff
  34:	str	x0, [sp, #104]
  38:	stp	x25, x26, [sp, #64]
  3c:	stp	x27, x28, [sp, #80]
  40:	str	q0, [sp, #128]
  44:	str	q1, [sp, #144]
  48:	ldr	q0, [sp, #112]
  4c:	ldp	x24, x25, [sp, #128]
  50:	ldp	x26, x27, [sp, #144]
  54:	ldr	q1, [sp, #96]
  58:	str	x27, [sp, #128]
  5c:	bl	0 <fmaxl>
  60:	str	q0, [sp, #96]
  64:	ldp	x23, x28, [sp, #96]
  68:	mov	w0, #0x7fff                	// #32767
  6c:	ubfx	x2, x28, #48, #15
  70:	cmp	w2, w0
  74:	b.eq	2c0 <__divtc3+0x2c0>  // b.none
  78:	movi	v1.2d, #0x0
  7c:	str	w2, [sp, #112]
  80:	bl	0 <__eqtf2>
  84:	cbz	w0, 2a8 <__divtc3+0x2a8>
  88:	ldr	w2, [sp, #112]
  8c:	cbz	w2, 310 <__divtc3+0x310>
  90:	mov	w4, #0xffffc001            	// #-16383
  94:	add	w0, w2, w4
  98:	bl	0 <__floatsitf>
  9c:	str	q0, [sp, #96]
  a0:	mov	w0, #0x1                   	// #1
  a4:	str	w0, [sp, #112]
  a8:	ldp	x1, x0, [sp, #96]
  ac:	stp	x1, x0, [sp, #176]
  b0:	mov	x23, x1
  b4:	and	x28, x0, #0x7fffffffffffffff
  b8:	adrp	x0, 0 <__divtc3>
  bc:	add	x0, x0, #0x0
  c0:	stp	x23, x28, [sp, #96]
  c4:	mov	w27, #0x1                   	// #1
  c8:	ldr	q1, [x0]
  cc:	ldr	q0, [sp, #96]
  d0:	bl	0 <__unordtf2>
  d4:	cbnz	w0, f8 <__divtc3+0xf8>
  d8:	adrp	x0, 0 <__divtc3>
  dc:	add	x0, x0, #0x0
  e0:	stp	x23, x28, [sp, #96]
  e4:	ldr	q1, [x0]
  e8:	ldr	q0, [sp, #96]
  ec:	bl	0 <__letf2>
  f0:	cmp	w0, #0x0
  f4:	cset	w27, le
  f8:	ldr	w0, [sp, #112]
  fc:	tst	w0, w27
 100:	mov	w27, #0x0                   	// #0
 104:	b.eq	154 <__divtc3+0x154>  // b.none
 108:	ldr	x0, [sp, #176]
 10c:	str	x0, [sp, #96]
 110:	ldr	x0, [sp, #184]
 114:	str	x0, [sp, #104]
 118:	ldr	q0, [sp, #96]
 11c:	bl	0 <__fixtfsi>
 120:	stp	x20, x19, [sp, #96]
 124:	neg	w27, w0
 128:	ldr	q0, [sp, #96]
 12c:	mov	w0, w27
 130:	bl	0 <scalbnl>
 134:	stp	x22, x21, [sp, #96]
 138:	mov	w0, w27
 13c:	str	q0, [sp, #112]
 140:	ldr	q0, [sp, #96]
 144:	ldp	x20, x19, [sp, #112]
 148:	bl	0 <scalbnl>
 14c:	str	q0, [sp, #96]
 150:	ldp	x22, x21, [sp, #96]
 154:	stp	x20, x19, [sp, #96]
 158:	stp	x20, x19, [sp, #112]
 15c:	ldr	q1, [sp, #96]
 160:	ldr	q0, [sp, #112]
 164:	bl	0 <__multf3>
 168:	stp	x22, x21, [sp, #96]
 16c:	stp	x22, x21, [sp, #112]
 170:	ldr	q1, [sp, #96]
 174:	str	q0, [sp, #96]
 178:	ldr	q0, [sp, #112]
 17c:	bl	0 <__multf3>
 180:	mov	v1.16b, v0.16b
 184:	ldr	q2, [sp, #96]
 188:	mov	v0.16b, v2.16b
 18c:	bl	0 <__addtf3>
 190:	stp	x24, x25, [sp, #96]
 194:	stp	x20, x19, [sp, #112]
 198:	ldr	q1, [sp, #96]
 19c:	str	q0, [sp, #144]
 1a0:	ldr	q0, [sp, #112]
 1a4:	bl	0 <__multf3>
 1a8:	stp	x22, x21, [sp, #112]
 1ac:	ldr	x0, [sp, #128]
 1b0:	stp	x26, x0, [sp, #96]
 1b4:	ldr	q1, [sp, #96]
 1b8:	str	q0, [sp, #96]
 1bc:	ldr	q0, [sp, #112]
 1c0:	bl	0 <__multf3>
 1c4:	mov	v1.16b, v0.16b
 1c8:	ldr	q2, [sp, #96]
 1cc:	mov	v0.16b, v2.16b
 1d0:	bl	0 <__addtf3>
 1d4:	ldr	q1, [sp, #144]
 1d8:	bl	0 <__divtf3>
 1dc:	mov	w0, w27
 1e0:	bl	0 <scalbnl>
 1e4:	ldr	x0, [sp, #128]
 1e8:	stp	x26, x0, [sp, #96]
 1ec:	stp	x20, x19, [sp, #112]
 1f0:	ldr	q1, [sp, #96]
 1f4:	str	q0, [sp, #160]
 1f8:	ldr	q0, [sp, #112]
 1fc:	bl	0 <__multf3>
 200:	stp	x24, x25, [sp, #96]
 204:	stp	x22, x21, [sp, #112]
 208:	ldr	q1, [sp, #96]
 20c:	str	q0, [sp, #96]
 210:	ldr	q0, [sp, #112]
 214:	bl	0 <__multf3>
 218:	mov	v1.16b, v0.16b
 21c:	ldr	q4, [sp, #96]
 220:	mov	v0.16b, v4.16b
 224:	bl	0 <__subtf3>
 228:	ldr	q1, [sp, #144]
 22c:	bl	0 <__divtf3>
 230:	ldr	q2, [sp, #160]
 234:	mov	w0, w27
 238:	str	q2, [sp, #96]
 23c:	bl	0 <scalbnl>
 240:	mov	v1.16b, v0.16b
 244:	str	q0, [sp, #160]
 248:	bl	0 <__unordtf2>
 24c:	cmp	w0, #0x0
 250:	cset	w1, ne  // ne = any
 254:	ldr	q2, [sp, #96]
 258:	str	w1, [sp, #112]
 25c:	mov	v1.16b, v2.16b
 260:	mov	v0.16b, v2.16b
 264:	bl	0 <__unordtf2>
 268:	cmp	w0, #0x0
 26c:	ldr	w1, [sp, #112]
 270:	cset	w0, ne  // ne = any
 274:	ldr	q2, [sp, #96]
 278:	tst	w0, w1
 27c:	ldr	q4, [sp, #160]
 280:	b.ne	398 <__divtc3+0x398>  // b.any
 284:	mov	v0.16b, v2.16b
 288:	mov	v1.16b, v4.16b
 28c:	ldp	x19, x20, [sp, #16]
 290:	ldp	x21, x22, [sp, #32]
 294:	ldp	x23, x24, [sp, #48]
 298:	ldp	x25, x26, [sp, #64]
 29c:	ldp	x27, x28, [sp, #80]
 2a0:	ldp	x29, x30, [sp], #208
 2a4:	ret
 2a8:	mov	x0, #0xffff000000000000    	// #-281474976710656
 2ac:	mov	w27, #0x0                   	// #0
 2b0:	mov	x23, #0x0                   	// #0
 2b4:	mov	x28, #0x7fff000000000000    	// #9223090561878065152
 2b8:	stp	xzr, x0, [sp, #176]
 2bc:	b	154 <__divtc3+0x154>
 2c0:	mvn	x2, x28
 2c4:	mov	v1.16b, v0.16b
 2c8:	lsr	x2, x2, #63
 2cc:	str	x2, [sp, #96]
 2d0:	bl	0 <__netf2>
 2d4:	cmp	w0, #0x0
 2d8:	ldr	x2, [sp, #96]
 2dc:	stp	x23, x28, [sp, #96]
 2e0:	cset	w0, ne  // ne = any
 2e4:	stp	x23, x28, [sp, #112]
 2e8:	orr	w2, w0, w2
 2ec:	ldr	q1, [sp, #96]
 2f0:	ldr	q0, [sp, #112]
 2f4:	cbz	w2, 378 <__divtc3+0x378>
 2f8:	bl	0 <__unordtf2>
 2fc:	cmp	w0, #0x0
 300:	cset	w0, eq  // eq = none
 304:	str	w0, [sp, #112]
 308:	stp	x23, x28, [sp, #176]
 30c:	b	b8 <__divtc3+0xb8>
 310:	ands	x1, x28, #0x7fffffffffffffff
 314:	mov	w2, #0x40                  	// #64
 318:	csel	x0, x1, x23, ne  // ne = any
 31c:	cmp	x1, #0x0
 320:	clz	x0, x0
 324:	csel	w2, wzr, w2, ne  // ne = any
 328:	add	w2, w2, w0
 32c:	lsr	x3, x23, #1
 330:	sub	w0, w2, #0xf
 334:	mov	w4, #0x3f                  	// #63
 338:	sub	w4, w4, w0
 33c:	subs	w2, w2, #0x4f
 340:	lsl	x1, x1, x0
 344:	lsr	x3, x3, x4
 348:	orr	x1, x3, x1
 34c:	lsl	x2, x23, x2
 350:	csel	x1, x2, x1, pl  // pl = nfrst
 354:	mov	w3, #0xffffc001            	// #-16383
 358:	mov	w2, #0x1                   	// #1
 35c:	ubfx	x1, x1, #48, #15
 360:	str	w2, [sp, #112]
 364:	add	w1, w1, w3
 368:	sub	w0, w1, w0
 36c:	bl	0 <__floatsitf>
 370:	str	q0, [sp, #96]
 374:	b	a8 <__divtc3+0xa8>
 378:	bl	0 <__unordtf2>
 37c:	cmp	w0, #0x0
 380:	cset	w0, eq  // eq = none
 384:	str	w0, [sp, #112]
 388:	eor	x0, x28, #0x8000000000000000
 38c:	str	x23, [sp, #176]
 390:	str	x0, [sp, #184]
 394:	b	b8 <__divtc3+0xb8>
 398:	movi	v1.2d, #0x0
 39c:	ldr	q0, [sp, #144]
 3a0:	str	q4, [sp, #112]
 3a4:	bl	0 <__eqtf2>
 3a8:	ldr	q2, [sp, #96]
 3ac:	ldr	q4, [sp, #112]
 3b0:	cbnz	w0, 468 <__divtc3+0x468>
 3b4:	stp	x24, x25, [sp, #96]
 3b8:	stp	x24, x25, [sp, #112]
 3bc:	ldr	q1, [sp, #96]
 3c0:	ldr	q0, [sp, #112]
 3c4:	str	q2, [sp, #160]
 3c8:	str	q4, [sp, #192]
 3cc:	bl	0 <__unordtf2>
 3d0:	cmp	w0, #0x0
 3d4:	ldr	x0, [sp, #128]
 3d8:	stp	x26, x0, [sp, #96]
 3dc:	cset	w1, eq  // eq = none
 3e0:	stp	x26, x0, [sp, #112]
 3e4:	ldr	q1, [sp, #96]
 3e8:	ldr	q0, [sp, #112]
 3ec:	str	w1, [sp, #144]
 3f0:	bl	0 <__unordtf2>
 3f4:	cmp	w0, #0x0
 3f8:	ldr	w1, [sp, #144]
 3fc:	cset	w0, eq  // eq = none
 400:	ldr	q2, [sp, #160]
 404:	orr	w1, w1, w0
 408:	ldr	q4, [sp, #192]
 40c:	tbz	w1, #0, 468 <__divtc3+0x468>
 410:	adrp	x0, 0 <__divtc3>
 414:	add	x0, x0, #0x0
 418:	ldr	q4, [x0]
 41c:	tbz	x19, #63, 42c <__divtc3+0x42c>
 420:	adrp	x0, 0 <__divtc3>
 424:	add	x0, x0, #0x0
 428:	ldr	q4, [x0]
 42c:	stp	x24, x25, [sp, #96]
 430:	mov	v0.16b, v4.16b
 434:	ldr	q1, [sp, #96]
 438:	str	q4, [sp, #144]
 43c:	bl	0 <__multf3>
 440:	str	q0, [sp, #112]
 444:	ldr	x0, [sp, #128]
 448:	stp	x26, x0, [sp, #96]
 44c:	ldr	q4, [sp, #144]
 450:	ldr	q1, [sp, #96]
 454:	mov	v0.16b, v4.16b
 458:	bl	0 <__multf3>
 45c:	mov	v4.16b, v0.16b
 460:	ldr	q2, [sp, #112]
 464:	b	284 <__divtc3+0x284>
 468:	adrp	x0, 0 <__divtc3>
 46c:	add	x0, x0, #0x0
 470:	and	x27, x25, #0x7fffffffffffffff
 474:	stp	x24, x27, [sp, #96]
 478:	ldr	q1, [x0]
 47c:	ldr	q0, [sp, #96]
 480:	str	q2, [sp, #112]
 484:	str	q4, [sp, #144]
 488:	str	x27, [sp, #160]
 48c:	bl	0 <__unordtf2>
 490:	ldr	q2, [sp, #112]
 494:	ldr	q4, [sp, #144]
 498:	cbnz	w0, 4c4 <__divtc3+0x4c4>
 49c:	adrp	x0, 0 <__divtc3>
 4a0:	add	x0, x0, #0x0
 4a4:	stp	x24, x27, [sp, #96]
 4a8:	ldr	q1, [x0]
 4ac:	ldr	q0, [sp, #96]
 4b0:	bl	0 <__letf2>
 4b4:	cmp	w0, #0x0
 4b8:	ldr	q2, [sp, #112]
 4bc:	ldr	q4, [sp, #144]
 4c0:	b.gt	528 <__divtc3+0x528>
 4c4:	ldr	x0, [sp, #128]
 4c8:	str	q2, [sp, #112]
 4cc:	str	q4, [sp, #144]
 4d0:	and	x1, x0, #0x7fffffffffffffff
 4d4:	adrp	x0, 0 <__divtc3>
 4d8:	add	x0, x0, #0x0
 4dc:	stp	x26, x1, [sp, #96]
 4e0:	ldr	q0, [sp, #96]
 4e4:	ldr	q1, [x0]
 4e8:	str	x1, [sp, #192]
 4ec:	bl	0 <__unordtf2>
 4f0:	ldr	q2, [sp, #112]
 4f4:	ldr	q4, [sp, #144]
 4f8:	cbnz	w0, 5c8 <__divtc3+0x5c8>
 4fc:	adrp	x0, 0 <__divtc3>
 500:	add	x0, x0, #0x0
 504:	ldr	x1, [sp, #192]
 508:	stp	x26, x1, [sp, #96]
 50c:	ldr	q1, [x0]
 510:	ldr	q0, [sp, #96]
 514:	bl	0 <__letf2>
 518:	cmp	w0, #0x0
 51c:	ldr	q2, [sp, #112]
 520:	ldr	q4, [sp, #144]
 524:	b.le	5c8 <__divtc3+0x5c8>
 528:	adrp	x0, 0 <__divtc3>
 52c:	add	x0, x0, #0x0
 530:	and	x1, x19, #0x7fffffffffffffff
 534:	stp	x20, x1, [sp, #96]
 538:	ldr	q1, [x0]
 53c:	mov	w0, #0x1                   	// #1
 540:	ldr	q0, [sp, #96]
 544:	mov	w27, w0
 548:	str	q2, [sp, #112]
 54c:	str	q4, [sp, #144]
 550:	str	x1, [sp, #192]
 554:	bl	0 <__unordtf2>
 558:	ldr	q2, [sp, #112]
 55c:	ldr	q4, [sp, #144]
 560:	cbnz	w0, 594 <__divtc3+0x594>
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	ldr	x1, [sp, #192]
 570:	stp	x20, x1, [sp, #96]
 574:	ldr	q1, [x0]
 578:	ldr	q0, [sp, #96]
 57c:	bl	0 <__letf2>
 580:	cmp	w0, #0x0
 584:	cset	w0, le
 588:	ldr	q2, [sp, #112]
 58c:	mov	w27, w0
 590:	ldr	q4, [sp, #144]
 594:	stp	x20, x19, [sp, #96]
 598:	stp	x20, x19, [sp, #112]
 59c:	ldr	q1, [sp, #96]
 5a0:	ldr	q0, [sp, #112]
 5a4:	str	q2, [sp, #144]
 5a8:	str	q4, [sp, #192]
 5ac:	bl	0 <__unordtf2>
 5b0:	cmp	w0, #0x0
 5b4:	cset	w0, eq  // eq = none
 5b8:	ldr	q2, [sp, #144]
 5bc:	tst	w0, w27
 5c0:	ldr	q4, [sp, #192]
 5c4:	b.ne	8a0 <__divtc3+0x8a0>  // b.any
 5c8:	adrp	x0, 0 <__divtc3>
 5cc:	add	x0, x0, #0x0
 5d0:	stp	x23, x28, [sp, #96]
 5d4:	ldr	q1, [x0]
 5d8:	ldr	q0, [sp, #96]
 5dc:	str	q2, [sp, #112]
 5e0:	str	q4, [sp, #144]
 5e4:	bl	0 <__unordtf2>
 5e8:	ldr	q2, [sp, #112]
 5ec:	ldr	q4, [sp, #144]
 5f0:	cbnz	w0, 284 <__divtc3+0x284>
 5f4:	adrp	x0, 0 <__divtc3>
 5f8:	add	x0, x0, #0x0
 5fc:	stp	x23, x28, [sp, #96]
 600:	ldr	q1, [x0]
 604:	ldr	q0, [sp, #96]
 608:	bl	0 <__letf2>
 60c:	cmp	w0, #0x0
 610:	ldr	q2, [sp, #112]
 614:	ldr	q4, [sp, #144]
 618:	b.le	284 <__divtc3+0x284>
 61c:	adrp	x0, 0 <__divtc3>
 620:	add	x0, x0, #0x0
 624:	ldr	x27, [sp, #160]
 628:	stp	x24, x27, [sp, #96]
 62c:	ldr	q1, [x0]
 630:	ldr	q0, [sp, #96]
 634:	mov	w23, #0x1                   	// #1
 638:	bl	0 <__unordtf2>
 63c:	ldr	q2, [sp, #112]
 640:	ldr	q4, [sp, #144]
 644:	cbnz	w0, 670 <__divtc3+0x670>
 648:	adrp	x0, 0 <__divtc3>
 64c:	add	x0, x0, #0x0
 650:	stp	x24, x27, [sp, #96]
 654:	ldr	q1, [x0]
 658:	ldr	q0, [sp, #96]
 65c:	bl	0 <__letf2>
 660:	cmp	w0, #0x0
 664:	cset	w23, le
 668:	ldr	q2, [sp, #112]
 66c:	ldr	q4, [sp, #144]
 670:	ldr	x0, [sp, #176]
 674:	str	x0, [sp, #96]
 678:	ldr	x0, [sp, #184]
 67c:	str	x0, [sp, #104]
 680:	movi	v1.2d, #0x0
 684:	str	q2, [sp, #144]
 688:	ldr	q0, [sp, #96]
 68c:	str	q4, [sp, #160]
 690:	bl	0 <__gttf2>
 694:	stp	x24, x25, [sp, #96]
 698:	cmp	w0, #0x0
 69c:	stp	x24, x25, [sp, #112]
 6a0:	cset	w0, gt
 6a4:	ldr	q1, [sp, #96]
 6a8:	ldr	q0, [sp, #112]
 6ac:	and	w23, w23, w0
 6b0:	bl	0 <__unordtf2>
 6b4:	cmp	w0, #0x0
 6b8:	cset	w0, eq  // eq = none
 6bc:	ldr	q2, [sp, #144]
 6c0:	tst	w0, w23
 6c4:	ldr	q4, [sp, #160]
 6c8:	b.eq	284 <__divtc3+0x284>  // b.none
 6cc:	ldr	x0, [sp, #128]
 6d0:	str	q2, [sp, #112]
 6d4:	str	q4, [sp, #144]
 6d8:	mov	w28, #0x1                   	// #1
 6dc:	and	x23, x0, #0x7fffffffffffffff
 6e0:	adrp	x0, 0 <__divtc3>
 6e4:	add	x0, x0, #0x0
 6e8:	stp	x26, x23, [sp, #96]
 6ec:	ldr	q0, [sp, #96]
 6f0:	ldr	q1, [x0]
 6f4:	bl	0 <__unordtf2>
 6f8:	ldr	q2, [sp, #112]
 6fc:	ldr	q4, [sp, #144]
 700:	cbnz	w0, 72c <__divtc3+0x72c>
 704:	adrp	x0, 0 <__divtc3>
 708:	add	x0, x0, #0x0
 70c:	stp	x26, x23, [sp, #96]
 710:	ldr	q1, [x0]
 714:	ldr	q0, [sp, #96]
 718:	bl	0 <__letf2>
 71c:	cmp	w0, #0x0
 720:	cset	w28, le
 724:	ldr	q2, [sp, #112]
 728:	ldr	q4, [sp, #144]
 72c:	ldr	x0, [sp, #128]
 730:	stp	x26, x0, [sp, #96]
 734:	stp	x26, x0, [sp, #112]
 738:	ldr	q1, [sp, #96]
 73c:	ldr	q0, [sp, #112]
 740:	str	q2, [sp, #144]
 744:	str	q4, [sp, #160]
 748:	bl	0 <__unordtf2>
 74c:	cmp	w0, #0x0
 750:	cset	w0, eq  // eq = none
 754:	ldr	q2, [sp, #144]
 758:	tst	w0, w28
 75c:	ldr	q4, [sp, #160]
 760:	b.eq	284 <__divtc3+0x284>  // b.none
 764:	adrp	x0, 0 <__divtc3>
 768:	add	x0, x0, #0x0
 76c:	and	x23, x19, #0x7fffffffffffffff
 770:	stp	x20, x23, [sp, #96]
 774:	ldr	q1, [x0]
 778:	ldr	q0, [sp, #96]
 77c:	bl	0 <__unordtf2>
 780:	cbnz	w0, aa0 <__divtc3+0xaa0>
 784:	adrp	x0, 0 <__divtc3>
 788:	add	x0, x0, #0x0
 78c:	stp	x20, x23, [sp, #96]
 790:	ldr	q1, [x0]
 794:	ldr	q0, [sp, #96]
 798:	bl	0 <__letf2>
 79c:	cmp	w0, #0x0
 7a0:	b.le	aa0 <__divtc3+0xaa0>
 7a4:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 7a8:	and	x19, x19, #0x8000000000000000
 7ac:	and	x20, x21, #0x7fffffffffffffff
 7b0:	orr	x19, x0, x19
 7b4:	adrp	x0, 0 <__divtc3>
 7b8:	add	x0, x0, #0x0
 7bc:	stp	x22, x20, [sp, #96]
 7c0:	ldr	q0, [sp, #96]
 7c4:	ldr	q1, [x0]
 7c8:	bl	0 <__unordtf2>
 7cc:	cbnz	w0, a98 <__divtc3+0xa98>
 7d0:	adrp	x0, 0 <__divtc3>
 7d4:	add	x0, x0, #0x0
 7d8:	stp	x22, x20, [sp, #96]
 7dc:	ldr	q1, [x0]
 7e0:	ldr	q0, [sp, #96]
 7e4:	bl	0 <__letf2>
 7e8:	cmp	w0, #0x0
 7ec:	b.le	a98 <__divtc3+0xa98>
 7f0:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 7f4:	stp	xzr, x19, [sp, #96]
 7f8:	and	x21, x21, #0x8000000000000000
 7fc:	orr	x21, x0, x21
 800:	stp	x24, x25, [sp, #112]
 804:	ldr	q1, [sp, #96]
 808:	ldr	q0, [sp, #112]
 80c:	bl	0 <__multf3>
 810:	stp	xzr, x21, [sp, #96]
 814:	ldr	x20, [sp, #128]
 818:	stp	x26, x20, [sp, #112]
 81c:	ldr	q1, [sp, #96]
 820:	str	q0, [sp, #96]
 824:	ldr	q0, [sp, #112]
 828:	bl	0 <__multf3>
 82c:	mov	v1.16b, v0.16b
 830:	ldr	q2, [sp, #96]
 834:	mov	v0.16b, v2.16b
 838:	bl	0 <__addtf3>
 83c:	movi	v1.2d, #0x0
 840:	bl	0 <__multf3>
 844:	stp	xzr, x19, [sp, #96]
 848:	stp	x26, x20, [sp, #112]
 84c:	ldr	q1, [sp, #96]
 850:	str	q0, [sp, #144]
 854:	ldr	q0, [sp, #112]
 858:	bl	0 <__multf3>
 85c:	stp	xzr, x21, [sp, #96]
 860:	stp	x24, x25, [sp, #112]
 864:	ldr	q1, [sp, #96]
 868:	str	q0, [sp, #128]
 86c:	ldr	q0, [sp, #112]
 870:	bl	0 <__multf3>
 874:	mov	v1.16b, v0.16b
 878:	ldr	q4, [sp, #128]
 87c:	mov	v0.16b, v4.16b
 880:	bl	0 <__subtf3>
 884:	movi	v1.2d, #0x0
 888:	ldr	q2, [sp, #144]
 88c:	str	q2, [sp, #96]
 890:	bl	0 <__multf3>
 894:	mov	v4.16b, v0.16b
 898:	ldr	q2, [sp, #96]
 89c:	b	284 <__divtc3+0x284>
 8a0:	adrp	x0, 0 <__divtc3>
 8a4:	add	x0, x0, #0x0
 8a8:	and	x1, x21, #0x7fffffffffffffff
 8ac:	stp	x22, x1, [sp, #96]
 8b0:	ldr	q1, [x0]
 8b4:	mov	w0, #0x1                   	// #1
 8b8:	ldr	q0, [sp, #96]
 8bc:	mov	w27, w0
 8c0:	str	q2, [sp, #112]
 8c4:	str	q4, [sp, #144]
 8c8:	str	x1, [sp, #192]
 8cc:	bl	0 <__unordtf2>
 8d0:	ldr	q2, [sp, #112]
 8d4:	ldr	q4, [sp, #144]
 8d8:	cbnz	w0, 90c <__divtc3+0x90c>
 8dc:	adrp	x0, 0 <__divtc3>
 8e0:	add	x0, x0, #0x0
 8e4:	ldr	x1, [sp, #192]
 8e8:	stp	x22, x1, [sp, #96]
 8ec:	ldr	q1, [x0]
 8f0:	ldr	q0, [sp, #96]
 8f4:	bl	0 <__letf2>
 8f8:	cmp	w0, #0x0
 8fc:	cset	w0, le
 900:	ldr	q2, [sp, #112]
 904:	mov	w27, w0
 908:	ldr	q4, [sp, #144]
 90c:	stp	x22, x21, [sp, #96]
 910:	stp	x22, x21, [sp, #112]
 914:	ldr	q1, [sp, #96]
 918:	ldr	q0, [sp, #112]
 91c:	str	q2, [sp, #144]
 920:	str	q4, [sp, #192]
 924:	bl	0 <__unordtf2>
 928:	cmp	w0, #0x0
 92c:	cset	w0, eq  // eq = none
 930:	ldr	q2, [sp, #144]
 934:	tst	w0, w27
 938:	ldr	q4, [sp, #192]
 93c:	b.eq	5c8 <__divtc3+0x5c8>  // b.none
 940:	adrp	x0, 0 <__divtc3>
 944:	add	x0, x0, #0x0
 948:	ldr	x23, [sp, #160]
 94c:	stp	x24, x23, [sp, #96]
 950:	ldr	q1, [x0]
 954:	ldr	q0, [sp, #96]
 958:	bl	0 <__unordtf2>
 95c:	cbnz	w0, a90 <__divtc3+0xa90>
 960:	adrp	x0, 0 <__divtc3>
 964:	add	x0, x0, #0x0
 968:	stp	x24, x23, [sp, #96]
 96c:	mov	x23, #0x3fff000000000000    	// #4611404543450677248
 970:	ldr	q1, [x0]
 974:	ldr	q0, [sp, #96]
 978:	bl	0 <__letf2>
 97c:	cmp	w0, #0x0
 980:	b.le	a90 <__divtc3+0xa90>
 984:	ldr	x0, [sp, #128]
 988:	and	x25, x25, #0x8000000000000000
 98c:	orr	x23, x23, x25
 990:	and	x24, x0, #0x7fffffffffffffff
 994:	adrp	x0, 0 <__divtc3>
 998:	add	x0, x0, #0x0
 99c:	stp	x26, x24, [sp, #96]
 9a0:	ldr	q0, [sp, #96]
 9a4:	ldr	q1, [x0]
 9a8:	bl	0 <__unordtf2>
 9ac:	cbnz	w0, a88 <__divtc3+0xa88>
 9b0:	adrp	x0, 0 <__divtc3>
 9b4:	add	x0, x0, #0x0
 9b8:	stp	x26, x24, [sp, #96]
 9bc:	mov	x24, #0x3fff000000000000    	// #4611404543450677248
 9c0:	ldr	q1, [x0]
 9c4:	ldr	q0, [sp, #96]
 9c8:	bl	0 <__letf2>
 9cc:	cmp	w0, #0x0
 9d0:	b.le	a88 <__divtc3+0xa88>
 9d4:	stp	xzr, x23, [sp, #96]
 9d8:	stp	x20, x19, [sp, #112]
 9dc:	ldr	q1, [sp, #96]
 9e0:	ldr	x0, [sp, #128]
 9e4:	ldr	q0, [sp, #112]
 9e8:	and	x27, x0, #0x8000000000000000
 9ec:	orr	x24, x24, x27
 9f0:	bl	0 <__multf3>
 9f4:	stp	xzr, x24, [sp, #96]
 9f8:	stp	x22, x21, [sp, #112]
 9fc:	ldr	q1, [sp, #96]
 a00:	str	q0, [sp, #96]
 a04:	ldr	q0, [sp, #112]
 a08:	bl	0 <__multf3>
 a0c:	mov	v1.16b, v0.16b
 a10:	ldr	q2, [sp, #96]
 a14:	mov	v0.16b, v2.16b
 a18:	bl	0 <__addtf3>
 a1c:	adrp	x0, 0 <__divtc3>
 a20:	add	x0, x0, #0x0
 a24:	ldr	q1, [x0]
 a28:	bl	0 <__multf3>
 a2c:	stp	xzr, x24, [sp, #96]
 a30:	stp	x20, x19, [sp, #112]
 a34:	ldr	q1, [sp, #96]
 a38:	str	q0, [sp, #128]
 a3c:	ldr	q0, [sp, #112]
 a40:	bl	0 <__multf3>
 a44:	stp	xzr, x23, [sp, #96]
 a48:	stp	x22, x21, [sp, #112]
 a4c:	ldr	q1, [sp, #96]
 a50:	str	q0, [sp, #96]
 a54:	ldr	q0, [sp, #112]
 a58:	bl	0 <__multf3>
 a5c:	mov	v1.16b, v0.16b
 a60:	ldr	q4, [sp, #96]
 a64:	mov	v0.16b, v4.16b
 a68:	bl	0 <__subtf3>
 a6c:	adrp	x0, 0 <__divtc3>
 a70:	add	x0, x0, #0x0
 a74:	ldr	q1, [x0]
 a78:	bl	0 <__multf3>
 a7c:	mov	v4.16b, v0.16b
 a80:	ldr	q2, [sp, #128]
 a84:	b	284 <__divtc3+0x284>
 a88:	mov	x24, #0x0                   	// #0
 a8c:	b	9d4 <__divtc3+0x9d4>
 a90:	mov	x23, #0x0                   	// #0
 a94:	b	984 <__divtc3+0x984>
 a98:	mov	x0, #0x0                   	// #0
 a9c:	b	7f4 <__divtc3+0x7f4>
 aa0:	mov	x0, #0x0                   	// #0
 aa4:	b	7a8 <__divtc3+0x7a8>

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	asr	x4, x1, #63
   8:	eor	x0, x0, x4
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	asr	x19, x3, #63
  18:	eor	x2, x2, x19
  1c:	eor	x3, x3, x19
  20:	subs	x2, x2, x19
  24:	eor	x1, x1, x4
  28:	sbc	x3, x3, x19
  2c:	eor	x19, x4, x19
  30:	subs	x0, x0, x4
  34:	sbc	x1, x1, x4
  38:	mov	x4, #0x0                   	// #0
  3c:	bl	0 <__udivmodti4>
  40:	eor	x0, x0, x19
  44:	subs	x0, x0, x19
  48:	eor	x1, x1, x19
  4c:	sbc	x1, x1, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #32
  58:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	w6, #0x7ffd                	// #32765
   8:	mov	x4, #0x0                   	// #0
   c:	mov	x29, sp
  10:	str	q0, [sp, #48]
  14:	ldp	x0, x1, [sp, #48]
  18:	str	q1, [sp, #64]
  1c:	ldp	x2, x3, [sp, #64]
  20:	ubfx	x11, x1, #48, #15
  24:	and	x10, x1, #0xffffffffffff
  28:	sub	w7, w11, #0x1
  2c:	eor	x8, x3, x1
  30:	cmp	w7, w6
  34:	and	x5, x8, #0x8000000000000000
  38:	ubfx	x13, x3, #48, #15
  3c:	and	x8, x3, #0xffffffffffff
  40:	mov	x7, x0
  44:	mov	x9, x2
  48:	b.hi	3b0 <__divtf3+0x3b0>  // b.pmore
  4c:	sub	w12, w13, #0x1
  50:	cmp	w12, w6
  54:	b.hi	3b0 <__divtf3+0x3b0>  // b.pmore
  58:	stp	x19, x20, [sp, #16]
  5c:	mov	w19, #0x0                   	// #0
  60:	str	x21, [sp, #32]
  64:	orr	x8, x8, #0x1000000000000
  68:	mov	x17, #0x6484                	// #25732
  6c:	movk	x17, #0xf9de, lsl #16
  70:	sub	w2, w11, w13
  74:	movk	x17, #0xf333, lsl #32
  78:	extr	x6, x8, x9, #49
  7c:	movk	x17, #0x7504, lsl #48
  80:	sub	x12, x17, x6
  84:	add	w2, w2, w19
  88:	and	x30, x6, #0xffffffff
  8c:	lsr	x18, x6, #32
  90:	lsl	w16, w9, #15
  94:	umulh	x3, x12, x6
  98:	ubfx	x0, x9, #17, #32
  9c:	orr	x1, x10, #0x1000000000000
  a0:	ubfx	x14, x7, #30, #32
  a4:	neg	x17, x3
  a8:	lsl	w15, w7, #2
  ac:	mov	x13, #0x1ffffffffffff       	// #562949953421311
  b0:	extr	x10, x1, x7, #62
  b4:	mneg	x3, x3, x12
  b8:	and	x10, x10, #0xffffffff
  bc:	umulh	x17, x17, x12
  c0:	ubfx	x1, x1, #30, #32
  c4:	lsr	x11, x8, #32
  c8:	extr	x17, x17, x3, #63
  cc:	umulh	x12, x17, x6
  d0:	neg	x3, x12
  d4:	mneg	x19, x12, x17
  d8:	and	x12, x8, #0xffffffff
  dc:	umulh	x3, x3, x17
  e0:	extr	x17, x3, x19, #63
  e4:	umulh	x19, x6, x17
  e8:	neg	x3, x19
  ec:	mneg	x19, x19, x17
  f0:	umulh	x3, x3, x17
  f4:	extr	x3, x3, x19, #63
  f8:	umulh	x19, x3, x6
  fc:	neg	x17, x19
 100:	mneg	x19, x19, x3
 104:	umulh	x17, x17, x3
 108:	extr	x3, x17, x19, #63
 10c:	mov	x19, x16
 110:	umulh	x6, x3, x6
 114:	neg	x17, x6
 118:	mneg	x6, x6, x3
 11c:	umulh	x17, x17, x3
 120:	extr	x6, x17, x6, #63
 124:	sub	x6, x6, #0x1
 128:	and	x17, x6, #0xffffffff
 12c:	lsr	x6, x6, #32
 130:	mul	x21, x18, x17
 134:	mul	x3, x30, x6
 138:	mul	x20, x0, x17
 13c:	mul	x16, x16, x6
 140:	adds	x3, x3, x21
 144:	cset	x21, cs  // cs = hs, nlast
 148:	mul	x19, x19, x17
 14c:	adds	x16, x16, x20
 150:	mul	x30, x30, x17
 154:	cset	x20, cs  // cs = hs, nlast
 158:	extr	x21, x21, x3, #32
 15c:	lsl	x3, x3, #32
 160:	extr	x20, x20, x16, #32
 164:	lsl	x16, x16, #32
 168:	madd	x18, x18, x6, x21
 16c:	adds	x16, x16, x19
 170:	madd	x0, x0, x6, x20
 174:	cinc	x0, x0, cs  // cs = hs, nlast
 178:	adds	x3, x3, x30
 17c:	cset	x16, cs  // cs = hs, nlast
 180:	adds	x0, x0, x3
 184:	adc	x16, x18, x16
 188:	negs	x0, x0
 18c:	ngc	x16, x16
 190:	and	x20, x0, #0xffffffff
 194:	and	x18, x16, #0xffffffff
 198:	lsr	x0, x0, #32
 19c:	lsr	x19, x16, #32
 1a0:	mul	x16, x20, x6
 1a4:	mul	x3, x18, x6
 1a8:	mul	x30, x19, x17
 1ac:	mul	x21, x0, x17
 1b0:	adds	x3, x3, x30
 1b4:	mul	x20, x20, x17
 1b8:	cset	x30, cs  // cs = hs, nlast
 1bc:	adds	x16, x16, x21
 1c0:	cset	x21, cs  // cs = hs, nlast
 1c4:	mul	x17, x18, x17
 1c8:	extr	x30, x30, x3, #32
 1cc:	lsl	x3, x3, #32
 1d0:	extr	x18, x21, x16, #32
 1d4:	lsl	x16, x16, #32
 1d8:	adds	x16, x16, x20
 1dc:	madd	x0, x0, x6, x18
 1e0:	madd	x16, x19, x6, x30
 1e4:	cinc	x0, x0, cs  // cs = hs, nlast
 1e8:	subs	x17, x17, #0x2
 1ec:	csetm	x6, cc  // cc = lo, ul, last
 1f0:	adds	x3, x3, x17
 1f4:	adc	x16, x16, x6
 1f8:	adds	x3, x0, x3
 1fc:	cinc	x16, x16, cs  // cs = hs, nlast
 200:	and	x21, x3, #0xffffffff
 204:	and	x17, x16, #0xffffffff
 208:	lsr	x0, x3, #32
 20c:	lsr	x18, x16, #32
 210:	mul	x20, x21, x10
 214:	mul	x3, x17, x15
 218:	mul	x30, x0, x14
 21c:	mul	x19, x17, x14
 220:	adds	x30, x30, x3
 224:	mul	x3, x0, x10
 228:	mul	x6, x18, x15
 22c:	cset	x16, cs  // cs = hs, nlast
 230:	adds	x30, x30, x20
 234:	mul	x20, x1, x21
 238:	cinc	x16, x16, cs  // cs = hs, nlast
 23c:	adds	x3, x3, x19
 240:	mul	x15, x0, x15
 244:	cset	x19, cs  // cs = hs, nlast
 248:	adds	x20, x20, x6
 24c:	mul	x6, x21, x14
 250:	cset	x21, cs  // cs = hs, nlast
 254:	adds	x3, x3, x20
 258:	adc	x19, x19, x21
 25c:	adds	x6, x6, x15
 260:	cset	x15, cs  // cs = hs, nlast
 264:	mul	x0, x1, x0
 268:	mul	x14, x18, x14
 26c:	extr	x19, x19, x3, #32
 270:	lsl	x3, x3, #32
 274:	extr	x15, x15, x6, #32
 278:	adds	x15, x15, x30
 27c:	mul	x30, x1, x17
 280:	cset	x6, cs  // cs = hs, nlast
 284:	adds	x3, x15, x3
 288:	mul	x15, x18, x10
 28c:	cinc	x3, x6, cs  // cs = hs, nlast
 290:	mul	x10, x17, x10
 294:	and	x6, x9, #0xffffffff
 298:	adds	x30, x30, x15
 29c:	mul	x1, x1, x18
 2a0:	cset	x17, cs  // cs = hs, nlast
 2a4:	lsr	x15, x9, #32
 2a8:	lsl	x18, x30, #32
 2ac:	adds	x3, x3, x18
 2b0:	extr	x30, x17, x30, #32
 2b4:	cinc	x30, x30, cs  // cs = hs, nlast
 2b8:	adds	x0, x0, x10
 2bc:	cset	x10, cs  // cs = hs, nlast
 2c0:	adds	x0, x14, x0
 2c4:	adc	x1, x1, x10
 2c8:	adds	x16, x16, x19
 2cc:	cset	x10, cs  // cs = hs, nlast
 2d0:	adds	x0, x0, x16
 2d4:	adc	x1, x1, x10
 2d8:	adds	x3, x3, x0
 2dc:	adc	x1, x30, x1
 2e0:	cmp	x1, x13
 2e4:	b.hi	41c <__divtf3+0x41c>  // b.pmore
 2e8:	and	x10, x3, #0xffffffff
 2ec:	lsr	x16, x3, #32
 2f0:	lsr	x13, x1, #32
 2f4:	and	x17, x1, #0xffffffff
 2f8:	lsl	x7, x7, #49
 2fc:	sub	w2, w2, #0x1
 300:	mul	x0, x16, x6
 304:	mul	x14, x10, x15
 308:	mul	x19, x16, x12
 30c:	adds	x14, x0, x14
 310:	mul	x16, x16, x15
 314:	mul	x0, x10, x6
 318:	cset	x18, cs  // cs = hs, nlast
 31c:	mul	x11, x10, x11
 320:	madd	x11, x6, x13, x11
 324:	negs	x0, x0
 328:	madd	x16, x17, x6, x16
 32c:	sbc	x7, x7, xzr
 330:	madd	x15, x17, x15, x19
 334:	extr	x18, x18, x14, #32
 338:	madd	x12, x10, x12, x16
 33c:	lsl	x13, x14, #32
 340:	adds	x11, x11, x15
 344:	mov	w6, #0x3fff                	// #16383
 348:	adds	x12, x12, x18
 34c:	add	w2, w2, w6
 350:	subs	x0, x0, x13
 354:	add	x11, x12, x11, lsl #32
 358:	sbc	x7, x7, x11
 35c:	mov	w6, #0x7ffe                	// #32766
 360:	cmp	w2, w6
 364:	b.gt	4a4 <__divtf3+0x4a4>
 368:	cmp	w2, #0x0
 36c:	b.gt	514 <__divtf3+0x514>
 370:	b.ne	39c <__divtf3+0x39c>  // b.any
 374:	and	x1, x1, #0xffffffffffff
 378:	extr	x7, x7, x0, #63
 37c:	cmp	x7, x8
 380:	lsl	x0, x0, #1
 384:	mov	x6, #0x1                   	// #1
 388:	b.ls	634 <__divtf3+0x634>  // b.plast
 38c:	adds	x3, x6, x3
 390:	cinc	x1, x1, cs  // cs = hs, nlast
 394:	tst	x1, #0x1000000000000
 398:	b.ne	640 <__divtf3+0x640>  // b.any
 39c:	fmov	d0, x4
 3a0:	ldp	x19, x20, [sp, #16]
 3a4:	fmov	v0.d[1], x5
 3a8:	ldr	x21, [sp, #32]
 3ac:	b	414 <__divtf3+0x414>
 3b0:	and	x6, x1, #0x7fffffffffffffff
 3b4:	mov	x12, #0x7fff000000000000    	// #9223090561878065152
 3b8:	cmp	x6, x12
 3bc:	b.hi	408 <__divtf3+0x408>  // b.pmore
 3c0:	b.eq	400 <__divtf3+0x400>  // b.none
 3c4:	and	x12, x3, #0x7fffffffffffffff
 3c8:	mov	x14, #0x7fff000000000000    	// #9223090561878065152
 3cc:	cmp	x12, x14
 3d0:	b.hi	4e8 <__divtf3+0x4e8>  // b.pmore
 3d4:	b.eq	4e4 <__divtf3+0x4e4>  // b.none
 3d8:	cbz	x0, 4c0 <__divtf3+0x4c0>
 3dc:	cbz	x2, 4fc <__divtf3+0x4fc>
 3e0:	orr	x6, x0, x6
 3e4:	cbnz	x6, 558 <__divtf3+0x558>
 3e8:	orr	x2, x2, x12
 3ec:	cbnz	x2, 508 <__divtf3+0x508>
 3f0:	adrp	x0, 0 <__divtf3>
 3f4:	add	x0, x0, #0x0
 3f8:	ldr	q0, [x0]
 3fc:	b	414 <__divtf3+0x414>
 400:	cbz	x0, 3c4 <__divtf3+0x3c4>
 404:	nop
 408:	fmov	d0, x0
 40c:	orr	x3, x1, #0x800000000000
 410:	fmov	v0.d[1], x3
 414:	ldp	x29, x30, [sp], #80
 418:	ret
 41c:	ubfx	x10, x3, #1, #32
 420:	extr	x0, x1, x3, #33
 424:	and	x0, x0, #0xffffffff
 428:	ubfx	x16, x1, #1, #32
 42c:	lsr	x13, x1, #33
 430:	lsl	x7, x7, #48
 434:	mul	x17, x10, x15
 438:	extr	x3, x1, x3, #1
 43c:	mul	x14, x0, x6
 440:	lsr	x1, x1, #1
 444:	mul	x20, x0, x12
 448:	adds	x17, x14, x17
 44c:	mul	x11, x10, x11
 450:	mul	x14, x0, x15
 454:	cset	x19, cs  // cs = hs, nlast
 458:	mul	x0, x10, x6
 45c:	lsl	x18, x17, #32
 460:	madd	x11, x6, x13, x11
 464:	extr	x17, x19, x17, #32
 468:	madd	x6, x16, x6, x14
 46c:	negs	x0, x0
 470:	madd	x15, x16, x15, x20
 474:	sbc	x7, x7, xzr
 478:	madd	x12, x10, x12, x6
 47c:	mov	w6, #0x3fff                	// #16383
 480:	adds	x15, x11, x15
 484:	add	w2, w2, w6
 488:	adds	x12, x12, x17
 48c:	mov	w6, #0x7ffe                	// #32766
 490:	subs	x0, x0, x18
 494:	add	x12, x12, x15, lsl #32
 498:	sbc	x7, x7, x12
 49c:	cmp	w2, w6
 4a0:	b.le	368 <__divtf3+0x368>
 4a4:	fmov	d0, x4
 4a8:	orr	x1, x5, #0x7fff000000000000
 4ac:	ldp	x19, x20, [sp, #16]
 4b0:	fmov	v0.d[1], x1
 4b4:	ldr	x21, [sp, #32]
 4b8:	ldp	x29, x30, [sp], #80
 4bc:	ret
 4c0:	mov	x14, #0x7fff000000000000    	// #9223090561878065152
 4c4:	cmp	x6, x14
 4c8:	b.ne	3dc <__divtf3+0x3dc>  // b.any
 4cc:	cbz	x2, 664 <__divtf3+0x664>
 4d0:	fmov	d0, x4
 4d4:	orr	x1, x5, #0x7fff000000000000
 4d8:	ldp	x29, x30, [sp], #80
 4dc:	fmov	v0.d[1], x1
 4e0:	ret
 4e4:	cbz	x2, 3d8 <__divtf3+0x3d8>
 4e8:	fmov	d0, x2
 4ec:	orr	x1, x3, #0x800000000000
 4f0:	ldp	x29, x30, [sp], #80
 4f4:	fmov	v0.d[1], x1
 4f8:	ret
 4fc:	mov	x14, #0x7fff000000000000    	// #9223090561878065152
 500:	cmp	x12, x14
 504:	b.ne	3e0 <__divtf3+0x3e0>  // b.any
 508:	fmov	d0, x4
 50c:	fmov	v0.d[1], x5
 510:	b	414 <__divtf3+0x414>
 514:	bfi	x1, x2, #48, #16
 518:	extr	x7, x7, x0, #63
 51c:	mov	x2, x1
 520:	cmp	x8, x7
 524:	lsl	x0, x0, #1
 528:	mov	x1, #0x1                   	// #1
 52c:	b.ls	624 <__divtf3+0x624>  // b.plast
 530:	mov	x1, #0x0                   	// #0
 534:	adds	x3, x1, x3
 538:	orr	x0, x3, x4
 53c:	fmov	d0, x0
 540:	cinc	x2, x2, cs  // cs = hs, nlast
 544:	orr	x1, x2, x5
 548:	fmov	v0.d[1], x1
 54c:	ldp	x19, x20, [sp, #16]
 550:	ldr	x21, [sp, #32]
 554:	b	414 <__divtf3+0x414>
 558:	orr	x12, x2, x12
 55c:	cbz	x12, 4d0 <__divtf3+0x4d0>
 560:	stp	x19, x20, [sp, #16]
 564:	tst	x1, #0x7fff000000000000
 568:	mov	w19, #0x0                   	// #0
 56c:	str	x21, [sp, #32]
 570:	b.ne	5c8 <__divtf3+0x5c8>  // b.any
 574:	cmp	x10, #0x0
 578:	mov	w6, #0x40                  	// #64
 57c:	csel	x1, x10, x0, ne  // ne = any
 580:	csel	w6, wzr, w6, ne  // ne = any
 584:	clz	x1, x1
 588:	lsr	x7, x0, #1
 58c:	add	w6, w6, w1
 590:	mov	w14, #0x3f                  	// #63
 594:	sub	w12, w6, #0xf
 598:	subs	w6, w6, #0x4f
 59c:	sub	w14, w14, w12
 5a0:	mov	w19, #0x1                   	// #1
 5a4:	lsl	x1, x10, x12
 5a8:	sub	w19, w19, w12
 5ac:	lsr	x7, x7, x14
 5b0:	orr	x1, x7, x1
 5b4:	lsl	x7, x0, x6
 5b8:	csel	x1, x7, x1, pl  // pl = nfrst
 5bc:	lsl	x0, x0, x12
 5c0:	mov	x10, x1
 5c4:	csel	x7, xzr, x0, pl  // pl = nfrst
 5c8:	tst	x3, #0x7fff000000000000
 5cc:	b.ne	64 <__divtf3+0x64>  // b.any
 5d0:	cmp	x8, #0x0
 5d4:	mov	w0, #0x40                  	// #64
 5d8:	csel	x1, x8, x2, ne  // ne = any
 5dc:	csel	w0, wzr, w0, ne  // ne = any
 5e0:	clz	x1, x1
 5e4:	mov	w3, #0x3f                  	// #63
 5e8:	add	w0, w0, w1
 5ec:	lsr	x1, x2, #1
 5f0:	sub	w9, w0, #0xf
 5f4:	subs	w0, w0, #0x4f
 5f8:	sub	w3, w3, w9
 5fc:	add	w19, w19, w9
 600:	lsl	x8, x8, x9
 604:	sub	w19, w19, #0x1
 608:	lsr	x1, x1, x3
 60c:	orr	x8, x1, x8
 610:	lsl	x9, x2, x9
 614:	csel	x9, xzr, x9, pl  // pl = nfrst
 618:	lsl	x1, x2, x0
 61c:	csel	x8, x1, x8, pl  // pl = nfrst
 620:	b	64 <__divtf3+0x64>
 624:	b.ne	534 <__divtf3+0x534>  // b.any
 628:	cmp	x9, x0
 62c:	b.ls	534 <__divtf3+0x534>  // b.plast
 630:	b	530 <__divtf3+0x530>
 634:	b.eq	658 <__divtf3+0x658>  // b.none
 638:	mov	x6, #0x0                   	// #0
 63c:	b	38c <__divtf3+0x38c>
 640:	fmov	d0, x4
 644:	orr	x1, x5, #0x1000000000000
 648:	ldp	x19, x20, [sp, #16]
 64c:	fmov	v0.d[1], x1
 650:	ldr	x21, [sp, #32]
 654:	b	414 <__divtf3+0x414>
 658:	cmp	x0, x9
 65c:	b.hi	38c <__divtf3+0x38c>  // b.pmore
 660:	b	638 <__divtf3+0x638>
 664:	cmp	x12, x6
 668:	b.ne	4d0 <__divtf3+0x4d0>  // b.any
 66c:	b	3f0 <__divtf3+0x3f0>

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	fmov	w0, s0
   4:	mov	w2, #0x7effffff            	// #2130706431
   8:	and	w1, w0, #0x7fffffff
   c:	and	w0, w0, #0x80000000
  10:	sub	w3, w1, #0x800, lsl #12
  14:	cmp	w3, w2
  18:	b.hi	34 <__extendsfdf2+0x34>  // b.pmore
  1c:	ubfiz	x1, x1, #29, #31
  20:	mov	x2, #0x3800000000000000    	// #4035225266123964416
  24:	add	x2, x1, x2
  28:	orr	x0, x2, x0, lsl #32
  2c:	fmov	d0, x0
  30:	ret
  34:	mov	w2, #0x7f7fffff            	// #2139095039
  38:	cmp	w1, w2
  3c:	b.ls	54 <__extendsfdf2+0x54>  // b.plast
  40:	ubfiz	x1, x1, #29, #23
  44:	orr	x2, x1, #0x7ff0000000000000
  48:	orr	x0, x2, x0, lsl #32
  4c:	fmov	d0, x0
  50:	ret
  54:	mov	x2, #0x0                   	// #0
  58:	cbz	w1, 28 <__extendsfdf2+0x28>
  5c:	clz	w3, w1
  60:	mov	w2, #0x389                 	// #905
  64:	add	w4, w3, #0x15
  68:	sub	w2, w2, w3
  6c:	lsl	x1, x1, x4
  70:	eor	x1, x1, #0x10000000000000
  74:	orr	x2, x1, x2, lsl #52
  78:	orr	x0, x2, x0, lsl #32
  7c:	fmov	d0, x0
  80:	ret

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	and	w1, w0, #0x7fff
   4:	mov	w2, #0x77ff                	// #30719
   8:	sub	w3, w1, #0x400
   c:	and	w0, w0, #0x8000
  10:	cmp	w2, w3, uxth
  14:	b.cc	2c <__extendhfsf2+0x2c>  // b.lo, b.ul, b.last
  18:	mov	w2, #0x38000000            	// #939524096
  1c:	add	w2, w2, w1, lsl #13
  20:	orr	w1, w2, w0, lsl #16
  24:	fmov	s0, w1
  28:	ret
  2c:	mov	w2, #0x7bff                	// #31743
  30:	cmp	w1, w2
  34:	b.ls	4c <__extendhfsf2+0x4c>  // b.plast
  38:	ubfiz	w1, w1, #13, #10
  3c:	orr	w2, w1, #0x7f800000
  40:	orr	w1, w2, w0, lsl #16
  44:	fmov	s0, w1
  48:	ret
  4c:	mov	w2, #0x0                   	// #0
  50:	cbz	w1, 20 <__extendhfsf2+0x20>
  54:	clz	w3, w1
  58:	mov	w2, #0x86                  	// #134
  5c:	sub	w4, w3, #0x8
  60:	sub	w3, w2, w3
  64:	lsl	w2, w1, w4
  68:	eor	w2, w2, #0x800000
  6c:	orr	w2, w2, w3, lsl #23
  70:	orr	w1, w2, w0, lsl #16
  74:	fmov	s0, w1
  78:	ret
  7c:	nop

0000000000000080 <__gnu_h2f_ieee>:
  80:	b	0 <__extendhfsf2>

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbnz	w0, 20 <__ffsdi2+0x20>
   4:	asr	x0, x0, #32
   8:	rbit	w1, w0
   c:	cmp	w0, #0x0
  10:	clz	w1, w1
  14:	add	w1, w1, #0x21
  18:	csel	w0, w0, w1, eq  // eq = none
  1c:	ret
  20:	rbit	w0, w0
  24:	clz	w0, w0
  28:	add	w0, w0, #0x1
  2c:	ret

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	rbit	w1, w0
   4:	cmp	w0, #0x0
   8:	clz	w1, w1
   c:	csinc	w0, wzr, w1, eq  // eq = none
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbnz	x0, 1c <__ffsti2+0x1c>
   4:	cmp	x1, #0x0
   8:	rbit	x1, x1
   c:	clz	x1, x1
  10:	add	w0, w1, #0x41
  14:	csel	w0, w0, wzr, ne  // ne = any
  18:	ret
  1c:	rbit	x0, x0
  20:	clz	x0, x0
  24:	add	w0, w0, #0x1
  28:	ret

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	fcmpe	d0, #0.0
   4:	b.mi	c <__fixdfdi+0xc>  // b.first
   8:	b	0 <__fixunsdfdi>
   c:	fneg	d0, d0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunsdfdi>
  1c:	neg	x0, x0
  20:	ldp	x29, x30, [sp], #16
  24:	ret

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	fmov	x1, d0
   4:	tbnz	x1, #63, 5c <__fixdfsi+0x5c>
   8:	mov	w4, #0x1                   	// #1
   c:	mov	w5, w4
  10:	ubfx	x2, x1, #52, #11
  14:	mov	w0, #0x0                   	// #0
  18:	subs	w3, w2, #0x3ff
  1c:	b.mi	48 <__fixdfsi+0x48>  // b.first
  20:	cmp	w3, #0x1f
  24:	b.hi	4c <__fixdfsi+0x4c>  // b.pmore
  28:	and	x1, x1, #0xfffffffffffff
  2c:	cmp	w3, #0x33
  30:	orr	x1, x1, #0x10000000000000
  34:	b.gt	68 <__fixdfsi+0x68>
  38:	mov	w0, #0x34                  	// #52
  3c:	sub	w3, w0, w3
  40:	lsr	x1, x1, x3
  44:	mul	w0, w4, w1
  48:	ret
  4c:	cmp	w5, #0x1
  50:	mov	w1, #0x80000000            	// #-2147483648
  54:	cinv	w0, w1, eq  // eq = none
  58:	ret
  5c:	mov	w4, #0xffffffff            	// #-1
  60:	mov	w5, w4
  64:	b	10 <__fixdfsi+0x10>
  68:	sub	w2, w2, #0x433
  6c:	lsl	w1, w1, w2
  70:	mul	w0, w1, w5
  74:	ret

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fmov	x2, d0
   4:	tbnz	x2, #63, 60 <__fixdfti+0x60>
   8:	lsr	x3, x2, #52
   c:	mov	w4, w3
  10:	subs	w3, w3, #0x3ff
  14:	b.mi	54 <__fixdfti+0x54>  // b.first
  18:	cmp	w3, #0x7f
  1c:	b.hi	dc <__fixdfti+0xdc>  // b.pmore
  20:	and	x2, x2, #0xfffffffffffff
  24:	cmp	w3, #0x33
  28:	orr	x2, x2, #0x10000000000000
  2c:	mov	x0, #0x1                   	// #1
  30:	mov	x1, #0x0                   	// #0
  34:	b.gt	90 <__fixdfti+0x90>
  38:	mov	w4, #0x34                  	// #52
  3c:	sub	w3, w4, w3
  40:	lsr	x2, x2, x3
  44:	umulh	x3, x2, x0
  48:	madd	x1, x2, x1, x3
  4c:	mul	x0, x2, x0
  50:	ret
  54:	mov	x0, #0x0                   	// #0
  58:	mov	x1, #0x0                   	// #0
  5c:	ret
  60:	ubfx	x3, x2, #52, #11
  64:	mov	w4, w3
  68:	subs	w3, w3, #0x3ff
  6c:	b.mi	54 <__fixdfti+0x54>  // b.first
  70:	cmp	w3, #0x7f
  74:	b.hi	cc <__fixdfti+0xcc>  // b.pmore
  78:	and	x2, x2, #0xfffffffffffff
  7c:	mov	x0, #0xffffffffffffffff    	// #-1
  80:	cmp	w3, #0x33
  84:	mov	x1, x0
  88:	orr	x2, x2, #0x10000000000000
  8c:	b.le	38 <__fixdfti+0x38>
  90:	sub	w7, w4, #0x433
  94:	subs	w4, w4, #0x473
  98:	lsr	x3, x2, #1
  9c:	mov	w6, #0x3f                  	// #63
  a0:	sub	w6, w6, w7
  a4:	lsl	x5, x2, x4
  a8:	lsl	x2, x2, x7
  ac:	csel	x2, xzr, x2, pl  // pl = nfrst
  b0:	lsr	x3, x3, x6
  b4:	csel	x5, x5, x3, pl  // pl = nfrst
  b8:	umulh	x3, x2, x0
  bc:	madd	x3, x5, x0, x3
  c0:	madd	x1, x2, x1, x3
  c4:	mul	x0, x2, x0
  c8:	ret
  cc:	adrp	x1, 0 <__fixdfti>
  d0:	mov	x0, #0x0                   	// #0
  d4:	ldr	x1, [x1]
  d8:	ret
  dc:	adrp	x1, 10 <__fixdfti+0x10>
  e0:	mov	x0, #0xffffffffffffffff    	// #-1
  e4:	ldr	x1, [x1]
  e8:	ret

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	fcmpe	s0, #0.0
   4:	b.mi	c <__fixsfdi+0xc>  // b.first
   8:	b	0 <__fixunssfdi>
   c:	fneg	s0, s0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunssfdi>
  1c:	neg	x0, x0
  20:	ldp	x29, x30, [sp], #16
  24:	ret

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	fmov	w1, s0
   4:	tbnz	w1, #31, 5c <__fixsfsi+0x5c>
   8:	mov	w4, #0x1                   	// #1
   c:	mov	w5, w4
  10:	ubfx	x2, x1, #23, #8
  14:	mov	w0, #0x0                   	// #0
  18:	subs	w3, w2, #0x7f
  1c:	b.mi	48 <__fixsfsi+0x48>  // b.first
  20:	cmp	w3, #0x1f
  24:	b.hi	4c <__fixsfsi+0x4c>  // b.pmore
  28:	and	w1, w1, #0x7fffff
  2c:	cmp	w3, #0x16
  30:	orr	w1, w1, #0x800000
  34:	b.gt	68 <__fixsfsi+0x68>
  38:	mov	w0, #0x17                  	// #23
  3c:	sub	w3, w0, w3
  40:	lsr	w1, w1, w3
  44:	mul	w0, w1, w4
  48:	ret
  4c:	cmp	w5, #0x1
  50:	mov	w1, #0x80000000            	// #-2147483648
  54:	cinv	w0, w1, eq  // eq = none
  58:	ret
  5c:	mov	w4, #0xffffffff            	// #-1
  60:	mov	w5, w4
  64:	b	10 <__fixsfsi+0x10>
  68:	sub	w2, w2, #0x96
  6c:	lsl	w1, w1, w2
  70:	mul	w0, w1, w5
  74:	ret

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fmov	w2, s0
   4:	tbnz	w2, #31, 5c <__fixsfti+0x5c>
   8:	lsr	w4, w2, #23
   c:	subs	w3, w4, #0x7f
  10:	b.mi	50 <__fixsfti+0x50>  // b.first
  14:	cmp	w3, #0x80
  18:	b.eq	c4 <__fixsfti+0xc4>  // b.none
  1c:	and	w2, w2, #0x7fffff
  20:	cmp	w3, #0x16
  24:	orr	w2, w2, #0x800000
  28:	mov	x0, #0x1                   	// #1
  2c:	mov	x1, #0x0                   	// #0
  30:	b.gt	88 <__fixsfti+0x88>
  34:	mov	w4, #0x17                  	// #23
  38:	sub	w3, w4, w3
  3c:	lsr	w3, w2, w3
  40:	umulh	x2, x3, x0
  44:	madd	x1, x3, x1, x2
  48:	mul	x0, x3, x0
  4c:	ret
  50:	mov	x0, #0x0                   	// #0
  54:	mov	x1, #0x0                   	// #0
  58:	ret
  5c:	ubfx	x4, x2, #23, #8
  60:	subs	w3, w4, #0x7f
  64:	b.mi	50 <__fixsfti+0x50>  // b.first
  68:	cmp	w3, #0x80
  6c:	b.eq	d4 <__fixsfti+0xd4>  // b.none
  70:	and	w2, w2, #0x7fffff
  74:	mov	x0, #0xffffffffffffffff    	// #-1
  78:	cmp	w3, #0x16
  7c:	mov	x1, x0
  80:	orr	w2, w2, #0x800000
  84:	b.le	34 <__fixsfti+0x34>
  88:	sub	w7, w4, #0x96
  8c:	subs	w4, w4, #0xd6
  90:	lsr	x3, x2, #1
  94:	mov	w6, #0x3f                  	// #63
  98:	sub	w6, w6, w7
  9c:	lsl	x5, x2, x4
  a0:	lsl	x2, x2, x7
  a4:	csel	x2, xzr, x2, pl  // pl = nfrst
  a8:	lsr	x3, x3, x6
  ac:	csel	x5, x5, x3, pl  // pl = nfrst
  b0:	umulh	x3, x2, x0
  b4:	madd	x3, x5, x0, x3
  b8:	madd	x1, x2, x1, x3
  bc:	mul	x0, x2, x0
  c0:	ret
  c4:	adrp	x1, 10 <__fixsfti+0x10>
  c8:	mov	x0, #0xffffffffffffffff    	// #-1
  cc:	ldr	x1, [x1]
  d0:	ret
  d4:	adrp	x1, 0 <__fixsfti>
  d8:	mov	x0, #0x0                   	// #0
  dc:	ldr	x1, [x1]
  e0:	ret

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	fcmpe	d0, #0.0
   4:	mov	x0, #0x0                   	// #0
   8:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   c:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
  10:	fmov	d1, x0
  14:	mov	x0, #0x41f0000000000000    	// #4751297606875873280
  18:	fmov	d2, x0
  1c:	fmul	d1, d0, d1
  20:	fcvtzu	w0, d1
  24:	ucvtf	d1, w0
  28:	fmul	d1, d1, d2
  2c:	fsub	d0, d0, d1
  30:	fcvtzu	w1, d0
  34:	orr	x0, x1, x0, lsl #32
  38:	ret

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	fmov	x1, d0
   4:	mov	w0, #0x0                   	// #0
   8:	tbnz	x1, #63, 40 <__fixunsdfsi+0x40>
   c:	lsr	x2, x1, #52
  10:	and	x1, x1, #0xfffffffffffff
  14:	orr	x1, x1, #0x10000000000000
  18:	subs	w3, w2, #0x3ff
  1c:	b.mi	40 <__fixunsdfsi+0x40>  // b.first
  20:	cmp	w3, #0x1f
  24:	mov	w0, #0xffffffff            	// #-1
  28:	b.hi	40 <__fixunsdfsi+0x40>  // b.pmore
  2c:	cmp	w3, #0x33
  30:	b.gt	44 <__fixunsdfsi+0x44>
  34:	mov	w0, #0x34                  	// #52
  38:	sub	w0, w0, w3
  3c:	lsr	x0, x1, x0
  40:	ret
  44:	sub	w0, w2, #0x433
  48:	lsl	w0, w1, w0
  4c:	ret

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	fmov	x0, d0
   4:	tbnz	x0, #63, 6c <__fixunsdfti+0x6c>
   8:	lsr	x1, x0, #52
   c:	and	x0, x0, #0xfffffffffffff
  10:	orr	x0, x0, #0x10000000000000
  14:	subs	w2, w1, #0x3ff
  18:	b.mi	6c <__fixunsdfti+0x6c>  // b.first
  1c:	cmp	w2, #0x7f
  20:	b.hi	78 <__fixunsdfti+0x78>  // b.pmore
  24:	cmp	w2, #0x33
  28:	b.gt	40 <__fixunsdfti+0x40>
  2c:	mov	w1, #0x34                  	// #52
  30:	sub	w2, w1, w2
  34:	mov	x1, #0x0                   	// #0
  38:	lsr	x0, x0, x2
  3c:	ret
  40:	sub	w5, w1, #0x433
  44:	subs	w2, w1, #0x473
  48:	lsr	x3, x0, #1
  4c:	mov	w4, #0x3f                  	// #63
  50:	sub	w4, w4, w5
  54:	lsl	x1, x0, x2
  58:	lsl	x0, x0, x5
  5c:	csel	x0, xzr, x0, pl  // pl = nfrst
  60:	lsr	x3, x3, x4
  64:	csel	x1, x1, x3, pl  // pl = nfrst
  68:	ret
  6c:	mov	x0, #0x0                   	// #0
  70:	mov	x1, #0x0                   	// #0
  74:	ret
  78:	mov	x0, #0xffffffffffffffff    	// #-1
  7c:	mov	x1, x0
  80:	ret

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	fcmpe	s0, #0.0
   4:	mov	x0, #0x0                   	// #0
   8:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   c:	fcvt	d0, s0
  10:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
  14:	fmov	d1, x0
  18:	mov	x0, #0x41f0000000000000    	// #4751297606875873280
  1c:	fmov	d2, x0
  20:	fmul	d1, d0, d1
  24:	fcvtzu	w1, d1
  28:	ucvtf	d1, w1
  2c:	fmul	d1, d1, d2
  30:	fsub	d0, d0, d1
  34:	fcvtzu	w0, d0
  38:	orr	x0, x0, x1, lsl #32
  3c:	ret

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	fmov	w1, s0
   4:	mov	w0, #0x0                   	// #0
   8:	tbnz	w1, #31, 40 <__fixunssfsi+0x40>
   c:	lsr	w2, w1, #23
  10:	and	w1, w1, #0x7fffff
  14:	orr	w1, w1, #0x800000
  18:	subs	w3, w2, #0x7f
  1c:	b.mi	40 <__fixunssfsi+0x40>  // b.first
  20:	cmp	w3, #0x1f
  24:	mov	w0, #0xffffffff            	// #-1
  28:	b.hi	40 <__fixunssfsi+0x40>  // b.pmore
  2c:	cmp	w3, #0x16
  30:	b.gt	44 <__fixunssfsi+0x44>
  34:	mov	w0, #0x17                  	// #23
  38:	sub	w0, w0, w3
  3c:	lsr	w0, w1, w0
  40:	ret
  44:	sub	w0, w2, #0x96
  48:	lsl	w0, w1, w0
  4c:	ret

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fmov	w0, s0
   4:	tbnz	w0, #31, 6c <__fixunssfti+0x6c>
   8:	lsr	w1, w0, #23
   c:	and	w0, w0, #0x7fffff
  10:	orr	w0, w0, #0x800000
  14:	subs	w2, w1, #0x7f
  18:	b.mi	6c <__fixunssfti+0x6c>  // b.first
  1c:	cmp	w2, #0x80
  20:	b.eq	78 <__fixunssfti+0x78>  // b.none
  24:	cmp	w2, #0x16
  28:	b.gt	40 <__fixunssfti+0x40>
  2c:	mov	w1, #0x17                  	// #23
  30:	sub	w2, w1, w2
  34:	mov	x1, #0x0                   	// #0
  38:	lsr	w0, w0, w2
  3c:	ret
  40:	sub	w5, w1, #0x96
  44:	subs	w2, w1, #0xd6
  48:	lsr	x3, x0, #1
  4c:	mov	w4, #0x3f                  	// #63
  50:	sub	w4, w4, w5
  54:	lsl	x1, x0, x2
  58:	lsl	x0, x0, x5
  5c:	csel	x0, xzr, x0, pl  // pl = nfrst
  60:	lsr	x3, x3, x4
  64:	csel	x1, x1, x3, pl  // pl = nfrst
  68:	ret
  6c:	mov	x0, #0x0                   	// #0
  70:	mov	x1, #0x0                   	// #0
  74:	ret
  78:	mov	x0, #0xffffffffffffffff    	// #-1
  7c:	mov	x1, x0
  80:	ret

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	mov	x1, #0x41f0000000000000    	// #4751297606875873280
   4:	fmov	d2, x1
   8:	mov	x1, #0x4330000000000000    	// #4841369599423283200
   c:	fmov	d1, x1
  10:	asr	x1, x0, #32
  14:	and	x0, x0, #0xffffffff
  18:	mov	x2, #0x4330000000000000    	// #4841369599423283200
  1c:	orr	x0, x0, x2
  20:	scvtf	d0, w1
  24:	fmul	d0, d0, d2
  28:	fsub	d0, d0, d1
  2c:	fmov	d1, x0
  30:	fadd	d0, d0, d1
  34:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	x0, 90 <__floatdisf+0x90>
   8:	asr	x2, x0, #63
   c:	mov	w3, #0x40                  	// #64
  10:	eor	x0, x0, x2
  14:	sub	x0, x0, x2
  18:	clz	x4, x0
  1c:	sub	w3, w3, w4
  20:	cmp	w3, #0x18
  24:	sub	w1, w3, #0x1
  28:	b.ls	94 <__floatdisf+0x94>  // b.plast
  2c:	cmp	w3, #0x19
  30:	b.eq	bc <__floatdisf+0xbc>  // b.none
  34:	cmp	w3, #0x1a
  38:	b.eq	5c <__floatdisf+0x5c>  // b.none
  3c:	add	w4, w4, #0x1a
  40:	mov	x6, #0xffffffffffffffff    	// #-1
  44:	sub	w5, w3, #0x1a
  48:	lsr	x4, x6, x4
  4c:	tst	x4, x0
  50:	cset	x6, ne  // ne = any
  54:	lsr	x0, x0, x5
  58:	orr	x0, x6, x0
  5c:	ubfx	x4, x0, #2, #1
  60:	orr	x0, x4, x0
  64:	add	x4, x0, #0x1
  68:	asr	x0, x4, #2
  6c:	tbz	w4, #26, a0 <__floatdisf+0xa0>
  70:	asr	x0, x4, #3
  74:	mov	w1, w3
  78:	and	w2, w2, #0x80000000
  7c:	and	w0, w0, #0x7fffff
  80:	add	w1, w1, #0x7f
  84:	orr	w0, w2, w0
  88:	orr	w0, w0, w1, lsl #23
  8c:	fmov	s0, w0
  90:	ret
  94:	mov	w4, #0x18                  	// #24
  98:	sub	w3, w4, w3
  9c:	lsl	x0, x0, x3
  a0:	and	w2, w2, #0x80000000
  a4:	add	w1, w1, #0x7f
  a8:	and	w0, w0, #0x7fffff
  ac:	orr	w0, w2, w0
  b0:	orr	w0, w0, w1, lsl #23
  b4:	fmov	s0, w0
  b8:	ret
  bc:	lsl	x0, x0, #1
  c0:	ubfx	x4, x0, #2, #1
  c4:	orr	x0, x4, x0
  c8:	add	x4, x0, #0x1
  cc:	asr	x0, x4, #2
  d0:	tbz	w4, #26, a0 <__floatdisf+0xa0>
  d4:	b	70 <__floatdisf+0x70>

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	movi	d0, #0x0
   4:	cmp	w0, #0x0
   8:	cbz	w0, 3c <__floatsidf+0x3c>
   c:	mov	x3, #0x0                   	// #0
  10:	b.lt	40 <__floatsidf+0x40>  // b.tstop
  14:	clz	w2, w0
  18:	sxtw	x0, w0
  1c:	add	w4, w2, #0x15
  20:	mov	w1, #0x41e                 	// #1054
  24:	sub	w1, w1, w2
  28:	lsl	x0, x0, x4
  2c:	eor	x0, x0, #0x10000000000000
  30:	add	x0, x0, x1, lsl #52
  34:	orr	x0, x0, x3
  38:	fmov	d0, x0
  3c:	ret
  40:	neg	w0, w0
  44:	mov	x3, #0x8000000000000000    	// #-9223372036854775808
  48:	b	14 <__floatsidf+0x14>

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	movi	v0.2s, #0x0
   4:	cmp	w0, #0x0
   8:	cbz	w0, 70 <__floatsisf+0x70>
   c:	mov	w4, #0x0                   	// #0
  10:	b.lt	84 <__floatsisf+0x84>  // b.tstop
  14:	clz	w2, w0
  18:	mov	w1, #0x1f                  	// #31
  1c:	sub	w1, w1, w2
  20:	cmp	w1, #0x17
  24:	b.le	74 <__floatsisf+0x74>
  28:	mov	w1, #0x8                   	// #8
  2c:	sub	w1, w1, w2
  30:	neg	w6, w1
  34:	mov	w5, #0x80000000            	// #-2147483648
  38:	lsr	w1, w0, w1
  3c:	eor	w1, w1, #0x800000
  40:	add	w3, w1, #0x1
  44:	lsl	w0, w0, w6
  48:	and	w3, w3, #0xfffffffe
  4c:	cmp	w0, w5
  50:	csel	w3, w3, w1, eq  // eq = none
  54:	add	w1, w1, #0x1
  58:	csel	w1, w3, w1, ls  // ls = plast
  5c:	mov	w0, #0x9e                  	// #158
  60:	sub	w0, w0, w2
  64:	add	w0, w1, w0, lsl #23
  68:	orr	w0, w0, w4
  6c:	fmov	s0, w0
  70:	ret
  74:	sub	w1, w2, #0x8
  78:	lsl	w0, w0, w1
  7c:	eor	w1, w0, #0x800000
  80:	b	5c <__floatsisf+0x5c>
  84:	neg	w0, w0
  88:	mov	w1, #0x1f                  	// #31
  8c:	clz	w2, w0
  90:	mov	w4, #0x80000000            	// #-2147483648
  94:	sub	w1, w1, w2
  98:	cmp	w1, #0x17
  9c:	b.gt	28 <__floatsisf+0x28>
  a0:	b	74 <__floatsisf+0x74>

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floattidf+0x10>
   8:	movi	d0, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-48]!
  14:	mov	x29, sp
  18:	stp	x21, x22, [sp, #32]
  1c:	asr	x21, x1, #63
  20:	eor	x0, x0, x21
  24:	stp	x19, x20, [sp, #16]
  28:	subs	x19, x0, x21
  2c:	eor	x20, x1, x21
  30:	sbc	x20, x20, x21
  34:	mov	x0, x19
  38:	mov	x1, x20
  3c:	bl	0 <__clzti2>
  40:	mov	w3, #0x80                  	// #128
  44:	sub	w3, w3, w0
  48:	mov	x22, x19
  4c:	cmp	w3, #0x35
  50:	sub	w2, w3, #0x1
  54:	b.le	128 <__floattidf+0x128>
  58:	cmp	w3, #0x36
  5c:	b.eq	14c <__floattidf+0x14c>  // b.none
  60:	cmp	w3, #0x37
  64:	b.eq	e4 <__floattidf+0xe4>  // b.none
  68:	add	w6, w0, #0x37
  6c:	mov	w5, #0x3f                  	// #63
  70:	sub	w1, w5, w6
  74:	subs	w0, w0, #0x9
  78:	mov	x4, #0xffffffffffffffff    	// #-1
  7c:	mov	x7, #0xfffffffffffffffe    	// #-2
  80:	lsl	x7, x7, x1
  84:	lsr	x1, x4, x6
  88:	orr	x1, x7, x1
  8c:	lsr	x7, x4, x0
  90:	csel	x1, x7, x1, pl  // pl = nfrst
  94:	lsr	x0, x4, x6
  98:	csel	x0, xzr, x0, pl  // pl = nfrst
  9c:	and	x0, x0, x20
  a0:	sub	w4, w3, #0x37
  a4:	and	x1, x1, x19
  a8:	lsl	x6, x20, #1
  ac:	orr	x1, x1, x0
  b0:	sub	w5, w5, w4
  b4:	cmp	x1, #0x0
  b8:	sub	w1, w3, #0x77
  bc:	cset	x7, ne  // ne = any
  c0:	lsl	x5, x6, x5
  c4:	cmp	w1, #0x0
  c8:	lsr	x0, x19, x4
  cc:	orr	x0, x5, x0
  d0:	lsr	x5, x20, x1
  d4:	csel	x0, x5, x0, ge  // ge = tcont
  d8:	lsr	x20, x20, x4
  dc:	orr	x22, x7, x0
  e0:	csel	x20, x20, xzr, lt  // lt = tstop
  e4:	ubfx	x0, x22, #2, #1
  e8:	orr	x0, x0, x22
  ec:	adds	x1, x0, #0x1
  f0:	cinc	x20, x20, cs  // cs = hs, nlast
  f4:	extr	x0, x20, x1, #2
  f8:	tbnz	x1, #55, 140 <__floattidf+0x140>
  fc:	add	w2, w2, #0x3ff
 100:	ubfx	x3, x0, #32, #20
 104:	and	w1, w21, #0x80000000
 108:	orr	w2, w3, w2, lsl #20
 10c:	orr	w1, w2, w1
 110:	ldp	x19, x20, [sp, #16]
 114:	bfi	x0, x1, #32, #32
 118:	fmov	d0, x0
 11c:	ldp	x21, x22, [sp, #32]
 120:	ldp	x29, x30, [sp], #48
 124:	ret
 128:	mov	w0, #0x35                  	// #53
 12c:	sub	w0, w0, w3
 130:	subs	w1, w0, #0x40
 134:	lsl	x0, x19, x0
 138:	csel	x0, xzr, x0, pl  // pl = nfrst
 13c:	b	fc <__floattidf+0xfc>
 140:	mov	w2, w3
 144:	extr	x0, x20, x1, #3
 148:	b	fc <__floattidf+0xfc>
 14c:	lsl	x22, x19, #1
 150:	extr	x20, x20, x19, #63
 154:	b	e4 <__floattidf+0xe4>

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floattisf+0x10>
   8:	movi	v0.2s, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-48]!
  14:	mov	x29, sp
  18:	stp	x21, x22, [sp, #32]
  1c:	asr	x22, x1, #63
  20:	eor	x0, x0, x22
  24:	stp	x19, x20, [sp, #16]
  28:	subs	x19, x0, x22
  2c:	eor	x1, x1, x22
  30:	sbc	x20, x1, x22
  34:	mov	x0, x19
  38:	mov	x1, x20
  3c:	bl	0 <__clzti2>
  40:	mov	w3, #0x80                  	// #128
  44:	sub	w3, w3, w0
  48:	mov	x21, x19
  4c:	cmp	w3, #0x18
  50:	sub	w2, w3, #0x1
  54:	b.le	124 <__floattisf+0x124>
  58:	cmp	w3, #0x19
  5c:	b.eq	148 <__floattisf+0x148>  // b.none
  60:	cmp	w3, #0x1a
  64:	b.eq	e4 <__floattisf+0xe4>  // b.none
  68:	add	w6, w0, #0x1a
  6c:	mov	w5, #0x3f                  	// #63
  70:	sub	w1, w5, w6
  74:	subs	w0, w0, #0x26
  78:	mov	x4, #0xffffffffffffffff    	// #-1
  7c:	mov	x7, #0xfffffffffffffffe    	// #-2
  80:	lsl	x7, x7, x1
  84:	lsr	x1, x4, x6
  88:	orr	x1, x7, x1
  8c:	lsr	x7, x4, x0
  90:	csel	x1, x7, x1, pl  // pl = nfrst
  94:	lsr	x0, x4, x6
  98:	csel	x0, xzr, x0, pl  // pl = nfrst
  9c:	and	x0, x0, x20
  a0:	sub	w4, w3, #0x1a
  a4:	and	x1, x1, x19
  a8:	lsl	x6, x20, #1
  ac:	orr	x1, x1, x0
  b0:	sub	w5, w5, w4
  b4:	cmp	x1, #0x0
  b8:	sub	w0, w3, #0x5a
  bc:	cset	x1, ne  // ne = any
  c0:	lsl	x5, x6, x5
  c4:	cmp	w0, #0x0
  c8:	lsr	x21, x19, x4
  cc:	orr	x21, x5, x21
  d0:	lsr	x5, x20, x0
  d4:	csel	x21, x5, x21, ge  // ge = tcont
  d8:	lsr	x20, x20, x4
  dc:	orr	x21, x1, x21
  e0:	csel	x20, x20, xzr, lt  // lt = tstop
  e4:	ubfx	x0, x21, #2, #1
  e8:	orr	x21, x0, x21
  ec:	adds	x21, x21, #0x1
  f0:	cinc	x20, x20, cs  // cs = hs, nlast
  f4:	extr	x0, x20, x21, #2
  f8:	tbnz	w21, #26, 13c <__floattisf+0x13c>
  fc:	and	w1, w22, #0x80000000
 100:	add	w2, w2, #0x7f
 104:	and	w0, w0, #0x7fffff
 108:	orr	w0, w1, w0
 10c:	orr	w0, w0, w2, lsl #23
 110:	fmov	s0, w0
 114:	ldp	x19, x20, [sp, #16]
 118:	ldp	x21, x22, [sp, #32]
 11c:	ldp	x29, x30, [sp], #48
 120:	ret
 124:	mov	w0, #0x18                  	// #24
 128:	sub	w0, w0, w3
 12c:	cmp	w0, #0x40
 130:	lsl	x0, x19, x0
 134:	csel	x0, xzr, x0, pl  // pl = nfrst
 138:	b	fc <__floattisf+0xfc>
 13c:	mov	w2, w3
 140:	extr	x0, x20, x21, #3
 144:	b	fc <__floattisf+0xfc>
 148:	lsl	x21, x19, #1
 14c:	extr	x20, x20, x19, #63
 150:	b	e4 <__floattisf+0xe4>

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	mov	x1, #0x100000              	// #1048576
   4:	mov	x2, #0x4530000000000000    	// #4985484787499139072
   8:	movk	x1, #0x4530, lsl #48
   c:	fmov	d0, x1
  10:	and	x1, x0, #0xffffffff
  14:	orr	x0, x2, x0, lsr #32
  18:	fmov	d1, x0
  1c:	mov	x2, #0x4330000000000000    	// #4841369599423283200
  20:	orr	x1, x1, x2
  24:	fsub	d0, d1, d0
  28:	fmov	d1, x1
  2c:	fadd	d0, d0, d1
  30:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	x0, 78 <__floatundisf+0x78>
   8:	clz	x4, x0
   c:	mov	w1, #0x40                  	// #64
  10:	sub	w2, w1, w4
  14:	cmp	w2, #0x18
  18:	sub	w3, w2, #0x1
  1c:	b.ls	7c <__floatundisf+0x7c>  // b.plast
  20:	cmp	w2, #0x19
  24:	b.eq	98 <__floatundisf+0x98>  // b.none
  28:	cmp	w2, #0x1a
  2c:	b.eq	50 <__floatundisf+0x50>  // b.none
  30:	add	w4, w4, #0x1a
  34:	mov	x5, #0xffffffffffffffff    	// #-1
  38:	sub	w1, w2, #0x1a
  3c:	lsr	x4, x5, x4
  40:	tst	x4, x0
  44:	cset	x4, ne  // ne = any
  48:	lsr	x0, x0, x1
  4c:	orr	x0, x0, x4
  50:	ubfx	x1, x0, #2, #1
  54:	orr	x0, x1, x0
  58:	add	x1, x0, #0x1
  5c:	lsr	x0, x1, #2
  60:	tbz	w1, #26, 88 <__floatundisf+0x88>
  64:	mov	w3, w2
  68:	lsr	x0, x1, #3
  6c:	add	w3, w3, #0x7f
  70:	bfi	w0, w3, #23, #9
  74:	fmov	s0, w0
  78:	ret
  7c:	mov	w1, #0x18                  	// #24
  80:	sub	w1, w1, w2
  84:	lsl	x0, x0, x1
  88:	add	w3, w3, #0x7f
  8c:	bfi	w0, w3, #23, #9
  90:	fmov	s0, w0
  94:	ret
  98:	lsl	x0, x0, #1
  9c:	ubfx	x1, x0, #2, #1
  a0:	orr	x0, x1, x0
  a4:	add	x1, x0, #0x1
  a8:	lsr	x0, x1, #2
  ac:	tbz	w1, #26, 88 <__floatundisf+0x88>
  b0:	b	64 <__floatundisf+0x64>

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	movi	d0, #0x0
   4:	cbz	w0, 2c <__floatunsidf+0x2c>
   8:	clz	w2, w0
   c:	mov	w0, w0
  10:	add	w3, w2, #0x15
  14:	mov	w1, #0x41e                 	// #1054
  18:	sub	w1, w1, w2
  1c:	lsl	x0, x0, x3
  20:	eor	x0, x0, #0x10000000000000
  24:	add	x0, x0, x1, lsl #52
  28:	fmov	d0, x0
  2c:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	movi	v0.2s, #0x0
   4:	cbz	w0, 60 <__floatunsisf+0x60>
   8:	clz	w2, w0
   c:	mov	w1, #0x1f                  	// #31
  10:	sub	w1, w1, w2
  14:	cmp	w1, #0x17
  18:	b.le	64 <__floatunsisf+0x64>
  1c:	mov	w1, #0x8                   	// #8
  20:	sub	w1, w1, w2
  24:	neg	w5, w1
  28:	mov	w4, #0x80000000            	// #-2147483648
  2c:	lsr	w1, w0, w1
  30:	eor	w1, w1, #0x800000
  34:	add	w3, w1, #0x1
  38:	lsl	w0, w0, w5
  3c:	and	w3, w3, #0xfffffffe
  40:	cmp	w0, w4
  44:	csel	w3, w3, w1, eq  // eq = none
  48:	add	w1, w1, #0x1
  4c:	csel	w1, w3, w1, ls  // ls = plast
  50:	mov	w0, #0x9e                  	// #158
  54:	sub	w0, w0, w2
  58:	add	w0, w1, w0, lsl #23
  5c:	fmov	s0, w0
  60:	ret
  64:	sub	w1, w2, #0x8
  68:	lsl	w0, w0, w1
  6c:	eor	w1, w0, #0x800000
  70:	b	50 <__floatunsisf+0x50>

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floatuntidf+0x10>
   8:	movi	d0, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-32]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	mov	x19, x1
  20:	mov	x20, x0
  24:	bl	0 <__clzti2>
  28:	mov	w1, #0x80                  	// #128
  2c:	sub	w2, w1, w0
  30:	cmp	w2, #0x35
  34:	sub	w3, w2, #0x1
  38:	b.le	104 <__floatuntidf+0x104>
  3c:	cmp	w2, #0x36
  40:	b.eq	160 <__floatuntidf+0x160>  // b.none
  44:	cmp	w2, #0x37
  48:	b.eq	cc <__floatuntidf+0xcc>  // b.none
  4c:	sub	w5, w2, #0x37
  50:	lsl	x4, x19, #1
  54:	mov	w7, #0x3f                  	// #63
  58:	sub	w6, w7, w5
  5c:	subs	w1, w2, #0x77
  60:	add	w8, w0, #0x37
  64:	lsl	x4, x4, x6
  68:	sub	w7, w7, w8
  6c:	lsr	x6, x20, x5
  70:	orr	x6, x4, x6
  74:	lsr	x4, x19, x1
  78:	sub	w0, w0, #0x9
  7c:	csel	x6, x4, x6, pl  // pl = nfrst
  80:	mov	x9, #0xfffffffffffffffe    	// #-2
  84:	mov	x4, #0xffffffffffffffff    	// #-1
  88:	lsr	x5, x19, x5
  8c:	lsl	x7, x9, x7
  90:	csel	x5, xzr, x5, pl  // pl = nfrst
  94:	lsr	x1, x4, x8
  98:	cmp	w0, #0x0
  9c:	orr	x1, x7, x1
  a0:	lsr	x7, x4, x0
  a4:	lsr	x4, x4, x8
  a8:	csel	x1, x7, x1, ge  // ge = tcont
  ac:	csel	x4, xzr, x4, ge  // ge = tcont
  b0:	and	x0, x1, x20
  b4:	and	x19, x4, x19
  b8:	orr	x0, x0, x19
  bc:	mov	x19, x5
  c0:	cmp	x0, #0x0
  c4:	cset	x0, ne  // ne = any
  c8:	orr	x20, x6, x0
  cc:	ubfx	x0, x20, #2, #1
  d0:	orr	x0, x0, x20
  d4:	adds	x20, x0, #0x1
  d8:	cinc	x19, x19, cs  // cs = hs, nlast
  dc:	extr	x0, x19, x20, #2
  e0:	tbnz	x20, #55, 138 <__floatuntidf+0x138>
  e4:	add	w1, w3, #0x3ff
  e8:	ubfx	x2, x0, #32, #20
  ec:	ldp	x19, x20, [sp, #16]
  f0:	orr	w1, w2, w1, lsl #20
  f4:	ldp	x29, x30, [sp], #32
  f8:	bfi	x0, x1, #32, #32
  fc:	fmov	d0, x0
 100:	ret
 104:	mov	w1, #0x35                  	// #53
 108:	sub	w1, w1, w2
 10c:	subs	w0, w1, #0x40
 110:	lsl	x0, x20, x1
 114:	csel	x0, xzr, x0, pl  // pl = nfrst
 118:	add	w1, w3, #0x3ff
 11c:	ubfx	x2, x0, #32, #20
 120:	orr	w1, w2, w1, lsl #20
 124:	ldp	x19, x20, [sp, #16]
 128:	bfi	x0, x1, #32, #32
 12c:	ldp	x29, x30, [sp], #32
 130:	fmov	d0, x0
 134:	ret
 138:	mov	w3, w2
 13c:	extr	x0, x19, x20, #3
 140:	add	w1, w3, #0x3ff
 144:	ubfx	x2, x0, #32, #20
 148:	orr	w1, w2, w1, lsl #20
 14c:	ldp	x19, x20, [sp, #16]
 150:	bfi	x0, x1, #32, #32
 154:	ldp	x29, x30, [sp], #32
 158:	fmov	d0, x0
 15c:	ret
 160:	extr	x19, x19, x20, #63
 164:	lsl	x20, x20, #1
 168:	b	cc <__floatuntidf+0xcc>

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	orr	x2, x0, x1
   4:	cbnz	x2, 10 <__floatuntisf+0x10>
   8:	movi	v0.2s, #0x0
   c:	ret
  10:	stp	x29, x30, [sp, #-32]!
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	mov	x20, x0
  20:	mov	x19, x1
  24:	bl	0 <__clzti2>
  28:	mov	w2, #0x80                  	// #128
  2c:	sub	w2, w2, w0
  30:	cmp	w2, #0x18
  34:	sub	w3, w2, #0x1
  38:	b.le	100 <__floatuntisf+0x100>
  3c:	cmp	w2, #0x19
  40:	b.eq	154 <__floatuntisf+0x154>  // b.none
  44:	cmp	w2, #0x1a
  48:	b.eq	cc <__floatuntisf+0xcc>  // b.none
  4c:	sub	w5, w2, #0x1a
  50:	subs	w1, w2, #0x5a
  54:	mov	w4, #0x3f                  	// #63
  58:	lsl	x9, x19, #1
  5c:	sub	w6, w4, w5
  60:	add	w8, w0, #0x1a
  64:	sub	w7, w4, w8
  68:	lsr	x4, x19, x1
  6c:	lsl	x9, x9, x6
  70:	sub	w0, w0, #0x26
  74:	lsr	x6, x20, x5
  78:	orr	x6, x9, x6
  7c:	csel	x6, x4, x6, pl  // pl = nfrst
  80:	mov	x9, #0xfffffffffffffffe    	// #-2
  84:	mov	x4, #0xffffffffffffffff    	// #-1
  88:	lsr	x5, x19, x5
  8c:	lsl	x7, x9, x7
  90:	csel	x5, xzr, x5, pl  // pl = nfrst
  94:	lsr	x1, x4, x8
  98:	cmp	w0, #0x0
  9c:	orr	x1, x7, x1
  a0:	lsr	x7, x4, x0
  a4:	lsr	x0, x4, x8
  a8:	csel	x1, x7, x1, ge  // ge = tcont
  ac:	csel	x0, xzr, x0, ge  // ge = tcont
  b0:	and	x1, x1, x20
  b4:	and	x19, x0, x19
  b8:	orr	x1, x1, x19
  bc:	mov	x19, x5
  c0:	cmp	x1, #0x0
  c4:	cset	x20, ne  // ne = any
  c8:	orr	x20, x6, x20
  cc:	ubfx	x0, x20, #2, #1
  d0:	orr	x20, x0, x20
  d4:	adds	x0, x20, #0x1
  d8:	cinc	x19, x19, cs  // cs = hs, nlast
  dc:	extr	x20, x19, x0, #2
  e0:	tbnz	w0, #26, 130 <__floatuntisf+0x130>
  e4:	add	w0, w3, #0x7f
  e8:	mov	w1, w20
  ec:	ldp	x19, x20, [sp, #16]
  f0:	bfi	w1, w0, #23, #9
  f4:	ldp	x29, x30, [sp], #32
  f8:	fmov	s0, w1
  fc:	ret
 100:	mov	w0, #0x18                  	// #24
 104:	sub	w0, w0, w2
 108:	cmp	w0, #0x40
 10c:	lsl	x20, x20, x0
 110:	add	w0, w3, #0x7f
 114:	csel	x20, xzr, x20, pl  // pl = nfrst
 118:	mov	w1, w20
 11c:	bfi	w1, w0, #23, #9
 120:	fmov	s0, w1
 124:	ldp	x19, x20, [sp, #16]
 128:	ldp	x29, x30, [sp], #32
 12c:	ret
 130:	mov	w3, w2
 134:	extr	x20, x19, x0, #3
 138:	add	w0, w3, #0x7f
 13c:	mov	w1, w20
 140:	ldp	x19, x20, [sp, #16]
 144:	bfi	w1, w0, #23, #9
 148:	ldp	x29, x30, [sp], #32
 14c:	fmov	s0, w1
 150:	ret
 154:	extr	x19, x19, x20, #63
 158:	lsl	x20, x20, #1
 15c:	b	cc <__floatuntisf+0xcc>

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text.unlikely:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	mov	x2, x0
   4:	tbz	w1, #5, 20 <__lshrdi3+0x20>
   8:	lsr	x2, x0, #32
   c:	sub	w1, w1, #0x20
  10:	mov	x0, #0x0                   	// #0
  14:	lsr	w1, w2, w1
  18:	bfxil	x0, x1, #0, #32
  1c:	ret
  20:	cbz	w1, 1c <__lshrdi3+0x1c>
  24:	lsr	x3, x0, #32
  28:	neg	w4, w1
  2c:	lsr	w2, w2, w1
  30:	mov	x0, #0x0                   	// #0
  34:	lsr	w1, w3, w1
  38:	bfi	x0, x1, #32, #32
  3c:	lsl	w3, w3, w4
  40:	orr	w2, w3, w2
  44:	bfxil	x0, x2, #0, #32
  48:	ret

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbz	w2, #6, 18 <__lshrti3+0x18>
   4:	sub	w2, w2, #0x40
   8:	mov	x4, #0x0                   	// #0
   c:	lsr	x0, x1, x2
  10:	mov	x1, x4
  14:	ret
  18:	cbz	w2, 14 <__lshrti3+0x14>
  1c:	neg	w5, w2
  20:	lsr	x3, x0, x2
  24:	lsr	x4, x1, x2
  28:	lsl	x0, x1, x5
  2c:	mov	x1, x4
  30:	orr	x0, x0, x3
  34:	b	14 <__lshrti3+0x14>

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	eor	x3, x1, x1, asr #63
   8:	sub	x1, x3, x1, asr #63
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	asr	x19, x0, #63
  18:	eor	x0, x0, x0, asr #63
  1c:	add	x2, sp, #0x28
  20:	sub	x0, x0, x19
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x0, [sp, #40]
  2c:	eor	x0, x19, x0
  30:	sub	x0, x0, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #48
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	asr	x4, x3, #63
   8:	eor	x2, x2, x4
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	asr	x19, x1, #63
  18:	subs	x2, x2, x4
  1c:	eor	x3, x3, x4
  20:	eor	x0, x0, x19
  24:	sbc	x3, x3, x4
  28:	eor	x1, x1, x19
  2c:	subs	x0, x0, x19
  30:	add	x4, sp, #0x20
  34:	sbc	x1, x1, x19
  38:	bl	0 <__udivmodti4>
  3c:	ldp	x0, x1, [sp, #32]
  40:	eor	x0, x0, x19
  44:	eor	x1, x1, x19
  48:	subs	x0, x0, x19
  4c:	sbc	x1, x1, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d16, d0, d3
   4:	fmul	d7, d2, d1
   8:	fmul	d6, d1, d3
   c:	fmov	d17, d1
  10:	fmul	d5, d0, d2
  14:	fmov	d4, d0
  18:	fadd	d1, d16, d7
  1c:	fsub	d0, d5, d6
  20:	fcmp	d1, d1
  24:	fccmp	d0, d0, #0x0, vs
  28:	b.vs	30 <__muldc3+0x30>
  2c:	ret
  30:	fabs	d19, d4
  34:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  38:	fmov	d18, x0
  3c:	fabs	d21, d17
  40:	fcmp	d19, d18
  44:	cset	w0, gt
  48:	b.gt	58 <__muldc3+0x58>
  4c:	fcmp	d21, d18
  50:	mov	w1, #0x0                   	// #0
  54:	b.le	94 <__muldc3+0x94>
  58:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  5c:	fmov	d19, x1
  60:	scvtf	d18, w0
  64:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  68:	fcmp	d21, d19
  6c:	fmov	d20, x0
  70:	bif	v4.8b, v18.8b, v20.8b
  74:	cset	w0, gt
  78:	fcmp	d2, d2
  7c:	scvtf	d18, w0
  80:	bif	v17.8b, v18.8b, v20.8b
  84:	b.vs	198 <__muldc3+0x198>
  88:	fcmp	d3, d3
  8c:	mov	w1, #0x1                   	// #1
  90:	b.vs	184 <__muldc3+0x184>
  94:	fabs	d19, d2
  98:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  9c:	fmov	d18, x0
  a0:	fabs	d20, d3
  a4:	fcmp	d19, d18
  a8:	cset	w0, gt
  ac:	b.gt	138 <__muldc3+0x138>
  b0:	fcmp	d20, d18
  b4:	b.gt	138 <__muldc3+0x138>
  b8:	cbnz	w1, 10c <__muldc3+0x10c>
  bc:	fabs	d5, d5
  c0:	fcmp	d5, d18
  c4:	b.gt	ec <__muldc3+0xec>
  c8:	fabs	d6, d6
  cc:	fcmp	d6, d18
  d0:	b.gt	ec <__muldc3+0xec>
  d4:	fabs	d16, d16
  d8:	fcmp	d16, d18
  dc:	b.gt	ec <__muldc3+0xec>
  e0:	fabs	d7, d7
  e4:	fcmp	d7, d18
  e8:	b.le	2c <__muldc3+0x2c>
  ec:	fcmp	d4, d4
  f0:	b.vs	1ec <__muldc3+0x1ec>
  f4:	fcmp	d17, d17
  f8:	b.vs	1d8 <__muldc3+0x1d8>
  fc:	fcmp	d2, d2
 100:	b.vs	1c4 <__muldc3+0x1c4>
 104:	fcmp	d3, d3
 108:	b.vs	1b0 <__muldc3+0x1b0>
 10c:	fmul	d0, d4, d2
 110:	fmul	d4, d4, d3
 114:	fmul	d2, d17, d2
 118:	fmul	d3, d17, d3
 11c:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 120:	fmov	d1, x0
 124:	fsub	d0, d0, d3
 128:	fadd	d2, d4, d2
 12c:	fmul	d0, d0, d1
 130:	fmul	d1, d2, d1
 134:	ret
 138:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
 13c:	fmov	d1, x1
 140:	scvtf	d0, w0
 144:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 148:	fcmp	d20, d1
 14c:	fmov	d5, x0
 150:	bif	v2.8b, v0.8b, v5.8b
 154:	cset	w0, gt
 158:	fcmp	d4, d4
 15c:	scvtf	d0, w0
 160:	bif	v3.8b, v0.8b, v5.8b
 164:	b.vs	1a4 <__muldc3+0x1a4>
 168:	fcmp	d17, d17
 16c:	b.vc	10c <__muldc3+0x10c>
 170:	movi	d0, #0x0
 174:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 178:	fmov	d1, x0
 17c:	bif	v17.8b, v0.8b, v1.8b
 180:	b	10c <__muldc3+0x10c>
 184:	movi	d18, #0x0
 188:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 18c:	fmov	d19, x0
 190:	bif	v3.8b, v18.8b, v19.8b
 194:	b	94 <__muldc3+0x94>
 198:	movi	d18, #0x0
 19c:	bif	v2.8b, v18.8b, v20.8b
 1a0:	b	88 <__muldc3+0x88>
 1a4:	movi	d0, #0x0
 1a8:	bif	v4.8b, v0.8b, v5.8b
 1ac:	b	168 <__muldc3+0x168>
 1b0:	movi	d0, #0x0
 1b4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1b8:	fmov	d1, x0
 1bc:	bif	v3.8b, v0.8b, v1.8b
 1c0:	b	10c <__muldc3+0x10c>
 1c4:	movi	d0, #0x0
 1c8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1cc:	fmov	d1, x0
 1d0:	bif	v2.8b, v0.8b, v1.8b
 1d4:	b	104 <__muldc3+0x104>
 1d8:	movi	d0, #0x0
 1dc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1e0:	fmov	d1, x0
 1e4:	bif	v17.8b, v0.8b, v1.8b
 1e8:	b	fc <__muldc3+0xfc>
 1ec:	movi	d0, #0x0
 1f0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1f4:	fmov	d1, x0
 1f8:	bif	v4.8b, v0.8b, v1.8b
 1fc:	b	f4 <__muldc3+0xf4>

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	fmov	x0, d0
   4:	fmov	x1, d1
   8:	ubfx	x4, x0, #52, #11
   c:	eor	x3, x0, x1
  10:	and	x7, x3, #0x8000000000000000
  14:	sub	w3, w4, #0x1
  18:	and	x2, x0, #0xfffffffffffff
  1c:	ubfx	x8, x1, #52, #11
  20:	cmp	w3, #0x7fd
  24:	and	x5, x1, #0xfffffffffffff
  28:	b.hi	e8 <__muldf3+0xe8>  // b.pmore
  2c:	sub	w3, w8, #0x1
  30:	cmp	w3, #0x7fd
  34:	b.hi	e8 <__muldf3+0xe8>  // b.pmore
  38:	mov	w0, #0x0                   	// #0
  3c:	lsr	x1, x2, #32
  40:	lsl	w3, w5, #11
  44:	orr	x1, x1, #0x100000
  48:	and	x2, x2, #0xffffffff
  4c:	ubfx	x5, x5, #21, #32
  50:	add	w4, w4, w8
  54:	orr	x5, x5, #0x80000000
  58:	add	w4, w4, w0
  5c:	mul	x0, x3, x1
  60:	mul	x3, x3, x2
  64:	mul	x2, x5, x2
  68:	and	x8, x0, #0xffffffff
  6c:	and	x6, x3, #0xffffffff
  70:	lsr	x0, x0, #32
  74:	add	x3, x8, x3, lsr #32
  78:	add	x3, x3, w2, uxtw
  7c:	madd	x5, x5, x1, x0
  80:	lsr	x1, x3, #32
  84:	add	x3, x6, x3, lsl #32
  88:	add	x2, x1, x2, lsr #32
  8c:	add	x1, x2, x5
  90:	tbnz	x1, #52, 168 <__muldf3+0x168>
  94:	sub	w4, w4, #0x3ff
  98:	extr	x1, x1, x3, #63
  9c:	lsl	x3, x3, #1
  a0:	orr	x0, x7, #0x7ff0000000000000
  a4:	cmp	w4, #0x7fe
  a8:	fmov	d0, x0
  ac:	b.gt	e4 <__muldf3+0xe4>
  b0:	cmp	w4, #0x0
  b4:	b.le	198 <__muldf3+0x198>
  b8:	mov	x0, x1
  bc:	bfi	x0, x4, #52, #12
  c0:	orr	x0, x7, x0
  c4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  c8:	add	x1, x0, #0x1
  cc:	cmp	x3, x2
  d0:	and	x1, x1, #0xfffffffffffffffe
  d4:	add	x4, x0, #0x1
  d8:	csel	x0, x1, x0, eq  // eq = none
  dc:	csel	x0, x0, x4, ls  // ls = plast
  e0:	fmov	d0, x0
  e4:	ret
  e8:	and	x6, x0, #0x7fffffffffffffff
  ec:	mov	x3, #0x7ff0000000000000    	// #9218868437227405312
  f0:	cmp	x6, x3
  f4:	b.hi	15c <__muldf3+0x15c>  // b.pmore
  f8:	and	x9, x1, #0x7fffffffffffffff
  fc:	cmp	x9, x3
 100:	b.hi	18c <__muldf3+0x18c>  // b.pmore
 104:	cmp	x6, x3
 108:	b.eq	1b0 <__muldf3+0x1b0>  // b.none
 10c:	cmp	x9, x3
 110:	b.eq	170 <__muldf3+0x170>  // b.none
 114:	cbz	x6, 1a8 <__muldf3+0x1a8>
 118:	cbz	x9, 1a8 <__muldf3+0x1a8>
 11c:	tst	x0, #0x7ff0000000000000
 120:	mov	w0, #0x0                   	// #0
 124:	b.ne	13c <__muldf3+0x13c>  // b.any
 128:	clz	x3, x2
 12c:	mov	w0, #0x1                   	// #1
 130:	sub	w3, w3, #0xb
 134:	sub	w0, w0, w3
 138:	lsl	x2, x2, x3
 13c:	tst	x1, #0x7ff0000000000000
 140:	b.ne	3c <__muldf3+0x3c>  // b.any
 144:	clz	x1, x5
 148:	sub	w1, w1, #0xb
 14c:	sub	w0, w0, w1
 150:	add	w0, w0, #0x1
 154:	lsl	x5, x5, x1
 158:	b	3c <__muldf3+0x3c>
 15c:	orr	x0, x0, #0x8000000000000
 160:	fmov	d0, x0
 164:	ret
 168:	sub	w4, w4, #0x3fe
 16c:	b	a0 <__muldf3+0xa0>
 170:	cmp	x6, #0x0
 174:	orr	x0, x7, #0x7ff0000000000000
 178:	fmov	d1, x0
 17c:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 180:	fmov	d0, x0
 184:	fcsel	d0, d0, d1, eq  // eq = none
 188:	ret
 18c:	orr	x0, x1, #0x8000000000000
 190:	fmov	d0, x0
 194:	ret
 198:	mov	w0, #0x1                   	// #1
 19c:	sub	w0, w0, w4
 1a0:	cmp	w0, #0x3f
 1a4:	b.ls	1b8 <__muldf3+0x1b8>  // b.plast
 1a8:	fmov	d0, x7
 1ac:	ret
 1b0:	cmp	x9, #0x0
 1b4:	b	174 <__muldf3+0x174>
 1b8:	add	w4, w4, #0x3f
 1bc:	lsr	x2, x3, x0
 1c0:	lsr	x0, x1, x0
 1c4:	lsl	x3, x3, x4
 1c8:	cmp	x3, #0x0
 1cc:	cset	x3, ne  // ne = any
 1d0:	lsl	x1, x1, x4
 1d4:	orr	x1, x1, x2
 1d8:	orr	x3, x1, x3
 1dc:	b	c0 <__muldf3+0xc0>

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	and	w2, w0, #0xffff
   4:	and	w3, w1, #0xffff
   8:	lsr	w6, w0, #16
   c:	lsr	w7, w1, #16
  10:	mov	x4, x0
  14:	mov	x0, #0x0                   	// #0
  18:	mul	w5, w2, w3
  1c:	asr	x9, x1, #32
  20:	mul	w3, w3, w6
  24:	asr	x8, x4, #32
  28:	mul	w2, w2, w7
  2c:	mul	w6, w6, w7
  30:	add	w3, w3, w5, lsr #16
  34:	add	w2, w2, w3, uxth
  38:	add	w3, w6, w3, lsr #16
  3c:	lsl	w6, w2, #16
  40:	add	w2, w3, w2, lsr #16
  44:	add	w5, w6, w5, uxth
  48:	bfxil	x0, x5, #0, #32
  4c:	bfi	x0, x2, #32, #32
  50:	asr	x2, x0, #32
  54:	madd	w4, w4, w9, w2
  58:	madd	w1, w1, w8, w4
  5c:	bfi	x0, x1, #32, #32
  60:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x3, x0
   4:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
   8:	cmp	x0, x4
   c:	mul	x0, x0, x1
  10:	b.eq	7c <__mulodi4+0x7c>  // b.none
  14:	cmp	x1, x4
  18:	b.eq	90 <__mulodi4+0x90>  // b.none
  1c:	asr	x6, x3, #63
  20:	asr	x5, x1, #63
  24:	eor	x3, x3, x6
  28:	eor	x1, x1, x5
  2c:	sub	x3, x3, x6
  30:	sub	x7, x1, x5
  34:	cmp	x3, #0x1
  38:	ccmp	x7, #0x1, #0x4, gt
  3c:	b.le	74 <__mulodi4+0x74>
  40:	cmp	x6, x5
  44:	b.eq	64 <__mulodi4+0x64>  // b.none
  48:	sub	x1, x5, x1
  4c:	sdiv	x4, x4, x1
  50:	cmp	x4, x3
  54:	b.ge	74 <__mulodi4+0x74>  // b.tcont
  58:	mov	w1, #0x1                   	// #1
  5c:	str	w1, [x2]
  60:	ret
  64:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  68:	sdiv	x7, x1, x7
  6c:	cmp	x7, x3
  70:	b.lt	58 <__mulodi4+0x58>  // b.tstop
  74:	str	wzr, [x2]
  78:	ret
  7c:	cmp	x1, #0x1
  80:	b.ls	74 <__mulodi4+0x74>  // b.plast
  84:	mov	w1, #0x1                   	// #1
  88:	str	w1, [x2]
  8c:	b	60 <__mulodi4+0x60>
  90:	cmp	x3, #0x1
  94:	b.hi	58 <__mulodi4+0x58>  // b.pmore
  98:	b	74 <__mulodi4+0x74>

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w3, w0
   4:	mov	w4, #0x80000000            	// #-2147483648
   8:	cmp	w0, w4
   c:	mul	w0, w0, w1
  10:	b.eq	7c <__mulosi4+0x7c>  // b.none
  14:	cmp	w1, w4
  18:	b.eq	90 <__mulosi4+0x90>  // b.none
  1c:	asr	w6, w3, #31
  20:	asr	w5, w1, #31
  24:	eor	w3, w3, w6
  28:	eor	w1, w1, w5
  2c:	sub	w3, w3, w6
  30:	sub	w7, w1, w5
  34:	cmp	w3, #0x1
  38:	ccmp	w7, #0x1, #0x4, gt
  3c:	b.le	74 <__mulosi4+0x74>
  40:	cmp	w6, w5
  44:	b.eq	64 <__mulosi4+0x64>  // b.none
  48:	sub	w1, w5, w1
  4c:	sdiv	w4, w4, w1
  50:	cmp	w4, w3
  54:	b.ge	74 <__mulosi4+0x74>  // b.tcont
  58:	mov	w1, #0x1                   	// #1
  5c:	str	w1, [x2]
  60:	ret
  64:	mov	w1, #0x7fffffff            	// #2147483647
  68:	sdiv	w7, w1, w7
  6c:	cmp	w7, w3
  70:	b.lt	58 <__mulosi4+0x58>  // b.tstop
  74:	str	wzr, [x2]
  78:	ret
  7c:	cmp	w1, #0x1
  80:	b.ls	74 <__mulosi4+0x74>  // b.plast
  84:	mov	w1, #0x1                   	// #1
  88:	str	w1, [x2]
  8c:	b	60 <__mulosi4+0x60>
  90:	cmp	w3, #0x1
  94:	b.hi	58 <__mulosi4+0x58>  // b.pmore
  98:	b	74 <__mulosi4+0x74>

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x21, x22, [sp, #32]
   c:	umulh	x21, x0, x2
  10:	madd	x21, x1, x2, x21
  14:	str	x23, [sp, #48]
  18:	mul	x22, x0, x2
  1c:	madd	x21, x0, x3, x21
  20:	mov	x23, x4
  24:	cbz	x0, e8 <__muloti4+0xe8>
  28:	cbz	x2, 150 <__muloti4+0x150>
  2c:	asr	x7, x1, #63
  30:	asr	x6, x3, #63
  34:	eor	x0, x0, x7
  38:	eor	x1, x1, x7
  3c:	stp	x19, x20, [sp, #16]
  40:	subs	x19, x0, x7
  44:	eor	x0, x2, x6
  48:	sbc	x20, x1, x7
  4c:	eor	x5, x3, x6
  50:	subs	x2, x0, x6
  54:	mov	w1, #0x1                   	// #1
  58:	sbc	x3, x5, x6
  5c:	cmp	x20, #0x0
  60:	b.le	c8 <__muloti4+0xc8>
  64:	mov	w1, #0x0                   	// #0
  68:	cmp	x3, #0x0
  6c:	mov	w4, #0x1                   	// #1
  70:	b.le	d8 <__muloti4+0xd8>
  74:	mov	w4, #0x0                   	// #0
  78:	orr	w1, w1, w4
  7c:	tbnz	w1, #0, 134 <__muloti4+0x134>
  80:	cmp	x7, x6
  84:	b.eq	11c <__muloti4+0x11c>  // b.none
  88:	subs	x2, x6, x0
  8c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  90:	sbc	x3, x6, x5
  94:	mov	x0, #0x0                   	// #0
  98:	bl	0 <__divti3>
  9c:	cmp	x20, x1
  a0:	b.le	130 <__muloti4+0x130>
  a4:	ldp	x19, x20, [sp, #16]
  a8:	mov	w0, #0x1                   	// #1
  ac:	str	w0, [x23]
  b0:	mov	x1, x21
  b4:	mov	x0, x22
  b8:	ldp	x21, x22, [sp, #32]
  bc:	ldr	x23, [sp, #48]
  c0:	ldp	x29, x30, [sp], #64
  c4:	ret
  c8:	b.ne	68 <__muloti4+0x68>  // b.any
  cc:	cmp	x19, #0x1
  d0:	b.ls	68 <__muloti4+0x68>  // b.plast
  d4:	b	64 <__muloti4+0x64>
  d8:	b.ne	78 <__muloti4+0x78>  // b.any
  dc:	cmp	x2, #0x1
  e0:	b.ls	78 <__muloti4+0x78>  // b.plast
  e4:	b	74 <__muloti4+0x74>
  e8:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  ec:	cmp	x1, x4
  f0:	b.ne	28 <__muloti4+0x28>  // b.any
  f4:	cbnz	x3, a8 <__muloti4+0xa8>
  f8:	cmp	x2, #0x1
  fc:	b.hi	a8 <__muloti4+0xa8>  // b.pmore
 100:	str	wzr, [x23]
 104:	mov	x0, x22
 108:	mov	x1, x21
 10c:	ldp	x21, x22, [sp, #32]
 110:	ldr	x23, [sp, #48]
 114:	ldp	x29, x30, [sp], #64
 118:	ret
 11c:	mov	x0, #0xffffffffffffffff    	// #-1
 120:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 124:	bl	0 <__divti3>
 128:	cmp	x20, x1
 12c:	b.gt	a4 <__muloti4+0xa4>
 130:	b.eq	140 <__muloti4+0x140>  // b.none
 134:	ldp	x19, x20, [sp, #16]
 138:	str	wzr, [x23]
 13c:	b	104 <__muloti4+0x104>
 140:	cmp	x19, x0
 144:	b.hi	a4 <__muloti4+0xa4>  // b.pmore
 148:	ldp	x19, x20, [sp, #16]
 14c:	b	138 <__muloti4+0x138>
 150:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
 154:	cmp	x3, x4
 158:	b.ne	2c <__muloti4+0x2c>  // b.any
 15c:	cbnz	x1, a8 <__muloti4+0xa8>
 160:	cmp	x0, #0x1
 164:	b.hi	a8 <__muloti4+0xa8>  // b.pmore
 168:	str	wzr, [x23]
 16c:	b	104 <__muloti4+0x104>

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s16, s0, s3
   4:	fmul	s7, s2, s1
   8:	fmul	s6, s1, s3
   c:	fmov	s17, s1
  10:	fmul	s5, s0, s2
  14:	fmov	s4, s0
  18:	fadd	s1, s16, s7
  1c:	fsub	s0, s5, s6
  20:	fcmp	s1, s1
  24:	fccmp	s0, s0, #0x0, vs
  28:	b.vs	30 <__mulsc3+0x30>
  2c:	ret
  30:	fabs	s19, s4
  34:	mov	w0, #0x7f7fffff            	// #2139095039
  38:	fmov	s18, w0
  3c:	fabs	s21, s17
  40:	fcmp	s19, s18
  44:	cset	w0, gt
  48:	b.gt	58 <__mulsc3+0x58>
  4c:	fcmp	s21, s18
  50:	mov	w1, #0x0                   	// #0
  54:	b.le	90 <__mulsc3+0x90>
  58:	mov	w1, #0x7f7fffff            	// #2139095039
  5c:	fmov	s19, w1
  60:	scvtf	s18, w0
  64:	movi	v20.2s, #0x80, lsl #24
  68:	fcmp	s21, s19
  6c:	bif	v4.8b, v18.8b, v20.8b
  70:	cset	w0, gt
  74:	fcmp	s2, s2
  78:	scvtf	s18, w0
  7c:	bif	v17.8b, v18.8b, v20.8b
  80:	b.vs	188 <__mulsc3+0x188>
  84:	fcmp	s3, s3
  88:	mov	w1, #0x1                   	// #1
  8c:	b.vs	178 <__mulsc3+0x178>
  90:	fabs	s19, s2
  94:	mov	w0, #0x7f7fffff            	// #2139095039
  98:	fmov	s18, w0
  9c:	fabs	s20, s3
  a0:	fcmp	s19, s18
  a4:	cset	w0, gt
  a8:	b.gt	134 <__mulsc3+0x134>
  ac:	fcmp	s20, s18
  b0:	b.gt	134 <__mulsc3+0x134>
  b4:	cbnz	w1, 108 <__mulsc3+0x108>
  b8:	fabs	s5, s5
  bc:	fcmp	s5, s18
  c0:	b.gt	e8 <__mulsc3+0xe8>
  c4:	fabs	s6, s6
  c8:	fcmp	s6, s18
  cc:	b.gt	e8 <__mulsc3+0xe8>
  d0:	fabs	s16, s16
  d4:	fcmp	s16, s18
  d8:	b.gt	e8 <__mulsc3+0xe8>
  dc:	fabs	s7, s7
  e0:	fcmp	s7, s18
  e4:	b.le	2c <__mulsc3+0x2c>
  e8:	fcmp	s4, s4
  ec:	b.vs	1d0 <__mulsc3+0x1d0>
  f0:	fcmp	s17, s17
  f4:	b.vs	1c0 <__mulsc3+0x1c0>
  f8:	fcmp	s2, s2
  fc:	b.vs	1b0 <__mulsc3+0x1b0>
 100:	fcmp	s3, s3
 104:	b.vs	1a0 <__mulsc3+0x1a0>
 108:	fmul	s0, s4, s2
 10c:	fmul	s4, s4, s3
 110:	fmul	s2, s17, s2
 114:	fmul	s3, s17, s3
 118:	mov	w0, #0x7f800000            	// #2139095040
 11c:	fmov	s1, w0
 120:	fsub	s0, s0, s3
 124:	fadd	s2, s4, s2
 128:	fmul	s0, s0, s1
 12c:	fmul	s1, s2, s1
 130:	ret
 134:	mov	w1, #0x7f7fffff            	// #2139095039
 138:	fmov	s1, w1
 13c:	scvtf	s0, w0
 140:	movi	v5.2s, #0x80, lsl #24
 144:	fcmp	s20, s1
 148:	bif	v2.8b, v0.8b, v5.8b
 14c:	cset	w0, gt
 150:	fcmp	s4, s4
 154:	scvtf	s0, w0
 158:	bif	v3.8b, v0.8b, v5.8b
 15c:	b.vs	194 <__mulsc3+0x194>
 160:	fcmp	s17, s17
 164:	b.vc	108 <__mulsc3+0x108>
 168:	movi	v0.2s, #0x0
 16c:	movi	v1.2s, #0x80, lsl #24
 170:	bif	v17.8b, v0.8b, v1.8b
 174:	b	108 <__mulsc3+0x108>
 178:	movi	v18.2s, #0x0
 17c:	movi	v19.2s, #0x80, lsl #24
 180:	bif	v3.8b, v18.8b, v19.8b
 184:	b	90 <__mulsc3+0x90>
 188:	movi	v18.2s, #0x0
 18c:	bif	v2.8b, v18.8b, v20.8b
 190:	b	84 <__mulsc3+0x84>
 194:	movi	v0.2s, #0x0
 198:	bif	v4.8b, v0.8b, v5.8b
 19c:	b	160 <__mulsc3+0x160>
 1a0:	movi	v0.2s, #0x0
 1a4:	movi	v1.2s, #0x80, lsl #24
 1a8:	bif	v3.8b, v0.8b, v1.8b
 1ac:	b	108 <__mulsc3+0x108>
 1b0:	movi	v0.2s, #0x0
 1b4:	movi	v1.2s, #0x80, lsl #24
 1b8:	bif	v2.8b, v0.8b, v1.8b
 1bc:	b	100 <__mulsc3+0x100>
 1c0:	movi	v0.2s, #0x0
 1c4:	movi	v1.2s, #0x80, lsl #24
 1c8:	bif	v17.8b, v0.8b, v1.8b
 1cc:	b	f8 <__mulsc3+0xf8>
 1d0:	movi	v0.2s, #0x0
 1d4:	movi	v1.2s, #0x80, lsl #24
 1d8:	bif	v4.8b, v0.8b, v1.8b
 1dc:	b	f0 <__mulsc3+0xf0>

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	fmov	w0, s0
   4:	fmov	w1, s1
   8:	ubfx	x2, x0, #23, #8
   c:	eor	w3, w0, w1
  10:	and	w7, w3, #0x80000000
  14:	sub	w3, w2, #0x1
  18:	cmp	w3, #0xfd
  1c:	and	w6, w0, #0x7fffff
  20:	ubfx	x8, x1, #23, #8
  24:	and	w3, w1, #0x7fffff
  28:	b.hi	b8 <__mulsf3+0xb8>  // b.pmore
  2c:	sub	w4, w8, #0x1
  30:	cmp	w4, #0xfd
  34:	b.hi	b8 <__mulsf3+0xb8>  // b.pmore
  38:	mov	w0, #0x0                   	// #0
  3c:	lsl	w3, w3, #8
  40:	orr	w6, w6, #0x800000
  44:	orr	w3, w3, #0x80000000
  48:	add	w2, w2, w8
  4c:	add	w2, w2, w0
  50:	umull	x3, w3, w6
  54:	mov	w1, w3
  58:	lsr	x0, x3, #32
  5c:	mov	w4, w0
  60:	tbnz	w0, #23, 138 <__mulsf3+0x138>
  64:	sub	w2, w2, #0x7f
  68:	extr	w4, w0, w3, #31
  6c:	lsl	w1, w3, #1
  70:	orr	w0, w7, #0x7f800000
  74:	cmp	w2, #0xfe
  78:	fmov	s0, w0
  7c:	b.gt	b4 <__mulsf3+0xb4>
  80:	cmp	w2, #0x0
  84:	b.le	168 <__mulsf3+0x168>
  88:	mov	w0, w4
  8c:	bfi	w0, w2, #23, #9
  90:	orr	w0, w7, w0
  94:	mov	w3, #0x80000000            	// #-2147483648
  98:	add	w2, w0, #0x1
  9c:	cmp	w1, w3
  a0:	and	w2, w2, #0xfffffffe
  a4:	add	w4, w0, #0x1
  a8:	csel	w0, w2, w0, eq  // eq = none
  ac:	csel	w0, w0, w4, ls  // ls = plast
  b0:	fmov	s0, w0
  b4:	ret
  b8:	and	w5, w0, #0x7fffffff
  bc:	mov	w4, #0x7f800000            	// #2139095040
  c0:	cmp	w5, w4
  c4:	b.hi	12c <__mulsf3+0x12c>  // b.pmore
  c8:	and	w9, w1, #0x7fffffff
  cc:	cmp	w9, w4
  d0:	b.hi	15c <__mulsf3+0x15c>  // b.pmore
  d4:	cmp	w5, w4
  d8:	b.eq	180 <__mulsf3+0x180>  // b.none
  dc:	cmp	w9, w4
  e0:	b.eq	140 <__mulsf3+0x140>  // b.none
  e4:	cbz	w5, 178 <__mulsf3+0x178>
  e8:	cbz	w9, 178 <__mulsf3+0x178>
  ec:	tst	w0, #0x7f800000
  f0:	mov	w0, #0x0                   	// #0
  f4:	b.ne	10c <__mulsf3+0x10c>  // b.any
  f8:	clz	w4, w6
  fc:	mov	w0, #0x1                   	// #1
 100:	sub	w4, w4, #0x8
 104:	sub	w0, w0, w4
 108:	lsl	w6, w6, w4
 10c:	tst	w1, #0x7f800000
 110:	b.ne	3c <__mulsf3+0x3c>  // b.any
 114:	clz	w1, w3
 118:	sub	w1, w1, #0x8
 11c:	sub	w0, w0, w1
 120:	add	w0, w0, #0x1
 124:	lsl	w3, w3, w1
 128:	b	3c <__mulsf3+0x3c>
 12c:	orr	w0, w0, #0x400000
 130:	fmov	s0, w0
 134:	ret
 138:	sub	w2, w2, #0x7e
 13c:	b	70 <__mulsf3+0x70>
 140:	cmp	w5, #0x0
 144:	orr	w0, w7, #0x7f800000
 148:	fmov	s1, w0
 14c:	mov	w0, #0x7fc00000            	// #2143289344
 150:	fmov	s0, w0
 154:	fcsel	s0, s0, s1, eq  // eq = none
 158:	ret
 15c:	orr	w0, w1, #0x400000
 160:	fmov	s0, w0
 164:	ret
 168:	mov	w0, #0x1                   	// #1
 16c:	sub	w0, w0, w2
 170:	cmp	w0, #0x1f
 174:	b.ls	188 <__mulsf3+0x188>  // b.plast
 178:	fmov	s0, w7
 17c:	ret
 180:	cmp	w9, #0x0
 184:	b	144 <__mulsf3+0x144>
 188:	add	w2, w2, #0x1f
 18c:	lsr	w3, w1, w0
 190:	lsr	w0, w4, w0
 194:	lsl	w1, w1, w2
 198:	cmp	w1, #0x0
 19c:	cset	w1, ne  // ne = any
 1a0:	lsl	w2, w4, w2
 1a4:	orr	w2, w2, w3
 1a8:	orr	w1, w2, w1
 1ac:	b	90 <__mulsf3+0x90>

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x4, x2, #0xffffffff
   4:	lsr	x7, x0, #32
   8:	and	x5, x0, #0xffffffff
   c:	lsr	x8, x2, #32
  10:	mov	x6, x0
  14:	mul	x0, x5, x4
  18:	mul	x4, x4, x7
  1c:	mul	x5, x5, x8
  20:	mul	x7, x7, x8
  24:	add	x4, x4, x0, lsr #32
  28:	add	x5, x5, w4, uxtw
  2c:	add	x4, x7, x4, lsr #32
  30:	add	x4, x4, x5, lsr #32
  34:	lsl	x5, x5, #32
  38:	add	x0, x5, w0, uxtw
  3c:	madd	x4, x6, x3, x4
  40:	madd	x1, x2, x1, x4
  44:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	w7, #0x7ffd                	// #32765
   8:	mov	x4, #0x0                   	// #0
   c:	mov	x29, sp
  10:	str	q0, [sp, #16]
  14:	ldp	x1, x2, [sp, #16]
  18:	str	q1, [sp, #32]
  1c:	ldr	x0, [sp, #40]
  20:	mov	x10, x0
  24:	and	x14, x0, #0xffffffffffff
  28:	eor	x9, x2, x0
  2c:	ubfx	x13, x0, #48, #15
  30:	ldr	x0, [sp, #32]
  34:	ubfx	x12, x2, #48, #15
  38:	sub	w8, w12, #0x1
  3c:	and	x5, x9, #0x8000000000000000
  40:	mov	x6, x0
  44:	cmp	w8, w7
  48:	mov	x0, x1
  4c:	and	x8, x2, #0xffffffffffff
  50:	mov	x18, x6
  54:	b.hi	1e8 <__multf3+0x1e8>  // b.pmore
  58:	sub	w9, w13, #0x1
  5c:	cmp	w9, w7
  60:	b.hi	1e8 <__multf3+0x1e8>  // b.pmore
  64:	mov	w11, #0x0                   	// #0
  68:	and	x30, x0, #0xffffffff
  6c:	extr	x1, x14, x18, #49
  70:	lsl	w7, w18, #15
  74:	lsr	x0, x0, #32
  78:	ubfx	x18, x18, #17, #32
  7c:	orr	x1, x1, #0x8000000000000000
  80:	and	x15, x1, #0xffffffff
  84:	orr	x8, x8, #0x1000000000000
  88:	and	x14, x8, #0xffffffff
  8c:	lsr	x9, x1, #32
  90:	mul	x10, x7, x0
  94:	lsr	x2, x8, #32
  98:	mul	x1, x18, x30
  9c:	add	w12, w12, w13
  a0:	mul	x3, x15, x30
  a4:	mul	x6, x18, x0
  a8:	adds	x10, x10, x1
  ac:	mul	x17, x7, x14
  b0:	cset	x1, cs  // cs = hs, nlast
  b4:	mul	x13, x15, x0
  b8:	adds	x6, x6, x3
  bc:	mul	x8, x18, x14
  c0:	cset	x16, cs  // cs = hs, nlast
  c4:	mul	x3, x9, x30
  c8:	adds	x6, x6, x17
  cc:	mul	x17, x2, x7
  d0:	cinc	x16, x16, cs  // cs = hs, nlast
  d4:	adds	x8, x8, x13
  d8:	extr	x1, x1, x10, #32
  dc:	cset	x13, cs  // cs = hs, nlast
  e0:	adds	x17, x17, x3
  e4:	cset	x3, cs  // cs = hs, nlast
  e8:	adds	x8, x8, x17
  ec:	adc	x13, x13, x3
  f0:	mul	x7, x7, x30
  f4:	adds	x1, x1, x6
  f8:	lsl	x30, x8, #32
  fc:	mul	x3, x2, x18
 100:	cset	x6, cs  // cs = hs, nlast
 104:	lsl	x10, x10, #32
 108:	adds	x1, x1, x30
 10c:	mul	x17, x15, x14
 110:	cinc	x18, x6, cs  // cs = hs, nlast
 114:	mul	x0, x9, x0
 118:	adds	x7, x10, x7
 11c:	cinc	x10, x1, cs  // cs = hs, nlast
 120:	adds	x3, x3, x17
 124:	mul	x1, x2, x9
 128:	cset	x6, cs  // cs = hs, nlast
 12c:	adds	x0, x0, x3
 130:	extr	x8, x13, x8, #32
 134:	adc	x1, x1, x6
 138:	mul	x2, x2, x15
 13c:	adds	x8, x16, x8
 140:	mul	x6, x9, x14
 144:	cset	x3, cs  // cs = hs, nlast
 148:	adds	x0, x0, x8
 14c:	adc	x1, x1, x3
 150:	adds	x2, x2, x6
 154:	cset	x8, cs  // cs = hs, nlast
 158:	add	w3, w12, w11
 15c:	lsl	x6, x2, #32
 160:	adds	x6, x6, x18
 164:	extr	x2, x8, x2, #32
 168:	cinc	x2, x2, cs  // cs = hs, nlast
 16c:	adds	x0, x0, x6
 170:	adc	x1, x1, x2
 174:	tbz	x1, #48, 304 <__multf3+0x304>
 178:	mov	w6, #0xffffc002            	// #-16382
 17c:	add	w2, w3, w6
 180:	mov	w3, #0x7ffe                	// #32766
 184:	cmp	w2, w3
 188:	b.gt	32c <__multf3+0x32c>
 18c:	cmp	w2, #0x0
 190:	b.le	3a0 <__multf3+0x3a0>
 194:	bfi	x1, x2, #48, #16
 198:	mov	x3, x0
 19c:	mov	x2, x1
 1a0:	mov	x6, #0x8000000000000000    	// #-9223372036854775808
 1a4:	orr	x0, x4, x3
 1a8:	cmp	x10, x6
 1ac:	orr	x1, x5, x2
 1b0:	b.hi	3c4 <__multf3+0x3c4>  // b.pmore
 1b4:	b.eq	3c0 <__multf3+0x3c0>  // b.none
 1b8:	cbnz	x7, 1d8 <__multf3+0x1d8>
 1bc:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
 1c0:	cmp	x10, x2
 1c4:	b.ne	1d8 <__multf3+0x1d8>  // b.any
 1c8:	adds	x3, x0, #0x1
 1cc:	cinc	x2, x1, cs  // cs = hs, nlast
 1d0:	and	x0, x3, #0xfffffffffffffffe
 1d4:	mov	x1, x2
 1d8:	fmov	d0, x0
 1dc:	ldp	x29, x30, [sp], #48
 1e0:	fmov	v0.d[1], x1
 1e4:	ret
 1e8:	and	x7, x2, #0x7fffffffffffffff
 1ec:	mov	x9, #0x7fff000000000000    	// #9223090561878065152
 1f0:	cmp	x7, x9
 1f4:	b.hi	2f0 <__multf3+0x2f0>  // b.pmore
 1f8:	b.eq	2e8 <__multf3+0x2e8>  // b.none
 1fc:	and	x9, x10, #0x7fffffffffffffff
 200:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
 204:	cmp	x9, x11
 208:	b.hi	368 <__multf3+0x368>  // b.pmore
 20c:	b.eq	364 <__multf3+0x364>  // b.none
 210:	cbz	x1, 340 <__multf3+0x340>
 214:	cbz	x6, 37c <__multf3+0x37c>
 218:	orr	x7, x1, x7
 21c:	cbz	x7, 3b0 <__multf3+0x3b0>
 220:	orr	x9, x6, x9
 224:	cbz	x9, 3b0 <__multf3+0x3b0>
 228:	tst	x2, #0x7fff000000000000
 22c:	mov	w3, #0x0                   	// #0
 230:	b.ne	284 <__multf3+0x284>  // b.any
 234:	cmp	x8, #0x0
 238:	mov	w7, #0x40                  	// #64
 23c:	csel	x0, x8, x1, ne  // ne = any
 240:	csel	w7, wzr, w7, ne  // ne = any
 244:	clz	x0, x0
 248:	mov	w2, #0x3f                  	// #63
 24c:	add	w7, w7, w0
 250:	lsr	x0, x1, #1
 254:	sub	w11, w7, #0xf
 258:	subs	w7, w7, #0x4f
 25c:	sub	w2, w2, w11
 260:	mov	w3, #0x1                   	// #1
 264:	lsl	x8, x8, x11
 268:	sub	w3, w3, w11
 26c:	lsr	x0, x0, x2
 270:	orr	x8, x0, x8
 274:	lsl	x0, x1, x7
 278:	csel	x8, x0, x8, pl  // pl = nfrst
 27c:	lsl	x1, x1, x11
 280:	csel	x0, xzr, x1, pl  // pl = nfrst
 284:	mov	w11, w3
 288:	tst	x10, #0x7fff000000000000
 28c:	b.ne	68 <__multf3+0x68>  // b.any
 290:	cmp	x14, #0x0
 294:	mov	w7, #0x40                  	// #64
 298:	csel	x1, x14, x6, ne  // ne = any
 29c:	csel	w7, wzr, w7, ne  // ne = any
 2a0:	clz	x1, x1
 2a4:	lsr	x9, x6, #1
 2a8:	add	w7, w7, w1
 2ac:	mov	w10, #0x3f                  	// #63
 2b0:	sub	w2, w7, #0xf
 2b4:	subs	w7, w7, #0x4f
 2b8:	sub	w10, w10, w2
 2bc:	sub	w3, w3, w2
 2c0:	lsl	x1, x14, x2
 2c4:	add	w11, w3, #0x1
 2c8:	lsr	x9, x9, x10
 2cc:	orr	x1, x9, x1
 2d0:	lsl	x9, x6, x7
 2d4:	csel	x1, x9, x1, pl  // pl = nfrst
 2d8:	lsl	x6, x6, x2
 2dc:	mov	x14, x1
 2e0:	csel	x18, xzr, x6, pl  // pl = nfrst
 2e4:	b	68 <__multf3+0x68>
 2e8:	cbz	x1, 1fc <__multf3+0x1fc>
 2ec:	nop
 2f0:	fmov	d0, x1
 2f4:	orr	x5, x2, #0x800000000000
 2f8:	fmov	v0.d[1], x5
 2fc:	ldp	x29, x30, [sp], #48
 300:	ret
 304:	mov	w2, #0xffffc001            	// #-16383
 308:	extr	x6, x10, x7, #63
 30c:	add	w2, w3, w2
 310:	extr	x1, x1, x0, #63
 314:	mov	w3, #0x7ffe                	// #32766
 318:	extr	x0, x0, x10, #63
 31c:	lsl	x7, x7, #1
 320:	mov	x10, x6
 324:	cmp	w2, w3
 328:	b.le	18c <__multf3+0x18c>
 32c:	fmov	d0, x4
 330:	orr	x1, x5, #0x7fff000000000000
 334:	ldp	x29, x30, [sp], #48
 338:	fmov	v0.d[1], x1
 33c:	ret
 340:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 344:	cmp	x7, x3
 348:	b.ne	214 <__multf3+0x214>  // b.any
 34c:	orr	x0, x6, x9
 350:	cbnz	x0, 390 <__multf3+0x390>
 354:	adrp	x0, 0 <__multf3>
 358:	add	x0, x0, #0x0
 35c:	ldr	q0, [x0]
 360:	b	2fc <__multf3+0x2fc>
 364:	cbz	x6, 210 <__multf3+0x210>
 368:	fmov	d0, x6
 36c:	orr	x1, x10, #0x800000000000
 370:	ldp	x29, x30, [sp], #48
 374:	fmov	v0.d[1], x1
 378:	ret
 37c:	mov	x3, #0x7fff000000000000    	// #9223090561878065152
 380:	cmp	x9, x3
 384:	b.ne	218 <__multf3+0x218>  // b.any
 388:	orr	x1, x1, x7
 38c:	cbz	x1, 354 <__multf3+0x354>
 390:	fmov	d0, x4
 394:	orr	x1, x5, x3
 398:	fmov	v0.d[1], x1
 39c:	b	2fc <__multf3+0x2fc>
 3a0:	mov	w6, #0x1                   	// #1
 3a4:	sub	w6, w6, w2
 3a8:	cmp	w6, #0x7f
 3ac:	b.ls	3d8 <__multf3+0x3d8>  // b.plast
 3b0:	fmov	d0, x4
 3b4:	ldp	x29, x30, [sp], #48
 3b8:	fmov	v0.d[1], x5
 3bc:	ret
 3c0:	cbz	x7, 1bc <__multf3+0x1bc>
 3c4:	adds	x2, x0, #0x1
 3c8:	cinc	x3, x1, cs  // cs = hs, nlast
 3cc:	mov	x0, x2
 3d0:	mov	x1, x3
 3d4:	b	1d8 <__multf3+0x1d8>
 3d8:	add	w3, w2, #0x7f
 3dc:	mov	w13, #0x3f                  	// #63
 3e0:	lsr	x8, x0, #1
 3e4:	sub	w16, w13, w3
 3e8:	adds	w2, w2, w13
 3ec:	lsl	x14, x10, #1
 3f0:	sub	w13, w13, w6
 3f4:	lsr	x8, x8, x16
 3f8:	lsl	x11, x1, x3
 3fc:	orr	x11, x8, x11
 400:	sub	w8, w6, #0x40
 404:	lsl	x9, x0, x2
 408:	lsl	x14, x14, x13
 40c:	lsr	x15, x7, #1
 410:	csel	x11, x9, x11, pl  // pl = nfrst
 414:	lsr	x12, x7, x6
 418:	lsl	x9, x0, x3
 41c:	orr	x12, x14, x12
 420:	csel	x9, xzr, x9, pl  // pl = nfrst
 424:	lsr	x14, x10, x8
 428:	cmp	w8, #0x0
 42c:	lsr	x15, x15, x16
 430:	csel	x12, x14, x12, ge  // ge = tcont
 434:	lsl	x14, x10, x3
 438:	lsr	x10, x10, x6
 43c:	orr	x14, x15, x14
 440:	csel	x10, xzr, x10, ge  // ge = tcont
 444:	cmp	w2, #0x0
 448:	lsl	x15, x7, x2
 44c:	csel	x14, x15, x14, ge  // ge = tcont
 450:	lsl	x7, x7, x3
 454:	csel	x7, xzr, x7, ge  // ge = tcont
 458:	orr	x7, x7, x14
 45c:	lsl	x2, x1, #1
 460:	cmp	x7, #0x0
 464:	lsr	x3, x0, x6
 468:	cset	x7, ne  // ne = any
 46c:	lsl	x13, x2, x13
 470:	cmp	w8, #0x0
 474:	orr	x3, x13, x3
 478:	orr	x9, x9, x12
 47c:	lsr	x0, x1, x8
 480:	lsr	x2, x1, x6
 484:	csel	x3, x0, x3, ge  // ge = tcont
 488:	orr	x10, x11, x10
 48c:	orr	x7, x9, x7
 490:	csel	x2, xzr, x2, ge  // ge = tcont
 494:	b	1a0 <__multf3+0x1a0>

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
   8:	cmp	x0, x2
   c:	mov	x29, sp
  10:	b.eq	78 <__mulvdi3+0x78>  // b.none
  14:	cmp	x1, x2
  18:	b.eq	64 <__mulvdi3+0x64>  // b.none
  1c:	asr	x5, x0, #63
  20:	asr	x3, x1, #63
  24:	eor	x4, x0, x5
  28:	eor	x6, x1, x3
  2c:	sub	x4, x4, x5
  30:	sub	x7, x6, x3
  34:	cmp	x4, #0x1
  38:	ccmp	x7, #0x1, #0x4, gt
  3c:	b.le	58 <__mulvdi3+0x58>
  40:	cmp	x5, x3
  44:	b.eq	8c <__mulvdi3+0x8c>  // b.none
  48:	sub	x3, x3, x6
  4c:	sdiv	x2, x2, x3
  50:	cmp	x2, x4
  54:	b.lt	b4 <__mulvdi3+0xb4>  // b.tstop
  58:	mul	x0, x0, x1
  5c:	ldp	x29, x30, [sp], #16
  60:	ret
  64:	cmp	x0, #0x1
  68:	b.hi	cc <__mulvdi3+0xcc>  // b.pmore
  6c:	lsl	x0, x0, #63
  70:	ldp	x29, x30, [sp], #16
  74:	ret
  78:	cmp	x1, #0x1
  7c:	b.hi	e4 <__mulvdi3+0xe4>  // b.pmore
  80:	lsl	x0, x1, #63
  84:	ldp	x29, x30, [sp], #16
  88:	ret
  8c:	mov	x2, #0x7fffffffffffffff    	// #9223372036854775807
  90:	sdiv	x7, x2, x7
  94:	cmp	x7, x4
  98:	b.ge	58 <__mulvdi3+0x58>  // b.tcont
  9c:	adrp	x2, 0 <__mulvdi3>
  a0:	adrp	x0, 0 <__mulvdi3>
  a4:	add	x2, x2, #0x0
  a8:	add	x0, x0, #0x0
  ac:	mov	w1, #0x29                  	// #41
  b0:	bl	0 <__compilerrt_abort_impl>
  b4:	adrp	x2, 0 <__mulvdi3>
  b8:	adrp	x0, 0 <__mulvdi3>
  bc:	add	x2, x2, #0x0
  c0:	add	x0, x0, #0x0
  c4:	mov	w1, #0x2c                  	// #44
  c8:	bl	0 <__compilerrt_abort_impl>
  cc:	adrp	x2, 0 <__mulvdi3>
  d0:	adrp	x0, 0 <__mulvdi3>
  d4:	add	x2, x2, #0x0
  d8:	add	x0, x0, #0x0
  dc:	mov	w1, #0x1f                  	// #31
  e0:	bl	0 <__compilerrt_abort_impl>
  e4:	adrp	x2, 0 <__mulvdi3>
  e8:	adrp	x0, 0 <__mulvdi3>
  ec:	add	x2, x2, #0x0
  f0:	add	x0, x0, #0x0
  f4:	mov	w1, #0x1a                  	// #26
  f8:	bl	0 <__compilerrt_abort_impl>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w2, #0x80000000            	// #-2147483648
   8:	cmp	w0, w2
   c:	mov	x29, sp
  10:	b.eq	78 <__mulvsi3+0x78>  // b.none
  14:	cmp	w1, w2
  18:	b.eq	64 <__mulvsi3+0x64>  // b.none
  1c:	asr	w5, w0, #31
  20:	asr	w3, w1, #31
  24:	eor	w4, w0, w5
  28:	eor	w6, w1, w3
  2c:	sub	w4, w4, w5
  30:	sub	w7, w6, w3
  34:	cmp	w4, #0x1
  38:	ccmp	w7, #0x1, #0x4, gt
  3c:	b.le	58 <__mulvsi3+0x58>
  40:	cmp	w5, w3
  44:	b.eq	8c <__mulvsi3+0x8c>  // b.none
  48:	sub	w3, w3, w6
  4c:	sdiv	w2, w2, w3
  50:	cmp	w2, w4
  54:	b.lt	b4 <__mulvsi3+0xb4>  // b.tstop
  58:	mul	w0, w0, w1
  5c:	ldp	x29, x30, [sp], #16
  60:	ret
  64:	cmp	w0, #0x1
  68:	b.hi	cc <__mulvsi3+0xcc>  // b.pmore
  6c:	lsl	w0, w0, #31
  70:	ldp	x29, x30, [sp], #16
  74:	ret
  78:	cmp	w1, #0x1
  7c:	b.hi	e4 <__mulvsi3+0xe4>  // b.pmore
  80:	lsl	w0, w1, #31
  84:	ldp	x29, x30, [sp], #16
  88:	ret
  8c:	mov	w2, #0x7fffffff            	// #2147483647
  90:	sdiv	w7, w2, w7
  94:	cmp	w7, w4
  98:	b.ge	58 <__mulvsi3+0x58>  // b.tcont
  9c:	adrp	x2, 0 <__mulvsi3>
  a0:	adrp	x0, 0 <__mulvsi3>
  a4:	add	x2, x2, #0x0
  a8:	add	x0, x0, #0x0
  ac:	mov	w1, #0x29                  	// #41
  b0:	bl	0 <__compilerrt_abort_impl>
  b4:	adrp	x2, 0 <__mulvsi3>
  b8:	adrp	x0, 0 <__mulvsi3>
  bc:	add	x2, x2, #0x0
  c0:	add	x0, x0, #0x0
  c4:	mov	w1, #0x2c                  	// #44
  c8:	bl	0 <__compilerrt_abort_impl>
  cc:	adrp	x2, 0 <__mulvsi3>
  d0:	adrp	x0, 0 <__mulvsi3>
  d4:	add	x2, x2, #0x0
  d8:	add	x0, x0, #0x0
  dc:	mov	w1, #0x1f                  	// #31
  e0:	bl	0 <__compilerrt_abort_impl>
  e4:	adrp	x2, 0 <__mulvsi3>
  e8:	adrp	x0, 0 <__mulvsi3>
  ec:	add	x2, x2, #0x0
  f0:	add	x0, x0, #0x0
  f4:	mov	w1, #0x1a                  	// #26
  f8:	bl	0 <__compilerrt_abort_impl>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x20, x3
  14:	stp	x23, x24, [sp, #48]
  18:	mov	x24, x1
  1c:	mov	x23, x2
  20:	cbz	x0, c8 <__mulvti3+0xc8>
  24:	cbz	x23, f8 <__mulvti3+0xf8>
  28:	asr	x5, x24, #63
  2c:	asr	x4, x20, #63
  30:	stp	x21, x22, [sp, #32]
  34:	eor	x22, x19, x5
  38:	subs	x22, x22, x5
  3c:	eor	x21, x24, x5
  40:	eor	x6, x23, x4
  44:	sbc	x21, x21, x5
  48:	eor	x8, x20, x4
  4c:	subs	x2, x6, x4
  50:	mov	w7, #0x1                   	// #1
  54:	sbc	x3, x8, x4
  58:	cmp	x21, #0x0
  5c:	b.le	138 <__mulvti3+0x138>
  60:	mov	w7, #0x0                   	// #0
  64:	cmp	x3, #0x0
  68:	mov	w0, #0x1                   	// #1
  6c:	b.le	128 <__mulvti3+0x128>
  70:	mov	w0, #0x0                   	// #0
  74:	orr	w7, w7, w0
  78:	tbnz	w7, #0, a4 <__mulvti3+0xa4>
  7c:	cmp	x5, x4
  80:	b.eq	148 <__mulvti3+0x148>  // b.none
  84:	subs	x2, x4, x6
  88:	mov	x0, #0x0                   	// #0
  8c:	sbc	x3, x4, x8
  90:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  94:	bl	0 <__divti3>
  98:	cmp	x21, x1
  9c:	b.gt	188 <__mulvti3+0x188>
  a0:	b.eq	180 <__mulvti3+0x180>  // b.none
  a4:	umulh	x1, x19, x23
  a8:	madd	x1, x24, x23, x1
  ac:	mul	x0, x19, x23
  b0:	madd	x1, x19, x20, x1
  b4:	ldp	x19, x20, [sp, #16]
  b8:	ldp	x21, x22, [sp, #32]
  bc:	ldp	x23, x24, [sp, #48]
  c0:	ldp	x29, x30, [sp], #64
  c4:	ret
  c8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  cc:	cmp	x1, x0
  d0:	b.ne	24 <__mulvti3+0x24>  // b.any
  d4:	cbnz	x3, 1a0 <__mulvti3+0x1a0>
  d8:	cmp	x2, #0x1
  dc:	b.hi	1a0 <__mulvti3+0x1a0>  // b.pmore
  e0:	lsl	x1, x2, #63
  e4:	mov	x0, #0x0                   	// #0
  e8:	ldp	x19, x20, [sp, #16]
  ec:	ldp	x23, x24, [sp, #48]
  f0:	ldp	x29, x30, [sp], #64
  f4:	ret
  f8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  fc:	cmp	x20, x0
 100:	b.ne	28 <__mulvti3+0x28>  // b.any
 104:	cbnz	x24, 1bc <__mulvti3+0x1bc>
 108:	cmp	x19, #0x1
 10c:	b.hi	1bc <__mulvti3+0x1bc>  // b.pmore
 110:	lsl	x1, x19, #63
 114:	mov	x0, #0x0                   	// #0
 118:	ldp	x19, x20, [sp, #16]
 11c:	ldp	x23, x24, [sp, #48]
 120:	ldp	x29, x30, [sp], #64
 124:	ret
 128:	b.ne	74 <__mulvti3+0x74>  // b.any
 12c:	cmp	x2, #0x1
 130:	b.ls	74 <__mulvti3+0x74>  // b.plast
 134:	b	70 <__mulvti3+0x70>
 138:	b.ne	64 <__mulvti3+0x64>  // b.any
 13c:	cmp	x22, #0x1
 140:	b.ls	64 <__mulvti3+0x64>  // b.plast
 144:	b	60 <__mulvti3+0x60>
 148:	mov	x0, #0xffffffffffffffff    	// #-1
 14c:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 150:	bl	0 <__divti3>
 154:	cmp	x21, x1
 158:	b.gt	168 <__mulvti3+0x168>
 15c:	b.ne	a4 <__mulvti3+0xa4>  // b.any
 160:	cmp	x22, x0
 164:	b.ls	a4 <__mulvti3+0xa4>  // b.plast
 168:	adrp	x2, 0 <__mulvti3>
 16c:	adrp	x0, 0 <__mulvti3>
 170:	add	x2, x2, #0x0
 174:	add	x0, x0, #0x0
 178:	mov	w1, #0x2b                  	// #43
 17c:	bl	0 <__compilerrt_abort_impl>
 180:	cmp	x22, x0
 184:	b.ls	a4 <__mulvti3+0xa4>  // b.plast
 188:	adrp	x2, 0 <__mulvti3>
 18c:	adrp	x0, 0 <__mulvti3>
 190:	add	x2, x2, #0x0
 194:	add	x0, x0, #0x0
 198:	mov	w1, #0x2e                  	// #46
 19c:	bl	0 <__compilerrt_abort_impl>
 1a0:	adrp	x2, 0 <__mulvti3>
 1a4:	adrp	x0, 0 <__mulvti3>
 1a8:	add	x2, x2, #0x0
 1ac:	add	x0, x0, #0x0
 1b0:	mov	w1, #0x1c                  	// #28
 1b4:	stp	x21, x22, [sp, #32]
 1b8:	bl	0 <__compilerrt_abort_impl>
 1bc:	adrp	x2, 0 <__mulvti3>
 1c0:	adrp	x0, 0 <__mulvti3>
 1c4:	add	x2, x2, #0x0
 1c8:	add	x0, x0, #0x0
 1cc:	mov	w1, #0x21                  	// #33
 1d0:	stp	x21, x22, [sp, #32]
 1d4:	bl	0 <__compilerrt_abort_impl>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
   4:	fmov	d1, x0
   8:	add	d0, d0, d1
   c:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	mov	w0, #0x80000000            	// #-2147483648
   4:	fmov	s1, w0
   8:	add	v0.2s, v0.2s, v1.2s
   c:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngc	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.eq	14 <__negvdi2+0x14>  // b.none
   c:	neg	x0, x0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	adrp	x2, 0 <__negvdi2>
  1c:	adrp	x0, 0 <__negvdi2>
  20:	mov	x29, sp
  24:	add	x2, x2, #0x0
  28:	add	x0, x0, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.eq	14 <__negvsi2+0x14>  // b.none
   c:	neg	w0, w0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	adrp	x2, 0 <__negvsi2>
  1c:	adrp	x0, 0 <__negvsi2>
  20:	mov	x29, sp
  24:	add	x2, x2, #0x0
  28:	add	x0, x0, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	cbz	x0, 10 <__negvti2+0x10>
   4:	negs	x0, x0
   8:	ngc	x1, x1
   c:	ret
  10:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  14:	cmp	x1, x2
  18:	b.ne	4 <__negvti2+0x4>  // b.any
  1c:	stp	x29, x30, [sp, #-16]!
  20:	adrp	x2, 0 <__negvti2>
  24:	adrp	x0, 0 <__negvti2>
  28:	mov	x29, sp
  2c:	add	x2, x2, #0x0
  30:	add	x0, x0, #0x0
  34:	mov	w1, #0x18                  	// #24
  38:	bl	0 <__compilerrt_abort_impl>

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	lsr	x1, x0, #32
   4:	eor	w0, w1, w0
   8:	b	0 <__paritysi2>

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w0, w0, w0, lsr #16
   4:	mov	w1, #0x6996                	// #27030
   8:	eor	w0, w0, w0, lsr #8
   c:	eor	w0, w0, w0, lsr #4
  10:	and	w0, w0, #0xf
  14:	asr	w0, w1, w0
  18:	and	w0, w0, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x0, x1, x0
   4:	b	0 <__paritydi2>

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	and	x1, x1, #0x5555555555555555
   8:	sub	x1, x0, x1
   c:	and	x2, x1, #0x3333333333333333
  10:	lsr	x0, x1, #2
  14:	and	x0, x0, #0x3333333333333333
  18:	add	x0, x0, x2
  1c:	add	x0, x0, x0, lsr #4
  20:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  24:	lsr	x1, x0, #32
  28:	add	w0, w1, w0
  2c:	add	w0, w0, w0, lsr #16
  30:	add	w0, w0, w0, lsr #8
  34:	and	w0, w0, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w1, w0, #1
   4:	and	w1, w1, #0x55555555
   8:	sub	w1, w0, w1
   c:	and	w2, w1, #0x33333333
  10:	lsr	w0, w1, #2
  14:	and	w0, w0, #0x33333333
  18:	add	w0, w0, w2
  1c:	add	w0, w0, w0, lsr #4
  20:	and	w0, w0, #0xf0f0f0f
  24:	add	w0, w0, w0, lsr #16
  28:	add	w0, w0, w0, lsr #8
  2c:	and	w0, w0, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x3, x1, #1
   4:	extr	x2, x1, x0, #1
   8:	and	x2, x2, #0x5555555555555555
   c:	subs	x0, x0, x2
  10:	and	x2, x3, #0x5555555555555555
  14:	sbc	x1, x1, x2
  18:	and	x3, x0, #0x3333333333333333
  1c:	and	x4, x1, #0x3333333333333333
  20:	extr	x0, x1, x0, #2
  24:	and	x2, x0, #0x3333333333333333
  28:	lsr	x0, x1, #2
  2c:	and	x0, x0, #0x3333333333333333
  30:	adds	x1, x2, x3
  34:	adc	x0, x0, x4
  38:	extr	x2, x0, x1, #4
  3c:	adds	x1, x2, x1
  40:	lsr	x2, x0, #4
  44:	adc	x0, x0, x2
  48:	and	x1, x1, #0xf0f0f0f0f0f0f0f
  4c:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  50:	add	x0, x0, x1
  54:	lsr	x1, x0, #32
  58:	add	w0, w1, w0
  5c:	add	w0, w0, w0, lsr #16
  60:	add	w0, w0, w0, lsr #8
  64:	and	w0, w0, #0xff
  68:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	fmov	d1, d0
   4:	mov	w1, w0
   8:	fmov	d0, #1.000000000000000000e+00
   c:	b	14 <__powidf2+0x14>
  10:	fmul	d1, d1, d1
  14:	add	w2, w1, w1, lsr #31
  18:	tbz	w1, #0, 20 <__powidf2+0x20>
  1c:	fmul	d0, d0, d1
  20:	cmp	wzr, w2, asr #1
  24:	asr	w1, w2, #1
  28:	b.ne	10 <__powidf2+0x10>  // b.any
  2c:	tbnz	w0, #31, 34 <__powidf2+0x34>
  30:	ret
  34:	fmov	d1, #1.000000000000000000e+00
  38:	fdiv	d0, d1, d0
  3c:	ret

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	fmov	s1, s0
   4:	mov	w1, w0
   8:	fmov	s0, #1.000000000000000000e+00
   c:	b	14 <__powisf2+0x14>
  10:	fmul	s1, s1, s1
  14:	add	w2, w1, w1, lsr #31
  18:	tbz	w1, #0, 20 <__powisf2+0x20>
  1c:	fmul	s0, s0, s1
  20:	cmp	wzr, w2, asr #1
  24:	asr	w1, w2, #1
  28:	b.ne	10 <__powisf2+0x10>  // b.any
  2c:	tbnz	w0, #31, 34 <__powisf2+0x34>
  30:	ret
  34:	fmov	s1, #1.000000000000000000e+00
  38:	fdiv	s0, s1, s0
  3c:	ret

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
   4:	fmov	d2, x0
   8:	add	d1, d1, d2
   c:	b	0 <__adddf3>

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	movi	v2.2s, #0x80, lsl #24
   4:	add	v1.2s, v1.2s, v2.2s
   8:	b	0 <__addsf3>

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x2, x0
   8:	sub	x0, x0, x1
   c:	mov	x29, sp
  10:	tbnz	x1, #63, 24 <__subvdi3+0x24>
  14:	cmp	x2, x0
  18:	b.lt	44 <__subvdi3+0x44>  // b.tstop
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	cmp	x2, x0
  28:	b.lt	1c <__subvdi3+0x1c>  // b.tstop
  2c:	adrp	x2, 0 <__subvdi3>
  30:	adrp	x0, 0 <__subvdi3>
  34:	add	x2, x2, #0x0
  38:	add	x0, x0, #0x0
  3c:	mov	w1, #0x1a                  	// #26
  40:	bl	0 <__compilerrt_abort_impl>
  44:	adrp	x2, 0 <__subvdi3>
  48:	adrp	x0, 0 <__subvdi3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x17                  	// #23
  58:	bl	0 <__compilerrt_abort_impl>

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w2, w0
   8:	sub	w0, w0, w1
   c:	mov	x29, sp
  10:	tbnz	w1, #31, 24 <__subvsi3+0x24>
  14:	cmp	w2, w0
  18:	b.lt	44 <__subvsi3+0x44>  // b.tstop
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	cmp	w2, w0
  28:	b.lt	1c <__subvsi3+0x1c>  // b.tstop
  2c:	adrp	x2, 0 <__subvsi3>
  30:	adrp	x0, 0 <__subvsi3>
  34:	add	x2, x2, #0x0
  38:	add	x0, x0, #0x0
  3c:	mov	w1, #0x1a                  	// #26
  40:	bl	0 <__compilerrt_abort_impl>
  44:	adrp	x2, 0 <__subvsi3>
  48:	adrp	x0, 0 <__subvsi3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x17                  	// #23
  58:	bl	0 <__compilerrt_abort_impl>

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x4, x0
   8:	subs	x0, x0, x2
   c:	mov	x29, sp
  10:	mov	x2, x1
  14:	sbc	x1, x1, x3
  18:	tbnz	x3, #63, 30 <__subvti3+0x30>
  1c:	cmp	x1, x2
  20:	b.gt	64 <__subvti3+0x64>
  24:	b.eq	5c <__subvti3+0x5c>  // b.none
  28:	ldp	x29, x30, [sp], #16
  2c:	ret
  30:	cmp	x1, x2
  34:	b.gt	28 <__subvti3+0x28>
  38:	b.ne	44 <__subvti3+0x44>  // b.any
  3c:	cmp	x0, x4
  40:	b.hi	28 <__subvti3+0x28>  // b.pmore
  44:	adrp	x2, 0 <__subvti3>
  48:	adrp	x0, 0 <__subvti3>
  4c:	add	x2, x2, #0x0
  50:	add	x0, x0, #0x0
  54:	mov	w1, #0x1c                  	// #28
  58:	bl	0 <__compilerrt_abort_impl>
  5c:	cmp	x0, x4
  60:	b.ls	28 <__subvti3+0x28>  // b.plast
  64:	adrp	x2, 0 <__subvti3>
  68:	adrp	x0, 0 <__subvti3>
  6c:	add	x2, x2, #0x0
  70:	add	x0, x0, #0x0
  74:	mov	w1, #0x19                  	// #25
  78:	bl	0 <__compilerrt_abort_impl>

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	sub	sp, sp, #0x10
   4:	str	q1, [sp]
   8:	ldp	x3, x2, [sp]
   c:	add	sp, sp, #0x10
  10:	fmov	d1, x3
  14:	eor	x1, x2, #0x8000000000000000
  18:	fmov	v1.d[1], x1
  1c:	b	0 <__addtf3>

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	fmov	x1, d0
   4:	mov	x3, #0xc0f0000000000000    	// #-4544132024016830464
   8:	mov	x0, #0xbf10000000000000    	// #-4679240012837945344
   c:	and	x2, x1, #0x7fffffffffffffff
  10:	add	x3, x2, x3
  14:	add	x0, x2, x0
  18:	cmp	x3, x0
  1c:	b.cs	50 <__truncdfhf2+0x50>  // b.hs, b.nlast
  20:	and	x3, x1, #0x3ffffffffff
  24:	mov	x0, #0x20000000000         	// #2199023255552
  28:	ubfx	x2, x2, #42, #16
  2c:	cmp	x3, x0
  30:	b.ls	e8 <__truncdfhf2+0xe8>  // b.plast
  34:	mov	w4, #0x4001                	// #16385
  38:	lsr	x1, x1, #48
  3c:	add	w2, w2, w4
  40:	and	x1, x1, #0x8000
  44:	and	w0, w2, #0xffff
  48:	orr	w0, w0, w1
  4c:	ret
  50:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  54:	cmp	x2, x0
  58:	b.ls	74 <__truncdfhf2+0x74>  // b.plast
  5c:	ubfx	x2, x2, #42, #9
  60:	orr	w0, w2, #0x7e00
  64:	lsr	x1, x1, #48
  68:	and	x1, x1, #0x8000
  6c:	orr	w0, w0, w1
  70:	ret
  74:	mov	x3, #0x40efffffffffffff    	// #4679240012837945343
  78:	mov	w0, #0x7c00                	// #31744
  7c:	cmp	x2, x3
  80:	b.hi	64 <__truncdfhf2+0x64>  // b.pmore
  84:	lsr	x3, x2, #52
  88:	mov	w2, #0x3f1                 	// #1009
  8c:	sub	w4, w2, w3
  90:	mov	w0, #0x0                   	// #0
  94:	cmp	w4, #0x34
  98:	b.gt	64 <__truncdfhf2+0x64>
  9c:	and	x0, x1, #0xfffffffffffff
  a0:	sub	w2, w3, #0x3b1
  a4:	orr	x0, x0, #0x10000000000000
  a8:	mov	x5, #0x20000000000         	// #2199023255552
  ac:	lsl	x2, x0, x2
  b0:	cmp	x2, #0x0
  b4:	cset	x3, ne  // ne = any
  b8:	lsr	x2, x0, x4
  bc:	ubfx	x0, x2, #42, #16
  c0:	orr	x3, x3, x2
  c4:	and	x3, x3, #0x3ffffffffff
  c8:	add	w2, w0, #0x1
  cc:	cmp	x3, x5
  d0:	and	w2, w2, #0xfffe
  d4:	add	w4, w0, #0x1
  d8:	csel	w2, w2, w0, eq  // eq = none
  dc:	and	w0, w4, #0xffff
  e0:	csel	w0, w2, w0, ls  // ls = plast
  e4:	b	64 <__truncdfhf2+0x64>
  e8:	add	w0, w2, #0x4, lsl #12
  ec:	mov	w3, #0x4001                	// #16385
  f0:	add	w2, w2, w3
  f4:	and	w0, w0, #0xffff
  f8:	and	w2, w2, #0xfffe
  fc:	lsr	x1, x1, #48
 100:	csel	w0, w2, w0, eq  // eq = none
 104:	and	x1, x1, #0x8000
 108:	orr	w0, w0, w1
 10c:	ret

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	fmov	x0, d0
   4:	mov	x3, #0xc7f0000000000000    	// #-4039728865751334912
   8:	mov	x2, #0xb810000000000000    	// #-5183643171103440896
   c:	and	x1, x0, #0x7fffffffffffffff
  10:	add	x3, x1, x3
  14:	add	x2, x1, x2
  18:	cmp	x3, x2
  1c:	b.cs	54 <__truncdfsf2+0x54>  // b.hs, b.nlast
  20:	and	x3, x0, #0x1fffffff
  24:	lsr	x1, x1, #29
  28:	mov	x2, #0x10000000            	// #268435456
  2c:	cmp	x3, x2
  30:	mov	w3, #0x1                   	// #1
  34:	movk	w3, #0x4000, lsl #16
  38:	add	w3, w3, w1
  3c:	b.ls	f0 <__truncdfsf2+0xf0>  // b.plast
  40:	lsr	x0, x0, #32
  44:	and	x0, x0, #0x80000000
  48:	orr	w0, w3, w0
  4c:	fmov	s0, w0
  50:	ret
  54:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  58:	cmp	x1, x2
  5c:	b.ls	7c <__truncdfsf2+0x7c>  // b.plast
  60:	ubfx	x1, x1, #29, #22
  64:	lsr	x0, x0, #32
  68:	orr	w3, w1, #0x7fc00000
  6c:	and	x0, x0, #0x80000000
  70:	orr	w0, w3, w0
  74:	fmov	s0, w0
  78:	ret
  7c:	mov	x2, #0x47efffffffffffff    	// #5183643171103440895
  80:	mov	w3, #0x7f800000            	// #2139095040
  84:	cmp	x1, x2
  88:	b.hi	40 <__truncdfsf2+0x40>  // b.pmore
  8c:	lsr	x2, x1, #52
  90:	mov	w4, #0x381                 	// #897
  94:	sub	w4, w4, w2
  98:	mov	w3, #0x0                   	// #0
  9c:	cmp	w4, #0x34
  a0:	b.gt	40 <__truncdfsf2+0x40>
  a4:	and	x3, x0, #0xfffffffffffff
  a8:	sub	w1, w2, #0x341
  ac:	orr	x2, x3, #0x10000000000000
  b0:	mov	x5, #0x10000000            	// #268435456
  b4:	lsl	x1, x2, x1
  b8:	cmp	x1, #0x0
  bc:	lsr	x2, x2, x4
  c0:	cset	x4, ne  // ne = any
  c4:	orr	x4, x4, x2
  c8:	lsr	x2, x2, #29
  cc:	add	w1, w2, #0x1
  d0:	and	x4, x4, #0x1fffffff
  d4:	mov	w3, w2
  d8:	cmp	x4, x5
  dc:	and	w1, w1, #0xfffffffe
  e0:	add	w2, w2, #0x1
  e4:	csel	w1, w1, w3, eq  // eq = none
  e8:	csel	w3, w1, w2, ls  // ls = plast
  ec:	b	40 <__truncdfsf2+0x40>
  f0:	mov	w2, #0x40000000            	// #1073741824
  f4:	and	w3, w3, #0xfffffffe
  f8:	add	w1, w2, w1
  fc:	lsr	x0, x0, #32
 100:	csel	w3, w3, w1, eq  // eq = none
 104:	and	x0, x0, #0x80000000
 108:	orr	w0, w3, w0
 10c:	fmov	s0, w0
 110:	ret

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	fmov	w1, s0
   4:	mov	w3, #0xc7800000            	// #-947912704
   8:	mov	w0, #0xb8800000            	// #-1199570944
   c:	and	w2, w1, #0x7fffffff
  10:	add	w3, w2, w3
  14:	add	w0, w2, w0
  18:	cmp	w3, w0
  1c:	b.cs	4c <__truncsfhf2+0x4c>  // b.hs, b.nlast
  20:	and	w0, w1, #0x1fff
  24:	ubfx	x2, x2, #13, #16
  28:	cmp	w0, #0x1, lsl #12
  2c:	b.ls	e8 <__truncsfhf2+0xe8>  // b.plast
  30:	mov	w4, #0x4001                	// #16385
  34:	lsr	w1, w1, #16
  38:	add	w2, w2, w4
  3c:	and	w1, w1, #0x8000
  40:	and	w0, w2, #0xffff
  44:	orr	w0, w0, w1
  48:	ret
  4c:	mov	w0, #0x7f800000            	// #2139095040
  50:	cmp	w2, w0
  54:	b.ls	70 <__truncsfhf2+0x70>  // b.plast
  58:	ubfx	x2, x2, #13, #9
  5c:	orr	w0, w2, #0x7e00
  60:	lsr	w1, w1, #16
  64:	and	w1, w1, #0x8000
  68:	orr	w0, w0, w1
  6c:	ret
  70:	mov	w3, #0x477fffff            	// #1199570943
  74:	mov	w0, #0x7c00                	// #31744
  78:	cmp	w2, w3
  7c:	b.hi	60 <__truncsfhf2+0x60>  // b.pmore
  80:	lsr	w3, w2, #23
  84:	mov	w2, #0x71                  	// #113
  88:	sub	w4, w2, w3
  8c:	mov	w0, #0x0                   	// #0
  90:	cmp	w4, #0x17
  94:	b.gt	60 <__truncsfhf2+0x60>
  98:	and	w0, w1, #0x7fffff
  9c:	sub	w2, w3, #0x51
  a0:	orr	w0, w0, #0x800000
  a4:	mov	w5, #0x1001                	// #4097
  a8:	lsl	w2, w0, w2
  ac:	cmp	w2, #0x0
  b0:	cset	w3, ne  // ne = any
  b4:	lsr	w2, w0, w4
  b8:	ubfx	x0, x2, #13, #16
  bc:	orr	w3, w3, w2
  c0:	and	w3, w3, #0x1fff
  c4:	add	w2, w0, #0x1
  c8:	cmp	w3, #0x1, lsl #12
  cc:	and	w2, w2, #0xfffe
  d0:	add	w4, w0, #0x1
  d4:	csel	w2, w2, w0, eq  // eq = none
  d8:	and	w0, w4, #0xffff
  dc:	cmp	w3, w5
  e0:	csel	w0, w2, w0, cc  // cc = lo, ul, last
  e4:	b	60 <__truncsfhf2+0x60>
  e8:	add	w0, w2, #0x4, lsl #12
  ec:	mov	w3, #0x4001                	// #16385
  f0:	add	w2, w2, w3
  f4:	and	w0, w0, #0xffff
  f8:	and	w2, w2, #0xfffe
  fc:	lsr	w1, w1, #16
 100:	csel	w0, w2, w0, eq  // eq = none
 104:	and	w1, w1, #0x8000
 108:	orr	w0, w0, w1
 10c:	ret

0000000000000110 <__gnu_f2h_ieee>:
 110:	b	0 <__truncsfhf2>

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	lsr	x3, x0, #32
   4:	mov	x2, x0
   8:	lsr	x0, x1, #32
   c:	cmp	w3, w0
  10:	b.cc	34 <__ucmpdi2+0x34>  // b.lo, b.ul, b.last
  14:	mov	w0, #0x2                   	// #2
  18:	b.hi	30 <__ucmpdi2+0x30>  // b.pmore
  1c:	cmp	w2, w1
  20:	mov	w3, #0x0                   	// #0
  24:	cset	w0, hi  // hi = pmore
  28:	add	w0, w0, #0x1
  2c:	csel	w0, w0, w3, cs  // cs = hs, nlast
  30:	ret
  34:	mov	w0, #0x0                   	// #0
  38:	ret

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.cc	2c <__ucmpti2+0x2c>  // b.lo, b.ul, b.last
  10:	mov	w0, #0x2                   	// #2
  14:	b.hi	2c <__ucmpti2+0x2c>  // b.pmore
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	mov	x2, #0x0                   	// #0
   4:	b	0 <__udivmoddi4>

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	lsr	x11, x0, #32
   4:	lsr	x4, x1, #32
   8:	mov	x9, x0
   c:	mov	w5, w11
  10:	mov	w10, w4
  14:	cbnz	x11, 34 <__udivmoddi4+0x34>
  18:	cbnz	x4, 130 <__udivmoddi4+0x130>
  1c:	udiv	w3, w0, w1
  20:	cbz	x2, 2c <__udivmoddi4+0x2c>
  24:	msub	w1, w3, w1, w0
  28:	str	x1, [x2]
  2c:	mov	w0, w3
  30:	ret
  34:	cbnz	w1, e4 <__udivmoddi4+0xe4>
  38:	cbz	x4, 198 <__udivmoddi4+0x198>
  3c:	cbz	w0, 1e0 <__udivmoddi4+0x1e0>
  40:	sub	w0, w4, #0x1
  44:	tst	w0, w4
  48:	b.eq	1f8 <__udivmoddi4+0x1f8>  // b.none
  4c:	clz	w4, w4
  50:	clz	w0, w11
  54:	sub	w4, w4, w0
  58:	cmp	w4, #0x1e
  5c:	b.hi	19c <__udivmoddi4+0x19c>  // b.pmore
  60:	add	w4, w4, #0x1
  64:	mov	w7, #0x20                  	// #32
  68:	sub	w7, w7, w4
  6c:	mov	w10, #0x0                   	// #0
  70:	lsl	w5, w11, w7
  74:	lsr	w0, w9, w4
  78:	orr	w5, w5, w0
  7c:	lsl	w7, w9, w7
  80:	lsr	w6, w11, w4
  84:	mov	w0, #0x0                   	// #0
  88:	b	94 <__udivmoddi4+0x94>
  8c:	lsr	x6, x3, #32
  90:	mov	w5, w3
  94:	subs	w4, w4, #0x1
  98:	extr	w6, w6, w5, #31
  9c:	extr	w5, w5, w7, #31
  a0:	bfi	x3, x6, #32, #32
  a4:	extr	w7, w7, w10, #31
  a8:	orr	w10, w0, w10, lsl #1
  ac:	bfxil	x3, x5, #0, #32
  b0:	mvn	x0, x3
  b4:	add	x0, x0, x1
  b8:	asr	x0, x0, #63
  bc:	and	x5, x0, x1
  c0:	and	w0, w0, #0x1
  c4:	sub	x3, x3, x5
  c8:	b.ne	8c <__udivmoddi4+0x8c>  // b.any
  cc:	bfi	x8, x7, #32, #32
  d0:	bfxil	x8, x10, #0, #32
  d4:	bfi	x0, x8, #1, #63
  d8:	cbz	x2, 30 <__udivmoddi4+0x30>
  dc:	str	x3, [x2]
  e0:	ret
  e4:	cbz	x4, 144 <__udivmoddi4+0x144>
  e8:	clz	w4, w4
  ec:	clz	w0, w11
  f0:	sub	w4, w4, w0
  f4:	cmp	w4, #0x1f
  f8:	b.hi	19c <__udivmoddi4+0x19c>  // b.pmore
  fc:	add	w4, w4, #0x1
 100:	mov	w7, w9
 104:	mov	x8, #0x0                   	// #0
 108:	b.eq	220 <__udivmoddi4+0x220>  // b.none
 10c:	mov	w7, #0x20                  	// #32
 110:	sub	w7, w7, w4
 114:	lsr	w0, w9, w4
 118:	mov	w10, #0x0                   	// #0
 11c:	lsl	w5, w11, w7
 120:	orr	w5, w5, w0
 124:	lsl	w7, w9, w7
 128:	lsr	w6, w11, w4
 12c:	b	84 <__udivmoddi4+0x84>
 130:	mov	x0, #0x0                   	// #0
 134:	cbz	x2, 30 <__udivmoddi4+0x30>
 138:	and	x9, x9, #0xffffffff
 13c:	str	x9, [x2]
 140:	ret
 144:	sub	w0, w1, #0x1
 148:	tst	w0, w1
 14c:	b.ne	1ac <__udivmoddi4+0x1ac>  // b.any
 150:	cbz	x2, 15c <__udivmoddi4+0x15c>
 154:	and	w0, w0, w9
 158:	str	x0, [x2]
 15c:	mov	x0, x9
 160:	cmp	w1, #0x1
 164:	b.eq	30 <__udivmoddi4+0x30>  // b.none
 168:	rbit	w1, w1
 16c:	mov	x8, #0x0                   	// #0
 170:	clz	w1, w1
 174:	neg	w2, w1
 178:	lsr	w3, w11, w1
 17c:	lsr	w0, w9, w1
 180:	bfi	x8, x3, #32, #32
 184:	lsl	w1, w11, w2
 188:	orr	w0, w1, w0
 18c:	bfxil	x8, x0, #0, #32
 190:	mov	x0, x8
 194:	ret
 198:	brk	#0x3e8
 19c:	mov	x0, #0x0                   	// #0
 1a0:	cbz	x2, 30 <__udivmoddi4+0x30>
 1a4:	str	x9, [x2]
 1a8:	ret
 1ac:	clz	w4, w11
 1b0:	clz	w0, w1
 1b4:	sub	w0, w0, w4
 1b8:	mov	w7, w9
 1bc:	add	w4, w0, #0x21
 1c0:	mov	w6, #0x0                   	// #0
 1c4:	cmp	w4, #0x20
 1c8:	b.eq	84 <__udivmoddi4+0x84>  // b.none
 1cc:	cmp	w4, #0x1f
 1d0:	b.hi	22c <__udivmoddi4+0x22c>  // b.pmore
 1d4:	mov	w7, #0x20                  	// #32
 1d8:	sub	w7, w7, w4
 1dc:	b	70 <__udivmoddi4+0x70>
 1e0:	udiv	w0, w11, w4
 1e4:	cbz	x2, 30 <__udivmoddi4+0x30>
 1e8:	msub	w1, w0, w4, w11
 1ec:	lsl	x1, x1, #32
 1f0:	str	x1, [x2]
 1f4:	ret
 1f8:	cbz	x2, 210 <__udivmoddi4+0x210>
 1fc:	mov	x3, #0x0                   	// #0
 200:	and	w0, w0, w11
 204:	bfxil	x3, x9, #0, #32
 208:	bfi	x3, x0, #32, #32
 20c:	str	x3, [x2]
 210:	rbit	w0, w4
 214:	clz	w0, w0
 218:	lsr	w0, w11, w0
 21c:	ret
 220:	mov	w6, #0x0                   	// #0
 224:	mov	w10, #0x0                   	// #0
 228:	b	84 <__udivmoddi4+0x84>
 22c:	add	w0, w0, #0x1
 230:	mov	w5, #0x40                  	// #64
 234:	sub	w5, w5, w4
 238:	mov	w6, w10
 23c:	lsr	w10, w9, w0
 240:	lsl	w7, w11, w5
 244:	orr	w7, w7, w10
 248:	lsl	w10, w9, w5
 24c:	lsr	w5, w11, w0
 250:	b	84 <__udivmoddi4+0x84>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w20, w1
  10:	mov	x19, x2
  14:	str	x21, [sp, #32]
  18:	mov	w21, w0
  1c:	bl	0 <__udivsi3>
  20:	msub	w1, w0, w20, w21
  24:	ldr	x21, [sp, #32]
  28:	str	w1, [x19]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	mov	x8, x1
   4:	mov	x10, x0
   8:	mov	x5, x1
   c:	mov	x12, x3
  10:	cbnz	x1, 34 <__udivmodti4+0x34>
  14:	cbnz	x3, 130 <__udivmodti4+0x130>
  18:	udiv	x9, x0, x2
  1c:	cbz	x4, 28 <__udivmodti4+0x28>
  20:	msub	x2, x9, x2, x0
  24:	stp	x2, xzr, [x4]
  28:	mov	x0, x9
  2c:	mov	x1, #0x0                   	// #0
  30:	ret
  34:	cbnz	x2, e8 <__udivmodti4+0xe8>
  38:	cbz	x3, 184 <__udivmodti4+0x184>
  3c:	cbz	x0, 1e4 <__udivmodti4+0x1e4>
  40:	sub	x6, x3, #0x1
  44:	tst	x6, x3
  48:	b.eq	208 <__udivmodti4+0x208>  // b.none
  4c:	clz	x6, x3
  50:	clz	x0, x1
  54:	sub	w6, w6, w0
  58:	cmp	w6, #0x3e
  5c:	b.hi	188 <__udivmodti4+0x188>  // b.pmore
  60:	add	w11, w6, #0x1
  64:	mov	w1, #0x40                  	// #64
  68:	sub	w8, w1, w11
  6c:	mov	x12, #0x0                   	// #0
  70:	lsr	x7, x5, x11
  74:	lsr	x0, x10, x11
  78:	lsl	x5, x5, x8
  7c:	orr	x5, x5, x0
  80:	lsl	x1, x10, x8
  84:	mov	w9, #0x0                   	// #0
  88:	extr	x10, x5, x1, #63
  8c:	mvn	x8, x10
  90:	extr	x7, x7, x5, #63
  94:	adds	x8, x8, x2
  98:	mvn	x0, x7
  9c:	adc	x0, x3, x0
  a0:	mov	x5, x9
  a4:	bfi	x5, x12, #1, #63
  a8:	extr	x1, x1, x12, #63
  ac:	asr	x0, x0, #63
  b0:	mov	x12, x5
  b4:	and	x6, x0, x2
  b8:	and	x8, x0, x3
  bc:	subs	x5, x10, x6
  c0:	and	w9, w0, #0x1
  c4:	sbc	x7, x7, x8
  c8:	subs	w11, w11, #0x1
  cc:	b.ne	88 <__udivmodti4+0x88>  // b.any
  d0:	mov	x0, x9
  d4:	extr	x1, x1, x12, #63
  d8:	bfi	x0, x12, #1, #63
  dc:	cbz	x4, 30 <__udivmodti4+0x30>
  e0:	stp	x5, x7, [x4]
  e4:	ret
  e8:	cbz	x3, 144 <__udivmodti4+0x144>
  ec:	clz	x6, x3
  f0:	clz	x7, x1
  f4:	sub	w6, w6, w7
  f8:	cmp	w6, #0x3f
  fc:	b.hi	188 <__udivmodti4+0x188>  // b.pmore
 100:	mov	x1, x0
 104:	add	w11, w6, #0x1
 108:	b.eq	228 <__udivmodti4+0x228>  // b.none
 10c:	mov	w0, #0x40                  	// #64
 110:	sub	w0, w0, w11
 114:	lsr	x6, x10, x11
 118:	mov	x12, #0x0                   	// #0
 11c:	lsl	x5, x8, x0
 120:	orr	x5, x5, x6
 124:	lsr	x7, x8, x11
 128:	lsl	x1, x10, x0
 12c:	b	84 <__udivmodti4+0x84>
 130:	mov	x0, #0x0                   	// #0
 134:	mov	x1, #0x0                   	// #0
 138:	cbz	x4, 30 <__udivmodti4+0x30>
 13c:	stp	x10, xzr, [x4]
 140:	ret
 144:	sub	x6, x2, #0x1
 148:	tst	x6, x2
 14c:	b.ne	19c <__udivmodti4+0x19c>  // b.any
 150:	cbz	x4, 15c <__udivmodti4+0x15c>
 154:	and	x6, x6, x0
 158:	stp	x6, xzr, [x4]
 15c:	cmp	x2, #0x1
 160:	b.eq	1fc <__udivmodti4+0x1fc>  // b.none
 164:	rbit	x3, x2
 168:	clz	x3, x3
 16c:	neg	w4, w3
 170:	lsr	x2, x10, x3
 174:	lsl	x0, x8, x4
 178:	orr	x0, x0, x2
 17c:	lsr	x1, x8, x3
 180:	ret
 184:	brk	#0x3e8
 188:	mov	x0, #0x0                   	// #0
 18c:	mov	x1, #0x0                   	// #0
 190:	cbz	x4, 30 <__udivmodti4+0x30>
 194:	stp	x10, x8, [x4]
 198:	ret
 19c:	clz	x0, x1
 1a0:	clz	x8, x2
 1a4:	sub	w8, w8, w0
 1a8:	mov	x1, x10
 1ac:	add	w11, w8, #0x41
 1b0:	mov	x7, #0x0                   	// #0
 1b4:	cmp	w11, #0x40
 1b8:	b.eq	84 <__udivmodti4+0x84>  // b.none
 1bc:	cmp	w11, #0x3f
 1c0:	b.hi	234 <__udivmodti4+0x234>  // b.pmore
 1c4:	mov	w0, #0x40                  	// #64
 1c8:	sub	w0, w0, w11
 1cc:	lsr	x7, x5, x11
 1d0:	lsr	x6, x10, x11
 1d4:	lsl	x5, x5, x0
 1d8:	orr	x5, x5, x6
 1dc:	lsl	x1, x10, x0
 1e0:	b	84 <__udivmodti4+0x84>
 1e4:	udiv	x0, x1, x3
 1e8:	cbz	x4, 1f4 <__udivmodti4+0x1f4>
 1ec:	msub	x1, x0, x3, x1
 1f0:	stp	xzr, x1, [x4]
 1f4:	mov	x1, #0x0                   	// #0
 1f8:	ret
 1fc:	mov	x0, x10
 200:	mov	x1, x8
 204:	ret
 208:	cbz	x4, 214 <__udivmodti4+0x214>
 20c:	and	x6, x6, x1
 210:	stp	x0, x6, [x4]
 214:	rbit	x0, x3
 218:	clz	x0, x0
 21c:	mov	x1, #0x0                   	// #0
 220:	lsr	x0, x8, x0
 224:	ret
 228:	mov	x7, #0x0                   	// #0
 22c:	mov	x12, #0x0                   	// #0
 230:	b	84 <__udivmodti4+0x84>
 234:	mov	w0, #0x80                  	// #128
 238:	sub	w0, w0, w11
 23c:	add	w8, w8, #0x1
 240:	mov	x7, x3
 244:	lsl	x1, x10, x0
 248:	mov	x12, x1
 24c:	lsl	x0, x5, x0
 250:	lsr	x6, x10, x8
 254:	orr	x1, x0, x6
 258:	lsr	x5, x5, x8
 25c:	b	84 <__udivmodti4+0x84>

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	cmp	w1, #0x0
   4:	mov	w5, w0
   8:	ccmp	w0, #0x0, #0x4, ne  // ne = any
   c:	mov	w0, #0x0                   	// #0
  10:	b.ne	18 <__udivsi3+0x18>  // b.any
  14:	ret
  18:	clz	w0, w5
  1c:	clz	w3, w1
  20:	sub	w3, w3, w0
  24:	mov	w0, #0x0                   	// #0
  28:	cmp	w3, #0x1f
  2c:	b.hi	14 <__udivsi3+0x14>  // b.pmore
  30:	mov	w0, w5
  34:	b.eq	14 <__udivsi3+0x14>  // b.none
  38:	add	w3, w3, #0x1
  3c:	sub	w7, w1, #0x1
  40:	neg	w4, w3
  44:	mov	w6, #0x0                   	// #0
  48:	lsr	w2, w5, w3
  4c:	lsl	w0, w5, w4
  50:	subs	w3, w3, #0x1
  54:	extr	w2, w2, w0, #31
  58:	sub	w4, w7, w2
  5c:	orr	w0, w6, w0, lsl #1
  60:	and	w5, w1, w4, asr #31
  64:	lsr	w6, w4, #31
  68:	sub	w2, w2, w5
  6c:	b.ne	50 <__udivsi3+0x50>  // b.any
  70:	orr	w0, w6, w0, lsl #1
  74:	ret

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, #0x0                   	// #0
   4:	b	0 <__udivmodti4>

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x2, sp, #0x18
   c:	bl	0 <__udivmoddi4>
  10:	ldr	x0, [sp, #24]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x4, sp, #0x10
   c:	bl	0 <__udivmodti4>
  10:	ldp	x0, x1, [sp, #16]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <emutls_init>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	adrp	x1, 0 <emutls_init>
   8:	adrp	x0, 0 <emutls_init>
   c:	mov	x29, sp
  10:	add	x1, x1, #0x0
  14:	add	x0, x0, #0x0
  18:	bl	0 <pthread_key_create>
  1c:	cbnz	w0, 28 <emutls_init+0x28>
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	bl	0 <abort>
  2c:	nop

0000000000000030 <emutls_key_destructor>:
  30:	stp	x29, x30, [sp, #-48]!
  34:	mov	x29, sp
  38:	stp	x19, x20, [sp, #16]
  3c:	mov	x20, x0
  40:	ldr	x19, [x0]
  44:	cbnz	x19, ac <emutls_key_destructor+0x7c>
  48:	ldr	x0, [x0, #8]
  4c:	str	x21, [sp, #32]
  50:	add	x21, x20, #0x10
  54:	cbz	x0, 78 <emutls_key_destructor+0x48>
  58:	ldr	x1, [x21, x19, lsl #3]
  5c:	cbz	x1, 8c <emutls_key_destructor+0x5c>
  60:	ldur	x0, [x1, #-8]
  64:	add	x19, x19, #0x1
  68:	bl	0 <free>
  6c:	ldr	x0, [x20, #8]
  70:	cmp	x19, x0
  74:	b.cc	58 <emutls_key_destructor+0x28>  // b.lo, b.ul, b.last
  78:	mov	x0, x20
  7c:	ldp	x19, x20, [sp, #16]
  80:	ldr	x21, [sp, #32]
  84:	ldp	x29, x30, [sp], #48
  88:	b	0 <free>
  8c:	add	x19, x19, #0x1
  90:	cmp	x0, x19
  94:	b.hi	58 <emutls_key_destructor+0x28>  // b.pmore
  98:	mov	x0, x20
  9c:	ldp	x19, x20, [sp, #16]
  a0:	ldr	x21, [sp, #32]
  a4:	ldp	x29, x30, [sp], #48
  a8:	b	0 <free>
  ac:	adrp	x0, 0 <emutls_init>
  b0:	sub	x19, x19, #0x1
  b4:	mov	x1, x20
  b8:	ldr	w0, [x0]
  bc:	str	x19, [x20]
  c0:	ldp	x19, x20, [sp, #16]
  c4:	ldp	x29, x30, [sp], #48
  c8:	b	0 <pthread_setspecific>
  cc:	nop

00000000000000d0 <__emutls_get_address>:
  d0:	stp	x29, x30, [sp, #-64]!
  d4:	mov	x29, sp
  d8:	stp	x19, x20, [sp, #16]
  dc:	add	x19, x0, #0x10
  e0:	stp	x21, x22, [sp, #32]
  e4:	mov	x21, x0
  e8:	stp	x23, x24, [sp, #48]
  ec:	ldar	x20, [x19]
  f0:	cbz	x20, 13c <__emutls_get_address+0x6c>
  f4:	adrp	x23, 0 <emutls_init>
  f8:	sub	x22, x20, #0x1
  fc:	ldr	w0, [x23]
 100:	bl	0 <pthread_getspecific>
 104:	mov	x19, x0
 108:	cbz	x0, 184 <__emutls_get_address+0xb4>
 10c:	ldr	x24, [x0, #8]
 110:	cmp	x24, x20
 114:	b.cc	244 <__emutls_get_address+0x174>  // b.lo, b.ul, b.last
 118:	add	x19, x19, x22, lsl #3
 11c:	ldr	x3, [x19, #16]
 120:	cbz	x3, 1d0 <__emutls_get_address+0x100>
 124:	mov	x0, x3
 128:	ldp	x19, x20, [sp, #16]
 12c:	ldp	x21, x22, [sp, #32]
 130:	ldp	x23, x24, [sp, #48]
 134:	ldp	x29, x30, [sp], #64
 138:	ret
 13c:	adrp	x1, 0 <emutls_init>
 140:	add	x1, x1, #0x0
 144:	adrp	x23, 0 <emutls_init>
 148:	add	x24, x23, #0x0
 14c:	add	x0, x24, #0x4
 150:	bl	0 <pthread_once>
 154:	add	x0, x24, #0x8
 158:	bl	0 <pthread_mutex_lock>
 15c:	ldr	x20, [x21, #16]
 160:	cbz	x20, 28c <__emutls_get_address+0x1bc>
 164:	sub	x22, x20, #0x1
 168:	add	x0, x23, #0x0
 16c:	add	x0, x0, #0x8
 170:	bl	0 <pthread_mutex_unlock>
 174:	ldr	w0, [x23]
 178:	bl	0 <pthread_getspecific>
 17c:	mov	x19, x0
 180:	cbnz	x0, 10c <__emutls_get_address+0x3c>
 184:	add	x0, x20, #0x11
 188:	and	x0, x0, #0xfffffffffffffff0
 18c:	sub	x20, x0, #0x2
 190:	lsl	x0, x0, #3
 194:	bl	0 <malloc>
 198:	mov	x19, x0
 19c:	cbz	x0, 2b8 <__emutls_get_address+0x1e8>
 1a0:	lsl	x2, x20, #3
 1a4:	mov	w1, #0x0                   	// #0
 1a8:	add	x0, x0, #0x10
 1ac:	bl	0 <memset>
 1b0:	ldr	w0, [x23]
 1b4:	mov	x1, x19
 1b8:	stp	xzr, x20, [x19]
 1bc:	add	x19, x19, x22, lsl #3
 1c0:	bl	0 <pthread_setspecific>
 1c4:	ldr	x3, [x19, #16]
 1c8:	cbnz	x3, 124 <__emutls_get_address+0x54>
 1cc:	nop
 1d0:	ldp	x22, x20, [x21]
 1d4:	mov	x3, #0x8                   	// #8
 1d8:	cmp	x20, x3
 1dc:	csel	x20, x20, x3, cs  // cs = hs, nlast
 1e0:	sub	x0, x20, #0x1
 1e4:	tst	x0, x20
 1e8:	b.ne	2b8 <__emutls_get_address+0x1e8>  // b.any
 1ec:	add	x0, x22, #0x7
 1f0:	add	x0, x0, x20
 1f4:	bl	0 <malloc>
 1f8:	cbz	x0, 2b8 <__emutls_get_address+0x1e8>
 1fc:	add	x3, x20, #0x7
 200:	neg	x20, x20
 204:	add	x3, x0, x3
 208:	mov	x2, x22
 20c:	and	x3, x3, x20
 210:	stur	x0, [x3, #-8]
 214:	ldr	x1, [x21, #24]
 218:	cbz	x1, 2a0 <__emutls_get_address+0x1d0>
 21c:	mov	x0, x3
 220:	bl	0 <memcpy>
 224:	mov	x3, x0
 228:	str	x3, [x19, #16]
 22c:	mov	x0, x3
 230:	ldp	x19, x20, [sp, #16]
 234:	ldp	x21, x22, [sp, #32]
 238:	ldp	x23, x24, [sp, #48]
 23c:	ldp	x29, x30, [sp], #64
 240:	ret
 244:	add	x1, x20, #0x11
 248:	and	x1, x1, #0xfffffffffffffff0
 24c:	sub	x20, x1, #0x2
 250:	lsl	x1, x1, #3
 254:	bl	0 <realloc>
 258:	mov	x19, x0
 25c:	cbz	x0, 2b8 <__emutls_get_address+0x1e8>
 260:	sub	x2, x20, x24
 264:	add	x0, x0, #0x10
 268:	add	x0, x0, x24, lsl #3
 26c:	mov	w1, #0x0                   	// #0
 270:	lsl	x2, x2, #3
 274:	bl	0 <memset>
 278:	ldr	w0, [x23]
 27c:	mov	x1, x19
 280:	str	x20, [x19, #8]
 284:	bl	0 <pthread_setspecific>
 288:	b	118 <__emutls_get_address+0x48>
 28c:	ldr	x22, [x24, #56]
 290:	add	x20, x22, #0x1
 294:	str	x20, [x24, #56]
 298:	stlr	x20, [x19]
 29c:	b	168 <__emutls_get_address+0x98>
 2a0:	mov	x0, x3
 2a4:	mov	w1, #0x0                   	// #0
 2a8:	bl	0 <memset>
 2ac:	mov	x3, x0
 2b0:	str	x3, [x19, #16]
 2b4:	b	22c <__emutls_get_address+0x15c>
 2b8:	bl	0 <abort>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	add	x1, x19, #0x30
  1c:	neg	x3, x0
  20:	add	x1, x1, x0
  24:	and	x0, x3, x19
  28:	ldr	x19, [sp, #16]
  2c:	and	x1, x1, x3
  30:	ldp	x29, x30, [sp], #32
  34:	sub	x1, x1, x0
  38:	mov	w2, #0x7                   	// #7
  3c:	b	0 <mprotect>

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x4, x1
   8:	mov	x1, x0
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	adrp	x19, 0 <stderr>
  18:	mov	x5, x2
  1c:	mov	x2, x4
  20:	ldr	x19, [x19]
  24:	mov	x4, x3
  28:	mov	x3, x5
  2c:	ldr	x0, [x19]
  30:	bl	0 <fprintf>
  34:	ldr	x0, [x19]
  38:	bl	0 <fflush>
  3c:	adrp	x2, 0 <__eprintf>
  40:	adrp	x0, 0 <__eprintf>
  44:	add	x2, x2, #0x0
  48:	add	x0, x0, #0x0
  4c:	mov	w1, #0x1a                  	// #26
  50:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_personality_v0>:
   0:	tbz	w1, #0, c <__gcc_personality_v0+0xc>
   4:	mov	w0, #0x8                   	// #8
   8:	ret
   c:	stp	x29, x30, [sp, #-64]!
  10:	mov	x0, x4
  14:	mov	x29, sp
  18:	stp	x19, x20, [sp, #16]
  1c:	mov	x20, x3
  20:	stp	x21, x22, [sp, #32]
  24:	mov	x21, x4
  28:	bl	0 <_Unwind_GetLanguageSpecificData>
  2c:	mov	x19, x0
  30:	cbz	x0, 1b0 <__gcc_personality_v0+0x1b0>
  34:	mov	x0, x21
  38:	str	x23, [sp, #48]
  3c:	bl	0 <_Unwind_GetIP>
  40:	mov	x23, x0
  44:	mov	x0, x21
  48:	bl	0 <_Unwind_GetRegionStart>
  4c:	mov	x1, x19
  50:	sub	x4, x23, #0x1
  54:	mov	x22, x0
  58:	sub	x4, x4, x0
  5c:	ldrb	w2, [x1], #1
  60:	cmp	w2, #0xff
  64:	b.ne	3a8 <__gcc_personality_v0+0x3a8>  // b.any
  68:	ldrb	w0, [x1], #1
  6c:	cmp	w0, #0xff
  70:	b.eq	80 <__gcc_personality_v0+0x80>  // b.none
  74:	nop
  78:	ldrsb	w0, [x1], #1
  7c:	tbnz	w0, #31, 78 <__gcc_personality_v0+0x78>
  80:	ldrb	w10, [x1], #1
  84:	mov	x0, #0x0                   	// #0
  88:	mov	x11, #0x0                   	// #0
  8c:	nop
  90:	ldrb	w3, [x1], #1
  94:	and	w2, w3, #0x7f
  98:	lsl	w2, w2, w0
  9c:	add	x0, x0, #0x7
  a0:	sxtw	x2, w2
  a4:	orr	x11, x11, x2
  a8:	tbnz	w3, #7, 90 <__gcc_personality_v0+0x90>
  ac:	add	x11, x1, w11, uxtw
  b0:	cmp	x11, x1
  b4:	b.ls	1ac <__gcc_personality_v0+0x1ac>  // b.plast
  b8:	sxtb	w12, w10
  bc:	and	w2, w10, #0xf
  c0:	and	w9, w10, #0x70
  c4:	nop
  c8:	cmp	w10, #0xff
  cc:	b.eq	34c <__gcc_personality_v0+0x34c>  // b.none
  d0:	cmp	w2, #0x4
  d4:	b.eq	ec <__gcc_personality_v0+0xec>  // b.none
  d8:	b.hi	20c <__gcc_personality_v0+0x20c>  // b.pmore
  dc:	cmp	w2, #0x2
  e0:	b.eq	390 <__gcc_personality_v0+0x390>  // b.none
  e4:	b.hi	1f8 <__gcc_personality_v0+0x1f8>  // b.pmore
  e8:	cbnz	w2, 1c4 <__gcc_personality_v0+0x1c4>
  ec:	mov	x5, x1
  f0:	ldr	x7, [x5], #8
  f4:	cbz	w9, 104 <__gcc_personality_v0+0x104>
  f8:	cmp	w9, #0x10
  fc:	b.ne	3e0 <__gcc_personality_v0+0x3e0>  // b.any
 100:	add	x7, x1, x7
 104:	tbz	w12, #31, 10c <__gcc_personality_v0+0x10c>
 108:	ldr	x7, [x7]
 10c:	cmp	w2, #0x4
 110:	b.eq	128 <__gcc_personality_v0+0x128>  // b.none
 114:	b.hi	278 <__gcc_personality_v0+0x278>  // b.pmore
 118:	cmp	w2, #0x2
 11c:	b.eq	39c <__gcc_personality_v0+0x39c>  // b.none
 120:	b.hi	264 <__gcc_personality_v0+0x264>  // b.pmore
 124:	cbnz	w2, 230 <__gcc_personality_v0+0x230>
 128:	mov	x6, x5
 12c:	ldr	x8, [x6], #8
 130:	cbz	w9, 140 <__gcc_personality_v0+0x140>
 134:	cmp	w9, #0x10
 138:	b.ne	3e0 <__gcc_personality_v0+0x3e0>  // b.any
 13c:	add	x8, x5, x8
 140:	tbz	w12, #31, 148 <__gcc_personality_v0+0x148>
 144:	ldr	x8, [x8]
 148:	cmp	w2, #0x4
 14c:	b.eq	164 <__gcc_personality_v0+0x164>  // b.none
 150:	b.hi	2e4 <__gcc_personality_v0+0x2e4>  // b.pmore
 154:	cmp	w2, #0x2
 158:	b.eq	36c <__gcc_personality_v0+0x36c>  // b.none
 15c:	b.hi	2d0 <__gcc_personality_v0+0x2d0>  // b.pmore
 160:	cbnz	w2, 29c <__gcc_personality_v0+0x29c>
 164:	mov	x1, x6
 168:	ldr	x19, [x1], #8
 16c:	cbz	w9, 17c <__gcc_personality_v0+0x17c>
 170:	cmp	w9, #0x10
 174:	b.ne	3e0 <__gcc_personality_v0+0x3e0>  // b.any
 178:	add	x19, x6, x19
 17c:	cmp	x4, x7
 180:	cset	w0, cs  // cs = hs, nlast
 184:	tbz	w12, #31, 18c <__gcc_personality_v0+0x18c>
 188:	ldr	x19, [x19]
 18c:	cmp	x19, #0x0
 190:	csel	w0, w0, wzr, ne  // ne = any
 194:	nop
 198:	ldrsb	w5, [x1], #1
 19c:	tbnz	w5, #31, 198 <__gcc_personality_v0+0x198>
 1a0:	cbnz	w0, 308 <__gcc_personality_v0+0x308>
 1a4:	cmp	x11, x1
 1a8:	b.hi	c8 <__gcc_personality_v0+0xc8>  // b.pmore
 1ac:	ldr	x23, [sp, #48]
 1b0:	mov	w0, #0x8                   	// #8
 1b4:	ldp	x19, x20, [sp, #16]
 1b8:	ldp	x21, x22, [sp, #32]
 1bc:	ldp	x29, x30, [sp], #64
 1c0:	ret
 1c4:	cmp	w2, #0x1
 1c8:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 1cc:	mov	x5, x1
 1d0:	mov	x0, #0x0                   	// #0
 1d4:	mov	x7, #0x0                   	// #0
 1d8:	ldrb	w6, [x5], #1
 1dc:	and	w3, w6, #0x7f
 1e0:	lsl	w3, w3, w0
 1e4:	add	x0, x0, #0x7
 1e8:	sxtw	x3, w3
 1ec:	orr	x7, x7, x3
 1f0:	tbnz	w6, #7, 1d8 <__gcc_personality_v0+0x1d8>
 1f4:	b	f4 <__gcc_personality_v0+0xf4>
 1f8:	cmp	w2, #0x3
 1fc:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 200:	mov	x5, x1
 204:	ldr	w7, [x5], #4
 208:	b	f4 <__gcc_personality_v0+0xf4>
 20c:	cmp	w2, #0xb
 210:	b.eq	378 <__gcc_personality_v0+0x378>  // b.none
 214:	cmp	w2, #0xc
 218:	b.eq	ec <__gcc_personality_v0+0xec>  // b.none
 21c:	cmp	w2, #0xa
 220:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 224:	mov	x5, x1
 228:	ldrsh	x7, [x5], #2
 22c:	b	f4 <__gcc_personality_v0+0xf4>
 230:	cmp	w2, #0x1
 234:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 238:	mov	x6, x5
 23c:	mov	x0, #0x0                   	// #0
 240:	mov	x8, #0x0                   	// #0
 244:	ldrb	w3, [x6], #1
 248:	and	w1, w3, #0x7f
 24c:	lsl	w1, w1, w0
 250:	add	x0, x0, #0x7
 254:	sxtw	x1, w1
 258:	orr	x8, x8, x1
 25c:	tbnz	w3, #7, 244 <__gcc_personality_v0+0x244>
 260:	b	130 <__gcc_personality_v0+0x130>
 264:	cmp	w2, #0x3
 268:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 26c:	mov	x6, x5
 270:	ldr	w8, [x6], #4
 274:	b	130 <__gcc_personality_v0+0x130>
 278:	cmp	w2, #0xb
 27c:	b.eq	384 <__gcc_personality_v0+0x384>  // b.none
 280:	cmp	w2, #0xc
 284:	b.eq	128 <__gcc_personality_v0+0x128>  // b.none
 288:	cmp	w2, #0xa
 28c:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 290:	mov	x6, x5
 294:	ldrsh	x8, [x6], #2
 298:	b	130 <__gcc_personality_v0+0x130>
 29c:	cmp	w2, #0x1
 2a0:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 2a4:	mov	x1, x6
 2a8:	mov	x0, #0x0                   	// #0
 2ac:	mov	x19, #0x0                   	// #0
 2b0:	ldrb	w5, [x1], #1
 2b4:	and	w3, w5, #0x7f
 2b8:	lsl	w3, w3, w0
 2bc:	add	x0, x0, #0x7
 2c0:	sxtw	x3, w3
 2c4:	orr	x19, x19, x3
 2c8:	tbnz	w5, #7, 2b0 <__gcc_personality_v0+0x2b0>
 2cc:	b	16c <__gcc_personality_v0+0x16c>
 2d0:	cmp	w2, #0x3
 2d4:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 2d8:	mov	x1, x6
 2dc:	ldr	w19, [x1], #4
 2e0:	b	16c <__gcc_personality_v0+0x16c>
 2e4:	cmp	w2, #0xb
 2e8:	b.eq	360 <__gcc_personality_v0+0x360>  // b.none
 2ec:	cmp	w2, #0xc
 2f0:	b.eq	164 <__gcc_personality_v0+0x164>  // b.none
 2f4:	cmp	w2, #0xa
 2f8:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 2fc:	mov	x1, x6
 300:	ldrsh	x19, [x1], #2
 304:	b	16c <__gcc_personality_v0+0x16c>
 308:	add	x7, x8, x7
 30c:	cmp	x7, x4
 310:	b.ls	1a4 <__gcc_personality_v0+0x1a4>  // b.plast
 314:	mov	x2, x20
 318:	mov	x0, x21
 31c:	mov	w1, #0x0                   	// #0
 320:	bl	0 <_Unwind_SetGR>
 324:	mov	x2, #0x0                   	// #0
 328:	mov	x0, x21
 32c:	mov	w1, #0x1                   	// #1
 330:	bl	0 <_Unwind_SetGR>
 334:	mov	x0, x21
 338:	add	x1, x22, x19
 33c:	bl	0 <_Unwind_SetIP>
 340:	mov	w0, #0x7                   	// #7
 344:	ldr	x23, [sp, #48]
 348:	b	1b4 <__gcc_personality_v0+0x1b4>
 34c:	mov	w0, #0x0                   	// #0
 350:	mov	x7, #0x0                   	// #0
 354:	mov	x8, #0x0                   	// #0
 358:	mov	x19, #0x0                   	// #0
 35c:	b	198 <__gcc_personality_v0+0x198>
 360:	mov	x1, x6
 364:	ldrsw	x19, [x1], #4
 368:	b	16c <__gcc_personality_v0+0x16c>
 36c:	mov	x1, x6
 370:	ldrh	w19, [x1], #2
 374:	b	16c <__gcc_personality_v0+0x16c>
 378:	mov	x5, x1
 37c:	ldrsw	x7, [x5], #4
 380:	b	f4 <__gcc_personality_v0+0xf4>
 384:	mov	x6, x5
 388:	ldrsw	x8, [x6], #4
 38c:	b	130 <__gcc_personality_v0+0x130>
 390:	mov	x5, x1
 394:	ldrh	w7, [x5], #2
 398:	b	f4 <__gcc_personality_v0+0xf4>
 39c:	mov	x6, x5
 3a0:	ldrh	w8, [x6], #2
 3a4:	b	130 <__gcc_personality_v0+0x130>
 3a8:	and	w0, w2, #0xf
 3ac:	cmp	w0, #0x4
 3b0:	b.eq	440 <__gcc_personality_v0+0x440>  // b.none
 3b4:	b.hi	41c <__gcc_personality_v0+0x41c>  // b.pmore
 3b8:	cmp	w0, #0x2
 3bc:	b.eq	450 <__gcc_personality_v0+0x450>  // b.none
 3c0:	b.hi	3f8 <__gcc_personality_v0+0x3f8>  // b.pmore
 3c4:	cbz	w0, 440 <__gcc_personality_v0+0x440>
 3c8:	cmp	w0, #0x1
 3cc:	b.ne	404 <__gcc_personality_v0+0x404>  // b.any
 3d0:	ldrsb	w0, [x1], #1
 3d4:	tbnz	w0, #31, 3d0 <__gcc_personality_v0+0x3d0>
 3d8:	tst	w2, #0x60
 3dc:	b.eq	68 <__gcc_personality_v0+0x68>  // b.none
 3e0:	adrp	x2, 0 <__gcc_personality_v0>
 3e4:	adrp	x0, 0 <__gcc_personality_v0>
 3e8:	add	x2, x2, #0x0
 3ec:	add	x0, x0, #0x0
 3f0:	mov	w1, #0x7a                  	// #122
 3f4:	bl	0 <__compilerrt_abort_impl>
 3f8:	add	x1, x19, #0x5
 3fc:	cmp	w0, #0x3
 400:	b.eq	3d8 <__gcc_personality_v0+0x3d8>  // b.none
 404:	adrp	x2, 0 <__gcc_personality_v0>
 408:	adrp	x0, 0 <__gcc_personality_v0>
 40c:	add	x2, x2, #0x0
 410:	add	x0, x0, #0x0
 414:	mov	w1, #0x68                  	// #104
 418:	bl	0 <__compilerrt_abort_impl>
 41c:	cmp	w0, #0xb
 420:	b.eq	448 <__gcc_personality_v0+0x448>  // b.none
 424:	cmp	w0, #0xc
 428:	add	x1, x19, #0x9
 42c:	b.eq	3d8 <__gcc_personality_v0+0x3d8>  // b.none
 430:	add	x1, x19, #0x3
 434:	cmp	w0, #0xa
 438:	b.eq	3d8 <__gcc_personality_v0+0x3d8>  // b.none
 43c:	b	404 <__gcc_personality_v0+0x404>
 440:	add	x1, x19, #0x9
 444:	b	3d8 <__gcc_personality_v0+0x3d8>
 448:	add	x1, x19, #0x5
 44c:	b	3d8 <__gcc_personality_v0+0x3d8>
 450:	add	x1, x19, #0x3
 454:	b	3d8 <__gcc_personality_v0+0x3d8>

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	adrp	x4, 0 <__clear_cache>
   4:	ldr	x3, [x4]
   8:	cbnz	x3, 18 <__clear_cache+0x18>
   c:	mrs	x2, ctr_el0
  10:	mov	x3, x2
  14:	str	x2, [x4]
  18:	tbnz	w3, #28, 50 <__clear_cache+0x50>
  1c:	ubfx	w2, w3, #16, #4
  20:	mov	w3, #0x4                   	// #4
  24:	lsl	w3, w3, w2
  28:	sxtw	x3, w3
  2c:	neg	x2, x3
  30:	and	x2, x0, x2
  34:	cmp	x1, x2
  38:	b.ls	50 <__clear_cache+0x50>  // b.plast
  3c:	nop
  40:	dc	cvau, x2
  44:	add	x2, x2, x3
  48:	cmp	x1, x2
  4c:	b.hi	40 <__clear_cache+0x40>  // b.pmore
  50:	dsb	ish
  54:	ldr	x2, [x4]
  58:	tbnz	w2, #29, 90 <__clear_cache+0x90>
  5c:	and	w3, w2, #0xf
  60:	mov	w2, #0x4                   	// #4
  64:	lsl	w2, w2, w3
  68:	sxtw	x2, w2
  6c:	neg	x3, x2
  70:	and	x0, x0, x3
  74:	cmp	x1, x0
  78:	b.ls	90 <__clear_cache+0x90>  // b.plast
  7c:	nop
  80:	ic	ivau, x0
  84:	add	x0, x0, x2
  88:	cmp	x1, x0
  8c:	b.hi	80 <__clear_cache+0x80>  // b.pmore
  90:	isb
  94:	ret

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x1, fpcr
   4:	ubfx	x1, x1, #22, #2
   8:	mov	w0, #0x0                   	// #0
   c:	sub	x1, x1, #0x1
  10:	cmp	x1, #0x2
  14:	b.hi	24 <__fe_getround+0x24>  // b.pmore
  18:	adrp	x0, 0 <__fe_getround>
  1c:	add	x0, x0, #0x0
  20:	ldr	w0, [x0, x1, lsl #2]
  24:	ret

0000000000000028 <__fe_raise_inexact>:
  28:	mrs	x0, fpsr
  2c:	orr	x0, x0, #0x10
  30:	msr	fpsr, x0
  34:	mov	w0, #0x0                   	// #0
  38:	ret
