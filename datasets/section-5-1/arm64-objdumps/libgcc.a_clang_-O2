In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgcc.a_clang_-O2:

_muldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	mov	x5, x0
   4:	and	x0, x2, #0xffffffff
   8:	and	x7, x5, #0xffffffff
   c:	lsr	x9, x2, #32
  10:	lsr	x6, x5, #32
  14:	mov	x8, #0x100000000           	// #4294967296
  18:	mul	x4, x7, x0
  1c:	mul	x0, x6, x0
  20:	madd	x7, x7, x9, x0
  24:	and	x10, x4, #0xffffffff
  28:	mul	x6, x6, x9
  2c:	add	x4, x7, x4, lsr #32
  30:	add	x8, x6, x8
  34:	cmp	x0, x4
  38:	csel	x6, x8, x6, hi  // hi = pmore
  3c:	add	x0, x10, x4, lsl #32
  40:	add	x4, x6, x4, lsr #32
  44:	madd	x4, x5, x3, x4
  48:	madd	x1, x2, x1, x4
  4c:	ret

_negdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	cmp	x0, #0x0
   4:	neg	x1, x1
   8:	cset	x2, ne  // ne = any
   c:	neg	x0, x0
  10:	sub	x1, x1, x2
  14:	ret

_lshrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	mov	x4, x1
   4:	cbz	x2, 28 <__lshrti3+0x28>
   8:	mov	x3, #0x40                  	// #64
   c:	sub	x3, x3, x2
  10:	cmp	x3, #0x0
  14:	b.le	2c <__lshrti3+0x2c>
  18:	lsl	x3, x1, x3
  1c:	lsr	x0, x0, x2
  20:	orr	x0, x0, x3
  24:	lsr	x1, x1, x2
  28:	ret
  2c:	neg	w0, w3
  30:	mov	x1, #0x0                   	// #0
  34:	lsr	x0, x4, x0
  38:	ret

_ashldi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	mov	x4, x0
   4:	cbz	x2, 28 <__ashlti3+0x28>
   8:	mov	x3, #0x40                  	// #64
   c:	sub	x3, x3, x2
  10:	cmp	x3, #0x0
  14:	b.le	2c <__ashlti3+0x2c>
  18:	lsr	x3, x0, x3
  1c:	lsl	x1, x1, x2
  20:	orr	x1, x1, x3
  24:	lsl	x0, x0, x2
  28:	ret
  2c:	neg	w1, w3
  30:	mov	x0, #0x0                   	// #0
  34:	lsl	x1, x4, x1
  38:	ret

_ashrdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	mov	x3, x1
   4:	cbz	x2, 28 <__ashrti3+0x28>
   8:	mov	x1, #0x40                  	// #64
   c:	sub	x1, x1, x2
  10:	cmp	x1, #0x0
  14:	b.le	2c <__ashrti3+0x2c>
  18:	lsl	x1, x3, x1
  1c:	lsr	x0, x0, x2
  20:	orr	x0, x0, x1
  24:	asr	x1, x3, x2
  28:	ret
  2c:	neg	w0, w1
  30:	asr	x1, x3, #63
  34:	asr	x0, x3, x0
  38:	ret

_cmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.lt	2c <__cmpti2+0x2c>  // b.tstop
  10:	mov	w0, #0x2                   	// #2
  14:	b.gt	2c <__cmpti2+0x2c>
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

_ucmpdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	mov	x4, x0
   4:	cmp	x1, x3
   8:	mov	w0, #0x0                   	// #0
   c:	b.cc	2c <__ucmpti2+0x2c>  // b.lo, b.ul, b.last
  10:	mov	w0, #0x2                   	// #2
  14:	b.hi	2c <__ucmpti2+0x2c>  // b.pmore
  18:	cmp	x4, x2
  1c:	mov	w3, #0x0                   	// #0
  20:	cset	w1, hi  // hi = pmore
  24:	add	w0, w1, #0x1
  28:	csel	w0, w0, w3, cs  // cs = hs, nlast
  2c:	ret

_clear_cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	b	0 <__aarch64_sync_cache_range>

_trampoline.o:     file format elf64-littleaarch64


__main.o:     file format elf64-littleaarch64


_absvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	tbnz	x0, #63, 8 <__absvdi2+0x8>
   4:	ret
   8:	negs	x0, x0
   c:	b.pl	4 <__absvdi2+0x4>  // b.nfrst
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <abort>
  1c:	nop

0000000000000020 <__absvsi2>:
  20:	tbnz	w0, #31, 28 <__absvsi2+0x8>
  24:	ret
  28:	negs	w0, w0
  2c:	b.pl	24 <__absvsi2+0x4>  // b.nfrst
  30:	stp	x29, x30, [sp, #-16]!
  34:	mov	x29, sp
  38:	bl	0 <abort>

_absvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	tbnz	x1, #63, 8 <__absvti2+0x8>
   4:	ret
   8:	negs	x0, x0
   c:	ngc	x1, x1
  10:	tbz	x1, #63, 4 <__absvti2+0x4>
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	bl	0 <abort>

_addvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	mov	x2, x0
   4:	add	x0, x0, x1
   8:	tbnz	x1, #63, 1c <__addvdi3+0x1c>
   c:	cmp	x2, x0
  10:	cset	w1, gt
  14:	cbnz	w1, 28 <__addvdi3+0x28>
  18:	ret
  1c:	cmp	x2, x0
  20:	cset	w1, lt  // lt = tstop
  24:	b	14 <__addvdi3+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__addvsi3>:
  38:	mov	w2, w0
  3c:	add	w0, w0, w1
  40:	tbnz	w1, #31, 54 <__addvsi3+0x1c>
  44:	cmp	w2, w0
  48:	cset	w1, gt
  4c:	cbnz	w1, 60 <__addvsi3+0x28>
  50:	ret
  54:	cmp	w2, w0
  58:	cset	w1, lt  // lt = tstop
  5c:	b	4c <__addvsi3+0x14>
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	0 <abort>

_addvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	mov	x4, x0
   4:	adds	x0, x0, x2
   8:	mov	x2, x1
   c:	adc	x1, x1, x3
  10:	tbnz	x3, #63, 34 <__addvti3+0x34>
  14:	cmp	x2, x1
  18:	mov	w2, #0x1                   	// #1
  1c:	b.le	28 <__addvti3+0x28>
  20:	cbnz	w2, 64 <__addvti3+0x64>
  24:	ret
  28:	b.eq	54 <__addvti3+0x54>  // b.none
  2c:	mov	w2, #0x0                   	// #0
  30:	b	20 <__addvti3+0x20>
  34:	cmp	x1, x2
  38:	mov	w2, #0x1                   	// #1
  3c:	b.gt	20 <__addvti3+0x20>
  40:	b.ne	2c <__addvti3+0x2c>  // b.any
  44:	cmp	x0, x4
  48:	b.hi	20 <__addvti3+0x20>  // b.pmore
  4c:	mov	w2, #0x0                   	// #0
  50:	b	20 <__addvti3+0x20>
  54:	cmp	x4, x0
  58:	b.hi	20 <__addvti3+0x20>  // b.pmore
  5c:	mov	w2, #0x0                   	// #0
  60:	b	20 <__addvti3+0x20>
  64:	stp	x29, x30, [sp, #-16]!
  68:	mov	x29, sp
  6c:	bl	0 <abort>

_subvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	mov	x2, x0
   4:	sub	x0, x0, x1
   8:	tbnz	x1, #63, 1c <__subvdi3+0x1c>
   c:	cmp	x2, x0
  10:	cset	w1, lt  // lt = tstop
  14:	cbnz	w1, 28 <__subvdi3+0x28>
  18:	ret
  1c:	cmp	x2, x0
  20:	cset	w1, gt
  24:	b	14 <__subvdi3+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__subvsi3>:
  38:	mov	w2, w0
  3c:	sub	w0, w0, w1
  40:	tbnz	w1, #31, 54 <__subvsi3+0x1c>
  44:	cmp	w2, w0
  48:	cset	w1, lt  // lt = tstop
  4c:	cbnz	w1, 60 <__subvsi3+0x28>
  50:	ret
  54:	cmp	w2, w0
  58:	cset	w1, gt
  5c:	b	4c <__subvsi3+0x14>
  60:	stp	x29, x30, [sp, #-16]!
  64:	mov	x29, sp
  68:	bl	0 <abort>

_subvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	mov	x4, x0
   4:	subs	x0, x0, x2
   8:	mov	x2, x1
   c:	sbc	x1, x1, x3
  10:	tbnz	x3, #63, 34 <__subvti3+0x34>
  14:	cmp	x1, x2
  18:	mov	w2, #0x1                   	// #1
  1c:	b.le	28 <__subvti3+0x28>
  20:	cbnz	w2, 64 <__subvti3+0x64>
  24:	ret
  28:	b.eq	54 <__subvti3+0x54>  // b.none
  2c:	mov	w2, #0x0                   	// #0
  30:	b	20 <__subvti3+0x20>
  34:	cmp	x2, x1
  38:	mov	w2, #0x1                   	// #1
  3c:	b.gt	20 <__subvti3+0x20>
  40:	b.ne	2c <__subvti3+0x2c>  // b.any
  44:	cmp	x4, x0
  48:	b.hi	20 <__subvti3+0x20>  // b.pmore
  4c:	mov	w2, #0x0                   	// #0
  50:	b	20 <__subvti3+0x20>
  54:	cmp	x0, x4
  58:	b.hi	20 <__subvti3+0x20>  // b.pmore
  5c:	mov	w2, #0x0                   	// #0
  60:	b	20 <__subvti3+0x20>
  64:	stp	x29, x30, [sp, #-16]!
  68:	mov	x29, sp
  6c:	bl	0 <abort>

_mulvsi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	mov	x2, x0
   4:	mul	x0, x0, x1
   8:	smulh	x2, x2, x1
   c:	cmp	x2, x0, asr #63
  10:	b.ne	18 <__mulvdi3+0x18>  // b.any
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	bl	0 <abort>
  24:	nop

0000000000000028 <__mulvsi3>:
  28:	smull	x0, w0, w1
  2c:	asr	x1, x0, #32
  30:	cmp	w1, w0, asr #31
  34:	b.ne	3c <__mulvsi3+0x14>  // b.any
  38:	ret
  3c:	stp	x29, x30, [sp, #-16]!
  40:	mov	x29, sp
  44:	bl	0 <abort>

_mulvdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	cmp	x1, x0, asr #63
   4:	asr	x4, x2, #63
   8:	b.ne	20 <__mulvti3+0x20>  // b.any
   c:	cmp	x4, x3
  10:	b.ne	64 <__mulvti3+0x64>  // b.any
  14:	smulh	x1, x0, x2
  18:	mul	x0, x0, x2
  1c:	ret
  20:	cmp	x4, x3
  24:	b.ne	b4 <__mulvti3+0xb4>  // b.any
  28:	mul	x6, x1, x2
  2c:	umulh	x4, x1, x2
  30:	mul	x7, x0, x2
  34:	mov	x5, x6
  38:	umulh	x3, x0, x2
  3c:	tbnz	x1, #63, ac <__mulvti3+0xac>
  40:	tbz	x2, #63, 4c <__mulvti3+0x4c>
  44:	subs	x5, x6, x0
  48:	sbc	x4, x4, x1
  4c:	adds	x1, x3, x5
  50:	cinc	x4, x4, cs  // cs = hs, nlast
  54:	cmp	x4, x1, asr #63
  58:	b.ne	98 <__mulvti3+0x98>  // b.any
  5c:	mov	x0, x7
  60:	ret
  64:	mul	x6, x3, x0
  68:	umulh	x4, x3, x0
  6c:	mul	x7, x2, x0
  70:	mov	x5, x6
  74:	umulh	x1, x2, x0
  78:	tbnz	x3, #63, a4 <__mulvti3+0xa4>
  7c:	tbz	x0, #63, 88 <__mulvti3+0x88>
  80:	subs	x5, x6, x2
  84:	sbc	x4, x4, x3
  88:	adds	x1, x1, x5
  8c:	cinc	x4, x4, cs  // cs = hs, nlast
  90:	cmp	x4, x1, asr #63
  94:	b.eq	5c <__mulvti3+0x5c>  // b.none
  98:	stp	x29, x30, [sp, #-16]!
  9c:	mov	x29, sp
  a0:	bl	0 <abort>
  a4:	sub	x4, x4, x0
  a8:	b	7c <__mulvti3+0x7c>
  ac:	sub	x4, x4, x2
  b0:	b	40 <__mulvti3+0x40>
  b4:	tbnz	x1, #63, d4 <__mulvti3+0xd4>
  b8:	tbnz	x3, #63, 100 <__mulvti3+0x100>
  bc:	orr	x4, x3, x1
  c0:	cbnz	x4, 98 <__mulvti3+0x98>
  c4:	umulh	x1, x0, x2
  c8:	mul	x0, x0, x2
  cc:	tbnz	x1, #63, 98 <__mulvti3+0x98>
  d0:	ret
  d4:	cmp	x3, #0x0
  d8:	b.lt	124 <__mulvti3+0x124>  // b.tstop
  dc:	ccmn	x1, #0x1, #0x0, eq  // eq = none
  e0:	b.ne	98 <__mulvti3+0x98>  // b.any
  e4:	umulh	x3, x0, x2
  e8:	mul	x1, x0, x2
  ec:	subs	x3, x3, x2
  f0:	b.pl	98 <__mulvti3+0x98>  // b.nfrst
  f4:	mov	x0, x1
  f8:	mov	x1, x3
  fc:	ret
 100:	cmp	x1, #0x0
 104:	ccmn	x3, #0x1, #0x0, eq  // eq = none
 108:	b.ne	98 <__mulvti3+0x98>  // b.any
 10c:	umulh	x1, x0, x2
 110:	mul	x2, x0, x2
 114:	subs	x1, x1, x0
 118:	b.pl	98 <__mulvti3+0x98>  // b.nfrst
 11c:	mov	x0, x2
 120:	ret
 124:	and	x4, x3, x1
 128:	cmn	x4, #0x1
 12c:	b.ne	98 <__mulvti3+0x98>  // b.any
 130:	orr	x1, x2, x0
 134:	cbz	x1, 98 <__mulvti3+0x98>
 138:	umulh	x4, x0, x2
 13c:	mul	x3, x0, x2
 140:	sub	x1, x4, x0
 144:	subs	x1, x1, x2
 148:	b.mi	98 <__mulvti3+0x98>  // b.first
 14c:	mov	x0, x3
 150:	ret

_negvsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, x0
   4:	neg	x0, x0
   8:	tbnz	x1, #63, 1c <__negvdi2+0x1c>
   c:	cmp	x0, #0x0
  10:	cset	w1, gt
  14:	cbnz	w1, 28 <__negvdi2+0x28>
  18:	ret
  1c:	lsr	x1, x0, #63
  20:	and	w1, w1, #0xff
  24:	b	14 <__negvdi2+0x14>
  28:	stp	x29, x30, [sp, #-16]!
  2c:	mov	x29, sp
  30:	bl	0 <abort>
  34:	nop

0000000000000038 <__negvsi2>:
  38:	mov	w1, w0
  3c:	neg	w0, w0
  40:	lsr	w2, w0, #31
  44:	tbnz	w1, #31, 50 <__negvsi2+0x18>
  48:	cmp	w0, #0x0
  4c:	cset	w2, gt
  50:	cbnz	w2, 58 <__negvsi2+0x20>
  54:	ret
  58:	stp	x29, x30, [sp, #-16]!
  5c:	mov	x29, sp
  60:	bl	0 <abort>

_negvdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	negs	x0, x0
   4:	mov	x2, x1
   8:	ngc	x1, x1
   c:	tbnz	x2, #63, 2c <__negvti2+0x2c>
  10:	asr	x2, x1, #63
  14:	subs	x3, x2, x0
  18:	sbc	x2, x2, x1
  1c:	lsr	x2, x2, #63
  20:	and	w2, w2, #0xff
  24:	cbnz	w2, 38 <__negvti2+0x38>
  28:	ret
  2c:	lsr	x2, x1, #63
  30:	and	w2, w2, #0xff
  34:	b	24 <__negvti2+0x24>
  38:	stp	x29, x30, [sp, #-16]!
  3c:	mov	x29, sp
  40:	bl	0 <abort>

_ctors.o:     file format elf64-littleaarch64


_ffssi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	rbit	x1, x0
   4:	cmp	x0, #0x0
   8:	clz	x1, x1
   c:	csinc	w0, wzr, w1, eq  // eq = none
  10:	ret

_ffsdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 1c <__ffsti2+0x1c>
   4:	mov	w1, #0x0                   	// #0
   8:	rbit	x0, x0
   c:	add	w1, w1, #0x1
  10:	clz	x0, x0
  14:	add	w0, w1, w0
  18:	ret
  1c:	mov	w0, #0x0                   	// #0
  20:	cbz	x1, 18 <__ffsti2+0x18>
  24:	mov	x0, x1
  28:	mov	w1, #0x40                  	// #64
  2c:	b	8 <__ffsti2+0x8>

_clz.o:     file format elf64-littleaarch64


_clzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	clz	x0, x0
   4:	ret

_clzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cbz	x1, 18 <__clzti2+0x18>
   4:	mov	x0, x1
   8:	mov	w1, #0x0                   	// #0
   c:	clz	x0, x0
  10:	add	w0, w1, w0
  14:	ret
  18:	clz	x0, x0
  1c:	mov	w1, #0x40                  	// #64
  20:	add	w0, w1, w0
  24:	ret

_ctzsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	rbit	x0, x0
   4:	clz	x0, x0
   8:	ret

_ctzdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cbz	x0, 1c <__ctzti2+0x1c>
   4:	mov	x1, x0
   8:	mov	w0, #0x0                   	// #0
   c:	rbit	x1, x1
  10:	clz	x1, x1
  14:	add	w0, w0, w1
  18:	ret
  1c:	rbit	x1, x1
  20:	mov	w0, #0x40                  	// #64
  24:	clz	x1, x1
  28:	add	w0, w0, w1
  2c:	ret

_popcount_tab.o:     file format elf64-littleaarch64


_popcountsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	mov	x3, #0x101010101010101     	// #72340172838076673
   8:	and	x1, x1, #0x5555555555555555
   c:	sub	x1, x0, x1
  10:	and	x2, x1, #0x3333333333333333
  14:	lsr	x1, x1, #2
  18:	and	x1, x1, #0x3333333333333333
  1c:	add	x1, x2, x1
  20:	add	x1, x1, x1, lsr #4
  24:	and	x0, x1, #0xf0f0f0f0f0f0f0f
  28:	mul	x0, x0, x3
  2c:	lsr	x0, x0, #56
  30:	ret

_popcountdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x3, x0, #1
   4:	lsr	x2, x1, #1
   8:	and	x3, x3, #0x5555555555555555
   c:	and	x2, x2, #0x5555555555555555
  10:	sub	x0, x0, x3
  14:	sub	x2, x1, x2
  18:	and	x3, x0, #0x3333333333333333
  1c:	and	x1, x2, #0x3333333333333333
  20:	lsr	x0, x0, #2
  24:	lsr	x2, x2, #2
  28:	and	x2, x2, #0x3333333333333333
  2c:	and	x0, x0, #0x3333333333333333
  30:	add	x0, x3, x0
  34:	add	x1, x1, x2
  38:	mov	x4, #0x101010101010101     	// #72340172838076673
  3c:	add	x0, x0, x0, lsr #4
  40:	add	x1, x1, x1, lsr #4
  44:	and	x2, x0, #0xf0f0f0f0f0f0f0f
  48:	and	x0, x1, #0xf0f0f0f0f0f0f0f
  4c:	add	x0, x0, x2
  50:	mul	x0, x0, x4
  54:	lsr	x0, x0, #56
  58:	ret

_paritysi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	eor	x1, x0, x0, lsr #32
   4:	mov	w2, #0x6996                	// #27030
   8:	eor	x1, x1, x1, lsr #16
   c:	eor	x1, x1, x1, lsr #8
  10:	eor	x1, x1, x1, lsr #4
  14:	and	x1, x1, #0xf
  18:	asr	w1, w2, w1
  1c:	and	w0, w1, #0x1
  20:	ret

_paritydi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x1, x0, x1
   4:	mov	w0, #0x6996                	// #27030
   8:	eor	x1, x1, x1, lsr #32
   c:	eor	x1, x1, x1, lsr #16
  10:	eor	x1, x1, x1, lsr #8
  14:	eor	x1, x1, x1, lsr #4
  18:	and	x1, x1, #0xf
  1c:	asr	w0, w0, w1
  20:	and	w0, w0, #0x1
  24:	ret

_powisf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	cmp	w0, #0x0
   4:	fmov	s2, #1.000000000000000000e+00
   8:	cneg	w2, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	s1, s0
  14:	lsr	w1, w2, #1
  18:	fcsel	s0, s0, s2, ne  // ne = any
  1c:	cmp	wzr, w2, lsr #1
  20:	b.eq	3c <__powisf2+0x3c>  // b.none
  24:	nop
  28:	fmul	s1, s1, s1
  2c:	tbz	w1, #0, 34 <__powisf2+0x34>
  30:	fmul	s0, s0, s1
  34:	lsr	w1, w1, #1
  38:	cbnz	w1, 28 <__powisf2+0x28>
  3c:	tbnz	w0, #31, 44 <__powisf2+0x44>
  40:	ret
  44:	fmov	s1, #1.000000000000000000e+00
  48:	fdiv	s0, s1, s0
  4c:	ret

_powidf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	cmp	w0, #0x0
   4:	fmov	d2, #1.000000000000000000e+00
   8:	cneg	w2, w0, lt  // lt = tstop
   c:	tst	x0, #0x1
  10:	fmov	d1, d0
  14:	lsr	w1, w2, #1
  18:	fcsel	d0, d0, d2, ne  // ne = any
  1c:	cmp	wzr, w2, lsr #1
  20:	b.eq	3c <__powidf2+0x3c>  // b.none
  24:	nop
  28:	fmul	d1, d1, d1
  2c:	tbz	w1, #0, 34 <__powidf2+0x34>
  30:	fmul	d0, d0, d1
  34:	lsr	w1, w1, #1
  38:	cbnz	w1, 28 <__powidf2+0x28>
  3c:	tbnz	w0, #31, 44 <__powidf2+0x44>
  40:	ret
  44:	fmov	d1, #1.000000000000000000e+00
  48:	fdiv	d0, d1, d0
  4c:	ret

_powixf2.o:     file format elf64-littleaarch64


_powitf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powitf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	cmp	w0, #0x0
   8:	mov	v2.16b, v0.16b
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	v4.16b, v0.16b
  18:	mov	w20, w0
  1c:	cneg	w19, w0, lt  // lt = tstop
  20:	tbnz	w20, #0, 30 <__powitf2+0x30>
  24:	adrp	x0, 0 <__powitf2>
  28:	add	x0, x0, #0x0
  2c:	ldr	q4, [x0]
  30:	cmp	wzr, w19, lsr #1
  34:	lsr	w19, w19, #1
  38:	b.eq	7c <__powitf2+0x7c>  // b.none
  3c:	nop
  40:	mov	v1.16b, v2.16b
  44:	mov	v0.16b, v2.16b
  48:	str	q4, [sp, #32]
  4c:	bl	0 <__multf3>
  50:	ldr	q4, [sp, #32]
  54:	mov	v2.16b, v0.16b
  58:	mov	v1.16b, v2.16b
  5c:	mov	v0.16b, v4.16b
  60:	tbz	w19, #0, 74 <__powitf2+0x74>
  64:	str	q2, [sp, #32]
  68:	bl	0 <__multf3>
  6c:	mov	v4.16b, v0.16b
  70:	ldr	q2, [sp, #32]
  74:	lsr	w19, w19, #1
  78:	cbnz	w19, 40 <__powitf2+0x40>
  7c:	tbz	w20, #31, 98 <__powitf2+0x98>
  80:	adrp	x0, 0 <__powitf2>
  84:	add	x0, x0, #0x0
  88:	mov	v1.16b, v4.16b
  8c:	ldr	q0, [x0]
  90:	bl	0 <__divtf3>
  94:	mov	v4.16b, v0.16b
  98:	mov	v0.16b, v4.16b
  9c:	ldp	x19, x20, [sp, #16]
  a0:	ldp	x29, x30, [sp], #48
  a4:	ret

_mulhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulhc3>:
   0:	fcvt	s5, h0
   4:	fcvt	s4, h1
   8:	fcvt	s19, h2
   c:	fcvt	s20, h3
  10:	mov	v22.h[0], v0.h[0]
  14:	mov	v21.h[0], v1.h[0]
  18:	fmul	s17, s5, s19
  1c:	fmul	s16, s4, s20
  20:	fmul	s7, s5, s20
  24:	fmul	s6, s19, s4
  28:	fcvt	h17, s17
  2c:	fcvt	h16, s16
  30:	fcvt	h7, s7
  34:	fcvt	h6, s6
  38:	fcvt	s17, h17
  3c:	fcvt	s16, h16
  40:	fcvt	s23, h7
  44:	fcvt	s24, h6
  48:	fsub	s0, s17, s16
  4c:	fadd	s1, s23, s24
  50:	fcvt	h0, s0
  54:	fcvt	h1, s1
  58:	fcvt	s7, h0
  5c:	fcvt	s6, h1
  60:	fcmp	s7, s7
  64:	cset	w0, vs
  68:	fcmp	s6, s6
  6c:	cset	w1, vs
  70:	ands	w0, w0, w1
  74:	b.eq	1d0 <__mulhc3+0x1d0>  // b.none
  78:	fabs	s18, s5
  7c:	fabs	s4, s4
  80:	mov	w1, #0xe000                	// #57344
  84:	movk	w1, #0x477f, lsl #16
  88:	fmov	s5, w1
  8c:	fcvt	h18, s18
  90:	fcvt	h4, s4
  94:	fcvt	s18, h18
  98:	fcvt	s4, h4
  9c:	fcmp	s18, s5
  a0:	b.le	1d4 <__mulhc3+0x1d4>
  a4:	mov	w1, #0xe000                	// #57344
  a8:	umov	w4, v22.h[0]
  ac:	movk	w1, #0x477f, lsl #16
  b0:	fmov	s5, w1
  b4:	umov	w3, v21.h[0]
  b8:	fcmp	s18, s5
  bc:	cset	w1, gt
  c0:	fcmp	s4, s5
  c4:	scvtf	d5, w1
  c8:	cset	w1, gt
  cc:	fcmp	s19, s19
  d0:	scvtf	d4, w1
  d4:	fcvt	h5, d5
  d8:	fcvt	h4, d4
  dc:	umov	w2, v5.h[0]
  e0:	umov	w1, v4.h[0]
  e4:	bfxil	w4, w2, #0, #15
  e8:	mov	w2, w3
  ec:	dup	v22.4h, w4
  f0:	bfxil	w2, w1, #0, #15
  f4:	dup	v21.4h, w2
  f8:	b.vs	2b8 <__mulhc3+0x2b8>
  fc:	fcmp	s20, s20
 100:	b.vs	2d0 <__mulhc3+0x2d0>
 104:	fabs	s6, s19
 108:	fabs	s7, s20
 10c:	mov	w1, #0xe000                	// #57344
 110:	movk	w1, #0x477f, lsl #16
 114:	fmov	s4, w1
 118:	fcvt	h6, s6
 11c:	fcvt	h7, s7
 120:	fcvt	s6, h6
 124:	fcvt	s7, h7
 128:	fcmp	s6, s4
 12c:	b.gt	138 <__mulhc3+0x138>
 130:	fcmp	s7, s4
 134:	b.le	1e4 <__mulhc3+0x1e4>
 138:	mov	w0, #0xe000                	// #57344
 13c:	umov	w3, v2.h[0]
 140:	movk	w0, #0x477f, lsl #16
 144:	fmov	s0, w0
 148:	umov	w2, v3.h[0]
 14c:	fcvt	s5, h22
 150:	fcmp	s6, s0
 154:	cset	w0, gt
 158:	fcmp	s7, s0
 15c:	scvtf	d1, w0
 160:	cset	w0, gt
 164:	fcmp	s5, s5
 168:	scvtf	d0, w0
 16c:	fcvt	h1, d1
 170:	fcvt	h0, d0
 174:	umov	w1, v1.h[0]
 178:	umov	w0, v0.h[0]
 17c:	bfxil	w3, w1, #0, #15
 180:	mov	w1, w2
 184:	dup	v2.4h, w3
 188:	bfxil	w1, w0, #0, #15
 18c:	dup	v3.4h, w1
 190:	b.vs	280 <__mulhc3+0x280>
 194:	fcvt	s4, h21
 198:	fcmp	s4, s4
 19c:	b.vs	298 <__mulhc3+0x298>
 1a0:	fcvt	s19, h2
 1a4:	fcvt	s20, h3
 1a8:	fmul	s0, s4, s20
 1ac:	fmul	s1, s19, s4
 1b0:	fnmsub	s0, s5, s19, s0
 1b4:	fmadd	s1, s5, s20, s1
 1b8:	mov	w0, #0x7f800000            	// #2139095040
 1bc:	fmov	s4, w0
 1c0:	fmul	s0, s0, s4
 1c4:	fmul	s1, s1, s4
 1c8:	fcvt	h0, s0
 1cc:	fcvt	h1, s1
 1d0:	ret
 1d4:	fcmp	s4, s5
 1d8:	b.gt	a4 <__mulhc3+0xa4>
 1dc:	mov	w0, #0x0                   	// #0
 1e0:	b	104 <__mulhc3+0x104>
 1e4:	cbnz	w0, 330 <__mulhc3+0x330>
 1e8:	fabs	s5, s17
 1ec:	fcvt	h5, s5
 1f0:	fcvt	s5, h5
 1f4:	fcmp	s5, s4
 1f8:	b.gt	228 <__mulhc3+0x228>
 1fc:	fabs	s16, s16
 200:	fcvt	h16, s16
 204:	fcvt	s16, h16
 208:	fcmp	s16, s4
 20c:	b.gt	228 <__mulhc3+0x228>
 210:	fabs	s5, s23
 214:	fcvt	h5, s5
 218:	fcvt	s5, h5
 21c:	fcmp	s5, s4
 220:	b.le	268 <__mulhc3+0x268>
 224:	nop
 228:	fcvt	s5, h22
 22c:	fcmp	s5, s5
 230:	b.vs	318 <__mulhc3+0x318>
 234:	fcvt	s4, h21
 238:	fcmp	s4, s4
 23c:	b.vs	300 <__mulhc3+0x300>
 240:	fcmp	s19, s19
 244:	b.vs	2e8 <__mulhc3+0x2e8>
 248:	fcmp	s20, s20
 24c:	b.vc	1a8 <__mulhc3+0x1a8>
 250:	umov	w0, v3.h[0]
 254:	movi	v1.4h, #0x0
 258:	tbz	w0, #15, 260 <__mulhc3+0x260>
 25c:	movi	v1.4h, #0x80, lsl #8
 260:	fcvt	s20, h1
 264:	b	1a8 <__mulhc3+0x1a8>
 268:	fabs	s5, s24
 26c:	fcvt	h5, s5
 270:	fcvt	s5, h5
 274:	fcmp	s5, s4
 278:	b.gt	228 <__mulhc3+0x228>
 27c:	ret
 280:	umov	w0, v22.h[0]
 284:	movi	v1.4h, #0x0
 288:	tbz	w0, #15, 290 <__mulhc3+0x290>
 28c:	movi	v1.4h, #0x80, lsl #8
 290:	fcvt	s5, h1
 294:	b	194 <__mulhc3+0x194>
 298:	umov	w0, v21.h[0]
 29c:	movi	v4.4h, #0x0
 2a0:	tbz	w0, #15, 2a8 <__mulhc3+0x2a8>
 2a4:	movi	v4.4h, #0x80, lsl #8
 2a8:	fcvt	s19, h2
 2ac:	fcvt	s4, h4
 2b0:	fcvt	s20, h3
 2b4:	b	1a8 <__mulhc3+0x1a8>
 2b8:	umov	w1, v2.h[0]
 2bc:	movi	v2.4h, #0x0
 2c0:	tbz	w1, #15, 2c8 <__mulhc3+0x2c8>
 2c4:	movi	v2.4h, #0x80, lsl #8
 2c8:	fcvt	s19, h2
 2cc:	b	fc <__mulhc3+0xfc>
 2d0:	umov	w1, v3.h[0]
 2d4:	movi	v3.4h, #0x0
 2d8:	tbz	w1, #15, 2e0 <__mulhc3+0x2e0>
 2dc:	movi	v3.4h, #0x80, lsl #8
 2e0:	fcvt	s20, h3
 2e4:	b	104 <__mulhc3+0x104>
 2e8:	umov	w0, v2.h[0]
 2ec:	movi	v19.4h, #0x0
 2f0:	tbz	w0, #15, 2f8 <__mulhc3+0x2f8>
 2f4:	movi	v19.4h, #0x80, lsl #8
 2f8:	fcvt	s19, h19
 2fc:	b	248 <__mulhc3+0x248>
 300:	umov	w0, v21.h[0]
 304:	movi	v0.4h, #0x0
 308:	tbz	w0, #15, 310 <__mulhc3+0x310>
 30c:	movi	v0.4h, #0x80, lsl #8
 310:	fcvt	s4, h0
 314:	b	240 <__mulhc3+0x240>
 318:	umov	w0, v22.h[0]
 31c:	movi	v1.4h, #0x0
 320:	tbz	w0, #15, 328 <__mulhc3+0x328>
 324:	movi	v1.4h, #0x80, lsl #8
 328:	fcvt	s5, h1
 32c:	b	234 <__mulhc3+0x234>
 330:	fcvt	s5, h22
 334:	fcvt	s4, h21
 338:	b	1a8 <__mulhc3+0x1a8>

_mulsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s7, s1, s3
   4:	fmul	s5, s0, s2
   8:	fmul	s16, s0, s3
   c:	fmov	s18, s0
  10:	fmul	s17, s2, s1
  14:	fmov	s4, s1
  18:	fsub	s0, s5, s7
  1c:	fadd	s1, s16, s17
  20:	fcmp	s0, s0
  24:	cset	w0, vs
  28:	fcmp	s1, s1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	164 <__mulsc3+0x164>  // b.none
  38:	fabs	s19, s18
  3c:	mov	w1, #0x7f7fffff            	// #2139095039
  40:	fmov	s6, w1
  44:	fabs	s21, s4
  48:	fcmp	s19, s6
  4c:	b.le	168 <__mulsc3+0x168>
  50:	mov	w1, #0x7f7fffff            	// #2139095039
  54:	fmov	s6, w1
  58:	movi	v20.2s, #0x80, lsl #24
  5c:	fcmp	s19, s6
  60:	cset	w1, gt
  64:	fcmp	s21, s6
  68:	scvtf	s19, w1
  6c:	cset	w1, gt
  70:	fcmp	s2, s2
  74:	scvtf	s6, w1
  78:	bif	v18.8b, v19.8b, v20.8b
  7c:	bif	v4.8b, v6.8b, v20.8b
  80:	b.vs	1a4 <__mulsc3+0x1a4>
  84:	fcmp	s3, s3
  88:	b.vs	1b0 <__mulsc3+0x1b0>
  8c:	fabs	s19, s2
  90:	mov	w1, #0x7f7fffff            	// #2139095039
  94:	fmov	s6, w1
  98:	fabs	s20, s3
  9c:	fcmp	s19, s6
  a0:	b.gt	108 <__mulsc3+0x108>
  a4:	fcmp	s20, s6
  a8:	b.gt	108 <__mulsc3+0x108>
  ac:	cbnz	w0, 144 <__mulsc3+0x144>
  b0:	fabs	s5, s5
  b4:	fcmp	s5, s6
  b8:	b.gt	d8 <__mulsc3+0xd8>
  bc:	fabs	s7, s7
  c0:	fcmp	s7, s6
  c4:	b.gt	d8 <__mulsc3+0xd8>
  c8:	fabs	s16, s16
  cc:	fcmp	s16, s6
  d0:	b.le	178 <__mulsc3+0x178>
  d4:	nop
  d8:	fcmp	s18, s18
  dc:	b.vs	1e0 <__mulsc3+0x1e0>
  e0:	fcmp	s4, s4
  e4:	b.vs	1d0 <__mulsc3+0x1d0>
  e8:	fcmp	s2, s2
  ec:	b.vs	1c0 <__mulsc3+0x1c0>
  f0:	fcmp	s3, s3
  f4:	b.vc	144 <__mulsc3+0x144>
  f8:	movi	v0.2s, #0x0
  fc:	movi	v1.2s, #0x80, lsl #24
 100:	bif	v3.8b, v0.8b, v1.8b
 104:	b	144 <__mulsc3+0x144>
 108:	mov	w0, #0x7f7fffff            	// #2139095039
 10c:	fmov	s0, w0
 110:	movi	v5.2s, #0x80, lsl #24
 114:	fcmp	s19, s0
 118:	cset	w0, gt
 11c:	fcmp	s20, s0
 120:	scvtf	s1, w0
 124:	cset	w0, gt
 128:	fcmp	s18, s18
 12c:	scvtf	s0, w0
 130:	bif	v2.8b, v1.8b, v5.8b
 134:	bif	v3.8b, v0.8b, v5.8b
 138:	b.vs	188 <__mulsc3+0x188>
 13c:	fcmp	s4, s4
 140:	b.vs	194 <__mulsc3+0x194>
 144:	fmul	s5, s4, s3
 148:	fmul	s4, s4, s2
 14c:	fmadd	s3, s18, s3, s4
 150:	fnmsub	s2, s18, s2, s5
 154:	mov	w0, #0x7f800000            	// #2139095040
 158:	fmov	s1, w0
 15c:	fmul	s0, s2, s1
 160:	fmul	s1, s3, s1
 164:	ret
 168:	fcmp	s21, s6
 16c:	b.gt	50 <__mulsc3+0x50>
 170:	mov	w0, #0x0                   	// #0
 174:	b	8c <__mulsc3+0x8c>
 178:	fabs	s17, s17
 17c:	fcmp	s17, s6
 180:	b.gt	d8 <__mulsc3+0xd8>
 184:	ret
 188:	movi	v0.2s, #0x0
 18c:	bif	v18.8b, v0.8b, v5.8b
 190:	b	13c <__mulsc3+0x13c>
 194:	movi	v0.2s, #0x0
 198:	movi	v1.2s, #0x80, lsl #24
 19c:	bif	v4.8b, v0.8b, v1.8b
 1a0:	b	144 <__mulsc3+0x144>
 1a4:	movi	v6.2s, #0x0
 1a8:	bif	v2.8b, v6.8b, v20.8b
 1ac:	b	84 <__mulsc3+0x84>
 1b0:	movi	v6.2s, #0x0
 1b4:	movi	v19.2s, #0x80, lsl #24
 1b8:	bif	v3.8b, v6.8b, v19.8b
 1bc:	b	8c <__mulsc3+0x8c>
 1c0:	movi	v0.2s, #0x0
 1c4:	movi	v1.2s, #0x80, lsl #24
 1c8:	bif	v2.8b, v0.8b, v1.8b
 1cc:	b	f0 <__mulsc3+0xf0>
 1d0:	movi	v0.2s, #0x0
 1d4:	movi	v1.2s, #0x80, lsl #24
 1d8:	bif	v4.8b, v0.8b, v1.8b
 1dc:	b	e8 <__mulsc3+0xe8>
 1e0:	movi	v0.2s, #0x0
 1e4:	movi	v1.2s, #0x80, lsl #24
 1e8:	bif	v18.8b, v0.8b, v1.8b
 1ec:	b	e0 <__mulsc3+0xe0>

_muldc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d7, d1, d3
   4:	fmul	d5, d0, d2
   8:	fmul	d16, d0, d3
   c:	fmov	d18, d0
  10:	fmul	d17, d2, d1
  14:	fmov	d4, d1
  18:	fsub	d0, d5, d7
  1c:	fadd	d1, d16, d17
  20:	fcmp	d0, d0
  24:	cset	w0, vs
  28:	fcmp	d1, d1
  2c:	cset	w1, vs
  30:	ands	w0, w0, w1
  34:	b.eq	16c <__muldc3+0x16c>  // b.none
  38:	fabs	d19, d18
  3c:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  40:	fmov	d6, x1
  44:	fabs	d21, d4
  48:	fcmp	d19, d6
  4c:	b.le	170 <__muldc3+0x170>
  50:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  54:	fmov	d6, x1
  58:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  5c:	fmov	d20, x1
  60:	fcmp	d19, d6
  64:	cset	w1, gt
  68:	fcmp	d21, d6
  6c:	scvtf	d19, w1
  70:	cset	w1, gt
  74:	fcmp	d2, d2
  78:	scvtf	d6, w1
  7c:	bif	v18.8b, v19.8b, v20.8b
  80:	bif	v4.8b, v6.8b, v20.8b
  84:	b.vs	1b0 <__muldc3+0x1b0>
  88:	fcmp	d3, d3
  8c:	b.vs	1bc <__muldc3+0x1bc>
  90:	fabs	d19, d2
  94:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  98:	fmov	d6, x1
  9c:	fabs	d20, d3
  a0:	fcmp	d19, d6
  a4:	b.gt	10c <__muldc3+0x10c>
  a8:	fcmp	d20, d6
  ac:	b.gt	10c <__muldc3+0x10c>
  b0:	cbnz	w0, 14c <__muldc3+0x14c>
  b4:	fabs	d5, d5
  b8:	fcmp	d5, d6
  bc:	b.gt	d8 <__muldc3+0xd8>
  c0:	fabs	d7, d7
  c4:	fcmp	d7, d6
  c8:	b.gt	d8 <__muldc3+0xd8>
  cc:	fabs	d16, d16
  d0:	fcmp	d16, d6
  d4:	b.le	180 <__muldc3+0x180>
  d8:	fcmp	d18, d18
  dc:	b.vs	1f8 <__muldc3+0x1f8>
  e0:	fcmp	d4, d4
  e4:	b.vs	1e4 <__muldc3+0x1e4>
  e8:	fcmp	d2, d2
  ec:	b.vs	1d0 <__muldc3+0x1d0>
  f0:	fcmp	d3, d3
  f4:	b.vc	14c <__muldc3+0x14c>
  f8:	movi	d0, #0x0
  fc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 100:	fmov	d1, x0
 104:	bif	v3.8b, v0.8b, v1.8b
 108:	b	14c <__muldc3+0x14c>
 10c:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 110:	fmov	d0, x0
 114:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 118:	fmov	d5, x0
 11c:	fcmp	d19, d0
 120:	cset	w0, gt
 124:	fcmp	d20, d0
 128:	scvtf	d1, w0
 12c:	cset	w0, gt
 130:	fcmp	d18, d18
 134:	scvtf	d0, w0
 138:	bif	v2.8b, v1.8b, v5.8b
 13c:	bif	v3.8b, v0.8b, v5.8b
 140:	b.vs	190 <__muldc3+0x190>
 144:	fcmp	d4, d4
 148:	b.vs	19c <__muldc3+0x19c>
 14c:	fmul	d5, d4, d3
 150:	fmul	d4, d4, d2
 154:	fmadd	d3, d18, d3, d4
 158:	fnmsub	d2, d18, d2, d5
 15c:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 160:	fmov	d1, x0
 164:	fmul	d0, d2, d1
 168:	fmul	d1, d3, d1
 16c:	ret
 170:	fcmp	d21, d6
 174:	b.gt	50 <__muldc3+0x50>
 178:	mov	w0, #0x0                   	// #0
 17c:	b	90 <__muldc3+0x90>
 180:	fabs	d17, d17
 184:	fcmp	d17, d6
 188:	b.gt	d8 <__muldc3+0xd8>
 18c:	ret
 190:	movi	d0, #0x0
 194:	bif	v18.8b, v0.8b, v5.8b
 198:	b	144 <__muldc3+0x144>
 19c:	movi	d0, #0x0
 1a0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1a4:	fmov	d1, x0
 1a8:	bif	v4.8b, v0.8b, v1.8b
 1ac:	b	14c <__muldc3+0x14c>
 1b0:	movi	d6, #0x0
 1b4:	bif	v2.8b, v6.8b, v20.8b
 1b8:	b	88 <__muldc3+0x88>
 1bc:	movi	d6, #0x0
 1c0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 1c4:	fmov	d19, x1
 1c8:	bif	v3.8b, v6.8b, v19.8b
 1cc:	b	90 <__muldc3+0x90>
 1d0:	movi	d0, #0x0
 1d4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1d8:	fmov	d1, x0
 1dc:	bif	v2.8b, v0.8b, v1.8b
 1e0:	b	f0 <__muldc3+0xf0>
 1e4:	movi	d0, #0x0
 1e8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1ec:	fmov	d1, x0
 1f0:	bif	v4.8b, v0.8b, v1.8b
 1f4:	b	e8 <__muldc3+0xe8>
 1f8:	movi	d0, #0x0
 1fc:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 200:	fmov	d1, x0
 204:	bif	v18.8b, v0.8b, v1.8b
 208:	b	e0 <__muldc3+0xe0>

_mulxc3.o:     file format elf64-littleaarch64


_multc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-256]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x23, x24, [sp, #48]
  10:	stp	x25, x26, [sp, #64]
  14:	str	q0, [sp, #96]
  18:	str	q2, [sp, #112]
  1c:	ldp	x20, x25, [sp, #96]
  20:	stp	x21, x22, [sp, #32]
  24:	ldp	x19, x24, [sp, #112]
  28:	stp	x19, x24, [sp, #96]
  2c:	stp	x20, x25, [sp, #112]
  30:	stp	x27, x28, [sp, #80]
  34:	ldr	q0, [sp, #112]
  38:	str	q1, [sp, #128]
  3c:	str	q3, [sp, #144]
  40:	ldp	x21, x23, [sp, #128]
  44:	ldp	x22, x26, [sp, #144]
  48:	ldr	q1, [sp, #96]
  4c:	bl	0 <__multf3>
  50:	stp	x22, x26, [sp, #96]
  54:	stp	x21, x23, [sp, #112]
  58:	ldr	q1, [sp, #96]
  5c:	str	q0, [sp, #128]
  60:	ldr	q0, [sp, #112]
  64:	ldp	x28, x27, [sp, #128]
  68:	bl	0 <__multf3>
  6c:	str	q0, [sp, #128]
  70:	ldp	x1, x0, [sp, #128]
  74:	stp	x22, x26, [sp, #96]
  78:	stp	x20, x25, [sp, #112]
  7c:	ldr	q1, [sp, #96]
  80:	ldr	q0, [sp, #112]
  84:	str	x1, [sp, #144]
  88:	str	x0, [sp, #168]
  8c:	bl	0 <__multf3>
  90:	str	q0, [sp, #128]
  94:	ldp	x1, x0, [sp, #128]
  98:	stp	x21, x23, [sp, #96]
  9c:	stp	x19, x24, [sp, #112]
  a0:	ldr	q1, [sp, #96]
  a4:	ldr	q0, [sp, #112]
  a8:	stp	x1, x0, [sp, #176]
  ac:	bl	0 <__multf3>
  b0:	str	q0, [sp, #128]
  b4:	ldp	x0, x2, [sp, #136]
  b8:	str	x2, [sp, #96]
  bc:	ldr	x2, [sp, #168]
  c0:	str	x2, [sp, #104]
  c4:	stp	x28, x27, [sp, #112]
  c8:	ldr	q1, [sp, #96]
  cc:	ldr	q0, [sp, #112]
  d0:	str	x0, [sp, #224]
  d4:	bl	0 <__subtf3>
  d8:	ldr	x1, [sp, #176]
  dc:	str	x1, [sp, #112]
  e0:	ldr	x2, [sp, #128]
  e4:	str	x2, [sp, #96]
  e8:	ldr	x1, [sp, #224]
  ec:	str	x1, [sp, #104]
  f0:	ldr	x0, [sp, #184]
  f4:	ldr	q1, [sp, #96]
  f8:	str	x0, [sp, #120]
  fc:	str	q0, [sp, #96]
 100:	ldr	q0, [sp, #112]
 104:	bl	0 <__addtf3>
 108:	str	q0, [sp, #112]
 10c:	ldr	q1, [sp, #96]
 110:	mov	v0.16b, v1.16b
 114:	bl	0 <__unordtf2>
 118:	cmp	w0, #0x0
 11c:	ldr	q0, [sp, #112]
 120:	cset	w1, ne  // ne = any
 124:	str	w1, [sp, #192]
 128:	mov	v1.16b, v0.16b
 12c:	bl	0 <__unordtf2>
 130:	cmp	w0, #0x0
 134:	ldr	w1, [sp, #192]
 138:	cset	w0, ne  // ne = any
 13c:	ands	w0, w0, w1
 140:	str	w0, [sp, #236]
 144:	b.eq	470 <__multc3+0x470>  // b.none
 148:	adrp	x1, 0 <__multc3>
 14c:	add	x1, x1, #0x0
 150:	and	x0, x25, #0x7fffffffffffffff
 154:	stp	x20, x0, [sp, #192]
 158:	ldr	q1, [x1]
 15c:	and	x1, x23, #0x7fffffffffffffff
 160:	ldr	q0, [sp, #192]
 164:	str	x1, [sp, #192]
 168:	str	x0, [sp, #208]
 16c:	bl	0 <__unordtf2>
 170:	cbnz	w0, 198 <__multc3+0x198>
 174:	adrp	x1, 0 <__multc3>
 178:	add	x1, x1, #0x0
 17c:	ldr	x0, [sp, #208]
 180:	stp	x20, x0, [sp, #192]
 184:	ldr	q1, [x1]
 188:	ldr	q0, [sp, #192]
 18c:	bl	0 <__letf2>
 190:	cmp	w0, #0x0
 194:	b.gt	1dc <__multc3+0x1dc>
 198:	adrp	x0, 0 <__multc3>
 19c:	add	x0, x0, #0x0
 1a0:	ldr	q1, [x0]
 1a4:	and	x0, x23, #0x7fffffffffffffff
 1a8:	stp	x21, x0, [sp, #192]
 1ac:	ldr	q0, [sp, #192]
 1b0:	bl	0 <__unordtf2>
 1b4:	cbnz	w0, 678 <__multc3+0x678>
 1b8:	adrp	x1, 0 <__multc3>
 1bc:	add	x1, x1, #0x0
 1c0:	and	x0, x23, #0x7fffffffffffffff
 1c4:	stp	x21, x0, [sp, #192]
 1c8:	ldr	q1, [x1]
 1cc:	ldr	q0, [sp, #192]
 1d0:	bl	0 <__letf2>
 1d4:	cmp	w0, #0x0
 1d8:	b.le	678 <__multc3+0x678>
 1dc:	adrp	x0, 0 <__multc3>
 1e0:	add	x0, x0, #0x0
 1e4:	mov	w1, #0x1                   	// #1
 1e8:	str	w1, [sp, #240]
 1ec:	ldr	q1, [x0]
 1f0:	ldr	x0, [sp, #208]
 1f4:	stp	x20, x0, [sp, #192]
 1f8:	ldr	q0, [sp, #192]
 1fc:	bl	0 <__unordtf2>
 200:	ldr	w1, [sp, #240]
 204:	cbz	w0, 504 <__multc3+0x504>
 208:	eor	w0, w1, #0x1
 20c:	mov	w1, #0x1                   	// #1
 210:	and	w0, w0, #0x1
 214:	str	w1, [sp, #240]
 218:	bl	0 <__floatsitf>
 21c:	str	q0, [sp, #208]
 220:	ldp	x20, x0, [sp, #208]
 224:	and	x1, x23, #0x7fffffffffffffff
 228:	stp	x21, x1, [sp, #192]
 22c:	ldr	q0, [sp, #192]
 230:	bfxil	x25, x0, #0, #63
 234:	adrp	x0, 0 <__multc3>
 238:	add	x0, x0, #0x0
 23c:	ldr	q1, [x0]
 240:	bl	0 <__unordtf2>
 244:	ldr	w1, [sp, #240]
 248:	cbz	w0, 4dc <__multc3+0x4dc>
 24c:	eor	w0, w1, #0x1
 250:	and	w0, w0, #0x1
 254:	bl	0 <__floatsitf>
 258:	str	q0, [sp, #240]
 25c:	ldp	x21, x0, [sp, #240]
 260:	stp	x19, x24, [sp, #192]
 264:	stp	x19, x24, [sp, #208]
 268:	ldr	q1, [sp, #192]
 26c:	ldr	q0, [sp, #208]
 270:	bfxil	x23, x0, #0, #63
 274:	bl	0 <__unordtf2>
 278:	cbnz	w0, 714 <__multc3+0x714>
 27c:	stp	x22, x26, [sp, #192]
 280:	stp	x22, x26, [sp, #208]
 284:	ldr	q1, [sp, #192]
 288:	ldr	q0, [sp, #208]
 28c:	bl	0 <__unordtf2>
 290:	cbnz	w0, 6fc <__multc3+0x6fc>
 294:	adrp	x1, 0 <__multc3>
 298:	add	x1, x1, #0x0
 29c:	and	x0, x24, #0x7fffffffffffffff
 2a0:	stp	x19, x0, [sp, #192]
 2a4:	ldr	q1, [x1]
 2a8:	and	x1, x26, #0x7fffffffffffffff
 2ac:	ldr	q0, [sp, #192]
 2b0:	str	x1, [sp, #192]
 2b4:	str	x0, [sp, #208]
 2b8:	bl	0 <__unordtf2>
 2bc:	cbnz	w0, 2e4 <__multc3+0x2e4>
 2c0:	adrp	x1, 0 <__multc3>
 2c4:	add	x1, x1, #0x0
 2c8:	ldr	x0, [sp, #208]
 2cc:	stp	x19, x0, [sp, #192]
 2d0:	ldr	q1, [x1]
 2d4:	ldr	q0, [sp, #192]
 2d8:	bl	0 <__letf2>
 2dc:	cmp	w0, #0x0
 2e0:	b.gt	328 <__multc3+0x328>
 2e4:	adrp	x0, 0 <__multc3>
 2e8:	add	x0, x0, #0x0
 2ec:	ldr	q1, [x0]
 2f0:	and	x0, x26, #0x7fffffffffffffff
 2f4:	stp	x22, x0, [sp, #192]
 2f8:	ldr	q0, [sp, #192]
 2fc:	bl	0 <__unordtf2>
 300:	cbnz	w0, 52c <__multc3+0x52c>
 304:	adrp	x1, 0 <__multc3>
 308:	add	x1, x1, #0x0
 30c:	and	x0, x26, #0x7fffffffffffffff
 310:	stp	x22, x0, [sp, #192]
 314:	ldr	q1, [x1]
 318:	ldr	q0, [sp, #192]
 31c:	bl	0 <__letf2>
 320:	cmp	w0, #0x0
 324:	b.le	52c <__multc3+0x52c>
 328:	adrp	x0, 0 <__multc3>
 32c:	add	x0, x0, #0x0
 330:	ldr	x28, [sp, #208]
 334:	stp	x19, x28, [sp, #96]
 338:	ldr	q1, [x0]
 33c:	ldr	q0, [sp, #96]
 340:	mov	w27, #0x1                   	// #1
 344:	bl	0 <__unordtf2>
 348:	cbz	w0, 4b8 <__multc3+0x4b8>
 34c:	eor	w0, w27, #0x1
 350:	and	x28, x26, #0x7fffffffffffffff
 354:	and	w0, w0, #0x1
 358:	bl	0 <__floatsitf>
 35c:	str	q0, [sp, #112]
 360:	mov	w27, #0x1                   	// #1
 364:	ldp	x19, x0, [sp, #112]
 368:	stp	x22, x28, [sp, #96]
 36c:	ldr	q0, [sp, #96]
 370:	bfxil	x24, x0, #0, #63
 374:	adrp	x0, 0 <__multc3>
 378:	add	x0, x0, #0x0
 37c:	ldr	q1, [x0]
 380:	bl	0 <__unordtf2>
 384:	cbz	w0, 494 <__multc3+0x494>
 388:	eor	w0, w27, #0x1
 38c:	and	w0, w0, #0x1
 390:	bl	0 <__floatsitf>
 394:	str	q0, [sp, #128]
 398:	ldp	x22, x0, [sp, #128]
 39c:	stp	x20, x25, [sp, #96]
 3a0:	stp	x20, x25, [sp, #112]
 3a4:	ldr	q1, [sp, #96]
 3a8:	ldr	q0, [sp, #112]
 3ac:	bfxil	x26, x0, #0, #63
 3b0:	bl	0 <__unordtf2>
 3b4:	cbnz	w0, 6cc <__multc3+0x6cc>
 3b8:	stp	x21, x23, [sp, #96]
 3bc:	stp	x21, x23, [sp, #112]
 3c0:	ldr	q1, [sp, #96]
 3c4:	ldr	q0, [sp, #112]
 3c8:	bl	0 <__unordtf2>
 3cc:	cbnz	w0, 6e4 <__multc3+0x6e4>
 3d0:	stp	x19, x24, [sp, #96]
 3d4:	stp	x20, x25, [sp, #112]
 3d8:	ldr	q1, [sp, #96]
 3dc:	ldr	q0, [sp, #112]
 3e0:	bl	0 <__multf3>
 3e4:	stp	x22, x26, [sp, #96]
 3e8:	stp	x21, x23, [sp, #112]
 3ec:	ldr	q1, [sp, #96]
 3f0:	str	q0, [sp, #96]
 3f4:	ldr	q0, [sp, #112]
 3f8:	bl	0 <__multf3>
 3fc:	mov	v1.16b, v0.16b
 400:	ldr	q2, [sp, #96]
 404:	mov	v0.16b, v2.16b
 408:	bl	0 <__subtf3>
 40c:	adrp	x0, 0 <__multc3>
 410:	add	x0, x0, #0x0
 414:	ldr	q1, [x0]
 418:	bl	0 <__multf3>
 41c:	stp	x22, x26, [sp, #96]
 420:	stp	x20, x25, [sp, #112]
 424:	ldr	q1, [sp, #96]
 428:	str	q0, [sp, #96]
 42c:	ldr	q0, [sp, #112]
 430:	bl	0 <__multf3>
 434:	stp	x19, x24, [sp, #112]
 438:	stp	x21, x23, [sp, #128]
 43c:	ldr	q1, [sp, #112]
 440:	str	q0, [sp, #112]
 444:	ldr	q0, [sp, #128]
 448:	bl	0 <__multf3>
 44c:	mov	v1.16b, v0.16b
 450:	ldr	q2, [sp, #112]
 454:	mov	v0.16b, v2.16b
 458:	bl	0 <__addtf3>
 45c:	adrp	x0, 0 <__multc3>
 460:	add	x0, x0, #0x0
 464:	ldr	q1, [x0]
 468:	bl	0 <__multf3>
 46c:	str	q0, [sp, #112]
 470:	ldp	x19, x20, [sp, #16]
 474:	ldp	x21, x22, [sp, #32]
 478:	ldp	x23, x24, [sp, #48]
 47c:	ldp	x25, x26, [sp, #64]
 480:	ldp	x27, x28, [sp, #80]
 484:	ldr	q0, [sp, #96]
 488:	ldr	q1, [sp, #112]
 48c:	ldp	x29, x30, [sp], #256
 490:	ret
 494:	adrp	x0, 0 <__multc3>
 498:	add	x0, x0, #0x0
 49c:	stp	x22, x28, [sp, #96]
 4a0:	ldr	q1, [x0]
 4a4:	ldr	q0, [sp, #96]
 4a8:	bl	0 <__letf2>
 4ac:	cmp	w0, #0x0
 4b0:	cset	w27, le
 4b4:	b	388 <__multc3+0x388>
 4b8:	adrp	x0, 0 <__multc3>
 4bc:	add	x0, x0, #0x0
 4c0:	stp	x19, x28, [sp, #96]
 4c4:	ldr	q1, [x0]
 4c8:	ldr	q0, [sp, #96]
 4cc:	bl	0 <__letf2>
 4d0:	cmp	w0, #0x0
 4d4:	cset	w27, le
 4d8:	b	34c <__multc3+0x34c>
 4dc:	adrp	x0, 0 <__multc3>
 4e0:	add	x0, x0, #0x0
 4e4:	and	x1, x23, #0x7fffffffffffffff
 4e8:	stp	x21, x1, [sp, #192]
 4ec:	ldr	q1, [x0]
 4f0:	ldr	q0, [sp, #192]
 4f4:	bl	0 <__letf2>
 4f8:	cmp	w0, #0x0
 4fc:	cset	w1, le
 500:	b	24c <__multc3+0x24c>
 504:	adrp	x1, 0 <__multc3>
 508:	add	x1, x1, #0x0
 50c:	ldr	x0, [sp, #208]
 510:	stp	x20, x0, [sp, #192]
 514:	ldr	q1, [x1]
 518:	ldr	q0, [sp, #192]
 51c:	bl	0 <__letf2>
 520:	cmp	w0, #0x0
 524:	cset	w1, le
 528:	b	208 <__multc3+0x208>
 52c:	ldr	w0, [sp, #236]
 530:	cbnz	w0, 3d0 <__multc3+0x3d0>
 534:	adrp	x0, 0 <__multc3>
 538:	add	x0, x0, #0x0
 53c:	and	x27, x27, #0x7fffffffffffffff
 540:	stp	x28, x27, [sp, #192]
 544:	ldr	q1, [x0]
 548:	ldr	q0, [sp, #192]
 54c:	bl	0 <__unordtf2>
 550:	cbnz	w0, 574 <__multc3+0x574>
 554:	adrp	x0, 0 <__multc3>
 558:	add	x0, x0, #0x0
 55c:	stp	x28, x27, [sp, #192]
 560:	ldr	q1, [x0]
 564:	ldr	q0, [sp, #192]
 568:	bl	0 <__letf2>
 56c:	cmp	w0, #0x0
 570:	b.gt	600 <__multc3+0x600>
 574:	ldr	x0, [sp, #168]
 578:	ldr	x28, [sp, #144]
 57c:	and	x27, x0, #0x7fffffffffffffff
 580:	adrp	x0, 0 <__multc3>
 584:	add	x0, x0, #0x0
 588:	stp	x28, x27, [sp, #192]
 58c:	ldr	q1, [x0]
 590:	ldr	q0, [sp, #192]
 594:	bl	0 <__unordtf2>
 598:	cbnz	w0, 5bc <__multc3+0x5bc>
 59c:	adrp	x0, 0 <__multc3>
 5a0:	add	x0, x0, #0x0
 5a4:	stp	x28, x27, [sp, #144]
 5a8:	ldr	q1, [x0]
 5ac:	ldr	q0, [sp, #144]
 5b0:	bl	0 <__letf2>
 5b4:	cmp	w0, #0x0
 5b8:	b.gt	600 <__multc3+0x600>
 5bc:	ldp	x28, x0, [sp, #176]
 5c0:	and	x27, x0, #0x7fffffffffffffff
 5c4:	adrp	x0, 0 <__multc3>
 5c8:	add	x0, x0, #0x0
 5cc:	stp	x28, x27, [sp, #144]
 5d0:	ldr	q0, [sp, #144]
 5d4:	ldr	q1, [x0]
 5d8:	bl	0 <__unordtf2>
 5dc:	cbnz	w0, 680 <__multc3+0x680>
 5e0:	adrp	x0, 0 <__multc3>
 5e4:	add	x0, x0, #0x0
 5e8:	stp	x28, x27, [sp, #144]
 5ec:	ldr	q1, [x0]
 5f0:	ldr	q0, [sp, #144]
 5f4:	bl	0 <__letf2>
 5f8:	cmp	w0, #0x0
 5fc:	b.le	680 <__multc3+0x680>
 600:	stp	x20, x25, [sp, #96]
 604:	stp	x20, x25, [sp, #112]
 608:	ldr	q1, [sp, #96]
 60c:	ldr	q0, [sp, #112]
 610:	bl	0 <__unordtf2>
 614:	cbnz	w0, 75c <__multc3+0x75c>
 618:	stp	x21, x23, [sp, #96]
 61c:	stp	x21, x23, [sp, #112]
 620:	ldr	q1, [sp, #96]
 624:	ldr	q0, [sp, #112]
 628:	bl	0 <__unordtf2>
 62c:	cbnz	w0, 744 <__multc3+0x744>
 630:	stp	x19, x24, [sp, #96]
 634:	stp	x19, x24, [sp, #112]
 638:	ldr	q1, [sp, #96]
 63c:	ldr	q0, [sp, #112]
 640:	bl	0 <__unordtf2>
 644:	cbnz	w0, 72c <__multc3+0x72c>
 648:	stp	x22, x26, [sp, #96]
 64c:	stp	x22, x26, [sp, #112]
 650:	ldr	q1, [sp, #96]
 654:	ldr	q0, [sp, #112]
 658:	bl	0 <__unordtf2>
 65c:	cbz	w0, 3d0 <__multc3+0x3d0>
 660:	mov	x22, #0x0                   	// #0
 664:	mov	x0, #0x0                   	// #0
 668:	tbz	x26, #63, 670 <__multc3+0x670>
 66c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 670:	mov	x26, x0
 674:	b	3d0 <__multc3+0x3d0>
 678:	str	wzr, [sp, #236]
 67c:	b	294 <__multc3+0x294>
 680:	ldr	x0, [sp, #224]
 684:	ldr	x28, [sp, #128]
 688:	and	x27, x0, #0x7fffffffffffffff
 68c:	adrp	x0, 0 <__multc3>
 690:	add	x0, x0, #0x0
 694:	stp	x28, x27, [sp, #144]
 698:	ldr	q1, [x0]
 69c:	ldr	q0, [sp, #144]
 6a0:	bl	0 <__unordtf2>
 6a4:	cbnz	w0, 470 <__multc3+0x470>
 6a8:	adrp	x0, 0 <__multc3>
 6ac:	add	x0, x0, #0x0
 6b0:	stp	x28, x27, [sp, #128]
 6b4:	ldr	q1, [x0]
 6b8:	ldr	q0, [sp, #128]
 6bc:	bl	0 <__letf2>
 6c0:	cmp	w0, #0x0
 6c4:	b.gt	600 <__multc3+0x600>
 6c8:	b	470 <__multc3+0x470>
 6cc:	mov	x20, #0x0                   	// #0
 6d0:	mov	x0, #0x0                   	// #0
 6d4:	tbz	x25, #63, 6dc <__multc3+0x6dc>
 6d8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6dc:	mov	x25, x0
 6e0:	b	3b8 <__multc3+0x3b8>
 6e4:	mov	x21, #0x0                   	// #0
 6e8:	mov	x0, #0x0                   	// #0
 6ec:	tbz	x23, #63, 6f4 <__multc3+0x6f4>
 6f0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6f4:	mov	x23, x0
 6f8:	b	3d0 <__multc3+0x3d0>
 6fc:	mov	x22, #0x0                   	// #0
 700:	mov	x0, #0x0                   	// #0
 704:	tbz	x26, #63, 70c <__multc3+0x70c>
 708:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 70c:	mov	x26, x0
 710:	b	294 <__multc3+0x294>
 714:	mov	x19, #0x0                   	// #0
 718:	mov	x0, #0x0                   	// #0
 71c:	tbz	x24, #63, 724 <__multc3+0x724>
 720:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 724:	mov	x24, x0
 728:	b	27c <__multc3+0x27c>
 72c:	mov	x19, #0x0                   	// #0
 730:	mov	x0, #0x0                   	// #0
 734:	tbz	x24, #63, 73c <__multc3+0x73c>
 738:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 73c:	mov	x24, x0
 740:	b	648 <__multc3+0x648>
 744:	mov	x21, #0x0                   	// #0
 748:	mov	x0, #0x0                   	// #0
 74c:	tbz	x23, #63, 754 <__multc3+0x754>
 750:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 754:	mov	x23, x0
 758:	b	630 <__multc3+0x630>
 75c:	mov	x20, #0x0                   	// #0
 760:	mov	x0, #0x0                   	// #0
 764:	tbz	x25, #63, 76c <__multc3+0x76c>
 768:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 76c:	mov	x25, x0
 770:	b	618 <__multc3+0x618>

_divhc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divhc3>:
   0:	fcvt	s19, h2
   4:	fcvt	s16, h3
   8:	mov	v17.h[0], v0.h[0]
   c:	mov	v18.h[0], v1.h[0]
  10:	fcvt	s20, h1
  14:	fcvt	s21, h0
  18:	fabs	s5, s19
  1c:	fabs	s6, s16
  20:	fcvt	h5, s5
  24:	fcvt	h6, s6
  28:	fcvt	s5, h5
  2c:	fcvt	s6, h6
  30:	fcmpe	s5, s6
  34:	b.pl	178 <__divhc3+0x178>  // b.nfrst
  38:	fdiv	s4, s19, s16
  3c:	fcvt	h4, s4
  40:	fcvt	s4, h4
  44:	fmadd	s1, s4, s19, s16
  48:	fmadd	s0, s4, s21, s20
  4c:	fnmsub	s4, s4, s20, s21
  50:	fcvt	h1, s1
  54:	fcvt	s1, h1
  58:	fdiv	s0, s0, s1
  5c:	fdiv	s4, s4, s1
  60:	fcvt	h0, s0
  64:	fcvt	h1, s4
  68:	fcvt	s7, h0
  6c:	fcvt	s4, h1
  70:	fcmp	s7, s7
  74:	cset	w1, vs
  78:	fcmp	s4, s4
  7c:	cset	w0, vs
  80:	tst	w1, w0
  84:	b.eq	174 <__divhc3+0x174>  // b.none
  88:	fcmp	s19, #0.0
  8c:	b.eq	194 <__divhc3+0x194>  // b.none
  90:	fabs	s4, s21
  94:	mov	w0, #0xe000                	// #57344
  98:	movk	w0, #0x477f, lsl #16
  9c:	fmov	s7, w0
  a0:	fcvt	h4, s4
  a4:	fcvt	s4, h4
  a8:	fcmp	s4, s7
  ac:	b.gt	1d8 <__divhc3+0x1d8>
  b0:	fabs	s22, s20
  b4:	fcvt	h22, s22
  b8:	fcvt	s22, h22
  bc:	fcmp	s22, s7
  c0:	b.gt	1d8 <__divhc3+0x1d8>
  c4:	mov	w0, #0xe000                	// #57344
  c8:	movk	w0, #0x477f, lsl #16
  cc:	fmov	s7, w0
  d0:	fcmp	s5, s7
  d4:	cset	w0, le
  d8:	fcmp	s6, s7
  dc:	cset	w1, le
  e0:	cmp	w0, #0x0
  e4:	ccmp	w1, #0x0, #0x4, ne  // ne = any
  e8:	b.ne	174 <__divhc3+0x174>  // b.any
  ec:	fcmp	s4, s7
  f0:	b.hi	174 <__divhc3+0x174>  // b.pmore
  f4:	fabs	s4, s20
  f8:	fcvt	h4, s4
  fc:	fcvt	s4, h4
 100:	fcmp	s4, s7
 104:	b.hi	174 <__divhc3+0x174>  // b.pmore
 108:	eor	w3, w1, #0x1
 10c:	eor	w2, w0, #0x1
 110:	umov	w1, v2.h[0]
 114:	umov	w0, v3.h[0]
 118:	scvtf	d2, w3
 11c:	scvtf	d0, w2
 120:	movi	d4, #0x0
 124:	fcvt	h1, d2
 128:	fcvt	h0, d0
 12c:	umov	w3, v1.h[0]
 130:	umov	w2, v0.h[0]
 134:	bfxil	w0, w3, #0, #15
 138:	bfxil	w1, w2, #0, #15
 13c:	dup	v2.4h, w0
 140:	dup	v1.4h, w1
 144:	fcvt	s2, h2
 148:	fcvt	s3, h1
 14c:	fmul	s0, s2, s20
 150:	fmul	s1, s2, s21
 154:	fmadd	s0, s3, s21, s0
 158:	fnmsub	s1, s3, s20, s1
 15c:	fcvt	d0, s0
 160:	fcvt	d1, s1
 164:	fmul	d0, d0, d4
 168:	fmul	d1, d1, d4
 16c:	fcvt	h0, d0
 170:	fcvt	h1, d1
 174:	ret
 178:	fdiv	s4, s16, s19
 17c:	fcvt	h4, s4
 180:	fcvt	s4, h4
 184:	fmadd	s1, s4, s16, s19
 188:	fmadd	s0, s4, s20, s21
 18c:	fmsub	s4, s4, s21, s20
 190:	b	50 <__divhc3+0x50>
 194:	fcmp	s16, #0.0
 198:	b.ne	90 <__divhc3+0x90>  // b.any
 19c:	fcmp	s21, s21
 1a0:	cset	w0, vc
 1a4:	fcmp	s20, s20
 1a8:	cset	w1, vc
 1ac:	orr	w0, w0, w1
 1b0:	cbz	w0, 90 <__divhc3+0x90>
 1b4:	umov	w0, v2.h[0]
 1b8:	movi	v1.4h, #0x7c, lsl #8
 1bc:	tbnz	w0, #15, 274 <__divhc3+0x274>
 1c0:	fcvt	s1, h1
 1c4:	fmul	s0, s1, s21
 1c8:	fmul	s1, s1, s20
 1cc:	fcvt	h0, s0
 1d0:	fcvt	h1, s1
 1d4:	ret
 1d8:	mov	w0, #0xe000                	// #57344
 1dc:	movk	w0, #0x477f, lsl #16
 1e0:	fmov	s7, w0
 1e4:	fcmp	s5, s7
 1e8:	b.hi	c4 <__divhc3+0xc4>  // b.pmore
 1ec:	fcmp	s6, s7
 1f0:	b.hi	c4 <__divhc3+0xc4>  // b.pmore
 1f4:	fcmp	s4, s7
 1f8:	mov	w2, #0x7f800000            	// #2139095040
 1fc:	fmov	s4, w2
 200:	fabs	s1, s20
 204:	umov	w0, v17.h[0]
 208:	umov	w1, v18.h[0]
 20c:	cset	w2, gt
 210:	fcvt	h1, s1
 214:	scvtf	d2, w2
 218:	fcvt	s1, h1
 21c:	fcvt	h0, d2
 220:	fcmp	s1, s7
 224:	umov	w2, v0.h[0]
 228:	bfxil	w0, w2, #0, #15
 22c:	dup	v0.4h, w0
 230:	cset	w0, gt
 234:	scvtf	d1, w0
 238:	fcvt	s2, h0
 23c:	fcvt	h0, d1
 240:	fmul	s1, s2, s16
 244:	umov	w0, v0.h[0]
 248:	bfxil	w1, w0, #0, #15
 24c:	dup	v3.4h, w1
 250:	fcvt	s3, h3
 254:	fmul	s0, s3, s16
 258:	fnmsub	s1, s3, s19, s1
 25c:	fmadd	s0, s2, s19, s0
 260:	fmul	s1, s1, s4
 264:	fmul	s0, s0, s4
 268:	fcvt	h1, s1
 26c:	fcvt	h0, s0
 270:	ret
 274:	movi	v1.4h, #0xfc, lsl #8
 278:	b	1c0 <__divhc3+0x1c0>

_divsc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	fabs	s6, s2
   4:	fabs	s7, s3
   8:	fmov	s16, s0
   c:	fmov	s17, s1
  10:	fcmpe	s6, s7
  14:	b.pl	74 <__divsc3+0x74>  // b.nfrst
  18:	fdiv	s4, s2, s3
  1c:	fmadd	s1, s2, s4, s3
  20:	fmadd	s5, s0, s4, s17
  24:	fnmsub	s4, s17, s4, s0
  28:	fdiv	s0, s5, s1
  2c:	fdiv	s1, s4, s1
  30:	fcmp	s0, s0
  34:	fccmp	s1, s1, #0x0, vs
  38:	b.vc	70 <__divsc3+0x70>
  3c:	fcmp	s2, #0.0
  40:	movi	v4.2s, #0x0
  44:	fccmp	s3, s4, #0x0, eq  // eq = none
  48:	b.ne	90 <__divsc3+0x90>  // b.any
  4c:	fcmp	s16, s16
  50:	fccmp	s17, s17, #0x0, vs
  54:	b.vs	90 <__divsc3+0x90>
  58:	movi	v1.2s, #0x80, lsl #24
  5c:	mov	w0, #0x7f800000            	// #2139095040
  60:	fmov	s3, w0
  64:	bif	v2.8b, v3.8b, v1.8b
  68:	fmul	s0, s2, s16
  6c:	fmul	s1, s2, s17
  70:	ret
  74:	fdiv	s4, s3, s2
  78:	fmadd	s1, s3, s4, s2
  7c:	fmadd	s5, s4, s17, s0
  80:	fmsub	s4, s4, s0, s17
  84:	fdiv	s0, s5, s1
  88:	fdiv	s1, s4, s1
  8c:	b	30 <__divsc3+0x30>
  90:	fabs	s5, s16
  94:	mov	w0, #0x7f7fffff            	// #2139095039
  98:	fmov	s4, w0
  9c:	fcmp	s5, s4
  a0:	b.gt	124 <__divsc3+0x124>
  a4:	fabs	s18, s17
  a8:	fcmp	s18, s4
  ac:	b.gt	124 <__divsc3+0x124>
  b0:	mov	w0, #0x7f7fffff            	// #2139095039
  b4:	fmov	s4, w0
  b8:	fcmp	s6, s4
  bc:	cset	w1, le
  c0:	fcmp	s7, s4
  c4:	cset	w0, le
  c8:	cmp	w1, #0x0
  cc:	ccmp	w0, #0x0, #0x4, ne  // ne = any
  d0:	b.ne	70 <__divsc3+0x70>  // b.any
  d4:	fcmp	s5, s4
  d8:	b.hi	70 <__divsc3+0x70>  // b.pmore
  dc:	fabs	s5, s17
  e0:	fcmp	s5, s4
  e4:	b.hi	70 <__divsc3+0x70>  // b.pmore
  e8:	eor	w0, w0, #0x1
  ec:	eor	w1, w1, #0x1
  f0:	movi	v5.2s, #0x80, lsl #24
  f4:	movi	v4.2s, #0x0
  f8:	scvtf	s0, w0
  fc:	scvtf	s1, w1
 100:	bit	v0.8b, v3.8b, v5.8b
 104:	bif	v2.8b, v1.8b, v5.8b
 108:	fmul	s1, s16, s0
 10c:	fmul	s5, s17, s0
 110:	fmadd	s0, s16, s2, s5
 114:	fnmsub	s1, s17, s2, s1
 118:	fmul	s0, s0, s4
 11c:	fmul	s1, s1, s4
 120:	ret
 124:	mov	w0, #0x7f7fffff            	// #2139095039
 128:	fmov	s4, w0
 12c:	fcmp	s6, s4
 130:	b.hi	b0 <__divsc3+0xb0>  // b.pmore
 134:	fcmp	s7, s4
 138:	b.hi	b0 <__divsc3+0xb0>  // b.pmore
 13c:	fcmp	s5, s4
 140:	fabs	s0, s17
 144:	mov	w0, #0x7f800000            	// #2139095040
 148:	fmov	s5, w0
 14c:	movi	v6.2s, #0x80, lsl #24
 150:	cset	w0, gt
 154:	fcmp	s0, s4
 158:	scvtf	s0, w0
 15c:	cset	w0, gt
 160:	scvtf	s1, w0
 164:	bif	v16.8b, v0.8b, v6.8b
 168:	bit	v1.8b, v17.8b, v6.8b
 16c:	fmul	s4, s3, s16
 170:	fmul	s0, s3, s1
 174:	fnmsub	s3, s2, s1, s4
 178:	fmadd	s2, s2, s16, s0
 17c:	fmul	s1, s3, s5
 180:	fmul	s0, s2, s5
 184:	ret

_divdc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	fabs	d6, d2
   4:	fabs	d7, d3
   8:	fmov	d16, d0
   c:	fmov	d17, d1
  10:	fcmpe	d6, d7
  14:	b.pl	78 <__divdc3+0x78>  // b.nfrst
  18:	fdiv	d4, d2, d3
  1c:	fmadd	d1, d2, d4, d3
  20:	fmadd	d5, d0, d4, d17
  24:	fnmsub	d4, d17, d4, d0
  28:	fdiv	d0, d5, d1
  2c:	fdiv	d1, d4, d1
  30:	fcmp	d0, d0
  34:	fccmp	d1, d1, #0x0, vs
  38:	b.vc	74 <__divdc3+0x74>
  3c:	fcmp	d2, #0.0
  40:	movi	d4, #0x0
  44:	fccmp	d3, d4, #0x0, eq  // eq = none
  48:	b.ne	94 <__divdc3+0x94>  // b.any
  4c:	fcmp	d16, d16
  50:	fccmp	d17, d17, #0x0, vs
  54:	b.vs	94 <__divdc3+0x94>
  58:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  5c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  60:	fmov	d3, x0
  64:	fmov	d1, x1
  68:	bif	v2.8b, v3.8b, v1.8b
  6c:	fmul	d0, d2, d16
  70:	fmul	d1, d2, d17
  74:	ret
  78:	fdiv	d4, d3, d2
  7c:	fmadd	d1, d3, d4, d2
  80:	fmadd	d5, d4, d17, d0
  84:	fmsub	d4, d4, d0, d17
  88:	fdiv	d0, d5, d1
  8c:	fdiv	d1, d4, d1
  90:	b	30 <__divdc3+0x30>
  94:	fabs	d5, d16
  98:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  9c:	fmov	d4, x0
  a0:	fcmp	d5, d4
  a4:	b.gt	12c <__divdc3+0x12c>
  a8:	fabs	d18, d17
  ac:	fcmp	d18, d4
  b0:	b.gt	12c <__divdc3+0x12c>
  b4:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
  b8:	fmov	d4, x0
  bc:	fcmp	d6, d4
  c0:	cset	w1, le
  c4:	fcmp	d7, d4
  c8:	cset	w0, le
  cc:	cmp	w1, #0x0
  d0:	ccmp	w0, #0x0, #0x4, ne  // ne = any
  d4:	b.ne	74 <__divdc3+0x74>  // b.any
  d8:	fcmp	d5, d4
  dc:	b.hi	74 <__divdc3+0x74>  // b.pmore
  e0:	fabs	d5, d17
  e4:	fcmp	d5, d4
  e8:	b.hi	74 <__divdc3+0x74>  // b.pmore
  ec:	eor	w0, w0, #0x1
  f0:	eor	w1, w1, #0x1
  f4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  f8:	fmov	d5, x2
  fc:	scvtf	d0, w0
 100:	scvtf	d1, w1
 104:	movi	d4, #0x0
 108:	bit	v0.8b, v3.8b, v5.8b
 10c:	bif	v2.8b, v1.8b, v5.8b
 110:	fmul	d1, d16, d0
 114:	fmul	d5, d17, d0
 118:	fmadd	d0, d16, d2, d5
 11c:	fnmsub	d1, d17, d2, d1
 120:	fmul	d0, d0, d4
 124:	fmul	d1, d1, d4
 128:	ret
 12c:	mov	x0, #0x7fefffffffffffff    	// #9218868437227405311
 130:	fmov	d4, x0
 134:	fcmp	d6, d4
 138:	b.hi	b4 <__divdc3+0xb4>  // b.pmore
 13c:	fcmp	d7, d4
 140:	b.hi	b4 <__divdc3+0xb4>  // b.pmore
 144:	fcmp	d5, d4
 148:	fabs	d0, d17
 14c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 150:	fmov	d6, x0
 154:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 158:	fmov	d5, x0
 15c:	cset	w0, gt
 160:	fcmp	d0, d4
 164:	scvtf	d0, w0
 168:	cset	w0, gt
 16c:	scvtf	d1, w0
 170:	bif	v16.8b, v0.8b, v6.8b
 174:	bit	v1.8b, v17.8b, v6.8b
 178:	fmul	d4, d3, d16
 17c:	fmul	d0, d3, d1
 180:	fnmsub	d3, d2, d1, d4
 184:	fmadd	d2, d2, d16, d0
 188:	fmul	d1, d3, d5
 18c:	fmul	d0, d2, d5
 190:	ret

_divxc3.o:     file format elf64-littleaarch64


_divtc3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-176]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x23, x24, [sp, #48]
  14:	str	q2, [sp, #96]
  18:	str	q3, [sp, #112]
  1c:	ldp	x19, x22, [sp, #96]
  20:	stp	x27, x28, [sp, #80]
  24:	ldp	x20, x23, [sp, #112]
  28:	stp	x25, x26, [sp, #64]
  2c:	and	x27, x22, #0x7fffffffffffffff
  30:	stp	x19, x27, [sp, #112]
  34:	str	q0, [sp, #128]
  38:	and	x28, x23, #0x7fffffffffffffff
  3c:	stp	x20, x28, [sp, #96]
  40:	ldr	q0, [sp, #112]
  44:	str	q1, [sp, #144]
  48:	ldp	x21, x25, [sp, #128]
  4c:	ldp	x24, x26, [sp, #144]
  50:	ldr	q1, [sp, #96]
  54:	bl	0 <__lttf2>
  58:	tbz	w0, #31, 370 <__divtc3+0x370>
  5c:	stp	x20, x23, [sp, #96]
  60:	stp	x19, x22, [sp, #112]
  64:	ldr	q1, [sp, #96]
  68:	ldr	q0, [sp, #112]
  6c:	bl	0 <__divtf3>
  70:	stp	x19, x22, [sp, #96]
  74:	mov	v1.16b, v0.16b
  78:	str	q0, [sp, #112]
  7c:	ldr	q0, [sp, #96]
  80:	bl	0 <__multf3>
  84:	stp	x20, x23, [sp, #96]
  88:	ldr	q1, [sp, #96]
  8c:	bl	0 <__addtf3>
  90:	stp	x21, x25, [sp, #96]
  94:	ldr	q2, [sp, #112]
  98:	str	q0, [sp, #112]
  9c:	ldr	q0, [sp, #96]
  a0:	mov	v1.16b, v2.16b
  a4:	str	q2, [sp, #144]
  a8:	bl	0 <__multf3>
  ac:	stp	x24, x26, [sp, #96]
  b0:	ldr	q1, [sp, #96]
  b4:	bl	0 <__addtf3>
  b8:	ldr	q4, [sp, #112]
  bc:	mov	v1.16b, v4.16b
  c0:	str	q4, [sp, #128]
  c4:	bl	0 <__divtf3>
  c8:	stp	x24, x26, [sp, #112]
  cc:	ldr	q2, [sp, #144]
  d0:	str	q0, [sp, #96]
  d4:	ldr	q0, [sp, #112]
  d8:	mov	v1.16b, v2.16b
  dc:	bl	0 <__multf3>
  e0:	stp	x21, x25, [sp, #112]
  e4:	ldr	q1, [sp, #112]
  e8:	bl	0 <__subtf3>
  ec:	ldr	q4, [sp, #128]
  f0:	mov	v1.16b, v4.16b
  f4:	bl	0 <__divtf3>
  f8:	str	q0, [sp, #112]
  fc:	ldr	q0, [sp, #96]
 100:	mov	v1.16b, v0.16b
 104:	bl	0 <__unordtf2>
 108:	cmp	w0, #0x0
 10c:	ldr	q0, [sp, #112]
 110:	cset	w1, ne  // ne = any
 114:	str	w1, [sp, #128]
 118:	mov	v1.16b, v0.16b
 11c:	bl	0 <__unordtf2>
 120:	cmp	w0, #0x0
 124:	ldr	w1, [sp, #128]
 128:	cset	w0, ne  // ne = any
 12c:	tst	w0, w1
 130:	b.eq	34c <__divtc3+0x34c>  // b.none
 134:	movi	v1.2d, #0x0
 138:	stp	x19, x22, [sp, #128]
 13c:	ldr	q0, [sp, #128]
 140:	bl	0 <__eqtf2>
 144:	stp	x20, x23, [sp, #128]
 148:	cmp	w0, #0x0
 14c:	movi	v1.2d, #0x0
 150:	cset	w1, eq  // eq = none
 154:	ldr	q0, [sp, #128]
 158:	str	w1, [sp, #144]
 15c:	bl	0 <__eqtf2>
 160:	cmp	w0, #0x0
 164:	ldr	w1, [sp, #144]
 168:	cset	w0, eq  // eq = none
 16c:	tst	w0, w1
 170:	b.ne	408 <__divtc3+0x408>  // b.any
 174:	adrp	x1, 0 <__divtc3>
 178:	add	x1, x1, #0x0
 17c:	and	x0, x25, #0x7fffffffffffffff
 180:	stp	x21, x0, [sp, #128]
 184:	ldr	q1, [x1]
 188:	ldr	q0, [sp, #128]
 18c:	str	x0, [sp, #144]
 190:	bl	0 <__unordtf2>
 194:	cbnz	w0, 4a0 <__divtc3+0x4a0>
 198:	adrp	x1, 0 <__divtc3>
 19c:	add	x1, x1, #0x0
 1a0:	ldr	x0, [sp, #144]
 1a4:	stp	x21, x0, [sp, #128]
 1a8:	ldr	q1, [x1]
 1ac:	ldr	q0, [sp, #128]
 1b0:	bl	0 <__letf2>
 1b4:	cmp	w0, #0x0
 1b8:	b.le	4a0 <__divtc3+0x4a0>
 1bc:	adrp	x0, 0 <__divtc3>
 1c0:	add	x0, x0, #0x0
 1c4:	stp	x19, x27, [sp, #128]
 1c8:	ldr	q1, [x0]
 1cc:	ldr	q0, [sp, #128]
 1d0:	bl	0 <__unordtf2>
 1d4:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 1d8:	adrp	x0, 0 <__divtc3>
 1dc:	add	x0, x0, #0x0
 1e0:	stp	x19, x27, [sp, #128]
 1e4:	ldr	q1, [x0]
 1e8:	ldr	q0, [sp, #128]
 1ec:	bl	0 <__gttf2>
 1f0:	cmp	w0, #0x0
 1f4:	b.gt	4e8 <__divtc3+0x4e8>
 1f8:	adrp	x0, 0 <__divtc3>
 1fc:	add	x0, x0, #0x0
 200:	stp	x20, x28, [sp, #128]
 204:	ldr	q1, [x0]
 208:	ldr	q0, [sp, #128]
 20c:	bl	0 <__unordtf2>
 210:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 214:	adrp	x0, 0 <__divtc3>
 218:	add	x0, x0, #0x0
 21c:	stp	x20, x28, [sp, #128]
 220:	ldr	q1, [x0]
 224:	ldr	q0, [sp, #128]
 228:	bl	0 <__gttf2>
 22c:	cmp	w0, #0x0
 230:	b.gt	4e8 <__divtc3+0x4e8>
 234:	adrp	x0, 0 <__divtc3>
 238:	add	x0, x0, #0x0
 23c:	ldr	x28, [sp, #144]
 240:	stp	x21, x28, [sp, #96]
 244:	ldr	q1, [x0]
 248:	ldr	q0, [sp, #96]
 24c:	mov	w27, #0x1                   	// #1
 250:	bl	0 <__unordtf2>
 254:	cbz	w0, 6ec <__divtc3+0x6ec>
 258:	eor	w0, w27, #0x1
 25c:	and	x27, x26, #0x7fffffffffffffff
 260:	and	w0, w0, #0x1
 264:	bl	0 <__floatsitf>
 268:	str	q0, [sp, #112]
 26c:	mov	w28, #0x1                   	// #1
 270:	ldp	x21, x0, [sp, #112]
 274:	stp	x24, x27, [sp, #96]
 278:	ldr	q0, [sp, #96]
 27c:	bfxil	x25, x0, #0, #63
 280:	adrp	x0, 0 <__divtc3>
 284:	add	x0, x0, #0x0
 288:	ldr	q1, [x0]
 28c:	bl	0 <__unordtf2>
 290:	cbz	w0, 6c8 <__divtc3+0x6c8>
 294:	eor	w0, w28, #0x1
 298:	and	w0, w0, #0x1
 29c:	bl	0 <__floatsitf>
 2a0:	str	q0, [sp, #128]
 2a4:	ldp	x24, x0, [sp, #128]
 2a8:	stp	x21, x25, [sp, #96]
 2ac:	stp	x19, x22, [sp, #112]
 2b0:	ldr	q1, [sp, #96]
 2b4:	ldr	q0, [sp, #112]
 2b8:	bfxil	x26, x0, #0, #63
 2bc:	bl	0 <__multf3>
 2c0:	stp	x24, x26, [sp, #96]
 2c4:	stp	x20, x23, [sp, #112]
 2c8:	ldr	q1, [sp, #96]
 2cc:	str	q0, [sp, #96]
 2d0:	ldr	q0, [sp, #112]
 2d4:	bl	0 <__multf3>
 2d8:	mov	v1.16b, v0.16b
 2dc:	ldr	q2, [sp, #96]
 2e0:	mov	v0.16b, v2.16b
 2e4:	bl	0 <__addtf3>
 2e8:	adrp	x0, 0 <__divtc3>
 2ec:	add	x0, x0, #0x0
 2f0:	ldr	q1, [x0]
 2f4:	bl	0 <__multf3>
 2f8:	stp	x24, x26, [sp, #96]
 2fc:	stp	x19, x22, [sp, #112]
 300:	ldr	q1, [sp, #96]
 304:	str	q0, [sp, #96]
 308:	ldr	q0, [sp, #112]
 30c:	bl	0 <__multf3>
 310:	stp	x21, x25, [sp, #112]
 314:	stp	x20, x23, [sp, #128]
 318:	ldr	q1, [sp, #112]
 31c:	str	q0, [sp, #112]
 320:	ldr	q0, [sp, #128]
 324:	bl	0 <__multf3>
 328:	mov	v1.16b, v0.16b
 32c:	ldr	q2, [sp, #112]
 330:	mov	v0.16b, v2.16b
 334:	bl	0 <__subtf3>
 338:	adrp	x0, 0 <__divtc3>
 33c:	add	x0, x0, #0x0
 340:	ldr	q1, [x0]
 344:	bl	0 <__multf3>
 348:	str	q0, [sp, #112]
 34c:	ldp	x19, x20, [sp, #16]
 350:	ldp	x21, x22, [sp, #32]
 354:	ldp	x23, x24, [sp, #48]
 358:	ldp	x25, x26, [sp, #64]
 35c:	ldp	x27, x28, [sp, #80]
 360:	ldr	q0, [sp, #96]
 364:	ldr	q1, [sp, #112]
 368:	ldp	x29, x30, [sp], #176
 36c:	ret
 370:	stp	x19, x22, [sp, #96]
 374:	stp	x20, x23, [sp, #112]
 378:	ldr	q1, [sp, #96]
 37c:	ldr	q0, [sp, #112]
 380:	bl	0 <__divtf3>
 384:	stp	x20, x23, [sp, #96]
 388:	mov	v1.16b, v0.16b
 38c:	str	q0, [sp, #128]
 390:	ldr	q0, [sp, #96]
 394:	bl	0 <__multf3>
 398:	stp	x19, x22, [sp, #96]
 39c:	ldr	q1, [sp, #96]
 3a0:	bl	0 <__addtf3>
 3a4:	stp	x24, x26, [sp, #96]
 3a8:	ldr	q2, [sp, #128]
 3ac:	ldr	q1, [sp, #96]
 3b0:	str	q0, [sp, #112]
 3b4:	mov	v0.16b, v2.16b
 3b8:	str	q2, [sp, #144]
 3bc:	bl	0 <__multf3>
 3c0:	stp	x21, x25, [sp, #96]
 3c4:	ldr	q1, [sp, #96]
 3c8:	bl	0 <__addtf3>
 3cc:	ldr	q4, [sp, #112]
 3d0:	mov	v1.16b, v4.16b
 3d4:	str	q4, [sp, #128]
 3d8:	bl	0 <__divtf3>
 3dc:	stp	x21, x25, [sp, #112]
 3e0:	mov	v6.16b, v0.16b
 3e4:	ldr	q1, [sp, #112]
 3e8:	ldr	q2, [sp, #144]
 3ec:	str	q6, [sp, #96]
 3f0:	mov	v0.16b, v2.16b
 3f4:	bl	0 <__multf3>
 3f8:	stp	x24, x26, [sp, #112]
 3fc:	mov	v1.16b, v0.16b
 400:	ldr	q0, [sp, #112]
 404:	b	e8 <__divtc3+0xe8>
 408:	stp	x21, x25, [sp, #128]
 40c:	stp	x21, x25, [sp, #144]
 410:	ldr	q1, [sp, #128]
 414:	ldr	q0, [sp, #144]
 418:	bl	0 <__unordtf2>
 41c:	stp	x24, x26, [sp, #128]
 420:	cmp	w0, #0x0
 424:	stp	x24, x26, [sp, #144]
 428:	cset	w1, eq  // eq = none
 42c:	ldr	q1, [sp, #128]
 430:	ldr	q0, [sp, #144]
 434:	str	w1, [sp, #168]
 438:	bl	0 <__unordtf2>
 43c:	cmp	w0, #0x0
 440:	ldr	w1, [sp, #168]
 444:	cset	w0, eq  // eq = none
 448:	orr	w1, w1, w0
 44c:	tbz	w1, #0, 174 <__divtc3+0x174>
 450:	adrp	x0, 0 <__divtc3>
 454:	add	x0, x0, #0x0
 458:	ldr	q2, [x0]
 45c:	tbz	x22, #63, 46c <__divtc3+0x46c>
 460:	adrp	x0, 0 <__divtc3>
 464:	add	x0, x0, #0x0
 468:	ldr	q2, [x0]
 46c:	stp	x21, x25, [sp, #96]
 470:	mov	v0.16b, v2.16b
 474:	ldr	q1, [sp, #96]
 478:	str	q2, [sp, #128]
 47c:	bl	0 <__multf3>
 480:	stp	x24, x26, [sp, #112]
 484:	ldr	q2, [sp, #128]
 488:	ldr	q1, [sp, #112]
 48c:	str	q0, [sp, #96]
 490:	mov	v0.16b, v2.16b
 494:	bl	0 <__multf3>
 498:	str	q0, [sp, #112]
 49c:	b	34c <__divtc3+0x34c>
 4a0:	adrp	x0, 0 <__divtc3>
 4a4:	add	x0, x0, #0x0
 4a8:	and	x1, x26, #0x7fffffffffffffff
 4ac:	stp	x24, x1, [sp, #128]
 4b0:	ldr	q1, [x0]
 4b4:	ldr	q0, [sp, #128]
 4b8:	str	x1, [sp, #168]
 4bc:	bl	0 <__unordtf2>
 4c0:	cbnz	w0, 4e8 <__divtc3+0x4e8>
 4c4:	adrp	x0, 0 <__divtc3>
 4c8:	add	x0, x0, #0x0
 4cc:	ldr	x1, [sp, #168]
 4d0:	stp	x24, x1, [sp, #128]
 4d4:	ldr	q1, [x0]
 4d8:	ldr	q0, [sp, #128]
 4dc:	bl	0 <__letf2>
 4e0:	cmp	w0, #0x0
 4e4:	b.gt	1bc <__divtc3+0x1bc>
 4e8:	adrp	x0, 0 <__divtc3>
 4ec:	add	x0, x0, #0x0
 4f0:	stp	x19, x27, [sp, #128]
 4f4:	mov	w1, #0x1                   	// #1
 4f8:	ldr	q1, [x0]
 4fc:	ldr	q0, [sp, #128]
 500:	str	w1, [sp, #168]
 504:	bl	0 <__unordtf2>
 508:	ldr	w1, [sp, #168]
 50c:	cbz	w0, 6a4 <__divtc3+0x6a4>
 510:	adrp	x0, 0 <__divtc3>
 514:	add	x0, x0, #0x0
 518:	stp	x20, x28, [sp, #128]
 51c:	and	w19, w1, #0xff
 520:	mov	w27, #0x1                   	// #1
 524:	ldr	q1, [x0]
 528:	ldr	q0, [sp, #128]
 52c:	bl	0 <__unordtf2>
 530:	cbz	w0, 680 <__divtc3+0x680>
 534:	and	w27, w27, #0xff
 538:	cmp	w19, #0x0
 53c:	ccmp	w27, #0x0, #0x4, ne  // ne = any
 540:	b.ne	34c <__divtc3+0x34c>  // b.any
 544:	adrp	x0, 0 <__divtc3>
 548:	add	x0, x0, #0x0
 54c:	ldr	x20, [sp, #144]
 550:	stp	x21, x20, [sp, #128]
 554:	ldr	q1, [x0]
 558:	ldr	q0, [sp, #128]
 55c:	bl	0 <__unordtf2>
 560:	cbnz	w0, 34c <__divtc3+0x34c>
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	stp	x21, x20, [sp, #128]
 570:	ldr	q1, [x0]
 574:	ldr	q0, [sp, #128]
 578:	bl	0 <__gttf2>
 57c:	cmp	w0, #0x0
 580:	b.gt	34c <__divtc3+0x34c>
 584:	adrp	x0, 0 <__divtc3>
 588:	add	x0, x0, #0x0
 58c:	and	x20, x26, #0x7fffffffffffffff
 590:	stp	x24, x20, [sp, #128]
 594:	ldr	q1, [x0]
 598:	ldr	q0, [sp, #128]
 59c:	bl	0 <__unordtf2>
 5a0:	cbnz	w0, 34c <__divtc3+0x34c>
 5a4:	adrp	x0, 0 <__divtc3>
 5a8:	add	x0, x0, #0x0
 5ac:	stp	x24, x20, [sp, #128]
 5b0:	ldr	q1, [x0]
 5b4:	ldr	q0, [sp, #128]
 5b8:	bl	0 <__gttf2>
 5bc:	cmp	w0, #0x0
 5c0:	b.gt	34c <__divtc3+0x34c>
 5c4:	eor	w0, w19, #0x1
 5c8:	bl	0 <__floatsitf>
 5cc:	str	q0, [sp, #96]
 5d0:	eor	w0, w27, #0x1
 5d4:	ldp	x20, x1, [sp, #96]
 5d8:	bfxil	x22, x1, #0, #63
 5dc:	bl	0 <__floatsitf>
 5e0:	str	q0, [sp, #128]
 5e4:	ldp	x19, x0, [sp, #128]
 5e8:	stp	x20, x22, [sp, #96]
 5ec:	stp	x21, x25, [sp, #112]
 5f0:	ldr	q1, [sp, #96]
 5f4:	ldr	q0, [sp, #112]
 5f8:	bfxil	x23, x0, #0, #63
 5fc:	bl	0 <__multf3>
 600:	stp	x19, x23, [sp, #96]
 604:	stp	x24, x26, [sp, #112]
 608:	ldr	q1, [sp, #96]
 60c:	str	q0, [sp, #96]
 610:	ldr	q0, [sp, #112]
 614:	bl	0 <__multf3>
 618:	mov	v1.16b, v0.16b
 61c:	ldr	q2, [sp, #96]
 620:	mov	v0.16b, v2.16b
 624:	bl	0 <__addtf3>
 628:	movi	v1.2d, #0x0
 62c:	bl	0 <__multf3>
 630:	stp	x20, x22, [sp, #96]
 634:	stp	x24, x26, [sp, #112]
 638:	ldr	q1, [sp, #96]
 63c:	str	q0, [sp, #96]
 640:	ldr	q0, [sp, #112]
 644:	bl	0 <__multf3>
 648:	stp	x19, x23, [sp, #112]
 64c:	stp	x21, x25, [sp, #128]
 650:	ldr	q1, [sp, #112]
 654:	str	q0, [sp, #112]
 658:	ldr	q0, [sp, #128]
 65c:	bl	0 <__multf3>
 660:	mov	v1.16b, v0.16b
 664:	ldr	q2, [sp, #112]
 668:	mov	v0.16b, v2.16b
 66c:	bl	0 <__subtf3>
 670:	movi	v1.2d, #0x0
 674:	bl	0 <__multf3>
 678:	str	q0, [sp, #112]
 67c:	b	34c <__divtc3+0x34c>
 680:	adrp	x0, 0 <__divtc3>
 684:	add	x0, x0, #0x0
 688:	stp	x20, x28, [sp, #128]
 68c:	ldr	q1, [x0]
 690:	ldr	q0, [sp, #128]
 694:	bl	0 <__letf2>
 698:	cmp	w0, #0x0
 69c:	cset	w27, le
 6a0:	b	534 <__divtc3+0x534>
 6a4:	adrp	x0, 0 <__divtc3>
 6a8:	add	x0, x0, #0x0
 6ac:	stp	x19, x27, [sp, #128]
 6b0:	ldr	q1, [x0]
 6b4:	ldr	q0, [sp, #128]
 6b8:	bl	0 <__letf2>
 6bc:	cmp	w0, #0x0
 6c0:	cset	w1, le
 6c4:	b	510 <__divtc3+0x510>
 6c8:	adrp	x0, 0 <__divtc3>
 6cc:	add	x0, x0, #0x0
 6d0:	stp	x24, x27, [sp, #96]
 6d4:	ldr	q1, [x0]
 6d8:	ldr	q0, [sp, #96]
 6dc:	bl	0 <__letf2>
 6e0:	cmp	w0, #0x0
 6e4:	cset	w28, le
 6e8:	b	294 <__divtc3+0x294>
 6ec:	adrp	x0, 0 <__divtc3>
 6f0:	add	x0, x0, #0x0
 6f4:	stp	x21, x28, [sp, #96]
 6f8:	ldr	q1, [x0]
 6fc:	ldr	q0, [sp, #96]
 700:	bl	0 <__letf2>
 704:	cmp	w0, #0x0
 708:	cset	w27, le
 70c:	b	258 <__divtc3+0x258>

_bswapsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

_bswapdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

_clrsbsi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbdi2>:
   0:	eor	x1, x0, x0, asr #63
   4:	mov	w2, #0x3f                  	// #63
   8:	clz	x0, x1
   c:	cmp	x1, #0x0
  10:	sub	w0, w0, #0x1
  14:	csel	w0, w0, w2, ne  // ne = any
  18:	ret

_clrsbdi2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clrsbti2>:
   0:	cbnz	x1, 14 <__clrsbti2+0x14>
   4:	mov	x1, x0
   8:	mov	w0, #0x7f                  	// #127
   c:	cbnz	x1, 40 <__clrsbti2+0x40>
  10:	ret
  14:	cmn	x1, #0x1
  18:	b.eq	34 <__clrsbti2+0x34>  // b.none
  1c:	tbnz	x1, #63, 48 <__clrsbti2+0x48>
  20:	mov	w0, #0x0                   	// #0
  24:	clz	x1, x1
  28:	sub	w0, w0, #0x1
  2c:	add	w0, w0, w1
  30:	ret
  34:	mvn	x1, x0
  38:	mov	w0, #0x7f                  	// #127
  3c:	cbz	x1, 10 <__clrsbti2+0x10>
  40:	mov	w0, #0x40                  	// #64
  44:	b	24 <__clrsbti2+0x24>
  48:	mvn	x1, x1
  4c:	b	20 <__clrsbti2+0x20>

_fixunssfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	movi	v1.2s, #0x5f, lsl #24
   4:	fcmpe	s0, s1
   8:	b.ge	14 <__fixunssfdi+0x14>  // b.tcont
   c:	fcvtzs	x0, s0
  10:	ret
  14:	fsub	s0, s0, s1
  18:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  1c:	fcvtzs	x0, s0
  20:	add	x0, x0, x1
  24:	ret

_fixunsdfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	mov	x0, #0x43e0000000000000    	// #4890909195324358656
   4:	fmov	d1, x0
   8:	fcmpe	d0, d1
   c:	b.ge	18 <__fixunsdfdi+0x18>  // b.tcont
  10:	fcvtzs	x0, d0
  14:	ret
  18:	fsub	d0, d0, d1
  1c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  20:	fcvtzs	x0, d0
  24:	add	x0, x0, x1
  28:	ret

_fixunsxfsi.o:     file format elf64-littleaarch64


_fixsfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fcmpe	s0, #0.0
   4:	b.mi	c <__fixsfti+0xc>  // b.first
   8:	b	0 <__fixunssfti>
   c:	fneg	s0, s0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunssfti>
  1c:	negs	x0, x0
  20:	ngc	x1, x1
  24:	ldp	x29, x30, [sp], #16
  28:	ret

_fixdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fcmpe	d0, #0.0
   4:	b.mi	c <__fixdfti+0xc>  // b.first
   8:	b	0 <__fixunsdfti>
   c:	fneg	d0, d0
  10:	stp	x29, x30, [sp, #-16]!
  14:	mov	x29, sp
  18:	bl	0 <__fixunsdfti>
  1c:	negs	x0, x0
  20:	ngc	x1, x1
  24:	ldp	x29, x30, [sp], #16
  28:	ret

_fixxfdi.o:     file format elf64-littleaarch64


_fixunssfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fcvt	d0, s0
   4:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   8:	fmov	d1, x0
   c:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
  10:	fmov	d2, x0
  14:	fmul	d1, d0, d1
  18:	fcvtzu	x1, d1
  1c:	ucvtf	d1, x1
  20:	fmsub	d0, d1, d2, d0
  24:	fcvtzu	x0, d0
  28:	ret

_fixunsdfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	mov	x0, #0x3bf0000000000000    	// #4318952042648305664
   4:	fmov	d1, x0
   8:	mov	x0, #0x43f0000000000000    	// #4895412794951729152
   c:	fmov	d2, x0
  10:	fmul	d1, d0, d1
  14:	fcvtzu	x1, d1
  18:	ucvtf	d1, x1
  1c:	fmsub	d0, d1, d2, d0
  20:	fcvtzu	x0, d0
  24:	ret

_fixunsxfdi.o:     file format elf64-littleaarch64


_floatdisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x2, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x3, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x2
  28:	b.hi	70 <__floattisf+0x70>  // b.pmore
  2c:	b.eq	68 <__floattisf+0x68>  // b.none
  30:	bl	0 <__floatditf>
  34:	adrp	x0, 0 <__floattisf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfsf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	cmn	x3, #0x2
  6c:	b.ls	30 <__floattisf+0x30>  // b.plast
  70:	and	x1, x19, #0xffffffffffff8000
  74:	tst	x19, #0x7fff
  78:	orr	x1, x1, #0x8000
  7c:	csel	x19, x1, x19, ne  // ne = any
  80:	b	30 <__floattisf+0x30>

_floatdidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x2, #0x3ffffffffffff       	// #1125899906842623
   8:	mov	x29, sp
   c:	str	x19, [sp, #16]
  10:	mov	x19, x0
  14:	subs	x3, x19, #0x1
  18:	mov	x0, x1
  1c:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  20:	adc	x1, x0, x1
  24:	cmp	x1, x2
  28:	b.hi	70 <__floattidf+0x70>  // b.pmore
  2c:	b.eq	68 <__floattidf+0x68>  // b.none
  30:	bl	0 <__floatditf>
  34:	adrp	x0, 0 <__floattidf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfdf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	cmn	x3, #0x2
  6c:	b.ls	30 <__floattidf+0x30>  // b.plast
  70:	and	x1, x19, #0xffffffffffff8000
  74:	tst	x19, #0x7fff
  78:	orr	x1, x1, #0x8000
  7c:	csel	x19, x1, x19, ne  // ne = any
  80:	b	30 <__floattidf+0x30>

_floatdixf.o:     file format elf64-littleaarch64


_floatundisf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntisf+0x30>  // b.plast
  20:	and	x1, x19, #0xffffffffffff8000
  24:	tst	x19, #0x7fff
  28:	orr	x1, x1, #0x8000
  2c:	csel	x19, x1, x19, ne  // ne = any
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntisf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfsf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundidf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x0, x1
  14:	mov	x1, #0x1ffffffffffff       	// #562949953421311
  18:	cmp	x0, x1
  1c:	b.ls	30 <__floatuntidf+0x30>  // b.plast
  20:	and	x1, x19, #0xffffffffffff8000
  24:	tst	x19, #0x7fff
  28:	orr	x1, x1, #0x8000
  2c:	csel	x19, x1, x19, ne  // ne = any
  30:	bl	0 <__floatunditf>
  34:	adrp	x0, 0 <__floatuntidf>
  38:	add	x0, x0, #0x0
  3c:	ldr	q1, [x0]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #32]
  48:	mov	x0, x19
  4c:	bl	0 <__floatunditf>
  50:	ldr	q1, [sp, #32]
  54:	bl	0 <__addtf3>
  58:	bl	0 <__trunctfdf2>
  5c:	ldr	x19, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret

_floatundixf.o:     file format elf64-littleaarch64


_eprintf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x4, x1
   8:	mov	x1, x0
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	adrp	x19, 0 <stderr>
  18:	mov	w5, w2
  1c:	mov	x2, x4
  20:	ldr	x19, [x19]
  24:	mov	x4, x3
  28:	mov	w3, w5
  2c:	ldr	x0, [x19]
  30:	bl	0 <fprintf>
  34:	ldr	x0, [x19]
  38:	bl	0 <fflush>
  3c:	bl	0 <abort>

__gcc_bcmp.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_bcmp>:
   0:	cbz	x2, 30 <__gcc_bcmp+0x30>
   4:	mov	x3, #0x0                   	// #0
   8:	b	14 <__gcc_bcmp+0x14>
   c:	cmp	x3, x2
  10:	b.eq	30 <__gcc_bcmp+0x30>  // b.none
  14:	ldrb	w4, [x0, x3]
  18:	ldrb	w5, [x1, x3]
  1c:	add	x3, x3, #0x1
  20:	cmp	w4, w5
  24:	b.eq	c <__gcc_bcmp+0xc>  // b.none
  28:	sub	w0, w4, w5
  2c:	ret
  30:	mov	w0, #0x0                   	// #0
  34:	ret

_divdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	mov	x6, #0x0                   	// #0
   4:	tbnz	x1, #63, e4 <__divti3+0xe4>
   8:	tbz	x3, #63, 18 <__divti3+0x18>
   c:	negs	x2, x2
  10:	mvn	x6, x6
  14:	ngc	x3, x3
  18:	mov	x4, x2
  1c:	mov	x7, x0
  20:	mov	x5, x1
  24:	cbnz	x3, d0 <__divti3+0xd0>
  28:	cmp	x2, x1
  2c:	b.ls	1f4 <__divti3+0x1f4>  // b.plast
  30:	clz	x0, x2
  34:	cbz	x0, 50 <__divti3+0x50>
  38:	neg	w1, w0
  3c:	lsl	x3, x5, x0
  40:	lsl	x4, x2, x0
  44:	lsr	x5, x7, x1
  48:	orr	x5, x5, x3
  4c:	lsl	x7, x7, x0
  50:	lsr	x2, x4, #32
  54:	and	x8, x4, #0xffffffff
  58:	udiv	x3, x5, x2
  5c:	mov	x0, x3
  60:	msub	x3, x3, x2, x5
  64:	mul	x5, x8, x0
  68:	extr	x1, x3, x7, #32
  6c:	cmp	x5, x1
  70:	b.ls	84 <__divti3+0x84>  // b.plast
  74:	adds	x1, x4, x1
  78:	ccmp	x5, x1, #0x0, cc  // cc = lo, ul, last
  7c:	b.hi	368 <__divti3+0x368>  // b.pmore
  80:	sub	x0, x0, #0x1
  84:	sub	x1, x1, x5
  88:	udiv	x5, x1, x2
  8c:	msub	x1, x5, x2, x1
  90:	mov	x2, x7
  94:	mul	x8, x8, x5
  98:	mov	x3, x5
  9c:	bfi	x2, x1, #32, #32
  a0:	cmp	x8, x2
  a4:	b.ls	b8 <__divti3+0xb8>  // b.plast
  a8:	adds	x2, x4, x2
  ac:	ccmp	x8, x2, #0x0, cc  // cc = lo, ul, last
  b0:	cinc	x3, x5, ls  // ls = plast
  b4:	sub	x3, x3, #0x2
  b8:	orr	x0, x3, x0, lsl #32
  bc:	mov	x1, #0x0                   	// #0
  c0:	cbz	x6, cc <__divti3+0xcc>
  c4:	negs	x0, x0
  c8:	ngc	x1, x1
  cc:	ret
  d0:	cmp	x3, x1
  d4:	b.ls	f4 <__divti3+0xf4>  // b.plast
  d8:	mov	x1, #0x0                   	// #0
  dc:	mov	x0, #0x0                   	// #0
  e0:	b	c0 <__divti3+0xc0>
  e4:	negs	x0, x0
  e8:	mov	x6, #0xffffffffffffffff    	// #-1
  ec:	ngc	x1, x1
  f0:	b	8 <__divti3+0x8>
  f4:	clz	x8, x3
  f8:	cbz	x8, 310 <__divti3+0x310>
  fc:	mov	x4, #0x40                  	// #64
 100:	sub	x4, x4, x8
 104:	lsl	x3, x3, x8
 108:	lsr	x5, x2, x4
 10c:	orr	x3, x5, x3
 110:	lsr	x7, x1, x4
 114:	and	x9, x3, #0xffffffff
 118:	lsr	x11, x3, #32
 11c:	lsl	x1, x1, x8
 120:	lsr	x4, x0, x4
 124:	orr	x4, x4, x1
 128:	lsl	x2, x2, x8
 12c:	udiv	x5, x7, x11
 130:	mov	x1, x5
 134:	msub	x5, x5, x11, x7
 138:	mul	x7, x9, x1
 13c:	extr	x5, x5, x4, #32
 140:	cmp	x7, x5
 144:	b.ls	158 <__divti3+0x158>  // b.plast
 148:	adds	x5, x3, x5
 14c:	ccmp	x7, x5, #0x0, cc  // cc = lo, ul, last
 150:	b.hi	350 <__divti3+0x350>  // b.pmore
 154:	sub	x1, x1, #0x1
 158:	sub	x7, x5, x7
 15c:	udiv	x10, x7, x11
 160:	msub	x7, x10, x11, x7
 164:	mov	x5, x10
 168:	mul	x9, x9, x10
 16c:	bfi	x4, x7, #32, #32
 170:	cmp	x9, x4
 174:	b.ls	188 <__divti3+0x188>  // b.plast
 178:	adds	x4, x3, x4
 17c:	ccmp	x9, x4, #0x0, cc  // cc = lo, ul, last
 180:	b.hi	338 <__divti3+0x338>  // b.pmore
 184:	sub	x5, x10, #0x1
 188:	orr	x1, x5, x1, lsl #32
 18c:	and	x10, x2, #0xffffffff
 190:	mov	w5, w5
 194:	lsr	x2, x2, #32
 198:	lsr	x7, x1, #32
 19c:	sub	x4, x4, x9
 1a0:	mov	x11, #0x100000000           	// #4294967296
 1a4:	mul	x12, x5, x10
 1a8:	mul	x9, x7, x10
 1ac:	madd	x5, x5, x2, x9
 1b0:	mul	x2, x7, x2
 1b4:	add	x3, x5, x12, lsr #32
 1b8:	add	x5, x2, x11
 1bc:	cmp	x9, x3
 1c0:	csel	x2, x5, x2, hi  // hi = pmore
 1c4:	add	x2, x2, x3, lsr #32
 1c8:	cmp	x4, x2
 1cc:	b.cc	1e8 <__divti3+0x1e8>  // b.lo, b.ul, b.last
 1d0:	and	x12, x12, #0xffffffff
 1d4:	lsl	x0, x0, x8
 1d8:	add	x3, x12, x3, lsl #32
 1dc:	cmp	x0, x3
 1e0:	ccmp	x4, x2, #0x0, cc  // cc = lo, ul, last
 1e4:	b.ne	320 <__divti3+0x320>  // b.any
 1e8:	sub	x0, x1, #0x1
 1ec:	mov	x1, #0x0                   	// #0
 1f0:	b	c0 <__divti3+0xc0>
 1f4:	cbnz	x2, 200 <__divti3+0x200>
 1f8:	mov	x2, #0x1                   	// #1
 1fc:	udiv	x4, x2, x3
 200:	clz	x8, x4
 204:	cbnz	x8, 280 <__divti3+0x280>
 208:	lsr	x11, x4, #32
 20c:	and	x10, x4, #0xffffffff
 210:	sub	x3, x1, x4
 214:	mov	x1, #0x1                   	// #1
 218:	udiv	x5, x3, x11
 21c:	mov	x0, x5
 220:	msub	x5, x5, x11, x3
 224:	mul	x2, x0, x10
 228:	extr	x3, x5, x7, #32
 22c:	cmp	x2, x3
 230:	b.ls	244 <__divti3+0x244>  // b.plast
 234:	adds	x3, x4, x3
 238:	ccmp	x2, x3, #0x0, cc  // cc = lo, ul, last
 23c:	b.hi	35c <__divti3+0x35c>  // b.pmore
 240:	sub	x0, x0, #0x1
 244:	sub	x3, x3, x2
 248:	mov	x2, x7
 24c:	udiv	x9, x3, x11
 250:	msub	x3, x9, x11, x3
 254:	mov	x8, x9
 258:	mul	x5, x9, x10
 25c:	bfi	x2, x3, #32, #32
 260:	cmp	x5, x2
 264:	b.ls	278 <__divti3+0x278>  // b.plast
 268:	adds	x2, x4, x2
 26c:	ccmp	x5, x2, #0x0, cc  // cc = lo, ul, last
 270:	cinc	x8, x9, ls  // ls = plast
 274:	sub	x8, x8, #0x2
 278:	orr	x0, x8, x0, lsl #32
 27c:	b	c0 <__divti3+0xc0>
 280:	lsl	x4, x4, x8
 284:	mov	x2, #0x40                  	// #64
 288:	sub	x2, x2, x8
 28c:	lsr	x11, x4, #32
 290:	lsl	x3, x1, x8
 294:	and	x10, x4, #0xffffffff
 298:	lsr	x1, x1, x2
 29c:	lsr	x5, x0, x2
 2a0:	orr	x5, x5, x3
 2a4:	udiv	x2, x1, x11
 2a8:	lsl	x7, x0, x8
 2ac:	msub	x1, x2, x11, x1
 2b0:	mov	x0, x2
 2b4:	mul	x2, x10, x2
 2b8:	extr	x1, x1, x5, #32
 2bc:	cmp	x2, x1
 2c0:	b.ls	2d4 <__divti3+0x2d4>  // b.plast
 2c4:	adds	x1, x4, x1
 2c8:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 2cc:	b.hi	32c <__divti3+0x32c>  // b.pmore
 2d0:	sub	x0, x0, #0x1
 2d4:	sub	x1, x1, x2
 2d8:	udiv	x2, x1, x11
 2dc:	msub	x1, x2, x11, x1
 2e0:	mov	x8, x2
 2e4:	mul	x3, x10, x2
 2e8:	bfi	x5, x1, #32, #32
 2ec:	cmp	x3, x5
 2f0:	b.ls	304 <__divti3+0x304>  // b.plast
 2f4:	adds	x5, x4, x5
 2f8:	ccmp	x3, x5, #0x0, cc  // cc = lo, ul, last
 2fc:	b.hi	344 <__divti3+0x344>  // b.pmore
 300:	sub	x8, x2, #0x1
 304:	sub	x3, x5, x3
 308:	orr	x1, x8, x0, lsl #32
 30c:	b	218 <__divti3+0x218>
 310:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 314:	mov	x1, #0x0                   	// #0
 318:	cset	x0, ls  // ls = plast
 31c:	b	c0 <__divti3+0xc0>
 320:	mov	x0, x1
 324:	mov	x1, #0x0                   	// #0
 328:	b	c0 <__divti3+0xc0>
 32c:	sub	x0, x0, #0x2
 330:	add	x1, x1, x4
 334:	b	2d4 <__divti3+0x2d4>
 338:	sub	x5, x10, #0x2
 33c:	add	x4, x4, x3
 340:	b	188 <__divti3+0x188>
 344:	sub	x8, x2, #0x2
 348:	add	x5, x5, x4
 34c:	b	304 <__divti3+0x304>
 350:	sub	x1, x1, #0x2
 354:	add	x5, x5, x3
 358:	b	158 <__divti3+0x158>
 35c:	sub	x0, x0, #0x2
 360:	add	x3, x3, x4
 364:	b	244 <__divti3+0x244>
 368:	sub	x0, x0, #0x2
 36c:	add	x1, x1, x4
 370:	b	84 <__divti3+0x84>

_moddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	mov	x6, #0x0                   	// #0
   4:	tbnz	x1, #63, e8 <__modti3+0xe8>
   8:	tbz	x3, #63, 14 <__modti3+0x14>
   c:	negs	x2, x2
  10:	ngc	x3, x3
  14:	mov	x4, x2
  18:	mov	x10, x0
  1c:	mov	x5, x1
  20:	cbnz	x3, bc <__modti3+0xbc>
  24:	cmp	x2, x1
  28:	b.ls	f8 <__modti3+0xf8>  // b.plast
  2c:	clz	x0, x2
  30:	mov	x7, x0
  34:	cbz	x0, 50 <__modti3+0x50>
  38:	neg	w1, w0
  3c:	lsl	x5, x5, x0
  40:	lsl	x4, x2, x0
  44:	lsr	x1, x10, x1
  48:	orr	x5, x1, x5
  4c:	lsl	x10, x10, x0
  50:	lsr	x2, x4, #32
  54:	and	x0, x4, #0xffffffff
  58:	udiv	x1, x5, x2
  5c:	msub	x5, x1, x2, x5
  60:	mul	x1, x0, x1
  64:	extr	x5, x5, x10, #32
  68:	cmp	x1, x5
  6c:	b.ls	80 <__modti3+0x80>  // b.plast
  70:	adds	x5, x4, x5
  74:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
  78:	add	x3, x5, x4
  7c:	csel	x5, x3, x5, hi  // hi = pmore
  80:	sub	x5, x5, x1
  84:	udiv	x1, x5, x2
  88:	msub	x5, x1, x2, x5
  8c:	mul	x0, x0, x1
  90:	mov	x1, x10
  94:	bfi	x1, x5, #32, #32
  98:	cmp	x0, x1
  9c:	b.hi	200 <__modti3+0x200>  // b.pmore
  a0:	sub	x0, x1, x0
  a4:	lsr	x0, x0, x7
  a8:	mov	x1, #0x0                   	// #0
  ac:	cbz	x6, b8 <__modti3+0xb8>
  b0:	negs	x0, x0
  b4:	ngc	x1, x1
  b8:	ret
  bc:	cmp	x3, x1
  c0:	mov	x4, x0
  c4:	b.hi	ac <__modti3+0xac>  // b.pmore
  c8:	clz	x8, x3
  cc:	cbnz	x8, 214 <__modti3+0x214>
  d0:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
  d4:	b.hi	e0 <__modti3+0xe0>  // b.pmore
  d8:	subs	x4, x0, x2
  dc:	sbc	x1, x1, x3
  e0:	mov	x0, x4
  e4:	b	ac <__modti3+0xac>
  e8:	negs	x0, x0
  ec:	mov	x6, #0xffffffffffffffff    	// #-1
  f0:	ngc	x1, x1
  f4:	b	8 <__modti3+0x8>
  f8:	cbnz	x2, 104 <__modti3+0x104>
  fc:	mov	x4, #0x1                   	// #1
 100:	udiv	x4, x4, x3
 104:	clz	x11, x4
 108:	mov	x7, x11
 10c:	cbnz	x11, 17c <__modti3+0x17c>
 110:	sub	x5, x1, x4
 114:	lsr	x12, x4, #32
 118:	and	x13, x4, #0xffffffff
 11c:	udiv	x8, x5, x12
 120:	msub	x5, x8, x12, x5
 124:	mul	x8, x8, x13
 128:	extr	x3, x5, x10, #32
 12c:	cmp	x8, x3
 130:	b.ls	144 <__modti3+0x144>  // b.plast
 134:	adds	x5, x4, x3
 138:	ccmp	x8, x5, #0x0, cc  // cc = lo, ul, last
 13c:	add	x3, x5, x4
 140:	csel	x3, x3, x5, hi  // hi = pmore
 144:	sub	x3, x3, x8
 148:	mov	x1, x10
 14c:	udiv	x2, x3, x12
 150:	msub	x3, x2, x12, x3
 154:	mul	x2, x2, x13
 158:	bfi	x1, x3, #32, #32
 15c:	cmp	x2, x1
 160:	b.ls	174 <__modti3+0x174>  // b.plast
 164:	adds	x1, x4, x1
 168:	add	x4, x1, x4
 16c:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 170:	csel	x1, x4, x1, hi  // hi = pmore
 174:	sub	x0, x1, x2
 178:	b	a4 <__modti3+0xa4>
 17c:	lsl	x4, x4, x11
 180:	mov	x5, #0x40                  	// #64
 184:	sub	x5, x5, x11
 188:	lsr	x12, x4, #32
 18c:	and	x13, x4, #0xffffffff
 190:	lsl	x2, x1, x11
 194:	lsr	x9, x1, x5
 198:	lsr	x5, x0, x5
 19c:	orr	x5, x5, x2
 1a0:	udiv	x8, x9, x12
 1a4:	lsl	x10, x0, x11
 1a8:	msub	x9, x8, x12, x9
 1ac:	mul	x0, x13, x8
 1b0:	extr	x8, x9, x5, #32
 1b4:	cmp	x0, x8
 1b8:	b.ls	1cc <__modti3+0x1cc>  // b.plast
 1bc:	adds	x8, x4, x8
 1c0:	ccmp	x0, x8, #0x0, cc  // cc = lo, ul, last
 1c4:	add	x1, x8, x4
 1c8:	csel	x8, x1, x8, hi  // hi = pmore
 1cc:	sub	x8, x8, x0
 1d0:	udiv	x1, x8, x12
 1d4:	msub	x8, x1, x12, x8
 1d8:	mul	x1, x13, x1
 1dc:	bfi	x5, x8, #32, #32
 1e0:	cmp	x1, x5
 1e4:	b.ls	1f8 <__modti3+0x1f8>  // b.plast
 1e8:	adds	x5, x4, x5
 1ec:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
 1f0:	add	x0, x5, x4
 1f4:	csel	x5, x0, x5, hi  // hi = pmore
 1f8:	sub	x5, x5, x1
 1fc:	b	11c <__modti3+0x11c>
 200:	adds	x1, x4, x1
 204:	add	x4, x1, x4
 208:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 20c:	csel	x1, x4, x1, hi  // hi = pmore
 210:	b	a0 <__modti3+0xa0>
 214:	mov	x10, #0x40                  	// #64
 218:	sub	x10, x10, x8
 21c:	lsl	x3, x3, x8
 220:	lsr	x9, x2, x10
 224:	orr	x9, x9, x3
 228:	lsr	x7, x1, x10
 22c:	and	x11, x9, #0xffffffff
 230:	lsr	x12, x9, #32
 234:	lsl	x4, x2, x8
 238:	lsr	x3, x0, x10
 23c:	lsl	x1, x1, x8
 240:	orr	x3, x3, x1
 244:	udiv	x2, x7, x12
 248:	lsl	x13, x0, x8
 24c:	mov	x5, x2
 250:	msub	x2, x2, x12, x7
 254:	mul	x0, x11, x5
 258:	extr	x2, x2, x3, #32
 25c:	cmp	x0, x2
 260:	b.ls	274 <__modti3+0x274>  // b.plast
 264:	adds	x2, x9, x2
 268:	ccmp	x0, x2, #0x0, cc  // cc = lo, ul, last
 26c:	b.hi	338 <__modti3+0x338>  // b.pmore
 270:	sub	x5, x5, #0x1
 274:	sub	x2, x2, x0
 278:	udiv	x0, x2, x12
 27c:	msub	x2, x0, x12, x2
 280:	mov	x7, x0
 284:	mul	x1, x11, x0
 288:	bfi	x3, x2, #32, #32
 28c:	mov	x2, x3
 290:	cmp	x1, x3
 294:	b.ls	2a8 <__modti3+0x2a8>  // b.plast
 298:	adds	x2, x9, x3
 29c:	ccmp	x1, x2, #0x0, cc  // cc = lo, ul, last
 2a0:	b.hi	32c <__modti3+0x32c>  // b.pmore
 2a4:	sub	x7, x0, #0x1
 2a8:	orr	x5, x7, x5, lsl #32
 2ac:	and	x12, x4, #0xffffffff
 2b0:	mov	w3, w7
 2b4:	lsr	x0, x4, #32
 2b8:	lsr	x5, x5, #32
 2bc:	sub	x1, x2, x1
 2c0:	mov	x14, #0x100000000           	// #4294967296
 2c4:	mul	x7, x3, x12
 2c8:	mul	x12, x5, x12
 2cc:	madd	x3, x3, x0, x12
 2d0:	and	x11, x7, #0xffffffff
 2d4:	mul	x2, x5, x0
 2d8:	add	x5, x3, x7, lsr #32
 2dc:	add	x0, x2, x14
 2e0:	cmp	x12, x5
 2e4:	csel	x2, x0, x2, hi  // hi = pmore
 2e8:	add	x7, x11, x5, lsl #32
 2ec:	add	x2, x2, x5, lsr #32
 2f0:	cmp	x1, x2
 2f4:	b.cc	300 <__modti3+0x300>  // b.lo, b.ul, b.last
 2f8:	ccmp	x13, x7, #0x2, eq  // eq = none
 2fc:	b.cs	30c <__modti3+0x30c>  // b.hs, b.nlast
 300:	subs	x7, x7, x4
 304:	cinc	x3, x9, cc  // cc = lo, ul, last
 308:	sub	x2, x2, x3
 30c:	subs	x0, x13, x7
 310:	cmp	x13, x7
 314:	sbc	x1, x1, x2
 318:	lsr	x0, x0, x8
 31c:	lsl	x10, x1, x10
 320:	orr	x0, x10, x0
 324:	lsr	x1, x1, x8
 328:	b	ac <__modti3+0xac>
 32c:	sub	x7, x0, #0x2
 330:	add	x2, x2, x9
 334:	b	2a8 <__modti3+0x2a8>
 338:	sub	x5, x5, #0x2
 33c:	add	x2, x2, x9
 340:	b	274 <__modti3+0x274>

_divmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodti4>:
   0:	mov	x5, #0x0                   	// #0
   4:	tbz	x1, #63, 14 <__divmodti4+0x14>
   8:	negs	x0, x0
   c:	mov	x5, #0xffffffffffffffff    	// #-1
  10:	ngc	x1, x1
  14:	mov	x6, x5
  18:	tbz	x3, #63, 28 <__divmodti4+0x28>
  1c:	negs	x2, x2
  20:	mvn	x6, x5
  24:	ngc	x3, x3
  28:	mov	x7, x2
  2c:	mov	x11, x0
  30:	mov	x8, x1
  34:	cbnz	x3, 100 <__divmodti4+0x100>
  38:	cmp	x2, x1
  3c:	b.ls	150 <__divmodti4+0x150>  // b.plast
  40:	clz	x0, x2
  44:	mov	x10, x0
  48:	cbz	x0, 64 <__divmodti4+0x64>
  4c:	neg	w1, w0
  50:	lsl	x3, x8, x0
  54:	lsl	x7, x2, x0
  58:	lsr	x8, x11, x1
  5c:	orr	x8, x8, x3
  60:	lsl	x11, x11, x0
  64:	lsr	x9, x7, #32
  68:	and	x2, x7, #0xffffffff
  6c:	udiv	x3, x8, x9
  70:	mov	x0, x3
  74:	msub	x3, x3, x9, x8
  78:	mul	x1, x2, x0
  7c:	extr	x3, x3, x11, #32
  80:	cmp	x1, x3
  84:	b.ls	98 <__divmodti4+0x98>  // b.plast
  88:	adds	x3, x7, x3
  8c:	ccmp	x1, x3, #0x0, cc  // cc = lo, ul, last
  90:	b.hi	3dc <__divmodti4+0x3dc>  // b.pmore
  94:	sub	x0, x0, #0x1
  98:	sub	x3, x3, x1
  9c:	udiv	x1, x3, x9
  a0:	msub	x3, x1, x9, x3
  a4:	mov	x8, x1
  a8:	mul	x2, x2, x1
  ac:	mov	x1, x11
  b0:	bfi	x1, x3, #32, #32
  b4:	cmp	x2, x1
  b8:	b.ls	cc <__divmodti4+0xcc>  // b.plast
  bc:	adds	x1, x7, x1
  c0:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
  c4:	b.hi	3e8 <__divmodti4+0x3e8>  // b.pmore
  c8:	sub	x8, x8, #0x1
  cc:	orr	x0, x8, x0, lsl #32
  d0:	sub	x2, x1, x2
  d4:	mov	x1, #0x0                   	// #0
  d8:	lsr	x2, x2, x10
  dc:	mov	x3, #0x0                   	// #0
  e0:	cbz	x6, ec <__divmodti4+0xec>
  e4:	negs	x0, x0
  e8:	ngc	x1, x1
  ec:	cbz	x5, f8 <__divmodti4+0xf8>
  f0:	negs	x2, x2
  f4:	ngc	x3, x3
  f8:	stp	x2, x3, [x4]
  fc:	ret
 100:	cmp	x3, x1
 104:	mov	x7, x0
 108:	b.ls	120 <__divmodti4+0x120>  // b.plast
 10c:	mov	x2, x0
 110:	mov	x3, x1
 114:	mov	x0, #0x0                   	// #0
 118:	mov	x1, #0x0                   	// #0
 11c:	b	e0 <__divmodti4+0xe0>
 120:	clz	x8, x3
 124:	cbnz	x8, 274 <__divmodti4+0x274>
 128:	ccmp	x2, x11, #0x0, cs  // cs = hs, nlast
 12c:	mov	x0, #0x0                   	// #0
 130:	b.hi	140 <__divmodti4+0x140>  // b.pmore
 134:	subs	x7, x11, x2
 138:	mov	x0, #0x1                   	// #1
 13c:	sbc	x1, x1, x3
 140:	mov	x3, x1
 144:	mov	x2, x7
 148:	mov	x1, #0x0                   	// #0
 14c:	b	e0 <__divmodti4+0xe0>
 150:	cbnz	x2, 15c <__divmodti4+0x15c>
 154:	mov	x7, #0x1                   	// #1
 158:	udiv	x7, x7, x3
 15c:	clz	x3, x7
 160:	mov	x10, x3
 164:	cbnz	x3, 1e4 <__divmodti4+0x1e4>
 168:	lsr	x12, x7, #32
 16c:	and	x2, x7, #0xffffffff
 170:	sub	x9, x1, x7
 174:	mov	x1, #0x1                   	// #1
 178:	udiv	x8, x9, x12
 17c:	mov	x0, x8
 180:	msub	x8, x8, x12, x9
 184:	mul	x3, x0, x2
 188:	extr	x8, x8, x11, #32
 18c:	cmp	x3, x8
 190:	b.ls	1a4 <__divmodti4+0x1a4>  // b.plast
 194:	adds	x8, x7, x8
 198:	ccmp	x3, x8, #0x0, cc  // cc = lo, ul, last
 19c:	b.hi	3d0 <__divmodti4+0x3d0>  // b.pmore
 1a0:	sub	x0, x0, #0x1
 1a4:	sub	x8, x8, x3
 1a8:	udiv	x9, x8, x12
 1ac:	msub	x8, x9, x12, x8
 1b0:	mov	x13, x9
 1b4:	mul	x3, x9, x2
 1b8:	mov	x2, x11
 1bc:	bfi	x2, x8, #32, #32
 1c0:	cmp	x3, x2
 1c4:	b.ls	1d8 <__divmodti4+0x1d8>  // b.plast
 1c8:	adds	x2, x7, x2
 1cc:	ccmp	x3, x2, #0x0, cc  // cc = lo, ul, last
 1d0:	b.hi	3c4 <__divmodti4+0x3c4>  // b.pmore
 1d4:	sub	x13, x9, #0x1
 1d8:	sub	x2, x2, x3
 1dc:	orr	x0, x13, x0, lsl #32
 1e0:	b	d8 <__divmodti4+0xd8>
 1e4:	lsl	x7, x7, x3
 1e8:	mov	x9, #0x40                  	// #64
 1ec:	sub	x9, x9, x3
 1f0:	lsr	x12, x7, #32
 1f4:	lsl	x11, x1, x3
 1f8:	and	x2, x7, #0xffffffff
 1fc:	lsr	x13, x1, x9
 200:	lsr	x8, x0, x9
 204:	orr	x8, x8, x11
 208:	udiv	x9, x13, x12
 20c:	lsl	x11, x0, x3
 210:	mov	x1, x9
 214:	msub	x9, x9, x12, x13
 218:	mul	x3, x2, x1
 21c:	extr	x0, x9, x8, #32
 220:	cmp	x3, x0
 224:	b.ls	238 <__divmodti4+0x238>  // b.plast
 228:	adds	x0, x7, x0
 22c:	ccmp	x3, x0, #0x0, cc  // cc = lo, ul, last
 230:	b.hi	3a0 <__divmodti4+0x3a0>  // b.pmore
 234:	sub	x1, x1, #0x1
 238:	sub	x0, x0, x3
 23c:	udiv	x3, x0, x12
 240:	msub	x0, x3, x12, x0
 244:	mov	x13, x3
 248:	mul	x9, x2, x3
 24c:	bfi	x8, x0, #32, #32
 250:	cmp	x9, x8
 254:	b.ls	268 <__divmodti4+0x268>  // b.plast
 258:	adds	x8, x7, x8
 25c:	ccmp	x9, x8, #0x0, cc  // cc = lo, ul, last
 260:	b.hi	3b8 <__divmodti4+0x3b8>  // b.pmore
 264:	sub	x13, x3, #0x1
 268:	sub	x9, x8, x9
 26c:	orr	x1, x13, x1, lsl #32
 270:	b	178 <__divmodti4+0x178>
 274:	mov	x9, #0x40                  	// #64
 278:	sub	x9, x9, x8
 27c:	lsl	x3, x3, x8
 280:	lsr	x10, x2, x9
 284:	orr	x10, x10, x3
 288:	lsr	x11, x1, x9
 28c:	and	x3, x10, #0xffffffff
 290:	lsr	x14, x10, #32
 294:	lsr	x7, x0, x9
 298:	lsl	x1, x1, x8
 29c:	orr	x1, x7, x1
 2a0:	lsl	x13, x0, x8
 2a4:	udiv	x7, x11, x14
 2a8:	lsl	x2, x2, x8
 2ac:	mov	x0, x7
 2b0:	msub	x7, x7, x14, x11
 2b4:	mul	x11, x3, x0
 2b8:	extr	x7, x7, x1, #32
 2bc:	cmp	x11, x7
 2c0:	b.ls	2d4 <__divmodti4+0x2d4>  // b.plast
 2c4:	adds	x7, x10, x7
 2c8:	ccmp	x11, x7, #0x0, cc  // cc = lo, ul, last
 2cc:	b.hi	394 <__divmodti4+0x394>  // b.pmore
 2d0:	sub	x0, x0, #0x1
 2d4:	sub	x7, x7, x11
 2d8:	udiv	x12, x7, x14
 2dc:	msub	x7, x12, x14, x7
 2e0:	mov	x11, x12
 2e4:	mul	x3, x3, x12
 2e8:	bfi	x1, x7, #32, #32
 2ec:	cmp	x3, x1
 2f0:	b.ls	304 <__divmodti4+0x304>  // b.plast
 2f4:	adds	x1, x10, x1
 2f8:	ccmp	x3, x1, #0x0, cc  // cc = lo, ul, last
 2fc:	b.hi	3ac <__divmodti4+0x3ac>  // b.pmore
 300:	sub	x11, x12, #0x1
 304:	orr	x0, x11, x0, lsl #32
 308:	and	x15, x2, #0xffffffff
 30c:	mov	w11, w11
 310:	lsr	x12, x2, #32
 314:	lsr	x14, x0, #32
 318:	sub	x3, x1, x3
 31c:	mov	x16, #0x100000000           	// #4294967296
 320:	mul	x7, x11, x15
 324:	mul	x15, x14, x15
 328:	madd	x11, x11, x12, x15
 32c:	and	x1, x7, #0xffffffff
 330:	mul	x12, x14, x12
 334:	add	x7, x11, x7, lsr #32
 338:	add	x11, x12, x16
 33c:	cmp	x15, x7
 340:	csel	x12, x11, x12, hi  // hi = pmore
 344:	add	x1, x1, x7, lsl #32
 348:	add	x7, x12, x7, lsr #32
 34c:	cmp	x3, x7
 350:	b.cc	35c <__divmodti4+0x35c>  // b.lo, b.ul, b.last
 354:	ccmp	x13, x1, #0x2, eq  // eq = none
 358:	b.cs	370 <__divmodti4+0x370>  // b.hs, b.nlast
 35c:	cmp	x1, x2
 360:	sub	x0, x0, #0x1
 364:	cinc	x10, x10, cc  // cc = lo, ul, last
 368:	sub	x1, x1, x2
 36c:	sub	x7, x7, x10
 370:	subs	x2, x13, x1
 374:	cmp	x13, x1
 378:	mov	x1, #0x0                   	// #0
 37c:	sbc	x3, x3, x7
 380:	lsr	x2, x2, x8
 384:	lsl	x9, x3, x9
 388:	orr	x2, x9, x2
 38c:	lsr	x3, x3, x8
 390:	b	e0 <__divmodti4+0xe0>
 394:	sub	x0, x0, #0x2
 398:	add	x7, x7, x10
 39c:	b	2d4 <__divmodti4+0x2d4>
 3a0:	sub	x1, x1, #0x2
 3a4:	add	x0, x0, x7
 3a8:	b	238 <__divmodti4+0x238>
 3ac:	sub	x11, x12, #0x2
 3b0:	add	x1, x1, x10
 3b4:	b	304 <__divmodti4+0x304>
 3b8:	sub	x13, x3, #0x2
 3bc:	add	x8, x8, x7
 3c0:	b	268 <__divmodti4+0x268>
 3c4:	sub	x13, x9, #0x2
 3c8:	add	x2, x2, x7
 3cc:	b	1d8 <__divmodti4+0x1d8>
 3d0:	sub	x0, x0, #0x2
 3d4:	add	x8, x8, x7
 3d8:	b	1a4 <__divmodti4+0x1a4>
 3dc:	sub	x0, x0, #0x2
 3e0:	add	x3, x3, x7
 3e4:	b	98 <__divmodti4+0x98>
 3e8:	sub	x8, x8, #0x2
 3ec:	add	x1, x1, x7
 3f0:	b	cc <__divmodti4+0xcc>

_udivdi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, x2
   4:	mov	x5, x0
   8:	mov	x6, x1
   c:	cbnz	x3, a8 <__udivti3+0xa8>
  10:	cmp	x2, x1
  14:	b.ls	1bc <__udivti3+0x1bc>  // b.plast
  18:	clz	x0, x2
  1c:	cbz	x0, 38 <__udivti3+0x38>
  20:	neg	w1, w0
  24:	lsl	x2, x6, x0
  28:	lsl	x4, x4, x0
  2c:	lsr	x6, x5, x1
  30:	orr	x6, x6, x2
  34:	lsl	x5, x5, x0
  38:	lsr	x3, x4, #32
  3c:	and	x7, x4, #0xffffffff
  40:	udiv	x2, x6, x3
  44:	mov	x0, x2
  48:	msub	x2, x2, x3, x6
  4c:	mul	x6, x7, x0
  50:	extr	x1, x2, x5, #32
  54:	cmp	x6, x1
  58:	b.ls	6c <__udivti3+0x6c>  // b.plast
  5c:	adds	x1, x4, x1
  60:	ccmp	x6, x1, #0x0, cc  // cc = lo, ul, last
  64:	b.hi	32c <__udivti3+0x32c>  // b.pmore
  68:	sub	x0, x0, #0x1
  6c:	sub	x1, x1, x6
  70:	udiv	x6, x1, x3
  74:	msub	x1, x6, x3, x1
  78:	mov	x2, x6
  7c:	mul	x7, x7, x6
  80:	bfi	x5, x1, #32, #32
  84:	cmp	x7, x5
  88:	b.ls	9c <__udivti3+0x9c>  // b.plast
  8c:	adds	x5, x4, x5
  90:	ccmp	x7, x5, #0x0, cc  // cc = lo, ul, last
  94:	cinc	x2, x6, ls  // ls = plast
  98:	sub	x2, x2, #0x2
  9c:	orr	x0, x2, x0, lsl #32
  a0:	mov	x1, #0x0                   	// #0
  a4:	ret
  a8:	cmp	x3, x1
  ac:	b.ls	bc <__udivti3+0xbc>  // b.plast
  b0:	mov	x1, #0x0                   	// #0
  b4:	mov	x0, #0x0                   	// #0
  b8:	ret
  bc:	clz	x7, x3
  c0:	cbz	x7, 2d4 <__udivti3+0x2d4>
  c4:	mov	x4, #0x40                  	// #64
  c8:	sub	x4, x4, x7
  cc:	lsl	x3, x3, x7
  d0:	lsr	x5, x2, x4
  d4:	orr	x3, x5, x3
  d8:	lsr	x6, x1, x4
  dc:	and	x8, x3, #0xffffffff
  e0:	lsr	x10, x3, #32
  e4:	lsr	x4, x0, x4
  e8:	lsl	x1, x1, x7
  ec:	orr	x1, x4, x1
  f0:	lsl	x2, x2, x7
  f4:	udiv	x5, x6, x10
  f8:	mov	x4, x5
  fc:	msub	x5, x5, x10, x6
 100:	mul	x6, x8, x4
 104:	extr	x5, x5, x1, #32
 108:	cmp	x6, x5
 10c:	b.ls	120 <__udivti3+0x120>  // b.plast
 110:	adds	x5, x3, x5
 114:	ccmp	x6, x5, #0x0, cc  // cc = lo, ul, last
 118:	b.hi	314 <__udivti3+0x314>  // b.pmore
 11c:	sub	x4, x4, #0x1
 120:	sub	x6, x5, x6
 124:	udiv	x9, x6, x10
 128:	msub	x6, x9, x10, x6
 12c:	mov	x5, x9
 130:	mul	x8, x8, x9
 134:	bfi	x1, x6, #32, #32
 138:	cmp	x8, x1
 13c:	b.ls	150 <__udivti3+0x150>  // b.plast
 140:	adds	x1, x3, x1
 144:	ccmp	x8, x1, #0x0, cc  // cc = lo, ul, last
 148:	b.hi	2fc <__udivti3+0x2fc>  // b.pmore
 14c:	sub	x5, x9, #0x1
 150:	orr	x4, x5, x4, lsl #32
 154:	and	x9, x2, #0xffffffff
 158:	mov	w5, w5
 15c:	lsr	x2, x2, #32
 160:	lsr	x6, x4, #32
 164:	sub	x1, x1, x8
 168:	mov	x10, #0x100000000           	// #4294967296
 16c:	mul	x11, x5, x9
 170:	mul	x8, x6, x9
 174:	madd	x5, x5, x2, x8
 178:	mul	x2, x6, x2
 17c:	add	x3, x5, x11, lsr #32
 180:	add	x5, x2, x10
 184:	cmp	x8, x3
 188:	csel	x2, x5, x2, hi  // hi = pmore
 18c:	add	x2, x2, x3, lsr #32
 190:	cmp	x1, x2
 194:	b.cc	1b0 <__udivti3+0x1b0>  // b.lo, b.ul, b.last
 198:	and	x11, x11, #0xffffffff
 19c:	lsl	x0, x0, x7
 1a0:	add	x3, x11, x3, lsl #32
 1a4:	cmp	x0, x3
 1a8:	ccmp	x1, x2, #0x0, cc  // cc = lo, ul, last
 1ac:	b.ne	2e4 <__udivti3+0x2e4>  // b.any
 1b0:	sub	x0, x4, #0x1
 1b4:	mov	x1, #0x0                   	// #0
 1b8:	ret
 1bc:	cbnz	x2, 1c8 <__udivti3+0x1c8>
 1c0:	mov	x2, #0x1                   	// #1
 1c4:	udiv	x4, x2, x4
 1c8:	clz	x8, x4
 1cc:	cbnz	x8, 244 <__udivti3+0x244>
 1d0:	lsr	x6, x4, #32
 1d4:	and	x7, x4, #0xffffffff
 1d8:	sub	x2, x1, x4
 1dc:	mov	x1, #0x1                   	// #1
 1e0:	udiv	x3, x2, x6
 1e4:	mov	x0, x3
 1e8:	msub	x3, x3, x6, x2
 1ec:	mul	x8, x0, x7
 1f0:	extr	x2, x3, x5, #32
 1f4:	cmp	x8, x2
 1f8:	b.ls	20c <__udivti3+0x20c>  // b.plast
 1fc:	adds	x2, x4, x2
 200:	ccmp	x8, x2, #0x0, cc  // cc = lo, ul, last
 204:	b.hi	320 <__udivti3+0x320>  // b.pmore
 208:	sub	x0, x0, #0x1
 20c:	sub	x2, x2, x8
 210:	udiv	x8, x2, x6
 214:	msub	x2, x8, x6, x2
 218:	mov	x3, x8
 21c:	mul	x7, x8, x7
 220:	bfi	x5, x2, #32, #32
 224:	cmp	x7, x5
 228:	b.ls	23c <__udivti3+0x23c>  // b.plast
 22c:	adds	x4, x4, x5
 230:	ccmp	x7, x4, #0x0, cc  // cc = lo, ul, last
 234:	cinc	x3, x8, ls  // ls = plast
 238:	sub	x3, x3, #0x2
 23c:	orr	x0, x3, x0, lsl #32
 240:	ret
 244:	lsl	x4, x4, x8
 248:	mov	x3, #0x40                  	// #64
 24c:	sub	x3, x3, x8
 250:	lsr	x6, x4, #32
 254:	lsl	x2, x1, x8
 258:	and	x7, x4, #0xffffffff
 25c:	lsr	x1, x1, x3
 260:	lsr	x3, x0, x3
 264:	orr	x3, x3, x2
 268:	udiv	x2, x1, x6
 26c:	lsl	x5, x0, x8
 270:	msub	x1, x2, x6, x1
 274:	mov	x0, x2
 278:	mul	x2, x7, x2
 27c:	extr	x1, x1, x3, #32
 280:	cmp	x2, x1
 284:	b.ls	298 <__udivti3+0x298>  // b.plast
 288:	adds	x1, x4, x1
 28c:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 290:	b.hi	2f0 <__udivti3+0x2f0>  // b.pmore
 294:	sub	x0, x0, #0x1
 298:	sub	x1, x1, x2
 29c:	udiv	x2, x1, x6
 2a0:	msub	x1, x2, x6, x1
 2a4:	mov	x8, x2
 2a8:	mul	x2, x7, x2
 2ac:	bfi	x3, x1, #32, #32
 2b0:	cmp	x2, x3
 2b4:	b.ls	2c8 <__udivti3+0x2c8>  // b.plast
 2b8:	adds	x3, x4, x3
 2bc:	ccmp	x2, x3, #0x0, cc  // cc = lo, ul, last
 2c0:	b.hi	308 <__udivti3+0x308>  // b.pmore
 2c4:	sub	x8, x8, #0x1
 2c8:	sub	x2, x3, x2
 2cc:	orr	x1, x8, x0, lsl #32
 2d0:	b	1e0 <__udivti3+0x1e0>
 2d4:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
 2d8:	mov	x1, #0x0                   	// #0
 2dc:	cset	x0, ls  // ls = plast
 2e0:	ret
 2e4:	mov	x0, x4
 2e8:	mov	x1, #0x0                   	// #0
 2ec:	ret
 2f0:	sub	x0, x0, #0x2
 2f4:	add	x1, x1, x4
 2f8:	b	298 <__udivti3+0x298>
 2fc:	sub	x5, x9, #0x2
 300:	add	x1, x1, x3
 304:	b	150 <__udivti3+0x150>
 308:	sub	x8, x8, #0x2
 30c:	add	x3, x3, x4
 310:	b	2c8 <__udivti3+0x2c8>
 314:	sub	x4, x4, #0x2
 318:	add	x5, x5, x3
 31c:	b	120 <__udivti3+0x120>
 320:	sub	x0, x0, #0x2
 324:	add	x2, x2, x4
 328:	b	20c <__udivti3+0x20c>
 32c:	sub	x0, x0, #0x2
 330:	add	x1, x1, x4
 334:	b	6c <__udivti3+0x6c>

_umoddi3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	mov	x4, x2
   4:	mov	x7, x0
   8:	mov	x6, x1
   c:	mov	x5, x1
  10:	cbnz	x3, a0 <__umodti3+0xa0>
  14:	cmp	x2, x1
  18:	b.ls	d4 <__umodti3+0xd4>  // b.plast
  1c:	clz	x0, x2
  20:	mov	x6, x0
  24:	cbz	x0, 40 <__umodti3+0x40>
  28:	neg	w1, w0
  2c:	lsl	x5, x5, x0
  30:	lsl	x4, x2, x0
  34:	lsr	x1, x7, x1
  38:	orr	x5, x1, x5
  3c:	lsl	x7, x7, x0
  40:	lsr	x2, x4, #32
  44:	and	x0, x4, #0xffffffff
  48:	udiv	x1, x5, x2
  4c:	msub	x5, x1, x2, x5
  50:	mul	x1, x0, x1
  54:	extr	x5, x5, x7, #32
  58:	cmp	x1, x5
  5c:	b.ls	70 <__umodti3+0x70>  // b.plast
  60:	adds	x5, x4, x5
  64:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
  68:	add	x3, x5, x4
  6c:	csel	x5, x3, x5, hi  // hi = pmore
  70:	sub	x5, x5, x1
  74:	udiv	x1, x5, x2
  78:	msub	x5, x1, x2, x5
  7c:	mul	x0, x0, x1
  80:	mov	x1, x7
  84:	bfi	x1, x5, #32, #32
  88:	cmp	x0, x1
  8c:	b.hi	1dc <__umodti3+0x1dc>  // b.pmore
  90:	sub	x0, x1, x0
  94:	lsr	x0, x0, x6
  98:	mov	x1, #0x0                   	// #0
  9c:	ret
  a0:	cmp	x3, x1
  a4:	mov	x4, x0
  a8:	b.hi	9c <__umodti3+0x9c>  // b.pmore
  ac:	clz	x7, x3
  b0:	cbnz	x7, 1f0 <__umodti3+0x1f0>
  b4:	cmp	x3, x1
  b8:	ccmp	x2, x0, #0x0, cs  // cs = hs, nlast
  bc:	b.hi	c8 <__umodti3+0xc8>  // b.pmore
  c0:	subs	x4, x0, x2
  c4:	sbc	x6, x1, x3
  c8:	mov	x0, x4
  cc:	mov	x1, x6
  d0:	ret
  d4:	cbnz	x2, e0 <__umodti3+0xe0>
  d8:	mov	x2, #0x1                   	// #1
  dc:	udiv	x4, x2, x4
  e0:	clz	x3, x4
  e4:	mov	x6, x3
  e8:	cbnz	x3, 158 <__umodti3+0x158>
  ec:	sub	x5, x1, x4
  f0:	lsr	x8, x4, #32
  f4:	and	x10, x4, #0xffffffff
  f8:	udiv	x3, x5, x8
  fc:	msub	x5, x3, x8, x5
 100:	mul	x9, x3, x10
 104:	extr	x3, x5, x7, #32
 108:	cmp	x9, x3
 10c:	b.ls	120 <__umodti3+0x120>  // b.plast
 110:	adds	x5, x4, x3
 114:	ccmp	x9, x5, #0x0, cc  // cc = lo, ul, last
 118:	add	x3, x5, x4
 11c:	csel	x3, x3, x5, hi  // hi = pmore
 120:	sub	x3, x3, x9
 124:	mov	x1, x7
 128:	udiv	x2, x3, x8
 12c:	msub	x3, x2, x8, x3
 130:	mul	x2, x2, x10
 134:	bfi	x1, x3, #32, #32
 138:	cmp	x2, x1
 13c:	b.ls	150 <__umodti3+0x150>  // b.plast
 140:	adds	x0, x4, x1
 144:	add	x4, x0, x4
 148:	ccmp	x2, x0, #0x0, cc  // cc = lo, ul, last
 14c:	csel	x1, x4, x0, hi  // hi = pmore
 150:	sub	x0, x1, x2
 154:	b	94 <__umodti3+0x94>
 158:	lsl	x4, x4, x3
 15c:	mov	x5, #0x40                  	// #64
 160:	sub	x5, x5, x3
 164:	lsr	x8, x4, #32
 168:	lsl	x2, x1, x3
 16c:	and	x10, x4, #0xffffffff
 170:	lsr	x1, x1, x5
 174:	lsr	x5, x0, x5
 178:	orr	x5, x5, x2
 17c:	udiv	x2, x1, x8
 180:	lsl	x7, x0, x3
 184:	msub	x1, x2, x8, x1
 188:	mul	x0, x10, x2
 18c:	extr	x2, x1, x5, #32
 190:	cmp	x0, x2
 194:	b.ls	1a8 <__umodti3+0x1a8>  // b.plast
 198:	adds	x2, x4, x2
 19c:	ccmp	x0, x2, #0x0, cc  // cc = lo, ul, last
 1a0:	add	x1, x2, x4
 1a4:	csel	x2, x1, x2, hi  // hi = pmore
 1a8:	sub	x2, x2, x0
 1ac:	udiv	x1, x2, x8
 1b0:	msub	x2, x1, x8, x2
 1b4:	mul	x1, x10, x1
 1b8:	bfi	x5, x2, #32, #32
 1bc:	cmp	x1, x5
 1c0:	b.ls	1d4 <__umodti3+0x1d4>  // b.plast
 1c4:	adds	x5, x4, x5
 1c8:	ccmp	x1, x5, #0x0, cc  // cc = lo, ul, last
 1cc:	add	x0, x5, x4
 1d0:	csel	x5, x0, x5, hi  // hi = pmore
 1d4:	sub	x5, x5, x1
 1d8:	b	f8 <__umodti3+0xf8>
 1dc:	adds	x1, x4, x1
 1e0:	add	x4, x1, x4
 1e4:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 1e8:	csel	x1, x4, x1, hi  // hi = pmore
 1ec:	b	90 <__umodti3+0x90>
 1f0:	mov	x8, #0x40                  	// #64
 1f4:	sub	x8, x8, x7
 1f8:	lsl	x3, x3, x7
 1fc:	lsr	x4, x2, x8
 200:	orr	x3, x4, x3
 204:	lsr	x6, x1, x8
 208:	and	x9, x3, #0xffffffff
 20c:	lsr	x10, x3, #32
 210:	lsr	x4, x0, x8
 214:	lsl	x1, x1, x7
 218:	orr	x1, x4, x1
 21c:	lsl	x12, x0, x7
 220:	udiv	x4, x6, x10
 224:	lsl	x2, x2, x7
 228:	mov	x5, x4
 22c:	msub	x4, x4, x10, x6
 230:	mul	x0, x9, x5
 234:	extr	x4, x4, x1, #32
 238:	cmp	x0, x4
 23c:	b.ls	250 <__umodti3+0x250>  // b.plast
 240:	adds	x4, x3, x4
 244:	ccmp	x0, x4, #0x0, cc  // cc = lo, ul, last
 248:	b.hi	310 <__umodti3+0x310>  // b.pmore
 24c:	sub	x5, x5, #0x1
 250:	sub	x4, x4, x0
 254:	udiv	x0, x4, x10
 258:	msub	x4, x0, x10, x4
 25c:	mov	x6, x0
 260:	mul	x0, x9, x0
 264:	bfi	x1, x4, #32, #32
 268:	cmp	x0, x1
 26c:	b.ls	280 <__umodti3+0x280>  // b.plast
 270:	adds	x1, x3, x1
 274:	ccmp	x0, x1, #0x0, cc  // cc = lo, ul, last
 278:	b.hi	304 <__umodti3+0x304>  // b.pmore
 27c:	sub	x6, x6, #0x1
 280:	orr	x5, x6, x5, lsl #32
 284:	and	x11, x2, #0xffffffff
 288:	mov	w9, w6
 28c:	lsr	x4, x2, #32
 290:	lsr	x5, x5, #32
 294:	sub	x1, x1, x0
 298:	mov	x13, #0x100000000           	// #4294967296
 29c:	mul	x6, x9, x11
 2a0:	mul	x11, x5, x11
 2a4:	madd	x9, x9, x4, x11
 2a8:	and	x10, x6, #0xffffffff
 2ac:	mul	x4, x5, x4
 2b0:	add	x5, x9, x6, lsr #32
 2b4:	add	x0, x4, x13
 2b8:	cmp	x11, x5
 2bc:	csel	x4, x0, x4, hi  // hi = pmore
 2c0:	add	x6, x10, x5, lsl #32
 2c4:	add	x4, x4, x5, lsr #32
 2c8:	cmp	x1, x4
 2cc:	b.cc	2d8 <__umodti3+0x2d8>  // b.lo, b.ul, b.last
 2d0:	ccmp	x12, x6, #0x2, eq  // eq = none
 2d4:	b.cs	2e4 <__umodti3+0x2e4>  // b.hs, b.nlast
 2d8:	subs	x6, x6, x2
 2dc:	cinc	x3, x3, cc  // cc = lo, ul, last
 2e0:	sub	x4, x4, x3
 2e4:	subs	x0, x12, x6
 2e8:	cmp	x12, x6
 2ec:	sbc	x1, x1, x4
 2f0:	lsr	x0, x0, x7
 2f4:	lsl	x8, x1, x8
 2f8:	orr	x0, x8, x0
 2fc:	lsr	x1, x1, x7
 300:	ret
 304:	sub	x6, x6, #0x2
 308:	add	x1, x1, x3
 30c:	b	280 <__umodti3+0x280>
 310:	sub	x5, x5, #0x2
 314:	add	x4, x4, x3
 318:	b	250 <__umodti3+0x250>

_udivmoddi4.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	mov	x9, x0
   4:	mov	x6, x1
   8:	mov	x7, x2
   c:	mov	x5, x0
  10:	mov	x8, x1
  14:	cbnz	x3, 154 <__udivmodti4+0x154>
  18:	cmp	x2, x1
  1c:	b.ls	c4 <__udivmodti4+0xc4>  // b.plast
  20:	clz	x0, x2
  24:	mov	x3, x0
  28:	cbz	x0, 44 <__udivmodti4+0x44>
  2c:	neg	w2, w0
  30:	lsl	x1, x1, x0
  34:	lsl	x7, x7, x0
  38:	lsr	x8, x9, x2
  3c:	orr	x8, x8, x1
  40:	lsl	x5, x9, x0
  44:	lsr	x9, x7, #32
  48:	and	x2, x7, #0xffffffff
  4c:	udiv	x1, x8, x9
  50:	mov	x0, x1
  54:	msub	x1, x1, x9, x8
  58:	mul	x6, x2, x0
  5c:	extr	x1, x1, x5, #32
  60:	cmp	x6, x1
  64:	b.ls	78 <__udivmodti4+0x78>  // b.plast
  68:	adds	x1, x7, x1
  6c:	ccmp	x6, x1, #0x0, cc  // cc = lo, ul, last
  70:	b.hi	3a8 <__udivmodti4+0x3a8>  // b.pmore
  74:	sub	x0, x0, #0x1
  78:	sub	x1, x1, x6
  7c:	udiv	x6, x1, x9
  80:	msub	x1, x6, x9, x1
  84:	mov	x8, x6
  88:	mul	x2, x2, x6
  8c:	bfi	x5, x1, #32, #32
  90:	cmp	x2, x5
  94:	b.ls	a8 <__udivmodti4+0xa8>  // b.plast
  98:	adds	x5, x7, x5
  9c:	ccmp	x2, x5, #0x0, cc  // cc = lo, ul, last
  a0:	b.hi	3b4 <__udivmodti4+0x3b4>  // b.pmore
  a4:	sub	x8, x6, #0x1
  a8:	sub	x5, x5, x2
  ac:	orr	x0, x8, x0, lsl #32
  b0:	mov	x1, #0x0                   	// #0
  b4:	cbz	x4, c0 <__udivmodti4+0xc0>
  b8:	lsr	x5, x5, x3
  bc:	stp	x5, xzr, [x4]
  c0:	ret
  c4:	cbnz	x2, d0 <__udivmodti4+0xd0>
  c8:	mov	x0, #0x1                   	// #1
  cc:	udiv	x7, x0, x2
  d0:	clz	x11, x7
  d4:	mov	x3, x11
  d8:	cbnz	x11, 1a4 <__udivmodti4+0x1a4>
  dc:	sub	x2, x6, x7
  e0:	lsr	x10, x7, #32
  e4:	and	x8, x7, #0xffffffff
  e8:	mov	x1, #0x1                   	// #1
  ec:	udiv	x6, x2, x10
  f0:	mov	x0, x6
  f4:	msub	x6, x6, x10, x2
  f8:	mul	x2, x0, x8
  fc:	extr	x6, x6, x5, #32
 100:	cmp	x2, x6
 104:	b.ls	118 <__udivmodti4+0x118>  // b.plast
 108:	adds	x6, x7, x6
 10c:	ccmp	x2, x6, #0x0, cc  // cc = lo, ul, last
 110:	b.hi	39c <__udivmodti4+0x39c>  // b.pmore
 114:	sub	x0, x0, #0x1
 118:	sub	x6, x6, x2
 11c:	udiv	x2, x6, x10
 120:	msub	x6, x2, x10, x6
 124:	mov	x9, x2
 128:	mul	x8, x2, x8
 12c:	bfi	x5, x6, #32, #32
 130:	cmp	x8, x5
 134:	b.ls	148 <__udivmodti4+0x148>  // b.plast
 138:	adds	x5, x7, x5
 13c:	ccmp	x8, x5, #0x0, cc  // cc = lo, ul, last
 140:	b.hi	390 <__udivmodti4+0x390>  // b.pmore
 144:	sub	x9, x2, #0x1
 148:	sub	x5, x5, x8
 14c:	orr	x0, x9, x0, lsl #32
 150:	b	b4 <__udivmodti4+0xb4>
 154:	cmp	x3, x1
 158:	b.ls	170 <__udivmodti4+0x170>  // b.plast
 15c:	mov	x1, #0x0                   	// #0
 160:	mov	x0, #0x0                   	// #0
 164:	cbz	x4, c0 <__udivmodti4+0xc0>
 168:	stp	x9, x6, [x4]
 16c:	ret
 170:	clz	x7, x3
 174:	cbnz	x7, 234 <__udivmodti4+0x234>
 178:	cmp	x3, x1
 17c:	mov	x0, #0x0                   	// #0
 180:	ccmp	x2, x9, #0x0, cs  // cs = hs, nlast
 184:	b.hi	194 <__udivmodti4+0x194>  // b.pmore
 188:	subs	x5, x9, x2
 18c:	mov	x0, #0x1                   	// #1
 190:	sbc	x8, x1, x3
 194:	mov	x1, #0x0                   	// #0
 198:	cbz	x4, c0 <__udivmodti4+0xc0>
 19c:	stp	x5, x8, [x4]
 1a0:	ret
 1a4:	lsl	x7, x7, x11
 1a8:	mov	x2, #0x40                  	// #64
 1ac:	sub	x2, x2, x11
 1b0:	lsr	x10, x7, #32
 1b4:	lsl	x0, x6, x11
 1b8:	and	x8, x7, #0xffffffff
 1bc:	lsr	x1, x6, x2
 1c0:	lsr	x2, x9, x2
 1c4:	orr	x6, x2, x0
 1c8:	udiv	x0, x1, x10
 1cc:	lsl	x5, x9, x11
 1d0:	mov	x9, x0
 1d4:	msub	x0, x0, x10, x1
 1d8:	mul	x1, x8, x9
 1dc:	extr	x0, x0, x6, #32
 1e0:	cmp	x1, x0
 1e4:	b.ls	1f8 <__udivmodti4+0x1f8>  // b.plast
 1e8:	adds	x0, x7, x0
 1ec:	ccmp	x1, x0, #0x0, cc  // cc = lo, ul, last
 1f0:	b.hi	36c <__udivmodti4+0x36c>  // b.pmore
 1f4:	sub	x9, x9, #0x1
 1f8:	sub	x0, x0, x1
 1fc:	udiv	x2, x0, x10
 200:	msub	x0, x2, x10, x0
 204:	mov	x1, x2
 208:	mul	x2, x8, x2
 20c:	bfi	x6, x0, #32, #32
 210:	cmp	x2, x6
 214:	b.ls	228 <__udivmodti4+0x228>  // b.plast
 218:	adds	x6, x7, x6
 21c:	ccmp	x2, x6, #0x0, cc  // cc = lo, ul, last
 220:	b.hi	384 <__udivmodti4+0x384>  // b.pmore
 224:	sub	x1, x1, #0x1
 228:	sub	x2, x6, x2
 22c:	orr	x1, x1, x9, lsl #32
 230:	b	ec <__udivmodti4+0xec>
 234:	mov	x8, #0x40                  	// #64
 238:	sub	x8, x8, x7
 23c:	lsl	x3, x3, x7
 240:	lsr	x0, x2, x8
 244:	orr	x3, x0, x3
 248:	lsr	x5, x1, x8
 24c:	and	x13, x3, #0xffffffff
 250:	lsr	x11, x3, #32
 254:	lsl	x6, x1, x7
 258:	lsr	x1, x9, x8
 25c:	orr	x6, x1, x6
 260:	lsl	x2, x2, x7
 264:	udiv	x1, x5, x11
 268:	lsl	x9, x9, x7
 26c:	mov	x0, x1
 270:	msub	x1, x1, x11, x5
 274:	mul	x5, x13, x0
 278:	extr	x1, x1, x6, #32
 27c:	cmp	x5, x1
 280:	b.ls	294 <__udivmodti4+0x294>  // b.plast
 284:	adds	x1, x3, x1
 288:	ccmp	x5, x1, #0x0, cc  // cc = lo, ul, last
 28c:	b.hi	360 <__udivmodti4+0x360>  // b.pmore
 290:	sub	x0, x0, #0x1
 294:	sub	x1, x1, x5
 298:	udiv	x10, x1, x11
 29c:	msub	x1, x10, x11, x1
 2a0:	mov	x5, x10
 2a4:	mul	x13, x13, x10
 2a8:	bfi	x6, x1, #32, #32
 2ac:	cmp	x13, x6
 2b0:	b.ls	2c4 <__udivmodti4+0x2c4>  // b.plast
 2b4:	adds	x6, x3, x6
 2b8:	ccmp	x13, x6, #0x0, cc  // cc = lo, ul, last
 2bc:	b.hi	378 <__udivmodti4+0x378>  // b.pmore
 2c0:	sub	x5, x10, #0x1
 2c4:	orr	x0, x5, x0, lsl #32
 2c8:	and	x12, x2, #0xffffffff
 2cc:	mov	w1, w5
 2d0:	lsr	x10, x2, #32
 2d4:	lsr	x11, x0, #32
 2d8:	sub	x6, x6, x13
 2dc:	mov	x14, #0x100000000           	// #4294967296
 2e0:	mul	x5, x1, x12
 2e4:	mul	x12, x11, x12
 2e8:	madd	x1, x1, x10, x12
 2ec:	and	x13, x5, #0xffffffff
 2f0:	mul	x10, x11, x10
 2f4:	add	x5, x1, x5, lsr #32
 2f8:	add	x1, x10, x14
 2fc:	cmp	x12, x5
 300:	csel	x10, x1, x10, hi  // hi = pmore
 304:	add	x11, x13, x5, lsl #32
 308:	add	x5, x10, x5, lsr #32
 30c:	cmp	x6, x5
 310:	b.cc	348 <__udivmodti4+0x348>  // b.lo, b.ul, b.last
 314:	ccmp	x9, x11, #0x2, eq  // eq = none
 318:	b.cc	348 <__udivmodti4+0x348>  // b.lo, b.ul, b.last
 31c:	mov	x1, #0x0                   	// #0
 320:	cbz	x4, c0 <__udivmodti4+0xc0>
 324:	subs	x2, x9, x11
 328:	cmp	x9, x11
 32c:	sbc	x6, x6, x5
 330:	lsr	x2, x2, x7
 334:	lsl	x8, x6, x8
 338:	orr	x8, x8, x2
 33c:	lsr	x6, x6, x7
 340:	stp	x8, x6, [x4]
 344:	ret
 348:	cmp	x11, x2
 34c:	sub	x0, x0, #0x1
 350:	cinc	x3, x3, cc  // cc = lo, ul, last
 354:	sub	x11, x11, x2
 358:	sub	x5, x5, x3
 35c:	b	31c <__udivmodti4+0x31c>
 360:	sub	x0, x0, #0x2
 364:	add	x1, x1, x3
 368:	b	294 <__udivmodti4+0x294>
 36c:	sub	x9, x9, #0x2
 370:	add	x0, x0, x7
 374:	b	1f8 <__udivmodti4+0x1f8>
 378:	sub	x5, x10, #0x2
 37c:	add	x6, x6, x3
 380:	b	2c4 <__udivmodti4+0x2c4>
 384:	sub	x1, x1, #0x2
 388:	add	x6, x6, x7
 38c:	b	228 <__udivmodti4+0x228>
 390:	sub	x9, x2, #0x2
 394:	add	x5, x5, x7
 398:	b	148 <__udivmodti4+0x148>
 39c:	sub	x0, x0, #0x2
 3a0:	add	x6, x6, x7
 3a4:	b	118 <__udivmodti4+0x118>
 3a8:	sub	x0, x0, #0x2
 3ac:	add	x1, x1, x7
 3b0:	b	78 <__udivmodti4+0x78>
 3b4:	sub	x8, x6, #0x2
 3b8:	add	x5, x5, x7
 3bc:	b	a8 <__udivmodti4+0xa8>

_udiv_w_sdiv.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udiv_w_sdiv>:
   0:	mov	x0, #0x0                   	// #0
   4:	ret

sync-cache.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__aarch64_sync_cache_range>:
   0:	adrp	x3, 0 <__aarch64_sync_cache_range>
   4:	ldr	w2, [x3]
   8:	cbnz	w2, 18 <__aarch64_sync_cache_range+0x18>
   c:	mrs	x4, ctr_el0
  10:	mov	w2, w4
  14:	str	w4, [x3]
  18:	ubfx	x4, x2, #16, #4
  1c:	mov	w3, #0x4                   	// #4
  20:	and	w5, w2, #0xf
  24:	lsl	w4, w3, w4
  28:	sub	w2, w4, #0x1
  2c:	bic	x2, x0, x2
  30:	sxtw	x4, w4
  34:	cmp	x2, x1
  38:	lsl	w3, w3, w5
  3c:	b.cs	50 <__aarch64_sync_cache_range+0x50>  // b.hs, b.nlast
  40:	dc	cvau, x2
  44:	add	x2, x2, x4
  48:	cmp	x1, x2
  4c:	b.hi	40 <__aarch64_sync_cache_range+0x40>  // b.pmore
  50:	dsb	ish
  54:	sub	w2, w3, #0x1
  58:	sxtw	x3, w3
  5c:	bic	x0, x0, x2
  60:	cmp	x1, x0
  64:	b.ls	78 <__aarch64_sync_cache_range+0x78>  // b.plast
  68:	ic	ivau, x0
  6c:	add	x0, x0, x3
  70:	cmp	x1, x0
  74:	b.hi	68 <__aarch64_sync_cache_range+0x68>  // b.pmore
  78:	dsb	ish
  7c:	isb
  80:	ret

sfp-exceptions.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__sfp_handle_exceptions>:
   0:	tbz	w0, #0, 10 <__sfp_handle_exceptions+0x10>
   4:	movi	v1.2s, #0x0
   8:	fdiv	s0, s1, s1
   c:	mrs	x1, fpsr
  10:	tbz	w0, #1, 24 <__sfp_handle_exceptions+0x24>
  14:	fmov	s1, #1.000000000000000000e+00
  18:	movi	v2.2s, #0x0
  1c:	fdiv	s0, s1, s2
  20:	mrs	x1, fpsr
  24:	tbz	w0, #2, 44 <__sfp_handle_exceptions+0x44>
  28:	mov	w2, #0xc5ae                	// #50606
  2c:	mov	w1, #0x7f7fffff            	// #2139095039
  30:	movk	w2, #0x749d, lsl #16
  34:	fmov	s1, w1
  38:	fmov	s2, w2
  3c:	fadd	s0, s1, s2
  40:	mrs	x1, fpsr
  44:	tbz	w0, #3, 54 <__sfp_handle_exceptions+0x54>
  48:	movi	v1.2s, #0x80, lsl #16
  4c:	fmul	s0, s1, s1
  50:	mrs	x1, fpsr
  54:	tbz	w0, #4, 6c <__sfp_handle_exceptions+0x6c>
  58:	mov	w0, #0x7f7fffff            	// #2139095039
  5c:	fmov	s2, #1.000000000000000000e+00
  60:	fmov	s1, w0
  64:	fsub	s0, s1, s2
  68:	mrs	x0, fpsr
  6c:	ret

addtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x6, x1, [sp, #16]
  14:	ldp	x0, x3, [sp, #32]
  18:	mrs	x15, fpcr
  1c:	lsr	x7, x1, #63
  20:	mov	x10, x0
  24:	ubfiz	x4, x3, #3, #48
  28:	lsr	x5, x3, #63
  2c:	mov	x12, x7
  30:	ubfiz	x2, x1, #3, #48
  34:	orr	x4, x4, x0, lsr #61
  38:	ubfx	x8, x1, #48, #15
  3c:	ubfx	x0, x3, #48, #15
  40:	and	w7, w7, #0xff
  44:	mov	x16, x12
  48:	cmp	x12, x5
  4c:	and	w11, w5, #0xff
  50:	orr	x2, x2, x6, lsr #61
  54:	mov	x1, x8
  58:	lsl	x9, x6, #3
  5c:	mov	x3, x0
  60:	lsl	x13, x10, #3
  64:	b.eq	210 <__addtf3+0x210>  // b.none
  68:	sub	w0, w8, w0
  6c:	cmp	w0, #0x0
  70:	b.le	1bc <__addtf3+0x1bc>
  74:	cbz	x3, 270 <__addtf3+0x270>
  78:	orr	x4, x4, #0x8000000000000
  7c:	mov	x3, #0x7fff                	// #32767
  80:	cmp	x1, x3
  84:	b.eq	474 <__addtf3+0x474>  // b.none
  88:	cmp	w0, #0x74
  8c:	b.gt	4f8 <__addtf3+0x4f8>
  90:	cmp	w0, #0x3f
  94:	b.gt	658 <__addtf3+0x658>
  98:	mov	w3, #0x40                  	// #64
  9c:	sub	w3, w3, w0
  a0:	lsr	x6, x13, x0
  a4:	lsl	x13, x13, x3
  a8:	cmp	x13, #0x0
  ac:	cset	x5, ne  // ne = any
  b0:	lsl	x3, x4, x3
  b4:	orr	x3, x3, x6
  b8:	lsr	x0, x4, x0
  bc:	orr	x3, x3, x5
  c0:	sub	x2, x2, x0
  c4:	subs	x9, x9, x3
  c8:	sbc	x2, x2, xzr
  cc:	and	x3, x2, #0x7ffffffffffff
  d0:	tbz	x2, #51, 2b0 <__addtf3+0x2b0>
  d4:	cbz	x3, 4dc <__addtf3+0x4dc>
  d8:	clz	x0, x3
  dc:	sub	w0, w0, #0xc
  e0:	neg	w2, w0
  e4:	lsl	x4, x3, x0
  e8:	lsl	x3, x9, x0
  ec:	lsr	x9, x9, x2
  f0:	orr	x2, x9, x4
  f4:	cmp	x1, w0, sxtw
  f8:	sxtw	x4, w0
  fc:	b.gt	4bc <__addtf3+0x4bc>
 100:	sub	w1, w0, w1
 104:	add	w0, w1, #0x1
 108:	cmp	w0, #0x3f
 10c:	b.gt	620 <__addtf3+0x620>
 110:	mov	w1, #0x40                  	// #64
 114:	sub	w1, w1, w0
 118:	lsr	x4, x3, x0
 11c:	lsl	x3, x3, x1
 120:	cmp	x3, #0x0
 124:	lsl	x9, x2, x1
 128:	cset	x1, ne  // ne = any
 12c:	orr	x9, x9, x4
 130:	lsr	x2, x2, x0
 134:	orr	x9, x9, x1
 138:	orr	x8, x9, x2
 13c:	cbz	x8, 2c4 <__addtf3+0x2c4>
 140:	and	x0, x9, #0x7
 144:	mov	x1, #0x0                   	// #0
 148:	mov	w5, #0x1                   	// #1
 14c:	cbz	x0, 508 <__addtf3+0x508>
 150:	and	x3, x15, #0xc00000
 154:	cmp	x3, #0x400, lsl #12
 158:	b.eq	44c <__addtf3+0x44c>  // b.none
 15c:	cmp	x3, #0x800, lsl #12
 160:	b.eq	42c <__addtf3+0x42c>  // b.none
 164:	cbz	x3, 458 <__addtf3+0x458>
 168:	and	x3, x2, #0x8000000000000
 16c:	mov	w0, #0x10                  	// #16
 170:	cbz	w5, 178 <__addtf3+0x178>
 174:	orr	w0, w0, #0x8
 178:	cbz	x3, 490 <__addtf3+0x490>
 17c:	add	x1, x1, #0x1
 180:	mov	x3, #0x7fff                	// #32767
 184:	cmp	x1, x3
 188:	b.eq	374 <__addtf3+0x374>  // b.none
 18c:	and	w1, w1, #0x7fff
 190:	extr	x8, x2, x9, #3
 194:	ubfx	x2, x2, #3, #48
 198:	mov	x5, #0x0                   	// #0
 19c:	orr	w1, w1, w7, lsl #15
 1a0:	bfxil	x5, x2, #0, #48
 1a4:	fmov	d0, x8
 1a8:	bfi	x5, x1, #48, #16
 1ac:	fmov	v0.d[1], x5
 1b0:	cbnz	w0, 3d0 <__addtf3+0x3d0>
 1b4:	ldp	x29, x30, [sp], #48
 1b8:	ret
 1bc:	mov	x14, x5
 1c0:	b.eq	2dc <__addtf3+0x2dc>  // b.none
 1c4:	cbnz	x8, 560 <__addtf3+0x560>
 1c8:	orr	x1, x2, x9
 1cc:	cbz	x1, 28c <__addtf3+0x28c>
 1d0:	cmn	w0, #0x1
 1d4:	b.eq	97c <__addtf3+0x97c>  // b.none
 1d8:	mov	x1, #0x7fff                	// #32767
 1dc:	mvn	w0, w0
 1e0:	cmp	x3, x1
 1e4:	b.ne	574 <__addtf3+0x574>  // b.any
 1e8:	orr	x0, x4, x13
 1ec:	cbnz	x0, 8d4 <__addtf3+0x8d4>
 1f0:	mov	x16, x14
 1f4:	nop
 1f8:	mov	x4, #0x0                   	// #0
 1fc:	fmov	d0, x4
 200:	lsl	x16, x16, #63
 204:	orr	x5, x16, #0x7fff000000000000
 208:	fmov	v0.d[1], x5
 20c:	b	1b4 <__addtf3+0x1b4>
 210:	sub	w5, w8, w0
 214:	cmp	w5, #0x0
 218:	b.le	3e4 <__addtf3+0x3e4>
 21c:	cbz	x0, 324 <__addtf3+0x324>
 220:	orr	x4, x4, #0x8000000000000
 224:	mov	x0, #0x7fff                	// #32767
 228:	cmp	x1, x0
 22c:	b.eq	474 <__addtf3+0x474>  // b.none
 230:	cmp	w5, #0x74
 234:	b.gt	608 <__addtf3+0x608>
 238:	cmp	w5, #0x3f
 23c:	b.gt	708 <__addtf3+0x708>
 240:	mov	w0, #0x40                  	// #64
 244:	sub	w0, w0, w5
 248:	lsr	x6, x13, x5
 24c:	lsl	x13, x13, x0
 250:	cmp	x13, #0x0
 254:	lsl	x0, x4, x0
 258:	cset	x3, ne  // ne = any
 25c:	orr	x0, x0, x6
 260:	lsr	x4, x4, x5
 264:	orr	x0, x0, x3
 268:	add	x2, x2, x4
 26c:	b	614 <__addtf3+0x614>
 270:	orr	x3, x4, x13
 274:	cbz	x3, 5e4 <__addtf3+0x5e4>
 278:	subs	w0, w0, #0x1
 27c:	b.ne	7c <__addtf3+0x7c>  // b.any
 280:	subs	x9, x9, x13
 284:	sbc	x2, x2, x4
 288:	b	cc <__addtf3+0xcc>
 28c:	mov	x0, #0x7fff                	// #32767
 290:	cmp	x3, x0
 294:	b.eq	9c8 <__addtf3+0x9c8>  // b.none
 298:	mov	w7, w11
 29c:	mov	x2, x4
 2a0:	mov	x9, x13
 2a4:	mov	x1, x3
 2a8:	mov	x12, x5
 2ac:	nop
 2b0:	orr	x8, x9, x2
 2b4:	and	x0, x9, #0x7
 2b8:	mov	w5, #0x0                   	// #0
 2bc:	cbnz	x1, 14c <__addtf3+0x14c>
 2c0:	cbnz	x8, 140 <__addtf3+0x140>
 2c4:	mov	x2, #0x0                   	// #0
 2c8:	mov	x1, #0x0                   	// #0
 2cc:	mov	w0, #0x0                   	// #0
 2d0:	and	x2, x2, #0xffffffffffff
 2d4:	and	w1, w1, #0x7fff
 2d8:	b	198 <__addtf3+0x198>
 2dc:	add	x5, x8, #0x1
 2e0:	tst	x5, #0x7ffe
 2e4:	b.ne	5b4 <__addtf3+0x5b4>  // b.any
 2e8:	orr	x5, x2, x9
 2ec:	orr	x8, x4, x13
 2f0:	cbnz	x1, 774 <__addtf3+0x774>
 2f4:	cbz	x5, 81c <__addtf3+0x81c>
 2f8:	cbz	x8, 830 <__addtf3+0x830>
 2fc:	subs	x5, x9, x13
 300:	cmp	x9, x13
 304:	sbc	x3, x2, x4
 308:	tbz	x3, #51, 9fc <__addtf3+0x9fc>
 30c:	subs	x9, x13, x9
 310:	mov	w7, w11
 314:	sbc	x2, x4, x2
 318:	mov	x12, x14
 31c:	orr	x8, x9, x2
 320:	b	13c <__addtf3+0x13c>
 324:	orr	x0, x4, x13
 328:	cbz	x0, 7fc <__addtf3+0x7fc>
 32c:	subs	w5, w5, #0x1
 330:	b.ne	224 <__addtf3+0x224>  // b.any
 334:	adds	x9, x9, x13
 338:	adc	x2, x4, x2
 33c:	nop
 340:	tbz	x2, #51, 2b0 <__addtf3+0x2b0>
 344:	add	x1, x1, #0x1
 348:	mov	x0, #0x7fff                	// #32767
 34c:	cmp	x1, x0
 350:	b.eq	83c <__addtf3+0x83c>  // b.none
 354:	and	x0, x9, #0x1
 358:	and	x3, x2, #0xfff7ffffffffffff
 35c:	orr	x9, x0, x9, lsr #1
 360:	mov	w5, #0x0                   	// #0
 364:	orr	x9, x9, x2, lsl #63
 368:	lsr	x2, x3, #1
 36c:	and	x0, x9, #0x7
 370:	b	14c <__addtf3+0x14c>
 374:	and	x3, x15, #0xc00000
 378:	cbz	x3, 3b0 <__addtf3+0x3b0>
 37c:	cmp	x3, #0x400, lsl #12
 380:	b.eq	3a8 <__addtf3+0x3a8>  // b.none
 384:	cmp	x3, #0x800, lsl #12
 388:	csel	w12, w12, wzr, eq  // eq = none
 38c:	cbnz	w12, 3b0 <__addtf3+0x3b0>
 390:	mov	w1, #0x14                  	// #20
 394:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 398:	orr	w0, w0, w1
 39c:	mov	x8, #0xffffffffffffffff    	// #-1
 3a0:	mov	x1, #0x7ffe                	// #32766
 3a4:	b	2d0 <__addtf3+0x2d0>
 3a8:	cbnz	x12, 390 <__addtf3+0x390>
 3ac:	nop
 3b0:	mov	w1, #0x14                  	// #20
 3b4:	and	x16, x7, #0xff
 3b8:	orr	w0, w0, w1
 3bc:	mov	x4, #0x0                   	// #0
 3c0:	fmov	d0, x4
 3c4:	lsl	x16, x16, #63
 3c8:	orr	x5, x16, #0x7fff000000000000
 3cc:	fmov	v0.d[1], x5
 3d0:	str	q0, [sp, #16]
 3d4:	bl	0 <__sfp_handle_exceptions>
 3d8:	ldr	q0, [sp, #16]
 3dc:	ldp	x29, x30, [sp], #48
 3e0:	ret
 3e4:	b.eq	524 <__addtf3+0x524>  // b.none
 3e8:	cbnz	x8, 6a8 <__addtf3+0x6a8>
 3ec:	orr	x0, x2, x9
 3f0:	cbz	x0, 914 <__addtf3+0x914>
 3f4:	cmn	w5, #0x1
 3f8:	b.eq	a74 <__addtf3+0xa74>  // b.none
 3fc:	mov	x0, #0x7fff                	// #32767
 400:	mvn	w5, w5
 404:	cmp	x3, x0
 408:	b.ne	6bc <__addtf3+0x6bc>  // b.any
 40c:	orr	x0, x4, x13
 410:	cbz	x0, 1f8 <__addtf3+0x1f8>
 414:	lsr	x5, x4, #50
 418:	mov	x9, x13
 41c:	eor	x5, x5, #0x1
 420:	mov	x2, x4
 424:	and	w5, w5, #0x1
 428:	b	488 <__addtf3+0x488>
 42c:	mov	w0, #0x10                  	// #16
 430:	cbz	x12, 43c <__addtf3+0x43c>
 434:	adds	x9, x9, #0x8
 438:	cinc	x2, x2, cs  // cs = hs, nlast
 43c:	and	x3, x2, #0x8000000000000
 440:	cbz	w5, 178 <__addtf3+0x178>
 444:	orr	w0, w0, #0x8
 448:	b	178 <__addtf3+0x178>
 44c:	mov	w0, #0x10                  	// #16
 450:	cbnz	x12, 43c <__addtf3+0x43c>
 454:	b	434 <__addtf3+0x434>
 458:	and	x3, x9, #0xf
 45c:	mov	w0, #0x10                  	// #16
 460:	cmp	x3, #0x4
 464:	b.eq	43c <__addtf3+0x43c>  // b.none
 468:	adds	x9, x9, #0x4
 46c:	cinc	x2, x2, cs  // cs = hs, nlast
 470:	b	43c <__addtf3+0x43c>
 474:	orr	x0, x2, x9
 478:	cbz	x0, 1f8 <__addtf3+0x1f8>
 47c:	lsr	x5, x2, #50
 480:	eor	x5, x5, #0x1
 484:	and	w5, w5, #0x1
 488:	mov	w0, w5
 48c:	mov	x1, #0x7fff                	// #32767
 490:	mov	x3, #0x7fff                	// #32767
 494:	extr	x8, x2, x9, #3
 498:	cmp	x1, x3
 49c:	lsr	x2, x2, #3
 4a0:	b.ne	2d0 <__addtf3+0x2d0>  // b.any
 4a4:	orr	x1, x8, x2
 4a8:	cbz	x1, b14 <__addtf3+0xb14>
 4ac:	orr	x2, x2, #0x800000000000
 4b0:	mov	w1, #0x7fff                	// #32767
 4b4:	and	x2, x2, #0xffffffffffff
 4b8:	b	198 <__addtf3+0x198>
 4bc:	mov	x9, x3
 4c0:	and	x2, x2, #0xfff7ffffffffffff
 4c4:	sub	x1, x1, x4
 4c8:	orr	x8, x9, x2
 4cc:	and	x0, x9, #0x7
 4d0:	mov	w5, #0x0                   	// #0
 4d4:	cbz	x1, 2c0 <__addtf3+0x2c0>
 4d8:	b	14c <__addtf3+0x14c>
 4dc:	clz	x2, x9
 4e0:	add	w0, w2, #0x34
 4e4:	cmp	w0, #0x3f
 4e8:	b.le	e0 <__addtf3+0xe0>
 4ec:	sub	w2, w2, #0xc
 4f0:	lsl	x2, x9, x2
 4f4:	b	f4 <__addtf3+0xf4>
 4f8:	orr	x4, x4, x13
 4fc:	cmp	x4, #0x0
 500:	cset	x3, ne  // ne = any
 504:	b	c4 <__addtf3+0xc4>
 508:	and	x3, x2, #0x8000000000000
 50c:	mov	w0, #0x0                   	// #0
 510:	cbz	w5, 178 <__addtf3+0x178>
 514:	mov	w0, #0x0                   	// #0
 518:	tbz	w15, #11, 178 <__addtf3+0x178>
 51c:	orr	w0, w0, #0x8
 520:	b	178 <__addtf3+0x178>
 524:	add	x0, x8, #0x1
 528:	tst	x0, #0x7ffe
 52c:	b.ne	734 <__addtf3+0x734>  // b.any
 530:	orr	x0, x2, x9
 534:	cbnz	x8, 8f0 <__addtf3+0x8f0>
 538:	orr	x8, x4, x13
 53c:	cbz	x0, 944 <__addtf3+0x944>
 540:	cbz	x8, 830 <__addtf3+0x830>
 544:	adds	x9, x9, x13
 548:	adc	x2, x4, x2
 54c:	tbz	x2, #51, 31c <__addtf3+0x31c>
 550:	and	x2, x2, #0xfff7ffffffffffff
 554:	and	x0, x9, #0x7
 558:	mov	x1, #0x1                   	// #1
 55c:	b	14c <__addtf3+0x14c>
 560:	mov	x1, #0x7fff                	// #32767
 564:	neg	w0, w0
 568:	orr	x2, x2, #0x8000000000000
 56c:	cmp	x3, x1
 570:	b.eq	1e8 <__addtf3+0x1e8>  // b.none
 574:	cmp	w0, #0x74
 578:	b.gt	684 <__addtf3+0x684>
 57c:	cmp	w0, #0x3f
 580:	b.gt	8a0 <__addtf3+0x8a0>
 584:	mov	w1, #0x40                  	// #64
 588:	sub	w1, w1, w0
 58c:	lsr	x5, x9, x0
 590:	lsl	x9, x9, x1
 594:	cmp	x9, #0x0
 598:	lsl	x9, x2, x1
 59c:	cset	x1, ne  // ne = any
 5a0:	orr	x9, x9, x5
 5a4:	lsr	x0, x2, x0
 5a8:	orr	x9, x9, x1
 5ac:	sub	x4, x4, x0
 5b0:	b	690 <__addtf3+0x690>
 5b4:	subs	x5, x9, x13
 5b8:	cmp	x9, x13
 5bc:	sbc	x3, x2, x4
 5c0:	tbnz	x3, #51, 75c <__addtf3+0x75c>
 5c4:	orr	x8, x5, x3
 5c8:	cbnz	x8, 888 <__addtf3+0x888>
 5cc:	and	x15, x15, #0xc00000
 5d0:	mov	x2, #0x0                   	// #0
 5d4:	cmp	x15, #0x800, lsl #12
 5d8:	mov	x1, #0x0                   	// #0
 5dc:	cset	w7, eq  // eq = none
 5e0:	b	2d0 <__addtf3+0x2d0>
 5e4:	mov	x0, #0x7fff                	// #32767
 5e8:	cmp	x8, x0
 5ec:	b.ne	2b0 <__addtf3+0x2b0>  // b.any
 5f0:	orr	x0, x2, x9
 5f4:	cbnz	x0, 47c <__addtf3+0x47c>
 5f8:	mov	x2, #0x0                   	// #0
 5fc:	mov	x8, #0x0                   	// #0
 600:	mov	w0, #0x0                   	// #0
 604:	b	4a4 <__addtf3+0x4a4>
 608:	orr	x4, x4, x13
 60c:	cmp	x4, #0x0
 610:	cset	x0, ne  // ne = any
 614:	adds	x9, x0, x9
 618:	cinc	x2, x2, cs  // cs = hs, nlast
 61c:	b	340 <__addtf3+0x340>
 620:	mov	w4, #0x80                  	// #128
 624:	sub	w4, w4, w0
 628:	cmp	w0, #0x40
 62c:	sub	w9, w1, #0x3f
 630:	lsl	x0, x2, x4
 634:	orr	x0, x3, x0
 638:	csel	x3, x0, x3, ne  // ne = any
 63c:	lsr	x9, x2, x9
 640:	cmp	x3, #0x0
 644:	mov	x2, #0x0                   	// #0
 648:	cset	x0, ne  // ne = any
 64c:	orr	x9, x0, x9
 650:	mov	x8, x9
 654:	b	13c <__addtf3+0x13c>
 658:	mov	w5, #0x80                  	// #128
 65c:	sub	w5, w5, w0
 660:	subs	w0, w0, #0x40
 664:	lsl	x5, x4, x5
 668:	orr	x5, x13, x5
 66c:	csel	x13, x5, x13, ne  // ne = any
 670:	lsr	x4, x4, x0
 674:	cmp	x13, #0x0
 678:	cset	x3, ne  // ne = any
 67c:	orr	x3, x3, x4
 680:	b	c4 <__addtf3+0xc4>
 684:	orr	x2, x2, x9
 688:	cmp	x2, #0x0
 68c:	cset	x9, ne  // ne = any
 690:	subs	x9, x13, x9
 694:	mov	w7, w11
 698:	sbc	x2, x4, xzr
 69c:	mov	x1, x3
 6a0:	mov	x12, x14
 6a4:	b	cc <__addtf3+0xcc>
 6a8:	mov	x0, #0x7fff                	// #32767
 6ac:	neg	w5, w5
 6b0:	orr	x2, x2, #0x8000000000000
 6b4:	cmp	x3, x0
 6b8:	b.eq	40c <__addtf3+0x40c>  // b.none
 6bc:	cmp	w5, #0x74
 6c0:	b.gt	890 <__addtf3+0x890>
 6c4:	cmp	w5, #0x3f
 6c8:	b.gt	950 <__addtf3+0x950>
 6cc:	mov	w1, #0x40                  	// #64
 6d0:	sub	w1, w1, w5
 6d4:	lsr	x6, x9, x5
 6d8:	lsl	x9, x9, x1
 6dc:	cmp	x9, #0x0
 6e0:	cset	x0, ne  // ne = any
 6e4:	lsl	x9, x2, x1
 6e8:	orr	x9, x9, x6
 6ec:	lsr	x5, x2, x5
 6f0:	orr	x9, x9, x0
 6f4:	add	x4, x4, x5
 6f8:	adds	x9, x9, x13
 6fc:	mov	x1, x3
 700:	cinc	x2, x4, cs  // cs = hs, nlast
 704:	b	340 <__addtf3+0x340>
 708:	mov	w0, #0x80                  	// #128
 70c:	sub	w0, w0, w5
 710:	subs	w5, w5, #0x40
 714:	lsl	x0, x4, x0
 718:	orr	x0, x13, x0
 71c:	csel	x13, x0, x13, ne  // ne = any
 720:	lsr	x4, x4, x5
 724:	cmp	x13, #0x0
 728:	cset	x0, ne  // ne = any
 72c:	orr	x0, x0, x4
 730:	b	614 <__addtf3+0x614>
 734:	mov	x1, #0x7fff                	// #32767
 738:	cmp	x0, x1
 73c:	b.eq	998 <__addtf3+0x998>  // b.none
 740:	adds	x9, x9, x13
 744:	mov	x1, x0
 748:	adc	x2, x4, x2
 74c:	ubfx	x0, x9, #1, #3
 750:	extr	x9, x2, x9, #1
 754:	lsr	x2, x2, #1
 758:	b	14c <__addtf3+0x14c>
 75c:	cmp	x13, x9
 760:	mov	w7, w11
 764:	sbc	x3, x4, x2
 768:	sub	x9, x13, x9
 76c:	mov	x12, x14
 770:	b	d4 <__addtf3+0xd4>
 774:	mov	x12, #0x7fff                	// #32767
 778:	cmp	x1, x12
 77c:	b.eq	7a8 <__addtf3+0x7a8>  // b.none
 780:	cmp	x3, x12
 784:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 788:	cbnz	x5, 7c0 <__addtf3+0x7c0>
 78c:	mov	w5, w0
 790:	cbnz	x8, ab8 <__addtf3+0xab8>
 794:	mov	w7, #0x0                   	// #0
 798:	mov	x2, #0xffffffffffff        	// #281474976710655
 79c:	mov	w0, #0x1                   	// #1
 7a0:	mov	x8, #0xffffffffffffffff    	// #-1
 7a4:	b	4ac <__addtf3+0x4ac>
 7a8:	cbz	x5, ad4 <__addtf3+0xad4>
 7ac:	lsr	x0, x2, #50
 7b0:	cmp	x3, x1
 7b4:	eor	x0, x0, #0x1
 7b8:	and	w0, w0, #0x1
 7bc:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 7c0:	cbz	x8, 9f4 <__addtf3+0x9f4>
 7c4:	bfi	x6, x2, #61, #3
 7c8:	lsr	x3, x2, #3
 7cc:	tbz	x2, #50, 7e8 <__addtf3+0x7e8>
 7d0:	lsr	x1, x4, #3
 7d4:	tbnz	x4, #50, 7e8 <__addtf3+0x7e8>
 7d8:	mov	x6, x10
 7dc:	mov	w7, w11
 7e0:	bfi	x6, x4, #61, #3
 7e4:	mov	x3, x1
 7e8:	extr	x2, x3, x6, #61
 7ec:	bfi	x6, x2, #61, #3
 7f0:	lsr	x2, x2, #3
 7f4:	mov	x8, x6
 7f8:	b	4a4 <__addtf3+0x4a4>
 7fc:	mov	x0, #0x7fff                	// #32767
 800:	cmp	x8, x0
 804:	b.ne	2b0 <__addtf3+0x2b0>  // b.any
 808:	orr	x0, x2, x9
 80c:	cbz	x0, 5f8 <__addtf3+0x5f8>
 810:	lsr	x5, x2, #50
 814:	eor	w5, w5, #0x1
 818:	b	488 <__addtf3+0x488>
 81c:	cbz	x8, 930 <__addtf3+0x930>
 820:	mov	w7, w11
 824:	mov	x2, x4
 828:	mov	x9, x13
 82c:	mov	x12, x14
 830:	mov	x1, #0x0                   	// #0
 834:	mov	x3, #0x0                   	// #0
 838:	b	514 <__addtf3+0x514>
 83c:	ands	x3, x15, #0xc00000
 840:	b.eq	8cc <__addtf3+0x8cc>  // b.none
 844:	cmp	x3, #0x400, lsl #12
 848:	eor	w0, w7, #0x1
 84c:	cset	w1, eq  // eq = none
 850:	tst	w1, w0
 854:	b.ne	af0 <__addtf3+0xaf0>  // b.any
 858:	cmp	x3, #0x800, lsl #12
 85c:	b.eq	a9c <__addtf3+0xa9c>  // b.none
 860:	cmp	x3, #0x400, lsl #12
 864:	mov	w0, #0x14                  	// #20
 868:	b.ne	378 <__addtf3+0x378>  // b.any
 86c:	mov	x2, #0xffffffffffffffff    	// #-1
 870:	mov	x1, #0x7ffe                	// #32766
 874:	mov	x9, x2
 878:	mov	w5, #0x0                   	// #0
 87c:	mov	w0, #0x14                  	// #20
 880:	cbnz	x12, 43c <__addtf3+0x43c>
 884:	b	434 <__addtf3+0x434>
 888:	mov	x9, x5
 88c:	b	d4 <__addtf3+0xd4>
 890:	orr	x2, x2, x9
 894:	cmp	x2, #0x0
 898:	cset	x9, ne  // ne = any
 89c:	b	6f8 <__addtf3+0x6f8>
 8a0:	mov	w1, #0x80                  	// #128
 8a4:	sub	w1, w1, w0
 8a8:	subs	w0, w0, #0x40
 8ac:	lsl	x1, x2, x1
 8b0:	orr	x1, x9, x1
 8b4:	csel	x9, x1, x9, ne  // ne = any
 8b8:	lsr	x2, x2, x0
 8bc:	cmp	x9, #0x0
 8c0:	cset	x9, ne  // ne = any
 8c4:	orr	x9, x9, x2
 8c8:	b	690 <__addtf3+0x690>
 8cc:	mov	w0, #0x14                  	// #20
 8d0:	b	3bc <__addtf3+0x3bc>
 8d4:	lsr	x5, x4, #50
 8d8:	mov	w7, w11
 8dc:	eor	x5, x5, #0x1
 8e0:	mov	x9, x13
 8e4:	and	w5, w5, #0x1
 8e8:	mov	x2, x4
 8ec:	b	488 <__addtf3+0x488>
 8f0:	mov	x8, #0x7fff                	// #32767
 8f4:	cmp	x1, x8
 8f8:	b.eq	a18 <__addtf3+0xa18>  // b.none
 8fc:	cmp	x3, x8
 900:	b.eq	a8c <__addtf3+0xa8c>  // b.none
 904:	cbnz	x0, a30 <__addtf3+0xa30>
 908:	mov	x2, x4
 90c:	mov	x9, x13
 910:	b	488 <__addtf3+0x488>
 914:	mov	x0, #0x7fff                	// #32767
 918:	cmp	x3, x0
 91c:	b.eq	ac8 <__addtf3+0xac8>  // b.none
 920:	mov	x2, x4
 924:	mov	x9, x13
 928:	mov	x1, x3
 92c:	b	2b0 <__addtf3+0x2b0>
 930:	and	x15, x15, #0xc00000
 934:	mov	x2, #0x0                   	// #0
 938:	cmp	x15, #0x800, lsl #12
 93c:	cset	w7, eq  // eq = none
 940:	b	2d0 <__addtf3+0x2d0>
 944:	mov	x2, x4
 948:	mov	x9, x13
 94c:	b	13c <__addtf3+0x13c>
 950:	mov	w0, #0x80                  	// #128
 954:	sub	w0, w0, w5
 958:	subs	w5, w5, #0x40
 95c:	lsl	x0, x2, x0
 960:	orr	x0, x9, x0
 964:	csel	x9, x0, x9, ne  // ne = any
 968:	lsr	x2, x2, x5
 96c:	cmp	x9, #0x0
 970:	cset	x9, ne  // ne = any
 974:	orr	x9, x9, x2
 978:	b	6f8 <__addtf3+0x6f8>
 97c:	cmp	x13, x9
 980:	mov	w7, w11
 984:	sbc	x2, x4, x2
 988:	sub	x9, x13, x9
 98c:	mov	x1, x3
 990:	mov	x12, x5
 994:	b	cc <__addtf3+0xcc>
 998:	ands	x3, x15, #0xc00000
 99c:	b.eq	8cc <__addtf3+0x8cc>  // b.none
 9a0:	cmp	x3, #0x400, lsl #12
 9a4:	eor	w0, w7, #0x1
 9a8:	csel	w0, w0, wzr, eq  // eq = none
 9ac:	cbnz	w0, af0 <__addtf3+0xaf0>
 9b0:	cmp	x3, #0x800, lsl #12
 9b4:	b.ne	860 <__addtf3+0x860>  // b.any
 9b8:	cbz	x12, aa0 <__addtf3+0xaa0>
 9bc:	mov	w0, #0x14                  	// #20
 9c0:	mov	x16, #0x1                   	// #1
 9c4:	b	3bc <__addtf3+0x3bc>
 9c8:	orr	x0, x4, x13
 9cc:	cbnz	x0, 8d4 <__addtf3+0x8d4>
 9d0:	mov	w7, w11
 9d4:	b	5f8 <__addtf3+0x5f8>
 9d8:	cbz	x8, ae4 <__addtf3+0xae4>
 9dc:	tst	x4, #0x4000000000000
 9e0:	csinc	w0, w0, wzr, ne  // ne = any
 9e4:	cbnz	x5, 7c4 <__addtf3+0x7c4>
 9e8:	mov	w7, w11
 9ec:	mov	x2, x4
 9f0:	mov	x9, x13
 9f4:	mov	w5, w0
 9f8:	b	488 <__addtf3+0x488>
 9fc:	orr	x8, x5, x3
 a00:	cbz	x8, 930 <__addtf3+0x930>
 a04:	and	x0, x5, #0x7
 a08:	mov	x9, x5
 a0c:	mov	x2, x3
 a10:	mov	w5, #0x1                   	// #1
 a14:	b	14c <__addtf3+0x14c>
 a18:	cbz	x0, a84 <__addtf3+0xa84>
 a1c:	lsr	x5, x2, #50
 a20:	cmp	x3, x1
 a24:	eor	x5, x5, #0x1
 a28:	and	w5, w5, #0x1
 a2c:	b.eq	afc <__addtf3+0xafc>  // b.none
 a30:	orr	x13, x4, x13
 a34:	cbz	x13, 488 <__addtf3+0x488>
 a38:	bfi	x6, x2, #61, #3
 a3c:	lsr	x0, x2, #3
 a40:	tbz	x2, #50, a5c <__addtf3+0xa5c>
 a44:	lsr	x1, x4, #3
 a48:	tbnz	x4, #50, a5c <__addtf3+0xa5c>
 a4c:	and	x6, x10, #0x1fffffffffffffff
 a50:	mov	w7, w11
 a54:	orr	x6, x6, x4, lsl #61
 a58:	mov	x0, x1
 a5c:	extr	x2, x0, x6, #61
 a60:	mov	w0, w5
 a64:	bfi	x6, x2, #61, #3
 a68:	lsr	x2, x2, #3
 a6c:	mov	x8, x6
 a70:	b	4a4 <__addtf3+0x4a4>
 a74:	adds	x9, x9, x13
 a78:	mov	x1, x3
 a7c:	adc	x2, x4, x2
 a80:	b	340 <__addtf3+0x340>
 a84:	cmp	x3, x1
 a88:	b.ne	908 <__addtf3+0x908>  // b.any
 a8c:	orr	x1, x4, x13
 a90:	cbnz	x1, b04 <__addtf3+0xb04>
 a94:	cbz	x0, 5f8 <__addtf3+0x5f8>
 a98:	b	488 <__addtf3+0x488>
 a9c:	cbnz	x16, 9bc <__addtf3+0x9bc>
 aa0:	mov	x2, #0xffffffffffffffff    	// #-1
 aa4:	mov	w7, #0x0                   	// #0
 aa8:	mov	x9, x2
 aac:	mov	x1, #0x7ffe                	// #32766
 ab0:	mov	w0, #0x14                  	// #20
 ab4:	b	17c <__addtf3+0x17c>
 ab8:	mov	w7, w11
 abc:	mov	x2, x4
 ac0:	mov	x9, x13
 ac4:	b	488 <__addtf3+0x488>
 ac8:	orr	x0, x4, x13
 acc:	cbz	x0, 5f8 <__addtf3+0x5f8>
 ad0:	b	414 <__addtf3+0x414>
 ad4:	cmp	x3, x1
 ad8:	b.eq	9d8 <__addtf3+0x9d8>  // b.none
 adc:	mov	w5, #0x0                   	// #0
 ae0:	b	790 <__addtf3+0x790>
 ae4:	cbnz	x5, 9f4 <__addtf3+0x9f4>
 ae8:	mov	w5, w0
 aec:	b	790 <__addtf3+0x790>
 af0:	mov	w0, #0x14                  	// #20
 af4:	mov	x16, #0x0                   	// #0
 af8:	b	3bc <__addtf3+0x3bc>
 afc:	orr	x1, x4, x13
 b00:	cbz	x1, 488 <__addtf3+0x488>
 b04:	tst	x4, #0x4000000000000
 b08:	csinc	w5, w5, wzr, ne  // ne = any
 b0c:	cbnz	x0, a38 <__addtf3+0xa38>
 b10:	b	908 <__addtf3+0x908>
 b14:	mov	x8, #0x0                   	// #0
 b18:	mov	w1, #0x7fff                	// #32767
 b1c:	mov	x2, #0x0                   	// #0
 b20:	b	198 <__addtf3+0x198>

divtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x2, x0, [sp, #16]
  14:	ldp	x5, x3, [sp, #32]
  18:	mrs	x10, fpcr
  1c:	lsr	x1, x0, #63
  20:	ubfx	x6, x0, #0, #48
  24:	and	w13, w1, #0xff
  28:	mov	x14, x1
  2c:	ubfx	x7, x0, #48, #15
  30:	cbz	w7, 450 <__divtf3+0x450>
  34:	mov	w4, #0x7fff                	// #32767
  38:	cmp	w7, w4
  3c:	b.eq	498 <__divtf3+0x498>  // b.none
  40:	and	x7, x7, #0xffff
  44:	extr	x6, x6, x2, #61
  48:	mov	x15, #0xffffffffffffc001    	// #-16383
  4c:	orr	x4, x6, #0x8000000000000
  50:	add	x7, x7, x15
  54:	lsl	x2, x2, #3
  58:	mov	x1, #0x0                   	// #0
  5c:	mov	x16, #0x0                   	// #0
  60:	mov	w0, #0x0                   	// #0
  64:	lsr	x8, x3, #63
  68:	ubfx	x6, x3, #0, #48
  6c:	and	w15, w8, #0xff
  70:	ubfx	x9, x3, #48, #15
  74:	cbz	w9, 40c <__divtf3+0x40c>
  78:	mov	w11, #0x7fff                	// #32767
  7c:	cmp	w9, w11
  80:	b.eq	144 <__divtf3+0x144>  // b.none
  84:	and	x9, x9, #0xffff
  88:	extr	x6, x6, x5, #61
  8c:	mov	x12, #0xffffffffffffc001    	// #-16383
  90:	orr	x6, x6, #0x8000000000000
  94:	add	x9, x9, x12
  98:	lsl	x5, x5, #3
  9c:	sub	x7, x7, x9
  a0:	mov	x9, #0x0                   	// #0
  a4:	eor	w11, w13, w15
  a8:	cmp	x1, #0x9
  ac:	and	x3, x11, #0xff
  b0:	mov	x12, x3
  b4:	b.gt	3e4 <__divtf3+0x3e4>
  b8:	cmp	x1, #0x7
  bc:	b.gt	5a4 <__divtf3+0x5a4>
  c0:	cmp	x1, #0x3
  c4:	b.eq	e0 <__divtf3+0xe0>  // b.none
  c8:	b.le	3b4 <__divtf3+0x3b4>
  cc:	cmp	x1, #0x5
  d0:	b.eq	3f4 <__divtf3+0x3f4>  // b.none
  d4:	b.le	1d8 <__divtf3+0x1d8>
  d8:	cmp	x1, #0x6
  dc:	b.eq	1a8 <__divtf3+0x1a8>  // b.none
  e0:	cmp	x9, #0x1
  e4:	b.eq	528 <__divtf3+0x528>  // b.none
  e8:	cbz	x9, fc <__divtf3+0xfc>
  ec:	cmp	x9, #0x2
  f0:	b.eq	72c <__divtf3+0x72c>  // b.none
  f4:	cmp	x9, #0x3
  f8:	b.eq	714 <__divtf3+0x714>  // b.none
  fc:	mov	x1, #0x3fff                	// #16383
 100:	mov	x12, x8
 104:	add	x3, x7, x1
 108:	cmp	x3, #0x0
 10c:	b.le	5e8 <__divtf3+0x5e8>
 110:	tst	x5, #0x7
 114:	b.ne	558 <__divtf3+0x558>  // b.any
 118:	and	w11, w12, #0x1
 11c:	tbz	x6, #52, 128 <__divtf3+0x128>
 120:	and	x6, x6, #0xffefffffffffffff
 124:	add	x3, x7, #0x4, lsl #12
 128:	mov	x1, #0x7ffe                	// #32766
 12c:	cmp	x3, x1
 130:	b.gt	6d0 <__divtf3+0x6d0>
 134:	and	w1, w3, #0x7fff
 138:	extr	x2, x6, x5, #3
 13c:	ubfx	x6, x6, #3, #48
 140:	b	1b4 <__divtf3+0x1b4>
 144:	mov	x9, #0xffffffffffff8001    	// #-32767
 148:	orr	x3, x6, x5
 14c:	add	x7, x7, x9
 150:	cbz	x3, 4cc <__divtf3+0x4cc>
 154:	tst	x6, #0x800000000000
 158:	orr	x1, x1, #0x3
 15c:	csinc	w0, w0, wzr, ne  // ne = any
 160:	mov	x9, #0x3                   	// #3
 164:	eor	w11, w13, w15
 168:	cmp	x1, #0x9
 16c:	and	x3, x11, #0xff
 170:	mov	x12, x3
 174:	b.le	b8 <__divtf3+0xb8>
 178:	cmp	x1, #0xf
 17c:	b.ne	3e4 <__divtf3+0x3e4>  // b.any
 180:	tbz	x4, #47, 590 <__divtf3+0x590>
 184:	tbnz	x6, #47, 590 <__divtf3+0x590>
 188:	orr	x6, x6, #0x800000000000
 18c:	mov	w11, w15
 190:	and	x6, x6, #0xffffffffffff
 194:	mov	x2, x5
 198:	mov	w1, #0x7fff                	// #32767
 19c:	b	1b4 <__divtf3+0x1b4>
 1a0:	cmp	x1, #0x2
 1a4:	b.ne	1e0 <__divtf3+0x1e0>  // b.any
 1a8:	mov	w1, #0x0                   	// #0
 1ac:	mov	x6, #0x0                   	// #0
 1b0:	mov	x2, #0x0                   	// #0
 1b4:	mov	x5, #0x0                   	// #0
 1b8:	orr	w1, w1, w11, lsl #15
 1bc:	bfxil	x5, x6, #0, #48
 1c0:	fmov	d0, x2
 1c4:	bfi	x5, x1, #48, #16
 1c8:	fmov	v0.d[1], x5
 1cc:	cbnz	w0, 3d4 <__divtf3+0x3d4>
 1d0:	ldp	x29, x30, [sp], #48
 1d4:	ret
 1d8:	cmp	x1, #0x4
 1dc:	b.eq	1a8 <__divtf3+0x1a8>  // b.none
 1e0:	cmp	x4, x6
 1e4:	b.ls	53c <__divtf3+0x53c>  // b.plast
 1e8:	lsr	x3, x4, #1
 1ec:	extr	x8, x4, x2, #1
 1f0:	lsl	x2, x2, #63
 1f4:	ubfx	x14, x6, #20, #32
 1f8:	extr	x9, x6, x5, #52
 1fc:	lsl	x13, x5, #12
 200:	and	x15, x9, #0xffffffff
 204:	udiv	x5, x3, x14
 208:	msub	x3, x5, x14, x3
 20c:	mul	x1, x15, x5
 210:	extr	x3, x3, x8, #32
 214:	cmp	x1, x3
 218:	b.ls	22c <__divtf3+0x22c>  // b.plast
 21c:	adds	x3, x9, x3
 220:	ccmp	x1, x3, #0x0, cc  // cc = lo, ul, last
 224:	b.hi	80c <__divtf3+0x80c>  // b.pmore
 228:	sub	x5, x5, #0x1
 22c:	sub	x3, x3, x1
 230:	mov	x4, x8
 234:	udiv	x1, x3, x14
 238:	msub	x3, x1, x14, x3
 23c:	mul	x6, x15, x1
 240:	bfi	x4, x3, #32, #32
 244:	cmp	x6, x4
 248:	b.ls	25c <__divtf3+0x25c>  // b.plast
 24c:	adds	x4, x9, x4
 250:	ccmp	x6, x4, #0x0, cc  // cc = lo, ul, last
 254:	b.hi	800 <__divtf3+0x800>  // b.pmore
 258:	sub	x1, x1, #0x1
 25c:	orr	x8, x1, x5, lsl #32
 260:	and	x17, x13, #0xffffffff
 264:	and	x1, x8, #0xffffffff
 268:	lsr	x16, x13, #32
 26c:	lsr	x5, x8, #32
 270:	sub	x4, x4, x6
 274:	mov	x18, #0x100000000           	// #4294967296
 278:	mul	x3, x1, x17
 27c:	mul	x30, x5, x17
 280:	madd	x6, x16, x1, x30
 284:	and	x1, x3, #0xffffffff
 288:	mul	x5, x5, x16
 28c:	add	x3, x6, x3, lsr #32
 290:	add	x6, x5, x18
 294:	cmp	x30, x3
 298:	csel	x5, x6, x5, hi  // hi = pmore
 29c:	add	x1, x1, x3, lsl #32
 2a0:	add	x5, x5, x3, lsr #32
 2a4:	cmp	x4, x5
 2a8:	b.cc	5b4 <__divtf3+0x5b4>  // b.lo, b.ul, b.last
 2ac:	ccmp	x2, x1, #0x2, eq  // eq = none
 2b0:	mov	x6, x8
 2b4:	b.cc	5b4 <__divtf3+0x5b4>  // b.lo, b.ul, b.last
 2b8:	subs	x8, x2, x1
 2bc:	mov	x3, #0x3fff                	// #16383
 2c0:	cmp	x2, x1
 2c4:	add	x3, x7, x3
 2c8:	sbc	x4, x4, x5
 2cc:	cmp	x9, x4
 2d0:	b.eq	818 <__divtf3+0x818>  // b.none
 2d4:	udiv	x5, x4, x14
 2d8:	msub	x4, x5, x14, x4
 2dc:	mul	x2, x15, x5
 2e0:	extr	x1, x4, x8, #32
 2e4:	cmp	x2, x1
 2e8:	b.ls	2fc <__divtf3+0x2fc>  // b.plast
 2ec:	adds	x1, x9, x1
 2f0:	ccmp	x2, x1, #0x0, cc  // cc = lo, ul, last
 2f4:	b.hi	8d0 <__divtf3+0x8d0>  // b.pmore
 2f8:	sub	x5, x5, #0x1
 2fc:	sub	x1, x1, x2
 300:	udiv	x2, x1, x14
 304:	msub	x1, x2, x14, x1
 308:	mul	x15, x15, x2
 30c:	bfi	x8, x1, #32, #32
 310:	mov	x1, x8
 314:	cmp	x15, x8
 318:	b.ls	32c <__divtf3+0x32c>  // b.plast
 31c:	adds	x1, x9, x8
 320:	ccmp	x15, x1, #0x0, cc  // cc = lo, ul, last
 324:	b.hi	8dc <__divtf3+0x8dc>  // b.pmore
 328:	sub	x2, x2, #0x1
 32c:	orr	x5, x2, x5, lsl #32
 330:	mov	x11, #0x100000000           	// #4294967296
 334:	and	x4, x5, #0xffffffff
 338:	sub	x1, x1, x15
 33c:	lsr	x14, x5, #32
 340:	mul	x2, x17, x4
 344:	mul	x17, x14, x17
 348:	madd	x4, x16, x4, x17
 34c:	and	x8, x2, #0xffffffff
 350:	mul	x16, x16, x14
 354:	add	x2, x4, x2, lsr #32
 358:	add	x4, x16, x11
 35c:	cmp	x17, x2
 360:	csel	x16, x4, x16, hi  // hi = pmore
 364:	add	x4, x8, x2, lsl #32
 368:	add	x16, x16, x2, lsr #32
 36c:	cmp	x1, x16
 370:	b.cs	750 <__divtf3+0x750>  // b.hs, b.nlast
 374:	adds	x2, x9, x1
 378:	sub	x8, x5, #0x1
 37c:	mov	x1, x2
 380:	b.cs	394 <__divtf3+0x394>  // b.hs, b.nlast
 384:	cmp	x2, x16
 388:	b.cc	850 <__divtf3+0x850>  // b.lo, b.ul, b.last
 38c:	ccmp	x13, x4, #0x2, eq  // eq = none
 390:	b.cc	850 <__divtf3+0x850>  // b.lo, b.ul, b.last
 394:	cmp	x13, x4
 398:	mov	x5, x8
 39c:	cset	w2, ne  // ne = any
 3a0:	cmp	w2, #0x0
 3a4:	orr	x2, x5, #0x1
 3a8:	ccmp	x1, x16, #0x0, eq  // eq = none
 3ac:	csel	x5, x2, x5, ne  // ne = any
 3b0:	b	108 <__divtf3+0x108>
 3b4:	cmp	x1, #0x1
 3b8:	b.ne	1a0 <__divtf3+0x1a0>  // b.any
 3bc:	mov	x4, #0x0                   	// #0
 3c0:	fmov	d0, x4
 3c4:	lsl	x3, x3, #63
 3c8:	orr	w0, w0, #0x2
 3cc:	orr	x5, x3, #0x7fff000000000000
 3d0:	fmov	v0.d[1], x5
 3d4:	str	q0, [sp, #16]
 3d8:	bl	0 <__sfp_handle_exceptions>
 3dc:	ldr	q0, [sp, #16]
 3e0:	b	1d0 <__divtf3+0x1d0>
 3e4:	cmp	x1, #0xb
 3e8:	b.gt	4b8 <__divtf3+0x4b8>
 3ec:	cmp	x1, #0xa
 3f0:	b.ne	e0 <__divtf3+0xe0>  // b.any
 3f4:	mov	w11, #0x0                   	// #0
 3f8:	mov	x6, #0xffffffffffff        	// #281474976710655
 3fc:	mov	x2, #0xffffffffffffffff    	// #-1
 400:	mov	w0, #0x1                   	// #1
 404:	mov	w1, #0x7fff                	// #32767
 408:	b	1b4 <__divtf3+0x1b4>
 40c:	orr	x3, x6, x5
 410:	cbz	x3, 4f8 <__divtf3+0x4f8>
 414:	cbz	x6, 6ac <__divtf3+0x6ac>
 418:	clz	x3, x6
 41c:	sub	x9, x3, #0xf
 420:	add	w12, w9, #0x3
 424:	mov	w11, #0x3d                  	// #61
 428:	sub	w9, w11, w9
 42c:	lsl	x6, x6, x12
 430:	lsr	x9, x5, x9
 434:	orr	x6, x9, x6
 438:	lsl	x5, x5, x12
 43c:	add	x7, x3, x7
 440:	mov	x11, #0x3fef                	// #16367
 444:	mov	x9, #0x0                   	// #0
 448:	add	x7, x7, x11
 44c:	b	a4 <__divtf3+0xa4>
 450:	orr	x4, x6, x2
 454:	cbz	x4, 4e0 <__divtf3+0x4e0>
 458:	cbz	x6, 688 <__divtf3+0x688>
 45c:	clz	x0, x6
 460:	sub	x4, x0, #0xf
 464:	add	w7, w4, #0x3
 468:	mov	w1, #0x3d                  	// #61
 46c:	sub	w4, w1, w4
 470:	lsl	x6, x6, x7
 474:	lsr	x4, x2, x4
 478:	orr	x4, x4, x6
 47c:	lsl	x2, x2, x7
 480:	mov	x7, #0xffffffffffffc011    	// #-16367
 484:	mov	x1, #0x0                   	// #0
 488:	sub	x7, x7, x0
 48c:	mov	x16, #0x0                   	// #0
 490:	mov	w0, #0x0                   	// #0
 494:	b	64 <__divtf3+0x64>
 498:	orr	x4, x6, x2
 49c:	cbnz	x4, 50c <__divtf3+0x50c>
 4a0:	mov	x2, #0x0                   	// #0
 4a4:	mov	x1, #0x8                   	// #8
 4a8:	mov	x7, #0x7fff                	// #32767
 4ac:	mov	x16, #0x2                   	// #2
 4b0:	mov	w0, #0x0                   	// #0
 4b4:	b	64 <__divtf3+0x64>
 4b8:	mov	x6, x4
 4bc:	mov	x5, x2
 4c0:	mov	x8, x14
 4c4:	mov	x9, x16
 4c8:	b	e0 <__divtf3+0xe0>
 4cc:	orr	x1, x1, #0x2
 4d0:	mov	x6, #0x0                   	// #0
 4d4:	mov	x5, #0x0                   	// #0
 4d8:	mov	x9, #0x2                   	// #2
 4dc:	b	164 <__divtf3+0x164>
 4e0:	mov	x2, #0x0                   	// #0
 4e4:	mov	x1, #0x4                   	// #4
 4e8:	mov	x7, #0x0                   	// #0
 4ec:	mov	x16, #0x1                   	// #1
 4f0:	mov	w0, #0x0                   	// #0
 4f4:	b	64 <__divtf3+0x64>
 4f8:	orr	x1, x1, #0x1
 4fc:	mov	x6, #0x0                   	// #0
 500:	mov	x5, #0x0                   	// #0
 504:	mov	x9, #0x1                   	// #1
 508:	b	a4 <__divtf3+0xa4>
 50c:	lsr	x0, x6, #47
 510:	mov	x4, x6
 514:	eor	w0, w0, #0x1
 518:	mov	x1, #0xc                   	// #12
 51c:	mov	x7, #0x7fff                	// #32767
 520:	mov	x16, #0x3                   	// #3
 524:	b	64 <__divtf3+0x64>
 528:	mov	w11, w8
 52c:	mov	w1, #0x0                   	// #0
 530:	mov	x6, #0x0                   	// #0
 534:	mov	x2, #0x0                   	// #0
 538:	b	1b4 <__divtf3+0x1b4>
 53c:	ccmp	x5, x2, #0x2, eq  // eq = none
 540:	b.ls	1e8 <__divtf3+0x1e8>  // b.plast
 544:	mov	x8, x2
 548:	sub	x7, x7, #0x1
 54c:	mov	x3, x4
 550:	mov	x2, #0x0                   	// #0
 554:	b	1f4 <__divtf3+0x1f4>
 558:	and	x1, x10, #0xc00000
 55c:	orr	w0, w0, #0x10
 560:	cmp	x1, #0x400, lsl #12
 564:	b.eq	8b8 <__divtf3+0x8b8>  // b.none
 568:	cmp	x1, #0x800, lsl #12
 56c:	b.eq	7cc <__divtf3+0x7cc>  // b.none
 570:	cbnz	x1, 118 <__divtf3+0x118>
 574:	and	x1, x5, #0xf
 578:	and	w11, w12, #0x1
 57c:	cmp	x1, #0x4
 580:	b.eq	11c <__divtf3+0x11c>  // b.none
 584:	adds	x5, x5, #0x4
 588:	cinc	x6, x6, cs  // cs = hs, nlast
 58c:	b	11c <__divtf3+0x11c>
 590:	orr	x6, x4, #0x800000000000
 594:	mov	w11, w13
 598:	and	x6, x6, #0xffffffffffff
 59c:	mov	w1, #0x7fff                	// #32767
 5a0:	b	1b4 <__divtf3+0x1b4>
 5a4:	mov	w1, #0x7fff                	// #32767
 5a8:	mov	x6, #0x0                   	// #0
 5ac:	mov	x2, #0x0                   	// #0
 5b0:	b	1b4 <__divtf3+0x1b4>
 5b4:	adds	x3, x2, x13
 5b8:	sub	x6, x8, #0x1
 5bc:	adc	x4, x4, x9
 5c0:	cset	x18, cs  // cs = hs, nlast
 5c4:	mov	x2, x3
 5c8:	cmp	x9, x4
 5cc:	b.cs	740 <__divtf3+0x740>  // b.hs, b.nlast
 5d0:	cmp	x5, x4
 5d4:	b.ls	768 <__divtf3+0x768>  // b.plast
 5d8:	adds	x2, x13, x3
 5dc:	sub	x6, x8, #0x2
 5e0:	adc	x4, x4, x9
 5e4:	b	2b8 <__divtf3+0x2b8>
 5e8:	mov	x1, #0x1                   	// #1
 5ec:	sub	x1, x1, x3
 5f0:	cmp	x1, #0x74
 5f4:	and	w11, w12, #0x1
 5f8:	b.le	614 <__divtf3+0x614>
 5fc:	orr	x2, x5, x6
 600:	cbnz	x2, 834 <__divtf3+0x834>
 604:	orr	w0, w0, #0x8
 608:	mov	w1, #0x0                   	// #0
 60c:	mov	x6, #0x0                   	// #0
 610:	b	6f8 <__divtf3+0x6f8>
 614:	cmp	x1, #0x3f
 618:	b.le	774 <__divtf3+0x774>
 61c:	mov	w2, #0x80                  	// #128
 620:	sub	w2, w2, w1
 624:	cmp	x1, #0x40
 628:	sub	w1, w1, #0x40
 62c:	lsl	x2, x6, x2
 630:	orr	x2, x5, x2
 634:	csel	x5, x2, x5, ne  // ne = any
 638:	lsr	x6, x6, x1
 63c:	cmp	x5, #0x0
 640:	cset	x2, ne  // ne = any
 644:	orr	x2, x2, x6
 648:	ands	x6, x2, #0x7
 64c:	b.eq	7a8 <__divtf3+0x7a8>  // b.none
 650:	mov	x6, #0x0                   	// #0
 654:	and	x10, x10, #0xc00000
 658:	orr	w0, w0, #0x10
 65c:	cmp	x10, #0x400, lsl #12
 660:	b.eq	8f4 <__divtf3+0x8f4>  // b.none
 664:	cmp	x10, #0x800, lsl #12
 668:	b.eq	908 <__divtf3+0x908>  // b.none
 66c:	cbz	x10, 870 <__divtf3+0x870>
 670:	tbnz	x6, #51, 888 <__divtf3+0x888>
 674:	orr	w0, w0, #0x8
 678:	extr	x2, x6, x2, #3
 67c:	mov	w1, #0x0                   	// #0
 680:	ubfx	x6, x6, #3, #48
 684:	b	6f8 <__divtf3+0x6f8>
 688:	clz	x7, x2
 68c:	add	x4, x7, #0x31
 690:	add	x0, x7, #0x40
 694:	cmp	x4, #0x3c
 698:	b.le	464 <__divtf3+0x464>
 69c:	sub	w4, w4, #0x3d
 6a0:	lsl	x4, x2, x4
 6a4:	mov	x2, #0x0                   	// #0
 6a8:	b	480 <__divtf3+0x480>
 6ac:	clz	x3, x5
 6b0:	add	x9, x3, #0x31
 6b4:	add	x3, x3, #0x40
 6b8:	cmp	x9, #0x3c
 6bc:	b.le	420 <__divtf3+0x420>
 6c0:	sub	w6, w9, #0x3d
 6c4:	lsl	x6, x5, x6
 6c8:	mov	x5, #0x0                   	// #0
 6cc:	b	43c <__divtf3+0x43c>
 6d0:	and	x2, x10, #0xc00000
 6d4:	cmp	x2, #0x400, lsl #12
 6d8:	b.eq	89c <__divtf3+0x89c>  // b.none
 6dc:	cmp	x2, #0x800, lsl #12
 6e0:	b.eq	7e4 <__divtf3+0x7e4>  // b.none
 6e4:	cbz	x2, 7c0 <__divtf3+0x7c0>
 6e8:	mov	x6, #0xffffffffffff        	// #281474976710655
 6ec:	mov	x2, #0xffffffffffffffff    	// #-1
 6f0:	mov	w3, #0x14                  	// #20
 6f4:	orr	w0, w0, w3
 6f8:	mov	x5, #0x0                   	// #0
 6fc:	orr	w1, w1, w11, lsl #15
 700:	bfxil	x5, x6, #0, #48
 704:	fmov	d0, x2
 708:	bfi	x5, x1, #48, #16
 70c:	fmov	v0.d[1], x5
 710:	b	3d4 <__divtf3+0x3d4>
 714:	orr	x6, x6, #0x800000000000
 718:	mov	w11, w8
 71c:	and	x6, x6, #0xffffffffffff
 720:	mov	x2, x5
 724:	mov	w1, #0x7fff                	// #32767
 728:	b	1b4 <__divtf3+0x1b4>
 72c:	mov	w11, w8
 730:	mov	w1, #0x7fff                	// #32767
 734:	mov	x6, #0x0                   	// #0
 738:	mov	x2, #0x0                   	// #0
 73c:	b	1b4 <__divtf3+0x1b4>
 740:	cmp	x18, #0x0
 744:	ccmp	x9, x4, #0x0, eq  // eq = none
 748:	b.ne	2b8 <__divtf3+0x2b8>  // b.any
 74c:	b	5d0 <__divtf3+0x5d0>
 750:	cmp	x4, #0x0
 754:	cset	w2, ne  // ne = any
 758:	cmp	w2, #0x0
 75c:	ccmp	x1, x16, #0x0, ne  // ne = any
 760:	b.ne	3a0 <__divtf3+0x3a0>  // b.any
 764:	b	374 <__divtf3+0x374>
 768:	ccmp	x1, x3, #0x0, eq  // eq = none
 76c:	b.ls	2b8 <__divtf3+0x2b8>  // b.plast
 770:	b	5d8 <__divtf3+0x5d8>
 774:	mov	w2, #0x40                  	// #64
 778:	sub	w2, w2, w1
 77c:	lsr	x4, x5, x1
 780:	lsl	x5, x5, x2
 784:	cmp	x5, #0x0
 788:	cset	x3, ne  // ne = any
 78c:	lsl	x2, x6, x2
 790:	orr	x2, x2, x4
 794:	lsr	x6, x6, x1
 798:	orr	x2, x2, x3
 79c:	tst	x2, #0x7
 7a0:	b.ne	654 <__divtf3+0x654>  // b.any
 7a4:	tbnz	x6, #51, 914 <__divtf3+0x914>
 7a8:	mov	w1, #0x0                   	// #0
 7ac:	extr	x2, x6, x2, #3
 7b0:	ubfx	x6, x6, #3, #48
 7b4:	tbz	w10, #11, 1b4 <__divtf3+0x1b4>
 7b8:	orr	w0, w0, #0x8
 7bc:	b	6f8 <__divtf3+0x6f8>
 7c0:	mov	w1, #0x7fff                	// #32767
 7c4:	mov	x6, #0x0                   	// #0
 7c8:	b	6f0 <__divtf3+0x6f0>
 7cc:	mov	w11, #0x0                   	// #0
 7d0:	cbz	x12, 11c <__divtf3+0x11c>
 7d4:	adds	x5, x5, #0x8
 7d8:	mov	w11, #0x1                   	// #1
 7dc:	cinc	x6, x6, cs  // cs = hs, nlast
 7e0:	b	11c <__divtf3+0x11c>
 7e4:	cmp	x12, #0x0
 7e8:	mov	w2, #0x7fff                	// #32767
 7ec:	mov	x6, #0xffffffffffff        	// #281474976710655
 7f0:	csel	w1, w1, w2, eq  // eq = none
 7f4:	csel	x6, x6, xzr, eq  // eq = none
 7f8:	csetm	x2, eq  // eq = none
 7fc:	b	6f0 <__divtf3+0x6f0>
 800:	sub	x1, x1, #0x2
 804:	add	x4, x4, x9
 808:	b	25c <__divtf3+0x25c>
 80c:	sub	x5, x5, #0x2
 810:	add	x3, x3, x9
 814:	b	22c <__divtf3+0x22c>
 818:	cmp	x3, #0x0
 81c:	mov	x5, #0xffffffffffffffff    	// #-1
 820:	b.gt	558 <__divtf3+0x558>
 824:	mov	x1, #0x1                   	// #1
 828:	sub	x1, x1, x3
 82c:	cmp	x1, #0x74
 830:	b.le	614 <__divtf3+0x614>
 834:	and	x10, x10, #0xc00000
 838:	orr	w0, w0, #0x10
 83c:	cmp	x10, #0x400, lsl #12
 840:	b.eq	8e8 <__divtf3+0x8e8>  // b.none
 844:	cmp	x10, #0x800, lsl #12
 848:	csel	x2, x12, xzr, eq  // eq = none
 84c:	b	604 <__divtf3+0x604>
 850:	lsl	x8, x13, #1
 854:	sub	x5, x5, #0x2
 858:	cmp	x13, x8
 85c:	cinc	x1, x9, hi  // hi = pmore
 860:	cmp	x4, x8
 864:	add	x1, x2, x1
 868:	cset	w2, ne  // ne = any
 86c:	b	3a0 <__divtf3+0x3a0>
 870:	and	x1, x2, #0xf
 874:	cmp	x1, #0x4
 878:	b.eq	884 <__divtf3+0x884>  // b.none
 87c:	adds	x2, x2, #0x4
 880:	cinc	x6, x6, cs  // cs = hs, nlast
 884:	tbz	x6, #51, 674 <__divtf3+0x674>
 888:	orr	w0, w0, #0x8
 88c:	mov	w1, #0x1                   	// #1
 890:	mov	x6, #0x0                   	// #0
 894:	mov	x2, #0x0                   	// #0
 898:	b	6f8 <__divtf3+0x6f8>
 89c:	cmp	x12, #0x0
 8a0:	mov	w2, #0x7fff                	// #32767
 8a4:	mov	x6, #0xffffffffffff        	// #281474976710655
 8a8:	csel	w1, w1, w2, ne  // ne = any
 8ac:	csel	x6, x6, xzr, ne  // ne = any
 8b0:	csetm	x2, ne  // ne = any
 8b4:	b	6f0 <__divtf3+0x6f0>
 8b8:	mov	w11, #0x1                   	// #1
 8bc:	cbnz	x12, 11c <__divtf3+0x11c>
 8c0:	adds	x5, x5, #0x8
 8c4:	mov	w11, #0x0                   	// #0
 8c8:	cinc	x6, x6, cs  // cs = hs, nlast
 8cc:	b	11c <__divtf3+0x11c>
 8d0:	sub	x5, x5, #0x2
 8d4:	add	x1, x1, x9
 8d8:	b	2fc <__divtf3+0x2fc>
 8dc:	sub	x2, x2, #0x2
 8e0:	add	x1, x1, x9
 8e4:	b	32c <__divtf3+0x32c>
 8e8:	mov	x2, #0x1                   	// #1
 8ec:	sub	x2, x2, x12
 8f0:	b	604 <__divtf3+0x604>
 8f4:	cbnz	x12, 884 <__divtf3+0x884>
 8f8:	adds	x2, x2, #0x8
 8fc:	cinc	x6, x6, cs  // cs = hs, nlast
 900:	tbnz	x6, #51, 888 <__divtf3+0x888>
 904:	b	674 <__divtf3+0x674>
 908:	cbnz	x12, 8f8 <__divtf3+0x8f8>
 90c:	tbnz	x6, #51, 888 <__divtf3+0x888>
 910:	b	674 <__divtf3+0x674>
 914:	orr	w0, w0, #0x10
 918:	b	888 <__divtf3+0x888>

eqtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eqtf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x6, x1, [sp, #16]
  14:	ldp	x7, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	lsr	x2, x1, #63
  24:	lsr	x3, x0, #63
  28:	ubfx	x9, x0, #0, #48
  2c:	mov	x5, #0x7fff                	// #32767
  30:	mov	x10, x6
  34:	cmp	x4, x5
  38:	and	w2, w2, #0xff
  3c:	ubfx	x1, x1, #0, #48
  40:	and	w3, w3, #0xff
  44:	ubfx	x0, x0, #48, #15
  48:	b.eq	7c <__eqtf2+0x7c>  // b.none
  4c:	cmp	x0, x5
  50:	b.eq	68 <__eqtf2+0x68>  // b.none
  54:	cmp	x4, x0
  58:	mov	w0, #0x1                   	// #1
  5c:	b.eq	94 <__eqtf2+0x94>  // b.none
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	orr	x8, x9, x7
  6c:	cbnz	x8, f8 <__eqtf2+0xf8>
  70:	mov	w0, #0x1                   	// #1
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x5, x1, x6
  80:	cbnz	x5, c8 <__eqtf2+0xc8>
  84:	cmp	x0, x4
  88:	b.ne	70 <__eqtf2+0x70>  // b.any
  8c:	orr	x8, x9, x7
  90:	cbnz	x8, f8 <__eqtf2+0xf8>
  94:	cmp	x1, x9
  98:	mov	w0, #0x1                   	// #1
  9c:	ccmp	x6, x7, #0x0, eq  // eq = none
  a0:	b.ne	60 <__eqtf2+0x60>  // b.any
  a4:	cmp	w2, w3
  a8:	mov	w0, #0x0                   	// #0
  ac:	b.eq	60 <__eqtf2+0x60>  // b.none
  b0:	mov	w0, #0x1                   	// #1
  b4:	cbnz	x4, 60 <__eqtf2+0x60>
  b8:	orr	x1, x1, x10
  bc:	cmp	x1, #0x0
  c0:	cset	w0, ne  // ne = any
  c4:	b	60 <__eqtf2+0x60>
  c8:	tst	x1, #0x800000000000
  cc:	b.ne	e4 <__eqtf2+0xe4>  // b.any
  d0:	mov	w0, #0x1                   	// #1
  d4:	bl	0 <__sfp_handle_exceptions>
  d8:	mov	w0, #0x1                   	// #1
  dc:	ldp	x29, x30, [sp], #48
  e0:	ret
  e4:	cmp	x0, x4
  e8:	mov	w0, #0x1                   	// #1
  ec:	b.ne	60 <__eqtf2+0x60>  // b.any
  f0:	orr	x8, x9, x7
  f4:	cbz	x8, 60 <__eqtf2+0x60>
  f8:	tst	x9, #0x800000000000
  fc:	b.eq	d0 <__eqtf2+0xd0>  // b.none
 100:	b	70 <__eqtf2+0x70>

getf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__getf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x8, x1, [sp, #16]
  14:	ldp	x9, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	ubfx	x10, x1, #0, #48
  24:	lsr	x2, x1, #63
  28:	mov	x7, #0x7fff                	// #32767
  2c:	mov	x5, x8
  30:	cmp	x4, x7
  34:	ubfx	x11, x0, #0, #48
  38:	ubfx	x6, x0, #48, #15
  3c:	lsr	x1, x0, #63
  40:	mov	x3, x9
  44:	b.eq	80 <__getf2+0x80>  // b.none
  48:	cmp	x6, x7
  4c:	b.eq	90 <__getf2+0x90>  // b.none
  50:	cbnz	x4, b8 <__getf2+0xb8>
  54:	orr	x5, x10, x8
  58:	cmp	x5, #0x0
  5c:	cset	w0, eq  // eq = none
  60:	cbnz	x6, 6c <__getf2+0x6c>
  64:	orr	x3, x11, x9
  68:	cbz	x3, d8 <__getf2+0xd8>
  6c:	cbz	w0, 9c <__getf2+0x9c>
  70:	cmp	x1, #0x0
  74:	csinv	w0, w0, wzr, ne  // ne = any
  78:	ldp	x29, x30, [sp], #48
  7c:	ret
  80:	orr	x0, x10, x8
  84:	cbnz	x0, e4 <__getf2+0xe4>
  88:	cmp	x6, x4
  8c:	b.ne	b8 <__getf2+0xb8>  // b.any
  90:	orr	x3, x11, x3
  94:	cbnz	x3, e4 <__getf2+0xe4>
  98:	cbz	x4, c8 <__getf2+0xc8>
  9c:	cmp	x2, x1
  a0:	b.eq	f8 <__getf2+0xf8>  // b.none
  a4:	cmp	x2, #0x0
  a8:	mov	w0, #0xffffffff            	// #-1
  ac:	cneg	w0, w0, eq  // eq = none
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	cbnz	x6, 9c <__getf2+0x9c>
  bc:	orr	x3, x11, x3
  c0:	cbnz	x3, 9c <__getf2+0x9c>
  c4:	b	a4 <__getf2+0xa4>
  c8:	orr	x5, x10, x5
  cc:	cmp	x5, #0x0
  d0:	cset	w0, eq  // eq = none
  d4:	b	6c <__getf2+0x6c>
  d8:	mov	w0, #0x0                   	// #0
  dc:	cbz	x5, 78 <__getf2+0x78>
  e0:	b	a4 <__getf2+0xa4>
  e4:	mov	w0, #0x1                   	// #1
  e8:	bl	0 <__sfp_handle_exceptions>
  ec:	mov	w0, #0xfffffffe            	// #-2
  f0:	ldp	x29, x30, [sp], #48
  f4:	ret
  f8:	cmp	x4, x6
  fc:	b.gt	a4 <__getf2+0xa4>
 100:	b.lt	138 <__getf2+0x138>  // b.tstop
 104:	cmp	x10, x11
 108:	b.hi	a4 <__getf2+0xa4>  // b.pmore
 10c:	cset	w0, eq  // eq = none
 110:	cmp	w0, #0x0
 114:	ccmp	x8, x9, #0x0, ne  // ne = any
 118:	b.hi	a4 <__getf2+0xa4>  // b.pmore
 11c:	cmp	x10, x11
 120:	b.cc	138 <__getf2+0x138>  // b.lo, b.ul, b.last
 124:	cmp	w0, #0x0
 128:	mov	w0, #0x0                   	// #0
 12c:	ccmp	x8, x9, #0x2, ne  // ne = any
 130:	b.cs	78 <__getf2+0x78>  // b.hs, b.nlast
 134:	nop
 138:	cmp	x2, #0x0
 13c:	mov	w0, #0x1                   	// #1
 140:	cneg	w0, w0, eq  // eq = none
 144:	b	78 <__getf2+0x78>

letf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__letf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x8, x1, [sp, #16]
  14:	ldp	x9, x0, [sp, #32]
  18:	mrs	x2, fpcr
  1c:	ubfx	x4, x1, #48, #15
  20:	ubfx	x10, x1, #0, #48
  24:	lsr	x2, x1, #63
  28:	mov	x5, #0x7fff                	// #32767
  2c:	mov	x6, x8
  30:	cmp	x4, x5
  34:	ubfx	x11, x0, #0, #48
  38:	ubfx	x7, x0, #48, #15
  3c:	lsr	x1, x0, #63
  40:	mov	x3, x9
  44:	b.eq	7c <__letf2+0x7c>  // b.none
  48:	cmp	x7, x5
  4c:	b.eq	8c <__letf2+0x8c>  // b.none
  50:	cbnz	x4, b8 <__letf2+0xb8>
  54:	orr	x6, x10, x8
  58:	cmp	x6, #0x0
  5c:	cset	w0, eq  // eq = none
  60:	cbnz	x7, a4 <__letf2+0xa4>
  64:	orr	x3, x11, x9
  68:	cbnz	x3, a4 <__letf2+0xa4>
  6c:	mov	w0, #0x0                   	// #0
  70:	cbnz	x6, cc <__letf2+0xcc>
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x0, x10, x8
  80:	cbnz	x0, e0 <__letf2+0xe0>
  84:	cmp	x7, x4
  88:	b.ne	b8 <__letf2+0xb8>  // b.any
  8c:	orr	x3, x11, x3
  90:	cbnz	x3, e0 <__letf2+0xe0>
  94:	cbnz	x4, c4 <__letf2+0xc4>
  98:	orr	x6, x10, x6
  9c:	cmp	x6, #0x0
  a0:	cset	w0, eq  // eq = none
  a4:	cbz	w0, c4 <__letf2+0xc4>
  a8:	cmp	x1, #0x0
  ac:	csinv	w0, w0, wzr, ne  // ne = any
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	cbnz	x7, c4 <__letf2+0xc4>
  bc:	orr	x3, x11, x3
  c0:	cbz	x3, cc <__letf2+0xcc>
  c4:	cmp	x2, x1
  c8:	b.eq	f4 <__letf2+0xf4>  // b.none
  cc:	cmp	x2, #0x0
  d0:	mov	w0, #0xffffffff            	// #-1
  d4:	cneg	w0, w0, eq  // eq = none
  d8:	ldp	x29, x30, [sp], #48
  dc:	ret
  e0:	mov	w0, #0x1                   	// #1
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	mov	w0, #0x2                   	// #2
  ec:	ldp	x29, x30, [sp], #48
  f0:	ret
  f4:	cmp	x4, x7
  f8:	b.gt	cc <__letf2+0xcc>
  fc:	b.lt	130 <__letf2+0x130>  // b.tstop
 100:	cmp	x10, x11
 104:	b.hi	cc <__letf2+0xcc>  // b.pmore
 108:	cset	w0, eq  // eq = none
 10c:	cmp	w0, #0x0
 110:	ccmp	x8, x9, #0x0, ne  // ne = any
 114:	b.hi	cc <__letf2+0xcc>  // b.pmore
 118:	cmp	x10, x11
 11c:	b.cc	130 <__letf2+0x130>  // b.lo, b.ul, b.last
 120:	cmp	w0, #0x0
 124:	mov	w0, #0x0                   	// #0
 128:	ccmp	x8, x9, #0x2, ne  // ne = any
 12c:	b.cs	74 <__letf2+0x74>  // b.hs, b.nlast
 130:	cmp	x2, #0x0
 134:	mov	w0, #0x1                   	// #1
 138:	cneg	w0, w0, eq  // eq = none
 13c:	b	74 <__letf2+0x74>

multf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #48]
   c:	str	q1, [sp, #64]
  10:	ldp	x1, x0, [sp, #48]
  14:	ldp	x6, x2, [sp, #64]
  18:	mrs	x11, fpcr
  1c:	lsr	x3, x0, #63
  20:	ubfx	x7, x0, #0, #48
  24:	and	w12, w3, #0xff
  28:	mov	x14, x3
  2c:	ubfx	x3, x0, #48, #15
  30:	cbz	w3, 3d8 <__multf3+0x3d8>
  34:	mov	w4, #0x7fff                	// #32767
  38:	cmp	w3, w4
  3c:	b.eq	47c <__multf3+0x47c>  // b.none
  40:	and	x3, x3, #0xffff
  44:	extr	x4, x7, x1, #61
  48:	mov	x18, #0xffffffffffffc001    	// #-16383
  4c:	orr	x7, x4, #0x8000000000000
  50:	add	x3, x3, x18
  54:	lsl	x5, x1, #3
  58:	mov	x16, #0x0                   	// #0
  5c:	mov	x1, #0x0                   	// #0
  60:	mov	w0, #0x0                   	// #0
  64:	lsr	x8, x2, #63
  68:	ubfx	x4, x2, #0, #48
  6c:	and	w15, w8, #0xff
  70:	mov	x13, x8
  74:	ubfx	x9, x2, #48, #15
  78:	cbz	w9, 438 <__multf3+0x438>
  7c:	mov	w8, #0x7fff                	// #32767
  80:	cmp	w9, w8
  84:	b.eq	108 <__multf3+0x108>  // b.none
  88:	and	x9, x9, #0xffff
  8c:	mov	x17, #0xffffffffffffc001    	// #-16383
  90:	add	x9, x9, x17
  94:	extr	x2, x4, x6, #61
  98:	add	x9, x9, x3
  9c:	lsl	x6, x6, #3
  a0:	orr	x4, x2, #0x8000000000000
  a4:	mov	x2, #0x0                   	// #0
  a8:	eor	w8, w12, w15
  ac:	cmp	x1, #0xa
  b0:	and	w10, w8, #0xff
  b4:	add	x3, x9, #0x1
  b8:	and	x8, x8, #0xff
  bc:	b.le	140 <__multf3+0x140>
  c0:	cmp	x1, #0xb
  c4:	b.eq	7d8 <__multf3+0x7d8>  // b.none
  c8:	mov	w15, w12
  cc:	mov	x13, x14
  d0:	mov	w10, w15
  d4:	cmp	x16, #0x2
  d8:	b.eq	49c <__multf3+0x49c>  // b.none
  dc:	mov	x4, x7
  e0:	mov	x6, x5
  e4:	mov	x2, x16
  e8:	mov	x8, x13
  ec:	cmp	x2, #0x3
  f0:	b.ne	15c <__multf3+0x15c>  // b.any
  f4:	orr	x4, x4, #0x800000000000
  f8:	mov	x5, x6
  fc:	and	x4, x4, #0xffffffffffff
 100:	mov	w1, #0x7fff                	// #32767
 104:	b	170 <__multf3+0x170>
 108:	mov	x8, #0x7fff                	// #32767
 10c:	orr	x2, x4, x6
 110:	add	x9, x3, x8
 114:	cbnz	x2, 194 <__multf3+0x194>
 118:	eor	w8, w12, w15
 11c:	orr	x1, x1, #0x2
 120:	and	w10, w8, #0xff
 124:	cmp	x1, #0xa
 128:	add	x3, x3, #0x8, lsl #12
 12c:	and	x8, x8, #0xff
 130:	mov	x6, #0x0                   	// #0
 134:	b.gt	74c <__multf3+0x74c>
 138:	mov	x4, #0x0                   	// #0
 13c:	mov	x2, #0x2                   	// #2
 140:	cmp	x1, #0x2
 144:	b.gt	1bc <__multf3+0x1bc>
 148:	sub	x1, x1, #0x1
 14c:	cmp	x1, #0x1
 150:	b.hi	1f8 <__multf3+0x1f8>  // b.pmore
 154:	cmp	x2, #0x2
 158:	b.eq	49c <__multf3+0x49c>  // b.none
 15c:	cmp	x2, #0x1
 160:	b.ne	358 <__multf3+0x358>  // b.any
 164:	mov	w1, #0x0                   	// #0
 168:	mov	x4, #0x0                   	// #0
 16c:	mov	x5, #0x0                   	// #0
 170:	mov	x3, #0x0                   	// #0
 174:	orr	w1, w1, w10, lsl #15
 178:	bfxil	x3, x4, #0, #48
 17c:	fmov	d0, x5
 180:	bfi	x3, x1, #48, #16
 184:	fmov	v0.d[1], x3
 188:	cbnz	w0, 5c8 <__multf3+0x5c8>
 18c:	ldp	x29, x30, [sp], #80
 190:	ret
 194:	tst	x4, #0x800000000000
 198:	eor	w8, w12, w15
 19c:	orr	x1, x1, #0x3
 1a0:	csinc	w0, w0, wzr, ne  // ne = any
 1a4:	and	w10, w8, #0xff
 1a8:	add	x3, x3, #0x8, lsl #12
 1ac:	cmp	x1, #0xa
 1b0:	and	x8, x8, #0xff
 1b4:	mov	x2, #0x3                   	// #3
 1b8:	b.gt	7cc <__multf3+0x7cc>
 1bc:	mov	x12, #0x1                   	// #1
 1c0:	mov	x14, #0x530                 	// #1328
 1c4:	lsl	x1, x12, x1
 1c8:	tst	x1, x14
 1cc:	b.ne	3cc <__multf3+0x3cc>  // b.any
 1d0:	mov	x14, #0x240                 	// #576
 1d4:	tst	x1, x14
 1d8:	b.ne	3b4 <__multf3+0x3b4>  // b.any
 1dc:	mov	x12, #0x88                  	// #136
 1e0:	tst	x1, x12
 1e4:	b.eq	1f8 <__multf3+0x1f8>  // b.none
 1e8:	mov	x7, x4
 1ec:	mov	x5, x6
 1f0:	mov	x16, x2
 1f4:	b	d0 <__multf3+0xd0>
 1f8:	lsr	x13, x5, #32
 1fc:	and	x12, x6, #0xffffffff
 200:	and	x15, x5, #0xffffffff
 204:	lsr	x6, x6, #32
 208:	and	x18, x4, #0xffffffff
 20c:	lsr	x2, x4, #32
 210:	mul	x4, x13, x12
 214:	stp	x21, x22, [sp, #32]
 218:	lsr	x22, x7, #32
 21c:	and	x5, x7, #0xffffffff
 220:	mul	x16, x12, x15
 224:	madd	x7, x6, x15, x4
 228:	stp	x19, x20, [sp, #16]
 22c:	mul	x1, x13, x18
 230:	mul	x17, x15, x18
 234:	and	x30, x16, #0xffffffff
 238:	madd	x15, x2, x15, x1
 23c:	add	x16, x7, x16, lsr #32
 240:	mul	x21, x22, x12
 244:	cmp	x4, x16
 248:	mul	x20, x22, x18
 24c:	mov	x14, #0x100000000           	// #4294967296
 250:	mul	x19, x13, x6
 254:	add	x15, x15, x17, lsr #32
 258:	mul	x12, x12, x5
 25c:	and	x17, x17, #0xffffffff
 260:	mul	x18, x5, x18
 264:	add	x4, x19, x14
 268:	madd	x7, x6, x5, x21
 26c:	csel	x19, x4, x19, hi  // hi = pmore
 270:	madd	x5, x2, x5, x20
 274:	cmp	x1, x15
 278:	mul	x13, x13, x2
 27c:	add	x17, x17, x15, lsl #32
 280:	mul	x6, x6, x22
 284:	add	x7, x7, x12, lsr #32
 288:	add	x5, x5, x18, lsr #32
 28c:	add	x4, x13, x14
 290:	mul	x2, x2, x22
 294:	csel	x13, x4, x13, hi  // hi = pmore
 298:	and	x1, x18, #0xffffffff
 29c:	cmp	x21, x7
 2a0:	add	x4, x6, x14
 2a4:	add	x30, x30, x16, lsl #32
 2a8:	csel	x6, x4, x6, hi  // hi = pmore
 2ac:	add	x13, x13, x15, lsr #32
 2b0:	cmp	x20, x5
 2b4:	add	x1, x1, x5, lsl #32
 2b8:	add	x16, x17, x16, lsr #32
 2bc:	add	x14, x2, x14
 2c0:	csel	x2, x14, x2, hi  // hi = pmore
 2c4:	add	x16, x19, x16
 2c8:	adds	x1, x1, x13
 2cc:	and	x12, x12, #0xffffffff
 2d0:	cset	x13, cs  // cs = hs, nlast
 2d4:	cmp	x16, x17
 2d8:	cset	x4, cc  // cc = lo, ul, last
 2dc:	add	x12, x12, x7, lsl #32
 2e0:	adds	x1, x1, x4
 2e4:	lsr	x5, x5, #32
 2e8:	cset	x4, cs  // cs = hs, nlast
 2ec:	cmp	x13, #0x0
 2f0:	ccmp	x4, #0x0, #0x0, eq  // eq = none
 2f4:	add	x7, x6, x7, lsr #32
 2f8:	cinc	x5, x5, ne  // ne = any
 2fc:	adds	x6, x16, x12
 300:	cset	x4, cs  // cs = hs, nlast
 304:	adds	x1, x1, x7
 308:	cset	x7, cs  // cs = hs, nlast
 30c:	adds	x4, x1, x4
 310:	cset	x1, cs  // cs = hs, nlast
 314:	cmp	x7, #0x0
 318:	orr	x30, x30, x6, lsl #13
 31c:	ccmp	x1, #0x0, #0x0, eq  // eq = none
 320:	cinc	x1, x2, ne  // ne = any
 324:	cmp	x30, #0x0
 328:	add	x1, x1, x5
 32c:	cset	x2, ne  // ne = any
 330:	orr	x6, x2, x6, lsr #51
 334:	orr	x6, x6, x4, lsl #13
 338:	extr	x4, x1, x4, #51
 33c:	tbz	x1, #39, 650 <__multf3+0x650>
 340:	ldp	x19, x20, [sp, #16]
 344:	and	x1, x6, #0x1
 348:	ldp	x21, x22, [sp, #32]
 34c:	orr	x6, x1, x6, lsr #1
 350:	orr	x6, x6, x4, lsl #63
 354:	lsr	x4, x4, #1
 358:	mov	x1, #0x3fff                	// #16383
 35c:	add	x2, x3, x1
 360:	cmp	x2, #0x0
 364:	b.le	4fc <__multf3+0x4fc>
 368:	tst	x6, #0x7
 36c:	b.eq	38c <__multf3+0x38c>  // b.none
 370:	and	x1, x11, #0xc00000
 374:	orr	w0, w0, #0x10
 378:	cmp	x1, #0x400, lsl #12
 37c:	b.eq	744 <__multf3+0x744>  // b.none
 380:	cmp	x1, #0x800, lsl #12
 384:	b.eq	6ec <__multf3+0x6ec>  // b.none
 388:	cbz	x1, 6d4 <__multf3+0x6d4>
 38c:	tbz	x4, #52, 398 <__multf3+0x398>
 390:	and	x4, x4, #0xffefffffffffffff
 394:	add	x2, x3, #0x4, lsl #12
 398:	mov	x1, #0x7ffe                	// #32766
 39c:	cmp	x2, x1
 3a0:	b.gt	624 <__multf3+0x624>
 3a4:	and	w1, w2, #0x7fff
 3a8:	extr	x5, x4, x6, #3
 3ac:	ubfx	x4, x4, #3, #48
 3b0:	b	170 <__multf3+0x170>
 3b4:	mov	w0, w12
 3b8:	mov	w10, #0x0                   	// #0
 3bc:	mov	x4, #0xffffffffffff        	// #281474976710655
 3c0:	mov	x5, #0xffffffffffffffff    	// #-1
 3c4:	mov	w1, #0x7fff                	// #32767
 3c8:	b	170 <__multf3+0x170>
 3cc:	mov	w15, w10
 3d0:	mov	x13, x8
 3d4:	b	d0 <__multf3+0xd0>
 3d8:	orr	x5, x7, x1
 3dc:	cbz	x5, 4e4 <__multf3+0x4e4>
 3e0:	cbz	x7, 600 <__multf3+0x600>
 3e4:	clz	x0, x7
 3e8:	sub	x4, x0, #0xf
 3ec:	add	w5, w4, #0x3
 3f0:	mov	w3, #0x3d                  	// #61
 3f4:	sub	w3, w3, w4
 3f8:	lsl	x4, x7, x5
 3fc:	lsr	x3, x1, x3
 400:	orr	x7, x3, x4
 404:	lsl	x5, x1, x5
 408:	lsr	x8, x2, #63
 40c:	mov	x3, #0xffffffffffffc011    	// #-16367
 410:	ubfx	x4, x2, #0, #48
 414:	sub	x3, x3, x0
 418:	and	w15, w8, #0xff
 41c:	mov	x13, x8
 420:	ubfx	x9, x2, #48, #15
 424:	mov	x1, #0x0                   	// #0
 428:	mov	x16, #0x0                   	// #0
 42c:	mov	w0, #0x0                   	// #0
 430:	cbnz	w9, 7c <__multf3+0x7c>
 434:	nop
 438:	orr	x2, x4, x6
 43c:	cbz	x2, 4ac <__multf3+0x4ac>
 440:	cbz	x4, 5dc <__multf3+0x5dc>
 444:	clz	x9, x4
 448:	sub	x2, x9, #0xf
 44c:	add	w10, w2, #0x3
 450:	mov	w8, #0x3d                  	// #61
 454:	sub	w8, w8, w2
 458:	lsl	x2, x4, x10
 45c:	lsr	x8, x6, x8
 460:	orr	x4, x8, x2
 464:	lsl	x6, x6, x10
 468:	sub	x9, x3, x9
 46c:	mov	x10, #0xffffffffffffc011    	// #-16367
 470:	mov	x2, #0x0                   	// #0
 474:	add	x9, x9, x10
 478:	b	a8 <__multf3+0xa8>
 47c:	orr	x5, x7, x1
 480:	cbnz	x5, 4c4 <__multf3+0x4c4>
 484:	mov	x7, #0x0                   	// #0
 488:	mov	x1, #0x8                   	// #8
 48c:	mov	x3, #0x7fff                	// #32767
 490:	mov	x16, #0x2                   	// #2
 494:	mov	w0, #0x0                   	// #0
 498:	b	64 <__multf3+0x64>
 49c:	mov	w1, #0x7fff                	// #32767
 4a0:	mov	x4, #0x0                   	// #0
 4a4:	mov	x5, #0x0                   	// #0
 4a8:	b	170 <__multf3+0x170>
 4ac:	orr	x1, x1, #0x1
 4b0:	mov	x9, x3
 4b4:	mov	x4, #0x0                   	// #0
 4b8:	mov	x6, #0x0                   	// #0
 4bc:	mov	x2, #0x1                   	// #1
 4c0:	b	a8 <__multf3+0xa8>
 4c4:	lsr	x0, x7, #47
 4c8:	mov	x5, x1
 4cc:	eor	x0, x0, #0x1
 4d0:	mov	x1, #0xc                   	// #12
 4d4:	and	w0, w0, #0x1
 4d8:	mov	x3, #0x7fff                	// #32767
 4dc:	mov	x16, #0x3                   	// #3
 4e0:	b	64 <__multf3+0x64>
 4e4:	mov	x7, #0x0                   	// #0
 4e8:	mov	x1, #0x4                   	// #4
 4ec:	mov	x3, #0x0                   	// #0
 4f0:	mov	x16, #0x1                   	// #1
 4f4:	mov	w0, #0x0                   	// #0
 4f8:	b	64 <__multf3+0x64>
 4fc:	mov	x1, #0x1                   	// #1
 500:	sub	x2, x1, x2
 504:	cmp	x2, #0x74
 508:	b.gt	580 <__multf3+0x580>
 50c:	cmp	x2, #0x3f
 510:	b.le	660 <__multf3+0x660>
 514:	mov	w1, #0x80                  	// #128
 518:	sub	w1, w1, w2
 51c:	cmp	x2, #0x40
 520:	sub	w2, w2, #0x40
 524:	lsl	x1, x4, x1
 528:	orr	x1, x6, x1
 52c:	csel	x6, x1, x6, ne  // ne = any
 530:	lsr	x2, x4, x2
 534:	cmp	x6, #0x0
 538:	cset	x5, ne  // ne = any
 53c:	orr	x5, x5, x2
 540:	ands	x2, x5, #0x7
 544:	b.eq	694 <__multf3+0x694>  // b.none
 548:	mov	x2, #0x0                   	// #0
 54c:	and	x11, x11, #0xc00000
 550:	orr	w0, w0, #0x10
 554:	cmp	x11, #0x400, lsl #12
 558:	b.eq	7a4 <__multf3+0x7a4>  // b.none
 55c:	cmp	x11, #0x800, lsl #12
 560:	b.eq	7b8 <__multf3+0x7b8>  // b.none
 564:	cbz	x11, 6fc <__multf3+0x6fc>
 568:	tbnz	x2, #51, 714 <__multf3+0x714>
 56c:	ubfx	x4, x2, #3, #48
 570:	extr	x5, x2, x5, #3
 574:	orr	w0, w0, #0x8
 578:	mov	w1, #0x0                   	// #0
 57c:	b	5b0 <__multf3+0x5b0>
 580:	orr	x5, x6, x4
 584:	cbz	x5, 5a4 <__multf3+0x5a4>
 588:	and	x11, x11, #0xc00000
 58c:	orr	w0, w0, #0x10
 590:	cmp	x11, #0x400, lsl #12
 594:	sub	x5, x1, x8
 598:	b.eq	5a4 <__multf3+0x5a4>  // b.none
 59c:	cmp	x11, #0x800, lsl #12
 5a0:	csel	x5, x8, xzr, eq  // eq = none
 5a4:	orr	w0, w0, #0x8
 5a8:	mov	w1, #0x0                   	// #0
 5ac:	mov	x4, #0x0                   	// #0
 5b0:	mov	x3, #0x0                   	// #0
 5b4:	fmov	d0, x5
 5b8:	bfxil	x3, x4, #0, #48
 5bc:	bfi	x3, x1, #48, #15
 5c0:	bfi	x3, x10, #63, #1
 5c4:	fmov	v0.d[1], x3
 5c8:	str	q0, [sp, #48]
 5cc:	bl	0 <__sfp_handle_exceptions>
 5d0:	ldr	q0, [sp, #48]
 5d4:	ldp	x29, x30, [sp], #80
 5d8:	ret
 5dc:	clz	x9, x6
 5e0:	add	x2, x9, #0x31
 5e4:	add	x9, x9, #0x40
 5e8:	cmp	x2, #0x3c
 5ec:	b.le	44c <__multf3+0x44c>
 5f0:	sub	w2, w2, #0x3d
 5f4:	lsl	x4, x6, x2
 5f8:	mov	x6, #0x0                   	// #0
 5fc:	b	468 <__multf3+0x468>
 600:	clz	x3, x1
 604:	add	x4, x3, #0x31
 608:	add	x0, x3, #0x40
 60c:	cmp	x4, #0x3c
 610:	b.le	3ec <__multf3+0x3ec>
 614:	sub	w4, w4, #0x3d
 618:	mov	x5, #0x0                   	// #0
 61c:	lsl	x7, x1, x4
 620:	b	408 <__multf3+0x408>
 624:	and	x5, x11, #0xc00000
 628:	cmp	x5, #0x400, lsl #12
 62c:	b.eq	728 <__multf3+0x728>  // b.none
 630:	cmp	x5, #0x800, lsl #12
 634:	b.eq	6b8 <__multf3+0x6b8>  // b.none
 638:	cbz	x5, 6ac <__multf3+0x6ac>
 63c:	mov	x4, #0xffffffffffff        	// #281474976710655
 640:	mov	x5, #0xffffffffffffffff    	// #-1
 644:	mov	w2, #0x14                  	// #20
 648:	orr	w0, w0, w2
 64c:	b	5b0 <__multf3+0x5b0>
 650:	mov	x3, x9
 654:	ldp	x19, x20, [sp, #16]
 658:	ldp	x21, x22, [sp, #32]
 65c:	b	358 <__multf3+0x358>
 660:	mov	w1, #0x40                  	// #64
 664:	sub	w1, w1, w2
 668:	lsr	x3, x6, x2
 66c:	lsl	x6, x6, x1
 670:	cmp	x6, #0x0
 674:	lsl	x5, x4, x1
 678:	cset	x1, ne  // ne = any
 67c:	orr	x5, x5, x3
 680:	lsr	x2, x4, x2
 684:	orr	x5, x5, x1
 688:	tst	x5, #0x7
 68c:	b.ne	54c <__multf3+0x54c>  // b.any
 690:	tbnz	x2, #51, 7c4 <__multf3+0x7c4>
 694:	ubfx	x4, x2, #3, #48
 698:	extr	x5, x2, x5, #3
 69c:	mov	w1, #0x0                   	// #0
 6a0:	tbz	w11, #11, 170 <__multf3+0x170>
 6a4:	orr	w0, w0, #0x8
 6a8:	b	5b0 <__multf3+0x5b0>
 6ac:	mov	w1, #0x7fff                	// #32767
 6b0:	mov	x4, #0x0                   	// #0
 6b4:	b	644 <__multf3+0x644>
 6b8:	cmp	x8, #0x0
 6bc:	mov	w2, #0x7fff                	// #32767
 6c0:	mov	x4, #0xffffffffffff        	// #281474976710655
 6c4:	csel	w1, w1, w2, eq  // eq = none
 6c8:	csel	x4, x4, xzr, eq  // eq = none
 6cc:	csetm	x5, eq  // eq = none
 6d0:	b	644 <__multf3+0x644>
 6d4:	and	x1, x6, #0xf
 6d8:	cmp	x1, #0x4
 6dc:	b.eq	38c <__multf3+0x38c>  // b.none
 6e0:	adds	x6, x6, #0x4
 6e4:	cinc	x4, x4, cs  // cs = hs, nlast
 6e8:	b	38c <__multf3+0x38c>
 6ec:	cbz	x8, 38c <__multf3+0x38c>
 6f0:	adds	x6, x6, #0x8
 6f4:	cinc	x4, x4, cs  // cs = hs, nlast
 6f8:	b	38c <__multf3+0x38c>
 6fc:	and	x1, x5, #0xf
 700:	cmp	x1, #0x4
 704:	b.eq	710 <__multf3+0x710>  // b.none
 708:	adds	x5, x5, #0x4
 70c:	cinc	x2, x2, cs  // cs = hs, nlast
 710:	tbz	x2, #51, 56c <__multf3+0x56c>
 714:	orr	w0, w0, #0x8
 718:	mov	w1, #0x1                   	// #1
 71c:	mov	x4, #0x0                   	// #0
 720:	mov	x5, #0x0                   	// #0
 724:	b	5b0 <__multf3+0x5b0>
 728:	cmp	x8, #0x0
 72c:	mov	w2, #0x7fff                	// #32767
 730:	mov	x4, #0xffffffffffff        	// #281474976710655
 734:	csel	w1, w1, w2, ne  // ne = any
 738:	csel	x4, x4, xzr, ne  // ne = any
 73c:	csetm	x5, ne  // ne = any
 740:	b	644 <__multf3+0x644>
 744:	cbnz	x8, 38c <__multf3+0x38c>
 748:	b	6f0 <__multf3+0x6f0>
 74c:	mov	x4, #0x2                   	// #2
 750:	cmp	x1, #0xf
 754:	b.ne	778 <__multf3+0x778>  // b.any
 758:	tbz	x7, #47, 790 <__multf3+0x790>
 75c:	tbnz	x2, #47, 790 <__multf3+0x790>
 760:	orr	x4, x2, #0x800000000000
 764:	mov	w10, w15
 768:	and	x4, x4, #0xffffffffffff
 76c:	mov	x5, x6
 770:	mov	w1, #0x7fff                	// #32767
 774:	b	170 <__multf3+0x170>
 778:	cmp	x1, #0xb
 77c:	b.ne	c8 <__multf3+0xc8>  // b.any
 780:	mov	x7, x2
 784:	mov	x5, x6
 788:	mov	x16, x4
 78c:	b	d0 <__multf3+0xd0>
 790:	orr	x4, x7, #0x800000000000
 794:	mov	w10, w12
 798:	and	x4, x4, #0xffffffffffff
 79c:	mov	w1, #0x7fff                	// #32767
 7a0:	b	170 <__multf3+0x170>
 7a4:	cbnz	x8, 710 <__multf3+0x710>
 7a8:	adds	x5, x5, #0x8
 7ac:	cinc	x2, x2, cs  // cs = hs, nlast
 7b0:	tbnz	x2, #51, 714 <__multf3+0x714>
 7b4:	b	56c <__multf3+0x56c>
 7b8:	cbnz	x8, 7a8 <__multf3+0x7a8>
 7bc:	tbnz	x2, #51, 714 <__multf3+0x714>
 7c0:	b	56c <__multf3+0x56c>
 7c4:	orr	w0, w0, #0x10
 7c8:	b	714 <__multf3+0x714>
 7cc:	mov	x2, x4
 7d0:	mov	x4, #0x3                   	// #3
 7d4:	b	750 <__multf3+0x750>
 7d8:	mov	w10, w15
 7dc:	mov	x8, x13
 7e0:	b	ec <__multf3+0xec>

negtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negtf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x1, #0x0                   	// #0
   8:	str	q0, [sp]
   c:	ldp	x3, x2, [sp]
  10:	add	sp, sp, #0x10
  14:	mov	x0, x3
  18:	lsr	x3, x2, #48
  1c:	bfxil	x1, x2, #0, #48
  20:	fmov	d0, x0
  24:	eor	w2, w3, #0x8000
  28:	bfi	x1, x2, #48, #16
  2c:	fmov	v0.d[1], x1
  30:	ret

subtf3.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x5, x1, [sp, #16]
  14:	ldp	x0, x2, [sp, #32]
  18:	mrs	x12, fpcr
  1c:	mov	x9, x0
  20:	ubfx	x0, x2, #48, #15
  24:	lsr	x6, x1, #63
  28:	ubfx	x7, x1, #48, #15
  2c:	ubfiz	x3, x1, #3, #48
  30:	mov	x13, x0
  34:	lsr	x4, x2, #63
  38:	ubfiz	x2, x2, #3, #48
  3c:	mov	x11, x6
  40:	and	w8, w6, #0xff
  44:	mov	x14, x6
  48:	sub	w0, w7, w0
  4c:	mov	x1, x7
  50:	orr	x3, x3, x5, lsr #61
  54:	mov	x7, #0x7fff                	// #32767
  58:	and	w4, w4, #0xff
  5c:	cmp	x13, x7
  60:	orr	x2, x2, x9, lsr #61
  64:	lsl	x6, x5, #3
  68:	lsl	x10, x9, #3
  6c:	b.eq	1dc <__subtf3+0x1dc>  // b.none
  70:	eor	w4, w4, #0x1
  74:	and	x4, x4, #0xff
  78:	cmp	x11, x4
  7c:	b.eq	274 <__subtf3+0x274>  // b.none
  80:	cmp	w0, #0x0
  84:	b.le	1f8 <__subtf3+0x1f8>
  88:	cbnz	x13, 364 <__subtf3+0x364>
  8c:	orr	x4, x2, x10
  90:	cbz	x4, 544 <__subtf3+0x544>
  94:	subs	w0, w0, #0x1
  98:	b.eq	818 <__subtf3+0x818>  // b.none
  9c:	mov	x4, #0x7fff                	// #32767
  a0:	cmp	x1, x4
  a4:	b.eq	518 <__subtf3+0x518>  // b.none
  a8:	cmp	w0, #0x74
  ac:	b.gt	534 <__subtf3+0x534>
  b0:	cmp	w0, #0x3f
  b4:	b.gt	6dc <__subtf3+0x6dc>
  b8:	mov	w4, #0x40                  	// #64
  bc:	sub	w4, w4, w0
  c0:	lsr	x7, x10, x0
  c4:	lsl	x10, x10, x4
  c8:	cmp	x10, #0x0
  cc:	lsl	x4, x2, x4
  d0:	cset	x5, ne  // ne = any
  d4:	orr	x4, x4, x7
  d8:	lsr	x2, x2, x0
  dc:	orr	x4, x4, x5
  e0:	sub	x3, x3, x2
  e4:	subs	x6, x6, x4
  e8:	sbc	x3, x3, xzr
  ec:	and	x5, x3, #0x7ffffffffffff
  f0:	tbz	x3, #51, 2f0 <__subtf3+0x2f0>
  f4:	cbz	x5, 4fc <__subtf3+0x4fc>
  f8:	clz	x0, x5
  fc:	sub	w0, w0, #0xc
 100:	neg	w3, w0
 104:	lsl	x2, x5, x0
 108:	lsl	x5, x6, x0
 10c:	lsr	x6, x6, x3
 110:	orr	x3, x6, x2
 114:	cmp	x1, w0, sxtw
 118:	sxtw	x2, w0
 11c:	b.gt	4dc <__subtf3+0x4dc>
 120:	sub	w1, w0, w1
 124:	add	w0, w1, #0x1
 128:	cmp	w0, #0x3f
 12c:	b.gt	6a4 <__subtf3+0x6a4>
 130:	mov	w1, #0x40                  	// #64
 134:	sub	w1, w1, w0
 138:	lsr	x2, x5, x0
 13c:	lsl	x5, x5, x1
 140:	cmp	x5, #0x0
 144:	lsl	x6, x3, x1
 148:	cset	x1, ne  // ne = any
 14c:	orr	x6, x6, x2
 150:	lsr	x3, x3, x0
 154:	orr	x6, x6, x1
 158:	orr	x7, x6, x3
 15c:	cbz	x7, 304 <__subtf3+0x304>
 160:	and	x0, x6, #0x7
 164:	mov	x1, #0x0                   	// #0
 168:	mov	w4, #0x1                   	// #1
 16c:	cbz	x0, 568 <__subtf3+0x568>
 170:	and	x2, x12, #0xc00000
 174:	cmp	x2, #0x400, lsl #12
 178:	b.eq	4b4 <__subtf3+0x4b4>  // b.none
 17c:	cmp	x2, #0x800, lsl #12
 180:	b.eq	494 <__subtf3+0x494>  // b.none
 184:	cbz	x2, 4c0 <__subtf3+0x4c0>
 188:	and	x2, x3, #0x8000000000000
 18c:	mov	w0, #0x10                  	// #16
 190:	cbz	w4, 198 <__subtf3+0x198>
 194:	orr	w0, w0, #0x8
 198:	cbz	x2, 248 <__subtf3+0x248>
 19c:	add	x1, x1, #0x1
 1a0:	mov	x2, #0x7fff                	// #32767
 1a4:	cmp	x1, x2
 1a8:	b.eq	3bc <__subtf3+0x3bc>  // b.none
 1ac:	ubfx	x7, x3, #3, #48
 1b0:	extr	x5, x3, x6, #3
 1b4:	and	w1, w1, #0x7fff
 1b8:	mov	x3, #0x0                   	// #0
 1bc:	orr	w1, w1, w8, lsl #15
 1c0:	bfxil	x3, x7, #0, #48
 1c4:	fmov	d0, x5
 1c8:	bfi	x3, x1, #48, #16
 1cc:	fmov	v0.d[1], x3
 1d0:	cbnz	w0, 418 <__subtf3+0x418>
 1d4:	ldp	x29, x30, [sp], #48
 1d8:	ret
 1dc:	orr	x7, x2, x10
 1e0:	cbz	x7, 70 <__subtf3+0x70>
 1e4:	and	x4, x4, #0xff
 1e8:	cmp	x11, x4
 1ec:	b.eq	42c <__subtf3+0x42c>  // b.none
 1f0:	cmp	w0, #0x0
 1f4:	b.gt	364 <__subtf3+0x364>
 1f8:	cbz	w0, 31c <__subtf3+0x31c>
 1fc:	mov	w8, w4
 200:	cbnz	x1, 638 <__subtf3+0x638>
 204:	orr	x1, x3, x6
 208:	cbz	x1, 2d0 <__subtf3+0x2d0>
 20c:	cmn	w0, #0x1
 210:	b.eq	978 <__subtf3+0x978>  // b.none
 214:	mov	x1, #0x7fff                	// #32767
 218:	mvn	w0, w0
 21c:	cmp	x13, x1
 220:	b.ne	64c <__subtf3+0x64c>  // b.any
 224:	orr	x0, x2, x10
 228:	and	x11, x8, #0xff
 22c:	cbz	x0, 5b0 <__subtf3+0x5b0>
 230:	lsr	x0, x2, #50
 234:	mov	x6, x10
 238:	eor	x0, x0, #0x1
 23c:	mov	x3, x2
 240:	and	w0, w0, #0x1
 244:	mov	x1, #0x7fff                	// #32767
 248:	mov	x2, #0x7fff                	// #32767
 24c:	extr	x5, x3, x6, #3
 250:	lsr	x7, x3, #3
 254:	cmp	x1, x2
 258:	b.ne	310 <__subtf3+0x310>  // b.any
 25c:	orr	x1, x7, x5
 260:	cbz	x1, acc <__subtf3+0xacc>
 264:	orr	x7, x7, #0x800000000000
 268:	mov	w1, #0x7fff                	// #32767
 26c:	and	x7, x7, #0xffffffffffff
 270:	b	1b8 <__subtf3+0x1b8>
 274:	cmp	w0, #0x0
 278:	b.le	42c <__subtf3+0x42c>
 27c:	cbz	x13, 36c <__subtf3+0x36c>
 280:	orr	x2, x2, #0x8000000000000
 284:	mov	x4, #0x7fff                	// #32767
 288:	cmp	x1, x4
 28c:	b.eq	518 <__subtf3+0x518>  // b.none
 290:	cmp	w0, #0x74
 294:	b.gt	68c <__subtf3+0x68c>
 298:	cmp	w0, #0x3f
 29c:	b.gt	728 <__subtf3+0x728>
 2a0:	mov	w4, #0x40                  	// #64
 2a4:	sub	w4, w4, w0
 2a8:	lsr	x7, x10, x0
 2ac:	lsl	x10, x10, x4
 2b0:	cmp	x10, #0x0
 2b4:	lsl	x4, x2, x4
 2b8:	cset	x5, ne  // ne = any
 2bc:	orr	x4, x4, x7
 2c0:	lsr	x2, x2, x0
 2c4:	orr	x0, x4, x5
 2c8:	add	x3, x3, x2
 2cc:	b	698 <__subtf3+0x698>
 2d0:	mov	x0, #0x7fff                	// #32767
 2d4:	cmp	x13, x0
 2d8:	b.eq	904 <__subtf3+0x904>  // b.none
 2dc:	mov	x3, x2
 2e0:	mov	x6, x10
 2e4:	mov	x1, x13
 2e8:	mov	x14, x4
 2ec:	nop
 2f0:	orr	x7, x6, x3
 2f4:	and	x0, x6, #0x7
 2f8:	mov	w4, #0x0                   	// #0
 2fc:	cbnz	x1, 16c <__subtf3+0x16c>
 300:	cbnz	x7, 160 <__subtf3+0x160>
 304:	mov	x5, #0x0                   	// #0
 308:	mov	x1, #0x0                   	// #0
 30c:	mov	w0, #0x0                   	// #0
 310:	and	x7, x7, #0xffffffffffff
 314:	and	w1, w1, #0x7fff
 318:	b	1b8 <__subtf3+0x1b8>
 31c:	add	x7, x1, #0x1
 320:	tst	x7, #0x7ffe
 324:	b.ne	608 <__subtf3+0x608>  // b.any
 328:	orr	x11, x3, x6
 32c:	orr	x7, x2, x10
 330:	cbnz	x1, 798 <__subtf3+0x798>
 334:	cbz	x11, 824 <__subtf3+0x824>
 338:	cbz	x7, 838 <__subtf3+0x838>
 33c:	subs	x9, x6, x10
 340:	cmp	x6, x10
 344:	sbc	x5, x3, x2
 348:	tbz	x5, #51, 9e0 <__subtf3+0x9e0>
 34c:	subs	x6, x10, x6
 350:	mov	w8, w4
 354:	sbc	x3, x2, x3
 358:	mov	x14, x4
 35c:	orr	x7, x6, x3
 360:	b	15c <__subtf3+0x15c>
 364:	orr	x2, x2, #0x8000000000000
 368:	b	9c <__subtf3+0x9c>
 36c:	orr	x4, x2, x10
 370:	cbz	x4, 544 <__subtf3+0x544>
 374:	subs	w0, w0, #0x1
 378:	b.ne	284 <__subtf3+0x284>  // b.any
 37c:	adds	x6, x6, x10
 380:	adc	x3, x2, x3
 384:	nop
 388:	tbz	x3, #51, 2f0 <__subtf3+0x2f0>
 38c:	add	x1, x1, #0x1
 390:	mov	x0, #0x7fff                	// #32767
 394:	cmp	x1, x0
 398:	b.eq	844 <__subtf3+0x844>  // b.none
 39c:	and	x0, x6, #0x1
 3a0:	and	x2, x3, #0xfff7ffffffffffff
 3a4:	orr	x6, x0, x6, lsr #1
 3a8:	mov	w4, #0x0                   	// #0
 3ac:	orr	x6, x6, x3, lsl #63
 3b0:	lsr	x3, x2, #1
 3b4:	and	x0, x6, #0x7
 3b8:	b	16c <__subtf3+0x16c>
 3bc:	and	x2, x12, #0xc00000
 3c0:	cbz	x2, 3f8 <__subtf3+0x3f8>
 3c4:	cmp	x2, #0x400, lsl #12
 3c8:	b.eq	3f4 <__subtf3+0x3f4>  // b.none
 3cc:	cmp	x2, #0x800, lsl #12
 3d0:	and	w14, w14, #0x1
 3d4:	csel	w14, w14, wzr, eq  // eq = none
 3d8:	cbnz	w14, 3f8 <__subtf3+0x3f8>
 3dc:	mov	w1, #0x14                  	// #20
 3e0:	mov	x5, #0xffffffffffffffff    	// #-1
 3e4:	orr	w0, w0, w1
 3e8:	mov	x7, #0x1fffffffffffffff    	// #2305843009213693951
 3ec:	mov	x1, #0x7ffe                	// #32766
 3f0:	b	310 <__subtf3+0x310>
 3f4:	cbnz	x14, 3dc <__subtf3+0x3dc>
 3f8:	mov	w1, #0x14                  	// #20
 3fc:	and	x11, x8, #0xff
 400:	orr	w0, w0, w1
 404:	mov	x2, #0x0                   	// #0
 408:	fmov	d0, x2
 40c:	lsl	x11, x11, #63
 410:	orr	x3, x11, #0x7fff000000000000
 414:	fmov	v0.d[1], x3
 418:	str	q0, [sp, #16]
 41c:	bl	0 <__sfp_handle_exceptions>
 420:	ldr	q0, [sp, #16]
 424:	ldp	x29, x30, [sp], #48
 428:	ret
 42c:	cbz	w0, 5c8 <__subtf3+0x5c8>
 430:	cbz	x1, 584 <__subtf3+0x584>
 434:	mov	x1, #0x7fff                	// #32767
 438:	neg	w0, w0
 43c:	orr	x3, x3, #0x8000000000000
 440:	cmp	x13, x1
 444:	b.eq	5a4 <__subtf3+0x5a4>  // b.none
 448:	cmp	w0, #0x74
 44c:	b.gt	898 <__subtf3+0x898>
 450:	cmp	w0, #0x3f
 454:	b.gt	94c <__subtf3+0x94c>
 458:	mov	w1, #0x40                  	// #64
 45c:	sub	w1, w1, w0
 460:	lsr	x4, x6, x0
 464:	lsl	x6, x6, x1
 468:	cmp	x6, #0x0
 46c:	lsl	x6, x3, x1
 470:	cset	x1, ne  // ne = any
 474:	orr	x6, x6, x4
 478:	lsr	x0, x3, x0
 47c:	orr	x6, x6, x1
 480:	add	x2, x2, x0
 484:	adds	x6, x6, x10
 488:	mov	x1, x13
 48c:	cinc	x3, x2, cs  // cs = hs, nlast
 490:	b	388 <__subtf3+0x388>
 494:	mov	w0, #0x10                  	// #16
 498:	cbz	x14, 4a4 <__subtf3+0x4a4>
 49c:	adds	x6, x6, #0x8
 4a0:	cinc	x3, x3, cs  // cs = hs, nlast
 4a4:	and	x2, x3, #0x8000000000000
 4a8:	cbz	w4, 198 <__subtf3+0x198>
 4ac:	orr	w0, w0, #0x8
 4b0:	b	198 <__subtf3+0x198>
 4b4:	mov	w0, #0x10                  	// #16
 4b8:	cbnz	x14, 4a4 <__subtf3+0x4a4>
 4bc:	b	49c <__subtf3+0x49c>
 4c0:	and	x2, x6, #0xf
 4c4:	mov	w0, #0x10                  	// #16
 4c8:	cmp	x2, #0x4
 4cc:	b.eq	4a4 <__subtf3+0x4a4>  // b.none
 4d0:	adds	x6, x6, #0x4
 4d4:	cinc	x3, x3, cs  // cs = hs, nlast
 4d8:	b	4a4 <__subtf3+0x4a4>
 4dc:	mov	x6, x5
 4e0:	and	x3, x3, #0xfff7ffffffffffff
 4e4:	sub	x1, x1, x2
 4e8:	orr	x7, x6, x3
 4ec:	and	x0, x6, #0x7
 4f0:	mov	w4, #0x0                   	// #0
 4f4:	cbz	x1, 300 <__subtf3+0x300>
 4f8:	b	16c <__subtf3+0x16c>
 4fc:	clz	x3, x6
 500:	add	w0, w3, #0x34
 504:	cmp	w0, #0x3f
 508:	b.le	100 <__subtf3+0x100>
 50c:	sub	w3, w3, #0xc
 510:	lsl	x3, x6, x3
 514:	b	114 <__subtf3+0x114>
 518:	orr	x0, x3, x6
 51c:	cbz	x0, 5b0 <__subtf3+0x5b0>
 520:	lsr	x0, x3, #50
 524:	mov	x1, #0x7fff                	// #32767
 528:	eor	x0, x0, #0x1
 52c:	and	w0, w0, #0x1
 530:	b	248 <__subtf3+0x248>
 534:	orr	x2, x2, x10
 538:	cmp	x2, #0x0
 53c:	cset	x4, ne  // ne = any
 540:	b	e4 <__subtf3+0xe4>
 544:	mov	x0, #0x7fff                	// #32767
 548:	cmp	x1, x0
 54c:	b.ne	2f0 <__subtf3+0x2f0>  // b.any
 550:	orr	x0, x3, x6
 554:	cbnz	x0, 520 <__subtf3+0x520>
 558:	mov	x5, #0x0                   	// #0
 55c:	mov	x7, #0x0                   	// #0
 560:	mov	w0, #0x0                   	// #0
 564:	b	25c <__subtf3+0x25c>
 568:	and	x2, x3, #0x8000000000000
 56c:	mov	w0, #0x0                   	// #0
 570:	cbz	w4, 198 <__subtf3+0x198>
 574:	mov	w0, #0x0                   	// #0
 578:	tbz	w12, #11, 198 <__subtf3+0x198>
 57c:	orr	w0, w0, #0x8
 580:	b	198 <__subtf3+0x198>
 584:	orr	x1, x3, x6
 588:	cbz	x1, 910 <__subtf3+0x910>
 58c:	cmn	w0, #0x1
 590:	b.eq	a60 <__subtf3+0xa60>  // b.none
 594:	mov	x1, #0x7fff                	// #32767
 598:	mvn	w0, w0
 59c:	cmp	x13, x1
 5a0:	b.ne	448 <__subtf3+0x448>  // b.any
 5a4:	orr	x0, x2, x10
 5a8:	cbnz	x0, 230 <__subtf3+0x230>
 5ac:	nop
 5b0:	mov	x2, #0x0                   	// #0
 5b4:	fmov	d0, x2
 5b8:	lsl	x0, x11, #63
 5bc:	orr	x3, x0, #0x7fff000000000000
 5c0:	fmov	v0.d[1], x3
 5c4:	b	1d4 <__subtf3+0x1d4>
 5c8:	add	x7, x1, #0x1
 5cc:	tst	x7, #0x7ffe
 5d0:	b.ne	754 <__subtf3+0x754>  // b.any
 5d4:	orr	x11, x3, x6
 5d8:	cbnz	x1, 8dc <__subtf3+0x8dc>
 5dc:	orr	x7, x2, x10
 5e0:	cbz	x11, 940 <__subtf3+0x940>
 5e4:	cbz	x7, 838 <__subtf3+0x838>
 5e8:	adds	x6, x6, x10
 5ec:	adc	x3, x2, x3
 5f0:	tbz	x3, #51, 158 <__subtf3+0x158>
 5f4:	and	x3, x3, #0xfff7ffffffffffff
 5f8:	and	x0, x6, #0x7
 5fc:	mov	w4, #0x0                   	// #0
 600:	mov	x1, #0x1                   	// #1
 604:	b	16c <__subtf3+0x16c>
 608:	subs	x9, x6, x10
 60c:	cmp	x6, x10
 610:	sbc	x5, x3, x2
 614:	tbnz	x5, #51, 780 <__subtf3+0x780>
 618:	orr	x7, x9, x5
 61c:	cbnz	x7, 890 <__subtf3+0x890>
 620:	and	x12, x12, #0xc00000
 624:	mov	x5, #0x0                   	// #0
 628:	cmp	x12, #0x800, lsl #12
 62c:	mov	x1, #0x0                   	// #0
 630:	cset	w8, eq  // eq = none
 634:	b	310 <__subtf3+0x310>
 638:	mov	x1, #0x7fff                	// #32767
 63c:	neg	w0, w0
 640:	orr	x3, x3, #0x8000000000000
 644:	cmp	x13, x1
 648:	b.eq	224 <__subtf3+0x224>  // b.none
 64c:	cmp	w0, #0x74
 650:	b.gt	708 <__subtf3+0x708>
 654:	cmp	w0, #0x3f
 658:	b.gt	8a8 <__subtf3+0x8a8>
 65c:	mov	w1, #0x40                  	// #64
 660:	sub	w1, w1, w0
 664:	lsr	x5, x6, x0
 668:	lsl	x6, x6, x1
 66c:	cmp	x6, #0x0
 670:	lsl	x6, x3, x1
 674:	cset	x1, ne  // ne = any
 678:	orr	x6, x6, x5
 67c:	lsr	x0, x3, x0
 680:	orr	x6, x6, x1
 684:	sub	x2, x2, x0
 688:	b	714 <__subtf3+0x714>
 68c:	orr	x2, x2, x10
 690:	cmp	x2, #0x0
 694:	cset	x0, ne  // ne = any
 698:	adds	x6, x0, x6
 69c:	cinc	x3, x3, cs  // cs = hs, nlast
 6a0:	b	388 <__subtf3+0x388>
 6a4:	mov	w2, #0x80                  	// #128
 6a8:	sub	w2, w2, w0
 6ac:	cmp	w0, #0x40
 6b0:	sub	w6, w1, #0x3f
 6b4:	lsl	x0, x3, x2
 6b8:	orr	x0, x5, x0
 6bc:	csel	x5, x0, x5, ne  // ne = any
 6c0:	lsr	x6, x3, x6
 6c4:	cmp	x5, #0x0
 6c8:	mov	x3, #0x0                   	// #0
 6cc:	cset	x0, ne  // ne = any
 6d0:	orr	x6, x0, x6
 6d4:	mov	x7, x6
 6d8:	b	15c <__subtf3+0x15c>
 6dc:	mov	w5, #0x80                  	// #128
 6e0:	sub	w5, w5, w0
 6e4:	subs	w0, w0, #0x40
 6e8:	lsl	x5, x2, x5
 6ec:	orr	x5, x10, x5
 6f0:	csel	x10, x5, x10, ne  // ne = any
 6f4:	lsr	x2, x2, x0
 6f8:	cmp	x10, #0x0
 6fc:	cset	x4, ne  // ne = any
 700:	orr	x4, x4, x2
 704:	b	e4 <__subtf3+0xe4>
 708:	orr	x3, x3, x6
 70c:	cmp	x3, #0x0
 710:	cset	x6, ne  // ne = any
 714:	subs	x6, x10, x6
 718:	mov	x1, x13
 71c:	sbc	x3, x2, xzr
 720:	mov	x14, x4
 724:	b	ec <__subtf3+0xec>
 728:	mov	w4, #0x80                  	// #128
 72c:	sub	w4, w4, w0
 730:	subs	w0, w0, #0x40
 734:	lsl	x4, x2, x4
 738:	orr	x4, x10, x4
 73c:	csel	x10, x4, x10, ne  // ne = any
 740:	lsr	x2, x2, x0
 744:	cmp	x10, #0x0
 748:	cset	x0, ne  // ne = any
 74c:	orr	x0, x0, x2
 750:	b	698 <__subtf3+0x698>
 754:	mov	x0, #0x7fff                	// #32767
 758:	cmp	x7, x0
 75c:	b.eq	990 <__subtf3+0x990>  // b.none
 760:	adds	x6, x6, x10
 764:	mov	x1, x7
 768:	adc	x3, x2, x3
 76c:	mov	w4, #0x0                   	// #0
 770:	ubfx	x0, x6, #1, #3
 774:	extr	x6, x3, x6, #1
 778:	lsr	x3, x3, #1
 77c:	b	16c <__subtf3+0x16c>
 780:	cmp	x10, x6
 784:	mov	w8, w4
 788:	sbc	x5, x2, x3
 78c:	sub	x6, x10, x6
 790:	mov	x14, x4
 794:	b	f4 <__subtf3+0xf4>
 798:	mov	x12, #0x7fff                	// #32767
 79c:	cmp	x1, x12
 7a0:	b.eq	7c8 <__subtf3+0x7c8>  // b.none
 7a4:	cmp	x13, x12
 7a8:	b.eq	9bc <__subtf3+0x9bc>  // b.none
 7ac:	cbnz	x11, 7e0 <__subtf3+0x7e0>
 7b0:	cbnz	x7, 9cc <__subtf3+0x9cc>
 7b4:	mov	x5, #0xffffffffffffffff    	// #-1
 7b8:	mov	x7, #0xffffffffffff        	// #281474976710655
 7bc:	mov	w0, #0x1                   	// #1
 7c0:	mov	w8, #0x0                   	// #0
 7c4:	b	264 <__subtf3+0x264>
 7c8:	cbz	x11, 9b4 <__subtf3+0x9b4>
 7cc:	lsr	x0, x3, #50
 7d0:	cmp	x13, x1
 7d4:	eor	x0, x0, #0x1
 7d8:	and	w0, w0, #0x1
 7dc:	b.eq	9bc <__subtf3+0x9bc>  // b.none
 7e0:	cbz	x7, 244 <__subtf3+0x244>
 7e4:	bfi	x5, x3, #61, #3
 7e8:	lsr	x7, x3, #3
 7ec:	tbz	x3, #50, 808 <__subtf3+0x808>
 7f0:	lsr	x1, x2, #3
 7f4:	tbnz	x2, #50, 808 <__subtf3+0x808>
 7f8:	mov	x5, x9
 7fc:	mov	w8, w4
 800:	bfi	x5, x2, #61, #3
 804:	mov	x7, x1
 808:	extr	x7, x7, x5, #61
 80c:	bfi	x5, x7, #61, #3
 810:	lsr	x7, x7, #3
 814:	b	25c <__subtf3+0x25c>
 818:	subs	x6, x6, x10
 81c:	sbc	x3, x3, x2
 820:	b	ec <__subtf3+0xec>
 824:	cbz	x7, 92c <__subtf3+0x92c>
 828:	mov	x3, x2
 82c:	mov	x6, x10
 830:	mov	w8, w4
 834:	mov	x14, x4
 838:	mov	x1, #0x0                   	// #0
 83c:	mov	x2, #0x0                   	// #0
 840:	b	574 <__subtf3+0x574>
 844:	ands	x2, x12, #0xc00000
 848:	b.eq	8d4 <__subtf3+0x8d4>  // b.none
 84c:	cmp	x2, #0x400, lsl #12
 850:	eor	w0, w8, #0x1
 854:	cset	w1, eq  // eq = none
 858:	tst	w1, w0
 85c:	b.ne	9a8 <__subtf3+0x9a8>  // b.any
 860:	cmp	x2, #0x800, lsl #12
 864:	b.eq	9fc <__subtf3+0x9fc>  // b.none
 868:	cmp	x2, #0x400, lsl #12
 86c:	mov	w0, #0x14                  	// #20
 870:	b.ne	3c0 <__subtf3+0x3c0>  // b.any
 874:	mov	x3, #0xffffffffffffffff    	// #-1
 878:	mov	x1, #0x7ffe                	// #32766
 87c:	mov	x6, x3
 880:	mov	w4, #0x0                   	// #0
 884:	mov	w0, #0x14                  	// #20
 888:	cbnz	x14, 4a4 <__subtf3+0x4a4>
 88c:	b	49c <__subtf3+0x49c>
 890:	mov	x6, x9
 894:	b	f4 <__subtf3+0xf4>
 898:	orr	x3, x3, x6
 89c:	cmp	x3, #0x0
 8a0:	cset	x6, ne  // ne = any
 8a4:	b	484 <__subtf3+0x484>
 8a8:	mov	w1, #0x80                  	// #128
 8ac:	sub	w1, w1, w0
 8b0:	subs	w0, w0, #0x40
 8b4:	lsl	x1, x3, x1
 8b8:	orr	x1, x6, x1
 8bc:	csel	x6, x1, x6, ne  // ne = any
 8c0:	lsr	x3, x3, x0
 8c4:	cmp	x6, #0x0
 8c8:	cset	x6, ne  // ne = any
 8cc:	orr	x6, x6, x3
 8d0:	b	714 <__subtf3+0x714>
 8d4:	mov	w0, #0x14                  	// #20
 8d8:	b	404 <__subtf3+0x404>
 8dc:	mov	x7, #0x7fff                	// #32767
 8e0:	cmp	x1, x7
 8e4:	b.eq	a18 <__subtf3+0xa18>  // b.none
 8e8:	cmp	x13, x7
 8ec:	b.eq	a7c <__subtf3+0xa7c>  // b.none
 8f0:	cbnz	x11, a30 <__subtf3+0xa30>
 8f4:	mov	x3, x2
 8f8:	mov	x6, x10
 8fc:	mov	x1, #0x7fff                	// #32767
 900:	b	248 <__subtf3+0x248>
 904:	orr	x0, x2, x10
 908:	cbz	x0, 558 <__subtf3+0x558>
 90c:	b	230 <__subtf3+0x230>
 910:	mov	x0, #0x7fff                	// #32767
 914:	cmp	x13, x0
 918:	b.eq	904 <__subtf3+0x904>  // b.none
 91c:	mov	x3, x2
 920:	mov	x6, x10
 924:	mov	x1, x13
 928:	b	2f0 <__subtf3+0x2f0>
 92c:	and	x12, x12, #0xc00000
 930:	mov	x5, #0x0                   	// #0
 934:	cmp	x12, #0x800, lsl #12
 938:	cset	w8, eq  // eq = none
 93c:	b	310 <__subtf3+0x310>
 940:	mov	x3, x2
 944:	mov	x6, x10
 948:	b	15c <__subtf3+0x15c>
 94c:	mov	w1, #0x80                  	// #128
 950:	sub	w1, w1, w0
 954:	subs	w0, w0, #0x40
 958:	lsl	x1, x3, x1
 95c:	orr	x1, x6, x1
 960:	csel	x6, x1, x6, ne  // ne = any
 964:	lsr	x3, x3, x0
 968:	cmp	x6, #0x0
 96c:	cset	x6, ne  // ne = any
 970:	orr	x6, x6, x3
 974:	b	484 <__subtf3+0x484>
 978:	cmp	x10, x6
 97c:	mov	x1, x13
 980:	sbc	x3, x2, x3
 984:	sub	x6, x10, x6
 988:	mov	x14, x4
 98c:	b	ec <__subtf3+0xec>
 990:	ands	x2, x12, #0xc00000
 994:	b.eq	8d4 <__subtf3+0x8d4>  // b.none
 998:	cmp	x2, #0x400, lsl #12
 99c:	eor	w0, w8, #0x1
 9a0:	csel	w0, w0, wzr, eq  // eq = none
 9a4:	cbz	w0, 860 <__subtf3+0x860>
 9a8:	mov	w0, #0x14                  	// #20
 9ac:	mov	x11, #0x0                   	// #0
 9b0:	b	404 <__subtf3+0x404>
 9b4:	cmp	x13, x1
 9b8:	b.ne	7b0 <__subtf3+0x7b0>  // b.any
 9bc:	cbz	x7, a70 <__subtf3+0xa70>
 9c0:	tst	x2, #0x4000000000000
 9c4:	csinc	w0, w0, wzr, ne  // ne = any
 9c8:	cbnz	x11, 7e4 <__subtf3+0x7e4>
 9cc:	mov	w8, w4
 9d0:	mov	x3, x2
 9d4:	mov	x6, x10
 9d8:	mov	x1, #0x7fff                	// #32767
 9dc:	b	248 <__subtf3+0x248>
 9e0:	orr	x7, x9, x5
 9e4:	cbz	x7, 92c <__subtf3+0x92c>
 9e8:	mov	x3, x5
 9ec:	and	x0, x9, #0x7
 9f0:	mov	x6, x9
 9f4:	mov	w4, #0x1                   	// #1
 9f8:	b	16c <__subtf3+0x16c>
 9fc:	cbnz	x11, a90 <__subtf3+0xa90>
 a00:	mov	x3, #0xffffffffffffffff    	// #-1
 a04:	mov	w8, #0x0                   	// #0
 a08:	mov	x6, x3
 a0c:	mov	x1, #0x7ffe                	// #32766
 a10:	mov	w0, #0x14                  	// #20
 a14:	b	19c <__subtf3+0x19c>
 a18:	cbz	x11, a9c <__subtf3+0xa9c>
 a1c:	lsr	x0, x3, #50
 a20:	cmp	x13, x1
 a24:	eor	x0, x0, #0x1
 a28:	and	w0, w0, #0x1
 a2c:	b.eq	abc <__subtf3+0xabc>  // b.none
 a30:	orr	x10, x2, x10
 a34:	cbz	x10, 244 <__subtf3+0x244>
 a38:	bfi	x5, x3, #61, #3
 a3c:	lsr	x7, x3, #3
 a40:	tbz	x3, #50, 808 <__subtf3+0x808>
 a44:	lsr	x1, x2, #3
 a48:	tbnz	x2, #50, 808 <__subtf3+0x808>
 a4c:	and	x5, x9, #0x1fffffffffffffff
 a50:	mov	w8, w4
 a54:	orr	x5, x5, x2, lsl #61
 a58:	mov	x7, x1
 a5c:	b	808 <__subtf3+0x808>
 a60:	adds	x6, x6, x10
 a64:	mov	x1, x13
 a68:	adc	x3, x2, x3
 a6c:	b	388 <__subtf3+0x388>
 a70:	cbz	x11, 7b4 <__subtf3+0x7b4>
 a74:	mov	x1, #0x7fff                	// #32767
 a78:	b	248 <__subtf3+0x248>
 a7c:	orr	x1, x2, x10
 a80:	cbnz	x1, aac <__subtf3+0xaac>
 a84:	cbz	x11, 558 <__subtf3+0x558>
 a88:	mov	x1, #0x7fff                	// #32767
 a8c:	b	248 <__subtf3+0x248>
 a90:	mov	w0, #0x14                  	// #20
 a94:	mov	x11, #0x1                   	// #1
 a98:	b	404 <__subtf3+0x404>
 a9c:	cmp	x13, x1
 aa0:	b.ne	8f4 <__subtf3+0x8f4>  // b.any
 aa4:	orr	x1, x2, x10
 aa8:	cbz	x1, 558 <__subtf3+0x558>
 aac:	tst	x2, #0x4000000000000
 ab0:	csinc	w0, w0, wzr, ne  // ne = any
 ab4:	cbnz	x11, a38 <__subtf3+0xa38>
 ab8:	b	8f4 <__subtf3+0x8f4>
 abc:	orr	x1, x2, x10
 ac0:	cbnz	x1, aac <__subtf3+0xaac>
 ac4:	mov	x1, #0x7fff                	// #32767
 ac8:	b	248 <__subtf3+0x248>
 acc:	mov	x5, #0x0                   	// #0
 ad0:	mov	w1, #0x7fff                	// #32767
 ad4:	mov	x7, #0x0                   	// #0
 ad8:	b	1b8 <__subtf3+0x1b8>

unordtf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__unordtf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #16]
   c:	str	q1, [sp, #32]
  10:	ldp	x3, x1, [sp, #16]
  14:	ldp	x2, x0, [sp, #32]
  18:	mrs	x4, fpcr
  1c:	ubfx	x5, x1, #48, #15
  20:	mov	x4, x3
  24:	mov	x3, x2
  28:	ubfx	x2, x1, #0, #48
  2c:	mov	x1, #0x7fff                	// #32767
  30:	ubfx	x7, x0, #0, #48
  34:	cmp	x5, x1
  38:	ubfx	x1, x0, #48, #15
  3c:	b.eq	58 <__unordtf2+0x58>  // b.none
  40:	mov	x6, #0x7fff                	// #32767
  44:	mov	w0, #0x0                   	// #0
  48:	cmp	x1, x6
  4c:	b.eq	7c <__unordtf2+0x7c>  // b.none
  50:	ldp	x29, x30, [sp], #48
  54:	ret
  58:	orr	x0, x4, x2
  5c:	cbz	x0, 40 <__unordtf2+0x40>
  60:	tst	x2, #0x800000000000
  64:	b.ne	9c <__unordtf2+0x9c>  // b.any
  68:	mov	w0, #0x1                   	// #1
  6c:	bl	0 <__sfp_handle_exceptions>
  70:	mov	w0, #0x1                   	// #1
  74:	ldp	x29, x30, [sp], #48
  78:	ret
  7c:	orr	x3, x7, x3
  80:	cbz	x3, 50 <__unordtf2+0x50>
  84:	cmp	x5, x1
  88:	b.eq	b0 <__unordtf2+0xb0>  // b.none
  8c:	tst	x7, #0x800000000000
  90:	mov	w0, #0x1                   	// #1
  94:	b.ne	50 <__unordtf2+0x50>  // b.any
  98:	b	68 <__unordtf2+0x68>
  9c:	cmp	x1, x5
  a0:	mov	w0, #0x1                   	// #1
  a4:	b.ne	50 <__unordtf2+0x50>  // b.any
  a8:	orr	x3, x7, x3
  ac:	b	c0 <__unordtf2+0xc0>
  b0:	orr	x4, x4, x2
  b4:	cbz	x4, 8c <__unordtf2+0x8c>
  b8:	tst	x2, #0x800000000000
  bc:	b.eq	68 <__unordtf2+0x68>  // b.none
  c0:	mov	w0, #0x1                   	// #1
  c4:	cbz	x3, 50 <__unordtf2+0x50>
  c8:	b	8c <__unordtf2+0x8c>

fixtfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	mrs	x2, fpcr
  18:	mov	x3, x0
  1c:	ubfx	x4, x1, #48, #15
  20:	mov	x0, #0x3ffe                	// #16382
  24:	cmp	x4, x0
  28:	ubfx	x0, x1, #0, #48
  2c:	b.gt	58 <__fixtfsi+0x58>
  30:	cbnz	x4, d4 <__fixtfsi+0xd4>
  34:	orr	x0, x0, x3
  38:	mov	w19, #0x0                   	// #0
  3c:	cbz	x0, 48 <__fixtfsi+0x48>
  40:	mov	w0, #0x10                  	// #16
  44:	bl	0 <__sfp_handle_exceptions>
  48:	mov	w0, w19
  4c:	ldr	x19, [sp, #16]
  50:	ldp	x29, x30, [sp], #48
  54:	ret
  58:	lsr	x19, x1, #63
  5c:	mov	x5, #0x401d                	// #16413
  60:	and	w2, w19, #0xff
  64:	cmp	x4, x5
  68:	b.le	9c <__fixtfsi+0x9c>
  6c:	mov	x1, #0x401e                	// #16414
  70:	cmp	x4, x1
  74:	mov	w19, #0x7fffffff            	// #2147483647
  78:	csel	w1, w2, wzr, eq  // eq = none
  7c:	add	w19, w2, w19
  80:	cbz	w1, e4 <__fixtfsi+0xe4>
  84:	cmp	xzr, x0, lsr #17
  88:	b.ne	e4 <__fixtfsi+0xe4>  // b.any
  8c:	orr	x0, x3, x0, lsl #47
  90:	cbz	x0, 48 <__fixtfsi+0x48>
  94:	mov	w0, #0x10                  	// #16
  98:	b	44 <__fixtfsi+0x44>
  9c:	mov	x1, x4
  a0:	orr	x0, x0, #0x1000000000000
  a4:	mov	w19, #0x402f                	// #16431
  a8:	mov	w4, #0xffffc011            	// #-16367
  ac:	add	w4, w1, w4
  b0:	sub	w1, w19, w1
  b4:	cmp	w2, #0x0
  b8:	lsr	x19, x0, x1
  bc:	cneg	w19, w19, ne  // ne = any
  c0:	lsl	x0, x0, x4
  c4:	orr	x0, x0, x3
  c8:	cbz	x0, 48 <__fixtfsi+0x48>
  cc:	mov	w0, #0x10                  	// #16
  d0:	b	44 <__fixtfsi+0x44>
  d4:	mov	w19, #0x0                   	// #0
  d8:	mov	w0, #0x10                  	// #16
  dc:	bl	0 <__sfp_handle_exceptions>
  e0:	b	48 <__fixtfsi+0x48>
  e4:	mov	w0, #0x1                   	// #1
  e8:	bl	0 <__sfp_handle_exceptions>
  ec:	b	48 <__fixtfsi+0x48>

fixunstfsi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x3, x1, #48, #15
  1c:	mov	x4, #0x3ffe                	// #16382
  20:	mov	x2, x0
  24:	cmp	x3, x4
  28:	ubfx	x19, x1, #0, #48
  2c:	b.gt	6c <__fixunstfsi+0x6c>
  30:	cbnz	x3, 50 <__fixunstfsi+0x50>
  34:	orr	x0, x0, x19
  38:	mov	w19, #0x0                   	// #0
  3c:	cbnz	x0, 50 <__fixunstfsi+0x50>
  40:	mov	w0, w19
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	mov	w0, #0x10                  	// #16
  54:	mov	w19, #0x0                   	// #0
  58:	bl	0 <__sfp_handle_exceptions>
  5c:	mov	w0, w19
  60:	ldr	x19, [sp, #16]
  64:	ldp	x29, x30, [sp], #48
  68:	ret
  6c:	lsr	x0, x1, #63
  70:	mov	x4, #0x401f                	// #16415
  74:	and	w0, w0, #0xff
  78:	mov	x5, #0x401e                	// #16414
  7c:	ands	x6, x0, #0xff
  80:	csel	x4, x4, x5, eq  // eq = none
  84:	cmp	x4, x3
  88:	b.le	a0 <__fixunstfsi+0xa0>
  8c:	cbz	x6, b0 <__fixunstfsi+0xb0>
  90:	mov	w0, #0x1                   	// #1
  94:	mov	w19, #0x0                   	// #0
  98:	bl	0 <__sfp_handle_exceptions>
  9c:	b	5c <__fixunstfsi+0x5c>
  a0:	sub	w19, w0, #0x1
  a4:	mov	w0, #0x1                   	// #1
  a8:	bl	0 <__sfp_handle_exceptions>
  ac:	b	5c <__fixunstfsi+0x5c>
  b0:	mov	x1, x3
  b4:	orr	x0, x19, #0x1000000000000
  b8:	mov	w3, #0xffffc011            	// #-16367
  bc:	mov	w19, #0x402f                	// #16431
  c0:	add	w3, w1, w3
  c4:	sub	w1, w19, w1
  c8:	lsl	x3, x0, x3
  cc:	orr	x2, x3, x2
  d0:	lsr	x19, x0, x1
  d4:	mov	w0, #0x10                  	// #16
  d8:	cbnz	x2, 58 <__fixunstfsi+0x58>
  dc:	b	40 <__fixunstfsi+0x40>

floatsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	cmp	w0, #0x0
   4:	cbz	w0, 50 <__floatsitf+0x50>
   8:	cneg	w1, w0, lt  // lt = tstop
   c:	mov	w4, #0x403e                	// #16446
  10:	clz	x3, x1
  14:	mov	w2, #0x402f                	// #16431
  18:	sub	w4, w4, w3
  1c:	lsr	w0, w0, #31
  20:	sub	w2, w2, w4
  24:	mov	x3, #0x0                   	// #0
  28:	and	w4, w4, #0x7fff
  2c:	lsl	x1, x1, x2
  30:	and	x1, x1, #0xffffffffffff
  34:	orr	w0, w4, w0, lsl #15
  38:	mov	x2, #0x0                   	// #0
  3c:	bfxil	x3, x1, #0, #48
  40:	fmov	d0, x2
  44:	bfi	x3, x0, #48, #16
  48:	fmov	v0.d[1], x3
  4c:	ret
  50:	mov	w4, #0x0                   	// #0
  54:	mov	x1, #0x0                   	// #0
  58:	mov	w0, #0x0                   	// #0
  5c:	mov	x3, #0x0                   	// #0
  60:	orr	w0, w4, w0, lsl #15
  64:	bfxil	x3, x1, #0, #48
  68:	mov	x2, #0x0                   	// #0
  6c:	fmov	d0, x2
  70:	bfi	x3, x0, #48, #16
  74:	fmov	v0.d[1], x3
  78:	ret

floatunsitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	cbz	w0, 44 <__floatunsitf+0x44>
   4:	mov	w0, w0
   8:	mov	w1, #0x403e                	// #16446
   c:	clz	x3, x0
  10:	mov	w2, #0x402f                	// #16431
  14:	sub	w1, w1, w3
  18:	mov	x3, #0x0                   	// #0
  1c:	sub	w2, w2, w1
  20:	and	w1, w1, #0x7fff
  24:	lsl	x0, x0, x2
  28:	and	x0, x0, #0xffffffffffff
  2c:	mov	x2, #0x0                   	// #0
  30:	fmov	d0, x2
  34:	bfxil	x3, x0, #0, #48
  38:	bfi	x3, x1, #48, #16
  3c:	fmov	v0.d[1], x3
  40:	ret
  44:	mov	x0, #0x0                   	// #0
  48:	mov	x3, #0x0                   	// #0
  4c:	bfxil	x3, x0, #0, #48
  50:	mov	x2, #0x0                   	// #0
  54:	fmov	d0, x2
  58:	mov	w1, #0x0                   	// #0
  5c:	bfi	x3, x1, #48, #16
  60:	fmov	v0.d[1], x3
  64:	ret

fixtfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x2, x0, [sp, #32]
  14:	mrs	x1, fpcr
  18:	ubfx	x19, x0, #48, #15
  1c:	mov	x3, #0x3ffe                	// #16382
  20:	ubfx	x4, x0, #0, #48
  24:	cmp	x19, x3
  28:	b.gt	50 <__fixtfdi+0x50>
  2c:	cbnz	x19, f0 <__fixtfdi+0xf0>
  30:	orr	x2, x4, x2
  34:	cbz	x2, 40 <__fixtfdi+0x40>
  38:	mov	w0, #0x10                  	// #16
  3c:	bl	0 <__sfp_handle_exceptions>
  40:	mov	x0, x19
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	lsr	x1, x0, #63
  54:	and	w3, w1, #0xff
  58:	mov	x1, #0x403d                	// #16445
  5c:	cmp	x19, x1
  60:	b.le	98 <__fixtfdi+0x98>
  64:	mov	x0, #0x403e                	// #16446
  68:	and	x5, x3, #0xff
  6c:	cmp	x19, x0
  70:	mov	x0, #0x7fffffffffffffff    	// #9223372036854775807
  74:	csel	w1, w3, wzr, eq  // eq = none
  78:	add	x19, x5, x0
  7c:	cbz	w1, 100 <__fixtfdi+0x100>
  80:	extr	x4, x4, x2, #49
  84:	cbnz	x4, 100 <__fixtfdi+0x100>
  88:	cmp	xzr, x2, lsl #15
  8c:	b.eq	40 <__fixtfdi+0x40>  // b.none
  90:	mov	w0, #0x10                  	// #16
  94:	b	3c <__fixtfdi+0x3c>
  98:	mov	x5, #0x406f                	// #16495
  9c:	sub	x1, x5, x19
  a0:	mov	x0, x19
  a4:	cmp	x1, #0x3f
  a8:	orr	x19, x4, #0x1000000000000
  ac:	b.le	10c <__fixtfdi+0x10c>
  b0:	mov	w4, #0xffffc011            	// #-16367
  b4:	add	w4, w0, w4
  b8:	cmp	x1, #0x40
  bc:	mov	w1, #0x402f                	// #16431
  c0:	sub	w0, w1, w0
  c4:	lsl	x1, x19, x4
  c8:	orr	x1, x2, x1
  cc:	csel	x2, x1, x2, ne  // ne = any
  d0:	lsr	x19, x19, x0
  d4:	cmp	x2, #0x0
  d8:	cset	w0, ne  // ne = any
  dc:	cmp	w3, #0x0
  e0:	cneg	x19, x19, ne  // ne = any
  e4:	cbz	w0, 40 <__fixtfdi+0x40>
  e8:	mov	w0, #0x10                  	// #16
  ec:	b	3c <__fixtfdi+0x3c>
  f0:	mov	x19, #0x0                   	// #0
  f4:	mov	w0, #0x10                  	// #16
  f8:	bl	0 <__sfp_handle_exceptions>
  fc:	b	40 <__fixtfdi+0x40>
 100:	mov	w0, #0x1                   	// #1
 104:	bl	0 <__sfp_handle_exceptions>
 108:	b	40 <__fixtfdi+0x40>
 10c:	mov	w6, #0xffffbfd1            	// #-16431
 110:	add	w4, w0, w6
 114:	sub	w0, w5, w0
 118:	lsl	x1, x2, x4
 11c:	cmp	x1, #0x0
 120:	lsr	x2, x2, x0
 124:	cset	w0, ne  // ne = any
 128:	lsl	x19, x19, x4
 12c:	orr	x19, x2, x19
 130:	b	dc <__fixtfdi+0xdc>

fixunstfdi.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldr	x19, [sp, #32]
  14:	ldr	x1, [sp, #40]
  18:	mrs	x0, fpcr
  1c:	ubfx	x3, x1, #48, #15
  20:	mov	x2, x19
  24:	mov	x4, #0x3ffe                	// #16382
  28:	ubfx	x19, x1, #0, #48
  2c:	cmp	x3, x4
  30:	b.gt	5c <__fixunstfdi+0x5c>
  34:	cbnz	x3, 40 <__fixunstfdi+0x40>
  38:	orr	x19, x2, x19
  3c:	cbz	x19, 4c <__fixunstfdi+0x4c>
  40:	mov	w0, #0x10                  	// #16
  44:	mov	x19, #0x0                   	// #0
  48:	bl	0 <__sfp_handle_exceptions>
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	lsr	x0, x1, #63
  60:	mov	x4, #0x403f                	// #16447
  64:	and	w0, w0, #0xff
  68:	and	x5, x0, #0xff
  6c:	sub	x4, x4, x5
  70:	cmp	x4, x3
  74:	b.le	c8 <__fixunstfdi+0xc8>
  78:	cbnz	x5, dc <__fixunstfdi+0xdc>
  7c:	mov	x1, x3
  80:	mov	x0, #0x406f                	// #16495
  84:	sub	x3, x0, x3
  88:	orr	x4, x19, #0x1000000000000
  8c:	cmp	x3, #0x3f
  90:	b.gt	ec <__fixunstfdi+0xec>
  94:	mov	w3, #0xffffbfd1            	// #-16431
  98:	add	w3, w1, w3
  9c:	sub	w1, w0, w1
  a0:	lsl	x0, x2, x3
  a4:	cmp	x0, #0x0
  a8:	lsr	x19, x2, x1
  ac:	cset	w0, ne  // ne = any
  b0:	lsl	x4, x4, x3
  b4:	orr	x19, x19, x4
  b8:	cbz	w0, 4c <__fixunstfdi+0x4c>
  bc:	mov	w0, #0x10                  	// #16
  c0:	bl	0 <__sfp_handle_exceptions>
  c4:	b	4c <__fixunstfdi+0x4c>
  c8:	eor	w19, w0, #0x1
  cc:	mov	w0, #0x1                   	// #1
  d0:	sbfx	x19, x19, #0, #1
  d4:	bl	0 <__sfp_handle_exceptions>
  d8:	b	4c <__fixunstfdi+0x4c>
  dc:	mov	w0, #0x1                   	// #1
  e0:	mov	x19, #0x0                   	// #0
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	b	4c <__fixunstfdi+0x4c>
  ec:	mov	w0, #0xffffc011            	// #-16367
  f0:	add	w5, w1, w0
  f4:	mov	w0, #0x402f                	// #16431
  f8:	cmp	x3, #0x40
  fc:	sub	w1, w0, w1
 100:	lsl	x0, x4, x5
 104:	orr	x0, x2, x0
 108:	csel	x2, x0, x2, ne  // ne = any
 10c:	lsr	x19, x4, x1
 110:	cmp	x2, #0x0
 114:	cset	w0, ne  // ne = any
 118:	b	b8 <__fixunstfdi+0xb8>

floatditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	cmp	x0, #0x0
   4:	cbz	x0, 68 <__floatditf+0x68>
   8:	cneg	x1, x0, lt  // lt = tstop
   c:	mov	w2, #0x403e                	// #16446
  10:	clz	x3, x1
  14:	mov	x4, #0x406f                	// #16495
  18:	sub	w2, w2, w3
  1c:	lsr	x0, x0, #63
  20:	and	w0, w0, #0xff
  24:	and	w5, w2, #0x7fff
  28:	sub	x3, x4, w2, sxtw
  2c:	cmp	x3, #0x3f
  30:	b.gt	94 <__floatditf+0x94>
  34:	sub	w4, w4, w2
  38:	mov	w3, #0xffffbfd1            	// #-16431
  3c:	add	w2, w2, w3
  40:	mov	x3, #0x0                   	// #0
  44:	lsl	x4, x1, x4
  48:	orr	w0, w5, w0, lsl #15
  4c:	lsr	x1, x1, x2
  50:	and	x1, x1, #0xffffffffffff
  54:	fmov	d0, x4
  58:	bfxil	x3, x1, #0, #48
  5c:	bfi	x3, x0, #48, #16
  60:	fmov	v0.d[1], x3
  64:	ret
  68:	mov	w5, #0x0                   	// #0
  6c:	mov	x1, #0x0                   	// #0
  70:	mov	w0, #0x0                   	// #0
  74:	mov	x3, #0x0                   	// #0
  78:	orr	w0, w5, w0, lsl #15
  7c:	bfxil	x3, x1, #0, #48
  80:	mov	x4, #0x0                   	// #0
  84:	fmov	d0, x4
  88:	bfi	x3, x0, #48, #16
  8c:	fmov	v0.d[1], x3
  90:	ret
  94:	mov	w3, #0x402f                	// #16431
  98:	sub	w2, w3, w2
  9c:	mov	x3, #0x0                   	// #0
  a0:	orr	w0, w5, w0, lsl #15
  a4:	lsl	x1, x1, x2
  a8:	and	x1, x1, #0xffffffffffff
  ac:	mov	x4, #0x0                   	// #0
  b0:	fmov	d0, x4
  b4:	bfxil	x3, x1, #0, #48
  b8:	bfi	x3, x0, #48, #16
  bc:	fmov	v0.d[1], x3
  c0:	ret

floatunditf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	cbz	x0, 54 <__floatunditf+0x54>
   4:	clz	x2, x0
   8:	mov	w1, #0x403e                	// #16446
   c:	sub	w1, w1, w2
  10:	mov	x2, #0x406f                	// #16495
  14:	and	w4, w1, #0x7fff
  18:	sub	x3, x2, w1, sxtw
  1c:	cmp	x3, #0x3f
  20:	b.gt	74 <__floatunditf+0x74>
  24:	sub	w2, w2, w1
  28:	mov	w3, #0xffffbfd1            	// #-16431
  2c:	add	w1, w1, w3
  30:	mov	x3, #0x0                   	// #0
  34:	lsr	x1, x0, x1
  38:	and	x1, x1, #0xffffffffffff
  3c:	lsl	x0, x0, x2
  40:	fmov	d0, x0
  44:	bfxil	x3, x1, #0, #48
  48:	bfi	x3, x4, #48, #16
  4c:	fmov	v0.d[1], x3
  50:	ret
  54:	mov	x1, #0x0                   	// #0
  58:	mov	x3, #0x0                   	// #0
  5c:	bfxil	x3, x1, #0, #48
  60:	fmov	d0, x0
  64:	mov	w4, #0x0                   	// #0
  68:	bfi	x3, x4, #48, #16
  6c:	fmov	v0.d[1], x3
  70:	ret
  74:	mov	w2, #0x402f                	// #16431
  78:	sub	w1, w2, w1
  7c:	mov	x3, #0x0                   	// #0
  80:	lsl	x1, x0, x1
  84:	and	x1, x1, #0xffffffffffff
  88:	mov	x0, #0x0                   	// #0
  8c:	fmov	d0, x0
  90:	bfxil	x3, x1, #0, #48
  94:	bfi	x3, x4, #48, #16
  98:	fmov	v0.d[1], x3
  9c:	ret

fixtfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x3, x0, [sp, #32]
  14:	mrs	x1, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	mov	x6, x3
  20:	mov	x5, #0x3ffe                	// #16382
  24:	ubfx	x3, x0, #0, #48
  28:	cmp	x4, x5
  2c:	b.gt	5c <__fixtfti+0x5c>
  30:	cbz	x4, 11c <__fixtfti+0x11c>
  34:	mov	x19, #0x0                   	// #0
  38:	mov	x1, #0x0                   	// #0
  3c:	mov	w0, #0x10                  	// #16
  40:	str	x1, [sp, #32]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	x1, [sp, #32]
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	lsr	x2, x0, #63
  60:	mov	x1, #0x407d                	// #16509
  64:	and	w2, w2, #0xff
  68:	cmp	x4, x1
  6c:	and	x7, x2, #0xff
  70:	b.le	ac <__fixtfti+0xac>
  74:	adrp	x1, 0 <__fixtfti>
  78:	mov	x0, #0x1                   	// #1
  7c:	sub	x19, x0, x7
  80:	mov	x5, #0x407e                	// #16510
  84:	ldr	x1, [x1]
  88:	asr	x7, x19, #63
  8c:	negs	x19, x19
  90:	sbc	x1, x1, x7
  94:	cmp	x4, x5
  98:	csel	w2, w2, wzr, eq  // eq = none
  9c:	cbz	w2, 40 <__fixtfti+0x40>
  a0:	orr	x3, x6, x3
  a4:	cbnz	x3, 40 <__fixtfti+0x40>
  a8:	b	4c <__fixtfti+0x4c>
  ac:	mov	x1, #0x406e                	// #16494
  b0:	mov	x0, x4
  b4:	orr	x5, x3, #0x1000000000000
  b8:	cmp	x4, x1
  bc:	b.gt	134 <__fixtfti+0x134>
  c0:	mov	x3, #0x406f                	// #16495
  c4:	sub	x4, x3, x4
  c8:	cmp	x4, #0x3f
  cc:	b.gt	184 <__fixtfti+0x184>
  d0:	mov	w2, #0xffffbfd1            	// #-16431
  d4:	add	w2, w0, w2
  d8:	sub	w0, w3, w0
  dc:	mov	x19, #0x0                   	// #0
  e0:	lsl	x1, x6, x2
  e4:	cmp	x1, #0x0
  e8:	lsr	x3, x6, x0
  ec:	cset	w4, ne  // ne = any
  f0:	lsl	x2, x5, x2
  f4:	orr	x1, x2, x3
  f8:	lsr	x0, x5, x0
  fc:	adds	x19, x1, x19
 100:	cinc	x1, x0, cs  // cs = hs, nlast
 104:	cbz	x7, 110 <__fixtfti+0x110>
 108:	negs	x19, x19
 10c:	ngc	x1, x1
 110:	cbz	w4, 4c <__fixtfti+0x4c>
 114:	mov	w0, #0x10                  	// #16
 118:	b	40 <__fixtfti+0x40>
 11c:	orr	x3, x6, x3
 120:	mov	x19, #0x0                   	// #0
 124:	mov	x1, #0x0                   	// #0
 128:	cbz	x3, 4c <__fixtfti+0x4c>
 12c:	mov	w0, #0x10                  	// #16
 130:	b	40 <__fixtfti+0x40>
 134:	mov	w3, #0xffffbf91            	// #-16495
 138:	add	w2, w4, w3
 13c:	mov	w3, #0x3f                  	// #63
 140:	sub	w4, w3, w2
 144:	lsr	x3, x6, #1
 148:	mov	x19, x6
 14c:	mov	w6, #0xffffbf51            	// #-16559
 150:	add	w0, w0, w6
 154:	cmp	w0, #0x0
 158:	lsr	x3, x3, x4
 15c:	lsl	x1, x5, x2
 160:	orr	x1, x3, x1
 164:	lsl	x3, x19, x0
 168:	csel	x1, x3, x1, ge  // ge = tcont
 16c:	lsl	x19, x19, x2
 170:	csel	x19, xzr, x19, ge  // ge = tcont
 174:	cbz	x7, 4c <__fixtfti+0x4c>
 178:	negs	x19, x19
 17c:	ngc	x1, x1
 180:	b	4c <__fixtfti+0x4c>
 184:	mov	w1, #0xffffc011            	// #-16367
 188:	add	w2, w0, w1
 18c:	mov	w1, #0x402f                	// #16431
 190:	cmp	x4, #0x40
 194:	sub	w0, w1, w0
 198:	lsl	x1, x5, x2
 19c:	orr	x1, x6, x1
 1a0:	mov	x19, #0x0                   	// #0
 1a4:	csel	x6, x1, x6, ne  // ne = any
 1a8:	lsr	x1, x5, x0
 1ac:	cmp	x6, #0x0
 1b0:	mov	x0, #0x0                   	// #0
 1b4:	cset	w4, ne  // ne = any
 1b8:	b	fc <__fixtfti+0xfc>

fixunstfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x1, x0, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	mov	x2, #0x3ffe                	// #16382
  20:	mov	x3, x1
  24:	cmp	x4, x2
  28:	ubfx	x5, x0, #0, #48
  2c:	b.gt	78 <__fixunstfti+0x78>
  30:	cbz	x4, 5c <__fixunstfti+0x5c>
  34:	mov	w0, #0x10                  	// #16
  38:	mov	x19, #0x0                   	// #0
  3c:	mov	x1, #0x0                   	// #0
  40:	str	x1, [sp, #32]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	x1, [sp, #32]
  4c:	mov	x0, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	orr	x1, x1, x5
  60:	mov	x19, #0x0                   	// #0
  64:	cbnz	x1, 34 <__fixunstfti+0x34>
  68:	mov	x0, x19
  6c:	ldr	x19, [sp, #16]
  70:	ldp	x29, x30, [sp], #48
  74:	ret
  78:	lsr	x1, x0, #63
  7c:	mov	x2, #0x407f                	// #16511
  80:	and	w1, w1, #0xff
  84:	and	x6, x1, #0xff
  88:	sub	x2, x2, x6
  8c:	cmp	x2, x4
  90:	b.le	f0 <__fixunstfti+0xf0>
  94:	cbnz	x6, 104 <__fixunstfti+0x104>
  98:	mov	x1, #0x406e                	// #16494
  9c:	mov	x0, x4
  a0:	orr	x5, x5, #0x1000000000000
  a4:	cmp	x4, x1
  a8:	b.le	114 <__fixunstfti+0x114>
  ac:	adds	x3, x3, x6
  b0:	mov	w2, #0x3f                  	// #63
  b4:	mov	w6, #0xffffbf91            	// #-16495
  b8:	add	w19, w4, w6
  bc:	sub	w4, w2, w19
  c0:	lsr	x2, x3, #1
  c4:	mov	w7, #0xffffbf51            	// #-16559
  c8:	add	w0, w0, w7
  cc:	cmp	w0, #0x0
  d0:	lsl	x1, x5, x19
  d4:	lsr	x2, x2, x4
  d8:	orr	x1, x2, x1
  dc:	lsl	x19, x3, x19
  e0:	csel	x19, xzr, x19, ge  // ge = tcont
  e4:	lsl	x2, x3, x0
  e8:	csel	x1, x2, x1, ge  // ge = tcont
  ec:	b	4c <__fixunstfti+0x4c>
  f0:	eor	w1, w1, #0x1
  f4:	mov	w0, #0x1                   	// #1
  f8:	sbfx	x19, x1, #0, #1
  fc:	mov	x1, x19
 100:	b	40 <__fixunstfti+0x40>
 104:	mov	w0, #0x1                   	// #1
 108:	mov	x19, #0x0                   	// #0
 10c:	mov	x1, #0x0                   	// #0
 110:	b	40 <__fixunstfti+0x40>
 114:	mov	x2, #0x406f                	// #16495
 118:	sub	x4, x2, x4
 11c:	cmp	x4, #0x3f
 120:	b.le	170 <__fixunstfti+0x170>
 124:	mov	w1, #0xffffc011            	// #-16367
 128:	add	w1, w0, w1
 12c:	cmp	x4, #0x40
 130:	mov	w2, #0x402f                	// #16431
 134:	lsl	x1, x5, x1
 138:	orr	x1, x3, x1
 13c:	csel	x3, x1, x3, ne  // ne = any
 140:	sub	w0, w2, w0
 144:	cmp	x3, #0x0
 148:	mov	x19, #0x0                   	// #0
 14c:	cset	w2, ne  // ne = any
 150:	lsr	x0, x5, x0
 154:	mov	x1, #0x0                   	// #0
 158:	adds	x3, x0, x19
 15c:	mov	w0, #0x10                  	// #16
 160:	mov	x19, x3
 164:	cinc	x1, x1, cs  // cs = hs, nlast
 168:	cbnz	w2, 40 <__fixunstfti+0x40>
 16c:	b	4c <__fixunstfti+0x4c>
 170:	mov	w4, #0xffffbfd1            	// #-16431
 174:	add	w4, w0, w4
 178:	sub	w1, w2, w0
 17c:	mov	x19, #0x0                   	// #0
 180:	lsl	x0, x3, x4
 184:	cmp	x0, #0x0
 188:	lsl	x4, x5, x4
 18c:	cset	w2, ne  // ne = any
 190:	lsr	x0, x3, x1
 194:	orr	x0, x4, x0
 198:	lsr	x1, x5, x1
 19c:	b	158 <__fixunstfti+0x158>

floattitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	mrs	x9, fpcr
   4:	orr	x2, x0, x1
   8:	cbz	x2, c8 <__floattitf+0xc8>
   c:	lsr	x4, x1, #63
  10:	mov	x2, x1
  14:	and	w8, w4, #0xff
  18:	tbnz	x1, #63, f4 <__floattitf+0xf4>
  1c:	mov	x3, x2
  20:	cbz	x2, 104 <__floattitf+0x104>
  24:	clz	x4, x2
  28:	mov	w5, #0x407e                	// #16510
  2c:	sub	w5, w5, w4
  30:	mov	w4, #0x406f                	// #16495
  34:	cmp	w5, w4
  38:	sxtw	x7, w5
  3c:	b.le	254 <__floattitf+0x254>
  40:	mov	w2, #0x4072                	// #16498
  44:	cmp	w5, w2
  48:	b.gt	150 <__floattitf+0x150>
  4c:	mov	x4, #0x4072                	// #16498
  50:	cmp	x7, x4
  54:	b.eq	74 <__floattitf+0x74>  // b.none
  58:	sub	w2, w2, w5
  5c:	mov	w4, #0xffffbfce            	// #-16434
  60:	add	w5, w5, w4
  64:	lsl	x3, x3, x2
  68:	lsr	x5, x0, x5
  6c:	orr	x3, x5, x3
  70:	lsl	x0, x0, x2
  74:	and	x5, x3, #0xfff7ffffffffffff
  78:	tst	x0, #0x7
  7c:	mov	w4, #0x0                   	// #0
  80:	b.eq	a0 <__floattitf+0xa0>  // b.none
  84:	and	x2, x9, #0xc00000
  88:	mov	w4, #0x10                  	// #16
  8c:	cmp	x2, #0x400, lsl #12
  90:	b.eq	20c <__floattitf+0x20c>  // b.none
  94:	cmp	x2, #0x800, lsl #12
  98:	b.eq	24c <__floattitf+0x24c>  // b.none
  9c:	cbz	x2, 230 <__floattitf+0x230>
  a0:	lsr	x1, x5, #3
  a4:	mov	x3, #0x0                   	// #0
  a8:	extr	x2, x5, x0, #3
  ac:	bfxil	x3, x1, #0, #48
  b0:	fmov	d0, x2
  b4:	bfi	x3, x7, #48, #15
  b8:	bfi	x3, x8, #63, #1
  bc:	fmov	v0.d[1], x3
  c0:	cbnz	w4, 1ec <__floattitf+0x1ec>
  c4:	ret
  c8:	mov	w8, #0x0                   	// #0
  cc:	mov	w4, #0x0                   	// #0
  d0:	mov	x1, #0x0                   	// #0
  d4:	mov	x6, #0x0                   	// #0
  d8:	mov	x3, #0x0                   	// #0
  dc:	orr	w4, w4, w8, lsl #15
  e0:	bfxil	x3, x1, #0, #48
  e4:	fmov	d0, x6
  e8:	bfi	x3, x4, #48, #16
  ec:	fmov	v0.d[1], x3
  f0:	ret
  f4:	negs	x0, x0
  f8:	ngc	x2, x1
  fc:	mov	x3, x2
 100:	cbnz	x2, 24 <__floattitf+0x24>
 104:	clz	x1, x0
 108:	mov	w3, #0x403e                	// #16446
 10c:	sub	w1, w3, w1
 110:	mov	x3, #0x406f                	// #16495
 114:	mov	w5, w1
 118:	and	w4, w1, #0x7fff
 11c:	sub	x3, x3, w1, sxtw
 120:	cmp	x3, #0x3f
 124:	b.gt	1d4 <__floattitf+0x1d4>
 128:	mov	w6, #0x406f                	// #16495
 12c:	mov	w13, #0xffffbfd1            	// #-16431
 130:	add	w1, w5, w13
 134:	sub	w5, w6, w5
 138:	lsr	x1, x0, x1
 13c:	lsl	x2, x2, x5
 140:	orr	x2, x1, x2
 144:	and	x1, x2, #0xffffffffffff
 148:	lsl	x6, x0, x5
 14c:	b	d8 <__floattitf+0xd8>
 150:	sub	w6, w5, w2
 154:	mov	w10, #0x3f                  	// #63
 158:	lsl	x2, x3, #1
 15c:	sub	w11, w10, w6
 160:	mov	w4, #0x40f2                	// #16626
 164:	mov	w12, #0xffffbf4e            	// #-16562
 168:	add	w12, w5, w12
 16c:	sub	w5, w4, w5
 170:	sub	w10, w10, w5
 174:	lsl	x4, x2, x11
 178:	lsr	x11, x0, #1
 17c:	cmp	w12, #0x0
 180:	lsr	x2, x0, x6
 184:	orr	x2, x4, x2
 188:	sub	w4, w5, #0x40
 18c:	lsr	x13, x3, x12
 190:	lsr	x6, x3, x6
 194:	csel	x2, x13, x2, ge  // ge = tcont
 198:	csel	x6, xzr, x6, ge  // ge = tcont
 19c:	lsr	x10, x11, x10
 1a0:	cmp	w4, #0x0
 1a4:	lsl	x3, x3, x5
 1a8:	orr	x3, x10, x3
 1ac:	lsl	x10, x0, x4
 1b0:	lsl	x0, x0, x5
 1b4:	csel	x3, x10, x3, ge  // ge = tcont
 1b8:	csel	x0, xzr, x0, ge  // ge = tcont
 1bc:	orr	x0, x0, x3
 1c0:	mov	x3, x6
 1c4:	cmp	x0, #0x0
 1c8:	cset	x0, ne  // ne = any
 1cc:	orr	x0, x2, x0
 1d0:	b	74 <__floattitf+0x74>
 1d4:	mov	w2, #0x402f                	// #16431
 1d8:	sub	w2, w2, w1
 1dc:	mov	x6, #0x0                   	// #0
 1e0:	lsl	x1, x0, x2
 1e4:	and	x1, x1, #0xffffffffffff
 1e8:	b	d8 <__floattitf+0xd8>
 1ec:	stp	x29, x30, [sp, #-32]!
 1f0:	mov	w0, w4
 1f4:	mov	x29, sp
 1f8:	str	q0, [sp, #16]
 1fc:	bl	0 <__sfp_handle_exceptions>
 200:	ldr	q0, [sp, #16]
 204:	ldp	x29, x30, [sp], #32
 208:	ret
 20c:	tbnz	x1, #63, a0 <__floattitf+0xa0>
 210:	adds	x0, x0, #0x8
 214:	cinc	x5, x5, cs  // cs = hs, nlast
 218:	and	x1, x5, #0x8000000000000
 21c:	cbz	x1, 288 <__floattitf+0x288>
 220:	and	x5, x5, #0xfff7ffffffffffff
 224:	add	x7, x7, #0x1
 228:	mov	w4, #0x10                  	// #16
 22c:	b	a0 <__floattitf+0xa0>
 230:	and	x1, x0, #0xf
 234:	cmp	x1, #0x4
 238:	b.eq	a0 <__floattitf+0xa0>  // b.none
 23c:	adds	x0, x0, #0x4
 240:	cinc	x5, x5, cs  // cs = hs, nlast
 244:	and	x1, x5, #0x8000000000000
 248:	b	21c <__floattitf+0x21c>
 24c:	tbz	x1, #63, a0 <__floattitf+0xa0>
 250:	b	210 <__floattitf+0x210>
 254:	mov	x1, #0x406f                	// #16495
 258:	mov	x6, x0
 25c:	cmp	x7, x1
 260:	b.ne	290 <__floattitf+0x290>  // b.any
 264:	and	x1, x2, #0xffffffffffff
 268:	mov	w4, w7
 26c:	mov	x3, #0x0                   	// #0
 270:	orr	w4, w4, w8, lsl #15
 274:	bfxil	x3, x1, #0, #48
 278:	fmov	d0, x6
 27c:	bfi	x3, x4, #48, #16
 280:	fmov	v0.d[1], x3
 284:	ret
 288:	mov	w4, #0x10                  	// #16
 28c:	b	a0 <__floattitf+0xa0>
 290:	and	w4, w5, #0x7fff
 294:	b	128 <__floattitf+0x128>

floatuntitf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	mrs	x6, fpcr
   4:	orr	x2, x0, x1
   8:	cbz	x2, cc <__floatuntitf+0xcc>
   c:	mov	x3, x1
  10:	cbz	x1, f0 <__floatuntitf+0xf0>
  14:	clz	x4, x1
  18:	mov	w2, #0x407e                	// #16510
  1c:	sub	w2, w2, w4
  20:	mov	w4, #0x406f                	// #16495
  24:	cmp	w2, w4
  28:	sxtw	x4, w2
  2c:	b.le	22c <__floatuntitf+0x22c>
  30:	mov	w1, #0x4072                	// #16498
  34:	cmp	w2, w1
  38:	b.gt	13c <__floatuntitf+0x13c>
  3c:	mov	x5, x0
  40:	mov	x0, #0x4072                	// #16498
  44:	cmp	x4, x0
  48:	b.eq	68 <__floatuntitf+0x68>  // b.none
  4c:	sub	w1, w1, w2
  50:	mov	w0, #0xffffbfce            	// #-16434
  54:	add	w2, w2, w0
  58:	lsl	x3, x3, x1
  5c:	lsr	x2, x5, x2
  60:	orr	x3, x2, x3
  64:	lsl	x5, x5, x1
  68:	and	x1, x3, #0xfff7ffffffffffff
  6c:	tst	x5, #0x7
  70:	mov	w0, #0x0                   	// #0
  74:	b.eq	a8 <__floatuntitf+0xa8>  // b.none
  78:	ands	x6, x6, #0xc00000
  7c:	b.eq	208 <__floatuntitf+0x208>  // b.none
  80:	cmp	x6, #0x400, lsl #12
  84:	b.ne	200 <__floatuntitf+0x200>  // b.any
  88:	adds	x0, x5, #0x8
  8c:	cinc	x1, x1, cs  // cs = hs, nlast
  90:	mov	x5, x0
  94:	and	x0, x1, #0x8000000000000
  98:	cbz	x0, 200 <__floatuntitf+0x200>
  9c:	and	x1, x1, #0xfff7ffffffffffff
  a0:	add	x4, x4, #0x1
  a4:	mov	w0, #0x10                  	// #16
  a8:	lsr	x6, x1, #3
  ac:	mov	x3, #0x0                   	// #0
  b0:	extr	x2, x1, x5, #3
  b4:	bfxil	x3, x6, #0, #48
  b8:	fmov	d0, x2
  bc:	bfi	x3, x4, #48, #16
  c0:	fmov	v0.d[1], x3
  c4:	cbnz	w0, 1e4 <__floatuntitf+0x1e4>
  c8:	ret
  cc:	mov	w4, #0x0                   	// #0
  d0:	mov	x1, #0x0                   	// #0
  d4:	mov	x5, #0x0                   	// #0
  d8:	mov	x3, #0x0                   	// #0
  dc:	fmov	d0, x5
  e0:	bfxil	x3, x1, #0, #48
  e4:	bfi	x3, x4, #48, #16
  e8:	fmov	v0.d[1], x3
  ec:	ret
  f0:	clz	x3, x0
  f4:	mov	w2, #0x403e                	// #16446
  f8:	sub	w3, w2, w3
  fc:	mov	x5, #0x406f                	// #16495
 100:	mov	w2, w3
 104:	and	w4, w3, #0x7fff
 108:	sub	x5, x5, w3, sxtw
 10c:	cmp	x5, #0x3f
 110:	b.gt	1b8 <__floatuntitf+0x1b8>
 114:	mov	w5, #0x406f                	// #16495
 118:	mov	w10, #0xffffbfd1            	// #-16431
 11c:	add	w3, w2, w10
 120:	sub	w2, w5, w2
 124:	lsr	x3, x0, x3
 128:	lsl	x1, x1, x2
 12c:	orr	x1, x3, x1
 130:	and	x1, x1, #0xffffffffffff
 134:	lsl	x5, x0, x2
 138:	b	d8 <__floatuntitf+0xd8>
 13c:	mov	w5, #0x40f2                	// #16626
 140:	sub	w5, w5, w2
 144:	lsr	x10, x0, #1
 148:	mov	w8, #0x3f                  	// #63
 14c:	sub	w11, w8, w5
 150:	subs	w9, w5, #0x40
 154:	lsl	x7, x3, x5
 158:	sub	w1, w2, w1
 15c:	lsr	x10, x10, x11
 160:	orr	x7, x10, x7
 164:	lsl	x5, x0, x5
 168:	csel	x5, xzr, x5, pl  // pl = nfrst
 16c:	lsl	x10, x0, x9
 170:	csel	x7, x10, x7, pl  // pl = nfrst
 174:	orr	x5, x5, x7
 178:	sub	w8, w8, w1
 17c:	lsl	x7, x3, #1
 180:	cmp	x5, #0x0
 184:	mov	w9, #0xffffbf4e            	// #-16562
 188:	add	w2, w2, w9
 18c:	lsr	x5, x0, x1
 190:	cset	x0, ne  // ne = any
 194:	lsl	x8, x7, x8
 198:	cmp	w2, #0x0
 19c:	orr	x5, x8, x5
 1a0:	lsr	x7, x3, x2
 1a4:	csel	x5, x7, x5, ge  // ge = tcont
 1a8:	lsr	x2, x3, x1
 1ac:	orr	x5, x0, x5
 1b0:	csel	x3, x2, xzr, lt  // lt = tstop
 1b4:	b	68 <__floatuntitf+0x68>
 1b8:	mov	w1, #0x402f                	// #16431
 1bc:	sub	w1, w1, w3
 1c0:	mov	x5, #0x0                   	// #0
 1c4:	lsl	x1, x0, x1
 1c8:	and	x1, x1, #0xffffffffffff
 1cc:	mov	x3, #0x0                   	// #0
 1d0:	fmov	d0, x5
 1d4:	bfxil	x3, x1, #0, #48
 1d8:	bfi	x3, x4, #48, #16
 1dc:	fmov	v0.d[1], x3
 1e0:	ret
 1e4:	stp	x29, x30, [sp, #-32]!
 1e8:	mov	x29, sp
 1ec:	str	q0, [sp, #16]
 1f0:	bl	0 <__sfp_handle_exceptions>
 1f4:	ldr	q0, [sp, #16]
 1f8:	ldp	x29, x30, [sp], #32
 1fc:	ret
 200:	mov	w0, #0x10                  	// #16
 204:	b	a8 <__floatuntitf+0xa8>
 208:	and	x2, x5, #0xf
 20c:	mov	w0, #0x10                  	// #16
 210:	cmp	x2, #0x4
 214:	b.eq	a8 <__floatuntitf+0xa8>  // b.none
 218:	adds	x0, x5, #0x4
 21c:	cinc	x1, x1, cs  // cs = hs, nlast
 220:	mov	x5, x0
 224:	and	x0, x1, #0x8000000000000
 228:	b	98 <__floatuntitf+0x98>
 22c:	mov	x3, #0x406f                	// #16495
 230:	mov	x5, x0
 234:	cmp	x4, x3
 238:	b.eq	1c8 <__floatuntitf+0x1c8>  // b.none
 23c:	and	w4, w2, #0x7fff
 240:	b	114 <__floatuntitf+0x114>

extendsftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	w0, s0
   8:	ubfx	x2, x0, #23, #8
   c:	and	x1, x0, #0x7fffff
  10:	add	x3, x2, #0x1
  14:	ubfx	x5, x0, #0, #23
  18:	tst	x3, #0xfe
  1c:	lsr	w0, w0, #31
  20:	b.eq	50 <__extendsftf2+0x50>  // b.none
  24:	lsl	x1, x1, #25
  28:	mov	w3, #0x3f80                	// #16256
  2c:	add	w4, w2, w3
  30:	mov	x3, #0x0                   	// #0
  34:	bfxil	x3, x1, #0, #48
  38:	mov	x2, #0x0                   	// #0
  3c:	fmov	d0, x2
  40:	bfi	x3, x4, #48, #15
  44:	bfi	x3, x0, #63, #1
  48:	fmov	v0.d[1], x3
  4c:	ret
  50:	cbnz	w2, 80 <__extendsftf2+0x80>
  54:	cbnz	x1, cc <__extendsftf2+0xcc>
  58:	mov	x1, #0x0                   	// #0
  5c:	mov	x3, #0x0                   	// #0
  60:	bfxil	x3, x1, #0, #48
  64:	mov	w4, #0x0                   	// #0
  68:	mov	x2, #0x0                   	// #0
  6c:	fmov	d0, x2
  70:	bfi	x3, x4, #48, #15
  74:	bfi	x3, x0, #63, #1
  78:	fmov	v0.d[1], x3
  7c:	ret
  80:	cbz	x1, 108 <__extendsftf2+0x108>
  84:	lsl	x1, x1, #25
  88:	mov	x3, #0x0                   	// #0
  8c:	orr	x1, x1, #0x800000000000
  90:	mov	x2, #0x0                   	// #0
  94:	fmov	d0, x2
  98:	bfxil	x3, x1, #0, #48
  9c:	orr	x3, x3, #0x7fff000000000000
  a0:	bfi	x3, x0, #63, #1
  a4:	fmov	v0.d[1], x3
  a8:	tbnz	w5, #22, 130 <__extendsftf2+0x130>
  ac:	stp	x29, x30, [sp, #-32]!
  b0:	mov	w0, #0x1                   	// #1
  b4:	mov	x29, sp
  b8:	str	q0, [sp, #16]
  bc:	bl	0 <__sfp_handle_exceptions>
  c0:	ldr	q0, [sp, #16]
  c4:	ldp	x29, x30, [sp], #32
  c8:	ret
  cc:	clz	x4, x1
  d0:	mov	w2, #0x3fa9                	// #16297
  d4:	sub	w3, w4, #0xf
  d8:	sub	w2, w2, w4
  dc:	and	w4, w2, #0x7fff
  e0:	mov	x2, #0x0                   	// #0
  e4:	lsl	x1, x1, x3
  e8:	and	x1, x1, #0xffffffffffff
  ec:	mov	x3, #0x0                   	// #0
  f0:	fmov	d0, x2
  f4:	bfxil	x3, x1, #0, #48
  f8:	bfi	x3, x4, #48, #15
  fc:	bfi	x3, x0, #63, #1
 100:	fmov	v0.d[1], x3
 104:	ret
 108:	mov	x1, #0x0                   	// #0
 10c:	mov	x3, #0x0                   	// #0
 110:	bfxil	x3, x1, #0, #48
 114:	mov	w4, #0x7fff                	// #32767
 118:	mov	x2, #0x0                   	// #0
 11c:	fmov	d0, x2
 120:	bfi	x3, x4, #48, #15
 124:	bfi	x3, x0, #63, #1
 128:	fmov	v0.d[1], x3
 12c:	ret
 130:	ret

extenddftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	mrs	x0, fpcr
   4:	fmov	x0, d0
   8:	ubfx	x1, x0, #52, #11
   c:	lsr	x4, x0, #63
  10:	add	x2, x1, #0x1
  14:	and	w4, w4, #0xff
  18:	tst	x2, #0x7fe
  1c:	ubfx	x0, x0, #0, #52
  20:	b.eq	54 <__extenddftf2+0x54>  // b.none
  24:	lsr	x5, x0, #4
  28:	mov	x3, #0x0                   	// #0
  2c:	and	x5, x5, #0xffffffffffff
  30:	mov	w2, #0x3c00                	// #15360
  34:	add	w1, w1, w2
  38:	lsl	x0, x0, #60
  3c:	bfxil	x3, x5, #0, #48
  40:	fmov	d0, x0
  44:	bfi	x3, x1, #48, #15
  48:	bfi	x3, x4, #63, #1
  4c:	fmov	v0.d[1], x3
  50:	ret
  54:	cbnz	x1, a8 <__extenddftf2+0xa8>
  58:	cbz	x0, f4 <__extenddftf2+0xf4>
  5c:	clz	x2, x0
  60:	cmp	w2, #0xe
  64:	b.gt	140 <__extenddftf2+0x140>
  68:	add	w1, w2, #0x31
  6c:	mov	w5, #0xf                   	// #15
  70:	sub	w5, w5, w2
  74:	lsr	x5, x0, x5
  78:	lsl	x0, x0, x1
  7c:	and	x5, x5, #0xffffffffffff
  80:	mov	w1, #0x3c0c                	// #15372
  84:	mov	x3, #0x0                   	// #0
  88:	sub	w1, w1, w2
  8c:	and	w1, w1, #0x7fff
  90:	bfxil	x3, x5, #0, #48
  94:	fmov	d0, x0
  98:	bfi	x3, x1, #48, #15
  9c:	bfi	x3, x4, #63, #1
  a0:	fmov	v0.d[1], x3
  a4:	ret
  a8:	cbz	x0, 118 <__extenddftf2+0x118>
  ac:	lsr	x1, x0, #4
  b0:	mov	x3, #0x0                   	// #0
  b4:	orr	x1, x1, #0x800000000000
  b8:	lsl	x2, x0, #60
  bc:	fmov	d0, x2
  c0:	bfxil	x3, x1, #0, #48
  c4:	orr	x3, x3, #0x7fff000000000000
  c8:	bfi	x3, x4, #63, #1
  cc:	fmov	v0.d[1], x3
  d0:	tbnz	x0, #51, 13c <__extenddftf2+0x13c>
  d4:	stp	x29, x30, [sp, #-32]!
  d8:	mov	w0, #0x1                   	// #1
  dc:	mov	x29, sp
  e0:	str	q0, [sp, #16]
  e4:	bl	0 <__sfp_handle_exceptions>
  e8:	ldr	q0, [sp, #16]
  ec:	ldp	x29, x30, [sp], #32
  f0:	ret
  f4:	mov	x5, #0x0                   	// #0
  f8:	mov	x3, #0x0                   	// #0
  fc:	bfxil	x3, x5, #0, #48
 100:	mov	w1, #0x0                   	// #0
 104:	fmov	d0, x0
 108:	bfi	x3, x1, #48, #15
 10c:	bfi	x3, x4, #63, #1
 110:	fmov	v0.d[1], x3
 114:	ret
 118:	mov	x5, #0x0                   	// #0
 11c:	mov	x3, #0x0                   	// #0
 120:	bfxil	x3, x5, #0, #48
 124:	mov	w1, #0x7fff                	// #32767
 128:	fmov	d0, x0
 12c:	bfi	x3, x1, #48, #15
 130:	bfi	x3, x4, #63, #1
 134:	fmov	v0.d[1], x3
 138:	ret
 13c:	ret
 140:	sub	w5, w2, #0xf
 144:	lsl	x5, x0, x5
 148:	mov	x0, #0x0                   	// #0
 14c:	b	7c <__extenddftf2+0x7c>

extendhftf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhftf2>:
   0:	mrs	x0, fpcr
   4:	umov	w1, v0.h[0]
   8:	mov	w0, #0x0                   	// #0
   c:	bfxil	w0, w1, #0, #16
  10:	and	w4, w0, #0x3ff
  14:	and	x2, x0, #0x3ff
  18:	ubfx	x1, x0, #10, #5
  1c:	ubfx	x0, x0, #15, #1
  20:	add	x3, x1, #0x1
  24:	tst	x3, #0x1e
  28:	b.eq	58 <__extendhftf2+0x58>  // b.none
  2c:	lsl	x4, x2, #38
  30:	mov	x3, #0x0                   	// #0
  34:	mov	w2, #0x3ff0                	// #16368
  38:	add	w1, w1, w2
  3c:	bfxil	x3, x4, #0, #48
  40:	mov	x2, #0x0                   	// #0
  44:	fmov	d0, x2
  48:	bfi	x3, x1, #48, #15
  4c:	bfi	x3, x0, #63, #1
  50:	fmov	v0.d[1], x3
  54:	ret
  58:	cbnz	x1, 88 <__extendhftf2+0x88>
  5c:	cbnz	x2, d4 <__extendhftf2+0xd4>
  60:	mov	x4, #0x0                   	// #0
  64:	mov	x3, #0x0                   	// #0
  68:	bfxil	x3, x4, #0, #48
  6c:	mov	w1, #0x0                   	// #0
  70:	mov	x2, #0x0                   	// #0
  74:	fmov	d0, x2
  78:	bfi	x3, x1, #48, #15
  7c:	bfi	x3, x0, #63, #1
  80:	fmov	v0.d[1], x3
  84:	ret
  88:	cbz	x2, 110 <__extendhftf2+0x110>
  8c:	lsl	x1, x2, #38
  90:	mov	x3, #0x0                   	// #0
  94:	orr	x1, x1, #0x800000000000
  98:	mov	x2, #0x0                   	// #0
  9c:	fmov	d0, x2
  a0:	bfxil	x3, x1, #0, #48
  a4:	orr	x3, x3, #0x7fff000000000000
  a8:	bfi	x3, x0, #63, #1
  ac:	fmov	v0.d[1], x3
  b0:	tbnz	w4, #9, 138 <__extendhftf2+0x138>
  b4:	stp	x29, x30, [sp, #-32]!
  b8:	mov	w0, #0x1                   	// #1
  bc:	mov	x29, sp
  c0:	str	q0, [sp, #16]
  c4:	bl	0 <__sfp_handle_exceptions>
  c8:	ldr	q0, [sp, #16]
  cc:	ldp	x29, x30, [sp], #32
  d0:	ret
  d4:	clz	x3, x2
  d8:	mov	w1, #0x4026                	// #16422
  dc:	sub	w4, w3, #0xf
  e0:	sub	w1, w1, w3
  e4:	mov	x3, #0x0                   	// #0
  e8:	and	w1, w1, #0x7fff
  ec:	lsl	x4, x2, x4
  f0:	and	x4, x4, #0xffffffffffff
  f4:	mov	x2, #0x0                   	// #0
  f8:	fmov	d0, x2
  fc:	bfxil	x3, x4, #0, #48
 100:	bfi	x3, x1, #48, #15
 104:	bfi	x3, x0, #63, #1
 108:	fmov	v0.d[1], x3
 10c:	ret
 110:	mov	x4, #0x0                   	// #0
 114:	mov	x3, #0x0                   	// #0
 118:	bfxil	x3, x4, #0, #48
 11c:	mov	w1, #0x7fff                	// #32767
 120:	mov	x2, #0x0                   	// #0
 124:	fmov	d0, x2
 128:	bfi	x3, x1, #48, #15
 12c:	bfi	x3, x0, #63, #1
 130:	fmov	v0.d[1], x3
 134:	ret
 138:	ret

trunctfsf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x5, x0, [sp, #32]
  14:	mrs	x3, fpcr
  18:	ubfx	x4, x0, #48, #15
  1c:	lsr	x6, x0, #63
  20:	add	x1, x4, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	orr	x0, x0, x5, lsr #61
  2c:	tst	x1, #0x7ffe
  30:	and	w6, w6, #0xff
  34:	lsl	x5, x5, #3
  38:	b.eq	b8 <__trunctfsf2+0xb8>  // b.none
  3c:	mov	x1, #0xffffffffffffc080    	// #-16256
  40:	add	x4, x4, x1
  44:	cmp	x4, #0xfe
  48:	b.le	f8 <__trunctfsf2+0xf8>
  4c:	ands	x0, x3, #0xc00000
  50:	b.eq	190 <__trunctfsf2+0x190>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	27c <__trunctfsf2+0x27c>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w7, w6, wzr, eq  // eq = none
  64:	cbnz	w7, 190 <__trunctfsf2+0x190>
  68:	mov	x4, #0xfe                  	// #254
  6c:	mov	x1, #0xffffffffffffffff    	// #-1
  70:	mov	w0, #0x14                  	// #20
  74:	b.ne	138 <__trunctfsf2+0x138>  // b.any
  78:	cmp	w6, #0x0
  7c:	add	x2, x1, #0x8
  80:	csel	x1, x2, x1, ne  // ne = any
  84:	and	x2, x1, #0x4000000
  88:	cbnz	w7, 140 <__trunctfsf2+0x140>
  8c:	cbnz	x2, 148 <__trunctfsf2+0x148>
  90:	lsr	x3, x1, #3
  94:	and	w2, w4, #0xff
  98:	mov	w1, w3
  9c:	bfi	w1, w2, #23, #9
  a0:	orr	w19, w1, w6, lsl #31
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	fmov	s0, w19
  ac:	ldr	x19, [sp, #16]
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	orr	x5, x0, x5
  bc:	cbnz	x4, ec <__trunctfsf2+0xec>
  c0:	cbnz	x5, 168 <__trunctfsf2+0x168>
  c4:	mov	w0, #0x0                   	// #0
  c8:	and	w4, w4, #0xff
  cc:	mov	x1, #0x0                   	// #0
  d0:	bfi	w1, w4, #23, #9
  d4:	orr	w19, w1, w6, lsl #31
  d8:	cbnz	w0, a4 <__trunctfsf2+0xa4>
  dc:	fmov	s0, w19
  e0:	ldr	x19, [sp, #16]
  e4:	ldp	x29, x30, [sp], #48
  e8:	ret
  ec:	cbnz	x5, 19c <__trunctfsf2+0x19c>
  f0:	mov	x4, #0xff                  	// #255
  f4:	b	c4 <__trunctfsf2+0xc4>
  f8:	cmp	x4, #0x0
  fc:	b.le	1c0 <__trunctfsf2+0x1c0>
 100:	orr	x5, x5, x0, lsl #39
 104:	mov	w7, #0x0                   	// #0
 108:	cmp	x5, #0x0
 10c:	cset	x1, ne  // ne = any
 110:	orr	x1, x1, x0, lsr #25
 114:	tst	x1, #0x7
 118:	b.eq	258 <__trunctfsf2+0x258>  // b.none
 11c:	and	x2, x3, #0xc00000
 120:	cmp	x2, #0x400, lsl #12
 124:	b.eq	180 <__trunctfsf2+0x180>  // b.none
 128:	cmp	x2, #0x800, lsl #12
 12c:	mov	w0, #0x10                  	// #16
 130:	b.eq	78 <__trunctfsf2+0x78>  // b.none
 134:	cbz	x2, 268 <__trunctfsf2+0x268>
 138:	and	x2, x1, #0x4000000
 13c:	cbz	w7, 144 <__trunctfsf2+0x144>
 140:	orr	w0, w0, #0x8
 144:	cbz	x2, 25c <__trunctfsf2+0x25c>
 148:	cmp	x4, #0xfe
 14c:	add	x4, x4, #0x1
 150:	b.eq	218 <__trunctfsf2+0x218>  // b.none
 154:	ubfx	x2, x1, #3, #23
 158:	orr	w4, w2, w4, lsl #23
 15c:	orr	w19, w4, w6, lsl #31
 160:	bl	0 <__sfp_handle_exceptions>
 164:	b	a8 <__trunctfsf2+0xa8>
 168:	and	x2, x3, #0xc00000
 16c:	mov	w7, #0x1                   	// #1
 170:	cmp	x2, #0x400, lsl #12
 174:	mov	x4, #0x0                   	// #0
 178:	mov	x1, #0x1                   	// #1
 17c:	b.ne	128 <__trunctfsf2+0x128>  // b.any
 180:	cbnz	w6, 188 <__trunctfsf2+0x188>
 184:	add	x1, x1, #0x8
 188:	mov	w0, #0x10                  	// #16
 18c:	b	84 <__trunctfsf2+0x84>
 190:	mov	x4, #0xff                  	// #255
 194:	mov	w0, #0x14                  	// #20
 198:	b	c8 <__trunctfsf2+0xc8>
 19c:	lsr	x2, x0, #50
 1a0:	lsr	x1, x0, #28
 1a4:	eor	w0, w2, #0x1
 1a8:	mov	x2, #0x7fff                	// #32767
 1ac:	cmp	x4, x2
 1b0:	orr	x1, x1, #0x400000
 1b4:	csel	w0, w0, wzr, eq  // eq = none
 1b8:	mov	w4, #0xff                  	// #255
 1bc:	b	d0 <__trunctfsf2+0xd0>
 1c0:	cmn	x4, #0x17
 1c4:	b.lt	168 <__trunctfsf2+0x168>  // b.tstop
 1c8:	orr	x0, x0, #0x8000000000000
 1cc:	add	w2, w4, #0x26
 1d0:	mov	w1, #0x1a                  	// #26
 1d4:	sub	w4, w1, w4
 1d8:	lsl	x2, x0, x2
 1dc:	orr	x5, x2, x5
 1e0:	cmp	x5, #0x0
 1e4:	lsr	x0, x0, x4
 1e8:	cset	x1, ne  // ne = any
 1ec:	orr	x1, x0, x1
 1f0:	ands	x2, x1, #0x7
 1f4:	b.eq	204 <__trunctfsf2+0x204>  // b.none
 1f8:	mov	w7, #0x1                   	// #1
 1fc:	mov	x4, #0x0                   	// #0
 200:	b	11c <__trunctfsf2+0x11c>
 204:	tbz	w3, #11, 254 <__trunctfsf2+0x254>
 208:	mov	w0, #0x0                   	// #0
 20c:	mov	x4, #0x0                   	// #0
 210:	orr	w0, w0, #0x8
 214:	b	144 <__trunctfsf2+0x144>
 218:	mov	w2, w4
 21c:	ands	x3, x3, #0xc00000
 220:	b.eq	248 <__trunctfsf2+0x248>  // b.none
 224:	cmp	x3, #0x400, lsl #12
 228:	b.eq	294 <__trunctfsf2+0x294>  // b.none
 22c:	cmp	x3, #0x800, lsl #12
 230:	mov	w3, #0xfe                  	// #254
 234:	csel	w1, w6, wzr, eq  // eq = none
 238:	mov	x4, #0x1fffffffffffffff    	// #2305843009213693951
 23c:	cmp	w1, #0x0
 240:	csel	w2, w2, w3, ne  // ne = any
 244:	csel	x3, xzr, x4, ne  // ne = any
 248:	mov	w1, #0x14                  	// #20
 24c:	orr	w0, w0, w1
 250:	b	98 <__trunctfsf2+0x98>
 254:	mov	x4, #0x0                   	// #0
 258:	mov	w0, #0x0                   	// #0
 25c:	lsr	x1, x1, #3
 260:	and	w4, w4, #0xff
 264:	b	d0 <__trunctfsf2+0xd0>
 268:	and	x2, x1, #0xf
 26c:	cmp	x2, #0x4
 270:	add	x2, x1, #0x4
 274:	csel	x1, x2, x1, ne  // ne = any
 278:	b	84 <__trunctfsf2+0x84>
 27c:	cbz	w6, 190 <__trunctfsf2+0x190>
 280:	mov	w0, #0x14                  	// #20
 284:	mov	x4, #0xfe                  	// #254
 288:	mov	x1, #0xffffffffffffffff    	// #-1
 28c:	mov	w7, #0x0                   	// #0
 290:	b	84 <__trunctfsf2+0x84>
 294:	cmp	w6, #0x0
 298:	mov	w1, #0xfe                  	// #254
 29c:	csel	w2, w4, w1, eq  // eq = none
 2a0:	mov	x4, #0x1fffffffffffffff    	// #2305843009213693951
 2a4:	csel	x3, xzr, x4, eq  // eq = none
 2a8:	b	248 <__trunctfsf2+0x248>

trunctfdf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x3, x0, [sp, #32]
  14:	mrs	x6, fpcr
  18:	ubfx	x2, x0, #48, #15
  1c:	lsr	x4, x0, #63
  20:	add	x1, x2, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	tst	x1, #0x7ffe
  2c:	and	w4, w4, #0xff
  30:	orr	x0, x0, x3, lsr #61
  34:	lsl	x5, x3, #3
  38:	b.eq	b8 <__trunctfdf2+0xb8>  // b.none
  3c:	mov	x1, #0xffffffffffffc400    	// #-15360
  40:	add	x2, x2, x1
  44:	cmp	x2, #0x7fe
  48:	b.le	fc <__trunctfdf2+0xfc>
  4c:	ands	x0, x6, #0xc00000
  50:	b.eq	194 <__trunctfdf2+0x194>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	2c8 <__trunctfdf2+0x2c8>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w7, w4, wzr, eq  // eq = none
  64:	cbnz	w7, 194 <__trunctfdf2+0x194>
  68:	mov	x1, #0xffffffffffffffff    	// #-1
  6c:	mov	x2, #0x7fe                 	// #2046
  70:	mov	w0, #0x14                  	// #20
  74:	b.ne	140 <__trunctfdf2+0x140>  // b.any
  78:	cmp	w4, #0x0
  7c:	add	x3, x1, #0x8
  80:	csel	x1, x3, x1, ne  // ne = any
  84:	and	x3, x1, #0x80000000000000
  88:	cbnz	w7, 148 <__trunctfdf2+0x148>
  8c:	cbnz	x3, 150 <__trunctfdf2+0x150>
  90:	lsr	x1, x1, #3
  94:	and	w3, w2, #0x7ff
  98:	and	x4, x4, #0xff
  9c:	bfi	x1, x3, #52, #12
  a0:	orr	x19, x1, x4, lsl #63
  a4:	bl	0 <__sfp_handle_exceptions>
  a8:	fmov	d0, x19
  ac:	ldr	x19, [sp, #16]
  b0:	ldp	x29, x30, [sp], #48
  b4:	ret
  b8:	orr	x1, x0, x5
  bc:	cbnz	x2, f0 <__trunctfdf2+0xf0>
  c0:	cbnz	x1, 16c <__trunctfdf2+0x16c>
  c4:	mov	w0, #0x0                   	// #0
  c8:	and	w2, w2, #0x7ff
  cc:	mov	x1, #0x0                   	// #0
  d0:	and	x4, x4, #0xff
  d4:	bfi	x1, x2, #52, #12
  d8:	orr	x19, x1, x4, lsl #63
  dc:	cbnz	w0, a4 <__trunctfdf2+0xa4>
  e0:	fmov	d0, x19
  e4:	ldr	x19, [sp, #16]
  e8:	ldp	x29, x30, [sp], #48
  ec:	ret
  f0:	cbnz	x1, 1a0 <__trunctfdf2+0x1a0>
  f4:	mov	x2, #0x7ff                 	// #2047
  f8:	b	c4 <__trunctfdf2+0xc4>
  fc:	cmp	x2, #0x0
 100:	b.le	1c8 <__trunctfdf2+0x1c8>
 104:	cmp	xzr, x3, lsl #7
 108:	mov	w7, #0x0                   	// #0
 10c:	cset	x1, ne  // ne = any
 110:	orr	x5, x1, x5, lsr #60
 114:	orr	x1, x5, x0, lsl #4
 118:	mov	w0, #0x0                   	// #0
 11c:	tst	x5, #0x7
 120:	b.eq	280 <__trunctfdf2+0x280>  // b.none
 124:	and	x3, x6, #0xc00000
 128:	cmp	x3, #0x400, lsl #12
 12c:	b.eq	184 <__trunctfdf2+0x184>  // b.none
 130:	cmp	x3, #0x800, lsl #12
 134:	mov	w0, #0x10                  	// #16
 138:	b.eq	78 <__trunctfdf2+0x78>  // b.none
 13c:	cbz	x3, 28c <__trunctfdf2+0x28c>
 140:	and	x3, x1, #0x80000000000000
 144:	cbz	w7, 14c <__trunctfdf2+0x14c>
 148:	orr	w0, w0, #0x8
 14c:	cbz	x3, 280 <__trunctfdf2+0x280>
 150:	cmp	x2, #0x7fe
 154:	add	x2, x2, #0x1
 158:	b.eq	228 <__trunctfdf2+0x228>  // b.none
 15c:	mov	x3, #0x1fefffffffffffff    	// #2301339409586323455
 160:	and	w2, w2, #0x7ff
 164:	and	x1, x3, x1, lsr #3
 168:	b	d0 <__trunctfdf2+0xd0>
 16c:	and	x3, x6, #0xc00000
 170:	mov	w7, #0x1                   	// #1
 174:	cmp	x3, #0x400, lsl #12
 178:	mov	x2, #0x0                   	// #0
 17c:	mov	x1, #0x1                   	// #1
 180:	b.ne	130 <__trunctfdf2+0x130>  // b.any
 184:	cbnz	w4, 18c <__trunctfdf2+0x18c>
 188:	add	x1, x1, #0x8
 18c:	mov	w0, #0x10                  	// #16
 190:	b	84 <__trunctfdf2+0x84>
 194:	mov	x2, #0x7ff                 	// #2047
 198:	mov	w0, #0x14                  	// #20
 19c:	b	c8 <__trunctfdf2+0xc8>
 1a0:	mov	x3, #0x7fff                	// #32767
 1a4:	extr	x1, x0, x5, #60
 1a8:	lsr	x0, x0, #50
 1ac:	cmp	x2, x3
 1b0:	lsr	x1, x1, #3
 1b4:	eor	w0, w0, #0x1
 1b8:	orr	x1, x1, #0x8000000000000
 1bc:	csel	w0, w0, wzr, eq  // eq = none
 1c0:	mov	w2, #0x7ff                 	// #2047
 1c4:	b	d0 <__trunctfdf2+0xd0>
 1c8:	cmn	x2, #0x34
 1cc:	b.lt	16c <__trunctfdf2+0x16c>  // b.tstop
 1d0:	mov	x3, #0x3d                  	// #61
 1d4:	sub	x7, x3, x2
 1d8:	orr	x0, x0, #0x8000000000000
 1dc:	cmp	x7, #0x3f
 1e0:	b.le	2a0 <__trunctfdf2+0x2a0>
 1e4:	add	w1, w2, #0x43
 1e8:	cmp	x7, #0x40
 1ec:	mov	w3, #0xfffffffd            	// #-3
 1f0:	sub	w2, w3, w2
 1f4:	lsl	x1, x0, x1
 1f8:	orr	x1, x5, x1
 1fc:	csel	x5, x1, x5, ne  // ne = any
 200:	lsr	x0, x0, x2
 204:	cmp	x5, #0x0
 208:	cset	x1, ne  // ne = any
 20c:	orr	x1, x1, x0
 210:	cmp	x1, #0x0
 214:	cset	w7, ne  // ne = any
 218:	tst	x1, #0x7
 21c:	b.eq	264 <__trunctfdf2+0x264>  // b.none
 220:	mov	x2, #0x0                   	// #0
 224:	b	124 <__trunctfdf2+0x124>
 228:	mov	w3, w2
 22c:	ands	x1, x6, #0xc00000
 230:	b.eq	258 <__trunctfdf2+0x258>  // b.none
 234:	cmp	x1, #0x400, lsl #12
 238:	b.eq	2e0 <__trunctfdf2+0x2e0>  // b.none
 23c:	cmp	x1, #0x800, lsl #12
 240:	mov	w5, #0x7fe                 	// #2046
 244:	csel	w1, w4, wzr, eq  // eq = none
 248:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 24c:	cmp	w1, #0x0
 250:	csel	w3, w3, w5, ne  // ne = any
 254:	csel	x1, xzr, x2, ne  // ne = any
 258:	mov	w2, #0x14                  	// #20
 25c:	orr	w0, w0, w2
 260:	b	98 <__trunctfdf2+0x98>
 264:	and	x3, x1, #0x80000000000000
 268:	cbnz	x1, 2f8 <__trunctfdf2+0x2f8>
 26c:	nop
 270:	mov	w0, #0x0                   	// #0
 274:	mov	x2, #0x1                   	// #1
 278:	cbnz	x3, 15c <__trunctfdf2+0x15c>
 27c:	mov	x2, #0x0                   	// #0
 280:	lsr	x1, x1, #3
 284:	and	w2, w2, #0x7ff
 288:	b	d0 <__trunctfdf2+0xd0>
 28c:	and	x3, x1, #0xf
 290:	cmp	x3, #0x4
 294:	add	x3, x1, #0x4
 298:	csel	x1, x3, x1, ne  // ne = any
 29c:	b	84 <__trunctfdf2+0x84>
 2a0:	add	w1, w2, #0x3
 2a4:	sub	w2, w3, w2
 2a8:	lsl	x3, x5, x1
 2ac:	cmp	x3, #0x0
 2b0:	cset	x3, ne  // ne = any
 2b4:	lsr	x2, x5, x2
 2b8:	orr	x2, x2, x3
 2bc:	lsl	x0, x0, x1
 2c0:	orr	x1, x0, x2
 2c4:	b	210 <__trunctfdf2+0x210>
 2c8:	cbz	w4, 194 <__trunctfdf2+0x194>
 2cc:	mov	x1, #0xffffffffffffffff    	// #-1
 2d0:	mov	x2, #0x7fe                 	// #2046
 2d4:	mov	w7, #0x0                   	// #0
 2d8:	mov	w0, #0x14                  	// #20
 2dc:	b	84 <__trunctfdf2+0x84>
 2e0:	cmp	w4, #0x0
 2e4:	mov	w1, #0x7fe                 	// #2046
 2e8:	csel	w3, w2, w1, eq  // eq = none
 2ec:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 2f0:	csel	x1, xzr, x2, eq  // eq = none
 2f4:	b	258 <__trunctfdf2+0x258>
 2f8:	tbz	w6, #11, 270 <__trunctfdf2+0x270>
 2fc:	mov	w0, #0x0                   	// #0
 300:	mov	x2, #0x0                   	// #0
 304:	orr	w0, w0, #0x8
 308:	b	14c <__trunctfdf2+0x14c>

trunctfhf2.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfhf2>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	str	q0, [sp, #32]
  10:	ldp	x4, x0, [sp, #32]
  14:	mrs	x2, fpcr
  18:	ubfx	x3, x0, #48, #15
  1c:	lsr	x19, x0, #63
  20:	add	x5, x3, #0x1
  24:	ubfiz	x0, x0, #3, #48
  28:	tst	x5, #0x7ffe
  2c:	orr	x0, x0, x4, lsr #61
  30:	and	w5, w19, #0xff
  34:	lsl	x4, x4, #3
  38:	b.eq	bc <__trunctfhf2+0xbc>  // b.none
  3c:	mov	x1, #0xffffffffffffc010    	// #-16368
  40:	add	x3, x3, x1
  44:	cmp	x3, #0x1e
  48:	b.le	108 <__trunctfhf2+0x108>
  4c:	ands	x0, x2, #0xc00000
  50:	b.eq	1a4 <__trunctfhf2+0x1a4>  // b.none
  54:	cmp	x0, #0x400, lsl #12
  58:	b.eq	28c <__trunctfhf2+0x28c>  // b.none
  5c:	cmp	x0, #0x800, lsl #12
  60:	csel	w6, w5, wzr, eq  // eq = none
  64:	cbnz	w6, 1a4 <__trunctfhf2+0x1a4>
  68:	mov	x3, #0x1e                  	// #30
  6c:	mov	x1, #0xffffffffffffffff    	// #-1
  70:	mov	w0, #0x14                  	// #20
  74:	b.ne	148 <__trunctfhf2+0x148>  // b.any
  78:	cmp	w5, #0x0
  7c:	add	x4, x1, #0x8
  80:	csel	x1, x4, x1, ne  // ne = any
  84:	and	x4, x1, #0x2000
  88:	cbnz	w6, 150 <__trunctfhf2+0x150>
  8c:	cbnz	x4, 158 <__trunctfhf2+0x158>
  90:	lsr	x2, x1, #3
  94:	and	w3, w3, #0x1f
  98:	mov	w1, w2
  9c:	bfi	w1, w3, #10, #22
  a0:	orr	w19, w1, w5, lsl #15
  a4:	sxth	x19, w19
  a8:	bl	0 <__sfp_handle_exceptions>
  ac:	dup	v0.4h, w19
  b0:	ldr	x19, [sp, #16]
  b4:	ldp	x29, x30, [sp], #48
  b8:	ret
  bc:	orr	x4, x0, x4
  c0:	cbnz	x3, fc <__trunctfhf2+0xfc>
  c4:	cbnz	x4, 17c <__trunctfhf2+0x17c>
  c8:	mov	w0, #0x0                   	// #0
  cc:	and	w3, w3, #0x1f
  d0:	mov	x19, #0x0                   	// #0
  d4:	nop
  d8:	mov	w1, w19
  dc:	bfi	w1, w3, #10, #22
  e0:	orr	w19, w1, w5, lsl #15
  e4:	sxth	x19, w19
  e8:	cbnz	w0, a8 <__trunctfhf2+0xa8>
  ec:	dup	v0.4h, w19
  f0:	ldr	x19, [sp, #16]
  f4:	ldp	x29, x30, [sp], #48
  f8:	ret
  fc:	cbnz	x4, 1b0 <__trunctfhf2+0x1b0>
 100:	mov	x3, #0x1f                  	// #31
 104:	b	c8 <__trunctfhf2+0xc8>
 108:	cmp	x3, #0x0
 10c:	b.le	1d4 <__trunctfhf2+0x1d4>
 110:	orr	x4, x4, x0, lsl #26
 114:	mov	w6, #0x0                   	// #0
 118:	cmp	x4, #0x0
 11c:	cset	x1, ne  // ne = any
 120:	orr	x1, x1, x0, lsr #38
 124:	tst	x1, #0x7
 128:	b.eq	268 <__trunctfhf2+0x268>  // b.none
 12c:	and	x4, x2, #0xc00000
 130:	cmp	x4, #0x400, lsl #12
 134:	b.eq	194 <__trunctfhf2+0x194>  // b.none
 138:	cmp	x4, #0x800, lsl #12
 13c:	mov	w0, #0x10                  	// #16
 140:	b.eq	78 <__trunctfhf2+0x78>  // b.none
 144:	cbz	x4, 278 <__trunctfhf2+0x278>
 148:	and	x4, x1, #0x2000
 14c:	cbz	w6, 154 <__trunctfhf2+0x154>
 150:	orr	w0, w0, #0x8
 154:	cbz	x4, 26c <__trunctfhf2+0x26c>
 158:	cmp	x3, #0x1e
 15c:	add	x3, x3, #0x1
 160:	b.eq	22c <__trunctfhf2+0x22c>  // b.none
 164:	ubfx	x1, x1, #3, #10
 168:	orr	w3, w1, w3, lsl #10
 16c:	orr	w19, w3, w5, lsl #15
 170:	sxth	x19, w19
 174:	bl	0 <__sfp_handle_exceptions>
 178:	b	ac <__trunctfhf2+0xac>
 17c:	and	x4, x2, #0xc00000
 180:	mov	w6, #0x1                   	// #1
 184:	cmp	x4, #0x400, lsl #12
 188:	mov	x3, #0x0                   	// #0
 18c:	mov	x1, #0x1                   	// #1
 190:	b.ne	138 <__trunctfhf2+0x138>  // b.any
 194:	cbnz	w5, 19c <__trunctfhf2+0x19c>
 198:	add	x1, x1, #0x8
 19c:	mov	w0, #0x10                  	// #16
 1a0:	b	84 <__trunctfhf2+0x84>
 1a4:	mov	x3, #0x1f                  	// #31
 1a8:	mov	w0, #0x14                  	// #20
 1ac:	b	cc <__trunctfhf2+0xcc>
 1b0:	lsr	x2, x0, #50
 1b4:	lsr	x1, x0, #41
 1b8:	eor	w0, w2, #0x1
 1bc:	mov	x2, #0x7fff                	// #32767
 1c0:	cmp	x3, x2
 1c4:	orr	x19, x1, #0x200
 1c8:	csel	w0, w0, wzr, eq  // eq = none
 1cc:	mov	w3, #0x1f                  	// #31
 1d0:	b	d8 <__trunctfhf2+0xd8>
 1d4:	cmn	x3, #0xa
 1d8:	b.lt	17c <__trunctfhf2+0x17c>  // b.tstop
 1dc:	orr	x0, x0, #0x8000000000000
 1e0:	add	w6, w3, #0x19
 1e4:	mov	w1, #0x27                  	// #39
 1e8:	sub	w3, w1, w3
 1ec:	lsl	x6, x0, x6
 1f0:	orr	x4, x6, x4
 1f4:	cmp	x4, #0x0
 1f8:	lsr	x0, x0, x3
 1fc:	cset	x1, ne  // ne = any
 200:	orr	x1, x0, x1
 204:	ands	x4, x1, #0x7
 208:	b.eq	218 <__trunctfhf2+0x218>  // b.none
 20c:	mov	w6, #0x1                   	// #1
 210:	mov	x3, #0x0                   	// #0
 214:	b	12c <__trunctfhf2+0x12c>
 218:	tbz	w2, #11, 264 <__trunctfhf2+0x264>
 21c:	mov	w0, #0x0                   	// #0
 220:	mov	x3, #0x0                   	// #0
 224:	orr	w0, w0, #0x8
 228:	b	154 <__trunctfhf2+0x154>
 22c:	ands	x2, x2, #0xc00000
 230:	b.eq	258 <__trunctfhf2+0x258>  // b.none
 234:	cmp	x2, #0x400, lsl #12
 238:	b.eq	2a4 <__trunctfhf2+0x2a4>  // b.none
 23c:	cmp	x2, #0x800, lsl #12
 240:	mov	w4, #0x1e                  	// #30
 244:	csel	w1, w5, wzr, eq  // eq = none
 248:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 24c:	cmp	w1, #0x0
 250:	csel	w3, w3, w4, ne  // ne = any
 254:	csel	x2, xzr, x2, ne  // ne = any
 258:	mov	w1, #0x14                  	// #20
 25c:	orr	w0, w0, w1
 260:	b	98 <__trunctfhf2+0x98>
 264:	mov	x3, #0x0                   	// #0
 268:	mov	w0, #0x0                   	// #0
 26c:	lsr	x19, x1, #3
 270:	and	w3, w3, #0x1f
 274:	b	d8 <__trunctfhf2+0xd8>
 278:	and	x4, x1, #0xf
 27c:	cmp	x4, #0x4
 280:	add	x4, x1, #0x4
 284:	csel	x1, x4, x1, ne  // ne = any
 288:	b	84 <__trunctfhf2+0x84>
 28c:	cbz	w5, 1a4 <__trunctfhf2+0x1a4>
 290:	mov	w0, #0x14                  	// #20
 294:	mov	x3, #0x1e                  	// #30
 298:	mov	x1, #0xffffffffffffffff    	// #-1
 29c:	mov	w6, #0x0                   	// #0
 2a0:	b	84 <__trunctfhf2+0x84>
 2a4:	cmp	w5, #0x0
 2a8:	mov	w1, #0x1e                  	// #30
 2ac:	mov	x2, #0x1fffffffffffffff    	// #2305843009213693951
 2b0:	csel	w3, w3, w1, eq  // eq = none
 2b4:	csel	x2, xzr, x2, eq  // eq = none
 2b8:	b	258 <__trunctfhf2+0x258>

fixhfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixhfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mrs	x0, fpcr
  10:	umov	w0, v0.h[0]
  14:	mov	w19, #0x0                   	// #0
  18:	bfxil	w19, w0, #0, #16
  1c:	ubfx	x1, x19, #10, #5
  20:	ubfx	x0, x19, #0, #10
  24:	cmp	x1, #0xe
  28:	ubfx	x19, x19, #15, #1
  2c:	b.gt	64 <__fixhfti+0x64>
  30:	and	x0, x0, #0xffff
  34:	mov	x19, #0x0                   	// #0
  38:	orr	x0, x0, x1
  3c:	mov	x1, #0x0                   	// #0
  40:	cbz	x0, 54 <__fixhfti+0x54>
  44:	mov	w0, #0x10                  	// #16
  48:	str	x1, [sp, #40]
  4c:	bl	0 <__sfp_handle_exceptions>
  50:	ldr	x1, [sp, #40]
  54:	mov	x0, x19
  58:	ldr	x19, [sp, #16]
  5c:	ldp	x29, x30, [sp], #48
  60:	ret
  64:	and	x2, x19, #0xff
  68:	cmp	x1, #0x1f
  6c:	b.eq	b8 <__fixhfti+0xb8>  // b.none
  70:	and	x0, x0, #0xffff
  74:	cmp	x1, #0x18
  78:	orr	x0, x0, #0x400
  7c:	b.le	d8 <__fixhfti+0xd8>
  80:	sub	w19, w1, #0x19
  84:	subs	w3, w1, #0x59
  88:	lsr	x4, x0, #1
  8c:	mov	w5, #0x3f                  	// #63
  90:	sub	w5, w5, w19
  94:	lsl	x1, x0, x3
  98:	lsl	x19, x0, x19
  9c:	csel	x19, xzr, x19, pl  // pl = nfrst
  a0:	lsr	x4, x4, x5
  a4:	csel	x1, x1, x4, pl  // pl = nfrst
  a8:	cbz	x2, 54 <__fixhfti+0x54>
  ac:	negs	x19, x19
  b0:	ngc	x1, x1
  b4:	b	54 <__fixhfti+0x54>
  b8:	adrp	x1, 0 <__fixhfti>
  bc:	mov	x0, #0x1                   	// #1
  c0:	sub	x19, x0, x2
  c4:	ldr	x1, [x1]
  c8:	asr	x2, x19, #63
  cc:	negs	x19, x19
  d0:	sbc	x1, x1, x2
  d4:	b	48 <__fixhfti+0x48>
  d8:	add	w4, w1, #0x27
  dc:	mov	w19, #0x19                  	// #25
  e0:	sub	w19, w19, w1
  e4:	mov	x1, #0x0                   	// #0
  e8:	lsl	x4, x0, x4
  ec:	lsr	x19, x0, x19
  f0:	cbz	x2, fc <__fixhfti+0xfc>
  f4:	negs	x19, x19
  f8:	csetm	x1, cc  // cc = lo, ul, last
  fc:	cbnz	x4, 44 <__fixhfti+0x44>
 100:	b	54 <__fixhfti+0x54>

fixunshfti.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunshfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mrs	x0, fpcr
  10:	umov	w0, v0.h[0]
  14:	mov	w1, #0x0                   	// #0
  18:	bfxil	w1, w0, #0, #16
  1c:	and	x19, x1, #0x3ff
  20:	ubfx	x0, x1, #10, #5
  24:	ubfx	x1, x1, #15, #1
  28:	cmp	x0, #0xe
  2c:	b.gt	64 <__fixunshfti+0x64>
  30:	orr	x19, x0, x19
  34:	mov	x1, #0x0                   	// #0
  38:	cbz	x19, 54 <__fixunshfti+0x54>
  3c:	mov	w0, #0x10                  	// #16
  40:	mov	x19, #0x0                   	// #0
  44:	mov	x1, #0x0                   	// #0
  48:	str	x1, [sp, #40]
  4c:	bl	0 <__sfp_handle_exceptions>
  50:	ldr	x1, [sp, #40]
  54:	mov	x0, x19
  58:	ldr	x19, [sp, #16]
  5c:	ldp	x29, x30, [sp], #48
  60:	ret
  64:	cmp	x0, #0x1e
  68:	cset	w3, gt
  6c:	orr	w3, w1, w3
  70:	cbz	w3, 88 <__fixunshfti+0x88>
  74:	eor	w1, w1, #0x1
  78:	mov	w0, #0x1                   	// #1
  7c:	sbfx	x19, x1, #0, #1
  80:	mov	x1, x19
  84:	b	48 <__fixunshfti+0x48>
  88:	mov	x2, x0
  8c:	orr	x19, x19, #0x400
  90:	cmp	x0, #0x18
  94:	b.gt	bc <__fixunshfti+0xbc>
  98:	mov	w1, #0x19                  	// #25
  9c:	add	w0, w0, #0x27
  a0:	sub	w2, w1, w2
  a4:	mov	x1, #0x0                   	// #0
  a8:	lsl	x0, x19, x0
  ac:	lsr	x19, x19, x2
  b0:	cbz	x0, 54 <__fixunshfti+0x54>
  b4:	mov	w0, #0x10                  	// #16
  b8:	b	48 <__fixunshfti+0x48>
  bc:	sub	w4, w0, #0x19
  c0:	subs	w2, w0, #0x59
  c4:	mov	w3, #0x3f                  	// #63
  c8:	lsr	x0, x19, #1
  cc:	sub	w3, w3, w4
  d0:	lsl	x1, x19, x2
  d4:	lsl	x19, x19, x4
  d8:	csel	x19, xzr, x19, pl  // pl = nfrst
  dc:	lsr	x0, x0, x3
  e0:	csel	x1, x1, x0, pl  // pl = nfrst
  e4:	mov	x0, x19
  e8:	ldr	x19, [sp, #16]
  ec:	ldp	x29, x30, [sp], #48
  f0:	ret

floattihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattihf>:
   0:	mrs	x5, fpcr
   4:	orr	x2, x0, x1
   8:	cbnz	x2, 14 <__floattihf+0x14>
   c:	fmov	d0, xzr
  10:	ret
  14:	lsr	x2, x1, #63
  18:	and	w6, w2, #0xff
  1c:	mov	x2, x1
  20:	tbnz	x1, #63, 68 <__floattihf+0x68>
  24:	cbz	x2, 74 <__floattihf+0x74>
  28:	and	x3, x5, #0xc00000
  2c:	mov	x2, x3
  30:	cbnz	x3, 164 <__floattihf+0x164>
  34:	mov	w4, #0x1f                  	// #31
  38:	mov	w0, #0x14                  	// #20
  3c:	bfi	w2, w4, #10, #22
  40:	orr	w1, w2, w6, lsl #15
  44:	sxth	x1, w1
  48:	fmov	d0, x1
  4c:	stp	x29, x30, [sp, #-32]!
  50:	mov	x29, sp
  54:	str	s0, [sp, #28]
  58:	bl	0 <__sfp_handle_exceptions>
  5c:	ldr	s0, [sp, #28]
  60:	ldp	x29, x30, [sp], #32
  64:	ret
  68:	negs	x0, x0
  6c:	ngc	x2, x1
  70:	cbnz	x2, 28 <__floattihf+0x28>
  74:	clz	x3, x0
  78:	mov	w4, #0x4e                  	// #78
  7c:	sub	w3, w4, w3
  80:	mov	x2, x0
  84:	cmp	w3, #0x1e
  88:	sxtw	x4, w3
  8c:	b.gt	28 <__floattihf+0x28>
  90:	cmp	w3, #0x19
  94:	b.le	140 <__floattihf+0x140>
  98:	cmp	x4, #0x1c
  9c:	b.le	1d0 <__floattihf+0x1d0>
  a0:	sub	w0, w3, #0x1c
  a4:	mov	w7, #0x9c                  	// #156
  a8:	sub	w7, w7, w3
  ac:	cmp	w3, #0x5c
  b0:	sub	w8, w7, #0x40
  b4:	lsr	x9, x2, #1
  b8:	mov	w10, #0x3f                  	// #63
  bc:	sub	w10, w10, w7
  c0:	lsr	x0, x2, x0
  c4:	csel	x0, xzr, x0, pl  // pl = nfrst
  c8:	cmp	w8, #0x0
  cc:	lsl	x3, x2, x8
  d0:	lsr	x9, x9, x10
  d4:	csel	x3, x3, x9, ge  // ge = tcont
  d8:	lsl	x2, x2, x7
  dc:	csel	x2, xzr, x2, ge  // ge = tcont
  e0:	orr	x2, x2, x3
  e4:	cmp	x2, #0x0
  e8:	cset	x2, ne  // ne = any
  ec:	orr	x2, x0, x2
  f0:	tst	x2, #0x7
  f4:	and	x2, x2, #0xffffffffffffdfff
  f8:	b.eq	22c <__floattihf+0x22c>  // b.none
  fc:	and	x3, x5, #0xc00000
 100:	cmp	x3, #0x400, lsl #12
 104:	b.eq	2e8 <__floattihf+0x2e8>  // b.none
 108:	cmp	x3, #0x800, lsl #12
 10c:	b.eq	29c <__floattihf+0x29c>  // b.none
 110:	cbz	x3, 2cc <__floattihf+0x2cc>
 114:	and	x5, x2, #0x2000
 118:	mov	w0, #0x10                  	// #16
 11c:	cbz	x5, 230 <__floattihf+0x230>
 120:	cmp	x4, #0x1e
 124:	add	x4, x4, #0x1
 128:	b.eq	2c0 <__floattihf+0x2c0>  // b.none
 12c:	mov	x1, #0xfffffffffffffbff    	// #-1025
 130:	and	w4, w4, #0x1f
 134:	movk	x1, #0x1fff, lsl #48
 138:	and	x2, x1, x2, lsr #3
 13c:	b	3c <__floattihf+0x3c>
 140:	cmp	x4, #0x19
 144:	b.ne	208 <__floattihf+0x208>  // b.any
 148:	and	x0, x0, #0x3ff
 14c:	mov	w1, #0x6400                	// #25600
 150:	orr	w0, w1, w0
 154:	orr	w1, w0, w6, lsl #15
 158:	sxth	x0, w1
 15c:	fmov	d0, x0
 160:	ret
 164:	mvn	x0, x1
 168:	cmp	x3, #0x400, lsl #12
 16c:	cset	w4, eq  // eq = none
 170:	lsr	x2, x0, #63
 174:	tst	w4, w2
 178:	b.ne	2f4 <__floattihf+0x2f4>  // b.any
 17c:	cmp	x3, #0x800, lsl #12
 180:	ccmp	w6, #0x0, #0x4, eq  // eq = none
 184:	b.ne	2f4 <__floattihf+0x2f4>  // b.any
 188:	cmp	x3, #0x400, lsl #12
 18c:	b.eq	27c <__floattihf+0x27c>  // b.none
 190:	cmp	x3, #0x800, lsl #12
 194:	b.eq	264 <__floattihf+0x264>  // b.none
 198:	cmp	x3, #0x400, lsl #12
 19c:	lsr	x0, x0, #63
 1a0:	cset	w1, eq  // eq = none
 1a4:	tst	w1, w0
 1a8:	b.ne	2a8 <__floattihf+0x2a8>  // b.any
 1ac:	cmp	x3, #0x800, lsl #12
 1b0:	ccmp	w6, #0x0, #0x4, eq  // eq = none
 1b4:	b.ne	304 <__floattihf+0x304>  // b.any
 1b8:	mov	w2, #0x7bff                	// #31743
 1bc:	mov	w0, #0x14                  	// #20
 1c0:	orr	w1, w2, w6, lsl #15
 1c4:	sxth	x1, w1
 1c8:	fmov	d0, x1
 1cc:	b	4c <__floattihf+0x4c>
 1d0:	b.eq	f0 <__floattihf+0xf0>  // b.none
 1d4:	mov	w2, #0x1c                  	// #28
 1d8:	sub	w2, w2, w3
 1dc:	lsl	x0, x0, x2
 1e0:	and	x2, x0, #0xffffffffffffdfff
 1e4:	tst	x0, #0x7
 1e8:	b.ne	fc <__floattihf+0xfc>  // b.any
 1ec:	and	w3, w3, #0x1f
 1f0:	ubfx	x2, x2, #3, #10
 1f4:	orr	w2, w2, w3, lsl #10
 1f8:	orr	w1, w2, w6, lsl #15
 1fc:	sxth	x0, w1
 200:	fmov	d0, x0
 204:	ret
 208:	mov	w1, #0x19                  	// #25
 20c:	sub	w1, w1, w3
 210:	and	w3, w3, #0x1f
 214:	lsl	x0, x0, x1
 218:	bfi	w0, w3, #10, #22
 21c:	orr	w1, w0, w6, lsl #15
 220:	sxth	x0, w1
 224:	fmov	d0, x0
 228:	ret
 22c:	mov	w0, #0x0                   	// #0
 230:	lsr	x1, x2, #3
 234:	and	w2, w4, #0x1f
 238:	cmp	x1, #0x0
 23c:	ccmp	x4, #0x1f, #0x0, ne  // ne = any
 240:	b.ne	24c <__floattihf+0x24c>  // b.any
 244:	orr	x1, x1, #0x200
 248:	mov	w2, #0x1f                  	// #31
 24c:	bfi	w1, w2, #10, #22
 250:	orr	w1, w1, w6, lsl #15
 254:	sxth	x1, w1
 258:	fmov	d0, x1
 25c:	cbnz	w0, 4c <__floattihf+0x4c>
 260:	ret
 264:	mov	x2, #0xffffffffffffffff    	// #-1
 268:	mov	x4, #0x1e                  	// #30
 26c:	mov	w0, #0x14                  	// #20
 270:	tbnz	x1, #63, 290 <__floattihf+0x290>
 274:	and	x5, x2, #0x2000
 278:	b	11c <__floattihf+0x11c>
 27c:	mov	x2, #0xffffffffffffffff    	// #-1
 280:	mov	x4, #0x1e                  	// #30
 284:	mov	w0, #0x14                  	// #20
 288:	tbnz	x1, #63, 274 <__floattihf+0x274>
 28c:	nop
 290:	add	x2, x2, #0x8
 294:	and	x5, x2, #0x2000
 298:	b	11c <__floattihf+0x11c>
 29c:	mov	w0, #0x10                  	// #16
 2a0:	tbz	x1, #63, 274 <__floattihf+0x274>
 2a4:	b	290 <__floattihf+0x290>
 2a8:	ubfiz	w1, w6, #15, #1
 2ac:	mov	w0, #0x14                  	// #20
 2b0:	orr	w1, w1, #0x7c00
 2b4:	sxth	x1, w1
 2b8:	fmov	d0, x1
 2bc:	b	4c <__floattihf+0x4c>
 2c0:	cbz	x3, 2a8 <__floattihf+0x2a8>
 2c4:	mvn	x0, x1
 2c8:	b	198 <__floattihf+0x198>
 2cc:	and	x0, x2, #0xf
 2d0:	cmp	x0, #0x4
 2d4:	b.eq	114 <__floattihf+0x114>  // b.none
 2d8:	add	x2, x2, #0x4
 2dc:	mov	w0, #0x10                  	// #16
 2e0:	and	x5, x2, #0x2000
 2e4:	b	11c <__floattihf+0x11c>
 2e8:	mov	w0, #0x10                  	// #16
 2ec:	tbz	x1, #63, 290 <__floattihf+0x290>
 2f0:	b	274 <__floattihf+0x274>
 2f4:	mov	w4, #0x1f                  	// #31
 2f8:	mov	w0, #0x14                  	// #20
 2fc:	mov	x2, #0x0                   	// #0
 300:	b	3c <__floattihf+0x3c>
 304:	mov	x0, #0xfffffffffffffc00    	// #-1024
 308:	fmov	d0, x0
 30c:	mov	w0, #0x14                  	// #20
 310:	b	4c <__floattihf+0x4c>

floatuntihf.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntihf>:
   0:	mrs	x5, fpcr
   4:	orr	x2, x0, x1
   8:	cbnz	x2, 14 <__floatuntihf+0x14>
   c:	fmov	d0, xzr
  10:	ret
  14:	cbz	x1, 54 <__floatuntihf+0x54>
  18:	and	x2, x5, #0x800000
  1c:	mov	w3, #0x1f                  	// #31
  20:	mov	x1, x2
  24:	mov	w0, #0x14                  	// #20
  28:	cbnz	x2, 184 <__floatuntihf+0x184>
  2c:	bfi	w1, w3, #10, #22
  30:	and	x1, x1, #0x7fff
  34:	fmov	d0, x1
  38:	stp	x29, x30, [sp, #-32]!
  3c:	mov	x29, sp
  40:	str	s0, [sp, #28]
  44:	bl	0 <__sfp_handle_exceptions>
  48:	ldr	s0, [sp, #28]
  4c:	ldp	x29, x30, [sp], #32
  50:	ret
  54:	clz	x2, x0
  58:	mov	w1, #0x4e                  	// #78
  5c:	sub	w2, w1, w2
  60:	mov	x1, x0
  64:	cmp	w2, #0x1e
  68:	sxtw	x3, w2
  6c:	b.gt	18 <__floatuntihf+0x18>
  70:	cmp	w2, #0x19
  74:	b.le	118 <__floatuntihf+0x118>
  78:	cmp	x3, #0x1c
  7c:	b.le	134 <__floatuntihf+0x134>
  80:	mov	w4, #0x9c                  	// #156
  84:	sub	w4, w4, w2
  88:	subs	w0, w4, #0x40
  8c:	lsr	x7, x1, #1
  90:	mov	w8, #0x3f                  	// #63
  94:	sub	w8, w8, w4
  98:	lsl	x6, x1, x0
  9c:	sub	w0, w2, #0x1c
  a0:	lsl	x4, x1, x4
  a4:	csel	x4, xzr, x4, pl  // pl = nfrst
  a8:	lsr	x7, x7, x8
  ac:	csel	x6, x6, x7, pl  // pl = nfrst
  b0:	orr	x4, x4, x6
  b4:	lsr	x1, x1, x0
  b8:	cmp	x4, #0x0
  bc:	cset	x0, ne  // ne = any
  c0:	cmp	w2, #0x5c
  c4:	csel	x1, xzr, x1, pl  // pl = nfrst
  c8:	orr	x1, x0, x1
  cc:	tst	x1, #0x7
  d0:	and	x1, x1, #0xffffffffffffdfff
  d4:	b.eq	200 <__floatuntihf+0x200>  // b.none
  d8:	ands	x0, x5, #0xc00000
  dc:	b.eq	1e8 <__floatuntihf+0x1e8>  // b.none
  e0:	cmp	x0, #0x400, lsl #12
  e4:	b.ne	ec <__floatuntihf+0xec>  // b.any
  e8:	add	x1, x1, #0x8
  ec:	and	x0, x1, #0x2000
  f0:	cbz	x0, 224 <__floatuntihf+0x224>
  f4:	cmp	x3, #0x1e
  f8:	add	x3, x3, #0x1
  fc:	b.eq	1bc <__floatuntihf+0x1bc>  // b.none
 100:	mov	x0, #0xfffffffffffffbff    	// #-1025
 104:	and	w3, w3, #0x1f
 108:	movk	x0, #0x1fff, lsl #48
 10c:	and	x1, x0, x1, lsr #3
 110:	mov	w0, #0x10                  	// #16
 114:	b	2c <__floatuntihf+0x2c>
 118:	cmp	x3, #0x19
 11c:	b.ne	164 <__floatuntihf+0x164>  // b.any
 120:	and	x1, x0, #0x3ff
 124:	mov	w0, #0x6400                	// #25600
 128:	orr	w0, w0, w1
 12c:	fmov	s0, w0
 130:	ret
 134:	b.eq	cc <__floatuntihf+0xcc>  // b.none
 138:	mov	w1, #0x1c                  	// #28
 13c:	sub	w1, w1, w2
 140:	lsl	x0, x0, x1
 144:	and	x1, x0, #0xffffffffffffdfff
 148:	tst	x0, #0x7
 14c:	b.ne	d8 <__floatuntihf+0xd8>  // b.any
 150:	ubfx	x1, x1, #3, #10
 154:	ubfiz	x2, x2, #10, #5
 158:	orr	x0, x2, x1
 15c:	fmov	d0, x0
 160:	ret
 164:	mov	w1, #0x19                  	// #25
 168:	sub	w1, w1, w2
 16c:	and	w2, w2, #0x1f
 170:	lsl	x0, x0, x1
 174:	bfi	w0, w2, #10, #22
 178:	and	x0, x0, #0x7fff
 17c:	fmov	d0, x0
 180:	ret
 184:	ands	x0, x5, #0xc00000
 188:	b.eq	1e0 <__floatuntihf+0x1e0>  // b.none
 18c:	cmp	x0, #0x400, lsl #12
 190:	b.ne	1c0 <__floatuntihf+0x1c0>  // b.any
 194:	mov	x1, #0x7                   	// #7
 198:	lsr	x1, x1, #3
 19c:	mov	w2, #0x1e                  	// #30
 1a0:	mov	w0, #0x14                  	// #20
 1a4:	nop
 1a8:	bfi	w1, w2, #10, #22
 1ac:	and	x1, x1, #0x7fff
 1b0:	fmov	d0, x1
 1b4:	cbnz	w0, 38 <__floatuntihf+0x38>
 1b8:	ret
 1bc:	and	x2, x5, #0x800000
 1c0:	mov	x0, #0x7c00                	// #31744
 1c4:	fmov	d0, x0
 1c8:	mov	w0, #0x14                  	// #20
 1cc:	cbz	x2, 38 <__floatuntihf+0x38>
 1d0:	mov	x0, #0x7bff                	// #31743
 1d4:	fmov	d0, x0
 1d8:	mov	w0, #0x14                  	// #20
 1dc:	b	38 <__floatuntihf+0x38>
 1e0:	mov	x1, #0x3                   	// #3
 1e4:	b	198 <__floatuntihf+0x198>
 1e8:	and	x0, x1, #0xf
 1ec:	cmp	x0, #0x4
 1f0:	b.eq	ec <__floatuntihf+0xec>  // b.none
 1f4:	add	x1, x1, #0x4
 1f8:	and	x0, x1, #0x2000
 1fc:	b	f0 <__floatuntihf+0xf0>
 200:	mov	w0, #0x0                   	// #0
 204:	lsr	x1, x1, #3
 208:	and	w2, w2, #0x1f
 20c:	cmp	x1, #0x0
 210:	ccmp	x3, #0x1f, #0x0, ne  // ne = any
 214:	b.ne	1a8 <__floatuntihf+0x1a8>  // b.any
 218:	orr	x1, x1, #0x200
 21c:	mov	w2, #0x1f                  	// #31
 220:	b	1a8 <__floatuntihf+0x1a8>
 224:	mov	w0, #0x10                  	// #16
 228:	b	204 <__floatuntihf+0x204>

enable-execute-stack.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	ret
