
/home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgmp.so.10.4.0_clang_-O1:     file format elf64-littleaarch64


Disassembly of section .init:

000000000000bfb0 <.init>:
    bfb0:	stp	x29, x30, [sp, #-16]!
    bfb4:	mov	x29, sp
    bfb8:	bl	d6c0 <__gmpn_cnd_add_n@plt+0x10>
    bfbc:	ldp	x29, x30, [sp], #16
    bfc0:	ret

Disassembly of section .plt:

000000000000bfd0 <memcpy@plt-0x20>:
    bfd0:	stp	x16, x30, [sp, #-16]!
    bfd4:	adrp	x16, 69000 <__gmp_limbroots_table@@Base+0x11338>
    bfd8:	ldr	x17, [x16, #4088]
    bfdc:	add	x16, x16, #0xff8
    bfe0:	br	x17
    bfe4:	nop
    bfe8:	nop
    bfec:	nop

000000000000bff0 <memcpy@plt>:
    bff0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    bff4:	ldr	x17, [x16]
    bff8:	add	x16, x16, #0x0
    bffc:	br	x17

000000000000c000 <__gmpz_tdiv_r_2exp@plt>:
    c000:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c004:	ldr	x17, [x16, #8]
    c008:	add	x16, x16, #0x8
    c00c:	br	x17

000000000000c010 <__gmpn_zero_p@plt>:
    c010:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c014:	ldr	x17, [x16, #16]
    c018:	add	x16, x16, #0x10
    c01c:	br	x17

000000000000c020 <__gmp_tmp_reentrant_free@plt>:
    c020:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c024:	ldr	x17, [x16, #24]
    c028:	add	x16, x16, #0x18
    c02c:	br	x17

000000000000c030 <__gmpn_tdiv_qr@plt>:
    c030:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c034:	ldr	x17, [x16, #32]
    c038:	add	x16, x16, #0x20
    c03c:	br	x17

000000000000c040 <__gmpq_cmp_ui@plt>:
    c040:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c044:	ldr	x17, [x16, #40]
    c048:	add	x16, x16, #0x28
    c04c:	br	x17

000000000000c050 <__gmpz_scan1@plt>:
    c050:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c054:	ldr	x17, [x16, #48]
    c058:	add	x16, x16, #0x30
    c05c:	br	x17

000000000000c060 <__gmp_randinit_mt_noseed@plt>:
    c060:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c064:	ldr	x17, [x16, #56]
    c068:	add	x16, x16, #0x38
    c06c:	br	x17

000000000000c070 <__gmpn_get_d@plt>:
    c070:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c074:	ldr	x17, [x16, #64]
    c078:	add	x16, x16, #0x40
    c07c:	br	x17

000000000000c080 <__gmpn_sqrmod_bnm1@plt>:
    c080:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c084:	ldr	x17, [x16, #72]
    c088:	add	x16, x16, #0x48
    c08c:	br	x17

000000000000c090 <strlen@plt>:
    c090:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c094:	ldr	x17, [x16, #80]
    c098:	add	x16, x16, #0x50
    c09c:	br	x17

000000000000c0a0 <__gmpf_add@plt>:
    c0a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0a4:	ldr	x17, [x16, #88]
    c0a8:	add	x16, x16, #0x58
    c0ac:	br	x17

000000000000c0b0 <__gmpz_init_set@plt>:
    c0b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0b4:	ldr	x17, [x16, #96]
    c0b8:	add	x16, x16, #0x60
    c0bc:	br	x17

000000000000c0c0 <__gmpn_gcd_1@plt>:
    c0c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0c4:	ldr	x17, [x16, #104]
    c0c8:	add	x16, x16, #0x68
    c0cc:	br	x17

000000000000c0d0 <__gmpz_tdiv_ui@plt>:
    c0d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0d4:	ldr	x17, [x16, #112]
    c0d8:	add	x16, x16, #0x70
    c0dc:	br	x17

000000000000c0e0 <__gmpn_toom_interpolate_12pts@plt>:
    c0e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0e4:	ldr	x17, [x16, #120]
    c0e8:	add	x16, x16, #0x78
    c0ec:	br	x17

000000000000c0f0 <raise@plt>:
    c0f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c0f4:	ldr	x17, [x16, #128]
    c0f8:	add	x16, x16, #0x80
    c0fc:	br	x17

000000000000c100 <__gmp_divide_by_zero@plt>:
    c100:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c104:	ldr	x17, [x16, #136]
    c108:	add	x16, x16, #0x88
    c10c:	br	x17

000000000000c110 <__gmpq_set_str@plt>:
    c110:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c114:	ldr	x17, [x16, #144]
    c118:	add	x16, x16, #0x90
    c11c:	br	x17

000000000000c120 <__gmpz_tdiv_qr@plt>:
    c120:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c124:	ldr	x17, [x16, #152]
    c128:	add	x16, x16, #0x98
    c12c:	br	x17

000000000000c130 <__gmpn_copyd@plt>:
    c130:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c134:	ldr	x17, [x16, #160]
    c138:	add	x16, x16, #0xa0
    c13c:	br	x17

000000000000c140 <__gmpn_matrix22_mul@plt>:
    c140:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c144:	ldr	x17, [x16, #168]
    c148:	add	x16, x16, #0xa8
    c14c:	br	x17

000000000000c150 <__gmpn_add_1@plt>:
    c150:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c154:	ldr	x17, [x16, #176]
    c158:	add	x16, x16, #0xb0
    c15c:	br	x17

000000000000c160 <__gmpn_cnd_sub_n@plt>:
    c160:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c164:	ldr	x17, [x16, #184]
    c168:	add	x16, x16, #0xb8
    c16c:	br	x17

000000000000c170 <__gmpn_gcd_22@plt>:
    c170:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c174:	ldr	x17, [x16, #192]
    c178:	add	x16, x16, #0xc0
    c17c:	br	x17

000000000000c180 <__gmpz_tdiv_q@plt>:
    c180:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c184:	ldr	x17, [x16, #200]
    c188:	add	x16, x16, #0xc8
    c18c:	br	x17

000000000000c190 <__gmpn_toom2_sqr@plt>:
    c190:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c194:	ldr	x17, [x16, #208]
    c198:	add	x16, x16, #0xd0
    c19c:	br	x17

000000000000c1a0 <__gmpn_andn_n@plt>:
    c1a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1a4:	ldr	x17, [x16, #216]
    c1a8:	add	x16, x16, #0xd8
    c1ac:	br	x17

000000000000c1b0 <__gmpz_jacobi@plt>:
    c1b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1b4:	ldr	x17, [x16, #224]
    c1b8:	add	x16, x16, #0xe0
    c1bc:	br	x17

000000000000c1c0 <__gmpz_realloc@plt>:
    c1c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1c4:	ldr	x17, [x16, #232]
    c1c8:	add	x16, x16, #0xe8
    c1cc:	br	x17

000000000000c1d0 <__gmpn_set_str@plt>:
    c1d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1d4:	ldr	x17, [x16, #240]
    c1d8:	add	x16, x16, #0xf0
    c1dc:	br	x17

000000000000c1e0 <__gmpn_toom33_mul@plt>:
    c1e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1e4:	ldr	x17, [x16, #248]
    c1e8:	add	x16, x16, #0xf8
    c1ec:	br	x17

000000000000c1f0 <__gmpn_sqrlo_basecase@plt>:
    c1f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c1f4:	ldr	x17, [x16, #256]
    c1f8:	add	x16, x16, #0x100
    c1fc:	br	x17

000000000000c200 <__gmpz_gcdext@plt>:
    c200:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c204:	ldr	x17, [x16, #264]
    c208:	add	x16, x16, #0x108
    c20c:	br	x17

000000000000c210 <__gmpz_set_str@plt>:
    c210:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c214:	ldr	x17, [x16, #272]
    c218:	add	x16, x16, #0x110
    c21c:	br	x17

000000000000c220 <__gmpn_mu_divappr_q_itch@plt>:
    c220:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c224:	ldr	x17, [x16, #280]
    c228:	add	x16, x16, #0x118
    c22c:	br	x17

000000000000c230 <__gmp_doscan@plt>:
    c230:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c234:	ldr	x17, [x16, #288]
    c238:	add	x16, x16, #0x120
    c23c:	br	x17

000000000000c240 <__gmpn_preinv_mu_div_qr_itch@plt>:
    c240:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c244:	ldr	x17, [x16, #296]
    c248:	add	x16, x16, #0x128
    c24c:	br	x17

000000000000c250 <__gmpz_cmpabs_ui@plt>:
    c250:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c254:	ldr	x17, [x16, #304]
    c258:	add	x16, x16, #0x130
    c25c:	br	x17

000000000000c260 <__gmpn_bc_set_str@plt>:
    c260:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c264:	ldr	x17, [x16, #312]
    c268:	add	x16, x16, #0x138
    c26c:	br	x17

000000000000c270 <__gmpz_sub_ui@plt>:
    c270:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c274:	ldr	x17, [x16, #320]
    c278:	add	x16, x16, #0x140
    c27c:	br	x17

000000000000c280 <__gmpq_get_str@plt>:
    c280:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c284:	ldr	x17, [x16, #328]
    c288:	add	x16, x16, #0x148
    c28c:	br	x17

000000000000c290 <__gmpn_sec_div_r@plt>:
    c290:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c294:	ldr	x17, [x16, #336]
    c298:	add	x16, x16, #0x150
    c29c:	br	x17

000000000000c2a0 <__gmpf_set@plt>:
    c2a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2a4:	ldr	x17, [x16, #344]
    c2a8:	add	x16, x16, #0x158
    c2ac:	br	x17

000000000000c2b0 <__gmpn_sublsh2_n@plt>:
    c2b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2b4:	ldr	x17, [x16, #352]
    c2b8:	add	x16, x16, #0x160
    c2bc:	br	x17

000000000000c2c0 <__gmpz_set_ui@plt>:
    c2c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2c4:	ldr	x17, [x16, #360]
    c2c8:	add	x16, x16, #0x168
    c2cc:	br	x17

000000000000c2d0 <__gmpn_lshift@plt>:
    c2d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2d4:	ldr	x17, [x16, #368]
    c2d8:	add	x16, x16, #0x170
    c2dc:	br	x17

000000000000c2e0 <__gmpn_sqr_basecase@plt>:
    c2e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2e4:	ldr	x17, [x16, #376]
    c2e8:	add	x16, x16, #0x178
    c2ec:	br	x17

000000000000c2f0 <__gmpn_rshift@plt>:
    c2f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c2f4:	ldr	x17, [x16, #384]
    c2f8:	add	x16, x16, #0x180
    c2fc:	br	x17

000000000000c300 <__gmp_invalid_operation@plt>:
    c300:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c304:	ldr	x17, [x16, #392]
    c308:	add	x16, x16, #0x188
    c30c:	br	x17

000000000000c310 <__gmpf_set_str@plt>:
    c310:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c314:	ldr	x17, [x16, #400]
    c318:	add	x16, x16, #0x190
    c31c:	br	x17

000000000000c320 <__cxa_finalize@plt>:
    c320:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c324:	ldr	x17, [x16, #408]
    c328:	add	x16, x16, #0x198
    c32c:	br	x17

000000000000c330 <putc@plt>:
    c330:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c334:	ldr	x17, [x16, #416]
    c338:	add	x16, x16, #0x1a0
    c33c:	br	x17

000000000000c340 <__gmpf_sub_ui@plt>:
    c340:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c344:	ldr	x17, [x16, #424]
    c348:	add	x16, x16, #0x1a8
    c34c:	br	x17

000000000000c350 <__gmpn_divrem_2@plt>:
    c350:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c354:	ldr	x17, [x16, #432]
    c358:	add	x16, x16, #0x1b0
    c35c:	br	x17

000000000000c360 <fputc@plt>:
    c360:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c364:	ldr	x17, [x16, #440]
    c368:	add	x16, x16, #0x1b8
    c36c:	br	x17

000000000000c370 <__gmpn_toom4_sqr@plt>:
    c370:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c374:	ldr	x17, [x16, #448]
    c378:	add	x16, x16, #0x1c0
    c37c:	br	x17

000000000000c380 <__gmpn_sec_powm_itch@plt>:
    c380:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c384:	ldr	x17, [x16, #456]
    c388:	add	x16, x16, #0x1c8
    c38c:	br	x17

000000000000c390 <__gmpn_perfect_power_p@plt>:
    c390:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c394:	ldr	x17, [x16, #464]
    c398:	add	x16, x16, #0x1d0
    c39c:	br	x17

000000000000c3a0 <__gmpn_mod_1_1p@plt>:
    c3a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3a4:	ldr	x17, [x16, #472]
    c3a8:	add	x16, x16, #0x1d8
    c3ac:	br	x17

000000000000c3b0 <__gmpz_sub@plt>:
    c3b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3b4:	ldr	x17, [x16, #480]
    c3b8:	add	x16, x16, #0x1e0
    c3bc:	br	x17

000000000000c3c0 <__gmpn_and_n@plt>:
    c3c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3c4:	ldr	x17, [x16, #488]
    c3c8:	add	x16, x16, #0x1e8
    c3cc:	br	x17

000000000000c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>:
    c3d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3d4:	ldr	x17, [x16, #496]
    c3d8:	add	x16, x16, #0x1f0
    c3dc:	br	x17

000000000000c3e0 <__gmpn_com@plt>:
    c3e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3e4:	ldr	x17, [x16, #504]
    c3e8:	add	x16, x16, #0x1f8
    c3ec:	br	x17

000000000000c3f0 <__gmpn_rootrem@plt>:
    c3f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c3f4:	ldr	x17, [x16, #512]
    c3f8:	add	x16, x16, #0x200
    c3fc:	br	x17

000000000000c400 <__gmpn_hgcd_step@plt>:
    c400:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c404:	ldr	x17, [x16, #520]
    c408:	add	x16, x16, #0x208
    c40c:	br	x17

000000000000c410 <__gmpn_bdiv_dbm1c@plt>:
    c410:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c414:	ldr	x17, [x16, #528]
    c418:	add	x16, x16, #0x210
    c41c:	br	x17

000000000000c420 <__gmpn_sub_n@plt>:
    c420:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c424:	ldr	x17, [x16, #536]
    c428:	add	x16, x16, #0x218
    c42c:	br	x17

000000000000c430 <__gmpn_mu_div_q@plt>:
    c430:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c434:	ldr	x17, [x16, #544]
    c438:	add	x16, x16, #0x220
    c43c:	br	x17

000000000000c440 <snprintf@plt>:
    c440:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c444:	ldr	x17, [x16, #552]
    c448:	add	x16, x16, #0x228
    c44c:	br	x17

000000000000c450 <__gmpn_bc_mulmod_bnm1@plt>:
    c450:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c454:	ldr	x17, [x16, #560]
    c458:	add	x16, x16, #0x230
    c45c:	br	x17

000000000000c460 <__gmpn_mul_fft@plt>:
    c460:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c464:	ldr	x17, [x16, #568]
    c468:	add	x16, x16, #0x238
    c46c:	br	x17

000000000000c470 <__gmpz_setbit@plt>:
    c470:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c474:	ldr	x17, [x16, #576]
    c478:	add	x16, x16, #0x240
    c47c:	br	x17

000000000000c480 <__gmpn_div_q@plt>:
    c480:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c484:	ldr	x17, [x16, #584]
    c488:	add	x16, x16, #0x248
    c48c:	br	x17

000000000000c490 <__gmpf_clear@plt>:
    c490:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c494:	ldr	x17, [x16, #592]
    c498:	add	x16, x16, #0x250
    c49c:	br	x17

000000000000c4a0 <__gmpz_n_pow_ui@plt>:
    c4a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4a4:	ldr	x17, [x16, #600]
    c4a8:	add	x16, x16, #0x258
    c4ac:	br	x17

000000000000c4b0 <__gmpf_get_prec@plt>:
    c4b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4b4:	ldr	x17, [x16, #608]
    c4b8:	add	x16, x16, #0x260
    c4bc:	br	x17

000000000000c4c0 <__gmpn_dc_set_str@plt>:
    c4c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4c4:	ldr	x17, [x16, #616]
    c4c8:	add	x16, x16, #0x268
    c4cc:	br	x17

000000000000c4d0 <__gmpz_powm@plt>:
    c4d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4d4:	ldr	x17, [x16, #624]
    c4d8:	add	x16, x16, #0x270
    c4dc:	br	x17

000000000000c4e0 <__gmpn_sec_add_1@plt>:
    c4e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4e4:	ldr	x17, [x16, #632]
    c4e8:	add	x16, x16, #0x278
    c4ec:	br	x17

000000000000c4f0 <__gmpn_toom_eval_pm2exp@plt>:
    c4f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c4f4:	ldr	x17, [x16, #640]
    c4f8:	add	x16, x16, #0x280
    c4fc:	br	x17

000000000000c500 <__gmpz_get_str@plt>:
    c500:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c504:	ldr	x17, [x16, #648]
    c508:	add	x16, x16, #0x288
    c50c:	br	x17

000000000000c510 <__gmpn_dcpi1_div_qr@plt>:
    c510:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c514:	ldr	x17, [x16, #656]
    c518:	add	x16, x16, #0x290
    c51c:	br	x17

000000000000c520 <__gmpn_powlo@plt>:
    c520:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c524:	ldr	x17, [x16, #664]
    c528:	add	x16, x16, #0x298
    c52c:	br	x17

000000000000c530 <__gmpz_oddfac_1@plt>:
    c530:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c534:	ldr	x17, [x16, #672]
    c538:	add	x16, x16, #0x2a0
    c53c:	br	x17

000000000000c540 <__gmpn_mod_1@plt>:
    c540:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c544:	ldr	x17, [x16, #680]
    c548:	add	x16, x16, #0x2a8
    c54c:	br	x17

000000000000c550 <__gmpz_divexact@plt>:
    c550:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c554:	ldr	x17, [x16, #688]
    c558:	add	x16, x16, #0x2b0
    c55c:	br	x17

000000000000c560 <nl_langinfo@plt>:
    c560:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c564:	ldr	x17, [x16, #696]
    c568:	add	x16, x16, #0x2b8
    c56c:	br	x17

000000000000c570 <__gmpn_cmp@plt>:
    c570:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c574:	ldr	x17, [x16, #704]
    c578:	add	x16, x16, #0x2c0
    c57c:	br	x17

000000000000c580 <malloc@plt>:
    c580:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c584:	ldr	x17, [x16, #712]
    c588:	add	x16, x16, #0x2c8
    c58c:	br	x17

000000000000c590 <__gmpz_set@plt>:
    c590:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c594:	ldr	x17, [x16, #720]
    c598:	add	x16, x16, #0x2d0
    c59c:	br	x17

000000000000c5a0 <__gmpn_divexact@plt>:
    c5a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5a4:	ldr	x17, [x16, #728]
    c5a8:	add	x16, x16, #0x2d8
    c5ac:	br	x17

000000000000c5b0 <__gmpn_sublsh1_n@plt>:
    c5b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5b4:	ldr	x17, [x16, #736]
    c5b8:	add	x16, x16, #0x2e0
    c5bc:	br	x17

000000000000c5c0 <__gmpz_fac_ui@plt>:
    c5c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5c4:	ldr	x17, [x16, #744]
    c5c8:	add	x16, x16, #0x2e8
    c5cc:	br	x17

000000000000c5d0 <__gmpn_mulmid_basecase@plt>:
    c5d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5d4:	ldr	x17, [x16, #752]
    c5d8:	add	x16, x16, #0x2f0
    c5dc:	br	x17

000000000000c5e0 <__gmpn_div_qr_1n_pi1@plt>:
    c5e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5e4:	ldr	x17, [x16, #760]
    c5e8:	add	x16, x16, #0x2f8
    c5ec:	br	x17

000000000000c5f0 <__gmpz_tstbit@plt>:
    c5f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c5f4:	ldr	x17, [x16, #768]
    c5f8:	add	x16, x16, #0x300
    c5fc:	br	x17

000000000000c600 <__gmp_randclear@plt>:
    c600:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c604:	ldr	x17, [x16, #776]
    c608:	add	x16, x16, #0x308
    c60c:	br	x17

000000000000c610 <__gmpf_set_d@plt>:
    c610:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c614:	ldr	x17, [x16, #784]
    c618:	add	x16, x16, #0x310
    c61c:	br	x17

000000000000c620 <__gmpz_mul@plt>:
    c620:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c624:	ldr	x17, [x16, #792]
    c628:	add	x16, x16, #0x318
    c62c:	br	x17

000000000000c630 <__gmpn_sec_tabselect@plt>:
    c630:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c634:	ldr	x17, [x16, #800]
    c638:	add	x16, x16, #0x320
    c63c:	br	x17

000000000000c640 <__gmpn_dcpi1_divappr_q@plt>:
    c640:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c644:	ldr	x17, [x16, #808]
    c648:	add	x16, x16, #0x328
    c64c:	br	x17

000000000000c650 <__gmpn_pi1_bdiv_q_1@plt>:
    c650:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c654:	ldr	x17, [x16, #816]
    c658:	add	x16, x16, #0x330
    c65c:	br	x17

000000000000c660 <__gmpn_matrix22_mul1_inverse_vector@plt>:
    c660:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c664:	ldr	x17, [x16, #824]
    c668:	add	x16, x16, #0x338
    c66c:	br	x17

000000000000c670 <__gmp_vasprintf@plt>:
    c670:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c674:	ldr	x17, [x16, #832]
    c678:	add	x16, x16, #0x340
    c67c:	br	x17

000000000000c680 <__gmpn_sbpi1_bdiv_q@plt>:
    c680:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c684:	ldr	x17, [x16, #840]
    c688:	add	x16, x16, #0x348
    c68c:	br	x17

000000000000c690 <__gmpz_lucas_mod@plt>:
    c690:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c694:	ldr	x17, [x16, #848]
    c698:	add	x16, x16, #0x350
    c69c:	br	x17

000000000000c6a0 <__gmpz_neg@plt>:
    c6a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6a4:	ldr	x17, [x16, #856]
    c6a8:	add	x16, x16, #0x358
    c6ac:	br	x17

000000000000c6b0 <__gmpz_perfect_square_p@plt>:
    c6b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6b4:	ldr	x17, [x16, #864]
    c6b8:	add	x16, x16, #0x360
    c6bc:	br	x17

000000000000c6c0 <__gmpn_toom_eval_pm2@plt>:
    c6c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6c4:	ldr	x17, [x16, #872]
    c6c8:	add	x16, x16, #0x368
    c6cc:	br	x17

000000000000c6d0 <__gmpz_out_str@plt>:
    c6d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6d4:	ldr	x17, [x16, #880]
    c6d8:	add	x16, x16, #0x370
    c6dc:	br	x17

000000000000c6e0 <__gmpn_mul_basecase@plt>:
    c6e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6e4:	ldr	x17, [x16, #888]
    c6e8:	add	x16, x16, #0x378
    c6ec:	br	x17

000000000000c6f0 <__gmpn_gcdext@plt>:
    c6f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c6f4:	ldr	x17, [x16, #896]
    c6f8:	add	x16, x16, #0x380
    c6fc:	br	x17

000000000000c700 <__isoc99_fscanf@plt>:
    c700:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c704:	ldr	x17, [x16, #904]
    c708:	add	x16, x16, #0x388
    c70c:	br	x17

000000000000c710 <__gmpz_swap@plt>:
    c710:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c714:	ldr	x17, [x16, #912]
    c718:	add	x16, x16, #0x390
    c71c:	br	x17

000000000000c720 <__gmpn_hgcd_itch@plt>:
    c720:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c724:	ldr	x17, [x16, #920]
    c728:	add	x16, x16, #0x398
    c72c:	br	x17

000000000000c730 <__gmpn_hgcd2@plt>:
    c730:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c734:	ldr	x17, [x16, #928]
    c738:	add	x16, x16, #0x3a0
    c73c:	br	x17

000000000000c740 <fgetc@plt>:
    c740:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c744:	ldr	x17, [x16, #936]
    c748:	add	x16, x16, #0x3a8
    c74c:	br	x17

000000000000c750 <__gmpz_mul_ui@plt>:
    c750:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c754:	ldr	x17, [x16, #944]
    c758:	add	x16, x16, #0x3b0
    c75c:	br	x17

000000000000c760 <__gmpn_sqrmod_bnm1_next_size@plt>:
    c760:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c764:	ldr	x17, [x16, #952]
    c768:	add	x16, x16, #0x3b8
    c76c:	br	x17

000000000000c770 <__gmpn_dcpi1_bdiv_qr@plt>:
    c770:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c774:	ldr	x17, [x16, #960]
    c778:	add	x16, x16, #0x3c0
    c77c:	br	x17

000000000000c780 <memset@plt>:
    c780:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c784:	ldr	x17, [x16, #968]
    c788:	add	x16, x16, #0x3c8
    c78c:	br	x17

000000000000c790 <__gmpz_2fac_ui@plt>:
    c790:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c794:	ldr	x17, [x16, #976]
    c798:	add	x16, x16, #0x3d0
    c79c:	br	x17

000000000000c7a0 <__gmpn_trialdiv@plt>:
    c7a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7a4:	ldr	x17, [x16, #984]
    c7a8:	add	x16, x16, #0x3d8
    c7ac:	br	x17

000000000000c7b0 <__gmpf_set_si@plt>:
    c7b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7b4:	ldr	x17, [x16, #992]
    c7b8:	add	x16, x16, #0x3e0
    c7bc:	br	x17

000000000000c7c0 <__gmpn_add_err2_n@plt>:
    c7c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7c4:	ldr	x17, [x16, #1000]
    c7c8:	add	x16, x16, #0x3e8
    c7cc:	br	x17

000000000000c7d0 <__gmpn_sbpi1_div_qr@plt>:
    c7d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7d4:	ldr	x17, [x16, #1008]
    c7d8:	add	x16, x16, #0x3f0
    c7dc:	br	x17

000000000000c7e0 <__gmpz_fdiv_q_2exp@plt>:
    c7e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7e4:	ldr	x17, [x16, #1016]
    c7e8:	add	x16, x16, #0x3f8
    c7ec:	br	x17

000000000000c7f0 <__gmpn_cnd_swap@plt>:
    c7f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c7f4:	ldr	x17, [x16, #1024]
    c7f8:	add	x16, x16, #0x400
    c7fc:	br	x17

000000000000c800 <__gmpf_cmp@plt>:
    c800:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c804:	ldr	x17, [x16, #1032]
    c808:	add	x16, x16, #0x408
    c80c:	br	x17

000000000000c810 <__gmpn_mod_1s_4p_cps@plt>:
    c810:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c814:	ldr	x17, [x16, #1040]
    c818:	add	x16, x16, #0x410
    c81c:	br	x17

000000000000c820 <__gmpn_scan0@plt>:
    c820:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c824:	ldr	x17, [x16, #1048]
    c828:	add	x16, x16, #0x418
    c82c:	br	x17

000000000000c830 <__gmpf_set_ui@plt>:
    c830:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c834:	ldr	x17, [x16, #1056]
    c838:	add	x16, x16, #0x420
    c83c:	br	x17

000000000000c840 <__gmpn_brootinv@plt>:
    c840:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c844:	ldr	x17, [x16, #1064]
    c848:	add	x16, x16, #0x428
    c84c:	br	x17

000000000000c850 <__gmp_assert_fail@plt>:
    c850:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c854:	ldr	x17, [x16, #1072]
    c858:	add	x16, x16, #0x430
    c85c:	br	x17

000000000000c860 <__gmpn_preinv_mod_1@plt>:
    c860:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c864:	ldr	x17, [x16, #1080]
    c868:	add	x16, x16, #0x438
    c86c:	br	x17

000000000000c870 <__gmpz_mul_2exp@plt>:
    c870:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c874:	ldr	x17, [x16, #1088]
    c878:	add	x16, x16, #0x440
    c87c:	br	x17

000000000000c880 <__gmpn_sbpi1_divappr_q@plt>:
    c880:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c884:	ldr	x17, [x16, #1096]
    c888:	add	x16, x16, #0x448
    c88c:	br	x17

000000000000c890 <__gmpn_mulmid@plt>:
    c890:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c894:	ldr	x17, [x16, #1104]
    c898:	add	x16, x16, #0x450
    c89c:	br	x17

000000000000c8a0 <__gmpn_mu_divappr_q@plt>:
    c8a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8a4:	ldr	x17, [x16, #1112]
    c8a8:	add	x16, x16, #0x458
    c8ac:	br	x17

000000000000c8b0 <__gmpn_toom44_mul@plt>:
    c8b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8b4:	ldr	x17, [x16, #1120]
    c8b8:	add	x16, x16, #0x460
    c8bc:	br	x17

000000000000c8c0 <__gmpn_jacobi_base@plt>:
    c8c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8c4:	ldr	x17, [x16, #1128]
    c8c8:	add	x16, x16, #0x468
    c8cc:	br	x17

000000000000c8d0 <__gmpn_toom63_mul@plt>:
    c8d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8d4:	ldr	x17, [x16, #1136]
    c8d8:	add	x16, x16, #0x470
    c8dc:	br	x17

000000000000c8e0 <__gmpn_bsqrtinv@plt>:
    c8e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8e4:	ldr	x17, [x16, #1144]
    c8e8:	add	x16, x16, #0x478
    c8ec:	br	x17

000000000000c8f0 <__gmpn_sub_nc@plt>:
    c8f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c8f4:	ldr	x17, [x16, #1152]
    c8f8:	add	x16, x16, #0x480
    c8fc:	br	x17

000000000000c900 <__gmpz_get_ui@plt>:
    c900:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c904:	ldr	x17, [x16, #1160]
    c908:	add	x16, x16, #0x488
    c90c:	br	x17

000000000000c910 <__gmpn_divexact_1@plt>:
    c910:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c914:	ldr	x17, [x16, #1168]
    c918:	add	x16, x16, #0x490
    c91c:	br	x17

000000000000c920 <__gmpn_hgcd_matrix_mul_1@plt>:
    c920:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c924:	ldr	x17, [x16, #1176]
    c928:	add	x16, x16, #0x498
    c92c:	br	x17

000000000000c930 <__gmpn_toom42_mulmid@plt>:
    c930:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c934:	ldr	x17, [x16, #1184]
    c938:	add	x16, x16, #0x4a0
    c93c:	br	x17

000000000000c940 <__gmp_randinit_default@plt>:
    c940:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c944:	ldr	x17, [x16, #1192]
    c948:	add	x16, x16, #0x4a8
    c94c:	br	x17

000000000000c950 <__gmpn_toom_interpolate_8pts@plt>:
    c950:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c954:	ldr	x17, [x16, #1200]
    c958:	add	x16, x16, #0x4b0
    c95c:	br	x17

000000000000c960 <realloc@plt>:
    c960:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c964:	ldr	x17, [x16, #1208]
    c968:	add	x16, x16, #0x4b8
    c96c:	br	x17

000000000000c970 <__gmpn_add@plt>:
    c970:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c974:	ldr	x17, [x16, #1216]
    c978:	add	x16, x16, #0x4c0
    c97c:	br	x17

000000000000c980 <__gmpn_redc_n@plt>:
    c980:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c984:	ldr	x17, [x16, #1224]
    c988:	add	x16, x16, #0x4c8
    c98c:	br	x17

000000000000c990 <__gmpn_modexact_1c_odd@plt>:
    c990:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c994:	ldr	x17, [x16, #1232]
    c998:	add	x16, x16, #0x4d0
    c99c:	br	x17

000000000000c9a0 <getc@plt>:
    c9a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9a4:	ldr	x17, [x16, #1240]
    c9a8:	add	x16, x16, #0x4d8
    c9ac:	br	x17

000000000000c9b0 <__gmpn_sec_pi1_div_r@plt>:
    c9b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9b4:	ldr	x17, [x16, #1248]
    c9b8:	add	x16, x16, #0x4e0
    c9bc:	br	x17

000000000000c9c0 <__gmpn_toom_interpolate_7pts@plt>:
    c9c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9c4:	ldr	x17, [x16, #1256]
    c9c8:	add	x16, x16, #0x4e8
    c9cc:	br	x17

000000000000c9d0 <__gmpn_sbpi1_bdiv_qr@plt>:
    c9d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9d4:	ldr	x17, [x16, #1264]
    c9d8:	add	x16, x16, #0x4f0
    c9dc:	br	x17

000000000000c9e0 <__gmpn_hgcd_matrix_init@plt>:
    c9e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9e4:	ldr	x17, [x16, #1272]
    c9e8:	add	x16, x16, #0x4f8
    c9ec:	br	x17

000000000000c9f0 <__gmpn_rsh1sub_n@plt>:
    c9f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    c9f4:	ldr	x17, [x16, #1280]
    c9f8:	add	x16, x16, #0x500
    c9fc:	br	x17

000000000000ca00 <__gmpn_toom32_mul@plt>:
    ca00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca04:	ldr	x17, [x16, #1288]
    ca08:	add	x16, x16, #0x508
    ca0c:	br	x17

000000000000ca10 <__gmpn_mulmod_bnm1_next_size@plt>:
    ca10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca14:	ldr	x17, [x16, #1296]
    ca18:	add	x16, x16, #0x510
    ca1c:	br	x17

000000000000ca20 <__gmpz_submul_ui@plt>:
    ca20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca24:	ldr	x17, [x16, #1304]
    ca28:	add	x16, x16, #0x518
    ca2c:	br	x17

000000000000ca30 <__gmpn_mu_bdiv_q_itch@plt>:
    ca30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca34:	ldr	x17, [x16, #1312]
    ca38:	add	x16, x16, #0x520
    ca3c:	br	x17

000000000000ca40 <__gmpz_set_d@plt>:
    ca40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca44:	ldr	x17, [x16, #1320]
    ca48:	add	x16, x16, #0x528
    ca4c:	br	x17

000000000000ca50 <__gmpn_hgcd_matrix_adjust@plt>:
    ca50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca54:	ldr	x17, [x16, #1328]
    ca58:	add	x16, x16, #0x530
    ca5c:	br	x17

000000000000ca60 <__gmpz_add_ui@plt>:
    ca60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca64:	ldr	x17, [x16, #1336]
    ca68:	add	x16, x16, #0x538
    ca6c:	br	x17

000000000000ca70 <__gmpn_bdiv_qr_itch@plt>:
    ca70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca74:	ldr	x17, [x16, #1344]
    ca78:	add	x16, x16, #0x540
    ca7c:	br	x17

000000000000ca80 <__gmon_start__@plt>:
    ca80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca84:	ldr	x17, [x16, #1352]
    ca88:	add	x16, x16, #0x548
    ca8c:	br	x17

000000000000ca90 <__gmpn_sqr@plt>:
    ca90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ca94:	ldr	x17, [x16, #1360]
    ca98:	add	x16, x16, #0x550
    ca9c:	br	x17

000000000000caa0 <__gmpz_urandomm@plt>:
    caa0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    caa4:	ldr	x17, [x16, #1368]
    caa8:	add	x16, x16, #0x558
    caac:	br	x17

000000000000cab0 <abort@plt>:
    cab0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cab4:	ldr	x17, [x16, #1376]
    cab8:	add	x16, x16, #0x560
    cabc:	br	x17

000000000000cac0 <__gmpn_toom_interpolate_6pts@plt>:
    cac0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cac4:	ldr	x17, [x16, #1384]
    cac8:	add	x16, x16, #0x568
    cacc:	br	x17

000000000000cad0 <__gmpn_div_qr_2n_pi1@plt>:
    cad0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cad4:	ldr	x17, [x16, #1392]
    cad8:	add	x16, x16, #0x570
    cadc:	br	x17

000000000000cae0 <__gmpn_broot_invm1@plt>:
    cae0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cae4:	ldr	x17, [x16, #1400]
    cae8:	add	x16, x16, #0x578
    caec:	br	x17

000000000000caf0 <__gmpn_sub_1@plt>:
    caf0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    caf4:	ldr	x17, [x16, #1408]
    caf8:	add	x16, x16, #0x580
    cafc:	br	x17

000000000000cb00 <__gmpz_clrbit@plt>:
    cb00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb04:	ldr	x17, [x16, #1416]
    cb08:	add	x16, x16, #0x588
    cb0c:	br	x17

000000000000cb10 <__gmpn_rsh1add_n@plt>:
    cb10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb14:	ldr	x17, [x16, #1424]
    cb18:	add	x16, x16, #0x590
    cb1c:	br	x17

000000000000cb20 <__gmpn_mu_div_qr@plt>:
    cb20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb24:	ldr	x17, [x16, #1432]
    cb28:	add	x16, x16, #0x598
    cb2c:	br	x17

000000000000cb30 <__gmpn_toom_couple_handling@plt>:
    cb30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb34:	ldr	x17, [x16, #1440]
    cb38:	add	x16, x16, #0x5a0
    cb3c:	br	x17

000000000000cb40 <__gmpn_mulmod_bnm1@plt>:
    cb40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb44:	ldr	x17, [x16, #1448]
    cb48:	add	x16, x16, #0x5a8
    cb4c:	br	x17

000000000000cb50 <__gmpn_mul_n@plt>:
    cb50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb54:	ldr	x17, [x16, #1456]
    cb58:	add	x16, x16, #0x5b0
    cb5c:	br	x17

000000000000cb60 <__gmpz_scan0@plt>:
    cb60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb64:	ldr	x17, [x16, #1464]
    cb68:	add	x16, x16, #0x5b8
    cb6c:	br	x17

000000000000cb70 <puts@plt>:
    cb70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb74:	ldr	x17, [x16, #1472]
    cb78:	add	x16, x16, #0x5c0
    cb7c:	br	x17

000000000000cb80 <__gmpz_stronglucas@plt>:
    cb80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb84:	ldr	x17, [x16, #1480]
    cb88:	add	x16, x16, #0x5c8
    cb8c:	br	x17

000000000000cb90 <__gmpz_inp_str_nowhite@plt>:
    cb90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cb94:	ldr	x17, [x16, #1488]
    cb98:	add	x16, x16, #0x5d0
    cb9c:	br	x17

000000000000cba0 <__gmpn_submul_1@plt>:
    cba0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cba4:	ldr	x17, [x16, #1496]
    cba8:	add	x16, x16, #0x5d8
    cbac:	br	x17

000000000000cbb0 <__gmpn_sqrlo@plt>:
    cbb0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cbb4:	ldr	x17, [x16, #1504]
    cbb8:	add	x16, x16, #0x5e0
    cbbc:	br	x17

000000000000cbc0 <__gmpz_divexact_gcd@plt>:
    cbc0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cbc4:	ldr	x17, [x16, #1512]
    cbc8:	add	x16, x16, #0x5e8
    cbcc:	br	x17

000000000000cbd0 <__gmpz_ui_pow_ui@plt>:
    cbd0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cbd4:	ldr	x17, [x16, #1520]
    cbd8:	add	x16, x16, #0x5f0
    cbdc:	br	x17

000000000000cbe0 <__gmpn_toom_interpolate_5pts@plt>:
    cbe0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cbe4:	ldr	x17, [x16, #1528]
    cbe8:	add	x16, x16, #0x5f8
    cbec:	br	x17

000000000000cbf0 <__gmpn_bdiv_q@plt>:
    cbf0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cbf4:	ldr	x17, [x16, #1536]
    cbf8:	add	x16, x16, #0x600
    cbfc:	br	x17

000000000000cc00 <__gmpn_toom53_mul@plt>:
    cc00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc04:	ldr	x17, [x16, #1544]
    cc08:	add	x16, x16, #0x608
    cc0c:	br	x17

000000000000cc10 <__gmpn_copyi@plt>:
    cc10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc14:	ldr	x17, [x16, #1552]
    cc18:	add	x16, x16, #0x610
    cc1c:	br	x17

000000000000cc20 <__gmpq_set_ui@plt>:
    cc20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc24:	ldr	x17, [x16, #1560]
    cc28:	add	x16, x16, #0x618
    cc2c:	br	x17

000000000000cc30 <__gmpn_add_n@plt>:
    cc30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc34:	ldr	x17, [x16, #1568]
    cc38:	add	x16, x16, #0x620
    cc3c:	br	x17

000000000000cc40 <__gmpz_tdiv_r@plt>:
    cc40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc44:	ldr	x17, [x16, #1576]
    cc48:	add	x16, x16, #0x628
    cc4c:	br	x17

000000000000cc50 <__gmpn_get_str@plt>:
    cc50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc54:	ldr	x17, [x16, #1584]
    cc58:	add	x16, x16, #0x630
    cc5c:	br	x17

000000000000cc60 <__gmpn_dcpi1_div_q@plt>:
    cc60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc64:	ldr	x17, [x16, #1592]
    cc68:	add	x16, x16, #0x638
    cc6c:	br	x17

000000000000cc70 <__gmpn_jacobi_2@plt>:
    cc70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc74:	ldr	x17, [x16, #1600]
    cc78:	add	x16, x16, #0x640
    cc7c:	br	x17

000000000000cc80 <__gmpn_mod_1_1p_cps@plt>:
    cc80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc84:	ldr	x17, [x16, #1608]
    cc88:	add	x16, x16, #0x648
    cc8c:	br	x17

000000000000cc90 <__gmpn_fft_best_k@plt>:
    cc90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cc94:	ldr	x17, [x16, #1616]
    cc98:	add	x16, x16, #0x650
    cc9c:	br	x17

000000000000cca0 <__ctype_b_loc@plt>:
    cca0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cca4:	ldr	x17, [x16, #1624]
    cca8:	add	x16, x16, #0x658
    ccac:	br	x17

000000000000ccb0 <__gmpn_hgcd2_jacobi@plt>:
    ccb0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ccb4:	ldr	x17, [x16, #1632]
    ccb8:	add	x16, x16, #0x660
    ccbc:	br	x17

000000000000ccc0 <__gmp_randinit_mt@plt>:
    ccc0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ccc4:	ldr	x17, [x16, #1640]
    ccc8:	add	x16, x16, #0x668
    cccc:	br	x17

000000000000ccd0 <__gmpn_compute_powtab@plt>:
    ccd0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ccd4:	ldr	x17, [x16, #1648]
    ccd8:	add	x16, x16, #0x670
    ccdc:	br	x17

000000000000cce0 <__gmpn_toom8h_mul@plt>:
    cce0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cce4:	ldr	x17, [x16, #1656]
    cce8:	add	x16, x16, #0x678
    ccec:	br	x17

000000000000ccf0 <__gmpz_kronecker_ui@plt>:
    ccf0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ccf4:	ldr	x17, [x16, #1664]
    ccf8:	add	x16, x16, #0x680
    ccfc:	br	x17

000000000000cd00 <__gmpn_xor_n@plt>:
    cd00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd04:	ldr	x17, [x16, #1672]
    cd08:	add	x16, x16, #0x688
    cd0c:	br	x17

000000000000cd10 <__gmpz_clear@plt>:
    cd10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd14:	ldr	x17, [x16, #1680]
    cd18:	add	x16, x16, #0x690
    cd1c:	br	x17

000000000000cd20 <strtol@plt>:
    cd20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd24:	ldr	x17, [x16, #1688]
    cd28:	add	x16, x16, #0x698
    cd2c:	br	x17

000000000000cd30 <__gmpq_set_si@plt>:
    cd30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd34:	ldr	x17, [x16, #1696]
    cd38:	add	x16, x16, #0x6a0
    cd3c:	br	x17

000000000000cd40 <__gmpz_millerrabin@plt>:
    cd40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd44:	ldr	x17, [x16, #1704]
    cd48:	add	x16, x16, #0x6a8
    cd4c:	br	x17

000000000000cd50 <fread@plt>:
    cd50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd54:	ldr	x17, [x16, #1712]
    cd58:	add	x16, x16, #0x6b0
    cd5c:	br	x17

000000000000cd60 <__gmpn_addlsh2_n@plt>:
    cd60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd64:	ldr	x17, [x16, #1720]
    cd68:	add	x16, x16, #0x6b8
    cd6c:	br	x17

000000000000cd70 <__gmpz_mul_si@plt>:
    cd70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd74:	ldr	x17, [x16, #1728]
    cd78:	add	x16, x16, #0x6c0
    cd7c:	br	x17

000000000000cd80 <__gmp_tmp_reentrant_alloc@plt>:
    cd80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd84:	ldr	x17, [x16, #1736]
    cd88:	add	x16, x16, #0x6c8
    cd8c:	br	x17

000000000000cd90 <__gmpz_invert@plt>:
    cd90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cd94:	ldr	x17, [x16, #1744]
    cd98:	add	x16, x16, #0x6d0
    cd9c:	br	x17

000000000000cda0 <__gmpn_rsblsh2_n@plt>:
    cda0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cda4:	ldr	x17, [x16, #1752]
    cda8:	add	x16, x16, #0x6d8
    cdac:	br	x17

000000000000cdb0 <__gmpf_neg@plt>:
    cdb0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cdb4:	ldr	x17, [x16, #1760]
    cdb8:	add	x16, x16, #0x6e0
    cdbc:	br	x17

000000000000cdc0 <__gmpn_ior_n@plt>:
    cdc0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cdc4:	ldr	x17, [x16, #1768]
    cdc8:	add	x16, x16, #0x6e8
    cdcc:	br	x17

000000000000cdd0 <__gmpn_gcd@plt>:
    cdd0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cdd4:	ldr	x17, [x16, #1776]
    cdd8:	add	x16, x16, #0x6f0
    cddc:	br	x17

000000000000cde0 <__gmpn_toom6h_mul@plt>:
    cde0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cde4:	ldr	x17, [x16, #1784]
    cde8:	add	x16, x16, #0x6f8
    cdec:	br	x17

000000000000cdf0 <free@plt>:
    cdf0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cdf4:	ldr	x17, [x16, #1792]
    cdf8:	add	x16, x16, #0x700
    cdfc:	br	x17

000000000000ce00 <__gmpn_addlsh1_n@plt>:
    ce00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce04:	ldr	x17, [x16, #1800]
    ce08:	add	x16, x16, #0x708
    ce0c:	br	x17

000000000000ce10 <ungetc@plt>:
    ce10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce14:	ldr	x17, [x16, #1808]
    ce18:	add	x16, x16, #0x710
    ce1c:	br	x17

000000000000ce20 <__gmpn_sec_powm@plt>:
    ce20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce24:	ldr	x17, [x16, #1816]
    ce28:	add	x16, x16, #0x718
    ce2c:	br	x17

000000000000ce30 <__gmpz_tdiv_q_2exp@plt>:
    ce30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce34:	ldr	x17, [x16, #1824]
    ce38:	add	x16, x16, #0x720
    ce3c:	br	x17

000000000000ce40 <__gmp_nextprime@plt>:
    ce40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce44:	ldr	x17, [x16, #1832]
    ce48:	add	x16, x16, #0x728
    ce4c:	br	x17

000000000000ce50 <__gmpz_roinit_n@plt>:
    ce50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce54:	ldr	x17, [x16, #1840]
    ce58:	add	x16, x16, #0x730
    ce5c:	br	x17

000000000000ce60 <__gmpn_nussbaumer_mul@plt>:
    ce60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce64:	ldr	x17, [x16, #1848]
    ce68:	add	x16, x16, #0x738
    ce6c:	br	x17

000000000000ce70 <__gmpn_mu_bdiv_qr@plt>:
    ce70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce74:	ldr	x17, [x16, #1856]
    ce78:	add	x16, x16, #0x740
    ce7c:	br	x17

000000000000ce80 <__gmpn_mod_1s_2p_cps@plt>:
    ce80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce84:	ldr	x17, [x16, #1864]
    ce88:	add	x16, x16, #0x748
    ce8c:	br	x17

000000000000ce90 <__gmpn_neg@plt>:
    ce90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ce94:	ldr	x17, [x16, #1872]
    ce98:	add	x16, x16, #0x750
    ce9c:	br	x17

000000000000cea0 <__gmpn_mul@plt>:
    cea0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cea4:	ldr	x17, [x16, #1880]
    cea8:	add	x16, x16, #0x758
    ceac:	br	x17

000000000000ceb0 <__gmpn_preinv_divrem_1@plt>:
    ceb0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ceb4:	ldr	x17, [x16, #1888]
    ceb8:	add	x16, x16, #0x760
    cebc:	br	x17

000000000000cec0 <__gmpn_add_err1_n@plt>:
    cec0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cec4:	ldr	x17, [x16, #1896]
    cec8:	add	x16, x16, #0x768
    cecc:	br	x17

000000000000ced0 <__gmpn_divrem_1@plt>:
    ced0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    ced4:	ldr	x17, [x16, #1904]
    ced8:	add	x16, x16, #0x770
    cedc:	br	x17

000000000000cee0 <__gmp_doprnt_integer@plt>:
    cee0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cee4:	ldr	x17, [x16, #1912]
    cee8:	add	x16, x16, #0x778
    ceec:	br	x17

000000000000cef0 <__gmpn_binvert@plt>:
    cef0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cef4:	ldr	x17, [x16, #1920]
    cef8:	add	x16, x16, #0x780
    cefc:	br	x17

000000000000cf00 <__gmpf_mul@plt>:
    cf00:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf04:	ldr	x17, [x16, #1928]
    cf08:	add	x16, x16, #0x788
    cf0c:	br	x17

000000000000cf10 <__gmpn_remove@plt>:
    cf10:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf14:	ldr	x17, [x16, #1936]
    cf18:	add	x16, x16, #0x790
    cf1c:	br	x17

000000000000cf20 <__gmpn_hgcd_appr@plt>:
    cf20:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf24:	ldr	x17, [x16, #1944]
    cf28:	add	x16, x16, #0x798
    cf2c:	br	x17

000000000000cf30 <__gmpn_toom_eval_dgr3_pm2@plt>:
    cf30:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf34:	ldr	x17, [x16, #1952]
    cf38:	add	x16, x16, #0x7a0
    cf3c:	br	x17

000000000000cf40 <__gmpz_prodlimbs@plt>:
    cf40:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf44:	ldr	x17, [x16, #1960]
    cf48:	add	x16, x16, #0x7a8
    cf4c:	br	x17

000000000000cf50 <__gmpn_popcount@plt>:
    cf50:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf54:	ldr	x17, [x16, #1968]
    cf58:	add	x16, x16, #0x7b0
    cf5c:	br	x17

000000000000cf60 <__gmpf_mul_2exp@plt>:
    cf60:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf64:	ldr	x17, [x16, #1976]
    cf68:	add	x16, x16, #0x7b8
    cf6c:	br	x17

000000000000cf70 <strchr@plt>:
    cf70:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf74:	ldr	x17, [x16, #1984]
    cf78:	add	x16, x16, #0x7c0
    cf7c:	br	x17

000000000000cf80 <__gmp_assert_header@plt>:
    cf80:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf84:	ldr	x17, [x16, #1992]
    cf88:	add	x16, x16, #0x7c8
    cf8c:	br	x17

000000000000cf90 <obstack_vprintf@plt>:
    cf90:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cf94:	ldr	x17, [x16, #2000]
    cf98:	add	x16, x16, #0x7d0
    cf9c:	br	x17

000000000000cfa0 <__gmpq_init@plt>:
    cfa0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cfa4:	ldr	x17, [x16, #2008]
    cfa8:	add	x16, x16, #0x7d8
    cfac:	br	x17

000000000000cfb0 <__gmpn_hgcd@plt>:
    cfb0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cfb4:	ldr	x17, [x16, #2016]
    cfb8:	add	x16, x16, #0x7e0
    cfbc:	br	x17

000000000000cfc0 <__gmpz_mod@plt>:
    cfc0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cfc4:	ldr	x17, [x16, #2024]
    cfc8:	add	x16, x16, #0x7e8
    cfcc:	br	x17

000000000000cfd0 <__gmpf_sub@plt>:
    cfd0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cfd4:	ldr	x17, [x16, #2032]
    cfd8:	add	x16, x16, #0x7f0
    cfdc:	br	x17

000000000000cfe0 <__gmpn_dcpi1_bdiv_q@plt>:
    cfe0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cfe4:	ldr	x17, [x16, #2040]
    cfe8:	add	x16, x16, #0x7f8
    cfec:	br	x17

000000000000cff0 <__gmpf_get_str@plt>:
    cff0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    cff4:	ldr	x17, [x16, #2048]
    cff8:	add	x16, x16, #0x800
    cffc:	br	x17

000000000000d000 <fwrite@plt>:
    d000:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d004:	ldr	x17, [x16, #2056]
    d008:	add	x16, x16, #0x808
    d00c:	br	x17

000000000000d010 <__gmpn_hamdist@plt>:
    d010:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d014:	ldr	x17, [x16, #2064]
    d018:	add	x16, x16, #0x810
    d01c:	br	x17

000000000000d020 <__gmpz_init_set_ui@plt>:
    d020:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d024:	ldr	x17, [x16, #2072]
    d028:	add	x16, x16, #0x818
    d02c:	br	x17

000000000000d030 <__gmpf_init@plt>:
    d030:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d034:	ldr	x17, [x16, #2080]
    d038:	add	x16, x16, #0x820
    d03c:	br	x17

000000000000d040 <__gmpz_cmp@plt>:
    d040:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d044:	ldr	x17, [x16, #2088]
    d048:	add	x16, x16, #0x828
    d04c:	br	x17

000000000000d050 <__gmpn_mod_1s_2p@plt>:
    d050:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d054:	ldr	x17, [x16, #2096]
    d058:	add	x16, x16, #0x830
    d05c:	br	x17

000000000000d060 <__gmpn_add_nc@plt>:
    d060:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d064:	ldr	x17, [x16, #2104]
    d068:	add	x16, x16, #0x838
    d06c:	br	x17

000000000000d070 <__gmpn_jacobi_n@plt>:
    d070:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d074:	ldr	x17, [x16, #2112]
    d078:	add	x16, x16, #0x840
    d07c:	br	x17

000000000000d080 <__gmpf_init2@plt>:
    d080:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d084:	ldr	x17, [x16, #2120]
    d088:	add	x16, x16, #0x848
    d08c:	br	x17

000000000000d090 <__gmpn_mullo_n@plt>:
    d090:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d094:	ldr	x17, [x16, #2128]
    d098:	add	x16, x16, #0x850
    d09c:	br	x17

000000000000d0a0 <__gmpf_div@plt>:
    d0a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0a4:	ldr	x17, [x16, #2136]
    d0a8:	add	x16, x16, #0x858
    d0ac:	br	x17

000000000000d0b0 <__gmpn_sbpi1_div_q@plt>:
    d0b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0b4:	ldr	x17, [x16, #2144]
    d0b8:	add	x16, x16, #0x860
    d0bc:	br	x17

000000000000d0c0 <__gmpn_sec_pi1_div_qr@plt>:
    d0c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0c4:	ldr	x17, [x16, #2152]
    d0c8:	add	x16, x16, #0x868
    d0cc:	br	x17

000000000000d0d0 <__gmpn_toom43_mul@plt>:
    d0d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0d4:	ldr	x17, [x16, #2160]
    d0d8:	add	x16, x16, #0x870
    d0dc:	br	x17

000000000000d0e0 <vsprintf@plt>:
    d0e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0e4:	ldr	x17, [x16, #2168]
    d0e8:	add	x16, x16, #0x878
    d0ec:	br	x17

000000000000d0f0 <__gmpn_div_qr_2u_pi1@plt>:
    d0f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d0f4:	ldr	x17, [x16, #2176]
    d0f8:	add	x16, x16, #0x880
    d0fc:	br	x17

000000000000d100 <__gmpn_zero@plt>:
    d100:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d104:	ldr	x17, [x16, #2184]
    d108:	add	x16, x16, #0x888
    d10c:	br	x17

000000000000d110 <__gmp_randinit_lc_2exp@plt>:
    d110:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d114:	ldr	x17, [x16, #2192]
    d118:	add	x16, x16, #0x890
    d11c:	br	x17

000000000000d120 <__gmpn_bdiv_qr@plt>:
    d120:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d124:	ldr	x17, [x16, #2200]
    d128:	add	x16, x16, #0x898
    d12c:	br	x17

000000000000d130 <__gmpn_mod_34lsub1@plt>:
    d130:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d134:	ldr	x17, [x16, #2208]
    d138:	add	x16, x16, #0x8a0
    d13c:	br	x17

000000000000d140 <__gmpz_gcd@plt>:
    d140:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d144:	ldr	x17, [x16, #2216]
    d148:	add	x16, x16, #0x8a8
    d14c:	br	x17

000000000000d150 <__gmpz_aorsmul_1@plt>:
    d150:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d154:	ldr	x17, [x16, #2224]
    d158:	add	x16, x16, #0x8b0
    d15c:	br	x17

000000000000d160 <__gmpz_add@plt>:
    d160:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d164:	ldr	x17, [x16, #2232]
    d168:	add	x16, x16, #0x8b8
    d16c:	br	x17

000000000000d170 <__gmpn_hgcd_matrix_mul@plt>:
    d170:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d174:	ldr	x17, [x16, #2240]
    d178:	add	x16, x16, #0x8c0
    d17c:	br	x17

000000000000d180 <__gmp_randseed@plt>:
    d180:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d184:	ldr	x17, [x16, #2248]
    d188:	add	x16, x16, #0x8c8
    d18c:	br	x17

000000000000d190 <__gmpn_toom_eval_pm1@plt>:
    d190:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d194:	ldr	x17, [x16, #2256]
    d198:	add	x16, x16, #0x8d0
    d19c:	br	x17

000000000000d1a0 <__gmpn_mu_div_q_itch@plt>:
    d1a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1a4:	ldr	x17, [x16, #2264]
    d1a8:	add	x16, x16, #0x8d8
    d1ac:	br	x17

000000000000d1b0 <__gmpq_mul@plt>:
    d1b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1b4:	ldr	x17, [x16, #2272]
    d1b8:	add	x16, x16, #0x8e0
    d1bc:	br	x17

000000000000d1c0 <__gmp_sqrt_of_negative@plt>:
    d1c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1c4:	ldr	x17, [x16, #2280]
    d1c8:	add	x16, x16, #0x8e8
    d1cc:	br	x17

000000000000d1d0 <__gmpn_powm@plt>:
    d1d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1d4:	ldr	x17, [x16, #2288]
    d1d8:	add	x16, x16, #0x8f0
    d1dc:	br	x17

000000000000d1e0 <__gmpn_mu_div_qr_itch@plt>:
    d1e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1e4:	ldr	x17, [x16, #2296]
    d1e8:	add	x16, x16, #0x8f8
    d1ec:	br	x17

000000000000d1f0 <__gmpn_hgcd_matrix_update_q@plt>:
    d1f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d1f4:	ldr	x17, [x16, #2304]
    d1f8:	add	x16, x16, #0x900
    d1fc:	br	x17

000000000000d200 <__gmp_doprnt@plt>:
    d200:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d204:	ldr	x17, [x16, #2312]
    d208:	add	x16, x16, #0x908
    d20c:	br	x17

000000000000d210 <_obstack_newchunk@plt>:
    d210:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d214:	ldr	x17, [x16, #2320]
    d218:	add	x16, x16, #0x910
    d21c:	br	x17

000000000000d220 <__gmpn_fib2m@plt>:
    d220:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d224:	ldr	x17, [x16, #2328]
    d228:	add	x16, x16, #0x918
    d22c:	br	x17

000000000000d230 <__gmpn_invertappr@plt>:
    d230:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d234:	ldr	x17, [x16, #2336]
    d238:	add	x16, x16, #0x920
    d23c:	br	x17

000000000000d240 <__gmpn_fib2_ui@plt>:
    d240:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d244:	ldr	x17, [x16, #2344]
    d248:	add	x16, x16, #0x928
    d24c:	br	x17

000000000000d250 <__gmpn_preinv_mu_div_qr@plt>:
    d250:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d254:	ldr	x17, [x16, #2352]
    d258:	add	x16, x16, #0x930
    d25c:	br	x17

000000000000d260 <__gmpn_rsblsh1_n@plt>:
    d260:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d264:	ldr	x17, [x16, #2360]
    d268:	add	x16, x16, #0x938
    d26c:	br	x17

000000000000d270 <__gmpz_init_set_str@plt>:
    d270:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d274:	ldr	x17, [x16, #2368]
    d278:	add	x16, x16, #0x940
    d27c:	br	x17

000000000000d280 <__gmpn_perfect_square_p@plt>:
    d280:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d284:	ldr	x17, [x16, #2376]
    d288:	add	x16, x16, #0x948
    d28c:	br	x17

000000000000d290 <__gmpz_fdiv_r_2exp@plt>:
    d290:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d294:	ldr	x17, [x16, #2384]
    d298:	add	x16, x16, #0x950
    d29c:	br	x17

000000000000d2a0 <__gmpz_inp_str@plt>:
    d2a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2a4:	ldr	x17, [x16, #2392]
    d2a8:	add	x16, x16, #0x958
    d2ac:	br	x17

000000000000d2b0 <__gmpn_toom_eval_pm2rexp@plt>:
    d2b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2b4:	ldr	x17, [x16, #2400]
    d2b8:	add	x16, x16, #0x960
    d2bc:	br	x17

000000000000d2c0 <__gmpn_redc_1@plt>:
    d2c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2c4:	ldr	x17, [x16, #2408]
    d2c8:	add	x16, x16, #0x968
    d2cc:	br	x17

000000000000d2d0 <__isoc99_sscanf@plt>:
    d2d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2d4:	ldr	x17, [x16, #2416]
    d2d8:	add	x16, x16, #0x970
    d2dc:	br	x17

000000000000d2e0 <vsnprintf@plt>:
    d2e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2e4:	ldr	x17, [x16, #2424]
    d2e8:	add	x16, x16, #0x978
    d2ec:	br	x17

000000000000d2f0 <__gmpn_strongfibo@plt>:
    d2f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d2f4:	ldr	x17, [x16, #2432]
    d2f8:	add	x16, x16, #0x980
    d2fc:	br	x17

000000000000d300 <__gmpz_init2@plt>:
    d300:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d304:	ldr	x17, [x16, #2440]
    d308:	add	x16, x16, #0x988
    d30c:	br	x17

000000000000d310 <__gmpn_gcdext_1@plt>:
    d310:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d314:	ldr	x17, [x16, #2448]
    d318:	add	x16, x16, #0x990
    d31c:	br	x17

000000000000d320 <__gmpn_scan1@plt>:
    d320:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d324:	ldr	x17, [x16, #2456]
    d328:	add	x16, x16, #0x998
    d32c:	br	x17

000000000000d330 <__gmpn_lshiftc@plt>:
    d330:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d334:	ldr	x17, [x16, #2464]
    d338:	add	x16, x16, #0x9a0
    d33c:	br	x17

000000000000d340 <__gmpn_sub@plt>:
    d340:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d344:	ldr	x17, [x16, #2472]
    d348:	add	x16, x16, #0x9a8
    d34c:	br	x17

000000000000d350 <__gmpn_mu_bdiv_qr_itch@plt>:
    d350:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d354:	ldr	x17, [x16, #2480]
    d358:	add	x16, x16, #0x9b0
    d35c:	br	x17

000000000000d360 <__gmpn_ni_invertappr@plt>:
    d360:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d364:	ldr	x17, [x16, #2488]
    d368:	add	x16, x16, #0x9b8
    d36c:	br	x17

000000000000d370 <__gmp_randinit_lc_2exp_size@plt>:
    d370:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d374:	ldr	x17, [x16, #2496]
    d378:	add	x16, x16, #0x9c0
    d37c:	br	x17

000000000000d380 <__gmp_init_primesieve@plt>:
    d380:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d384:	ldr	x17, [x16, #2504]
    d388:	add	x16, x16, #0x9c8
    d38c:	br	x17

000000000000d390 <__gmpn_gcdext_lehmer_n@plt>:
    d390:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d394:	ldr	x17, [x16, #2512]
    d398:	add	x16, x16, #0x9d0
    d39c:	br	x17

000000000000d3a0 <__gmpn_random2@plt>:
    d3a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3a4:	ldr	x17, [x16, #2520]
    d3a8:	add	x16, x16, #0x9d8
    d3ac:	br	x17

000000000000d3b0 <__gmpn_fft_next_size@plt>:
    d3b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3b4:	ldr	x17, [x16, #2528]
    d3b8:	add	x16, x16, #0x9e0
    d3bc:	br	x17

000000000000d3c0 <__gmpn_binvert_itch@plt>:
    d3c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3c4:	ldr	x17, [x16, #2536]
    d3c8:	add	x16, x16, #0x9e8
    d3cc:	br	x17

000000000000d3d0 <__gmpz_cmp_ui@plt>:
    d3d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3d4:	ldr	x17, [x16, #2544]
    d3d8:	add	x16, x16, #0x9f0
    d3dc:	br	x17

000000000000d3e0 <__gmp_primesieve@plt>:
    d3e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3e4:	ldr	x17, [x16, #2552]
    d3e8:	add	x16, x16, #0x9f8
    d3ec:	br	x17

000000000000d3f0 <__gmpn_pow_1@plt>:
    d3f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d3f4:	ldr	x17, [x16, #2560]
    d3f8:	add	x16, x16, #0xa00
    d3fc:	br	x17

000000000000d400 <__gmpz_export@plt>:
    d400:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d404:	ldr	x17, [x16, #2568]
    d408:	add	x16, x16, #0xa08
    d40c:	br	x17

000000000000d410 <__gmp_doprnt_mpf2@plt>:
    d410:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d414:	ldr	x17, [x16, #2576]
    d418:	add	x16, x16, #0xa10
    d41c:	br	x17

000000000000d420 <__gmpn_mul_1c@plt>:
    d420:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d424:	ldr	x17, [x16, #2584]
    d428:	add	x16, x16, #0xa18
    d42c:	br	x17

000000000000d430 <__gmpz_init@plt>:
    d430:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d434:	ldr	x17, [x16, #2592]
    d438:	add	x16, x16, #0xa20
    d43c:	br	x17

000000000000d440 <__gmpz_sizeinbase@plt>:
    d440:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d444:	ldr	x17, [x16, #2600]
    d448:	add	x16, x16, #0xa28
    d44c:	br	x17

000000000000d450 <__gmpz_set_si@plt>:
    d450:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d454:	ldr	x17, [x16, #2608]
    d458:	add	x16, x16, #0xa30
    d45c:	br	x17

000000000000d460 <__gmp_extract_double@plt>:
    d460:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d464:	ldr	x17, [x16, #2616]
    d468:	add	x16, x16, #0xa38
    d46c:	br	x17

000000000000d470 <__gmpn_mullo_basecase@plt>:
    d470:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d474:	ldr	x17, [x16, #2624]
    d478:	add	x16, x16, #0xa40
    d47c:	br	x17

000000000000d480 <__gmpn_toom3_sqr@plt>:
    d480:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d484:	ldr	x17, [x16, #2632]
    d488:	add	x16, x16, #0xa48
    d48c:	br	x17

000000000000d490 <__gmpn_gcd_subdiv_step@plt>:
    d490:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d494:	ldr	x17, [x16, #2640]
    d498:	add	x16, x16, #0xa50
    d49c:	br	x17

000000000000d4a0 <__gmpz_powm_ui@plt>:
    d4a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4a4:	ldr	x17, [x16, #2648]
    d4a8:	add	x16, x16, #0xa58
    d4ac:	br	x17

000000000000d4b0 <vfprintf@plt>:
    d4b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4b4:	ldr	x17, [x16, #2656]
    d4b8:	add	x16, x16, #0xa60
    d4bc:	br	x17

000000000000d4c0 <printf@plt>:
    d4c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4c4:	ldr	x17, [x16, #2664]
    d4c8:	add	x16, x16, #0xa68
    d4cc:	br	x17

000000000000d4d0 <__gmpn_hgcd_reduce@plt>:
    d4d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4d4:	ldr	x17, [x16, #2672]
    d4d8:	add	x16, x16, #0xa70
    d4dc:	br	x17

000000000000d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>:
    d4e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4e4:	ldr	x17, [x16, #2680]
    d4e8:	add	x16, x16, #0xa78
    d4ec:	br	x17

000000000000d4f0 <putchar@plt>:
    d4f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d4f4:	ldr	x17, [x16, #2688]
    d4f8:	add	x16, x16, #0xa80
    d4fc:	br	x17

000000000000d500 <__gmpz_addmul_ui@plt>:
    d500:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d504:	ldr	x17, [x16, #2696]
    d508:	add	x16, x16, #0xa88
    d50c:	br	x17

000000000000d510 <__gmpn_sqr_diag_addlsh1@plt>:
    d510:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d514:	ldr	x17, [x16, #2704]
    d518:	add	x16, x16, #0xa90
    d51c:	br	x17

000000000000d520 <__gmpn_gcd_11@plt>:
    d520:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d524:	ldr	x17, [x16, #2712]
    d528:	add	x16, x16, #0xa98
    d52c:	br	x17

000000000000d530 <__gmpn_toom_interpolate_16pts@plt>:
    d530:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d534:	ldr	x17, [x16, #2720]
    d538:	add	x16, x16, #0xaa0
    d53c:	br	x17

000000000000d540 <__gmpn_divisible_p@plt>:
    d540:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d544:	ldr	x17, [x16, #2728]
    d548:	add	x16, x16, #0xaa8
    d54c:	br	x17

000000000000d550 <__gmpn_sub_err2_n@plt>:
    d550:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d554:	ldr	x17, [x16, #2736]
    d558:	add	x16, x16, #0xab0
    d55c:	br	x17

000000000000d560 <__gmpn_bdiv_q_itch@plt>:
    d560:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d564:	ldr	x17, [x16, #2744]
    d568:	add	x16, x16, #0xab8
    d56c:	br	x17

000000000000d570 <__gmpn_hgcd_jacobi@plt>:
    d570:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d574:	ldr	x17, [x16, #2752]
    d578:	add	x16, x16, #0xac0
    d57c:	br	x17

000000000000d580 <__gmpn_divrem@plt>:
    d580:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d584:	ldr	x17, [x16, #2760]
    d588:	add	x16, x16, #0xac8
    d58c:	br	x17

000000000000d590 <__gmpn_sqrtrem@plt>:
    d590:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d594:	ldr	x17, [x16, #2768]
    d598:	add	x16, x16, #0xad0
    d59c:	br	x17

000000000000d5a0 <__gmpn_mu_bdiv_q@plt>:
    d5a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5a4:	ldr	x17, [x16, #2776]
    d5a8:	add	x16, x16, #0xad8
    d5ac:	br	x17

000000000000d5b0 <__gmp_exception@plt>:
    d5b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5b4:	ldr	x17, [x16, #2784]
    d5b8:	add	x16, x16, #0xae0
    d5bc:	br	x17

000000000000d5c0 <__gmpn_dcpi1_div_qr_n@plt>:
    d5c0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5c4:	ldr	x17, [x16, #2792]
    d5c8:	add	x16, x16, #0xae8
    d5cc:	br	x17

000000000000d5d0 <__gmpn_invert_limb@plt>:
    d5d0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5d4:	ldr	x17, [x16, #2800]
    d5d8:	add	x16, x16, #0xaf0
    d5dc:	br	x17

000000000000d5e0 <__gmpn_addmul_1@plt>:
    d5e0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5e4:	ldr	x17, [x16, #2808]
    d5e8:	add	x16, x16, #0xaf8
    d5ec:	br	x17

000000000000d5f0 <__gmpn_mod_1s_4p@plt>:
    d5f0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d5f4:	ldr	x17, [x16, #2816]
    d5f8:	add	x16, x16, #0xb00
    d5fc:	br	x17

000000000000d600 <fprintf@plt>:
    d600:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d604:	ldr	x17, [x16, #2824]
    d608:	add	x16, x16, #0xb08
    d60c:	br	x17

000000000000d610 <__gmpz_urandomb@plt>:
    d610:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d614:	ldr	x17, [x16, #2832]
    d618:	add	x16, x16, #0xb10
    d61c:	br	x17

000000000000d620 <__gmpn_hgcd_mul_matrix1_vector@plt>:
    d620:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d624:	ldr	x17, [x16, #2840]
    d628:	add	x16, x16, #0xb18
    d62c:	br	x17

000000000000d630 <__gmpn_toom22_mul@plt>:
    d630:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d634:	ldr	x17, [x16, #2848]
    d638:	add	x16, x16, #0xb20
    d63c:	br	x17

000000000000d640 <__gmp_mt_recalc_buffer@plt>:
    d640:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d644:	ldr	x17, [x16, #2856]
    d648:	add	x16, x16, #0xb28
    d64c:	br	x17

000000000000d650 <__gmpn_toom6_sqr@plt>:
    d650:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d654:	ldr	x17, [x16, #2864]
    d658:	add	x16, x16, #0xb30
    d65c:	br	x17

000000000000d660 <__gmpn_toom42_mul@plt>:
    d660:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d664:	ldr	x17, [x16, #2872]
    d668:	add	x16, x16, #0xb38
    d66c:	br	x17

000000000000d670 <__gmpn_mul_1@plt>:
    d670:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d674:	ldr	x17, [x16, #2880]
    d678:	add	x16, x16, #0xb40
    d67c:	br	x17

000000000000d680 <ferror@plt>:
    d680:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d684:	ldr	x17, [x16, #2888]
    d688:	add	x16, x16, #0xb48
    d68c:	br	x17

000000000000d690 <__gmpn_toom8_sqr@plt>:
    d690:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d694:	ldr	x17, [x16, #2896]
    d698:	add	x16, x16, #0xb50
    d69c:	br	x17

000000000000d6a0 <__gmpf_div_2exp@plt>:
    d6a0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d6a4:	ldr	x17, [x16, #2904]
    d6a8:	add	x16, x16, #0xb58
    d6ac:	br	x17

000000000000d6b0 <__gmpn_cnd_add_n@plt>:
    d6b0:	adrp	x16, 6a000 <memcpy@GLIBC_2.17>
    d6b4:	ldr	x17, [x16, #2912]
    d6b8:	add	x16, x16, #0xb60
    d6bc:	br	x17

Disassembly of section .text:

000000000000d6c0 <__gmp_assert_header@@Base-0xd4>:
    d6c0:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d6c4:	ldr	x0, [x0, #3904]
    d6c8:	cbz	x0, d6d0 <__gmpn_cnd_add_n@plt+0x20>
    d6cc:	b	ca80 <__gmon_start__@plt>
    d6d0:	ret
    d6d4:	nop
    d6d8:	adrp	x0, 6a000 <memcpy@GLIBC_2.17>
    d6dc:	add	x0, x0, #0xb90
    d6e0:	adrp	x1, 6a000 <memcpy@GLIBC_2.17>
    d6e4:	add	x1, x1, #0xb90
    d6e8:	cmp	x1, x0
    d6ec:	b.eq	d704 <__gmpn_cnd_add_n@plt+0x54>  // b.none
    d6f0:	adrp	x1, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d6f4:	ldr	x1, [x1, #3784]
    d6f8:	cbz	x1, d704 <__gmpn_cnd_add_n@plt+0x54>
    d6fc:	mov	x16, x1
    d700:	br	x16
    d704:	ret
    d708:	adrp	x0, 6a000 <memcpy@GLIBC_2.17>
    d70c:	add	x0, x0, #0xb90
    d710:	adrp	x1, 6a000 <memcpy@GLIBC_2.17>
    d714:	add	x1, x1, #0xb90
    d718:	sub	x1, x1, x0
    d71c:	lsr	x2, x1, #63
    d720:	add	x1, x2, x1, asr #3
    d724:	cmp	xzr, x1, asr #1
    d728:	asr	x1, x1, #1
    d72c:	b.eq	d744 <__gmpn_cnd_add_n@plt+0x94>  // b.none
    d730:	adrp	x2, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d734:	ldr	x2, [x2, #4056]
    d738:	cbz	x2, d744 <__gmpn_cnd_add_n@plt+0x94>
    d73c:	mov	x16, x2
    d740:	br	x16
    d744:	ret
    d748:	stp	x29, x30, [sp, #-32]!
    d74c:	mov	x29, sp
    d750:	str	x19, [sp, #16]
    d754:	adrp	x19, 6a000 <memcpy@GLIBC_2.17>
    d758:	ldrb	w0, [x19, #2960]
    d75c:	cbnz	w0, d784 <__gmpn_cnd_add_n@plt+0xd4>
    d760:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d764:	ldr	x0, [x0, #3800]
    d768:	cbz	x0, d778 <__gmpn_cnd_add_n@plt+0xc8>
    d76c:	adrp	x0, 6a000 <memcpy@GLIBC_2.17>
    d770:	ldr	x0, [x0, #2920]
    d774:	bl	c320 <__cxa_finalize@plt>
    d778:	bl	d6d8 <__gmpn_cnd_add_n@plt+0x28>
    d77c:	mov	w0, #0x1                   	// #1
    d780:	strb	w0, [x19, #2960]
    d784:	ldr	x19, [sp, #16]
    d788:	ldp	x29, x30, [sp], #32
    d78c:	ret
    d790:	b	d708 <__gmpn_cnd_add_n@plt+0x58>

000000000000d794 <__gmp_assert_header@@Base>:
    d794:	stp	x29, x30, [sp, #-32]!
    d798:	stp	x20, x19, [sp, #16]
    d79c:	mov	x29, sp
    d7a0:	cbz	x0, d7b0 <__gmp_assert_header@@Base+0x1c>
    d7a4:	ldrb	w8, [x0]
    d7a8:	mov	x2, x0
    d7ac:	cbnz	w8, d7bc <__gmp_assert_header@@Base+0x28>
    d7b0:	ldp	x20, x19, [sp, #16]
    d7b4:	ldp	x29, x30, [sp], #32
    d7b8:	ret
    d7bc:	adrp	x20, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d7c0:	ldr	x20, [x20, #3824]
    d7c4:	mov	w19, w1
    d7c8:	adrp	x1, 4c000 <__gmp_randclear_mt@@Base+0x18>
    d7cc:	add	x1, x1, #0x440
    d7d0:	ldr	x0, [x20]
    d7d4:	bl	d600 <fprintf@plt>
    d7d8:	cmn	w19, #0x1
    d7dc:	b.eq	d7b0 <__gmp_assert_header@@Base+0x1c>  // b.none
    d7e0:	ldr	x0, [x20]
    d7e4:	adrp	x1, 4c000 <__gmp_randclear_mt@@Base+0x18>
    d7e8:	add	x1, x1, #0x444
    d7ec:	mov	w2, w19
    d7f0:	bl	d600 <fprintf@plt>
    d7f4:	b	d7b0 <__gmp_assert_header@@Base+0x1c>

000000000000d7f8 <__gmp_assert_fail@@Base>:
    d7f8:	stp	x29, x30, [sp, #-32]!
    d7fc:	str	x19, [sp, #16]
    d800:	mov	x29, sp
    d804:	mov	x19, x2
    d808:	bl	cf80 <__gmp_assert_header@plt>
    d80c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d810:	ldr	x8, [x8, #3824]
    d814:	adrp	x1, 4c000 <__gmp_randclear_mt@@Base+0x18>
    d818:	add	x1, x1, #0x449
    d81c:	mov	x2, x19
    d820:	ldr	x0, [x8]
    d824:	bl	d600 <fprintf@plt>
    d828:	bl	cab0 <abort@plt>

000000000000d82c <__gmpn_divexact_by3@@Base>:
    d82c:	stp	x29, x30, [sp, #-16]!
    d830:	mov	x3, #0x5555555555555555    	// #6148914691236517205
    d834:	mov	x4, xzr
    d838:	mov	x29, sp
    d83c:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
    d840:	and	x0, x0, #0x3
    d844:	ldp	x29, x30, [sp], #16
    d848:	ret

000000000000d84c <__gmpn_divmod_1@@Base>:
    d84c:	stp	x29, x30, [sp, #-16]!
    d850:	mov	x4, x3
    d854:	mov	x3, x2
    d858:	mov	x2, x1
    d85c:	mov	x1, xzr
    d860:	mov	x29, sp
    d864:	bl	ced0 <__gmpn_divrem_1@plt>
    d868:	ldp	x29, x30, [sp], #16
    d86c:	ret

000000000000d870 <__gmpz_legendre@@Base>:
    d870:	stp	x29, x30, [sp, #-16]!
    d874:	mov	x29, sp
    d878:	bl	c1b0 <__gmpz_jacobi@plt>
    d87c:	ldp	x29, x30, [sp], #16
    d880:	ret

000000000000d884 <__gmp_exception@@Base>:
    d884:	stp	x29, x30, [sp, #-16]!
    d888:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d88c:	ldr	x8, [x8, #3896]
    d890:	mov	x29, sp
    d894:	ldr	w9, [x8]
    d898:	orr	w9, w9, w0
    d89c:	mov	w0, #0x8                   	// #8
    d8a0:	str	w9, [x8]
    d8a4:	bl	c0f0 <raise@plt>
    d8a8:	bl	cab0 <abort@plt>

000000000000d8ac <__gmp_sqrt_of_negative@@Base>:
    d8ac:	stp	x29, x30, [sp, #-16]!
    d8b0:	mov	w0, #0x4                   	// #4
    d8b4:	mov	x29, sp
    d8b8:	bl	d5b0 <__gmp_exception@plt>

000000000000d8bc <__gmp_divide_by_zero@@Base>:
    d8bc:	stp	x29, x30, [sp, #-16]!
    d8c0:	mov	w0, #0x2                   	// #2
    d8c4:	mov	x29, sp
    d8c8:	bl	d5b0 <__gmp_exception@plt>

000000000000d8cc <__gmp_extract_double@@Base>:
    d8cc:	fcmp	d0, #0.0
    d8d0:	b.ne	d8e4 <__gmp_extract_double@@Base+0x18>  // b.any
    d8d4:	mov	w8, wzr
    d8d8:	stp	xzr, xzr, [x0]
    d8dc:	mov	w0, w8
    d8e0:	ret
    d8e4:	fmov	x9, d0
    d8e8:	ubfx	x8, x9, #52, #11
    d8ec:	lsl	x9, x9, #11
    d8f0:	orr	x9, x9, #0x8000000000000000
    d8f4:	cbnz	x8, d908 <__gmp_extract_double@@Base+0x3c>
    d8f8:	mov	w8, #0x1                   	// #1
    d8fc:	lsl	x9, x9, #1
    d900:	sub	x8, x8, #0x1
    d904:	tbz	x9, #63, d8fc <__gmp_extract_double@@Base+0x30>
    d908:	add	x11, x8, #0xc02
    d90c:	add	x8, x8, #0xc41
    d910:	cmp	x11, #0x0
    d914:	csel	x8, x8, x11, lt  // lt = tstop
    d918:	and	w10, w11, #0x3f
    d91c:	asr	x8, x8, #6
    d920:	cbz	w10, d93c <__gmp_extract_double@@Base+0x70>
    d924:	neg	w11, w10
    d928:	lsr	x11, x9, x11
    d92c:	sub	x8, x8, #0x3f
    d930:	str	x11, [x0, #8]
    d934:	lsl	x10, x9, x10
    d938:	b	d948 <__gmp_extract_double@@Base+0x7c>
    d93c:	mov	x10, xzr
    d940:	sub	x8, x8, #0x40
    d944:	str	x9, [x0, #8]
    d948:	str	x10, [x0]
    d94c:	mov	w0, w8
    d950:	ret

000000000000d954 <__gmp_invalid_operation@@Base>:
    d954:	stp	x29, x30, [sp, #-16]!
    d958:	mov	w0, #0x8                   	// #8
    d95c:	mov	x29, sp
    d960:	bl	c0f0 <raise@plt>
    d964:	bl	cab0 <abort@plt>

000000000000d968 <__gmp_default_allocate@@Base>:
    d968:	stp	x29, x30, [sp, #-32]!
    d96c:	str	x19, [sp, #16]
    d970:	mov	x29, sp
    d974:	mov	x19, x0
    d978:	bl	c580 <malloc@plt>
    d97c:	cbz	x0, d98c <__gmp_default_allocate@@Base+0x24>
    d980:	ldr	x19, [sp, #16]
    d984:	ldp	x29, x30, [sp], #32
    d988:	ret
    d98c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d990:	ldr	x8, [x8, #3824]
    d994:	adrp	x1, 4c000 <__gmp_randclear_mt@@Base+0x18>
    d998:	add	x1, x1, #0x466
    d99c:	mov	x2, x19
    d9a0:	ldr	x0, [x8]
    d9a4:	bl	d600 <fprintf@plt>
    d9a8:	bl	cab0 <abort@plt>

000000000000d9ac <__gmp_default_reallocate@@Base>:
    d9ac:	stp	x29, x30, [sp, #-32]!
    d9b0:	stp	x20, x19, [sp, #16]
    d9b4:	mov	x20, x1
    d9b8:	mov	x1, x2
    d9bc:	mov	x29, sp
    d9c0:	mov	x19, x2
    d9c4:	bl	c960 <realloc@plt>
    d9c8:	cbz	x0, d9d8 <__gmp_default_reallocate@@Base+0x2c>
    d9cc:	ldp	x20, x19, [sp, #16]
    d9d0:	ldp	x29, x30, [sp], #32
    d9d4:	ret
    d9d8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    d9dc:	ldr	x8, [x8, #3824]
    d9e0:	adrp	x1, 4c000 <__gmp_randclear_mt@@Base+0x18>
    d9e4:	add	x1, x1, #0x491
    d9e8:	mov	x2, x20
    d9ec:	ldr	x0, [x8]
    d9f0:	mov	x3, x19
    d9f4:	bl	d600 <fprintf@plt>
    d9f8:	bl	cab0 <abort@plt>

000000000000d9fc <__gmp_default_free@@Base>:
    d9fc:	stp	x29, x30, [sp, #-16]!
    da00:	mov	x29, sp
    da04:	bl	cdf0 <free@plt>
    da08:	ldp	x29, x30, [sp], #16
    da0c:	ret

000000000000da10 <__gmp_get_memory_functions@@Base>:
    da10:	cbz	x0, da24 <__gmp_get_memory_functions@@Base+0x14>
    da14:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da18:	ldr	x8, [x8, #3840]
    da1c:	ldr	x8, [x8]
    da20:	str	x8, [x0]
    da24:	cbz	x1, da38 <__gmp_get_memory_functions@@Base+0x28>
    da28:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da2c:	ldr	x8, [x8, #3792]
    da30:	ldr	x8, [x8]
    da34:	str	x8, [x1]
    da38:	cbz	x2, da4c <__gmp_get_memory_functions@@Base+0x3c>
    da3c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da40:	ldr	x8, [x8, #4016]
    da44:	ldr	x8, [x8]
    da48:	str	x8, [x2]
    da4c:	ret

000000000000da50 <__gmp_set_memory_functions@@Base>:
    da50:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da54:	ldr	x8, [x8, #4008]
    da58:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da5c:	ldr	x9, [x9, #3840]
    da60:	cmp	x0, #0x0
    da64:	csel	x8, x8, x0, eq  // eq = none
    da68:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da6c:	str	x8, [x9]
    da70:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da74:	ldr	x8, [x8, #3912]
    da78:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da7c:	ldr	x9, [x9, #4032]
    da80:	ldr	x10, [x10, #3792]
    da84:	cmp	x1, #0x0
    da88:	csel	x8, x8, x1, eq  // eq = none
    da8c:	cmp	x2, #0x0
    da90:	str	x8, [x10]
    da94:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    da98:	ldr	x8, [x8, #4016]
    da9c:	csel	x9, x9, x2, eq  // eq = none
    daa0:	str	x9, [x8]
    daa4:	ret

000000000000daa8 <__gmp_nextprime@@Base>:
    daa8:	stp	x29, x30, [sp, #-16]!
    daac:	ldr	x9, [x0]
    dab0:	mov	x29, sp
    dab4:	add	x8, x0, x9
    dab8:	ldrb	w8, [x8, #24]
    dabc:	add	x9, x9, #0x1
    dac0:	cbnz	w8, dab4 <__gmp_nextprime@@Base+0xc>
    dac4:	cmp	x9, #0x201
    dac8:	add	x8, x0, #0x18
    dacc:	b.eq	daf4 <__gmp_nextprime@@Base+0x4c>  // b.none
    dad0:	add	x9, x0, x9
    dad4:	ldr	x10, [x0, #8]
    dad8:	add	x9, x9, #0x17
    dadc:	sub	x8, x9, x8
    dae0:	add	x9, x8, #0x1
    dae4:	str	x9, [x0]
    dae8:	add	x0, x10, x8, lsl #1
    daec:	ldp	x29, x30, [sp], #16
    daf0:	ret
    daf4:	ldr	x9, [x0, #8]
    daf8:	cmp	x9, #0x2
    dafc:	b.hi	db14 <__gmp_nextprime@@Base+0x6c>  // b.pmore
    db00:	mov	x8, #0xfffffffffffffc03    	// #-1021
    db04:	str	x8, [x0, #8]
    db08:	mov	w0, #0x2                   	// #2
    db0c:	ldp	x29, x30, [sp], #16
    db10:	ret
    db14:	movi	v0.2d, #0x0
    db18:	stp	q0, q0, [x8, #480]
    db1c:	stp	q0, q0, [x8, #448]
    db20:	stp	q0, q0, [x8, #416]
    db24:	stp	q0, q0, [x8, #384]
    db28:	stp	q0, q0, [x8, #352]
    db2c:	stp	q0, q0, [x8, #320]
    db30:	stp	q0, q0, [x8, #288]
    db34:	stp	q0, q0, [x8, #256]
    db38:	stp	q0, q0, [x8, #224]
    db3c:	stp	q0, q0, [x8, #192]
    db40:	stp	q0, q0, [x8, #160]
    db44:	stp	q0, q0, [x8, #128]
    db48:	stp	q0, q0, [x8, #96]
    db4c:	stp	q0, q0, [x8, #64]
    db50:	stp	q0, q0, [x8, #32]
    db54:	stp	q0, q0, [x8]
    db58:	ldr	x8, [x0, #16]
    db5c:	add	x10, x9, #0x400
    db60:	str	x10, [x0, #8]
    db64:	add	x11, x8, #0x1
    db68:	mul	x12, x11, x11
    db6c:	add	x11, x9, #0x7ff
    db70:	cmp	x12, x11
    db74:	b.hi	db90 <__gmp_nextprime@@Base+0xe8>  // b.pmore
    db78:	add	x12, x8, #0x2
    db7c:	mul	x12, x12, x12
    db80:	cmp	x12, x11
    db84:	add	x8, x8, #0x1
    db88:	b.ls	db78 <__gmp_nextprime@@Base+0xd0>  // b.plast
    db8c:	str	x8, [x0, #16]
    db90:	add	x9, x9, #0x403
    db94:	mov	x11, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    db98:	movk	x11, #0xaaab
    db9c:	lsr	x9, x9, #1
    dba0:	umulh	x11, x9, x11
    dba4:	lsr	x11, x11, #1
    dba8:	add	x11, x11, x11, lsl #1
    dbac:	subs	x9, x9, x11
    dbb0:	eor	x9, x9, #0x3
    dbb4:	csel	x9, xzr, x9, eq  // eq = none
    dbb8:	add	x10, x10, x9, lsl #1
    dbbc:	add	x11, x9, #0x3
    dbc0:	cmp	x10, #0x4
    dbc4:	csel	x9, x11, x9, cc  // cc = lo, ul, last
    dbc8:	add	x9, x0, x9
    dbcc:	add	x8, x0, #0x218
    dbd0:	add	x9, x9, #0x18
    dbd4:	mov	w10, #0x1                   	// #1
    dbd8:	strb	w10, [x9], #3
    dbdc:	cmp	x9, x8
    dbe0:	b.cc	dbd8 <__gmp_nextprime@@Base+0x130>  // b.lo, b.ul, b.last
    dbe4:	ldr	x9, [x0, #8]
    dbe8:	mov	x10, #0xcccccccccccccccc    	// #-3689348814741910324
    dbec:	movk	x10, #0xcccd
    dbf0:	add	x11, x9, #0x5
    dbf4:	lsr	x11, x11, #1
    dbf8:	umulh	x10, x11, x10
    dbfc:	lsr	x10, x10, #2
    dc00:	add	x10, x10, x10, lsl #2
    dc04:	subs	x10, x11, x10
    dc08:	mov	w11, #0x5                   	// #5
    dc0c:	sub	x10, x11, x10
    dc10:	csel	x10, xzr, x10, eq  // eq = none
    dc14:	add	x9, x9, x10, lsl #1
    dc18:	add	x11, x10, #0x5
    dc1c:	cmp	x9, #0x6
    dc20:	csel	x9, x11, x10, cc  // cc = lo, ul, last
    dc24:	cmp	x9, #0x1ff
    dc28:	b.gt	dc44 <__gmp_nextprime@@Base+0x19c>
    dc2c:	add	x9, x0, x9
    dc30:	add	x9, x9, #0x18
    dc34:	mov	w10, #0x1                   	// #1
    dc38:	strb	w10, [x9], #5
    dc3c:	cmp	x9, x8
    dc40:	b.cc	dc38 <__gmp_nextprime@@Base+0x190>  // b.lo, b.ul, b.last
    dc44:	ldr	x9, [x0, #8]
    dc48:	mov	x10, #0x2493                	// #9363
    dc4c:	movk	x10, #0x9249, lsl #16
    dc50:	movk	x10, #0x4924, lsl #32
    dc54:	add	x11, x9, #0x7
    dc58:	movk	x10, #0x2492, lsl #48
    dc5c:	lsr	x11, x11, #1
    dc60:	umulh	x10, x11, x10
    dc64:	sub	x12, x11, x10
    dc68:	add	x10, x10, x12, lsr #1
    dc6c:	lsr	x10, x10, #2
    dc70:	sub	x10, x10, x10, lsl #3
    dc74:	adds	x10, x11, x10
    dc78:	eor	x10, x10, #0x7
    dc7c:	csel	x10, xzr, x10, eq  // eq = none
    dc80:	add	x9, x9, x10, lsl #1
    dc84:	add	x11, x10, #0x7
    dc88:	cmp	x9, #0x8
    dc8c:	csel	x9, x11, x10, cc  // cc = lo, ul, last
    dc90:	add	x9, x0, x9
    dc94:	add	x9, x9, #0x18
    dc98:	mov	w10, #0x1                   	// #1
    dc9c:	strb	w10, [x9], #7
    dca0:	cmp	x9, x8
    dca4:	b.cc	dc9c <__gmp_nextprime@@Base+0x1f4>  // b.lo, b.ul, b.last
    dca8:	ldr	x9, [x0, #16]
    dcac:	cmp	x9, #0xb
    dcb0:	b.cs	dcc4 <__gmp_nextprime@@Base+0x21c>  // b.hs, b.nlast
    dcb4:	str	xzr, [x0]
    dcb8:	bl	ce40 <__gmp_nextprime@plt>
    dcbc:	ldp	x29, x30, [sp], #16
    dcc0:	ret
    dcc4:	adrp	x11, 4c000 <__gmp_randclear_mt@@Base+0x18>
    dcc8:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    dccc:	mov	x14, xzr
    dcd0:	mov	w9, #0xb                   	// #11
    dcd4:	mov	w10, #0x1                   	// #1
    dcd8:	add	x11, x11, #0x72e
    dcdc:	movk	x12, #0xaaab
    dce0:	mov	w13, #0x30                  	// #48
    dce4:	b	dd0c <__gmp_nextprime@@Base+0x264>
    dce8:	ldrb	w15, [x11, x14]
    dcec:	ldr	x16, [x0, #16]
    dcf0:	add	x14, x14, #0x1
    dcf4:	umulh	x17, x14, x12
    dcf8:	add	x9, x9, x15
    dcfc:	lsr	x15, x17, #5
    dd00:	cmp	x9, x16
    dd04:	msub	x14, x15, x13, x14
    dd08:	b.hi	dcb4 <__gmp_nextprime@@Base+0x20c>  // b.pmore
    dd0c:	ldr	x15, [x0, #8]
    dd10:	add	x16, x15, x9
    dd14:	lsr	x16, x16, #1
    dd18:	udiv	x17, x16, x9
    dd1c:	msub	x16, x17, x9, x16
    dd20:	sub	x17, x9, x16
    dd24:	cmp	x16, #0x0
    dd28:	csel	x16, xzr, x17, eq  // eq = none
    dd2c:	add	x15, x15, x16, lsl #1
    dd30:	cmp	x15, x9
    dd34:	csel	x15, xzr, x9, hi  // hi = pmore
    dd38:	add	x15, x15, x16
    dd3c:	cmp	x15, #0x1ff
    dd40:	b.gt	dce8 <__gmp_nextprime@@Base+0x240>
    dd44:	add	x15, x0, x15
    dd48:	add	x15, x15, #0x18
    dd4c:	strb	w10, [x15]
    dd50:	add	x15, x15, x9
    dd54:	cmp	x15, x8
    dd58:	b.cc	dd4c <__gmp_nextprime@@Base+0x2a4>  // b.lo, b.ul, b.last
    dd5c:	b	dce8 <__gmp_nextprime@@Base+0x240>

000000000000dd60 <__gmp_init_primesieve@@Base>:
    dd60:	mov	w8, #0x200                 	// #512
    dd64:	stp	xzr, xzr, [x0, #8]
    dd68:	str	x8, [x0]
    dd6c:	strb	wzr, [x0, #536]
    dd70:	ret

000000000000dd74 <__gmp_primesieve@@Base>:
    dd74:	stp	x29, x30, [sp, #-80]!
    dd78:	stp	x20, x19, [sp, #64]
    dd7c:	mov	x19, x0
    dd80:	mov	x0, x1
    dd84:	str	x25, [sp, #16]
    dd88:	stp	x24, x23, [sp, #32]
    dd8c:	stp	x22, x21, [sp, #48]
    dd90:	mov	x29, sp
    dd94:	mov	x22, x1
    dd98:	bl	de58 <__gmp_primesieve@@Base+0xe4>
    dd9c:	lsr	x24, x0, #6
    dda0:	mov	x21, x0
    dda4:	cmp	x0, #0x40, lsl #12
    dda8:	add	x20, x24, #0x1
    ddac:	b.cc	ddfc <__gmp_primesieve@@Base+0x88>  // b.lo, b.ul, b.last
    ddb0:	mov	w25, #0x800                 	// #2048
    ddb4:	bfxil	x25, x20, #0, #11
    ddb8:	lsl	x22, x25, #6
    ddbc:	mov	x0, x22
    ddc0:	bl	e054 <__gmp_primesieve@@Base+0x2e0>
    ddc4:	mov	x1, x0
    ddc8:	mov	x0, x19
    ddcc:	bl	de74 <__gmp_primesieve@@Base+0x100>
    ddd0:	add	x23, x19, x25, lsl #3
    ddd4:	mov	x0, x23
    ddd8:	mov	x1, x22
    dddc:	mov	x2, x19
    dde0:	bl	e068 <__gmp_primesieve@@Base+0x2f4>
    dde4:	add	x25, x25, #0x800
    dde8:	add	x22, x22, #0x20, lsl #12
    ddec:	cmp	x25, x24
    ddf0:	add	x23, x23, #0x4, lsl #12
    ddf4:	b.ls	ddd4 <__gmp_primesieve@@Base+0x60>  // b.plast
    ddf8:	b	de08 <__gmp_primesieve@@Base+0x94>
    ddfc:	mov	x0, x19
    de00:	mov	x1, x22
    de04:	bl	de74 <__gmp_primesieve@@Base+0x100>
    de08:	add	w8, w21, #0x1
    de0c:	ands	x8, x8, #0x3f
    de10:	b.eq	de2c <__gmp_primesieve@@Base+0xb8>  // b.none
    de14:	lsl	x9, x24, #3
    de18:	ldr	x10, [x19, x9]
    de1c:	mov	x11, #0xffffffffffffffff    	// #-1
    de20:	lsl	x8, x11, x8
    de24:	orr	x8, x10, x8
    de28:	str	x8, [x19, x9]
    de2c:	mov	x0, x19
    de30:	mov	x1, x20
    de34:	lsl	x21, x20, #6
    de38:	bl	cf50 <__gmpn_popcount@plt>
    de3c:	sub	x0, x21, x0
    de40:	ldp	x20, x19, [sp, #64]
    de44:	ldp	x22, x21, [sp, #48]
    de48:	ldp	x24, x23, [sp, #32]
    de4c:	ldr	x25, [sp, #16]
    de50:	ldp	x29, x30, [sp], #80
    de54:	ret
    de58:	sub	x8, x0, #0x5
    de5c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    de60:	orr	x8, x8, #0x1
    de64:	movk	x9, #0xaaab
    de68:	umulh	x8, x8, x9
    de6c:	lsr	x0, x8, #1
    de70:	ret
    de74:	stp	x29, x30, [sp, #-80]!
    de78:	stp	x20, x19, [sp, #64]
    de7c:	mov	x19, x0
    de80:	mov	x0, x1
    de84:	stp	x26, x25, [sp, #16]
    de88:	stp	x24, x23, [sp, #32]
    de8c:	stp	x22, x21, [sp, #48]
    de90:	mov	x29, sp
    de94:	mov	x21, x1
    de98:	bl	de58 <__gmp_primesieve@@Base+0xe4>
    de9c:	cmp	x0, #0x0
    dea0:	add	x8, x0, #0x3f
    dea4:	csel	x9, x8, x0, lt  // lt = tstop
    dea8:	mov	x20, x0
    deac:	cmp	x8, #0x7f
    deb0:	asr	x22, x9, #6
    deb4:	b.cc	dec8 <__gmp_primesieve@@Base+0x154>  // b.lo, b.ul, b.last
    deb8:	add	x0, x19, #0x8
    debc:	mov	x1, x22
    dec0:	mov	x2, xzr
    dec4:	bl	e22c <__gmp_primesieve@@Base+0x4b8>
    dec8:	add	x8, x20, #0x1
    decc:	mov	x9, #0x8480                	// #33920
    ded0:	add	x10, x20, #0x40
    ded4:	cmp	x8, #0x0
    ded8:	movk	x9, #0x6912, lsl #16
    dedc:	csinc	x10, x10, x20, lt  // lt = tstop
    dee0:	movk	x9, #0xc9e0, lsl #32
    dee4:	and	x10, x10, #0xffffffffffffffc0
    dee8:	movk	x9, #0x3294, lsl #48
    deec:	subs	x8, x8, x10
    def0:	str	x9, [x19]
    def4:	b.eq	df10 <__gmp_primesieve@@Base+0x19c>  // b.none
    def8:	lsl	x9, x22, #3
    defc:	ldr	x10, [x19, x9]
    df00:	mov	x11, #0xffffffffffffffff    	// #-1
    df04:	lsl	x8, x11, x8
    df08:	orr	x8, x10, x8
    df0c:	str	x8, [x19, x9]
    df10:	cmp	x21, #0xd3
    df14:	b.cc	e03c <__gmp_primesieve@@Base+0x2c8>  // b.lo, b.ul, b.last
    df18:	mov	w0, #0xd3                  	// #211
    df1c:	bl	de58 <__gmp_primesieve@@Base+0xe4>
    df20:	cmp	x0, #0x3f
    df24:	cset	w8, hi  // hi = pmore
    df28:	mov	w23, #0x1                   	// #1
    df2c:	lsl	x26, x8, #2
    df30:	mov	x22, xzr
    df34:	lsl	x24, x23, x26
    df38:	mov	w25, #0x40                  	// #64
    df3c:	b	df50 <__gmp_primesieve@@Base+0x1dc>
    df40:	ror	x8, x24, #63
    df44:	add	x22, x22, x24, lsr #63
    df48:	mov	x24, x8
    df4c:	mov	x26, x21
    df50:	ldr	x8, [x19, x22, lsl #3]
    df54:	add	x21, x26, #0x1
    df58:	tst	x8, x24
    df5c:	b.ne	df40 <__gmp_primesieve@@Base+0x1cc>  // b.any
    df60:	mov	x0, x21
    df64:	bl	e054 <__gmp_primesieve@@Base+0x2e0>
    df68:	and	x11, x21, #0x1
    df6c:	neg	x9, x11
    df70:	add	x10, x26, #0x2
    df74:	add	x8, x0, #0x1
    df78:	and	x9, x10, x9
    df7c:	madd	x8, x8, x21, x9
    df80:	sub	x12, x8, #0x1
    df84:	cmp	x12, x20
    df88:	b.gt	e03c <__gmp_primesieve@@Base+0x2c8>
    df8c:	lsl	x8, x0, #1
    df90:	add	x9, x8, #0x3f
    df94:	cmp	x8, #0x0
    df98:	csel	x9, x9, x8, lt  // lt = tstop
    df9c:	and	x9, x9, #0xffffffffffffffc0
    dfa0:	sub	x10, x8, x9
    dfa4:	lsl	x13, x23, x12
    dfa8:	and	x9, x10, #0xfffffffe
    dfac:	sub	w10, w25, w10
    dfb0:	add	x14, x12, #0x3f
    dfb4:	cmp	x12, #0x0
    dfb8:	csel	x14, x14, x12, lt  // lt = tstop
    dfbc:	asr	x14, x14, #6
    dfc0:	lsl	x14, x14, #3
    dfc4:	ldr	x16, [x19, x14]
    dfc8:	lsl	x15, x13, x9
    dfcc:	lsr	x17, x13, x10
    dfd0:	add	x12, x12, x8
    dfd4:	orr	x13, x16, x13
    dfd8:	cmp	x12, x20
    dfdc:	str	x13, [x19, x14]
    dfe0:	orr	x13, x15, x17
    dfe4:	b.le	dfb0 <__gmp_primesieve@@Base+0x23c>
    dfe8:	add	x12, x21, x21, lsl #1
    dfec:	add	x12, x12, #0x6
    dff0:	madd	x11, x12, x21, x11
    dff4:	cmp	x11, x20
    dff8:	b.gt	df40 <__gmp_primesieve@@Base+0x1cc>
    dffc:	lsl	x12, x23, x11
    e000:	add	x13, x11, #0x3f
    e004:	cmp	x11, #0x0
    e008:	csel	x13, x13, x11, lt  // lt = tstop
    e00c:	asr	x13, x13, #6
    e010:	lsl	x13, x13, #3
    e014:	ldr	x15, [x19, x13]
    e018:	lsl	x14, x12, x9
    e01c:	lsr	x16, x12, x10
    e020:	add	x11, x11, x8
    e024:	orr	x12, x15, x12
    e028:	cmp	x11, x20
    e02c:	str	x12, [x19, x13]
    e030:	orr	x12, x14, x16
    e034:	b.le	e000 <__gmp_primesieve@@Base+0x28c>
    e038:	b	df40 <__gmp_primesieve@@Base+0x1cc>
    e03c:	ldp	x20, x19, [sp, #64]
    e040:	ldp	x22, x21, [sp, #48]
    e044:	ldp	x24, x23, [sp, #32]
    e048:	ldp	x26, x25, [sp, #16]
    e04c:	ldp	x29, x30, [sp], #80
    e050:	ret
    e054:	add	x8, x0, x0, lsl #1
    e058:	and	x9, x0, #0x1
    e05c:	add	x8, x8, x9
    e060:	add	x0, x8, #0x1
    e064:	ret
    e068:	stp	x29, x30, [sp, #-96]!
    e06c:	stp	x20, x19, [sp, #80]
    e070:	mov	x19, x2
    e074:	mov	x20, x1
    e078:	sub	x2, x1, #0x40
    e07c:	mov	w1, #0x800                 	// #2048
    e080:	stp	x28, x27, [sp, #16]
    e084:	stp	x26, x25, [sp, #32]
    e088:	stp	x24, x23, [sp, #48]
    e08c:	stp	x22, x21, [sp, #64]
    e090:	mov	x29, sp
    e094:	mov	x21, x0
    e098:	bl	e22c <__gmp_primesieve@@Base+0x4b8>
    e09c:	mov	w8, #0x1ffff               	// #131071
    e0a0:	mov	x23, xzr
    e0a4:	mov	w25, #0x10                  	// #16
    e0a8:	mov	w28, #0x4                   	// #4
    e0ac:	mov	w24, #0x1                   	// #1
    e0b0:	add	x26, x20, x8
    e0b4:	mov	w27, #0x40                  	// #64
    e0b8:	b	e0cc <__gmp_primesieve@@Base+0x358>
    e0bc:	ror	x8, x25, #63
    e0c0:	add	x23, x23, x25, lsr #63
    e0c4:	mov	x28, x22
    e0c8:	mov	x25, x8
    e0cc:	ldr	x8, [x19, x23, lsl #3]
    e0d0:	add	x22, x28, #0x1
    e0d4:	tst	x8, x25
    e0d8:	b.ne	e0bc <__gmp_primesieve@@Base+0x348>  // b.any
    e0dc:	mov	x0, x22
    e0e0:	bl	e054 <__gmp_primesieve@@Base+0x2e0>
    e0e4:	and	x10, x22, #0x1
    e0e8:	neg	x9, x10
    e0ec:	add	x11, x28, #0x2
    e0f0:	add	x8, x0, #0x1
    e0f4:	and	x9, x11, x9
    e0f8:	madd	x8, x8, x22, x9
    e0fc:	sub	x11, x8, #0x1
    e100:	cmp	x11, x26
    e104:	b.gt	e210 <__gmp_primesieve@@Base+0x49c>
    e108:	lsl	x8, x0, #1
    e10c:	add	x9, x8, #0x3f
    e110:	cmp	x8, #0x0
    e114:	csel	x9, x9, x8, lt  // lt = tstop
    e118:	and	x9, x9, #0xffffffffffffffc0
    e11c:	cmp	x11, x20
    e120:	sub	x9, x8, x9
    e124:	b.ge	e13c <__gmp_primesieve@@Base+0x3c8>  // b.tcont
    e128:	mvn	x12, x11
    e12c:	add	x12, x12, x20
    e130:	sdiv	x12, x12, x8
    e134:	add	x12, x12, #0x1
    e138:	madd	x11, x12, x8, x11
    e13c:	sub	x11, x11, x20
    e140:	cmp	x11, #0x20, lsl #12
    e144:	b.ge	e190 <__gmp_primesieve@@Base+0x41c>  // b.tcont
    e148:	sub	w14, w27, w9
    e14c:	lsl	x12, x24, x11
    e150:	and	x13, x9, #0xfffffffe
    e154:	and	x14, x14, #0xfffffffe
    e158:	add	x15, x11, #0x3f
    e15c:	cmp	x11, #0x0
    e160:	csel	x15, x15, x11, lt  // lt = tstop
    e164:	asr	x15, x15, #6
    e168:	lsl	x15, x15, #3
    e16c:	ldr	x17, [x21, x15]
    e170:	lsl	x16, x12, x13
    e174:	lsr	x18, x12, x14
    e178:	add	x11, x11, x8
    e17c:	orr	x12, x17, x12
    e180:	cmp	x11, #0x20, lsl #12
    e184:	str	x12, [x21, x15]
    e188:	orr	x12, x16, x18
    e18c:	b.lt	e158 <__gmp_primesieve@@Base+0x3e4>  // b.tstop
    e190:	add	x11, x22, x22, lsl #1
    e194:	add	x11, x11, #0x6
    e198:	madd	x10, x11, x22, x10
    e19c:	cmp	x10, x20
    e1a0:	b.ge	e1b8 <__gmp_primesieve@@Base+0x444>  // b.tcont
    e1a4:	mvn	x11, x10
    e1a8:	add	x11, x11, x20
    e1ac:	sdiv	x11, x11, x8
    e1b0:	add	x11, x11, #0x1
    e1b4:	madd	x10, x11, x8, x10
    e1b8:	sub	x10, x10, x20
    e1bc:	cmp	x10, #0x20, lsl #12
    e1c0:	b.ge	e0bc <__gmp_primesieve@@Base+0x348>  // b.tcont
    e1c4:	and	x12, x9, #0xfffffffe
    e1c8:	sub	w9, w27, w9
    e1cc:	lsl	x11, x24, x10
    e1d0:	and	x9, x9, #0xfffffffe
    e1d4:	add	x13, x10, #0x3f
    e1d8:	cmp	x10, #0x0
    e1dc:	csel	x13, x13, x10, lt  // lt = tstop
    e1e0:	asr	x13, x13, #6
    e1e4:	lsl	x13, x13, #3
    e1e8:	ldr	x15, [x21, x13]
    e1ec:	lsl	x14, x11, x12
    e1f0:	lsr	x16, x11, x9
    e1f4:	add	x10, x10, x8
    e1f8:	orr	x11, x15, x11
    e1fc:	cmp	x10, #0x20, lsl #12
    e200:	str	x11, [x21, x13]
    e204:	orr	x11, x14, x16
    e208:	b.lt	e1d4 <__gmp_primesieve@@Base+0x460>  // b.tstop
    e20c:	b	e0bc <__gmp_primesieve@@Base+0x348>
    e210:	ldp	x20, x19, [sp, #80]
    e214:	ldp	x22, x21, [sp, #64]
    e218:	ldp	x24, x23, [sp, #48]
    e21c:	ldp	x26, x25, [sp, #32]
    e220:	ldp	x28, x27, [sp, #16]
    e224:	ldp	x29, x30, [sp], #96
    e228:	ret
    e22c:	mov	x11, #0x184                 	// #388
    e230:	mov	x8, #0x2058                	// #8280
    e234:	mov	x13, #0x2120                	// #8480
    e238:	movk	x11, #0x4023, lsl #16
    e23c:	movk	x8, #0x489, lsl #16
    e240:	movk	x13, #0x8840, lsl #16
    e244:	mov	x10, #0x4421                	// #17441
    e248:	mov	x9, #0x1244                	// #4676
    e24c:	movk	x11, #0x180c, lsl #32
    e250:	movk	x8, #0x4a12, lsl #32
    e254:	movk	x13, #0x210, lsl #32
    e258:	movk	x10, #0x1008, lsl #16
    e25c:	movk	x9, #0x3068, lsl #16
    e260:	movk	x11, #0x9402, lsl #48
    e264:	movk	x8, #0x8121, lsl #48
    e268:	movk	x13, #0x285, lsl #48
    e26c:	movk	x10, #0xa412, lsl #32
    e270:	movk	x9, #0xc81, lsl #32
    e274:	cbz	x2, e390 <__gmp_primesieve@@Base+0x61c>
    e278:	mov	x14, #0x7905                	// #30981
    e27c:	movk	x14, #0x904a, lsl #16
    e280:	movk	x14, #0x4a7, lsl #32
    e284:	lsr	x12, x2, #1
    e288:	movk	x14, #0x4a79, lsl #48
    e28c:	umulh	x14, x12, x14
    e290:	lsr	x14, x14, #4
    e294:	mov	w15, #0x6e                  	// #110
    e298:	msub	x14, x14, x15, x2
    e29c:	cbz	x14, e31c <__gmp_primesieve@@Base+0x5a8>
    e2a0:	cmp	x14, #0x3f
    e2a4:	b.hi	e2dc <__gmp_primesieve@@Base+0x568>  // b.pmore
    e2a8:	neg	x16, x14
    e2ac:	lsr	x15, x8, x14
    e2b0:	lsl	x17, x9, x16
    e2b4:	subs	x16, x14, #0x2e
    e2b8:	orr	x15, x17, x15
    e2bc:	b.hi	e304 <__gmp_primesieve@@Base+0x590>  // b.pmore
    e2c0:	mov	w16, #0x2e                  	// #46
    e2c4:	sub	x16, x16, x14
    e2c8:	lsl	x8, x8, x16
    e2cc:	lsr	x9, x9, x14
    e2d0:	orr	x9, x8, x9
    e2d4:	mov	x8, x15
    e2d8:	b	e31c <__gmp_primesieve@@Base+0x5a8>
    e2dc:	mov	w15, #0x6e                  	// #110
    e2e0:	lsr	x16, x9, x14
    e2e4:	sub	x17, x14, #0x2e
    e2e8:	sub	x14, x15, x14
    e2ec:	lsr	x15, x8, x17
    e2f0:	lsl	x8, x8, x14
    e2f4:	lsl	x9, x9, x14
    e2f8:	orr	x8, x8, x16
    e2fc:	orr	x9, x9, x15
    e300:	b	e31c <__gmp_primesieve@@Base+0x5a8>
    e304:	mov	w9, #0x6e                  	// #110
    e308:	sub	x9, x9, x14
    e30c:	lsl	x9, x8, x9
    e310:	orr	x14, x15, x9
    e314:	lsr	x9, x8, x16
    e318:	mov	x8, x14
    e31c:	mov	x14, #0x2d03                	// #11523
    e320:	movk	x14, #0x2d0, lsl #16
    e324:	movk	x14, #0xd02d, lsl #32
    e328:	movk	x14, #0x2d02, lsl #48
    e32c:	umulh	x12, x12, x14
    e330:	lsr	x12, x12, #4
    e334:	mov	w14, #0xb6                  	// #182
    e338:	msub	x15, x12, x14, x2
    e33c:	cbz	x15, e390 <__gmp_primesieve@@Base+0x61c>
    e340:	subs	x14, x15, #0x40
    e344:	b.hi	e3f8 <__gmp_primesieve@@Base+0x684>  // b.pmore
    e348:	neg	x12, x15
    e34c:	lsr	x14, x11, x15
    e350:	lsr	x16, x13, x15
    e354:	cmp	x15, #0x40
    e358:	lsl	x17, x13, x12
    e35c:	lsl	x18, x10, x12
    e360:	csel	x12, xzr, x14, eq  // eq = none
    e364:	csel	x14, xzr, x16, eq  // eq = none
    e368:	subs	x13, x15, #0x36
    e36c:	orr	x12, x17, x12
    e370:	orr	x14, x18, x14
    e374:	b.hi	e448 <__gmp_primesieve@@Base+0x6d4>  // b.pmore
    e378:	mov	w13, #0x36                  	// #54
    e37c:	sub	x13, x13, x15
    e380:	lsl	x11, x11, x13
    e384:	lsr	x10, x10, x15
    e388:	orr	x10, x11, x10
    e38c:	b	e3b0 <__gmp_primesieve@@Base+0x63c>
    e390:	mov	x12, #0x184                 	// #388
    e394:	mov	x14, #0x2120                	// #8480
    e398:	movk	x12, #0x4023, lsl #16
    e39c:	movk	x14, #0x8840, lsl #16
    e3a0:	movk	x12, #0x180c, lsl #32
    e3a4:	movk	x14, #0x210, lsl #32
    e3a8:	movk	x12, #0x9402, lsl #48
    e3ac:	movk	x14, #0x285, lsl #48
    e3b0:	orr	x11, x12, x8
    e3b4:	cmp	x1, #0x1
    e3b8:	str	x11, [x0]
    e3bc:	b.eq	e3f4 <__gmp_primesieve@@Base+0x680>  // b.none
    e3c0:	orr	x11, x9, x8, lsl #46
    e3c4:	extr	x8, x9, x8, #18
    e3c8:	lsr	x13, x14, #10
    e3cc:	orr	x9, x14, x11
    e3d0:	lsr	x11, x11, #18
    e3d4:	subs	x1, x1, #0x2
    e3d8:	extr	x14, x14, x12, #10
    e3dc:	orr	x12, x10, x12, lsl #54
    e3e0:	str	x9, [x0, #8]
    e3e4:	add	x0, x0, #0x10
    e3e8:	mov	x9, x11
    e3ec:	mov	x10, x13
    e3f0:	b.ne	e3b0 <__gmp_primesieve@@Base+0x63c>  // b.any
    e3f4:	ret
    e3f8:	cmp	x15, #0x7f
    e3fc:	b.hi	e460 <__gmp_primesieve@@Base+0x6ec>  // b.pmore
    e400:	neg	x16, x15
    e404:	lsr	x12, x13, x15
    e408:	lsl	x17, x10, x16
    e40c:	subs	x16, x15, #0x76
    e410:	orr	x12, x12, x17
    e414:	b.hi	e494 <__gmp_primesieve@@Base+0x720>  // b.pmore
    e418:	lsr	x10, x10, x14
    e41c:	mov	w14, #0x76                  	// #118
    e420:	sub	x16, x14, x15
    e424:	lsl	x14, x11, x16
    e428:	cmp	x15, #0x76
    e42c:	orr	x14, x10, x14
    e430:	lsl	x10, x13, x16
    e434:	b.eq	e3b0 <__gmp_primesieve@@Base+0x63c>  // b.none
    e438:	sub	x13, x15, #0x36
    e43c:	lsr	x11, x11, x13
    e440:	orr	x10, x10, x11
    e444:	b	e3b0 <__gmp_primesieve@@Base+0x63c>
    e448:	mov	w10, #0x76                  	// #118
    e44c:	sub	x10, x10, x15
    e450:	lsl	x10, x11, x10
    e454:	orr	x14, x14, x10
    e458:	lsr	x10, x11, x13
    e45c:	b	e3b0 <__gmp_primesieve@@Base+0x63c>
    e460:	mov	w12, #0xb6                  	// #182
    e464:	sub	x16, x15, #0x76
    e468:	sub	x12, x12, x15
    e46c:	lsr	x14, x10, x15
    e470:	lsr	x15, x11, x16
    e474:	lsr	x16, x13, x16
    e478:	lsl	x11, x11, x12
    e47c:	lsl	x13, x13, x12
    e480:	lsl	x10, x10, x12
    e484:	orr	x12, x11, x14
    e488:	orr	x14, x13, x15
    e48c:	orr	x10, x10, x16
    e490:	b	e3b0 <__gmp_primesieve@@Base+0x63c>
    e494:	mov	w10, #0xb6                  	// #182
    e498:	sub	x10, x10, x15
    e49c:	lsr	x14, x11, x16
    e4a0:	lsl	x11, x11, x10
    e4a4:	lsl	x10, x13, x10
    e4a8:	orr	x12, x12, x11
    e4ac:	orr	x14, x10, x14
    e4b0:	lsr	x10, x13, x16
    e4b4:	b	e3b0 <__gmp_primesieve@@Base+0x63c>

000000000000e4b8 <__gmp_tmp_reentrant_alloc@@Base>:
    e4b8:	stp	x29, x30, [sp, #-32]!
    e4bc:	stp	x20, x19, [sp, #16]
    e4c0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e4c4:	ldr	x8, [x8, #3840]
    e4c8:	add	x20, x1, #0x10
    e4cc:	mov	x19, x0
    e4d0:	mov	x0, x20
    e4d4:	ldr	x8, [x8]
    e4d8:	mov	x29, sp
    e4dc:	blr	x8
    e4e0:	str	x20, [x0, #8]
    e4e4:	ldr	x9, [x19]
    e4e8:	add	x8, x0, #0x10
    e4ec:	str	x9, [x0]
    e4f0:	str	x0, [x19]
    e4f4:	ldp	x20, x19, [sp, #16]
    e4f8:	mov	x0, x8
    e4fc:	ldp	x29, x30, [sp], #32
    e500:	ret

000000000000e504 <__gmp_tmp_reentrant_free@@Base>:
    e504:	stp	x29, x30, [sp, #-32]!
    e508:	stp	x20, x19, [sp, #16]
    e50c:	mov	x29, sp
    e510:	cbz	x0, e530 <__gmp_tmp_reentrant_free@@Base+0x2c>
    e514:	adrp	x19, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e518:	ldr	x19, [x19, #4016]
    e51c:	ldr	x8, [x19]
    e520:	ldp	x20, x1, [x0]
    e524:	blr	x8
    e528:	mov	x0, x20
    e52c:	cbnz	x20, e51c <__gmp_tmp_reentrant_free@@Base+0x18>
    e530:	ldp	x20, x19, [sp, #16]
    e534:	ldp	x29, x30, [sp], #32
    e538:	ret

000000000000e53c <__gmpf_init@@Base>:
    e53c:	stp	x29, x30, [sp, #-32]!
    e540:	str	x19, [sp, #16]
    e544:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e548:	ldr	x8, [x8, #3960]
    e54c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e550:	mov	x19, x0
    e554:	mov	x29, sp
    e558:	ldr	x8, [x8]
    e55c:	str	xzr, [x0, #8]
    e560:	stp	w8, wzr, [x0]
    e564:	ldr	x9, [x9, #3840]
    e568:	lsl	x8, x8, #3
    e56c:	add	x0, x8, #0x8
    e570:	ldr	x9, [x9]
    e574:	blr	x9
    e578:	str	x0, [x19, #16]
    e57c:	ldr	x19, [sp, #16]
    e580:	ldp	x29, x30, [sp], #32
    e584:	ret

000000000000e588 <__gmpf_init2@@Base>:
    e588:	stp	x29, x30, [sp, #-32]!
    e58c:	cmp	x1, #0x35
    e590:	mov	w8, #0x35                  	// #53
    e594:	csel	x8, x1, x8, hi  // hi = pmore
    e598:	add	x8, x8, #0x7f
    e59c:	lsr	x8, x8, #6
    e5a0:	str	x19, [sp, #16]
    e5a4:	str	xzr, [x0, #8]
    e5a8:	stp	w8, wzr, [x0]
    e5ac:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e5b0:	ldr	x9, [x9, #3840]
    e5b4:	lsl	x8, x8, #3
    e5b8:	mov	x19, x0
    e5bc:	add	x0, x8, #0x8
    e5c0:	ldr	x9, [x9]
    e5c4:	mov	x29, sp
    e5c8:	blr	x9
    e5cc:	str	x0, [x19, #16]
    e5d0:	ldr	x19, [sp, #16]
    e5d4:	ldp	x29, x30, [sp], #32
    e5d8:	ret

000000000000e5dc <__gmpf_inits@@Base>:
    e5dc:	sub	sp, sp, #0xf0
    e5e0:	stp	x29, x30, [sp, #224]
    e5e4:	add	x29, sp, #0xe0
    e5e8:	mov	x8, #0xffffffffffffffc8    	// #-56
    e5ec:	mov	x9, sp
    e5f0:	sub	x10, x29, #0x58
    e5f4:	movk	x8, #0xff80, lsl #32
    e5f8:	add	x11, x29, #0x10
    e5fc:	add	x9, x9, #0x80
    e600:	add	x10, x10, #0x38
    e604:	stp	x1, x2, [x29, #-88]
    e608:	stp	x3, x4, [x29, #-72]
    e60c:	stp	x5, x6, [x29, #-56]
    e610:	stur	x7, [x29, #-40]
    e614:	stp	q0, q1, [sp]
    e618:	stp	q2, q3, [sp, #32]
    e61c:	stp	q4, q5, [sp, #64]
    e620:	stp	q6, q7, [sp, #96]
    e624:	stp	x9, x8, [x29, #-16]
    e628:	stp	x11, x10, [x29, #-32]
    e62c:	b	e644 <__gmpf_inits@@Base+0x68>
    e630:	ldur	x8, [x29, #-32]
    e634:	add	x9, x8, #0x8
    e638:	stur	x9, [x29, #-32]
    e63c:	ldr	x0, [x8]
    e640:	cbz	x0, e670 <__gmpf_inits@@Base+0x94>
    e644:	bl	d030 <__gmpf_init@plt>
    e648:	ldursw	x8, [x29, #-8]
    e64c:	tbz	w8, #31, e630 <__gmpf_inits@@Base+0x54>
    e650:	add	w9, w8, #0x8
    e654:	cmp	w9, #0x0
    e658:	stur	w9, [x29, #-8]
    e65c:	b.gt	e630 <__gmpf_inits@@Base+0x54>
    e660:	ldur	x9, [x29, #-24]
    e664:	add	x8, x9, x8
    e668:	ldr	x0, [x8]
    e66c:	cbnz	x0, e644 <__gmpf_inits@@Base+0x68>
    e670:	ldp	x29, x30, [sp, #224]
    e674:	add	sp, sp, #0xf0
    e678:	ret

000000000000e67c <__gmpf_set@@Base>:
    e67c:	stp	x29, x30, [sp, #-16]!
    e680:	ldrsw	x10, [x1, #4]
    e684:	ldrsw	x9, [x0]
    e688:	ldp	x12, x11, [x1, #8]
    e68c:	ldr	x8, [x0, #16]
    e690:	cmp	x10, #0x0
    e694:	add	x13, x9, #0x1
    e698:	str	x12, [x0, #8]
    e69c:	cneg	x12, x10, mi  // mi = first
    e6a0:	subs	x13, x12, x13
    e6a4:	add	x13, x11, x13, lsl #3
    e6a8:	csinc	x2, x12, x9, le
    e6ac:	csel	x1, x13, x11, gt
    e6b0:	neg	w9, w2
    e6b4:	cmp	w10, #0x0
    e6b8:	csel	x9, x2, x9, ge  // ge = tcont
    e6bc:	str	w9, [x0, #4]
    e6c0:	mov	x0, x8
    e6c4:	mov	x29, sp
    e6c8:	bl	cc10 <__gmpn_copyi@plt>
    e6cc:	ldp	x29, x30, [sp], #16
    e6d0:	ret

000000000000e6d4 <__gmpf_set_ui@@Base>:
    e6d4:	ldr	x8, [x0, #16]
    e6d8:	cmp	x1, #0x0
    e6dc:	cset	w9, ne  // ne = any
    e6e0:	str	x1, [x8]
    e6e4:	str	w9, [x0, #4]
    e6e8:	str	x9, [x0, #8]
    e6ec:	ret

000000000000e6f0 <__gmpf_set_si@@Base>:
    e6f0:	ldr	x8, [x0, #16]
    e6f4:	cmp	x1, #0x0
    e6f8:	cset	w10, ne  // ne = any
    e6fc:	csetm	x11, ne  // ne = any
    e700:	cneg	x9, x1, mi  // mi = first
    e704:	csel	x11, x10, x11, ge  // ge = tcont
    e708:	str	x9, [x8]
    e70c:	str	x10, [x0, #8]
    e710:	str	w11, [x0, #4]
    e714:	ret

000000000000e718 <__gmpf_set_str@@Base>:
    e718:	stp	x29, x30, [sp, #-96]!
    e71c:	stp	x28, x27, [sp, #16]
    e720:	stp	x26, x25, [sp, #32]
    e724:	stp	x24, x23, [sp, #48]
    e728:	stp	x22, x21, [sp, #64]
    e72c:	stp	x20, x19, [sp, #80]
    e730:	mov	x29, sp
    e734:	sub	sp, sp, #0x50
    e738:	mov	x19, x0
    e73c:	mov	w0, #0x10000               	// #65536
    e740:	mov	w20, w2
    e744:	mov	x21, x1
    e748:	bl	c560 <nl_langinfo@plt>
    e74c:	mov	x22, x0
    e750:	bl	c090 <strlen@plt>
    e754:	mov	x23, x0
    e758:	bl	cca0 <__ctype_b_loc@plt>
    e75c:	ldr	x9, [x0]
    e760:	mov	x24, x0
    e764:	ldrb	w8, [x21], #1
    e768:	ldrh	w10, [x9, x8, lsl #1]
    e76c:	tbnz	w10, #13, e764 <__gmpf_set_str@@Base+0x4c>
    e770:	cmp	w8, #0x2d
    e774:	b.ne	e784 <__gmpf_set_str@@Base+0x6c>  // b.any
    e778:	ldrb	w8, [x21]
    e77c:	mov	w13, #0x1                   	// #1
    e780:	b	e78c <__gmpf_set_str@@Base+0x74>
    e784:	mov	w13, wzr
    e788:	sub	x21, x21, #0x1
    e78c:	cmp	w20, #0x0
    e790:	mov	w9, #0xa                   	// #10
    e794:	adrp	x28, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e798:	csel	w10, w9, w20, eq  // eq = none
    e79c:	ldr	x28, [x28, #3920]
    e7a0:	cmp	w10, #0x0
    e7a4:	cneg	w20, w10, mi  // mi = first
    e7a8:	csel	w14, w9, w10, lt  // lt = tstop
    e7ac:	cmp	w20, #0x25
    e7b0:	b.lt	e7c0 <__gmpf_set_str@@Base+0xa8>  // b.tstop
    e7b4:	cmp	w20, #0x3e
    e7b8:	b.gt	ed9c <__gmpf_set_str@@Base+0x684>
    e7bc:	add	x28, x28, #0xd0
    e7c0:	ldrb	w8, [x28, w8, uxtw]
    e7c4:	cmp	w20, w8
    e7c8:	b.gt	e80c <__gmpf_set_str@@Base+0xf4>
    e7cc:	cbz	x23, e7fc <__gmpf_set_str@@Base+0xe4>
    e7d0:	mov	x8, x21
    e7d4:	mov	x9, x22
    e7d8:	mov	x10, x23
    e7dc:	ldrb	w11, [x8]
    e7e0:	ldrb	w12, [x9]
    e7e4:	cmp	w11, w12
    e7e8:	b.ne	ed9c <__gmpf_set_str@@Base+0x684>  // b.any
    e7ec:	subs	x10, x10, #0x1
    e7f0:	add	x9, x9, #0x1
    e7f4:	add	x8, x8, #0x1
    e7f8:	b.ne	e7dc <__gmpf_set_str@@Base+0xc4>  // b.any
    e7fc:	ldrb	w8, [x21, x23]
    e800:	ldrb	w8, [x28, x8]
    e804:	cmp	w20, w8
    e808:	b.le	ed9c <__gmpf_set_str@@Base+0x684>
    e80c:	mov	x0, x21
    e810:	stur	x14, [x29, #-40]
    e814:	stur	w13, [x29, #-28]
    e818:	bl	c090 <strlen@plt>
    e81c:	mov	x26, x0
    e820:	mov	x9, x0
    e824:	subs	x8, x9, #0x1
    e828:	b.eq	e860 <__gmpf_set_str@@Base+0x148>  // b.none
    e82c:	add	x27, x21, x9
    e830:	ldurb	w10, [x27, #-1]
    e834:	cmp	w10, #0x40
    e838:	b.eq	e858 <__gmpf_set_str@@Base+0x140>  // b.none
    e83c:	cmp	w20, #0xa
    e840:	mov	x9, x8
    e844:	b.gt	e824 <__gmpf_set_str@@Base+0x10c>
    e848:	orr	w9, w10, #0x20
    e84c:	cmp	w9, #0x65
    e850:	mov	x9, x8
    e854:	b.ne	e824 <__gmpf_set_str@@Base+0x10c>  // b.any
    e858:	mov	x26, x8
    e85c:	b	e864 <__gmpf_set_str@@Base+0x14c>
    e860:	mov	x27, xzr
    e864:	add	x1, x26, #0x1
    e868:	mov	w8, #0x7f00                	// #32512
    e86c:	cmp	x1, x8
    e870:	stur	xzr, [x29, #-8]
    e874:	b.hi	e980 <__gmpf_set_str@@Base+0x268>  // b.pmore
    e878:	add	x9, x1, #0xf
    e87c:	mov	x8, sp
    e880:	and	x9, x9, #0xfffffffffffffff0
    e884:	sub	x14, x8, x9
    e888:	mov	sp, x14
    e88c:	stur	x19, [x29, #-24]
    e890:	cbz	x26, e994 <__gmpf_set_str@@Base+0x27c>
    e894:	mov	x19, xzr
    e898:	mov	x25, xzr
    e89c:	mov	w15, wzr
    e8a0:	mov	x12, xzr
    e8a4:	sub	x16, x23, #0x1
    e8a8:	mov	x13, x14
    e8ac:	b	e8d4 <__gmpf_set_str@@Base+0x1bc>
    e8b0:	add	x21, x21, x16
    e8b4:	add	x19, x19, x16
    e8b8:	mov	w8, #0x1                   	// #1
    e8bc:	mov	x25, x13
    e8c0:	cbz	w8, ed9c <__gmpf_set_str@@Base+0x684>
    e8c4:	add	x19, x19, #0x1
    e8c8:	cmp	x19, x26
    e8cc:	add	x21, x21, #0x1
    e8d0:	b.cs	e9a0 <__gmpf_set_str@@Base+0x288>  // b.hs, b.nlast
    e8d4:	ldrb	w8, [x21]
    e8d8:	ldr	x9, [x24]
    e8dc:	ldrh	w9, [x9, x8, lsl #1]
    e8e0:	tbnz	w9, #13, e8c4 <__gmpf_set_str@@Base+0x1ac>
    e8e4:	cbz	x23, e908 <__gmpf_set_str@@Base+0x1f0>
    e8e8:	mov	x9, xzr
    e8ec:	ldrb	w10, [x21, x9]
    e8f0:	ldrb	w11, [x22, x9]
    e8f4:	cmp	w10, w11
    e8f8:	b.ne	e910 <__gmpf_set_str@@Base+0x1f8>  // b.any
    e8fc:	add	x9, x9, #0x1
    e900:	cmp	x23, x9
    e904:	b.ne	e8ec <__gmpf_set_str@@Base+0x1d4>  // b.any
    e908:	cbnz	x25, e944 <__gmpf_set_str@@Base+0x22c>
    e90c:	b	e8b0 <__gmpf_set_str@@Base+0x198>
    e910:	ldrb	w8, [x28, x8]
    e914:	cmp	w20, w8
    e918:	b.le	e944 <__gmpf_set_str@@Base+0x22c>
    e91c:	cmp	w8, #0x0
    e920:	strb	w8, [x13]
    e924:	cset	w8, ne  // ne = any
    e928:	orr	w15, w15, w8
    e92c:	add	x13, x13, w15, sxtw
    e930:	mov	w8, #0x1                   	// #1
    e934:	cbz	x25, e8c0 <__gmpf_set_str@@Base+0x1a8>
    e938:	sub	w9, w8, w15
    e93c:	add	x12, x12, w9, sxtw
    e940:	b	e8c0 <__gmpf_set_str@@Base+0x1a8>
    e944:	ldur	x0, [x29, #-8]
    e948:	cbnz	x0, e954 <__gmpf_set_str@@Base+0x23c>
    e94c:	mov	w8, wzr
    e950:	b	e8c0 <__gmpf_set_str@@Base+0x1a8>
    e954:	stp	x12, x13, [x29, #-56]
    e958:	stur	x14, [x29, #-64]
    e95c:	stur	w15, [x29, #-68]
    e960:	stur	x16, [x29, #-80]
    e964:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    e968:	ldur	x16, [x29, #-80]
    e96c:	ldur	w15, [x29, #-68]
    e970:	ldp	x14, x12, [x29, #-64]
    e974:	ldur	x13, [x29, #-48]
    e978:	mov	w8, wzr
    e97c:	b	e8c0 <__gmpf_set_str@@Base+0x1a8>
    e980:	sub	x0, x29, #0x8
    e984:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    e988:	mov	x14, x0
    e98c:	stur	x19, [x29, #-24]
    e990:	cbnz	x26, e894 <__gmpf_set_str@@Base+0x17c>
    e994:	mov	x12, xzr
    e998:	mov	x25, xzr
    e99c:	mov	x13, x14
    e9a0:	subs	x22, x13, x14
    e9a4:	b.eq	eaa4 <__gmpf_set_str@@Base+0x38c>  // b.none
    e9a8:	ldur	x19, [x29, #-24]
    e9ac:	stur	x13, [x29, #-48]
    e9b0:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    e9b4:	mov	w10, #0x28                  	// #40
    e9b8:	ldrsw	x8, [x19]
    e9bc:	ldr	x9, [x9, #3936]
    e9c0:	mov	x26, x12
    e9c4:	add	x21, x8, #0x1
    e9c8:	umaddl	x9, w20, w10, x9
    e9cc:	ldr	x9, [x9, #16]
    e9d0:	umulh	x8, x9, x22
    e9d4:	and	x8, x8, #0x1ffffffffffffff8
    e9d8:	mov	w9, #0x7ef0                	// #32496
    e9dc:	cmp	x8, x9
    e9e0:	add	x1, x8, #0x10
    e9e4:	b.hi	ed60 <__gmpf_set_str@@Base+0x648>  // b.pmore
    e9e8:	add	x9, x1, #0xf
    e9ec:	mov	x8, sp
    e9f0:	and	x9, x9, #0x7ffffffffffffff0
    e9f4:	sub	x23, x8, x9
    e9f8:	mov	sp, x23
    e9fc:	mov	x0, x23
    ea00:	mov	x1, x14
    ea04:	mov	x2, x22
    ea08:	mov	w3, w20
    ea0c:	bl	c1d0 <__gmpn_set_str@plt>
    ea10:	subs	x8, x0, x21
    ea14:	add	x9, x23, x8, lsl #3
    ea18:	mov	x24, x0
    ea1c:	csel	x22, x21, x0, gt
    ea20:	csel	x23, x9, x23, gt
    ea24:	csel	x14, x8, xzr, gt
    ea28:	cbz	x27, eab4 <__gmpf_set_str@@Base+0x39c>
    ea2c:	ldrb	w9, [x27]
    ea30:	cmp	w9, #0x2d
    ea34:	cset	w10, eq  // eq = none
    ea38:	csetm	x8, eq  // eq = none
    ea3c:	cmp	w9, #0x2b
    ea40:	cset	w9, eq  // eq = none
    ea44:	orr	w12, w10, w9
    ea48:	add	x11, x27, x12
    ea4c:	ldrb	w9, [x11]
    ea50:	ldur	x10, [x29, #-40]
    ea54:	ldrb	w9, [x28, x9]
    ea58:	sxtw	x10, w10
    ea5c:	cmp	x9, x10
    ea60:	b.ge	eabc <__gmpf_set_str@@Base+0x3a4>  // b.tcont
    ea64:	ldrb	w11, [x11, #1]
    ea68:	ldrb	w11, [x28, x11]
    ea6c:	cmp	x11, x10
    ea70:	b.ge	ea90 <__gmpf_set_str@@Base+0x378>  // b.tcont
    ea74:	add	x12, x12, x27
    ea78:	add	x12, x12, #0x2
    ea7c:	ldrb	w13, [x12], #1
    ea80:	madd	x9, x9, x10, x11
    ea84:	ldrb	w11, [x28, x13]
    ea88:	cmp	x11, x10
    ea8c:	b.lt	ea7c <__gmpf_set_str@@Base+0x364>  // b.tstop
    ea90:	eor	x9, x9, x8
    ea94:	sub	x8, x9, x8
    ea98:	mov	w9, #0x1                   	// #1
    ea9c:	cbz	w9, ed9c <__gmpf_set_str@@Base+0x684>
    eaa0:	b	eacc <__gmpf_set_str@@Base+0x3b4>
    eaa4:	ldur	x8, [x29, #-24]
    eaa8:	str	wzr, [x8, #4]
    eaac:	str	xzr, [x8, #8]
    eab0:	b	ed44 <__gmpf_set_str@@Base+0x62c>
    eab4:	mov	x8, xzr
    eab8:	b	eacc <__gmpf_set_str@@Base+0x3b4>
    eabc:	ldur	x0, [x29, #-8]
    eac0:	cbnz	x0, ed88 <__gmpf_set_str@@Base+0x670>
    eac4:	mov	w9, wzr
    eac8:	cbz	w9, ed9c <__gmpf_set_str@@Base+0x684>
    eacc:	ldur	x10, [x29, #-48]
    ead0:	sub	x9, x26, x25
    ead4:	cmp	x25, #0x0
    ead8:	add	x9, x9, x10
    eadc:	csel	x9, xzr, x9, eq  // eq = none
    eae0:	subs	x27, x8, x9
    eae4:	cneg	x25, x27, mi  // mi = first
    eae8:	cbz	x25, eb90 <__gmpf_set_str@@Base+0x478>
    eaec:	add	x19, x21, #0x1
    eaf0:	lsl	x1, x19, #5
    eaf4:	mov	w8, #0x7f00                	// #32512
    eaf8:	mov	x26, x14
    eafc:	mov	w24, w20
    eb00:	cmp	x1, x8
    eb04:	lsl	x28, x19, #1
    eb08:	b.hi	ed78 <__gmpf_set_str@@Base+0x660>  // b.pmore
    eb0c:	add	x9, x1, #0xf
    eb10:	mov	x8, sp
    eb14:	and	x9, x9, #0xfffffffffffffff0
    eb18:	sub	x20, x8, x9
    eb1c:	mov	sp, x20
    eb20:	add	x5, x20, x28, lsl #3
    eb24:	sub	x1, x29, #0x10
    eb28:	mov	x0, x20
    eb2c:	mov	x2, x24
    eb30:	mov	x3, x25
    eb34:	mov	x4, x21
    eb38:	bl	edf8 <__gmpf_set_str@@Base+0x6e0>
    eb3c:	mov	x24, x0
    eb40:	tbnz	x27, #63, ebbc <__gmpf_set_str@@Base+0x4a4>
    eb44:	add	x19, x24, x22
    eb48:	lsl	x1, x19, #3
    eb4c:	mov	w8, #0x7f00                	// #32512
    eb50:	cmp	x1, x8
    eb54:	b.hi	edc0 <__gmpf_set_str@@Base+0x6a8>  // b.pmore
    eb58:	add	x9, x1, #0xf
    eb5c:	mov	x8, sp
    eb60:	and	x9, x9, #0xfffffffffffffff0
    eb64:	sub	x25, x8, x9
    eb68:	mov	sp, x25
    eb6c:	ldur	w27, [x29, #-28]
    eb70:	mov	x0, x25
    eb74:	cmp	x24, x22
    eb78:	b.le	ecd4 <__gmpf_set_str@@Base+0x5bc>
    eb7c:	mov	x1, x20
    eb80:	mov	x2, x24
    eb84:	mov	x3, x23
    eb88:	mov	x4, x22
    eb8c:	b	ece4 <__gmpf_set_str@@Base+0x5cc>
    eb90:	ldr	x0, [x19, #16]
    eb94:	mov	x1, x23
    eb98:	mov	x2, x22
    eb9c:	bl	cc10 <__gmpn_copyi@plt>
    eba0:	ldur	w9, [x29, #-28]
    eba4:	neg	w8, w22
    eba8:	str	x24, [x19, #8]
    ebac:	cmp	w9, #0x0
    ebb0:	csel	x8, x22, x8, eq  // eq = none
    ebb4:	str	w8, [x19, #4]
    ebb8:	b	ed44 <__gmpf_set_str@@Base+0x62c>
    ebbc:	cmp	x24, x22
    ebc0:	b.le	ec24 <__gmpf_set_str@@Base+0x50c>
    ebc4:	lsl	x8, x24, #3
    ebc8:	add	x1, x8, #0x8
    ebcc:	mov	w8, #0x7f00                	// #32512
    ebd0:	cmp	x1, x8
    ebd4:	b.hi	ede0 <__gmpf_set_str@@Base+0x6c8>  // b.pmore
    ebd8:	add	x9, x1, #0xf
    ebdc:	mov	x8, sp
    ebe0:	and	x9, x9, #0xfffffffffffffff0
    ebe4:	sub	x25, x8, x9
    ebe8:	mov	sp, x25
    ebec:	subs	x27, x24, x22
    ebf0:	b.eq	ec04 <__gmpf_set_str@@Base+0x4ec>  // b.none
    ebf4:	lsl	x2, x27, #3
    ebf8:	mov	x0, x25
    ebfc:	mov	w1, wzr
    ec00:	bl	c780 <memset@plt>
    ec04:	add	x8, x25, x24, lsl #3
    ec08:	sub	x0, x8, x22, lsl #3
    ec0c:	mov	x1, x23
    ec10:	mov	x2, x22
    ec14:	bl	cc10 <__gmpn_copyi@plt>
    ec18:	sub	x26, x26, x27
    ec1c:	mov	x22, x24
    ec20:	mov	x23, x25
    ec24:	add	x8, x20, x24, lsl #3
    ec28:	ldur	x8, [x8, #-8]
    ec2c:	ldur	w27, [x29, #-28]
    ec30:	tbnz	x8, #63, ec70 <__gmpf_set_str@@Base+0x558>
    ec34:	clz	x25, x8
    ec38:	mov	x0, x20
    ec3c:	mov	x1, x20
    ec40:	mov	x2, x24
    ec44:	mov	w3, w25
    ec48:	bl	c2d0 <__gmpn_lshift@plt>
    ec4c:	mov	x0, x23
    ec50:	mov	x1, x23
    ec54:	mov	x2, x22
    ec58:	mov	w3, w25
    ec5c:	bl	c2d0 <__gmpn_lshift@plt>
    ec60:	cbz	x0, ec70 <__gmpf_set_str@@Base+0x558>
    ec64:	add	x8, x22, #0x1
    ec68:	str	x0, [x23, x22, lsl #3]
    ec6c:	mov	x22, x8
    ec70:	lsl	x1, x19, #3
    ec74:	mov	w8, #0x7f00                	// #32512
    ec78:	cmp	x1, x8
    ec7c:	b.hi	edd0 <__gmpf_set_str@@Base+0x6b8>  // b.pmore
    ec80:	add	x9, x1, #0xf
    ec84:	mov	x8, sp
    ec88:	and	x9, x9, #0xfffffffffffffff0
    ec8c:	sub	x25, x8, x9
    ec90:	mov	sp, x25
    ec94:	sub	x19, x22, x24
    ec98:	sub	x1, x21, x19
    ec9c:	mov	x0, x25
    eca0:	mov	x2, x23
    eca4:	mov	x3, x22
    eca8:	mov	x4, x20
    ecac:	mov	x5, x24
    ecb0:	bl	d580 <__gmpn_divrem@plt>
    ecb4:	ldur	x8, [x29, #-16]
    ecb8:	add	x9, x0, x19
    ecbc:	sub	x8, x26, x8
    ecc0:	add	x19, x9, x8
    ecc4:	cbz	x0, ed1c <__gmpf_set_str@@Base+0x604>
    ecc8:	str	x0, [x25, x21, lsl #3]
    eccc:	add	x25, x25, #0x8
    ecd0:	b	ed1c <__gmpf_set_str@@Base+0x604>
    ecd4:	mov	x1, x23
    ecd8:	mov	x2, x22
    ecdc:	mov	x3, x20
    ece0:	mov	x4, x24
    ece4:	bl	cea0 <__gmpn_mul@plt>
    ece8:	add	x8, x25, x19, lsl #3
    ecec:	ldur	x8, [x8, #-8]
    ecf0:	ldur	x9, [x29, #-16]
    ecf4:	cmp	x8, #0x0
    ecf8:	cset	w8, eq  // eq = none
    ecfc:	sub	x8, x19, x8
    ed00:	add	x10, x9, x26
    ed04:	subs	x9, x8, x21
    ed08:	add	x19, x10, x8
    ed0c:	b.le	ed18 <__gmpf_set_str@@Base+0x600>
    ed10:	add	x25, x25, x9, lsl #3
    ed14:	b	ed1c <__gmpf_set_str@@Base+0x604>
    ed18:	mov	x21, x8
    ed1c:	ldur	x20, [x29, #-24]
    ed20:	mov	x1, x25
    ed24:	mov	x2, x21
    ed28:	ldr	x0, [x20, #16]
    ed2c:	bl	cc10 <__gmpn_copyi@plt>
    ed30:	neg	w8, w21
    ed34:	cmp	w27, #0x0
    ed38:	csel	x8, x21, x8, eq  // eq = none
    ed3c:	str	w8, [x20, #4]
    ed40:	str	x19, [x20, #8]
    ed44:	ldur	x8, [x29, #-8]
    ed48:	mov	w0, wzr
    ed4c:	cbz	x8, eda0 <__gmpf_set_str@@Base+0x688>
    ed50:	mov	x0, x8
    ed54:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    ed58:	mov	w0, wzr
    ed5c:	b	eda0 <__gmpf_set_str@@Base+0x688>
    ed60:	sub	x0, x29, #0x8
    ed64:	mov	x23, x14
    ed68:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    ed6c:	mov	x14, x23
    ed70:	mov	x23, x0
    ed74:	b	e9fc <__gmpf_set_str@@Base+0x2e4>
    ed78:	sub	x0, x29, #0x8
    ed7c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    ed80:	mov	x20, x0
    ed84:	b	eb20 <__gmpf_set_str@@Base+0x408>
    ed88:	mov	x27, x14
    ed8c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    ed90:	mov	x14, x27
    ed94:	mov	w9, wzr
    ed98:	cbnz	w9, eacc <__gmpf_set_str@@Base+0x3b4>
    ed9c:	mov	w0, #0xffffffff            	// #-1
    eda0:	mov	sp, x29
    eda4:	ldp	x20, x19, [sp, #80]
    eda8:	ldp	x22, x21, [sp, #64]
    edac:	ldp	x24, x23, [sp, #48]
    edb0:	ldp	x26, x25, [sp, #32]
    edb4:	ldp	x28, x27, [sp, #16]
    edb8:	ldp	x29, x30, [sp], #96
    edbc:	ret
    edc0:	sub	x0, x29, #0x8
    edc4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    edc8:	mov	x25, x0
    edcc:	b	eb6c <__gmpf_set_str@@Base+0x454>
    edd0:	sub	x0, x29, #0x8
    edd4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    edd8:	mov	x25, x0
    eddc:	b	ec94 <__gmpf_set_str@@Base+0x57c>
    ede0:	sub	x0, x29, #0x8
    ede4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    ede8:	mov	x25, x0
    edec:	subs	x27, x24, x22
    edf0:	b.ne	ebf4 <__gmpf_set_str@@Base+0x4dc>  // b.any
    edf4:	b	ec04 <__gmpf_set_str@@Base+0x4ec>
    edf8:	sub	sp, sp, #0x70
    edfc:	clz	x9, x3
    ee00:	stp	x22, x21, [sp, #80]
    ee04:	stp	x20, x19, [sp, #96]
    ee08:	mov	x21, x4
    ee0c:	mov	x20, x0
    ee10:	cmp	w9, #0x3e
    ee14:	stp	x29, x30, [sp, #16]
    ee18:	stp	x28, x27, [sp, #32]
    ee1c:	stp	x26, x25, [sp, #48]
    ee20:	stp	x24, x23, [sp, #64]
    ee24:	add	x29, sp, #0x10
    ee28:	str	x1, [sp, #8]
    ee2c:	str	x2, [x0]
    ee30:	b.hi	eee8 <__gmpf_set_str@@Base+0x7d0>  // b.pmore
    ee34:	mov	w10, #0x3e                  	// #62
    ee38:	mov	w11, #0x3f                  	// #63
    ee3c:	mov	x22, x5
    ee40:	mov	x23, x3
    ee44:	mov	x24, x2
    ee48:	mov	x27, xzr
    ee4c:	mov	x8, xzr
    ee50:	sub	w28, w10, w9
    ee54:	sub	w19, w11, w9
    ee58:	mov	w25, #0x1                   	// #1
    ee5c:	mov	x9, x20
    ee60:	b	ee78 <__gmpf_set_str@@Base+0x760>
    ee64:	sub	w19, w19, #0x1
    ee68:	cmp	w19, #0x0
    ee6c:	sub	x28, x28, #0x1
    ee70:	mov	x9, x26
    ee74:	b.le	eef8 <__gmpf_set_str@@Base+0x7e0>
    ee78:	mov	x26, x22
    ee7c:	add	x1, x9, x8, lsl #3
    ee80:	mov	x0, x26
    ee84:	mov	x2, x25
    ee88:	mov	x22, x9
    ee8c:	bl	ca90 <__gmpn_sqr@plt>
    ee90:	add	x8, x26, x25, lsl #4
    ee94:	ldur	x8, [x8, #-8]
    ee98:	lsl	x9, x25, #1
    ee9c:	cmp	x8, #0x0
    eea0:	cset	w8, eq  // eq = none
    eea4:	sub	x9, x9, x8
    eea8:	subs	x8, x9, x21
    eeac:	csel	x8, x8, xzr, gt
    eeb0:	csel	x25, x21, x9, gt
    eeb4:	lsr	x9, x23, x28
    eeb8:	add	x27, x8, x27, lsl #1
    eebc:	tbz	w9, #0, ee64 <__gmpf_set_str@@Base+0x74c>
    eec0:	add	x1, x26, x8, lsl #3
    eec4:	mov	x0, x26
    eec8:	mov	x2, x25
    eecc:	mov	x3, x24
    eed0:	bl	d670 <__gmpn_mul_1@plt>
    eed4:	cmp	x0, #0x0
    eed8:	mov	x8, xzr
    eedc:	str	x0, [x26, x25, lsl #3]
    eee0:	cinc	x25, x25, ne  // ne = any
    eee4:	b	ee64 <__gmpf_set_str@@Base+0x74c>
    eee8:	mov	x8, xzr
    eeec:	mov	x27, xzr
    eef0:	mov	w25, #0x1                   	// #1
    eef4:	mov	x26, x20
    eef8:	subs	x9, x25, x21
    eefc:	add	x10, x26, x9, lsl #3
    ef00:	csel	x9, x9, xzr, gt
    ef04:	csel	x21, x21, x25, gt
    ef08:	add	x19, x9, x27
    ef0c:	csel	x9, x10, x26, gt
    ef10:	add	x1, x9, x8, lsl #3
    ef14:	mov	x0, x20
    ef18:	mov	x2, x21
    ef1c:	bl	cc10 <__gmpn_copyi@plt>
    ef20:	ldr	x8, [sp, #8]
    ef24:	mov	x0, x21
    ef28:	str	x19, [x8]
    ef2c:	ldp	x20, x19, [sp, #96]
    ef30:	ldp	x22, x21, [sp, #80]
    ef34:	ldp	x24, x23, [sp, #64]
    ef38:	ldp	x26, x25, [sp, #48]
    ef3c:	ldp	x28, x27, [sp, #32]
    ef40:	ldp	x29, x30, [sp, #16]
    ef44:	add	sp, sp, #0x70
    ef48:	ret

000000000000ef4c <__gmpf_set_d@@Base>:
    ef4c:	stp	x29, x30, [sp, #-32]!
    ef50:	fmov	x8, d0
    ef54:	mvn	x8, x8
    ef58:	tst	x8, #0x7ff0000000000000
    ef5c:	str	x19, [sp, #16]
    ef60:	mov	x29, sp
    ef64:	b.eq	efb4 <__gmpf_set_d@@Base+0x68>  // b.none
    ef68:	mov	x19, x0
    ef6c:	fcmp	d0, #0.0
    ef70:	b.eq	efa8 <__gmpf_set_d@@Base+0x5c>  // b.none
    ef74:	ldr	x0, [x19, #16]
    ef78:	fneg	d1, d0
    ef7c:	mov	w8, #0x2                   	// #2
    ef80:	mov	w9, #0xfffffffe            	// #-2
    ef84:	fcsel	d0, d0, d1, ge  // ge = tcont
    ef88:	csel	w8, w9, w8, mi  // mi = first
    ef8c:	str	w8, [x19, #4]
    ef90:	bl	d460 <__gmp_extract_double@plt>
    ef94:	sxtw	x8, w0
    ef98:	str	x8, [x19, #8]
    ef9c:	ldr	x19, [sp, #16]
    efa0:	ldp	x29, x30, [sp], #32
    efa4:	ret
    efa8:	mov	x8, xzr
    efac:	str	wzr, [x19, #4]
    efb0:	b	ef98 <__gmpf_set_d@@Base+0x4c>
    efb4:	bl	c300 <__gmp_invalid_operation@plt>

000000000000efb8 <__gmpf_set_z@@Base>:
    efb8:	stp	x29, x30, [sp, #-16]!
    efbc:	ldrsw	x10, [x1, #4]
    efc0:	ldrsw	x9, [x0]
    efc4:	ldr	x11, [x1, #8]
    efc8:	ldr	x8, [x0, #16]
    efcc:	cmp	x10, #0x0
    efd0:	add	x12, x9, #0x1
    efd4:	cneg	x13, x10, mi  // mi = first
    efd8:	subs	x12, x13, x12
    efdc:	add	x12, x11, x12, lsl #3
    efe0:	csinc	x2, x13, x9, le
    efe4:	csel	x1, x12, x11, gt
    efe8:	neg	w9, w2
    efec:	cmp	w10, #0x0
    eff0:	csel	x9, x2, x9, ge  // ge = tcont
    eff4:	str	x13, [x0, #8]
    eff8:	str	w9, [x0, #4]
    effc:	mov	x0, x8
    f000:	mov	x29, sp
    f004:	bl	cc10 <__gmpn_copyi@plt>
    f008:	ldp	x29, x30, [sp], #16
    f00c:	ret

000000000000f010 <__gmpf_init_set@@Base>:
    f010:	stp	x29, x30, [sp, #-48]!
    f014:	stp	x22, x21, [sp, #16]
    f018:	stp	x20, x19, [sp, #32]
    f01c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f020:	ldr	x8, [x8, #3960]
    f024:	mov	x20, x0
    f028:	mov	x29, sp
    f02c:	mov	x19, x1
    f030:	ldr	x21, [x8]
    f034:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f038:	ldr	x8, [x8, #3840]
    f03c:	add	x22, x21, #0x1
    f040:	lsl	x0, x22, #3
    f044:	ldr	x8, [x8]
    f048:	blr	x8
    f04c:	str	x0, [x20, #16]
    f050:	str	w21, [x20]
    f054:	ldrsw	x8, [x19, #4]
    f058:	ldp	x9, x10, [x19, #8]
    f05c:	cmp	x8, #0x0
    f060:	str	x9, [x20, #8]
    f064:	cneg	x9, x8, mi  // mi = first
    f068:	subs	x11, x9, x22
    f06c:	add	x11, x10, x11, lsl #3
    f070:	csinc	x2, x9, x21, le
    f074:	csel	x1, x11, x10, gt
    f078:	neg	w9, w2
    f07c:	cmp	w8, #0x0
    f080:	csel	x8, x2, x9, ge  // ge = tcont
    f084:	str	w8, [x20, #4]
    f088:	bl	cc10 <__gmpn_copyi@plt>
    f08c:	ldp	x20, x19, [sp, #32]
    f090:	ldp	x22, x21, [sp, #16]
    f094:	ldp	x29, x30, [sp], #48
    f098:	ret

000000000000f09c <__gmpf_init_set_ui@@Base>:
    f09c:	stp	x29, x30, [sp, #-32]!
    f0a0:	stp	x20, x19, [sp, #16]
    f0a4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f0a8:	ldr	x8, [x8, #3960]
    f0ac:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f0b0:	mov	x20, x0
    f0b4:	mov	x29, sp
    f0b8:	ldr	x8, [x8]
    f0bc:	mov	x19, x1
    f0c0:	str	w8, [x0]
    f0c4:	ldr	x9, [x9, #3840]
    f0c8:	lsl	x8, x8, #3
    f0cc:	add	x0, x8, #0x8
    f0d0:	ldr	x9, [x9]
    f0d4:	blr	x9
    f0d8:	cmp	x19, #0x0
    f0dc:	cset	w8, ne  // ne = any
    f0e0:	str	x0, [x20, #16]
    f0e4:	str	x19, [x0]
    f0e8:	str	w8, [x20, #4]
    f0ec:	str	x8, [x20, #8]
    f0f0:	ldp	x20, x19, [sp, #16]
    f0f4:	ldp	x29, x30, [sp], #32
    f0f8:	ret

000000000000f0fc <__gmpf_init_set_si@@Base>:
    f0fc:	stp	x29, x30, [sp, #-32]!
    f100:	stp	x20, x19, [sp, #16]
    f104:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f108:	ldr	x8, [x8, #3960]
    f10c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f110:	mov	x20, x0
    f114:	mov	x29, sp
    f118:	ldr	x8, [x8]
    f11c:	mov	x19, x1
    f120:	str	w8, [x0]
    f124:	ldr	x9, [x9, #3840]
    f128:	lsl	x8, x8, #3
    f12c:	add	x0, x8, #0x8
    f130:	ldr	x9, [x9]
    f134:	blr	x9
    f138:	cmp	x19, #0x0
    f13c:	cneg	x8, x19, mi  // mi = first
    f140:	cset	w9, ne  // ne = any
    f144:	csetm	x10, ne  // ne = any
    f148:	str	x0, [x20, #16]
    f14c:	str	x8, [x0]
    f150:	csel	x8, x9, x10, ge  // ge = tcont
    f154:	str	x9, [x20, #8]
    f158:	str	w8, [x20, #4]
    f15c:	ldp	x20, x19, [sp, #16]
    f160:	ldp	x29, x30, [sp], #32
    f164:	ret

000000000000f168 <__gmpf_init_set_str@@Base>:
    f168:	stp	x29, x30, [sp, #-48]!
    f16c:	str	x21, [sp, #16]
    f170:	stp	x20, x19, [sp, #32]
    f174:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f178:	ldr	x8, [x8, #3960]
    f17c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f180:	mov	x21, x0
    f184:	mov	x29, sp
    f188:	ldr	x8, [x8]
    f18c:	str	xzr, [x0, #8]
    f190:	mov	w19, w2
    f194:	mov	x20, x1
    f198:	stp	w8, wzr, [x0]
    f19c:	ldr	x9, [x9, #3840]
    f1a0:	lsl	x8, x8, #3
    f1a4:	add	x0, x8, #0x8
    f1a8:	ldr	x9, [x9]
    f1ac:	blr	x9
    f1b0:	str	x0, [x21, #16]
    f1b4:	mov	x0, x21
    f1b8:	mov	x1, x20
    f1bc:	mov	w2, w19
    f1c0:	bl	c310 <__gmpf_set_str@plt>
    f1c4:	ldp	x20, x19, [sp, #32]
    f1c8:	ldr	x21, [sp, #16]
    f1cc:	ldp	x29, x30, [sp], #48
    f1d0:	ret

000000000000f1d4 <__gmpf_init_set_d@@Base>:
    f1d4:	str	d8, [sp, #-32]!
    f1d8:	stp	x29, x30, [sp, #8]
    f1dc:	str	x19, [sp, #24]
    f1e0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f1e4:	ldr	x8, [x8, #3960]
    f1e8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f1ec:	mov	x19, x0
    f1f0:	mov	x29, sp
    f1f4:	ldr	x8, [x8]
    f1f8:	mov	v8.16b, v0.16b
    f1fc:	str	w8, [x0]
    f200:	ldr	x9, [x9, #3840]
    f204:	lsl	x8, x8, #3
    f208:	add	x0, x8, #0x8
    f20c:	ldr	x9, [x9]
    f210:	blr	x9
    f214:	str	x0, [x19, #16]
    f218:	mov	x0, x19
    f21c:	mov	v0.16b, v8.16b
    f220:	bl	c610 <__gmpf_set_d@plt>
    f224:	ldr	x19, [sp, #24]
    f228:	ldp	x29, x30, [sp, #8]
    f22c:	ldr	d8, [sp], #32
    f230:	ret

000000000000f234 <__gmpf_clear@@Base>:
    f234:	stp	x29, x30, [sp, #-16]!
    f238:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f23c:	ldr	x8, [x8, #4016]
    f240:	ldrsw	x9, [x0]
    f244:	ldr	x0, [x0, #16]
    f248:	mov	x29, sp
    f24c:	ldr	x8, [x8]
    f250:	lsl	x9, x9, #3
    f254:	add	x1, x9, #0x8
    f258:	blr	x8
    f25c:	ldp	x29, x30, [sp], #16
    f260:	ret

000000000000f264 <__gmpf_clears@@Base>:
    f264:	sub	sp, sp, #0x100
    f268:	stp	x29, x30, [sp, #224]
    f26c:	add	x29, sp, #0xe0
    f270:	mov	x8, #0xffffffffffffffc8    	// #-56
    f274:	mov	x9, sp
    f278:	sub	x10, x29, #0x58
    f27c:	movk	x8, #0xff80, lsl #32
    f280:	add	x11, x29, #0x20
    f284:	add	x9, x9, #0x80
    f288:	add	x10, x10, #0x38
    f28c:	str	x19, [sp, #240]
    f290:	stp	x1, x2, [x29, #-88]
    f294:	stp	x3, x4, [x29, #-72]
    f298:	stp	x5, x6, [x29, #-56]
    f29c:	stur	x7, [x29, #-40]
    f2a0:	stp	q0, q1, [sp]
    f2a4:	stp	q2, q3, [sp, #32]
    f2a8:	stp	q4, q5, [sp, #64]
    f2ac:	stp	q6, q7, [sp, #96]
    f2b0:	stp	x9, x8, [x29, #-16]
    f2b4:	stp	x11, x10, [x29, #-32]
    f2b8:	adrp	x19, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f2bc:	ldr	x19, [x19, #4016]
    f2c0:	b	f2d8 <__gmpf_clears@@Base+0x74>
    f2c4:	ldur	x8, [x29, #-32]
    f2c8:	add	x9, x8, #0x8
    f2cc:	stur	x9, [x29, #-32]
    f2d0:	ldr	x0, [x8]
    f2d4:	cbz	x0, f318 <__gmpf_clears@@Base+0xb4>
    f2d8:	ldrsw	x8, [x0]
    f2dc:	ldr	x9, [x19]
    f2e0:	ldr	x0, [x0, #16]
    f2e4:	lsl	x8, x8, #3
    f2e8:	add	x1, x8, #0x8
    f2ec:	blr	x9
    f2f0:	ldursw	x8, [x29, #-8]
    f2f4:	tbz	w8, #31, f2c4 <__gmpf_clears@@Base+0x60>
    f2f8:	add	w9, w8, #0x8
    f2fc:	cmp	w9, #0x0
    f300:	stur	w9, [x29, #-8]
    f304:	b.gt	f2c4 <__gmpf_clears@@Base+0x60>
    f308:	ldur	x9, [x29, #-24]
    f30c:	add	x8, x9, x8
    f310:	ldr	x0, [x8]
    f314:	cbnz	x0, f2d8 <__gmpf_clears@@Base+0x74>
    f318:	ldr	x19, [sp, #240]
    f31c:	ldp	x29, x30, [sp, #224]
    f320:	add	sp, sp, #0x100
    f324:	ret

000000000000f328 <__gmpf_get_str@@Base>:
    f328:	stp	x29, x30, [sp, #-96]!
    f32c:	stp	x28, x27, [sp, #16]
    f330:	stp	x26, x25, [sp, #32]
    f334:	stp	x24, x23, [sp, #48]
    f338:	stp	x22, x21, [sp, #64]
    f33c:	stp	x20, x19, [sp, #80]
    f340:	mov	x29, sp
    f344:	sub	sp, sp, #0x60
    f348:	ldr	w8, [x4, #4]
    f34c:	ldp	x11, x22, [x4, #8]
    f350:	mov	x20, x4
    f354:	mov	w21, w2
    f358:	cmp	w8, #0x0
    f35c:	mov	x23, x1
    f360:	cneg	w25, w8, mi  // mi = first
    f364:	cmp	w2, #0x2
    f368:	mov	x27, x0
    f36c:	b.lt	f384 <__gmpf_get_str@@Base+0x5c>  // b.tstop
    f370:	cmp	w21, #0x25
    f374:	b.ge	f394 <__gmpf_get_str@@Base+0x6c>  // b.tcont
    f378:	adrp	x19, 4c000 <__gmp_randclear_mt@@Base+0x18>
    f37c:	add	x19, x19, #0x79d
    f380:	b	f3b4 <__gmpf_get_str@@Base+0x8c>
    f384:	cmn	w21, #0x2
    f388:	b.le	f3a0 <__gmpf_get_str@@Base+0x78>
    f38c:	mov	w21, #0xa                   	// #10
    f390:	b	f3ac <__gmpf_get_str@@Base+0x84>
    f394:	cmp	w21, #0x3e
    f398:	b.le	f3ac <__gmpf_get_str@@Base+0x84>
    f39c:	b	f848 <__gmpf_get_str@@Base+0x520>
    f3a0:	cmn	w21, #0x24
    f3a4:	b.lt	f848 <__gmpf_get_str@@Base+0x520>  // b.tstop
    f3a8:	neg	w21, w21
    f3ac:	adrp	x19, 4c000 <__gmp_randclear_mt@@Base+0x18>
    f3b0:	add	x19, x19, #0x75e
    f3b4:	adrp	x26, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f3b8:	ldr	x26, [x26, #3936]
    f3bc:	ldrsw	x9, [x20]
    f3c0:	mov	w8, #0x28                  	// #40
    f3c4:	smaddl	x8, w21, w8, x26
    f3c8:	ldr	x28, [x8, #8]
    f3cc:	lsl	x8, x9, #6
    f3d0:	sub	x8, x8, #0x40
    f3d4:	umulh	x8, x28, x8
    f3d8:	add	x8, x8, #0x2
    f3dc:	sub	x9, x3, #0x1
    f3e0:	cmp	x9, x8
    f3e4:	csel	x9, x3, x8, cc  // cc = lo, ul, last
    f3e8:	stur	x9, [x29, #-24]
    f3ec:	cbz	x27, f5a4 <__gmpf_get_str@@Base+0x27c>
    f3f0:	mov	x24, xzr
    f3f4:	cbz	w25, f5cc <__gmpf_get_str@@Base+0x2a4>
    f3f8:	ldur	x10, [x29, #-24]
    f3fc:	mov	w8, #0x7f00                	// #32512
    f400:	stp	x24, x23, [x29, #-48]
    f404:	sxtw	x24, w21
    f408:	add	x1, x10, #0x83
    f40c:	cmp	x1, x8
    f410:	stur	x20, [x29, #-32]
    f414:	stur	xzr, [x29, #-8]
    f418:	stur	x27, [x29, #-64]
    f41c:	b.hi	f814 <__gmpf_get_str@@Base+0x4ec>  // b.pmore
    f420:	add	x9, x1, #0xf
    f424:	mov	x8, sp
    f428:	and	x9, x9, #0xfffffffffffffff0
    f42c:	sub	x23, x8, x9
    f430:	mov	sp, x23
    f434:	mov	w8, #0x28                  	// #40
    f438:	madd	x8, x24, x8, x26
    f43c:	ldr	x8, [x8, #16]
    f440:	umulh	x8, x8, x10
    f444:	ubfx	x21, x8, #3, #58
    f448:	add	x20, x21, #0x2
    f44c:	subs	x8, x25, x20
    f450:	lsl	x9, x20, #1
    f454:	add	x8, x22, x8, lsl #3
    f458:	csel	x27, x20, x25, hi  // hi = pmore
    f45c:	add	x25, x9, #0x4
    f460:	csel	x8, x8, x22, hi  // hi = pmore
    f464:	stur	x8, [x29, #-72]
    f468:	lsl	x1, x25, #4
    f46c:	mov	w8, #0x7f00                	// #32512
    f470:	cmp	x1, x8
    f474:	b.hi	f830 <__gmpf_get_str@@Base+0x508>  // b.pmore
    f478:	add	x9, x1, #0xf
    f47c:	mov	x8, sp
    f480:	and	x9, x9, #0xfffffffffffffff0
    f484:	sub	x26, x8, x9
    f488:	mov	sp, x26
    f48c:	subs	x22, x11, x20
    f490:	add	x25, x26, x25, lsl #3
    f494:	stur	x11, [x29, #-56]
    f498:	b.le	f5e0 <__gmpf_get_str@@Base+0x2b8>
    f49c:	add	x4, x21, #0x3
    f4a0:	sub	x1, x29, #0x10
    f4a4:	mov	x0, x26
    f4a8:	mov	x2, x24
    f4ac:	mov	x5, x25
    f4b0:	lsl	x8, x22, #6
    f4b4:	umulh	x3, x28, x8
    f4b8:	stur	x3, [x29, #-88]
    f4bc:	bl	f870 <__gmpf_get_str@@Base+0x548>
    f4c0:	ldur	x21, [x29, #-16]
    f4c4:	stur	x0, [x29, #-80]
    f4c8:	sub	x8, x22, x21
    f4cc:	add	x22, x8, x20
    f4d0:	lsl	x1, x22, #3
    f4d4:	mov	w8, #0x7f00                	// #32512
    f4d8:	cmp	x1, x8
    f4dc:	b.hi	f850 <__gmpf_get_str@@Base+0x528>  // b.pmore
    f4e0:	add	x9, x1, #0xf
    f4e4:	mov	x8, sp
    f4e8:	and	x9, x9, #0xfffffffffffffff0
    f4ec:	sub	x28, x8, x9
    f4f0:	mov	sp, x28
    f4f4:	ldur	x8, [x29, #-56]
    f4f8:	subs	x20, x22, x27
    f4fc:	b.eq	f518 <__gmpf_get_str@@Base+0x1f0>  // b.none
    f500:	sub	x8, x8, x21
    f504:	sub	x8, x8, x27
    f508:	lsl	x2, x8, #3
    f50c:	mov	x0, x28
    f510:	mov	w1, wzr
    f514:	bl	c780 <memset@plt>
    f518:	ldur	x1, [x29, #-72]
    f51c:	add	x0, x28, x20, lsl #3
    f520:	mov	x2, x27
    f524:	bl	cc10 <__gmpn_copyi@plt>
    f528:	ldur	x21, [x29, #-80]
    f52c:	mov	w8, #0x7f00                	// #32512
    f530:	lsl	x1, x21, #3
    f534:	cmp	x1, x8
    f538:	b.hi	f860 <__gmpf_get_str@@Base+0x538>  // b.pmore
    f53c:	add	x9, x1, #0xf
    f540:	mov	x8, sp
    f544:	and	x9, x9, #0xfffffffffffffff0
    f548:	sub	x1, x8, x9
    f54c:	mov	sp, x1
    f550:	ldur	x27, [x29, #-64]
    f554:	ldur	x20, [x29, #-88]
    f558:	mov	x0, x25
    f55c:	mov	x2, xzr
    f560:	mov	x3, x28
    f564:	mov	x4, x22
    f568:	mov	x5, x26
    f56c:	mov	x6, x21
    f570:	bl	c030 <__gmpn_tdiv_qr@plt>
    f574:	sub	x8, x22, x21
    f578:	ldr	x9, [x25, x8, lsl #3]
    f57c:	mov	x0, x23
    f580:	mov	w1, w24
    f584:	mov	x2, x25
    f588:	cmp	x9, #0x0
    f58c:	cset	w9, eq  // eq = none
    f590:	sub	x8, x8, x9
    f594:	add	x3, x8, #0x1
    f598:	bl	cc50 <__gmpn_get_str@plt>
    f59c:	add	x8, x0, x20
    f5a0:	b	f6c0 <__gmpf_get_str@@Base+0x398>
    f5a4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f5a8:	ldr	x8, [x8, #3840]
    f5ac:	add	x24, x9, #0x2
    f5b0:	mov	x0, x24
    f5b4:	mov	x27, x11
    f5b8:	ldr	x8, [x8]
    f5bc:	blr	x8
    f5c0:	mov	x11, x27
    f5c4:	mov	x27, x0
    f5c8:	cbnz	w25, f3f8 <__gmpf_get_str@@Base+0xd0>
    f5cc:	mov	x20, xzr
    f5d0:	str	xzr, [x23]
    f5d4:	strb	wzr, [x27]
    f5d8:	cbnz	x24, f754 <__gmpf_get_str@@Base+0x42c>
    f5dc:	b	f77c <__gmpf_get_str@@Base+0x454>
    f5e0:	sub	x8, x20, x11
    f5e4:	lsl	x8, x8, #6
    f5e8:	umulh	x28, x28, x8
    f5ec:	add	x4, x21, #0x3
    f5f0:	sub	x1, x29, #0x10
    f5f4:	mov	x0, x26
    f5f8:	mov	x2, x24
    f5fc:	mov	x3, x28
    f600:	mov	x5, x25
    f604:	bl	f870 <__gmpf_get_str@@Base+0x548>
    f608:	mov	x22, x0
    f60c:	cmp	x27, x0
    f610:	b.le	f62c <__gmpf_get_str@@Base+0x304>
    f614:	ldur	x1, [x29, #-72]
    f618:	mov	x0, x25
    f61c:	mov	x2, x27
    f620:	mov	x3, x26
    f624:	mov	x4, x22
    f628:	b	f640 <__gmpf_get_str@@Base+0x318>
    f62c:	ldur	x3, [x29, #-72]
    f630:	mov	x0, x25
    f634:	mov	x1, x26
    f638:	mov	x2, x22
    f63c:	mov	x4, x27
    f640:	bl	cea0 <__gmpn_mul@plt>
    f644:	add	x8, x22, x27
    f648:	add	x9, x25, x8, lsl #3
    f64c:	ldur	x9, [x9, #-8]
    f650:	ldur	x11, [x29, #-56]
    f654:	ldur	x10, [x29, #-16]
    f658:	cmp	x9, #0x0
    f65c:	sub	x11, x27, x11
    f660:	cset	w9, eq  // eq = none
    f664:	subs	x20, x11, x10
    f668:	sub	x22, x8, x9
    f66c:	b.pl	f6a4 <__gmpf_get_str@@Base+0x37c>  // b.nfrst
    f670:	neg	x8, x20
    f674:	sub	x0, x25, x20, lsl #3
    f678:	mov	x1, x25
    f67c:	mov	x2, x22
    f680:	lsl	x27, x8, #3
    f684:	bl	c130 <__gmpn_copyd@plt>
    f688:	add	x8, x26, x21, lsl #4
    f68c:	add	x0, x8, #0x40
    f690:	mov	w1, wzr
    f694:	mov	x2, x27
    f698:	bl	c780 <memset@plt>
    f69c:	sub	x22, x22, x20
    f6a0:	mov	x20, xzr
    f6a4:	add	x2, x25, x20, lsl #3
    f6a8:	sub	x3, x22, x20
    f6ac:	mov	x0, x23
    f6b0:	mov	w1, w24
    f6b4:	bl	cc50 <__gmpn_get_str@plt>
    f6b8:	ldur	x27, [x29, #-64]
    f6bc:	sub	x8, x0, x28
    f6c0:	ldur	x12, [x29, #-24]
    f6c4:	cmp	x0, x12
    f6c8:	b.ls	f6d8 <__gmpf_get_str@@Base+0x3b0>  // b.plast
    f6cc:	ldrb	w9, [x23, x12]
    f6d0:	cmp	w24, w9, lsl #1
    f6d4:	b.le	f7c0 <__gmpf_get_str@@Base+0x498>
    f6d8:	ldur	x13, [x29, #-32]
    f6dc:	ldur	x24, [x29, #-48]
    f6e0:	cmp	x12, x0
    f6e4:	csel	x9, x0, x12, hi  // hi = pmore
    f6e8:	sub	x10, x23, #0x1
    f6ec:	cbz	x9, f708 <__gmpf_get_str@@Base+0x3e0>
    f6f0:	ldrb	w12, [x10, x9]
    f6f4:	sub	x11, x9, #0x1
    f6f8:	mov	x9, x11
    f6fc:	cbz	w12, f6ec <__gmpf_get_str@@Base+0x3c4>
    f700:	add	x20, x11, #0x1
    f704:	b	f70c <__gmpf_get_str@@Base+0x3e4>
    f708:	mov	x20, xzr
    f70c:	ldr	w9, [x13, #4]
    f710:	add	x9, x27, x9, lsr #31
    f714:	cbz	x20, f734 <__gmpf_get_str@@Base+0x40c>
    f718:	mov	x10, x9
    f71c:	mov	x11, x20
    f720:	ldrb	w12, [x23], #1
    f724:	subs	x11, x11, #0x1
    f728:	ldrb	w12, [x19, x12]
    f72c:	strb	w12, [x10], #1
    f730:	b.ne	f720 <__gmpf_get_str@@Base+0x3f8>  // b.any
    f734:	strb	wzr, [x9, x20]
    f738:	ldur	x9, [x29, #-40]
    f73c:	str	x8, [x9]
    f740:	ldr	w8, [x13, #4]
    f744:	tbnz	w8, #31, f7a0 <__gmpf_get_str@@Base+0x478>
    f748:	ldur	x0, [x29, #-8]
    f74c:	cbnz	x0, f7b4 <__gmpf_get_str@@Base+0x48c>
    f750:	cbz	x24, f77c <__gmpf_get_str@@Base+0x454>
    f754:	add	x2, x20, #0x1
    f758:	cmp	x24, x2
    f75c:	b.eq	f77c <__gmpf_get_str@@Base+0x454>  // b.none
    f760:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    f764:	ldr	x8, [x8, #3792]
    f768:	mov	x0, x27
    f76c:	mov	x1, x24
    f770:	ldr	x8, [x8]
    f774:	blr	x8
    f778:	mov	x27, x0
    f77c:	mov	x0, x27
    f780:	mov	sp, x29
    f784:	ldp	x20, x19, [sp, #80]
    f788:	ldp	x22, x21, [sp, #64]
    f78c:	ldp	x24, x23, [sp, #48]
    f790:	ldp	x26, x25, [sp, #32]
    f794:	ldp	x28, x27, [sp, #16]
    f798:	ldp	x29, x30, [sp], #96
    f79c:	ret
    f7a0:	mov	w8, #0x2d                  	// #45
    f7a4:	add	x20, x20, #0x1
    f7a8:	strb	w8, [x27]
    f7ac:	ldur	x0, [x29, #-8]
    f7b0:	cbz	x0, f750 <__gmpf_get_str@@Base+0x428>
    f7b4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    f7b8:	cbnz	x24, f754 <__gmpf_get_str@@Base+0x42c>
    f7bc:	b	f77c <__gmpf_get_str@@Base+0x454>
    f7c0:	sub	x9, x12, #0x1
    f7c4:	mov	w10, #0x1                   	// #1
    f7c8:	mov	x0, x12
    f7cc:	b	f7dc <__gmpf_get_str@@Base+0x4b4>
    f7d0:	mov	w11, wzr
    f7d4:	sub	x9, x9, #0x1
    f7d8:	tbz	w11, #0, f6d8 <__gmpf_get_str@@Base+0x3b0>
    f7dc:	ldrb	w11, [x23, x9]
    f7e0:	add	w11, w11, #0x1
    f7e4:	cmp	w24, w11, uxtb
    f7e8:	strb	w11, [x23, x9]
    f7ec:	b.ne	f7d0 <__gmpf_get_str@@Base+0x4a8>  // b.any
    f7f0:	cbz	x9, f800 <__gmpf_get_str@@Base+0x4d8>
    f7f4:	sub	x0, x0, #0x1
    f7f8:	mov	w11, #0x1                   	// #1
    f7fc:	b	f7d4 <__gmpf_get_str@@Base+0x4ac>
    f800:	mov	w11, wzr
    f804:	strb	w10, [x23]
    f808:	add	x8, x8, #0x1
    f80c:	mov	w0, #0x1                   	// #1
    f810:	b	f7d4 <__gmpf_get_str@@Base+0x4ac>
    f814:	sub	x0, x29, #0x8
    f818:	mov	x20, x11
    f81c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    f820:	ldur	x10, [x29, #-24]
    f824:	mov	x11, x20
    f828:	mov	x23, x0
    f82c:	b	f434 <__gmpf_get_str@@Base+0x10c>
    f830:	sub	x0, x29, #0x8
    f834:	mov	x22, x11
    f838:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    f83c:	mov	x11, x22
    f840:	mov	x26, x0
    f844:	b	f48c <__gmpf_get_str@@Base+0x164>
    f848:	mov	x27, xzr
    f84c:	b	f77c <__gmpf_get_str@@Base+0x454>
    f850:	sub	x0, x29, #0x8
    f854:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    f858:	mov	x28, x0
    f85c:	b	f4f4 <__gmpf_get_str@@Base+0x1cc>
    f860:	sub	x0, x29, #0x8
    f864:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    f868:	mov	x1, x0
    f86c:	b	f550 <__gmpf_get_str@@Base+0x228>
    f870:	sub	sp, sp, #0x70
    f874:	stp	x20, x19, [sp, #96]
    f878:	mov	x20, x0
    f87c:	stp	x29, x30, [sp, #16]
    f880:	stp	x28, x27, [sp, #32]
    f884:	stp	x26, x25, [sp, #48]
    f888:	stp	x24, x23, [sp, #64]
    f88c:	stp	x22, x21, [sp, #80]
    f890:	add	x29, sp, #0x10
    f894:	cbz	x3, f900 <__gmpf_get_str@@Base+0x5d8>
    f898:	clz	x9, x3
    f89c:	mov	x21, x4
    f8a0:	mov	x23, x3
    f8a4:	mov	x24, x2
    f8a8:	cmp	x9, #0x3f
    f8ac:	str	x2, [x20]
    f8b0:	str	x1, [sp, #8]
    f8b4:	b.ne	f934 <__gmpf_get_str@@Base+0x60c>  // b.any
    f8b8:	mov	x8, xzr
    f8bc:	mov	x27, xzr
    f8c0:	mov	w25, #0x1                   	// #1
    f8c4:	mov	x26, x20
    f8c8:	subs	x9, x25, x21
    f8cc:	add	x10, x26, x9, lsl #3
    f8d0:	csel	x10, x10, x26, gt
    f8d4:	csel	x9, x9, xzr, gt
    f8d8:	add	x1, x10, x8, lsl #3
    f8dc:	csel	x21, x21, x25, gt
    f8e0:	cmp	x1, x20
    f8e4:	add	x19, x9, x27
    f8e8:	b.eq	f8f8 <__gmpf_get_str@@Base+0x5d0>  // b.none
    f8ec:	mov	x0, x20
    f8f0:	mov	x2, x21
    f8f4:	bl	cc10 <__gmpn_copyi@plt>
    f8f8:	ldr	x1, [sp, #8]
    f8fc:	b	f90c <__gmpf_get_str@@Base+0x5e4>
    f900:	mov	w21, #0x1                   	// #1
    f904:	mov	x19, xzr
    f908:	str	x21, [x20]
    f90c:	str	x19, [x1]
    f910:	mov	x0, x21
    f914:	ldp	x20, x19, [sp, #96]
    f918:	ldp	x22, x21, [sp, #80]
    f91c:	ldp	x24, x23, [sp, #64]
    f920:	ldp	x26, x25, [sp, #48]
    f924:	ldp	x28, x27, [sp, #32]
    f928:	ldp	x29, x30, [sp, #16]
    f92c:	add	sp, sp, #0x70
    f930:	ret
    f934:	mov	w10, #0x3e                  	// #62
    f938:	mov	w11, #0x3f                  	// #63
    f93c:	mov	x22, x5
    f940:	mov	x27, xzr
    f944:	mov	x8, xzr
    f948:	sub	w28, w10, w9
    f94c:	sub	w19, w11, w9
    f950:	mov	w25, #0x1                   	// #1
    f954:	mov	x9, x20
    f958:	b	f970 <__gmpf_get_str@@Base+0x648>
    f95c:	sub	w19, w19, #0x1
    f960:	cmp	w19, #0x0
    f964:	sub	x28, x28, #0x1
    f968:	mov	x9, x26
    f96c:	b.le	f8c8 <__gmpf_get_str@@Base+0x5a0>
    f970:	mov	x26, x22
    f974:	add	x1, x9, x8, lsl #3
    f978:	mov	x0, x26
    f97c:	mov	x2, x25
    f980:	mov	x22, x9
    f984:	bl	ca90 <__gmpn_sqr@plt>
    f988:	add	x8, x26, x25, lsl #4
    f98c:	ldur	x8, [x8, #-8]
    f990:	lsl	x9, x25, #1
    f994:	lsr	x10, x23, x28
    f998:	cmp	x8, #0x0
    f99c:	cset	w8, eq  // eq = none
    f9a0:	sub	x9, x9, x8
    f9a4:	subs	x8, x9, x21
    f9a8:	csel	x8, x8, xzr, gt
    f9ac:	add	x27, x8, x27, lsl #1
    f9b0:	csel	x25, x21, x9, gt
    f9b4:	tbz	w10, #0, f95c <__gmpf_get_str@@Base+0x634>
    f9b8:	add	x1, x26, x8, lsl #3
    f9bc:	mov	x0, x26
    f9c0:	mov	x2, x25
    f9c4:	mov	x3, x24
    f9c8:	bl	d670 <__gmpn_mul_1@plt>
    f9cc:	cmp	x0, #0x0
    f9d0:	mov	x8, xzr
    f9d4:	str	x0, [x26, x25, lsl #3]
    f9d8:	cinc	x25, x25, ne  // ne = any
    f9dc:	b	f95c <__gmpf_get_str@@Base+0x634>

000000000000f9e0 <__gmpf_dump@@Base>:
    f9e0:	sub	sp, sp, #0x30
    f9e4:	mov	x4, x0
    f9e8:	add	x1, sp, #0x8
    f9ec:	mov	w2, #0xa                   	// #10
    f9f0:	mov	x0, xzr
    f9f4:	mov	x3, xzr
    f9f8:	stp	x29, x30, [sp, #16]
    f9fc:	stp	x20, x19, [sp, #32]
    fa00:	add	x29, sp, #0x10
    fa04:	bl	cff0 <__gmpf_get_str@plt>
    fa08:	ldrb	w8, [x0]
    fa0c:	mov	x19, x0
    fa10:	cmp	w8, #0x2d
    fa14:	b.ne	fa2c <__gmpf_dump@@Base+0x4c>  // b.any
    fa18:	ldr	x2, [sp, #8]
    fa1c:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
    fa20:	add	x1, x19, #0x1
    fa24:	add	x0, x0, #0x7c2
    fa28:	b	fa3c <__gmpf_dump@@Base+0x5c>
    fa2c:	ldr	x2, [sp, #8]
    fa30:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
    fa34:	add	x0, x0, #0x7c3
    fa38:	mov	x1, x19
    fa3c:	bl	d4c0 <printf@plt>
    fa40:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fa44:	ldr	x8, [x8, #4016]
    fa48:	mov	x0, x19
    fa4c:	ldr	x20, [x8]
    fa50:	bl	c090 <strlen@plt>
    fa54:	add	x1, x0, #0x1
    fa58:	mov	x0, x19
    fa5c:	blr	x20
    fa60:	ldp	x20, x19, [sp, #32]
    fa64:	ldp	x29, x30, [sp, #16]
    fa68:	add	sp, sp, #0x30
    fa6c:	ret

000000000000fa70 <__gmpf_size@@Base>:
    fa70:	ldr	w8, [x0, #4]
    fa74:	cmp	w8, #0x0
    fa78:	cneg	w0, w8, mi  // mi = first
    fa7c:	ret

000000000000fa80 <__gmpf_eq@@Base>:
    fa80:	ldrsw	x10, [x0, #4]
    fa84:	ldrsw	x9, [x1, #4]
    fa88:	eor	w11, w9, w10
    fa8c:	tbnz	w11, #31, fba0 <__gmpf_eq@@Base+0x120>
    fa90:	orr	w11, w9, w10
    fa94:	cmp	w11, #0x0
    fa98:	mov	x8, x0
    fa9c:	cset	w0, eq  // eq = none
    faa0:	cbz	w10, fbd4 <__gmpf_eq@@Base+0x154>
    faa4:	cbz	w9, fbd4 <__gmpf_eq@@Base+0x154>
    faa8:	ldr	x11, [x8, #8]
    faac:	ldr	x12, [x1, #8]
    fab0:	cmp	x11, x12
    fab4:	b.ne	fba0 <__gmpf_eq@@Base+0x120>  // b.any
    fab8:	ldr	x8, [x8, #16]
    fabc:	cmp	x10, #0x0
    fac0:	ldr	x12, [x1, #16]
    fac4:	cneg	x11, x10, mi  // mi = first
    fac8:	add	x13, x8, x11, lsl #3
    facc:	cmp	x9, #0x0
    fad0:	mov	x10, x13
    fad4:	ldr	x8, [x10, #-8]!
    fad8:	cneg	x9, x9, mi  // mi = first
    fadc:	add	x16, x12, x9, lsl #3
    fae0:	ldur	x12, [x16, #-8]
    fae4:	clz	x8, x8
    fae8:	mov	w14, #0x3f                  	// #63
    faec:	sub	w14, w14, w8
    faf0:	lsr	x12, x12, x14
    faf4:	cmp	x12, #0x1
    faf8:	b.ne	fba0 <__gmpf_eq@@Base+0x120>  // b.any
    fafc:	add	x8, x8, x2
    fb00:	add	x12, x8, #0x3f
    fb04:	lsr	x12, x12, #6
    fb08:	cmp	x11, x12
    fb0c:	csel	x11, x11, x12, lt  // lt = tstop
    fb10:	cmp	x9, x12
    fb14:	csel	x12, x9, x12, lt  // lt = tstop
    fb18:	cmp	x11, x12
    fb1c:	csel	x15, x11, x12, lt  // lt = tstop
    fb20:	add	x9, x11, x12
    fb24:	lsl	x14, x15, #3
    fb28:	sub	x9, x9, x15
    fb2c:	sub	x13, x13, x14
    fb30:	sub	x14, x16, x14
    fb34:	sub	x16, x16, #0x8
    fb38:	mov	x17, x15
    fb3c:	cmp	x17, #0x2
    fb40:	b.lt	fb5c <__gmpf_eq@@Base+0xdc>  // b.tstop
    fb44:	ldr	x18, [x10], #-8
    fb48:	ldr	x0, [x16], #-8
    fb4c:	sub	x17, x17, #0x1
    fb50:	cmp	x18, x0
    fb54:	b.eq	fb3c <__gmpf_eq@@Base+0xbc>  // b.none
    fb58:	b	fba0 <__gmpf_eq@@Base+0x120>
    fb5c:	ldr	x16, [x13]
    fb60:	ldr	x17, [x14]
    fb64:	subs	x10, x9, x15
    fb68:	b.eq	fba8 <__gmpf_eq@@Base+0x128>  // b.none
    fb6c:	cmp	x16, x17
    fb70:	b.ne	fba0 <__gmpf_eq@@Base+0x120>  // b.any
    fb74:	cmp	x11, x12
    fb78:	csel	x11, x13, x14, gt
    fb7c:	neg	x15, x10
    fb80:	sub	x12, x11, #0x8
    fb84:	cmp	x10, #0x2
    fb88:	b.lt	fbb0 <__gmpf_eq@@Base+0x130>  // b.tstop
    fb8c:	ldr	x13, [x12], #-8
    fb90:	mov	w0, wzr
    fb94:	sub	x10, x10, #0x1
    fb98:	cbz	x13, fb84 <__gmpf_eq@@Base+0x104>
    fb9c:	b	fbd4 <__gmpf_eq@@Base+0x154>
    fba0:	mov	w0, wzr
    fba4:	ret
    fba8:	eor	x10, x17, x16
    fbac:	b	fbb4 <__gmpf_eq@@Base+0x134>
    fbb0:	ldr	x10, [x11, x15, lsl #3]
    fbb4:	sub	x8, x8, x9, lsl #6
    fbb8:	add	x8, x8, #0x40
    fbbc:	mov	w9, #0x40                  	// #64
    fbc0:	subs	x8, x9, x8
    fbc4:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
    fbc8:	lsr	x8, x10, x8
    fbcc:	cmp	x8, #0x0
    fbd0:	cset	w0, eq  // eq = none
    fbd4:	ret

000000000000fbd8 <__gmpf_reldiff@@Base>:
    fbd8:	stp	x29, x30, [sp, #-48]!
    fbdc:	str	x21, [sp, #16]
    fbe0:	stp	x20, x19, [sp, #32]
    fbe4:	mov	x29, sp
    fbe8:	sub	sp, sp, #0x20
    fbec:	ldr	w8, [x1, #4]
    fbf0:	mov	x21, x2
    fbf4:	mov	x19, x0
    fbf8:	cbz	w8, fc90 <__gmpf_reldiff@@Base+0xb8>
    fbfc:	str	xzr, [x29, #24]
    fc00:	ldr	w9, [x19]
    fc04:	cmp	w8, #0x0
    fc08:	cneg	w8, w8, mi  // mi = first
    fc0c:	mov	x20, x1
    fc10:	add	w8, w9, w8
    fc14:	sbfiz	x9, x8, #3, #32
    fc18:	add	x1, x9, #0x8
    fc1c:	mov	w9, #0x7f00                	// #32512
    fc20:	cmp	x1, x9
    fc24:	stur	w8, [x29, #-24]
    fc28:	b.hi	fca8 <__gmpf_reldiff@@Base+0xd0>  // b.pmore
    fc2c:	add	x9, x1, #0xf
    fc30:	mov	x8, sp
    fc34:	and	x9, x9, #0xfffffffffffffff0
    fc38:	sub	x0, x8, x9
    fc3c:	mov	sp, x0
    fc40:	stur	x0, [x29, #-8]
    fc44:	sub	x0, x29, #0x18
    fc48:	mov	x1, x20
    fc4c:	mov	x2, x21
    fc50:	bl	cfd0 <__gmpf_sub@plt>
    fc54:	ldur	w8, [x29, #-20]
    fc58:	sub	x1, x29, #0x18
    fc5c:	mov	x0, x19
    fc60:	mov	x2, x20
    fc64:	cmp	w8, #0x0
    fc68:	cneg	w8, w8, mi  // mi = first
    fc6c:	stur	w8, [x29, #-20]
    fc70:	bl	d0a0 <__gmpf_div@plt>
    fc74:	ldr	x0, [x29, #24]
    fc78:	cbnz	x0, fcb4 <__gmpf_reldiff@@Base+0xdc>
    fc7c:	mov	sp, x29
    fc80:	ldp	x20, x19, [sp, #32]
    fc84:	ldr	x21, [sp, #16]
    fc88:	ldp	x29, x30, [sp], #48
    fc8c:	ret
    fc90:	ldr	w8, [x21, #4]
    fc94:	mov	x0, x19
    fc98:	cmp	w8, #0x0
    fc9c:	cset	w1, ne  // ne = any
    fca0:	bl	c830 <__gmpf_set_ui@plt>
    fca4:	b	fc7c <__gmpf_reldiff@@Base+0xa4>
    fca8:	add	x0, x29, #0x18
    fcac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    fcb0:	b	fc40 <__gmpf_reldiff@@Base+0x68>
    fcb4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    fcb8:	b	fc7c <__gmpf_reldiff@@Base+0xa4>

000000000000fcbc <__gmpf_sqrt@@Base>:
    fcbc:	stp	x29, x30, [sp, #-80]!
    fcc0:	stp	x26, x25, [sp, #16]
    fcc4:	stp	x24, x23, [sp, #32]
    fcc8:	stp	x22, x21, [sp, #48]
    fccc:	stp	x20, x19, [sp, #64]
    fcd0:	mov	x29, sp
    fcd4:	sub	sp, sp, #0x10
    fcd8:	ldrsw	x20, [x1, #4]
    fcdc:	mov	x19, x0
    fce0:	cmp	w20, #0x0
    fce4:	b.le	fd58 <__gmpf_sqrt@@Base+0x9c>
    fce8:	stur	xzr, [x29, #-8]
    fcec:	ldp	x8, x22, [x1, #8]
    fcf0:	ldrsw	x9, [x19]
    fcf4:	mov	w10, #0x7f00                	// #32512
    fcf8:	and	x24, x8, #0x1
    fcfc:	lsl	x25, x9, #1
    fd00:	add	x8, x24, x8
    fd04:	sub	x21, x25, x24
    fd08:	cmp	x8, #0x0
    fd0c:	lsl	x1, x21, #3
    fd10:	cinc	x8, x8, lt  // lt = tstop
    fd14:	cmp	x1, x10
    fd18:	asr	x8, x8, #1
    fd1c:	str	w9, [x19, #4]
    fd20:	str	x8, [x19, #8]
    fd24:	b.hi	fd68 <__gmpf_sqrt@@Base+0xac>  // b.pmore
    fd28:	add	x9, x1, #0xf
    fd2c:	mov	x8, sp
    fd30:	and	x9, x9, #0xfffffffffffffff0
    fd34:	sub	x23, x8, x9
    fd38:	mov	sp, x23
    fd3c:	subs	x26, x21, x20
    fd40:	b.ge	fd7c <__gmpf_sqrt@@Base+0xc0>  // b.tcont
    fd44:	sub	x8, x20, x21
    fd48:	add	x1, x22, x8, lsl #3
    fd4c:	mov	x0, x23
    fd50:	mov	x2, x21
    fd54:	b	fda4 <__gmpf_sqrt@@Base+0xe8>
    fd58:	tbnz	w20, #31, fde8 <__gmpf_sqrt@@Base+0x12c>
    fd5c:	str	wzr, [x19, #4]
    fd60:	str	xzr, [x19, #8]
    fd64:	b	fdc4 <__gmpf_sqrt@@Base+0x108>
    fd68:	sub	x0, x29, #0x8
    fd6c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
    fd70:	mov	x23, x0
    fd74:	subs	x26, x21, x20
    fd78:	b.lt	fd44 <__gmpf_sqrt@@Base+0x88>  // b.tstop
    fd7c:	b.eq	fd98 <__gmpf_sqrt@@Base+0xdc>  // b.none
    fd80:	sub	x8, x25, x20
    fd84:	sub	x8, x8, x24
    fd88:	lsl	x2, x8, #3
    fd8c:	mov	x0, x23
    fd90:	mov	w1, wzr
    fd94:	bl	c780 <memset@plt>
    fd98:	add	x0, x23, x26, lsl #3
    fd9c:	mov	x1, x22
    fda0:	mov	x2, x20
    fda4:	bl	cc10 <__gmpn_copyi@plt>
    fda8:	ldr	x0, [x19, #16]
    fdac:	mov	x1, xzr
    fdb0:	mov	x2, x23
    fdb4:	mov	x3, x21
    fdb8:	bl	d590 <__gmpn_sqrtrem@plt>
    fdbc:	ldur	x0, [x29, #-8]
    fdc0:	cbnz	x0, fde0 <__gmpf_sqrt@@Base+0x124>
    fdc4:	mov	sp, x29
    fdc8:	ldp	x20, x19, [sp, #64]
    fdcc:	ldp	x22, x21, [sp, #48]
    fdd0:	ldp	x24, x23, [sp, #32]
    fdd4:	ldp	x26, x25, [sp, #16]
    fdd8:	ldp	x29, x30, [sp], #80
    fddc:	ret
    fde0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
    fde4:	b	fdc4 <__gmpf_sqrt@@Base+0x108>
    fde8:	bl	d1c0 <__gmp_sqrt_of_negative@plt>

000000000000fdec <__gmpf_random2@@Base>:
    fdec:	sub	sp, sp, #0x40
    fdf0:	cmp	x1, #0x0
    fdf4:	stp	x20, x19, [sp, #48]
    fdf8:	cneg	x8, x1, mi  // mi = first
    fdfc:	mov	x19, x0
    fe00:	stp	x29, x30, [sp, #16]
    fe04:	stp	x22, x21, [sp, #32]
    fe08:	add	x29, sp, #0x10
    fe0c:	cbz	x8, fea4 <__gmpf_random2@@Base+0xb8>
    fe10:	ldrsw	x9, [x19]
    fe14:	ldr	x0, [x19, #16]
    fe18:	mov	x20, x1
    fe1c:	mov	x21, x2
    fe20:	add	x10, x9, #0x1
    fe24:	cmp	x8, x10
    fe28:	csinc	x22, x8, x9, le
    fe2c:	mov	x1, x22
    fe30:	bl	d3a0 <__gmpn_random2@plt>
    fe34:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fe38:	ldr	x8, [x8, #4040]
    fe3c:	ldrb	w9, [x8]
    fe40:	cbnz	w9, fe58 <__gmpf_random2@@Base+0x6c>
    fe44:	mov	w9, #0x1                   	// #1
    fe48:	strb	w9, [x8]
    fe4c:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fe50:	ldr	x0, [x0, #3976]
    fe54:	bl	c060 <__gmp_randinit_mt_noseed@plt>
    fe58:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fe5c:	ldr	x0, [x0, #3976]
    fe60:	add	x1, sp, #0x8
    fe64:	mov	w2, #0x40                  	// #64
    fe68:	ldr	x8, [x0, #24]
    fe6c:	ldr	x8, [x8, #8]
    fe70:	blr	x8
    fe74:	ldr	x8, [sp, #8]
    fe78:	cmp	x21, #0x0
    fe7c:	mov	w9, #0x1                   	// #1
    fe80:	cneg	x10, x21, mi  // mi = first
    fe84:	bfi	x9, x10, #1, #63
    fe88:	udiv	x11, x8, x9
    fe8c:	msub	x8, x11, x9, x8
    fe90:	cmp	x20, #0x0
    fe94:	sub	x8, x8, x10
    fe98:	str	x8, [x19, #8]
    fe9c:	cneg	w8, w22, lt  // lt = tstop
    fea0:	b	fea8 <__gmpf_random2@@Base+0xbc>
    fea4:	str	xzr, [x19, #8]
    fea8:	str	w8, [x19, #4]
    feac:	ldp	x20, x19, [sp, #48]
    feb0:	ldp	x22, x21, [sp, #32]
    feb4:	ldp	x29, x30, [sp, #16]
    feb8:	add	sp, sp, #0x40
    febc:	ret

000000000000fec0 <__gmpf_inp_str@@Base>:
    fec0:	sub	sp, sp, #0x70
    fec4:	stp	x29, x30, [sp, #16]
    fec8:	add	x29, sp, #0x10
    fecc:	stp	x28, x27, [sp, #32]
    fed0:	stp	x26, x25, [sp, #48]
    fed4:	stp	x24, x23, [sp, #64]
    fed8:	stp	x22, x21, [sp, #80]
    fedc:	stp	x20, x19, [sp, #96]
    fee0:	stur	w2, [x29, #-4]
    fee4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fee8:	ldr	x8, [x8, #3888]
    feec:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
    fef0:	mov	x20, x0
    fef4:	cmp	x1, #0x0
    fef8:	ldr	x8, [x8]
    fefc:	ldr	x9, [x9, #3840]
    ff00:	mov	w0, #0x64                  	// #100
    ff04:	csel	x22, x8, x1, eq  // eq = none
    ff08:	ldr	x9, [x9]
    ff0c:	blr	x9
    ff10:	mov	x21, x0
    ff14:	mov	x27, #0xffffffffffffffff    	// #-1
    ff18:	mov	x0, x22
    ff1c:	bl	c9a0 <getc@plt>
    ff20:	mov	w24, w0
    ff24:	bl	cca0 <__ctype_b_loc@plt>
    ff28:	ldr	x8, [x0]
    ff2c:	add	x27, x27, #0x1
    ff30:	ldrh	w8, [x8, w24, sxtw #1]
    ff34:	tbnz	w8, #13, ff18 <__gmpf_inp_str@@Base+0x58>
    ff38:	adrp	x19, 69000 <__gmp_limbroots_table@@Base+0x11338>
    ff3c:	ldr	x19, [x19, #3792]
    ff40:	mov	x25, x0
    ff44:	mov	x28, xzr
    ff48:	mov	w23, #0x64                  	// #100
    ff4c:	cmp	x28, x23
    ff50:	b.cc	ff78 <__gmpf_inp_str@@Base+0xb8>  // b.lo, b.ul, b.last
    ff54:	ldr	x8, [x19]
    ff58:	add	x9, x23, x23, lsl #1
    ff5c:	lsr	x26, x9, #1
    ff60:	mov	x0, x21
    ff64:	mov	x1, x23
    ff68:	mov	x2, x26
    ff6c:	blr	x8
    ff70:	mov	x21, x0
    ff74:	mov	x23, x26
    ff78:	cmn	w24, #0x1
    ff7c:	b.eq	ffb0 <__gmpf_inp_str@@Base+0xf0>  // b.none
    ff80:	ldr	x8, [x25]
    ff84:	ldrh	w8, [x8, w24, sxtw #1]
    ff88:	tbnz	w8, #13, ffb0 <__gmpf_inp_str@@Base+0xf0>
    ff8c:	mov	x0, x22
    ff90:	add	x26, x28, #0x1
    ff94:	strb	w24, [x21, x28]
    ff98:	bl	c9a0 <getc@plt>
    ff9c:	mov	w24, w0
    ffa0:	mov	x28, x26
    ffa4:	cmp	x28, x23
    ffa8:	b.cs	ff54 <__gmpf_inp_str@@Base+0x94>  // b.hs, b.nlast
    ffac:	b	ff78 <__gmpf_inp_str@@Base+0xb8>
    ffb0:	mov	w0, w24
    ffb4:	mov	x1, x22
    ffb8:	bl	ce10 <ungetc@plt>
    ffbc:	cmp	x28, x23
    ffc0:	b.cc	ffe8 <__gmpf_inp_str@@Base+0x128>  // b.lo, b.ul, b.last
    ffc4:	ldr	x8, [x19]
    ffc8:	add	x9, x23, x23, lsl #1
    ffcc:	lsr	x22, x9, #1
    ffd0:	mov	x0, x21
    ffd4:	mov	x1, x23
    ffd8:	mov	x2, x22
    ffdc:	blr	x8
    ffe0:	mov	x21, x0
    ffe4:	mov	x23, x22
    ffe8:	ldur	w2, [x29, #-4]
    ffec:	mov	x0, x20
    fff0:	mov	x1, x21
    fff4:	strb	wzr, [x21, x28]
    fff8:	bl	c310 <__gmpf_set_str@plt>
    fffc:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   10000:	ldr	x8, [x8, #4016]
   10004:	mov	w19, w0
   10008:	mov	x0, x21
   1000c:	mov	x1, x23
   10010:	ldr	x8, [x8]
   10014:	blr	x8
   10018:	add	x8, x27, x28
   1001c:	cmn	w19, #0x1
   10020:	ldp	x20, x19, [sp, #96]
   10024:	ldp	x22, x21, [sp, #80]
   10028:	ldp	x24, x23, [sp, #64]
   1002c:	ldp	x26, x25, [sp, #48]
   10030:	ldp	x28, x27, [sp, #32]
   10034:	ldp	x29, x30, [sp, #16]
   10038:	csel	x0, xzr, x8, eq  // eq = none
   1003c:	add	sp, sp, #0x70
   10040:	ret

0000000000010044 <__gmpf_out_str@@Base>:
   10044:	stp	x29, x30, [sp, #-80]!
   10048:	str	x25, [sp, #16]
   1004c:	stp	x24, x23, [sp, #32]
   10050:	stp	x22, x21, [sp, #48]
   10054:	stp	x20, x19, [sp, #64]
   10058:	mov	x29, sp
   1005c:	sub	sp, sp, #0x10
   10060:	cmp	w1, #0x0
   10064:	mov	w8, #0xa                   	// #10
   10068:	mov	x22, x3
   1006c:	mov	x23, x2
   10070:	csel	w19, w8, w1, eq  // eq = none
   10074:	stur	xzr, [x29, #-8]
   10078:	cbnz	x2, 100a4 <__gmpf_out_str@@Base+0x60>
   1007c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   10080:	ldr	x8, [x8, #3936]
   10084:	ldrsw	x10, [x22]
   10088:	mov	w9, #0x28                  	// #40
   1008c:	smaddl	x8, w19, w9, x8
   10090:	ldr	x8, [x8, #8]
   10094:	lsl	x9, x10, #6
   10098:	sub	x9, x9, #0x40
   1009c:	umulh	x8, x8, x9
   100a0:	add	x23, x8, #0x2
   100a4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   100a8:	ldr	x8, [x8, #3856]
   100ac:	cmp	x0, #0x0
   100b0:	add	x1, x23, #0x2
   100b4:	mov	w9, #0x7f00                	// #32512
   100b8:	ldr	x8, [x8]
   100bc:	csel	x20, x8, x0, eq  // eq = none
   100c0:	cmp	x1, x9
   100c4:	b.hi	101e0 <__gmpf_out_str@@Base+0x19c>  // b.pmore
   100c8:	add	x9, x1, #0xf
   100cc:	mov	x8, sp
   100d0:	and	x9, x9, #0xfffffffffffffff0
   100d4:	sub	x21, x8, x9
   100d8:	mov	sp, x21
   100dc:	add	x1, x29, #0x18
   100e0:	mov	x0, x21
   100e4:	mov	w2, w19
   100e8:	mov	x3, x23
   100ec:	mov	x4, x22
   100f0:	bl	cff0 <__gmpf_get_str@plt>
   100f4:	mov	x0, x21
   100f8:	bl	c090 <strlen@plt>
   100fc:	ldrb	w8, [x21]
   10100:	mov	x22, x0
   10104:	cmp	w8, #0x2d
   10108:	b.ne	10128 <__gmpf_out_str@@Base+0xe4>  // b.any
   1010c:	mov	w0, #0x2d                  	// #45
   10110:	mov	x1, x20
   10114:	add	x21, x21, #0x1
   10118:	bl	c360 <fputc@plt>
   1011c:	sub	x22, x22, #0x1
   10120:	mov	w25, #0x2                   	// #2
   10124:	b	1012c <__gmpf_out_str@@Base+0xe8>
   10128:	mov	w25, #0x1                   	// #1
   1012c:	mov	w0, #0x10000               	// #65536
   10130:	bl	c560 <nl_langinfo@plt>
   10134:	mov	x23, x0
   10138:	bl	c090 <strlen@plt>
   1013c:	mov	x24, x0
   10140:	mov	w0, #0x30                  	// #48
   10144:	mov	x1, x20
   10148:	bl	c330 <putc@plt>
   1014c:	mov	w1, #0x1                   	// #1
   10150:	mov	x0, x23
   10154:	mov	x2, x24
   10158:	mov	x3, x20
   1015c:	bl	d000 <fwrite@plt>
   10160:	mov	w1, #0x1                   	// #1
   10164:	mov	x0, x21
   10168:	mov	x2, x22
   1016c:	mov	x3, x20
   10170:	bl	d000 <fwrite@plt>
   10174:	ldr	x2, [x29, #24]
   10178:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1017c:	adrp	x9, 4c000 <__gmp_randclear_mt@@Base+0x18>
   10180:	add	x8, x8, #0x7d2
   10184:	add	x9, x9, #0x7cd
   10188:	cmp	w19, #0xb
   1018c:	mov	x21, x0
   10190:	csel	x1, x9, x8, lt  // lt = tstop
   10194:	mov	x0, x20
   10198:	bl	d600 <fprintf@plt>
   1019c:	mov	w8, w0
   101a0:	ldur	x0, [x29, #-8]
   101a4:	add	x9, x25, x24
   101a8:	add	x9, x9, x21
   101ac:	add	x19, x9, w8, sxtw
   101b0:	cbnz	x0, 101f0 <__gmpf_out_str@@Base+0x1ac>
   101b4:	mov	x0, x20
   101b8:	bl	d680 <ferror@plt>
   101bc:	cmp	w0, #0x0
   101c0:	csel	x0, x19, xzr, eq  // eq = none
   101c4:	mov	sp, x29
   101c8:	ldp	x20, x19, [sp, #64]
   101cc:	ldp	x22, x21, [sp, #48]
   101d0:	ldp	x24, x23, [sp, #32]
   101d4:	ldr	x25, [sp, #16]
   101d8:	ldp	x29, x30, [sp], #80
   101dc:	ret
   101e0:	sub	x0, x29, #0x8
   101e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   101e8:	mov	x21, x0
   101ec:	b	100dc <__gmpf_out_str@@Base+0x98>
   101f0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   101f4:	b	101b4 <__gmpf_out_str@@Base+0x170>

00000000000101f8 <__gmpf_add@@Base>:
   101f8:	stp	x29, x30, [sp, #-96]!
   101fc:	stp	x28, x27, [sp, #16]
   10200:	stp	x26, x25, [sp, #32]
   10204:	stp	x24, x23, [sp, #48]
   10208:	stp	x22, x21, [sp, #64]
   1020c:	stp	x20, x19, [sp, #80]
   10210:	mov	x29, sp
   10214:	sub	sp, sp, #0x30
   10218:	ldr	w28, [x1, #4]
   1021c:	mov	x19, x0
   10220:	cbz	w28, 1032c <__gmpf_add@@Base+0x134>
   10224:	ldr	w8, [x2, #4]
   10228:	cbz	w8, 10328 <__gmpf_add@@Base+0x130>
   1022c:	eor	w9, w8, w28
   10230:	tbnz	w9, #31, 10344 <__gmpf_add@@Base+0x14c>
   10234:	stur	xzr, [x29, #-24]
   10238:	ldr	x9, [x1, #8]
   1023c:	ldr	x10, [x2, #8]
   10240:	ldrsw	x24, [x19]
   10244:	ldr	x0, [x19, #16]
   10248:	mov	w11, #0x7f00                	// #32512
   1024c:	cmp	x9, x10
   10250:	csel	x10, x1, x2, lt  // lt = tstop
   10254:	csel	x12, x2, x1, lt  // lt = tstop
   10258:	csel	w9, w28, w8, lt  // lt = tstop
   1025c:	csel	w8, w8, w28, lt  // lt = tstop
   10260:	ldp	x27, x13, [x12, #8]
   10264:	ldp	x10, x12, [x10, #8]
   10268:	sxtw	x8, w8
   1026c:	sxtw	x9, w9
   10270:	cmp	x8, #0x0
   10274:	cneg	x8, x8, mi  // mi = first
   10278:	cmp	x9, #0x0
   1027c:	cneg	x9, x9, mi  // mi = first
   10280:	subs	x14, x8, x24
   10284:	sub	x21, x27, x10
   10288:	add	x10, x13, x14, lsl #3
   1028c:	csel	x20, x24, x8, gt
   10290:	add	x8, x21, x9
   10294:	csel	x23, x10, x13, gt
   10298:	subs	x8, x8, x24
   1029c:	sub	x10, x24, x21
   102a0:	add	x8, x12, x8, lsl #3
   102a4:	lsl	x1, x24, #3
   102a8:	csel	x25, x10, x9, gt
   102ac:	csel	x26, x8, x12, gt
   102b0:	cmp	x1, x11
   102b4:	b.hi	10418 <__gmpf_add@@Base+0x220>  // b.pmore
   102b8:	add	x9, x1, #0xf
   102bc:	mov	x8, sp
   102c0:	and	x9, x9, #0xfffffffffffffff0
   102c4:	sub	x22, x8, x9
   102c8:	mov	sp, x22
   102cc:	cmp	x21, x24
   102d0:	b.ge	10434 <__gmpf_add@@Base+0x23c>  // b.tcont
   102d4:	subs	x8, x20, x21
   102d8:	add	x24, x25, x21
   102dc:	stur	x0, [x29, #-32]
   102e0:	b.le	1036c <__gmpf_add@@Base+0x174>
   102e4:	subs	x9, x24, x20
   102e8:	b.le	103bc <__gmpf_add@@Base+0x1c4>
   102ec:	mov	x0, x22
   102f0:	mov	x1, x26
   102f4:	mov	x2, x9
   102f8:	mov	x25, x9
   102fc:	mov	x21, x8
   10300:	bl	cc10 <__gmpn_copyi@plt>
   10304:	lsl	x8, x25, #3
   10308:	add	x0, x22, x8
   1030c:	add	x3, x26, x8
   10310:	mov	x1, x23
   10314:	mov	x2, x20
   10318:	mov	x4, x21
   1031c:	bl	c970 <__gmpn_add@plt>
   10320:	mov	x23, x0
   10324:	b	103f4 <__gmpf_add@@Base+0x1fc>
   10328:	mov	x2, x1
   1032c:	cmp	x2, x19
   10330:	b.eq	10464 <__gmpf_add@@Base+0x26c>  // b.none
   10334:	mov	x0, x19
   10338:	mov	x1, x2
   1033c:	bl	c2a0 <__gmpf_set@plt>
   10340:	b	10464 <__gmpf_add@@Base+0x26c>
   10344:	neg	w8, w8
   10348:	stur	w8, [x29, #-20]
   1034c:	ldr	x8, [x2, #8]
   10350:	mov	x0, x19
   10354:	stur	x8, [x29, #-16]
   10358:	ldr	x8, [x2, #16]
   1035c:	sub	x2, x29, #0x18
   10360:	stur	x8, [x29, #-8]
   10364:	bl	cfd0 <__gmpf_sub@plt>
   10368:	b	10464 <__gmpf_add@@Base+0x26c>
   1036c:	sub	x8, x24, x20
   10370:	mov	x0, x22
   10374:	mov	x1, x26
   10378:	mov	x2, x25
   1037c:	stur	x8, [x29, #-40]
   10380:	bl	cc10 <__gmpn_copyi@plt>
   10384:	cmp	x20, x21
   10388:	b.eq	103a0 <__gmpf_add@@Base+0x1a8>  // b.none
   1038c:	sub	x8, x21, x20
   10390:	add	x0, x22, x25, lsl #3
   10394:	lsl	x2, x8, #3
   10398:	mov	w1, wzr
   1039c:	bl	c780 <memset@plt>
   103a0:	ldur	x8, [x29, #-40]
   103a4:	mov	x1, x23
   103a8:	mov	x2, x20
   103ac:	add	x0, x22, x8, lsl #3
   103b0:	bl	cc10 <__gmpn_copyi@plt>
   103b4:	mov	x23, xzr
   103b8:	b	103f4 <__gmpf_add@@Base+0x1fc>
   103bc:	sub	x24, x8, x25
   103c0:	mov	x0, x22
   103c4:	mov	x1, x23
   103c8:	mov	x2, x24
   103cc:	bl	cc10 <__gmpn_copyi@plt>
   103d0:	lsl	x8, x24, #3
   103d4:	add	x0, x22, x8
   103d8:	add	x1, x23, x8
   103dc:	sub	x2, x20, x24
   103e0:	mov	x3, x26
   103e4:	mov	x4, x25
   103e8:	bl	c970 <__gmpn_add@plt>
   103ec:	mov	x23, x0
   103f0:	mov	x24, x20
   103f4:	ldur	x20, [x29, #-32]
   103f8:	mov	x1, x22
   103fc:	mov	x2, x24
   10400:	mov	x0, x20
   10404:	bl	cc10 <__gmpn_copyi@plt>
   10408:	str	x23, [x20, x24, lsl #3]
   1040c:	add	x20, x23, x24
   10410:	add	x27, x23, x27
   10414:	b	10448 <__gmpf_add@@Base+0x250>
   10418:	stur	x0, [x29, #-32]
   1041c:	sub	x0, x29, #0x18
   10420:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   10424:	mov	x22, x0
   10428:	ldur	x0, [x29, #-32]
   1042c:	cmp	x21, x24
   10430:	b.lt	102d4 <__gmpf_add@@Base+0xdc>  // b.tstop
   10434:	cmp	x0, x23
   10438:	b.eq	10448 <__gmpf_add@@Base+0x250>  // b.none
   1043c:	mov	x1, x23
   10440:	mov	x2, x20
   10444:	bl	cc10 <__gmpn_copyi@plt>
   10448:	neg	w8, w20
   1044c:	cmp	w28, #0x0
   10450:	csel	x8, x8, x20, lt  // lt = tstop
   10454:	str	w8, [x19, #4]
   10458:	str	x27, [x19, #8]
   1045c:	ldur	x0, [x29, #-24]
   10460:	cbnz	x0, 10484 <__gmpf_add@@Base+0x28c>
   10464:	mov	sp, x29
   10468:	ldp	x20, x19, [sp, #80]
   1046c:	ldp	x22, x21, [sp, #64]
   10470:	ldp	x24, x23, [sp, #48]
   10474:	ldp	x26, x25, [sp, #32]
   10478:	ldp	x28, x27, [sp, #16]
   1047c:	ldp	x29, x30, [sp], #96
   10480:	ret
   10484:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   10488:	b	10464 <__gmpf_add@@Base+0x26c>

000000000001048c <__gmpf_add_ui@@Base>:
   1048c:	sub	sp, sp, #0x60
   10490:	stp	x29, x30, [sp, #32]
   10494:	stp	x24, x23, [sp, #48]
   10498:	stp	x22, x21, [sp, #64]
   1049c:	stp	x20, x19, [sp, #80]
   104a0:	ldrsw	x23, [x1, #4]
   104a4:	ldr	x20, [x1, #8]
   104a8:	mov	x21, x2
   104ac:	mov	x24, x1
   104b0:	cmp	w23, #0x0
   104b4:	mov	x19, x0
   104b8:	add	x29, sp, #0x20
   104bc:	b.le	10514 <__gmpf_add_ui@@Base+0x88>
   104c0:	ldr	x1, [x24, #16]
   104c4:	ldr	x22, [x19, #16]
   104c8:	ldrsw	x8, [x19]
   104cc:	cbz	x21, 104e0 <__gmpf_add_ui@@Base+0x54>
   104d0:	cmp	x20, #0x1
   104d4:	b.lt	10548 <__gmpf_add_ui@@Base+0xbc>  // b.tstop
   104d8:	cmp	x20, x8
   104dc:	b.le	105c4 <__gmpf_add_ui@@Base+0x138>
   104e0:	cmp	x24, x19
   104e4:	b.eq	10684 <__gmpf_add_ui@@Base+0x1f8>  // b.none
   104e8:	cmp	w23, w8
   104ec:	csinc	x20, x23, x8, le
   104f0:	add	x8, x1, x23, lsl #3
   104f4:	sub	x1, x8, x20, lsl #3
   104f8:	mov	x0, x22
   104fc:	mov	x2, x20
   10500:	bl	cc10 <__gmpn_copyi@plt>
   10504:	str	w20, [x19, #4]
   10508:	ldr	x8, [x24, #8]
   1050c:	str	x8, [x19, #8]
   10510:	b	10684 <__gmpf_add_ui@@Base+0x1f8>
   10514:	cbz	w23, 105b4 <__gmpf_add_ui@@Base+0x128>
   10518:	neg	w8, w23
   1051c:	str	w8, [sp, #12]
   10520:	ldr	x8, [x24, #16]
   10524:	add	x1, sp, #0x8
   10528:	mov	x0, x19
   1052c:	mov	x2, x21
   10530:	stp	x20, x8, [sp, #16]
   10534:	bl	c340 <__gmpf_sub_ui@plt>
   10538:	ldr	w8, [x19, #4]
   1053c:	neg	w8, w8
   10540:	str	w8, [x19, #4]
   10544:	b	10684 <__gmpf_add_ui@@Base+0x1f8>
   10548:	neg	x9, x20
   1054c:	cmp	x9, x8
   10550:	b.ge	10608 <__gmpf_add_ui@@Base+0x17c>  // b.tcont
   10554:	add	x10, x8, x20
   10558:	sub	x9, x23, x20
   1055c:	sub	x10, x23, x10
   10560:	cmp	x9, x8
   10564:	add	x8, x10, #0x1
   10568:	add	x8, x1, x8, lsl #3
   1056c:	csinc	x9, xzr, x10, lt  // lt = tstop
   10570:	csel	x1, x1, x8, lt  // lt = tstop
   10574:	cmp	x22, x1
   10578:	sub	x23, x23, x9
   1057c:	b.eq	1058c <__gmpf_add_ui@@Base+0x100>  // b.none
   10580:	mov	x0, x22
   10584:	mov	x2, x23
   10588:	bl	cc10 <__gmpn_copyi@plt>
   1058c:	cbz	x20, 105a0 <__gmpf_add_ui@@Base+0x114>
   10590:	add	x0, x22, x23, lsl #3
   10594:	neg	x2, x20, lsl #3
   10598:	mov	w1, wzr
   1059c:	bl	c780 <memset@plt>
   105a0:	sub	x8, x23, x20
   105a4:	mov	w9, #0x1                   	// #1
   105a8:	str	x21, [x22, x8, lsl #3]
   105ac:	add	w8, w8, #0x1
   105b0:	b	1067c <__gmpf_add_ui@@Base+0x1f0>
   105b4:	mov	x0, x19
   105b8:	mov	x1, x21
   105bc:	bl	c830 <__gmpf_set_ui@plt>
   105c0:	b	10684 <__gmpf_add_ui@@Base+0x1f8>
   105c4:	cmp	x20, x23
   105c8:	b.le	1061c <__gmpf_add_ui@@Base+0x190>
   105cc:	add	x8, x22, x20, lsl #3
   105d0:	sub	x0, x8, x23, lsl #3
   105d4:	mov	x2, x23
   105d8:	bl	c130 <__gmpn_copyd@plt>
   105dc:	mvn	x8, x23
   105e0:	adds	x8, x20, x8
   105e4:	str	x21, [x22]
   105e8:	b.eq	105fc <__gmpf_add_ui@@Base+0x170>  // b.none
   105ec:	add	x0, x22, #0x8
   105f0:	lsl	x2, x8, #3
   105f4:	mov	w1, wzr
   105f8:	bl	c780 <memset@plt>
   105fc:	str	w20, [x19, #4]
   10600:	str	x20, [x19, #8]
   10604:	b	10684 <__gmpf_add_ui@@Base+0x1f8>
   10608:	mov	w8, #0x1                   	// #1
   1060c:	str	x21, [x22]
   10610:	str	w8, [x19, #4]
   10614:	str	x8, [x19, #8]
   10618:	b	10684 <__gmpf_add_ui@@Base+0x1f8>
   1061c:	sub	x9, x23, x8
   10620:	cmp	w23, w8
   10624:	add	x9, x1, x9, lsl #3
   10628:	csel	w8, w23, w8, lt  // lt = tstop
   1062c:	csel	x23, x9, x1, gt
   10630:	cmp	x22, x23
   10634:	sxtw	x24, w8
   10638:	b.eq	1064c <__gmpf_add_ui@@Base+0x1c0>  // b.none
   1063c:	sub	x2, x24, x20
   10640:	mov	x0, x22
   10644:	mov	x1, x23
   10648:	bl	cc10 <__gmpn_copyi@plt>
   1064c:	lsl	x8, x24, #3
   10650:	lsl	x9, x20, #3
   10654:	add	x22, x22, x8
   10658:	add	x8, x23, x8
   1065c:	sub	x0, x22, x9
   10660:	sub	x1, x8, x9
   10664:	mov	x2, x20
   10668:	mov	x3, x21
   1066c:	bl	c150 <__gmpn_add_1@plt>
   10670:	str	x0, [x22]
   10674:	add	w8, w24, w0
   10678:	add	x9, x0, x20
   1067c:	str	w8, [x19, #4]
   10680:	str	x9, [x19, #8]
   10684:	ldp	x20, x19, [sp, #80]
   10688:	ldp	x22, x21, [sp, #64]
   1068c:	ldp	x24, x23, [sp, #48]
   10690:	ldp	x29, x30, [sp, #32]
   10694:	add	sp, sp, #0x60
   10698:	ret

000000000001069c <__gmpf_sub@@Base>:
   1069c:	stp	x29, x30, [sp, #-96]!
   106a0:	stp	x28, x27, [sp, #16]
   106a4:	stp	x26, x25, [sp, #32]
   106a8:	stp	x24, x23, [sp, #48]
   106ac:	stp	x22, x21, [sp, #64]
   106b0:	stp	x20, x19, [sp, #80]
   106b4:	mov	x29, sp
   106b8:	sub	sp, sp, #0x30
   106bc:	ldr	w8, [x1, #4]
   106c0:	mov	x19, x0
   106c4:	cbz	w8, 107dc <__gmpf_sub@@Base+0x140>
   106c8:	ldr	w9, [x2, #4]
   106cc:	cbz	w9, 107ec <__gmpf_sub@@Base+0x150>
   106d0:	eor	w10, w9, w8
   106d4:	tbnz	w10, #31, 10800 <__gmpf_sub@@Base+0x164>
   106d8:	stur	xzr, [x29, #-24]
   106dc:	ldr	x10, [x1, #8]
   106e0:	ldr	x11, [x2, #8]
   106e4:	ldrsw	x12, [x19]
   106e8:	ldr	x0, [x19, #16]
   106ec:	cmp	x10, x11
   106f0:	cset	w10, lt  // lt = tstop
   106f4:	csel	w11, w8, w9, lt  // lt = tstop
   106f8:	csel	w9, w9, w8, lt  // lt = tstop
   106fc:	csel	x13, x1, x2, lt  // lt = tstop
   10700:	csel	x14, x2, x1, lt  // lt = tstop
   10704:	eor	w22, w10, w8, lsr #31
   10708:	sxtw	x10, w9
   1070c:	ldp	x27, x23, [x14, #8]
   10710:	ldp	x28, x9, [x13, #8]
   10714:	sxtw	x8, w11
   10718:	cmp	x10, #0x0
   1071c:	cneg	x10, x10, mi  // mi = first
   10720:	cmp	x8, #0x0
   10724:	sub	x20, x27, x28
   10728:	cneg	x11, x8, mi  // mi = first
   1072c:	cmp	x20, #0x1
   10730:	add	x8, x12, #0x1
   10734:	b.gt	1075c <__gmpf_sub@@Base+0xc0>
   10738:	cbz	x20, 10838 <__gmpf_sub@@Base+0x19c>
   1073c:	add	x12, x23, x10, lsl #3
   10740:	ldur	x12, [x12, #-8]
   10744:	cmp	x12, #0x1
   10748:	b.ne	1075c <__gmpf_sub@@Base+0xc0>  // b.any
   1074c:	add	x12, x9, x11, lsl #3
   10750:	ldur	x12, [x12, #-8]
   10754:	cmn	x12, #0x1
   10758:	b.eq	10958 <__gmpf_sub@@Base+0x2bc>  // b.none
   1075c:	mov	x26, x27
   10760:	mov	x13, x11
   10764:	subs	x11, x10, x8
   10768:	add	x11, x23, x11, lsl #3
   1076c:	add	x12, x13, x20
   10770:	csel	x21, x8, x10, gt
   10774:	csel	x23, x11, x23, gt
   10778:	subs	x10, x8, x20
   1077c:	subs	x11, x12, x8
   10780:	add	x11, x9, x11, lsl #3
   10784:	csel	x24, x10, x13, gt
   10788:	csel	x25, x11, x9, gt
   1078c:	cmp	x8, x20
   10790:	b.le	10828 <__gmpf_sub@@Base+0x18c>
   10794:	lsl	x1, x8, #3
   10798:	mov	w8, #0x7f00                	// #32512
   1079c:	cmp	x1, x8
   107a0:	stur	w22, [x29, #-28]
   107a4:	b.hi	10cac <__gmpf_sub@@Base+0x610>  // b.pmore
   107a8:	add	x9, x1, #0xf
   107ac:	mov	x8, sp
   107b0:	and	x9, x9, #0xfffffffffffffff0
   107b4:	sub	x22, x8, x9
   107b8:	mov	sp, x22
   107bc:	cbz	x24, 107d4 <__gmpf_sub@@Base+0x138>
   107c0:	ldr	x8, [x25]
   107c4:	cbnz	x8, 10890 <__gmpf_sub@@Base+0x1f4>
   107c8:	subs	x24, x24, #0x1
   107cc:	add	x25, x25, #0x8
   107d0:	b.ne	107c0 <__gmpf_sub@@Base+0x124>  // b.any
   107d4:	mov	x1, x23
   107d8:	b	10d38 <__gmpf_sub@@Base+0x69c>
   107dc:	mov	x0, x19
   107e0:	mov	x1, x2
   107e4:	bl	cdb0 <__gmpf_neg@plt>
   107e8:	b	10d78 <__gmpf_sub@@Base+0x6dc>
   107ec:	cmp	x19, x1
   107f0:	b.eq	10d78 <__gmpf_sub@@Base+0x6dc>  // b.none
   107f4:	mov	x0, x19
   107f8:	bl	c2a0 <__gmpf_set@plt>
   107fc:	b	10d78 <__gmpf_sub@@Base+0x6dc>
   10800:	neg	w8, w9
   10804:	stur	w8, [x29, #-20]
   10808:	ldr	x8, [x2, #8]
   1080c:	mov	x0, x19
   10810:	stur	x8, [x29, #-16]
   10814:	ldr	x8, [x2, #16]
   10818:	sub	x2, x29, #0x18
   1081c:	stur	x8, [x29, #-8]
   10820:	bl	c0a0 <__gmpf_add@plt>
   10824:	b	10d78 <__gmpf_sub@@Base+0x6dc>
   10828:	cmp	x0, x23
   1082c:	b.eq	10d44 <__gmpf_sub@@Base+0x6a8>  // b.none
   10830:	mov	x1, x23
   10834:	b	1094c <__gmpf_sub@@Base+0x2b0>
   10838:	add	x15, x9, x11, lsl #3
   1083c:	add	x16, x23, x10, lsl #3
   10840:	mov	x14, xzr
   10844:	sub	x12, x11, x10
   10848:	sub	x13, x10, x11
   1084c:	sub	x15, x15, #0x8
   10850:	sub	x16, x16, #0x8
   10854:	lsl	x17, x14, #3
   10858:	ldr	x18, [x16, x17]
   1085c:	ldr	x17, [x15, x17]
   10860:	cmp	x18, x17
   10864:	add	x17, x10, x14
   10868:	b.ne	108c4 <__gmpf_sub@@Base+0x228>  // b.any
   1086c:	sub	x17, x17, #0x1
   10870:	cbz	x17, 1090c <__gmpf_sub@@Base+0x270>
   10874:	sub	x14, x14, #0x1
   10878:	cmn	x11, x14
   1087c:	b.ne	10854 <__gmpf_sub@@Base+0x1b8>  // b.any
   10880:	mov	x10, x11
   10884:	mov	x12, x13
   10888:	mov	x9, x23
   1088c:	b	10910 <__gmpf_sub@@Base+0x274>
   10890:	cbz	x21, 108a8 <__gmpf_sub@@Base+0x20c>
   10894:	ldr	x8, [x23]
   10898:	cbnz	x8, 1097c <__gmpf_sub@@Base+0x2e0>
   1089c:	subs	x21, x21, #0x1
   108a0:	add	x23, x23, #0x8
   108a4:	b.ne	10894 <__gmpf_sub@@Base+0x1f8>  // b.any
   108a8:	mov	x1, x25
   108ac:	mov	x2, x24
   108b0:	bl	cc10 <__gmpn_copyi@plt>
   108b4:	ldur	w22, [x29, #-28]
   108b8:	mov	x21, x24
   108bc:	eor	w22, w22, #0x1
   108c0:	b	10d44 <__gmpf_sub@@Base+0x6a8>
   108c4:	add	x10, x11, x14
   108c8:	csel	x13, x17, x10, cc  // cc = lo, ul, last
   108cc:	csel	x15, x23, x9, cc  // cc = lo, ul, last
   108d0:	csel	x23, x9, x23, cc  // cc = lo, ul, last
   108d4:	csel	x10, x10, x17, cc  // cc = lo, ul, last
   108d8:	sub	x11, x13, #0x1
   108dc:	add	x9, x23, x10, lsl #3
   108e0:	ldr	x12, [x15, x11, lsl #3]
   108e4:	ldur	x9, [x9, #-8]
   108e8:	cset	w16, cc  // cc = lo, ul, last
   108ec:	eor	w22, w22, w16
   108f0:	add	x12, x12, #0x1
   108f4:	cmp	x9, x12
   108f8:	add	x12, x27, x14
   108fc:	mov	x9, x15
   10900:	mov	x26, x12
   10904:	b.ne	10764 <__gmpf_sub@@Base+0xc8>  // b.any
   10908:	b	109f0 <__gmpf_sub@@Base+0x354>
   1090c:	eor	w22, w22, #0x1
   10910:	sub	x10, x27, x10
   10914:	cbz	x12, 10938 <__gmpf_sub@@Base+0x29c>
   10918:	sub	x26, x10, x12
   1091c:	sub	x11, x9, #0x8
   10920:	ldr	x13, [x11, x12, lsl #3]
   10924:	cbnz	x13, 10938 <__gmpf_sub@@Base+0x29c>
   10928:	sub	x12, x12, #0x1
   1092c:	sub	x10, x10, #0x1
   10930:	cbnz	x12, 10920 <__gmpf_sub@@Base+0x284>
   10934:	b	1093c <__gmpf_sub@@Base+0x2a0>
   10938:	mov	x26, x10
   1093c:	subs	x10, x12, x8
   10940:	add	x10, x9, x10, lsl #3
   10944:	csel	x21, x8, x12, gt
   10948:	csel	x1, x10, x9, gt
   1094c:	mov	x2, x21
   10950:	bl	cc10 <__gmpn_copyi@plt>
   10954:	b	10d44 <__gmpf_sub@@Base+0x6a8>
   10958:	cmp	x10, #0x2
   1095c:	b.lt	109ec <__gmpf_sub@@Base+0x350>  // b.tstop
   10960:	add	x12, x23, x10, lsl #3
   10964:	ldur	x14, [x12, #-16]
   10968:	mov	x12, x27
   1096c:	mov	x26, x27
   10970:	mov	x13, x11
   10974:	cbnz	x14, 10764 <__gmpf_sub@@Base+0xc8>
   10978:	b	109f0 <__gmpf_sub@@Base+0x354>
   1097c:	subs	x8, x21, x20
   10980:	b.le	10abc <__gmpf_sub@@Base+0x420>
   10984:	cbz	x20, 10bfc <__gmpf_sub@@Base+0x560>
   10988:	add	x20, x24, x20
   1098c:	mov	x28, x0
   10990:	subs	x9, x20, x21
   10994:	b.le	10c44 <__gmpf_sub@@Base+0x5a8>
   10998:	mov	x0, x22
   1099c:	mov	x1, x25
   109a0:	mov	x2, x9
   109a4:	mov	x24, x9
   109a8:	mov	x27, x8
   109ac:	bl	ce90 <__gmpn_neg@plt>
   109b0:	lsl	x8, x24, #3
   109b4:	add	x24, x22, x8
   109b8:	add	x3, x25, x8
   109bc:	mov	x0, x24
   109c0:	mov	x1, x23
   109c4:	mov	x2, x21
   109c8:	mov	x4, x27
   109cc:	bl	d340 <__gmpn_sub@plt>
   109d0:	ldr	x8, [x24]
   109d4:	sub	x9, x8, #0x1
   109d8:	str	x9, [x24], #8
   109dc:	cbz	x8, 109d0 <__gmpf_sub@@Base+0x334>
   109e0:	mov	x21, x20
   109e4:	mov	x0, x28
   109e8:	b	10d10 <__gmpf_sub@@Base+0x674>
   109ec:	mov	x12, x27
   109f0:	lsl	x13, x11, #3
   109f4:	mov	w14, #0x8                   	// #8
   109f8:	lsl	x1, x8, #3
   109fc:	add	x15, x13, x9
   10a00:	mov	x20, x0
   10a04:	mov	x2, xzr
   10a08:	sub	x3, x14, x13
   10a0c:	sub	x17, x15, #0x8
   10a10:	add	x18, x23, x10, lsl #3
   10a14:	mov	x4, x1
   10a18:	add	x0, x10, x2
   10a1c:	mov	x16, x3
   10a20:	mov	x14, x2
   10a24:	mov	x15, x4
   10a28:	add	x13, x11, x2
   10a2c:	sub	x24, x0, #0x1
   10a30:	cbz	x13, 10a5c <__gmpf_sub@@Base+0x3c0>
   10a34:	cbz	x24, 10a5c <__gmpf_sub@@Base+0x3c0>
   10a38:	add	x2, x18, x14, lsl #3
   10a3c:	ldur	x2, [x2, #-16]
   10a40:	cbnz	x2, 10a5c <__gmpf_sub@@Base+0x3c0>
   10a44:	ldr	x4, [x17, x14, lsl #3]
   10a48:	add	x3, x16, #0x8
   10a4c:	sub	x2, x14, #0x1
   10a50:	cmn	x4, #0x1
   10a54:	add	x4, x15, #0x8
   10a58:	b.eq	10a18 <__gmpf_sub@@Base+0x37c>  // b.none
   10a5c:	add	x17, x12, x14
   10a60:	sub	x26, x17, #0x1
   10a64:	cbz	x24, 10a80 <__gmpf_sub@@Base+0x3e4>
   10a68:	cmp	x0, x8
   10a6c:	b.le	10b28 <__gmpf_sub@@Base+0x48c>
   10a70:	add	x10, x23, x10, lsl #3
   10a74:	sub	x24, x8, #0x1
   10a78:	sub	x23, x10, x15
   10a7c:	b	10b28 <__gmpf_sub@@Base+0x48c>
   10a80:	cbz	x13, 10b28 <__gmpf_sub@@Base+0x48c>
   10a84:	mov	x10, xzr
   10a88:	sub	x15, x9, x16
   10a8c:	sub	x26, x26, x13
   10a90:	add	x13, x11, x14
   10a94:	ldr	x16, [x15]
   10a98:	cmn	x16, #0x1
   10a9c:	b.ne	10b14 <__gmpf_sub@@Base+0x478>  // b.any
   10aa0:	add	x16, x13, x10
   10aa4:	sub	x10, x10, #0x1
   10aa8:	cmp	x16, #0x1
   10aac:	sub	x15, x15, #0x8
   10ab0:	b.ne	10a94 <__gmpf_sub@@Base+0x3f8>  // b.any
   10ab4:	mov	x13, xzr
   10ab8:	b	10b28 <__gmpf_sub@@Base+0x48c>
   10abc:	add	x8, x24, x20
   10ac0:	stp	x8, x0, [x29, #-48]
   10ac4:	mov	x0, x22
   10ac8:	mov	x1, x25
   10acc:	mov	x2, x24
   10ad0:	sub	x20, x8, x21
   10ad4:	bl	ce90 <__gmpn_neg@plt>
   10ad8:	cmp	x24, x20
   10adc:	b.ge	10af8 <__gmpf_sub@@Base+0x45c>  // b.tcont
   10ae0:	add	x8, x21, x28
   10ae4:	sub	x8, x27, x8
   10ae8:	add	x0, x22, x24, lsl #3
   10aec:	lsl	x2, x8, #3
   10af0:	mov	w1, #0xff                  	// #255
   10af4:	bl	c780 <memset@plt>
   10af8:	add	x0, x22, x20, lsl #3
   10afc:	mov	w3, #0x1                   	// #1
   10b00:	mov	x1, x23
   10b04:	mov	x2, x21
   10b08:	bl	caf0 <__gmpn_sub_1@plt>
   10b0c:	ldp	x21, x0, [x29, #-48]
   10b10:	b	10d10 <__gmpf_sub@@Base+0x674>
   10b14:	add	x11, x11, x14
   10b18:	add	x12, x12, x14
   10b1c:	add	x13, x11, x10
   10b20:	add	x10, x12, x10
   10b24:	sub	x26, x10, #0x1
   10b28:	sub	x10, x8, #0x1
   10b2c:	cmp	x13, x8
   10b30:	sub	x11, x13, x10
   10b34:	mov	w8, #0x7f00                	// #32512
   10b38:	csel	x21, x13, x10, lt  // lt = tstop
   10b3c:	add	x10, x9, x11, lsl #3
   10b40:	csel	x25, x9, x10, lt  // lt = tstop
   10b44:	cmp	x1, x8
   10b48:	stur	w22, [x29, #-28]
   10b4c:	b.hi	10cc8 <__gmpf_sub@@Base+0x62c>  // b.pmore
   10b50:	add	x9, x1, #0xf
   10b54:	mov	x8, sp
   10b58:	and	x9, x9, #0xfffffffffffffff0
   10b5c:	sub	x22, x8, x9
   10b60:	mov	sp, x22
   10b64:	cbz	x21, 10cd8 <__gmpf_sub@@Base+0x63c>
   10b68:	cbz	x24, 10ba8 <__gmpf_sub@@Base+0x50c>
   10b6c:	subs	x27, x24, x21
   10b70:	b.ge	10bbc <__gmpf_sub@@Base+0x520>  // b.tcont
   10b74:	sub	x27, x21, x24
   10b78:	mov	x0, x22
   10b7c:	mov	x1, x25
   10b80:	mov	x2, x27
   10b84:	bl	ce90 <__gmpn_neg@plt>
   10b88:	lsl	x8, x27, #3
   10b8c:	mov	x4, x0
   10b90:	add	x0, x22, x8
   10b94:	add	x2, x25, x8
   10b98:	mov	x1, x23
   10b9c:	mov	x3, x24
   10ba0:	bl	c8f0 <__gmpn_sub_nc@plt>
   10ba4:	b	10be8 <__gmpf_sub@@Base+0x54c>
   10ba8:	mov	x0, x22
   10bac:	mov	x1, x25
   10bb0:	mov	x2, x21
   10bb4:	bl	ce90 <__gmpn_neg@plt>
   10bb8:	b	10be8 <__gmpf_sub@@Base+0x54c>
   10bbc:	mov	x0, x22
   10bc0:	mov	x1, x23
   10bc4:	mov	x2, x27
   10bc8:	bl	cc10 <__gmpn_copyi@plt>
   10bcc:	lsl	x8, x27, #3
   10bd0:	add	x0, x22, x8
   10bd4:	add	x1, x23, x8
   10bd8:	mov	x2, x25
   10bdc:	mov	x3, x21
   10be0:	bl	c420 <__gmpn_sub_n@plt>
   10be4:	mov	x21, x24
   10be8:	cbz	x0, 10bf4 <__gmpf_sub@@Base+0x558>
   10bec:	mov	w8, #0x2e                  	// #46
   10bf0:	b	10cfc <__gmpf_sub@@Base+0x660>
   10bf4:	mov	x24, x21
   10bf8:	b	10ce8 <__gmpf_sub@@Base+0x64c>
   10bfc:	mov	x20, x0
   10c00:	subs	x27, x21, x24
   10c04:	b.ge	10c7c <__gmpf_sub@@Base+0x5e0>  // b.tcont
   10c08:	sub	x27, x24, x21
   10c0c:	mov	x0, x22
   10c10:	mov	x1, x25
   10c14:	mov	x2, x27
   10c18:	bl	ce90 <__gmpn_neg@plt>
   10c1c:	lsl	x8, x27, #3
   10c20:	add	x0, x22, x8
   10c24:	add	x2, x25, x8
   10c28:	mov	w4, #0x1                   	// #1
   10c2c:	mov	x1, x23
   10c30:	mov	x3, x21
   10c34:	bl	c8f0 <__gmpn_sub_nc@plt>
   10c38:	mov	x21, x24
   10c3c:	mov	x0, x20
   10c40:	b	10d10 <__gmpf_sub@@Base+0x674>
   10c44:	sub	x27, x8, x24
   10c48:	mov	x0, x22
   10c4c:	mov	x1, x23
   10c50:	mov	x2, x27
   10c54:	bl	cc10 <__gmpn_copyi@plt>
   10c58:	lsl	x8, x27, #3
   10c5c:	add	x0, x22, x8
   10c60:	add	x1, x23, x8
   10c64:	sub	x2, x21, x27
   10c68:	mov	x3, x25
   10c6c:	mov	x4, x24
   10c70:	bl	d340 <__gmpn_sub@plt>
   10c74:	mov	x0, x28
   10c78:	b	10d10 <__gmpf_sub@@Base+0x674>
   10c7c:	mov	x0, x22
   10c80:	mov	x1, x23
   10c84:	mov	x2, x27
   10c88:	bl	cc10 <__gmpn_copyi@plt>
   10c8c:	lsl	x8, x27, #3
   10c90:	add	x0, x22, x8
   10c94:	add	x1, x23, x8
   10c98:	mov	x2, x25
   10c9c:	mov	x3, x24
   10ca0:	bl	c420 <__gmpn_sub_n@plt>
   10ca4:	mov	x0, x20
   10ca8:	b	10d10 <__gmpf_sub@@Base+0x674>
   10cac:	stur	x0, [x29, #-40]
   10cb0:	sub	x0, x29, #0x18
   10cb4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   10cb8:	mov	x22, x0
   10cbc:	ldur	x0, [x29, #-40]
   10cc0:	cbnz	x24, 107c0 <__gmpf_sub@@Base+0x124>
   10cc4:	b	107d4 <__gmpf_sub@@Base+0x138>
   10cc8:	sub	x0, x29, #0x18
   10ccc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   10cd0:	mov	x22, x0
   10cd4:	cbnz	x21, 10b68 <__gmpf_sub@@Base+0x4cc>
   10cd8:	mov	x0, x22
   10cdc:	mov	x1, x23
   10ce0:	mov	x2, x24
   10ce4:	bl	cc10 <__gmpn_copyi@plt>
   10ce8:	mov	w8, #0x1                   	// #1
   10cec:	add	x21, x24, #0x1
   10cf0:	add	x26, x26, #0x1
   10cf4:	str	x8, [x22, x24, lsl #3]
   10cf8:	mov	w8, #0x23                  	// #35
   10cfc:	mov	x0, x20
   10d00:	cmp	w8, #0x23
   10d04:	b.eq	10d34 <__gmpf_sub@@Base+0x698>  // b.none
   10d08:	cmp	w8, #0x2e
   10d0c:	b.ne	10d78 <__gmpf_sub@@Base+0x6dc>  // b.any
   10d10:	cbz	x21, 10d34 <__gmpf_sub@@Base+0x698>
   10d14:	sub	x8, x26, x21
   10d18:	add	x9, x22, x21, lsl #3
   10d1c:	ldur	x9, [x9, #-8]
   10d20:	cbnz	x9, 10d34 <__gmpf_sub@@Base+0x698>
   10d24:	sub	x21, x21, #0x1
   10d28:	sub	x26, x26, #0x1
   10d2c:	cbnz	x21, 10d18 <__gmpf_sub@@Base+0x67c>
   10d30:	mov	x26, x8
   10d34:	mov	x1, x22
   10d38:	mov	x2, x21
   10d3c:	bl	cc10 <__gmpn_copyi@plt>
   10d40:	ldur	w22, [x29, #-28]
   10d44:	ldur	x0, [x29, #-24]
   10d48:	cbnz	x0, 10d68 <__gmpf_sub@@Base+0x6cc>
   10d4c:	cbz	x21, 10d70 <__gmpf_sub@@Base+0x6d4>
   10d50:	neg	w8, w21
   10d54:	cmp	w22, #0x0
   10d58:	csel	x8, x21, x8, eq  // eq = none
   10d5c:	str	w8, [x19, #4]
   10d60:	str	x26, [x19, #8]
   10d64:	b	10d78 <__gmpf_sub@@Base+0x6dc>
   10d68:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   10d6c:	cbnz	x21, 10d50 <__gmpf_sub@@Base+0x6b4>
   10d70:	str	wzr, [x19, #4]
   10d74:	str	xzr, [x19, #8]
   10d78:	mov	sp, x29
   10d7c:	ldp	x20, x19, [sp, #80]
   10d80:	ldp	x22, x21, [sp, #64]
   10d84:	ldp	x24, x23, [sp, #48]
   10d88:	ldp	x26, x25, [sp, #32]
   10d8c:	ldp	x28, x27, [sp, #16]
   10d90:	ldp	x29, x30, [sp], #96
   10d94:	ret

0000000000010d98 <__gmpf_sub_ui@@Base>:
   10d98:	sub	sp, sp, #0x30
   10d9c:	stp	x29, x30, [sp, #32]
   10da0:	add	x29, sp, #0x20
   10da4:	cbz	x2, 10dc8 <__gmpf_sub_ui@@Base+0x30>
   10da8:	str	x2, [sp]
   10dac:	mov	w8, #0x1                   	// #1
   10db0:	mov	x9, sp
   10db4:	add	x2, sp, #0x8
   10db8:	str	w8, [sp, #12]
   10dbc:	stp	x8, x9, [sp, #16]
   10dc0:	bl	cfd0 <__gmpf_sub@plt>
   10dc4:	b	10dcc <__gmpf_sub_ui@@Base+0x34>
   10dc8:	bl	c2a0 <__gmpf_set@plt>
   10dcc:	ldp	x29, x30, [sp, #32]
   10dd0:	add	sp, sp, #0x30
   10dd4:	ret

0000000000010dd8 <__gmpf_ui_sub@@Base>:
   10dd8:	sub	sp, sp, #0x30
   10ddc:	stp	x29, x30, [sp, #32]
   10de0:	add	x29, sp, #0x20
   10de4:	cbz	x1, 10e08 <__gmpf_ui_sub@@Base+0x30>
   10de8:	str	x1, [sp]
   10dec:	mov	w8, #0x1                   	// #1
   10df0:	mov	x9, sp
   10df4:	add	x1, sp, #0x8
   10df8:	str	w8, [sp, #12]
   10dfc:	stp	x8, x9, [sp, #16]
   10e00:	bl	cfd0 <__gmpf_sub@plt>
   10e04:	b	10e10 <__gmpf_ui_sub@@Base+0x38>
   10e08:	mov	x1, x2
   10e0c:	bl	cdb0 <__gmpf_neg@plt>
   10e10:	ldp	x29, x30, [sp, #32]
   10e14:	add	sp, sp, #0x30
   10e18:	ret

0000000000010e1c <__gmpf_mul@@Base>:
   10e1c:	stp	x29, x30, [sp, #-96]!
   10e20:	stp	x28, x27, [sp, #16]
   10e24:	stp	x26, x25, [sp, #32]
   10e28:	stp	x24, x23, [sp, #48]
   10e2c:	stp	x22, x21, [sp, #64]
   10e30:	stp	x20, x19, [sp, #80]
   10e34:	mov	x29, sp
   10e38:	sub	sp, sp, #0x10
   10e3c:	ldrsw	x27, [x0]
   10e40:	ldrsw	x8, [x1, #4]
   10e44:	mov	x20, x1
   10e48:	mov	x19, x0
   10e4c:	mov	x21, x2
   10e50:	cmp	x1, x2
   10e54:	b.eq	10ef0 <__gmpf_mul@@Base+0xd4>  // b.none
   10e58:	ldrsw	x9, [x21, #4]
   10e5c:	ldr	x10, [x20, #16]
   10e60:	cmp	x8, #0x0
   10e64:	ldr	x11, [x21, #16]
   10e68:	cneg	x12, x8, mi  // mi = first
   10e6c:	eor	w8, w9, w8
   10e70:	cmp	x9, #0x0
   10e74:	sxtw	x28, w8
   10e78:	cneg	x8, x9, mi  // mi = first
   10e7c:	subs	x9, x12, x27
   10e80:	add	x9, x10, x9, lsl #3
   10e84:	csel	x23, x27, x12, gt
   10e88:	csel	x24, x9, x10, gt
   10e8c:	subs	x9, x8, x27
   10e90:	add	x9, x11, x9, lsl #3
   10e94:	csel	x26, x9, x11, gt
   10e98:	csel	x25, x27, x8, gt
   10e9c:	cbz	x23, 10f60 <__gmpf_mul@@Base+0x144>
   10ea0:	cbz	x25, 10f60 <__gmpf_mul@@Base+0x144>
   10ea4:	add	x8, x25, x23
   10ea8:	stp	x8, xzr, [x29, #-16]
   10eac:	lsl	x1, x8, #3
   10eb0:	mov	w8, #0x7f00                	// #32512
   10eb4:	cmp	x1, x8
   10eb8:	b.hi	1102c <__gmpf_mul@@Base+0x210>  // b.pmore
   10ebc:	add	x9, x1, #0xf
   10ec0:	mov	x8, sp
   10ec4:	and	x9, x9, #0xfffffffffffffff0
   10ec8:	sub	x22, x8, x9
   10ecc:	mov	sp, x22
   10ed0:	mov	x0, x22
   10ed4:	cmp	x23, x25
   10ed8:	b.ge	10f74 <__gmpf_mul@@Base+0x158>  // b.tcont
   10edc:	mov	x1, x26
   10ee0:	mov	x2, x25
   10ee4:	mov	x3, x24
   10ee8:	mov	x4, x23
   10eec:	b	10f84 <__gmpf_mul@@Base+0x168>
   10ef0:	ldr	x9, [x20, #16]
   10ef4:	cmp	x8, #0x0
   10ef8:	cneg	x8, x8, mi  // mi = first
   10efc:	subs	x10, x8, x27
   10f00:	add	x10, x9, x10, lsl #3
   10f04:	csel	x23, x27, x8, gt
   10f08:	csel	x24, x10, x9, gt
   10f0c:	cbz	x23, 10f98 <__gmpf_mul@@Base+0x17c>
   10f10:	lsl	x1, x23, #4
   10f14:	mov	w8, #0x7f00                	// #32512
   10f18:	cmp	x1, x8
   10f1c:	lsl	x25, x23, #1
   10f20:	stur	xzr, [x29, #-8]
   10f24:	b.hi	1103c <__gmpf_mul@@Base+0x220>  // b.pmore
   10f28:	add	x9, x1, #0xf
   10f2c:	mov	x8, sp
   10f30:	and	x9, x9, #0xfffffffffffffff0
   10f34:	sub	x22, x8, x9
   10f38:	mov	sp, x22
   10f3c:	mov	x0, x22
   10f40:	mov	x1, x24
   10f44:	mov	x2, x23
   10f48:	bl	ca90 <__gmpn_sqr@plt>
   10f4c:	add	x8, x22, x25, lsl #3
   10f50:	ldur	x0, [x8, #-8]
   10f54:	mov	w8, #0x1                   	// #1
   10f58:	tbnz	w8, #0, 10fa8 <__gmpf_mul@@Base+0x18c>
   10f5c:	b	11004 <__gmpf_mul@@Base+0x1e8>
   10f60:	mov	w8, wzr
   10f64:	str	wzr, [x19, #4]
   10f68:	str	xzr, [x19, #8]
   10f6c:	cbnz	w8, 10fac <__gmpf_mul@@Base+0x190>
   10f70:	b	11004 <__gmpf_mul@@Base+0x1e8>
   10f74:	mov	x1, x24
   10f78:	mov	x2, x23
   10f7c:	mov	x3, x26
   10f80:	mov	x4, x25
   10f84:	bl	cea0 <__gmpn_mul@plt>
   10f88:	ldur	x25, [x29, #-16]
   10f8c:	mov	w8, #0x1                   	// #1
   10f90:	cbnz	w8, 10fac <__gmpf_mul@@Base+0x190>
   10f94:	b	11004 <__gmpf_mul@@Base+0x1e8>
   10f98:	mov	w8, wzr
   10f9c:	str	wzr, [x19, #4]
   10fa0:	str	xzr, [x19, #8]
   10fa4:	tbz	w8, #0, 11004 <__gmpf_mul@@Base+0x1e8>
   10fa8:	mov	x28, xzr
   10fac:	cmp	x0, #0x0
   10fb0:	cset	w24, eq  // eq = none
   10fb4:	add	x8, x27, #0x1
   10fb8:	ldr	x0, [x19, #16]
   10fbc:	sub	x9, x25, x24
   10fc0:	subs	x8, x9, x8
   10fc4:	add	x8, x22, x8, lsl #3
   10fc8:	csinc	x23, x9, x27, le
   10fcc:	csel	x1, x8, x22, gt
   10fd0:	mov	x2, x23
   10fd4:	bl	cc10 <__gmpn_copyi@plt>
   10fd8:	ldr	x8, [x20, #8]
   10fdc:	ldr	x9, [x21, #8]
   10fe0:	neg	w10, w23
   10fe4:	cmp	x28, #0x0
   10fe8:	sub	x8, x8, x24
   10fec:	csel	x10, x23, x10, ge  // ge = tcont
   10ff0:	add	x8, x8, x9
   10ff4:	str	x8, [x19, #8]
   10ff8:	str	w10, [x19, #4]
   10ffc:	ldur	x0, [x29, #-8]
   11000:	cbnz	x0, 11024 <__gmpf_mul@@Base+0x208>
   11004:	mov	sp, x29
   11008:	ldp	x20, x19, [sp, #80]
   1100c:	ldp	x22, x21, [sp, #64]
   11010:	ldp	x24, x23, [sp, #48]
   11014:	ldp	x26, x25, [sp, #32]
   11018:	ldp	x28, x27, [sp, #16]
   1101c:	ldp	x29, x30, [sp], #96
   11020:	ret
   11024:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   11028:	b	11004 <__gmpf_mul@@Base+0x1e8>
   1102c:	sub	x0, x29, #0x8
   11030:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11034:	mov	x22, x0
   11038:	b	10ed0 <__gmpf_mul@@Base+0xb4>
   1103c:	sub	x0, x29, #0x8
   11040:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11044:	mov	x22, x0
   11048:	b	10f3c <__gmpf_mul@@Base+0x120>

000000000001104c <__gmpf_mul_ui@@Base>:
   1104c:	stp	x29, x30, [sp, #-64]!
   11050:	stp	x20, x19, [sp, #48]
   11054:	mov	x19, x0
   11058:	str	x23, [sp, #16]
   1105c:	stp	x22, x21, [sp, #32]
   11060:	mov	x29, sp
   11064:	cbz	x2, 11134 <__gmpf_mul_ui@@Base+0xe8>
   11068:	ldr	w8, [x1, #4]
   1106c:	mov	x20, x1
   11070:	cbz	w8, 11134 <__gmpf_mul_ui@@Base+0xe8>
   11074:	ldrsw	x21, [x19]
   11078:	sxtw	x23, w8
   1107c:	cmp	w23, #0x0
   11080:	ldr	x1, [x20, #16]
   11084:	cneg	x9, x23, lt  // lt = tstop
   11088:	sub	x8, x9, x21
   1108c:	mov	x3, x2
   11090:	cmp	x8, #0x1
   11094:	b.lt	110e0 <__gmpf_mul_ui@@Base+0x94>  // b.tstop
   11098:	add	x9, x1, x8, lsl #3
   1109c:	ldur	x9, [x9, #-8]
   110a0:	sub	x10, x1, #0x10
   110a4:	mov	x11, x8
   110a8:	umulh	x4, x9, x3
   110ac:	sub	x12, x11, #0x1
   110b0:	cmp	x12, #0x1
   110b4:	b.lt	110d8 <__gmpf_mul_ui@@Base+0x8c>  // b.tstop
   110b8:	mul	x13, x9, x3
   110bc:	ldr	x9, [x10, x11, lsl #3]
   110c0:	umulh	x11, x9, x3
   110c4:	adds	x11, x11, x13
   110c8:	cinc	x4, x4, cs  // cs = hs, nlast
   110cc:	cmn	x11, #0x1
   110d0:	mov	x11, x12
   110d4:	b.eq	110ac <__gmpf_mul_ui@@Base+0x60>  // b.none
   110d8:	add	x1, x1, x8, lsl #3
   110dc:	b	110e8 <__gmpf_mul_ui@@Base+0x9c>
   110e0:	mov	x4, xzr
   110e4:	mov	x21, x9
   110e8:	ldr	x22, [x19, #16]
   110ec:	mov	x2, x21
   110f0:	mov	x0, x22
   110f4:	bl	d420 <__gmpn_mul_1c@plt>
   110f8:	str	x0, [x22, x21, lsl #3]
   110fc:	ldr	x8, [x20, #8]
   11100:	cmp	x0, #0x0
   11104:	cinc	x9, x21, ne  // ne = any
   11108:	neg	w10, w9
   1110c:	cinc	x8, x8, ne  // ne = any
   11110:	cmp	w23, #0x0
   11114:	str	x8, [x19, #8]
   11118:	csel	x8, x9, x10, ge  // ge = tcont
   1111c:	str	w8, [x19, #4]
   11120:	ldp	x20, x19, [sp, #48]
   11124:	ldp	x22, x21, [sp, #32]
   11128:	ldr	x23, [sp, #16]
   1112c:	ldp	x29, x30, [sp], #64
   11130:	ret
   11134:	str	wzr, [x19, #4]
   11138:	str	xzr, [x19, #8]
   1113c:	b	11120 <__gmpf_mul_ui@@Base+0xd4>

0000000000011140 <__gmpf_div@@Base>:
   11140:	stp	x29, x30, [sp, #-96]!
   11144:	stp	x28, x27, [sp, #16]
   11148:	stp	x26, x25, [sp, #32]
   1114c:	stp	x24, x23, [sp, #48]
   11150:	stp	x22, x21, [sp, #64]
   11154:	stp	x20, x19, [sp, #80]
   11158:	mov	x29, sp
   1115c:	sub	sp, sp, #0x30
   11160:	ldrsw	x26, [x2, #4]
   11164:	cbz	w26, 113b4 <__gmpf_div@@Base+0x274>
   11168:	ldrsw	x28, [x1, #4]
   1116c:	mov	x19, x0
   11170:	cbz	w28, 111f0 <__gmpf_div@@Base+0xb0>
   11174:	ldr	x8, [x2, #8]
   11178:	cmp	x28, #0x0
   1117c:	ldrsw	x27, [x19]
   11180:	cneg	x11, x28, mi  // mi = first
   11184:	cmp	x26, #0x0
   11188:	cneg	x21, x26, mi  // mi = first
   1118c:	ldr	x20, [x19, #16]
   11190:	ldp	x23, x10, [x1, #8]
   11194:	stp	x8, xzr, [x29, #-16]
   11198:	sub	x8, x21, x11
   1119c:	ldr	x3, [x2, #16]
   111a0:	add	x8, x8, x27
   111a4:	neg	x9, x8
   111a8:	and	x9, x9, x8, asr #63
   111ac:	cmp	x8, #0x0
   111b0:	add	x12, x10, x9, lsl #3
   111b4:	sub	x2, x11, x9
   111b8:	b.gt	111fc <__gmpf_div@@Base+0xbc>
   111bc:	cmp	x20, x10
   111c0:	b.eq	111fc <__gmpf_div@@Base+0xbc>  // b.none
   111c4:	lsl	x8, x2, #3
   111c8:	add	x1, x8, #0x8
   111cc:	mov	w8, #0x7f00                	// #32512
   111d0:	cmp	x1, x8
   111d4:	b.hi	1133c <__gmpf_div@@Base+0x1fc>  // b.pmore
   111d8:	add	x9, x1, #0xf
   111dc:	mov	x8, sp
   111e0:	and	x9, x9, #0xfffffffffffffff0
   111e4:	sub	x25, x8, x9
   111e8:	mov	sp, x25
   111ec:	b	11278 <__gmpf_div@@Base+0x138>
   111f0:	str	wzr, [x19, #4]
   111f4:	str	xzr, [x19, #8]
   111f8:	b	11314 <__gmpf_div@@Base+0x1d4>
   111fc:	add	x22, x21, x27
   11200:	lsl	x10, x22, #3
   11204:	add	x1, x10, #0x8
   11208:	mov	w10, #0x7f00                	// #32512
   1120c:	stur	x23, [x29, #-32]
   11210:	cmp	x1, x10
   11214:	add	x23, x9, x8
   11218:	b.hi	11364 <__gmpf_div@@Base+0x224>  // b.pmore
   1121c:	add	x9, x1, #0xf
   11220:	mov	x8, sp
   11224:	and	x9, x9, #0xfffffffffffffff0
   11228:	sub	x25, x8, x9
   1122c:	mov	sp, x25
   11230:	stur	x3, [x29, #-40]
   11234:	cbz	x23, 11260 <__gmpf_div@@Base+0x120>
   11238:	stur	x2, [x29, #-24]
   1123c:	lsl	x2, x23, #3
   11240:	mov	x0, x25
   11244:	mov	w1, wzr
   11248:	mov	x24, x22
   1124c:	mov	x22, x12
   11250:	bl	c780 <memset@plt>
   11254:	ldur	x2, [x29, #-24]
   11258:	mov	x12, x22
   1125c:	mov	x22, x24
   11260:	add	x0, x25, x23, lsl #3
   11264:	mov	x1, x12
   11268:	bl	cc10 <__gmpn_copyi@plt>
   1126c:	ldp	x3, x23, [x29, #-40]
   11270:	mov	x2, x22
   11274:	mov	x12, x25
   11278:	eor	w28, w26, w28
   1127c:	cmp	x20, x3
   11280:	add	x24, x27, #0x1
   11284:	b.ne	112c8 <__gmpf_div@@Base+0x188>  // b.any
   11288:	lsl	x1, x21, #3
   1128c:	mov	w8, #0x7f00                	// #32512
   11290:	cmp	x1, x8
   11294:	stp	x12, x2, [x29, #-32]
   11298:	b.hi	1139c <__gmpf_div@@Base+0x25c>  // b.pmore
   1129c:	add	x9, x1, #0xf
   112a0:	mov	x8, sp
   112a4:	and	x9, x9, #0xfffffffffffffff0
   112a8:	sub	x26, x8, x9
   112ac:	mov	sp, x26
   112b0:	mov	x0, x26
   112b4:	mov	x1, x3
   112b8:	mov	x2, x21
   112bc:	bl	cc10 <__gmpn_copyi@plt>
   112c0:	ldp	x12, x2, [x29, #-32]
   112c4:	mov	x3, x26
   112c8:	mov	x0, x20
   112cc:	mov	x1, x12
   112d0:	mov	x4, x21
   112d4:	mov	x5, x25
   112d8:	bl	c480 <__gmpn_div_q@plt>
   112dc:	ldr	x8, [x20, x27, lsl #3]
   112e0:	ldp	x9, x0, [x29, #-16]
   112e4:	cmp	x8, #0x0
   112e8:	cset	w8, eq  // eq = none
   112ec:	sub	x9, x23, x9
   112f0:	sub	x10, x24, x8
   112f4:	cmp	w28, #0x0
   112f8:	sub	x8, x9, x8
   112fc:	neg	w9, w10
   11300:	add	x8, x8, #0x1
   11304:	csel	x9, x10, x9, ge  // ge = tcont
   11308:	str	w9, [x19, #4]
   1130c:	str	x8, [x19, #8]
   11310:	cbnz	x0, 11334 <__gmpf_div@@Base+0x1f4>
   11314:	mov	sp, x29
   11318:	ldp	x20, x19, [sp, #80]
   1131c:	ldp	x22, x21, [sp, #64]
   11320:	ldp	x24, x23, [sp, #48]
   11324:	ldp	x26, x25, [sp, #32]
   11328:	ldp	x28, x27, [sp, #16]
   1132c:	ldp	x29, x30, [sp], #96
   11330:	ret
   11334:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   11338:	b	11314 <__gmpf_div@@Base+0x1d4>
   1133c:	sub	x0, x29, #0x8
   11340:	mov	x24, x2
   11344:	mov	x22, x12
   11348:	mov	x25, x3
   1134c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11350:	mov	x3, x25
   11354:	mov	x12, x22
   11358:	mov	x2, x24
   1135c:	mov	x25, x0
   11360:	b	11278 <__gmpf_div@@Base+0x138>
   11364:	sub	x0, x29, #0x8
   11368:	mov	x24, x2
   1136c:	stur	x22, [x29, #-24]
   11370:	mov	x22, x12
   11374:	mov	x25, x3
   11378:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1137c:	mov	x12, x22
   11380:	ldur	x22, [x29, #-24]
   11384:	mov	x3, x25
   11388:	mov	x2, x24
   1138c:	mov	x25, x0
   11390:	stur	x3, [x29, #-40]
   11394:	cbnz	x23, 11238 <__gmpf_div@@Base+0xf8>
   11398:	b	11260 <__gmpf_div@@Base+0x120>
   1139c:	sub	x0, x29, #0x8
   113a0:	mov	x22, x3
   113a4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   113a8:	mov	x3, x22
   113ac:	mov	x26, x0
   113b0:	b	112b0 <__gmpf_div@@Base+0x170>
   113b4:	bl	c100 <__gmp_divide_by_zero@plt>

00000000000113b8 <__gmpf_div_ui@@Base>:
   113b8:	stp	x29, x30, [sp, #-96]!
   113bc:	stp	x28, x27, [sp, #16]
   113c0:	stp	x26, x25, [sp, #32]
   113c4:	stp	x24, x23, [sp, #48]
   113c8:	stp	x22, x21, [sp, #64]
   113cc:	stp	x20, x19, [sp, #80]
   113d0:	mov	x29, sp
   113d4:	sub	sp, sp, #0x10
   113d8:	cbz	x2, 11518 <__gmpf_div_ui@@Base+0x160>
   113dc:	ldrsw	x27, [x1, #4]
   113e0:	mov	x20, x1
   113e4:	mov	x19, x0
   113e8:	cbz	w27, 1144c <__gmpf_div_ui@@Base+0x94>
   113ec:	ldrsw	x28, [x19]
   113f0:	stur	xzr, [x29, #-8]
   113f4:	ldr	x23, [x19, #16]
   113f8:	ldr	x24, [x20, #16]
   113fc:	lsl	x8, x28, #3
   11400:	cmp	w27, #0x0
   11404:	add	x1, x8, #0x10
   11408:	mov	w8, #0x7f00                	// #32512
   1140c:	mov	x21, x2
   11410:	cneg	x25, x27, lt  // lt = tstop
   11414:	cmp	x1, x8
   11418:	add	x22, x28, #0x1
   1141c:	b.hi	11458 <__gmpf_div_ui@@Base+0xa0>  // b.pmore
   11420:	add	x9, x1, #0xf
   11424:	mov	x8, sp
   11428:	and	x9, x9, #0xfffffffffffffff0
   1142c:	sub	x26, x8, x9
   11430:	mov	sp, x26
   11434:	subs	x8, x25, x22
   11438:	b.le	1146c <__gmpf_div_ui@@Base+0xb4>
   1143c:	add	x24, x24, x8, lsl #3
   11440:	mov	x25, x22
   11444:	mov	x0, x26
   11448:	b	11498 <__gmpf_div_ui@@Base+0xe0>
   1144c:	str	wzr, [x19, #4]
   11450:	str	xzr, [x19, #8]
   11454:	b	114f0 <__gmpf_div_ui@@Base+0x138>
   11458:	sub	x0, x29, #0x8
   1145c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11460:	mov	x26, x0
   11464:	subs	x8, x25, x22
   11468:	b.gt	1143c <__gmpf_div_ui@@Base+0x84>
   1146c:	subs	x8, x22, x25
   11470:	b.eq	11494 <__gmpf_div_ui@@Base+0xdc>  // b.none
   11474:	stur	x8, [x29, #-16]
   11478:	sub	x8, x28, x25
   1147c:	lsl	x8, x8, #3
   11480:	add	x2, x8, #0x8
   11484:	mov	x0, x26
   11488:	mov	w1, wzr
   1148c:	bl	c780 <memset@plt>
   11490:	ldur	x8, [x29, #-16]
   11494:	add	x0, x26, x8, lsl #3
   11498:	mov	x1, x24
   1149c:	mov	x2, x25
   114a0:	bl	cc10 <__gmpn_copyi@plt>
   114a4:	mov	x0, x23
   114a8:	mov	x1, xzr
   114ac:	mov	x2, x26
   114b0:	mov	x3, x22
   114b4:	mov	x4, x21
   114b8:	bl	ced0 <__gmpn_divrem_1@plt>
   114bc:	ldr	x8, [x23, x28, lsl #3]
   114c0:	ldr	x9, [x20, #8]
   114c4:	cmp	x8, #0x0
   114c8:	cset	w8, eq  // eq = none
   114cc:	sub	x10, x22, x8
   114d0:	cmp	w27, #0x0
   114d4:	sub	x8, x9, x8
   114d8:	neg	w9, w10
   114dc:	csel	x9, x10, x9, ge  // ge = tcont
   114e0:	str	w9, [x19, #4]
   114e4:	str	x8, [x19, #8]
   114e8:	ldur	x0, [x29, #-8]
   114ec:	cbnz	x0, 11510 <__gmpf_div_ui@@Base+0x158>
   114f0:	mov	sp, x29
   114f4:	ldp	x20, x19, [sp, #80]
   114f8:	ldp	x22, x21, [sp, #64]
   114fc:	ldp	x24, x23, [sp, #48]
   11500:	ldp	x26, x25, [sp, #32]
   11504:	ldp	x28, x27, [sp, #16]
   11508:	ldp	x29, x30, [sp], #96
   1150c:	ret
   11510:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   11514:	b	114f0 <__gmpf_div_ui@@Base+0x138>
   11518:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001151c <__gmpf_cmp_z@@Base>:
   1151c:	sub	sp, sp, #0x30
   11520:	stp	x29, x30, [sp, #32]
   11524:	ldrsw	x8, [x1, #4]
   11528:	add	x29, sp, #0x20
   1152c:	cmp	x8, #0x0
   11530:	str	w8, [sp, #12]
   11534:	cneg	x8, x8, mi  // mi = first
   11538:	str	x8, [sp, #16]
   1153c:	ldr	x8, [x1, #8]
   11540:	add	x1, sp, #0x8
   11544:	str	x8, [sp, #24]
   11548:	bl	c800 <__gmpf_cmp@plt>
   1154c:	ldp	x29, x30, [sp, #32]
   11550:	add	sp, sp, #0x30
   11554:	ret

0000000000011558 <__gmpf_cmp@@Base>:
   11558:	stp	x29, x30, [sp, #-32]!
   1155c:	ldrsw	x9, [x0, #4]
   11560:	ldrsw	x8, [x1, #4]
   11564:	mov	w10, #0x1                   	// #1
   11568:	str	x19, [sp, #16]
   1156c:	cmp	w9, #0x0
   11570:	eor	w11, w8, w9
   11574:	cneg	w19, w10, lt  // lt = tstop
   11578:	mov	x29, sp
   1157c:	tbnz	w11, #31, 11640 <__gmpf_cmp@@Base+0xe8>
   11580:	cbz	w9, 115a4 <__gmpf_cmp@@Base+0x4c>
   11584:	cbz	w8, 115b0 <__gmpf_cmp@@Base+0x58>
   11588:	ldr	x10, [x0, #8]
   1158c:	ldr	x11, [x1, #8]
   11590:	cmp	x10, x11
   11594:	b.gt	11640 <__gmpf_cmp@@Base+0xe8>
   11598:	b.ge	115b8 <__gmpf_cmp@@Base+0x60>  // b.tcont
   1159c:	neg	w19, w19
   115a0:	b	11640 <__gmpf_cmp@@Base+0xe8>
   115a4:	cmp	w8, #0x0
   115a8:	csetm	w19, ne  // ne = any
   115ac:	b	11640 <__gmpf_cmp@@Base+0xe8>
   115b0:	mov	w19, #0x1                   	// #1
   115b4:	b	11640 <__gmpf_cmp@@Base+0xe8>
   115b8:	ldr	x0, [x0, #16]
   115bc:	ldr	x1, [x1, #16]
   115c0:	cmp	w9, #0x0
   115c4:	cneg	x2, x9, lt  // lt = tstop
   115c8:	ldr	x10, [x0]
   115cc:	cmp	x8, #0x0
   115d0:	cneg	x8, x8, mi  // mi = first
   115d4:	cbnz	x10, 115e4 <__gmpf_cmp@@Base+0x8c>
   115d8:	ldr	x9, [x0, #8]!
   115dc:	sub	x2, x2, #0x1
   115e0:	cbz	x9, 115d8 <__gmpf_cmp@@Base+0x80>
   115e4:	ldr	x9, [x1]
   115e8:	cbnz	x9, 115f8 <__gmpf_cmp@@Base+0xa0>
   115ec:	ldr	x9, [x1, #8]!
   115f0:	sub	x8, x8, #0x1
   115f4:	cbz	x9, 115ec <__gmpf_cmp@@Base+0x94>
   115f8:	cmp	x2, x8
   115fc:	b.le	11618 <__gmpf_cmp@@Base+0xc0>
   11600:	add	x9, x0, x2, lsl #3
   11604:	sub	x0, x9, x8, lsl #3
   11608:	mov	x2, x8
   1160c:	bl	c570 <__gmpn_cmp@plt>
   11610:	add	w0, w0, #0x1
   11614:	b	11638 <__gmpf_cmp@@Base+0xe0>
   11618:	cmp	x8, x2
   1161c:	b.le	11630 <__gmpf_cmp@@Base+0xd8>
   11620:	add	x8, x1, x8, lsl #3
   11624:	sub	x1, x8, x2, lsl #3
   11628:	bl	c570 <__gmpn_cmp@plt>
   1162c:	b	11638 <__gmpf_cmp@@Base+0xe0>
   11630:	bl	c570 <__gmpn_cmp@plt>
   11634:	cbz	w0, 11650 <__gmpf_cmp@@Base+0xf8>
   11638:	cmp	w0, #0x0
   1163c:	cneg	w19, w19, le
   11640:	mov	w0, w19
   11644:	ldr	x19, [sp, #16]
   11648:	ldp	x29, x30, [sp], #32
   1164c:	ret
   11650:	mov	w19, wzr
   11654:	b	11640 <__gmpf_cmp@@Base+0xe8>

0000000000011658 <__gmpf_cmp_d@@Base>:
   11658:	sub	sp, sp, #0x50
   1165c:	fmov	x8, d0
   11660:	mvn	x9, x8
   11664:	str	x19, [sp, #64]
   11668:	tst	x9, #0x7ff0000000000000
   1166c:	mov	x19, x0
   11670:	stp	x29, x30, [sp, #48]
   11674:	add	x29, sp, #0x30
   11678:	b.eq	116e0 <__gmpf_cmp_d@@Base+0x88>  // b.none
   1167c:	mov	w8, #0x1                   	// #1
   11680:	cbz	w8, 116d0 <__gmpf_cmp_d@@Base+0x78>
   11684:	fcmp	d0, #0.0
   11688:	b.ne	11694 <__gmpf_cmp_d@@Base+0x3c>  // b.any
   1168c:	ldr	w0, [x19, #4]
   11690:	b	116d0 <__gmpf_cmp_d@@Base+0x78>
   11694:	sub	x8, x29, #0x10
   11698:	mov	w9, #0xfffffffe            	// #-2
   1169c:	mov	w10, #0x2                   	// #2
   116a0:	fneg	d1, d0
   116a4:	str	x8, [sp, #24]
   116a8:	csel	w8, w10, w9, ge  // ge = tcont
   116ac:	fcsel	d0, d0, d1, ge  // ge = tcont
   116b0:	sub	x0, x29, #0x10
   116b4:	str	w8, [sp, #12]
   116b8:	bl	d460 <__gmp_extract_double@plt>
   116bc:	sxtw	x8, w0
   116c0:	add	x1, sp, #0x8
   116c4:	mov	x0, x19
   116c8:	str	x8, [sp, #16]
   116cc:	bl	c800 <__gmpf_cmp@plt>
   116d0:	ldr	x19, [sp, #64]
   116d4:	ldp	x29, x30, [sp, #48]
   116d8:	add	sp, sp, #0x50
   116dc:	ret
   116e0:	tst	x8, #0xfffffffffffff
   116e4:	b.ne	11700 <__gmpf_cmp_d@@Base+0xa8>  // b.any
   116e8:	fcmp	d0, #0.0
   116ec:	mov	w9, #0xffffffff            	// #-1
   116f0:	mov	w8, wzr
   116f4:	csinc	w0, w9, wzr, pl  // pl = nfrst
   116f8:	cbnz	w8, 11684 <__gmpf_cmp_d@@Base+0x2c>
   116fc:	b	116d0 <__gmpf_cmp_d@@Base+0x78>
   11700:	bl	c300 <__gmp_invalid_operation@plt>

0000000000011704 <__gmpf_cmp_ui@@Base>:
   11704:	ldrsw	x8, [x0, #4]
   11708:	tbnz	w8, #31, 11754 <__gmpf_cmp_ui@@Base+0x50>
   1170c:	cbz	x1, 1175c <__gmpf_cmp_ui@@Base+0x58>
   11710:	ldr	x9, [x0, #8]
   11714:	cmp	x9, #0x1
   11718:	b.ne	11768 <__gmpf_cmp_ui@@Base+0x64>  // b.any
   1171c:	ldr	x9, [x0, #16]
   11720:	sub	x8, x8, #0x1
   11724:	ldr	x10, [x9, x8, lsl #3]
   11728:	cmp	x10, x1
   1172c:	b.ne	11774 <__gmpf_cmp_ui@@Base+0x70>  // b.any
   11730:	ldr	x10, [x9]
   11734:	cbnz	x10, 11748 <__gmpf_cmp_ui@@Base+0x44>
   11738:	add	x9, x9, #0x8
   1173c:	ldr	x10, [x9], #8
   11740:	sub	x8, x8, #0x1
   11744:	cbz	x10, 1173c <__gmpf_cmp_ui@@Base+0x38>
   11748:	cmp	x8, #0x0
   1174c:	cset	w0, gt
   11750:	ret
   11754:	mov	w0, #0xffffffff            	// #-1
   11758:	ret
   1175c:	cmp	w8, #0x0
   11760:	cset	w0, ne  // ne = any
   11764:	ret
   11768:	mov	w8, #0xffffffff            	// #-1
   1176c:	cneg	w0, w8, ge  // ge = tcont
   11770:	ret
   11774:	mov	w8, #0xffffffff            	// #-1
   11778:	cneg	w0, w8, cs  // cs = hs, nlast
   1177c:	ret

0000000000011780 <__gmpf_cmp_si@@Base>:
   11780:	ldrsw	x10, [x0, #4]
   11784:	lsr	x9, x1, #63
   11788:	cmp	w9, w10, lsr #31
   1178c:	b.ne	117fc <__gmpf_cmp_si@@Base+0x7c>  // b.any
   11790:	cbz	w10, 1180c <__gmpf_cmp_si@@Base+0x8c>
   11794:	mov	x8, x0
   11798:	mov	w0, #0x1                   	// #1
   1179c:	cbz	x1, 117f8 <__gmpf_cmp_si@@Base+0x78>
   117a0:	ldr	x11, [x8, #8]
   117a4:	cmp	w10, #0x0
   117a8:	cneg	w9, w0, lt  // lt = tstop
   117ac:	cmp	x1, #0x0
   117b0:	cneg	x12, x1, mi  // mi = first
   117b4:	cmp	x11, #0x1
   117b8:	b.ne	11818 <__gmpf_cmp_si@@Base+0x98>  // b.any
   117bc:	ldr	x11, [x8, #16]
   117c0:	cmp	w10, #0x0
   117c4:	cneg	x8, x10, lt  // lt = tstop
   117c8:	sub	x8, x8, #0x1
   117cc:	ldr	x10, [x11, x8, lsl #3]
   117d0:	cmp	x10, x12
   117d4:	b.ne	11820 <__gmpf_cmp_si@@Base+0xa0>  // b.any
   117d8:	ldr	x10, [x11]
   117dc:	cbnz	x10, 117f0 <__gmpf_cmp_si@@Base+0x70>
   117e0:	add	x10, x11, #0x8
   117e4:	ldr	x11, [x10], #8
   117e8:	sub	x8, x8, #0x1
   117ec:	cbz	x11, 117e4 <__gmpf_cmp_si@@Base+0x64>
   117f0:	cmp	x8, #0x0
   117f4:	csel	w0, w9, wzr, gt
   117f8:	ret
   117fc:	cmp	w10, #0x0
   11800:	mov	w8, #0x1                   	// #1
   11804:	cneg	w0, w8, lt  // lt = tstop
   11808:	ret
   1180c:	cmp	x1, #0x0
   11810:	csetm	w0, ne  // ne = any
   11814:	ret
   11818:	cneg	w0, w9, lt  // lt = tstop
   1181c:	ret
   11820:	cneg	w0, w9, cc  // cc = lo, ul, last
   11824:	ret

0000000000011828 <__gmpf_mul_2exp@@Base>:
   11828:	stp	x29, x30, [sp, #-80]!
   1182c:	stp	x24, x23, [sp, #32]
   11830:	stp	x22, x21, [sp, #48]
   11834:	stp	x20, x19, [sp, #64]
   11838:	ldrsw	x24, [x1, #4]
   1183c:	mov	x19, x0
   11840:	str	x25, [sp, #16]
   11844:	mov	x29, sp
   11848:	cbz	w24, 11918 <__gmpf_mul_2exp@@Base+0xf0>
   1184c:	ldr	x21, [x19, #16]
   11850:	ldrsw	x22, [x19]
   11854:	ldp	x25, x1, [x1, #8]
   11858:	cmp	w24, #0x0
   1185c:	mov	x20, x2
   11860:	cneg	x23, x24, lt  // lt = tstop
   11864:	ands	x3, x2, #0x3f
   11868:	b.eq	11898 <__gmpf_mul_2exp@@Base+0x70>  // b.none
   1186c:	subs	x8, x23, x22
   11870:	b.le	118c8 <__gmpf_mul_2exp@@Base+0xa0>
   11874:	add	x1, x1, x8, lsl #3
   11878:	mov	w8, #0x40                  	// #64
   1187c:	add	x0, x21, #0x8
   11880:	sub	w3, w8, w3
   11884:	mov	x2, x22
   11888:	bl	c2f0 <__gmpn_rshift@plt>
   1188c:	str	x0, [x21]
   11890:	ldr	x0, [x21, x22, lsl #3]
   11894:	b	118dc <__gmpf_mul_2exp@@Base+0xb4>
   11898:	add	x8, x22, #0x1
   1189c:	subs	x8, x23, x8
   118a0:	add	x8, x1, x8, lsl #3
   118a4:	csel	x1, x8, x1, gt
   118a8:	csinc	x22, x23, x22, le
   118ac:	cmp	x21, x1
   118b0:	b.eq	118c0 <__gmpf_mul_2exp@@Base+0x98>  // b.none
   118b4:	mov	x0, x21
   118b8:	mov	x2, x22
   118bc:	bl	cc10 <__gmpn_copyi@plt>
   118c0:	add	x8, x25, x20, lsr #6
   118c4:	b	118ec <__gmpf_mul_2exp@@Base+0xc4>
   118c8:	mov	x0, x21
   118cc:	mov	x2, x23
   118d0:	bl	c2d0 <__gmpn_lshift@plt>
   118d4:	mov	x22, x23
   118d8:	str	x0, [x21, x23, lsl #3]
   118dc:	cmp	x0, #0x0
   118e0:	add	x8, x25, x20, lsr #6
   118e4:	cinc	x22, x22, ne  // ne = any
   118e8:	cinc	x8, x8, ne  // ne = any
   118ec:	str	x8, [x19, #8]
   118f0:	neg	w8, w22
   118f4:	cmp	w24, #0x0
   118f8:	csel	x8, x22, x8, ge  // ge = tcont
   118fc:	str	w8, [x19, #4]
   11900:	ldp	x20, x19, [sp, #64]
   11904:	ldp	x22, x21, [sp, #48]
   11908:	ldp	x24, x23, [sp, #32]
   1190c:	ldr	x25, [sp, #16]
   11910:	ldp	x29, x30, [sp], #80
   11914:	ret
   11918:	str	wzr, [x19, #4]
   1191c:	str	xzr, [x19, #8]
   11920:	b	11900 <__gmpf_mul_2exp@@Base+0xd8>

0000000000011924 <__gmpf_div_2exp@@Base>:
   11924:	stp	x29, x30, [sp, #-80]!
   11928:	stp	x24, x23, [sp, #32]
   1192c:	stp	x22, x21, [sp, #48]
   11930:	stp	x20, x19, [sp, #64]
   11934:	ldrsw	x24, [x1, #4]
   11938:	mov	x19, x0
   1193c:	str	x25, [sp, #16]
   11940:	mov	x29, sp
   11944:	cbz	w24, 11a20 <__gmpf_div_2exp@@Base+0xfc>
   11948:	ldr	x21, [x19, #16]
   1194c:	ldrsw	x22, [x19]
   11950:	ldp	x25, x1, [x1, #8]
   11954:	cmp	w24, #0x0
   11958:	mov	x20, x2
   1195c:	cneg	x23, x24, lt  // lt = tstop
   11960:	ands	x3, x2, #0x3f
   11964:	b.eq	1198c <__gmpf_div_2exp@@Base+0x68>  // b.none
   11968:	subs	x8, x23, x22
   1196c:	b.le	119bc <__gmpf_div_2exp@@Base+0x98>
   11970:	add	x1, x1, x8, lsl #3
   11974:	add	x0, x21, #0x8
   11978:	mov	x2, x22
   1197c:	bl	c2f0 <__gmpn_rshift@plt>
   11980:	str	x0, [x21]
   11984:	ldr	x0, [x21, x22, lsl #3]
   11988:	b	119d8 <__gmpf_div_2exp@@Base+0xb4>
   1198c:	add	x8, x22, #0x1
   11990:	subs	x8, x23, x8
   11994:	add	x8, x1, x8, lsl #3
   11998:	csel	x1, x8, x1, gt
   1199c:	csinc	x22, x23, x22, le
   119a0:	cmp	x21, x1
   119a4:	b.eq	119b4 <__gmpf_div_2exp@@Base+0x90>  // b.none
   119a8:	mov	x0, x21
   119ac:	mov	x2, x22
   119b0:	bl	cc10 <__gmpn_copyi@plt>
   119b4:	sub	x8, x25, x20, lsr #6
   119b8:	b	119f4 <__gmpf_div_2exp@@Base+0xd0>
   119bc:	mov	w8, #0x40                  	// #64
   119c0:	sub	w3, w8, w3
   119c4:	mov	x0, x21
   119c8:	mov	x2, x23
   119cc:	bl	c2d0 <__gmpn_lshift@plt>
   119d0:	mov	x22, x23
   119d4:	str	x0, [x21, x23, lsl #3]
   119d8:	lsr	x8, x20, #6
   119dc:	sub	x9, x25, x8
   119e0:	mvn	x8, x8
   119e4:	cmp	x0, #0x0
   119e8:	add	x8, x25, x8
   119ec:	cinc	x22, x22, ne  // ne = any
   119f0:	csel	x8, x8, x9, eq  // eq = none
   119f4:	str	x8, [x19, #8]
   119f8:	neg	w8, w22
   119fc:	cmp	w24, #0x0
   11a00:	csel	x8, x22, x8, ge  // ge = tcont
   11a04:	str	w8, [x19, #4]
   11a08:	ldp	x20, x19, [sp, #64]
   11a0c:	ldp	x22, x21, [sp, #48]
   11a10:	ldp	x24, x23, [sp, #32]
   11a14:	ldr	x25, [sp, #16]
   11a18:	ldp	x29, x30, [sp], #80
   11a1c:	ret
   11a20:	str	wzr, [x19, #4]
   11a24:	str	xzr, [x19, #8]
   11a28:	b	11a08 <__gmpf_div_2exp@@Base+0xe4>

0000000000011a2c <__gmpf_abs@@Base>:
   11a2c:	stp	x29, x30, [sp, #-48]!
   11a30:	stp	x20, x19, [sp, #32]
   11a34:	ldr	w8, [x1, #4]
   11a38:	mov	x19, x0
   11a3c:	str	x21, [sp, #16]
   11a40:	mov	x29, sp
   11a44:	cmp	w8, #0x0
   11a48:	cneg	w20, w8, mi  // mi = first
   11a4c:	cmp	x0, x1
   11a50:	b.eq	11a88 <__gmpf_abs@@Base+0x5c>  // b.none
   11a54:	ldrsw	x8, [x19]
   11a58:	ldr	x9, [x1, #16]
   11a5c:	ldr	x0, [x19, #16]
   11a60:	mov	x21, x1
   11a64:	add	x10, x8, #0x1
   11a68:	subs	x10, x20, x10
   11a6c:	add	x10, x9, x10, lsl #3
   11a70:	csinc	x20, x20, x8, le
   11a74:	csel	x1, x10, x9, gt
   11a78:	mov	x2, x20
   11a7c:	bl	cc10 <__gmpn_copyi@plt>
   11a80:	ldr	x8, [x21, #8]
   11a84:	str	x8, [x19, #8]
   11a88:	str	w20, [x19, #4]
   11a8c:	ldp	x20, x19, [sp, #32]
   11a90:	ldr	x21, [sp, #16]
   11a94:	ldp	x29, x30, [sp], #48
   11a98:	ret

0000000000011a9c <__gmpf_neg@@Base>:
   11a9c:	stp	x29, x30, [sp, #-48]!
   11aa0:	stp	x20, x19, [sp, #32]
   11aa4:	ldrsw	x8, [x1, #4]
   11aa8:	str	x21, [sp, #16]
   11aac:	mov	x19, x0
   11ab0:	cmp	x0, x1
   11ab4:	neg	x21, x8
   11ab8:	mov	x29, sp
   11abc:	b.eq	11b00 <__gmpf_neg@@Base+0x64>  // b.none
   11ac0:	ldrsw	x9, [x19]
   11ac4:	ldr	x10, [x1, #16]
   11ac8:	cmp	w8, #0x1
   11acc:	ldr	x0, [x19, #16]
   11ad0:	cneg	x11, x21, ge  // ge = tcont
   11ad4:	add	x12, x9, #0x1
   11ad8:	subs	x12, x11, x12
   11adc:	add	x12, x10, x12, lsl #3
   11ae0:	mov	x20, x1
   11ae4:	csinc	x2, x11, x9, le
   11ae8:	csel	x1, x12, x10, gt
   11aec:	cmp	w8, #0x1
   11af0:	cneg	x21, x2, ge  // ge = tcont
   11af4:	bl	cc10 <__gmpn_copyi@plt>
   11af8:	ldr	x8, [x20, #8]
   11afc:	str	x8, [x19, #8]
   11b00:	str	w21, [x19, #4]
   11b04:	ldp	x20, x19, [sp, #32]
   11b08:	ldr	x21, [sp, #16]
   11b0c:	ldp	x29, x30, [sp], #48
   11b10:	ret

0000000000011b14 <__gmpf_set_q@@Base>:
   11b14:	stp	x29, x30, [sp, #-96]!
   11b18:	stp	x28, x27, [sp, #16]
   11b1c:	stp	x26, x25, [sp, #32]
   11b20:	stp	x24, x23, [sp, #48]
   11b24:	stp	x22, x21, [sp, #64]
   11b28:	stp	x20, x19, [sp, #80]
   11b2c:	mov	x29, sp
   11b30:	sub	sp, sp, #0x20
   11b34:	ldrsw	x27, [x1, #4]
   11b38:	mov	x19, x0
   11b3c:	cbz	w27, 11be4 <__gmpf_set_q@@Base+0xd0>
   11b40:	ldrsw	x20, [x1, #20]
   11b44:	ldr	x8, [x19, #16]
   11b48:	ldrsw	x22, [x19]
   11b4c:	cmp	w27, #0x0
   11b50:	cneg	x24, x27, lt  // lt = tstop
   11b54:	stur	x8, [x29, #-24]
   11b58:	sub	x8, x24, x20
   11b5c:	add	x8, x8, #0x1
   11b60:	add	x9, x22, #0x1
   11b64:	sub	x28, x9, x8
   11b68:	ldr	x25, [x1, #8]
   11b6c:	ldr	x3, [x1, #24]
   11b70:	add	x23, x28, x24
   11b74:	stp	x8, xzr, [x29, #-16]
   11b78:	lsl	x8, x23, #3
   11b7c:	add	x1, x8, #0x8
   11b80:	mov	w8, #0x7f00                	// #32512
   11b84:	cmp	x1, x8
   11b88:	stur	x9, [x29, #-32]
   11b8c:	b.hi	11bf0 <__gmpf_set_q@@Base+0xdc>  // b.pmore
   11b90:	add	x9, x1, #0xf
   11b94:	mov	x8, sp
   11b98:	and	x9, x9, #0xfffffffffffffff0
   11b9c:	sub	x26, x8, x9
   11ba0:	mov	sp, x26
   11ba4:	cmp	x28, #0x1
   11ba8:	b.lt	11c0c <__gmpf_set_q@@Base+0xf8>  // b.tstop
   11bac:	add	x8, x20, x22
   11bb0:	sub	x8, x8, x24
   11bb4:	lsl	x2, x8, #3
   11bb8:	mov	x0, x26
   11bbc:	mov	w1, wzr
   11bc0:	mov	x21, x3
   11bc4:	bl	c780 <memset@plt>
   11bc8:	add	x0, x26, x28, lsl #3
   11bcc:	mov	x1, x25
   11bd0:	mov	x2, x24
   11bd4:	bl	cc10 <__gmpn_copyi@plt>
   11bd8:	mov	x3, x21
   11bdc:	mov	x1, x26
   11be0:	b	11c10 <__gmpf_set_q@@Base+0xfc>
   11be4:	str	wzr, [x19, #4]
   11be8:	str	xzr, [x19, #8]
   11bec:	b	11c5c <__gmpf_set_q@@Base+0x148>
   11bf0:	sub	x0, x29, #0x8
   11bf4:	mov	x21, x3
   11bf8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11bfc:	mov	x3, x21
   11c00:	mov	x26, x0
   11c04:	cmp	x28, #0x1
   11c08:	b.ge	11bac <__gmpf_set_q@@Base+0x98>  // b.tcont
   11c0c:	sub	x1, x25, x28, lsl #3
   11c10:	ldur	x21, [x29, #-24]
   11c14:	mov	x2, x23
   11c18:	mov	x4, x20
   11c1c:	mov	x5, x26
   11c20:	mov	x0, x21
   11c24:	bl	c480 <__gmpn_div_q@plt>
   11c28:	ldr	x8, [x21, x22, lsl #3]
   11c2c:	ldur	x9, [x29, #-32]
   11c30:	ldp	x10, x0, [x29, #-16]
   11c34:	cmp	x8, #0x0
   11c38:	cset	w8, eq  // eq = none
   11c3c:	sub	x9, x9, x8
   11c40:	sub	x8, x10, x8
   11c44:	str	x8, [x19, #8]
   11c48:	neg	w8, w9
   11c4c:	cmp	w27, #0x0
   11c50:	csel	x8, x9, x8, ge  // ge = tcont
   11c54:	str	w8, [x19, #4]
   11c58:	cbnz	x0, 11c7c <__gmpf_set_q@@Base+0x168>
   11c5c:	mov	sp, x29
   11c60:	ldp	x20, x19, [sp, #80]
   11c64:	ldp	x22, x21, [sp, #64]
   11c68:	ldp	x24, x23, [sp, #48]
   11c6c:	ldp	x26, x25, [sp, #32]
   11c70:	ldp	x28, x27, [sp, #16]
   11c74:	ldp	x29, x30, [sp], #96
   11c78:	ret
   11c7c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   11c80:	b	11c5c <__gmpf_set_q@@Base+0x148>

0000000000011c84 <__gmpf_get_d@@Base>:
   11c84:	stp	x29, x30, [sp, #-16]!
   11c88:	ldrsw	x2, [x0, #4]
   11c8c:	mov	x29, sp
   11c90:	cbz	w2, 11cb4 <__gmpf_get_d@@Base+0x30>
   11c94:	ldp	x8, x0, [x0, #8]
   11c98:	cmp	x2, #0x0
   11c9c:	cneg	x1, x2, mi  // mi = first
   11ca0:	sub	x8, x8, x1
   11ca4:	lsl	x3, x8, #6
   11ca8:	bl	c070 <__gmpn_get_d@plt>
   11cac:	ldp	x29, x30, [sp], #16
   11cb0:	ret
   11cb4:	fmov	d0, xzr
   11cb8:	ldp	x29, x30, [sp], #16
   11cbc:	ret

0000000000011cc0 <__gmpf_get_d_2exp@@Base>:
   11cc0:	stp	x29, x30, [sp, #-16]!
   11cc4:	ldrsw	x2, [x1, #4]
   11cc8:	mov	x29, sp
   11ccc:	cbz	w2, 11d08 <__gmpf_get_d_2exp@@Base+0x48>
   11cd0:	ldp	x9, x8, [x1, #8]
   11cd4:	cmp	x2, #0x0
   11cd8:	cneg	x1, x2, mi  // mi = first
   11cdc:	add	x10, x8, x1, lsl #3
   11ce0:	ldur	x10, [x10, #-8]
   11ce4:	lsl	x9, x9, #6
   11ce8:	clz	x10, x10
   11cec:	sub	x9, x9, x10
   11cf0:	str	x9, [x0]
   11cf4:	sub	x3, x10, x1, lsl #6
   11cf8:	mov	x0, x8
   11cfc:	bl	c070 <__gmpn_get_d@plt>
   11d00:	ldp	x29, x30, [sp], #16
   11d04:	ret
   11d08:	str	xzr, [x0]
   11d0c:	fmov	d0, xzr
   11d10:	ldp	x29, x30, [sp], #16
   11d14:	ret

0000000000011d18 <__gmpf_set_default_prec@@Base>:
   11d18:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   11d1c:	cmp	x0, #0x35
   11d20:	mov	w8, #0x35                  	// #53
   11d24:	ldr	x9, [x9, #3960]
   11d28:	csel	x8, x0, x8, hi  // hi = pmore
   11d2c:	add	x8, x8, #0x7f
   11d30:	lsr	x8, x8, #6
   11d34:	str	x8, [x9]
   11d38:	ret

0000000000011d3c <__gmpf_set_prec@@Base>:
   11d3c:	stp	x29, x30, [sp, #-48]!
   11d40:	stp	x22, x21, [sp, #16]
   11d44:	stp	x20, x19, [sp, #32]
   11d48:	cmp	x1, #0x35
   11d4c:	mov	w8, #0x35                  	// #53
   11d50:	ldrsw	x22, [x0]
   11d54:	csel	x8, x1, x8, hi  // hi = pmore
   11d58:	add	x8, x8, #0x7f
   11d5c:	lsr	x8, x8, #6
   11d60:	cmp	x8, x22
   11d64:	mov	x29, sp
   11d68:	b.eq	11dd8 <__gmpf_set_prec@@Base+0x9c>  // b.none
   11d6c:	ldrsw	x10, [x0, #4]
   11d70:	ldr	x21, [x0, #16]
   11d74:	add	x20, x8, #0x1
   11d78:	mov	x19, x0
   11d7c:	cmp	x10, #0x0
   11d80:	cneg	x9, x10, mi  // mi = first
   11d84:	cmp	x9, x20
   11d88:	str	w8, [x0]
   11d8c:	b.le	11db4 <__gmpf_set_prec@@Base+0x78>
   11d90:	mvn	x11, x8
   11d94:	cmp	w10, #0x0
   11d98:	add	x9, x21, x9, lsl #3
   11d9c:	csinv	x8, x20, x8, ge  // ge = tcont
   11da0:	add	x1, x9, x11, lsl #3
   11da4:	mov	x0, x21
   11da8:	mov	x2, x20
   11dac:	str	w8, [x19, #4]
   11db0:	bl	cc10 <__gmpn_copyi@plt>
   11db4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   11db8:	ldr	x8, [x8, #3792]
   11dbc:	lsl	x9, x22, #3
   11dc0:	add	x1, x9, #0x8
   11dc4:	lsl	x2, x20, #3
   11dc8:	ldr	x8, [x8]
   11dcc:	mov	x0, x21
   11dd0:	blr	x8
   11dd4:	str	x0, [x19, #16]
   11dd8:	ldp	x20, x19, [sp, #32]
   11ddc:	ldp	x22, x21, [sp, #16]
   11de0:	ldp	x29, x30, [sp], #48
   11de4:	ret

0000000000011de8 <__gmpf_set_prec_raw@@Base>:
   11de8:	cmp	x1, #0x35
   11dec:	mov	w8, #0x35                  	// #53
   11df0:	csel	x8, x1, x8, hi  // hi = pmore
   11df4:	add	x8, x8, #0x7f
   11df8:	lsr	x8, x8, #6
   11dfc:	str	w8, [x0]
   11e00:	ret

0000000000011e04 <__gmpf_get_default_prec@@Base>:
   11e04:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   11e08:	ldr	x8, [x8, #3960]
   11e0c:	ldr	x8, [x8]
   11e10:	lsl	x8, x8, #6
   11e14:	sub	x0, x8, #0x40
   11e18:	ret

0000000000011e1c <__gmpf_get_prec@@Base>:
   11e1c:	ldrsw	x8, [x0]
   11e20:	lsl	x8, x8, #6
   11e24:	sub	x0, x8, #0x40
   11e28:	ret

0000000000011e2c <__gmpf_ui_div@@Base>:
   11e2c:	stp	x29, x30, [sp, #-96]!
   11e30:	stp	x28, x27, [sp, #16]
   11e34:	stp	x26, x25, [sp, #32]
   11e38:	stp	x24, x23, [sp, #48]
   11e3c:	stp	x22, x21, [sp, #64]
   11e40:	stp	x20, x19, [sp, #80]
   11e44:	mov	x29, sp
   11e48:	sub	sp, sp, #0x30
   11e4c:	ldrsw	x28, [x2, #4]
   11e50:	cbz	w28, 11fc0 <__gmpf_ui_div@@Base+0x194>
   11e54:	mov	x11, x1
   11e58:	mov	x19, x0
   11e5c:	cbz	x1, 11f98 <__gmpf_ui_div@@Base+0x16c>
   11e60:	ldrsw	x12, [x19]
   11e64:	ldr	x21, [x19, #16]
   11e68:	ldp	x27, x23, [x2, #8]
   11e6c:	cmp	w28, #0x0
   11e70:	cneg	x22, x28, lt  // lt = tstop
   11e74:	add	x10, x12, #0x1
   11e78:	add	x26, x22, x10
   11e7c:	cmp	x21, x23
   11e80:	sub	x24, x26, #0x1
   11e84:	csel	x8, x22, xzr, eq  // eq = none
   11e88:	add	x9, x24, x22
   11e8c:	add	x8, x9, x8
   11e90:	lsl	x1, x8, #3
   11e94:	mov	w8, #0x7f00                	// #32512
   11e98:	cmp	x1, x8
   11e9c:	mov	w13, #0x2                   	// #2
   11ea0:	stp	x10, xzr, [x29, #-16]
   11ea4:	stp	x12, x11, [x29, #-32]
   11ea8:	b.hi	11fa4 <__gmpf_ui_div@@Base+0x178>  // b.pmore
   11eac:	add	x9, x1, #0xf
   11eb0:	mov	x8, sp
   11eb4:	and	x9, x9, #0xfffffffffffffff0
   11eb8:	sub	x25, x8, x9
   11ebc:	mov	sp, x25
   11ec0:	sub	x20, x26, #0x2
   11ec4:	cmp	x21, x23
   11ec8:	add	x26, x25, x22, lsl #3
   11ecc:	b.ne	11ef4 <__gmpf_ui_div@@Base+0xc8>  // b.any
   11ed0:	stur	x27, [x29, #-40]
   11ed4:	add	x27, x26, x24, lsl #3
   11ed8:	mov	x0, x27
   11edc:	mov	x1, x23
   11ee0:	mov	x2, x22
   11ee4:	bl	cc10 <__gmpn_copyi@plt>
   11ee8:	mov	x23, x27
   11eec:	ldur	x27, [x29, #-40]
   11ef0:	mov	w13, #0x2                   	// #2
   11ef4:	sub	x8, x13, x27
   11ef8:	ldur	x27, [x29, #-32]
   11efc:	stur	x8, [x29, #-40]
   11f00:	cbz	x20, 11f1c <__gmpf_ui_div@@Base+0xf0>
   11f04:	add	x8, x22, x27
   11f08:	lsl	x8, x8, #3
   11f0c:	add	x0, x25, x22, lsl #3
   11f10:	sub	x2, x8, #0x8
   11f14:	mov	w1, wzr
   11f18:	bl	c780 <memset@plt>
   11f1c:	ldur	x8, [x29, #-24]
   11f20:	mov	x0, x21
   11f24:	mov	x1, x25
   11f28:	mov	x2, xzr
   11f2c:	mov	x3, x26
   11f30:	mov	x4, x24
   11f34:	mov	x5, x23
   11f38:	mov	x6, x22
   11f3c:	str	x8, [x26, x20, lsl #3]
   11f40:	bl	c030 <__gmpn_tdiv_qr@plt>
   11f44:	ldr	x8, [x21, x27, lsl #3]
   11f48:	ldp	x9, x0, [x29, #-16]
   11f4c:	ldur	x10, [x29, #-40]
   11f50:	cmp	x8, #0x0
   11f54:	cset	w8, eq  // eq = none
   11f58:	sub	x9, x9, x8
   11f5c:	cmp	w28, #0x0
   11f60:	sub	x8, x10, x8
   11f64:	neg	w10, w9
   11f68:	csel	x9, x9, x10, ge  // ge = tcont
   11f6c:	str	w9, [x19, #4]
   11f70:	str	x8, [x19, #8]
   11f74:	cbnz	x0, 11fb8 <__gmpf_ui_div@@Base+0x18c>
   11f78:	mov	sp, x29
   11f7c:	ldp	x20, x19, [sp, #80]
   11f80:	ldp	x22, x21, [sp, #64]
   11f84:	ldp	x24, x23, [sp, #48]
   11f88:	ldp	x26, x25, [sp, #32]
   11f8c:	ldp	x28, x27, [sp, #16]
   11f90:	ldp	x29, x30, [sp], #96
   11f94:	ret
   11f98:	str	wzr, [x19, #4]
   11f9c:	str	xzr, [x19, #8]
   11fa0:	b	11f78 <__gmpf_ui_div@@Base+0x14c>
   11fa4:	sub	x0, x29, #0x8
   11fa8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   11fac:	mov	w13, #0x2                   	// #2
   11fb0:	mov	x25, x0
   11fb4:	b	11ec0 <__gmpf_ui_div@@Base+0x94>
   11fb8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   11fbc:	b	11f78 <__gmpf_ui_div@@Base+0x14c>
   11fc0:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000011fc4 <__gmpf_sqrt_ui@@Base>:
   11fc4:	stp	x29, x30, [sp, #-64]!
   11fc8:	stp	x24, x23, [sp, #16]
   11fcc:	stp	x22, x21, [sp, #32]
   11fd0:	stp	x20, x19, [sp, #48]
   11fd4:	mov	x29, sp
   11fd8:	sub	sp, sp, #0x10
   11fdc:	mov	x20, x1
   11fe0:	cmp	x1, #0x1
   11fe4:	mov	x19, x0
   11fe8:	b.ls	12084 <__gmpf_sqrt_ui@@Base+0xc0>  // b.plast
   11fec:	stur	xzr, [x29, #-8]
   11ff0:	ldr	w23, [x19]
   11ff4:	mov	w9, #0x7f00                	// #32512
   11ff8:	sbfiz	x8, x23, #1, #32
   11ffc:	sub	x21, x8, #0x1
   12000:	lsl	x1, x21, #3
   12004:	cmp	x1, x9
   12008:	sub	x24, x8, #0x2
   1200c:	b.hi	12098 <__gmpf_sqrt_ui@@Base+0xd4>  // b.pmore
   12010:	add	x9, x1, #0xf
   12014:	mov	x8, sp
   12018:	and	x9, x9, #0xfffffffffffffff0
   1201c:	sub	x22, x8, x9
   12020:	mov	sp, x22
   12024:	cbz	x24, 12040 <__gmpf_sqrt_ui@@Base+0x7c>
   12028:	sxtw	x8, w23
   1202c:	lsl	x8, x8, #4
   12030:	sub	x2, x8, #0x10
   12034:	mov	x0, x22
   12038:	mov	w1, wzr
   1203c:	bl	c780 <memset@plt>
   12040:	str	x20, [x22, x24, lsl #3]
   12044:	ldr	x0, [x19, #16]
   12048:	mov	x1, xzr
   1204c:	mov	x2, x22
   12050:	mov	x3, x21
   12054:	bl	d590 <__gmpn_sqrtrem@plt>
   12058:	mov	w8, #0x1                   	// #1
   1205c:	str	w23, [x19, #4]
   12060:	str	x8, [x19, #8]
   12064:	ldur	x0, [x29, #-8]
   12068:	cbnz	x0, 120ac <__gmpf_sqrt_ui@@Base+0xe8>
   1206c:	mov	sp, x29
   12070:	ldp	x20, x19, [sp, #48]
   12074:	ldp	x22, x21, [sp, #32]
   12078:	ldp	x24, x23, [sp, #16]
   1207c:	ldp	x29, x30, [sp], #64
   12080:	ret
   12084:	ldr	x8, [x19, #16]
   12088:	str	x20, [x19, #8]
   1208c:	str	w20, [x19, #4]
   12090:	str	x20, [x8]
   12094:	b	1206c <__gmpf_sqrt_ui@@Base+0xa8>
   12098:	sub	x0, x29, #0x8
   1209c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   120a0:	mov	x22, x0
   120a4:	cbnz	x24, 12028 <__gmpf_sqrt_ui@@Base+0x64>
   120a8:	b	12040 <__gmpf_sqrt_ui@@Base+0x7c>
   120ac:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   120b0:	b	1206c <__gmpf_sqrt_ui@@Base+0xa8>

00000000000120b4 <__gmpf_ceil@@Base>:
   120b4:	stp	x29, x30, [sp, #-16]!
   120b8:	mov	w2, #0x1                   	// #1
   120bc:	mov	x29, sp
   120c0:	bl	120cc <__gmpf_ceil@@Base+0x18>
   120c4:	ldp	x29, x30, [sp], #16
   120c8:	ret
   120cc:	stp	x29, x30, [sp, #-48]!
   120d0:	stp	x22, x21, [sp, #16]
   120d4:	stp	x20, x19, [sp, #32]
   120d8:	ldrsw	x22, [x1, #4]
   120dc:	mov	x19, x0
   120e0:	mov	x29, sp
   120e4:	cbz	w22, 12194 <__gmpf_ceil@@Base+0xe0>
   120e8:	ldr	x9, [x1, #8]
   120ec:	ldr	x20, [x19, #16]
   120f0:	cmp	x9, #0x0
   120f4:	b.le	12178 <__gmpf_ceil@@Base+0xc4>
   120f8:	ldrsw	x11, [x19]
   120fc:	str	x9, [x19, #8]
   12100:	cmp	w22, #0x0
   12104:	ldr	x8, [x1, #16]
   12108:	cneg	x10, x22, lt  // lt = tstop
   1210c:	cmp	x10, x9
   12110:	csel	x9, x10, x9, lt  // lt = tstop
   12114:	add	x12, x11, #0x1
   12118:	cmp	x9, x12
   1211c:	add	x13, x8, x10, lsl #3
   12120:	csinc	x21, x9, x11, lt  // lt = tstop
   12124:	eor	w9, w22, w2
   12128:	sub	x1, x13, x21, lsl #3
   1212c:	tbnz	w9, #31, 12150 <__gmpf_ceil@@Base+0x9c>
   12130:	cmp	x8, x1
   12134:	b.eq	12150 <__gmpf_ceil@@Base+0x9c>  // b.none
   12138:	lsl	x9, x10, #3
   1213c:	sub	x9, x9, x21, lsl #3
   12140:	ldr	x10, [x8], #8
   12144:	cbnz	x10, 121ac <__gmpf_ceil@@Base+0xf8>
   12148:	subs	x9, x9, #0x8
   1214c:	b.ne	12140 <__gmpf_ceil@@Base+0x8c>  // b.any
   12150:	neg	x8, x21
   12154:	cmp	w22, #0x0
   12158:	csel	x8, x21, x8, ge  // ge = tcont
   1215c:	cmp	x20, x1
   12160:	str	w8, [x19, #4]
   12164:	b.eq	1219c <__gmpf_ceil@@Base+0xe8>  // b.none
   12168:	mov	x0, x20
   1216c:	mov	x2, x21
   12170:	bl	cc10 <__gmpn_copyi@plt>
   12174:	b	1219c <__gmpf_ceil@@Base+0xe8>
   12178:	eor	w8, w22, w2
   1217c:	tbnz	w8, #31, 12194 <__gmpf_ceil@@Base+0xe0>
   12180:	mov	w8, #0x1                   	// #1
   12184:	str	x8, [x20]
   12188:	str	x8, [x19, #8]
   1218c:	str	w2, [x19, #4]
   12190:	b	1219c <__gmpf_ceil@@Base+0xe8>
   12194:	str	wzr, [x19, #4]
   12198:	str	xzr, [x19, #8]
   1219c:	ldp	x20, x19, [sp, #32]
   121a0:	ldp	x22, x21, [sp, #16]
   121a4:	ldp	x29, x30, [sp], #48
   121a8:	ret
   121ac:	mov	w3, #0x1                   	// #1
   121b0:	mov	x0, x20
   121b4:	mov	x2, x21
   121b8:	bl	c150 <__gmpn_add_1@plt>
   121bc:	cbz	x0, 121d4 <__gmpf_ceil@@Base+0x120>
   121c0:	mov	w21, #0x1                   	// #1
   121c4:	str	x21, [x20]
   121c8:	ldr	x8, [x19, #8]
   121cc:	add	x8, x8, #0x1
   121d0:	str	x8, [x19, #8]
   121d4:	neg	w8, w21
   121d8:	cmp	w22, #0x0
   121dc:	csel	x8, x21, x8, ge  // ge = tcont
   121e0:	str	w8, [x19, #4]
   121e4:	b	1219c <__gmpf_ceil@@Base+0xe8>

00000000000121e8 <__gmpf_floor@@Base>:
   121e8:	stp	x29, x30, [sp, #-16]!
   121ec:	mov	w2, #0xffffffff            	// #-1
   121f0:	mov	x29, sp
   121f4:	bl	120cc <__gmpf_ceil@@Base+0x18>
   121f8:	ldp	x29, x30, [sp], #16
   121fc:	ret

0000000000012200 <__gmpf_trunc@@Base>:
   12200:	stp	x29, x30, [sp, #-16]!
   12204:	ldr	x9, [x1, #8]
   12208:	mov	x29, sp
   1220c:	cmp	x9, #0x1
   12210:	b.lt	1227c <__gmpf_trunc@@Base+0x7c>  // b.tstop
   12214:	ldr	w8, [x1, #4]
   12218:	cbz	w8, 1227c <__gmpf_trunc@@Base+0x7c>
   1221c:	sxtw	x10, w8
   12220:	ldrsw	x11, [x0]
   12224:	cmp	w10, #0x0
   12228:	ldr	x12, [x1, #16]
   1222c:	cneg	x13, x10, lt  // lt = tstop
   12230:	cmp	x13, x9
   12234:	str	x9, [x0, #8]
   12238:	ldr	x8, [x0, #16]
   1223c:	csel	x9, x13, x9, lt  // lt = tstop
   12240:	add	x14, x11, #0x1
   12244:	cmp	x9, x14
   12248:	add	x12, x12, x13, lsl #3
   1224c:	csinc	x2, x9, x11, lt  // lt = tstop
   12250:	cmp	w10, #0x0
   12254:	neg	w9, w2
   12258:	sub	x1, x12, x2, lsl #3
   1225c:	csel	x9, x2, x9, ge  // ge = tcont
   12260:	cmp	x8, x1
   12264:	str	w9, [x0, #4]
   12268:	b.eq	12274 <__gmpf_trunc@@Base+0x74>  // b.none
   1226c:	mov	x0, x8
   12270:	bl	cc10 <__gmpn_copyi@plt>
   12274:	ldp	x29, x30, [sp], #16
   12278:	ret
   1227c:	str	wzr, [x0, #4]
   12280:	str	xzr, [x0, #8]
   12284:	ldp	x29, x30, [sp], #16
   12288:	ret

000000000001228c <__gmpf_pow_ui@@Base>:
   1228c:	sub	sp, sp, #0x60
   12290:	stp	x22, x21, [sp, #64]
   12294:	stp	x20, x19, [sp, #80]
   12298:	mov	x21, x2
   1229c:	mov	x20, x1
   122a0:	cmp	x2, #0x1
   122a4:	mov	x19, x0
   122a8:	stp	x29, x30, [sp, #32]
   122ac:	str	x23, [sp, #48]
   122b0:	add	x29, sp, #0x20
   122b4:	b.hi	122cc <__gmpf_pow_ui@@Base+0x40>  // b.pmore
   122b8:	cbz	x21, 1238c <__gmpf_pow_ui@@Base+0x100>
   122bc:	mov	x0, x19
   122c0:	mov	x1, x20
   122c4:	bl	c2a0 <__gmpf_set@plt>
   122c8:	b	12398 <__gmpf_pow_ui@@Base+0x10c>
   122cc:	mov	x0, x19
   122d0:	clz	x23, x21
   122d4:	bl	c4b0 <__gmpf_get_prec@plt>
   122d8:	sub	x8, x0, x23
   122dc:	add	x1, x8, #0x3f
   122e0:	add	x0, sp, #0x8
   122e4:	bl	d080 <__gmpf_init2@plt>
   122e8:	add	x0, sp, #0x8
   122ec:	mov	x1, x20
   122f0:	bl	c2a0 <__gmpf_set@plt>
   122f4:	cmp	w23, #0x3d
   122f8:	b.hi	1234c <__gmpf_pow_ui@@Base+0xc0>  // b.pmore
   122fc:	mov	w8, #0x3e                  	// #62
   12300:	mov	w9, #0x3f                  	// #63
   12304:	sub	w22, w8, w23
   12308:	sub	w23, w9, w23
   1230c:	b	12320 <__gmpf_pow_ui@@Base+0x94>
   12310:	sub	w23, w23, #0x1
   12314:	cmp	w23, #0x1
   12318:	sub	x22, x22, #0x1
   1231c:	b.le	1234c <__gmpf_pow_ui@@Base+0xc0>
   12320:	add	x0, sp, #0x8
   12324:	add	x1, sp, #0x8
   12328:	add	x2, sp, #0x8
   1232c:	bl	cf00 <__gmpf_mul@plt>
   12330:	lsr	x8, x21, x22
   12334:	tbz	w8, #0, 12310 <__gmpf_pow_ui@@Base+0x84>
   12338:	add	x0, sp, #0x8
   1233c:	add	x1, sp, #0x8
   12340:	mov	x2, x20
   12344:	bl	cf00 <__gmpf_mul@plt>
   12348:	b	12310 <__gmpf_pow_ui@@Base+0x84>
   1234c:	tbnz	w21, #0, 12360 <__gmpf_pow_ui@@Base+0xd4>
   12350:	add	x1, sp, #0x8
   12354:	add	x2, sp, #0x8
   12358:	mov	x0, x19
   1235c:	b	1237c <__gmpf_pow_ui@@Base+0xf0>
   12360:	add	x0, sp, #0x8
   12364:	add	x1, sp, #0x8
   12368:	add	x2, sp, #0x8
   1236c:	bl	cf00 <__gmpf_mul@plt>
   12370:	add	x1, sp, #0x8
   12374:	mov	x0, x19
   12378:	mov	x2, x20
   1237c:	bl	cf00 <__gmpf_mul@plt>
   12380:	add	x0, sp, #0x8
   12384:	bl	c490 <__gmpf_clear@plt>
   12388:	b	12398 <__gmpf_pow_ui@@Base+0x10c>
   1238c:	mov	w1, #0x1                   	// #1
   12390:	mov	x0, x19
   12394:	bl	c830 <__gmpf_set_ui@plt>
   12398:	ldp	x20, x19, [sp, #80]
   1239c:	ldp	x22, x21, [sp, #64]
   123a0:	ldr	x23, [sp, #48]
   123a4:	ldp	x29, x30, [sp, #32]
   123a8:	add	sp, sp, #0x60
   123ac:	ret

00000000000123b0 <__gmpf_urandomb@@Base>:
   123b0:	stp	x29, x30, [sp, #-48]!
   123b4:	stp	x22, x21, [sp, #16]
   123b8:	stp	x20, x19, [sp, #32]
   123bc:	ldrsw	x8, [x0]
   123c0:	add	x9, x2, #0x3f
   123c4:	ldr	x10, [x1, #24]
   123c8:	lsr	x9, x9, #6
   123cc:	add	x11, x8, #0x1
   123d0:	cmp	x9, x11
   123d4:	cset	w12, gt
   123d8:	cmp	x9, #0x0
   123dc:	cset	w13, eq  // eq = none
   123e0:	ldr	x21, [x0, #16]
   123e4:	orr	w12, w13, w12
   123e8:	ldr	x10, [x10, #8]
   123ec:	lsl	x11, x11, #6
   123f0:	cmp	w12, #0x0
   123f4:	csel	x22, x11, x2, ne  // ne = any
   123f8:	mov	x19, x0
   123fc:	mov	x0, x1
   12400:	mov	x1, x21
   12404:	mov	x2, x22
   12408:	mov	x29, sp
   1240c:	csinc	x20, x9, x8, eq  // eq = none
   12410:	blr	x10
   12414:	ands	x8, x22, #0x3f
   12418:	b.eq	12434 <__gmpf_urandomb@@Base+0x84>  // b.none
   1241c:	mov	w9, #0x40                  	// #64
   12420:	sub	w3, w9, w8
   12424:	mov	x0, x21
   12428:	mov	x1, x21
   1242c:	mov	x2, x20
   12430:	bl	c2d0 <__gmpn_lshift@plt>
   12434:	cbz	x20, 12464 <__gmpf_urandomb@@Base+0xb4>
   12438:	add	x10, x21, x20, lsl #3
   1243c:	mov	x9, xzr
   12440:	neg	x8, x20
   12444:	sub	x10, x10, #0x8
   12448:	ldr	x11, [x10, x9, lsl #3]
   1244c:	cbnz	x11, 1246c <__gmpf_urandomb@@Base+0xbc>
   12450:	sub	x9, x9, #0x1
   12454:	cmn	x20, x9
   12458:	b.ne	12448 <__gmpf_urandomb@@Base+0x98>  // b.any
   1245c:	mov	x20, xzr
   12460:	b	12474 <__gmpf_urandomb@@Base+0xc4>
   12464:	mov	x8, xzr
   12468:	b	12474 <__gmpf_urandomb@@Base+0xc4>
   1246c:	add	x20, x20, x9
   12470:	mov	x8, x9
   12474:	str	x8, [x19, #8]
   12478:	str	w20, [x19, #4]
   1247c:	ldp	x20, x19, [sp, #32]
   12480:	ldp	x22, x21, [sp, #16]
   12484:	ldp	x29, x30, [sp], #48
   12488:	ret

000000000001248c <__gmpf_swap@@Base>:
   1248c:	ldr	w8, [x1]
   12490:	ldr	w9, [x0]
   12494:	str	w8, [x0]
   12498:	str	w9, [x1]
   1249c:	ldr	w8, [x1, #4]
   124a0:	ldr	w9, [x0, #4]
   124a4:	str	w8, [x0, #4]
   124a8:	str	w9, [x1, #4]
   124ac:	ldr	x8, [x1, #8]
   124b0:	ldr	x9, [x0, #8]
   124b4:	str	x8, [x0, #8]
   124b8:	str	x9, [x1, #8]
   124bc:	ldr	x8, [x1, #16]
   124c0:	ldr	x9, [x0, #16]
   124c4:	str	x8, [x0, #16]
   124c8:	str	x9, [x1, #16]
   124cc:	ret

00000000000124d0 <__gmpf_fits_sint_p@@Base>:
   124d0:	ldr	x8, [x0, #8]
   124d4:	cmp	x8, #0x1
   124d8:	b.lt	12510 <__gmpf_fits_sint_p@@Base+0x40>  // b.tstop
   124dc:	cmp	x8, #0x1
   124e0:	b.ne	12518 <__gmpf_fits_sint_p@@Base+0x48>  // b.any
   124e4:	ldrsw	x8, [x0, #4]
   124e8:	ldr	x9, [x0, #16]
   124ec:	cmp	w8, #0x0
   124f0:	cneg	x8, x8, lt  // lt = tstop
   124f4:	add	x8, x9, x8, lsl #3
   124f8:	ldur	x8, [x8, #-8]
   124fc:	mov	w9, #0x7fffffff            	// #2147483647
   12500:	cinc	x9, x9, lt  // lt = tstop
   12504:	cmp	x8, x9
   12508:	cset	w0, ls  // ls = plast
   1250c:	ret
   12510:	mov	w0, #0x1                   	// #1
   12514:	ret
   12518:	mov	w0, wzr
   1251c:	ret

0000000000012520 <__gmpf_fits_slong_p@@Base>:
   12520:	ldr	x8, [x0, #8]
   12524:	cmp	x8, #0x1
   12528:	b.lt	12560 <__gmpf_fits_slong_p@@Base+0x40>  // b.tstop
   1252c:	cmp	x8, #0x1
   12530:	b.ne	12568 <__gmpf_fits_slong_p@@Base+0x48>  // b.any
   12534:	ldrsw	x8, [x0, #4]
   12538:	ldr	x9, [x0, #16]
   1253c:	cmp	w8, #0x0
   12540:	cneg	x8, x8, lt  // lt = tstop
   12544:	add	x8, x9, x8, lsl #3
   12548:	ldur	x8, [x8, #-8]
   1254c:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   12550:	cinv	x9, x9, lt  // lt = tstop
   12554:	cmp	x8, x9
   12558:	cset	w0, ls  // ls = plast
   1255c:	ret
   12560:	mov	w0, #0x1                   	// #1
   12564:	ret
   12568:	mov	w0, wzr
   1256c:	ret

0000000000012570 <__gmpf_fits_sshort_p@@Base>:
   12570:	ldr	x8, [x0, #8]
   12574:	cmp	x8, #0x1
   12578:	b.lt	125b0 <__gmpf_fits_sshort_p@@Base+0x40>  // b.tstop
   1257c:	cmp	x8, #0x1
   12580:	b.ne	125b8 <__gmpf_fits_sshort_p@@Base+0x48>  // b.any
   12584:	ldrsw	x8, [x0, #4]
   12588:	ldr	x9, [x0, #16]
   1258c:	cmp	w8, #0x0
   12590:	cneg	x8, x8, lt  // lt = tstop
   12594:	add	x8, x9, x8, lsl #3
   12598:	ldur	x8, [x8, #-8]
   1259c:	mov	w9, #0x7fff                	// #32767
   125a0:	cinc	x9, x9, lt  // lt = tstop
   125a4:	cmp	x8, x9
   125a8:	cset	w0, ls  // ls = plast
   125ac:	ret
   125b0:	mov	w0, #0x1                   	// #1
   125b4:	ret
   125b8:	mov	w0, wzr
   125bc:	ret

00000000000125c0 <__gmpf_fits_uint_p@@Base>:
   125c0:	ldr	x9, [x0, #8]
   125c4:	cmp	x9, #0x1
   125c8:	b.lt	12600 <__gmpf_fits_uint_p@@Base+0x40>  // b.tstop
   125cc:	mov	x8, x0
   125d0:	cmp	x9, #0x1
   125d4:	mov	w0, wzr
   125d8:	b.ne	125fc <__gmpf_fits_uint_p@@Base+0x3c>  // b.any
   125dc:	ldr	w9, [x8, #4]
   125e0:	tbnz	w9, #31, 125fc <__gmpf_fits_uint_p@@Base+0x3c>
   125e4:	ldr	x8, [x8, #16]
   125e8:	sxtw	x9, w9
   125ec:	add	x8, x8, x9, lsl #3
   125f0:	ldur	w8, [x8, #-4]
   125f4:	cmp	w8, #0x0
   125f8:	cset	w0, eq  // eq = none
   125fc:	ret
   12600:	mov	w0, #0x1                   	// #1
   12604:	ret

0000000000012608 <__gmpf_fits_ulong_p@@Base>:
   12608:	ldr	x8, [x0, #8]
   1260c:	cmp	x8, #0x1
   12610:	b.lt	12628 <__gmpf_fits_ulong_p@@Base+0x20>  // b.tstop
   12614:	ldr	w9, [x0, #4]
   12618:	tbnz	w9, #31, 12630 <__gmpf_fits_ulong_p@@Base+0x28>
   1261c:	cmp	x8, #0x1
   12620:	cset	w0, eq  // eq = none
   12624:	ret
   12628:	mov	w0, #0x1                   	// #1
   1262c:	ret
   12630:	mov	w0, wzr
   12634:	ret

0000000000012638 <__gmpf_fits_ushort_p@@Base>:
   12638:	ldr	x9, [x0, #8]
   1263c:	cmp	x9, #0x1
   12640:	b.lt	12678 <__gmpf_fits_ushort_p@@Base+0x40>  // b.tstop
   12644:	mov	x8, x0
   12648:	cmp	x9, #0x1
   1264c:	mov	w0, wzr
   12650:	b.ne	12674 <__gmpf_fits_ushort_p@@Base+0x3c>  // b.any
   12654:	ldr	w9, [x8, #4]
   12658:	tbnz	w9, #31, 12674 <__gmpf_fits_ushort_p@@Base+0x3c>
   1265c:	ldr	x8, [x8, #16]
   12660:	sxtw	x9, w9
   12664:	add	x8, x8, x9, lsl #3
   12668:	ldur	x8, [x8, #-8]
   1266c:	cmp	x8, #0x10, lsl #12
   12670:	cset	w0, cc  // cc = lo, ul, last
   12674:	ret
   12678:	mov	w0, #0x1                   	// #1
   1267c:	ret

0000000000012680 <__gmpf_get_si@@Base>:
   12680:	ldr	x9, [x0, #8]
   12684:	cmp	x9, #0x1
   12688:	b.lt	126bc <__gmpf_get_si@@Base+0x3c>  // b.tstop
   1268c:	ldrsw	x8, [x0, #4]
   12690:	cmp	x8, #0x0
   12694:	cneg	x10, x8, mi  // mi = first
   12698:	subs	x9, x10, x9
   1269c:	b.ge	126c4 <__gmpf_get_si@@Base+0x44>  // b.tcont
   126a0:	mov	x9, xzr
   126a4:	cmp	w8, #0x1
   126a8:	b.ge	126d4 <__gmpf_get_si@@Base+0x54>  // b.tcont
   126ac:	sub	x8, x9, #0x1
   126b0:	orr	x8, x8, #0x8000000000000000
   126b4:	eor	x0, x8, #0x7fffffffffffffff
   126b8:	ret
   126bc:	mov	x0, xzr
   126c0:	ret
   126c4:	ldr	x10, [x0, #16]
   126c8:	ldr	x9, [x10, x9, lsl #3]
   126cc:	cmp	w8, #0x1
   126d0:	b.lt	126ac <__gmpf_get_si@@Base+0x2c>  // b.tstop
   126d4:	and	x0, x9, #0x7fffffffffffffff
   126d8:	ret

00000000000126dc <__gmpf_get_ui@@Base>:
   126dc:	ldr	x8, [x0, #8]
   126e0:	cmp	x8, #0x1
   126e4:	b.lt	126fc <__gmpf_get_ui@@Base+0x20>  // b.tstop
   126e8:	ldrsw	x9, [x0, #4]
   126ec:	cmp	x9, #0x0
   126f0:	cneg	x9, x9, mi  // mi = first
   126f4:	subs	x8, x9, x8
   126f8:	b.ge	12704 <__gmpf_get_ui@@Base+0x28>  // b.tcont
   126fc:	mov	x0, xzr
   12700:	ret
   12704:	ldr	x9, [x0, #16]
   12708:	ldr	x0, [x9, x8, lsl #3]
   1270c:	ret

0000000000012710 <__gmpf_integer_p@@Base>:
   12710:	ldr	x8, [x0, #8]
   12714:	ldrsw	x9, [x0, #4]
   12718:	cmp	x8, #0x0
   1271c:	b.le	12750 <__gmpf_integer_p@@Base+0x40>
   12720:	ldr	x10, [x0, #16]
   12724:	cmp	x9, #0x0
   12728:	cneg	x9, x9, mi  // mi = first
   1272c:	ldr	x11, [x10]
   12730:	cbnz	x11, 12744 <__gmpf_integer_p@@Base+0x34>
   12734:	add	x10, x10, #0x8
   12738:	ldr	x11, [x10], #8
   1273c:	sub	x9, x9, #0x1
   12740:	cbz	x11, 12738 <__gmpf_integer_p@@Base+0x28>
   12744:	cmp	x9, x8
   12748:	cset	w0, le
   1274c:	ret
   12750:	cmp	w9, #0x0
   12754:	cset	w0, eq  // eq = none
   12758:	ret

000000000001275c <__gmpz_abs@@Base>:
   1275c:	stp	x29, x30, [sp, #-48]!
   12760:	stp	x20, x19, [sp, #32]
   12764:	ldr	w8, [x1, #4]
   12768:	mov	x19, x0
   1276c:	str	x21, [sp, #16]
   12770:	mov	x29, sp
   12774:	cmp	w8, #0x0
   12778:	cneg	w20, w8, mi  // mi = first
   1277c:	cmp	x1, x0
   12780:	b.eq	127a4 <__gmpz_abs@@Base+0x48>  // b.none
   12784:	ldrsw	x8, [x19]
   12788:	mov	x21, x1
   1278c:	cmp	x20, x8
   12790:	b.gt	127b8 <__gmpz_abs@@Base+0x5c>
   12794:	ldr	x0, [x19, #8]
   12798:	ldr	x1, [x21, #8]
   1279c:	mov	x2, x20
   127a0:	bl	cc10 <__gmpn_copyi@plt>
   127a4:	str	w20, [x19, #4]
   127a8:	ldp	x20, x19, [sp, #32]
   127ac:	ldr	x21, [sp, #16]
   127b0:	ldp	x29, x30, [sp], #48
   127b4:	ret
   127b8:	mov	x0, x19
   127bc:	mov	x1, x20
   127c0:	bl	c1c0 <__gmpz_realloc@plt>
   127c4:	b	12798 <__gmpz_abs@@Base+0x3c>

00000000000127c8 <__gmpz_add@@Base>:
   127c8:	stp	x29, x30, [sp, #-80]!
   127cc:	stp	x26, x25, [sp, #16]
   127d0:	stp	x24, x23, [sp, #32]
   127d4:	stp	x22, x21, [sp, #48]
   127d8:	stp	x20, x19, [sp, #64]
   127dc:	ldrsw	x8, [x1, #4]
   127e0:	ldrsw	x9, [x2, #4]
   127e4:	ldrsw	x10, [x0]
   127e8:	mov	x19, x0
   127ec:	cmp	x8, #0x0
   127f0:	cneg	x11, x8, mi  // mi = first
   127f4:	cmp	x9, #0x0
   127f8:	cneg	x12, x9, mi  // mi = first
   127fc:	cmp	x11, x12
   12800:	csel	x20, x12, x11, lt  // lt = tstop
   12804:	csel	x22, x11, x12, lt  // lt = tstop
   12808:	csel	x26, x8, x9, lt  // lt = tstop
   1280c:	csel	x25, x9, x8, lt  // lt = tstop
   12810:	csel	x23, x1, x2, lt  // lt = tstop
   12814:	csel	x24, x2, x1, lt  // lt = tstop
   12818:	cmp	x20, x10
   1281c:	mov	x29, sp
   12820:	b.ge	1293c <__gmpz_add@@Base+0x174>  // b.tcont
   12824:	ldr	x21, [x19, #8]
   12828:	ldr	x24, [x24, #8]
   1282c:	ldr	x23, [x23, #8]
   12830:	eor	x8, x25, x26
   12834:	tbnz	x8, #63, 12864 <__gmpz_add@@Base+0x9c>
   12838:	mov	x0, x21
   1283c:	mov	x1, x24
   12840:	mov	x2, x20
   12844:	mov	x3, x23
   12848:	mov	x4, x22
   1284c:	bl	c970 <__gmpn_add@plt>
   12850:	add	x8, x0, x20
   12854:	cmp	x25, #0x0
   12858:	cneg	x8, x8, lt  // lt = tstop
   1285c:	str	x0, [x21, x20, lsl #3]
   12860:	b	128e8 <__gmpz_add@@Base+0x120>
   12864:	cmp	x20, x22
   12868:	b.ne	128b0 <__gmpz_add@@Base+0xe8>  // b.any
   1286c:	mov	x0, x24
   12870:	mov	x1, x23
   12874:	mov	x2, x20
   12878:	bl	c570 <__gmpn_cmp@plt>
   1287c:	tbnz	w0, #31, 12904 <__gmpz_add@@Base+0x13c>
   12880:	mov	x0, x21
   12884:	mov	x1, x24
   12888:	mov	x2, x23
   1288c:	mov	x3, x20
   12890:	bl	c420 <__gmpn_sub_n@plt>
   12894:	sub	x8, x21, #0x8
   12898:	mov	x9, x20
   1289c:	subs	x20, x20, #0x1
   128a0:	b.lt	128e0 <__gmpz_add@@Base+0x118>  // b.tstop
   128a4:	ldr	x10, [x8, x9, lsl #3]
   128a8:	cbz	x10, 12898 <__gmpz_add@@Base+0xd0>
   128ac:	b	128e0 <__gmpz_add@@Base+0x118>
   128b0:	mov	x0, x21
   128b4:	mov	x1, x24
   128b8:	mov	x2, x20
   128bc:	mov	x3, x23
   128c0:	mov	x4, x22
   128c4:	bl	d340 <__gmpn_sub@plt>
   128c8:	sub	x8, x21, #0x8
   128cc:	mov	x9, x20
   128d0:	subs	x20, x20, #0x1
   128d4:	b.lt	128e0 <__gmpz_add@@Base+0x118>  // b.tstop
   128d8:	ldr	x10, [x8, x9, lsl #3]
   128dc:	cbz	x10, 128cc <__gmpz_add@@Base+0x104>
   128e0:	cmp	x25, #0x0
   128e4:	cneg	x8, x9, lt  // lt = tstop
   128e8:	str	w8, [x19, #4]
   128ec:	ldp	x20, x19, [sp, #64]
   128f0:	ldp	x22, x21, [sp, #48]
   128f4:	ldp	x24, x23, [sp, #32]
   128f8:	ldp	x26, x25, [sp, #16]
   128fc:	ldp	x29, x30, [sp], #80
   12900:	ret
   12904:	mov	x0, x21
   12908:	mov	x1, x23
   1290c:	mov	x2, x24
   12910:	mov	x3, x20
   12914:	bl	c420 <__gmpn_sub_n@plt>
   12918:	sub	x8, x21, #0x8
   1291c:	mov	x9, x20
   12920:	subs	x20, x20, #0x1
   12924:	b.lt	12930 <__gmpz_add@@Base+0x168>  // b.tstop
   12928:	ldr	x10, [x8, x9, lsl #3]
   1292c:	cbz	x10, 1291c <__gmpz_add@@Base+0x154>
   12930:	cmp	x25, #0x0
   12934:	cneg	x8, x9, ge  // ge = tcont
   12938:	b	128e8 <__gmpz_add@@Base+0x120>
   1293c:	add	x1, x20, #0x1
   12940:	mov	x0, x19
   12944:	bl	c1c0 <__gmpz_realloc@plt>
   12948:	mov	x21, x0
   1294c:	b	12828 <__gmpz_add@@Base+0x60>

0000000000012950 <__gmpz_add_ui@@Base>:
   12950:	stp	x29, x30, [sp, #-64]!
   12954:	stp	x24, x23, [sp, #16]
   12958:	stp	x22, x21, [sp, #32]
   1295c:	stp	x20, x19, [sp, #48]
   12960:	ldrsw	x24, [x1, #4]
   12964:	mov	x20, x2
   12968:	mov	x19, x0
   1296c:	mov	x29, sp
   12970:	cbz	w24, 129b4 <__gmpz_add_ui@@Base+0x64>
   12974:	ldrsw	x8, [x19]
   12978:	cmp	w24, #0x0
   1297c:	cneg	x21, x24, lt  // lt = tstop
   12980:	mov	x23, x1
   12984:	cmp	x21, x8
   12988:	b.ge	129d4 <__gmpz_add_ui@@Base+0x84>  // b.tcont
   1298c:	ldr	x22, [x19, #8]
   12990:	ldr	x1, [x23, #8]
   12994:	tbnz	w24, #31, 129ec <__gmpz_add_ui@@Base+0x9c>
   12998:	mov	x0, x22
   1299c:	mov	x2, x21
   129a0:	mov	x3, x20
   129a4:	bl	c150 <__gmpn_add_1@plt>
   129a8:	str	x0, [x22, x21, lsl #3]
   129ac:	add	x8, x0, x21
   129b0:	b	12a34 <__gmpz_add_ui@@Base+0xe4>
   129b4:	ldr	w8, [x19]
   129b8:	cmp	w8, #0x0
   129bc:	b.le	12a4c <__gmpz_add_ui@@Base+0xfc>
   129c0:	ldr	x0, [x19, #8]
   129c4:	cmp	x20, #0x0
   129c8:	str	x20, [x0]
   129cc:	cset	w8, ne  // ne = any
   129d0:	b	12a34 <__gmpz_add_ui@@Base+0xe4>
   129d4:	add	x1, x21, #0x1
   129d8:	mov	x0, x19
   129dc:	bl	c1c0 <__gmpz_realloc@plt>
   129e0:	mov	x22, x0
   129e4:	ldr	x1, [x23, #8]
   129e8:	tbz	w24, #31, 12998 <__gmpz_add_ui@@Base+0x48>
   129ec:	cmp	x21, #0x1
   129f0:	b.ne	12a10 <__gmpz_add_ui@@Base+0xc0>  // b.any
   129f4:	ldr	x8, [x1]
   129f8:	cmp	x8, x20
   129fc:	b.cs	12a10 <__gmpz_add_ui@@Base+0xc0>  // b.hs, b.nlast
   12a00:	sub	x8, x20, x8
   12a04:	str	x8, [x22]
   12a08:	mov	w8, #0x1                   	// #1
   12a0c:	b	12a34 <__gmpz_add_ui@@Base+0xe4>
   12a10:	mov	x0, x22
   12a14:	mov	x2, x21
   12a18:	mov	x3, x20
   12a1c:	bl	caf0 <__gmpn_sub_1@plt>
   12a20:	add	x8, x22, x21, lsl #3
   12a24:	ldur	x8, [x8, #-8]
   12a28:	cmp	x8, #0x0
   12a2c:	cset	w8, eq  // eq = none
   12a30:	sub	x8, x8, x21
   12a34:	str	w8, [x19, #4]
   12a38:	ldp	x20, x19, [sp, #48]
   12a3c:	ldp	x22, x21, [sp, #32]
   12a40:	ldp	x24, x23, [sp, #16]
   12a44:	ldp	x29, x30, [sp], #64
   12a48:	ret
   12a4c:	mov	w1, #0x1                   	// #1
   12a50:	mov	x0, x19
   12a54:	bl	c1c0 <__gmpz_realloc@plt>
   12a58:	b	129c4 <__gmpz_add_ui@@Base+0x74>

0000000000012a5c <__gmpz_addmul@@Base>:
   12a5c:	stp	x29, x30, [sp, #-16]!
   12a60:	mov	x3, xzr
   12a64:	mov	x29, sp
   12a68:	bl	12a74 <__gmpz_addmul@@Base+0x18>
   12a6c:	ldp	x29, x30, [sp], #16
   12a70:	ret
   12a74:	stp	x29, x30, [sp, #-96]!
   12a78:	stp	x28, x27, [sp, #16]
   12a7c:	stp	x26, x25, [sp, #32]
   12a80:	stp	x24, x23, [sp, #48]
   12a84:	stp	x22, x21, [sp, #64]
   12a88:	stp	x20, x19, [sp, #80]
   12a8c:	mov	x29, sp
   12a90:	sub	sp, sp, #0x10
   12a94:	ldrsw	x8, [x1, #4]
   12a98:	cbz	w8, 12c8c <__gmpz_addmul@@Base+0x230>
   12a9c:	ldr	w9, [x2, #4]
   12aa0:	cbz	w9, 12c8c <__gmpz_addmul@@Base+0x230>
   12aa4:	sxtw	x9, w9
   12aa8:	cmp	x9, #0x0
   12aac:	cneg	x10, x9, mi  // mi = first
   12ab0:	cmp	x8, #0x0
   12ab4:	cneg	x11, x8, mi  // mi = first
   12ab8:	cmp	x10, x11
   12abc:	csel	x10, x8, x9, gt
   12ac0:	csel	x8, x9, x8, gt
   12ac4:	csel	x28, x1, x2, gt
   12ac8:	csel	x23, x2, x1, gt
   12acc:	cmp	x10, #0x0
   12ad0:	cneg	x22, x10, mi  // mi = first
   12ad4:	mov	x19, x0
   12ad8:	cmp	x22, #0x1
   12adc:	eor	x3, x10, x3
   12ae0:	b.ne	12afc <__gmpz_addmul@@Base+0xa0>  // b.any
   12ae4:	ldr	x8, [x28, #8]
   12ae8:	mov	x0, x19
   12aec:	mov	x1, x23
   12af0:	ldr	x2, [x8]
   12af4:	bl	d150 <__gmpz_aorsmul_1@plt>
   12af8:	b	12c8c <__gmpz_addmul@@Base+0x230>
   12afc:	ldpsw	x10, x26, [x19]
   12b00:	cmp	x8, #0x0
   12b04:	cneg	x25, x8, mi  // mi = first
   12b08:	add	x27, x25, x22
   12b0c:	cmp	x26, #0x0
   12b10:	cneg	x21, x26, mi  // mi = first
   12b14:	cmp	x21, x27
   12b18:	csel	x9, x21, x27, gt
   12b1c:	cmp	x9, x10
   12b20:	eor	x24, x3, x8
   12b24:	b.ge	12c40 <__gmpz_addmul@@Base+0x1e4>  // b.tcont
   12b28:	ldr	x20, [x19, #8]
   12b2c:	eor	x24, x24, x26
   12b30:	cbz	w26, 12c58 <__gmpz_addmul@@Base+0x1fc>
   12b34:	lsl	x1, x27, #3
   12b38:	mov	w8, #0x7f00                	// #32512
   12b3c:	cmp	x1, x8
   12b40:	stp	x24, xzr, [x29, #-16]
   12b44:	b.hi	12cac <__gmpz_addmul@@Base+0x250>  // b.pmore
   12b48:	add	x9, x1, #0xf
   12b4c:	mov	x8, sp
   12b50:	and	x9, x9, #0xfffffffffffffff0
   12b54:	sub	x24, x8, x9
   12b58:	mov	sp, x24
   12b5c:	ldr	x1, [x23, #8]
   12b60:	ldr	x3, [x28, #8]
   12b64:	mov	x0, x24
   12b68:	mov	x2, x25
   12b6c:	mov	x4, x22
   12b70:	bl	cea0 <__gmpn_mul@plt>
   12b74:	cmp	x0, #0x0
   12b78:	cset	w8, eq  // eq = none
   12b7c:	sub	x22, x27, x8
   12b80:	ldur	x8, [x29, #-16]
   12b84:	tbnz	x8, #63, 12bb8 <__gmpz_addmul@@Base+0x15c>
   12b88:	cmp	x21, x22
   12b8c:	csel	x4, x21, x22, lt  // lt = tstop
   12b90:	csel	x21, x22, x21, lt  // lt = tstop
   12b94:	csel	x3, x20, x24, lt  // lt = tstop
   12b98:	csel	x1, x24, x20, lt  // lt = tstop
   12b9c:	mov	x0, x20
   12ba0:	mov	x2, x21
   12ba4:	bl	c970 <__gmpn_add@plt>
   12ba8:	cmp	x0, #0x0
   12bac:	str	x0, [x20, x21, lsl #3]
   12bb0:	cinc	x8, x21, ne  // ne = any
   12bb4:	b	12c20 <__gmpz_addmul@@Base+0x1c4>
   12bb8:	cmp	x21, x22
   12bbc:	b.ge	12bd4 <__gmpz_addmul@@Base+0x178>  // b.tcont
   12bc0:	neg	x26, x26
   12bc4:	mov	x1, x24
   12bc8:	mov	x4, x21
   12bcc:	mov	x24, x20
   12bd0:	b	12bf8 <__gmpz_addmul@@Base+0x19c>
   12bd4:	b.ne	12bec <__gmpz_addmul@@Base+0x190>  // b.any
   12bd8:	mov	x0, x20
   12bdc:	mov	x1, x24
   12be0:	mov	x2, x21
   12be4:	bl	c570 <__gmpn_cmp@plt>
   12be8:	tbnz	w0, #31, 12bc0 <__gmpz_addmul@@Base+0x164>
   12bec:	mov	x4, x22
   12bf0:	mov	x1, x20
   12bf4:	mov	x22, x21
   12bf8:	mov	x0, x20
   12bfc:	mov	x2, x22
   12c00:	mov	x3, x24
   12c04:	bl	d340 <__gmpn_sub@plt>
   12c08:	sub	x9, x20, #0x8
   12c0c:	mov	x8, x22
   12c10:	subs	x22, x22, #0x1
   12c14:	b.lt	12c20 <__gmpz_addmul@@Base+0x1c4>  // b.tstop
   12c18:	ldr	x10, [x9, x8, lsl #3]
   12c1c:	cbz	x10, 12c0c <__gmpz_addmul@@Base+0x1b0>
   12c20:	neg	w9, w8
   12c24:	cmp	x26, #0x0
   12c28:	csel	x8, x8, x9, ge  // ge = tcont
   12c2c:	str	w8, [x19, #4]
   12c30:	ldur	x0, [x29, #-8]
   12c34:	cbz	x0, 12c8c <__gmpz_addmul@@Base+0x230>
   12c38:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   12c3c:	b	12c8c <__gmpz_addmul@@Base+0x230>
   12c40:	add	x1, x9, #0x1
   12c44:	mov	x0, x19
   12c48:	bl	c1c0 <__gmpz_realloc@plt>
   12c4c:	mov	x20, x0
   12c50:	eor	x24, x24, x26
   12c54:	cbnz	w26, 12b34 <__gmpz_addmul@@Base+0xd8>
   12c58:	ldr	x1, [x23, #8]
   12c5c:	ldr	x3, [x28, #8]
   12c60:	mov	x0, x20
   12c64:	mov	x2, x25
   12c68:	mov	x4, x22
   12c6c:	bl	cea0 <__gmpn_mul@plt>
   12c70:	cmp	x0, #0x0
   12c74:	cset	w8, eq  // eq = none
   12c78:	sub	x8, x27, x8
   12c7c:	neg	w9, w8
   12c80:	cmp	x24, #0x0
   12c84:	csel	x8, x8, x9, ge  // ge = tcont
   12c88:	str	w8, [x19, #4]
   12c8c:	mov	sp, x29
   12c90:	ldp	x20, x19, [sp, #80]
   12c94:	ldp	x22, x21, [sp, #64]
   12c98:	ldp	x24, x23, [sp, #48]
   12c9c:	ldp	x26, x25, [sp, #32]
   12ca0:	ldp	x28, x27, [sp, #16]
   12ca4:	ldp	x29, x30, [sp], #96
   12ca8:	ret
   12cac:	sub	x0, x29, #0x8
   12cb0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   12cb4:	mov	x24, x0
   12cb8:	b	12b5c <__gmpz_addmul@@Base+0x100>

0000000000012cbc <__gmpz_submul@@Base>:
   12cbc:	stp	x29, x30, [sp, #-16]!
   12cc0:	mov	x3, #0xffffffffffffffff    	// #-1
   12cc4:	mov	x29, sp
   12cc8:	bl	12a74 <__gmpz_addmul@@Base+0x18>
   12ccc:	ldp	x29, x30, [sp], #16
   12cd0:	ret

0000000000012cd4 <__gmpz_aorsmul_1@@Base>:
   12cd4:	sub	sp, sp, #0x70
   12cd8:	stp	x29, x30, [sp, #16]
   12cdc:	stp	x28, x27, [sp, #32]
   12ce0:	stp	x26, x25, [sp, #48]
   12ce4:	stp	x24, x23, [sp, #64]
   12ce8:	stp	x22, x21, [sp, #80]
   12cec:	stp	x20, x19, [sp, #96]
   12cf0:	add	x29, sp, #0x10
   12cf4:	cbz	x2, 12f00 <__gmpz_aorsmul_1@@Base+0x22c>
   12cf8:	ldr	w8, [x1, #4]
   12cfc:	mov	x25, x1
   12d00:	cbz	w8, 12f00 <__gmpz_aorsmul_1@@Base+0x22c>
   12d04:	ldrsw	x24, [x0, #4]
   12d08:	sxtw	x8, w8
   12d0c:	cmp	x8, #0x0
   12d10:	mov	x22, x2
   12d14:	mov	x19, x0
   12d18:	eor	x21, x8, x3
   12d1c:	cneg	x23, x8, mi  // mi = first
   12d20:	cbz	w24, 12d9c <__gmpz_aorsmul_1@@Base+0xc8>
   12d24:	cmp	x24, #0x0
   12d28:	ldrsw	x8, [x19]
   12d2c:	cneg	x26, x24, mi  // mi = first
   12d30:	cmp	x23, x26
   12d34:	csel	x20, x26, x23, lt  // lt = tstop
   12d38:	eor	x21, x21, x24
   12d3c:	cmp	x20, x8
   12d40:	add	x8, x20, #0x1
   12d44:	b.ge	12f3c <__gmpz_aorsmul_1@@Base+0x268>  // b.tcont
   12d48:	ldr	x0, [x19, #8]
   12d4c:	ldr	x25, [x25, #8]
   12d50:	subs	x27, x23, x26
   12d54:	csel	x28, x26, x23, gt
   12d58:	tbnz	x21, #63, 12dd4 <__gmpz_aorsmul_1@@Base+0x100>
   12d5c:	mov	x1, x25
   12d60:	mov	x2, x28
   12d64:	mov	x3, x22
   12d68:	mov	x21, x0
   12d6c:	bl	d5e0 <__gmpn_addmul_1@plt>
   12d70:	mov	x4, x0
   12d74:	cmp	x27, #0x1
   12d78:	add	x21, x21, x28, lsl #3
   12d7c:	b.lt	12e70 <__gmpz_aorsmul_1@@Base+0x19c>  // b.tstop
   12d80:	add	x1, x25, x28, lsl #3
   12d84:	mov	x0, x21
   12d88:	mov	x2, x27
   12d8c:	mov	x3, x22
   12d90:	bl	d420 <__gmpn_mul_1c@plt>
   12d94:	mov	x4, x0
   12d98:	b	12e74 <__gmpz_aorsmul_1@@Base+0x1a0>
   12d9c:	ldrsw	x8, [x19]
   12da0:	cmp	x23, x8
   12da4:	b.ge	12f54 <__gmpz_aorsmul_1@@Base+0x280>  // b.tcont
   12da8:	ldr	x20, [x19, #8]
   12dac:	ldr	x1, [x25, #8]
   12db0:	mov	x0, x20
   12db4:	mov	x2, x23
   12db8:	mov	x3, x22
   12dbc:	bl	d670 <__gmpn_mul_1@plt>
   12dc0:	cmp	x0, #0x0
   12dc4:	str	x0, [x20, x23, lsl #3]
   12dc8:	cinc	x8, x23, ne  // ne = any
   12dcc:	mov	x24, x21
   12dd0:	b	12ef0 <__gmpz_aorsmul_1@@Base+0x21c>
   12dd4:	str	x8, [sp]
   12dd8:	neg	x8, x24
   12ddc:	mov	x1, x25
   12de0:	mov	x2, x28
   12de4:	mov	x3, x22
   12de8:	str	x8, [sp, #8]
   12dec:	mov	x21, x0
   12df0:	bl	cba0 <__gmpn_submul_1@plt>
   12df4:	subs	x28, x23, x26
   12df8:	mov	x27, x0
   12dfc:	b.le	12e84 <__gmpz_aorsmul_1@@Base+0x1b0>
   12e00:	mov	x0, x21
   12e04:	mov	x1, x21
   12e08:	mov	x2, x26
   12e0c:	bl	c3e0 <__gmpn_com@plt>
   12e10:	mov	w3, #0x1                   	// #1
   12e14:	mov	x0, x21
   12e18:	mov	x1, x21
   12e1c:	mov	x2, x26
   12e20:	bl	c150 <__gmpn_add_1@plt>
   12e24:	adds	x24, x0, x27
   12e28:	lsl	x8, x26, #3
   12e2c:	cinc	x9, x24, eq  // eq = none
   12e30:	add	x23, x21, x8
   12e34:	sub	x4, x9, #0x1
   12e38:	add	x1, x25, x8
   12e3c:	mov	x0, x23
   12e40:	mov	x2, x28
   12e44:	mov	x3, x22
   12e48:	bl	d420 <__gmpn_mul_1c@plt>
   12e4c:	cmp	x0, #0x0
   12e50:	str	x0, [x21, x20, lsl #3]
   12e54:	cinc	x20, x20, ne  // ne = any
   12e58:	cbnz	x24, 12ed4 <__gmpz_aorsmul_1@@Base+0x200>
   12e5c:	ldr	x8, [x23]
   12e60:	sub	x9, x8, #0x1
   12e64:	str	x9, [x23], #8
   12e68:	cbz	x8, 12e5c <__gmpz_aorsmul_1@@Base+0x188>
   12e6c:	b	12ed4 <__gmpz_aorsmul_1@@Base+0x200>
   12e70:	tbnz	x27, #63, 12f20 <__gmpz_aorsmul_1@@Base+0x24c>
   12e74:	cmp	x4, #0x0
   12e78:	str	x4, [x21, x27, lsl #3]
   12e7c:	cinc	x8, x20, ne  // ne = any
   12e80:	b	12ef0 <__gmpz_aorsmul_1@@Base+0x21c>
   12e84:	b.eq	12ea0 <__gmpz_aorsmul_1@@Base+0x1cc>  // b.none
   12e88:	add	x0, x21, x23, lsl #3
   12e8c:	sub	x2, x26, x23
   12e90:	mov	x1, x0
   12e94:	mov	x3, x27
   12e98:	bl	caf0 <__gmpn_sub_1@plt>
   12e9c:	mov	x27, x0
   12ea0:	cbz	x27, 12ed8 <__gmpz_aorsmul_1@@Base+0x204>
   12ea4:	sub	x8, x27, #0x1
   12ea8:	mov	x0, x21
   12eac:	mov	x1, x21
   12eb0:	mov	x2, x20
   12eb4:	str	x8, [x21, x20, lsl #3]
   12eb8:	bl	c3e0 <__gmpn_com@plt>
   12ebc:	mov	x8, x21
   12ec0:	ldr	x9, [x8]
   12ec4:	adds	x9, x9, #0x1
   12ec8:	str	x9, [x8], #8
   12ecc:	b.cs	12ec0 <__gmpz_aorsmul_1@@Base+0x1ec>  // b.hs, b.nlast
   12ed0:	ldr	x20, [sp]
   12ed4:	ldr	x24, [sp, #8]
   12ed8:	sub	x9, x21, #0x8
   12edc:	mov	x8, x20
   12ee0:	subs	x20, x20, #0x1
   12ee4:	b.lt	12ef0 <__gmpz_aorsmul_1@@Base+0x21c>  // b.tstop
   12ee8:	ldr	x10, [x9, x8, lsl #3]
   12eec:	cbz	x10, 12edc <__gmpz_aorsmul_1@@Base+0x208>
   12ef0:	neg	w9, w8
   12ef4:	cmp	x24, #0x0
   12ef8:	csel	x8, x8, x9, ge  // ge = tcont
   12efc:	str	w8, [x19, #4]
   12f00:	ldp	x20, x19, [sp, #96]
   12f04:	ldp	x22, x21, [sp, #80]
   12f08:	ldp	x24, x23, [sp, #64]
   12f0c:	ldp	x26, x25, [sp, #48]
   12f10:	ldp	x28, x27, [sp, #32]
   12f14:	ldp	x29, x30, [sp, #16]
   12f18:	add	sp, sp, #0x70
   12f1c:	ret
   12f20:	neg	x27, x27
   12f24:	mov	x0, x21
   12f28:	mov	x1, x21
   12f2c:	mov	x2, x27
   12f30:	mov	x3, x4
   12f34:	bl	c150 <__gmpn_add_1@plt>
   12f38:	b	12d94 <__gmpz_aorsmul_1@@Base+0xc0>
   12f3c:	mov	x0, x19
   12f40:	mov	x1, x8
   12f44:	mov	x27, x8
   12f48:	bl	c1c0 <__gmpz_realloc@plt>
   12f4c:	mov	x8, x27
   12f50:	b	12d4c <__gmpz_aorsmul_1@@Base+0x78>
   12f54:	add	x1, x23, #0x1
   12f58:	mov	x0, x19
   12f5c:	bl	c1c0 <__gmpz_realloc@plt>
   12f60:	mov	x20, x0
   12f64:	b	12dac <__gmpz_aorsmul_1@@Base+0xd8>

0000000000012f68 <__gmpz_addmul_ui@@Base>:
   12f68:	stp	x29, x30, [sp, #-16]!
   12f6c:	mov	x3, xzr
   12f70:	mov	x29, sp
   12f74:	bl	d150 <__gmpz_aorsmul_1@plt>
   12f78:	ldp	x29, x30, [sp], #16
   12f7c:	ret

0000000000012f80 <__gmpz_submul_ui@@Base>:
   12f80:	stp	x29, x30, [sp, #-16]!
   12f84:	mov	x3, #0xffffffffffffffff    	// #-1
   12f88:	mov	x29, sp
   12f8c:	bl	d150 <__gmpz_aorsmul_1@plt>
   12f90:	ldp	x29, x30, [sp], #16
   12f94:	ret

0000000000012f98 <__gmpz_and@@Base>:
   12f98:	stp	x29, x30, [sp, #-96]!
   12f9c:	stp	x28, x27, [sp, #16]
   12fa0:	stp	x26, x25, [sp, #32]
   12fa4:	stp	x24, x23, [sp, #48]
   12fa8:	stp	x22, x21, [sp, #64]
   12fac:	stp	x20, x19, [sp, #80]
   12fb0:	mov	x29, sp
   12fb4:	sub	sp, sp, #0x10
   12fb8:	ldr	w8, [x1, #4]
   12fbc:	ldr	w9, [x2, #4]
   12fc0:	mov	x19, x0
   12fc4:	cmp	w8, w9
   12fc8:	csel	x10, x1, x2, lt  // lt = tstop
   12fcc:	csel	x11, x2, x1, lt  // lt = tstop
   12fd0:	ldr	x20, [x11, #8]
   12fd4:	ldr	x23, [x10, #8]
   12fd8:	csel	w10, w8, w9, lt  // lt = tstop
   12fdc:	sxtw	x27, w10
   12fe0:	csel	w8, w9, w8, lt  // lt = tstop
   12fe4:	tbnz	w10, #31, 13044 <__gmpz_and@@Base+0xac>
   12fe8:	sub	x9, x27, #0x1
   12fec:	add	w8, w27, #0x1
   12ff0:	add	x10, x9, #0x1
   12ff4:	cmp	x10, #0x1
   12ff8:	b.lt	130cc <__gmpz_and@@Base+0x134>  // b.tstop
   12ffc:	lsl	x10, x9, #3
   13000:	ldr	x11, [x20, x10]
   13004:	ldr	x10, [x23, x10]
   13008:	sub	x9, x9, #0x1
   1300c:	sub	w8, w8, #0x1
   13010:	and	x10, x10, x11
   13014:	cbz	x10, 12ff0 <__gmpz_and@@Base+0x58>
   13018:	ldrsw	x10, [x19]
   1301c:	add	x21, x9, #0x2
   13020:	str	w8, [x19, #4]
   13024:	cmp	x21, x10
   13028:	b.gt	13220 <__gmpz_and@@Base+0x288>
   1302c:	ldr	x0, [x19, #8]
   13030:	mov	x1, x20
   13034:	mov	x2, x23
   13038:	mov	x3, x21
   1303c:	bl	c3c0 <__gmpn_and_n@plt>
   13040:	b	13200 <__gmpz_and@@Base+0x268>
   13044:	sxtw	x21, w8
   13048:	neg	x22, x27
   1304c:	stur	xzr, [x29, #-8]
   13050:	tbnz	w21, #31, 130d4 <__gmpz_and@@Base+0x13c>
   13054:	lsl	x25, x22, #3
   13058:	mov	w8, #0x7f00                	// #32512
   1305c:	cmp	x25, x8
   13060:	b.hi	13230 <__gmpz_and@@Base+0x298>  // b.pmore
   13064:	add	x9, x25, #0xf
   13068:	mov	x8, sp
   1306c:	and	x9, x9, #0xfffffffffffffff0
   13070:	sub	x24, x8, x9
   13074:	mov	sp, x24
   13078:	mov	w3, #0x1                   	// #1
   1307c:	mov	x0, x24
   13080:	mov	x1, x23
   13084:	mov	x2, x22
   13088:	bl	caf0 <__gmpn_sub_1@plt>
   1308c:	cmp	x21, x22
   13090:	b.le	131a0 <__gmpz_and@@Base+0x208>
   13094:	ldr	w8, [x19]
   13098:	cmp	w21, w8
   1309c:	b.gt	13274 <__gmpz_and@@Base+0x2dc>
   130a0:	ldr	x23, [x19, #8]
   130a4:	mov	x0, x23
   130a8:	mov	x1, x20
   130ac:	mov	x2, x24
   130b0:	mov	x3, x22
   130b4:	bl	c1a0 <__gmpn_andn_n@plt>
   130b8:	add	x0, x23, x25
   130bc:	add	x1, x20, x25
   130c0:	add	x2, x21, x27
   130c4:	bl	cc10 <__gmpn_copyi@plt>
   130c8:	b	131f4 <__gmpz_and@@Base+0x25c>
   130cc:	str	wzr, [x19, #4]
   130d0:	b	13200 <__gmpz_and@@Base+0x268>
   130d4:	add	x8, x21, x27
   130d8:	neg	x1, x8, lsl #3
   130dc:	mov	w8, #0x7f00                	// #32512
   130e0:	cmp	x1, x8
   130e4:	neg	x24, x21
   130e8:	b.hi	1324c <__gmpz_and@@Base+0x2b4>  // b.pmore
   130ec:	add	x9, x1, #0xf
   130f0:	mov	x8, sp
   130f4:	and	x9, x9, #0xfffffffffffffff0
   130f8:	sub	x25, x8, x9
   130fc:	mov	sp, x25
   13100:	mov	w3, #0x1                   	// #1
   13104:	mov	x0, x25
   13108:	mov	x1, x20
   1310c:	mov	x2, x24
   13110:	add	x26, x25, x24, lsl #3
   13114:	mov	w28, #0x1                   	// #1
   13118:	bl	caf0 <__gmpn_sub_1@plt>
   1311c:	mov	w3, #0x1                   	// #1
   13120:	mov	x0, x26
   13124:	mov	x1, x23
   13128:	mov	x2, x22
   1312c:	bl	caf0 <__gmpn_sub_1@plt>
   13130:	ldrsw	x8, [x19]
   13134:	sub	x1, x28, x27
   13138:	cmp	x1, x8
   1313c:	b.gt	1325c <__gmpz_and@@Base+0x2c4>
   13140:	ldr	x20, [x19, #8]
   13144:	lsl	x8, x24, #3
   13148:	add	x0, x20, x8
   1314c:	add	x1, x26, x8
   13150:	sub	x2, x21, x27
   13154:	bl	cc10 <__gmpn_copyi@plt>
   13158:	mov	x0, x20
   1315c:	mov	x1, x25
   13160:	mov	x2, x26
   13164:	mov	x3, x24
   13168:	bl	cdc0 <__gmpn_ior_n@plt>
   1316c:	ldur	x0, [x29, #-8]
   13170:	cbnz	x0, 1326c <__gmpz_and@@Base+0x2d4>
   13174:	mov	x8, x20
   13178:	str	xzr, [x20, x22, lsl #3]
   1317c:	ldr	x9, [x8]
   13180:	adds	x9, x9, #0x1
   13184:	str	x9, [x8], #8
   13188:	b.cs	1317c <__gmpz_and@@Base+0x1e4>  // b.hs, b.nlast
   1318c:	lsl	x8, x22, #3
   13190:	ldr	w8, [x20, x8]
   13194:	sub	w8, w27, w8
   13198:	str	w8, [x19, #4]
   1319c:	b	13200 <__gmpz_and@@Base+0x268>
   131a0:	sub	x8, x24, #0x8
   131a4:	subs	x9, x21, #0x1
   131a8:	b.lt	131f0 <__gmpz_and@@Base+0x258>  // b.tstop
   131ac:	lsl	x10, x21, #3
   131b0:	add	x11, x20, x10
   131b4:	ldur	x11, [x11, #-8]
   131b8:	ldr	x10, [x8, x10]
   131bc:	mov	x21, x9
   131c0:	bics	xzr, x11, x10
   131c4:	b.eq	131a4 <__gmpz_and@@Base+0x20c>  // b.none
   131c8:	ldrsw	x8, [x19]
   131cc:	add	x21, x9, #0x1
   131d0:	cmp	x21, x8
   131d4:	b.gt	13288 <__gmpz_and@@Base+0x2f0>
   131d8:	ldr	x0, [x19, #8]
   131dc:	mov	x1, x20
   131e0:	mov	x2, x24
   131e4:	mov	x3, x21
   131e8:	bl	c1a0 <__gmpn_andn_n@plt>
   131ec:	b	131f4 <__gmpz_and@@Base+0x25c>
   131f0:	mov	x21, xzr
   131f4:	str	w21, [x19, #4]
   131f8:	ldur	x0, [x29, #-8]
   131fc:	cbnz	x0, 13244 <__gmpz_and@@Base+0x2ac>
   13200:	mov	sp, x29
   13204:	ldp	x20, x19, [sp, #80]
   13208:	ldp	x22, x21, [sp, #64]
   1320c:	ldp	x24, x23, [sp, #48]
   13210:	ldp	x26, x25, [sp, #32]
   13214:	ldp	x28, x27, [sp, #16]
   13218:	ldp	x29, x30, [sp], #96
   1321c:	ret
   13220:	mov	x0, x19
   13224:	mov	x1, x21
   13228:	bl	c1c0 <__gmpz_realloc@plt>
   1322c:	b	13030 <__gmpz_and@@Base+0x98>
   13230:	sub	x0, x29, #0x8
   13234:	mov	x1, x25
   13238:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1323c:	mov	x24, x0
   13240:	b	13078 <__gmpz_and@@Base+0xe0>
   13244:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   13248:	b	13200 <__gmpz_and@@Base+0x268>
   1324c:	sub	x0, x29, #0x8
   13250:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   13254:	mov	x25, x0
   13258:	b	13100 <__gmpz_and@@Base+0x168>
   1325c:	mov	x0, x19
   13260:	bl	c1c0 <__gmpz_realloc@plt>
   13264:	mov	x20, x0
   13268:	b	13144 <__gmpz_and@@Base+0x1ac>
   1326c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   13270:	b	13174 <__gmpz_and@@Base+0x1dc>
   13274:	mov	x0, x19
   13278:	mov	x1, x21
   1327c:	bl	c1c0 <__gmpz_realloc@plt>
   13280:	mov	x23, x0
   13284:	b	130a4 <__gmpz_and@@Base+0x10c>
   13288:	mov	x0, x19
   1328c:	mov	x1, x21
   13290:	bl	c1c0 <__gmpz_realloc@plt>
   13294:	b	131dc <__gmpz_and@@Base+0x244>

0000000000013298 <__gmpz_array_init@@Base>:
   13298:	stp	x29, x30, [sp, #-48]!
   1329c:	str	x21, [sp, #16]
   132a0:	stp	x20, x19, [sp, #32]
   132a4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   132a8:	ldr	x9, [x9, #3840]
   132ac:	add	x8, x2, #0x3f
   132b0:	cmp	x2, #0x0
   132b4:	csel	x8, x8, x2, lt  // lt = tstop
   132b8:	asr	x21, x8, #6
   132bc:	ldr	x8, [x9]
   132c0:	add	x9, x21, #0x1
   132c4:	mul	x9, x1, x9
   132c8:	mov	x20, x0
   132cc:	lsl	x0, x9, #3
   132d0:	mov	x29, sp
   132d4:	mov	x19, x1
   132d8:	blr	x8
   132dc:	cmp	x19, #0x1
   132e0:	b.lt	1330c <__gmpz_array_init@@Base+0x74>  // b.tstop
   132e4:	lsl	x10, x21, #3
   132e8:	add	w8, w21, #0x2
   132ec:	add	x9, x20, #0x4
   132f0:	add	x10, x10, #0x8
   132f4:	stp	w8, wzr, [x9, #-4]
   132f8:	stur	x0, [x9, #4]
   132fc:	subs	x19, x19, #0x1
   13300:	add	x9, x9, #0x10
   13304:	add	x0, x0, x10
   13308:	b.ne	132f4 <__gmpz_array_init@@Base+0x5c>  // b.any
   1330c:	ldp	x20, x19, [sp, #32]
   13310:	ldr	x21, [sp, #16]
   13314:	ldp	x29, x30, [sp], #48
   13318:	ret

000000000001331c <__gmpz_bin_ui@@Base>:
   1331c:	sub	sp, sp, #0x60
   13320:	stp	x29, x30, [sp, #48]
   13324:	stp	x22, x21, [sp, #64]
   13328:	stp	x20, x19, [sp, #80]
   1332c:	ldr	w8, [x1, #4]
   13330:	mov	x20, x2
   13334:	mov	x21, x1
   13338:	mov	x19, x0
   1333c:	add	x29, sp, #0x30
   13340:	tbnz	w8, #31, 13374 <__gmpz_bin_ui@@Base+0x58>
   13344:	mov	x0, x21
   13348:	mov	x1, x20
   1334c:	bl	d3d0 <__gmpz_cmp_ui@plt>
   13350:	tbnz	w0, #31, 13498 <__gmpz_bin_ui@@Base+0x17c>
   13354:	sub	x0, x29, #0x10
   13358:	bl	d430 <__gmpz_init@plt>
   1335c:	sub	x0, x29, #0x10
   13360:	mov	x1, x21
   13364:	mov	x2, x20
   13368:	bl	c270 <__gmpz_sub_ui@plt>
   1336c:	mov	x22, xzr
   13370:	b	1339c <__gmpz_bin_ui@@Base+0x80>
   13374:	sub	x0, x29, #0x10
   13378:	bl	d430 <__gmpz_init@plt>
   1337c:	sub	x0, x29, #0x10
   13380:	mov	w2, #0x1                   	// #1
   13384:	mov	x1, x21
   13388:	bl	ca60 <__gmpz_add_ui@plt>
   1338c:	sub	x0, x29, #0x10
   13390:	sub	x1, x29, #0x10
   13394:	bl	c6a0 <__gmpz_neg@plt>
   13398:	and	x22, x20, #0x1
   1339c:	sub	x0, x29, #0x10
   133a0:	mov	x1, x20
   133a4:	bl	d3d0 <__gmpz_cmp_ui@plt>
   133a8:	tbnz	w0, #31, 133cc <__gmpz_bin_ui@@Base+0xb0>
   133ac:	cmp	x20, #0x1
   133b0:	b.hi	133f0 <__gmpz_bin_ui@@Base+0xd4>  // b.pmore
   133b4:	cbz	x20, 134a0 <__gmpz_bin_ui@@Base+0x184>
   133b8:	sub	x1, x29, #0x10
   133bc:	mov	w2, #0x1                   	// #1
   133c0:	mov	x0, x19
   133c4:	bl	ca60 <__gmpz_add_ui@plt>
   133c8:	b	134c0 <__gmpz_bin_ui@@Base+0x1a4>
   133cc:	sub	x0, x29, #0x10
   133d0:	bl	c900 <__gmpz_get_ui@plt>
   133d4:	mov	x21, x0
   133d8:	sub	x0, x29, #0x10
   133dc:	mov	x1, x20
   133e0:	bl	c2c0 <__gmpz_set_ui@plt>
   133e4:	mov	x20, x21
   133e8:	cmp	x20, #0x1
   133ec:	b.ls	133b4 <__gmpz_bin_ui@@Base+0x98>  // b.plast
   133f0:	add	x0, sp, #0x10
   133f4:	bl	d430 <__gmpz_init@plt>
   133f8:	mov	x0, sp
   133fc:	bl	d430 <__gmpz_init@plt>
   13400:	add	x0, sp, #0x10
   13404:	sub	x1, x29, #0x10
   13408:	mov	x3, sp
   1340c:	mov	x2, x20
   13410:	mov	x4, x19
   13414:	bl	13500 <__gmpz_bin_ui@@Base+0x1e4>
   13418:	lsr	x8, x20, #1
   1341c:	and	x9, x8, #0x5555555555555555
   13420:	sub	x9, x20, x9
   13424:	lsr	x10, x9, #2
   13428:	and	x9, x9, #0x3333333333333333
   1342c:	and	x10, x10, #0x3333333333333333
   13430:	add	x9, x10, x9
   13434:	add	x9, x9, x9, lsr #4
   13438:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   1343c:	add	x9, x9, x9, lsr #8
   13440:	add	x9, x9, x9, lsr #16
   13444:	sub	x8, x20, x8
   13448:	lsr	x10, x9, #32
   1344c:	add	w9, w10, w9
   13450:	sub	x8, x8, x20, lsr #2
   13454:	sub	x2, x8, w9, uxtb
   13458:	add	x0, sp, #0x10
   1345c:	add	x1, sp, #0x10
   13460:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   13464:	mov	x0, sp
   13468:	mov	x1, x20
   1346c:	mov	w2, wzr
   13470:	bl	c530 <__gmpz_oddfac_1@plt>
   13474:	add	x1, sp, #0x10
   13478:	mov	x2, sp
   1347c:	mov	x0, x19
   13480:	bl	c550 <__gmpz_divexact@plt>
   13484:	add	x0, sp, #0x10
   13488:	bl	cd10 <__gmpz_clear@plt>
   1348c:	mov	x0, sp
   13490:	bl	cd10 <__gmpz_clear@plt>
   13494:	b	134c0 <__gmpz_bin_ui@@Base+0x1a4>
   13498:	str	wzr, [x19, #4]
   1349c:	b	134dc <__gmpz_bin_ui@@Base+0x1c0>
   134a0:	ldr	w8, [x19]
   134a4:	mov	w9, #0x1                   	// #1
   134a8:	str	w9, [x19, #4]
   134ac:	cmp	w8, #0x0
   134b0:	b.le	134f0 <__gmpz_bin_ui@@Base+0x1d4>
   134b4:	ldr	x0, [x19, #8]
   134b8:	mov	w8, #0x1                   	// #1
   134bc:	str	x8, [x0]
   134c0:	sub	x0, x29, #0x10
   134c4:	bl	cd10 <__gmpz_clear@plt>
   134c8:	ldr	w8, [x19, #4]
   134cc:	neg	w9, w22
   134d0:	eor	w8, w8, w9
   134d4:	add	w8, w8, w22
   134d8:	str	w8, [x19, #4]
   134dc:	ldp	x20, x19, [sp, #80]
   134e0:	ldp	x22, x21, [sp, #64]
   134e4:	ldp	x29, x30, [sp, #48]
   134e8:	add	sp, sp, #0x60
   134ec:	ret
   134f0:	mov	w1, #0x1                   	// #1
   134f4:	mov	x0, x19
   134f8:	bl	c1c0 <__gmpz_realloc@plt>
   134fc:	b	134b8 <__gmpz_bin_ui@@Base+0x19c>
   13500:	stp	x29, x30, [sp, #-64]!
   13504:	stp	x22, x21, [sp, #32]
   13508:	mov	x22, x0
   1350c:	mov	x0, x1
   13510:	stp	x24, x23, [sp, #16]
   13514:	stp	x20, x19, [sp, #48]
   13518:	mov	x29, sp
   1351c:	mov	x21, x4
   13520:	mov	x20, x3
   13524:	mov	x23, x2
   13528:	mov	x19, x1
   1352c:	bl	13644 <__gmpz_bin_ui@@Base+0x328>
   13530:	mov	w1, #0x1                   	// #1
   13534:	mov	x0, x19
   13538:	bl	13684 <__gmpz_bin_ui@@Base+0x368>
   1353c:	str	wzr, [x22, #4]
   13540:	tbz	w23, #0, 1355c <__gmpz_bin_ui@@Base+0x240>
   13544:	mov	x0, x22
   13548:	mov	x1, x19
   1354c:	bl	c590 <__gmpz_set@plt>
   13550:	mov	w1, #0x1                   	// #1
   13554:	mov	x0, x19
   13558:	bl	13684 <__gmpz_bin_ui@@Base+0x368>
   1355c:	lsr	x24, x23, #1
   13560:	mov	x0, x21
   13564:	mov	x1, x19
   13568:	mov	x2, x24
   1356c:	mov	x3, x20
   13570:	bl	136c4 <__gmpz_bin_ui@@Base+0x3a8>
   13574:	mov	x0, x21
   13578:	bl	13644 <__gmpz_bin_ui@@Base+0x328>
   1357c:	tbz	w23, #1, 135b4 <__gmpz_bin_ui@@Base+0x298>
   13580:	ldr	w8, [x22, #4]
   13584:	cbz	w8, 1359c <__gmpz_bin_ui@@Base+0x280>
   13588:	mov	x0, x22
   1358c:	mov	x1, x22
   13590:	mov	x2, x21
   13594:	bl	c620 <__gmpz_mul@plt>
   13598:	b	135a8 <__gmpz_bin_ui@@Base+0x28c>
   1359c:	mov	x0, x22
   135a0:	mov	x1, x21
   135a4:	bl	c590 <__gmpz_set@plt>
   135a8:	sub	x1, x24, #0x1
   135ac:	mov	x0, x21
   135b0:	bl	13684 <__gmpz_bin_ui@@Base+0x368>
   135b4:	lsr	x24, x23, #2
   135b8:	cbz	x24, 13618 <__gmpz_bin_ui@@Base+0x2fc>
   135bc:	mov	x0, x20
   135c0:	mov	x1, x21
   135c4:	mov	x2, x24
   135c8:	mov	x3, x19
   135cc:	bl	136c4 <__gmpz_bin_ui@@Base+0x3a8>
   135d0:	ldr	w8, [x22, #4]
   135d4:	cbz	w8, 1362c <__gmpz_bin_ui@@Base+0x310>
   135d8:	mov	x0, x22
   135dc:	mov	x1, x22
   135e0:	mov	x2, x20
   135e4:	bl	c620 <__gmpz_mul@plt>
   135e8:	cmp	x23, #0x8
   135ec:	b.cc	13618 <__gmpz_bin_ui@@Base+0x2fc>  // b.lo, b.ul, b.last
   135f0:	mov	x0, x21
   135f4:	mov	x1, x24
   135f8:	bl	13790 <__gmpz_bin_ui@@Base+0x474>
   135fc:	sub	x3, x24, #0x1
   13600:	mov	x0, x22
   13604:	mov	x1, x21
   13608:	mov	x2, x20
   1360c:	mov	x4, xzr
   13610:	mov	x5, x19
   13614:	bl	137d8 <__gmpz_bin_ui@@Base+0x4bc>
   13618:	ldp	x20, x19, [sp, #48]
   1361c:	ldp	x22, x21, [sp, #32]
   13620:	ldp	x24, x23, [sp, #16]
   13624:	ldp	x29, x30, [sp], #64
   13628:	ret
   1362c:	mov	x0, x22
   13630:	mov	x1, x20
   13634:	bl	c590 <__gmpz_set@plt>
   13638:	cmp	x23, #0x8
   1363c:	b.cs	135f0 <__gmpz_bin_ui@@Base+0x2d4>  // b.hs, b.nlast
   13640:	b	13618 <__gmpz_bin_ui@@Base+0x2fc>
   13644:	stp	x29, x30, [sp, #-32]!
   13648:	str	x19, [sp, #16]
   1364c:	ldp	w8, w19, [x0]
   13650:	mov	x29, sp
   13654:	sxtw	x19, w19
   13658:	add	x1, x19, #0x2
   1365c:	cmp	w1, w8
   13660:	b.gt	1367c <__gmpz_bin_ui@@Base+0x360>
   13664:	ldr	x0, [x0, #8]
   13668:	add	x8, x0, x19, lsl #3
   1366c:	stp	xzr, xzr, [x8]
   13670:	ldr	x19, [sp, #16]
   13674:	ldp	x29, x30, [sp], #32
   13678:	ret
   1367c:	bl	c1c0 <__gmpz_realloc@plt>
   13680:	b	13668 <__gmpz_bin_ui@@Base+0x34c>
   13684:	ldr	x8, [x0, #8]
   13688:	ldr	x9, [x8]
   1368c:	adds	x9, x9, x1
   13690:	str	x9, [x8]
   13694:	b.cc	136ac <__gmpz_bin_ui@@Base+0x390>  // b.lo, b.ul, b.last
   13698:	add	x9, x8, #0x8
   1369c:	ldr	x10, [x9]
   136a0:	adds	x10, x10, #0x1
   136a4:	str	x10, [x9], #8
   136a8:	b.cs	1369c <__gmpz_bin_ui@@Base+0x380>  // b.hs, b.nlast
   136ac:	ldrsw	x9, [x0, #4]
   136b0:	ldr	x8, [x8, x9, lsl #3]
   136b4:	cmp	x8, #0x0
   136b8:	cinc	w8, w9, ne  // ne = any
   136bc:	str	w8, [x0, #4]
   136c0:	ret
   136c4:	sub	sp, sp, #0x40
   136c8:	stp	x20, x19, [sp, #48]
   136cc:	sub	x20, x2, #0x1
   136d0:	mov	x19, x0
   136d4:	mov	x0, x3
   136d8:	mov	x2, x20
   136dc:	stp	x29, x30, [sp, #16]
   136e0:	stp	x22, x21, [sp, #32]
   136e4:	add	x29, sp, #0x10
   136e8:	mov	x21, x3
   136ec:	mov	x22, x1
   136f0:	bl	ca60 <__gmpz_add_ui@plt>
   136f4:	mov	x0, x19
   136f8:	mov	x1, x21
   136fc:	mov	x2, x21
   13700:	bl	c620 <__gmpz_mul@plt>
   13704:	mov	x0, x19
   13708:	mov	x1, x19
   1370c:	mov	x2, x22
   13710:	bl	d160 <__gmpz_add@plt>
   13714:	mov	x0, x19
   13718:	bl	13934 <__gmpz_bin_ui@@Base+0x618>
   1371c:	mov	x8, #0x100000000           	// #4294967296
   13720:	cmp	x20, x8
   13724:	b.hi	13754 <__gmpz_bin_ui@@Base+0x438>  // b.pmore
   13728:	and	x8, x20, #0x1
   1372c:	add	x8, x8, x20
   13730:	lsr	x9, x20, #1
   13734:	mul	x1, x8, x9
   13738:	mov	x0, x19
   1373c:	bl	13790 <__gmpz_bin_ui@@Base+0x474>
   13740:	ldp	x20, x19, [sp, #48]
   13744:	ldp	x22, x21, [sp, #32]
   13748:	ldp	x29, x30, [sp, #16]
   1374c:	add	sp, sp, #0x40
   13750:	ret
   13754:	and	x8, x20, #0x1
   13758:	add	x1, x8, x20
   1375c:	mov	x0, sp
   13760:	bl	d020 <__gmpz_init_set_ui@plt>
   13764:	lsr	x2, x20, #1
   13768:	mov	x0, sp
   1376c:	mov	x1, sp
   13770:	bl	c750 <__gmpz_mul_ui@plt>
   13774:	mov	x2, sp
   13778:	mov	x0, x19
   1377c:	mov	x1, x19
   13780:	bl	c3b0 <__gmpz_sub@plt>
   13784:	mov	x0, sp
   13788:	bl	cd10 <__gmpz_clear@plt>
   1378c:	b	13740 <__gmpz_bin_ui@@Base+0x424>
   13790:	ldr	x8, [x0, #8]
   13794:	ldr	x9, [x8]
   13798:	subs	x9, x9, x1
   1379c:	str	x9, [x8]
   137a0:	b.cs	137b8 <__gmpz_bin_ui@@Base+0x49c>  // b.hs, b.nlast
   137a4:	add	x9, x8, #0x8
   137a8:	ldr	x10, [x9]
   137ac:	sub	x11, x10, #0x1
   137b0:	str	x11, [x9], #8
   137b4:	cbz	x10, 137a8 <__gmpz_bin_ui@@Base+0x48c>
   137b8:	ldr	w9, [x0, #4]
   137bc:	sub	w10, w9, #0x1
   137c0:	ldr	x8, [x8, w10, sxtw #3]
   137c4:	cmp	x8, #0x0
   137c8:	cset	w8, eq  // eq = none
   137cc:	sub	w8, w9, w8
   137d0:	str	w8, [x0, #4]
   137d4:	ret
   137d8:	sub	sp, sp, #0x60
   137dc:	sub	x8, x3, x4
   137e0:	stp	x26, x25, [sp, #32]
   137e4:	stp	x22, x21, [sp, #64]
   137e8:	stp	x20, x19, [sp, #80]
   137ec:	mov	x19, x4
   137f0:	mov	x25, x3
   137f4:	mov	x21, x2
   137f8:	mov	x22, x1
   137fc:	cmp	x8, #0x4
   13800:	mov	x20, x0
   13804:	stp	x29, x30, [sp, #16]
   13808:	stp	x24, x23, [sp, #48]
   1380c:	add	x29, sp, #0x10
   13810:	b.hi	13864 <__gmpz_bin_ui@@Base+0x548>  // b.pmore
   13814:	lsl	x23, x25, #2
   13818:	add	x1, x23, #0x2
   1381c:	mov	x0, x22
   13820:	bl	13684 <__gmpz_bin_ui@@Base+0x368>
   13824:	mov	x0, x21
   13828:	mov	x1, x22
   1382c:	mov	x2, x23
   13830:	bl	d500 <__gmpz_addmul_ui@plt>
   13834:	mov	x0, x21
   13838:	mov	x1, x25
   1383c:	bl	13790 <__gmpz_bin_ui@@Base+0x474>
   13840:	mov	x0, x20
   13844:	mov	x1, x20
   13848:	mov	x2, x21
   1384c:	bl	c620 <__gmpz_mul@plt>
   13850:	sub	x25, x25, #0x1
   13854:	cmp	x25, x19
   13858:	sub	x23, x23, #0x4
   1385c:	b.hi	13818 <__gmpz_bin_ui@@Base+0x4fc>  // b.pmore
   13860:	b	13918 <__gmpz_bin_ui@@Base+0x5fc>
   13864:	add	x8, x19, x25
   13868:	lsr	x24, x8, #1
   1386c:	add	x26, x24, #0x1
   13870:	mov	x0, x20
   13874:	mov	x1, x22
   13878:	mov	x2, x21
   1387c:	mov	x3, x25
   13880:	mov	x4, x26
   13884:	mov	x23, x5
   13888:	bl	137d8 <__gmpz_bin_ui@@Base+0x4bc>
   1388c:	mov	w1, #0x2                   	// #2
   13890:	bfi	x1, x26, #2, #62
   13894:	mov	x0, x22
   13898:	lsl	x25, x26, #2
   1389c:	bl	13684 <__gmpz_bin_ui@@Base+0x368>
   138a0:	mov	x0, x21
   138a4:	mov	x1, x22
   138a8:	mov	x2, x25
   138ac:	bl	d500 <__gmpz_addmul_ui@plt>
   138b0:	mov	x0, x21
   138b4:	mov	x1, x26
   138b8:	bl	13790 <__gmpz_bin_ui@@Base+0x474>
   138bc:	cbz	x23, 138d4 <__gmpz_bin_ui@@Base+0x5b8>
   138c0:	mov	x0, x23
   138c4:	mov	x1, x21
   138c8:	str	wzr, [sp]
   138cc:	bl	c590 <__gmpz_set@plt>
   138d0:	b	138e4 <__gmpz_bin_ui@@Base+0x5c8>
   138d4:	mov	x0, sp
   138d8:	mov	x1, x21
   138dc:	mov	x23, sp
   138e0:	bl	c0b0 <__gmpz_init_set@plt>
   138e4:	mov	x0, x23
   138e8:	mov	x1, x22
   138ec:	mov	x2, x21
   138f0:	mov	x3, x24
   138f4:	mov	x4, x19
   138f8:	mov	x5, xzr
   138fc:	bl	137d8 <__gmpz_bin_ui@@Base+0x4bc>
   13900:	mov	x0, x20
   13904:	mov	x1, x20
   13908:	mov	x2, x23
   1390c:	bl	c620 <__gmpz_mul@plt>
   13910:	mov	x0, sp
   13914:	bl	cd10 <__gmpz_clear@plt>
   13918:	ldp	x20, x19, [sp, #80]
   1391c:	ldp	x22, x21, [sp, #64]
   13920:	ldp	x24, x23, [sp, #48]
   13924:	ldp	x26, x25, [sp, #32]
   13928:	ldp	x29, x30, [sp, #16]
   1392c:	add	sp, sp, #0x60
   13930:	ret
   13934:	stp	x29, x30, [sp, #-48]!
   13938:	stp	x20, x19, [sp, #32]
   1393c:	str	x21, [sp, #16]
   13940:	ldrsw	x20, [x0, #4]
   13944:	ldr	x21, [x0, #8]
   13948:	mov	x19, x0
   1394c:	mov	w3, #0x1                   	// #1
   13950:	mov	x2, x20
   13954:	mov	x0, x21
   13958:	mov	x1, x21
   1395c:	mov	x29, sp
   13960:	bl	c2f0 <__gmpn_rshift@plt>
   13964:	add	x8, x21, x20, lsl #3
   13968:	ldur	x8, [x8, #-8]
   1396c:	ldr	w9, [x19, #4]
   13970:	ldr	x21, [sp, #16]
   13974:	cmp	x8, #0x0
   13978:	cset	w8, eq  // eq = none
   1397c:	sub	w8, w9, w8
   13980:	str	w8, [x19, #4]
   13984:	ldp	x20, x19, [sp, #32]
   13988:	ldp	x29, x30, [sp], #48
   1398c:	ret

0000000000013990 <__gmpz_bin_uiui@@Base>:
   13990:	stp	x29, x30, [sp, #-32]!
   13994:	stp	x20, x19, [sp, #16]
   13998:	subs	x8, x1, x2
   1399c:	mov	x19, x0
   139a0:	mov	x29, sp
   139a4:	b.cc	13a68 <__gmpz_bin_uiui@@Base+0xd8>  // b.lo, b.ul, b.last
   139a8:	cmp	x8, x2
   139ac:	csel	x2, x2, x8, hi  // hi = pmore
   139b0:	cmp	x2, #0x1
   139b4:	b.hi	139d8 <__gmpz_bin_uiui@@Base+0x48>  // b.pmore
   139b8:	ldr	w8, [x19]
   139bc:	cmp	x2, #0x0
   139c0:	csinc	x20, x1, xzr, ne  // ne = any
   139c4:	cmp	w8, #0x0
   139c8:	b.le	13a70 <__gmpz_bin_uiui@@Base+0xe0>
   139cc:	ldr	x0, [x19, #8]
   139d0:	str	x20, [x0]
   139d4:	b	13a00 <__gmpz_bin_uiui@@Base+0x70>
   139d8:	cmp	x1, #0x43
   139dc:	b.hi	13a14 <__gmpz_bin_uiui@@Base+0x84>  // b.pmore
   139e0:	mov	w0, w1
   139e4:	mov	w1, w2
   139e8:	bl	13aa0 <__gmpz_bin_uiui@@Base+0x110>
   139ec:	ldr	w8, [x19]
   139f0:	cmp	w8, #0x0
   139f4:	b.le	13a80 <__gmpz_bin_uiui@@Base+0xf0>
   139f8:	ldr	x8, [x19, #8]
   139fc:	str	x0, [x8]
   13a00:	mov	w8, #0x1                   	// #1
   13a04:	str	w8, [x19, #4]
   13a08:	ldp	x20, x19, [sp, #16]
   13a0c:	ldp	x29, x30, [sp], #32
   13a10:	ret
   13a14:	cmp	x2, #0x19
   13a18:	b.hi	13a28 <__gmpz_bin_uiui@@Base+0x98>  // b.pmore
   13a1c:	mov	x0, x19
   13a20:	bl	13b0c <__gmpz_bin_uiui@@Base+0x17c>
   13a24:	b	13a08 <__gmpz_bin_uiui@@Base+0x78>
   13a28:	cmp	x2, #0x46
   13a2c:	b.hi	13a3c <__gmpz_bin_uiui@@Base+0xac>  // b.pmore
   13a30:	mov	x0, x19
   13a34:	bl	13d1c <__gmpz_bin_uiui@@Base+0x38c>
   13a38:	b	13a08 <__gmpz_bin_uiui@@Base+0x78>
   13a3c:	cmp	x2, #0x200
   13a40:	b.cc	13a5c <__gmpz_bin_uiui@@Base+0xcc>  // b.lo, b.ul, b.last
   13a44:	lsr	x8, x1, #4
   13a48:	cmp	x2, x8
   13a4c:	b.ls	13a5c <__gmpz_bin_uiui@@Base+0xcc>  // b.plast
   13a50:	mov	x0, x19
   13a54:	bl	13e8c <__gmpz_bin_uiui@@Base+0x4fc>
   13a58:	b	13a08 <__gmpz_bin_uiui@@Base+0x78>
   13a5c:	mov	x0, x19
   13a60:	bl	142d0 <__gmpz_bin_uiui@@Base+0x940>
   13a64:	b	13a08 <__gmpz_bin_uiui@@Base+0x78>
   13a68:	str	wzr, [x19, #4]
   13a6c:	b	13a08 <__gmpz_bin_uiui@@Base+0x78>
   13a70:	mov	w1, #0x1                   	// #1
   13a74:	mov	x0, x19
   13a78:	bl	c1c0 <__gmpz_realloc@plt>
   13a7c:	b	139d0 <__gmpz_bin_uiui@@Base+0x40>
   13a80:	mov	w1, #0x1                   	// #1
   13a84:	mov	x20, x0
   13a88:	mov	x0, x19
   13a8c:	bl	c1c0 <__gmpz_realloc@plt>
   13a90:	mov	x8, x0
   13a94:	mov	x0, x20
   13a98:	str	x20, [x8]
   13a9c:	b	13a00 <__gmpz_bin_uiui@@Base+0x70>
   13aa0:	adrp	x10, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13aa4:	sub	w11, w0, w1
   13aa8:	sub	w9, w1, #0x2
   13aac:	add	x10, x10, #0x7d8
   13ab0:	sub	w13, w11, #0x2
   13ab4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13ab8:	ldr	x9, [x10, w9, uxtw #3]
   13abc:	ldr	x10, [x10, w13, uxtw #3]
   13ac0:	adrp	x13, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13ac4:	ldr	x8, [x8, #3848]
   13ac8:	ldr	x13, [x13, #3992]
   13acc:	lsr	w12, w0, #1
   13ad0:	lsr	w14, w1, #1
   13ad4:	sub	w12, w12, #0x1
   13ad8:	sub	w14, w14, #0x1
   13adc:	lsr	w11, w11, #1
   13ae0:	ldr	x8, [x8, w0, uxtw #3]
   13ae4:	ldrb	w12, [x13, w12, uxtw]
   13ae8:	ldrb	w14, [x13, w14, uxtw]
   13aec:	sub	w11, w11, #0x1
   13af0:	ldrb	w11, [x13, w11, uxtw]
   13af4:	mul	x8, x9, x8
   13af8:	sub	w9, w12, w14
   13afc:	mul	x8, x8, x10
   13b00:	sub	w9, w9, w11
   13b04:	lsl	x0, x8, x9
   13b08:	ret
   13b0c:	sub	sp, sp, #0x70
   13b10:	stp	x24, x23, [sp, #64]
   13b14:	mov	x23, x0
   13b18:	mov	x0, x1
   13b1c:	stp	x29, x30, [sp, #16]
   13b20:	stp	x28, x27, [sp, #32]
   13b24:	stp	x26, x25, [sp, #48]
   13b28:	stp	x22, x21, [sp, #80]
   13b2c:	stp	x20, x19, [sp, #96]
   13b30:	add	x29, sp, #0x10
   13b34:	mov	x20, x2
   13b38:	mov	x21, x1
   13b3c:	bl	146c0 <__gmpz_bin_uiui@@Base+0xd30>
   13b40:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13b44:	ldr	x9, [x9, #3992]
   13b48:	cmp	w0, #0x8
   13b4c:	mov	w8, #0x8                   	// #8
   13b50:	csel	w26, w0, w8, cc  // cc = lo, ul, last
   13b54:	add	x8, x9, x20, lsr #1
   13b58:	ldurb	w24, [x8, #-1]
   13b5c:	sub	x8, x21, x20
   13b60:	cmp	x26, x20
   13b64:	add	x22, x8, #0x1
   13b68:	b.cs	13c80 <__gmpz_bin_uiui@@Base+0x2f0>  // b.hs, b.nlast
   13b6c:	clz	x8, x21
   13b70:	mov	w9, #0x40                  	// #64
   13b74:	sub	w8, w9, w8
   13b78:	ldrsw	x9, [x23]
   13b7c:	mul	x8, x8, x20
   13b80:	lsr	x8, x8, #6
   13b84:	add	x1, x8, #0x3
   13b88:	cmp	x1, x9
   13b8c:	str	x23, [sp, #8]
   13b90:	b.gt	13cfc <__gmpz_bin_uiui@@Base+0x36c>
   13b94:	ldr	x21, [x23, #8]
   13b98:	adrp	x27, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13b9c:	sub	w19, w26, #0x1
   13ba0:	add	x27, x27, #0x9a0
   13ba4:	ldr	x8, [x27, w19, uxtw #3]
   13ba8:	mov	x0, x22
   13bac:	blr	x8
   13bb0:	adrp	x28, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13bb4:	add	x28, x28, #0xb5f
   13bb8:	ldrb	w8, [x28, w19, uxtw]
   13bbc:	add	x23, x22, x26
   13bc0:	sub	w19, w20, w26
   13bc4:	mov	w2, #0x1                   	// #1
   13bc8:	sub	w22, w24, w8
   13bcc:	str	x0, [x21]
   13bd0:	cmp	w26, w19
   13bd4:	csel	w26, w26, w19, cc  // cc = lo, ul, last
   13bd8:	sub	w25, w26, #0x1
   13bdc:	ldr	x8, [x27, w25, uxtw #3]
   13be0:	mov	x0, x23
   13be4:	mov	x24, x2
   13be8:	blr	x8
   13bec:	ldrb	w8, [x28, w25, uxtw]
   13bf0:	mov	x3, x0
   13bf4:	mov	x0, x21
   13bf8:	mov	x1, x21
   13bfc:	mov	x2, x24
   13c00:	add	x23, x23, x26
   13c04:	sub	w22, w22, w8
   13c08:	bl	d670 <__gmpn_mul_1@plt>
   13c0c:	cmp	x0, #0x0
   13c10:	cinc	x2, x24, ne  // ne = any
   13c14:	subs	w19, w19, w26
   13c18:	str	x0, [x21, x24, lsl #3]
   13c1c:	b.ne	13bd0 <__gmpz_bin_uiui@@Base+0x240>  // b.any
   13c20:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13c24:	ldr	x9, [x9, #3848]
   13c28:	adrp	x10, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13c2c:	lsl	x8, x20, #3
   13c30:	add	x10, x10, #0x7d8
   13c34:	ldr	x3, [x9, x8]
   13c38:	add	x8, x8, x10
   13c3c:	ldur	x4, [x8, #-16]
   13c40:	mov	x25, x0
   13c44:	mov	x0, x21
   13c48:	mov	x1, x21
   13c4c:	mov	w5, w22
   13c50:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   13c54:	cmp	x25, #0x0
   13c58:	ldr	x23, [sp, #8]
   13c5c:	cinc	x9, x24, ne  // ne = any
   13c60:	cinc	w8, w24, ne  // ne = any
   13c64:	add	x9, x21, x9, lsl #3
   13c68:	add	w8, w8, #0x1
   13c6c:	sub	x9, x9, #0x8
   13c70:	ldr	x10, [x9], #-8
   13c74:	sub	w8, w8, #0x1
   13c78:	cbz	x10, 13c70 <__gmpz_bin_uiui@@Base+0x2e0>
   13c7c:	b	13cd8 <__gmpz_bin_uiui@@Base+0x348>
   13c80:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   13c84:	sub	x19, x20, #0x1
   13c88:	add	x8, x8, #0x9a0
   13c8c:	ldr	x8, [x8, x19, lsl #3]
   13c90:	mov	x0, x22
   13c94:	blr	x8
   13c98:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13c9c:	add	x8, x8, #0x7d8
   13ca0:	adrp	x9, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13ca4:	add	x9, x9, #0xb5f
   13ca8:	add	x8, x8, x20, lsl #3
   13cac:	ldrb	w9, [x9, x19]
   13cb0:	ldur	x8, [x8, #-16]
   13cb4:	ldr	w10, [x23]
   13cb8:	sub	w9, w24, w9
   13cbc:	mul	x8, x8, x0
   13cc0:	cmp	w10, #0x0
   13cc4:	lsr	x19, x8, x9
   13cc8:	b.le	13d0c <__gmpz_bin_uiui@@Base+0x37c>
   13ccc:	ldr	x0, [x23, #8]
   13cd0:	str	x19, [x0]
   13cd4:	mov	w8, #0x1                   	// #1
   13cd8:	str	w8, [x23, #4]
   13cdc:	ldp	x20, x19, [sp, #96]
   13ce0:	ldp	x22, x21, [sp, #80]
   13ce4:	ldp	x24, x23, [sp, #64]
   13ce8:	ldp	x26, x25, [sp, #48]
   13cec:	ldp	x28, x27, [sp, #32]
   13cf0:	ldp	x29, x30, [sp, #16]
   13cf4:	add	sp, sp, #0x70
   13cf8:	ret
   13cfc:	mov	x0, x23
   13d00:	bl	c1c0 <__gmpz_realloc@plt>
   13d04:	mov	x21, x0
   13d08:	b	13b98 <__gmpz_bin_uiui@@Base+0x208>
   13d0c:	mov	w1, #0x1                   	// #1
   13d10:	mov	x0, x23
   13d14:	bl	c1c0 <__gmpz_realloc@plt>
   13d18:	b	13cd0 <__gmpz_bin_uiui@@Base+0x340>
   13d1c:	sub	sp, sp, #0x190
   13d20:	stp	x20, x19, [sp, #384]
   13d24:	lsr	x20, x2, #1
   13d28:	stp	x22, x21, [sp, #368]
   13d2c:	mov	x21, x2
   13d30:	mov	x22, x1
   13d34:	mov	x19, x0
   13d38:	cmp	x2, #0x33
   13d3c:	mov	x2, x20
   13d40:	stp	x29, x30, [sp, #320]
   13d44:	str	x28, [sp, #336]
   13d48:	stp	x24, x23, [sp, #352]
   13d4c:	add	x29, sp, #0x140
   13d50:	b.hi	13d5c <__gmpz_bin_uiui@@Base+0x3cc>  // b.pmore
   13d54:	bl	13b0c <__gmpz_bin_uiui@@Base+0x17c>
   13d58:	b	13d60 <__gmpz_bin_uiui@@Base+0x3d0>
   13d5c:	bl	13d1c <__gmpz_bin_uiui@@Base+0x38c>
   13d60:	sub	x23, x22, x20
   13d64:	cmp	x23, #0x43
   13d68:	sub	x21, x21, x20
   13d6c:	b.hi	13db4 <__gmpz_bin_uiui@@Base+0x424>  // b.pmore
   13d70:	ldp	w8, w24, [x19]
   13d74:	sxtw	x24, w24
   13d78:	cmp	w24, w8
   13d7c:	b.ge	13e78 <__gmpz_bin_uiui@@Base+0x4e8>  // b.tcont
   13d80:	ldr	x22, [x19, #8]
   13d84:	mov	w0, w23
   13d88:	mov	w1, w21
   13d8c:	bl	13aa0 <__gmpz_bin_uiui@@Base+0x110>
   13d90:	mov	x3, x0
   13d94:	mov	x0, x22
   13d98:	mov	x1, x22
   13d9c:	mov	x2, x24
   13da0:	bl	d670 <__gmpn_mul_1@plt>
   13da4:	cmp	x0, #0x0
   13da8:	str	x0, [x22, x24, lsl #3]
   13dac:	cinc	x23, x24, ne  // ne = any
   13db0:	b	13dfc <__gmpz_bin_uiui@@Base+0x46c>
   13db4:	mov	w8, #0x26                  	// #38
   13db8:	add	x9, sp, #0x10
   13dbc:	cmp	x21, #0x19
   13dc0:	mov	x0, sp
   13dc4:	mov	x1, x23
   13dc8:	mov	x2, x21
   13dcc:	str	w8, [sp]
   13dd0:	str	x9, [sp, #8]
   13dd4:	b.hi	13de0 <__gmpz_bin_uiui@@Base+0x450>  // b.pmore
   13dd8:	bl	13b0c <__gmpz_bin_uiui@@Base+0x17c>
   13ddc:	b	13de4 <__gmpz_bin_uiui@@Base+0x454>
   13de0:	bl	13d1c <__gmpz_bin_uiui@@Base+0x38c>
   13de4:	mov	x2, sp
   13de8:	mov	x0, x19
   13dec:	mov	x1, x19
   13df0:	bl	c620 <__gmpz_mul@plt>
   13df4:	ldr	x22, [x19, #8]
   13df8:	ldrsw	x23, [x19, #4]
   13dfc:	adrp	x11, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13e00:	sub	x8, x21, #0xd
   13e04:	adrp	x9, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13e08:	adrp	x10, 4c000 <__gmp_randclear_mt@@Base+0x18>
   13e0c:	add	x11, x11, #0xb48
   13e10:	add	x9, x9, #0x9d8
   13e14:	add	x10, x10, #0xa90
   13e18:	lsl	x12, x8, #3
   13e1c:	ldrb	w8, [x11, x8]
   13e20:	ldr	x3, [x9, x12]
   13e24:	ldr	x4, [x10, x12]
   13e28:	cmp	x21, x20
   13e2c:	cset	w9, ne  // ne = any
   13e30:	sub	w5, w8, w9
   13e34:	mov	x0, x22
   13e38:	mov	x1, x22
   13e3c:	mov	x2, x23
   13e40:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   13e44:	sub	x8, x22, #0x8
   13e48:	ldr	x9, [x8, x23, lsl #3]
   13e4c:	sub	x23, x23, #0x1
   13e50:	cbz	x9, 13e48 <__gmpz_bin_uiui@@Base+0x4b8>
   13e54:	add	w8, w23, #0x1
   13e58:	str	w8, [x19, #4]
   13e5c:	ldp	x20, x19, [sp, #384]
   13e60:	ldp	x22, x21, [sp, #368]
   13e64:	ldp	x24, x23, [sp, #352]
   13e68:	ldr	x28, [sp, #336]
   13e6c:	ldp	x29, x30, [sp, #320]
   13e70:	add	sp, sp, #0x190
   13e74:	ret
   13e78:	add	x1, x24, #0x1
   13e7c:	mov	x0, x19
   13e80:	bl	c1c0 <__gmpz_realloc@plt>
   13e84:	mov	x22, x0
   13e88:	b	13d84 <__gmpz_bin_uiui@@Base+0x3f4>
   13e8c:	stp	x29, x30, [sp, #-96]!
   13e90:	stp	x28, x27, [sp, #16]
   13e94:	stp	x26, x25, [sp, #32]
   13e98:	stp	x24, x23, [sp, #48]
   13e9c:	stp	x22, x21, [sp, #64]
   13ea0:	stp	x20, x19, [sp, #80]
   13ea4:	mov	x29, sp
   13ea8:	sub	sp, sp, #0x30
   13eac:	mov	x19, x0
   13eb0:	mov	x0, x1
   13eb4:	mov	x23, x2
   13eb8:	mov	x21, x1
   13ebc:	stur	xzr, [x29, #-8]
   13ec0:	bl	147bc <__gmpz_bin_uiui@@Base+0xe2c>
   13ec4:	lsl	x1, x0, #3
   13ec8:	mov	w8, #0x7f00                	// #32512
   13ecc:	cmp	x1, x8
   13ed0:	stur	x19, [x29, #-32]
   13ed4:	b.hi	14270 <__gmpz_bin_uiui@@Base+0x8e0>  // b.pmore
   13ed8:	add	x9, x1, #0xf
   13edc:	mov	x8, sp
   13ee0:	and	x9, x9, #0xfffffffffffffff0
   13ee4:	sub	x20, x8, x9
   13ee8:	mov	sp, x20
   13eec:	mov	x0, x20
   13ef0:	mov	x1, x21
   13ef4:	bl	d3e0 <__gmp_primesieve@plt>
   13ef8:	add	x19, x0, #0x1
   13efc:	mov	x0, x21
   13f00:	bl	146c0 <__gmpz_bin_uiui@@Base+0xd30>
   13f04:	mov	w8, w0
   13f08:	udiv	x8, x19, x8
   13f0c:	lsl	x8, x8, #3
   13f10:	add	x1, x8, #0x8
   13f14:	mov	w8, #0x7f00                	// #32512
   13f18:	cmp	x1, x8
   13f1c:	b.hi	14280 <__gmpz_bin_uiui@@Base+0x8f0>  // b.pmore
   13f20:	add	x9, x1, #0xf
   13f24:	mov	x8, sp
   13f28:	and	x9, x9, #0xfffffffffffffff0
   13f2c:	sub	x8, x8, x9
   13f30:	stur	x8, [x29, #-16]
   13f34:	mov	sp, x8
   13f38:	mov	x8, #0xffffffffffffffff    	// #-1
   13f3c:	sub	x11, x21, x23
   13f40:	lsr	x9, x23, #1
   13f44:	udiv	x12, x8, x21
   13f48:	lsr	x8, x11, #1
   13f4c:	and	x9, x9, #0x5555555555555555
   13f50:	lsr	x10, x21, #1
   13f54:	and	x8, x8, #0x5555555555555555
   13f58:	sub	x9, x23, x9
   13f5c:	stp	x10, x11, [x29, #-48]
   13f60:	and	x10, x10, #0x5555555555555555
   13f64:	sub	x8, x11, x8
   13f68:	lsr	x11, x9, #2
   13f6c:	sub	x10, x21, x10
   13f70:	and	x9, x9, #0x3333333333333333
   13f74:	and	x11, x11, #0x3333333333333333
   13f78:	add	x9, x11, x9
   13f7c:	lsr	x11, x10, #2
   13f80:	and	x10, x10, #0x3333333333333333
   13f84:	and	x11, x11, #0x3333333333333333
   13f88:	add	x10, x11, x10
   13f8c:	lsr	x11, x8, #2
   13f90:	and	x8, x8, #0x3333333333333333
   13f94:	and	x11, x11, #0x3333333333333333
   13f98:	add	x9, x9, x9, lsr #4
   13f9c:	add	x8, x11, x8
   13fa0:	add	x10, x10, x10, lsr #4
   13fa4:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   13fa8:	add	x8, x8, x8, lsr #4
   13fac:	and	x10, x10, #0xf0f0f0f0f0f0f0f
   13fb0:	add	x9, x9, x9, lsr #8
   13fb4:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   13fb8:	add	x10, x10, x10, lsr #8
   13fbc:	add	x9, x9, x9, lsr #16
   13fc0:	add	x8, x8, x8, lsr #8
   13fc4:	add	x10, x10, x10, lsr #16
   13fc8:	lsr	x11, x9, #32
   13fcc:	add	x8, x8, x8, lsr #16
   13fd0:	add	w9, w11, w9
   13fd4:	lsr	x11, x10, #32
   13fd8:	add	w10, w11, w10
   13fdc:	lsr	x11, x8, #32
   13fe0:	and	x9, x9, #0xff
   13fe4:	add	w8, w11, w8
   13fe8:	sub	x9, x9, w10, uxtb
   13fec:	add	x8, x9, w8, uxtb
   13ff0:	mov	w9, #0x1                   	// #1
   13ff4:	lsl	x28, x9, x8
   13ff8:	cmp	x28, x12
   13ffc:	stur	x12, [x29, #-24]
   14000:	b.ls	14018 <__gmpz_bin_uiui@@Base+0x688>  // b.plast
   14004:	ldur	x8, [x29, #-16]
   14008:	mov	w19, #0x1                   	// #1
   1400c:	str	x28, [x8]
   14010:	mov	w28, #0x1                   	// #1
   14014:	b	1401c <__gmpz_bin_uiui@@Base+0x68c>
   14018:	mov	x19, xzr
   1401c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   14020:	mov	x8, xzr
   14024:	movk	x9, #0xaaab
   14028:	mov	x11, x23
   1402c:	mov	x10, x21
   14030:	umulh	x12, x11, x9
   14034:	lsr	x12, x12, #1
   14038:	add	x13, x12, x12, lsl #1
   1403c:	sub	x11, x11, x13
   14040:	umulh	x13, x10, x9
   14044:	lsr	x13, x13, #1
   14048:	add	x8, x11, x8
   1404c:	add	x11, x13, x13, lsl #1
   14050:	sub	x11, x10, x11
   14054:	cmp	x11, x8
   14058:	add	x11, x28, x28, lsl #1
   1405c:	cset	w8, cc  // cc = lo, ul, last
   14060:	csel	x28, x11, x28, cc  // cc = lo, ul, last
   14064:	cmp	x10, #0x8
   14068:	mov	x11, x12
   1406c:	mov	x10, x13
   14070:	b.hi	14030 <__gmpz_bin_uiui@@Base+0x6a0>  // b.pmore
   14074:	mov	x0, x21
   14078:	bl	147d8 <__gmpz_bin_uiui@@Base+0xe48>
   1407c:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   14080:	mov	x27, x0
   14084:	mov	w0, #0x5                   	// #5
   14088:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   1408c:	mov	w8, #0x1                   	// #1
   14090:	mov	x26, x0
   14094:	lsr	x22, x0, #6
   14098:	lsl	x24, x8, x0
   1409c:	b	140b4 <__gmpz_bin_uiui@@Base+0x724>
   140a0:	ror	x25, x24, #63
   140a4:	cmp	x26, x27
   140a8:	add	x22, x22, x24, lsr #63
   140ac:	mov	x24, x25
   140b0:	b.hi	14130 <__gmpz_bin_uiui@@Base+0x7a0>  // b.pmore
   140b4:	ldr	x8, [x20, x22, lsl #3]
   140b8:	add	x26, x26, #0x1
   140bc:	tst	x8, x24
   140c0:	b.ne	140a0 <__gmpz_bin_uiui@@Base+0x710>  // b.any
   140c4:	mov	x0, x26
   140c8:	bl	1481c <__gmpz_bin_uiui@@Base+0xe8c>
   140cc:	ldur	x8, [x29, #-24]
   140d0:	cmp	x28, x8
   140d4:	b.ls	140ec <__gmpz_bin_uiui@@Base+0x75c>  // b.plast
   140d8:	ldur	x9, [x29, #-16]
   140dc:	add	x8, x19, #0x1
   140e0:	str	x28, [x9, x19, lsl #3]
   140e4:	mov	x19, x8
   140e8:	mov	w28, #0x1                   	// #1
   140ec:	mov	x8, xzr
   140f0:	mov	x9, x21
   140f4:	mov	x10, x23
   140f8:	udiv	x11, x10, x0
   140fc:	udiv	x12, x9, x0
   14100:	msub	x10, x11, x0, x10
   14104:	msub	x9, x12, x0, x9
   14108:	add	x8, x10, x8
   1410c:	cmp	x9, x8
   14110:	csinc	x9, x0, xzr, cc  // cc = lo, ul, last
   14114:	cset	w8, cc  // cc = lo, ul, last
   14118:	cmp	x12, x0
   1411c:	mul	x28, x9, x28
   14120:	mov	x9, x12
   14124:	mov	x10, x11
   14128:	b.cs	140f8 <__gmpz_bin_uiui@@Base+0x768>  // b.hs, b.nlast
   1412c:	b	140a0 <__gmpz_bin_uiui@@Base+0x710>
   14130:	ldur	x8, [x29, #-24]
   14134:	ldur	x0, [x29, #-48]
   14138:	lsl	x27, x8, #1
   1413c:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   14140:	mov	x24, x0
   14144:	b	14160 <__gmpz_bin_uiui@@Base+0x7d0>
   14148:	mul	x28, x0, x28
   1414c:	ror	x8, x25, #63
   14150:	cmp	x26, x24
   14154:	add	x22, x22, x25, lsr #63
   14158:	mov	x25, x8
   1415c:	b.hi	141b0 <__gmpz_bin_uiui@@Base+0x820>  // b.pmore
   14160:	ldr	x8, [x20, x22, lsl #3]
   14164:	add	x26, x26, #0x1
   14168:	tst	x8, x25
   1416c:	b.ne	1414c <__gmpz_bin_uiui@@Base+0x7bc>  // b.any
   14170:	mov	x0, x26
   14174:	bl	1481c <__gmpz_bin_uiui@@Base+0xe8c>
   14178:	udiv	x8, x21, x0
   1417c:	udiv	x9, x23, x0
   14180:	msub	x8, x8, x0, x21
   14184:	msub	x9, x9, x0, x23
   14188:	cmp	x8, x9
   1418c:	b.cs	1414c <__gmpz_bin_uiui@@Base+0x7bc>  // b.hs, b.nlast
   14190:	cmp	x28, x27
   14194:	b.ls	14148 <__gmpz_bin_uiui@@Base+0x7b8>  // b.plast
   14198:	ldur	x9, [x29, #-16]
   1419c:	add	x8, x19, #0x1
   141a0:	str	x28, [x9, x19, lsl #3]
   141a4:	mov	x19, x8
   141a8:	mov	x28, x0
   141ac:	b	1414c <__gmpz_bin_uiui@@Base+0x7bc>
   141b0:	ldur	x8, [x29, #-24]
   141b4:	ldur	x0, [x29, #-40]
   141b8:	and	x22, x8, #0x7fffffffffffffff
   141bc:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   141c0:	add	x23, x0, #0x1
   141c4:	mov	w8, #0x1                   	// #1
   141c8:	mov	x0, x21
   141cc:	lsr	x24, x23, #6
   141d0:	lsl	x25, x8, x23
   141d4:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   141d8:	mov	x21, x0
   141dc:	b	141f8 <__gmpz_bin_uiui@@Base+0x868>
   141e0:	mul	x28, x0, x28
   141e4:	ror	x8, x25, #63
   141e8:	cmp	x23, x21
   141ec:	add	x24, x24, x25, lsr #63
   141f0:	mov	x25, x8
   141f4:	b.hi	14230 <__gmpz_bin_uiui@@Base+0x8a0>  // b.pmore
   141f8:	ldr	x8, [x20, x24, lsl #3]
   141fc:	add	x23, x23, #0x1
   14200:	tst	x8, x25
   14204:	b.ne	141e4 <__gmpz_bin_uiui@@Base+0x854>  // b.any
   14208:	mov	x0, x23
   1420c:	bl	1481c <__gmpz_bin_uiui@@Base+0xe8c>
   14210:	cmp	x28, x22
   14214:	b.ls	141e0 <__gmpz_bin_uiui@@Base+0x850>  // b.plast
   14218:	ldur	x9, [x29, #-16]
   1421c:	add	x8, x19, #0x1
   14220:	str	x28, [x9, x19, lsl #3]
   14224:	mov	x19, x8
   14228:	mov	x28, x0
   1422c:	b	141e4 <__gmpz_bin_uiui@@Base+0x854>
   14230:	cbz	x19, 14290 <__gmpz_bin_uiui@@Base+0x900>
   14234:	ldur	x1, [x29, #-16]
   14238:	ldur	x0, [x29, #-32]
   1423c:	add	x2, x19, #0x1
   14240:	str	x28, [x1, x19, lsl #3]
   14244:	bl	cf40 <__gmpz_prodlimbs@plt>
   14248:	ldur	x0, [x29, #-8]
   1424c:	cbnz	x0, 142b8 <__gmpz_bin_uiui@@Base+0x928>
   14250:	mov	sp, x29
   14254:	ldp	x20, x19, [sp, #80]
   14258:	ldp	x22, x21, [sp, #64]
   1425c:	ldp	x24, x23, [sp, #48]
   14260:	ldp	x26, x25, [sp, #32]
   14264:	ldp	x28, x27, [sp, #16]
   14268:	ldp	x29, x30, [sp], #96
   1426c:	ret
   14270:	sub	x0, x29, #0x8
   14274:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   14278:	mov	x20, x0
   1427c:	b	13eec <__gmpz_bin_uiui@@Base+0x55c>
   14280:	sub	x0, x29, #0x8
   14284:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   14288:	stur	x0, [x29, #-16]
   1428c:	b	13f38 <__gmpz_bin_uiui@@Base+0x5a8>
   14290:	ldur	x19, [x29, #-32]
   14294:	ldr	w8, [x19]
   14298:	cmp	w8, #0x0
   1429c:	b.le	142c0 <__gmpz_bin_uiui@@Base+0x930>
   142a0:	ldr	x0, [x19, #8]
   142a4:	mov	w8, #0x1                   	// #1
   142a8:	str	x28, [x0]
   142ac:	str	w8, [x19, #4]
   142b0:	ldur	x0, [x29, #-8]
   142b4:	cbz	x0, 14250 <__gmpz_bin_uiui@@Base+0x8c0>
   142b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   142bc:	b	14250 <__gmpz_bin_uiui@@Base+0x8c0>
   142c0:	mov	w1, #0x1                   	// #1
   142c4:	mov	x0, x19
   142c8:	bl	c1c0 <__gmpz_realloc@plt>
   142cc:	b	142a4 <__gmpz_bin_uiui@@Base+0x914>
   142d0:	stp	x29, x30, [sp, #-96]!
   142d4:	stp	x28, x27, [sp, #16]
   142d8:	stp	x26, x25, [sp, #32]
   142dc:	stp	x24, x23, [sp, #48]
   142e0:	stp	x22, x21, [sp, #64]
   142e4:	stp	x20, x19, [sp, #80]
   142e8:	mov	x29, sp
   142ec:	sub	sp, sp, #0x40
   142f0:	lsr	x8, x1, #6
   142f4:	add	x8, x8, x8, lsl #1
   142f8:	add	x10, x8, #0x3
   142fc:	cmp	x8, #0x27
   14300:	lsr	x8, x10, #1
   14304:	mov	w9, #0x27                  	// #39
   14308:	add	x8, x8, #0x13
   1430c:	csel	x8, x9, x8, cc  // cc = lo, ul, last
   14310:	cmp	x8, x2
   14314:	csel	x8, x8, x2, lt  // lt = tstop
   14318:	lsl	x9, x8, #3
   1431c:	mov	x21, x1
   14320:	add	x1, x9, #0xb0
   14324:	mov	w9, #0x7f00                	// #32512
   14328:	cmp	x1, x9
   1432c:	add	x19, x8, #0x1
   14330:	stp	x2, xzr, [x29, #-16]
   14334:	stur	x0, [x29, #-48]
   14338:	b.hi	14698 <__gmpz_bin_uiui@@Base+0xd08>  // b.pmore
   1433c:	add	x9, x1, #0xf
   14340:	mov	x8, sp
   14344:	and	x9, x9, #0xfffffffffffffff0
   14348:	sub	x20, x8, x9
   1434c:	mov	sp, x20
   14350:	mov	x0, x21
   14354:	add	x24, x20, x19, lsl #3
   14358:	bl	146c0 <__gmpz_bin_uiui@@Base+0xd30>
   1435c:	ldur	x19, [x29, #-16]
   14360:	mov	w22, w0
   14364:	mov	x0, x19
   14368:	bl	146c0 <__gmpz_bin_uiui@@Base+0xd30>
   1436c:	mov	x9, #0x41ef                	// #16879
   14370:	movk	x9, #0x7ec2, lsl #16
   14374:	sub	x10, x21, x19
   14378:	movk	x9, #0x8186, lsl #32
   1437c:	mov	w27, w0
   14380:	stur	x21, [x29, #-56]
   14384:	mov	x21, x19
   14388:	mov	w23, #0x1                   	// #1
   1438c:	movk	x9, #0x3352, lsl #48
   14390:	mov	w8, #0x1a                  	// #26
   14394:	add	x26, x10, #0x1
   14398:	mov	w25, #0x1                   	// #1
   1439c:	stur	x10, [x29, #-64]
   143a0:	str	x23, [x20]
   143a4:	stur	w22, [x29, #-28]
   143a8:	stur	x24, [x29, #-24]
   143ac:	stur	x20, [x29, #-40]
   143b0:	sub	x10, x21, x8
   143b4:	add	x11, x10, #0x1
   143b8:	mov	w12, w27
   143bc:	cmp	x11, x12
   143c0:	csinc	x27, x12, x10, hi  // hi = pmore
   143c4:	str	x9, [x24]
   143c8:	cbz	w27, 14448 <__gmpz_bin_uiui@@Base+0xab8>
   143cc:	mov	w19, #0x1                   	// #1
   143d0:	mov	x28, x8
   143d4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   143d8:	sub	w8, w27, #0x1
   143dc:	add	x9, x9, #0x9a0
   143e0:	ldr	x8, [x9, w8, uxtw #3]
   143e4:	mov	x0, x28
   143e8:	blr	x8
   143ec:	rbit	x8, x0
   143f0:	clz	x8, x8
   143f4:	and	x22, x27, #0xffffffff
   143f8:	lsr	x3, x0, x8
   143fc:	mov	x0, x24
   14400:	mov	x1, x24
   14404:	mov	x2, x19
   14408:	add	x28, x22, x28
   1440c:	bl	d670 <__gmpn_mul_1@plt>
   14410:	sub	x8, x21, x28
   14414:	cmp	x0, #0x0
   14418:	add	x9, x8, #0x1
   1441c:	str	x0, [x24, x19, lsl #3]
   14420:	cinc	x19, x19, ne  // ne = any
   14424:	cmp	x22, x9
   14428:	csinc	x27, x22, x8, cc  // cc = lo, ul, last
   1442c:	cmp	x19, #0x13
   14430:	b.hi	14438 <__gmpz_bin_uiui@@Base+0xaa8>  // b.pmore
   14434:	cbnz	x27, 143d4 <__gmpz_bin_uiui@@Base+0xa44>
   14438:	ldur	x20, [x29, #-40]
   1443c:	subs	w22, w28, w25
   14440:	b.ne	14458 <__gmpz_bin_uiui@@Base+0xac8>  // b.any
   14444:	b	144b0 <__gmpz_bin_uiui@@Base+0xb20>
   14448:	mov	x28, x8
   1444c:	mov	w19, #0x1                   	// #1
   14450:	subs	w22, w28, w25
   14454:	b.eq	144b0 <__gmpz_bin_uiui@@Base+0xb20>  // b.none
   14458:	ldur	w24, [x29, #-28]
   1445c:	adrp	x25, 69000 <__gmp_limbroots_table@@Base+0x11338>
   14460:	add	x25, x25, #0x9a0
   14464:	cmp	w24, w22
   14468:	csel	w21, w24, w22, cc  // cc = lo, ul, last
   1446c:	sub	w8, w21, #0x1
   14470:	ldr	x8, [x25, w8, uxtw #3]
   14474:	mov	x0, x26
   14478:	blr	x8
   1447c:	rbit	x8, x0
   14480:	clz	x8, x8
   14484:	lsr	x3, x0, x8
   14488:	mov	x0, x20
   1448c:	mov	x1, x20
   14490:	mov	x2, x23
   14494:	add	x26, x26, x21
   14498:	bl	d670 <__gmpn_mul_1@plt>
   1449c:	cmp	x0, #0x0
   144a0:	str	x0, [x20, x23, lsl #3]
   144a4:	cinc	x23, x23, ne  // ne = any
   144a8:	subs	w22, w22, w21
   144ac:	b.ne	14464 <__gmpz_bin_uiui@@Base+0xad4>  // b.any
   144b0:	ldur	x24, [x29, #-24]
   144b4:	add	x9, x20, x23, lsl #3
   144b8:	adrp	x13, 69000 <__gmp_limbroots_table@@Base+0x11338>
   144bc:	ldur	x9, [x9, #-8]
   144c0:	ldr	x8, [x24]
   144c4:	add	x10, x24, x19, lsl #3
   144c8:	ldur	x10, [x10, #-8]
   144cc:	ldr	x13, [x13, #3952]
   144d0:	ubfx	x12, x8, #1, #7
   144d4:	sub	x11, x23, x19
   144d8:	cmp	x9, x10
   144dc:	ldrb	w12, [x13, x12]
   144e0:	mov	w10, #0x2                   	// #2
   144e4:	cinc	x23, x11, cs  // cs = hs, nlast
   144e8:	cmp	x19, x23
   144ec:	msub	x9, x8, x12, x10
   144f0:	mul	x9, x9, x12
   144f4:	msub	x10, x9, x8, x10
   144f8:	mul	x9, x9, x10
   144fc:	orr	x10, xzr, #0xfffffffffffffffe
   14500:	madd	x8, x9, x8, x10
   14504:	csel	x4, x19, x23, lt  // lt = tstop
   14508:	mul	x5, x8, x9
   1450c:	mov	x0, x20
   14510:	mov	x1, x20
   14514:	mov	x2, x23
   14518:	mov	x3, x24
   1451c:	bl	c680 <__gmpn_sbpi1_bdiv_q@plt>
   14520:	mov	x0, x20
   14524:	mov	x1, x20
   14528:	mov	x2, x23
   1452c:	bl	ce90 <__gmpn_neg@plt>
   14530:	cbz	w27, 14568 <__gmpz_bin_uiui@@Base+0xbd8>
   14534:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   14538:	sub	w8, w27, #0x1
   1453c:	add	x9, x9, #0x9a0
   14540:	ldr	x8, [x9, w8, uxtw #3]
   14544:	mov	x0, x28
   14548:	blr	x8
   1454c:	ldur	x21, [x29, #-16]
   14550:	rbit	x9, x0
   14554:	clz	x9, x9
   14558:	add	x8, x28, w27, uxtw
   1455c:	lsr	x9, x0, x9
   14560:	mov	x25, x28
   14564:	b	143b0 <__gmpz_bin_uiui@@Base+0xa20>
   14568:	ldp	x9, x11, [x29, #-64]
   1456c:	ldur	x10, [x29, #-16]
   14570:	lsr	x8, x9, #1
   14574:	and	x8, x8, #0x5555555555555555
   14578:	sub	x8, x9, x8
   1457c:	lsr	x9, x10, #1
   14580:	and	x9, x9, #0x5555555555555555
   14584:	sub	x9, x10, x9
   14588:	lsr	x10, x11, #1
   1458c:	and	x10, x10, #0x5555555555555555
   14590:	sub	x10, x11, x10
   14594:	lsr	x11, x8, #2
   14598:	and	x8, x8, #0x3333333333333333
   1459c:	and	x11, x11, #0x3333333333333333
   145a0:	add	x8, x11, x8
   145a4:	lsr	x11, x9, #2
   145a8:	and	x9, x9, #0x3333333333333333
   145ac:	and	x11, x11, #0x3333333333333333
   145b0:	add	x9, x11, x9
   145b4:	lsr	x11, x10, #2
   145b8:	and	x10, x10, #0x3333333333333333
   145bc:	and	x11, x11, #0x3333333333333333
   145c0:	add	x8, x8, x8, lsr #4
   145c4:	add	x10, x11, x10
   145c8:	add	x9, x9, x9, lsr #4
   145cc:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   145d0:	add	x10, x10, x10, lsr #4
   145d4:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   145d8:	add	x8, x8, x8, lsr #8
   145dc:	and	x10, x10, #0xf0f0f0f0f0f0f0f
   145e0:	add	x9, x9, x9, lsr #8
   145e4:	add	x8, x8, x8, lsr #16
   145e8:	add	x10, x10, x10, lsr #8
   145ec:	add	x9, x9, x9, lsr #16
   145f0:	lsr	x11, x8, #32
   145f4:	add	x10, x10, x10, lsr #16
   145f8:	add	w8, w11, w8
   145fc:	lsr	x11, x9, #32
   14600:	add	w9, w11, w9
   14604:	lsr	x11, x10, #32
   14608:	and	w9, w9, #0xff
   1460c:	add	w10, w11, w10
   14610:	sub	w9, w9, w10, uxtb
   14614:	adds	w3, w9, w8, uxtb
   14618:	b.eq	14638 <__gmpz_bin_uiui@@Base+0xca8>  // b.none
   1461c:	mov	x0, x20
   14620:	mov	x1, x20
   14624:	mov	x2, x23
   14628:	bl	c2d0 <__gmpn_lshift@plt>
   1462c:	cmp	x0, #0x0
   14630:	str	x0, [x20, x23, lsl #3]
   14634:	cinc	x23, x23, ne  // ne = any
   14638:	ldur	x21, [x29, #-48]
   1463c:	add	x8, x20, x23, lsl #3
   14640:	ldur	x8, [x8, #-8]
   14644:	ldrsw	x9, [x21]
   14648:	cmp	x8, #0x0
   1464c:	cset	w8, eq  // eq = none
   14650:	sub	x19, x23, x8
   14654:	cmp	x19, x9
   14658:	b.gt	146a8 <__gmpz_bin_uiui@@Base+0xd18>
   1465c:	ldr	x0, [x21, #8]
   14660:	mov	x1, x20
   14664:	mov	x2, x19
   14668:	str	w19, [x21, #4]
   1466c:	bl	cc10 <__gmpn_copyi@plt>
   14670:	ldur	x0, [x29, #-8]
   14674:	cbnz	x0, 146b8 <__gmpz_bin_uiui@@Base+0xd28>
   14678:	mov	sp, x29
   1467c:	ldp	x20, x19, [sp, #80]
   14680:	ldp	x22, x21, [sp, #64]
   14684:	ldp	x24, x23, [sp, #48]
   14688:	ldp	x26, x25, [sp, #32]
   1468c:	ldp	x28, x27, [sp, #16]
   14690:	ldp	x29, x30, [sp], #96
   14694:	ret
   14698:	sub	x0, x29, #0x8
   1469c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   146a0:	mov	x20, x0
   146a4:	b	14350 <__gmpz_bin_uiui@@Base+0x9c0>
   146a8:	mov	x0, x21
   146ac:	mov	x1, x19
   146b0:	bl	c1c0 <__gmpz_realloc@plt>
   146b4:	b	14660 <__gmpz_bin_uiui@@Base+0xcd0>
   146b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   146bc:	b	14678 <__gmpz_bin_uiui@@Base+0xce8>
   146c0:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   146c4:	ldr	x9, [x9, #3880]
   146c8:	mov	x8, x0
   146cc:	mov	w0, #0x9                   	// #9
   146d0:	sub	w10, w0, #0x2
   146d4:	ldr	x10, [x9, w10, uxtw #3]
   146d8:	sub	w0, w0, #0x1
   146dc:	cmp	x10, x8
   146e0:	b.cc	146d0 <__gmpz_bin_uiui@@Base+0xd40>  // b.lo, b.ul, b.last
   146e4:	ret
   146e8:	ret
   146ec:	add	x9, x0, #0x1
   146f0:	orr	x8, x0, #0x1
   146f4:	lsr	x9, x9, #1
   146f8:	mul	x0, x9, x8
   146fc:	ret
   14700:	add	x8, x0, #0x1
   14704:	mul	x8, x8, x0
   14708:	lsr	x8, x8, #1
   1470c:	add	x9, x0, #0x2
   14710:	mul	x0, x8, x9
   14714:	ret
   14718:	add	x8, x0, #0x3
   1471c:	mul	x8, x8, x0
   14720:	lsr	x8, x8, #1
   14724:	add	x9, x8, #0x1
   14728:	mul	x0, x9, x8
   1472c:	ret
   14730:	add	x8, x0, #0x3
   14734:	mul	x8, x8, x0
   14738:	add	x9, x0, #0x4
   1473c:	lsr	x8, x8, #1
   14740:	mul	x9, x8, x9
   14744:	add	x8, x8, #0x1
   14748:	mul	x0, x9, x8
   1474c:	ret
   14750:	add	x8, x0, #0x5
   14754:	mul	x8, x8, x0
   14758:	add	x9, x8, #0x5
   1475c:	mul	x9, x9, x9
   14760:	lsr	x9, x9, #3
   14764:	lsr	x8, x8, #1
   14768:	mul	x0, x9, x8
   1476c:	ret
   14770:	add	x8, x0, #0x5
   14774:	mul	x8, x8, x0
   14778:	add	x9, x0, #0x6
   1477c:	add	x10, x8, #0x5
   14780:	mul	x8, x8, x9
   14784:	mul	x9, x10, x10
   14788:	lsr	x9, x9, #3
   1478c:	lsr	x8, x8, #1
   14790:	mul	x0, x9, x8
   14794:	ret
   14798:	add	x8, x0, #0x7
   1479c:	mul	x8, x8, x0
   147a0:	add	x9, x8, #0xa
   147a4:	mul	x9, x9, x8
   147a8:	lsr	x9, x9, #3
   147ac:	add	x8, x8, x9
   147b0:	add	x8, x8, #0x9
   147b4:	mul	x0, x8, x9
   147b8:	ret
   147bc:	stp	x29, x30, [sp, #-16]!
   147c0:	mov	x29, sp
   147c4:	bl	14800 <__gmpz_bin_uiui@@Base+0xe70>
   147c8:	lsr	x8, x0, #6
   147cc:	add	x0, x8, #0x1
   147d0:	ldp	x29, x30, [sp], #16
   147d4:	ret
   147d8:	clz	x8, x0
   147dc:	mov	w9, #0x40                  	// #64
   147e0:	sub	w8, w9, w8
   147e4:	mov	w10, #0x1                   	// #1
   147e8:	asr	w8, w8, #1
   147ec:	lsl	x9, x10, x8
   147f0:	lsr	x8, x0, x8
   147f4:	add	x8, x9, x8
   147f8:	lsr	x0, x8, #1
   147fc:	ret
   14800:	sub	x8, x0, #0x5
   14804:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   14808:	orr	x8, x8, #0x1
   1480c:	movk	x9, #0xaaab
   14810:	umulh	x8, x8, x9
   14814:	lsr	x0, x8, #1
   14818:	ret
   1481c:	add	x8, x0, x0, lsl #1
   14820:	and	x9, x0, #0x1
   14824:	add	x8, x8, x9
   14828:	add	x0, x8, #0x1
   1482c:	ret

0000000000014830 <__gmpz_cdiv_q@@Base>:
   14830:	stp	x29, x30, [sp, #-64]!
   14834:	str	x23, [sp, #16]
   14838:	stp	x22, x21, [sp, #32]
   1483c:	stp	x20, x19, [sp, #48]
   14840:	mov	x29, sp
   14844:	sub	sp, sp, #0x10
   14848:	ldrsw	x22, [x2, #4]
   1484c:	ldr	w23, [x1, #4]
   14850:	mov	x21, x1
   14854:	mov	w9, #0x7f00                	// #32512
   14858:	cmp	x22, #0x0
   1485c:	cneg	x8, x22, mi  // mi = first
   14860:	lsl	x1, x8, #3
   14864:	mov	x20, x2
   14868:	mov	x19, x0
   1486c:	cmp	x1, x9
   14870:	str	xzr, [x29, #24]
   14874:	stur	w8, [x29, #-16]
   14878:	b.hi	148e8 <__gmpz_cdiv_q@@Base+0xb8>  // b.pmore
   1487c:	add	x9, x1, #0xf
   14880:	mov	x8, sp
   14884:	and	x9, x9, #0xfffffffffffffff0
   14888:	sub	x0, x8, x9
   1488c:	mov	sp, x0
   14890:	stur	x0, [x29, #-8]
   14894:	sub	x1, x29, #0x10
   14898:	mov	x0, x19
   1489c:	mov	x2, x21
   148a0:	mov	x3, x20
   148a4:	bl	c120 <__gmpz_tdiv_qr@plt>
   148a8:	eor	w8, w22, w23
   148ac:	tbnz	w8, #31, 148c8 <__gmpz_cdiv_q@@Base+0x98>
   148b0:	ldur	w8, [x29, #-12]
   148b4:	cbz	w8, 148c8 <__gmpz_cdiv_q@@Base+0x98>
   148b8:	mov	w2, #0x1                   	// #1
   148bc:	mov	x0, x19
   148c0:	mov	x1, x19
   148c4:	bl	ca60 <__gmpz_add_ui@plt>
   148c8:	ldr	x0, [x29, #24]
   148cc:	cbnz	x0, 148f4 <__gmpz_cdiv_q@@Base+0xc4>
   148d0:	mov	sp, x29
   148d4:	ldp	x20, x19, [sp, #48]
   148d8:	ldp	x22, x21, [sp, #32]
   148dc:	ldr	x23, [sp, #16]
   148e0:	ldp	x29, x30, [sp], #64
   148e4:	ret
   148e8:	add	x0, x29, #0x18
   148ec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   148f0:	b	14890 <__gmpz_cdiv_q@@Base+0x60>
   148f4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   148f8:	b	148d0 <__gmpz_cdiv_q@@Base+0xa0>

00000000000148fc <__gmpz_cdiv_q_ui@@Base>:
   148fc:	stp	x29, x30, [sp, #-64]!
   14900:	stp	x24, x23, [sp, #16]
   14904:	stp	x22, x21, [sp, #32]
   14908:	stp	x20, x19, [sp, #48]
   1490c:	mov	x29, sp
   14910:	cbz	x2, 149cc <__gmpz_cdiv_q_ui@@Base+0xd0>
   14914:	ldrsw	x24, [x1, #4]
   14918:	mov	x23, x1
   1491c:	mov	x19, x0
   14920:	cbz	w24, 14998 <__gmpz_cdiv_q_ui@@Base+0x9c>
   14924:	ldrsw	x8, [x19]
   14928:	cmp	w24, #0x0
   1492c:	cneg	x21, x24, lt  // lt = tstop
   14930:	mov	x20, x2
   14934:	cmp	x21, x8
   14938:	b.gt	149b8 <__gmpz_cdiv_q_ui@@Base+0xbc>
   1493c:	ldr	x22, [x19, #8]
   14940:	ldr	x2, [x23, #8]
   14944:	mov	x0, x22
   14948:	mov	x1, xzr
   1494c:	mov	x3, x21
   14950:	mov	x4, x20
   14954:	bl	ced0 <__gmpn_divrem_1@plt>
   14958:	tbnz	w24, #31, 14978 <__gmpz_cdiv_q_ui@@Base+0x7c>
   1495c:	cbz	x0, 14978 <__gmpz_cdiv_q_ui@@Base+0x7c>
   14960:	mov	x8, x22
   14964:	ldr	x9, [x8]
   14968:	adds	x9, x9, #0x1
   1496c:	str	x9, [x8], #8
   14970:	b.cs	14964 <__gmpz_cdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   14974:	sub	x0, x20, x0
   14978:	add	x8, x22, x21, lsl #3
   1497c:	ldur	x8, [x8, #-8]
   14980:	cmp	x8, #0x0
   14984:	cset	w8, eq  // eq = none
   14988:	sub	w8, w21, w8
   1498c:	cmp	w24, #0x0
   14990:	cneg	w8, w8, lt  // lt = tstop
   14994:	b	149a0 <__gmpz_cdiv_q_ui@@Base+0xa4>
   14998:	mov	w8, wzr
   1499c:	mov	x0, xzr
   149a0:	str	w8, [x19, #4]
   149a4:	ldp	x20, x19, [sp, #48]
   149a8:	ldp	x22, x21, [sp, #32]
   149ac:	ldp	x24, x23, [sp, #16]
   149b0:	ldp	x29, x30, [sp], #64
   149b4:	ret
   149b8:	mov	x0, x19
   149bc:	mov	x1, x21
   149c0:	bl	c1c0 <__gmpz_realloc@plt>
   149c4:	mov	x22, x0
   149c8:	b	14940 <__gmpz_cdiv_q_ui@@Base+0x44>
   149cc:	bl	c100 <__gmp_divide_by_zero@plt>

00000000000149d0 <__gmpz_cdiv_qr@@Base>:
   149d0:	stp	x29, x30, [sp, #-64]!
   149d4:	str	x23, [sp, #16]
   149d8:	stp	x22, x21, [sp, #32]
   149dc:	stp	x20, x19, [sp, #48]
   149e0:	mov	x29, sp
   149e4:	sub	sp, sp, #0x10
   149e8:	ldrsw	x23, [x3, #4]
   149ec:	mov	x20, x3
   149f0:	mov	x22, x2
   149f4:	mov	x19, x1
   149f8:	mov	x21, x0
   149fc:	cmp	x0, x3
   14a00:	str	xzr, [x29, #24]
   14a04:	b.eq	14a10 <__gmpz_cdiv_qr@@Base+0x40>  // b.none
   14a08:	cmp	x19, x20
   14a0c:	b.ne	14a54 <__gmpz_cdiv_qr@@Base+0x84>  // b.any
   14a10:	cmp	x23, #0x0
   14a14:	cneg	x8, x23, mi  // mi = first
   14a18:	lsl	x1, x8, #3
   14a1c:	mov	w9, #0x7f00                	// #32512
   14a20:	cmp	x1, x9
   14a24:	stur	w8, [x29, #-16]
   14a28:	b.hi	14ac4 <__gmpz_cdiv_qr@@Base+0xf4>  // b.pmore
   14a2c:	add	x9, x1, #0xf
   14a30:	mov	x8, sp
   14a34:	and	x9, x9, #0xfffffffffffffff0
   14a38:	sub	x0, x8, x9
   14a3c:	mov	sp, x0
   14a40:	stur	x0, [x29, #-8]
   14a44:	sub	x0, x29, #0x10
   14a48:	mov	x1, x20
   14a4c:	bl	c590 <__gmpz_set@plt>
   14a50:	sub	x20, x29, #0x10
   14a54:	ldr	w8, [x22, #4]
   14a58:	mov	x0, x21
   14a5c:	mov	x1, x19
   14a60:	mov	x2, x22
   14a64:	mov	x3, x20
   14a68:	eor	w23, w8, w23
   14a6c:	bl	c120 <__gmpz_tdiv_qr@plt>
   14a70:	tbnz	w23, #31, 14a9c <__gmpz_cdiv_qr@@Base+0xcc>
   14a74:	ldr	w8, [x19, #4]
   14a78:	cbz	w8, 14a9c <__gmpz_cdiv_qr@@Base+0xcc>
   14a7c:	mov	w2, #0x1                   	// #1
   14a80:	mov	x0, x21
   14a84:	mov	x1, x21
   14a88:	bl	ca60 <__gmpz_add_ui@plt>
   14a8c:	mov	x0, x19
   14a90:	mov	x1, x19
   14a94:	mov	x2, x20
   14a98:	bl	c3b0 <__gmpz_sub@plt>
   14a9c:	ldr	x0, [x29, #24]
   14aa0:	cbnz	x0, 14abc <__gmpz_cdiv_qr@@Base+0xec>
   14aa4:	mov	sp, x29
   14aa8:	ldp	x20, x19, [sp, #48]
   14aac:	ldp	x22, x21, [sp, #32]
   14ab0:	ldr	x23, [sp, #16]
   14ab4:	ldp	x29, x30, [sp], #64
   14ab8:	ret
   14abc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   14ac0:	b	14aa4 <__gmpz_cdiv_qr@@Base+0xd4>
   14ac4:	add	x0, x29, #0x18
   14ac8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   14acc:	b	14a40 <__gmpz_cdiv_qr@@Base+0x70>

0000000000014ad0 <__gmpz_cdiv_qr_ui@@Base>:
   14ad0:	stp	x29, x30, [sp, #-80]!
   14ad4:	str	x25, [sp, #16]
   14ad8:	stp	x24, x23, [sp, #32]
   14adc:	stp	x22, x21, [sp, #48]
   14ae0:	stp	x20, x19, [sp, #64]
   14ae4:	mov	x29, sp
   14ae8:	cbz	x3, 14bf4 <__gmpz_cdiv_qr_ui@@Base+0x124>
   14aec:	ldrsw	x25, [x2, #4]
   14af0:	mov	x22, x2
   14af4:	mov	x20, x1
   14af8:	mov	x19, x0
   14afc:	cbz	w25, 14b78 <__gmpz_cdiv_qr_ui@@Base+0xa8>
   14b00:	ldrsw	x8, [x19]
   14b04:	cmp	w25, #0x0
   14b08:	cneg	x21, x25, lt  // lt = tstop
   14b0c:	mov	x24, x3
   14b10:	cmp	x21, x8
   14b14:	b.gt	14bd0 <__gmpz_cdiv_qr_ui@@Base+0x100>
   14b18:	ldr	x23, [x19, #8]
   14b1c:	ldr	x2, [x22, #8]
   14b20:	mov	x0, x23
   14b24:	mov	x1, xzr
   14b28:	mov	x3, x21
   14b2c:	mov	x4, x24
   14b30:	bl	ced0 <__gmpn_divrem_1@plt>
   14b34:	mov	x22, x0
   14b38:	cbz	x0, 14b8c <__gmpz_cdiv_qr_ui@@Base+0xbc>
   14b3c:	tbnz	w25, #31, 14b58 <__gmpz_cdiv_qr_ui@@Base+0x88>
   14b40:	mov	x8, x23
   14b44:	ldr	x9, [x8]
   14b48:	adds	x9, x9, #0x1
   14b4c:	str	x9, [x8], #8
   14b50:	b.cs	14b44 <__gmpz_cdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   14b54:	sub	x22, x24, x22
   14b58:	ldr	w8, [x20]
   14b5c:	cmp	w8, #0x0
   14b60:	b.le	14be4 <__gmpz_cdiv_qr_ui@@Base+0x114>
   14b64:	ldr	x0, [x20, #8]
   14b68:	cmp	x22, #0x0
   14b6c:	csetm	w8, ne  // ne = any
   14b70:	str	x22, [x0]
   14b74:	b	14b90 <__gmpz_cdiv_qr_ui@@Base+0xc0>
   14b78:	mov	w8, wzr
   14b7c:	mov	x22, xzr
   14b80:	str	wzr, [x19, #4]
   14b84:	mov	x19, x20
   14b88:	b	14bb0 <__gmpz_cdiv_qr_ui@@Base+0xe0>
   14b8c:	mov	w8, wzr
   14b90:	str	w8, [x20, #4]
   14b94:	add	x8, x23, x21, lsl #3
   14b98:	ldur	x8, [x8, #-8]
   14b9c:	cmp	x8, #0x0
   14ba0:	cset	w8, eq  // eq = none
   14ba4:	sub	w8, w21, w8
   14ba8:	cmp	w25, #0x0
   14bac:	cneg	w8, w8, lt  // lt = tstop
   14bb0:	str	w8, [x19, #4]
   14bb4:	mov	x0, x22
   14bb8:	ldp	x20, x19, [sp, #64]
   14bbc:	ldp	x22, x21, [sp, #48]
   14bc0:	ldp	x24, x23, [sp, #32]
   14bc4:	ldr	x25, [sp, #16]
   14bc8:	ldp	x29, x30, [sp], #80
   14bcc:	ret
   14bd0:	mov	x0, x19
   14bd4:	mov	x1, x21
   14bd8:	bl	c1c0 <__gmpz_realloc@plt>
   14bdc:	mov	x23, x0
   14be0:	b	14b1c <__gmpz_cdiv_qr_ui@@Base+0x4c>
   14be4:	mov	w1, #0x1                   	// #1
   14be8:	mov	x0, x20
   14bec:	bl	c1c0 <__gmpz_realloc@plt>
   14bf0:	b	14b68 <__gmpz_cdiv_qr_ui@@Base+0x98>
   14bf4:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000014bf8 <__gmpz_cdiv_r@@Base>:
   14bf8:	stp	x29, x30, [sp, #-48]!
   14bfc:	stp	x22, x21, [sp, #16]
   14c00:	stp	x20, x19, [sp, #32]
   14c04:	mov	x29, sp
   14c08:	sub	sp, sp, #0x20
   14c0c:	ldrsw	x22, [x2, #4]
   14c10:	mov	x19, x2
   14c14:	mov	x21, x1
   14c18:	mov	x20, x0
   14c1c:	cmp	x0, x2
   14c20:	stur	xzr, [x29, #-24]
   14c24:	b.ne	14c6c <__gmpz_cdiv_r@@Base+0x74>  // b.any
   14c28:	cmp	x22, #0x0
   14c2c:	cneg	x8, x22, mi  // mi = first
   14c30:	lsl	x1, x8, #3
   14c34:	mov	w9, #0x7f00                	// #32512
   14c38:	cmp	x1, x9
   14c3c:	stur	w8, [x29, #-16]
   14c40:	b.hi	14cc4 <__gmpz_cdiv_r@@Base+0xcc>  // b.pmore
   14c44:	add	x9, x1, #0xf
   14c48:	mov	x8, sp
   14c4c:	and	x9, x9, #0xfffffffffffffff0
   14c50:	sub	x0, x8, x9
   14c54:	mov	sp, x0
   14c58:	stur	x0, [x29, #-8]
   14c5c:	sub	x0, x29, #0x10
   14c60:	mov	x1, x19
   14c64:	bl	c590 <__gmpz_set@plt>
   14c68:	sub	x19, x29, #0x10
   14c6c:	mov	x0, x20
   14c70:	mov	x1, x21
   14c74:	mov	x2, x19
   14c78:	bl	cc40 <__gmpz_tdiv_r@plt>
   14c7c:	ldr	w8, [x21, #4]
   14c80:	eor	w8, w8, w22
   14c84:	tbnz	w8, #31, 14ca0 <__gmpz_cdiv_r@@Base+0xa8>
   14c88:	ldr	w8, [x20, #4]
   14c8c:	cbz	w8, 14ca0 <__gmpz_cdiv_r@@Base+0xa8>
   14c90:	mov	x0, x20
   14c94:	mov	x1, x20
   14c98:	mov	x2, x19
   14c9c:	bl	c3b0 <__gmpz_sub@plt>
   14ca0:	ldur	x0, [x29, #-24]
   14ca4:	cbnz	x0, 14cbc <__gmpz_cdiv_r@@Base+0xc4>
   14ca8:	mov	sp, x29
   14cac:	ldp	x20, x19, [sp, #32]
   14cb0:	ldp	x22, x21, [sp, #16]
   14cb4:	ldp	x29, x30, [sp], #48
   14cb8:	ret
   14cbc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   14cc0:	b	14ca8 <__gmpz_cdiv_r@@Base+0xb0>
   14cc4:	sub	x0, x29, #0x18
   14cc8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   14ccc:	b	14c58 <__gmpz_cdiv_r@@Base+0x60>

0000000000014cd0 <__gmpz_cdiv_r_ui@@Base>:
   14cd0:	stp	x29, x30, [sp, #-48]!
   14cd4:	str	x21, [sp, #16]
   14cd8:	stp	x20, x19, [sp, #32]
   14cdc:	mov	x29, sp
   14ce0:	cbz	x2, 14d60 <__gmpz_cdiv_r_ui@@Base+0x90>
   14ce4:	ldrsw	x21, [x1, #4]
   14ce8:	mov	x19, x0
   14cec:	cbz	w21, 14d30 <__gmpz_cdiv_r_ui@@Base+0x60>
   14cf0:	ldr	x0, [x1, #8]
   14cf4:	cmp	w21, #0x0
   14cf8:	cneg	x1, x21, lt  // lt = tstop
   14cfc:	mov	x20, x2
   14d00:	bl	c540 <__gmpn_mod_1@plt>
   14d04:	cbz	x0, 14d30 <__gmpz_cdiv_r_ui@@Base+0x60>
   14d08:	ldr	w8, [x19]
   14d0c:	sub	x9, x20, x0
   14d10:	cmp	w21, #0x0
   14d14:	csel	x20, x9, x0, ge  // ge = tcont
   14d18:	cmp	w8, #0x0
   14d1c:	b.le	14d50 <__gmpz_cdiv_r_ui@@Base+0x80>
   14d20:	ldr	x0, [x19, #8]
   14d24:	mov	w8, #0xffffffff            	// #-1
   14d28:	str	x20, [x0]
   14d2c:	b	14d38 <__gmpz_cdiv_r_ui@@Base+0x68>
   14d30:	mov	w8, wzr
   14d34:	mov	x20, xzr
   14d38:	str	w8, [x19, #4]
   14d3c:	mov	x0, x20
   14d40:	ldp	x20, x19, [sp, #32]
   14d44:	ldr	x21, [sp, #16]
   14d48:	ldp	x29, x30, [sp], #48
   14d4c:	ret
   14d50:	mov	w1, #0x1                   	// #1
   14d54:	mov	x0, x19
   14d58:	bl	c1c0 <__gmpz_realloc@plt>
   14d5c:	b	14d24 <__gmpz_cdiv_r_ui@@Base+0x54>
   14d60:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000014d64 <__gmpz_cdiv_ui@@Base>:
   14d64:	stp	x29, x30, [sp, #-32]!
   14d68:	stp	x20, x19, [sp, #16]
   14d6c:	mov	x29, sp
   14d70:	cbz	x1, 14dbc <__gmpz_cdiv_ui@@Base+0x58>
   14d74:	ldrsw	x20, [x0, #4]
   14d78:	cbz	w20, 14dac <__gmpz_cdiv_ui@@Base+0x48>
   14d7c:	ldr	x0, [x0, #8]
   14d80:	mov	x19, x1
   14d84:	cmp	w20, #0x0
   14d88:	cneg	x1, x20, lt  // lt = tstop
   14d8c:	mov	x2, x19
   14d90:	bl	c540 <__gmpn_mod_1@plt>
   14d94:	cmp	x0, #0x0
   14d98:	mov	w9, #0xffffffff            	// #-1
   14d9c:	sub	x8, x19, x0
   14da0:	ccmp	w20, w9, #0x4, ne  // ne = any
   14da4:	csel	x0, x8, x0, gt
   14da8:	b	14db0 <__gmpz_cdiv_ui@@Base+0x4c>
   14dac:	mov	x0, xzr
   14db0:	ldp	x20, x19, [sp, #16]
   14db4:	ldp	x29, x30, [sp], #32
   14db8:	ret
   14dbc:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000014dc0 <__gmpz_cdiv_q_2exp@@Base>:
   14dc0:	stp	x29, x30, [sp, #-16]!
   14dc4:	mov	w3, #0x1                   	// #1
   14dc8:	mov	x29, sp
   14dcc:	bl	14dd8 <__gmpz_cdiv_q_2exp@@Base+0x18>
   14dd0:	ldp	x29, x30, [sp], #16
   14dd4:	ret
   14dd8:	stp	x29, x30, [sp, #-80]!
   14ddc:	stp	x26, x25, [sp, #16]
   14de0:	stp	x24, x23, [sp, #32]
   14de4:	stp	x22, x21, [sp, #48]
   14de8:	stp	x20, x19, [sp, #64]
   14dec:	ldrsw	x25, [x1, #4]
   14df0:	ldrsw	x8, [x0]
   14df4:	lsr	x26, x2, #6
   14df8:	mov	x19, x0
   14dfc:	cmp	x25, #0x0
   14e00:	cneg	x9, x25, mi  // mi = first
   14e04:	sub	x20, x9, x26
   14e08:	cmp	x20, #0x0
   14e0c:	mov	w23, w3
   14e10:	mov	x29, sp
   14e14:	b.le	14ea0 <__gmpz_cdiv_q_2exp@@Base+0xe0>
   14e18:	mov	x22, x2
   14e1c:	mov	x24, x1
   14e20:	cmp	x20, x8
   14e24:	b.ge	14f28 <__gmpz_cdiv_q_2exp@@Base+0x168>  // b.tcont
   14e28:	ldr	x21, [x19, #8]
   14e2c:	ldr	x9, [x24, #8]
   14e30:	eor	w8, w25, w23
   14e34:	mov	x23, xzr
   14e38:	tbnz	w8, #31, 14e58 <__gmpz_cdiv_q_2exp@@Base+0x98>
   14e3c:	cbz	x26, 14e58 <__gmpz_cdiv_q_2exp@@Base+0x98>
   14e40:	mov	x10, xzr
   14e44:	ldr	x23, [x9, x10, lsl #3]
   14e48:	add	x10, x10, #0x1
   14e4c:	cmp	x10, x26
   14e50:	b.cs	14e58 <__gmpz_cdiv_q_2exp@@Base+0x98>  // b.hs, b.nlast
   14e54:	cbz	x23, 14e44 <__gmpz_cdiv_q_2exp@@Base+0x84>
   14e58:	ands	x3, x22, #0x3f
   14e5c:	add	x1, x9, x26, lsl #3
   14e60:	b.eq	14ec8 <__gmpz_cdiv_q_2exp@@Base+0x108>  // b.none
   14e64:	mov	w9, #0xffffffff            	// #-1
   14e68:	eor	w8, w9, w8, asr #31
   14e6c:	mov	x0, x21
   14e70:	mov	x2, x20
   14e74:	sxtw	x22, w8
   14e78:	bl	c2f0 <__gmpn_rshift@plt>
   14e7c:	add	x8, x21, x20, lsl #3
   14e80:	ldur	x8, [x8, #-8]
   14e84:	and	x9, x0, x22
   14e88:	orr	x23, x9, x23
   14e8c:	cmp	x8, #0x0
   14e90:	cset	w8, eq  // eq = none
   14e94:	sub	x20, x20, x8
   14e98:	cbnz	x23, 14ed8 <__gmpz_cdiv_q_2exp@@Base+0x118>
   14e9c:	b	14f04 <__gmpz_cdiv_q_2exp@@Base+0x144>
   14ea0:	cmp	w8, #0x0
   14ea4:	b.le	14f3c <__gmpz_cdiv_q_2exp@@Base+0x17c>
   14ea8:	ldr	x0, [x19, #8]
   14eac:	eor	w9, w25, w23
   14eb0:	cmp	w9, #0x0
   14eb4:	mov	w8, #0x1                   	// #1
   14eb8:	ccmp	w25, #0x0, #0x4, ge  // ge = tcont
   14ebc:	str	x8, [x0]
   14ec0:	csel	w8, wzr, w23, eq  // eq = none
   14ec4:	b	14f0c <__gmpz_cdiv_q_2exp@@Base+0x14c>
   14ec8:	mov	x0, x21
   14ecc:	mov	x2, x20
   14ed0:	bl	cc10 <__gmpn_copyi@plt>
   14ed4:	cbz	x23, 14f04 <__gmpz_cdiv_q_2exp@@Base+0x144>
   14ed8:	cbz	x20, 14efc <__gmpz_cdiv_q_2exp@@Base+0x13c>
   14edc:	mov	w3, #0x1                   	// #1
   14ee0:	mov	x0, x21
   14ee4:	mov	x1, x21
   14ee8:	mov	x2, x20
   14eec:	bl	c150 <__gmpn_add_1@plt>
   14ef0:	str	x0, [x21, x20, lsl #3]
   14ef4:	add	x20, x0, x20
   14ef8:	b	14f04 <__gmpz_cdiv_q_2exp@@Base+0x144>
   14efc:	mov	w20, #0x1                   	// #1
   14f00:	str	x20, [x21]
   14f04:	cmp	w25, #0x0
   14f08:	cneg	w8, w20, lt  // lt = tstop
   14f0c:	str	w8, [x19, #4]
   14f10:	ldp	x20, x19, [sp, #64]
   14f14:	ldp	x22, x21, [sp, #48]
   14f18:	ldp	x24, x23, [sp, #32]
   14f1c:	ldp	x26, x25, [sp, #16]
   14f20:	ldp	x29, x30, [sp], #80
   14f24:	ret
   14f28:	add	x1, x20, #0x1
   14f2c:	mov	x0, x19
   14f30:	bl	c1c0 <__gmpz_realloc@plt>
   14f34:	mov	x21, x0
   14f38:	b	14e2c <__gmpz_cdiv_q_2exp@@Base+0x6c>
   14f3c:	mov	w1, #0x1                   	// #1
   14f40:	mov	x0, x19
   14f44:	bl	c1c0 <__gmpz_realloc@plt>
   14f48:	b	14eac <__gmpz_cdiv_q_2exp@@Base+0xec>

0000000000014f4c <__gmpz_fdiv_q_2exp@@Base>:
   14f4c:	stp	x29, x30, [sp, #-16]!
   14f50:	mov	w3, #0xffffffff            	// #-1
   14f54:	mov	x29, sp
   14f58:	bl	14dd8 <__gmpz_cdiv_q_2exp@@Base+0x18>
   14f5c:	ldp	x29, x30, [sp], #16
   14f60:	ret

0000000000014f64 <__gmpz_cdiv_r_2exp@@Base>:
   14f64:	stp	x29, x30, [sp, #-16]!
   14f68:	mov	w3, #0x1                   	// #1
   14f6c:	mov	x29, sp
   14f70:	bl	14f7c <__gmpz_cdiv_r_2exp@@Base+0x18>
   14f74:	ldp	x29, x30, [sp], #16
   14f78:	ret
   14f7c:	stp	x29, x30, [sp, #-96]!
   14f80:	stp	x26, x25, [sp, #32]
   14f84:	stp	x24, x23, [sp, #48]
   14f88:	stp	x22, x21, [sp, #64]
   14f8c:	stp	x20, x19, [sp, #80]
   14f90:	str	x27, [sp, #16]
   14f94:	ldr	w27, [x1, #4]
   14f98:	mov	x19, x0
   14f9c:	mov	x29, sp
   14fa0:	cbz	w27, 150f0 <__gmpz_cdiv_r_2exp@@Base+0x18c>
   14fa4:	ldr	x20, [x1, #8]
   14fa8:	sxtw	x23, w27
   14fac:	cmp	x23, #0x0
   14fb0:	mov	x21, x1
   14fb4:	lsr	x24, x2, #6
   14fb8:	and	x25, x2, #0x3f
   14fbc:	eor	w8, w27, w3
   14fc0:	cneg	x26, x23, mi  // mi = first
   14fc4:	tbnz	w8, #31, 15058 <__gmpz_cdiv_r_2exp@@Base+0xf4>
   14fc8:	cmp	x26, x24
   14fcc:	b.le	15004 <__gmpz_cdiv_r_2exp@@Base+0xa0>
   14fd0:	cbz	x24, 14ff0 <__gmpz_cdiv_r_2exp@@Base+0x8c>
   14fd4:	mov	x8, x20
   14fd8:	mov	x9, x24
   14fdc:	ldr	x10, [x8]
   14fe0:	cbnz	x10, 15004 <__gmpz_cdiv_r_2exp@@Base+0xa0>
   14fe4:	subs	x9, x9, #0x1
   14fe8:	add	x8, x8, #0x8
   14fec:	b.ne	14fdc <__gmpz_cdiv_r_2exp@@Base+0x78>  // b.any
   14ff0:	ldr	x8, [x20, x24, lsl #3]
   14ff4:	mov	x9, #0xffffffffffffffff    	// #-1
   14ff8:	lsl	x9, x9, x25
   14ffc:	bics	xzr, x8, x9
   15000:	b.eq	150ec <__gmpz_cdiv_r_2exp@@Base+0x188>  // b.none
   15004:	ldrsw	x8, [x19]
   15008:	neg	x23, x23
   1500c:	add	x22, x24, #0x1
   15010:	cmp	x24, x8
   15014:	b.ge	15110 <__gmpz_cdiv_r_2exp@@Base+0x1ac>  // b.tcont
   15018:	ldr	x20, [x19, #8]
   1501c:	ldr	x1, [x21, #8]
   15020:	cmp	x26, x24
   15024:	csel	x21, x22, x26, gt
   15028:	mov	x0, x20
   1502c:	mov	x2, x21
   15030:	bl	ce90 <__gmpn_neg@plt>
   15034:	cmp	x21, x24
   15038:	b.hi	150a0 <__gmpz_cdiv_r_2exp@@Base+0x13c>  // b.pmore
   1503c:	sub	x8, x24, x21
   15040:	lsl	x8, x8, #3
   15044:	add	x0, x20, x21, lsl #3
   15048:	add	x2, x8, #0x8
   1504c:	mov	w1, #0xff                  	// #255
   15050:	bl	c780 <memset@plt>
   15054:	b	150a0 <__gmpz_cdiv_r_2exp@@Base+0x13c>
   15058:	cmp	x19, x21
   1505c:	b.eq	15098 <__gmpz_cdiv_r_2exp@@Base+0x134>  // b.none
   15060:	ldrsw	x8, [x19]
   15064:	cmp	x26, x24
   15068:	csinc	x21, x26, x24, le
   1506c:	cmp	x21, x8
   15070:	b.gt	15124 <__gmpz_cdiv_r_2exp@@Base+0x1c0>
   15074:	ldr	x22, [x19, #8]
   15078:	mov	x0, x22
   1507c:	mov	x1, x20
   15080:	mov	x2, x21
   15084:	bl	cc10 <__gmpn_copyi@plt>
   15088:	cmp	x26, x24
   1508c:	mov	x20, x22
   15090:	b.gt	150a0 <__gmpz_cdiv_r_2exp@@Base+0x13c>
   15094:	b	150f0 <__gmpz_cdiv_r_2exp@@Base+0x18c>
   15098:	cmp	x26, x24
   1509c:	b.le	150f4 <__gmpz_cdiv_r_2exp@@Base+0x190>
   150a0:	lsl	x8, x24, #3
   150a4:	ldr	x9, [x20, x8]
   150a8:	mov	x10, #0xffffffffffffffff    	// #-1
   150ac:	lsl	x10, x10, x25
   150b0:	bics	x9, x9, x10
   150b4:	str	x9, [x20, x8]
   150b8:	b.eq	150c4 <__gmpz_cdiv_r_2exp@@Base+0x160>  // b.none
   150bc:	mov	x8, x24
   150c0:	b	150dc <__gmpz_cdiv_r_2exp@@Base+0x178>
   150c4:	sub	x9, x20, #0x8
   150c8:	subs	x8, x24, #0x1
   150cc:	b.lt	150ec <__gmpz_cdiv_r_2exp@@Base+0x188>  // b.tstop
   150d0:	ldr	x10, [x9, x24, lsl #3]
   150d4:	mov	x24, x8
   150d8:	cbz	x10, 150c8 <__gmpz_cdiv_r_2exp@@Base+0x164>
   150dc:	mvn	w9, w8
   150e0:	cmp	x23, #0x0
   150e4:	csinc	w27, w9, w8, lt  // lt = tstop
   150e8:	b	150f0 <__gmpz_cdiv_r_2exp@@Base+0x18c>
   150ec:	mov	w27, wzr
   150f0:	str	w27, [x19, #4]
   150f4:	ldp	x20, x19, [sp, #80]
   150f8:	ldp	x22, x21, [sp, #64]
   150fc:	ldp	x24, x23, [sp, #48]
   15100:	ldp	x26, x25, [sp, #32]
   15104:	ldr	x27, [sp, #16]
   15108:	ldp	x29, x30, [sp], #96
   1510c:	ret
   15110:	mov	x0, x19
   15114:	mov	x1, x22
   15118:	bl	c1c0 <__gmpz_realloc@plt>
   1511c:	mov	x20, x0
   15120:	b	1501c <__gmpz_cdiv_r_2exp@@Base+0xb8>
   15124:	mov	x0, x19
   15128:	mov	x1, x21
   1512c:	bl	c1c0 <__gmpz_realloc@plt>
   15130:	mov	x22, x0
   15134:	b	15078 <__gmpz_cdiv_r_2exp@@Base+0x114>

0000000000015138 <__gmpz_fdiv_r_2exp@@Base>:
   15138:	stp	x29, x30, [sp, #-16]!
   1513c:	mov	w3, #0xffffffff            	// #-1
   15140:	mov	x29, sp
   15144:	bl	14f7c <__gmpz_cdiv_r_2exp@@Base+0x18>
   15148:	ldp	x29, x30, [sp], #16
   1514c:	ret

0000000000015150 <__gmpz_clear@@Base>:
   15150:	stp	x29, x30, [sp, #-16]!
   15154:	ldrsw	x8, [x0]
   15158:	mov	x29, sp
   1515c:	cbz	w8, 15178 <__gmpz_clear@@Base+0x28>
   15160:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   15164:	ldr	x9, [x9, #4016]
   15168:	ldr	x0, [x0, #8]
   1516c:	lsl	x1, x8, #3
   15170:	ldr	x9, [x9]
   15174:	blr	x9
   15178:	ldp	x29, x30, [sp], #16
   1517c:	ret

0000000000015180 <__gmpz_clears@@Base>:
   15180:	sub	sp, sp, #0x100
   15184:	stp	x29, x30, [sp, #224]
   15188:	add	x29, sp, #0xe0
   1518c:	mov	x8, #0xffffffffffffffc8    	// #-56
   15190:	mov	x9, sp
   15194:	sub	x10, x29, #0x58
   15198:	movk	x8, #0xff80, lsl #32
   1519c:	add	x11, x29, #0x20
   151a0:	add	x9, x9, #0x80
   151a4:	add	x10, x10, #0x38
   151a8:	str	x19, [sp, #240]
   151ac:	stp	x1, x2, [x29, #-88]
   151b0:	stp	x3, x4, [x29, #-72]
   151b4:	stp	x5, x6, [x29, #-56]
   151b8:	stur	x7, [x29, #-40]
   151bc:	stp	q0, q1, [sp]
   151c0:	stp	q2, q3, [sp, #32]
   151c4:	stp	q4, q5, [sp, #64]
   151c8:	stp	q6, q7, [sp, #96]
   151cc:	stp	x9, x8, [x29, #-16]
   151d0:	stp	x11, x10, [x29, #-32]
   151d4:	adrp	x19, 69000 <__gmp_limbroots_table@@Base+0x11338>
   151d8:	ldr	x19, [x19, #4016]
   151dc:	b	151f4 <__gmpz_clears@@Base+0x74>
   151e0:	ldur	x8, [x29, #-32]
   151e4:	add	x9, x8, #0x8
   151e8:	stur	x9, [x29, #-32]
   151ec:	ldr	x0, [x8]
   151f0:	cbz	x0, 15234 <__gmpz_clears@@Base+0xb4>
   151f4:	ldrsw	x8, [x0]
   151f8:	cbz	w8, 1520c <__gmpz_clears@@Base+0x8c>
   151fc:	ldr	x9, [x19]
   15200:	ldr	x0, [x0, #8]
   15204:	lsl	x1, x8, #3
   15208:	blr	x9
   1520c:	ldursw	x8, [x29, #-8]
   15210:	tbz	w8, #31, 151e0 <__gmpz_clears@@Base+0x60>
   15214:	add	w9, w8, #0x8
   15218:	cmp	w9, #0x0
   1521c:	stur	w9, [x29, #-8]
   15220:	b.gt	151e0 <__gmpz_clears@@Base+0x60>
   15224:	ldur	x9, [x29, #-24]
   15228:	add	x8, x9, x8
   1522c:	ldr	x0, [x8]
   15230:	cbnz	x0, 151f4 <__gmpz_clears@@Base+0x74>
   15234:	ldr	x19, [sp, #240]
   15238:	ldp	x29, x30, [sp, #224]
   1523c:	add	sp, sp, #0x100
   15240:	ret

0000000000015244 <__gmpz_clrbit@@Base>:
   15244:	sub	sp, sp, #0x40
   15248:	stp	x29, x30, [sp, #16]
   1524c:	stp	x20, x19, [sp, #48]
   15250:	ldrsw	x8, [x0, #4]
   15254:	ldr	x19, [x0, #8]
   15258:	mov	w9, #0x1                   	// #1
   1525c:	str	x21, [sp, #32]
   15260:	lsr	x20, x1, #6
   15264:	lsl	x21, x9, x1
   15268:	add	x29, sp, #0x10
   1526c:	tbnz	w8, #31, 152bc <__gmpz_clrbit@@Base+0x78>
   15270:	cmp	x20, x8
   15274:	b.ge	15398 <__gmpz_clrbit@@Base+0x154>  // b.tcont
   15278:	lsl	x9, x20, #3
   1527c:	ldr	x10, [x19, x9]
   15280:	bics	xzr, x10, x21
   15284:	bic	x11, x10, x21
   15288:	cinc	x10, x20, eq  // eq = none
   1528c:	cmp	x10, x8
   15290:	str	x11, [x19, x9]
   15294:	b.ne	15398 <__gmpz_clrbit@@Base+0x154>  // b.any
   15298:	sub	x8, x19, #0x8
   1529c:	subs	x9, x20, #0x1
   152a0:	b.lt	153d4 <__gmpz_clrbit@@Base+0x190>  // b.tstop
   152a4:	ldr	x10, [x8, x20, lsl #3]
   152a8:	mov	x20, x9
   152ac:	cbz	x10, 1529c <__gmpz_clrbit@@Base+0x58>
   152b0:	add	x8, x9, #0x1
   152b4:	str	w8, [x0, #4]
   152b8:	b	15398 <__gmpz_clrbit@@Base+0x154>
   152bc:	neg	x9, x8
   152c0:	cmp	x20, x9
   152c4:	b.ge	152f8 <__gmpz_clrbit@@Base+0xb4>  // b.tcont
   152c8:	mov	x10, xzr
   152cc:	ldr	x11, [x19, x10, lsl #3]
   152d0:	add	x10, x10, #0x1
   152d4:	cbz	x11, 152cc <__gmpz_clrbit@@Base+0x88>
   152d8:	sub	x11, x10, #0x1
   152dc:	cmp	x20, x11
   152e0:	b.ls	15330 <__gmpz_clrbit@@Base+0xec>  // b.plast
   152e4:	lsl	x8, x20, #3
   152e8:	ldr	x9, [x19, x8]
   152ec:	orr	x9, x9, x21
   152f0:	str	x9, [x19, x8]
   152f4:	b	15398 <__gmpz_clrbit@@Base+0x154>
   152f8:	ldrsw	x10, [x0]
   152fc:	cmp	x20, x10
   15300:	b.ge	153ac <__gmpz_clrbit@@Base+0x168>  // b.tcont
   15304:	mvn	w10, w20
   15308:	cmn	x20, x8
   1530c:	str	w10, [x0, #4]
   15310:	b.eq	15328 <__gmpz_clrbit@@Base+0xe4>  // b.none
   15314:	add	x8, x20, x8
   15318:	add	x0, x19, x9, lsl #3
   1531c:	lsl	x2, x8, #3
   15320:	mov	w1, wzr
   15324:	bl	c780 <memset@plt>
   15328:	str	x21, [x19, x20, lsl #3]
   1532c:	b	15398 <__gmpz_clrbit@@Base+0x154>
   15330:	add	x11, x20, #0x1
   15334:	cmp	x11, x10
   15338:	b.ne	15398 <__gmpz_clrbit@@Base+0x154>  // b.any
   1533c:	lsl	x10, x20, #3
   15340:	ldr	x11, [x19, x10]
   15344:	sub	x11, x11, #0x1
   15348:	orr	x11, x11, x21
   1534c:	adds	x11, x11, #0x1
   15350:	str	x11, [x19, x10]
   15354:	b.cc	15398 <__gmpz_clrbit@@Base+0x154>  // b.lo, b.ul, b.last
   15358:	ldrsw	x10, [x0]
   1535c:	mov	w11, #0x1                   	// #1
   15360:	sub	x1, x11, x8
   15364:	cmp	x1, x10
   15368:	b.gt	153e0 <__gmpz_clrbit@@Base+0x19c>
   1536c:	add	x10, x19, x20, lsl #3
   15370:	add	x10, x10, #0x8
   15374:	str	xzr, [x19, x9, lsl #3]
   15378:	ldr	x11, [x10]
   1537c:	adds	x11, x11, #0x1
   15380:	str	x11, [x10], #8
   15384:	b.cs	15378 <__gmpz_clrbit@@Base+0x134>  // b.hs, b.nlast
   15388:	lsl	x9, x9, #3
   1538c:	ldr	w9, [x19, x9]
   15390:	sub	w8, w8, w9
   15394:	str	w8, [x0, #4]
   15398:	ldp	x20, x19, [sp, #48]
   1539c:	ldr	x21, [sp, #32]
   153a0:	ldp	x29, x30, [sp, #16]
   153a4:	add	sp, sp, #0x40
   153a8:	ret
   153ac:	add	x1, x20, #0x1
   153b0:	str	x0, [x29, #24]
   153b4:	str	x8, [sp, #8]
   153b8:	mov	x19, x9
   153bc:	bl	c1c0 <__gmpz_realloc@plt>
   153c0:	mov	x9, x19
   153c4:	ldr	x8, [sp, #8]
   153c8:	mov	x19, x0
   153cc:	ldr	x0, [x29, #24]
   153d0:	b	15304 <__gmpz_clrbit@@Base+0xc0>
   153d4:	mov	x8, xzr
   153d8:	str	w8, [x0, #4]
   153dc:	b	15398 <__gmpz_clrbit@@Base+0x154>
   153e0:	str	x0, [x29, #24]
   153e4:	mov	x19, x8
   153e8:	mov	x21, x9
   153ec:	bl	c1c0 <__gmpz_realloc@plt>
   153f0:	mov	x8, x19
   153f4:	mov	x19, x0
   153f8:	ldr	x0, [x29, #24]
   153fc:	mov	x9, x21
   15400:	b	1536c <__gmpz_clrbit@@Base+0x128>

0000000000015404 <__gmpz_cmp@@Base>:
   15404:	ldrsw	x9, [x0, #4]
   15408:	ldrsw	x10, [x1, #4]
   1540c:	mov	x8, x0
   15410:	subs	x0, x9, x10
   15414:	b.eq	1541c <__gmpz_cmp@@Base+0x18>  // b.none
   15418:	ret
   1541c:	ldr	x8, [x8, #8]
   15420:	ldr	x10, [x1, #8]
   15424:	cmp	w9, #0x0
   15428:	cneg	x11, x9, lt  // lt = tstop
   1542c:	sub	x8, x8, #0x8
   15430:	sub	x10, x10, #0x8
   15434:	subs	x12, x11, #0x1
   15438:	b.lt	15460 <__gmpz_cmp@@Base+0x5c>  // b.tstop
   1543c:	lsl	x11, x11, #3
   15440:	ldr	x13, [x8, x11]
   15444:	ldr	x11, [x10, x11]
   15448:	cmp	x13, x11
   1544c:	mov	x11, x12
   15450:	b.eq	15434 <__gmpz_cmp@@Base+0x30>  // b.none
   15454:	mov	w8, #0x1                   	// #1
   15458:	cneg	w8, w8, ls  // ls = plast
   1545c:	b	15464 <__gmpz_cmp@@Base+0x60>
   15460:	mov	w8, wzr
   15464:	cmp	w9, #0x0
   15468:	cneg	w0, w8, lt  // lt = tstop
   1546c:	ret

0000000000015470 <__gmpz_cmp_d@@Base>:
   15470:	sub	sp, sp, #0x40
   15474:	fmov	x8, d0
   15478:	mvn	x9, x8
   1547c:	tst	x9, #0x7ff0000000000000
   15480:	stp	x29, x30, [sp, #16]
   15484:	str	x21, [sp, #32]
   15488:	stp	x20, x19, [sp, #48]
   1548c:	add	x29, sp, #0x10
   15490:	b.eq	155a4 <__gmpz_cmp_d@@Base+0x134>  // b.none
   15494:	mov	x19, x0
   15498:	ldr	w0, [x0, #4]
   1549c:	fcmp	d0, #0.0
   154a0:	b.ne	154b8 <__gmpz_cmp_d@@Base+0x48>  // b.any
   154a4:	ldp	x20, x19, [sp, #48]
   154a8:	ldr	x21, [sp, #32]
   154ac:	ldp	x29, x30, [sp, #16]
   154b0:	add	sp, sp, #0x40
   154b4:	ret
   154b8:	cbz	w0, 155ac <__gmpz_cmp_d@@Base+0x13c>
   154bc:	sxtw	x21, w0
   154c0:	tbnz	w0, #31, 154d8 <__gmpz_cmp_d@@Base+0x68>
   154c4:	mov	w20, #0x1                   	// #1
   154c8:	fcmp	d0, #0.0
   154cc:	mov	w0, #0x1                   	// #1
   154d0:	b.pl	154ec <__gmpz_cmp_d@@Base+0x7c>  // b.nfrst
   154d4:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   154d8:	fcmp	d0, #0.0
   154dc:	b.ge	15544 <__gmpz_cmp_d@@Base+0xd4>  // b.tcont
   154e0:	fneg	d0, d0
   154e4:	neg	x21, x21
   154e8:	mov	w20, #0xffffffff            	// #-1
   154ec:	fmov	d1, #1.000000000000000000e+00
   154f0:	fcmp	d0, d1
   154f4:	b.pl	15500 <__gmpz_cmp_d@@Base+0x90>  // b.nfrst
   154f8:	mov	w0, w20
   154fc:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   15500:	mov	x0, sp
   15504:	bl	d460 <__gmp_extract_double@plt>
   15508:	sxtw	x8, w0
   1550c:	cmp	x21, x8
   15510:	b.ne	1554c <__gmpz_cmp_d@@Base+0xdc>  // b.any
   15514:	ldr	x8, [x19, #8]
   15518:	ldr	x10, [sp, #8]
   1551c:	add	x9, x8, x21, lsl #3
   15520:	ldur	x9, [x9, #-8]
   15524:	cmp	x9, x10
   15528:	b.ne	15594 <__gmpz_cmp_d@@Base+0x124>  // b.any
   1552c:	cmp	x21, #0x1
   15530:	b.ne	15554 <__gmpz_cmp_d@@Base+0xe4>  // b.any
   15534:	ldr	x8, [sp]
   15538:	cmp	x8, #0x0
   1553c:	csneg	w0, wzr, w20, eq  // eq = none
   15540:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   15544:	mov	w0, #0xffffffff            	// #-1
   15548:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   1554c:	cneg	w0, w20, lt  // lt = tstop
   15550:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   15554:	add	x9, x8, x21, lsl #3
   15558:	ldur	x9, [x9, #-16]
   1555c:	ldr	x10, [sp]
   15560:	cmp	x9, x10
   15564:	b.ne	15594 <__gmpz_cmp_d@@Base+0x124>  // b.any
   15568:	cmp	x21, #0x3
   1556c:	b.lt	1559c <__gmpz_cmp_d@@Base+0x12c>  // b.tstop
   15570:	sub	x8, x8, #0x18
   15574:	ldr	x9, [x8, x21, lsl #3]
   15578:	cbnz	x9, 154f8 <__gmpz_cmp_d@@Base+0x88>
   1557c:	sub	x9, x21, #0x3
   15580:	mov	w0, wzr
   15584:	sub	x21, x21, #0x1
   15588:	cmp	x9, #0x1
   1558c:	b.ge	15574 <__gmpz_cmp_d@@Base+0x104>  // b.tcont
   15590:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   15594:	cneg	w0, w20, cc  // cc = lo, ul, last
   15598:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   1559c:	mov	w0, wzr
   155a0:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   155a4:	tst	x8, #0xfffffffffffff
   155a8:	b.ne	155bc <__gmpz_cmp_d@@Base+0x14c>  // b.any
   155ac:	fcmp	d0, #0.0
   155b0:	mov	w8, #0xffffffff            	// #-1
   155b4:	csinc	w0, w8, wzr, pl  // pl = nfrst
   155b8:	b	154a4 <__gmpz_cmp_d@@Base+0x34>
   155bc:	bl	c300 <__gmp_invalid_operation@plt>

00000000000155c0 <__gmpz_cmp_si@@Base>:
   155c0:	ldr	w8, [x0, #4]
   155c4:	cmp	x1, #0x0
   155c8:	asr	x9, x1, #63
   155cc:	cinc	x9, x9, gt
   155d0:	cbz	w8, 15600 <__gmpz_cmp_si@@Base+0x40>
   155d4:	sxtw	x10, w8
   155d8:	cmp	x9, x10
   155dc:	b.ne	15600 <__gmpz_cmp_si@@Base+0x40>  // b.any
   155e0:	ldr	x9, [x0, #8]
   155e4:	cmp	x1, #0x0
   155e8:	cneg	x10, x1, mi  // mi = first
   155ec:	ldr	x9, [x9]
   155f0:	cmp	x9, x10
   155f4:	b.ne	15608 <__gmpz_cmp_si@@Base+0x48>  // b.any
   155f8:	mov	w0, wzr
   155fc:	ret
   15600:	sub	w0, w8, w9
   15604:	ret
   15608:	cneg	w0, w8, ls  // ls = plast
   1560c:	ret

0000000000015610 <__gmpz_cmp_ui@@Base>:
   15610:	ldr	w8, [x0, #4]
   15614:	cmp	w8, #0x1
   15618:	b.eq	1562c <__gmpz_cmp_ui@@Base+0x1c>  // b.none
   1561c:	cbnz	w8, 15644 <__gmpz_cmp_ui@@Base+0x34>
   15620:	cmp	x1, #0x0
   15624:	csetm	w0, ne  // ne = any
   15628:	ret
   1562c:	ldr	x8, [x0, #8]
   15630:	ldr	x8, [x8]
   15634:	cmp	x8, x1
   15638:	b.ls	15654 <__gmpz_cmp_ui@@Base+0x44>  // b.plast
   1563c:	mov	w0, #0x1                   	// #1
   15640:	ret
   15644:	cmp	w8, #0x1
   15648:	mov	w8, #0xffffffff            	// #-1
   1564c:	cneg	w0, w8, ge  // ge = tcont
   15650:	ret
   15654:	csetm	w0, cc  // cc = lo, ul, last
   15658:	ret

000000000001565c <__gmpz_cmpabs@@Base>:
   1565c:	ldr	w9, [x0, #4]
   15660:	ldr	w10, [x1, #4]
   15664:	mov	x8, x0
   15668:	cmp	w9, #0x0
   1566c:	cneg	w9, w9, mi  // mi = first
   15670:	cmp	w10, #0x0
   15674:	cneg	w10, w10, mi  // mi = first
   15678:	subs	x0, x9, x10
   1567c:	b.eq	15684 <__gmpz_cmpabs@@Base+0x28>  // b.none
   15680:	ret
   15684:	ldr	x8, [x8, #8]
   15688:	ldr	x10, [x1, #8]
   1568c:	sub	x8, x8, #0x8
   15690:	sub	x10, x10, #0x8
   15694:	subs	x11, x9, #0x1
   15698:	b.lt	156c0 <__gmpz_cmpabs@@Base+0x64>  // b.tstop
   1569c:	lsl	x9, x9, #3
   156a0:	ldr	x12, [x8, x9]
   156a4:	ldr	x9, [x10, x9]
   156a8:	cmp	x12, x9
   156ac:	mov	x9, x11
   156b0:	b.eq	15694 <__gmpz_cmpabs@@Base+0x38>  // b.none
   156b4:	mov	w8, #0x1                   	// #1
   156b8:	cneg	w0, w8, ls  // ls = plast
   156bc:	ret
   156c0:	mov	w0, wzr
   156c4:	ret

00000000000156c8 <__gmpz_cmpabs_d@@Base>:
   156c8:	sub	sp, sp, #0x30
   156cc:	fmov	x8, d0
   156d0:	mvn	x9, x8
   156d4:	tst	x9, #0x7ff0000000000000
   156d8:	stp	x29, x30, [sp, #16]
   156dc:	stp	x20, x19, [sp, #32]
   156e0:	add	x29, sp, #0x10
   156e4:	b.eq	157e0 <__gmpz_cmpabs_d@@Base+0x118>  // b.none
   156e8:	ldrsw	x8, [x0, #4]
   156ec:	mov	x19, x0
   156f0:	fcmp	d0, #0.0
   156f4:	b.ne	15710 <__gmpz_cmpabs_d@@Base+0x48>  // b.any
   156f8:	cmp	w8, #0x0
   156fc:	cset	w0, ne  // ne = any
   15700:	ldp	x20, x19, [sp, #32]
   15704:	ldp	x29, x30, [sp, #16]
   15708:	add	sp, sp, #0x30
   1570c:	ret
   15710:	cbz	w8, 157e8 <__gmpz_cmpabs_d@@Base+0x120>
   15714:	cmp	x8, #0x0
   15718:	fneg	d1, d0
   1571c:	cneg	x20, x8, mi  // mi = first
   15720:	fcmp	d0, #0.0
   15724:	fcsel	d0, d0, d1, ge  // ge = tcont
   15728:	fmov	d1, #1.000000000000000000e+00
   1572c:	fcmp	d0, d1
   15730:	b.pl	1573c <__gmpz_cmpabs_d@@Base+0x74>  // b.nfrst
   15734:	mov	w0, #0x1                   	// #1
   15738:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   1573c:	mov	x0, sp
   15740:	bl	d460 <__gmp_extract_double@plt>
   15744:	sxtw	x8, w0
   15748:	cmp	x20, x8
   1574c:	b.ne	15780 <__gmpz_cmpabs_d@@Base+0xb8>  // b.any
   15750:	ldr	x8, [x19, #8]
   15754:	ldr	x10, [sp, #8]
   15758:	add	x9, x8, x20, lsl #3
   1575c:	ldur	x9, [x9, #-8]
   15760:	cmp	x9, x10
   15764:	b.ne	157cc <__gmpz_cmpabs_d@@Base+0x104>  // b.any
   15768:	cmp	x20, #0x1
   1576c:	b.ne	1578c <__gmpz_cmpabs_d@@Base+0xc4>  // b.any
   15770:	ldr	x8, [sp]
   15774:	cmp	x8, #0x0
   15778:	csetm	w0, ne  // ne = any
   1577c:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   15780:	mov	w8, #0xffffffff            	// #-1
   15784:	cneg	w0, w8, ge  // ge = tcont
   15788:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   1578c:	add	x9, x8, x20, lsl #3
   15790:	ldur	x9, [x9, #-16]
   15794:	ldr	x10, [sp]
   15798:	cmp	x9, x10
   1579c:	b.ne	157cc <__gmpz_cmpabs_d@@Base+0x104>  // b.any
   157a0:	cmp	x20, #0x3
   157a4:	b.lt	157d8 <__gmpz_cmpabs_d@@Base+0x110>  // b.tstop
   157a8:	sub	x8, x8, #0x18
   157ac:	ldr	x9, [x8, x20, lsl #3]
   157b0:	cbnz	x9, 15734 <__gmpz_cmpabs_d@@Base+0x6c>
   157b4:	sub	x9, x20, #0x3
   157b8:	mov	w0, wzr
   157bc:	sub	x20, x20, #0x1
   157c0:	cmp	x9, #0x1
   157c4:	b.ge	157ac <__gmpz_cmpabs_d@@Base+0xe4>  // b.tcont
   157c8:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   157cc:	mov	w8, #0xffffffff            	// #-1
   157d0:	cneg	w0, w8, cs  // cs = hs, nlast
   157d4:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   157d8:	mov	w0, wzr
   157dc:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   157e0:	tst	x8, #0xfffffffffffff
   157e4:	b.ne	157f0 <__gmpz_cmpabs_d@@Base+0x128>  // b.any
   157e8:	mov	w0, #0xffffffff            	// #-1
   157ec:	b	15700 <__gmpz_cmpabs_d@@Base+0x38>
   157f0:	bl	c300 <__gmp_invalid_operation@plt>

00000000000157f4 <__gmpz_cmpabs_ui@@Base>:
   157f4:	ldrsw	x8, [x0, #4]
   157f8:	cbz	w8, 15824 <__gmpz_cmpabs_ui@@Base+0x30>
   157fc:	cmp	x8, #0x0
   15800:	cneg	x8, x8, mi  // mi = first
   15804:	cmp	x8, #0x1
   15808:	b.ne	1581c <__gmpz_cmpabs_ui@@Base+0x28>  // b.any
   1580c:	ldr	x8, [x0, #8]
   15810:	ldr	x8, [x8]
   15814:	cmp	x8, x1
   15818:	b.ls	15830 <__gmpz_cmpabs_ui@@Base+0x3c>  // b.plast
   1581c:	mov	w0, #0x1                   	// #1
   15820:	ret
   15824:	cmp	x1, #0x0
   15828:	csetm	w0, ne  // ne = any
   1582c:	ret
   15830:	csetm	w0, cc  // cc = lo, ul, last
   15834:	ret

0000000000015838 <__gmpz_com@@Base>:
   15838:	stp	x29, x30, [sp, #-64]!
   1583c:	stp	x22, x21, [sp, #32]
   15840:	stp	x20, x19, [sp, #48]
   15844:	ldrsw	x20, [x1, #4]
   15848:	mov	x21, x1
   1584c:	mov	x19, x0
   15850:	str	x23, [sp, #16]
   15854:	mov	x29, sp
   15858:	tbnz	w20, #31, 15894 <__gmpz_com@@Base+0x5c>
   1585c:	ldr	w8, [x19]
   15860:	cbz	w20, 158e8 <__gmpz_com@@Base+0xb0>
   15864:	cmp	w20, w8
   15868:	b.ge	15904 <__gmpz_com@@Base+0xcc>  // b.tcont
   1586c:	ldr	x22, [x19, #8]
   15870:	ldr	x1, [x21, #8]
   15874:	mov	w3, #0x1                   	// #1
   15878:	mov	x0, x22
   1587c:	mov	x2, x20
   15880:	bl	c150 <__gmpn_add_1@plt>
   15884:	add	w8, w20, w0
   15888:	str	x0, [x22, x20, lsl #3]
   1588c:	neg	w8, w8
   15890:	b	158d0 <__gmpz_com@@Base+0x98>
   15894:	ldrsw	x8, [x19]
   15898:	neg	x22, x20
   1589c:	cmp	x22, x8
   158a0:	b.gt	15918 <__gmpz_com@@Base+0xe0>
   158a4:	ldr	x23, [x19, #8]
   158a8:	ldr	x1, [x21, #8]
   158ac:	mov	w3, #0x1                   	// #1
   158b0:	mov	x0, x23
   158b4:	mov	x2, x22
   158b8:	bl	caf0 <__gmpn_sub_1@plt>
   158bc:	mvn	x8, x20
   158c0:	ldr	x8, [x23, x8, lsl #3]
   158c4:	cmp	x8, #0x0
   158c8:	csetm	w8, eq  // eq = none
   158cc:	sub	w8, w8, w20
   158d0:	str	w8, [x19, #4]
   158d4:	ldp	x20, x19, [sp, #48]
   158d8:	ldp	x22, x21, [sp, #32]
   158dc:	ldr	x23, [sp, #16]
   158e0:	ldp	x29, x30, [sp], #64
   158e4:	ret
   158e8:	cmp	w8, #0x0
   158ec:	b.le	1592c <__gmpz_com@@Base+0xf4>
   158f0:	ldr	x0, [x19, #8]
   158f4:	mov	w8, #0x1                   	// #1
   158f8:	str	x8, [x0]
   158fc:	mov	w8, #0xffffffff            	// #-1
   15900:	b	158d0 <__gmpz_com@@Base+0x98>
   15904:	add	x1, x20, #0x1
   15908:	mov	x0, x19
   1590c:	bl	c1c0 <__gmpz_realloc@plt>
   15910:	mov	x22, x0
   15914:	b	15870 <__gmpz_com@@Base+0x38>
   15918:	mov	x0, x19
   1591c:	mov	x1, x22
   15920:	bl	c1c0 <__gmpz_realloc@plt>
   15924:	mov	x23, x0
   15928:	b	158a8 <__gmpz_com@@Base+0x70>
   1592c:	mov	w1, #0x1                   	// #1
   15930:	mov	x0, x19
   15934:	bl	c1c0 <__gmpz_realloc@plt>
   15938:	b	158f4 <__gmpz_com@@Base+0xbc>

000000000001593c <__gmpz_combit@@Base>:
   1593c:	stp	x29, x30, [sp, #-80]!
   15940:	stp	x24, x23, [sp, #32]
   15944:	stp	x22, x21, [sp, #48]
   15948:	stp	x20, x19, [sp, #64]
   1594c:	ldrsw	x23, [x0, #4]
   15950:	ldr	x20, [x0, #8]
   15954:	lsr	x21, x1, #6
   15958:	mov	w8, #0x1                   	// #1
   1595c:	add	x22, x21, #0x1
   15960:	cmp	x22, x23
   15964:	lsl	x24, x8, x1
   15968:	str	x25, [sp, #16]
   1596c:	mov	x29, sp
   15970:	b.ge	15988 <__gmpz_combit@@Base+0x4c>  // b.tcont
   15974:	lsl	x8, x21, #3
   15978:	ldr	x9, [x20, x8]
   1597c:	eor	x9, x9, x24
   15980:	str	x9, [x20, x8]
   15984:	b	15a48 <__gmpz_combit@@Base+0x10c>
   15988:	neg	x25, x23
   1598c:	mov	x19, x0
   15990:	cmp	x21, x25
   15994:	b.ge	159bc <__gmpz_combit@@Base+0x80>  // b.tcont
   15998:	cbz	x21, 159ac <__gmpz_combit@@Base+0x70>
   1599c:	mov	x0, x20
   159a0:	mov	x1, x21
   159a4:	bl	c010 <__gmpn_zero_p@plt>
   159a8:	cbz	w0, 159bc <__gmpz_combit@@Base+0x80>
   159ac:	ldr	x8, [x20, x21, lsl #3]
   159b0:	sub	x9, x24, #0x1
   159b4:	tst	x8, x9
   159b8:	b.eq	15a60 <__gmpz_combit@@Base+0x124>  // b.none
   159bc:	cmp	x23, #0x0
   159c0:	cneg	x25, x23, mi  // mi = first
   159c4:	cmp	x21, x25
   159c8:	b.ge	15a0c <__gmpz_combit@@Base+0xd0>  // b.tcont
   159cc:	lsl	x8, x21, #3
   159d0:	ldr	x9, [x20, x8]
   159d4:	eor	x9, x9, x24
   159d8:	cmp	x9, #0x0
   159dc:	cinc	x10, x21, eq  // eq = none
   159e0:	cmp	x10, x25
   159e4:	str	x9, [x20, x8]
   159e8:	b.ne	15a48 <__gmpz_combit@@Base+0x10c>  // b.any
   159ec:	sub	x8, x20, #0x8
   159f0:	subs	x9, x21, #0x1
   159f4:	b.lt	15b18 <__gmpz_combit@@Base+0x1dc>  // b.tstop
   159f8:	ldr	x10, [x8, x21, lsl #3]
   159fc:	mov	x21, x9
   15a00:	cbz	x10, 159f0 <__gmpz_combit@@Base+0xb4>
   15a04:	add	x8, x9, #0x1
   15a08:	b	15b1c <__gmpz_combit@@Base+0x1e0>
   15a0c:	ldrsw	x8, [x19]
   15a10:	cmp	x21, x8
   15a14:	b.ge	15afc <__gmpz_combit@@Base+0x1c0>  // b.tcont
   15a18:	subs	x8, x21, x25
   15a1c:	b.eq	15a30 <__gmpz_combit@@Base+0xf4>  // b.none
   15a20:	add	x0, x20, x25, lsl #3
   15a24:	lsl	x2, x8, #3
   15a28:	mov	w1, wzr
   15a2c:	bl	c780 <memset@plt>
   15a30:	str	x24, [x20, x21, lsl #3]
   15a34:	ldr	w8, [x19, #4]
   15a38:	mvn	w9, w21
   15a3c:	cmp	w8, #0x0
   15a40:	csel	x8, x22, x9, ge  // ge = tcont
   15a44:	str	w8, [x19, #4]
   15a48:	ldp	x20, x19, [sp, #64]
   15a4c:	ldp	x22, x21, [sp, #48]
   15a50:	ldp	x24, x23, [sp, #32]
   15a54:	ldr	x25, [sp, #16]
   15a58:	ldp	x29, x30, [sp], #80
   15a5c:	ret
   15a60:	tst	x8, x24
   15a64:	b.eq	15ac0 <__gmpz_combit@@Base+0x184>  // b.none
   15a68:	ldrsw	x8, [x19]
   15a6c:	mov	w9, #0x1                   	// #1
   15a70:	sub	x1, x9, x23
   15a74:	cmp	x1, x8
   15a78:	b.gt	15b2c <__gmpz_combit@@Base+0x1f0>
   15a7c:	str	xzr, [x20, x25, lsl #3]
   15a80:	lsl	x8, x21, #3
   15a84:	ldr	x9, [x20, x8]
   15a88:	adds	x9, x9, x24
   15a8c:	str	x9, [x20, x8]
   15a90:	b.cc	15aac <__gmpz_combit@@Base+0x170>  // b.lo, b.ul, b.last
   15a94:	add	x8, x20, x21, lsl #3
   15a98:	add	x8, x8, #0x8
   15a9c:	ldr	x9, [x8]
   15aa0:	adds	x9, x9, #0x1
   15aa4:	str	x9, [x8], #8
   15aa8:	b.cs	15a9c <__gmpz_combit@@Base+0x160>  // b.hs, b.nlast
   15aac:	lsl	x8, x25, #3
   15ab0:	ldr	w8, [x20, x8]
   15ab4:	sub	w8, w23, w8
   15ab8:	str	w8, [x19, #4]
   15abc:	b	15a48 <__gmpz_combit@@Base+0x10c>
   15ac0:	subs	x8, x8, x24
   15ac4:	str	x8, [x20, x21, lsl #3]
   15ac8:	b.cs	15ae4 <__gmpz_combit@@Base+0x1a8>  // b.hs, b.nlast
   15acc:	add	x8, x20, x21, lsl #3
   15ad0:	add	x8, x8, #0x8
   15ad4:	ldr	x9, [x8]
   15ad8:	sub	x10, x9, #0x1
   15adc:	str	x10, [x8], #8
   15ae0:	cbz	x9, 15ad4 <__gmpz_combit@@Base+0x198>
   15ae4:	mvn	x8, x23
   15ae8:	ldr	x8, [x20, x8, lsl #3]
   15aec:	cmp	x8, #0x0
   15af0:	cinc	w8, w23, eq  // eq = none
   15af4:	str	w8, [x19, #4]
   15af8:	b	15a48 <__gmpz_combit@@Base+0x10c>
   15afc:	mov	x0, x19
   15b00:	mov	x1, x22
   15b04:	bl	c1c0 <__gmpz_realloc@plt>
   15b08:	mov	x20, x0
   15b0c:	subs	x8, x21, x25
   15b10:	b.ne	15a20 <__gmpz_combit@@Base+0xe4>  // b.any
   15b14:	b	15a30 <__gmpz_combit@@Base+0xf4>
   15b18:	mov	x8, xzr
   15b1c:	neg	w9, w8
   15b20:	cmp	w23, #0x0
   15b24:	csel	x8, x8, x9, ge  // ge = tcont
   15b28:	b	15a44 <__gmpz_combit@@Base+0x108>
   15b2c:	mov	x0, x19
   15b30:	bl	c1c0 <__gmpz_realloc@plt>
   15b34:	mov	x20, x0
   15b38:	b	15a7c <__gmpz_combit@@Base+0x140>

0000000000015b3c <__gmpz_congruent_p@@Base>:
   15b3c:	stp	x29, x30, [sp, #-80]!
   15b40:	stp	x26, x25, [sp, #16]
   15b44:	stp	x24, x23, [sp, #32]
   15b48:	stp	x22, x21, [sp, #48]
   15b4c:	stp	x20, x19, [sp, #64]
   15b50:	mov	x29, sp
   15b54:	sub	sp, sp, #0x10
   15b58:	ldrsw	x8, [x2, #4]
   15b5c:	cbz	w8, 15d9c <__gmpz_congruent_p@@Base+0x260>
   15b60:	ldr	w9, [x0, #4]
   15b64:	ldr	w10, [x1, #4]
   15b68:	cmp	x8, #0x0
   15b6c:	cneg	x21, x8, mi  // mi = first
   15b70:	cmp	w9, #0x0
   15b74:	cneg	w8, w9, mi  // mi = first
   15b78:	cmp	w10, #0x0
   15b7c:	cneg	w9, w10, mi  // mi = first
   15b80:	cmp	w8, w9
   15b84:	csel	x11, x1, x0, lt  // lt = tstop
   15b88:	ldrsw	x8, [x11, #4]
   15b8c:	csel	x10, x0, x1, lt  // lt = tstop
   15b90:	ldr	x20, [x2, #8]
   15b94:	ldrsw	x9, [x10, #4]
   15b98:	ldr	x22, [x11, #8]
   15b9c:	cmp	x8, #0x0
   15ba0:	cneg	x19, x8, mi  // mi = first
   15ba4:	cbz	w9, 15bec <__gmpz_congruent_p@@Base+0xb0>
   15ba8:	ldr	x23, [x10, #8]
   15bac:	ldr	x26, [x20]
   15bb0:	ldr	x10, [x22]
   15bb4:	eor	w8, w9, w8
   15bb8:	ldr	x25, [x23]
   15bbc:	cmp	x9, #0x0
   15bc0:	cneg	x24, x9, mi  // mi = first
   15bc4:	neg	x9, x26
   15bc8:	cmp	w8, #0x0
   15bcc:	and	x9, x26, x9
   15bd0:	cneg	x10, x10, lt  // lt = tstop
   15bd4:	sub	x9, x9, #0x1
   15bd8:	sub	x10, x10, x25
   15bdc:	tst	x9, x10
   15be0:	b.eq	15c08 <__gmpz_congruent_p@@Base+0xcc>  // b.none
   15be4:	mov	w19, wzr
   15be8:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15bec:	mov	x0, x22
   15bf0:	mov	x1, x19
   15bf4:	mov	x2, x20
   15bf8:	mov	x3, x21
   15bfc:	bl	d540 <__gmpn_divisible_p@plt>
   15c00:	mov	w19, w0
   15c04:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15c08:	cmp	x24, #0x1
   15c0c:	b.ne	15c80 <__gmpz_congruent_p@@Base+0x144>  // b.any
   15c10:	cmp	x21, #0x1
   15c14:	b.ne	15c40 <__gmpz_congruent_p@@Base+0x104>  // b.any
   15c18:	tbz	w8, #31, 15d1c <__gmpz_congruent_p@@Base+0x1e0>
   15c1c:	subs	x8, x26, x25
   15c20:	b.cs	15d18 <__gmpz_congruent_p@@Base+0x1dc>  // b.hs, b.nlast
   15c24:	clz	x8, x26
   15c28:	lsl	x8, x26, x8
   15c2c:	cmp	x25, x8
   15c30:	cset	w9, hi  // hi = pmore
   15c34:	lsl	x8, x8, x9
   15c38:	sub	x25, x8, x25
   15c3c:	b	15d1c <__gmpz_congruent_p@@Base+0x1e0>
   15c40:	cmp	x21, #0x2
   15c44:	b.ne	15c80 <__gmpz_congruent_p@@Base+0x144>  // b.any
   15c48:	cbz	x26, 15c80 <__gmpz_congruent_p@@Base+0x144>
   15c4c:	ldr	x10, [x20, #8]
   15c50:	cmp	x10, x9
   15c54:	b.ls	15c60 <__gmpz_congruent_p@@Base+0x124>  // b.plast
   15c58:	mov	w9, #0x1                   	// #1
   15c5c:	b	15c7c <__gmpz_congruent_p@@Base+0x140>
   15c60:	rbit	x11, x26
   15c64:	clz	x11, x11
   15c68:	lsr	x12, x26, x11
   15c6c:	neg	x11, x11
   15c70:	lsl	x10, x10, x11
   15c74:	mov	w9, wzr
   15c78:	orr	x26, x10, x12
   15c7c:	cbz	w9, 15c18 <__gmpz_congruent_p@@Base+0xdc>
   15c80:	lsl	x9, x19, #3
   15c84:	mov	w10, #0x7ef8                	// #32504
   15c88:	cmp	x9, x10
   15c8c:	add	x1, x9, #0x8
   15c90:	stur	xzr, [x29, #-8]
   15c94:	b.hi	15dac <__gmpz_congruent_p@@Base+0x270>  // b.pmore
   15c98:	add	x10, x1, #0xf
   15c9c:	mov	x9, sp
   15ca0:	and	x10, x10, #0xfffffffffffffff0
   15ca4:	sub	x25, x9, x10
   15ca8:	mov	sp, x25
   15cac:	tbnz	w8, #31, 15dc4 <__gmpz_congruent_p@@Base+0x288>
   15cb0:	cmp	x19, x24
   15cb4:	b.gt	15ccc <__gmpz_congruent_p@@Base+0x190>
   15cb8:	mov	x0, x22
   15cbc:	mov	x1, x23
   15cc0:	mov	x2, x19
   15cc4:	bl	c570 <__gmpn_cmp@plt>
   15cc8:	tbnz	w0, #31, 15d00 <__gmpz_congruent_p@@Base+0x1c4>
   15ccc:	mov	x0, x25
   15cd0:	mov	x1, x22
   15cd4:	mov	x2, x19
   15cd8:	mov	x3, x23
   15cdc:	mov	x4, x24
   15ce0:	bl	d340 <__gmpn_sub@plt>
   15ce4:	sub	x8, x25, #0x8
   15ce8:	mov	x1, x19
   15cec:	subs	x19, x19, #0x1
   15cf0:	b.lt	15de8 <__gmpz_congruent_p@@Base+0x2ac>  // b.tstop
   15cf4:	ldr	x9, [x8, x1, lsl #3]
   15cf8:	cbz	x9, 15ce8 <__gmpz_congruent_p@@Base+0x1ac>
   15cfc:	b	15de8 <__gmpz_congruent_p@@Base+0x2ac>
   15d00:	mov	x0, x25
   15d04:	mov	x1, x23
   15d08:	mov	x2, x22
   15d0c:	mov	x3, x19
   15d10:	bl	c420 <__gmpn_sub_n@plt>
   15d14:	b	15ce4 <__gmpz_congruent_p@@Base+0x1a8>
   15d18:	mov	x25, x8
   15d1c:	cmp	x19, #0x28
   15d20:	b.lt	15d48 <__gmpz_congruent_p@@Base+0x20c>  // b.tstop
   15d24:	mov	x0, x22
   15d28:	mov	x1, x19
   15d2c:	mov	x2, x26
   15d30:	bl	c540 <__gmpn_mod_1@plt>
   15d34:	cmp	x25, x26
   15d38:	b.cs	15d88 <__gmpz_congruent_p@@Base+0x24c>  // b.hs, b.nlast
   15d3c:	cmp	x0, x25
   15d40:	cset	w19, eq  // eq = none
   15d44:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15d48:	rbit	x8, x26
   15d4c:	clz	x8, x8
   15d50:	tst	x26, #0x1
   15d54:	csel	x8, x8, xzr, eq  // eq = none
   15d58:	lsr	x20, x26, x8
   15d5c:	mov	x0, x22
   15d60:	mov	x1, x19
   15d64:	mov	x2, x20
   15d68:	mov	x3, x25
   15d6c:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   15d70:	cmp	x0, #0x0
   15d74:	cset	w8, eq  // eq = none
   15d78:	cmp	x0, x20
   15d7c:	cset	w9, eq  // eq = none
   15d80:	orr	w19, w8, w9
   15d84:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15d88:	udiv	x8, x25, x26
   15d8c:	msub	x8, x8, x26, x25
   15d90:	cmp	x0, x8
   15d94:	cset	w19, eq  // eq = none
   15d98:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15d9c:	bl	d040 <__gmpz_cmp@plt>
   15da0:	cmp	w0, #0x0
   15da4:	cset	w19, eq  // eq = none
   15da8:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>
   15dac:	sub	x0, x29, #0x8
   15db0:	mov	w25, w8
   15db4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   15db8:	mov	w8, w25
   15dbc:	mov	x25, x0
   15dc0:	tbz	w8, #31, 15cb0 <__gmpz_congruent_p@@Base+0x174>
   15dc4:	mov	x0, x25
   15dc8:	mov	x1, x22
   15dcc:	mov	x2, x19
   15dd0:	mov	x3, x23
   15dd4:	mov	x4, x24
   15dd8:	bl	c970 <__gmpn_add@plt>
   15ddc:	cmp	x0, #0x0
   15de0:	cinc	x1, x19, ne  // ne = any
   15de4:	str	x0, [x25, x19, lsl #3]
   15de8:	mov	x0, x25
   15dec:	mov	x2, x20
   15df0:	mov	x3, x21
   15df4:	bl	d540 <__gmpn_divisible_p@plt>
   15df8:	ldur	x8, [x29, #-8]
   15dfc:	mov	w19, w0
   15e00:	cbnz	x8, 15e24 <__gmpz_congruent_p@@Base+0x2e8>
   15e04:	mov	w0, w19
   15e08:	mov	sp, x29
   15e0c:	ldp	x20, x19, [sp, #64]
   15e10:	ldp	x22, x21, [sp, #48]
   15e14:	ldp	x24, x23, [sp, #32]
   15e18:	ldp	x26, x25, [sp, #16]
   15e1c:	ldp	x29, x30, [sp], #80
   15e20:	ret
   15e24:	mov	x0, x8
   15e28:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   15e2c:	b	15e04 <__gmpz_congruent_p@@Base+0x2c8>

0000000000015e30 <__gmpz_congruent_2exp_p@@Base>:
   15e30:	stp	x29, x30, [sp, #-64]!
   15e34:	stp	x24, x23, [sp, #16]
   15e38:	stp	x22, x21, [sp, #32]
   15e3c:	stp	x20, x19, [sp, #48]
   15e40:	ldrsw	x8, [x0, #4]
   15e44:	ldrsw	x9, [x1, #4]
   15e48:	lsr	x22, x2, #6
   15e4c:	mov	x29, sp
   15e50:	cmp	x8, #0x0
   15e54:	cneg	x11, x8, mi  // mi = first
   15e58:	cmp	x9, #0x0
   15e5c:	cneg	x12, x9, mi  // mi = first
   15e60:	cmp	x11, x12
   15e64:	csel	x23, x12, x11, lt  // lt = tstop
   15e68:	csel	x24, x11, x12, lt  // lt = tstop
   15e6c:	csel	x11, x1, x0, lt  // lt = tstop
   15e70:	ldr	x19, [x11, #8]
   15e74:	mov	x11, #0xffffffffffffffff    	// #-1
   15e78:	lsl	x11, x11, x2
   15e7c:	csel	x10, x0, x1, lt  // lt = tstop
   15e80:	mvn	x21, x11
   15e84:	cbz	x24, 15f7c <__gmpz_congruent_2exp_p@@Base+0x14c>
   15e88:	ldr	x20, [x10, #8]
   15e8c:	eor	w8, w9, w8
   15e90:	tbnz	w8, #31, 15eb4 <__gmpz_congruent_2exp_p@@Base+0x84>
   15e94:	cmp	x24, x22
   15e98:	csel	x2, x24, x22, cc  // cc = lo, ul, last
   15e9c:	mov	x0, x19
   15ea0:	mov	x1, x20
   15ea4:	bl	c570 <__gmpn_cmp@plt>
   15ea8:	cbz	w0, 15f60 <__gmpz_congruent_2exp_p@@Base+0x130>
   15eac:	mov	w0, wzr
   15eb0:	b	15fbc <__gmpz_congruent_2exp_p@@Base+0x18c>
   15eb4:	mov	x9, xzr
   15eb8:	and	w8, w2, #0x3f
   15ebc:	lsl	x11, x9, #3
   15ec0:	ldr	x10, [x19, x11]
   15ec4:	ldr	x11, [x20, x11]
   15ec8:	cmp	x22, x9
   15ecc:	add	x11, x11, x10
   15ed0:	b.eq	15fb4 <__gmpz_congruent_2exp_p@@Base+0x184>  // b.none
   15ed4:	cbnz	x11, 15eac <__gmpz_congruent_2exp_p@@Base+0x7c>
   15ed8:	add	x9, x9, #0x1
   15edc:	cbz	x10, 15ebc <__gmpz_congruent_2exp_p@@Base+0x8c>
   15ee0:	cmp	x9, x24
   15ee4:	b.cs	15f18 <__gmpz_congruent_2exp_p@@Base+0xe8>  // b.hs, b.nlast
   15ee8:	lsl	x10, x9, #3
   15eec:	ldr	x11, [x19, x10]
   15ef0:	ldr	x10, [x20, x10]
   15ef4:	cmp	x9, x22
   15ef8:	eor	x10, x10, x11
   15efc:	b.cs	15fd0 <__gmpz_congruent_2exp_p@@Base+0x1a0>  // b.hs, b.nlast
   15f00:	cmn	x10, #0x1
   15f04:	b.ne	15eac <__gmpz_congruent_2exp_p@@Base+0x7c>  // b.any
   15f08:	add	x9, x9, #0x1
   15f0c:	cmp	x9, x24
   15f10:	b.cc	15ee8 <__gmpz_congruent_2exp_p@@Base+0xb8>  // b.lo, b.ul, b.last
   15f14:	mov	x9, x24
   15f18:	cmp	x23, x22
   15f1c:	b.cc	15eac <__gmpz_congruent_2exp_p@@Base+0x7c>  // b.lo, b.ul, b.last
   15f20:	cmp	x9, x22
   15f24:	b.cs	15f48 <__gmpz_congruent_2exp_p@@Base+0x118>  // b.hs, b.nlast
   15f28:	sub	x10, x22, x9
   15f2c:	add	x9, x19, x9, lsl #3
   15f30:	ldr	x11, [x9]
   15f34:	cmn	x11, #0x1
   15f38:	b.ne	15eac <__gmpz_congruent_2exp_p@@Base+0x7c>  // b.any
   15f3c:	subs	x10, x10, #0x1
   15f40:	add	x9, x9, #0x8
   15f44:	b.ne	15f30 <__gmpz_congruent_2exp_p@@Base+0x100>  // b.any
   15f48:	cbz	w8, 15fd8 <__gmpz_congruent_2exp_p@@Base+0x1a8>
   15f4c:	cmp	x23, x22
   15f50:	b.eq	15eac <__gmpz_congruent_2exp_p@@Base+0x7c>  // b.none
   15f54:	ldr	x8, [x19, x22, lsl #3]
   15f58:	add	x8, x8, #0x1
   15f5c:	b	15fa4 <__gmpz_congruent_2exp_p@@Base+0x174>
   15f60:	cmp	x24, x22
   15f64:	b.ls	15f7c <__gmpz_congruent_2exp_p@@Base+0x14c>  // b.plast
   15f68:	lsl	x8, x22, #3
   15f6c:	ldr	x9, [x19, x8]
   15f70:	ldr	x8, [x20, x8]
   15f74:	sub	x8, x9, x8
   15f78:	b	15fa4 <__gmpz_congruent_2exp_p@@Base+0x174>
   15f7c:	cmp	x23, x22
   15f80:	b.ls	15fac <__gmpz_congruent_2exp_p@@Base+0x17c>  // b.plast
   15f84:	cmp	x24, x22
   15f88:	b.cs	15fa0 <__gmpz_congruent_2exp_p@@Base+0x170>  // b.hs, b.nlast
   15f8c:	ldr	x8, [x19, x24, lsl #3]
   15f90:	cbnz	x8, 15eac <__gmpz_congruent_2exp_p@@Base+0x7c>
   15f94:	add	x24, x24, #0x1
   15f98:	cmp	x24, x22
   15f9c:	b.cc	15f8c <__gmpz_congruent_2exp_p@@Base+0x15c>  // b.lo, b.ul, b.last
   15fa0:	ldr	x8, [x19, x22, lsl #3]
   15fa4:	tst	x8, x21
   15fa8:	b	15fb8 <__gmpz_congruent_2exp_p@@Base+0x188>
   15fac:	cmp	x23, x24
   15fb0:	b	15fb8 <__gmpz_congruent_2exp_p@@Base+0x188>
   15fb4:	tst	x11, x21
   15fb8:	cset	w0, eq  // eq = none
   15fbc:	ldp	x20, x19, [sp, #48]
   15fc0:	ldp	x22, x21, [sp, #32]
   15fc4:	ldp	x24, x23, [sp, #16]
   15fc8:	ldp	x29, x30, [sp], #64
   15fcc:	ret
   15fd0:	bics	xzr, x21, x10
   15fd4:	b	15fb8 <__gmpz_congruent_2exp_p@@Base+0x188>
   15fd8:	mov	w0, #0x1                   	// #1
   15fdc:	b	15fbc <__gmpz_congruent_2exp_p@@Base+0x18c>

0000000000015fe0 <__gmpz_congruent_ui_p@@Base>:
   15fe0:	stp	x29, x30, [sp, #-32]!
   15fe4:	stp	x20, x19, [sp, #16]
   15fe8:	mov	x20, x1
   15fec:	mov	x29, sp
   15ff0:	cbz	x2, 160fc <__gmpz_congruent_ui_p@@Base+0x11c>
   15ff4:	ldrsw	x1, [x0, #4]
   15ff8:	mov	x19, x2
   15ffc:	cbz	w1, 1602c <__gmpz_congruent_ui_p@@Base+0x4c>
   16000:	tbz	w1, #31, 16058 <__gmpz_congruent_ui_p@@Base+0x78>
   16004:	subs	x8, x19, x20
   16008:	neg	x1, x1
   1600c:	b.cs	16054 <__gmpz_congruent_ui_p@@Base+0x74>  // b.hs, b.nlast
   16010:	clz	x8, x19
   16014:	lsl	x8, x19, x8
   16018:	cmp	x8, x20
   1601c:	cset	w9, cc  // cc = lo, ul, last
   16020:	lsl	x8, x8, x9
   16024:	sub	x20, x8, x20
   16028:	b	16058 <__gmpz_congruent_ui_p@@Base+0x78>
   1602c:	cmp	x19, x20
   16030:	b.ls	16040 <__gmpz_congruent_ui_p@@Base+0x60>  // b.plast
   16034:	cmp	x20, #0x0
   16038:	cset	w0, eq  // eq = none
   1603c:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>
   16040:	udiv	x8, x20, x19
   16044:	msub	x8, x8, x19, x20
   16048:	cmp	x8, #0x0
   1604c:	cset	w0, eq  // eq = none
   16050:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>
   16054:	mov	x20, x8
   16058:	ldr	x0, [x0, #8]
   1605c:	cmp	x1, #0x28
   16060:	b.lt	16080 <__gmpz_congruent_ui_p@@Base+0xa0>  // b.tstop
   16064:	mov	x2, x19
   16068:	bl	c540 <__gmpn_mod_1@plt>
   1606c:	cmp	x20, x19
   16070:	b.cs	160b8 <__gmpz_congruent_ui_p@@Base+0xd8>  // b.hs, b.nlast
   16074:	cmp	x0, x20
   16078:	cset	w0, eq  // eq = none
   1607c:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>
   16080:	tbnz	w19, #0, 160d0 <__gmpz_congruent_ui_p@@Base+0xf0>
   16084:	ldr	x8, [x0]
   16088:	neg	x9, x19
   1608c:	and	x9, x9, x19
   16090:	rbit	x10, x19
   16094:	sub	x9, x9, #0x1
   16098:	sub	x8, x8, x20
   1609c:	clz	x10, x10
   160a0:	tst	x8, x9
   160a4:	and	x11, x8, x9
   160a8:	csel	x8, x10, xzr, eq  // eq = none
   160ac:	cbz	x11, 160cc <__gmpz_congruent_ui_p@@Base+0xec>
   160b0:	mov	w0, wzr
   160b4:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>
   160b8:	udiv	x8, x20, x19
   160bc:	msub	x8, x8, x19, x20
   160c0:	cmp	x0, x8
   160c4:	cset	w0, eq  // eq = none
   160c8:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>
   160cc:	lsr	x19, x19, x8
   160d0:	mov	x2, x19
   160d4:	mov	x3, x20
   160d8:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   160dc:	cmp	x0, #0x0
   160e0:	cset	w8, eq  // eq = none
   160e4:	cmp	x0, x19
   160e8:	cset	w9, eq  // eq = none
   160ec:	orr	w0, w8, w9
   160f0:	ldp	x20, x19, [sp, #16]
   160f4:	ldp	x29, x30, [sp], #32
   160f8:	ret
   160fc:	mov	x1, x20
   16100:	bl	d3d0 <__gmpz_cmp_ui@plt>
   16104:	cmp	w0, #0x0
   16108:	cset	w0, eq  // eq = none
   1610c:	b	160f0 <__gmpz_congruent_ui_p@@Base+0x110>

0000000000016110 <__gmpz_divexact@@Base>:
   16110:	stp	x29, x30, [sp, #-80]!
   16114:	stp	x24, x23, [sp, #32]
   16118:	stp	x22, x21, [sp, #48]
   1611c:	stp	x20, x19, [sp, #64]
   16120:	ldr	w8, [x1, #4]
   16124:	ldr	w9, [x2, #4]
   16128:	mov	x19, x0
   1612c:	str	x25, [sp, #16]
   16130:	cmp	w8, #0x0
   16134:	cneg	w23, w8, mi  // mi = first
   16138:	cmp	w9, #0x0
   1613c:	cneg	w22, w9, mi  // mi = first
   16140:	cmp	w23, w22
   16144:	mov	x29, sp
   16148:	b.cs	16154 <__gmpz_divexact@@Base+0x44>  // b.hs, b.nlast
   1614c:	str	wzr, [x19, #4]
   16150:	b	1622c <__gmpz_divexact@@Base+0x11c>
   16154:	sub	x25, x23, x22
   16158:	mov	x20, x1
   1615c:	mov	x21, x2
   16160:	cmp	x19, x1
   16164:	add	x1, x25, #0x1
   16168:	str	xzr, [x29, #24]
   1616c:	b.eq	1618c <__gmpz_divexact@@Base+0x7c>  // b.none
   16170:	cmp	x19, x21
   16174:	b.eq	1618c <__gmpz_divexact@@Base+0x7c>  // b.none
   16178:	ldrsw	x8, [x19]
   1617c:	cmp	x25, x8
   16180:	b.ge	16260 <__gmpz_divexact@@Base+0x150>  // b.tcont
   16184:	ldr	x24, [x19, #8]
   16188:	b	161b0 <__gmpz_divexact@@Base+0xa0>
   1618c:	lsl	x1, x1, #3
   16190:	mov	w8, #0x7f00                	// #32512
   16194:	cmp	x1, x8
   16198:	b.hi	16270 <__gmpz_divexact@@Base+0x160>  // b.pmore
   1619c:	add	x9, x1, #0xf
   161a0:	mov	x8, sp
   161a4:	and	x9, x9, #0xfffffffffffffff0
   161a8:	sub	x24, x8, x9
   161ac:	mov	sp, x24
   161b0:	ldr	x1, [x20, #8]
   161b4:	ldr	x3, [x21, #8]
   161b8:	mov	x0, x24
   161bc:	mov	x2, x23
   161c0:	mov	x4, x22
   161c4:	bl	c5a0 <__gmpn_divexact@plt>
   161c8:	add	x22, x25, #0x1
   161cc:	mov	x23, x25
   161d0:	cmp	x22, #0x1
   161d4:	b.lt	161e4 <__gmpz_divexact@@Base+0xd4>  // b.tstop
   161d8:	ldr	x8, [x24, x23, lsl #3]
   161dc:	sub	x25, x23, #0x1
   161e0:	cbz	x8, 161c8 <__gmpz_divexact@@Base+0xb8>
   161e4:	ldr	x0, [x19, #8]
   161e8:	cmp	x24, x0
   161ec:	b.eq	16208 <__gmpz_divexact@@Base+0xf8>  // b.none
   161f0:	ldrsw	x8, [x19]
   161f4:	cmp	x22, x8
   161f8:	b.gt	16250 <__gmpz_divexact@@Base+0x140>
   161fc:	mov	x1, x24
   16200:	mov	x2, x22
   16204:	bl	cc10 <__gmpn_copyi@plt>
   16208:	ldr	w8, [x20, #4]
   1620c:	ldr	w9, [x21, #4]
   16210:	eor	w8, w9, w8
   16214:	mvn	w9, w23
   16218:	cmp	w8, #0x0
   1621c:	csel	x8, x22, x9, ge  // ge = tcont
   16220:	str	w8, [x19, #4]
   16224:	ldr	x0, [x29, #24]
   16228:	cbnz	x0, 16248 <__gmpz_divexact@@Base+0x138>
   1622c:	mov	sp, x29
   16230:	ldp	x20, x19, [sp, #64]
   16234:	ldp	x22, x21, [sp, #48]
   16238:	ldp	x24, x23, [sp, #32]
   1623c:	ldr	x25, [sp, #16]
   16240:	ldp	x29, x30, [sp], #80
   16244:	ret
   16248:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1624c:	b	1622c <__gmpz_divexact@@Base+0x11c>
   16250:	mov	x0, x19
   16254:	mov	x1, x22
   16258:	bl	c1c0 <__gmpz_realloc@plt>
   1625c:	b	161fc <__gmpz_divexact@@Base+0xec>
   16260:	mov	x0, x19
   16264:	bl	c1c0 <__gmpz_realloc@plt>
   16268:	mov	x24, x0
   1626c:	b	161b0 <__gmpz_divexact@@Base+0xa0>
   16270:	add	x0, x29, #0x18
   16274:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   16278:	mov	x24, x0
   1627c:	b	161b0 <__gmpz_divexact@@Base+0xa0>

0000000000016280 <__gmpz_divexact_gcd@@Base>:
   16280:	stp	x29, x30, [sp, #-32]!
   16284:	stp	x20, x19, [sp, #16]
   16288:	ldr	w8, [x1, #4]
   1628c:	mov	x19, x0
   16290:	mov	x29, sp
   16294:	cbz	w8, 162f4 <__gmpz_divexact_gcd@@Base+0x74>
   16298:	ldr	w8, [x2, #4]
   1629c:	cmp	w8, #0x1
   162a0:	b.ne	162fc <__gmpz_divexact_gcd@@Base+0x7c>  // b.any
   162a4:	ldr	x8, [x2, #8]
   162a8:	ldr	x20, [x8]
   162ac:	tbnz	w20, #0, 162c8 <__gmpz_divexact_gcd@@Base+0x48>
   162b0:	rbit	x8, x20
   162b4:	clz	x2, x8
   162b8:	mov	x0, x19
   162bc:	lsr	x20, x20, x2
   162c0:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   162c4:	mov	x1, x19
   162c8:	cmp	x20, #0x5
   162cc:	b.eq	16310 <__gmpz_divexact_gcd@@Base+0x90>  // b.none
   162d0:	cmp	x20, #0x3
   162d4:	b.eq	1631c <__gmpz_divexact_gcd@@Base+0x9c>  // b.none
   162d8:	cmp	x20, #0x1
   162dc:	b.ne	16328 <__gmpz_divexact_gcd@@Base+0xa8>  // b.any
   162e0:	cmp	x1, x19
   162e4:	b.eq	16304 <__gmpz_divexact_gcd@@Base+0x84>  // b.none
   162e8:	mov	x0, x19
   162ec:	bl	c590 <__gmpz_set@plt>
   162f0:	b	16304 <__gmpz_divexact_gcd@@Base+0x84>
   162f4:	str	wzr, [x19, #4]
   162f8:	b	16304 <__gmpz_divexact_gcd@@Base+0x84>
   162fc:	mov	x0, x19
   16300:	bl	c550 <__gmpz_divexact@plt>
   16304:	ldp	x20, x19, [sp, #16]
   16308:	ldp	x29, x30, [sp], #32
   1630c:	ret
   16310:	mov	x0, x19
   16314:	bl	163d4 <__gmpz_divexact_gcd@@Base+0x154>
   16318:	b	16304 <__gmpz_divexact_gcd@@Base+0x84>
   1631c:	mov	x0, x19
   16320:	bl	16338 <__gmpz_divexact_gcd@@Base+0xb8>
   16324:	b	16304 <__gmpz_divexact_gcd@@Base+0x84>
   16328:	mov	x0, x19
   1632c:	mov	x2, x20
   16330:	bl	16470 <__gmpz_divexact_gcd@@Base+0x1f0>
   16334:	b	16304 <__gmpz_divexact_gcd@@Base+0x84>
   16338:	stp	x29, x30, [sp, #-64]!
   1633c:	stp	x22, x21, [sp, #32]
   16340:	stp	x20, x19, [sp, #48]
   16344:	str	x23, [sp, #16]
   16348:	ldrsw	x23, [x1, #4]
   1634c:	ldrsw	x8, [x0]
   16350:	mov	x21, x1
   16354:	mov	x19, x0
   16358:	cmp	x23, #0x0
   1635c:	cneg	x20, x23, mi  // mi = first
   16360:	cmp	x20, x8
   16364:	mov	x29, sp
   16368:	b.gt	163c0 <__gmpz_divexact_gcd@@Base+0x140>
   1636c:	ldr	x22, [x19, #8]
   16370:	ldr	x1, [x21, #8]
   16374:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   16378:	mov	x0, x22
   1637c:	mov	x2, x20
   16380:	mov	x4, xzr
   16384:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   16388:	add	x8, x22, x20, lsl #3
   1638c:	ldur	x8, [x8, #-8]
   16390:	cmp	x8, #0x0
   16394:	cset	w8, eq  // eq = none
   16398:	sub	x8, x20, x8
   1639c:	neg	w9, w8
   163a0:	cmp	w23, #0x0
   163a4:	csel	x8, x8, x9, gt
   163a8:	str	w8, [x19, #4]
   163ac:	ldp	x20, x19, [sp, #48]
   163b0:	ldp	x22, x21, [sp, #32]
   163b4:	ldr	x23, [sp, #16]
   163b8:	ldp	x29, x30, [sp], #64
   163bc:	ret
   163c0:	mov	x0, x19
   163c4:	mov	x1, x20
   163c8:	bl	c1c0 <__gmpz_realloc@plt>
   163cc:	mov	x22, x0
   163d0:	b	16370 <__gmpz_divexact_gcd@@Base+0xf0>
   163d4:	stp	x29, x30, [sp, #-64]!
   163d8:	stp	x22, x21, [sp, #32]
   163dc:	stp	x20, x19, [sp, #48]
   163e0:	str	x23, [sp, #16]
   163e4:	ldrsw	x23, [x1, #4]
   163e8:	ldrsw	x8, [x0]
   163ec:	mov	x21, x1
   163f0:	mov	x19, x0
   163f4:	cmp	x23, #0x0
   163f8:	cneg	x20, x23, mi  // mi = first
   163fc:	cmp	x20, x8
   16400:	mov	x29, sp
   16404:	b.gt	1645c <__gmpz_divexact_gcd@@Base+0x1dc>
   16408:	ldr	x22, [x19, #8]
   1640c:	ldr	x1, [x21, #8]
   16410:	mov	x3, #0x3333333333333333    	// #3689348814741910323
   16414:	mov	x0, x22
   16418:	mov	x2, x20
   1641c:	mov	x4, xzr
   16420:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   16424:	add	x8, x22, x20, lsl #3
   16428:	ldur	x8, [x8, #-8]
   1642c:	cmp	x8, #0x0
   16430:	cset	w8, eq  // eq = none
   16434:	sub	x8, x20, x8
   16438:	neg	w9, w8
   1643c:	cmp	w23, #0x0
   16440:	csel	x8, x8, x9, gt
   16444:	str	w8, [x19, #4]
   16448:	ldp	x20, x19, [sp, #48]
   1644c:	ldp	x22, x21, [sp, #32]
   16450:	ldr	x23, [sp, #16]
   16454:	ldp	x29, x30, [sp], #64
   16458:	ret
   1645c:	mov	x0, x19
   16460:	mov	x1, x20
   16464:	bl	c1c0 <__gmpz_realloc@plt>
   16468:	mov	x22, x0
   1646c:	b	1640c <__gmpz_divexact_gcd@@Base+0x18c>
   16470:	stp	x29, x30, [sp, #-64]!
   16474:	stp	x24, x23, [sp, #16]
   16478:	stp	x22, x21, [sp, #32]
   1647c:	stp	x20, x19, [sp, #48]
   16480:	ldrsw	x24, [x1, #4]
   16484:	ldrsw	x8, [x0]
   16488:	mov	x21, x1
   1648c:	mov	x19, x0
   16490:	cmp	x24, #0x0
   16494:	cneg	x20, x24, mi  // mi = first
   16498:	cmp	x20, x8
   1649c:	mov	x22, x2
   164a0:	mov	x29, sp
   164a4:	b.gt	164f8 <__gmpz_divexact_gcd@@Base+0x278>
   164a8:	ldr	x23, [x19, #8]
   164ac:	ldr	x1, [x21, #8]
   164b0:	mov	x0, x23
   164b4:	mov	x2, x20
   164b8:	mov	x3, x22
   164bc:	bl	c910 <__gmpn_divexact_1@plt>
   164c0:	add	x8, x23, x20, lsl #3
   164c4:	ldur	x8, [x8, #-8]
   164c8:	cmp	x8, #0x0
   164cc:	cset	w8, eq  // eq = none
   164d0:	sub	x8, x20, x8
   164d4:	neg	w9, w8
   164d8:	cmp	w24, #0x0
   164dc:	csel	x8, x8, x9, gt
   164e0:	str	w8, [x19, #4]
   164e4:	ldp	x20, x19, [sp, #48]
   164e8:	ldp	x22, x21, [sp, #32]
   164ec:	ldp	x24, x23, [sp, #16]
   164f0:	ldp	x29, x30, [sp], #64
   164f4:	ret
   164f8:	mov	x0, x19
   164fc:	mov	x1, x20
   16500:	bl	c1c0 <__gmpz_realloc@plt>
   16504:	mov	x23, x0
   16508:	b	164ac <__gmpz_divexact_gcd@@Base+0x22c>

000000000001650c <__gmpz_divexact_ui@@Base>:
   1650c:	stp	x29, x30, [sp, #-64]!
   16510:	stp	x24, x23, [sp, #16]
   16514:	stp	x22, x21, [sp, #32]
   16518:	stp	x20, x19, [sp, #48]
   1651c:	mov	x29, sp
   16520:	cbz	x2, 165b4 <__gmpz_divexact_ui@@Base+0xa8>
   16524:	ldrsw	x24, [x1, #4]
   16528:	mov	x21, x1
   1652c:	mov	x19, x0
   16530:	cbz	w24, 16584 <__gmpz_divexact_ui@@Base+0x78>
   16534:	ldrsw	x8, [x19]
   16538:	cmp	w24, #0x0
   1653c:	cneg	x22, x24, lt  // lt = tstop
   16540:	mov	x20, x2
   16544:	cmp	x22, x8
   16548:	b.gt	165a0 <__gmpz_divexact_ui@@Base+0x94>
   1654c:	ldr	x23, [x19, #8]
   16550:	ldr	x1, [x21, #8]
   16554:	mov	x0, x23
   16558:	mov	x2, x22
   1655c:	mov	x3, x20
   16560:	bl	c910 <__gmpn_divexact_1@plt>
   16564:	add	x8, x23, x22, lsl #3
   16568:	ldur	x8, [x8, #-8]
   1656c:	cmp	x8, #0x0
   16570:	cset	w8, eq  // eq = none
   16574:	sub	w8, w22, w8
   16578:	cmp	w24, #0x0
   1657c:	cneg	w8, w8, lt  // lt = tstop
   16580:	b	16588 <__gmpz_divexact_ui@@Base+0x7c>
   16584:	mov	w8, wzr
   16588:	str	w8, [x19, #4]
   1658c:	ldp	x20, x19, [sp, #48]
   16590:	ldp	x22, x21, [sp, #32]
   16594:	ldp	x24, x23, [sp, #16]
   16598:	ldp	x29, x30, [sp], #64
   1659c:	ret
   165a0:	mov	x0, x19
   165a4:	mov	x1, x22
   165a8:	bl	c1c0 <__gmpz_realloc@plt>
   165ac:	mov	x23, x0
   165b0:	b	16550 <__gmpz_divexact_ui@@Base+0x44>
   165b4:	bl	c100 <__gmp_divide_by_zero@plt>

00000000000165b8 <__gmpz_divisible_p@@Base>:
   165b8:	stp	x29, x30, [sp, #-16]!
   165bc:	ldrsw	x8, [x1, #4]
   165c0:	ldrsw	x9, [x0, #4]
   165c4:	mov	x29, sp
   165c8:	cbz	w8, 165f0 <__gmpz_divisible_p@@Base+0x38>
   165cc:	ldr	x0, [x0, #8]
   165d0:	ldr	x2, [x1, #8]
   165d4:	cmp	x9, #0x0
   165d8:	cneg	x1, x9, mi  // mi = first
   165dc:	cmp	x8, #0x0
   165e0:	cneg	x3, x8, mi  // mi = first
   165e4:	bl	d540 <__gmpn_divisible_p@plt>
   165e8:	ldp	x29, x30, [sp], #16
   165ec:	ret
   165f0:	cmp	w9, #0x0
   165f4:	cset	w0, eq  // eq = none
   165f8:	ldp	x29, x30, [sp], #16
   165fc:	ret

0000000000016600 <__gmpz_divisible_ui_p@@Base>:
   16600:	stp	x29, x30, [sp, #-16]!
   16604:	ldrsw	x9, [x0, #4]
   16608:	mov	x8, x0
   1660c:	mov	x29, sp
   16610:	cmp	x9, #0x0
   16614:	cset	w10, eq  // eq = none
   16618:	cmp	x1, #0x0
   1661c:	cset	w11, ne  // ne = any
   16620:	orr	w0, w10, w11
   16624:	cbz	x1, 16690 <__gmpz_divisible_ui_p@@Base+0x90>
   16628:	cbz	w9, 16690 <__gmpz_divisible_ui_p@@Base+0x90>
   1662c:	ldr	x0, [x8, #8]
   16630:	cmp	x9, #0x0
   16634:	mov	x2, x1
   16638:	cneg	x1, x9, mi  // mi = first
   1663c:	cmp	x1, #0x28
   16640:	b.lt	1664c <__gmpz_divisible_ui_p@@Base+0x4c>  // b.tstop
   16644:	bl	c540 <__gmpn_mod_1@plt>
   16648:	b	16688 <__gmpz_divisible_ui_p@@Base+0x88>
   1664c:	tbnz	w2, #0, 16680 <__gmpz_divisible_ui_p@@Base+0x80>
   16650:	ldr	x8, [x0]
   16654:	neg	x9, x2
   16658:	and	x9, x9, x2
   1665c:	sub	x9, x9, #0x1
   16660:	tst	x8, x9
   16664:	b.eq	16674 <__gmpz_divisible_ui_p@@Base+0x74>  // b.none
   16668:	mov	w0, wzr
   1666c:	ldp	x29, x30, [sp], #16
   16670:	ret
   16674:	rbit	x8, x2
   16678:	clz	x8, x8
   1667c:	lsr	x2, x2, x8
   16680:	mov	x3, xzr
   16684:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   16688:	cmp	x0, #0x0
   1668c:	cset	w0, eq  // eq = none
   16690:	ldp	x29, x30, [sp], #16
   16694:	ret

0000000000016698 <__gmpz_divisible_2exp_p@@Base>:
   16698:	ldr	w8, [x0, #4]
   1669c:	cmp	w8, #0x0
   166a0:	cneg	w9, w8, mi  // mi = first
   166a4:	lsr	x8, x1, #6
   166a8:	cmp	x8, x9
   166ac:	b.cs	166ec <__gmpz_divisible_2exp_p@@Base+0x54>  // b.hs, b.nlast
   166b0:	ldr	x9, [x0, #8]
   166b4:	cbz	x8, 166d4 <__gmpz_divisible_2exp_p@@Base+0x3c>
   166b8:	mov	x10, x9
   166bc:	mov	x11, x8
   166c0:	ldr	x12, [x10]
   166c4:	cbnz	x12, 166f8 <__gmpz_divisible_2exp_p@@Base+0x60>
   166c8:	subs	x11, x11, #0x1
   166cc:	add	x10, x10, #0x8
   166d0:	b.ne	166c0 <__gmpz_divisible_2exp_p@@Base+0x28>  // b.any
   166d4:	ldr	x8, [x9, x8, lsl #3]
   166d8:	mov	x9, #0xffffffffffffffff    	// #-1
   166dc:	lsl	x9, x9, x1
   166e0:	bics	xzr, x8, x9
   166e4:	cset	w0, eq  // eq = none
   166e8:	ret
   166ec:	cmp	w9, #0x0
   166f0:	cset	w0, eq  // eq = none
   166f4:	ret
   166f8:	mov	w0, wzr
   166fc:	ret

0000000000016700 <__gmpz_dump@@Base>:
   16700:	stp	x29, x30, [sp, #-32]!
   16704:	mov	x2, x0
   16708:	mov	w1, #0xa                   	// #10
   1670c:	mov	x0, xzr
   16710:	stp	x20, x19, [sp, #16]
   16714:	mov	x29, sp
   16718:	bl	c500 <__gmpz_get_str@plt>
   1671c:	mov	x19, x0
   16720:	bl	cb70 <puts@plt>
   16724:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   16728:	ldr	x8, [x8, #4016]
   1672c:	mov	x0, x19
   16730:	ldr	x20, [x8]
   16734:	bl	c090 <strlen@plt>
   16738:	add	x1, x0, #0x1
   1673c:	mov	x0, x19
   16740:	blr	x20
   16744:	ldp	x20, x19, [sp, #16]
   16748:	ldp	x29, x30, [sp], #32
   1674c:	ret

0000000000016750 <__gmpz_export@@Base>:
   16750:	sub	sp, sp, #0x70
   16754:	stp	x29, x30, [sp, #16]
   16758:	stp	x28, x27, [sp, #32]
   1675c:	stp	x26, x25, [sp, #48]
   16760:	stp	x24, x23, [sp, #64]
   16764:	stp	x22, x21, [sp, #80]
   16768:	stp	x20, x19, [sp, #96]
   1676c:	ldrsw	x9, [x6, #4]
   16770:	cmp	x1, #0x0
   16774:	add	x8, sp, #0x8
   16778:	mov	x19, x0
   1677c:	csel	x8, x8, x1, eq  // eq = none
   16780:	add	x29, sp, #0x10
   16784:	cbz	w9, 1696c <__gmpz_export@@Base+0x21c>
   16788:	ldr	x21, [x6, #8]
   1678c:	cmp	x9, #0x0
   16790:	cneg	x26, x9, mi  // mi = first
   16794:	lsl	x10, x3, #3
   16798:	add	x9, x21, x26, lsl #3
   1679c:	ldur	x9, [x9, #-8]
   167a0:	sub	x27, x10, x5
   167a4:	add	x10, x27, x26, lsl #6
   167a8:	mov	x24, x5
   167ac:	clz	x9, x9
   167b0:	mvn	x9, x9
   167b4:	add	x28, x9, x10
   167b8:	mov	w25, w4
   167bc:	mov	x22, x3
   167c0:	mov	w23, w2
   167c4:	udiv	x20, x28, x27
   167c8:	str	x20, [x8]
   167cc:	cbnz	x19, 167e8 <__gmpz_export@@Base+0x98>
   167d0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   167d4:	ldr	x8, [x8, #3840]
   167d8:	mul	x0, x20, x22
   167dc:	ldr	x8, [x8]
   167e0:	blr	x8
   167e4:	mov	x19, x0
   167e8:	cmp	w25, #0x0
   167ec:	csinv	w17, w25, wzr, ne  // ne = any
   167f0:	cbz	x24, 16994 <__gmpz_export@@Base+0x244>
   167f4:	cmp	w17, #0x0
   167f8:	cneg	x12, x22, lt  // lt = tstop
   167fc:	cmp	w23, #0x0
   16800:	cneg	x13, x22, ge  // ge = tcont
   16804:	cmp	x27, x28
   16808:	b.hi	16970 <__gmpz_export@@Base+0x220>  // b.pmore
   1680c:	sub	x16, x20, #0x1
   16810:	mov	x8, xzr
   16814:	and	w11, w27, #0x7
   16818:	mov	x15, #0xffffffffffffffff    	// #-1
   1681c:	cmp	w23, #0x0
   16820:	mov	w0, #0x40                  	// #64
   16824:	mul	x2, x16, x22
   16828:	sub	x18, x22, #0x1
   1682c:	lsl	x1, x15, x11
   16830:	sub	w15, w0, w11
   16834:	csel	x0, x2, x8, ge  // ge = tcont
   16838:	cmp	w17, #0x0
   1683c:	sub	x14, x8, w17, sxtw
   16840:	add	x17, x19, x0
   16844:	csel	x18, x18, x8, ge  // ge = tcont
   16848:	mov	w9, wzr
   1684c:	lsr	x10, x27, #3
   16850:	add	x12, x12, x13
   16854:	add	x13, x21, x26, lsl #3
   16858:	mvn	x16, x1
   1685c:	add	x17, x17, x18
   16860:	mov	w18, #0x8                   	// #8
   16864:	mov	x0, x8
   16868:	b	1687c <__gmpz_export@@Base+0x12c>
   1686c:	add	x8, x8, #0x1
   16870:	cmp	x8, x20
   16874:	add	x17, x17, x12
   16878:	b.cs	16970 <__gmpz_export@@Base+0x220>  // b.hs, b.nlast
   1687c:	cbz	x10, 168f0 <__gmpz_export@@Base+0x1a0>
   16880:	mov	x1, x10
   16884:	b	168a4 <__gmpz_export@@Base+0x154>
   16888:	strb	w0, [x17]
   1688c:	lsr	x0, x0, #8
   16890:	mov	w2, #0xfffffff8            	// #-8
   16894:	add	w9, w9, w2
   16898:	subs	x1, x1, #0x1
   1689c:	add	x17, x17, x14
   168a0:	b.eq	168dc <__gmpz_export@@Base+0x18c>  // b.none
   168a4:	cmp	w9, #0x8
   168a8:	b.ge	16888 <__gmpz_export@@Base+0x138>  // b.tcont
   168ac:	cmp	x21, x13
   168b0:	b.eq	168bc <__gmpz_export@@Base+0x16c>  // b.none
   168b4:	ldr	x2, [x21], #8
   168b8:	b	168c0 <__gmpz_export@@Base+0x170>
   168bc:	mov	x2, xzr
   168c0:	lsl	x3, x2, x9
   168c4:	sub	w4, w18, w9
   168c8:	orr	w3, w3, w0
   168cc:	lsr	x0, x2, x4
   168d0:	mov	w2, #0x38                  	// #56
   168d4:	strb	w3, [x17]
   168d8:	b	16894 <__gmpz_export@@Base+0x144>
   168dc:	mov	x1, x10
   168e0:	cbnz	w11, 168f8 <__gmpz_export@@Base+0x1a8>
   168e4:	cmp	x1, x22
   168e8:	b.cs	1686c <__gmpz_export@@Base+0x11c>  // b.hs, b.nlast
   168ec:	b	16954 <__gmpz_export@@Base+0x204>
   168f0:	mov	x1, xzr
   168f4:	cbz	w11, 168e4 <__gmpz_export@@Base+0x194>
   168f8:	cmp	w9, w11
   168fc:	b.ge	16910 <__gmpz_export@@Base+0x1c0>  // b.tcont
   16900:	cmp	x21, x13
   16904:	b.eq	16924 <__gmpz_export@@Base+0x1d4>  // b.none
   16908:	ldr	x2, [x21], #8
   1690c:	b	16928 <__gmpz_export@@Base+0x1d8>
   16910:	and	w2, w0, w16
   16914:	lsr	x0, x0, x11
   16918:	strb	w2, [x17]
   1691c:	sub	w9, w9, w11
   16920:	b	16944 <__gmpz_export@@Base+0x1f4>
   16924:	mov	x2, xzr
   16928:	lsl	x3, x2, x9
   1692c:	sub	w4, w11, w9
   16930:	orr	w3, w3, w0
   16934:	lsr	x0, x2, x4
   16938:	and	w2, w3, w16
   1693c:	add	w9, w15, w9
   16940:	strb	w2, [x17]
   16944:	add	x17, x17, x14
   16948:	add	x1, x1, #0x1
   1694c:	cmp	x1, x22
   16950:	b.cs	1686c <__gmpz_export@@Base+0x11c>  // b.hs, b.nlast
   16954:	sub	x1, x22, x1
   16958:	strb	wzr, [x17]
   1695c:	subs	x1, x1, #0x1
   16960:	add	x17, x17, x14
   16964:	b.ne	16958 <__gmpz_export@@Base+0x208>  // b.any
   16968:	b	1686c <__gmpz_export@@Base+0x11c>
   1696c:	str	xzr, [x8]
   16970:	mov	x0, x19
   16974:	ldp	x20, x19, [sp, #96]
   16978:	ldp	x22, x21, [sp, #80]
   1697c:	ldp	x24, x23, [sp, #64]
   16980:	ldp	x26, x25, [sp, #48]
   16984:	ldp	x28, x27, [sp, #32]
   16988:	ldp	x29, x30, [sp, #16]
   1698c:	add	sp, sp, #0x70
   16990:	ret
   16994:	cmp	x22, #0x8
   16998:	b.ne	167f4 <__gmpz_export@@Base+0xa4>  // b.any
   1699c:	and	x8, x19, #0x7
   169a0:	cbnz	x8, 167f4 <__gmpz_export@@Base+0xa4>
   169a4:	and	w8, w17, w23
   169a8:	cmn	w8, #0x1
   169ac:	b.eq	169e8 <__gmpz_export@@Base+0x298>  // b.none
   169b0:	cmp	w23, #0x1
   169b4:	b.ne	169fc <__gmpz_export@@Base+0x2ac>  // b.any
   169b8:	cmn	w17, #0x1
   169bc:	b.ne	169fc <__gmpz_export@@Base+0x2ac>  // b.any
   169c0:	cmp	x20, #0x1
   169c4:	b.lt	16970 <__gmpz_export@@Base+0x220>  // b.tstop
   169c8:	mov	x8, xzr
   169cc:	add	x9, x21, x20, lsl #3
   169d0:	ldr	x10, [x9, #-8]!
   169d4:	str	x10, [x19, x8, lsl #3]
   169d8:	add	x8, x8, #0x1
   169dc:	cmp	x8, x20
   169e0:	b.lt	169d0 <__gmpz_export@@Base+0x280>  // b.tstop
   169e4:	b	16970 <__gmpz_export@@Base+0x220>
   169e8:	mov	x0, x19
   169ec:	mov	x1, x21
   169f0:	mov	x2, x20
   169f4:	bl	cc10 <__gmpn_copyi@plt>
   169f8:	b	16970 <__gmpz_export@@Base+0x220>
   169fc:	cmn	w23, #0x1
   16a00:	b.ne	16a74 <__gmpz_export@@Base+0x324>  // b.any
   16a04:	cmp	w17, #0x1
   16a08:	b.ne	16a74 <__gmpz_export@@Base+0x324>  // b.any
   16a0c:	cmp	x20, #0x1
   16a10:	b.lt	16970 <__gmpz_export@@Base+0x220>  // b.tstop
   16a14:	mov	x8, xzr
   16a18:	lsl	x9, x8, #3
   16a1c:	ldr	x10, [x21, x9]
   16a20:	add	x8, x8, #0x1
   16a24:	cmp	x8, x20
   16a28:	lsl	x13, x10, #40
   16a2c:	and	x13, x13, #0xff000000000000
   16a30:	lsr	x12, x10, #16
   16a34:	bfi	x13, x10, #56, #8
   16a38:	lsr	x11, x10, #24
   16a3c:	bfi	x13, x12, #40, #8
   16a40:	lsr	x12, x10, #8
   16a44:	and	x12, x12, #0xff000000
   16a48:	bfi	x13, x11, #32, #8
   16a4c:	orr	x12, x13, x12
   16a50:	lsr	x13, x10, #40
   16a54:	and	x11, x11, #0xff0000
   16a58:	and	x13, x13, #0xff00
   16a5c:	orr	x11, x12, x11
   16a60:	orr	x11, x11, x13
   16a64:	add	x10, x11, x10, lsr #56
   16a68:	str	x10, [x19, x9]
   16a6c:	b.lt	16a18 <__gmpz_export@@Base+0x2c8>  // b.tstop
   16a70:	b	16970 <__gmpz_export@@Base+0x220>
   16a74:	cmp	w23, #0x1
   16a78:	b.ne	167f4 <__gmpz_export@@Base+0xa4>  // b.any
   16a7c:	cmp	w17, #0x1
   16a80:	b.ne	167f4 <__gmpz_export@@Base+0xa4>  // b.any
   16a84:	cmp	x20, #0x1
   16a88:	b.lt	16970 <__gmpz_export@@Base+0x220>  // b.tstop
   16a8c:	mov	x8, xzr
   16a90:	add	x9, x21, x20, lsl #3
   16a94:	ldr	x10, [x9, #-8]!
   16a98:	lsl	x13, x10, #40
   16a9c:	and	x13, x13, #0xff000000000000
   16aa0:	lsr	x12, x10, #16
   16aa4:	bfi	x13, x10, #56, #8
   16aa8:	lsr	x11, x10, #24
   16aac:	bfi	x13, x12, #40, #8
   16ab0:	lsr	x12, x10, #8
   16ab4:	and	x12, x12, #0xff000000
   16ab8:	bfi	x13, x11, #32, #8
   16abc:	orr	x12, x13, x12
   16ac0:	lsr	x13, x10, #40
   16ac4:	and	x11, x11, #0xff0000
   16ac8:	and	x13, x13, #0xff00
   16acc:	orr	x11, x12, x11
   16ad0:	orr	x11, x11, x13
   16ad4:	add	x10, x11, x10, lsr #56
   16ad8:	str	x10, [x19, x8, lsl #3]
   16adc:	add	x8, x8, #0x1
   16ae0:	cmp	x8, x20
   16ae4:	b.lt	16a94 <__gmpz_export@@Base+0x344>  // b.tstop
   16ae8:	b	16970 <__gmpz_export@@Base+0x220>

0000000000016aec <__gmpz_mfac_uiui@@Base>:
   16aec:	stp	x29, x30, [sp, #-64]!
   16af0:	stp	x24, x23, [sp, #16]
   16af4:	stp	x22, x21, [sp, #32]
   16af8:	stp	x20, x19, [sp, #48]
   16afc:	mov	x29, sp
   16b00:	sub	sp, sp, #0x30
   16b04:	mov	x20, x1
   16b08:	subs	x8, x1, #0x3
   16b0c:	mov	x19, x0
   16b10:	b.cc	16b80 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   16b14:	sub	x9, x2, #0x1
   16b18:	mov	x22, x2
   16b1c:	cmp	x8, x9
   16b20:	b.cc	16b80 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   16b24:	sub	x0, x29, #0x8
   16b28:	mov	w1, #0x1                   	// #1
   16b2c:	mov	x2, x22
   16b30:	stur	x20, [x29, #-8]
   16b34:	bl	c0c0 <__gmpn_gcd_1@plt>
   16b38:	mov	x21, x0
   16b3c:	cmp	x0, #0x2
   16b40:	b.cc	16b4c <__gmpz_mfac_uiui@@Base+0x60>  // b.lo, b.ul, b.last
   16b44:	udiv	x20, x20, x21
   16b48:	udiv	x22, x22, x21
   16b4c:	cmp	x22, #0x2
   16b50:	b.hi	16ba8 <__gmpz_mfac_uiui@@Base+0xbc>  // b.pmore
   16b54:	cmp	x22, #0x1
   16b58:	b.ne	16bf0 <__gmpz_mfac_uiui@@Base+0x104>  // b.any
   16b5c:	cmp	x21, #0x3
   16b60:	b.cc	16c98 <__gmpz_mfac_uiui@@Base+0x1ac>  // b.lo, b.ul, b.last
   16b64:	sub	x0, x29, #0x18
   16b68:	bl	d430 <__gmpz_init@plt>
   16b6c:	sub	x0, x29, #0x18
   16b70:	mov	x1, x20
   16b74:	bl	c5c0 <__gmpz_fac_ui@plt>
   16b78:	stur	x20, [x29, #-8]
   16b7c:	b	16d34 <__gmpz_mfac_uiui@@Base+0x248>
   16b80:	ldr	w8, [x19]
   16b84:	cmp	x20, #0x0
   16b88:	cinc	x20, x20, eq  // eq = none
   16b8c:	cmp	w8, #0x0
   16b90:	b.le	16cd0 <__gmpz_mfac_uiui@@Base+0x1e4>
   16b94:	ldr	x0, [x19, #8]
   16b98:	mov	w8, #0x1                   	// #1
   16b9c:	str	x20, [x0]
   16ba0:	str	w8, [x19, #4]
   16ba4:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16ba8:	udiv	x8, x20, x22
   16bac:	cmp	x21, #0x2
   16bb0:	add	x24, x8, #0x1
   16bb4:	sub	x23, x20, x22
   16bb8:	stur	x24, [x29, #-8]
   16bbc:	b.cc	16c1c <__gmpz_mfac_uiui@@Base+0x130>  // b.lo, b.ul, b.last
   16bc0:	mov	x0, x23
   16bc4:	bl	16da8 <__gmpz_mfac_uiui@@Base+0x2bc>
   16bc8:	ldrsw	x8, [x19]
   16bcc:	mov	w9, w0
   16bd0:	udiv	x9, x24, x9
   16bd4:	add	x1, x9, #0x2
   16bd8:	cmp	x1, x8
   16bdc:	b.hi	16ce0 <__gmpz_mfac_uiui@@Base+0x1f4>  // b.pmore
   16be0:	ldr	x24, [x19, #8]
   16be4:	cmp	x23, x22
   16be8:	b.hi	16c60 <__gmpz_mfac_uiui@@Base+0x174>  // b.pmore
   16bec:	b	16d00 <__gmpz_mfac_uiui@@Base+0x214>
   16bf0:	cmp	x21, #0x2
   16bf4:	b.cc	16cb0 <__gmpz_mfac_uiui@@Base+0x1c4>  // b.lo, b.ul, b.last
   16bf8:	sub	x0, x29, #0x18
   16bfc:	bl	d430 <__gmpz_init@plt>
   16c00:	sub	x0, x29, #0x18
   16c04:	mov	x1, x20
   16c08:	bl	c790 <__gmpz_2fac_ui@plt>
   16c0c:	lsr	x8, x20, #1
   16c10:	add	x8, x8, #0x1
   16c14:	stur	x8, [x29, #-8]
   16c18:	b	16d34 <__gmpz_mfac_uiui@@Base+0x248>
   16c1c:	mov	x0, x23
   16c20:	stur	xzr, [x29, #-40]
   16c24:	bl	16da8 <__gmpz_mfac_uiui@@Base+0x2bc>
   16c28:	mov	w8, w0
   16c2c:	udiv	x8, x24, x8
   16c30:	lsl	x8, x8, #3
   16c34:	add	x1, x8, #0x10
   16c38:	mov	w8, #0x7f00                	// #32512
   16c3c:	cmp	x1, x8
   16c40:	b.hi	16cec <__gmpz_mfac_uiui@@Base+0x200>  // b.pmore
   16c44:	add	x9, x1, #0xf
   16c48:	mov	x8, sp
   16c4c:	and	x9, x9, #0xfffffffffffffff0
   16c50:	sub	x24, x8, x9
   16c54:	mov	sp, x24
   16c58:	cmp	x23, x22
   16c5c:	b.ls	16d00 <__gmpz_mfac_uiui@@Base+0x214>  // b.plast
   16c60:	mov	x9, xzr
   16c64:	mov	x8, x23
   16c68:	b	16c7c <__gmpz_mfac_uiui@@Base+0x190>
   16c6c:	mul	x20, x8, x20
   16c70:	sub	x8, x8, x22
   16c74:	cmp	x8, x22
   16c78:	b.ls	16d08 <__gmpz_mfac_uiui@@Base+0x21c>  // b.plast
   16c7c:	umulh	x10, x23, x20
   16c80:	cbz	x10, 16c6c <__gmpz_mfac_uiui@@Base+0x180>
   16c84:	add	x10, x9, #0x1
   16c88:	str	x20, [x24, x9, lsl #3]
   16c8c:	mov	x20, x8
   16c90:	mov	x9, x10
   16c94:	b	16c70 <__gmpz_mfac_uiui@@Base+0x184>
   16c98:	cmp	x21, #0x2
   16c9c:	b.ne	16cc0 <__gmpz_mfac_uiui@@Base+0x1d4>  // b.any
   16ca0:	lsl	x1, x20, #1
   16ca4:	mov	x0, x19
   16ca8:	bl	c790 <__gmpz_2fac_ui@plt>
   16cac:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16cb0:	mov	x0, x19
   16cb4:	mov	x1, x20
   16cb8:	bl	c790 <__gmpz_2fac_ui@plt>
   16cbc:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16cc0:	mov	x0, x19
   16cc4:	mov	x1, x20
   16cc8:	bl	c5c0 <__gmpz_fac_ui@plt>
   16ccc:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16cd0:	mov	w1, #0x1                   	// #1
   16cd4:	mov	x0, x19
   16cd8:	bl	c1c0 <__gmpz_realloc@plt>
   16cdc:	b	16b98 <__gmpz_mfac_uiui@@Base+0xac>
   16ce0:	mov	x0, x19
   16ce4:	bl	c1c0 <__gmpz_realloc@plt>
   16ce8:	b	16cf4 <__gmpz_mfac_uiui@@Base+0x208>
   16cec:	sub	x0, x29, #0x28
   16cf0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   16cf4:	mov	x24, x0
   16cf8:	cmp	x23, x22
   16cfc:	b.hi	16c60 <__gmpz_mfac_uiui@@Base+0x174>  // b.pmore
   16d00:	mov	x9, xzr
   16d04:	mov	x8, x23
   16d08:	add	x10, x24, x9, lsl #3
   16d0c:	add	x22, x9, #0x2
   16d10:	cmp	x21, #0x2
   16d14:	stp	x8, x20, [x10]
   16d18:	b.cc	16d70 <__gmpz_mfac_uiui@@Base+0x284>  // b.lo, b.ul, b.last
   16d1c:	sub	x0, x29, #0x18
   16d20:	bl	d430 <__gmpz_init@plt>
   16d24:	sub	x0, x29, #0x18
   16d28:	mov	x1, x24
   16d2c:	mov	x2, x22
   16d30:	bl	cf40 <__gmpz_prodlimbs@plt>
   16d34:	sub	x0, x29, #0x28
   16d38:	bl	d430 <__gmpz_init@plt>
   16d3c:	ldur	x2, [x29, #-8]
   16d40:	sub	x0, x29, #0x28
   16d44:	mov	x1, x21
   16d48:	bl	cbd0 <__gmpz_ui_pow_ui@plt>
   16d4c:	sub	x1, x29, #0x28
   16d50:	sub	x2, x29, #0x18
   16d54:	mov	x0, x19
   16d58:	bl	c620 <__gmpz_mul@plt>
   16d5c:	sub	x0, x29, #0x28
   16d60:	bl	cd10 <__gmpz_clear@plt>
   16d64:	sub	x0, x29, #0x18
   16d68:	bl	cd10 <__gmpz_clear@plt>
   16d6c:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16d70:	mov	x0, x19
   16d74:	mov	x1, x24
   16d78:	mov	x2, x22
   16d7c:	bl	cf40 <__gmpz_prodlimbs@plt>
   16d80:	ldur	x0, [x29, #-40]
   16d84:	cbnz	x0, 16da0 <__gmpz_mfac_uiui@@Base+0x2b4>
   16d88:	mov	sp, x29
   16d8c:	ldp	x20, x19, [sp, #48]
   16d90:	ldp	x22, x21, [sp, #32]
   16d94:	ldp	x24, x23, [sp, #16]
   16d98:	ldp	x29, x30, [sp], #64
   16d9c:	ret
   16da0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   16da4:	b	16d88 <__gmpz_mfac_uiui@@Base+0x29c>
   16da8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   16dac:	ldr	x9, [x9, #3880]
   16db0:	mov	x8, x0
   16db4:	mov	w0, #0x9                   	// #9
   16db8:	sub	w10, w0, #0x2
   16dbc:	ldr	x10, [x9, w10, uxtw #3]
   16dc0:	sub	w0, w0, #0x1
   16dc4:	cmp	x10, x8
   16dc8:	b.cc	16db8 <__gmpz_mfac_uiui@@Base+0x2cc>  // b.lo, b.ul, b.last
   16dcc:	ret

0000000000016dd0 <__gmpz_2fac_ui@@Base>:
   16dd0:	stp	x29, x30, [sp, #-32]!
   16dd4:	stp	x20, x19, [sp, #16]
   16dd8:	mov	x19, x0
   16ddc:	mov	x29, sp
   16de0:	tbnz	w1, #0, 16e08 <__gmpz_2fac_ui@@Base+0x38>
   16de4:	sub	x8, x1, #0x1
   16de8:	cmp	x8, #0x50
   16dec:	lsr	x8, x1, #1
   16df0:	b.hi	16e44 <__gmpz_2fac_ui@@Base+0x74>  // b.pmore
   16df4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   16df8:	ldr	x9, [x9, #3992]
   16dfc:	add	x9, x8, x9
   16e00:	ldurb	w20, [x9, #-1]
   16e04:	b	16e78 <__gmpz_2fac_ui@@Base+0xa8>
   16e08:	cmp	x1, #0x21
   16e0c:	b.hi	16e9c <__gmpz_2fac_ui@@Base+0xcc>  // b.pmore
   16e10:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   16e14:	ldr	w9, [x19]
   16e18:	ldr	x10, [x10, #3928]
   16e1c:	lsl	x8, x1, #2
   16e20:	and	x8, x8, #0xfffffffffffffff8
   16e24:	cmp	w9, #0x0
   16e28:	ldr	x20, [x10, x8]
   16e2c:	b.le	16f60 <__gmpz_2fac_ui@@Base+0x190>
   16e30:	ldr	x0, [x19, #8]
   16e34:	mov	w8, #0x1                   	// #1
   16e38:	str	x20, [x0]
   16e3c:	str	w8, [x19, #4]
   16e40:	b	16f50 <__gmpz_2fac_ui@@Base+0x180>
   16e44:	and	x9, x8, #0x5555555555555555
   16e48:	sub	x9, x1, x9
   16e4c:	lsr	x10, x9, #2
   16e50:	and	x10, x10, #0x3333333333333333
   16e54:	and	x9, x9, #0x3333333333333333
   16e58:	add	x9, x10, x9
   16e5c:	add	x9, x9, x9, lsr #4
   16e60:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   16e64:	add	x9, x9, x9, lsr #8
   16e68:	add	x9, x9, x9, lsr #16
   16e6c:	lsr	x10, x9, #32
   16e70:	add	w9, w10, w9
   16e74:	sub	x20, x1, w9, uxtb
   16e78:	mov	x0, x19
   16e7c:	mov	x1, x8
   16e80:	mov	w2, wzr
   16e84:	bl	c530 <__gmpz_oddfac_1@plt>
   16e88:	mov	x0, x19
   16e8c:	mov	x1, x19
   16e90:	mov	x2, x20
   16e94:	bl	c870 <__gmpz_mul_2exp@plt>
   16e98:	b	16f50 <__gmpz_2fac_ui@@Base+0x180>
   16e9c:	cmp	x1, #0x1d7
   16ea0:	b.ls	16eb4 <__gmpz_2fac_ui@@Base+0xe4>  // b.plast
   16ea4:	mov	w2, #0x1                   	// #1
   16ea8:	mov	x0, x19
   16eac:	bl	c530 <__gmpz_oddfac_1@plt>
   16eb0:	b	16f50 <__gmpz_2fac_ui@@Base+0x180>
   16eb4:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   16eb8:	movk	x8, #0xaaab
   16ebc:	umulh	x8, x1, x8
   16ec0:	and	x8, x8, #0xfffffffffffffff8
   16ec4:	add	x8, x8, #0x17
   16ec8:	and	x8, x8, #0xfffffffffffffff0
   16ecc:	mov	x9, sp
   16ed0:	sub	x8, x9, x8
   16ed4:	mov	sp, x8
   16ed8:	mov	x9, #0xd941                	// #55617
   16edc:	movk	x9, #0xc030, lsl #16
   16ee0:	movk	x9, #0x2099, lsl #32
   16ee4:	movk	x9, #0x57e2, lsl #48
   16ee8:	sub	x10, x1, #0x2
   16eec:	cmp	x10, #0x22
   16ef0:	str	x9, [x8]
   16ef4:	mov	w9, #0x1                   	// #1
   16ef8:	b.cc	16f3c <__gmpz_2fac_ui@@Base+0x16c>  // b.lo, b.ul, b.last
   16efc:	mov	x11, #0x3869                	// #14441
   16f00:	movk	x11, #0xfba9, lsl #16
   16f04:	movk	x11, #0xd8f2, lsl #32
   16f08:	movk	x11, #0x8a, lsl #48
   16f0c:	b	16f20 <__gmpz_2fac_ui@@Base+0x150>
   16f10:	mul	x1, x10, x1
   16f14:	sub	x10, x10, #0x2
   16f18:	cmp	x10, #0x21
   16f1c:	b.ls	16f3c <__gmpz_2fac_ui@@Base+0x16c>  // b.plast
   16f20:	cmp	x1, x11
   16f24:	b.cc	16f10 <__gmpz_2fac_ui@@Base+0x140>  // b.lo, b.ul, b.last
   16f28:	add	x12, x9, #0x1
   16f2c:	str	x1, [x8, x9, lsl #3]
   16f30:	mov	x1, x10
   16f34:	mov	x9, x12
   16f38:	b	16f14 <__gmpz_2fac_ui@@Base+0x144>
   16f3c:	add	x2, x9, #0x1
   16f40:	str	x1, [x8, x9, lsl #3]
   16f44:	mov	x0, x19
   16f48:	mov	x1, x8
   16f4c:	bl	cf40 <__gmpz_prodlimbs@plt>
   16f50:	mov	sp, x29
   16f54:	ldp	x20, x19, [sp, #16]
   16f58:	ldp	x29, x30, [sp], #32
   16f5c:	ret
   16f60:	mov	w1, #0x1                   	// #1
   16f64:	mov	x0, x19
   16f68:	bl	c1c0 <__gmpz_realloc@plt>
   16f6c:	b	16e34 <__gmpz_2fac_ui@@Base+0x64>

0000000000016f70 <__gmpz_fac_ui@@Base>:
   16f70:	stp	x29, x30, [sp, #-32]!
   16f74:	stp	x20, x19, [sp, #16]
   16f78:	mov	x20, x1
   16f7c:	cmp	x1, #0x14
   16f80:	mov	x19, x0
   16f84:	mov	x29, sp
   16f88:	b.hi	16fb8 <__gmpz_fac_ui@@Base+0x48>  // b.pmore
   16f8c:	adrp	x9, 4c000 <__gmp_randclear_mt@@Base+0x18>
   16f90:	ldr	w8, [x19]
   16f94:	add	x9, x9, #0xb68
   16f98:	ldr	x20, [x9, x20, lsl #3]
   16f9c:	cmp	w8, #0x0
   16fa0:	b.le	170dc <__gmpz_fac_ui@@Base+0x16c>
   16fa4:	ldr	x0, [x19, #8]
   16fa8:	mov	w8, #0x1                   	// #1
   16fac:	str	x20, [x0]
   16fb0:	str	w8, [x19, #4]
   16fb4:	b	170cc <__gmpz_fac_ui@@Base+0x15c>
   16fb8:	cmp	x20, #0x17
   16fbc:	b.ls	16ff0 <__gmpz_fac_ui@@Base+0x80>  // b.plast
   16fc0:	mov	x0, x19
   16fc4:	mov	x1, x20
   16fc8:	mov	w2, wzr
   16fcc:	bl	c530 <__gmpz_oddfac_1@plt>
   16fd0:	cmp	x20, #0x51
   16fd4:	lsr	x8, x20, #1
   16fd8:	b.hi	1708c <__gmpz_fac_ui@@Base+0x11c>  // b.pmore
   16fdc:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   16fe0:	ldr	x9, [x9, #3992]
   16fe4:	add	x8, x8, x9
   16fe8:	ldurb	w2, [x8, #-1]
   16fec:	b	170c0 <__gmpz_fac_ui@@Base+0x150>
   16ff0:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   16ff4:	sub	x8, x20, #0x15
   16ff8:	movk	x9, #0xcccd
   16ffc:	umulh	x8, x8, x9
   17000:	and	x8, x8, #0xfffffffffffffff8
   17004:	add	x8, x8, #0x1f
   17008:	and	x8, x8, #0xfffffffffffffff0
   1700c:	mov	x9, sp
   17010:	sub	x1, x9, x8
   17014:	mov	sp, x1
   17018:	mov	x8, #0x82b40000            	// #2192834560
   1701c:	movk	x8, #0x677c, lsl #32
   17020:	movk	x8, #0x21c3, lsl #48
   17024:	sub	x9, x20, #0x1
   17028:	cmp	x9, #0x15
   1702c:	str	x8, [x1]
   17030:	mov	w8, #0x1                   	// #1
   17034:	b.cc	17078 <__gmpz_fac_ui@@Base+0x108>  // b.lo, b.ul, b.last
   17038:	mov	x10, #0x3d71                	// #15729
   1703c:	movk	x10, #0xd70a, lsl #16
   17040:	movk	x10, #0x70a3, lsl #32
   17044:	movk	x10, #0xa3d, lsl #48
   17048:	b	1705c <__gmpz_fac_ui@@Base+0xec>
   1704c:	mul	x20, x9, x20
   17050:	sub	x9, x9, #0x1
   17054:	cmp	x9, #0x14
   17058:	b.ls	17078 <__gmpz_fac_ui@@Base+0x108>  // b.plast
   1705c:	cmp	x20, x10
   17060:	b.cc	1704c <__gmpz_fac_ui@@Base+0xdc>  // b.lo, b.ul, b.last
   17064:	add	x11, x8, #0x1
   17068:	str	x20, [x1, x8, lsl #3]
   1706c:	mov	x8, x11
   17070:	mov	x20, x9
   17074:	b	17050 <__gmpz_fac_ui@@Base+0xe0>
   17078:	add	x2, x8, #0x1
   1707c:	mov	x0, x19
   17080:	str	x20, [x1, x8, lsl #3]
   17084:	bl	cf40 <__gmpz_prodlimbs@plt>
   17088:	b	170cc <__gmpz_fac_ui@@Base+0x15c>
   1708c:	and	x8, x8, #0x5555555555555555
   17090:	sub	x8, x20, x8
   17094:	lsr	x9, x8, #2
   17098:	and	x9, x9, #0x3333333333333333
   1709c:	and	x8, x8, #0x3333333333333333
   170a0:	add	x8, x9, x8
   170a4:	add	x8, x8, x8, lsr #4
   170a8:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   170ac:	add	x8, x8, x8, lsr #8
   170b0:	add	x8, x8, x8, lsr #16
   170b4:	lsr	x9, x8, #32
   170b8:	add	w8, w9, w8
   170bc:	sub	x2, x20, w8, uxtb
   170c0:	mov	x0, x19
   170c4:	mov	x1, x19
   170c8:	bl	c870 <__gmpz_mul_2exp@plt>
   170cc:	mov	sp, x29
   170d0:	ldp	x20, x19, [sp, #16]
   170d4:	ldp	x29, x30, [sp], #32
   170d8:	ret
   170dc:	mov	w1, #0x1                   	// #1
   170e0:	mov	x0, x19
   170e4:	bl	c1c0 <__gmpz_realloc@plt>
   170e8:	b	16fa8 <__gmpz_fac_ui@@Base+0x38>

00000000000170ec <__gmpz_oddfac_1@@Base>:
   170ec:	stp	x29, x30, [sp, #-96]!
   170f0:	stp	x28, x27, [sp, #16]
   170f4:	stp	x26, x25, [sp, #32]
   170f8:	stp	x24, x23, [sp, #48]
   170fc:	stp	x22, x21, [sp, #64]
   17100:	stp	x20, x19, [sp, #80]
   17104:	mov	x29, sp
   17108:	sub	sp, sp, #0x30
   1710c:	mov	x20, x1
   17110:	cmp	x1, #0x19
   17114:	mov	x19, x0
   17118:	stur	w2, [x29, #-36]
   1711c:	b.hi	1714c <__gmpz_oddfac_1@@Base+0x60>  // b.pmore
   17120:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17124:	ldr	w8, [x19]
   17128:	ldr	x9, [x9, #3848]
   1712c:	cmp	w8, #0x0
   17130:	ldr	x20, [x9, x20, lsl #3]
   17134:	b.le	174c8 <__gmpz_oddfac_1@@Base+0x3dc>
   17138:	ldr	x0, [x19, #8]
   1713c:	mov	w8, #0x1                   	// #1
   17140:	str	x20, [x0]
   17144:	str	w8, [x19, #4]
   17148:	b	174a8 <__gmpz_oddfac_1@@Base+0x3bc>
   1714c:	cmp	x20, #0x23
   17150:	b.cs	171a4 <__gmpz_oddfac_1@@Base+0xb8>  // b.hs, b.nlast
   17154:	ldr	w8, [x19]
   17158:	cmp	w8, #0x1
   1715c:	b.le	174d8 <__gmpz_oddfac_1@@Base+0x3ec>
   17160:	ldr	x0, [x19, #8]
   17164:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17168:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1716c:	ldr	x9, [x9, #3928]
   17170:	ldr	x10, [x10, #3848]
   17174:	lsl	x8, x20, #2
   17178:	sub	x11, x8, #0x4
   1717c:	and	x8, x8, #0xfffffffffffffff8
   17180:	and	x11, x11, #0xfffffffffffffff8
   17184:	ldr	x8, [x10, x8]
   17188:	ldr	x9, [x9, x11]
   1718c:	mov	w10, #0x2                   	// #2
   17190:	umulh	x11, x9, x8
   17194:	mul	x8, x8, x9
   17198:	stp	x8, x11, [x0]
   1719c:	str	w10, [x19, #4]
   171a0:	b	174a8 <__gmpz_oddfac_1@@Base+0x3bc>
   171a4:	cmp	x20, #0xec
   171a8:	b.cc	171cc <__gmpz_oddfac_1@@Base+0xe0>  // b.lo, b.ul, b.last
   171ac:	mov	w28, wzr
   171b0:	mov	x8, x20
   171b4:	lsr	x13, x8, #1
   171b8:	cmp	x8, #0x1d7
   171bc:	add	w28, w28, #0x1
   171c0:	mov	x8, x13
   171c4:	b.hi	171b4 <__gmpz_oddfac_1@@Base+0xc8>  // b.pmore
   171c8:	b	171d4 <__gmpz_oddfac_1@@Base+0xe8>
   171cc:	mov	w28, wzr
   171d0:	mov	x13, x20
   171d4:	mov	x8, #0x2493                	// #9363
   171d8:	movk	x8, #0x9249, lsl #16
   171dc:	movk	x8, #0x4924, lsl #32
   171e0:	movk	x8, #0x2492, lsl #48
   171e4:	umulh	x8, x13, x8
   171e8:	sub	x9, x13, x8
   171ec:	add	x8, x8, x9, lsr #1
   171f0:	lsl	x8, x8, #1
   171f4:	and	x8, x8, #0xfffffffffffffff8
   171f8:	add	x8, x8, #0x17
   171fc:	and	x8, x8, #0xfffffffffffffff0
   17200:	mov	x9, sp
   17204:	sub	x1, x9, x8
   17208:	mov	sp, x1
   1720c:	mov	x10, #0x70d0                	// #28880
   17210:	mov	x12, #0xd941                	// #55617
   17214:	movk	x10, #0xf752, lsl #16
   17218:	movk	x12, #0xc030, lsl #16
   1721c:	movk	x10, #0xb1e5, lsl #32
   17220:	movk	x12, #0x2099, lsl #32
   17224:	mov	x8, xzr
   17228:	movk	x10, #0x115, lsl #48
   1722c:	mov	w9, #0x1                   	// #1
   17230:	movk	x12, #0x57e2, lsl #48
   17234:	b	17248 <__gmpz_oddfac_1@@Base+0x15c>
   17238:	lsl	x10, x10, #1
   1723c:	cmp	x11, #0x45
   17240:	lsr	x13, x11, #1
   17244:	b.ls	17288 <__gmpz_oddfac_1@@Base+0x19c>  // b.plast
   17248:	mov	x11, x13
   1724c:	str	x12, [x1, x8, lsl #3]
   17250:	add	x8, x8, #0x1
   17254:	mov	w13, #0x23                  	// #35
   17258:	b	1726c <__gmpz_oddfac_1@@Base+0x180>
   1725c:	mul	x9, x9, x13
   17260:	add	x13, x13, #0x2
   17264:	cmp	x13, x11
   17268:	b.hi	17238 <__gmpz_oddfac_1@@Base+0x14c>  // b.pmore
   1726c:	cmp	x9, x10
   17270:	b.ls	1725c <__gmpz_oddfac_1@@Base+0x170>  // b.plast
   17274:	add	x14, x8, #0x1
   17278:	str	x9, [x1, x8, lsl #3]
   1727c:	mov	x8, x14
   17280:	mov	x9, x13
   17284:	b	17260 <__gmpz_oddfac_1@@Base+0x174>
   17288:	lsl	x12, x13, #2
   1728c:	adrp	x13, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17290:	adrp	x14, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17294:	ldr	x13, [x13, #3928]
   17298:	ldr	x14, [x14, #3848]
   1729c:	lsl	x11, x11, #1
   172a0:	sub	x12, x12, #0x4
   172a4:	and	x11, x11, #0xfffffffffffffff8
   172a8:	and	x12, x12, #0xfffffffffffffff8
   172ac:	ldr	x12, [x13, x12]
   172b0:	ldr	x11, [x14, x11]
   172b4:	add	x10, x1, x8, lsl #3
   172b8:	add	x2, x8, #0x3
   172bc:	mov	x0, x19
   172c0:	stp	x9, x12, [x10]
   172c4:	str	x11, [x10, #16]
   172c8:	bl	cf40 <__gmpz_prodlimbs@plt>
   172cc:	cbz	w28, 174a8 <__gmpz_oddfac_1@@Base+0x3bc>
   172d0:	lsr	x8, x20, #6
   172d4:	add	x21, x8, #0x4
   172d8:	cmp	x8, #0xfdc
   172dc:	lsl	x1, x21, #3
   172e0:	stur	xzr, [x29, #-24]
   172e4:	stur	w21, [x29, #-16]
   172e8:	b.hi	174e8 <__gmpz_oddfac_1@@Base+0x3fc>  // b.pmore
   172ec:	add	x9, x1, #0xf
   172f0:	mov	x8, sp
   172f4:	and	x9, x9, #0x7ffffffffffffff0
   172f8:	sub	x0, x8, x9
   172fc:	mov	sp, x0
   17300:	lsl	x8, x21, #2
   17304:	and	x8, x8, #0x1ffffffffffffff8
   17308:	add	x8, x0, x8
   1730c:	add	x22, x8, #0x8
   17310:	stur	x0, [x29, #-8]
   17314:	sub	x1, x20, #0x1
   17318:	mov	x0, x22
   1731c:	bl	d3e0 <__gmp_primesieve@plt>
   17320:	add	x21, x0, #0x1
   17324:	mov	x0, x20
   17328:	bl	1750c <__gmpz_oddfac_1@@Base+0x420>
   1732c:	mov	w8, w0
   17330:	udiv	x8, x21, x8
   17334:	lsl	x8, x8, #3
   17338:	add	x1, x8, #0x8
   1733c:	mov	w8, #0x7f00                	// #32512
   17340:	cmp	x1, x8
   17344:	b.hi	174f4 <__gmpz_oddfac_1@@Base+0x408>  // b.pmore
   17348:	add	x9, x1, #0xf
   1734c:	mov	x8, sp
   17350:	and	x9, x9, #0xfffffffffffffff0
   17354:	sub	x23, x8, x9
   17358:	mov	sp, x23
   1735c:	sub	w21, w28, #0x1
   17360:	lsr	x1, x20, x21
   17364:	sub	x0, x29, #0x10
   17368:	mov	x2, x22
   1736c:	mov	x3, x23
   17370:	bl	17534 <__gmpz_oddfac_1@@Base+0x448>
   17374:	stur	xzr, [x29, #-32]
   17378:	ldur	w8, [x29, #-36]
   1737c:	ldrsw	x24, [x19, #4]
   17380:	cmp	w8, w28
   17384:	b.ne	173c0 <__gmpz_oddfac_1@@Base+0x2d4>  // b.any
   17388:	lsl	x1, x24, #3
   1738c:	mov	w8, #0x7f00                	// #32512
   17390:	cmp	x1, x8
   17394:	b.hi	17480 <__gmpz_oddfac_1@@Base+0x394>  // b.pmore
   17398:	add	x9, x1, #0xf
   1739c:	mov	x8, sp
   173a0:	and	x9, x9, #0xfffffffffffffff0
   173a4:	sub	x25, x8, x9
   173a8:	mov	sp, x25
   173ac:	ldr	x1, [x19, #8]
   173b0:	mov	x0, x25
   173b4:	mov	x2, x24
   173b8:	bl	cc10 <__gmpn_copyi@plt>
   173bc:	b	1740c <__gmpz_oddfac_1@@Base+0x320>
   173c0:	lsl	x1, x24, #4
   173c4:	mov	w8, #0x7f00                	// #32512
   173c8:	cmp	x1, x8
   173cc:	lsl	x26, x24, #1
   173d0:	b.hi	17490 <__gmpz_oddfac_1@@Base+0x3a4>  // b.pmore
   173d4:	add	x9, x1, #0xf
   173d8:	mov	x8, sp
   173dc:	and	x9, x9, #0xfffffffffffffff0
   173e0:	sub	x25, x8, x9
   173e4:	mov	sp, x25
   173e8:	ldr	x1, [x19, #8]
   173ec:	mov	x0, x25
   173f0:	mov	x2, x24
   173f4:	bl	ca90 <__gmpn_sqr@plt>
   173f8:	add	x8, x25, x26, lsl #3
   173fc:	ldur	x8, [x8, #-8]
   17400:	cmp	x8, #0x0
   17404:	cset	w8, eq  // eq = none
   17408:	sub	x24, x26, x8
   1740c:	ldursw	x27, [x29, #-12]
   17410:	ldrsw	x8, [x19]
   17414:	add	x26, x24, x27
   17418:	cmp	x26, x8
   1741c:	b.gt	17460 <__gmpz_oddfac_1@@Base+0x374>
   17420:	ldr	x0, [x19, #8]
   17424:	ldur	x3, [x29, #-8]
   17428:	mov	x1, x25
   1742c:	mov	x2, x24
   17430:	mov	x4, x27
   17434:	sub	w28, w28, #0x1
   17438:	bl	cea0 <__gmpn_mul@plt>
   1743c:	cmp	x0, #0x0
   17440:	cset	w8, eq  // eq = none
   17444:	sub	w8, w26, w8
   17448:	str	w8, [x19, #4]
   1744c:	ldur	x0, [x29, #-32]
   17450:	cbnz	x0, 17470 <__gmpz_oddfac_1@@Base+0x384>
   17454:	sub	x21, x21, #0x1
   17458:	cbnz	w28, 17360 <__gmpz_oddfac_1@@Base+0x274>
   1745c:	b	174a0 <__gmpz_oddfac_1@@Base+0x3b4>
   17460:	mov	x0, x19
   17464:	mov	x1, x26
   17468:	bl	c1c0 <__gmpz_realloc@plt>
   1746c:	b	17424 <__gmpz_oddfac_1@@Base+0x338>
   17470:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   17474:	sub	x21, x21, #0x1
   17478:	cbnz	w28, 17360 <__gmpz_oddfac_1@@Base+0x274>
   1747c:	b	174a0 <__gmpz_oddfac_1@@Base+0x3b4>
   17480:	sub	x0, x29, #0x20
   17484:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17488:	mov	x25, x0
   1748c:	b	173ac <__gmpz_oddfac_1@@Base+0x2c0>
   17490:	sub	x0, x29, #0x20
   17494:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17498:	mov	x25, x0
   1749c:	b	173e8 <__gmpz_oddfac_1@@Base+0x2fc>
   174a0:	ldur	x0, [x29, #-24]
   174a4:	cbnz	x0, 17504 <__gmpz_oddfac_1@@Base+0x418>
   174a8:	mov	sp, x29
   174ac:	ldp	x20, x19, [sp, #80]
   174b0:	ldp	x22, x21, [sp, #64]
   174b4:	ldp	x24, x23, [sp, #48]
   174b8:	ldp	x26, x25, [sp, #32]
   174bc:	ldp	x28, x27, [sp, #16]
   174c0:	ldp	x29, x30, [sp], #96
   174c4:	ret
   174c8:	mov	w1, #0x1                   	// #1
   174cc:	mov	x0, x19
   174d0:	bl	c1c0 <__gmpz_realloc@plt>
   174d4:	b	1713c <__gmpz_oddfac_1@@Base+0x50>
   174d8:	mov	w1, #0x2                   	// #2
   174dc:	mov	x0, x19
   174e0:	bl	c1c0 <__gmpz_realloc@plt>
   174e4:	b	17164 <__gmpz_oddfac_1@@Base+0x78>
   174e8:	sub	x0, x29, #0x18
   174ec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   174f0:	b	17300 <__gmpz_oddfac_1@@Base+0x214>
   174f4:	sub	x0, x29, #0x18
   174f8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   174fc:	mov	x23, x0
   17500:	b	1735c <__gmpz_oddfac_1@@Base+0x270>
   17504:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   17508:	b	174a8 <__gmpz_oddfac_1@@Base+0x3bc>
   1750c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17510:	ldr	x9, [x9, #3880]
   17514:	mov	x8, x0
   17518:	mov	w0, #0x9                   	// #9
   1751c:	sub	w10, w0, #0x2
   17520:	ldr	x10, [x9, w10, uxtw #3]
   17524:	sub	w0, w0, #0x1
   17528:	cmp	x10, x8
   1752c:	b.cc	1751c <__gmpz_oddfac_1@@Base+0x430>  // b.lo, b.ul, b.last
   17530:	ret
   17534:	sub	sp, sp, #0x80
   17538:	and	x8, x1, #0x1
   1753c:	stp	x22, x21, [sp, #96]
   17540:	and	x22, x1, #0xfffffffffffffffe
   17544:	neg	x8, x8
   17548:	mov	x9, #0xffffffffffffffff    	// #-1
   1754c:	sub	x10, x22, #0x1
   17550:	and	x8, x8, x1
   17554:	stp	x28, x27, [sp, #48]
   17558:	stp	x26, x25, [sp, #64]
   1755c:	orr	x27, x8, #0x1
   17560:	udiv	x26, x9, x10
   17564:	mov	x21, x2
   17568:	cmp	x27, x26
   1756c:	stp	x29, x30, [sp, #32]
   17570:	stp	x24, x23, [sp, #80]
   17574:	stp	x20, x19, [sp, #112]
   17578:	add	x29, sp, #0x20
   1757c:	stp	x1, x0, [sp, #8]
   17580:	b.ls	17594 <__gmpz_oddfac_1@@Base+0x4a8>  // b.plast
   17584:	str	x27, [x3]
   17588:	mov	w28, #0x1                   	// #1
   1758c:	mov	w27, #0x1                   	// #1
   17590:	b	17598 <__gmpz_oddfac_1@@Base+0x4ac>
   17594:	mov	x28, xzr
   17598:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1759c:	movk	x8, #0xaaab
   175a0:	mov	x9, x22
   175a4:	stur	x3, [x29, #-8]
   175a8:	umulh	x10, x9, x8
   175ac:	lsr	x10, x10, #1
   175b0:	add	x11, x27, x27, lsl #1
   175b4:	tst	x10, #0x1
   175b8:	csel	x27, x27, x11, eq  // eq = none
   175bc:	cmp	x9, #0x8
   175c0:	mov	x9, x10
   175c4:	b.hi	175a8 <__gmpz_oddfac_1@@Base+0x4bc>  // b.pmore
   175c8:	mov	x0, x22
   175cc:	bl	177a0 <__gmpz_oddfac_1@@Base+0x6b4>
   175d0:	bl	177c8 <__gmpz_oddfac_1@@Base+0x6dc>
   175d4:	mov	x25, x0
   175d8:	mov	w0, #0x5                   	// #5
   175dc:	bl	177c8 <__gmpz_oddfac_1@@Base+0x6dc>
   175e0:	mov	w8, #0x1                   	// #1
   175e4:	mov	x24, x0
   175e8:	lsr	x19, x0, #6
   175ec:	lsl	x20, x8, x0
   175f0:	b	17608 <__gmpz_oddfac_1@@Base+0x51c>
   175f4:	ror	x23, x20, #63
   175f8:	cmp	x24, x25
   175fc:	add	x19, x19, x20, lsr #63
   17600:	mov	x20, x23
   17604:	b.hi	1765c <__gmpz_oddfac_1@@Base+0x570>  // b.pmore
   17608:	ldr	x8, [x21, x19, lsl #3]
   1760c:	add	x24, x24, #0x1
   17610:	tst	x8, x20
   17614:	b.ne	175f4 <__gmpz_oddfac_1@@Base+0x508>  // b.any
   17618:	mov	x0, x24
   1761c:	bl	177e4 <__gmpz_oddfac_1@@Base+0x6f8>
   17620:	cmp	x27, x26
   17624:	b.ls	1763c <__gmpz_oddfac_1@@Base+0x550>  // b.plast
   17628:	ldur	x9, [x29, #-8]
   1762c:	add	x8, x28, #0x1
   17630:	str	x27, [x9, x28, lsl #3]
   17634:	mov	x28, x8
   17638:	mov	w27, #0x1                   	// #1
   1763c:	mov	x8, x22
   17640:	udiv	x8, x8, x0
   17644:	tst	x8, #0x1
   17648:	csinc	x9, x0, xzr, ne  // ne = any
   1764c:	cmp	x8, x0
   17650:	mul	x27, x9, x27
   17654:	b.cs	17640 <__gmpz_oddfac_1@@Base+0x554>  // b.hs, b.nlast
   17658:	b	175f4 <__gmpz_oddfac_1@@Base+0x508>
   1765c:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   17660:	movk	x8, #0xaaab
   17664:	umulh	x8, x22, x8
   17668:	lsr	x0, x8, #1
   1766c:	add	x20, x26, x26, lsl #1
   17670:	bl	177c8 <__gmpz_oddfac_1@@Base+0x6dc>
   17674:	mov	x25, x0
   17678:	b	17694 <__gmpz_oddfac_1@@Base+0x5a8>
   1767c:	mul	x27, x0, x27
   17680:	ror	x8, x23, #63
   17684:	cmp	x24, x25
   17688:	add	x19, x19, x23, lsr #63
   1768c:	mov	x23, x8
   17690:	b.hi	176d4 <__gmpz_oddfac_1@@Base+0x5e8>  // b.pmore
   17694:	ldr	x8, [x21, x19, lsl #3]
   17698:	add	x24, x24, #0x1
   1769c:	tst	x8, x23
   176a0:	b.ne	17680 <__gmpz_oddfac_1@@Base+0x594>  // b.any
   176a4:	mov	x0, x24
   176a8:	bl	177e4 <__gmpz_oddfac_1@@Base+0x6f8>
   176ac:	udiv	x8, x22, x0
   176b0:	tbz	w8, #0, 17680 <__gmpz_oddfac_1@@Base+0x594>
   176b4:	cmp	x27, x20
   176b8:	b.ls	1767c <__gmpz_oddfac_1@@Base+0x590>  // b.plast
   176bc:	ldur	x9, [x29, #-8]
   176c0:	add	x8, x28, #0x1
   176c4:	str	x27, [x9, x28, lsl #3]
   176c8:	mov	x28, x8
   176cc:	mov	x27, x0
   176d0:	b	17680 <__gmpz_oddfac_1@@Base+0x594>
   176d4:	ldr	x8, [sp, #8]
   176d8:	lsr	x0, x8, #1
   176dc:	bl	177c8 <__gmpz_oddfac_1@@Base+0x6dc>
   176e0:	add	x23, x0, #0x1
   176e4:	mov	w8, #0x1                   	// #1
   176e8:	mov	x0, x22
   176ec:	lsr	x19, x23, #6
   176f0:	lsl	x20, x8, x23
   176f4:	bl	177c8 <__gmpz_oddfac_1@@Base+0x6dc>
   176f8:	ldur	x24, [x29, #-8]
   176fc:	mov	x22, x0
   17700:	b	1771c <__gmpz_oddfac_1@@Base+0x630>
   17704:	mul	x27, x0, x27
   17708:	ror	x8, x20, #63
   1770c:	cmp	x23, x22
   17710:	add	x19, x19, x20, lsr #63
   17714:	mov	x20, x8
   17718:	b.hi	17750 <__gmpz_oddfac_1@@Base+0x664>  // b.pmore
   1771c:	ldr	x8, [x21, x19, lsl #3]
   17720:	add	x23, x23, #0x1
   17724:	tst	x8, x20
   17728:	b.ne	17708 <__gmpz_oddfac_1@@Base+0x61c>  // b.any
   1772c:	mov	x0, x23
   17730:	bl	177e4 <__gmpz_oddfac_1@@Base+0x6f8>
   17734:	cmp	x27, x26
   17738:	b.ls	17704 <__gmpz_oddfac_1@@Base+0x618>  // b.plast
   1773c:	add	x8, x28, #0x1
   17740:	str	x27, [x24, x28, lsl #3]
   17744:	mov	x28, x8
   17748:	mov	x27, x0
   1774c:	b	17708 <__gmpz_oddfac_1@@Base+0x61c>
   17750:	cbz	x28, 17788 <__gmpz_oddfac_1@@Base+0x69c>
   17754:	ldr	x0, [sp, #16]
   17758:	add	x2, x28, #0x1
   1775c:	mov	x1, x24
   17760:	str	x27, [x24, x28, lsl #3]
   17764:	bl	cf40 <__gmpz_prodlimbs@plt>
   17768:	ldp	x20, x19, [sp, #112]
   1776c:	ldp	x22, x21, [sp, #96]
   17770:	ldp	x24, x23, [sp, #80]
   17774:	ldp	x26, x25, [sp, #64]
   17778:	ldp	x28, x27, [sp, #48]
   1777c:	ldp	x29, x30, [sp, #32]
   17780:	add	sp, sp, #0x80
   17784:	ret
   17788:	ldr	x10, [sp, #16]
   1778c:	mov	w9, #0x1                   	// #1
   17790:	ldr	x8, [x10, #8]
   17794:	str	x27, [x8]
   17798:	str	w9, [x10, #4]
   1779c:	b	17768 <__gmpz_oddfac_1@@Base+0x67c>
   177a0:	clz	x8, x0
   177a4:	mov	w9, #0x40                  	// #64
   177a8:	sub	w8, w9, w8
   177ac:	mov	w10, #0x1                   	// #1
   177b0:	asr	w8, w8, #1
   177b4:	lsl	x9, x10, x8
   177b8:	lsr	x8, x0, x8
   177bc:	add	x8, x9, x8
   177c0:	lsr	x0, x8, #1
   177c4:	ret
   177c8:	sub	x8, x0, #0x5
   177cc:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   177d0:	orr	x8, x8, #0x1
   177d4:	movk	x9, #0xaaab
   177d8:	umulh	x8, x8, x9
   177dc:	lsr	x0, x8, #1
   177e0:	ret
   177e4:	add	x8, x0, x0, lsl #1
   177e8:	and	x9, x0, #0x1
   177ec:	add	x8, x8, x9
   177f0:	add	x0, x8, #0x1
   177f4:	ret

00000000000177f8 <__gmpz_prodlimbs@@Base>:
   177f8:	stp	x29, x30, [sp, #-64]!
   177fc:	str	x23, [sp, #16]
   17800:	stp	x22, x21, [sp, #32]
   17804:	stp	x20, x19, [sp, #48]
   17808:	mov	x29, sp
   1780c:	sub	sp, sp, #0x20
   17810:	mov	x20, x1
   17814:	cmp	x2, #0xd
   17818:	mov	x19, x0
   1781c:	b.le	178b8 <__gmpz_prodlimbs@@Base+0xc0>
   17820:	asr	x21, x2, #1
   17824:	sub	x22, x2, x21
   17828:	lsl	x1, x22, #3
   1782c:	mov	w8, #0x7f00                	// #32512
   17830:	cmp	x1, x8
   17834:	str	xzr, [x29, #24]
   17838:	stur	w22, [x29, #-32]
   1783c:	b.hi	17940 <__gmpz_prodlimbs@@Base+0x148>  // b.pmore
   17840:	add	x9, x1, #0xf
   17844:	mov	x8, sp
   17848:	and	x9, x9, #0xfffffffffffffff0
   1784c:	sub	x0, x8, x9
   17850:	mov	sp, x0
   17854:	stur	x0, [x29, #-24]
   17858:	add	x1, x20, x21, lsl #3
   1785c:	sub	x0, x29, #0x20
   17860:	mov	x2, x22
   17864:	stur	x1, [x29, #-8]
   17868:	stur	w22, [x29, #-16]
   1786c:	bl	cf40 <__gmpz_prodlimbs@plt>
   17870:	mov	x22, x0
   17874:	sub	x0, x29, #0x10
   17878:	mov	x1, x20
   1787c:	mov	x2, x21
   17880:	bl	cf40 <__gmpz_prodlimbs@plt>
   17884:	ldrsw	x8, [x19]
   17888:	add	x20, x0, x22
   1788c:	mov	x21, x0
   17890:	cmp	x20, x8
   17894:	b.gt	1794c <__gmpz_prodlimbs@@Base+0x154>
   17898:	ldr	x0, [x19, #8]
   1789c:	cmp	x21, x22
   178a0:	b.ge	17960 <__gmpz_prodlimbs@@Base+0x168>  // b.tcont
   178a4:	ldur	x1, [x29, #-24]
   178a8:	ldur	x3, [x29, #-8]
   178ac:	mov	x2, x22
   178b0:	mov	x4, x21
   178b4:	b	17970 <__gmpz_prodlimbs@@Base+0x178>
   178b8:	cmp	x2, #0x3
   178bc:	b.lt	17904 <__gmpz_prodlimbs@@Base+0x10c>  // b.tstop
   178c0:	mov	w8, #0x2                   	// #2
   178c4:	mov	w23, #0x1                   	// #1
   178c8:	sub	x22, x8, x2
   178cc:	mov	w21, #0x1                   	// #1
   178d0:	ldr	x3, [x20, x23, lsl #3]
   178d4:	mov	x0, x20
   178d8:	mov	x1, x20
   178dc:	mov	x2, x21
   178e0:	bl	d670 <__gmpn_mul_1@plt>
   178e4:	add	x23, x23, #0x1
   178e8:	cmp	x0, #0x0
   178ec:	add	x8, x22, x23
   178f0:	str	x0, [x20, x21, lsl #3]
   178f4:	cinc	x21, x21, ne  // ne = any
   178f8:	cmp	x8, #0x1
   178fc:	b.ne	178d0 <__gmpz_prodlimbs@@Base+0xd8>  // b.any
   17900:	b	1790c <__gmpz_prodlimbs@@Base+0x114>
   17904:	mov	w21, #0x1                   	// #1
   17908:	mov	w23, #0x1                   	// #1
   1790c:	ldrsw	x8, [x19]
   17910:	cmp	x21, x8
   17914:	b.ge	179ac <__gmpz_prodlimbs@@Base+0x1b4>  // b.tcont
   17918:	ldr	x22, [x19, #8]
   1791c:	ldr	x3, [x20, x23, lsl #3]
   17920:	mov	x0, x22
   17924:	mov	x1, x20
   17928:	mov	x2, x21
   1792c:	bl	d670 <__gmpn_mul_1@plt>
   17930:	cmp	x0, #0x0
   17934:	str	x0, [x22, x21, lsl #3]
   17938:	cinc	x8, x21, ne  // ne = any
   1793c:	b	1798c <__gmpz_prodlimbs@@Base+0x194>
   17940:	add	x0, x29, #0x18
   17944:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17948:	b	17854 <__gmpz_prodlimbs@@Base+0x5c>
   1794c:	mov	x0, x19
   17950:	mov	x1, x20
   17954:	bl	c1c0 <__gmpz_realloc@plt>
   17958:	cmp	x21, x22
   1795c:	b.lt	178a4 <__gmpz_prodlimbs@@Base+0xac>  // b.tstop
   17960:	ldur	x1, [x29, #-8]
   17964:	ldur	x3, [x29, #-24]
   17968:	mov	x2, x21
   1796c:	mov	x4, x22
   17970:	bl	cea0 <__gmpn_mul@plt>
   17974:	mov	x21, x0
   17978:	ldr	x0, [x29, #24]
   1797c:	cbnz	x0, 179c0 <__gmpz_prodlimbs@@Base+0x1c8>
   17980:	cmp	x21, #0x0
   17984:	cset	w8, eq  // eq = none
   17988:	sub	x8, x20, x8
   1798c:	str	w8, [x19, #4]
   17990:	sxtw	x0, w8
   17994:	mov	sp, x29
   17998:	ldp	x20, x19, [sp, #48]
   1799c:	ldp	x22, x21, [sp, #32]
   179a0:	ldr	x23, [sp, #16]
   179a4:	ldp	x29, x30, [sp], #64
   179a8:	ret
   179ac:	add	x1, x21, #0x1
   179b0:	mov	x0, x19
   179b4:	bl	c1c0 <__gmpz_realloc@plt>
   179b8:	mov	x22, x0
   179bc:	b	1791c <__gmpz_prodlimbs@@Base+0x124>
   179c0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   179c4:	b	17980 <__gmpz_prodlimbs@@Base+0x188>

00000000000179c8 <__gmpz_fdiv_q_ui@@Base>:
   179c8:	stp	x29, x30, [sp, #-64]!
   179cc:	stp	x24, x23, [sp, #16]
   179d0:	stp	x22, x21, [sp, #32]
   179d4:	stp	x20, x19, [sp, #48]
   179d8:	mov	x29, sp
   179dc:	cbz	x2, 17a98 <__gmpz_fdiv_q_ui@@Base+0xd0>
   179e0:	ldrsw	x24, [x1, #4]
   179e4:	mov	x23, x1
   179e8:	mov	x19, x0
   179ec:	cbz	w24, 17a64 <__gmpz_fdiv_q_ui@@Base+0x9c>
   179f0:	ldrsw	x8, [x19]
   179f4:	cmp	w24, #0x0
   179f8:	cneg	x21, x24, lt  // lt = tstop
   179fc:	mov	x20, x2
   17a00:	cmp	x21, x8
   17a04:	b.gt	17a84 <__gmpz_fdiv_q_ui@@Base+0xbc>
   17a08:	ldr	x22, [x19, #8]
   17a0c:	ldr	x2, [x23, #8]
   17a10:	mov	x0, x22
   17a14:	mov	x1, xzr
   17a18:	mov	x3, x21
   17a1c:	mov	x4, x20
   17a20:	bl	ced0 <__gmpn_divrem_1@plt>
   17a24:	tbz	w24, #31, 17a44 <__gmpz_fdiv_q_ui@@Base+0x7c>
   17a28:	cbz	x0, 17a44 <__gmpz_fdiv_q_ui@@Base+0x7c>
   17a2c:	mov	x8, x22
   17a30:	ldr	x9, [x8]
   17a34:	adds	x9, x9, #0x1
   17a38:	str	x9, [x8], #8
   17a3c:	b.cs	17a30 <__gmpz_fdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   17a40:	sub	x0, x20, x0
   17a44:	add	x8, x22, x21, lsl #3
   17a48:	ldur	x8, [x8, #-8]
   17a4c:	cmp	x8, #0x0
   17a50:	cset	w8, eq  // eq = none
   17a54:	sub	w8, w21, w8
   17a58:	cmp	w24, #0x0
   17a5c:	cneg	w8, w8, lt  // lt = tstop
   17a60:	b	17a6c <__gmpz_fdiv_q_ui@@Base+0xa4>
   17a64:	mov	w8, wzr
   17a68:	mov	x0, xzr
   17a6c:	str	w8, [x19, #4]
   17a70:	ldp	x20, x19, [sp, #48]
   17a74:	ldp	x22, x21, [sp, #32]
   17a78:	ldp	x24, x23, [sp, #16]
   17a7c:	ldp	x29, x30, [sp], #64
   17a80:	ret
   17a84:	mov	x0, x19
   17a88:	mov	x1, x21
   17a8c:	bl	c1c0 <__gmpz_realloc@plt>
   17a90:	mov	x22, x0
   17a94:	b	17a0c <__gmpz_fdiv_q_ui@@Base+0x44>
   17a98:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000017a9c <__gmpz_fdiv_qr@@Base>:
   17a9c:	stp	x29, x30, [sp, #-64]!
   17aa0:	str	x23, [sp, #16]
   17aa4:	stp	x22, x21, [sp, #32]
   17aa8:	stp	x20, x19, [sp, #48]
   17aac:	mov	x29, sp
   17ab0:	sub	sp, sp, #0x10
   17ab4:	ldrsw	x23, [x3, #4]
   17ab8:	mov	x20, x3
   17abc:	mov	x22, x2
   17ac0:	mov	x19, x1
   17ac4:	mov	x21, x0
   17ac8:	cmp	x0, x3
   17acc:	str	xzr, [x29, #24]
   17ad0:	b.eq	17adc <__gmpz_fdiv_qr@@Base+0x40>  // b.none
   17ad4:	cmp	x19, x20
   17ad8:	b.ne	17b20 <__gmpz_fdiv_qr@@Base+0x84>  // b.any
   17adc:	cmp	x23, #0x0
   17ae0:	cneg	x8, x23, mi  // mi = first
   17ae4:	lsl	x1, x8, #3
   17ae8:	mov	w9, #0x7f00                	// #32512
   17aec:	cmp	x1, x9
   17af0:	stur	w8, [x29, #-16]
   17af4:	b.hi	17b90 <__gmpz_fdiv_qr@@Base+0xf4>  // b.pmore
   17af8:	add	x9, x1, #0xf
   17afc:	mov	x8, sp
   17b00:	and	x9, x9, #0xfffffffffffffff0
   17b04:	sub	x0, x8, x9
   17b08:	mov	sp, x0
   17b0c:	stur	x0, [x29, #-8]
   17b10:	sub	x0, x29, #0x10
   17b14:	mov	x1, x20
   17b18:	bl	c590 <__gmpz_set@plt>
   17b1c:	sub	x20, x29, #0x10
   17b20:	ldr	w8, [x22, #4]
   17b24:	mov	x0, x21
   17b28:	mov	x1, x19
   17b2c:	mov	x2, x22
   17b30:	mov	x3, x20
   17b34:	eor	w23, w8, w23
   17b38:	bl	c120 <__gmpz_tdiv_qr@plt>
   17b3c:	tbz	w23, #31, 17b68 <__gmpz_fdiv_qr@@Base+0xcc>
   17b40:	ldr	w8, [x19, #4]
   17b44:	cbz	w8, 17b68 <__gmpz_fdiv_qr@@Base+0xcc>
   17b48:	mov	w2, #0x1                   	// #1
   17b4c:	mov	x0, x21
   17b50:	mov	x1, x21
   17b54:	bl	c270 <__gmpz_sub_ui@plt>
   17b58:	mov	x0, x19
   17b5c:	mov	x1, x19
   17b60:	mov	x2, x20
   17b64:	bl	d160 <__gmpz_add@plt>
   17b68:	ldr	x0, [x29, #24]
   17b6c:	cbnz	x0, 17b88 <__gmpz_fdiv_qr@@Base+0xec>
   17b70:	mov	sp, x29
   17b74:	ldp	x20, x19, [sp, #48]
   17b78:	ldp	x22, x21, [sp, #32]
   17b7c:	ldr	x23, [sp, #16]
   17b80:	ldp	x29, x30, [sp], #64
   17b84:	ret
   17b88:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   17b8c:	b	17b70 <__gmpz_fdiv_qr@@Base+0xd4>
   17b90:	add	x0, x29, #0x18
   17b94:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17b98:	b	17b0c <__gmpz_fdiv_qr@@Base+0x70>

0000000000017b9c <__gmpz_fdiv_qr_ui@@Base>:
   17b9c:	stp	x29, x30, [sp, #-80]!
   17ba0:	str	x25, [sp, #16]
   17ba4:	stp	x24, x23, [sp, #32]
   17ba8:	stp	x22, x21, [sp, #48]
   17bac:	stp	x20, x19, [sp, #64]
   17bb0:	mov	x29, sp
   17bb4:	cbz	x3, 17cc0 <__gmpz_fdiv_qr_ui@@Base+0x124>
   17bb8:	ldrsw	x25, [x2, #4]
   17bbc:	mov	x22, x2
   17bc0:	mov	x20, x1
   17bc4:	mov	x19, x0
   17bc8:	cbz	w25, 17c44 <__gmpz_fdiv_qr_ui@@Base+0xa8>
   17bcc:	ldrsw	x8, [x19]
   17bd0:	cmp	w25, #0x0
   17bd4:	cneg	x21, x25, lt  // lt = tstop
   17bd8:	mov	x24, x3
   17bdc:	cmp	x21, x8
   17be0:	b.gt	17c9c <__gmpz_fdiv_qr_ui@@Base+0x100>
   17be4:	ldr	x23, [x19, #8]
   17be8:	ldr	x2, [x22, #8]
   17bec:	mov	x0, x23
   17bf0:	mov	x1, xzr
   17bf4:	mov	x3, x21
   17bf8:	mov	x4, x24
   17bfc:	bl	ced0 <__gmpn_divrem_1@plt>
   17c00:	mov	x22, x0
   17c04:	cbz	x0, 17c58 <__gmpz_fdiv_qr_ui@@Base+0xbc>
   17c08:	tbz	w25, #31, 17c24 <__gmpz_fdiv_qr_ui@@Base+0x88>
   17c0c:	mov	x8, x23
   17c10:	ldr	x9, [x8]
   17c14:	adds	x9, x9, #0x1
   17c18:	str	x9, [x8], #8
   17c1c:	b.cs	17c10 <__gmpz_fdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   17c20:	sub	x22, x24, x22
   17c24:	ldr	w8, [x20]
   17c28:	cmp	w8, #0x0
   17c2c:	b.le	17cb0 <__gmpz_fdiv_qr_ui@@Base+0x114>
   17c30:	ldr	x0, [x20, #8]
   17c34:	cmp	x22, #0x0
   17c38:	cset	w8, ne  // ne = any
   17c3c:	str	x22, [x0]
   17c40:	b	17c5c <__gmpz_fdiv_qr_ui@@Base+0xc0>
   17c44:	mov	w8, wzr
   17c48:	mov	x22, xzr
   17c4c:	str	wzr, [x19, #4]
   17c50:	mov	x19, x20
   17c54:	b	17c7c <__gmpz_fdiv_qr_ui@@Base+0xe0>
   17c58:	mov	w8, wzr
   17c5c:	str	w8, [x20, #4]
   17c60:	add	x8, x23, x21, lsl #3
   17c64:	ldur	x8, [x8, #-8]
   17c68:	cmp	x8, #0x0
   17c6c:	cset	w8, eq  // eq = none
   17c70:	sub	w8, w21, w8
   17c74:	cmp	w25, #0x0
   17c78:	cneg	w8, w8, lt  // lt = tstop
   17c7c:	str	w8, [x19, #4]
   17c80:	mov	x0, x22
   17c84:	ldp	x20, x19, [sp, #64]
   17c88:	ldp	x22, x21, [sp, #48]
   17c8c:	ldp	x24, x23, [sp, #32]
   17c90:	ldr	x25, [sp, #16]
   17c94:	ldp	x29, x30, [sp], #80
   17c98:	ret
   17c9c:	mov	x0, x19
   17ca0:	mov	x1, x21
   17ca4:	bl	c1c0 <__gmpz_realloc@plt>
   17ca8:	mov	x23, x0
   17cac:	b	17be8 <__gmpz_fdiv_qr_ui@@Base+0x4c>
   17cb0:	mov	w1, #0x1                   	// #1
   17cb4:	mov	x0, x20
   17cb8:	bl	c1c0 <__gmpz_realloc@plt>
   17cbc:	b	17c34 <__gmpz_fdiv_qr_ui@@Base+0x98>
   17cc0:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000017cc4 <__gmpz_fdiv_r@@Base>:
   17cc4:	stp	x29, x30, [sp, #-48]!
   17cc8:	stp	x22, x21, [sp, #16]
   17ccc:	stp	x20, x19, [sp, #32]
   17cd0:	mov	x29, sp
   17cd4:	sub	sp, sp, #0x20
   17cd8:	ldrsw	x22, [x2, #4]
   17cdc:	mov	x19, x2
   17ce0:	mov	x21, x1
   17ce4:	mov	x20, x0
   17ce8:	cmp	x0, x2
   17cec:	stur	xzr, [x29, #-24]
   17cf0:	b.ne	17d38 <__gmpz_fdiv_r@@Base+0x74>  // b.any
   17cf4:	cmp	x22, #0x0
   17cf8:	cneg	x8, x22, mi  // mi = first
   17cfc:	lsl	x1, x8, #3
   17d00:	mov	w9, #0x7f00                	// #32512
   17d04:	cmp	x1, x9
   17d08:	stur	w8, [x29, #-16]
   17d0c:	b.hi	17d90 <__gmpz_fdiv_r@@Base+0xcc>  // b.pmore
   17d10:	add	x9, x1, #0xf
   17d14:	mov	x8, sp
   17d18:	and	x9, x9, #0xfffffffffffffff0
   17d1c:	sub	x0, x8, x9
   17d20:	mov	sp, x0
   17d24:	stur	x0, [x29, #-8]
   17d28:	sub	x0, x29, #0x10
   17d2c:	mov	x1, x19
   17d30:	bl	c590 <__gmpz_set@plt>
   17d34:	sub	x19, x29, #0x10
   17d38:	mov	x0, x20
   17d3c:	mov	x1, x21
   17d40:	mov	x2, x19
   17d44:	bl	cc40 <__gmpz_tdiv_r@plt>
   17d48:	ldr	w8, [x21, #4]
   17d4c:	eor	w8, w8, w22
   17d50:	tbz	w8, #31, 17d6c <__gmpz_fdiv_r@@Base+0xa8>
   17d54:	ldr	w8, [x20, #4]
   17d58:	cbz	w8, 17d6c <__gmpz_fdiv_r@@Base+0xa8>
   17d5c:	mov	x0, x20
   17d60:	mov	x1, x20
   17d64:	mov	x2, x19
   17d68:	bl	d160 <__gmpz_add@plt>
   17d6c:	ldur	x0, [x29, #-24]
   17d70:	cbnz	x0, 17d88 <__gmpz_fdiv_r@@Base+0xc4>
   17d74:	mov	sp, x29
   17d78:	ldp	x20, x19, [sp, #32]
   17d7c:	ldp	x22, x21, [sp, #16]
   17d80:	ldp	x29, x30, [sp], #48
   17d84:	ret
   17d88:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   17d8c:	b	17d74 <__gmpz_fdiv_r@@Base+0xb0>
   17d90:	sub	x0, x29, #0x18
   17d94:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17d98:	b	17d24 <__gmpz_fdiv_r@@Base+0x60>

0000000000017d9c <__gmpz_fdiv_r_ui@@Base>:
   17d9c:	stp	x29, x30, [sp, #-48]!
   17da0:	str	x21, [sp, #16]
   17da4:	stp	x20, x19, [sp, #32]
   17da8:	mov	x29, sp
   17dac:	cbz	x2, 17e2c <__gmpz_fdiv_r_ui@@Base+0x90>
   17db0:	ldrsw	x21, [x1, #4]
   17db4:	mov	x19, x0
   17db8:	cbz	w21, 17dfc <__gmpz_fdiv_r_ui@@Base+0x60>
   17dbc:	ldr	x0, [x1, #8]
   17dc0:	cmp	x21, #0x0
   17dc4:	cneg	x1, x21, mi  // mi = first
   17dc8:	mov	x20, x2
   17dcc:	bl	c540 <__gmpn_mod_1@plt>
   17dd0:	cbz	x0, 17dfc <__gmpz_fdiv_r_ui@@Base+0x60>
   17dd4:	ldr	w8, [x19]
   17dd8:	sub	x9, x20, x0
   17ddc:	cmp	w21, #0x0
   17de0:	csel	x20, x9, x0, lt  // lt = tstop
   17de4:	cmp	w8, #0x0
   17de8:	b.le	17e1c <__gmpz_fdiv_r_ui@@Base+0x80>
   17dec:	ldr	x0, [x19, #8]
   17df0:	mov	w8, #0x1                   	// #1
   17df4:	str	x20, [x0]
   17df8:	b	17e04 <__gmpz_fdiv_r_ui@@Base+0x68>
   17dfc:	mov	w8, wzr
   17e00:	mov	x20, xzr
   17e04:	str	w8, [x19, #4]
   17e08:	mov	x0, x20
   17e0c:	ldp	x20, x19, [sp, #32]
   17e10:	ldr	x21, [sp, #16]
   17e14:	ldp	x29, x30, [sp], #48
   17e18:	ret
   17e1c:	mov	w1, #0x1                   	// #1
   17e20:	mov	x0, x19
   17e24:	bl	c1c0 <__gmpz_realloc@plt>
   17e28:	b	17df0 <__gmpz_fdiv_r_ui@@Base+0x54>
   17e2c:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000017e30 <__gmpz_fdiv_q@@Base>:
   17e30:	stp	x29, x30, [sp, #-64]!
   17e34:	str	x23, [sp, #16]
   17e38:	stp	x22, x21, [sp, #32]
   17e3c:	stp	x20, x19, [sp, #48]
   17e40:	mov	x29, sp
   17e44:	sub	sp, sp, #0x10
   17e48:	ldrsw	x22, [x2, #4]
   17e4c:	ldr	w23, [x1, #4]
   17e50:	mov	x21, x1
   17e54:	mov	w9, #0x7f00                	// #32512
   17e58:	cmp	x22, #0x0
   17e5c:	cneg	x8, x22, mi  // mi = first
   17e60:	lsl	x1, x8, #3
   17e64:	mov	x20, x2
   17e68:	mov	x19, x0
   17e6c:	cmp	x1, x9
   17e70:	str	xzr, [x29, #24]
   17e74:	stur	w8, [x29, #-16]
   17e78:	b.hi	17ee8 <__gmpz_fdiv_q@@Base+0xb8>  // b.pmore
   17e7c:	add	x9, x1, #0xf
   17e80:	mov	x8, sp
   17e84:	and	x9, x9, #0xfffffffffffffff0
   17e88:	sub	x0, x8, x9
   17e8c:	mov	sp, x0
   17e90:	stur	x0, [x29, #-8]
   17e94:	sub	x1, x29, #0x10
   17e98:	mov	x0, x19
   17e9c:	mov	x2, x21
   17ea0:	mov	x3, x20
   17ea4:	bl	c120 <__gmpz_tdiv_qr@plt>
   17ea8:	eor	w8, w22, w23
   17eac:	tbz	w8, #31, 17ec8 <__gmpz_fdiv_q@@Base+0x98>
   17eb0:	ldur	w8, [x29, #-12]
   17eb4:	cbz	w8, 17ec8 <__gmpz_fdiv_q@@Base+0x98>
   17eb8:	mov	w2, #0x1                   	// #1
   17ebc:	mov	x0, x19
   17ec0:	mov	x1, x19
   17ec4:	bl	c270 <__gmpz_sub_ui@plt>
   17ec8:	ldr	x0, [x29, #24]
   17ecc:	cbnz	x0, 17ef4 <__gmpz_fdiv_q@@Base+0xc4>
   17ed0:	mov	sp, x29
   17ed4:	ldp	x20, x19, [sp, #48]
   17ed8:	ldp	x22, x21, [sp, #32]
   17edc:	ldr	x23, [sp, #16]
   17ee0:	ldp	x29, x30, [sp], #64
   17ee4:	ret
   17ee8:	add	x0, x29, #0x18
   17eec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   17ef0:	b	17e90 <__gmpz_fdiv_q@@Base+0x60>
   17ef4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   17ef8:	b	17ed0 <__gmpz_fdiv_q@@Base+0xa0>

0000000000017efc <__gmpz_fdiv_ui@@Base>:
   17efc:	stp	x29, x30, [sp, #-32]!
   17f00:	stp	x20, x19, [sp, #16]
   17f04:	mov	x29, sp
   17f08:	cbz	x1, 17f50 <__gmpz_fdiv_ui@@Base+0x54>
   17f0c:	ldrsw	x20, [x0, #4]
   17f10:	cbz	w20, 17f40 <__gmpz_fdiv_ui@@Base+0x44>
   17f14:	ldr	x0, [x0, #8]
   17f18:	mov	x19, x1
   17f1c:	cmp	x20, #0x0
   17f20:	cneg	x1, x20, mi  // mi = first
   17f24:	mov	x2, x19
   17f28:	bl	c540 <__gmpn_mod_1@plt>
   17f2c:	cmp	x0, #0x0
   17f30:	sub	x8, x19, x0
   17f34:	ccmp	w20, #0x0, #0x0, ne  // ne = any
   17f38:	csel	x0, x8, x0, lt  // lt = tstop
   17f3c:	b	17f44 <__gmpz_fdiv_ui@@Base+0x48>
   17f40:	mov	x0, xzr
   17f44:	ldp	x20, x19, [sp, #16]
   17f48:	ldp	x29, x30, [sp], #32
   17f4c:	ret
   17f50:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000017f54 <__gmpz_fib_ui@@Base>:
   17f54:	stp	x29, x30, [sp, #-96]!
   17f58:	stp	x20, x19, [sp, #80]
   17f5c:	mov	x20, x1
   17f60:	cmp	x1, #0x5d
   17f64:	mov	x19, x0
   17f68:	str	x27, [sp, #16]
   17f6c:	stp	x26, x25, [sp, #32]
   17f70:	stp	x24, x23, [sp, #48]
   17f74:	stp	x22, x21, [sp, #64]
   17f78:	mov	x29, sp
   17f7c:	b.hi	17fb4 <__gmpz_fib_ui@@Base+0x60>  // b.pmore
   17f80:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   17f84:	ldr	x8, [x8, #3808]
   17f88:	ldr	w9, [x19]
   17f8c:	add	x8, x8, x20, lsl #3
   17f90:	ldr	x21, [x8, #8]
   17f94:	cmp	w9, #0x0
   17f98:	b.le	18138 <__gmpz_fib_ui@@Base+0x1e4>
   17f9c:	ldr	x0, [x19, #8]
   17fa0:	cmp	x20, #0x0
   17fa4:	cset	w8, ne  // ne = any
   17fa8:	str	x21, [x0]
   17fac:	str	w8, [x19, #4]
   17fb0:	b	18118 <__gmpz_fib_ui@@Base+0x1c4>
   17fb4:	lsr	x8, x20, #6
   17fb8:	mov	w9, #0x17                  	// #23
   17fbc:	mul	x22, x8, x9
   17fc0:	ldrsw	x8, [x19]
   17fc4:	lsr	x9, x22, #6
   17fc8:	add	x23, x9, #0x5
   17fcc:	lsl	x1, x23, #1
   17fd0:	cmp	x1, x8
   17fd4:	lsr	x24, x20, #1
   17fd8:	b.gt	18148 <__gmpz_fib_ui@@Base+0x1f4>
   17fdc:	ldr	x21, [x19, #8]
   17fe0:	lsr	x8, x22, #8
   17fe4:	cmp	x8, #0x1fa
   17fe8:	lsl	x1, x23, #4
   17fec:	str	xzr, [x29, #24]
   17ff0:	b.hi	18158 <__gmpz_fib_ui@@Base+0x204>  // b.pmore
   17ff4:	add	x9, x1, #0xf
   17ff8:	mov	x8, sp
   17ffc:	and	x9, x9, #0x7ffffffffffffff0
   18000:	sub	x22, x8, x9
   18004:	mov	sp, x22
   18008:	add	x23, x22, x23, lsl #3
   1800c:	mov	x0, x22
   18010:	mov	x1, x23
   18014:	mov	x2, x24
   18018:	bl	d240 <__gmpn_fib2_ui@plt>
   1801c:	mov	x24, x0
   18020:	tbnz	w20, #0, 18060 <__gmpz_fib_ui@@Base+0x10c>
   18024:	mov	x0, x23
   18028:	mov	x1, x22
   1802c:	mov	x2, x23
   18030:	mov	x3, x24
   18034:	bl	ce00 <__gmpn_addlsh1_n@plt>
   18038:	cmp	x0, #0x0
   1803c:	str	x0, [x23, x24, lsl #3]
   18040:	cinc	x2, x24, ne  // ne = any
   18044:	mov	x0, x21
   18048:	mov	x1, x23
   1804c:	mov	x3, x22
   18050:	mov	x4, x24
   18054:	add	x25, x2, x24
   18058:	bl	cea0 <__gmpn_mul@plt>
   1805c:	b	180ec <__gmpz_fib_ui@@Base+0x198>
   18060:	mov	w3, #0x1                   	// #1
   18064:	mov	x0, x21
   18068:	mov	x1, x22
   1806c:	mov	x2, x24
   18070:	bl	c2d0 <__gmpn_lshift@plt>
   18074:	mov	x25, x0
   18078:	mov	x0, x22
   1807c:	mov	x1, x21
   18080:	mov	x2, x23
   18084:	mov	x3, x24
   18088:	bl	cc30 <__gmpn_add_n@plt>
   1808c:	adds	x8, x0, x25
   18090:	lsl	x27, x24, #3
   18094:	mov	x0, x23
   18098:	mov	x1, x21
   1809c:	mov	x2, x23
   180a0:	mov	x3, x24
   180a4:	str	x8, [x22, x27]
   180a8:	cinc	x26, x24, ne  // ne = any
   180ac:	bl	c420 <__gmpn_sub_n@plt>
   180b0:	sub	x8, x25, x0
   180b4:	add	x4, x8, x24
   180b8:	mov	x0, x21
   180bc:	mov	x1, x22
   180c0:	mov	x2, x26
   180c4:	mov	x3, x23
   180c8:	str	x8, [x23, x27]
   180cc:	add	x25, x26, x4
   180d0:	bl	cea0 <__gmpn_mul@plt>
   180d4:	ldr	x8, [x21]
   180d8:	tst	x20, #0x2
   180dc:	mov	w9, #0x2                   	// #2
   180e0:	cneg	x9, x9, ne  // ne = any
   180e4:	add	x8, x8, x9
   180e8:	str	x8, [x21]
   180ec:	cmp	x0, #0x0
   180f0:	cset	w8, eq  // eq = none
   180f4:	sub	x8, x25, x8
   180f8:	add	x9, x21, x8, lsl #3
   180fc:	ldur	x9, [x9, #-8]
   18100:	cmp	x9, #0x0
   18104:	cset	w9, eq  // eq = none
   18108:	sub	w8, w8, w9
   1810c:	str	w8, [x19, #4]
   18110:	ldr	x0, [x29, #24]
   18114:	cbnz	x0, 18168 <__gmpz_fib_ui@@Base+0x214>
   18118:	mov	sp, x29
   1811c:	ldp	x20, x19, [sp, #80]
   18120:	ldp	x22, x21, [sp, #64]
   18124:	ldp	x24, x23, [sp, #48]
   18128:	ldp	x26, x25, [sp, #32]
   1812c:	ldr	x27, [sp, #16]
   18130:	ldp	x29, x30, [sp], #96
   18134:	ret
   18138:	mov	w1, #0x1                   	// #1
   1813c:	mov	x0, x19
   18140:	bl	c1c0 <__gmpz_realloc@plt>
   18144:	b	17fa0 <__gmpz_fib_ui@@Base+0x4c>
   18148:	mov	x0, x19
   1814c:	bl	c1c0 <__gmpz_realloc@plt>
   18150:	mov	x21, x0
   18154:	b	17fe0 <__gmpz_fib_ui@@Base+0x8c>
   18158:	add	x0, x29, #0x18
   1815c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   18160:	mov	x22, x0
   18164:	b	18008 <__gmpz_fib_ui@@Base+0xb4>
   18168:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1816c:	b	18118 <__gmpz_fib_ui@@Base+0x1c4>

0000000000018170 <__gmpz_fib2_ui@@Base>:
   18170:	stp	x29, x30, [sp, #-64]!
   18174:	stp	x22, x21, [sp, #32]
   18178:	stp	x20, x19, [sp, #48]
   1817c:	mov	x20, x2
   18180:	mov	x19, x1
   18184:	cmp	x2, #0x5d
   18188:	mov	x21, x0
   1818c:	str	x23, [sp, #16]
   18190:	mov	x29, sp
   18194:	b.hi	181f0 <__gmpz_fib2_ui@@Base+0x80>  // b.pmore
   18198:	adrp	x22, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1819c:	ldr	x22, [x22, #3808]
   181a0:	ldr	w8, [x21]
   181a4:	add	x9, x22, x20, lsl #3
   181a8:	ldr	x23, [x9, #8]
   181ac:	cmp	w8, #0x0
   181b0:	b.le	18264 <__gmpz_fib2_ui@@Base+0xf4>
   181b4:	ldr	x0, [x21, #8]
   181b8:	cmp	x20, #0x0
   181bc:	cset	w8, ne  // ne = any
   181c0:	str	x23, [x0]
   181c4:	str	w8, [x21, #4]
   181c8:	ldr	w8, [x19]
   181cc:	sbfiz	x9, x20, #3, #32
   181d0:	ldr	x21, [x22, x9]
   181d4:	cmp	w8, #0x0
   181d8:	b.le	18274 <__gmpz_fib2_ui@@Base+0x104>
   181dc:	ldr	x0, [x19, #8]
   181e0:	cmp	x20, #0x1
   181e4:	str	x21, [x0]
   181e8:	cset	w8, ne  // ne = any
   181ec:	b	1824c <__gmpz_fib2_ui@@Base+0xdc>
   181f0:	lsr	x8, x20, #5
   181f4:	mov	w9, #0x17                  	// #23
   181f8:	ldrsw	x10, [x21]
   181fc:	mul	x8, x8, x9
   18200:	lsr	x8, x8, #6
   18204:	add	x23, x8, #0x4
   18208:	cmp	x23, x10
   1820c:	b.gt	18284 <__gmpz_fib2_ui@@Base+0x114>
   18210:	ldr	x22, [x21, #8]
   18214:	ldrsw	x8, [x19]
   18218:	cmp	x23, x8
   1821c:	b.gt	18298 <__gmpz_fib2_ui@@Base+0x128>
   18220:	ldr	x23, [x19, #8]
   18224:	mov	x0, x22
   18228:	mov	x1, x23
   1822c:	mov	x2, x20
   18230:	bl	d240 <__gmpn_fib2_ui@plt>
   18234:	str	w0, [x21, #4]
   18238:	add	x8, x23, x0, lsl #3
   1823c:	ldur	x8, [x8, #-8]
   18240:	cmp	x8, #0x0
   18244:	cset	w8, eq  // eq = none
   18248:	sub	w8, w0, w8
   1824c:	str	w8, [x19, #4]
   18250:	ldp	x20, x19, [sp, #48]
   18254:	ldp	x22, x21, [sp, #32]
   18258:	ldr	x23, [sp, #16]
   1825c:	ldp	x29, x30, [sp], #64
   18260:	ret
   18264:	mov	w1, #0x1                   	// #1
   18268:	mov	x0, x21
   1826c:	bl	c1c0 <__gmpz_realloc@plt>
   18270:	b	181b8 <__gmpz_fib2_ui@@Base+0x48>
   18274:	mov	w1, #0x1                   	// #1
   18278:	mov	x0, x19
   1827c:	bl	c1c0 <__gmpz_realloc@plt>
   18280:	b	181e0 <__gmpz_fib2_ui@@Base+0x70>
   18284:	mov	x0, x21
   18288:	mov	x1, x23
   1828c:	bl	c1c0 <__gmpz_realloc@plt>
   18290:	mov	x22, x0
   18294:	b	18214 <__gmpz_fib2_ui@@Base+0xa4>
   18298:	mov	x0, x19
   1829c:	mov	x1, x23
   182a0:	bl	c1c0 <__gmpz_realloc@plt>
   182a4:	mov	x23, x0
   182a8:	b	18224 <__gmpz_fib2_ui@@Base+0xb4>

00000000000182ac <__gmpz_fits_sint_p@@Base>:
   182ac:	ldr	x8, [x0, #8]
   182b0:	ldr	w9, [x0, #4]
   182b4:	ldr	x8, [x8]
   182b8:	cmn	w9, #0x1
   182bc:	b.eq	182dc <__gmpz_fits_sint_p@@Base+0x30>  // b.none
   182c0:	cbz	w9, 182ec <__gmpz_fits_sint_p@@Base+0x40>
   182c4:	cmp	w9, #0x1
   182c8:	b.ne	182f4 <__gmpz_fits_sint_p@@Base+0x48>  // b.any
   182cc:	lsr	x8, x8, #31
   182d0:	cmp	x8, #0x0
   182d4:	cset	w0, eq  // eq = none
   182d8:	ret
   182dc:	mov	w9, #0x80000001            	// #-2147483647
   182e0:	cmp	x8, x9
   182e4:	cset	w0, cc  // cc = lo, ul, last
   182e8:	ret
   182ec:	mov	w0, #0x1                   	// #1
   182f0:	ret
   182f4:	mov	w0, wzr
   182f8:	ret

00000000000182fc <__gmpz_fits_slong_p@@Base>:
   182fc:	ldr	x8, [x0, #8]
   18300:	ldr	w9, [x0, #4]
   18304:	ldr	x8, [x8]
   18308:	cmn	w9, #0x1
   1830c:	b.eq	18328 <__gmpz_fits_slong_p@@Base+0x2c>  // b.none
   18310:	cbz	w9, 18338 <__gmpz_fits_slong_p@@Base+0x3c>
   18314:	cmp	w9, #0x1
   18318:	b.ne	18340 <__gmpz_fits_slong_p@@Base+0x44>  // b.any
   1831c:	lsr	x8, x8, #63
   18320:	eor	w0, w8, #0x1
   18324:	ret
   18328:	mov	x9, #0x8000000000000001    	// #-9223372036854775807
   1832c:	cmp	x8, x9
   18330:	cset	w0, cc  // cc = lo, ul, last
   18334:	ret
   18338:	mov	w0, #0x1                   	// #1
   1833c:	ret
   18340:	mov	w0, wzr
   18344:	ret

0000000000018348 <__gmpz_fits_sshort_p@@Base>:
   18348:	ldr	x8, [x0, #8]
   1834c:	ldr	w9, [x0, #4]
   18350:	ldr	x8, [x8]
   18354:	cmn	w9, #0x1
   18358:	b.eq	18374 <__gmpz_fits_sshort_p@@Base+0x2c>  // b.none
   1835c:	cbz	w9, 18380 <__gmpz_fits_sshort_p@@Base+0x38>
   18360:	cmp	w9, #0x1
   18364:	b.ne	18388 <__gmpz_fits_sshort_p@@Base+0x40>  // b.any
   18368:	cmp	x8, #0x8, lsl #12
   1836c:	cset	w0, cc  // cc = lo, ul, last
   18370:	ret
   18374:	cmp	x8, #0x8, lsl #12
   18378:	cset	w0, ls  // ls = plast
   1837c:	ret
   18380:	mov	w0, #0x1                   	// #1
   18384:	ret
   18388:	mov	w0, wzr
   1838c:	ret

0000000000018390 <__gmpz_fits_uint_p@@Base>:
   18390:	ldr	w8, [x0, #4]
   18394:	cbz	w8, 183b4 <__gmpz_fits_uint_p@@Base+0x24>
   18398:	cmp	w8, #0x1
   1839c:	b.ne	183bc <__gmpz_fits_uint_p@@Base+0x2c>  // b.any
   183a0:	ldr	x8, [x0, #8]
   183a4:	ldr	w8, [x8, #4]
   183a8:	cmp	w8, #0x0
   183ac:	cset	w0, eq  // eq = none
   183b0:	ret
   183b4:	mov	w0, #0x1                   	// #1
   183b8:	ret
   183bc:	mov	w0, wzr
   183c0:	ret

00000000000183c4 <__gmpz_fits_ulong_p@@Base>:
   183c4:	ldr	w8, [x0, #4]
   183c8:	cmp	w8, #0x2
   183cc:	cset	w0, cc  // cc = lo, ul, last
   183d0:	ret

00000000000183d4 <__gmpz_fits_ushort_p@@Base>:
   183d4:	ldr	w8, [x0, #4]
   183d8:	cbz	w8, 183f8 <__gmpz_fits_ushort_p@@Base+0x24>
   183dc:	cmp	w8, #0x1
   183e0:	b.ne	18400 <__gmpz_fits_ushort_p@@Base+0x2c>  // b.any
   183e4:	ldr	x8, [x0, #8]
   183e8:	ldr	x8, [x8]
   183ec:	cmp	x8, #0x10, lsl #12
   183f0:	cset	w0, cc  // cc = lo, ul, last
   183f4:	ret
   183f8:	mov	w0, #0x1                   	// #1
   183fc:	ret
   18400:	mov	w0, wzr
   18404:	ret

0000000000018408 <__gmpz_gcd@@Base>:
   18408:	stp	x29, x30, [sp, #-96]!
   1840c:	stp	x26, x25, [sp, #32]
   18410:	stp	x24, x23, [sp, #48]
   18414:	stp	x22, x21, [sp, #64]
   18418:	stp	x20, x19, [sp, #80]
   1841c:	ldr	w8, [x1, #4]
   18420:	ldr	w9, [x2, #4]
   18424:	ldr	x22, [x2, #8]
   18428:	mov	x19, x0
   1842c:	cmp	w8, #0x0
   18430:	cneg	w21, w8, mi  // mi = first
   18434:	cmp	w9, #0x0
   18438:	cneg	w20, w9, mi  // mi = first
   1843c:	str	x27, [sp, #16]
   18440:	mov	x29, sp
   18444:	cbz	w21, 18470 <__gmpz_gcd@@Base+0x68>
   18448:	ldr	x23, [x1, #8]
   1844c:	cbz	w20, 1849c <__gmpz_gcd@@Base+0x94>
   18450:	cmp	w21, #0x1
   18454:	b.ne	184c8 <__gmpz_gcd@@Base+0xc0>  // b.any
   18458:	mov	w8, #0x1                   	// #1
   1845c:	str	w8, [x19, #4]
   18460:	ldr	x2, [x23]
   18464:	mov	x0, x22
   18468:	mov	x1, x20
   1846c:	b	184e4 <__gmpz_gcd@@Base+0xdc>
   18470:	cmp	x19, x2
   18474:	str	w20, [x19, #4]
   18478:	b.eq	18728 <__gmpz_gcd@@Base+0x320>  // b.none
   1847c:	ldrsw	x8, [x19]
   18480:	cmp	x20, x8
   18484:	b.gt	1875c <__gmpz_gcd@@Base+0x354>
   18488:	ldr	x0, [x19, #8]
   1848c:	mov	x1, x22
   18490:	mov	x2, x20
   18494:	bl	cc10 <__gmpn_copyi@plt>
   18498:	b	18728 <__gmpz_gcd@@Base+0x320>
   1849c:	cmp	x19, x1
   184a0:	str	w21, [x19, #4]
   184a4:	b.eq	18728 <__gmpz_gcd@@Base+0x320>  // b.none
   184a8:	ldrsw	x8, [x19]
   184ac:	cmp	x21, x8
   184b0:	b.gt	1876c <__gmpz_gcd@@Base+0x364>
   184b4:	ldr	x0, [x19, #8]
   184b8:	mov	x1, x23
   184bc:	mov	x2, x21
   184c0:	bl	cc10 <__gmpn_copyi@plt>
   184c4:	b	18728 <__gmpz_gcd@@Base+0x320>
   184c8:	cmp	w20, #0x1
   184cc:	b.ne	18504 <__gmpz_gcd@@Base+0xfc>  // b.any
   184d0:	mov	w8, #0x1                   	// #1
   184d4:	str	w8, [x19, #4]
   184d8:	ldr	x2, [x22]
   184dc:	mov	x0, x23
   184e0:	mov	x1, x21
   184e4:	bl	c0c0 <__gmpn_gcd_1@plt>
   184e8:	ldr	w8, [x19]
   184ec:	mov	x20, x0
   184f0:	cmp	w8, #0x0
   184f4:	b.le	18748 <__gmpz_gcd@@Base+0x340>
   184f8:	ldr	x8, [x19, #8]
   184fc:	str	x20, [x8]
   18500:	b	18728 <__gmpz_gcd@@Base+0x320>
   18504:	sub	x25, x23, #0x8
   18508:	str	xzr, [x29, #24]
   1850c:	ldr	x8, [x25, #8]!
   18510:	cbz	x8, 1850c <__gmpz_gcd@@Base+0x104>
   18514:	sub	x9, x25, x23
   18518:	asr	x27, x9, #3
   1851c:	sub	x21, x21, x27
   18520:	rbit	x8, x8
   18524:	lsl	x1, x21, #3
   18528:	mov	w9, #0x7f00                	// #32512
   1852c:	cmp	x1, x9
   18530:	clz	x24, x8
   18534:	b.hi	1877c <__gmpz_gcd@@Base+0x374>  // b.pmore
   18538:	add	x9, x1, #0xf
   1853c:	mov	x8, sp
   18540:	and	x9, x9, #0xfffffffffffffff0
   18544:	sub	x23, x8, x9
   18548:	mov	sp, x23
   1854c:	mov	x0, x23
   18550:	mov	x1, x25
   18554:	mov	x2, x21
   18558:	cbz	x24, 1857c <__gmpz_gcd@@Base+0x174>
   1855c:	mov	w3, w24
   18560:	bl	c2f0 <__gmpn_rshift@plt>
   18564:	add	x8, x23, x21, lsl #3
   18568:	ldur	x8, [x8, #-8]
   1856c:	cmp	x8, #0x0
   18570:	cset	w8, eq  // eq = none
   18574:	sub	x21, x21, x8
   18578:	b	18580 <__gmpz_gcd@@Base+0x178>
   1857c:	bl	cc10 <__gmpn_copyi@plt>
   18580:	sub	x1, x22, #0x8
   18584:	ldr	x8, [x1, #8]!
   18588:	cbz	x8, 18584 <__gmpz_gcd@@Base+0x17c>
   1858c:	sub	x9, x1, x22
   18590:	asr	x26, x9, #3
   18594:	sub	x25, x20, x26
   18598:	rbit	x10, x8
   1859c:	lsl	x8, x25, #3
   185a0:	mov	w9, #0x7f00                	// #32512
   185a4:	cmp	x8, x9
   185a8:	clz	x22, x10
   185ac:	b.hi	1878c <__gmpz_gcd@@Base+0x384>  // b.pmore
   185b0:	add	x8, x8, #0xf
   185b4:	mov	x9, sp
   185b8:	and	x8, x8, #0xfffffffffffffff0
   185bc:	sub	x20, x9, x8
   185c0:	mov	sp, x20
   185c4:	mov	x0, x20
   185c8:	mov	x2, x25
   185cc:	cbz	x22, 185f8 <__gmpz_gcd@@Base+0x1f0>
   185d0:	mov	w3, w22
   185d4:	bl	c2f0 <__gmpn_rshift@plt>
   185d8:	add	x8, x20, x25, lsl #3
   185dc:	ldur	x8, [x8, #-8]
   185e0:	cmp	x8, #0x0
   185e4:	cset	w8, eq  // eq = none
   185e8:	sub	x25, x25, x8
   185ec:	cmp	x27, x26
   185f0:	b.le	18604 <__gmpz_gcd@@Base+0x1fc>
   185f4:	b	18610 <__gmpz_gcd@@Base+0x208>
   185f8:	bl	cc10 <__gmpn_copyi@plt>
   185fc:	cmp	x27, x26
   18600:	b.gt	18610 <__gmpz_gcd@@Base+0x208>
   18604:	b.ge	1861c <__gmpz_gcd@@Base+0x214>  // b.tcont
   18608:	mov	x26, x27
   1860c:	mov	x22, x24
   18610:	cmp	x21, x25
   18614:	b.ge	18630 <__gmpz_gcd@@Base+0x228>  // b.tcont
   18618:	b	1864c <__gmpz_gcd@@Base+0x244>
   1861c:	cmp	x24, x22
   18620:	csel	x22, x24, x22, cc  // cc = lo, ul, last
   18624:	mov	x26, x27
   18628:	cmp	x21, x25
   1862c:	b.lt	1864c <__gmpz_gcd@@Base+0x244>  // b.tstop
   18630:	b.ne	18664 <__gmpz_gcd@@Base+0x25c>  // b.any
   18634:	add	x8, x23, x21, lsl #3
   18638:	add	x9, x20, x25, lsl #3
   1863c:	ldur	x8, [x8, #-8]
   18640:	ldur	x9, [x9, #-8]
   18644:	cmp	x8, x9
   18648:	b.cs	18664 <__gmpz_gcd@@Base+0x25c>  // b.hs, b.nlast
   1864c:	mov	x0, x20
   18650:	mov	x1, x20
   18654:	mov	x2, x25
   18658:	mov	x3, x23
   1865c:	mov	x4, x21
   18660:	b	18678 <__gmpz_gcd@@Base+0x270>
   18664:	mov	x0, x20
   18668:	mov	x1, x23
   1866c:	mov	x2, x21
   18670:	mov	x3, x20
   18674:	mov	x4, x25
   18678:	bl	cdd0 <__gmpn_gcd@plt>
   1867c:	mov	x23, x0
   18680:	add	x21, x0, x26
   18684:	cbz	x22, 186e8 <__gmpz_gcd@@Base+0x2e0>
   18688:	add	x8, x20, x23, lsl #3
   1868c:	ldur	x8, [x8, #-8]
   18690:	neg	x9, x22
   18694:	ldrsw	x10, [x19]
   18698:	lsr	x8, x8, x9
   1869c:	cmp	x8, #0x0
   186a0:	cinc	x21, x21, ne  // ne = any
   186a4:	cmp	x21, x10
   186a8:	b.gt	187b0 <__gmpz_gcd@@Base+0x3a8>
   186ac:	ldr	x24, [x19, #8]
   186b0:	cbz	x26, 186c4 <__gmpz_gcd@@Base+0x2bc>
   186b4:	lsl	x2, x26, #3
   186b8:	mov	x0, x24
   186bc:	mov	w1, wzr
   186c0:	bl	c780 <memset@plt>
   186c4:	add	x24, x24, x26, lsl #3
   186c8:	mov	x0, x24
   186cc:	mov	x1, x20
   186d0:	mov	x2, x23
   186d4:	mov	w3, w22
   186d8:	bl	c2d0 <__gmpn_lshift@plt>
   186dc:	cbz	x0, 1871c <__gmpz_gcd@@Base+0x314>
   186e0:	str	x0, [x24, x23, lsl #3]
   186e4:	b	1871c <__gmpz_gcd@@Base+0x314>
   186e8:	ldrsw	x8, [x19]
   186ec:	cmp	x21, x8
   186f0:	b.gt	187c8 <__gmpz_gcd@@Base+0x3c0>
   186f4:	ldr	x22, [x19, #8]
   186f8:	cbz	x26, 1870c <__gmpz_gcd@@Base+0x304>
   186fc:	lsl	x2, x26, #3
   18700:	mov	x0, x22
   18704:	mov	w1, wzr
   18708:	bl	c780 <memset@plt>
   1870c:	add	x0, x22, x26, lsl #3
   18710:	mov	x1, x20
   18714:	mov	x2, x23
   18718:	bl	cc10 <__gmpn_copyi@plt>
   1871c:	str	w21, [x19, #4]
   18720:	ldr	x0, [x29, #24]
   18724:	cbnz	x0, 187a8 <__gmpz_gcd@@Base+0x3a0>
   18728:	mov	sp, x29
   1872c:	ldp	x20, x19, [sp, #80]
   18730:	ldp	x22, x21, [sp, #64]
   18734:	ldp	x24, x23, [sp, #48]
   18738:	ldp	x26, x25, [sp, #32]
   1873c:	ldr	x27, [sp, #16]
   18740:	ldp	x29, x30, [sp], #96
   18744:	ret
   18748:	mov	w1, #0x1                   	// #1
   1874c:	mov	x0, x19
   18750:	bl	c1c0 <__gmpz_realloc@plt>
   18754:	str	x20, [x0]
   18758:	b	18728 <__gmpz_gcd@@Base+0x320>
   1875c:	mov	x0, x19
   18760:	mov	x1, x20
   18764:	bl	c1c0 <__gmpz_realloc@plt>
   18768:	b	1848c <__gmpz_gcd@@Base+0x84>
   1876c:	mov	x0, x19
   18770:	mov	x1, x21
   18774:	bl	c1c0 <__gmpz_realloc@plt>
   18778:	b	184b8 <__gmpz_gcd@@Base+0xb0>
   1877c:	add	x0, x29, #0x18
   18780:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   18784:	mov	x23, x0
   18788:	b	1854c <__gmpz_gcd@@Base+0x144>
   1878c:	add	x0, x29, #0x18
   18790:	mov	x20, x1
   18794:	mov	x1, x8
   18798:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1879c:	mov	x1, x20
   187a0:	mov	x20, x0
   187a4:	b	185c4 <__gmpz_gcd@@Base+0x1bc>
   187a8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   187ac:	b	18728 <__gmpz_gcd@@Base+0x320>
   187b0:	mov	x0, x19
   187b4:	mov	x1, x21
   187b8:	bl	c1c0 <__gmpz_realloc@plt>
   187bc:	mov	x24, x0
   187c0:	cbnz	x26, 186b4 <__gmpz_gcd@@Base+0x2ac>
   187c4:	b	186c4 <__gmpz_gcd@@Base+0x2bc>
   187c8:	mov	x0, x19
   187cc:	mov	x1, x21
   187d0:	bl	c1c0 <__gmpz_realloc@plt>
   187d4:	mov	x22, x0
   187d8:	cbnz	x26, 186fc <__gmpz_gcd@@Base+0x2f4>
   187dc:	b	1870c <__gmpz_gcd@@Base+0x304>

00000000000187e0 <__gmpz_gcd_ui@@Base>:
   187e0:	stp	x29, x30, [sp, #-48]!
   187e4:	stp	x22, x21, [sp, #16]
   187e8:	stp	x20, x19, [sp, #32]
   187ec:	ldr	w8, [x1, #4]
   187f0:	mov	x20, x2
   187f4:	mov	x19, x0
   187f8:	mov	x29, sp
   187fc:	cmp	w8, #0x0
   18800:	cneg	w21, w8, mi  // mi = first
   18804:	cbz	w21, 18824 <__gmpz_gcd_ui@@Base+0x44>
   18808:	mov	x22, x1
   1880c:	cbz	x20, 1884c <__gmpz_gcd_ui@@Base+0x6c>
   18810:	ldr	x0, [x22, #8]
   18814:	mov	x1, x21
   18818:	mov	x2, x20
   1881c:	bl	c0c0 <__gmpn_gcd_1@plt>
   18820:	mov	x20, x0
   18824:	cbz	x19, 18888 <__gmpz_gcd_ui@@Base+0xa8>
   18828:	ldr	w8, [x19]
   1882c:	cmp	w8, #0x0
   18830:	b.le	1889c <__gmpz_gcd_ui@@Base+0xbc>
   18834:	ldr	x0, [x19, #8]
   18838:	cmp	x20, #0x0
   1883c:	cset	w8, ne  // ne = any
   18840:	str	x20, [x0]
   18844:	str	w8, [x19, #4]
   18848:	b	18888 <__gmpz_gcd_ui@@Base+0xa8>
   1884c:	cbz	x19, 18878 <__gmpz_gcd_ui@@Base+0x98>
   18850:	cmp	x22, x19
   18854:	b.eq	18874 <__gmpz_gcd_ui@@Base+0x94>  // b.none
   18858:	ldrsw	x8, [x19]
   1885c:	cmp	x21, x8
   18860:	b.gt	188ac <__gmpz_gcd_ui@@Base+0xcc>
   18864:	ldr	x0, [x19, #8]
   18868:	ldr	x1, [x22, #8]
   1886c:	mov	x2, x21
   18870:	bl	cc10 <__gmpn_copyi@plt>
   18874:	str	w21, [x19, #4]
   18878:	ldr	x8, [x22, #8]
   1887c:	cmp	w21, #0x1
   18880:	ldr	x8, [x8]
   18884:	csel	x20, x8, xzr, eq  // eq = none
   18888:	mov	x0, x20
   1888c:	ldp	x20, x19, [sp, #32]
   18890:	ldp	x22, x21, [sp, #16]
   18894:	ldp	x29, x30, [sp], #48
   18898:	ret
   1889c:	mov	w1, #0x1                   	// #1
   188a0:	mov	x0, x19
   188a4:	bl	c1c0 <__gmpz_realloc@plt>
   188a8:	b	18838 <__gmpz_gcd_ui@@Base+0x58>
   188ac:	mov	x0, x19
   188b0:	mov	x1, x21
   188b4:	bl	c1c0 <__gmpz_realloc@plt>
   188b8:	b	18864 <__gmpz_gcd_ui@@Base+0x84>

00000000000188bc <__gmpz_gcdext@@Base>:
   188bc:	stp	x29, x30, [sp, #-96]!
   188c0:	stp	x28, x27, [sp, #16]
   188c4:	stp	x26, x25, [sp, #32]
   188c8:	stp	x24, x23, [sp, #48]
   188cc:	stp	x22, x21, [sp, #64]
   188d0:	stp	x20, x19, [sp, #80]
   188d4:	mov	x29, sp
   188d8:	sub	sp, sp, #0x50
   188dc:	ldr	w8, [x3, #4]
   188e0:	ldr	w9, [x4, #4]
   188e4:	mov	x19, x0
   188e8:	cmp	w8, #0x0
   188ec:	cneg	w8, w8, mi  // mi = first
   188f0:	cmp	w9, #0x0
   188f4:	cneg	w9, w9, mi  // mi = first
   188f8:	cmp	w8, w9
   188fc:	csel	w26, w8, w9, cc  // cc = lo, ul, last
   18900:	csel	w23, w9, w8, cc  // cc = lo, ul, last
   18904:	csel	x21, x3, x4, cc  // cc = lo, ul, last
   18908:	csel	x24, x4, x3, cc  // cc = lo, ul, last
   1890c:	csel	x25, x1, x2, cc  // cc = lo, ul, last
   18910:	csel	x20, x2, x1, cc  // cc = lo, ul, last
   18914:	cbz	w26, 18a78 <__gmpz_gcdext@@Base+0x1bc>
   18918:	add	x8, x26, x26, lsl #1
   1891c:	add	x8, x23, x8
   18920:	add	x8, x8, #0x1
   18924:	cmp	x8, #0xfe0
   18928:	lsl	x1, x8, #3
   1892c:	stur	xzr, [x29, #-16]
   18930:	stur	x25, [x29, #-72]
   18934:	b.hi	18af8 <__gmpz_gcdext@@Base+0x23c>  // b.pmore
   18938:	add	x9, x1, #0xf
   1893c:	mov	x8, sp
   18940:	and	x9, x9, #0xfffffffff0
   18944:	sub	x22, x8, x9
   18948:	mov	sp, x22
   1894c:	lsl	x8, x26, #3
   18950:	add	x27, x22, x8
   18954:	ldr	x1, [x24, #8]
   18958:	add	x9, x27, x8
   1895c:	add	x28, x9, #0x8
   18960:	add	x25, x28, x8
   18964:	mov	x0, x25
   18968:	mov	x2, x23
   1896c:	bl	cc10 <__gmpn_copyi@plt>
   18970:	ldr	x1, [x21, #8]
   18974:	mov	x0, x28
   18978:	mov	x2, x26
   1897c:	bl	cc10 <__gmpn_copyi@plt>
   18980:	sub	x2, x29, #0x8
   18984:	mov	x0, x22
   18988:	mov	x1, x27
   1898c:	mov	x3, x25
   18990:	mov	x4, x23
   18994:	mov	x5, x28
   18998:	mov	x6, x26
   1899c:	bl	c6f0 <__gmpn_gcdext@plt>
   189a0:	ldur	x8, [x29, #-8]
   189a4:	ldr	w9, [x24, #4]
   189a8:	ldur	x25, [x29, #-72]
   189ac:	mov	x26, x0
   189b0:	cmp	x8, #0x0
   189b4:	cneg	x28, x8, mi  // mi = first
   189b8:	cmp	w9, #0x0
   189bc:	cneg	x8, x8, lt  // lt = tstop
   189c0:	stur	x8, [x29, #-8]
   189c4:	cbz	x25, 18a1c <__gmpz_gcdext@@Base+0x160>
   189c8:	stur	w8, [x29, #-60]
   189cc:	add	x8, x27, x28, lsl #3
   189d0:	add	w9, w23, w28
   189d4:	stur	x8, [x29, #-24]
   189d8:	add	w8, w9, #0x1
   189dc:	sub	x0, x29, #0x20
   189e0:	sub	x1, x29, #0x40
   189e4:	mov	x2, x24
   189e8:	stur	x22, [x29, #-40]
   189ec:	stur	w26, [x29, #-44]
   189f0:	stur	x27, [x29, #-56]
   189f4:	stur	w8, [x29, #-32]
   189f8:	bl	c620 <__gmpz_mul@plt>
   189fc:	sub	x0, x29, #0x20
   18a00:	sub	x1, x29, #0x30
   18a04:	sub	x2, x29, #0x20
   18a08:	bl	c3b0 <__gmpz_sub@plt>
   18a0c:	sub	x1, x29, #0x20
   18a10:	mov	x0, x25
   18a14:	mov	x2, x21
   18a18:	bl	c550 <__gmpz_divexact@plt>
   18a1c:	cbz	x20, 18a44 <__gmpz_gcdext@@Base+0x188>
   18a20:	ldrsw	x8, [x20]
   18a24:	cmp	x28, x8
   18a28:	b.gt	18b08 <__gmpz_gcdext@@Base+0x24c>
   18a2c:	ldr	x0, [x20, #8]
   18a30:	mov	x1, x27
   18a34:	mov	x2, x28
   18a38:	bl	cc10 <__gmpn_copyi@plt>
   18a3c:	ldur	x8, [x29, #-8]
   18a40:	str	w8, [x20, #4]
   18a44:	cbz	x19, 18a68 <__gmpz_gcdext@@Base+0x1ac>
   18a48:	ldrsw	x8, [x19]
   18a4c:	cmp	x26, x8
   18a50:	b.gt	18b18 <__gmpz_gcdext@@Base+0x25c>
   18a54:	ldr	x0, [x19, #8]
   18a58:	mov	x1, x22
   18a5c:	mov	x2, x26
   18a60:	bl	cc10 <__gmpn_copyi@plt>
   18a64:	str	w26, [x19, #4]
   18a68:	ldur	x0, [x29, #-16]
   18a6c:	cbz	x0, 18ad8 <__gmpz_gcdext@@Base+0x21c>
   18a70:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   18a74:	b	18ad8 <__gmpz_gcdext@@Base+0x21c>
   18a78:	ldr	w8, [x24, #4]
   18a7c:	cmp	w23, #0x0
   18a80:	cset	w9, ne  // ne = any
   18a84:	cmp	w8, #0x0
   18a88:	csinv	w22, w9, wzr, ge  // ge = tcont
   18a8c:	cbz	x19, 18ab0 <__gmpz_gcdext@@Base+0x1f4>
   18a90:	ldrsw	x8, [x19]
   18a94:	cmp	x23, x8
   18a98:	b.gt	18b28 <__gmpz_gcdext@@Base+0x26c>
   18a9c:	ldr	x0, [x19, #8]
   18aa0:	ldr	x1, [x24, #8]
   18aa4:	mov	x2, x23
   18aa8:	bl	cc10 <__gmpn_copyi@plt>
   18aac:	str	w23, [x19, #4]
   18ab0:	cbz	x25, 18ab8 <__gmpz_gcdext@@Base+0x1fc>
   18ab4:	str	wzr, [x25, #4]
   18ab8:	cbz	x20, 18ad8 <__gmpz_gcdext@@Base+0x21c>
   18abc:	ldr	w8, [x20]
   18ac0:	str	w22, [x20, #4]
   18ac4:	cmp	w8, #0x0
   18ac8:	b.le	18b38 <__gmpz_gcdext@@Base+0x27c>
   18acc:	ldr	x0, [x20, #8]
   18ad0:	mov	w8, #0x1                   	// #1
   18ad4:	str	x8, [x0]
   18ad8:	mov	sp, x29
   18adc:	ldp	x20, x19, [sp, #80]
   18ae0:	ldp	x22, x21, [sp, #64]
   18ae4:	ldp	x24, x23, [sp, #48]
   18ae8:	ldp	x26, x25, [sp, #32]
   18aec:	ldp	x28, x27, [sp, #16]
   18af0:	ldp	x29, x30, [sp], #96
   18af4:	ret
   18af8:	sub	x0, x29, #0x10
   18afc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   18b00:	mov	x22, x0
   18b04:	b	1894c <__gmpz_gcdext@@Base+0x90>
   18b08:	mov	x0, x20
   18b0c:	mov	x1, x28
   18b10:	bl	c1c0 <__gmpz_realloc@plt>
   18b14:	b	18a30 <__gmpz_gcdext@@Base+0x174>
   18b18:	mov	x0, x19
   18b1c:	mov	x1, x26
   18b20:	bl	c1c0 <__gmpz_realloc@plt>
   18b24:	b	18a58 <__gmpz_gcdext@@Base+0x19c>
   18b28:	mov	x0, x19
   18b2c:	mov	x1, x23
   18b30:	bl	c1c0 <__gmpz_realloc@plt>
   18b34:	b	18aa0 <__gmpz_gcdext@@Base+0x1e4>
   18b38:	mov	w1, #0x1                   	// #1
   18b3c:	mov	x0, x20
   18b40:	bl	c1c0 <__gmpz_realloc@plt>
   18b44:	b	18ad0 <__gmpz_gcdext@@Base+0x214>

0000000000018b48 <__gmpz_get_d@@Base>:
   18b48:	stp	x29, x30, [sp, #-16]!
   18b4c:	ldrsw	x2, [x0, #4]
   18b50:	mov	x29, sp
   18b54:	cbz	w2, 18b74 <__gmpz_get_d@@Base+0x2c>
   18b58:	ldr	x0, [x0, #8]
   18b5c:	cmp	x2, #0x0
   18b60:	cneg	x1, x2, mi  // mi = first
   18b64:	mov	x3, xzr
   18b68:	bl	c070 <__gmpn_get_d@plt>
   18b6c:	ldp	x29, x30, [sp], #16
   18b70:	ret
   18b74:	fmov	d0, xzr
   18b78:	ldp	x29, x30, [sp], #16
   18b7c:	ret

0000000000018b80 <__gmpz_get_d_2exp@@Base>:
   18b80:	stp	x29, x30, [sp, #-16]!
   18b84:	ldrsw	x2, [x1, #4]
   18b88:	mov	x29, sp
   18b8c:	cbz	w2, 18bc8 <__gmpz_get_d_2exp@@Base+0x48>
   18b90:	ldr	x8, [x1, #8]
   18b94:	cmp	x2, #0x0
   18b98:	cneg	x1, x2, mi  // mi = first
   18b9c:	lsl	x10, x1, #6
   18ba0:	add	x9, x8, x1, lsl #3
   18ba4:	ldur	x9, [x9, #-8]
   18ba8:	clz	x9, x9
   18bac:	sub	x9, x10, x9
   18bb0:	str	x9, [x0]
   18bb4:	neg	x3, x9
   18bb8:	mov	x0, x8
   18bbc:	bl	c070 <__gmpn_get_d@plt>
   18bc0:	ldp	x29, x30, [sp], #16
   18bc4:	ret
   18bc8:	str	xzr, [x0]
   18bcc:	fmov	d0, xzr
   18bd0:	ldp	x29, x30, [sp], #16
   18bd4:	ret

0000000000018bd8 <__gmpz_get_si@@Base>:
   18bd8:	ldr	x8, [x0, #8]
   18bdc:	ldr	w9, [x0, #4]
   18be0:	ldr	x8, [x8]
   18be4:	cmp	w9, #0x1
   18be8:	b.lt	18bf4 <__gmpz_get_si@@Base+0x1c>  // b.tstop
   18bec:	and	x0, x8, #0x7fffffffffffffff
   18bf0:	ret
   18bf4:	tbnz	w9, #31, 18c00 <__gmpz_get_si@@Base+0x28>
   18bf8:	mov	x0, xzr
   18bfc:	ret
   18c00:	sub	x8, x8, #0x1
   18c04:	orr	x8, x8, #0x8000000000000000
   18c08:	eor	x0, x8, #0x7fffffffffffffff
   18c0c:	ret

0000000000018c10 <__gmpz_get_str@@Base>:
   18c10:	stp	x29, x30, [sp, #-80]!
   18c14:	stp	x26, x25, [sp, #16]
   18c18:	stp	x24, x23, [sp, #32]
   18c1c:	stp	x22, x21, [sp, #48]
   18c20:	stp	x20, x19, [sp, #64]
   18c24:	mov	x29, sp
   18c28:	sub	sp, sp, #0x10
   18c2c:	ldrsw	x20, [x2, #4]
   18c30:	mov	x22, x2
   18c34:	mov	w21, w1
   18c38:	cmp	w1, #0x2
   18c3c:	mov	x19, x0
   18c40:	b.lt	18cb0 <__gmpz_get_str@@Base+0xa0>  // b.tstop
   18c44:	cmp	w21, #0x25
   18c48:	b.ge	18cc0 <__gmpz_get_str@@Base+0xb0>  // b.tcont
   18c4c:	adrp	x26, 4c000 <__gmp_randclear_mt@@Base+0x18>
   18c50:	add	x26, x26, #0x79d
   18c54:	cbnz	x19, 18ce4 <__gmpz_get_str@@Base+0xd4>
   18c58:	cmp	x20, #0x0
   18c5c:	cneg	x8, x20, mi  // mi = first
   18c60:	cbz	x8, 18de8 <__gmpz_get_str@@Base+0x1d8>
   18c64:	ldr	x9, [x22, #8]
   18c68:	adrp	x11, 69000 <__gmp_limbroots_table@@Base+0x11338>
   18c6c:	sub	w10, w21, #0x1
   18c70:	tst	w21, w10
   18c74:	add	x9, x9, x8, lsl #3
   18c78:	ldur	x9, [x9, #-8]
   18c7c:	ldr	x11, [x11, #3936]
   18c80:	lsl	x8, x8, #6
   18c84:	mov	w10, #0x28                  	// #40
   18c88:	clz	x9, x9
   18c8c:	sub	x8, x8, x9
   18c90:	sxtw	x9, w21
   18c94:	madd	x9, x9, x10, x11
   18c98:	b.ne	18df0 <__gmpz_get_str@@Base+0x1e0>  // b.any
   18c9c:	ldrsw	x9, [x9, #24]
   18ca0:	add	x8, x8, x9
   18ca4:	sub	x8, x8, #0x1
   18ca8:	udiv	x8, x8, x9
   18cac:	b	18e00 <__gmpz_get_str@@Base+0x1f0>
   18cb0:	cmn	w21, #0x2
   18cb4:	b.le	18ccc <__gmpz_get_str@@Base+0xbc>
   18cb8:	mov	w21, #0xa                   	// #10
   18cbc:	b	18cd8 <__gmpz_get_str@@Base+0xc8>
   18cc0:	cmp	w21, #0x3e
   18cc4:	b.le	18cd8 <__gmpz_get_str@@Base+0xc8>
   18cc8:	b	18e3c <__gmpz_get_str@@Base+0x22c>
   18ccc:	cmn	w21, #0x24
   18cd0:	b.lt	18e3c <__gmpz_get_str@@Base+0x22c>  // b.tstop
   18cd4:	neg	w21, w21
   18cd8:	adrp	x26, 4c000 <__gmp_randclear_mt@@Base+0x18>
   18cdc:	add	x26, x26, #0x75e
   18ce0:	cbz	x19, 18c58 <__gmpz_get_str@@Base+0x48>
   18ce4:	mov	x23, xzr
   18ce8:	mov	x24, x19
   18cec:	tbz	w20, #31, 18d00 <__gmpz_get_str@@Base+0xf0>
   18cf0:	mov	w8, #0x2d                  	// #45
   18cf4:	mov	x24, x19
   18cf8:	strb	w8, [x24], #1
   18cfc:	neg	x20, x20
   18d00:	sub	w8, w21, #0x1
   18d04:	tst	w21, w8
   18d08:	stur	xzr, [x29, #-8]
   18d0c:	b.ne	18d18 <__gmpz_get_str@@Base+0x108>  // b.any
   18d10:	ldr	x25, [x22, #8]
   18d14:	b	18d50 <__gmpz_get_str@@Base+0x140>
   18d18:	lsl	x8, x20, #3
   18d1c:	orr	x1, x8, #0x8
   18d20:	mov	w8, #0x7f00                	// #32512
   18d24:	cmp	x1, x8
   18d28:	b.hi	18e44 <__gmpz_get_str@@Base+0x234>  // b.pmore
   18d2c:	add	x9, x1, #0xf
   18d30:	mov	x8, sp
   18d34:	and	x9, x9, #0xfffffffffffffff0
   18d38:	sub	x25, x8, x9
   18d3c:	mov	sp, x25
   18d40:	ldr	x1, [x22, #8]
   18d44:	mov	x0, x25
   18d48:	mov	x2, x20
   18d4c:	bl	cc10 <__gmpn_copyi@plt>
   18d50:	mov	x0, x24
   18d54:	mov	w1, w21
   18d58:	mov	x2, x25
   18d5c:	mov	x3, x20
   18d60:	bl	cc50 <__gmpn_get_str@plt>
   18d64:	mov	x20, x0
   18d68:	cbz	x0, 18d88 <__gmpz_get_str@@Base+0x178>
   18d6c:	mov	x8, x20
   18d70:	mov	x9, x24
   18d74:	ldrb	w10, [x9]
   18d78:	subs	x8, x8, #0x1
   18d7c:	ldrb	w10, [x26, x10]
   18d80:	strb	w10, [x9], #1
   18d84:	b.ne	18d74 <__gmpz_get_str@@Base+0x164>  // b.any
   18d88:	strb	wzr, [x24, x20]
   18d8c:	ldur	x0, [x29, #-8]
   18d90:	cbnz	x0, 18e30 <__gmpz_get_str@@Base+0x220>
   18d94:	cbz	x23, 18dc8 <__gmpz_get_str@@Base+0x1b8>
   18d98:	sub	x8, x24, x19
   18d9c:	add	x8, x8, x20
   18da0:	add	x2, x8, #0x1
   18da4:	cmp	x23, x2
   18da8:	b.eq	18dc8 <__gmpz_get_str@@Base+0x1b8>  // b.none
   18dac:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   18db0:	ldr	x8, [x8, #3792]
   18db4:	mov	x0, x19
   18db8:	mov	x1, x23
   18dbc:	ldr	x8, [x8]
   18dc0:	blr	x8
   18dc4:	mov	x19, x0
   18dc8:	mov	x0, x19
   18dcc:	mov	sp, x29
   18dd0:	ldp	x20, x19, [sp, #64]
   18dd4:	ldp	x22, x21, [sp, #48]
   18dd8:	ldp	x24, x23, [sp, #32]
   18ddc:	ldp	x26, x25, [sp, #16]
   18de0:	ldp	x29, x30, [sp], #80
   18de4:	ret
   18de8:	mov	w8, #0x1                   	// #1
   18dec:	b	18e00 <__gmpz_get_str@@Base+0x1f0>
   18df0:	ldr	x9, [x9, #8]
   18df4:	add	x9, x9, #0x1
   18df8:	umulh	x8, x9, x8
   18dfc:	add	x8, x8, #0x1
   18e00:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   18e04:	ldr	x9, [x9, #3840]
   18e08:	ubfx	x10, x20, #31, #1
   18e0c:	add	w10, w10, #0x1
   18e10:	add	x23, x8, x10
   18e14:	ldr	x9, [x9]
   18e18:	mov	x0, x23
   18e1c:	blr	x9
   18e20:	mov	x19, x0
   18e24:	mov	x24, x19
   18e28:	tbz	w20, #31, 18d00 <__gmpz_get_str@@Base+0xf0>
   18e2c:	b	18cf0 <__gmpz_get_str@@Base+0xe0>
   18e30:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   18e34:	cbnz	x23, 18d98 <__gmpz_get_str@@Base+0x188>
   18e38:	b	18dc8 <__gmpz_get_str@@Base+0x1b8>
   18e3c:	mov	x19, xzr
   18e40:	b	18dc8 <__gmpz_get_str@@Base+0x1b8>
   18e44:	sub	x0, x29, #0x8
   18e48:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   18e4c:	mov	x25, x0
   18e50:	b	18d40 <__gmpz_get_str@@Base+0x130>

0000000000018e54 <__gmpz_get_ui@@Base>:
   18e54:	ldr	x8, [x0, #8]
   18e58:	ldr	w9, [x0, #4]
   18e5c:	ldr	x8, [x8]
   18e60:	cmp	w9, #0x0
   18e64:	csel	x0, xzr, x8, eq  // eq = none
   18e68:	ret

0000000000018e6c <__gmpz_getlimbn@@Base>:
   18e6c:	tbnz	x1, #63, 18e90 <__gmpz_getlimbn@@Base+0x24>
   18e70:	ldr	w8, [x0, #4]
   18e74:	cmp	w8, #0x0
   18e78:	cneg	w8, w8, mi  // mi = first
   18e7c:	cmp	x8, x1
   18e80:	b.le	18e90 <__gmpz_getlimbn@@Base+0x24>
   18e84:	ldr	x8, [x0, #8]
   18e88:	ldr	x0, [x8, x1, lsl #3]
   18e8c:	ret
   18e90:	mov	x0, xzr
   18e94:	ret

0000000000018e98 <__gmpz_hamdist@@Base>:
   18e98:	stp	x29, x30, [sp, #-80]!
   18e9c:	stp	x24, x23, [sp, #32]
   18ea0:	stp	x22, x21, [sp, #48]
   18ea4:	stp	x20, x19, [sp, #64]
   18ea8:	ldrsw	x12, [x0, #4]
   18eac:	ldrsw	x9, [x1, #4]
   18eb0:	ldr	x8, [x0, #8]
   18eb4:	ldr	x10, [x1, #8]
   18eb8:	str	x25, [sp, #16]
   18ebc:	mov	x29, sp
   18ec0:	tbnz	w12, #31, 18f08 <__gmpz_hamdist@@Base+0x70>
   18ec4:	tbnz	w9, #31, 18f0c <__gmpz_hamdist@@Base+0x74>
   18ec8:	cmp	w12, w9
   18ecc:	csel	w11, w12, w9, lt  // lt = tstop
   18ed0:	csel	w13, w12, w9, gt
   18ed4:	sxtw	x19, w11
   18ed8:	sxtw	x21, w13
   18edc:	csel	x20, x10, x8, lt  // lt = tstop
   18ee0:	cbz	w11, 18f14 <__gmpz_hamdist@@Base+0x7c>
   18ee4:	cmp	w12, w9
   18ee8:	csel	x1, x8, x10, lt  // lt = tstop
   18eec:	mov	x0, x20
   18ef0:	mov	x2, x19
   18ef4:	bl	d010 <__gmpn_hamdist@plt>
   18ef8:	mov	x22, x0
   18efc:	subs	x1, x21, x19
   18f00:	b.ne	18f20 <__gmpz_hamdist@@Base+0x88>  // b.any
   18f04:	b	190c0 <__gmpz_hamdist@@Base+0x228>
   18f08:	tbnz	w9, #31, 18f28 <__gmpz_hamdist@@Base+0x90>
   18f0c:	mov	x22, #0xffffffffffffffff    	// #-1
   18f10:	b	190c0 <__gmpz_hamdist@@Base+0x228>
   18f14:	mov	x22, xzr
   18f18:	subs	x1, x21, x19
   18f1c:	b.eq	190c0 <__gmpz_hamdist@@Base+0x228>  // b.none
   18f20:	add	x0, x20, x19, lsl #3
   18f24:	b	190b8 <__gmpz_hamdist@@Base+0x220>
   18f28:	mov	x11, xzr
   18f2c:	neg	x24, x12
   18f30:	neg	x19, x9
   18f34:	ldr	x12, [x8, x11]
   18f38:	ldr	x9, [x10, x11]
   18f3c:	sub	x24, x24, #0x1
   18f40:	sub	x19, x19, #0x1
   18f44:	cbnz	x12, 18f64 <__gmpz_hamdist@@Base+0xcc>
   18f48:	add	x11, x11, #0x8
   18f4c:	cbz	x9, 18f34 <__gmpz_hamdist@@Base+0x9c>
   18f50:	add	x20, x10, x11
   18f54:	add	x21, x8, x11
   18f58:	mov	x12, x9
   18f5c:	mov	x9, xzr
   18f60:	b	18f80 <__gmpz_hamdist@@Base+0xe8>
   18f64:	mov	x0, x24
   18f68:	add	x10, x10, x11
   18f6c:	add	x8, x8, x11
   18f70:	add	x21, x10, #0x8
   18f74:	add	x20, x8, #0x8
   18f78:	mov	x24, x19
   18f7c:	mov	x19, x0
   18f80:	neg	x8, x12
   18f84:	neg	x10, x9
   18f88:	eor	x8, x10, x8
   18f8c:	lsr	x10, x8, #1
   18f90:	and	x10, x10, #0x5555555555555555
   18f94:	sub	x8, x8, x10
   18f98:	lsr	x10, x8, #2
   18f9c:	and	x10, x10, #0x3333333333333333
   18fa0:	and	x8, x8, #0x3333333333333333
   18fa4:	add	x8, x10, x8
   18fa8:	add	x8, x8, x8, lsr #4
   18fac:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   18fb0:	add	x8, x8, x8, lsr #8
   18fb4:	add	x8, x8, x8, lsr #16
   18fb8:	lsr	x10, x8, #32
   18fbc:	add	w8, w10, w8
   18fc0:	and	x22, x8, #0xff
   18fc4:	cbnz	x9, 1906c <__gmpz_hamdist@@Base+0x1d4>
   18fc8:	mov	w8, #0x40                  	// #64
   18fcc:	sub	x25, x8, x22
   18fd0:	mov	w8, #0x1                   	// #1
   18fd4:	ldr	x23, [x21], #8
   18fd8:	sub	x25, x25, #0x40
   18fdc:	sub	x8, x8, #0x1
   18fe0:	cbz	x23, 18fd4 <__gmpz_hamdist@@Base+0x13c>
   18fe4:	neg	x9, x8
   18fe8:	cmp	x9, x19
   18fec:	csneg	x22, x19, x8, ge  // ge = tcont
   18ff0:	add	x24, x24, x8
   18ff4:	cbz	x22, 19018 <__gmpz_hamdist@@Base+0x180>
   18ff8:	mov	x0, x20
   18ffc:	mov	x1, x22
   19000:	bl	cf50 <__gmpn_popcount@plt>
   19004:	add	x8, x0, x25
   19008:	sub	x19, x19, x22
   1900c:	neg	x8, x8
   19010:	add	x20, x20, x22, lsl #3
   19014:	b	1901c <__gmpz_hamdist@@Base+0x184>
   19018:	neg	x8, x25
   1901c:	sub	x24, x24, #0x1
   19020:	sub	x9, x23, #0x1
   19024:	cbz	x19, 19034 <__gmpz_hamdist@@Base+0x19c>
   19028:	ldr	x10, [x20], #8
   1902c:	sub	x19, x19, #0x1
   19030:	eor	x9, x10, x9
   19034:	lsr	x10, x9, #1
   19038:	and	x10, x10, #0x5555555555555555
   1903c:	sub	x9, x9, x10
   19040:	lsr	x10, x9, #2
   19044:	and	x10, x10, #0x3333333333333333
   19048:	and	x9, x9, #0x3333333333333333
   1904c:	add	x9, x10, x9
   19050:	add	x9, x9, x9, lsr #4
   19054:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   19058:	add	x9, x9, x9, lsr #8
   1905c:	add	x9, x9, x9, lsr #16
   19060:	lsr	x10, x9, #32
   19064:	add	w9, w10, w9
   19068:	add	x22, x8, w9, uxtb
   1906c:	cmp	x19, x24
   19070:	csel	x23, x19, x24, lt  // lt = tstop
   19074:	cbz	x23, 190a0 <__gmpz_hamdist@@Base+0x208>
   19078:	mov	x0, x20
   1907c:	mov	x1, x21
   19080:	mov	x2, x23
   19084:	bl	d010 <__gmpn_hamdist@plt>
   19088:	lsl	x8, x23, #3
   1908c:	add	x22, x0, x22
   19090:	sub	x19, x19, x23
   19094:	sub	x24, x24, x23
   19098:	add	x20, x20, x8
   1909c:	add	x21, x21, x8
   190a0:	cbnz	x19, 190b0 <__gmpz_hamdist@@Base+0x218>
   190a4:	mov	x19, x24
   190a8:	mov	x20, x21
   190ac:	cbz	x24, 190c0 <__gmpz_hamdist@@Base+0x228>
   190b0:	mov	x0, x20
   190b4:	mov	x1, x19
   190b8:	bl	cf50 <__gmpn_popcount@plt>
   190bc:	add	x22, x0, x22
   190c0:	mov	x0, x22
   190c4:	ldp	x20, x19, [sp, #64]
   190c8:	ldp	x22, x21, [sp, #48]
   190cc:	ldp	x24, x23, [sp, #32]
   190d0:	ldr	x25, [sp, #16]
   190d4:	ldp	x29, x30, [sp], #80
   190d8:	ret

00000000000190dc <__gmpz_import@@Base>:
   190dc:	stp	x29, x30, [sp, #-96]!
   190e0:	stp	x26, x25, [sp, #32]
   190e4:	stp	x24, x23, [sp, #48]
   190e8:	stp	x22, x21, [sp, #64]
   190ec:	stp	x20, x19, [sp, #80]
   190f0:	lsl	x8, x3, #3
   190f4:	ldrsw	x9, [x0]
   190f8:	str	x27, [sp, #16]
   190fc:	sub	x27, x8, x5
   19100:	orr	x8, xzr, #0x3f
   19104:	madd	x8, x27, x1, x8
   19108:	lsr	x20, x8, #6
   1910c:	mov	x22, x6
   19110:	mov	x25, x5
   19114:	mov	w26, w4
   19118:	mov	x23, x3
   1911c:	mov	x21, x1
   19120:	mov	x19, x0
   19124:	cmp	x20, x9
   19128:	mov	w24, w2
   1912c:	mov	x29, sp
   19130:	b.gt	19384 <__gmpz_import@@Base+0x2a8>
   19134:	ldr	x0, [x19, #8]
   19138:	cmp	w26, #0x0
   1913c:	csinv	w15, w26, wzr, ne  // ne = any
   19140:	cbz	x25, 19240 <__gmpz_import@@Base+0x164>
   19144:	add	x8, x27, #0x7
   19148:	cmp	w15, #0x0
   1914c:	lsr	x8, x8, #3
   19150:	cneg	x9, x8, lt  // lt = tstop
   19154:	cmp	w24, #0x0
   19158:	cneg	x10, x23, ge  // ge = tcont
   1915c:	cbz	x21, 1927c <__gmpz_import@@Base+0x1a0>
   19160:	sub	x13, x21, #0x1
   19164:	mov	x8, xzr
   19168:	cmp	w24, #0x0
   1916c:	mul	x13, x13, x23
   19170:	csel	x13, x13, x8, ge  // ge = tcont
   19174:	and	w11, w27, #0x7
   19178:	add	x14, x22, x13
   1917c:	mov	x13, #0xffffffffffffffff    	// #-1
   19180:	sub	x16, x23, #0x1
   19184:	cmp	w15, #0x0
   19188:	lsl	x13, x13, x11
   1918c:	csel	x16, x16, x8, ge  // ge = tcont
   19190:	mov	w12, wzr
   19194:	add	x9, x9, x10
   19198:	lsr	x10, x27, #3
   1919c:	mvn	x13, x13
   191a0:	add	x14, x14, x16
   191a4:	sub	x15, x8, w15, sxtw
   191a8:	mov	x16, x8
   191ac:	b	191c0 <__gmpz_import@@Base+0xe4>
   191b0:	add	x8, x8, #0x1
   191b4:	cmp	x8, x21
   191b8:	add	x14, x14, x9
   191bc:	b.eq	19284 <__gmpz_import@@Base+0x1a8>  // b.none
   191c0:	cbz	x10, 19208 <__gmpz_import@@Base+0x12c>
   191c4:	mov	x17, x10
   191c8:	b	191e4 <__gmpz_import@@Base+0x108>
   191cc:	neg	w12, w12
   191d0:	str	x16, [x0], #8
   191d4:	lsr	x16, x18, x12
   191d8:	mov	w12, w1
   191dc:	subs	x17, x17, #0x1
   191e0:	b.eq	19208 <__gmpz_import@@Base+0x12c>  // b.none
   191e4:	ldrb	w18, [x14]
   191e8:	add	x14, x14, x15
   191ec:	subs	w1, w12, #0x38
   191f0:	lsl	x2, x18, x12
   191f4:	orr	x16, x2, x16
   191f8:	b.ge	191cc <__gmpz_import@@Base+0xf0>  // b.tcont
   191fc:	add	w12, w12, #0x8
   19200:	subs	x17, x17, #0x1
   19204:	b.ne	191e4 <__gmpz_import@@Base+0x108>  // b.any
   19208:	cbz	w11, 191b0 <__gmpz_import@@Base+0xd4>
   1920c:	ldrb	w17, [x14]
   19210:	add	x14, x14, x15
   19214:	and	x17, x17, x13
   19218:	lsl	x1, x17, x12
   1921c:	add	w12, w12, w11
   19220:	subs	w18, w12, #0x40
   19224:	orr	x16, x1, x16
   19228:	b.lt	191b0 <__gmpz_import@@Base+0xd4>  // b.tstop
   1922c:	sub	w12, w11, w18
   19230:	str	x16, [x0], #8
   19234:	lsr	x16, x17, x12
   19238:	mov	w12, w18
   1923c:	b	191b0 <__gmpz_import@@Base+0xd4>
   19240:	cmn	w24, #0x1
   19244:	cset	w8, eq  // eq = none
   19248:	cmp	x23, #0x8
   1924c:	cset	w9, eq  // eq = none
   19250:	and	w9, w8, w9
   19254:	cmp	w9, #0x1
   19258:	and	x8, x22, #0x7
   1925c:	b.ne	192d4 <__gmpz_import@@Base+0x1f8>  // b.any
   19260:	cmn	w15, #0x1
   19264:	b.ne	192d4 <__gmpz_import@@Base+0x1f8>  // b.any
   19268:	cbnz	x8, 192d4 <__gmpz_import@@Base+0x1f8>
   1926c:	mov	x1, x22
   19270:	mov	x2, x21
   19274:	bl	cc10 <__gmpn_copyi@plt>
   19278:	b	1928c <__gmpz_import@@Base+0x1b0>
   1927c:	mov	x16, xzr
   19280:	mov	w12, wzr
   19284:	cbz	w12, 1928c <__gmpz_import@@Base+0x1b0>
   19288:	str	x16, [x0]
   1928c:	ldr	x8, [x19, #8]
   19290:	sub	x8, x8, #0x8
   19294:	subs	x9, x20, #0x1
   19298:	b.lt	192b0 <__gmpz_import@@Base+0x1d4>  // b.tstop
   1929c:	ldr	x10, [x8, x20, lsl #3]
   192a0:	mov	x20, x9
   192a4:	cbz	x10, 19294 <__gmpz_import@@Base+0x1b8>
   192a8:	add	x8, x9, #0x1
   192ac:	b	192b4 <__gmpz_import@@Base+0x1d8>
   192b0:	mov	x8, xzr
   192b4:	str	w8, [x19, #4]
   192b8:	ldp	x20, x19, [sp, #80]
   192bc:	ldp	x22, x21, [sp, #64]
   192c0:	ldp	x24, x23, [sp, #48]
   192c4:	ldp	x26, x25, [sp, #32]
   192c8:	ldr	x27, [sp, #16]
   192cc:	ldp	x29, x30, [sp], #96
   192d0:	ret
   192d4:	cmp	w15, #0x1
   192d8:	cset	w10, ne  // ne = any
   192dc:	eor	w9, w9, #0x1
   192e0:	orr	w9, w9, w10
   192e4:	tbnz	w9, #0, 19348 <__gmpz_import@@Base+0x26c>
   192e8:	cbnz	x8, 19348 <__gmpz_import@@Base+0x26c>
   192ec:	cmp	x21, #0x1
   192f0:	b.lt	1928c <__gmpz_import@@Base+0x1b0>  // b.tstop
   192f4:	ldr	x8, [x22], #8
   192f8:	subs	x21, x21, #0x1
   192fc:	lsl	x11, x8, #40
   19300:	and	x11, x11, #0xff000000000000
   19304:	lsr	x10, x8, #16
   19308:	bfi	x11, x8, #56, #8
   1930c:	lsr	x9, x8, #24
   19310:	bfi	x11, x10, #40, #8
   19314:	lsr	x10, x8, #8
   19318:	and	x10, x10, #0xff000000
   1931c:	bfi	x11, x9, #32, #8
   19320:	orr	x10, x11, x10
   19324:	lsr	x11, x8, #40
   19328:	and	x9, x9, #0xff0000
   1932c:	and	x11, x11, #0xff00
   19330:	orr	x9, x10, x9
   19334:	orr	x9, x9, x11
   19338:	add	x8, x9, x8, lsr #56
   1933c:	str	x8, [x0], #8
   19340:	b.ne	192f4 <__gmpz_import@@Base+0x218>  // b.any
   19344:	b	1928c <__gmpz_import@@Base+0x1b0>
   19348:	cmp	w24, #0x1
   1934c:	b.ne	19144 <__gmpz_import@@Base+0x68>  // b.any
   19350:	cmp	x23, #0x8
   19354:	b.ne	19144 <__gmpz_import@@Base+0x68>  // b.any
   19358:	cmn	w15, #0x1
   1935c:	b.ne	19144 <__gmpz_import@@Base+0x68>  // b.any
   19360:	cbnz	x8, 19144 <__gmpz_import@@Base+0x68>
   19364:	cmp	x21, #0x1
   19368:	b.lt	1928c <__gmpz_import@@Base+0x1b0>  // b.tstop
   1936c:	sub	x8, x22, #0x8
   19370:	ldr	x9, [x8, x21, lsl #3]
   19374:	subs	x21, x21, #0x1
   19378:	str	x9, [x0], #8
   1937c:	b.ne	19370 <__gmpz_import@@Base+0x294>  // b.any
   19380:	b	1928c <__gmpz_import@@Base+0x1b0>
   19384:	mov	x0, x19
   19388:	mov	x1, x20
   1938c:	bl	c1c0 <__gmpz_realloc@plt>
   19390:	b	19138 <__gmpz_import@@Base+0x5c>

0000000000019394 <__gmpz_init@@Base>:
   19394:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   19398:	add	x8, x8, #0xc10
   1939c:	stp	xzr, x8, [x0]
   193a0:	ret

00000000000193a4 <__gmpz_init2@@Base>:
   193a4:	stp	x29, x30, [sp, #-32]!
   193a8:	cmp	x1, #0x0
   193ac:	cset	w8, ne  // ne = any
   193b0:	sub	x8, x1, x8
   193b4:	mov	x9, #0x1fffffffc0          	// #137438953408
   193b8:	cmp	x8, x9
   193bc:	stp	x20, x19, [sp, #16]
   193c0:	mov	x29, sp
   193c4:	b.cs	193fc <__gmpz_init2@@Base+0x58>  // b.hs, b.nlast
   193c8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   193cc:	ldr	x9, [x9, #3840]
   193d0:	lsr	x8, x8, #6
   193d4:	add	x20, x8, #0x1
   193d8:	mov	x19, x0
   193dc:	ldr	x9, [x9]
   193e0:	lsl	x0, x20, #3
   193e4:	blr	x9
   193e8:	str	x0, [x19, #8]
   193ec:	stp	w20, wzr, [x19]
   193f0:	ldp	x20, x19, [sp, #16]
   193f4:	ldp	x29, x30, [sp], #32
   193f8:	ret
   193fc:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19400:	ldr	x8, [x8, #3824]
   19404:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
   19408:	add	x0, x0, #0xc18
   1940c:	mov	w1, #0x1a                  	// #26
   19410:	ldr	x3, [x8]
   19414:	mov	w2, #0x1                   	// #1
   19418:	bl	d000 <fwrite@plt>
   1941c:	bl	cab0 <abort@plt>

0000000000019420 <__gmpz_inits@@Base>:
   19420:	sub	sp, sp, #0xe0
   19424:	mov	x8, #0xffffffffffffffc8    	// #-56
   19428:	mov	x9, sp
   1942c:	movk	x8, #0xff80, lsl #32
   19430:	add	x9, x9, #0x80
   19434:	add	x10, sp, #0x88
   19438:	stp	x9, x8, [sp, #208]
   1943c:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   19440:	add	x11, sp, #0xe0
   19444:	add	x10, x10, #0x38
   19448:	add	x8, x8, #0xc38
   1944c:	stp	x1, x2, [sp, #136]
   19450:	stp	x3, x4, [sp, #152]
   19454:	stp	x5, x6, [sp, #168]
   19458:	stp	q0, q1, [sp]
   1945c:	stp	q2, q3, [sp, #32]
   19460:	stp	q4, q5, [sp, #64]
   19464:	stp	q6, q7, [sp, #96]
   19468:	str	x10, [sp, #200]
   1946c:	stp	x7, x11, [sp, #184]
   19470:	b	19488 <__gmpz_inits@@Base+0x68>
   19474:	ldr	x9, [sp, #192]
   19478:	add	x10, x9, #0x8
   1947c:	str	x10, [sp, #192]
   19480:	ldr	x0, [x9]
   19484:	cbz	x0, 194b4 <__gmpz_inits@@Base+0x94>
   19488:	stp	xzr, x8, [x0]
   1948c:	ldrsw	x9, [sp, #216]
   19490:	tbz	w9, #31, 19474 <__gmpz_inits@@Base+0x54>
   19494:	add	w10, w9, #0x8
   19498:	cmp	w10, #0x0
   1949c:	str	w10, [sp, #216]
   194a0:	b.gt	19474 <__gmpz_inits@@Base+0x54>
   194a4:	ldr	x10, [sp, #200]
   194a8:	add	x9, x10, x9
   194ac:	ldr	x0, [x9]
   194b0:	cbnz	x0, 19488 <__gmpz_inits@@Base+0x68>
   194b4:	add	sp, sp, #0xe0
   194b8:	ret

00000000000194bc <__gmpz_inp_raw@@Base>:
   194bc:	sub	sp, sp, #0x50
   194c0:	stp	x29, x30, [sp, #16]
   194c4:	stp	x24, x23, [sp, #32]
   194c8:	stp	x22, x21, [sp, #48]
   194cc:	stp	x20, x19, [sp, #64]
   194d0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   194d4:	ldr	x8, [x8, #3888]
   194d8:	cmp	x1, #0x0
   194dc:	add	x29, sp, #0x10
   194e0:	mov	x19, x0
   194e4:	ldr	x8, [x8]
   194e8:	sub	x0, x29, #0x4
   194ec:	mov	w2, #0x1                   	// #1
   194f0:	csel	x23, x8, x1, eq  // eq = none
   194f4:	mov	w1, #0x4                   	// #4
   194f8:	mov	x3, x23
   194fc:	bl	cd50 <fread@plt>
   19500:	cmp	x0, #0x1
   19504:	b.ne	19644 <__gmpz_inp_raw@@Base+0x188>  // b.any
   19508:	ldur	w8, [x29, #-4]
   1950c:	lsl	x8, x8, #32
   19510:	rev	x8, x8
   19514:	lsr	x10, x8, #31
   19518:	orr	x9, x8, #0xffffffff00000000
   1951c:	cmp	x10, #0x0
   19520:	csel	x24, x8, x9, eq  // eq = none
   19524:	cmp	x24, #0x0
   19528:	cneg	x20, x24, mi  // mi = first
   1952c:	lsl	x8, x20, #3
   19530:	add	x8, x8, #0x3f
   19534:	lsr	x21, x8, #6
   19538:	cbz	x21, 1964c <__gmpz_inp_raw@@Base+0x190>
   1953c:	ldrsw	x8, [x19]
   19540:	cmp	x21, x8
   19544:	b.gt	1967c <__gmpz_inp_raw@@Base+0x1c0>
   19548:	ldr	x22, [x19, #8]
   1954c:	add	x8, x22, x21, lsl #3
   19550:	sub	x0, x8, x20
   19554:	mov	w2, #0x1                   	// #1
   19558:	mov	x1, x20
   1955c:	mov	x3, x23
   19560:	str	xzr, [x22]
   19564:	bl	cd50 <fread@plt>
   19568:	cmp	x0, #0x1
   1956c:	mov	x0, xzr
   19570:	b.ne	19664 <__gmpz_inp_raw@@Base+0x1a8>  // b.any
   19574:	add	x8, x21, #0x1
   19578:	lsr	x8, x8, #1
   1957c:	cbz	x8, 19624 <__gmpz_inp_raw@@Base+0x168>
   19580:	add	x9, x22, x21, lsl #3
   19584:	sub	x9, x9, #0x8
   19588:	mov	x10, x22
   1958c:	ldr	x11, [x9]
   19590:	ldr	x12, [x10]
   19594:	subs	x8, x8, #0x1
   19598:	lsl	x15, x11, #40
   1959c:	and	x15, x15, #0xff000000000000
   195a0:	lsr	x14, x11, #16
   195a4:	bfi	x15, x11, #56, #8
   195a8:	bfi	x15, x14, #40, #8
   195ac:	lsl	x14, x12, #40
   195b0:	lsr	x13, x11, #24
   195b4:	lsr	x16, x11, #8
   195b8:	and	x14, x14, #0xff000000000000
   195bc:	lsr	x17, x12, #16
   195c0:	bfi	x14, x12, #56, #8
   195c4:	and	x16, x16, #0xff000000
   195c8:	bfi	x15, x13, #32, #8
   195cc:	bfi	x14, x17, #40, #8
   195d0:	lsr	x17, x12, #24
   195d4:	orr	x15, x15, x16
   195d8:	lsr	x16, x12, #8
   195dc:	and	x16, x16, #0xff000000
   195e0:	bfi	x14, x17, #32, #8
   195e4:	and	x13, x13, #0xff0000
   195e8:	orr	x14, x14, x16
   195ec:	orr	x13, x15, x13
   195f0:	and	x15, x17, #0xff0000
   195f4:	orr	x14, x14, x15
   195f8:	lsr	x15, x11, #40
   195fc:	and	x15, x15, #0xff00
   19600:	orr	x13, x13, x15
   19604:	lsr	x15, x12, #40
   19608:	and	x15, x15, #0xff00
   1960c:	orr	x14, x14, x15
   19610:	add	x11, x13, x11, lsr #56
   19614:	add	x12, x14, x12, lsr #56
   19618:	str	x11, [x10], #8
   1961c:	str	x12, [x9], #-8
   19620:	b.ne	1958c <__gmpz_inp_raw@@Base+0xd0>  // b.any
   19624:	sub	x8, x22, #0x8
   19628:	subs	x9, x21, #0x1
   1962c:	b.lt	1964c <__gmpz_inp_raw@@Base+0x190>  // b.tstop
   19630:	ldr	x10, [x8, x21, lsl #3]
   19634:	mov	x21, x9
   19638:	cbz	x10, 19628 <__gmpz_inp_raw@@Base+0x16c>
   1963c:	add	x8, x9, #0x1
   19640:	b	19650 <__gmpz_inp_raw@@Base+0x194>
   19644:	mov	x0, xzr
   19648:	b	19664 <__gmpz_inp_raw@@Base+0x1a8>
   1964c:	mov	x8, xzr
   19650:	neg	w9, w8
   19654:	cmp	x24, #0x0
   19658:	csel	x8, x8, x9, ge  // ge = tcont
   1965c:	add	x0, x20, #0x4
   19660:	str	w8, [x19, #4]
   19664:	ldp	x20, x19, [sp, #64]
   19668:	ldp	x22, x21, [sp, #48]
   1966c:	ldp	x24, x23, [sp, #32]
   19670:	ldp	x29, x30, [sp, #16]
   19674:	add	sp, sp, #0x50
   19678:	ret
   1967c:	mov	x0, x19
   19680:	mov	x1, x21
   19684:	bl	c1c0 <__gmpz_realloc@plt>
   19688:	mov	x22, x0
   1968c:	b	1954c <__gmpz_inp_raw@@Base+0x90>

0000000000019690 <__gmpz_inp_str@@Base>:
   19690:	stp	x29, x30, [sp, #-64]!
   19694:	str	x23, [sp, #16]
   19698:	stp	x22, x21, [sp, #32]
   1969c:	stp	x20, x19, [sp, #48]
   196a0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   196a4:	ldr	x8, [x8, #3888]
   196a8:	cmp	x1, #0x0
   196ac:	mov	w19, w2
   196b0:	mov	x21, x0
   196b4:	ldr	x8, [x8]
   196b8:	mov	x20, xzr
   196bc:	mov	x29, sp
   196c0:	csel	x22, x8, x1, eq  // eq = none
   196c4:	mov	x0, x22
   196c8:	bl	c9a0 <getc@plt>
   196cc:	mov	w23, w0
   196d0:	add	x20, x20, #0x1
   196d4:	bl	cca0 <__ctype_b_loc@plt>
   196d8:	ldr	x8, [x0]
   196dc:	ldrh	w8, [x8, w23, sxtw #1]
   196e0:	tbnz	w8, #13, 196c4 <__gmpz_inp_str@@Base+0x34>
   196e4:	mov	x0, x21
   196e8:	mov	x1, x22
   196ec:	mov	w2, w19
   196f0:	mov	w3, w23
   196f4:	mov	x4, x20
   196f8:	bl	cb90 <__gmpz_inp_str_nowhite@plt>
   196fc:	ldp	x20, x19, [sp, #48]
   19700:	ldp	x22, x21, [sp, #32]
   19704:	ldr	x23, [sp, #16]
   19708:	ldp	x29, x30, [sp], #64
   1970c:	ret

0000000000019710 <__gmpz_inp_str_nowhite@@Base>:
   19710:	sub	sp, sp, #0x70
   19714:	stp	x29, x30, [sp, #16]
   19718:	stp	x28, x27, [sp, #32]
   1971c:	stp	x26, x25, [sp, #48]
   19720:	stp	x24, x23, [sp, #64]
   19724:	stp	x22, x21, [sp, #80]
   19728:	stp	x20, x19, [sp, #96]
   1972c:	str	x0, [sp, #8]
   19730:	adrp	x28, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19734:	ldr	x28, [x28, #3920]
   19738:	mov	x20, x4
   1973c:	mov	w23, w3
   19740:	mov	w22, w2
   19744:	mov	x21, x1
   19748:	cmp	w2, #0x25
   1974c:	add	x29, sp, #0x10
   19750:	b.lt	19760 <__gmpz_inp_str_nowhite@@Base+0x50>  // b.tstop
   19754:	cmp	w22, #0x3e
   19758:	b.gt	197e4 <__gmpz_inp_str_nowhite@@Base+0xd4>
   1975c:	add	x28, x28, #0xd0
   19760:	cmp	w23, #0x2d
   19764:	b.ne	19788 <__gmpz_inp_str_nowhite@@Base+0x78>  // b.any
   19768:	mov	x0, x21
   1976c:	bl	c9a0 <getc@plt>
   19770:	mov	w23, w0
   19774:	add	x20, x20, #0x1
   19778:	mov	w19, #0x1                   	// #1
   1977c:	cmn	w23, #0x1
   19780:	b.ne	19794 <__gmpz_inp_str_nowhite@@Base+0x84>  // b.any
   19784:	b	197e4 <__gmpz_inp_str_nowhite@@Base+0xd4>
   19788:	mov	w19, wzr
   1978c:	cmn	w23, #0x1
   19790:	b.eq	197e4 <__gmpz_inp_str_nowhite@@Base+0xd4>  // b.none
   19794:	ldrb	w8, [x28, w23, sxtw]
   19798:	cmp	w22, #0x0
   1979c:	mov	w9, #0xa                   	// #10
   197a0:	csel	w9, w9, w22, eq  // eq = none
   197a4:	cmp	w9, w8
   197a8:	b.le	197e4 <__gmpz_inp_str_nowhite@@Base+0xd4>
   197ac:	cbnz	w22, 197f0 <__gmpz_inp_str_nowhite@@Base+0xe0>
   197b0:	cmp	w23, #0x30
   197b4:	b.ne	197ec <__gmpz_inp_str_nowhite@@Base+0xdc>  // b.any
   197b8:	mov	x0, x21
   197bc:	bl	c9a0 <getc@plt>
   197c0:	orr	w8, w0, #0x20
   197c4:	cmp	w8, #0x78
   197c8:	b.ne	1995c <__gmpz_inp_str_nowhite@@Base+0x24c>  // b.any
   197cc:	mov	x0, x21
   197d0:	bl	c9a0 <getc@plt>
   197d4:	mov	w23, w0
   197d8:	add	x20, x20, #0x2
   197dc:	mov	w22, #0x10                  	// #16
   197e0:	b	197f0 <__gmpz_inp_str_nowhite@@Base+0xe0>
   197e4:	mov	x20, xzr
   197e8:	b	19938 <__gmpz_inp_str_nowhite@@Base+0x228>
   197ec:	mov	w22, #0xa                   	// #10
   197f0:	cmp	w23, #0x30
   197f4:	str	w19, [sp, #4]
   197f8:	b.ne	19814 <__gmpz_inp_str_nowhite@@Base+0x104>  // b.any
   197fc:	mov	x0, x21
   19800:	bl	c9a0 <getc@plt>
   19804:	cmp	w0, #0x30
   19808:	add	x20, x20, #0x1
   1980c:	b.eq	197fc <__gmpz_inp_str_nowhite@@Base+0xec>  // b.none
   19810:	mov	w23, w0
   19814:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19818:	ldr	x8, [x8, #3840]
   1981c:	mov	w0, #0x64                  	// #100
   19820:	mov	w24, #0x64                  	// #100
   19824:	ldr	x8, [x8]
   19828:	blr	x8
   1982c:	mov	x25, x0
   19830:	mov	x27, xzr
   19834:	b	19840 <__gmpz_inp_str_nowhite@@Base+0x130>
   19838:	mov	w8, wzr
   1983c:	tbz	w8, #0, 198a8 <__gmpz_inp_str_nowhite@@Base+0x198>
   19840:	cmn	w23, #0x1
   19844:	b.eq	198a8 <__gmpz_inp_str_nowhite@@Base+0x198>  // b.none
   19848:	ldrb	w19, [x28, w23, sxtw]
   1984c:	cmp	w22, w19
   19850:	b.le	19838 <__gmpz_inp_str_nowhite@@Base+0x128>
   19854:	cmp	x27, x24
   19858:	b.cc	19888 <__gmpz_inp_str_nowhite@@Base+0x178>  // b.lo, b.ul, b.last
   1985c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19860:	ldr	x8, [x8, #3792]
   19864:	add	x9, x24, x24, lsl #1
   19868:	lsr	x23, x9, #1
   1986c:	mov	x0, x25
   19870:	ldr	x8, [x8]
   19874:	mov	x1, x24
   19878:	mov	x2, x23
   1987c:	blr	x8
   19880:	mov	x25, x0
   19884:	mov	x24, x23
   19888:	mov	x0, x21
   1988c:	add	x26, x27, #0x1
   19890:	strb	w19, [x25, x27]
   19894:	bl	c9a0 <getc@plt>
   19898:	mov	w23, w0
   1989c:	mov	w8, #0x1                   	// #1
   198a0:	mov	x27, x26
   198a4:	tbnz	w8, #0, 19840 <__gmpz_inp_str_nowhite@@Base+0x130>
   198a8:	mov	w0, w23
   198ac:	mov	x1, x21
   198b0:	bl	ce10 <ungetc@plt>
   198b4:	add	x8, x20, x27
   198b8:	sub	x20, x8, #0x1
   198bc:	cbz	x27, 19914 <__gmpz_inp_str_nowhite@@Base+0x204>
   198c0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   198c4:	ldr	x8, [x8, #3936]
   198c8:	ldr	x19, [sp, #8]
   198cc:	mov	w9, #0x28                  	// #40
   198d0:	smaddl	x8, w22, w9, x8
   198d4:	ldr	x8, [x8, #16]
   198d8:	ldrsw	x9, [x19]
   198dc:	umulh	x8, x8, x27
   198e0:	ubfx	x8, x8, #3, #58
   198e4:	add	x1, x8, #0x2
   198e8:	cmp	x1, x9
   198ec:	b.gt	1998c <__gmpz_inp_str_nowhite@@Base+0x27c>
   198f0:	ldr	x0, [x19, #8]
   198f4:	mov	x1, x25
   198f8:	mov	x2, x27
   198fc:	mov	w3, w22
   19900:	bl	c1d0 <__gmpn_set_str@plt>
   19904:	ldr	w8, [sp, #4]
   19908:	cmp	w8, #0x0
   1990c:	cneg	w8, w0, ne  // ne = any
   19910:	b	1991c <__gmpz_inp_str_nowhite@@Base+0x20c>
   19914:	ldr	x19, [sp, #8]
   19918:	mov	w8, wzr
   1991c:	str	w8, [x19, #4]
   19920:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19924:	ldr	x8, [x8, #4016]
   19928:	mov	x0, x25
   1992c:	mov	x1, x24
   19930:	ldr	x8, [x8]
   19934:	blr	x8
   19938:	mov	x0, x20
   1993c:	ldp	x20, x19, [sp, #96]
   19940:	ldp	x22, x21, [sp, #80]
   19944:	ldp	x24, x23, [sp, #64]
   19948:	ldp	x26, x25, [sp, #48]
   1994c:	ldp	x28, x27, [sp, #32]
   19950:	ldp	x29, x30, [sp, #16]
   19954:	add	sp, sp, #0x70
   19958:	ret
   1995c:	cmp	w8, #0x62
   19960:	b.ne	1997c <__gmpz_inp_str_nowhite@@Base+0x26c>  // b.any
   19964:	mov	x0, x21
   19968:	bl	c9a0 <getc@plt>
   1996c:	mov	w23, w0
   19970:	add	x20, x20, #0x2
   19974:	mov	w22, #0x2                   	// #2
   19978:	b	197f0 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1997c:	mov	w23, w0
   19980:	add	x20, x20, #0x1
   19984:	mov	w22, #0x8                   	// #8
   19988:	b	197f0 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1998c:	mov	x0, x19
   19990:	bl	c1c0 <__gmpz_realloc@plt>
   19994:	b	198f0 <__gmpz_inp_str_nowhite@@Base+0x1e0>

0000000000019998 <__gmpz_invert@@Base>:
   19998:	stp	x29, x30, [sp, #-64]!
   1999c:	stp	x24, x23, [sp, #16]
   199a0:	stp	x22, x21, [sp, #32]
   199a4:	stp	x20, x19, [sp, #48]
   199a8:	mov	x29, sp
   199ac:	sub	sp, sp, #0x30
   199b0:	ldr	w8, [x1, #4]
   199b4:	ldr	w9, [x2, #4]
   199b8:	mov	x20, x2
   199bc:	mov	x21, x1
   199c0:	cmp	w8, #0x0
   199c4:	cneg	w8, w8, mi  // mi = first
   199c8:	cmp	w9, #0x0
   199cc:	cneg	w9, w9, mi  // mi = first
   199d0:	cmp	x8, x9
   199d4:	csel	x23, x8, x9, hi  // hi = pmore
   199d8:	add	x24, x23, #0x1
   199dc:	mov	x19, x0
   199e0:	cmp	x23, #0xfdf
   199e4:	lsl	x22, x24, #3
   199e8:	stur	xzr, [x29, #-40]
   199ec:	stur	w24, [x29, #-16]
   199f0:	b.hi	19b04 <__gmpz_invert@@Base+0x16c>  // b.pmore
   199f4:	add	x9, x22, #0xf
   199f8:	mov	x8, sp
   199fc:	and	x9, x9, #0x1ffffffff0
   19a00:	sub	x0, x8, x9
   19a04:	mov	sp, x0
   19a08:	cmp	x23, #0xfdf
   19a0c:	stur	x0, [x29, #-8]
   19a10:	stur	w24, [x29, #-32]
   19a14:	b.hi	19a30 <__gmpz_invert@@Base+0x98>  // b.pmore
   19a18:	add	x9, x22, #0xf
   19a1c:	mov	x8, sp
   19a20:	and	x9, x9, #0x1ffffffff0
   19a24:	sub	x0, x8, x9
   19a28:	mov	sp, x0
   19a2c:	b	19a3c <__gmpz_invert@@Base+0xa4>
   19a30:	sub	x0, x29, #0x28
   19a34:	mov	x1, x22
   19a38:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   19a3c:	stur	x0, [x29, #-24]
   19a40:	sub	x0, x29, #0x10
   19a44:	sub	x1, x29, #0x20
   19a48:	mov	x2, xzr
   19a4c:	mov	x3, x21
   19a50:	mov	x4, x20
   19a54:	bl	c200 <__gmpz_gcdext@plt>
   19a58:	ldur	w8, [x29, #-12]
   19a5c:	cmp	w8, #0x1
   19a60:	b.ne	19a94 <__gmpz_invert@@Base+0xfc>  // b.any
   19a64:	ldur	x8, [x29, #-8]
   19a68:	ldr	x8, [x8]
   19a6c:	cmp	x8, #0x1
   19a70:	b.ne	19a94 <__gmpz_invert@@Base+0xfc>  // b.any
   19a74:	ldur	w8, [x29, #-28]
   19a78:	tbnz	w8, #31, 19aa8 <__gmpz_invert@@Base+0x110>
   19a7c:	sub	x1, x29, #0x20
   19a80:	mov	x0, x19
   19a84:	bl	c590 <__gmpz_set@plt>
   19a88:	ldur	x0, [x29, #-40]
   19a8c:	cbz	x0, 19ac8 <__gmpz_invert@@Base+0x130>
   19a90:	b	19afc <__gmpz_invert@@Base+0x164>
   19a94:	ldur	x0, [x29, #-40]
   19a98:	cbz	x0, 19acc <__gmpz_invert@@Base+0x134>
   19a9c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   19aa0:	mov	w0, wzr
   19aa4:	b	19acc <__gmpz_invert@@Base+0x134>
   19aa8:	ldr	w8, [x20, #4]
   19aac:	tbnz	w8, #31, 19ae4 <__gmpz_invert@@Base+0x14c>
   19ab0:	sub	x1, x29, #0x20
   19ab4:	mov	x0, x19
   19ab8:	mov	x2, x20
   19abc:	bl	d160 <__gmpz_add@plt>
   19ac0:	ldur	x0, [x29, #-40]
   19ac4:	cbnz	x0, 19afc <__gmpz_invert@@Base+0x164>
   19ac8:	mov	w0, #0x1                   	// #1
   19acc:	mov	sp, x29
   19ad0:	ldp	x20, x19, [sp, #48]
   19ad4:	ldp	x22, x21, [sp, #32]
   19ad8:	ldp	x24, x23, [sp, #16]
   19adc:	ldp	x29, x30, [sp], #64
   19ae0:	ret
   19ae4:	sub	x1, x29, #0x20
   19ae8:	mov	x0, x19
   19aec:	mov	x2, x20
   19af0:	bl	c3b0 <__gmpz_sub@plt>
   19af4:	ldur	x0, [x29, #-40]
   19af8:	cbz	x0, 19ac8 <__gmpz_invert@@Base+0x130>
   19afc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   19b00:	b	19ac8 <__gmpz_invert@@Base+0x130>
   19b04:	sub	x0, x29, #0x28
   19b08:	mov	x1, x22
   19b0c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   19b10:	b	19a08 <__gmpz_invert@@Base+0x70>

0000000000019b14 <__gmpz_ior@@Base>:
   19b14:	stp	x29, x30, [sp, #-80]!
   19b18:	stp	x26, x25, [sp, #16]
   19b1c:	stp	x24, x23, [sp, #32]
   19b20:	stp	x22, x21, [sp, #48]
   19b24:	stp	x20, x19, [sp, #64]
   19b28:	mov	x29, sp
   19b2c:	sub	sp, sp, #0x10
   19b30:	ldr	w9, [x1, #4]
   19b34:	ldr	w10, [x2, #4]
   19b38:	ldr	x21, [x0, #8]
   19b3c:	mov	x19, x0
   19b40:	cmp	w9, w10
   19b44:	csel	x8, x2, x1, lt  // lt = tstop
   19b48:	ldr	x23, [x8, #8]
   19b4c:	csel	w11, w9, w10, lt  // lt = tstop
   19b50:	csel	w24, w10, w9, lt  // lt = tstop
   19b54:	sxtw	x22, w11
   19b58:	sxtw	x20, w24
   19b5c:	csel	x26, x1, x2, lt  // lt = tstop
   19b60:	tbnz	w11, #31, 19bac <__gmpz_ior@@Base+0x98>
   19b64:	cmp	x21, x23
   19b68:	b.eq	19b8c <__gmpz_ior@@Base+0x78>  // b.none
   19b6c:	ldr	w8, [x19]
   19b70:	cmp	w24, w8
   19b74:	b.gt	19d74 <__gmpz_ior@@Base+0x260>
   19b78:	lsl	x8, x22, #3
   19b7c:	add	x0, x21, x8
   19b80:	add	x1, x23, x8
   19b84:	sub	x2, x20, x22
   19b88:	bl	cc10 <__gmpn_copyi@plt>
   19b8c:	cbz	w22, 19ba4 <__gmpz_ior@@Base+0x90>
   19b90:	ldr	x2, [x26, #8]
   19b94:	mov	x0, x21
   19b98:	mov	x1, x23
   19b9c:	mov	x3, x22
   19ba0:	bl	cdc0 <__gmpn_ior_n@plt>
   19ba4:	str	w24, [x19, #4]
   19ba8:	b	19e28 <__gmpz_ior@@Base+0x314>
   19bac:	stur	xzr, [x29, #-8]
   19bb0:	tbnz	w24, #31, 19c38 <__gmpz_ior@@Base+0x124>
   19bb4:	ldrsw	x9, [x19]
   19bb8:	neg	x25, x22
   19bbc:	cmp	x25, x9
   19bc0:	b.gt	19d88 <__gmpz_ior@@Base+0x274>
   19bc4:	ldr	x23, [x8, #8]
   19bc8:	lsl	x1, x25, #3
   19bcc:	mov	w8, #0x7f00                	// #32512
   19bd0:	cmp	x1, x8
   19bd4:	b.hi	19da4 <__gmpz_ior@@Base+0x290>  // b.pmore
   19bd8:	add	x9, x1, #0xf
   19bdc:	mov	x8, sp
   19be0:	and	x9, x9, #0xfffffffffffffff0
   19be4:	sub	x24, x8, x9
   19be8:	mov	sp, x24
   19bec:	ldr	x1, [x26, #8]
   19bf0:	mov	w3, #0x1                   	// #1
   19bf4:	mov	x0, x24
   19bf8:	mov	x2, x25
   19bfc:	bl	caf0 <__gmpn_sub_1@plt>
   19c00:	mvn	x8, x22
   19c04:	ldr	x8, [x24, x8, lsl #3]
   19c08:	cmp	x8, #0x0
   19c0c:	csetm	x8, eq  // eq = none
   19c10:	sub	x25, x8, x22
   19c14:	subs	x2, x25, x20
   19c18:	b.le	19cf0 <__gmpz_ior@@Base+0x1dc>
   19c1c:	lsl	x8, x20, #3
   19c20:	add	x0, x21, x8
   19c24:	add	x1, x24, x8
   19c28:	bl	cc10 <__gmpn_copyi@plt>
   19c2c:	mov	x22, x25
   19c30:	cbnz	x22, 19d20 <__gmpz_ior@@Base+0x20c>
   19c34:	b	19d64 <__gmpz_ior@@Base+0x250>
   19c38:	neg	x22, x20
   19c3c:	lsl	x1, x22, #4
   19c40:	mov	w8, #0x7f00                	// #32512
   19c44:	cmp	x1, x8
   19c48:	b.hi	19db4 <__gmpz_ior@@Base+0x2a0>  // b.pmore
   19c4c:	add	x9, x1, #0xf
   19c50:	mov	x8, sp
   19c54:	and	x9, x9, #0xfffffffffffffff0
   19c58:	sub	x21, x8, x9
   19c5c:	mov	sp, x21
   19c60:	mov	w3, #0x1                   	// #1
   19c64:	mov	x0, x21
   19c68:	mov	x1, x23
   19c6c:	mov	x2, x22
   19c70:	add	x24, x21, x22, lsl #3
   19c74:	bl	caf0 <__gmpn_sub_1@plt>
   19c78:	ldr	x1, [x26, #8]
   19c7c:	mov	w3, #0x1                   	// #1
   19c80:	mov	x0, x24
   19c84:	mov	x2, x22
   19c88:	bl	caf0 <__gmpn_sub_1@plt>
   19c8c:	sub	x8, x21, x20, lsl #3
   19c90:	sub	x9, x21, x20, lsl #4
   19c94:	mov	x10, xzr
   19c98:	sub	x8, x8, #0x8
   19c9c:	sub	x9, x9, #0x8
   19ca0:	add	x23, x22, x10
   19ca4:	mov	x25, x10
   19ca8:	cmp	x23, #0x1
   19cac:	b.lt	19cc8 <__gmpz_ior@@Base+0x1b4>  // b.tstop
   19cb0:	lsl	x10, x25, #3
   19cb4:	ldr	x11, [x8, x10]
   19cb8:	ldr	x10, [x9, x10]
   19cbc:	and	x11, x10, x11
   19cc0:	sub	x10, x25, #0x1
   19cc4:	cbz	x11, 19ca0 <__gmpz_ior@@Base+0x18c>
   19cc8:	ldrsw	x8, [x19]
   19ccc:	cmp	x23, x8
   19cd0:	b.ge	19dc4 <__gmpz_ior@@Base+0x2b0>  // b.tcont
   19cd4:	ldr	x22, [x19, #8]
   19cd8:	cmp	x20, x25
   19cdc:	b.ne	19de0 <__gmpz_ior@@Base+0x2cc>  // b.any
   19ce0:	mov	w8, #0x1                   	// #1
   19ce4:	str	x8, [x22]
   19ce8:	mov	w8, #0xffffffff            	// #-1
   19cec:	b	19e1c <__gmpz_ior@@Base+0x308>
   19cf0:	sub	x8, x23, #0x8
   19cf4:	sub	x9, x24, #0x8
   19cf8:	mov	x22, x25
   19cfc:	subs	x25, x25, #0x1
   19d00:	b.lt	19d5c <__gmpz_ior@@Base+0x248>  // b.tstop
   19d04:	lsl	x10, x22, #3
   19d08:	ldr	x11, [x8, x10]
   19d0c:	ldr	x10, [x9, x10]
   19d10:	mov	x20, x22
   19d14:	bics	xzr, x10, x11
   19d18:	b.eq	19cf8 <__gmpz_ior@@Base+0x1e4>  // b.none
   19d1c:	cbz	x22, 19d64 <__gmpz_ior@@Base+0x250>
   19d20:	cbz	x20, 19d38 <__gmpz_ior@@Base+0x224>
   19d24:	mov	x0, x21
   19d28:	mov	x1, x24
   19d2c:	mov	x2, x23
   19d30:	mov	x3, x20
   19d34:	bl	c1a0 <__gmpn_andn_n@plt>
   19d38:	mov	w3, #0x1                   	// #1
   19d3c:	mov	x0, x21
   19d40:	mov	x1, x21
   19d44:	mov	x2, x22
   19d48:	bl	c150 <__gmpn_add_1@plt>
   19d4c:	cbz	x0, 19d6c <__gmpz_ior@@Base+0x258>
   19d50:	str	x0, [x21, x22, lsl #3]
   19d54:	add	x22, x22, #0x1
   19d58:	b	19d6c <__gmpz_ior@@Base+0x258>
   19d5c:	mov	x20, x22
   19d60:	cbnz	x22, 19d20 <__gmpz_ior@@Base+0x20c>
   19d64:	mov	w22, #0x1                   	// #1
   19d68:	str	x22, [x21]
   19d6c:	neg	w8, w22
   19d70:	b	19e1c <__gmpz_ior@@Base+0x308>
   19d74:	mov	x0, x19
   19d78:	mov	x1, x20
   19d7c:	bl	c1c0 <__gmpz_realloc@plt>
   19d80:	mov	x21, x0
   19d84:	b	19b78 <__gmpz_ior@@Base+0x64>
   19d88:	mov	x0, x19
   19d8c:	mov	x1, x25
   19d90:	mov	x21, x8
   19d94:	bl	c1c0 <__gmpz_realloc@plt>
   19d98:	mov	x8, x21
   19d9c:	mov	x21, x0
   19da0:	b	19bc4 <__gmpz_ior@@Base+0xb0>
   19da4:	sub	x0, x29, #0x8
   19da8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   19dac:	mov	x24, x0
   19db0:	b	19bec <__gmpz_ior@@Base+0xd8>
   19db4:	sub	x0, x29, #0x8
   19db8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   19dbc:	mov	x21, x0
   19dc0:	b	19c60 <__gmpz_ior@@Base+0x14c>
   19dc4:	add	x8, x22, x25
   19dc8:	add	x1, x8, #0x1
   19dcc:	mov	x0, x19
   19dd0:	bl	c1c0 <__gmpz_realloc@plt>
   19dd4:	mov	x22, x0
   19dd8:	cmp	x20, x25
   19ddc:	b.eq	19ce0 <__gmpz_ior@@Base+0x1cc>  // b.none
   19de0:	mov	x0, x22
   19de4:	mov	x1, x21
   19de8:	mov	x2, x24
   19dec:	mov	x3, x23
   19df0:	bl	c3c0 <__gmpn_and_n@plt>
   19df4:	sub	x8, x22, x20, lsl #3
   19df8:	str	xzr, [x8, x25, lsl #3]
   19dfc:	ldr	x9, [x22]
   19e00:	adds	x9, x9, #0x1
   19e04:	str	x9, [x22], #8
   19e08:	b.cs	19dfc <__gmpz_ior@@Base+0x2e8>  // b.hs, b.nlast
   19e0c:	lsl	x9, x25, #3
   19e10:	ldr	w8, [x8, x9]
   19e14:	sub	w8, w20, w8
   19e18:	sub	w8, w8, w25
   19e1c:	str	w8, [x19, #4]
   19e20:	ldur	x0, [x29, #-8]
   19e24:	cbnz	x0, 19e44 <__gmpz_ior@@Base+0x330>
   19e28:	mov	sp, x29
   19e2c:	ldp	x20, x19, [sp, #64]
   19e30:	ldp	x22, x21, [sp, #48]
   19e34:	ldp	x24, x23, [sp, #32]
   19e38:	ldp	x26, x25, [sp, #16]
   19e3c:	ldp	x29, x30, [sp], #80
   19e40:	ret
   19e44:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   19e48:	b	19e28 <__gmpz_ior@@Base+0x314>

0000000000019e4c <__gmpz_init_set@@Base>:
   19e4c:	stp	x29, x30, [sp, #-48]!
   19e50:	stp	x22, x21, [sp, #16]
   19e54:	stp	x20, x19, [sp, #32]
   19e58:	ldrsw	x22, [x1, #4]
   19e5c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19e60:	mov	x20, x0
   19e64:	mov	x29, sp
   19e68:	cmp	x22, #0x0
   19e6c:	cneg	x21, x22, mi  // mi = first
   19e70:	cmp	x21, #0x1
   19e74:	csinc	x8, x21, xzr, gt
   19e78:	str	w8, [x0]
   19e7c:	ldr	x9, [x9, #3840]
   19e80:	sbfiz	x0, x8, #3, #32
   19e84:	mov	x19, x1
   19e88:	ldr	x9, [x9]
   19e8c:	blr	x9
   19e90:	str	x0, [x20, #8]
   19e94:	ldr	x1, [x19, #8]
   19e98:	mov	x2, x21
   19e9c:	bl	cc10 <__gmpn_copyi@plt>
   19ea0:	str	w22, [x20, #4]
   19ea4:	ldp	x20, x19, [sp, #32]
   19ea8:	ldp	x22, x21, [sp, #16]
   19eac:	ldp	x29, x30, [sp], #48
   19eb0:	ret

0000000000019eb4 <__gmpz_init_set_d@@Base>:
   19eb4:	stp	x29, x30, [sp, #-16]!
   19eb8:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   19ebc:	add	x8, x8, #0xc40
   19ec0:	mov	x29, sp
   19ec4:	stp	xzr, x8, [x0]
   19ec8:	bl	ca40 <__gmpz_set_d@plt>
   19ecc:	ldp	x29, x30, [sp], #16
   19ed0:	ret

0000000000019ed4 <__gmpz_init_set_si@@Base>:
   19ed4:	stp	x29, x30, [sp, #-32]!
   19ed8:	mov	w8, #0x1                   	// #1
   19edc:	stp	x20, x19, [sp, #16]
   19ee0:	str	w8, [x0]
   19ee4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19ee8:	ldr	x8, [x8, #3840]
   19eec:	mov	x19, x0
   19ef0:	mov	w0, #0x8                   	// #8
   19ef4:	mov	x29, sp
   19ef8:	ldr	x8, [x8]
   19efc:	mov	x20, x1
   19f00:	blr	x8
   19f04:	cmp	x20, #0x0
   19f08:	cneg	x8, x20, mi  // mi = first
   19f0c:	cset	w9, ne  // ne = any
   19f10:	csetm	w10, ne  // ne = any
   19f14:	str	x0, [x19, #8]
   19f18:	str	x8, [x0]
   19f1c:	csel	w8, w9, w10, ge  // ge = tcont
   19f20:	str	w8, [x19, #4]
   19f24:	ldp	x20, x19, [sp, #16]
   19f28:	ldp	x29, x30, [sp], #32
   19f2c:	ret

0000000000019f30 <__gmpz_init_set_str@@Base>:
   19f30:	stp	x29, x30, [sp, #-16]!
   19f34:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   19f38:	add	x8, x8, #0xc48
   19f3c:	mov	x29, sp
   19f40:	stp	xzr, x8, [x0]
   19f44:	bl	c210 <__gmpz_set_str@plt>
   19f48:	ldp	x29, x30, [sp], #16
   19f4c:	ret

0000000000019f50 <__gmpz_init_set_ui@@Base>:
   19f50:	stp	x29, x30, [sp, #-32]!
   19f54:	mov	w8, #0x1                   	// #1
   19f58:	stp	x20, x19, [sp, #16]
   19f5c:	str	w8, [x0]
   19f60:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   19f64:	ldr	x8, [x8, #3840]
   19f68:	mov	x19, x0
   19f6c:	mov	w0, #0x8                   	// #8
   19f70:	mov	x29, sp
   19f74:	ldr	x8, [x8]
   19f78:	mov	x20, x1
   19f7c:	blr	x8
   19f80:	cmp	x20, #0x0
   19f84:	cset	w8, ne  // ne = any
   19f88:	str	x0, [x19, #8]
   19f8c:	str	x20, [x0]
   19f90:	str	w8, [x19, #4]
   19f94:	ldp	x20, x19, [sp, #16]
   19f98:	ldp	x29, x30, [sp], #32
   19f9c:	ret

0000000000019fa0 <__gmpz_jacobi@@Base>:
   19fa0:	stp	x29, x30, [sp, #-80]!
   19fa4:	stp	x26, x25, [sp, #16]
   19fa8:	stp	x24, x23, [sp, #32]
   19fac:	stp	x22, x21, [sp, #48]
   19fb0:	stp	x20, x19, [sp, #64]
   19fb4:	mov	x29, sp
   19fb8:	sub	sp, sp, #0x10
   19fbc:	ldr	x8, [x0, #8]
   19fc0:	ldrsw	x10, [x0, #4]
   19fc4:	ldrsw	x13, [x1, #4]
   19fc8:	ldr	x9, [x8]
   19fcc:	cbz	w13, 1a0c4 <__gmpz_jacobi@@Base+0x124>
   19fd0:	ldr	x3, [x1, #8]
   19fd4:	ldr	x11, [x3]
   19fd8:	cbz	w10, 1a0e0 <__gmpz_jacobi@@Base+0x140>
   19fdc:	orr	w12, w11, w9
   19fe0:	tbz	w12, #0, 1a0fc <__gmpz_jacobi@@Base+0x15c>
   19fe4:	and	w12, w10, w13
   19fe8:	cmp	w13, #0x0
   19fec:	lsr	w12, w12, #30
   19ff0:	cneg	x4, x13, lt  // lt = tstop
   19ff4:	cbz	x11, 1a2b8 <__gmpz_jacobi@@Base+0x318>
   19ff8:	rbit	x13, x11
   19ffc:	clz	x20, x13
   1a000:	and	w12, w12, #0x2
   1a004:	cmp	x4, #0x2
   1a008:	lsr	x19, x11, x20
   1a00c:	b.lt	1a03c <__gmpz_jacobi@@Base+0x9c>  // b.tstop
   1a010:	cbz	w20, 1a03c <__gmpz_jacobi@@Base+0x9c>
   1a014:	ldr	x11, [x3, #8]
   1a018:	neg	x13, x20
   1a01c:	cmp	x4, #0x2
   1a020:	lsl	x13, x11, x13
   1a024:	orr	x19, x13, x19
   1a028:	b.ne	1a03c <__gmpz_jacobi@@Base+0x9c>  // b.any
   1a02c:	lsr	x11, x11, x20
   1a030:	cmp	x11, #0x0
   1a034:	mov	w11, #0x1                   	// #1
   1a038:	cinc	x4, x11, ne  // ne = any
   1a03c:	and	w11, w19, w10, asr #31
   1a040:	cmp	w10, #0x0
   1a044:	eor	w26, w11, w12
   1a048:	cneg	x10, x10, lt  // lt = tstop
   1a04c:	cbz	x9, 1a2c8 <__gmpz_jacobi@@Base+0x328>
   1a050:	cmp	x10, x4
   1a054:	b.ge	1a120 <__gmpz_jacobi@@Base+0x180>  // b.tcont
   1a058:	rbit	x11, x9
   1a05c:	clz	x20, x11
   1a060:	cmp	x10, #0x2
   1a064:	lsr	x21, x9, x20
   1a068:	b.lt	1a098 <__gmpz_jacobi@@Base+0xf8>  // b.tstop
   1a06c:	cbz	w20, 1a098 <__gmpz_jacobi@@Base+0xf8>
   1a070:	ldr	x9, [x8, #8]
   1a074:	neg	x11, x20
   1a078:	cmp	x10, #0x2
   1a07c:	lsl	x11, x9, x11
   1a080:	orr	x21, x11, x21
   1a084:	b.ne	1a098 <__gmpz_jacobi@@Base+0xf8>  // b.any
   1a088:	lsr	x9, x9, x20
   1a08c:	cmp	x9, #0x0
   1a090:	mov	w9, #0x1                   	// #1
   1a094:	cinc	x10, x9, ne  // ne = any
   1a098:	and	w9, w21, w19
   1a09c:	eor	w26, w26, w9
   1a0a0:	mov	x22, x10
   1a0a4:	mov	x23, x8
   1a0a8:	cmp	x22, #0x1
   1a0ac:	b.eq	1a140 <__gmpz_jacobi@@Base+0x1a0>  // b.none
   1a0b0:	cmp	x4, x22, lsl #1
   1a0b4:	stur	xzr, [x29, #-8]
   1a0b8:	b.ge	1a168 <__gmpz_jacobi@@Base+0x1c8>  // b.tcont
   1a0bc:	lsl	x1, x22, #4
   1a0c0:	b	1a170 <__gmpz_jacobi@@Base+0x1d0>
   1a0c4:	cmp	w10, #0x1
   1a0c8:	b.eq	1a0d4 <__gmpz_jacobi@@Base+0x134>  // b.none
   1a0cc:	cmn	w10, #0x1
   1a0d0:	b.ne	1a0fc <__gmpz_jacobi@@Base+0x15c>  // b.any
   1a0d4:	cmp	x9, #0x1
   1a0d8:	cset	w19, eq  // eq = none
   1a0dc:	b	1a100 <__gmpz_jacobi@@Base+0x160>
   1a0e0:	cmp	w13, #0x1
   1a0e4:	b.eq	1a0f0 <__gmpz_jacobi@@Base+0x150>  // b.none
   1a0e8:	cmn	w13, #0x1
   1a0ec:	b.ne	1a0fc <__gmpz_jacobi@@Base+0x15c>  // b.any
   1a0f0:	cmp	x11, #0x1
   1a0f4:	cset	w19, eq  // eq = none
   1a0f8:	b	1a100 <__gmpz_jacobi@@Base+0x160>
   1a0fc:	mov	w19, wzr
   1a100:	mov	w0, w19
   1a104:	mov	sp, x29
   1a108:	ldp	x20, x19, [sp, #64]
   1a10c:	ldp	x22, x21, [sp, #48]
   1a110:	ldp	x24, x23, [sp, #32]
   1a114:	ldp	x26, x25, [sp, #16]
   1a118:	ldp	x29, x30, [sp], #80
   1a11c:	ret
   1a120:	mov	x21, x19
   1a124:	mov	x19, x9
   1a128:	mov	x22, x4
   1a12c:	mov	x4, x10
   1a130:	mov	x23, x3
   1a134:	mov	x3, x8
   1a138:	cmp	x22, #0x1
   1a13c:	b.ne	1a0b0 <__gmpz_jacobi@@Base+0x110>  // b.any
   1a140:	lsr	x8, x19, #1
   1a144:	eor	w8, w8, w19
   1a148:	and	w8, w8, w20, lsl #1
   1a14c:	cmp	x21, #0x1
   1a150:	eor	w20, w8, w26
   1a154:	b.ne	1a200 <__gmpz_jacobi@@Base+0x260>  // b.any
   1a158:	and	w8, w20, #0x2
   1a15c:	mov	w9, #0x1                   	// #1
   1a160:	sub	w19, w9, w8
   1a164:	b	1a100 <__gmpz_jacobi@@Base+0x160>
   1a168:	lsl	x8, x4, #3
   1a16c:	add	x1, x8, #0x8
   1a170:	mov	w8, #0x7f00                	// #32512
   1a174:	cmp	x1, x8
   1a178:	b.hi	1a2d8 <__gmpz_jacobi@@Base+0x338>  // b.pmore
   1a17c:	add	x9, x1, #0xf
   1a180:	mov	x8, sp
   1a184:	and	x9, x9, #0xfffffffffffffff0
   1a188:	sub	x24, x8, x9
   1a18c:	mov	sp, x24
   1a190:	cmp	x4, x22
   1a194:	add	x25, x24, x22, lsl #3
   1a198:	b.le	1a224 <__gmpz_jacobi@@Base+0x284>
   1a19c:	mov	x0, x25
   1a1a0:	mov	x1, x24
   1a1a4:	mov	x2, xzr
   1a1a8:	mov	x5, x23
   1a1ac:	mov	x6, x22
   1a1b0:	bl	c030 <__gmpn_tdiv_qr@plt>
   1a1b4:	cbz	w20, 1a238 <__gmpz_jacobi@@Base+0x298>
   1a1b8:	lsr	x8, x19, #1
   1a1bc:	eor	w8, w8, w19
   1a1c0:	and	w8, w8, w20, lsl #1
   1a1c4:	mov	x0, x25
   1a1c8:	mov	x1, x23
   1a1cc:	mov	x2, x22
   1a1d0:	mov	w3, w20
   1a1d4:	eor	w26, w8, w26
   1a1d8:	bl	c2f0 <__gmpn_rshift@plt>
   1a1dc:	lsl	x8, x22, #3
   1a1e0:	sub	x8, x8, #0x8
   1a1e4:	ldr	x9, [x24, x8]
   1a1e8:	ldr	x8, [x25, x8]
   1a1ec:	orr	x8, x8, x9
   1a1f0:	cmp	x8, #0x0
   1a1f4:	cset	w8, eq  // eq = none
   1a1f8:	sub	x22, x22, x8
   1a1fc:	b	1a248 <__gmpz_jacobi@@Base+0x2a8>
   1a200:	cmp	x4, #0x2
   1a204:	b.lt	1a2a0 <__gmpz_jacobi@@Base+0x300>  // b.tstop
   1a208:	cmp	x4, #0x28
   1a20c:	b.lt	1a284 <__gmpz_jacobi@@Base+0x2e4>  // b.tstop
   1a210:	mov	x0, x3
   1a214:	mov	x1, x4
   1a218:	mov	x2, x21
   1a21c:	bl	c540 <__gmpn_mod_1@plt>
   1a220:	b	1a29c <__gmpz_jacobi@@Base+0x2fc>
   1a224:	mov	x0, x24
   1a228:	mov	x1, x3
   1a22c:	mov	x2, x22
   1a230:	bl	cc10 <__gmpn_copyi@plt>
   1a234:	cbnz	w20, 1a1b8 <__gmpz_jacobi@@Base+0x218>
   1a238:	mov	x0, x25
   1a23c:	mov	x1, x23
   1a240:	mov	x2, x22
   1a244:	bl	cc10 <__gmpn_copyi@plt>
   1a248:	ldr	w0, [x24]
   1a24c:	ubfx	w2, w26, #1, #1
   1a250:	mov	w1, w21
   1a254:	bl	1a2f8 <__gmpz_jacobi@@Base+0x358>
   1a258:	mov	w3, w0
   1a25c:	mov	x0, x24
   1a260:	mov	x1, x25
   1a264:	mov	x2, x22
   1a268:	bl	d070 <__gmpn_jacobi_n@plt>
   1a26c:	ldur	x8, [x29, #-8]
   1a270:	mov	w19, w0
   1a274:	cbz	x8, 1a100 <__gmpz_jacobi@@Base+0x160>
   1a278:	mov	x0, x8
   1a27c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1a280:	b	1a100 <__gmpz_jacobi@@Base+0x160>
   1a284:	mov	x0, x3
   1a288:	mov	x1, x4
   1a28c:	mov	x2, x21
   1a290:	mov	x3, xzr
   1a294:	eor	w20, w20, w21
   1a298:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1a29c:	mov	x19, x0
   1a2a0:	mov	x0, x19
   1a2a4:	mov	x1, x21
   1a2a8:	mov	w2, w20
   1a2ac:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1a2b0:	mov	w19, w0
   1a2b4:	b	1a100 <__gmpz_jacobi@@Base+0x160>
   1a2b8:	ldr	x11, [x3, #8]!
   1a2bc:	sub	x4, x4, #0x1
   1a2c0:	cbnz	x11, 19ff8 <__gmpz_jacobi@@Base+0x58>
   1a2c4:	b	1a2b8 <__gmpz_jacobi@@Base+0x318>
   1a2c8:	ldr	x9, [x8, #8]!
   1a2cc:	sub	x10, x10, #0x1
   1a2d0:	cbnz	x9, 1a050 <__gmpz_jacobi@@Base+0xb0>
   1a2d4:	b	1a2c8 <__gmpz_jacobi@@Base+0x328>
   1a2d8:	sub	x0, x29, #0x8
   1a2dc:	mov	x24, x3
   1a2e0:	mov	x25, x4
   1a2e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1a2e8:	mov	x4, x25
   1a2ec:	mov	x3, x24
   1a2f0:	mov	x24, x0
   1a2f4:	b	1a190 <__gmpz_jacobi@@Base+0x1f0>
   1a2f8:	and	w8, w1, #0x2
   1a2fc:	bfi	w8, w0, #2, #2
   1a300:	add	w0, w8, w2
   1a304:	ret

000000000001a308 <__gmpz_si_kronecker@@Base>:
   1a308:	stp	x29, x30, [sp, #-48]!
   1a30c:	stp	x20, x19, [sp, #32]
   1a310:	ldrsw	x11, [x1, #4]
   1a314:	mov	x8, x0
   1a318:	str	x21, [sp, #16]
   1a31c:	mov	x29, sp
   1a320:	cbz	w11, 1a350 <__gmpz_si_kronecker@@Base+0x48>
   1a324:	ldr	x0, [x1, #8]
   1a328:	lsr	x10, x8, #63
   1a32c:	and	w9, w10, w11, lsr #31
   1a330:	cmp	x11, #0x0
   1a334:	ldr	x20, [x0]
   1a338:	lsl	w9, w9, #1
   1a33c:	cneg	x1, x11, mi  // mi = first
   1a340:	tbnz	w20, #0, 1a368 <__gmpz_si_kronecker@@Base+0x60>
   1a344:	tbnz	w8, #0, 1a3b4 <__gmpz_si_kronecker@@Base+0xac>
   1a348:	mov	w0, wzr
   1a34c:	b	1a438 <__gmpz_si_kronecker@@Base+0x130>
   1a350:	cmp	x8, #0x1
   1a354:	cset	w9, eq  // eq = none
   1a358:	cmn	x8, #0x1
   1a35c:	cset	w8, eq  // eq = none
   1a360:	orr	w0, w9, w8
   1a364:	b	1a438 <__gmpz_si_kronecker@@Base+0x130>
   1a368:	and	w10, w20, w10, lsl #1
   1a36c:	cmp	x8, #0x0
   1a370:	cneg	x19, x8, mi  // mi = first
   1a374:	eor	w21, w9, w10
   1a378:	tbnz	w19, #0, 1a39c <__gmpz_si_kronecker@@Base+0x94>
   1a37c:	cbz	x19, 1a424 <__gmpz_si_kronecker@@Base+0x11c>
   1a380:	rbit	x8, x8
   1a384:	lsr	x9, x20, #1
   1a388:	clz	x8, x8
   1a38c:	eor	w9, w9, w20
   1a390:	lsr	x19, x19, x8
   1a394:	and	w8, w9, w8, lsl #1
   1a398:	eor	w21, w8, w21
   1a39c:	cmp	x19, #0x1
   1a3a0:	b.ne	1a3ec <__gmpz_si_kronecker@@Base+0xe4>  // b.any
   1a3a4:	and	w8, w21, #0x2
   1a3a8:	mov	w9, #0x1                   	// #1
   1a3ac:	sub	w0, w9, w8
   1a3b0:	b	1a438 <__gmpz_si_kronecker@@Base+0x130>
   1a3b4:	cbz	x20, 1a448 <__gmpz_si_kronecker@@Base+0x140>
   1a3b8:	tbnz	w20, #0, 1a3d4 <__gmpz_si_kronecker@@Base+0xcc>
   1a3bc:	mov	x11, #0x8000000000000000    	// #-9223372036854775808
   1a3c0:	cmp	x20, x11
   1a3c4:	b.eq	1a458 <__gmpz_si_kronecker@@Base+0x150>  // b.none
   1a3c8:	rbit	x11, x20
   1a3cc:	clz	x11, x11
   1a3d0:	lsr	x20, x20, x11
   1a3d4:	and	w10, w20, w10, lsl #1
   1a3d8:	cmp	x8, #0x0
   1a3dc:	eor	w21, w9, w10
   1a3e0:	cneg	x19, x8, mi  // mi = first
   1a3e4:	cmp	x19, #0x1
   1a3e8:	b.eq	1a3a4 <__gmpz_si_kronecker@@Base+0x9c>  // b.none
   1a3ec:	cmp	x1, #0x28
   1a3f0:	b.lt	1a400 <__gmpz_si_kronecker@@Base+0xf8>  // b.tstop
   1a3f4:	mov	x2, x19
   1a3f8:	bl	c540 <__gmpn_mod_1@plt>
   1a3fc:	b	1a410 <__gmpz_si_kronecker@@Base+0x108>
   1a400:	mov	x2, x19
   1a404:	mov	x3, xzr
   1a408:	eor	w21, w21, w19
   1a40c:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1a410:	and	w8, w20, w19
   1a414:	eor	w2, w21, w8
   1a418:	mov	x1, x19
   1a41c:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1a420:	b	1a438 <__gmpz_si_kronecker@@Base+0x130>
   1a424:	cmp	x1, #0x1
   1a428:	cset	w8, eq  // eq = none
   1a42c:	cmp	x20, #0x1
   1a430:	cset	w9, eq  // eq = none
   1a434:	and	w0, w8, w9
   1a438:	ldp	x20, x19, [sp, #32]
   1a43c:	ldr	x21, [sp, #16]
   1a440:	ldp	x29, x30, [sp], #48
   1a444:	ret
   1a448:	ldr	x20, [x0, #8]!
   1a44c:	sub	x1, x1, #0x1
   1a450:	cbnz	x20, 1a3b8 <__gmpz_si_kronecker@@Base+0xb0>
   1a454:	b	1a448 <__gmpz_si_kronecker@@Base+0x140>
   1a458:	cmp	x1, #0x1
   1a45c:	b.ne	1a470 <__gmpz_si_kronecker@@Base+0x168>  // b.any
   1a460:	eor	w8, w8, w8, lsr #1
   1a464:	and	w8, w8, #0x2
   1a468:	eor	w8, w9, w8
   1a46c:	b	1a3a8 <__gmpz_si_kronecker@@Base+0xa0>
   1a470:	ldr	x11, [x0, #8]
   1a474:	lsl	x20, x11, #1
   1a478:	b	1a3d4 <__gmpz_si_kronecker@@Base+0xcc>

000000000001a47c <__gmpz_ui_kronecker@@Base>:
   1a47c:	stp	x29, x30, [sp, #-48]!
   1a480:	stp	x20, x19, [sp, #32]
   1a484:	ldr	w8, [x1, #4]
   1a488:	mov	x19, x0
   1a48c:	str	x21, [sp, #16]
   1a490:	mov	x29, sp
   1a494:	cmp	w8, #0x0
   1a498:	cneg	w8, w8, mi  // mi = first
   1a49c:	cbz	w8, 1a4b8 <__gmpz_ui_kronecker@@Base+0x3c>
   1a4a0:	ldr	x0, [x1, #8]
   1a4a4:	ldr	x20, [x0]
   1a4a8:	tbnz	w20, #0, 1a4c4 <__gmpz_ui_kronecker@@Base+0x48>
   1a4ac:	tbnz	w19, #0, 1a504 <__gmpz_ui_kronecker@@Base+0x88>
   1a4b0:	mov	w0, wzr
   1a4b4:	b	1a588 <__gmpz_ui_kronecker@@Base+0x10c>
   1a4b8:	cmp	x19, #0x1
   1a4bc:	cset	w0, eq  // eq = none
   1a4c0:	b	1a588 <__gmpz_ui_kronecker@@Base+0x10c>
   1a4c4:	cbz	x19, 1a550 <__gmpz_ui_kronecker@@Base+0xd4>
   1a4c8:	tbnz	w19, #0, 1a534 <__gmpz_ui_kronecker@@Base+0xb8>
   1a4cc:	rbit	x9, x19
   1a4d0:	lsr	x10, x20, #1
   1a4d4:	clz	x9, x9
   1a4d8:	eor	w10, w10, w20
   1a4dc:	lsr	x19, x19, x9
   1a4e0:	and	w21, w10, w9, lsl #1
   1a4e4:	cmp	x19, #0x1
   1a4e8:	b.eq	1a540 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1a4ec:	cmp	w8, #0x28
   1a4f0:	sxtw	x1, w8
   1a4f4:	b.lt	1a568 <__gmpz_ui_kronecker@@Base+0xec>  // b.tstop
   1a4f8:	mov	x2, x19
   1a4fc:	bl	c540 <__gmpn_mod_1@plt>
   1a500:	b	1a578 <__gmpz_ui_kronecker@@Base+0xfc>
   1a504:	cbz	x20, 1a598 <__gmpz_ui_kronecker@@Base+0x11c>
   1a508:	tbnz	w20, #0, 1a534 <__gmpz_ui_kronecker@@Base+0xb8>
   1a50c:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   1a510:	cmp	x20, x9
   1a514:	b.eq	1a5a8 <__gmpz_ui_kronecker@@Base+0x12c>  // b.none
   1a518:	rbit	x9, x20
   1a51c:	clz	x9, x9
   1a520:	mov	w21, wzr
   1a524:	lsr	x20, x20, x9
   1a528:	cmp	x19, #0x1
   1a52c:	b.eq	1a540 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1a530:	b	1a4ec <__gmpz_ui_kronecker@@Base+0x70>
   1a534:	mov	w21, wzr
   1a538:	cmp	x19, #0x1
   1a53c:	b.ne	1a4ec <__gmpz_ui_kronecker@@Base+0x70>  // b.any
   1a540:	and	w8, w21, #0x2
   1a544:	mov	w9, #0x1                   	// #1
   1a548:	sub	w0, w9, w8
   1a54c:	b	1a588 <__gmpz_ui_kronecker@@Base+0x10c>
   1a550:	cmp	w8, #0x1
   1a554:	cset	w8, eq  // eq = none
   1a558:	cmp	x20, #0x1
   1a55c:	cset	w9, eq  // eq = none
   1a560:	and	w0, w8, w9
   1a564:	b	1a588 <__gmpz_ui_kronecker@@Base+0x10c>
   1a568:	mov	x2, x19
   1a56c:	mov	x3, xzr
   1a570:	eor	w21, w21, w19
   1a574:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1a578:	and	w8, w19, w20
   1a57c:	eor	w2, w21, w8
   1a580:	mov	x1, x19
   1a584:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1a588:	ldp	x20, x19, [sp, #32]
   1a58c:	ldr	x21, [sp, #16]
   1a590:	ldp	x29, x30, [sp], #48
   1a594:	ret
   1a598:	ldr	x20, [x0, #8]!
   1a59c:	sub	w8, w8, #0x1
   1a5a0:	cbnz	x20, 1a508 <__gmpz_ui_kronecker@@Base+0x8c>
   1a5a4:	b	1a598 <__gmpz_ui_kronecker@@Base+0x11c>
   1a5a8:	cmp	w8, #0x1
   1a5ac:	b.ne	1a5bc <__gmpz_ui_kronecker@@Base+0x140>  // b.any
   1a5b0:	eor	w8, w19, w19, lsr #1
   1a5b4:	and	w8, w8, #0x2
   1a5b8:	b	1a544 <__gmpz_ui_kronecker@@Base+0xc8>
   1a5bc:	ldr	x9, [x0, #8]
   1a5c0:	mov	w21, wzr
   1a5c4:	lsl	x20, x9, #1
   1a5c8:	cmp	x19, #0x1
   1a5cc:	b.eq	1a540 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1a5d0:	b	1a4ec <__gmpz_ui_kronecker@@Base+0x70>

000000000001a5d4 <__gmpz_kronecker_si@@Base>:
   1a5d4:	stp	x29, x30, [sp, #-32]!
   1a5d8:	stp	x20, x19, [sp, #16]
   1a5dc:	ldrsw	x9, [x0, #4]
   1a5e0:	mov	x29, sp
   1a5e4:	cbz	w9, 1a624 <__gmpz_kronecker_si@@Base+0x50>
   1a5e8:	ldr	x0, [x0, #8]
   1a5ec:	ubfx	x10, x9, #31, #1
   1a5f0:	lsr	x8, x1, #63
   1a5f4:	and	w8, w10, w8
   1a5f8:	cmp	x1, #0x0
   1a5fc:	cneg	x19, x1, mi  // mi = first
   1a600:	lsl	w11, w8, #1
   1a604:	tbnz	w19, #0, 1a69c <__gmpz_kronecker_si@@Base+0xc8>
   1a608:	ldr	x8, [x0]
   1a60c:	cbz	x19, 1a63c <__gmpz_kronecker_si@@Base+0x68>
   1a610:	tbnz	w8, #0, 1a664 <__gmpz_kronecker_si@@Base+0x90>
   1a614:	mov	w12, wzr
   1a618:	mov	w8, wzr
   1a61c:	cbnz	w12, 1a69c <__gmpz_kronecker_si@@Base+0xc8>
   1a620:	b	1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a624:	cmp	x1, #0x1
   1a628:	cset	w8, eq  // eq = none
   1a62c:	cmn	x1, #0x1
   1a630:	cset	w9, eq  // eq = none
   1a634:	orr	w8, w8, w9
   1a638:	b	1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a63c:	cmp	w9, #0x1
   1a640:	b.eq	1a64c <__gmpz_kronecker_si@@Base+0x78>  // b.none
   1a644:	cmn	w9, #0x1
   1a648:	b.ne	1a68c <__gmpz_kronecker_si@@Base+0xb8>  // b.any
   1a64c:	cmp	x8, #0x1
   1a650:	mov	w12, wzr
   1a654:	mov	x19, xzr
   1a658:	cset	w8, eq  // eq = none
   1a65c:	cbnz	w12, 1a69c <__gmpz_kronecker_si@@Base+0xc8>
   1a660:	b	1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a664:	rbit	x12, x1
   1a668:	lsr	x13, x8, #1
   1a66c:	clz	x12, x12
   1a670:	eor	w8, w13, w8
   1a674:	and	w8, w8, w12, lsl #1
   1a678:	lsr	x19, x19, x12
   1a67c:	eor	w11, w8, w11
   1a680:	mov	w12, #0x1                   	// #1
   1a684:	cbnz	w12, 1a69c <__gmpz_kronecker_si@@Base+0xc8>
   1a688:	b	1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a68c:	mov	w12, wzr
   1a690:	mov	x19, xzr
   1a694:	mov	w8, wzr
   1a698:	cbz	w12, 1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a69c:	cmp	x19, #0x1
   1a6a0:	b.ne	1a6b4 <__gmpz_kronecker_si@@Base+0xe0>  // b.any
   1a6a4:	and	w8, w11, #0x2
   1a6a8:	mov	w9, #0x1                   	// #1
   1a6ac:	sub	w8, w9, w8
   1a6b0:	b	1a6f8 <__gmpz_kronecker_si@@Base+0x124>
   1a6b4:	cmp	x9, #0x0
   1a6b8:	and	w8, w19, w10, lsl #1
   1a6bc:	cneg	x1, x9, mi  // mi = first
   1a6c0:	cmp	x1, #0x28
   1a6c4:	eor	w20, w8, w11
   1a6c8:	b.lt	1a6d8 <__gmpz_kronecker_si@@Base+0x104>  // b.tstop
   1a6cc:	mov	x2, x19
   1a6d0:	bl	c540 <__gmpn_mod_1@plt>
   1a6d4:	b	1a6e8 <__gmpz_kronecker_si@@Base+0x114>
   1a6d8:	mov	x2, x19
   1a6dc:	mov	x3, xzr
   1a6e0:	eor	w20, w20, w19
   1a6e4:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1a6e8:	mov	x1, x19
   1a6ec:	mov	w2, w20
   1a6f0:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1a6f4:	mov	w8, w0
   1a6f8:	ldp	x20, x19, [sp, #16]
   1a6fc:	mov	w0, w8
   1a700:	ldp	x29, x30, [sp], #32
   1a704:	ret

000000000001a708 <__gmpz_kronecker_ui@@Base>:
   1a708:	stp	x29, x30, [sp, #-32]!
   1a70c:	stp	x20, x19, [sp, #16]
   1a710:	ldrsw	x9, [x0, #4]
   1a714:	mov	x19, x1
   1a718:	mov	x29, sp
   1a71c:	cbz	w9, 1a758 <__gmpz_kronecker_ui@@Base+0x50>
   1a720:	ldr	x0, [x0, #8]
   1a724:	tbnz	w19, #0, 1a764 <__gmpz_kronecker_ui@@Base+0x5c>
   1a728:	ldr	x8, [x0]
   1a72c:	cbz	x19, 1a790 <__gmpz_kronecker_ui@@Base+0x88>
   1a730:	tbnz	w8, #0, 1a7e8 <__gmpz_kronecker_ui@@Base+0xe0>
   1a734:	mov	w10, wzr
   1a738:	mov	w8, wzr
   1a73c:	cbz	w10, 1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a740:	cmp	x19, #0x1
   1a744:	b.ne	1a774 <__gmpz_kronecker_ui@@Base+0x6c>  // b.any
   1a748:	and	w8, w20, #0x2
   1a74c:	mov	w9, #0x1                   	// #1
   1a750:	sub	w8, w9, w8
   1a754:	b	1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a758:	cmp	x19, #0x1
   1a75c:	cset	w8, eq  // eq = none
   1a760:	b	1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a764:	and	w8, w19, w9, lsr #30
   1a768:	and	w20, w8, #0x2
   1a76c:	cmp	x19, #0x1
   1a770:	b.eq	1a748 <__gmpz_kronecker_ui@@Base+0x40>  // b.none
   1a774:	cmp	x9, #0x0
   1a778:	cneg	x1, x9, mi  // mi = first
   1a77c:	cmp	x1, #0x28
   1a780:	b.lt	1a7b8 <__gmpz_kronecker_ui@@Base+0xb0>  // b.tstop
   1a784:	mov	x2, x19
   1a788:	bl	c540 <__gmpn_mod_1@plt>
   1a78c:	b	1a7c8 <__gmpz_kronecker_ui@@Base+0xc0>
   1a790:	cmp	w9, #0x1
   1a794:	b.eq	1a7a0 <__gmpz_kronecker_ui@@Base+0x98>  // b.none
   1a798:	cmn	w9, #0x1
   1a79c:	b.ne	1a818 <__gmpz_kronecker_ui@@Base+0x110>  // b.any
   1a7a0:	cmp	x8, #0x1
   1a7a4:	mov	w10, wzr
   1a7a8:	mov	x19, xzr
   1a7ac:	cset	w8, eq  // eq = none
   1a7b0:	cbnz	w10, 1a740 <__gmpz_kronecker_ui@@Base+0x38>
   1a7b4:	b	1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a7b8:	mov	x2, x19
   1a7bc:	mov	x3, xzr
   1a7c0:	eor	w20, w20, w19
   1a7c4:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1a7c8:	mov	x1, x19
   1a7cc:	mov	w2, w20
   1a7d0:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1a7d4:	mov	w8, w0
   1a7d8:	ldp	x20, x19, [sp, #16]
   1a7dc:	mov	w0, w8
   1a7e0:	ldp	x29, x30, [sp], #32
   1a7e4:	ret
   1a7e8:	rbit	x10, x19
   1a7ec:	lsr	x11, x8, #1
   1a7f0:	clz	x10, x10
   1a7f4:	eor	w8, w11, w8
   1a7f8:	lsr	x19, x19, x10
   1a7fc:	and	w8, w8, w10, lsl #1
   1a800:	and	w10, w19, w9, lsr #30
   1a804:	and	w10, w10, #0x2
   1a808:	eor	w20, w8, w10
   1a80c:	mov	w10, #0x1                   	// #1
   1a810:	cbnz	w10, 1a740 <__gmpz_kronecker_ui@@Base+0x38>
   1a814:	b	1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a818:	mov	w10, wzr
   1a81c:	mov	x19, xzr
   1a820:	mov	w8, wzr
   1a824:	cbz	w10, 1a7d8 <__gmpz_kronecker_ui@@Base+0xd0>
   1a828:	b	1a740 <__gmpz_kronecker_ui@@Base+0x38>

000000000001a82c <__gmpz_lcm@@Base>:
   1a82c:	stp	x29, x30, [sp, #-64]!
   1a830:	str	x23, [sp, #16]
   1a834:	stp	x22, x21, [sp, #32]
   1a838:	stp	x20, x19, [sp, #48]
   1a83c:	mov	x29, sp
   1a840:	sub	sp, sp, #0x10
   1a844:	ldrsw	x8, [x1, #4]
   1a848:	mov	x19, x0
   1a84c:	cbz	w8, 1a904 <__gmpz_lcm@@Base+0xd8>
   1a850:	ldr	w9, [x2, #4]
   1a854:	mov	x20, x2
   1a858:	cbz	w9, 1a904 <__gmpz_lcm@@Base+0xd8>
   1a85c:	sxtw	x9, w9
   1a860:	cmp	x8, #0x0
   1a864:	cneg	x8, x8, mi  // mi = first
   1a868:	cmp	x9, #0x0
   1a86c:	mov	x21, x1
   1a870:	cneg	x9, x9, mi  // mi = first
   1a874:	cmp	x8, #0x1
   1a878:	b.eq	1a90c <__gmpz_lcm@@Base+0xe0>  // b.none
   1a87c:	cmp	x9, #0x1
   1a880:	b.eq	1a90c <__gmpz_lcm@@Base+0xe0>  // b.none
   1a884:	lsl	x1, x8, #3
   1a888:	mov	w9, #0x7f00                	// #32512
   1a88c:	cmp	x1, x9
   1a890:	str	xzr, [x29, #24]
   1a894:	stur	w8, [x29, #-16]
   1a898:	b.hi	1a984 <__gmpz_lcm@@Base+0x158>  // b.pmore
   1a89c:	add	x9, x1, #0xf
   1a8a0:	mov	x8, sp
   1a8a4:	and	x9, x9, #0xfffffffffffffff0
   1a8a8:	sub	x0, x8, x9
   1a8ac:	mov	sp, x0
   1a8b0:	stur	x0, [x29, #-8]
   1a8b4:	sub	x0, x29, #0x10
   1a8b8:	mov	x1, x21
   1a8bc:	mov	x2, x20
   1a8c0:	bl	d140 <__gmpz_gcd@plt>
   1a8c4:	sub	x0, x29, #0x10
   1a8c8:	sub	x2, x29, #0x10
   1a8cc:	mov	x1, x21
   1a8d0:	bl	c550 <__gmpz_divexact@plt>
   1a8d4:	sub	x1, x29, #0x10
   1a8d8:	mov	x0, x19
   1a8dc:	mov	x2, x20
   1a8e0:	bl	c620 <__gmpz_mul@plt>
   1a8e4:	ldr	w8, [x19, #4]
   1a8e8:	cmp	w8, #0x0
   1a8ec:	cneg	w8, w8, mi  // mi = first
   1a8f0:	str	w8, [x19, #4]
   1a8f4:	ldr	x0, [x29, #24]
   1a8f8:	cbz	x0, 1a96c <__gmpz_lcm@@Base+0x140>
   1a8fc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1a900:	b	1a96c <__gmpz_lcm@@Base+0x140>
   1a904:	str	wzr, [x19, #4]
   1a908:	b	1a96c <__gmpz_lcm@@Base+0x140>
   1a90c:	ldrsw	x10, [x19]
   1a910:	cmp	x8, #0x1
   1a914:	csel	x22, x9, x8, eq  // eq = none
   1a918:	csel	x23, x21, x20, eq  // eq = none
   1a91c:	csel	x20, x20, x21, eq  // eq = none
   1a920:	cmp	x22, x10
   1a924:	b.ge	1a990 <__gmpz_lcm@@Base+0x164>  // b.tcont
   1a928:	ldr	x8, [x23, #8]
   1a92c:	ldr	x20, [x20, #8]
   1a930:	mov	x1, x22
   1a934:	ldr	x21, [x8]
   1a938:	mov	x0, x20
   1a93c:	mov	x2, x21
   1a940:	bl	c0c0 <__gmpn_gcd_1@plt>
   1a944:	ldr	x23, [x19, #8]
   1a948:	udiv	x3, x21, x0
   1a94c:	mov	x1, x20
   1a950:	mov	x2, x22
   1a954:	mov	x0, x23
   1a958:	bl	d670 <__gmpn_mul_1@plt>
   1a95c:	cmp	x0, #0x0
   1a960:	cinc	w8, w22, ne  // ne = any
   1a964:	str	x0, [x23, x22, lsl #3]
   1a968:	str	w8, [x19, #4]
   1a96c:	mov	sp, x29
   1a970:	ldp	x20, x19, [sp, #48]
   1a974:	ldp	x22, x21, [sp, #32]
   1a978:	ldr	x23, [sp, #16]
   1a97c:	ldp	x29, x30, [sp], #64
   1a980:	ret
   1a984:	add	x0, x29, #0x18
   1a988:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1a98c:	b	1a8b0 <__gmpz_lcm@@Base+0x84>
   1a990:	add	x1, x22, #0x1
   1a994:	mov	x0, x19
   1a998:	bl	c1c0 <__gmpz_realloc@plt>
   1a99c:	b	1a928 <__gmpz_lcm@@Base+0xfc>

000000000001a9a0 <__gmpz_lcm_ui@@Base>:
   1a9a0:	stp	x29, x30, [sp, #-64]!
   1a9a4:	stp	x20, x19, [sp, #48]
   1a9a8:	mov	x19, x0
   1a9ac:	mov	w8, wzr
   1a9b0:	str	x23, [sp, #16]
   1a9b4:	stp	x22, x21, [sp, #32]
   1a9b8:	mov	x29, sp
   1a9bc:	cbz	x2, 1aa20 <__gmpz_lcm_ui@@Base+0x80>
   1a9c0:	ldr	w9, [x1, #4]
   1a9c4:	mov	x22, x1
   1a9c8:	cbz	w9, 1aa20 <__gmpz_lcm_ui@@Base+0x80>
   1a9cc:	ldrsw	x8, [x19]
   1a9d0:	sxtw	x9, w9
   1a9d4:	cmp	x9, #0x0
   1a9d8:	cneg	x21, x9, mi  // mi = first
   1a9dc:	mov	x20, x2
   1a9e0:	cmp	x21, x8
   1a9e4:	b.ge	1aa38 <__gmpz_lcm_ui@@Base+0x98>  // b.tcont
   1a9e8:	ldr	x22, [x22, #8]
   1a9ec:	mov	x1, x21
   1a9f0:	mov	x2, x20
   1a9f4:	mov	x0, x22
   1a9f8:	bl	c0c0 <__gmpn_gcd_1@plt>
   1a9fc:	ldr	x23, [x19, #8]
   1aa00:	udiv	x3, x20, x0
   1aa04:	mov	x1, x22
   1aa08:	mov	x2, x21
   1aa0c:	mov	x0, x23
   1aa10:	bl	d670 <__gmpn_mul_1@plt>
   1aa14:	cmp	x0, #0x0
   1aa18:	cinc	w8, w21, ne  // ne = any
   1aa1c:	str	x0, [x23, x21, lsl #3]
   1aa20:	str	w8, [x19, #4]
   1aa24:	ldp	x20, x19, [sp, #48]
   1aa28:	ldp	x22, x21, [sp, #32]
   1aa2c:	ldr	x23, [sp, #16]
   1aa30:	ldp	x29, x30, [sp], #64
   1aa34:	ret
   1aa38:	add	x1, x21, #0x1
   1aa3c:	mov	x0, x19
   1aa40:	bl	c1c0 <__gmpz_realloc@plt>
   1aa44:	b	1a9e8 <__gmpz_lcm_ui@@Base+0x48>

000000000001aa48 <__gmpz_limbs_finish@@Base>:
   1aa48:	cmp	x1, #0x0
   1aa4c:	cneg	x9, x1, mi  // mi = first
   1aa50:	mov	x8, x9
   1aa54:	subs	x9, x9, #0x1
   1aa58:	b.lt	1aa6c <__gmpz_limbs_finish@@Base+0x24>  // b.tstop
   1aa5c:	ldr	x10, [x0, #8]
   1aa60:	add	x10, x10, x8, lsl #3
   1aa64:	ldur	x10, [x10, #-8]
   1aa68:	cbz	x10, 1aa50 <__gmpz_limbs_finish@@Base+0x8>
   1aa6c:	neg	w9, w8
   1aa70:	cmp	x1, #0x0
   1aa74:	csel	x8, x9, x8, lt  // lt = tstop
   1aa78:	str	w8, [x0, #4]
   1aa7c:	ret

000000000001aa80 <__gmpz_limbs_modify@@Base>:
   1aa80:	stp	x29, x30, [sp, #-16]!
   1aa84:	ldrsw	x8, [x0]
   1aa88:	mov	x29, sp
   1aa8c:	cmp	x8, x1
   1aa90:	b.lt	1aaa0 <__gmpz_limbs_modify@@Base+0x20>  // b.tstop
   1aa94:	ldr	x0, [x0, #8]
   1aa98:	ldp	x29, x30, [sp], #16
   1aa9c:	ret
   1aaa0:	bl	c1c0 <__gmpz_realloc@plt>
   1aaa4:	ldp	x29, x30, [sp], #16
   1aaa8:	ret

000000000001aaac <__gmpz_limbs_read@@Base>:
   1aaac:	ldr	x0, [x0, #8]
   1aab0:	ret

000000000001aab4 <__gmpz_limbs_write@@Base>:
   1aab4:	stp	x29, x30, [sp, #-16]!
   1aab8:	ldrsw	x8, [x0]
   1aabc:	mov	x29, sp
   1aac0:	cmp	x8, x1
   1aac4:	b.lt	1aad4 <__gmpz_limbs_write@@Base+0x20>  // b.tstop
   1aac8:	ldr	x0, [x0, #8]
   1aacc:	ldp	x29, x30, [sp], #16
   1aad0:	ret
   1aad4:	bl	c1c0 <__gmpz_realloc@plt>
   1aad8:	ldp	x29, x30, [sp], #16
   1aadc:	ret

000000000001aae0 <__gmpz_lucas_mod@@Base>:
   1aae0:	stp	x29, x30, [sp, #-96]!
   1aae4:	stp	x22, x21, [sp, #64]
   1aae8:	mov	x21, x1
   1aaec:	mov	w1, #0x1                   	// #1
   1aaf0:	str	x27, [sp, #16]
   1aaf4:	stp	x26, x25, [sp, #32]
   1aaf8:	stp	x24, x23, [sp, #48]
   1aafc:	stp	x20, x19, [sp, #80]
   1ab00:	mov	x29, sp
   1ab04:	mov	x19, x6
   1ab08:	mov	x22, x5
   1ab0c:	mov	x20, x4
   1ab10:	mov	x24, x3
   1ab14:	mov	x23, x2
   1ab18:	mov	x25, x0
   1ab1c:	bl	c2c0 <__gmpz_set_ui@plt>
   1ab20:	mov	w1, #0x2                   	// #2
   1ab24:	mov	x0, x20
   1ab28:	bl	d440 <__gmpz_sizeinbase@plt>
   1ab2c:	sub	x26, x0, #0x2
   1ab30:	cmp	x26, x24
   1ab34:	b.cc	1ad20 <__gmpz_lucas_mod@@Base+0x240>  // b.lo, b.ul, b.last
   1ab38:	mov	w1, #0x1                   	// #1
   1ab3c:	mov	x0, x21
   1ab40:	bl	c2c0 <__gmpz_set_ui@plt>
   1ab44:	neg	x27, x23
   1ab48:	b	1ab78 <__gmpz_lucas_mod@@Base+0x98>
   1ab4c:	mov	x0, x21
   1ab50:	mov	x1, x22
   1ab54:	mov	x2, x20
   1ab58:	bl	cc40 <__gmpz_tdiv_r@plt>
   1ab5c:	mov	x0, x25
   1ab60:	mov	x1, x19
   1ab64:	mov	x2, x20
   1ab68:	bl	cc40 <__gmpz_tdiv_r@plt>
   1ab6c:	sub	x26, x26, #0x1
   1ab70:	cmp	x26, x24
   1ab74:	b.cc	1ac2c <__gmpz_lucas_mod@@Base+0x14c>  // b.lo, b.ul, b.last
   1ab78:	mov	x0, x22
   1ab7c:	mov	x1, x21
   1ab80:	mov	x2, x21
   1ab84:	bl	c620 <__gmpz_mul@plt>
   1ab88:	mov	x0, x21
   1ab8c:	mov	x1, x25
   1ab90:	mov	x2, x21
   1ab94:	bl	c3b0 <__gmpz_sub@plt>
   1ab98:	mov	x0, x19
   1ab9c:	mov	x1, x21
   1aba0:	mov	x2, x21
   1aba4:	bl	c620 <__gmpz_mul@plt>
   1aba8:	mov	x0, x21
   1abac:	mov	x1, x25
   1abb0:	mov	x2, x25
   1abb4:	bl	c620 <__gmpz_mul@plt>
   1abb8:	mov	x0, x19
   1abbc:	mov	x1, x22
   1abc0:	mov	x2, x19
   1abc4:	bl	c3b0 <__gmpz_sub@plt>
   1abc8:	mov	x0, x22
   1abcc:	mov	x1, x21
   1abd0:	cmp	x23, #0x1
   1abd4:	b.lt	1abe4 <__gmpz_lucas_mod@@Base+0x104>  // b.tstop
   1abd8:	mov	x2, x23
   1abdc:	bl	ca20 <__gmpz_submul_ui@plt>
   1abe0:	b	1abec <__gmpz_lucas_mod@@Base+0x10c>
   1abe4:	mov	x2, x27
   1abe8:	bl	d500 <__gmpz_addmul_ui@plt>
   1abec:	mov	x0, x20
   1abf0:	mov	x1, x26
   1abf4:	bl	c5f0 <__gmpz_tstbit@plt>
   1abf8:	cbz	w0, 1ab4c <__gmpz_lucas_mod@@Base+0x6c>
   1abfc:	mov	x0, x19
   1ac00:	mov	x1, x19
   1ac04:	mov	x2, x23
   1ac08:	bl	cd70 <__gmpz_mul_si@plt>
   1ac0c:	mov	x0, x19
   1ac10:	mov	x1, x22
   1ac14:	mov	x2, x19
   1ac18:	bl	c3b0 <__gmpz_sub@plt>
   1ac1c:	mov	x0, x22
   1ac20:	mov	x1, x19
   1ac24:	bl	c710 <__gmpz_swap@plt>
   1ac28:	b	1ab4c <__gmpz_lucas_mod@@Base+0x6c>
   1ac2c:	ldr	w8, [x21, #4]
   1ac30:	cmp	w8, #0x0
   1ac34:	cset	w0, eq  // eq = none
   1ac38:	cbz	w8, 1ad04 <__gmpz_lucas_mod@@Base+0x224>
   1ac3c:	neg	x2, x23, lsl #1
   1ac40:	mov	x0, x22
   1ac44:	mov	x1, x25
   1ac48:	bl	cd70 <__gmpz_mul_si@plt>
   1ac4c:	mov	x0, x22
   1ac50:	mov	x1, x21
   1ac54:	mov	x2, x22
   1ac58:	bl	d160 <__gmpz_add@plt>
   1ac5c:	mov	x0, x25
   1ac60:	mov	x1, x22
   1ac64:	mov	x2, x20
   1ac68:	bl	cc40 <__gmpz_tdiv_r@plt>
   1ac6c:	ldr	w8, [x25, #4]
   1ac70:	cmp	w8, #0x0
   1ac74:	cset	w0, eq  // eq = none
   1ac78:	cmp	x24, #0x2
   1ac7c:	b.cc	1ad04 <__gmpz_lucas_mod@@Base+0x224>  // b.lo, b.ul, b.last
   1ac80:	cbz	w8, 1ad04 <__gmpz_lucas_mod@@Base+0x224>
   1ac84:	mov	x0, x19
   1ac88:	mov	x1, x22
   1ac8c:	mov	x2, x22
   1ac90:	bl	c620 <__gmpz_mul@plt>
   1ac94:	mov	x0, x22
   1ac98:	mov	x1, x21
   1ac9c:	mov	x2, x21
   1aca0:	bl	c620 <__gmpz_mul@plt>
   1aca4:	mov	x0, x19
   1aca8:	mov	x1, x19
   1acac:	mov	x2, x22
   1acb0:	bl	c3b0 <__gmpz_sub@plt>
   1acb4:	mov	w2, #0x2                   	// #2
   1acb8:	mov	x0, x19
   1acbc:	mov	x1, x19
   1acc0:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   1acc4:	cmp	x23, #0x1
   1acc8:	b.lt	1ace0 <__gmpz_lucas_mod@@Base+0x200>  // b.tstop
   1accc:	mov	x0, x19
   1acd0:	mov	x1, x22
   1acd4:	mov	x2, x23
   1acd8:	bl	d500 <__gmpz_addmul_ui@plt>
   1acdc:	b	1acf0 <__gmpz_lucas_mod@@Base+0x210>
   1ace0:	neg	x2, x23
   1ace4:	mov	x0, x19
   1ace8:	mov	x1, x22
   1acec:	bl	ca20 <__gmpz_submul_ui@plt>
   1acf0:	mov	x0, x21
   1acf4:	mov	x1, x19
   1acf8:	mov	x2, x20
   1acfc:	bl	cc40 <__gmpz_tdiv_r@plt>
   1ad00:	mov	w0, wzr
   1ad04:	ldp	x20, x19, [sp, #80]
   1ad08:	ldp	x22, x21, [sp, #64]
   1ad0c:	ldp	x24, x23, [sp, #48]
   1ad10:	ldp	x26, x25, [sp, #32]
   1ad14:	ldr	x27, [sp, #16]
   1ad18:	ldp	x29, x30, [sp], #96
   1ad1c:	ret
   1ad20:	mov	x0, x21
   1ad24:	mov	x1, x23
   1ad28:	bl	d450 <__gmpz_set_si@plt>
   1ad2c:	b	1ad00 <__gmpz_lucas_mod@@Base+0x220>

000000000001ad30 <__gmpz_lucnum_ui@@Base>:
   1ad30:	stp	x29, x30, [sp, #-96]!
   1ad34:	stp	x20, x19, [sp, #80]
   1ad38:	mov	x20, x1
   1ad3c:	cmp	x1, #0x5c
   1ad40:	mov	x19, x0
   1ad44:	str	x27, [sp, #16]
   1ad48:	stp	x26, x25, [sp, #32]
   1ad4c:	stp	x24, x23, [sp, #48]
   1ad50:	stp	x22, x21, [sp, #64]
   1ad54:	mov	x29, sp
   1ad58:	b.hi	1ad98 <__gmpz_lucnum_ui@@Base+0x68>  // b.pmore
   1ad5c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1ad60:	ldr	x8, [x8, #3808]
   1ad64:	sbfiz	x9, x20, #3, #32
   1ad68:	ldr	w10, [x19]
   1ad6c:	add	x11, x8, x20, lsl #3
   1ad70:	ldr	x11, [x11, #8]
   1ad74:	ldr	x8, [x8, x9]
   1ad78:	cmp	w10, #0x0
   1ad7c:	add	x20, x11, x8, lsl #1
   1ad80:	b.le	1aff8 <__gmpz_lucnum_ui@@Base+0x2c8>
   1ad84:	ldr	x0, [x19, #8]
   1ad88:	mov	w8, #0x1                   	// #1
   1ad8c:	str	x20, [x0]
   1ad90:	str	w8, [x19, #4]
   1ad94:	b	1af60 <__gmpz_lucnum_ui@@Base+0x230>
   1ad98:	lsr	x8, x20, #5
   1ad9c:	mov	w9, #0x17                  	// #23
   1ada0:	ldrsw	x10, [x19]
   1ada4:	mul	x8, x8, x9
   1ada8:	lsr	x24, x8, #6
   1adac:	add	x22, x24, #0x6
   1adb0:	cmp	x22, x10
   1adb4:	b.gt	1b008 <__gmpz_lucnum_ui@@Base+0x2d8>
   1adb8:	ldr	x21, [x19, #8]
   1adbc:	mov	w23, #0xf6c0                	// #63168
   1adc0:	movk	w23, #0x3, lsl #16
   1adc4:	cmp	x24, #0xfda
   1adc8:	lsl	x1, x22, #3
   1adcc:	str	xzr, [x29, #24]
   1add0:	b.hi	1b01c <__gmpz_lucnum_ui@@Base+0x2ec>  // b.pmore
   1add4:	add	x9, x1, #0xf
   1add8:	mov	x8, sp
   1addc:	and	x9, x9, #0x7ffffffffffffff0
   1ade0:	sub	x0, x8, x9
   1ade4:	mov	sp, x0
   1ade8:	mov	w26, wzr
   1adec:	mov	x22, x21
   1adf0:	mov	x21, x0
   1adf4:	lsr	x2, x20, #1
   1adf8:	tbnz	w20, #0, 1ae48 <__gmpz_lucnum_ui@@Base+0x118>
   1adfc:	cmp	x20, #0xb9
   1ae00:	add	w26, w26, #0x1
   1ae04:	mov	x0, x22
   1ae08:	mov	x20, x2
   1ae0c:	b.hi	1adec <__gmpz_lucnum_ui@@Base+0xbc>  // b.pmore
   1ae10:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1ae14:	ldr	x8, [x8, #3808]
   1ae18:	sbfiz	x9, x2, #3, #32
   1ae1c:	mov	w23, #0x1                   	// #1
   1ae20:	mov	x24, x22
   1ae24:	add	x10, x8, x2, lsl #3
   1ae28:	ldr	x8, [x8, x9]
   1ae2c:	ldr	x9, [x10, #8]
   1ae30:	mov	x22, x21
   1ae34:	mov	x20, x2
   1ae38:	add	x8, x9, x8, lsl #1
   1ae3c:	str	x8, [x21]
   1ae40:	cbnz	w26, 1af98 <__gmpz_lucnum_ui@@Base+0x268>
   1ae44:	b	1af54 <__gmpz_lucnum_ui@@Base+0x224>
   1ae48:	lsr	x8, x20, #6
   1ae4c:	mov	w9, #0x17                  	// #23
   1ae50:	mul	x8, x8, x9
   1ae54:	lsr	x9, x8, #3
   1ae58:	add	x10, x23, #0x80
   1ae5c:	and	x9, x9, #0xffffffffffffff8
   1ae60:	cmp	x8, x10
   1ae64:	add	x1, x9, #0x20
   1ae68:	b.cs	1b030 <__gmpz_lucnum_ui@@Base+0x300>  // b.hs, b.nlast
   1ae6c:	add	x9, x1, #0xf
   1ae70:	mov	x8, sp
   1ae74:	and	x9, x9, #0x3ffffffffffffff0
   1ae78:	sub	x23, x8, x9
   1ae7c:	mov	sp, x23
   1ae80:	mov	x0, x21
   1ae84:	mov	x1, x23
   1ae88:	bl	d240 <__gmpn_fib2_ui@plt>
   1ae8c:	lsl	x27, x0, #3
   1ae90:	add	x8, x27, x23
   1ae94:	ldur	x8, [x8, #-8]
   1ae98:	mov	x24, x0
   1ae9c:	mov	x1, x23
   1aea0:	mov	x2, x21
   1aea4:	cmp	x8, #0x0
   1aea8:	cset	w8, eq  // eq = none
   1aeac:	sub	x25, x0, x8
   1aeb0:	mov	x0, x21
   1aeb4:	mov	x3, x24
   1aeb8:	bl	ce00 <__gmpn_addlsh1_n@plt>
   1aebc:	cmp	x0, #0x0
   1aec0:	cinc	x24, x24, ne  // ne = any
   1aec4:	str	x0, [x21, x27]
   1aec8:	mov	x0, x22
   1aecc:	mov	x1, x21
   1aed0:	mov	x2, x24
   1aed4:	mov	x3, x23
   1aed8:	mov	x4, x25
   1aedc:	bl	cea0 <__gmpn_mul@plt>
   1aee0:	cmp	x0, #0x0
   1aee4:	add	x8, x24, x25
   1aee8:	cset	w9, eq  // eq = none
   1aeec:	sub	x23, x8, x9
   1aef0:	mov	x0, x22
   1aef4:	mov	x1, x22
   1aef8:	mov	x2, x22
   1aefc:	mov	x3, x23
   1af00:	bl	cd60 <__gmpn_addlsh2_n@plt>
   1af04:	str	x0, [x22, x23, lsl #3]
   1af08:	ldr	x8, [x22]
   1af0c:	cmp	x0, #0x0
   1af10:	cinc	x23, x23, ne  // ne = any
   1af14:	tbnz	w20, #1, 1af44 <__gmpz_lucnum_ui@@Base+0x214>
   1af18:	sub	x9, x8, #0x4
   1af1c:	cmp	x8, #0x3
   1af20:	str	x9, [x22]
   1af24:	b.hi	1af4c <__gmpz_lucnum_ui@@Base+0x21c>  // b.pmore
   1af28:	mov	w8, #0x8                   	// #8
   1af2c:	ldr	x9, [x22, x8]
   1af30:	sub	x10, x9, #0x1
   1af34:	str	x10, [x22, x8]
   1af38:	add	x8, x8, #0x8
   1af3c:	cbz	x9, 1af2c <__gmpz_lucnum_ui@@Base+0x1fc>
   1af40:	b	1af4c <__gmpz_lucnum_ui@@Base+0x21c>
   1af44:	add	x8, x8, #0x4
   1af48:	str	x8, [x22]
   1af4c:	mov	x24, x21
   1af50:	cbnz	w26, 1af98 <__gmpz_lucnum_ui@@Base+0x268>
   1af54:	str	w23, [x19, #4]
   1af58:	ldr	x0, [x29, #24]
   1af5c:	cbnz	x0, 1b028 <__gmpz_lucnum_ui@@Base+0x2f8>
   1af60:	mov	sp, x29
   1af64:	ldp	x20, x19, [sp, #80]
   1af68:	ldp	x22, x21, [sp, #64]
   1af6c:	ldp	x24, x23, [sp, #48]
   1af70:	ldp	x26, x25, [sp, #32]
   1af74:	ldr	x27, [sp, #16]
   1af78:	ldp	x29, x30, [sp], #96
   1af7c:	ret
   1af80:	add	x8, x8, #0x2
   1af84:	mov	x20, xzr
   1af88:	str	x8, [x21]
   1af8c:	subs	w26, w26, #0x1
   1af90:	mov	x22, x21
   1af94:	b.eq	1af54 <__gmpz_lucnum_ui@@Base+0x224>  // b.none
   1af98:	mov	x21, x24
   1af9c:	mov	x0, x21
   1afa0:	mov	x1, x22
   1afa4:	mov	x2, x23
   1afa8:	mov	x24, x22
   1afac:	bl	ca90 <__gmpn_sqr@plt>
   1afb0:	add	x8, x21, x23, lsl #4
   1afb4:	ldur	x9, [x8, #-8]
   1afb8:	ldr	x8, [x21]
   1afbc:	lsl	x10, x23, #1
   1afc0:	cmp	x9, #0x0
   1afc4:	cset	w9, eq  // eq = none
   1afc8:	sub	x23, x10, x9
   1afcc:	tbnz	w20, #0, 1af80 <__gmpz_lucnum_ui@@Base+0x250>
   1afd0:	sub	x9, x8, #0x2
   1afd4:	cmp	x8, #0x1
   1afd8:	str	x9, [x21]
   1afdc:	b.hi	1af8c <__gmpz_lucnum_ui@@Base+0x25c>  // b.pmore
   1afe0:	add	x8, x21, #0x8
   1afe4:	ldr	x9, [x8]
   1afe8:	sub	x10, x9, #0x1
   1afec:	str	x10, [x8], #8
   1aff0:	cbz	x9, 1afe4 <__gmpz_lucnum_ui@@Base+0x2b4>
   1aff4:	b	1af8c <__gmpz_lucnum_ui@@Base+0x25c>
   1aff8:	mov	w1, #0x1                   	// #1
   1affc:	mov	x0, x19
   1b000:	bl	c1c0 <__gmpz_realloc@plt>
   1b004:	b	1ad88 <__gmpz_lucnum_ui@@Base+0x58>
   1b008:	mov	x0, x19
   1b00c:	mov	x1, x22
   1b010:	bl	c1c0 <__gmpz_realloc@plt>
   1b014:	mov	x21, x0
   1b018:	b	1adbc <__gmpz_lucnum_ui@@Base+0x8c>
   1b01c:	add	x0, x29, #0x18
   1b020:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b024:	b	1ade8 <__gmpz_lucnum_ui@@Base+0xb8>
   1b028:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1b02c:	b	1af60 <__gmpz_lucnum_ui@@Base+0x230>
   1b030:	add	x0, x29, #0x18
   1b034:	mov	x23, x2
   1b038:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b03c:	mov	x2, x23
   1b040:	mov	x23, x0
   1b044:	b	1ae80 <__gmpz_lucnum_ui@@Base+0x150>

000000000001b048 <__gmpz_lucnum2_ui@@Base>:
   1b048:	stp	x29, x30, [sp, #-80]!
   1b04c:	stp	x22, x21, [sp, #48]
   1b050:	stp	x20, x19, [sp, #64]
   1b054:	mov	x20, x2
   1b058:	mov	x19, x1
   1b05c:	cmp	x2, #0x5c
   1b060:	mov	x21, x0
   1b064:	str	x25, [sp, #16]
   1b068:	stp	x24, x23, [sp, #32]
   1b06c:	mov	x29, sp
   1b070:	b.hi	1b0e4 <__gmpz_lucnum2_ui@@Base+0x9c>  // b.pmore
   1b074:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1b078:	ldr	x8, [x8, #3808]
   1b07c:	sbfiz	x9, x20, #3, #32
   1b080:	ldr	w10, [x21]
   1b084:	add	x11, x8, x20, lsl #3
   1b088:	ldr	x23, [x11, #8]
   1b08c:	ldr	x22, [x8, x9]
   1b090:	cmp	w10, #0x0
   1b094:	add	x24, x23, x22, lsl #1
   1b098:	b.le	1b1c4 <__gmpz_lucnum2_ui@@Base+0x17c>
   1b09c:	ldr	x0, [x21, #8]
   1b0a0:	mov	w8, #0x1                   	// #1
   1b0a4:	str	x24, [x0]
   1b0a8:	str	w8, [x21, #4]
   1b0ac:	ldr	w9, [x19]
   1b0b0:	lsl	x8, x23, #1
   1b0b4:	sub	x8, x8, x22
   1b0b8:	cmp	x20, #0x0
   1b0bc:	csinc	x21, x8, xzr, ne  // ne = any
   1b0c0:	cmp	w9, #0x0
   1b0c4:	b.le	1b1d4 <__gmpz_lucnum2_ui@@Base+0x18c>
   1b0c8:	ldr	x0, [x19, #8]
   1b0cc:	cmp	x20, #0x0
   1b0d0:	mov	w8, #0xffffffff            	// #-1
   1b0d4:	cneg	w8, w8, ne  // ne = any
   1b0d8:	str	x21, [x0]
   1b0dc:	str	w8, [x19, #4]
   1b0e0:	b	1b1a8 <__gmpz_lucnum2_ui@@Base+0x160>
   1b0e4:	lsr	x8, x20, #5
   1b0e8:	mov	w9, #0x17                  	// #23
   1b0ec:	mul	x8, x8, x9
   1b0f0:	lsr	x23, x8, #6
   1b0f4:	lsl	x8, x23, #3
   1b0f8:	cmp	x23, #0xfdc
   1b0fc:	add	x1, x8, #0x20
   1b100:	str	xzr, [x29, #24]
   1b104:	b.hi	1b1e4 <__gmpz_lucnum2_ui@@Base+0x19c>  // b.pmore
   1b108:	add	x9, x1, #0xf
   1b10c:	mov	x8, sp
   1b110:	and	x9, x9, #0x7ffffffffffffff0
   1b114:	sub	x22, x8, x9
   1b118:	mov	sp, x22
   1b11c:	ldrsw	x8, [x21]
   1b120:	add	x24, x23, #0x5
   1b124:	cmp	x24, x8
   1b128:	b.gt	1b1f4 <__gmpz_lucnum2_ui@@Base+0x1ac>
   1b12c:	ldr	x23, [x21, #8]
   1b130:	ldrsw	x8, [x19]
   1b134:	cmp	x24, x8
   1b138:	b.gt	1b208 <__gmpz_lucnum2_ui@@Base+0x1c0>
   1b13c:	ldr	x24, [x19, #8]
   1b140:	mov	x0, x24
   1b144:	mov	x1, x22
   1b148:	mov	x2, x20
   1b14c:	bl	d240 <__gmpn_fib2_ui@plt>
   1b150:	mov	x20, x0
   1b154:	mov	x0, x23
   1b158:	mov	x1, x24
   1b15c:	mov	x2, x22
   1b160:	mov	x3, x20
   1b164:	bl	ce00 <__gmpn_addlsh1_n@plt>
   1b168:	lsl	x25, x20, #3
   1b16c:	cmp	x0, #0x0
   1b170:	str	x0, [x23, x25]
   1b174:	cinc	w8, w20, ne  // ne = any
   1b178:	mov	x0, x24
   1b17c:	mov	x1, x22
   1b180:	mov	x2, x24
   1b184:	mov	x3, x20
   1b188:	str	w8, [x21, #4]
   1b18c:	bl	d260 <__gmpn_rsblsh1_n@plt>
   1b190:	cmp	x0, #0x0
   1b194:	cinc	w8, w20, ne  // ne = any
   1b198:	str	x0, [x24, x25]
   1b19c:	str	w8, [x19, #4]
   1b1a0:	ldr	x0, [x29, #24]
   1b1a4:	cbnz	x0, 1b21c <__gmpz_lucnum2_ui@@Base+0x1d4>
   1b1a8:	mov	sp, x29
   1b1ac:	ldp	x20, x19, [sp, #64]
   1b1b0:	ldp	x22, x21, [sp, #48]
   1b1b4:	ldp	x24, x23, [sp, #32]
   1b1b8:	ldr	x25, [sp, #16]
   1b1bc:	ldp	x29, x30, [sp], #80
   1b1c0:	ret
   1b1c4:	mov	w1, #0x1                   	// #1
   1b1c8:	mov	x0, x21
   1b1cc:	bl	c1c0 <__gmpz_realloc@plt>
   1b1d0:	b	1b0a0 <__gmpz_lucnum2_ui@@Base+0x58>
   1b1d4:	mov	w1, #0x1                   	// #1
   1b1d8:	mov	x0, x19
   1b1dc:	bl	c1c0 <__gmpz_realloc@plt>
   1b1e0:	b	1b0cc <__gmpz_lucnum2_ui@@Base+0x84>
   1b1e4:	add	x0, x29, #0x18
   1b1e8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b1ec:	mov	x22, x0
   1b1f0:	b	1b11c <__gmpz_lucnum2_ui@@Base+0xd4>
   1b1f4:	mov	x0, x21
   1b1f8:	mov	x1, x24
   1b1fc:	bl	c1c0 <__gmpz_realloc@plt>
   1b200:	mov	x23, x0
   1b204:	b	1b130 <__gmpz_lucnum2_ui@@Base+0xe8>
   1b208:	mov	x0, x19
   1b20c:	mov	x1, x24
   1b210:	bl	c1c0 <__gmpz_realloc@plt>
   1b214:	mov	x24, x0
   1b218:	b	1b140 <__gmpz_lucnum2_ui@@Base+0xf8>
   1b21c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1b220:	b	1b1a8 <__gmpz_lucnum2_ui@@Base+0x160>

000000000001b224 <__gmpz_millerrabin@@Base>:
   1b224:	stp	x29, x30, [sp, #-48]!
   1b228:	stp	x22, x21, [sp, #16]
   1b22c:	stp	x20, x19, [sp, #32]
   1b230:	mov	x29, sp
   1b234:	sub	sp, sp, #0x70
   1b238:	stur	xzr, [x29, #-104]
   1b23c:	ldrsw	x8, [x0, #4]
   1b240:	mov	w20, w1
   1b244:	mov	w9, #0x7f00                	// #32512
   1b248:	mov	x19, x0
   1b24c:	add	x8, x8, #0x1
   1b250:	lsl	x1, x8, #3
   1b254:	cmp	x1, x9
   1b258:	stur	w8, [x29, #-16]
   1b25c:	b.hi	1b454 <__gmpz_millerrabin@@Base+0x230>  // b.pmore
   1b260:	add	x9, x1, #0xf
   1b264:	mov	x8, sp
   1b268:	and	x9, x9, #0xfffffffffffffff0
   1b26c:	sub	x0, x8, x9
   1b270:	mov	sp, x0
   1b274:	stur	x0, [x29, #-8]
   1b278:	sub	x0, x29, #0x10
   1b27c:	mov	w2, #0x1                   	// #1
   1b280:	mov	x1, x19
   1b284:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   1b288:	ldrsw	x8, [x19, #4]
   1b28c:	mov	w9, #0x7f00                	// #32512
   1b290:	add	x8, x8, #0x1
   1b294:	lsl	x1, x8, #3
   1b298:	cmp	x1, x9
   1b29c:	stur	w8, [x29, #-32]
   1b2a0:	b.hi	1b460 <__gmpz_millerrabin@@Base+0x23c>  // b.pmore
   1b2a4:	add	x9, x1, #0xf
   1b2a8:	mov	x8, sp
   1b2ac:	and	x9, x9, #0xfffffffffffffff0
   1b2b0:	sub	x0, x8, x9
   1b2b4:	mov	sp, x0
   1b2b8:	stur	x0, [x29, #-24]
   1b2bc:	ldrsw	x8, [x19, #4]
   1b2c0:	lsl	w9, w8, #1
   1b2c4:	lsl	x1, x8, #4
   1b2c8:	mov	w8, #0x7f00                	// #32512
   1b2cc:	cmp	x1, x8
   1b2d0:	stur	w9, [x29, #-48]
   1b2d4:	b.hi	1b46c <__gmpz_millerrabin@@Base+0x248>  // b.pmore
   1b2d8:	add	x9, x1, #0xf
   1b2dc:	mov	x8, sp
   1b2e0:	and	x9, x9, #0xfffffffffffffff0
   1b2e4:	sub	x0, x8, x9
   1b2e8:	mov	sp, x0
   1b2ec:	stur	x0, [x29, #-40]
   1b2f0:	ldrsw	x8, [x19, #4]
   1b2f4:	mov	w9, #0x7f00                	// #32512
   1b2f8:	lsl	x1, x8, #3
   1b2fc:	cmp	x1, x9
   1b300:	stur	w8, [x29, #-64]
   1b304:	b.hi	1b478 <__gmpz_millerrabin@@Base+0x254>  // b.pmore
   1b308:	add	x9, x1, #0xf
   1b30c:	mov	x8, sp
   1b310:	and	x9, x9, #0xfffffffffffffff0
   1b314:	sub	x0, x8, x9
   1b318:	mov	sp, x0
   1b31c:	stur	x0, [x29, #-56]
   1b320:	sub	x0, x29, #0x10
   1b324:	mov	x1, xzr
   1b328:	bl	c050 <__gmpz_scan1@plt>
   1b32c:	mov	x21, x0
   1b330:	sub	x0, x29, #0x40
   1b334:	sub	x1, x29, #0x10
   1b338:	mov	x2, x21
   1b33c:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   1b340:	sub	x0, x29, #0x20
   1b344:	mov	w1, #0x2                   	// #2
   1b348:	add	x21, x21, #0x1
   1b34c:	bl	c2c0 <__gmpz_set_ui@plt>
   1b350:	sub	x1, x29, #0x20
   1b354:	sub	x2, x29, #0x30
   1b358:	sub	x3, x29, #0x40
   1b35c:	mov	x0, x19
   1b360:	mov	x4, x21
   1b364:	bl	1b48c <__gmpz_millerrabin@@Base+0x268>
   1b368:	cbz	w0, 1b38c <__gmpz_millerrabin@@Base+0x168>
   1b36c:	sub	x1, x29, #0x20
   1b370:	sub	x2, x29, #0x30
   1b374:	mov	x0, x19
   1b378:	bl	cb80 <__gmpz_stronglucas@plt>
   1b37c:	cmp	w0, #0x0
   1b380:	cset	w22, ne  // ne = any
   1b384:	cbnz	w22, 1b394 <__gmpz_millerrabin@@Base+0x170>
   1b388:	b	1b434 <__gmpz_millerrabin@@Base+0x210>
   1b38c:	mov	w22, wzr
   1b390:	cbz	w22, 1b434 <__gmpz_millerrabin@@Base+0x210>
   1b394:	ldr	x8, [x19, #8]
   1b398:	ldr	w9, [x19, #4]
   1b39c:	ldr	x8, [x8]
   1b3a0:	lsr	x8, x8, #46
   1b3a4:	cmp	x8, #0x13
   1b3a8:	cset	w8, cc  // cc = lo, ul, last
   1b3ac:	cmp	w9, w8
   1b3b0:	b.ne	1b3bc <__gmpz_millerrabin@@Base+0x198>  // b.any
   1b3b4:	mov	w22, #0x2                   	// #2
   1b3b8:	b	1b434 <__gmpz_millerrabin@@Base+0x210>
   1b3bc:	cmp	w20, #0x19
   1b3c0:	b.lt	1b434 <__gmpz_millerrabin@@Base+0x210>  // b.tstop
   1b3c4:	sub	x0, x29, #0x10
   1b3c8:	sub	x1, x29, #0x10
   1b3cc:	mov	w2, #0x2                   	// #2
   1b3d0:	sub	w20, w20, #0x18
   1b3d4:	bl	c270 <__gmpz_sub_ui@plt>
   1b3d8:	sub	x0, x29, #0x60
   1b3dc:	bl	c940 <__gmp_randinit_default@plt>
   1b3e0:	sub	x0, x29, #0x20
   1b3e4:	sub	x1, x29, #0x60
   1b3e8:	sub	x2, x29, #0x10
   1b3ec:	bl	caa0 <__gmpz_urandomm@plt>
   1b3f0:	sub	x0, x29, #0x20
   1b3f4:	sub	x1, x29, #0x20
   1b3f8:	mov	w2, #0x3                   	// #3
   1b3fc:	bl	ca60 <__gmpz_add_ui@plt>
   1b400:	sub	x1, x29, #0x20
   1b404:	sub	x2, x29, #0x30
   1b408:	sub	x3, x29, #0x40
   1b40c:	mov	x0, x19
   1b410:	mov	x4, x21
   1b414:	bl	1b48c <__gmpz_millerrabin@@Base+0x268>
   1b418:	cmp	w20, #0x2
   1b41c:	mov	w22, w0
   1b420:	b.lt	1b42c <__gmpz_millerrabin@@Base+0x208>  // b.tstop
   1b424:	sub	w20, w20, #0x1
   1b428:	cbnz	w22, 1b3e0 <__gmpz_millerrabin@@Base+0x1bc>
   1b42c:	sub	x0, x29, #0x60
   1b430:	bl	c600 <__gmp_randclear@plt>
   1b434:	ldur	x0, [x29, #-104]
   1b438:	cbnz	x0, 1b484 <__gmpz_millerrabin@@Base+0x260>
   1b43c:	mov	w0, w22
   1b440:	mov	sp, x29
   1b444:	ldp	x20, x19, [sp, #32]
   1b448:	ldp	x22, x21, [sp, #16]
   1b44c:	ldp	x29, x30, [sp], #48
   1b450:	ret
   1b454:	sub	x0, x29, #0x68
   1b458:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b45c:	b	1b274 <__gmpz_millerrabin@@Base+0x50>
   1b460:	sub	x0, x29, #0x68
   1b464:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b468:	b	1b2b8 <__gmpz_millerrabin@@Base+0x94>
   1b46c:	sub	x0, x29, #0x68
   1b470:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b474:	b	1b2ec <__gmpz_millerrabin@@Base+0xc8>
   1b478:	sub	x0, x29, #0x68
   1b47c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b480:	b	1b31c <__gmpz_millerrabin@@Base+0xf8>
   1b484:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1b488:	b	1b43c <__gmpz_millerrabin@@Base+0x218>
   1b48c:	stp	x29, x30, [sp, #-64]!
   1b490:	stp	x22, x21, [sp, #32]
   1b494:	mov	x21, x0
   1b498:	stp	x20, x19, [sp, #48]
   1b49c:	mov	x20, x2
   1b4a0:	mov	x0, x2
   1b4a4:	mov	x2, x3
   1b4a8:	mov	x3, x21
   1b4ac:	str	x23, [sp, #16]
   1b4b0:	mov	x29, sp
   1b4b4:	mov	x19, x4
   1b4b8:	bl	c4d0 <__gmpz_powm@plt>
   1b4bc:	mov	w1, #0x1                   	// #1
   1b4c0:	mov	x0, x20
   1b4c4:	mov	w22, #0x1                   	// #1
   1b4c8:	bl	d3d0 <__gmpz_cmp_ui@plt>
   1b4cc:	cbz	w0, 1b4e4 <__gmpz_millerrabin@@Base+0x2c0>
   1b4d0:	mov	x0, x20
   1b4d4:	mov	x1, x21
   1b4d8:	bl	1b55c <__gmpz_millerrabin@@Base+0x338>
   1b4dc:	cbz	w0, 1b4fc <__gmpz_millerrabin@@Base+0x2d8>
   1b4e0:	mov	w22, #0x1                   	// #1
   1b4e4:	mov	w0, w22
   1b4e8:	ldp	x20, x19, [sp, #48]
   1b4ec:	ldp	x22, x21, [sp, #32]
   1b4f0:	ldr	x23, [sp, #16]
   1b4f4:	ldp	x29, x30, [sp], #64
   1b4f8:	ret
   1b4fc:	cmp	x19, #0x2
   1b500:	b.cc	1b554 <__gmpz_millerrabin@@Base+0x330>  // b.lo, b.ul, b.last
   1b504:	mov	w23, #0x2                   	// #2
   1b508:	mov	w2, #0x2                   	// #2
   1b50c:	mov	x0, x20
   1b510:	mov	x1, x20
   1b514:	mov	x3, x21
   1b518:	bl	d4a0 <__gmpz_powm_ui@plt>
   1b51c:	mov	x0, x20
   1b520:	mov	x1, x21
   1b524:	bl	1b55c <__gmpz_millerrabin@@Base+0x338>
   1b528:	cbnz	w0, 1b4e0 <__gmpz_millerrabin@@Base+0x2bc>
   1b52c:	mov	w1, #0x1                   	// #1
   1b530:	mov	x0, x20
   1b534:	bl	d3d0 <__gmpz_cmp_ui@plt>
   1b538:	cmp	w0, #0x1
   1b53c:	mov	w22, wzr
   1b540:	b.lt	1b4e4 <__gmpz_millerrabin@@Base+0x2c0>  // b.tstop
   1b544:	cmp	x23, x19
   1b548:	add	x23, x23, #0x1
   1b54c:	b.cc	1b508 <__gmpz_millerrabin@@Base+0x2e4>  // b.lo, b.ul, b.last
   1b550:	b	1b4e4 <__gmpz_millerrabin@@Base+0x2c0>
   1b554:	mov	w22, wzr
   1b558:	b	1b4e4 <__gmpz_millerrabin@@Base+0x2c0>
   1b55c:	ldrsw	x8, [x1, #4]
   1b560:	ldr	w9, [x0, #4]
   1b564:	cmp	w9, w8
   1b568:	b.ne	1b5b0 <__gmpz_millerrabin@@Base+0x38c>  // b.any
   1b56c:	ldr	x9, [x0, #8]
   1b570:	ldr	x10, [x1, #8]
   1b574:	ldr	x11, [x9]
   1b578:	ldr	x12, [x10]
   1b57c:	eor	x11, x11, #0x1
   1b580:	cmp	x11, x12
   1b584:	b.ne	1b5b0 <__gmpz_millerrabin@@Base+0x38c>  // b.any
   1b588:	sub	x9, x9, #0x8
   1b58c:	sub	x10, x10, #0x8
   1b590:	cmp	x8, #0x2
   1b594:	b.lt	1b5b8 <__gmpz_millerrabin@@Base+0x394>  // b.tstop
   1b598:	lsl	x11, x8, #3
   1b59c:	ldr	x12, [x9, x11]
   1b5a0:	ldr	x11, [x10, x11]
   1b5a4:	sub	x8, x8, #0x1
   1b5a8:	cmp	x12, x11
   1b5ac:	b.eq	1b590 <__gmpz_millerrabin@@Base+0x36c>  // b.none
   1b5b0:	mov	w0, wzr
   1b5b4:	ret
   1b5b8:	mov	w0, #0x1                   	// #1
   1b5bc:	ret

000000000001b5c0 <__gmpz_mod@@Base>:
   1b5c0:	stp	x29, x30, [sp, #-48]!
   1b5c4:	stp	x22, x21, [sp, #16]
   1b5c8:	stp	x20, x19, [sp, #32]
   1b5cc:	mov	x29, sp
   1b5d0:	sub	sp, sp, #0x20
   1b5d4:	stur	xzr, [x29, #-24]
   1b5d8:	ldr	w8, [x2, #4]
   1b5dc:	mov	x22, x2
   1b5e0:	mov	x19, x0
   1b5e4:	mov	x20, x1
   1b5e8:	cmp	w8, #0x0
   1b5ec:	cneg	w21, w8, mi  // mi = first
   1b5f0:	cmp	x0, x2
   1b5f4:	b.eq	1b604 <__gmpz_mod@@Base+0x44>  // b.none
   1b5f8:	ldr	x8, [x22, #8]
   1b5fc:	stur	x8, [x29, #-8]
   1b600:	b	1b634 <__gmpz_mod@@Base+0x74>
   1b604:	cmp	w21, #0xfe0
   1b608:	lsl	x1, x21, #3
   1b60c:	b.hi	1b684 <__gmpz_mod@@Base+0xc4>  // b.pmore
   1b610:	add	x9, x1, #0xf
   1b614:	mov	x8, sp
   1b618:	and	x9, x9, #0xffffffff0
   1b61c:	sub	x0, x8, x9
   1b620:	mov	sp, x0
   1b624:	stur	x0, [x29, #-8]
   1b628:	ldr	x1, [x22, #8]
   1b62c:	mov	x2, x21
   1b630:	bl	cc10 <__gmpn_copyi@plt>
   1b634:	sub	x2, x29, #0x10
   1b638:	mov	x0, x19
   1b63c:	mov	x1, x20
   1b640:	stur	w21, [x29, #-12]
   1b644:	bl	cc40 <__gmpz_tdiv_r@plt>
   1b648:	ldr	w8, [x19, #4]
   1b64c:	tbz	w8, #31, 1b660 <__gmpz_mod@@Base+0xa0>
   1b650:	sub	x2, x29, #0x10
   1b654:	mov	x0, x19
   1b658:	mov	x1, x19
   1b65c:	bl	d160 <__gmpz_add@plt>
   1b660:	ldur	x0, [x29, #-24]
   1b664:	cbnz	x0, 1b67c <__gmpz_mod@@Base+0xbc>
   1b668:	mov	sp, x29
   1b66c:	ldp	x20, x19, [sp, #32]
   1b670:	ldp	x22, x21, [sp, #16]
   1b674:	ldp	x29, x30, [sp], #48
   1b678:	ret
   1b67c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1b680:	b	1b668 <__gmpz_mod@@Base+0xa8>
   1b684:	sub	x0, x29, #0x18
   1b688:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b68c:	b	1b624 <__gmpz_mod@@Base+0x64>

000000000001b690 <__gmpz_mul@@Base>:
   1b690:	stp	x29, x30, [sp, #-96]!
   1b694:	stp	x28, x27, [sp, #16]
   1b698:	stp	x26, x25, [sp, #32]
   1b69c:	stp	x24, x23, [sp, #48]
   1b6a0:	stp	x22, x21, [sp, #64]
   1b6a4:	stp	x20, x19, [sp, #80]
   1b6a8:	mov	x29, sp
   1b6ac:	sub	sp, sp, #0x10
   1b6b0:	ldrsw	x8, [x1, #4]
   1b6b4:	ldrsw	x9, [x2, #4]
   1b6b8:	mov	x19, x0
   1b6bc:	cmp	x8, #0x0
   1b6c0:	cneg	x10, x8, mi  // mi = first
   1b6c4:	cmp	x9, #0x0
   1b6c8:	cneg	x11, x9, mi  // mi = first
   1b6cc:	cmp	x10, x11
   1b6d0:	csel	x21, x10, x11, lt  // lt = tstop
   1b6d4:	csel	x20, x11, x10, lt  // lt = tstop
   1b6d8:	csel	x22, x1, x2, lt  // lt = tstop
   1b6dc:	csel	x23, x2, x1, lt  // lt = tstop
   1b6e0:	cmp	x21, #0x1
   1b6e4:	eor	w26, w9, w8
   1b6e8:	b.eq	1b6f8 <__gmpz_mul@@Base+0x68>  // b.none
   1b6ec:	cbnz	x21, 1b740 <__gmpz_mul@@Base+0xb0>
   1b6f0:	str	wzr, [x19, #4]
   1b6f4:	b	1b8bc <__gmpz_mul@@Base+0x22c>
   1b6f8:	ldrsw	x8, [x19]
   1b6fc:	cmp	x20, x8
   1b700:	b.ge	1b8dc <__gmpz_mul@@Base+0x24c>  // b.tcont
   1b704:	ldr	x21, [x19, #8]
   1b708:	ldr	x8, [x22, #8]
   1b70c:	ldr	x1, [x23, #8]
   1b710:	mov	x0, x21
   1b714:	mov	x2, x20
   1b718:	ldr	x3, [x8]
   1b71c:	bl	d670 <__gmpn_mul_1@plt>
   1b720:	cmp	x0, #0x0
   1b724:	cinc	x8, x20, ne  // ne = any
   1b728:	neg	w9, w8
   1b72c:	cmp	w26, #0x0
   1b730:	csel	x8, x8, x9, ge  // ge = tcont
   1b734:	str	x0, [x21, x20, lsl #3]
   1b738:	str	w8, [x19, #4]
   1b73c:	b	1b8bc <__gmpz_mul@@Base+0x22c>
   1b740:	stur	xzr, [x29, #-8]
   1b744:	ldrsw	x27, [x19]
   1b748:	ldr	x23, [x23, #8]
   1b74c:	ldr	x24, [x22, #8]
   1b750:	ldr	x22, [x19, #8]
   1b754:	add	x28, x20, x21
   1b758:	cmp	x28, x27
   1b75c:	b.le	1b7d8 <__gmpz_mul@@Base+0x148>
   1b760:	cbz	w27, 1b78c <__gmpz_mul@@Base+0xfc>
   1b764:	cmp	x22, x23
   1b768:	b.eq	1b790 <__gmpz_mul@@Base+0x100>  // b.none
   1b76c:	cmp	x22, x24
   1b770:	b.eq	1b790 <__gmpz_mul@@Base+0x100>  // b.none
   1b774:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1b778:	ldr	x8, [x8, #4016]
   1b77c:	lsl	x1, x27, #3
   1b780:	mov	x0, x22
   1b784:	ldr	x8, [x8]
   1b788:	blr	x8
   1b78c:	mov	x22, xzr
   1b790:	str	w28, [x19]
   1b794:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1b798:	ldr	x8, [x8, #3840]
   1b79c:	lsl	x0, x28, #3
   1b7a0:	mov	x25, x22
   1b7a4:	ldr	x8, [x8]
   1b7a8:	blr	x8
   1b7ac:	mov	x22, x0
   1b7b0:	str	x0, [x19, #8]
   1b7b4:	cmp	x23, x24
   1b7b8:	b.eq	1b864 <__gmpz_mul@@Base+0x1d4>  // b.none
   1b7bc:	mov	x0, x22
   1b7c0:	mov	x1, x23
   1b7c4:	mov	x2, x20
   1b7c8:	mov	x3, x24
   1b7cc:	mov	x4, x21
   1b7d0:	bl	cea0 <__gmpn_mul@plt>
   1b7d4:	b	1b87c <__gmpz_mul@@Base+0x1ec>
   1b7d8:	cmp	x22, x23
   1b7dc:	b.eq	1b81c <__gmpz_mul@@Base+0x18c>  // b.none
   1b7e0:	cmp	x22, x24
   1b7e4:	b.ne	1b858 <__gmpz_mul@@Base+0x1c8>  // b.any
   1b7e8:	lsl	x1, x21, #3
   1b7ec:	mov	w8, #0x7f00                	// #32512
   1b7f0:	cmp	x1, x8
   1b7f4:	b.hi	1b908 <__gmpz_mul@@Base+0x278>  // b.pmore
   1b7f8:	add	x9, x1, #0xf
   1b7fc:	mov	x8, sp
   1b800:	and	x9, x9, #0xfffffffffffffff0
   1b804:	sub	x24, x8, x9
   1b808:	mov	sp, x24
   1b80c:	mov	x0, x24
   1b810:	mov	x1, x22
   1b814:	mov	x2, x21
   1b818:	b	1b854 <__gmpz_mul@@Base+0x1c4>
   1b81c:	lsl	x1, x20, #3
   1b820:	mov	w8, #0x7f00                	// #32512
   1b824:	cmp	x1, x8
   1b828:	b.hi	1b8f8 <__gmpz_mul@@Base+0x268>  // b.pmore
   1b82c:	add	x9, x1, #0xf
   1b830:	mov	x8, sp
   1b834:	and	x9, x9, #0xfffffffffffffff0
   1b838:	sub	x23, x8, x9
   1b83c:	mov	sp, x23
   1b840:	cmp	x22, x24
   1b844:	csel	x24, x23, x24, eq  // eq = none
   1b848:	mov	x0, x23
   1b84c:	mov	x1, x22
   1b850:	mov	x2, x20
   1b854:	bl	cc10 <__gmpn_copyi@plt>
   1b858:	mov	x25, xzr
   1b85c:	cmp	x23, x24
   1b860:	b.ne	1b7bc <__gmpz_mul@@Base+0x12c>  // b.any
   1b864:	mov	x0, x22
   1b868:	mov	x1, x23
   1b86c:	mov	x2, x20
   1b870:	bl	ca90 <__gmpn_sqr@plt>
   1b874:	add	x8, x22, x28, lsl #3
   1b878:	ldur	x0, [x8, #-8]
   1b87c:	cmp	x0, #0x0
   1b880:	cset	w8, eq  // eq = none
   1b884:	sub	x8, x28, x8
   1b888:	neg	w9, w8
   1b88c:	cmp	w26, #0x0
   1b890:	csel	x8, x9, x8, lt  // lt = tstop
   1b894:	str	w8, [x19, #4]
   1b898:	cbz	x25, 1b8b4 <__gmpz_mul@@Base+0x224>
   1b89c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1b8a0:	ldr	x8, [x8, #4016]
   1b8a4:	lsl	x1, x27, #3
   1b8a8:	mov	x0, x25
   1b8ac:	ldr	x8, [x8]
   1b8b0:	blr	x8
   1b8b4:	ldur	x0, [x29, #-8]
   1b8b8:	cbnz	x0, 1b8f0 <__gmpz_mul@@Base+0x260>
   1b8bc:	mov	sp, x29
   1b8c0:	ldp	x20, x19, [sp, #80]
   1b8c4:	ldp	x22, x21, [sp, #64]
   1b8c8:	ldp	x24, x23, [sp, #48]
   1b8cc:	ldp	x26, x25, [sp, #32]
   1b8d0:	ldp	x28, x27, [sp, #16]
   1b8d4:	ldp	x29, x30, [sp], #96
   1b8d8:	ret
   1b8dc:	add	x1, x20, #0x1
   1b8e0:	mov	x0, x19
   1b8e4:	bl	c1c0 <__gmpz_realloc@plt>
   1b8e8:	mov	x21, x0
   1b8ec:	b	1b708 <__gmpz_mul@@Base+0x78>
   1b8f0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1b8f4:	b	1b8bc <__gmpz_mul@@Base+0x22c>
   1b8f8:	sub	x0, x29, #0x8
   1b8fc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b900:	mov	x23, x0
   1b904:	b	1b840 <__gmpz_mul@@Base+0x1b0>
   1b908:	sub	x0, x29, #0x8
   1b90c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1b910:	mov	x24, x0
   1b914:	b	1b80c <__gmpz_mul@@Base+0x17c>

000000000001b918 <__gmpz_mul_2exp@@Base>:
   1b918:	stp	x29, x30, [sp, #-80]!
   1b91c:	stp	x24, x23, [sp, #32]
   1b920:	stp	x22, x21, [sp, #48]
   1b924:	stp	x20, x19, [sp, #64]
   1b928:	ldr	w8, [x1, #4]
   1b92c:	mov	x20, x1
   1b930:	mov	x19, x0
   1b934:	str	x25, [sp, #16]
   1b938:	cmp	w8, #0x0
   1b93c:	cneg	w22, w8, mi  // mi = first
   1b940:	mov	x29, sp
   1b944:	cbz	w22, 1b990 <__gmpz_mul_2exp@@Base+0x78>
   1b948:	ldrsw	x8, [x19]
   1b94c:	lsr	x25, x2, #6
   1b950:	add	x24, x25, x22
   1b954:	mov	x23, x2
   1b958:	cmp	x24, x8
   1b95c:	b.ge	1b9dc <__gmpz_mul_2exp@@Base+0xc4>  // b.tcont
   1b960:	ldr	x21, [x19, #8]
   1b964:	ldr	x1, [x20, #8]
   1b968:	ands	x3, x23, #0x3f
   1b96c:	add	x0, x21, x25, lsl #3
   1b970:	mov	x2, x22
   1b974:	b.eq	1b998 <__gmpz_mul_2exp@@Base+0x80>  // b.none
   1b978:	bl	c2d0 <__gmpn_lshift@plt>
   1b97c:	cmp	x0, #0x0
   1b980:	str	x0, [x21, x24, lsl #3]
   1b984:	cinc	x24, x24, ne  // ne = any
   1b988:	cbnz	x25, 1b9a0 <__gmpz_mul_2exp@@Base+0x88>
   1b98c:	b	1b9b0 <__gmpz_mul_2exp@@Base+0x98>
   1b990:	mov	x24, xzr
   1b994:	b	1b9b0 <__gmpz_mul_2exp@@Base+0x98>
   1b998:	bl	c130 <__gmpn_copyd@plt>
   1b99c:	cbz	x25, 1b9b0 <__gmpz_mul_2exp@@Base+0x98>
   1b9a0:	lsl	x2, x25, #3
   1b9a4:	mov	x0, x21
   1b9a8:	mov	w1, wzr
   1b9ac:	bl	c780 <memset@plt>
   1b9b0:	ldr	w8, [x20, #4]
   1b9b4:	neg	w9, w24
   1b9b8:	ldr	x25, [sp, #16]
   1b9bc:	cmp	w8, #0x0
   1b9c0:	csel	x8, x24, x9, ge  // ge = tcont
   1b9c4:	str	w8, [x19, #4]
   1b9c8:	ldp	x20, x19, [sp, #64]
   1b9cc:	ldp	x22, x21, [sp, #48]
   1b9d0:	ldp	x24, x23, [sp, #32]
   1b9d4:	ldp	x29, x30, [sp], #80
   1b9d8:	ret
   1b9dc:	add	x1, x24, #0x1
   1b9e0:	mov	x0, x19
   1b9e4:	bl	c1c0 <__gmpz_realloc@plt>
   1b9e8:	mov	x21, x0
   1b9ec:	b	1b964 <__gmpz_mul_2exp@@Base+0x4c>

000000000001b9f0 <__gmpz_mul_si@@Base>:
   1b9f0:	stp	x29, x30, [sp, #-80]!
   1b9f4:	stp	x20, x19, [sp, #64]
   1b9f8:	mov	x19, x0
   1b9fc:	mov	w8, wzr
   1ba00:	str	x25, [sp, #16]
   1ba04:	stp	x24, x23, [sp, #32]
   1ba08:	stp	x22, x21, [sp, #48]
   1ba0c:	mov	x29, sp
   1ba10:	cbz	x2, 1ba74 <__gmpz_mul_si@@Base+0x84>
   1ba14:	ldr	w9, [x1, #4]
   1ba18:	mov	x21, x1
   1ba1c:	cbz	w9, 1ba74 <__gmpz_mul_si@@Base+0x84>
   1ba20:	ldrsw	x8, [x19]
   1ba24:	sxtw	x25, w9
   1ba28:	cmp	x25, #0x0
   1ba2c:	cneg	x22, x25, mi  // mi = first
   1ba30:	cmp	x2, #0x0
   1ba34:	mov	x20, x2
   1ba38:	cneg	x23, x2, mi  // mi = first
   1ba3c:	cmp	x22, x8
   1ba40:	b.ge	1ba90 <__gmpz_mul_si@@Base+0xa0>  // b.tcont
   1ba44:	ldr	x24, [x19, #8]
   1ba48:	ldr	x1, [x21, #8]
   1ba4c:	mov	x0, x24
   1ba50:	mov	x2, x22
   1ba54:	mov	x3, x23
   1ba58:	bl	d670 <__gmpn_mul_1@plt>
   1ba5c:	cmp	x0, #0x0
   1ba60:	lsr	x8, x20, #63
   1ba64:	cinc	w9, w22, ne  // ne = any
   1ba68:	cmp	w8, w25, lsr #31
   1ba6c:	cneg	w8, w9, ne  // ne = any
   1ba70:	str	x0, [x24, x22, lsl #3]
   1ba74:	str	w8, [x19, #4]
   1ba78:	ldp	x20, x19, [sp, #64]
   1ba7c:	ldp	x22, x21, [sp, #48]
   1ba80:	ldp	x24, x23, [sp, #32]
   1ba84:	ldr	x25, [sp, #16]
   1ba88:	ldp	x29, x30, [sp], #80
   1ba8c:	ret
   1ba90:	add	x1, x22, #0x1
   1ba94:	mov	x0, x19
   1ba98:	bl	c1c0 <__gmpz_realloc@plt>
   1ba9c:	mov	x24, x0
   1baa0:	b	1ba48 <__gmpz_mul_si@@Base+0x58>

000000000001baa4 <__gmpz_mul_ui@@Base>:
   1baa4:	stp	x29, x30, [sp, #-64]!
   1baa8:	stp	x20, x19, [sp, #48]
   1baac:	mov	x19, x0
   1bab0:	mov	w8, wzr
   1bab4:	stp	x24, x23, [sp, #16]
   1bab8:	stp	x22, x21, [sp, #32]
   1babc:	mov	x29, sp
   1bac0:	cbz	x2, 1bb18 <__gmpz_mul_ui@@Base+0x74>
   1bac4:	ldr	w9, [x1, #4]
   1bac8:	mov	x21, x1
   1bacc:	cbz	w9, 1bb18 <__gmpz_mul_ui@@Base+0x74>
   1bad0:	ldrsw	x8, [x19]
   1bad4:	sxtw	x24, w9
   1bad8:	cmp	x24, #0x0
   1badc:	cneg	x22, x24, mi  // mi = first
   1bae0:	mov	x20, x2
   1bae4:	cmp	x22, x8
   1bae8:	b.ge	1bb30 <__gmpz_mul_ui@@Base+0x8c>  // b.tcont
   1baec:	ldr	x23, [x19, #8]
   1baf0:	ldr	x1, [x21, #8]
   1baf4:	mov	x0, x23
   1baf8:	mov	x2, x22
   1bafc:	mov	x3, x20
   1bb00:	bl	d670 <__gmpn_mul_1@plt>
   1bb04:	cmp	x0, #0x0
   1bb08:	cinc	w8, w22, ne  // ne = any
   1bb0c:	cmp	w24, #0x0
   1bb10:	cneg	w8, w8, lt  // lt = tstop
   1bb14:	str	x0, [x23, x22, lsl #3]
   1bb18:	str	w8, [x19, #4]
   1bb1c:	ldp	x20, x19, [sp, #48]
   1bb20:	ldp	x22, x21, [sp, #32]
   1bb24:	ldp	x24, x23, [sp, #16]
   1bb28:	ldp	x29, x30, [sp], #64
   1bb2c:	ret
   1bb30:	add	x1, x22, #0x1
   1bb34:	mov	x0, x19
   1bb38:	bl	c1c0 <__gmpz_realloc@plt>
   1bb3c:	mov	x23, x0
   1bb40:	b	1baf0 <__gmpz_mul_ui@@Base+0x4c>

000000000001bb44 <__gmpz_n_pow_ui@@Base>:
   1bb44:	stp	x29, x30, [sp, #-96]!
   1bb48:	stp	x28, x27, [sp, #16]
   1bb4c:	stp	x26, x25, [sp, #32]
   1bb50:	stp	x24, x23, [sp, #48]
   1bb54:	stp	x22, x21, [sp, #64]
   1bb58:	stp	x20, x19, [sp, #80]
   1bb5c:	mov	x29, sp
   1bb60:	sub	sp, sp, #0x40
   1bb64:	mov	x26, x0
   1bb68:	cbz	x3, 1bc34 <__gmpz_n_pow_ui@@Base+0xf0>
   1bb6c:	cbz	x2, 1bc50 <__gmpz_n_pow_ui@@Base+0x10c>
   1bb70:	ldr	x10, [x1]
   1bb74:	ldr	x8, [x26, #8]
   1bb78:	cmp	x2, #0x0
   1bb7c:	mov	x21, x3
   1bb80:	cset	w11, lt  // lt = tstop
   1bb84:	cneg	x22, x2, mi  // mi = first
   1bb88:	mov	x9, xzr
   1bb8c:	mov	x23, x1
   1bb90:	stur	w11, [x29, #-52]
   1bb94:	cbnz	x10, 1bba8 <__gmpz_n_pow_ui@@Base+0x64>
   1bb98:	ldr	x10, [x23, #8]!
   1bb9c:	add	x9, x9, x21
   1bba0:	sub	x22, x22, #0x1
   1bba4:	cbz	x10, 1bb98 <__gmpz_n_pow_ui@@Base+0x54>
   1bba8:	rbit	x11, x10
   1bbac:	clz	x25, x11
   1bbb0:	lsr	x24, x10, x25
   1bbb4:	mul	x10, x25, x21
   1bbb8:	cmp	x22, #0x2
   1bbbc:	add	x27, x9, x10, lsr #6
   1bbc0:	and	x28, x10, #0x3f
   1bbc4:	stur	xzr, [x29, #-24]
   1bbc8:	b.eq	1bc58 <__gmpz_n_pow_ui@@Base+0x114>  // b.none
   1bbcc:	cmp	x22, #0x1
   1bbd0:	b.eq	1bca0 <__gmpz_n_pow_ui@@Base+0x15c>  // b.none
   1bbd4:	cmp	x8, x1
   1bbd8:	b.eq	1bbe0 <__gmpz_n_pow_ui@@Base+0x9c>  // b.none
   1bbdc:	cbz	w25, 1bd00 <__gmpz_n_pow_ui@@Base+0x1bc>
   1bbe0:	lsl	x1, x22, #3
   1bbe4:	mov	w8, #0x7f00                	// #32512
   1bbe8:	cmp	x1, x8
   1bbec:	b.hi	1c030 <__gmpz_n_pow_ui@@Base+0x4ec>  // b.pmore
   1bbf0:	add	x9, x1, #0xf
   1bbf4:	mov	x8, sp
   1bbf8:	and	x9, x9, #0xfffffffffffffff0
   1bbfc:	sub	x24, x8, x9
   1bc00:	mov	sp, x24
   1bc04:	mov	x0, x24
   1bc08:	mov	x1, x23
   1bc0c:	mov	x2, x22
   1bc10:	cbz	w25, 1bcf8 <__gmpz_n_pow_ui@@Base+0x1b4>
   1bc14:	mov	w3, w25
   1bc18:	bl	c2f0 <__gmpn_rshift@plt>
   1bc1c:	add	x8, x24, x22, lsl #3
   1bc20:	ldur	x8, [x8, #-8]
   1bc24:	cmp	x8, #0x0
   1bc28:	cset	w8, eq  // eq = none
   1bc2c:	sub	x22, x22, x8
   1bc30:	b	1bcfc <__gmpz_n_pow_ui@@Base+0x1b8>
   1bc34:	ldr	w8, [x26]
   1bc38:	cmp	w8, #0x0
   1bc3c:	b.le	1c008 <__gmpz_n_pow_ui@@Base+0x4c4>
   1bc40:	ldr	x0, [x26, #8]
   1bc44:	mov	w8, #0x1                   	// #1
   1bc48:	str	x8, [x0]
   1bc4c:	b	1bf34 <__gmpz_n_pow_ui@@Base+0x3f0>
   1bc50:	mov	w8, wzr
   1bc54:	b	1bf34 <__gmpz_n_pow_ui@@Base+0x3f0>
   1bc58:	ldr	x8, [x23, #8]
   1bc5c:	neg	x9, x25
   1bc60:	cmp	w25, #0x0
   1bc64:	lsl	x9, x8, x9
   1bc68:	csel	x10, xzr, x9, eq  // eq = none
   1bc6c:	lsr	x9, x8, x25
   1bc70:	orr	x24, x10, x24
   1bc74:	cbz	x9, 1bc94 <__gmpz_n_pow_ui@@Base+0x150>
   1bc78:	sub	x23, x29, #0x10
   1bc7c:	stp	x24, x9, [x29, #-16]
   1bc80:	mov	w8, #0x1                   	// #1
   1bc84:	mov	w22, #0x2                   	// #2
   1bc88:	mov	x24, x9
   1bc8c:	tbz	w8, #0, 1bca0 <__gmpz_n_pow_ui@@Base+0x15c>
   1bc90:	b	1bd08 <__gmpz_n_pow_ui@@Base+0x1c4>
   1bc94:	mov	w22, #0x1                   	// #1
   1bc98:	mov	w8, wzr
   1bc9c:	tbnz	wzr, #0, 1bd08 <__gmpz_n_pow_ui@@Base+0x1c4>
   1bca0:	mov	w25, #0x1                   	// #1
   1bca4:	mov	x20, x21
   1bca8:	lsr	x8, x24, #32
   1bcac:	cbnz	x8, 1bcd0 <__gmpz_n_pow_ui@@Base+0x18c>
   1bcb0:	tst	x20, #0x1
   1bcb4:	csinc	x8, x24, xzr, ne  // ne = any
   1bcb8:	lsr	x20, x20, #1
   1bcbc:	mul	x25, x8, x25
   1bcc0:	cbz	x20, 1bcd0 <__gmpz_n_pow_ui@@Base+0x18c>
   1bcc4:	mul	x24, x24, x24
   1bcc8:	lsr	x8, x24, #32
   1bccc:	cbz	x8, 1bcb0 <__gmpz_n_pow_ui@@Base+0x16c>
   1bcd0:	cbz	x28, 1bd10 <__gmpz_n_pow_ui@@Base+0x1cc>
   1bcd4:	cmp	x25, #0x1
   1bcd8:	b.eq	1bd10 <__gmpz_n_pow_ui@@Base+0x1cc>  // b.none
   1bcdc:	neg	x8, x28
   1bce0:	lsr	x8, x25, x8
   1bce4:	cmp	x8, #0x0
   1bce8:	csel	x8, x28, xzr, eq  // eq = none
   1bcec:	csel	x28, xzr, x28, eq  // eq = none
   1bcf0:	lsl	x25, x25, x8
   1bcf4:	b	1bd10 <__gmpz_n_pow_ui@@Base+0x1cc>
   1bcf8:	bl	cc10 <__gmpn_copyi@plt>
   1bcfc:	mov	x23, x24
   1bd00:	add	x8, x23, x22, lsl #3
   1bd04:	ldur	x24, [x8, #-8]
   1bd08:	mov	w25, #0x1                   	// #1
   1bd0c:	mov	x20, x21
   1bd10:	clz	x8, x24
   1bd14:	lsl	x9, x22, #6
   1bd18:	sub	x8, x9, x8
   1bd1c:	mul	x8, x8, x20
   1bd20:	ldrsw	x9, [x26]
   1bd24:	lsr	x8, x8, #6
   1bd28:	add	x19, x8, #0x5
   1bd2c:	add	x1, x19, x27
   1bd30:	cmp	x1, x9
   1bd34:	stur	x26, [x29, #-32]
   1bd38:	b.gt	1bfec <__gmpz_n_pow_ui@@Base+0x4a8>
   1bd3c:	ldr	x26, [x26, #8]
   1bd40:	cbz	x27, 1bd54 <__gmpz_n_pow_ui@@Base+0x210>
   1bd44:	lsl	x2, x27, #3
   1bd48:	mov	x0, x26
   1bd4c:	mov	w1, wzr
   1bd50:	bl	c780 <memset@plt>
   1bd54:	add	x12, x26, x27, lsl #3
   1bd58:	stp	x28, x27, [x29, #-48]
   1bd5c:	cbz	x20, 1bdd4 <__gmpz_n_pow_ui@@Base+0x290>
   1bd60:	cmp	x22, #0x2
   1bd64:	mvn	w8, w20
   1bd68:	cset	w9, lt  // lt = tstop
   1bd6c:	and	x8, x8, #0x1
   1bd70:	orr	x8, x8, x9
   1bd74:	lsr	x8, x19, x8
   1bd78:	cmp	x8, #0xfe0
   1bd7c:	lsl	x1, x8, #3
   1bd80:	b.hi	1c018 <__gmpz_n_pow_ui@@Base+0x4d4>  // b.pmore
   1bd84:	add	x9, x1, #0xf
   1bd88:	mov	x8, sp
   1bd8c:	and	x9, x9, #0x7ffffffffffffff0
   1bd90:	sub	x27, x8, x9
   1bd94:	mov	sp, x27
   1bd98:	clz	x19, x20
   1bd9c:	mov	w8, #0x3e                  	// #62
   1bda0:	cmp	x22, #0x1
   1bda4:	sub	w28, w8, w19
   1bda8:	b.ne	1bde8 <__gmpz_n_pow_ui@@Base+0x2a4>  // b.any
   1bdac:	tst	w28, #0x1
   1bdb0:	csel	x26, x27, x12, eq  // eq = none
   1bdb4:	cmp	w19, #0x3f
   1bdb8:	str	x24, [x26]
   1bdbc:	b.ne	1be44 <__gmpz_n_pow_ui@@Base+0x300>  // b.any
   1bdc0:	mov	w27, #0x1                   	// #1
   1bdc4:	cmp	x25, #0x1
   1bdc8:	b.ne	1bed0 <__gmpz_n_pow_ui@@Base+0x38c>  // b.any
   1bdcc:	ldur	w25, [x29, #-52]
   1bdd0:	b	1bef4 <__gmpz_n_pow_ui@@Base+0x3b0>
   1bdd4:	str	x25, [x12]
   1bdd8:	ldur	w25, [x29, #-52]
   1bddc:	mov	w27, #0x1                   	// #1
   1bde0:	mov	x26, x12
   1bde4:	b	1bef4 <__gmpz_n_pow_ui@@Base+0x3b0>
   1bde8:	mov	w9, #0x6996                	// #27030
   1bdec:	mov	x8, xzr
   1bdf0:	movk	w9, #0x9669, lsl #16
   1bdf4:	mov	x10, x20
   1bdf8:	and	x11, x10, #0x1f
   1bdfc:	sxtw	x8, w8
   1be00:	lsr	x11, x9, x11
   1be04:	lsr	x10, x10, #5
   1be08:	eor	x8, x8, x11
   1be0c:	cbnz	x10, 1bdf8 <__gmpz_n_pow_ui@@Base+0x2b4>
   1be10:	eor	w24, w28, w8
   1be14:	tst	w24, #0x1
   1be18:	csel	x26, x12, x27, eq  // eq = none
   1be1c:	mov	x0, x26
   1be20:	mov	x1, x23
   1be24:	mov	x2, x22
   1be28:	stur	x12, [x29, #-64]
   1be2c:	bl	cc10 <__gmpn_copyi@plt>
   1be30:	ldur	w25, [x29, #-52]
   1be34:	cmp	w19, #0x3f
   1be38:	b.ne	1bf58 <__gmpz_n_pow_ui@@Base+0x414>  // b.any
   1be3c:	mov	x27, x22
   1be40:	b	1bef4 <__gmpz_n_pow_ui@@Base+0x3b0>
   1be44:	tst	w28, #0x1
   1be48:	mov	w8, #0x3f                  	// #63
   1be4c:	csel	x22, x12, x27, eq  // eq = none
   1be50:	sub	w19, w8, w19
   1be54:	mov	w27, #0x1                   	// #1
   1be58:	mov	x0, x26
   1be5c:	b	1be74 <__gmpz_n_pow_ui@@Base+0x330>
   1be60:	sub	w19, w19, #0x1
   1be64:	cmp	w19, #0x0
   1be68:	sub	x28, x28, #0x1
   1be6c:	mov	x0, x26
   1be70:	b.le	1bdc4 <__gmpz_n_pow_ui@@Base+0x280>
   1be74:	mov	x26, x22
   1be78:	mov	x22, x0
   1be7c:	mov	x0, x26
   1be80:	mov	x1, x22
   1be84:	mov	x2, x27
   1be88:	bl	ca90 <__gmpn_sqr@plt>
   1be8c:	add	x8, x26, x27, lsl #4
   1be90:	ldur	x8, [x8, #-8]
   1be94:	lsl	x9, x27, #1
   1be98:	lsr	x10, x20, x28
   1be9c:	cmp	x8, #0x0
   1bea0:	cset	w8, eq  // eq = none
   1bea4:	sub	x27, x9, x8
   1bea8:	tbz	w10, #0, 1be60 <__gmpz_n_pow_ui@@Base+0x31c>
   1beac:	mov	x0, x26
   1beb0:	mov	x1, x26
   1beb4:	mov	x2, x27
   1beb8:	mov	x3, x24
   1bebc:	bl	d670 <__gmpn_mul_1@plt>
   1bec0:	cmp	x0, #0x0
   1bec4:	str	x0, [x26, x27, lsl #3]
   1bec8:	cinc	x27, x27, ne  // ne = any
   1becc:	b	1be60 <__gmpz_n_pow_ui@@Base+0x31c>
   1bed0:	mov	x0, x26
   1bed4:	mov	x1, x26
   1bed8:	mov	x2, x27
   1bedc:	mov	x3, x25
   1bee0:	bl	d670 <__gmpn_mul_1@plt>
   1bee4:	ldur	w25, [x29, #-52]
   1bee8:	cmp	x0, #0x0
   1beec:	str	x0, [x26, x27, lsl #3]
   1bef0:	cinc	x27, x27, ne  // ne = any
   1bef4:	ldur	x0, [x29, #-24]
   1bef8:	cbnz	x0, 1c000 <__gmpz_n_pow_ui@@Base+0x4bc>
   1befc:	ldur	x3, [x29, #-48]
   1bf00:	and	w19, w21, w25
   1bf04:	cbz	x3, 1bf24 <__gmpz_n_pow_ui@@Base+0x3e0>
   1bf08:	mov	x0, x26
   1bf0c:	mov	x1, x26
   1bf10:	mov	x2, x27
   1bf14:	bl	c2d0 <__gmpn_lshift@plt>
   1bf18:	cmp	x0, #0x0
   1bf1c:	str	x0, [x26, x27, lsl #3]
   1bf20:	cinc	x27, x27, ne  // ne = any
   1bf24:	ldp	x8, x26, [x29, #-40]
   1bf28:	cmp	w19, #0x0
   1bf2c:	add	w8, w27, w8
   1bf30:	cneg	w8, w8, ne  // ne = any
   1bf34:	str	w8, [x26, #4]
   1bf38:	mov	sp, x29
   1bf3c:	ldp	x20, x19, [sp, #80]
   1bf40:	ldp	x22, x21, [sp, #64]
   1bf44:	ldp	x24, x23, [sp, #48]
   1bf48:	ldp	x26, x25, [sp, #32]
   1bf4c:	ldp	x28, x27, [sp, #16]
   1bf50:	ldp	x29, x30, [sp], #96
   1bf54:	ret
   1bf58:	ldur	x9, [x29, #-64]
   1bf5c:	tst	w24, #0x1
   1bf60:	mov	w8, #0x3f                  	// #63
   1bf64:	sub	w19, w8, w19
   1bf68:	csel	x24, x27, x9, eq  // eq = none
   1bf6c:	mov	x27, x22
   1bf70:	b	1bfac <__gmpz_n_pow_ui@@Base+0x468>
   1bf74:	mov	x0, x26
   1bf78:	mov	x1, x24
   1bf7c:	mov	x2, x27
   1bf80:	mov	x3, x23
   1bf84:	mov	x4, x22
   1bf88:	bl	cea0 <__gmpn_mul@plt>
   1bf8c:	cmp	x0, #0x0
   1bf90:	cset	w8, eq  // eq = none
   1bf94:	add	x9, x27, x22
   1bf98:	sub	x27, x9, x8
   1bf9c:	sub	w19, w19, #0x1
   1bfa0:	cmp	w19, #0x0
   1bfa4:	sub	x28, x28, #0x1
   1bfa8:	b.le	1bef4 <__gmpz_n_pow_ui@@Base+0x3b0>
   1bfac:	mov	x0, x24
   1bfb0:	mov	x1, x26
   1bfb4:	mov	x2, x27
   1bfb8:	bl	ca90 <__gmpn_sqr@plt>
   1bfbc:	add	x8, x24, x27, lsl #4
   1bfc0:	ldur	x8, [x8, #-8]
   1bfc4:	lsl	x9, x27, #1
   1bfc8:	lsr	x10, x20, x28
   1bfcc:	cmp	x8, #0x0
   1bfd0:	cset	w8, eq  // eq = none
   1bfd4:	sub	x27, x9, x8
   1bfd8:	tbnz	w10, #0, 1bf74 <__gmpz_n_pow_ui@@Base+0x430>
   1bfdc:	mov	x0, x26
   1bfe0:	mov	x26, x24
   1bfe4:	mov	x24, x0
   1bfe8:	b	1bf9c <__gmpz_n_pow_ui@@Base+0x458>
   1bfec:	mov	x0, x26
   1bff0:	bl	c1c0 <__gmpz_realloc@plt>
   1bff4:	mov	x26, x0
   1bff8:	cbnz	x27, 1bd44 <__gmpz_n_pow_ui@@Base+0x200>
   1bffc:	b	1bd54 <__gmpz_n_pow_ui@@Base+0x210>
   1c000:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1c004:	b	1befc <__gmpz_n_pow_ui@@Base+0x3b8>
   1c008:	mov	w1, #0x1                   	// #1
   1c00c:	mov	x0, x26
   1c010:	bl	c1c0 <__gmpz_realloc@plt>
   1c014:	b	1bc44 <__gmpz_n_pow_ui@@Base+0x100>
   1c018:	sub	x0, x29, #0x18
   1c01c:	mov	x19, x12
   1c020:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1c024:	mov	x12, x19
   1c028:	mov	x27, x0
   1c02c:	b	1bd98 <__gmpz_n_pow_ui@@Base+0x254>
   1c030:	sub	x0, x29, #0x18
   1c034:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1c038:	mov	x24, x0
   1c03c:	b	1bc04 <__gmpz_n_pow_ui@@Base+0xc0>

000000000001c040 <__gmpz_neg@@Base>:
   1c040:	stp	x29, x30, [sp, #-48]!
   1c044:	stp	x22, x21, [sp, #16]
   1c048:	stp	x20, x19, [sp, #32]
   1c04c:	ldrsw	x22, [x1, #4]
   1c050:	mov	x19, x0
   1c054:	cmp	x1, x0
   1c058:	mov	x29, sp
   1c05c:	b.eq	1c088 <__gmpz_neg@@Base+0x48>  // b.none
   1c060:	ldrsw	x8, [x19]
   1c064:	cmp	x22, #0x0
   1c068:	cneg	x21, x22, mi  // mi = first
   1c06c:	mov	x20, x1
   1c070:	cmp	x21, x8
   1c074:	b.gt	1c0a0 <__gmpz_neg@@Base+0x60>
   1c078:	ldr	x0, [x19, #8]
   1c07c:	ldr	x1, [x20, #8]
   1c080:	mov	x2, x21
   1c084:	bl	cc10 <__gmpn_copyi@plt>
   1c088:	neg	w8, w22
   1c08c:	str	w8, [x19, #4]
   1c090:	ldp	x20, x19, [sp, #32]
   1c094:	ldp	x22, x21, [sp, #16]
   1c098:	ldp	x29, x30, [sp], #48
   1c09c:	ret
   1c0a0:	mov	x0, x19
   1c0a4:	mov	x1, x21
   1c0a8:	bl	c1c0 <__gmpz_realloc@plt>
   1c0ac:	b	1c07c <__gmpz_neg@@Base+0x3c>

000000000001c0b0 <__gmpz_nextprime@@Base>:
   1c0b0:	stp	x29, x30, [sp, #-96]!
   1c0b4:	stp	x20, x19, [sp, #80]
   1c0b8:	mov	x20, x1
   1c0bc:	mov	x19, x0
   1c0c0:	mov	w1, #0x2                   	// #2
   1c0c4:	mov	x0, x20
   1c0c8:	str	x27, [sp, #16]
   1c0cc:	stp	x26, x25, [sp, #32]
   1c0d0:	stp	x24, x23, [sp, #48]
   1c0d4:	stp	x22, x21, [sp, #64]
   1c0d8:	mov	x29, sp
   1c0dc:	bl	d3d0 <__gmpz_cmp_ui@plt>
   1c0e0:	tbnz	w0, #31, 1c224 <__gmpz_nextprime@@Base+0x174>
   1c0e4:	mov	w2, #0x1                   	// #1
   1c0e8:	mov	x0, x19
   1c0ec:	mov	x1, x20
   1c0f0:	bl	ca60 <__gmpz_add_ui@plt>
   1c0f4:	mov	x0, x19
   1c0f8:	mov	x1, xzr
   1c0fc:	bl	c470 <__gmpz_setbit@plt>
   1c100:	mov	w1, #0x7                   	// #7
   1c104:	mov	x0, x19
   1c108:	bl	d3d0 <__gmpz_cmp_ui@plt>
   1c10c:	cmp	w0, #0x1
   1c110:	b.lt	1c230 <__gmpz_nextprime@@Base+0x180>  // b.tstop
   1c114:	ldrsw	x8, [x19, #4]
   1c118:	ldr	x9, [x19, #8]
   1c11c:	add	x9, x9, x8, lsl #3
   1c120:	ldur	x9, [x9, #-8]
   1c124:	lsl	x8, x8, #6
   1c128:	clz	x9, x9
   1c12c:	sub	x8, x8, x9
   1c130:	lsr	x9, x8, #1
   1c134:	cmp	x8, #0x14d
   1c138:	mov	w8, #0xa6                  	// #166
   1c13c:	csel	w21, w8, w9, hi  // hi = pmore
   1c140:	lsl	x8, x21, #1
   1c144:	add	x8, x8, #0xf
   1c148:	and	x8, x8, #0x3fffffff0
   1c14c:	mov	x9, sp
   1c150:	sub	x22, x9, x8
   1c154:	mov	sp, x22
   1c158:	adrp	x24, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1c15c:	mov	w23, #0xfffe                	// #65534
   1c160:	add	x24, x24, #0xc50
   1c164:	b	1c174 <__gmpz_nextprime@@Base+0xc4>
   1c168:	mov	x0, x19
   1c16c:	mov	x1, x19
   1c170:	bl	ca60 <__gmpz_add_ui@plt>
   1c174:	cbz	w21, 1c1a8 <__gmpz_nextprime@@Base+0xf8>
   1c178:	mov	x25, x21
   1c17c:	mov	x26, x22
   1c180:	mov	x27, x24
   1c184:	mov	w20, #0x3                   	// #3
   1c188:	mov	x0, x19
   1c18c:	mov	x1, x20
   1c190:	bl	c0d0 <__gmpz_tdiv_ui@plt>
   1c194:	strh	w0, [x26], #2
   1c198:	ldrb	w8, [x27], #1
   1c19c:	subs	x25, x25, #0x1
   1c1a0:	add	x20, x20, x8
   1c1a4:	b.ne	1c188 <__gmpz_nextprime@@Base+0xd8>  // b.any
   1c1a8:	mov	x2, xzr
   1c1ac:	mov	w20, wzr
   1c1b0:	b	1c1e4 <__gmpz_nextprime@@Base+0x134>
   1c1b4:	mov	x0, x19
   1c1b8:	mov	x1, x19
   1c1bc:	bl	ca60 <__gmpz_add_ui@plt>
   1c1c0:	mov	w1, #0x19                  	// #25
   1c1c4:	mov	x0, x19
   1c1c8:	bl	cd40 <__gmpz_millerrabin@plt>
   1c1cc:	cbnz	w0, 1c230 <__gmpz_nextprime@@Base+0x180>
   1c1d0:	mov	x2, xzr
   1c1d4:	cmp	w20, w23
   1c1d8:	add	w20, w20, #0x2
   1c1dc:	add	x2, x2, #0x2
   1c1e0:	b.cs	1c168 <__gmpz_nextprime@@Base+0xb8>  // b.hs, b.nlast
   1c1e4:	cbz	w21, 1c1b4 <__gmpz_nextprime@@Base+0x104>
   1c1e8:	mov	x8, x21
   1c1ec:	mov	x9, x22
   1c1f0:	mov	x10, x24
   1c1f4:	mov	w11, #0x3                   	// #3
   1c1f8:	ldrh	w12, [x9]
   1c1fc:	add	x12, x12, w20, uxtw
   1c200:	udiv	x13, x12, x11
   1c204:	msub	x12, x13, x11, x12
   1c208:	cbz	x12, 1c1d4 <__gmpz_nextprime@@Base+0x124>
   1c20c:	ldrb	w12, [x10], #1
   1c210:	subs	x8, x8, #0x1
   1c214:	add	x9, x9, #0x2
   1c218:	add	x11, x11, x12
   1c21c:	b.ne	1c1f8 <__gmpz_nextprime@@Base+0x148>  // b.any
   1c220:	b	1c1b4 <__gmpz_nextprime@@Base+0x104>
   1c224:	mov	w1, #0x2                   	// #2
   1c228:	mov	x0, x19
   1c22c:	bl	c2c0 <__gmpz_set_ui@plt>
   1c230:	mov	sp, x29
   1c234:	ldp	x20, x19, [sp, #80]
   1c238:	ldp	x22, x21, [sp, #64]
   1c23c:	ldp	x24, x23, [sp, #48]
   1c240:	ldp	x26, x25, [sp, #32]
   1c244:	ldr	x27, [sp, #16]
   1c248:	ldp	x29, x30, [sp], #96
   1c24c:	ret

000000000001c250 <__gmpz_out_raw@@Base>:
   1c250:	stp	x29, x30, [sp, #-80]!
   1c254:	stp	x24, x23, [sp, #32]
   1c258:	stp	x22, x21, [sp, #48]
   1c25c:	stp	x20, x19, [sp, #64]
   1c260:	str	x25, [sp, #16]
   1c264:	ldrsw	x23, [x1, #4]
   1c268:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1c26c:	ldr	x8, [x8, #3840]
   1c270:	mov	x21, x0
   1c274:	cmp	x23, #0x0
   1c278:	cneg	x25, x23, mi  // mi = first
   1c27c:	ldr	x8, [x8]
   1c280:	ubfiz	x24, x25, #3, #58
   1c284:	add	x19, x24, #0x8
   1c288:	mov	x0, x19
   1c28c:	mov	x29, sp
   1c290:	mov	x22, x1
   1c294:	blr	x8
   1c298:	mov	x20, x0
   1c29c:	add	x0, x0, #0x8
   1c2a0:	cbz	x24, 1c318 <__gmpz_out_raw@@Base+0xc8>
   1c2a4:	ldr	x9, [x22, #8]
   1c2a8:	add	x8, x0, x24
   1c2ac:	add	x10, x25, #0x1
   1c2b0:	ldr	x11, [x9], #8
   1c2b4:	sub	x10, x10, #0x1
   1c2b8:	cmp	x10, #0x1
   1c2bc:	lsl	x14, x11, #40
   1c2c0:	and	x14, x14, #0xff000000000000
   1c2c4:	lsr	x13, x11, #16
   1c2c8:	bfi	x14, x11, #56, #8
   1c2cc:	lsr	x12, x11, #24
   1c2d0:	bfi	x14, x13, #40, #8
   1c2d4:	lsr	x13, x11, #8
   1c2d8:	and	x13, x13, #0xff000000
   1c2dc:	bfi	x14, x12, #32, #8
   1c2e0:	orr	x13, x14, x13
   1c2e4:	lsr	x14, x11, #40
   1c2e8:	and	x12, x12, #0xff0000
   1c2ec:	and	x14, x14, #0xff00
   1c2f0:	orr	x12, x13, x12
   1c2f4:	orr	x12, x12, x14
   1c2f8:	add	x12, x12, x11, lsr #56
   1c2fc:	str	x12, [x8, #-8]!
   1c300:	b.gt	1c2b0 <__gmpz_out_raw@@Base+0x60>
   1c304:	clz	x9, x11
   1c308:	lsr	x9, x9, #3
   1c30c:	add	x0, x8, x9
   1c310:	sub	x8, x24, x9
   1c314:	b	1c31c <__gmpz_out_raw@@Base+0xcc>
   1c318:	mov	x8, xzr
   1c31c:	cmp	w23, #0x0
   1c320:	cneg	w9, w8, lt  // lt = tstop
   1c324:	rev	w9, w9
   1c328:	str	w9, [x0, #-4]!
   1c32c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1c330:	ldr	x9, [x9, #3856]
   1c334:	add	x22, x8, #0x4
   1c338:	cmp	x21, #0x0
   1c33c:	mov	w2, #0x1                   	// #1
   1c340:	ldr	x9, [x9]
   1c344:	mov	x1, x22
   1c348:	csel	x3, x9, x21, eq  // eq = none
   1c34c:	bl	d000 <fwrite@plt>
   1c350:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1c354:	ldr	x8, [x8, #4016]
   1c358:	cmp	x0, #0x1
   1c35c:	mov	x0, x20
   1c360:	mov	x1, x19
   1c364:	ldr	x8, [x8]
   1c368:	csel	x21, x22, xzr, eq  // eq = none
   1c36c:	blr	x8
   1c370:	mov	x0, x21
   1c374:	ldp	x20, x19, [sp, #64]
   1c378:	ldp	x22, x21, [sp, #48]
   1c37c:	ldp	x24, x23, [sp, #32]
   1c380:	ldr	x25, [sp, #16]
   1c384:	ldp	x29, x30, [sp], #80
   1c388:	ret

000000000001c38c <__gmpz_out_str@@Base>:
   1c38c:	stp	x29, x30, [sp, #-80]!
   1c390:	stp	x26, x25, [sp, #16]
   1c394:	stp	x24, x23, [sp, #32]
   1c398:	stp	x22, x21, [sp, #48]
   1c39c:	stp	x20, x19, [sp, #64]
   1c3a0:	mov	x29, sp
   1c3a4:	sub	sp, sp, #0x10
   1c3a8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1c3ac:	ldr	x8, [x8, #3856]
   1c3b0:	ldrsw	x21, [x2, #4]
   1c3b4:	cmp	x0, #0x0
   1c3b8:	mov	x22, x2
   1c3bc:	ldr	x8, [x8]
   1c3c0:	mov	w20, w1
   1c3c4:	csel	x19, x8, x0, eq  // eq = none
   1c3c8:	cmp	w1, #0x2
   1c3cc:	b.lt	1c3fc <__gmpz_out_str@@Base+0x70>  // b.tstop
   1c3d0:	cmp	w20, #0x25
   1c3d4:	b.ge	1c40c <__gmpz_out_str@@Base+0x80>  // b.tcont
   1c3d8:	adrp	x25, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1c3dc:	add	x25, x25, #0x79d
   1c3e0:	tbz	w21, #31, 1c430 <__gmpz_out_str@@Base+0xa4>
   1c3e4:	mov	w0, #0x2d                  	// #45
   1c3e8:	mov	x1, x19
   1c3ec:	bl	c360 <fputc@plt>
   1c3f0:	neg	x21, x21
   1c3f4:	mov	w26, #0x1                   	// #1
   1c3f8:	b	1c434 <__gmpz_out_str@@Base+0xa8>
   1c3fc:	cmn	w20, #0x2
   1c400:	b.le	1c418 <__gmpz_out_str@@Base+0x8c>
   1c404:	mov	w20, #0xa                   	// #10
   1c408:	b	1c424 <__gmpz_out_str@@Base+0x98>
   1c40c:	cmp	w20, #0x3e
   1c410:	b.le	1c424 <__gmpz_out_str@@Base+0x98>
   1c414:	b	1c564 <__gmpz_out_str@@Base+0x1d8>
   1c418:	cmn	w20, #0x24
   1c41c:	b.lt	1c564 <__gmpz_out_str@@Base+0x1d8>  // b.tstop
   1c420:	neg	w20, w20
   1c424:	adrp	x25, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1c428:	add	x25, x25, #0x75e
   1c42c:	tbnz	w21, #31, 1c3e4 <__gmpz_out_str@@Base+0x58>
   1c430:	mov	x26, xzr
   1c434:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1c438:	ldr	x8, [x8, #3936]
   1c43c:	mov	w9, #0x28                  	// #40
   1c440:	stur	xzr, [x29, #-8]
   1c444:	smaddl	x8, w20, w9, x8
   1c448:	ldr	x8, [x8, #8]
   1c44c:	lsl	x9, x21, #6
   1c450:	umulh	x8, x8, x9
   1c454:	add	x1, x8, #0x3
   1c458:	mov	w8, #0x7f00                	// #32512
   1c45c:	cmp	x1, x8
   1c460:	b.hi	1c548 <__gmpz_out_str@@Base+0x1bc>  // b.pmore
   1c464:	add	x9, x1, #0xf
   1c468:	mov	x8, sp
   1c46c:	and	x9, x9, #0xfffffffffffffff0
   1c470:	sub	x23, x8, x9
   1c474:	mov	sp, x23
   1c478:	sub	w8, w20, #0x1
   1c47c:	tst	w20, w8
   1c480:	b.ne	1c48c <__gmpz_out_str@@Base+0x100>  // b.any
   1c484:	ldr	x24, [x22, #8]
   1c488:	b	1c4c4 <__gmpz_out_str@@Base+0x138>
   1c48c:	lsl	x8, x21, #3
   1c490:	orr	x1, x8, #0x8
   1c494:	mov	w8, #0x7f00                	// #32512
   1c498:	cmp	x1, x8
   1c49c:	b.hi	1c56c <__gmpz_out_str@@Base+0x1e0>  // b.pmore
   1c4a0:	add	x9, x1, #0xf
   1c4a4:	mov	x8, sp
   1c4a8:	and	x9, x9, #0xfffffffffffffff0
   1c4ac:	sub	x24, x8, x9
   1c4b0:	mov	sp, x24
   1c4b4:	ldr	x1, [x22, #8]
   1c4b8:	mov	x0, x24
   1c4bc:	mov	x2, x21
   1c4c0:	bl	cc10 <__gmpn_copyi@plt>
   1c4c4:	mov	x0, x23
   1c4c8:	mov	w1, w20
   1c4cc:	mov	x2, x24
   1c4d0:	mov	x3, x21
   1c4d4:	bl	cc50 <__gmpn_get_str@plt>
   1c4d8:	mov	x2, x0
   1c4dc:	cbz	x0, 1c4fc <__gmpz_out_str@@Base+0x170>
   1c4e0:	mov	x8, x23
   1c4e4:	mov	x9, x2
   1c4e8:	ldrb	w10, [x8]
   1c4ec:	subs	x9, x9, #0x1
   1c4f0:	ldrb	w10, [x25, x10]
   1c4f4:	strb	w10, [x8], #1
   1c4f8:	b.ne	1c4e8 <__gmpz_out_str@@Base+0x15c>  // b.any
   1c4fc:	mov	w1, #0x1                   	// #1
   1c500:	mov	x0, x23
   1c504:	mov	x3, x19
   1c508:	strb	wzr, [x23, x2]
   1c50c:	bl	d000 <fwrite@plt>
   1c510:	ldur	x8, [x29, #-8]
   1c514:	add	x20, x0, x26
   1c518:	cbnz	x8, 1c558 <__gmpz_out_str@@Base+0x1cc>
   1c51c:	mov	x0, x19
   1c520:	bl	d680 <ferror@plt>
   1c524:	cmp	w0, #0x0
   1c528:	csel	x0, x20, xzr, eq  // eq = none
   1c52c:	mov	sp, x29
   1c530:	ldp	x20, x19, [sp, #64]
   1c534:	ldp	x22, x21, [sp, #48]
   1c538:	ldp	x24, x23, [sp, #32]
   1c53c:	ldp	x26, x25, [sp, #16]
   1c540:	ldp	x29, x30, [sp], #80
   1c544:	ret
   1c548:	sub	x0, x29, #0x8
   1c54c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1c550:	mov	x23, x0
   1c554:	b	1c478 <__gmpz_out_str@@Base+0xec>
   1c558:	mov	x0, x8
   1c55c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1c560:	b	1c51c <__gmpz_out_str@@Base+0x190>
   1c564:	mov	x0, xzr
   1c568:	b	1c52c <__gmpz_out_str@@Base+0x1a0>
   1c56c:	sub	x0, x29, #0x8
   1c570:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1c574:	mov	x24, x0
   1c578:	b	1c4b4 <__gmpz_out_str@@Base+0x128>

000000000001c57c <__gmpz_perfect_power_p@@Base>:
   1c57c:	stp	x29, x30, [sp, #-16]!
   1c580:	ldr	x8, [x0, #8]
   1c584:	ldrsw	x1, [x0, #4]
   1c588:	mov	x29, sp
   1c58c:	mov	x0, x8
   1c590:	bl	c390 <__gmpn_perfect_power_p@plt>
   1c594:	ldp	x29, x30, [sp], #16
   1c598:	ret

000000000001c59c <__gmpz_perfect_square_p@@Base>:
   1c59c:	stp	x29, x30, [sp, #-16]!
   1c5a0:	ldrsw	x1, [x0, #4]
   1c5a4:	mov	x29, sp
   1c5a8:	cmp	w1, #0x1
   1c5ac:	b.lt	1c5c0 <__gmpz_perfect_square_p@@Base+0x24>  // b.tstop
   1c5b0:	ldr	x0, [x0, #8]
   1c5b4:	bl	d280 <__gmpn_perfect_square_p@plt>
   1c5b8:	ldp	x29, x30, [sp], #16
   1c5bc:	ret
   1c5c0:	mvn	w8, w1
   1c5c4:	lsr	w0, w8, #31
   1c5c8:	ldp	x29, x30, [sp], #16
   1c5cc:	ret

000000000001c5d0 <__gmpz_popcount@@Base>:
   1c5d0:	stp	x29, x30, [sp, #-16]!
   1c5d4:	ldrsw	x1, [x0, #4]
   1c5d8:	mov	x29, sp
   1c5dc:	cmp	w1, #0x1
   1c5e0:	b.lt	1c5f4 <__gmpz_popcount@@Base+0x24>  // b.tstop
   1c5e4:	ldr	x0, [x0, #8]
   1c5e8:	bl	cf50 <__gmpn_popcount@plt>
   1c5ec:	ldp	x29, x30, [sp], #16
   1c5f0:	ret
   1c5f4:	sbfx	x0, x1, #31, #1
   1c5f8:	ldp	x29, x30, [sp], #16
   1c5fc:	ret

000000000001c600 <__gmpz_pow_ui@@Base>:
   1c600:	stp	x29, x30, [sp, #-16]!
   1c604:	cmp	x2, #0x2
   1c608:	mov	x29, sp
   1c60c:	b.eq	1c630 <__gmpz_pow_ui@@Base+0x30>  // b.none
   1c610:	mov	x3, x2
   1c614:	cmp	x2, #0x1
   1c618:	b.eq	1c640 <__gmpz_pow_ui@@Base+0x40>  // b.none
   1c61c:	cbnz	x3, 1c64c <__gmpz_pow_ui@@Base+0x4c>
   1c620:	mov	w1, #0x1                   	// #1
   1c624:	bl	c2c0 <__gmpz_set_ui@plt>
   1c628:	ldp	x29, x30, [sp], #16
   1c62c:	ret
   1c630:	mov	x2, x1
   1c634:	bl	c620 <__gmpz_mul@plt>
   1c638:	ldp	x29, x30, [sp], #16
   1c63c:	ret
   1c640:	bl	c590 <__gmpz_set@plt>
   1c644:	ldp	x29, x30, [sp], #16
   1c648:	ret
   1c64c:	ldr	x8, [x1, #8]
   1c650:	ldrsw	x2, [x1, #4]
   1c654:	mov	x1, x8
   1c658:	bl	c4a0 <__gmpz_n_pow_ui@plt>
   1c65c:	ldp	x29, x30, [sp], #16
   1c660:	ret

000000000001c664 <__gmpz_powm@@Base>:
   1c664:	stp	x29, x30, [sp, #-96]!
   1c668:	stp	x28, x27, [sp, #16]
   1c66c:	stp	x26, x25, [sp, #32]
   1c670:	stp	x24, x23, [sp, #48]
   1c674:	stp	x22, x21, [sp, #64]
   1c678:	stp	x20, x19, [sp, #80]
   1c67c:	mov	x29, sp
   1c680:	sub	sp, sp, #0x50
   1c684:	ldr	w8, [x3, #4]
   1c688:	cmp	w8, #0x0
   1c68c:	cneg	w20, w8, mi  // mi = first
   1c690:	cbz	w20, 1cd4c <__gmpz_powm@@Base+0x6e8>
   1c694:	ldr	x10, [x3, #8]
   1c698:	stur	xzr, [x29, #-24]
   1c69c:	ldrsw	x19, [x2, #4]
   1c6a0:	mov	x24, x3
   1c6a4:	mov	x21, x2
   1c6a8:	mov	x25, x1
   1c6ac:	mov	x22, x0
   1c6b0:	cmp	w19, #0x0
   1c6b4:	b.le	1cac8 <__gmpz_powm@@Base+0x464>
   1c6b8:	ldr	w8, [x25, #4]
   1c6bc:	cmp	w8, #0x0
   1c6c0:	cneg	w26, w8, mi  // mi = first
   1c6c4:	cbz	w26, 1cb28 <__gmpz_powm@@Base+0x4c4>
   1c6c8:	ldr	x23, [x21, #8]
   1c6cc:	cmp	x19, #0x1
   1c6d0:	b.ne	1c6e0 <__gmpz_powm@@Base+0x7c>  // b.any
   1c6d4:	ldr	x8, [x23]
   1c6d8:	cmp	x8, #0x1
   1c6dc:	b.eq	1cb64 <__gmpz_powm@@Base+0x500>  // b.none
   1c6e0:	ldr	x8, [x10]
   1c6e4:	cbz	x8, 1cb40 <__gmpz_powm@@Base+0x4dc>
   1c6e8:	mov	x27, xzr
   1c6ec:	sub	x28, x20, x27
   1c6f0:	tbnz	w8, #0, 1c778 <__gmpz_powm@@Base+0x114>
   1c6f4:	lsl	x1, x28, #3
   1c6f8:	mov	w8, #0x7f00                	// #32512
   1c6fc:	cmp	x1, x8
   1c700:	b.hi	1cbac <__gmpz_powm@@Base+0x548>  // b.pmore
   1c704:	add	x9, x1, #0xf
   1c708:	mov	x8, sp
   1c70c:	and	x9, x9, #0xfffffffffffffff0
   1c710:	sub	x21, x8, x9
   1c714:	mov	sp, x21
   1c718:	ldr	x8, [x10]
   1c71c:	mov	x0, x21
   1c720:	mov	x1, x10
   1c724:	mov	x2, x28
   1c728:	rbit	x8, x8
   1c72c:	clz	x3, x8
   1c730:	stur	x3, [x29, #-48]
   1c734:	bl	c2f0 <__gmpn_rshift@plt>
   1c738:	add	x8, x21, x28, lsl #3
   1c73c:	ldur	x8, [x8, #-8]
   1c740:	add	x27, x27, #0x1
   1c744:	mov	x10, x21
   1c748:	cmp	x8, #0x0
   1c74c:	cset	w8, eq  // eq = none
   1c750:	sub	x28, x28, x8
   1c754:	stur	x10, [x29, #-40]
   1c758:	cbnz	x27, 1c784 <__gmpz_powm@@Base+0x120>
   1c75c:	mov	x0, x28
   1c760:	bl	d3c0 <__gmpn_binvert_itch@plt>
   1c764:	lsl	x8, x20, #1
   1c768:	cmp	x0, x8
   1c76c:	csel	x9, x0, x8, gt
   1c770:	mov	x8, x20
   1c774:	b	1c7a0 <__gmpz_powm@@Base+0x13c>
   1c778:	stur	xzr, [x29, #-48]
   1c77c:	stur	x10, [x29, #-40]
   1c780:	cbz	x27, 1c75c <__gmpz_powm@@Base+0xf8>
   1c784:	cmp	x27, x28
   1c788:	csel	x0, x27, x28, gt
   1c78c:	bl	d3c0 <__gmpn_binvert_itch@plt>
   1c790:	lsl	x9, x20, #1
   1c794:	cmp	x0, x9
   1c798:	add	x8, x9, x20
   1c79c:	csel	x9, x0, x9, gt
   1c7a0:	add	x8, x9, x8
   1c7a4:	lsl	x1, x8, #3
   1c7a8:	mov	w8, #0x7f00                	// #32512
   1c7ac:	cmp	x1, x8
   1c7b0:	stur	x24, [x29, #-72]
   1c7b4:	stur	x23, [x29, #-32]
   1c7b8:	b.hi	1cb54 <__gmpz_powm@@Base+0x4f0>  // b.pmore
   1c7bc:	add	x9, x1, #0xf
   1c7c0:	mov	x8, sp
   1c7c4:	and	x9, x9, #0xfffffffffffffff0
   1c7c8:	sub	x24, x8, x9
   1c7cc:	mov	sp, x24
   1c7d0:	ldr	x21, [x25, #8]
   1c7d4:	ldp	x5, x3, [x29, #-40]
   1c7d8:	add	x23, x24, x20, lsl #3
   1c7dc:	mov	x0, x24
   1c7e0:	mov	x1, x21
   1c7e4:	mov	x2, x26
   1c7e8:	mov	x4, x19
   1c7ec:	mov	x6, x28
   1c7f0:	mov	x7, x23
   1c7f4:	bl	d1d0 <__gmpn_powm@plt>
   1c7f8:	cbz	x27, 1c9fc <__gmpz_powm@@Base+0x398>
   1c7fc:	cmp	x27, x26
   1c800:	stp	x25, x22, [x29, #-64]
   1c804:	b.le	1c860 <__gmpz_powm@@Base+0x1fc>
   1c808:	lsl	x25, x27, #3
   1c80c:	mov	w8, #0x7f00                	// #32512
   1c810:	cmp	x25, x8
   1c814:	b.hi	1cbdc <__gmpz_powm@@Base+0x578>  // b.pmore
   1c818:	add	x9, x25, #0xf
   1c81c:	mov	x8, sp
   1c820:	and	x9, x9, #0xfffffffffffffff0
   1c824:	sub	x22, x8, x9
   1c828:	mov	sp, x22
   1c82c:	mov	x0, x22
   1c830:	mov	x1, x21
   1c834:	mov	x2, x26
   1c838:	bl	cc10 <__gmpn_copyi@plt>
   1c83c:	cmp	x27, x26
   1c840:	mov	x21, x22
   1c844:	b.eq	1c860 <__gmpz_powm@@Base+0x1fc>  // b.none
   1c848:	lsl	x8, x26, #3
   1c84c:	add	x0, x22, x8
   1c850:	sub	x2, x25, x8
   1c854:	mov	w1, wzr
   1c858:	bl	c780 <memset@plt>
   1c85c:	mov	x21, x22
   1c860:	ldr	x8, [x21]
   1c864:	tbnz	w8, #0, 1c8a8 <__gmpz_powm@@Base+0x244>
   1c868:	cmp	x19, #0x2
   1c86c:	b.ge	1c8c8 <__gmpz_powm@@Base+0x264>  // b.tcont
   1c870:	ldur	x10, [x29, #-32]
   1c874:	ldur	x12, [x29, #-48]
   1c878:	ubfiz	w8, w8, #1, #3
   1c87c:	mov	w9, #0x1213                	// #4627
   1c880:	ldr	x10, [x10]
   1c884:	cmp	w12, #0x0
   1c888:	cset	w11, ne  // ne = any
   1c88c:	lsr	w8, w9, w8
   1c890:	sub	x9, x27, x11
   1c894:	and	w8, w8, #0x3
   1c898:	add	x9, x12, x9, lsl #6
   1c89c:	mul	x8, x10, x8
   1c8a0:	cmp	x8, x9
   1c8a4:	b.cs	1c8c8 <__gmpz_powm@@Base+0x264>  // b.hs, b.nlast
   1c8a8:	ldur	x2, [x29, #-32]
   1c8ac:	add	x5, x23, x27, lsl #3
   1c8b0:	mov	x0, x23
   1c8b4:	mov	x1, x21
   1c8b8:	mov	x3, x19
   1c8bc:	mov	x4, x27
   1c8c0:	bl	c520 <__gmpn_powlo@plt>
   1c8c4:	b	1c8d8 <__gmpz_powm@@Base+0x274>
   1c8c8:	add	x0, x24, x20, lsl #3
   1c8cc:	lsl	x2, x27, #3
   1c8d0:	mov	w1, wzr
   1c8d4:	bl	c780 <memset@plt>
   1c8d8:	ldur	x1, [x29, #-40]
   1c8dc:	cmp	x27, x28
   1c8e0:	b.le	1c940 <__gmpz_powm@@Base+0x2dc>
   1c8e4:	ldp	x25, x22, [x29, #-64]
   1c8e8:	lsl	x19, x27, #3
   1c8ec:	mov	w8, #0x7f00                	// #32512
   1c8f0:	cmp	x19, x8
   1c8f4:	b.hi	1cbf0 <__gmpz_powm@@Base+0x58c>  // b.pmore
   1c8f8:	add	x9, x19, #0xf
   1c8fc:	mov	x8, sp
   1c900:	and	x9, x9, #0xfffffffffffffff0
   1c904:	sub	x21, x8, x9
   1c908:	mov	sp, x21
   1c90c:	mov	x0, x21
   1c910:	mov	x2, x28
   1c914:	bl	cc10 <__gmpn_copyi@plt>
   1c918:	cmp	x27, x28
   1c91c:	mov	x1, x21
   1c920:	b.eq	1c944 <__gmpz_powm@@Base+0x2e0>  // b.none
   1c924:	lsl	x8, x28, #3
   1c928:	add	x0, x21, x8
   1c92c:	sub	x2, x19, x8
   1c930:	mov	w1, wzr
   1c934:	bl	c780 <memset@plt>
   1c938:	mov	x1, x21
   1c93c:	b	1c944 <__gmpz_powm@@Base+0x2e0>
   1c940:	ldp	x25, x22, [x29, #-64]
   1c944:	add	x21, x23, x20, lsl #3
   1c948:	add	x19, x23, x20, lsl #4
   1c94c:	mov	x0, x21
   1c950:	mov	x2, x27
   1c954:	mov	x3, x19
   1c958:	mov	x26, x1
   1c95c:	bl	cef0 <__gmpn_binvert@plt>
   1c960:	cmp	x27, x28
   1c964:	csel	x4, x27, x28, lt  // lt = tstop
   1c968:	mov	x0, x23
   1c96c:	mov	x1, x23
   1c970:	mov	x2, x27
   1c974:	mov	x3, x24
   1c978:	bl	d340 <__gmpn_sub@plt>
   1c97c:	mov	x0, x19
   1c980:	mov	x1, x21
   1c984:	mov	x2, x23
   1c988:	mov	x3, x27
   1c98c:	bl	d090 <__gmpn_mullo_n@plt>
   1c990:	ldur	x11, [x29, #-48]
   1c994:	cbz	w11, 1c9b0 <__gmpz_powm@@Base+0x34c>
   1c998:	add	x8, x19, x27, lsl #3
   1c99c:	ldur	x9, [x8, #-8]
   1c9a0:	mov	x10, #0xffffffffffffffff    	// #-1
   1c9a4:	lsl	x10, x10, x11
   1c9a8:	bic	x9, x9, x10
   1c9ac:	stur	x9, [x8, #-8]
   1c9b0:	mov	x0, x23
   1c9b4:	cmp	x27, x28
   1c9b8:	b.le	1c9d0 <__gmpz_powm@@Base+0x36c>
   1c9bc:	mov	x1, x19
   1c9c0:	mov	x2, x27
   1c9c4:	mov	x3, x26
   1c9c8:	mov	x4, x28
   1c9cc:	b	1c9e0 <__gmpz_powm@@Base+0x37c>
   1c9d0:	mov	x1, x26
   1c9d4:	mov	x2, x28
   1c9d8:	mov	x3, x19
   1c9dc:	mov	x4, x27
   1c9e0:	bl	cea0 <__gmpn_mul@plt>
   1c9e4:	mov	x0, x24
   1c9e8:	mov	x1, x23
   1c9ec:	mov	x2, x20
   1c9f0:	mov	x3, x24
   1c9f4:	mov	x4, x28
   1c9f8:	bl	c970 <__gmpn_add@plt>
   1c9fc:	ldur	x11, [x29, #-32]
   1ca00:	sub	x19, x24, #0x8
   1ca04:	mov	x8, x20
   1ca08:	subs	x9, x8, #0x1
   1ca0c:	b.lt	1ca2c <__gmpz_powm@@Base+0x3c8>  // b.tstop
   1ca10:	ldr	x10, [x19, x8, lsl #3]
   1ca14:	mov	x8, x9
   1ca18:	cbz	x10, 1ca08 <__gmpz_powm@@Base+0x3a4>
   1ca1c:	add	x26, x9, #0x1
   1ca20:	ldrb	w8, [x11]
   1ca24:	tbnz	w8, #0, 1ca38 <__gmpz_powm@@Base+0x3d4>
   1ca28:	b	1ca80 <__gmpz_powm@@Base+0x41c>
   1ca2c:	mov	x26, xzr
   1ca30:	ldrb	w8, [x11]
   1ca34:	tbz	w8, #0, 1ca80 <__gmpz_powm@@Base+0x41c>
   1ca38:	cbz	x26, 1ca80 <__gmpz_powm@@Base+0x41c>
   1ca3c:	ldr	w8, [x25, #4]
   1ca40:	tbz	w8, #31, 1ca80 <__gmpz_powm@@Base+0x41c>
   1ca44:	ldur	x8, [x29, #-72]
   1ca48:	mov	x0, x24
   1ca4c:	mov	x2, x20
   1ca50:	mov	x3, x24
   1ca54:	ldr	x1, [x8, #8]
   1ca58:	mov	x4, x26
   1ca5c:	bl	d340 <__gmpn_sub@plt>
   1ca60:	subs	x8, x20, #0x1
   1ca64:	b.lt	1ca7c <__gmpz_powm@@Base+0x418>  // b.tstop
   1ca68:	ldr	x9, [x19, x20, lsl #3]
   1ca6c:	mov	x20, x8
   1ca70:	cbz	x9, 1ca60 <__gmpz_powm@@Base+0x3fc>
   1ca74:	add	x26, x8, #0x1
   1ca78:	b	1ca80 <__gmpz_powm@@Base+0x41c>
   1ca7c:	mov	x26, xzr
   1ca80:	ldrsw	x8, [x22]
   1ca84:	cmp	x26, x8
   1ca88:	b.gt	1cb30 <__gmpz_powm@@Base+0x4cc>
   1ca8c:	ldr	x0, [x22, #8]
   1ca90:	mov	x1, x24
   1ca94:	mov	x2, x26
   1ca98:	str	w26, [x22, #4]
   1ca9c:	bl	cc10 <__gmpn_copyi@plt>
   1caa0:	ldur	x0, [x29, #-24]
   1caa4:	cbnz	x0, 1cb20 <__gmpz_powm@@Base+0x4bc>
   1caa8:	mov	sp, x29
   1caac:	ldp	x20, x19, [sp, #80]
   1cab0:	ldp	x22, x21, [sp, #64]
   1cab4:	ldp	x24, x23, [sp, #48]
   1cab8:	ldp	x26, x25, [sp, #32]
   1cabc:	ldp	x28, x27, [sp, #16]
   1cac0:	ldp	x29, x30, [sp], #96
   1cac4:	ret
   1cac8:	cbz	w19, 1cbc4 <__gmpz_powm@@Base+0x560>
   1cacc:	add	x8, x20, #0x1
   1cad0:	cmp	w20, #0xfdf
   1cad4:	lsl	x1, x8, #3
   1cad8:	mov	x23, x10
   1cadc:	stur	w8, [x29, #-16]
   1cae0:	b.hi	1cd0c <__gmpz_powm@@Base+0x6a8>  // b.pmore
   1cae4:	add	x9, x1, #0xf
   1cae8:	mov	x8, sp
   1caec:	and	x9, x9, #0x1ffffffff0
   1caf0:	sub	x0, x8, x9
   1caf4:	mov	sp, x0
   1caf8:	stur	x0, [x29, #-8]
   1cafc:	sub	x0, x29, #0x10
   1cb00:	mov	x1, x25
   1cb04:	mov	x2, x24
   1cb08:	bl	cd90 <__gmpz_invert@plt>
   1cb0c:	cbz	w0, 1cd4c <__gmpz_powm@@Base+0x6e8>
   1cb10:	sub	x25, x29, #0x10
   1cb14:	neg	x19, x19
   1cb18:	mov	x10, x23
   1cb1c:	b	1c6b8 <__gmpz_powm@@Base+0x54>
   1cb20:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1cb24:	b	1caa8 <__gmpz_powm@@Base+0x444>
   1cb28:	str	wzr, [x22, #4]
   1cb2c:	b	1caa0 <__gmpz_powm@@Base+0x43c>
   1cb30:	mov	x0, x22
   1cb34:	mov	x1, x26
   1cb38:	bl	c1c0 <__gmpz_realloc@plt>
   1cb3c:	b	1ca8c <__gmpz_powm@@Base+0x428>
   1cb40:	mov	x27, xzr
   1cb44:	ldr	x8, [x10, #8]!
   1cb48:	add	x27, x27, #0x1
   1cb4c:	cbnz	x8, 1c6ec <__gmpz_powm@@Base+0x88>
   1cb50:	b	1cb44 <__gmpz_powm@@Base+0x4e0>
   1cb54:	sub	x0, x29, #0x18
   1cb58:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cb5c:	mov	x24, x0
   1cb60:	b	1c7d0 <__gmpz_powm@@Base+0x16c>
   1cb64:	cmp	w20, #0xfe0
   1cb68:	lsl	x1, x20, #3
   1cb6c:	b.hi	1cd18 <__gmpz_powm@@Base+0x6b4>  // b.pmore
   1cb70:	add	x9, x1, #0xf
   1cb74:	mov	x8, sp
   1cb78:	and	x9, x9, #0xffffffff0
   1cb7c:	sub	x24, x8, x9
   1cb80:	mov	sp, x24
   1cb84:	ldr	x19, [x25, #8]
   1cb88:	cmp	w26, w20
   1cb8c:	b.cs	1cc08 <__gmpz_powm@@Base+0x5a4>  // b.hs, b.nlast
   1cb90:	ldr	w8, [x25, #4]
   1cb94:	tbnz	w8, #31, 1ccd8 <__gmpz_powm@@Base+0x674>
   1cb98:	mov	x0, x24
   1cb9c:	mov	x1, x19
   1cba0:	mov	x2, x26
   1cba4:	bl	cc10 <__gmpn_copyi@plt>
   1cba8:	b	1ca80 <__gmpz_powm@@Base+0x41c>
   1cbac:	sub	x0, x29, #0x18
   1cbb0:	mov	x21, x10
   1cbb4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cbb8:	mov	x10, x21
   1cbbc:	mov	x21, x0
   1cbc0:	b	1c718 <__gmpz_powm@@Base+0xb4>
   1cbc4:	cmp	w20, #0x1
   1cbc8:	b.ne	1cc78 <__gmpz_powm@@Base+0x614>  // b.any
   1cbcc:	ldr	x8, [x10]
   1cbd0:	cmp	x8, #0x1
   1cbd4:	cset	w8, ne  // ne = any
   1cbd8:	b	1cc7c <__gmpz_powm@@Base+0x618>
   1cbdc:	sub	x0, x29, #0x18
   1cbe0:	mov	x1, x25
   1cbe4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cbe8:	mov	x22, x0
   1cbec:	b	1c82c <__gmpz_powm@@Base+0x1c8>
   1cbf0:	sub	x0, x29, #0x18
   1cbf4:	mov	x1, x19
   1cbf8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cbfc:	ldur	x1, [x29, #-40]
   1cc00:	mov	x21, x0
   1cc04:	b	1c90c <__gmpz_powm@@Base+0x2a8>
   1cc08:	sub	x8, x26, x20
   1cc0c:	lsl	x8, x8, #3
   1cc10:	add	x1, x8, #0x8
   1cc14:	mov	w8, #0x7f00                	// #32512
   1cc18:	mov	x21, x10
   1cc1c:	cmp	x1, x8
   1cc20:	b.hi	1cd40 <__gmpz_powm@@Base+0x6dc>  // b.pmore
   1cc24:	add	x9, x1, #0xf
   1cc28:	mov	x8, sp
   1cc2c:	and	x9, x9, #0xfffffffffffffff0
   1cc30:	sub	x0, x8, x9
   1cc34:	mov	sp, x0
   1cc38:	mov	x1, x24
   1cc3c:	mov	x2, xzr
   1cc40:	mov	x3, x19
   1cc44:	mov	x4, x26
   1cc48:	mov	x5, x21
   1cc4c:	mov	x6, x20
   1cc50:	bl	c030 <__gmpn_tdiv_qr@plt>
   1cc54:	sub	x19, x24, #0x8
   1cc58:	mov	x8, x20
   1cc5c:	subs	x9, x8, #0x1
   1cc60:	b.lt	1cc9c <__gmpz_powm@@Base+0x638>  // b.tstop
   1cc64:	ldr	x10, [x19, x8, lsl #3]
   1cc68:	mov	x8, x9
   1cc6c:	cbz	x10, 1cc5c <__gmpz_powm@@Base+0x5f8>
   1cc70:	add	x26, x9, #0x1
   1cc74:	b	1cca0 <__gmpz_powm@@Base+0x63c>
   1cc78:	mov	w8, #0x1                   	// #1
   1cc7c:	ldr	w9, [x22]
   1cc80:	str	w8, [x22, #4]
   1cc84:	cmp	w9, #0x0
   1cc88:	b.le	1cd30 <__gmpz_powm@@Base+0x6cc>
   1cc8c:	ldr	x0, [x22, #8]
   1cc90:	mov	w8, #0x1                   	// #1
   1cc94:	str	x8, [x0]
   1cc98:	b	1caa0 <__gmpz_powm@@Base+0x43c>
   1cc9c:	mov	x26, xzr
   1cca0:	cbz	x26, 1ca80 <__gmpz_powm@@Base+0x41c>
   1cca4:	ldr	w8, [x25, #4]
   1cca8:	tbz	w8, #31, 1ca80 <__gmpz_powm@@Base+0x41c>
   1ccac:	mov	x0, x24
   1ccb0:	mov	x1, x21
   1ccb4:	mov	x2, x20
   1ccb8:	mov	x3, x24
   1ccbc:	mov	x4, x26
   1ccc0:	bl	d340 <__gmpn_sub@plt>
   1ccc4:	ldr	x9, [x19, x20, lsl #3]
   1ccc8:	sub	x8, x20, #0x1
   1cccc:	mov	x20, x8
   1ccd0:	cbz	x9, 1ccc4 <__gmpz_powm@@Base+0x660>
   1ccd4:	b	1ca74 <__gmpz_powm@@Base+0x410>
   1ccd8:	mov	x0, x24
   1ccdc:	mov	x1, x10
   1cce0:	mov	x2, x20
   1cce4:	mov	x3, x19
   1cce8:	mov	x4, x26
   1ccec:	bl	d340 <__gmpn_sub@plt>
   1ccf0:	sub	x8, x24, #0x8
   1ccf4:	ldr	x10, [x8, x20, lsl #3]
   1ccf8:	sub	x9, x20, #0x1
   1ccfc:	mov	x20, x9
   1cd00:	cbz	x10, 1ccf4 <__gmpz_powm@@Base+0x690>
   1cd04:	add	x26, x9, #0x1
   1cd08:	b	1ca80 <__gmpz_powm@@Base+0x41c>
   1cd0c:	sub	x0, x29, #0x18
   1cd10:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cd14:	b	1caf8 <__gmpz_powm@@Base+0x494>
   1cd18:	sub	x0, x29, #0x18
   1cd1c:	mov	x19, x10
   1cd20:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cd24:	mov	x10, x19
   1cd28:	mov	x24, x0
   1cd2c:	b	1cb84 <__gmpz_powm@@Base+0x520>
   1cd30:	mov	w1, #0x1                   	// #1
   1cd34:	mov	x0, x22
   1cd38:	bl	c1c0 <__gmpz_realloc@plt>
   1cd3c:	b	1cc90 <__gmpz_powm@@Base+0x62c>
   1cd40:	sub	x0, x29, #0x18
   1cd44:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cd48:	b	1cc38 <__gmpz_powm@@Base+0x5d4>
   1cd4c:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001cd50 <__gmpz_powm_sec@@Base>:
   1cd50:	stp	x29, x30, [sp, #-96]!
   1cd54:	stp	x28, x27, [sp, #16]
   1cd58:	stp	x26, x25, [sp, #32]
   1cd5c:	stp	x24, x23, [sp, #48]
   1cd60:	stp	x22, x21, [sp, #64]
   1cd64:	stp	x20, x19, [sp, #80]
   1cd68:	mov	x29, sp
   1cd6c:	sub	sp, sp, #0x10
   1cd70:	ldr	w8, [x3, #4]
   1cd74:	cmp	w8, #0x0
   1cd78:	cneg	w20, w8, mi  // mi = first
   1cd7c:	cbz	w20, 1cf5c <__gmpz_powm_sec@@Base+0x20c>
   1cd80:	ldr	x25, [x3, #8]
   1cd84:	mov	x21, x3
   1cd88:	ldr	x8, [x25]
   1cd8c:	tbz	w8, #0, 1cf5c <__gmpz_powm_sec@@Base+0x20c>
   1cd90:	ldrsw	x9, [x2, #4]
   1cd94:	mov	x24, x2
   1cd98:	mov	x19, x0
   1cd9c:	cmp	w9, #0x0
   1cda0:	b.le	1cee4 <__gmpz_powm_sec@@Base+0x194>
   1cda4:	ldr	w8, [x1, #4]
   1cda8:	mov	x22, x1
   1cdac:	cmp	w8, #0x0
   1cdb0:	cneg	w26, w8, mi  // mi = first
   1cdb4:	cbz	w26, 1cf1c <__gmpz_powm_sec@@Base+0x1cc>
   1cdb8:	lsl	x27, x9, #6
   1cdbc:	mov	x0, x26
   1cdc0:	mov	x1, x27
   1cdc4:	mov	x2, x20
   1cdc8:	stur	xzr, [x29, #-8]
   1cdcc:	bl	c380 <__gmpn_sec_powm_itch@plt>
   1cdd0:	add	x8, x0, x20
   1cdd4:	lsl	x1, x8, #3
   1cdd8:	mov	w8, #0x7f00                	// #32512
   1cddc:	cmp	x1, x8
   1cde0:	b.hi	1cf24 <__gmpz_powm_sec@@Base+0x1d4>  // b.pmore
   1cde4:	add	x9, x1, #0xf
   1cde8:	mov	x8, sp
   1cdec:	and	x9, x9, #0xfffffffffffffff0
   1cdf0:	sub	x23, x8, x9
   1cdf4:	mov	sp, x23
   1cdf8:	ldr	x28, [x24, #8]
   1cdfc:	ldr	x1, [x22, #8]
   1ce00:	add	x7, x23, x20, lsl #3
   1ce04:	mov	x0, x23
   1ce08:	mov	x2, x26
   1ce0c:	mov	x3, x28
   1ce10:	mov	x4, x27
   1ce14:	mov	x5, x25
   1ce18:	mov	x6, x20
   1ce1c:	bl	ce20 <__gmpn_sec_powm@plt>
   1ce20:	sub	x25, x23, #0x8
   1ce24:	mov	x8, x20
   1ce28:	subs	x9, x8, #0x1
   1ce2c:	b.lt	1ce4c <__gmpz_powm_sec@@Base+0xfc>  // b.tstop
   1ce30:	ldr	x10, [x25, x8, lsl #3]
   1ce34:	mov	x8, x9
   1ce38:	cbz	x10, 1ce28 <__gmpz_powm_sec@@Base+0xd8>
   1ce3c:	add	x24, x9, #0x1
   1ce40:	ldrb	w8, [x28]
   1ce44:	tbnz	w8, #0, 1ce58 <__gmpz_powm_sec@@Base+0x108>
   1ce48:	b	1ce9c <__gmpz_powm_sec@@Base+0x14c>
   1ce4c:	mov	x24, xzr
   1ce50:	ldrb	w8, [x28]
   1ce54:	tbz	w8, #0, 1ce9c <__gmpz_powm_sec@@Base+0x14c>
   1ce58:	cbz	x24, 1ce9c <__gmpz_powm_sec@@Base+0x14c>
   1ce5c:	ldr	w8, [x22, #4]
   1ce60:	tbz	w8, #31, 1ce9c <__gmpz_powm_sec@@Base+0x14c>
   1ce64:	ldr	x1, [x21, #8]
   1ce68:	mov	x0, x23
   1ce6c:	mov	x2, x20
   1ce70:	mov	x3, x23
   1ce74:	mov	x4, x24
   1ce78:	bl	d340 <__gmpn_sub@plt>
   1ce7c:	subs	x8, x20, #0x1
   1ce80:	b.lt	1ce98 <__gmpz_powm_sec@@Base+0x148>  // b.tstop
   1ce84:	ldr	x9, [x25, x20, lsl #3]
   1ce88:	mov	x20, x8
   1ce8c:	cbz	x9, 1ce7c <__gmpz_powm_sec@@Base+0x12c>
   1ce90:	add	x24, x8, #0x1
   1ce94:	b	1ce9c <__gmpz_powm_sec@@Base+0x14c>
   1ce98:	mov	x24, xzr
   1ce9c:	ldrsw	x8, [x19]
   1cea0:	cmp	x24, x8
   1cea4:	b.gt	1cf34 <__gmpz_powm_sec@@Base+0x1e4>
   1cea8:	ldr	x0, [x19, #8]
   1ceac:	mov	x1, x23
   1ceb0:	mov	x2, x24
   1ceb4:	str	w24, [x19, #4]
   1ceb8:	bl	cc10 <__gmpn_copyi@plt>
   1cebc:	ldur	x0, [x29, #-8]
   1cec0:	cbnz	x0, 1cf44 <__gmpz_powm_sec@@Base+0x1f4>
   1cec4:	mov	sp, x29
   1cec8:	ldp	x20, x19, [sp, #80]
   1cecc:	ldp	x22, x21, [sp, #64]
   1ced0:	ldp	x24, x23, [sp, #48]
   1ced4:	ldp	x26, x25, [sp, #32]
   1ced8:	ldp	x28, x27, [sp, #16]
   1cedc:	ldp	x29, x30, [sp], #96
   1cee0:	ret
   1cee4:	cbnz	w9, 1cf5c <__gmpz_powm_sec@@Base+0x20c>
   1cee8:	ldr	w9, [x19]
   1ceec:	cmp	w20, #0x1
   1cef0:	cset	w10, ne  // ne = any
   1cef4:	cmp	x8, #0x1
   1cef8:	cset	w8, ne  // ne = any
   1cefc:	orr	w8, w10, w8
   1cf00:	cmp	w9, #0x0
   1cf04:	str	w8, [x19, #4]
   1cf08:	b.le	1cf4c <__gmpz_powm_sec@@Base+0x1fc>
   1cf0c:	ldr	x0, [x19, #8]
   1cf10:	mov	w8, #0x1                   	// #1
   1cf14:	str	x8, [x0]
   1cf18:	b	1cec4 <__gmpz_powm_sec@@Base+0x174>
   1cf1c:	str	wzr, [x19, #4]
   1cf20:	b	1cec4 <__gmpz_powm_sec@@Base+0x174>
   1cf24:	sub	x0, x29, #0x8
   1cf28:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1cf2c:	mov	x23, x0
   1cf30:	b	1cdf8 <__gmpz_powm_sec@@Base+0xa8>
   1cf34:	mov	x0, x19
   1cf38:	mov	x1, x24
   1cf3c:	bl	c1c0 <__gmpz_realloc@plt>
   1cf40:	b	1cea8 <__gmpz_powm_sec@@Base+0x158>
   1cf44:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1cf48:	b	1cec4 <__gmpz_powm_sec@@Base+0x174>
   1cf4c:	mov	w1, #0x1                   	// #1
   1cf50:	mov	x0, x19
   1cf54:	bl	c1c0 <__gmpz_realloc@plt>
   1cf58:	b	1cf10 <__gmpz_powm_sec@@Base+0x1c0>
   1cf5c:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001cf60 <__gmpz_powm_ui@@Base>:
   1cf60:	stp	x29, x30, [sp, #-96]!
   1cf64:	stp	x28, x27, [sp, #16]
   1cf68:	stp	x26, x25, [sp, #32]
   1cf6c:	stp	x24, x23, [sp, #48]
   1cf70:	stp	x22, x21, [sp, #64]
   1cf74:	stp	x20, x19, [sp, #80]
   1cf78:	mov	x29, sp
   1cf7c:	sub	sp, sp, #0x40
   1cf80:	mov	x26, x3
   1cf84:	mov	x28, x2
   1cf88:	mov	x27, x1
   1cf8c:	cmp	x2, #0x13
   1cf90:	mov	x25, x0
   1cf94:	b.hi	1cfcc <__gmpz_powm_ui@@Base+0x6c>  // b.pmore
   1cf98:	ldr	w8, [x26, #4]
   1cf9c:	cmp	w8, #0x0
   1cfa0:	cneg	w19, w8, mi  // mi = first
   1cfa4:	cbz	w19, 1d45c <__gmpz_powm_ui@@Base+0x4fc>
   1cfa8:	ldr	x24, [x26, #8]
   1cfac:	cmp	x28, #0x1
   1cfb0:	b.hi	1cff4 <__gmpz_powm_ui@@Base+0x94>  // b.pmore
   1cfb4:	b.ne	1d054 <__gmpz_powm_ui@@Base+0xf4>  // b.any
   1cfb8:	mov	x0, x25
   1cfbc:	mov	x1, x27
   1cfc0:	mov	x2, x26
   1cfc4:	bl	cfc0 <__gmpz_mod@plt>
   1cfc8:	b	1d3dc <__gmpz_powm_ui@@Base+0x47c>
   1cfcc:	sub	x8, x29, #0x8
   1cfd0:	mov	w9, #0x1                   	// #1
   1cfd4:	sub	x2, x29, #0x18
   1cfd8:	mov	x0, x25
   1cfdc:	mov	x1, x27
   1cfe0:	mov	x3, x26
   1cfe4:	stp	x8, x28, [x29, #-16]
   1cfe8:	stur	w9, [x29, #-20]
   1cfec:	bl	c4d0 <__gmpz_powm@plt>
   1cff0:	b	1d3dc <__gmpz_powm_ui@@Base+0x47c>
   1cff4:	stur	xzr, [x29, #-8]
   1cff8:	sub	x20, x19, #0x1
   1cffc:	ldr	x8, [x24, x20, lsl #3]
   1d000:	lsl	x22, x19, #3
   1d004:	clz	x23, x8
   1d008:	cbz	w23, 1d040 <__gmpz_powm_ui@@Base+0xe0>
   1d00c:	cmp	w19, #0xfe0
   1d010:	b.hi	1d404 <__gmpz_powm_ui@@Base+0x4a4>  // b.pmore
   1d014:	add	x9, x22, #0xf
   1d018:	mov	x8, sp
   1d01c:	and	x9, x9, #0xffffffff0
   1d020:	sub	x21, x8, x9
   1d024:	mov	sp, x21
   1d028:	mov	x0, x21
   1d02c:	mov	x1, x24
   1d030:	mov	x2, x19
   1d034:	mov	w3, w23
   1d038:	bl	c2d0 <__gmpn_lshift@plt>
   1d03c:	mov	x24, x21
   1d040:	cmp	w19, #0x1
   1d044:	stur	x23, [x29, #-48]
   1d048:	b.ne	1d06c <__gmpz_powm_ui@@Base+0x10c>  // b.any
   1d04c:	mov	x23, xzr
   1d050:	b	1d074 <__gmpz_powm_ui@@Base+0x114>
   1d054:	cmp	w19, #0x1
   1d058:	b.ne	1d324 <__gmpz_powm_ui@@Base+0x3c4>  // b.any
   1d05c:	ldr	x8, [x24]
   1d060:	cmp	x8, #0x1
   1d064:	cset	w8, ne  // ne = any
   1d068:	b	1d328 <__gmpz_powm_ui@@Base+0x3c8>
   1d06c:	add	x8, x24, x19, lsl #3
   1d070:	ldur	x23, [x8, #-16]
   1d074:	ldr	x21, [x24, x20, lsl #3]
   1d078:	mov	x0, x21
   1d07c:	bl	d5d0 <__gmpn_invert_limb@plt>
   1d080:	mul	x8, x0, x21
   1d084:	adds	x8, x8, x23
   1d088:	b.cc	1d0a4 <__gmpz_powm_ui@@Base+0x144>  // b.lo, b.ul, b.last
   1d08c:	subs	x8, x8, x21
   1d090:	cset	w9, cs  // cs = hs, nlast
   1d094:	csel	x10, x21, xzr, cs  // cs = hs, nlast
   1d098:	mvn	x9, x9
   1d09c:	add	x0, x9, x0
   1d0a0:	sub	x8, x8, x10
   1d0a4:	umulh	x9, x23, x0
   1d0a8:	adds	x9, x9, x8
   1d0ac:	stur	x28, [x29, #-32]
   1d0b0:	b.cc	1d0d8 <__gmpz_powm_ui@@Base+0x178>  // b.lo, b.ul, b.last
   1d0b4:	cmp	x9, x21
   1d0b8:	sub	x8, x0, #0x1
   1d0bc:	b.cc	1d0dc <__gmpz_powm_ui@@Base+0x17c>  // b.lo, b.ul, b.last
   1d0c0:	mul	x10, x0, x23
   1d0c4:	cmp	x9, x21
   1d0c8:	sub	x11, x0, #0x2
   1d0cc:	ccmp	x10, x23, #0x2, ls  // ls = plast
   1d0d0:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   1d0d4:	b	1d0dc <__gmpz_powm_ui@@Base+0x17c>
   1d0d8:	mov	x8, x0
   1d0dc:	stur	x8, [x29, #-24]
   1d0e0:	ldr	w8, [x27, #4]
   1d0e4:	ldr	x28, [x27, #8]
   1d0e8:	stur	x27, [x29, #-40]
   1d0ec:	cmp	w8, #0x0
   1d0f0:	cneg	w27, w8, mi  // mi = first
   1d0f4:	cmp	w27, w19
   1d0f8:	b.ls	1d160 <__gmpz_powm_ui@@Base+0x200>  // b.plast
   1d0fc:	cmp	w19, #0xfe0
   1d100:	b.hi	1d438 <__gmpz_powm_ui@@Base+0x4d8>  // b.pmore
   1d104:	add	x9, x22, #0xf
   1d108:	mov	x8, sp
   1d10c:	and	x9, x9, #0xffffffff0
   1d110:	sub	x21, x8, x9
   1d114:	mov	sp, x21
   1d118:	sub	x5, x29, #0x18
   1d11c:	mov	x0, x21
   1d120:	mov	x1, x28
   1d124:	mov	x2, x27
   1d128:	mov	x3, x24
   1d12c:	mov	x4, x19
   1d130:	bl	1d460 <__gmpz_powm_ui@@Base+0x500>
   1d134:	sub	x8, x21, #0x8
   1d138:	mov	x10, x19
   1d13c:	subs	x9, x10, #0x1
   1d140:	b.lt	1d158 <__gmpz_powm_ui@@Base+0x1f8>  // b.tstop
   1d144:	ldr	x11, [x8, x10, lsl #3]
   1d148:	mov	x10, x9
   1d14c:	cbz	x11, 1d13c <__gmpz_powm_ui@@Base+0x1dc>
   1d150:	add	x27, x9, #0x1
   1d154:	b	1d15c <__gmpz_powm_ui@@Base+0x1fc>
   1d158:	mov	x27, xzr
   1d15c:	mov	x28, x21
   1d160:	cbz	x27, 1d314 <__gmpz_powm_ui@@Base+0x3b4>
   1d164:	add	x20, x19, #0x1
   1d168:	mov	w8, #0x1                   	// #1
   1d16c:	add	x9, x20, x19
   1d170:	bfi	x8, x19, #1, #32
   1d174:	add	x8, x9, x8
   1d178:	cmp	x8, #0xfe0
   1d17c:	lsl	x1, x8, #3
   1d180:	stp	x26, x25, [x29, #-64]
   1d184:	b.hi	1d418 <__gmpz_powm_ui@@Base+0x4b8>  // b.pmore
   1d188:	add	x9, x1, #0xf
   1d18c:	mov	x8, sp
   1d190:	and	x9, x9, #0x7ffffffff0
   1d194:	sub	x26, x8, x9
   1d198:	mov	sp, x26
   1d19c:	add	x22, x26, x19, lsl #3
   1d1a0:	mov	x0, x26
   1d1a4:	mov	x1, x28
   1d1a8:	mov	x2, x27
   1d1ac:	add	x25, x22, x20, lsl #3
   1d1b0:	bl	cc10 <__gmpn_copyi@plt>
   1d1b4:	ldur	x9, [x29, #-32]
   1d1b8:	mov	x21, x27
   1d1bc:	clz	x8, x9
   1d1c0:	lsl	x20, x9, x8
   1d1c4:	sub	w23, w8, #0x3f
   1d1c8:	b	1d1e4 <__gmpz_powm_ui@@Base+0x284>
   1d1cc:	mov	x0, x26
   1d1d0:	mov	x1, x25
   1d1d4:	mov	x2, x21
   1d1d8:	bl	cc10 <__gmpn_copyi@plt>
   1d1dc:	adds	w23, w23, #0x1
   1d1e0:	b.cs	1d2a8 <__gmpz_powm_ui@@Base+0x348>  // b.hs, b.nlast
   1d1e4:	mov	x0, x25
   1d1e8:	mov	x1, x26
   1d1ec:	mov	x2, x21
   1d1f0:	lsl	x20, x20, #1
   1d1f4:	bl	ca90 <__gmpn_sqr@plt>
   1d1f8:	add	x8, x25, x21, lsl #4
   1d1fc:	ldur	x8, [x8, #-8]
   1d200:	lsl	x9, x21, #1
   1d204:	cmp	x8, #0x0
   1d208:	cset	w8, eq  // eq = none
   1d20c:	sub	x21, x9, x8
   1d210:	cmp	x21, x19
   1d214:	b.lt	1d238 <__gmpz_powm_ui@@Base+0x2d8>  // b.tstop
   1d218:	sub	x4, x29, #0x18
   1d21c:	mov	x0, x25
   1d220:	mov	x1, x21
   1d224:	mov	x2, x24
   1d228:	mov	x3, x19
   1d22c:	mov	x5, x22
   1d230:	bl	1d544 <__gmpz_powm_ui@@Base+0x5e4>
   1d234:	mov	x21, x19
   1d238:	mov	x0, x26
   1d23c:	mov	x1, x25
   1d240:	mov	x2, x21
   1d244:	bl	cc10 <__gmpn_copyi@plt>
   1d248:	tbz	x20, #63, 1d1dc <__gmpz_powm_ui@@Base+0x27c>
   1d24c:	mov	x0, x25
   1d250:	mov	x1, x26
   1d254:	mov	x2, x21
   1d258:	mov	x3, x28
   1d25c:	mov	x4, x27
   1d260:	bl	cea0 <__gmpn_mul@plt>
   1d264:	add	x8, x21, x27
   1d268:	add	x9, x25, x8, lsl #3
   1d26c:	ldur	x9, [x9, #-8]
   1d270:	cmp	x9, #0x0
   1d274:	cset	w9, eq  // eq = none
   1d278:	sub	x21, x8, x9
   1d27c:	cmp	x21, x19
   1d280:	b.lt	1d1cc <__gmpz_powm_ui@@Base+0x26c>  // b.tstop
   1d284:	sub	x4, x29, #0x18
   1d288:	mov	x0, x25
   1d28c:	mov	x1, x21
   1d290:	mov	x2, x24
   1d294:	mov	x3, x19
   1d298:	mov	x5, x22
   1d29c:	bl	1d544 <__gmpz_powm_ui@@Base+0x5e4>
   1d2a0:	mov	x21, x19
   1d2a4:	b	1d1cc <__gmpz_powm_ui@@Base+0x26c>
   1d2a8:	ldur	x20, [x29, #-48]
   1d2ac:	cbz	w20, 1d348 <__gmpz_powm_ui@@Base+0x3e8>
   1d2b0:	mov	x0, x25
   1d2b4:	mov	x1, x26
   1d2b8:	mov	x2, x21
   1d2bc:	mov	w3, w20
   1d2c0:	bl	c2d0 <__gmpn_lshift@plt>
   1d2c4:	ldp	x27, x28, [x29, #-40]
   1d2c8:	cmp	x0, #0x0
   1d2cc:	str	x0, [x25, x21, lsl #3]
   1d2d0:	cinc	x21, x21, ne  // ne = any
   1d2d4:	cmp	x21, x19
   1d2d8:	b.lt	1d2fc <__gmpz_powm_ui@@Base+0x39c>  // b.tstop
   1d2dc:	sub	x4, x29, #0x18
   1d2e0:	mov	x0, x25
   1d2e4:	mov	x1, x21
   1d2e8:	mov	x2, x24
   1d2ec:	mov	x3, x19
   1d2f0:	mov	x5, x22
   1d2f4:	bl	1d544 <__gmpz_powm_ui@@Base+0x5e4>
   1d2f8:	mov	x21, x19
   1d2fc:	mov	x0, x26
   1d300:	mov	x1, x25
   1d304:	mov	x2, x21
   1d308:	mov	w3, w20
   1d30c:	bl	c2f0 <__gmpn_rshift@plt>
   1d310:	b	1d34c <__gmpz_powm_ui@@Base+0x3ec>
   1d314:	str	wzr, [x25, #4]
   1d318:	ldur	x0, [x29, #-8]
   1d31c:	cbz	x0, 1d3dc <__gmpz_powm_ui@@Base+0x47c>
   1d320:	b	1d3fc <__gmpz_powm_ui@@Base+0x49c>
   1d324:	mov	w8, #0x1                   	// #1
   1d328:	ldr	w9, [x25]
   1d32c:	str	w8, [x25, #4]
   1d330:	cmp	w9, #0x0
   1d334:	b.le	1d44c <__gmpz_powm_ui@@Base+0x4ec>
   1d338:	ldr	x0, [x25, #8]
   1d33c:	mov	w8, #0x1                   	// #1
   1d340:	str	x8, [x0]
   1d344:	b	1d3dc <__gmpz_powm_ui@@Base+0x47c>
   1d348:	ldp	x27, x28, [x29, #-40]
   1d34c:	ldur	x23, [x29, #-56]
   1d350:	sub	x20, x26, #0x8
   1d354:	mov	x22, x21
   1d358:	subs	x21, x21, #0x1
   1d35c:	b.lt	1d368 <__gmpz_powm_ui@@Base+0x408>  // b.tstop
   1d360:	ldr	x8, [x20, x22, lsl #3]
   1d364:	cbz	x8, 1d354 <__gmpz_powm_ui@@Base+0x3f4>
   1d368:	tbz	w28, #0, 1d3b4 <__gmpz_powm_ui@@Base+0x454>
   1d36c:	cbz	x22, 1d3b4 <__gmpz_powm_ui@@Base+0x454>
   1d370:	ldr	w8, [x27, #4]
   1d374:	tbz	w8, #31, 1d3b4 <__gmpz_powm_ui@@Base+0x454>
   1d378:	ldur	x8, [x29, #-64]
   1d37c:	mov	x0, x26
   1d380:	mov	x2, x19
   1d384:	mov	x3, x26
   1d388:	ldr	x1, [x8, #8]
   1d38c:	mov	x4, x22
   1d390:	bl	d340 <__gmpn_sub@plt>
   1d394:	subs	x8, x19, #0x1
   1d398:	b.lt	1d3b0 <__gmpz_powm_ui@@Base+0x450>  // b.tstop
   1d39c:	ldr	x9, [x20, x19, lsl #3]
   1d3a0:	mov	x19, x8
   1d3a4:	cbz	x9, 1d394 <__gmpz_powm_ui@@Base+0x434>
   1d3a8:	add	x22, x8, #0x1
   1d3ac:	b	1d3b4 <__gmpz_powm_ui@@Base+0x454>
   1d3b0:	mov	x22, xzr
   1d3b4:	ldrsw	x8, [x23]
   1d3b8:	cmp	x22, x8
   1d3bc:	b.gt	1d428 <__gmpz_powm_ui@@Base+0x4c8>
   1d3c0:	ldr	x0, [x23, #8]
   1d3c4:	mov	x1, x26
   1d3c8:	mov	x2, x22
   1d3cc:	str	w22, [x23, #4]
   1d3d0:	bl	cc10 <__gmpn_copyi@plt>
   1d3d4:	ldur	x0, [x29, #-8]
   1d3d8:	cbnz	x0, 1d3fc <__gmpz_powm_ui@@Base+0x49c>
   1d3dc:	mov	sp, x29
   1d3e0:	ldp	x20, x19, [sp, #80]
   1d3e4:	ldp	x22, x21, [sp, #64]
   1d3e8:	ldp	x24, x23, [sp, #48]
   1d3ec:	ldp	x26, x25, [sp, #32]
   1d3f0:	ldp	x28, x27, [sp, #16]
   1d3f4:	ldp	x29, x30, [sp], #96
   1d3f8:	ret
   1d3fc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1d400:	b	1d3dc <__gmpz_powm_ui@@Base+0x47c>
   1d404:	sub	x0, x29, #0x8
   1d408:	mov	x1, x22
   1d40c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d410:	mov	x21, x0
   1d414:	b	1d028 <__gmpz_powm_ui@@Base+0xc8>
   1d418:	sub	x0, x29, #0x8
   1d41c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d420:	mov	x26, x0
   1d424:	b	1d19c <__gmpz_powm_ui@@Base+0x23c>
   1d428:	mov	x0, x23
   1d42c:	mov	x1, x22
   1d430:	bl	c1c0 <__gmpz_realloc@plt>
   1d434:	b	1d3c0 <__gmpz_powm_ui@@Base+0x460>
   1d438:	sub	x0, x29, #0x8
   1d43c:	mov	x1, x22
   1d440:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d444:	mov	x21, x0
   1d448:	b	1d118 <__gmpz_powm_ui@@Base+0x1b8>
   1d44c:	mov	w1, #0x1                   	// #1
   1d450:	mov	x0, x25
   1d454:	bl	c1c0 <__gmpz_realloc@plt>
   1d458:	b	1d33c <__gmpz_powm_ui@@Base+0x3dc>
   1d45c:	bl	c100 <__gmp_divide_by_zero@plt>
   1d460:	stp	x29, x30, [sp, #-80]!
   1d464:	stp	x26, x25, [sp, #16]
   1d468:	stp	x24, x23, [sp, #32]
   1d46c:	stp	x22, x21, [sp, #48]
   1d470:	stp	x20, x19, [sp, #64]
   1d474:	mov	x29, sp
   1d478:	sub	sp, sp, #0x10
   1d47c:	mov	w8, #0x1                   	// #1
   1d480:	bfi	x8, x2, #1, #63
   1d484:	sub	x8, x8, x4
   1d488:	mov	x24, x1
   1d48c:	lsl	x1, x8, #3
   1d490:	mov	w8, #0x7f00                	// #32512
   1d494:	mov	x21, x5
   1d498:	mov	x19, x4
   1d49c:	mov	x22, x3
   1d4a0:	mov	x23, x2
   1d4a4:	mov	x20, x0
   1d4a8:	cmp	x1, x8
   1d4ac:	stur	xzr, [x29, #-8]
   1d4b0:	b.hi	1d52c <__gmpz_powm_ui@@Base+0x5cc>  // b.pmore
   1d4b4:	add	x9, x1, #0xf
   1d4b8:	mov	x8, sp
   1d4bc:	and	x9, x9, #0xfffffffffffffff0
   1d4c0:	sub	x25, x8, x9
   1d4c4:	mov	sp, x25
   1d4c8:	mov	x0, x25
   1d4cc:	mov	x1, x24
   1d4d0:	mov	x2, x23
   1d4d4:	add	x26, x25, x23, lsl #3
   1d4d8:	bl	cc10 <__gmpn_copyi@plt>
   1d4dc:	mov	x0, x25
   1d4e0:	mov	x1, x23
   1d4e4:	mov	x2, x22
   1d4e8:	mov	x3, x19
   1d4ec:	mov	x4, x21
   1d4f0:	mov	x5, x26
   1d4f4:	bl	1d544 <__gmpz_powm_ui@@Base+0x5e4>
   1d4f8:	mov	x0, x20
   1d4fc:	mov	x1, x25
   1d500:	mov	x2, x19
   1d504:	bl	cc10 <__gmpn_copyi@plt>
   1d508:	ldur	x0, [x29, #-8]
   1d50c:	cbnz	x0, 1d53c <__gmpz_powm_ui@@Base+0x5dc>
   1d510:	mov	sp, x29
   1d514:	ldp	x20, x19, [sp, #64]
   1d518:	ldp	x22, x21, [sp, #48]
   1d51c:	ldp	x24, x23, [sp, #32]
   1d520:	ldp	x26, x25, [sp, #16]
   1d524:	ldp	x29, x30, [sp], #80
   1d528:	ret
   1d52c:	sub	x0, x29, #0x8
   1d530:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d534:	mov	x25, x0
   1d538:	b	1d4c8 <__gmpz_powm_ui@@Base+0x568>
   1d53c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1d540:	b	1d510 <__gmpz_powm_ui@@Base+0x5b0>
   1d544:	stp	x29, x30, [sp, #-80]!
   1d548:	stp	x24, x23, [sp, #32]
   1d54c:	stp	x22, x21, [sp, #48]
   1d550:	stp	x20, x19, [sp, #64]
   1d554:	mov	x23, x5
   1d558:	mov	x8, x4
   1d55c:	mov	x22, x2
   1d560:	mov	x20, x1
   1d564:	cmp	x3, #0x2
   1d568:	mov	x19, x0
   1d56c:	str	x25, [sp, #16]
   1d570:	mov	x29, sp
   1d574:	b.eq	1d5a4 <__gmpz_powm_ui@@Base+0x644>  // b.none
   1d578:	mov	x21, x3
   1d57c:	cmp	x3, #0x1
   1d580:	b.ne	1d5c4 <__gmpz_powm_ui@@Base+0x664>  // b.any
   1d584:	ldr	x4, [x22]
   1d588:	mov	x0, x23
   1d58c:	mov	x1, xzr
   1d590:	mov	x2, x19
   1d594:	mov	x3, x20
   1d598:	bl	ced0 <__gmpn_divrem_1@plt>
   1d59c:	str	x0, [x19]
   1d5a0:	b	1d65c <__gmpz_powm_ui@@Base+0x6fc>
   1d5a4:	ldp	x5, x4, [x22]
   1d5a8:	ldr	x6, [x8]
   1d5ac:	mov	x0, x23
   1d5b0:	mov	x1, x19
   1d5b4:	mov	x2, x19
   1d5b8:	mov	x3, x20
   1d5bc:	bl	cad0 <__gmpn_div_qr_2n_pi1@plt>
   1d5c0:	b	1d65c <__gmpz_powm_ui@@Base+0x6fc>
   1d5c4:	cmp	x21, #0x2a
   1d5c8:	b.lt	1d640 <__gmpz_powm_ui@@Base+0x6e0>  // b.tstop
   1d5cc:	sub	x9, x20, x21
   1d5d0:	cmp	x9, #0x29
   1d5d4:	b.le	1d640 <__gmpz_powm_ui@@Base+0x6e0>
   1d5d8:	cmp	x20, #0x7cc
   1d5dc:	b.lt	1d620 <__gmpz_powm_ui@@Base+0x6c0>  // b.tstop
   1d5e0:	cmp	x21, #0x62
   1d5e4:	b.lt	1d620 <__gmpz_powm_ui@@Base+0x6c0>  // b.tstop
   1d5e8:	mov	x9, #0x200000000000        	// #35184372088832
   1d5ec:	mov	x10, #0x800000000000        	// #140737488355328
   1d5f0:	movk	x9, #0x409c, lsl #48
   1d5f4:	movk	x10, #0x4058, lsl #48
   1d5f8:	scvtf	d0, x21
   1d5fc:	scvtf	d1, x20
   1d600:	fmov	d2, x9
   1d604:	fmov	d3, x10
   1d608:	fmul	d2, d0, d2
   1d60c:	fmul	d3, d1, d3
   1d610:	fadd	d2, d3, d2
   1d614:	fmul	d0, d1, d0
   1d618:	fcmp	d2, d0
   1d61c:	b.le	1d674 <__gmpz_powm_ui@@Base+0x714>
   1d620:	mov	x0, x23
   1d624:	mov	x1, x19
   1d628:	mov	x2, x20
   1d62c:	mov	x3, x22
   1d630:	mov	x4, x21
   1d634:	mov	x5, x8
   1d638:	bl	c510 <__gmpn_dcpi1_div_qr@plt>
   1d63c:	b	1d65c <__gmpz_powm_ui@@Base+0x6fc>
   1d640:	ldr	x5, [x8]
   1d644:	mov	x0, x23
   1d648:	mov	x1, x19
   1d64c:	mov	x2, x20
   1d650:	mov	x3, x22
   1d654:	mov	x4, x21
   1d658:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   1d65c:	ldp	x20, x19, [sp, #64]
   1d660:	ldp	x22, x21, [sp, #48]
   1d664:	ldp	x24, x23, [sp, #32]
   1d668:	ldr	x25, [sp, #16]
   1d66c:	ldp	x29, x30, [sp], #80
   1d670:	ret
   1d674:	mov	x0, x20
   1d678:	mov	x1, x21
   1d67c:	mov	w2, wzr
   1d680:	str	xzr, [x29, #24]
   1d684:	bl	d1e0 <__gmpn_mu_div_qr_itch@plt>
   1d688:	mov	x24, x0
   1d68c:	lsl	x1, x21, #3
   1d690:	add	x0, x29, #0x18
   1d694:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d698:	mov	x25, x0
   1d69c:	lsl	x1, x24, #3
   1d6a0:	add	x0, x29, #0x18
   1d6a4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d6a8:	mov	x6, x0
   1d6ac:	mov	x0, x23
   1d6b0:	mov	x1, x25
   1d6b4:	mov	x2, x19
   1d6b8:	mov	x3, x20
   1d6bc:	mov	x4, x22
   1d6c0:	mov	x5, x21
   1d6c4:	bl	cb20 <__gmpn_mu_div_qr@plt>
   1d6c8:	mov	x0, x19
   1d6cc:	mov	x1, x25
   1d6d0:	mov	x2, x21
   1d6d4:	bl	cc10 <__gmpn_copyi@plt>
   1d6d8:	ldr	x0, [x29, #24]
   1d6dc:	cbz	x0, 1d65c <__gmpz_powm_ui@@Base+0x6fc>
   1d6e0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1d6e4:	b	1d65c <__gmpz_powm_ui@@Base+0x6fc>

000000000001d6e8 <__gmpz_primorial_ui@@Base>:
   1d6e8:	stp	x29, x30, [sp, #-96]!
   1d6ec:	stp	x28, x27, [sp, #16]
   1d6f0:	stp	x26, x25, [sp, #32]
   1d6f4:	stp	x24, x23, [sp, #48]
   1d6f8:	stp	x22, x21, [sp, #64]
   1d6fc:	stp	x20, x19, [sp, #80]
   1d700:	mov	x29, sp
   1d704:	sub	sp, sp, #0x10
   1d708:	mov	x20, x1
   1d70c:	cmp	x1, #0x4
   1d710:	mov	x19, x0
   1d714:	b.hi	1d748 <__gmpz_primorial_ui@@Base+0x60>  // b.pmore
   1d718:	ldr	w8, [x19]
   1d71c:	add	w9, w20, w20, lsl #1
   1d720:	mov	w10, #0x6c89                	// #27785
   1d724:	lsr	w9, w10, w9
   1d728:	cmp	w8, #0x0
   1d72c:	and	w20, w9, #0x7
   1d730:	b.le	1d888 <__gmpz_primorial_ui@@Base+0x1a0>
   1d734:	ldr	x0, [x19, #8]
   1d738:	mov	w8, #0x1                   	// #1
   1d73c:	str	x20, [x0]
   1d740:	str	w8, [x19, #4]
   1d744:	b	1d848 <__gmpz_primorial_ui@@Base+0x160>
   1d748:	ldrsw	x9, [x19]
   1d74c:	lsr	x8, x20, #6
   1d750:	add	x8, x8, x20, lsr #7
   1d754:	cmp	x8, x9
   1d758:	b.ge	1d898 <__gmpz_primorial_ui@@Base+0x1b0>  // b.tcont
   1d75c:	ldr	x21, [x19, #8]
   1d760:	mov	x0, x21
   1d764:	mov	x1, x20
   1d768:	bl	d3e0 <__gmp_primesieve@plt>
   1d76c:	add	x22, x0, #0x1
   1d770:	mov	x0, x20
   1d774:	bl	1d8bc <__gmpz_primorial_ui@@Base+0x1d4>
   1d778:	mov	w8, w0
   1d77c:	udiv	x8, x22, x8
   1d780:	lsl	x8, x8, #3
   1d784:	add	x1, x8, #0x8
   1d788:	mov	w8, #0x7f00                	// #32512
   1d78c:	cmp	x1, x8
   1d790:	stur	xzr, [x29, #-8]
   1d794:	b.hi	1d8ac <__gmpz_primorial_ui@@Base+0x1c4>  // b.pmore
   1d798:	add	x9, x1, #0xf
   1d79c:	mov	x8, sp
   1d7a0:	and	x9, x9, #0xfffffffffffffff0
   1d7a4:	sub	x22, x8, x9
   1d7a8:	mov	sp, x22
   1d7ac:	mov	w0, #0x5                   	// #5
   1d7b0:	bl	1d8e4 <__gmpz_primorial_ui@@Base+0x1fc>
   1d7b4:	mov	w8, #0x1                   	// #1
   1d7b8:	mov	x23, x0
   1d7bc:	lsr	x25, x0, #6
   1d7c0:	lsl	x27, x8, x0
   1d7c4:	mov	x0, x20
   1d7c8:	bl	1d8e4 <__gmpz_primorial_ui@@Base+0x1fc>
   1d7cc:	mov	x24, x0
   1d7d0:	mov	x28, xzr
   1d7d4:	mov	w26, #0x6                   	// #6
   1d7d8:	b	1d7f4 <__gmpz_primorial_ui@@Base+0x10c>
   1d7dc:	mul	x26, x0, x26
   1d7e0:	ror	x8, x27, #63
   1d7e4:	cmp	x23, x24
   1d7e8:	add	x25, x25, x27, lsr #63
   1d7ec:	mov	x27, x8
   1d7f0:	b.hi	1d828 <__gmpz_primorial_ui@@Base+0x140>  // b.pmore
   1d7f4:	ldr	x8, [x21, x25, lsl #3]
   1d7f8:	add	x23, x23, #0x1
   1d7fc:	tst	x8, x27
   1d800:	b.ne	1d7e0 <__gmpz_primorial_ui@@Base+0xf8>  // b.any
   1d804:	mov	x0, x23
   1d808:	bl	1d900 <__gmpz_primorial_ui@@Base+0x218>
   1d80c:	umulh	x8, x20, x26
   1d810:	cbz	x8, 1d7dc <__gmpz_primorial_ui@@Base+0xf4>
   1d814:	add	x8, x28, #0x1
   1d818:	str	x26, [x22, x28, lsl #3]
   1d81c:	mov	x26, x0
   1d820:	mov	x28, x8
   1d824:	b	1d7e0 <__gmpz_primorial_ui@@Base+0xf8>
   1d828:	cbz	x28, 1d868 <__gmpz_primorial_ui@@Base+0x180>
   1d82c:	add	x2, x28, #0x1
   1d830:	mov	x0, x19
   1d834:	mov	x1, x22
   1d838:	str	x26, [x22, x28, lsl #3]
   1d83c:	bl	cf40 <__gmpz_prodlimbs@plt>
   1d840:	ldur	x0, [x29, #-8]
   1d844:	cbnz	x0, 1d880 <__gmpz_primorial_ui@@Base+0x198>
   1d848:	mov	sp, x29
   1d84c:	ldp	x20, x19, [sp, #80]
   1d850:	ldp	x22, x21, [sp, #64]
   1d854:	ldp	x24, x23, [sp, #48]
   1d858:	ldp	x26, x25, [sp, #32]
   1d85c:	ldp	x28, x27, [sp, #16]
   1d860:	ldp	x29, x30, [sp], #96
   1d864:	ret
   1d868:	ldr	x8, [x19, #8]
   1d86c:	mov	w9, #0x1                   	// #1
   1d870:	str	x26, [x8]
   1d874:	str	w9, [x19, #4]
   1d878:	ldur	x0, [x29, #-8]
   1d87c:	cbz	x0, 1d848 <__gmpz_primorial_ui@@Base+0x160>
   1d880:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1d884:	b	1d848 <__gmpz_primorial_ui@@Base+0x160>
   1d888:	mov	w1, #0x1                   	// #1
   1d88c:	mov	x0, x19
   1d890:	bl	c1c0 <__gmpz_realloc@plt>
   1d894:	b	1d738 <__gmpz_primorial_ui@@Base+0x50>
   1d898:	add	x1, x8, #0x1
   1d89c:	mov	x0, x19
   1d8a0:	bl	c1c0 <__gmpz_realloc@plt>
   1d8a4:	mov	x21, x0
   1d8a8:	b	1d760 <__gmpz_primorial_ui@@Base+0x78>
   1d8ac:	sub	x0, x29, #0x8
   1d8b0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1d8b4:	mov	x22, x0
   1d8b8:	b	1d7ac <__gmpz_primorial_ui@@Base+0xc4>
   1d8bc:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1d8c0:	ldr	x9, [x9, #3880]
   1d8c4:	mov	x8, x0
   1d8c8:	mov	w0, #0x9                   	// #9
   1d8cc:	sub	w10, w0, #0x2
   1d8d0:	ldr	x10, [x9, w10, uxtw #3]
   1d8d4:	sub	w0, w0, #0x1
   1d8d8:	cmp	x10, x8
   1d8dc:	b.cc	1d8cc <__gmpz_primorial_ui@@Base+0x1e4>  // b.lo, b.ul, b.last
   1d8e0:	ret
   1d8e4:	sub	x8, x0, #0x5
   1d8e8:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1d8ec:	orr	x8, x8, #0x1
   1d8f0:	movk	x9, #0xaaab
   1d8f4:	umulh	x8, x8, x9
   1d8f8:	lsr	x0, x8, #1
   1d8fc:	ret
   1d900:	add	x8, x0, x0, lsl #1
   1d904:	and	x9, x0, #0x1
   1d908:	add	x8, x8, x9
   1d90c:	add	x0, x8, #0x1
   1d910:	ret

000000000001d914 <__gmpz_probab_prime_p@@Base>:
   1d914:	sub	sp, sp, #0xb0
   1d918:	stp	x20, x19, [sp, #160]
   1d91c:	mov	w19, w1
   1d920:	mov	w1, #0x4240                	// #16960
   1d924:	movk	w1, #0xf, lsl #16
   1d928:	stp	x29, x30, [sp, #80]
   1d92c:	stp	x28, x27, [sp, #96]
   1d930:	stp	x26, x25, [sp, #112]
   1d934:	stp	x24, x23, [sp, #128]
   1d938:	stp	x22, x21, [sp, #144]
   1d93c:	add	x29, sp, #0x50
   1d940:	mov	x20, x0
   1d944:	bl	d3d0 <__gmpz_cmp_ui@plt>
   1d948:	cmp	w0, #0x0
   1d94c:	b.gt	1d980 <__gmpz_probab_prime_p@@Base+0x6c>
   1d950:	mov	w1, #0x4240                	// #16960
   1d954:	movk	w1, #0xf, lsl #16
   1d958:	mov	x0, x20
   1d95c:	bl	c250 <__gmpz_cmpabs_ui@plt>
   1d960:	cmp	w0, #0x0
   1d964:	b.le	1dce8 <__gmpz_probab_prime_p@@Base+0x3d4>
   1d968:	ldr	x8, [x20, #8]
   1d96c:	stur	x8, [x29, #-8]
   1d970:	ldr	w8, [x20, #4]
   1d974:	sub	x20, x29, #0x10
   1d978:	neg	w8, w8
   1d97c:	stur	w8, [x29, #-12]
   1d980:	ldr	x22, [x20, #8]
   1d984:	ldrsw	x21, [x20, #4]
   1d988:	ldr	w8, [x22]
   1d98c:	cmp	x21, #0x0
   1d990:	cset	w9, ne  // ne = any
   1d994:	tst	w8, w9
   1d998:	b.eq	1d9c4 <__gmpz_probab_prime_p@@Base+0xb0>  // b.none
   1d99c:	mov	x2, #0x4e1d                	// #19997
   1d9a0:	movk	x2, #0x30e9, lsl #16
   1d9a4:	movk	x2, #0xf97c, lsl #32
   1d9a8:	movk	x2, #0xe221, lsl #48
   1d9ac:	cmp	w21, #0x14
   1d9b0:	b.le	1d9cc <__gmpz_probab_prime_p@@Base+0xb8>
   1d9b4:	mov	x0, x22
   1d9b8:	mov	x1, x21
   1d9bc:	bl	c540 <__gmpn_mod_1@plt>
   1d9c0:	b	1d9e8 <__gmpz_probab_prime_p@@Base+0xd4>
   1d9c4:	mov	w0, wzr
   1d9c8:	b	1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1d9cc:	mov	x3, #0xb36b                	// #45931
   1d9d0:	movk	x3, #0xc938, lsl #16
   1d9d4:	movk	x3, #0xe6cf, lsl #32
   1d9d8:	movk	x3, #0x21cf, lsl #48
   1d9dc:	mov	x0, x22
   1d9e0:	mov	x1, x21
   1d9e4:	bl	c860 <__gmpn_preinv_mod_1@plt>
   1d9e8:	mov	x9, #0x521d                	// #21021
   1d9ec:	movk	x9, #0x8c13, lsl #16
   1d9f0:	mov	x10, #0x304e                	// #12366
   1d9f4:	movk	x9, #0xb2b7, lsl #32
   1d9f8:	movk	x10, #0xcade, lsl #16
   1d9fc:	movk	x9, #0x21cf, lsl #48
   1da00:	movk	x10, #0x873e, lsl #32
   1da04:	mul	x9, x0, x9
   1da08:	movk	x10, #0x4d4, lsl #48
   1da0c:	mov	x8, x0
   1da10:	cmp	x9, x10
   1da14:	mov	w0, wzr
   1da18:	b.cc	1dd44 <__gmpz_probab_prime_p@@Base+0x430>  // b.lo, b.ul, b.last
   1da1c:	mov	x9, #0x7263                	// #29283
   1da20:	movk	x9, #0x3105, lsl #16
   1da24:	movk	x9, #0x82b9, lsl #32
   1da28:	movk	x9, #0x5c98, lsl #48
   1da2c:	umulh	x9, x8, x9
   1da30:	sub	x10, x8, x9
   1da34:	add	x9, x9, x10, lsr #1
   1da38:	lsr	x9, x9, #5
   1da3c:	mov	w10, #0x2f                  	// #47
   1da40:	msub	x9, x9, x10, x8
   1da44:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1da48:	mov	x9, #0xa0bf                	// #41151
   1da4c:	movk	x9, #0xe82f, lsl #16
   1da50:	movk	x9, #0xfa0b, lsl #32
   1da54:	movk	x9, #0xbe82, lsl #48
   1da58:	umulh	x9, x8, x9
   1da5c:	lsr	x9, x9, #5
   1da60:	mov	w10, #0x2b                  	// #43
   1da64:	msub	x9, x9, x10, x8
   1da68:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1da6c:	mov	x9, #0xce0d                	// #52749
   1da70:	movk	x9, #0xe0c7, lsl #16
   1da74:	movk	x9, #0xc7c, lsl #32
   1da78:	movk	x9, #0xc7ce, lsl #48
   1da7c:	umulh	x9, x8, x9
   1da80:	lsr	x9, x9, #5
   1da84:	mov	w10, #0x29                  	// #41
   1da88:	msub	x9, x9, x10, x8
   1da8c:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1da90:	mov	x9, #0x7c8b                	// #31883
   1da94:	movk	x9, #0xdd6, lsl #16
   1da98:	movk	x9, #0xc8a6, lsl #32
   1da9c:	movk	x9, #0xdd67, lsl #48
   1daa0:	umulh	x9, x8, x9
   1daa4:	lsr	x9, x9, #5
   1daa8:	mov	w10, #0x25                  	// #37
   1daac:	msub	x9, x9, x10, x8
   1dab0:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dab4:	mov	x9, #0x4211                	// #16913
   1dab8:	movk	x9, #0x2108, lsl #16
   1dabc:	movk	x9, #0x1084, lsl #32
   1dac0:	movk	x9, #0x842, lsl #48
   1dac4:	umulh	x9, x8, x9
   1dac8:	sub	x10, x8, x9
   1dacc:	add	x9, x9, x10, lsr #1
   1dad0:	lsr	x9, x9, #4
   1dad4:	sub	x9, x9, x9, lsl #5
   1dad8:	add	x9, x8, x9
   1dadc:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dae0:	mov	x9, #0x611b                	// #24859
   1dae4:	movk	x9, #0xa7b9, lsl #16
   1dae8:	movk	x9, #0x9611, lsl #32
   1daec:	movk	x9, #0x1a7b, lsl #48
   1daf0:	umulh	x9, x8, x9
   1daf4:	sub	x10, x8, x9
   1daf8:	add	x9, x9, x10, lsr #1
   1dafc:	lsr	x9, x9, #4
   1db00:	mov	w10, #0x1d                  	// #29
   1db04:	msub	x9, x9, x10, x8
   1db08:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1db0c:	mov	x9, #0x42c9                	// #17097
   1db10:	movk	x9, #0xb216, lsl #16
   1db14:	movk	x9, #0x8590, lsl #32
   1db18:	movk	x9, #0x642c, lsl #48
   1db1c:	umulh	x9, x8, x9
   1db20:	sub	x10, x8, x9
   1db24:	add	x9, x9, x10, lsr #1
   1db28:	lsr	x9, x9, #4
   1db2c:	mov	w10, #0x17                  	// #23
   1db30:	msub	x9, x9, x10, x8
   1db34:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1db38:	mov	x9, #0x435f                	// #17247
   1db3c:	movk	x9, #0xd79, lsl #16
   1db40:	movk	x9, #0x35e5, lsl #32
   1db44:	movk	x9, #0xd794, lsl #48
   1db48:	umulh	x9, x8, x9
   1db4c:	lsr	x9, x9, #4
   1db50:	mov	w10, #0x13                  	// #19
   1db54:	msub	x9, x9, x10, x8
   1db58:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1db5c:	mov	x9, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   1db60:	movk	x9, #0xf0f1
   1db64:	umulh	x9, x8, x9
   1db68:	lsr	x9, x9, #4
   1db6c:	add	x9, x9, x9, lsl #4
   1db70:	sub	x9, x8, x9
   1db74:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1db78:	mov	x9, #0x4ec5                	// #20165
   1db7c:	movk	x9, #0xc4ec, lsl #16
   1db80:	movk	x9, #0xec4e, lsl #32
   1db84:	movk	x9, #0x4ec4, lsl #48
   1db88:	umulh	x9, x8, x9
   1db8c:	lsr	x9, x9, #2
   1db90:	mov	w10, #0xd                   	// #13
   1db94:	msub	x9, x9, x10, x8
   1db98:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1db9c:	mov	x9, #0x8ba3                	// #35747
   1dba0:	movk	x9, #0xba2e, lsl #16
   1dba4:	movk	x9, #0xa2e8, lsl #32
   1dba8:	movk	x9, #0x2e8b, lsl #48
   1dbac:	umulh	x9, x8, x9
   1dbb0:	lsr	x9, x9, #1
   1dbb4:	mov	w10, #0xb                   	// #11
   1dbb8:	msub	x9, x9, x10, x8
   1dbbc:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dbc0:	mov	x9, #0x2493                	// #9363
   1dbc4:	movk	x9, #0x9249, lsl #16
   1dbc8:	movk	x9, #0x4924, lsl #32
   1dbcc:	movk	x9, #0x2492, lsl #48
   1dbd0:	umulh	x9, x8, x9
   1dbd4:	sub	x10, x8, x9
   1dbd8:	add	x9, x9, x10, lsr #1
   1dbdc:	lsr	x9, x9, #2
   1dbe0:	sub	x9, x9, x9, lsl #3
   1dbe4:	add	x9, x8, x9
   1dbe8:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dbec:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1dbf0:	movk	x9, #0xaaab
   1dbf4:	umulh	x9, x8, x9
   1dbf8:	lsr	x9, x9, #1
   1dbfc:	add	x9, x9, x9, lsl #1
   1dc00:	sub	x9, x8, x9
   1dc04:	cbz	x9, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dc08:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   1dc0c:	movk	x9, #0xcccd
   1dc10:	umulh	x9, x8, x9
   1dc14:	lsr	x9, x9, #2
   1dc18:	add	x9, x9, x9, lsl #2
   1dc1c:	sub	x8, x8, x9
   1dc20:	cbz	x8, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dc24:	mov	w1, #0x2                   	// #2
   1dc28:	mov	x0, x20
   1dc2c:	bl	d440 <__gmpz_sizeinbase@plt>
   1dc30:	cmp	x0, #0x3c
   1dc34:	b.cc	1dcd8 <__gmpz_probab_prime_p@@Base+0x3c4>  // b.lo, b.ul, b.last
   1dc38:	add	x26, sp, #0x4
   1dc3c:	mov	x23, x0
   1dc40:	mov	w28, wzr
   1dc44:	mov	w24, #0x3b                  	// #59
   1dc48:	sub	x27, x26, #0x4
   1dc4c:	mov	w25, #0x1                   	// #1
   1dc50:	b	1dc70 <__gmpz_probab_prime_p@@Base+0x35c>
   1dc54:	mul	x25, x25, x24
   1dc58:	add	w8, w28, #0x1
   1dc5c:	str	w24, [x26, w28, sxtw #2]
   1dc60:	mov	w28, w8
   1dc64:	add	x24, x24, #0x2
   1dc68:	cmp	x24, x23
   1dc6c:	b.cs	1dcd8 <__gmpz_probab_prime_p@@Base+0x3c4>  // b.hs, b.nlast
   1dc70:	mov	x0, x24
   1dc74:	bl	1dd64 <__gmpz_probab_prime_p@@Base+0x450>
   1dc78:	cbz	w0, 1dc64 <__gmpz_probab_prime_p@@Base+0x350>
   1dc7c:	umulh	x8, x25, x24
   1dc80:	cbz	x8, 1dc54 <__gmpz_probab_prime_p@@Base+0x340>
   1dc84:	mov	x0, x22
   1dc88:	mov	x1, x21
   1dc8c:	mov	x2, x25
   1dc90:	cmp	w21, #0x27
   1dc94:	b.le	1dca0 <__gmpz_probab_prime_p@@Base+0x38c>
   1dc98:	bl	c540 <__gmpn_mod_1@plt>
   1dc9c:	b	1dca8 <__gmpz_probab_prime_p@@Base+0x394>
   1dca0:	mov	x3, xzr
   1dca4:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1dca8:	sxtw	x8, w28
   1dcac:	subs	x9, x8, #0x1
   1dcb0:	b.lt	1dccc <__gmpz_probab_prime_p@@Base+0x3b8>  // b.tstop
   1dcb4:	ldr	w2, [x27, x8, lsl #2]
   1dcb8:	udiv	x8, x0, x2
   1dcbc:	msub	x10, x8, x2, x0
   1dcc0:	mov	x8, x9
   1dcc4:	cbnz	x10, 1dcac <__gmpz_probab_prime_p@@Base+0x398>
   1dcc8:	b	1dd08 <__gmpz_probab_prime_p@@Base+0x3f4>
   1dccc:	mov	w28, wzr
   1dcd0:	mov	x25, x24
   1dcd4:	b	1dc58 <__gmpz_probab_prime_p@@Base+0x344>
   1dcd8:	mov	x0, x20
   1dcdc:	mov	w1, w19
   1dce0:	bl	cd40 <__gmpz_millerrabin@plt>
   1dce4:	b	1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dce8:	mov	x0, x20
   1dcec:	bl	c900 <__gmpz_get_ui@plt>
   1dcf0:	cmp	x0, #0x1
   1dcf4:	cset	w8, hi  // hi = pmore
   1dcf8:	tst	x0, x8
   1dcfc:	b.eq	1dd30 <__gmpz_probab_prime_p@@Base+0x41c>  // b.none
   1dd00:	bl	1dd64 <__gmpz_probab_prime_p@@Base+0x450>
   1dd04:	b	1dd38 <__gmpz_probab_prime_p@@Base+0x424>
   1dd08:	mov	x0, x22
   1dd0c:	mov	x1, x21
   1dd10:	bl	c540 <__gmpn_mod_1@plt>
   1dd14:	cbz	x0, 1dd44 <__gmpz_probab_prime_p@@Base+0x430>
   1dd18:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1dd1c:	adrp	x2, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1dd20:	add	x0, x0, #0xcf7
   1dd24:	add	x2, x2, #0xd02
   1dd28:	mov	w1, #0x83                  	// #131
   1dd2c:	bl	c850 <__gmp_assert_fail@plt>
   1dd30:	cmp	x0, #0x2
   1dd34:	cset	w0, eq  // eq = none
   1dd38:	cmp	w0, #0x0
   1dd3c:	cset	w8, ne  // ne = any
   1dd40:	lsl	w0, w8, #1
   1dd44:	ldp	x20, x19, [sp, #160]
   1dd48:	ldp	x22, x21, [sp, #144]
   1dd4c:	ldp	x24, x23, [sp, #128]
   1dd50:	ldp	x26, x25, [sp, #112]
   1dd54:	ldp	x28, x27, [sp, #96]
   1dd58:	ldp	x29, x30, [sp, #80]
   1dd5c:	add	sp, sp, #0xb0
   1dd60:	ret
   1dd64:	mov	w8, #0x3                   	// #3
   1dd68:	udiv	x9, x0, x8
   1dd6c:	cmp	x9, x8
   1dd70:	b.cc	1dd8c <__gmpz_probab_prime_p@@Base+0x478>  // b.lo, b.ul, b.last
   1dd74:	mul	x9, x9, x8
   1dd78:	cmp	x9, x0
   1dd7c:	add	x8, x8, #0x2
   1dd80:	b.ne	1dd68 <__gmpz_probab_prime_p@@Base+0x454>  // b.any
   1dd84:	mov	w0, wzr
   1dd88:	ret
   1dd8c:	mov	w0, #0x1                   	// #1
   1dd90:	ret

000000000001dd94 <__gmpz_random@@Base>:
   1dd94:	stp	x29, x30, [sp, #-32]!
   1dd98:	stp	x20, x19, [sp, #16]
   1dd9c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1dda0:	ldr	x8, [x8, #4040]
   1dda4:	mov	x20, x1
   1dda8:	mov	x19, x0
   1ddac:	mov	x29, sp
   1ddb0:	ldrb	w9, [x8]
   1ddb4:	cbnz	w9, 1ddcc <__gmpz_random@@Base+0x38>
   1ddb8:	mov	w9, #0x1                   	// #1
   1ddbc:	strb	w9, [x8]
   1ddc0:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1ddc4:	ldr	x0, [x0, #3976]
   1ddc8:	bl	c060 <__gmp_randinit_mt_noseed@plt>
   1ddcc:	adrp	x1, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1ddd0:	ldr	x1, [x1, #3976]
   1ddd4:	cmp	x20, #0x0
   1ddd8:	cneg	x8, x20, mi  // mi = first
   1dddc:	lsl	x2, x8, #6
   1dde0:	mov	x0, x19
   1dde4:	bl	d610 <__gmpz_urandomb@plt>
   1dde8:	tbz	x20, #63, 1ddf8 <__gmpz_random@@Base+0x64>
   1ddec:	ldr	w8, [x19, #4]
   1ddf0:	neg	w8, w8
   1ddf4:	str	w8, [x19, #4]
   1ddf8:	ldp	x20, x19, [sp, #16]
   1ddfc:	ldp	x29, x30, [sp], #32
   1de00:	ret

000000000001de04 <__gmpz_random2@@Base>:
   1de04:	stp	x29, x30, [sp, #-48]!
   1de08:	cmp	x1, #0x0
   1de0c:	str	x21, [sp, #16]
   1de10:	stp	x20, x19, [sp, #32]
   1de14:	mov	x19, x1
   1de18:	mov	x20, x0
   1de1c:	cneg	x21, x1, mi  // mi = first
   1de20:	mov	x29, sp
   1de24:	cbz	x1, 1de40 <__gmpz_random2@@Base+0x3c>
   1de28:	ldrsw	x8, [x20]
   1de2c:	cmp	x21, x8
   1de30:	b.gt	1de54 <__gmpz_random2@@Base+0x50>
   1de34:	ldr	x0, [x20, #8]
   1de38:	mov	x1, x21
   1de3c:	bl	d3a0 <__gmpn_random2@plt>
   1de40:	str	w19, [x20, #4]
   1de44:	ldp	x20, x19, [sp, #32]
   1de48:	ldr	x21, [sp, #16]
   1de4c:	ldp	x29, x30, [sp], #48
   1de50:	ret
   1de54:	mov	x0, x20
   1de58:	mov	x1, x21
   1de5c:	bl	c1c0 <__gmpz_realloc@plt>
   1de60:	b	1de38 <__gmpz_random2@@Base+0x34>

000000000001de64 <__gmpz_realloc@@Base>:
   1de64:	stp	x29, x30, [sp, #-32]!
   1de68:	cmp	x1, #0x1
   1de6c:	stp	x20, x19, [sp, #16]
   1de70:	csinc	x20, x1, xzr, gt
   1de74:	mov	w8, #0x80000000            	// #-2147483648
   1de78:	cmp	x20, x8
   1de7c:	mov	x29, sp
   1de80:	b.ge	1def0 <__gmpz_realloc@@Base+0x8c>  // b.tcont
   1de84:	ldrsw	x8, [x0]
   1de88:	mov	x19, x0
   1de8c:	cbz	w8, 1dec8 <__gmpz_realloc@@Base+0x64>
   1de90:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1de94:	ldr	x9, [x9, #3792]
   1de98:	ldr	x0, [x19, #8]
   1de9c:	lsl	x1, x8, #3
   1dea0:	lsl	x2, x20, #3
   1dea4:	ldr	x9, [x9]
   1dea8:	blr	x9
   1deac:	ldr	w8, [x19, #4]
   1deb0:	cmp	w8, #0x0
   1deb4:	cneg	w8, w8, mi  // mi = first
   1deb8:	cmp	x20, x8
   1debc:	b.ge	1dedc <__gmpz_realloc@@Base+0x78>  // b.tcont
   1dec0:	str	wzr, [x19, #4]
   1dec4:	b	1dedc <__gmpz_realloc@@Base+0x78>
   1dec8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1decc:	ldr	x8, [x8, #3840]
   1ded0:	lsl	x0, x20, #3
   1ded4:	ldr	x8, [x8]
   1ded8:	blr	x8
   1dedc:	str	x0, [x19, #8]
   1dee0:	str	w20, [x19]
   1dee4:	ldp	x20, x19, [sp, #16]
   1dee8:	ldp	x29, x30, [sp], #32
   1deec:	ret
   1def0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1def4:	ldr	x8, [x8, #3824]
   1def8:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1defc:	add	x0, x0, #0xc18
   1df00:	mov	w1, #0x1a                  	// #26
   1df04:	ldr	x3, [x8]
   1df08:	mov	w2, #0x1                   	// #1
   1df0c:	bl	d000 <fwrite@plt>
   1df10:	bl	cab0 <abort@plt>

000000000001df14 <__gmpz_realloc2@@Base>:
   1df14:	stp	x29, x30, [sp, #-32]!
   1df18:	cmp	x1, #0x0
   1df1c:	cset	w8, ne  // ne = any
   1df20:	sub	x8, x1, x8
   1df24:	mov	x9, #0x1fffffffc0          	// #137438953408
   1df28:	cmp	x8, x9
   1df2c:	stp	x20, x19, [sp, #16]
   1df30:	mov	x29, sp
   1df34:	b.cs	1dfb0 <__gmpz_realloc2@@Base+0x9c>  // b.hs, b.nlast
   1df38:	ldrsw	x9, [x0]
   1df3c:	lsr	x8, x8, #6
   1df40:	mov	x19, x0
   1df44:	add	x20, x8, #0x1
   1df48:	cbz	w9, 1df88 <__gmpz_realloc2@@Base+0x74>
   1df4c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1df50:	ldr	x8, [x8, #3792]
   1df54:	ldr	x0, [x19, #8]
   1df58:	lsl	x1, x9, #3
   1df5c:	lsl	x2, x20, #3
   1df60:	ldr	x8, [x8]
   1df64:	blr	x8
   1df68:	ldr	w8, [x19, #4]
   1df6c:	str	x0, [x19, #8]
   1df70:	cmp	w8, #0x0
   1df74:	cneg	w8, w8, mi  // mi = first
   1df78:	cmp	x20, x8
   1df7c:	b.cs	1dfa0 <__gmpz_realloc2@@Base+0x8c>  // b.hs, b.nlast
   1df80:	str	wzr, [x19, #4]
   1df84:	b	1dfa0 <__gmpz_realloc2@@Base+0x8c>
   1df88:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1df8c:	ldr	x8, [x8, #3840]
   1df90:	lsl	x0, x20, #3
   1df94:	ldr	x8, [x8]
   1df98:	blr	x8
   1df9c:	str	x0, [x19, #8]
   1dfa0:	str	w20, [x19]
   1dfa4:	ldp	x20, x19, [sp, #16]
   1dfa8:	ldp	x29, x30, [sp], #32
   1dfac:	ret
   1dfb0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1dfb4:	ldr	x8, [x8, #3824]
   1dfb8:	adrp	x0, 4c000 <__gmp_randclear_mt@@Base+0x18>
   1dfbc:	add	x0, x0, #0xc18
   1dfc0:	mov	w1, #0x1a                  	// #26
   1dfc4:	ldr	x3, [x8]
   1dfc8:	mov	w2, #0x1                   	// #1
   1dfcc:	bl	d000 <fwrite@plt>
   1dfd0:	bl	cab0 <abort@plt>

000000000001dfd4 <__gmpz_remove@@Base>:
   1dfd4:	stp	x29, x30, [sp, #-80]!
   1dfd8:	stp	x28, x25, [sp, #16]
   1dfdc:	stp	x24, x23, [sp, #32]
   1dfe0:	stp	x22, x21, [sp, #48]
   1dfe4:	stp	x20, x19, [sp, #64]
   1dfe8:	mov	x29, sp
   1dfec:	sub	sp, sp, #0x420
   1dff0:	ldr	x8, [x2, #8]
   1dff4:	ldrsw	x9, [x2, #4]
   1dff8:	ldrsw	x24, [x1, #4]
   1dffc:	mov	x21, x1
   1e000:	ldr	x8, [x8]
   1e004:	cmp	x9, #0x0
   1e008:	cneg	x22, x9, mi  // mi = first
   1e00c:	mov	x19, x0
   1e010:	cmp	x8, #0x1
   1e014:	cset	w10, eq  // eq = none
   1e018:	cbz	w24, 1e288 <__gmpz_remove@@Base+0x2b4>
   1e01c:	cmp	x22, x10
   1e020:	b.le	1e288 <__gmpz_remove@@Base+0x2b4>
   1e024:	mov	x20, x2
   1e028:	and	x23, x9, #0xffffffff
   1e02c:	tbnz	w8, #0, 1e07c <__gmpz_remove@@Base+0xa8>
   1e030:	cmp	x8, #0x2
   1e034:	cset	w8, eq  // eq = none
   1e038:	cmp	x22, x8
   1e03c:	b.ne	1e0d8 <__gmpz_remove@@Base+0x104>  // b.any
   1e040:	mov	x0, x21
   1e044:	mov	x1, xzr
   1e048:	bl	c050 <__gmpz_scan1@plt>
   1e04c:	mov	x20, x0
   1e050:	mov	x0, x19
   1e054:	mov	x1, x21
   1e058:	mov	x2, x20
   1e05c:	bl	c7e0 <__gmpz_fdiv_q_2exp@plt>
   1e060:	ubfx	x8, x23, #31, #1
   1e064:	tst	x20, x8
   1e068:	b.eq	1e124 <__gmpz_remove@@Base+0x150>  // b.none
   1e06c:	mov	x0, x19
   1e070:	mov	x1, x19
   1e074:	bl	c6a0 <__gmpz_neg@plt>
   1e078:	b	1e124 <__gmpz_remove@@Base+0x150>
   1e07c:	cmp	x24, #0x0
   1e080:	cneg	x1, x24, mi  // mi = first
   1e084:	str	x1, [sp]
   1e088:	ldrsw	x8, [x19]
   1e08c:	cmp	x1, x8
   1e090:	b.gt	1e2a0 <__gmpz_remove@@Base+0x2cc>
   1e094:	ldr	x0, [x19, #8]
   1e098:	ldr	x2, [x21, #8]
   1e09c:	ldr	x3, [sp]
   1e0a0:	ldr	x4, [x20, #8]
   1e0a4:	mov	x1, sp
   1e0a8:	mov	x6, #0xffffffffffffffff    	// #-1
   1e0ac:	mov	x5, x22
   1e0b0:	bl	cf10 <__gmpn_remove@plt>
   1e0b4:	ldr	x8, [sp]
   1e0b8:	and	x9, x0, x23, lsr #31
   1e0bc:	ubfx	x10, x24, #31, #1
   1e0c0:	cmp	x9, x10
   1e0c4:	neg	w11, w8
   1e0c8:	csel	x8, x8, x11, eq  // eq = none
   1e0cc:	mov	x20, x0
   1e0d0:	str	w8, [x19, #4]
   1e0d4:	b	1e124 <__gmpz_remove@@Base+0x150>
   1e0d8:	sub	x0, x29, #0x20
   1e0dc:	bl	d430 <__gmpz_init@plt>
   1e0e0:	sub	x0, x29, #0x10
   1e0e4:	bl	d430 <__gmpz_init@plt>
   1e0e8:	sub	x0, x29, #0x10
   1e0ec:	sub	x1, x29, #0x20
   1e0f0:	mov	x2, x21
   1e0f4:	mov	x3, x20
   1e0f8:	bl	c120 <__gmpz_tdiv_qr@plt>
   1e0fc:	ldur	w8, [x29, #-28]
   1e100:	cbz	w8, 1e144 <__gmpz_remove@@Base+0x170>
   1e104:	mov	x0, x19
   1e108:	mov	x1, x21
   1e10c:	bl	c590 <__gmpz_set@plt>
   1e110:	mov	x20, xzr
   1e114:	sub	x0, x29, #0x10
   1e118:	bl	cd10 <__gmpz_clear@plt>
   1e11c:	sub	x0, x29, #0x20
   1e120:	bl	cd10 <__gmpz_clear@plt>
   1e124:	mov	x0, x20
   1e128:	add	sp, sp, #0x420
   1e12c:	ldp	x20, x19, [sp, #64]
   1e130:	ldp	x22, x21, [sp, #48]
   1e134:	ldp	x24, x23, [sp, #32]
   1e138:	ldp	x28, x25, [sp, #16]
   1e13c:	ldp	x29, x30, [sp], #80
   1e140:	ret
   1e144:	mov	x0, sp
   1e148:	mov	x1, x20
   1e14c:	mov	x22, sp
   1e150:	bl	c0b0 <__gmpz_init_set@plt>
   1e154:	sub	x1, x29, #0x10
   1e158:	mov	x0, x19
   1e15c:	bl	c710 <__gmpz_swap@plt>
   1e160:	ldr	w8, [x19, #4]
   1e164:	ldr	w9, [sp, #4]
   1e168:	cmp	w8, #0x0
   1e16c:	cneg	w8, w8, mi  // mi = first
   1e170:	cmp	w9, #0x0
   1e174:	cneg	w9, w9, mi  // mi = first
   1e178:	lsl	w9, w9, #1
   1e17c:	sub	w9, w9, #0x1
   1e180:	cmp	w8, w9
   1e184:	b.ge	1e190 <__gmpz_remove@@Base+0x1bc>  // b.tcont
   1e188:	mov	w23, #0x1                   	// #1
   1e18c:	b	1e218 <__gmpz_remove@@Base+0x244>
   1e190:	add	x20, x22, #0x10
   1e194:	mov	w23, #0x1                   	// #1
   1e198:	mov	x0, x20
   1e19c:	sub	x21, x20, #0x10
   1e1a0:	bl	d430 <__gmpz_init@plt>
   1e1a4:	mov	x0, x20
   1e1a8:	mov	x1, x21
   1e1ac:	mov	x2, x21
   1e1b0:	bl	c620 <__gmpz_mul@plt>
   1e1b4:	sub	x0, x29, #0x10
   1e1b8:	sub	x1, x29, #0x20
   1e1bc:	mov	x2, x19
   1e1c0:	mov	x3, x20
   1e1c4:	bl	c120 <__gmpz_tdiv_qr@plt>
   1e1c8:	ldur	w8, [x29, #-28]
   1e1cc:	cbnz	w8, 1e210 <__gmpz_remove@@Base+0x23c>
   1e1d0:	sub	x1, x29, #0x10
   1e1d4:	mov	x0, x19
   1e1d8:	bl	c710 <__gmpz_swap@plt>
   1e1dc:	ldr	w8, [x19, #4]
   1e1e0:	ldr	w9, [x20, #4]
   1e1e4:	add	w23, w23, #0x1
   1e1e8:	add	x20, x20, #0x10
   1e1ec:	cmp	w8, #0x0
   1e1f0:	cneg	w8, w8, mi  // mi = first
   1e1f4:	cmp	w9, #0x0
   1e1f8:	cneg	w9, w9, mi  // mi = first
   1e1fc:	lsl	w9, w9, #1
   1e200:	sub	w9, w9, #0x1
   1e204:	cmp	w8, w9
   1e208:	b.ge	1e198 <__gmpz_remove@@Base+0x1c4>  // b.tcont
   1e20c:	b	1e218 <__gmpz_remove@@Base+0x244>
   1e210:	mov	x0, x20
   1e214:	bl	cd10 <__gmpz_clear@plt>
   1e218:	mov	x8, #0xffffffffffffffff    	// #-1
   1e21c:	sub	w25, w23, #0x1
   1e220:	lsl	x8, x8, x23
   1e224:	add	w24, w23, #0x1
   1e228:	add	x21, x22, w25, sxtw #4
   1e22c:	mvn	x20, x8
   1e230:	mov	w22, #0x1                   	// #1
   1e234:	b	1e254 <__gmpz_remove@@Base+0x280>
   1e238:	mov	x0, x21
   1e23c:	bl	cd10 <__gmpz_clear@plt>
   1e240:	sub	w24, w24, #0x1
   1e244:	sub	x21, x21, #0x10
   1e248:	cmp	w24, #0x1
   1e24c:	sub	x25, x25, #0x1
   1e250:	b.le	1e114 <__gmpz_remove@@Base+0x140>
   1e254:	sub	x0, x29, #0x10
   1e258:	sub	x1, x29, #0x20
   1e25c:	mov	x2, x19
   1e260:	mov	x3, x21
   1e264:	bl	c120 <__gmpz_tdiv_qr@plt>
   1e268:	ldur	w8, [x29, #-28]
   1e26c:	cbnz	w8, 1e238 <__gmpz_remove@@Base+0x264>
   1e270:	lsl	x8, x22, x25
   1e274:	sub	x1, x29, #0x10
   1e278:	mov	x0, x19
   1e27c:	add	x20, x8, x20
   1e280:	bl	c710 <__gmpz_swap@plt>
   1e284:	b	1e238 <__gmpz_remove@@Base+0x264>
   1e288:	cbz	x22, 1e2ac <__gmpz_remove@@Base+0x2d8>
   1e28c:	mov	x0, x19
   1e290:	mov	x1, x21
   1e294:	bl	c590 <__gmpz_set@plt>
   1e298:	mov	x20, xzr
   1e29c:	b	1e124 <__gmpz_remove@@Base+0x150>
   1e2a0:	mov	x0, x19
   1e2a4:	bl	c1c0 <__gmpz_realloc@plt>
   1e2a8:	b	1e098 <__gmpz_remove@@Base+0xc4>
   1e2ac:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001e2b0 <__gmpz_roinit_n@@Base>:
   1e2b0:	cmp	x2, #0x0
   1e2b4:	cneg	x9, x2, mi  // mi = first
   1e2b8:	mov	x8, x9
   1e2bc:	subs	x9, x9, #0x1
   1e2c0:	b.lt	1e2d0 <__gmpz_roinit_n@@Base+0x20>  // b.tstop
   1e2c4:	add	x10, x1, x8, lsl #3
   1e2c8:	ldur	x10, [x10, #-8]
   1e2cc:	cbz	x10, 1e2b8 <__gmpz_roinit_n@@Base+0x8>
   1e2d0:	neg	w9, w8
   1e2d4:	cmp	x2, #0x0
   1e2d8:	csel	x8, x9, x8, lt  // lt = tstop
   1e2dc:	stp	wzr, w8, [x0]
   1e2e0:	str	x1, [x0, #8]
   1e2e4:	ret

000000000001e2e8 <__gmpz_root@@Base>:
   1e2e8:	stp	x29, x30, [sp, #-96]!
   1e2ec:	stp	x26, x25, [sp, #32]
   1e2f0:	stp	x24, x23, [sp, #48]
   1e2f4:	stp	x22, x21, [sp, #64]
   1e2f8:	stp	x20, x19, [sp, #80]
   1e2fc:	ldr	w8, [x1, #4]
   1e300:	mov	x22, x2
   1e304:	mov	x20, x1
   1e308:	mov	x19, x0
   1e30c:	str	x27, [sp, #16]
   1e310:	mov	x29, sp
   1e314:	tbnz	w22, #0, 1e31c <__gmpz_root@@Base+0x34>
   1e318:	tbnz	w8, #31, 1e460 <__gmpz_root@@Base+0x178>
   1e31c:	cbz	x22, 1e464 <__gmpz_root@@Base+0x17c>
   1e320:	sxtw	x26, w8
   1e324:	cbz	w26, 1e360 <__gmpz_root@@Base+0x78>
   1e328:	cmp	w26, #0x0
   1e32c:	cneg	x23, x26, lt  // lt = tstop
   1e330:	sub	x8, x23, #0x1
   1e334:	udiv	x27, x8, x22
   1e338:	cmp	x20, x19
   1e33c:	add	x21, x27, #0x1
   1e340:	str	xzr, [x29, #24]
   1e344:	b.eq	1e370 <__gmpz_root@@Base+0x88>  // b.none
   1e348:	cbz	x19, 1e370 <__gmpz_root@@Base+0x88>
   1e34c:	ldrsw	x8, [x19]
   1e350:	cmp	x21, x8
   1e354:	b.gt	1e440 <__gmpz_root@@Base+0x158>
   1e358:	ldr	x24, [x19, #8]
   1e35c:	b	1e394 <__gmpz_root@@Base+0xac>
   1e360:	cbz	x19, 1e430 <__gmpz_root@@Base+0x148>
   1e364:	str	wzr, [x19, #4]
   1e368:	mov	w0, #0x1                   	// #1
   1e36c:	b	1e410 <__gmpz_root@@Base+0x128>
   1e370:	lsl	x1, x21, #3
   1e374:	mov	w8, #0x7f00                	// #32512
   1e378:	cmp	x1, x8
   1e37c:	b.hi	1e450 <__gmpz_root@@Base+0x168>  // b.pmore
   1e380:	add	x9, x1, #0xf
   1e384:	mov	x8, sp
   1e388:	and	x9, x9, #0xfffffffffffffff0
   1e38c:	sub	x24, x8, x9
   1e390:	mov	sp, x24
   1e394:	ldr	x25, [x20, #8]
   1e398:	mov	x0, x24
   1e39c:	cmp	x22, #0x1
   1e3a0:	b.ne	1e3bc <__gmpz_root@@Base+0xd4>  // b.any
   1e3a4:	mov	x1, x25
   1e3a8:	mov	x2, x23
   1e3ac:	bl	cc10 <__gmpn_copyi@plt>
   1e3b0:	mov	x22, xzr
   1e3b4:	cbnz	x19, 1e3d8 <__gmpz_root@@Base+0xf0>
   1e3b8:	b	1e400 <__gmpz_root@@Base+0x118>
   1e3bc:	mov	x1, xzr
   1e3c0:	mov	x2, x25
   1e3c4:	mov	x3, x23
   1e3c8:	mov	x4, x22
   1e3cc:	bl	c3f0 <__gmpn_rootrem@plt>
   1e3d0:	mov	x22, x0
   1e3d4:	cbz	x19, 1e400 <__gmpz_root@@Base+0x118>
   1e3d8:	mvn	w8, w27
   1e3dc:	cmp	w26, #0x0
   1e3e0:	csel	x8, x21, x8, ge  // ge = tcont
   1e3e4:	cmp	x20, x19
   1e3e8:	str	w8, [x19, #4]
   1e3ec:	b.ne	1e400 <__gmpz_root@@Base+0x118>  // b.any
   1e3f0:	mov	x0, x25
   1e3f4:	mov	x1, x24
   1e3f8:	mov	x2, x21
   1e3fc:	bl	cc10 <__gmpn_copyi@plt>
   1e400:	ldr	x0, [x29, #24]
   1e404:	cbnz	x0, 1e438 <__gmpz_root@@Base+0x150>
   1e408:	cmp	x22, #0x0
   1e40c:	cset	w0, eq  // eq = none
   1e410:	mov	sp, x29
   1e414:	ldp	x20, x19, [sp, #80]
   1e418:	ldp	x22, x21, [sp, #64]
   1e41c:	ldp	x24, x23, [sp, #48]
   1e420:	ldp	x26, x25, [sp, #32]
   1e424:	ldr	x27, [sp, #16]
   1e428:	ldp	x29, x30, [sp], #96
   1e42c:	ret
   1e430:	mov	w0, #0x1                   	// #1
   1e434:	b	1e410 <__gmpz_root@@Base+0x128>
   1e438:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1e43c:	b	1e408 <__gmpz_root@@Base+0x120>
   1e440:	mov	x0, x19
   1e444:	mov	x1, x21
   1e448:	bl	c1c0 <__gmpz_realloc@plt>
   1e44c:	b	1e458 <__gmpz_root@@Base+0x170>
   1e450:	add	x0, x29, #0x18
   1e454:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1e458:	mov	x24, x0
   1e45c:	b	1e394 <__gmpz_root@@Base+0xac>
   1e460:	bl	d1c0 <__gmp_sqrt_of_negative@plt>
   1e464:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001e468 <__gmpz_rootrem@@Base>:
   1e468:	stp	x29, x30, [sp, #-96]!
   1e46c:	stp	x28, x27, [sp, #16]
   1e470:	stp	x26, x25, [sp, #32]
   1e474:	stp	x24, x23, [sp, #48]
   1e478:	stp	x22, x21, [sp, #64]
   1e47c:	stp	x20, x19, [sp, #80]
   1e480:	mov	x29, sp
   1e484:	sub	sp, sp, #0x10
   1e488:	ldr	w8, [x2, #4]
   1e48c:	mov	x23, x3
   1e490:	mov	x20, x2
   1e494:	mov	x19, x1
   1e498:	mov	x21, x0
   1e49c:	tbnz	w23, #0, 1e4a4 <__gmpz_rootrem@@Base+0x3c>
   1e4a0:	tbnz	w8, #31, 1e678 <__gmpz_rootrem@@Base+0x210>
   1e4a4:	cbz	x23, 1e67c <__gmpz_rootrem@@Base+0x214>
   1e4a8:	sxtw	x28, w8
   1e4ac:	cbz	w28, 1e4f0 <__gmpz_rootrem@@Base+0x88>
   1e4b0:	cmp	w28, #0x0
   1e4b4:	cneg	x24, x28, lt  // lt = tstop
   1e4b8:	sub	x8, x24, #0x1
   1e4bc:	udiv	x22, x8, x23
   1e4c0:	cmp	x20, x21
   1e4c4:	add	x1, x22, #0x1
   1e4c8:	stp	x1, xzr, [x29, #-16]
   1e4cc:	b.eq	1e500 <__gmpz_rootrem@@Base+0x98>  // b.none
   1e4d0:	cbz	x21, 1e500 <__gmpz_rootrem@@Base+0x98>
   1e4d4:	ldrsw	x8, [x21]
   1e4d8:	cmp	x1, x8
   1e4dc:	b.gt	1e550 <__gmpz_rootrem@@Base+0xe8>
   1e4e0:	ldr	x25, [x21, #8]
   1e4e4:	cmp	x20, x19
   1e4e8:	b.ne	1e52c <__gmpz_rootrem@@Base+0xc4>  // b.any
   1e4ec:	b	1e570 <__gmpz_rootrem@@Base+0x108>
   1e4f0:	cbz	x21, 1e4f8 <__gmpz_rootrem@@Base+0x90>
   1e4f4:	str	wzr, [x21, #4]
   1e4f8:	str	wzr, [x19, #4]
   1e4fc:	b	1e628 <__gmpz_rootrem@@Base+0x1c0>
   1e500:	lsl	x1, x1, #3
   1e504:	mov	w8, #0x7f00                	// #32512
   1e508:	cmp	x1, x8
   1e50c:	b.hi	1e55c <__gmpz_rootrem@@Base+0xf4>  // b.pmore
   1e510:	add	x9, x1, #0xf
   1e514:	mov	x8, sp
   1e518:	and	x9, x9, #0xfffffffffffffff0
   1e51c:	sub	x25, x8, x9
   1e520:	mov	sp, x25
   1e524:	cmp	x20, x19
   1e528:	b.eq	1e570 <__gmpz_rootrem@@Base+0x108>  // b.none
   1e52c:	ldrsw	x8, [x19]
   1e530:	cmp	x24, x8
   1e534:	b.gt	1e540 <__gmpz_rootrem@@Base+0xd8>
   1e538:	ldr	x26, [x19, #8]
   1e53c:	b	1e594 <__gmpz_rootrem@@Base+0x12c>
   1e540:	mov	x0, x19
   1e544:	mov	x1, x24
   1e548:	bl	c1c0 <__gmpz_realloc@plt>
   1e54c:	b	1e670 <__gmpz_rootrem@@Base+0x208>
   1e550:	mov	x0, x21
   1e554:	bl	c1c0 <__gmpz_realloc@plt>
   1e558:	b	1e564 <__gmpz_rootrem@@Base+0xfc>
   1e55c:	sub	x0, x29, #0x8
   1e560:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1e564:	mov	x25, x0
   1e568:	cmp	x20, x19
   1e56c:	b.ne	1e52c <__gmpz_rootrem@@Base+0xc4>  // b.any
   1e570:	lsl	x1, x24, #3
   1e574:	mov	w8, #0x7f00                	// #32512
   1e578:	cmp	x1, x8
   1e57c:	b.hi	1e668 <__gmpz_rootrem@@Base+0x200>  // b.pmore
   1e580:	add	x9, x1, #0xf
   1e584:	mov	x8, sp
   1e588:	and	x9, x9, #0xfffffffffffffff0
   1e58c:	sub	x26, x8, x9
   1e590:	mov	sp, x26
   1e594:	ldr	x27, [x20, #8]
   1e598:	mov	x0, x25
   1e59c:	cmp	x23, #0x1
   1e5a0:	b.ne	1e5c0 <__gmpz_rootrem@@Base+0x158>  // b.any
   1e5a4:	mov	x1, x27
   1e5a8:	mov	x2, x24
   1e5ac:	bl	cc10 <__gmpn_copyi@plt>
   1e5b0:	mov	x23, xzr
   1e5b4:	ldur	x2, [x29, #-16]
   1e5b8:	cbnz	x21, 1e5e0 <__gmpz_rootrem@@Base+0x178>
   1e5bc:	b	1e5f8 <__gmpz_rootrem@@Base+0x190>
   1e5c0:	mov	x1, x26
   1e5c4:	mov	x2, x27
   1e5c8:	mov	x3, x24
   1e5cc:	mov	x4, x23
   1e5d0:	bl	c3f0 <__gmpn_rootrem@plt>
   1e5d4:	mov	x23, x0
   1e5d8:	ldur	x2, [x29, #-16]
   1e5dc:	cbz	x21, 1e5f8 <__gmpz_rootrem@@Base+0x190>
   1e5e0:	mvn	w8, w22
   1e5e4:	cmp	w28, #0x0
   1e5e8:	csel	x8, x2, x8, ge  // ge = tcont
   1e5ec:	cmp	x20, x21
   1e5f0:	str	w8, [x21, #4]
   1e5f4:	b.eq	1e648 <__gmpz_rootrem@@Base+0x1e0>  // b.none
   1e5f8:	cmp	x20, x19
   1e5fc:	b.ne	1e610 <__gmpz_rootrem@@Base+0x1a8>  // b.any
   1e600:	mov	x0, x27
   1e604:	mov	x1, x26
   1e608:	mov	x2, x23
   1e60c:	bl	cc10 <__gmpn_copyi@plt>
   1e610:	neg	w8, w23
   1e614:	cmp	w28, #0x0
   1e618:	csel	x8, x23, x8, ge  // ge = tcont
   1e61c:	str	w8, [x19, #4]
   1e620:	ldur	x0, [x29, #-8]
   1e624:	cbnz	x0, 1e660 <__gmpz_rootrem@@Base+0x1f8>
   1e628:	mov	sp, x29
   1e62c:	ldp	x20, x19, [sp, #80]
   1e630:	ldp	x22, x21, [sp, #64]
   1e634:	ldp	x24, x23, [sp, #48]
   1e638:	ldp	x26, x25, [sp, #32]
   1e63c:	ldp	x28, x27, [sp, #16]
   1e640:	ldp	x29, x30, [sp], #96
   1e644:	ret
   1e648:	mov	x0, x27
   1e64c:	mov	x1, x25
   1e650:	bl	cc10 <__gmpn_copyi@plt>
   1e654:	cmp	x20, x19
   1e658:	b.ne	1e610 <__gmpz_rootrem@@Base+0x1a8>  // b.any
   1e65c:	b	1e600 <__gmpz_rootrem@@Base+0x198>
   1e660:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1e664:	b	1e628 <__gmpz_rootrem@@Base+0x1c0>
   1e668:	sub	x0, x29, #0x8
   1e66c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1e670:	mov	x26, x0
   1e674:	b	1e594 <__gmpz_rootrem@@Base+0x12c>
   1e678:	bl	d1c0 <__gmp_sqrt_of_negative@plt>
   1e67c:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001e680 <__gmpz_rrandomb@@Base>:
   1e680:	stp	x29, x30, [sp, #-48]!
   1e684:	add	x8, x2, #0x3f
   1e688:	stp	x20, x19, [sp, #32]
   1e68c:	mov	x19, x0
   1e690:	lsr	x20, x8, #6
   1e694:	stp	x22, x21, [sp, #16]
   1e698:	mov	x29, sp
   1e69c:	cbz	x2, 1e6c4 <__gmpz_rrandomb@@Base+0x44>
   1e6a0:	ldrsw	x8, [x19]
   1e6a4:	mov	x21, x2
   1e6a8:	mov	x22, x1
   1e6ac:	cmp	x20, x8
   1e6b0:	b.gt	1e6d8 <__gmpz_rrandomb@@Base+0x58>
   1e6b4:	ldr	x0, [x19, #8]
   1e6b8:	mov	x1, x22
   1e6bc:	mov	x2, x21
   1e6c0:	bl	1e6e8 <__gmpz_rrandomb@@Base+0x68>
   1e6c4:	str	w20, [x19, #4]
   1e6c8:	ldp	x20, x19, [sp, #32]
   1e6cc:	ldp	x22, x21, [sp, #16]
   1e6d0:	ldp	x29, x30, [sp], #48
   1e6d4:	ret
   1e6d8:	mov	x0, x19
   1e6dc:	mov	x1, x20
   1e6e0:	bl	c1c0 <__gmpz_realloc@plt>
   1e6e4:	b	1e6b8 <__gmpz_rrandomb@@Base+0x38>
   1e6e8:	stp	x29, x30, [sp, #-80]!
   1e6ec:	stp	x22, x21, [sp, #48]
   1e6f0:	mov	x21, x2
   1e6f4:	add	x9, x2, #0x3f
   1e6f8:	neg	w10, w21
   1e6fc:	mov	x11, #0xffffffffffffffff    	// #-1
   1e700:	lsr	x8, x9, #6
   1e704:	stp	x20, x19, [sp, #64]
   1e708:	mov	x19, x1
   1e70c:	mov	x20, x0
   1e710:	lsr	x10, x11, x10
   1e714:	add	x11, x0, x8, lsl #3
   1e718:	cmp	x9, #0x80
   1e71c:	str	x25, [sp, #16]
   1e720:	stp	x24, x23, [sp, #32]
   1e724:	mov	x29, sp
   1e728:	stur	x10, [x11, #-8]
   1e72c:	b.cc	1e758 <__gmpz_rrandomb@@Base+0xd8>  // b.lo, b.ul, b.last
   1e730:	cmp	x8, #0x2
   1e734:	mov	w9, #0x2                   	// #2
   1e738:	csel	x9, x8, x9, cc  // cc = lo, ul, last
   1e73c:	sub	x8, x8, x9
   1e740:	add	x10, x20, x9, lsl #3
   1e744:	lsl	x8, x8, #3
   1e748:	sub	x0, x10, #0x10
   1e74c:	add	x2, x8, #0x8
   1e750:	mov	w1, #0xff                  	// #255
   1e754:	bl	c780 <memset@plt>
   1e758:	ldr	x8, [x19, #24]
   1e75c:	add	x1, x29, #0x18
   1e760:	mov	w2, #0x20                  	// #32
   1e764:	mov	x0, x19
   1e768:	ldr	x8, [x8, #8]
   1e76c:	blr	x8
   1e770:	ldr	x8, [x29, #24]
   1e774:	add	x22, x20, #0x8
   1e778:	mov	w24, #0x1                   	// #1
   1e77c:	and	x8, x8, #0x3
   1e780:	add	x8, x8, #0x1
   1e784:	udiv	x8, x21, x8
   1e788:	cmp	w8, #0x0
   1e78c:	cinc	w23, w8, eq  // eq = none
   1e790:	b	1e79c <__gmpz_rrandomb@@Base+0x11c>
   1e794:	cmp	x25, x8
   1e798:	b.ls	1e84c <__gmpz_rrandomb@@Base+0x1cc>  // b.plast
   1e79c:	ldr	x8, [x19, #24]
   1e7a0:	add	x1, x29, #0x18
   1e7a4:	mov	w2, #0x20                  	// #32
   1e7a8:	mov	x0, x19
   1e7ac:	ldr	x8, [x8, #8]
   1e7b0:	blr	x8
   1e7b4:	ldr	x8, [x29, #24]
   1e7b8:	udiv	x9, x8, x23
   1e7bc:	msub	w8, w9, w23, w8
   1e7c0:	add	w8, w8, #0x1
   1e7c4:	subs	x8, x21, x8
   1e7c8:	csel	x25, xzr, x8, cc  // cc = lo, ul, last
   1e7cc:	b.ls	1e84c <__gmpz_rrandomb@@Base+0x1cc>  // b.plast
   1e7d0:	lsr	x8, x25, #3
   1e7d4:	and	x8, x8, #0x1ffffffffffffff8
   1e7d8:	ldr	x9, [x20, x8]
   1e7dc:	lsl	x10, x24, x25
   1e7e0:	add	x1, x29, #0x18
   1e7e4:	mov	w2, #0x20                  	// #32
   1e7e8:	eor	x9, x9, x10
   1e7ec:	str	x9, [x20, x8]
   1e7f0:	ldr	x8, [x19, #24]
   1e7f4:	mov	x0, x19
   1e7f8:	ldr	x8, [x8, #8]
   1e7fc:	blr	x8
   1e800:	ldr	x8, [x29, #24]
   1e804:	udiv	x9, x8, x23
   1e808:	msub	w8, w9, w23, w8
   1e80c:	add	w8, w8, #0x1
   1e810:	subs	x9, x25, x8
   1e814:	csel	x21, xzr, x9, cc  // cc = lo, ul, last
   1e818:	lsr	x9, x21, #6
   1e81c:	lsl	x10, x9, #3
   1e820:	ldr	x11, [x20, x10]
   1e824:	lsl	x12, x24, x21
   1e828:	adds	x11, x11, x12
   1e82c:	str	x11, [x20, x10]
   1e830:	b.cc	1e794 <__gmpz_rrandomb@@Base+0x114>  // b.lo, b.ul, b.last
   1e834:	add	x9, x22, x9, lsl #3
   1e838:	ldr	x10, [x9]
   1e83c:	adds	x10, x10, #0x1
   1e840:	str	x10, [x9], #8
   1e844:	b.cs	1e838 <__gmpz_rrandomb@@Base+0x1b8>  // b.hs, b.nlast
   1e848:	b	1e794 <__gmpz_rrandomb@@Base+0x114>
   1e84c:	ldp	x20, x19, [sp, #64]
   1e850:	ldp	x22, x21, [sp, #48]
   1e854:	ldp	x24, x23, [sp, #32]
   1e858:	ldr	x25, [sp, #16]
   1e85c:	ldp	x29, x30, [sp], #80
   1e860:	ret

000000000001e864 <__gmpz_scan0@@Base>:
   1e864:	ldrsw	x12, [x0, #4]
   1e868:	lsr	x13, x1, #6
   1e86c:	cmp	x12, #0x0
   1e870:	cneg	x10, x12, mi  // mi = first
   1e874:	cmp	x13, x10
   1e878:	b.ge	1e8d4 <__gmpz_scan0@@Base+0x70>  // b.tcont
   1e87c:	ldr	x8, [x0, #8]
   1e880:	add	x9, x8, x13, lsl #3
   1e884:	ldr	x11, [x9]
   1e888:	tbnz	w12, #31, 1e8e0 <__gmpz_scan0@@Base+0x7c>
   1e88c:	mov	x12, #0xffffffffffffffff    	// #-1
   1e890:	lsl	x12, x12, x1
   1e894:	orn	x11, x11, x12
   1e898:	cmn	x11, #0x1
   1e89c:	b.ne	1e8cc <__gmpz_scan0@@Base+0x68>  // b.any
   1e8a0:	lsl	x11, x10, #3
   1e8a4:	sub	x11, x11, x13, lsl #3
   1e8a8:	sub	x12, x11, #0x8
   1e8ac:	cbz	x12, 1e944 <__gmpz_scan0@@Base+0xe0>
   1e8b0:	ldr	x11, [x9, #8]
   1e8b4:	add	x13, x9, #0x8
   1e8b8:	sub	x12, x12, #0x8
   1e8bc:	mov	x9, x13
   1e8c0:	cmn	x11, #0x1
   1e8c4:	b.eq	1e8ac <__gmpz_scan0@@Base+0x48>  // b.none
   1e8c8:	mov	x9, x13
   1e8cc:	mvn	x10, x11
   1e8d0:	b	1e930 <__gmpz_scan0@@Base+0xcc>
   1e8d4:	cmp	w12, #0x0
   1e8d8:	csinv	x0, x1, xzr, ge  // ge = tcont
   1e8dc:	ret
   1e8e0:	add	x12, x8, x10, lsl #3
   1e8e4:	sub	x10, x8, #0x8
   1e8e8:	lsl	x13, x13, #3
   1e8ec:	cbz	x13, 1e900 <__gmpz_scan0@@Base+0x9c>
   1e8f0:	ldr	x14, [x10, x13]
   1e8f4:	sub	x13, x13, #0x8
   1e8f8:	cbz	x14, 1e8ec <__gmpz_scan0@@Base+0x88>
   1e8fc:	b	1e904 <__gmpz_scan0@@Base+0xa0>
   1e900:	sub	x11, x11, #0x1
   1e904:	mov	x10, #0xffffffffffffffff    	// #-1
   1e908:	lsl	x10, x10, x1
   1e90c:	ands	x10, x11, x10
   1e910:	b.ne	1e928 <__gmpz_scan0@@Base+0xc4>  // b.any
   1e914:	add	x13, x9, #0x8
   1e918:	cmp	x13, x12
   1e91c:	b.eq	1e94c <__gmpz_scan0@@Base+0xe8>  // b.none
   1e920:	ldr	x10, [x9, #8]!
   1e924:	cbz	x10, 1e920 <__gmpz_scan0@@Base+0xbc>
   1e928:	mov	w11, #0x1                   	// #1
   1e92c:	cbz	w11, 1e95c <__gmpz_scan0@@Base+0xf8>
   1e930:	rbit	x10, x10
   1e934:	clz	x10, x10
   1e938:	sub	x8, x9, x8
   1e93c:	add	x0, x10, x8, lsl #3
   1e940:	ret
   1e944:	lsl	x0, x10, #6
   1e948:	ret
   1e94c:	mov	x10, xzr
   1e950:	mov	w11, wzr
   1e954:	mov	x9, x13
   1e958:	cbnz	w11, 1e930 <__gmpz_scan0@@Base+0xcc>
   1e95c:	mov	x0, #0xffffffffffffffff    	// #-1
   1e960:	ret

000000000001e964 <__gmpz_scan1@@Base>:
   1e964:	stp	x29, x30, [sp, #-64]!
   1e968:	stp	x24, x23, [sp, #16]
   1e96c:	stp	x22, x21, [sp, #32]
   1e970:	stp	x20, x19, [sp, #48]
   1e974:	ldrsw	x8, [x0, #4]
   1e978:	lsr	x21, x1, #6
   1e97c:	mov	x20, x1
   1e980:	mov	x29, sp
   1e984:	cmp	x8, #0x0
   1e988:	cneg	x23, x8, mi  // mi = first
   1e98c:	cmp	x21, x23
   1e990:	b.ge	1e9d0 <__gmpz_scan1@@Base+0x6c>  // b.tcont
   1e994:	ldr	x19, [x0, #8]
   1e998:	add	x22, x19, x21, lsl #3
   1e99c:	cbz	x20, 1ea40 <__gmpz_scan1@@Base+0xdc>
   1e9a0:	ldr	x24, [x22]
   1e9a4:	tbnz	w8, #31, 1e9dc <__gmpz_scan1@@Base+0x78>
   1e9a8:	mov	x8, #0xffffffffffffffff    	// #-1
   1e9ac:	lsl	x8, x8, x20
   1e9b0:	ands	x8, x24, x8
   1e9b4:	b.ne	1ea48 <__gmpz_scan1@@Base+0xe4>  // b.any
   1e9b8:	add	x8, x19, x23, lsl #3
   1e9bc:	sub	x8, x8, #0x8
   1e9c0:	cmp	x22, x8
   1e9c4:	b.ne	1ea3c <__gmpz_scan1@@Base+0xd8>  // b.any
   1e9c8:	mov	x0, #0xffffffffffffffff    	// #-1
   1e9cc:	b	1ea58 <__gmpz_scan1@@Base+0xf4>
   1e9d0:	cmp	w8, #0x0
   1e9d4:	csinv	x0, x20, xzr, lt  // lt = tstop
   1e9d8:	b	1ea58 <__gmpz_scan1@@Base+0xf4>
   1e9dc:	cbz	x21, 1e9f0 <__gmpz_scan1@@Base+0x8c>
   1e9e0:	mov	x0, x19
   1e9e4:	mov	x1, x21
   1e9e8:	bl	c010 <__gmpn_zero_p@plt>
   1e9ec:	cbz	w0, 1e9f8 <__gmpz_scan1@@Base+0x94>
   1e9f0:	cbz	x24, 1ea3c <__gmpz_scan1@@Base+0xd8>
   1e9f4:	sub	x24, x24, #0x1
   1e9f8:	mov	x8, #0xffffffffffffffff    	// #-1
   1e9fc:	lsl	x8, x8, x20
   1ea00:	orn	x8, x24, x8
   1ea04:	cmn	x8, #0x1
   1ea08:	b.ne	1ea2c <__gmpz_scan1@@Base+0xc8>  // b.any
   1ea0c:	lsl	x8, x23, #3
   1ea10:	sub	x8, x8, x21, lsl #3
   1ea14:	sub	x9, x8, #0x8
   1ea18:	cbz	x9, 1ea34 <__gmpz_scan1@@Base+0xd0>
   1ea1c:	ldr	x8, [x22, #8]!
   1ea20:	sub	x9, x9, #0x8
   1ea24:	cmn	x8, #0x1
   1ea28:	b.eq	1ea18 <__gmpz_scan1@@Base+0xb4>  // b.none
   1ea2c:	mvn	x8, x8
   1ea30:	b	1ea48 <__gmpz_scan1@@Base+0xe4>
   1ea34:	lsl	x0, x23, #6
   1ea38:	b	1ea58 <__gmpz_scan1@@Base+0xf4>
   1ea3c:	add	x22, x22, #0x8
   1ea40:	ldr	x8, [x22]
   1ea44:	cbz	x8, 1ea3c <__gmpz_scan1@@Base+0xd8>
   1ea48:	rbit	x8, x8
   1ea4c:	clz	x8, x8
   1ea50:	sub	x9, x22, x19
   1ea54:	add	x0, x8, x9, lsl #3
   1ea58:	ldp	x20, x19, [sp, #48]
   1ea5c:	ldp	x22, x21, [sp, #32]
   1ea60:	ldp	x24, x23, [sp, #16]
   1ea64:	ldp	x29, x30, [sp], #64
   1ea68:	ret

000000000001ea6c <__gmpz_set@@Base>:
   1ea6c:	stp	x29, x30, [sp, #-48]!
   1ea70:	stp	x22, x21, [sp, #16]
   1ea74:	stp	x20, x19, [sp, #32]
   1ea78:	ldrsw	x22, [x1, #4]
   1ea7c:	ldrsw	x8, [x0]
   1ea80:	mov	x20, x1
   1ea84:	mov	x19, x0
   1ea88:	cmp	x22, #0x0
   1ea8c:	cneg	x21, x22, mi  // mi = first
   1ea90:	cmp	x21, x8
   1ea94:	mov	x29, sp
   1ea98:	b.gt	1eac0 <__gmpz_set@@Base+0x54>
   1ea9c:	ldr	x0, [x19, #8]
   1eaa0:	ldr	x1, [x20, #8]
   1eaa4:	mov	x2, x21
   1eaa8:	bl	cc10 <__gmpn_copyi@plt>
   1eaac:	str	w22, [x19, #4]
   1eab0:	ldp	x20, x19, [sp, #32]
   1eab4:	ldp	x22, x21, [sp, #16]
   1eab8:	ldp	x29, x30, [sp], #48
   1eabc:	ret
   1eac0:	mov	x0, x19
   1eac4:	mov	x1, x21
   1eac8:	bl	c1c0 <__gmpz_realloc@plt>
   1eacc:	b	1eaa0 <__gmpz_set@@Base+0x34>

000000000001ead0 <__gmpz_set_d@@Base>:
   1ead0:	sub	sp, sp, #0x50
   1ead4:	str	d8, [sp, #16]
   1ead8:	mov	v8.16b, v0.16b
   1eadc:	fmov	x8, d8
   1eae0:	mvn	x8, x8
   1eae4:	tst	x8, #0x7ff0000000000000
   1eae8:	stp	x29, x30, [sp, #32]
   1eaec:	stp	x22, x21, [sp, #48]
   1eaf0:	stp	x20, x19, [sp, #64]
   1eaf4:	add	x29, sp, #0x10
   1eaf8:	b.eq	1ebb4 <__gmpz_set_d@@Base+0xe4>  // b.none
   1eafc:	fneg	d0, d8
   1eb00:	fcmp	d8, #0.0
   1eb04:	mov	x19, x0
   1eb08:	fcsel	d0, d8, d0, ge  // ge = tcont
   1eb0c:	mov	x0, sp
   1eb10:	bl	d460 <__gmp_extract_double@plt>
   1eb14:	ldrsw	x8, [x19]
   1eb18:	sxtw	x9, w0
   1eb1c:	bic	x20, x9, x9, asr #63
   1eb20:	cmp	x20, x8
   1eb24:	b.gt	1eb9c <__gmpz_set_d@@Base+0xcc>
   1eb28:	ldr	x21, [x19, #8]
   1eb2c:	cbz	x20, 1eb74 <__gmpz_set_d@@Base+0xa4>
   1eb30:	cmp	x20, #0x1
   1eb34:	b.eq	1eb6c <__gmpz_set_d@@Base+0x9c>  // b.none
   1eb38:	subs	x22, x20, #0x2
   1eb3c:	b.eq	1eb5c <__gmpz_set_d@@Base+0x8c>  // b.none
   1eb40:	b.eq	1eb58 <__gmpz_set_d@@Base+0x88>  // b.none
   1eb44:	lsl	x8, x20, #3
   1eb48:	sub	x2, x8, #0x10
   1eb4c:	mov	x0, x21
   1eb50:	mov	w1, wzr
   1eb54:	bl	c780 <memset@plt>
   1eb58:	add	x21, x21, x22, lsl #3
   1eb5c:	ldr	x8, [sp, #8]
   1eb60:	str	x8, [x21, #8]
   1eb64:	ldr	x8, [sp]
   1eb68:	b	1eb70 <__gmpz_set_d@@Base+0xa0>
   1eb6c:	ldr	x8, [sp, #8]
   1eb70:	str	x8, [x21]
   1eb74:	neg	w8, w20
   1eb78:	fcmp	d8, #0.0
   1eb7c:	csel	x8, x8, x20, mi  // mi = first
   1eb80:	str	w8, [x19, #4]
   1eb84:	ldp	x20, x19, [sp, #64]
   1eb88:	ldp	x22, x21, [sp, #48]
   1eb8c:	ldp	x29, x30, [sp, #32]
   1eb90:	ldr	d8, [sp, #16]
   1eb94:	add	sp, sp, #0x50
   1eb98:	ret
   1eb9c:	mov	x0, x19
   1eba0:	mov	x1, x20
   1eba4:	bl	c1c0 <__gmpz_realloc@plt>
   1eba8:	mov	x21, x0
   1ebac:	cbnz	x20, 1eb30 <__gmpz_set_d@@Base+0x60>
   1ebb0:	b	1eb74 <__gmpz_set_d@@Base+0xa4>
   1ebb4:	bl	c300 <__gmp_invalid_operation@plt>

000000000001ebb8 <__gmpz_set_f@@Base>:
   1ebb8:	stp	x29, x30, [sp, #-64]!
   1ebbc:	stp	x24, x23, [sp, #16]
   1ebc0:	stp	x22, x21, [sp, #32]
   1ebc4:	stp	x20, x19, [sp, #48]
   1ebc8:	ldr	x19, [x1, #8]
   1ebcc:	mov	x22, x0
   1ebd0:	mov	x29, sp
   1ebd4:	cmp	x19, #0x0
   1ebd8:	b.le	1ec38 <__gmpz_set_f@@Base+0x80>
   1ebdc:	ldrsw	x8, [x22]
   1ebe0:	mov	x21, x1
   1ebe4:	cmp	x19, x8
   1ebe8:	b.gt	1ec6c <__gmpz_set_f@@Base+0xb4>
   1ebec:	ldr	x20, [x22, #8]
   1ebf0:	ldrsw	x8, [x21, #4]
   1ebf4:	ldr	x21, [x21, #16]
   1ebf8:	neg	w9, w19
   1ebfc:	cmp	w8, #0x0
   1ec00:	csel	x9, x19, x9, ge  // ge = tcont
   1ec04:	cmp	x8, #0x0
   1ec08:	cneg	x23, x8, mi  // mi = first
   1ec0c:	subs	x24, x19, x23
   1ec10:	str	w9, [x22, #4]
   1ec14:	b.le	1ec40 <__gmpz_set_f@@Base+0x88>
   1ec18:	b.eq	1ec2c <__gmpz_set_f@@Base+0x74>  // b.none
   1ec1c:	lsl	x2, x24, #3
   1ec20:	mov	x0, x20
   1ec24:	mov	w1, wzr
   1ec28:	bl	c780 <memset@plt>
   1ec2c:	add	x20, x20, x24, lsl #3
   1ec30:	mov	x19, x23
   1ec34:	b	1ec48 <__gmpz_set_f@@Base+0x90>
   1ec38:	str	wzr, [x22, #4]
   1ec3c:	b	1ec58 <__gmpz_set_f@@Base+0xa0>
   1ec40:	sub	x8, x23, x19
   1ec44:	add	x21, x21, x8, lsl #3
   1ec48:	mov	x0, x20
   1ec4c:	mov	x1, x21
   1ec50:	mov	x2, x19
   1ec54:	bl	cc10 <__gmpn_copyi@plt>
   1ec58:	ldp	x20, x19, [sp, #48]
   1ec5c:	ldp	x22, x21, [sp, #32]
   1ec60:	ldp	x24, x23, [sp, #16]
   1ec64:	ldp	x29, x30, [sp], #64
   1ec68:	ret
   1ec6c:	mov	x0, x22
   1ec70:	mov	x1, x19
   1ec74:	bl	c1c0 <__gmpz_realloc@plt>
   1ec78:	mov	x20, x0
   1ec7c:	b	1ebf0 <__gmpz_set_f@@Base+0x38>

000000000001ec80 <__gmpz_set_q@@Base>:
   1ec80:	stp	x29, x30, [sp, #-16]!
   1ec84:	add	x2, x1, #0x10
   1ec88:	mov	x29, sp
   1ec8c:	bl	c180 <__gmpz_tdiv_q@plt>
   1ec90:	ldp	x29, x30, [sp], #16
   1ec94:	ret

000000000001ec98 <__gmpz_set_si@@Base>:
   1ec98:	stp	x29, x30, [sp, #-48]!
   1ec9c:	stp	x20, x19, [sp, #32]
   1eca0:	ldr	w8, [x0]
   1eca4:	cmp	x1, #0x0
   1eca8:	str	x21, [sp, #16]
   1ecac:	mov	x19, x0
   1ecb0:	mov	x20, x1
   1ecb4:	cneg	x21, x1, mi  // mi = first
   1ecb8:	cmp	w8, #0x0
   1ecbc:	mov	x29, sp
   1ecc0:	b.le	1ecf0 <__gmpz_set_si@@Base+0x58>
   1ecc4:	ldr	x0, [x19, #8]
   1ecc8:	cmp	x20, #0x0
   1eccc:	cset	w8, ne  // ne = any
   1ecd0:	csetm	w9, ne  // ne = any
   1ecd4:	csel	w8, w8, w9, ge  // ge = tcont
   1ecd8:	str	x21, [x0]
   1ecdc:	str	w8, [x19, #4]
   1ece0:	ldp	x20, x19, [sp, #32]
   1ece4:	ldr	x21, [sp, #16]
   1ece8:	ldp	x29, x30, [sp], #48
   1ecec:	ret
   1ecf0:	mov	w1, #0x1                   	// #1
   1ecf4:	mov	x0, x19
   1ecf8:	bl	c1c0 <__gmpz_realloc@plt>
   1ecfc:	b	1ecc8 <__gmpz_set_si@@Base+0x30>

000000000001ed00 <__gmpz_set_str@@Base>:
   1ed00:	stp	x29, x30, [sp, #-80]!
   1ed04:	stp	x26, x25, [sp, #16]
   1ed08:	stp	x24, x23, [sp, #32]
   1ed0c:	stp	x22, x21, [sp, #48]
   1ed10:	stp	x20, x19, [sp, #64]
   1ed14:	mov	x29, sp
   1ed18:	sub	sp, sp, #0x20
   1ed1c:	adrp	x24, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1ed20:	ldr	x24, [x24, #3920]
   1ed24:	mov	w20, w2
   1ed28:	mov	x21, x1
   1ed2c:	mov	x19, x0
   1ed30:	cmp	w2, #0x25
   1ed34:	b.lt	1ed44 <__gmpz_set_str@@Base+0x44>  // b.tstop
   1ed38:	cmp	w20, #0x3e
   1ed3c:	b.gt	1eeb0 <__gmpz_set_str@@Base+0x1b0>
   1ed40:	add	x24, x24, #0xd0
   1ed44:	bl	cca0 <__ctype_b_loc@plt>
   1ed48:	ldr	x8, [x0]
   1ed4c:	mov	x22, x0
   1ed50:	ldrb	w26, [x21], #1
   1ed54:	ldrh	w9, [x8, x26, lsl #1]
   1ed58:	tbnz	w9, #13, 1ed50 <__gmpz_set_str@@Base+0x50>
   1ed5c:	cmp	w26, #0x2d
   1ed60:	b.ne	1ed70 <__gmpz_set_str@@Base+0x70>  // b.any
   1ed64:	ldrb	w26, [x21], #1
   1ed68:	mov	w25, #0x1                   	// #1
   1ed6c:	b	1ed74 <__gmpz_set_str@@Base+0x74>
   1ed70:	mov	w25, wzr
   1ed74:	ldrb	w9, [x24, w26, uxtw]
   1ed78:	cmp	w20, #0x0
   1ed7c:	mov	w10, #0xa                   	// #10
   1ed80:	csel	w10, w10, w20, eq  // eq = none
   1ed84:	cmp	w10, w9
   1ed88:	b.le	1eeb0 <__gmpz_set_str@@Base+0x1b0>
   1ed8c:	cbnz	w20, 1edec <__gmpz_set_str@@Base+0xec>
   1ed90:	cmp	w26, #0x30
   1ed94:	b.ne	1edbc <__gmpz_set_str@@Base+0xbc>  // b.any
   1ed98:	mov	x9, x21
   1ed9c:	ldrb	w26, [x9], #1
   1eda0:	orr	w10, w26, #0x20
   1eda4:	cmp	w10, #0x78
   1eda8:	b.ne	1edc4 <__gmpz_set_str@@Base+0xc4>  // b.any
   1edac:	ldrb	w26, [x21, #1]
   1edb0:	add	x21, x21, #0x2
   1edb4:	mov	w20, #0x10                  	// #16
   1edb8:	b	1edec <__gmpz_set_str@@Base+0xec>
   1edbc:	mov	w20, #0xa                   	// #10
   1edc0:	b	1edec <__gmpz_set_str@@Base+0xec>
   1edc4:	cmp	w10, #0x62
   1edc8:	b.ne	1eddc <__gmpz_set_str@@Base+0xdc>  // b.any
   1edcc:	ldrb	w26, [x21, #1]
   1edd0:	add	x21, x21, #0x2
   1edd4:	mov	w20, #0x2                   	// #2
   1edd8:	b	1edec <__gmpz_set_str@@Base+0xec>
   1eddc:	mov	w20, #0x8                   	// #8
   1ede0:	mov	x21, x9
   1ede4:	b	1edec <__gmpz_set_str@@Base+0xec>
   1ede8:	ldrb	w26, [x21], #1
   1edec:	cmp	w26, #0x30
   1edf0:	b.eq	1ede8 <__gmpz_set_str@@Base+0xe8>  // b.none
   1edf4:	ldrh	w9, [x8, w26, sxtw #1]
   1edf8:	tbnz	w9, #13, 1ede8 <__gmpz_set_str@@Base+0xe8>
   1edfc:	cbz	w26, 1eeb8 <__gmpz_set_str@@Base+0x1b8>
   1ee00:	sub	x0, x21, #0x1
   1ee04:	stur	xzr, [x29, #-8]
   1ee08:	bl	c090 <strlen@plt>
   1ee0c:	add	x1, x0, #0x1
   1ee10:	mov	w8, #0x7f00                	// #32512
   1ee14:	mov	x23, x0
   1ee18:	cmp	x1, x8
   1ee1c:	b.hi	1eec4 <__gmpz_set_str@@Base+0x1c4>  // b.pmore
   1ee20:	add	x9, x1, #0xf
   1ee24:	mov	x8, sp
   1ee28:	and	x9, x9, #0xfffffffffffffff0
   1ee2c:	sub	x1, x8, x9
   1ee30:	mov	sp, x1
   1ee34:	mov	x8, x1
   1ee38:	cbz	x23, 1eed8 <__gmpz_set_str@@Base+0x1d8>
   1ee3c:	mov	x9, xzr
   1ee40:	mov	x8, x1
   1ee44:	b	1ee58 <__gmpz_set_str@@Base+0x158>
   1ee48:	ldrb	w26, [x21, x9]
   1ee4c:	add	x9, x9, #0x1
   1ee50:	cmp	x23, x9
   1ee54:	b.eq	1eed8 <__gmpz_set_str@@Base+0x1d8>  // b.none
   1ee58:	ldr	x10, [x22]
   1ee5c:	ldrh	w10, [x10, w26, sxtw #1]
   1ee60:	tbnz	w10, #13, 1ee48 <__gmpz_set_str@@Base+0x148>
   1ee64:	sxtw	x10, w26
   1ee68:	ldrb	w10, [x24, x10]
   1ee6c:	cmp	w20, w10
   1ee70:	b.le	1ee84 <__gmpz_set_str@@Base+0x184>
   1ee74:	strb	w10, [x8], #1
   1ee78:	mov	w10, #0x1                   	// #1
   1ee7c:	cbnz	w10, 1ee48 <__gmpz_set_str@@Base+0x148>
   1ee80:	b	1eeb0 <__gmpz_set_str@@Base+0x1b0>
   1ee84:	ldur	x0, [x29, #-8]
   1ee88:	cbnz	x0, 1ee94 <__gmpz_set_str@@Base+0x194>
   1ee8c:	mov	w10, wzr
   1ee90:	b	1ee7c <__gmpz_set_str@@Base+0x17c>
   1ee94:	stp	x1, x9, [x29, #-24]
   1ee98:	mov	x26, x8
   1ee9c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1eea0:	ldp	x1, x9, [x29, #-24]
   1eea4:	mov	x8, x26
   1eea8:	mov	w10, wzr
   1eeac:	b	1ee7c <__gmpz_set_str@@Base+0x17c>
   1eeb0:	mov	w0, #0xffffffff            	// #-1
   1eeb4:	b	1ef34 <__gmpz_set_str@@Base+0x234>
   1eeb8:	mov	w0, wzr
   1eebc:	str	wzr, [x19, #4]
   1eec0:	b	1ef34 <__gmpz_set_str@@Base+0x234>
   1eec4:	sub	x0, x29, #0x8
   1eec8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1eecc:	mov	x1, x0
   1eed0:	mov	x8, x1
   1eed4:	cbnz	x23, 1ee3c <__gmpz_set_str@@Base+0x13c>
   1eed8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1eedc:	ldr	x9, [x9, #3936]
   1eee0:	mov	w10, #0x28                  	// #40
   1eee4:	sub	x21, x8, x1
   1eee8:	smaddl	x9, w20, w10, x9
   1eeec:	ldrsw	x10, [x19]
   1eef0:	ldr	x9, [x9, #16]
   1eef4:	umulh	x8, x9, x21
   1eef8:	ubfx	x8, x8, #3, #58
   1eefc:	add	x8, x8, #0x2
   1ef00:	cmp	x8, x10
   1ef04:	b.gt	1ef50 <__gmpz_set_str@@Base+0x250>
   1ef08:	ldr	x0, [x19, #8]
   1ef0c:	mov	x2, x21
   1ef10:	mov	w3, w20
   1ef14:	bl	c1d0 <__gmpn_set_str@plt>
   1ef18:	neg	w8, w0
   1ef1c:	cmp	w25, #0x0
   1ef20:	csel	x8, x0, x8, eq  // eq = none
   1ef24:	str	w8, [x19, #4]
   1ef28:	ldur	x8, [x29, #-8]
   1ef2c:	mov	w0, wzr
   1ef30:	cbnz	x8, 1ef68 <__gmpz_set_str@@Base+0x268>
   1ef34:	mov	sp, x29
   1ef38:	ldp	x20, x19, [sp, #64]
   1ef3c:	ldp	x22, x21, [sp, #48]
   1ef40:	ldp	x24, x23, [sp, #32]
   1ef44:	ldp	x26, x25, [sp, #16]
   1ef48:	ldp	x29, x30, [sp], #80
   1ef4c:	ret
   1ef50:	mov	x0, x19
   1ef54:	mov	x22, x1
   1ef58:	mov	x1, x8
   1ef5c:	bl	c1c0 <__gmpz_realloc@plt>
   1ef60:	mov	x1, x22
   1ef64:	b	1ef08 <__gmpz_set_str@@Base+0x208>
   1ef68:	mov	x0, x8
   1ef6c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1ef70:	mov	w0, wzr
   1ef74:	b	1ef34 <__gmpz_set_str@@Base+0x234>

000000000001ef78 <__gmpz_set_ui@@Base>:
   1ef78:	stp	x29, x30, [sp, #-32]!
   1ef7c:	stp	x20, x19, [sp, #16]
   1ef80:	ldr	w8, [x0]
   1ef84:	mov	x19, x0
   1ef88:	mov	x20, x1
   1ef8c:	mov	x29, sp
   1ef90:	cmp	w8, #0x0
   1ef94:	b.le	1efb8 <__gmpz_set_ui@@Base+0x40>
   1ef98:	ldr	x0, [x19, #8]
   1ef9c:	cmp	x20, #0x0
   1efa0:	cset	w8, ne  // ne = any
   1efa4:	str	x20, [x0]
   1efa8:	str	w8, [x19, #4]
   1efac:	ldp	x20, x19, [sp, #16]
   1efb0:	ldp	x29, x30, [sp], #32
   1efb4:	ret
   1efb8:	mov	w1, #0x1                   	// #1
   1efbc:	mov	x0, x19
   1efc0:	bl	c1c0 <__gmpz_realloc@plt>
   1efc4:	b	1ef9c <__gmpz_set_ui@@Base+0x24>

000000000001efc8 <__gmpz_setbit@@Base>:
   1efc8:	stp	x29, x30, [sp, #-64]!
   1efcc:	stp	x24, x23, [sp, #16]
   1efd0:	stp	x22, x21, [sp, #32]
   1efd4:	stp	x20, x19, [sp, #48]
   1efd8:	ldrsw	x23, [x0, #4]
   1efdc:	ldr	x20, [x0, #8]
   1efe0:	mov	w8, #0x1                   	// #1
   1efe4:	mov	x19, x0
   1efe8:	lsr	x22, x1, #6
   1efec:	lsl	x24, x8, x1
   1eff0:	mov	x29, sp
   1eff4:	tbnz	w23, #31, 1f014 <__gmpz_setbit@@Base+0x4c>
   1eff8:	cmp	x22, x23
   1effc:	b.ge	1f080 <__gmpz_setbit@@Base+0xb8>  // b.tcont
   1f000:	lsl	x8, x22, #3
   1f004:	ldr	x9, [x20, x8]
   1f008:	orr	x9, x9, x24
   1f00c:	str	x9, [x20, x8]
   1f010:	b	1f0b0 <__gmpz_setbit@@Base+0xe8>
   1f014:	neg	x8, x23
   1f018:	cmp	x22, x8
   1f01c:	b.ge	1f0b0 <__gmpz_setbit@@Base+0xe8>  // b.tcont
   1f020:	mov	x9, xzr
   1f024:	ldr	x10, [x20, x9, lsl #3]
   1f028:	add	x9, x9, #0x1
   1f02c:	cbz	x10, 1f024 <__gmpz_setbit@@Base+0x5c>
   1f030:	sub	x10, x9, #0x1
   1f034:	cmp	x22, x10
   1f038:	b.ls	1f0c4 <__gmpz_setbit@@Base+0xfc>  // b.plast
   1f03c:	lsl	x9, x22, #3
   1f040:	ldr	x10, [x20, x9]
   1f044:	bics	xzr, x10, x24
   1f048:	bic	x11, x10, x24
   1f04c:	cinc	x10, x22, eq  // eq = none
   1f050:	cmp	x10, x8
   1f054:	str	x11, [x20, x9]
   1f058:	b.ne	1f0b0 <__gmpz_setbit@@Base+0xe8>  // b.any
   1f05c:	sub	x8, x20, #0x8
   1f060:	subs	x9, x22, #0x1
   1f064:	b.lt	1f138 <__gmpz_setbit@@Base+0x170>  // b.tstop
   1f068:	ldr	x10, [x8, x22, lsl #3]
   1f06c:	mov	x22, x9
   1f070:	cbz	x10, 1f060 <__gmpz_setbit@@Base+0x98>
   1f074:	add	x8, x9, #0x1
   1f078:	neg	w8, w8
   1f07c:	b	1f11c <__gmpz_setbit@@Base+0x154>
   1f080:	ldrsw	x8, [x19]
   1f084:	add	x21, x22, #0x1
   1f088:	cmp	x22, x8
   1f08c:	b.ge	1f124 <__gmpz_setbit@@Base+0x15c>  // b.tcont
   1f090:	subs	x8, x22, x23
   1f094:	str	w21, [x19, #4]
   1f098:	b.eq	1f0ac <__gmpz_setbit@@Base+0xe4>  // b.none
   1f09c:	add	x0, x20, x23, lsl #3
   1f0a0:	lsl	x2, x8, #3
   1f0a4:	mov	w1, wzr
   1f0a8:	bl	c780 <memset@plt>
   1f0ac:	str	x24, [x20, x22, lsl #3]
   1f0b0:	ldp	x20, x19, [sp, #48]
   1f0b4:	ldp	x22, x21, [sp, #32]
   1f0b8:	ldp	x24, x23, [sp, #16]
   1f0bc:	ldp	x29, x30, [sp], #64
   1f0c0:	ret
   1f0c4:	ldr	x8, [x20, x22, lsl #3]
   1f0c8:	add	x10, x22, #0x1
   1f0cc:	cmp	x10, x9
   1f0d0:	b.ne	1f0e8 <__gmpz_setbit@@Base+0x120>  // b.any
   1f0d4:	sub	x8, x8, #0x1
   1f0d8:	bic	x8, x8, x24
   1f0dc:	add	x8, x8, #0x1
   1f0e0:	str	x8, [x20, x22, lsl #3]
   1f0e4:	b	1f0b0 <__gmpz_setbit@@Base+0xe8>
   1f0e8:	subs	x8, x8, x24
   1f0ec:	str	x8, [x20, x22, lsl #3]
   1f0f0:	b.cs	1f10c <__gmpz_setbit@@Base+0x144>  // b.hs, b.nlast
   1f0f4:	add	x8, x20, x22, lsl #3
   1f0f8:	add	x8, x8, #0x8
   1f0fc:	ldr	x9, [x8]
   1f100:	sub	x10, x9, #0x1
   1f104:	str	x10, [x8], #8
   1f108:	cbz	x9, 1f0fc <__gmpz_setbit@@Base+0x134>
   1f10c:	mvn	x8, x23
   1f110:	ldr	x8, [x20, x8, lsl #3]
   1f114:	cmp	x8, #0x0
   1f118:	cinc	w8, w23, eq  // eq = none
   1f11c:	str	w8, [x19, #4]
   1f120:	b	1f0b0 <__gmpz_setbit@@Base+0xe8>
   1f124:	mov	x0, x19
   1f128:	mov	x1, x21
   1f12c:	bl	c1c0 <__gmpz_realloc@plt>
   1f130:	mov	x20, x0
   1f134:	b	1f090 <__gmpz_setbit@@Base+0xc8>
   1f138:	mov	x8, xzr
   1f13c:	neg	w8, w8
   1f140:	b	1f11c <__gmpz_setbit@@Base+0x154>

000000000001f144 <__gmpz_size@@Base>:
   1f144:	ldr	w8, [x0, #4]
   1f148:	cmp	w8, #0x0
   1f14c:	cneg	w0, w8, mi  // mi = first
   1f150:	ret

000000000001f154 <__gmpz_sizeinbase@@Base>:
   1f154:	ldr	w8, [x0, #4]
   1f158:	cmp	w8, #0x0
   1f15c:	cneg	w8, w8, mi  // mi = first
   1f160:	cbz	w8, 1f1b4 <__gmpz_sizeinbase@@Base+0x60>
   1f164:	ldr	x9, [x0, #8]
   1f168:	sub	w10, w8, #0x1
   1f16c:	adrp	x11, 69000 <__gmp_limbroots_table@@Base+0x11338>
   1f170:	mov	w8, w8
   1f174:	ldr	x9, [x9, w10, sxtw #3]
   1f178:	ldr	x11, [x11, #3936]
   1f17c:	sub	w10, w1, #0x1
   1f180:	lsl	x8, x8, #6
   1f184:	clz	x9, x9
   1f188:	tst	w1, w10
   1f18c:	sub	x8, x8, x9
   1f190:	sxtw	x9, w1
   1f194:	mov	w10, #0x28                  	// #40
   1f198:	madd	x9, x9, x10, x11
   1f19c:	b.ne	1f1bc <__gmpz_sizeinbase@@Base+0x68>  // b.any
   1f1a0:	ldrsw	x9, [x9, #24]
   1f1a4:	add	x8, x8, x9
   1f1a8:	sub	x8, x8, #0x1
   1f1ac:	udiv	x0, x8, x9
   1f1b0:	ret
   1f1b4:	mov	w0, #0x1                   	// #1
   1f1b8:	ret
   1f1bc:	ldr	x9, [x9, #8]
   1f1c0:	add	x9, x9, #0x1
   1f1c4:	umulh	x8, x9, x8
   1f1c8:	add	x0, x8, #0x1
   1f1cc:	ret

000000000001f1d0 <__gmpz_sqrt@@Base>:
   1f1d0:	stp	x29, x30, [sp, #-48]!
   1f1d4:	stp	x22, x21, [sp, #16]
   1f1d8:	stp	x20, x19, [sp, #32]
   1f1dc:	mov	x29, sp
   1f1e0:	sub	sp, sp, #0x10
   1f1e4:	ldrsw	x19, [x1, #4]
   1f1e8:	cmp	w19, #0x0
   1f1ec:	b.le	1f2a0 <__gmpz_sqrt@@Base+0xd0>
   1f1f0:	add	x8, x19, #0x1
   1f1f4:	add	x9, x19, #0x2
   1f1f8:	cmp	x8, #0x0
   1f1fc:	csinc	x8, x9, x19, lt  // lt = tstop
   1f200:	asr	x21, x8, #1
   1f204:	str	w21, [x0, #4]
   1f208:	ldr	x20, [x1, #8]
   1f20c:	cmp	x0, x1
   1f210:	b.eq	1f238 <__gmpz_sqrt@@Base+0x68>  // b.none
   1f214:	ldrsw	x8, [x0]
   1f218:	cmp	x21, x8
   1f21c:	b.gt	1f2ac <__gmpz_sqrt@@Base+0xdc>
   1f220:	ldr	x0, [x0, #8]
   1f224:	mov	x1, xzr
   1f228:	mov	x2, x20
   1f22c:	mov	x3, x19
   1f230:	bl	d590 <__gmpn_sqrtrem@plt>
   1f234:	b	1f28c <__gmpz_sqrt@@Base+0xbc>
   1f238:	lsl	x1, x21, #3
   1f23c:	mov	w8, #0x7f00                	// #32512
   1f240:	cmp	x1, x8
   1f244:	stur	xzr, [x29, #-8]
   1f248:	b.hi	1f2b8 <__gmpz_sqrt@@Base+0xe8>  // b.pmore
   1f24c:	add	x9, x1, #0xf
   1f250:	mov	x8, sp
   1f254:	and	x9, x9, #0xfffffffffffffff0
   1f258:	sub	x22, x8, x9
   1f25c:	mov	sp, x22
   1f260:	mov	x0, x22
   1f264:	mov	x1, xzr
   1f268:	mov	x2, x20
   1f26c:	mov	x3, x19
   1f270:	bl	d590 <__gmpn_sqrtrem@plt>
   1f274:	mov	x0, x20
   1f278:	mov	x1, x22
   1f27c:	mov	x2, x21
   1f280:	bl	cc10 <__gmpn_copyi@plt>
   1f284:	ldur	x0, [x29, #-8]
   1f288:	cbnz	x0, 1f2c8 <__gmpz_sqrt@@Base+0xf8>
   1f28c:	mov	sp, x29
   1f290:	ldp	x20, x19, [sp, #32]
   1f294:	ldp	x22, x21, [sp, #16]
   1f298:	ldp	x29, x30, [sp], #48
   1f29c:	ret
   1f2a0:	cbnz	w19, 1f2d0 <__gmpz_sqrt@@Base+0x100>
   1f2a4:	str	wzr, [x0, #4]
   1f2a8:	b	1f28c <__gmpz_sqrt@@Base+0xbc>
   1f2ac:	mov	x1, x21
   1f2b0:	bl	c1c0 <__gmpz_realloc@plt>
   1f2b4:	b	1f224 <__gmpz_sqrt@@Base+0x54>
   1f2b8:	sub	x0, x29, #0x8
   1f2bc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1f2c0:	mov	x22, x0
   1f2c4:	b	1f260 <__gmpz_sqrt@@Base+0x90>
   1f2c8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1f2cc:	b	1f28c <__gmpz_sqrt@@Base+0xbc>
   1f2d0:	bl	d1c0 <__gmp_sqrt_of_negative@plt>

000000000001f2d4 <__gmpz_sqrtrem@@Base>:
   1f2d4:	stp	x29, x30, [sp, #-80]!
   1f2d8:	stp	x24, x23, [sp, #32]
   1f2dc:	stp	x22, x21, [sp, #48]
   1f2e0:	stp	x20, x19, [sp, #64]
   1f2e4:	ldrsw	x20, [x2, #4]
   1f2e8:	mov	x19, x1
   1f2ec:	mov	x21, x0
   1f2f0:	str	x25, [sp, #16]
   1f2f4:	cmp	w20, #0x0
   1f2f8:	mov	x29, sp
   1f2fc:	b.le	1f3e0 <__gmpz_sqrtrem@@Base+0x10c>
   1f300:	ldr	w8, [x19]
   1f304:	mov	x25, x2
   1f308:	cmp	w20, w8
   1f30c:	b.gt	1f3ec <__gmpz_sqrtrem@@Base+0x118>
   1f310:	ldr	x22, [x19, #8]
   1f314:	add	x8, x20, #0x1
   1f318:	add	x9, x20, #0x2
   1f31c:	cmp	x8, #0x0
   1f320:	csinc	x8, x9, x20, lt  // lt = tstop
   1f324:	asr	x24, x8, #1
   1f328:	str	w24, [x21, #4]
   1f32c:	ldr	x23, [x25, #8]
   1f330:	cmp	x21, x25
   1f334:	b.eq	1f360 <__gmpz_sqrtrem@@Base+0x8c>  // b.none
   1f338:	ldrsw	x8, [x21]
   1f33c:	cmp	x24, x8
   1f340:	b.gt	1f400 <__gmpz_sqrtrem@@Base+0x12c>
   1f344:	ldr	x0, [x21, #8]
   1f348:	mov	x1, x22
   1f34c:	mov	x2, x23
   1f350:	mov	x3, x20
   1f354:	bl	d590 <__gmpn_sqrtrem@plt>
   1f358:	mov	x20, x0
   1f35c:	b	1f3c0 <__gmpz_sqrtrem@@Base+0xec>
   1f360:	lsl	x1, x24, #3
   1f364:	mov	w8, #0x7f00                	// #32512
   1f368:	cmp	x1, x8
   1f36c:	str	xzr, [x29, #24]
   1f370:	b.hi	1f410 <__gmpz_sqrtrem@@Base+0x13c>  // b.pmore
   1f374:	add	x9, x1, #0xf
   1f378:	mov	x8, sp
   1f37c:	and	x9, x9, #0xfffffffffffffff0
   1f380:	sub	x25, x8, x9
   1f384:	mov	sp, x25
   1f388:	mov	x0, x25
   1f38c:	mov	x1, x22
   1f390:	mov	x2, x23
   1f394:	mov	x3, x20
   1f398:	bl	d590 <__gmpn_sqrtrem@plt>
   1f39c:	cmp	x19, x21
   1f3a0:	mov	x20, x0
   1f3a4:	b.eq	1f3b8 <__gmpz_sqrtrem@@Base+0xe4>  // b.none
   1f3a8:	mov	x0, x23
   1f3ac:	mov	x1, x25
   1f3b0:	mov	x2, x24
   1f3b4:	bl	cc10 <__gmpn_copyi@plt>
   1f3b8:	ldr	x0, [x29, #24]
   1f3bc:	cbnz	x0, 1f420 <__gmpz_sqrtrem@@Base+0x14c>
   1f3c0:	str	w20, [x19, #4]
   1f3c4:	mov	sp, x29
   1f3c8:	ldp	x20, x19, [sp, #64]
   1f3cc:	ldp	x22, x21, [sp, #48]
   1f3d0:	ldp	x24, x23, [sp, #32]
   1f3d4:	ldr	x25, [sp, #16]
   1f3d8:	ldp	x29, x30, [sp], #80
   1f3dc:	ret
   1f3e0:	cbnz	w20, 1f428 <__gmpz_sqrtrem@@Base+0x154>
   1f3e4:	str	wzr, [x21, #4]
   1f3e8:	b	1f3c0 <__gmpz_sqrtrem@@Base+0xec>
   1f3ec:	mov	x0, x19
   1f3f0:	mov	x1, x20
   1f3f4:	bl	c1c0 <__gmpz_realloc@plt>
   1f3f8:	mov	x22, x0
   1f3fc:	b	1f314 <__gmpz_sqrtrem@@Base+0x40>
   1f400:	mov	x0, x21
   1f404:	mov	x1, x24
   1f408:	bl	c1c0 <__gmpz_realloc@plt>
   1f40c:	b	1f348 <__gmpz_sqrtrem@@Base+0x74>
   1f410:	add	x0, x29, #0x18
   1f414:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1f418:	mov	x25, x0
   1f41c:	b	1f388 <__gmpz_sqrtrem@@Base+0xb4>
   1f420:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1f424:	b	1f3c0 <__gmpz_sqrtrem@@Base+0xec>
   1f428:	bl	d1c0 <__gmp_sqrt_of_negative@plt>

000000000001f42c <__gmpz_stronglucas@@Base>:
   1f42c:	sub	sp, sp, #0x70
   1f430:	stp	x29, x30, [sp, #48]
   1f434:	stp	x24, x23, [sp, #64]
   1f438:	stp	x22, x21, [sp, #80]
   1f43c:	stp	x20, x19, [sp, #96]
   1f440:	ldr	w8, [x0, #4]
   1f444:	mov	x19, x1
   1f448:	ldr	x1, [x0, #8]
   1f44c:	add	x29, sp, #0x30
   1f450:	cmp	w8, #0x0
   1f454:	mov	x20, x2
   1f458:	cneg	w2, w8, mi  // mi = first
   1f45c:	sub	x0, x29, #0x10
   1f460:	bl	ce50 <__gmpz_roinit_n@plt>
   1f464:	ldur	x22, [x29, #-8]
   1f468:	ldursw	x23, [x29, #-12]
   1f46c:	mov	x0, x22
   1f470:	mov	x1, x23
   1f474:	bl	d130 <__gmpn_mod_34lsub1@plt>
   1f478:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   1f47c:	movk	x8, #0xcccd
   1f480:	umulh	x8, x0, x8
   1f484:	ubfx	x8, x8, #2, #30
   1f488:	mov	x21, x0
   1f48c:	add	w8, w8, w8, lsl #2
   1f490:	sub	w8, w21, w8
   1f494:	tbnz	w8, #1, 1f540 <__gmpz_stronglucas@@Base+0x114>
   1f498:	mov	x8, #0x2493                	// #9363
   1f49c:	movk	x8, #0x9249, lsl #16
   1f4a0:	movk	x8, #0x4924, lsl #32
   1f4a4:	movk	x8, #0x2492, lsl #48
   1f4a8:	umulh	x8, x21, x8
   1f4ac:	sub	x9, x21, x8
   1f4b0:	add	x8, x8, x9, lsr #1
   1f4b4:	lsr	x8, x8, #2
   1f4b8:	sub	x8, x8, x8, lsl #3
   1f4bc:	add	x8, x21, x8
   1f4c0:	sub	x9, x8, #0x1
   1f4c4:	tst	x8, x9
   1f4c8:	b.ne	1f554 <__gmpz_stronglucas@@Base+0x128>  // b.any
   1f4cc:	sub	x0, x29, #0x10
   1f4d0:	mov	w1, #0xb                   	// #11
   1f4d4:	mov	w24, #0xb                   	// #11
   1f4d8:	bl	ccf0 <__gmpz_kronecker_ui@plt>
   1f4dc:	cmn	w0, #0x1
   1f4e0:	b.eq	1f558 <__gmpz_stronglucas@@Base+0x12c>  // b.none
   1f4e4:	mov	x8, #0x4ec5                	// #20165
   1f4e8:	movk	x8, #0xc4ec, lsl #16
   1f4ec:	movk	x8, #0xec4e, lsl #32
   1f4f0:	movk	x8, #0x4ec4, lsl #48
   1f4f4:	umulh	x8, x21, x8
   1f4f8:	lsr	x8, x8, #2
   1f4fc:	mov	w24, #0xd                   	// #13
   1f500:	msub	w8, w8, w24, w21
   1f504:	sub	w8, w8, w8, lsr #3
   1f508:	and	x8, x8, #0x7
   1f50c:	cmp	x8, #0x4
   1f510:	b.hi	1f558 <__gmpz_stronglucas@@Base+0x12c>  // b.pmore
   1f514:	cmp	x8, #0x2
   1f518:	b.eq	1f558 <__gmpz_stronglucas@@Base+0x12c>  // b.none
   1f51c:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1f520:	movk	x8, #0xaaab
   1f524:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1f528:	madd	x8, x21, x8, x9
   1f52c:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   1f530:	cmp	x8, x9
   1f534:	b.cs	1f64c <__gmpz_stronglucas@@Base+0x220>  // b.hs, b.nlast
   1f538:	mov	w24, #0xf                   	// #15
   1f53c:	b	1f558 <__gmpz_stronglucas@@Base+0x12c>
   1f540:	ldr	x2, [x19, #8]
   1f544:	mov	x0, x22
   1f548:	mov	x1, x23
   1f54c:	bl	d2f0 <__gmpn_strongfibo@plt>
   1f550:	b	1f634 <__gmpz_stronglucas@@Base+0x208>
   1f554:	mov	w24, #0x7                   	// #7
   1f558:	lsr	x8, x24, #2
   1f55c:	neg	x9, x8
   1f560:	tst	x24, #0x2
   1f564:	sub	x0, x29, #0x10
   1f568:	mov	x1, xzr
   1f56c:	csinc	x22, x9, x8, eq  // eq = none
   1f570:	bl	cb60 <__gmpz_scan0@plt>
   1f574:	mov	x21, x0
   1f578:	add	x0, sp, #0x10
   1f57c:	bl	d430 <__gmpz_init@plt>
   1f580:	mov	x0, sp
   1f584:	bl	d430 <__gmpz_init@plt>
   1f588:	sub	x4, x29, #0x10
   1f58c:	add	x5, sp, #0x10
   1f590:	mov	x6, sp
   1f594:	mov	x0, x19
   1f598:	mov	x1, x20
   1f59c:	mov	x2, x22
   1f5a0:	mov	x3, x21
   1f5a4:	bl	c690 <__gmpz_lucas_mod@plt>
   1f5a8:	cbnz	w0, 1f61c <__gmpz_stronglucas@@Base+0x1f0>
   1f5ac:	subs	x21, x21, #0x1
   1f5b0:	b.eq	1f61c <__gmpz_stronglucas@@Base+0x1f0>  // b.none
   1f5b4:	mov	x0, sp
   1f5b8:	mov	x1, x19
   1f5bc:	mov	x2, x19
   1f5c0:	bl	c620 <__gmpz_mul@plt>
   1f5c4:	mov	x0, sp
   1f5c8:	mov	w2, #0x2                   	// #2
   1f5cc:	mov	x1, x20
   1f5d0:	bl	ca20 <__gmpz_submul_ui@plt>
   1f5d4:	mov	x1, sp
   1f5d8:	sub	x2, x29, #0x10
   1f5dc:	mov	x0, x19
   1f5e0:	bl	cc40 <__gmpz_tdiv_r@plt>
   1f5e4:	ldr	w8, [x19, #4]
   1f5e8:	cbz	w8, 1f618 <__gmpz_stronglucas@@Base+0x1ec>
   1f5ec:	subs	x21, x21, #0x1
   1f5f0:	b.eq	1f61c <__gmpz_stronglucas@@Base+0x1f0>  // b.none
   1f5f4:	mov	x0, sp
   1f5f8:	mov	x1, x20
   1f5fc:	mov	x2, x20
   1f600:	bl	c620 <__gmpz_mul@plt>
   1f604:	mov	x1, sp
   1f608:	sub	x2, x29, #0x10
   1f60c:	mov	x0, x20
   1f610:	bl	cc40 <__gmpz_tdiv_r@plt>
   1f614:	b	1f5b4 <__gmpz_stronglucas@@Base+0x188>
   1f618:	mov	w21, #0x1                   	// #1
   1f61c:	add	x0, sp, #0x10
   1f620:	bl	cd10 <__gmpz_clear@plt>
   1f624:	mov	x0, sp
   1f628:	bl	cd10 <__gmpz_clear@plt>
   1f62c:	cmp	x21, #0x0
   1f630:	cset	w0, ne  // ne = any
   1f634:	ldp	x20, x19, [sp, #96]
   1f638:	ldp	x22, x21, [sp, #80]
   1f63c:	ldp	x24, x23, [sp, #64]
   1f640:	ldp	x29, x30, [sp, #48]
   1f644:	add	sp, sp, #0x70
   1f648:	ret
   1f64c:	mov	x8, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   1f650:	movk	x8, #0xf0f1
   1f654:	umulh	x8, x21, x8
   1f658:	lsr	x8, x8, #4
   1f65c:	add	x8, x8, x8, lsl #4
   1f660:	sub	x8, x21, x8
   1f664:	sub	x9, x8, #0x1
   1f668:	tst	x8, x9
   1f66c:	b.eq	1f688 <__gmpz_stronglucas@@Base+0x25c>  // b.none
   1f670:	mov	w24, #0x11                  	// #17
   1f674:	mov	w9, #0x10                  	// #16
   1f678:	sub	x10, x24, x8
   1f67c:	sub	x8, x9, x8
   1f680:	tst	x10, x8
   1f684:	b.ne	1f558 <__gmpz_stronglucas@@Base+0x12c>  // b.any
   1f688:	sub	x0, x29, #0x10
   1f68c:	bl	c6b0 <__gmpz_perfect_square_p@plt>
   1f690:	cbnz	w0, 1f760 <__gmpz_stronglucas@@Base+0x334>
   1f694:	cmp	w23, #0x2
   1f698:	b.eq	1f6b4 <__gmpz_stronglucas@@Base+0x288>  // b.none
   1f69c:	cmp	w23, #0x1
   1f6a0:	b.ne	1f6cc <__gmpz_stronglucas@@Base+0x2a0>  // b.any
   1f6a4:	ldr	x0, [x22]
   1f6a8:	bl	1f76c <__gmpz_stronglucas@@Base+0x340>
   1f6ac:	str	x0, [sp, #16]
   1f6b0:	b	1f6d4 <__gmpz_stronglucas@@Base+0x2a8>
   1f6b4:	add	x0, sp, #0x10
   1f6b8:	mov	w3, #0x2                   	// #2
   1f6bc:	mov	x1, xzr
   1f6c0:	mov	x2, x22
   1f6c4:	bl	d590 <__gmpn_sqrtrem@plt>
   1f6c8:	b	1f6d4 <__gmpz_stronglucas@@Base+0x2a8>
   1f6cc:	mov	x8, #0xffffffffffffffff    	// #-1
   1f6d0:	str	x8, [sp, #16]
   1f6d4:	ldr	x24, [sp, #16]
   1f6d8:	ldur	x22, [x29, #-8]
   1f6dc:	ldursw	x23, [x29, #-12]
   1f6e0:	mov	w8, #0x13                  	// #19
   1f6e4:	sub	x9, x8, #0x2
   1f6e8:	cmp	x9, x24
   1f6ec:	b.cs	1f748 <__gmpz_stronglucas@@Base+0x31c>  // b.hs, b.nlast
   1f6f0:	mov	x21, x8
   1f6f4:	mov	x0, x22
   1f6f8:	mov	x1, x23
   1f6fc:	mov	x2, x8
   1f700:	cmp	w23, #0x28
   1f704:	b.lt	1f714 <__gmpz_stronglucas@@Base+0x2e8>  // b.tstop
   1f708:	bl	c540 <__gmpn_mod_1@plt>
   1f70c:	mov	w2, wzr
   1f710:	b	1f720 <__gmpz_stronglucas@@Base+0x2f4>
   1f714:	mov	x3, xzr
   1f718:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   1f71c:	mov	w2, w21
   1f720:	cbz	x0, 1f758 <__gmpz_stronglucas@@Base+0x32c>
   1f724:	mov	x1, x21
   1f728:	bl	c8c0 <__gmpn_jacobi_base@plt>
   1f72c:	cmp	w0, #0x1
   1f730:	add	x8, x21, #0x2
   1f734:	b.eq	1f6e4 <__gmpz_stronglucas@@Base+0x2b8>  // b.none
   1f738:	mov	w8, #0x1                   	// #1
   1f73c:	mov	x24, x21
   1f740:	cbnz	w8, 1f558 <__gmpz_stronglucas@@Base+0x12c>
   1f744:	b	1f634 <__gmpz_stronglucas@@Base+0x208>
   1f748:	mov	w8, wzr
   1f74c:	mov	w0, #0x1                   	// #1
   1f750:	mov	x21, x9
   1f754:	b	1f73c <__gmpz_stronglucas@@Base+0x310>
   1f758:	mov	w8, wzr
   1f75c:	b	1f73c <__gmpz_stronglucas@@Base+0x310>
   1f760:	mov	w8, wzr
   1f764:	mov	w0, wzr
   1f768:	b	1f73c <__gmpz_stronglucas@@Base+0x310>
   1f76c:	clz	x8, x0
   1f770:	mov	w9, #0x40                  	// #64
   1f774:	sub	w8, w9, w8
   1f778:	mov	w10, #0x1                   	// #1
   1f77c:	asr	w8, w8, #1
   1f780:	lsl	x9, x10, x8
   1f784:	lsr	x8, x0, x8
   1f788:	add	x8, x9, x8
   1f78c:	lsr	x0, x8, #1
   1f790:	ret

000000000001f794 <__gmpz_sub@@Base>:
   1f794:	stp	x29, x30, [sp, #-80]!
   1f798:	stp	x26, x25, [sp, #16]
   1f79c:	stp	x24, x23, [sp, #32]
   1f7a0:	stp	x22, x21, [sp, #48]
   1f7a4:	stp	x20, x19, [sp, #64]
   1f7a8:	ldrsw	x8, [x2, #4]
   1f7ac:	ldrsw	x9, [x1, #4]
   1f7b0:	ldrsw	x10, [x0]
   1f7b4:	mov	x19, x0
   1f7b8:	neg	x11, x8
   1f7bc:	cmp	x9, #0x0
   1f7c0:	cneg	x12, x9, mi  // mi = first
   1f7c4:	cmp	x11, #0x0
   1f7c8:	cneg	x11, x8, pl  // pl = nfrst
   1f7cc:	cmp	x12, x11
   1f7d0:	csel	x20, x11, x12, lt  // lt = tstop
   1f7d4:	csel	x22, x12, x11, lt  // lt = tstop
   1f7d8:	csneg	x26, x9, x8, lt  // lt = tstop
   1f7dc:	csneg	x25, x9, x8, ge  // ge = tcont
   1f7e0:	csel	x23, x1, x2, lt  // lt = tstop
   1f7e4:	csel	x24, x2, x1, lt  // lt = tstop
   1f7e8:	cmp	x20, x10
   1f7ec:	mov	x29, sp
   1f7f0:	b.ge	1f90c <__gmpz_sub@@Base+0x178>  // b.tcont
   1f7f4:	ldr	x21, [x19, #8]
   1f7f8:	ldr	x24, [x24, #8]
   1f7fc:	ldr	x23, [x23, #8]
   1f800:	eor	x8, x25, x26
   1f804:	tbnz	x8, #63, 1f834 <__gmpz_sub@@Base+0xa0>
   1f808:	mov	x0, x21
   1f80c:	mov	x1, x24
   1f810:	mov	x2, x20
   1f814:	mov	x3, x23
   1f818:	mov	x4, x22
   1f81c:	bl	c970 <__gmpn_add@plt>
   1f820:	add	x8, x0, x20
   1f824:	cmp	x25, #0x0
   1f828:	cneg	x8, x8, lt  // lt = tstop
   1f82c:	str	x0, [x21, x20, lsl #3]
   1f830:	b	1f8b8 <__gmpz_sub@@Base+0x124>
   1f834:	cmp	x20, x22
   1f838:	b.ne	1f880 <__gmpz_sub@@Base+0xec>  // b.any
   1f83c:	mov	x0, x24
   1f840:	mov	x1, x23
   1f844:	mov	x2, x20
   1f848:	bl	c570 <__gmpn_cmp@plt>
   1f84c:	tbnz	w0, #31, 1f8d4 <__gmpz_sub@@Base+0x140>
   1f850:	mov	x0, x21
   1f854:	mov	x1, x24
   1f858:	mov	x2, x23
   1f85c:	mov	x3, x20
   1f860:	bl	c420 <__gmpn_sub_n@plt>
   1f864:	sub	x8, x21, #0x8
   1f868:	mov	x9, x20
   1f86c:	subs	x20, x20, #0x1
   1f870:	b.lt	1f8b0 <__gmpz_sub@@Base+0x11c>  // b.tstop
   1f874:	ldr	x10, [x8, x9, lsl #3]
   1f878:	cbz	x10, 1f868 <__gmpz_sub@@Base+0xd4>
   1f87c:	b	1f8b0 <__gmpz_sub@@Base+0x11c>
   1f880:	mov	x0, x21
   1f884:	mov	x1, x24
   1f888:	mov	x2, x20
   1f88c:	mov	x3, x23
   1f890:	mov	x4, x22
   1f894:	bl	d340 <__gmpn_sub@plt>
   1f898:	sub	x8, x21, #0x8
   1f89c:	mov	x9, x20
   1f8a0:	subs	x20, x20, #0x1
   1f8a4:	b.lt	1f8b0 <__gmpz_sub@@Base+0x11c>  // b.tstop
   1f8a8:	ldr	x10, [x8, x9, lsl #3]
   1f8ac:	cbz	x10, 1f89c <__gmpz_sub@@Base+0x108>
   1f8b0:	cmp	x25, #0x0
   1f8b4:	cneg	x8, x9, lt  // lt = tstop
   1f8b8:	str	w8, [x19, #4]
   1f8bc:	ldp	x20, x19, [sp, #64]
   1f8c0:	ldp	x22, x21, [sp, #48]
   1f8c4:	ldp	x24, x23, [sp, #32]
   1f8c8:	ldp	x26, x25, [sp, #16]
   1f8cc:	ldp	x29, x30, [sp], #80
   1f8d0:	ret
   1f8d4:	mov	x0, x21
   1f8d8:	mov	x1, x23
   1f8dc:	mov	x2, x24
   1f8e0:	mov	x3, x20
   1f8e4:	bl	c420 <__gmpn_sub_n@plt>
   1f8e8:	sub	x8, x21, #0x8
   1f8ec:	mov	x9, x20
   1f8f0:	subs	x20, x20, #0x1
   1f8f4:	b.lt	1f900 <__gmpz_sub@@Base+0x16c>  // b.tstop
   1f8f8:	ldr	x10, [x8, x9, lsl #3]
   1f8fc:	cbz	x10, 1f8ec <__gmpz_sub@@Base+0x158>
   1f900:	cmp	x25, #0x0
   1f904:	cneg	x8, x9, ge  // ge = tcont
   1f908:	b	1f8b8 <__gmpz_sub@@Base+0x124>
   1f90c:	add	x1, x20, #0x1
   1f910:	mov	x0, x19
   1f914:	bl	c1c0 <__gmpz_realloc@plt>
   1f918:	mov	x21, x0
   1f91c:	b	1f7f8 <__gmpz_sub@@Base+0x64>

000000000001f920 <__gmpz_sub_ui@@Base>:
   1f920:	stp	x29, x30, [sp, #-64]!
   1f924:	stp	x24, x23, [sp, #16]
   1f928:	stp	x22, x21, [sp, #32]
   1f92c:	stp	x20, x19, [sp, #48]
   1f930:	ldrsw	x24, [x1, #4]
   1f934:	mov	x20, x2
   1f938:	mov	x19, x0
   1f93c:	mov	x29, sp
   1f940:	cbz	w24, 1f98c <__gmpz_sub_ui@@Base+0x6c>
   1f944:	ldrsw	x8, [x19]
   1f948:	cmp	x24, #0x0
   1f94c:	cneg	x21, x24, mi  // mi = first
   1f950:	mov	x23, x1
   1f954:	cmp	x21, x8
   1f958:	b.ge	1f9d4 <__gmpz_sub_ui@@Base+0xb4>  // b.tcont
   1f95c:	ldr	x22, [x19, #8]
   1f960:	ldr	x1, [x23, #8]
   1f964:	tbnz	w24, #31, 1f9ec <__gmpz_sub_ui@@Base+0xcc>
   1f968:	cmp	x21, #0x1
   1f96c:	b.ne	1f9ac <__gmpz_sub_ui@@Base+0x8c>  // b.any
   1f970:	ldr	x8, [x1]
   1f974:	cmp	x8, x20
   1f978:	b.cs	1f9ac <__gmpz_sub_ui@@Base+0x8c>  // b.hs, b.nlast
   1f97c:	sub	x8, x20, x8
   1f980:	str	x8, [x22]
   1f984:	mov	x8, #0xffffffffffffffff    	// #-1
   1f988:	b	1fa08 <__gmpz_sub_ui@@Base+0xe8>
   1f98c:	ldr	w8, [x19]
   1f990:	cmp	w8, #0x0
   1f994:	b.le	1fa20 <__gmpz_sub_ui@@Base+0x100>
   1f998:	ldr	x0, [x19, #8]
   1f99c:	cmp	x20, #0x0
   1f9a0:	str	x20, [x0]
   1f9a4:	csetm	w8, ne  // ne = any
   1f9a8:	b	1fa08 <__gmpz_sub_ui@@Base+0xe8>
   1f9ac:	mov	x0, x22
   1f9b0:	mov	x2, x21
   1f9b4:	mov	x3, x20
   1f9b8:	bl	caf0 <__gmpn_sub_1@plt>
   1f9bc:	add	x8, x22, x21, lsl #3
   1f9c0:	ldur	x8, [x8, #-8]
   1f9c4:	cmp	x8, #0x0
   1f9c8:	cset	w8, eq  // eq = none
   1f9cc:	sub	x8, x21, x8
   1f9d0:	b	1fa08 <__gmpz_sub_ui@@Base+0xe8>
   1f9d4:	add	x1, x21, #0x1
   1f9d8:	mov	x0, x19
   1f9dc:	bl	c1c0 <__gmpz_realloc@plt>
   1f9e0:	mov	x22, x0
   1f9e4:	ldr	x1, [x23, #8]
   1f9e8:	tbz	w24, #31, 1f968 <__gmpz_sub_ui@@Base+0x48>
   1f9ec:	mov	x0, x22
   1f9f0:	mov	x2, x21
   1f9f4:	mov	x3, x20
   1f9f8:	bl	c150 <__gmpn_add_1@plt>
   1f9fc:	add	x8, x21, x0
   1fa00:	str	x0, [x22, x21, lsl #3]
   1fa04:	neg	x8, x8
   1fa08:	str	w8, [x19, #4]
   1fa0c:	ldp	x20, x19, [sp, #48]
   1fa10:	ldp	x22, x21, [sp, #32]
   1fa14:	ldp	x24, x23, [sp, #16]
   1fa18:	ldp	x29, x30, [sp], #64
   1fa1c:	ret
   1fa20:	mov	w1, #0x1                   	// #1
   1fa24:	mov	x0, x19
   1fa28:	bl	c1c0 <__gmpz_realloc@plt>
   1fa2c:	b	1f99c <__gmpz_sub_ui@@Base+0x7c>

000000000001fa30 <__gmpz_swap@@Base>:
   1fa30:	ldr	w8, [x1]
   1fa34:	ldr	w9, [x0]
   1fa38:	str	w8, [x0]
   1fa3c:	str	w9, [x1]
   1fa40:	ldr	w8, [x1, #4]
   1fa44:	ldr	w9, [x0, #4]
   1fa48:	str	w8, [x0, #4]
   1fa4c:	str	w9, [x1, #4]
   1fa50:	ldr	x8, [x0, #8]
   1fa54:	ldr	x9, [x1, #8]
   1fa58:	str	x8, [x1, #8]
   1fa5c:	str	x9, [x0, #8]
   1fa60:	ret

000000000001fa64 <__gmpz_tdiv_ui@@Base>:
   1fa64:	stp	x29, x30, [sp, #-16]!
   1fa68:	mov	x29, sp
   1fa6c:	cbz	x1, 1faa0 <__gmpz_tdiv_ui@@Base+0x3c>
   1fa70:	ldrsw	x8, [x0, #4]
   1fa74:	cbz	w8, 1fa94 <__gmpz_tdiv_ui@@Base+0x30>
   1fa78:	ldr	x0, [x0, #8]
   1fa7c:	cmp	x8, #0x0
   1fa80:	mov	x2, x1
   1fa84:	cneg	x1, x8, mi  // mi = first
   1fa88:	bl	c540 <__gmpn_mod_1@plt>
   1fa8c:	ldp	x29, x30, [sp], #16
   1fa90:	ret
   1fa94:	mov	x0, xzr
   1fa98:	ldp	x29, x30, [sp], #16
   1fa9c:	ret
   1faa0:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001faa4 <__gmpz_tdiv_q@@Base>:
   1faa4:	stp	x29, x30, [sp, #-96]!
   1faa8:	stp	x28, x27, [sp, #16]
   1faac:	stp	x26, x25, [sp, #32]
   1fab0:	stp	x24, x23, [sp, #48]
   1fab4:	stp	x22, x21, [sp, #64]
   1fab8:	stp	x20, x19, [sp, #80]
   1fabc:	mov	x29, sp
   1fac0:	sub	sp, sp, #0x10
   1fac4:	ldrsw	x27, [x1, #4]
   1fac8:	ldrsw	x28, [x2, #4]
   1facc:	cmp	x27, #0x0
   1fad0:	cneg	x20, x27, mi  // mi = first
   1fad4:	cmp	x28, #0x0
   1fad8:	cneg	x21, x28, mi  // mi = first
   1fadc:	cbz	x21, 1fc38 <__gmpz_tdiv_q@@Base+0x194>
   1fae0:	mov	x19, x0
   1fae4:	sub	x22, x20, x21
   1fae8:	tbnz	x22, #63, 1fbe4 <__gmpz_tdiv_q@@Base+0x140>
   1faec:	ldrsw	x8, [x19]
   1faf0:	mov	x23, x1
   1faf4:	mov	x25, x2
   1faf8:	add	x1, x22, #0x1
   1fafc:	cmp	x22, x8
   1fb00:	stur	x1, [x29, #-16]
   1fb04:	b.ge	1fc08 <__gmpz_tdiv_q@@Base+0x164>  // b.tcont
   1fb08:	ldr	x24, [x19, #8]
   1fb0c:	stur	xzr, [x29, #-8]
   1fb10:	ldr	x25, [x25, #8]
   1fb14:	cmp	x25, x24
   1fb18:	b.ne	1fb54 <__gmpz_tdiv_q@@Base+0xb0>  // b.any
   1fb1c:	lsl	x1, x21, #3
   1fb20:	mov	w8, #0x7f00                	// #32512
   1fb24:	cmp	x1, x8
   1fb28:	b.hi	1fc28 <__gmpz_tdiv_q@@Base+0x184>  // b.pmore
   1fb2c:	add	x9, x1, #0xf
   1fb30:	mov	x8, sp
   1fb34:	and	x9, x9, #0xfffffffffffffff0
   1fb38:	sub	x26, x8, x9
   1fb3c:	mov	sp, x26
   1fb40:	mov	x0, x26
   1fb44:	mov	x1, x25
   1fb48:	mov	x2, x21
   1fb4c:	bl	cc10 <__gmpn_copyi@plt>
   1fb50:	mov	x25, x26
   1fb54:	lsl	x8, x20, #3
   1fb58:	mov	w9, #0x7ef8                	// #32504
   1fb5c:	cmp	x8, x9
   1fb60:	add	x1, x8, #0x8
   1fb64:	b.hi	1fc18 <__gmpz_tdiv_q@@Base+0x174>  // b.pmore
   1fb68:	add	x9, x1, #0xf
   1fb6c:	mov	x8, sp
   1fb70:	and	x9, x9, #0xfffffffffffffff0
   1fb74:	sub	x26, x8, x9
   1fb78:	mov	sp, x26
   1fb7c:	ldr	x1, [x23, #8]
   1fb80:	cmp	x1, x24
   1fb84:	b.ne	1fb98 <__gmpz_tdiv_q@@Base+0xf4>  // b.any
   1fb88:	mov	x0, x26
   1fb8c:	mov	x2, x20
   1fb90:	bl	cc10 <__gmpn_copyi@plt>
   1fb94:	mov	x1, x26
   1fb98:	mov	x0, x24
   1fb9c:	mov	x2, x20
   1fba0:	mov	x3, x25
   1fba4:	mov	x4, x21
   1fba8:	mov	x5, x26
   1fbac:	bl	c480 <__gmpn_div_q@plt>
   1fbb0:	ldr	x8, [x24, x22, lsl #3]
   1fbb4:	ldp	x10, x0, [x29, #-16]
   1fbb8:	eor	w9, w28, w27
   1fbbc:	cmp	x8, #0x0
   1fbc0:	cset	w8, eq  // eq = none
   1fbc4:	sub	x8, x10, x8
   1fbc8:	neg	w10, w8
   1fbcc:	cmp	w9, #0x0
   1fbd0:	csel	x8, x8, x10, ge  // ge = tcont
   1fbd4:	str	w8, [x19, #4]
   1fbd8:	cbz	x0, 1fbe8 <__gmpz_tdiv_q@@Base+0x144>
   1fbdc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1fbe0:	b	1fbe8 <__gmpz_tdiv_q@@Base+0x144>
   1fbe4:	str	wzr, [x19, #4]
   1fbe8:	mov	sp, x29
   1fbec:	ldp	x20, x19, [sp, #80]
   1fbf0:	ldp	x22, x21, [sp, #64]
   1fbf4:	ldp	x24, x23, [sp, #48]
   1fbf8:	ldp	x26, x25, [sp, #32]
   1fbfc:	ldp	x28, x27, [sp, #16]
   1fc00:	ldp	x29, x30, [sp], #96
   1fc04:	ret
   1fc08:	mov	x0, x19
   1fc0c:	bl	c1c0 <__gmpz_realloc@plt>
   1fc10:	mov	x24, x0
   1fc14:	b	1fb0c <__gmpz_tdiv_q@@Base+0x68>
   1fc18:	sub	x0, x29, #0x8
   1fc1c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1fc20:	mov	x26, x0
   1fc24:	b	1fb7c <__gmpz_tdiv_q@@Base+0xd8>
   1fc28:	sub	x0, x29, #0x8
   1fc2c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1fc30:	mov	x26, x0
   1fc34:	b	1fb40 <__gmpz_tdiv_q@@Base+0x9c>
   1fc38:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001fc3c <__gmpz_tdiv_q_2exp@@Base>:
   1fc3c:	stp	x29, x30, [sp, #-80]!
   1fc40:	stp	x24, x23, [sp, #32]
   1fc44:	stp	x22, x21, [sp, #48]
   1fc48:	stp	x20, x19, [sp, #64]
   1fc4c:	ldrsw	x24, [x1, #4]
   1fc50:	str	x25, [sp, #16]
   1fc54:	lsr	x25, x2, #6
   1fc58:	mov	x19, x0
   1fc5c:	cmp	x24, #0x0
   1fc60:	cneg	x8, x24, mi  // mi = first
   1fc64:	sub	x20, x8, x25
   1fc68:	cmp	x20, #0x1
   1fc6c:	mov	x29, sp
   1fc70:	b.lt	1fcc0 <__gmpz_tdiv_q_2exp@@Base+0x84>  // b.tstop
   1fc74:	ldrsw	x8, [x19]
   1fc78:	mov	x21, x2
   1fc7c:	mov	x22, x1
   1fc80:	cmp	x20, x8
   1fc84:	b.gt	1fcfc <__gmpz_tdiv_q_2exp@@Base+0xc0>
   1fc88:	ldr	x23, [x19, #8]
   1fc8c:	ldr	x8, [x22, #8]
   1fc90:	ands	x3, x21, #0x3f
   1fc94:	add	x1, x8, x25, lsl #3
   1fc98:	b.eq	1fcc8 <__gmpz_tdiv_q_2exp@@Base+0x8c>  // b.none
   1fc9c:	mov	x0, x23
   1fca0:	mov	x2, x20
   1fca4:	bl	c2f0 <__gmpn_rshift@plt>
   1fca8:	add	x8, x23, x20, lsl #3
   1fcac:	ldur	x8, [x8, #-8]
   1fcb0:	cmp	x8, #0x0
   1fcb4:	cset	w8, eq  // eq = none
   1fcb8:	sub	x20, x20, x8
   1fcbc:	b	1fcd4 <__gmpz_tdiv_q_2exp@@Base+0x98>
   1fcc0:	mov	x20, xzr
   1fcc4:	b	1fcd4 <__gmpz_tdiv_q_2exp@@Base+0x98>
   1fcc8:	mov	x0, x23
   1fccc:	mov	x2, x20
   1fcd0:	bl	cc10 <__gmpn_copyi@plt>
   1fcd4:	neg	w8, w20
   1fcd8:	cmp	w24, #0x0
   1fcdc:	csel	x8, x20, x8, ge  // ge = tcont
   1fce0:	str	w8, [x19, #4]
   1fce4:	ldp	x20, x19, [sp, #64]
   1fce8:	ldp	x22, x21, [sp, #48]
   1fcec:	ldp	x24, x23, [sp, #32]
   1fcf0:	ldr	x25, [sp, #16]
   1fcf4:	ldp	x29, x30, [sp], #80
   1fcf8:	ret
   1fcfc:	mov	x0, x19
   1fd00:	mov	x1, x20
   1fd04:	bl	c1c0 <__gmpz_realloc@plt>
   1fd08:	mov	x23, x0
   1fd0c:	b	1fc8c <__gmpz_tdiv_q_2exp@@Base+0x50>

000000000001fd10 <__gmpz_tdiv_q_ui@@Base>:
   1fd10:	stp	x29, x30, [sp, #-64]!
   1fd14:	stp	x24, x23, [sp, #16]
   1fd18:	stp	x22, x21, [sp, #32]
   1fd1c:	stp	x20, x19, [sp, #48]
   1fd20:	mov	x29, sp
   1fd24:	cbz	x2, 1fdc0 <__gmpz_tdiv_q_ui@@Base+0xb0>
   1fd28:	ldrsw	x24, [x1, #4]
   1fd2c:	mov	x22, x1
   1fd30:	mov	x19, x0
   1fd34:	cbz	w24, 1fd8c <__gmpz_tdiv_q_ui@@Base+0x7c>
   1fd38:	ldrsw	x8, [x19]
   1fd3c:	cmp	w24, #0x0
   1fd40:	cneg	x21, x24, lt  // lt = tstop
   1fd44:	mov	x20, x2
   1fd48:	cmp	x21, x8
   1fd4c:	b.gt	1fdac <__gmpz_tdiv_q_ui@@Base+0x9c>
   1fd50:	ldr	x23, [x19, #8]
   1fd54:	ldr	x2, [x22, #8]
   1fd58:	mov	x0, x23
   1fd5c:	mov	x1, xzr
   1fd60:	mov	x3, x21
   1fd64:	mov	x4, x20
   1fd68:	bl	ced0 <__gmpn_divrem_1@plt>
   1fd6c:	add	x8, x23, x21, lsl #3
   1fd70:	ldur	x8, [x8, #-8]
   1fd74:	cmp	x8, #0x0
   1fd78:	cset	w8, eq  // eq = none
   1fd7c:	sub	w8, w21, w8
   1fd80:	cmp	w24, #0x0
   1fd84:	cneg	w8, w8, lt  // lt = tstop
   1fd88:	b	1fd94 <__gmpz_tdiv_q_ui@@Base+0x84>
   1fd8c:	mov	w8, wzr
   1fd90:	mov	x0, xzr
   1fd94:	str	w8, [x19, #4]
   1fd98:	ldp	x20, x19, [sp, #48]
   1fd9c:	ldp	x22, x21, [sp, #32]
   1fda0:	ldp	x24, x23, [sp, #16]
   1fda4:	ldp	x29, x30, [sp], #64
   1fda8:	ret
   1fdac:	mov	x0, x19
   1fdb0:	mov	x1, x21
   1fdb4:	bl	c1c0 <__gmpz_realloc@plt>
   1fdb8:	mov	x23, x0
   1fdbc:	b	1fd54 <__gmpz_tdiv_q_ui@@Base+0x44>
   1fdc0:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001fdc4 <__gmpz_tdiv_qr@@Base>:
   1fdc4:	stp	x29, x30, [sp, #-96]!
   1fdc8:	stp	x28, x27, [sp, #16]
   1fdcc:	stp	x26, x25, [sp, #32]
   1fdd0:	stp	x24, x23, [sp, #48]
   1fdd4:	stp	x22, x21, [sp, #64]
   1fdd8:	stp	x20, x19, [sp, #80]
   1fddc:	mov	x29, sp
   1fde0:	sub	sp, sp, #0x20
   1fde4:	ldrsw	x24, [x2, #4]
   1fde8:	ldrsw	x28, [x3, #4]
   1fdec:	cmp	x24, #0x0
   1fdf0:	cneg	x22, x24, mi  // mi = first
   1fdf4:	cmp	x28, #0x0
   1fdf8:	cneg	x21, x28, mi  // mi = first
   1fdfc:	cbz	x21, 1fff8 <__gmpz_tdiv_qr@@Base+0x234>
   1fe00:	ldrsw	x8, [x1]
   1fe04:	mov	x26, x3
   1fe08:	mov	x27, x2
   1fe0c:	mov	x19, x1
   1fe10:	mov	x25, x0
   1fe14:	cmp	x21, x8
   1fe18:	sub	x20, x22, x21
   1fe1c:	b.gt	1ff6c <__gmpz_tdiv_qr@@Base+0x1a8>
   1fe20:	ldr	x23, [x19, #8]
   1fe24:	tbnz	x20, #63, 1ff80 <__gmpz_tdiv_qr@@Base+0x1bc>
   1fe28:	ldrsw	x8, [x25]
   1fe2c:	cmp	x20, x8
   1fe30:	add	x8, x20, #0x1
   1fe34:	stp	x25, x8, [x29, #-24]
   1fe38:	b.ge	1ffc4 <__gmpz_tdiv_qr@@Base+0x200>  // b.tcont
   1fe3c:	ldr	x25, [x25, #8]
   1fe40:	stur	xzr, [x29, #-8]
   1fe44:	ldr	x26, [x26, #8]
   1fe48:	ldr	x27, [x27, #8]
   1fe4c:	stur	x28, [x29, #-32]
   1fe50:	cmp	x26, x23
   1fe54:	b.eq	1fe60 <__gmpz_tdiv_qr@@Base+0x9c>  // b.none
   1fe58:	cmp	x26, x25
   1fe5c:	b.ne	1fe9c <__gmpz_tdiv_qr@@Base+0xd8>  // b.any
   1fe60:	lsl	x1, x21, #3
   1fe64:	mov	w8, #0x7f00                	// #32512
   1fe68:	cmp	x1, x8
   1fe6c:	b.hi	1ffd8 <__gmpz_tdiv_qr@@Base+0x214>  // b.pmore
   1fe70:	add	x9, x1, #0xf
   1fe74:	mov	x8, sp
   1fe78:	and	x9, x9, #0xfffffffffffffff0
   1fe7c:	sub	x28, x8, x9
   1fe80:	mov	sp, x28
   1fe84:	mov	x0, x28
   1fe88:	mov	x1, x26
   1fe8c:	mov	x2, x21
   1fe90:	bl	cc10 <__gmpn_copyi@plt>
   1fe94:	mov	x26, x28
   1fe98:	ldur	x28, [x29, #-32]
   1fe9c:	cmp	x27, x23
   1fea0:	b.eq	1feac <__gmpz_tdiv_qr@@Base+0xe8>  // b.none
   1fea4:	cmp	x27, x25
   1fea8:	b.ne	1fee8 <__gmpz_tdiv_qr@@Base+0x124>  // b.any
   1feac:	lsl	x1, x22, #3
   1feb0:	mov	w8, #0x7f00                	// #32512
   1feb4:	cmp	x1, x8
   1feb8:	b.hi	1ffe8 <__gmpz_tdiv_qr@@Base+0x224>  // b.pmore
   1febc:	add	x9, x1, #0xf
   1fec0:	mov	x8, sp
   1fec4:	and	x9, x9, #0xfffffffffffffff0
   1fec8:	sub	x28, x8, x9
   1fecc:	mov	sp, x28
   1fed0:	mov	x0, x28
   1fed4:	mov	x1, x27
   1fed8:	mov	x2, x22
   1fedc:	bl	cc10 <__gmpn_copyi@plt>
   1fee0:	mov	x27, x28
   1fee4:	ldur	x28, [x29, #-32]
   1fee8:	mov	x0, x25
   1feec:	mov	x1, x23
   1fef0:	mov	x2, xzr
   1fef4:	mov	x3, x27
   1fef8:	mov	x4, x22
   1fefc:	mov	x5, x26
   1ff00:	mov	x6, x21
   1ff04:	bl	c030 <__gmpn_tdiv_qr@plt>
   1ff08:	ldr	x8, [x25, x20, lsl #3]
   1ff0c:	ldur	x9, [x29, #-16]
   1ff10:	sub	x10, x23, #0x8
   1ff14:	cmp	x8, #0x0
   1ff18:	cset	w8, eq  // eq = none
   1ff1c:	sub	x8, x9, x8
   1ff20:	mov	x9, x21
   1ff24:	subs	x21, x21, #0x1
   1ff28:	b.lt	1ff34 <__gmpz_tdiv_qr@@Base+0x170>  // b.tstop
   1ff2c:	ldr	x11, [x10, x9, lsl #3]
   1ff30:	cbz	x11, 1ff20 <__gmpz_tdiv_qr@@Base+0x15c>
   1ff34:	eor	w10, w28, w24
   1ff38:	cmp	w10, #0x0
   1ff3c:	ldur	x10, [x29, #-24]
   1ff40:	neg	w11, w8
   1ff44:	neg	w12, w9
   1ff48:	csel	x8, x8, x11, ge  // ge = tcont
   1ff4c:	cmp	w24, #0x0
   1ff50:	str	w8, [x10, #4]
   1ff54:	csel	x8, x9, x12, ge  // ge = tcont
   1ff58:	str	w8, [x19, #4]
   1ff5c:	ldur	x0, [x29, #-8]
   1ff60:	cbz	x0, 1ffa4 <__gmpz_tdiv_qr@@Base+0x1e0>
   1ff64:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   1ff68:	b	1ffa4 <__gmpz_tdiv_qr@@Base+0x1e0>
   1ff6c:	mov	x0, x19
   1ff70:	mov	x1, x21
   1ff74:	bl	c1c0 <__gmpz_realloc@plt>
   1ff78:	mov	x23, x0
   1ff7c:	tbz	x20, #63, 1fe28 <__gmpz_tdiv_qr@@Base+0x64>
   1ff80:	cmp	x27, x19
   1ff84:	b.eq	1ffa0 <__gmpz_tdiv_qr@@Base+0x1dc>  // b.none
   1ff88:	ldr	x1, [x27, #8]
   1ff8c:	mov	x0, x23
   1ff90:	mov	x2, x22
   1ff94:	bl	cc10 <__gmpn_copyi@plt>
   1ff98:	ldr	w8, [x27, #4]
   1ff9c:	str	w8, [x19, #4]
   1ffa0:	str	wzr, [x25, #4]
   1ffa4:	mov	sp, x29
   1ffa8:	ldp	x20, x19, [sp, #80]
   1ffac:	ldp	x22, x21, [sp, #64]
   1ffb0:	ldp	x24, x23, [sp, #48]
   1ffb4:	ldp	x26, x25, [sp, #32]
   1ffb8:	ldp	x28, x27, [sp, #16]
   1ffbc:	ldp	x29, x30, [sp], #96
   1ffc0:	ret
   1ffc4:	ldur	x1, [x29, #-16]
   1ffc8:	mov	x0, x25
   1ffcc:	bl	c1c0 <__gmpz_realloc@plt>
   1ffd0:	mov	x25, x0
   1ffd4:	b	1fe40 <__gmpz_tdiv_qr@@Base+0x7c>
   1ffd8:	sub	x0, x29, #0x8
   1ffdc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1ffe0:	mov	x28, x0
   1ffe4:	b	1fe84 <__gmpz_tdiv_qr@@Base+0xc0>
   1ffe8:	sub	x0, x29, #0x8
   1ffec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   1fff0:	mov	x28, x0
   1fff4:	b	1fed0 <__gmpz_tdiv_qr@@Base+0x10c>
   1fff8:	bl	c100 <__gmp_divide_by_zero@plt>

000000000001fffc <__gmpz_tdiv_qr_ui@@Base>:
   1fffc:	stp	x29, x30, [sp, #-80]!
   20000:	str	x25, [sp, #16]
   20004:	stp	x24, x23, [sp, #32]
   20008:	stp	x22, x21, [sp, #48]
   2000c:	stp	x20, x19, [sp, #64]
   20010:	mov	x29, sp
   20014:	cbz	x3, 20108 <__gmpz_tdiv_qr_ui@@Base+0x10c>
   20018:	ldrsw	x25, [x2, #4]
   2001c:	mov	x24, x2
   20020:	mov	x20, x1
   20024:	mov	x19, x0
   20028:	cbz	w25, 20090 <__gmpz_tdiv_qr_ui@@Base+0x94>
   2002c:	ldrsw	x8, [x19]
   20030:	cmp	w25, #0x0
   20034:	cneg	x21, x25, lt  // lt = tstop
   20038:	mov	x22, x3
   2003c:	cmp	x21, x8
   20040:	b.gt	200e4 <__gmpz_tdiv_qr_ui@@Base+0xe8>
   20044:	ldr	x23, [x19, #8]
   20048:	ldr	x2, [x24, #8]
   2004c:	mov	x0, x23
   20050:	mov	x1, xzr
   20054:	mov	x3, x21
   20058:	mov	x4, x22
   2005c:	bl	ced0 <__gmpn_divrem_1@plt>
   20060:	mov	x22, x0
   20064:	cbz	x0, 200a4 <__gmpz_tdiv_qr_ui@@Base+0xa8>
   20068:	ldr	w8, [x20]
   2006c:	cmp	w25, #0x0
   20070:	mov	w9, #0x1                   	// #1
   20074:	cneg	w9, w9, lt  // lt = tstop
   20078:	cmp	w8, #0x0
   2007c:	str	w9, [x20, #4]
   20080:	b.le	200f8 <__gmpz_tdiv_qr_ui@@Base+0xfc>
   20084:	ldr	x0, [x20, #8]
   20088:	str	x22, [x0]
   2008c:	b	200a8 <__gmpz_tdiv_qr_ui@@Base+0xac>
   20090:	mov	w8, wzr
   20094:	mov	x22, xzr
   20098:	str	wzr, [x19, #4]
   2009c:	mov	x19, x20
   200a0:	b	200c4 <__gmpz_tdiv_qr_ui@@Base+0xc8>
   200a4:	str	wzr, [x20, #4]
   200a8:	add	x8, x23, x21, lsl #3
   200ac:	ldur	x8, [x8, #-8]
   200b0:	cmp	x8, #0x0
   200b4:	cset	w8, eq  // eq = none
   200b8:	sub	w8, w21, w8
   200bc:	cmp	w25, #0x0
   200c0:	cneg	w8, w8, lt  // lt = tstop
   200c4:	str	w8, [x19, #4]
   200c8:	mov	x0, x22
   200cc:	ldp	x20, x19, [sp, #64]
   200d0:	ldp	x22, x21, [sp, #48]
   200d4:	ldp	x24, x23, [sp, #32]
   200d8:	ldr	x25, [sp, #16]
   200dc:	ldp	x29, x30, [sp], #80
   200e0:	ret
   200e4:	mov	x0, x19
   200e8:	mov	x1, x21
   200ec:	bl	c1c0 <__gmpz_realloc@plt>
   200f0:	mov	x23, x0
   200f4:	b	20048 <__gmpz_tdiv_qr_ui@@Base+0x4c>
   200f8:	mov	w1, #0x1                   	// #1
   200fc:	mov	x0, x20
   20100:	bl	c1c0 <__gmpz_realloc@plt>
   20104:	b	20088 <__gmpz_tdiv_qr_ui@@Base+0x8c>
   20108:	bl	c100 <__gmp_divide_by_zero@plt>

000000000002010c <__gmpz_tdiv_r@@Base>:
   2010c:	stp	x29, x30, [sp, #-96]!
   20110:	stp	x26, x25, [sp, #32]
   20114:	stp	x24, x23, [sp, #48]
   20118:	stp	x22, x21, [sp, #64]
   2011c:	stp	x20, x19, [sp, #80]
   20120:	str	x27, [sp, #16]
   20124:	ldrsw	x27, [x1, #4]
   20128:	ldr	w8, [x2, #4]
   2012c:	mov	x29, sp
   20130:	cmp	x27, #0x0
   20134:	cneg	x20, x27, mi  // mi = first
   20138:	cmp	w8, #0x0
   2013c:	cneg	w21, w8, mi  // mi = first
   20140:	cbz	w21, 20328 <__gmpz_tdiv_r@@Base+0x21c>
   20144:	mov	x24, x1
   20148:	mov	x19, x0
   2014c:	sub	x23, x20, x21
   20150:	tbnz	x23, #63, 201b0 <__gmpz_tdiv_r@@Base+0xa4>
   20154:	ldrsw	x8, [x19]
   20158:	mov	x25, x2
   2015c:	cmp	x21, x8
   20160:	b.gt	202cc <__gmpz_tdiv_r@@Base+0x1c0>
   20164:	ldr	x22, [x19, #8]
   20168:	lsl	x8, x23, #3
   2016c:	add	x1, x8, #0x8
   20170:	mov	w8, #0x7f00                	// #32512
   20174:	cmp	x1, x8
   20178:	str	xzr, [x29, #24]
   2017c:	b.hi	202e0 <__gmpz_tdiv_r@@Base+0x1d4>  // b.pmore
   20180:	add	x9, x1, #0xf
   20184:	mov	x8, sp
   20188:	and	x9, x9, #0xfffffffffffffff0
   2018c:	sub	x23, x8, x9
   20190:	mov	sp, x23
   20194:	ldr	x25, [x25, #8]
   20198:	ldr	x24, [x24, #8]
   2019c:	cmp	x25, x22
   201a0:	b.eq	201dc <__gmpz_tdiv_r@@Base+0xd0>  // b.none
   201a4:	cmp	x24, x22
   201a8:	b.ne	20250 <__gmpz_tdiv_r@@Base+0x144>  // b.any
   201ac:	b	20218 <__gmpz_tdiv_r@@Base+0x10c>
   201b0:	cmp	x24, x19
   201b4:	b.eq	202ac <__gmpz_tdiv_r@@Base+0x1a0>  // b.none
   201b8:	ldrsw	x8, [x19]
   201bc:	str	w27, [x19, #4]
   201c0:	cmp	x20, x8
   201c4:	b.gt	202f8 <__gmpz_tdiv_r@@Base+0x1ec>
   201c8:	ldr	x0, [x19, #8]
   201cc:	ldr	x1, [x24, #8]
   201d0:	mov	x2, x20
   201d4:	bl	cc10 <__gmpn_copyi@plt>
   201d8:	b	202ac <__gmpz_tdiv_r@@Base+0x1a0>
   201dc:	cmp	w21, #0xfe0
   201e0:	lsl	x1, x21, #3
   201e4:	b.hi	20308 <__gmpz_tdiv_r@@Base+0x1fc>  // b.pmore
   201e8:	add	x9, x1, #0xf
   201ec:	mov	x8, sp
   201f0:	and	x9, x9, #0xffffffff0
   201f4:	sub	x26, x8, x9
   201f8:	mov	sp, x26
   201fc:	mov	x0, x26
   20200:	mov	x1, x25
   20204:	mov	x2, x21
   20208:	bl	cc10 <__gmpn_copyi@plt>
   2020c:	mov	x25, x26
   20210:	cmp	x24, x22
   20214:	b.ne	20250 <__gmpz_tdiv_r@@Base+0x144>  // b.any
   20218:	lsl	x1, x20, #3
   2021c:	mov	w8, #0x7f00                	// #32512
   20220:	cmp	x1, x8
   20224:	b.hi	20318 <__gmpz_tdiv_r@@Base+0x20c>  // b.pmore
   20228:	add	x9, x1, #0xf
   2022c:	mov	x8, sp
   20230:	and	x9, x9, #0xfffffffffffffff0
   20234:	sub	x26, x8, x9
   20238:	mov	sp, x26
   2023c:	mov	x0, x26
   20240:	mov	x1, x24
   20244:	mov	x2, x20
   20248:	bl	cc10 <__gmpn_copyi@plt>
   2024c:	mov	x24, x26
   20250:	mov	x0, x23
   20254:	mov	x1, x22
   20258:	mov	x2, xzr
   2025c:	mov	x3, x24
   20260:	mov	x4, x20
   20264:	mov	x5, x25
   20268:	mov	x6, x21
   2026c:	bl	c030 <__gmpn_tdiv_qr@plt>
   20270:	sub	x8, x22, #0x8
   20274:	subs	x9, x21, #0x1
   20278:	b.lt	20290 <__gmpz_tdiv_r@@Base+0x184>  // b.tstop
   2027c:	ldr	x10, [x8, x21, lsl #3]
   20280:	mov	x21, x9
   20284:	cbz	x10, 20274 <__gmpz_tdiv_r@@Base+0x168>
   20288:	add	x8, x9, #0x1
   2028c:	b	20294 <__gmpz_tdiv_r@@Base+0x188>
   20290:	mov	x8, xzr
   20294:	neg	w9, w8
   20298:	cmp	w27, #0x0
   2029c:	csel	x8, x8, x9, ge  // ge = tcont
   202a0:	str	w8, [x19, #4]
   202a4:	ldr	x0, [x29, #24]
   202a8:	cbnz	x0, 202f0 <__gmpz_tdiv_r@@Base+0x1e4>
   202ac:	mov	sp, x29
   202b0:	ldp	x20, x19, [sp, #80]
   202b4:	ldp	x22, x21, [sp, #64]
   202b8:	ldp	x24, x23, [sp, #48]
   202bc:	ldp	x26, x25, [sp, #32]
   202c0:	ldr	x27, [sp, #16]
   202c4:	ldp	x29, x30, [sp], #96
   202c8:	ret
   202cc:	mov	x0, x19
   202d0:	mov	x1, x21
   202d4:	bl	c1c0 <__gmpz_realloc@plt>
   202d8:	mov	x22, x0
   202dc:	b	20168 <__gmpz_tdiv_r@@Base+0x5c>
   202e0:	add	x0, x29, #0x18
   202e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   202e8:	mov	x23, x0
   202ec:	b	20194 <__gmpz_tdiv_r@@Base+0x88>
   202f0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   202f4:	b	202ac <__gmpz_tdiv_r@@Base+0x1a0>
   202f8:	mov	x0, x19
   202fc:	mov	x1, x20
   20300:	bl	c1c0 <__gmpz_realloc@plt>
   20304:	b	201cc <__gmpz_tdiv_r@@Base+0xc0>
   20308:	add	x0, x29, #0x18
   2030c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20310:	mov	x26, x0
   20314:	b	201fc <__gmpz_tdiv_r@@Base+0xf0>
   20318:	add	x0, x29, #0x18
   2031c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20320:	mov	x26, x0
   20324:	b	2023c <__gmpz_tdiv_r@@Base+0x130>
   20328:	bl	c100 <__gmp_divide_by_zero@plt>

000000000002032c <__gmpz_tdiv_r_2exp@@Base>:
   2032c:	stp	x29, x30, [sp, #-64]!
   20330:	stp	x22, x21, [sp, #32]
   20334:	stp	x20, x19, [sp, #48]
   20338:	ldr	w8, [x1, #4]
   2033c:	lsr	x21, x2, #6
   20340:	mov	x20, x1
   20344:	mov	x19, x0
   20348:	cmp	w8, #0x0
   2034c:	cneg	w22, w8, mi  // mi = first
   20350:	cmp	x21, x22
   20354:	str	x23, [sp, #16]
   20358:	mov	x29, sp
   2035c:	b.cs	2039c <__gmpz_tdiv_r_2exp@@Base+0x70>  // b.hs, b.nlast
   20360:	ldr	x8, [x20, #8]
   20364:	mov	x10, #0xffffffffffffffff    	// #-1
   20368:	lsl	x10, x10, x2
   2036c:	ldr	x9, [x8, x21, lsl #3]
   20370:	bics	x23, x9, x10
   20374:	b.eq	203ec <__gmpz_tdiv_r_2exp@@Base+0xc0>  // b.none
   20378:	ldrsw	x8, [x19]
   2037c:	add	x22, x21, #0x1
   20380:	cmp	x21, x8
   20384:	b.ge	2043c <__gmpz_tdiv_r_2exp@@Base+0x110>  // b.tcont
   20388:	ldr	x8, [x19, #8]
   2038c:	str	x23, [x8, x21, lsl #3]
   20390:	cmp	x19, x20
   20394:	b.ne	203b4 <__gmpz_tdiv_r_2exp@@Base+0x88>  // b.any
   20398:	b	203c4 <__gmpz_tdiv_r_2exp@@Base+0x98>
   2039c:	ldrsw	x8, [x19]
   203a0:	cmp	x22, x8
   203a4:	b.gt	2042c <__gmpz_tdiv_r_2exp@@Base+0x100>
   203a8:	mov	x21, x22
   203ac:	cmp	x19, x20
   203b0:	b.eq	203c4 <__gmpz_tdiv_r_2exp@@Base+0x98>  // b.none
   203b4:	ldr	x0, [x19, #8]
   203b8:	ldr	x1, [x20, #8]
   203bc:	mov	x2, x21
   203c0:	bl	cc10 <__gmpn_copyi@plt>
   203c4:	ldr	w8, [x20, #4]
   203c8:	neg	w9, w22
   203cc:	ldr	x23, [sp, #16]
   203d0:	cmp	w8, #0x0
   203d4:	csel	x8, x22, x9, ge  // ge = tcont
   203d8:	str	w8, [x19, #4]
   203dc:	ldp	x20, x19, [sp, #48]
   203e0:	ldp	x22, x21, [sp, #32]
   203e4:	ldp	x29, x30, [sp], #64
   203e8:	ret
   203ec:	sub	x8, x8, #0x8
   203f0:	subs	x9, x21, #0x1
   203f4:	b.lt	2040c <__gmpz_tdiv_r_2exp@@Base+0xe0>  // b.tstop
   203f8:	ldr	x10, [x8, x21, lsl #3]
   203fc:	mov	x21, x9
   20400:	cbz	x10, 203f0 <__gmpz_tdiv_r_2exp@@Base+0xc4>
   20404:	add	x21, x9, #0x1
   20408:	b	20410 <__gmpz_tdiv_r_2exp@@Base+0xe4>
   2040c:	mov	x21, xzr
   20410:	ldrsw	x8, [x19]
   20414:	cmp	x21, x8
   20418:	b.gt	2044c <__gmpz_tdiv_r_2exp@@Base+0x120>
   2041c:	mov	x22, x21
   20420:	cmp	x19, x20
   20424:	b.ne	203b4 <__gmpz_tdiv_r_2exp@@Base+0x88>  // b.any
   20428:	b	203c4 <__gmpz_tdiv_r_2exp@@Base+0x98>
   2042c:	mov	x0, x19
   20430:	mov	x1, x22
   20434:	bl	c1c0 <__gmpz_realloc@plt>
   20438:	b	203a8 <__gmpz_tdiv_r_2exp@@Base+0x7c>
   2043c:	mov	x0, x19
   20440:	mov	x1, x22
   20444:	bl	c1c0 <__gmpz_realloc@plt>
   20448:	b	20388 <__gmpz_tdiv_r_2exp@@Base+0x5c>
   2044c:	mov	x0, x19
   20450:	mov	x1, x21
   20454:	bl	c1c0 <__gmpz_realloc@plt>
   20458:	b	2041c <__gmpz_tdiv_r_2exp@@Base+0xf0>

000000000002045c <__gmpz_tdiv_r_ui@@Base>:
   2045c:	stp	x29, x30, [sp, #-48]!
   20460:	str	x21, [sp, #16]
   20464:	stp	x20, x19, [sp, #32]
   20468:	mov	x29, sp
   2046c:	cbz	x2, 204e8 <__gmpz_tdiv_r_ui@@Base+0x8c>
   20470:	ldrsw	x21, [x1, #4]
   20474:	mov	x20, x0
   20478:	cbz	w21, 204bc <__gmpz_tdiv_r_ui@@Base+0x60>
   2047c:	ldr	x0, [x1, #8]
   20480:	cmp	w21, #0x0
   20484:	cneg	x1, x21, lt  // lt = tstop
   20488:	bl	c540 <__gmpn_mod_1@plt>
   2048c:	mov	x19, x0
   20490:	cbz	x0, 204c0 <__gmpz_tdiv_r_ui@@Base+0x64>
   20494:	ldr	w8, [x20]
   20498:	cmp	w21, #0x0
   2049c:	mov	w9, #0x1                   	// #1
   204a0:	cneg	w9, w9, lt  // lt = tstop
   204a4:	cmp	w8, #0x0
   204a8:	str	w9, [x20, #4]
   204ac:	b.le	204d8 <__gmpz_tdiv_r_ui@@Base+0x7c>
   204b0:	ldr	x0, [x20, #8]
   204b4:	str	x19, [x0]
   204b8:	b	204c4 <__gmpz_tdiv_r_ui@@Base+0x68>
   204bc:	mov	x19, xzr
   204c0:	str	wzr, [x20, #4]
   204c4:	mov	x0, x19
   204c8:	ldp	x20, x19, [sp, #32]
   204cc:	ldr	x21, [sp, #16]
   204d0:	ldp	x29, x30, [sp], #48
   204d4:	ret
   204d8:	mov	w1, #0x1                   	// #1
   204dc:	mov	x0, x20
   204e0:	bl	c1c0 <__gmpz_realloc@plt>
   204e4:	b	204b4 <__gmpz_tdiv_r_ui@@Base+0x58>
   204e8:	bl	c100 <__gmp_divide_by_zero@plt>

00000000000204ec <__gmpz_tstbit@@Base>:
   204ec:	ldr	w11, [x0, #4]
   204f0:	lsr	x10, x1, #6
   204f4:	cmp	w11, #0x0
   204f8:	cneg	w8, w11, mi  // mi = first
   204fc:	cmp	x10, x8
   20500:	b.cs	20540 <__gmpz_tstbit@@Base+0x54>  // b.hs, b.nlast
   20504:	ldr	x12, [x0, #8]
   20508:	ldr	x9, [x12, x10, lsl #3]
   2050c:	mov	x8, x9
   20510:	tbz	w11, #31, 20534 <__gmpz_tstbit@@Base+0x48>
   20514:	neg	x8, x9
   20518:	sub	x11, x12, #0x8
   2051c:	lsl	x10, x10, #3
   20520:	cbz	x10, 20534 <__gmpz_tstbit@@Base+0x48>
   20524:	ldr	x12, [x11, x10]
   20528:	sub	x10, x10, #0x8
   2052c:	cbz	x12, 20520 <__gmpz_tstbit@@Base+0x34>
   20530:	mvn	x8, x9
   20534:	lsr	x8, x8, x1
   20538:	and	w0, w8, #0x1
   2053c:	ret
   20540:	lsr	w0, w11, #31
   20544:	ret

0000000000020548 <__gmpz_ui_pow_ui@@Base>:
   20548:	sub	sp, sp, #0x20
   2054c:	cmp	x1, #0x0
   20550:	mov	x3, x2
   20554:	str	x1, [sp, #8]
   20558:	cset	w2, ne  // ne = any
   2055c:	add	x1, sp, #0x8
   20560:	stp	x29, x30, [sp, #16]
   20564:	add	x29, sp, #0x10
   20568:	bl	c4a0 <__gmpz_n_pow_ui@plt>
   2056c:	ldp	x29, x30, [sp, #16]
   20570:	add	sp, sp, #0x20
   20574:	ret

0000000000020578 <__gmpz_ui_sub@@Base>:
   20578:	stp	x29, x30, [sp, #-64]!
   2057c:	stp	x24, x23, [sp, #16]
   20580:	stp	x22, x21, [sp, #32]
   20584:	stp	x20, x19, [sp, #48]
   20588:	ldrsw	x21, [x2, #4]
   2058c:	mov	x22, x2
   20590:	mov	x20, x1
   20594:	mov	x19, x0
   20598:	cmp	w21, #0x2
   2059c:	mov	x29, sp
   205a0:	b.lt	205dc <__gmpz_ui_sub@@Base+0x64>  // b.tstop
   205a4:	ldr	w8, [x19]
   205a8:	cmp	w21, w8
   205ac:	b.gt	2066c <__gmpz_ui_sub@@Base+0xf4>
   205b0:	ldr	x23, [x19, #8]
   205b4:	ldr	x1, [x22, #8]
   205b8:	mov	x0, x23
   205bc:	mov	x2, x21
   205c0:	mov	x3, x20
   205c4:	bl	caf0 <__gmpn_sub_1@plt>
   205c8:	add	x8, x23, x21, lsl #3
   205cc:	ldur	x8, [x8, #-8]
   205d0:	cmp	x8, #0x0
   205d4:	cset	w8, eq  // eq = none
   205d8:	b	20650 <__gmpz_ui_sub@@Base+0xd8>
   205dc:	tbnz	w21, #31, 20614 <__gmpz_ui_sub@@Base+0x9c>
   205e0:	ldr	x8, [x22, #8]
   205e4:	ldr	w9, [x19]
   205e8:	neg	x10, x21
   205ec:	ldr	x8, [x8]
   205f0:	cmp	w9, #0x0
   205f4:	and	x21, x8, x10
   205f8:	b.le	20680 <__gmpz_ui_sub@@Base+0x108>
   205fc:	ldr	x0, [x19, #8]
   20600:	subs	x8, x21, x20
   20604:	b.ls	20694 <__gmpz_ui_sub@@Base+0x11c>  // b.plast
   20608:	str	x8, [x0]
   2060c:	mov	w8, #0xffffffff            	// #-1
   20610:	b	20654 <__gmpz_ui_sub@@Base+0xdc>
   20614:	ldrsw	x8, [x19]
   20618:	mov	w9, #0x1                   	// #1
   2061c:	sub	x1, x9, x21
   20620:	neg	x23, x21
   20624:	cmp	x1, x8
   20628:	b.gt	206a4 <__gmpz_ui_sub@@Base+0x12c>
   2062c:	ldr	x24, [x19, #8]
   20630:	ldr	x1, [x22, #8]
   20634:	mov	x0, x24
   20638:	mov	x2, x23
   2063c:	mov	x3, x20
   20640:	bl	c150 <__gmpn_add_1@plt>
   20644:	cmp	x0, #0x0
   20648:	cset	w8, ne  // ne = any
   2064c:	str	x0, [x24, x23, lsl #3]
   20650:	sub	w8, w8, w21
   20654:	str	w8, [x19, #4]
   20658:	ldp	x20, x19, [sp, #48]
   2065c:	ldp	x22, x21, [sp, #32]
   20660:	ldp	x24, x23, [sp, #16]
   20664:	ldp	x29, x30, [sp], #64
   20668:	ret
   2066c:	mov	x0, x19
   20670:	mov	x1, x21
   20674:	bl	c1c0 <__gmpz_realloc@plt>
   20678:	mov	x23, x0
   2067c:	b	205b4 <__gmpz_ui_sub@@Base+0x3c>
   20680:	mov	w1, #0x1                   	// #1
   20684:	mov	x0, x19
   20688:	bl	c1c0 <__gmpz_realloc@plt>
   2068c:	subs	x8, x21, x20
   20690:	b.hi	20608 <__gmpz_ui_sub@@Base+0x90>  // b.pmore
   20694:	subs	x8, x20, x21
   20698:	str	x8, [x0]
   2069c:	cset	w8, ne  // ne = any
   206a0:	b	20654 <__gmpz_ui_sub@@Base+0xdc>
   206a4:	mov	x0, x19
   206a8:	bl	c1c0 <__gmpz_realloc@plt>
   206ac:	mov	x24, x0
   206b0:	b	20630 <__gmpz_ui_sub@@Base+0xb8>

00000000000206b4 <__gmpz_urandomb@@Base>:
   206b4:	stp	x29, x30, [sp, #-64]!
   206b8:	stp	x22, x21, [sp, #32]
   206bc:	stp	x20, x19, [sp, #48]
   206c0:	ldrsw	x8, [x0]
   206c4:	add	x9, x2, #0x3f
   206c8:	lsr	x20, x9, #6
   206cc:	mov	x19, x0
   206d0:	mov	x21, x2
   206d4:	cmp	x20, x8
   206d8:	mov	x22, x1
   206dc:	str	x23, [sp, #16]
   206e0:	mov	x29, sp
   206e4:	b.gt	20740 <__gmpz_urandomb@@Base+0x8c>
   206e8:	ldr	x23, [x19, #8]
   206ec:	ldr	x8, [x22, #24]
   206f0:	mov	x0, x22
   206f4:	mov	x1, x23
   206f8:	mov	x2, x21
   206fc:	ldr	x8, [x8, #8]
   20700:	blr	x8
   20704:	sub	x8, x23, #0x8
   20708:	subs	x9, x20, #0x1
   2070c:	b.lt	20724 <__gmpz_urandomb@@Base+0x70>  // b.tstop
   20710:	ldr	x10, [x8, x20, lsl #3]
   20714:	mov	x20, x9
   20718:	cbz	x10, 20708 <__gmpz_urandomb@@Base+0x54>
   2071c:	add	x8, x9, #0x1
   20720:	b	20728 <__gmpz_urandomb@@Base+0x74>
   20724:	mov	x8, xzr
   20728:	str	w8, [x19, #4]
   2072c:	ldp	x20, x19, [sp, #48]
   20730:	ldp	x22, x21, [sp, #32]
   20734:	ldr	x23, [sp, #16]
   20738:	ldp	x29, x30, [sp], #64
   2073c:	ret
   20740:	mov	x0, x19
   20744:	mov	x1, x20
   20748:	bl	c1c0 <__gmpz_realloc@plt>
   2074c:	mov	x23, x0
   20750:	b	206ec <__gmpz_urandomb@@Base+0x38>

0000000000020754 <__gmpz_urandomm@@Base>:
   20754:	stp	x29, x30, [sp, #-96]!
   20758:	stp	x26, x25, [sp, #32]
   2075c:	stp	x24, x23, [sp, #48]
   20760:	stp	x22, x21, [sp, #64]
   20764:	stp	x20, x19, [sp, #80]
   20768:	ldr	w8, [x2, #4]
   2076c:	str	x27, [sp, #16]
   20770:	mov	x29, sp
   20774:	cmp	w8, #0x0
   20778:	cneg	w20, w8, mi  // mi = first
   2077c:	cbz	w20, 20938 <__gmpz_urandomm@@Base+0x1e4>
   20780:	ldr	x21, [x2, #8]
   20784:	sub	x23, x20, #0x1
   20788:	mov	x25, x2
   2078c:	mov	x22, x1
   20790:	ldr	x24, [x21, x23, lsl #3]
   20794:	mov	x19, x0
   20798:	sub	x8, x24, #0x1
   2079c:	tst	x24, x8
   207a0:	b.ne	207b4 <__gmpz_urandomm@@Base+0x60>  // b.any
   207a4:	cmp	w20, #0x1
   207a8:	b.ne	207bc <__gmpz_urandomm@@Base+0x68>  // b.any
   207ac:	mov	x8, #0xffffffffffffffff    	// #-1
   207b0:	b	207d0 <__gmpz_urandomm@@Base+0x7c>
   207b4:	mov	x8, xzr
   207b8:	b	207d0 <__gmpz_urandomm@@Base+0x7c>
   207bc:	mov	x0, x21
   207c0:	mov	x1, x23
   207c4:	bl	c010 <__gmpn_zero_p@plt>
   207c8:	cmp	w0, #0x0
   207cc:	csetm	x8, ne  // ne = any
   207d0:	clz	x9, x24
   207d4:	lsl	x10, x20, #6
   207d8:	sub	x9, x10, x9
   207dc:	adds	x24, x8, x9
   207e0:	b.eq	208d4 <__gmpz_urandomm@@Base+0x180>  // b.none
   207e4:	cmp	x19, x25
   207e8:	str	xzr, [x29, #24]
   207ec:	b.ne	20824 <__gmpz_urandomm@@Base+0xd0>  // b.any
   207f0:	cmp	w20, #0xfe0
   207f4:	lsl	x1, x20, #3
   207f8:	b.hi	20928 <__gmpz_urandomm@@Base+0x1d4>  // b.pmore
   207fc:	add	x9, x1, #0xf
   20800:	mov	x8, sp
   20804:	and	x9, x9, #0xffffffff0
   20808:	sub	x25, x8, x9
   2080c:	mov	sp, x25
   20810:	mov	x0, x25
   20814:	mov	x1, x21
   20818:	mov	x2, x20
   2081c:	bl	cc10 <__gmpn_copyi@plt>
   20820:	mov	x21, x25
   20824:	ldrsw	x8, [x19]
   20828:	cmp	x20, x8
   2082c:	b.gt	2090c <__gmpz_urandomm@@Base+0x1b8>
   20830:	ldr	x25, [x19, #8]
   20834:	mov	w26, #0x50                  	// #80
   20838:	mov	w27, #0x1                   	// #1
   2083c:	str	xzr, [x25, x23, lsl #3]
   20840:	ldr	x8, [x22, #24]
   20844:	mov	x0, x22
   20848:	mov	x1, x25
   2084c:	mov	x2, x24
   20850:	ldr	x8, [x8, #8]
   20854:	blr	x8
   20858:	mov	x8, x23
   2085c:	add	x9, x8, #0x1
   20860:	cmp	x9, #0x1
   20864:	b.lt	2088c <__gmpz_urandomm@@Base+0x138>  // b.tstop
   20868:	lsl	x9, x8, #3
   2086c:	ldr	x10, [x25, x9]
   20870:	ldr	x9, [x21, x9]
   20874:	sub	x8, x8, #0x1
   20878:	cmp	x10, x9
   2087c:	b.eq	2085c <__gmpz_urandomm@@Base+0x108>  // b.none
   20880:	cneg	w8, w27, ls  // ls = plast
   20884:	tbz	w8, #31, 20894 <__gmpz_urandomm@@Base+0x140>
   20888:	b	2089c <__gmpz_urandomm@@Base+0x148>
   2088c:	mov	w8, wzr
   20890:	tbnz	w8, #31, 2089c <__gmpz_urandomm@@Base+0x148>
   20894:	subs	w26, w26, #0x1
   20898:	b.ne	20840 <__gmpz_urandomm@@Base+0xec>  // b.any
   2089c:	cbnz	w26, 208b4 <__gmpz_urandomm@@Base+0x160>
   208a0:	mov	x0, x25
   208a4:	mov	x1, x25
   208a8:	mov	x2, x21
   208ac:	mov	x3, x20
   208b0:	bl	c420 <__gmpn_sub_n@plt>
   208b4:	sub	x8, x25, #0x8
   208b8:	subs	x9, x20, #0x1
   208bc:	b.lt	208dc <__gmpz_urandomm@@Base+0x188>  // b.tstop
   208c0:	ldr	x10, [x8, x20, lsl #3]
   208c4:	mov	x20, x9
   208c8:	cbz	x10, 208b8 <__gmpz_urandomm@@Base+0x164>
   208cc:	add	x8, x9, #0x1
   208d0:	b	208e0 <__gmpz_urandomm@@Base+0x18c>
   208d4:	str	wzr, [x19, #4]
   208d8:	b	208ec <__gmpz_urandomm@@Base+0x198>
   208dc:	mov	x8, xzr
   208e0:	str	w8, [x19, #4]
   208e4:	ldr	x0, [x29, #24]
   208e8:	cbnz	x0, 20920 <__gmpz_urandomm@@Base+0x1cc>
   208ec:	mov	sp, x29
   208f0:	ldp	x20, x19, [sp, #80]
   208f4:	ldp	x22, x21, [sp, #64]
   208f8:	ldp	x24, x23, [sp, #48]
   208fc:	ldp	x26, x25, [sp, #32]
   20900:	ldr	x27, [sp, #16]
   20904:	ldp	x29, x30, [sp], #96
   20908:	ret
   2090c:	mov	x0, x19
   20910:	mov	x1, x20
   20914:	bl	c1c0 <__gmpz_realloc@plt>
   20918:	mov	x25, x0
   2091c:	b	20834 <__gmpz_urandomm@@Base+0xe0>
   20920:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   20924:	b	208ec <__gmpz_urandomm@@Base+0x198>
   20928:	add	x0, x29, #0x18
   2092c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20930:	mov	x25, x0
   20934:	b	20810 <__gmpz_urandomm@@Base+0xbc>
   20938:	bl	c100 <__gmp_divide_by_zero@plt>

000000000002093c <__gmpz_xor@@Base>:
   2093c:	stp	x29, x30, [sp, #-96]!
   20940:	stp	x28, x27, [sp, #16]
   20944:	stp	x26, x25, [sp, #32]
   20948:	stp	x24, x23, [sp, #48]
   2094c:	stp	x22, x21, [sp, #64]
   20950:	stp	x20, x19, [sp, #80]
   20954:	mov	x29, sp
   20958:	sub	sp, sp, #0x10
   2095c:	ldr	w8, [x1, #4]
   20960:	ldr	w9, [x2, #4]
   20964:	ldr	x23, [x0, #8]
   20968:	mov	x19, x0
   2096c:	cmp	w8, w9
   20970:	csel	x25, x2, x1, lt  // lt = tstop
   20974:	ldr	x24, [x25, #8]
   20978:	csel	w10, w8, w9, gt
   2097c:	csel	w8, w8, w9, lt  // lt = tstop
   20980:	sxtw	x20, w10
   20984:	sxtw	x21, w8
   20988:	csel	x27, x1, x2, lt  // lt = tstop
   2098c:	tbnz	w8, #31, 209f0 <__gmpz_xor@@Base+0xb4>
   20990:	cmp	x23, x24
   20994:	b.eq	209b8 <__gmpz_xor@@Base+0x7c>  // b.none
   20998:	ldr	w8, [x19]
   2099c:	cmp	w20, w8
   209a0:	b.gt	20bb0 <__gmpz_xor@@Base+0x274>
   209a4:	lsl	x8, x21, #3
   209a8:	add	x0, x23, x8
   209ac:	add	x1, x24, x8
   209b0:	sub	x2, x20, x21
   209b4:	bl	cc10 <__gmpn_copyi@plt>
   209b8:	cbz	w21, 209d0 <__gmpz_xor@@Base+0x94>
   209bc:	ldr	x2, [x27, #8]
   209c0:	mov	x0, x23
   209c4:	mov	x1, x24
   209c8:	mov	x3, x21
   209cc:	bl	cd00 <__gmpn_xor_n@plt>
   209d0:	sub	x8, x23, #0x8
   209d4:	mov	x9, x20
   209d8:	subs	x20, x20, #0x1
   209dc:	b.lt	209e8 <__gmpz_xor@@Base+0xac>  // b.tstop
   209e0:	ldr	x10, [x8, x9, lsl #3]
   209e4:	cbz	x10, 209d4 <__gmpz_xor@@Base+0x98>
   209e8:	str	w9, [x19, #4]
   209ec:	b	20b90 <__gmpz_xor@@Base+0x254>
   209f0:	neg	x22, x21
   209f4:	stur	xzr, [x29, #-8]
   209f8:	tbnz	w20, #31, 20a6c <__gmpz_xor@@Base+0x130>
   209fc:	ldrsw	x8, [x19]
   20a00:	cmp	x22, x20
   20a04:	csel	x28, x20, x22, lt  // lt = tstop
   20a08:	cmp	x28, x8
   20a0c:	b.ge	20bc4 <__gmpz_xor@@Base+0x288>  // b.tcont
   20a10:	ldr	x24, [x25, #8]
   20a14:	lsl	x26, x22, #3
   20a18:	mov	w8, #0x7f00                	// #32512
   20a1c:	cmp	x26, x8
   20a20:	b.hi	20bd8 <__gmpz_xor@@Base+0x29c>  // b.pmore
   20a24:	add	x9, x26, #0xf
   20a28:	mov	x8, sp
   20a2c:	and	x9, x9, #0xfffffffffffffff0
   20a30:	sub	x25, x8, x9
   20a34:	mov	sp, x25
   20a38:	ldr	x1, [x27, #8]
   20a3c:	mov	w3, #0x1                   	// #1
   20a40:	mov	x0, x25
   20a44:	mov	x2, x22
   20a48:	bl	caf0 <__gmpn_sub_1@plt>
   20a4c:	subs	x2, x22, x20
   20a50:	b.le	20b24 <__gmpz_xor@@Base+0x1e8>
   20a54:	lsl	x8, x20, #3
   20a58:	add	x0, x23, x8
   20a5c:	add	x1, x25, x8
   20a60:	bl	cc10 <__gmpn_copyi@plt>
   20a64:	cbnz	w20, 20b38 <__gmpz_xor@@Base+0x1fc>
   20a68:	b	20b4c <__gmpz_xor@@Base+0x210>
   20a6c:	add	x8, x20, x21
   20a70:	neg	x1, x8, lsl #3
   20a74:	mov	w8, #0x7f00                	// #32512
   20a78:	cmp	x1, x8
   20a7c:	neg	x23, x20
   20a80:	b.hi	20bf4 <__gmpz_xor@@Base+0x2b8>  // b.pmore
   20a84:	add	x9, x1, #0xf
   20a88:	mov	x8, sp
   20a8c:	and	x9, x9, #0xfffffffffffffff0
   20a90:	sub	x25, x8, x9
   20a94:	mov	sp, x25
   20a98:	mov	w3, #0x1                   	// #1
   20a9c:	mov	x0, x25
   20aa0:	mov	x1, x24
   20aa4:	mov	x2, x23
   20aa8:	add	x26, x25, x23, lsl #3
   20aac:	bl	caf0 <__gmpn_sub_1@plt>
   20ab0:	ldr	x1, [x27, #8]
   20ab4:	mov	w3, #0x1                   	// #1
   20ab8:	mov	x0, x26
   20abc:	mov	x2, x22
   20ac0:	bl	caf0 <__gmpn_sub_1@plt>
   20ac4:	ldrsw	x8, [x19]
   20ac8:	cmp	x22, x8
   20acc:	b.gt	20c04 <__gmpz_xor@@Base+0x2c8>
   20ad0:	ldr	x24, [x19, #8]
   20ad4:	lsl	x8, x23, #3
   20ad8:	add	x0, x24, x8
   20adc:	add	x1, x26, x8
   20ae0:	sub	x2, x20, x21
   20ae4:	bl	cc10 <__gmpn_copyi@plt>
   20ae8:	mov	x0, x24
   20aec:	mov	x1, x25
   20af0:	mov	x2, x26
   20af4:	mov	x3, x23
   20af8:	bl	cd00 <__gmpn_xor_n@plt>
   20afc:	ldur	x0, [x29, #-8]
   20b00:	cbnz	x0, 20c18 <__gmpz_xor@@Base+0x2dc>
   20b04:	sub	x9, x24, #0x8
   20b08:	subs	x10, x22, #0x1
   20b0c:	mov	w8, w22
   20b10:	b.lt	20b8c <__gmpz_xor@@Base+0x250>  // b.tstop
   20b14:	ldr	x11, [x9, x22, lsl #3]
   20b18:	mov	x22, x10
   20b1c:	cbz	x11, 20b08 <__gmpz_xor@@Base+0x1cc>
   20b20:	b	20b8c <__gmpz_xor@@Base+0x250>
   20b24:	add	x0, x23, x26
   20b28:	add	x1, x24, x26
   20b2c:	add	x2, x20, x21
   20b30:	bl	cc10 <__gmpn_copyi@plt>
   20b34:	mov	x20, x22
   20b38:	mov	x0, x23
   20b3c:	mov	x1, x24
   20b40:	mov	x2, x25
   20b44:	mov	x3, x20
   20b48:	bl	cd00 <__gmpn_xor_n@plt>
   20b4c:	ldur	x0, [x29, #-8]
   20b50:	cbnz	x0, 20bec <__gmpz_xor@@Base+0x2b0>
   20b54:	mov	x8, x23
   20b58:	str	xzr, [x23, x28, lsl #3]
   20b5c:	ldr	x9, [x8]
   20b60:	adds	x9, x9, #0x1
   20b64:	str	x9, [x8], #8
   20b68:	b.cs	20b5c <__gmpz_xor@@Base+0x220>  // b.hs, b.nlast
   20b6c:	ldr	x8, [x23, x28, lsl #3]
   20b70:	add	x9, x8, x28
   20b74:	mvn	w8, w9
   20b78:	add	x9, x23, x9, lsl #3
   20b7c:	sub	x9, x9, #0x8
   20b80:	ldr	x10, [x9], #-8
   20b84:	add	w8, w8, #0x1
   20b88:	cbz	x10, 20b80 <__gmpz_xor@@Base+0x244>
   20b8c:	str	w8, [x19, #4]
   20b90:	mov	sp, x29
   20b94:	ldp	x20, x19, [sp, #80]
   20b98:	ldp	x22, x21, [sp, #64]
   20b9c:	ldp	x24, x23, [sp, #48]
   20ba0:	ldp	x26, x25, [sp, #32]
   20ba4:	ldp	x28, x27, [sp, #16]
   20ba8:	ldp	x29, x30, [sp], #96
   20bac:	ret
   20bb0:	mov	x0, x19
   20bb4:	mov	x1, x20
   20bb8:	bl	c1c0 <__gmpz_realloc@plt>
   20bbc:	mov	x23, x0
   20bc0:	b	209a4 <__gmpz_xor@@Base+0x68>
   20bc4:	add	x1, x28, #0x1
   20bc8:	mov	x0, x19
   20bcc:	bl	c1c0 <__gmpz_realloc@plt>
   20bd0:	mov	x23, x0
   20bd4:	b	20a10 <__gmpz_xor@@Base+0xd4>
   20bd8:	sub	x0, x29, #0x8
   20bdc:	mov	x1, x26
   20be0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20be4:	mov	x25, x0
   20be8:	b	20a38 <__gmpz_xor@@Base+0xfc>
   20bec:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   20bf0:	b	20b54 <__gmpz_xor@@Base+0x218>
   20bf4:	sub	x0, x29, #0x8
   20bf8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20bfc:	mov	x25, x0
   20c00:	b	20a98 <__gmpz_xor@@Base+0x15c>
   20c04:	mov	x0, x19
   20c08:	mov	x1, x22
   20c0c:	bl	c1c0 <__gmpz_realloc@plt>
   20c10:	mov	x24, x0
   20c14:	b	20ad4 <__gmpz_xor@@Base+0x198>
   20c18:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   20c1c:	b	20b04 <__gmpz_xor@@Base+0x1c8>

0000000000020c20 <__gmpq_abs@@Base>:
   20c20:	stp	x29, x30, [sp, #-64]!
   20c24:	stp	x22, x21, [sp, #32]
   20c28:	stp	x20, x19, [sp, #48]
   20c2c:	ldr	w8, [x1, #4]
   20c30:	mov	x19, x0
   20c34:	str	x23, [sp, #16]
   20c38:	mov	x29, sp
   20c3c:	cmp	w8, #0x0
   20c40:	cneg	w20, w8, mi  // mi = first
   20c44:	cmp	x0, x1
   20c48:	b.eq	20c98 <__gmpq_abs@@Base+0x78>  // b.none
   20c4c:	ldrsw	x8, [x19]
   20c50:	ldr	w23, [x1, #20]
   20c54:	mov	x21, x1
   20c58:	cmp	x20, x8
   20c5c:	sxtw	x22, w23
   20c60:	b.gt	20cb0 <__gmpq_abs@@Base+0x90>
   20c64:	ldr	x0, [x19, #8]
   20c68:	ldr	x1, [x21, #8]
   20c6c:	mov	x2, x20
   20c70:	bl	cc10 <__gmpn_copyi@plt>
   20c74:	mov	x0, x19
   20c78:	ldr	w8, [x0, #16]!
   20c7c:	cmp	w23, w8
   20c80:	b.gt	20cc0 <__gmpq_abs@@Base+0xa0>
   20c84:	ldr	x0, [x19, #24]
   20c88:	str	w23, [x19, #20]
   20c8c:	ldr	x1, [x21, #24]
   20c90:	mov	x2, x22
   20c94:	bl	cc10 <__gmpn_copyi@plt>
   20c98:	str	w20, [x19, #4]
   20c9c:	ldp	x20, x19, [sp, #48]
   20ca0:	ldp	x22, x21, [sp, #32]
   20ca4:	ldr	x23, [sp, #16]
   20ca8:	ldp	x29, x30, [sp], #64
   20cac:	ret
   20cb0:	mov	x0, x19
   20cb4:	mov	x1, x20
   20cb8:	bl	c1c0 <__gmpz_realloc@plt>
   20cbc:	b	20c68 <__gmpq_abs@@Base+0x48>
   20cc0:	mov	x1, x22
   20cc4:	bl	c1c0 <__gmpz_realloc@plt>
   20cc8:	b	20c88 <__gmpq_abs@@Base+0x68>

0000000000020ccc <__gmpq_add@@Base>:
   20ccc:	stp	x29, x30, [sp, #-16]!
   20cd0:	adrp	x3, 69000 <__gmp_limbroots_table@@Base+0x11338>
   20cd4:	ldr	x3, [x3, #4000]
   20cd8:	mov	x29, sp
   20cdc:	bl	20ce8 <__gmpq_add@@Base+0x1c>
   20ce0:	ldp	x29, x30, [sp], #16
   20ce4:	ret
   20ce8:	stp	x29, x30, [sp, #-96]!
   20cec:	stp	x28, x27, [sp, #16]
   20cf0:	stp	x26, x25, [sp, #32]
   20cf4:	stp	x24, x23, [sp, #48]
   20cf8:	stp	x22, x21, [sp, #64]
   20cfc:	stp	x20, x19, [sp, #80]
   20d00:	mov	x29, sp
   20d04:	sub	sp, sp, #0x50
   20d08:	ldr	w8, [x1, #4]
   20d0c:	ldr	w9, [x2, #4]
   20d10:	ldrsw	x26, [x1, #20]
   20d14:	ldrsw	x25, [x2, #20]
   20d18:	cmp	w8, #0x0
   20d1c:	cneg	w28, w8, mi  // mi = first
   20d20:	cmp	w9, #0x0
   20d24:	cneg	w27, w9, mi  // mi = first
   20d28:	cmp	x26, x25
   20d2c:	csel	x8, x26, x25, lt  // lt = tstop
   20d30:	mov	x23, x1
   20d34:	mov	w10, #0x7f00                	// #32512
   20d38:	lsl	x1, x8, #3
   20d3c:	mov	x21, x3
   20d40:	mov	x22, x2
   20d44:	mov	x19, x0
   20d48:	cmp	x1, x10
   20d4c:	stur	xzr, [x29, #-56]
   20d50:	stur	w8, [x29, #-16]
   20d54:	b.hi	20f70 <__gmpq_add@@Base+0x2a4>  // b.pmore
   20d58:	add	x9, x1, #0xf
   20d5c:	mov	x8, sp
   20d60:	and	x9, x9, #0xfffffffffffffff0
   20d64:	sub	x0, x8, x9
   20d68:	mov	sp, x0
   20d6c:	add	x25, x25, x28
   20d70:	lsl	x1, x25, #3
   20d74:	mov	w8, #0x7f00                	// #32512
   20d78:	add	x24, x23, #0x10
   20d7c:	add	x20, x22, #0x10
   20d80:	cmp	x1, x8
   20d84:	stur	x0, [x29, #-8]
   20d88:	stur	w25, [x29, #-32]
   20d8c:	b.hi	20f7c <__gmpq_add@@Base+0x2b0>  // b.pmore
   20d90:	add	x9, x1, #0xf
   20d94:	mov	x8, sp
   20d98:	and	x9, x9, #0xfffffffffffffff0
   20d9c:	sub	x0, x8, x9
   20da0:	mov	sp, x0
   20da4:	add	x26, x27, x26
   20da8:	lsl	x1, x26, #3
   20dac:	mov	w8, #0x7f00                	// #32512
   20db0:	cmp	x1, x8
   20db4:	stur	x0, [x29, #-24]
   20db8:	stur	w26, [x29, #-48]
   20dbc:	b.hi	20f88 <__gmpq_add@@Base+0x2bc>  // b.pmore
   20dc0:	add	x9, x1, #0xf
   20dc4:	mov	x8, sp
   20dc8:	and	x9, x9, #0xfffffffffffffff0
   20dcc:	sub	x0, x8, x9
   20dd0:	mov	sp, x0
   20dd4:	stur	x0, [x29, #-40]
   20dd8:	sub	x0, x29, #0x10
   20ddc:	mov	x1, x24
   20de0:	mov	x2, x20
   20de4:	bl	d140 <__gmpz_gcd@plt>
   20de8:	ldursw	x8, [x29, #-12]
   20dec:	cmp	w8, #0x1
   20df0:	b.ne	20e44 <__gmpq_add@@Base+0x178>  // b.any
   20df4:	ldur	x9, [x29, #-8]
   20df8:	ldr	x9, [x9]
   20dfc:	cmp	x9, #0x1
   20e00:	b.ne	20e44 <__gmpq_add@@Base+0x178>  // b.any
   20e04:	sub	x0, x29, #0x20
   20e08:	mov	x1, x23
   20e0c:	mov	x2, x20
   20e10:	bl	c620 <__gmpz_mul@plt>
   20e14:	sub	x0, x29, #0x30
   20e18:	mov	x1, x22
   20e1c:	mov	x2, x24
   20e20:	bl	c620 <__gmpz_mul@plt>
   20e24:	sub	x1, x29, #0x20
   20e28:	sub	x2, x29, #0x30
   20e2c:	mov	x0, x19
   20e30:	blr	x21
   20e34:	add	x0, x19, #0x10
   20e38:	mov	x1, x24
   20e3c:	mov	x2, x20
   20e40:	b	20f44 <__gmpq_add@@Base+0x278>
   20e44:	cmp	x25, x26
   20e48:	csel	x9, x25, x26, gt
   20e4c:	sub	w10, w9, w8
   20e50:	sub	x8, x9, x8
   20e54:	lsl	x8, x8, #3
   20e58:	add	x1, x8, #0x10
   20e5c:	mov	w8, #0x7f00                	// #32512
   20e60:	add	w9, w10, #0x2
   20e64:	cmp	x1, x8
   20e68:	stur	w9, [x29, #-72]
   20e6c:	b.hi	20f9c <__gmpq_add@@Base+0x2d0>  // b.pmore
   20e70:	add	x9, x1, #0xf
   20e74:	mov	x8, sp
   20e78:	and	x9, x9, #0xfffffffffffffff0
   20e7c:	sub	x0, x8, x9
   20e80:	mov	sp, x0
   20e84:	stur	x0, [x29, #-64]
   20e88:	sub	x0, x29, #0x48
   20e8c:	sub	x2, x29, #0x10
   20e90:	mov	x1, x20
   20e94:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   20e98:	sub	x0, x29, #0x30
   20e9c:	sub	x2, x29, #0x10
   20ea0:	mov	x1, x24
   20ea4:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   20ea8:	sub	x0, x29, #0x20
   20eac:	sub	x2, x29, #0x48
   20eb0:	mov	x1, x23
   20eb4:	bl	c620 <__gmpz_mul@plt>
   20eb8:	sub	x0, x29, #0x48
   20ebc:	sub	x2, x29, #0x30
   20ec0:	mov	x1, x22
   20ec4:	bl	c620 <__gmpz_mul@plt>
   20ec8:	sub	x0, x29, #0x48
   20ecc:	sub	x1, x29, #0x20
   20ed0:	sub	x2, x29, #0x48
   20ed4:	blr	x21
   20ed8:	sub	x0, x29, #0x10
   20edc:	sub	x1, x29, #0x48
   20ee0:	sub	x2, x29, #0x10
   20ee4:	bl	d140 <__gmpz_gcd@plt>
   20ee8:	ldur	w8, [x29, #-12]
   20eec:	cmp	w8, #0x1
   20ef0:	b.ne	20f14 <__gmpq_add@@Base+0x248>  // b.any
   20ef4:	ldur	x8, [x29, #-8]
   20ef8:	ldr	x8, [x8]
   20efc:	cmp	x8, #0x1
   20f00:	b.ne	20f14 <__gmpq_add@@Base+0x248>  // b.any
   20f04:	sub	x1, x29, #0x48
   20f08:	mov	x0, x19
   20f0c:	bl	c590 <__gmpz_set@plt>
   20f10:	b	20f38 <__gmpq_add@@Base+0x26c>
   20f14:	sub	x1, x29, #0x48
   20f18:	sub	x2, x29, #0x10
   20f1c:	mov	x0, x19
   20f20:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   20f24:	sub	x0, x29, #0x20
   20f28:	sub	x2, x29, #0x10
   20f2c:	mov	x1, x20
   20f30:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   20f34:	sub	x20, x29, #0x20
   20f38:	add	x0, x19, #0x10
   20f3c:	sub	x2, x29, #0x30
   20f40:	mov	x1, x20
   20f44:	bl	c620 <__gmpz_mul@plt>
   20f48:	ldur	x0, [x29, #-56]
   20f4c:	cbnz	x0, 20f94 <__gmpq_add@@Base+0x2c8>
   20f50:	mov	sp, x29
   20f54:	ldp	x20, x19, [sp, #80]
   20f58:	ldp	x22, x21, [sp, #64]
   20f5c:	ldp	x24, x23, [sp, #48]
   20f60:	ldp	x26, x25, [sp, #32]
   20f64:	ldp	x28, x27, [sp, #16]
   20f68:	ldp	x29, x30, [sp], #96
   20f6c:	ret
   20f70:	sub	x0, x29, #0x38
   20f74:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20f78:	b	20d6c <__gmpq_add@@Base+0xa0>
   20f7c:	sub	x0, x29, #0x38
   20f80:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20f84:	b	20da4 <__gmpq_add@@Base+0xd8>
   20f88:	sub	x0, x29, #0x38
   20f8c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20f90:	b	20dd4 <__gmpq_add@@Base+0x108>
   20f94:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   20f98:	b	20f50 <__gmpq_add@@Base+0x284>
   20f9c:	sub	x0, x29, #0x38
   20fa0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   20fa4:	b	20e84 <__gmpq_add@@Base+0x1b8>

0000000000020fa8 <__gmpq_sub@@Base>:
   20fa8:	stp	x29, x30, [sp, #-16]!
   20fac:	adrp	x3, 69000 <__gmp_limbroots_table@@Base+0x11338>
   20fb0:	ldr	x3, [x3, #3832]
   20fb4:	mov	x29, sp
   20fb8:	bl	20ce8 <__gmpq_add@@Base+0x1c>
   20fbc:	ldp	x29, x30, [sp], #16
   20fc0:	ret

0000000000020fc4 <__gmpq_canonicalize@@Base>:
   20fc4:	stp	x29, x30, [sp, #-32]!
   20fc8:	stp	x20, x19, [sp, #16]
   20fcc:	mov	x29, sp
   20fd0:	sub	sp, sp, #0x20
   20fd4:	ldr	w8, [x0, #20]
   20fd8:	mov	x19, x0
   20fdc:	tbnz	w8, #31, 20fe8 <__gmpq_canonicalize@@Base+0x24>
   20fe0:	cbnz	w8, 20ffc <__gmpq_canonicalize@@Base+0x38>
   20fe4:	bl	c100 <__gmp_divide_by_zero@plt>
   20fe8:	ldr	w9, [x19, #4]
   20fec:	neg	w8, w8
   20ff0:	str	w8, [x19, #20]
   20ff4:	neg	w9, w9
   20ff8:	str	w9, [x19, #4]
   20ffc:	stur	xzr, [x29, #-24]
   21000:	ldr	w8, [x19, #4]
   21004:	ldr	w9, [x19, #20]
   21008:	add	x20, x19, #0x10
   2100c:	cmp	w8, #0x0
   21010:	cneg	w8, w8, mi  // mi = first
   21014:	cmp	w8, w9
   21018:	csel	w8, w8, w9, gt
   2101c:	add	w9, w8, #0x1
   21020:	cmp	w8, #0xfdf
   21024:	lsl	x1, x9, #3
   21028:	stur	w9, [x29, #-16]
   2102c:	b.hi	210ac <__gmpq_canonicalize@@Base+0xe8>  // b.pmore
   21030:	add	x9, x1, #0xf
   21034:	mov	x8, sp
   21038:	and	x9, x9, #0xffffffff0
   2103c:	sub	x0, x8, x9
   21040:	mov	sp, x0
   21044:	stur	x0, [x29, #-8]
   21048:	sub	x0, x29, #0x10
   2104c:	mov	x1, x19
   21050:	mov	x2, x20
   21054:	bl	d140 <__gmpz_gcd@plt>
   21058:	ldur	w8, [x29, #-12]
   2105c:	cmp	w8, #0x1
   21060:	b.ne	21074 <__gmpq_canonicalize@@Base+0xb0>  // b.any
   21064:	ldur	x8, [x29, #-8]
   21068:	ldr	x8, [x8]
   2106c:	cmp	x8, #0x1
   21070:	b.eq	21094 <__gmpq_canonicalize@@Base+0xd0>  // b.none
   21074:	sub	x2, x29, #0x10
   21078:	mov	x0, x19
   2107c:	mov	x1, x19
   21080:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21084:	sub	x2, x29, #0x10
   21088:	mov	x0, x20
   2108c:	mov	x1, x20
   21090:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21094:	ldur	x0, [x29, #-24]
   21098:	cbnz	x0, 210b8 <__gmpq_canonicalize@@Base+0xf4>
   2109c:	mov	sp, x29
   210a0:	ldp	x20, x19, [sp, #16]
   210a4:	ldp	x29, x30, [sp], #32
   210a8:	ret
   210ac:	sub	x0, x29, #0x18
   210b0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   210b4:	b	21044 <__gmpq_canonicalize@@Base+0x80>
   210b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   210bc:	b	2109c <__gmpq_canonicalize@@Base+0xd8>

00000000000210c0 <__gmpq_clear@@Base>:
   210c0:	stp	x29, x30, [sp, #-32]!
   210c4:	stp	x20, x19, [sp, #16]
   210c8:	adrp	x20, 69000 <__gmp_limbroots_table@@Base+0x11338>
   210cc:	ldrsw	x8, [x0]
   210d0:	ldr	x20, [x20, #4016]
   210d4:	mov	x19, x0
   210d8:	mov	x29, sp
   210dc:	cbz	w8, 210f0 <__gmpq_clear@@Base+0x30>
   210e0:	ldr	x9, [x20]
   210e4:	ldr	x0, [x19, #8]
   210e8:	lsl	x1, x8, #3
   210ec:	blr	x9
   210f0:	ldrsw	x8, [x19, #16]
   210f4:	cbz	w8, 21108 <__gmpq_clear@@Base+0x48>
   210f8:	ldr	x9, [x20]
   210fc:	ldr	x0, [x19, #24]
   21100:	lsl	x1, x8, #3
   21104:	blr	x9
   21108:	ldp	x20, x19, [sp, #16]
   2110c:	ldp	x29, x30, [sp], #32
   21110:	ret

0000000000021114 <__gmpq_clears@@Base>:
   21114:	sub	sp, sp, #0x100
   21118:	stp	x29, x30, [sp, #224]
   2111c:	add	x29, sp, #0xe0
   21120:	mov	x8, #0xffffffffffffffc8    	// #-56
   21124:	mov	x9, sp
   21128:	sub	x10, x29, #0x58
   2112c:	movk	x8, #0xff80, lsl #32
   21130:	add	x11, x29, #0x20
   21134:	add	x9, x9, #0x80
   21138:	add	x10, x10, #0x38
   2113c:	stp	x20, x19, [sp, #240]
   21140:	stp	x1, x2, [x29, #-88]
   21144:	stp	x3, x4, [x29, #-72]
   21148:	stp	x5, x6, [x29, #-56]
   2114c:	stur	x7, [x29, #-40]
   21150:	stp	q0, q1, [sp]
   21154:	stp	q2, q3, [sp, #32]
   21158:	stp	q4, q5, [sp, #64]
   2115c:	stp	q6, q7, [sp, #96]
   21160:	stp	x9, x8, [x29, #-16]
   21164:	stp	x11, x10, [x29, #-32]
   21168:	adrp	x20, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2116c:	ldr	x20, [x20, #4016]
   21170:	mov	x19, x0
   21174:	b	2118c <__gmpq_clears@@Base+0x78>
   21178:	ldur	x8, [x29, #-32]
   2117c:	add	x9, x8, #0x8
   21180:	stur	x9, [x29, #-32]
   21184:	ldr	x19, [x8]
   21188:	cbz	x19, 211e4 <__gmpq_clears@@Base+0xd0>
   2118c:	ldrsw	x8, [x19]
   21190:	cbz	w8, 211a4 <__gmpq_clears@@Base+0x90>
   21194:	ldr	x9, [x20]
   21198:	ldr	x0, [x19, #8]
   2119c:	lsl	x1, x8, #3
   211a0:	blr	x9
   211a4:	ldrsw	x8, [x19, #16]
   211a8:	cbz	w8, 211bc <__gmpq_clears@@Base+0xa8>
   211ac:	ldr	x9, [x20]
   211b0:	ldr	x0, [x19, #24]
   211b4:	lsl	x1, x8, #3
   211b8:	blr	x9
   211bc:	ldursw	x8, [x29, #-8]
   211c0:	tbz	w8, #31, 21178 <__gmpq_clears@@Base+0x64>
   211c4:	add	w9, w8, #0x8
   211c8:	cmp	w9, #0x0
   211cc:	stur	w9, [x29, #-8]
   211d0:	b.gt	21178 <__gmpq_clears@@Base+0x64>
   211d4:	ldur	x9, [x29, #-24]
   211d8:	add	x8, x9, x8
   211dc:	ldr	x19, [x8]
   211e0:	cbnz	x19, 2118c <__gmpq_clears@@Base+0x78>
   211e4:	ldp	x20, x19, [sp, #240]
   211e8:	ldp	x29, x30, [sp, #224]
   211ec:	add	sp, sp, #0x100
   211f0:	ret

00000000000211f4 <__gmpq_cmp@@Base>:
   211f4:	stp	x29, x30, [sp, #-16]!
   211f8:	add	x2, x1, #0x10
   211fc:	mov	x29, sp
   21200:	bl	2120c <__gmpq_cmp@@Base+0x18>
   21204:	ldp	x29, x30, [sp], #16
   21208:	ret
   2120c:	stp	x29, x30, [sp, #-96]!
   21210:	str	x27, [sp, #16]
   21214:	stp	x26, x25, [sp, #32]
   21218:	stp	x24, x23, [sp, #48]
   2121c:	stp	x22, x21, [sp, #64]
   21220:	stp	x20, x19, [sp, #80]
   21224:	mov	x29, sp
   21228:	sub	sp, sp, #0x10
   2122c:	ldr	w26, [x0, #4]
   21230:	ldrsw	x10, [x1, #4]
   21234:	cbz	w26, 212b4 <__gmpq_cmp@@Base+0xc0>
   21238:	cbz	w10, 212bc <__gmpq_cmp@@Base+0xc8>
   2123c:	eor	w8, w10, w26
   21240:	tbnz	w8, #31, 212bc <__gmpq_cmp@@Base+0xc8>
   21244:	ldrsw	x8, [x2, #4]
   21248:	ldr	x9, [x2, #8]
   2124c:	ldrsw	x21, [x0, #20]
   21250:	ldr	x12, [x0, #24]
   21254:	sxtw	x14, w26
   21258:	add	x9, x9, x8, lsl #3
   2125c:	ldur	x11, [x9, #-8]
   21260:	add	x9, x12, x21, lsl #3
   21264:	ldur	x13, [x9, #-8]
   21268:	cmp	x14, #0x0
   2126c:	orr	x9, x11, x8
   21270:	cneg	x4, x14, mi  // mi = first
   21274:	cmp	x9, #0x1
   21278:	cset	w12, eq  // eq = none
   2127c:	orr	x15, x13, x21
   21280:	mov	x19, x1
   21284:	mov	x20, x0
   21288:	cmp	x15, x12
   2128c:	b.ne	212c4 <__gmpq_cmp@@Base+0xd0>  // b.any
   21290:	cmp	w26, w10
   21294:	b.ne	21398 <__gmpq_cmp@@Base+0x1a4>  // b.any
   21298:	ldr	x0, [x20, #8]
   2129c:	ldr	x1, [x19, #8]
   212a0:	mov	x2, x4
   212a4:	bl	c570 <__gmpn_cmp@plt>
   212a8:	cmp	w26, #0x0
   212ac:	cneg	w0, w0, le
   212b0:	b	21478 <__gmpq_cmp@@Base+0x284>
   212b4:	neg	w0, w10
   212b8:	b	21478 <__gmpq_cmp@@Base+0x284>
   212bc:	mov	w0, w26
   212c0:	b	21478 <__gmpq_cmp@@Base+0x284>
   212c4:	cmp	x10, #0x0
   212c8:	cneg	x22, x10, mi  // mi = first
   212cc:	add	x27, x22, x21
   212d0:	add	x25, x4, x8
   212d4:	add	x10, x27, #0x1
   212d8:	cmp	x25, x10
   212dc:	mov	w0, w26
   212e0:	b.gt	21478 <__gmpq_cmp@@Base+0x284>
   212e4:	add	x10, x27, x12
   212e8:	add	x15, x25, #0x1
   212ec:	cmp	x10, x15
   212f0:	neg	x0, x14
   212f4:	b.gt	21478 <__gmpq_cmp@@Base+0x284>
   212f8:	ldr	x10, [x20, #8]
   212fc:	ldr	x14, [x19, #8]
   21300:	lsl	x15, x25, #6
   21304:	clz	x13, x13
   21308:	add	x10, x10, x4, lsl #3
   2130c:	ldur	x10, [x10, #-8]
   21310:	add	x14, x14, x22, lsl #3
   21314:	ldur	x14, [x14, #-8]
   21318:	clz	x11, x11
   2131c:	clz	x10, x10
   21320:	sub	x10, x15, x10
   21324:	lsl	x15, x27, #6
   21328:	clz	x14, x14
   2132c:	sub	x14, x15, x14
   21330:	sub	x13, x14, x13
   21334:	sub	x10, x10, x11
   21338:	add	x11, x13, #0x1
   2133c:	add	x12, x13, x12
   21340:	add	x13, x10, #0x1
   21344:	cmp	x10, x11
   21348:	csel	w0, w26, w0, hi  // hi = pmore
   2134c:	cmp	x12, x13
   21350:	b.hi	21478 <__gmpq_cmp@@Base+0x284>  // b.pmore
   21354:	cmp	x10, x11
   21358:	b.hi	21478 <__gmpq_cmp@@Base+0x284>  // b.pmore
   2135c:	cmp	x9, #0x1
   21360:	str	xzr, [x29, #24]
   21364:	b.ne	213a0 <__gmpq_cmp@@Base+0x1ac>  // b.any
   21368:	lsl	x1, x27, #3
   2136c:	mov	w8, #0x7f00                	// #32512
   21370:	cmp	x1, x8
   21374:	b.hi	214a0 <__gmpq_cmp@@Base+0x2ac>  // b.pmore
   21378:	add	x9, x1, #0xf
   2137c:	mov	x8, sp
   21380:	and	x9, x9, #0xfffffffffffffff0
   21384:	sub	x24, x8, x9
   21388:	mov	sp, x24
   2138c:	ldr	x23, [x20, #8]
   21390:	mov	x8, #0xffffffffffffffff    	// #-1
   21394:	b	21408 <__gmpq_cmp@@Base+0x214>
   21398:	sub	w0, w26, w10
   2139c:	b	21478 <__gmpq_cmp@@Base+0x284>
   213a0:	add	x9, x27, x25
   213a4:	lsl	x1, x9, #3
   213a8:	mov	w9, #0x7f00                	// #32512
   213ac:	cmp	x1, x9
   213b0:	b.hi	214b0 <__gmpq_cmp@@Base+0x2bc>  // b.pmore
   213b4:	add	x10, x1, #0xf
   213b8:	mov	x9, sp
   213bc:	and	x10, x10, #0xfffffffffffffff0
   213c0:	sub	x23, x9, x10
   213c4:	mov	sp, x23
   213c8:	cmp	x4, x8
   213cc:	add	x24, x23, x25, lsl #3
   213d0:	b.ge	213e8 <__gmpq_cmp@@Base+0x1f4>  // b.tcont
   213d4:	ldr	x1, [x2, #8]
   213d8:	ldr	x3, [x20, #8]
   213dc:	mov	x0, x23
   213e0:	mov	x2, x8
   213e4:	b	213fc <__gmpq_cmp@@Base+0x208>
   213e8:	ldr	x1, [x20, #8]
   213ec:	ldr	x3, [x2, #8]
   213f0:	mov	x0, x23
   213f4:	mov	x2, x4
   213f8:	mov	x4, x8
   213fc:	bl	cea0 <__gmpn_mul@plt>
   21400:	cmp	x0, #0x0
   21404:	csetm	x8, eq  // eq = none
   21408:	cmp	x22, x21
   2140c:	add	x25, x25, x8
   21410:	b.ge	2142c <__gmpq_cmp@@Base+0x238>  // b.tcont
   21414:	ldr	x1, [x20, #24]
   21418:	ldr	x3, [x19, #8]
   2141c:	mov	x0, x24
   21420:	mov	x2, x21
   21424:	mov	x4, x22
   21428:	b	21440 <__gmpq_cmp@@Base+0x24c>
   2142c:	ldr	x1, [x19, #8]
   21430:	ldr	x3, [x20, #24]
   21434:	mov	x0, x24
   21438:	mov	x2, x22
   2143c:	mov	x4, x21
   21440:	bl	cea0 <__gmpn_mul@plt>
   21444:	sub	x8, x25, x27
   21448:	cmp	x0, #0x0
   2144c:	cinc	x19, x8, eq  // eq = none
   21450:	cbnz	x19, 21468 <__gmpq_cmp@@Base+0x274>
   21454:	mov	x0, x23
   21458:	mov	x1, x24
   2145c:	mov	x2, x25
   21460:	bl	c570 <__gmpn_cmp@plt>
   21464:	sxtw	x19, w0
   21468:	ldr	x0, [x29, #24]
   2146c:	cbnz	x0, 21498 <__gmpq_cmp@@Base+0x2a4>
   21470:	cmp	w26, #0x0
   21474:	cneg	w0, w19, lt  // lt = tstop
   21478:	mov	sp, x29
   2147c:	ldp	x20, x19, [sp, #80]
   21480:	ldp	x22, x21, [sp, #64]
   21484:	ldp	x24, x23, [sp, #48]
   21488:	ldp	x26, x25, [sp, #32]
   2148c:	ldr	x27, [sp, #16]
   21490:	ldp	x29, x30, [sp], #96
   21494:	ret
   21498:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2149c:	b	21470 <__gmpq_cmp@@Base+0x27c>
   214a0:	add	x0, x29, #0x18
   214a4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   214a8:	mov	x24, x0
   214ac:	b	2138c <__gmpq_cmp@@Base+0x198>
   214b0:	add	x0, x29, #0x18
   214b4:	stur	x2, [x29, #-8]
   214b8:	mov	x23, x4
   214bc:	mov	x24, x8
   214c0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   214c4:	ldur	x2, [x29, #-8]
   214c8:	mov	x8, x24
   214cc:	mov	x4, x23
   214d0:	mov	x23, x0
   214d4:	b	213c8 <__gmpq_cmp@@Base+0x1d4>

00000000000214d8 <__gmpq_cmp_z@@Base>:
   214d8:	stp	x29, x30, [sp, #-16]!
   214dc:	adrp	x2, 69000 <__gmp_limbroots_table@@Base+0x11338>
   214e0:	add	x2, x2, #0x9e0
   214e4:	mov	x29, sp
   214e8:	bl	2120c <__gmpq_cmp@@Base+0x18>
   214ec:	ldp	x29, x30, [sp], #16
   214f0:	ret

00000000000214f4 <__gmpq_cmp_si@@Base>:
   214f4:	sub	sp, sp, #0x30
   214f8:	stp	x29, x30, [sp, #32]
   214fc:	add	x29, sp, #0x20
   21500:	tbnz	x1, #63, 2150c <__gmpq_cmp_si@@Base+0x18>
   21504:	bl	c040 <__gmpq_cmp_ui@plt>
   21508:	b	2154c <__gmpq_cmp_si@@Base+0x58>
   2150c:	ldr	w8, [x0, #4]
   21510:	tbnz	w8, #31, 2151c <__gmpq_cmp_si@@Base+0x28>
   21514:	mov	w0, #0x1                   	// #1
   21518:	b	2154c <__gmpq_cmp_si@@Base+0x58>
   2151c:	neg	w8, w8
   21520:	str	w8, [sp, #4]
   21524:	ldr	x8, [x0, #8]
   21528:	neg	x1, x1
   2152c:	str	x8, [sp, #8]
   21530:	ldr	w8, [x0, #20]
   21534:	str	w8, [sp, #20]
   21538:	ldr	x8, [x0, #24]
   2153c:	mov	x0, sp
   21540:	str	x8, [sp, #24]
   21544:	bl	c040 <__gmpq_cmp_ui@plt>
   21548:	neg	w0, w0
   2154c:	ldp	x29, x30, [sp, #32]
   21550:	add	sp, sp, #0x30
   21554:	ret

0000000000021558 <__gmpq_cmp_ui@@Base>:
   21558:	stp	x29, x30, [sp, #-64]!
   2155c:	stp	x24, x23, [sp, #16]
   21560:	stp	x22, x21, [sp, #32]
   21564:	stp	x20, x19, [sp, #48]
   21568:	mov	x29, sp
   2156c:	sub	sp, sp, #0x10
   21570:	cbz	x2, 216a8 <__gmpq_cmp_ui@@Base+0x150>
   21574:	mov	x21, x0
   21578:	ldr	w0, [x0, #4]
   2157c:	mov	x19, x1
   21580:	cbz	x1, 21664 <__gmpq_cmp_ui@@Base+0x10c>
   21584:	cmp	w0, #0x1
   21588:	b.lt	215c0 <__gmpq_cmp_ui@@Base+0x68>  // b.tstop
   2158c:	ldrsw	x20, [x21, #20]
   21590:	cmp	x19, x2
   21594:	sxtw	x22, w0
   21598:	mov	x3, x2
   2159c:	cinc	x8, x20, hi  // hi = pmore
   215a0:	cmp	x8, x22
   215a4:	b.lt	21664 <__gmpq_cmp_ui@@Base+0x10c>  // b.tstop
   215a8:	cmp	x3, x19
   215ac:	cinc	x8, x22, hi  // hi = pmore
   215b0:	cmp	x8, x20
   215b4:	b.ge	215c8 <__gmpq_cmp_ui@@Base+0x70>  // b.tcont
   215b8:	neg	w0, w0
   215bc:	b	21664 <__gmpq_cmp_ui@@Base+0x10c>
   215c0:	mov	w0, #0xffffffff            	// #-1
   215c4:	b	21664 <__gmpq_cmp_ui@@Base+0x10c>
   215c8:	add	x24, x22, #0x1
   215cc:	add	x8, x20, x24
   215d0:	lsl	x8, x8, #3
   215d4:	add	x1, x8, #0x8
   215d8:	mov	w8, #0x7f00                	// #32512
   215dc:	cmp	x1, x8
   215e0:	stur	xzr, [x29, #-8]
   215e4:	b.hi	2167c <__gmpq_cmp_ui@@Base+0x124>  // b.pmore
   215e8:	add	x9, x1, #0xf
   215ec:	mov	x8, sp
   215f0:	and	x9, x9, #0xfffffffffffffff0
   215f4:	sub	x23, x8, x9
   215f8:	mov	sp, x23
   215fc:	ldr	x1, [x21, #8]
   21600:	mov	x0, x23
   21604:	mov	x2, x22
   21608:	add	x24, x23, x24, lsl #3
   2160c:	bl	d670 <__gmpn_mul_1@plt>
   21610:	str	x0, [x23, x22, lsl #3]
   21614:	ldr	x1, [x21, #24]
   21618:	cmp	x0, #0x0
   2161c:	mov	x0, x24
   21620:	mov	x2, x20
   21624:	mov	x3, x19
   21628:	cinc	x21, x22, ne  // ne = any
   2162c:	bl	d670 <__gmpn_mul_1@plt>
   21630:	cmp	x0, #0x0
   21634:	cset	w9, ne  // ne = any
   21638:	sub	w10, w21, w20
   2163c:	mov	x8, x0
   21640:	subs	w0, w10, w9
   21644:	str	x8, [x24, x20, lsl #3]
   21648:	b.ne	2165c <__gmpq_cmp_ui@@Base+0x104>  // b.any
   2164c:	mov	x0, x23
   21650:	mov	x1, x24
   21654:	mov	x2, x21
   21658:	bl	c570 <__gmpn_cmp@plt>
   2165c:	ldur	x8, [x29, #-8]
   21660:	cbnz	x8, 21694 <__gmpq_cmp_ui@@Base+0x13c>
   21664:	mov	sp, x29
   21668:	ldp	x20, x19, [sp, #48]
   2166c:	ldp	x22, x21, [sp, #32]
   21670:	ldp	x24, x23, [sp, #16]
   21674:	ldp	x29, x30, [sp], #64
   21678:	ret
   2167c:	sub	x0, x29, #0x8
   21680:	mov	x23, x3
   21684:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21688:	mov	x3, x23
   2168c:	mov	x23, x0
   21690:	b	215fc <__gmpq_cmp_ui@@Base+0xa4>
   21694:	mov	x19, x0
   21698:	mov	x0, x8
   2169c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   216a0:	mov	x0, x19
   216a4:	b	21664 <__gmpq_cmp_ui@@Base+0x10c>
   216a8:	bl	c100 <__gmp_divide_by_zero@plt>

00000000000216ac <__gmpq_div@@Base>:
   216ac:	stp	x29, x30, [sp, #-80]!
   216b0:	str	x25, [sp, #16]
   216b4:	stp	x24, x23, [sp, #32]
   216b8:	stp	x22, x21, [sp, #48]
   216bc:	stp	x20, x19, [sp, #64]
   216c0:	mov	x29, sp
   216c4:	sub	sp, sp, #0x40
   216c8:	ldr	w8, [x2, #4]
   216cc:	cbz	w8, 21980 <__gmpq_div@@Base+0x2d4>
   216d0:	mov	x21, x2
   216d4:	mov	x20, x1
   216d8:	mov	x19, x0
   216dc:	cmp	x0, x2
   216e0:	b.eq	218d4 <__gmpq_div@@Base+0x228>  // b.none
   216e4:	ldr	w9, [x20, #4]
   216e8:	cmp	w9, #0x0
   216ec:	cneg	w22, w9, mi  // mi = first
   216f0:	cbz	w22, 21874 <__gmpq_div@@Base+0x1c8>
   216f4:	sxtw	x8, w8
   216f8:	cmp	x8, #0x0
   216fc:	cneg	x23, x8, mi  // mi = first
   21700:	cmp	x23, x22
   21704:	csel	x8, x22, x23, gt
   21708:	cmp	x8, #0xfe0
   2170c:	lsl	x1, x8, #3
   21710:	str	xzr, [x29, #24]
   21714:	stur	w8, [x29, #-16]
   21718:	b.hi	21928 <__gmpq_div@@Base+0x27c>  // b.pmore
   2171c:	add	x9, x1, #0xf
   21720:	mov	x8, sp
   21724:	and	x9, x9, #0xfffffffffffffff0
   21728:	sub	x0, x8, x9
   2172c:	mov	sp, x0
   21730:	cmp	x23, x22
   21734:	csel	x8, x22, x23, lt  // lt = tstop
   21738:	lsl	x1, x8, #3
   2173c:	mov	w9, #0x7f00                	// #32512
   21740:	cmp	x1, x9
   21744:	stur	x0, [x29, #-8]
   21748:	stur	w8, [x29, #-48]
   2174c:	b.hi	21934 <__gmpq_div@@Base+0x288>  // b.pmore
   21750:	add	x9, x1, #0xf
   21754:	mov	x8, sp
   21758:	and	x9, x9, #0xfffffffffffffff0
   2175c:	sub	x0, x8, x9
   21760:	mov	sp, x0
   21764:	stur	x0, [x29, #-40]
   21768:	ldrsw	x24, [x21, #20]
   2176c:	ldrsw	x25, [x20, #20]
   21770:	mov	w9, #0x7f00                	// #32512
   21774:	add	x23, x21, #0x10
   21778:	add	x22, x20, #0x10
   2177c:	cmp	x25, x24
   21780:	csel	x8, x25, x24, lt  // lt = tstop
   21784:	lsl	x1, x8, #3
   21788:	cmp	x1, x9
   2178c:	stur	w8, [x29, #-32]
   21790:	b.hi	21940 <__gmpq_div@@Base+0x294>  // b.pmore
   21794:	add	x9, x1, #0xf
   21798:	mov	x8, sp
   2179c:	and	x9, x9, #0xfffffffffffffff0
   217a0:	sub	x0, x8, x9
   217a4:	mov	sp, x0
   217a8:	cmp	x25, x24
   217ac:	csel	x8, x25, x24, gt
   217b0:	lsl	x1, x8, #3
   217b4:	mov	w9, #0x7f00                	// #32512
   217b8:	cmp	x1, x9
   217bc:	stur	x0, [x29, #-24]
   217c0:	stur	w8, [x29, #-64]
   217c4:	b.hi	2194c <__gmpq_div@@Base+0x2a0>  // b.pmore
   217c8:	add	x9, x1, #0xf
   217cc:	mov	x8, sp
   217d0:	and	x9, x9, #0xfffffffffffffff0
   217d4:	sub	x0, x8, x9
   217d8:	mov	sp, x0
   217dc:	stur	x0, [x29, #-56]
   217e0:	sub	x0, x29, #0x10
   217e4:	mov	x1, x20
   217e8:	mov	x2, x21
   217ec:	bl	d140 <__gmpz_gcd@plt>
   217f0:	sub	x0, x29, #0x20
   217f4:	mov	x1, x23
   217f8:	mov	x2, x22
   217fc:	bl	d140 <__gmpz_gcd@plt>
   21800:	sub	x0, x29, #0x30
   21804:	sub	x2, x29, #0x10
   21808:	mov	x1, x20
   2180c:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21810:	sub	x0, x29, #0x40
   21814:	sub	x2, x29, #0x20
   21818:	mov	x1, x23
   2181c:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21820:	sub	x1, x29, #0x30
   21824:	sub	x2, x29, #0x40
   21828:	mov	x0, x19
   2182c:	bl	c620 <__gmpz_mul@plt>
   21830:	sub	x0, x29, #0x30
   21834:	sub	x2, x29, #0x10
   21838:	mov	x1, x21
   2183c:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21840:	sub	x0, x29, #0x40
   21844:	sub	x2, x29, #0x20
   21848:	mov	x1, x22
   2184c:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   21850:	add	x0, x19, #0x10
   21854:	sub	x1, x29, #0x30
   21858:	sub	x2, x29, #0x40
   2185c:	bl	c620 <__gmpz_mul@plt>
   21860:	ldr	w8, [x19, #20]
   21864:	tbnz	w8, #31, 2189c <__gmpq_div@@Base+0x1f0>
   21868:	ldr	x0, [x29, #24]
   2186c:	cbz	x0, 218b8 <__gmpq_div@@Base+0x20c>
   21870:	b	21958 <__gmpq_div@@Base+0x2ac>
   21874:	mov	x0, x19
   21878:	ldr	w8, [x0, #16]!
   2187c:	cmp	w8, #0x0
   21880:	stur	wzr, [x0, #-12]
   21884:	b.le	21960 <__gmpq_div@@Base+0x2b4>
   21888:	ldr	x0, [x19, #24]
   2188c:	mov	w8, #0x1                   	// #1
   21890:	str	x8, [x0]
   21894:	str	w8, [x19, #20]
   21898:	b	218b8 <__gmpq_div@@Base+0x20c>
   2189c:	ldr	w9, [x19, #4]
   218a0:	neg	w8, w8
   218a4:	str	w8, [x19, #20]
   218a8:	neg	w8, w9
   218ac:	str	w8, [x19, #4]
   218b0:	ldr	x0, [x29, #24]
   218b4:	cbnz	x0, 21958 <__gmpq_div@@Base+0x2ac>
   218b8:	mov	sp, x29
   218bc:	ldp	x20, x19, [sp, #64]
   218c0:	ldp	x22, x21, [sp, #48]
   218c4:	ldp	x24, x23, [sp, #32]
   218c8:	ldr	x25, [sp, #16]
   218cc:	ldp	x29, x30, [sp], #80
   218d0:	ret
   218d4:	cmp	x20, x21
   218d8:	b.eq	2196c <__gmpq_div@@Base+0x2c0>  // b.none
   218dc:	ldr	x10, [x19, #8]
   218e0:	ldr	x11, [x19, #24]
   218e4:	ldr	w12, [x19]
   218e8:	ldp	w13, w9, [x19, #16]
   218ec:	cmp	w8, #0x1
   218f0:	str	x11, [x19, #8]
   218f4:	str	x10, [x19, #24]
   218f8:	str	w13, [x19]
   218fc:	str	w12, [x19, #16]
   21900:	b.ge	2190c <__gmpq_div@@Base+0x260>  // b.tcont
   21904:	neg	w9, w9
   21908:	neg	w8, w8
   2190c:	str	w9, [x19, #4]
   21910:	mov	x0, x19
   21914:	mov	x1, x19
   21918:	mov	x2, x20
   2191c:	str	w8, [x19, #20]
   21920:	bl	d1b0 <__gmpq_mul@plt>
   21924:	b	218b8 <__gmpq_div@@Base+0x20c>
   21928:	add	x0, x29, #0x18
   2192c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21930:	b	21730 <__gmpq_div@@Base+0x84>
   21934:	add	x0, x29, #0x18
   21938:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2193c:	b	21764 <__gmpq_div@@Base+0xb8>
   21940:	add	x0, x29, #0x18
   21944:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21948:	b	217a8 <__gmpq_div@@Base+0xfc>
   2194c:	add	x0, x29, #0x18
   21950:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21954:	b	217dc <__gmpq_div@@Base+0x130>
   21958:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2195c:	b	218b8 <__gmpq_div@@Base+0x20c>
   21960:	mov	w1, #0x1                   	// #1
   21964:	bl	c1c0 <__gmpz_realloc@plt>
   21968:	b	2188c <__gmpq_div@@Base+0x1e0>
   2196c:	mov	w1, #0x1                   	// #1
   21970:	mov	w2, #0x1                   	// #1
   21974:	mov	x0, x19
   21978:	bl	cc20 <__gmpq_set_ui@plt>
   2197c:	b	218b8 <__gmpq_div@@Base+0x20c>
   21980:	bl	c100 <__gmp_divide_by_zero@plt>

0000000000021984 <__gmpq_get_d@@Base>:
   21984:	str	d8, [sp, #-96]!
   21988:	stp	x29, x30, [sp, #8]
   2198c:	str	x27, [sp, #24]
   21990:	stp	x26, x25, [sp, #32]
   21994:	stp	x24, x23, [sp, #48]
   21998:	stp	x22, x21, [sp, #64]
   2199c:	stp	x20, x19, [sp, #80]
   219a0:	mov	x29, sp
   219a4:	sub	sp, sp, #0x20
   219a8:	ldrsw	x19, [x0, #4]
   219ac:	cbz	w19, 21adc <__gmpq_get_d@@Base+0x158>
   219b0:	ldrsw	x8, [x0, #20]
   219b4:	cmp	x19, #0x0
   219b8:	stur	xzr, [x29, #-32]
   219bc:	cneg	x24, x19, mi  // mi = first
   219c0:	cmp	x8, #0x0
   219c4:	ldr	x25, [x0, #8]
   219c8:	ldr	x22, [x0, #24]
   219cc:	cneg	x21, x8, mi  // mi = first
   219d0:	mov	x9, #0xfffffffffffffffe    	// #-2
   219d4:	sub	x27, x21, x24
   219d8:	sub	x8, x9, x27
   219dc:	cmn	x27, #0x1
   219e0:	lsl	x20, x8, #6
   219e4:	b.lt	21a40 <__gmpq_get_d@@Base+0xbc>  // b.tstop
   219e8:	lsl	x8, x21, #3
   219ec:	mov	w9, #0x7ee8                	// #32488
   219f0:	add	x26, x27, #0x2
   219f4:	cmp	x8, x9
   219f8:	add	x1, x8, #0x18
   219fc:	b.hi	21aec <__gmpq_get_d@@Base+0x168>  // b.pmore
   21a00:	add	x9, x1, #0xf
   21a04:	mov	x8, sp
   21a08:	and	x9, x9, #0xfffffffffffffff0
   21a0c:	sub	x23, x8, x9
   21a10:	mov	sp, x23
   21a14:	lsl	x8, x27, #3
   21a18:	add	x2, x8, #0x10
   21a1c:	mov	x0, x23
   21a20:	mov	w1, wzr
   21a24:	bl	c780 <memset@plt>
   21a28:	add	x0, x23, x26, lsl #3
   21a2c:	mov	x1, x25
   21a30:	mov	x2, x24
   21a34:	bl	cc10 <__gmpn_copyi@plt>
   21a38:	mov	x24, x23
   21a3c:	b	21a6c <__gmpq_get_d@@Base+0xe8>
   21a40:	add	x24, x25, x8, lsl #3
   21a44:	lsl	x8, x21, #3
   21a48:	mov	w9, #0x7ee8                	// #32488
   21a4c:	cmp	x8, x9
   21a50:	add	x1, x8, #0x18
   21a54:	b.hi	21afc <__gmpq_get_d@@Base+0x178>  // b.pmore
   21a58:	add	x9, x1, #0xf
   21a5c:	mov	x8, sp
   21a60:	and	x9, x9, #0xfffffffffffffff0
   21a64:	sub	x23, x8, x9
   21a68:	mov	sp, x23
   21a6c:	add	x2, x21, #0x2
   21a70:	sub	x0, x29, #0x18
   21a74:	mov	x1, x24
   21a78:	mov	x3, x22
   21a7c:	mov	x4, x21
   21a80:	mov	x5, x23
   21a84:	bl	c480 <__gmpn_div_q@plt>
   21a88:	ldur	x8, [x29, #-8]
   21a8c:	sub	x0, x29, #0x18
   21a90:	mov	x2, x19
   21a94:	mov	x3, x20
   21a98:	cmp	x8, #0x0
   21a9c:	mov	w8, #0x2                   	// #2
   21aa0:	cinc	x1, x8, ne  // ne = any
   21aa4:	bl	c070 <__gmpn_get_d@plt>
   21aa8:	ldur	x0, [x29, #-32]
   21aac:	mov	v8.16b, v0.16b
   21ab0:	cbnz	x0, 21ae4 <__gmpq_get_d@@Base+0x160>
   21ab4:	mov	v0.16b, v8.16b
   21ab8:	mov	sp, x29
   21abc:	ldp	x20, x19, [sp, #80]
   21ac0:	ldp	x22, x21, [sp, #64]
   21ac4:	ldp	x24, x23, [sp, #48]
   21ac8:	ldp	x26, x25, [sp, #32]
   21acc:	ldr	x27, [sp, #24]
   21ad0:	ldp	x29, x30, [sp, #8]
   21ad4:	ldr	d8, [sp], #96
   21ad8:	ret
   21adc:	fmov	d8, xzr
   21ae0:	b	21ab4 <__gmpq_get_d@@Base+0x130>
   21ae4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   21ae8:	b	21ab4 <__gmpq_get_d@@Base+0x130>
   21aec:	sub	x0, x29, #0x20
   21af0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21af4:	mov	x23, x0
   21af8:	b	21a14 <__gmpq_get_d@@Base+0x90>
   21afc:	sub	x0, x29, #0x20
   21b00:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   21b04:	mov	x23, x0
   21b08:	b	21a6c <__gmpq_get_d@@Base+0xe8>

0000000000021b0c <__gmpq_get_den@@Base>:
   21b0c:	stp	x29, x30, [sp, #-48]!
   21b10:	stp	x22, x21, [sp, #16]
   21b14:	stp	x20, x19, [sp, #32]
   21b18:	ldr	w22, [x1, #20]
   21b1c:	ldr	w8, [x0]
   21b20:	mov	x19, x1
   21b24:	mov	x20, x0
   21b28:	sxtw	x21, w22
   21b2c:	cmp	w22, w8
   21b30:	mov	x29, sp
   21b34:	b.gt	21b5c <__gmpq_get_den@@Base+0x50>
   21b38:	ldr	x0, [x20, #8]
   21b3c:	str	w22, [x20, #4]
   21b40:	ldr	x1, [x19, #24]
   21b44:	mov	x2, x21
   21b48:	bl	cc10 <__gmpn_copyi@plt>
   21b4c:	ldp	x20, x19, [sp, #32]
   21b50:	ldp	x22, x21, [sp, #16]
   21b54:	ldp	x29, x30, [sp], #48
   21b58:	ret
   21b5c:	mov	x0, x20
   21b60:	mov	x1, x21
   21b64:	bl	c1c0 <__gmpz_realloc@plt>
   21b68:	b	21b3c <__gmpq_get_den@@Base+0x30>

0000000000021b6c <__gmpq_get_num@@Base>:
   21b6c:	stp	x29, x30, [sp, #-48]!
   21b70:	stp	x22, x21, [sp, #16]
   21b74:	stp	x20, x19, [sp, #32]
   21b78:	ldrsw	x22, [x1, #4]
   21b7c:	ldrsw	x8, [x0]
   21b80:	mov	x19, x1
   21b84:	mov	x21, x0
   21b88:	cmp	x22, #0x0
   21b8c:	cneg	x20, x22, mi  // mi = first
   21b90:	cmp	x20, x8
   21b94:	mov	x29, sp
   21b98:	b.gt	21bc0 <__gmpq_get_num@@Base+0x54>
   21b9c:	ldr	x0, [x21, #8]
   21ba0:	str	w22, [x21, #4]
   21ba4:	ldr	x1, [x19, #8]
   21ba8:	mov	x2, x20
   21bac:	bl	cc10 <__gmpn_copyi@plt>
   21bb0:	ldp	x20, x19, [sp, #32]
   21bb4:	ldp	x22, x21, [sp, #16]
   21bb8:	ldp	x29, x30, [sp], #48
   21bbc:	ret
   21bc0:	mov	x0, x21
   21bc4:	mov	x1, x20
   21bc8:	bl	c1c0 <__gmpz_realloc@plt>
   21bcc:	b	21ba0 <__gmpq_get_num@@Base+0x34>

0000000000021bd0 <__gmpq_get_str@@Base>:
   21bd0:	stp	x29, x30, [sp, #-64]!
   21bd4:	add	w8, w1, #0x24
   21bd8:	cmp	w8, #0x62
   21bdc:	str	x23, [sp, #16]
   21be0:	stp	x22, x21, [sp, #32]
   21be4:	stp	x20, x19, [sp, #48]
   21be8:	mov	x29, sp
   21bec:	b.ls	21bf8 <__gmpq_get_str@@Base+0x28>  // b.plast
   21bf0:	mov	x19, xzr
   21bf4:	b	21d04 <__gmpq_get_str@@Base+0x134>
   21bf8:	mov	x21, x2
   21bfc:	mov	w20, w1
   21c00:	mov	x19, x0
   21c04:	cbz	x0, 21c10 <__gmpq_get_str@@Base+0x40>
   21c08:	mov	x22, xzr
   21c0c:	b	21c78 <__gmpq_get_str@@Base+0xa8>
   21c10:	cmp	w20, #0x0
   21c14:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   21c18:	cneg	w11, w20, mi  // mi = first
   21c1c:	mov	w8, #0xa                   	// #10
   21c20:	ldr	x9, [x9, #3936]
   21c24:	cmp	w11, #0x2
   21c28:	csel	w20, w8, w20, lt  // lt = tstop
   21c2c:	ldr	w11, [x21, #4]
   21c30:	cmp	w20, #0x0
   21c34:	mov	w10, #0x28                  	// #40
   21c38:	cneg	w8, w20, mi  // mi = first
   21c3c:	umaddl	x8, w8, w10, x9
   21c40:	ldr	w9, [x21, #20]
   21c44:	cmp	w11, #0x0
   21c48:	cneg	w10, w11, mi  // mi = first
   21c4c:	ldr	x8, [x8, #8]
   21c50:	add	w9, w10, w9
   21c54:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   21c58:	ldr	x10, [x10, #3840]
   21c5c:	sbfiz	x9, x9, #6, #32
   21c60:	umulh	x8, x8, x9
   21c64:	add	x22, x8, #0x6
   21c68:	ldr	x10, [x10]
   21c6c:	mov	x0, x22
   21c70:	blr	x10
   21c74:	mov	x19, x0
   21c78:	mov	x0, x19
   21c7c:	mov	w1, w20
   21c80:	mov	x2, x21
   21c84:	bl	c500 <__gmpz_get_str@plt>
   21c88:	mov	x0, x19
   21c8c:	bl	c090 <strlen@plt>
   21c90:	ldr	w8, [x21, #20]
   21c94:	cmp	w8, #0x1
   21c98:	b.ne	21cac <__gmpq_get_str@@Base+0xdc>  // b.any
   21c9c:	ldr	x8, [x21, #24]
   21ca0:	ldr	x8, [x8]
   21ca4:	cmp	x8, #0x1
   21ca8:	b.eq	21cd8 <__gmpq_get_str@@Base+0x108>  // b.none
   21cac:	add	x23, x0, #0x1
   21cb0:	add	x2, x21, #0x10
   21cb4:	mov	w8, #0x2f                  	// #47
   21cb8:	add	x21, x19, x23
   21cbc:	strb	w8, [x19, x0]
   21cc0:	mov	x0, x21
   21cc4:	mov	w1, w20
   21cc8:	bl	c500 <__gmpz_get_str@plt>
   21ccc:	mov	x0, x21
   21cd0:	bl	c090 <strlen@plt>
   21cd4:	add	x0, x0, x23
   21cd8:	cbz	x22, 21d04 <__gmpq_get_str@@Base+0x134>
   21cdc:	add	x2, x0, #0x1
   21ce0:	cmp	x22, x2
   21ce4:	b.eq	21d04 <__gmpq_get_str@@Base+0x134>  // b.none
   21ce8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   21cec:	ldr	x8, [x8, #3792]
   21cf0:	mov	x0, x19
   21cf4:	mov	x1, x22
   21cf8:	ldr	x8, [x8]
   21cfc:	blr	x8
   21d00:	mov	x19, x0
   21d04:	mov	x0, x19
   21d08:	ldp	x20, x19, [sp, #48]
   21d0c:	ldp	x22, x21, [sp, #32]
   21d10:	ldr	x23, [sp, #16]
   21d14:	ldp	x29, x30, [sp], #64
   21d18:	ret

0000000000021d1c <__gmpq_init@@Base>:
   21d1c:	stp	x29, x30, [sp, #-32]!
   21d20:	adrp	x8, 4c000 <__gmp_randclear_mt@@Base+0x18>
   21d24:	stp	x20, x19, [sp, #16]
   21d28:	add	x8, x8, #0xd68
   21d2c:	mov	w20, #0x1                   	// #1
   21d30:	stp	xzr, x8, [x0]
   21d34:	str	w20, [x0, #16]
   21d38:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   21d3c:	ldr	x8, [x8, #3840]
   21d40:	mov	x19, x0
   21d44:	mov	w0, #0x8                   	// #8
   21d48:	mov	x29, sp
   21d4c:	ldr	x8, [x8]
   21d50:	blr	x8
   21d54:	str	x0, [x19, #24]
   21d58:	str	x20, [x0]
   21d5c:	str	w20, [x19, #20]
   21d60:	ldp	x20, x19, [sp, #16]
   21d64:	ldp	x29, x30, [sp], #32
   21d68:	ret

0000000000021d6c <__gmpq_inits@@Base>:
   21d6c:	sub	sp, sp, #0xf0
   21d70:	stp	x29, x30, [sp, #224]
   21d74:	add	x29, sp, #0xe0
   21d78:	mov	x8, #0xffffffffffffffc8    	// #-56
   21d7c:	mov	x9, sp
   21d80:	sub	x10, x29, #0x58
   21d84:	movk	x8, #0xff80, lsl #32
   21d88:	add	x11, x29, #0x10
   21d8c:	add	x9, x9, #0x80
   21d90:	add	x10, x10, #0x38
   21d94:	stp	x1, x2, [x29, #-88]
   21d98:	stp	x3, x4, [x29, #-72]
   21d9c:	stp	x5, x6, [x29, #-56]
   21da0:	stur	x7, [x29, #-40]
   21da4:	stp	q0, q1, [sp]
   21da8:	stp	q2, q3, [sp, #32]
   21dac:	stp	q4, q5, [sp, #64]
   21db0:	stp	q6, q7, [sp, #96]
   21db4:	stp	x9, x8, [x29, #-16]
   21db8:	stp	x11, x10, [x29, #-32]
   21dbc:	b	21dd4 <__gmpq_inits@@Base+0x68>
   21dc0:	ldur	x8, [x29, #-32]
   21dc4:	add	x9, x8, #0x8
   21dc8:	stur	x9, [x29, #-32]
   21dcc:	ldr	x0, [x8]
   21dd0:	cbz	x0, 21e00 <__gmpq_inits@@Base+0x94>
   21dd4:	bl	cfa0 <__gmpq_init@plt>
   21dd8:	ldursw	x8, [x29, #-8]
   21ddc:	tbz	w8, #31, 21dc0 <__gmpq_inits@@Base+0x54>
   21de0:	add	w9, w8, #0x8
   21de4:	cmp	w9, #0x0
   21de8:	stur	w9, [x29, #-8]
   21dec:	b.gt	21dc0 <__gmpq_inits@@Base+0x54>
   21df0:	ldur	x9, [x29, #-24]
   21df4:	add	x8, x9, x8
   21df8:	ldr	x0, [x8]
   21dfc:	cbnz	x0, 21dd4 <__gmpq_inits@@Base+0x68>
   21e00:	ldp	x29, x30, [sp, #224]
   21e04:	add	sp, sp, #0xf0
   21e08:	ret

0000000000021e0c <__gmpq_inp_str@@Base>:
   21e0c:	stp	x29, x30, [sp, #-64]!
   21e10:	str	x23, [sp, #16]
   21e14:	stp	x22, x21, [sp, #32]
   21e18:	stp	x20, x19, [sp, #48]
   21e1c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   21e20:	ldr	x8, [x8, #3888]
   21e24:	mov	x22, x0
   21e28:	cmp	x1, #0x0
   21e2c:	mov	w20, w2
   21e30:	ldr	x8, [x8]
   21e34:	ldr	w9, [x22, #16]!
   21e38:	mov	x19, x0
   21e3c:	mov	w10, #0x1                   	// #1
   21e40:	csel	x21, x8, x1, eq  // eq = none
   21e44:	cmp	w9, #0x0
   21e48:	mov	x29, sp
   21e4c:	str	w10, [x22, #4]
   21e50:	b.le	21ee8 <__gmpq_inp_str@@Base+0xdc>
   21e54:	ldr	x0, [x19, #24]
   21e58:	mov	w8, #0x1                   	// #1
   21e5c:	str	x8, [x0]
   21e60:	mov	x0, x19
   21e64:	mov	x1, x21
   21e68:	mov	w2, w20
   21e6c:	bl	d2a0 <__gmpz_inp_str@plt>
   21e70:	mov	x23, x0
   21e74:	cbz	x0, 21ed0 <__gmpq_inp_str@@Base+0xc4>
   21e78:	mov	x0, x21
   21e7c:	bl	c9a0 <getc@plt>
   21e80:	cmp	w0, #0x2f
   21e84:	b.ne	21ec8 <__gmpq_inp_str@@Base+0xbc>  // b.any
   21e88:	mov	x0, x21
   21e8c:	bl	c9a0 <getc@plt>
   21e90:	mov	w3, w0
   21e94:	add	x4, x23, #0x2
   21e98:	mov	x0, x22
   21e9c:	mov	x1, x21
   21ea0:	mov	w2, w20
   21ea4:	bl	cb90 <__gmpz_inp_str_nowhite@plt>
   21ea8:	mov	x23, x0
   21eac:	cbnz	x0, 21ed0 <__gmpq_inp_str@@Base+0xc4>
   21eb0:	ldr	x8, [x19, #24]
   21eb4:	mov	w9, #0x1                   	// #1
   21eb8:	str	wzr, [x19, #4]
   21ebc:	str	w9, [x19, #20]
   21ec0:	str	x9, [x8]
   21ec4:	b	21ed0 <__gmpq_inp_str@@Base+0xc4>
   21ec8:	mov	x1, x21
   21ecc:	bl	ce10 <ungetc@plt>
   21ed0:	mov	x0, x23
   21ed4:	ldp	x20, x19, [sp, #48]
   21ed8:	ldp	x22, x21, [sp, #32]
   21edc:	ldr	x23, [sp, #16]
   21ee0:	ldp	x29, x30, [sp], #64
   21ee4:	ret
   21ee8:	mov	w1, #0x1                   	// #1
   21eec:	mov	x0, x22
   21ef0:	bl	c1c0 <__gmpz_realloc@plt>
   21ef4:	b	21e58 <__gmpq_inp_str@@Base+0x4c>

0000000000021ef8 <__gmpq_inv@@Base>:
   21ef8:	stp	x29, x30, [sp, #-64]!
   21efc:	stp	x22, x21, [sp, #32]
   21f00:	stp	x20, x19, [sp, #48]
   21f04:	ldrsw	x19, [x1, #4]
   21f08:	ldrsw	x8, [x1, #20]
   21f0c:	mov	x20, x1
   21f10:	mov	x21, x0
   21f14:	str	x23, [sp, #16]
   21f18:	mov	x29, sp
   21f1c:	tbnz	w19, #31, 21f28 <__gmpq_inv@@Base+0x30>
   21f20:	cbnz	w19, 21f30 <__gmpq_inv@@Base+0x38>
   21f24:	bl	c100 <__gmp_divide_by_zero@plt>
   21f28:	neg	x19, x19
   21f2c:	neg	x8, x8
   21f30:	cmp	x21, x20
   21f34:	str	w19, [x21, #20]
   21f38:	str	w8, [x21, #4]
   21f3c:	b.eq	21f88 <__gmpq_inv@@Base+0x90>  // b.none
   21f40:	ldrsw	x9, [x21]
   21f44:	cmp	x8, #0x0
   21f48:	cneg	x23, x8, mi  // mi = first
   21f4c:	add	x22, x21, #0x10
   21f50:	cmp	x23, x9
   21f54:	b.gt	21fbc <__gmpq_inv@@Base+0xc4>
   21f58:	ldr	x0, [x21, #8]
   21f5c:	ldr	x1, [x20, #24]
   21f60:	mov	x2, x23
   21f64:	bl	cc10 <__gmpn_copyi@plt>
   21f68:	ldrsw	x8, [x22]
   21f6c:	cmp	x19, x8
   21f70:	b.gt	21fcc <__gmpq_inv@@Base+0xd4>
   21f74:	ldr	x0, [x21, #24]
   21f78:	ldr	x1, [x20, #8]
   21f7c:	mov	x2, x19
   21f80:	bl	cc10 <__gmpn_copyi@plt>
   21f84:	b	21fa8 <__gmpq_inv@@Base+0xb0>
   21f88:	ldr	x8, [x21, #24]
   21f8c:	ldr	x9, [x21, #8]
   21f90:	ldr	w10, [x21, #16]
   21f94:	ldr	w11, [x21]
   21f98:	str	x8, [x21, #8]
   21f9c:	str	x9, [x21, #24]
   21fa0:	str	w10, [x21]
   21fa4:	str	w11, [x21, #16]
   21fa8:	ldp	x20, x19, [sp, #48]
   21fac:	ldp	x22, x21, [sp, #32]
   21fb0:	ldr	x23, [sp, #16]
   21fb4:	ldp	x29, x30, [sp], #64
   21fb8:	ret
   21fbc:	mov	x0, x21
   21fc0:	mov	x1, x23
   21fc4:	bl	c1c0 <__gmpz_realloc@plt>
   21fc8:	b	21f5c <__gmpq_inv@@Base+0x64>
   21fcc:	mov	x0, x22
   21fd0:	mov	x1, x19
   21fd4:	bl	c1c0 <__gmpz_realloc@plt>
   21fd8:	b	21f78 <__gmpq_inv@@Base+0x80>

0000000000021fdc <__gmpq_mul_2exp@@Base>:
   21fdc:	stp	x29, x30, [sp, #-16]!
   21fe0:	mov	x4, x2
   21fe4:	mov	x2, x1
   21fe8:	add	x1, x0, #0x10
   21fec:	add	x3, x2, #0x10
   21ff0:	mov	x29, sp
   21ff4:	bl	22000 <__gmpq_mul_2exp@@Base+0x24>
   21ff8:	ldp	x29, x30, [sp], #16
   21ffc:	ret
   22000:	stp	x29, x30, [sp, #-96]!
   22004:	stp	x28, x27, [sp, #16]
   22008:	stp	x26, x25, [sp, #32]
   2200c:	stp	x24, x23, [sp, #48]
   22010:	stp	x22, x21, [sp, #64]
   22014:	stp	x20, x19, [sp, #80]
   22018:	ldr	x8, [x3, #8]
   2201c:	ldrsw	x27, [x3, #4]
   22020:	mov	x20, x4
   22024:	mov	x19, x2
   22028:	ldr	x26, [x8]
   2202c:	cmp	x27, #0x0
   22030:	cneg	x9, x27, mi  // mi = first
   22034:	mov	x21, x1
   22038:	cmp	x26, #0x0
   2203c:	cset	w28, eq  // eq = none
   22040:	cmp	x4, #0x40
   22044:	mov	x22, x0
   22048:	mov	x23, x8
   2204c:	mov	x29, sp
   22050:	b.cc	22074 <__gmpq_mul_2exp@@Base+0x98>  // b.lo, b.ul, b.last
   22054:	cbnz	x26, 22074 <__gmpq_mul_2exp@@Base+0x98>
   22058:	ldr	x26, [x23, #8]!
   2205c:	sub	x20, x20, #0x40
   22060:	cmp	x26, #0x0
   22064:	cset	w28, eq  // eq = none
   22068:	cmp	x20, #0x40
   2206c:	b.cc	22074 <__gmpq_mul_2exp@@Base+0x98>  // b.lo, b.ul, b.last
   22070:	cbz	x26, 22058 <__gmpq_mul_2exp@@Base+0x7c>
   22074:	ldrsw	x10, [x21]
   22078:	sub	x8, x23, x8
   2207c:	sub	x24, x9, x8, asr #3
   22080:	cmp	x24, x10
   22084:	b.gt	220dc <__gmpq_mul_2exp@@Base+0x100>
   22088:	ldr	x25, [x21, #8]
   2208c:	cbz	x20, 220f0 <__gmpq_mul_2exp@@Base+0x114>
   22090:	tbnz	w26, #0, 220f0 <__gmpq_mul_2exp@@Base+0x114>
   22094:	rbit	x8, x26
   22098:	clz	x8, x8
   2209c:	cmp	x8, x20
   220a0:	csel	x8, x8, x20, cc  // cc = lo, ul, last
   220a4:	cmp	w28, #0x0
   220a8:	csel	x26, x20, x8, ne  // ne = any
   220ac:	mov	x0, x25
   220b0:	mov	x1, x23
   220b4:	mov	x2, x24
   220b8:	mov	w3, w26
   220bc:	bl	c2f0 <__gmpn_rshift@plt>
   220c0:	add	x8, x25, x24, lsl #3
   220c4:	ldur	x8, [x8, #-8]
   220c8:	sub	x20, x20, x26
   220cc:	cmp	x8, #0x0
   220d0:	cset	w8, eq  // eq = none
   220d4:	sub	x24, x24, x8
   220d8:	b	22108 <__gmpq_mul_2exp@@Base+0x12c>
   220dc:	mov	x0, x21
   220e0:	mov	x1, x24
   220e4:	bl	c1c0 <__gmpz_realloc@plt>
   220e8:	mov	x25, x0
   220ec:	cbnz	x20, 22090 <__gmpq_mul_2exp@@Base+0xb4>
   220f0:	cmp	x23, x25
   220f4:	b.eq	22108 <__gmpq_mul_2exp@@Base+0x12c>  // b.none
   220f8:	mov	x0, x25
   220fc:	mov	x1, x23
   22100:	mov	x2, x24
   22104:	bl	cc10 <__gmpn_copyi@plt>
   22108:	neg	w8, w24
   2210c:	cmp	w27, #0x0
   22110:	csel	x8, x24, x8, ge  // ge = tcont
   22114:	str	w8, [x21, #4]
   22118:	cbz	x20, 22130 <__gmpq_mul_2exp@@Base+0x154>
   2211c:	mov	x0, x22
   22120:	mov	x1, x19
   22124:	mov	x2, x20
   22128:	bl	c870 <__gmpz_mul_2exp@plt>
   2212c:	b	22144 <__gmpq_mul_2exp@@Base+0x168>
   22130:	cmp	x22, x19
   22134:	b.eq	22144 <__gmpq_mul_2exp@@Base+0x168>  // b.none
   22138:	mov	x0, x22
   2213c:	mov	x1, x19
   22140:	bl	c590 <__gmpz_set@plt>
   22144:	ldp	x20, x19, [sp, #80]
   22148:	ldp	x22, x21, [sp, #64]
   2214c:	ldp	x24, x23, [sp, #48]
   22150:	ldp	x26, x25, [sp, #32]
   22154:	ldp	x28, x27, [sp, #16]
   22158:	ldp	x29, x30, [sp], #96
   2215c:	ret

0000000000022160 <__gmpq_div_2exp@@Base>:
   22160:	stp	x29, x30, [sp, #-16]!
   22164:	ldr	w8, [x1, #4]
   22168:	mov	x3, x1
   2216c:	mov	x1, x0
   22170:	mov	x29, sp
   22174:	cbz	w8, 22190 <__gmpq_div_2exp@@Base+0x30>
   22178:	mov	x4, x2
   2217c:	add	x0, x1, #0x10
   22180:	add	x2, x3, #0x10
   22184:	bl	22000 <__gmpq_mul_2exp@@Base+0x24>
   22188:	ldp	x29, x30, [sp], #16
   2218c:	ret
   22190:	mov	x0, x1
   22194:	ldr	w8, [x0, #16]!
   22198:	mov	w9, #0x1                   	// #1
   2219c:	cmp	w8, #0x0
   221a0:	stur	wzr, [x0, #-12]
   221a4:	str	w9, [x0, #4]
   221a8:	b.le	221c0 <__gmpq_div_2exp@@Base+0x60>
   221ac:	ldr	x0, [x1, #24]
   221b0:	mov	w8, #0x1                   	// #1
   221b4:	str	x8, [x0]
   221b8:	ldp	x29, x30, [sp], #16
   221bc:	ret
   221c0:	mov	w1, #0x1                   	// #1
   221c4:	bl	c1c0 <__gmpz_realloc@plt>
   221c8:	b	221b0 <__gmpq_div_2exp@@Base+0x50>

00000000000221cc <__gmpq_mul@@Base>:
   221cc:	stp	x29, x30, [sp, #-96]!
   221d0:	str	x27, [sp, #16]
   221d4:	stp	x26, x25, [sp, #32]
   221d8:	stp	x24, x23, [sp, #48]
   221dc:	stp	x22, x21, [sp, #64]
   221e0:	stp	x20, x19, [sp, #80]
   221e4:	mov	x29, sp
   221e8:	sub	sp, sp, #0x40
   221ec:	mov	x20, x1
   221f0:	cmp	x1, x2
   221f4:	mov	x19, x0
   221f8:	b.eq	22394 <__gmpq_mul@@Base+0x1c8>  // b.none
   221fc:	ldr	w8, [x20, #4]
   22200:	ldr	w9, [x2, #4]
   22204:	mov	x21, x2
   22208:	cmp	w8, #0x0
   2220c:	cneg	w26, w8, mi  // mi = first
   22210:	cmp	w9, #0x0
   22214:	cneg	w24, w9, mi  // mi = first
   22218:	cbz	w26, 223b8 <__gmpq_mul@@Base+0x1ec>
   2221c:	cbz	w24, 223b8 <__gmpq_mul@@Base+0x1ec>
   22220:	ldrsw	x27, [x21, #20]
   22224:	ldrsw	x25, [x20, #20]
   22228:	mov	w9, #0x7f00                	// #32512
   2222c:	str	xzr, [x29, #24]
   22230:	cmp	x26, x27
   22234:	csel	x8, x26, x27, lt  // lt = tstop
   22238:	lsl	x1, x8, #3
   2223c:	cmp	x1, x9
   22240:	stur	w8, [x29, #-16]
   22244:	b.hi	223fc <__gmpq_mul@@Base+0x230>  // b.pmore
   22248:	add	x9, x1, #0xf
   2224c:	mov	x8, sp
   22250:	and	x9, x9, #0xfffffffffffffff0
   22254:	sub	x0, x8, x9
   22258:	mov	sp, x0
   2225c:	cmp	x24, x25
   22260:	csel	x8, x24, x25, lt  // lt = tstop
   22264:	lsl	x1, x8, #3
   22268:	mov	w9, #0x7f00                	// #32512
   2226c:	cmp	x1, x9
   22270:	stur	x0, [x29, #-8]
   22274:	stur	w8, [x29, #-32]
   22278:	b.hi	22408 <__gmpq_mul@@Base+0x23c>  // b.pmore
   2227c:	add	x9, x1, #0xf
   22280:	mov	x8, sp
   22284:	and	x9, x9, #0xfffffffffffffff0
   22288:	sub	x0, x8, x9
   2228c:	mov	sp, x0
   22290:	cmp	x26, x27
   22294:	csel	x8, x26, x27, gt
   22298:	lsl	x1, x8, #3
   2229c:	mov	w9, #0x7f00                	// #32512
   222a0:	add	x22, x20, #0x10
   222a4:	add	x23, x21, #0x10
   222a8:	cmp	x1, x9
   222ac:	stur	x0, [x29, #-24]
   222b0:	stur	w8, [x29, #-48]
   222b4:	b.hi	22414 <__gmpq_mul@@Base+0x248>  // b.pmore
   222b8:	add	x9, x1, #0xf
   222bc:	mov	x8, sp
   222c0:	and	x9, x9, #0xfffffffffffffff0
   222c4:	sub	x0, x8, x9
   222c8:	mov	sp, x0
   222cc:	cmp	x24, x25
   222d0:	csel	x8, x24, x25, gt
   222d4:	lsl	x1, x8, #3
   222d8:	mov	w9, #0x7f00                	// #32512
   222dc:	cmp	x1, x9
   222e0:	stur	x0, [x29, #-40]
   222e4:	stur	w8, [x29, #-64]
   222e8:	b.hi	22420 <__gmpq_mul@@Base+0x254>  // b.pmore
   222ec:	add	x9, x1, #0xf
   222f0:	mov	x8, sp
   222f4:	and	x9, x9, #0xfffffffffffffff0
   222f8:	sub	x0, x8, x9
   222fc:	mov	sp, x0
   22300:	stur	x0, [x29, #-56]
   22304:	sub	x0, x29, #0x10
   22308:	mov	x1, x20
   2230c:	mov	x2, x23
   22310:	bl	d140 <__gmpz_gcd@plt>
   22314:	sub	x0, x29, #0x20
   22318:	mov	x1, x21
   2231c:	mov	x2, x22
   22320:	bl	d140 <__gmpz_gcd@plt>
   22324:	sub	x0, x29, #0x30
   22328:	sub	x2, x29, #0x10
   2232c:	mov	x1, x20
   22330:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   22334:	sub	x0, x29, #0x40
   22338:	sub	x2, x29, #0x20
   2233c:	mov	x1, x21
   22340:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   22344:	sub	x1, x29, #0x30
   22348:	sub	x2, x29, #0x40
   2234c:	mov	x0, x19
   22350:	bl	c620 <__gmpz_mul@plt>
   22354:	sub	x0, x29, #0x30
   22358:	sub	x2, x29, #0x10
   2235c:	mov	x1, x23
   22360:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   22364:	sub	x0, x29, #0x40
   22368:	sub	x2, x29, #0x20
   2236c:	mov	x1, x22
   22370:	bl	cbc0 <__gmpz_divexact_gcd@plt>
   22374:	add	x0, x19, #0x10
   22378:	sub	x1, x29, #0x30
   2237c:	sub	x2, x29, #0x40
   22380:	bl	c620 <__gmpz_mul@plt>
   22384:	ldr	x0, [x29, #24]
   22388:	cbz	x0, 223dc <__gmpq_mul@@Base+0x210>
   2238c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   22390:	b	223dc <__gmpq_mul@@Base+0x210>
   22394:	mov	x0, x19
   22398:	mov	x1, x20
   2239c:	mov	x2, x20
   223a0:	bl	c620 <__gmpz_mul@plt>
   223a4:	add	x1, x20, #0x10
   223a8:	add	x0, x19, #0x10
   223ac:	mov	x2, x1
   223b0:	bl	c620 <__gmpz_mul@plt>
   223b4:	b	223dc <__gmpq_mul@@Base+0x210>
   223b8:	mov	x0, x19
   223bc:	ldr	w8, [x0, #16]!
   223c0:	cmp	w8, #0x0
   223c4:	stur	wzr, [x0, #-12]
   223c8:	b.le	2242c <__gmpq_mul@@Base+0x260>
   223cc:	ldr	x0, [x19, #24]
   223d0:	mov	w8, #0x1                   	// #1
   223d4:	str	x8, [x0]
   223d8:	str	w8, [x19, #20]
   223dc:	mov	sp, x29
   223e0:	ldp	x20, x19, [sp, #80]
   223e4:	ldp	x22, x21, [sp, #64]
   223e8:	ldp	x24, x23, [sp, #48]
   223ec:	ldp	x26, x25, [sp, #32]
   223f0:	ldr	x27, [sp, #16]
   223f4:	ldp	x29, x30, [sp], #96
   223f8:	ret
   223fc:	add	x0, x29, #0x18
   22400:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   22404:	b	2225c <__gmpq_mul@@Base+0x90>
   22408:	add	x0, x29, #0x18
   2240c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   22410:	b	22290 <__gmpq_mul@@Base+0xc4>
   22414:	add	x0, x29, #0x18
   22418:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2241c:	b	222cc <__gmpq_mul@@Base+0x100>
   22420:	add	x0, x29, #0x18
   22424:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   22428:	b	22300 <__gmpq_mul@@Base+0x134>
   2242c:	mov	w1, #0x1                   	// #1
   22430:	bl	c1c0 <__gmpz_realloc@plt>
   22434:	b	223d0 <__gmpq_mul@@Base+0x204>

0000000000022438 <__gmpq_neg@@Base>:
   22438:	stp	x29, x30, [sp, #-64]!
   2243c:	stp	x22, x21, [sp, #32]
   22440:	stp	x20, x19, [sp, #48]
   22444:	ldrsw	x22, [x1, #4]
   22448:	mov	x19, x0
   2244c:	cmp	x1, x0
   22450:	str	x23, [sp, #16]
   22454:	mov	x29, sp
   22458:	b.eq	224b0 <__gmpq_neg@@Base+0x78>  // b.none
   2245c:	ldrsw	x8, [x19]
   22460:	cmp	x22, #0x0
   22464:	cneg	x21, x22, mi  // mi = first
   22468:	mov	x20, x1
   2246c:	cmp	x21, x8
   22470:	b.gt	224cc <__gmpq_neg@@Base+0x94>
   22474:	ldr	x0, [x19, #8]
   22478:	ldr	x1, [x20, #8]
   2247c:	mov	x2, x21
   22480:	bl	cc10 <__gmpn_copyi@plt>
   22484:	mov	x0, x19
   22488:	ldr	w23, [x20, #20]
   2248c:	ldr	w8, [x0, #16]!
   22490:	sxtw	x21, w23
   22494:	cmp	w23, w8
   22498:	b.gt	224dc <__gmpq_neg@@Base+0xa4>
   2249c:	ldr	x0, [x19, #24]
   224a0:	str	w23, [x19, #20]
   224a4:	ldr	x1, [x20, #24]
   224a8:	mov	x2, x21
   224ac:	bl	cc10 <__gmpn_copyi@plt>
   224b0:	neg	w8, w22
   224b4:	str	w8, [x19, #4]
   224b8:	ldp	x20, x19, [sp, #48]
   224bc:	ldp	x22, x21, [sp, #32]
   224c0:	ldr	x23, [sp, #16]
   224c4:	ldp	x29, x30, [sp], #64
   224c8:	ret
   224cc:	mov	x0, x19
   224d0:	mov	x1, x21
   224d4:	bl	c1c0 <__gmpz_realloc@plt>
   224d8:	b	22478 <__gmpq_neg@@Base+0x40>
   224dc:	mov	x1, x21
   224e0:	bl	c1c0 <__gmpz_realloc@plt>
   224e4:	b	224a0 <__gmpq_neg@@Base+0x68>

00000000000224e8 <__gmpq_out_str@@Base>:
   224e8:	stp	x29, x30, [sp, #-48]!
   224ec:	stp	x22, x21, [sp, #16]
   224f0:	stp	x20, x19, [sp, #32]
   224f4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   224f8:	ldr	x8, [x8, #3856]
   224fc:	cmp	x0, #0x0
   22500:	mov	x29, sp
   22504:	mov	x22, x2
   22508:	ldr	x8, [x8]
   2250c:	mov	w21, w1
   22510:	csel	x19, x8, x0, eq  // eq = none
   22514:	mov	x0, x19
   22518:	bl	c6d0 <__gmpz_out_str@plt>
   2251c:	add	x22, x22, #0x10
   22520:	mov	x20, x0
   22524:	mov	w1, #0x1                   	// #1
   22528:	mov	x0, x22
   2252c:	bl	d3d0 <__gmpz_cmp_ui@plt>
   22530:	cbz	w0, 22558 <__gmpq_out_str@@Base+0x70>
   22534:	mov	w0, #0x2f                  	// #47
   22538:	mov	x1, x19
   2253c:	bl	c330 <putc@plt>
   22540:	mov	x0, x19
   22544:	mov	w1, w21
   22548:	mov	x2, x22
   2254c:	bl	c6d0 <__gmpz_out_str@plt>
   22550:	add	x8, x20, x0
   22554:	add	x20, x8, #0x1
   22558:	mov	x0, x19
   2255c:	bl	d680 <ferror@plt>
   22560:	cmp	w0, #0x0
   22564:	csel	x0, x20, xzr, eq  // eq = none
   22568:	ldp	x20, x19, [sp, #32]
   2256c:	ldp	x22, x21, [sp, #16]
   22570:	ldp	x29, x30, [sp], #48
   22574:	ret

0000000000022578 <__gmpq_set@@Base>:
   22578:	stp	x29, x30, [sp, #-48]!
   2257c:	stp	x20, x19, [sp, #32]
   22580:	ldrsw	x8, [x1, #4]
   22584:	ldrsw	x9, [x0]
   22588:	str	x21, [sp, #16]
   2258c:	mov	x19, x1
   22590:	cmp	x8, #0x0
   22594:	cneg	x21, x8, mi  // mi = first
   22598:	mov	x20, x0
   2259c:	cmp	x21, x9
   225a0:	mov	x29, sp
   225a4:	str	w8, [x0, #4]
   225a8:	b.gt	225f4 <__gmpq_set@@Base+0x7c>
   225ac:	ldr	x0, [x20, #8]
   225b0:	ldr	x1, [x19, #8]
   225b4:	mov	x2, x21
   225b8:	bl	cc10 <__gmpn_copyi@plt>
   225bc:	mov	x0, x20
   225c0:	ldrsw	x21, [x19, #20]
   225c4:	ldr	w8, [x0, #16]!
   225c8:	cmp	w21, w8
   225cc:	str	w21, [x0, #4]
   225d0:	b.gt	22604 <__gmpq_set@@Base+0x8c>
   225d4:	ldr	x0, [x20, #24]
   225d8:	ldr	x1, [x19, #24]
   225dc:	mov	x2, x21
   225e0:	bl	cc10 <__gmpn_copyi@plt>
   225e4:	ldp	x20, x19, [sp, #32]
   225e8:	ldr	x21, [sp, #16]
   225ec:	ldp	x29, x30, [sp], #48
   225f0:	ret
   225f4:	mov	x0, x20
   225f8:	mov	x1, x21
   225fc:	bl	c1c0 <__gmpz_realloc@plt>
   22600:	b	225b0 <__gmpq_set@@Base+0x38>
   22604:	mov	x1, x21
   22608:	bl	c1c0 <__gmpz_realloc@plt>
   2260c:	b	225d8 <__gmpq_set@@Base+0x60>

0000000000022610 <__gmpq_set_den@@Base>:
   22610:	stp	x29, x30, [sp, #-32]!
   22614:	stp	x20, x19, [sp, #16]
   22618:	ldrsw	x9, [x1, #4]
   2261c:	mov	x8, x0
   22620:	ldrsw	x10, [x8, #16]!
   22624:	mov	x19, x1
   22628:	cmp	x9, #0x0
   2262c:	cneg	x20, x9, mi  // mi = first
   22630:	cmp	x20, x10
   22634:	mov	x29, sp
   22638:	str	w9, [x8, #4]
   2263c:	b.gt	2265c <__gmpq_set_den@@Base+0x4c>
   22640:	ldr	x0, [x0, #24]
   22644:	ldr	x1, [x19, #8]
   22648:	mov	x2, x20
   2264c:	bl	cc10 <__gmpn_copyi@plt>
   22650:	ldp	x20, x19, [sp, #16]
   22654:	ldp	x29, x30, [sp], #32
   22658:	ret
   2265c:	mov	x0, x8
   22660:	mov	x1, x20
   22664:	bl	c1c0 <__gmpz_realloc@plt>
   22668:	b	22644 <__gmpq_set_den@@Base+0x34>

000000000002266c <__gmpq_set_num@@Base>:
   2266c:	stp	x29, x30, [sp, #-32]!
   22670:	stp	x20, x19, [sp, #16]
   22674:	ldrsw	x8, [x1, #4]
   22678:	ldrsw	x9, [x0]
   2267c:	mov	x19, x1
   22680:	mov	x29, sp
   22684:	cmp	x8, #0x0
   22688:	cneg	x20, x8, mi  // mi = first
   2268c:	cmp	x20, x9
   22690:	str	w8, [x0, #4]
   22694:	b.gt	226b4 <__gmpq_set_num@@Base+0x48>
   22698:	ldr	x0, [x0, #8]
   2269c:	ldr	x1, [x19, #8]
   226a0:	mov	x2, x20
   226a4:	bl	cc10 <__gmpn_copyi@plt>
   226a8:	ldp	x20, x19, [sp, #16]
   226ac:	ldp	x29, x30, [sp], #32
   226b0:	ret
   226b4:	mov	x1, x20
   226b8:	bl	c1c0 <__gmpz_realloc@plt>
   226bc:	b	2269c <__gmpq_set_num@@Base+0x30>

00000000000226c0 <__gmpq_set_si@@Base>:
   226c0:	stp	x29, x30, [sp, #-48]!
   226c4:	stp	x20, x19, [sp, #32]
   226c8:	mov	x19, x0
   226cc:	stp	x22, x21, [sp, #16]
   226d0:	mov	x29, sp
   226d4:	cbz	x1, 2270c <__gmpq_set_si@@Base+0x4c>
   226d8:	ldr	w8, [x19]
   226dc:	cmp	x1, #0x0
   226e0:	mov	x20, x2
   226e4:	mov	x21, x1
   226e8:	cneg	x22, x1, mi  // mi = first
   226ec:	cmp	w8, #0x0
   226f0:	b.le	22758 <__gmpq_set_si@@Base+0x98>
   226f4:	ldr	x0, [x19, #8]
   226f8:	cmp	x21, #0x0
   226fc:	mov	w8, #0x1                   	// #1
   22700:	cneg	w8, w8, le
   22704:	str	x22, [x0]
   22708:	b	22714 <__gmpq_set_si@@Base+0x54>
   2270c:	mov	w8, wzr
   22710:	mov	w20, #0x1                   	// #1
   22714:	mov	x0, x19
   22718:	ldr	w9, [x0, #16]!
   2271c:	cmp	w9, #0x0
   22720:	stur	w8, [x0, #-12]
   22724:	b.le	2274c <__gmpq_set_si@@Base+0x8c>
   22728:	ldr	x0, [x19, #24]
   2272c:	cmp	x20, #0x0
   22730:	cset	w8, ne  // ne = any
   22734:	str	x20, [x0]
   22738:	str	w8, [x19, #20]
   2273c:	ldp	x20, x19, [sp, #32]
   22740:	ldp	x22, x21, [sp, #16]
   22744:	ldp	x29, x30, [sp], #48
   22748:	ret
   2274c:	mov	w1, #0x1                   	// #1
   22750:	bl	c1c0 <__gmpz_realloc@plt>
   22754:	b	2272c <__gmpq_set_si@@Base+0x6c>
   22758:	mov	w1, #0x1                   	// #1
   2275c:	mov	x0, x19
   22760:	bl	c1c0 <__gmpz_realloc@plt>
   22764:	b	226f8 <__gmpq_set_si@@Base+0x38>

0000000000022768 <__gmpq_set_str@@Base>:
   22768:	stp	x29, x30, [sp, #-80]!
   2276c:	stp	x22, x21, [sp, #48]
   22770:	mov	x21, x1
   22774:	stp	x20, x19, [sp, #64]
   22778:	mov	x20, x0
   2277c:	mov	w1, #0x2f                  	// #47
   22780:	mov	x0, x21
   22784:	str	x25, [sp, #16]
   22788:	stp	x24, x23, [sp, #32]
   2278c:	mov	x29, sp
   22790:	mov	w19, w2
   22794:	bl	cf70 <strchr@plt>
   22798:	cbz	x0, 2280c <__gmpq_set_str@@Base+0xa4>
   2279c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   227a0:	ldr	x8, [x8, #3840]
   227a4:	sub	x23, x0, x21
   227a8:	add	x24, x23, #0x1
   227ac:	mov	x22, x0
   227b0:	ldr	x8, [x8]
   227b4:	mov	x0, x24
   227b8:	blr	x8
   227bc:	mov	x1, x21
   227c0:	mov	x2, x23
   227c4:	mov	x25, x0
   227c8:	bl	bff0 <memcpy@plt>
   227cc:	mov	x0, x20
   227d0:	mov	x1, x25
   227d4:	mov	w2, w19
   227d8:	strb	wzr, [x25, x23]
   227dc:	bl	c210 <__gmpz_set_str@plt>
   227e0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   227e4:	ldr	x8, [x8, #4016]
   227e8:	mov	w21, w0
   227ec:	mov	x0, x25
   227f0:	mov	x1, x24
   227f4:	ldr	x8, [x8]
   227f8:	blr	x8
   227fc:	cbnz	w21, 22844 <__gmpq_set_str@@Base+0xdc>
   22800:	add	x0, x20, #0x10
   22804:	add	x1, x22, #0x1
   22808:	b	22838 <__gmpq_set_str@@Base+0xd0>
   2280c:	mov	x0, x20
   22810:	ldr	w8, [x0, #16]!
   22814:	mov	w9, #0x1                   	// #1
   22818:	cmp	w8, #0x0
   2281c:	str	w9, [x0, #4]
   22820:	b.le	22860 <__gmpq_set_str@@Base+0xf8>
   22824:	ldr	x0, [x20, #24]
   22828:	mov	w8, #0x1                   	// #1
   2282c:	str	x8, [x0]
   22830:	mov	x0, x20
   22834:	mov	x1, x21
   22838:	mov	w2, w19
   2283c:	bl	c210 <__gmpz_set_str@plt>
   22840:	mov	w21, w0
   22844:	mov	w0, w21
   22848:	ldp	x20, x19, [sp, #64]
   2284c:	ldp	x22, x21, [sp, #48]
   22850:	ldp	x24, x23, [sp, #32]
   22854:	ldr	x25, [sp, #16]
   22858:	ldp	x29, x30, [sp], #80
   2285c:	ret
   22860:	mov	w1, #0x1                   	// #1
   22864:	bl	c1c0 <__gmpz_realloc@plt>
   22868:	b	22828 <__gmpq_set_str@@Base+0xc0>

000000000002286c <__gmpq_set_ui@@Base>:
   2286c:	stp	x29, x30, [sp, #-48]!
   22870:	stp	x20, x19, [sp, #32]
   22874:	mov	x19, x0
   22878:	str	x21, [sp, #16]
   2287c:	mov	x29, sp
   22880:	cbz	x1, 228a8 <__gmpq_set_ui@@Base+0x3c>
   22884:	ldr	w8, [x19]
   22888:	mov	x20, x2
   2288c:	mov	x21, x1
   22890:	cmp	w8, #0x0
   22894:	b.le	228f4 <__gmpq_set_ui@@Base+0x88>
   22898:	ldr	x0, [x19, #8]
   2289c:	mov	w8, #0x1                   	// #1
   228a0:	str	x21, [x0]
   228a4:	b	228b0 <__gmpq_set_ui@@Base+0x44>
   228a8:	mov	w8, wzr
   228ac:	mov	w20, #0x1                   	// #1
   228b0:	mov	x0, x19
   228b4:	ldr	w9, [x0, #16]!
   228b8:	cmp	w9, #0x0
   228bc:	stur	w8, [x0, #-12]
   228c0:	b.le	228e8 <__gmpq_set_ui@@Base+0x7c>
   228c4:	ldr	x0, [x19, #24]
   228c8:	cmp	x20, #0x0
   228cc:	cset	w8, ne  // ne = any
   228d0:	str	x20, [x0]
   228d4:	str	w8, [x19, #20]
   228d8:	ldp	x20, x19, [sp, #32]
   228dc:	ldr	x21, [sp, #16]
   228e0:	ldp	x29, x30, [sp], #48
   228e4:	ret
   228e8:	mov	w1, #0x1                   	// #1
   228ec:	bl	c1c0 <__gmpz_realloc@plt>
   228f0:	b	228c8 <__gmpq_set_ui@@Base+0x5c>
   228f4:	mov	w1, #0x1                   	// #1
   228f8:	mov	x0, x19
   228fc:	bl	c1c0 <__gmpz_realloc@plt>
   22900:	b	2289c <__gmpq_set_ui@@Base+0x30>

0000000000022904 <__gmpq_equal@@Base>:
   22904:	ldrsw	x9, [x0, #4]
   22908:	ldr	w8, [x1, #4]
   2290c:	cmp	w9, w8
   22910:	b.ne	22994 <__gmpq_equal@@Base+0x90>  // b.any
   22914:	ldrsw	x8, [x0, #20]
   22918:	ldr	w10, [x1, #20]
   2291c:	cmp	w8, w10
   22920:	b.ne	22994 <__gmpq_equal@@Base+0x90>  // b.any
   22924:	cmp	x9, #0x0
   22928:	cneg	x9, x9, mi  // mi = first
   2292c:	cmp	x9, #0x1
   22930:	b.lt	2295c <__gmpq_equal@@Base+0x58>  // b.tstop
   22934:	ldr	x10, [x0, #8]
   22938:	ldr	x11, [x1, #8]
   2293c:	ldr	x12, [x10]
   22940:	ldr	x13, [x11]
   22944:	cmp	x12, x13
   22948:	b.ne	22994 <__gmpq_equal@@Base+0x90>  // b.any
   2294c:	subs	x9, x9, #0x1
   22950:	add	x11, x11, #0x8
   22954:	add	x10, x10, #0x8
   22958:	b.ne	2293c <__gmpq_equal@@Base+0x38>  // b.any
   2295c:	cmp	w8, #0x1
   22960:	b.lt	2298c <__gmpq_equal@@Base+0x88>  // b.tstop
   22964:	ldr	x9, [x0, #24]
   22968:	ldr	x10, [x1, #24]
   2296c:	ldr	x11, [x9]
   22970:	ldr	x12, [x10]
   22974:	cmp	x11, x12
   22978:	b.ne	22994 <__gmpq_equal@@Base+0x90>  // b.any
   2297c:	subs	x8, x8, #0x1
   22980:	add	x10, x10, #0x8
   22984:	add	x9, x9, #0x8
   22988:	b.ne	2296c <__gmpq_equal@@Base+0x68>  // b.any
   2298c:	mov	w0, #0x1                   	// #1
   22990:	ret
   22994:	mov	w0, wzr
   22998:	ret

000000000002299c <__gmpq_set_z@@Base>:
   2299c:	stp	x29, x30, [sp, #-48]!
   229a0:	stp	x20, x19, [sp, #32]
   229a4:	ldrsw	x8, [x1, #4]
   229a8:	ldrsw	x9, [x0]
   229ac:	str	x21, [sp, #16]
   229b0:	mov	x20, x1
   229b4:	cmp	x8, #0x0
   229b8:	cneg	x21, x8, mi  // mi = first
   229bc:	mov	x19, x0
   229c0:	cmp	x21, x9
   229c4:	mov	x29, sp
   229c8:	str	w8, [x0, #4]
   229cc:	b.gt	22a10 <__gmpq_set_z@@Base+0x74>
   229d0:	ldr	x0, [x19, #8]
   229d4:	ldr	x1, [x20, #8]
   229d8:	mov	x2, x21
   229dc:	bl	cc10 <__gmpn_copyi@plt>
   229e0:	mov	x0, x19
   229e4:	ldr	w8, [x0, #16]!
   229e8:	cmp	w8, #0x0
   229ec:	b.le	22a20 <__gmpq_set_z@@Base+0x84>
   229f0:	ldr	x0, [x19, #24]
   229f4:	mov	w8, #0x1                   	// #1
   229f8:	str	x8, [x0]
   229fc:	str	w8, [x19, #20]
   22a00:	ldp	x20, x19, [sp, #32]
   22a04:	ldr	x21, [sp, #16]
   22a08:	ldp	x29, x30, [sp], #48
   22a0c:	ret
   22a10:	mov	x0, x19
   22a14:	mov	x1, x21
   22a18:	bl	c1c0 <__gmpz_realloc@plt>
   22a1c:	b	229d4 <__gmpq_set_z@@Base+0x38>
   22a20:	mov	w1, #0x1                   	// #1
   22a24:	bl	c1c0 <__gmpz_realloc@plt>
   22a28:	b	229f4 <__gmpq_set_z@@Base+0x58>

0000000000022a2c <__gmpq_set_d@@Base>:
   22a2c:	sub	sp, sp, #0x70
   22a30:	stp	d9, d8, [sp, #16]
   22a34:	mov	v8.16b, v0.16b
   22a38:	fmov	x8, d8
   22a3c:	mvn	x8, x8
   22a40:	tst	x8, #0x7ff0000000000000
   22a44:	stp	x29, x30, [sp, #32]
   22a48:	str	x25, [sp, #48]
   22a4c:	stp	x24, x23, [sp, #64]
   22a50:	stp	x22, x21, [sp, #80]
   22a54:	stp	x20, x19, [sp, #96]
   22a58:	add	x29, sp, #0x10
   22a5c:	b.eq	22c88 <__gmpq_set_d@@Base+0x25c>  // b.none
   22a60:	fneg	d0, d8
   22a64:	fcmp	d8, #0.0
   22a68:	fcsel	d9, d8, d0, ge  // ge = tcont
   22a6c:	mov	x19, x0
   22a70:	mov	x0, sp
   22a74:	mov	v0.16b, v9.16b
   22a78:	bl	d460 <__gmp_extract_double@plt>
   22a7c:	cmp	w0, #0x1
   22a80:	sxtw	x20, w0
   22a84:	b.gt	22abc <__gmpq_set_d@@Base+0x90>
   22a88:	fcmp	d9, #0.0
   22a8c:	b.ne	22b54 <__gmpq_set_d@@Base+0x128>  // b.any
   22a90:	mov	x0, x19
   22a94:	ldr	w8, [x0, #16]!
   22a98:	mov	w9, #0x1                   	// #1
   22a9c:	cmp	w8, #0x0
   22aa0:	stur	wzr, [x0, #-12]
   22aa4:	str	w9, [x0, #4]
   22aa8:	b.le	22ba4 <__gmpq_set_d@@Base+0x178>
   22aac:	ldr	x0, [x19, #24]
   22ab0:	mov	w8, #0x1                   	// #1
   22ab4:	str	x8, [x0]
   22ab8:	b	22b34 <__gmpq_set_d@@Base+0x108>
   22abc:	ldr	w8, [x19]
   22ac0:	cmp	w20, w8
   22ac4:	b.gt	22b7c <__gmpq_set_d@@Base+0x150>
   22ac8:	ldr	x21, [x19, #8]
   22acc:	cmp	w20, #0x2
   22ad0:	b.eq	22af4 <__gmpq_set_d@@Base+0xc8>  // b.none
   22ad4:	subs	x22, x20, #0x2
   22ad8:	b.eq	22af0 <__gmpq_set_d@@Base+0xc4>  // b.none
   22adc:	lsl	x8, x20, #3
   22ae0:	sub	x2, x8, #0x10
   22ae4:	mov	x0, x21
   22ae8:	mov	w1, wzr
   22aec:	bl	c780 <memset@plt>
   22af0:	add	x21, x21, x22, lsl #3
   22af4:	ldr	x8, [sp, #8]
   22af8:	mov	x0, x19
   22afc:	str	x8, [x21, #8]
   22b00:	ldr	x8, [sp]
   22b04:	str	x8, [x21]
   22b08:	ldr	w8, [x0, #16]!
   22b0c:	cmp	w8, #0x0
   22b10:	b.le	22b98 <__gmpq_set_d@@Base+0x16c>
   22b14:	ldr	x0, [x19, #24]
   22b18:	mov	w25, #0x1                   	// #1
   22b1c:	str	x25, [x0]
   22b20:	neg	w8, w20
   22b24:	fcmp	d8, #0.0
   22b28:	csel	x8, x8, x20, mi  // mi = first
   22b2c:	str	w25, [x19, #20]
   22b30:	str	w8, [x19, #4]
   22b34:	ldp	x20, x19, [sp, #96]
   22b38:	ldp	x22, x21, [sp, #80]
   22b3c:	ldp	x24, x23, [sp, #64]
   22b40:	ldr	x25, [sp, #48]
   22b44:	ldp	x29, x30, [sp, #32]
   22b48:	ldp	d9, d8, [sp, #16]
   22b4c:	add	sp, sp, #0x70
   22b50:	ret
   22b54:	ldr	w8, [x19]
   22b58:	cmp	w8, #0x1
   22b5c:	b.le	22bb0 <__gmpq_set_d@@Base+0x184>
   22b60:	ldr	x22, [x19, #8]
   22b64:	ldp	x9, x8, [sp]
   22b68:	cbz	x9, 22bc8 <__gmpq_set_d@@Base+0x19c>
   22b6c:	str	x8, [x22, #8]
   22b70:	mov	w21, #0x2                   	// #2
   22b74:	mov	x8, x9
   22b78:	b	22bcc <__gmpq_set_d@@Base+0x1a0>
   22b7c:	mov	x0, x19
   22b80:	mov	x1, x20
   22b84:	bl	c1c0 <__gmpz_realloc@plt>
   22b88:	mov	x21, x0
   22b8c:	cmp	w20, #0x2
   22b90:	b.ne	22ad4 <__gmpq_set_d@@Base+0xa8>  // b.any
   22b94:	b	22af4 <__gmpq_set_d@@Base+0xc8>
   22b98:	mov	w1, #0x1                   	// #1
   22b9c:	bl	c1c0 <__gmpz_realloc@plt>
   22ba0:	b	22b18 <__gmpq_set_d@@Base+0xec>
   22ba4:	mov	w1, #0x1                   	// #1
   22ba8:	bl	c1c0 <__gmpz_realloc@plt>
   22bac:	b	22ab0 <__gmpq_set_d@@Base+0x84>
   22bb0:	mov	w1, #0x2                   	// #2
   22bb4:	mov	x0, x19
   22bb8:	bl	c1c0 <__gmpz_realloc@plt>
   22bbc:	mov	x22, x0
   22bc0:	ldp	x9, x8, [sp]
   22bc4:	cbnz	x9, 22b6c <__gmpq_set_d@@Base+0x140>
   22bc8:	mov	w21, #0x1                   	// #1
   22bcc:	str	x8, [x22]
   22bd0:	mov	x0, x19
   22bd4:	ldrsw	x8, [x0, #16]!
   22bd8:	sub	x24, x21, x20
   22bdc:	add	x20, x24, #0x1
   22be0:	cmp	x20, x8
   22be4:	b.gt	22c70 <__gmpq_set_d@@Base+0x244>
   22be8:	ldr	x23, [x19, #24]
   22bec:	subs	x25, x20, #0x1
   22bf0:	b.eq	22c04 <__gmpq_set_d@@Base+0x1d8>  // b.none
   22bf4:	lsl	x2, x24, #3
   22bf8:	mov	x0, x23
   22bfc:	mov	w1, wzr
   22c00:	bl	c780 <memset@plt>
   22c04:	mov	w8, #0x1                   	// #1
   22c08:	str	x8, [x23, x25, lsl #3]
   22c0c:	ldr	x8, [x22]
   22c10:	ldr	x9, [x23]
   22c14:	orr	x8, x9, x8
   22c18:	rbit	x8, x8
   22c1c:	clz	x24, x8
   22c20:	cbz	w24, 22c64 <__gmpq_set_d@@Base+0x238>
   22c24:	mov	x0, x22
   22c28:	mov	x1, x22
   22c2c:	mov	x2, x21
   22c30:	mov	w3, w24
   22c34:	bl	c2f0 <__gmpn_rshift@plt>
   22c38:	add	x8, x22, x21, lsl #3
   22c3c:	ldur	x8, [x8, #-8]
   22c40:	neg	w9, w24
   22c44:	mov	w10, #0x1                   	// #1
   22c48:	add	x11, x23, x20, lsl #3
   22c4c:	cmp	x8, #0x0
   22c50:	lsl	x9, x10, x9
   22c54:	cset	w8, eq  // eq = none
   22c58:	sub	x20, x21, x8
   22c5c:	stur	x9, [x11, #-16]
   22c60:	b	22b20 <__gmpq_set_d@@Base+0xf4>
   22c64:	mov	x25, x20
   22c68:	mov	x20, x21
   22c6c:	b	22b20 <__gmpq_set_d@@Base+0xf4>
   22c70:	mov	x1, x20
   22c74:	bl	c1c0 <__gmpz_realloc@plt>
   22c78:	mov	x23, x0
   22c7c:	subs	x25, x20, #0x1
   22c80:	b.ne	22bf4 <__gmpq_set_d@@Base+0x1c8>  // b.any
   22c84:	b	22c04 <__gmpq_set_d@@Base+0x1d8>
   22c88:	bl	c300 <__gmp_invalid_operation@plt>

0000000000022c8c <__gmpq_set_f@@Base>:
   22c8c:	stp	x29, x30, [sp, #-96]!
   22c90:	stp	x26, x25, [sp, #32]
   22c94:	stp	x24, x23, [sp, #48]
   22c98:	stp	x22, x21, [sp, #64]
   22c9c:	stp	x20, x19, [sp, #80]
   22ca0:	ldrsw	x26, [x1, #4]
   22ca4:	mov	x19, x0
   22ca8:	str	x27, [sp, #16]
   22cac:	mov	x29, sp
   22cb0:	cbz	w26, 22d68 <__gmpq_set_f@@Base+0xdc>
   22cb4:	ldp	x21, x22, [x1, #8]
   22cb8:	cmp	w26, #0x0
   22cbc:	cneg	x20, x26, lt  // lt = tstop
   22cc0:	ldr	x25, [x22]
   22cc4:	cbnz	x25, 22cd4 <__gmpq_set_f@@Base+0x48>
   22cc8:	ldr	x25, [x22, #8]!
   22ccc:	sub	x20, x20, #0x1
   22cd0:	cbz	x25, 22cc8 <__gmpq_set_f@@Base+0x3c>
   22cd4:	subs	x27, x20, x21
   22cd8:	b.le	22d94 <__gmpq_set_f@@Base+0x108>
   22cdc:	ldrsw	x8, [x19]
   22ce0:	cmp	x20, x8
   22ce4:	b.gt	22e08 <__gmpq_set_f@@Base+0x17c>
   22ce8:	ldr	x24, [x19, #8]
   22cec:	mov	x0, x19
   22cf0:	ldrsw	x8, [x0, #16]!
   22cf4:	cmp	x27, x8
   22cf8:	b.ge	22e1c <__gmpq_set_f@@Base+0x190>  // b.tcont
   22cfc:	ldr	x23, [x19, #24]
   22d00:	tbnz	w25, #0, 22e2c <__gmpq_set_f@@Base+0x1a0>
   22d04:	rbit	x8, x25
   22d08:	clz	x25, x8
   22d0c:	mov	x0, x24
   22d10:	mov	x1, x22
   22d14:	mov	x2, x20
   22d18:	mov	w3, w25
   22d1c:	sub	x27, x27, #0x1
   22d20:	bl	c2f0 <__gmpn_rshift@plt>
   22d24:	add	x8, x24, x20, lsl #3
   22d28:	ldur	x8, [x8, #-8]
   22d2c:	cmp	x8, #0x0
   22d30:	cset	w8, eq  // eq = none
   22d34:	sub	x22, x20, x8
   22d38:	cbz	x27, 22d54 <__gmpq_set_f@@Base+0xc8>
   22d3c:	mvn	x8, x21
   22d40:	add	x8, x20, x8
   22d44:	lsl	x2, x8, #3
   22d48:	mov	x0, x23
   22d4c:	mov	w1, wzr
   22d50:	bl	c780 <memset@plt>
   22d54:	sub	w8, w25, #0x1
   22d58:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   22d5c:	lsr	x8, x9, x8
   22d60:	mov	x20, x22
   22d64:	b	22e54 <__gmpq_set_f@@Base+0x1c8>
   22d68:	mov	x0, x19
   22d6c:	ldr	w8, [x0, #16]!
   22d70:	mov	w9, #0x1                   	// #1
   22d74:	cmp	w8, #0x0
   22d78:	stur	wzr, [x0, #-12]
   22d7c:	str	w9, [x0, #4]
   22d80:	b.le	22dfc <__gmpq_set_f@@Base+0x170>
   22d84:	ldr	x0, [x19, #24]
   22d88:	mov	w8, #0x1                   	// #1
   22d8c:	str	x8, [x0]
   22d90:	b	22e70 <__gmpq_set_f@@Base+0x1e4>
   22d94:	ldrsw	x8, [x19]
   22d98:	cmp	x21, x8
   22d9c:	b.gt	22e8c <__gmpq_set_f@@Base+0x200>
   22da0:	ldr	x23, [x19, #8]
   22da4:	cmp	x20, x21
   22da8:	b.eq	22dc0 <__gmpq_set_f@@Base+0x134>  // b.none
   22dac:	sub	x8, x21, x20
   22db0:	lsl	x2, x8, #3
   22db4:	mov	x0, x23
   22db8:	mov	w1, wzr
   22dbc:	bl	c780 <memset@plt>
   22dc0:	add	x8, x23, x21, lsl #3
   22dc4:	sub	x0, x8, x20, lsl #3
   22dc8:	mov	x1, x22
   22dcc:	mov	x2, x20
   22dd0:	bl	cc10 <__gmpn_copyi@plt>
   22dd4:	mov	x0, x19
   22dd8:	ldr	w9, [x0, #16]!
   22ddc:	neg	w8, w21
   22de0:	cmp	w26, #0x0
   22de4:	mov	w10, #0x1                   	// #1
   22de8:	csel	x8, x21, x8, ge  // ge = tcont
   22dec:	cmp	w9, #0x0
   22df0:	stur	w8, [x0, #-12]
   22df4:	str	w10, [x0, #4]
   22df8:	b.gt	22d84 <__gmpq_set_f@@Base+0xf8>
   22dfc:	mov	w1, #0x1                   	// #1
   22e00:	bl	c1c0 <__gmpz_realloc@plt>
   22e04:	b	22d88 <__gmpq_set_f@@Base+0xfc>
   22e08:	mov	x0, x19
   22e0c:	mov	x1, x20
   22e10:	bl	c1c0 <__gmpz_realloc@plt>
   22e14:	mov	x24, x0
   22e18:	b	22cec <__gmpq_set_f@@Base+0x60>
   22e1c:	add	x1, x27, #0x1
   22e20:	bl	c1c0 <__gmpz_realloc@plt>
   22e24:	mov	x23, x0
   22e28:	tbz	w25, #0, 22d04 <__gmpq_set_f@@Base+0x78>
   22e2c:	mov	x0, x24
   22e30:	mov	x1, x22
   22e34:	mov	x2, x20
   22e38:	bl	cc10 <__gmpn_copyi@plt>
   22e3c:	cbz	x27, 22e50 <__gmpq_set_f@@Base+0x1c4>
   22e40:	lsl	x2, x27, #3
   22e44:	mov	x0, x23
   22e48:	mov	w1, wzr
   22e4c:	bl	c780 <memset@plt>
   22e50:	mov	w8, #0x1                   	// #1
   22e54:	str	x8, [x23, x27, lsl #3]
   22e58:	neg	w8, w20
   22e5c:	cmp	w26, #0x0
   22e60:	add	w9, w27, #0x1
   22e64:	csel	x8, x20, x8, ge  // ge = tcont
   22e68:	str	w8, [x19, #4]
   22e6c:	str	w9, [x19, #20]
   22e70:	ldp	x20, x19, [sp, #80]
   22e74:	ldp	x22, x21, [sp, #64]
   22e78:	ldp	x24, x23, [sp, #48]
   22e7c:	ldp	x26, x25, [sp, #32]
   22e80:	ldr	x27, [sp, #16]
   22e84:	ldp	x29, x30, [sp], #96
   22e88:	ret
   22e8c:	mov	x0, x19
   22e90:	mov	x1, x21
   22e94:	bl	c1c0 <__gmpz_realloc@plt>
   22e98:	mov	x23, x0
   22e9c:	cmp	x20, x21
   22ea0:	b.ne	22dac <__gmpq_set_f@@Base+0x120>  // b.any
   22ea4:	b	22dc0 <__gmpq_set_f@@Base+0x134>

0000000000022ea8 <__gmpq_swap@@Base>:
   22ea8:	ldr	w8, [x1]
   22eac:	ldr	w9, [x0]
   22eb0:	str	w8, [x0]
   22eb4:	str	w9, [x1]
   22eb8:	ldr	w8, [x1, #16]
   22ebc:	ldr	w9, [x0, #16]
   22ec0:	str	w8, [x0, #16]
   22ec4:	str	w9, [x1, #16]
   22ec8:	ldr	w8, [x1, #4]
   22ecc:	ldr	w9, [x0, #4]
   22ed0:	str	w8, [x0, #4]
   22ed4:	str	w9, [x1, #4]
   22ed8:	ldr	w8, [x1, #20]
   22edc:	ldr	w9, [x0, #20]
   22ee0:	str	w8, [x0, #20]
   22ee4:	str	w9, [x1, #20]
   22ee8:	ldr	x8, [x1, #8]
   22eec:	ldr	x9, [x0, #8]
   22ef0:	str	x8, [x0, #8]
   22ef4:	str	x9, [x1, #8]
   22ef8:	ldr	x8, [x1, #24]
   22efc:	ldr	x9, [x0, #24]
   22f00:	str	x8, [x0, #24]
   22f04:	str	x9, [x1, #24]
   22f08:	ret

0000000000022f0c <__gmpn_add@@Base>:
   22f0c:	stp	x29, x30, [sp, #-48]!
   22f10:	stp	x22, x21, [sp, #16]
   22f14:	stp	x20, x19, [sp, #32]
   22f18:	mov	x21, x4
   22f1c:	mov	x22, x2
   22f20:	mov	x19, x1
   22f24:	mov	x20, x0
   22f28:	mov	x29, sp
   22f2c:	cbz	x4, 22f6c <__gmpn_add@@Base+0x60>
   22f30:	mov	x0, x20
   22f34:	mov	x1, x19
   22f38:	mov	x2, x3
   22f3c:	mov	x3, x21
   22f40:	bl	cc30 <__gmpn_add_n@plt>
   22f44:	cbz	x0, 22f6c <__gmpn_add@@Base+0x60>
   22f48:	mov	w0, #0x1                   	// #1
   22f4c:	cmp	x21, x22
   22f50:	b.ge	22fa4 <__gmpn_add@@Base+0x98>  // b.tcont
   22f54:	lsl	x8, x21, #3
   22f58:	ldr	x9, [x19, x8]
   22f5c:	add	x21, x21, #0x1
   22f60:	adds	x9, x9, #0x1
   22f64:	str	x9, [x20, x8]
   22f68:	b.cs	22f4c <__gmpn_add@@Base+0x40>  // b.hs, b.nlast
   22f6c:	cmp	x20, x19
   22f70:	mov	x0, xzr
   22f74:	b.eq	22fa4 <__gmpn_add@@Base+0x98>  // b.none
   22f78:	cmp	x21, x22
   22f7c:	b.ge	22fa4 <__gmpn_add@@Base+0x98>  // b.tcont
   22f80:	lsl	x10, x21, #3
   22f84:	sub	x8, x22, x21
   22f88:	add	x9, x20, x10
   22f8c:	add	x10, x19, x10
   22f90:	ldr	x11, [x10], #8
   22f94:	subs	x8, x8, #0x1
   22f98:	str	x11, [x9], #8
   22f9c:	b.ne	22f90 <__gmpn_add@@Base+0x84>  // b.any
   22fa0:	mov	x0, xzr
   22fa4:	ldp	x20, x19, [sp, #32]
   22fa8:	ldp	x22, x21, [sp, #16]
   22fac:	ldp	x29, x30, [sp], #48
   22fb0:	ret

0000000000022fb4 <__gmpn_add_1@@Base>:
   22fb4:	ldr	x8, [x1]
   22fb8:	adds	x8, x8, x3
   22fbc:	str	x8, [x0]
   22fc0:	b.cc	2302c <__gmpn_add_1@@Base+0x78>  // b.lo, b.ul, b.last
   22fc4:	mov	x10, #0xfffffffffffffff8    	// #-8
   22fc8:	mov	w8, #0x1                   	// #1
   22fcc:	mov	w9, #0x1                   	// #1
   22fd0:	cmp	x9, x2
   22fd4:	b.ge	2305c <__gmpn_add_1@@Base+0xa8>  // b.tcont
   22fd8:	lsl	x11, x9, #3
   22fdc:	ldr	x12, [x1, x11]
   22fe0:	add	x9, x9, #0x1
   22fe4:	sub	x10, x10, #0x8
   22fe8:	adds	x12, x12, #0x1
   22fec:	str	x12, [x0, x11]
   22ff0:	b.cs	22fd0 <__gmpn_add_1@@Base+0x1c>  // b.hs, b.nlast
   22ff4:	cmp	x1, x0
   22ff8:	mov	x8, xzr
   22ffc:	b.eq	2305c <__gmpn_add_1@@Base+0xa8>  // b.none
   23000:	cmp	x9, x2
   23004:	b.ge	2305c <__gmpn_add_1@@Base+0xa8>  // b.tcont
   23008:	sub	x8, x1, x10
   2300c:	sub	x10, x0, x10
   23010:	ldr	x11, [x8], #8
   23014:	sub	x2, x2, #0x1
   23018:	cmp	x9, x2
   2301c:	str	x11, [x10], #8
   23020:	b.ne	23010 <__gmpn_add_1@@Base+0x5c>  // b.any
   23024:	mov	x0, xzr
   23028:	ret
   2302c:	cmp	x1, x0
   23030:	mov	x8, xzr
   23034:	b.eq	2305c <__gmpn_add_1@@Base+0xa8>  // b.none
   23038:	cmp	x2, #0x2
   2303c:	b.lt	2305c <__gmpn_add_1@@Base+0xa8>  // b.tstop
   23040:	sub	x8, x2, #0x1
   23044:	add	x9, x0, #0x8
   23048:	add	x10, x1, #0x8
   2304c:	ldr	x11, [x10], #8
   23050:	subs	x8, x8, #0x1
   23054:	str	x11, [x9], #8
   23058:	b.ne	2304c <__gmpn_add_1@@Base+0x98>  // b.any
   2305c:	mov	x0, x8
   23060:	ret
   23064:	nop
   23068:	nop
   2306c:	nop

0000000000023070 <__gmpn_add_nc@@Base>:
   23070:	cmp	x4, #0x1
   23074:	b	2307c <__gmpn_add_n@@Base+0x4>

0000000000023078 <__gmpn_add_n@@Base>:
   23078:	cmn	xzr, xzr
   2307c:	lsr	x18, x3, #2
   23080:	tbz	w3, #0, 230c8 <__gmpn_add_n@@Base+0x50>
   23084:	ldr	x7, [x1]
   23088:	ldr	x11, [x2]
   2308c:	adcs	x13, x7, x11
   23090:	str	x13, [x0], #8
   23094:	tbnz	w3, #1, 230b0 <__gmpn_add_n@@Base+0x38>
   23098:	cbz	x18, 2312c <__gmpn_add_n@@Base+0xb4>
   2309c:	ldp	x4, x5, [x1, #8]
   230a0:	ldp	x8, x9, [x2, #8]
   230a4:	sub	x1, x1, #0x8
   230a8:	sub	x2, x2, #0x8
   230ac:	b	23104 <__gmpn_add_n@@Base+0x8c>
   230b0:	ldp	x6, x7, [x1, #8]
   230b4:	ldp	x10, x11, [x2, #8]
   230b8:	add	x1, x1, #0x8
   230bc:	add	x2, x2, #0x8
   230c0:	cbz	x18, 23120 <__gmpn_add_n@@Base+0xa8>
   230c4:	b	230f0 <__gmpn_add_n@@Base+0x78>
   230c8:	tbnz	w3, #1, 230e0 <__gmpn_add_n@@Base+0x68>
   230cc:	ldp	x4, x5, [x1]
   230d0:	ldp	x8, x9, [x2]
   230d4:	sub	x1, x1, #0x10
   230d8:	sub	x2, x2, #0x10
   230dc:	b	23104 <__gmpn_add_n@@Base+0x8c>
   230e0:	ldp	x6, x7, [x1]
   230e4:	ldp	x10, x11, [x2]
   230e8:	cbz	x18, 23120 <__gmpn_add_n@@Base+0xa8>
   230ec:	nop
   230f0:	ldp	x4, x5, [x1, #16]
   230f4:	ldp	x8, x9, [x2, #16]
   230f8:	adcs	x12, x6, x10
   230fc:	adcs	x13, x7, x11
   23100:	stp	x12, x13, [x0], #16
   23104:	ldp	x6, x7, [x1, #32]!
   23108:	ldp	x10, x11, [x2, #32]!
   2310c:	adcs	x12, x4, x8
   23110:	adcs	x13, x5, x9
   23114:	stp	x12, x13, [x0], #16
   23118:	sub	x18, x18, #0x1
   2311c:	cbnz	x18, 230f0 <__gmpn_add_n@@Base+0x78>
   23120:	adcs	x12, x6, x10
   23124:	adcs	x13, x7, x11
   23128:	stp	x12, x13, [x0]
   2312c:	cset	x0, cs  // cs = hs, nlast
   23130:	ret

0000000000023134 <__gmpn_sub@@Base>:
   23134:	stp	x29, x30, [sp, #-48]!
   23138:	stp	x22, x21, [sp, #16]
   2313c:	stp	x20, x19, [sp, #32]
   23140:	mov	x21, x4
   23144:	mov	x22, x2
   23148:	mov	x19, x1
   2314c:	mov	x20, x0
   23150:	mov	x29, sp
   23154:	cbz	x4, 23194 <__gmpn_sub@@Base+0x60>
   23158:	mov	x0, x20
   2315c:	mov	x1, x19
   23160:	mov	x2, x3
   23164:	mov	x3, x21
   23168:	bl	c420 <__gmpn_sub_n@plt>
   2316c:	cbz	x0, 23194 <__gmpn_sub@@Base+0x60>
   23170:	mov	w0, #0x1                   	// #1
   23174:	cmp	x21, x22
   23178:	b.ge	231cc <__gmpn_sub@@Base+0x98>  // b.tcont
   2317c:	lsl	x8, x21, #3
   23180:	ldr	x9, [x19, x8]
   23184:	add	x21, x21, #0x1
   23188:	sub	x10, x9, #0x1
   2318c:	str	x10, [x20, x8]
   23190:	cbz	x9, 23174 <__gmpn_sub@@Base+0x40>
   23194:	cmp	x20, x19
   23198:	mov	x0, xzr
   2319c:	b.eq	231cc <__gmpn_sub@@Base+0x98>  // b.none
   231a0:	cmp	x21, x22
   231a4:	b.ge	231cc <__gmpn_sub@@Base+0x98>  // b.tcont
   231a8:	lsl	x10, x21, #3
   231ac:	sub	x8, x22, x21
   231b0:	add	x9, x20, x10
   231b4:	add	x10, x19, x10
   231b8:	ldr	x11, [x10], #8
   231bc:	subs	x8, x8, #0x1
   231c0:	str	x11, [x9], #8
   231c4:	b.ne	231b8 <__gmpn_sub@@Base+0x84>  // b.any
   231c8:	mov	x0, xzr
   231cc:	ldp	x20, x19, [sp, #32]
   231d0:	ldp	x22, x21, [sp, #16]
   231d4:	ldp	x29, x30, [sp], #48
   231d8:	ret

00000000000231dc <__gmpn_sub_1@@Base>:
   231dc:	ldr	x8, [x1]
   231e0:	subs	x8, x8, x3
   231e4:	str	x8, [x0]
   231e8:	b.cs	23254 <__gmpn_sub_1@@Base+0x78>  // b.hs, b.nlast
   231ec:	mov	x10, #0xfffffffffffffff8    	// #-8
   231f0:	mov	w8, #0x1                   	// #1
   231f4:	mov	w9, #0x1                   	// #1
   231f8:	cmp	x9, x2
   231fc:	b.ge	23284 <__gmpn_sub_1@@Base+0xa8>  // b.tcont
   23200:	lsl	x11, x9, #3
   23204:	ldr	x12, [x1, x11]
   23208:	add	x9, x9, #0x1
   2320c:	sub	x10, x10, #0x8
   23210:	sub	x13, x12, #0x1
   23214:	str	x13, [x0, x11]
   23218:	cbz	x12, 231f8 <__gmpn_sub_1@@Base+0x1c>
   2321c:	cmp	x1, x0
   23220:	mov	x8, xzr
   23224:	b.eq	23284 <__gmpn_sub_1@@Base+0xa8>  // b.none
   23228:	cmp	x9, x2
   2322c:	b.ge	23284 <__gmpn_sub_1@@Base+0xa8>  // b.tcont
   23230:	sub	x8, x1, x10
   23234:	sub	x10, x0, x10
   23238:	ldr	x11, [x8], #8
   2323c:	sub	x2, x2, #0x1
   23240:	cmp	x9, x2
   23244:	str	x11, [x10], #8
   23248:	b.ne	23238 <__gmpn_sub_1@@Base+0x5c>  // b.any
   2324c:	mov	x0, xzr
   23250:	ret
   23254:	cmp	x1, x0
   23258:	mov	x8, xzr
   2325c:	b.eq	23284 <__gmpn_sub_1@@Base+0xa8>  // b.none
   23260:	cmp	x2, #0x2
   23264:	b.lt	23284 <__gmpn_sub_1@@Base+0xa8>  // b.tstop
   23268:	sub	x8, x2, #0x1
   2326c:	add	x9, x0, #0x8
   23270:	add	x10, x1, #0x8
   23274:	ldr	x11, [x10], #8
   23278:	subs	x8, x8, #0x1
   2327c:	str	x11, [x9], #8
   23280:	b.ne	23274 <__gmpn_sub_1@@Base+0x98>  // b.any
   23284:	mov	x0, x8
   23288:	ret
   2328c:	nop

0000000000023290 <__gmpn_sub_nc@@Base>:
   23290:	cmp	xzr, x4
   23294:	b	2329c <__gmpn_sub_n@@Base+0x4>

0000000000023298 <__gmpn_sub_n@@Base>:
   23298:	cmp	xzr, xzr
   2329c:	lsr	x18, x3, #2
   232a0:	tbz	w3, #0, 232e8 <__gmpn_sub_n@@Base+0x50>
   232a4:	ldr	x7, [x1]
   232a8:	ldr	x11, [x2]
   232ac:	sbcs	x13, x7, x11
   232b0:	str	x13, [x0], #8
   232b4:	tbnz	w3, #1, 232d0 <__gmpn_sub_n@@Base+0x38>
   232b8:	cbz	x18, 2334c <__gmpn_sub_n@@Base+0xb4>
   232bc:	ldp	x4, x5, [x1, #8]
   232c0:	ldp	x8, x9, [x2, #8]
   232c4:	sub	x1, x1, #0x8
   232c8:	sub	x2, x2, #0x8
   232cc:	b	23324 <__gmpn_sub_n@@Base+0x8c>
   232d0:	ldp	x6, x7, [x1, #8]
   232d4:	ldp	x10, x11, [x2, #8]
   232d8:	add	x1, x1, #0x8
   232dc:	add	x2, x2, #0x8
   232e0:	cbz	x18, 23340 <__gmpn_sub_n@@Base+0xa8>
   232e4:	b	23310 <__gmpn_sub_n@@Base+0x78>
   232e8:	tbnz	w3, #1, 23300 <__gmpn_sub_n@@Base+0x68>
   232ec:	ldp	x4, x5, [x1]
   232f0:	ldp	x8, x9, [x2]
   232f4:	sub	x1, x1, #0x10
   232f8:	sub	x2, x2, #0x10
   232fc:	b	23324 <__gmpn_sub_n@@Base+0x8c>
   23300:	ldp	x6, x7, [x1]
   23304:	ldp	x10, x11, [x2]
   23308:	cbz	x18, 23340 <__gmpn_sub_n@@Base+0xa8>
   2330c:	nop
   23310:	ldp	x4, x5, [x1, #16]
   23314:	ldp	x8, x9, [x2, #16]
   23318:	sbcs	x12, x6, x10
   2331c:	sbcs	x13, x7, x11
   23320:	stp	x12, x13, [x0], #16
   23324:	ldp	x6, x7, [x1, #32]!
   23328:	ldp	x10, x11, [x2, #32]!
   2332c:	sbcs	x12, x4, x8
   23330:	sbcs	x13, x5, x9
   23334:	stp	x12, x13, [x0], #16
   23338:	sub	x18, x18, #0x1
   2333c:	cbnz	x18, 23310 <__gmpn_sub_n@@Base+0x78>
   23340:	sbcs	x12, x6, x10
   23344:	sbcs	x13, x7, x11
   23348:	stp	x12, x13, [x0]
   2334c:	cset	x0, cc  // cc = lo, ul, last
   23350:	ret
   23354:	nop
   23358:	nop
   2335c:	nop

0000000000023360 <__gmpn_cnd_add_n@@Base>:
   23360:	cmp	x0, #0x1
   23364:	sbc	x0, x0, x0
   23368:	cmn	xzr, xzr
   2336c:	lsr	x18, x4, #2
   23370:	tbz	w4, #0, 233bc <__gmpn_cnd_add_n@@Base+0x5c>
   23374:	ldr	x13, [x3]
   23378:	ldr	x11, [x2]
   2337c:	bic	x7, x13, x0
   23380:	adcs	x9, x11, x7
   23384:	str	x9, [x1]
   23388:	tbnz	w4, #1, 233a8 <__gmpn_cnd_add_n@@Base+0x48>
   2338c:	cbz	x18, 23434 <__gmpn_cnd_add_n@@Base+0xd4>
   23390:	ldp	x12, x13, [x3, #8]
   23394:	ldp	x10, x11, [x2, #8]
   23398:	sub	x2, x2, #0x8
   2339c:	sub	x3, x3, #0x8
   233a0:	sub	x1, x1, #0x18
   233a4:	b	233fc <__gmpn_cnd_add_n@@Base+0x9c>
   233a8:	ldp	x12, x13, [x3, #8]!
   233ac:	ldp	x10, x11, [x2, #8]!
   233b0:	sub	x1, x1, #0x8
   233b4:	cbz	x18, 23420 <__gmpn_cnd_add_n@@Base+0xc0>
   233b8:	b	233e0 <__gmpn_cnd_add_n@@Base+0x80>
   233bc:	ldp	x12, x13, [x3]
   233c0:	ldp	x10, x11, [x2]
   233c4:	tbnz	w4, #1, 233d8 <__gmpn_cnd_add_n@@Base+0x78>
   233c8:	sub	x2, x2, #0x10
   233cc:	sub	x3, x3, #0x10
   233d0:	sub	x1, x1, #0x20
   233d4:	b	233fc <__gmpn_cnd_add_n@@Base+0x9c>
   233d8:	sub	x1, x1, #0x10
   233dc:	cbz	x18, 23420 <__gmpn_cnd_add_n@@Base+0xc0>
   233e0:	bic	x6, x12, x0
   233e4:	bic	x7, x13, x0
   233e8:	ldp	x12, x13, [x3, #16]
   233ec:	adcs	x8, x10, x6
   233f0:	adcs	x9, x11, x7
   233f4:	ldp	x10, x11, [x2, #16]
   233f8:	stp	x8, x9, [x1, #16]
   233fc:	bic	x6, x12, x0
   23400:	bic	x7, x13, x0
   23404:	ldp	x12, x13, [x3, #32]!
   23408:	adcs	x8, x10, x6
   2340c:	adcs	x9, x11, x7
   23410:	ldp	x10, x11, [x2, #32]!
   23414:	stp	x8, x9, [x1, #32]!
   23418:	sub	x18, x18, #0x1
   2341c:	cbnz	x18, 233e0 <__gmpn_cnd_add_n@@Base+0x80>
   23420:	bic	x6, x12, x0
   23424:	bic	x7, x13, x0
   23428:	adcs	x8, x10, x6
   2342c:	adcs	x9, x11, x7
   23430:	stp	x8, x9, [x1, #16]
   23434:	cset	x0, cs  // cs = hs, nlast
   23438:	ret
   2343c:	nop

0000000000023440 <__gmpn_cnd_sub_n@@Base>:
   23440:	cmp	x0, #0x1
   23444:	sbc	x0, x0, x0
   23448:	cmp	xzr, xzr
   2344c:	lsr	x18, x4, #2
   23450:	tbz	w4, #0, 2349c <__gmpn_cnd_sub_n@@Base+0x5c>
   23454:	ldr	x13, [x3]
   23458:	ldr	x11, [x2]
   2345c:	bic	x7, x13, x0
   23460:	sbcs	x9, x11, x7
   23464:	str	x9, [x1]
   23468:	tbnz	w4, #1, 23488 <__gmpn_cnd_sub_n@@Base+0x48>
   2346c:	cbz	x18, 23514 <__gmpn_cnd_sub_n@@Base+0xd4>
   23470:	ldp	x12, x13, [x3, #8]
   23474:	ldp	x10, x11, [x2, #8]
   23478:	sub	x2, x2, #0x8
   2347c:	sub	x3, x3, #0x8
   23480:	sub	x1, x1, #0x18
   23484:	b	234dc <__gmpn_cnd_sub_n@@Base+0x9c>
   23488:	ldp	x12, x13, [x3, #8]!
   2348c:	ldp	x10, x11, [x2, #8]!
   23490:	sub	x1, x1, #0x8
   23494:	cbz	x18, 23500 <__gmpn_cnd_sub_n@@Base+0xc0>
   23498:	b	234c0 <__gmpn_cnd_sub_n@@Base+0x80>
   2349c:	ldp	x12, x13, [x3]
   234a0:	ldp	x10, x11, [x2]
   234a4:	tbnz	w4, #1, 234b8 <__gmpn_cnd_sub_n@@Base+0x78>
   234a8:	sub	x2, x2, #0x10
   234ac:	sub	x3, x3, #0x10
   234b0:	sub	x1, x1, #0x20
   234b4:	b	234dc <__gmpn_cnd_sub_n@@Base+0x9c>
   234b8:	sub	x1, x1, #0x10
   234bc:	cbz	x18, 23500 <__gmpn_cnd_sub_n@@Base+0xc0>
   234c0:	bic	x6, x12, x0
   234c4:	bic	x7, x13, x0
   234c8:	ldp	x12, x13, [x3, #16]
   234cc:	sbcs	x8, x10, x6
   234d0:	sbcs	x9, x11, x7
   234d4:	ldp	x10, x11, [x2, #16]
   234d8:	stp	x8, x9, [x1, #16]
   234dc:	bic	x6, x12, x0
   234e0:	bic	x7, x13, x0
   234e4:	ldp	x12, x13, [x3, #32]!
   234e8:	sbcs	x8, x10, x6
   234ec:	sbcs	x9, x11, x7
   234f0:	ldp	x10, x11, [x2, #32]!
   234f4:	stp	x8, x9, [x1, #32]!
   234f8:	sub	x18, x18, #0x1
   234fc:	cbnz	x18, 234c0 <__gmpn_cnd_sub_n@@Base+0x80>
   23500:	bic	x6, x12, x0
   23504:	bic	x7, x13, x0
   23508:	sbcs	x8, x10, x6
   2350c:	sbcs	x9, x11, x7
   23510:	stp	x8, x9, [x1, #16]
   23514:	cset	x0, cc  // cc = lo, ul, last
   23518:	ret

000000000002351c <__gmpn_cnd_swap@@Base>:
   2351c:	sub	sp, sp, #0x10
   23520:	cmp	x0, #0x0
   23524:	csetm	x8, ne  // ne = any
   23528:	cmp	x3, #0x1
   2352c:	str	x8, [sp, #8]
   23530:	b.lt	23560 <__gmpn_cnd_swap@@Base+0x44>  // b.tstop
   23534:	ldr	x8, [x1]
   23538:	ldr	x9, [x2]
   2353c:	ldr	x10, [sp, #8]
   23540:	subs	x3, x3, #0x1
   23544:	eor	x11, x9, x8
   23548:	and	x10, x11, x10
   2354c:	eor	x8, x10, x8
   23550:	eor	x9, x10, x9
   23554:	str	x8, [x1], #8
   23558:	str	x9, [x2], #8
   2355c:	b.ne	23534 <__gmpn_cnd_swap@@Base+0x18>  // b.any
   23560:	add	sp, sp, #0x10
   23564:	ret

0000000000023568 <__gmpn_neg@@Base>:
   23568:	stp	x29, x30, [sp, #-16]!
   2356c:	ldr	x8, [x1]
   23570:	mov	x29, sp
   23574:	cbz	x8, 235ac <__gmpn_neg@@Base+0x44>
   23578:	neg	x8, x8
   2357c:	subs	x2, x2, #0x1
   23580:	str	x8, [x0]
   23584:	b.eq	23594 <__gmpn_neg@@Base+0x2c>  // b.none
   23588:	add	x0, x0, #0x8
   2358c:	add	x1, x1, #0x8
   23590:	bl	c3e0 <__gmpn_com@plt>
   23594:	mov	w0, #0x1                   	// #1
   23598:	ldp	x29, x30, [sp], #16
   2359c:	ret
   235a0:	ldr	x8, [x1, #8]!
   235a4:	add	x0, x0, #0x8
   235a8:	cbnz	x8, 23578 <__gmpn_neg@@Base+0x10>
   235ac:	subs	x2, x2, #0x1
   235b0:	str	xzr, [x0]
   235b4:	b.ne	235a0 <__gmpn_neg@@Base+0x38>  // b.any
   235b8:	mov	x0, xzr
   235bc:	ldp	x29, x30, [sp], #16
   235c0:	ret
   235c4:	nop
   235c8:	nop
   235cc:	nop

00000000000235d0 <__gmpn_com@@Base>:
   235d0:	cmp	x2, #0x3
   235d4:	b.le	23628 <__gmpn_com@@Base+0x58>
   235d8:	tbz	w0, #3, 235ec <__gmpn_com@@Base+0x1c>
   235dc:	ld1	{v22.1d}, [x1], #8
   235e0:	sub	x2, x2, #0x1
   235e4:	mvn	v22.8b, v22.8b
   235e8:	st1	{v22.1d}, [x0], #8
   235ec:	ld1	{v26.2d}, [x1], #16
   235f0:	subs	x2, x2, #0x6
   235f4:	b.lt	23620 <__gmpn_com@@Base+0x50>  // b.tstop
   235f8:	nop
   235fc:	nop
   23600:	ld1	{v22.2d}, [x1], #16
   23604:	mvn	v26.16b, v26.16b
   23608:	st1	{v26.2d}, [x0], #16
   2360c:	ld1	{v26.2d}, [x1], #16
   23610:	mvn	v22.16b, v22.16b
   23614:	st1	{v22.2d}, [x0], #16
   23618:	subs	x2, x2, #0x4
   2361c:	b.ge	23600 <__gmpn_com@@Base+0x30>  // b.tcont
   23620:	mvn	v26.16b, v26.16b
   23624:	st1	{v26.2d}, [x0], #16
   23628:	tbz	w2, #1, 23638 <__gmpn_com@@Base+0x68>
   2362c:	ld1	{v22.2d}, [x1], #16
   23630:	mvn	v22.16b, v22.16b
   23634:	st1	{v22.2d}, [x0], #16
   23638:	tbz	w2, #0, 23648 <__gmpn_com@@Base+0x78>
   2363c:	ld1	{v22.1d}, [x1]
   23640:	mvn	v22.8b, v22.8b
   23644:	st1	{v22.1d}, [x0]
   23648:	ret
   2364c:	nop

0000000000023650 <__gmpn_mul_1c@@Base>:
   23650:	cmn	xzr, xzr
   23654:	b	2365c <__gmpn_mul_1@@Base+0x4>

0000000000023658 <__gmpn_mul_1@@Base>:
   23658:	adds	x4, xzr, xzr
   2365c:	lsr	x18, x2, #2
   23660:	tbnz	w2, #0, 23690 <__gmpn_mul_1@@Base+0x38>
   23664:	mov	x11, x4
   23668:	tbz	w2, #1, 236ac <__gmpn_mul_1@@Base+0x54>
   2366c:	ldp	x4, x5, [x1]
   23670:	mul	x8, x4, x3
   23674:	umulh	x10, x4, x3
   23678:	cbz	x18, 23688 <__gmpn_mul_1@@Base+0x30>
   2367c:	ldp	x6, x7, [x1, #16]!
   23680:	mul	x9, x5, x3
   23684:	b	236f8 <__gmpn_mul_1@@Base+0xa0>
   23688:	mul	x9, x5, x3
   2368c:	b	2373c <__gmpn_mul_1@@Base+0xe4>
   23690:	ldr	x7, [x1], #8
   23694:	mul	x9, x7, x3
   23698:	umulh	x11, x7, x3
   2369c:	adds	x9, x9, x4
   236a0:	str	x9, [x0], #8
   236a4:	tbnz	w2, #1, 2366c <__gmpn_mul_1@@Base+0x14>
   236a8:	cbz	x18, 2374c <__gmpn_mul_1@@Base+0xf4>
   236ac:	ldp	x6, x7, [x1]
   236b0:	mul	x8, x6, x3
   236b4:	umulh	x10, x6, x3
   236b8:	ldp	x4, x5, [x1, #16]
   236bc:	mul	x9, x7, x3
   236c0:	adcs	x12, x8, x11
   236c4:	umulh	x11, x7, x3
   236c8:	add	x0, x0, #0x10
   236cc:	sub	x18, x18, #0x1
   236d0:	cbz	x18, 23728 <__gmpn_mul_1@@Base+0xd0>
   236d4:	nop
   236d8:	nop
   236dc:	nop
   236e0:	mul	x8, x4, x3
   236e4:	ldp	x6, x7, [x1, #32]!
   236e8:	adcs	x13, x9, x10
   236ec:	umulh	x10, x4, x3
   236f0:	mul	x9, x5, x3
   236f4:	stp	x12, x13, [x0, #-16]
   236f8:	adcs	x12, x8, x11
   236fc:	umulh	x11, x5, x3
   23700:	mul	x8, x6, x3
   23704:	ldp	x4, x5, [x1, #16]
   23708:	adcs	x13, x9, x10
   2370c:	umulh	x10, x6, x3
   23710:	mul	x9, x7, x3
   23714:	stp	x12, x13, [x0], #32
   23718:	adcs	x12, x8, x11
   2371c:	umulh	x11, x7, x3
   23720:	sub	x18, x18, #0x1
   23724:	cbnz	x18, 236e0 <__gmpn_mul_1@@Base+0x88>
   23728:	mul	x8, x4, x3
   2372c:	adcs	x13, x9, x10
   23730:	umulh	x10, x4, x3
   23734:	mul	x9, x5, x3
   23738:	stp	x12, x13, [x0, #-16]
   2373c:	adcs	x12, x8, x11
   23740:	umulh	x11, x5, x3
   23744:	adcs	x13, x9, x10
   23748:	stp	x12, x13, [x0]
   2374c:	adc	x0, x11, xzr
   23750:	ret
   23754:	nop
   23758:	nop
   2375c:	nop

0000000000023760 <__gmpn_addmul_1@@Base>:
   23760:	adds	x15, xzr, xzr
   23764:	tbz	w2, #0, 23784 <__gmpn_addmul_1@@Base+0x24>
   23768:	ldr	x4, [x1], #8
   2376c:	mul	x8, x4, x3
   23770:	umulh	x12, x4, x3
   23774:	ldr	x4, [x0]
   23778:	adds	x8, x4, x8
   2377c:	cinc	x15, x12, cs  // cs = hs, nlast
   23780:	str	x8, [x0], #8
   23784:	tbz	w2, #1, 237bc <__gmpn_addmul_1@@Base+0x5c>
   23788:	ldp	x4, x5, [x1], #16
   2378c:	mul	x8, x4, x3
   23790:	umulh	x12, x4, x3
   23794:	mul	x9, x5, x3
   23798:	umulh	x13, x5, x3
   2379c:	adds	x8, x8, x15
   237a0:	adcs	x9, x9, x12
   237a4:	ldp	x4, x5, [x0]
   237a8:	adc	x15, x13, xzr
   237ac:	adds	x8, x4, x8
   237b0:	adcs	x9, x5, x9
   237b4:	cinc	x15, x15, cs  // cs = hs, nlast
   237b8:	stp	x8, x9, [x0], #16
   237bc:	lsr	x2, x2, #2
   237c0:	cbz	x2, 237d0 <__gmpn_addmul_1@@Base+0x70>
   237c4:	ldp	x4, x5, [x1], #32
   237c8:	ldp	x6, x7, [x1, #-16]
   237cc:	b	23804 <__gmpn_addmul_1@@Base+0xa4>
   237d0:	mov	x0, x15
   237d4:	ret
   237d8:	nop
   237dc:	nop
   237e0:	ldp	x4, x5, [x1], #32
   237e4:	ldp	x6, x7, [x1, #-16]
   237e8:	adds	x8, x16, x8
   237ec:	adcs	x9, x17, x9
   237f0:	stp	x8, x9, [x0], #32
   237f4:	adcs	x10, x12, x10
   237f8:	adcs	x11, x13, x11
   237fc:	stp	x10, x11, [x0, #-16]
   23800:	cinc	x15, x15, cs  // cs = hs, nlast
   23804:	sub	x2, x2, #0x1
   23808:	mul	x8, x4, x3
   2380c:	umulh	x12, x4, x3
   23810:	mul	x9, x5, x3
   23814:	umulh	x13, x5, x3
   23818:	adds	x8, x8, x15
   2381c:	mul	x10, x6, x3
   23820:	umulh	x14, x6, x3
   23824:	adcs	x9, x9, x12
   23828:	mul	x11, x7, x3
   2382c:	umulh	x15, x7, x3
   23830:	adcs	x10, x10, x13
   23834:	ldp	x16, x17, [x0]
   23838:	adcs	x11, x11, x14
   2383c:	ldp	x12, x13, [x0, #16]
   23840:	adc	x15, x15, xzr
   23844:	cbnz	x2, 237e0 <__gmpn_addmul_1@@Base+0x80>
   23848:	adds	x8, x16, x8
   2384c:	adcs	x9, x17, x9
   23850:	adcs	x10, x12, x10
   23854:	adcs	x11, x13, x11
   23858:	stp	x8, x9, [x0]
   2385c:	stp	x10, x11, [x0, #16]
   23860:	cinc	x0, x15, cs  // cs = hs, nlast
   23864:	ret
   23868:	nop
   2386c:	nop

0000000000023870 <__gmpn_submul_1@@Base>:
   23870:	adds	x15, xzr, xzr
   23874:	tbz	w2, #0, 23894 <__gmpn_submul_1@@Base+0x24>
   23878:	ldr	x4, [x1], #8
   2387c:	mul	x8, x4, x3
   23880:	umulh	x12, x4, x3
   23884:	ldr	x4, [x0]
   23888:	subs	x8, x4, x8
   2388c:	cinc	x15, x12, cc  // cc = lo, ul, last
   23890:	str	x8, [x0], #8
   23894:	tbz	w2, #1, 238cc <__gmpn_submul_1@@Base+0x5c>
   23898:	ldp	x4, x5, [x1], #16
   2389c:	mul	x8, x4, x3
   238a0:	umulh	x12, x4, x3
   238a4:	mul	x9, x5, x3
   238a8:	umulh	x13, x5, x3
   238ac:	adds	x8, x8, x15
   238b0:	adcs	x9, x9, x12
   238b4:	ldp	x4, x5, [x0]
   238b8:	adc	x15, x13, xzr
   238bc:	subs	x8, x4, x8
   238c0:	sbcs	x9, x5, x9
   238c4:	cinc	x15, x15, cc  // cc = lo, ul, last
   238c8:	stp	x8, x9, [x0], #16
   238cc:	lsr	x2, x2, #2
   238d0:	cbz	x2, 238e0 <__gmpn_submul_1@@Base+0x70>
   238d4:	ldp	x4, x5, [x1], #32
   238d8:	ldp	x6, x7, [x1, #-16]
   238dc:	b	23914 <__gmpn_submul_1@@Base+0xa4>
   238e0:	mov	x0, x15
   238e4:	ret
   238e8:	nop
   238ec:	nop
   238f0:	ldp	x4, x5, [x1], #32
   238f4:	ldp	x6, x7, [x1, #-16]
   238f8:	subs	x8, x16, x8
   238fc:	sbcs	x9, x17, x9
   23900:	stp	x8, x9, [x0], #32
   23904:	sbcs	x10, x12, x10
   23908:	sbcs	x11, x13, x11
   2390c:	stp	x10, x11, [x0, #-16]
   23910:	cinc	x15, x15, cc  // cc = lo, ul, last
   23914:	sub	x2, x2, #0x1
   23918:	mul	x8, x4, x3
   2391c:	umulh	x12, x4, x3
   23920:	mul	x9, x5, x3
   23924:	umulh	x13, x5, x3
   23928:	adds	x8, x8, x15
   2392c:	mul	x10, x6, x3
   23930:	umulh	x14, x6, x3
   23934:	adcs	x9, x9, x12
   23938:	mul	x11, x7, x3
   2393c:	umulh	x15, x7, x3
   23940:	adcs	x10, x10, x13
   23944:	ldp	x16, x17, [x0]
   23948:	adcs	x11, x11, x14
   2394c:	ldp	x12, x13, [x0, #16]
   23950:	adc	x15, x15, xzr
   23954:	cbnz	x2, 238f0 <__gmpn_submul_1@@Base+0x80>
   23958:	subs	x8, x16, x8
   2395c:	sbcs	x9, x17, x9
   23960:	sbcs	x10, x12, x10
   23964:	sbcs	x11, x13, x11
   23968:	stp	x8, x9, [x0]
   2396c:	stp	x10, x11, [x0, #16]
   23970:	cinc	x0, x15, cc  // cc = lo, ul, last
   23974:	ret

0000000000023978 <__gmpn_add_err1_n@@Base>:
   23978:	mov	x8, xzr
   2397c:	mov	x9, xzr
   23980:	sub	x10, x4, #0x8
   23984:	ldr	x11, [x10, x5, lsl #3]
   23988:	ldr	x12, [x1], #8
   2398c:	ldr	x13, [x2], #8
   23990:	adds	x12, x13, x12
   23994:	cset	w13, cs  // cs = hs, nlast
   23998:	adds	x12, x12, x6
   2399c:	cset	w14, cs  // cs = hs, nlast
   239a0:	orr	w6, w13, w14
   239a4:	cmp	w6, #0x0
   239a8:	csel	x11, x11, xzr, ne  // ne = any
   239ac:	adds	x9, x11, x9
   239b0:	cinc	x8, x8, cs  // cs = hs, nlast
   239b4:	subs	x5, x5, #0x1
   239b8:	str	x12, [x0], #8
   239bc:	b.ne	23984 <__gmpn_add_err1_n@@Base+0xc>  // b.any
   239c0:	mov	x0, x6
   239c4:	stp	x9, x8, [x3]
   239c8:	ret

00000000000239cc <__gmpn_add_err2_n@@Base>:
   239cc:	mov	x8, xzr
   239d0:	mov	x9, xzr
   239d4:	mov	x10, xzr
   239d8:	mov	x11, xzr
   239dc:	sub	x12, x5, #0x8
   239e0:	sub	x13, x4, #0x8
   239e4:	lsl	x14, x6, #3
   239e8:	ldr	x15, [x13, x14]
   239ec:	ldr	x14, [x12, x14]
   239f0:	ldr	x16, [x1], #8
   239f4:	ldr	x17, [x2], #8
   239f8:	adds	x16, x17, x16
   239fc:	cset	w17, cs  // cs = hs, nlast
   23a00:	adds	x16, x16, x7
   23a04:	cset	w18, cs  // cs = hs, nlast
   23a08:	orr	w7, w17, w18
   23a0c:	sbfx	x17, x7, #0, #1
   23a10:	and	x15, x15, x17
   23a14:	and	x14, x14, x17
   23a18:	adds	x11, x15, x11
   23a1c:	cinc	x10, x10, cs  // cs = hs, nlast
   23a20:	adds	x9, x14, x9
   23a24:	cinc	x8, x8, cs  // cs = hs, nlast
   23a28:	subs	x6, x6, #0x1
   23a2c:	str	x16, [x0], #8
   23a30:	b.ne	239e4 <__gmpn_add_err2_n@@Base+0x18>  // b.any
   23a34:	mov	x0, x7
   23a38:	stp	x11, x10, [x3]
   23a3c:	stp	x9, x8, [x3, #16]
   23a40:	ret

0000000000023a44 <__gmpn_add_err3_n@@Base>:
   23a44:	str	x21, [sp, #-32]!
   23a48:	mov	x8, x0
   23a4c:	ldr	x0, [sp, #32]
   23a50:	lsl	x16, x7, #3
   23a54:	sub	x18, x16, #0x8
   23a58:	mov	x14, xzr
   23a5c:	mov	x9, xzr
   23a60:	mov	x10, xzr
   23a64:	mov	x11, xzr
   23a68:	mov	x12, xzr
   23a6c:	mov	x13, xzr
   23a70:	mov	x15, xzr
   23a74:	add	x16, x4, x18
   23a78:	add	x17, x5, x18
   23a7c:	add	x18, x6, x18
   23a80:	stp	x20, x19, [sp, #16]
   23a84:	lsl	x5, x14, #3
   23a88:	ldr	x4, [x16], #-8
   23a8c:	ldr	x6, [x17], #-8
   23a90:	ldr	x19, [x18], #-8
   23a94:	ldr	x20, [x1, x5]
   23a98:	ldr	x21, [x2, x5]
   23a9c:	add	x14, x14, #0x1
   23aa0:	adds	x20, x21, x20
   23aa4:	cset	w21, cs  // cs = hs, nlast
   23aa8:	adds	x20, x20, x0
   23aac:	cset	w0, cs  // cs = hs, nlast
   23ab0:	orr	w0, w21, w0
   23ab4:	sbfx	x21, x0, #0, #1
   23ab8:	and	x4, x4, x21
   23abc:	and	x6, x6, x21
   23ac0:	adds	x15, x4, x15
   23ac4:	and	x19, x19, x21
   23ac8:	cinc	x13, x13, cs  // cs = hs, nlast
   23acc:	adds	x12, x6, x12
   23ad0:	cinc	x11, x11, cs  // cs = hs, nlast
   23ad4:	adds	x10, x19, x10
   23ad8:	cinc	x9, x9, cs  // cs = hs, nlast
   23adc:	cmp	x7, x14
   23ae0:	str	x20, [x8, x5]
   23ae4:	b.ne	23a84 <__gmpn_add_err3_n@@Base+0x40>  // b.any
   23ae8:	stp	x15, x13, [x3]
   23aec:	stp	x12, x11, [x3, #16]
   23af0:	stp	x10, x9, [x3, #32]
   23af4:	ldp	x20, x19, [sp, #16]
   23af8:	ldr	x21, [sp], #32
   23afc:	ret

0000000000023b00 <__gmpn_sub_err1_n@@Base>:
   23b00:	mov	x8, xzr
   23b04:	mov	x9, xzr
   23b08:	sub	x10, x4, #0x8
   23b0c:	ldr	x11, [x10, x5, lsl #3]
   23b10:	ldr	x12, [x1], #8
   23b14:	ldr	x13, [x2], #8
   23b18:	subs	x12, x12, x13
   23b1c:	cset	w13, cc  // cc = lo, ul, last
   23b20:	subs	x12, x12, x6
   23b24:	cset	w14, cc  // cc = lo, ul, last
   23b28:	orr	w6, w13, w14
   23b2c:	cmp	w6, #0x0
   23b30:	csel	x11, x11, xzr, ne  // ne = any
   23b34:	adds	x9, x11, x9
   23b38:	cinc	x8, x8, cs  // cs = hs, nlast
   23b3c:	subs	x5, x5, #0x1
   23b40:	str	x12, [x0], #8
   23b44:	b.ne	23b0c <__gmpn_sub_err1_n@@Base+0xc>  // b.any
   23b48:	mov	x0, x6
   23b4c:	stp	x9, x8, [x3]
   23b50:	ret

0000000000023b54 <__gmpn_sub_err2_n@@Base>:
   23b54:	mov	x8, xzr
   23b58:	mov	x9, xzr
   23b5c:	mov	x10, xzr
   23b60:	mov	x11, xzr
   23b64:	sub	x12, x5, #0x8
   23b68:	sub	x13, x4, #0x8
   23b6c:	lsl	x14, x6, #3
   23b70:	ldr	x15, [x13, x14]
   23b74:	ldr	x14, [x12, x14]
   23b78:	ldr	x16, [x1], #8
   23b7c:	ldr	x17, [x2], #8
   23b80:	subs	x16, x16, x17
   23b84:	cset	w17, cc  // cc = lo, ul, last
   23b88:	subs	x16, x16, x7
   23b8c:	cset	w18, cc  // cc = lo, ul, last
   23b90:	orr	w7, w17, w18
   23b94:	sbfx	x17, x7, #0, #1
   23b98:	and	x15, x15, x17
   23b9c:	and	x14, x14, x17
   23ba0:	adds	x11, x15, x11
   23ba4:	cinc	x10, x10, cs  // cs = hs, nlast
   23ba8:	adds	x9, x14, x9
   23bac:	cinc	x8, x8, cs  // cs = hs, nlast
   23bb0:	subs	x6, x6, #0x1
   23bb4:	str	x16, [x0], #8
   23bb8:	b.ne	23b6c <__gmpn_sub_err2_n@@Base+0x18>  // b.any
   23bbc:	mov	x0, x7
   23bc0:	stp	x11, x10, [x3]
   23bc4:	stp	x9, x8, [x3, #16]
   23bc8:	ret

0000000000023bcc <__gmpn_sub_err3_n@@Base>:
   23bcc:	str	x21, [sp, #-32]!
   23bd0:	mov	x8, x0
   23bd4:	ldr	x0, [sp, #32]
   23bd8:	lsl	x16, x7, #3
   23bdc:	sub	x18, x16, #0x8
   23be0:	mov	x14, xzr
   23be4:	mov	x9, xzr
   23be8:	mov	x10, xzr
   23bec:	mov	x11, xzr
   23bf0:	mov	x12, xzr
   23bf4:	mov	x13, xzr
   23bf8:	mov	x15, xzr
   23bfc:	add	x16, x4, x18
   23c00:	add	x17, x5, x18
   23c04:	add	x18, x6, x18
   23c08:	stp	x20, x19, [sp, #16]
   23c0c:	lsl	x5, x14, #3
   23c10:	ldr	x4, [x16], #-8
   23c14:	ldr	x6, [x17], #-8
   23c18:	ldr	x19, [x18], #-8
   23c1c:	ldr	x20, [x1, x5]
   23c20:	ldr	x21, [x2, x5]
   23c24:	add	x14, x14, #0x1
   23c28:	subs	x20, x20, x21
   23c2c:	cset	w21, cc  // cc = lo, ul, last
   23c30:	subs	x20, x20, x0
   23c34:	cset	w0, cc  // cc = lo, ul, last
   23c38:	orr	w0, w21, w0
   23c3c:	sbfx	x21, x0, #0, #1
   23c40:	and	x4, x4, x21
   23c44:	and	x6, x6, x21
   23c48:	adds	x15, x4, x15
   23c4c:	and	x19, x19, x21
   23c50:	cinc	x13, x13, cs  // cs = hs, nlast
   23c54:	adds	x12, x6, x12
   23c58:	cinc	x11, x11, cs  // cs = hs, nlast
   23c5c:	adds	x10, x19, x10
   23c60:	cinc	x9, x9, cs  // cs = hs, nlast
   23c64:	cmp	x7, x14
   23c68:	str	x20, [x8, x5]
   23c6c:	b.ne	23c0c <__gmpn_sub_err3_n@@Base+0x40>  // b.any
   23c70:	stp	x15, x13, [x3]
   23c74:	stp	x12, x11, [x3, #16]
   23c78:	stp	x10, x9, [x3, #32]
   23c7c:	ldp	x20, x19, [sp, #16]
   23c80:	ldr	x21, [sp], #32
   23c84:	ret
   23c88:	nop
   23c8c:	nop

0000000000023c90 <__gmpn_lshift@@Base>:
   23c90:	add	x16, x0, x2, lsl #3
   23c94:	add	x1, x1, x2, lsl #3
   23c98:	neg	x8, x3
   23c9c:	lsr	x18, x2, #2
   23ca0:	tbz	w2, #0, 23ce0 <__gmpn_lshift@@Base+0x50>
   23ca4:	ldur	x4, [x1, #-8]
   23ca8:	tbnz	w2, #1, 23cd0 <__gmpn_lshift@@Base+0x40>
   23cac:	lsr	x0, x4, x8
   23cb0:	lsl	x2, x4, x3
   23cb4:	cbnz	x18, 23cc0 <__gmpn_lshift@@Base+0x30>
   23cb8:	stur	x2, [x16, #-8]
   23cbc:	ret
   23cc0:	ldp	x4, x5, [x1, #-24]
   23cc4:	sub	x1, x1, #0x8
   23cc8:	add	x16, x16, #0x10
   23ccc:	b	23d54 <__gmpn_lshift@@Base+0xc4>
   23cd0:	lsr	x0, x4, x8
   23cd4:	lsl	x2, x4, x3
   23cd8:	ldp	x6, x7, [x1, #-24]!
   23cdc:	b	23d78 <__gmpn_lshift@@Base+0xe8>
   23ce0:	ldp	x4, x5, [x1, #-16]
   23ce4:	tbz	w2, #1, 23d20 <__gmpn_lshift@@Base+0x90>
   23ce8:	lsr	x0, x5, x8
   23cec:	lsl	x13, x5, x3
   23cf0:	lsr	x10, x4, x8
   23cf4:	lsl	x2, x4, x3
   23cf8:	cbnz	x18, 23d08 <__gmpn_lshift@@Base+0x78>
   23cfc:	orr	x10, x10, x13
   23d00:	stp	x2, x10, [x16, #-16]
   23d04:	ret
   23d08:	ldp	x4, x5, [x1, #-32]
   23d0c:	orr	x10, x10, x13
   23d10:	stur	x10, [x16, #-8]
   23d14:	sub	x1, x1, #0x10
   23d18:	add	x16, x16, #0x8
   23d1c:	b	23d54 <__gmpn_lshift@@Base+0xc4>
   23d20:	lsr	x0, x5, x8
   23d24:	lsl	x13, x5, x3
   23d28:	lsr	x10, x4, x8
   23d2c:	lsl	x2, x4, x3
   23d30:	ldp	x6, x7, [x1, #-32]!
   23d34:	orr	x10, x10, x13
   23d38:	str	x10, [x16, #-8]!
   23d3c:	b	23d74 <__gmpn_lshift@@Base+0xe4>
   23d40:	ldp	x4, x5, [x1, #-16]
   23d44:	orr	x10, x10, x13
   23d48:	orr	x11, x12, x2
   23d4c:	stp	x10, x11, [x16, #-16]
   23d50:	lsl	x2, x6, x3
   23d54:	lsr	x10, x4, x8
   23d58:	lsl	x13, x5, x3
   23d5c:	lsr	x12, x5, x8
   23d60:	ldp	x6, x7, [x1, #-32]!
   23d64:	orr	x10, x10, x13
   23d68:	orr	x11, x12, x2
   23d6c:	stp	x10, x11, [x16, #-32]!
   23d70:	lsl	x2, x4, x3
   23d74:	sub	x18, x18, #0x1
   23d78:	lsr	x10, x6, x8
   23d7c:	lsl	x13, x7, x3
   23d80:	lsr	x12, x7, x8
   23d84:	cbnz	x18, 23d40 <__gmpn_lshift@@Base+0xb0>
   23d88:	orr	x10, x10, x13
   23d8c:	orr	x11, x12, x2
   23d90:	lsl	x2, x6, x3
   23d94:	stp	x10, x11, [x16, #-16]
   23d98:	stur	x2, [x16, #-24]
   23d9c:	ret

0000000000023da0 <__gmpn_rshift@@Base>:
   23da0:	mov	x16, x0
   23da4:	neg	x8, x3
   23da8:	lsr	x18, x2, #2
   23dac:	tbz	w2, #0, 23df0 <__gmpn_rshift@@Base+0x50>
   23db0:	ldr	x5, [x1]
   23db4:	tbnz	w2, #1, 23ddc <__gmpn_rshift@@Base+0x3c>
   23db8:	lsl	x0, x5, x8
   23dbc:	lsr	x2, x5, x3
   23dc0:	cbnz	x18, 23dcc <__gmpn_rshift@@Base+0x2c>
   23dc4:	str	x2, [x16]
   23dc8:	ret
   23dcc:	ldp	x4, x5, [x1, #8]
   23dd0:	sub	x1, x1, #0x8
   23dd4:	sub	x16, x16, #0x20
   23dd8:	b	23e64 <__gmpn_rshift@@Base+0xc4>
   23ddc:	lsl	x0, x5, x8
   23de0:	lsr	x2, x5, x3
   23de4:	ldp	x6, x7, [x1, #8]!
   23de8:	sub	x16, x16, #0x10
   23dec:	b	23e88 <__gmpn_rshift@@Base+0xe8>
   23df0:	ldp	x4, x5, [x1]
   23df4:	tbz	w2, #1, 23e28 <__gmpn_rshift@@Base+0x88>
   23df8:	lsl	x0, x4, x8
   23dfc:	lsr	x13, x4, x3
   23e00:	lsl	x10, x5, x8
   23e04:	lsr	x2, x5, x3
   23e08:	cbnz	x18, 23e18 <__gmpn_rshift@@Base+0x78>
   23e0c:	orr	x10, x10, x13
   23e10:	stp	x10, x2, [x16]
   23e14:	ret
   23e18:	ldp	x4, x5, [x1, #16]
   23e1c:	orr	x10, x10, x13
   23e20:	str	x10, [x16], #-24
   23e24:	b	23e64 <__gmpn_rshift@@Base+0xc4>
   23e28:	lsl	x0, x4, x8
   23e2c:	lsr	x13, x4, x3
   23e30:	lsl	x10, x5, x8
   23e34:	lsr	x2, x5, x3
   23e38:	ldp	x6, x7, [x1, #16]!
   23e3c:	orr	x10, x10, x13
   23e40:	str	x10, [x16], #-8
   23e44:	b	23e84 <__gmpn_rshift@@Base+0xe4>
   23e48:	nop
   23e4c:	nop
   23e50:	ldp	x4, x5, [x1, #16]
   23e54:	orr	x10, x10, x13
   23e58:	orr	x11, x12, x2
   23e5c:	stp	x11, x10, [x16, #16]
   23e60:	lsr	x2, x7, x3
   23e64:	lsl	x10, x5, x8
   23e68:	lsl	x12, x4, x8
   23e6c:	lsr	x13, x4, x3
   23e70:	ldp	x6, x7, [x1, #32]!
   23e74:	orr	x10, x10, x13
   23e78:	orr	x11, x12, x2
   23e7c:	stp	x11, x10, [x16, #32]!
   23e80:	lsr	x2, x5, x3
   23e84:	sub	x18, x18, #0x1
   23e88:	lsl	x10, x7, x8
   23e8c:	lsl	x12, x6, x8
   23e90:	lsr	x13, x6, x3
   23e94:	cbnz	x18, 23e50 <__gmpn_rshift@@Base+0xb0>
   23e98:	orr	x10, x10, x13
   23e9c:	orr	x11, x12, x2
   23ea0:	lsr	x2, x7, x3
   23ea4:	stp	x11, x10, [x16, #16]
   23ea8:	str	x2, [x16, #32]
   23eac:	ret

0000000000023eb0 <__gmpn_divexact_1@@Base>:
   23eb0:	rbit	x8, x3
   23eb4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   23eb8:	tst	x3, #0x1
   23ebc:	ldr	x9, [x9, #3952]
   23ec0:	clz	x10, x8
   23ec4:	csel	x8, x10, xzr, eq  // eq = none
   23ec8:	lsr	x8, x3, x8
   23ecc:	ubfx	x11, x8, #1, #7
   23ed0:	ldrb	w9, [x9, x11]
   23ed4:	mov	w12, #0x2                   	// #2
   23ed8:	msub	x11, x8, x9, x12
   23edc:	mul	x9, x11, x9
   23ee0:	msub	x13, x9, x8, x12
   23ee4:	ldr	x11, [x1]
   23ee8:	mul	x9, x9, x13
   23eec:	msub	x12, x9, x8, x12
   23ef0:	mul	x9, x9, x12
   23ef4:	cbz	x10, 23f54 <__gmpn_divexact_1@@Base+0xa4>
   23ef8:	tbnz	w3, #0, 23f54 <__gmpn_divexact_1@@Base+0xa4>
   23efc:	cmp	x2, #0x2
   23f00:	b.lt	23f9c <__gmpn_divexact_1@@Base+0xec>  // b.tstop
   23f04:	mov	w14, #0x40                  	// #64
   23f08:	mov	x12, xzr
   23f0c:	sub	x13, x2, #0x1
   23f10:	sub	w14, w14, w10
   23f14:	add	x15, x1, #0x8
   23f18:	mov	x16, x0
   23f1c:	mov	x17, x11
   23f20:	ldr	x11, [x15], #8
   23f24:	lsr	x17, x17, x10
   23f28:	lsl	x18, x11, x14
   23f2c:	orr	x17, x18, x17
   23f30:	subs	x12, x17, x12
   23f34:	mul	x17, x12, x9
   23f38:	umulh	x12, x17, x8
   23f3c:	cinc	x12, x12, cc  // cc = lo, ul, last
   23f40:	subs	x13, x13, #0x1
   23f44:	str	x17, [x16], #8
   23f48:	mov	x17, x11
   23f4c:	b.ne	23f20 <__gmpn_divexact_1@@Base+0x70>  // b.any
   23f50:	b	23fa0 <__gmpn_divexact_1@@Base+0xf0>
   23f54:	mul	x10, x9, x11
   23f58:	cmp	x2, #0x2
   23f5c:	str	x10, [x0]
   23f60:	b.lt	23f98 <__gmpn_divexact_1@@Base+0xe8>  // b.tstop
   23f64:	mov	x12, xzr
   23f68:	sub	x11, x2, #0x1
   23f6c:	add	x13, x0, #0x8
   23f70:	add	x14, x1, #0x8
   23f74:	ldr	x15, [x14], #8
   23f78:	umulh	x10, x10, x8
   23f7c:	add	x10, x10, x12
   23f80:	subs	x10, x15, x10
   23f84:	mul	x10, x10, x9
   23f88:	cset	w12, cc  // cc = lo, ul, last
   23f8c:	subs	x11, x11, #0x1
   23f90:	str	x10, [x13], #8
   23f94:	b.ne	23f74 <__gmpn_divexact_1@@Base+0xc4>  // b.any
   23f98:	ret
   23f9c:	mov	x12, xzr
   23fa0:	lsr	x8, x11, x10
   23fa4:	sub	x8, x8, x12
   23fa8:	mul	x8, x8, x9
   23fac:	add	x9, x0, x2, lsl #3
   23fb0:	stur	x8, [x9, #-8]
   23fb4:	ret

0000000000023fb8 <__gmpn_divexact_by3c@@Base>:
   23fb8:	stp	x29, x30, [sp, #-16]!
   23fbc:	mov	x8, #0x5555555555555555    	// #6148914691236517205
   23fc0:	mul	x4, x3, x8
   23fc4:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   23fc8:	mov	x29, sp
   23fcc:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   23fd0:	and	x0, x0, #0x3
   23fd4:	ldp	x29, x30, [sp], #16
   23fd8:	ret

0000000000023fdc <__gmpn_divisible_p@@Base>:
   23fdc:	stp	x29, x30, [sp, #-80]!
   23fe0:	stp	x26, x25, [sp, #16]
   23fe4:	stp	x24, x23, [sp, #32]
   23fe8:	stp	x22, x21, [sp, #48]
   23fec:	stp	x20, x19, [sp, #64]
   23ff0:	mov	x29, sp
   23ff4:	sub	sp, sp, #0x10
   23ff8:	mov	x21, x1
   23ffc:	cmp	x1, x3
   24000:	b.ge	24010 <__gmpn_divisible_p@@Base+0x34>  // b.tcont
   24004:	cmp	x21, #0x0
   24008:	cset	w23, eq  // eq = none
   2400c:	b	24078 <__gmpn_divisible_p@@Base+0x9c>
   24010:	mov	x20, x2
   24014:	ldr	x2, [x2]
   24018:	ldr	x8, [x0]
   2401c:	mov	x19, x3
   24020:	mov	x22, x0
   24024:	cbz	x2, 24070 <__gmpn_divisible_p@@Base+0x94>
   24028:	neg	x9, x2
   2402c:	and	x9, x2, x9
   24030:	sub	x9, x9, #0x1
   24034:	tst	x9, x8
   24038:	b.ne	24074 <__gmpn_divisible_p@@Base+0x98>  // b.any
   2403c:	cmp	x19, #0x1
   24040:	b.ne	24098 <__gmpn_divisible_p@@Base+0xbc>  // b.any
   24044:	cmp	x21, #0x28
   24048:	b.lt	240bc <__gmpn_divisible_p@@Base+0xe0>  // b.tstop
   2404c:	mov	x0, x22
   24050:	mov	x1, x21
   24054:	bl	c540 <__gmpn_mod_1@plt>
   24058:	b	240d8 <__gmpn_divisible_p@@Base+0xfc>
   2405c:	ldr	x8, [x22, #8]!
   24060:	ldr	x2, [x20, #8]!
   24064:	sub	x21, x21, #0x1
   24068:	sub	x19, x19, #0x1
   2406c:	cbnz	x2, 24028 <__gmpn_divisible_p@@Base+0x4c>
   24070:	cbz	x8, 2405c <__gmpn_divisible_p@@Base+0x80>
   24074:	mov	w23, wzr
   24078:	mov	w0, w23
   2407c:	mov	sp, x29
   24080:	ldp	x20, x19, [sp, #64]
   24084:	ldp	x22, x21, [sp, #48]
   24088:	ldp	x24, x23, [sp, #32]
   2408c:	ldp	x26, x25, [sp, #16]
   24090:	ldp	x29, x30, [sp], #80
   24094:	ret
   24098:	rbit	x8, x2
   2409c:	cmp	x19, #0x2
   240a0:	clz	x24, x8
   240a4:	b.ne	24124 <__gmpn_divisible_p@@Base+0x148>  // b.any
   240a8:	ldr	x8, [x20, #8]
   240ac:	cmp	x8, x9
   240b0:	b.ls	240e4 <__gmpn_divisible_p@@Base+0x108>  // b.plast
   240b4:	mov	w8, #0x1                   	// #1
   240b8:	b	24120 <__gmpn_divisible_p@@Base+0x144>
   240bc:	rbit	x8, x2
   240c0:	clz	x8, x8
   240c4:	lsr	x2, x2, x8
   240c8:	mov	x0, x22
   240cc:	mov	x1, x21
   240d0:	mov	x3, xzr
   240d4:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   240d8:	cmp	x0, #0x0
   240dc:	cset	w23, eq  // eq = none
   240e0:	b	24078 <__gmpn_divisible_p@@Base+0x9c>
   240e4:	neg	x10, x24
   240e8:	lsr	x9, x2, x24
   240ec:	lsl	x8, x8, x10
   240f0:	cmp	x21, #0x27
   240f4:	orr	x2, x8, x9
   240f8:	mov	x0, x22
   240fc:	mov	x1, x21
   24100:	b.le	2410c <__gmpn_divisible_p@@Base+0x130>
   24104:	bl	c540 <__gmpn_mod_1@plt>
   24108:	b	24114 <__gmpn_divisible_p@@Base+0x138>
   2410c:	mov	x3, xzr
   24110:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   24114:	cmp	x0, #0x0
   24118:	mov	w8, wzr
   2411c:	cset	w23, eq  // eq = none
   24120:	cbz	w8, 24078 <__gmpn_divisible_p@@Base+0x9c>
   24124:	add	x26, x21, #0x1
   24128:	sub	x8, x21, x19
   2412c:	add	x8, x8, x26
   24130:	lsl	x8, x8, #3
   24134:	add	x1, x8, #0x8
   24138:	mov	w8, #0x7f00                	// #32512
   2413c:	cmp	x1, x8
   24140:	stur	xzr, [x29, #-8]
   24144:	b.hi	241b4 <__gmpn_divisible_p@@Base+0x1d8>  // b.pmore
   24148:	add	x9, x1, #0xf
   2414c:	mov	x8, sp
   24150:	and	x9, x9, #0xfffffffffffffff0
   24154:	sub	x23, x8, x9
   24158:	mov	sp, x23
   2415c:	cbz	w24, 241c4 <__gmpn_divisible_p@@Base+0x1e8>
   24160:	lsl	x1, x19, #3
   24164:	mov	w8, #0x7f00                	// #32512
   24168:	cmp	x1, x8
   2416c:	b.hi	24374 <__gmpn_divisible_p@@Base+0x398>  // b.pmore
   24170:	add	x9, x1, #0xf
   24174:	mov	x8, sp
   24178:	and	x9, x9, #0xfffffffffffffff0
   2417c:	sub	x25, x8, x9
   24180:	mov	sp, x25
   24184:	mov	x0, x25
   24188:	mov	x1, x20
   2418c:	mov	x2, x19
   24190:	mov	w3, w24
   24194:	bl	c2f0 <__gmpn_rshift@plt>
   24198:	mov	x0, x23
   2419c:	mov	x1, x22
   241a0:	mov	x2, x21
   241a4:	mov	w3, w24
   241a8:	bl	c2f0 <__gmpn_rshift@plt>
   241ac:	mov	x20, x25
   241b0:	b	241d4 <__gmpn_divisible_p@@Base+0x1f8>
   241b4:	sub	x0, x29, #0x8
   241b8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   241bc:	mov	x23, x0
   241c0:	cbnz	w24, 24160 <__gmpn_divisible_p@@Base+0x184>
   241c4:	mov	x0, x23
   241c8:	mov	x1, x22
   241cc:	mov	x2, x21
   241d0:	bl	cc10 <__gmpn_copyi@plt>
   241d4:	add	x8, x23, x21, lsl #3
   241d8:	add	x9, x20, x19, lsl #3
   241dc:	ldur	x8, [x8, #-8]
   241e0:	ldur	x9, [x9, #-8]
   241e4:	cmp	x8, x9
   241e8:	b.cs	24208 <__gmpn_divisible_p@@Base+0x22c>  // b.hs, b.nlast
   241ec:	cmp	x21, x19
   241f0:	b.ne	24210 <__gmpn_divisible_p@@Base+0x234>  // b.any
   241f4:	ldur	x0, [x29, #-8]
   241f8:	cbz	x0, 24074 <__gmpn_divisible_p@@Base+0x98>
   241fc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   24200:	mov	w23, wzr
   24204:	b	24078 <__gmpn_divisible_p@@Base+0x9c>
   24208:	str	xzr, [x23, x21, lsl #3]
   2420c:	mov	x21, x26
   24210:	cmp	x19, #0x27
   24214:	add	x22, x23, x26, lsl #3
   24218:	b.lt	24280 <__gmpn_divisible_p@@Base+0x2a4>  // b.tstop
   2421c:	sub	x24, x21, x19
   24220:	cmp	x24, #0x26
   24224:	b.le	24280 <__gmpn_divisible_p@@Base+0x2a4>
   24228:	cmp	x19, #0x326
   2422c:	b.le	24320 <__gmpn_divisible_p@@Base+0x344>
   24230:	mov	x0, x21
   24234:	mov	x1, x19
   24238:	bl	d350 <__gmpn_mu_bdiv_qr_itch@plt>
   2423c:	lsl	x1, x0, #3
   24240:	mov	w8, #0x7f00                	// #32512
   24244:	cmp	x1, x8
   24248:	b.hi	24384 <__gmpn_divisible_p@@Base+0x3a8>  // b.pmore
   2424c:	add	x9, x1, #0xf
   24250:	mov	x8, sp
   24254:	and	x9, x9, #0xfffffffffffffff0
   24258:	sub	x6, x8, x9
   2425c:	mov	sp, x6
   24260:	mov	x0, x22
   24264:	mov	x1, x23
   24268:	mov	x2, x23
   2426c:	mov	x3, x21
   24270:	mov	x4, x20
   24274:	mov	x5, x19
   24278:	bl	ce70 <__gmpn_mu_bdiv_qr@plt>
   2427c:	b	242d4 <__gmpn_divisible_p@@Base+0x2f8>
   24280:	ldr	x8, [x20]
   24284:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   24288:	ldr	x9, [x9, #3952]
   2428c:	mov	x0, x22
   24290:	ubfx	x10, x8, #1, #7
   24294:	mov	x1, x23
   24298:	ldrb	w9, [x9, x10]
   2429c:	mov	w10, #0x2                   	// #2
   242a0:	mov	x2, x21
   242a4:	mov	x3, x20
   242a8:	msub	x11, x8, x9, x10
   242ac:	mul	x9, x11, x9
   242b0:	msub	x10, x9, x8, x10
   242b4:	mul	x9, x9, x10
   242b8:	orr	x10, xzr, #0xfffffffffffffffe
   242bc:	madd	x8, x9, x8, x10
   242c0:	mul	x5, x8, x9
   242c4:	mov	x4, x19
   242c8:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   242cc:	sub	x8, x21, x19
   242d0:	add	x23, x23, x8, lsl #3
   242d4:	sub	x8, x23, #0x8
   242d8:	sub	x9, x20, #0x8
   242dc:	subs	x10, x19, #0x1
   242e0:	b.lt	2430c <__gmpn_divisible_p@@Base+0x330>  // b.tstop
   242e4:	lsl	x11, x19, #3
   242e8:	ldr	x12, [x8, x11]
   242ec:	ldr	x11, [x9, x11]
   242f0:	mov	x19, x10
   242f4:	cmp	x12, x11
   242f8:	b.eq	242dc <__gmpn_divisible_p@@Base+0x300>  // b.none
   242fc:	mov	w23, wzr
   24300:	ldur	x0, [x29, #-8]
   24304:	cbz	x0, 24078 <__gmpn_divisible_p@@Base+0x9c>
   24308:	b	24318 <__gmpn_divisible_p@@Base+0x33c>
   2430c:	mov	w23, #0x1                   	// #1
   24310:	ldur	x0, [x29, #-8]
   24314:	cbz	x0, 24078 <__gmpn_divisible_p@@Base+0x9c>
   24318:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2431c:	b	24078 <__gmpn_divisible_p@@Base+0x9c>
   24320:	ldr	x8, [x20]
   24324:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   24328:	ldr	x9, [x9, #3952]
   2432c:	mov	x0, x22
   24330:	ubfx	x10, x8, #1, #7
   24334:	mov	x1, x23
   24338:	ldrb	w9, [x9, x10]
   2433c:	mov	w10, #0x2                   	// #2
   24340:	mov	x2, x21
   24344:	mov	x3, x20
   24348:	msub	x11, x8, x9, x10
   2434c:	mul	x9, x11, x9
   24350:	msub	x10, x9, x8, x10
   24354:	mul	x9, x9, x10
   24358:	orr	x10, xzr, #0xfffffffffffffffe
   2435c:	madd	x8, x9, x8, x10
   24360:	mul	x5, x8, x9
   24364:	mov	x4, x19
   24368:	bl	c770 <__gmpn_dcpi1_bdiv_qr@plt>
   2436c:	add	x23, x23, x24, lsl #3
   24370:	b	242d4 <__gmpn_divisible_p@@Base+0x2f8>
   24374:	sub	x0, x29, #0x8
   24378:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2437c:	mov	x25, x0
   24380:	b	24184 <__gmpn_divisible_p@@Base+0x1a8>
   24384:	sub	x0, x29, #0x8
   24388:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2438c:	mov	x6, x0
   24390:	b	24260 <__gmpn_divisible_p@@Base+0x284>

0000000000024394 <__gmpn_divrem@@Base>:
   24394:	stp	x29, x30, [sp, #-96]!
   24398:	stp	x28, x27, [sp, #16]
   2439c:	stp	x26, x25, [sp, #32]
   243a0:	stp	x24, x23, [sp, #48]
   243a4:	stp	x22, x21, [sp, #64]
   243a8:	stp	x20, x19, [sp, #80]
   243ac:	mov	x29, sp
   243b0:	sub	sp, sp, #0x10
   243b4:	mov	x21, x4
   243b8:	mov	x22, x3
   243bc:	mov	x20, x2
   243c0:	mov	x24, x1
   243c4:	cmp	x5, #0x2
   243c8:	mov	x19, x0
   243cc:	b.eq	24448 <__gmpn_divrem@@Base+0xb4>  // b.none
   243d0:	mov	x23, x5
   243d4:	cmp	x5, #0x1
   243d8:	b.ne	24468 <__gmpn_divrem@@Base+0xd4>  // b.any
   243dc:	add	x25, x22, x24
   243e0:	lsl	x1, x25, #3
   243e4:	mov	w8, #0x7f00                	// #32512
   243e8:	cmp	x1, x8
   243ec:	stur	xzr, [x29, #-8]
   243f0:	b.hi	24508 <__gmpn_divrem@@Base+0x174>  // b.pmore
   243f4:	add	x9, x1, #0xf
   243f8:	mov	x8, sp
   243fc:	and	x9, x9, #0xfffffffffffffff0
   24400:	sub	x23, x8, x9
   24404:	mov	sp, x23
   24408:	ldr	x4, [x21]
   2440c:	mov	x0, x23
   24410:	mov	x1, x24
   24414:	mov	x2, x20
   24418:	mov	x3, x22
   2441c:	bl	ced0 <__gmpn_divrem_1@plt>
   24420:	str	x0, [x20]
   24424:	sub	x20, x25, #0x1
   24428:	mov	x0, x19
   2442c:	mov	x1, x23
   24430:	mov	x2, x20
   24434:	bl	cc10 <__gmpn_copyi@plt>
   24438:	ldur	x0, [x29, #-8]
   2443c:	ldr	x19, [x23, x20, lsl #3]
   24440:	cbz	x0, 244dc <__gmpn_divrem@@Base+0x148>
   24444:	b	24500 <__gmpn_divrem@@Base+0x16c>
   24448:	mov	x0, x19
   2444c:	mov	x1, x24
   24450:	mov	x2, x20
   24454:	mov	x3, x22
   24458:	mov	x4, x21
   2445c:	bl	c350 <__gmpn_divrem_2@plt>
   24460:	mov	x19, x0
   24464:	b	244dc <__gmpn_divrem@@Base+0x148>
   24468:	stur	xzr, [x29, #-8]
   2446c:	cbnz	x24, 24518 <__gmpn_divrem@@Base+0x184>
   24470:	sub	x24, x22, x23
   24474:	lsl	x8, x24, #3
   24478:	add	x1, x8, #0x8
   2447c:	mov	w8, #0x7f00                	// #32512
   24480:	cmp	x1, x8
   24484:	b.hi	245b0 <__gmpn_divrem@@Base+0x21c>  // b.pmore
   24488:	add	x9, x1, #0xf
   2448c:	mov	x8, sp
   24490:	and	x9, x9, #0xfffffffffffffff0
   24494:	sub	x25, x8, x9
   24498:	mov	sp, x25
   2449c:	mov	x0, x25
   244a0:	mov	x1, x20
   244a4:	mov	x2, xzr
   244a8:	mov	x3, x20
   244ac:	mov	x4, x22
   244b0:	mov	x5, x21
   244b4:	mov	x6, x23
   244b8:	bl	c030 <__gmpn_tdiv_qr@plt>
   244bc:	mov	x0, x19
   244c0:	mov	x1, x25
   244c4:	mov	x2, x24
   244c8:	bl	cc10 <__gmpn_copyi@plt>
   244cc:	add	x8, x25, x24, lsl #3
   244d0:	ldur	x0, [x29, #-8]
   244d4:	ldr	x19, [x8]
   244d8:	cbnz	x0, 24500 <__gmpn_divrem@@Base+0x16c>
   244dc:	mov	x0, x19
   244e0:	mov	sp, x29
   244e4:	ldp	x20, x19, [sp, #80]
   244e8:	ldp	x22, x21, [sp, #64]
   244ec:	ldp	x24, x23, [sp, #48]
   244f0:	ldp	x26, x25, [sp, #32]
   244f4:	ldp	x28, x27, [sp, #16]
   244f8:	ldp	x29, x30, [sp], #96
   244fc:	ret
   24500:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   24504:	b	244dc <__gmpn_divrem@@Base+0x148>
   24508:	sub	x0, x29, #0x8
   2450c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   24510:	mov	x23, x0
   24514:	b	24408 <__gmpn_divrem@@Base+0x74>
   24518:	sub	x8, x22, x23
   2451c:	add	x26, x22, x24
   24520:	add	x25, x8, x24
   24524:	add	x8, x26, x25
   24528:	lsl	x8, x8, #3
   2452c:	add	x1, x8, #0x8
   24530:	mov	w8, #0x7f00                	// #32512
   24534:	cmp	x1, x8
   24538:	b.hi	245c0 <__gmpn_divrem@@Base+0x22c>  // b.pmore
   2453c:	add	x9, x1, #0xf
   24540:	mov	x8, sp
   24544:	and	x9, x9, #0xfffffffffffffff0
   24548:	sub	x27, x8, x9
   2454c:	mov	sp, x27
   24550:	lsl	x24, x24, #3
   24554:	mov	x0, x27
   24558:	mov	w1, wzr
   2455c:	mov	x2, x24
   24560:	bl	c780 <memset@plt>
   24564:	add	x0, x27, x24
   24568:	mov	x1, x20
   2456c:	mov	x2, x22
   24570:	add	x28, x27, x26, lsl #3
   24574:	bl	cc10 <__gmpn_copyi@plt>
   24578:	mov	x0, x28
   2457c:	mov	x1, x20
   24580:	mov	x2, xzr
   24584:	mov	x3, x27
   24588:	mov	x4, x26
   2458c:	mov	x5, x21
   24590:	mov	x6, x23
   24594:	bl	c030 <__gmpn_tdiv_qr@plt>
   24598:	mov	x0, x19
   2459c:	mov	x1, x28
   245a0:	mov	x2, x25
   245a4:	bl	cc10 <__gmpn_copyi@plt>
   245a8:	add	x8, x28, x25, lsl #3
   245ac:	b	244d0 <__gmpn_divrem@@Base+0x13c>
   245b0:	sub	x0, x29, #0x8
   245b4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   245b8:	mov	x25, x0
   245bc:	b	2449c <__gmpn_divrem@@Base+0x108>
   245c0:	sub	x0, x29, #0x8
   245c4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   245c8:	mov	x27, x0
   245cc:	b	24550 <__gmpn_divrem@@Base+0x1bc>

00000000000245d0 <__gmpn_divrem_1@@Base>:
   245d0:	stp	x29, x30, [sp, #-80]!
   245d4:	adds	x8, x3, x1
   245d8:	str	x25, [sp, #16]
   245dc:	stp	x24, x23, [sp, #32]
   245e0:	stp	x22, x21, [sp, #48]
   245e4:	stp	x20, x19, [sp, #64]
   245e8:	mov	x29, sp
   245ec:	b.eq	24638 <__gmpn_divrem_1@@Base+0x68>  // b.none
   245f0:	sub	x9, x8, #0x1
   245f4:	mov	x20, x4
   245f8:	mov	x21, x3
   245fc:	mov	x23, x2
   24600:	mov	x19, x1
   24604:	add	x24, x0, x9, lsl #3
   24608:	tbnz	x4, #63, 246f4 <__gmpn_divrem_1@@Base+0x124>
   2460c:	cbz	x21, 24640 <__gmpn_divrem_1@@Base+0x70>
   24610:	sub	x10, x21, #0x1
   24614:	ldr	x22, [x23, x10, lsl #3]
   24618:	cmp	x22, x20
   2461c:	b.cs	24640 <__gmpn_divrem_1@@Base+0x70>  // b.hs, b.nlast
   24620:	str	xzr, [x24]
   24624:	cbz	x9, 24c10 <__gmpn_divrem_1@@Base+0x640>
   24628:	sub	x24, x24, #0x8
   2462c:	mov	x8, x9
   24630:	mov	x21, x10
   24634:	b	24644 <__gmpn_divrem_1@@Base+0x74>
   24638:	mov	x22, xzr
   2463c:	b	24c10 <__gmpn_divrem_1@@Base+0x640>
   24640:	mov	x22, xzr
   24644:	clz	x25, x20
   24648:	cmp	x8, #0x3
   2464c:	lsl	x20, x20, x25
   24650:	lsl	x22, x22, x25
   24654:	b.le	247d0 <__gmpn_divrem_1@@Base+0x200>
   24658:	mov	x0, x20
   2465c:	bl	d5d0 <__gmpn_invert_limb@plt>
   24660:	cbz	x21, 24ac0 <__gmpn_divrem_1@@Base+0x4f0>
   24664:	add	x8, x23, x21, lsl #3
   24668:	ldur	x12, [x8, #-8]
   2466c:	neg	w8, w25
   24670:	cmp	x21, #0x2
   24674:	lsr	x8, x12, x8
   24678:	orr	x11, x8, x22
   2467c:	b.lt	24a78 <__gmpn_divrem_1@@Base+0x4a8>  // b.tstop
   24680:	mov	w8, #0x40                  	// #64
   24684:	sub	w8, w8, w25
   24688:	sub	x9, x23, #0x10
   2468c:	ldr	x10, [x9, x21, lsl #3]
   24690:	lsl	x12, x12, x25
   24694:	umulh	x13, x11, x0
   24698:	lsr	x14, x10, x8
   2469c:	orr	x12, x14, x12
   246a0:	mul	x14, x11, x0
   246a4:	add	x11, x11, #0x1
   246a8:	adds	x15, x14, x12
   246ac:	adc	x11, x13, x11
   246b0:	msub	x12, x11, x20, x12
   246b4:	cmp	x12, x15
   246b8:	cset	w14, hi  // hi = pmore
   246bc:	sub	x11, x11, x14
   246c0:	csel	x14, x20, xzr, hi  // hi = pmore
   246c4:	add	x12, x14, x12
   246c8:	cmp	x12, x20
   246cc:	sub	x13, x21, #0x2
   246d0:	cinc	x14, x11, cs  // cs = hs, nlast
   246d4:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   246d8:	sub	x21, x21, #0x1
   246dc:	cmp	x13, #0x0
   246e0:	sub	x11, x12, x11
   246e4:	str	x14, [x24], #-8
   246e8:	mov	x12, x10
   246ec:	b.gt	2468c <__gmpn_divrem_1@@Base+0xbc>
   246f0:	b	24a7c <__gmpn_divrem_1@@Base+0x4ac>
   246f4:	cbz	x21, 24900 <__gmpn_divrem_1@@Base+0x330>
   246f8:	sub	x21, x21, #0x1
   246fc:	ldr	x8, [x23, x21, lsl #3]
   24700:	cmp	x8, x20
   24704:	cset	w10, cs  // cs = hs, nlast
   24708:	csel	x11, x20, xzr, cs  // cs = hs, nlast
   2470c:	str	x10, [x24], #-8
   24710:	sub	x22, x8, x11
   24714:	mov	x8, x9
   24718:	cmp	x8, #0x2
   2471c:	b.le	2490c <__gmpn_divrem_1@@Base+0x33c>
   24720:	mov	x0, x20
   24724:	bl	d5d0 <__gmpn_invert_limb@plt>
   24728:	cmp	x21, #0x1
   2472c:	b.lt	24788 <__gmpn_divrem_1@@Base+0x1b8>  // b.tstop
   24730:	add	x8, x21, #0x1
   24734:	sub	x9, x23, #0x10
   24738:	ldr	x10, [x9, x8, lsl #3]
   2473c:	umulh	x11, x22, x0
   24740:	mul	x12, x22, x0
   24744:	add	x13, x22, #0x1
   24748:	adds	x14, x12, x10
   2474c:	adc	x11, x11, x13
   24750:	msub	x10, x11, x20, x10
   24754:	cmp	x10, x14
   24758:	csel	x13, x20, xzr, hi  // hi = pmore
   2475c:	cset	w12, hi  // hi = pmore
   24760:	add	x10, x13, x10
   24764:	sub	x11, x11, x12
   24768:	cmp	x10, x20
   2476c:	sub	x8, x8, #0x1
   24770:	csel	x12, xzr, x20, cc  // cc = lo, ul, last
   24774:	cinc	x11, x11, cs  // cs = hs, nlast
   24778:	cmp	x8, #0x1
   2477c:	sub	x22, x10, x12
   24780:	str	x11, [x24], #-8
   24784:	b.gt	24738 <__gmpn_divrem_1@@Base+0x168>
   24788:	cmp	x19, #0x1
   2478c:	b.lt	24c10 <__gmpn_divrem_1@@Base+0x640>  // b.tstop
   24790:	add	x8, x19, #0x1
   24794:	umulh	x9, x22, x0
   24798:	add	x9, x22, x9
   2479c:	add	x9, x9, #0x1
   247a0:	mul	x10, x22, x0
   247a4:	mneg	x11, x9, x20
   247a8:	cmp	x10, x11
   247ac:	cset	w10, cc  // cc = lo, ul, last
   247b0:	sub	x8, x8, #0x1
   247b4:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   247b8:	sub	x10, x9, x10
   247bc:	msub	x22, x9, x20, x11
   247c0:	cmp	x8, #0x1
   247c4:	str	x10, [x24], #-8
   247c8:	b.gt	24794 <__gmpn_divrem_1@@Base+0x1c4>
   247cc:	b	24c10 <__gmpn_divrem_1@@Base+0x640>
   247d0:	lsr	x8, x20, #32
   247d4:	cbz	x21, 24b5c <__gmpn_divrem_1@@Base+0x58c>
   247d8:	add	x9, x23, x21, lsl #3
   247dc:	ldur	x9, [x9, #-8]
   247e0:	neg	w10, w25
   247e4:	cmp	x21, #0x2
   247e8:	lsr	x10, x9, x10
   247ec:	orr	x13, x10, x22
   247f0:	b.lt	248bc <__gmpn_divrem_1@@Base+0x2ec>  // b.tstop
   247f4:	mov	w11, #0x40                  	// #64
   247f8:	and	x10, x20, #0xffffffff
   247fc:	sub	w11, w11, w25
   24800:	sub	x12, x23, #0x10
   24804:	b	24828 <__gmpn_divrem_1@@Base+0x258>
   24808:	mov	x17, x16
   2480c:	sub	x13, x13, x15
   24810:	orr	x14, x17, x14, lsl #32
   24814:	sub	x15, x21, #0x2
   24818:	sub	x21, x21, #0x1
   2481c:	cmp	x15, #0x0
   24820:	str	x14, [x24], #-8
   24824:	b.le	248bc <__gmpn_divrem_1@@Base+0x2ec>
   24828:	mov	x14, x9
   2482c:	ldr	x9, [x12, x21, lsl #3]
   24830:	udiv	x17, x13, x8
   24834:	lsl	x14, x14, x25
   24838:	msub	w16, w17, w8, w13
   2483c:	lsr	x13, x9, x11
   24840:	orr	x13, x13, x14
   24844:	mul	x15, x17, x10
   24848:	extr	x16, x16, x13, #32
   2484c:	cmp	x16, x15
   24850:	b.cs	24878 <__gmpn_divrem_1@@Base+0x2a8>  // b.hs, b.nlast
   24854:	add	x16, x16, x20
   24858:	cmp	x16, x20
   2485c:	sub	x14, x17, #0x1
   24860:	b.cc	2487c <__gmpn_divrem_1@@Base+0x2ac>  // b.lo, b.ul, b.last
   24864:	cmp	x16, x15
   24868:	b.cs	2487c <__gmpn_divrem_1@@Base+0x2ac>  // b.hs, b.nlast
   2486c:	sub	x14, x17, #0x2
   24870:	add	x16, x16, x20
   24874:	b	2487c <__gmpn_divrem_1@@Base+0x2ac>
   24878:	mov	x14, x17
   2487c:	sub	x15, x16, x15
   24880:	udiv	x16, x15, x8
   24884:	msub	w17, w16, w8, w15
   24888:	mul	x15, x16, x10
   2488c:	bfi	x13, x17, #32, #32
   24890:	cmp	x13, x15
   24894:	b.cs	24808 <__gmpn_divrem_1@@Base+0x238>  // b.hs, b.nlast
   24898:	add	x13, x13, x20
   2489c:	cmp	x13, x20
   248a0:	sub	x17, x16, #0x1
   248a4:	b.cc	2480c <__gmpn_divrem_1@@Base+0x23c>  // b.lo, b.ul, b.last
   248a8:	cmp	x13, x15
   248ac:	b.cs	2480c <__gmpn_divrem_1@@Base+0x23c>  // b.hs, b.nlast
   248b0:	sub	x17, x16, #0x2
   248b4:	add	x13, x13, x20
   248b8:	b	2480c <__gmpn_divrem_1@@Base+0x23c>
   248bc:	udiv	x14, x13, x8
   248c0:	and	x11, x20, #0xffffffff
   248c4:	msub	w10, w14, w8, w13
   248c8:	lsl	x9, x9, x25
   248cc:	mul	x12, x14, x11
   248d0:	extr	x13, x10, x9, #32
   248d4:	cmp	x13, x12
   248d8:	b.cs	24b08 <__gmpn_divrem_1@@Base+0x538>  // b.hs, b.nlast
   248dc:	add	x13, x13, x20
   248e0:	cmp	x13, x20
   248e4:	sub	x10, x14, #0x1
   248e8:	b.cc	24b0c <__gmpn_divrem_1@@Base+0x53c>  // b.lo, b.ul, b.last
   248ec:	cmp	x13, x12
   248f0:	b.cs	24b0c <__gmpn_divrem_1@@Base+0x53c>  // b.hs, b.nlast
   248f4:	sub	x10, x14, #0x2
   248f8:	add	x13, x13, x20
   248fc:	b	24b0c <__gmpn_divrem_1@@Base+0x53c>
   24900:	mov	x22, xzr
   24904:	cmp	x8, #0x2
   24908:	b.gt	24720 <__gmpn_divrem_1@@Base+0x150>
   2490c:	cmp	x21, #0x1
   24910:	lsr	x8, x20, #32
   24914:	b.lt	249c8 <__gmpn_divrem_1@@Base+0x3f8>  // b.tstop
   24918:	and	x9, x20, #0xffffffff
   2491c:	add	x10, x21, #0x1
   24920:	sub	x11, x23, #0x10
   24924:	b	24944 <__gmpn_divrem_1@@Base+0x374>
   24928:	mov	x16, x15
   2492c:	sub	x22, x12, x14
   24930:	orr	x12, x16, x13, lsl #32
   24934:	sub	x10, x10, #0x1
   24938:	cmp	x10, #0x1
   2493c:	str	x12, [x24], #-8
   24940:	b.le	249c8 <__gmpn_divrem_1@@Base+0x3f8>
   24944:	ldr	x12, [x11, x10, lsl #3]
   24948:	udiv	x16, x22, x8
   2494c:	msub	w13, w16, w8, w22
   24950:	mul	x14, x16, x9
   24954:	extr	x15, x13, x12, #32
   24958:	cmp	x15, x14
   2495c:	b.cs	24984 <__gmpn_divrem_1@@Base+0x3b4>  // b.hs, b.nlast
   24960:	add	x15, x15, x20
   24964:	cmp	x15, x20
   24968:	sub	x13, x16, #0x1
   2496c:	b.cc	24988 <__gmpn_divrem_1@@Base+0x3b8>  // b.lo, b.ul, b.last
   24970:	cmp	x15, x14
   24974:	b.cs	24988 <__gmpn_divrem_1@@Base+0x3b8>  // b.hs, b.nlast
   24978:	sub	x13, x16, #0x2
   2497c:	add	x15, x15, x20
   24980:	b	24988 <__gmpn_divrem_1@@Base+0x3b8>
   24984:	mov	x13, x16
   24988:	sub	x14, x15, x14
   2498c:	udiv	x15, x14, x8
   24990:	msub	w16, w15, w8, w14
   24994:	mul	x14, x15, x9
   24998:	bfi	x12, x16, #32, #32
   2499c:	cmp	x12, x14
   249a0:	b.cs	24928 <__gmpn_divrem_1@@Base+0x358>  // b.hs, b.nlast
   249a4:	add	x12, x12, x20
   249a8:	cmp	x12, x20
   249ac:	sub	x16, x15, #0x1
   249b0:	b.cc	2492c <__gmpn_divrem_1@@Base+0x35c>  // b.lo, b.ul, b.last
   249b4:	cmp	x12, x14
   249b8:	b.cs	2492c <__gmpn_divrem_1@@Base+0x35c>  // b.hs, b.nlast
   249bc:	sub	x16, x15, #0x2
   249c0:	add	x12, x12, x20
   249c4:	b	2492c <__gmpn_divrem_1@@Base+0x35c>
   249c8:	cmp	x19, #0x1
   249cc:	b.lt	24c10 <__gmpn_divrem_1@@Base+0x640>  // b.tstop
   249d0:	and	x9, x20, #0xffffffff
   249d4:	add	x10, x19, #0x1
   249d8:	b	249f8 <__gmpn_divrem_1@@Base+0x428>
   249dc:	mov	x15, x14
   249e0:	orr	x11, x15, x11, lsl #32
   249e4:	sub	x10, x10, #0x1
   249e8:	sub	x22, x13, x12
   249ec:	cmp	x10, #0x1
   249f0:	str	x11, [x24], #-8
   249f4:	b.le	24c10 <__gmpn_divrem_1@@Base+0x640>
   249f8:	udiv	x14, x22, x8
   249fc:	msub	w11, w14, w8, w22
   24a00:	mul	x12, x14, x9
   24a04:	lsl	x13, x11, #32
   24a08:	cmp	x13, x12
   24a0c:	b.cs	24a34 <__gmpn_divrem_1@@Base+0x464>  // b.hs, b.nlast
   24a10:	add	x13, x13, x20
   24a14:	cmp	x13, x20
   24a18:	sub	x11, x14, #0x1
   24a1c:	b.cc	24a38 <__gmpn_divrem_1@@Base+0x468>  // b.lo, b.ul, b.last
   24a20:	cmp	x13, x12
   24a24:	b.cs	24a38 <__gmpn_divrem_1@@Base+0x468>  // b.hs, b.nlast
   24a28:	sub	x11, x14, #0x2
   24a2c:	add	x13, x13, x20
   24a30:	b	24a38 <__gmpn_divrem_1@@Base+0x468>
   24a34:	mov	x11, x14
   24a38:	sub	x12, x13, x12
   24a3c:	udiv	x14, x12, x8
   24a40:	msub	w13, w14, w8, w12
   24a44:	mul	x12, x14, x9
   24a48:	lsl	x13, x13, #32
   24a4c:	cmp	x13, x12
   24a50:	b.cs	249dc <__gmpn_divrem_1@@Base+0x40c>  // b.hs, b.nlast
   24a54:	add	x13, x13, x20
   24a58:	cmp	x13, x20
   24a5c:	sub	x15, x14, #0x1
   24a60:	b.cc	249e0 <__gmpn_divrem_1@@Base+0x410>  // b.lo, b.ul, b.last
   24a64:	cmp	x13, x12
   24a68:	b.cs	249e0 <__gmpn_divrem_1@@Base+0x410>  // b.hs, b.nlast
   24a6c:	sub	x15, x14, #0x2
   24a70:	add	x13, x13, x20
   24a74:	b	249e0 <__gmpn_divrem_1@@Base+0x410>
   24a78:	mov	x10, x12
   24a7c:	umulh	x8, x11, x0
   24a80:	mul	x9, x11, x0
   24a84:	lsl	x10, x10, x25
   24a88:	add	x11, x11, #0x1
   24a8c:	adds	x12, x9, x10
   24a90:	adc	x8, x8, x11
   24a94:	msub	x9, x8, x20, x10
   24a98:	cmp	x9, x12
   24a9c:	csel	x11, x20, xzr, hi  // hi = pmore
   24aa0:	cset	w10, hi  // hi = pmore
   24aa4:	add	x9, x11, x9
   24aa8:	sub	x8, x8, x10
   24aac:	cmp	x9, x20
   24ab0:	cinc	x8, x8, cs  // cs = hs, nlast
   24ab4:	csel	x10, xzr, x20, cc  // cc = lo, ul, last
   24ab8:	sub	x22, x9, x10
   24abc:	str	x8, [x24], #-8
   24ac0:	cmp	x19, #0x1
   24ac4:	b.lt	24c0c <__gmpn_divrem_1@@Base+0x63c>  // b.tstop
   24ac8:	add	x8, x19, #0x1
   24acc:	umulh	x9, x22, x0
   24ad0:	add	x9, x22, x9
   24ad4:	add	x9, x9, #0x1
   24ad8:	mul	x10, x22, x0
   24adc:	mneg	x11, x9, x20
   24ae0:	cmp	x10, x11
   24ae4:	cset	w10, cc  // cc = lo, ul, last
   24ae8:	sub	x8, x8, #0x1
   24aec:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   24af0:	sub	x10, x9, x10
   24af4:	msub	x22, x9, x20, x11
   24af8:	cmp	x8, #0x1
   24afc:	str	x10, [x24], #-8
   24b00:	b.gt	24acc <__gmpn_divrem_1@@Base+0x4fc>
   24b04:	b	24c0c <__gmpn_divrem_1@@Base+0x63c>
   24b08:	mov	x10, x14
   24b0c:	sub	x13, x13, x12
   24b10:	udiv	x12, x13, x8
   24b14:	msub	w13, w12, w8, w13
   24b18:	mul	x11, x12, x11
   24b1c:	bfi	x9, x13, #32, #32
   24b20:	cmp	x9, x11
   24b24:	b.cs	24b4c <__gmpn_divrem_1@@Base+0x57c>  // b.hs, b.nlast
   24b28:	add	x9, x9, x20
   24b2c:	cmp	x9, x20
   24b30:	sub	x13, x12, #0x1
   24b34:	b.cc	24b50 <__gmpn_divrem_1@@Base+0x580>  // b.lo, b.ul, b.last
   24b38:	cmp	x9, x11
   24b3c:	b.cs	24b50 <__gmpn_divrem_1@@Base+0x580>  // b.hs, b.nlast
   24b40:	sub	x13, x12, #0x2
   24b44:	add	x9, x9, x20
   24b48:	b	24b50 <__gmpn_divrem_1@@Base+0x580>
   24b4c:	mov	x13, x12
   24b50:	sub	x22, x9, x11
   24b54:	orr	x9, x13, x10, lsl #32
   24b58:	str	x9, [x24], #-8
   24b5c:	cmp	x19, #0x1
   24b60:	b.lt	24c0c <__gmpn_divrem_1@@Base+0x63c>  // b.tstop
   24b64:	and	x9, x20, #0xffffffff
   24b68:	add	x10, x19, #0x1
   24b6c:	b	24b8c <__gmpn_divrem_1@@Base+0x5bc>
   24b70:	mov	x15, x14
   24b74:	orr	x11, x15, x11, lsl #32
   24b78:	sub	x10, x10, #0x1
   24b7c:	sub	x22, x13, x12
   24b80:	cmp	x10, #0x1
   24b84:	str	x11, [x24], #-8
   24b88:	b.le	24c0c <__gmpn_divrem_1@@Base+0x63c>
   24b8c:	udiv	x14, x22, x8
   24b90:	msub	w11, w14, w8, w22
   24b94:	mul	x12, x14, x9
   24b98:	lsl	x13, x11, #32
   24b9c:	cmp	x13, x12
   24ba0:	b.cs	24bc8 <__gmpn_divrem_1@@Base+0x5f8>  // b.hs, b.nlast
   24ba4:	add	x13, x13, x20
   24ba8:	cmp	x13, x20
   24bac:	sub	x11, x14, #0x1
   24bb0:	b.cc	24bcc <__gmpn_divrem_1@@Base+0x5fc>  // b.lo, b.ul, b.last
   24bb4:	cmp	x13, x12
   24bb8:	b.cs	24bcc <__gmpn_divrem_1@@Base+0x5fc>  // b.hs, b.nlast
   24bbc:	sub	x11, x14, #0x2
   24bc0:	add	x13, x13, x20
   24bc4:	b	24bcc <__gmpn_divrem_1@@Base+0x5fc>
   24bc8:	mov	x11, x14
   24bcc:	sub	x12, x13, x12
   24bd0:	udiv	x14, x12, x8
   24bd4:	msub	w13, w14, w8, w12
   24bd8:	mul	x12, x14, x9
   24bdc:	lsl	x13, x13, #32
   24be0:	cmp	x13, x12
   24be4:	b.cs	24b70 <__gmpn_divrem_1@@Base+0x5a0>  // b.hs, b.nlast
   24be8:	add	x13, x13, x20
   24bec:	cmp	x13, x20
   24bf0:	sub	x15, x14, #0x1
   24bf4:	b.cc	24b74 <__gmpn_divrem_1@@Base+0x5a4>  // b.lo, b.ul, b.last
   24bf8:	cmp	x13, x12
   24bfc:	b.cs	24b74 <__gmpn_divrem_1@@Base+0x5a4>  // b.hs, b.nlast
   24c00:	sub	x15, x14, #0x2
   24c04:	add	x13, x13, x20
   24c08:	b	24b74 <__gmpn_divrem_1@@Base+0x5a4>
   24c0c:	lsr	x22, x22, x25
   24c10:	mov	x0, x22
   24c14:	ldp	x20, x19, [sp, #64]
   24c18:	ldp	x22, x21, [sp, #48]
   24c1c:	ldp	x24, x23, [sp, #32]
   24c20:	ldr	x25, [sp, #16]
   24c24:	ldp	x29, x30, [sp], #80
   24c28:	ret

0000000000024c2c <__gmpn_divrem_2@@Base>:
   24c2c:	stp	x29, x30, [sp, #-96]!
   24c30:	stp	x28, x27, [sp, #16]
   24c34:	stp	x26, x25, [sp, #32]
   24c38:	stp	x24, x23, [sp, #48]
   24c3c:	stp	x22, x21, [sp, #64]
   24c40:	stp	x20, x19, [sp, #80]
   24c44:	add	x28, x2, x3, lsl #3
   24c48:	ldr	x27, [x28, #-16]!
   24c4c:	ldp	x25, x20, [x4]
   24c50:	mov	x24, x3
   24c54:	mov	x22, x2
   24c58:	ldr	x26, [x28, #8]
   24c5c:	mov	x19, x1
   24c60:	mov	x23, x0
   24c64:	mov	x29, sp
   24c68:	cmp	x26, x20
   24c6c:	b.cc	24c7c <__gmpn_divrem_2@@Base+0x50>  // b.lo, b.ul, b.last
   24c70:	b.hi	24c84 <__gmpn_divrem_2@@Base+0x58>  // b.pmore
   24c74:	cmp	x27, x25
   24c78:	b.cs	24c84 <__gmpn_divrem_2@@Base+0x58>  // b.hs, b.nlast
   24c7c:	mov	x21, xzr
   24c80:	b	24c94 <__gmpn_divrem_2@@Base+0x68>
   24c84:	subs	x8, x27, x25
   24c88:	sbc	x26, x26, x20
   24c8c:	mov	w21, #0x1                   	// #1
   24c90:	mov	x27, x8
   24c94:	mov	x0, x20
   24c98:	bl	d5d0 <__gmpn_invert_limb@plt>
   24c9c:	mul	x8, x0, x20
   24ca0:	adds	x8, x8, x25
   24ca4:	b.cc	24cc0 <__gmpn_divrem_2@@Base+0x94>  // b.lo, b.ul, b.last
   24ca8:	subs	x8, x8, x20
   24cac:	cset	w9, cs  // cs = hs, nlast
   24cb0:	csel	x10, x20, xzr, cs  // cs = hs, nlast
   24cb4:	mvn	x9, x9
   24cb8:	add	x0, x9, x0
   24cbc:	sub	x8, x8, x10
   24cc0:	umulh	x9, x25, x0
   24cc4:	adds	x9, x9, x8
   24cc8:	b.cc	24d7c <__gmpn_divrem_2@@Base+0x150>  // b.lo, b.ul, b.last
   24ccc:	cmp	x9, x20
   24cd0:	sub	x8, x0, #0x1
   24cd4:	b.cs	24dc4 <__gmpn_divrem_2@@Base+0x198>  // b.hs, b.nlast
   24cd8:	subs	x9, x24, #0x3
   24cdc:	b.lt	24d88 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   24ce0:	add	x10, x23, x19, lsl #3
   24ce4:	ldr	x11, [x22, x9, lsl #3]
   24ce8:	mul	x12, x26, x8
   24cec:	umulh	x13, x26, x8
   24cf0:	adds	x14, x12, x27
   24cf4:	adc	x12, x13, x26
   24cf8:	msub	x13, x12, x20, x27
   24cfc:	subs	x17, x11, x25
   24d00:	sbc	x11, x13, x20
   24d04:	mul	x15, x12, x25
   24d08:	umulh	x16, x25, x12
   24d0c:	subs	x13, x17, x15
   24d10:	sbc	x11, x11, x16
   24d14:	cmp	x11, x14
   24d18:	cset	w14, cs  // cs = hs, nlast
   24d1c:	csetm	x15, cs  // cs = hs, nlast
   24d20:	sub	x12, x12, x14
   24d24:	and	x14, x25, x15
   24d28:	and	x15, x20, x15
   24d2c:	adds	x27, x13, x14
   24d30:	adc	x26, x11, x15
   24d34:	cmp	x26, x20
   24d38:	add	x11, x12, #0x1
   24d3c:	b.cs	24d58 <__gmpn_divrem_2@@Base+0x12c>  // b.hs, b.nlast
   24d40:	sub	x12, x9, #0x1
   24d44:	cmp	x9, #0x0
   24d48:	str	x11, [x10, x9, lsl #3]
   24d4c:	mov	x9, x12
   24d50:	b.gt	24ce4 <__gmpn_divrem_2@@Base+0xb8>
   24d54:	b	24d94 <__gmpn_divrem_2@@Base+0x168>
   24d58:	cmp	x27, x25
   24d5c:	b.cs	24d68 <__gmpn_divrem_2@@Base+0x13c>  // b.hs, b.nlast
   24d60:	cmp	x26, x20
   24d64:	b.ls	24d40 <__gmpn_divrem_2@@Base+0x114>  // b.plast
   24d68:	subs	x12, x27, x25
   24d6c:	sbc	x26, x26, x20
   24d70:	add	x11, x11, #0x1
   24d74:	mov	x27, x12
   24d78:	b	24d40 <__gmpn_divrem_2@@Base+0x114>
   24d7c:	mov	x8, x0
   24d80:	subs	x9, x24, #0x3
   24d84:	b.ge	24ce0 <__gmpn_divrem_2@@Base+0xb4>  // b.tcont
   24d88:	cmp	x19, #0x1
   24d8c:	b.lt	24da0 <__gmpn_divrem_2@@Base+0x174>  // b.tstop
   24d90:	b	24de4 <__gmpn_divrem_2@@Base+0x1b8>
   24d94:	mov	x28, x22
   24d98:	cmp	x19, #0x1
   24d9c:	b.ge	24de4 <__gmpn_divrem_2@@Base+0x1b8>  // b.tcont
   24da0:	stp	x27, x26, [x28]
   24da4:	mov	x0, x21
   24da8:	ldp	x20, x19, [sp, #80]
   24dac:	ldp	x22, x21, [sp, #64]
   24db0:	ldp	x24, x23, [sp, #48]
   24db4:	ldp	x26, x25, [sp, #32]
   24db8:	ldp	x28, x27, [sp, #16]
   24dbc:	ldp	x29, x30, [sp], #96
   24dc0:	ret
   24dc4:	mul	x10, x0, x25
   24dc8:	cmp	x9, x20
   24dcc:	sub	x11, x0, #0x2
   24dd0:	ccmp	x10, x25, #0x2, ls  // ls = plast
   24dd4:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   24dd8:	subs	x9, x24, #0x3
   24ddc:	b.lt	24d88 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   24de0:	b	24ce0 <__gmpn_divrem_2@@Base+0xb4>
   24de4:	sub	x9, x23, #0x8
   24de8:	mov	x11, xzr
   24dec:	mul	x12, x26, x8
   24df0:	umulh	x13, x26, x8
   24df4:	adds	x14, x12, x27
   24df8:	adc	x12, x13, x26
   24dfc:	msub	x13, x12, x20, x27
   24e00:	subs	x17, x11, x25
   24e04:	sbc	x11, x13, x20
   24e08:	mul	x15, x12, x25
   24e0c:	umulh	x16, x25, x12
   24e10:	subs	x13, x17, x15
   24e14:	sbc	x11, x11, x16
   24e18:	cmp	x11, x14
   24e1c:	cset	w14, cs  // cs = hs, nlast
   24e20:	csetm	x15, cs  // cs = hs, nlast
   24e24:	sub	x12, x12, x14
   24e28:	and	x14, x25, x15
   24e2c:	and	x15, x20, x15
   24e30:	adds	x27, x13, x14
   24e34:	adc	x26, x11, x15
   24e38:	sub	x10, x19, #0x1
   24e3c:	cmp	x26, x20
   24e40:	add	x11, x12, #0x1
   24e44:	b.cs	24e60 <__gmpn_divrem_2@@Base+0x234>  // b.hs, b.nlast
   24e48:	add	x12, x10, #0x1
   24e4c:	cmp	x12, #0x1
   24e50:	str	x11, [x9, x19, lsl #3]
   24e54:	mov	x19, x10
   24e58:	b.gt	24de8 <__gmpn_divrem_2@@Base+0x1bc>
   24e5c:	b	24da0 <__gmpn_divrem_2@@Base+0x174>
   24e60:	cmp	x27, x25
   24e64:	b.cs	24e70 <__gmpn_divrem_2@@Base+0x244>  // b.hs, b.nlast
   24e68:	cmp	x26, x20
   24e6c:	b.ls	24e48 <__gmpn_divrem_2@@Base+0x21c>  // b.plast
   24e70:	subs	x12, x27, x25
   24e74:	sbc	x26, x26, x20
   24e78:	add	x11, x11, #0x1
   24e7c:	mov	x27, x12
   24e80:	b	24e48 <__gmpn_divrem_2@@Base+0x21c>

0000000000024e84 <__gmpn_fib2_ui@@Base>:
   24e84:	stp	x29, x30, [sp, #-96]!
   24e88:	stp	x24, x23, [sp, #48]
   24e8c:	stp	x22, x21, [sp, #64]
   24e90:	stp	x20, x19, [sp, #80]
   24e94:	mov	x19, x2
   24e98:	mov	x20, x1
   24e9c:	cmp	x2, #0x5e
   24ea0:	mov	x21, x0
   24ea4:	mov	w24, #0x1                   	// #1
   24ea8:	str	x27, [sp, #16]
   24eac:	stp	x26, x25, [sp, #32]
   24eb0:	mov	x29, sp
   24eb4:	b.cc	24ed4 <__gmpn_fib2_ui@@Base+0x50>  // b.lo, b.ul, b.last
   24eb8:	mov	x9, x19
   24ebc:	lsr	x8, x9, #1
   24ec0:	cmp	x9, #0xbb
   24ec4:	lsl	x24, x24, #1
   24ec8:	mov	x9, x8
   24ecc:	b.hi	24ebc <__gmpn_fib2_ui@@Base+0x38>  // b.pmore
   24ed0:	b	24ed8 <__gmpn_fib2_ui@@Base+0x54>
   24ed4:	mov	x8, x19
   24ed8:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   24edc:	ldr	x10, [x10, #3808]
   24ee0:	sbfiz	x9, x8, #3, #32
   24ee4:	cmp	x24, #0x1
   24ee8:	add	x8, x10, x8, lsl #3
   24eec:	ldr	x9, [x10, x9]
   24ef0:	ldr	x8, [x8, #8]
   24ef4:	str	x9, [x20]
   24ef8:	str	x8, [x21]
   24efc:	b.ne	24f08 <__gmpn_fib2_ui@@Base+0x84>  // b.any
   24f00:	mov	w23, #0x1                   	// #1
   24f04:	b	25060 <__gmpn_fib2_ui@@Base+0x1dc>
   24f08:	lsr	x8, x19, #5
   24f0c:	mov	w9, #0x17                  	// #23
   24f10:	mul	x8, x8, x9
   24f14:	lsr	x8, x8, #6
   24f18:	lsl	x9, x8, #3
   24f1c:	cmp	x8, #0xfdc
   24f20:	add	x1, x9, #0x20
   24f24:	str	xzr, [x29, #24]
   24f28:	b.hi	25084 <__gmpn_fib2_ui@@Base+0x200>  // b.pmore
   24f2c:	add	x9, x1, #0xf
   24f30:	mov	x8, sp
   24f34:	and	x9, x9, #0x7ffffffffffffff0
   24f38:	sub	x22, x8, x9
   24f3c:	mov	sp, x22
   24f40:	add	x25, x21, #0x8
   24f44:	mov	w23, #0x1                   	// #1
   24f48:	b	24f7c <__gmpn_fib2_ui@@Base+0xf8>
   24f4c:	mov	x0, x21
   24f50:	mov	x1, x21
   24f54:	mov	x2, x20
   24f58:	mov	x3, x23
   24f5c:	bl	c420 <__gmpn_sub_n@plt>
   24f60:	add	x8, x21, x23, lsl #3
   24f64:	ldur	x8, [x8, #-8]
   24f68:	cmp	x8, #0x0
   24f6c:	cset	w8, eq  // eq = none
   24f70:	sub	x23, x23, x8
   24f74:	cmp	x24, #0x1
   24f78:	b.eq	25058 <__gmpn_fib2_ui@@Base+0x1d4>  // b.none
   24f7c:	mov	x0, x22
   24f80:	mov	x1, x21
   24f84:	mov	x2, x23
   24f88:	bl	ca90 <__gmpn_sqr@plt>
   24f8c:	mov	x0, x21
   24f90:	mov	x1, x20
   24f94:	mov	x2, x23
   24f98:	bl	ca90 <__gmpn_sqr@plt>
   24f9c:	add	x8, x22, x23, lsl #4
   24fa0:	ldur	x8, [x8, #-8]
   24fa4:	lsl	x9, x23, #1
   24fa8:	mov	x0, x20
   24fac:	mov	x1, x22
   24fb0:	cmp	x8, #0x0
   24fb4:	cset	w8, eq  // eq = none
   24fb8:	sub	x23, x9, x8
   24fbc:	mov	x2, x21
   24fc0:	mov	x3, x23
   24fc4:	bl	cc30 <__gmpn_add_n@plt>
   24fc8:	lsl	x26, x23, #3
   24fcc:	str	x0, [x20, x26]
   24fd0:	ldr	x8, [x21]
   24fd4:	tst	x24, x19
   24fd8:	cset	w9, ne  // ne = any
   24fdc:	mov	x0, x21
   24fe0:	orr	x8, x8, x9, lsl #1
   24fe4:	mov	x1, x21
   24fe8:	mov	x2, x22
   24fec:	mov	x3, x23
   24ff0:	cset	w27, eq  // eq = none
   24ff4:	str	x8, [x21]
   24ff8:	bl	cda0 <__gmpn_rsblsh2_n@plt>
   24ffc:	str	x0, [x21, x26]
   25000:	ldr	x8, [x21]
   25004:	adds	x8, x8, w27, uxtw #1
   25008:	str	x8, [x21]
   2500c:	b.cc	25024 <__gmpn_fib2_ui@@Base+0x1a0>  // b.lo, b.ul, b.last
   25010:	mov	x8, x25
   25014:	ldr	x9, [x8]
   25018:	adds	x9, x9, #0x1
   2501c:	str	x9, [x8], #8
   25020:	b.cs	25014 <__gmpn_fib2_ui@@Base+0x190>  // b.hs, b.nlast
   25024:	ldr	x8, [x21, x23, lsl #3]
   25028:	lsr	x24, x24, #1
   2502c:	cmp	x8, #0x0
   25030:	cinc	x23, x23, ne  // ne = any
   25034:	tst	x24, x19
   25038:	b.eq	24f4c <__gmpn_fib2_ui@@Base+0xc8>  // b.none
   2503c:	mov	x0, x20
   25040:	mov	x1, x21
   25044:	mov	x2, x20
   25048:	mov	x3, x23
   2504c:	bl	c420 <__gmpn_sub_n@plt>
   25050:	cmp	x24, #0x1
   25054:	b.ne	24f7c <__gmpn_fib2_ui@@Base+0xf8>  // b.any
   25058:	ldr	x0, [x29, #24]
   2505c:	cbnz	x0, 25094 <__gmpn_fib2_ui@@Base+0x210>
   25060:	mov	x0, x23
   25064:	mov	sp, x29
   25068:	ldp	x20, x19, [sp, #80]
   2506c:	ldp	x22, x21, [sp, #64]
   25070:	ldp	x24, x23, [sp, #48]
   25074:	ldp	x26, x25, [sp, #32]
   25078:	ldr	x27, [sp, #16]
   2507c:	ldp	x29, x30, [sp], #96
   25080:	ret
   25084:	add	x0, x29, #0x18
   25088:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2508c:	mov	x22, x0
   25090:	b	24f40 <__gmpn_fib2_ui@@Base+0xbc>
   25094:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   25098:	b	25060 <__gmpn_fib2_ui@@Base+0x1dc>

000000000002509c <__gmpn_fib2m@@Base>:
   2509c:	stp	x29, x30, [sp, #-96]!
   250a0:	stp	x28, x27, [sp, #16]
   250a4:	stp	x26, x25, [sp, #32]
   250a8:	stp	x24, x23, [sp, #48]
   250ac:	stp	x22, x21, [sp, #64]
   250b0:	stp	x20, x19, [sp, #80]
   250b4:	mov	x29, sp
   250b8:	sub	sp, sp, #0x40
   250bc:	mov	x11, #0x2c84                	// #11396
   250c0:	stp	x4, x2, [x29, #-40]
   250c4:	movk	x11, #0x2164, lsl #16
   250c8:	sub	x10, x3, #0x1
   250cc:	movk	x11, #0x590b, lsl #32
   250d0:	ldr	x8, [x2, x10, lsl #3]
   250d4:	mov	w9, #0x5c                  	// #92
   250d8:	movk	x11, #0x2c8, lsl #48
   250dc:	mul	x9, x5, x9
   250e0:	cmp	x5, x11
   250e4:	csinv	x9, x9, xzr, ls  // ls = plast
   250e8:	clz	x13, x8
   250ec:	clz	x14, x9
   250f0:	mov	x19, x5
   250f4:	mov	x21, x1
   250f8:	subs	w11, w14, w13
   250fc:	mov	x23, x0
   25100:	b.cs	25138 <__gmpn_fib2m@@Base+0x9c>  // b.hs, b.nlast
   25104:	subs	x12, x3, #0x2
   25108:	b.lt	25140 <__gmpn_fib2m@@Base+0xa4>  // b.tstop
   2510c:	ldur	x11, [x29, #-32]
   25110:	sub	w10, w13, w14
   25114:	lsl	x8, x8, x10
   25118:	ldr	x13, [x11, x12, lsl #3]
   2511c:	mov	w11, #0x40                  	// #64
   25120:	sub	w11, w11, w10
   25124:	neg	w10, w10
   25128:	lsr	x10, x13, x10
   2512c:	orr	x8, x10, x8
   25130:	mov	x10, x12
   25134:	b	25144 <__gmpn_fib2m@@Base+0xa8>
   25138:	lsr	x8, x8, x11
   2513c:	b	25144 <__gmpn_fib2m@@Base+0xa8>
   25140:	mov	w11, wzr
   25144:	lsl	x10, x10, #6
   25148:	cmp	x8, x9
   2514c:	add	x9, x10, w11, sxtw
   25150:	cset	w10, hi  // hi = pmore
   25154:	lsr	x25, x8, x10
   25158:	mov	x0, x23
   2515c:	mov	x1, x21
   25160:	mov	x2, x25
   25164:	cinc	x22, x9, hi  // hi = pmore
   25168:	bl	d240 <__gmpn_fib2_ui@plt>
   2516c:	mov	x24, x0
   25170:	cmp	x0, x19
   25174:	b.eq	251a4 <__gmpn_fib2m@@Base+0x108>  // b.none
   25178:	sub	x8, x19, x24
   2517c:	lsl	x20, x24, #3
   25180:	lsl	x26, x8, #3
   25184:	add	x0, x23, x20
   25188:	mov	w1, wzr
   2518c:	mov	x2, x26
   25190:	bl	c780 <memset@plt>
   25194:	add	x0, x21, x20
   25198:	mov	w1, wzr
   2519c:	mov	x2, x26
   251a0:	bl	c780 <memset@plt>
   251a4:	cbz	x22, 253a0 <__gmpn_fib2m@@Base+0x304>
   251a8:	and	w8, w25, #0x1
   251ac:	cmp	x19, #0x2
   251b0:	stur	w8, [x29, #-20]
   251b4:	cset	w8, lt  // lt = tstop
   251b8:	bfi	x8, x19, #1, #63
   251bc:	lsl	x1, x8, #3
   251c0:	mov	w8, #0x7f00                	// #32512
   251c4:	cmp	x1, x8
   251c8:	lsl	x28, x19, #1
   251cc:	stur	xzr, [x29, #-16]
   251d0:	b.hi	25410 <__gmpn_fib2m@@Base+0x374>  // b.pmore
   251d4:	add	x9, x1, #0xf
   251d8:	mov	x8, sp
   251dc:	and	x9, x9, #0xfffffffffffffff0
   251e0:	sub	x25, x8, x9
   251e4:	mov	sp, x25
   251e8:	orr	x24, x28, #0x1
   251ec:	add	x8, x23, #0x8
   251f0:	lsl	x27, x28, #3
   251f4:	stp	x19, x8, [x29, #-64]
   251f8:	b	2529c <__gmpn_fib2m@@Base+0x200>
   251fc:	ldr	x8, [x23, x27]
   25200:	sub	x22, x22, #0x1
   25204:	lsr	x9, x22, #3
   25208:	and	x9, x9, #0x1ffffffffffffff8
   2520c:	sub	x10, x8, #0x1
   25210:	str	x10, [x23, x27]
   25214:	ldur	x10, [x29, #-32]
   25218:	ldr	x9, [x10, x9]
   2521c:	lsr	x9, x9, x22
   25220:	tst	w9, #0x1
   25224:	and	w20, w9, #0x1
   25228:	csel	x28, x21, x23, ne  // ne = any
   2522c:	stur	w20, [x29, #-20]
   25230:	cbz	x8, 25334 <__gmpn_fib2m@@Base+0x298>
   25234:	mov	x0, x28
   25238:	mov	x1, x23
   2523c:	mov	x2, x21
   25240:	mov	x3, x24
   25244:	bl	25428 <__gmpn_fib2m@@Base+0x38c>
   25248:	ldur	x20, [x29, #-40]
   2524c:	lsr	w8, w0, #31
   25250:	mov	x28, x26
   25254:	stur	w8, [x29, #-44]
   25258:	mov	x0, x25
   2525c:	mov	x1, x23
   25260:	mov	x2, xzr
   25264:	mov	x3, x23
   25268:	mov	x4, x24
   2526c:	mov	x5, x20
   25270:	mov	x6, x19
   25274:	bl	c030 <__gmpn_tdiv_qr@plt>
   25278:	mov	x0, x25
   2527c:	mov	x1, x21
   25280:	mov	x2, xzr
   25284:	mov	x3, x21
   25288:	mov	x4, x24
   2528c:	mov	x5, x20
   25290:	mov	x6, x19
   25294:	bl	c030 <__gmpn_tdiv_qr@plt>
   25298:	cbz	x22, 25390 <__gmpn_fib2m@@Base+0x2f4>
   2529c:	mov	x0, x25
   252a0:	mov	x1, x23
   252a4:	mov	x2, x19
   252a8:	bl	ca90 <__gmpn_sqr@plt>
   252ac:	mov	x0, x23
   252b0:	mov	x1, x21
   252b4:	mov	x2, x19
   252b8:	bl	ca90 <__gmpn_sqr@plt>
   252bc:	mov	x0, x21
   252c0:	mov	x1, x25
   252c4:	mov	x2, x23
   252c8:	mov	x3, x28
   252cc:	bl	cc30 <__gmpn_add_n@plt>
   252d0:	str	x0, [x21, x27]
   252d4:	ldur	w9, [x29, #-20]
   252d8:	ldr	x8, [x23]
   252dc:	mov	x0, x23
   252e0:	mov	x1, x23
   252e4:	lsl	w20, w9, #1
   252e8:	orr	x8, x8, x20
   252ec:	mov	x2, x25
   252f0:	mov	x3, x28
   252f4:	str	x8, [x23]
   252f8:	mov	x26, x28
   252fc:	bl	cda0 <__gmpn_rsblsh2_n@plt>
   25300:	add	x8, x0, #0x1
   25304:	str	x8, [x23, x27]
   25308:	ldr	x8, [x23]
   2530c:	eor	w9, w20, #0x2
   25310:	adds	x8, x8, x9
   25314:	str	x8, [x23]
   25318:	b.cc	251fc <__gmpn_fib2m@@Base+0x160>  // b.lo, b.ul, b.last
   2531c:	ldur	x8, [x29, #-56]
   25320:	ldr	x9, [x8]
   25324:	adds	x9, x9, #0x1
   25328:	str	x9, [x8], #8
   2532c:	b.cs	25320 <__gmpn_fib2m@@Base+0x284>  // b.hs, b.nlast
   25330:	b	251fc <__gmpn_fib2m@@Base+0x160>
   25334:	ldr	x19, [x21, x27]
   25338:	cmp	w20, #0x0
   2533c:	cset	w8, eq  // eq = none
   25340:	mov	x0, x28
   25344:	mov	x1, x21
   25348:	mov	x2, x23
   2534c:	mov	x3, x26
   25350:	stur	w8, [x29, #-44]
   25354:	bl	c420 <__gmpn_sub_n@plt>
   25358:	sub	x8, x19, x0
   2535c:	add	x8, x8, #0x1
   25360:	str	x8, [x28, x27]
   25364:	cbz	w20, 25380 <__gmpn_fib2m@@Base+0x2e4>
   25368:	mov	x0, x23
   2536c:	mov	x1, x23
   25370:	mov	x2, x26
   25374:	bl	ce90 <__gmpn_neg@plt>
   25378:	eor	x8, x0, #0x1
   2537c:	str	x8, [x23, x26, lsl #3]
   25380:	ldur	x19, [x29, #-64]
   25384:	ldur	x20, [x29, #-40]
   25388:	mov	x28, x26
   2538c:	b	25258 <__gmpn_fib2m@@Base+0x1bc>
   25390:	ldur	x0, [x29, #-16]
   25394:	cbnz	x0, 25420 <__gmpn_fib2m@@Base+0x384>
   25398:	ldur	w0, [x29, #-44]
   2539c:	b	253f0 <__gmpn_fib2m@@Base+0x354>
   253a0:	cmp	x24, x19
   253a4:	b.ne	253ec <__gmpn_fib2m@@Base+0x350>  // b.any
   253a8:	ldur	x20, [x29, #-40]
   253ac:	sub	x0, x29, #0x10
   253b0:	mov	x1, x23
   253b4:	mov	x2, xzr
   253b8:	mov	x3, x23
   253bc:	mov	x4, x24
   253c0:	mov	x5, x20
   253c4:	mov	x6, x19
   253c8:	bl	c030 <__gmpn_tdiv_qr@plt>
   253cc:	sub	x0, x29, #0x10
   253d0:	mov	x1, x21
   253d4:	mov	x2, xzr
   253d8:	mov	x3, x21
   253dc:	mov	x4, x24
   253e0:	mov	x5, x20
   253e4:	mov	x6, x19
   253e8:	bl	c030 <__gmpn_tdiv_qr@plt>
   253ec:	mov	w0, wzr
   253f0:	mov	sp, x29
   253f4:	ldp	x20, x19, [sp, #80]
   253f8:	ldp	x22, x21, [sp, #64]
   253fc:	ldp	x24, x23, [sp, #48]
   25400:	ldp	x26, x25, [sp, #32]
   25404:	ldp	x28, x27, [sp, #16]
   25408:	ldp	x29, x30, [sp], #96
   2540c:	ret
   25410:	sub	x0, x29, #0x10
   25414:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   25418:	mov	x25, x0
   2541c:	b	251e8 <__gmpn_fib2m@@Base+0x14c>
   25420:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   25424:	b	25398 <__gmpn_fib2m@@Base+0x2fc>
   25428:	stp	x29, x30, [sp, #-16]!
   2542c:	cmp	x3, #0x1
   25430:	mov	x29, sp
   25434:	b.lt	25474 <__gmpn_fib2m@@Base+0x3d8>  // b.tstop
   25438:	mov	x8, x1
   2543c:	sub	x9, x1, #0x8
   25440:	sub	x10, x2, #0x8
   25444:	sub	x11, x0, #0x8
   25448:	lsl	x12, x3, #3
   2544c:	ldr	x13, [x9, x12]
   25450:	ldr	x12, [x10, x12]
   25454:	cmp	x13, x12
   25458:	b.ne	25480 <__gmpn_fib2m@@Base+0x3e4>  // b.any
   2545c:	sub	x12, x3, #0x1
   25460:	add	x13, x12, #0x1
   25464:	cmp	x13, #0x1
   25468:	str	xzr, [x11, x3, lsl #3]
   2546c:	mov	x3, x12
   25470:	b.gt	25448 <__gmpn_fib2m@@Base+0x3ac>
   25474:	mov	w0, wzr
   25478:	ldp	x29, x30, [sp], #16
   2547c:	ret
   25480:	b.ls	25498 <__gmpn_fib2m@@Base+0x3fc>  // b.plast
   25484:	mov	x1, x8
   25488:	bl	c420 <__gmpn_sub_n@plt>
   2548c:	mov	w0, #0x1                   	// #1
   25490:	ldp	x29, x30, [sp], #16
   25494:	ret
   25498:	mov	x1, x2
   2549c:	mov	x2, x8
   254a0:	bl	c420 <__gmpn_sub_n@plt>
   254a4:	mov	w0, #0xffffffff            	// #-1
   254a8:	ldp	x29, x30, [sp], #16
   254ac:	ret

00000000000254b0 <__gmpn_mod_1@@Base>:
   254b0:	sub	sp, sp, #0x70
   254b4:	stp	x29, x30, [sp, #64]
   254b8:	str	x21, [sp, #80]
   254bc:	stp	x20, x19, [sp, #96]
   254c0:	add	x29, sp, #0x40
   254c4:	cbz	x1, 25520 <__gmpn_mod_1@@Base+0x70>
   254c8:	mov	x21, x2
   254cc:	mov	x19, x1
   254d0:	mov	x20, x0
   254d4:	tbnz	x2, #63, 2559c <__gmpn_mod_1@@Base+0xec>
   254d8:	cmp	x19, #0x5
   254dc:	b.le	25528 <__gmpn_mod_1@@Base+0x78>
   254e0:	cmp	x19, #0x9
   254e4:	b.le	2553c <__gmpn_mod_1@@Base+0x8c>
   254e8:	cmp	x19, #0x14
   254ec:	b.lt	25564 <__gmpn_mod_1@@Base+0xb4>  // b.tstop
   254f0:	lsr	x8, x21, #62
   254f4:	cbnz	x8, 25564 <__gmpn_mod_1@@Base+0xb4>
   254f8:	add	x0, sp, #0x8
   254fc:	mov	x1, x21
   25500:	bl	c810 <__gmpn_mod_1s_4p_cps@plt>
   25504:	ldr	x8, [sp, #16]
   25508:	add	x3, sp, #0x8
   2550c:	mov	x0, x20
   25510:	mov	x1, x19
   25514:	lsl	x2, x21, x8
   25518:	bl	d5f0 <__gmpn_mod_1s_4p@plt>
   2551c:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   25520:	mov	x0, xzr
   25524:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   25528:	mov	x0, x20
   2552c:	mov	x1, x19
   25530:	mov	x2, x21
   25534:	bl	25714 <__gmpn_mod_1@@Base+0x264>
   25538:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   2553c:	add	x0, sp, #0x8
   25540:	mov	x1, x21
   25544:	bl	cc80 <__gmpn_mod_1_1p_cps@plt>
   25548:	ldr	x8, [sp, #16]
   2554c:	add	x3, sp, #0x8
   25550:	mov	x0, x20
   25554:	mov	x1, x19
   25558:	lsl	x2, x21, x8
   2555c:	bl	c3a0 <__gmpn_mod_1_1p@plt>
   25560:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   25564:	add	x0, sp, #0x8
   25568:	mov	x1, x21
   2556c:	bl	ce80 <__gmpn_mod_1s_2p_cps@plt>
   25570:	ldr	x8, [sp, #16]
   25574:	add	x3, sp, #0x8
   25578:	mov	x0, x20
   2557c:	mov	x1, x19
   25580:	lsl	x2, x21, x8
   25584:	bl	d050 <__gmpn_mod_1s_2p@plt>
   25588:	ldp	x20, x19, [sp, #96]
   2558c:	ldr	x21, [sp, #80]
   25590:	ldp	x29, x30, [sp, #64]
   25594:	add	sp, sp, #0x70
   25598:	ret
   2559c:	cmp	x19, #0x7
   255a0:	b.le	255c8 <__gmpn_mod_1@@Base+0x118>
   255a4:	add	x0, sp, #0x8
   255a8:	mov	x1, x21
   255ac:	bl	cc80 <__gmpn_mod_1_1p_cps@plt>
   255b0:	add	x3, sp, #0x8
   255b4:	mov	x0, x20
   255b8:	mov	x1, x19
   255bc:	mov	x2, x21
   255c0:	bl	c3a0 <__gmpn_mod_1_1p@plt>
   255c4:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   255c8:	mov	x0, x20
   255cc:	mov	x1, x19
   255d0:	mov	x2, x21
   255d4:	bl	255dc <__gmpn_mod_1@@Base+0x12c>
   255d8:	b	25588 <__gmpn_mod_1@@Base+0xd8>
   255dc:	stp	x29, x30, [sp, #-48]!
   255e0:	stp	x22, x21, [sp, #16]
   255e4:	stp	x20, x19, [sp, #32]
   255e8:	add	x8, x0, x1, lsl #3
   255ec:	ldur	x8, [x8, #-8]
   255f0:	mov	x29, sp
   255f4:	cmp	x8, x2
   255f8:	csel	x9, xzr, x2, cc  // cc = lo, ul, last
   255fc:	cmp	x1, #0x1
   25600:	sub	x21, x8, x9
   25604:	b.eq	25700 <__gmpn_mod_1@@Base+0x250>  // b.none
   25608:	mov	x20, x1
   2560c:	mov	x22, x0
   25610:	mov	x19, x2
   25614:	cmp	x1, #0x3
   25618:	b.le	25670 <__gmpn_mod_1@@Base+0x1c0>
   2561c:	mov	x0, x19
   25620:	bl	d5d0 <__gmpn_invert_limb@plt>
   25624:	sub	x8, x22, #0x10
   25628:	ldr	x9, [x8, x20, lsl #3]
   2562c:	umulh	x10, x21, x0
   25630:	mul	x11, x21, x0
   25634:	add	x12, x21, #0x1
   25638:	adds	x13, x11, x9
   2563c:	adc	x10, x10, x12
   25640:	msub	x9, x10, x19, x9
   25644:	cmp	x9, x13
   25648:	csel	x10, x19, xzr, hi  // hi = pmore
   2564c:	add	x9, x10, x9
   25650:	cmp	x9, x19
   25654:	sub	x11, x20, #0x2
   25658:	csel	x10, xzr, x19, cc  // cc = lo, ul, last
   2565c:	sub	x20, x20, #0x1
   25660:	cmp	x11, #0x0
   25664:	sub	x21, x9, x10
   25668:	b.gt	25628 <__gmpn_mod_1@@Base+0x178>
   2566c:	b	25700 <__gmpn_mod_1@@Base+0x250>
   25670:	cmp	x20, #0x2
   25674:	b.lt	25700 <__gmpn_mod_1@@Base+0x250>  // b.tstop
   25678:	lsr	x8, x19, #32
   2567c:	and	x9, x19, #0xffffffff
   25680:	sub	x10, x22, #0x10
   25684:	b	2569c <__gmpn_mod_1@@Base+0x1ec>
   25688:	sub	x13, x20, #0x2
   2568c:	sub	x20, x20, #0x1
   25690:	cmp	x13, #0x0
   25694:	sub	x21, x11, x12
   25698:	b.le	25700 <__gmpn_mod_1@@Base+0x250>
   2569c:	ldr	x11, [x10, x20, lsl #3]
   256a0:	udiv	x12, x21, x8
   256a4:	msub	w13, w12, w8, w21
   256a8:	mul	x12, x12, x9
   256ac:	extr	x13, x13, x11, #32
   256b0:	cmp	x13, x12
   256b4:	b.cs	256cc <__gmpn_mod_1@@Base+0x21c>  // b.hs, b.nlast
   256b8:	add	x13, x13, x19
   256bc:	cmp	x13, x12
   256c0:	ccmp	x13, x19, #0x0, cc  // cc = lo, ul, last
   256c4:	csel	x14, x19, xzr, cs  // cs = hs, nlast
   256c8:	add	x13, x14, x13
   256cc:	sub	x12, x13, x12
   256d0:	udiv	x13, x12, x8
   256d4:	msub	w14, w13, w8, w12
   256d8:	mul	x12, x13, x9
   256dc:	bfi	x11, x14, #32, #32
   256e0:	cmp	x11, x12
   256e4:	b.cs	25688 <__gmpn_mod_1@@Base+0x1d8>  // b.hs, b.nlast
   256e8:	add	x11, x11, x19
   256ec:	cmp	x11, x12
   256f0:	ccmp	x11, x19, #0x0, cc  // cc = lo, ul, last
   256f4:	csel	x13, x19, xzr, cs  // cs = hs, nlast
   256f8:	add	x11, x13, x11
   256fc:	b	25688 <__gmpn_mod_1@@Base+0x1d8>
   25700:	mov	x0, x21
   25704:	ldp	x20, x19, [sp, #32]
   25708:	ldp	x22, x21, [sp, #16]
   2570c:	ldp	x29, x30, [sp], #48
   25710:	ret
   25714:	stp	x29, x30, [sp, #-80]!
   25718:	stp	x24, x23, [sp, #32]
   2571c:	stp	x22, x21, [sp, #48]
   25720:	stp	x20, x19, [sp, #64]
   25724:	sub	x8, x1, #0x1
   25728:	mov	x21, x0
   2572c:	ldr	x0, [x0, x8, lsl #3]
   25730:	str	x25, [sp, #16]
   25734:	mov	x29, sp
   25738:	cmp	x0, x2
   2573c:	b.cs	2574c <__gmpn_mod_1@@Base+0x29c>  // b.hs, b.nlast
   25740:	mov	x20, x8
   25744:	cbnz	x8, 25754 <__gmpn_mod_1@@Base+0x2a4>
   25748:	b	2592c <__gmpn_mod_1@@Base+0x47c>
   2574c:	mov	x20, x1
   25750:	mov	x0, xzr
   25754:	add	x8, x21, x20, lsl #3
   25758:	ldur	x25, [x8, #-8]
   2575c:	clz	x22, x2
   25760:	mov	w9, #0x40                  	// #64
   25764:	sub	w23, w9, w22
   25768:	neg	w9, w22
   2576c:	lsl	x8, x0, x22
   25770:	lsr	x9, x25, x9
   25774:	lsl	x19, x2, x22
   25778:	cmp	x20, #0x3
   2577c:	orr	x24, x9, x8
   25780:	b.le	2581c <__gmpn_mod_1@@Base+0x36c>
   25784:	mov	x0, x19
   25788:	bl	d5d0 <__gmpn_invert_limb@plt>
   2578c:	sub	x8, x21, #0x10
   25790:	ldr	x9, [x8, x20, lsl #3]
   25794:	lsl	x10, x25, x22
   25798:	umulh	x11, x24, x0
   2579c:	add	x13, x24, #0x1
   257a0:	lsr	x12, x9, x23
   257a4:	orr	x10, x12, x10
   257a8:	mul	x12, x24, x0
   257ac:	adds	x14, x12, x10
   257b0:	adc	x11, x11, x13
   257b4:	msub	x10, x11, x19, x10
   257b8:	cmp	x10, x14
   257bc:	csel	x11, x19, xzr, hi  // hi = pmore
   257c0:	add	x10, x11, x10
   257c4:	cmp	x10, x19
   257c8:	sub	x12, x20, #0x2
   257cc:	csel	x11, xzr, x19, cc  // cc = lo, ul, last
   257d0:	sub	x20, x20, #0x1
   257d4:	cmp	x12, #0x0
   257d8:	sub	x24, x10, x11
   257dc:	mov	x25, x9
   257e0:	b.gt	25790 <__gmpn_mod_1@@Base+0x2e0>
   257e4:	umulh	x8, x24, x0
   257e8:	mul	x10, x24, x0
   257ec:	lsl	x9, x9, x22
   257f0:	add	x11, x24, #0x1
   257f4:	adds	x12, x10, x9
   257f8:	adc	x8, x8, x11
   257fc:	msub	x8, x8, x19, x9
   25800:	cmp	x8, x12
   25804:	csel	x9, x19, xzr, hi  // hi = pmore
   25808:	add	x8, x9, x8
   2580c:	cmp	x8, x19
   25810:	csel	x9, xzr, x19, cc  // cc = lo, ul, last
   25814:	sub	x8, x8, x9
   25818:	b	25928 <__gmpn_mod_1@@Base+0x478>
   2581c:	cmp	x20, #0x2
   25820:	lsr	x8, x19, #32
   25824:	b.lt	258bc <__gmpn_mod_1@@Base+0x40c>  // b.tstop
   25828:	and	x10, x19, #0xffffffff
   2582c:	sub	x11, x21, #0x10
   25830:	b	2584c <__gmpn_mod_1@@Base+0x39c>
   25834:	sub	x14, x20, #0x2
   25838:	sub	x20, x20, #0x1
   2583c:	cmp	x14, #0x0
   25840:	sub	x24, x12, x13
   25844:	mov	x25, x9
   25848:	b.le	258c0 <__gmpn_mod_1@@Base+0x410>
   2584c:	ldr	x9, [x11, x20, lsl #3]
   25850:	lsl	x12, x25, x22
   25854:	udiv	x13, x24, x8
   25858:	msub	w14, w13, w8, w24
   2585c:	lsr	x15, x9, x23
   25860:	orr	x12, x15, x12
   25864:	mul	x13, x13, x10
   25868:	extr	x14, x14, x12, #32
   2586c:	cmp	x14, x13
   25870:	b.cs	25888 <__gmpn_mod_1@@Base+0x3d8>  // b.hs, b.nlast
   25874:	add	x14, x14, x19
   25878:	cmp	x14, x13
   2587c:	ccmp	x14, x19, #0x0, cc  // cc = lo, ul, last
   25880:	csel	x15, x19, xzr, cs  // cs = hs, nlast
   25884:	add	x14, x15, x14
   25888:	sub	x13, x14, x13
   2588c:	udiv	x14, x13, x8
   25890:	msub	w15, w14, w8, w13
   25894:	mul	x13, x14, x10
   25898:	bfi	x12, x15, #32, #32
   2589c:	cmp	x12, x13
   258a0:	b.cs	25834 <__gmpn_mod_1@@Base+0x384>  // b.hs, b.nlast
   258a4:	add	x12, x12, x19
   258a8:	cmp	x12, x13
   258ac:	ccmp	x12, x19, #0x0, cc  // cc = lo, ul, last
   258b0:	csel	x14, x19, xzr, cs  // cs = hs, nlast
   258b4:	add	x12, x14, x12
   258b8:	b	25834 <__gmpn_mod_1@@Base+0x384>
   258bc:	mov	x9, x25
   258c0:	udiv	x11, x24, x8
   258c4:	and	x10, x19, #0xffffffff
   258c8:	msub	w12, w11, w8, w24
   258cc:	lsl	x9, x9, x22
   258d0:	mul	x11, x11, x10
   258d4:	extr	x12, x12, x9, #32
   258d8:	cmp	x12, x11
   258dc:	b.cs	258f4 <__gmpn_mod_1@@Base+0x444>  // b.hs, b.nlast
   258e0:	add	x12, x12, x19
   258e4:	cmp	x12, x11
   258e8:	ccmp	x12, x19, #0x0, cc  // cc = lo, ul, last
   258ec:	csel	x13, x19, xzr, cs  // cs = hs, nlast
   258f0:	add	x12, x13, x12
   258f4:	sub	x11, x12, x11
   258f8:	udiv	x12, x11, x8
   258fc:	msub	w11, w12, w8, w11
   25900:	mul	x8, x12, x10
   25904:	bfi	x9, x11, #32, #32
   25908:	cmp	x9, x8
   2590c:	b.cs	25924 <__gmpn_mod_1@@Base+0x474>  // b.hs, b.nlast
   25910:	add	x9, x9, x19
   25914:	cmp	x9, x8
   25918:	ccmp	x9, x19, #0x0, cc  // cc = lo, ul, last
   2591c:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   25920:	add	x9, x10, x9
   25924:	sub	x8, x9, x8
   25928:	lsr	x0, x8, x22
   2592c:	ldp	x20, x19, [sp, #64]
   25930:	ldp	x22, x21, [sp, #48]
   25934:	ldp	x24, x23, [sp, #32]
   25938:	ldr	x25, [sp, #16]
   2593c:	ldp	x29, x30, [sp], #80
   25940:	ret
   25944:	nop
   25948:	nop
   2594c:	nop
   25950:	nop
   25954:	nop
   25958:	nop
   2595c:	nop

0000000000025960 <__gmpn_mod_34lsub1@@Base>:
   25960:	subs	x1, x1, #0x3
   25964:	mov	x8, #0x0                   	// #0
   25968:	b.lt	25a04 <__gmpn_mod_34lsub1@@Base+0xa4>  // b.tstop
   2596c:	ldp	x2, x3, [x0]
   25970:	ldr	x4, [x0, #16]
   25974:	add	x0, x0, #0x18
   25978:	subs	x1, x1, #0x3
   2597c:	b.lt	259a8 <__gmpn_mod_34lsub1@@Base+0x48>  // b.tstop
   25980:	cmn	x0, #0x0
   25984:	ldp	x5, x6, [x0]
   25988:	ldr	x7, [x0, #16]
   2598c:	add	x0, x0, #0x18
   25990:	sub	x1, x1, #0x3
   25994:	adcs	x2, x2, x5
   25998:	adcs	x3, x3, x6
   2599c:	adcs	x4, x4, x7
   259a0:	tbz	x1, #63, 25984 <__gmpn_mod_34lsub1@@Base+0x24>
   259a4:	adc	x8, xzr, xzr
   259a8:	cmn	x1, #0x2
   259ac:	mov	x5, #0x0                   	// #0
   259b0:	b.cc	259b8 <__gmpn_mod_34lsub1@@Base+0x58>  // b.lo, b.ul, b.last
   259b4:	ldr	x5, [x0], #8
   259b8:	mov	x6, #0x0                   	// #0
   259bc:	b.ls	259c4 <__gmpn_mod_34lsub1@@Base+0x64>  // b.plast
   259c0:	ldr	x6, [x0], #8
   259c4:	adds	x2, x2, x5
   259c8:	adcs	x3, x3, x6
   259cc:	adcs	x4, x4, xzr
   259d0:	adc	x8, x8, xzr
   259d4:	and	x0, x2, #0xffffffffffff
   259d8:	add	x0, x0, x2, lsr #48
   259dc:	add	x0, x0, x8
   259e0:	lsl	x8, x3, #16
   259e4:	and	x1, x8, #0xffffffffffff
   259e8:	add	x0, x0, x1
   259ec:	add	x0, x0, x3, lsr #32
   259f0:	lsl	x8, x4, #32
   259f4:	and	x1, x8, #0xffffffffffff
   259f8:	add	x0, x0, x1
   259fc:	add	x0, x0, x4, lsr #16
   25a00:	ret
   25a04:	cmn	x1, #0x1
   25a08:	b.ne	25a18 <__gmpn_mod_34lsub1@@Base+0xb8>  // b.any
   25a0c:	ldp	x2, x3, [x0]
   25a10:	mov	x4, #0x0                   	// #0
   25a14:	b	259d4 <__gmpn_mod_34lsub1@@Base+0x74>
   25a18:	ldr	x2, [x0]
   25a1c:	and	x0, x2, #0xffffffffffff
   25a20:	add	x0, x0, x2, lsr #48
   25a24:	ret

0000000000025a28 <__gmpn_modexact_1c_odd@@Base>:
   25a28:	subs	x8, x1, #0x1
   25a2c:	b.ne	25a54 <__gmpn_modexact_1c_odd@@Base+0x2c>  // b.any
   25a30:	ldr	x8, [x0]
   25a34:	subs	x9, x8, x3
   25a38:	b.ls	25ac4 <__gmpn_modexact_1c_odd@@Base+0x9c>  // b.plast
   25a3c:	udiv	x8, x9, x2
   25a40:	msub	x8, x8, x2, x9
   25a44:	sub	x9, x2, x8
   25a48:	cmp	x8, #0x0
   25a4c:	csel	x0, xzr, x9, eq  // eq = none
   25a50:	ret
   25a54:	adrp	x11, 69000 <__gmp_limbroots_table@@Base+0x11338>
   25a58:	ldr	x11, [x11, #3952]
   25a5c:	ubfx	x10, x2, #1, #7
   25a60:	mov	x9, xzr
   25a64:	ldrb	w10, [x11, x10]
   25a68:	mov	w11, #0x2                   	// #2
   25a6c:	msub	x12, x10, x2, x11
   25a70:	mul	x10, x12, x10
   25a74:	msub	x12, x10, x2, x11
   25a78:	mul	x10, x10, x12
   25a7c:	msub	x11, x10, x2, x11
   25a80:	mul	x10, x10, x11
   25a84:	ldr	x11, [x0, x9, lsl #3]
   25a88:	add	x9, x9, #0x1
   25a8c:	subs	x11, x11, x3
   25a90:	mul	x11, x11, x10
   25a94:	umulh	x11, x11, x2
   25a98:	cinc	x3, x11, cc  // cc = lo, ul, last
   25a9c:	cmp	x9, x8
   25aa0:	b.lt	25a84 <__gmpn_modexact_1c_odd@@Base+0x5c>  // b.tstop
   25aa4:	ldr	x8, [x0, x9, lsl #3]
   25aa8:	cmp	x8, x2
   25aac:	b.ls	25ad4 <__gmpn_modexact_1c_odd@@Base+0xac>  // b.plast
   25ab0:	subs	x8, x8, x3
   25ab4:	mul	x8, x8, x10
   25ab8:	umulh	x8, x8, x2
   25abc:	cinc	x0, x8, cc  // cc = lo, ul, last
   25ac0:	ret
   25ac4:	sub	x8, x3, x8
   25ac8:	udiv	x9, x8, x2
   25acc:	msub	x0, x9, x2, x8
   25ad0:	ret
   25ad4:	subs	x8, x3, x8
   25ad8:	csel	x9, x2, xzr, cc  // cc = lo, ul, last
   25adc:	add	x0, x8, x9
   25ae0:	ret

0000000000025ae4 <__gmpn_preinv_divrem_1@@Base>:
   25ae4:	sub	x12, x3, #0x1
   25ae8:	ldr	x11, [x2, x12, lsl #3]
   25aec:	add	x10, x12, x1
   25af0:	mov	w8, w6
   25af4:	lsl	x9, x4, x6
   25af8:	add	x10, x0, x10, lsl #3
   25afc:	cbz	w6, 25b1c <__gmpn_preinv_divrem_1@@Base+0x38>
   25b00:	cmp	x11, x4
   25b04:	b.cs	25b94 <__gmpn_preinv_divrem_1@@Base+0xb0>  // b.hs, b.nlast
   25b08:	lsl	x11, x11, x8
   25b0c:	str	xzr, [x10], #-8
   25b10:	mov	x3, x12
   25b14:	cbnz	x12, 25b98 <__gmpn_preinv_divrem_1@@Base+0xb4>
   25b18:	b	25c70 <__gmpn_preinv_divrem_1@@Base+0x18c>
   25b1c:	cmp	x11, x9
   25b20:	cset	w12, cs  // cs = hs, nlast
   25b24:	csel	x13, x9, xzr, cs  // cs = hs, nlast
   25b28:	cmp	x3, #0x1
   25b2c:	sub	x11, x11, x13
   25b30:	str	x12, [x10], #-8
   25b34:	b.le	25c70 <__gmpn_preinv_divrem_1@@Base+0x18c>
   25b38:	sub	x12, x2, #0x10
   25b3c:	ldr	x13, [x12, x3, lsl #3]
   25b40:	umulh	x14, x11, x5
   25b44:	mul	x15, x11, x5
   25b48:	add	x11, x11, #0x1
   25b4c:	adds	x16, x15, x13
   25b50:	adc	x11, x14, x11
   25b54:	msub	x13, x11, x9, x13
   25b58:	cmp	x13, x16
   25b5c:	cset	w15, hi  // hi = pmore
   25b60:	sub	x11, x11, x15
   25b64:	csel	x15, x9, xzr, hi  // hi = pmore
   25b68:	add	x13, x15, x13
   25b6c:	cmp	x13, x9
   25b70:	sub	x14, x3, #0x2
   25b74:	csel	x15, xzr, x9, cc  // cc = lo, ul, last
   25b78:	cinc	x16, x11, cs  // cs = hs, nlast
   25b7c:	sub	x3, x3, #0x1
   25b80:	cmp	x14, #0x0
   25b84:	sub	x11, x13, x15
   25b88:	str	x16, [x10], #-8
   25b8c:	b.gt	25b3c <__gmpn_preinv_divrem_1@@Base+0x58>
   25b90:	b	25c70 <__gmpn_preinv_divrem_1@@Base+0x18c>
   25b94:	mov	x11, xzr
   25b98:	add	x12, x2, x3, lsl #3
   25b9c:	ldur	x15, [x12, #-8]
   25ba0:	neg	w12, w6
   25ba4:	cmp	x3, #0x2
   25ba8:	lsr	x12, x15, x12
   25bac:	orr	x14, x12, x11
   25bb0:	b.lt	25c28 <__gmpn_preinv_divrem_1@@Base+0x144>  // b.tstop
   25bb4:	mov	w11, #0x40                  	// #64
   25bb8:	sub	w11, w11, w6
   25bbc:	sub	x12, x2, #0x10
   25bc0:	ldr	x13, [x12, x3, lsl #3]
   25bc4:	lsl	x15, x15, x8
   25bc8:	umulh	x16, x14, x5
   25bcc:	mul	x17, x14, x5
   25bd0:	lsr	x18, x13, x11
   25bd4:	add	x14, x14, #0x1
   25bd8:	orr	x15, x18, x15
   25bdc:	adds	x0, x17, x15
   25be0:	adc	x14, x16, x14
   25be4:	msub	x15, x14, x9, x15
   25be8:	cmp	x15, x0
   25bec:	csel	x17, x9, xzr, hi  // hi = pmore
   25bf0:	cset	w16, hi  // hi = pmore
   25bf4:	add	x15, x17, x15
   25bf8:	sub	x14, x14, x16
   25bfc:	cmp	x15, x9
   25c00:	sub	x18, x3, #0x2
   25c04:	cinc	x16, x14, cs  // cs = hs, nlast
   25c08:	csel	x14, xzr, x9, cc  // cc = lo, ul, last
   25c0c:	sub	x3, x3, #0x1
   25c10:	cmp	x18, #0x0
   25c14:	sub	x14, x15, x14
   25c18:	str	x16, [x10], #-8
   25c1c:	mov	x15, x13
   25c20:	b.gt	25bc0 <__gmpn_preinv_divrem_1@@Base+0xdc>
   25c24:	b	25c2c <__gmpn_preinv_divrem_1@@Base+0x148>
   25c28:	mov	x13, x15
   25c2c:	umulh	x11, x14, x5
   25c30:	mul	x12, x14, x5
   25c34:	lsl	x13, x13, x8
   25c38:	add	x14, x14, #0x1
   25c3c:	adds	x15, x12, x13
   25c40:	adc	x11, x11, x14
   25c44:	msub	x12, x11, x9, x13
   25c48:	cmp	x12, x15
   25c4c:	csel	x14, x9, xzr, hi  // hi = pmore
   25c50:	cset	w13, hi  // hi = pmore
   25c54:	add	x12, x14, x12
   25c58:	sub	x11, x11, x13
   25c5c:	cmp	x12, x9
   25c60:	cinc	x13, x11, cs  // cs = hs, nlast
   25c64:	csel	x11, xzr, x9, cc  // cc = lo, ul, last
   25c68:	sub	x11, x12, x11
   25c6c:	str	x13, [x10], #-8
   25c70:	cmp	x1, #0x1
   25c74:	b.lt	25cac <__gmpn_preinv_divrem_1@@Base+0x1c8>  // b.tstop
   25c78:	umulh	x12, x11, x5
   25c7c:	mul	x13, x11, x5
   25c80:	add	x11, x11, x12
   25c84:	add	x11, x11, #0x1
   25c88:	mneg	x12, x11, x9
   25c8c:	cmp	x13, x12
   25c90:	cset	w12, cc  // cc = lo, ul, last
   25c94:	csel	x13, x9, xzr, cc  // cc = lo, ul, last
   25c98:	sub	x12, x11, x12
   25c9c:	msub	x11, x11, x9, x13
   25ca0:	subs	x1, x1, #0x1
   25ca4:	str	x12, [x10], #-8
   25ca8:	b.ne	25c78 <__gmpn_preinv_divrem_1@@Base+0x194>  // b.any
   25cac:	lsr	x0, x11, x8
   25cb0:	ret

0000000000025cb4 <__gmpn_preinv_mod_1@@Base>:
   25cb4:	add	x9, x0, x1, lsl #3
   25cb8:	ldur	x9, [x9, #-8]
   25cbc:	mov	x8, x0
   25cc0:	cmp	x9, x2
   25cc4:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   25cc8:	cmp	x1, #0x2
   25ccc:	sub	x0, x9, x10
   25cd0:	b.lt	25d1c <__gmpn_preinv_mod_1@@Base+0x68>  // b.tstop
   25cd4:	sub	x8, x8, #0x10
   25cd8:	ldr	x9, [x8, x1, lsl #3]
   25cdc:	umulh	x10, x0, x3
   25ce0:	mul	x11, x0, x3
   25ce4:	add	x12, x0, #0x1
   25ce8:	adds	x13, x11, x9
   25cec:	adc	x10, x10, x12
   25cf0:	msub	x9, x10, x2, x9
   25cf4:	cmp	x9, x13
   25cf8:	csel	x10, x2, xzr, hi  // hi = pmore
   25cfc:	add	x9, x10, x9
   25d00:	cmp	x9, x2
   25d04:	sub	x11, x1, #0x2
   25d08:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   25d0c:	sub	x1, x1, #0x1
   25d10:	cmp	x11, #0x0
   25d14:	sub	x0, x9, x10
   25d18:	b.gt	25cd8 <__gmpn_preinv_mod_1@@Base+0x24>
   25d1c:	ret

0000000000025d20 <__gmpn_dump@@Base>:
   25d20:	stp	x29, x30, [sp, #-48]!
   25d24:	stp	x20, x19, [sp, #32]
   25d28:	mov	x19, x0
   25d2c:	sub	x8, x0, #0x8
   25d30:	str	x21, [sp, #16]
   25d34:	mov	x29, sp
   25d38:	mov	x20, x1
   25d3c:	subs	x1, x1, #0x1
   25d40:	b.lt	25d4c <__gmpn_dump@@Base+0x2c>  // b.tstop
   25d44:	ldr	x9, [x8, x20, lsl #3]
   25d48:	cbz	x9, 25d38 <__gmpn_dump@@Base+0x18>
   25d4c:	cbz	x20, 25da0 <__gmpn_dump@@Base+0x80>
   25d50:	add	x8, x19, x20, lsl #3
   25d54:	ldur	x1, [x8, #-8]
   25d58:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   25d5c:	add	x0, x0, #0x890
   25d60:	bl	d4c0 <printf@plt>
   25d64:	cmp	x20, #0x1
   25d68:	b.eq	25d94 <__gmpn_dump@@Base+0x74>  // b.none
   25d6c:	sub	x21, x19, #0x10
   25d70:	adrp	x19, 4f000 <__gmpn_bases@@Base+0x1f98>
   25d74:	add	x19, x19, #0x894
   25d78:	ldr	x2, [x21, x20, lsl #3]
   25d7c:	mov	w1, #0x10                  	// #16
   25d80:	mov	x0, x19
   25d84:	bl	d4c0 <printf@plt>
   25d88:	sub	x20, x20, #0x1
   25d8c:	cmp	x20, #0x1
   25d90:	b.ne	25d78 <__gmpn_dump@@Base+0x58>  // b.any
   25d94:	mov	w0, #0xa                   	// #10
   25d98:	bl	d4f0 <putchar@plt>
   25d9c:	b	25dac <__gmpn_dump@@Base+0x8c>
   25da0:	adrp	x0, 58000 <__gmp_limbroots_table@@Base+0x338>
   25da4:	add	x0, x0, #0x294
   25da8:	bl	cb70 <puts@plt>
   25dac:	ldp	x20, x19, [sp, #32]
   25db0:	ldr	x21, [sp, #16]
   25db4:	ldp	x29, x30, [sp], #48
   25db8:	ret

0000000000025dbc <__gmpn_mod_1_1p_cps@@Base>:
   25dbc:	stp	x29, x30, [sp, #-48]!
   25dc0:	str	x21, [sp, #16]
   25dc4:	clz	x21, x1
   25dc8:	stp	x20, x19, [sp, #32]
   25dcc:	lsl	x20, x1, x21
   25dd0:	mov	x19, x0
   25dd4:	mov	x0, x20
   25dd8:	mov	x29, sp
   25ddc:	bl	d5d0 <__gmpn_invert_limb@plt>
   25de0:	stp	x0, x21, [x19]
   25de4:	cbz	x21, 25e08 <__gmpn_mod_1_1p_cps@@Base+0x4c>
   25de8:	neg	w8, w21
   25dec:	mov	w9, #0x1                   	// #1
   25df0:	lsr	x8, x0, x8
   25df4:	lsl	x9, x9, x21
   25df8:	orr	x8, x8, x9
   25dfc:	mneg	x8, x20, x8
   25e00:	lsr	x8, x8, x21
   25e04:	str	x8, [x19, #16]
   25e08:	mneg	x8, x20, x0
   25e0c:	str	x8, [x19, #24]
   25e10:	ldp	x20, x19, [sp, #32]
   25e14:	ldr	x21, [sp, #16]
   25e18:	ldp	x29, x30, [sp], #48
   25e1c:	ret

0000000000025e20 <__gmpn_mod_1_1p@@Base>:
   25e20:	add	x10, x0, x1, lsl #3
   25e24:	ldp	x9, x11, [x10, #-16]
   25e28:	cmp	x1, #0x3
   25e2c:	b.lt	25ecc <__gmpn_mod_1_1p@@Base+0xac>  // b.tstop
   25e30:	ldr	x8, [x3, #24]
   25e34:	ldur	x10, [x10, #-24]
   25e38:	umulh	x13, x11, x8
   25e3c:	mov	w12, #0x2                   	// #2
   25e40:	mul	x11, x8, x11
   25e44:	adds	x10, x10, x11
   25e48:	cset	w11, cs  // cs = hs, nlast
   25e4c:	adds	x9, x13, x9
   25e50:	cset	w13, cs  // cs = hs, nlast
   25e54:	csinc	x12, x12, xzr, cs  // cs = hs, nlast
   25e58:	adds	x9, x9, x11
   25e5c:	csel	x11, x13, x12, cc  // cc = lo, ul, last
   25e60:	cmp	x1, #0x3
   25e64:	neg	x13, x11
   25e68:	b.eq	25ec0 <__gmpn_mod_1_1p@@Base+0xa0>  // b.none
   25e6c:	sub	x11, x0, #0x20
   25e70:	mov	w12, #0x2                   	// #2
   25e74:	ldr	x15, [x11, x1, lsl #3]
   25e78:	and	x13, x13, x8
   25e7c:	adds	x10, x10, x13
   25e80:	umulh	x14, x9, x8
   25e84:	mul	x9, x9, x8
   25e88:	csel	x13, x2, xzr, cs  // cs = hs, nlast
   25e8c:	sub	x13, x10, x13
   25e90:	adds	x10, x15, x9
   25e94:	cset	w9, cs  // cs = hs, nlast
   25e98:	adds	x13, x14, x13
   25e9c:	cset	w14, cs  // cs = hs, nlast
   25ea0:	csinc	x15, x12, xzr, cs  // cs = hs, nlast
   25ea4:	adds	x9, x13, x9
   25ea8:	sub	x13, x1, #0x4
   25eac:	csel	x14, x14, x15, cc  // cc = lo, ul, last
   25eb0:	sub	x1, x1, #0x1
   25eb4:	cmp	x13, #0x0
   25eb8:	neg	x13, x14
   25ebc:	b.gt	25e74 <__gmpn_mod_1_1p@@Base+0x54>
   25ec0:	and	x8, x13, x2
   25ec4:	sub	x11, x9, x8
   25ec8:	mov	x9, x10
   25ecc:	ldr	x8, [x3, #8]
   25ed0:	cbz	w8, 25f38 <__gmpn_mod_1_1p@@Base+0x118>
   25ed4:	ldr	x10, [x3, #16]
   25ed8:	umulh	x13, x11, x10
   25edc:	neg	w12, w8
   25ee0:	mul	x10, x10, x11
   25ee4:	adds	x9, x10, x9
   25ee8:	cinc	x10, x13, cs  // cs = hs, nlast
   25eec:	lsr	x11, x9, x12
   25ef0:	lsl	x10, x10, x8
   25ef4:	orr	x10, x10, x11
   25ef8:	lsl	x9, x9, x8
   25efc:	ldr	x11, [x3]
   25f00:	umulh	x12, x10, x11
   25f04:	mul	x11, x11, x10
   25f08:	add	x10, x10, #0x1
   25f0c:	adds	x13, x11, x9
   25f10:	adc	x10, x12, x10
   25f14:	msub	x9, x10, x2, x9
   25f18:	cmp	x9, x13
   25f1c:	csel	x10, x2, xzr, hi  // hi = pmore
   25f20:	add	x9, x10, x9
   25f24:	cmp	x9, x2
   25f28:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   25f2c:	sub	x9, x9, x10
   25f30:	lsr	x0, x9, x8
   25f34:	ret
   25f38:	cmp	x11, x2
   25f3c:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   25f40:	sub	x10, x11, x10
   25f44:	b	25efc <__gmpn_mod_1_1p@@Base+0xdc>

0000000000025f48 <__gmpn_mod_1s_2p_cps@@Base>:
   25f48:	stp	x29, x30, [sp, #-48]!
   25f4c:	str	x21, [sp, #16]
   25f50:	clz	x21, x1
   25f54:	stp	x20, x19, [sp, #32]
   25f58:	lsl	x20, x1, x21
   25f5c:	mov	x19, x0
   25f60:	mov	x0, x20
   25f64:	mov	x29, sp
   25f68:	bl	d5d0 <__gmpn_invert_limb@plt>
   25f6c:	neg	w8, w21
   25f70:	mov	w9, #0x1                   	// #1
   25f74:	lsr	x8, x0, x8
   25f78:	lsl	x9, x9, x21
   25f7c:	orr	x8, x8, x9
   25f80:	mul	x9, x8, x20
   25f84:	mneg	x8, x8, x20
   25f88:	lsr	x10, x8, x21
   25f8c:	umulh	x8, x8, x0
   25f90:	mvn	x8, x8
   25f94:	add	x8, x9, x8
   25f98:	mneg	x11, x9, x0
   25f9c:	mul	x8, x8, x20
   25fa0:	cmp	x8, x11
   25fa4:	csel	x9, x20, xzr, hi  // hi = pmore
   25fa8:	add	x8, x9, x8
   25fac:	lsr	x9, x8, x21
   25fb0:	umulh	x11, x8, x0
   25fb4:	mul	x12, x8, x0
   25fb8:	add	x8, x8, x11
   25fbc:	mvn	x8, x8
   25fc0:	mul	x8, x20, x8
   25fc4:	cmp	x8, x12
   25fc8:	stp	x10, x9, [x19, #16]
   25fcc:	csel	x9, x20, xzr, hi  // hi = pmore
   25fd0:	add	x8, x9, x8
   25fd4:	lsr	x8, x8, x21
   25fd8:	stp	x0, x21, [x19]
   25fdc:	str	x8, [x19, #32]
   25fe0:	ldp	x20, x19, [sp, #32]
   25fe4:	ldr	x21, [sp, #16]
   25fe8:	ldp	x29, x30, [sp], #48
   25fec:	ret

0000000000025ff0 <__gmpn_mod_1s_2p@@Base>:
   25ff0:	ldp	x8, x9, [x3, #16]
   25ff4:	ldr	x10, [x3, #32]
   25ff8:	tbnz	w1, #0, 260a4 <__gmpn_mod_1s_2p@@Base+0xb4>
   25ffc:	add	x12, x0, x1, lsl #3
   26000:	ldp	x12, x11, [x12, #-16]
   26004:	cmp	x1, #0x4
   26008:	b.lt	26058 <__gmpn_mod_1s_2p@@Base+0x68>  // b.tstop
   2600c:	add	x14, x0, x1, lsl #3
   26010:	ldp	x14, x15, [x14, #-32]
   26014:	mov	x13, xzr
   26018:	mul	x16, x15, x8
   2601c:	umulh	x15, x15, x8
   26020:	adds	x17, x16, x14
   26024:	adc	x13, x15, x13
   26028:	mul	x14, x12, x9
   2602c:	umulh	x12, x12, x9
   26030:	mul	x15, x11, x10
   26034:	umulh	x11, x11, x10
   26038:	adds	x16, x17, x14
   2603c:	adc	x13, x13, x12
   26040:	sub	x14, x1, #0x4
   26044:	adds	x12, x15, x16
   26048:	adc	x11, x11, x13
   2604c:	sub	x1, x1, #0x2
   26050:	cmp	x14, #0x1
   26054:	b.gt	2600c <__gmpn_mod_1s_2p@@Base+0x1c>
   26058:	mul	x10, x11, x8
   2605c:	umulh	x8, x11, x8
   26060:	ldp	x11, x13, [x3]
   26064:	mov	x9, xzr
   26068:	adds	x14, x12, x10
   2606c:	adc	x9, x8, x9
   26070:	neg	w10, w13
   26074:	lsl	x9, x9, x13
   26078:	lsr	x10, x14, x10
   2607c:	orr	x9, x10, x9
   26080:	umulh	x10, x9, x11
   26084:	mul	x11, x9, x11
   26088:	add	x9, x9, #0x1
   2608c:	and	x8, x13, #0xffffffff
   26090:	lsl	x12, x14, x13
   26094:	adds	x13, x11, x12
   26098:	adc	x9, x10, x9
   2609c:	msub	x9, x9, x2, x12
   260a0:	b	260dc <__gmpn_mod_1s_2p@@Base+0xec>
   260a4:	subs	x13, x1, #0x1
   260a8:	b.ne	26104 <__gmpn_mod_1s_2p@@Base+0x114>  // b.any
   260ac:	ldp	x11, x9, [x3]
   260b0:	ldr	x10, [x0]
   260b4:	neg	w12, w9
   260b8:	and	x8, x9, #0xffffffff
   260bc:	lsl	x9, x10, x9
   260c0:	lsr	x10, x10, x12
   260c4:	umulh	x12, x10, x11
   260c8:	mul	x11, x10, x11
   260cc:	add	x10, x10, #0x1
   260d0:	adds	x13, x11, x9
   260d4:	adc	x10, x12, x10
   260d8:	msub	x9, x10, x2, x9
   260dc:	cmp	x9, x13
   260e0:	cset	w10, hi  // hi = pmore
   260e4:	cmp	w10, #0x0
   260e8:	csel	x10, x2, xzr, ne  // ne = any
   260ec:	add	x9, x10, x9
   260f0:	cmp	x9, x2
   260f4:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   260f8:	sub	x9, x9, x10
   260fc:	lsr	x0, x9, x8
   26100:	ret
   26104:	add	x12, x0, x1, lsl #3
   26108:	ldp	x12, x15, [x12, #-24]
   2610c:	ldr	x14, [x0, x13, lsl #3]
   26110:	mov	x11, xzr
   26114:	mov	x1, x13
   26118:	mul	x17, x15, x8
   2611c:	umulh	x15, x15, x8
   26120:	adds	x18, x17, x12
   26124:	adc	x11, x15, x11
   26128:	mul	x16, x14, x9
   2612c:	umulh	x14, x14, x9
   26130:	adds	x12, x16, x18
   26134:	adc	x11, x14, x11
   26138:	cmp	x1, #0x4
   2613c:	b.ge	2600c <__gmpn_mod_1s_2p@@Base+0x1c>  // b.tcont
   26140:	b	26058 <__gmpn_mod_1s_2p@@Base+0x68>

0000000000026144 <__gmpn_mod_1s_3p_cps@@Base>:
   26144:	stp	x29, x30, [sp, #-48]!
   26148:	str	x21, [sp, #16]
   2614c:	clz	x21, x1
   26150:	stp	x20, x19, [sp, #32]
   26154:	lsl	x20, x1, x21
   26158:	mov	x19, x0
   2615c:	mov	x0, x20
   26160:	mov	x29, sp
   26164:	bl	d5d0 <__gmpn_invert_limb@plt>
   26168:	neg	w8, w21
   2616c:	mov	w9, #0x1                   	// #1
   26170:	lsr	x8, x0, x8
   26174:	lsl	x9, x9, x21
   26178:	orr	x8, x8, x9
   2617c:	mul	x9, x8, x20
   26180:	mneg	x8, x8, x20
   26184:	lsr	x10, x8, x21
   26188:	umulh	x8, x8, x0
   2618c:	mvn	x8, x8
   26190:	add	x8, x9, x8
   26194:	mneg	x11, x9, x0
   26198:	mul	x8, x8, x20
   2619c:	cmp	x8, x11
   261a0:	csel	x9, x20, xzr, hi  // hi = pmore
   261a4:	add	x8, x9, x8
   261a8:	lsr	x9, x8, x21
   261ac:	umulh	x11, x8, x0
   261b0:	stp	x10, x9, [x19, #16]
   261b4:	mul	x9, x8, x0
   261b8:	add	x8, x8, x11
   261bc:	mvn	x8, x8
   261c0:	mul	x8, x20, x8
   261c4:	cmp	x8, x9
   261c8:	csel	x9, x20, xzr, hi  // hi = pmore
   261cc:	add	x8, x9, x8
   261d0:	lsr	x9, x8, x21
   261d4:	umulh	x10, x8, x0
   261d8:	mul	x11, x8, x0
   261dc:	add	x8, x8, x10
   261e0:	mvn	x8, x8
   261e4:	mul	x8, x20, x8
   261e8:	cmp	x8, x11
   261ec:	csel	x10, x20, xzr, hi  // hi = pmore
   261f0:	add	x8, x10, x8
   261f4:	lsr	x8, x8, x21
   261f8:	stp	x0, x21, [x19]
   261fc:	stp	x9, x8, [x19, #32]
   26200:	ldp	x20, x19, [sp, #32]
   26204:	ldr	x21, [sp, #16]
   26208:	ldp	x29, x30, [sp], #48
   2620c:	ret

0000000000026210 <__gmpn_mod_1s_3p@@Base>:
   26210:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   26214:	ldp	x8, x9, [x3, #16]
   26218:	ldp	x10, x11, [x3, #32]
   2621c:	movk	x12, #0xaaab
   26220:	mul	x12, x1, x12
   26224:	lsr	x12, x12, #62
   26228:	cmp	w12, #0x2
   2622c:	b.eq	2627c <__gmpn_mod_1s_3p@@Base+0x6c>  // b.none
   26230:	cmp	w12, #0x1
   26234:	b.eq	26294 <__gmpn_mod_1s_3p@@Base+0x84>  // b.none
   26238:	cbnz	w12, 262b0 <__gmpn_mod_1s_3p@@Base+0xa0>
   2623c:	add	x13, x0, x1, lsl #3
   26240:	ldp	x14, x13, [x13, #-16]
   26244:	mov	x12, xzr
   26248:	sub	x1, x1, #0x3
   2624c:	ldr	x15, [x0, x1, lsl #3]
   26250:	mul	x16, x14, x8
   26254:	umulh	x14, x14, x8
   26258:	adds	x18, x16, x15
   2625c:	adc	x12, x14, x12
   26260:	mul	x17, x13, x9
   26264:	umulh	x14, x13, x9
   26268:	adds	x13, x17, x18
   2626c:	adc	x12, x14, x12
   26270:	cmp	x1, #0x3
   26274:	b.ge	262b8 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   26278:	b	26314 <__gmpn_mod_1s_3p@@Base+0x104>
   2627c:	sub	x1, x1, #0x1
   26280:	ldr	x13, [x0, x1, lsl #3]
   26284:	mov	x12, xzr
   26288:	cmp	x1, #0x3
   2628c:	b.ge	262b8 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   26290:	b	26314 <__gmpn_mod_1s_3p@@Base+0x104>
   26294:	add	x12, x0, x1, lsl #3
   26298:	sub	x1, x1, #0x2
   2629c:	ldur	x12, [x12, #-8]
   262a0:	ldr	x13, [x0, x1, lsl #3]
   262a4:	cmp	x1, #0x3
   262a8:	b.ge	262b8 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   262ac:	b	26314 <__gmpn_mod_1s_3p@@Base+0x104>
   262b0:	cmp	x1, #0x3
   262b4:	b.lt	26314 <__gmpn_mod_1s_3p@@Base+0x104>  // b.tstop
   262b8:	add	x15, x0, x1, lsl #3
   262bc:	ldp	x18, x17, [x15, #-24]
   262c0:	ldur	x15, [x15, #-8]
   262c4:	mov	x14, xzr
   262c8:	mul	x16, x13, x10
   262cc:	mul	x4, x17, x8
   262d0:	umulh	x17, x17, x8
   262d4:	adds	x5, x4, x18
   262d8:	adc	x14, x17, x14
   262dc:	umulh	x13, x13, x10
   262e0:	mul	x17, x12, x11
   262e4:	umulh	x12, x12, x11
   262e8:	mul	x18, x15, x9
   262ec:	umulh	x15, x15, x9
   262f0:	adds	x4, x5, x18
   262f4:	adc	x14, x14, x15
   262f8:	adds	x15, x4, x16
   262fc:	adc	x14, x14, x13
   26300:	adds	x13, x17, x15
   26304:	adc	x12, x12, x14
   26308:	cmp	x1, #0x5
   2630c:	sub	x1, x1, #0x3
   26310:	b.gt	262b8 <__gmpn_mod_1s_3p@@Base+0xa8>
   26314:	mul	x10, x12, x8
   26318:	umulh	x8, x12, x8
   2631c:	ldp	x12, x11, [x3]
   26320:	mov	x9, xzr
   26324:	adds	x14, x13, x10
   26328:	adc	x8, x8, x9
   2632c:	neg	w10, w11
   26330:	lsl	x8, x8, x11
   26334:	lsr	x10, x14, x10
   26338:	orr	x8, x10, x8
   2633c:	umulh	x10, x8, x12
   26340:	mul	x12, x8, x12
   26344:	add	x8, x8, #0x1
   26348:	lsl	x13, x14, x11
   2634c:	adds	x14, x12, x13
   26350:	adc	x8, x10, x8
   26354:	msub	x8, x8, x2, x13
   26358:	cmp	x8, x14
   2635c:	csel	x10, x2, x9, hi  // hi = pmore
   26360:	add	x8, x10, x8
   26364:	cmp	x8, x2
   26368:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   2636c:	sub	x8, x8, x9
   26370:	lsr	x0, x8, x11
   26374:	ret

0000000000026378 <__gmpn_mod_1s_4p_cps@@Base>:
   26378:	stp	x29, x30, [sp, #-48]!
   2637c:	str	x21, [sp, #16]
   26380:	clz	x21, x1
   26384:	stp	x20, x19, [sp, #32]
   26388:	lsl	x20, x1, x21
   2638c:	mov	x19, x0
   26390:	mov	x0, x20
   26394:	mov	x29, sp
   26398:	bl	d5d0 <__gmpn_invert_limb@plt>
   2639c:	neg	w8, w21
   263a0:	mov	w9, #0x1                   	// #1
   263a4:	lsr	x8, x0, x8
   263a8:	lsl	x9, x9, x21
   263ac:	orr	x8, x8, x9
   263b0:	mul	x9, x8, x20
   263b4:	mneg	x8, x8, x20
   263b8:	lsr	x10, x8, x21
   263bc:	umulh	x8, x8, x0
   263c0:	mvn	x8, x8
   263c4:	add	x8, x9, x8
   263c8:	mneg	x11, x9, x0
   263cc:	mul	x8, x8, x20
   263d0:	cmp	x8, x11
   263d4:	csel	x9, x20, xzr, hi  // hi = pmore
   263d8:	add	x8, x9, x8
   263dc:	lsr	x9, x8, x21
   263e0:	umulh	x11, x8, x0
   263e4:	mul	x12, x8, x0
   263e8:	add	x8, x8, x11
   263ec:	mvn	x8, x8
   263f0:	mul	x8, x20, x8
   263f4:	cmp	x8, x12
   263f8:	stp	x10, x9, [x19, #16]
   263fc:	csel	x9, x20, xzr, hi  // hi = pmore
   26400:	add	x8, x9, x8
   26404:	lsr	x9, x8, x21
   26408:	umulh	x10, x8, x0
   2640c:	mul	x11, x8, x0
   26410:	add	x8, x8, x10
   26414:	mvn	x8, x8
   26418:	mul	x8, x20, x8
   2641c:	cmp	x8, x11
   26420:	csel	x10, x20, xzr, hi  // hi = pmore
   26424:	add	x8, x10, x8
   26428:	lsr	x10, x8, x21
   2642c:	umulh	x11, x8, x0
   26430:	mul	x12, x8, x0
   26434:	add	x8, x8, x11
   26438:	mvn	x8, x8
   2643c:	mul	x8, x20, x8
   26440:	cmp	x8, x12
   26444:	stp	x9, x10, [x19, #32]
   26448:	csel	x9, x20, xzr, hi  // hi = pmore
   2644c:	add	x8, x9, x8
   26450:	lsr	x8, x8, x21
   26454:	stp	x0, x21, [x19]
   26458:	str	x8, [x19, #48]
   2645c:	ldp	x20, x19, [sp, #32]
   26460:	ldr	x21, [sp, #16]
   26464:	ldp	x29, x30, [sp], #48
   26468:	ret

000000000002646c <__gmpn_mod_1s_4p@@Base>:
   2646c:	ldp	x8, x9, [x3, #16]
   26470:	ldp	x10, x11, [x3, #32]
   26474:	ldr	x12, [x3, #48]
   26478:	adrp	x14, 4f000 <__gmpn_bases@@Base+0x1f98>
   2647c:	and	x13, x1, #0x3
   26480:	add	x14, x14, #0x89a
   26484:	adr	x15, 26494 <__gmpn_mod_1s_4p@@Base+0x28>
   26488:	ldrb	w16, [x14, x13]
   2648c:	add	x15, x15, x16, lsl #2
   26490:	br	x15
   26494:	add	x14, x0, x1, lsl #3
   26498:	ldp	x16, x18, [x14, #-24]
   2649c:	ldur	x14, [x14, #-8]
   264a0:	mov	x13, xzr
   264a4:	sub	x15, x1, #0x4
   264a8:	ldr	x17, [x0, x15, lsl #3]
   264ac:	mul	x1, x16, x8
   264b0:	umulh	x16, x16, x8
   264b4:	adds	x4, x1, x17
   264b8:	adc	x13, x16, x13
   264bc:	mul	x16, x18, x9
   264c0:	umulh	x17, x18, x9
   264c4:	adds	x18, x4, x16
   264c8:	adc	x13, x13, x17
   264cc:	mul	x16, x14, x10
   264d0:	umulh	x17, x14, x10
   264d4:	adds	x14, x16, x18
   264d8:	adc	x13, x17, x13
   264dc:	cmp	x15, #0x4
   264e0:	b.ge	26558 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   264e4:	b	265cc <__gmpn_mod_1s_4p@@Base+0x160>
   264e8:	add	x13, x0, x1, lsl #3
   264ec:	sub	x15, x1, #0x2
   264f0:	ldur	x13, [x13, #-8]
   264f4:	ldr	x14, [x0, x15, lsl #3]
   264f8:	cmp	x15, #0x4
   264fc:	b.ge	26558 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   26500:	b	265cc <__gmpn_mod_1s_4p@@Base+0x160>
   26504:	add	x14, x0, x1, lsl #3
   26508:	ldp	x16, x14, [x14, #-16]
   2650c:	mov	x13, xzr
   26510:	sub	x15, x1, #0x3
   26514:	ldr	x17, [x0, x15, lsl #3]
   26518:	mul	x18, x16, x8
   2651c:	umulh	x16, x16, x8
   26520:	adds	x4, x18, x17
   26524:	adc	x13, x16, x13
   26528:	mul	x1, x14, x9
   2652c:	umulh	x16, x14, x9
   26530:	adds	x14, x1, x4
   26534:	adc	x13, x16, x13
   26538:	cmp	x15, #0x4
   2653c:	b.ge	26558 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   26540:	b	265cc <__gmpn_mod_1s_4p@@Base+0x160>
   26544:	sub	x15, x1, #0x1
   26548:	ldr	x14, [x0, x15, lsl #3]
   2654c:	mov	x13, xzr
   26550:	cmp	x15, #0x4
   26554:	b.lt	265cc <__gmpn_mod_1s_4p@@Base+0x160>  // b.tstop
   26558:	add	x16, x15, #0x4
   2655c:	add	x15, x0, x15, lsl #3
   26560:	sub	x15, x15, #0x10
   26564:	ldp	x0, x18, [x15, #-16]
   26568:	mov	x17, xzr
   2656c:	sub	x16, x16, #0x4
   26570:	mul	x1, x18, x8
   26574:	umulh	x18, x18, x8
   26578:	adds	x5, x1, x0
   2657c:	adc	x17, x18, x17
   26580:	ldp	x4, x18, [x15], #-32
   26584:	umulh	x1, x4, x9
   26588:	mul	x0, x4, x9
   2658c:	adds	x4, x5, x0
   26590:	adc	x17, x17, x1
   26594:	mul	x0, x18, x10
   26598:	umulh	x18, x18, x10
   2659c:	adds	x1, x4, x0
   265a0:	adc	x17, x17, x18
   265a4:	mul	x18, x14, x11
   265a8:	umulh	x14, x14, x11
   265ac:	mul	x0, x13, x12
   265b0:	umulh	x13, x13, x12
   265b4:	adds	x4, x1, x18
   265b8:	adc	x17, x17, x14
   265bc:	adds	x14, x0, x4
   265c0:	adc	x13, x13, x17
   265c4:	cmp	x16, #0x7
   265c8:	b.gt	26564 <__gmpn_mod_1s_4p@@Base+0xf8>
   265cc:	ldp	x12, x11, [x3]
   265d0:	mul	x10, x13, x8
   265d4:	umulh	x8, x13, x8
   265d8:	mov	x9, xzr
   265dc:	adds	x13, x14, x10
   265e0:	adc	x8, x8, x9
   265e4:	neg	w10, w11
   265e8:	lsl	x8, x8, x11
   265ec:	lsr	x10, x13, x10
   265f0:	orr	x8, x10, x8
   265f4:	umulh	x10, x8, x12
   265f8:	mul	x12, x8, x12
   265fc:	add	x8, x8, #0x1
   26600:	lsl	x13, x13, x11
   26604:	adds	x14, x12, x13
   26608:	adc	x8, x10, x8
   2660c:	msub	x8, x8, x2, x13
   26610:	cmp	x8, x14
   26614:	csel	x10, x2, x9, hi  // hi = pmore
   26618:	add	x8, x10, x8
   2661c:	cmp	x8, x2
   26620:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   26624:	sub	x8, x8, x9
   26628:	lsr	x0, x8, x11
   2662c:	ret

0000000000026630 <__gmpn_lshiftc@@Base>:
   26630:	add	x16, x0, x2, lsl #3
   26634:	add	x1, x1, x2, lsl #3
   26638:	neg	x8, x3
   2663c:	lsr	x18, x2, #2
   26640:	tbz	w2, #0, 26684 <__gmpn_lshiftc@@Base+0x54>
   26644:	ldur	x4, [x1, #-8]
   26648:	tbnz	w2, #1, 26674 <__gmpn_lshiftc@@Base+0x44>
   2664c:	lsr	x0, x4, x8
   26650:	lsl	x2, x4, x3
   26654:	cbnz	x18, 26664 <__gmpn_lshiftc@@Base+0x34>
   26658:	mvn	x2, x2
   2665c:	stur	x2, [x16, #-8]
   26660:	ret
   26664:	ldp	x4, x5, [x1, #-24]
   26668:	sub	x1, x1, #0x8
   2666c:	add	x16, x16, #0x10
   26670:	b	26704 <__gmpn_lshiftc@@Base+0xd4>
   26674:	lsr	x0, x4, x8
   26678:	lsl	x2, x4, x3
   2667c:	ldp	x6, x7, [x1, #-24]!
   26680:	b	26728 <__gmpn_lshiftc@@Base+0xf8>
   26684:	ldp	x4, x5, [x1, #-16]
   26688:	tbz	w2, #1, 266c8 <__gmpn_lshiftc@@Base+0x98>
   2668c:	lsr	x0, x5, x8
   26690:	lsl	x13, x5, x3
   26694:	lsr	x10, x4, x8
   26698:	lsl	x2, x4, x3
   2669c:	cbnz	x18, 266b0 <__gmpn_lshiftc@@Base+0x80>
   266a0:	eon	x10, x10, x13
   266a4:	mvn	x2, x2
   266a8:	stp	x2, x10, [x16, #-16]
   266ac:	ret
   266b0:	ldp	x4, x5, [x1, #-32]
   266b4:	eon	x10, x10, x13
   266b8:	stur	x10, [x16, #-8]
   266bc:	sub	x1, x1, #0x10
   266c0:	add	x16, x16, #0x8
   266c4:	b	26704 <__gmpn_lshiftc@@Base+0xd4>
   266c8:	lsr	x0, x5, x8
   266cc:	lsl	x13, x5, x3
   266d0:	lsr	x10, x4, x8
   266d4:	lsl	x2, x4, x3
   266d8:	ldp	x6, x7, [x1, #-32]!
   266dc:	eon	x10, x10, x13
   266e0:	str	x10, [x16, #-8]!
   266e4:	b	26724 <__gmpn_lshiftc@@Base+0xf4>
   266e8:	nop
   266ec:	nop
   266f0:	ldp	x4, x5, [x1, #-16]
   266f4:	eon	x10, x10, x13
   266f8:	eon	x11, x12, x2
   266fc:	stp	x10, x11, [x16, #-16]
   26700:	lsl	x2, x6, x3
   26704:	lsr	x10, x4, x8
   26708:	lsl	x13, x5, x3
   2670c:	lsr	x12, x5, x8
   26710:	ldp	x6, x7, [x1, #-32]!
   26714:	eon	x10, x10, x13
   26718:	eon	x11, x12, x2
   2671c:	stp	x10, x11, [x16, #-32]!
   26720:	lsl	x2, x4, x3
   26724:	sub	x18, x18, #0x1
   26728:	lsr	x10, x6, x8
   2672c:	lsl	x13, x7, x3
   26730:	lsr	x12, x7, x8
   26734:	cbnz	x18, 266f0 <__gmpn_lshiftc@@Base+0xc0>
   26738:	eon	x10, x10, x13
   2673c:	eon	x11, x12, x2
   26740:	lsl	x2, x6, x3
   26744:	stp	x10, x11, [x16, #-16]
   26748:	mvn	x2, x2
   2674c:	stur	x2, [x16, #-24]
   26750:	ret

0000000000026754 <__gmpn_mul@@Base>:
   26754:	stp	x29, x30, [sp, #-96]!
   26758:	stp	x28, x27, [sp, #16]
   2675c:	stp	x26, x25, [sp, #32]
   26760:	stp	x24, x23, [sp, #48]
   26764:	stp	x22, x21, [sp, #64]
   26768:	stp	x20, x19, [sp, #80]
   2676c:	mov	x29, sp
   26770:	sub	sp, sp, #0xa0
   26774:	mov	x19, x4
   26778:	mov	x28, x3
   2677c:	mov	x20, x2
   26780:	mov	x23, x1
   26784:	cmp	x2, #0xd
   26788:	mov	x21, x0
   2678c:	b.le	26898 <__gmpn_mul@@Base+0x144>
   26790:	cmp	x20, x19
   26794:	b.ne	267b0 <__gmpn_mul@@Base+0x5c>  // b.any
   26798:	mov	x0, x21
   2679c:	mov	x1, x23
   267a0:	mov	x2, x28
   267a4:	mov	x3, x20
   267a8:	bl	cb50 <__gmpn_mul_n@plt>
   267ac:	b	268b0 <__gmpn_mul@@Base+0x15c>
   267b0:	cmp	x19, #0xd
   267b4:	b.gt	268dc <__gmpn_mul@@Base+0x188>
   267b8:	cmp	x20, #0x1f5
   267bc:	b.lt	26898 <__gmpn_mul@@Base+0x144>  // b.tstop
   267c0:	cmp	x19, #0x1
   267c4:	b.eq	26898 <__gmpn_mul@@Base+0x144>  // b.none
   267c8:	mov	w2, #0x1f4                 	// #500
   267cc:	mov	x0, x21
   267d0:	mov	x1, x23
   267d4:	mov	x3, x28
   267d8:	mov	x4, x19
   267dc:	bl	c6e0 <__gmpn_mul_basecase@plt>
   267e0:	add	x24, x21, #0xfa0
   267e4:	sub	x0, x29, #0x70
   267e8:	mov	x1, x24
   267ec:	mov	x2, x19
   267f0:	bl	cc10 <__gmpn_copyi@plt>
   267f4:	sub	x22, x20, #0x1f4
   267f8:	cmp	x20, #0x3e9
   267fc:	add	x23, x23, #0xfa0
   26800:	b.lt	26a7c <__gmpn_mul@@Base+0x328>  // b.tstop
   26804:	add	x8, x21, x19, lsl #3
   26808:	add	x25, x8, #0xfa8
   2680c:	lsl	x26, x19, #3
   26810:	mov	x21, x24
   26814:	b	26844 <__gmpn_mul@@Base+0xf0>
   26818:	add	x21, x21, #0xfa0
   2681c:	sub	x0, x29, #0x70
   26820:	mov	x1, x21
   26824:	mov	x2, x19
   26828:	bl	cc10 <__gmpn_copyi@plt>
   2682c:	sub	x20, x22, #0x1f4
   26830:	add	x23, x23, #0xfa0
   26834:	cmp	x22, #0x3e8
   26838:	add	x25, x25, #0xfa0
   2683c:	mov	x22, x20
   26840:	b.le	26a84 <__gmpn_mul@@Base+0x330>
   26844:	mov	w2, #0x1f4                 	// #500
   26848:	mov	x0, x21
   2684c:	mov	x1, x23
   26850:	mov	x3, x28
   26854:	mov	x4, x19
   26858:	bl	c6e0 <__gmpn_mul_basecase@plt>
   2685c:	sub	x2, x29, #0x70
   26860:	mov	x0, x21
   26864:	mov	x1, x21
   26868:	mov	x3, x19
   2686c:	bl	cc30 <__gmpn_add_n@plt>
   26870:	ldr	x8, [x21, x26]
   26874:	adds	x8, x8, x0
   26878:	str	x8, [x21, x26]
   2687c:	b.cc	26818 <__gmpn_mul@@Base+0xc4>  // b.lo, b.ul, b.last
   26880:	mov	x8, x25
   26884:	ldr	x9, [x8]
   26888:	adds	x9, x9, #0x1
   2688c:	str	x9, [x8], #8
   26890:	b.cs	26884 <__gmpn_mul@@Base+0x130>  // b.hs, b.nlast
   26894:	b	26818 <__gmpn_mul@@Base+0xc4>
   26898:	mov	x0, x21
   2689c:	mov	x1, x23
   268a0:	mov	x2, x20
   268a4:	mov	x3, x28
   268a8:	mov	x4, x19
   268ac:	bl	c6e0 <__gmpn_mul_basecase@plt>
   268b0:	add	x8, x19, x20
   268b4:	add	x8, x21, x8, lsl #3
   268b8:	ldur	x0, [x8, #-8]
   268bc:	mov	sp, x29
   268c0:	ldp	x20, x19, [sp, #80]
   268c4:	ldp	x22, x21, [sp, #64]
   268c8:	ldp	x24, x23, [sp, #48]
   268cc:	ldp	x26, x25, [sp, #32]
   268d0:	ldp	x28, x27, [sp, #16]
   268d4:	ldp	x29, x30, [sp], #96
   268d8:	ret
   268dc:	cmp	x19, #0x30
   268e0:	stur	x28, [x29, #-120]
   268e4:	b.le	26928 <__gmpn_mul@@Base+0x1d4>
   268e8:	add	x8, x19, x20
   268ec:	mov	w9, #0x1900                	// #6400
   268f0:	cmp	x8, x9
   268f4:	b.lt	26988 <__gmpn_mul@@Base+0x234>  // b.tstop
   268f8:	add	x25, x19, x19, lsl #1
   268fc:	cmp	x25, #0xc7f
   26900:	b.le	26988 <__gmpn_mul@@Base+0x234>
   26904:	cmp	x20, x19, lsl #3
   26908:	b.ge	26e8c <__gmpn_mul@@Base+0x738>  // b.tcont
   2690c:	mov	x0, x21
   26910:	mov	x1, x23
   26914:	mov	x2, x20
   26918:	mov	x3, x28
   2691c:	mov	x4, x19
   26920:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   26924:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26928:	add	x8, x19, x19, lsl #3
   2692c:	cmp	x8, #0x0
   26930:	cinc	x8, x8, lt  // lt = tstop
   26934:	lsl	x8, x8, #2
   26938:	and	x8, x8, #0xfffffffffffffff8
   2693c:	add	x8, x8, #0x40f
   26940:	and	x8, x8, #0xfffffffffffffff0
   26944:	mov	x9, sp
   26948:	sub	x5, x9, x8
   2694c:	mov	sp, x5
   26950:	add	x26, x19, x19, lsl #1
   26954:	cmp	x26, x20
   26958:	b.le	26aa4 <__gmpn_mul@@Base+0x350>
   2695c:	lsl	x8, x20, #2
   26960:	add	x9, x19, x19, lsl #2
   26964:	cmp	x8, x9
   26968:	b.ge	26bd4 <__gmpn_mul@@Base+0x480>  // b.tcont
   2696c:	mov	x0, x21
   26970:	mov	x1, x23
   26974:	mov	x2, x20
   26978:	mov	x3, x28
   2697c:	mov	x4, x19
   26980:	bl	d630 <__gmpn_toom22_mul@plt>
   26984:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26988:	cmp	x19, #0x52
   2698c:	b.lt	26a00 <__gmpn_mul@@Base+0x2ac>  // b.tstop
   26990:	add	x8, x20, x20, lsl #1
   26994:	add	x8, x8, #0xc
   26998:	cmp	x8, x19, lsl #2
   2699c:	b.ge	26a00 <__gmpn_mul@@Base+0x2ac>  // b.tcont
   269a0:	cmp	x19, #0xac
   269a4:	stur	xzr, [x29, #-112]
   269a8:	b.le	26f74 <__gmpn_mul@@Base+0x820>
   269ac:	mov	x0, x20
   269b0:	mov	x1, x19
   269b4:	cmp	x19, #0xeb
   269b8:	b.le	2710c <__gmpn_mul@@Base+0x9b8>
   269bc:	bl	27214 <__gmpn_mul@@Base+0xac0>
   269c0:	lsl	x1, x0, #3
   269c4:	mov	w8, #0x7f00                	// #32512
   269c8:	cmp	x1, x8
   269cc:	b.hi	271e0 <__gmpn_mul@@Base+0xa8c>  // b.pmore
   269d0:	add	x9, x1, #0xf
   269d4:	mov	x8, sp
   269d8:	and	x9, x9, #0xfffffffffffffff0
   269dc:	sub	x5, x8, x9
   269e0:	mov	sp, x5
   269e4:	mov	x0, x21
   269e8:	mov	x1, x23
   269ec:	mov	x2, x20
   269f0:	mov	x3, x28
   269f4:	mov	x4, x19
   269f8:	bl	cce0 <__gmpn_toom8h_mul@plt>
   269fc:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26a00:	lsl	x8, x19, #5
   26a04:	add	x1, x8, #0x200
   26a08:	mov	w8, #0x7f00                	// #32512
   26a0c:	cmp	x1, x8
   26a10:	stur	xzr, [x29, #-112]
   26a14:	b.hi	271c0 <__gmpn_mul@@Base+0xa6c>  // b.pmore
   26a18:	add	x9, x1, #0xf
   26a1c:	mov	x8, sp
   26a20:	and	x9, x9, #0xfffffffffffffff0
   26a24:	sub	x8, x8, x9
   26a28:	stur	x8, [x29, #-128]
   26a2c:	mov	sp, x8
   26a30:	lsl	x9, x20, #1
   26a34:	add	x8, x19, x19, lsl #2
   26a38:	cmp	x9, x8
   26a3c:	stur	x8, [x29, #-136]
   26a40:	b.ge	26b7c <__gmpn_mul@@Base+0x428>  // b.tcont
   26a44:	add	x8, x20, x20, lsl #1
   26a48:	lsl	x11, x19, #3
   26a4c:	lsl	x10, x8, #1
   26a50:	sub	x8, x11, x19
   26a54:	cmp	x10, x8
   26a58:	b.ge	26cdc <__gmpn_mul@@Base+0x588>  // b.tcont
   26a5c:	ldur	x5, [x29, #-128]
   26a60:	mov	x0, x21
   26a64:	mov	x1, x23
   26a68:	mov	x2, x20
   26a6c:	mov	x3, x28
   26a70:	mov	x4, x19
   26a74:	bl	c1e0 <__gmpn_toom33_mul@plt>
   26a78:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26a7c:	mov	x21, x24
   26a80:	mov	x20, x22
   26a84:	mov	x0, x21
   26a88:	cmp	x20, x19
   26a8c:	b.le	26b24 <__gmpn_mul@@Base+0x3d0>
   26a90:	mov	x1, x23
   26a94:	mov	x2, x20
   26a98:	mov	x3, x28
   26a9c:	mov	x4, x19
   26aa0:	b	26b34 <__gmpn_mul@@Base+0x3e0>
   26aa4:	mov	x8, sp
   26aa8:	sub	x22, x8, x19, lsl #5
   26aac:	mov	sp, x22
   26ab0:	lsl	x27, x19, #1
   26ab4:	mov	x0, x21
   26ab8:	mov	x1, x23
   26abc:	mov	x2, x27
   26ac0:	mov	x3, x28
   26ac4:	mov	x4, x19
   26ac8:	stur	x5, [x29, #-128]
   26acc:	bl	d660 <__gmpn_toom42_mul@plt>
   26ad0:	lsl	x8, x19, #4
   26ad4:	sub	x20, x20, x27
   26ad8:	add	x25, x21, x8
   26adc:	cmp	x20, x26
   26ae0:	add	x23, x23, x8
   26ae4:	stp	x8, x22, [x29, #-144]
   26ae8:	b.ge	26c00 <__gmpn_mul@@Base+0x4ac>  // b.tcont
   26aec:	ldur	x5, [x29, #-128]
   26af0:	lsl	x8, x20, #2
   26af4:	add	x9, x19, x19, lsl #2
   26af8:	cmp	x8, x9
   26afc:	lsl	x24, x19, #3
   26b00:	b.ge	26cb0 <__gmpn_mul@@Base+0x55c>  // b.tcont
   26b04:	ldur	x26, [x29, #-136]
   26b08:	ldur	x3, [x29, #-120]
   26b0c:	mov	x1, x23
   26b10:	mov	x2, x20
   26b14:	mov	x0, x26
   26b18:	mov	x4, x19
   26b1c:	bl	d630 <__gmpn_toom22_mul@plt>
   26b20:	b	26f18 <__gmpn_mul@@Base+0x7c4>
   26b24:	mov	x1, x28
   26b28:	mov	x2, x19
   26b2c:	mov	x3, x23
   26b30:	mov	x4, x20
   26b34:	bl	c6e0 <__gmpn_mul_basecase@plt>
   26b38:	sub	x2, x29, #0x70
   26b3c:	mov	x0, x21
   26b40:	mov	x1, x21
   26b44:	mov	x3, x19
   26b48:	bl	cc30 <__gmpn_add_n@plt>
   26b4c:	lsl	x8, x19, #3
   26b50:	ldr	x9, [x21, x8]
   26b54:	adds	x9, x9, x0
   26b58:	str	x9, [x21, x8]
   26b5c:	b.cc	268b0 <__gmpn_mul@@Base+0x15c>  // b.lo, b.ul, b.last
   26b60:	add	x8, x21, x19, lsl #3
   26b64:	add	x8, x8, #0x8
   26b68:	ldr	x9, [x8]
   26b6c:	adds	x9, x9, #0x1
   26b70:	str	x9, [x8], #8
   26b74:	b.cs	26b68 <__gmpn_mul@@Base+0x414>  // b.hs, b.nlast
   26b78:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26b7c:	mov	w8, #0x1c                  	// #28
   26b80:	mul	x8, x19, x8
   26b84:	and	x1, x8, #0xfffffffffffffff8
   26b88:	mov	w8, #0x7f00                	// #32512
   26b8c:	cmp	x1, x8
   26b90:	b.hi	271d0 <__gmpn_mul@@Base+0xa7c>  // b.pmore
   26b94:	add	x9, x1, #0xf
   26b98:	mov	x8, sp
   26b9c:	and	x9, x9, #0xfffffffffffffff0
   26ba0:	sub	x25, x8, x9
   26ba4:	mov	sp, x25
   26ba8:	lsl	x27, x19, #1
   26bac:	cmp	x19, #0x4f
   26bb0:	mov	x0, x21
   26bb4:	mov	x1, x23
   26bb8:	mov	x2, x27
   26bbc:	mov	x3, x28
   26bc0:	mov	x4, x19
   26bc4:	b.le	26d10 <__gmpn_mul@@Base+0x5bc>
   26bc8:	ldur	x5, [x29, #-128]
   26bcc:	bl	c8d0 <__gmpn_toom63_mul@plt>
   26bd0:	b	26d18 <__gmpn_mul@@Base+0x5c4>
   26bd4:	lsl	x9, x19, #3
   26bd8:	sub	x9, x9, x19
   26bdc:	mov	x0, x21
   26be0:	mov	x1, x23
   26be4:	mov	x2, x20
   26be8:	mov	x3, x28
   26bec:	mov	x4, x19
   26bf0:	cmp	x8, x9
   26bf4:	b.ge	26f0c <__gmpn_mul@@Base+0x7b8>  // b.tcont
   26bf8:	bl	ca00 <__gmpn_toom32_mul@plt>
   26bfc:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26c00:	add	x8, x22, x19, lsl #3
   26c04:	ldur	x5, [x29, #-128]
   26c08:	stur	x8, [x29, #-152]
   26c0c:	mov	w8, #0x18                  	// #24
   26c10:	madd	x8, x19, x8, x21
   26c14:	add	x28, x8, #0x8
   26c18:	lsl	x22, x27, #3
   26c1c:	b	26c40 <__gmpn_mul@@Base+0x4ec>
   26c20:	ldur	x8, [x29, #-144]
   26c24:	ldur	x5, [x29, #-128]
   26c28:	sub	x20, x20, x27
   26c2c:	add	x25, x25, x22
   26c30:	add	x23, x23, x22
   26c34:	cmp	x20, x26
   26c38:	add	x28, x28, x8
   26c3c:	b.lt	26af0 <__gmpn_mul@@Base+0x39c>  // b.tstop
   26c40:	ldur	x21, [x29, #-136]
   26c44:	ldur	x3, [x29, #-120]
   26c48:	mov	x1, x23
   26c4c:	mov	x2, x27
   26c50:	mov	x0, x21
   26c54:	mov	x4, x19
   26c58:	bl	d660 <__gmpn_toom42_mul@plt>
   26c5c:	mov	x0, x25
   26c60:	mov	x1, x25
   26c64:	mov	x2, x21
   26c68:	mov	x3, x19
   26c6c:	bl	cc30 <__gmpn_add_n@plt>
   26c70:	ldur	x1, [x29, #-152]
   26c74:	add	x24, x25, x19, lsl #3
   26c78:	mov	x21, x0
   26c7c:	mov	x0, x24
   26c80:	mov	x2, x27
   26c84:	bl	cc10 <__gmpn_copyi@plt>
   26c88:	ldr	x8, [x24]
   26c8c:	adds	x8, x8, x21
   26c90:	str	x8, [x24]
   26c94:	b.cc	26c20 <__gmpn_mul@@Base+0x4cc>  // b.lo, b.ul, b.last
   26c98:	mov	x8, x28
   26c9c:	ldr	x9, [x8]
   26ca0:	adds	x9, x9, #0x1
   26ca4:	str	x9, [x8], #8
   26ca8:	b.cs	26c9c <__gmpn_mul@@Base+0x548>  // b.hs, b.nlast
   26cac:	b	26c20 <__gmpn_mul@@Base+0x4cc>
   26cb0:	ldur	x26, [x29, #-136]
   26cb4:	ldur	x3, [x29, #-120]
   26cb8:	sub	x9, x24, x19
   26cbc:	cmp	x8, x9
   26cc0:	mov	x0, x26
   26cc4:	mov	x1, x23
   26cc8:	mov	x2, x20
   26ccc:	mov	x4, x19
   26cd0:	b.ge	26f14 <__gmpn_mul@@Base+0x7c0>  // b.tcont
   26cd4:	bl	ca00 <__gmpn_toom32_mul@plt>
   26cd8:	b	26f18 <__gmpn_mul@@Base+0x7c4>
   26cdc:	add	x11, x19, x19, lsl #1
   26ce0:	cmp	x9, x11
   26ce4:	b.ge	26fac <__gmpn_mul@@Base+0x858>  // b.tcont
   26ce8:	cmp	x19, #0x50
   26cec:	b.le	26fcc <__gmpn_mul@@Base+0x878>
   26cf0:	ldur	x5, [x29, #-128]
   26cf4:	mov	x0, x21
   26cf8:	mov	x1, x23
   26cfc:	mov	x2, x20
   26d00:	mov	x3, x28
   26d04:	mov	x4, x19
   26d08:	bl	d0d0 <__gmpn_toom43_mul@plt>
   26d0c:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26d10:	ldur	x5, [x29, #-128]
   26d14:	bl	d660 <__gmpn_toom42_mul@plt>
   26d18:	ldur	x8, [x29, #-136]
   26d1c:	lsl	x22, x27, #3
   26d20:	sub	x20, x20, x27
   26d24:	add	x26, x21, x22
   26d28:	cmp	x8, x20, lsl #1
   26d2c:	add	x23, x23, x22
   26d30:	b.le	26d54 <__gmpn_mul@@Base+0x600>
   26d34:	mov	x0, x25
   26d38:	cmp	x20, x19
   26d3c:	b.ge	26e18 <__gmpn_mul@@Base+0x6c4>  // b.tcont
   26d40:	ldur	x1, [x29, #-120]
   26d44:	mov	x2, x19
   26d48:	mov	x3, x23
   26d4c:	mov	x4, x20
   26d50:	b	26e28 <__gmpn_mul@@Base+0x6d4>
   26d54:	add	x8, x25, x19, lsl #3
   26d58:	stur	x8, [x29, #-144]
   26d5c:	mov	w8, #0x18                  	// #24
   26d60:	madd	x8, x19, x8, x21
   26d64:	add	x28, x8, #0x8
   26d68:	lsl	x8, x19, #4
   26d6c:	stur	x8, [x29, #-152]
   26d70:	b	26d94 <__gmpn_mul@@Base+0x640>
   26d74:	ldur	x8, [x29, #-136]
   26d78:	sub	x20, x20, x27
   26d7c:	add	x26, x26, x22
   26d80:	add	x23, x23, x22
   26d84:	cmp	x8, x20, lsl #1
   26d88:	ldur	x8, [x29, #-152]
   26d8c:	add	x28, x28, x8
   26d90:	b.gt	26d34 <__gmpn_mul@@Base+0x5e0>
   26d94:	mov	x0, x25
   26d98:	mov	x1, x23
   26d9c:	mov	x2, x27
   26da0:	cmp	x19, #0x4f
   26da4:	b.le	26db8 <__gmpn_mul@@Base+0x664>
   26da8:	ldp	x5, x3, [x29, #-128]
   26dac:	mov	x4, x19
   26db0:	bl	c8d0 <__gmpn_toom63_mul@plt>
   26db4:	b	26dc4 <__gmpn_mul@@Base+0x670>
   26db8:	ldp	x5, x3, [x29, #-128]
   26dbc:	mov	x4, x19
   26dc0:	bl	d660 <__gmpn_toom42_mul@plt>
   26dc4:	mov	x0, x26
   26dc8:	mov	x1, x26
   26dcc:	mov	x2, x25
   26dd0:	mov	x3, x19
   26dd4:	bl	cc30 <__gmpn_add_n@plt>
   26dd8:	ldur	x1, [x29, #-144]
   26ddc:	add	x24, x26, x19, lsl #3
   26de0:	mov	x21, x0
   26de4:	mov	x0, x24
   26de8:	mov	x2, x27
   26dec:	bl	cc10 <__gmpn_copyi@plt>
   26df0:	ldr	x8, [x24]
   26df4:	adds	x8, x8, x21
   26df8:	str	x8, [x24]
   26dfc:	b.cc	26d74 <__gmpn_mul@@Base+0x620>  // b.lo, b.ul, b.last
   26e00:	mov	x8, x28
   26e04:	ldr	x9, [x8]
   26e08:	adds	x9, x9, #0x1
   26e0c:	str	x9, [x8], #8
   26e10:	b.cs	26e04 <__gmpn_mul@@Base+0x6b0>  // b.hs, b.nlast
   26e14:	b	26d74 <__gmpn_mul@@Base+0x620>
   26e18:	ldur	x3, [x29, #-120]
   26e1c:	mov	x1, x23
   26e20:	mov	x2, x20
   26e24:	mov	x4, x19
   26e28:	bl	cea0 <__gmpn_mul@plt>
   26e2c:	mov	x0, x26
   26e30:	mov	x1, x26
   26e34:	mov	x2, x25
   26e38:	mov	x3, x19
   26e3c:	bl	cc30 <__gmpn_add_n@plt>
   26e40:	lsl	x23, x19, #3
   26e44:	add	x22, x26, x23
   26e48:	mov	x21, x0
   26e4c:	add	x1, x25, x23
   26e50:	mov	x0, x22
   26e54:	mov	x2, x20
   26e58:	bl	cc10 <__gmpn_copyi@plt>
   26e5c:	ldr	x8, [x22]
   26e60:	adds	x8, x8, x21
   26e64:	str	x8, [x22]
   26e68:	b.cc	26e84 <__gmpn_mul@@Base+0x730>  // b.lo, b.ul, b.last
   26e6c:	add	x8, x23, #0x8
   26e70:	ldr	x9, [x26, x8]
   26e74:	adds	x9, x9, #0x1
   26e78:	str	x9, [x26, x8]
   26e7c:	add	x8, x8, #0x8
   26e80:	b.cs	26e70 <__gmpn_mul@@Base+0x71c>  // b.hs, b.nlast
   26e84:	mov	x21, x26
   26e88:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26e8c:	lsl	x26, x19, #3
   26e90:	add	x8, x26, x19
   26e94:	lsl	x8, x8, #2
   26e98:	and	x1, x8, #0xfffffffffffffff8
   26e9c:	sub	x0, x29, #0x70
   26ea0:	stur	xzr, [x29, #-112]
   26ea4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   26ea8:	mov	x24, x0
   26eac:	mov	x0, x21
   26eb0:	mov	x1, x23
   26eb4:	mov	x2, x25
   26eb8:	mov	x3, x28
   26ebc:	mov	x4, x19
   26ec0:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   26ec4:	lsl	x22, x25, #3
   26ec8:	sub	x20, x20, x25
   26ecc:	sub	x9, x26, x19
   26ed0:	add	x8, x21, x22
   26ed4:	cmp	x9, x20, lsl #1
   26ed8:	add	x23, x23, x22
   26edc:	stur	x26, [x29, #-152]
   26ee0:	stur	x9, [x29, #-128]
   26ee4:	b.le	26fec <__gmpn_mul@@Base+0x898>
   26ee8:	mov	x21, x8
   26eec:	mov	x0, x24
   26ef0:	cmp	x20, x19
   26ef4:	b.ge	2709c <__gmpn_mul@@Base+0x948>  // b.tcont
   26ef8:	mov	x1, x28
   26efc:	mov	x2, x19
   26f00:	mov	x3, x23
   26f04:	mov	x4, x20
   26f08:	b	270ac <__gmpn_mul@@Base+0x958>
   26f0c:	bl	d660 <__gmpn_toom42_mul@plt>
   26f10:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26f14:	bl	d660 <__gmpn_toom42_mul@plt>
   26f18:	mov	x0, x25
   26f1c:	mov	x1, x25
   26f20:	mov	x2, x26
   26f24:	mov	x3, x19
   26f28:	bl	cc30 <__gmpn_add_n@plt>
   26f2c:	add	x22, x25, x24
   26f30:	mov	x21, x0
   26f34:	add	x1, x26, x24
   26f38:	mov	x0, x22
   26f3c:	mov	x2, x20
   26f40:	bl	cc10 <__gmpn_copyi@plt>
   26f44:	ldr	x8, [x22]
   26f48:	adds	x8, x8, x21
   26f4c:	str	x8, [x22]
   26f50:	b.cc	26f6c <__gmpn_mul@@Base+0x818>  // b.lo, b.ul, b.last
   26f54:	add	x8, x24, #0x8
   26f58:	ldr	x9, [x25, x8]
   26f5c:	adds	x9, x9, #0x1
   26f60:	str	x9, [x25, x8]
   26f64:	add	x8, x8, #0x8
   26f68:	b.cs	26f58 <__gmpn_mul@@Base+0x804>  // b.hs, b.nlast
   26f6c:	mov	x21, x25
   26f70:	b	268b0 <__gmpn_mul@@Base+0x15c>
   26f74:	mov	w8, #0x18                  	// #24
   26f78:	mul	x8, x20, x8
   26f7c:	add	x8, x8, #0x20f
   26f80:	and	x8, x8, #0xfffffffffffffff0
   26f84:	mov	x9, sp
   26f88:	sub	x5, x9, x8
   26f8c:	mov	sp, x5
   26f90:	mov	x0, x21
   26f94:	mov	x1, x23
   26f98:	mov	x2, x20
   26f9c:	mov	x3, x28
   26fa0:	mov	x4, x19
   26fa4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   26fa8:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26fac:	mov	w9, #0xb                   	// #11
   26fb0:	mul	x9, x19, x9
   26fb4:	cmp	x10, x9
   26fb8:	b.ge	27144 <__gmpn_mul@@Base+0x9f0>  // b.tcont
   26fbc:	cmp	x8, x20, lsl #2
   26fc0:	b.le	2716c <__gmpn_mul@@Base+0xa18>
   26fc4:	cmp	x19, #0x4b
   26fc8:	b.gt	27174 <__gmpn_mul@@Base+0xa20>
   26fcc:	ldur	x5, [x29, #-128]
   26fd0:	mov	x0, x21
   26fd4:	mov	x1, x23
   26fd8:	mov	x2, x20
   26fdc:	mov	x3, x28
   26fe0:	mov	x4, x19
   26fe4:	bl	ca00 <__gmpn_toom32_mul@plt>
   26fe8:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   26fec:	add	x9, x24, x19, lsl #3
   26ff0:	stur	x9, [x29, #-136]
   26ff4:	add	x9, x21, x19, lsl #5
   26ff8:	add	x10, x19, x19, lsl #1
   26ffc:	add	x26, x9, #0x8
   27000:	lsl	x9, x10, #3
   27004:	mov	x21, x8
   27008:	stur	x9, [x29, #-144]
   2700c:	b	27030 <__gmpn_mul@@Base+0x8dc>
   27010:	ldp	x8, x28, [x29, #-128]
   27014:	sub	x20, x20, x25
   27018:	add	x21, x21, x22
   2701c:	add	x23, x23, x22
   27020:	cmp	x8, x20, lsl #1
   27024:	ldur	x8, [x29, #-144]
   27028:	add	x26, x26, x8
   2702c:	b.gt	26eec <__gmpn_mul@@Base+0x798>
   27030:	mov	x0, x24
   27034:	mov	x1, x23
   27038:	mov	x2, x25
   2703c:	mov	x3, x28
   27040:	mov	x4, x19
   27044:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   27048:	mov	x0, x21
   2704c:	mov	x1, x21
   27050:	mov	x2, x24
   27054:	mov	x3, x19
   27058:	bl	cc30 <__gmpn_add_n@plt>
   2705c:	ldur	x1, [x29, #-136]
   27060:	add	x28, x21, x19, lsl #3
   27064:	mov	x27, x0
   27068:	mov	x0, x28
   2706c:	mov	x2, x25
   27070:	bl	cc10 <__gmpn_copyi@plt>
   27074:	ldr	x8, [x28]
   27078:	adds	x8, x8, x27
   2707c:	str	x8, [x28]
   27080:	b.cc	27010 <__gmpn_mul@@Base+0x8bc>  // b.lo, b.ul, b.last
   27084:	mov	x8, x26
   27088:	ldr	x9, [x8]
   2708c:	adds	x9, x9, #0x1
   27090:	str	x9, [x8], #8
   27094:	b.cs	27088 <__gmpn_mul@@Base+0x934>  // b.hs, b.nlast
   27098:	b	27010 <__gmpn_mul@@Base+0x8bc>
   2709c:	mov	x1, x23
   270a0:	mov	x2, x20
   270a4:	mov	x3, x28
   270a8:	mov	x4, x19
   270ac:	bl	cea0 <__gmpn_mul@plt>
   270b0:	mov	x0, x21
   270b4:	mov	x1, x21
   270b8:	mov	x2, x24
   270bc:	mov	x3, x19
   270c0:	bl	cc30 <__gmpn_add_n@plt>
   270c4:	ldur	x8, [x29, #-152]
   270c8:	mov	x22, x0
   270cc:	mov	x2, x20
   270d0:	add	x23, x21, x8
   270d4:	add	x1, x24, x8
   270d8:	mov	x0, x23
   270dc:	bl	cc10 <__gmpn_copyi@plt>
   270e0:	ldr	x8, [x23]
   270e4:	adds	x8, x8, x22
   270e8:	str	x8, [x23]
   270ec:	b.cc	271b0 <__gmpn_mul@@Base+0xa5c>  // b.lo, b.ul, b.last
   270f0:	add	x8, x21, x19, lsl #3
   270f4:	add	x8, x8, #0x8
   270f8:	ldr	x9, [x8]
   270fc:	adds	x9, x9, #0x1
   27100:	str	x9, [x8], #8
   27104:	b.cs	270f8 <__gmpn_mul@@Base+0x9a4>  // b.hs, b.nlast
   27108:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   2710c:	bl	271f0 <__gmpn_mul@@Base+0xa9c>
   27110:	lsl	x8, x0, #3
   27114:	add	x8, x8, #0xf
   27118:	and	x8, x8, #0xfffffffffffffff0
   2711c:	mov	x9, sp
   27120:	sub	x5, x9, x8
   27124:	mov	sp, x5
   27128:	mov	x0, x21
   2712c:	mov	x1, x23
   27130:	mov	x2, x20
   27134:	mov	x3, x28
   27138:	mov	x4, x19
   2713c:	bl	cde0 <__gmpn_toom6h_mul@plt>
   27140:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   27144:	cmp	x19, #0x4f
   27148:	b.le	27194 <__gmpn_mul@@Base+0xa40>
   2714c:	ldur	x5, [x29, #-128]
   27150:	mov	x0, x21
   27154:	mov	x1, x23
   27158:	mov	x2, x20
   2715c:	mov	x3, x28
   27160:	mov	x4, x19
   27164:	bl	c8d0 <__gmpn_toom63_mul@plt>
   27168:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   2716c:	cmp	x19, #0x50
   27170:	b.le	27194 <__gmpn_mul@@Base+0xa40>
   27174:	ldur	x5, [x29, #-128]
   27178:	mov	x0, x21
   2717c:	mov	x1, x23
   27180:	mov	x2, x20
   27184:	mov	x3, x28
   27188:	mov	x4, x19
   2718c:	bl	cc00 <__gmpn_toom53_mul@plt>
   27190:	b	271b0 <__gmpn_mul@@Base+0xa5c>
   27194:	ldur	x5, [x29, #-128]
   27198:	mov	x0, x21
   2719c:	mov	x1, x23
   271a0:	mov	x2, x20
   271a4:	mov	x3, x28
   271a8:	mov	x4, x19
   271ac:	bl	d660 <__gmpn_toom42_mul@plt>
   271b0:	ldur	x0, [x29, #-112]
   271b4:	cbz	x0, 268b0 <__gmpn_mul@@Base+0x15c>
   271b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   271bc:	b	268b0 <__gmpn_mul@@Base+0x15c>
   271c0:	sub	x0, x29, #0x70
   271c4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   271c8:	stur	x0, [x29, #-128]
   271cc:	b	26a30 <__gmpn_mul@@Base+0x2dc>
   271d0:	sub	x0, x29, #0x70
   271d4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   271d8:	mov	x25, x0
   271dc:	b	26ba8 <__gmpn_mul@@Base+0x454>
   271e0:	sub	x0, x29, #0x70
   271e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   271e8:	mov	x5, x0
   271ec:	b	269e4 <__gmpn_mul@@Base+0x290>
   271f0:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   271f4:	add	x8, x1, x0
   271f8:	movk	x9, #0xcccd
   271fc:	umulh	x8, x8, x9
   27200:	lsr	x8, x8, #3
   27204:	mov	w9, #0xc                   	// #12
   27208:	mul	x8, x8, x9
   2720c:	add	x0, x8, #0x18c
   27210:	ret
   27214:	mov	x9, #0x4925                	// #18725
   27218:	movk	x9, #0x2492, lsl #16
   2721c:	add	x8, x1, x0
   27220:	movk	x9, #0x9249, lsl #32
   27224:	movk	x9, #0x4924, lsl #48
   27228:	lsr	x8, x8, #1
   2722c:	umulh	x8, x8, x9
   27230:	mov	w10, #0x78                  	// #120
   27234:	lsr	x8, x8, #1
   27238:	orr	x9, xzr, #0x78
   2723c:	madd	x8, x8, x10, x9
   27240:	asr	x8, x8, #3
   27244:	add	x0, x8, #0x19e
   27248:	ret

000000000002724c <__gmpn_fft_best_k@@Base>:
   2724c:	adrp	x8, 4f000 <__gmpn_bases@@Base+0x1f98>
   27250:	add	x8, x8, #0x900
   27254:	mov	w9, #0x1d8                 	// #472
   27258:	smaddl	x9, w1, w9, x8
   2725c:	ldr	w10, [x9], #4
   27260:	lsr	w8, w10, #27
   27264:	ldr	w10, [x9], #4
   27268:	and	x11, x10, #0x7ffffff
   2726c:	lsl	x11, x11, x8
   27270:	cmp	x11, x0
   27274:	b.lt	27260 <__gmpn_fft_best_k@@Base+0x14>  // b.tstop
   27278:	mov	w0, w8
   2727c:	ret

0000000000027280 <__gmpn_fft_next_size@@Base>:
   27280:	sub	x8, x0, #0x1
   27284:	asr	x8, x8, x1
   27288:	add	x8, x8, #0x1
   2728c:	lsl	x0, x8, x1
   27290:	ret

0000000000027294 <__gmpn_mul_fft@@Base>:
   27294:	sub	sp, sp, #0xd0
   27298:	cmp	x2, x4
   2729c:	stp	x26, x25, [sp, #144]
   272a0:	stp	x22, x21, [sp, #176]
   272a4:	stp	x20, x19, [sp, #192]
   272a8:	mov	x26, x1
   272ac:	mov	x22, x0
   272b0:	cset	w20, eq  // eq = none
   272b4:	cmp	x3, x5
   272b8:	mov	x0, x1
   272bc:	mov	w1, w6
   272c0:	stp	x29, x30, [sp, #112]
   272c4:	stp	x28, x27, [sp, #128]
   272c8:	stp	x24, x23, [sp, #160]
   272cc:	add	x29, sp, #0x70
   272d0:	mov	w19, w6
   272d4:	mov	x23, x5
   272d8:	mov	x24, x4
   272dc:	mov	x25, x3
   272e0:	mov	x27, x2
   272e4:	cset	w21, eq  // eq = none
   272e8:	bl	d3b0 <__gmpn_fft_next_size@plt>
   272ec:	cmp	x0, x26
   272f0:	b.ne	27594 <__gmpn_mul_fft@@Base+0x300>  // b.any
   272f4:	stp	x24, x23, [sp, #40]
   272f8:	add	w24, w19, #0x1
   272fc:	sbfiz	x1, x24, #3, #32
   27300:	sub	x0, x29, #0x8
   27304:	stur	x25, [x29, #-32]
   27308:	stur	x22, [x29, #-24]
   2730c:	and	w22, w20, w21
   27310:	stur	xzr, [x29, #-8]
   27314:	lsl	x21, x26, #6
   27318:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2731c:	mov	w8, #0x8                   	// #8
   27320:	mov	x23, x0
   27324:	lsl	x1, x8, x19
   27328:	sub	x0, x29, #0x8
   2732c:	mov	w20, w19
   27330:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   27334:	tbnz	w19, #31, 27358 <__gmpn_mul_fft@@Base+0xc4>
   27338:	mov	x8, xzr
   2733c:	mov	w9, #0x1                   	// #1
   27340:	str	x0, [x23, x8, lsl #3]
   27344:	lsl	x10, x9, x8
   27348:	add	x8, x8, #0x1
   2734c:	cmp	x24, x8
   27350:	add	x0, x0, x10, lsl #2
   27354:	b.ne	27340 <__gmpn_mul_fft@@Base+0xac>  // b.any
   27358:	mov	x0, x23
   2735c:	mov	w1, w19
   27360:	bl	275c4 <__gmpn_mul_fft@@Base+0x330>
   27364:	asr	x24, x21, x20
   27368:	sub	x8, x24, #0x1
   2736c:	add	x9, x24, #0x3e
   27370:	cmp	x8, #0x0
   27374:	mov	w0, w19
   27378:	csel	x21, x9, x8, lt  // lt = tstop
   2737c:	bl	27638 <__gmpn_mul_fft@@Base+0x3a4>
   27380:	lsl	x8, x24, #1
   27384:	add	x8, x8, w19, sxtw
   27388:	add	x8, x8, #0x2
   2738c:	sdiv	x8, x8, x0
   27390:	add	x8, x8, #0x1
   27394:	mul	x25, x8, x0
   27398:	add	x8, x25, #0x3f
   2739c:	cmp	x25, #0x0
   273a0:	mov	w9, #0x13c                 	// #316
   273a4:	mov	w10, #0x110                 	// #272
   273a8:	csel	x8, x8, x25, lt  // lt = tstop
   273ac:	cmp	w22, #0x0
   273b0:	asr	x28, x8, #6
   273b4:	csel	x8, x10, x9, ne  // ne = any
   273b8:	cmp	x28, x8
   273bc:	b.lt	27414 <__gmpn_mul_fft@@Base+0x180>  // b.tstop
   273c0:	mov	x0, x28
   273c4:	mov	w1, w22
   273c8:	bl	cc90 <__gmpn_fft_best_k@plt>
   273cc:	mov	w8, #0x1                   	// #1
   273d0:	lsl	x8, x8, x0
   273d4:	sub	x9, x8, #0x1
   273d8:	tst	x9, x28
   273dc:	b.eq	27414 <__gmpn_mul_fft@@Base+0x180>  // b.none
   273e0:	mov	w24, #0x1                   	// #1
   273e4:	add	x9, x8, x28
   273e8:	sub	x9, x9, #0x1
   273ec:	neg	x8, x8
   273f0:	and	x28, x9, x8
   273f4:	mov	x0, x28
   273f8:	mov	w1, w22
   273fc:	bl	cc90 <__gmpn_fft_best_k@plt>
   27400:	lsl	x8, x24, x0
   27404:	sub	x9, x8, #0x1
   27408:	tst	x9, x28
   2740c:	b.ne	273e4 <__gmpn_mul_fft@@Base+0x150>  // b.any
   27410:	lsl	x25, x28, #6
   27414:	cmp	x28, x26
   27418:	str	x19, [sp, #56]
   2741c:	stur	x27, [x29, #-40]
   27420:	stur	x26, [x29, #-16]
   27424:	b.ge	275ac <__gmpn_mul_fft@@Base+0x318>  // b.tcont
   27428:	asr	x9, x21, #6
   2742c:	add	x21, x28, #0x1
   27430:	mov	w8, #0x1                   	// #1
   27434:	lsl	x1, x21, #4
   27438:	sub	x0, x29, #0x8
   2743c:	lsl	x27, x8, x20
   27440:	add	x24, x9, #0x1
   27444:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   27448:	lsl	x8, x21, x20
   2744c:	lsl	x26, x8, #3
   27450:	mov	x19, x0
   27454:	sub	x0, x29, #0x8
   27458:	mov	x1, x26
   2745c:	asr	x25, x25, x20
   27460:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   27464:	lsl	x20, x27, #3
   27468:	mov	x21, x0
   2746c:	sub	x0, x29, #0x8
   27470:	mov	x1, x20
   27474:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   27478:	ldp	x4, x5, [x29, #-40]
   2747c:	mov	x1, x0
   27480:	mov	x0, x21
   27484:	mov	x21, x24
   27488:	mov	x2, x27
   2748c:	mov	x3, x28
   27490:	mov	x6, x21
   27494:	mov	x7, x25
   27498:	stur	x19, [x29, #-48]
   2749c:	str	x19, [sp]
   274a0:	mov	x24, x1
   274a4:	bl	27668 <__gmpn_mul_fft@@Base+0x3d4>
   274a8:	cbz	w22, 274e0 <__gmpn_mul_fft@@Base+0x24c>
   274ac:	sub	x8, x27, #0x1
   274b0:	madd	x8, x21, x8, x28
   274b4:	lsl	x8, x8, #3
   274b8:	add	x1, x8, #0x8
   274bc:	sub	x0, x29, #0x8
   274c0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   274c4:	mov	x26, x0
   274c8:	sub	x0, x29, #0x8
   274cc:	mov	x1, x20
   274d0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   274d4:	ldur	x8, [x29, #-48]
   274d8:	mov	x20, x0
   274dc:	b	2752c <__gmpn_mul_fft@@Base+0x298>
   274e0:	sub	x0, x29, #0x8
   274e4:	mov	x1, x26
   274e8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   274ec:	mov	x26, x0
   274f0:	sub	x0, x29, #0x8
   274f4:	mov	x1, x20
   274f8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   274fc:	ldur	x19, [x29, #-48]
   27500:	ldp	x4, x5, [sp, #40]
   27504:	mov	x20, x0
   27508:	mov	x0, x26
   2750c:	mov	x1, x20
   27510:	mov	x2, x27
   27514:	mov	x3, x28
   27518:	mov	x6, x21
   2751c:	mov	x7, x25
   27520:	str	x19, [sp]
   27524:	bl	27668 <__gmpn_mul_fft@@Base+0x3d4>
   27528:	mov	x8, x19
   2752c:	ldp	x0, x1, [x29, #-24]
   27530:	ldr	x2, [sp, #56]
   27534:	mov	x3, x24
   27538:	mov	x4, x20
   2753c:	mov	x5, x26
   27540:	mov	x6, x28
   27544:	mov	x7, x21
   27548:	str	w22, [sp, #24]
   2754c:	stp	x23, x8, [sp, #8]
   27550:	str	x25, [sp]
   27554:	bl	278e0 <__gmpn_mul_fft@@Base+0x64c>
   27558:	ldur	x8, [x29, #-8]
   2755c:	mov	x19, x0
   27560:	cbnz	x8, 27588 <__gmpn_mul_fft@@Base+0x2f4>
   27564:	mov	x0, x19
   27568:	ldp	x20, x19, [sp, #192]
   2756c:	ldp	x22, x21, [sp, #176]
   27570:	ldp	x24, x23, [sp, #160]
   27574:	ldp	x26, x25, [sp, #144]
   27578:	ldp	x28, x27, [sp, #128]
   2757c:	ldp	x29, x30, [sp, #112]
   27580:	add	sp, sp, #0xd0
   27584:	ret
   27588:	mov	x0, x8
   2758c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   27590:	b	27564 <__gmpn_mul_fft@@Base+0x2d0>
   27594:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   27598:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   2759c:	add	x0, x0, #0x89e
   275a0:	add	x2, x2, #0x8a8
   275a4:	mov	w1, #0x365                 	// #869
   275a8:	bl	c850 <__gmp_assert_fail@plt>
   275ac:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   275b0:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   275b4:	add	x0, x0, #0x89e
   275b8:	add	x2, x2, #0x8cb
   275bc:	mov	w1, #0x38b                 	// #907
   275c0:	bl	c850 <__gmp_assert_fail@plt>
   275c4:	ldr	x8, [x0]
   275c8:	cmp	w1, #0x1
   275cc:	str	wzr, [x8]
   275d0:	b.lt	27634 <__gmpn_mul_fft@@Base+0x3a0>  // b.tstop
   275d4:	add	w8, w1, #0x1
   275d8:	mov	w9, #0x1                   	// #1
   275dc:	mov	w10, #0x1                   	// #1
   275e0:	b	275f4 <__gmpn_mul_fft@@Base+0x360>
   275e4:	add	x9, x9, #0x1
   275e8:	cmp	x9, x8
   275ec:	lsl	w10, w10, #1
   275f0:	b.eq	27634 <__gmpn_mul_fft@@Base+0x3a0>  // b.none
   275f4:	cbz	w10, 275e4 <__gmpn_mul_fft@@Base+0x350>
   275f8:	add	x12, x0, x9, lsl #3
   275fc:	ldr	x11, [x0, x9, lsl #3]
   27600:	ldur	x12, [x12, #-8]
   27604:	sxtw	x13, w10
   27608:	mov	w14, w10
   2760c:	ldr	w15, [x12], #4
   27610:	mov	w16, #0x1                   	// #1
   27614:	subs	x14, x14, #0x1
   27618:	lsl	w17, w15, #1
   2761c:	bfi	w16, w15, #1, #31
   27620:	str	w17, [x11]
   27624:	str	w16, [x11, x13, lsl #2]
   27628:	add	x11, x11, #0x4
   2762c:	b.ne	2760c <__gmpn_mul_fft@@Base+0x378>  // b.any
   27630:	b	275e4 <__gmpn_mul_fft@@Base+0x350>
   27634:	ret
   27638:	mov	w8, #0x40                  	// #64
   2763c:	cmp	w0, #0x1
   27640:	b.lt	27660 <__gmpn_mul_fft@@Base+0x3cc>  // b.tstop
   27644:	mov	w9, w0
   27648:	mov	x10, x8
   2764c:	cmp	w9, #0x2
   27650:	lsr	x8, x8, #1
   27654:	b.lt	27660 <__gmpn_mul_fft@@Base+0x3cc>  // b.tstop
   27658:	sub	w9, w9, #0x1
   2765c:	tbz	w10, #1, 27648 <__gmpn_mul_fft@@Base+0x3b4>
   27660:	lsl	x0, x8, x0
   27664:	ret
   27668:	sub	sp, sp, #0x90
   2766c:	stp	x26, x25, [sp, #80]
   27670:	mul	x26, x6, x2
   27674:	stp	x29, x30, [sp, #48]
   27678:	stp	x28, x27, [sp, #64]
   2767c:	stp	x22, x21, [sp, #112]
   27680:	stp	x20, x19, [sp, #128]
   27684:	add	x29, sp, #0x30
   27688:	mov	x21, x6
   2768c:	mov	x19, x5
   27690:	mov	x28, x4
   27694:	mov	x25, x0
   27698:	cmp	x26, x5
   2769c:	stp	x24, x23, [sp, #96]
   276a0:	stp	x7, x3, [sp, #16]
   276a4:	str	x1, [sp, #8]
   276a8:	stp	x2, xzr, [x29, #-16]
   276ac:	b.ge	277c0 <__gmpn_mul_fft@@Base+0x52c>  // b.tcont
   276b0:	sub	x20, x19, x26
   276b4:	add	x19, x26, #0x1
   276b8:	lsl	x1, x19, #3
   276bc:	sub	x0, x29, #0x8
   276c0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   276c4:	mov	x24, x0
   276c8:	subs	x27, x20, x26
   276cc:	add	x3, x28, x26, lsl #3
   276d0:	b.le	27748 <__gmpn_mul_fft@@Base+0x4b4>
   276d4:	mov	x1, x28
   276d8:	mov	x2, x3
   276dc:	mov	x3, x26
   276e0:	bl	c420 <__gmpn_sub_n@plt>
   276e4:	mov	x20, x0
   276e8:	cmp	x27, x26
   276ec:	add	x28, x28, x26, lsl #4
   276f0:	mov	w8, wzr
   276f4:	b.le	27760 <__gmpn_mul_fft@@Base+0x4cc>
   276f8:	mov	w22, wzr
   276fc:	lsl	x23, x26, #3
   27700:	b	27728 <__gmpn_mul_fft@@Base+0x494>
   27704:	bl	cc30 <__gmpn_add_n@plt>
   27708:	sub	x20, x20, x0
   2770c:	eor	w22, w22, #0x1
   27710:	sub	x27, x27, x26
   27714:	cmp	w22, #0x0
   27718:	cset	w8, ne  // ne = any
   2771c:	cmp	x27, x26
   27720:	add	x28, x28, x23
   27724:	b.le	27760 <__gmpn_mul_fft@@Base+0x4cc>
   27728:	mov	x0, x24
   2772c:	mov	x1, x24
   27730:	mov	x2, x28
   27734:	mov	x3, x26
   27738:	tbz	w8, #0, 27704 <__gmpn_mul_fft@@Base+0x470>
   2773c:	bl	c420 <__gmpn_sub_n@plt>
   27740:	add	x20, x0, x20
   27744:	b	2770c <__gmpn_mul_fft@@Base+0x478>
   27748:	mov	x1, x28
   2774c:	mov	x2, x26
   27750:	mov	x4, x20
   27754:	bl	d340 <__gmpn_sub@plt>
   27758:	mov	x3, x0
   2775c:	b	277a8 <__gmpn_mul_fft@@Base+0x514>
   27760:	mov	x0, x24
   27764:	mov	x1, x24
   27768:	mov	x2, x26
   2776c:	mov	x3, x28
   27770:	mov	x4, x27
   27774:	cbz	w8, 2779c <__gmpn_mul_fft@@Base+0x508>
   27778:	bl	d340 <__gmpn_sub@plt>
   2777c:	add	x3, x0, x20
   27780:	tbz	x3, #63, 277a8 <__gmpn_mul_fft@@Base+0x514>
   27784:	neg	x3, x3
   27788:	mov	x0, x24
   2778c:	mov	x1, x24
   27790:	mov	x2, x26
   27794:	bl	caf0 <__gmpn_sub_1@plt>
   27798:	b	277b8 <__gmpn_mul_fft@@Base+0x524>
   2779c:	bl	c970 <__gmpn_add@plt>
   277a0:	sub	x3, x20, x0
   277a4:	tbnz	x3, #63, 27784 <__gmpn_mul_fft@@Base+0x4f0>
   277a8:	mov	x0, x24
   277ac:	mov	x1, x24
   277b0:	mov	x2, x26
   277b4:	bl	c150 <__gmpn_add_1@plt>
   277b8:	mov	x28, x24
   277bc:	str	x0, [x24, x26, lsl #3]
   277c0:	ldur	x8, [x29, #-16]
   277c4:	subs	x8, x8, #0x1
   277c8:	str	x8, [sp]
   277cc:	b.lt	27894 <__gmpn_mul_fft@@Base+0x600>  // b.tstop
   277d0:	ldr	x8, [sp, #24]
   277d4:	ldr	x26, [x29, #96]
   277d8:	mov	x20, xzr
   277dc:	mov	x23, xzr
   277e0:	add	x22, x8, #0x1
   277e4:	lsl	x8, x8, #3
   277e8:	add	x27, x8, #0x8
   277ec:	b	27824 <__gmpn_mul_fft@@Base+0x590>
   277f0:	ldr	x3, [sp, #24]
   277f4:	mov	x0, x25
   277f8:	mov	x1, x26
   277fc:	mov	x2, x20
   27800:	add	x28, x28, x21, lsl #3
   27804:	bl	27c40 <__gmpn_mul_fft@@Base+0x9ac>
   27808:	ldur	x8, [x29, #-16]
   2780c:	add	x23, x23, #0x1
   27810:	add	x25, x25, x27
   27814:	cmp	x8, x23
   27818:	ldr	x8, [sp, #16]
   2781c:	add	x20, x20, x8
   27820:	b.eq	27894 <__gmpn_mul_fft@@Base+0x600>  // b.none
   27824:	ldr	x8, [sp, #8]
   27828:	cmp	x19, #0x1
   2782c:	str	x25, [x8, x23, lsl #3]
   27830:	b.lt	27874 <__gmpn_mul_fft@@Base+0x5e0>  // b.tstop
   27834:	ldr	x8, [sp]
   27838:	cmp	x19, x21
   2783c:	mov	x0, x26
   27840:	mov	x1, x28
   27844:	ccmp	x23, x8, #0x0, ge  // ge = tcont
   27848:	csel	x24, x21, x19, lt  // lt = tstop
   2784c:	mov	x2, x24
   27850:	sub	x19, x19, x24
   27854:	bl	cc10 <__gmpn_copyi@plt>
   27858:	subs	x8, x22, x24
   2785c:	b.eq	277f0 <__gmpn_mul_fft@@Base+0x55c>  // b.none
   27860:	add	x0, x26, x24, lsl #3
   27864:	lsl	x2, x8, #3
   27868:	mov	w1, wzr
   2786c:	bl	c780 <memset@plt>
   27870:	b	277f0 <__gmpn_mul_fft@@Base+0x55c>
   27874:	ldr	x8, [sp, #24]
   27878:	cmn	x8, #0x1
   2787c:	b.eq	27808 <__gmpn_mul_fft@@Base+0x574>  // b.none
   27880:	mov	x0, x25
   27884:	mov	w1, wzr
   27888:	mov	x2, x27
   2788c:	bl	c780 <memset@plt>
   27890:	b	27808 <__gmpn_mul_fft@@Base+0x574>
   27894:	cbnz	x19, 278c8 <__gmpn_mul_fft@@Base+0x634>
   27898:	ldur	x0, [x29, #-8]
   2789c:	cbnz	x0, 278c0 <__gmpn_mul_fft@@Base+0x62c>
   278a0:	ldp	x20, x19, [sp, #128]
   278a4:	ldp	x22, x21, [sp, #112]
   278a8:	ldp	x24, x23, [sp, #96]
   278ac:	ldp	x26, x25, [sp, #80]
   278b0:	ldp	x28, x27, [sp, #64]
   278b4:	ldp	x29, x30, [sp, #48]
   278b8:	add	sp, sp, #0x90
   278bc:	ret
   278c0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   278c4:	b	278a0 <__gmpn_mul_fft@@Base+0x60c>
   278c8:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   278cc:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   278d0:	add	x0, x0, #0x89e
   278d4:	add	x2, x2, #0x8d7
   278d8:	mov	w1, #0x2e7                 	// #743
   278dc:	bl	c850 <__gmp_assert_fail@plt>
   278e0:	sub	sp, sp, #0xb0
   278e4:	stp	x29, x30, [sp, #80]
   278e8:	add	x29, sp, #0x50
   278ec:	stp	x26, x25, [sp, #112]
   278f0:	stp	x20, x19, [sp, #160]
   278f4:	ldp	x8, x25, [x29, #104]
   278f8:	ldr	x20, [x29, #96]
   278fc:	stp	x22, x21, [sp, #144]
   27900:	ldr	w22, [x29, #120]
   27904:	mov	w9, #0x1                   	// #1
   27908:	stp	x28, x27, [sp, #96]
   2790c:	lsl	x26, x9, x2
   27910:	add	x27, x8, w2, sxtw #3
   27914:	lsl	x21, x20, #1
   27918:	stp	x24, x23, [sp, #128]
   2791c:	mov	x24, x6
   27920:	stp	x1, x5, [sp, #24]
   27924:	mov	x23, x4
   27928:	mov	x19, x3
   2792c:	str	x0, [sp, #16]
   27930:	sxtw	x28, w2
   27934:	stur	x2, [x29, #-32]
   27938:	mov	w5, #0x1                   	// #1
   2793c:	mov	x0, x3
   27940:	mov	x1, x26
   27944:	mov	x2, x27
   27948:	mov	x3, x21
   2794c:	mov	x4, x6
   27950:	mov	x6, x25
   27954:	stur	x7, [x29, #-8]
   27958:	bl	27e54 <__gmpn_mul_fft@@Base+0xbc0>
   2795c:	cbnz	w22, 27980 <__gmpn_mul_fft@@Base+0x6ec>
   27960:	mov	w5, #0x1                   	// #1
   27964:	mov	x0, x23
   27968:	mov	x1, x26
   2796c:	mov	x2, x27
   27970:	mov	x3, x21
   27974:	mov	x4, x24
   27978:	mov	x6, x25
   2797c:	bl	27e54 <__gmpn_mul_fft@@Base+0xbc0>
   27980:	cmp	w22, #0x0
   27984:	csel	x1, x19, x23, ne  // ne = any
   27988:	mov	x0, x19
   2798c:	mov	x2, x24
   27990:	mov	x3, x26
   27994:	bl	2800c <__gmpn_mul_fft@@Base+0xd78>
   27998:	mov	x0, x19
   2799c:	mov	x1, x26
   279a0:	mov	x2, x21
   279a4:	mov	x3, x24
   279a8:	mov	x4, x25
   279ac:	bl	284b4 <__gmpn_mul_fft@@Base+0x1220>
   279b0:	add	x8, x25, x24, lsl #3
   279b4:	add	x0, x8, #0x8
   279b8:	str	x0, [x23]
   279bc:	ldr	x1, [x19]
   279c0:	mov	x2, x28
   279c4:	mov	x3, x24
   279c8:	stur	x25, [x29, #-16]
   279cc:	mov	x25, x23
   279d0:	bl	2863c <__gmpn_mul_fft@@Base+0x13a8>
   279d4:	cmp	x26, #0x2
   279d8:	b.lt	27a10 <__gmpn_mul_fft@@Base+0x77c>  // b.tstop
   279dc:	sub	x21, x26, #0x1
   279e0:	add	x22, x19, #0x8
   279e4:	madd	x19, x20, x21, x28
   279e8:	add	x23, x25, #0x8
   279ec:	ldur	x0, [x22, #-8]
   279f0:	mov	x2, x19
   279f4:	mov	x3, x24
   279f8:	str	x0, [x23], #8
   279fc:	ldr	x1, [x22], #8
   27a00:	bl	2863c <__gmpn_mul_fft@@Base+0x13a8>
   27a04:	sub	x21, x21, #0x1
   27a08:	sub	x19, x19, x20
   27a0c:	cbnz	x21, 279ec <__gmpn_mul_fft@@Base+0x758>
   27a10:	adds	x21, x24, #0x1
   27a14:	b.cs	27a2c <__gmpn_mul_fft@@Base+0x798>  // b.hs, b.nlast
   27a18:	ldur	x0, [x29, #-16]
   27a1c:	lsl	x8, x24, #3
   27a20:	add	x2, x8, #0x8
   27a24:	mov	w1, wzr
   27a28:	bl	c780 <memset@plt>
   27a2c:	ldur	x8, [x29, #-8]
   27a30:	sub	x9, x26, #0x1
   27a34:	stur	x9, [x29, #-24]
   27a38:	madd	x20, x9, x8, x24
   27a3c:	adds	x19, x20, #0x1
   27a40:	b.cs	27a58 <__gmpn_mul_fft@@Base+0x7c4>  // b.hs, b.nlast
   27a44:	ldr	x0, [sp, #32]
   27a48:	lsl	x8, x20, #3
   27a4c:	add	x2, x8, #0x8
   27a50:	mov	w1, wzr
   27a54:	bl	c780 <memset@plt>
   27a58:	ldur	x8, [x29, #-32]
   27a5c:	mov	x28, xzr
   27a60:	stp	x20, x19, [sp]
   27a64:	cmp	w8, #0x3f
   27a68:	b.ne	27ad8 <__gmpn_mul_fft@@Base+0x844>  // b.any
   27a6c:	cmp	x28, #0x1
   27a70:	b.eq	27bbc <__gmpn_mul_fft@@Base+0x928>  // b.none
   27a74:	ldp	x22, x21, [sp, #16]
   27a78:	ldr	x23, [sp, #8]
   27a7c:	cmn	x28, #0x1
   27a80:	b.ne	27c0c <__gmpn_mul_fft@@Base+0x978>  // b.any
   27a84:	ldr	x8, [sp, #32]
   27a88:	mov	w3, #0x1                   	// #1
   27a8c:	mov	x2, x21
   27a90:	add	x8, x8, x23, lsl #3
   27a94:	sub	x19, x8, x21, lsl #3
   27a98:	mov	x0, x19
   27a9c:	mov	x1, x19
   27aa0:	bl	c150 <__gmpn_add_1@plt>
   27aa4:	cbz	x0, 27c0c <__gmpn_mul_fft@@Base+0x978>
   27aa8:	sub	x0, x19, #0x8
   27aac:	add	x2, x21, #0x1
   27ab0:	mov	w3, #0x1                   	// #1
   27ab4:	mov	x1, x0
   27ab8:	bl	caf0 <__gmpn_sub_1@plt>
   27abc:	ldr	x8, [sp, #32]
   27ac0:	ldr	x9, [sp]
   27ac4:	mov	w2, #0x1                   	// #1
   27ac8:	mov	w3, #0x1                   	// #1
   27acc:	add	x0, x8, x9, lsl #3
   27ad0:	mov	x1, x0
   27ad4:	b	27be4 <__gmpn_mul_fft@@Base+0x950>
   27ad8:	ldur	x9, [x29, #-8]
   27adc:	lsl	x27, x24, #3
   27ae0:	mov	w20, #0x1                   	// #1
   27ae4:	mov	w22, #0x1                   	// #1
   27ae8:	lsl	x8, x9, #1
   27aec:	stur	x8, [x29, #-32]
   27af0:	ldur	x8, [x29, #-24]
   27af4:	neg	x23, x9, lsl #3
   27af8:	str	x24, [sp, #40]
   27afc:	mul	x8, x9, x8
   27b00:	ldr	x9, [sp, #32]
   27b04:	add	x19, x9, x8, lsl #3
   27b08:	b	27b28 <__gmpn_mul_fft@@Base+0x894>
   27b0c:	ldur	x8, [x29, #-8]
   27b10:	sub	x26, x26, #0x1
   27b14:	add	x22, x22, #0x1
   27b18:	add	x19, x19, x23
   27b1c:	cmp	x26, #0x0
   27b20:	add	x20, x20, x8
   27b24:	b.le	27a6c <__gmpn_mul_fft@@Base+0x7d8>
   27b28:	ldur	x8, [x29, #-24]
   27b2c:	mov	x0, x19
   27b30:	mov	x1, x19
   27b34:	mov	x3, x21
   27b38:	and	x24, x22, x8
   27b3c:	ldr	x2, [x25, x24, lsl #3]
   27b40:	bl	cc30 <__gmpn_add_n@plt>
   27b44:	cbz	x0, 27b64 <__gmpn_mul_fft@@Base+0x8d0>
   27b48:	add	x8, x19, x27
   27b4c:	add	x0, x8, #0x8
   27b50:	sub	x2, x20, #0x1
   27b54:	mov	w3, #0x1                   	// #1
   27b58:	mov	x1, x0
   27b5c:	bl	c150 <__gmpn_add_1@plt>
   27b60:	add	x28, x0, x28
   27b64:	ldur	x1, [x29, #-16]
   27b68:	ldur	x8, [x29, #-32]
   27b6c:	mov	x2, x21
   27b70:	str	x26, [x1, x8, lsl #3]
   27b74:	ldr	x0, [x25, x24, lsl #3]
   27b78:	bl	c570 <__gmpn_cmp@plt>
   27b7c:	cmp	w0, #0x1
   27b80:	b.lt	27b0c <__gmpn_mul_fft@@Base+0x878>  // b.tstop
   27b84:	ldr	x8, [sp, #40]
   27b88:	mov	w3, #0x1                   	// #1
   27b8c:	mov	x0, x19
   27b90:	mov	x1, x19
   27b94:	add	x2, x8, x20
   27b98:	bl	caf0 <__gmpn_sub_1@plt>
   27b9c:	sub	x24, x28, x0
   27ba0:	add	x0, x19, x27
   27ba4:	mov	w3, #0x1                   	// #1
   27ba8:	mov	x1, x0
   27bac:	mov	x2, x20
   27bb0:	bl	caf0 <__gmpn_sub_1@plt>
   27bb4:	sub	x28, x24, x0
   27bb8:	b	27b0c <__gmpn_mul_fft@@Base+0x878>
   27bbc:	ldp	x21, x8, [sp, #24]
   27bc0:	ldp	x23, x22, [sp, #8]
   27bc4:	lsl	x19, x21, #1
   27bc8:	cmp	x23, x19
   27bcc:	add	x8, x8, x23, lsl #3
   27bd0:	b.ge	27bec <__gmpn_mul_fft@@Base+0x958>  // b.tcont
   27bd4:	sub	x0, x8, x21, lsl #3
   27bd8:	mov	x1, x0
   27bdc:	mov	x2, x21
   27be0:	mov	x3, x28
   27be4:	bl	caf0 <__gmpn_sub_1@plt>
   27be8:	b	27c0c <__gmpn_mul_fft@@Base+0x978>
   27bec:	sub	x20, x8, x19, lsl #3
   27bf0:	mov	x0, x20
   27bf4:	mov	x1, x20
   27bf8:	mov	x2, x19
   27bfc:	mov	x3, x28
   27c00:	bl	c150 <__gmpn_add_1@plt>
   27c04:	mov	x28, x0
   27c08:	cbnz	x0, 27bf0 <__gmpn_mul_fft@@Base+0x95c>
   27c0c:	ldr	x2, [sp, #32]
   27c10:	mov	x0, x22
   27c14:	mov	x1, x21
   27c18:	mov	x3, x23
   27c1c:	bl	28674 <__gmpn_mul_fft@@Base+0x13e0>
   27c20:	ldp	x20, x19, [sp, #160]
   27c24:	ldp	x22, x21, [sp, #144]
   27c28:	ldp	x24, x23, [sp, #128]
   27c2c:	ldp	x26, x25, [sp, #112]
   27c30:	ldp	x28, x27, [sp, #96]
   27c34:	ldp	x29, x30, [sp, #80]
   27c38:	add	sp, sp, #0xb0
   27c3c:	ret
   27c40:	stp	x29, x30, [sp, #-80]!
   27c44:	stp	x22, x21, [sp, #48]
   27c48:	lsr	x21, x2, #6
   27c4c:	stp	x24, x23, [sp, #32]
   27c50:	stp	x20, x19, [sp, #64]
   27c54:	mov	x19, x3
   27c58:	mov	x23, x1
   27c5c:	mov	x20, x0
   27c60:	subs	x22, x21, x3
   27c64:	and	w24, w2, #0x3f
   27c68:	add	x8, x1, x3, lsl #3
   27c6c:	str	x25, [sp, #16]
   27c70:	mov	x29, sp
   27c74:	b.ge	27cb8 <__gmpn_mul_fft@@Base+0xa24>  // b.tcont
   27c78:	sub	x1, x8, x21, lsl #3
   27c7c:	add	x2, x21, #0x1
   27c80:	mov	x0, x20
   27c84:	cbz	w24, 27cf0 <__gmpn_mul_fft@@Base+0xa5c>
   27c88:	mov	w3, w24
   27c8c:	bl	d330 <__gmpn_lshiftc@plt>
   27c90:	add	x0, x20, x21, lsl #3
   27c94:	ldr	x8, [x0]
   27c98:	sub	x2, x19, x21
   27c9c:	mov	x1, x23
   27ca0:	mov	w3, w24
   27ca4:	mvn	x22, x8
   27ca8:	bl	c2d0 <__gmpn_lshift@plt>
   27cac:	mov	x3, x0
   27cb0:	cbnz	x21, 27d10 <__gmpn_mul_fft@@Base+0xa7c>
   27cb4:	b	27ddc <__gmpn_mul_fft@@Base+0xb48>
   27cb8:	sub	x1, x8, x22, lsl #3
   27cbc:	cbz	w24, 27d1c <__gmpn_mul_fft@@Base+0xa88>
   27cc0:	add	x2, x22, #0x1
   27cc4:	mov	x0, x20
   27cc8:	mov	w3, w24
   27ccc:	bl	c2d0 <__gmpn_lshift@plt>
   27cd0:	add	x0, x20, x22, lsl #3
   27cd4:	ldr	x25, [x0]
   27cd8:	sub	x2, x19, x22
   27cdc:	mov	x1, x23
   27ce0:	mov	w3, w24
   27ce4:	bl	d330 <__gmpn_lshiftc@plt>
   27ce8:	add	x8, x0, #0x1
   27cec:	b	27d40 <__gmpn_mul_fft@@Base+0xaac>
   27cf0:	bl	c3e0 <__gmpn_com@plt>
   27cf4:	ldr	x22, [x23, x19, lsl #3]
   27cf8:	add	x0, x20, x21, lsl #3
   27cfc:	sub	x2, x19, x21
   27d00:	mov	x1, x23
   27d04:	bl	cc10 <__gmpn_copyi@plt>
   27d08:	mov	x3, xzr
   27d0c:	cbz	x21, 27ddc <__gmpn_mul_fft@@Base+0xb48>
   27d10:	cbz	x3, 27db0 <__gmpn_mul_fft@@Base+0xb1c>
   27d14:	sub	x3, x3, #0x1
   27d18:	b	27dc8 <__gmpn_mul_fft@@Base+0xb34>
   27d1c:	mov	x0, x20
   27d20:	mov	x2, x22
   27d24:	bl	cc10 <__gmpn_copyi@plt>
   27d28:	ldr	x25, [x23, x19, lsl #3]
   27d2c:	add	x0, x20, x22, lsl #3
   27d30:	sub	x2, x19, x22
   27d34:	mov	x1, x23
   27d38:	bl	c3e0 <__gmpn_com@plt>
   27d3c:	mov	w8, #0x1                   	// #1
   27d40:	str	xzr, [x20, x19, lsl #3]
   27d44:	ldr	x9, [x20]
   27d48:	adds	x8, x9, x8
   27d4c:	str	x8, [x20]
   27d50:	b.cc	27d68 <__gmpn_mul_fft@@Base+0xad4>  // b.lo, b.ul, b.last
   27d54:	add	x8, x20, #0x8
   27d58:	ldr	x9, [x8]
   27d5c:	adds	x9, x9, #0x1
   27d60:	str	x9, [x8], #8
   27d64:	b.cs	27d58 <__gmpn_mul_fft@@Base+0xac4>  // b.hs, b.nlast
   27d68:	adds	x9, x25, #0x1
   27d6c:	cset	w8, cs  // cs = hs, nlast
   27d70:	add	x10, x20, x22, lsl #3
   27d74:	lsl	x11, x8, #3
   27d78:	ldr	x12, [x10, x11]
   27d7c:	csinc	x9, x9, xzr, cc  // cc = lo, ul, last
   27d80:	adds	x9, x12, x9
   27d84:	str	x9, [x10, x11]
   27d88:	b.cc	27e3c <__gmpn_mul_fft@@Base+0xba8>  // b.lo, b.ul, b.last
   27d8c:	add	x8, x21, x8
   27d90:	sub	x8, x8, x19
   27d94:	add	x8, x20, x8, lsl #3
   27d98:	add	x8, x8, #0x8
   27d9c:	ldr	x9, [x8]
   27da0:	adds	x9, x9, #0x1
   27da4:	str	x9, [x8], #8
   27da8:	b.cs	27d9c <__gmpn_mul_fft@@Base+0xb08>  // b.hs, b.nlast
   27dac:	b	27e3c <__gmpn_mul_fft@@Base+0xba8>
   27db0:	mov	w3, #0x1                   	// #1
   27db4:	mov	x0, x20
   27db8:	mov	x1, x20
   27dbc:	mov	x2, x19
   27dc0:	bl	c150 <__gmpn_add_1@plt>
   27dc4:	mov	x3, x0
   27dc8:	mov	x0, x20
   27dcc:	mov	x1, x20
   27dd0:	mov	x2, x21
   27dd4:	bl	caf0 <__gmpn_sub_1@plt>
   27dd8:	add	x3, x0, #0x1
   27ddc:	add	x23, x20, x21, lsl #3
   27de0:	sub	x21, x19, x21
   27de4:	mov	x0, x23
   27de8:	mov	x1, x23
   27dec:	mov	x2, x21
   27df0:	bl	caf0 <__gmpn_sub_1@plt>
   27df4:	neg	x8, x0
   27df8:	lsl	x24, x19, #3
   27dfc:	mov	x0, x23
   27e00:	mov	x1, x23
   27e04:	mov	x2, x21
   27e08:	mov	x3, x22
   27e0c:	str	x8, [x20, x24]
   27e10:	bl	caf0 <__gmpn_sub_1@plt>
   27e14:	ldr	x8, [x20, x24]
   27e18:	subs	x8, x8, x0
   27e1c:	str	x8, [x20, x24]
   27e20:	b.pl	27e3c <__gmpn_mul_fft@@Base+0xba8>  // b.nfrst
   27e24:	mov	w3, #0x1                   	// #1
   27e28:	mov	x0, x20
   27e2c:	mov	x1, x20
   27e30:	mov	x2, x19
   27e34:	bl	c150 <__gmpn_add_1@plt>
   27e38:	str	x0, [x20, x19, lsl #3]
   27e3c:	ldp	x20, x19, [sp, #64]
   27e40:	ldp	x22, x21, [sp, #48]
   27e44:	ldp	x24, x23, [sp, #32]
   27e48:	ldr	x25, [sp, #16]
   27e4c:	ldp	x29, x30, [sp], #80
   27e50:	ret
   27e54:	sub	sp, sp, #0x70
   27e58:	stp	x24, x23, [sp, #64]
   27e5c:	stp	x22, x21, [sp, #80]
   27e60:	stp	x20, x19, [sp, #96]
   27e64:	mov	x21, x6
   27e68:	mov	x24, x5
   27e6c:	mov	x19, x4
   27e70:	cmp	x1, #0x2
   27e74:	mov	x20, x0
   27e78:	stp	x29, x30, [sp, #16]
   27e7c:	stp	x28, x27, [sp, #32]
   27e80:	stp	x26, x25, [sp, #48]
   27e84:	add	x29, sp, #0x10
   27e88:	b.ne	27f2c <__gmpn_mul_fft@@Base+0xc98>  // b.any
   27e8c:	ldr	x1, [x20]
   27e90:	add	x22, x19, #0x1
   27e94:	mov	x0, x21
   27e98:	mov	x2, x22
   27e9c:	bl	cc10 <__gmpn_copyi@plt>
   27ea0:	ldr	x0, [x20]
   27ea4:	lsl	x23, x24, #3
   27ea8:	ldr	x2, [x20, x23]
   27eac:	mov	x3, x22
   27eb0:	mov	x1, x0
   27eb4:	bl	cc30 <__gmpn_add_n@plt>
   27eb8:	ldr	x0, [x20, x23]
   27ebc:	mov	x1, x21
   27ec0:	mov	x3, x22
   27ec4:	mov	x2, x0
   27ec8:	bl	c420 <__gmpn_sub_n@plt>
   27ecc:	ldr	x1, [x20]
   27ed0:	mov	x21, x0
   27ed4:	ldr	x8, [x1, x19, lsl #3]
   27ed8:	cmp	x8, #0x2
   27edc:	b.cc	27f00 <__gmpn_mul_fft@@Base+0xc6c>  // b.lo, b.ul, b.last
   27ee0:	sub	x3, x8, #0x1
   27ee4:	mov	x0, x1
   27ee8:	mov	x2, x19
   27eec:	bl	caf0 <__gmpn_sub_1@plt>
   27ef0:	ldr	x8, [x20]
   27ef4:	mov	w9, #0x1                   	// #1
   27ef8:	sub	x9, x9, x0
   27efc:	str	x9, [x8, x19, lsl #3]
   27f00:	cbz	x21, 27fec <__gmpn_mul_fft@@Base+0xd58>
   27f04:	ldr	x0, [x20, x23]
   27f08:	lsl	x21, x19, #3
   27f0c:	mov	x2, x19
   27f10:	ldr	x8, [x0, x21]
   27f14:	mov	x1, x0
   27f18:	neg	x3, x8
   27f1c:	bl	c150 <__gmpn_add_1@plt>
   27f20:	ldr	x8, [x20, x23]
   27f24:	str	x0, [x8, x21]
   27f28:	b	27fec <__gmpn_mul_fft@@Base+0xd58>
   27f2c:	mov	x26, x2
   27f30:	ldr	x28, [x26], #-8
   27f34:	asr	x23, x1, #1
   27f38:	lsl	x27, x3, #1
   27f3c:	mov	x22, x3
   27f40:	mov	x25, x1
   27f44:	lsl	x5, x24, #1
   27f48:	mov	x0, x20
   27f4c:	mov	x1, x23
   27f50:	mov	x2, x26
   27f54:	mov	x3, x27
   27f58:	mov	x4, x19
   27f5c:	mov	x6, x21
   27f60:	str	x5, [sp, #8]
   27f64:	bl	27e54 <__gmpn_mul_fft@@Base+0xbc0>
   27f68:	ldr	x5, [sp, #8]
   27f6c:	add	x0, x20, x24, lsl #3
   27f70:	mov	x1, x23
   27f74:	mov	x2, x26
   27f78:	mov	x3, x27
   27f7c:	mov	x4, x19
   27f80:	mov	x6, x21
   27f84:	bl	27e54 <__gmpn_mul_fft@@Base+0xbc0>
   27f88:	cmp	x25, #0x2
   27f8c:	b.lt	27fec <__gmpn_mul_fft@@Base+0xd58>  // b.tstop
   27f90:	mov	x25, xzr
   27f94:	lsl	x26, x24, #4
   27f98:	lsl	x24, x24, #3
   27f9c:	ldr	x1, [x20, x24]
   27fa0:	ldrsw	x8, [x28], #8
   27fa4:	mov	x0, x21
   27fa8:	mov	x3, x19
   27fac:	mul	x2, x8, x22
   27fb0:	bl	27c40 <__gmpn_mul_fft@@Base+0x9ac>
   27fb4:	ldr	x0, [x20, x24]
   27fb8:	ldr	x1, [x20]
   27fbc:	mov	x2, x21
   27fc0:	mov	x3, x19
   27fc4:	bl	2874c <__gmpn_mul_fft@@Base+0x14b8>
   27fc8:	ldr	x0, [x20]
   27fcc:	mov	x2, x21
   27fd0:	mov	x3, x19
   27fd4:	mov	x1, x0
   27fd8:	bl	287bc <__gmpn_mul_fft@@Base+0x1528>
   27fdc:	add	x25, x25, #0x1
   27fe0:	cmp	x25, x23
   27fe4:	add	x20, x20, x26
   27fe8:	b.lt	27f9c <__gmpn_mul_fft@@Base+0xd08>  // b.tstop
   27fec:	ldp	x20, x19, [sp, #96]
   27ff0:	ldp	x22, x21, [sp, #80]
   27ff4:	ldp	x24, x23, [sp, #64]
   27ff8:	ldp	x26, x25, [sp, #48]
   27ffc:	ldp	x28, x27, [sp, #32]
   28000:	ldp	x29, x30, [sp, #16]
   28004:	add	sp, sp, #0x70
   28008:	ret
   2800c:	sub	sp, sp, #0xf0
   28010:	cmp	x0, x1
   28014:	mov	w8, #0x13c                 	// #316
   28018:	mov	w9, #0x110                 	// #272
   2801c:	stp	x22, x21, [sp, #208]
   28020:	csel	x21, x9, x8, eq  // eq = none
   28024:	stp	x29, x30, [sp, #144]
   28028:	stp	x20, x19, [sp, #224]
   2802c:	add	x29, sp, #0x90
   28030:	mov	x20, x3
   28034:	cset	w22, eq  // eq = none
   28038:	cmp	x21, x2
   2803c:	stp	x28, x27, [sp, #160]
   28040:	stp	x26, x25, [sp, #176]
   28044:	stp	x24, x23, [sp, #192]
   28048:	stp	x0, x1, [x29, #-32]
   2804c:	stp	x2, xzr, [x29, #-16]
   28050:	b.le	2817c <__gmpn_mul_fft@@Base+0xee8>
   28054:	ldur	x8, [x29, #-16]
   28058:	sub	x0, x29, #0x8
   2805c:	lsl	x1, x8, #4
   28060:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28064:	cmp	x20, #0x1
   28068:	b.lt	28454 <__gmpn_mul_fft@@Base+0x11c0>  // b.tstop
   2806c:	ldp	x28, x8, [x29, #-24]
   28070:	ldur	x27, [x29, #-32]
   28074:	ldur	x19, [x29, #-16]
   28078:	mov	x21, x0
   2807c:	lsl	x22, x8, #1
   28080:	add	x23, x0, x8, lsl #3
   28084:	b	280b4 <__gmpn_mul_fft@@Base+0xe20>
   28088:	ldur	x2, [x29, #-16]
   2808c:	mov	w3, #0x1                   	// #1
   28090:	mov	x0, x24
   28094:	mov	x1, x24
   28098:	bl	c150 <__gmpn_add_1@plt>
   2809c:	cmp	x0, #0x0
   280a0:	cset	w8, ne  // ne = any
   280a4:	ldur	x9, [x29, #-16]
   280a8:	subs	x20, x20, #0x1
   280ac:	str	x8, [x24, x9, lsl #3]
   280b0:	b.eq	28454 <__gmpn_mul_fft@@Base+0x11c0>  // b.none
   280b4:	ldr	x24, [x27], #8
   280b8:	ldr	x25, [x28], #8
   280bc:	ldp	x9, x8, [x29, #-32]
   280c0:	mov	x0, x21
   280c4:	cmp	x9, x8
   280c8:	b.eq	280e0 <__gmpn_mul_fft@@Base+0xe4c>  // b.none
   280cc:	ldur	x3, [x29, #-16]
   280d0:	mov	x1, x25
   280d4:	mov	x2, x24
   280d8:	bl	cb50 <__gmpn_mul_n@plt>
   280dc:	b	280ec <__gmpn_mul_fft@@Base+0xe58>
   280e0:	ldur	x2, [x29, #-16]
   280e4:	mov	x1, x24
   280e8:	bl	ca90 <__gmpn_sqr@plt>
   280ec:	ldur	x8, [x29, #-16]
   280f0:	ldr	x8, [x24, x8, lsl #3]
   280f4:	cbz	x8, 28114 <__gmpn_mul_fft@@Base+0xe80>
   280f8:	ldur	x3, [x29, #-16]
   280fc:	mov	x0, x23
   28100:	mov	x1, x23
   28104:	mov	x2, x25
   28108:	bl	cc30 <__gmpn_add_n@plt>
   2810c:	mov	x26, x0
   28110:	b	28118 <__gmpn_mul_fft@@Base+0xe84>
   28114:	mov	x26, xzr
   28118:	ldur	x8, [x29, #-16]
   2811c:	ldr	x8, [x25, x8, lsl #3]
   28120:	cbz	x8, 28144 <__gmpn_mul_fft@@Base+0xeb0>
   28124:	mov	x0, x23
   28128:	mov	x1, x23
   2812c:	mov	x2, x24
   28130:	mov	x3, x19
   28134:	bl	cc30 <__gmpn_add_n@plt>
   28138:	ldr	x8, [x24, x19, lsl #3]
   2813c:	add	x9, x0, x26
   28140:	add	x26, x9, x8
   28144:	cbz	x26, 2815c <__gmpn_mul_fft@@Base+0xec8>
   28148:	mov	x0, x21
   2814c:	mov	x1, x21
   28150:	mov	x2, x22
   28154:	mov	x3, x26
   28158:	bl	c150 <__gmpn_add_1@plt>
   2815c:	ldur	x3, [x29, #-16]
   28160:	mov	x0, x24
   28164:	mov	x1, x21
   28168:	mov	x2, x23
   2816c:	bl	c420 <__gmpn_sub_n@plt>
   28170:	cbnz	x0, 28088 <__gmpn_mul_fft@@Base+0xdf4>
   28174:	mov	w8, wzr
   28178:	b	280a4 <__gmpn_mul_fft@@Base+0xe10>
   2817c:	ldur	x19, [x29, #-16]
   28180:	mov	w1, w22
   28184:	mov	x0, x19
   28188:	bl	cc90 <__gmpn_fft_best_k@plt>
   2818c:	mov	w8, #0x1                   	// #1
   28190:	lsl	x23, x8, x0
   28194:	sub	x8, x23, #0x1
   28198:	tst	x8, x19
   2819c:	b.ne	28484 <__gmpn_mul_fft@@Base+0x11f0>  // b.any
   281a0:	ldur	x9, [x29, #-16]
   281a4:	mov	w10, w0
   281a8:	cmp	x23, #0x40
   281ac:	mov	w8, #0x40                  	// #64
   281b0:	lsl	x9, x9, #6
   281b4:	asr	x9, x9, x10
   281b8:	lsl	x9, x9, #1
   281bc:	csel	x8, x23, x8, gt
   281c0:	add	x9, x9, w10, sxtw
   281c4:	add	x9, x9, x8
   281c8:	add	x9, x9, #0x2
   281cc:	sdiv	x9, x9, x8
   281d0:	mul	x24, x9, x8
   281d4:	add	x8, x24, #0x3f
   281d8:	cmp	x24, #0x0
   281dc:	csel	x8, x8, x24, lt  // lt = tstop
   281e0:	asr	x26, x8, #6
   281e4:	cmp	x26, x21
   281e8:	stur	x10, [x29, #-64]
   281ec:	b.lt	28244 <__gmpn_mul_fft@@Base+0xfb0>  // b.tstop
   281f0:	mov	x0, x26
   281f4:	mov	w1, w22
   281f8:	bl	cc90 <__gmpn_fft_best_k@plt>
   281fc:	mov	w8, #0x1                   	// #1
   28200:	lsl	x8, x8, x0
   28204:	sub	x9, x8, #0x1
   28208:	tst	x9, x26
   2820c:	b.eq	28244 <__gmpn_mul_fft@@Base+0xfb0>  // b.none
   28210:	mov	w21, #0x1                   	// #1
   28214:	add	x9, x8, x26
   28218:	sub	x9, x9, #0x1
   2821c:	neg	x8, x8
   28220:	and	x26, x9, x8
   28224:	mov	x0, x26
   28228:	mov	w1, w22
   2822c:	bl	cc90 <__gmpn_fft_best_k@plt>
   28230:	lsl	x8, x21, x0
   28234:	sub	x9, x8, #0x1
   28238:	tst	x9, x26
   2823c:	b.ne	28214 <__gmpn_mul_fft@@Base+0xf80>  // b.any
   28240:	lsl	x24, x26, #6
   28244:	ldur	x8, [x29, #-16]
   28248:	cmp	x26, x8
   2824c:	b.ge	2849c <__gmpn_mul_fft@@Base+0x1208>  // b.tcont
   28250:	lsl	x21, x23, #3
   28254:	sub	x0, x29, #0x8
   28258:	mov	x1, x21
   2825c:	str	w22, [sp, #68]
   28260:	str	x23, [sp, #56]
   28264:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28268:	str	x0, [sp, #48]
   2826c:	sub	x0, x29, #0x8
   28270:	mov	x1, x21
   28274:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28278:	ldur	x23, [x29, #-64]
   2827c:	add	x21, x26, #0x1
   28280:	lsl	x8, x21, #1
   28284:	stur	x0, [x29, #-48]
   28288:	lsl	x8, x8, x23
   2828c:	lsl	x1, x8, #3
   28290:	sub	x0, x29, #0x8
   28294:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28298:	str	x0, [sp, #72]
   2829c:	lsl	x1, x21, #4
   282a0:	sub	x0, x29, #0x8
   282a4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   282a8:	add	w8, w23, #0x1
   282ac:	stur	x0, [x29, #-40]
   282b0:	sbfiz	x1, x8, #3, #32
   282b4:	sub	x0, x29, #0x8
   282b8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   282bc:	mov	w8, #0x8                   	// #8
   282c0:	mov	x28, x0
   282c4:	lsl	x1, x8, x23
   282c8:	sub	x0, x29, #0x8
   282cc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   282d0:	tbnz	w23, #31, 282fc <__gmpn_mul_fft@@Base+0x1068>
   282d4:	ldur	x9, [x29, #-64]
   282d8:	mov	x8, xzr
   282dc:	mov	w10, #0x1                   	// #1
   282e0:	add	w9, w9, #0x1
   282e4:	str	x0, [x28, x8, lsl #3]
   282e8:	lsl	x11, x10, x8
   282ec:	add	x8, x8, #0x1
   282f0:	cmp	x9, x8
   282f4:	add	x0, x0, x11, lsl #2
   282f8:	b.ne	282e4 <__gmpn_mul_fft@@Base+0x1050>  // b.any
   282fc:	mov	x0, x28
   28300:	ldur	x1, [x29, #-64]
   28304:	bl	275c4 <__gmpn_mul_fft@@Base+0x330>
   28308:	cmp	x20, #0x1
   2830c:	b.lt	28454 <__gmpn_mul_fft@@Base+0x11c0>  // b.tstop
   28310:	ldur	x9, [x29, #-64]
   28314:	ldur	x10, [x29, #-16]
   28318:	ldp	x27, x25, [x29, #-32]
   2831c:	str	x28, [sp, #32]
   28320:	lsl	x8, x21, x9
   28324:	asr	x22, x10, x9
   28328:	ldr	x10, [sp, #72]
   2832c:	asr	x23, x24, x9
   28330:	add	x21, x10, x8, lsl #3
   28334:	lsl	x8, x22, x9
   28338:	add	x8, x8, #0x1
   2833c:	str	x8, [sp, #40]
   28340:	b	283c0 <__gmpn_mul_fft@@Base+0x112c>
   28344:	ldp	x24, x2, [sp, #48]
   28348:	ldr	x4, [x27]
   2834c:	ldur	x8, [x29, #-40]
   28350:	ldr	x0, [sp, #72]
   28354:	ldr	x5, [sp, #40]
   28358:	mov	x1, x24
   2835c:	mov	x3, x26
   28360:	str	x8, [sp]
   28364:	mov	x6, x22
   28368:	mov	x7, x23
   2836c:	bl	27668 <__gmpn_mul_fft@@Base+0x3d4>
   28370:	ldr	w8, [sp, #68]
   28374:	ldur	x19, [x29, #-16]
   28378:	ldr	x0, [x27]
   2837c:	ldur	x4, [x29, #-48]
   28380:	str	w8, [sp, #24]
   28384:	ldur	x8, [x29, #-40]
   28388:	mov	x1, x19
   2838c:	ldur	x2, [x29, #-64]
   28390:	mov	x3, x24
   28394:	mov	x5, x21
   28398:	mov	x6, x26
   2839c:	mov	x7, x22
   283a0:	stp	x28, x8, [sp, #8]
   283a4:	str	x23, [sp]
   283a8:	bl	278e0 <__gmpn_mul_fft@@Base+0x64c>
   283ac:	ldr	x8, [x27], #8
   283b0:	subs	x20, x20, #0x1
   283b4:	add	x25, x25, #0x8
   283b8:	str	x0, [x8, x19, lsl #3]
   283bc:	b.eq	28454 <__gmpn_mul_fft@@Base+0x11c0>  // b.none
   283c0:	ldr	x0, [x27]
   283c4:	ldur	x1, [x29, #-16]
   283c8:	bl	2882c <__gmpn_mul_fft@@Base+0x1598>
   283cc:	ldp	x9, x8, [x29, #-32]
   283d0:	cmp	x9, x8
   283d4:	b.eq	28344 <__gmpn_mul_fft@@Base+0x10b0>  // b.none
   283d8:	ldr	x0, [x25]
   283dc:	ldur	x1, [x29, #-16]
   283e0:	bl	2882c <__gmpn_mul_fft@@Base+0x1598>
   283e4:	ldp	x28, x24, [sp, #48]
   283e8:	ldr	x19, [sp, #40]
   283ec:	ldr	x4, [x27]
   283f0:	stur	x20, [x29, #-56]
   283f4:	mov	x20, x26
   283f8:	mov	x26, x23
   283fc:	ldur	x23, [x29, #-40]
   28400:	ldr	x0, [sp, #72]
   28404:	mov	x1, x28
   28408:	mov	x2, x24
   2840c:	mov	x3, x20
   28410:	mov	x5, x19
   28414:	mov	x6, x22
   28418:	mov	x7, x26
   2841c:	str	x23, [sp]
   28420:	bl	27668 <__gmpn_mul_fft@@Base+0x3d4>
   28424:	ldr	x4, [x25]
   28428:	str	x23, [sp]
   2842c:	mov	x23, x26
   28430:	mov	x26, x20
   28434:	ldp	x20, x1, [x29, #-56]
   28438:	mov	x2, x24
   2843c:	mov	x24, x28
   28440:	ldr	x28, [sp, #32]
   28444:	mov	x0, x21
   28448:	mov	x3, x26
   2844c:	mov	x5, x19
   28450:	b	28364 <__gmpn_mul_fft@@Base+0x10d0>
   28454:	ldur	x0, [x29, #-8]
   28458:	cbnz	x0, 2847c <__gmpn_mul_fft@@Base+0x11e8>
   2845c:	ldp	x20, x19, [sp, #224]
   28460:	ldp	x22, x21, [sp, #208]
   28464:	ldp	x24, x23, [sp, #192]
   28468:	ldp	x26, x25, [sp, #176]
   2846c:	ldp	x28, x27, [sp, #160]
   28470:	ldp	x29, x30, [sp, #144]
   28474:	add	sp, sp, #0xf0
   28478:	ret
   2847c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   28480:	b	2845c <__gmpn_mul_fft@@Base+0x11c8>
   28484:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   28488:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   2848c:	add	x0, x0, #0x89e
   28490:	add	x2, x2, #0x8df
   28494:	mov	w1, #0x1d9                 	// #473
   28498:	bl	c850 <__gmp_assert_fail@plt>
   2849c:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   284a0:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   284a4:	add	x0, x0, #0x89e
   284a8:	add	x2, x2, #0x8f3
   284ac:	mov	w1, #0x1ef                 	// #495
   284b0:	bl	c850 <__gmp_assert_fail@plt>
   284b4:	stp	x29, x30, [sp, #-96]!
   284b8:	stp	x22, x21, [sp, #64]
   284bc:	stp	x20, x19, [sp, #80]
   284c0:	mov	x21, x4
   284c4:	mov	x20, x3
   284c8:	cmp	x1, #0x2
   284cc:	mov	x19, x0
   284d0:	str	x27, [sp, #16]
   284d4:	stp	x26, x25, [sp, #32]
   284d8:	stp	x24, x23, [sp, #48]
   284dc:	mov	x29, sp
   284e0:	b.ne	2857c <__gmpn_mul_fft@@Base+0x12e8>  // b.any
   284e4:	ldr	x1, [x19]
   284e8:	add	x22, x20, #0x1
   284ec:	mov	x0, x21
   284f0:	mov	x2, x22
   284f4:	bl	cc10 <__gmpn_copyi@plt>
   284f8:	ldp	x0, x2, [x19]
   284fc:	mov	x3, x22
   28500:	mov	x1, x0
   28504:	bl	cc30 <__gmpn_add_n@plt>
   28508:	ldr	x0, [x19, #8]
   2850c:	mov	x1, x21
   28510:	mov	x3, x22
   28514:	mov	x2, x0
   28518:	bl	c420 <__gmpn_sub_n@plt>
   2851c:	ldr	x1, [x19]
   28520:	mov	x21, x0
   28524:	ldr	x8, [x1, x20, lsl #3]
   28528:	cmp	x8, #0x2
   2852c:	b.cc	28550 <__gmpn_mul_fft@@Base+0x12bc>  // b.lo, b.ul, b.last
   28530:	sub	x3, x8, #0x1
   28534:	mov	x0, x1
   28538:	mov	x2, x20
   2853c:	bl	caf0 <__gmpn_sub_1@plt>
   28540:	ldr	x8, [x19]
   28544:	mov	w9, #0x1                   	// #1
   28548:	sub	x9, x9, x0
   2854c:	str	x9, [x8, x20, lsl #3]
   28550:	cbz	x21, 28620 <__gmpn_mul_fft@@Base+0x138c>
   28554:	ldr	x0, [x19, #8]
   28558:	lsl	x21, x20, #3
   2855c:	mov	x2, x20
   28560:	ldr	x8, [x0, x21]
   28564:	mov	x1, x0
   28568:	neg	x3, x8
   2856c:	bl	c150 <__gmpn_add_1@plt>
   28570:	ldr	x8, [x19, #8]
   28574:	str	x0, [x8, x21]
   28578:	b	28620 <__gmpn_mul_fft@@Base+0x138c>
   2857c:	asr	x23, x1, #1
   28580:	lsl	x26, x2, #1
   28584:	mov	x22, x2
   28588:	mov	x25, x1
   2858c:	mov	x0, x19
   28590:	mov	x1, x23
   28594:	mov	x2, x26
   28598:	mov	x3, x20
   2859c:	mov	x4, x21
   285a0:	bl	284b4 <__gmpn_mul_fft@@Base+0x1220>
   285a4:	add	x24, x19, x23, lsl #3
   285a8:	mov	x0, x24
   285ac:	mov	x1, x23
   285b0:	mov	x2, x26
   285b4:	mov	x3, x20
   285b8:	mov	x4, x21
   285bc:	bl	284b4 <__gmpn_mul_fft@@Base+0x1220>
   285c0:	cmp	x25, #0x2
   285c4:	b.lt	28620 <__gmpn_mul_fft@@Base+0x138c>  // b.tstop
   285c8:	mov	x25, xzr
   285cc:	mov	x26, xzr
   285d0:	lsl	x27, x26, #3
   285d4:	ldr	x1, [x24, x27]
   285d8:	mov	x0, x21
   285dc:	mov	x2, x25
   285e0:	mov	x3, x20
   285e4:	bl	27c40 <__gmpn_mul_fft@@Base+0x9ac>
   285e8:	ldr	x0, [x24, x27]
   285ec:	ldr	x1, [x19, x27]
   285f0:	mov	x2, x21
   285f4:	mov	x3, x20
   285f8:	bl	2874c <__gmpn_mul_fft@@Base+0x14b8>
   285fc:	ldr	x0, [x19, x27]
   28600:	mov	x2, x21
   28604:	mov	x3, x20
   28608:	mov	x1, x0
   2860c:	bl	287bc <__gmpn_mul_fft@@Base+0x1528>
   28610:	add	x26, x26, #0x1
   28614:	cmp	x26, x23
   28618:	add	x25, x25, x22
   2861c:	b.lt	285d0 <__gmpn_mul_fft@@Base+0x133c>  // b.tstop
   28620:	ldp	x20, x19, [sp, #80]
   28624:	ldp	x22, x21, [sp, #64]
   28628:	ldp	x24, x23, [sp, #48]
   2862c:	ldp	x26, x25, [sp, #32]
   28630:	ldr	x27, [sp, #16]
   28634:	ldp	x29, x30, [sp], #96
   28638:	ret
   2863c:	stp	x29, x30, [sp, #-32]!
   28640:	lsl	x8, x3, #7
   28644:	sub	x2, x8, x2
   28648:	stp	x20, x19, [sp, #16]
   2864c:	mov	x29, sp
   28650:	mov	x19, x3
   28654:	mov	x20, x0
   28658:	bl	27c40 <__gmpn_mul_fft@@Base+0x9ac>
   2865c:	mov	x0, x20
   28660:	mov	x1, x19
   28664:	bl	2882c <__gmpn_mul_fft@@Base+0x1598>
   28668:	ldp	x20, x19, [sp, #16]
   2866c:	ldp	x29, x30, [sp], #32
   28670:	ret
   28674:	stp	x29, x30, [sp, #-64]!
   28678:	lsl	x8, x1, #1
   2867c:	stp	x22, x21, [sp, #32]
   28680:	sub	x22, x3, x8
   28684:	stp	x20, x19, [sp, #48]
   28688:	mov	x21, x2
   2868c:	mov	x19, x1
   28690:	cmp	x22, #0x1
   28694:	mov	x20, x0
   28698:	str	x23, [sp, #16]
   2869c:	mov	x29, sp
   286a0:	b.lt	286dc <__gmpn_mul_fft@@Base+0x1448>  // b.tstop
   286a4:	add	x2, x21, x8, lsl #3
   286a8:	mov	x0, x20
   286ac:	mov	x1, x21
   286b0:	mov	x3, x22
   286b4:	bl	cc30 <__gmpn_add_n@plt>
   286b8:	lsl	x8, x22, #3
   286bc:	mov	x3, x0
   286c0:	add	x0, x20, x8
   286c4:	add	x1, x21, x8
   286c8:	sub	x2, x19, x22
   286cc:	bl	c150 <__gmpn_add_1@plt>
   286d0:	mov	x23, x0
   286d4:	mov	x22, x19
   286d8:	b	286f4 <__gmpn_mul_fft@@Base+0x1460>
   286dc:	mov	x0, x20
   286e0:	mov	x1, x21
   286e4:	mov	x2, x19
   286e8:	sub	x22, x3, x19
   286ec:	bl	cc10 <__gmpn_copyi@plt>
   286f0:	mov	x23, xzr
   286f4:	add	x2, x21, x19, lsl #3
   286f8:	mov	x0, x20
   286fc:	mov	x1, x20
   28700:	mov	x3, x22
   28704:	bl	c420 <__gmpn_sub_n@plt>
   28708:	mov	x3, x0
   2870c:	add	x0, x20, x22, lsl #3
   28710:	sub	x2, x19, x22
   28714:	mov	x1, x0
   28718:	bl	caf0 <__gmpn_sub_1@plt>
   2871c:	subs	x0, x23, x0
   28720:	b.pl	28738 <__gmpn_mul_fft@@Base+0x14a4>  // b.nfrst
   28724:	mov	w3, #0x1                   	// #1
   28728:	mov	x0, x20
   2872c:	mov	x1, x20
   28730:	mov	x2, x19
   28734:	bl	c150 <__gmpn_add_1@plt>
   28738:	ldp	x20, x19, [sp, #48]
   2873c:	ldp	x22, x21, [sp, #32]
   28740:	ldr	x23, [sp, #16]
   28744:	ldp	x29, x30, [sp], #64
   28748:	ret
   2874c:	stp	x29, x30, [sp, #-48]!
   28750:	stp	x20, x19, [sp, #32]
   28754:	lsl	x20, x3, #3
   28758:	ldr	x8, [x1, x20]
   2875c:	ldr	x9, [x2, x20]
   28760:	str	x21, [sp, #16]
   28764:	mov	x29, sp
   28768:	mov	x19, x0
   2876c:	sub	x21, x8, x9
   28770:	bl	c420 <__gmpn_sub_n@plt>
   28774:	sub	x8, x21, x0
   28778:	neg	x9, x8
   2877c:	and	x9, x9, x8, asr #63
   28780:	add	x8, x9, x8
   28784:	str	x8, [x19, x20]
   28788:	ldr	x8, [x19]
   2878c:	adds	x8, x8, x9
   28790:	str	x8, [x19]
   28794:	b.cc	287ac <__gmpn_mul_fft@@Base+0x1518>  // b.lo, b.ul, b.last
   28798:	add	x8, x19, #0x8
   2879c:	ldr	x9, [x8]
   287a0:	adds	x9, x9, #0x1
   287a4:	str	x9, [x8], #8
   287a8:	b.cs	2879c <__gmpn_mul_fft@@Base+0x1508>  // b.hs, b.nlast
   287ac:	ldp	x20, x19, [sp, #32]
   287b0:	ldr	x21, [sp, #16]
   287b4:	ldp	x29, x30, [sp], #48
   287b8:	ret
   287bc:	stp	x29, x30, [sp, #-48]!
   287c0:	stp	x20, x19, [sp, #32]
   287c4:	lsl	x20, x3, #3
   287c8:	ldr	x8, [x1, x20]
   287cc:	ldr	x9, [x2, x20]
   287d0:	str	x21, [sp, #16]
   287d4:	mov	x29, sp
   287d8:	mov	x19, x0
   287dc:	add	x21, x9, x8
   287e0:	bl	cc30 <__gmpn_add_n@plt>
   287e4:	adds	x8, x21, x0
   287e8:	sub	x9, x8, #0x1
   287ec:	csel	x9, xzr, x9, eq  // eq = none
   287f0:	sub	x8, x8, x9
   287f4:	str	x8, [x19, x20]
   287f8:	ldr	x8, [x19]
   287fc:	subs	x8, x8, x9
   28800:	str	x8, [x19]
   28804:	b.cs	2881c <__gmpn_mul_fft@@Base+0x1588>  // b.hs, b.nlast
   28808:	add	x8, x19, #0x8
   2880c:	ldr	x9, [x8]
   28810:	sub	x10, x9, #0x1
   28814:	str	x10, [x8], #8
   28818:	cbz	x9, 2880c <__gmpn_mul_fft@@Base+0x1578>
   2881c:	ldp	x20, x19, [sp, #32]
   28820:	ldr	x21, [sp, #16]
   28824:	ldp	x29, x30, [sp], #48
   28828:	ret
   2882c:	stp	x29, x30, [sp, #-32]!
   28830:	stp	x20, x19, [sp, #16]
   28834:	ldr	x8, [x0, x1, lsl #3]
   28838:	mov	x29, sp
   2883c:	cbz	x8, 28888 <__gmpn_mul_fft@@Base+0x15f4>
   28840:	mov	x19, x1
   28844:	mov	x20, x0
   28848:	mov	x8, x0
   2884c:	ldr	x9, [x8]
   28850:	sub	x10, x9, #0x1
   28854:	str	x10, [x8], #8
   28858:	cbz	x9, 2884c <__gmpn_mul_fft@@Base+0x15b8>
   2885c:	ldr	x8, [x20, x19, lsl #3]
   28860:	cbz	x8, 2886c <__gmpn_mul_fft@@Base+0x15d8>
   28864:	mov	x8, xzr
   28868:	b	28884 <__gmpn_mul_fft@@Base+0x15f0>
   2886c:	cbz	x19, 28880 <__gmpn_mul_fft@@Base+0x15ec>
   28870:	lsl	x2, x19, #3
   28874:	mov	x0, x20
   28878:	mov	w1, wzr
   2887c:	bl	c780 <memset@plt>
   28880:	mov	w8, #0x1                   	// #1
   28884:	str	x8, [x20, x19, lsl #3]
   28888:	ldp	x20, x19, [sp, #16]
   2888c:	ldp	x29, x30, [sp], #32
   28890:	ret

0000000000028894 <__gmpn_mul_n@@Base>:
   28894:	stp	x29, x30, [sp, #-32]!
   28898:	stp	x28, x19, [sp, #16]
   2889c:	mov	x29, sp
   288a0:	sub	sp, sp, #0x720
   288a4:	mov	x4, x3
   288a8:	mov	x3, x2
   288ac:	cmp	x4, #0xd
   288b0:	mov	x19, sp
   288b4:	b.le	288ec <__gmpn_mul_n@@Base+0x58>
   288b8:	cmp	x4, #0x30
   288bc:	b.le	288f8 <__gmpn_mul_n@@Base+0x64>
   288c0:	cmp	x4, #0x51
   288c4:	b.le	28908 <__gmpn_mul_n@@Base+0x74>
   288c8:	cmp	x4, #0xac
   288cc:	b.le	28930 <__gmpn_mul_n@@Base+0x9c>
   288d0:	cmp	x4, #0xeb
   288d4:	b.le	28958 <__gmpn_mul_n@@Base+0xc4>
   288d8:	cmp	x4, #0xc7f
   288dc:	b.le	28974 <__gmpn_mul_n@@Base+0xe0>
   288e0:	mov	x2, x4
   288e4:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   288e8:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   288ec:	mov	x2, x4
   288f0:	bl	c6e0 <__gmpn_mul_basecase@plt>
   288f4:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   288f8:	add	x5, x19, #0x20
   288fc:	mov	x2, x4
   28900:	bl	d630 <__gmpn_toom22_mul@plt>
   28904:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   28908:	mov	w8, #0x18                  	// #24
   2890c:	mul	x8, x4, x8
   28910:	add	x8, x8, #0x20f
   28914:	and	x8, x8, #0xfffffffffffffff0
   28918:	mov	x9, sp
   2891c:	sub	x5, x9, x8
   28920:	mov	sp, x5
   28924:	mov	x2, x4
   28928:	bl	c1e0 <__gmpn_toom33_mul@plt>
   2892c:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   28930:	mov	w8, #0x18                  	// #24
   28934:	mul	x8, x4, x8
   28938:	add	x8, x8, #0x20f
   2893c:	and	x8, x8, #0xfffffffffffffff0
   28940:	mov	x9, sp
   28944:	sub	x5, x9, x8
   28948:	mov	sp, x5
   2894c:	mov	x2, x4
   28950:	bl	c8b0 <__gmpn_toom44_mul@plt>
   28954:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   28958:	mov	x8, sp
   2895c:	sub	x8, x8, x4, lsl #4
   28960:	sub	x5, x8, #0xc00
   28964:	mov	sp, x5
   28968:	mov	x2, x4
   2896c:	bl	cde0 <__gmpn_toom6h_mul@plt>
   28970:	b	289b8 <__gmpn_mul_n@@Base+0x124>
   28974:	lsl	x8, x4, #4
   28978:	sub	x8, x8, x4
   2897c:	add	x8, x8, #0xcf0
   28980:	and	x8, x8, #0xfffffffffffffff8
   28984:	mov	w9, #0x7f00                	// #32512
   28988:	cmp	x8, x9
   2898c:	str	xzr, [x19, #32]
   28990:	b.hi	289c8 <__gmpn_mul_n@@Base+0x134>  // b.pmore
   28994:	add	x8, x8, #0xf
   28998:	mov	x9, sp
   2899c:	and	x8, x8, #0xfffffffffffffff0
   289a0:	sub	x5, x9, x8
   289a4:	mov	sp, x5
   289a8:	mov	x2, x4
   289ac:	bl	cce0 <__gmpn_toom8h_mul@plt>
   289b0:	ldr	x0, [x19, #32]
   289b4:	cbnz	x0, 289f0 <__gmpn_mul_n@@Base+0x15c>
   289b8:	mov	sp, x29
   289bc:	ldp	x28, x19, [sp, #16]
   289c0:	ldp	x29, x30, [sp], #32
   289c4:	ret
   289c8:	stp	x1, x0, [x19, #16]
   289cc:	add	x0, x19, #0x20
   289d0:	mov	x1, x8
   289d4:	stp	x3, x4, [x19]
   289d8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   289dc:	ldp	x4, x1, [x19, #8]
   289e0:	ldr	x3, [x19]
   289e4:	mov	x5, x0
   289e8:	ldr	x0, [x19, #24]
   289ec:	b	289a8 <__gmpn_mul_n@@Base+0x114>
   289f0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   289f4:	b	289b8 <__gmpn_mul_n@@Base+0x124>

00000000000289f8 <__gmpn_sqr@@Base>:
   289f8:	stp	x29, x30, [sp, #-32]!
   289fc:	stp	x28, x19, [sp, #16]
   28a00:	mov	x29, sp
   28a04:	sub	sp, sp, #0x840
   28a08:	mov	x8, x1
   28a0c:	cmp	x2, #0x11
   28a10:	mov	x19, sp
   28a14:	b.le	28a54 <__gmpn_sqr@@Base+0x5c>
   28a18:	cmp	x2, #0x42
   28a1c:	b.le	28a60 <__gmpn_sqr@@Base+0x68>
   28a20:	cmp	x2, #0xa5
   28a24:	b.le	28a70 <__gmpn_sqr@@Base+0x78>
   28a28:	cmp	x2, #0xdd
   28a2c:	b.le	28a98 <__gmpn_sqr@@Base+0xa0>
   28a30:	cmp	x2, #0x14c
   28a34:	b.le	28ac0 <__gmpn_sqr@@Base+0xc8>
   28a38:	cmp	x2, #0xa7f
   28a3c:	b.le	28adc <__gmpn_sqr@@Base+0xe4>
   28a40:	mov	x1, x8
   28a44:	mov	x3, x8
   28a48:	mov	x4, x2
   28a4c:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   28a50:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28a54:	mov	x1, x8
   28a58:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   28a5c:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28a60:	add	x3, x19, #0x20
   28a64:	mov	x1, x8
   28a68:	bl	c190 <__gmpn_toom2_sqr@plt>
   28a6c:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28a70:	mov	w9, #0x18                  	// #24
   28a74:	mul	x9, x2, x9
   28a78:	add	x9, x9, #0x20f
   28a7c:	and	x9, x9, #0xfffffffffffffff0
   28a80:	mov	x10, sp
   28a84:	sub	x3, x10, x9
   28a88:	mov	sp, x3
   28a8c:	mov	x1, x8
   28a90:	bl	d480 <__gmpn_toom3_sqr@plt>
   28a94:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28a98:	mov	w9, #0x18                  	// #24
   28a9c:	mul	x9, x2, x9
   28aa0:	add	x9, x9, #0x20f
   28aa4:	and	x9, x9, #0xfffffffffffffff0
   28aa8:	mov	x10, sp
   28aac:	sub	x3, x10, x9
   28ab0:	mov	sp, x3
   28ab4:	mov	x1, x8
   28ab8:	bl	c370 <__gmpn_toom4_sqr@plt>
   28abc:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28ac0:	mov	x9, sp
   28ac4:	sub	x9, x9, x2, lsl #4
   28ac8:	sub	x3, x9, #0xc00
   28acc:	mov	sp, x3
   28ad0:	mov	x1, x8
   28ad4:	bl	d650 <__gmpn_toom6_sqr@plt>
   28ad8:	b	28b20 <__gmpn_sqr@@Base+0x128>
   28adc:	lsl	x9, x2, #4
   28ae0:	sub	x9, x9, x2
   28ae4:	add	x9, x9, #0xd50
   28ae8:	and	x1, x9, #0xfffffffffffffff8
   28aec:	mov	w9, #0x7f00                	// #32512
   28af0:	cmp	x1, x9
   28af4:	str	xzr, [x19, #32]
   28af8:	b.hi	28b30 <__gmpn_sqr@@Base+0x138>  // b.pmore
   28afc:	add	x10, x1, #0xf
   28b00:	mov	x9, sp
   28b04:	and	x10, x10, #0xfffffffffffffff0
   28b08:	sub	x3, x9, x10
   28b0c:	mov	sp, x3
   28b10:	mov	x1, x8
   28b14:	bl	d690 <__gmpn_toom8_sqr@plt>
   28b18:	ldr	x0, [x19, #32]
   28b1c:	cbnz	x0, 28b50 <__gmpn_sqr@@Base+0x158>
   28b20:	mov	sp, x29
   28b24:	ldp	x28, x19, [sp, #16]
   28b28:	ldp	x29, x30, [sp], #32
   28b2c:	ret
   28b30:	stp	x2, x0, [x19, #16]
   28b34:	add	x0, x19, #0x20
   28b38:	str	x8, [x19, #8]
   28b3c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28b40:	ldp	x8, x2, [x19, #8]
   28b44:	mov	x3, x0
   28b48:	ldr	x0, [x19, #24]
   28b4c:	b	28b10 <__gmpn_sqr@@Base+0x118>
   28b50:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   28b54:	b	28b20 <__gmpn_sqr@@Base+0x128>

0000000000028b58 <__gmpn_mul_basecase@@Base>:
   28b58:	stp	x29, x30, [sp, #-64]!
   28b5c:	stp	x22, x21, [sp, #32]
   28b60:	stp	x20, x19, [sp, #48]
   28b64:	str	x23, [sp, #16]
   28b68:	mov	x23, x3
   28b6c:	ldr	x3, [x3]
   28b70:	mov	x29, sp
   28b74:	mov	x22, x4
   28b78:	mov	x19, x2
   28b7c:	mov	x20, x1
   28b80:	mov	x21, x0
   28b84:	bl	d670 <__gmpn_mul_1@plt>
   28b88:	cmp	x22, #0x2
   28b8c:	str	x0, [x21, x19, lsl #3]
   28b90:	b.lt	28bc8 <__gmpn_mul_basecase@@Base+0x70>  // b.tstop
   28b94:	add	x21, x21, #0x8
   28b98:	add	x23, x23, #0x8
   28b9c:	add	x22, x22, #0x1
   28ba0:	ldr	x3, [x23], #8
   28ba4:	mov	x0, x21
   28ba8:	mov	x1, x20
   28bac:	mov	x2, x19
   28bb0:	bl	d5e0 <__gmpn_addmul_1@plt>
   28bb4:	sub	x22, x22, #0x1
   28bb8:	str	x0, [x21, x19, lsl #3]
   28bbc:	cmp	x22, #0x2
   28bc0:	add	x21, x21, #0x8
   28bc4:	b.gt	28ba0 <__gmpn_mul_basecase@@Base+0x48>
   28bc8:	ldp	x20, x19, [sp, #48]
   28bcc:	ldp	x22, x21, [sp, #32]
   28bd0:	ldr	x23, [sp, #16]
   28bd4:	ldp	x29, x30, [sp], #64
   28bd8:	ret

0000000000028bdc <__gmpn_sqr_basecase@@Base>:
   28bdc:	stp	x29, x30, [sp, #-64]!
   28be0:	stp	x24, x23, [sp, #16]
   28be4:	stp	x20, x19, [sp, #48]
   28be8:	mov	x19, x2
   28bec:	mov	x20, x1
   28bf0:	subs	x2, x2, #0x1
   28bf4:	mov	x23, x0
   28bf8:	stp	x22, x21, [sp, #32]
   28bfc:	mov	x29, sp
   28c00:	b.ne	28c18 <__gmpn_sqr_basecase@@Base+0x3c>  // b.any
   28c04:	ldr	x8, [x20]
   28c08:	umulh	x9, x8, x8
   28c0c:	mul	x8, x8, x8
   28c10:	stp	x8, x9, [x23]
   28c14:	b	28c98 <__gmpn_sqr_basecase@@Base+0xbc>
   28c18:	mov	x1, x20
   28c1c:	ldr	x3, [x1], #8
   28c20:	add	x22, x23, #0x8
   28c24:	mov	x0, x22
   28c28:	bl	d670 <__gmpn_mul_1@plt>
   28c2c:	subs	x21, x19, #0x2
   28c30:	str	x0, [x23, x19, lsl #3]
   28c34:	b.eq	28c7c <__gmpn_sqr_basecase@@Base+0xa0>  // b.none
   28c38:	add	x8, x23, x19, lsl #3
   28c3c:	mov	x24, xzr
   28c40:	add	x22, x23, #0x18
   28c44:	add	x23, x8, #0x8
   28c48:	add	x8, x20, x24
   28c4c:	ldr	x3, [x8, #8]
   28c50:	add	x1, x8, #0x10
   28c54:	mov	x0, x22
   28c58:	mov	x2, x21
   28c5c:	bl	d5e0 <__gmpn_addmul_1@plt>
   28c60:	str	x0, [x23, x24]
   28c64:	subs	x21, x21, #0x1
   28c68:	add	x22, x22, #0x10
   28c6c:	add	x24, x24, #0x8
   28c70:	b.ne	28c48 <__gmpn_sqr_basecase@@Base+0x6c>  // b.any
   28c74:	sub	x22, x22, #0x10
   28c78:	add	x20, x20, x24
   28c7c:	sub	x8, x22, x19, lsl #4
   28c80:	sub	x9, x20, x19, lsl #3
   28c84:	add	x0, x8, #0x18
   28c88:	add	x1, x8, #0x20
   28c8c:	add	x2, x9, #0x10
   28c90:	mov	x3, x19
   28c94:	bl	d510 <__gmpn_sqr_diag_addlsh1@plt>
   28c98:	ldp	x20, x19, [sp, #48]
   28c9c:	ldp	x22, x21, [sp, #32]
   28ca0:	ldp	x24, x23, [sp, #16]
   28ca4:	ldp	x29, x30, [sp], #64
   28ca8:	ret

0000000000028cac <__gmpn_nussbaumer_mul@@Base>:
   28cac:	stp	x29, x30, [sp, #-64]!
   28cb0:	stp	x24, x23, [sp, #16]
   28cb4:	stp	x22, x21, [sp, #32]
   28cb8:	stp	x20, x19, [sp, #48]
   28cbc:	mov	x29, sp
   28cc0:	sub	sp, sp, #0x10
   28cc4:	mov	x22, x4
   28cc8:	mov	x23, x3
   28ccc:	mov	x19, x2
   28cd0:	mov	x20, x1
   28cd4:	mov	x21, x0
   28cd8:	cmp	x1, x3
   28cdc:	stur	xzr, [x29, #-8]
   28ce0:	b.ne	28d3c <__gmpn_nussbaumer_mul@@Base+0x90>  // b.any
   28ce4:	cmp	x19, x22
   28ce8:	b.ne	28d3c <__gmpn_nussbaumer_mul@@Base+0x90>  // b.any
   28cec:	lsl	x0, x19, #1
   28cf0:	bl	c760 <__gmpn_sqrmod_bnm1_next_size@plt>
   28cf4:	mov	x1, x19
   28cf8:	mov	x22, x0
   28cfc:	bl	28ddc <__gmpn_nussbaumer_mul@@Base+0x130>
   28d00:	lsl	x1, x0, #3
   28d04:	mov	w8, #0x7f00                	// #32512
   28d08:	cmp	x1, x8
   28d0c:	b.hi	28dbc <__gmpn_nussbaumer_mul@@Base+0x110>  // b.pmore
   28d10:	add	x9, x1, #0xf
   28d14:	mov	x8, sp
   28d18:	and	x9, x9, #0xfffffffffffffff0
   28d1c:	sub	x4, x8, x9
   28d20:	mov	sp, x4
   28d24:	mov	x0, x21
   28d28:	mov	x1, x22
   28d2c:	mov	x2, x20
   28d30:	mov	x3, x19
   28d34:	bl	c080 <__gmpn_sqrmod_bnm1@plt>
   28d38:	b	28d94 <__gmpn_nussbaumer_mul@@Base+0xe8>
   28d3c:	add	x0, x22, x19
   28d40:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   28d44:	mov	x1, x19
   28d48:	mov	x2, x22
   28d4c:	mov	x24, x0
   28d50:	bl	28df0 <__gmpn_nussbaumer_mul@@Base+0x144>
   28d54:	lsl	x1, x0, #3
   28d58:	mov	w8, #0x7f00                	// #32512
   28d5c:	cmp	x1, x8
   28d60:	b.hi	28dcc <__gmpn_nussbaumer_mul@@Base+0x120>  // b.pmore
   28d64:	add	x9, x1, #0xf
   28d68:	mov	x8, sp
   28d6c:	and	x9, x9, #0xfffffffffffffff0
   28d70:	sub	x6, x8, x9
   28d74:	mov	sp, x6
   28d78:	mov	x0, x21
   28d7c:	mov	x1, x24
   28d80:	mov	x2, x20
   28d84:	mov	x3, x19
   28d88:	mov	x4, x23
   28d8c:	mov	x5, x22
   28d90:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   28d94:	ldur	x0, [x29, #-8]
   28d98:	cbnz	x0, 28db4 <__gmpn_nussbaumer_mul@@Base+0x108>
   28d9c:	mov	sp, x29
   28da0:	ldp	x20, x19, [sp, #48]
   28da4:	ldp	x22, x21, [sp, #32]
   28da8:	ldp	x24, x23, [sp, #16]
   28dac:	ldp	x29, x30, [sp], #64
   28db0:	ret
   28db4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   28db8:	b	28d9c <__gmpn_nussbaumer_mul@@Base+0xf0>
   28dbc:	sub	x0, x29, #0x8
   28dc0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28dc4:	mov	x4, x0
   28dc8:	b	28d24 <__gmpn_nussbaumer_mul@@Base+0x78>
   28dcc:	sub	x0, x29, #0x8
   28dd0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   28dd4:	mov	x6, x0
   28dd8:	b	28d78 <__gmpn_nussbaumer_mul@@Base+0xcc>
   28ddc:	cmp	x1, x0, asr #1
   28de0:	csel	x8, x1, xzr, gt
   28de4:	add	x8, x0, x8
   28de8:	add	x0, x8, #0x3
   28dec:	ret
   28df0:	asr	x8, x0, #1
   28df4:	cmp	x8, x2
   28df8:	csel	x9, x0, x8, lt  // lt = tstop
   28dfc:	cmp	x8, x1
   28e00:	csel	x8, x9, xzr, lt  // lt = tstop
   28e04:	add	x8, x0, x8
   28e08:	add	x0, x8, #0x4
   28e0c:	ret

0000000000028e10 <__gmpn_mulmid_basecase@@Base>:
   28e10:	stp	x29, x30, [sp, #-80]!
   28e14:	stp	x24, x23, [sp, #32]
   28e18:	stp	x22, x21, [sp, #48]
   28e1c:	stp	x20, x19, [sp, #64]
   28e20:	mov	x22, x3
   28e24:	ldr	x3, [x3]
   28e28:	sub	x24, x4, #0x1
   28e2c:	sub	x20, x2, x24
   28e30:	mov	x23, x1
   28e34:	add	x1, x1, x24, lsl #3
   28e38:	mov	x2, x20
   28e3c:	str	x25, [sp, #16]
   28e40:	mov	x29, sp
   28e44:	mov	x21, x4
   28e48:	mov	x19, x0
   28e4c:	bl	d670 <__gmpn_mul_1@plt>
   28e50:	mov	x25, xzr
   28e54:	cbz	x24, 28e9c <__gmpn_mulmid_basecase@@Base+0x8c>
   28e58:	add	x8, x23, x21, lsl #3
   28e5c:	sub	x21, x8, #0x10
   28e60:	add	x22, x22, #0x8
   28e64:	mov	x23, x0
   28e68:	ldr	x3, [x22], #8
   28e6c:	mov	x0, x19
   28e70:	mov	x1, x21
   28e74:	mov	x2, x20
   28e78:	bl	d5e0 <__gmpn_addmul_1@plt>
   28e7c:	mov	x8, x0
   28e80:	mov	x9, xzr
   28e84:	adds	x0, x23, x8
   28e88:	adc	x25, x25, x9
   28e8c:	sub	x24, x24, #0x1
   28e90:	sub	x21, x21, #0x8
   28e94:	mov	x23, x0
   28e98:	cbnz	x24, 28e68 <__gmpn_mulmid_basecase@@Base+0x58>
   28e9c:	add	x8, x19, x20, lsl #3
   28ea0:	stp	x0, x25, [x8]
   28ea4:	ldp	x20, x19, [sp, #64]
   28ea8:	ldp	x22, x21, [sp, #48]
   28eac:	ldp	x24, x23, [sp, #32]
   28eb0:	ldr	x25, [sp, #16]
   28eb4:	ldp	x29, x30, [sp], #80
   28eb8:	ret

0000000000028ebc <__gmpn_toom42_mulmid@@Base>:
   28ebc:	sub	sp, sp, #0x120
   28ec0:	cmp	x3, #0x0
   28ec4:	stp	x26, x25, [sp, #224]
   28ec8:	cinc	x26, x3, lt  // lt = tstop
   28ecc:	stp	x24, x23, [sp, #240]
   28ed0:	and	x8, x3, #0x1
   28ed4:	asr	x24, x26, #1
   28ed8:	stp	x28, x27, [sp, #208]
   28edc:	stp	x22, x21, [sp, #256]
   28ee0:	add	x28, x1, x8, lsl #3
   28ee4:	lsl	x22, x24, #3
   28ee8:	stp	x20, x19, [sp, #272]
   28eec:	add	x20, x4, #0x10
   28ef0:	add	x23, x28, x22
   28ef4:	add	x25, x2, x22
   28ef8:	mov	x19, x2
   28efc:	mov	x21, x0
   28f00:	str	x4, [sp, #40]
   28f04:	stp	x8, x3, [sp, #56]
   28f08:	sub	x5, x24, #0x1
   28f0c:	add	x3, sp, #0x58
   28f10:	mov	x0, x20
   28f14:	mov	x1, x28
   28f18:	mov	x2, x23
   28f1c:	mov	x4, x25
   28f20:	mov	x6, xzr
   28f24:	stp	x29, x30, [sp, #192]
   28f28:	add	x29, sp, #0xc0
   28f2c:	bl	cec0 <__gmpn_add_err1_n@plt>
   28f30:	add	x8, x20, x22
   28f34:	lsl	x27, x24, #4
   28f38:	mov	x7, x0
   28f3c:	stp	x8, x23, [sp, #16]
   28f40:	sub	x0, x8, #0x8
   28f44:	add	x8, x28, x27
   28f48:	sub	x1, x23, #0x8
   28f4c:	and	x23, x26, #0xfffffffffffffffe
   28f50:	sub	x26, x8, #0x8
   28f54:	add	x8, sp, #0x58
   28f58:	add	x3, x8, #0x10
   28f5c:	mov	x2, x26
   28f60:	mov	x4, x25
   28f64:	mov	x5, x19
   28f68:	mov	x6, x24
   28f6c:	str	x1, [sp, #32]
   28f70:	bl	c7c0 <__gmpn_add_err2_n@plt>
   28f74:	add	x8, x20, x27
   28f78:	add	x27, x23, x24
   28f7c:	mov	x6, x0
   28f80:	str	x20, [sp, #72]
   28f84:	sub	x0, x8, #0x8
   28f88:	add	x8, x28, x27, lsl #3
   28f8c:	add	x20, sp, #0x58
   28f90:	sub	x2, x8, #0x8
   28f94:	add	x3, x20, #0x30
   28f98:	mov	x1, x26
   28f9c:	mov	x4, x19
   28fa0:	mov	x5, x24
   28fa4:	str	x23, [sp, #48]
   28fa8:	str	x28, [sp, #8]
   28fac:	bl	cec0 <__gmpn_add_err1_n@plt>
   28fb0:	mov	x0, x25
   28fb4:	mov	x1, x19
   28fb8:	mov	x2, x24
   28fbc:	bl	c570 <__gmpn_cmp@plt>
   28fc0:	add	x8, x21, x22
   28fc4:	mov	x28, x8
   28fc8:	add	x8, x8, #0x10
   28fcc:	add	x3, x20, #0x40
   28fd0:	str	x8, [sp, #80]
   28fd4:	tbnz	w0, #31, 29000 <__gmpn_toom42_mulmid@@Base+0x144>
   28fd8:	ldr	x4, [sp, #32]
   28fdc:	mov	x0, x8
   28fe0:	mov	x1, x25
   28fe4:	mov	x2, x19
   28fe8:	mov	x5, x26
   28fec:	mov	x6, x24
   28ff0:	mov	x7, xzr
   28ff4:	bl	d550 <__gmpn_sub_err2_n@plt>
   28ff8:	str	wzr, [sp, #32]
   28ffc:	b	29028 <__gmpn_toom42_mulmid@@Base+0x16c>
   29000:	ldr	x4, [sp, #32]
   29004:	mov	x0, x8
   29008:	mov	x1, x19
   2900c:	mov	x2, x25
   29010:	mov	x5, x26
   29014:	mov	x6, x24
   29018:	mov	x7, xzr
   2901c:	bl	d550 <__gmpn_sub_err2_n@plt>
   29020:	mov	w8, #0x1                   	// #1
   29024:	str	w8, [sp, #32]
   29028:	ldr	x26, [sp, #64]
   2902c:	ldr	x23, [sp, #40]
   29030:	cmp	x26, #0x27
   29034:	b.gt	290a8 <__gmpn_toom42_mulmid@@Base+0x1ec>
   29038:	ldr	x27, [sp, #48]
   2903c:	ldr	x1, [sp, #72]
   29040:	mov	x0, x21
   29044:	mov	x3, x25
   29048:	sub	x20, x27, #0x1
   2904c:	mov	x2, x20
   29050:	mov	x4, x24
   29054:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   29058:	add	x8, x21, x24, lsl #3
   2905c:	ldp	x9, x8, [x8]
   29060:	ldp	x10, x11, [sp, #104]
   29064:	ldr	x1, [sp, #24]
   29068:	ldr	x3, [sp, #80]
   2906c:	mov	x0, x23
   29070:	adds	x9, x9, x10
   29074:	cinc	x8, x8, cs  // cs = hs, nlast
   29078:	add	x8, x8, x11
   2907c:	mov	x2, x20
   29080:	mov	x4, x24
   29084:	stp	x9, x8, [sp, #104]
   29088:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   2908c:	ldr	x1, [sp, #16]
   29090:	mov	x0, x28
   29094:	mov	x2, x20
   29098:	mov	x3, x19
   2909c:	mov	x4, x24
   290a0:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   290a4:	b	29118 <__gmpn_toom42_mulmid@@Base+0x25c>
   290a8:	ldr	x1, [sp, #72]
   290ac:	add	x8, x23, x27, lsl #3
   290b0:	add	x20, x8, #0x8
   290b4:	mov	x0, x21
   290b8:	mov	x2, x25
   290bc:	mov	x3, x24
   290c0:	mov	x4, x20
   290c4:	bl	c930 <__gmpn_toom42_mulmid@plt>
   290c8:	add	x8, x21, x24, lsl #3
   290cc:	ldp	x9, x8, [x8]
   290d0:	ldp	x10, x11, [sp, #104]
   290d4:	ldr	x1, [sp, #24]
   290d8:	ldr	x2, [sp, #80]
   290dc:	mov	x0, x23
   290e0:	adds	x9, x9, x10
   290e4:	cinc	x8, x8, cs  // cs = hs, nlast
   290e8:	add	x8, x8, x11
   290ec:	mov	x3, x24
   290f0:	mov	x4, x20
   290f4:	stp	x9, x8, [sp, #104]
   290f8:	bl	c930 <__gmpn_toom42_mulmid@plt>
   290fc:	ldr	x1, [sp, #16]
   29100:	mov	x0, x28
   29104:	mov	x2, x19
   29108:	mov	x3, x24
   2910c:	mov	x4, x20
   29110:	bl	c930 <__gmpn_toom42_mulmid@plt>
   29114:	ldr	x27, [sp, #48]
   29118:	ldr	x8, [sp, #88]
   2911c:	ldp	x9, x10, [x21]
   29120:	subs	x8, x9, x8
   29124:	str	x8, [x21]
   29128:	ldr	x8, [sp, #96]
   2912c:	cinc	x8, x8, cc  // cc = lo, ul, last
   29130:	subs	x8, x10, x8
   29134:	str	x8, [x21, #8]
   29138:	b.cc	29328 <__gmpn_toom42_mulmid@@Base+0x46c>  // b.lo, b.ul, b.last
   2913c:	ldp	x10, x8, [sp, #112]
   29140:	ldr	x9, [sp, #104]
   29144:	ldr	x11, [x21, x22]
   29148:	ldr	x12, [sp, #128]
   2914c:	subs	x8, x9, x8
   29150:	cset	w9, cc  // cc = lo, ul, last
   29154:	adds	x8, x11, x8
   29158:	add	x11, x24, #0x1
   2915c:	lsl	x20, x11, #3
   29160:	str	x8, [x21, x22]
   29164:	ldr	x8, [x21, x20]
   29168:	sub	x10, x10, x12
   2916c:	sub	x9, x10, x9
   29170:	cinc	x9, x9, cs  // cs = hs, nlast
   29174:	adds	x10, x9, x8
   29178:	asr	x8, x9, #63
   2917c:	cinc	x8, x8, cs  // cs = hs, nlast
   29180:	str	x10, [x21, x20]
   29184:	cbnz	x8, 29348 <__gmpn_toom42_mulmid@@Base+0x48c>
   29188:	lsl	x8, x27, #3
   2918c:	ldr	x9, [sp, #136]
   29190:	ldr	x10, [x21, x8]
   29194:	adds	x9, x9, x10
   29198:	orr	x10, x8, #0x8
   2919c:	str	x9, [x21, x8]
   291a0:	ldr	x8, [x21, x10]
   291a4:	ldr	x9, [sp, #144]
   291a8:	add	x8, x9, x8
   291ac:	cinc	x8, x8, cs  // cs = hs, nlast
   291b0:	str	x8, [x21, x10]
   291b4:	ldr	x8, [sp, #152]
   291b8:	ldp	x9, x10, [x23]
   291bc:	adds	x8, x8, x9
   291c0:	str	x8, [x23]
   291c4:	ldr	x8, [sp, #160]
   291c8:	cinc	x9, x10, cs  // cs = hs, nlast
   291cc:	add	x8, x9, x8
   291d0:	cmp	x8, x10
   291d4:	str	x8, [x23, #8]
   291d8:	b.cc	29368 <__gmpn_toom42_mulmid@@Base+0x4ac>  // b.lo, b.ul, b.last
   291dc:	ldr	x8, [x23, x22]
   291e0:	ldr	x9, [sp, #168]
   291e4:	subs	x8, x8, x9
   291e8:	str	x8, [x23, x22]
   291ec:	ldr	x8, [x23, x20]
   291f0:	ldr	x9, [sp, #176]
   291f4:	cset	w10, cc  // cc = lo, ul, last
   291f8:	sub	x8, x8, x9
   291fc:	sub	x8, x8, x10
   29200:	str	x8, [x23, x20]
   29204:	lsr	x3, x8, #63
   29208:	ldr	w8, [sp, #32]
   2920c:	cbz	w8, 292dc <__gmpn_toom42_mulmid@@Base+0x420>
   29210:	ldr	x0, [sp, #80]
   29214:	mov	x2, x24
   29218:	mov	x1, x0
   2921c:	bl	caf0 <__gmpn_sub_1@plt>
   29220:	add	x20, x24, #0x2
   29224:	add	x2, x27, #0x2
   29228:	mov	x0, x21
   2922c:	mov	x1, x21
   29230:	mov	x3, x23
   29234:	mov	x4, x20
   29238:	bl	c970 <__gmpn_add@plt>
   2923c:	mov	x0, x28
   29240:	mov	x1, x28
   29244:	mov	x2, x23
   29248:	mov	x3, x20
   2924c:	bl	c420 <__gmpn_sub_n@plt>
   29250:	ldr	x8, [sp, #56]
   29254:	cbz	x8, 292bc <__gmpn_toom42_mulmid@@Base+0x400>
   29258:	sub	x20, x26, #0x1
   2925c:	ldr	x22, [sp, #8]
   29260:	ldr	x3, [x19, x20, lsl #3]
   29264:	mov	x0, x21
   29268:	mov	x2, x26
   2926c:	sub	x1, x22, #0x8
   29270:	bl	d5e0 <__gmpn_addmul_1@plt>
   29274:	lsl	x8, x26, #3
   29278:	mov	x3, x19
   2927c:	add	x19, x21, x8
   29280:	ldr	x9, [x19]
   29284:	add	x8, x22, x8
   29288:	sub	x1, x8, #0x8
   2928c:	mov	x2, x20
   29290:	adds	x9, x9, x0
   29294:	cset	w10, cs  // cs = hs, nlast
   29298:	add	x0, sp, #0x58
   2929c:	mov	x4, x20
   292a0:	stp	x9, x10, [x19]
   292a4:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   292a8:	sub	x0, x19, #0x8
   292ac:	add	x2, sp, #0x58
   292b0:	mov	w3, #0x3                   	// #3
   292b4:	mov	x1, x0
   292b8:	bl	cc30 <__gmpn_add_n@plt>
   292bc:	ldp	x20, x19, [sp, #272]
   292c0:	ldp	x22, x21, [sp, #256]
   292c4:	ldp	x24, x23, [sp, #240]
   292c8:	ldp	x26, x25, [sp, #224]
   292cc:	ldp	x28, x27, [sp, #208]
   292d0:	ldp	x29, x30, [sp, #192]
   292d4:	add	sp, sp, #0x120
   292d8:	ret
   292dc:	ldr	x0, [sp, #80]
   292e0:	mov	x2, x24
   292e4:	mov	x1, x0
   292e8:	bl	c150 <__gmpn_add_1@plt>
   292ec:	add	x20, x24, #0x2
   292f0:	add	x2, x27, #0x2
   292f4:	mov	x0, x21
   292f8:	mov	x1, x21
   292fc:	mov	x3, x23
   29300:	mov	x4, x20
   29304:	bl	d340 <__gmpn_sub@plt>
   29308:	mov	x0, x28
   2930c:	mov	x1, x28
   29310:	mov	x2, x23
   29314:	mov	x3, x20
   29318:	bl	cc30 <__gmpn_add_n@plt>
   2931c:	ldr	x8, [sp, #56]
   29320:	cbnz	x8, 29258 <__gmpn_toom42_mulmid@@Base+0x39c>
   29324:	b	292bc <__gmpn_toom42_mulmid@@Base+0x400>
   29328:	cmp	x26, #0x6
   2932c:	b.lt	29380 <__gmpn_toom42_mulmid@@Base+0x4c4>  // b.tstop
   29330:	add	x0, x21, #0x10
   29334:	sub	x2, x24, #0x2
   29338:	mov	w3, #0x1                   	// #1
   2933c:	mov	x1, x0
   29340:	bl	caf0 <__gmpn_sub_1@plt>
   29344:	b	29384 <__gmpn_toom42_mulmid@@Base+0x4c8>
   29348:	mov	w3, #0x1                   	// #1
   2934c:	cmp	x8, #0x1
   29350:	b.ne	293a0 <__gmpn_toom42_mulmid@@Base+0x4e4>  // b.any
   29354:	ldr	x0, [sp, #80]
   29358:	mov	x2, x24
   2935c:	mov	x1, x0
   29360:	bl	c150 <__gmpn_add_1@plt>
   29364:	b	29188 <__gmpn_toom42_mulmid@@Base+0x2cc>
   29368:	ldr	x0, [sp, #72]
   2936c:	mov	w3, #0x1                   	// #1
   29370:	mov	x2, x24
   29374:	mov	x1, x0
   29378:	bl	c150 <__gmpn_add_1@plt>
   2937c:	b	291dc <__gmpn_toom42_mulmid@@Base+0x320>
   29380:	mov	w0, #0x1                   	// #1
   29384:	ldp	x8, x9, [sp, #104]
   29388:	subs	x8, x8, x0
   2938c:	str	x8, [sp, #104]
   29390:	cset	w8, cc  // cc = lo, ul, last
   29394:	sub	x8, x9, x8
   29398:	str	x8, [sp, #112]
   2939c:	b	2913c <__gmpn_toom42_mulmid@@Base+0x280>
   293a0:	ldr	x0, [sp, #80]
   293a4:	mov	x2, x24
   293a8:	mov	x1, x0
   293ac:	bl	caf0 <__gmpn_sub_1@plt>
   293b0:	b	29188 <__gmpn_toom42_mulmid@@Base+0x2cc>

00000000000293b4 <__gmpn_mulmid_n@@Base>:
   293b4:	stp	x29, x30, [sp, #-48]!
   293b8:	stp	x22, x21, [sp, #16]
   293bc:	stp	x20, x19, [sp, #32]
   293c0:	mov	x29, sp
   293c4:	sub	sp, sp, #0x10
   293c8:	mov	x19, x3
   293cc:	mov	x20, x2
   293d0:	mov	x21, x1
   293d4:	cmp	x3, #0x13
   293d8:	mov	x22, x0
   293dc:	b.gt	29400 <__gmpn_mulmid_n@@Base+0x4c>
   293e0:	lsl	x8, x19, #1
   293e4:	sub	x2, x8, #0x1
   293e8:	mov	x0, x22
   293ec:	mov	x1, x21
   293f0:	mov	x3, x20
   293f4:	mov	x4, x19
   293f8:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   293fc:	b	2944c <__gmpn_mulmid_n@@Base+0x98>
   29400:	mov	w8, #0x18                  	// #24
   29404:	orr	x9, xzr, #0x200
   29408:	madd	x1, x19, x8, x9
   2940c:	mov	w8, #0x7f00                	// #32512
   29410:	cmp	x1, x8
   29414:	stur	xzr, [x29, #-8]
   29418:	b.hi	29460 <__gmpn_mulmid_n@@Base+0xac>  // b.pmore
   2941c:	add	x9, x1, #0xf
   29420:	mov	x8, sp
   29424:	and	x9, x9, #0xfffffffffffffff0
   29428:	sub	x4, x8, x9
   2942c:	mov	sp, x4
   29430:	mov	x0, x22
   29434:	mov	x1, x21
   29438:	mov	x2, x20
   2943c:	mov	x3, x19
   29440:	bl	c930 <__gmpn_toom42_mulmid@plt>
   29444:	ldur	x0, [x29, #-8]
   29448:	cbnz	x0, 29470 <__gmpn_mulmid_n@@Base+0xbc>
   2944c:	mov	sp, x29
   29450:	ldp	x20, x19, [sp, #32]
   29454:	ldp	x22, x21, [sp, #16]
   29458:	ldp	x29, x30, [sp], #48
   2945c:	ret
   29460:	sub	x0, x29, #0x8
   29464:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   29468:	mov	x4, x0
   2946c:	b	29430 <__gmpn_mulmid_n@@Base+0x7c>
   29470:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   29474:	b	2944c <__gmpn_mulmid_n@@Base+0x98>

0000000000029478 <__gmpn_mulmid@@Base>:
   29478:	stp	x29, x30, [sp, #-96]!
   2947c:	stp	x28, x27, [sp, #16]
   29480:	stp	x26, x25, [sp, #32]
   29484:	stp	x24, x23, [sp, #48]
   29488:	stp	x22, x21, [sp, #64]
   2948c:	stp	x20, x19, [sp, #80]
   29490:	mov	x29, sp
   29494:	sub	sp, sp, #0x30
   29498:	mov	x19, x4
   2949c:	mov	x22, x3
   294a0:	mov	x24, x2
   294a4:	mov	x20, x1
   294a8:	cmp	x4, #0x13
   294ac:	mov	x21, x0
   294b0:	b.gt	29570 <__gmpn_mulmid@@Base+0xf8>
   294b4:	cmp	x24, #0xdb
   294b8:	b.le	29584 <__gmpn_mulmid@@Base+0x10c>
   294bc:	mov	w8, #0xdd                  	// #221
   294c0:	mov	w2, #0xdc                  	// #220
   294c4:	mov	x0, x21
   294c8:	mov	x1, x20
   294cc:	mov	x3, x22
   294d0:	mov	x4, x19
   294d4:	sub	x25, x8, x19
   294d8:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   294dc:	sub	x23, x24, x25
   294e0:	cmp	x23, #0xdc
   294e4:	b.lt	296e0 <__gmpn_mulmid@@Base+0x268>  // b.tstop
   294e8:	lsl	x8, x19, #3
   294ec:	mov	w9, #0x6e8                 	// #1768
   294f0:	mov	w10, #0x6f8                 	// #1784
   294f4:	sub	x26, x9, x8
   294f8:	sub	x8, x10, x8
   294fc:	stur	x8, [x29, #-16]
   29500:	b	29514 <__gmpn_mulmid@@Base+0x9c>
   29504:	sub	x23, x23, x25
   29508:	cmp	x23, #0xdb
   2950c:	mov	x21, x24
   29510:	b.le	296e4 <__gmpn_mulmid@@Base+0x26c>
   29514:	add	x24, x21, x26
   29518:	ldp	x28, x27, [x24]
   2951c:	add	x20, x20, x25, lsl #3
   29520:	mov	w2, #0xdc                  	// #220
   29524:	mov	x0, x24
   29528:	mov	x1, x20
   2952c:	mov	x3, x22
   29530:	mov	x4, x19
   29534:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   29538:	ldp	x8, x9, [x24]
   2953c:	adds	x8, x8, x28
   29540:	str	x8, [x24]
   29544:	cinc	x8, x27, cs  // cs = hs, nlast
   29548:	adds	x8, x9, x8
   2954c:	str	x8, [x24, #8]
   29550:	b.cc	29504 <__gmpn_mulmid@@Base+0x8c>  // b.lo, b.ul, b.last
   29554:	ldur	x8, [x29, #-16]
   29558:	ldr	x9, [x21, x8]
   2955c:	adds	x9, x9, #0x1
   29560:	str	x9, [x21, x8]
   29564:	add	x8, x8, #0x8
   29568:	b.cs	29558 <__gmpn_mulmid@@Base+0xe0>  // b.hs, b.nlast
   2956c:	b	29504 <__gmpn_mulmid@@Base+0x8c>
   29570:	sub	x28, x24, x19
   29574:	cmp	x28, #0x13
   29578:	b.ge	295a0 <__gmpn_mulmid@@Base+0x128>  // b.tcont
   2957c:	cmp	x19, #0xdb
   29580:	b.gt	2974c <__gmpn_mulmid@@Base+0x2d4>
   29584:	mov	x0, x21
   29588:	mov	x1, x20
   2958c:	mov	x2, x24
   29590:	mov	x3, x22
   29594:	mov	x4, x19
   29598:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   2959c:	b	29820 <__gmpn_mulmid@@Base+0x3a8>
   295a0:	add	x23, x28, #0x1
   295a4:	subs	x26, x23, x19
   295a8:	b.ge	29840 <__gmpn_mulmid@@Base+0x3c8>  // b.tcont
   295ac:	add	x8, x23, x23, lsl #1
   295b0:	add	x8, x28, x8
   295b4:	lsl	x8, x8, #3
   295b8:	add	x1, x8, #0x218
   295bc:	mov	w8, #0x7f00                	// #32512
   295c0:	cmp	x1, x8
   295c4:	add	x8, x28, #0x3
   295c8:	stp	x8, xzr, [x29, #-16]
   295cc:	b.hi	29988 <__gmpn_mulmid@@Base+0x510>  // b.pmore
   295d0:	add	x9, x1, #0xf
   295d4:	mov	x8, sp
   295d8:	and	x9, x9, #0xfffffffffffffff0
   295dc:	sub	x26, x8, x9
   295e0:	mov	sp, x26
   295e4:	add	x8, x26, x23, lsl #3
   295e8:	sub	x27, x19, x23
   295ec:	add	x4, x8, #0x10
   295f0:	add	x2, x22, x27, lsl #3
   295f4:	mov	x0, x21
   295f8:	mov	x1, x20
   295fc:	mov	x3, x23
   29600:	mov	x25, x2
   29604:	stur	x4, [x29, #-40]
   29608:	bl	c930 <__gmpn_toom42_mulmid@plt>
   2960c:	cmp	x27, x28
   29610:	b.le	296ac <__gmpn_mulmid@@Base+0x234>
   29614:	lsl	x11, x24, #3
   29618:	lsl	x8, x19, #3
   2961c:	sub	x10, x11, x8
   29620:	mov	w9, #0x18                  	// #24
   29624:	add	x10, x10, #0x8
   29628:	ldur	x25, [x29, #-40]
   2962c:	sub	x8, x8, x11
   29630:	mul	x9, x19, x9
   29634:	stp	x10, x28, [x29, #-32]
   29638:	mov	x10, x24
   2963c:	sub	x24, x8, #0x8
   29640:	sub	x8, x9, x10, lsl #4
   29644:	sub	x28, x8, #0x10
   29648:	stur	x11, [x29, #-48]
   2964c:	ldur	x8, [x29, #-32]
   29650:	add	x2, x22, x28
   29654:	mov	x0, x26
   29658:	mov	x3, x23
   2965c:	add	x20, x20, x8
   29660:	mov	x1, x20
   29664:	mov	x4, x25
   29668:	bl	c930 <__gmpn_toom42_mulmid@plt>
   2966c:	ldur	x3, [x29, #-16]
   29670:	mov	x0, x21
   29674:	mov	x1, x21
   29678:	mov	x2, x26
   2967c:	bl	cc30 <__gmpn_add_n@plt>
   29680:	ldur	x8, [x29, #-24]
   29684:	sub	x27, x27, x23
   29688:	add	x22, x22, x24
   2968c:	cmp	x27, x8
   29690:	b.gt	2964c <__gmpn_mulmid@@Base+0x1d4>
   29694:	ldur	x9, [x29, #-48]
   29698:	lsl	x8, x19, #4
   2969c:	ldur	x28, [x29, #-24]
   296a0:	sub	x8, x8, x9
   296a4:	add	x8, x8, x22
   296a8:	sub	x25, x8, #0x8
   296ac:	ldur	x19, [x29, #-16]
   296b0:	cbz	x27, 29818 <__gmpn_mulmid@@Base+0x3a0>
   296b4:	add	x1, x20, x23, lsl #3
   296b8:	sub	x3, x25, x27, lsl #3
   296bc:	add	x2, x27, x28
   296c0:	mov	x0, x26
   296c4:	mov	x4, x27
   296c8:	bl	c890 <__gmpn_mulmid@plt>
   296cc:	mov	x0, x21
   296d0:	mov	x1, x21
   296d4:	mov	x2, x26
   296d8:	mov	x3, x19
   296dc:	b	29814 <__gmpn_mulmid@@Base+0x39c>
   296e0:	mov	x24, x21
   296e4:	cmp	x23, x19
   296e8:	b.lt	29820 <__gmpn_mulmid@@Base+0x3a8>  // b.tstop
   296ec:	lsl	x8, x25, #3
   296f0:	add	x21, x24, x8
   296f4:	ldp	x25, x26, [x21]
   296f8:	add	x1, x20, x8
   296fc:	mov	x0, x21
   29700:	mov	x2, x23
   29704:	mov	x3, x22
   29708:	mov	x4, x19
   2970c:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   29710:	ldp	x8, x9, [x21]
   29714:	adds	x8, x8, x25
   29718:	str	x8, [x21]
   2971c:	cinc	x8, x26, cs  // cs = hs, nlast
   29720:	adds	x8, x9, x8
   29724:	str	x8, [x21, #8]
   29728:	b.cc	29820 <__gmpn_mulmid@@Base+0x3a8>  // b.lo, b.ul, b.last
   2972c:	mov	w8, #0xdf                  	// #223
   29730:	sub	x8, x8, x19
   29734:	add	x8, x24, x8, lsl #3
   29738:	ldr	x9, [x8]
   2973c:	adds	x9, x9, #0x1
   29740:	str	x9, [x8], #8
   29744:	b.cs	29738 <__gmpn_mulmid@@Base+0x2c0>  // b.hs, b.nlast
   29748:	b	29820 <__gmpn_mulmid@@Base+0x3a8>
   2974c:	add	x23, x28, #0x3
   29750:	lsl	x1, x23, #3
   29754:	mov	w8, #0x7f00                	// #32512
   29758:	cmp	x1, x8
   2975c:	stur	xzr, [x29, #-8]
   29760:	b.hi	29998 <__gmpn_mulmid@@Base+0x520>  // b.pmore
   29764:	add	x9, x1, #0xf
   29768:	mov	x8, sp
   2976c:	and	x9, x9, #0xfffffffffffffff0
   29770:	sub	x25, x8, x9
   29774:	mov	sp, x25
   29778:	sub	x26, x19, #0xdc
   2977c:	add	x22, x22, x26, lsl #3
   29780:	sub	x24, x24, x26
   29784:	mov	w4, #0xdc                  	// #220
   29788:	mov	x0, x21
   2978c:	mov	x1, x20
   29790:	mov	x2, x24
   29794:	mov	x3, x22
   29798:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   2979c:	cmp	x19, #0x1b8
   297a0:	b.lt	297e8 <__gmpn_mulmid@@Base+0x370>  // b.tstop
   297a4:	add	x20, x20, #0x6e0
   297a8:	sub	x22, x22, #0x6e0
   297ac:	mov	w4, #0xdc                  	// #220
   297b0:	mov	x0, x25
   297b4:	mov	x1, x20
   297b8:	mov	x2, x24
   297bc:	mov	x3, x22
   297c0:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   297c4:	mov	x0, x21
   297c8:	mov	x1, x21
   297cc:	mov	x2, x25
   297d0:	mov	x3, x23
   297d4:	bl	cc30 <__gmpn_add_n@plt>
   297d8:	sub	x19, x19, #0xdc
   297dc:	cmp	x19, #0x1b7
   297e0:	b.gt	297a4 <__gmpn_mulmid@@Base+0x32c>
   297e4:	sub	x26, x19, #0xdc
   297e8:	cbz	x26, 29818 <__gmpn_mulmid@@Base+0x3a0>
   297ec:	add	x1, x20, #0x6e0
   297f0:	sub	x3, x22, x26, lsl #3
   297f4:	add	x2, x26, x28
   297f8:	mov	x0, x25
   297fc:	mov	x4, x26
   29800:	bl	c5d0 <__gmpn_mulmid_basecase@plt>
   29804:	mov	x0, x21
   29808:	mov	x1, x21
   2980c:	mov	x2, x25
   29810:	mov	x3, x23
   29814:	bl	cc30 <__gmpn_add_n@plt>
   29818:	ldur	x0, [x29, #-8]
   2981c:	cbnz	x0, 29980 <__gmpn_mulmid@@Base+0x508>
   29820:	mov	sp, x29
   29824:	ldp	x20, x19, [sp, #80]
   29828:	ldp	x22, x21, [sp, #64]
   2982c:	ldp	x24, x23, [sp, #48]
   29830:	ldp	x26, x25, [sp, #32]
   29834:	ldp	x28, x27, [sp, #16]
   29838:	ldp	x29, x30, [sp], #96
   2983c:	ret
   29840:	mov	w8, #0x18                  	// #24
   29844:	orr	x9, xzr, #0x200
   29848:	madd	x1, x19, x8, x9
   2984c:	mov	w8, #0x7f00                	// #32512
   29850:	cmp	x1, x8
   29854:	stur	xzr, [x29, #-8]
   29858:	b.hi	299a8 <__gmpn_mulmid@@Base+0x530>  // b.pmore
   2985c:	add	x9, x1, #0xf
   29860:	mov	x8, sp
   29864:	and	x9, x9, #0xfffffffffffffff0
   29868:	sub	x24, x8, x9
   2986c:	mov	sp, x24
   29870:	mov	x0, x21
   29874:	mov	x1, x20
   29878:	mov	x2, x22
   2987c:	mov	x3, x19
   29880:	mov	x4, x24
   29884:	bl	c930 <__gmpn_toom42_mulmid@plt>
   29888:	cmp	x26, x19
   2988c:	lsl	x27, x19, #3
   29890:	b.ge	298fc <__gmpn_mulmid@@Base+0x484>  // b.tcont
   29894:	mov	x24, x21
   29898:	ldur	x0, [x29, #-8]
   2989c:	cbnz	x0, 299b8 <__gmpn_mulmid@@Base+0x540>
   298a0:	cbz	x26, 29820 <__gmpn_mulmid@@Base+0x3a8>
   298a4:	add	x21, x24, x27
   298a8:	ldp	x25, x26, [x21]
   298ac:	add	x1, x20, x27
   298b0:	sub	x2, x23, #0x1
   298b4:	mov	x0, x21
   298b8:	mov	x3, x22
   298bc:	mov	x4, x19
   298c0:	bl	c890 <__gmpn_mulmid@plt>
   298c4:	ldp	x8, x9, [x21]
   298c8:	adds	x8, x8, x25
   298cc:	str	x8, [x21]
   298d0:	cinc	x8, x26, cs  // cs = hs, nlast
   298d4:	adds	x8, x9, x8
   298d8:	str	x8, [x21, #8]
   298dc:	b.cc	29820 <__gmpn_mulmid@@Base+0x3a8>  // b.lo, b.ul, b.last
   298e0:	add	x8, x24, x19, lsl #3
   298e4:	add	x8, x8, #0x10
   298e8:	ldr	x9, [x8]
   298ec:	adds	x9, x9, #0x1
   298f0:	str	x9, [x8], #8
   298f4:	b.cs	298e8 <__gmpn_mulmid@@Base+0x470>  // b.hs, b.nlast
   298f8:	b	29820 <__gmpn_mulmid@@Base+0x3a8>
   298fc:	add	x8, x27, #0x10
   29900:	stp	x8, x24, [x29, #-24]
   29904:	b	29918 <__gmpn_mulmid@@Base+0x4a0>
   29908:	sub	x26, x23, x19
   2990c:	cmp	x26, x19
   29910:	mov	x21, x24
   29914:	b.lt	29898 <__gmpn_mulmid@@Base+0x420>  // b.tstop
   29918:	add	x24, x21, x27
   2991c:	ldur	x4, [x29, #-16]
   29920:	ldr	x28, [x21, x27]
   29924:	ldr	x25, [x24, #8]
   29928:	add	x20, x20, x27
   2992c:	mov	x0, x24
   29930:	mov	x1, x20
   29934:	mov	x2, x22
   29938:	mov	x3, x19
   2993c:	mov	x23, x26
   29940:	bl	c930 <__gmpn_toom42_mulmid@plt>
   29944:	ldr	x8, [x21, x27]
   29948:	adds	x8, x8, x28
   2994c:	str	x8, [x21, x27]
   29950:	ldr	x8, [x24, #8]
   29954:	cinc	x9, x25, cs  // cs = hs, nlast
   29958:	adds	x8, x8, x9
   2995c:	str	x8, [x24, #8]
   29960:	b.cc	29908 <__gmpn_mulmid@@Base+0x490>  // b.lo, b.ul, b.last
   29964:	ldur	x8, [x29, #-24]
   29968:	ldr	x9, [x21, x8]
   2996c:	adds	x9, x9, #0x1
   29970:	str	x9, [x21, x8]
   29974:	add	x8, x8, #0x8
   29978:	b.cs	29968 <__gmpn_mulmid@@Base+0x4f0>  // b.hs, b.nlast
   2997c:	b	29908 <__gmpn_mulmid@@Base+0x490>
   29980:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   29984:	b	29820 <__gmpn_mulmid@@Base+0x3a8>
   29988:	sub	x0, x29, #0x8
   2998c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   29990:	mov	x26, x0
   29994:	b	295e4 <__gmpn_mulmid@@Base+0x16c>
   29998:	sub	x0, x29, #0x8
   2999c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   299a0:	mov	x25, x0
   299a4:	b	29778 <__gmpn_mulmid@@Base+0x300>
   299a8:	sub	x0, x29, #0x8
   299ac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   299b0:	mov	x24, x0
   299b4:	b	29870 <__gmpn_mulmid@@Base+0x3f8>
   299b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   299bc:	cbnz	x26, 298a4 <__gmpn_mulmid@@Base+0x42c>
   299c0:	b	29820 <__gmpn_mulmid@@Base+0x3a8>

00000000000299c4 <__gmpn_random@@Base>:
   299c4:	stp	x29, x30, [sp, #-48]!
   299c8:	str	x21, [sp, #16]
   299cc:	stp	x20, x19, [sp, #32]
   299d0:	mov	x29, sp
   299d4:	cbz	x1, 29a50 <__gmpn_random@@Base+0x8c>
   299d8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   299dc:	ldr	x8, [x8, #4040]
   299e0:	mov	x20, x1
   299e4:	mov	x21, x0
   299e8:	ldrb	w9, [x8]
   299ec:	cbnz	w9, 29a04 <__gmpn_random@@Base+0x40>
   299f0:	mov	w9, #0x1                   	// #1
   299f4:	strb	w9, [x8]
   299f8:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   299fc:	ldr	x0, [x0, #3976]
   29a00:	bl	c060 <__gmp_randinit_mt_noseed@plt>
   29a04:	adrp	x19, 69000 <__gmp_limbroots_table@@Base+0x11338>
   29a08:	ldr	x19, [x19, #3976]
   29a0c:	lsl	x2, x20, #6
   29a10:	mov	x1, x21
   29a14:	ldr	x8, [x19, #24]
   29a18:	mov	x0, x19
   29a1c:	ldr	x8, [x8, #8]
   29a20:	blr	x8
   29a24:	add	x20, x21, x20, lsl #3
   29a28:	ldr	x8, [x20, #-8]!
   29a2c:	cbnz	x8, 29a50 <__gmpn_random@@Base+0x8c>
   29a30:	ldr	x8, [x19, #24]
   29a34:	mov	w2, #0x40                  	// #64
   29a38:	mov	x0, x19
   29a3c:	mov	x1, x20
   29a40:	ldr	x8, [x8, #8]
   29a44:	blr	x8
   29a48:	ldr	x8, [x20]
   29a4c:	cbz	x8, 29a30 <__gmpn_random@@Base+0x6c>
   29a50:	ldp	x20, x19, [sp, #32]
   29a54:	ldr	x21, [sp, #16]
   29a58:	ldp	x29, x30, [sp], #48
   29a5c:	ret

0000000000029a60 <__gmpn_random2@@Base>:
   29a60:	sub	sp, sp, #0x30
   29a64:	stp	x29, x30, [sp, #16]
   29a68:	stp	x20, x19, [sp, #32]
   29a6c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   29a70:	ldr	x8, [x8, #4040]
   29a74:	mov	x19, x1
   29a78:	mov	x20, x0
   29a7c:	add	x29, sp, #0x10
   29a80:	ldrb	w9, [x8]
   29a84:	cbnz	w9, 29a9c <__gmpn_random2@@Base+0x3c>
   29a88:	mov	w9, #0x1                   	// #1
   29a8c:	strb	w9, [x8]
   29a90:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   29a94:	ldr	x0, [x0, #3976]
   29a98:	bl	c060 <__gmp_randinit_mt_noseed@plt>
   29a9c:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   29aa0:	ldr	x0, [x0, #3976]
   29aa4:	add	x1, sp, #0x8
   29aa8:	mov	w2, #0x20                  	// #32
   29aac:	ldr	x8, [x0, #24]
   29ab0:	ldr	x8, [x8, #8]
   29ab4:	blr	x8
   29ab8:	ldr	x8, [sp, #8]
   29abc:	lsl	x9, x19, #6
   29ac0:	mov	x0, x20
   29ac4:	and	x8, x8, #0x3f
   29ac8:	sub	x1, x9, x8
   29acc:	bl	29ae0 <__gmpn_random2@@Base+0x80>
   29ad0:	ldp	x20, x19, [sp, #32]
   29ad4:	ldp	x29, x30, [sp, #16]
   29ad8:	add	sp, sp, #0x30
   29adc:	ret
   29ae0:	stp	x29, x30, [sp, #-80]!
   29ae4:	stp	x20, x19, [sp, #64]
   29ae8:	mov	x20, x1
   29aec:	add	x9, x1, #0x3f
   29af0:	neg	w10, w20
   29af4:	mov	x11, #0xffffffffffffffff    	// #-1
   29af8:	lsr	x8, x9, #6
   29afc:	mov	x19, x0
   29b00:	lsr	x10, x11, x10
   29b04:	add	x11, x0, x8, lsl #3
   29b08:	cmp	x9, #0x80
   29b0c:	str	x25, [sp, #16]
   29b10:	stp	x24, x23, [sp, #32]
   29b14:	stp	x22, x21, [sp, #48]
   29b18:	mov	x29, sp
   29b1c:	stur	x10, [x11, #-8]
   29b20:	b.cc	29b4c <__gmpn_random2@@Base+0xec>  // b.lo, b.ul, b.last
   29b24:	cmp	x8, #0x2
   29b28:	mov	w9, #0x2                   	// #2
   29b2c:	csel	x9, x8, x9, cc  // cc = lo, ul, last
   29b30:	sub	x8, x8, x9
   29b34:	add	x10, x19, x9, lsl #3
   29b38:	lsl	x8, x8, #3
   29b3c:	sub	x0, x10, #0x10
   29b40:	add	x2, x8, #0x8
   29b44:	mov	w1, #0xff                  	// #255
   29b48:	bl	c780 <memset@plt>
   29b4c:	adrp	x21, 69000 <__gmp_limbroots_table@@Base+0x11338>
   29b50:	ldr	x21, [x21, #3976]
   29b54:	add	x1, x29, #0x18
   29b58:	mov	w2, #0x20                  	// #32
   29b5c:	ldr	x8, [x21, #24]
   29b60:	mov	x0, x21
   29b64:	ldr	x8, [x8, #8]
   29b68:	blr	x8
   29b6c:	ldr	x8, [x29, #24]
   29b70:	add	x22, x19, #0x8
   29b74:	mov	w24, #0x1                   	// #1
   29b78:	and	x8, x8, #0x3
   29b7c:	add	x8, x8, #0x1
   29b80:	udiv	x8, x20, x8
   29b84:	cmp	w8, #0x0
   29b88:	cinc	w23, w8, eq  // eq = none
   29b8c:	b	29b98 <__gmpn_random2@@Base+0x138>
   29b90:	cmp	x25, x8
   29b94:	b.ls	29c48 <__gmpn_random2@@Base+0x1e8>  // b.plast
   29b98:	ldr	x8, [x21, #24]
   29b9c:	add	x1, x29, #0x18
   29ba0:	mov	w2, #0x20                  	// #32
   29ba4:	mov	x0, x21
   29ba8:	ldr	x8, [x8, #8]
   29bac:	blr	x8
   29bb0:	ldr	x8, [x29, #24]
   29bb4:	udiv	x9, x8, x23
   29bb8:	msub	w8, w9, w23, w8
   29bbc:	add	w8, w8, #0x1
   29bc0:	subs	x8, x20, x8
   29bc4:	csel	x25, xzr, x8, cc  // cc = lo, ul, last
   29bc8:	b.ls	29c48 <__gmpn_random2@@Base+0x1e8>  // b.plast
   29bcc:	lsr	x8, x25, #3
   29bd0:	and	x8, x8, #0x1ffffffffffffff8
   29bd4:	ldr	x9, [x19, x8]
   29bd8:	lsl	x10, x24, x25
   29bdc:	add	x1, x29, #0x18
   29be0:	mov	w2, #0x20                  	// #32
   29be4:	eor	x9, x9, x10
   29be8:	str	x9, [x19, x8]
   29bec:	ldr	x8, [x21, #24]
   29bf0:	mov	x0, x21
   29bf4:	ldr	x8, [x8, #8]
   29bf8:	blr	x8
   29bfc:	ldr	x8, [x29, #24]
   29c00:	udiv	x9, x8, x23
   29c04:	msub	w8, w9, w23, w8
   29c08:	add	w8, w8, #0x1
   29c0c:	subs	x9, x25, x8
   29c10:	csel	x20, xzr, x9, cc  // cc = lo, ul, last
   29c14:	lsr	x9, x20, #6
   29c18:	lsl	x10, x9, #3
   29c1c:	ldr	x11, [x19, x10]
   29c20:	lsl	x12, x24, x20
   29c24:	adds	x11, x11, x12
   29c28:	str	x11, [x19, x10]
   29c2c:	b.cc	29b90 <__gmpn_random2@@Base+0x130>  // b.lo, b.ul, b.last
   29c30:	add	x9, x22, x9, lsl #3
   29c34:	ldr	x10, [x9]
   29c38:	adds	x10, x10, #0x1
   29c3c:	str	x10, [x9], #8
   29c40:	b.cs	29c34 <__gmpn_random2@@Base+0x1d4>  // b.hs, b.nlast
   29c44:	b	29b90 <__gmpn_random2@@Base+0x130>
   29c48:	ldp	x20, x19, [sp, #64]
   29c4c:	ldp	x22, x21, [sp, #48]
   29c50:	ldp	x24, x23, [sp, #32]
   29c54:	ldr	x25, [sp, #16]
   29c58:	ldp	x29, x30, [sp], #80
   29c5c:	ret

0000000000029c60 <__gmpn_pow_1@@Base>:
   29c60:	stp	x29, x30, [sp, #-80]!
   29c64:	stp	x20, x19, [sp, #64]
   29c68:	mov	x19, x2
   29c6c:	mov	x20, x1
   29c70:	cmp	x3, #0x2
   29c74:	stp	x26, x25, [sp, #16]
   29c78:	stp	x24, x23, [sp, #32]
   29c7c:	stp	x22, x21, [sp, #48]
   29c80:	mov	x29, sp
   29c84:	b.cs	29c9c <__gmpn_pow_1@@Base+0x3c>  // b.hs, b.nlast
   29c88:	cbz	x3, 29e20 <__gmpn_pow_1@@Base+0x1c0>
   29c8c:	mov	x1, x20
   29c90:	mov	x2, x19
   29c94:	bl	cc10 <__gmpn_copyi@plt>
   29c98:	b	29e28 <__gmpn_pow_1@@Base+0x1c8>
   29c9c:	mov	x9, xzr
   29ca0:	mov	w8, #0x40                  	// #64
   29ca4:	mov	x10, x3
   29ca8:	sxtw	x9, w9
   29cac:	eor	x9, x9, x10
   29cb0:	lsr	x10, x10, #1
   29cb4:	sub	w8, w8, #0x1
   29cb8:	cbnz	x10, 29ca8 <__gmpn_pow_1@@Base+0x48>
   29cbc:	lsl	x25, x3, x8
   29cc0:	cmp	x19, #0x1
   29cc4:	sub	w26, w8, #0x3e
   29cc8:	b.ne	29d5c <__gmpn_pow_1@@Base+0xfc>  // b.any
   29ccc:	ldr	x20, [x20]
   29cd0:	tst	w8, #0x1
   29cd4:	csel	x21, x0, x4, eq  // eq = none
   29cd8:	umulh	x9, x20, x20
   29cdc:	mul	x10, x20, x20
   29ce0:	csel	x8, x4, x0, eq  // eq = none
   29ce4:	stp	x10, x9, [x21]
   29ce8:	cmp	x9, #0x0
   29cec:	mov	w9, #0x1                   	// #1
   29cf0:	cinc	x19, x9, ne  // ne = any
   29cf4:	lsl	x25, x25, #1
   29cf8:	mov	x22, x8
   29cfc:	tbz	x25, #63, 29d20 <__gmpn_pow_1@@Base+0xc0>
   29d00:	mov	x0, x21
   29d04:	mov	x1, x21
   29d08:	mov	x2, x19
   29d0c:	mov	x3, x20
   29d10:	bl	d670 <__gmpn_mul_1@plt>
   29d14:	cmp	x0, #0x0
   29d18:	str	x0, [x21, x19, lsl #3]
   29d1c:	cinc	x19, x19, ne  // ne = any
   29d20:	cbz	w26, 29e28 <__gmpn_pow_1@@Base+0x1c8>
   29d24:	mov	x0, x22
   29d28:	mov	x1, x21
   29d2c:	mov	x2, x19
   29d30:	bl	ca90 <__gmpn_sqr@plt>
   29d34:	add	x8, x22, x19, lsl #4
   29d38:	ldur	x8, [x8, #-8]
   29d3c:	lsl	x9, x19, #1
   29d40:	add	w26, w26, #0x1
   29d44:	cmp	x8, #0x0
   29d48:	cset	w8, eq  // eq = none
   29d4c:	sub	x19, x9, x8
   29d50:	mov	x8, x21
   29d54:	mov	x21, x22
   29d58:	b	29cf4 <__gmpn_pow_1@@Base+0x94>
   29d5c:	eor	w8, w8, w9
   29d60:	tst	w8, #0x1
   29d64:	csel	x23, x4, x0, eq  // eq = none
   29d68:	csel	x21, x0, x4, eq  // eq = none
   29d6c:	mov	x0, x23
   29d70:	mov	x1, x20
   29d74:	mov	x2, x19
   29d78:	bl	ca90 <__gmpn_sqr@plt>
   29d7c:	add	x8, x23, x19, lsl #4
   29d80:	ldur	x8, [x8, #-8]
   29d84:	lsl	x9, x19, #1
   29d88:	cmp	x8, #0x0
   29d8c:	cset	w8, eq  // eq = none
   29d90:	sub	x22, x9, x8
   29d94:	lsl	x25, x25, #1
   29d98:	tbnz	x25, #63, 29dac <__gmpn_pow_1@@Base+0x14c>
   29d9c:	mov	x24, x21
   29da0:	mov	x21, x23
   29da4:	cbnz	w26, 29ddc <__gmpn_pow_1@@Base+0x17c>
   29da8:	b	29e18 <__gmpn_pow_1@@Base+0x1b8>
   29dac:	mov	x0, x21
   29db0:	mov	x1, x23
   29db4:	mov	x2, x22
   29db8:	mov	x3, x20
   29dbc:	mov	x4, x19
   29dc0:	add	x24, x22, x19
   29dc4:	bl	cea0 <__gmpn_mul@plt>
   29dc8:	cmp	x0, #0x0
   29dcc:	cset	w8, eq  // eq = none
   29dd0:	sub	x22, x24, x8
   29dd4:	mov	x24, x23
   29dd8:	cbz	w26, 29e18 <__gmpn_pow_1@@Base+0x1b8>
   29ddc:	mov	x0, x24
   29de0:	mov	x1, x21
   29de4:	mov	x2, x22
   29de8:	bl	ca90 <__gmpn_sqr@plt>
   29dec:	add	x8, x24, x22, lsl #4
   29df0:	ldur	x8, [x8, #-8]
   29df4:	lsl	x9, x22, #1
   29df8:	add	w26, w26, #0x1
   29dfc:	mov	x23, x24
   29e00:	cmp	x8, #0x0
   29e04:	cset	w8, eq  // eq = none
   29e08:	sub	x22, x9, x8
   29e0c:	lsl	x25, x25, #1
   29e10:	tbz	x25, #63, 29d9c <__gmpn_pow_1@@Base+0x13c>
   29e14:	b	29dac <__gmpn_pow_1@@Base+0x14c>
   29e18:	mov	x19, x22
   29e1c:	b	29e28 <__gmpn_pow_1@@Base+0x1c8>
   29e20:	mov	w19, #0x1                   	// #1
   29e24:	str	x19, [x0]
   29e28:	mov	x0, x19
   29e2c:	ldp	x20, x19, [sp, #64]
   29e30:	ldp	x22, x21, [sp, #48]
   29e34:	ldp	x24, x23, [sp, #32]
   29e38:	ldp	x26, x25, [sp, #16]
   29e3c:	ldp	x29, x30, [sp], #80
   29e40:	ret

0000000000029e44 <__gmpn_rootrem@@Base>:
   29e44:	stp	x29, x30, [sp, #-64]!
   29e48:	stp	x24, x23, [sp, #16]
   29e4c:	stp	x22, x21, [sp, #32]
   29e50:	stp	x20, x19, [sp, #48]
   29e54:	mov	x29, sp
   29e58:	sub	sp, sp, #0x10
   29e5c:	cmp	x4, #0x2
   29e60:	mov	x20, x0
   29e64:	b.eq	29f60 <__gmpn_rootrem@@Base+0x11c>  // b.none
   29e68:	mov	x19, x4
   29e6c:	cbnz	x1, 29f30 <__gmpn_rootrem@@Base+0xec>
   29e70:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   29e74:	add	x8, x3, #0x2
   29e78:	movk	x9, #0x5556
   29e7c:	smulh	x8, x8, x9
   29e80:	add	x8, x8, x8, lsr #63
   29e84:	cmp	x8, x19
   29e88:	b.ls	29f30 <__gmpn_rootrem@@Base+0xec>  // b.plast
   29e8c:	sub	x8, x3, #0x1
   29e90:	add	x21, x19, x3
   29e94:	udiv	x24, x8, x19
   29e98:	add	x8, x21, x24
   29e9c:	lsl	x8, x8, #3
   29ea0:	add	x1, x8, #0x10
   29ea4:	mov	w8, #0x7f00                	// #32512
   29ea8:	cmp	x1, x8
   29eac:	stur	xzr, [x29, #-8]
   29eb0:	b.hi	29f6c <__gmpn_rootrem@@Base+0x128>  // b.pmore
   29eb4:	add	x9, x1, #0xf
   29eb8:	mov	x8, sp
   29ebc:	and	x9, x9, #0xfffffffffffffff0
   29ec0:	sub	x22, x8, x9
   29ec4:	mov	sp, x22
   29ec8:	lsl	x23, x19, #3
   29ecc:	add	x0, x22, x23
   29ed0:	mov	x1, x2
   29ed4:	mov	x2, x3
   29ed8:	bl	cc10 <__gmpn_copyi@plt>
   29edc:	mov	x0, x22
   29ee0:	mov	w1, wzr
   29ee4:	mov	x2, x23
   29ee8:	bl	c780 <memset@plt>
   29eec:	add	x23, x22, x21, lsl #3
   29ef0:	mov	w5, #0x1                   	// #1
   29ef4:	mov	x0, x23
   29ef8:	mov	x1, xzr
   29efc:	mov	x2, x22
   29f00:	mov	x3, x21
   29f04:	mov	x4, x19
   29f08:	bl	29f8c <__gmpn_rootrem@@Base+0x148>
   29f0c:	mov	x19, x0
   29f10:	add	x1, x23, #0x8
   29f14:	add	x2, x24, #0x1
   29f18:	mov	x0, x20
   29f1c:	bl	cc10 <__gmpn_copyi@plt>
   29f20:	ldur	x0, [x29, #-8]
   29f24:	cbz	x0, 29f44 <__gmpn_rootrem@@Base+0x100>
   29f28:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   29f2c:	b	29f44 <__gmpn_rootrem@@Base+0x100>
   29f30:	mov	x0, x20
   29f34:	mov	x4, x19
   29f38:	mov	w5, wzr
   29f3c:	bl	29f8c <__gmpn_rootrem@@Base+0x148>
   29f40:	mov	x19, x0
   29f44:	mov	x0, x19
   29f48:	mov	sp, x29
   29f4c:	ldp	x20, x19, [sp, #48]
   29f50:	ldp	x22, x21, [sp, #32]
   29f54:	ldp	x24, x23, [sp, #16]
   29f58:	ldp	x29, x30, [sp], #64
   29f5c:	ret
   29f60:	mov	x0, x20
   29f64:	bl	d590 <__gmpn_sqrtrem@plt>
   29f68:	b	29f40 <__gmpn_rootrem@@Base+0xfc>
   29f6c:	sub	x0, x29, #0x8
   29f70:	mov	x22, x3
   29f74:	mov	x23, x2
   29f78:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   29f7c:	mov	x2, x23
   29f80:	mov	x3, x22
   29f84:	mov	x22, x0
   29f88:	b	29ec8 <__gmpn_rootrem@@Base+0x84>
   29f8c:	stp	x29, x30, [sp, #-96]!
   29f90:	stp	x28, x27, [sp, #16]
   29f94:	stp	x26, x25, [sp, #32]
   29f98:	stp	x24, x23, [sp, #48]
   29f9c:	stp	x22, x21, [sp, #64]
   29fa0:	stp	x20, x19, [sp, #80]
   29fa4:	mov	x29, sp
   29fa8:	sub	sp, sp, #0x2b0
   29fac:	sub	x20, x3, #0x1
   29fb0:	ldr	x9, [x2, x20, lsl #3]
   29fb4:	lsl	x11, x3, #6
   29fb8:	mov	x23, x2
   29fbc:	mov	x22, x3
   29fc0:	clz	x8, x9
   29fc4:	add	w10, w8, #0x1
   29fc8:	sub	x2, x11, x10
   29fcc:	mov	x25, x1
   29fd0:	cmp	x2, x4
   29fd4:	mov	x24, x0
   29fd8:	mov	x19, sp
   29fdc:	b.cs	2a01c <__gmpn_rootrem@@Base+0x1d8>  // b.hs, b.nlast
   29fe0:	mov	w8, #0x1                   	// #1
   29fe4:	str	x8, [x24]
   29fe8:	cbz	x25, 2a008 <__gmpn_rootrem@@Base+0x1c4>
   29fec:	mov	w3, #0x1                   	// #1
   29ff0:	mov	x0, x25
   29ff4:	mov	x1, x23
   29ff8:	mov	x2, x22
   29ffc:	bl	caf0 <__gmpn_sub_1@plt>
   2a000:	mov	x8, xzr
   2a004:	add	x23, x25, x20, lsl #3
   2a008:	ldr	x9, [x23]
   2a00c:	cmp	x9, x8
   2a010:	cset	w8, eq  // eq = none
   2a014:	sub	x9, x22, x8
   2a018:	b	2a6d4 <__gmpn_rootrem@@Base+0x890>
   2a01c:	mov	w27, w5
   2a020:	mov	x21, x4
   2a024:	cmp	w10, #0x40
   2a028:	b.ne	2a038 <__gmpn_rootrem@@Base+0x1f4>  // b.any
   2a02c:	add	x8, x23, x22, lsl #3
   2a030:	ldur	x1, [x8, #-16]
   2a034:	b	2a05c <__gmpn_rootrem@@Base+0x218>
   2a038:	cmp	x22, #0x1
   2a03c:	cset	w11, ne  // ne = any
   2a040:	sub	x11, x20, x11
   2a044:	ldr	x11, [x23, x11, lsl #3]
   2a048:	lsl	x9, x9, x10
   2a04c:	mov	w10, #0x3f                  	// #63
   2a050:	sub	w8, w10, w8
   2a054:	lsr	x8, x11, x8
   2a058:	orr	x1, x8, x9
   2a05c:	mov	x0, x24
   2a060:	mov	x3, x21
   2a064:	bl	2a72c <__gmpn_rootrem@@Base+0x8e8>
   2a068:	mov	w20, w0
   2a06c:	sub	x26, x21, #0x1
   2a070:	cmp	w0, #0x9
   2a074:	str	x21, [x19, #48]
   2a078:	str	x20, [x19, #160]
   2a07c:	b.cc	2a0c8 <__gmpn_rootrem@@Base+0x284>  // b.lo, b.ul, b.last
   2a080:	lsr	x8, x26, #2
   2a084:	mov	w9, #0x43                  	// #67
   2a088:	add	x10, x19, #0xa0
   2a08c:	clz	x8, x8
   2a090:	mov	x21, xzr
   2a094:	sub	w8, w9, w8
   2a098:	add	x9, x10, #0x8
   2a09c:	mov	x12, x20
   2a0a0:	add	x10, x12, x8
   2a0a4:	sub	x11, x12, #0x1
   2a0a8:	cmp	x12, x8
   2a0ac:	lsr	x10, x10, #1
   2a0b0:	csel	x12, x10, x11, hi  // hi = pmore
   2a0b4:	str	x12, [x9, x21, lsl #3]
   2a0b8:	cmp	x12, #0x8
   2a0bc:	add	x21, x21, #0x1
   2a0c0:	b.hi	2a0a0 <__gmpn_rootrem@@Base+0x25c>  // b.pmore
   2a0c4:	b	2a0d0 <__gmpn_rootrem@@Base+0x28c>
   2a0c8:	mov	w21, wzr
   2a0cc:	mov	x12, x20
   2a0d0:	ldr	x8, [x24]
   2a0d4:	mov	w9, #0x8                   	// #8
   2a0d8:	sub	x9, x9, x12
   2a0dc:	cmp	w21, #0x41
   2a0e0:	lsr	x8, x8, x9
   2a0e4:	str	x23, [x19, #96]
   2a0e8:	str	x12, [x19, #144]
   2a0ec:	str	x8, [x24]
   2a0f0:	b.cs	2a714 <__gmpn_rootrem@@Base+0x8d0>  // b.hs, b.nlast
   2a0f4:	adrp	x8, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a0f8:	ldr	d0, [x8, #3248]
   2a0fc:	ldr	x8, [x19, #48]
   2a100:	mov	x9, x22
   2a104:	add	x22, x22, #0x1
   2a108:	str	x9, [x19, #112]
   2a10c:	ucvtf	d1, x8
   2a110:	mov	x8, #0x3f90000000000000    	// #4580160821035794432
   2a114:	fmul	d0, d1, d0
   2a118:	fmov	d1, x8
   2a11c:	fmul	d0, d0, d1
   2a120:	fcvtzs	x8, d0
   2a124:	add	x8, x9, x8
   2a128:	add	x23, x8, #0x2
   2a12c:	add	x8, x22, x23, lsl #1
   2a130:	lsl	x1, x8, #3
   2a134:	mov	w8, #0x7f00                	// #32512
   2a138:	cmp	x1, x8
   2a13c:	str	xzr, [x19, #152]
   2a140:	str	w27, [x19, #12]
   2a144:	b.hi	2a6f8 <__gmpn_rootrem@@Base+0x8b4>  // b.pmore
   2a148:	add	x9, x1, #0xf
   2a14c:	mov	x8, sp
   2a150:	and	x9, x9, #0xfffffffffffffff0
   2a154:	sub	x0, x8, x9
   2a158:	mov	sp, x0
   2a15c:	add	x27, x0, x22, lsl #3
   2a160:	cmp	x25, #0x0
   2a164:	add	x28, x27, x23, lsl #3
   2a168:	csel	x22, x0, x25, eq  // eq = none
   2a16c:	str	x25, [x19, #16]
   2a170:	str	x0, [x19, #32]
   2a174:	cbz	w21, 2a604 <__gmpn_rootrem@@Base+0x7c0>
   2a178:	sxtw	x9, w21
   2a17c:	str	x9, [x19, #136]
   2a180:	add	x9, x22, #0x8
   2a184:	str	x9, [x19, #24]
   2a188:	sub	x9, x22, #0x8
   2a18c:	ldr	x8, [x19, #48]
   2a190:	str	x9, [x19, #40]
   2a194:	ldr	x23, [x19, #96]
   2a198:	ldr	x9, [x19, #144]
   2a19c:	mul	x8, x20, x8
   2a1a0:	mov	w25, #0x1                   	// #1
   2a1a4:	sub	x8, x8, x9
   2a1a8:	str	x8, [x19, #128]
   2a1ac:	b	2a1e8 <__gmpn_rootrem@@Base+0x3a4>
   2a1b0:	ldr	x23, [x19, #96]
   2a1b4:	mov	w8, #0x1                   	// #1
   2a1b8:	ldr	x9, [x19, #72]
   2a1bc:	mov	x10, #0xffffffffffffffff    	// #-1
   2a1c0:	add	x8, x24, x8, lsl #3
   2a1c4:	mvn	w9, w9
   2a1c8:	lsr	x9, x10, x9
   2a1cc:	stur	x9, [x8, #-8]
   2a1d0:	ldp	x10, x9, [x19, #80]
   2a1d4:	ldr	x8, [x9]
   2a1d8:	orr	x8, x8, x10
   2a1dc:	str	x8, [x9]
   2a1e0:	ldr	x8, [x19, #136]
   2a1e4:	cbz	w8, 2a60c <__gmpn_rootrem@@Base+0x7c8>
   2a1e8:	ldr	x8, [x19, #144]
   2a1ec:	ldr	x9, [x19, #128]
   2a1f0:	mov	x0, x22
   2a1f4:	msub	x10, x8, x26, x9
   2a1f8:	ldr	x8, [x19, #112]
   2a1fc:	lsr	x9, x10, #6
   2a200:	ands	x3, x10, #0x3f
   2a204:	add	x1, x23, x9, lsl #3
   2a208:	sub	x20, x8, x9
   2a20c:	mov	x2, x20
   2a210:	stp	x9, x10, [x19, #120]
   2a214:	b.eq	2a220 <__gmpn_rootrem@@Base+0x3dc>  // b.none
   2a218:	bl	c2f0 <__gmpn_rshift@plt>
   2a21c:	b	2a224 <__gmpn_rootrem@@Base+0x3e0>
   2a220:	bl	cc10 <__gmpn_copyi@plt>
   2a224:	add	x8, x22, x20, lsl #3
   2a228:	ldur	x8, [x8, #-8]
   2a22c:	cmp	x8, #0x0
   2a230:	cset	w8, eq  // eq = none
   2a234:	csetm	x9, eq  // eq = none
   2a238:	sub	x20, x20, x8
   2a23c:	str	x9, [x19, #104]
   2a240:	mov	x0, x28
   2a244:	mov	x1, x24
   2a248:	mov	x2, x25
   2a24c:	mov	x3, x26
   2a250:	mov	x4, x27
   2a254:	bl	d3f0 <__gmpn_pow_1@plt>
   2a258:	mov	x21, x0
   2a25c:	mov	x0, x27
   2a260:	mov	x1, x28
   2a264:	mov	x2, x21
   2a268:	mov	x3, x24
   2a26c:	mov	x4, x25
   2a270:	bl	cea0 <__gmpn_mul@plt>
   2a274:	add	x8, x21, x25
   2a278:	add	x9, x27, x8, lsl #3
   2a27c:	ldur	x9, [x9, #-8]
   2a280:	cmp	x9, #0x0
   2a284:	cset	w9, eq  // eq = none
   2a288:	sub	x23, x8, x9
   2a28c:	cmp	x23, x20
   2a290:	b.gt	2a2b0 <__gmpn_rootrem@@Base+0x46c>
   2a294:	b.ne	2a2c8 <__gmpn_rootrem@@Base+0x484>  // b.any
   2a298:	mov	x0, x27
   2a29c:	mov	x1, x22
   2a2a0:	mov	x2, x20
   2a2a4:	bl	c570 <__gmpn_cmp@plt>
   2a2a8:	cmp	w0, #0x1
   2a2ac:	b.lt	2a2cc <__gmpn_rootrem@@Base+0x488>  // b.tstop
   2a2b0:	mov	x8, x24
   2a2b4:	ldr	x9, [x8]
   2a2b8:	sub	x10, x9, #0x1
   2a2bc:	str	x10, [x8], #8
   2a2c0:	cbz	x9, 2a2b4 <__gmpn_rootrem@@Base+0x470>
   2a2c4:	b	2a240 <__gmpn_rootrem@@Base+0x3fc>
   2a2c8:	mov	w0, #0x1                   	// #1
   2a2cc:	ldp	x13, x8, [x19, #128]
   2a2d0:	add	x9, x19, #0xa0
   2a2d4:	sub	x11, x8, #0x1
   2a2d8:	ldr	x8, [x9, x8, lsl #3]
   2a2dc:	ldr	x9, [x9, x11, lsl #3]
   2a2e0:	sub	x10, x13, #0x1
   2a2e4:	str	x11, [x19, #88]
   2a2e8:	sub	x8, x9, x8
   2a2ec:	lsr	x12, x8, #6
   2a2f0:	sub	x9, x13, x8
   2a2f4:	stp	x12, x8, [x19, #136]
   2a2f8:	lsr	x8, x10, #6
   2a2fc:	lsr	x10, x9, #6
   2a300:	sub	x8, x8, x10
   2a304:	add	x13, x8, #0x1
   2a308:	str	x9, [x19, #128]
   2a30c:	cbz	w0, 2a390 <__gmpn_rootrem@@Base+0x54c>
   2a310:	mov	x0, x22
   2a314:	mov	x1, x22
   2a318:	mov	x2, x20
   2a31c:	mov	x3, x27
   2a320:	mov	x4, x23
   2a324:	stp	x10, x13, [x19, #64]
   2a328:	bl	d340 <__gmpn_sub@plt>
   2a32c:	ldp	x9, x8, [x19, #104]
   2a330:	mov	x23, xzr
   2a334:	add	x8, x8, x9
   2a338:	ldr	x9, [x19, #120]
   2a33c:	sub	x8, x8, x9
   2a340:	ldr	x9, [x19, #40]
   2a344:	add	x8, x9, x8, lsl #3
   2a348:	ldr	x9, [x8, x23, lsl #3]
   2a34c:	sub	x23, x23, #0x1
   2a350:	cbz	x9, 2a348 <__gmpn_rootrem@@Base+0x504>
   2a354:	ldr	x8, [x19, #144]
   2a358:	mov	x1, x22
   2a35c:	and	x3, x8, #0x3f
   2a360:	ldr	x8, [x19, #136]
   2a364:	add	x20, x22, x8, lsl #3
   2a368:	ldp	x9, x8, [x19, #104]
   2a36c:	mov	x0, x20
   2a370:	add	x8, x8, x9
   2a374:	ldr	x9, [x19, #120]
   2a378:	sub	x8, x8, x9
   2a37c:	add	x8, x8, x23
   2a380:	add	x2, x8, #0x1
   2a384:	cbz	x3, 2a3a8 <__gmpn_rootrem@@Base+0x564>
   2a388:	bl	c2d0 <__gmpn_lshift@plt>
   2a38c:	b	2a3b0 <__gmpn_rootrem@@Base+0x56c>
   2a390:	ldr	x9, [x19, #96]
   2a394:	ldr	x23, [x19, #144]
   2a398:	str	xzr, [x19, #80]
   2a39c:	str	xzr, [x19, #56]
   2a3a0:	str	x12, [x19, #104]
   2a3a4:	b	2a420 <__gmpn_rootrem@@Base+0x5dc>
   2a3a8:	bl	c130 <__gmpn_copyd@plt>
   2a3ac:	mov	x0, xzr
   2a3b0:	ldr	x8, [x19, #112]
   2a3b4:	ldr	x9, [x19, #136]
   2a3b8:	ldr	x13, [x19, #72]
   2a3bc:	add	x8, x8, x9
   2a3c0:	ldr	x9, [x19, #104]
   2a3c4:	add	x8, x8, x9
   2a3c8:	ldr	x9, [x19, #120]
   2a3cc:	sub	x9, x8, x9
   2a3d0:	add	x8, x9, x23
   2a3d4:	cbz	x0, 2a3ec <__gmpn_rootrem@@Base+0x5a8>
   2a3d8:	ldr	x10, [x19, #24]
   2a3dc:	add	x9, x10, x9, lsl #3
   2a3e0:	add	x10, x8, #0x2
   2a3e4:	str	x0, [x9, x23, lsl #3]
   2a3e8:	b	2a3f0 <__gmpn_rootrem@@Base+0x5ac>
   2a3ec:	add	x10, x8, #0x1
   2a3f0:	ldp	x11, x23, [x19, #136]
   2a3f4:	ldr	x9, [x19, #96]
   2a3f8:	ldr	x12, [x20]
   2a3fc:	sub	x8, x13, #0x1
   2a400:	cmp	x8, x11
   2a404:	str	x10, [x19, #104]
   2a408:	str	x12, [x19, #80]
   2a40c:	b.le	2a41c <__gmpn_rootrem@@Base+0x5d8>
   2a410:	add	x8, x22, x11, lsl #3
   2a414:	ldr	x8, [x8, #8]
   2a418:	str	x8, [x19, #56]
   2a41c:	ldr	x10, [x19, #64]
   2a420:	ldr	x8, [x19, #128]
   2a424:	add	x1, x9, x10, lsl #3
   2a428:	mov	x0, x22
   2a42c:	mov	x20, x13
   2a430:	ands	x3, x8, #0x3f
   2a434:	mov	x2, x13
   2a438:	b.eq	2a444 <__gmpn_rootrem@@Base+0x600>  // b.none
   2a43c:	bl	c2f0 <__gmpn_rshift@plt>
   2a440:	b	2a448 <__gmpn_rootrem@@Base+0x604>
   2a444:	bl	cc10 <__gmpn_copyi@plt>
   2a448:	ldr	x12, [x19, #136]
   2a44c:	and	x8, x23, #0x3f
   2a450:	str	x8, [x19, #120]
   2a454:	mov	w10, #0x1                   	// #1
   2a458:	lsl	x8, x12, #3
   2a45c:	ldr	x9, [x22, x8]
   2a460:	lsl	x11, x10, x23
   2a464:	str	x11, [x19, #64]
   2a468:	sub	x11, x11, #0x1
   2a46c:	and	x9, x9, x11
   2a470:	ldr	x11, [x19, #80]
   2a474:	sub	x10, x20, #0x1
   2a478:	mov	x20, x12
   2a47c:	cmp	x10, x12
   2a480:	orr	x9, x9, x11
   2a484:	str	x9, [x22, x8]
   2a488:	b.le	2a498 <__gmpn_rootrem@@Base+0x654>
   2a48c:	ldr	x9, [x19, #56]
   2a490:	add	x8, x22, x20, lsl #3
   2a494:	str	x9, [x8, #8]
   2a498:	ldr	x3, [x19, #48]
   2a49c:	mov	x0, x28
   2a4a0:	mov	x1, x28
   2a4a4:	mov	x2, x21
   2a4a8:	bl	d670 <__gmpn_mul_1@plt>
   2a4ac:	ldr	x3, [x19, #120]
   2a4b0:	cmp	x0, #0x0
   2a4b4:	str	x0, [x28, x21, lsl #3]
   2a4b8:	cinc	x23, x21, ne  // ne = any
   2a4bc:	add	x21, x24, x20, lsl #3
   2a4c0:	cbz	x3, 2a4d8 <__gmpn_rootrem@@Base+0x694>
   2a4c4:	mov	x0, x21
   2a4c8:	mov	x1, x24
   2a4cc:	mov	x2, x25
   2a4d0:	bl	c2d0 <__gmpn_lshift@plt>
   2a4d4:	b	2a4ec <__gmpn_rootrem@@Base+0x6a8>
   2a4d8:	mov	x0, x21
   2a4dc:	mov	x1, x24
   2a4e0:	mov	x2, x25
   2a4e4:	bl	c130 <__gmpn_copyd@plt>
   2a4e8:	mov	x0, xzr
   2a4ec:	ldr	x2, [x19, #104]
   2a4f0:	ldr	x8, [x19, #88]
   2a4f4:	add	x25, x20, x25
   2a4f8:	cbz	x0, 2a504 <__gmpn_rootrem@@Base+0x6c0>
   2a4fc:	str	x0, [x24, x25, lsl #3]
   2a500:	add	x25, x25, #0x1
   2a504:	str	x8, [x19, #136]
   2a508:	ldr	x8, [x21]
   2a50c:	mov	x4, x23
   2a510:	subs	x20, x2, x23
   2a514:	stp	x8, x21, [x19, #80]
   2a518:	ldr	x8, [x19, #144]
   2a51c:	sub	x8, x8, #0x1
   2a520:	lsr	x21, x8, #6
   2a524:	b.lt	2a5e8 <__gmpn_rootrem@@Base+0x7a4>  // b.tstop
   2a528:	add	x23, x21, #0x1
   2a52c:	cmp	x20, x23
   2a530:	str	x8, [x19, #72]
   2a534:	b.gt	2a558 <__gmpn_rootrem@@Base+0x714>
   2a538:	ldr	x5, [x19, #32]
   2a53c:	mov	x0, x27
   2a540:	mov	x1, x22
   2a544:	mov	x3, x28
   2a548:	bl	c480 <__gmpn_div_q@plt>
   2a54c:	ldr	x8, [x27, x20, lsl #3]
   2a550:	cmp	x8, #0x0
   2a554:	cinc	x20, x20, ne  // ne = any
   2a558:	ldr	x8, [x19, #120]
   2a55c:	cmp	x23, x20
   2a560:	b.ge	2a594 <__gmpn_rootrem@@Base+0x750>  // b.tcont
   2a564:	cbz	x21, 2a1b0 <__gmpn_rootrem@@Base+0x36c>
   2a568:	lsl	x2, x21, #3
   2a56c:	mov	w1, #0xff                  	// #255
   2a570:	mov	x0, x24
   2a574:	bl	c780 <memset@plt>
   2a578:	ldr	x23, [x19, #96]
   2a57c:	mov	x8, xzr
   2a580:	add	x8, x8, #0x1
   2a584:	cmp	x21, x8
   2a588:	b.ne	2a580 <__gmpn_rootrem@@Base+0x73c>  // b.any
   2a58c:	add	x8, x8, #0x1
   2a590:	b	2a1b8 <__gmpn_rootrem@@Base+0x374>
   2a594:	b.ne	2a5b0 <__gmpn_rootrem@@Base+0x76c>  // b.any
   2a598:	cbz	x8, 2a5b0 <__gmpn_rootrem@@Base+0x76c>
   2a59c:	add	x8, x27, x20, lsl #3
   2a5a0:	ldur	x8, [x8, #-8]
   2a5a4:	ldr	x9, [x19, #64]
   2a5a8:	cmp	x8, x9
   2a5ac:	b.cs	2a564 <__gmpn_rootrem@@Base+0x720>  // b.hs, b.nlast
   2a5b0:	mov	x0, x24
   2a5b4:	mov	x1, x27
   2a5b8:	mov	x2, x20
   2a5bc:	bl	cc10 <__gmpn_copyi@plt>
   2a5c0:	cmp	x23, x20
   2a5c4:	ldr	x23, [x19, #96]
   2a5c8:	b.eq	2a1d0 <__gmpn_rootrem@@Base+0x38c>  // b.none
   2a5cc:	sub	x8, x21, x20
   2a5d0:	lsl	x8, x8, #3
   2a5d4:	add	x0, x24, x20, lsl #3
   2a5d8:	add	x2, x8, #0x8
   2a5dc:	mov	w1, wzr
   2a5e0:	bl	c780 <memset@plt>
   2a5e4:	b	2a1d0 <__gmpn_rootrem@@Base+0x38c>
   2a5e8:	lsl	x8, x21, #3
   2a5ec:	add	x2, x8, #0x8
   2a5f0:	mov	x0, x24
   2a5f4:	mov	w1, wzr
   2a5f8:	bl	c780 <memset@plt>
   2a5fc:	ldr	x23, [x19, #96]
   2a600:	b	2a1d0 <__gmpn_rootrem@@Base+0x38c>
   2a604:	ldr	x23, [x19, #96]
   2a608:	mov	w25, #0x1                   	// #1
   2a60c:	ldp	x9, x21, [x19, #104]
   2a610:	ldr	w8, [x19, #12]
   2a614:	cbz	w8, 2a624 <__gmpn_rootrem@@Base+0x7e0>
   2a618:	ldr	x8, [x24]
   2a61c:	cmp	x8, #0x1
   2a620:	b.hi	2a6cc <__gmpn_rootrem@@Base+0x888>  // b.pmore
   2a624:	ldr	x22, [x19, #48]
   2a628:	mov	x0, x27
   2a62c:	mov	x1, x24
   2a630:	mov	x2, x25
   2a634:	mov	x3, x22
   2a638:	mov	x4, x28
   2a63c:	bl	d3f0 <__gmpn_pow_1@plt>
   2a640:	cmp	x0, x21
   2a644:	b.gt	2a668 <__gmpn_rootrem@@Base+0x824>
   2a648:	mov	x20, x0
   2a64c:	b.ne	2a680 <__gmpn_rootrem@@Base+0x83c>  // b.any
   2a650:	mov	x0, x27
   2a654:	mov	x1, x23
   2a658:	mov	x2, x21
   2a65c:	bl	c570 <__gmpn_cmp@plt>
   2a660:	cmp	w0, #0x1
   2a664:	b.lt	2a684 <__gmpn_rootrem@@Base+0x840>  // b.tstop
   2a668:	mov	x8, x24
   2a66c:	ldr	x9, [x8]
   2a670:	sub	x10, x9, #0x1
   2a674:	str	x10, [x8], #8
   2a678:	cbz	x9, 2a66c <__gmpn_rootrem@@Base+0x828>
   2a67c:	b	2a628 <__gmpn_rootrem@@Base+0x7e4>
   2a680:	mov	w0, #0x1                   	// #1
   2a684:	ldr	x8, [x19, #16]
   2a688:	cmp	w0, #0x0
   2a68c:	cset	w9, ne  // ne = any
   2a690:	cbz	x8, 2a6cc <__gmpn_rootrem@@Base+0x888>
   2a694:	cbz	w0, 2a6cc <__gmpn_rootrem@@Base+0x888>
   2a698:	ldr	x22, [x19, #16]
   2a69c:	mov	x1, x23
   2a6a0:	mov	x2, x21
   2a6a4:	mov	x3, x27
   2a6a8:	mov	x0, x22
   2a6ac:	mov	x4, x20
   2a6b0:	bl	d340 <__gmpn_sub@plt>
   2a6b4:	sub	x8, x22, #0x8
   2a6b8:	ldr	x10, [x8, x21, lsl #3]
   2a6bc:	sub	x9, x21, #0x1
   2a6c0:	mov	x21, x9
   2a6c4:	cbz	x10, 2a6b8 <__gmpn_rootrem@@Base+0x874>
   2a6c8:	add	x9, x9, #0x1
   2a6cc:	ldr	x0, [x19, #152]
   2a6d0:	cbnz	x0, 2a704 <__gmpn_rootrem@@Base+0x8c0>
   2a6d4:	mov	x0, x9
   2a6d8:	mov	sp, x29
   2a6dc:	ldp	x20, x19, [sp, #80]
   2a6e0:	ldp	x22, x21, [sp, #64]
   2a6e4:	ldp	x24, x23, [sp, #48]
   2a6e8:	ldp	x26, x25, [sp, #32]
   2a6ec:	ldp	x28, x27, [sp, #16]
   2a6f0:	ldp	x29, x30, [sp], #96
   2a6f4:	ret
   2a6f8:	add	x0, x19, #0x98
   2a6fc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2a700:	b	2a15c <__gmpn_rootrem@@Base+0x318>
   2a704:	mov	x20, x9
   2a708:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2a70c:	mov	x9, x20
   2a710:	b	2a6d4 <__gmpn_rootrem@@Base+0x890>
   2a714:	adrp	x0, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a718:	adrp	x2, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a71c:	add	x0, x0, #0xcb8
   2a720:	add	x2, x2, #0xcc2
   2a724:	mov	w1, #0x11b                 	// #283
   2a728:	bl	c850 <__gmp_assert_fail@plt>
   2a72c:	lsr	x8, x2, #56
   2a730:	cbnz	x8, 2a770 <__gmpn_rootrem@@Base+0x92c>
   2a734:	adrp	x9, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a738:	lsr	x8, x1, #56
   2a73c:	add	x9, x9, #0xcd4
   2a740:	ldrb	w8, [x9, x8]
   2a744:	bfi	x8, x2, #8, #56
   2a748:	udiv	x9, x8, x3
   2a74c:	lsr	x8, x9, #8
   2a750:	and	x9, x9, #0xff
   2a754:	adrp	x10, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a758:	add	x10, x10, #0xdd4
   2a75c:	ldrb	w9, [x10, x9]
   2a760:	orr	x9, x9, #0x100
   2a764:	str	x9, [x0]
   2a768:	mov	w0, w8
   2a76c:	ret
   2a770:	adrp	x9, 4f000 <__gmpn_bases@@Base+0x1f98>
   2a774:	lsr	x8, x1, #56
   2a778:	add	x9, x9, #0xcd4
   2a77c:	ldrb	w9, [x9, x8]
   2a780:	udiv	x8, x2, x3
   2a784:	msub	x10, x8, x3, x2
   2a788:	bfi	x9, x10, #8, #56
   2a78c:	udiv	x9, x9, x3
   2a790:	b	2a754 <__gmpn_rootrem@@Base+0x910>

000000000002a794 <__gmpn_sqrtrem@@Base>:
   2a794:	stp	x29, x30, [sp, #-80]!
   2a798:	stp	x26, x25, [sp, #16]
   2a79c:	stp	x24, x23, [sp, #32]
   2a7a0:	stp	x22, x21, [sp, #48]
   2a7a4:	stp	x20, x19, [sp, #64]
   2a7a8:	mov	x29, sp
   2a7ac:	sub	sp, sp, #0x20
   2a7b0:	add	x8, x2, x3, lsl #3
   2a7b4:	ldur	x21, [x8, #-8]
   2a7b8:	mov	x19, x1
   2a7bc:	mov	x20, x0
   2a7c0:	clz	x8, x21
   2a7c4:	lsr	x9, x21, #62
   2a7c8:	ubfx	x8, x8, #1, #31
   2a7cc:	cmp	x9, #0x0
   2a7d0:	csel	w23, wzr, w8, ne  // ne = any
   2a7d4:	cmp	x3, #0x2
   2a7d8:	b.eq	2a814 <__gmpn_sqrtrem@@Base+0x80>  // b.none
   2a7dc:	mov	x22, x3
   2a7e0:	cmp	x3, #0x1
   2a7e4:	b.ne	2a874 <__gmpn_sqrtrem@@Base+0xe0>  // b.any
   2a7e8:	cbz	w23, 2a924 <__gmpn_sqrtrem@@Base+0x190>
   2a7ec:	lsl	w8, w23, #1
   2a7f0:	lsl	x1, x21, x8
   2a7f4:	sub	x0, x29, #0x8
   2a7f8:	bl	2ab6c <__gmpn_sqrtrem@@Base+0x3d8>
   2a7fc:	lsr	x8, x0, x23
   2a800:	str	x8, [x20]
   2a804:	cbz	x19, 2a940 <__gmpn_sqrtrem@@Base+0x1ac>
   2a808:	msub	x8, x8, x8, x21
   2a80c:	stur	x8, [x29, #-8]
   2a810:	b	2a93c <__gmpn_sqrtrem@@Base+0x1a8>
   2a814:	cmp	x19, #0x0
   2a818:	sub	x8, x29, #0x18
   2a81c:	csel	x19, x8, x19, eq  // eq = none
   2a820:	cbz	w23, 2a950 <__gmpn_sqrtrem@@Base+0x1bc>
   2a824:	ldr	x22, [x2]
   2a828:	lsl	w8, w23, #1
   2a82c:	neg	w10, w8
   2a830:	lsl	x9, x21, x8
   2a834:	lsr	x10, x22, x10
   2a838:	lsl	x8, x22, x8
   2a83c:	orr	x9, x10, x9
   2a840:	mov	x0, x20
   2a844:	mov	x1, x19
   2a848:	mov	x2, x19
   2a84c:	stur	x22, [x29, #-8]
   2a850:	stp	x8, x9, [x19]
   2a854:	bl	2ac0c <__gmpn_sqrtrem@@Base+0x478>
   2a858:	ldr	x8, [x20]
   2a85c:	lsr	x8, x8, x23
   2a860:	str	x8, [x20]
   2a864:	msub	x8, x8, x8, x22
   2a868:	stur	x8, [x29, #-8]
   2a86c:	str	x8, [x19]
   2a870:	b	2a944 <__gmpn_sqrtrem@@Base+0x1b0>
   2a874:	add	x24, x22, #0x1
   2a878:	add	x8, x22, #0x2
   2a87c:	cmp	x24, #0x0
   2a880:	csinc	x8, x8, x22, lt  // lt = tstop
   2a884:	asr	x21, x8, #1
   2a888:	cbnz	x19, 2a8b4 <__gmpn_sqrtrem@@Base+0x120>
   2a88c:	cmp	x22, #0x9
   2a890:	b.lt	2a8b4 <__gmpn_sqrtrem@@Base+0x120>  // b.tstop
   2a894:	and	w4, w22, #0x1
   2a898:	mov	x0, x20
   2a89c:	mov	x1, x2
   2a8a0:	mov	x2, x21
   2a8a4:	mov	w3, w23
   2a8a8:	bl	2ac94 <__gmpn_sqrtrem@@Base+0x500>
   2a8ac:	sxtw	x19, w0
   2a8b0:	b	2ab1c <__gmpn_sqrtrem@@Base+0x388>
   2a8b4:	and	x26, x22, #0x1
   2a8b8:	orr	x8, x26, x23
   2a8bc:	stur	xzr, [x29, #-24]
   2a8c0:	cbz	x8, 2a974 <__gmpn_sqrtrem@@Base+0x1e0>
   2a8c4:	add	x8, x24, #0x3
   2a8c8:	cmp	x24, #0x0
   2a8cc:	lsl	x25, x21, #1
   2a8d0:	csel	x8, x8, x24, lt  // lt = tstop
   2a8d4:	add	x8, x25, x8, lsr #2
   2a8d8:	lsl	x8, x8, #3
   2a8dc:	add	x1, x8, #0x8
   2a8e0:	mov	w8, #0x7f00                	// #32512
   2a8e4:	cmp	x1, x8
   2a8e8:	b.hi	2ab44 <__gmpn_sqrtrem@@Base+0x3b0>  // b.pmore
   2a8ec:	add	x9, x1, #0xf
   2a8f0:	mov	x8, sp
   2a8f4:	and	x9, x9, #0xfffffffffffffff0
   2a8f8:	sub	x24, x8, x9
   2a8fc:	mov	sp, x24
   2a900:	add	x25, x24, x25, lsl #3
   2a904:	add	x0, x24, x26, lsl #3
   2a908:	str	xzr, [x24]
   2a90c:	cbz	w23, 2aa00 <__gmpn_sqrtrem@@Base+0x26c>
   2a910:	lsl	w3, w23, #1
   2a914:	mov	x1, x2
   2a918:	mov	x2, x22
   2a91c:	bl	c2d0 <__gmpn_lshift@plt>
   2a920:	b	2aa0c <__gmpn_sqrtrem@@Base+0x278>
   2a924:	sub	x0, x29, #0x8
   2a928:	mov	x1, x21
   2a92c:	bl	2ab6c <__gmpn_sqrtrem@@Base+0x3d8>
   2a930:	str	x0, [x20]
   2a934:	cbz	x19, 2a940 <__gmpn_sqrtrem@@Base+0x1ac>
   2a938:	ldur	x8, [x29, #-8]
   2a93c:	str	x8, [x19]
   2a940:	ldur	x8, [x29, #-8]
   2a944:	cmp	x8, #0x0
   2a948:	cset	w19, ne  // ne = any
   2a94c:	b	2ab1c <__gmpn_sqrtrem@@Base+0x388>
   2a950:	mov	x0, x20
   2a954:	mov	x1, x19
   2a958:	bl	2ac0c <__gmpn_sqrtrem@@Base+0x478>
   2a95c:	ldr	x8, [x19]
   2a960:	str	x0, [x19, #8]
   2a964:	orr	x8, x8, x0
   2a968:	cmp	x8, #0x0
   2a96c:	cinc	x19, x0, ne  // ne = any
   2a970:	b	2ab1c <__gmpn_sqrtrem@@Base+0x388>
   2a974:	cmp	x19, x2
   2a978:	b.eq	2a9a8 <__gmpn_sqrtrem@@Base+0x214>  // b.none
   2a97c:	cbnz	x19, 2a998 <__gmpn_sqrtrem@@Base+0x204>
   2a980:	lsl	x8, x22, #3
   2a984:	add	x8, x8, #0xf
   2a988:	mov	x9, sp
   2a98c:	and	x8, x8, #0xfffffffffffffff0
   2a990:	sub	x19, x9, x8
   2a994:	mov	sp, x19
   2a998:	mov	x0, x19
   2a99c:	mov	x1, x2
   2a9a0:	mov	x2, x22
   2a9a4:	bl	cc10 <__gmpn_copyi@plt>
   2a9a8:	add	x8, x24, #0x3
   2a9ac:	cmp	x24, #0x0
   2a9b0:	csel	x8, x8, x24, lt  // lt = tstop
   2a9b4:	lsl	x8, x8, #1
   2a9b8:	and	x8, x8, #0xfffffffffffffff8
   2a9bc:	add	x1, x8, #0x8
   2a9c0:	mov	w8, #0x7f00                	// #32512
   2a9c4:	cmp	x1, x8
   2a9c8:	b.hi	2ab5c <__gmpn_sqrtrem@@Base+0x3c8>  // b.pmore
   2a9cc:	add	x9, x1, #0xf
   2a9d0:	mov	x8, sp
   2a9d4:	and	x9, x9, #0xfffffffffffffff0
   2a9d8:	sub	x4, x8, x9
   2a9dc:	mov	sp, x4
   2a9e0:	mov	x0, x20
   2a9e4:	mov	x1, x19
   2a9e8:	mov	x2, x21
   2a9ec:	mov	x3, xzr
   2a9f0:	bl	2b05c <__gmpn_sqrtrem@@Base+0x8c8>
   2a9f4:	add	x20, x0, x21
   2a9f8:	str	x0, [x19, x21, lsl #3]
   2a9fc:	b	2aafc <__gmpn_sqrtrem@@Base+0x368>
   2aa00:	mov	x1, x2
   2aa04:	mov	x2, x22
   2aa08:	bl	cc10 <__gmpn_copyi@plt>
   2aa0c:	add	w23, w23, w26, lsl #5
   2aa10:	mov	x8, #0xffffffffffffffff    	// #-1
   2aa14:	mov	x9, #0xfffffffffffffffe    	// #-2
   2aa18:	lsl	x26, x8, x23
   2aa1c:	sub	x8, x9, x26
   2aa20:	cmp	x19, #0x0
   2aa24:	csel	x3, x8, xzr, eq  // eq = none
   2aa28:	mov	x0, x20
   2aa2c:	mov	x1, x24
   2aa30:	mov	x2, x21
   2aa34:	mov	x4, x25
   2aa38:	bl	2b05c <__gmpn_sqrtrem@@Base+0x8c8>
   2aa3c:	stur	x0, [x29, #-8]
   2aa40:	ldr	x8, [x20]
   2aa44:	mov	x25, x0
   2aa48:	mov	x0, x24
   2aa4c:	mov	x1, x20
   2aa50:	bic	x8, x8, x26
   2aa54:	lsl	x3, x8, #1
   2aa58:	mov	x2, x21
   2aa5c:	stur	x8, [x29, #-32]
   2aa60:	bl	d5e0 <__gmpn_addmul_1@plt>
   2aa64:	ldur	x3, [x29, #-32]
   2aa68:	add	x25, x0, x25
   2aa6c:	sub	x1, x29, #0x20
   2aa70:	mov	w2, #0x1                   	// #1
   2aa74:	mov	x0, x24
   2aa78:	stur	x25, [x29, #-8]
   2aa7c:	bl	cba0 <__gmpn_submul_1@plt>
   2aa80:	cmp	x22, #0x3
   2aa84:	mov	x3, x0
   2aa88:	b.lt	2aaa0 <__gmpn_sqrtrem@@Base+0x30c>  // b.tstop
   2aa8c:	add	x0, x24, #0x8
   2aa90:	sub	x2, x21, #0x1
   2aa94:	mov	x1, x0
   2aa98:	bl	caf0 <__gmpn_sub_1@plt>
   2aa9c:	mov	x3, x0
   2aaa0:	sub	x22, x25, x3
   2aaa4:	mov	x0, x20
   2aaa8:	mov	x1, x20
   2aaac:	mov	x2, x21
   2aab0:	mov	w3, w23
   2aab4:	stur	x22, [x29, #-8]
   2aab8:	bl	c2f0 <__gmpn_rshift@plt>
   2aabc:	cmp	x19, #0x0
   2aac0:	lsl	w8, w23, #1
   2aac4:	csel	x19, x24, x19, eq  // eq = none
   2aac8:	cmp	w23, #0x20
   2aacc:	add	x9, x24, #0x8
   2aad0:	sub	w10, w8, #0x40
   2aad4:	cinc	x20, x21, cc  // cc = lo, ul, last
   2aad8:	csel	w3, w8, w10, cc  // cc = lo, ul, last
   2aadc:	csel	x1, x24, x9, cc  // cc = lo, ul, last
   2aae0:	mov	x0, x19
   2aae4:	mov	x2, x20
   2aae8:	str	x22, [x24, x21, lsl #3]
   2aaec:	cbz	w3, 2aaf8 <__gmpn_sqrtrem@@Base+0x364>
   2aaf0:	bl	c2f0 <__gmpn_rshift@plt>
   2aaf4:	b	2aafc <__gmpn_sqrtrem@@Base+0x368>
   2aaf8:	bl	cc10 <__gmpn_copyi@plt>
   2aafc:	sub	x8, x19, #0x8
   2ab00:	mov	x19, x20
   2ab04:	subs	x20, x20, #0x1
   2ab08:	b.lt	2ab14 <__gmpn_sqrtrem@@Base+0x380>  // b.tstop
   2ab0c:	ldr	x9, [x8, x19, lsl #3]
   2ab10:	cbz	x9, 2ab00 <__gmpn_sqrtrem@@Base+0x36c>
   2ab14:	ldur	x0, [x29, #-24]
   2ab18:	cbnz	x0, 2ab3c <__gmpn_sqrtrem@@Base+0x3a8>
   2ab1c:	mov	x0, x19
   2ab20:	mov	sp, x29
   2ab24:	ldp	x20, x19, [sp, #64]
   2ab28:	ldp	x22, x21, [sp, #48]
   2ab2c:	ldp	x24, x23, [sp, #32]
   2ab30:	ldp	x26, x25, [sp, #16]
   2ab34:	ldp	x29, x30, [sp], #80
   2ab38:	ret
   2ab3c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2ab40:	b	2ab1c <__gmpn_sqrtrem@@Base+0x388>
   2ab44:	sub	x0, x29, #0x18
   2ab48:	mov	x24, x2
   2ab4c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2ab50:	mov	x2, x24
   2ab54:	mov	x24, x0
   2ab58:	b	2a900 <__gmpn_sqrtrem@@Base+0x16c>
   2ab5c:	sub	x0, x29, #0x18
   2ab60:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2ab64:	mov	x4, x0
   2ab68:	b	2a9e0 <__gmpn_sqrtrem@@Base+0x24c>
   2ab6c:	lsr	x8, x1, #55
   2ab70:	adrp	x9, 4f000 <__gmpn_bases@@Base+0x1f98>
   2ab74:	add	x9, x9, #0xed4
   2ab78:	sub	w8, w8, #0x80
   2ab7c:	ldrb	w8, [x9, w8, uxtw]
   2ab80:	lsr	x10, x1, #31
   2ab84:	mov	x9, #0x1ffff00000000       	// #562945658454016
   2ab88:	movk	x9, #0xfffd, lsl #16
   2ab8c:	orr	x8, x8, #0x100
   2ab90:	mul	x10, x8, x10
   2ab94:	msub	x9, x10, x8, x9
   2ab98:	asr	x9, x9, #16
   2ab9c:	mul	x9, x9, x8
   2aba0:	asr	x9, x9, #18
   2aba4:	lsr	x11, x1, #24
   2aba8:	add	x8, x9, x8, lsl #16
   2abac:	mul	x9, x8, x11
   2abb0:	lsl	x12, x1, #14
   2abb4:	lsr	x11, x9, #25
   2abb8:	mov	x10, #0xffffff0000000000    	// #-1099511627776
   2abbc:	msub	x11, x11, x11, x12
   2abc0:	add	x10, x11, x10
   2abc4:	asr	x10, x10, #24
   2abc8:	mul	x8, x10, x8
   2abcc:	add	x8, x9, x8, asr #15
   2abd0:	lsr	x8, x8, #32
   2abd4:	mul	x9, x8, x8
   2abd8:	lsl	x10, x8, #1
   2abdc:	sub	x12, x1, #0x1
   2abe0:	mov	w11, #0x1                   	// #1
   2abe4:	add	x10, x9, x10
   2abe8:	bfi	x11, x8, #1, #32
   2abec:	cmp	x10, x12
   2abf0:	csneg	x10, xzr, x11, hi  // hi = pmore
   2abf4:	sub	x9, x1, x9
   2abf8:	cinc	x8, x8, ls  // ls = plast
   2abfc:	add	x9, x9, x10
   2ac00:	str	x9, [x0]
   2ac04:	mov	x0, x8
   2ac08:	ret
   2ac0c:	stp	x29, x30, [sp, #-48]!
   2ac10:	str	x21, [sp, #16]
   2ac14:	stp	x20, x19, [sp, #32]
   2ac18:	mov	x20, x1
   2ac1c:	ldp	x21, x1, [x2]
   2ac20:	mov	x19, x0
   2ac24:	mov	x0, x20
   2ac28:	mov	x29, sp
   2ac2c:	bl	2ab6c <__gmpn_sqrtrem@@Base+0x3d8>
   2ac30:	ldr	x8, [x20]
   2ac34:	extr	x8, x8, x21, #33
   2ac38:	udiv	x9, x8, x0
   2ac3c:	sub	x11, x9, x9, lsr #32
   2ac40:	msub	x8, x11, x0, x8
   2ac44:	lsr	x9, x8, #31
   2ac48:	bfi	x21, x8, #33, #31
   2ac4c:	mul	x8, x11, x11
   2ac50:	subs	x10, x21, x8
   2ac54:	cset	w8, cc  // cc = lo, ul, last
   2ac58:	subs	w9, w9, w8
   2ac5c:	orr	x8, x11, x0, lsl #32
   2ac60:	b.pl	2ac78 <__gmpn_sqrtrem@@Base+0x4e4>  // b.nfrst
   2ac64:	adds	x10, x10, x8
   2ac68:	sub	x8, x8, #0x1
   2ac6c:	cinc	w9, w9, cs  // cs = hs, nlast
   2ac70:	adds	x10, x10, x8
   2ac74:	cinc	w9, w9, cs  // cs = hs, nlast
   2ac78:	str	x10, [x20]
   2ac7c:	str	x8, [x19]
   2ac80:	ldp	x20, x19, [sp, #32]
   2ac84:	ldr	x21, [sp, #16]
   2ac88:	sxtw	x0, w9
   2ac8c:	ldp	x29, x30, [sp], #48
   2ac90:	ret
   2ac94:	stp	x29, x30, [sp, #-96]!
   2ac98:	stp	x28, x27, [sp, #16]
   2ac9c:	stp	x26, x25, [sp, #32]
   2aca0:	stp	x24, x23, [sp, #48]
   2aca4:	stp	x22, x21, [sp, #64]
   2aca8:	stp	x20, x19, [sp, #80]
   2acac:	mov	x29, sp
   2acb0:	sub	sp, sp, #0x40
   2acb4:	sub	x8, x2, #0x1
   2acb8:	cmp	x8, #0x0
   2acbc:	csel	x8, x2, x8, lt  // lt = tstop
   2acc0:	asr	x23, x8, #1
   2acc4:	add	x8, x23, x2, lsl #1
   2acc8:	lsl	x8, x8, #3
   2accc:	mov	x24, x1
   2acd0:	add	x1, x8, #0x20
   2acd4:	mov	w8, #0x7f00                	// #32512
   2acd8:	mov	w21, w3
   2acdc:	mov	x19, x2
   2ace0:	mov	x20, x0
   2ace4:	cmp	x1, x8
   2ace8:	sub	x27, x2, x23
   2acec:	stur	xzr, [x29, #-8]
   2acf0:	b.hi	2b02c <__gmpn_sqrtrem@@Base+0x898>  // b.pmore
   2acf4:	add	x9, x1, #0xf
   2acf8:	mov	x8, sp
   2acfc:	and	x9, x9, #0xfffffffffffffff0
   2ad00:	sub	x22, x8, x9
   2ad04:	mov	sp, x22
   2ad08:	add	x8, x22, x19, lsl #3
   2ad0c:	add	x26, x8, #0x8
   2ad10:	stp	w4, w21, [x29, #-16]
   2ad14:	stur	x24, [x29, #-48]
   2ad18:	cbz	w21, 2ad58 <__gmpn_sqrtrem@@Base+0x5c4>
   2ad1c:	add	w8, w4, #0x1
   2ad20:	mov	x9, #0xfffffffffffffff8    	// #-8
   2ad24:	cmp	x23, x8
   2ad28:	add	x10, x24, x23, lsl #3
   2ad2c:	csel	x8, x9, xzr, gt
   2ad30:	add	x11, x19, x27
   2ad34:	add	x0, x26, x8
   2ad38:	add	x8, x10, x8
   2ad3c:	cinc	x9, x11, gt
   2ad40:	sub	x8, x8, w4, uxtw #3
   2ad44:	add	x2, x9, #0x1
   2ad48:	sub	x1, x8, #0x8
   2ad4c:	lsl	w3, w21, #1
   2ad50:	bl	c2d0 <__gmpn_lshift@plt>
   2ad54:	b	2ad74 <__gmpn_sqrtrem@@Base+0x5e0>
   2ad58:	add	x8, x24, x23, lsl #3
   2ad5c:	add	x9, x19, x27
   2ad60:	sub	x8, x8, w4, uxtw #3
   2ad64:	sub	x1, x8, #0x8
   2ad68:	add	x2, x9, #0x1
   2ad6c:	mov	x0, x26
   2ad70:	bl	cc10 <__gmpn_copyi@plt>
   2ad74:	lsl	x21, x23, #3
   2ad78:	add	x8, x26, x21
   2ad7c:	mov	x25, x20
   2ad80:	add	x24, x20, x21
   2ad84:	add	x20, x8, #0x8
   2ad88:	mov	x0, x24
   2ad8c:	mov	x1, x20
   2ad90:	mov	x2, x27
   2ad94:	mov	x3, xzr
   2ad98:	mov	x4, x22
   2ad9c:	bl	2b05c <__gmpn_sqrtrem@@Base+0x8c8>
   2ada0:	mov	x28, x0
   2ada4:	cbz	x0, 2adbc <__gmpn_sqrtrem@@Base+0x628>
   2ada8:	mov	x0, x20
   2adac:	mov	x1, x20
   2adb0:	mov	x2, x24
   2adb4:	mov	x3, x27
   2adb8:	bl	c420 <__gmpn_sub_n@plt>
   2adbc:	add	x8, x26, x19, lsl #3
   2adc0:	stp	x20, x22, [x29, #-40]
   2adc4:	add	x20, x8, #0x8
   2adc8:	add	x2, x19, #0x1
   2adcc:	mov	x0, x20
   2add0:	mov	x1, x26
   2add4:	mov	x3, x24
   2add8:	mov	x4, x27
   2addc:	mov	x5, x22
   2ade0:	stur	x26, [x29, #-24]
   2ade4:	bl	2b288 <__gmpn_sqrtrem@@Base+0xaf4>
   2ade8:	add	x22, x23, #0x1
   2adec:	ldr	x8, [x20, x22, lsl #3]
   2adf0:	add	x26, x8, x28
   2adf4:	cmp	x26, #0x2
   2adf8:	b.cc	2ae1c <__gmpn_sqrtrem@@Base+0x688>  // b.lo, b.ul, b.last
   2adfc:	mov	w1, #0xff                  	// #255
   2ae00:	mov	x0, x25
   2ae04:	mov	x2, x21
   2ae08:	mov	x20, x25
   2ae0c:	bl	c780 <memset@plt>
   2ae10:	ldp	w12, w25, [x29, #-16]
   2ae14:	mov	w21, #0x1                   	// #1
   2ae18:	b	2ae7c <__gmpn_sqrtrem@@Base+0x6e8>
   2ae1c:	add	x1, x20, #0x8
   2ae20:	mov	w3, #0x1                   	// #1
   2ae24:	mov	x0, x25
   2ae28:	mov	x2, x23
   2ae2c:	mov	w21, #0x1                   	// #1
   2ae30:	stur	x1, [x29, #-56]
   2ae34:	bl	c2f0 <__gmpn_rshift@plt>
   2ae38:	add	x8, x25, x23, lsl #3
   2ae3c:	ldur	x9, [x8, #-8]
   2ae40:	ldur	w12, [x29, #-16]
   2ae44:	mov	w10, #0x40                  	// #64
   2ae48:	orr	x9, x9, x26, lsl #63
   2ae4c:	stur	x9, [x8, #-8]
   2ae50:	ldp	x8, x9, [x20]
   2ae54:	mov	x20, x25
   2ae58:	ldur	w25, [x29, #-12]
   2ae5c:	lsr	w10, w10, w12
   2ae60:	mvn	w11, w25
   2ae64:	add	w10, w10, w11
   2ae68:	mov	x11, #0xffffffffffffffff    	// #-1
   2ae6c:	lsr	x10, x11, x10
   2ae70:	and	x9, x9, x10
   2ae74:	orr	x8, x9, x8, lsr #3
   2ae78:	cbz	x8, 2aecc <__gmpn_sqrtrem@@Base+0x738>
   2ae7c:	ldur	x0, [x29, #-8]
   2ae80:	cbnz	x0, 2b044 <__gmpn_sqrtrem@@Base+0x8b0>
   2ae84:	orr	w8, w12, w25
   2ae88:	cbz	w8, 2aea8 <__gmpn_sqrtrem@@Base+0x714>
   2ae8c:	cmp	w12, #0x0
   2ae90:	cset	w8, ne  // ne = any
   2ae94:	add	w3, w25, w8, lsl #5
   2ae98:	mov	x0, x20
   2ae9c:	mov	x1, x20
   2aea0:	mov	x2, x19
   2aea4:	bl	c2f0 <__gmpn_rshift@plt>
   2aea8:	mov	w0, w21
   2aeac:	mov	sp, x29
   2aeb0:	ldp	x20, x19, [sp, #80]
   2aeb4:	ldp	x22, x21, [sp, #64]
   2aeb8:	ldp	x24, x23, [sp, #48]
   2aebc:	ldp	x26, x25, [sp, #32]
   2aec0:	ldp	x28, x27, [sp, #16]
   2aec4:	ldp	x29, x30, [sp], #96
   2aec8:	ret
   2aecc:	ldur	x0, [x29, #-32]
   2aed0:	ldur	x3, [x29, #-56]
   2aed4:	mov	x1, x24
   2aed8:	mov	x2, x27
   2aedc:	mov	x4, x22
   2aee0:	mov	w26, w12
   2aee4:	mov	x22, x0
   2aee8:	bl	cea0 <__gmpn_mul@plt>
   2aeec:	ldur	x8, [x29, #-24]
   2aef0:	mov	x2, x22
   2aef4:	mov	x3, x27
   2aef8:	add	x28, x8, #0x8
   2aefc:	mov	x0, x28
   2af00:	mov	x1, x28
   2af04:	bl	c420 <__gmpn_sub_n@plt>
   2af08:	add	x21, x28, x27, lsl #3
   2af0c:	ldr	x8, [x21]
   2af10:	subs	x8, x8, x0
   2af14:	str	x8, [x21]
   2af18:	b.cs	2af3c <__gmpn_sqrtrem@@Base+0x7a8>  // b.hs, b.nlast
   2af1c:	lsl	x8, x19, #4
   2af20:	sub	x8, x8, x23, lsl #3
   2af24:	add	x8, x8, x22
   2af28:	add	x8, x8, #0x18
   2af2c:	ldr	x9, [x8]
   2af30:	sub	x10, x9, #0x1
   2af34:	str	x10, [x8], #8
   2af38:	cbz	x9, 2af2c <__gmpn_sqrtrem@@Base+0x798>
   2af3c:	add	x1, x22, x27, lsl #3
   2af40:	mov	x0, x21
   2af44:	mov	x2, x23
   2af48:	bl	c570 <__gmpn_cmp@plt>
   2af4c:	tbz	w0, #31, 2af8c <__gmpn_sqrtrem@@Base+0x7f8>
   2af50:	mov	x0, x28
   2af54:	mov	x1, x28
   2af58:	mov	x2, x24
   2af5c:	mov	x3, x27
   2af60:	bl	ce00 <__gmpn_addlsh1_n@plt>
   2af64:	mov	x3, x0
   2af68:	mov	x0, x21
   2af6c:	mov	x1, x21
   2af70:	mov	x2, x23
   2af74:	bl	c150 <__gmpn_add_1@plt>
   2af78:	mov	x8, x20
   2af7c:	ldr	x9, [x8]
   2af80:	sub	x10, x9, #0x1
   2af84:	str	x10, [x8], #8
   2af88:	cbz	x9, 2af7c <__gmpn_sqrtrem@@Base+0x7e8>
   2af8c:	ldur	x0, [x29, #-40]
   2af90:	sub	x1, x27, x23
   2af94:	bl	c010 <__gmpn_zero_p@plt>
   2af98:	cbz	w0, 2afe4 <__gmpn_sqrtrem@@Base+0x850>
   2af9c:	mov	x0, x22
   2afa0:	mov	x1, x20
   2afa4:	mov	x2, x23
   2afa8:	bl	ca90 <__gmpn_sqr@plt>
   2afac:	add	x1, x22, x23, lsl #3
   2afb0:	mov	x0, x28
   2afb4:	mov	x2, x23
   2afb8:	bl	c570 <__gmpn_cmp@plt>
   2afbc:	mov	w21, w0
   2afc0:	cbnz	w0, 2b008 <__gmpn_sqrtrem@@Base+0x874>
   2afc4:	cbz	w25, 2aff0 <__gmpn_sqrtrem@@Base+0x85c>
   2afc8:	ldur	x21, [x29, #-24]
   2afcc:	ldur	x1, [x29, #-48]
   2afd0:	lsl	w3, w25, #1
   2afd4:	mov	x2, x23
   2afd8:	mov	x0, x21
   2afdc:	bl	c2d0 <__gmpn_lshift@plt>
   2afe0:	b	2aff4 <__gmpn_sqrtrem@@Base+0x860>
   2afe4:	mov	w21, #0x1                   	// #1
   2afe8:	mov	w12, w26
   2afec:	b	2ae7c <__gmpn_sqrtrem@@Base+0x6e8>
   2aff0:	ldur	x21, [x29, #-48]
   2aff4:	add	x1, x22, w26, uxtw #3
   2aff8:	sub	x2, x23, w26, uxtw
   2affc:	mov	x0, x21
   2b000:	bl	c570 <__gmpn_cmp@plt>
   2b004:	mov	w21, w0
   2b008:	mov	w12, w26
   2b00c:	tbz	w21, #31, 2ae7c <__gmpn_sqrtrem@@Base+0x6e8>
   2b010:	mov	x8, x20
   2b014:	ldr	x9, [x8]
   2b018:	sub	x10, x9, #0x1
   2b01c:	str	x10, [x8], #8
   2b020:	cbz	x9, 2b014 <__gmpn_sqrtrem@@Base+0x880>
   2b024:	mov	w21, #0x1                   	// #1
   2b028:	b	2ae7c <__gmpn_sqrtrem@@Base+0x6e8>
   2b02c:	sub	x0, x29, #0x8
   2b030:	mov	w22, w4
   2b034:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2b038:	mov	w4, w22
   2b03c:	mov	x22, x0
   2b040:	b	2ad08 <__gmpn_sqrtrem@@Base+0x574>
   2b044:	mov	w22, w12
   2b048:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2b04c:	mov	w12, w22
   2b050:	orr	w8, w12, w25
   2b054:	cbnz	w8, 2ae8c <__gmpn_sqrtrem@@Base+0x6f8>
   2b058:	b	2aea8 <__gmpn_sqrtrem@@Base+0x714>
   2b05c:	sub	sp, sp, #0x90
   2b060:	cmp	x2, #0x0
   2b064:	cinc	x8, x2, lt  // lt = tstop
   2b068:	stp	x26, x25, [sp, #80]
   2b06c:	asr	x25, x8, #1
   2b070:	stp	x24, x23, [sp, #96]
   2b074:	stp	x22, x21, [sp, #112]
   2b078:	stp	x20, x19, [sp, #128]
   2b07c:	sub	x22, x2, x25
   2b080:	add	x24, x0, x25, lsl #3
   2b084:	add	x19, x1, x25, lsl #4
   2b088:	stp	x29, x30, [sp, #48]
   2b08c:	stp	x28, x27, [sp, #64]
   2b090:	add	x29, sp, #0x30
   2b094:	mov	x27, x4
   2b098:	mov	x23, x2
   2b09c:	mov	x21, x1
   2b0a0:	mov	x20, x0
   2b0a4:	cmp	x22, #0x1
   2b0a8:	mov	x0, x24
   2b0ac:	mov	x1, x19
   2b0b0:	stur	x3, [x29, #-8]
   2b0b4:	str	x8, [sp, #24]
   2b0b8:	b.ne	2b0d0 <__gmpn_sqrtrem@@Base+0x93c>  // b.any
   2b0bc:	mov	x2, x19
   2b0c0:	bl	2ac0c <__gmpn_sqrtrem@@Base+0x478>
   2b0c4:	mov	x26, x0
   2b0c8:	cbnz	x0, 2b0e8 <__gmpn_sqrtrem@@Base+0x954>
   2b0cc:	b	2b0fc <__gmpn_sqrtrem@@Base+0x968>
   2b0d0:	mov	x2, x22
   2b0d4:	mov	x3, xzr
   2b0d8:	mov	x4, x27
   2b0dc:	bl	2b05c <__gmpn_sqrtrem@@Base+0x8c8>
   2b0e0:	mov	x26, x0
   2b0e4:	cbz	x0, 2b0fc <__gmpn_sqrtrem@@Base+0x968>
   2b0e8:	mov	x0, x19
   2b0ec:	mov	x1, x19
   2b0f0:	mov	x2, x24
   2b0f4:	mov	x3, x22
   2b0f8:	bl	c420 <__gmpn_sub_n@plt>
   2b0fc:	str	x19, [sp, #8]
   2b100:	lsl	x19, x25, #3
   2b104:	add	x28, x21, x19
   2b108:	mov	x0, x27
   2b10c:	mov	x1, x28
   2b110:	mov	x2, xzr
   2b114:	mov	x3, x28
   2b118:	mov	x4, x23
   2b11c:	mov	x5, x24
   2b120:	mov	x6, x22
   2b124:	stur	x23, [x29, #-16]
   2b128:	str	x24, [sp, #16]
   2b12c:	bl	c030 <__gmpn_tdiv_qr@plt>
   2b130:	ldr	x8, [x27, x19]
   2b134:	ldr	x23, [x27]
   2b138:	mov	w3, #0x1                   	// #1
   2b13c:	mov	x0, x20
   2b140:	mov	x1, x27
   2b144:	mov	x2, x25
   2b148:	add	x24, x8, x26
   2b14c:	mov	w26, #0x1                   	// #1
   2b150:	bl	c2f0 <__gmpn_rshift@plt>
   2b154:	add	x8, x19, x20
   2b158:	ldur	x9, [x8, #-8]
   2b15c:	orr	x9, x9, x24, lsl #63
   2b160:	stur	x9, [x8, #-8]
   2b164:	ldr	x8, [x20]
   2b168:	ldur	x9, [x29, #-8]
   2b16c:	tst	x8, x9
   2b170:	b.ne	2b264 <__gmpn_sqrtrem@@Base+0xad0>  // b.any
   2b174:	ldr	x8, [sp, #24]
   2b178:	lsr	x26, x24, #1
   2b17c:	and	x27, x8, #0xfffffffffffffffe
   2b180:	tbnz	w23, #0, 2b18c <__gmpn_sqrtrem@@Base+0x9f8>
   2b184:	mov	x23, xzr
   2b188:	b	2b1a4 <__gmpn_sqrtrem@@Base+0xa10>
   2b18c:	ldr	x2, [sp, #16]
   2b190:	mov	x0, x28
   2b194:	mov	x1, x28
   2b198:	mov	x3, x22
   2b19c:	bl	cc30 <__gmpn_add_n@plt>
   2b1a0:	sxtw	x23, w0
   2b1a4:	ldur	x24, [x29, #-16]
   2b1a8:	mov	x1, x20
   2b1ac:	mov	x2, x25
   2b1b0:	add	x28, x21, x24, lsl #3
   2b1b4:	mov	x0, x28
   2b1b8:	bl	ca90 <__gmpn_sqr@plt>
   2b1bc:	mov	x0, x21
   2b1c0:	mov	x1, x21
   2b1c4:	mov	x2, x28
   2b1c8:	mov	x3, x27
   2b1cc:	bl	c420 <__gmpn_sub_n@plt>
   2b1d0:	add	w8, w0, w26
   2b1d4:	cmp	x25, x22
   2b1d8:	sxtw	x3, w8
   2b1dc:	b.eq	2b1f4 <__gmpn_sqrtrem@@Base+0xa60>  // b.none
   2b1e0:	ldr	x0, [sp, #8]
   2b1e4:	mov	w2, #0x1                   	// #1
   2b1e8:	mov	x1, x0
   2b1ec:	bl	caf0 <__gmpn_sub_1@plt>
   2b1f0:	mov	x3, x0
   2b1f4:	sub	x23, x23, x3
   2b1f8:	tbz	w23, #31, 2b260 <__gmpn_sqrtrem@@Base+0xacc>
   2b1fc:	ldr	x0, [sp, #16]
   2b200:	mov	x2, x22
   2b204:	mov	x3, x26
   2b208:	mov	x1, x0
   2b20c:	bl	c150 <__gmpn_add_1@plt>
   2b210:	mov	x22, x0
   2b214:	mov	x0, x21
   2b218:	mov	x1, x21
   2b21c:	mov	x2, x20
   2b220:	mov	x3, x24
   2b224:	bl	ce00 <__gmpn_addlsh1_n@plt>
   2b228:	add	w8, w23, w22, lsl #1
   2b22c:	add	w19, w8, w0
   2b230:	mov	w3, #0x1                   	// #1
   2b234:	mov	x0, x21
   2b238:	mov	x1, x21
   2b23c:	mov	x2, x24
   2b240:	bl	caf0 <__gmpn_sub_1@plt>
   2b244:	sxtw	x8, w19
   2b248:	sub	x23, x8, x0
   2b24c:	mov	w3, #0x1                   	// #1
   2b250:	mov	x0, x20
   2b254:	mov	x1, x20
   2b258:	mov	x2, x24
   2b25c:	bl	caf0 <__gmpn_sub_1@plt>
   2b260:	sxtw	x26, w23
   2b264:	mov	x0, x26
   2b268:	ldp	x20, x19, [sp, #128]
   2b26c:	ldp	x22, x21, [sp, #112]
   2b270:	ldp	x24, x23, [sp, #96]
   2b274:	ldp	x26, x25, [sp, #80]
   2b278:	ldp	x28, x27, [sp, #64]
   2b27c:	ldp	x29, x30, [sp, #48]
   2b280:	add	sp, sp, #0x90
   2b284:	ret
   2b288:	stp	x29, x30, [sp, #-80]!
   2b28c:	stp	x26, x25, [sp, #16]
   2b290:	stp	x24, x23, [sp, #32]
   2b294:	stp	x22, x21, [sp, #48]
   2b298:	stp	x20, x19, [sp, #64]
   2b29c:	mov	x29, sp
   2b2a0:	sub	sp, sp, #0x10
   2b2a4:	mov	x21, x0
   2b2a8:	mov	x0, x5
   2b2ac:	mov	x24, x5
   2b2b0:	mov	x19, x4
   2b2b4:	mov	x22, x3
   2b2b8:	mov	x20, x2
   2b2bc:	mov	x23, x1
   2b2c0:	bl	cc10 <__gmpn_copyi@plt>
   2b2c4:	add	x26, x22, x19, lsl #3
   2b2c8:	ldur	x25, [x26, #-8]
   2b2cc:	mov	x0, x25
   2b2d0:	bl	d5d0 <__gmpn_invert_limb@plt>
   2b2d4:	ldur	x8, [x26, #-16]
   2b2d8:	mul	x9, x0, x25
   2b2dc:	adds	x9, x9, x8
   2b2e0:	b.cc	2b2fc <__gmpn_sqrtrem@@Base+0xb68>  // b.lo, b.ul, b.last
   2b2e4:	subs	x9, x9, x25
   2b2e8:	cset	w10, cs  // cs = hs, nlast
   2b2ec:	csel	x11, x25, xzr, cs  // cs = hs, nlast
   2b2f0:	mvn	x10, x10
   2b2f4:	add	x0, x10, x0
   2b2f8:	sub	x9, x9, x11
   2b2fc:	umulh	x10, x8, x0
   2b300:	adds	x9, x10, x9
   2b304:	b.cc	2b32c <__gmpn_sqrtrem@@Base+0xb98>  // b.lo, b.ul, b.last
   2b308:	cmp	x9, x25
   2b30c:	sub	x5, x0, #0x1
   2b310:	b.cc	2b330 <__gmpn_sqrtrem@@Base+0xb9c>  // b.lo, b.ul, b.last
   2b314:	mul	x10, x0, x8
   2b318:	cmp	x9, x25
   2b31c:	sub	x11, x0, #0x2
   2b320:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2b324:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2b328:	b	2b330 <__gmpn_sqrtrem@@Base+0xb9c>
   2b32c:	mov	x5, x0
   2b330:	cmp	x19, #0x97
   2b334:	stur	x5, [x29, #-8]
   2b338:	b.le	2b3ac <__gmpn_sqrtrem@@Base+0xc18>
   2b33c:	cmp	x19, #0x3e5
   2b340:	b.le	2b3c8 <__gmpn_sqrtrem@@Base+0xc34>
   2b344:	mov	x0, x20
   2b348:	mov	x1, x19
   2b34c:	mov	w2, wzr
   2b350:	bl	c220 <__gmpn_mu_divappr_q_itch@plt>
   2b354:	lsl	x1, x0, #3
   2b358:	mov	w8, #0x7f00                	// #32512
   2b35c:	cmp	x1, x8
   2b360:	stur	xzr, [x29, #-16]
   2b364:	b.hi	2b40c <__gmpn_sqrtrem@@Base+0xc78>  // b.pmore
   2b368:	add	x9, x1, #0xf
   2b36c:	mov	x8, sp
   2b370:	and	x9, x9, #0xfffffffffffffff0
   2b374:	sub	x5, x8, x9
   2b378:	mov	sp, x5
   2b37c:	mov	x0, x21
   2b380:	mov	x1, x23
   2b384:	mov	x2, x20
   2b388:	mov	x3, x22
   2b38c:	mov	x4, x19
   2b390:	bl	c8a0 <__gmpn_mu_divappr_q@plt>
   2b394:	ldur	x8, [x29, #-16]
   2b398:	mov	x22, x0
   2b39c:	cbz	x8, 2b3e8 <__gmpn_sqrtrem@@Base+0xc54>
   2b3a0:	mov	x0, x8
   2b3a4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2b3a8:	b	2b3e8 <__gmpn_sqrtrem@@Base+0xc54>
   2b3ac:	mov	x0, x21
   2b3b0:	mov	x1, x24
   2b3b4:	mov	x2, x20
   2b3b8:	mov	x3, x22
   2b3bc:	mov	x4, x19
   2b3c0:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   2b3c4:	b	2b3e4 <__gmpn_sqrtrem@@Base+0xc50>
   2b3c8:	sub	x5, x29, #0x8
   2b3cc:	mov	x0, x21
   2b3d0:	mov	x1, x24
   2b3d4:	mov	x2, x20
   2b3d8:	mov	x3, x22
   2b3dc:	mov	x4, x19
   2b3e0:	bl	c640 <__gmpn_dcpi1_divappr_q@plt>
   2b3e4:	mov	x22, x0
   2b3e8:	sub	x8, x20, x19
   2b3ec:	str	x22, [x21, x8, lsl #3]
   2b3f0:	mov	sp, x29
   2b3f4:	ldp	x20, x19, [sp, #64]
   2b3f8:	ldp	x22, x21, [sp, #48]
   2b3fc:	ldp	x24, x23, [sp, #32]
   2b400:	ldp	x26, x25, [sp, #16]
   2b404:	ldp	x29, x30, [sp], #80
   2b408:	ret
   2b40c:	sub	x0, x29, #0x10
   2b410:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2b414:	mov	x5, x0
   2b418:	b	2b37c <__gmpn_sqrtrem@@Base+0xbe8>

000000000002b41c <__gmpn_sizeinbase@@Base>:
   2b41c:	cbz	x1, 2b468 <__gmpn_sizeinbase@@Base+0x4c>
   2b420:	add	x8, x0, x1, lsl #3
   2b424:	ldur	x8, [x8, #-8]
   2b428:	adrp	x11, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2b42c:	ldr	x11, [x11, #3936]
   2b430:	lsl	x9, x1, #6
   2b434:	sub	w10, w2, #0x1
   2b438:	clz	x8, x8
   2b43c:	tst	w2, w10
   2b440:	sub	x8, x9, x8
   2b444:	sxtw	x9, w2
   2b448:	mov	w10, #0x28                  	// #40
   2b44c:	madd	x9, x9, x10, x11
   2b450:	b.ne	2b470 <__gmpn_sizeinbase@@Base+0x54>  // b.any
   2b454:	ldrsw	x9, [x9, #24]
   2b458:	add	x8, x8, x9
   2b45c:	sub	x8, x8, #0x1
   2b460:	udiv	x0, x8, x9
   2b464:	ret
   2b468:	mov	w0, #0x1                   	// #1
   2b46c:	ret
   2b470:	ldr	x9, [x9, #8]
   2b474:	add	x9, x9, #0x1
   2b478:	umulh	x8, x9, x8
   2b47c:	add	x0, x8, #0x1
   2b480:	ret

000000000002b484 <__gmpn_get_str@@Base>:
   2b484:	stp	x29, x30, [sp, #-80]!
   2b488:	stp	x28, x25, [sp, #16]
   2b48c:	stp	x24, x23, [sp, #32]
   2b490:	stp	x22, x21, [sp, #48]
   2b494:	stp	x20, x19, [sp, #64]
   2b498:	mov	x29, sp
   2b49c:	sub	sp, sp, #0xa10
   2b4a0:	mov	x19, x0
   2b4a4:	cbz	x3, 2b568 <__gmpn_get_str@@Base+0xe4>
   2b4a8:	sub	w8, w1, #0x1
   2b4ac:	mov	x21, x3
   2b4b0:	mov	x20, x2
   2b4b4:	mov	w22, w1
   2b4b8:	tst	w1, w8
   2b4bc:	b.ne	2b574 <__gmpn_get_str@@Base+0xf0>  // b.any
   2b4c0:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2b4c4:	ldr	x9, [x9, #3936]
   2b4c8:	mov	w11, #0x28                  	// #40
   2b4cc:	sub	x8, x21, #0x1
   2b4d0:	ldr	x10, [x20, x8, lsl #3]
   2b4d4:	smaddl	x9, w22, w11, x9
   2b4d8:	ldr	x9, [x9, #24]
   2b4dc:	lsl	x12, x21, #6
   2b4e0:	clz	x13, x10
   2b4e4:	sub	x12, x12, x13
   2b4e8:	sxtw	x13, w9
   2b4ec:	udiv	x13, x12, x13
   2b4f0:	msub	w13, w13, w9, w12
   2b4f4:	mov	w11, #0xffffffff            	// #-1
   2b4f8:	cmp	w13, #0x0
   2b4fc:	sub	w13, w9, w13
   2b500:	lsl	w11, w11, w9
   2b504:	sub	w12, w12, w8, lsl #6
   2b508:	csel	w13, wzr, w13, eq  // eq = none
   2b50c:	add	w14, w12, w13
   2b510:	eor	w11, w11, #0xff
   2b514:	mov	x12, x19
   2b518:	subs	w13, w14, w9
   2b51c:	b.mi	2b534 <__gmpn_get_str@@Base+0xb0>  // b.first
   2b520:	lsr	x14, x10, x13
   2b524:	and	w14, w14, w11
   2b528:	subs	w13, w13, w9
   2b52c:	strb	w14, [x12], #1
   2b530:	b.pl	2b520 <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   2b534:	subs	x8, x8, #0x1
   2b538:	b.lt	2b610 <__gmpn_get_str@@Base+0x18c>  // b.tstop
   2b53c:	neg	w14, w13
   2b540:	lsl	x14, x10, x14
   2b544:	ldr	x10, [x20, x8, lsl #3]
   2b548:	and	w15, w14, w11
   2b54c:	add	w14, w13, #0x40
   2b550:	lsr	x13, x10, x13
   2b554:	orr	w13, w13, w15
   2b558:	strb	w13, [x12], #1
   2b55c:	subs	w13, w14, w9
   2b560:	b.pl	2b520 <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   2b564:	b	2b534 <__gmpn_get_str@@Base+0xb0>
   2b568:	strb	wzr, [x19]
   2b56c:	mov	w19, #0x1                   	// #1
   2b570:	b	2b634 <__gmpn_get_str@@Base+0x1b0>
   2b574:	cmp	x21, #0x1c
   2b578:	b.le	2b618 <__gmpn_get_str@@Base+0x194>
   2b57c:	lsl	x23, x21, #3
   2b580:	add	x1, x23, #0x400
   2b584:	add	x0, sp, #0x8
   2b588:	str	xzr, [sp, #8]
   2b58c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2b590:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2b594:	ldr	x8, [x8, #3936]
   2b598:	mov	w24, #0x28                  	// #40
   2b59c:	lsl	x10, x21, #6
   2b5a0:	mov	x1, x0
   2b5a4:	smaddl	x8, w22, w24, x8
   2b5a8:	ldr	x9, [x8, #8]
   2b5ac:	ldrsw	x8, [x8]
   2b5b0:	umulh	x9, x9, x10
   2b5b4:	add	x0, sp, #0x10
   2b5b8:	mov	w3, w22
   2b5bc:	udiv	x8, x9, x8
   2b5c0:	add	x2, x8, #0x1
   2b5c4:	add	x25, sp, #0x10
   2b5c8:	bl	ccd0 <__gmpn_compute_powtab@plt>
   2b5cc:	mov	x22, x0
   2b5d0:	add	x1, x23, #0x200
   2b5d4:	add	x0, sp, #0x8
   2b5d8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2b5dc:	mov	x5, x0
   2b5e0:	smaddl	x4, w22, w24, x25
   2b5e4:	mov	x0, x19
   2b5e8:	mov	x1, xzr
   2b5ec:	mov	x2, x20
   2b5f0:	mov	x3, x21
   2b5f4:	bl	2b904 <__gmpn_get_str@@Base+0x480>
   2b5f8:	ldr	x8, [sp, #8]
   2b5fc:	sub	x19, x0, x19
   2b600:	cbz	x8, 2b634 <__gmpn_get_str@@Base+0x1b0>
   2b604:	mov	x0, x8
   2b608:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2b60c:	b	2b634 <__gmpn_get_str@@Base+0x1b0>
   2b610:	sub	x19, x12, x19
   2b614:	b	2b634 <__gmpn_get_str@@Base+0x1b0>
   2b618:	mov	x0, x19
   2b61c:	mov	x1, xzr
   2b620:	mov	x2, x20
   2b624:	mov	x3, x21
   2b628:	mov	w4, w22
   2b62c:	bl	2b654 <__gmpn_get_str@@Base+0x1d0>
   2b630:	sub	x19, x0, x19
   2b634:	mov	x0, x19
   2b638:	add	sp, sp, #0xa10
   2b63c:	ldp	x20, x19, [sp, #64]
   2b640:	ldp	x22, x21, [sp, #48]
   2b644:	ldp	x24, x23, [sp, #32]
   2b648:	ldp	x28, x25, [sp, #16]
   2b64c:	ldp	x29, x30, [sp], #80
   2b650:	ret
   2b654:	stp	x29, x30, [sp, #-96]!
   2b658:	stp	x28, x27, [sp, #16]
   2b65c:	stp	x26, x25, [sp, #32]
   2b660:	stp	x24, x23, [sp, #48]
   2b664:	stp	x22, x21, [sp, #64]
   2b668:	stp	x20, x19, [sp, #80]
   2b66c:	mov	x29, sp
   2b670:	sub	sp, sp, #0x5a0
   2b674:	mov	x21, x3
   2b678:	mov	x20, x1
   2b67c:	cmp	w4, #0xa
   2b680:	mov	x19, x0
   2b684:	b.ne	2b7b0 <__gmpn_get_str@@Base+0x32c>  // b.any
   2b688:	add	x23, sp, #0x10
   2b68c:	add	x22, x23, #0x8
   2b690:	mov	x0, x22
   2b694:	mov	x1, x2
   2b698:	mov	x2, x21
   2b69c:	bl	cc10 <__gmpn_copyi@plt>
   2b6a0:	add	x8, sp, #0xf8
   2b6a4:	cmp	x21, #0x2
   2b6a8:	add	x26, x8, #0x49d
   2b6ac:	b.lt	2b77c <__gmpn_get_str@@Base+0x2f8>  // b.tstop
   2b6b0:	mov	w24, #0xa                   	// #10
   2b6b4:	mov	w25, #0x64                  	// #100
   2b6b8:	mov	w27, #0x3e8                 	// #1000
   2b6bc:	mov	w28, #0x2710                	// #10000
   2b6c0:	mov	x5, #0xc34a                	// #49994
   2b6c4:	mov	x4, #0x89e80000            	// #2313682944
   2b6c8:	movk	x5, #0x6d2a, lsl #16
   2b6cc:	movk	x4, #0x2304, lsl #32
   2b6d0:	movk	x5, #0x94fb, lsl #32
   2b6d4:	add	x0, sp, #0x10
   2b6d8:	mov	w1, #0x1                   	// #1
   2b6dc:	movk	x4, #0x8ac7, lsl #48
   2b6e0:	movk	x5, #0xd83c, lsl #48
   2b6e4:	mov	x2, x22
   2b6e8:	mov	x3, x21
   2b6ec:	mov	w6, wzr
   2b6f0:	bl	ceb0 <__gmpn_preinv_divrem_1@plt>
   2b6f4:	ldr	x10, [sp, #16]
   2b6f8:	ldr	x9, [x23, x21, lsl #3]
   2b6fc:	mov	x8, xzr
   2b700:	add	x10, x10, #0x1
   2b704:	umulh	x11, x10, x24
   2b708:	sturb	w11, [x26, #-19]
   2b70c:	mul	x11, x10, x25
   2b710:	umulh	x11, x11, x24
   2b714:	sturb	w11, [x26, #-17]
   2b718:	mul	x11, x10, x27
   2b71c:	umulh	x11, x11, x24
   2b720:	cmp	x9, #0x0
   2b724:	sturb	w11, [x26, #-16]
   2b728:	add	x11, x10, x10, lsl #2
   2b72c:	mul	x10, x10, x28
   2b730:	csetm	x9, eq  // eq = none
   2b734:	lsl	x11, x11, #1
   2b738:	lsr	x10, x10, #4
   2b73c:	umulh	x11, x11, x24
   2b740:	sturb	w11, [x26, #-18]
   2b744:	add	x10, x10, x10, lsl #2
   2b748:	add	x11, x26, x8
   2b74c:	add	x8, x8, #0x1
   2b750:	lsl	x12, x10, #1
   2b754:	ubfx	x10, x10, #59, #4
   2b758:	cmp	w8, #0xf
   2b75c:	sturb	w10, [x11, #-15]
   2b760:	and	x10, x12, #0xffffffffffffffe
   2b764:	b.ne	2b744 <__gmpn_get_str@@Base+0x2c0>  // b.any
   2b768:	add	x21, x21, x9
   2b76c:	add	x8, x26, x8
   2b770:	cmp	x21, #0x1
   2b774:	sub	x26, x8, #0x22
   2b778:	b.gt	2b6c0 <__gmpn_get_str@@Base+0x23c>
   2b77c:	ldr	x8, [sp, #24]
   2b780:	cbz	x8, 2b890 <__gmpn_get_str@@Base+0x40c>
   2b784:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   2b788:	movk	x9, #0xcccd
   2b78c:	mov	w10, #0xfffffff6            	// #-10
   2b790:	umulh	x11, x8, x9
   2b794:	lsr	x11, x11, #3
   2b798:	madd	w12, w11, w10, w8
   2b79c:	cmp	x8, #0xa
   2b7a0:	strb	w12, [x26, #-1]!
   2b7a4:	mov	x8, x11
   2b7a8:	b.cs	2b790 <__gmpn_get_str@@Base+0x30c>  // b.hs, b.nlast
   2b7ac:	b	2b890 <__gmpn_get_str@@Base+0x40c>
   2b7b0:	add	x8, sp, #0x10
   2b7b4:	add	x22, x8, #0x8
   2b7b8:	mov	w23, w4
   2b7bc:	mov	x0, x22
   2b7c0:	mov	x1, x2
   2b7c4:	mov	x2, x21
   2b7c8:	sxtw	x27, w23
   2b7cc:	bl	cc10 <__gmpn_copyi@plt>
   2b7d0:	add	x8, sp, #0xf8
   2b7d4:	cmp	x21, #0x2
   2b7d8:	add	x26, x8, #0x49d
   2b7dc:	b.lt	2b870 <__gmpn_get_str@@Base+0x3ec>  // b.tstop
   2b7e0:	str	x22, [sp, #8]
   2b7e4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2b7e8:	ldr	x8, [x8, #3936]
   2b7ec:	mov	w9, #0x28                  	// #40
   2b7f0:	smaddl	x8, w23, w9, x8
   2b7f4:	ldr	w28, [x8]
   2b7f8:	ldp	x23, x8, [x8, #24]
   2b7fc:	neg	x22, x28
   2b800:	clz	x25, x23
   2b804:	neg	x24, x28, lsl #1
   2b808:	str	x8, [sp]
   2b80c:	ldp	x5, x2, [sp]
   2b810:	add	x0, sp, #0x10
   2b814:	mov	w1, #0x1                   	// #1
   2b818:	mov	x3, x21
   2b81c:	mov	x4, x23
   2b820:	mov	w6, w25
   2b824:	bl	ceb0 <__gmpn_preinv_divrem_1@plt>
   2b828:	add	x8, sp, #0x10
   2b82c:	ldr	x9, [x8, x21, lsl #3]
   2b830:	ldr	x10, [sp, #16]
   2b834:	add	x8, x26, x22
   2b838:	add	x26, x26, x24
   2b83c:	cmp	x9, #0x0
   2b840:	add	x10, x10, #0x1
   2b844:	csetm	x9, eq  // eq = none
   2b848:	mov	w11, w28
   2b84c:	umulh	x12, x10, x27
   2b850:	mul	x10, x10, x27
   2b854:	subs	w11, w11, #0x1
   2b858:	strb	w12, [x8], #1
   2b85c:	add	x26, x26, #0x1
   2b860:	b.ne	2b84c <__gmpn_get_str@@Base+0x3c8>  // b.any
   2b864:	add	x21, x21, x9
   2b868:	cmp	x21, #0x1
   2b86c:	b.gt	2b80c <__gmpn_get_str@@Base+0x388>
   2b870:	ldr	x8, [sp, #24]
   2b874:	cbz	x8, 2b890 <__gmpn_get_str@@Base+0x40c>
   2b878:	udiv	x9, x8, x27
   2b87c:	msub	w10, w9, w27, w8
   2b880:	cmp	x8, x27
   2b884:	strb	w10, [x26, #-1]!
   2b888:	mov	x8, x9
   2b88c:	b.cs	2b878 <__gmpn_get_str@@Base+0x3f4>  // b.hs, b.nlast
   2b890:	add	x8, sp, #0xf8
   2b894:	add	x21, x8, #0x49d
   2b898:	sub	x22, x21, x26
   2b89c:	cmp	x22, x20
   2b8a0:	b.cs	2b8c8 <__gmpn_get_str@@Base+0x444>  // b.hs, b.nlast
   2b8a4:	add	x8, x26, x20
   2b8a8:	sub	x2, x8, x21
   2b8ac:	mov	x0, x19
   2b8b0:	mov	w1, wzr
   2b8b4:	bl	c780 <memset@plt>
   2b8b8:	sub	x20, x20, #0x1
   2b8bc:	cmp	x22, x20
   2b8c0:	add	x19, x19, #0x1
   2b8c4:	b.cc	2b8b8 <__gmpn_get_str@@Base+0x434>  // b.lo, b.ul, b.last
   2b8c8:	cbz	x22, 2b8e0 <__gmpn_get_str@@Base+0x45c>
   2b8cc:	sub	x8, x26, x21
   2b8d0:	ldrb	w9, [x26], #1
   2b8d4:	adds	x8, x8, #0x1
   2b8d8:	strb	w9, [x19], #1
   2b8dc:	b.cc	2b8d0 <__gmpn_get_str@@Base+0x44c>  // b.lo, b.ul, b.last
   2b8e0:	mov	x0, x19
   2b8e4:	add	sp, sp, #0x5a0
   2b8e8:	ldp	x20, x19, [sp, #80]
   2b8ec:	ldp	x22, x21, [sp, #64]
   2b8f0:	ldp	x24, x23, [sp, #48]
   2b8f4:	ldp	x26, x25, [sp, #32]
   2b8f8:	ldp	x28, x27, [sp, #16]
   2b8fc:	ldp	x29, x30, [sp], #96
   2b900:	ret
   2b904:	stp	x29, x30, [sp, #-96]!
   2b908:	stp	x26, x25, [sp, #32]
   2b90c:	stp	x24, x23, [sp, #48]
   2b910:	stp	x22, x21, [sp, #64]
   2b914:	stp	x20, x19, [sp, #80]
   2b918:	mov	x23, x4
   2b91c:	mov	x25, x3
   2b920:	mov	x21, x2
   2b924:	mov	x22, x1
   2b928:	cmp	x3, #0xe
   2b92c:	mov	x19, x0
   2b930:	stp	x28, x27, [sp, #16]
   2b934:	mov	x29, sp
   2b938:	b.le	2b968 <__gmpn_get_str@@Base+0x4e4>
   2b93c:	ldp	x26, x28, [x23, #8]
   2b940:	mov	x20, x5
   2b944:	add	x24, x28, x26
   2b948:	cmp	x24, x25
   2b94c:	b.le	2b988 <__gmpn_get_str@@Base+0x504>
   2b950:	sub	x4, x23, #0x28
   2b954:	mov	x0, x19
   2b958:	mov	x1, x22
   2b95c:	mov	x2, x21
   2b960:	mov	x3, x25
   2b964:	b	2ba38 <__gmpn_get_str@@Base+0x5b4>
   2b968:	cbz	x25, 2b9e8 <__gmpn_get_str@@Base+0x564>
   2b96c:	ldr	w4, [x23, #32]
   2b970:	mov	x0, x19
   2b974:	mov	x1, x22
   2b978:	mov	x2, x21
   2b97c:	mov	x3, x25
   2b980:	bl	2b654 <__gmpn_get_str@@Base+0x1d0>
   2b984:	b	2ba40 <__gmpn_get_str@@Base+0x5bc>
   2b988:	ldr	x27, [x23]
   2b98c:	b.ne	2b9a4 <__gmpn_get_str@@Base+0x520>  // b.any
   2b990:	add	x0, x21, x28, lsl #3
   2b994:	sub	x2, x25, x28
   2b998:	mov	x1, x27
   2b99c:	bl	c570 <__gmpn_cmp@plt>
   2b9a0:	tbnz	w0, #31, 2b950 <__gmpn_get_str@@Base+0x4cc>
   2b9a4:	add	x1, x21, x28, lsl #3
   2b9a8:	sub	x25, x25, x28
   2b9ac:	mov	x0, x20
   2b9b0:	mov	x2, xzr
   2b9b4:	mov	x3, x1
   2b9b8:	mov	x4, x25
   2b9bc:	mov	x5, x27
   2b9c0:	mov	x6, x26
   2b9c4:	bl	c030 <__gmpn_tdiv_qr@plt>
   2b9c8:	sub	x8, x25, x26
   2b9cc:	ldr	x9, [x20, x8, lsl #3]
   2b9d0:	cmp	x9, #0x0
   2b9d4:	cinc	x3, x8, ne  // ne = any
   2b9d8:	cbz	x22, 2ba0c <__gmpn_get_str@@Base+0x588>
   2b9dc:	ldr	x8, [x23, #24]
   2b9e0:	sub	x1, x22, x8
   2b9e4:	b	2ba10 <__gmpn_get_str@@Base+0x58c>
   2b9e8:	cbz	x22, 2ba44 <__gmpn_get_str@@Base+0x5c0>
   2b9ec:	mov	x0, x19
   2b9f0:	mov	w1, wzr
   2b9f4:	mov	x2, x22
   2b9f8:	bl	c780 <memset@plt>
   2b9fc:	subs	x22, x22, #0x1
   2ba00:	add	x19, x19, #0x1
   2ba04:	b.ne	2b9fc <__gmpn_get_str@@Base+0x578>  // b.any
   2ba08:	b	2ba44 <__gmpn_get_str@@Base+0x5c0>
   2ba0c:	mov	x1, xzr
   2ba10:	sub	x22, x23, #0x28
   2ba14:	add	x5, x20, x3, lsl #3
   2ba18:	mov	x0, x19
   2ba1c:	mov	x2, x20
   2ba20:	mov	x4, x22
   2ba24:	bl	2b904 <__gmpn_get_str@@Base+0x480>
   2ba28:	ldr	x1, [x23, #24]
   2ba2c:	mov	x2, x21
   2ba30:	mov	x3, x24
   2ba34:	mov	x4, x22
   2ba38:	mov	x5, x20
   2ba3c:	bl	2b904 <__gmpn_get_str@@Base+0x480>
   2ba40:	mov	x19, x0
   2ba44:	mov	x0, x19
   2ba48:	ldp	x20, x19, [sp, #80]
   2ba4c:	ldp	x22, x21, [sp, #64]
   2ba50:	ldp	x24, x23, [sp, #48]
   2ba54:	ldp	x26, x25, [sp, #32]
   2ba58:	ldp	x28, x27, [sp, #16]
   2ba5c:	ldp	x29, x30, [sp], #96
   2ba60:	ret

000000000002ba64 <__gmpn_set_str@@Base>:
   2ba64:	stp	x29, x30, [sp, #-96]!
   2ba68:	str	x28, [sp, #16]
   2ba6c:	stp	x26, x25, [sp, #32]
   2ba70:	stp	x24, x23, [sp, #48]
   2ba74:	stp	x22, x21, [sp, #64]
   2ba78:	stp	x20, x19, [sp, #80]
   2ba7c:	mov	x29, sp
   2ba80:	sub	sp, sp, #0xa00
   2ba84:	sub	w8, w3, #0x1
   2ba88:	mov	w22, w3
   2ba8c:	mov	x21, x2
   2ba90:	mov	x20, x1
   2ba94:	tst	w3, w8
   2ba98:	mov	x19, x0
   2ba9c:	b.ne	2bacc <__gmpn_set_str@@Base+0x68>  // b.any
   2baa0:	add	x8, x20, x21
   2baa4:	sub	x9, x8, #0x1
   2baa8:	cmp	x9, x20
   2baac:	b.cs	2bb60 <__gmpn_set_str@@Base+0xfc>  // b.hs, b.nlast
   2bab0:	mov	x0, xzr
   2bab4:	mov	x8, xzr
   2bab8:	cbz	x8, 2bbd4 <__gmpn_set_str@@Base+0x170>
   2babc:	add	x9, x0, #0x1
   2bac0:	str	x8, [x19, x0, lsl #3]
   2bac4:	mov	x0, x9
   2bac8:	b	2bbd4 <__gmpn_set_str@@Base+0x170>
   2bacc:	cmp	x21, #0x717
   2bad0:	b.ls	2bbc0 <__gmpn_set_str@@Base+0x15c>  // b.plast
   2bad4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2bad8:	ldr	x8, [x8, #3936]
   2badc:	mov	w24, #0x28                  	// #40
   2bae0:	smull	x9, w22, w24
   2bae4:	add	x0, x29, #0x18
   2bae8:	ldrsw	x8, [x8, x9]
   2baec:	str	xzr, [x29, #24]
   2baf0:	udiv	x8, x21, x8
   2baf4:	lsl	x25, x8, #3
   2baf8:	add	x1, x25, #0x408
   2bafc:	add	x23, x8, #0x1
   2bb00:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2bb04:	mov	x1, x0
   2bb08:	mov	x0, sp
   2bb0c:	mov	x2, x23
   2bb10:	mov	w3, w22
   2bb14:	mov	x26, sp
   2bb18:	bl	ccd0 <__gmpn_compute_powtab@plt>
   2bb1c:	madd	x22, x0, x24, x26
   2bb20:	add	x1, x25, #0x208
   2bb24:	add	x0, x29, #0x18
   2bb28:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2bb2c:	mov	x4, x0
   2bb30:	mov	x0, x19
   2bb34:	mov	x1, x20
   2bb38:	mov	x2, x21
   2bb3c:	mov	x3, x22
   2bb40:	bl	c4c0 <__gmpn_dc_set_str@plt>
   2bb44:	ldr	x8, [x29, #24]
   2bb48:	cbz	x8, 2bbd4 <__gmpn_set_str@@Base+0x170>
   2bb4c:	mov	x19, x0
   2bb50:	mov	x0, x8
   2bb54:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2bb58:	mov	x0, x19
   2bb5c:	b	2bbd4 <__gmpn_set_str@@Base+0x170>
   2bb60:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2bb64:	ldr	x10, [x10, #3936]
   2bb68:	mov	w12, #0x28                  	// #40
   2bb6c:	mov	w11, wzr
   2bb70:	mov	x8, xzr
   2bb74:	smaddl	x10, w22, w12, x10
   2bb78:	ldr	w10, [x10, #24]
   2bb7c:	mov	x0, xzr
   2bb80:	b	2bb90 <__gmpn_set_str@@Base+0x12c>
   2bb84:	sub	x9, x9, #0x1
   2bb88:	cmp	x9, x20
   2bb8c:	b.cc	2bab8 <__gmpn_set_str@@Base+0x54>  // b.lo, b.ul, b.last
   2bb90:	ldrb	w12, [x9]
   2bb94:	lsl	x14, x12, x11
   2bb98:	add	w11, w11, w10
   2bb9c:	subs	w13, w11, #0x40
   2bba0:	orr	x8, x14, x8
   2bba4:	b.lt	2bb84 <__gmpn_set_str@@Base+0x120>  // b.tstop
   2bba8:	str	x8, [x19, x0, lsl #3]
   2bbac:	sub	w8, w10, w13
   2bbb0:	add	x0, x0, #0x1
   2bbb4:	lsr	w8, w12, w8
   2bbb8:	mov	w11, w13
   2bbbc:	b	2bb84 <__gmpn_set_str@@Base+0x120>
   2bbc0:	mov	x0, x19
   2bbc4:	mov	x1, x20
   2bbc8:	mov	x2, x21
   2bbcc:	mov	w3, w22
   2bbd0:	bl	c260 <__gmpn_bc_set_str@plt>
   2bbd4:	add	sp, sp, #0xa00
   2bbd8:	ldp	x20, x19, [sp, #80]
   2bbdc:	ldp	x22, x21, [sp, #64]
   2bbe0:	ldp	x24, x23, [sp, #48]
   2bbe4:	ldp	x26, x25, [sp, #32]
   2bbe8:	ldr	x28, [sp, #16]
   2bbec:	ldp	x29, x30, [sp], #96
   2bbf0:	ret

000000000002bbf4 <__gmpn_bc_set_str@@Base>:
   2bbf4:	sub	sp, sp, #0x70
   2bbf8:	stp	x29, x30, [sp, #16]
   2bbfc:	stp	x28, x27, [sp, #32]
   2bc00:	stp	x26, x25, [sp, #48]
   2bc04:	stp	x24, x23, [sp, #64]
   2bc08:	stp	x22, x21, [sp, #80]
   2bc0c:	stp	x20, x19, [sp, #96]
   2bc10:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2bc14:	ldr	x8, [x8, #3936]
   2bc18:	mov	w9, #0x28                  	// #40
   2bc1c:	mov	x21, x1
   2bc20:	ldrb	w4, [x21], #1
   2bc24:	smaddl	x8, w3, w9, x8
   2bc28:	ldrsw	x26, [x8]
   2bc2c:	mov	w23, w3
   2bc30:	mov	x22, x2
   2bc34:	mov	x19, x0
   2bc38:	cmp	x26, x2
   2bc3c:	sxtw	x25, w23
   2bc40:	mov	x20, xzr
   2bc44:	add	x29, sp, #0x10
   2bc48:	b.cs	2bd00 <__gmpn_bc_set_str@@Base+0x10c>  // b.hs, b.nlast
   2bc4c:	ldr	x8, [x8, #24]
   2bc50:	mov	w24, #0xa                   	// #10
   2bc54:	mov	x27, x26
   2bc58:	str	x8, [sp, #8]
   2bc5c:	sub	w8, w26, #0x1
   2bc60:	sxtw	x28, w8
   2bc64:	b	2bc84 <__gmpn_bc_set_str@@Base+0x90>
   2bc68:	cbz	x4, 2bcf8 <__gmpn_bc_set_str@@Base+0x104>
   2bc6c:	str	x4, [x19]
   2bc70:	mov	w20, #0x1                   	// #1
   2bc74:	ldrb	w4, [x21], #1
   2bc78:	add	x27, x27, x26
   2bc7c:	cmp	x27, x22
   2bc80:	b.cs	2bd04 <__gmpn_bc_set_str@@Base+0x110>  // b.hs, b.nlast
   2bc84:	cmp	w23, #0xa
   2bc88:	b.ne	2bcac <__gmpn_bc_set_str@@Base+0xb8>  // b.any
   2bc8c:	mov	x8, xzr
   2bc90:	ldrb	w9, [x21, x8]
   2bc94:	add	x8, x8, #0x1
   2bc98:	cmp	x8, #0x12
   2bc9c:	madd	x4, x4, x24, x9
   2bca0:	b.ne	2bc90 <__gmpn_bc_set_str@@Base+0x9c>  // b.any
   2bca4:	add	x21, x21, #0x12
   2bca8:	b	2bccc <__gmpn_bc_set_str@@Base+0xd8>
   2bcac:	cbz	w28, 2bccc <__gmpn_bc_set_str@@Base+0xd8>
   2bcb0:	mov	x8, x21
   2bcb4:	mov	x9, x28
   2bcb8:	ldrb	w10, [x8], #1
   2bcbc:	subs	x9, x9, #0x1
   2bcc0:	madd	x4, x4, x25, x10
   2bcc4:	b.ne	2bcb8 <__gmpn_bc_set_str@@Base+0xc4>  // b.any
   2bcc8:	add	x21, x21, x28
   2bccc:	cbz	x20, 2bc68 <__gmpn_bc_set_str@@Base+0x74>
   2bcd0:	ldr	x3, [sp, #8]
   2bcd4:	mov	x0, x19
   2bcd8:	mov	x1, x19
   2bcdc:	mov	x2, x20
   2bce0:	bl	d420 <__gmpn_mul_1c@plt>
   2bce4:	cbz	x0, 2bc74 <__gmpn_bc_set_str@@Base+0x80>
   2bce8:	add	x8, x20, #0x1
   2bcec:	str	x0, [x19, x20, lsl #3]
   2bcf0:	mov	x20, x8
   2bcf4:	b	2bc74 <__gmpn_bc_set_str@@Base+0x80>
   2bcf8:	mov	x20, xzr
   2bcfc:	b	2bc74 <__gmpn_bc_set_str@@Base+0x80>
   2bd00:	mov	x27, x26
   2bd04:	cmp	w23, #0xa
   2bd08:	b.ne	2bd48 <__gmpn_bc_set_str@@Base+0x154>  // b.any
   2bd0c:	sub	x8, x22, x27
   2bd10:	add	x9, x8, #0x12
   2bd14:	cmp	x9, #0x1
   2bd18:	b.lt	2bd7c <__gmpn_bc_set_str@@Base+0x188>  // b.tstop
   2bd1c:	add	x8, x8, #0x13
   2bd20:	mov	w9, #0xa                   	// #10
   2bd24:	mov	x3, x25
   2bd28:	ldrb	w10, [x21], #1
   2bd2c:	add	x11, x3, x3, lsl #2
   2bd30:	sub	x8, x8, #0x1
   2bd34:	cmp	x8, #0x1
   2bd38:	madd	x4, x4, x9, x10
   2bd3c:	lsl	x3, x11, #1
   2bd40:	b.gt	2bd28 <__gmpn_bc_set_str@@Base+0x134>
   2bd44:	b	2bd80 <__gmpn_bc_set_str@@Base+0x18c>
   2bd48:	add	x8, x26, x22
   2bd4c:	mvn	x9, x27
   2bd50:	add	x8, x8, x9
   2bd54:	cmp	x8, #0x1
   2bd58:	b.lt	2bda8 <__gmpn_bc_set_str@@Base+0x1b4>  // b.tstop
   2bd5c:	mov	x3, x25
   2bd60:	ldrb	w9, [x21], #1
   2bd64:	sub	x8, x8, #0x1
   2bd68:	cmp	x8, #0x0
   2bd6c:	mul	x3, x3, x25
   2bd70:	madd	x4, x4, x25, x9
   2bd74:	b.gt	2bd60 <__gmpn_bc_set_str@@Base+0x16c>
   2bd78:	b	2bd80 <__gmpn_bc_set_str@@Base+0x18c>
   2bd7c:	mov	x3, x25
   2bd80:	cbz	x20, 2bdb0 <__gmpn_bc_set_str@@Base+0x1bc>
   2bd84:	mov	x0, x19
   2bd88:	mov	x1, x19
   2bd8c:	mov	x2, x20
   2bd90:	bl	d420 <__gmpn_mul_1c@plt>
   2bd94:	cbz	x0, 2bdc4 <__gmpn_bc_set_str@@Base+0x1d0>
   2bd98:	add	x8, x20, #0x1
   2bd9c:	str	x0, [x19, x20, lsl #3]
   2bda0:	mov	x20, x8
   2bda4:	b	2bdc4 <__gmpn_bc_set_str@@Base+0x1d0>
   2bda8:	mov	x3, x25
   2bdac:	cbnz	x20, 2bd84 <__gmpn_bc_set_str@@Base+0x190>
   2bdb0:	cbz	x4, 2bdc0 <__gmpn_bc_set_str@@Base+0x1cc>
   2bdb4:	str	x4, [x19]
   2bdb8:	mov	w20, #0x1                   	// #1
   2bdbc:	b	2bdc4 <__gmpn_bc_set_str@@Base+0x1d0>
   2bdc0:	mov	x20, xzr
   2bdc4:	mov	x0, x20
   2bdc8:	ldp	x20, x19, [sp, #96]
   2bdcc:	ldp	x22, x21, [sp, #80]
   2bdd0:	ldp	x24, x23, [sp, #64]
   2bdd4:	ldp	x26, x25, [sp, #48]
   2bdd8:	ldp	x28, x27, [sp, #32]
   2bddc:	ldp	x29, x30, [sp, #16]
   2bde0:	add	sp, sp, #0x70
   2bde4:	ret

000000000002bde8 <__gmpn_dc_set_str@@Base>:
   2bde8:	stp	x29, x30, [sp, #-80]!
   2bdec:	stp	x26, x25, [sp, #16]
   2bdf0:	stp	x24, x23, [sp, #32]
   2bdf4:	stp	x22, x21, [sp, #48]
   2bdf8:	stp	x20, x19, [sp, #64]
   2bdfc:	ldr	x23, [x3, #24]
   2be00:	mov	x21, x4
   2be04:	mov	x20, x3
   2be08:	mov	x24, x2
   2be0c:	mov	x25, x1
   2be10:	cmp	x23, x2
   2be14:	mov	x19, x0
   2be18:	mov	x29, sp
   2be1c:	b.cs	2be44 <__gmpn_dc_set_str@@Base+0x5c>  // b.hs, b.nlast
   2be20:	sub	x2, x24, x23
   2be24:	cmp	x2, #0x313
   2be28:	b.ls	2be68 <__gmpn_dc_set_str@@Base+0x80>  // b.plast
   2be2c:	sub	x3, x20, #0x28
   2be30:	mov	x0, x21
   2be34:	mov	x1, x25
   2be38:	mov	x4, x19
   2be3c:	bl	c4c0 <__gmpn_dc_set_str@plt>
   2be40:	b	2be78 <__gmpn_dc_set_str@@Base+0x90>
   2be44:	cmp	x24, #0x313
   2be48:	b.ls	2beb0 <__gmpn_dc_set_str@@Base+0xc8>  // b.plast
   2be4c:	sub	x3, x20, #0x28
   2be50:	mov	x0, x19
   2be54:	mov	x1, x25
   2be58:	mov	x2, x24
   2be5c:	mov	x4, x21
   2be60:	bl	c4c0 <__gmpn_dc_set_str@plt>
   2be64:	b	2bfb0 <__gmpn_dc_set_str@@Base+0x1c8>
   2be68:	ldr	w3, [x20, #32]
   2be6c:	mov	x0, x21
   2be70:	mov	x1, x25
   2be74:	bl	c260 <__gmpn_bc_set_str@plt>
   2be78:	ldp	x4, x26, [x20, #8]
   2be7c:	mov	x22, x0
   2be80:	cbz	x0, 2bec8 <__gmpn_dc_set_str@@Base+0xe0>
   2be84:	ldr	x3, [x20]
   2be88:	cmp	x4, x22
   2be8c:	add	x0, x19, x26, lsl #3
   2be90:	b.le	2bedc <__gmpn_dc_set_str@@Base+0xf4>
   2be94:	mov	x1, x3
   2be98:	mov	x2, x4
   2be9c:	mov	x3, x21
   2bea0:	mov	x4, x22
   2bea4:	bl	cea0 <__gmpn_mul@plt>
   2bea8:	cbnz	x26, 2beec <__gmpn_dc_set_str@@Base+0x104>
   2beac:	b	2befc <__gmpn_dc_set_str@@Base+0x114>
   2beb0:	ldr	w3, [x20, #32]
   2beb4:	mov	x0, x19
   2beb8:	mov	x1, x25
   2bebc:	mov	x2, x24
   2bec0:	bl	c260 <__gmpn_bc_set_str@plt>
   2bec4:	b	2bfb0 <__gmpn_dc_set_str@@Base+0x1c8>
   2bec8:	add	x8, x26, x4
   2becc:	adds	x8, x8, #0x1
   2bed0:	b.eq	2befc <__gmpn_dc_set_str@@Base+0x114>  // b.none
   2bed4:	lsl	x2, x8, #3
   2bed8:	b	2bef0 <__gmpn_dc_set_str@@Base+0x108>
   2bedc:	mov	x1, x21
   2bee0:	mov	x2, x22
   2bee4:	bl	cea0 <__gmpn_mul@plt>
   2bee8:	cbz	x26, 2befc <__gmpn_dc_set_str@@Base+0x114>
   2beec:	lsl	x2, x26, #3
   2bef0:	mov	x0, x19
   2bef4:	mov	w1, wzr
   2bef8:	bl	c780 <memset@plt>
   2befc:	add	x8, x25, x24
   2bf00:	cmp	x23, #0x313
   2bf04:	sub	x1, x8, x23
   2bf08:	b.ls	2bf38 <__gmpn_dc_set_str@@Base+0x150>  // b.plast
   2bf0c:	ldr	x8, [x20, #8]
   2bf10:	sub	x3, x20, #0x28
   2bf14:	mov	x0, x21
   2bf18:	mov	x2, x23
   2bf1c:	add	x8, x21, x8, lsl #3
   2bf20:	add	x8, x8, x26, lsl #3
   2bf24:	add	x4, x8, #0x8
   2bf28:	bl	c4c0 <__gmpn_dc_set_str@plt>
   2bf2c:	mov	x23, x0
   2bf30:	cbnz	x0, 2bf50 <__gmpn_dc_set_str@@Base+0x168>
   2bf34:	b	2bf90 <__gmpn_dc_set_str@@Base+0x1a8>
   2bf38:	ldr	w3, [x20, #32]
   2bf3c:	mov	x0, x21
   2bf40:	mov	x2, x23
   2bf44:	bl	c260 <__gmpn_bc_set_str@plt>
   2bf48:	mov	x23, x0
   2bf4c:	cbz	x0, 2bf90 <__gmpn_dc_set_str@@Base+0x1a8>
   2bf50:	mov	x0, x19
   2bf54:	mov	x1, x19
   2bf58:	mov	x2, x21
   2bf5c:	mov	x3, x23
   2bf60:	bl	cc30 <__gmpn_add_n@plt>
   2bf64:	lsl	x8, x23, #3
   2bf68:	ldr	x9, [x19, x8]
   2bf6c:	adds	x9, x9, x0
   2bf70:	str	x9, [x19, x8]
   2bf74:	b.cc	2bf90 <__gmpn_dc_set_str@@Base+0x1a8>  // b.lo, b.ul, b.last
   2bf78:	add	x8, x19, x23, lsl #3
   2bf7c:	add	x8, x8, #0x8
   2bf80:	ldr	x9, [x8]
   2bf84:	adds	x9, x9, #0x1
   2bf88:	str	x9, [x8], #8
   2bf8c:	b.cs	2bf80 <__gmpn_dc_set_str@@Base+0x198>  // b.hs, b.nlast
   2bf90:	ldr	x8, [x20, #8]
   2bf94:	add	x9, x26, x22
   2bf98:	add	x8, x9, x8
   2bf9c:	add	x9, x19, x8, lsl #3
   2bfa0:	ldur	x9, [x9, #-8]
   2bfa4:	cmp	x9, #0x0
   2bfa8:	cset	w9, eq  // eq = none
   2bfac:	sub	x0, x8, x9
   2bfb0:	ldp	x20, x19, [sp, #64]
   2bfb4:	ldp	x22, x21, [sp, #48]
   2bfb8:	ldp	x24, x23, [sp, #32]
   2bfbc:	ldp	x26, x25, [sp, #16]
   2bfc0:	ldp	x29, x30, [sp], #80
   2bfc4:	ret

000000000002bfc8 <__gmpn_compute_powtab@@Base>:
   2bfc8:	stp	x29, x30, [sp, #-64]!
   2bfcc:	str	x28, [sp, #16]
   2bfd0:	stp	x22, x21, [sp, #32]
   2bfd4:	stp	x20, x19, [sp, #48]
   2bfd8:	mov	x29, sp
   2bfdc:	sub	sp, sp, #0x200
   2bfe0:	mov	x21, x1
   2bfe4:	mov	x22, x0
   2bfe8:	mov	x0, sp
   2bfec:	mov	x1, x2
   2bff0:	mov	w2, w3
   2bff4:	mov	w19, w3
   2bff8:	bl	2c058 <__gmpn_compute_powtab@@Base+0x90>
   2bffc:	mov	x20, x0
   2c000:	tbnz	x0, #63, 2c020 <__gmpn_compute_powtab@@Base+0x58>
   2c004:	mov	x3, sp
   2c008:	mov	x0, x22
   2c00c:	mov	x1, x21
   2c010:	mov	w2, w19
   2c014:	mov	x4, x20
   2c018:	bl	2c150 <__gmpn_compute_powtab@@Base+0x188>
   2c01c:	b	2c03c <__gmpn_compute_powtab@@Base+0x74>
   2c020:	neg	x20, x20
   2c024:	mov	x3, sp
   2c028:	mov	x0, x22
   2c02c:	mov	x1, x21
   2c030:	mov	w2, w19
   2c034:	mov	x4, x20
   2c038:	bl	2c438 <__gmpn_compute_powtab@@Base+0x470>
   2c03c:	mov	x0, x20
   2c040:	add	sp, sp, #0x200
   2c044:	ldp	x20, x19, [sp, #48]
   2c048:	ldp	x22, x21, [sp, #32]
   2c04c:	ldr	x28, [sp, #16]
   2c050:	ldp	x29, x30, [sp], #64
   2c054:	ret
   2c058:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2c05c:	ldr	x9, [x9, #3936]
   2c060:	mov	w8, #0x28                  	// #40
   2c064:	smull	x8, w2, w8
   2c068:	ldrsw	x10, [x9, x8]
   2c06c:	add	x8, x1, #0x1
   2c070:	lsr	x9, x8, #1
   2c074:	cmp	x9, #0x1
   2c078:	b.ne	2c084 <__gmpn_compute_powtab@@Base+0xbc>  // b.any
   2c07c:	mov	x8, xzr
   2c080:	b	2c0a8 <__gmpn_compute_powtab@@Base+0xe0>
   2c084:	mov	x8, xzr
   2c088:	mov	x11, x9
   2c08c:	mul	x12, x11, x10
   2c090:	add	x11, x11, #0x1
   2c094:	lsr	x11, x11, #1
   2c098:	str	x12, [x0, x8, lsl #3]
   2c09c:	cmp	x11, #0x1
   2c0a0:	add	x8, x8, #0x1
   2c0a4:	b.ne	2c08c <__gmpn_compute_powtab@@Base+0xc4>  // b.any
   2c0a8:	cmp	x8, #0x2
   2c0ac:	str	x10, [x0, x8, lsl #3]
   2c0b0:	b.lt	2c120 <__gmpn_compute_powtab@@Base+0x158>  // b.tstop
   2c0b4:	sub	x12, x1, #0x1
   2c0b8:	sub	x14, x8, #0x1
   2c0bc:	mov	w11, #0x1                   	// #1
   2c0c0:	mov	w10, #0x1                   	// #1
   2c0c4:	b	2c0e0 <__gmpn_compute_powtab@@Base+0x118>
   2c0c8:	tst	x14, #0x1
   2c0cc:	csel	w14, w14, wzr, ne  // ne = any
   2c0d0:	cmp	x13, #0x0
   2c0d4:	add	w10, w14, w10
   2c0d8:	mov	x14, x13
   2c0dc:	b.le	2c12c <__gmpn_compute_powtab@@Base+0x164>
   2c0e0:	sub	x13, x14, #0x1
   2c0e4:	lsr	x14, x12, x14
   2c0e8:	add	x14, x14, #0x1
   2c0ec:	tst	x14, #0x1
   2c0f0:	lsl	x15, x14, x13
   2c0f4:	csel	w16, w14, wzr, ne  // ne = any
   2c0f8:	cmp	x9, x15
   2c0fc:	add	w11, w16, w11
   2c100:	b.eq	2c0c8 <__gmpn_compute_powtab@@Base+0x100>  // b.none
   2c104:	cmp	x14, #0x2
   2c108:	cset	w15, hi  // hi = pmore
   2c10c:	tst	x14, #0x1
   2c110:	cset	w16, eq  // eq = none
   2c114:	and	w15, w15, w16
   2c118:	lsl	w14, w14, w15
   2c11c:	b	2c0d0 <__gmpn_compute_powtab@@Base+0x108>
   2c120:	mov	w10, #0x1                   	// #1
   2c124:	mov	w9, #0x1                   	// #1
   2c128:	b	2c144 <__gmpn_compute_powtab@@Base+0x17c>
   2c12c:	mov	w9, #0x9f                  	// #159
   2c130:	mul	w9, w11, w9
   2c134:	mov	w11, #0x851f                	// #34079
   2c138:	movk	w11, #0x51eb, lsl #16
   2c13c:	umull	x9, w9, w11
   2c140:	lsr	x9, x9, #37
   2c144:	cmp	w10, w9
   2c148:	cneg	x0, x8, hi  // hi = pmore
   2c14c:	ret
   2c150:	sub	sp, sp, #0x90
   2c154:	stp	x29, x30, [sp, #48]
   2c158:	stp	x28, x27, [sp, #64]
   2c15c:	stp	x26, x25, [sp, #80]
   2c160:	stp	x24, x23, [sp, #96]
   2c164:	stp	x22, x21, [sp, #112]
   2c168:	stp	x20, x19, [sp, #128]
   2c16c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2c170:	ldr	x8, [x8, #3936]
   2c174:	mov	w9, #0x28                  	// #40
   2c178:	mov	x21, x3
   2c17c:	add	x24, x1, #0x8
   2c180:	smaddl	x8, w2, w9, x8
   2c184:	ldr	x3, [x8, #24]
   2c188:	ldrsw	x22, [x8]
   2c18c:	mov	w19, #0x1                   	// #1
   2c190:	mov	w20, w2
   2c194:	mov	x26, x0
   2c198:	str	x3, [x1]
   2c19c:	stp	x1, x19, [x0]
   2c1a0:	str	w2, [x0, #32]
   2c1a4:	stp	xzr, x22, [x0, #16]
   2c1a8:	mov	w2, #0x1                   	// #1
   2c1ac:	mov	x0, x24
   2c1b0:	add	x29, sp, #0x30
   2c1b4:	mov	x25, x4
   2c1b8:	mov	x27, x1
   2c1bc:	add	x28, x1, #0x18
   2c1c0:	str	x3, [sp, #16]
   2c1c4:	bl	d670 <__gmpn_mul_1@plt>
   2c1c8:	ldr	x8, [x27, #8]
   2c1cc:	lsl	x11, x22, #1
   2c1d0:	str	x0, [x27, #16]
   2c1d4:	stur	w20, [x29, #-20]
   2c1d8:	cmp	x8, #0x0
   2c1dc:	cset	w10, eq  // eq = none
   2c1e0:	cinc	x23, x19, ne  // ne = any
   2c1e4:	add	x24, x24, w10, uxtw #3
   2c1e8:	str	w20, [x26, #72]
   2c1ec:	stp	x24, x23, [x26, #40]
   2c1f0:	stp	x10, x11, [x26, #56]
   2c1f4:	ldr	x8, [x21]
   2c1f8:	lsl	x9, x22, x25
   2c1fc:	str	x22, [sp, #8]
   2c200:	cmp	x8, x9
   2c204:	b.ne	2c224 <__gmpn_compute_powtab@@Base+0x25c>  // b.any
   2c208:	stp	x11, x10, [x29, #-16]
   2c20c:	add	x27, x26, #0x50
   2c210:	mov	x8, #0xfffffffffffffffe    	// #-2
   2c214:	mov	x22, x28
   2c218:	cmn	x8, x25
   2c21c:	b.pl	2c2cc <__gmpn_compute_powtab@@Base+0x304>  // b.nfrst
   2c220:	b	2c418 <__gmpn_compute_powtab@@Base+0x450>
   2c224:	add	x19, x22, x22, lsl #1
   2c228:	sub	x9, x25, #0x2
   2c22c:	lsl	x9, x19, x9
   2c230:	cmp	x9, x8
   2c234:	b.ls	2c250 <__gmpn_compute_powtab@@Base+0x288>  // b.plast
   2c238:	ldr	x8, [x24]
   2c23c:	add	x9, x27, #0x30
   2c240:	str	x8, [x27, #24]
   2c244:	ldr	x8, [x24, #8]
   2c248:	str	x8, [x27, #32]
   2c24c:	b	2c2a0 <__gmpn_compute_powtab@@Base+0x2d8>
   2c250:	ldr	x3, [sp, #16]
   2c254:	add	x8, x27, #0x38
   2c258:	mov	x0, x28
   2c25c:	mov	x1, x24
   2c260:	mov	x2, x23
   2c264:	stur	x8, [x29, #-8]
   2c268:	mov	x24, x10
   2c26c:	bl	d670 <__gmpn_mul_1@plt>
   2c270:	str	x0, [x28, x23, lsl #3]
   2c274:	ldr	x8, [x27, #24]
   2c278:	cmp	x0, #0x0
   2c27c:	cinc	x9, x23, ne  // ne = any
   2c280:	mov	x10, x24
   2c284:	cmp	x8, #0x0
   2c288:	cset	w8, eq  // eq = none
   2c28c:	sub	x23, x9, x8
   2c290:	ldur	x9, [x29, #-8]
   2c294:	cinc	x10, x24, eq  // eq = none
   2c298:	add	x28, x28, w8, uxtw #3
   2c29c:	mov	x11, x19
   2c2a0:	stp	x28, x23, [x26, #80]
   2c2a4:	ldur	w8, [x29, #-20]
   2c2a8:	mov	x24, x28
   2c2ac:	add	x27, x26, #0x78
   2c2b0:	mov	x22, x9
   2c2b4:	str	w8, [x26, #112]
   2c2b8:	mov	x8, #0xfffffffffffffffd    	// #-3
   2c2bc:	stp	x11, x10, [x29, #-16]
   2c2c0:	stp	x10, x11, [x26, #96]
   2c2c4:	cmn	x8, x25
   2c2c8:	b.mi	2c418 <__gmpn_compute_powtab@@Base+0x450>  // b.first
   2c2cc:	add	x8, x8, x25
   2c2d0:	add	x28, x8, #0x1
   2c2d4:	str	x21, [sp]
   2c2d8:	b	2c2ec <__gmpn_compute_powtab@@Base+0x324>
   2c2dc:	cmp	x19, #0x0
   2c2e0:	add	x27, x27, #0x28
   2c2e4:	mov	x28, x19
   2c2e8:	b.le	2c418 <__gmpn_compute_powtab@@Base+0x450>
   2c2ec:	mov	x0, x22
   2c2f0:	mov	x1, x24
   2c2f4:	mov	x2, x23
   2c2f8:	sub	x19, x28, #0x1
   2c2fc:	lsl	x20, x23, #1
   2c300:	add	x25, x22, x23, lsl #4
   2c304:	bl	ca90 <__gmpn_sqr@plt>
   2c308:	ldur	x8, [x25, #-8]
   2c30c:	ldur	x12, [x29, #-16]
   2c310:	ldr	x9, [x22]
   2c314:	ldr	x11, [sp, #8]
   2c318:	cmp	x8, #0x0
   2c31c:	cset	w8, eq  // eq = none
   2c320:	ldr	x10, [x21]
   2c324:	sub	x20, x20, x8
   2c328:	ldur	x8, [x29, #-8]
   2c32c:	lsl	x12, x12, #1
   2c330:	add	x26, x12, x11
   2c334:	cmp	x9, #0x0
   2c338:	lsl	x11, x26, x19
   2c33c:	cset	w21, eq  // eq = none
   2c340:	cmp	x11, x10
   2c344:	add	x24, x22, w21, uxtw #3
   2c348:	sub	x23, x20, x21
   2c34c:	bfi	x21, x8, #1, #63
   2c350:	b.ls	2c35c <__gmpn_compute_powtab@@Base+0x394>  // b.plast
   2c354:	mov	x10, x21
   2c358:	b	2c398 <__gmpn_compute_powtab@@Base+0x3d0>
   2c35c:	ldr	x3, [sp, #16]
   2c360:	mov	x0, x24
   2c364:	mov	x1, x24
   2c368:	mov	x2, x23
   2c36c:	bl	d670 <__gmpn_mul_1@plt>
   2c370:	str	x0, [x22, x20, lsl #3]
   2c374:	ldr	x8, [x24]
   2c378:	cmp	x0, #0x0
   2c37c:	cinc	x9, x23, ne  // ne = any
   2c380:	mov	x12, x26
   2c384:	cmp	x8, #0x0
   2c388:	cset	w8, eq  // eq = none
   2c38c:	cinc	x10, x21, eq  // eq = none
   2c390:	add	x24, x24, w8, uxtw #3
   2c394:	sub	x23, x9, x8
   2c398:	ldr	x21, [sp]
   2c39c:	stp	x24, x23, [x27]
   2c3a0:	ldur	w8, [x29, #-20]
   2c3a4:	add	x22, x25, #0x10
   2c3a8:	str	w8, [x27, #32]
   2c3ac:	stp	x12, x10, [x29, #-16]
   2c3b0:	stp	x10, x12, [x27, #16]
   2c3b4:	ldur	x8, [x27, #-16]
   2c3b8:	ldr	x9, [x21, x28, lsl #3]
   2c3bc:	cmp	x8, x9
   2c3c0:	b.cs	2c2dc <__gmpn_compute_powtab@@Base+0x314>  // b.hs, b.nlast
   2c3c4:	ldp	x26, x25, [x27, #-40]
   2c3c8:	ldr	x3, [sp, #16]
   2c3cc:	mov	x0, x26
   2c3d0:	mov	x1, x26
   2c3d4:	mov	x2, x25
   2c3d8:	bl	d670 <__gmpn_mul_1@plt>
   2c3dc:	str	x0, [x26, x25, lsl #3]
   2c3e0:	ldr	x8, [x21, x28, lsl #3]
   2c3e4:	ldur	x9, [x27, #-24]
   2c3e8:	cmp	x0, #0x0
   2c3ec:	cinc	x10, x25, ne  // ne = any
   2c3f0:	stur	x8, [x27, #-16]
   2c3f4:	ldr	x8, [x26]
   2c3f8:	cmp	x8, #0x0
   2c3fc:	cset	w8, eq  // eq = none
   2c400:	cinc	x9, x9, eq  // eq = none
   2c404:	add	x11, x26, w8, uxtw #3
   2c408:	sub	x8, x10, x8
   2c40c:	stp	x11, x8, [x27, #-40]
   2c410:	stur	x9, [x27, #-24]
   2c414:	b	2c2dc <__gmpn_compute_powtab@@Base+0x314>
   2c418:	ldp	x20, x19, [sp, #128]
   2c41c:	ldp	x22, x21, [sp, #112]
   2c420:	ldp	x24, x23, [sp, #96]
   2c424:	ldp	x26, x25, [sp, #80]
   2c428:	ldp	x28, x27, [sp, #64]
   2c42c:	ldp	x29, x30, [sp, #48]
   2c430:	add	sp, sp, #0x90
   2c434:	ret
   2c438:	sub	sp, sp, #0x80
   2c43c:	stp	x29, x30, [sp, #32]
   2c440:	stp	x28, x27, [sp, #48]
   2c444:	stp	x26, x25, [sp, #64]
   2c448:	stp	x24, x23, [sp, #80]
   2c44c:	stp	x22, x21, [sp, #96]
   2c450:	stp	x20, x19, [sp, #112]
   2c454:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2c458:	ldr	x8, [x8, #3936]
   2c45c:	mov	w9, #0x28                  	// #40
   2c460:	mov	x22, x0
   2c464:	subs	x19, x4, #0x1
   2c468:	smaddl	x8, w2, w9, x8
   2c46c:	ldr	x10, [x8, #24]
   2c470:	ldrsw	x28, [x8]
   2c474:	mov	w9, #0x1                   	// #1
   2c478:	add	x29, sp, #0x20
   2c47c:	str	x10, [x1]
   2c480:	stp	x1, x9, [x0]
   2c484:	str	x4, [sp, #8]
   2c488:	str	w2, [x0, #32]
   2c48c:	stp	xzr, x28, [x0, #16]
   2c490:	b.mi	2c5bc <__gmpn_compute_powtab@@Base+0x5f4>  // b.first
   2c494:	lsr	x8, x10, #19
   2c498:	str	x8, [sp, #16]
   2c49c:	neg	x8, x10
   2c4a0:	and	x8, x10, x8
   2c4a4:	mov	x20, x3
   2c4a8:	mov	w21, w2
   2c4ac:	mov	x23, xzr
   2c4b0:	add	x25, x1, #0x8
   2c4b4:	sub	x24, x8, #0x1
   2c4b8:	mov	w27, #0x1                   	// #1
   2c4bc:	str	x10, [sp]
   2c4c0:	stur	x28, [x29, #-8]
   2c4c4:	b	2c4ec <__gmpn_compute_powtab@@Base+0x524>
   2c4c8:	mov	x1, x25
   2c4cc:	stp	x1, x26, [x22, #40]
   2c4d0:	str	w21, [x22, #72]
   2c4d4:	subs	x19, x19, #0x1
   2c4d8:	stp	x23, x28, [x22, #56]
   2c4dc:	mov	x22, x8
   2c4e0:	mov	x25, x9
   2c4e4:	mov	x27, x26
   2c4e8:	b.mi	2c5c0 <__gmpn_compute_powtab@@Base+0x5f8>  // b.first
   2c4ec:	mov	x0, x25
   2c4f0:	mov	x2, x27
   2c4f4:	lsl	x26, x27, #1
   2c4f8:	bl	ca90 <__gmpn_sqr@plt>
   2c4fc:	sub	x8, x26, #0x1
   2c500:	ldr	x9, [x25, x8, lsl #3]
   2c504:	ldr	x10, [x20, x19, lsl #3]
   2c508:	lsl	x28, x28, #1
   2c50c:	cmp	x9, #0x0
   2c510:	csel	x26, x8, x26, eq  // eq = none
   2c514:	cmp	x28, x10
   2c518:	b.eq	2c568 <__gmpn_compute_powtab@@Base+0x5a0>  // b.none
   2c51c:	cmp	w21, #0xa
   2c520:	b.ne	2c5a4 <__gmpn_compute_powtab@@Base+0x5dc>  // b.any
   2c524:	mov	x4, #0xce15                	// #52757
   2c528:	ldr	x3, [sp, #16]
   2c52c:	movk	x4, #0x6559, lsl #16
   2c530:	movk	x4, #0x7250, lsl #32
   2c534:	movk	x4, #0x26b1, lsl #48
   2c538:	mov	w5, #0x13                  	// #19
   2c53c:	mov	x0, x25
   2c540:	mov	x1, x25
   2c544:	mov	x2, x26
   2c548:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   2c54c:	add	x8, x25, x26, lsl #3
   2c550:	ldur	x8, [x8, #-8]
   2c554:	cmp	x8, #0x0
   2c558:	cset	w8, eq  // eq = none
   2c55c:	sub	x26, x26, x8
   2c560:	ldur	x8, [x29, #-8]
   2c564:	sub	x28, x28, x8
   2c568:	ldr	x10, [x25]
   2c56c:	add	x8, x22, #0x28
   2c570:	add	x9, x25, x27, lsl #4
   2c574:	lsl	x23, x23, #1
   2c578:	cbnz	x10, 2c4c8 <__gmpn_compute_powtab@@Base+0x500>
   2c57c:	mov	x1, x25
   2c580:	ldr	x10, [x1, #8]!
   2c584:	tst	x10, x24
   2c588:	b.ne	2c4c8 <__gmpn_compute_powtab@@Base+0x500>  // b.any
   2c58c:	ldr	x10, [x25, #8]
   2c590:	sub	x26, x26, #0x1
   2c594:	add	x23, x23, #0x1
   2c598:	mov	x25, x1
   2c59c:	cbz	x10, 2c580 <__gmpn_compute_powtab@@Base+0x5b8>
   2c5a0:	b	2c4cc <__gmpn_compute_powtab@@Base+0x504>
   2c5a4:	ldr	x3, [sp]
   2c5a8:	mov	x0, x25
   2c5ac:	mov	x1, x25
   2c5b0:	mov	x2, x26
   2c5b4:	bl	c910 <__gmpn_divexact_1@plt>
   2c5b8:	b	2c54c <__gmpn_compute_powtab@@Base+0x584>
   2c5bc:	mov	x8, x22
   2c5c0:	ldr	x9, [sp, #8]
   2c5c4:	tbnz	x9, #63, 2c608 <__gmpn_compute_powtab@@Base+0x640>
   2c5c8:	add	x9, x9, #0x1
   2c5cc:	add	x8, x8, #0x8
   2c5d0:	ldp	x10, x13, [x8, #-8]
   2c5d4:	ldr	x12, [x8, #8]
   2c5d8:	sub	x9, x9, #0x1
   2c5dc:	ldr	x11, [x10]
   2c5e0:	cmp	x11, #0x0
   2c5e4:	cset	w11, eq  // eq = none
   2c5e8:	cinc	x12, x12, eq  // eq = none
   2c5ec:	add	x10, x10, w11, uxtw #3
   2c5f0:	sub	x11, x13, x11
   2c5f4:	cmp	x9, #0x0
   2c5f8:	stp	x10, x11, [x8, #-8]
   2c5fc:	str	x12, [x8, #8]
   2c600:	sub	x8, x8, #0x28
   2c604:	b.gt	2c5d0 <__gmpn_compute_powtab@@Base+0x608>
   2c608:	ldp	x20, x19, [sp, #112]
   2c60c:	ldp	x22, x21, [sp, #96]
   2c610:	ldp	x24, x23, [sp, #80]
   2c614:	ldp	x26, x25, [sp, #64]
   2c618:	ldp	x28, x27, [sp, #48]
   2c61c:	ldp	x29, x30, [sp, #32]
   2c620:	add	sp, sp, #0x80
   2c624:	ret

000000000002c628 <__gmpn_scan0@@Base>:
   2c628:	lsr	x8, x1, #3
   2c62c:	and	x8, x8, #0x1ffffffffffffff8
   2c630:	add	x8, x0, x8
   2c634:	ldr	x9, [x8], #8
   2c638:	mov	x10, #0xffffffffffffffff    	// #-1
   2c63c:	lsl	x10, x10, x1
   2c640:	bics	x9, x10, x9
   2c644:	b.ne	2c658 <__gmpn_scan0@@Base+0x30>  // b.any
   2c648:	ldr	x9, [x8], #8
   2c64c:	cmn	x9, #0x1
   2c650:	b.eq	2c648 <__gmpn_scan0@@Base+0x20>  // b.none
   2c654:	mvn	x9, x9
   2c658:	rbit	x9, x9
   2c65c:	clz	x9, x9
   2c660:	sub	x8, x8, x0
   2c664:	orr	x9, x9, #0xffffffffffffffc0
   2c668:	add	x0, x9, x8, lsl #3
   2c66c:	ret

000000000002c670 <__gmpn_scan1@@Base>:
   2c670:	lsr	x8, x1, #3
   2c674:	and	x8, x8, #0x1ffffffffffffff8
   2c678:	add	x8, x0, x8
   2c67c:	ldr	x9, [x8], #8
   2c680:	mov	x10, #0xffffffffffffffff    	// #-1
   2c684:	lsl	x10, x10, x1
   2c688:	ands	x9, x9, x10
   2c68c:	b.ne	2c698 <__gmpn_scan1@@Base+0x28>  // b.any
   2c690:	ldr	x9, [x8], #8
   2c694:	cbz	x9, 2c690 <__gmpn_scan1@@Base+0x20>
   2c698:	rbit	x9, x9
   2c69c:	clz	x9, x9
   2c6a0:	sub	x8, x8, x0
   2c6a4:	orr	x9, x9, #0xffffffffffffffc0
   2c6a8:	add	x0, x9, x8, lsl #3
   2c6ac:	ret

000000000002c6b0 <__gmpn_popcount@@Base>:
   2c6b0:	mov	x11, #0x1fff                	// #8191
   2c6b4:	cmp	x1, x11
   2c6b8:	b.hi	2c794 <__gmpn_popcount@@Base+0xe4>  // b.pmore
   2c6bc:	movi	v4.16b, #0x0
   2c6c0:	movi	v5.16b, #0x0
   2c6c4:	tbz	w1, #0, 2c6d8 <__gmpn_popcount@@Base+0x28>
   2c6c8:	sub	x1, x1, #0x1
   2c6cc:	ld1	{v0.1d}, [x0], #8
   2c6d0:	cnt	v6.16b, v0.16b
   2c6d4:	uadalp	v4.8h, v6.16b
   2c6d8:	tbz	w1, #1, 2c6ec <__gmpn_popcount@@Base+0x3c>
   2c6dc:	sub	x1, x1, #0x2
   2c6e0:	ld1	{v0.2d}, [x0], #16
   2c6e4:	cnt	v6.16b, v0.16b
   2c6e8:	uadalp	v4.8h, v6.16b
   2c6ec:	tbz	w1, #2, 2c710 <__gmpn_popcount@@Base+0x60>
   2c6f0:	subs	x1, x1, #0x4
   2c6f4:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c6f8:	b.ls	2c768 <__gmpn_popcount@@Base+0xb8>  // b.plast
   2c6fc:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c700:	sub	x1, x1, #0x4
   2c704:	cnt	v6.16b, v0.16b
   2c708:	cnt	v7.16b, v1.16b
   2c70c:	b	2c744 <__gmpn_popcount@@Base+0x94>
   2c710:	subs	x1, x1, #0x8
   2c714:	b.cc	2c77c <__gmpn_popcount@@Base+0xcc>  // b.lo, b.ul, b.last
   2c718:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c71c:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c720:	cnt	v6.16b, v2.16b
   2c724:	cnt	v7.16b, v3.16b
   2c728:	subs	x1, x1, #0x8
   2c72c:	b.cc	2c760 <__gmpn_popcount@@Base+0xb0>  // b.lo, b.ul, b.last
   2c730:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c734:	uadalp	v4.8h, v6.16b
   2c738:	cnt	v6.16b, v0.16b
   2c73c:	uadalp	v5.8h, v7.16b
   2c740:	cnt	v7.16b, v1.16b
   2c744:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c748:	subs	x1, x1, #0x8
   2c74c:	uadalp	v4.8h, v6.16b
   2c750:	cnt	v6.16b, v2.16b
   2c754:	uadalp	v5.8h, v7.16b
   2c758:	cnt	v7.16b, v3.16b
   2c75c:	b.cs	2c730 <__gmpn_popcount@@Base+0x80>  // b.hs, b.nlast
   2c760:	uadalp	v4.8h, v6.16b
   2c764:	uadalp	v5.8h, v7.16b
   2c768:	cnt	v6.16b, v0.16b
   2c76c:	cnt	v7.16b, v1.16b
   2c770:	uadalp	v4.8h, v6.16b
   2c774:	uadalp	v5.8h, v7.16b
   2c778:	add	v4.8h, v4.8h, v5.8h
   2c77c:	uaddlp	v4.4s, v4.8h
   2c780:	uaddlp	v4.2d, v4.4s
   2c784:	mov	x0, v4.d[0]
   2c788:	mov	x1, v4.d[1]
   2c78c:	add	x0, x0, x1
   2c790:	ret
   2c794:	mov	x8, x30
   2c798:	mov	x7, x1
   2c79c:	mov	x4, #0x0                   	// #0
   2c7a0:	mov	x9, #0xff80                	// #65408
   2c7a4:	mov	x10, #0x1ff0                	// #8176
   2c7a8:	add	x5, x0, x9
   2c7ac:	mov	x1, #0x1fe8                	// #8168
   2c7b0:	movi	v4.16b, #0x0
   2c7b4:	movi	v5.16b, #0x0
   2c7b8:	bl	2c718 <__gmpn_popcount@@Base+0x68>
   2c7bc:	add	x4, x4, x0
   2c7c0:	mov	x0, x5
   2c7c4:	sub	x7, x7, x10
   2c7c8:	cmp	x7, x11
   2c7cc:	b.hi	2c7a8 <__gmpn_popcount@@Base+0xf8>  // b.pmore
   2c7d0:	mov	x1, x7
   2c7d4:	bl	2c6bc <__gmpn_popcount@@Base+0xc>
   2c7d8:	add	x0, x4, x0
   2c7dc:	mov	x30, x8
   2c7e0:	ret
   2c7e4:	nop

000000000002c7e8 <__gmpn_hamdist@@Base>:
   2c7e8:	mov	x11, #0x1fff                	// #8191
   2c7ec:	cmp	x2, x11
   2c7f0:	b.hi	2c91c <__gmpn_hamdist@@Base+0x134>  // b.pmore
   2c7f4:	movi	v4.16b, #0x0
   2c7f8:	movi	v5.16b, #0x0
   2c7fc:	tbz	w2, #0, 2c818 <__gmpn_hamdist@@Base+0x30>
   2c800:	sub	x2, x2, #0x1
   2c804:	ld1	{v0.1d}, [x0], #8
   2c808:	ld1	{v16.1d}, [x1], #8
   2c80c:	eor	v0.16b, v0.16b, v16.16b
   2c810:	cnt	v6.16b, v0.16b
   2c814:	uadalp	v4.8h, v6.16b
   2c818:	tbz	w2, #1, 2c834 <__gmpn_hamdist@@Base+0x4c>
   2c81c:	sub	x2, x2, #0x2
   2c820:	ld1	{v0.2d}, [x0], #16
   2c824:	ld1	{v16.2d}, [x1], #16
   2c828:	eor	v0.16b, v0.16b, v16.16b
   2c82c:	cnt	v6.16b, v0.16b
   2c830:	uadalp	v4.8h, v6.16b
   2c834:	tbz	w2, #2, 2c868 <__gmpn_hamdist@@Base+0x80>
   2c838:	subs	x2, x2, #0x4
   2c83c:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c840:	ld1	{v16.2d, v17.2d}, [x1], #32
   2c844:	b.ls	2c8e8 <__gmpn_hamdist@@Base+0x100>  // b.plast
   2c848:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c84c:	ld1	{v18.2d, v19.2d}, [x1], #32
   2c850:	eor	v0.16b, v0.16b, v16.16b
   2c854:	eor	v1.16b, v1.16b, v17.16b
   2c858:	sub	x2, x2, #0x4
   2c85c:	cnt	v6.16b, v0.16b
   2c860:	cnt	v7.16b, v1.16b
   2c864:	b	2c8b8 <__gmpn_hamdist@@Base+0xd0>
   2c868:	subs	x2, x2, #0x8
   2c86c:	b.cc	2c904 <__gmpn_hamdist@@Base+0x11c>  // b.lo, b.ul, b.last
   2c870:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c874:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c878:	ld1	{v18.2d, v19.2d}, [x1], #32
   2c87c:	ld1	{v16.2d, v17.2d}, [x1], #32
   2c880:	eor	v2.16b, v2.16b, v18.16b
   2c884:	eor	v3.16b, v3.16b, v19.16b
   2c888:	cnt	v6.16b, v2.16b
   2c88c:	cnt	v7.16b, v3.16b
   2c890:	subs	x2, x2, #0x8
   2c894:	b.cc	2c8e0 <__gmpn_hamdist@@Base+0xf8>  // b.lo, b.ul, b.last
   2c898:	ld1	{v2.2d, v3.2d}, [x0], #32
   2c89c:	ld1	{v18.2d, v19.2d}, [x1], #32
   2c8a0:	eor	v0.16b, v0.16b, v16.16b
   2c8a4:	eor	v1.16b, v1.16b, v17.16b
   2c8a8:	uadalp	v4.8h, v6.16b
   2c8ac:	cnt	v6.16b, v0.16b
   2c8b0:	uadalp	v5.8h, v7.16b
   2c8b4:	cnt	v7.16b, v1.16b
   2c8b8:	ld1	{v0.2d, v1.2d}, [x0], #32
   2c8bc:	ld1	{v16.2d, v17.2d}, [x1], #32
   2c8c0:	eor	v2.16b, v2.16b, v18.16b
   2c8c4:	eor	v3.16b, v3.16b, v19.16b
   2c8c8:	subs	x2, x2, #0x8
   2c8cc:	uadalp	v4.8h, v6.16b
   2c8d0:	cnt	v6.16b, v2.16b
   2c8d4:	uadalp	v5.8h, v7.16b
   2c8d8:	cnt	v7.16b, v3.16b
   2c8dc:	b.cs	2c898 <__gmpn_hamdist@@Base+0xb0>  // b.hs, b.nlast
   2c8e0:	uadalp	v4.8h, v6.16b
   2c8e4:	uadalp	v5.8h, v7.16b
   2c8e8:	eor	v0.16b, v0.16b, v16.16b
   2c8ec:	eor	v1.16b, v1.16b, v17.16b
   2c8f0:	cnt	v6.16b, v0.16b
   2c8f4:	cnt	v7.16b, v1.16b
   2c8f8:	uadalp	v4.8h, v6.16b
   2c8fc:	uadalp	v5.8h, v7.16b
   2c900:	add	v4.8h, v4.8h, v5.8h
   2c904:	uaddlp	v4.4s, v4.8h
   2c908:	uaddlp	v4.2d, v4.4s
   2c90c:	mov	x0, v4.d[0]
   2c910:	mov	x1, v4.d[1]
   2c914:	add	x0, x0, x1
   2c918:	ret
   2c91c:	mov	x8, x30
   2c920:	mov	x7, x2
   2c924:	mov	x4, #0x0                   	// #0
   2c928:	mov	x9, #0xff80                	// #65408
   2c92c:	mov	x10, #0x1ff0                	// #8176
   2c930:	add	x5, x0, x9
   2c934:	add	x6, x1, x9
   2c938:	mov	x2, #0x1fe8                	// #8168
   2c93c:	movi	v4.16b, #0x0
   2c940:	movi	v5.16b, #0x0
   2c944:	bl	2c870 <__gmpn_hamdist@@Base+0x88>
   2c948:	add	x4, x4, x0
   2c94c:	mov	x0, x5
   2c950:	mov	x1, x6
   2c954:	sub	x7, x7, x10
   2c958:	cmp	x7, x11
   2c95c:	b.hi	2c930 <__gmpn_hamdist@@Base+0x148>  // b.pmore
   2c960:	mov	x2, x7
   2c964:	bl	2c7f4 <__gmpn_hamdist@@Base+0xc>
   2c968:	add	x0, x4, x0
   2c96c:	mov	x30, x8
   2c970:	ret

000000000002c974 <__gmpn_cmp@@Base>:
   2c974:	sub	x8, x0, #0x8
   2c978:	sub	x9, x1, #0x8
   2c97c:	subs	x10, x2, #0x1
   2c980:	b.lt	2c9a8 <__gmpn_cmp@@Base+0x34>  // b.tstop
   2c984:	lsl	x11, x2, #3
   2c988:	ldr	x12, [x8, x11]
   2c98c:	ldr	x11, [x9, x11]
   2c990:	mov	x2, x10
   2c994:	cmp	x12, x11
   2c998:	b.eq	2c97c <__gmpn_cmp@@Base+0x8>  // b.none
   2c99c:	mov	w8, #0x1                   	// #1
   2c9a0:	cneg	w0, w8, ls  // ls = plast
   2c9a4:	ret
   2c9a8:	mov	w0, wzr
   2c9ac:	ret

000000000002c9b0 <__gmpn_zero_p@@Base>:
   2c9b0:	sub	x8, x0, #0x8
   2c9b4:	ldr	x9, [x8, x1, lsl #3]
   2c9b8:	cbnz	x9, 2c9cc <__gmpn_zero_p@@Base+0x1c>
   2c9bc:	sub	x1, x1, #0x1
   2c9c0:	cbnz	x1, 2c9b4 <__gmpn_zero_p@@Base+0x4>
   2c9c4:	mov	w0, #0x1                   	// #1
   2c9c8:	ret
   2c9cc:	mov	w0, wzr
   2c9d0:	ret

000000000002c9d4 <__gmpn_perfect_square_p@@Base>:
   2c9d4:	stp	x29, x30, [sp, #-32]!
   2c9d8:	stp	x20, x19, [sp, #16]
   2c9dc:	mov	x29, sp
   2c9e0:	sub	sp, sp, #0x10
   2c9e4:	ldr	x8, [x0]
   2c9e8:	adrp	x10, 50000 <__gmpn_bases@@Base+0x2f98>
   2c9ec:	add	x10, x10, #0x58
   2c9f0:	ubfx	x9, x8, #6, #2
   2c9f4:	ldr	x9, [x10, x9, lsl #3]
   2c9f8:	lsr	x8, x9, x8
   2c9fc:	tbz	w8, #0, 2caf8 <__gmpn_perfect_square_p@@Base+0x124>
   2ca00:	mov	x19, x0
   2ca04:	mov	x20, x1
   2ca08:	bl	d130 <__gmpn_mod_34lsub1@plt>
   2ca0c:	mov	x9, #0x2fd3                	// #12243
   2ca10:	and	x8, x0, #0xffffffffffff
   2ca14:	movk	x9, #0xd2fd, lsl #16
   2ca18:	movk	x9, #0xfd2f, lsl #32
   2ca1c:	add	x8, x8, x0, lsr #48
   2ca20:	mul	x9, x8, x9
   2ca24:	mov	w10, #0x5b                  	// #91
   2ca28:	and	x9, x9, #0x1ffffffffffff
   2ca2c:	mul	x9, x9, x10
   2ca30:	mov	w10, #0x1                   	// #1
   2ca34:	lsr	x9, x9, #49
   2ca38:	cmp	w9, #0x40
   2ca3c:	lsl	x9, x10, x9
   2ca40:	mov	x10, #0x20e1                	// #8417
   2ca44:	movk	x10, #0x9538, lsl #16
   2ca48:	mov	w11, #0x1240                	// #4672
   2ca4c:	movk	x10, #0xa206, lsl #32
   2ca50:	movk	w11, #0x219, lsl #16
   2ca54:	movk	x10, #0x8850, lsl #48
   2ca58:	csel	x10, x10, x11, cc  // cc = lo, ul, last
   2ca5c:	tst	x10, x9
   2ca60:	and	x11, x10, x9
   2ca64:	cset	w9, eq  // eq = none
   2ca68:	cbz	x11, 2caf4 <__gmpn_perfect_square_p@@Base+0x120>
   2ca6c:	mov	x9, #0xfcfd                	// #64765
   2ca70:	movk	x9, #0xfcfc, lsl #16
   2ca74:	movk	x9, #0xfcfc, lsl #32
   2ca78:	mul	x9, x8, x9
   2ca7c:	mov	w10, #0x55                  	// #85
   2ca80:	mov	x12, #0xa105                	// #41221
   2ca84:	and	x9, x9, #0x1ffffffffffff
   2ca88:	movk	x12, #0x4206, lsl #16
   2ca8c:	mul	x9, x9, x10
   2ca90:	mov	w11, #0x2158                	// #8536
   2ca94:	movk	x12, #0x8c4b, lsl #32
   2ca98:	lsr	x9, x9, #49
   2ca9c:	movk	w11, #0x8, lsl #16
   2caa0:	movk	x12, #0x10b4, lsl #48
   2caa4:	mov	w13, #0x1                   	// #1
   2caa8:	cmp	w9, #0x40
   2caac:	lsl	x9, x13, x9
   2cab0:	csel	x10, x12, x11, cc  // cc = lo, ul, last
   2cab4:	tst	x10, x9
   2cab8:	and	x11, x10, x9
   2cabc:	cset	w9, eq  // eq = none
   2cac0:	cbz	x11, 2caf4 <__gmpn_perfect_square_p@@Base+0x120>
   2cac4:	mov	x9, #0x8e39                	// #36409
   2cac8:	movk	x9, #0x38e3, lsl #16
   2cacc:	movk	x9, #0xe38e, lsl #32
   2cad0:	mul	x9, x8, x9
   2cad4:	and	x9, x9, #0x1ffffffffffff
   2cad8:	add	x9, x9, x9, lsl #3
   2cadc:	lsr	x9, x9, #49
   2cae0:	mov	w10, #0x93                  	// #147
   2cae4:	lsr	x9, x10, x9
   2cae8:	tbnz	w9, #0, 2cb00 <__gmpn_perfect_square_p@@Base+0x12c>
   2caec:	mvn	w8, w9
   2caf0:	and	w9, w8, #0x1
   2caf4:	cbz	w9, 2cb54 <__gmpn_perfect_square_p@@Base+0x180>
   2caf8:	mov	w19, wzr
   2cafc:	b	2cbb0 <__gmpn_perfect_square_p@@Base+0x1dc>
   2cb00:	mov	x9, #0xa3a1                	// #41889
   2cb04:	movk	x9, #0x5f02, lsl #16
   2cb08:	movk	x9, #0xfd5c, lsl #32
   2cb0c:	mul	x8, x8, x9
   2cb10:	mov	w10, #0x61                  	// #97
   2cb14:	and	x8, x8, #0x1ffffffffffff
   2cb18:	mov	x9, #0x1b5f                	// #7007
   2cb1c:	mov	x11, #0x8b47                	// #35655
   2cb20:	mul	x8, x8, x10
   2cb24:	movk	x9, #0x8b45, lsl #16
   2cb28:	movk	x11, #0xeb62, lsl #16
   2cb2c:	lsr	x8, x8, #49
   2cb30:	movk	x9, #0x981b, lsl #32
   2cb34:	movk	x11, #0x1, lsl #32
   2cb38:	cmp	w8, #0x40
   2cb3c:	movk	x9, #0x6067, lsl #48
   2cb40:	csel	x9, x9, x11, cc  // cc = lo, ul, last
   2cb44:	lsr	x8, x9, x8
   2cb48:	tst	x8, #0x1
   2cb4c:	cset	w9, eq  // eq = none
   2cb50:	cbnz	w9, 2caf8 <__gmpn_perfect_square_p@@Base+0x124>
   2cb54:	add	x8, x20, #0x1
   2cb58:	add	x9, x20, #0x2
   2cb5c:	cmp	x8, #0x0
   2cb60:	csinc	x8, x9, x20, lt  // lt = tstop
   2cb64:	lsl	x8, x8, #2
   2cb68:	and	x1, x8, #0xfffffffffffffff8
   2cb6c:	mov	w8, #0x7f00                	// #32512
   2cb70:	cmp	x1, x8
   2cb74:	stur	xzr, [x29, #-8]
   2cb78:	b.hi	2cbc4 <__gmpn_perfect_square_p@@Base+0x1f0>  // b.pmore
   2cb7c:	add	x9, x1, #0xf
   2cb80:	mov	x8, sp
   2cb84:	and	x9, x9, #0xfffffffffffffff0
   2cb88:	sub	x0, x8, x9
   2cb8c:	mov	sp, x0
   2cb90:	mov	x1, xzr
   2cb94:	mov	x2, x19
   2cb98:	mov	x3, x20
   2cb9c:	bl	d590 <__gmpn_sqrtrem@plt>
   2cba0:	ldur	x8, [x29, #-8]
   2cba4:	cmp	x0, #0x0
   2cba8:	cset	w19, eq  // eq = none
   2cbac:	cbnz	x8, 2cbd0 <__gmpn_perfect_square_p@@Base+0x1fc>
   2cbb0:	mov	w0, w19
   2cbb4:	mov	sp, x29
   2cbb8:	ldp	x20, x19, [sp, #16]
   2cbbc:	ldp	x29, x30, [sp], #32
   2cbc0:	ret
   2cbc4:	sub	x0, x29, #0x8
   2cbc8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2cbcc:	b	2cb90 <__gmpn_perfect_square_p@@Base+0x1bc>
   2cbd0:	mov	x0, x8
   2cbd4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2cbd8:	b	2cbb0 <__gmpn_perfect_square_p@@Base+0x1dc>

000000000002cbdc <__gmpn_perfect_power_p@@Base>:
   2cbdc:	stp	x29, x30, [sp, #-80]!
   2cbe0:	stp	x26, x25, [sp, #16]
   2cbe4:	stp	x24, x23, [sp, #32]
   2cbe8:	stp	x22, x21, [sp, #48]
   2cbec:	stp	x20, x19, [sp, #64]
   2cbf0:	mov	x29, sp
   2cbf4:	sub	sp, sp, #0x30
   2cbf8:	mov	x19, x1
   2cbfc:	mov	x20, x0
   2cc00:	stur	x1, [x29, #-8]
   2cc04:	tbnz	x1, #63, 2cc14 <__gmpn_perfect_power_p@@Base+0x38>
   2cc08:	ldur	x21, [x29, #-8]
   2cc0c:	cbnz	x21, 2cc24 <__gmpn_perfect_power_p@@Base+0x48>
   2cc10:	b	2cc38 <__gmpn_perfect_power_p@@Base+0x5c>
   2cc14:	neg	x8, x19
   2cc18:	stur	x8, [x29, #-8]
   2cc1c:	ldur	x21, [x29, #-8]
   2cc20:	cbz	x21, 2cc38 <__gmpn_perfect_power_p@@Base+0x5c>
   2cc24:	cmp	x21, #0x1
   2cc28:	b.ne	2cc40 <__gmpn_perfect_power_p@@Base+0x64>  // b.any
   2cc2c:	ldr	x8, [x20]
   2cc30:	cmp	x8, #0x1
   2cc34:	b.ne	2cc40 <__gmpn_perfect_power_p@@Base+0x64>  // b.any
   2cc38:	mov	w24, #0x1                   	// #1
   2cc3c:	b	2ced8 <__gmpn_perfect_power_p@@Base+0x2fc>
   2cc40:	mov	x0, x20
   2cc44:	mov	x1, xzr
   2cc48:	stur	xzr, [x29, #-40]
   2cc4c:	bl	d320 <__gmpn_scan1@plt>
   2cc50:	mov	x22, x0
   2cc54:	cbz	x0, 2cc70 <__gmpn_perfect_power_p@@Base+0x94>
   2cc58:	subs	x9, x22, #0x1
   2cc5c:	b.ne	2cc78 <__gmpn_perfect_power_p@@Base+0x9c>  // b.any
   2cc60:	mov	w8, wzr
   2cc64:	mov	x23, xzr
   2cc68:	mov	w24, wzr
   2cc6c:	b	2cd30 <__gmpn_perfect_power_p@@Base+0x154>
   2cc70:	mov	x23, x22
   2cc74:	b	2cd34 <__gmpn_perfect_power_p@@Base+0x158>
   2cc78:	lsr	x8, x22, #6
   2cc7c:	add	x10, x8, #0x1
   2cc80:	cmp	x10, x21
   2cc84:	b.ne	2ccb8 <__gmpn_perfect_power_p@@Base+0xdc>  // b.any
   2cc88:	ldr	x10, [x20, x8, lsl #3]
   2cc8c:	sub	x11, x10, #0x1
   2cc90:	tst	x10, x11
   2cc94:	b.ne	2ccb8 <__gmpn_perfect_power_p@@Base+0xdc>  // b.any
   2cc98:	cmp	x19, #0x0
   2cc9c:	cset	w10, ge  // ge = tcont
   2cca0:	tst	x22, x9
   2cca4:	cset	w9, ne  // ne = any
   2cca8:	mov	w8, wzr
   2ccac:	mov	x23, xzr
   2ccb0:	orr	w24, w9, w10
   2ccb4:	b	2cd30 <__gmpn_perfect_power_p@@Base+0x154>
   2ccb8:	and	x23, x22, #0x3f
   2ccbc:	sub	x9, x21, x8
   2ccc0:	add	x20, x20, x8, lsl #3
   2ccc4:	stur	x9, [x29, #-8]
   2ccc8:	cbz	x23, 2cd2c <__gmpn_perfect_power_p@@Base+0x150>
   2cccc:	lsl	x1, x9, #3
   2ccd0:	mov	w8, #0x7f00                	// #32512
   2ccd4:	cmp	x1, x8
   2ccd8:	b.hi	2cf00 <__gmpn_perfect_power_p@@Base+0x324>  // b.pmore
   2ccdc:	add	x9, x1, #0xf
   2cce0:	mov	x8, sp
   2cce4:	and	x9, x9, #0xfffffffffffffff0
   2cce8:	sub	x21, x8, x9
   2ccec:	mov	sp, x21
   2ccf0:	ldur	x2, [x29, #-8]
   2ccf4:	mov	x0, x21
   2ccf8:	mov	x1, x20
   2ccfc:	mov	w3, w23
   2cd00:	bl	c2f0 <__gmpn_rshift@plt>
   2cd04:	ldur	x8, [x29, #-8]
   2cd08:	mov	x20, x21
   2cd0c:	add	x9, x21, x8, lsl #3
   2cd10:	ldur	x9, [x9, #-8]
   2cd14:	cmp	x9, #0x0
   2cd18:	cset	w9, eq  // eq = none
   2cd1c:	sub	x8, x8, x9
   2cd20:	stur	x8, [x29, #-8]
   2cd24:	mov	w8, #0x1                   	// #1
   2cd28:	b	2cd30 <__gmpn_perfect_power_p@@Base+0x154>
   2cd2c:	mov	w8, #0x1                   	// #1
   2cd30:	cbz	w8, 2ced8 <__gmpn_perfect_power_p@@Base+0x2fc>
   2cd34:	ldur	x1, [x29, #-8]
   2cd38:	mov	w8, #0x2                   	// #2
   2cd3c:	sub	x3, x29, #0x1c
   2cd40:	mov	x0, x20
   2cd44:	cmp	x1, #0x64
   2cd48:	cset	w9, gt
   2cd4c:	csinc	x8, x8, xzr, gt
   2cd50:	cmp	x1, #0x14
   2cd54:	csel	x25, x9, x8, le
   2cd58:	adrp	x8, 50000 <__gmpn_bases@@Base+0x2f98>
   2cd5c:	add	x8, x8, #0x80
   2cd60:	ldrh	w24, [x8, x25, lsl #1]
   2cd64:	stur	x22, [x29, #-16]
   2cd68:	stur	wzr, [x29, #-28]
   2cd6c:	mov	x2, x24
   2cd70:	bl	c7a0 <__gmpn_trialdiv@plt>
   2cd74:	cbz	x0, 2ce7c <__gmpn_perfect_power_p@@Base+0x2a0>
   2cd78:	mov	x22, x0
   2cd7c:	cbnz	x23, 2cda8 <__gmpn_perfect_power_p@@Base+0x1cc>
   2cd80:	ldur	x8, [x29, #-8]
   2cd84:	lsl	x1, x8, #3
   2cd88:	mov	w8, #0x7f00                	// #32512
   2cd8c:	cmp	x1, x8
   2cd90:	b.hi	2cf10 <__gmpn_perfect_power_p@@Base+0x334>  // b.pmore
   2cd94:	add	x9, x1, #0xf
   2cd98:	mov	x8, sp
   2cd9c:	and	x9, x9, #0xfffffffffffffff0
   2cda0:	sub	x21, x8, x9
   2cda4:	mov	sp, x21
   2cda8:	adrp	x23, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2cdac:	ldr	x23, [x23, #3952]
   2cdb0:	mov	w26, #0x2                   	// #2
   2cdb4:	b	2cdd4 <__gmpn_perfect_power_p@@Base+0x1f8>
   2cdb8:	sub	x3, x29, #0x1c
   2cdbc:	mov	x0, x21
   2cdc0:	mov	x2, x24
   2cdc4:	bl	c7a0 <__gmpn_trialdiv@plt>
   2cdc8:	mov	x22, x0
   2cdcc:	mov	x20, x21
   2cdd0:	cbz	x0, 2ce78 <__gmpn_perfect_power_p@@Base+0x29c>
   2cdd4:	ubfx	x8, x22, #1, #7
   2cdd8:	ldrb	w8, [x23, x8]
   2cddc:	ldur	x3, [x29, #-8]
   2cde0:	sub	x1, x29, #0x8
   2cde4:	sub	x4, x29, #0x18
   2cde8:	msub	x9, x22, x8, x26
   2cdec:	mul	x8, x9, x8
   2cdf0:	msub	x9, x8, x22, x26
   2cdf4:	mul	x8, x8, x9
   2cdf8:	msub	x9, x8, x22, x26
   2cdfc:	mul	x8, x8, x9
   2ce00:	mov	w5, #0x1                   	// #1
   2ce04:	mov	x6, #0xffffffffffffffff    	// #-1
   2ce08:	mov	x0, x21
   2ce0c:	mov	x2, x20
   2ce10:	stur	x8, [x29, #-24]
   2ce14:	bl	cf10 <__gmpn_remove@plt>
   2ce18:	ldur	x8, [x29, #-16]
   2ce1c:	mov	x2, x0
   2ce20:	cbz	x8, 2ce34 <__gmpn_perfect_power_p@@Base+0x258>
   2ce24:	sub	x0, x29, #0x10
   2ce28:	mov	w1, #0x1                   	// #1
   2ce2c:	bl	c0c0 <__gmpn_gcd_1@plt>
   2ce30:	mov	x2, x0
   2ce34:	subs	x8, x2, #0x1
   2ce38:	stur	x2, [x29, #-16]
   2ce3c:	b.eq	2ce70 <__gmpn_perfect_power_p@@Base+0x294>  // b.none
   2ce40:	ldur	x1, [x29, #-8]
   2ce44:	cmp	x1, #0x1
   2ce48:	b.ne	2cdb8 <__gmpn_perfect_power_p@@Base+0x1dc>  // b.any
   2ce4c:	ldr	x9, [x21]
   2ce50:	cmp	x9, #0x1
   2ce54:	b.ne	2cdb8 <__gmpn_perfect_power_p@@Base+0x1dc>  // b.any
   2ce58:	cmp	x19, #0x0
   2ce5c:	cset	w9, ge  // ge = tcont
   2ce60:	tst	x2, x8
   2ce64:	cset	w8, ne  // ne = any
   2ce68:	orr	w24, w8, w9
   2ce6c:	b	2ced0 <__gmpn_perfect_power_p@@Base+0x2f4>
   2ce70:	mov	w24, wzr
   2ce74:	b	2ced0 <__gmpn_perfect_power_p@@Base+0x2f4>
   2ce78:	mov	x20, x21
   2ce7c:	ldp	x3, x1, [x29, #-16]
   2ce80:	adrp	x8, 50000 <__gmpn_bases@@Base+0x2f98>
   2ce84:	add	x8, x8, #0x88
   2ce88:	ldr	d0, [x8, x25, lsl #3]
   2ce8c:	add	x8, x20, x1, lsl #3
   2ce90:	ldur	x8, [x8, #-8]
   2ce94:	adrp	x9, 50000 <__gmpn_bases@@Base+0x2f98>
   2ce98:	ldr	d1, [x9, #120]
   2ce9c:	lsl	x9, x1, #6
   2cea0:	clz	x8, x8
   2cea4:	sub	x4, x9, x8
   2cea8:	ucvtf	d2, x4
   2ceac:	fmul	d0, d0, d2
   2ceb0:	fadd	d0, d0, d1
   2ceb4:	fcvtzu	x8, d0
   2ceb8:	lsr	x5, x19, #63
   2cebc:	add	x2, x8, #0x1
   2cec0:	mov	x0, x20
   2cec4:	stur	x2, [x29, #-24]
   2cec8:	bl	2cf20 <__gmpn_perfect_power_p@@Base+0x344>
   2cecc:	mov	w24, w0
   2ced0:	ldur	x0, [x29, #-40]
   2ced4:	cbnz	x0, 2cef8 <__gmpn_perfect_power_p@@Base+0x31c>
   2ced8:	mov	w0, w24
   2cedc:	mov	sp, x29
   2cee0:	ldp	x20, x19, [sp, #64]
   2cee4:	ldp	x22, x21, [sp, #48]
   2cee8:	ldp	x24, x23, [sp, #32]
   2ceec:	ldp	x26, x25, [sp, #16]
   2cef0:	ldp	x29, x30, [sp], #80
   2cef4:	ret
   2cef8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2cefc:	b	2ced8 <__gmpn_perfect_power_p@@Base+0x2fc>
   2cf00:	sub	x0, x29, #0x28
   2cf04:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2cf08:	mov	x21, x0
   2cf0c:	b	2ccf0 <__gmpn_perfect_power_p@@Base+0x114>
   2cf10:	sub	x0, x29, #0x28
   2cf14:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2cf18:	mov	x21, x0
   2cf1c:	b	2cda8 <__gmpn_perfect_power_p@@Base+0x1cc>
   2cf20:	stp	x29, x30, [sp, #-96]!
   2cf24:	stp	x28, x27, [sp, #16]
   2cf28:	stp	x26, x25, [sp, #32]
   2cf2c:	stp	x24, x23, [sp, #48]
   2cf30:	stp	x22, x21, [sp, #64]
   2cf34:	stp	x20, x19, [sp, #80]
   2cf38:	mov	x29, sp
   2cf3c:	sub	sp, sp, #0x240
   2cf40:	mov	x19, sp
   2cf44:	mov	x22, x0
   2cf48:	add	x0, x19, #0x18
   2cf4c:	str	w5, [x19, #12]
   2cf50:	mov	x20, x4
   2cf54:	mov	x23, x3
   2cf58:	mov	x25, x2
   2cf5c:	mov	x21, x1
   2cf60:	str	xzr, [x19, #16]
   2cf64:	bl	d380 <__gmp_init_primesieve@plt>
   2cf68:	mov	w8, #0x38                  	// #56
   2cf6c:	mul	x1, x21, x8
   2cf70:	mov	w8, #0x7f00                	// #32512
   2cf74:	cmp	x1, x8
   2cf78:	add	x26, x20, #0x3
   2cf7c:	b.hi	2d0e8 <__gmpn_perfect_power_p@@Base+0x50c>  // b.pmore
   2cf80:	add	x9, x1, #0xf
   2cf84:	mov	x8, sp
   2cf88:	and	x9, x9, #0xfffffffffffffff0
   2cf8c:	sub	x24, x8, x9
   2cf90:	mov	sp, x24
   2cf94:	lsl	x2, x21, #3
   2cf98:	str	x26, [x19]
   2cf9c:	lsr	x28, x26, #1
   2cfa0:	add	x26, x24, x2
   2cfa4:	add	x27, x26, x2
   2cfa8:	cbz	x21, 2cfb8 <__gmpn_perfect_power_p@@Base+0x3dc>
   2cfac:	mov	x0, x26
   2cfb0:	mov	w1, wzr
   2cfb4:	bl	c780 <memset@plt>
   2cfb8:	sub	x8, x28, #0x1
   2cfbc:	lsr	x28, x8, #6
   2cfc0:	add	x2, x28, #0x1
   2cfc4:	mov	x0, x24
   2cfc8:	mov	x1, x22
   2cfcc:	mov	x3, x27
   2cfd0:	bl	cef0 <__gmpn_binvert@plt>
   2cfd4:	ldr	x8, [x19]
   2cfd8:	ubfx	x8, x8, #1, #6
   2cfdc:	cbz	x8, 2cff8 <__gmpn_perfect_power_p@@Base+0x41c>
   2cfe0:	lsl	x9, x28, #3
   2cfe4:	ldr	x10, [x24, x9]
   2cfe8:	mov	x11, #0xffffffffffffffff    	// #-1
   2cfec:	lsl	x8, x11, x8
   2cff0:	bic	x8, x10, x8
   2cff4:	str	x8, [x24, x9]
   2cff8:	ldr	w8, [x19, #12]
   2cffc:	cbz	w8, 2d008 <__gmpn_perfect_power_p@@Base+0x42c>
   2d000:	add	x0, x19, #0x18
   2d004:	bl	ce40 <__gmp_nextprime@plt>
   2d008:	cbz	x23, 2d0a8 <__gmpn_perfect_power_p@@Base+0x4cc>
   2d00c:	add	x8, x23, #0x1
   2d010:	cmp	x8, x25
   2d014:	add	x0, x19, #0x18
   2d018:	csinc	x25, x25, x23, hi  // hi = pmore
   2d01c:	bl	ce40 <__gmp_nextprime@plt>
   2d020:	cmp	x0, x25
   2d024:	b.cs	2d0b8 <__gmpn_perfect_power_p@@Base+0x4dc>  // b.hs, b.nlast
   2d028:	mov	x2, x0
   2d02c:	b	2d044 <__gmpn_perfect_power_p@@Base+0x468>
   2d030:	add	x0, x19, #0x18
   2d034:	bl	ce40 <__gmp_nextprime@plt>
   2d038:	mov	x2, x0
   2d03c:	cmp	x0, x25
   2d040:	b.cs	2d0b8 <__gmpn_perfect_power_p@@Base+0x4dc>  // b.hs, b.nlast
   2d044:	udiv	x8, x23, x2
   2d048:	msub	x8, x8, x2, x23
   2d04c:	cbnz	x8, 2d030 <__gmpn_perfect_power_p@@Base+0x454>
   2d050:	mov	x0, x26
   2d054:	mov	x1, x22
   2d058:	mov	x3, x24
   2d05c:	mov	x4, x21
   2d060:	mov	x5, x20
   2d064:	mov	x6, x27
   2d068:	bl	2d0f8 <__gmpn_perfect_power_p@@Base+0x51c>
   2d06c:	cbz	w0, 2d030 <__gmpn_perfect_power_p@@Base+0x454>
   2d070:	mov	w20, #0x1                   	// #1
   2d074:	ldr	x0, [x19, #16]
   2d078:	cbz	x0, 2d0c4 <__gmpn_perfect_power_p@@Base+0x4e8>
   2d07c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2d080:	b	2d0c4 <__gmpn_perfect_power_p@@Base+0x4e8>
   2d084:	mov	x2, x0
   2d088:	mov	x0, x26
   2d08c:	mov	x1, x22
   2d090:	mov	x3, x24
   2d094:	mov	x4, x21
   2d098:	mov	x5, x20
   2d09c:	mov	x6, x27
   2d0a0:	bl	2d0f8 <__gmpn_perfect_power_p@@Base+0x51c>
   2d0a4:	cbnz	w0, 2d070 <__gmpn_perfect_power_p@@Base+0x494>
   2d0a8:	add	x0, x19, #0x18
   2d0ac:	bl	ce40 <__gmp_nextprime@plt>
   2d0b0:	cmp	x0, x25
   2d0b4:	b.cc	2d084 <__gmpn_perfect_power_p@@Base+0x4a8>  // b.lo, b.ul, b.last
   2d0b8:	mov	w20, wzr
   2d0bc:	ldr	x0, [x19, #16]
   2d0c0:	cbnz	x0, 2d07c <__gmpn_perfect_power_p@@Base+0x4a0>
   2d0c4:	mov	w0, w20
   2d0c8:	mov	sp, x29
   2d0cc:	ldp	x20, x19, [sp, #80]
   2d0d0:	ldp	x22, x21, [sp, #64]
   2d0d4:	ldp	x24, x23, [sp, #48]
   2d0d8:	ldp	x26, x25, [sp, #32]
   2d0dc:	ldp	x28, x27, [sp, #16]
   2d0e0:	ldp	x29, x30, [sp], #96
   2d0e4:	ret
   2d0e8:	add	x0, x19, #0x10
   2d0ec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2d0f0:	mov	x24, x0
   2d0f4:	b	2cf94 <__gmpn_perfect_power_p@@Base+0x3b8>
   2d0f8:	stp	x29, x30, [sp, #-96]!
   2d0fc:	stp	x24, x23, [sp, #48]
   2d100:	stp	x22, x21, [sp, #64]
   2d104:	stp	x20, x19, [sp, #80]
   2d108:	mov	x20, x6
   2d10c:	mov	x21, x5
   2d110:	mov	x22, x4
   2d114:	mov	x23, x1
   2d118:	cmp	x2, #0x2
   2d11c:	mov	x19, x0
   2d120:	str	x27, [sp, #16]
   2d124:	stp	x26, x25, [sp, #32]
   2d128:	mov	x29, sp
   2d12c:	b.ne	2d198 <__gmpn_perfect_power_p@@Base+0x5bc>  // b.any
   2d130:	add	x8, x21, #0x1
   2d134:	lsr	x25, x8, #1
   2d138:	lsr	x26, x8, #7
   2d13c:	mov	x0, x19
   2d140:	mov	x1, x3
   2d144:	mov	x2, x25
   2d148:	mov	x3, x20
   2d14c:	add	x24, x26, #0x1
   2d150:	bl	c8e0 <__gmpn_bsqrtinv@plt>
   2d154:	cbz	w0, 2d228 <__gmpn_perfect_power_p@@Base+0x64c>
   2d158:	lsl	x27, x26, #3
   2d15c:	ldr	x8, [x19, x27]
   2d160:	mov	x9, #0xffffffffffffffff    	// #-1
   2d164:	lsl	x9, x9, x25
   2d168:	mvn	x25, x9
   2d16c:	bic	x8, x8, x9
   2d170:	str	x8, [x19, x27]
   2d174:	mov	x8, x26
   2d178:	add	x9, x8, #0x1
   2d17c:	cmp	x9, #0x1
   2d180:	b.lt	2d244 <__gmpn_perfect_power_p@@Base+0x668>  // b.tstop
   2d184:	ldr	x9, [x19, x8, lsl #3]
   2d188:	sub	x8, x8, #0x1
   2d18c:	cbz	x9, 2d178 <__gmpn_perfect_power_p@@Base+0x59c>
   2d190:	add	x3, x8, #0x2
   2d194:	b	2d248 <__gmpn_perfect_power_p@@Base+0x66c>
   2d198:	sub	x8, x21, #0x1
   2d19c:	udiv	x8, x8, x2
   2d1a0:	lsr	x24, x8, #6
   2d1a4:	mov	x25, x2
   2d1a8:	add	x26, x24, #0x1
   2d1ac:	mov	x0, x19
   2d1b0:	mov	x1, x3
   2d1b4:	mov	x2, x26
   2d1b8:	mov	x3, x25
   2d1bc:	mov	x4, x20
   2d1c0:	add	w27, w8, #0x1
   2d1c4:	bl	c840 <__gmpn_brootinv@plt>
   2d1c8:	ands	x8, x27, #0x3f
   2d1cc:	b.eq	2d1e8 <__gmpn_perfect_power_p@@Base+0x60c>  // b.none
   2d1d0:	lsl	x9, x24, #3
   2d1d4:	ldr	x10, [x19, x9]
   2d1d8:	mov	x11, #0xffffffffffffffff    	// #-1
   2d1dc:	lsl	x8, x11, x8
   2d1e0:	bic	x8, x10, x8
   2d1e4:	str	x8, [x19, x9]
   2d1e8:	mov	x24, x26
   2d1ec:	subs	x26, x26, #0x1
   2d1f0:	b.lt	2d200 <__gmpn_perfect_power_p@@Base+0x624>  // b.tstop
   2d1f4:	ldr	x8, [x19, x26, lsl #3]
   2d1f8:	cbz	x8, 2d1e8 <__gmpn_perfect_power_p@@Base+0x60c>
   2d1fc:	b	2d204 <__gmpn_perfect_power_p@@Base+0x628>
   2d200:	mov	x24, xzr
   2d204:	mov	x0, x23
   2d208:	mov	x1, x22
   2d20c:	mov	x2, x19
   2d210:	mov	x3, x24
   2d214:	mov	x4, x25
   2d218:	mov	x5, x21
   2d21c:	mov	x6, x20
   2d220:	bl	2d2e0 <__gmpn_perfect_power_p@@Base+0x704>
   2d224:	cbnz	w0, 2d268 <__gmpn_perfect_power_p@@Base+0x68c>
   2d228:	cbz	x24, 2d23c <__gmpn_perfect_power_p@@Base+0x660>
   2d22c:	lsl	x2, x24, #3
   2d230:	mov	x0, x19
   2d234:	mov	w1, wzr
   2d238:	bl	c780 <memset@plt>
   2d23c:	mov	w0, wzr
   2d240:	b	2d26c <__gmpn_perfect_power_p@@Base+0x690>
   2d244:	mov	x3, xzr
   2d248:	mov	w4, #0x2                   	// #2
   2d24c:	mov	x0, x23
   2d250:	mov	x1, x22
   2d254:	mov	x2, x19
   2d258:	mov	x5, x21
   2d25c:	mov	x6, x20
   2d260:	bl	2d2e0 <__gmpn_perfect_power_p@@Base+0x704>
   2d264:	cbz	w0, 2d288 <__gmpn_perfect_power_p@@Base+0x6ac>
   2d268:	mov	w0, #0x1                   	// #1
   2d26c:	ldp	x20, x19, [sp, #80]
   2d270:	ldp	x22, x21, [sp, #64]
   2d274:	ldp	x24, x23, [sp, #48]
   2d278:	ldp	x26, x25, [sp, #32]
   2d27c:	ldr	x27, [sp, #16]
   2d280:	ldp	x29, x30, [sp], #96
   2d284:	ret
   2d288:	mov	x0, x19
   2d28c:	mov	x1, x19
   2d290:	mov	x2, x24
   2d294:	bl	ce90 <__gmpn_neg@plt>
   2d298:	ldr	x8, [x19, x27]
   2d29c:	and	x8, x8, x25
   2d2a0:	str	x8, [x19, x27]
   2d2a4:	add	x8, x26, #0x1
   2d2a8:	cmp	x8, #0x1
   2d2ac:	b.lt	2d2c4 <__gmpn_perfect_power_p@@Base+0x6e8>  // b.tstop
   2d2b0:	ldr	x8, [x19, x26, lsl #3]
   2d2b4:	sub	x26, x26, #0x1
   2d2b8:	cbz	x8, 2d2a4 <__gmpn_perfect_power_p@@Base+0x6c8>
   2d2bc:	add	x24, x26, #0x2
   2d2c0:	b	2d2c8 <__gmpn_perfect_power_p@@Base+0x6ec>
   2d2c4:	mov	x24, xzr
   2d2c8:	mov	w4, #0x2                   	// #2
   2d2cc:	mov	x0, x23
   2d2d0:	mov	x1, x22
   2d2d4:	mov	x2, x19
   2d2d8:	mov	x3, x24
   2d2dc:	b	2d218 <__gmpn_perfect_power_p@@Base+0x63c>
   2d2e0:	stp	x29, x30, [sp, #-80]!
   2d2e4:	stp	x26, x25, [sp, #16]
   2d2e8:	stp	x24, x23, [sp, #32]
   2d2ec:	stp	x22, x21, [sp, #48]
   2d2f0:	stp	x20, x19, [sp, #64]
   2d2f4:	mov	x29, sp
   2d2f8:	sub	sp, sp, #0x10
   2d2fc:	mov	x19, x6
   2d300:	mov	x24, x5
   2d304:	mov	x22, x3
   2d308:	mov	x23, x2
   2d30c:	mov	x20, x1
   2d310:	mov	x21, x0
   2d314:	cmp	x3, #0x1
   2d318:	stur	x4, [x29, #-8]
   2d31c:	b.ne	2d334 <__gmpn_perfect_power_p@@Base+0x758>  // b.any
   2d320:	ldr	x8, [x23]
   2d324:	cmp	x8, #0x1
   2d328:	b.ne	2d334 <__gmpn_perfect_power_p@@Base+0x758>  // b.any
   2d32c:	mov	w24, wzr
   2d330:	b	2d438 <__gmpn_perfect_power_p@@Base+0x85c>
   2d334:	asr	x8, x20, #1
   2d338:	add	x26, x8, #0x1
   2d33c:	cmp	x26, #0x2
   2d340:	b.cc	2d384 <__gmpn_perfect_power_p@@Base+0x7a8>  // b.lo, b.ul, b.last
   2d344:	mov	w25, #0x1                   	// #1
   2d348:	add	x5, x19, x25, lsl #3
   2d34c:	sub	x2, x29, #0x8
   2d350:	mov	w3, #0x1                   	// #1
   2d354:	mov	x0, x19
   2d358:	mov	x1, x23
   2d35c:	mov	x4, x25
   2d360:	bl	c520 <__gmpn_powlo@plt>
   2d364:	mov	x0, x19
   2d368:	mov	x1, x21
   2d36c:	mov	x2, x25
   2d370:	bl	c570 <__gmpn_cmp@plt>
   2d374:	cbnz	w0, 2d32c <__gmpn_perfect_power_p@@Base+0x750>
   2d378:	lsl	x25, x25, #1
   2d37c:	cmp	x25, x26
   2d380:	b.cc	2d348 <__gmpn_perfect_power_p@@Base+0x76c>  // b.lo, b.ul, b.last
   2d384:	add	x8, x23, x22, lsl #3
   2d388:	ldur	x9, [x8, #-8]
   2d38c:	ldur	x8, [x29, #-8]
   2d390:	sub	x12, x24, #0x1
   2d394:	mov	w24, wzr
   2d398:	clz	x9, x9
   2d39c:	mvn	x9, x9
   2d3a0:	add	x11, x9, x22, lsl #6
   2d3a4:	mul	x9, x11, x8
   2d3a8:	cmp	x9, #0x0
   2d3ac:	sub	x9, x9, #0x1
   2d3b0:	cset	w10, eq  // eq = none
   2d3b4:	cmp	x9, x12
   2d3b8:	b.hi	2d438 <__gmpn_perfect_power_p@@Base+0x85c>  // b.pmore
   2d3bc:	umulh	x11, x8, x11
   2d3c0:	cmp	x11, x10
   2d3c4:	b.ne	2d438 <__gmpn_perfect_power_p@@Base+0x85c>  // b.any
   2d3c8:	adds	x8, x9, x8
   2d3cc:	b.cs	2d47c <__gmpn_perfect_power_p@@Base+0x8a0>  // b.hs, b.nlast
   2d3d0:	lsr	x8, x8, #6
   2d3d4:	lsl	x9, x8, #3
   2d3d8:	cmp	x8, #0xfde
   2d3dc:	add	x1, x9, #0x10
   2d3e0:	stur	xzr, [x29, #-16]
   2d3e4:	b.hi	2d46c <__gmpn_perfect_power_p@@Base+0x890>  // b.pmore
   2d3e8:	add	x9, x1, #0xf
   2d3ec:	mov	x8, sp
   2d3f0:	and	x9, x9, #0x7ffffffffffffff0
   2d3f4:	sub	x4, x8, x9
   2d3f8:	mov	sp, x4
   2d3fc:	ldur	x3, [x29, #-8]
   2d400:	mov	x0, x19
   2d404:	mov	x1, x23
   2d408:	mov	x2, x22
   2d40c:	bl	d3f0 <__gmpn_pow_1@plt>
   2d410:	cmp	x0, x20
   2d414:	b.ne	2d42c <__gmpn_perfect_power_p@@Base+0x850>  // b.any
   2d418:	mov	x0, x19
   2d41c:	mov	x1, x21
   2d420:	mov	x2, x20
   2d424:	bl	c570 <__gmpn_cmp@plt>
   2d428:	cbz	w0, 2d458 <__gmpn_perfect_power_p@@Base+0x87c>
   2d42c:	mov	w24, wzr
   2d430:	ldur	x0, [x29, #-16]
   2d434:	cbnz	x0, 2d464 <__gmpn_perfect_power_p@@Base+0x888>
   2d438:	mov	w0, w24
   2d43c:	mov	sp, x29
   2d440:	ldp	x20, x19, [sp, #64]
   2d444:	ldp	x22, x21, [sp, #48]
   2d448:	ldp	x24, x23, [sp, #32]
   2d44c:	ldp	x26, x25, [sp, #16]
   2d450:	ldp	x29, x30, [sp], #80
   2d454:	ret
   2d458:	mov	w24, #0x1                   	// #1
   2d45c:	ldur	x0, [x29, #-16]
   2d460:	cbz	x0, 2d438 <__gmpn_perfect_power_p@@Base+0x85c>
   2d464:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2d468:	b	2d438 <__gmpn_perfect_power_p@@Base+0x85c>
   2d46c:	sub	x0, x29, #0x10
   2d470:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2d474:	mov	x4, x0
   2d478:	b	2d3fc <__gmpn_perfect_power_p@@Base+0x820>
   2d47c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   2d480:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   2d484:	add	x0, x0, #0xa0
   2d488:	add	x2, x2, #0xaa
   2d48c:	mov	w1, #0x60                  	// #96
   2d490:	bl	c850 <__gmp_assert_fail@plt>

000000000002d494 <__gmpn_strongfibo@@Base>:
   2d494:	stp	x29, x30, [sp, #-80]!
   2d498:	stp	x20, x19, [sp, #64]
   2d49c:	mov	x20, x1
   2d4a0:	mov	x1, xzr
   2d4a4:	str	x25, [sp, #16]
   2d4a8:	stp	x24, x23, [sp, #32]
   2d4ac:	stp	x22, x21, [sp, #48]
   2d4b0:	mov	x29, sp
   2d4b4:	mov	x23, x2
   2d4b8:	mov	x21, x0
   2d4bc:	bl	c820 <__gmpn_scan0@plt>
   2d4c0:	lsr	x8, x0, #6
   2d4c4:	mov	x19, x0
   2d4c8:	sub	x22, x20, x8
   2d4cc:	and	w3, w19, #0x3f
   2d4d0:	add	x1, x21, x8, lsl #3
   2d4d4:	mov	x0, x23
   2d4d8:	mov	x2, x22
   2d4dc:	cbz	w3, 2d61c <__gmpn_strongfibo@@Base+0x188>
   2d4e0:	bl	c2f0 <__gmpn_rshift@plt>
   2d4e4:	ldr	x8, [x23]
   2d4e8:	add	x9, x23, x22, lsl #3
   2d4ec:	mov	w10, #0x7f00                	// #32512
   2d4f0:	orr	x8, x8, #0x1
   2d4f4:	str	x8, [x23]
   2d4f8:	ldur	x8, [x9, #-8]
   2d4fc:	lsl	x9, x20, #5
   2d500:	add	x1, x9, #0x30
   2d504:	str	xzr, [x29, #24]
   2d508:	cmp	x8, #0x0
   2d50c:	cset	w8, eq  // eq = none
   2d510:	cmp	x1, x10
   2d514:	sub	x25, x22, x8
   2d518:	b.hi	2d624 <__gmpn_strongfibo@@Base+0x190>  // b.pmore
   2d51c:	add	x9, x1, #0xf
   2d520:	mov	x8, sp
   2d524:	and	x9, x9, #0xfffffffffffffff0
   2d528:	sub	x22, x8, x9
   2d52c:	mov	sp, x22
   2d530:	add	x8, x22, x20, lsl #4
   2d534:	add	x24, x8, #0x18
   2d538:	mov	x0, x24
   2d53c:	mov	x1, x23
   2d540:	mov	x2, x25
   2d544:	mov	x3, x21
   2d548:	mov	x4, x20
   2d54c:	mov	x5, x22
   2d550:	bl	2d664 <__gmpn_strongfibo@@Base+0x1d0>
   2d554:	cbz	x0, 2d5f0 <__gmpn_strongfibo@@Base+0x15c>
   2d558:	subs	x8, x19, #0x1
   2d55c:	b.eq	2d634 <__gmpn_strongfibo@@Base+0x1a0>  // b.none
   2d560:	mov	x23, x0
   2d564:	mov	x0, x22
   2d568:	mov	x1, x24
   2d56c:	mov	x2, x23
   2d570:	bl	ca90 <__gmpn_sqr@plt>
   2d574:	ldr	x8, [x22]
   2d578:	lsl	x4, x23, #1
   2d57c:	cmp	x4, x20
   2d580:	orr	x8, x8, #0x2
   2d584:	str	x8, [x22]
   2d588:	b.lt	2d648 <__gmpn_strongfibo@@Base+0x1b4>  // b.tstop
   2d58c:	mov	x0, x24
   2d590:	mov	x1, x22
   2d594:	mov	x2, xzr
   2d598:	mov	x3, x22
   2d59c:	mov	x5, x21
   2d5a0:	mov	x6, x20
   2d5a4:	bl	c030 <__gmpn_tdiv_qr@plt>
   2d5a8:	mov	x0, x22
   2d5ac:	mov	x1, x20
   2d5b0:	bl	c010 <__gmpn_zero_p@plt>
   2d5b4:	cbz	w0, 2d5c8 <__gmpn_strongfibo@@Base+0x134>
   2d5b8:	mov	w19, #0x1                   	// #1
   2d5bc:	ldr	x0, [x29, #24]
   2d5c0:	cbz	x0, 2d5f8 <__gmpn_strongfibo@@Base+0x164>
   2d5c4:	b	2d640 <__gmpn_strongfibo@@Base+0x1ac>
   2d5c8:	subs	x19, x19, #0x2
   2d5cc:	b.eq	2d5f0 <__gmpn_strongfibo@@Base+0x15c>  // b.none
   2d5d0:	add	x8, x22, x20, lsl #3
   2d5d4:	add	x4, x8, #0x8
   2d5d8:	mov	x0, x22
   2d5dc:	mov	x1, x21
   2d5e0:	mov	x2, x20
   2d5e4:	mov	x3, x19
   2d5e8:	bl	2d764 <__gmpn_strongfibo@@Base+0x2d0>
   2d5ec:	mov	x19, x0
   2d5f0:	ldr	x0, [x29, #24]
   2d5f4:	cbnz	x0, 2d640 <__gmpn_strongfibo@@Base+0x1ac>
   2d5f8:	cmp	x19, #0x0
   2d5fc:	cset	w0, ne  // ne = any
   2d600:	mov	sp, x29
   2d604:	ldp	x20, x19, [sp, #64]
   2d608:	ldp	x22, x21, [sp, #48]
   2d60c:	ldp	x24, x23, [sp, #32]
   2d610:	ldr	x25, [sp, #16]
   2d614:	ldp	x29, x30, [sp], #80
   2d618:	ret
   2d61c:	bl	cc10 <__gmpn_copyi@plt>
   2d620:	b	2d4e4 <__gmpn_strongfibo@@Base+0x50>
   2d624:	add	x0, x29, #0x18
   2d628:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2d62c:	mov	x22, x0
   2d630:	b	2d530 <__gmpn_strongfibo@@Base+0x9c>
   2d634:	mov	x19, x8
   2d638:	ldr	x0, [x29, #24]
   2d63c:	cbz	x0, 2d5f8 <__gmpn_strongfibo@@Base+0x164>
   2d640:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2d644:	b	2d5f8 <__gmpn_strongfibo@@Base+0x164>
   2d648:	lsl	x8, x23, #4
   2d64c:	lsl	x9, x20, #3
   2d650:	add	x0, x22, x8
   2d654:	sub	x2, x9, x8
   2d658:	mov	w1, wzr
   2d65c:	bl	c780 <memset@plt>
   2d660:	b	2d5a8 <__gmpn_strongfibo@@Base+0x114>
   2d664:	stp	x29, x30, [sp, #-64]!
   2d668:	stp	x20, x19, [sp, #48]
   2d66c:	mov	x19, x4
   2d670:	mov	x20, x3
   2d674:	stp	x22, x21, [sp, #32]
   2d678:	mov	x22, x5
   2d67c:	mov	x3, x2
   2d680:	mov	x2, x1
   2d684:	mov	x1, x5
   2d688:	mov	x4, x20
   2d68c:	mov	x5, x19
   2d690:	str	x23, [sp, #16]
   2d694:	mov	x29, sp
   2d698:	mov	x21, x0
   2d69c:	bl	d220 <__gmpn_fib2m@plt>
   2d6a0:	mov	w23, w0
   2d6a4:	mov	x0, x21
   2d6a8:	mov	x1, x19
   2d6ac:	bl	c010 <__gmpn_zero_p@plt>
   2d6b0:	cbz	w0, 2d6bc <__gmpn_strongfibo@@Base+0x228>
   2d6b4:	mov	x0, xzr
   2d6b8:	b	2d750 <__gmpn_strongfibo@@Base+0x2bc>
   2d6bc:	mov	x0, x21
   2d6c0:	mov	x1, x21
   2d6c4:	mov	x2, x22
   2d6c8:	mov	x3, x19
   2d6cc:	cbz	w23, 2d6fc <__gmpn_strongfibo@@Base+0x268>
   2d6d0:	bl	d260 <__gmpn_rsblsh1_n@plt>
   2d6d4:	mov	x22, x0
   2d6d8:	cmp	x0, #0x2
   2d6dc:	b.cc	2d720 <__gmpn_strongfibo@@Base+0x28c>  // b.lo, b.ul, b.last
   2d6e0:	mov	x0, x21
   2d6e4:	mov	x1, x21
   2d6e8:	mov	x2, x20
   2d6ec:	mov	x3, x19
   2d6f0:	bl	cc30 <__gmpn_add_n@plt>
   2d6f4:	add	x22, x0, x22
   2d6f8:	b	2d720 <__gmpn_strongfibo@@Base+0x28c>
   2d6fc:	bl	ce00 <__gmpn_addlsh1_n@plt>
   2d700:	mov	x22, x0
   2d704:	b	2d720 <__gmpn_strongfibo@@Base+0x28c>
   2d708:	mov	x0, x21
   2d70c:	mov	x1, x21
   2d710:	mov	x2, x20
   2d714:	mov	x3, x19
   2d718:	bl	c420 <__gmpn_sub_n@plt>
   2d71c:	sub	x22, x22, x0
   2d720:	cbnz	x22, 2d708 <__gmpn_strongfibo@@Base+0x274>
   2d724:	mov	x0, x21
   2d728:	mov	x1, x20
   2d72c:	mov	x2, x19
   2d730:	bl	c570 <__gmpn_cmp@plt>
   2d734:	tbz	w0, #31, 2d708 <__gmpn_strongfibo@@Base+0x274>
   2d738:	sub	x8, x21, #0x8
   2d73c:	mov	x0, x19
   2d740:	subs	x19, x19, #0x1
   2d744:	b.lt	2d750 <__gmpn_strongfibo@@Base+0x2bc>  // b.tstop
   2d748:	ldr	x9, [x8, x0, lsl #3]
   2d74c:	cbz	x9, 2d73c <__gmpn_strongfibo@@Base+0x2a8>
   2d750:	ldp	x20, x19, [sp, #48]
   2d754:	ldp	x22, x21, [sp, #32]
   2d758:	ldr	x23, [sp, #16]
   2d75c:	ldp	x29, x30, [sp], #64
   2d760:	ret
   2d764:	stp	x29, x30, [sp, #-96]!
   2d768:	stp	x28, x27, [sp, #16]
   2d76c:	stp	x26, x25, [sp, #32]
   2d770:	stp	x24, x23, [sp, #48]
   2d774:	stp	x22, x21, [sp, #64]
   2d778:	stp	x20, x19, [sp, #80]
   2d77c:	mov	x20, x4
   2d780:	mov	x19, x3
   2d784:	mov	x21, x2
   2d788:	mov	x22, x1
   2d78c:	mov	x23, x0
   2d790:	lsl	x24, x2, #1
   2d794:	add	x25, x4, x2, lsl #4
   2d798:	add	x26, x0, #0x8
   2d79c:	sub	x27, x2, #0x1
   2d7a0:	mov	x29, sp
   2d7a4:	b	2d7b8 <__gmpn_strongfibo@@Base+0x324>
   2d7a8:	sub	x8, x28, #0x2
   2d7ac:	str	x8, [x23]
   2d7b0:	subs	x19, x19, #0x1
   2d7b4:	b.eq	2d834 <__gmpn_strongfibo@@Base+0x3a0>  // b.none
   2d7b8:	mov	x0, x20
   2d7bc:	mov	x1, x23
   2d7c0:	mov	x2, x21
   2d7c4:	bl	ca90 <__gmpn_sqr@plt>
   2d7c8:	mov	x0, x25
   2d7cc:	mov	x1, x23
   2d7d0:	mov	x2, xzr
   2d7d4:	mov	x3, x20
   2d7d8:	mov	x4, x24
   2d7dc:	mov	x5, x22
   2d7e0:	mov	x6, x21
   2d7e4:	bl	c030 <__gmpn_tdiv_qr@plt>
   2d7e8:	ldr	x28, [x23]
   2d7ec:	cmp	x28, #0x4
   2d7f0:	b.hi	2d7a8 <__gmpn_strongfibo@@Base+0x314>  // b.pmore
   2d7f4:	cmp	x21, #0x1
   2d7f8:	b.eq	2d83c <__gmpn_strongfibo@@Base+0x3a8>  // b.none
   2d7fc:	mov	x0, x26
   2d800:	mov	x1, x27
   2d804:	bl	c010 <__gmpn_zero_p@plt>
   2d808:	cbnz	w0, 2d83c <__gmpn_strongfibo@@Base+0x3a8>
   2d80c:	sub	x8, x28, #0x2
   2d810:	cmp	x28, #0x1
   2d814:	str	x8, [x23]
   2d818:	b.hi	2d7b0 <__gmpn_strongfibo@@Base+0x31c>  // b.pmore
   2d81c:	mov	x8, x26
   2d820:	ldr	x9, [x8]
   2d824:	sub	x10, x9, #0x1
   2d828:	str	x10, [x8], #8
   2d82c:	cbz	x9, 2d820 <__gmpn_strongfibo@@Base+0x38c>
   2d830:	b	2d7b0 <__gmpn_strongfibo@@Base+0x31c>
   2d834:	mov	x0, xzr
   2d838:	b	2d844 <__gmpn_strongfibo@@Base+0x3b0>
   2d83c:	cmp	x28, #0x2
   2d840:	csel	x0, x19, xzr, eq  // eq = none
   2d844:	ldp	x20, x19, [sp, #80]
   2d848:	ldp	x22, x21, [sp, #64]
   2d84c:	ldp	x24, x23, [sp, #48]
   2d850:	ldp	x26, x25, [sp, #32]
   2d854:	ldp	x28, x27, [sp, #16]
   2d858:	ldp	x29, x30, [sp], #96
   2d85c:	ret

000000000002d860 <__gmpn_gcd_11@@Base>:
   2d860:	subs	x3, x0, x1
   2d864:	b.eq	2d88c <__gmpn_gcd_11@@Base+0x2c>  // b.none
   2d868:	nop
   2d86c:	nop
   2d870:	rbit	x12, x3
   2d874:	clz	x12, x12
   2d878:	cneg	x3, x3, cc  // cc = lo, ul, last
   2d87c:	csel	x0, x1, x0, cs  // cs = hs, nlast
   2d880:	lsr	x1, x3, x12
   2d884:	subs	x3, x0, x1
   2d888:	b.ne	2d870 <__gmpn_gcd_11@@Base+0x10>  // b.any
   2d88c:	ret

000000000002d890 <__gmpn_gcd_22@@Base>:
   2d890:	subs	x5, x1, x3
   2d894:	cbz	x5, 2d908 <__gmpn_gcd_22@@Base+0x78>
   2d898:	sbcs	x6, x0, x2
   2d89c:	rbit	x7, x5
   2d8a0:	cneg	x5, x5, cc  // cc = lo, ul, last
   2d8a4:	cinv	x6, x6, cc  // cc = lo, ul, last
   2d8a8:	csel	x3, x3, x1, cs  // cs = hs, nlast
   2d8ac:	csel	x2, x2, x0, cs  // cs = hs, nlast
   2d8b0:	clz	x7, x7
   2d8b4:	neg	x8, x7
   2d8b8:	lsr	x1, x5, x7
   2d8bc:	lsl	x14, x6, x8
   2d8c0:	lsr	x0, x6, x7
   2d8c4:	orr	x1, x1, x14
   2d8c8:	orr	x11, x0, x2
   2d8cc:	cbnz	x11, 2d890 <__gmpn_gcd_22@@Base>
   2d8d0:	subs	x4, x1, x3
   2d8d4:	b.eq	2d8fc <__gmpn_gcd_22@@Base+0x6c>  // b.none
   2d8d8:	nop
   2d8dc:	nop
   2d8e0:	rbit	x12, x4
   2d8e4:	clz	x12, x12
   2d8e8:	cneg	x4, x4, cc  // cc = lo, ul, last
   2d8ec:	csel	x1, x3, x1, cs  // cs = hs, nlast
   2d8f0:	lsr	x3, x4, x12
   2d8f4:	subs	x4, x1, x3
   2d8f8:	b.ne	2d8e0 <__gmpn_gcd_22@@Base+0x50>  // b.any
   2d8fc:	mov	x0, x1
   2d900:	mov	x1, #0x0                   	// #0
   2d904:	ret
   2d908:	subs	x5, x0, x2
   2d90c:	b.eq	2d920 <__gmpn_gcd_22@@Base+0x90>  // b.none
   2d910:	mov	x6, #0x0                   	// #0
   2d914:	rbit	x7, x5
   2d918:	cneg	x5, x5, cc  // cc = lo, ul, last
   2d91c:	b	2d8a8 <__gmpn_gcd_22@@Base+0x18>
   2d920:	mov	x0, x3
   2d924:	mov	x1, x2
   2d928:	ret

000000000002d92c <__gmpn_gcd_1@@Base>:
   2d92c:	stp	x29, x30, [sp, #-32]!
   2d930:	stp	x20, x19, [sp, #16]
   2d934:	ldr	x9, [x0]
   2d938:	rbit	x8, x2
   2d93c:	clz	x8, x8
   2d940:	cmp	x1, #0x2
   2d944:	rbit	x10, x9
   2d948:	lsr	x19, x2, x8
   2d94c:	clz	x10, x10
   2d950:	mov	x29, sp
   2d954:	b.lt	2d97c <__gmpn_gcd_1@@Base+0x50>  // b.tstop
   2d958:	cmp	x8, x10
   2d95c:	ccmp	x9, #0x0, #0x4, cs  // cs = hs, nlast
   2d960:	csel	x20, x8, x10, eq  // eq = none
   2d964:	mov	x2, x19
   2d968:	cmp	x1, #0x27
   2d96c:	b.le	2d9b8 <__gmpn_gcd_1@@Base+0x8c>
   2d970:	bl	c540 <__gmpn_mod_1@plt>
   2d974:	cbnz	x0, 2d9c4 <__gmpn_gcd_1@@Base+0x98>
   2d978:	b	2d9dc <__gmpn_gcd_1@@Base+0xb0>
   2d97c:	lsr	x9, x9, x10
   2d980:	cmp	x8, x10
   2d984:	csel	x20, x8, x10, cc  // cc = lo, ul, last
   2d988:	cmp	x19, x9
   2d98c:	csel	x0, x19, x9, hi  // hi = pmore
   2d990:	csel	x19, x9, x19, hi  // hi = pmore
   2d994:	cmp	x19, x0, lsr #16
   2d998:	b.cs	2d9d0 <__gmpn_gcd_1@@Base+0xa4>  // b.hs, b.nlast
   2d99c:	udiv	x8, x0, x19
   2d9a0:	msub	x8, x8, x19, x0
   2d9a4:	cbz	x8, 2d9dc <__gmpn_gcd_1@@Base+0xb0>
   2d9a8:	rbit	x9, x8
   2d9ac:	clz	x9, x9
   2d9b0:	lsr	x0, x8, x9
   2d9b4:	b	2d9d0 <__gmpn_gcd_1@@Base+0xa4>
   2d9b8:	mov	x3, xzr
   2d9bc:	bl	c990 <__gmpn_modexact_1c_odd@plt>
   2d9c0:	cbz	x0, 2d9dc <__gmpn_gcd_1@@Base+0xb0>
   2d9c4:	rbit	x8, x0
   2d9c8:	clz	x8, x8
   2d9cc:	lsr	x0, x0, x8
   2d9d0:	mov	x1, x19
   2d9d4:	bl	d520 <__gmpn_gcd_11@plt>
   2d9d8:	mov	x19, x0
   2d9dc:	lsl	x0, x19, x20
   2d9e0:	ldp	x20, x19, [sp, #16]
   2d9e4:	ldp	x29, x30, [sp], #32
   2d9e8:	ret

000000000002d9ec <__gmpn_gcd@@Base>:
   2d9ec:	stp	x29, x30, [sp, #-96]!
   2d9f0:	stp	x28, x27, [sp, #16]
   2d9f4:	stp	x26, x25, [sp, #32]
   2d9f8:	stp	x24, x23, [sp, #48]
   2d9fc:	stp	x22, x21, [sp, #64]
   2da00:	stp	x20, x19, [sp, #80]
   2da04:	mov	x29, sp
   2da08:	sub	sp, sp, #0x50
   2da0c:	sub	x8, x2, x4
   2da10:	cmp	x8, x4
   2da14:	mov	x22, x4
   2da18:	mov	x20, x3
   2da1c:	mov	x24, x2
   2da20:	mov	x21, x1
   2da24:	csinc	x23, x4, x8, lt  // lt = tstop
   2da28:	cmp	x4, #0x14a
   2da2c:	mov	x19, x0
   2da30:	b.lt	2da88 <__gmpn_gcd@@Base+0x9c>  // b.tstop
   2da34:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   2da38:	lsl	x8, x22, #1
   2da3c:	movk	x9, #0x5556
   2da40:	smulh	x8, x8, x9
   2da44:	add	x25, x8, x8, lsr #63
   2da48:	sub	x0, x22, x25
   2da4c:	add	x8, x0, #0x1
   2da50:	add	x9, x0, #0x2
   2da54:	cmp	x8, #0x0
   2da58:	csinc	x8, x9, x0, lt  // lt = tstop
   2da5c:	lsl	x8, x8, #1
   2da60:	and	x26, x8, #0xfffffffffffffffc
   2da64:	bl	c720 <__gmpn_hgcd_itch@plt>
   2da68:	add	x8, x25, x22
   2da6c:	sub	x9, x8, #0x1
   2da70:	cmp	x0, x8
   2da74:	csel	x8, x9, x0, lt  // lt = tstop
   2da78:	add	x8, x26, x8
   2da7c:	add	x8, x8, #0x4
   2da80:	cmp	x8, x23
   2da84:	csel	x23, x8, x23, gt
   2da88:	lsl	x1, x23, #3
   2da8c:	mov	w8, #0x7f00                	// #32512
   2da90:	cmp	x1, x8
   2da94:	stur	xzr, [x29, #-24]
   2da98:	b.hi	2db00 <__gmpn_gcd@@Base+0x114>  // b.pmore
   2da9c:	add	x9, x1, #0xf
   2daa0:	mov	x8, sp
   2daa4:	and	x9, x9, #0xfffffffffffffff0
   2daa8:	sub	x23, x8, x9
   2daac:	mov	sp, x23
   2dab0:	cmp	x24, x22
   2dab4:	b.le	2db14 <__gmpn_gcd@@Base+0x128>
   2dab8:	mov	x0, x23
   2dabc:	mov	x1, x21
   2dac0:	mov	x2, xzr
   2dac4:	mov	x3, x21
   2dac8:	mov	x4, x24
   2dacc:	mov	x5, x20
   2dad0:	mov	x6, x22
   2dad4:	bl	c030 <__gmpn_tdiv_qr@plt>
   2dad8:	mov	x0, x21
   2dadc:	mov	x1, x22
   2dae0:	bl	c010 <__gmpn_zero_p@plt>
   2dae4:	cbz	w0, 2db14 <__gmpn_gcd@@Base+0x128>
   2dae8:	mov	x0, x19
   2daec:	mov	x1, x20
   2daf0:	mov	x2, x22
   2daf4:	bl	cc10 <__gmpn_copyi@plt>
   2daf8:	stur	x22, [x29, #-8]
   2dafc:	b	2dcf4 <__gmpn_gcd@@Base+0x308>
   2db00:	sub	x0, x29, #0x18
   2db04:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2db08:	mov	x23, x0
   2db0c:	cmp	x24, x22
   2db10:	b.gt	2dab8 <__gmpn_gcd@@Base+0xcc>
   2db14:	mov	x28, #0x5555555555555555    	// #6148914691236517205
   2db18:	adrp	x24, 2d000 <__gmpn_perfect_power_p@@Base+0x424>
   2db1c:	movk	x28, #0x5556
   2db20:	add	x24, x24, #0xdd4
   2db24:	stur	x19, [x29, #-16]
   2db28:	b	2db54 <__gmpn_gcd@@Base+0x168>
   2db2c:	add	x1, x0, x25
   2db30:	sub	x0, x29, #0x48
   2db34:	mov	x2, x21
   2db38:	mov	x3, x20
   2db3c:	mov	x4, x25
   2db40:	mov	x5, x26
   2db44:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   2db48:	mov	x22, x0
   2db4c:	mov	w8, wzr
   2db50:	cbnz	w8, 2dcec <__gmpn_gcd@@Base+0x300>
   2db54:	cmp	x22, #0x149
   2db58:	b.le	2dbf4 <__gmpn_gcd@@Base+0x208>
   2db5c:	lsl	x8, x22, #1
   2db60:	smulh	x8, x8, x28
   2db64:	add	x25, x8, x8, lsr #63
   2db68:	sub	x27, x22, x25
   2db6c:	add	x8, x27, #0x1
   2db70:	add	x9, x27, #0x2
   2db74:	cmp	x8, #0x0
   2db78:	csinc	x8, x9, x27, lt  // lt = tstop
   2db7c:	lsl	x8, x8, #4
   2db80:	sub	x0, x29, #0x48
   2db84:	mov	x1, x27
   2db88:	mov	x2, x23
   2db8c:	and	x26, x8, #0xffffffffffffffe0
   2db90:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   2db94:	add	x9, x26, x23
   2db98:	lsl	x8, x25, #3
   2db9c:	add	x26, x9, #0x20
   2dba0:	add	x0, x21, x8
   2dba4:	add	x1, x20, x8
   2dba8:	sub	x3, x29, #0x48
   2dbac:	mov	x2, x27
   2dbb0:	mov	x4, x26
   2dbb4:	bl	cfb0 <__gmpn_hgcd@plt>
   2dbb8:	cmp	x0, #0x1
   2dbbc:	b.ge	2db2c <__gmpn_gcd@@Base+0x140>  // b.tcont
   2dbc0:	sub	x5, x29, #0x10
   2dbc4:	mov	x0, x21
   2dbc8:	mov	x1, x20
   2dbcc:	mov	x2, x22
   2dbd0:	mov	x3, xzr
   2dbd4:	mov	x4, x24
   2dbd8:	mov	x6, x23
   2dbdc:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   2dbe0:	mov	x22, x0
   2dbe4:	cbnz	x0, 2db4c <__gmpn_gcd@@Base+0x160>
   2dbe8:	mov	w8, #0x12                  	// #18
   2dbec:	cbz	w8, 2db54 <__gmpn_gcd@@Base+0x168>
   2dbf0:	b	2dcec <__gmpn_gcd@@Base+0x300>
   2dbf4:	adrp	x24, 2d000 <__gmpn_perfect_power_p@@Base+0x424>
   2dbf8:	add	x24, x24, #0xdd4
   2dbfc:	b	2dc30 <__gmpn_gcd@@Base+0x244>
   2dc00:	sub	x0, x29, #0x48
   2dc04:	mov	x1, x23
   2dc08:	mov	x2, x21
   2dc0c:	mov	x3, x20
   2dc10:	mov	x4, x22
   2dc14:	bl	c660 <__gmpn_matrix22_mul1_inverse_vector@plt>
   2dc18:	mov	x22, x0
   2dc1c:	mov	x0, x21
   2dc20:	mov	x21, x23
   2dc24:	mov	x23, x0
   2dc28:	mov	w8, wzr
   2dc2c:	cbnz	wzr, 2dcec <__gmpn_gcd@@Base+0x300>
   2dc30:	cmp	x22, #0x3
   2dc34:	b.lt	2dd24 <__gmpn_gcd@@Base+0x338>  // b.tstop
   2dc38:	lsl	x8, x22, #3
   2dc3c:	sub	x9, x8, #0x8
   2dc40:	ldr	x0, [x21, x9]
   2dc44:	ldr	x2, [x20, x9]
   2dc48:	orr	x9, x2, x0
   2dc4c:	tbnz	x9, #63, 2dca4 <__gmpn_gcd@@Base+0x2b8>
   2dc50:	sub	x10, x8, #0x10
   2dc54:	sub	x8, x8, #0x18
   2dc58:	ldr	x12, [x21, x10]
   2dc5c:	ldr	x14, [x21, x8]
   2dc60:	ldr	x10, [x20, x10]
   2dc64:	ldr	x8, [x20, x8]
   2dc68:	clz	x9, x9
   2dc6c:	neg	w13, w9
   2dc70:	lsl	x11, x0, x9
   2dc74:	lsl	x15, x2, x9
   2dc78:	lsr	x16, x12, x13
   2dc7c:	lsl	x12, x12, x9
   2dc80:	lsr	x14, x14, x13
   2dc84:	lsl	x9, x10, x9
   2dc88:	lsr	x10, x10, x13
   2dc8c:	lsr	x8, x8, x13
   2dc90:	orr	x0, x16, x11
   2dc94:	orr	x1, x14, x12
   2dc98:	orr	x2, x10, x15
   2dc9c:	orr	x3, x8, x9
   2dca0:	b	2dcb0 <__gmpn_gcd@@Base+0x2c4>
   2dca4:	sub	x8, x8, #0x10
   2dca8:	ldr	x1, [x21, x8]
   2dcac:	ldr	x3, [x20, x8]
   2dcb0:	sub	x4, x29, #0x48
   2dcb4:	bl	c730 <__gmpn_hgcd2@plt>
   2dcb8:	cbnz	w0, 2dc00 <__gmpn_gcd@@Base+0x214>
   2dcbc:	sub	x5, x29, #0x10
   2dcc0:	mov	x0, x21
   2dcc4:	mov	x1, x20
   2dcc8:	mov	x2, x22
   2dccc:	mov	x3, xzr
   2dcd0:	mov	x4, x24
   2dcd4:	mov	x6, x23
   2dcd8:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   2dcdc:	mov	x22, x0
   2dce0:	cbnz	x0, 2dc28 <__gmpn_gcd@@Base+0x23c>
   2dce4:	mov	w8, #0x12                  	// #18
   2dce8:	cbz	w8, 2dc30 <__gmpn_gcd@@Base+0x244>
   2dcec:	cmp	w8, #0x12
   2dcf0:	b.ne	2dd04 <__gmpn_gcd@@Base+0x318>  // b.any
   2dcf4:	ldur	x0, [x29, #-24]
   2dcf8:	cbnz	x0, 2ddcc <__gmpn_gcd@@Base+0x3e0>
   2dcfc:	ldur	x0, [x29, #-8]
   2dd00:	b	2dd04 <__gmpn_gcd@@Base+0x318>
   2dd04:	mov	sp, x29
   2dd08:	ldp	x20, x19, [sp, #80]
   2dd0c:	ldp	x22, x21, [sp, #64]
   2dd10:	ldp	x24, x23, [sp, #48]
   2dd14:	ldp	x26, x25, [sp, #32]
   2dd18:	ldp	x28, x27, [sp, #16]
   2dd1c:	ldp	x29, x30, [sp], #96
   2dd20:	ret
   2dd24:	ldr	x8, [x21]
   2dd28:	tst	x8, #0x1
   2dd2c:	csel	x11, x21, x20, eq  // eq = none
   2dd30:	csel	x9, x20, x21, eq  // eq = none
   2dd34:	ldr	x8, [x9]
   2dd38:	ldr	x10, [x11]
   2dd3c:	cmp	x22, #0x1
   2dd40:	b.ne	2dd68 <__gmpn_gcd@@Base+0x37c>  // b.any
   2dd44:	rbit	x9, x10
   2dd48:	clz	x9, x9
   2dd4c:	lsr	x1, x10, x9
   2dd50:	mov	x0, x8
   2dd54:	bl	d520 <__gmpn_gcd_11@plt>
   2dd58:	str	x0, [x19]
   2dd5c:	mov	w8, #0x12                  	// #18
   2dd60:	mov	w9, #0x1                   	// #1
   2dd64:	b	2ddb8 <__gmpn_gcd@@Base+0x3cc>
   2dd68:	ldr	x11, [x11, #8]
   2dd6c:	cmp	x10, #0x0
   2dd70:	csel	x3, x11, x10, eq  // eq = none
   2dd74:	csel	x2, xzr, x11, eq  // eq = none
   2dd78:	tbnz	w3, #0, 2dd98 <__gmpn_gcd@@Base+0x3ac>
   2dd7c:	rbit	x10, x3
   2dd80:	clz	x10, x10
   2dd84:	neg	w11, w10
   2dd88:	lsr	x12, x3, x10
   2dd8c:	lsl	x11, x2, x11
   2dd90:	orr	x3, x11, x12
   2dd94:	lsr	x2, x2, x10
   2dd98:	ldr	x0, [x9, #8]
   2dd9c:	mov	x1, x8
   2dda0:	bl	c170 <__gmpn_gcd_22@plt>
   2dda4:	cmp	x1, #0x0
   2dda8:	mov	w9, #0x1                   	// #1
   2ddac:	mov	w8, wzr
   2ddb0:	cinc	x9, x9, ne  // ne = any
   2ddb4:	stp	x0, x1, [x19]
   2ddb8:	cmp	w8, #0x12
   2ddbc:	stur	x9, [x29, #-8]
   2ddc0:	b.eq	2dcf4 <__gmpn_gcd@@Base+0x308>  // b.none
   2ddc4:	cbz	w8, 2dcf4 <__gmpn_gcd@@Base+0x308>
   2ddc8:	b	2dd04 <__gmpn_gcd@@Base+0x318>
   2ddcc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2ddd0:	b	2dcfc <__gmpn_gcd@@Base+0x310>
   2ddd4:	stp	x29, x30, [sp, #-32]!
   2ddd8:	stp	x20, x19, [sp, #16]
   2dddc:	mov	x20, x0
   2dde0:	ldr	x0, [x0]
   2dde4:	mov	x29, sp
   2dde8:	mov	x19, x2
   2ddec:	bl	cc10 <__gmpn_copyi@plt>
   2ddf0:	str	x19, [x20, #8]
   2ddf4:	ldp	x20, x19, [sp, #16]
   2ddf8:	ldp	x29, x30, [sp], #32
   2ddfc:	ret

000000000002de00 <__gmpn_gcdext_1@@Base>:
   2de00:	mov	x8, xzr
   2de04:	mov	x10, xzr
   2de08:	mov	w9, #0x1                   	// #1
   2de0c:	cmp	x2, x3
   2de10:	mov	w11, #0x1                   	// #1
   2de14:	b.cc	2de2c <__gmpn_gcdext_1@@Base+0x2c>  // b.lo, b.ul, b.last
   2de18:	udiv	x12, x2, x3
   2de1c:	msub	x2, x12, x3, x2
   2de20:	cbz	x2, 2de44 <__gmpn_gcdext_1@@Base+0x44>
   2de24:	msub	x9, x12, x10, x9
   2de28:	msub	x8, x12, x11, x8
   2de2c:	udiv	x12, x3, x2
   2de30:	msub	x3, x12, x2, x3
   2de34:	cbz	x3, 2de50 <__gmpn_gcdext_1@@Base+0x50>
   2de38:	msub	x10, x12, x9, x10
   2de3c:	msub	x11, x12, x8, x11
   2de40:	b	2de18 <__gmpn_gcdext_1@@Base+0x18>
   2de44:	mov	x9, x10
   2de48:	mov	x8, x11
   2de4c:	mov	x2, x3
   2de50:	str	x9, [x0]
   2de54:	mov	x0, x2
   2de58:	str	x8, [x1]
   2de5c:	ret

000000000002de60 <__gmpn_gcdext@@Base>:
   2de60:	stp	x29, x30, [sp, #-96]!
   2de64:	stp	x28, x27, [sp, #16]
   2de68:	stp	x26, x25, [sp, #32]
   2de6c:	stp	x24, x23, [sp, #48]
   2de70:	stp	x22, x21, [sp, #64]
   2de74:	stp	x20, x19, [sp, #80]
   2de78:	mov	x29, sp
   2de7c:	sub	sp, sp, #0xc0
   2de80:	mov	w8, #0x3                   	// #3
   2de84:	sub	x9, x4, x6
   2de88:	bfi	x8, x6, #2, #62
   2de8c:	cmp	x9, x8
   2de90:	mov	x21, x6
   2de94:	mov	x24, x5
   2de98:	mov	x19, x4
   2de9c:	mov	x25, x3
   2dea0:	mov	x27, x2
   2dea4:	mov	x23, x0
   2dea8:	add	x20, x6, #0x1
   2deac:	csinc	x22, x8, x9, lt  // lt = tstop
   2deb0:	cmp	x6, #0xf2
   2deb4:	stur	xzr, [x29, #-80]
   2deb8:	stur	x1, [x29, #-152]
   2debc:	b.lt	2df3c <__gmpn_gcdext@@Base+0xdc>  // b.tstop
   2dec0:	mov	x8, #0x5555555555555555    	// #6148914691236517205
   2dec4:	cmp	x21, #0x0
   2dec8:	movk	x8, #0x5556
   2decc:	cinc	x9, x21, lt  // lt = tstop
   2ded0:	smulh	x8, x21, x8
   2ded4:	asr	x9, x9, #1
   2ded8:	add	x8, x8, x8, lsr #63
   2dedc:	cmp	x9, x8
   2dee0:	csel	x10, x9, x8, lt  // lt = tstop
   2dee4:	sub	x0, x21, x10
   2dee8:	csel	x28, x9, x8, gt
   2deec:	add	x8, x0, #0x1
   2def0:	add	x9, x0, #0x2
   2def4:	cmp	x8, #0x0
   2def8:	csinc	x8, x9, x0, lt  // lt = tstop
   2defc:	lsl	x8, x8, #1
   2df00:	and	x8, x8, #0xfffffffffffffffc
   2df04:	add	x26, x8, #0x4
   2df08:	bl	c720 <__gmpn_hgcd_itch@plt>
   2df0c:	add	x8, x28, x21
   2df10:	sub	x9, x8, #0x1
   2df14:	cmp	x0, x8
   2df18:	csel	x8, x9, x0, lt  // lt = tstop
   2df1c:	add	x8, x8, x26
   2df20:	cmp	x8, x22
   2df24:	csel	x8, x8, x22, gt
   2df28:	cmp	x8, #0x6a1
   2df2c:	mov	w9, #0x6a1                 	// #1697
   2df30:	csel	x8, x8, x9, gt
   2df34:	add	x22, x8, x20, lsl #1
   2df38:	b	2df3c <__gmpn_gcdext@@Base+0xdc>
   2df3c:	lsl	x1, x22, #3
   2df40:	mov	w8, #0x7f00                	// #32512
   2df44:	cmp	x1, x8
   2df48:	b.hi	2dfbc <__gmpn_gcdext@@Base+0x15c>  // b.pmore
   2df4c:	add	x9, x1, #0xf
   2df50:	mov	x8, sp
   2df54:	and	x9, x9, #0xfffffffffffffff0
   2df58:	sub	x22, x8, x9
   2df5c:	mov	sp, x22
   2df60:	cmp	x19, x21
   2df64:	b.le	2dfd0 <__gmpn_gcdext@@Base+0x170>
   2df68:	mov	x0, x22
   2df6c:	mov	x1, x25
   2df70:	mov	x2, xzr
   2df74:	mov	x3, x25
   2df78:	mov	x4, x19
   2df7c:	mov	x5, x24
   2df80:	mov	x6, x21
   2df84:	bl	c030 <__gmpn_tdiv_qr@plt>
   2df88:	mov	x0, x25
   2df8c:	mov	x1, x21
   2df90:	bl	c010 <__gmpn_zero_p@plt>
   2df94:	cbz	w0, 2dfd0 <__gmpn_gcdext@@Base+0x170>
   2df98:	mov	x0, x23
   2df9c:	mov	x1, x24
   2dfa0:	mov	x2, x21
   2dfa4:	bl	cc10 <__gmpn_copyi@plt>
   2dfa8:	str	xzr, [x27]
   2dfac:	ldur	x0, [x29, #-80]
   2dfb0:	cbnz	x0, 2e510 <__gmpn_gcdext@@Base+0x6b0>
   2dfb4:	stur	x21, [x29, #-144]
   2dfb8:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2dfbc:	sub	x0, x29, #0x50
   2dfc0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2dfc4:	mov	x22, x0
   2dfc8:	cmp	x19, x21
   2dfcc:	b.gt	2df68 <__gmpn_gcdext@@Base+0x108>
   2dfd0:	cmp	x21, #0xf1
   2dfd4:	b.le	2e0c4 <__gmpn_gcdext@@Base+0x264>
   2dfd8:	cbz	x20, 2dff0 <__gmpn_gcdext@@Base+0x190>
   2dfdc:	lsl	x8, x21, #4
   2dfe0:	add	x2, x8, #0x10
   2dfe4:	mov	x0, x22
   2dfe8:	mov	w1, wzr
   2dfec:	bl	c780 <memset@plt>
   2dff0:	ldur	x9, [x29, #-152]
   2dff4:	cmp	x21, #0x0
   2dff8:	lsl	x8, x20, #3
   2dffc:	stp	x27, x23, [x29, #-168]
   2e000:	stp	x9, x27, [x29, #-56]
   2e004:	cinc	x9, x21, lt  // lt = tstop
   2e008:	stur	x23, [x29, #-72]
   2e00c:	add	x23, x22, x8
   2e010:	asr	x19, x9, #1
   2e014:	add	x27, x23, x8
   2e018:	sub	x20, x21, x19
   2e01c:	sub	x0, x29, #0x80
   2e020:	mov	x1, x20
   2e024:	mov	x2, x27
   2e028:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   2e02c:	lsl	x8, x19, #3
   2e030:	add	x26, x27, x26, lsl #3
   2e034:	add	x0, x25, x8
   2e038:	add	x1, x24, x8
   2e03c:	sub	x3, x29, #0x80
   2e040:	mov	x2, x20
   2e044:	mov	x4, x26
   2e048:	bl	cfb0 <__gmpn_hgcd@plt>
   2e04c:	cmp	x0, #0x1
   2e050:	b.lt	2e0fc <__gmpn_gcdext@@Base+0x29c>  // b.tstop
   2e054:	add	x1, x0, x19
   2e058:	sub	x0, x29, #0x80
   2e05c:	mov	x2, x25
   2e060:	mov	x3, x24
   2e064:	mov	x4, x19
   2e068:	mov	x5, x26
   2e06c:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   2e070:	ldur	x1, [x29, #-96]
   2e074:	ldur	x2, [x29, #-120]
   2e078:	mov	x28, x0
   2e07c:	mov	x0, x22
   2e080:	bl	cc10 <__gmpn_copyi@plt>
   2e084:	ldur	x1, [x29, #-88]
   2e088:	ldur	x2, [x29, #-120]
   2e08c:	mov	x0, x23
   2e090:	bl	cc10 <__gmpn_copyi@plt>
   2e094:	ldur	x8, [x29, #-120]
   2e098:	sub	x9, x22, #0x8
   2e09c:	add	x10, x22, x21, lsl #3
   2e0a0:	lsl	x11, x8, #3
   2e0a4:	ldr	x12, [x9, x11]
   2e0a8:	ldr	x13, [x10, x11]
   2e0ac:	sub	x11, x8, #0x1
   2e0b0:	mov	x8, x11
   2e0b4:	orr	x12, x13, x12
   2e0b8:	cbz	x12, 2e0a0 <__gmpn_gcdext@@Base+0x240>
   2e0bc:	add	x19, x11, #0x1
   2e0c0:	b	2e140 <__gmpn_gcdext@@Base+0x2e0>
   2e0c4:	ldur	x1, [x29, #-152]
   2e0c8:	mov	x0, x23
   2e0cc:	mov	x2, x27
   2e0d0:	mov	x3, x25
   2e0d4:	mov	x4, x24
   2e0d8:	mov	x5, x21
   2e0dc:	mov	x6, x22
   2e0e0:	bl	d390 <__gmpn_gcdext_lehmer_n@plt>
   2e0e4:	ldur	x8, [x29, #-80]
   2e0e8:	stur	x0, [x29, #-144]
   2e0ec:	cbz	x8, 2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e0f0:	mov	x0, x8
   2e0f4:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2e0f8:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e0fc:	mov	w8, #0x1                   	// #1
   2e100:	add	x9, x27, x21, lsl #3
   2e104:	str	x8, [x23]
   2e108:	stp	x23, x9, [x29, #-24]
   2e10c:	stp	x8, x22, [x29, #-40]
   2e110:	adrp	x4, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2e114:	ldr	x4, [x4, #3968]
   2e118:	sub	x5, x29, #0x48
   2e11c:	mov	x0, x25
   2e120:	mov	x1, x24
   2e124:	mov	x2, x21
   2e128:	mov	x3, xzr
   2e12c:	mov	x6, x27
   2e130:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   2e134:	cbz	x0, 2e2ac <__gmpn_gcdext@@Base+0x44c>
   2e138:	ldur	x19, [x29, #-40]
   2e13c:	mov	x28, x0
   2e140:	mov	w8, #0x1                   	// #1
   2e144:	cbz	w8, 2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e148:	stur	x23, [x29, #-136]
   2e14c:	b	2e1ac <__gmpn_gcdext@@Base+0x34c>
   2e150:	add	x1, x0, x23
   2e154:	sub	x0, x29, #0x80
   2e158:	mov	x2, x25
   2e15c:	mov	x3, x24
   2e160:	mov	x4, x23
   2e164:	mov	x5, x26
   2e168:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   2e16c:	mov	x28, x0
   2e170:	mov	x0, x26
   2e174:	mov	x1, x22
   2e178:	mov	x2, x19
   2e17c:	bl	cc10 <__gmpn_copyi@plt>
   2e180:	ldur	x23, [x29, #-136]
   2e184:	add	x5, x26, x19, lsl #3
   2e188:	sub	x0, x29, #0x80
   2e18c:	mov	x1, x22
   2e190:	mov	x2, x26
   2e194:	mov	x3, x23
   2e198:	mov	x4, x19
   2e19c:	bl	2e5dc <__gmpn_gcdext@@Base+0x77c>
   2e1a0:	mov	x19, x0
   2e1a4:	mov	w8, #0x1                   	// #1
   2e1a8:	tbz	w8, #0, 2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e1ac:	cmp	x28, #0xf2
   2e1b0:	b.lt	2e26c <__gmpn_gcdext@@Base+0x40c>  // b.tstop
   2e1b4:	mov	x8, #0x5555555555555555    	// #6148914691236517205
   2e1b8:	movk	x8, #0x5556
   2e1bc:	smulh	x8, x28, x8
   2e1c0:	add	x23, x8, x8, lsr #63
   2e1c4:	sub	x20, x28, x23
   2e1c8:	sub	x0, x29, #0x80
   2e1cc:	mov	x1, x20
   2e1d0:	mov	x2, x27
   2e1d4:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   2e1d8:	lsl	x8, x23, #3
   2e1dc:	add	x0, x25, x8
   2e1e0:	add	x1, x24, x8
   2e1e4:	sub	x3, x29, #0x80
   2e1e8:	mov	x2, x20
   2e1ec:	mov	x4, x26
   2e1f0:	bl	cfb0 <__gmpn_hgcd@plt>
   2e1f4:	cmp	x0, #0x1
   2e1f8:	b.ge	2e150 <__gmpn_gcdext@@Base+0x2f0>  // b.tcont
   2e1fc:	ldur	x23, [x29, #-136]
   2e200:	add	x8, x27, x28, lsl #3
   2e204:	stp	x19, x22, [x29, #-40]
   2e208:	adrp	x4, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2e20c:	stp	x23, x8, [x29, #-24]
   2e210:	ldr	x4, [x4, #3968]
   2e214:	sub	x5, x29, #0x48
   2e218:	mov	x0, x25
   2e21c:	mov	x1, x24
   2e220:	mov	x2, x28
   2e224:	mov	x3, xzr
   2e228:	mov	x6, x27
   2e22c:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   2e230:	cbz	x0, 2e248 <__gmpn_gcdext@@Base+0x3e8>
   2e234:	ldur	x19, [x29, #-40]
   2e238:	mov	x28, x0
   2e23c:	mov	w8, #0x1                   	// #1
   2e240:	tbnz	w8, #0, 2e1ac <__gmpn_gcdext@@Base+0x34c>
   2e244:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e248:	ldur	x0, [x29, #-80]
   2e24c:	cbnz	x0, 2e264 <__gmpn_gcdext@@Base+0x404>
   2e250:	ldur	x8, [x29, #-64]
   2e254:	mov	x28, xzr
   2e258:	stur	x8, [x29, #-144]
   2e25c:	tbnz	wzr, #0, 2e1ac <__gmpn_gcdext@@Base+0x34c>
   2e260:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e264:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2e268:	b	2e250 <__gmpn_gcdext@@Base+0x3f0>
   2e26c:	mov	x0, x25
   2e270:	mov	x1, x24
   2e274:	mov	x2, x28
   2e278:	bl	c570 <__gmpn_cmp@plt>
   2e27c:	cbz	w0, 2e51c <__gmpn_gcdext@@Base+0x6bc>
   2e280:	cmp	x19, #0x1
   2e284:	b.ne	2e2cc <__gmpn_gcdext@@Base+0x46c>  // b.any
   2e288:	ldr	x8, [x22]
   2e28c:	cbnz	x8, 2e2cc <__gmpn_gcdext@@Base+0x46c>
   2e290:	ldp	x0, x1, [x29, #-160]
   2e294:	ldur	x2, [x29, #-168]
   2e298:	mov	x3, x25
   2e29c:	mov	x4, x24
   2e2a0:	mov	x5, x28
   2e2a4:	mov	x6, x27
   2e2a8:	b	2e0e0 <__gmpn_gcdext@@Base+0x280>
   2e2ac:	ldur	x0, [x29, #-80]
   2e2b0:	cbnz	x0, 2e5c8 <__gmpn_gcdext@@Base+0x768>
   2e2b4:	ldur	x9, [x29, #-64]
   2e2b8:	mov	w8, wzr
   2e2bc:	mov	x28, xzr
   2e2c0:	stur	x9, [x29, #-144]
   2e2c4:	cbz	w8, 2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e2c8:	b	2e148 <__gmpn_gcdext@@Base+0x2e8>
   2e2cc:	lsl	x20, x28, #3
   2e2d0:	add	x26, x27, x20
   2e2d4:	mov	x0, x26
   2e2d8:	mov	x1, x25
   2e2dc:	mov	x2, x28
   2e2e0:	bl	cc10 <__gmpn_copyi@plt>
   2e2e4:	add	x20, x26, x20
   2e2e8:	mov	x0, x20
   2e2ec:	mov	x1, x24
   2e2f0:	mov	x2, x28
   2e2f4:	bl	cc10 <__gmpn_copyi@plt>
   2e2f8:	ldur	x0, [x29, #-160]
   2e2fc:	add	x6, x26, x28, lsl #4
   2e300:	sub	x2, x29, #0x80
   2e304:	mov	x1, x27
   2e308:	mov	x3, x26
   2e30c:	mov	x4, x20
   2e310:	mov	x5, x28
   2e314:	stp	x20, x26, [x29, #-184]
   2e318:	bl	d390 <__gmpn_gcdext_lehmer_n@plt>
   2e31c:	sub	x8, x22, #0x8
   2e320:	mov	x9, x19
   2e324:	stur	x0, [x29, #-144]
   2e328:	mov	x26, x9
   2e32c:	subs	x9, x9, #0x1
   2e330:	b.lt	2e33c <__gmpn_gcdext@@Base+0x4dc>  // b.tstop
   2e334:	ldr	x10, [x8, x26, lsl #3]
   2e338:	cbz	x10, 2e328 <__gmpn_gcdext@@Base+0x4c8>
   2e33c:	ldur	x7, [x29, #-128]
   2e340:	cbz	x7, 2e38c <__gmpn_gcdext@@Base+0x52c>
   2e344:	ldur	x8, [x29, #-184]
   2e348:	add	x8, x8, #0x8
   2e34c:	str	x8, [sp, #-16]!
   2e350:	ldur	x0, [x29, #-176]
   2e354:	ldur	x4, [x29, #-160]
   2e358:	ldur	x5, [x29, #-144]
   2e35c:	mov	x1, x25
   2e360:	mov	x2, x24
   2e364:	mov	x3, x28
   2e368:	mov	x6, x27
   2e36c:	bl	2e760 <__gmpn_gcdext@@Base+0x900>
   2e370:	add	sp, sp, #0x10
   2e374:	ldur	x8, [x29, #-128]
   2e378:	mov	x24, x0
   2e37c:	cmp	x8, #0x0
   2e380:	b.le	2e3ac <__gmpn_gcdext@@Base+0x54c>
   2e384:	mov	w28, wzr
   2e388:	b	2e3b8 <__gmpn_gcdext@@Base+0x558>
   2e38c:	ldur	x0, [x29, #-152]
   2e390:	mov	x1, x22
   2e394:	mov	x2, x26
   2e398:	bl	cc10 <__gmpn_copyi@plt>
   2e39c:	ldur	x9, [x29, #-168]
   2e3a0:	neg	x8, x26
   2e3a4:	str	x8, [x9]
   2e3a8:	b	2e4e4 <__gmpn_gcdext@@Base+0x684>
   2e3ac:	neg	x8, x8
   2e3b0:	mov	w28, #0x1                   	// #1
   2e3b4:	stur	x8, [x29, #-128]
   2e3b8:	ldur	x25, [x29, #-152]
   2e3bc:	add	x8, x22, x21, lsl #3
   2e3c0:	mov	x21, x19
   2e3c4:	subs	x19, x19, #0x1
   2e3c8:	b.lt	2e3d4 <__gmpn_gcdext@@Base+0x574>  // b.tstop
   2e3cc:	ldr	x9, [x8, x21, lsl #3]
   2e3d0:	cbz	x9, 2e3c0 <__gmpn_gcdext@@Base+0x560>
   2e3d4:	ldur	x4, [x29, #-128]
   2e3d8:	mov	x0, x25
   2e3dc:	cmp	x4, x21
   2e3e0:	b.le	2e3f8 <__gmpn_gcdext@@Base+0x598>
   2e3e4:	mov	x1, x27
   2e3e8:	mov	x2, x4
   2e3ec:	mov	x3, x23
   2e3f0:	mov	x4, x21
   2e3f4:	b	2e404 <__gmpn_gcdext@@Base+0x5a4>
   2e3f8:	mov	x1, x23
   2e3fc:	mov	x2, x21
   2e400:	mov	x3, x27
   2e404:	bl	cea0 <__gmpn_mul@plt>
   2e408:	ldur	x8, [x29, #-128]
   2e40c:	ldur	x23, [x29, #-168]
   2e410:	add	x9, x8, x21
   2e414:	add	x9, x25, x9, lsl #3
   2e418:	ldur	x9, [x9, #-8]
   2e41c:	cmp	x9, #0x0
   2e420:	cset	w9, eq  // eq = none
   2e424:	sub	x8, x8, x9
   2e428:	cmp	x24, #0x1
   2e42c:	add	x19, x8, x21
   2e430:	b.lt	2e4d8 <__gmpn_gcdext@@Base+0x678>  // b.tstop
   2e434:	cmp	x24, x26
   2e438:	b.le	2e458 <__gmpn_gcdext@@Base+0x5f8>
   2e43c:	ldur	x27, [x29, #-136]
   2e440:	ldur	x1, [x29, #-176]
   2e444:	mov	x2, x24
   2e448:	mov	x3, x22
   2e44c:	mov	x0, x27
   2e450:	mov	x4, x26
   2e454:	b	2e470 <__gmpn_gcdext@@Base+0x610>
   2e458:	ldur	x27, [x29, #-136]
   2e45c:	ldur	x3, [x29, #-176]
   2e460:	mov	x1, x22
   2e464:	mov	x2, x26
   2e468:	mov	x0, x27
   2e46c:	mov	x4, x24
   2e470:	bl	cea0 <__gmpn_mul@plt>
   2e474:	add	x8, x24, x26
   2e478:	add	x8, x27, x8, lsl #3
   2e47c:	ldur	x8, [x8, #-8]
   2e480:	mov	x0, x25
   2e484:	cmp	x8, #0x0
   2e488:	cset	w8, eq  // eq = none
   2e48c:	sub	x8, x24, x8
   2e490:	add	x21, x8, x26
   2e494:	cmp	x21, x19
   2e498:	b.le	2e4b8 <__gmpn_gcdext@@Base+0x658>
   2e49c:	mov	x1, x27
   2e4a0:	mov	x2, x21
   2e4a4:	mov	x3, x25
   2e4a8:	mov	x4, x19
   2e4ac:	bl	c970 <__gmpn_add@plt>
   2e4b0:	mov	x19, x21
   2e4b4:	b	2e4cc <__gmpn_gcdext@@Base+0x66c>
   2e4b8:	mov	x1, x25
   2e4bc:	mov	x2, x19
   2e4c0:	mov	x3, x27
   2e4c4:	mov	x4, x21
   2e4c8:	bl	c970 <__gmpn_add@plt>
   2e4cc:	cmp	x0, #0x0
   2e4d0:	str	x0, [x25, x19, lsl #3]
   2e4d4:	cinc	x19, x19, ne  // ne = any
   2e4d8:	cmp	w28, #0x0
   2e4dc:	cneg	x8, x19, ne  // ne = any
   2e4e0:	str	x8, [x23]
   2e4e4:	ldur	x0, [x29, #-80]
   2e4e8:	cbnz	x0, 2e0f4 <__gmpn_gcdext@@Base+0x294>
   2e4ec:	ldur	x0, [x29, #-144]
   2e4f0:	mov	sp, x29
   2e4f4:	ldp	x20, x19, [sp, #80]
   2e4f8:	ldp	x22, x21, [sp, #64]
   2e4fc:	ldp	x24, x23, [sp, #48]
   2e500:	ldp	x26, x25, [sp, #32]
   2e504:	ldp	x28, x27, [sp, #16]
   2e508:	ldp	x29, x30, [sp], #96
   2e50c:	ret
   2e510:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2e514:	stur	x21, [x29, #-144]
   2e518:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e51c:	ldur	x0, [x29, #-160]
   2e520:	mov	x1, x25
   2e524:	mov	x2, x28
   2e528:	bl	cc10 <__gmpn_copyi@plt>
   2e52c:	sub	x8, x22, #0x8
   2e530:	add	x9, x22, x21, lsl #3
   2e534:	mov	x10, x19
   2e538:	subs	x11, x10, #0x1
   2e53c:	b.lt	2e55c <__gmpn_gcdext@@Base+0x6fc>  // b.tstop
   2e540:	lsl	x10, x10, #3
   2e544:	ldr	x12, [x8, x10]
   2e548:	ldr	x10, [x9, x10]
   2e54c:	cmp	x12, x10
   2e550:	mov	x10, x11
   2e554:	b.eq	2e538 <__gmpn_gcdext@@Base+0x6d8>  // b.none
   2e558:	b.ls	2e588 <__gmpn_gcdext@@Base+0x728>  // b.plast
   2e55c:	add	x8, x22, x21, lsl #3
   2e560:	ldr	x10, [x8, x19, lsl #3]
   2e564:	sub	x9, x19, #0x1
   2e568:	mov	x19, x9
   2e56c:	cbz	x10, 2e560 <__gmpn_gcdext@@Base+0x700>
   2e570:	ldur	x0, [x29, #-152]
   2e574:	ldur	x1, [x29, #-136]
   2e578:	add	x19, x9, #0x1
   2e57c:	mov	x2, x19
   2e580:	bl	cc10 <__gmpn_copyi@plt>
   2e584:	b	2e5b0 <__gmpn_gcdext@@Base+0x750>
   2e588:	mov	x20, x19
   2e58c:	subs	x19, x19, #0x1
   2e590:	b.lt	2e59c <__gmpn_gcdext@@Base+0x73c>  // b.tstop
   2e594:	ldr	x9, [x8, x20, lsl #3]
   2e598:	cbz	x9, 2e588 <__gmpn_gcdext@@Base+0x728>
   2e59c:	ldur	x0, [x29, #-152]
   2e5a0:	mov	x1, x22
   2e5a4:	mov	x2, x20
   2e5a8:	bl	cc10 <__gmpn_copyi@plt>
   2e5ac:	neg	x19, x20
   2e5b0:	ldur	x8, [x29, #-168]
   2e5b4:	str	x19, [x8]
   2e5b8:	ldur	x0, [x29, #-80]
   2e5bc:	cbnz	x0, 2e5d0 <__gmpn_gcdext@@Base+0x770>
   2e5c0:	stur	x28, [x29, #-144]
   2e5c4:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e5c8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2e5cc:	b	2e2b4 <__gmpn_gcdext@@Base+0x454>
   2e5d0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2e5d4:	stur	x28, [x29, #-144]
   2e5d8:	b	2e4ec <__gmpn_gcdext@@Base+0x68c>
   2e5dc:	stp	x29, x30, [sp, #-80]!
   2e5e0:	str	x25, [sp, #16]
   2e5e4:	stp	x24, x23, [sp, #32]
   2e5e8:	stp	x22, x21, [sp, #48]
   2e5ec:	stp	x20, x19, [sp, #64]
   2e5f0:	mov	x21, x4
   2e5f4:	mov	x19, x3
   2e5f8:	ldp	x4, x3, [x0, #8]
   2e5fc:	mov	x23, x5
   2e600:	mov	x24, x2
   2e604:	mov	x22, x0
   2e608:	cmp	x4, x21
   2e60c:	mov	x20, x1
   2e610:	mov	x0, x5
   2e614:	mov	x29, sp
   2e618:	b.ge	2e640 <__gmpn_gcdext@@Base+0x7e0>  // b.tcont
   2e61c:	mov	x1, x24
   2e620:	mov	x2, x21
   2e624:	bl	cea0 <__gmpn_mul@plt>
   2e628:	ldr	x3, [x22, #32]
   2e62c:	ldr	x4, [x22, #8]
   2e630:	mov	x0, x20
   2e634:	mov	x1, x19
   2e638:	mov	x2, x21
   2e63c:	b	2e668 <__gmpn_gcdext@@Base+0x808>
   2e640:	mov	x1, x3
   2e644:	mov	x2, x4
   2e648:	mov	x3, x24
   2e64c:	mov	x4, x21
   2e650:	bl	cea0 <__gmpn_mul@plt>
   2e654:	ldr	x1, [x22, #32]
   2e658:	ldr	x2, [x22, #8]
   2e65c:	mov	x0, x20
   2e660:	mov	x3, x19
   2e664:	mov	x4, x21
   2e668:	bl	cea0 <__gmpn_mul@plt>
   2e66c:	ldr	x8, [x22, #8]
   2e670:	mov	x0, x20
   2e674:	mov	x1, x20
   2e678:	mov	x2, x23
   2e67c:	add	x3, x8, x21
   2e680:	bl	cc30 <__gmpn_add_n@plt>
   2e684:	ldr	x4, [x22, #8]
   2e688:	ldr	x3, [x22, #40]
   2e68c:	mov	x25, x0
   2e690:	mov	x0, x23
   2e694:	cmp	x4, x21
   2e698:	b.ge	2e6c0 <__gmpn_gcdext@@Base+0x860>  // b.tcont
   2e69c:	mov	x1, x19
   2e6a0:	mov	x2, x21
   2e6a4:	bl	cea0 <__gmpn_mul@plt>
   2e6a8:	ldr	x3, [x22, #24]
   2e6ac:	ldr	x4, [x22, #8]
   2e6b0:	mov	x0, x19
   2e6b4:	mov	x1, x24
   2e6b8:	mov	x2, x21
   2e6bc:	b	2e6e8 <__gmpn_gcdext@@Base+0x888>
   2e6c0:	mov	x1, x3
   2e6c4:	mov	x2, x4
   2e6c8:	mov	x3, x19
   2e6cc:	mov	x4, x21
   2e6d0:	bl	cea0 <__gmpn_mul@plt>
   2e6d4:	ldr	x1, [x22, #24]
   2e6d8:	ldr	x2, [x22, #8]
   2e6dc:	mov	x0, x19
   2e6e0:	mov	x3, x24
   2e6e4:	mov	x4, x21
   2e6e8:	bl	cea0 <__gmpn_mul@plt>
   2e6ec:	ldr	x8, [x22, #8]
   2e6f0:	mov	x0, x19
   2e6f4:	mov	x1, x19
   2e6f8:	mov	x2, x23
   2e6fc:	add	x3, x8, x21
   2e700:	bl	cc30 <__gmpn_add_n@plt>
   2e704:	ldr	x8, [x22, #8]
   2e708:	orr	x9, x0, x25
   2e70c:	add	x8, x8, x21
   2e710:	cbz	x9, 2e728 <__gmpn_gcdext@@Base+0x8c8>
   2e714:	lsl	x9, x8, #3
   2e718:	str	x25, [x20, x9]
   2e71c:	str	x0, [x19, x9]
   2e720:	add	x0, x8, #0x1
   2e724:	b	2e748 <__gmpn_gcdext@@Base+0x8e8>
   2e728:	sub	x8, x8, #0x1
   2e72c:	lsl	x9, x8, #3
   2e730:	ldr	x10, [x20, x9]
   2e734:	ldr	x9, [x19, x9]
   2e738:	sub	x8, x8, #0x1
   2e73c:	orr	x9, x9, x10
   2e740:	cbz	x9, 2e72c <__gmpn_gcdext@@Base+0x8cc>
   2e744:	add	x0, x8, #0x2
   2e748:	ldp	x20, x19, [sp, #64]
   2e74c:	ldp	x22, x21, [sp, #48]
   2e750:	ldp	x24, x23, [sp, #32]
   2e754:	ldr	x25, [sp, #16]
   2e758:	ldp	x29, x30, [sp], #80
   2e75c:	ret
   2e760:	stp	x29, x30, [sp, #-96]!
   2e764:	mov	x29, sp
   2e768:	stp	x22, x21, [sp, #64]
   2e76c:	ldr	x21, [x29, #96]
   2e770:	mov	x22, x3
   2e774:	cmp	x7, #0x0
   2e778:	str	x27, [sp, #16]
   2e77c:	stp	x26, x25, [sp, #32]
   2e780:	stp	x24, x23, [sp, #48]
   2e784:	stp	x20, x19, [sp, #80]
   2e788:	mov	x27, x7
   2e78c:	mov	x23, x5
   2e790:	mov	x24, x4
   2e794:	mov	x20, x2
   2e798:	mov	x3, x1
   2e79c:	mov	x19, x0
   2e7a0:	cneg	x25, x7, mi  // mi = first
   2e7a4:	sub	x8, x1, #0x8
   2e7a8:	mov	x9, x22
   2e7ac:	mov	x26, x9
   2e7b0:	subs	x9, x9, #0x1
   2e7b4:	b.lt	2e7c0 <__gmpn_gcdext@@Base+0x960>  // b.tstop
   2e7b8:	ldr	x10, [x8, x26, lsl #3]
   2e7bc:	cbz	x10, 2e7ac <__gmpn_gcdext@@Base+0x94c>
   2e7c0:	mov	x0, x21
   2e7c4:	cmp	x26, x25
   2e7c8:	b.ge	2e7dc <__gmpn_gcdext@@Base+0x97c>  // b.tcont
   2e7cc:	mov	x1, x6
   2e7d0:	mov	x2, x25
   2e7d4:	mov	x4, x26
   2e7d8:	b	2e7ec <__gmpn_gcdext@@Base+0x98c>
   2e7dc:	mov	x1, x3
   2e7e0:	mov	x2, x26
   2e7e4:	mov	x3, x6
   2e7e8:	mov	x4, x25
   2e7ec:	bl	cea0 <__gmpn_mul@plt>
   2e7f0:	cmp	x27, #0x1
   2e7f4:	add	x27, x25, x26
   2e7f8:	mov	x0, x21
   2e7fc:	mov	x1, x21
   2e800:	b.lt	2e838 <__gmpn_gcdext@@Base+0x9d8>  // b.tstop
   2e804:	mov	x2, x27
   2e808:	mov	x3, x24
   2e80c:	mov	x4, x23
   2e810:	bl	d340 <__gmpn_sub@plt>
   2e814:	sub	x8, x21, #0x8
   2e818:	mov	x23, x27
   2e81c:	subs	x27, x27, #0x1
   2e820:	b.lt	2e82c <__gmpn_gcdext@@Base+0x9cc>  // b.tstop
   2e824:	ldr	x9, [x8, x23, lsl #3]
   2e828:	cbz	x9, 2e818 <__gmpn_gcdext@@Base+0x9b8>
   2e82c:	cbnz	x23, 2e864 <__gmpn_gcdext@@Base+0xa04>
   2e830:	mov	x0, xzr
   2e834:	b	2e8bc <__gmpn_gcdext@@Base+0xa5c>
   2e838:	mov	x2, x27
   2e83c:	mov	x3, x24
   2e840:	mov	x4, x23
   2e844:	bl	c970 <__gmpn_add@plt>
   2e848:	add	x8, x21, x25, lsl #3
   2e84c:	add	x8, x8, x26, lsl #3
   2e850:	ldur	x8, [x8, #-8]
   2e854:	cmp	x8, #0x0
   2e858:	cset	w8, eq  // eq = none
   2e85c:	sub	x8, x25, x8
   2e860:	add	x23, x8, x26
   2e864:	lsl	x9, x23, #3
   2e868:	sub	x8, x22, #0x1
   2e86c:	sub	x9, x9, x22, lsl #3
   2e870:	add	x4, x8, #0x1
   2e874:	mov	x22, x8
   2e878:	cmp	x4, #0x1
   2e87c:	mov	x24, x9
   2e880:	b.lt	2e894 <__gmpn_gcdext@@Base+0xa34>  // b.tstop
   2e884:	ldr	x10, [x20, x22, lsl #3]
   2e888:	sub	x8, x22, #0x1
   2e88c:	add	x9, x24, #0x8
   2e890:	cbz	x10, 2e870 <__gmpn_gcdext@@Base+0xa10>
   2e894:	mov	x0, x19
   2e898:	mov	x1, x21
   2e89c:	mov	x2, x23
   2e8a0:	mov	x3, x20
   2e8a4:	bl	c5a0 <__gmpn_divexact@plt>
   2e8a8:	ldr	x8, [x19, x24]
   2e8ac:	cmp	x8, #0x0
   2e8b0:	cset	w8, eq  // eq = none
   2e8b4:	sub	x8, x23, x8
   2e8b8:	sub	x0, x8, x22
   2e8bc:	ldp	x20, x19, [sp, #80]
   2e8c0:	ldp	x22, x21, [sp, #64]
   2e8c4:	ldp	x24, x23, [sp, #48]
   2e8c8:	ldp	x26, x25, [sp, #32]
   2e8cc:	ldr	x27, [sp, #16]
   2e8d0:	ldp	x29, x30, [sp], #96
   2e8d4:	ret

000000000002e8d8 <__gmpn_gcd_subdiv_step@@Base>:
   2e8d8:	sub	sp, sp, #0x70
   2e8dc:	add	x8, x0, x2, lsl #3
   2e8e0:	stp	x26, x25, [sp, #48]
   2e8e4:	stp	x24, x23, [sp, #64]
   2e8e8:	stp	x22, x21, [sp, #80]
   2e8ec:	stp	x20, x19, [sp, #96]
   2e8f0:	mov	x21, x6
   2e8f4:	mov	x22, x5
   2e8f8:	mov	x20, x4
   2e8fc:	mov	x25, x3
   2e900:	mov	x23, x1
   2e904:	mov	x24, x0
   2e908:	mov	x11, xzr
   2e90c:	sub	x10, x8, #0x8
   2e910:	stp	x29, x30, [sp, #16]
   2e914:	stp	x28, x27, [sp, #32]
   2e918:	add	x29, sp, #0x10
   2e91c:	add	x19, x2, x11
   2e920:	mov	x9, x11
   2e924:	subs	x8, x19, #0x1
   2e928:	b.lt	2e94c <__gmpn_gcd_subdiv_step@@Base+0x74>  // b.tstop
   2e92c:	ldr	x12, [x10, x9, lsl #3]
   2e930:	sub	x11, x9, #0x1
   2e934:	cbz	x12, 2e91c <__gmpn_gcd_subdiv_step@@Base+0x44>
   2e938:	b	2e94c <__gmpn_gcd_subdiv_step@@Base+0x74>
   2e93c:	add	x9, x23, x28, lsl #3
   2e940:	ldur	x11, [x9, #-8]
   2e944:	add	x9, x10, #0x1
   2e948:	cbnz	x11, 2e95c <__gmpn_gcd_subdiv_step@@Base+0x84>
   2e94c:	mov	x28, x2
   2e950:	subs	x2, x2, #0x1
   2e954:	mov	x10, x9
   2e958:	b.ge	2e93c <__gmpn_gcd_subdiv_step@@Base+0x64>  // b.tcont
   2e95c:	cbz	x10, 2e99c <__gmpn_gcd_subdiv_step@@Base+0xc4>
   2e960:	cmp	x19, x28
   2e964:	csel	x8, x19, x28, gt
   2e968:	csel	x9, x24, x23, gt
   2e96c:	cset	w26, gt
   2e970:	csel	x19, x28, x19, gt
   2e974:	csel	x24, x23, x24, gt
   2e978:	mov	x28, x8
   2e97c:	mov	x23, x9
   2e980:	b	2e9d8 <__gmpn_gcd_subdiv_step@@Base+0x100>
   2e984:	lsl	x9, x8, #3
   2e988:	ldr	x10, [x24, x9]
   2e98c:	ldr	x9, [x23, x9]
   2e990:	sub	x8, x8, #0x1
   2e994:	cmp	x10, x9
   2e998:	b.ne	2e9b0 <__gmpn_gcd_subdiv_step@@Base+0xd8>  // b.any
   2e99c:	add	x9, x8, #0x1
   2e9a0:	cmp	x9, #0x1
   2e9a4:	b.ge	2e984 <__gmpn_gcd_subdiv_step@@Base+0xac>  // b.tcont
   2e9a8:	mov	w8, wzr
   2e9ac:	b	2e9b8 <__gmpn_gcd_subdiv_step@@Base+0xe0>
   2e9b0:	mov	w8, #0x1                   	// #1
   2e9b4:	cneg	w8, w8, ls  // ls = plast
   2e9b8:	cbz	w8, 2eba0 <__gmpn_gcd_subdiv_step@@Base+0x2c8>
   2e9bc:	cmp	w8, #0x0
   2e9c0:	csel	x9, x24, x23, gt
   2e9c4:	cset	w26, gt
   2e9c8:	csel	x24, x23, x24, gt
   2e9cc:	mov	w8, #0x1                   	// #1
   2e9d0:	mov	x23, x9
   2e9d4:	tbz	w8, #0, 2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2e9d8:	cmp	x19, x25
   2e9dc:	b.le	2ea58 <__gmpn_gcd_subdiv_step@@Base+0x180>
   2e9e0:	mov	x0, x23
   2e9e4:	mov	x1, x23
   2e9e8:	mov	x2, x28
   2e9ec:	mov	x3, x24
   2e9f0:	mov	x4, x19
   2e9f4:	bl	d340 <__gmpn_sub@plt>
   2e9f8:	mov	x27, x28
   2e9fc:	subs	x28, x28, #0x1
   2ea00:	b.lt	2ea10 <__gmpn_gcd_subdiv_step@@Base+0x138>  // b.tstop
   2ea04:	add	x8, x23, x27, lsl #3
   2ea08:	ldur	x8, [x8, #-8]
   2ea0c:	cbz	x8, 2e9f8 <__gmpn_gcd_subdiv_step@@Base+0x120>
   2ea10:	cmp	x27, x25
   2ea14:	b.le	2ea7c <__gmpn_gcd_subdiv_step@@Base+0x1a4>
   2ea18:	cmp	x19, x27
   2ea1c:	b.ne	2eaa0 <__gmpn_gcd_subdiv_step@@Base+0x1c8>  // b.any
   2ea20:	sub	x8, x19, #0x1
   2ea24:	str	x21, [sp, #8]
   2ea28:	add	x9, x8, #0x1
   2ea2c:	cmp	x9, #0x1
   2ea30:	b.lt	2eae8 <__gmpn_gcd_subdiv_step@@Base+0x210>  // b.tstop
   2ea34:	lsl	x9, x8, #3
   2ea38:	ldr	x10, [x24, x9]
   2ea3c:	ldr	x9, [x23, x9]
   2ea40:	sub	x8, x8, #0x1
   2ea44:	cmp	x10, x9
   2ea48:	b.eq	2ea28 <__gmpn_gcd_subdiv_step@@Base+0x150>  // b.none
   2ea4c:	mov	w8, #0x1                   	// #1
   2ea50:	cneg	w21, w8, ls  // ls = plast
   2ea54:	b	2eaec <__gmpn_gcd_subdiv_step@@Base+0x214>
   2ea58:	cbnz	x25, 2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2ea5c:	eor	w5, w26, #0x1
   2ea60:	mov	x0, x22
   2ea64:	mov	x1, x23
   2ea68:	mov	x2, x28
   2ea6c:	mov	x3, xzr
   2ea70:	mov	x4, xzr
   2ea74:	blr	x20
   2ea78:	b	2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2ea7c:	mov	x0, x23
   2ea80:	mov	x1, x24
   2ea84:	mov	x2, x19
   2ea88:	mov	x3, x23
   2ea8c:	mov	x4, x27
   2ea90:	bl	c970 <__gmpn_add@plt>
   2ea94:	cbz	x0, 2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2ea98:	str	x0, [x23, x19, lsl #3]
   2ea9c:	b	2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2eaa0:	adrp	x3, 50000 <__gmpn_bases@@Base+0x2f98>
   2eaa4:	add	x3, x3, #0xb8
   2eaa8:	mov	w4, #0x1                   	// #1
   2eaac:	mov	x0, x22
   2eab0:	mov	x1, xzr
   2eab4:	mov	x2, xzr
   2eab8:	mov	w5, w26
   2eabc:	blr	x20
   2eac0:	cmp	x19, x27
   2eac4:	cset	w8, gt
   2eac8:	csel	x9, x19, x27, gt
   2eacc:	csel	x10, x24, x23, gt
   2ead0:	csel	x19, x27, x19, gt
   2ead4:	eor	w26, w26, w8
   2ead8:	csel	x24, x23, x24, gt
   2eadc:	mov	x27, x9
   2eae0:	mov	x23, x10
   2eae4:	b	2eb34 <__gmpn_gcd_subdiv_step@@Base+0x25c>
   2eae8:	mov	w21, wzr
   2eaec:	cbz	w21, 2ec04 <__gmpn_gcd_subdiv_step@@Base+0x32c>
   2eaf0:	adrp	x3, 50000 <__gmpn_bases@@Base+0x2f98>
   2eaf4:	add	x3, x3, #0xb8
   2eaf8:	mov	w4, #0x1                   	// #1
   2eafc:	mov	x0, x22
   2eb00:	mov	x1, xzr
   2eb04:	mov	x2, xzr
   2eb08:	mov	w5, w26
   2eb0c:	mov	w28, #0x1                   	// #1
   2eb10:	blr	x20
   2eb14:	cmp	w21, #0x0
   2eb18:	cset	w8, gt
   2eb1c:	csel	x9, x24, x23, gt
   2eb20:	eor	w26, w26, w8
   2eb24:	csel	x24, x23, x24, gt
   2eb28:	mov	x23, x9
   2eb2c:	ldr	x21, [sp, #8]
   2eb30:	tbz	w28, #0, 2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2eb34:	mov	x0, x21
   2eb38:	mov	x1, x23
   2eb3c:	mov	x2, xzr
   2eb40:	mov	x3, x23
   2eb44:	mov	x4, x27
   2eb48:	mov	x5, x24
   2eb4c:	mov	x6, x19
   2eb50:	bl	c030 <__gmpn_tdiv_qr@plt>
   2eb54:	sub	x8, x27, x19
   2eb58:	add	x27, x8, #0x1
   2eb5c:	mov	x8, x19
   2eb60:	mov	x4, x8
   2eb64:	subs	x8, x8, #0x1
   2eb68:	b.lt	2eb78 <__gmpn_gcd_subdiv_step@@Base+0x2a0>  // b.tstop
   2eb6c:	add	x9, x23, x4, lsl #3
   2eb70:	ldur	x9, [x9, #-8]
   2eb74:	cbz	x9, 2eb60 <__gmpn_gcd_subdiv_step@@Base+0x288>
   2eb78:	cmp	x4, x25
   2eb7c:	b.le	2ebd0 <__gmpn_gcd_subdiv_step@@Base+0x2f8>
   2eb80:	mov	x0, x22
   2eb84:	mov	x1, xzr
   2eb88:	mov	x2, xzr
   2eb8c:	mov	x3, x21
   2eb90:	mov	x4, x27
   2eb94:	mov	w5, w26
   2eb98:	blr	x20
   2eb9c:	b	2ec98 <__gmpn_gcd_subdiv_step@@Base+0x3c0>
   2eba0:	cbnz	x25, 2ebc0 <__gmpn_gcd_subdiv_step@@Base+0x2e8>
   2eba4:	mov	w5, #0xffffffff            	// #-1
   2eba8:	mov	x0, x22
   2ebac:	mov	x1, x24
   2ebb0:	mov	x2, x19
   2ebb4:	mov	x3, xzr
   2ebb8:	mov	x4, xzr
   2ebbc:	blr	x20
   2ebc0:	mov	w8, wzr
   2ebc4:	mov	w26, wzr
   2ebc8:	tbnz	w8, #0, 2e9d8 <__gmpn_gcd_subdiv_step@@Base+0x100>
   2ebcc:	b	2ec94 <__gmpn_gcd_subdiv_step@@Base+0x3bc>
   2ebd0:	cbz	x25, 2ec28 <__gmpn_gcd_subdiv_step@@Base+0x350>
   2ebd4:	cmp	x4, #0x1
   2ebd8:	b.lt	2ec44 <__gmpn_gcd_subdiv_step@@Base+0x36c>  // b.tstop
   2ebdc:	mov	x0, x23
   2ebe0:	mov	x1, x24
   2ebe4:	mov	x2, x19
   2ebe8:	mov	x3, x23
   2ebec:	bl	c970 <__gmpn_add@plt>
   2ebf0:	cbz	x0, 2ec54 <__gmpn_gcd_subdiv_step@@Base+0x37c>
   2ebf4:	add	x8, x19, #0x1
   2ebf8:	str	x0, [x23, x19, lsl #3]
   2ebfc:	mov	x19, x8
   2ec00:	b	2ec54 <__gmpn_gcd_subdiv_step@@Base+0x37c>
   2ec04:	cmp	x25, #0x1
   2ec08:	b.lt	2ec6c <__gmpn_gcd_subdiv_step@@Base+0x394>  // b.tstop
   2ec0c:	adrp	x3, 50000 <__gmpn_bases@@Base+0x2f98>
   2ec10:	add	x3, x3, #0xb8
   2ec14:	mov	w4, #0x1                   	// #1
   2ec18:	mov	x0, x22
   2ec1c:	mov	x1, xzr
   2ec20:	mov	x2, xzr
   2ec24:	b	2ec80 <__gmpn_gcd_subdiv_step@@Base+0x3a8>
   2ec28:	mov	x0, x22
   2ec2c:	mov	x1, x24
   2ec30:	mov	x2, x19
   2ec34:	mov	x3, x21
   2ec38:	mov	x4, x27
   2ec3c:	mov	w5, w26
   2ec40:	b	2ea74 <__gmpn_gcd_subdiv_step@@Base+0x19c>
   2ec44:	mov	x0, x23
   2ec48:	mov	x1, x24
   2ec4c:	mov	x2, x19
   2ec50:	bl	cc10 <__gmpn_copyi@plt>
   2ec54:	mov	x8, x21
   2ec58:	ldr	x9, [x8]
   2ec5c:	sub	x10, x9, #0x1
   2ec60:	str	x10, [x8], #8
   2ec64:	cbz	x9, 2ec58 <__gmpn_gcd_subdiv_step@@Base+0x380>
   2ec68:	b	2eb80 <__gmpn_gcd_subdiv_step@@Base+0x2a8>
   2ec6c:	mov	x0, x22
   2ec70:	mov	x1, x23
   2ec74:	mov	x2, x27
   2ec78:	mov	x3, xzr
   2ec7c:	mov	x4, xzr
   2ec80:	mov	w5, w26
   2ec84:	blr	x20
   2ec88:	mov	w28, wzr
   2ec8c:	ldr	x21, [sp, #8]
   2ec90:	tbnz	w28, #0, 2eb34 <__gmpn_gcd_subdiv_step@@Base+0x25c>
   2ec94:	mov	x19, xzr
   2ec98:	mov	x0, x19
   2ec9c:	ldp	x20, x19, [sp, #96]
   2eca0:	ldp	x22, x21, [sp, #80]
   2eca4:	ldp	x24, x23, [sp, #64]
   2eca8:	ldp	x26, x25, [sp, #48]
   2ecac:	ldp	x28, x27, [sp, #32]
   2ecb0:	ldp	x29, x30, [sp, #16]
   2ecb4:	add	sp, sp, #0x70
   2ecb8:	ret

000000000002ecbc <__gmpn_gcdext_hook@@Base>:
   2ecbc:	stp	x29, x30, [sp, #-80]!
   2ecc0:	stp	x24, x23, [sp, #32]
   2ecc4:	stp	x22, x21, [sp, #48]
   2ecc8:	stp	x20, x19, [sp, #64]
   2eccc:	ldr	x20, [x0, #32]
   2ecd0:	mov	w21, w5
   2ecd4:	mov	x19, x0
   2ecd8:	str	x25, [sp, #16]
   2ecdc:	mov	x29, sp
   2ece0:	cbz	x1, 2ed2c <__gmpn_gcdext_hook@@Base+0x70>
   2ece4:	ldr	x0, [x19]
   2ece8:	mov	x23, x2
   2ecec:	bl	cc10 <__gmpn_copyi@plt>
   2ecf0:	str	x23, [x19, #8]
   2ecf4:	tbz	w21, #31, 2edcc <__gmpn_gcdext_hook@@Base+0x110>
   2ecf8:	sub	x8, x20, #0x1
   2ecfc:	add	x9, x8, #0x1
   2ed00:	cmp	x9, #0x1
   2ed04:	b.lt	2edc8 <__gmpn_gcdext_hook@@Base+0x10c>  // b.tstop
   2ed08:	ldp	x9, x10, [x19, #40]
   2ed0c:	lsl	x11, x8, #3
   2ed10:	sub	x8, x8, #0x1
   2ed14:	ldr	x9, [x9, x11]
   2ed18:	ldr	x10, [x10, x11]
   2ed1c:	cmp	x9, x10
   2ed20:	b.eq	2ecfc <__gmpn_gcdext_hook@@Base+0x40>  // b.none
   2ed24:	cset	w21, ls  // ls = plast
   2ed28:	b	2edcc <__gmpn_gcdext_hook@@Base+0x110>
   2ed2c:	add	x10, x3, x4, lsl #3
   2ed30:	ldp	x8, x9, [x19, #40]
   2ed34:	ldur	x10, [x10, #-8]
   2ed38:	cmp	w21, #0x0
   2ed3c:	mov	x22, x4
   2ed40:	csel	x21, x8, x9, eq  // eq = none
   2ed44:	csel	x8, x9, x8, eq  // eq = none
   2ed48:	cmp	x10, #0x0
   2ed4c:	cset	w9, eq  // eq = none
   2ed50:	sub	x4, x4, x9
   2ed54:	csetm	x25, eq  // eq = none
   2ed58:	cmp	x4, #0x1
   2ed5c:	b.ne	2ed84 <__gmpn_gcdext_hook@@Base+0xc8>  // b.any
   2ed60:	ldr	x3, [x3]
   2ed64:	mov	x0, x21
   2ed68:	cmp	x3, #0x1
   2ed6c:	b.ne	2ee18 <__gmpn_gcdext_hook@@Base+0x15c>  // b.any
   2ed70:	mov	x1, x21
   2ed74:	mov	x2, x8
   2ed78:	mov	x3, x20
   2ed7c:	bl	cc30 <__gmpn_add_n@plt>
   2ed80:	b	2eeb4 <__gmpn_gcdext_hook@@Base+0x1f8>
   2ed84:	sub	x9, x8, #0x8
   2ed88:	mov	x10, x20
   2ed8c:	mov	x24, x10
   2ed90:	subs	x10, x10, #0x1
   2ed94:	b.lt	2eda0 <__gmpn_gcdext_hook@@Base+0xe4>  // b.tstop
   2ed98:	ldr	x11, [x9, x24, lsl #3]
   2ed9c:	cbz	x11, 2ed8c <__gmpn_gcdext_hook@@Base+0xd0>
   2eda0:	cbz	x24, 2ee28 <__gmpn_gcdext_hook@@Base+0x16c>
   2eda4:	ldr	x23, [x19, #56]
   2eda8:	cmp	x4, x24
   2edac:	mov	x0, x23
   2edb0:	b.le	2ee34 <__gmpn_gcdext_hook@@Base+0x178>
   2edb4:	mov	x1, x3
   2edb8:	mov	x2, x4
   2edbc:	mov	x3, x8
   2edc0:	mov	x4, x24
   2edc4:	b	2ee3c <__gmpn_gcdext_hook@@Base+0x180>
   2edc8:	mov	w21, wzr
   2edcc:	cmp	w21, #0x0
   2edd0:	mov	w8, #0x30                  	// #48
   2edd4:	mov	w9, #0x28                  	// #40
   2edd8:	csel	x8, x9, x8, ne  // ne = any
   2eddc:	ldr	x1, [x19, x8]
   2ede0:	mov	x22, x20
   2ede4:	subs	x20, x20, #0x1
   2ede8:	b.lt	2edf8 <__gmpn_gcdext_hook@@Base+0x13c>  // b.tstop
   2edec:	add	x8, x1, x22, lsl #3
   2edf0:	ldur	x8, [x8, #-8]
   2edf4:	cbz	x8, 2ede0 <__gmpn_gcdext_hook@@Base+0x124>
   2edf8:	ldr	x0, [x19, #16]
   2edfc:	mov	x2, x22
   2ee00:	bl	cc10 <__gmpn_copyi@plt>
   2ee04:	ldr	x8, [x19, #24]
   2ee08:	cmp	w21, #0x0
   2ee0c:	cneg	x9, x22, ne  // ne = any
   2ee10:	str	x9, [x8]
   2ee14:	b	2eec4 <__gmpn_gcdext_hook@@Base+0x208>
   2ee18:	mov	x1, x8
   2ee1c:	mov	x2, x20
   2ee20:	bl	d5e0 <__gmpn_addmul_1@plt>
   2ee24:	b	2eeb4 <__gmpn_gcdext_hook@@Base+0x1f8>
   2ee28:	mov	w8, wzr
   2ee2c:	cbnz	w8, 2eeb4 <__gmpn_gcdext_hook@@Base+0x1f8>
   2ee30:	b	2eec4 <__gmpn_gcdext_hook@@Base+0x208>
   2ee34:	mov	x1, x8
   2ee38:	mov	x2, x24
   2ee3c:	bl	cea0 <__gmpn_mul@plt>
   2ee40:	lsl	x8, x22, #3
   2ee44:	add	x8, x8, x25, lsl #3
   2ee48:	add	x8, x8, x23
   2ee4c:	add	x8, x8, x24, lsl #3
   2ee50:	ldur	x8, [x8, #-8]
   2ee54:	add	x9, x22, x25
   2ee58:	mov	x0, x21
   2ee5c:	cmp	x8, #0x0
   2ee60:	cset	w8, eq  // eq = none
   2ee64:	sub	x8, x9, x8
   2ee68:	add	x22, x8, x24
   2ee6c:	cmp	x22, x20
   2ee70:	b.ge	2ee94 <__gmpn_gcdext_hook@@Base+0x1d8>  // b.tcont
   2ee74:	mov	x1, x21
   2ee78:	mov	x2, x20
   2ee7c:	mov	x3, x23
   2ee80:	mov	x4, x22
   2ee84:	bl	c970 <__gmpn_add@plt>
   2ee88:	mov	w8, #0x1                   	// #1
   2ee8c:	cbnz	w8, 2eeb4 <__gmpn_gcdext_hook@@Base+0x1f8>
   2ee90:	b	2eec4 <__gmpn_gcdext_hook@@Base+0x208>
   2ee94:	mov	x1, x23
   2ee98:	mov	x2, x22
   2ee9c:	mov	x3, x21
   2eea0:	mov	x4, x20
   2eea4:	bl	c970 <__gmpn_add@plt>
   2eea8:	mov	w8, #0x1                   	// #1
   2eeac:	mov	x20, x22
   2eeb0:	cbz	w8, 2eec4 <__gmpn_gcdext_hook@@Base+0x208>
   2eeb4:	cmp	x0, #0x0
   2eeb8:	cinc	x8, x20, ne  // ne = any
   2eebc:	str	x0, [x21, x20, lsl #3]
   2eec0:	str	x8, [x19, #32]
   2eec4:	ldp	x20, x19, [sp, #64]
   2eec8:	ldp	x22, x21, [sp, #48]
   2eecc:	ldp	x24, x23, [sp, #32]
   2eed0:	ldr	x25, [sp, #16]
   2eed4:	ldp	x29, x30, [sp], #80
   2eed8:	ret

000000000002eedc <__gmpn_gcdext_lehmer_n@@Base>:
   2eedc:	sub	sp, sp, #0xf0
   2eee0:	stp	x28, x27, [sp, #160]
   2eee4:	stp	x26, x25, [sp, #176]
   2eee8:	stp	x22, x21, [sp, #208]
   2eeec:	stp	x20, x19, [sp, #224]
   2eef0:	mov	x22, x6
   2eef4:	mov	x25, x5
   2eef8:	mov	x28, x4
   2eefc:	mov	x27, x3
   2ef00:	mov	x20, x2
   2ef04:	mov	x21, x1
   2ef08:	adds	x19, x5, #0x1
   2ef0c:	mov	x26, x0
   2ef10:	stp	x29, x30, [sp, #144]
   2ef14:	stp	x24, x23, [sp, #192]
   2ef18:	add	x29, sp, #0x90
   2ef1c:	b.cs	2ef38 <__gmpn_gcdext_lehmer_n@@Base+0x5c>  // b.hs, b.nlast
   2ef20:	mov	w8, #0x18                  	// #24
   2ef24:	orr	x9, xzr, #0x18
   2ef28:	madd	x2, x25, x8, x9
   2ef2c:	mov	x0, x22
   2ef30:	mov	w1, wzr
   2ef34:	bl	c780 <memset@plt>
   2ef38:	lsl	x8, x19, #3
   2ef3c:	add	x23, x22, x8
   2ef40:	mov	w19, #0x1                   	// #1
   2ef44:	mov	x24, x22
   2ef48:	stp	x22, x21, [sp, #16]
   2ef4c:	add	x22, x23, x8
   2ef50:	str	x19, [x23]
   2ef54:	stp	x26, x25, [sp]
   2ef58:	stur	x26, [x29, #-64]
   2ef5c:	stp	x21, x20, [x29, #-48]
   2ef60:	add	x26, x22, x8
   2ef64:	mov	w21, #0x1                   	// #1
   2ef68:	str	x20, [sp, #32]
   2ef6c:	b	2efc4 <__gmpn_gcdext_lehmer_n@@Base+0xe8>
   2ef70:	add	x0, sp, #0x30
   2ef74:	mov	x1, x26
   2ef78:	mov	x2, x27
   2ef7c:	mov	x3, x28
   2ef80:	mov	x4, x25
   2ef84:	bl	c660 <__gmpn_matrix22_mul1_inverse_vector@plt>
   2ef88:	mov	x25, x0
   2ef8c:	add	x0, sp, #0x30
   2ef90:	mov	x1, x22
   2ef94:	mov	x2, x24
   2ef98:	mov	x3, x23
   2ef9c:	mov	x4, x21
   2efa0:	bl	d620 <__gmpn_hgcd_mul_matrix1_vector@plt>
   2efa4:	mov	x21, x0
   2efa8:	mov	x0, x22
   2efac:	mov	x1, x26
   2efb0:	mov	x22, x24
   2efb4:	mov	x24, x0
   2efb8:	mov	x26, x27
   2efbc:	mov	x27, x1
   2efc0:	tbz	w19, #0, 2f288 <__gmpn_gcdext_lehmer_n@@Base+0x3ac>
   2efc4:	cmp	x25, #0x2
   2efc8:	b.lt	2f0d4 <__gmpn_gcdext_lehmer_n@@Base+0x1f8>  // b.tstop
   2efcc:	lsl	x9, x25, #3
   2efd0:	sub	x8, x9, #0x8
   2efd4:	ldr	x0, [x27, x8]
   2efd8:	ldr	x2, [x28, x8]
   2efdc:	orr	x8, x2, x0
   2efe0:	tbnz	x8, #63, 2f020 <__gmpn_gcdext_lehmer_n@@Base+0x144>
   2efe4:	cmp	x25, #0x2
   2efe8:	clz	x8, x8
   2efec:	b.ne	2f030 <__gmpn_gcdext_lehmer_n@@Base+0x154>  // b.any
   2eff0:	ldp	x10, x9, [x27]
   2eff4:	ldp	x13, x12, [x28]
   2eff8:	neg	w11, w8
   2effc:	lsl	x9, x9, x8
   2f000:	lsr	x14, x10, x11
   2f004:	lsl	x1, x10, x8
   2f008:	lsl	x10, x12, x8
   2f00c:	lsr	x11, x13, x11
   2f010:	orr	x0, x14, x9
   2f014:	orr	x2, x11, x10
   2f018:	lsl	x3, x13, x8
   2f01c:	b	2f07c <__gmpn_gcdext_lehmer_n@@Base+0x1a0>
   2f020:	sub	x8, x9, #0x10
   2f024:	ldr	x1, [x27, x8]
   2f028:	ldr	x3, [x28, x8]
   2f02c:	b	2f07c <__gmpn_gcdext_lehmer_n@@Base+0x1a0>
   2f030:	sub	x11, x9, #0x10
   2f034:	sub	x9, x9, #0x18
   2f038:	ldr	x14, [x27, x11]
   2f03c:	ldr	x15, [x27, x9]
   2f040:	ldr	x11, [x28, x11]
   2f044:	ldr	x9, [x28, x9]
   2f048:	neg	w12, w8
   2f04c:	lsl	x10, x0, x8
   2f050:	lsl	x13, x2, x8
   2f054:	lsr	x16, x14, x12
   2f058:	lsl	x14, x14, x8
   2f05c:	lsr	x15, x15, x12
   2f060:	lsl	x8, x11, x8
   2f064:	lsr	x11, x11, x12
   2f068:	lsr	x9, x9, x12
   2f06c:	orr	x0, x16, x10
   2f070:	orr	x1, x15, x14
   2f074:	orr	x2, x11, x13
   2f078:	orr	x3, x9, x8
   2f07c:	add	x4, sp, #0x30
   2f080:	bl	c730 <__gmpn_hgcd2@plt>
   2f084:	cbnz	w0, 2ef70 <__gmpn_gcdext_lehmer_n@@Base+0x94>
   2f088:	stp	x23, x22, [x29, #-16]
   2f08c:	stp	x21, x24, [x29, #-32]
   2f090:	adrp	x4, 69000 <__gmp_limbroots_table@@Base+0x11338>
   2f094:	ldr	x4, [x4, #3968]
   2f098:	sub	x5, x29, #0x40
   2f09c:	mov	x0, x27
   2f0a0:	mov	x1, x28
   2f0a4:	mov	x2, x25
   2f0a8:	mov	x3, xzr
   2f0ac:	mov	x6, x26
   2f0b0:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   2f0b4:	mov	x25, x0
   2f0b8:	cbz	x0, 2f0c8 <__gmpn_gcdext_lehmer_n@@Base+0x1ec>
   2f0bc:	ldur	x21, [x29, #-32]
   2f0c0:	tbnz	w19, #0, 2efc4 <__gmpn_gcdext_lehmer_n@@Base+0xe8>
   2f0c4:	b	2f288 <__gmpn_gcdext_lehmer_n@@Base+0x3ac>
   2f0c8:	ldur	x20, [x29, #-56]
   2f0cc:	tbnz	wzr, #0, 2efc4 <__gmpn_gcdext_lehmer_n@@Base+0xe8>
   2f0d0:	b	2f288 <__gmpn_gcdext_lehmer_n@@Base+0x3ac>
   2f0d4:	ldr	x8, [x27]
   2f0d8:	cbz	x8, 2f2ac <__gmpn_gcdext_lehmer_n@@Base+0x3d0>
   2f0dc:	ldr	x3, [x28]
   2f0e0:	ldp	x19, x22, [sp, #16]
   2f0e4:	ldp	x25, x20, [sp]
   2f0e8:	cbz	x3, 2f2c4 <__gmpn_gcdext_lehmer_n@@Base+0x3e8>
   2f0ec:	cmp	x8, x3
   2f0f0:	b.ne	2f170 <__gmpn_gcdext_lehmer_n@@Base+0x294>  // b.any
   2f0f4:	str	x8, [x25]
   2f0f8:	add	x8, x19, x20, lsl #3
   2f0fc:	mov	x9, x21
   2f100:	subs	x10, x9, #0x1
   2f104:	b.lt	2f128 <__gmpn_gcdext_lehmer_n@@Base+0x24c>  // b.tstop
   2f108:	lsl	x9, x9, #3
   2f10c:	add	x11, x24, x9
   2f110:	ldur	x11, [x11, #-8]
   2f114:	ldr	x9, [x8, x9]
   2f118:	cmp	x11, x9
   2f11c:	mov	x9, x10
   2f120:	b.eq	2f100 <__gmpn_gcdext_lehmer_n@@Base+0x224>  // b.none
   2f124:	b.ls	2f154 <__gmpn_gcdext_lehmer_n@@Base+0x278>  // b.plast
   2f128:	add	x8, x19, x20, lsl #3
   2f12c:	ldr	x10, [x8, x21, lsl #3]
   2f130:	sub	x9, x21, #0x1
   2f134:	mov	x21, x9
   2f138:	cbz	x10, 2f12c <__gmpn_gcdext_lehmer_n@@Base+0x250>
   2f13c:	add	x19, x9, #0x1
   2f140:	mov	x0, x22
   2f144:	mov	x1, x23
   2f148:	mov	x2, x19
   2f14c:	bl	cc10 <__gmpn_copyi@plt>
   2f150:	b	2f27c <__gmpn_gcdext_lehmer_n@@Base+0x3a0>
   2f154:	mov	x19, x21
   2f158:	subs	x21, x21, #0x1
   2f15c:	b.lt	2f1c4 <__gmpn_gcdext_lehmer_n@@Base+0x2e8>  // b.tstop
   2f160:	add	x8, x24, x19, lsl #3
   2f164:	ldur	x8, [x8, #-8]
   2f168:	cbz	x8, 2f154 <__gmpn_gcdext_lehmer_n@@Base+0x278>
   2f16c:	b	2f1c4 <__gmpn_gcdext_lehmer_n@@Base+0x2e8>
   2f170:	ldr	x2, [x27]
   2f174:	add	x0, sp, #0x30
   2f178:	add	x1, sp, #0x28
   2f17c:	bl	d310 <__gmpn_gcdext_1@plt>
   2f180:	str	x0, [x25]
   2f184:	ldr	x8, [sp, #48]
   2f188:	cbz	x8, 2f1b8 <__gmpn_gcdext_lehmer_n@@Base+0x2dc>
   2f18c:	ldr	x9, [sp, #40]
   2f190:	cbz	x9, 2f1dc <__gmpn_gcdext_lehmer_n@@Base+0x300>
   2f194:	cmp	x8, #0x1
   2f198:	b.lt	2f1f8 <__gmpn_gcdext_lehmer_n@@Base+0x31c>  // b.tstop
   2f19c:	neg	x8, x9
   2f1a0:	mov	w20, wzr
   2f1a4:	str	x8, [sp, #40]
   2f1a8:	b	2f204 <__gmpn_gcdext_lehmer_n@@Base+0x328>
   2f1ac:	add	x8, x24, x19, lsl #3
   2f1b0:	ldur	x8, [x8, #-8]
   2f1b4:	cbnz	x8, 2f1c4 <__gmpn_gcdext_lehmer_n@@Base+0x2e8>
   2f1b8:	mov	x19, x21
   2f1bc:	subs	x21, x21, #0x1
   2f1c0:	b.ge	2f1ac <__gmpn_gcdext_lehmer_n@@Base+0x2d0>  // b.tcont
   2f1c4:	mov	x0, x22
   2f1c8:	mov	x1, x24
   2f1cc:	mov	x2, x19
   2f1d0:	bl	cc10 <__gmpn_copyi@plt>
   2f1d4:	neg	x19, x19
   2f1d8:	b	2f27c <__gmpn_gcdext_lehmer_n@@Base+0x3a0>
   2f1dc:	add	x8, x19, x20, lsl #3
   2f1e0:	mov	x19, x21
   2f1e4:	subs	x21, x21, #0x1
   2f1e8:	b.lt	2f140 <__gmpn_gcdext_lehmer_n@@Base+0x264>  // b.tstop
   2f1ec:	ldr	x9, [x8, x19, lsl #3]
   2f1f0:	cbz	x9, 2f1e0 <__gmpn_gcdext_lehmer_n@@Base+0x304>
   2f1f4:	b	2f140 <__gmpn_gcdext_lehmer_n@@Base+0x264>
   2f1f8:	neg	x8, x8
   2f1fc:	mov	w20, #0x1                   	// #1
   2f200:	str	x8, [sp, #48]
   2f204:	ldr	x3, [sp, #48]
   2f208:	mov	x0, x22
   2f20c:	mov	x1, x23
   2f210:	mov	x2, x21
   2f214:	bl	d670 <__gmpn_mul_1@plt>
   2f218:	ldr	x3, [sp, #40]
   2f21c:	mov	x19, x0
   2f220:	mov	x0, x22
   2f224:	mov	x1, x24
   2f228:	mov	x2, x21
   2f22c:	bl	d5e0 <__gmpn_addmul_1@plt>
   2f230:	orr	x8, x0, x19
   2f234:	cbz	x8, 2f25c <__gmpn_gcdext_lehmer_n@@Base+0x380>
   2f238:	adds	x9, x0, x19
   2f23c:	add	x8, x21, #0x1
   2f240:	str	x9, [x22, x21, lsl #3]
   2f244:	b.cc	2f258 <__gmpn_gcdext_lehmer_n@@Base+0x37c>  // b.lo, b.ul, b.last
   2f248:	add	x21, x21, #0x2
   2f24c:	mov	w9, #0x1                   	// #1
   2f250:	str	x9, [x22, x8, lsl #3]
   2f254:	b	2f25c <__gmpn_gcdext_lehmer_n@@Base+0x380>
   2f258:	mov	x21, x8
   2f25c:	sub	x8, x22, #0x8
   2f260:	ldr	x10, [x8, x21, lsl #3]
   2f264:	sub	x9, x21, #0x1
   2f268:	mov	x21, x9
   2f26c:	cbz	x10, 2f260 <__gmpn_gcdext_lehmer_n@@Base+0x384>
   2f270:	mvn	x8, x9
   2f274:	cmp	w20, #0x0
   2f278:	csinc	x19, x8, x9, ne  // ne = any
   2f27c:	ldr	x8, [sp, #32]
   2f280:	mov	w20, #0x1                   	// #1
   2f284:	str	x19, [x8]
   2f288:	mov	x0, x20
   2f28c:	ldp	x20, x19, [sp, #224]
   2f290:	ldp	x22, x21, [sp, #208]
   2f294:	ldp	x24, x23, [sp, #192]
   2f298:	ldp	x26, x25, [sp, #176]
   2f29c:	ldp	x28, x27, [sp, #160]
   2f2a0:	ldp	x29, x30, [sp, #144]
   2f2a4:	add	sp, sp, #0xf0
   2f2a8:	ret
   2f2ac:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   2f2b0:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   2f2b4:	add	x0, x0, #0xc0
   2f2b8:	add	x2, x2, #0xd0
   2f2bc:	mov	w1, #0xf9                  	// #249
   2f2c0:	bl	c850 <__gmp_assert_fail@plt>
   2f2c4:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   2f2c8:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   2f2cc:	add	x0, x0, #0xc0
   2f2d0:	add	x2, x2, #0xda
   2f2d4:	mov	w1, #0xfa                  	// #250
   2f2d8:	bl	c850 <__gmp_assert_fail@plt>

000000000002f2dc <__gmpn_div_q@@Base>:
   2f2dc:	stp	x29, x30, [sp, #-96]!
   2f2e0:	stp	x28, x27, [sp, #16]
   2f2e4:	stp	x26, x25, [sp, #32]
   2f2e8:	stp	x24, x23, [sp, #48]
   2f2ec:	stp	x22, x21, [sp, #64]
   2f2f0:	stp	x20, x19, [sp, #80]
   2f2f4:	mov	x29, sp
   2f2f8:	sub	sp, sp, #0x60
   2f2fc:	stur	xzr, [x29, #-16]
   2f300:	subs	x25, x4, #0x1
   2f304:	ldr	x28, [x3, x25, lsl #3]
   2f308:	mov	x21, x2
   2f30c:	mov	x2, x1
   2f310:	mov	x19, x0
   2f314:	b.ne	2f330 <__gmpn_div_q@@Base+0x54>  // b.any
   2f318:	mov	x0, x19
   2f31c:	mov	x1, xzr
   2f320:	mov	x3, x21
   2f324:	mov	x4, x28
   2f328:	bl	ced0 <__gmpn_divrem_1@plt>
   2f32c:	b	2fd84 <__gmpn_div_q@@Base+0xaa8>
   2f330:	sub	x22, x21, x4
   2f334:	add	x8, x22, #0x6
   2f338:	mov	x23, x5
   2f33c:	mov	x20, x4
   2f340:	mov	x24, x3
   2f344:	cmp	x8, x4
   2f348:	b.ge	2f434 <__gmpn_div_q@@Base+0x158>  // b.tcont
   2f34c:	add	x8, x22, #0x2
   2f350:	stp	x22, x8, [x29, #-48]
   2f354:	lsl	x26, x8, #3
   2f358:	mov	w8, #0x7f00                	// #32512
   2f35c:	cmp	x26, x8
   2f360:	add	x25, x22, #0x1
   2f364:	mov	x22, x24
   2f368:	stur	x23, [x29, #-24]
   2f36c:	b.hi	2f914 <__gmpn_div_q@@Base+0x638>  // b.pmore
   2f370:	add	x9, x26, #0xf
   2f374:	mov	x8, sp
   2f378:	and	x9, x9, #0xfffffffffffffff0
   2f37c:	sub	x8, x8, x9
   2f380:	stur	x8, [x29, #-56]
   2f384:	mov	sp, x8
   2f388:	ldur	x8, [x29, #-24]
   2f38c:	mov	w27, #0x1                   	// #1
   2f390:	bfi	x27, x25, #1, #63
   2f394:	lsl	x24, x27, #3
   2f398:	cmp	x8, x2
   2f39c:	stur	x25, [x29, #-32]
   2f3a0:	b.ne	2f3cc <__gmpn_div_q@@Base+0xf0>  // b.any
   2f3a4:	add	x1, x24, #0x8
   2f3a8:	mov	w8, #0x7f00                	// #32512
   2f3ac:	cmp	x1, x8
   2f3b0:	b.hi	2fa38 <__gmpn_div_q@@Base+0x75c>  // b.pmore
   2f3b4:	add	x9, x1, #0xf
   2f3b8:	mov	x8, sp
   2f3bc:	and	x9, x9, #0xfffffffffffffff0
   2f3c0:	sub	x8, x8, x9
   2f3c4:	stur	x8, [x29, #-24]
   2f3c8:	mov	sp, x8
   2f3cc:	stp	x2, x22, [x29, #-80]
   2f3d0:	tbnz	x28, #63, 2fa54 <__gmpn_div_q@@Base+0x778>
   2f3d4:	ldur	x25, [x29, #-24]
   2f3d8:	clz	x28, x28
   2f3dc:	add	x8, x2, x21, lsl #3
   2f3e0:	sub	x1, x8, x24
   2f3e4:	mov	x0, x25
   2f3e8:	mov	x2, x27
   2f3ec:	mov	w3, w28
   2f3f0:	mov	x23, x21
   2f3f4:	bl	c2d0 <__gmpn_lshift@plt>
   2f3f8:	cmp	x0, #0x0
   2f3fc:	mov	w8, #0x7f00                	// #32512
   2f400:	cset	w9, ne  // ne = any
   2f404:	cinc	x21, x27, ne  // ne = any
   2f408:	cmp	x26, x8
   2f40c:	str	x0, [x25, x24]
   2f410:	stur	x9, [x29, #-88]
   2f414:	stur	x0, [x29, #-64]
   2f418:	b.hi	2f4c4 <__gmpn_div_q@@Base+0x1e8>  // b.pmore
   2f41c:	add	x9, x26, #0xf
   2f420:	mov	x8, sp
   2f424:	and	x9, x9, #0xfffffffffffffff0
   2f428:	sub	x27, x8, x9
   2f42c:	mov	sp, x27
   2f430:	b	2f4d4 <__gmpn_div_q@@Base+0x1f8>
   2f434:	tbnz	x28, #63, 2f930 <__gmpn_div_q@@Base+0x654>
   2f438:	clz	x27, x28
   2f43c:	mov	x0, x23
   2f440:	mov	x1, x2
   2f444:	mov	x2, x21
   2f448:	mov	w3, w27
   2f44c:	bl	c2d0 <__gmpn_lshift@plt>
   2f450:	cmp	x0, #0x0
   2f454:	lsl	x1, x20, #3
   2f458:	mov	w8, #0x7f00                	// #32512
   2f45c:	cinc	x28, x21, ne  // ne = any
   2f460:	cmp	x1, x8
   2f464:	stur	x0, [x29, #-24]
   2f468:	str	x0, [x23, x21, lsl #3]
   2f46c:	b.hi	2f974 <__gmpn_div_q@@Base+0x698>  // b.pmore
   2f470:	add	x9, x1, #0xf
   2f474:	mov	x8, sp
   2f478:	and	x9, x9, #0xfffffffffffffff0
   2f47c:	sub	x26, x8, x9
   2f480:	mov	sp, x26
   2f484:	mov	x0, x26
   2f488:	mov	x1, x24
   2f48c:	mov	x2, x20
   2f490:	mov	w3, w27
   2f494:	bl	c2d0 <__gmpn_lshift@plt>
   2f498:	cmp	x20, #0x2
   2f49c:	b.ne	2f598 <__gmpn_div_q@@Base+0x2bc>  // b.any
   2f4a0:	mov	x0, x19
   2f4a4:	mov	x1, xzr
   2f4a8:	mov	x2, x23
   2f4ac:	mov	x3, x28
   2f4b0:	mov	x4, x26
   2f4b4:	bl	c350 <__gmpn_divrem_2@plt>
   2f4b8:	ldur	x25, [x29, #-24]
   2f4bc:	cbnz	x25, 2fd7c <__gmpn_div_q@@Base+0xaa0>
   2f4c0:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2f4c4:	sub	x0, x29, #0x10
   2f4c8:	mov	x1, x26
   2f4cc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2f4d0:	mov	x27, x0
   2f4d4:	mov	x24, x22
   2f4d8:	add	x8, x22, x20, lsl #3
   2f4dc:	ldp	x22, x26, [x29, #-48]
   2f4e0:	mov	x9, #0xfffffffffffffffe    	// #-2
   2f4e4:	mov	x0, x27
   2f4e8:	mov	w3, w28
   2f4ec:	sub	x9, x9, x22
   2f4f0:	add	x1, x8, x9, lsl #3
   2f4f4:	mov	x2, x26
   2f4f8:	bl	c2d0 <__gmpn_lshift@plt>
   2f4fc:	sub	x8, x20, x22
   2f500:	add	x8, x24, x8, lsl #3
   2f504:	ldur	x8, [x8, #-24]
   2f508:	ldr	x9, [x27]
   2f50c:	ldp	x25, x24, [x29, #-32]
   2f510:	neg	w10, w28
   2f514:	lsr	x8, x8, x10
   2f518:	orr	x8, x9, x8
   2f51c:	str	x8, [x27]
   2f520:	cbz	x22, 2f658 <__gmpn_div_q@@Base+0x37c>
   2f524:	cmp	x22, #0x95
   2f528:	mov	x28, x21
   2f52c:	b.le	2f688 <__gmpn_div_q@@Base+0x3ac>
   2f530:	cmp	x22, #0x3e3
   2f534:	mov	x21, x23
   2f538:	b.le	2f760 <__gmpn_div_q@@Base+0x484>
   2f53c:	mov	x0, x28
   2f540:	mov	x1, x26
   2f544:	mov	w2, wzr
   2f548:	bl	c220 <__gmpn_mu_divappr_q_itch@plt>
   2f54c:	lsl	x1, x0, #3
   2f550:	mov	w8, #0x7f00                	// #32512
   2f554:	cmp	x1, x8
   2f558:	b.hi	2fc54 <__gmpn_div_q@@Base+0x978>  // b.pmore
   2f55c:	add	x9, x1, #0xf
   2f560:	mov	x8, sp
   2f564:	and	x9, x9, #0xfffffffffffffff0
   2f568:	sub	x5, x8, x9
   2f56c:	mov	sp, x5
   2f570:	ldur	x22, [x29, #-56]
   2f574:	mov	x1, x24
   2f578:	mov	x2, x28
   2f57c:	mov	x3, x27
   2f580:	mov	x0, x22
   2f584:	mov	x4, x26
   2f588:	bl	c8a0 <__gmpn_mu_divappr_q@plt>
   2f58c:	ldur	x8, [x29, #-64]
   2f590:	cbnz	x8, 2f87c <__gmpn_div_q@@Base+0x5a0>
   2f594:	b	2fcdc <__gmpn_div_q@@Base+0xa00>
   2f598:	cmp	x20, #0x98
   2f59c:	b.lt	2f6f4 <__gmpn_div_q@@Base+0x418>  // b.tstop
   2f5a0:	sub	x8, x28, x20
   2f5a4:	cmp	x8, #0x97
   2f5a8:	b.le	2f6f4 <__gmpn_div_q@@Base+0x418>
   2f5ac:	cmp	x21, #0x7cc
   2f5b0:	b.lt	2f5ec <__gmpn_div_q@@Base+0x310>  // b.tstop
   2f5b4:	mov	x8, #0x200000000000        	// #35184372088832
   2f5b8:	mov	x9, #0x800000000000        	// #140737488355328
   2f5bc:	movk	x8, #0x409c, lsl #48
   2f5c0:	movk	x9, #0x4058, lsl #48
   2f5c4:	scvtf	d0, x20
   2f5c8:	scvtf	d1, x21
   2f5cc:	fmov	d2, x8
   2f5d0:	fmov	d3, x9
   2f5d4:	fmul	d2, d0, d2
   2f5d8:	fmul	d3, d1, d3
   2f5dc:	fadd	d2, d3, d2
   2f5e0:	fmul	d0, d1, d0
   2f5e4:	fcmp	d2, d0
   2f5e8:	b.le	2f8ac <__gmpn_div_q@@Base+0x5d0>
   2f5ec:	ldr	x21, [x26, x25, lsl #3]
   2f5f0:	mov	x0, x21
   2f5f4:	bl	d5d0 <__gmpn_invert_limb@plt>
   2f5f8:	add	x8, x26, x20, lsl #3
   2f5fc:	ldur	x8, [x8, #-16]
   2f600:	mul	x9, x0, x21
   2f604:	adds	x9, x9, x8
   2f608:	b.cc	2f624 <__gmpn_div_q@@Base+0x348>  // b.lo, b.ul, b.last
   2f60c:	subs	x9, x9, x21
   2f610:	cset	w10, cs  // cs = hs, nlast
   2f614:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   2f618:	mvn	x10, x10
   2f61c:	add	x0, x10, x0
   2f620:	sub	x9, x9, x11
   2f624:	ldur	x25, [x29, #-24]
   2f628:	umulh	x10, x8, x0
   2f62c:	adds	x10, x10, x9
   2f630:	b.cc	2f820 <__gmpn_div_q@@Base+0x544>  // b.lo, b.ul, b.last
   2f634:	cmp	x10, x21
   2f638:	sub	x9, x0, #0x1
   2f63c:	b.cc	2f824 <__gmpn_div_q@@Base+0x548>  // b.lo, b.ul, b.last
   2f640:	mul	x11, x0, x8
   2f644:	cmp	x10, x21
   2f648:	sub	x12, x0, #0x2
   2f64c:	ccmp	x11, x8, #0x2, ls  // ls = plast
   2f650:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   2f654:	b	2f824 <__gmpn_div_q@@Base+0x548>
   2f658:	ldur	x22, [x29, #-56]
   2f65c:	mov	x1, xzr
   2f660:	mov	x2, x24
   2f664:	mov	x3, x21
   2f668:	mov	x0, x22
   2f66c:	mov	x4, x27
   2f670:	mov	x28, x21
   2f674:	bl	c350 <__gmpn_divrem_2@plt>
   2f678:	mov	x21, x23
   2f67c:	ldur	x8, [x29, #-64]
   2f680:	cbnz	x8, 2f87c <__gmpn_div_q@@Base+0x5a0>
   2f684:	b	2fcdc <__gmpn_div_q@@Base+0xa00>
   2f688:	ldr	x26, [x27, x25, lsl #3]
   2f68c:	mov	x0, x26
   2f690:	bl	d5d0 <__gmpn_invert_limb@plt>
   2f694:	ldr	x8, [x27, x22, lsl #3]
   2f698:	mul	x9, x0, x26
   2f69c:	mov	x21, x23
   2f6a0:	adds	x9, x9, x8
   2f6a4:	b.cc	2f6c0 <__gmpn_div_q@@Base+0x3e4>  // b.lo, b.ul, b.last
   2f6a8:	subs	x9, x9, x26
   2f6ac:	cset	w10, cs  // cs = hs, nlast
   2f6b0:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   2f6b4:	mvn	x10, x10
   2f6b8:	add	x0, x10, x0
   2f6bc:	sub	x9, x9, x11
   2f6c0:	ldur	x22, [x29, #-56]
   2f6c4:	umulh	x10, x8, x0
   2f6c8:	adds	x9, x10, x9
   2f6cc:	b.cc	2f7c8 <__gmpn_div_q@@Base+0x4ec>  // b.lo, b.ul, b.last
   2f6d0:	cmp	x9, x26
   2f6d4:	sub	x5, x0, #0x1
   2f6d8:	b.cc	2f7cc <__gmpn_div_q@@Base+0x4f0>  // b.lo, b.ul, b.last
   2f6dc:	mul	x10, x0, x8
   2f6e0:	cmp	x9, x26
   2f6e4:	sub	x11, x0, #0x2
   2f6e8:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2f6ec:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2f6f0:	b	2f7cc <__gmpn_div_q@@Base+0x4f0>
   2f6f4:	ldr	x21, [x26, x25, lsl #3]
   2f6f8:	mov	x0, x21
   2f6fc:	bl	d5d0 <__gmpn_invert_limb@plt>
   2f700:	add	x8, x26, x20, lsl #3
   2f704:	ldur	x8, [x8, #-16]
   2f708:	mul	x9, x0, x21
   2f70c:	adds	x9, x9, x8
   2f710:	b.cc	2f72c <__gmpn_div_q@@Base+0x450>  // b.lo, b.ul, b.last
   2f714:	subs	x9, x9, x21
   2f718:	cset	w10, cs  // cs = hs, nlast
   2f71c:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   2f720:	mvn	x10, x10
   2f724:	add	x0, x10, x0
   2f728:	sub	x9, x9, x11
   2f72c:	ldur	x25, [x29, #-24]
   2f730:	umulh	x10, x8, x0
   2f734:	adds	x9, x10, x9
   2f738:	b.cc	2f7f8 <__gmpn_div_q@@Base+0x51c>  // b.lo, b.ul, b.last
   2f73c:	cmp	x9, x21
   2f740:	sub	x5, x0, #0x1
   2f744:	b.cc	2f7fc <__gmpn_div_q@@Base+0x520>  // b.lo, b.ul, b.last
   2f748:	mul	x10, x0, x8
   2f74c:	cmp	x9, x21
   2f750:	sub	x11, x0, #0x2
   2f754:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2f758:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2f75c:	b	2f7fc <__gmpn_div_q@@Base+0x520>
   2f760:	ldr	x26, [x27, x25, lsl #3]
   2f764:	mov	x0, x26
   2f768:	bl	d5d0 <__gmpn_invert_limb@plt>
   2f76c:	ldr	x8, [x27, x22, lsl #3]
   2f770:	mul	x9, x0, x26
   2f774:	adds	x9, x9, x8
   2f778:	b.cc	2f794 <__gmpn_div_q@@Base+0x4b8>  // b.lo, b.ul, b.last
   2f77c:	subs	x9, x9, x26
   2f780:	cset	w10, cs  // cs = hs, nlast
   2f784:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   2f788:	mvn	x10, x10
   2f78c:	add	x0, x10, x0
   2f790:	sub	x9, x9, x11
   2f794:	ldur	x22, [x29, #-56]
   2f798:	umulh	x10, x8, x0
   2f79c:	adds	x10, x10, x9
   2f7a0:	b.cc	2f84c <__gmpn_div_q@@Base+0x570>  // b.lo, b.ul, b.last
   2f7a4:	cmp	x10, x26
   2f7a8:	sub	x9, x0, #0x1
   2f7ac:	b.cc	2f850 <__gmpn_div_q@@Base+0x574>  // b.lo, b.ul, b.last
   2f7b0:	mul	x11, x0, x8
   2f7b4:	cmp	x10, x26
   2f7b8:	sub	x12, x0, #0x2
   2f7bc:	ccmp	x11, x8, #0x2, ls  // ls = plast
   2f7c0:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   2f7c4:	b	2f850 <__gmpn_div_q@@Base+0x574>
   2f7c8:	mov	x5, x0
   2f7cc:	ldur	x26, [x29, #-40]
   2f7d0:	mov	x0, x22
   2f7d4:	mov	x1, x24
   2f7d8:	mov	x2, x28
   2f7dc:	mov	x3, x27
   2f7e0:	mov	x4, x26
   2f7e4:	stur	x5, [x29, #-8]
   2f7e8:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   2f7ec:	ldur	x8, [x29, #-64]
   2f7f0:	cbnz	x8, 2f87c <__gmpn_div_q@@Base+0x5a0>
   2f7f4:	b	2fcdc <__gmpn_div_q@@Base+0xa00>
   2f7f8:	mov	x5, x0
   2f7fc:	mov	x0, x19
   2f800:	mov	x1, x23
   2f804:	mov	x2, x28
   2f808:	mov	x3, x26
   2f80c:	mov	x4, x20
   2f810:	stur	x5, [x29, #-8]
   2f814:	bl	d0b0 <__gmpn_sbpi1_div_q@plt>
   2f818:	cbnz	x25, 2fd7c <__gmpn_div_q@@Base+0xaa0>
   2f81c:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2f820:	mov	x9, x0
   2f824:	sub	x5, x29, #0x8
   2f828:	mov	x0, x19
   2f82c:	mov	x1, x23
   2f830:	mov	x2, x28
   2f834:	mov	x3, x26
   2f838:	mov	x4, x20
   2f83c:	stur	x9, [x29, #-8]
   2f840:	bl	cc60 <__gmpn_dcpi1_div_q@plt>
   2f844:	cbnz	x25, 2fd7c <__gmpn_div_q@@Base+0xaa0>
   2f848:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2f84c:	mov	x9, x0
   2f850:	ldur	x26, [x29, #-40]
   2f854:	sub	x5, x29, #0x8
   2f858:	mov	x0, x22
   2f85c:	mov	x1, x24
   2f860:	mov	x2, x28
   2f864:	mov	x3, x27
   2f868:	mov	x4, x26
   2f86c:	stur	x9, [x29, #-8]
   2f870:	bl	c640 <__gmpn_dcpi1_divappr_q@plt>
   2f874:	ldur	x8, [x29, #-64]
   2f878:	cbz	x8, 2fcdc <__gmpn_div_q@@Base+0xa00>
   2f87c:	cbz	x0, 2fce0 <__gmpn_div_q@@Base+0xa04>
   2f880:	cmp	x28, x26
   2f884:	b.le	2fce0 <__gmpn_div_q@@Base+0xa04>
   2f888:	ldur	x8, [x29, #-88]
   2f88c:	mov	w1, #0xff                  	// #255
   2f890:	mov	x0, x22
   2f894:	add	x8, x8, x21
   2f898:	sub	x8, x8, x20
   2f89c:	lsl	x8, x8, #3
   2f8a0:	add	x2, x8, #0x8
   2f8a4:	bl	c780 <memset@plt>
   2f8a8:	b	2fce0 <__gmpn_div_q@@Base+0xa04>
   2f8ac:	mov	x0, x28
   2f8b0:	mov	x1, x20
   2f8b4:	mov	w2, wzr
   2f8b8:	bl	d1a0 <__gmpn_mu_div_q_itch@plt>
   2f8bc:	lsl	x1, x0, #3
   2f8c0:	mov	w8, #0x7f00                	// #32512
   2f8c4:	cmp	x1, x8
   2f8c8:	b.hi	2fe08 <__gmpn_div_q@@Base+0xb2c>  // b.pmore
   2f8cc:	add	x9, x1, #0xf
   2f8d0:	mov	x8, sp
   2f8d4:	and	x9, x9, #0xfffffffffffffff0
   2f8d8:	sub	x5, x8, x9
   2f8dc:	mov	sp, x5
   2f8e0:	ldur	x25, [x29, #-24]
   2f8e4:	mov	x0, x19
   2f8e8:	mov	x1, x23
   2f8ec:	mov	x2, x28
   2f8f0:	mov	x3, x26
   2f8f4:	mov	x4, x20
   2f8f8:	bl	c430 <__gmpn_mu_div_q@plt>
   2f8fc:	cbnz	x25, 2fd7c <__gmpn_div_q@@Base+0xaa0>
   2f900:	str	x0, [x19, x22, lsl #3]
   2f904:	ldur	x0, [x29, #-16]
   2f908:	cbz	x0, 2fd84 <__gmpn_div_q@@Base+0xaa8>
   2f90c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   2f910:	b	2fd84 <__gmpn_div_q@@Base+0xaa8>
   2f914:	sub	x0, x29, #0x10
   2f918:	mov	x1, x26
   2f91c:	mov	x24, x2
   2f920:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2f924:	mov	x2, x24
   2f928:	stur	x0, [x29, #-56]
   2f92c:	b	2f388 <__gmpn_div_q@@Base+0xac>
   2f930:	cmp	x23, x2
   2f934:	b.eq	2f950 <__gmpn_div_q@@Base+0x674>  // b.none
   2f938:	mov	x0, x23
   2f93c:	mov	x1, x2
   2f940:	mov	x25, x2
   2f944:	mov	x2, x21
   2f948:	bl	cc10 <__gmpn_copyi@plt>
   2f94c:	mov	x2, x25
   2f950:	cmp	x20, #0x2
   2f954:	b.ne	2f984 <__gmpn_div_q@@Base+0x6a8>  // b.any
   2f958:	mov	x0, x19
   2f95c:	mov	x1, xzr
   2f960:	mov	x2, x23
   2f964:	mov	x3, x21
   2f968:	mov	x4, x24
   2f96c:	bl	c350 <__gmpn_divrem_2@plt>
   2f970:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2f974:	sub	x0, x29, #0x10
   2f978:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2f97c:	mov	x26, x0
   2f980:	b	2f484 <__gmpn_div_q@@Base+0x1a8>
   2f984:	cmp	x20, #0x98
   2f988:	b.lt	2fb64 <__gmpn_div_q@@Base+0x888>  // b.tstop
   2f98c:	cmp	x22, #0x97
   2f990:	b.le	2fb64 <__gmpn_div_q@@Base+0x888>
   2f994:	cmp	x21, #0x7cc
   2f998:	b.lt	2f9d4 <__gmpn_div_q@@Base+0x6f8>  // b.tstop
   2f99c:	mov	x8, #0x200000000000        	// #35184372088832
   2f9a0:	mov	x9, #0x800000000000        	// #140737488355328
   2f9a4:	movk	x8, #0x409c, lsl #48
   2f9a8:	movk	x9, #0x4058, lsl #48
   2f9ac:	scvtf	d0, x20
   2f9b0:	scvtf	d1, x21
   2f9b4:	fmov	d2, x8
   2f9b8:	fmov	d3, x9
   2f9bc:	fmul	d2, d0, d2
   2f9c0:	fmul	d3, d1, d3
   2f9c4:	fadd	d2, d3, d2
   2f9c8:	fmul	d0, d1, d0
   2f9cc:	fcmp	d2, d0
   2f9d0:	b.le	2fdb4 <__gmpn_div_q@@Base+0xad8>
   2f9d4:	mov	x0, x28
   2f9d8:	bl	d5d0 <__gmpn_invert_limb@plt>
   2f9dc:	add	x8, x24, x20, lsl #3
   2f9e0:	ldur	x8, [x8, #-16]
   2f9e4:	mul	x9, x0, x28
   2f9e8:	adds	x9, x9, x8
   2f9ec:	b.cc	2fa08 <__gmpn_div_q@@Base+0x72c>  // b.lo, b.ul, b.last
   2f9f0:	subs	x9, x9, x28
   2f9f4:	cset	w10, cs  // cs = hs, nlast
   2f9f8:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   2f9fc:	mvn	x10, x10
   2fa00:	add	x0, x10, x0
   2fa04:	sub	x9, x9, x11
   2fa08:	umulh	x10, x8, x0
   2fa0c:	adds	x10, x10, x9
   2fa10:	b.cc	2fc88 <__gmpn_div_q@@Base+0x9ac>  // b.lo, b.ul, b.last
   2fa14:	cmp	x10, x28
   2fa18:	sub	x9, x0, #0x1
   2fa1c:	b.cc	2fc8c <__gmpn_div_q@@Base+0x9b0>  // b.lo, b.ul, b.last
   2fa20:	mul	x11, x0, x8
   2fa24:	cmp	x10, x28
   2fa28:	sub	x12, x0, #0x2
   2fa2c:	ccmp	x11, x8, #0x2, ls  // ls = plast
   2fa30:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   2fa34:	b	2fc8c <__gmpn_div_q@@Base+0x9b0>
   2fa38:	sub	x0, x29, #0x10
   2fa3c:	mov	x25, x2
   2fa40:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fa44:	mov	x2, x25
   2fa48:	stur	x0, [x29, #-24]
   2fa4c:	stp	x2, x22, [x29, #-80]
   2fa50:	tbz	x28, #63, 2f3d4 <__gmpn_div_q@@Base+0xf8>
   2fa54:	ldur	x0, [x29, #-24]
   2fa58:	add	x8, x2, x21, lsl #3
   2fa5c:	sub	x1, x8, x27, lsl #3
   2fa60:	mov	x2, x27
   2fa64:	bl	cc10 <__gmpn_copyi@plt>
   2fa68:	add	x8, x22, x20, lsl #3
   2fa6c:	ldur	x22, [x29, #-48]
   2fa70:	mov	x9, #0xfffffffffffffffe    	// #-2
   2fa74:	sub	x9, x9, x22
   2fa78:	add	x25, x8, x9, lsl #3
   2fa7c:	cbz	x22, 2fae4 <__gmpn_div_q@@Base+0x808>
   2fa80:	cmp	x22, #0x95
   2fa84:	b.le	2fb04 <__gmpn_div_q@@Base+0x828>
   2fa88:	cmp	x22, #0x3e3
   2fa8c:	b.le	2fbc8 <__gmpn_div_q@@Base+0x8ec>
   2fa90:	ldur	x1, [x29, #-40]
   2fa94:	mov	x0, x27
   2fa98:	mov	w2, wzr
   2fa9c:	bl	c220 <__gmpn_mu_divappr_q_itch@plt>
   2faa0:	lsl	x1, x0, #3
   2faa4:	mov	w8, #0x7f00                	// #32512
   2faa8:	cmp	x1, x8
   2faac:	b.hi	2fe18 <__gmpn_div_q@@Base+0xb3c>  // b.pmore
   2fab0:	add	x9, x1, #0xf
   2fab4:	mov	x8, sp
   2fab8:	and	x9, x9, #0xfffffffffffffff0
   2fabc:	sub	x5, x8, x9
   2fac0:	mov	sp, x5
   2fac4:	ldur	x22, [x29, #-56]
   2fac8:	ldur	x1, [x29, #-24]
   2facc:	ldur	x4, [x29, #-40]
   2fad0:	mov	x2, x27
   2fad4:	mov	x0, x22
   2fad8:	mov	x3, x25
   2fadc:	bl	c8a0 <__gmpn_mu_divappr_q@plt>
   2fae0:	b	2fcd8 <__gmpn_div_q@@Base+0x9fc>
   2fae4:	ldur	x22, [x29, #-56]
   2fae8:	ldur	x2, [x29, #-24]
   2faec:	mov	x1, xzr
   2faf0:	mov	x3, x27
   2faf4:	mov	x0, x22
   2faf8:	mov	x4, x25
   2fafc:	bl	c350 <__gmpn_divrem_2@plt>
   2fb00:	b	2fcd8 <__gmpn_div_q@@Base+0x9fc>
   2fb04:	mov	x0, x28
   2fb08:	bl	d5d0 <__gmpn_invert_limb@plt>
   2fb0c:	ldr	x8, [x25, x22, lsl #3]
   2fb10:	mul	x9, x0, x28
   2fb14:	adds	x9, x9, x8
   2fb18:	b.cc	2fb34 <__gmpn_div_q@@Base+0x858>  // b.lo, b.ul, b.last
   2fb1c:	subs	x9, x9, x28
   2fb20:	cset	w10, cs  // cs = hs, nlast
   2fb24:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   2fb28:	mvn	x10, x10
   2fb2c:	add	x0, x10, x0
   2fb30:	sub	x9, x9, x11
   2fb34:	umulh	x10, x8, x0
   2fb38:	adds	x9, x10, x9
   2fb3c:	b.cc	2fc2c <__gmpn_div_q@@Base+0x950>  // b.lo, b.ul, b.last
   2fb40:	cmp	x9, x28
   2fb44:	sub	x5, x0, #0x1
   2fb48:	b.cc	2fc30 <__gmpn_div_q@@Base+0x954>  // b.lo, b.ul, b.last
   2fb4c:	mul	x10, x0, x8
   2fb50:	cmp	x9, x28
   2fb54:	sub	x11, x0, #0x2
   2fb58:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2fb5c:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2fb60:	b	2fc30 <__gmpn_div_q@@Base+0x954>
   2fb64:	mov	x0, x28
   2fb68:	bl	d5d0 <__gmpn_invert_limb@plt>
   2fb6c:	add	x8, x24, x20, lsl #3
   2fb70:	ldur	x8, [x8, #-16]
   2fb74:	mul	x9, x0, x28
   2fb78:	adds	x9, x9, x8
   2fb7c:	b.cc	2fb98 <__gmpn_div_q@@Base+0x8bc>  // b.lo, b.ul, b.last
   2fb80:	subs	x9, x9, x28
   2fb84:	cset	w10, cs  // cs = hs, nlast
   2fb88:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   2fb8c:	mvn	x10, x10
   2fb90:	add	x0, x10, x0
   2fb94:	sub	x9, x9, x11
   2fb98:	umulh	x10, x8, x0
   2fb9c:	adds	x9, x10, x9
   2fba0:	b.cc	2fc64 <__gmpn_div_q@@Base+0x988>  // b.lo, b.ul, b.last
   2fba4:	cmp	x9, x28
   2fba8:	sub	x5, x0, #0x1
   2fbac:	b.cc	2fc68 <__gmpn_div_q@@Base+0x98c>  // b.lo, b.ul, b.last
   2fbb0:	mul	x10, x0, x8
   2fbb4:	cmp	x9, x28
   2fbb8:	sub	x11, x0, #0x2
   2fbbc:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2fbc0:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2fbc4:	b	2fc68 <__gmpn_div_q@@Base+0x98c>
   2fbc8:	mov	x0, x28
   2fbcc:	bl	d5d0 <__gmpn_invert_limb@plt>
   2fbd0:	ldur	x8, [x29, #-48]
   2fbd4:	mul	x9, x0, x28
   2fbd8:	ldr	x8, [x25, x8, lsl #3]
   2fbdc:	adds	x9, x9, x8
   2fbe0:	b.cc	2fbfc <__gmpn_div_q@@Base+0x920>  // b.lo, b.ul, b.last
   2fbe4:	subs	x9, x9, x28
   2fbe8:	cset	w10, cs  // cs = hs, nlast
   2fbec:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   2fbf0:	mvn	x10, x10
   2fbf4:	add	x0, x10, x0
   2fbf8:	sub	x9, x9, x11
   2fbfc:	umulh	x10, x8, x0
   2fc00:	adds	x10, x10, x9
   2fc04:	b.cc	2fcb0 <__gmpn_div_q@@Base+0x9d4>  // b.lo, b.ul, b.last
   2fc08:	cmp	x10, x28
   2fc0c:	sub	x9, x0, #0x1
   2fc10:	b.cc	2fcb4 <__gmpn_div_q@@Base+0x9d8>  // b.lo, b.ul, b.last
   2fc14:	mul	x11, x0, x8
   2fc18:	cmp	x10, x28
   2fc1c:	sub	x12, x0, #0x2
   2fc20:	ccmp	x11, x8, #0x2, ls  // ls = plast
   2fc24:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   2fc28:	b	2fcb4 <__gmpn_div_q@@Base+0x9d8>
   2fc2c:	mov	x5, x0
   2fc30:	ldur	x22, [x29, #-56]
   2fc34:	ldur	x1, [x29, #-24]
   2fc38:	ldur	x4, [x29, #-40]
   2fc3c:	mov	x2, x27
   2fc40:	mov	x0, x22
   2fc44:	mov	x3, x25
   2fc48:	stur	x5, [x29, #-8]
   2fc4c:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   2fc50:	b	2fcd8 <__gmpn_div_q@@Base+0x9fc>
   2fc54:	sub	x0, x29, #0x10
   2fc58:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fc5c:	mov	x5, x0
   2fc60:	b	2f570 <__gmpn_div_q@@Base+0x294>
   2fc64:	mov	x5, x0
   2fc68:	mov	x0, x19
   2fc6c:	mov	x1, x23
   2fc70:	mov	x2, x21
   2fc74:	mov	x3, x24
   2fc78:	mov	x4, x20
   2fc7c:	stur	x5, [x29, #-8]
   2fc80:	bl	d0b0 <__gmpn_sbpi1_div_q@plt>
   2fc84:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2fc88:	mov	x9, x0
   2fc8c:	sub	x5, x29, #0x8
   2fc90:	mov	x0, x19
   2fc94:	mov	x1, x23
   2fc98:	mov	x2, x21
   2fc9c:	mov	x3, x24
   2fca0:	mov	x4, x20
   2fca4:	stur	x9, [x29, #-8]
   2fca8:	bl	cc60 <__gmpn_dcpi1_div_q@plt>
   2fcac:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2fcb0:	mov	x9, x0
   2fcb4:	ldur	x22, [x29, #-56]
   2fcb8:	ldur	x1, [x29, #-24]
   2fcbc:	ldur	x4, [x29, #-40]
   2fcc0:	sub	x5, x29, #0x8
   2fcc4:	mov	x0, x22
   2fcc8:	mov	x2, x27
   2fccc:	mov	x3, x25
   2fcd0:	stur	x9, [x29, #-8]
   2fcd4:	bl	c640 <__gmpn_dcpi1_divappr_q@plt>
   2fcd8:	ldur	x25, [x29, #-32]
   2fcdc:	str	x0, [x22, x25, lsl #3]
   2fce0:	add	x23, x22, #0x8
   2fce4:	mov	x0, x19
   2fce8:	mov	x1, x23
   2fcec:	mov	x2, x25
   2fcf0:	bl	cc10 <__gmpn_copyi@plt>
   2fcf4:	ldr	x8, [x22]
   2fcf8:	cmp	x8, #0x4
   2fcfc:	b.hi	2fd7c <__gmpn_div_q@@Base+0xaa0>  // b.pmore
   2fd00:	add	x22, x21, #0x1
   2fd04:	lsl	x1, x22, #3
   2fd08:	mov	w8, #0x7f00                	// #32512
   2fd0c:	cmp	x1, x8
   2fd10:	b.hi	2fda4 <__gmpn_div_q@@Base+0xac8>  // b.pmore
   2fd14:	add	x9, x1, #0xf
   2fd18:	mov	x8, sp
   2fd1c:	and	x9, x9, #0xfffffffffffffff0
   2fd20:	sub	x25, x8, x9
   2fd24:	mov	sp, x25
   2fd28:	ldur	x1, [x29, #-72]
   2fd2c:	ldur	x4, [x29, #-32]
   2fd30:	mov	x0, x25
   2fd34:	mov	x2, x20
   2fd38:	mov	x3, x23
   2fd3c:	bl	cea0 <__gmpn_mul@plt>
   2fd40:	ldr	x8, [x25, x21, lsl #3]
   2fd44:	cmp	x8, #0x0
   2fd48:	cset	w8, eq  // eq = none
   2fd4c:	sub	x8, x22, x8
   2fd50:	cmp	x8, x21
   2fd54:	b.gt	2fd6c <__gmpn_div_q@@Base+0xa90>
   2fd58:	ldur	x0, [x29, #-80]
   2fd5c:	mov	x1, x25
   2fd60:	mov	x2, x21
   2fd64:	bl	c570 <__gmpn_cmp@plt>
   2fd68:	tbz	w0, #31, 2fd7c <__gmpn_div_q@@Base+0xaa0>
   2fd6c:	ldr	x8, [x19]
   2fd70:	sub	x9, x8, #0x1
   2fd74:	str	x9, [x19], #8
   2fd78:	cbz	x8, 2fd6c <__gmpn_div_q@@Base+0xa90>
   2fd7c:	ldur	x0, [x29, #-16]
   2fd80:	cbnz	x0, 2f90c <__gmpn_div_q@@Base+0x630>
   2fd84:	mov	sp, x29
   2fd88:	ldp	x20, x19, [sp, #80]
   2fd8c:	ldp	x22, x21, [sp, #64]
   2fd90:	ldp	x24, x23, [sp, #48]
   2fd94:	ldp	x26, x25, [sp, #32]
   2fd98:	ldp	x28, x27, [sp, #16]
   2fd9c:	ldp	x29, x30, [sp], #96
   2fda0:	ret
   2fda4:	sub	x0, x29, #0x10
   2fda8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fdac:	mov	x25, x0
   2fdb0:	b	2fd28 <__gmpn_div_q@@Base+0xa4c>
   2fdb4:	mov	x25, x2
   2fdb8:	mov	x0, x21
   2fdbc:	mov	x1, x20
   2fdc0:	mov	w2, wzr
   2fdc4:	bl	d1a0 <__gmpn_mu_div_q_itch@plt>
   2fdc8:	lsl	x1, x0, #3
   2fdcc:	mov	w8, #0x7f00                	// #32512
   2fdd0:	cmp	x1, x8
   2fdd4:	b.hi	2fe28 <__gmpn_div_q@@Base+0xb4c>  // b.pmore
   2fdd8:	add	x9, x1, #0xf
   2fddc:	mov	x8, sp
   2fde0:	and	x9, x9, #0xfffffffffffffff0
   2fde4:	sub	x5, x8, x9
   2fde8:	mov	sp, x5
   2fdec:	mov	x0, x19
   2fdf0:	mov	x1, x25
   2fdf4:	mov	x2, x21
   2fdf8:	mov	x3, x24
   2fdfc:	mov	x4, x20
   2fe00:	bl	c430 <__gmpn_mu_div_q@plt>
   2fe04:	b	2f900 <__gmpn_div_q@@Base+0x624>
   2fe08:	sub	x0, x29, #0x10
   2fe0c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fe10:	mov	x5, x0
   2fe14:	b	2f8e0 <__gmpn_div_q@@Base+0x604>
   2fe18:	sub	x0, x29, #0x10
   2fe1c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fe20:	mov	x5, x0
   2fe24:	b	2fac4 <__gmpn_div_q@@Base+0x7e8>
   2fe28:	sub	x0, x29, #0x10
   2fe2c:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   2fe30:	mov	x5, x0
   2fe34:	b	2fdec <__gmpn_div_q@@Base+0xb10>

000000000002fe38 <__gmpn_tdiv_qr@@Base>:
   2fe38:	stp	x29, x30, [sp, #-96]!
   2fe3c:	stp	x28, x27, [sp, #16]
   2fe40:	stp	x26, x25, [sp, #32]
   2fe44:	stp	x24, x23, [sp, #48]
   2fe48:	stp	x22, x21, [sp, #64]
   2fe4c:	stp	x20, x19, [sp, #80]
   2fe50:	mov	x29, sp
   2fe54:	sub	sp, sp, #0x50
   2fe58:	cbnz	x2, 30940 <__gmpn_tdiv_qr@@Base+0xb08>
   2fe5c:	mov	x21, x6
   2fe60:	mov	x22, x5
   2fe64:	mov	x25, x4
   2fe68:	mov	x23, x3
   2fe6c:	mov	x26, x1
   2fe70:	mov	x20, x0
   2fe74:	subs	x19, x6, #0x1
   2fe78:	b.eq	30050 <__gmpn_tdiv_qr@@Base+0x218>  // b.none
   2fe7c:	cmp	x21, #0x2
   2fe80:	b.eq	2ff94 <__gmpn_tdiv_qr@@Base+0x15c>  // b.none
   2fe84:	cbz	x21, 30958 <__gmpn_tdiv_qr@@Base+0xb20>
   2fe88:	stur	xzr, [x29, #-8]
   2fe8c:	add	x8, x23, x25, lsl #3
   2fe90:	ldur	x28, [x8, #-8]
   2fe94:	ldr	x27, [x22, x19, lsl #3]
   2fe98:	sub	x9, x25, x21
   2fe9c:	str	xzr, [x20, x9, lsl #3]
   2fea0:	cmp	x28, x27
   2fea4:	cinc	x24, x25, cs  // cs = hs, nlast
   2fea8:	cset	w8, cs  // cs = hs, nlast
   2feac:	cmp	x24, x21, lsl #1
   2feb0:	b.ge	30070 <__gmpn_tdiv_qr@@Base+0x238>  // b.tcont
   2feb4:	adds	x9, x9, x8
   2feb8:	b.eq	30164 <__gmpn_tdiv_qr@@Base+0x32c>  // b.none
   2febc:	ldr	x8, [x22, x19, lsl #3]
   2fec0:	sub	x10, x21, x9
   2fec4:	lsl	x11, x9, #3
   2fec8:	stp	x9, x26, [x29, #-40]
   2fecc:	stp	x11, x10, [x29, #-56]
   2fed0:	tbnz	x8, #63, 302dc <__gmpn_tdiv_qr@@Base+0x4a4>
   2fed4:	mov	w9, #0x7f00                	// #32512
   2fed8:	cmp	x11, x9
   2fedc:	clz	x8, x8
   2fee0:	stur	x8, [x29, #-64]
   2fee4:	b.hi	308d8 <__gmpn_tdiv_qr@@Base+0xaa0>  // b.pmore
   2fee8:	add	x9, x11, #0xf
   2feec:	mov	x8, sp
   2fef0:	and	x9, x9, #0xfffffffffffffff0
   2fef4:	sub	x0, x8, x9
   2fef8:	mov	sp, x0
   2fefc:	ldur	x26, [x29, #-40]
   2ff00:	ldur	x19, [x29, #-64]
   2ff04:	add	x24, x22, x10, lsl #3
   2ff08:	mov	x1, x24
   2ff0c:	mov	x2, x26
   2ff10:	mov	w3, w19
   2ff14:	stur	x0, [x29, #-72]
   2ff18:	bl	c2d0 <__gmpn_lshift@plt>
   2ff1c:	ldur	x12, [x29, #-72]
   2ff20:	ldur	x8, [x24, #-8]
   2ff24:	neg	w9, w19
   2ff28:	mov	w1, #0x8                   	// #8
   2ff2c:	ldr	x10, [x12]
   2ff30:	mov	w11, #0x7f00                	// #32512
   2ff34:	bfi	x1, x26, #4, #60
   2ff38:	lsr	x8, x8, x9
   2ff3c:	cmp	x1, x11
   2ff40:	orr	x8, x10, x8
   2ff44:	lsl	x24, x26, #1
   2ff48:	str	x8, [x12]
   2ff4c:	b.hi	308f0 <__gmpn_tdiv_qr@@Base+0xab8>  // b.pmore
   2ff50:	add	x9, x1, #0xf
   2ff54:	mov	x8, sp
   2ff58:	and	x9, x9, #0xfffffffffffffff0
   2ff5c:	sub	x26, x8, x9
   2ff60:	mov	sp, x26
   2ff64:	ldur	x19, [x29, #-64]
   2ff68:	add	x8, x23, x25, lsl #3
   2ff6c:	sub	x1, x8, x24, lsl #3
   2ff70:	mov	x0, x26
   2ff74:	mov	x2, x24
   2ff78:	mov	w3, w19
   2ff7c:	bl	c2d0 <__gmpn_lshift@plt>
   2ff80:	cmp	x28, x27
   2ff84:	b.cc	30338 <__gmpn_tdiv_qr@@Base+0x500>  // b.lo, b.ul, b.last
   2ff88:	str	x0, [x26, x24, lsl #3]
   2ff8c:	add	x26, x26, #0x8
   2ff90:	b	3035c <__gmpn_tdiv_qr@@Base+0x524>
   2ff94:	stur	xzr, [x29, #-8]
   2ff98:	ldr	x8, [x22, #8]
   2ff9c:	tbnz	x8, #63, 300fc <__gmpn_tdiv_qr@@Base+0x2c4>
   2ffa0:	ldr	x9, [x22]
   2ffa4:	clz	x21, x8
   2ffa8:	mov	w10, #0x40                  	// #64
   2ffac:	lsl	x11, x25, #3
   2ffb0:	sub	w19, w10, w21
   2ffb4:	neg	w10, w21
   2ffb8:	mov	w12, #0x7f00                	// #32512
   2ffbc:	lsl	x8, x8, x21
   2ffc0:	add	x1, x11, #0x8
   2ffc4:	lsr	x10, x9, x10
   2ffc8:	cmp	x1, x12
   2ffcc:	lsl	x9, x9, x21
   2ffd0:	orr	x8, x10, x8
   2ffd4:	stp	x9, x8, [x29, #-24]
   2ffd8:	b.hi	307a8 <__gmpn_tdiv_qr@@Base+0x970>  // b.pmore
   2ffdc:	add	x9, x1, #0xf
   2ffe0:	mov	x8, sp
   2ffe4:	and	x9, x9, #0xfffffffffffffff0
   2ffe8:	sub	x22, x8, x9
   2ffec:	mov	sp, x22
   2fff0:	mov	x0, x22
   2fff4:	mov	x1, x23
   2fff8:	mov	x2, x25
   2fffc:	mov	w3, w21
   30000:	bl	c2d0 <__gmpn_lshift@plt>
   30004:	cmp	x0, #0x0
   30008:	mov	x23, x0
   3000c:	str	x0, [x22, x25, lsl #3]
   30010:	cinc	x3, x25, ne  // ne = any
   30014:	sub	x4, x29, #0x18
   30018:	mov	x0, x20
   3001c:	mov	x1, xzr
   30020:	mov	x2, x22
   30024:	bl	c350 <__gmpn_divrem_2@plt>
   30028:	cbnz	x23, 30034 <__gmpn_tdiv_qr@@Base+0x1fc>
   3002c:	add	x8, x20, x25, lsl #3
   30030:	stur	x0, [x8, #-16]
   30034:	ldp	x8, x9, [x22]
   30038:	lsr	x8, x8, x21
   3003c:	lsl	x10, x9, x19
   30040:	lsr	x9, x9, x21
   30044:	orr	x8, x10, x8
   30048:	stp	x8, x9, [x26]
   3004c:	b	308a8 <__gmpn_tdiv_qr@@Base+0xa70>
   30050:	ldr	x4, [x22]
   30054:	mov	x0, x20
   30058:	mov	x1, xzr
   3005c:	mov	x2, x23
   30060:	mov	x3, x25
   30064:	bl	ced0 <__gmpn_divrem_1@plt>
   30068:	str	x0, [x26]
   3006c:	b	308b0 <__gmpn_tdiv_qr@@Base+0xa78>
   30070:	ldr	x8, [x22, x19, lsl #3]
   30074:	stur	x26, [x29, #-32]
   30078:	tbnz	x8, #63, 30170 <__gmpn_tdiv_qr@@Base+0x338>
   3007c:	lsl	x1, x21, #3
   30080:	mov	w9, #0x7f00                	// #32512
   30084:	cmp	x1, x9
   30088:	clz	x26, x8
   3008c:	b.hi	307c8 <__gmpn_tdiv_qr@@Base+0x990>  // b.pmore
   30090:	add	x9, x1, #0xf
   30094:	mov	x8, sp
   30098:	and	x9, x9, #0xfffffffffffffff0
   3009c:	sub	x28, x8, x9
   300a0:	mov	sp, x28
   300a4:	mov	x0, x28
   300a8:	mov	x1, x22
   300ac:	mov	x2, x21
   300b0:	mov	w3, w26
   300b4:	bl	c2d0 <__gmpn_lshift@plt>
   300b8:	lsl	x8, x25, #3
   300bc:	add	x1, x8, #0x8
   300c0:	mov	w8, #0x7f00                	// #32512
   300c4:	cmp	x1, x8
   300c8:	b.hi	307d8 <__gmpn_tdiv_qr@@Base+0x9a0>  // b.pmore
   300cc:	add	x9, x1, #0xf
   300d0:	mov	x8, sp
   300d4:	and	x9, x9, #0xfffffffffffffff0
   300d8:	sub	x27, x8, x9
   300dc:	mov	sp, x27
   300e0:	mov	x0, x27
   300e4:	mov	x1, x23
   300e8:	mov	x2, x25
   300ec:	mov	w3, w26
   300f0:	bl	c2d0 <__gmpn_lshift@plt>
   300f4:	mov	x22, x28
   300f8:	b	301b0 <__gmpn_tdiv_qr@@Base+0x378>
   300fc:	lsl	x1, x25, #3
   30100:	mov	w8, #0x7f00                	// #32512
   30104:	cmp	x1, x8
   30108:	b.hi	307b8 <__gmpn_tdiv_qr@@Base+0x980>  // b.pmore
   3010c:	add	x9, x1, #0xf
   30110:	mov	x8, sp
   30114:	and	x9, x9, #0xfffffffffffffff0
   30118:	sub	x21, x8, x9
   3011c:	mov	sp, x21
   30120:	mov	x0, x21
   30124:	mov	x1, x23
   30128:	mov	x2, x25
   3012c:	bl	cc10 <__gmpn_copyi@plt>
   30130:	mov	x0, x20
   30134:	mov	x1, xzr
   30138:	mov	x2, x21
   3013c:	mov	x3, x25
   30140:	mov	x4, x22
   30144:	bl	c350 <__gmpn_divrem_2@plt>
   30148:	add	x8, x20, x25, lsl #3
   3014c:	stur	x0, [x8, #-16]
   30150:	ldr	x8, [x21]
   30154:	str	x8, [x26]
   30158:	ldr	x8, [x21, #8]
   3015c:	str	x8, [x26, #8]
   30160:	b	308a8 <__gmpn_tdiv_qr@@Base+0xa70>
   30164:	mov	x0, x26
   30168:	mov	x1, x23
   3016c:	b	3029c <__gmpn_tdiv_qr@@Base+0x464>
   30170:	lsl	x8, x25, #3
   30174:	add	x1, x8, #0x8
   30178:	mov	w8, #0x7f00                	// #32512
   3017c:	cmp	x1, x8
   30180:	b.hi	30900 <__gmpn_tdiv_qr@@Base+0xac8>  // b.pmore
   30184:	add	x9, x1, #0xf
   30188:	mov	x8, sp
   3018c:	and	x9, x9, #0xfffffffffffffff0
   30190:	sub	x27, x8, x9
   30194:	mov	sp, x27
   30198:	mov	x0, x27
   3019c:	mov	x1, x23
   301a0:	mov	x2, x25
   301a4:	bl	cc10 <__gmpn_copyi@plt>
   301a8:	mov	x0, xzr
   301ac:	mov	w26, wzr
   301b0:	str	x0, [x27, x25, lsl #3]
   301b4:	ldr	x23, [x22, x19, lsl #3]
   301b8:	mov	x0, x23
   301bc:	bl	d5d0 <__gmpn_invert_limb@plt>
   301c0:	add	x8, x22, x21, lsl #3
   301c4:	ldur	x8, [x8, #-16]
   301c8:	mul	x9, x0, x23
   301cc:	adds	x9, x9, x8
   301d0:	b.cc	301ec <__gmpn_tdiv_qr@@Base+0x3b4>  // b.lo, b.ul, b.last
   301d4:	subs	x9, x9, x23
   301d8:	cset	w10, cs  // cs = hs, nlast
   301dc:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   301e0:	mvn	x10, x10
   301e4:	add	x0, x10, x0
   301e8:	sub	x9, x9, x11
   301ec:	umulh	x10, x8, x0
   301f0:	adds	x9, x10, x9
   301f4:	b.cc	3021c <__gmpn_tdiv_qr@@Base+0x3e4>  // b.lo, b.ul, b.last
   301f8:	cmp	x9, x23
   301fc:	sub	x5, x0, #0x1
   30200:	b.cc	30220 <__gmpn_tdiv_qr@@Base+0x3e8>  // b.lo, b.ul, b.last
   30204:	mul	x10, x0, x8
   30208:	cmp	x9, x23
   3020c:	sub	x11, x0, #0x2
   30210:	ccmp	x10, x8, #0x2, ls  // ls = plast
   30214:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   30218:	b	30220 <__gmpn_tdiv_qr@@Base+0x3e8>
   3021c:	mov	x5, x0
   30220:	cmp	x21, #0x29
   30224:	stur	x5, [x29, #-24]
   30228:	b.le	302a8 <__gmpn_tdiv_qr@@Base+0x470>
   3022c:	cmp	x21, #0x62
   30230:	b.lt	30274 <__gmpn_tdiv_qr@@Base+0x43c>  // b.tstop
   30234:	cmp	x24, #0x7cc
   30238:	b.lt	30274 <__gmpn_tdiv_qr@@Base+0x43c>  // b.tstop
   3023c:	mov	x8, #0x200000000000        	// #35184372088832
   30240:	mov	x9, #0x800000000000        	// #140737488355328
   30244:	movk	x8, #0x409c, lsl #48
   30248:	movk	x9, #0x4058, lsl #48
   3024c:	scvtf	d0, x21
   30250:	scvtf	d1, x24
   30254:	fmov	d2, x8
   30258:	fmov	d3, x9
   3025c:	fmul	d2, d0, d2
   30260:	fmul	d3, d1, d3
   30264:	fadd	d2, d2, d3
   30268:	fmul	d0, d0, d1
   3026c:	fcmp	d2, d0
   30270:	b.le	3043c <__gmpn_tdiv_qr@@Base+0x604>
   30274:	sub	x5, x29, #0x18
   30278:	mov	x0, x20
   3027c:	mov	x1, x27
   30280:	mov	x2, x24
   30284:	mov	x3, x22
   30288:	mov	x4, x21
   3028c:	bl	c510 <__gmpn_dcpi1_div_qr@plt>
   30290:	cbnz	w26, 302c4 <__gmpn_tdiv_qr@@Base+0x48c>
   30294:	ldur	x0, [x29, #-32]
   30298:	mov	x1, x27
   3029c:	mov	x2, x21
   302a0:	bl	cc10 <__gmpn_copyi@plt>
   302a4:	b	308a8 <__gmpn_tdiv_qr@@Base+0xa70>
   302a8:	mov	x0, x20
   302ac:	mov	x1, x27
   302b0:	mov	x2, x24
   302b4:	mov	x3, x22
   302b8:	mov	x4, x21
   302bc:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   302c0:	cbz	w26, 30294 <__gmpn_tdiv_qr@@Base+0x45c>
   302c4:	ldur	x0, [x29, #-32]
   302c8:	mov	x1, x27
   302cc:	mov	x2, x21
   302d0:	mov	w3, w26
   302d4:	bl	c2f0 <__gmpn_rshift@plt>
   302d8:	b	308a8 <__gmpn_tdiv_qr@@Base+0xa70>
   302dc:	mov	w1, #0x8                   	// #8
   302e0:	bfi	x1, x9, #4, #60
   302e4:	mov	w8, #0x7f00                	// #32512
   302e8:	add	x19, x22, x10, lsl #3
   302ec:	cmp	x1, x8
   302f0:	lsl	x24, x9, #1
   302f4:	b.hi	30910 <__gmpn_tdiv_qr@@Base+0xad8>  // b.pmore
   302f8:	add	x9, x1, #0xf
   302fc:	mov	x8, sp
   30300:	and	x9, x9, #0xfffffffffffffff0
   30304:	sub	x26, x8, x9
   30308:	mov	sp, x26
   3030c:	add	x8, x23, x25, lsl #3
   30310:	sub	x1, x8, x24, lsl #3
   30314:	mov	x0, x26
   30318:	mov	x2, x24
   3031c:	bl	cc10 <__gmpn_copyi@plt>
   30320:	cmp	x28, x27
   30324:	b.cc	3049c <__gmpn_tdiv_qr@@Base+0x664>  // b.lo, b.ul, b.last
   30328:	mov	w28, wzr
   3032c:	str	xzr, [x26, x24, lsl #3]
   30330:	add	x26, x26, #0x8
   30334:	b	304a0 <__gmpn_tdiv_qr@@Base+0x668>
   30338:	mvn	x8, x24
   3033c:	add	x8, x8, x25
   30340:	ldr	x8, [x23, x8, lsl #3]
   30344:	ldr	x9, [x26]
   30348:	mov	w10, #0x40                  	// #64
   3034c:	sub	w10, w10, w19
   30350:	lsr	x8, x8, x10
   30354:	orr	x8, x9, x8
   30358:	str	x8, [x26]
   3035c:	ldur	x27, [x29, #-40]
   30360:	mov	x28, x19
   30364:	ldur	x19, [x29, #-72]
   30368:	cmp	x27, #0x2
   3036c:	b.eq	304ac <__gmpn_tdiv_qr@@Base+0x674>  // b.none
   30370:	cmp	x27, #0x1
   30374:	b.ne	303c4 <__gmpn_tdiv_qr@@Base+0x58c>  // b.any
   30378:	ldr	x9, [x19]
   3037c:	ldp	x8, x10, [x26]
   30380:	lsr	x12, x9, #32
   30384:	udiv	x15, x10, x12
   30388:	and	x11, x9, #0xffffffff
   3038c:	msub	w10, w15, w12, w10
   30390:	mul	x13, x15, x11
   30394:	extr	x14, x10, x8, #32
   30398:	cmp	x14, x13
   3039c:	b.cs	304c8 <__gmpn_tdiv_qr@@Base+0x690>  // b.hs, b.nlast
   303a0:	add	x14, x14, x9
   303a4:	cmp	x14, x9
   303a8:	sub	x10, x15, #0x1
   303ac:	b.cc	304cc <__gmpn_tdiv_qr@@Base+0x694>  // b.lo, b.ul, b.last
   303b0:	cmp	x14, x13
   303b4:	b.cs	304cc <__gmpn_tdiv_qr@@Base+0x694>  // b.hs, b.nlast
   303b8:	sub	x10, x15, #0x2
   303bc:	add	x14, x14, x9
   303c0:	b	304cc <__gmpn_tdiv_qr@@Base+0x694>
   303c4:	mov	x8, x27
   303c8:	mov	x27, x23
   303cc:	add	x23, x19, x8, lsl #3
   303d0:	ldur	x24, [x23, #-8]
   303d4:	mov	x0, x24
   303d8:	bl	d5d0 <__gmpn_invert_limb@plt>
   303dc:	ldur	x8, [x23, #-16]
   303e0:	mul	x9, x0, x24
   303e4:	adds	x9, x9, x8
   303e8:	b.cc	30404 <__gmpn_tdiv_qr@@Base+0x5cc>  // b.lo, b.ul, b.last
   303ec:	subs	x9, x9, x24
   303f0:	cset	w10, cs  // cs = hs, nlast
   303f4:	csel	x11, x24, xzr, cs  // cs = hs, nlast
   303f8:	mvn	x10, x10
   303fc:	add	x0, x10, x0
   30400:	sub	x9, x9, x11
   30404:	umulh	x10, x8, x0
   30408:	adds	x9, x10, x9
   3040c:	mov	x23, x27
   30410:	b.cc	30540 <__gmpn_tdiv_qr@@Base+0x708>  // b.lo, b.ul, b.last
   30414:	ldur	x27, [x29, #-40]
   30418:	cmp	x9, x24
   3041c:	sub	x5, x0, #0x1
   30420:	b.cc	30548 <__gmpn_tdiv_qr@@Base+0x710>  // b.lo, b.ul, b.last
   30424:	mul	x10, x0, x8
   30428:	cmp	x9, x24
   3042c:	sub	x11, x0, #0x2
   30430:	ccmp	x10, x8, #0x2, ls  // ls = plast
   30434:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   30438:	b	30548 <__gmpn_tdiv_qr@@Base+0x710>
   3043c:	mov	x0, x24
   30440:	mov	x1, x21
   30444:	mov	w2, wzr
   30448:	bl	d1e0 <__gmpn_mu_div_qr_itch@plt>
   3044c:	lsl	x1, x0, #3
   30450:	mov	w8, #0x7f00                	// #32512
   30454:	cmp	x1, x8
   30458:	b.hi	30920 <__gmpn_tdiv_qr@@Base+0xae8>  // b.pmore
   3045c:	add	x9, x1, #0xf
   30460:	mov	x8, sp
   30464:	and	x9, x9, #0xfffffffffffffff0
   30468:	sub	x6, x8, x9
   3046c:	mov	sp, x6
   30470:	ldur	x19, [x29, #-32]
   30474:	mov	x0, x20
   30478:	mov	x2, x27
   3047c:	mov	x3, x24
   30480:	mov	x1, x19
   30484:	mov	x4, x22
   30488:	mov	x5, x21
   3048c:	bl	cb20 <__gmpn_mu_div_qr@plt>
   30490:	mov	x27, x19
   30494:	cbnz	w26, 302c4 <__gmpn_tdiv_qr@@Base+0x48c>
   30498:	b	30294 <__gmpn_tdiv_qr@@Base+0x45c>
   3049c:	mov	w28, wzr
   304a0:	ldur	x27, [x29, #-40]
   304a4:	cmp	x27, #0x2
   304a8:	b.ne	30370 <__gmpn_tdiv_qr@@Base+0x538>  // b.any
   304ac:	mov	w3, #0x4                   	// #4
   304b0:	mov	x0, x20
   304b4:	mov	x1, xzr
   304b8:	mov	x2, x26
   304bc:	mov	x4, x19
   304c0:	bl	c350 <__gmpn_divrem_2@plt>
   304c4:	b	30610 <__gmpn_tdiv_qr@@Base+0x7d8>
   304c8:	mov	x10, x15
   304cc:	sub	x14, x14, x13
   304d0:	udiv	x13, x14, x12
   304d4:	msub	w12, w13, w12, w14
   304d8:	mul	x11, x13, x11
   304dc:	bfi	x8, x12, #32, #32
   304e0:	cmp	x8, x11
   304e4:	b.cs	30510 <__gmpn_tdiv_qr@@Base+0x6d8>  // b.hs, b.nlast
   304e8:	ldp	x15, x14, [x29, #-56]
   304ec:	add	x8, x8, x9
   304f0:	cmp	x8, x9
   304f4:	sub	x12, x13, #0x1
   304f8:	b.cc	30518 <__gmpn_tdiv_qr@@Base+0x6e0>  // b.lo, b.ul, b.last
   304fc:	cmp	x8, x11
   30500:	b.cs	30518 <__gmpn_tdiv_qr@@Base+0x6e0>  // b.hs, b.nlast
   30504:	sub	x12, x13, #0x2
   30508:	add	x8, x8, x9
   3050c:	b	30518 <__gmpn_tdiv_qr@@Base+0x6e0>
   30510:	ldp	x15, x14, [x29, #-56]
   30514:	mov	x12, x13
   30518:	sub	x8, x8, x11
   3051c:	orr	x9, x12, x10, lsl #32
   30520:	str	x8, [x26]
   30524:	str	x9, [x20]
   30528:	cmp	x14, #0x2
   3052c:	b.lt	3061c <__gmpn_tdiv_qr@@Base+0x7e4>  // b.tstop
   30530:	add	x8, x22, x14, lsl #3
   30534:	ldur	x8, [x8, #-16]
   30538:	lsr	x8, x8, #1
   3053c:	b	30620 <__gmpn_tdiv_qr@@Base+0x7e8>
   30540:	ldur	x27, [x29, #-40]
   30544:	mov	x5, x0
   30548:	cmp	x27, #0x29
   3054c:	stur	x5, [x29, #-24]
   30550:	b.le	305d8 <__gmpn_tdiv_qr@@Base+0x7a0>
   30554:	cmp	x27, #0x3e5
   30558:	lsl	x24, x27, #1
   3055c:	b.le	305f4 <__gmpn_tdiv_qr@@Base+0x7bc>
   30560:	mov	x0, x24
   30564:	mov	x1, x27
   30568:	mov	w2, wzr
   3056c:	bl	d1e0 <__gmpn_mu_div_qr_itch@plt>
   30570:	lsl	x1, x0, #3
   30574:	mov	w8, #0x7f00                	// #32512
   30578:	cmp	x1, x8
   3057c:	b.hi	30930 <__gmpn_tdiv_qr@@Base+0xaf8>  // b.pmore
   30580:	add	x9, x1, #0xf
   30584:	mov	x8, sp
   30588:	and	x9, x9, #0xfffffffffffffff0
   3058c:	sub	x6, x8, x9
   30590:	mov	sp, x6
   30594:	ldur	x9, [x29, #-32]
   30598:	sub	x8, x25, x27
   3059c:	mov	x0, x20
   305a0:	mov	x2, x26
   305a4:	add	x8, x9, x8, lsl #3
   305a8:	cmp	x23, x9
   305ac:	csel	x25, x8, x9, eq  // eq = none
   305b0:	mov	x1, x25
   305b4:	mov	x3, x24
   305b8:	mov	x4, x19
   305bc:	mov	x5, x27
   305c0:	bl	cb20 <__gmpn_mu_div_qr@plt>
   305c4:	mov	x0, x26
   305c8:	mov	x1, x25
   305cc:	mov	x2, x27
   305d0:	bl	cc10 <__gmpn_copyi@plt>
   305d4:	b	30610 <__gmpn_tdiv_qr@@Base+0x7d8>
   305d8:	lsl	x2, x27, #1
   305dc:	mov	x0, x20
   305e0:	mov	x1, x26
   305e4:	mov	x3, x19
   305e8:	mov	x4, x27
   305ec:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   305f0:	b	30610 <__gmpn_tdiv_qr@@Base+0x7d8>
   305f4:	sub	x5, x29, #0x18
   305f8:	mov	x0, x20
   305fc:	mov	x1, x26
   30600:	mov	x2, x24
   30604:	mov	x3, x19
   30608:	mov	x4, x27
   3060c:	bl	c510 <__gmpn_dcpi1_div_qr@plt>
   30610:	ldp	x15, x14, [x29, #-56]
   30614:	cmp	x14, #0x2
   30618:	b.ge	30530 <__gmpn_tdiv_qr@@Base+0x6f8>  // b.tcont
   3061c:	mov	x8, xzr
   30620:	sub	x25, x14, #0x1
   30624:	ldr	x10, [x22, x25, lsl #3]
   30628:	sub	x11, x15, #0x8
   3062c:	ldr	x12, [x20, x11]
   30630:	ldr	x11, [x26, x11]
   30634:	mvn	w9, w28
   30638:	lsl	x10, x10, x28
   3063c:	lsr	x8, x8, x9
   30640:	orr	x8, x10, x8
   30644:	umulh	x8, x8, x12
   30648:	cmp	x11, x8
   3064c:	mov	x24, x27
   30650:	b.cs	30694 <__gmpn_tdiv_qr@@Base+0x85c>  // b.hs, b.nlast
   30654:	mov	x2, x19
   30658:	mov	x19, x14
   3065c:	mov	x8, x20
   30660:	ldr	x9, [x8]
   30664:	sub	x10, x9, #0x1
   30668:	str	x10, [x8], #8
   3066c:	cbz	x9, 30660 <__gmpn_tdiv_qr@@Base+0x828>
   30670:	mov	x0, x26
   30674:	mov	x1, x26
   30678:	mov	x3, x27
   3067c:	bl	cc30 <__gmpn_add_n@plt>
   30680:	mov	x24, x27
   30684:	mov	x14, x19
   30688:	cbz	x0, 30694 <__gmpn_tdiv_qr@@Base+0x85c>
   3068c:	add	x24, x27, #0x1
   30690:	str	x0, [x26, x27, lsl #3]
   30694:	stur	x23, [x29, #-80]
   30698:	cbz	w28, 3070c <__gmpn_tdiv_qr@@Base+0x8d4>
   3069c:	mov	w8, #0x40                  	// #64
   306a0:	sub	w3, w8, w28
   306a4:	mov	x0, x26
   306a8:	mov	x1, x26
   306ac:	mov	x2, x24
   306b0:	bl	c2d0 <__gmpn_lshift@plt>
   306b4:	lsl	x8, x25, #3
   306b8:	ldr	x9, [x23, x8]
   306bc:	ldr	x10, [x26]
   306c0:	mov	x12, x28
   306c4:	mov	x11, #0xffffffffffffffff    	// #-1
   306c8:	lsr	x11, x11, x12
   306cc:	and	x9, x9, x11
   306d0:	orr	x9, x10, x9
   306d4:	str	x9, [x26]
   306d8:	ldr	x8, [x22, x8]
   306dc:	mov	x28, x0
   306e0:	mov	x0, x26
   306e4:	mov	x1, x20
   306e8:	and	x3, x8, x11
   306ec:	mov	x2, x27
   306f0:	bl	cba0 <__gmpn_submul_1@plt>
   306f4:	cmp	x27, x24
   306f8:	b.ne	30718 <__gmpn_tdiv_qr@@Base+0x8e0>  // b.any
   306fc:	subs	x8, x28, x0
   30700:	cset	w23, cc  // cc = lo, ul, last
   30704:	add	x24, x24, #0x1
   30708:	b	30728 <__gmpn_tdiv_qr@@Base+0x8f0>
   3070c:	mov	x23, xzr
   30710:	mov	x25, x14
   30714:	b	3072c <__gmpn_tdiv_qr@@Base+0x8f4>
   30718:	ldr	x8, [x26, x27, lsl #3]
   3071c:	subs	x8, x8, x0
   30720:	b.cc	3095c <__gmpn_tdiv_qr@@Base+0xb24>  // b.lo, b.ul, b.last
   30724:	mov	x23, xzr
   30728:	str	x8, [x26, x27, lsl #3]
   3072c:	lsl	x1, x21, #3
   30730:	mov	w8, #0x7f00                	// #32512
   30734:	cmp	x1, x8
   30738:	b.hi	307e8 <__gmpn_tdiv_qr@@Base+0x9b0>  // b.pmore
   3073c:	add	x9, x1, #0xf
   30740:	mov	x8, sp
   30744:	and	x9, x9, #0xfffffffffffffff0
   30748:	sub	x28, x8, x9
   3074c:	mov	sp, x28
   30750:	cmp	x25, x27
   30754:	b.ge	307fc <__gmpn_tdiv_qr@@Base+0x9c4>  // b.tcont
   30758:	cbz	x25, 30774 <__gmpn_tdiv_qr@@Base+0x93c>
   3075c:	mov	x0, x28
   30760:	mov	x1, x20
   30764:	mov	x2, x27
   30768:	mov	x3, x22
   3076c:	mov	x4, x25
   30770:	b	30810 <__gmpn_tdiv_qr@@Base+0x9d8>
   30774:	ldur	x0, [x29, #-32]
   30778:	mov	x1, x26
   3077c:	mov	x2, x24
   30780:	mov	x26, x0
   30784:	bl	cc10 <__gmpn_copyi@plt>
   30788:	cmp	x24, x21
   3078c:	b.eq	30880 <__gmpn_tdiv_qr@@Base+0xa48>  // b.none
   30790:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   30794:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   30798:	add	x0, x0, #0xe4
   3079c:	add	x2, x2, #0x106
   307a0:	mov	w1, #0x169                 	// #361
   307a4:	bl	c850 <__gmp_assert_fail@plt>
   307a8:	sub	x0, x29, #0x8
   307ac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   307b0:	mov	x22, x0
   307b4:	b	2fff0 <__gmpn_tdiv_qr@@Base+0x1b8>
   307b8:	sub	x0, x29, #0x8
   307bc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   307c0:	mov	x21, x0
   307c4:	b	30120 <__gmpn_tdiv_qr@@Base+0x2e8>
   307c8:	sub	x0, x29, #0x8
   307cc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   307d0:	mov	x28, x0
   307d4:	b	300a4 <__gmpn_tdiv_qr@@Base+0x26c>
   307d8:	sub	x0, x29, #0x8
   307dc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   307e0:	mov	x27, x0
   307e4:	b	300e0 <__gmpn_tdiv_qr@@Base+0x2a8>
   307e8:	sub	x0, x29, #0x8
   307ec:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   307f0:	mov	x28, x0
   307f4:	cmp	x25, x27
   307f8:	b.lt	30758 <__gmpn_tdiv_qr@@Base+0x920>  // b.tstop
   307fc:	mov	x0, x28
   30800:	mov	x1, x22
   30804:	mov	x2, x25
   30808:	mov	x3, x20
   3080c:	mov	x4, x27
   30810:	bl	cea0 <__gmpn_mul@plt>
   30814:	lsl	x19, x25, #3
   30818:	add	x3, x28, x19
   3081c:	mov	x0, x26
   30820:	mov	x1, x26
   30824:	mov	x2, x24
   30828:	mov	x4, x27
   3082c:	bl	d340 <__gmpn_sub@plt>
   30830:	ldur	x8, [x29, #-32]
   30834:	mov	x27, x0
   30838:	sub	x2, x21, x25
   3083c:	mov	x1, x26
   30840:	add	x19, x8, x19
   30844:	mov	x0, x19
   30848:	mov	x26, x8
   3084c:	bl	cc10 <__gmpn_copyi@plt>
   30850:	ldur	x1, [x29, #-80]
   30854:	mov	x0, x26
   30858:	mov	x2, x28
   3085c:	mov	x3, x25
   30860:	orr	x23, x27, x23
   30864:	bl	c420 <__gmpn_sub_n@plt>
   30868:	mov	x3, x0
   3086c:	mov	x0, x19
   30870:	mov	x1, x19
   30874:	mov	x2, x24
   30878:	bl	caf0 <__gmpn_sub_1@plt>
   3087c:	orr	x23, x23, x0
   30880:	cbz	x23, 308a8 <__gmpn_tdiv_qr@@Base+0xa70>
   30884:	ldr	x8, [x20]
   30888:	sub	x9, x8, #0x1
   3088c:	str	x9, [x20], #8
   30890:	cbz	x8, 30884 <__gmpn_tdiv_qr@@Base+0xa4c>
   30894:	mov	x0, x26
   30898:	mov	x1, x26
   3089c:	mov	x2, x22
   308a0:	mov	x3, x21
   308a4:	bl	cc30 <__gmpn_add_n@plt>
   308a8:	ldur	x0, [x29, #-8]
   308ac:	cbnz	x0, 308d0 <__gmpn_tdiv_qr@@Base+0xa98>
   308b0:	mov	sp, x29
   308b4:	ldp	x20, x19, [sp, #80]
   308b8:	ldp	x22, x21, [sp, #64]
   308bc:	ldp	x24, x23, [sp, #48]
   308c0:	ldp	x26, x25, [sp, #32]
   308c4:	ldp	x28, x27, [sp, #16]
   308c8:	ldp	x29, x30, [sp], #96
   308cc:	ret
   308d0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   308d4:	b	308b0 <__gmpn_tdiv_qr@@Base+0xa78>
   308d8:	sub	x0, x29, #0x8
   308dc:	mov	x1, x11
   308e0:	mov	x24, x10
   308e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   308e8:	mov	x10, x24
   308ec:	b	2fefc <__gmpn_tdiv_qr@@Base+0xc4>
   308f0:	sub	x0, x29, #0x8
   308f4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   308f8:	mov	x26, x0
   308fc:	b	2ff64 <__gmpn_tdiv_qr@@Base+0x12c>
   30900:	sub	x0, x29, #0x8
   30904:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   30908:	mov	x27, x0
   3090c:	b	30198 <__gmpn_tdiv_qr@@Base+0x360>
   30910:	sub	x0, x29, #0x8
   30914:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   30918:	mov	x26, x0
   3091c:	b	3030c <__gmpn_tdiv_qr@@Base+0x4d4>
   30920:	sub	x0, x29, #0x8
   30924:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   30928:	mov	x6, x0
   3092c:	b	30470 <__gmpn_tdiv_qr@@Base+0x638>
   30930:	sub	x0, x29, #0x8
   30934:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   30938:	mov	x6, x0
   3093c:	b	30594 <__gmpn_tdiv_qr@@Base+0x75c>
   30940:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   30944:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   30948:	add	x0, x0, #0xe4
   3094c:	add	x2, x2, #0xee
   30950:	mov	w1, #0x32                  	// #50
   30954:	bl	c850 <__gmp_assert_fail@plt>
   30958:	bl	c100 <__gmp_divide_by_zero@plt>
   3095c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   30960:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   30964:	add	x0, x0, #0xe4
   30968:	add	x2, x2, #0xf7
   3096c:	mov	w1, #0x154                 	// #340
   30970:	bl	c850 <__gmp_assert_fail@plt>

0000000000030974 <__gmpn_jacobi_base@@Base>:
   30974:	cbz	x0, 309f8 <__gmpn_jacobi_base@@Base+0x84>
   30978:	lsr	x8, x1, #1
   3097c:	rbit	x9, x0
   30980:	clz	x9, x9
   30984:	eor	w10, w8, w1, lsr #2
   30988:	and	w10, w10, w9
   3098c:	lsr	x11, x0, x9
   30990:	eor	w9, w10, w2, asr #1
   30994:	lsr	x10, x11, #1
   30998:	subs	x11, x10, x8
   3099c:	b.eq	309e4 <__gmpn_jacobi_base@@Base+0x70>  // b.none
   309a0:	asr	x12, x11, #63
   309a4:	and	w10, w10, w8
   309a8:	and	w10, w10, w12
   309ac:	eor	w9, w9, w10
   309b0:	and	x10, x12, x11
   309b4:	add	x8, x10, x8
   309b8:	rbit	x10, x11
   309bc:	eor	x11, x12, x11
   309c0:	clz	x10, x10
   309c4:	sub	x11, x11, x12
   309c8:	add	w12, w10, #0x1
   309cc:	lsr	x13, x8, #1
   309d0:	lsr	x10, x11, x12
   309d4:	eor	w11, w13, w8
   309d8:	and	w11, w12, w11
   309dc:	eor	w9, w9, w11
   309e0:	mov	w11, #0x1                   	// #1
   309e4:	cbz	w11, 309fc <__gmpn_jacobi_base@@Base+0x88>
   309e8:	cbnz	x8, 30998 <__gmpn_jacobi_base@@Base+0x24>
   309ec:	ubfiz	w8, w9, #1, #1
   309f0:	mov	w9, #0x1                   	// #1
   309f4:	sub	w0, w9, w8
   309f8:	ret
   309fc:	mov	w0, wzr
   30a00:	ret

0000000000030a04 <__gmpn_jacobi_2@@Base>:
   30a04:	stp	x29, x30, [sp, #-16]!
   30a08:	mov	x8, x0
   30a0c:	ldr	x10, [x8, #8]
   30a10:	ldp	x9, x8, [x1]
   30a14:	ldr	x0, [x0]
   30a18:	lsl	w2, w2, #1
   30a1c:	mov	x29, sp
   30a20:	cmp	x9, #0x1
   30a24:	b.ne	30a40 <__gmpn_jacobi_2@@Base+0x3c>  // b.any
   30a28:	cbnz	x8, 30a40 <__gmpn_jacobi_2@@Base+0x3c>
   30a2c:	and	w8, w2, #0x2
   30a30:	mov	w9, #0x1                   	// #1
   30a34:	sub	w0, w9, w8
   30a38:	ldp	x29, x30, [sp], #16
   30a3c:	ret
   30a40:	cbz	x0, 30b74 <__gmpn_jacobi_2@@Base+0x170>
   30a44:	tbnz	w0, #0, 30a70 <__gmpn_jacobi_2@@Base+0x6c>
   30a48:	rbit	x11, x0
   30a4c:	clz	x11, x11
   30a50:	eor	w12, w9, w9, lsr #1
   30a54:	neg	x13, x11
   30a58:	lsr	x14, x0, x11
   30a5c:	and	w12, w12, w11, lsl #1
   30a60:	lsl	x13, x10, x13
   30a64:	lsr	x10, x10, x11
   30a68:	orr	x0, x13, x14
   30a6c:	eor	w2, w2, w12
   30a70:	cbz	x10, 30bac <__gmpn_jacobi_2@@Base+0x1a8>
   30a74:	cbz	x8, 30ce0 <__gmpn_jacobi_2@@Base+0x2dc>
   30a78:	cmp	x10, x8
   30a7c:	b.ls	30abc <__gmpn_jacobi_2@@Base+0xb8>  // b.plast
   30a80:	eor	x11, x9, x9, lsr #1
   30a84:	subs	x13, x0, x9
   30a88:	sbc	x12, x10, x8
   30a8c:	cbz	x13, 30bc8 <__gmpn_jacobi_2@@Base+0x1c4>
   30a90:	rbit	x10, x13
   30a94:	clz	x10, x10
   30a98:	neg	x15, x10
   30a9c:	and	w14, w11, w10, lsl #1
   30aa0:	lsr	x13, x13, x10
   30aa4:	lsr	x10, x12, x10
   30aa8:	lsl	x12, x12, x15
   30aac:	eor	w2, w2, w14
   30ab0:	cmp	x10, x8
   30ab4:	orr	x0, x12, x13
   30ab8:	b.hi	30a84 <__gmpn_jacobi_2@@Base+0x80>  // b.pmore
   30abc:	cmp	x10, x8
   30ac0:	b.eq	30b24 <__gmpn_jacobi_2@@Base+0x120>  // b.none
   30ac4:	and	w11, w0, w9
   30ac8:	eor	w2, w2, w11
   30acc:	cbz	x10, 30bb8 <__gmpn_jacobi_2@@Base+0x1b4>
   30ad0:	cmp	x8, x10
   30ad4:	b.ls	30b14 <__gmpn_jacobi_2@@Base+0x110>  // b.plast
   30ad8:	eor	x11, x0, x0, lsr #1
   30adc:	subs	x12, x9, x0
   30ae0:	sbc	x9, x8, x10
   30ae4:	cbz	x12, 30c00 <__gmpn_jacobi_2@@Base+0x1fc>
   30ae8:	rbit	x8, x12
   30aec:	clz	x8, x8
   30af0:	neg	x14, x8
   30af4:	and	w13, w11, w8, lsl #1
   30af8:	lsr	x12, x12, x8
   30afc:	lsr	x8, x9, x8
   30b00:	lsl	x9, x9, x14
   30b04:	eor	w2, w2, w13
   30b08:	cmp	x8, x10
   30b0c:	orr	x9, x9, x12
   30b10:	b.hi	30adc <__gmpn_jacobi_2@@Base+0xd8>  // b.pmore
   30b14:	and	w11, w9, w0
   30b18:	cmp	x10, x8
   30b1c:	eor	w2, w2, w11
   30b20:	b.ne	30a74 <__gmpn_jacobi_2@@Base+0x70>  // b.any
   30b24:	cmp	x0, x9
   30b28:	csel	x8, x0, x9, cc  // cc = lo, ul, last
   30b2c:	csel	x11, x9, x0, cc  // cc = lo, ul, last
   30b30:	subs	x11, x11, x8
   30b34:	b.eq	30cc4 <__gmpn_jacobi_2@@Base+0x2c0>  // b.none
   30b38:	and	w12, w9, w0
   30b3c:	cmp	x0, x9
   30b40:	rbit	x9, x11
   30b44:	lsr	x13, x8, #1
   30b48:	csel	w12, w12, wzr, cc  // cc = lo, ul, last
   30b4c:	clz	x9, x9
   30b50:	eor	w13, w13, w8
   30b54:	eor	w12, w12, w2
   30b58:	and	w13, w13, w9, lsl #1
   30b5c:	lsr	x1, x11, x9
   30b60:	cmp	x1, #0x1
   30b64:	eor	w9, w12, w13
   30b68:	b.ne	30ce8 <__gmpn_jacobi_2@@Base+0x2e4>  // b.any
   30b6c:	and	w8, w9, #0x2
   30b70:	b	30a30 <__gmpn_jacobi_2@@Base+0x2c>
   30b74:	cbz	x10, 30cc4 <__gmpn_jacobi_2@@Base+0x2c0>
   30b78:	rbit	x11, x10
   30b7c:	clz	x11, x11
   30b80:	lsr	x12, x9, #1
   30b84:	lsl	w13, w11, #1
   30b88:	eor	w12, w12, w9
   30b8c:	lsr	x1, x10, x11
   30b90:	orr	w10, w13, #0x80
   30b94:	and	w10, w10, w12
   30b98:	cmp	x1, #0x1
   30b9c:	eor	w10, w2, w10
   30ba0:	b.ne	30c40 <__gmpn_jacobi_2@@Base+0x23c>  // b.any
   30ba4:	and	w8, w10, #0x2
   30ba8:	b	30a30 <__gmpn_jacobi_2@@Base+0x2c>
   30bac:	cbz	x8, 30cd0 <__gmpn_jacobi_2@@Base+0x2cc>
   30bb0:	and	w10, w0, w9
   30bb4:	eor	w2, w2, w10
   30bb8:	mov	x1, x0
   30bbc:	cmp	x1, #0x1
   30bc0:	b.ne	30c50 <__gmpn_jacobi_2@@Base+0x24c>  // b.any
   30bc4:	b	30a2c <__gmpn_jacobi_2@@Base+0x28>
   30bc8:	rbit	x10, x12
   30bcc:	clz	x10, x10
   30bd0:	lsr	x11, x9, #1
   30bd4:	lsl	w13, w10, #1
   30bd8:	lsr	x1, x12, x10
   30bdc:	eor	w11, w11, w9
   30be0:	add	w10, w13, #0x80
   30be4:	and	w12, w1, w9
   30be8:	and	w10, w10, w11
   30bec:	eor	w11, w2, w12
   30bf0:	eor	w2, w11, w10
   30bf4:	cmp	x1, #0x1
   30bf8:	b.eq	30a2c <__gmpn_jacobi_2@@Base+0x28>  // b.none
   30bfc:	b	30c50 <__gmpn_jacobi_2@@Base+0x24c>
   30c00:	rbit	x8, x9
   30c04:	clz	x8, x8
   30c08:	lsr	x11, x0, #1
   30c0c:	lsl	w12, w8, #1
   30c10:	lsr	x1, x9, x8
   30c14:	eor	w11, w11, w0
   30c18:	add	w8, w12, #0x80
   30c1c:	and	w9, w1, w0
   30c20:	and	w8, w8, w11
   30c24:	eor	w9, w2, w9
   30c28:	eor	w2, w9, w8
   30c2c:	mov	x9, x0
   30c30:	mov	x8, x10
   30c34:	cmp	x1, #0x1
   30c38:	b.ne	30c50 <__gmpn_jacobi_2@@Base+0x24c>  // b.any
   30c3c:	b	30a2c <__gmpn_jacobi_2@@Base+0x28>
   30c40:	and	w11, w1, w9
   30c44:	eor	w2, w10, w11
   30c48:	cmp	x1, #0x1
   30c4c:	b.eq	30a2c <__gmpn_jacobi_2@@Base+0x28>  // b.none
   30c50:	cbz	x8, 30c90 <__gmpn_jacobi_2@@Base+0x28c>
   30c54:	eor	x10, x1, x1, lsr #1
   30c58:	subs	x11, x9, x1
   30c5c:	cset	w9, cc  // cc = lo, ul, last
   30c60:	sub	x9, x8, x9
   30c64:	cbz	x11, 30c98 <__gmpn_jacobi_2@@Base+0x294>
   30c68:	rbit	x8, x11
   30c6c:	clz	x12, x8
   30c70:	neg	x13, x12
   30c74:	lsr	x11, x11, x12
   30c78:	lsr	x8, x9, x12
   30c7c:	and	w12, w10, w12, lsl #1
   30c80:	lsl	x9, x9, x13
   30c84:	orr	x9, x9, x11
   30c88:	eor	w2, w2, w12
   30c8c:	cbnz	x8, 30c58 <__gmpn_jacobi_2@@Base+0x254>
   30c90:	mov	x0, x9
   30c94:	b	30cd4 <__gmpn_jacobi_2@@Base+0x2d0>
   30c98:	cbz	x9, 30cc4 <__gmpn_jacobi_2@@Base+0x2c0>
   30c9c:	rbit	x8, x9
   30ca0:	clz	x8, x8
   30ca4:	lsr	x10, x1, #1
   30ca8:	lsl	w11, w8, #1
   30cac:	eor	w10, w10, w1
   30cb0:	orr	w11, w11, #0x80
   30cb4:	and	w10, w11, w10
   30cb8:	eor	w2, w2, w10
   30cbc:	lsr	x0, x9, x8
   30cc0:	b	30cd4 <__gmpn_jacobi_2@@Base+0x2d0>
   30cc4:	mov	w0, wzr
   30cc8:	ldp	x29, x30, [sp], #16
   30ccc:	ret
   30cd0:	mov	x1, x9
   30cd4:	bl	c8c0 <__gmpn_jacobi_base@plt>
   30cd8:	ldp	x29, x30, [sp], #16
   30cdc:	ret
   30ce0:	mov	x1, x9
   30ce4:	b	30c2c <__gmpn_jacobi_2@@Base+0x228>
   30ce8:	and	w11, w1, w8
   30cec:	eor	w2, w9, w11
   30cf0:	mov	x9, x8
   30cf4:	b	30c30 <__gmpn_jacobi_2@@Base+0x22c>

0000000000030cf8 <__gmpn_jacobi_n@@Base>:
   30cf8:	stp	x29, x30, [sp, #-96]!
   30cfc:	stp	x28, x27, [sp, #16]
   30d00:	stp	x26, x25, [sp, #32]
   30d04:	stp	x24, x23, [sp, #48]
   30d08:	stp	x22, x21, [sp, #64]
   30d0c:	stp	x20, x19, [sp, #80]
   30d10:	mov	x29, sp
   30d14:	sub	sp, sp, #0x40
   30d18:	mov	x22, x2
   30d1c:	mov	x19, x1
   30d20:	mov	x20, x0
   30d24:	cmp	x2, #0x14a
   30d28:	mov	x8, x2
   30d2c:	stur	w3, [x29, #-4]
   30d30:	b.lt	30d88 <__gmpn_jacobi_n@@Base+0x90>  // b.tstop
   30d34:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   30d38:	lsl	x8, x22, #1
   30d3c:	movk	x9, #0x5556
   30d40:	smulh	x8, x8, x9
   30d44:	add	x21, x8, x8, lsr #63
   30d48:	sub	x0, x22, x21
   30d4c:	add	x8, x0, #0x1
   30d50:	add	x9, x0, #0x2
   30d54:	cmp	x8, #0x0
   30d58:	csinc	x8, x9, x0, lt  // lt = tstop
   30d5c:	lsl	x8, x8, #1
   30d60:	and	x23, x8, #0xfffffffffffffffc
   30d64:	bl	c720 <__gmpn_hgcd_itch@plt>
   30d68:	add	x8, x21, x22
   30d6c:	sub	x9, x8, #0x1
   30d70:	cmp	x0, x8
   30d74:	csel	x8, x9, x0, lt  // lt = tstop
   30d78:	add	x8, x23, x8
   30d7c:	add	x8, x8, #0x4
   30d80:	cmp	x8, x22
   30d84:	csel	x8, x8, x22, gt
   30d88:	lsl	x1, x8, #3
   30d8c:	mov	w8, #0x7f00                	// #32512
   30d90:	cmp	x1, x8
   30d94:	stur	xzr, [x29, #-16]
   30d98:	b.hi	310a8 <__gmpn_jacobi_n@@Base+0x3b0>  // b.pmore
   30d9c:	add	x9, x1, #0xf
   30da0:	mov	x8, sp
   30da4:	and	x9, x9, #0xfffffffffffffff0
   30da8:	sub	x21, x8, x9
   30dac:	mov	sp, x21
   30db0:	mov	x28, #0x5555555555555555    	// #6148914691236517205
   30db4:	adrp	x24, 31000 <__gmpn_jacobi_n@@Base+0x308>
   30db8:	movk	x28, #0x5556
   30dbc:	add	x24, x24, #0xc0
   30dc0:	b	30dec <__gmpn_jacobi_n@@Base+0xf4>
   30dc4:	add	x1, x0, x25
   30dc8:	sub	x0, x29, #0x40
   30dcc:	mov	x2, x20
   30dd0:	mov	x3, x19
   30dd4:	mov	x4, x25
   30dd8:	mov	x5, x26
   30ddc:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   30de0:	mov	x22, x0
   30de4:	mov	w8, #0x1                   	// #1
   30de8:	tbz	w8, #0, 31084 <__gmpn_jacobi_n@@Base+0x38c>
   30dec:	cmp	x22, #0x149
   30df0:	b.le	30ec8 <__gmpn_jacobi_n@@Base+0x1d0>
   30df4:	lsl	x8, x22, #1
   30df8:	smulh	x8, x8, x28
   30dfc:	add	x25, x8, x8, lsr #63
   30e00:	sub	x27, x22, x25
   30e04:	add	x8, x27, #0x1
   30e08:	add	x9, x27, #0x2
   30e0c:	cmp	x8, #0x0
   30e10:	csinc	x8, x9, x27, lt  // lt = tstop
   30e14:	lsl	x8, x8, #4
   30e18:	sub	x0, x29, #0x40
   30e1c:	mov	x1, x27
   30e20:	mov	x2, x21
   30e24:	and	x26, x8, #0xffffffffffffffe0
   30e28:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   30e2c:	add	x9, x26, x21
   30e30:	lsl	x8, x25, #3
   30e34:	add	x26, x9, #0x20
   30e38:	add	x0, x20, x8
   30e3c:	add	x1, x19, x8
   30e40:	sub	x3, x29, #0x40
   30e44:	sub	x4, x29, #0x4
   30e48:	mov	x2, x27
   30e4c:	mov	x5, x26
   30e50:	bl	d570 <__gmpn_hgcd_jacobi@plt>
   30e54:	cmp	x0, #0x1
   30e58:	b.ge	30dc4 <__gmpn_jacobi_n@@Base+0xcc>  // b.tcont
   30e5c:	sub	x5, x29, #0x4
   30e60:	mov	x0, x20
   30e64:	mov	x1, x19
   30e68:	mov	x2, x22
   30e6c:	mov	x3, xzr
   30e70:	mov	x4, x24
   30e74:	mov	x6, x21
   30e78:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   30e7c:	cbnz	x0, 30de0 <__gmpn_jacobi_n@@Base+0xe8>
   30e80:	ldur	x0, [x29, #-16]
   30e84:	cbnz	x0, 30ec0 <__gmpn_jacobi_n@@Base+0x1c8>
   30e88:	ldur	w0, [x29, #-4]
   30e8c:	cmp	w0, #0x1f
   30e90:	b.ne	30ea8 <__gmpn_jacobi_n@@Base+0x1b0>  // b.any
   30e94:	mov	x22, xzr
   30e98:	mov	w23, wzr
   30e9c:	mov	w8, wzr
   30ea0:	tbnz	wzr, #0, 30dec <__gmpn_jacobi_n@@Base+0xf4>
   30ea4:	b	31084 <__gmpn_jacobi_n@@Base+0x38c>
   30ea8:	bl	31118 <__gmpn_jacobi_n@@Base+0x420>
   30eac:	mov	w23, w0
   30eb0:	mov	x22, xzr
   30eb4:	mov	w8, wzr
   30eb8:	tbnz	wzr, #0, 30dec <__gmpn_jacobi_n@@Base+0xf4>
   30ebc:	b	31084 <__gmpn_jacobi_n@@Base+0x38c>
   30ec0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   30ec4:	b	30e88 <__gmpn_jacobi_n@@Base+0x190>
   30ec8:	adrp	x24, 31000 <__gmpn_jacobi_n@@Base+0x308>
   30ecc:	add	x24, x24, #0xc0
   30ed0:	b	30f04 <__gmpn_jacobi_n@@Base+0x20c>
   30ed4:	sub	x0, x29, #0x40
   30ed8:	mov	x1, x21
   30edc:	mov	x2, x20
   30ee0:	mov	x3, x19
   30ee4:	mov	x4, x22
   30ee8:	bl	c660 <__gmpn_matrix22_mul1_inverse_vector@plt>
   30eec:	mov	x22, x0
   30ef0:	mov	w8, #0x1                   	// #1
   30ef4:	mov	x0, x20
   30ef8:	mov	x20, x21
   30efc:	mov	x21, x0
   30f00:	tbz	w8, #0, 31084 <__gmpn_jacobi_n@@Base+0x38c>
   30f04:	cmp	x22, #0x3
   30f08:	b.lt	3100c <__gmpn_jacobi_n@@Base+0x314>  // b.tstop
   30f0c:	lsl	x8, x22, #3
   30f10:	sub	x9, x8, #0x8
   30f14:	ldr	x0, [x20, x9]
   30f18:	ldr	x2, [x19, x9]
   30f1c:	orr	x9, x2, x0
   30f20:	tbnz	x9, #63, 30f78 <__gmpn_jacobi_n@@Base+0x280>
   30f24:	sub	x10, x8, #0x10
   30f28:	sub	x8, x8, #0x18
   30f2c:	ldr	x12, [x20, x10]
   30f30:	ldr	x14, [x20, x8]
   30f34:	ldr	x10, [x19, x10]
   30f38:	ldr	x8, [x19, x8]
   30f3c:	clz	x9, x9
   30f40:	neg	w13, w9
   30f44:	lsl	x11, x0, x9
   30f48:	lsl	x15, x2, x9
   30f4c:	lsr	x16, x12, x13
   30f50:	lsl	x12, x12, x9
   30f54:	lsr	x14, x14, x13
   30f58:	lsl	x9, x10, x9
   30f5c:	lsr	x10, x10, x13
   30f60:	lsr	x8, x8, x13
   30f64:	orr	x0, x16, x11
   30f68:	orr	x1, x14, x12
   30f6c:	orr	x2, x10, x15
   30f70:	orr	x3, x8, x9
   30f74:	b	30f84 <__gmpn_jacobi_n@@Base+0x28c>
   30f78:	sub	x8, x8, #0x10
   30f7c:	ldr	x1, [x20, x8]
   30f80:	ldr	x3, [x19, x8]
   30f84:	sub	x4, x29, #0x40
   30f88:	sub	x5, x29, #0x4
   30f8c:	bl	ccb0 <__gmpn_hgcd2_jacobi@plt>
   30f90:	cbnz	w0, 30ed4 <__gmpn_jacobi_n@@Base+0x1dc>
   30f94:	sub	x5, x29, #0x4
   30f98:	mov	x0, x20
   30f9c:	mov	x1, x19
   30fa0:	mov	x2, x22
   30fa4:	mov	x3, xzr
   30fa8:	mov	x4, x24
   30fac:	mov	x6, x21
   30fb0:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   30fb4:	cbz	x0, 30fc4 <__gmpn_jacobi_n@@Base+0x2cc>
   30fb8:	mov	x22, x0
   30fbc:	mov	w8, #0x1                   	// #1
   30fc0:	b	30ff8 <__gmpn_jacobi_n@@Base+0x300>
   30fc4:	ldur	x0, [x29, #-16]
   30fc8:	cbnz	x0, 31004 <__gmpn_jacobi_n@@Base+0x30c>
   30fcc:	ldur	w0, [x29, #-4]
   30fd0:	cmp	w0, #0x1f
   30fd4:	b.ne	30fe8 <__gmpn_jacobi_n@@Base+0x2f0>  // b.any
   30fd8:	mov	w8, wzr
   30fdc:	mov	x22, xzr
   30fe0:	mov	w23, wzr
   30fe4:	b	30ff8 <__gmpn_jacobi_n@@Base+0x300>
   30fe8:	bl	31118 <__gmpn_jacobi_n@@Base+0x420>
   30fec:	mov	w23, w0
   30ff0:	mov	w8, wzr
   30ff4:	mov	x22, xzr
   30ff8:	mov	x0, x21
   30ffc:	mov	x21, x20
   31000:	b	30ef8 <__gmpn_jacobi_n@@Base+0x200>
   31004:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   31008:	b	30fcc <__gmpn_jacobi_n@@Base+0x2d4>
   3100c:	ldur	w8, [x29, #-4]
   31010:	cmp	w8, #0xf
   31014:	csel	x1, x20, x19, hi  // hi = pmore
   31018:	csel	x0, x19, x20, hi  // hi = pmore
   3101c:	cmp	x22, #0x1
   31020:	b.ne	31054 <__gmpn_jacobi_n@@Base+0x35c>  // b.any
   31024:	ldr	x19, [x0]
   31028:	ldur	x0, [x29, #-16]
   3102c:	ldr	x20, [x1]
   31030:	cbnz	x0, 310b8 <__gmpn_jacobi_n@@Base+0x3c0>
   31034:	ldur	w8, [x29, #-4]
   31038:	cmp	x20, #0x1
   3103c:	lsl	w2, w8, #1
   31040:	b.ne	31074 <__gmpn_jacobi_n@@Base+0x37c>  // b.any
   31044:	and	w8, w2, #0x2
   31048:	mov	w9, #0x1                   	// #1
   3104c:	sub	w23, w9, w8
   31050:	b	31084 <__gmpn_jacobi_n@@Base+0x38c>
   31054:	and	w2, w8, #0x1
   31058:	bl	cc70 <__gmpn_jacobi_2@plt>
   3105c:	ldur	x8, [x29, #-16]
   31060:	mov	w23, w0
   31064:	cbz	x8, 31084 <__gmpn_jacobi_n@@Base+0x38c>
   31068:	mov	x0, x8
   3106c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   31070:	b	31084 <__gmpn_jacobi_n@@Base+0x38c>
   31074:	mov	x0, x19
   31078:	mov	x1, x20
   3107c:	bl	c8c0 <__gmpn_jacobi_base@plt>
   31080:	mov	w23, w0
   31084:	mov	w0, w23
   31088:	mov	sp, x29
   3108c:	ldp	x20, x19, [sp, #80]
   31090:	ldp	x22, x21, [sp, #64]
   31094:	ldp	x24, x23, [sp, #48]
   31098:	ldp	x26, x25, [sp, #32]
   3109c:	ldp	x28, x27, [sp, #16]
   310a0:	ldp	x29, x30, [sp], #96
   310a4:	ret
   310a8:	sub	x0, x29, #0x10
   310ac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   310b0:	mov	x21, x0
   310b4:	b	30db0 <__gmpn_jacobi_n@@Base+0xb8>
   310b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   310bc:	b	31034 <__gmpn_jacobi_n@@Base+0x33c>
   310c0:	stp	x29, x30, [sp, #-32]!
   310c4:	str	x19, [sp, #16]
   310c8:	mov	x19, x0
   310cc:	mov	x29, sp
   310d0:	cbz	x1, 310e8 <__gmpn_jacobi_n@@Base+0x3f0>
   310d4:	cmp	x2, #0x1
   310d8:	b.ne	31104 <__gmpn_jacobi_n@@Base+0x40c>  // b.any
   310dc:	ldr	x8, [x1]
   310e0:	cmp	x8, #0x1
   310e4:	b.ne	31104 <__gmpn_jacobi_n@@Base+0x40c>  // b.any
   310e8:	cbz	x3, 3110c <__gmpn_jacobi_n@@Base+0x414>
   310ec:	ldr	w8, [x3]
   310f0:	ldr	w0, [x19]
   310f4:	mov	w1, w5
   310f8:	and	w2, w8, #0x3
   310fc:	bl	31128 <__gmpn_jacobi_n@@Base+0x430>
   31100:	b	31108 <__gmpn_jacobi_n@@Base+0x410>
   31104:	mov	w0, #0x1f                  	// #31
   31108:	str	w0, [x19]
   3110c:	ldr	x19, [sp, #16]
   31110:	ldp	x29, x30, [sp], #32
   31114:	ret
   31118:	ubfiz	w8, w0, #1, #1
   3111c:	mov	w9, #0x1                   	// #1
   31120:	sub	w0, w9, w8
   31124:	ret
   31128:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   3112c:	ldr	x9, [x9, #3872]
   31130:	lsl	w8, w1, #2
   31134:	add	w8, w8, w0, lsl #3
   31138:	add	w8, w8, w2
   3113c:	ldrb	w0, [x9, w8, uxtw]
   31140:	ret

0000000000031144 <__gmpn_get_d@@Base>:
   31144:	fmov	d0, xzr
   31148:	cbz	x1, 311d4 <__gmpn_get_d@@Base+0x90>
   3114c:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   31150:	lsl	x8, x1, #6
   31154:	sub	x9, x9, x3
   31158:	cmp	x8, x9
   3115c:	b.hi	311d8 <__gmpn_get_d@@Base+0x94>  // b.pmore
   31160:	add	x10, x0, x1, lsl #3
   31164:	ldur	x9, [x10, #-8]
   31168:	add	x8, x8, x3
   3116c:	cmp	x1, #0x2
   31170:	clz	x11, x9
   31174:	mvn	x12, x11
   31178:	add	x8, x8, x12
   3117c:	lsl	x9, x9, x11
   31180:	b.lt	3119c <__gmpn_get_d@@Base+0x58>  // b.tstop
   31184:	cmp	w11, #0xc
   31188:	b.cc	3119c <__gmpn_get_d@@Base+0x58>  // b.lo, b.ul, b.last
   3118c:	ldur	x10, [x10, #-16]
   31190:	neg	w11, w11
   31194:	lsr	x10, x10, x11
   31198:	orr	x9, x10, x9
   3119c:	cmp	x8, #0x3ff
   311a0:	b.gt	311d8 <__gmpn_get_d@@Base+0x94>
   311a4:	lsr	x10, x9, #43
   311a8:	cmn	x8, #0x3ff
   311ac:	lsr	x9, x9, #11
   311b0:	b.le	311e8 <__gmpn_get_d@@Base+0xa4>
   311b4:	mov	x11, #0x3ff0000000000000    	// #4607182418800017408
   311b8:	and	x12, x2, #0x8000000000000000
   311bc:	add	x8, x11, x8, lsl #52
   311c0:	bfxil	x12, x9, #0, #32
   311c4:	and	x8, x8, #0x7ff0000000000000
   311c8:	bfi	x12, x10, #32, #20
   311cc:	orr	x8, x12, x8
   311d0:	fmov	d0, x8
   311d4:	ret
   311d8:	mov	x10, xzr
   311dc:	mov	x9, xzr
   311e0:	mov	w8, #0x400                 	// #1024
   311e4:	b	311b4 <__gmpn_get_d@@Base+0x70>
   311e8:	cmn	x8, #0x432
   311ec:	b.ge	311fc <__gmpn_get_d@@Base+0xb8>  // b.tcont
   311f0:	mov	w11, wzr
   311f4:	cbnz	w11, 311b4 <__gmpn_get_d@@Base+0x70>
   311f8:	b	311d4 <__gmpn_get_d@@Base+0x90>
   311fc:	mov	w10, #0xfffffc02            	// #-1022
   31200:	sub	w8, w10, w8
   31204:	lsr	x9, x9, x8
   31208:	mov	w11, #0x1                   	// #1
   3120c:	lsr	x10, x9, #32
   31210:	mov	x8, #0xfffffffffffffc01    	// #-1023
   31214:	cbnz	w11, 311b4 <__gmpn_get_d@@Base+0x70>
   31218:	b	311d4 <__gmpn_get_d@@Base+0x90>

000000000003121c <__gmpn_matrix22_mul_itch@@Base>:
   3121c:	cmp	x0, #0xa
   31220:	b.lt	3123c <__gmpn_matrix22_mul_itch@@Base+0x20>  // b.tstop
   31224:	cmp	x1, #0x9
   31228:	b.le	3123c <__gmpn_matrix22_mul_itch@@Base+0x20>
   3122c:	add	x8, x1, x0
   31230:	add	x8, x8, x8, lsl #1
   31234:	add	x0, x8, #0x5
   31238:	ret
   3123c:	add	x8, x0, x0, lsl #1
   31240:	add	x0, x8, x1, lsl #1
   31244:	ret

0000000000031248 <__gmpn_matrix22_mul@@Base>:
   31248:	sub	sp, sp, #0xb0
   3124c:	stp	x29, x30, [sp, #80]
   31250:	add	x29, sp, #0x50
   31254:	stp	x28, x27, [sp, #96]
   31258:	stp	x26, x25, [sp, #112]
   3125c:	ldp	x27, x26, [x29, #104]
   31260:	ldr	x8, [x29, #96]
   31264:	stp	x24, x23, [sp, #128]
   31268:	stp	x22, x21, [sp, #144]
   3126c:	mov	x22, x4
   31270:	mov	x25, x1
   31274:	cmp	x4, #0xa
   31278:	mov	x24, x0
   3127c:	stp	x20, x19, [sp, #160]
   31280:	stp	x6, x7, [x29, #-16]
   31284:	stp	x3, x5, [x29, #-32]
   31288:	str	x2, [sp, #40]
   3128c:	str	x8, [sp, #32]
   31290:	b.lt	312e0 <__gmpn_matrix22_mul@@Base+0x98>  // b.tstop
   31294:	cmp	x27, #0x9
   31298:	b.le	312e0 <__gmpn_matrix22_mul@@Base+0x98>
   3129c:	ldp	x8, x2, [sp, #32]
   312a0:	ldp	x3, x5, [x29, #-32]
   312a4:	ldp	x6, x7, [x29, #-16]
   312a8:	mov	x0, x24
   312ac:	mov	x1, x25
   312b0:	mov	x4, x22
   312b4:	stp	x27, x26, [sp, #8]
   312b8:	str	x8, [sp]
   312bc:	bl	31414 <__gmpn_matrix22_mul@@Base+0x1cc>
   312c0:	ldp	x20, x19, [sp, #160]
   312c4:	ldp	x22, x21, [sp, #144]
   312c8:	ldp	x24, x23, [sp, #128]
   312cc:	ldp	x26, x25, [sp, #112]
   312d0:	ldp	x28, x27, [sp, #96]
   312d4:	ldp	x29, x30, [sp, #80]
   312d8:	add	sp, sp, #0xb0
   312dc:	ret
   312e0:	lsl	x8, x22, #3
   312e4:	add	x21, x26, x8
   312e8:	add	x20, x27, x22
   312ec:	add	x8, x21, x8
   312f0:	mov	w19, #0x2                   	// #2
   312f4:	add	x23, x8, x27, lsl #3
   312f8:	lsl	x28, x20, #3
   312fc:	b	3139c <__gmpn_matrix22_mul@@Base+0x154>
   31300:	ldur	x1, [x29, #-24]
   31304:	mov	x2, x27
   31308:	mov	x3, x24
   3130c:	mov	x4, x22
   31310:	bl	cea0 <__gmpn_mul@plt>
   31314:	ldr	x1, [sp, #32]
   31318:	mov	x0, x23
   3131c:	mov	x2, x27
   31320:	mov	x3, x25
   31324:	mov	x4, x22
   31328:	bl	cea0 <__gmpn_mul@plt>
   3132c:	ldur	x1, [x29, #-8]
   31330:	mov	x0, x24
   31334:	mov	x2, x27
   31338:	mov	x3, x25
   3133c:	mov	x4, x22
   31340:	bl	cea0 <__gmpn_mul@plt>
   31344:	ldur	x1, [x29, #-16]
   31348:	mov	x0, x25
   3134c:	mov	x2, x27
   31350:	mov	x3, x26
   31354:	mov	x4, x22
   31358:	bl	cea0 <__gmpn_mul@plt>
   3135c:	mov	x0, x24
   31360:	mov	x1, x24
   31364:	mov	x2, x21
   31368:	mov	x3, x20
   3136c:	bl	cc30 <__gmpn_add_n@plt>
   31370:	str	x0, [x24, x28]
   31374:	mov	x0, x25
   31378:	mov	x1, x25
   3137c:	mov	x2, x23
   31380:	mov	x3, x20
   31384:	bl	cc30 <__gmpn_add_n@plt>
   31388:	str	x0, [x25, x28]
   3138c:	ldr	x24, [sp, #40]
   31390:	ldur	x25, [x29, #-32]
   31394:	subs	w19, w19, #0x1
   31398:	b.eq	312c0 <__gmpn_matrix22_mul@@Base+0x78>  // b.none
   3139c:	mov	x0, x26
   313a0:	mov	x1, x24
   313a4:	mov	x2, x22
   313a8:	bl	cc10 <__gmpn_copyi@plt>
   313ac:	mov	x0, x21
   313b0:	cmp	x22, x27
   313b4:	b.lt	31300 <__gmpn_matrix22_mul@@Base+0xb8>  // b.tstop
   313b8:	ldur	x3, [x29, #-24]
   313bc:	mov	x1, x24
   313c0:	mov	x2, x22
   313c4:	mov	x4, x27
   313c8:	bl	cea0 <__gmpn_mul@plt>
   313cc:	ldr	x3, [sp, #32]
   313d0:	mov	x0, x23
   313d4:	mov	x1, x25
   313d8:	mov	x2, x22
   313dc:	mov	x4, x27
   313e0:	bl	cea0 <__gmpn_mul@plt>
   313e4:	ldur	x3, [x29, #-8]
   313e8:	mov	x0, x24
   313ec:	mov	x1, x25
   313f0:	mov	x2, x22
   313f4:	mov	x4, x27
   313f8:	bl	cea0 <__gmpn_mul@plt>
   313fc:	ldur	x3, [x29, #-16]
   31400:	mov	x0, x25
   31404:	mov	x1, x26
   31408:	mov	x2, x22
   3140c:	mov	x4, x27
   31410:	b	31358 <__gmpn_matrix22_mul@@Base+0x110>
   31414:	sub	sp, sp, #0xc0
   31418:	stp	x29, x30, [sp, #96]
   3141c:	add	x29, sp, #0x60
   31420:	stp	x28, x27, [sp, #112]
   31424:	ldp	x28, x27, [x29, #104]
   31428:	stp	x20, x19, [sp, #176]
   3142c:	add	x19, x4, #0x1
   31430:	stp	x26, x25, [sp, #128]
   31434:	add	x25, x27, x19, lsl #3
   31438:	add	x8, x28, #0x1
   3143c:	stp	x22, x21, [sp, #160]
   31440:	add	x22, x25, x8, lsl #3
   31444:	stp	x24, x23, [sp, #144]
   31448:	mov	x26, x4
   3144c:	mov	x20, x3
   31450:	mov	x21, x1
   31454:	mov	x23, x0
   31458:	cmp	x4, x28
   3145c:	add	x24, x28, x4
   31460:	mov	x0, x22
   31464:	stp	x5, x6, [sp, #40]
   31468:	stp	x8, x2, [x29, #-32]
   3146c:	stur	x7, [x29, #-16]
   31470:	b.lt	31488 <__gmpn_matrix22_mul@@Base+0x240>  // b.tstop
   31474:	mov	x1, x21
   31478:	mov	x2, x26
   3147c:	mov	x3, x7
   31480:	mov	x4, x28
   31484:	b	31498 <__gmpn_matrix22_mul@@Base+0x250>
   31488:	mov	x1, x7
   3148c:	mov	x2, x28
   31490:	mov	x3, x21
   31494:	mov	x4, x26
   31498:	bl	cea0 <__gmpn_mul@plt>
   3149c:	ldur	x2, [x29, #-24]
   314a0:	add	x8, x24, #0x1
   314a4:	mov	x0, x20
   314a8:	mov	x1, x20
   314ac:	mov	x3, x26
   314b0:	str	x8, [sp, #32]
   314b4:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   314b8:	str	w0, [sp, #4]
   314bc:	stur	x19, [x29, #-40]
   314c0:	cbz	w0, 314e4 <__gmpn_matrix22_mul@@Base+0x29c>
   314c4:	mov	x0, x21
   314c8:	mov	x1, x21
   314cc:	mov	x2, x20
   314d0:	mov	x3, x26
   314d4:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   314d8:	mov	w9, w0
   314dc:	mov	x0, xzr
   314e0:	b	314fc <__gmpn_matrix22_mul@@Base+0x2b4>
   314e4:	mov	x0, x21
   314e8:	mov	x1, x21
   314ec:	mov	x2, x20
   314f0:	mov	x3, x26
   314f4:	bl	cc30 <__gmpn_add_n@plt>
   314f8:	mov	w9, wzr
   314fc:	ldr	x8, [sp, #32]
   31500:	ldr	x19, [x29, #96]
   31504:	str	x0, [x21, x26, lsl #3]
   31508:	str	w9, [sp, #8]
   3150c:	add	x8, x22, x8, lsl #3
   31510:	stur	x8, [x29, #-8]
   31514:	str	x19, [sp, #24]
   31518:	cbz	w9, 31538 <__gmpn_matrix22_mul@@Base+0x2f0>
   3151c:	mov	x0, x27
   31520:	mov	x1, x21
   31524:	mov	x2, x23
   31528:	mov	x3, x26
   3152c:	bl	cc30 <__gmpn_add_n@plt>
   31530:	str	wzr, [sp, #12]
   31534:	b	31590 <__gmpn_matrix22_mul@@Base+0x348>
   31538:	mov	x19, x27
   3153c:	ldr	x27, [x21, x26, lsl #3]
   31540:	cbz	x27, 3156c <__gmpn_matrix22_mul@@Base+0x324>
   31544:	mov	x0, x19
   31548:	mov	x1, x21
   3154c:	mov	x2, x23
   31550:	mov	x3, x26
   31554:	bl	c420 <__gmpn_sub_n@plt>
   31558:	mov	w8, #0x1                   	// #1
   3155c:	sub	x0, x27, x0
   31560:	mov	x27, x19
   31564:	str	w8, [sp, #12]
   31568:	b	3158c <__gmpn_matrix22_mul@@Base+0x344>
   3156c:	mov	x0, x19
   31570:	mov	x1, x23
   31574:	mov	x2, x21
   31578:	mov	x3, x26
   3157c:	mov	x27, x19
   31580:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   31584:	str	w0, [sp, #12]
   31588:	mov	x0, xzr
   3158c:	ldr	x19, [sp, #24]
   31590:	cmp	x26, x28
   31594:	str	x27, [sp, #16]
   31598:	str	x0, [x27, x26, lsl #3]
   3159c:	b.lt	315bc <__gmpn_matrix22_mul@@Base+0x374>  // b.tstop
   315a0:	ldur	x27, [x29, #-8]
   315a4:	ldr	x3, [sp, #40]
   315a8:	mov	x1, x23
   315ac:	mov	x2, x26
   315b0:	mov	x0, x27
   315b4:	mov	x4, x28
   315b8:	b	315d4 <__gmpn_matrix22_mul@@Base+0x38c>
   315bc:	ldur	x27, [x29, #-8]
   315c0:	ldr	x1, [sp, #40]
   315c4:	mov	x2, x28
   315c8:	mov	x3, x23
   315cc:	mov	x0, x27
   315d0:	mov	x4, x26
   315d4:	bl	cea0 <__gmpn_mul@plt>
   315d8:	mov	x0, x23
   315dc:	mov	x1, x22
   315e0:	mov	x2, x27
   315e4:	mov	x3, x24
   315e8:	bl	cc30 <__gmpn_add_n@plt>
   315ec:	ldur	x2, [x29, #-16]
   315f0:	str	x0, [x23, x24, lsl #3]
   315f4:	mov	x0, x25
   315f8:	mov	x1, x19
   315fc:	mov	x3, x28
   31600:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   31604:	cmp	x26, x28
   31608:	mov	w23, w0
   3160c:	mov	x0, x27
   31610:	b.lt	31628 <__gmpn_matrix22_mul@@Base+0x3e0>  // b.tstop
   31614:	mov	x1, x20
   31618:	mov	x2, x26
   3161c:	mov	x3, x25
   31620:	mov	x4, x28
   31624:	b	31638 <__gmpn_matrix22_mul@@Base+0x3f0>
   31628:	mov	x1, x25
   3162c:	mov	x2, x28
   31630:	mov	x3, x20
   31634:	mov	x4, x26
   31638:	bl	cea0 <__gmpn_mul@plt>
   3163c:	ldur	x19, [x29, #-40]
   31640:	ldr	x8, [sp, #48]
   31644:	mov	x0, x25
   31648:	str	xzr, [x27, x24, lsl #3]
   3164c:	cbz	w23, 3166c <__gmpn_matrix22_mul@@Base+0x424>
   31650:	mov	x1, x8
   31654:	mov	x2, x25
   31658:	mov	x3, x28
   3165c:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   31660:	stur	w0, [x29, #-16]
   31664:	mov	x0, xzr
   31668:	b	31680 <__gmpn_matrix22_mul@@Base+0x438>
   3166c:	mov	x1, x25
   31670:	mov	x2, x8
   31674:	mov	x3, x28
   31678:	bl	cc30 <__gmpn_add_n@plt>
   3167c:	stur	wzr, [x29, #-16]
   31680:	ldr	w27, [sp, #8]
   31684:	str	x0, [x25, x28, lsl #3]
   31688:	cbz	x0, 316ac <__gmpn_matrix22_mul@@Base+0x464>
   3168c:	mov	x0, x20
   31690:	cmp	x28, x26
   31694:	b.ge	316cc <__gmpn_matrix22_mul@@Base+0x484>  // b.tcont
   31698:	ldur	x4, [x29, #-32]
   3169c:	mov	x1, x21
   316a0:	mov	x2, x26
   316a4:	mov	x3, x25
   316a8:	b	316dc <__gmpn_matrix22_mul@@Base+0x494>
   316ac:	mov	x0, x20
   316b0:	cmp	x19, x28
   316b4:	b.ge	31700 <__gmpn_matrix22_mul@@Base+0x4b8>  // b.tcont
   316b8:	mov	x1, x25
   316bc:	mov	x2, x28
   316c0:	mov	x3, x21
   316c4:	mov	x4, x19
   316c8:	b	31710 <__gmpn_matrix22_mul@@Base+0x4c8>
   316cc:	ldur	x2, [x29, #-32]
   316d0:	mov	x1, x25
   316d4:	mov	x3, x21
   316d8:	mov	x4, x26
   316dc:	bl	cea0 <__gmpn_mul@plt>
   316e0:	ldr	x8, [x21, x26, lsl #3]
   316e4:	cbz	x8, 31714 <__gmpn_matrix22_mul@@Base+0x4cc>
   316e8:	ldur	x3, [x29, #-32]
   316ec:	add	x0, x20, x26, lsl #3
   316f0:	mov	x1, x0
   316f4:	mov	x2, x25
   316f8:	bl	cc30 <__gmpn_add_n@plt>
   316fc:	b	31714 <__gmpn_matrix22_mul@@Base+0x4cc>
   31700:	mov	x1, x21
   31704:	mov	x2, x19
   31708:	mov	x3, x25
   3170c:	mov	x4, x28
   31710:	bl	cea0 <__gmpn_mul@plt>
   31714:	ldur	w8, [x29, #-16]
   31718:	mov	x0, x20
   3171c:	str	xzr, [x22, x24, lsl #3]
   31720:	cmp	w27, w8
   31724:	b.ne	31740 <__gmpn_matrix22_mul@@Base+0x4f8>  // b.any
   31728:	ldr	x3, [sp, #32]
   3172c:	mov	x1, x20
   31730:	mov	x2, x22
   31734:	bl	cc30 <__gmpn_add_n@plt>
   31738:	mov	w24, wzr
   3173c:	b	31754 <__gmpn_matrix22_mul@@Base+0x50c>
   31740:	ldr	x3, [sp, #32]
   31744:	mov	x1, x22
   31748:	mov	x2, x20
   3174c:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   31750:	mov	w24, w0
   31754:	ldr	w8, [sp, #4]
   31758:	eor	w23, w8, w23
   3175c:	ldur	w8, [x29, #-16]
   31760:	cbz	w8, 31780 <__gmpn_matrix22_mul@@Base+0x538>
   31764:	ldr	x2, [sp, #40]
   31768:	mov	x0, x25
   3176c:	mov	x1, x25
   31770:	mov	x3, x28
   31774:	bl	cc30 <__gmpn_add_n@plt>
   31778:	str	x0, [x25, x28, lsl #3]
   3177c:	b	317cc <__gmpn_matrix22_mul@@Base+0x584>
   31780:	ldr	x8, [x25, x28, lsl #3]
   31784:	cbz	x8, 317b4 <__gmpn_matrix22_mul@@Base+0x56c>
   31788:	ldr	x2, [sp, #40]
   3178c:	mov	x0, x25
   31790:	mov	x1, x25
   31794:	mov	x3, x28
   31798:	bl	c420 <__gmpn_sub_n@plt>
   3179c:	lsl	x8, x28, #3
   317a0:	ldr	x9, [x25, x8]
   317a4:	stur	wzr, [x29, #-16]
   317a8:	sub	x9, x9, x0
   317ac:	str	x9, [x25, x8]
   317b0:	b	317cc <__gmpn_matrix22_mul@@Base+0x584>
   317b4:	ldr	x2, [sp, #40]
   317b8:	mov	x0, x25
   317bc:	mov	x1, x25
   317c0:	mov	x3, x28
   317c4:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   317c8:	stur	w0, [x29, #-16]
   317cc:	cmp	x28, x26
   317d0:	eor	w19, w23, #0x1
   317d4:	mov	x0, x22
   317d8:	b.ge	317f0 <__gmpn_matrix22_mul@@Base+0x5a8>  // b.tcont
   317dc:	ldp	x4, x23, [x29, #-32]
   317e0:	mov	x2, x26
   317e4:	mov	x3, x25
   317e8:	mov	x1, x23
   317ec:	b	31800 <__gmpn_matrix22_mul@@Base+0x5b8>
   317f0:	ldp	x2, x23, [x29, #-32]
   317f4:	mov	x1, x25
   317f8:	mov	x4, x26
   317fc:	mov	x3, x23
   31800:	bl	cea0 <__gmpn_mul@plt>
   31804:	mov	x0, x21
   31808:	cbz	w27, 31820 <__gmpn_matrix22_mul@@Base+0x5d8>
   3180c:	mov	x1, x23
   31810:	mov	x2, x21
   31814:	mov	x3, x26
   31818:	bl	c420 <__gmpn_sub_n@plt>
   3181c:	b	31840 <__gmpn_matrix22_mul@@Base+0x5f8>
   31820:	mov	x1, x21
   31824:	mov	x2, x23
   31828:	mov	x3, x26
   3182c:	bl	cc30 <__gmpn_add_n@plt>
   31830:	lsl	x8, x26, #3
   31834:	ldr	x9, [x21, x8]
   31838:	add	x9, x9, x0
   3183c:	str	x9, [x21, x8]
   31840:	ldur	x27, [x29, #-40]
   31844:	ldur	w4, [x29, #-16]
   31848:	mov	x0, x23
   3184c:	mov	x1, x20
   31850:	add	x23, x27, x28
   31854:	mov	w2, w24
   31858:	mov	x3, x22
   3185c:	mov	x5, x23
   31860:	str	x21, [sp, #40]
   31864:	bl	31a2c <__gmpn_matrix22_mul@@Base+0x7e4>
   31868:	ldur	x3, [x29, #-8]
   3186c:	stur	w0, [x29, #-16]
   31870:	mov	x0, x20
   31874:	mov	x1, x20
   31878:	mov	w2, w24
   3187c:	mov	w4, w19
   31880:	mov	x5, x23
   31884:	bl	31a2c <__gmpn_matrix22_mul@@Base+0x7e4>
   31888:	cmp	x27, x28
   3188c:	mov	w24, w0
   31890:	mov	x0, x22
   31894:	b.ge	318b0 <__gmpn_matrix22_mul@@Base+0x668>  // b.tcont
   31898:	ldr	x21, [sp, #48]
   3189c:	ldr	x3, [sp, #16]
   318a0:	mov	x2, x28
   318a4:	mov	x4, x27
   318a8:	mov	x1, x21
   318ac:	b	318c4 <__gmpn_matrix22_mul@@Base+0x67c>
   318b0:	ldr	x21, [sp, #48]
   318b4:	ldr	x1, [sp, #16]
   318b8:	mov	x2, x27
   318bc:	mov	x4, x28
   318c0:	mov	x3, x21
   318c4:	bl	cea0 <__gmpn_mul@plt>
   318c8:	ldr	x1, [sp, #24]
   318cc:	mov	x0, x25
   318d0:	mov	x2, x21
   318d4:	mov	x3, x28
   318d8:	bl	cc30 <__gmpn_add_n@plt>
   318dc:	cmp	x26, x28
   318e0:	str	x0, [x25, x28, lsl #3]
   318e4:	b.ge	31904 <__gmpn_matrix22_mul@@Base+0x6bc>  // b.tcont
   318e8:	ldr	x21, [sp, #40]
   318ec:	ldur	x0, [x29, #-8]
   318f0:	ldur	x2, [x29, #-32]
   318f4:	mov	x1, x25
   318f8:	mov	x3, x21
   318fc:	mov	x4, x27
   31900:	b	3191c <__gmpn_matrix22_mul@@Base+0x6d4>
   31904:	ldr	x21, [sp, #40]
   31908:	ldur	x0, [x29, #-8]
   3190c:	ldur	x4, [x29, #-32]
   31910:	mov	x2, x27
   31914:	mov	x1, x21
   31918:	mov	x3, x25
   3191c:	bl	cea0 <__gmpn_mul@plt>
   31920:	ldr	w4, [sp, #12]
   31924:	mov	x0, x21
   31928:	mov	x1, x20
   3192c:	mov	w2, w24
   31930:	mov	x3, x22
   31934:	mov	x5, x23
   31938:	bl	31a2c <__gmpn_matrix22_mul@@Base+0x7e4>
   3193c:	mov	x0, x20
   31940:	cbz	w24, 31978 <__gmpn_matrix22_mul@@Base+0x730>
   31944:	ldur	x19, [x29, #-8]
   31948:	mov	x2, x20
   3194c:	mov	x3, x23
   31950:	mov	x1, x19
   31954:	bl	cc30 <__gmpn_add_n@plt>
   31958:	ldur	w8, [x29, #-16]
   3195c:	cbz	w8, 31994 <__gmpn_matrix22_mul@@Base+0x74c>
   31960:	ldur	x0, [x29, #-24]
   31964:	mov	x1, x19
   31968:	mov	x3, x23
   3196c:	mov	x2, x0
   31970:	bl	cc30 <__gmpn_add_n@plt>
   31974:	b	319a8 <__gmpn_matrix22_mul@@Base+0x760>
   31978:	ldur	x19, [x29, #-8]
   3197c:	mov	x2, x20
   31980:	mov	x3, x23
   31984:	mov	x1, x19
   31988:	bl	c420 <__gmpn_sub_n@plt>
   3198c:	ldur	w8, [x29, #-16]
   31990:	cbnz	w8, 31960 <__gmpn_matrix22_mul@@Base+0x718>
   31994:	ldur	x0, [x29, #-24]
   31998:	mov	x1, x19
   3199c:	mov	x3, x23
   319a0:	mov	x2, x0
   319a4:	bl	c420 <__gmpn_sub_n@plt>
   319a8:	ldp	x20, x19, [sp, #176]
   319ac:	ldp	x22, x21, [sp, #160]
   319b0:	ldp	x24, x23, [sp, #144]
   319b4:	ldp	x26, x25, [sp, #128]
   319b8:	ldp	x28, x27, [sp, #112]
   319bc:	ldp	x29, x30, [sp, #96]
   319c0:	add	sp, sp, #0xc0
   319c4:	ret
   319c8:	stp	x29, x30, [sp, #-16]!
   319cc:	mov	x8, x1
   319d0:	sub	x9, x3, #0x1
   319d4:	mov	x29, sp
   319d8:	add	x10, x9, #0x1
   319dc:	cmp	x10, #0x1
   319e0:	b.lt	31a00 <__gmpn_matrix22_mul@@Base+0x7b8>  // b.tstop
   319e4:	lsl	x10, x9, #3
   319e8:	ldr	x11, [x8, x10]
   319ec:	ldr	x10, [x2, x10]
   319f0:	sub	x9, x9, #0x1
   319f4:	cmp	x11, x10
   319f8:	b.eq	319d8 <__gmpn_matrix22_mul@@Base+0x790>  // b.none
   319fc:	b.ls	31a14 <__gmpn_matrix22_mul@@Base+0x7cc>  // b.plast
   31a00:	mov	x1, x8
   31a04:	bl	c420 <__gmpn_sub_n@plt>
   31a08:	mov	w0, wzr
   31a0c:	ldp	x29, x30, [sp], #16
   31a10:	ret
   31a14:	mov	x1, x2
   31a18:	mov	x2, x8
   31a1c:	bl	c420 <__gmpn_sub_n@plt>
   31a20:	mov	w0, #0x1                   	// #1
   31a24:	ldp	x29, x30, [sp], #16
   31a28:	ret
   31a2c:	stp	x29, x30, [sp, #-32]!
   31a30:	str	x19, [sp, #16]
   31a34:	mov	w19, w2
   31a38:	cmp	w2, w4
   31a3c:	mov	x29, sp
   31a40:	b.ne	31a54 <__gmpn_matrix22_mul@@Base+0x80c>  // b.any
   31a44:	mov	x2, x3
   31a48:	mov	x3, x5
   31a4c:	bl	cc30 <__gmpn_add_n@plt>
   31a50:	b	31a64 <__gmpn_matrix22_mul@@Base+0x81c>
   31a54:	mov	x2, x3
   31a58:	mov	x3, x5
   31a5c:	bl	319c8 <__gmpn_matrix22_mul@@Base+0x780>
   31a60:	eor	w19, w0, w19
   31a64:	mov	w0, w19
   31a68:	ldr	x19, [sp, #16]
   31a6c:	ldp	x29, x30, [sp], #32
   31a70:	ret

0000000000031a74 <__gmpn_matrix22_mul1_inverse_vector@@Base>:
   31a74:	stp	x29, x30, [sp, #-64]!
   31a78:	stp	x22, x21, [sp, #32]
   31a7c:	stp	x20, x19, [sp, #48]
   31a80:	mov	x20, x3
   31a84:	ldr	x3, [x0, #24]
   31a88:	str	x23, [sp, #16]
   31a8c:	mov	x21, x2
   31a90:	mov	x22, x0
   31a94:	mov	x23, x1
   31a98:	mov	x0, x1
   31a9c:	mov	x1, x2
   31aa0:	mov	x2, x4
   31aa4:	mov	x29, sp
   31aa8:	mov	x19, x4
   31aac:	bl	d670 <__gmpn_mul_1@plt>
   31ab0:	ldr	x3, [x22, #8]
   31ab4:	mov	x0, x23
   31ab8:	mov	x1, x20
   31abc:	mov	x2, x19
   31ac0:	bl	cba0 <__gmpn_submul_1@plt>
   31ac4:	ldr	x3, [x22]
   31ac8:	mov	x0, x20
   31acc:	mov	x1, x20
   31ad0:	mov	x2, x19
   31ad4:	bl	d670 <__gmpn_mul_1@plt>
   31ad8:	ldr	x3, [x22, #16]
   31adc:	mov	x0, x20
   31ae0:	mov	x1, x21
   31ae4:	mov	x2, x19
   31ae8:	bl	cba0 <__gmpn_submul_1@plt>
   31aec:	lsl	x8, x19, #3
   31af0:	sub	x8, x8, #0x8
   31af4:	ldr	x9, [x23, x8]
   31af8:	ldr	x8, [x20, x8]
   31afc:	ldp	x22, x21, [sp, #32]
   31b00:	ldr	x23, [sp, #16]
   31b04:	orr	x8, x8, x9
   31b08:	cmp	x8, #0x0
   31b0c:	cset	w8, eq  // eq = none
   31b10:	sub	x0, x19, x8
   31b14:	ldp	x20, x19, [sp, #48]
   31b18:	ldp	x29, x30, [sp], #64
   31b1c:	ret

0000000000031b20 <__gmpn_hgcd_matrix_init@@Base>:
   31b20:	stp	x29, x30, [sp, #-48]!
   31b24:	add	x8, x1, #0x1
   31b28:	add	x9, x1, #0x2
   31b2c:	cmp	x8, #0x0
   31b30:	csinc	x8, x9, x1, lt  // lt = tstop
   31b34:	asr	x8, x8, #1
   31b38:	str	x21, [sp, #16]
   31b3c:	stp	x20, x19, [sp, #32]
   31b40:	mov	x19, x2
   31b44:	mov	x20, x0
   31b48:	mov	w10, #0x1                   	// #1
   31b4c:	adds	x21, x8, #0x1
   31b50:	mov	x29, sp
   31b54:	stp	x21, x10, [x0]
   31b58:	b.cs	31b70 <__gmpn_hgcd_matrix_init@@Base+0x50>  // b.hs, b.nlast
   31b5c:	lsl	x8, x8, #5
   31b60:	add	x2, x8, #0x20
   31b64:	mov	x0, x19
   31b68:	mov	w1, wzr
   31b6c:	bl	c780 <memset@plt>
   31b70:	add	x8, x19, x21, lsl #3
   31b74:	mov	w10, #0x18                  	// #24
   31b78:	stp	x19, x8, [x20, #16]
   31b7c:	mul	x8, x21, x10
   31b80:	add	x9, x19, x21, lsl #4
   31b84:	mov	w11, #0x1                   	// #1
   31b88:	add	x10, x19, x8
   31b8c:	stp	x9, x10, [x20, #32]
   31b90:	str	x11, [x19, x8]
   31b94:	str	x11, [x19]
   31b98:	ldp	x20, x19, [sp, #32]
   31b9c:	ldr	x21, [sp, #16]
   31ba0:	ldp	x29, x30, [sp], #48
   31ba4:	ret

0000000000031ba8 <__gmpn_hgcd_matrix_update_q@@Base>:
   31ba8:	sub	sp, sp, #0x90
   31bac:	mov	x14, x1
   31bb0:	cmp	x2, #0x1
   31bb4:	stp	x29, x30, [sp, #48]
   31bb8:	stp	x28, x27, [sp, #64]
   31bbc:	stp	x26, x25, [sp, #80]
   31bc0:	stp	x24, x23, [sp, #96]
   31bc4:	stp	x22, x21, [sp, #112]
   31bc8:	stp	x20, x19, [sp, #128]
   31bcc:	add	x29, sp, #0x30
   31bd0:	b.ne	31c54 <__gmpn_hgcd_matrix_update_q@@Base+0xac>  // b.any
   31bd4:	mov	w8, w3
   31bd8:	mov	w9, #0x1                   	// #1
   31bdc:	lsl	x22, x8, #3
   31be0:	sub	w8, w9, w3
   31be4:	ldr	x20, [x14]
   31be8:	add	x19, x0, #0x10
   31bec:	mov	x24, x0
   31bf0:	lsl	x21, x8, #3
   31bf4:	ldr	x0, [x19, x22]
   31bf8:	ldr	x1, [x19, x21]
   31bfc:	ldr	x2, [x24, #8]
   31c00:	mov	x3, x20
   31c04:	bl	d5e0 <__gmpn_addmul_1@plt>
   31c08:	add	x23, x24, #0x20
   31c0c:	ldr	x8, [x23, x22]
   31c10:	ldr	x1, [x23, x21]
   31c14:	ldr	x2, [x24, #8]
   31c18:	mov	x21, x0
   31c1c:	mov	x0, x8
   31c20:	mov	x3, x20
   31c24:	bl	d5e0 <__gmpn_addmul_1@plt>
   31c28:	ldr	x8, [x19, x22]
   31c2c:	ldr	x9, [x24, #8]
   31c30:	str	x21, [x8, x9, lsl #3]
   31c34:	ldr	x8, [x23, x22]
   31c38:	ldr	x9, [x24, #8]
   31c3c:	str	x0, [x8, x9, lsl #3]
   31c40:	ldr	x8, [x24, #8]
   31c44:	orr	x9, x0, x21
   31c48:	cmp	x9, #0x0
   31c4c:	cinc	x8, x8, ne  // ne = any
   31c50:	b	31dc8 <__gmpn_hgcd_matrix_update_q@@Base+0x220>
   31c54:	ldr	x8, [x0, #8]
   31c58:	mov	w9, #0x1                   	// #1
   31c5c:	sub	w9, w9, w3
   31c60:	add	x10, x0, w9, uxtw #3
   31c64:	add	x11, x8, x2
   31c68:	lsl	x11, x11, #3
   31c6c:	mov	x23, x4
   31c70:	mov	x20, x2
   31c74:	add	x9, x10, #0x10
   31c78:	add	x10, x10, #0x20
   31c7c:	sub	x11, x11, #0x8
   31c80:	mov	x12, x8
   31c84:	add	x25, x20, x12
   31c88:	mov	x22, x12
   31c8c:	cmp	x25, x8
   31c90:	mov	x26, x11
   31c94:	b.le	31cc0 <__gmpn_hgcd_matrix_update_q@@Base+0x118>
   31c98:	ldr	x11, [x9]
   31c9c:	add	x11, x11, x22, lsl #3
   31ca0:	ldur	x11, [x11, #-8]
   31ca4:	cbnz	x11, 31cc0 <__gmpn_hgcd_matrix_update_q@@Base+0x118>
   31ca8:	ldr	x11, [x10]
   31cac:	sub	x12, x22, #0x1
   31cb0:	add	x11, x11, x22, lsl #3
   31cb4:	ldur	x13, [x11, #-8]
   31cb8:	sub	x11, x26, #0x8
   31cbc:	cbz	x13, 31c84 <__gmpn_hgcd_matrix_update_q@@Base+0xdc>
   31cc0:	mov	w8, #0x1                   	// #1
   31cc4:	sub	w8, w8, w3
   31cc8:	add	x9, x0, w3, uxtw #3
   31ccc:	add	x8, x0, w8, uxtw #3
   31cd0:	mov	x28, xzr
   31cd4:	add	x27, x9, #0x10
   31cd8:	add	x21, x8, #0x10
   31cdc:	sub	x19, x29, #0x10
   31ce0:	mov	x24, x14
   31ce4:	str	w3, [sp, #20]
   31ce8:	str	x9, [sp, #8]
   31cec:	str	x0, [sp, #24]
   31cf0:	b	31d34 <__gmpn_hgcd_matrix_update_q@@Base+0x18c>
   31cf4:	mov	x1, x14
   31cf8:	mov	x2, x20
   31cfc:	mov	x4, x22
   31d00:	bl	cea0 <__gmpn_mul@plt>
   31d04:	ldr	x8, [sp, #24]
   31d08:	ldr	x0, [x27, x28]
   31d0c:	mov	x1, x23
   31d10:	mov	x2, x25
   31d14:	ldr	x4, [x8, #8]
   31d18:	mov	x3, x0
   31d1c:	bl	c970 <__gmpn_add@plt>
   31d20:	add	x28, x28, #0x10
   31d24:	cmp	x28, #0x20
   31d28:	str	x0, [x19], #8
   31d2c:	mov	x14, x24
   31d30:	b.eq	31d58 <__gmpn_hgcd_matrix_update_q@@Base+0x1b0>  // b.none
   31d34:	ldr	x3, [x21, x28]
   31d38:	mov	x0, x23
   31d3c:	cmp	x22, x20
   31d40:	b.lt	31cf4 <__gmpn_hgcd_matrix_update_q@@Base+0x14c>  // b.tstop
   31d44:	mov	x1, x3
   31d48:	mov	x2, x22
   31d4c:	mov	x3, x14
   31d50:	mov	x4, x20
   31d54:	b	31d00 <__gmpn_hgcd_matrix_update_q@@Base+0x158>
   31d58:	ldr	x10, [sp, #8]
   31d5c:	ldp	x9, x8, [x29, #-16]
   31d60:	ldr	x11, [x10, #16]
   31d64:	ldr	w10, [sp, #20]
   31d68:	orr	x12, x8, x9
   31d6c:	mov	w10, w10
   31d70:	cbz	x12, 31d98 <__gmpn_hgcd_matrix_update_q@@Base+0x1f0>
   31d74:	ldr	x24, [sp, #24]
   31d78:	add	x11, x11, x26
   31d7c:	str	x9, [x11, #8]
   31d80:	add	x10, x24, x10, lsl #3
   31d84:	ldr	x9, [x10, #32]
   31d88:	add	x9, x9, x20, lsl #3
   31d8c:	str	x8, [x9, x22, lsl #3]
   31d90:	mov	w8, #0x1                   	// #1
   31d94:	b	31dc0 <__gmpn_hgcd_matrix_update_q@@Base+0x218>
   31d98:	ldr	x24, [sp, #24]
   31d9c:	ldr	x9, [x11, x26]
   31da0:	add	x8, x24, x10, lsl #3
   31da4:	ldr	x8, [x8, #32]
   31da8:	add	x8, x8, x20, lsl #3
   31dac:	add	x8, x8, x22, lsl #3
   31db0:	ldur	x8, [x8, #-8]
   31db4:	orr	x8, x8, x9
   31db8:	cmp	x8, #0x0
   31dbc:	csetm	x8, eq  // eq = none
   31dc0:	add	x8, x8, x20
   31dc4:	add	x8, x8, x22
   31dc8:	str	x8, [x24, #8]
   31dcc:	ldp	x20, x19, [sp, #128]
   31dd0:	ldp	x22, x21, [sp, #112]
   31dd4:	ldp	x24, x23, [sp, #96]
   31dd8:	ldp	x26, x25, [sp, #80]
   31ddc:	ldp	x28, x27, [sp, #64]
   31de0:	ldp	x29, x30, [sp, #48]
   31de4:	add	sp, sp, #0x90
   31de8:	ret

0000000000031dec <__gmpn_hgcd_matrix_mul_1@@Base>:
   31dec:	stp	x29, x30, [sp, #-48]!
   31df0:	stp	x22, x21, [sp, #16]
   31df4:	stp	x20, x19, [sp, #32]
   31df8:	mov	x19, x2
   31dfc:	ldp	x2, x8, [x0, #8]
   31e00:	mov	x20, x0
   31e04:	mov	x21, x1
   31e08:	mov	x0, x19
   31e0c:	mov	x1, x8
   31e10:	mov	x29, sp
   31e14:	bl	cc10 <__gmpn_copyi@plt>
   31e18:	ldp	x1, x3, [x20, #16]
   31e1c:	ldr	x4, [x20, #8]
   31e20:	mov	x0, x21
   31e24:	mov	x2, x19
   31e28:	bl	d620 <__gmpn_hgcd_mul_matrix1_vector@plt>
   31e2c:	ldr	x1, [x20, #32]
   31e30:	ldr	x2, [x20, #8]
   31e34:	mov	x22, x0
   31e38:	mov	x0, x19
   31e3c:	bl	cc10 <__gmpn_copyi@plt>
   31e40:	ldp	x1, x3, [x20, #32]
   31e44:	ldr	x4, [x20, #8]
   31e48:	mov	x0, x21
   31e4c:	mov	x2, x19
   31e50:	bl	d620 <__gmpn_hgcd_mul_matrix1_vector@plt>
   31e54:	cmp	x22, x0
   31e58:	csel	x8, x22, x0, gt
   31e5c:	str	x8, [x20, #8]
   31e60:	ldp	x20, x19, [sp, #32]
   31e64:	ldp	x22, x21, [sp, #16]
   31e68:	ldp	x29, x30, [sp], #48
   31e6c:	ret

0000000000031e70 <__gmpn_hgcd_matrix_mul@@Base>:
   31e70:	sub	sp, sp, #0x40
   31e74:	stp	x29, x30, [sp, #32]
   31e78:	stp	x20, x19, [sp, #48]
   31e7c:	mov	x20, x1
   31e80:	mov	x19, x0
   31e84:	ldp	x1, x8, [x0, #24]
   31e88:	ldp	x10, x5, [x20, #8]
   31e8c:	ldr	x3, [x0, #40]
   31e90:	ldr	x0, [x0, #16]
   31e94:	ldr	x4, [x19, #8]
   31e98:	ldp	x6, x7, [x20, #24]
   31e9c:	ldr	x9, [x20, #40]
   31ea0:	stp	x10, x2, [sp, #8]
   31ea4:	mov	x2, x8
   31ea8:	add	x29, sp, #0x20
   31eac:	str	x9, [sp]
   31eb0:	bl	c140 <__gmpn_matrix22_mul@plt>
   31eb4:	ldr	x8, [x20, #8]
   31eb8:	ldp	x9, x10, [x19, #8]
   31ebc:	ldp	x11, x12, [x19, #24]
   31ec0:	ldr	x13, [x19, #40]
   31ec4:	add	x8, x8, x9
   31ec8:	lsl	x9, x8, #3
   31ecc:	ldr	x14, [x10, x9]
   31ed0:	ldr	x15, [x11, x9]
   31ed4:	ldr	x16, [x12, x9]
   31ed8:	ldr	x9, [x13, x9]
   31edc:	orr	x14, x15, x14
   31ee0:	orr	x14, x14, x16
   31ee4:	orr	x9, x14, x9
   31ee8:	cmp	x9, #0x0
   31eec:	cset	w9, eq  // eq = none
   31ef0:	sub	x8, x8, x9
   31ef4:	lsl	x9, x8, #3
   31ef8:	ldr	x14, [x10, x9]
   31efc:	ldr	x15, [x11, x9]
   31f00:	ldr	x16, [x12, x9]
   31f04:	ldr	x9, [x13, x9]
   31f08:	orr	x14, x15, x14
   31f0c:	orr	x14, x14, x16
   31f10:	orr	x9, x14, x9
   31f14:	cmp	x9, #0x0
   31f18:	cset	w9, eq  // eq = none
   31f1c:	sub	x8, x8, x9
   31f20:	lsl	x9, x8, #3
   31f24:	ldr	x10, [x10, x9]
   31f28:	ldr	x11, [x11, x9]
   31f2c:	ldr	x12, [x12, x9]
   31f30:	ldr	x9, [x13, x9]
   31f34:	orr	x10, x11, x10
   31f38:	orr	x10, x10, x12
   31f3c:	orr	x9, x10, x9
   31f40:	cmp	x9, #0x0
   31f44:	cset	w9, eq  // eq = none
   31f48:	sub	x8, x8, x9
   31f4c:	add	x8, x8, #0x1
   31f50:	str	x8, [x19, #8]
   31f54:	ldp	x20, x19, [sp, #48]
   31f58:	ldp	x29, x30, [sp, #32]
   31f5c:	add	sp, sp, #0x40
   31f60:	ret

0000000000031f64 <__gmpn_hgcd_matrix_adjust@@Base>:
   31f64:	stp	x29, x30, [sp, #-96]!
   31f68:	stp	x28, x27, [sp, #16]
   31f6c:	stp	x26, x25, [sp, #32]
   31f70:	stp	x24, x23, [sp, #48]
   31f74:	stp	x22, x21, [sp, #64]
   31f78:	stp	x20, x19, [sp, #80]
   31f7c:	mov	x22, x4
   31f80:	ldr	x4, [x0, #8]
   31f84:	mov	x20, x3
   31f88:	ldr	x3, [x0, #40]
   31f8c:	add	x26, x5, x22, lsl #3
   31f90:	mov	x25, x5
   31f94:	mov	x21, x2
   31f98:	mov	x19, x1
   31f9c:	mov	x23, x0
   31fa0:	cmp	x4, x22
   31fa4:	add	x24, x26, x4, lsl #3
   31fa8:	mov	x0, x5
   31fac:	mov	x29, sp
   31fb0:	b.ge	31fd8 <__gmpn_hgcd_matrix_adjust@@Base+0x74>  // b.tcont
   31fb4:	mov	x1, x21
   31fb8:	mov	x2, x22
   31fbc:	bl	cea0 <__gmpn_mul@plt>
   31fc0:	ldr	x3, [x23, #32]
   31fc4:	ldr	x4, [x23, #8]
   31fc8:	mov	x0, x24
   31fcc:	mov	x1, x21
   31fd0:	mov	x2, x22
   31fd4:	b	32000 <__gmpn_hgcd_matrix_adjust@@Base+0x9c>
   31fd8:	mov	x1, x3
   31fdc:	mov	x2, x4
   31fe0:	mov	x3, x21
   31fe4:	mov	x4, x22
   31fe8:	bl	cea0 <__gmpn_mul@plt>
   31fec:	ldr	x1, [x23, #32]
   31ff0:	ldr	x2, [x23, #8]
   31ff4:	mov	x0, x24
   31ff8:	mov	x3, x21
   31ffc:	mov	x4, x22
   32000:	bl	cea0 <__gmpn_mul@plt>
   32004:	mov	x0, x21
   32008:	mov	x1, x25
   3200c:	mov	x2, x22
   32010:	bl	cc10 <__gmpn_copyi@plt>
   32014:	ldr	x4, [x23, #8]
   32018:	add	x0, x21, x22, lsl #3
   3201c:	sub	x27, x19, x22
   32020:	mov	x1, x0
   32024:	mov	x2, x27
   32028:	mov	x3, x26
   3202c:	bl	c970 <__gmpn_add@plt>
   32030:	ldr	x4, [x23, #8]
   32034:	ldr	x3, [x23, #24]
   32038:	mov	x28, x0
   3203c:	mov	x0, x25
   32040:	cmp	x4, x22
   32044:	b.ge	32054 <__gmpn_hgcd_matrix_adjust@@Base+0xf0>  // b.tcont
   32048:	mov	x1, x20
   3204c:	mov	x2, x22
   32050:	b	32064 <__gmpn_hgcd_matrix_adjust@@Base+0x100>
   32054:	mov	x1, x3
   32058:	mov	x2, x4
   3205c:	mov	x3, x20
   32060:	mov	x4, x22
   32064:	bl	cea0 <__gmpn_mul@plt>
   32068:	ldr	x8, [x23, #8]
   3206c:	mov	x0, x21
   32070:	mov	x1, x21
   32074:	mov	x2, x19
   32078:	add	x4, x8, x22
   3207c:	mov	x3, x25
   32080:	bl	d340 <__gmpn_sub@plt>
   32084:	ldp	x4, x3, [x23, #8]
   32088:	sub	x28, x28, x0
   3208c:	mov	x0, x25
   32090:	cmp	x4, x22
   32094:	b.ge	320a4 <__gmpn_hgcd_matrix_adjust@@Base+0x140>  // b.tcont
   32098:	mov	x1, x20
   3209c:	mov	x2, x22
   320a0:	b	320b4 <__gmpn_hgcd_matrix_adjust@@Base+0x150>
   320a4:	mov	x1, x3
   320a8:	mov	x2, x4
   320ac:	mov	x3, x20
   320b0:	mov	x4, x22
   320b4:	bl	cea0 <__gmpn_mul@plt>
   320b8:	mov	x0, x20
   320bc:	mov	x1, x25
   320c0:	mov	x2, x22
   320c4:	bl	cc10 <__gmpn_copyi@plt>
   320c8:	ldr	x4, [x23, #8]
   320cc:	add	x0, x20, x22, lsl #3
   320d0:	mov	x1, x0
   320d4:	mov	x2, x27
   320d8:	mov	x3, x26
   320dc:	bl	c970 <__gmpn_add@plt>
   320e0:	ldr	x8, [x23, #8]
   320e4:	mov	x25, x0
   320e8:	mov	x0, x20
   320ec:	mov	x1, x20
   320f0:	add	x4, x8, x22
   320f4:	mov	x2, x19
   320f8:	mov	x3, x24
   320fc:	bl	d340 <__gmpn_sub@plt>
   32100:	sub	x8, x25, x0
   32104:	orr	x9, x8, x28
   32108:	cbz	x9, 32120 <__gmpn_hgcd_matrix_adjust@@Base+0x1bc>
   3210c:	lsl	x9, x19, #3
   32110:	add	x19, x19, #0x1
   32114:	str	x28, [x21, x9]
   32118:	str	x8, [x20, x9]
   3211c:	b	3212c <__gmpn_hgcd_matrix_adjust@@Base+0x1c8>
   32120:	sub	x8, x19, #0x1
   32124:	ldr	x9, [x21, x8, lsl #3]
   32128:	cbz	x9, 3214c <__gmpn_hgcd_matrix_adjust@@Base+0x1e8>
   3212c:	mov	x0, x19
   32130:	ldp	x20, x19, [sp, #80]
   32134:	ldp	x22, x21, [sp, #64]
   32138:	ldp	x24, x23, [sp, #48]
   3213c:	ldp	x26, x25, [sp, #32]
   32140:	ldp	x28, x27, [sp, #16]
   32144:	ldp	x29, x30, [sp], #96
   32148:	ret
   3214c:	ldr	x9, [x20, x8, lsl #3]
   32150:	cmp	x9, #0x0
   32154:	csel	x0, x8, x19, eq  // eq = none
   32158:	b	32130 <__gmpn_hgcd_matrix_adjust@@Base+0x1cc>

000000000003215c <__gmpn_hgcd2@@Base>:
   3215c:	sub	sp, sp, #0x70
   32160:	stp	x22, x21, [sp, #80]
   32164:	mov	x21, x0
   32168:	cmp	x0, #0x2
   3216c:	mov	w0, wzr
   32170:	stp	x29, x30, [sp, #16]
   32174:	str	x27, [sp, #32]
   32178:	stp	x26, x25, [sp, #48]
   3217c:	stp	x24, x23, [sp, #64]
   32180:	stp	x20, x19, [sp, #96]
   32184:	add	x29, sp, #0x10
   32188:	b.cc	32398 <__gmpn_hgcd2@@Base+0x23c>  // b.lo, b.ul, b.last
   3218c:	mov	x20, x2
   32190:	cmp	x2, #0x2
   32194:	b.cc	32398 <__gmpn_hgcd2@@Base+0x23c>  // b.lo, b.ul, b.last
   32198:	mov	x19, x4
   3219c:	mov	x22, x3
   321a0:	mov	x23, x1
   321a4:	cmp	x21, x20
   321a8:	b.hi	321b8 <__gmpn_hgcd2@@Base+0x5c>  // b.pmore
   321ac:	b.ne	321d8 <__gmpn_hgcd2@@Base+0x7c>  // b.any
   321b0:	cmp	x23, x22
   321b4:	b.ls	321d8 <__gmpn_hgcd2@@Base+0x7c>  // b.plast
   321b8:	subs	x8, x23, x22
   321bc:	sbc	x21, x21, x20
   321c0:	cmp	x21, #0x2
   321c4:	b.cc	321e8 <__gmpn_hgcd2@@Base+0x8c>  // b.lo, b.ul, b.last
   321c8:	mov	x24, xzr
   321cc:	mov	w25, #0x1                   	// #1
   321d0:	mov	x23, x8
   321d4:	b	321fc <__gmpn_hgcd2@@Base+0xa0>
   321d8:	subs	x8, x22, x23
   321dc:	sbc	x20, x20, x21
   321e0:	cmp	x20, #0x2
   321e4:	b.cs	321f0 <__gmpn_hgcd2@@Base+0x94>  // b.hs, b.nlast
   321e8:	mov	w0, wzr
   321ec:	b	32398 <__gmpn_hgcd2@@Base+0x23c>
   321f0:	mov	x25, xzr
   321f4:	mov	w24, #0x1                   	// #1
   321f8:	mov	x22, x8
   321fc:	mov	w26, #0x1                   	// #1
   32200:	cmp	x21, x20
   32204:	mov	w27, #0x1                   	// #1
   32208:	b.cc	3227c <__gmpn_hgcd2@@Base+0x120>  // b.lo, b.ul, b.last
   3220c:	cmp	x21, x20
   32210:	b.eq	3238c <__gmpn_hgcd2@@Base+0x230>  // b.none
   32214:	lsr	x8, x21, #32
   32218:	cbz	x8, 322f0 <__gmpn_hgcd2@@Base+0x194>
   3221c:	mov	x8, x23
   32220:	subs	x23, x8, x22
   32224:	sbc	x21, x21, x20
   32228:	cmp	x21, #0x2
   3222c:	b.cc	3238c <__gmpn_hgcd2@@Base+0x230>  // b.lo, b.ul, b.last
   32230:	cmp	x21, x20
   32234:	b.ls	32274 <__gmpn_hgcd2@@Base+0x118>  // b.plast
   32238:	mov	x0, sp
   3223c:	mov	x1, x21
   32240:	mov	x2, x23
   32244:	mov	x3, x20
   32248:	mov	x4, x22
   3224c:	bl	323c4 <__gmpn_hgcd2@@Base+0x268>
   32250:	ldr	x21, [sp, #8]
   32254:	cmp	x21, #0x1
   32258:	cinc	x8, x0, hi  // hi = pmore
   3225c:	cmp	x21, #0x2
   32260:	madd	x27, x8, x24, x27
   32264:	madd	x25, x8, x26, x25
   32268:	b.cc	3238c <__gmpn_hgcd2@@Base+0x230>  // b.lo, b.ul, b.last
   3226c:	ldr	x23, [sp]
   32270:	b	3227c <__gmpn_hgcd2@@Base+0x120>
   32274:	add	x25, x25, x26
   32278:	add	x27, x27, x24
   3227c:	cmp	x21, x20
   32280:	b.eq	3238c <__gmpn_hgcd2@@Base+0x230>  // b.none
   32284:	lsr	x8, x20, #32
   32288:	cbz	x8, 322fc <__gmpn_hgcd2@@Base+0x1a0>
   3228c:	mov	x8, x22
   32290:	subs	x22, x8, x23
   32294:	sbc	x20, x20, x21
   32298:	cmp	x20, #0x2
   3229c:	b.cc	3238c <__gmpn_hgcd2@@Base+0x230>  // b.lo, b.ul, b.last
   322a0:	cmp	x20, x21
   322a4:	b.ls	322e4 <__gmpn_hgcd2@@Base+0x188>  // b.plast
   322a8:	mov	x0, sp
   322ac:	mov	x1, x20
   322b0:	mov	x2, x22
   322b4:	mov	x3, x21
   322b8:	mov	x4, x23
   322bc:	bl	323c4 <__gmpn_hgcd2@@Base+0x268>
   322c0:	ldr	x20, [sp, #8]
   322c4:	cmp	x20, #0x1
   322c8:	cinc	x8, x0, hi  // hi = pmore
   322cc:	cmp	x20, #0x2
   322d0:	madd	x24, x8, x27, x24
   322d4:	madd	x26, x8, x25, x26
   322d8:	b.cc	3238c <__gmpn_hgcd2@@Base+0x230>  // b.lo, b.ul, b.last
   322dc:	ldr	x22, [sp]
   322e0:	b	3220c <__gmpn_hgcd2@@Base+0xb0>
   322e4:	add	x26, x25, x26
   322e8:	add	x24, x27, x24
   322ec:	b	3220c <__gmpn_hgcd2@@Base+0xb0>
   322f0:	extr	x21, x21, x23, #32
   322f4:	extr	x20, x20, x22, #32
   322f8:	b	32308 <__gmpn_hgcd2@@Base+0x1ac>
   322fc:	extr	x21, x21, x23, #32
   32300:	extr	x20, x20, x22, #32
   32304:	b	32350 <__gmpn_hgcd2@@Base+0x1f4>
   32308:	sub	x21, x21, x20
   3230c:	lsr	x8, x21, #33
   32310:	cbz	x8, 3238c <__gmpn_hgcd2@@Base+0x230>
   32314:	cmp	x21, x20
   32318:	b.ls	32348 <__gmpn_hgcd2@@Base+0x1ec>  // b.plast
   3231c:	mov	x0, x21
   32320:	mov	x1, x20
   32324:	bl	32448 <__gmpn_hgcd2@@Base+0x2ec>
   32328:	lsr	x8, x0, #33
   3232c:	cmp	x8, #0x0
   32330:	cinc	x9, x1, ne  // ne = any
   32334:	mov	x21, x0
   32338:	madd	x27, x9, x24, x27
   3233c:	madd	x25, x9, x26, x25
   32340:	cbnz	x8, 32350 <__gmpn_hgcd2@@Base+0x1f4>
   32344:	b	3238c <__gmpn_hgcd2@@Base+0x230>
   32348:	add	x25, x25, x26
   3234c:	add	x27, x27, x24
   32350:	sub	x20, x20, x21
   32354:	lsr	x8, x20, #33
   32358:	cbz	x8, 3238c <__gmpn_hgcd2@@Base+0x230>
   3235c:	cmp	x20, x21
   32360:	b.ls	323b8 <__gmpn_hgcd2@@Base+0x25c>  // b.plast
   32364:	mov	x0, x20
   32368:	mov	x1, x21
   3236c:	bl	32448 <__gmpn_hgcd2@@Base+0x2ec>
   32370:	lsr	x8, x0, #33
   32374:	cmp	x8, #0x0
   32378:	cinc	x9, x1, ne  // ne = any
   3237c:	mov	x20, x0
   32380:	madd	x24, x9, x27, x24
   32384:	madd	x26, x9, x25, x26
   32388:	cbnz	x8, 32308 <__gmpn_hgcd2@@Base+0x1ac>
   3238c:	mov	w0, #0x1                   	// #1
   32390:	stp	x26, x25, [x19]
   32394:	stp	x24, x27, [x19, #16]
   32398:	ldp	x20, x19, [sp, #96]
   3239c:	ldp	x22, x21, [sp, #80]
   323a0:	ldp	x24, x23, [sp, #64]
   323a4:	ldp	x26, x25, [sp, #48]
   323a8:	ldr	x27, [sp, #32]
   323ac:	ldp	x29, x30, [sp, #16]
   323b0:	add	sp, sp, #0x70
   323b4:	ret
   323b8:	add	x26, x25, x26
   323bc:	add	x24, x27, x24
   323c0:	b	32308 <__gmpn_hgcd2@@Base+0x1ac>
   323c4:	clz	x11, x1
   323c8:	clz	x10, x3
   323cc:	mov	w9, #0x3f                  	// #63
   323d0:	sub	w12, w10, w11
   323d4:	lsl	x13, x3, x12
   323d8:	sub	w14, w9, w12
   323dc:	lsl	x9, x4, x12
   323e0:	lsr	x12, x4, #1
   323e4:	mvn	w15, w10
   323e8:	lsr	x10, x12, x14
   323ec:	mov	x8, xzr
   323f0:	add	x10, x10, x13
   323f4:	add	w11, w15, w11
   323f8:	cmp	x2, x9
   323fc:	cset	w12, cs  // cs = hs, nlast
   32400:	cmp	x1, x10
   32404:	cset	w13, hi  // hi = pmore
   32408:	csel	w12, w12, w13, eq  // eq = none
   3240c:	sbfx	x13, x12, #0, #1
   32410:	bfi	x12, x8, #1, #63
   32414:	mov	x8, x12
   32418:	and	x12, x9, x13
   3241c:	and	x13, x10, x13
   32420:	subs	x14, x2, x12
   32424:	sbc	x1, x1, x13
   32428:	extr	x9, x10, x9, #1
   3242c:	mov	x2, x14
   32430:	adds	w11, w11, #0x1
   32434:	lsr	x10, x10, #1
   32438:	b.cc	323f8 <__gmpn_hgcd2@@Base+0x29c>  // b.lo, b.ul, b.last
   3243c:	stp	x2, x1, [x0]
   32440:	mov	x0, x8
   32444:	ret
   32448:	udiv	x8, x0, x1
   3244c:	msub	x0, x8, x1, x0
   32450:	mov	x1, x8
   32454:	ret

0000000000032458 <__gmpn_hgcd_mul_matrix1_vector@@Base>:
   32458:	stp	x29, x30, [sp, #-64]!
   3245c:	stp	x24, x23, [sp, #16]
   32460:	stp	x22, x21, [sp, #32]
   32464:	stp	x20, x19, [sp, #48]
   32468:	mov	x20, x3
   3246c:	ldr	x3, [x0]
   32470:	mov	x21, x2
   32474:	mov	x22, x0
   32478:	mov	x23, x1
   3247c:	mov	x0, x1
   32480:	mov	x1, x2
   32484:	mov	x2, x4
   32488:	mov	x29, sp
   3248c:	mov	x19, x4
   32490:	bl	d670 <__gmpn_mul_1@plt>
   32494:	ldr	x3, [x22, #16]
   32498:	mov	x24, x0
   3249c:	mov	x0, x23
   324a0:	mov	x1, x20
   324a4:	mov	x2, x19
   324a8:	bl	d5e0 <__gmpn_addmul_1@plt>
   324ac:	ldr	x3, [x22, #24]
   324b0:	add	x24, x0, x24
   324b4:	mov	x0, x20
   324b8:	mov	x1, x20
   324bc:	mov	x2, x19
   324c0:	bl	d670 <__gmpn_mul_1@plt>
   324c4:	ldr	x3, [x22, #8]
   324c8:	mov	x22, x0
   324cc:	mov	x0, x20
   324d0:	mov	x1, x21
   324d4:	mov	x2, x19
   324d8:	bl	d5e0 <__gmpn_addmul_1@plt>
   324dc:	add	x8, x0, x22
   324e0:	lsl	x9, x19, #3
   324e4:	orr	x10, x8, x24
   324e8:	str	x24, [x23, x9]
   324ec:	cmp	x10, #0x0
   324f0:	str	x8, [x20, x9]
   324f4:	cinc	x0, x19, ne  // ne = any
   324f8:	ldp	x20, x19, [sp, #48]
   324fc:	ldp	x22, x21, [sp, #32]
   32500:	ldp	x24, x23, [sp, #16]
   32504:	ldp	x29, x30, [sp], #64
   32508:	ret

000000000003250c <__gmpn_hgcd_step@@Base>:
   3250c:	sub	sp, sp, #0x60
   32510:	lsl	x8, x0, #3
   32514:	stp	x29, x30, [sp, #32]
   32518:	stp	x24, x23, [sp, #48]
   3251c:	stp	x22, x21, [sp, #64]
   32520:	stp	x20, x19, [sp, #80]
   32524:	sub	x9, x8, #0x8
   32528:	mov	x21, x2
   3252c:	mov	x20, x0
   32530:	ldr	x0, [x1, x9]
   32534:	ldr	x2, [x2, x9]
   32538:	add	x9, x3, #0x1
   3253c:	mov	x19, x5
   32540:	mov	x23, x4
   32544:	mov	x22, x1
   32548:	mov	x24, x3
   3254c:	cmp	x9, x20
   32550:	orr	x9, x2, x0
   32554:	add	x29, sp, #0x20
   32558:	b.ne	32568 <__gmpn_hgcd_step@@Base+0x5c>  // b.any
   3255c:	cmp	x9, #0x4
   32560:	b.cs	325c0 <__gmpn_hgcd_step@@Base+0xb4>  // b.hs, b.nlast
   32564:	b	32614 <__gmpn_hgcd_step@@Base+0x108>
   32568:	tbnz	x9, #63, 325c0 <__gmpn_hgcd_step@@Base+0xb4>
   3256c:	sub	x10, x8, #0x10
   32570:	sub	x8, x8, #0x18
   32574:	ldr	x12, [x22, x10]
   32578:	ldr	x14, [x22, x8]
   3257c:	ldr	x10, [x21, x10]
   32580:	ldr	x8, [x21, x8]
   32584:	clz	x9, x9
   32588:	neg	w13, w9
   3258c:	lsl	x11, x0, x9
   32590:	lsl	x15, x2, x9
   32594:	lsr	x16, x12, x13
   32598:	lsl	x12, x12, x9
   3259c:	lsr	x14, x14, x13
   325a0:	lsl	x9, x10, x9
   325a4:	lsr	x10, x10, x13
   325a8:	lsr	x8, x8, x13
   325ac:	orr	x0, x16, x11
   325b0:	orr	x1, x14, x12
   325b4:	orr	x2, x10, x15
   325b8:	orr	x3, x8, x9
   325bc:	b	325cc <__gmpn_hgcd_step@@Base+0xc0>
   325c0:	sub	x8, x8, #0x10
   325c4:	ldr	x1, [x22, x8]
   325c8:	ldr	x3, [x21, x8]
   325cc:	mov	x4, sp
   325d0:	bl	c730 <__gmpn_hgcd2@plt>
   325d4:	cbz	w0, 32614 <__gmpn_hgcd_step@@Base+0x108>
   325d8:	mov	x1, sp
   325dc:	mov	x0, x23
   325e0:	mov	x2, x19
   325e4:	bl	c920 <__gmpn_hgcd_matrix_mul_1@plt>
   325e8:	mov	x0, x19
   325ec:	mov	x1, x22
   325f0:	mov	x2, x20
   325f4:	bl	cc10 <__gmpn_copyi@plt>
   325f8:	mov	x0, sp
   325fc:	mov	x1, x22
   32600:	mov	x2, x19
   32604:	mov	x3, x21
   32608:	mov	x4, x20
   3260c:	bl	c660 <__gmpn_matrix22_mul1_inverse_vector@plt>
   32610:	b	32638 <__gmpn_hgcd_step@@Base+0x12c>
   32614:	adrp	x4, 32000 <__gmpn_hgcd_matrix_adjust@@Base+0x9c>
   32618:	add	x4, x4, #0x650
   3261c:	mov	x0, x22
   32620:	mov	x1, x21
   32624:	mov	x2, x20
   32628:	mov	x3, x24
   3262c:	mov	x5, x23
   32630:	mov	x6, x19
   32634:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   32638:	ldp	x20, x19, [sp, #80]
   3263c:	ldp	x22, x21, [sp, #64]
   32640:	ldp	x24, x23, [sp, #48]
   32644:	ldp	x29, x30, [sp, #32]
   32648:	add	sp, sp, #0x60
   3264c:	ret
   32650:	stp	x29, x30, [sp, #-16]!
   32654:	add	x9, x3, x4, lsl #3
   32658:	mov	x8, x4
   3265c:	add	x4, x9, #0x8
   32660:	mov	x29, sp
   32664:	subs	x8, x8, #0x1
   32668:	b.lt	32688 <__gmpn_hgcd_step@@Base+0x17c>  // b.tstop
   3266c:	ldur	x9, [x4, #-16]
   32670:	sub	x4, x4, #0x8
   32674:	cbz	x9, 32664 <__gmpn_hgcd_step@@Base+0x158>
   32678:	add	x2, x8, #0x1
   3267c:	mov	x1, x3
   32680:	mov	w3, w5
   32684:	bl	d1f0 <__gmpn_hgcd_matrix_update_q@plt>
   32688:	ldp	x29, x30, [sp], #16
   3268c:	ret

0000000000032690 <__gmpn_hgcd_reduce_itch@@Base>:
   32690:	stp	x29, x30, [sp, #-48]!
   32694:	str	x21, [sp, #16]
   32698:	cmp	x0, #0x68e
   3269c:	sub	x21, x0, x1
   326a0:	stp	x20, x19, [sp, #32]
   326a4:	mov	x29, sp
   326a8:	b.le	326bc <__gmpn_hgcd_reduce_itch@@Base+0x2c>
   326ac:	mov	x0, x21
   326b0:	bl	c720 <__gmpn_hgcd_itch@plt>
   326b4:	add	x0, x0, x21, lsl #1
   326b8:	b	326dc <__gmpn_hgcd_reduce_itch@@Base+0x4c>
   326bc:	mov	x20, x0
   326c0:	mov	x0, x21
   326c4:	mov	x19, x1
   326c8:	bl	c720 <__gmpn_hgcd_itch@plt>
   326cc:	add	x8, x20, x19
   326d0:	sub	x8, x8, #0x1
   326d4:	cmp	x0, x8
   326d8:	csel	x0, x8, x0, lt  // lt = tstop
   326dc:	ldp	x20, x19, [sp, #32]
   326e0:	ldr	x21, [sp, #16]
   326e4:	ldp	x29, x30, [sp], #48
   326e8:	ret

00000000000326ec <__gmpn_hgcd_reduce@@Base>:
   326ec:	stp	x29, x30, [sp, #-80]!
   326f0:	stp	x24, x23, [sp, #32]
   326f4:	stp	x22, x21, [sp, #48]
   326f8:	stp	x20, x19, [sp, #64]
   326fc:	mov	x22, x5
   32700:	mov	x24, x4
   32704:	mov	x23, x3
   32708:	mov	x19, x2
   3270c:	mov	x20, x1
   32710:	mov	x21, x0
   32714:	cmp	x3, #0x68e
   32718:	add	x8, x1, x4, lsl #3
   3271c:	str	x25, [sp, #16]
   32720:	mov	x29, sp
   32724:	b.le	3278c <__gmpn_hgcd_reduce@@Base+0xa0>
   32728:	sub	x25, x23, x24
   3272c:	mov	x0, x22
   32730:	mov	x1, x8
   32734:	mov	x2, x25
   32738:	bl	cc10 <__gmpn_copyi@plt>
   3273c:	add	x8, x22, x23, lsl #3
   32740:	lsl	x9, x24, #3
   32744:	sub	x24, x8, x9
   32748:	add	x1, x19, x9
   3274c:	mov	x0, x24
   32750:	mov	x2, x25
   32754:	bl	cc10 <__gmpn_copyi@plt>
   32758:	add	x4, x22, x25, lsl #4
   3275c:	mov	x0, x22
   32760:	mov	x1, x24
   32764:	mov	x2, x25
   32768:	mov	x3, x21
   3276c:	bl	cf20 <__gmpn_hgcd_appr@plt>
   32770:	cbz	w0, 327cc <__gmpn_hgcd_reduce@@Base+0xe0>
   32774:	mov	x0, x21
   32778:	mov	x1, x20
   3277c:	mov	x2, x19
   32780:	mov	x3, x23
   32784:	bl	327e8 <__gmpn_hgcd_reduce@@Base+0xfc>
   32788:	b	327d0 <__gmpn_hgcd_reduce@@Base+0xe4>
   3278c:	add	x1, x19, x24, lsl #3
   32790:	sub	x2, x23, x24
   32794:	mov	x0, x8
   32798:	mov	x3, x21
   3279c:	mov	x4, x22
   327a0:	bl	cfb0 <__gmpn_hgcd@plt>
   327a4:	cmp	x0, #0x1
   327a8:	b.lt	327cc <__gmpn_hgcd_reduce@@Base+0xe0>  // b.tstop
   327ac:	add	x1, x0, x24
   327b0:	mov	x0, x21
   327b4:	mov	x2, x20
   327b8:	mov	x3, x19
   327bc:	mov	x4, x24
   327c0:	mov	x5, x22
   327c4:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   327c8:	b	327d0 <__gmpn_hgcd_reduce@@Base+0xe4>
   327cc:	mov	x0, xzr
   327d0:	ldp	x20, x19, [sp, #64]
   327d4:	ldp	x22, x21, [sp, #48]
   327d8:	ldp	x24, x23, [sp, #32]
   327dc:	ldr	x25, [sp, #16]
   327e0:	ldp	x29, x30, [sp], #80
   327e4:	ret
   327e8:	stp	x29, x30, [sp, #-96]!
   327ec:	stp	x28, x27, [sp, #16]
   327f0:	stp	x26, x25, [sp, #32]
   327f4:	stp	x24, x23, [sp, #48]
   327f8:	stp	x22, x21, [sp, #64]
   327fc:	stp	x20, x19, [sp, #80]
   32800:	mov	x29, sp
   32804:	sub	sp, sp, #0x60
   32808:	mov	x22, x3
   3280c:	mov	x19, x2
   32810:	mov	x20, x1
   32814:	mov	x21, x0
   32818:	mov	x8, x3
   3281c:	mov	x3, x8
   32820:	subs	x8, x8, #0x1
   32824:	b.lt	32834 <__gmpn_hgcd_reduce@@Base+0x148>  // b.tstop
   32828:	add	x9, x20, x3, lsl #3
   3282c:	ldur	x9, [x9, #-8]
   32830:	cbz	x9, 3281c <__gmpn_hgcd_reduce@@Base+0x130>
   32834:	mov	x9, x22
   32838:	mov	x8, x9
   3283c:	subs	x9, x9, #0x1
   32840:	b.lt	32850 <__gmpn_hgcd_reduce@@Base+0x164>  // b.tstop
   32844:	add	x10, x19, x8, lsl #3
   32848:	ldur	x10, [x10, #-8]
   3284c:	cbz	x10, 32838 <__gmpn_hgcd_reduce@@Base+0x14c>
   32850:	ldr	x10, [x21, #8]
   32854:	mov	x9, xzr
   32858:	sub	x11, x29, #0x20
   3285c:	b	3286c <__gmpn_hgcd_reduce@@Base+0x180>
   32860:	add	x9, x9, #0x1
   32864:	cmp	x9, #0x2
   32868:	b.eq	328b8 <__gmpn_hgcd_reduce@@Base+0x1cc>  // b.none
   3286c:	mov	x12, xzr
   32870:	b	32888 <__gmpn_hgcd_reduce@@Base+0x19c>
   32874:	add	x13, x11, x9, lsl #4
   32878:	str	x14, [x13, x12, lsl #3]
   3287c:	add	x12, x12, #0x1
   32880:	cmp	x12, #0x2
   32884:	b.eq	32860 <__gmpn_hgcd_reduce@@Base+0x174>  // b.none
   32888:	add	x13, x21, x9, lsl #4
   3288c:	add	x13, x13, x12, lsl #3
   32890:	add	x13, x13, #0x10
   32894:	mov	x15, x10
   32898:	mov	x14, x15
   3289c:	subs	x15, x15, #0x1
   328a0:	b.lt	32874 <__gmpn_hgcd_reduce@@Base+0x188>  // b.tstop
   328a4:	ldr	x16, [x13]
   328a8:	add	x16, x16, x14, lsl #3
   328ac:	ldur	x16, [x16, #-8]
   328b0:	cbz	x16, 32898 <__gmpn_hgcd_reduce@@Base+0x1ac>
   328b4:	b	32874 <__gmpn_hgcd_reduce@@Base+0x188>
   328b8:	ldur	x27, [x29, #-24]
   328bc:	stur	xzr, [x29, #-40]
   328c0:	cbz	x27, 32c00 <__gmpn_hgcd_reduce@@Base+0x514>
   328c4:	ldur	x9, [x29, #-16]
   328c8:	cbz	x9, 32c18 <__gmpn_hgcd_reduce@@Base+0x52c>
   328cc:	ldur	x11, [x29, #-32]
   328d0:	ldur	x10, [x29, #-8]
   328d4:	sub	x9, x8, x9
   328d8:	sub	x12, x3, x27
   328dc:	stur	x11, [x29, #-80]
   328e0:	sub	x11, x3, x11
   328e4:	sub	x10, x8, x10
   328e8:	cmp	x11, x9
   328ec:	csel	x8, x11, x9, lt  // lt = tstop
   328f0:	cmp	x12, x10
   328f4:	stp	x11, x9, [x29, #-56]
   328f8:	csel	x9, x12, x10, lt  // lt = tstop
   328fc:	cmp	x8, x9
   32900:	csel	x8, x8, x9, gt
   32904:	add	x0, x8, #0x2
   32908:	stp	x10, x12, [x29, #-72]
   3290c:	stur	x8, [x29, #-88]
   32910:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   32914:	ldr	x2, [x21, #8]
   32918:	mov	x1, x0
   3291c:	mov	x24, x0
   32920:	bl	32d6c <__gmpn_hgcd_reduce@@Base+0x680>
   32924:	add	x8, x0, x24, lsl #1
   32928:	lsl	x1, x8, #3
   3292c:	mov	w8, #0x7f00                	// #32512
   32930:	cmp	x1, x8
   32934:	b.hi	32c6c <__gmpn_hgcd_reduce@@Base+0x580>  // b.pmore
   32938:	add	x9, x1, #0xf
   3293c:	mov	x8, sp
   32940:	and	x9, x9, #0xfffffffffffffff0
   32944:	sub	x25, x8, x9
   32948:	mov	sp, x25
   3294c:	lsl	x8, x24, #3
   32950:	add	x26, x25, x8
   32954:	cmp	x24, x22
   32958:	add	x28, x26, x8
   3295c:	b.ge	329e4 <__gmpn_hgcd_reduce@@Base+0x2f8>  // b.tcont
   32960:	sub	x22, x22, x24
   32964:	add	x3, x20, x24, lsl #3
   32968:	mov	x0, x20
   3296c:	mov	x1, x20
   32970:	mov	x2, x24
   32974:	mov	x4, x22
   32978:	bl	c970 <__gmpn_add@plt>
   3297c:	ldr	x8, [x20]
   32980:	adds	x8, x8, x0
   32984:	str	x8, [x20]
   32988:	b.cc	329a0 <__gmpn_hgcd_reduce@@Base+0x2b4>  // b.lo, b.ul, b.last
   3298c:	add	x8, x20, #0x8
   32990:	ldr	x9, [x8]
   32994:	adds	x9, x9, #0x1
   32998:	str	x9, [x8], #8
   3299c:	b.cs	32990 <__gmpn_hgcd_reduce@@Base+0x2a4>  // b.hs, b.nlast
   329a0:	add	x3, x19, x24, lsl #3
   329a4:	mov	x0, x19
   329a8:	mov	x1, x19
   329ac:	mov	x2, x24
   329b0:	mov	x4, x22
   329b4:	bl	c970 <__gmpn_add@plt>
   329b8:	ldr	x8, [x19]
   329bc:	mov	x22, x24
   329c0:	adds	x8, x8, x0
   329c4:	str	x8, [x19]
   329c8:	b.cc	329e4 <__gmpn_hgcd_reduce@@Base+0x2f8>  // b.lo, b.ul, b.last
   329cc:	add	x8, x19, #0x8
   329d0:	ldr	x9, [x8]
   329d4:	adds	x9, x9, #0x1
   329d8:	str	x9, [x8], #8
   329dc:	b.cs	329d0 <__gmpn_hgcd_reduce@@Base+0x2e4>  // b.hs, b.nlast
   329e0:	mov	x22, x24
   329e4:	ldur	x23, [x29, #-8]
   329e8:	ldr	x4, [x21, #40]
   329ec:	mov	x0, x25
   329f0:	mov	x1, x24
   329f4:	mov	x2, x20
   329f8:	mov	x3, x22
   329fc:	mov	x5, x23
   32a00:	mov	x6, x28
   32a04:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   32a08:	ldr	x4, [x21, #24]
   32a0c:	mov	x0, x26
   32a10:	mov	x1, x24
   32a14:	mov	x2, x19
   32a18:	mov	x3, x22
   32a1c:	mov	x5, x27
   32a20:	mov	x6, x28
   32a24:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   32a28:	add	x8, x23, x22
   32a2c:	cmp	x8, x24
   32a30:	b.ge	32a54 <__gmpn_hgcd_reduce@@Base+0x368>  // b.tcont
   32a34:	sub	x8, x24, x22
   32a38:	subs	x8, x8, x23
   32a3c:	b.eq	32a54 <__gmpn_hgcd_reduce@@Base+0x368>  // b.none
   32a40:	add	x9, x22, x23
   32a44:	add	x0, x25, x9, lsl #3
   32a48:	lsl	x2, x8, #3
   32a4c:	mov	w1, wzr
   32a50:	bl	c780 <memset@plt>
   32a54:	add	x8, x22, x27
   32a58:	cmp	x8, x24
   32a5c:	ldur	x8, [x29, #-88]
   32a60:	add	x23, x8, #0x1
   32a64:	b.ge	32a8c <__gmpn_hgcd_reduce@@Base+0x3a0>  // b.tcont
   32a68:	sub	x8, x24, x22
   32a6c:	subs	x8, x8, x27
   32a70:	b.eq	32a8c <__gmpn_hgcd_reduce@@Base+0x3a0>  // b.none
   32a74:	add	x9, x24, x22
   32a78:	add	x9, x9, x27
   32a7c:	add	x0, x25, x9, lsl #3
   32a80:	lsl	x2, x8, #3
   32a84:	mov	w1, wzr
   32a88:	bl	c780 <memset@plt>
   32a8c:	mov	x0, x25
   32a90:	mov	x1, x25
   32a94:	mov	x2, x26
   32a98:	mov	x3, x24
   32a9c:	bl	c420 <__gmpn_sub_n@plt>
   32aa0:	ldr	x8, [x25]
   32aa4:	subs	x8, x8, x0
   32aa8:	str	x8, [x25]
   32aac:	b.cs	32ac4 <__gmpn_hgcd_reduce@@Base+0x3d8>  // b.hs, b.nlast
   32ab0:	add	x8, x25, #0x8
   32ab4:	ldr	x9, [x8]
   32ab8:	sub	x10, x9, #0x1
   32abc:	str	x10, [x8], #8
   32ac0:	cbz	x9, 32ab4 <__gmpn_hgcd_reduce@@Base+0x3c8>
   32ac4:	ldur	x27, [x29, #-16]
   32ac8:	ldr	x4, [x21, #32]
   32acc:	mov	x0, x26
   32ad0:	mov	x1, x24
   32ad4:	mov	x2, x20
   32ad8:	mov	x3, x22
   32adc:	mov	x5, x27
   32ae0:	mov	x6, x28
   32ae4:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   32ae8:	mov	x0, x20
   32aec:	mov	x1, x25
   32af0:	mov	x2, x23
   32af4:	bl	cc10 <__gmpn_copyi@plt>
   32af8:	ldr	x4, [x21, #16]
   32afc:	ldur	x21, [x29, #-80]
   32b00:	mov	x0, x25
   32b04:	mov	x1, x24
   32b08:	mov	x2, x19
   32b0c:	mov	x3, x22
   32b10:	mov	x5, x21
   32b14:	mov	x6, x28
   32b18:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   32b1c:	add	x8, x27, x22
   32b20:	cmp	x8, x24
   32b24:	b.ge	32b4c <__gmpn_hgcd_reduce@@Base+0x460>  // b.tcont
   32b28:	sub	x8, x24, x22
   32b2c:	subs	x8, x8, x27
   32b30:	b.eq	32b4c <__gmpn_hgcd_reduce@@Base+0x460>  // b.none
   32b34:	add	x9, x24, x22
   32b38:	add	x9, x9, x27
   32b3c:	add	x0, x25, x9, lsl #3
   32b40:	lsl	x2, x8, #3
   32b44:	mov	w1, wzr
   32b48:	bl	c780 <memset@plt>
   32b4c:	add	x8, x22, x21
   32b50:	cmp	x8, x24
   32b54:	b.ge	32b78 <__gmpn_hgcd_reduce@@Base+0x48c>  // b.tcont
   32b58:	sub	x8, x24, x22
   32b5c:	subs	x8, x8, x21
   32b60:	b.eq	32b78 <__gmpn_hgcd_reduce@@Base+0x48c>  // b.none
   32b64:	add	x9, x22, x21
   32b68:	add	x0, x25, x9, lsl #3
   32b6c:	lsl	x2, x8, #3
   32b70:	mov	w1, wzr
   32b74:	bl	c780 <memset@plt>
   32b78:	mov	x0, x25
   32b7c:	mov	x1, x25
   32b80:	mov	x2, x26
   32b84:	mov	x3, x24
   32b88:	bl	c420 <__gmpn_sub_n@plt>
   32b8c:	ldr	x8, [x25]
   32b90:	subs	x8, x8, x0
   32b94:	str	x8, [x25]
   32b98:	b.cs	32bb0 <__gmpn_hgcd_reduce@@Base+0x4c4>  // b.hs, b.nlast
   32b9c:	add	x8, x25, #0x8
   32ba0:	ldr	x9, [x8]
   32ba4:	sub	x10, x9, #0x1
   32ba8:	str	x10, [x8], #8
   32bac:	cbz	x9, 32ba0 <__gmpn_hgcd_reduce@@Base+0x4b4>
   32bb0:	mov	x0, x19
   32bb4:	mov	x1, x25
   32bb8:	mov	x2, x23
   32bbc:	bl	cc10 <__gmpn_copyi@plt>
   32bc0:	ldp	x9, x8, [x29, #-72]
   32bc4:	cmp	x8, x9
   32bc8:	csel	x8, x8, x9, lt  // lt = tstop
   32bcc:	ldp	x10, x9, [x29, #-56]
   32bd0:	cmp	x10, x9
   32bd4:	csel	x9, x10, x9, lt  // lt = tstop
   32bd8:	cmp	x8, x9
   32bdc:	csel	x8, x8, x9, gt
   32be0:	lsl	x9, x8, #3
   32be4:	ldr	x10, [x20, x9]
   32be8:	ldr	x9, [x19, x9]
   32bec:	sub	x8, x8, #0x1
   32bf0:	orr	x9, x9, x10
   32bf4:	cbz	x9, 32be0 <__gmpn_hgcd_reduce@@Base+0x4f4>
   32bf8:	add	x19, x8, #0x2
   32bfc:	b	32c38 <__gmpn_hgcd_reduce@@Base+0x54c>
   32c00:	ldr	x4, [x21, #32]
   32c04:	ldur	x5, [x29, #-16]
   32c08:	mov	x0, x19
   32c0c:	mov	x1, x8
   32c10:	mov	x2, x20
   32c14:	b	32c30 <__gmpn_hgcd_reduce@@Base+0x544>
   32c18:	ldr	x4, [x21, #24]
   32c1c:	mov	x0, x20
   32c20:	mov	x1, x3
   32c24:	mov	x2, x19
   32c28:	mov	x3, x8
   32c2c:	mov	x5, x27
   32c30:	bl	32c7c <__gmpn_hgcd_reduce@@Base+0x590>
   32c34:	mov	x19, x0
   32c38:	ldur	x0, [x29, #-40]
   32c3c:	cbnz	x0, 32c64 <__gmpn_hgcd_reduce@@Base+0x578>
   32c40:	mov	x0, x19
   32c44:	mov	sp, x29
   32c48:	ldp	x20, x19, [sp, #80]
   32c4c:	ldp	x22, x21, [sp, #64]
   32c50:	ldp	x24, x23, [sp, #48]
   32c54:	ldp	x26, x25, [sp, #32]
   32c58:	ldp	x28, x27, [sp, #16]
   32c5c:	ldp	x29, x30, [sp], #96
   32c60:	ret
   32c64:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   32c68:	b	32c40 <__gmpn_hgcd_reduce@@Base+0x554>
   32c6c:	sub	x0, x29, #0x28
   32c70:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   32c74:	mov	x25, x0
   32c78:	b	3294c <__gmpn_hgcd_reduce@@Base+0x260>
   32c7c:	stp	x29, x30, [sp, #-80]!
   32c80:	stp	x26, x25, [sp, #16]
   32c84:	stp	x24, x23, [sp, #32]
   32c88:	stp	x22, x21, [sp, #48]
   32c8c:	stp	x20, x19, [sp, #64]
   32c90:	mov	x29, sp
   32c94:	sub	sp, sp, #0x10
   32c98:	add	x26, x5, x3
   32c9c:	mov	x20, x1
   32ca0:	lsl	x1, x26, #3
   32ca4:	mov	w8, #0x7f00                	// #32512
   32ca8:	mov	x22, x5
   32cac:	mov	x23, x4
   32cb0:	mov	x19, x3
   32cb4:	mov	x24, x2
   32cb8:	mov	x21, x0
   32cbc:	cmp	x1, x8
   32cc0:	stur	xzr, [x29, #-8]
   32cc4:	b.hi	32d54 <__gmpn_hgcd_reduce@@Base+0x668>  // b.pmore
   32cc8:	add	x9, x1, #0xf
   32ccc:	mov	x8, sp
   32cd0:	and	x9, x9, #0xfffffffffffffff0
   32cd4:	sub	x25, x8, x9
   32cd8:	mov	sp, x25
   32cdc:	mov	x0, x25
   32ce0:	mov	x1, x24
   32ce4:	mov	x2, x19
   32ce8:	mov	x3, x23
   32cec:	mov	x4, x22
   32cf0:	bl	cea0 <__gmpn_mul@plt>
   32cf4:	cmp	x26, x20
   32cf8:	cset	w8, gt
   32cfc:	sub	x4, x26, x8
   32d00:	mov	x0, x21
   32d04:	mov	x1, x21
   32d08:	mov	x2, x20
   32d0c:	mov	x3, x25
   32d10:	bl	d340 <__gmpn_sub@plt>
   32d14:	ldur	x0, [x29, #-8]
   32d18:	cbnz	x0, 32d64 <__gmpn_hgcd_reduce@@Base+0x678>
   32d1c:	sub	x8, x21, #0x8
   32d20:	mov	x0, x20
   32d24:	cmp	x20, x19
   32d28:	b.le	32d38 <__gmpn_hgcd_reduce@@Base+0x64c>
   32d2c:	ldr	x9, [x8, x0, lsl #3]
   32d30:	sub	x20, x0, #0x1
   32d34:	cbz	x9, 32d20 <__gmpn_hgcd_reduce@@Base+0x634>
   32d38:	mov	sp, x29
   32d3c:	ldp	x20, x19, [sp, #64]
   32d40:	ldp	x22, x21, [sp, #48]
   32d44:	ldp	x24, x23, [sp, #32]
   32d48:	ldp	x26, x25, [sp, #16]
   32d4c:	ldp	x29, x30, [sp], #80
   32d50:	ret
   32d54:	sub	x0, x29, #0x8
   32d58:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   32d5c:	mov	x25, x0
   32d60:	b	32cdc <__gmpn_hgcd_reduce@@Base+0x5f0>
   32d64:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   32d68:	b	32d1c <__gmpn_hgcd_reduce@@Base+0x630>
   32d6c:	asr	x8, x0, #1
   32d70:	cmp	x8, x2
   32d74:	csel	x9, x0, x8, lt  // lt = tstop
   32d78:	cmp	x8, x1
   32d7c:	csel	x8, x9, xzr, lt  // lt = tstop
   32d80:	add	x8, x0, x8
   32d84:	add	x0, x8, #0x4
   32d88:	ret

0000000000032d8c <__gmpn_hgcd_itch@@Base>:
   32d8c:	cmp	x0, #0x65
   32d90:	b.lt	32dec <__gmpn_hgcd_itch@@Base+0x60>  // b.tstop
   32d94:	mov	x9, #0xd70b                	// #55051
   32d98:	movk	x9, #0x70a3, lsl #16
   32d9c:	movk	x9, #0xa3d, lsl #32
   32da0:	sub	x8, x0, #0x1
   32da4:	movk	x9, #0xa3d7, lsl #48
   32da8:	smulh	x9, x8, x9
   32dac:	add	x8, x9, x8
   32db0:	add	x9, x0, #0x3
   32db4:	add	x11, x0, #0x6
   32db8:	cmp	x9, #0x0
   32dbc:	csel	x9, x11, x9, lt  // lt = tstop
   32dc0:	asr	x11, x8, #6
   32dc4:	add	x8, x11, x8, lsr #63
   32dc8:	mov	w10, #0x40                  	// #64
   32dcc:	clz	x8, x8
   32dd0:	sub	w8, w10, w8
   32dd4:	mov	w10, #0x16                  	// #22
   32dd8:	mov	w11, #0x14                  	// #20
   32ddc:	asr	x9, x9, #2
   32de0:	mul	w8, w8, w10
   32de4:	madd	x8, x9, x11, x8
   32de8:	add	x0, x8, #0x65
   32dec:	ret

0000000000032df0 <__gmpn_hgcd@@Base>:
   32df0:	sub	sp, sp, #0x90
   32df4:	cmp	x2, #0x0
   32df8:	cinc	x8, x2, lt  // lt = tstop
   32dfc:	stp	x26, x25, [sp, #80]
   32e00:	asr	x25, x8, #1
   32e04:	stp	x22, x21, [sp, #112]
   32e08:	add	x22, x25, #0x1
   32e0c:	cmp	x22, x2
   32e10:	stp	x29, x30, [sp, #48]
   32e14:	stp	x28, x27, [sp, #64]
   32e18:	stp	x24, x23, [sp, #96]
   32e1c:	stp	x20, x19, [sp, #128]
   32e20:	add	x29, sp, #0x30
   32e24:	b.ge	32ecc <__gmpn_hgcd@@Base+0xdc>  // b.tcont
   32e28:	mov	x19, x4
   32e2c:	mov	x20, x3
   32e30:	mov	x24, x2
   32e34:	mov	x21, x1
   32e38:	mov	x23, x0
   32e3c:	cmp	x2, #0x65
   32e40:	b.lt	32ed4 <__gmpn_hgcd@@Base+0xe4>  // b.tstop
   32e44:	add	x8, x24, x24, lsl #1
   32e48:	add	x9, x8, #0x3
   32e4c:	cmp	x8, #0x0
   32e50:	csel	x8, x9, x8, lt  // lt = tstop
   32e54:	asr	x8, x8, #2
   32e58:	mov	x0, x20
   32e5c:	mov	x1, x23
   32e60:	mov	x2, x21
   32e64:	mov	x3, x24
   32e68:	mov	x4, x25
   32e6c:	mov	x5, x19
   32e70:	add	x26, x8, #0x1
   32e74:	bl	d4d0 <__gmpn_hgcd_reduce@plt>
   32e78:	cmp	x0, #0x0
   32e7c:	cset	w8, ne  // ne = any
   32e80:	csel	x0, x24, x0, eq  // eq = none
   32e84:	mov	x24, x0
   32e88:	cmp	x0, x26
   32e8c:	mov	w28, w8
   32e90:	b.le	32f30 <__gmpn_hgcd@@Base+0x140>
   32e94:	mov	x0, x24
   32e98:	mov	x1, x23
   32e9c:	mov	x2, x21
   32ea0:	mov	x3, x22
   32ea4:	mov	x4, x20
   32ea8:	mov	x5, x19
   32eac:	bl	c400 <__gmpn_hgcd_step@plt>
   32eb0:	mov	w8, #0x1                   	// #1
   32eb4:	cbnz	x0, 32e84 <__gmpn_hgcd@@Base+0x94>
   32eb8:	cmp	w28, #0x0
   32ebc:	mov	w8, wzr
   32ec0:	csel	x0, xzr, x24, eq  // eq = none
   32ec4:	cbnz	w8, 32ed8 <__gmpn_hgcd@@Base+0xe8>
   32ec8:	b	32f10 <__gmpn_hgcd@@Base+0x120>
   32ecc:	mov	x0, xzr
   32ed0:	b	32f10 <__gmpn_hgcd@@Base+0x120>
   32ed4:	mov	w28, wzr
   32ed8:	mov	x0, x24
   32edc:	mov	x1, x23
   32ee0:	mov	x2, x21
   32ee4:	mov	x3, x22
   32ee8:	mov	x4, x20
   32eec:	mov	x5, x19
   32ef0:	mov	x25, x24
   32ef4:	mov	w26, w28
   32ef8:	bl	c400 <__gmpn_hgcd_step@plt>
   32efc:	mov	x24, x0
   32f00:	mov	w28, #0x1                   	// #1
   32f04:	cbnz	x0, 32ed8 <__gmpn_hgcd@@Base+0xe8>
   32f08:	cmp	w26, #0x0
   32f0c:	csel	x0, xzr, x25, eq  // eq = none
   32f10:	ldp	x20, x19, [sp, #128]
   32f14:	ldp	x22, x21, [sp, #112]
   32f18:	ldp	x24, x23, [sp, #96]
   32f1c:	ldp	x26, x25, [sp, #80]
   32f20:	ldp	x28, x27, [sp, #64]
   32f24:	ldp	x29, x30, [sp, #48]
   32f28:	add	sp, sp, #0x90
   32f2c:	ret
   32f30:	add	x8, x25, #0x3
   32f34:	cmp	x24, x8
   32f38:	b.le	32fd4 <__gmpn_hgcd@@Base+0x1e4>
   32f3c:	lsl	x8, x22, #1
   32f40:	sub	x8, x8, x24
   32f44:	add	x25, x8, #0x1
   32f48:	sub	x27, x24, x25
   32f4c:	add	x8, x27, #0x1
   32f50:	add	x9, x27, #0x2
   32f54:	cmp	x8, #0x0
   32f58:	csinc	x8, x9, x27, lt  // lt = tstop
   32f5c:	lsl	x8, x8, #4
   32f60:	mov	x0, sp
   32f64:	mov	x1, x27
   32f68:	mov	x2, x19
   32f6c:	and	x26, x8, #0xffffffffffffffe0
   32f70:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   32f74:	add	x9, x26, x19
   32f78:	lsl	x8, x25, #3
   32f7c:	add	x26, x9, #0x20
   32f80:	add	x0, x23, x8
   32f84:	add	x1, x21, x8
   32f88:	mov	x3, sp
   32f8c:	mov	x2, x27
   32f90:	mov	x4, x26
   32f94:	bl	cfb0 <__gmpn_hgcd@plt>
   32f98:	cmp	x0, #0x1
   32f9c:	b.lt	32fd4 <__gmpn_hgcd@@Base+0x1e4>  // b.tstop
   32fa0:	add	x1, x0, x25
   32fa4:	mov	x0, sp
   32fa8:	mov	x2, x23
   32fac:	mov	x3, x21
   32fb0:	mov	x4, x25
   32fb4:	mov	x5, x26
   32fb8:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   32fbc:	mov	x24, x0
   32fc0:	mov	x1, sp
   32fc4:	mov	x0, x20
   32fc8:	mov	x2, x26
   32fcc:	bl	d170 <__gmpn_hgcd_matrix_mul@plt>
   32fd0:	mov	w28, #0x1                   	// #1
   32fd4:	mov	w8, #0x1                   	// #1
   32fd8:	cbnz	w8, 32ed8 <__gmpn_hgcd@@Base+0xe8>
   32fdc:	b	32f10 <__gmpn_hgcd@@Base+0x120>

0000000000032fe0 <__gmpn_hgcd_appr_itch@@Base>:
   32fe0:	cmp	x0, #0x68
   32fe4:	b.lt	33040 <__gmpn_hgcd_appr_itch@@Base+0x60>  // b.tstop
   32fe8:	mov	x9, #0x13e3                	// #5091
   32fec:	movk	x9, #0x2548, lsl #16
   32ff0:	movk	x9, #0x65e7, lsl #32
   32ff4:	sub	x8, x0, #0x1
   32ff8:	movk	x9, #0x9f11, lsl #48
   32ffc:	smulh	x9, x8, x9
   33000:	add	x8, x9, x8
   33004:	add	x9, x0, #0x3
   33008:	add	x11, x0, #0x6
   3300c:	cmp	x9, #0x0
   33010:	csel	x9, x11, x9, lt  // lt = tstop
   33014:	asr	x11, x8, #6
   33018:	add	x8, x11, x8, lsr #63
   3301c:	mov	w10, #0x40                  	// #64
   33020:	clz	x8, x8
   33024:	sub	w8, w10, w8
   33028:	mov	w10, #0x16                  	// #22
   3302c:	mov	w11, #0x14                  	// #20
   33030:	asr	x9, x9, #2
   33034:	mul	w8, w8, w10
   33038:	madd	x8, x9, x11, x8
   3303c:	add	x0, x8, #0x65
   33040:	ret

0000000000033044 <__gmpn_hgcd_appr@@Base>:
   33044:	sub	sp, sp, #0xa0
   33048:	cmp	x2, #0x3
   3304c:	stp	x29, x30, [sp, #64]
   33050:	stp	x28, x27, [sp, #80]
   33054:	stp	x26, x25, [sp, #96]
   33058:	stp	x24, x23, [sp, #112]
   3305c:	stp	x22, x21, [sp, #128]
   33060:	stp	x20, x19, [sp, #144]
   33064:	add	x29, sp, #0x40
   33068:	b.ge	33094 <__gmpn_hgcd_appr@@Base+0x50>  // b.tcont
   3306c:	mov	w24, wzr
   33070:	mov	w0, w24
   33074:	ldp	x20, x19, [sp, #144]
   33078:	ldp	x22, x21, [sp, #128]
   3307c:	ldp	x24, x23, [sp, #112]
   33080:	ldp	x26, x25, [sp, #96]
   33084:	ldp	x28, x27, [sp, #80]
   33088:	ldp	x29, x30, [sp, #64]
   3308c:	add	sp, sp, #0xa0
   33090:	ret
   33094:	cmp	x2, #0x0
   33098:	cinc	x8, x2, lt  // lt = tstop
   3309c:	asr	x26, x8, #1
   330a0:	mov	x19, x4
   330a4:	mov	x20, x3
   330a8:	mov	x25, x2
   330ac:	mov	x21, x1
   330b0:	mov	x22, x0
   330b4:	cmp	x2, #0x67
   330b8:	add	x23, x26, #0x1
   330bc:	b.le	3313c <__gmpn_hgcd_appr@@Base+0xf8>
   330c0:	add	x8, x25, x25, lsl #1
   330c4:	add	x9, x8, #0x3
   330c8:	cmp	x8, #0x0
   330cc:	csel	x8, x9, x8, lt  // lt = tstop
   330d0:	asr	x8, x8, #2
   330d4:	mov	x0, x20
   330d8:	mov	x1, x22
   330dc:	mov	x2, x21
   330e0:	mov	x3, x25
   330e4:	mov	x4, x26
   330e8:	mov	x5, x19
   330ec:	add	x27, x8, #0x1
   330f0:	bl	d4d0 <__gmpn_hgcd_reduce@plt>
   330f4:	cmp	x0, #0x0
   330f8:	cset	w28, ne  // ne = any
   330fc:	csel	x25, x25, x0, eq  // eq = none
   33100:	cmp	x25, x27
   33104:	b.le	33308 <__gmpn_hgcd_appr@@Base+0x2c4>
   33108:	mov	x0, x25
   3310c:	mov	x1, x22
   33110:	mov	x2, x21
   33114:	mov	x3, x23
   33118:	mov	x4, x20
   3311c:	mov	x5, x19
   33120:	bl	c400 <__gmpn_hgcd_step@plt>
   33124:	cmp	x0, #0x0
   33128:	csel	w24, w28, w24, eq  // eq = none
   3312c:	csinc	w28, w28, wzr, eq  // eq = none
   33130:	csel	x25, x25, x0, eq  // eq = none
   33134:	cbnz	x0, 33100 <__gmpn_hgcd_appr@@Base+0xbc>
   33138:	b	33070 <__gmpn_hgcd_appr@@Base+0x2c>
   3313c:	mov	w27, wzr
   33140:	mov	w24, wzr
   33144:	b	33154 <__gmpn_hgcd_appr@@Base+0x110>
   33148:	mov	w8, #0x9                   	// #9
   3314c:	cmp	w8, #0x9
   33150:	b.eq	33248 <__gmpn_hgcd_appr@@Base+0x204>  // b.none
   33154:	cmp	x25, #0x3
   33158:	b.lt	33248 <__gmpn_hgcd_appr@@Base+0x204>  // b.tstop
   3315c:	mov	x0, x25
   33160:	mov	x1, x22
   33164:	mov	x2, x21
   33168:	mov	x3, x23
   3316c:	mov	x4, x20
   33170:	mov	x5, x19
   33174:	bl	c400 <__gmpn_hgcd_step@plt>
   33178:	cbz	x0, 33148 <__gmpn_hgcd_appr@@Base+0x104>
   3317c:	lsl	w8, w27, #1
   33180:	add	x9, x8, x0, lsl #6
   33184:	add	x9, x9, #0x40
   33188:	mov	x26, x0
   3318c:	cmp	x9, x23, lsl #7
   33190:	b.gt	331d8 <__gmpn_hgcd_appr@@Base+0x194>
   33194:	lsl	x9, x23, #1
   33198:	sub	x9, x9, x26
   3319c:	lsl	x9, x9, #6
   331a0:	sub	x8, x9, x8
   331a4:	add	x9, x8, #0x3f
   331a8:	cmp	x8, #0x0
   331ac:	csel	x25, x9, x8, lt  // lt = tstop
   331b0:	cbz	w27, 331f0 <__gmpn_hgcd_appr@@Base+0x1ac>
   331b4:	sub	w27, w27, #0x1
   331b8:	asr	x9, x25, #6
   331bc:	lsl	x10, x9, #3
   331c0:	mov	w8, wzr
   331c4:	sub	x26, x26, x9
   331c8:	add	x22, x22, x10
   331cc:	add	x21, x21, x10
   331d0:	sub	x23, x23, x9
   331d4:	cbnz	w8, 331dc <__gmpn_hgcd_appr@@Base+0x198>
   331d8:	mov	w8, wzr
   331dc:	mov	w24, #0x1                   	// #1
   331e0:	mov	x25, x26
   331e4:	cmp	w8, #0x9
   331e8:	b.ne	33154 <__gmpn_hgcd_appr@@Base+0x110>  // b.any
   331ec:	b	33248 <__gmpn_hgcd_appr@@Base+0x204>
   331f0:	add	x28, x23, #0x1
   331f4:	cmp	x28, x26
   331f8:	b.eq	3322c <__gmpn_hgcd_appr@@Base+0x1e8>  // b.none
   331fc:	mvn	x9, x23
   33200:	add	x8, x22, x23, lsl #3
   33204:	add	x24, x26, x9
   33208:	add	x0, x8, #0x8
   3320c:	mov	x1, x24
   33210:	bl	c010 <__gmpn_zero_p@plt>
   33214:	cbnz	w0, 3322c <__gmpn_hgcd_appr@@Base+0x1e8>
   33218:	add	x8, x21, x23, lsl #3
   3321c:	add	x0, x8, #0x8
   33220:	mov	x1, x24
   33224:	bl	c010 <__gmpn_zero_p@plt>
   33228:	cbz	w0, 3323c <__gmpn_hgcd_appr@@Base+0x1f8>
   3322c:	mov	w27, wzr
   33230:	mov	w8, #0x8                   	// #8
   33234:	cbnz	w8, 331dc <__gmpn_hgcd_appr@@Base+0x198>
   33238:	b	331d8 <__gmpn_hgcd_appr@@Base+0x194>
   3323c:	mov	w27, #0x3f                  	// #63
   33240:	mov	x23, x28
   33244:	b	331b8 <__gmpn_hgcd_appr@@Base+0x174>
   33248:	cbz	w27, 332d4 <__gmpn_hgcd_appr@@Base+0x290>
   3324c:	mov	w8, #0x40                  	// #64
   33250:	sub	w26, w8, w27
   33254:	mov	x0, x22
   33258:	mov	x1, x22
   3325c:	mov	x2, x25
   33260:	mov	w3, w26
   33264:	bl	c2f0 <__gmpn_rshift@plt>
   33268:	str	x0, [x22, #-8]!
   3326c:	mov	x0, x21
   33270:	mov	x1, x21
   33274:	mov	x2, x25
   33278:	mov	w3, w26
   3327c:	bl	c2f0 <__gmpn_rshift@plt>
   33280:	str	x0, [x21, #-8]!
   33284:	lsl	x8, x25, #3
   33288:	ldr	x9, [x22, x8]
   3328c:	ldr	x8, [x21, x8]
   33290:	orr	x8, x8, x9
   33294:	cmp	x8, #0x0
   33298:	cinc	x25, x25, ne  // ne = any
   3329c:	cmp	x25, #0x3
   332a0:	b.lt	332d4 <__gmpn_hgcd_appr@@Base+0x290>  // b.tstop
   332a4:	mov	x0, x25
   332a8:	mov	x1, x22
   332ac:	mov	x2, x21
   332b0:	mov	x3, x23
   332b4:	mov	x4, x20
   332b8:	mov	x5, x19
   332bc:	bl	c400 <__gmpn_hgcd_step@plt>
   332c0:	cmp	x0, #0x0
   332c4:	csel	x25, x25, x0, eq  // eq = none
   332c8:	cbnz	x0, 3329c <__gmpn_hgcd_appr@@Base+0x258>
   332cc:	mov	w24, #0x1                   	// #1
   332d0:	b	33070 <__gmpn_hgcd_appr@@Base+0x2c>
   332d4:	cmp	x25, #0x2
   332d8:	b.ne	33070 <__gmpn_hgcd_appr@@Base+0x2c>  // b.any
   332dc:	ldp	x1, x0, [x22]
   332e0:	ldp	x3, x2, [x21]
   332e4:	add	x4, sp, #0x10
   332e8:	bl	c730 <__gmpn_hgcd2@plt>
   332ec:	cbz	w0, 33070 <__gmpn_hgcd_appr@@Base+0x2c>
   332f0:	add	x1, sp, #0x10
   332f4:	mov	x0, x20
   332f8:	mov	x2, x19
   332fc:	bl	c920 <__gmpn_hgcd_matrix_mul_1@plt>
   33300:	mov	w24, #0x1                   	// #1
   33304:	b	33070 <__gmpn_hgcd_appr@@Base+0x2c>
   33308:	add	x8, x26, #0x3
   3330c:	cmp	x25, x8
   33310:	b.le	333a4 <__gmpn_hgcd_appr@@Base+0x360>
   33314:	lsl	x8, x23, #1
   33318:	sub	x8, x8, x25
   3331c:	add	x26, x8, #0x1
   33320:	sub	x27, x25, x26
   33324:	add	x8, x27, #0x1
   33328:	add	x9, x27, #0x2
   3332c:	cmp	x8, #0x0
   33330:	csinc	x8, x9, x27, lt  // lt = tstop
   33334:	lsl	x8, x8, #4
   33338:	and	x8, x8, #0xffffffffffffffe0
   3333c:	add	x0, sp, #0x10
   33340:	mov	x1, x27
   33344:	mov	x2, x19
   33348:	str	x8, [sp, #8]
   3334c:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   33350:	ldr	x9, [sp, #8]
   33354:	lsl	x8, x26, #3
   33358:	add	x0, x22, x8
   3335c:	add	x1, x21, x8
   33360:	add	x9, x9, x19
   33364:	add	x26, x9, #0x20
   33368:	add	x3, sp, #0x10
   3336c:	mov	x2, x27
   33370:	mov	x4, x26
   33374:	bl	cf20 <__gmpn_hgcd_appr@plt>
   33378:	cbz	w0, 3339c <__gmpn_hgcd_appr@@Base+0x358>
   3337c:	add	x1, sp, #0x10
   33380:	mov	x0, x20
   33384:	mov	x2, x26
   33388:	bl	d170 <__gmpn_hgcd_matrix_mul@plt>
   3338c:	mov	w8, wzr
   33390:	mov	w24, #0x1                   	// #1
   33394:	cbnz	w8, 333a4 <__gmpn_hgcd_appr@@Base+0x360>
   33398:	b	33070 <__gmpn_hgcd_appr@@Base+0x2c>
   3339c:	mov	w8, #0x1                   	// #1
   333a0:	cbz	w8, 33070 <__gmpn_hgcd_appr@@Base+0x2c>
   333a4:	mov	x0, x25
   333a8:	mov	x1, x22
   333ac:	mov	x2, x21
   333b0:	mov	x3, x23
   333b4:	mov	x4, x20
   333b8:	mov	x5, x19
   333bc:	bl	c400 <__gmpn_hgcd_step@plt>
   333c0:	cmp	x0, #0x0
   333c4:	csel	w24, w28, w24, eq  // eq = none
   333c8:	csinc	w28, w28, wzr, eq  // eq = none
   333cc:	csel	x25, x25, x0, eq  // eq = none
   333d0:	cbnz	x0, 333a4 <__gmpn_hgcd_appr@@Base+0x360>
   333d4:	b	33070 <__gmpn_hgcd_appr@@Base+0x2c>

00000000000333d8 <__gmpn_hgcd2_jacobi@@Base>:
   333d8:	sub	sp, sp, #0x80
   333dc:	stp	x26, x25, [sp, #64]
   333e0:	mov	x25, x1
   333e4:	cmp	x0, #0x2
   333e8:	mov	w1, wzr
   333ec:	stp	x29, x30, [sp, #32]
   333f0:	stp	x28, x27, [sp, #48]
   333f4:	stp	x24, x23, [sp, #80]
   333f8:	stp	x22, x21, [sp, #96]
   333fc:	stp	x20, x19, [sp, #112]
   33400:	add	x29, sp, #0x20
   33404:	b.cc	336d0 <__gmpn_hgcd2_jacobi@@Base+0x2f8>  // b.lo, b.ul, b.last
   33408:	mov	x22, x2
   3340c:	cmp	x2, #0x2
   33410:	b.cc	336d0 <__gmpn_hgcd2_jacobi@@Base+0x2f8>  // b.lo, b.ul, b.last
   33414:	mov	x23, x0
   33418:	ldr	w0, [x5]
   3341c:	mov	x24, x3
   33420:	cmp	x23, x22
   33424:	b.hi	33434 <__gmpn_hgcd2_jacobi@@Base+0x5c>  // b.pmore
   33428:	b.ne	33464 <__gmpn_hgcd2_jacobi@@Base+0x8c>  // b.any
   3342c:	cmp	x25, x24
   33430:	b.ls	33464 <__gmpn_hgcd2_jacobi@@Base+0x8c>  // b.plast
   33434:	subs	x8, x25, x24
   33438:	sbc	x23, x23, x22
   3343c:	cmp	x23, #0x2
   33440:	b.cs	3344c <__gmpn_hgcd2_jacobi@@Base+0x74>  // b.hs, b.nlast
   33444:	mov	w1, wzr
   33448:	b	336d0 <__gmpn_hgcd2_jacobi@@Base+0x2f8>
   3344c:	stp	x4, x5, [sp]
   33450:	mov	x26, xzr
   33454:	mov	w1, #0x1                   	// #1
   33458:	mov	x25, x8
   3345c:	mov	w27, #0x1                   	// #1
   33460:	b	33488 <__gmpn_hgcd2_jacobi@@Base+0xb0>
   33464:	subs	x8, x24, x25
   33468:	sbc	x22, x22, x23
   3346c:	cmp	x22, #0x2
   33470:	mov	w1, wzr
   33474:	b.cc	336d0 <__gmpn_hgcd2_jacobi@@Base+0x2f8>  // b.lo, b.ul, b.last
   33478:	mov	x27, xzr
   3347c:	mov	w26, #0x1                   	// #1
   33480:	mov	x24, x8
   33484:	stp	x4, x5, [sp]
   33488:	mov	w2, #0x1                   	// #1
   3348c:	mov	w28, #0x1                   	// #1
   33490:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   33494:	mov	w21, w0
   33498:	cmp	x23, x22
   3349c:	mov	w19, #0x1                   	// #1
   334a0:	b.cc	33538 <__gmpn_hgcd2_jacobi@@Base+0x160>  // b.lo, b.ul, b.last
   334a4:	cmp	x23, x22
   334a8:	b.eq	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.none
   334ac:	lsr	x8, x23, #32
   334b0:	cbz	x8, 335d0 <__gmpn_hgcd2_jacobi@@Base+0x1f8>
   334b4:	mov	x8, x25
   334b8:	subs	x25, x8, x24
   334bc:	sbc	x23, x23, x22
   334c0:	cmp	x23, #0x2
   334c4:	b.cc	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.lo, b.ul, b.last
   334c8:	cmp	x23, x22
   334cc:	b.ls	3351c <__gmpn_hgcd2_jacobi@@Base+0x144>  // b.plast
   334d0:	add	x0, sp, #0x10
   334d4:	mov	x1, x23
   334d8:	mov	x2, x25
   334dc:	mov	x3, x22
   334e0:	mov	x4, x24
   334e4:	bl	33730 <__gmpn_hgcd2_jacobi@@Base+0x358>
   334e8:	ldp	x25, x23, [sp, #16]
   334ec:	mov	w1, #0x1                   	// #1
   334f0:	cmp	x23, #0x2
   334f4:	cinc	x20, x0, cs  // cs = hs, nlast
   334f8:	and	w2, w20, #0x3
   334fc:	mov	w0, w21
   33500:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   33504:	mov	w21, w0
   33508:	cmp	x23, #0x2
   3350c:	madd	x19, x20, x26, x19
   33510:	madd	x27, x20, x28, x27
   33514:	b.cs	33538 <__gmpn_hgcd2_jacobi@@Base+0x160>  // b.hs, b.nlast
   33518:	b	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>
   3351c:	mov	w1, #0x1                   	// #1
   33520:	mov	w2, #0x1                   	// #1
   33524:	mov	w0, w21
   33528:	add	x27, x27, x28
   3352c:	add	x19, x19, x26
   33530:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   33534:	mov	w21, w0
   33538:	cmp	x23, x22
   3353c:	b.eq	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.none
   33540:	lsr	x8, x22, #32
   33544:	cbz	x8, 335dc <__gmpn_hgcd2_jacobi@@Base+0x204>
   33548:	mov	x8, x24
   3354c:	subs	x24, x8, x25
   33550:	sbc	x22, x22, x23
   33554:	cmp	x22, #0x2
   33558:	b.cc	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.lo, b.ul, b.last
   3355c:	cmp	x22, x23
   33560:	b.ls	335b0 <__gmpn_hgcd2_jacobi@@Base+0x1d8>  // b.plast
   33564:	add	x0, sp, #0x10
   33568:	mov	x1, x22
   3356c:	mov	x2, x24
   33570:	mov	x3, x23
   33574:	mov	x4, x25
   33578:	bl	33730 <__gmpn_hgcd2_jacobi@@Base+0x358>
   3357c:	ldp	x24, x22, [sp, #16]
   33580:	mov	w1, wzr
   33584:	cmp	x22, #0x2
   33588:	cinc	x20, x0, cs  // cs = hs, nlast
   3358c:	and	w2, w20, #0x3
   33590:	mov	w0, w21
   33594:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   33598:	mov	w21, w0
   3359c:	cmp	x22, #0x2
   335a0:	madd	x26, x20, x19, x26
   335a4:	madd	x28, x20, x27, x28
   335a8:	b.cs	334a4 <__gmpn_hgcd2_jacobi@@Base+0xcc>  // b.hs, b.nlast
   335ac:	b	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>
   335b0:	mov	w2, #0x1                   	// #1
   335b4:	mov	w0, w21
   335b8:	mov	w1, wzr
   335bc:	add	x28, x27, x28
   335c0:	add	x26, x19, x26
   335c4:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   335c8:	mov	w21, w0
   335cc:	b	334a4 <__gmpn_hgcd2_jacobi@@Base+0xcc>
   335d0:	extr	x23, x23, x25, #32
   335d4:	extr	x22, x22, x24, #32
   335d8:	b	335e8 <__gmpn_hgcd2_jacobi@@Base+0x210>
   335dc:	extr	x23, x23, x25, #32
   335e0:	extr	x22, x22, x24, #32
   335e4:	b	33660 <__gmpn_hgcd2_jacobi@@Base+0x288>
   335e8:	subs	x23, x23, x22
   335ec:	b.eq	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.none
   335f0:	lsr	x8, x23, #33
   335f4:	cbz	x8, 336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>
   335f8:	cmp	x23, x22
   335fc:	b.ls	33644 <__gmpn_hgcd2_jacobi@@Base+0x26c>  // b.plast
   33600:	add	x0, sp, #0x10
   33604:	mov	x1, x23
   33608:	mov	x2, x22
   3360c:	bl	33808 <__gmpn_hgcd2_jacobi@@Base+0x430>
   33610:	ldr	x23, [sp, #16]
   33614:	mov	w1, #0x1                   	// #1
   33618:	lsr	x20, x23, #33
   3361c:	cmp	x20, #0x0
   33620:	cinc	x24, x0, ne  // ne = any
   33624:	and	w2, w24, #0x3
   33628:	mov	w0, w21
   3362c:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   33630:	mov	w21, w0
   33634:	madd	x19, x24, x26, x19
   33638:	madd	x27, x24, x28, x27
   3363c:	cbnz	x20, 33660 <__gmpn_hgcd2_jacobi@@Base+0x288>
   33640:	b	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>
   33644:	mov	w1, #0x1                   	// #1
   33648:	mov	w2, #0x1                   	// #1
   3364c:	mov	w0, w21
   33650:	add	x27, x27, x28
   33654:	add	x19, x19, x26
   33658:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   3365c:	mov	w21, w0
   33660:	subs	x22, x22, x23
   33664:	b.eq	336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>  // b.none
   33668:	lsr	x8, x22, #33
   3366c:	cbz	x8, 336b8 <__gmpn_hgcd2_jacobi@@Base+0x2e0>
   33670:	cmp	x22, x23
   33674:	b.ls	336f4 <__gmpn_hgcd2_jacobi@@Base+0x31c>  // b.plast
   33678:	add	x0, sp, #0x10
   3367c:	mov	x1, x22
   33680:	mov	x2, x23
   33684:	bl	33808 <__gmpn_hgcd2_jacobi@@Base+0x430>
   33688:	ldr	x22, [sp, #16]
   3368c:	mov	w1, wzr
   33690:	lsr	x20, x22, #33
   33694:	cmp	x20, #0x0
   33698:	cinc	x24, x0, ne  // ne = any
   3369c:	and	w2, w24, #0x3
   336a0:	mov	w0, w21
   336a4:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   336a8:	mov	w21, w0
   336ac:	madd	x26, x24, x19, x26
   336b0:	madd	x28, x24, x27, x28
   336b4:	cbnz	x20, 335e8 <__gmpn_hgcd2_jacobi@@Base+0x210>
   336b8:	ldr	x8, [sp]
   336bc:	mov	w1, #0x1                   	// #1
   336c0:	stp	x28, x27, [x8]
   336c4:	stp	x26, x19, [x8, #16]
   336c8:	ldr	x8, [sp, #8]
   336cc:	str	w21, [x8]
   336d0:	ldp	x20, x19, [sp, #112]
   336d4:	ldp	x22, x21, [sp, #96]
   336d8:	ldp	x24, x23, [sp, #80]
   336dc:	ldp	x26, x25, [sp, #64]
   336e0:	ldp	x28, x27, [sp, #48]
   336e4:	ldp	x29, x30, [sp, #32]
   336e8:	mov	w0, w1
   336ec:	add	sp, sp, #0x80
   336f0:	ret
   336f4:	mov	w2, #0x1                   	// #1
   336f8:	mov	w0, w21
   336fc:	mov	w1, wzr
   33700:	add	x28, x27, x28
   33704:	add	x26, x19, x26
   33708:	bl	33714 <__gmpn_hgcd2_jacobi@@Base+0x33c>
   3370c:	mov	w21, w0
   33710:	b	335e8 <__gmpn_hgcd2_jacobi@@Base+0x210>
   33714:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   33718:	ldr	x9, [x9, #3872]
   3371c:	lsl	w8, w1, #2
   33720:	add	w8, w8, w0, lsl #3
   33724:	add	w8, w8, w2
   33728:	ldrb	w0, [x9, w8, uxtw]
   3372c:	ret
   33730:	tbnz	x1, #63, 337ac <__gmpn_hgcd2_jacobi@@Base+0x3d4>
   33734:	mov	w9, wzr
   33738:	b	33748 <__gmpn_hgcd2_jacobi@@Base+0x370>
   3373c:	extr	x3, x3, x4, #63
   33740:	lsl	x4, x4, #1
   33744:	sub	w9, w9, #0x1
   33748:	cmp	x3, x1
   3374c:	b.cc	3373c <__gmpn_hgcd2_jacobi@@Base+0x364>  // b.lo, b.ul, b.last
   33750:	b.ne	3375c <__gmpn_hgcd2_jacobi@@Base+0x384>  // b.any
   33754:	cmp	x4, x2
   33758:	b.ls	3373c <__gmpn_hgcd2_jacobi@@Base+0x364>  // b.plast
   3375c:	mov	x8, xzr
   33760:	cbnz	w9, 33788 <__gmpn_hgcd2_jacobi@@Base+0x3b0>
   33764:	stp	x2, x1, [x0]
   33768:	mov	x0, x8
   3376c:	ret
   33770:	subs	x10, x2, x4
   33774:	sbc	x1, x1, x3
   33778:	orr	x8, x8, #0x1
   3377c:	mov	x2, x10
   33780:	adds	w9, w9, #0x1
   33784:	b.cs	33764 <__gmpn_hgcd2_jacobi@@Base+0x38c>  // b.hs, b.nlast
   33788:	extr	x4, x3, x4, #1
   3378c:	lsr	x3, x3, #1
   33790:	cmp	x1, x3
   33794:	lsl	x8, x8, #1
   33798:	b.hi	33770 <__gmpn_hgcd2_jacobi@@Base+0x398>  // b.pmore
   3379c:	b.ne	33780 <__gmpn_hgcd2_jacobi@@Base+0x3a8>  // b.any
   337a0:	cmp	x2, x4
   337a4:	b.cs	33770 <__gmpn_hgcd2_jacobi@@Base+0x398>  // b.hs, b.nlast
   337a8:	b	33780 <__gmpn_hgcd2_jacobi@@Base+0x3a8>
   337ac:	mov	w9, #0x1                   	// #1
   337b0:	tbnz	x3, #63, 337c4 <__gmpn_hgcd2_jacobi@@Base+0x3ec>
   337b4:	extr	x3, x3, x4, #63
   337b8:	lsl	x4, x4, #1
   337bc:	add	w9, w9, #0x1
   337c0:	tbz	x3, #63, 337b4 <__gmpn_hgcd2_jacobi@@Base+0x3dc>
   337c4:	mov	x8, xzr
   337c8:	b	337ec <__gmpn_hgcd2_jacobi@@Base+0x414>
   337cc:	subs	x10, x2, x4
   337d0:	sbc	x1, x1, x3
   337d4:	orr	x8, x8, #0x1
   337d8:	mov	x2, x10
   337dc:	extr	x4, x3, x4, #1
   337e0:	subs	w9, w9, #0x1
   337e4:	lsr	x3, x3, #1
   337e8:	b.eq	33764 <__gmpn_hgcd2_jacobi@@Base+0x38c>  // b.none
   337ec:	cmp	x1, x3
   337f0:	lsl	x8, x8, #1
   337f4:	b.hi	337cc <__gmpn_hgcd2_jacobi@@Base+0x3f4>  // b.pmore
   337f8:	b.ne	337dc <__gmpn_hgcd2_jacobi@@Base+0x404>  // b.any
   337fc:	cmp	x2, x4
   33800:	b.cs	337cc <__gmpn_hgcd2_jacobi@@Base+0x3f4>  // b.hs, b.nlast
   33804:	b	337dc <__gmpn_hgcd2_jacobi@@Base+0x404>
   33808:	tbnz	x1, #63, 3381c <__gmpn_hgcd2_jacobi@@Base+0x444>
   3380c:	cmp	x2, x1
   33810:	b.ls	3385c <__gmpn_hgcd2_jacobi@@Base+0x484>  // b.plast
   33814:	mov	w9, wzr
   33818:	b	33870 <__gmpn_hgcd2_jacobi@@Base+0x498>
   3381c:	mov	w9, #0x1                   	// #1
   33820:	tbnz	x2, #63, 33830 <__gmpn_hgcd2_jacobi@@Base+0x458>
   33824:	lsl	x2, x2, #1
   33828:	add	w9, w9, #0x1
   3382c:	tbz	x2, #63, 33824 <__gmpn_hgcd2_jacobi@@Base+0x44c>
   33830:	mov	x10, xzr
   33834:	cmp	x1, x2
   33838:	cset	w8, cs  // cs = hs, nlast
   3383c:	csel	x11, xzr, x2, cc  // cc = lo, ul, last
   33840:	bfi	x8, x10, #1, #63
   33844:	sub	x1, x1, x11
   33848:	subs	w9, w9, #0x1
   3384c:	lsr	x2, x2, #1
   33850:	mov	x10, x8
   33854:	b.ne	33834 <__gmpn_hgcd2_jacobi@@Base+0x45c>  // b.any
   33858:	b	338a4 <__gmpn_hgcd2_jacobi@@Base+0x4cc>
   3385c:	mov	w9, wzr
   33860:	lsl	x2, x2, #1
   33864:	cmp	x2, x1
   33868:	add	w9, w9, #0x1
   3386c:	b.ls	33860 <__gmpn_hgcd2_jacobi@@Base+0x488>  // b.plast
   33870:	cbz	w9, 338a0 <__gmpn_hgcd2_jacobi@@Base+0x4c8>
   33874:	mov	x10, xzr
   33878:	lsr	x2, x2, #1
   3387c:	cmp	x1, x2
   33880:	cset	w8, cs  // cs = hs, nlast
   33884:	csel	x11, xzr, x2, cc  // cc = lo, ul, last
   33888:	bfi	x8, x10, #1, #63
   3388c:	subs	w9, w9, #0x1
   33890:	sub	x1, x1, x11
   33894:	mov	x10, x8
   33898:	b.ne	33878 <__gmpn_hgcd2_jacobi@@Base+0x4a0>  // b.any
   3389c:	b	338a4 <__gmpn_hgcd2_jacobi@@Base+0x4cc>
   338a0:	mov	x8, xzr
   338a4:	str	x1, [x0]
   338a8:	mov	x0, x8
   338ac:	ret

00000000000338b0 <__gmpn_hgcd_jacobi@@Base>:
   338b0:	sub	sp, sp, #0xa0
   338b4:	cmp	x2, #0x0
   338b8:	cinc	x8, x2, lt  // lt = tstop
   338bc:	stp	x26, x25, [sp, #96]
   338c0:	asr	x26, x8, #1
   338c4:	stp	x24, x23, [sp, #112]
   338c8:	add	x23, x26, #0x1
   338cc:	cmp	x23, x2
   338d0:	stp	x29, x30, [sp, #64]
   338d4:	stp	x28, x27, [sp, #80]
   338d8:	stp	x22, x21, [sp, #128]
   338dc:	stp	x20, x19, [sp, #144]
   338e0:	add	x29, sp, #0x40
   338e4:	b.ge	3396c <__gmpn_hgcd_jacobi@@Base+0xbc>  // b.tcont
   338e8:	mov	x19, x5
   338ec:	mov	x20, x4
   338f0:	mov	x21, x3
   338f4:	mov	x25, x2
   338f8:	mov	x22, x1
   338fc:	mov	x24, x0
   33900:	cmp	x2, #0x65
   33904:	b.lt	33974 <__gmpn_hgcd_jacobi@@Base+0xc4>  // b.tstop
   33908:	add	x8, x25, x25, lsl #1
   3390c:	lsl	x9, x26, #3
   33910:	add	x10, x8, #0x3
   33914:	cmp	x8, #0x0
   33918:	add	x0, x24, x9
   3391c:	add	x1, x22, x9
   33920:	csel	x8, x10, x8, lt  // lt = tstop
   33924:	sub	x2, x25, x26
   33928:	mov	x3, x21
   3392c:	mov	x4, x20
   33930:	mov	x5, x19
   33934:	asr	x27, x8, #2
   33938:	bl	d570 <__gmpn_hgcd_jacobi@plt>
   3393c:	cmp	x0, #0x1
   33940:	b.lt	3397c <__gmpn_hgcd_jacobi@@Base+0xcc>  // b.tstop
   33944:	add	x1, x0, x26
   33948:	mov	x0, x21
   3394c:	mov	x2, x24
   33950:	mov	x3, x22
   33954:	mov	x4, x26
   33958:	mov	x5, x19
   3395c:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   33960:	mov	x25, x0
   33964:	mov	w8, #0x1                   	// #1
   33968:	b	33980 <__gmpn_hgcd_jacobi@@Base+0xd0>
   3396c:	mov	x0, xzr
   33970:	b	33acc <__gmpn_hgcd_jacobi@@Base+0x21c>
   33974:	mov	w27, wzr
   33978:	b	33a90 <__gmpn_hgcd_jacobi@@Base+0x1e0>
   3397c:	mov	w8, wzr
   33980:	add	x28, x27, #0x1
   33984:	mov	x0, x25
   33988:	mov	x25, x0
   3398c:	cmp	x0, x28
   33990:	mov	w27, w8
   33994:	b.le	339d4 <__gmpn_hgcd_jacobi@@Base+0x124>
   33998:	mov	x0, x25
   3399c:	mov	x1, x24
   339a0:	mov	x2, x22
   339a4:	mov	x3, x23
   339a8:	mov	x4, x21
   339ac:	mov	x5, x20
   339b0:	mov	x6, x19
   339b4:	bl	33aec <__gmpn_hgcd_jacobi@@Base+0x23c>
   339b8:	mov	w8, #0x1                   	// #1
   339bc:	cbnz	x0, 33988 <__gmpn_hgcd_jacobi@@Base+0xd8>
   339c0:	cmp	w27, #0x0
   339c4:	mov	w8, wzr
   339c8:	csel	x0, xzr, x25, eq  // eq = none
   339cc:	cbnz	w8, 33a90 <__gmpn_hgcd_jacobi@@Base+0x1e0>
   339d0:	b	33acc <__gmpn_hgcd_jacobi@@Base+0x21c>
   339d4:	add	x8, x26, #0x3
   339d8:	cmp	x25, x8
   339dc:	b.le	33a88 <__gmpn_hgcd_jacobi@@Base+0x1d8>
   339e0:	lsl	x8, x23, #1
   339e4:	sub	x8, x8, x25
   339e8:	add	x26, x8, #0x1
   339ec:	sub	x28, x25, x26
   339f0:	add	x8, x28, #0x1
   339f4:	add	x9, x28, #0x2
   339f8:	cmp	x8, #0x0
   339fc:	csinc	x8, x9, x28, lt  // lt = tstop
   33a00:	lsl	x8, x8, #4
   33a04:	and	x8, x8, #0xffffffffffffffe0
   33a08:	add	x0, sp, #0x10
   33a0c:	mov	x1, x28
   33a10:	mov	x2, x19
   33a14:	str	x8, [sp, #8]
   33a18:	bl	c9e0 <__gmpn_hgcd_matrix_init@plt>
   33a1c:	ldr	x9, [sp, #8]
   33a20:	lsl	x8, x26, #3
   33a24:	add	x0, x24, x8
   33a28:	add	x1, x22, x8
   33a2c:	add	x9, x9, x19
   33a30:	add	x8, x9, #0x20
   33a34:	add	x3, sp, #0x10
   33a38:	mov	x2, x28
   33a3c:	mov	x4, x20
   33a40:	mov	x5, x8
   33a44:	mov	x28, x8
   33a48:	bl	d570 <__gmpn_hgcd_jacobi@plt>
   33a4c:	cmp	x0, #0x1
   33a50:	b.lt	33a88 <__gmpn_hgcd_jacobi@@Base+0x1d8>  // b.tstop
   33a54:	add	x1, x0, x26
   33a58:	add	x0, sp, #0x10
   33a5c:	mov	x2, x24
   33a60:	mov	x3, x22
   33a64:	mov	x4, x26
   33a68:	mov	x5, x28
   33a6c:	bl	ca50 <__gmpn_hgcd_matrix_adjust@plt>
   33a70:	mov	x25, x0
   33a74:	add	x1, sp, #0x10
   33a78:	mov	x0, x21
   33a7c:	mov	x2, x28
   33a80:	bl	d170 <__gmpn_hgcd_matrix_mul@plt>
   33a84:	mov	w27, #0x1                   	// #1
   33a88:	mov	w8, #0x1                   	// #1
   33a8c:	cbz	w8, 33acc <__gmpn_hgcd_jacobi@@Base+0x21c>
   33a90:	mov	x0, x25
   33a94:	mov	x1, x24
   33a98:	mov	x2, x22
   33a9c:	mov	x3, x23
   33aa0:	mov	x4, x21
   33aa4:	mov	x5, x20
   33aa8:	mov	x6, x19
   33aac:	mov	x26, x25
   33ab0:	mov	w28, w27
   33ab4:	bl	33aec <__gmpn_hgcd_jacobi@@Base+0x23c>
   33ab8:	mov	x25, x0
   33abc:	mov	w27, #0x1                   	// #1
   33ac0:	cbnz	x0, 33a90 <__gmpn_hgcd_jacobi@@Base+0x1e0>
   33ac4:	cmp	w28, #0x0
   33ac8:	csel	x0, xzr, x26, eq  // eq = none
   33acc:	ldp	x20, x19, [sp, #144]
   33ad0:	ldp	x22, x21, [sp, #128]
   33ad4:	ldp	x24, x23, [sp, #112]
   33ad8:	ldp	x26, x25, [sp, #96]
   33adc:	ldp	x28, x27, [sp, #80]
   33ae0:	ldp	x29, x30, [sp, #64]
   33ae4:	add	sp, sp, #0xa0
   33ae8:	ret
   33aec:	sub	sp, sp, #0x80
   33af0:	lsl	x8, x0, #3
   33af4:	stp	x29, x30, [sp, #48]
   33af8:	stp	x24, x23, [sp, #80]
   33afc:	stp	x22, x21, [sp, #96]
   33b00:	stp	x20, x19, [sp, #112]
   33b04:	sub	x9, x8, #0x8
   33b08:	mov	x20, x2
   33b0c:	mov	x21, x0
   33b10:	ldr	x0, [x1, x9]
   33b14:	ldr	x2, [x2, x9]
   33b18:	add	x9, x3, #0x1
   33b1c:	str	x25, [sp, #64]
   33b20:	mov	x19, x6
   33b24:	mov	x25, x5
   33b28:	mov	x23, x4
   33b2c:	mov	x22, x1
   33b30:	mov	x24, x3
   33b34:	cmp	x9, x21
   33b38:	orr	x9, x2, x0
   33b3c:	add	x29, sp, #0x30
   33b40:	b.ne	33b50 <__gmpn_hgcd_jacobi@@Base+0x2a0>  // b.any
   33b44:	cmp	x9, #0x4
   33b48:	b.cs	33ba8 <__gmpn_hgcd_jacobi@@Base+0x2f8>  // b.hs, b.nlast
   33b4c:	b	33c00 <__gmpn_hgcd_jacobi@@Base+0x350>
   33b50:	tbnz	x9, #63, 33ba8 <__gmpn_hgcd_jacobi@@Base+0x2f8>
   33b54:	sub	x10, x8, #0x10
   33b58:	sub	x8, x8, #0x18
   33b5c:	ldr	x12, [x22, x10]
   33b60:	ldr	x14, [x22, x8]
   33b64:	ldr	x10, [x20, x10]
   33b68:	ldr	x8, [x20, x8]
   33b6c:	clz	x9, x9
   33b70:	neg	w13, w9
   33b74:	lsl	x11, x0, x9
   33b78:	lsl	x15, x2, x9
   33b7c:	lsr	x16, x12, x13
   33b80:	lsl	x12, x12, x9
   33b84:	lsr	x14, x14, x13
   33b88:	lsl	x9, x10, x9
   33b8c:	lsr	x10, x10, x13
   33b90:	lsr	x8, x8, x13
   33b94:	orr	x0, x16, x11
   33b98:	orr	x1, x14, x12
   33b9c:	orr	x2, x10, x15
   33ba0:	orr	x3, x8, x9
   33ba4:	b	33bb4 <__gmpn_hgcd_jacobi@@Base+0x304>
   33ba8:	sub	x8, x8, #0x10
   33bac:	ldr	x1, [x22, x8]
   33bb0:	ldr	x3, [x20, x8]
   33bb4:	add	x4, sp, #0x10
   33bb8:	mov	x5, x25
   33bbc:	bl	ccb0 <__gmpn_hgcd2_jacobi@plt>
   33bc0:	cbz	w0, 33c00 <__gmpn_hgcd_jacobi@@Base+0x350>
   33bc4:	add	x1, sp, #0x10
   33bc8:	mov	x0, x23
   33bcc:	mov	x2, x19
   33bd0:	bl	c920 <__gmpn_hgcd_matrix_mul_1@plt>
   33bd4:	mov	x0, x19
   33bd8:	mov	x1, x22
   33bdc:	mov	x2, x21
   33be0:	bl	cc10 <__gmpn_copyi@plt>
   33be4:	add	x0, sp, #0x10
   33be8:	mov	x1, x22
   33bec:	mov	x2, x19
   33bf0:	mov	x3, x20
   33bf4:	mov	x4, x21
   33bf8:	bl	c660 <__gmpn_matrix22_mul1_inverse_vector@plt>
   33bfc:	b	33c28 <__gmpn_hgcd_jacobi@@Base+0x378>
   33c00:	adrp	x4, 33000 <__gmpn_hgcd_appr_itch@@Base+0x20>
   33c04:	add	x4, x4, #0xc44
   33c08:	mov	x5, sp
   33c0c:	mov	x0, x22
   33c10:	mov	x1, x20
   33c14:	mov	x2, x21
   33c18:	mov	x3, x24
   33c1c:	mov	x6, x19
   33c20:	stp	x23, x25, [sp]
   33c24:	bl	d490 <__gmpn_gcd_subdiv_step@plt>
   33c28:	ldp	x20, x19, [sp, #112]
   33c2c:	ldp	x22, x21, [sp, #96]
   33c30:	ldp	x24, x23, [sp, #80]
   33c34:	ldr	x25, [sp, #64]
   33c38:	ldp	x29, x30, [sp, #48]
   33c3c:	add	sp, sp, #0x80
   33c40:	ret
   33c44:	stp	x29, x30, [sp, #-48]!
   33c48:	add	x9, x3, x4, lsl #3
   33c4c:	str	x21, [sp, #16]
   33c50:	stp	x20, x19, [sp, #32]
   33c54:	mov	w19, w5
   33c58:	mov	x8, x4
   33c5c:	mov	x20, x3
   33c60:	mov	x21, x0
   33c64:	add	x4, x9, #0x8
   33c68:	mov	x29, sp
   33c6c:	subs	x8, x8, #0x1
   33c70:	b.lt	33cb0 <__gmpn_hgcd_jacobi@@Base+0x400>  // b.tstop
   33c74:	ldur	x9, [x4, #-16]
   33c78:	sub	x4, x4, #0x8
   33c7c:	cbz	x9, 33c6c <__gmpn_hgcd_jacobi@@Base+0x3bc>
   33c80:	ldr	x0, [x21]
   33c84:	add	x2, x8, #0x1
   33c88:	mov	x1, x20
   33c8c:	mov	w3, w19
   33c90:	bl	d1f0 <__gmpn_hgcd_matrix_update_q@plt>
   33c94:	ldr	x21, [x21, #8]
   33c98:	ldr	w8, [x20]
   33c9c:	mov	w1, w19
   33ca0:	ldr	w0, [x21]
   33ca4:	and	w2, w8, #0x3
   33ca8:	bl	33cc0 <__gmpn_hgcd_jacobi@@Base+0x410>
   33cac:	str	w0, [x21]
   33cb0:	ldp	x20, x19, [sp, #32]
   33cb4:	ldr	x21, [sp, #16]
   33cb8:	ldp	x29, x30, [sp], #48
   33cbc:	ret
   33cc0:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   33cc4:	ldr	x9, [x9, #3872]
   33cc8:	lsl	w8, w1, #2
   33ccc:	add	w8, w8, w0, lsl #3
   33cd0:	add	w8, w8, w2
   33cd4:	ldrb	w0, [x9, w8, uxtw]
   33cd8:	ret

0000000000033cdc <__gmpn_mullo_n@@Base>:
   33cdc:	stp	x29, x30, [sp, #-64]!
   33ce0:	stp	x22, x21, [sp, #32]
   33ce4:	stp	x20, x19, [sp, #48]
   33ce8:	mov	x19, x3
   33cec:	mov	x20, x2
   33cf0:	mov	x22, x1
   33cf4:	cmp	x3, #0x25
   33cf8:	mov	x21, x0
   33cfc:	str	x23, [sp, #16]
   33d00:	mov	x29, sp
   33d04:	b.le	33d78 <__gmpn_mullo_n@@Base+0x9c>
   33d08:	mov	x0, x19
   33d0c:	str	xzr, [x29, #24]
   33d10:	bl	33de0 <__gmpn_mullo_n@@Base+0x104>
   33d14:	lsl	x1, x0, #3
   33d18:	mov	w8, #0x7f00                	// #32512
   33d1c:	cmp	x1, x8
   33d20:	b.hi	33dc8 <__gmpn_mullo_n@@Base+0xec>  // b.pmore
   33d24:	add	x9, x1, #0xf
   33d28:	mov	x8, sp
   33d2c:	and	x9, x9, #0xfffffffffffffff0
   33d30:	sub	x23, x8, x9
   33d34:	mov	sp, x23
   33d38:	mov	w8, #0x186c                	// #6252
   33d3c:	cmp	x19, x8
   33d40:	b.le	33d90 <__gmpn_mullo_n@@Base+0xb4>
   33d44:	mov	x0, x23
   33d48:	mov	x1, x22
   33d4c:	mov	x2, x19
   33d50:	mov	x3, x20
   33d54:	mov	x4, x19
   33d58:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   33d5c:	mov	x0, x21
   33d60:	mov	x1, x23
   33d64:	mov	x2, x19
   33d68:	bl	cc10 <__gmpn_copyi@plt>
   33d6c:	ldr	x0, [x29, #24]
   33d70:	cbz	x0, 33db0 <__gmpn_mullo_n@@Base+0xd4>
   33d74:	b	33dd8 <__gmpn_mullo_n@@Base+0xfc>
   33d78:	mov	x0, x21
   33d7c:	mov	x1, x22
   33d80:	mov	x2, x20
   33d84:	mov	x3, x19
   33d88:	bl	d470 <__gmpn_mullo_basecase@plt>
   33d8c:	b	33db0 <__gmpn_mullo_n@@Base+0xd4>
   33d90:	mov	x0, x21
   33d94:	mov	x1, x22
   33d98:	mov	x2, x20
   33d9c:	mov	x3, x19
   33da0:	mov	x4, x23
   33da4:	bl	33de8 <__gmpn_mullo_n@@Base+0x10c>
   33da8:	ldr	x0, [x29, #24]
   33dac:	cbnz	x0, 33dd8 <__gmpn_mullo_n@@Base+0xfc>
   33db0:	mov	sp, x29
   33db4:	ldp	x20, x19, [sp, #48]
   33db8:	ldp	x22, x21, [sp, #32]
   33dbc:	ldr	x23, [sp, #16]
   33dc0:	ldp	x29, x30, [sp], #64
   33dc4:	ret
   33dc8:	add	x0, x29, #0x18
   33dcc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   33dd0:	mov	x23, x0
   33dd4:	b	33d38 <__gmpn_mullo_n@@Base+0x5c>
   33dd8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   33ddc:	b	33db0 <__gmpn_mullo_n@@Base+0xd4>
   33de0:	lsl	x0, x0, #1
   33de4:	ret
   33de8:	stp	x29, x30, [sp, #-80]!
   33dec:	stp	x24, x23, [sp, #32]
   33df0:	stp	x22, x21, [sp, #48]
   33df4:	stp	x20, x19, [sp, #64]
   33df8:	mov	x21, x4
   33dfc:	mov	x24, x3
   33e00:	mov	x20, x2
   33e04:	mov	x19, x1
   33e08:	cmp	x3, #0x45
   33e0c:	mov	x23, x0
   33e10:	str	x25, [sp, #16]
   33e14:	mov	x29, sp
   33e18:	b.le	33e40 <__gmpn_mullo_n@@Base+0x164>
   33e1c:	cmp	x24, #0x68
   33e20:	b.le	33e5c <__gmpn_mullo_n@@Base+0x180>
   33e24:	cmp	x24, #0x105
   33e28:	b.le	33f34 <__gmpn_mullo_n@@Base+0x258>
   33e2c:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   33e30:	movk	x8, #0xcccd
   33e34:	umulh	x8, x24, x8
   33e38:	lsr	x22, x8, #3
   33e3c:	b	33e70 <__gmpn_mullo_n@@Base+0x194>
   33e40:	mov	x9, #0xe38f                	// #58255
   33e44:	movk	x9, #0x8e38, lsl #16
   33e48:	mov	w8, #0xb                   	// #11
   33e4c:	movk	x9, #0x38e3, lsl #32
   33e50:	mul	x8, x24, x8
   33e54:	movk	x9, #0xe38e, lsl #48
   33e58:	b	33e68 <__gmpn_mullo_n@@Base+0x18c>
   33e5c:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   33e60:	add	x8, x24, x24, lsl #3
   33e64:	movk	x9, #0xcccd
   33e68:	umulh	x8, x8, x9
   33e6c:	lsr	x22, x8, #5
   33e70:	sub	x25, x24, x22
   33e74:	mov	x0, x21
   33e78:	mov	x1, x19
   33e7c:	mov	x2, x20
   33e80:	mov	x3, x25
   33e84:	bl	cb50 <__gmpn_mul_n@plt>
   33e88:	mov	x0, x23
   33e8c:	mov	x1, x21
   33e90:	mov	x2, x25
   33e94:	bl	cc10 <__gmpn_copyi@plt>
   33e98:	add	x24, x21, x24, lsl #3
   33e9c:	cmp	x22, #0x25
   33ea0:	add	x1, x19, x25, lsl #3
   33ea4:	mov	x0, x24
   33ea8:	mov	x2, x20
   33eac:	mov	x3, x22
   33eb0:	b.ls	33ec0 <__gmpn_mullo_n@@Base+0x1e4>  // b.plast
   33eb4:	mov	x4, x24
   33eb8:	bl	33de8 <__gmpn_mullo_n@@Base+0x10c>
   33ebc:	b	33ec4 <__gmpn_mullo_n@@Base+0x1e8>
   33ec0:	bl	d470 <__gmpn_mullo_basecase@plt>
   33ec4:	lsl	x25, x25, #3
   33ec8:	add	x23, x23, x25
   33ecc:	add	x1, x21, x25
   33ed0:	mov	x0, x23
   33ed4:	mov	x2, x24
   33ed8:	mov	x3, x22
   33edc:	bl	cc30 <__gmpn_add_n@plt>
   33ee0:	cmp	x22, #0x25
   33ee4:	add	x2, x20, x25
   33ee8:	mov	x0, x24
   33eec:	mov	x1, x19
   33ef0:	mov	x3, x22
   33ef4:	b.ls	33f04 <__gmpn_mullo_n@@Base+0x228>  // b.plast
   33ef8:	mov	x4, x24
   33efc:	bl	33de8 <__gmpn_mullo_n@@Base+0x10c>
   33f00:	b	33f08 <__gmpn_mullo_n@@Base+0x22c>
   33f04:	bl	d470 <__gmpn_mullo_basecase@plt>
   33f08:	mov	x0, x23
   33f0c:	mov	x1, x23
   33f10:	mov	x2, x24
   33f14:	mov	x3, x22
   33f18:	bl	cc30 <__gmpn_add_n@plt>
   33f1c:	ldp	x20, x19, [sp, #64]
   33f20:	ldp	x22, x21, [sp, #48]
   33f24:	ldp	x24, x23, [sp, #32]
   33f28:	ldr	x25, [sp, #16]
   33f2c:	ldp	x29, x30, [sp], #80
   33f30:	ret
   33f34:	mov	x9, #0xa41b                	// #42011
   33f38:	movk	x9, #0x1a41, lsl #16
   33f3c:	lsl	x8, x24, #3
   33f40:	movk	x9, #0x41a4, lsl #32
   33f44:	sub	x8, x8, x24
   33f48:	movk	x9, #0xa41a, lsl #48
   33f4c:	umulh	x9, x8, x9
   33f50:	sub	x8, x8, x9
   33f54:	add	x8, x9, x8, lsr #1
   33f58:	b	33e6c <__gmpn_mullo_n@@Base+0x190>

0000000000033f5c <__gmpn_mullo_basecase@@Base>:
   33f5c:	stp	x29, x30, [sp, #-80]!
   33f60:	stp	x24, x23, [sp, #32]
   33f64:	stp	x22, x21, [sp, #48]
   33f68:	stp	x20, x19, [sp, #64]
   33f6c:	mov	x22, x2
   33f70:	subs	x2, x3, #0x1
   33f74:	ldr	x8, [x1]
   33f78:	ldr	x9, [x22, x2, lsl #3]
   33f7c:	mov	x19, x0
   33f80:	str	x25, [sp, #16]
   33f84:	mov	x29, sp
   33f88:	mul	x24, x9, x8
   33f8c:	b.eq	33ff8 <__gmpn_mullo_basecase@@Base+0x9c>  // b.none
   33f90:	ldr	x23, [x22]
   33f94:	ldr	x25, [x1, x2, lsl #3]
   33f98:	mov	x21, x3
   33f9c:	mov	x0, x19
   33fa0:	mov	x3, x23
   33fa4:	mov	x20, x1
   33fa8:	bl	d670 <__gmpn_mul_1@plt>
   33fac:	add	x8, x0, x24
   33fb0:	cmp	x21, #0x3
   33fb4:	madd	x24, x25, x23, x8
   33fb8:	add	x19, x19, #0x8
   33fbc:	b.lt	33ff8 <__gmpn_mullo_basecase@@Base+0x9c>  // b.tstop
   33fc0:	sub	x21, x21, #0x2
   33fc4:	add	x23, x22, #0x8
   33fc8:	ldr	x22, [x23], #8
   33fcc:	ldr	x25, [x20, x21, lsl #3]
   33fd0:	mov	x0, x19
   33fd4:	mov	x1, x20
   33fd8:	mov	x2, x21
   33fdc:	mov	x3, x22
   33fe0:	bl	d5e0 <__gmpn_addmul_1@plt>
   33fe4:	add	x8, x0, x24
   33fe8:	subs	x21, x21, #0x1
   33fec:	madd	x24, x25, x22, x8
   33ff0:	add	x19, x19, #0x8
   33ff4:	b.gt	33fc8 <__gmpn_mullo_basecase@@Base+0x6c>
   33ff8:	str	x24, [x19]
   33ffc:	ldp	x20, x19, [sp, #64]
   34000:	ldp	x22, x21, [sp, #48]
   34004:	ldp	x24, x23, [sp, #32]
   34008:	ldr	x25, [sp, #16]
   3400c:	ldp	x29, x30, [sp], #80
   34010:	ret

0000000000034014 <__gmpn_sqrlo@@Base>:
   34014:	stp	x29, x30, [sp, #-48]!
   34018:	stp	x22, x21, [sp, #16]
   3401c:	stp	x20, x19, [sp, #32]
   34020:	mov	x29, sp
   34024:	sub	sp, sp, #0x10
   34028:	mov	x19, x2
   3402c:	mov	x20, x1
   34030:	cmp	x2, #0x3
   34034:	mov	x21, x0
   34038:	b.le	340b4 <__gmpn_sqrlo@@Base+0xa0>
   3403c:	cmp	x19, #0x42
   34040:	b.le	340cc <__gmpn_sqrlo@@Base+0xb8>
   34044:	mov	x0, x19
   34048:	stur	xzr, [x29, #-8]
   3404c:	bl	34128 <__gmpn_sqrlo@@Base+0x114>
   34050:	lsl	x1, x0, #3
   34054:	mov	w8, #0x7f00                	// #32512
   34058:	cmp	x1, x8
   3405c:	b.hi	34110 <__gmpn_sqrlo@@Base+0xfc>  // b.pmore
   34060:	add	x9, x1, #0xf
   34064:	mov	x8, sp
   34068:	and	x9, x9, #0xfffffffffffffff0
   3406c:	sub	x22, x8, x9
   34070:	mov	sp, x22
   34074:	mov	w8, #0x1477                	// #5239
   34078:	cmp	x19, x8
   3407c:	b.le	340e0 <__gmpn_sqrlo@@Base+0xcc>
   34080:	mov	x0, x22
   34084:	mov	x1, x20
   34088:	mov	x2, x19
   3408c:	mov	x3, x20
   34090:	mov	x4, x19
   34094:	bl	ce60 <__gmpn_nussbaumer_mul@plt>
   34098:	mov	x0, x21
   3409c:	mov	x1, x22
   340a0:	mov	x2, x19
   340a4:	bl	cc10 <__gmpn_copyi@plt>
   340a8:	ldur	x0, [x29, #-8]
   340ac:	cbz	x0, 340fc <__gmpn_sqrlo@@Base+0xe8>
   340b0:	b	34120 <__gmpn_sqrlo@@Base+0x10c>
   340b4:	mov	x0, x21
   340b8:	mov	x1, x20
   340bc:	mov	x2, x20
   340c0:	mov	x3, x19
   340c4:	bl	d470 <__gmpn_mullo_basecase@plt>
   340c8:	b	340fc <__gmpn_sqrlo@@Base+0xe8>
   340cc:	mov	x0, x21
   340d0:	mov	x1, x20
   340d4:	mov	x2, x19
   340d8:	bl	c1f0 <__gmpn_sqrlo_basecase@plt>
   340dc:	b	340fc <__gmpn_sqrlo@@Base+0xe8>
   340e0:	mov	x0, x21
   340e4:	mov	x1, x20
   340e8:	mov	x2, x19
   340ec:	mov	x3, x22
   340f0:	bl	34130 <__gmpn_sqrlo@@Base+0x11c>
   340f4:	ldur	x0, [x29, #-8]
   340f8:	cbnz	x0, 34120 <__gmpn_sqrlo@@Base+0x10c>
   340fc:	mov	sp, x29
   34100:	ldp	x20, x19, [sp, #32]
   34104:	ldp	x22, x21, [sp, #16]
   34108:	ldp	x29, x30, [sp], #48
   3410c:	ret
   34110:	sub	x0, x29, #0x8
   34114:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   34118:	mov	x22, x0
   3411c:	b	34074 <__gmpn_sqrlo@@Base+0x60>
   34120:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   34124:	b	340fc <__gmpn_sqrlo@@Base+0xe8>
   34128:	lsl	x0, x0, #1
   3412c:	ret
   34130:	stp	x29, x30, [sp, #-64]!
   34134:	stp	x22, x21, [sp, #32]
   34138:	stp	x20, x19, [sp, #48]
   3413c:	mov	x19, x3
   34140:	mov	x22, x2
   34144:	mov	x21, x1
   34148:	cmp	x2, #0x5f
   3414c:	mov	x20, x0
   34150:	stp	x24, x23, [sp, #16]
   34154:	mov	x29, sp
   34158:	b.le	34180 <__gmpn_sqrlo@@Base+0x16c>
   3415c:	cmp	x22, #0xd5
   34160:	b.le	3419c <__gmpn_sqrlo@@Base+0x188>
   34164:	cmp	x22, #0x171
   34168:	b.le	34228 <__gmpn_sqrlo@@Base+0x214>
   3416c:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   34170:	movk	x8, #0xcccd
   34174:	umulh	x8, x22, x8
   34178:	lsr	x23, x8, #3
   3417c:	b	341b0 <__gmpn_sqrlo@@Base+0x19c>
   34180:	mov	x9, #0xe38f                	// #58255
   34184:	movk	x9, #0x8e38, lsl #16
   34188:	mov	w8, #0xb                   	// #11
   3418c:	movk	x9, #0x38e3, lsl #32
   34190:	mul	x8, x22, x8
   34194:	movk	x9, #0xe38e, lsl #48
   34198:	b	341a8 <__gmpn_sqrlo@@Base+0x194>
   3419c:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   341a0:	add	x8, x22, x22, lsl #3
   341a4:	movk	x9, #0xcccd
   341a8:	umulh	x8, x8, x9
   341ac:	lsr	x23, x8, #5
   341b0:	sub	x24, x22, x23
   341b4:	mov	x0, x19
   341b8:	mov	x1, x21
   341bc:	mov	x2, x24
   341c0:	bl	ca90 <__gmpn_sqr@plt>
   341c4:	mov	x0, x20
   341c8:	mov	x1, x19
   341cc:	mov	x2, x24
   341d0:	bl	cc10 <__gmpn_copyi@plt>
   341d4:	add	x22, x19, x22, lsl #3
   341d8:	cmp	x23, #0x25
   341dc:	add	x1, x21, x24, lsl #3
   341e0:	mov	x0, x22
   341e4:	mov	x2, x21
   341e8:	mov	x3, x23
   341ec:	b.ls	341f8 <__gmpn_sqrlo@@Base+0x1e4>  // b.plast
   341f0:	bl	d090 <__gmpn_mullo_n@plt>
   341f4:	b	341fc <__gmpn_sqrlo@@Base+0x1e8>
   341f8:	bl	d470 <__gmpn_mullo_basecase@plt>
   341fc:	lsl	x8, x24, #3
   34200:	add	x0, x20, x8
   34204:	add	x1, x19, x8
   34208:	mov	x2, x22
   3420c:	mov	x3, x23
   34210:	bl	ce00 <__gmpn_addlsh1_n@plt>
   34214:	ldp	x20, x19, [sp, #48]
   34218:	ldp	x22, x21, [sp, #32]
   3421c:	ldp	x24, x23, [sp, #16]
   34220:	ldp	x29, x30, [sp], #64
   34224:	ret
   34228:	mov	x9, #0xa41b                	// #42011
   3422c:	movk	x9, #0x1a41, lsl #16
   34230:	lsl	x8, x22, #3
   34234:	movk	x9, #0x41a4, lsl #32
   34238:	sub	x8, x8, x22
   3423c:	movk	x9, #0xa41a, lsl #48
   34240:	umulh	x9, x8, x9
   34244:	sub	x8, x8, x9
   34248:	add	x8, x9, x8, lsr #1
   3424c:	b	341ac <__gmpn_sqrlo@@Base+0x198>

0000000000034250 <__gmpn_sqrlo_basecase@@Base>:
   34250:	stp	x29, x30, [sp, #-96]!
   34254:	stp	x28, x27, [sp, #16]
   34258:	stp	x26, x25, [sp, #32]
   3425c:	stp	x24, x23, [sp, #48]
   34260:	stp	x22, x21, [sp, #64]
   34264:	stp	x20, x19, [sp, #80]
   34268:	mov	x29, sp
   3426c:	sub	sp, sp, #0x240
   34270:	ldr	x24, [x1]
   34274:	sub	x19, x2, #0x1
   34278:	ldr	x20, [x1, x19, lsl #3]
   3427c:	sub	x25, x2, #0x2
   34280:	mov	x27, x2
   34284:	mov	x23, x1
   34288:	mov	x21, x0
   3428c:	add	x1, x1, #0x8
   34290:	add	x0, sp, #0x28
   34294:	mov	x2, x25
   34298:	mov	x3, x24
   3429c:	add	x22, sp, #0x28
   342a0:	bl	d670 <__gmpn_mul_1@plt>
   342a4:	cmp	x27, #0x5
   342a8:	madd	x28, x20, x24, x0
   342ac:	b.lt	34340 <__gmpn_sqrlo_basecase@@Base+0xf0>  // b.tstop
   342b0:	add	x8, x23, x27, lsl #3
   342b4:	stp	x25, x27, [sp, #8]
   342b8:	stp	x23, x21, [sp, #24]
   342bc:	mov	x20, xzr
   342c0:	add	x24, x22, #0x10
   342c4:	sub	x25, x27, #0x4
   342c8:	add	x26, x23, #0x10
   342cc:	sub	x23, x8, #0x10
   342d0:	mov	w22, #0x3                   	// #3
   342d4:	ldur	x27, [x26, #-8]
   342d8:	mov	x21, x19
   342dc:	ldr	x19, [x23, x20, lsl #3]
   342e0:	mov	x0, x24
   342e4:	mov	x1, x26
   342e8:	mov	x2, x25
   342ec:	mov	x3, x27
   342f0:	bl	d5e0 <__gmpn_addmul_1@plt>
   342f4:	add	x8, x0, x28
   342f8:	add	x22, x22, #0x2
   342fc:	add	x24, x24, #0x10
   34300:	sub	x25, x25, #0x2
   34304:	add	x26, x26, #0x8
   34308:	madd	x28, x19, x27, x8
   3430c:	mov	x19, x21
   34310:	cmp	x22, x21
   34314:	sub	x20, x20, #0x1
   34318:	b.lt	342d4 <__gmpn_sqrlo_basecase@@Base+0x84>  // b.tstop
   3431c:	ldp	x23, x21, [sp, #24]
   34320:	ldp	x25, x27, [sp, #8]
   34324:	mov	w8, #0x1                   	// #1
   34328:	sub	x8, x8, x20
   3432c:	tbz	w19, #0, 34348 <__gmpn_sqrlo_basecase@@Base+0xf8>
   34330:	add	x8, x23, x8, lsl #3
   34334:	ldp	x9, x8, [x8]
   34338:	mul	x8, x8, x9
   3433c:	b	3434c <__gmpn_sqrlo_basecase@@Base+0xfc>
   34340:	mov	w8, #0x1                   	// #1
   34344:	tbnz	w19, #0, 34330 <__gmpn_sqrlo_basecase@@Base+0xe0>
   34348:	mov	x8, xzr
   3434c:	add	x8, x8, x28
   34350:	add	x9, sp, #0x28
   34354:	cmp	x27, #0x2
   34358:	str	x8, [x9, x25, lsl #3]
   3435c:	asr	x8, x27, #1
   34360:	b.lt	3438c <__gmpn_sqrlo_basecase@@Base+0x13c>  // b.tstop
   34364:	mov	x9, xzr
   34368:	add	x10, x21, #0x8
   3436c:	ldr	x11, [x23, x9, lsl #3]
   34370:	add	x9, x9, #0x1
   34374:	umulh	x12, x11, x11
   34378:	cmp	x9, x8
   3437c:	mul	x11, x11, x11
   34380:	stp	x11, x12, [x10, #-8]
   34384:	add	x10, x10, #0x10
   34388:	b.lt	3436c <__gmpn_sqrlo_basecase@@Base+0x11c>  // b.tstop
   3438c:	tbz	w27, #0, 3439c <__gmpn_sqrlo_basecase@@Base+0x14c>
   34390:	ldr	x8, [x23, x8, lsl #3]
   34394:	mul	x8, x8, x8
   34398:	str	x8, [x21, x19, lsl #3]
   3439c:	add	x0, x21, #0x8
   343a0:	add	x2, sp, #0x28
   343a4:	mov	x1, x0
   343a8:	mov	x3, x19
   343ac:	bl	ce00 <__gmpn_addlsh1_n@plt>
   343b0:	add	sp, sp, #0x240
   343b4:	ldp	x20, x19, [sp, #80]
   343b8:	ldp	x22, x21, [sp, #64]
   343bc:	ldp	x24, x23, [sp, #48]
   343c0:	ldp	x26, x25, [sp, #32]
   343c4:	ldp	x28, x27, [sp, #16]
   343c8:	ldp	x29, x30, [sp], #96
   343cc:	ret

00000000000343d0 <__gmpn_toom22_mul@@Base>:
   343d0:	sub	sp, sp, #0x80
   343d4:	stp	x22, x21, [sp, #96]
   343d8:	asr	x21, x2, #1
   343dc:	sub	x22, x2, x21
   343e0:	stp	x29, x30, [sp, #32]
   343e4:	stp	x28, x27, [sp, #48]
   343e8:	stp	x26, x25, [sp, #64]
   343ec:	stp	x24, x23, [sp, #80]
   343f0:	stp	x20, x19, [sp, #112]
   343f4:	add	x29, sp, #0x20
   343f8:	mov	x28, x4
   343fc:	mov	x26, x3
   34400:	mov	x27, x1
   34404:	mov	x19, x0
   34408:	sub	x25, x4, x22
   3440c:	subs	x8, x21, x22
   34410:	add	x24, x0, x22, lsl #3
   34414:	stp	x8, x2, [sp]
   34418:	stur	x5, [x29, #-8]
   3441c:	b.ne	344a0 <__gmpn_toom22_mul@@Base+0xd0>  // b.any
   34420:	add	x20, x27, x22, lsl #3
   34424:	mov	x0, x27
   34428:	mov	x1, x20
   3442c:	mov	x2, x22
   34430:	mov	x23, x5
   34434:	bl	c570 <__gmpn_cmp@plt>
   34438:	tbnz	w0, #31, 3453c <__gmpn_toom22_mul@@Base+0x16c>
   3443c:	mov	x0, x19
   34440:	mov	x1, x27
   34444:	mov	x2, x20
   34448:	mov	x3, x22
   3444c:	bl	c420 <__gmpn_sub_n@plt>
   34450:	stur	wzr, [x29, #-12]
   34454:	subs	x20, x22, x25
   34458:	b.eq	344ec <__gmpn_toom22_mul@@Base+0x11c>  // b.none
   3445c:	add	x0, x26, x25, lsl #3
   34460:	mov	x1, x20
   34464:	bl	c010 <__gmpn_zero_p@plt>
   34468:	cbz	w0, 34484 <__gmpn_toom22_mul@@Base+0xb4>
   3446c:	add	x23, x26, x22, lsl #3
   34470:	mov	x0, x26
   34474:	mov	x1, x23
   34478:	mov	x2, x25
   3447c:	bl	c570 <__gmpn_cmp@plt>
   34480:	tbnz	w0, #31, 34590 <__gmpn_toom22_mul@@Base+0x1c0>
   34484:	add	x3, x26, x22, lsl #3
   34488:	mov	x0, x24
   3448c:	mov	x1, x26
   34490:	mov	x2, x22
   34494:	mov	x4, x25
   34498:	bl	d340 <__gmpn_sub@plt>
   3449c:	b	345d8 <__gmpn_toom22_mul@@Base+0x208>
   344a0:	ldr	x23, [x27, x21, lsl #3]
   344a4:	cbnz	x23, 344c0 <__gmpn_toom22_mul@@Base+0xf0>
   344a8:	add	x20, x27, x22, lsl #3
   344ac:	mov	x0, x27
   344b0:	mov	x1, x20
   344b4:	mov	x2, x21
   344b8:	bl	c570 <__gmpn_cmp@plt>
   344bc:	tbnz	w0, #31, 3481c <__gmpn_toom22_mul@@Base+0x44c>
   344c0:	add	x2, x27, x22, lsl #3
   344c4:	mov	x0, x19
   344c8:	mov	x1, x27
   344cc:	mov	x3, x21
   344d0:	bl	c420 <__gmpn_sub_n@plt>
   344d4:	sub	x8, x23, x0
   344d8:	stur	wzr, [x29, #-12]
   344dc:	str	x8, [x19, x21, lsl #3]
   344e0:	ldur	x23, [x29, #-8]
   344e4:	subs	x20, x22, x25
   344e8:	b.ne	3445c <__gmpn_toom22_mul@@Base+0x8c>  // b.any
   344ec:	add	x20, x26, x22, lsl #3
   344f0:	mov	x0, x26
   344f4:	mov	x1, x20
   344f8:	mov	x2, x22
   344fc:	bl	c570 <__gmpn_cmp@plt>
   34500:	tbnz	w0, #31, 34564 <__gmpn_toom22_mul@@Base+0x194>
   34504:	mov	x0, x24
   34508:	mov	x1, x26
   3450c:	mov	x2, x20
   34510:	mov	x3, x22
   34514:	bl	c420 <__gmpn_sub_n@plt>
   34518:	cmp	x22, #0xd
   3451c:	b.gt	345e4 <__gmpn_toom22_mul@@Base+0x214>
   34520:	mov	x0, x23
   34524:	mov	x1, x19
   34528:	mov	x2, x22
   3452c:	mov	x3, x24
   34530:	mov	x4, x22
   34534:	bl	c6e0 <__gmpn_mul_basecase@plt>
   34538:	b	34600 <__gmpn_toom22_mul@@Base+0x230>
   3453c:	mov	x0, x19
   34540:	mov	x1, x20
   34544:	mov	x2, x27
   34548:	mov	x3, x22
   3454c:	bl	c420 <__gmpn_sub_n@plt>
   34550:	mov	w8, #0x1                   	// #1
   34554:	stur	w8, [x29, #-12]
   34558:	subs	x20, x22, x25
   3455c:	b.eq	344ec <__gmpn_toom22_mul@@Base+0x11c>  // b.none
   34560:	b	3445c <__gmpn_toom22_mul@@Base+0x8c>
   34564:	mov	x0, x24
   34568:	mov	x1, x20
   3456c:	mov	x2, x26
   34570:	mov	x3, x22
   34574:	bl	c420 <__gmpn_sub_n@plt>
   34578:	ldur	w8, [x29, #-12]
   3457c:	eor	w8, w8, #0x1
   34580:	stur	w8, [x29, #-12]
   34584:	cmp	x22, #0xd
   34588:	b.gt	345e4 <__gmpn_toom22_mul@@Base+0x214>
   3458c:	b	34520 <__gmpn_toom22_mul@@Base+0x150>
   34590:	mov	x0, x24
   34594:	mov	x1, x23
   34598:	mov	x2, x26
   3459c:	mov	x3, x25
   345a0:	bl	c420 <__gmpn_sub_n@plt>
   345a4:	cbz	x20, 345cc <__gmpn_toom22_mul@@Base+0x1fc>
   345a8:	ldr	x9, [sp, #8]
   345ac:	add	x0, x19, x28, lsl #3
   345b0:	mov	w1, wzr
   345b4:	lsl	x8, x9, #1
   345b8:	sub	x8, x8, x28
   345bc:	and	x9, x9, #0x1ffffffffffffffe
   345c0:	sub	x8, x8, x9
   345c4:	lsl	x2, x8, #3
   345c8:	bl	c780 <memset@plt>
   345cc:	ldur	w8, [x29, #-12]
   345d0:	eor	w8, w8, #0x1
   345d4:	stur	w8, [x29, #-12]
   345d8:	ldur	x23, [x29, #-8]
   345dc:	cmp	x22, #0xd
   345e0:	b.le	34520 <__gmpn_toom22_mul@@Base+0x150>
   345e4:	add	x5, x23, x22, lsl #4
   345e8:	mov	x0, x23
   345ec:	mov	x1, x19
   345f0:	mov	x2, x22
   345f4:	mov	x3, x24
   345f8:	mov	x4, x22
   345fc:	bl	d630 <__gmpn_toom22_mul@plt>
   34600:	cmp	x21, x25
   34604:	lsl	x28, x22, #1
   34608:	b.le	34650 <__gmpn_toom22_mul@@Base+0x280>
   3460c:	cmp	x25, #0xd
   34610:	b.le	346a0 <__gmpn_toom22_mul@@Base+0x2d0>
   34614:	add	x8, x25, x25, lsl #2
   34618:	lsl	x9, x22, #4
   3461c:	lsl	x10, x22, #3
   34620:	add	x0, x19, x9
   34624:	add	x1, x27, x10
   34628:	add	x3, x26, x10
   3462c:	cmp	x8, x21, lsl #2
   34630:	add	x5, x23, x9
   34634:	mov	x2, x21
   34638:	mov	x4, x25
   3463c:	b.gt	34678 <__gmpn_toom22_mul@@Base+0x2a8>
   34640:	bl	ca00 <__gmpn_toom32_mul@plt>
   34644:	cmp	x22, #0xd
   34648:	b.le	34684 <__gmpn_toom22_mul@@Base+0x2b4>
   3464c:	b	346d0 <__gmpn_toom22_mul@@Base+0x300>
   34650:	ldr	x9, [sp, #8]
   34654:	lsl	x8, x22, #3
   34658:	add	x0, x19, x22, lsl #4
   3465c:	add	x1, x27, x8
   34660:	cmp	x9, #0x1b
   34664:	add	x3, x26, x8
   34668:	b.le	346bc <__gmpn_toom22_mul@@Base+0x2ec>
   3466c:	add	x5, x23, x28, lsl #3
   34670:	mov	x2, x21
   34674:	mov	x4, x21
   34678:	bl	d630 <__gmpn_toom22_mul@plt>
   3467c:	cmp	x22, #0xd
   34680:	b.gt	346d0 <__gmpn_toom22_mul@@Base+0x300>
   34684:	mov	x0, x19
   34688:	mov	x1, x27
   3468c:	mov	x2, x22
   34690:	mov	x3, x26
   34694:	mov	x4, x22
   34698:	bl	c6e0 <__gmpn_mul_basecase@plt>
   3469c:	b	346ec <__gmpn_toom22_mul@@Base+0x31c>
   346a0:	lsl	x8, x22, #3
   346a4:	add	x0, x19, x22, lsl #4
   346a8:	add	x1, x27, x8
   346ac:	add	x3, x26, x8
   346b0:	mov	x2, x21
   346b4:	mov	x4, x25
   346b8:	b	346c4 <__gmpn_toom22_mul@@Base+0x2f4>
   346bc:	mov	x2, x21
   346c0:	mov	x4, x21
   346c4:	bl	c6e0 <__gmpn_mul_basecase@plt>
   346c8:	cmp	x22, #0xd
   346cc:	b.le	34684 <__gmpn_toom22_mul@@Base+0x2b4>
   346d0:	add	x5, x23, x22, lsl #4
   346d4:	mov	x0, x19
   346d8:	mov	x1, x27
   346dc:	mov	x2, x22
   346e0:	mov	x3, x26
   346e4:	mov	x4, x22
   346e8:	bl	d630 <__gmpn_toom22_mul@plt>
   346ec:	add	x26, x19, x22, lsl #4
   346f0:	mov	x0, x26
   346f4:	mov	x1, x24
   346f8:	mov	x2, x26
   346fc:	mov	x3, x22
   34700:	bl	cc30 <__gmpn_add_n@plt>
   34704:	mov	x27, x0
   34708:	mov	x0, x24
   3470c:	mov	x1, x26
   34710:	mov	x2, x19
   34714:	mov	x3, x22
   34718:	bl	cc30 <__gmpn_add_n@plt>
   3471c:	ldr	x8, [sp]
   34720:	mov	x20, x0
   34724:	add	x3, x26, x22, lsl #3
   34728:	mov	x0, x26
   3472c:	add	x4, x8, x25
   34730:	mov	x1, x26
   34734:	mov	x2, x22
   34738:	bl	c970 <__gmpn_add@plt>
   3473c:	ldur	w8, [x29, #-12]
   34740:	add	x23, x0, x27
   34744:	cbz	w8, 34764 <__gmpn_toom22_mul@@Base+0x394>
   34748:	ldur	x2, [x29, #-8]
   3474c:	mov	x0, x24
   34750:	mov	x1, x24
   34754:	mov	x3, x28
   34758:	bl	cc30 <__gmpn_add_n@plt>
   3475c:	add	x8, x0, x23
   34760:	b	34784 <__gmpn_toom22_mul@@Base+0x3b4>
   34764:	ldur	x2, [x29, #-8]
   34768:	mov	x0, x24
   3476c:	mov	x1, x24
   34770:	mov	x3, x28
   34774:	bl	c420 <__gmpn_sub_n@plt>
   34778:	sub	x8, x23, x0
   3477c:	cmn	x8, #0x1
   34780:	b.eq	34840 <__gmpn_toom22_mul@@Base+0x470>  // b.none
   34784:	ldr	x9, [x26]
   34788:	add	x10, x20, x27
   3478c:	adds	x9, x9, x10
   34790:	str	x9, [x26]
   34794:	b.cc	347bc <__gmpn_toom22_mul@@Base+0x3ec>  // b.lo, b.ul, b.last
   34798:	ldr	x9, [sp, #8]
   3479c:	lsl	x9, x9, #1
   347a0:	sub	x9, x9, x21, lsl #1
   347a4:	add	x9, x19, x9, lsl #3
   347a8:	add	x9, x9, #0x8
   347ac:	ldr	x10, [x9]
   347b0:	adds	x10, x10, #0x1
   347b4:	str	x10, [x9], #8
   347b8:	b.cs	347ac <__gmpn_toom22_mul@@Base+0x3dc>  // b.hs, b.nlast
   347bc:	mov	w9, #0x18                  	// #24
   347c0:	mul	x9, x22, x9
   347c4:	ldr	x10, [x19, x9]
   347c8:	adds	x8, x10, x8
   347cc:	str	x8, [x19, x9]
   347d0:	b.cc	347fc <__gmpn_toom22_mul@@Base+0x42c>  // b.lo, b.ul, b.last
   347d4:	ldr	x8, [sp, #8]
   347d8:	add	x9, x21, x21, lsl #1
   347dc:	add	x8, x8, x8, lsl #1
   347e0:	sub	x8, x8, x9
   347e4:	add	x8, x19, x8, lsl #3
   347e8:	add	x8, x8, #0x8
   347ec:	ldr	x9, [x8]
   347f0:	adds	x9, x9, #0x1
   347f4:	str	x9, [x8], #8
   347f8:	b.cs	347ec <__gmpn_toom22_mul@@Base+0x41c>  // b.hs, b.nlast
   347fc:	ldp	x20, x19, [sp, #112]
   34800:	ldp	x22, x21, [sp, #96]
   34804:	ldp	x24, x23, [sp, #80]
   34808:	ldp	x26, x25, [sp, #64]
   3480c:	ldp	x28, x27, [sp, #48]
   34810:	ldp	x29, x30, [sp, #32]
   34814:	add	sp, sp, #0x80
   34818:	ret
   3481c:	mov	x0, x19
   34820:	mov	x1, x20
   34824:	mov	x2, x27
   34828:	mov	x3, x21
   3482c:	bl	c420 <__gmpn_sub_n@plt>
   34830:	mov	w8, #0x1                   	// #1
   34834:	str	xzr, [x19, x21, lsl #3]
   34838:	stur	w8, [x29, #-12]
   3483c:	b	344e0 <__gmpn_toom22_mul@@Base+0x110>
   34840:	lsl	x2, x22, #3
   34844:	mov	x0, x26
   34848:	mov	w1, wzr
   3484c:	bl	c780 <memset@plt>
   34850:	b	347fc <__gmpn_toom22_mul@@Base+0x42c>

0000000000034854 <__gmpn_toom32_mul@@Base>:
   34854:	sub	sp, sp, #0xd0
   34858:	add	x8, x4, x4, lsl #1
   3485c:	stp	x28, x27, [sp, #128]
   34860:	stp	x26, x25, [sp, #144]
   34864:	stp	x20, x19, [sp, #192]
   34868:	mov	x20, x5
   3486c:	mov	x26, x4
   34870:	mov	x28, x3
   34874:	mov	x27, x1
   34878:	cmp	x8, x2, lsl #1
   3487c:	mov	x19, x0
   34880:	stp	x29, x30, [sp, #112]
   34884:	stp	x24, x23, [sp, #160]
   34888:	stp	x22, x21, [sp, #176]
   3488c:	add	x29, sp, #0x70
   34890:	b.le	348a0 <__gmpn_toom32_mul@@Base+0x4c>
   34894:	sub	x8, x26, #0x1
   34898:	asr	x8, x8, #1
   3489c:	b	348b4 <__gmpn_toom32_mul@@Base+0x60>
   348a0:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   348a4:	sub	x8, x2, #0x1
   348a8:	movk	x9, #0xaaab
   348ac:	umulh	x8, x8, x9
   348b0:	lsr	x8, x8, #1
   348b4:	add	x22, x8, #0x1
   348b8:	lsl	x21, x22, #1
   348bc:	sub	x4, x2, x21
   348c0:	add	x3, x27, x22, lsl #4
   348c4:	mov	x0, x19
   348c8:	mov	x1, x27
   348cc:	mov	x2, x22
   348d0:	stur	x8, [x29, #-40]
   348d4:	sub	x25, x26, x22
   348d8:	str	x3, [sp, #32]
   348dc:	stur	x4, [x29, #-32]
   348e0:	bl	c970 <__gmpn_add@plt>
   348e4:	mov	x24, x0
   348e8:	stur	x21, [x29, #-24]
   348ec:	cbnz	x0, 34908 <__gmpn_toom32_mul@@Base+0xb4>
   348f0:	add	x23, x27, x22, lsl #3
   348f4:	mov	x0, x19
   348f8:	mov	x1, x23
   348fc:	mov	x2, x22
   34900:	bl	c570 <__gmpn_cmp@plt>
   34904:	tbnz	w0, #31, 34a40 <__gmpn_toom32_mul@@Base+0x1ec>
   34908:	add	x0, x19, x21, lsl #3
   3490c:	add	x2, x27, x22, lsl #3
   34910:	mov	x1, x19
   34914:	mov	x3, x22
   34918:	bl	c420 <__gmpn_sub_n@plt>
   3491c:	sub	x8, x24, x0
   34920:	stur	wzr, [x29, #-12]
   34924:	str	x8, [sp, #40]
   34928:	lsl	x21, x22, #3
   3492c:	add	x2, x27, x21
   34930:	mov	x0, x19
   34934:	mov	x1, x19
   34938:	mov	x3, x22
   3493c:	str	x27, [sp, #56]
   34940:	bl	cc30 <__gmpn_add_n@plt>
   34944:	add	x8, x0, x24
   34948:	add	x23, x19, x21
   3494c:	mov	x24, x28
   34950:	subs	x27, x22, x25
   34954:	add	x28, x28, x21
   34958:	mov	x0, x23
   3495c:	mov	x1, x24
   34960:	str	x8, [sp, #48]
   34964:	stp	x21, x28, [sp, #16]
   34968:	str	x24, [sp, #8]
   3496c:	b.ne	349bc <__gmpn_toom32_mul@@Base+0x168>  // b.any
   34970:	mov	x2, x28
   34974:	mov	x3, x22
   34978:	mov	x27, x28
   3497c:	bl	cc30 <__gmpn_add_n@plt>
   34980:	mov	x28, x0
   34984:	mov	x0, x24
   34988:	mov	x1, x27
   3498c:	mov	x2, x22
   34990:	bl	c570 <__gmpn_cmp@plt>
   34994:	ldur	x26, [x29, #-24]
   34998:	mov	w9, #0x18                  	// #24
   3499c:	mov	w8, w0
   349a0:	madd	x0, x22, x9, x19
   349a4:	tbnz	w8, #31, 34a1c <__gmpn_toom32_mul@@Base+0x1c8>
   349a8:	mov	x1, x24
   349ac:	mov	x2, x27
   349b0:	mov	x3, x22
   349b4:	bl	c420 <__gmpn_sub_n@plt>
   349b8:	b	34a38 <__gmpn_toom32_mul@@Base+0x1e4>
   349bc:	mov	x2, x22
   349c0:	mov	x3, x28
   349c4:	mov	x4, x25
   349c8:	bl	c970 <__gmpn_add@plt>
   349cc:	mov	x21, x0
   349d0:	add	x0, x24, x25, lsl #3
   349d4:	mov	x1, x27
   349d8:	bl	c010 <__gmpn_zero_p@plt>
   349dc:	cbz	w0, 349f4 <__gmpn_toom32_mul@@Base+0x1a0>
   349e0:	mov	x0, x24
   349e4:	mov	x1, x28
   349e8:	mov	x2, x25
   349ec:	bl	c570 <__gmpn_cmp@plt>
   349f0:	tbnz	w0, #31, 34a64 <__gmpn_toom32_mul@@Base+0x210>
   349f4:	mov	w8, #0x18                  	// #24
   349f8:	madd	x0, x22, x8, x19
   349fc:	mov	x1, x24
   34a00:	mov	x2, x22
   34a04:	mov	x3, x28
   34a08:	mov	x4, x25
   34a0c:	bl	d340 <__gmpn_sub@plt>
   34a10:	ldp	x27, x26, [x29, #-32]
   34a14:	ldur	x24, [x29, #-40]
   34a18:	b	34ab8 <__gmpn_toom32_mul@@Base+0x264>
   34a1c:	mov	x1, x27
   34a20:	mov	x2, x24
   34a24:	mov	x3, x22
   34a28:	bl	c420 <__gmpn_sub_n@plt>
   34a2c:	ldur	w8, [x29, #-12]
   34a30:	eor	w8, w8, #0x1
   34a34:	stur	w8, [x29, #-12]
   34a38:	ldp	x24, x27, [x29, #-40]
   34a3c:	b	34abc <__gmpn_toom32_mul@@Base+0x268>
   34a40:	add	x0, x19, x21, lsl #3
   34a44:	mov	x1, x23
   34a48:	mov	x2, x19
   34a4c:	mov	x3, x22
   34a50:	bl	c420 <__gmpn_sub_n@plt>
   34a54:	mov	w8, #0x1                   	// #1
   34a58:	str	xzr, [sp, #40]
   34a5c:	stur	w8, [x29, #-12]
   34a60:	b	34928 <__gmpn_toom32_mul@@Base+0xd4>
   34a64:	mov	w8, #0x18                  	// #24
   34a68:	madd	x0, x22, x8, x19
   34a6c:	mov	x1, x28
   34a70:	mov	x2, x24
   34a74:	mov	x3, x25
   34a78:	str	x0, [sp]
   34a7c:	bl	c420 <__gmpn_sub_n@plt>
   34a80:	ldur	x24, [x29, #-40]
   34a84:	cbz	x27, 34aa8 <__gmpn_toom32_mul@@Base+0x254>
   34a88:	ldr	x8, [sp]
   34a8c:	mov	w1, wzr
   34a90:	add	x0, x8, x25, lsl #3
   34a94:	lsl	x8, x24, #1
   34a98:	sub	x8, x8, x26
   34a9c:	lsl	x8, x8, #3
   34aa0:	add	x2, x8, #0x10
   34aa4:	bl	c780 <memset@plt>
   34aa8:	ldur	w8, [x29, #-12]
   34aac:	ldp	x27, x26, [x29, #-32]
   34ab0:	eor	w8, w8, #0x1
   34ab4:	stur	w8, [x29, #-12]
   34ab8:	mov	x28, x21
   34abc:	ldr	x21, [sp, #48]
   34ac0:	mov	x0, x20
   34ac4:	mov	x1, x19
   34ac8:	mov	x2, x23
   34acc:	mov	x3, x22
   34ad0:	bl	cb50 <__gmpn_mul_n@plt>
   34ad4:	cmp	x21, #0x2
   34ad8:	b.eq	34b00 <__gmpn_toom32_mul@@Base+0x2ac>  // b.none
   34adc:	cmp	x21, #0x1
   34ae0:	b.ne	34b1c <__gmpn_toom32_mul@@Base+0x2c8>  // b.any
   34ae4:	add	x0, x20, x22, lsl #3
   34ae8:	mov	x1, x0
   34aec:	mov	x2, x23
   34af0:	mov	x3, x22
   34af4:	bl	cc30 <__gmpn_add_n@plt>
   34af8:	add	x21, x0, x28
   34afc:	b	34b20 <__gmpn_toom32_mul@@Base+0x2cc>
   34b00:	add	x0, x20, x22, lsl #3
   34b04:	mov	x1, x0
   34b08:	mov	x2, x23
   34b0c:	mov	x3, x22
   34b10:	bl	ce00 <__gmpn_addlsh1_n@plt>
   34b14:	add	x21, x0, x28, lsl #1
   34b18:	b	34b20 <__gmpn_toom32_mul@@Base+0x2cc>
   34b1c:	mov	x21, xzr
   34b20:	stur	x25, [x29, #-48]
   34b24:	mov	x25, x24
   34b28:	stur	x23, [x29, #-8]
   34b2c:	cbz	x28, 34b48 <__gmpn_toom32_mul@@Base+0x2f4>
   34b30:	add	x0, x20, x22, lsl #3
   34b34:	mov	x1, x0
   34b38:	mov	x2, x19
   34b3c:	mov	x3, x22
   34b40:	bl	cc30 <__gmpn_add_n@plt>
   34b44:	add	x21, x0, x21
   34b48:	lsl	x23, x26, #3
   34b4c:	add	x8, x22, x22, lsl #1
   34b50:	str	x21, [x20, x23]
   34b54:	add	x24, x19, x23
   34b58:	add	x21, x19, x8, lsl #3
   34b5c:	mov	x0, x19
   34b60:	mov	x1, x24
   34b64:	mov	x2, x21
   34b68:	mov	x3, x22
   34b6c:	str	x8, [sp, #48]
   34b70:	bl	cb50 <__gmpn_mul_n@plt>
   34b74:	ldr	x8, [sp, #40]
   34b78:	cbz	x8, 34b94 <__gmpn_toom32_mul@@Base+0x340>
   34b7c:	ldur	x0, [x29, #-8]
   34b80:	mov	x2, x21
   34b84:	mov	x3, x22
   34b88:	mov	x1, x0
   34b8c:	bl	cc30 <__gmpn_add_n@plt>
   34b90:	b	34b98 <__gmpn_toom32_mul@@Base+0x344>
   34b94:	mov	x0, xzr
   34b98:	ldur	w8, [x29, #-12]
   34b9c:	orr	x3, x26, #0x1
   34ba0:	str	x0, [x24]
   34ba4:	cbz	w8, 34bc0 <__gmpn_toom32_mul@@Base+0x36c>
   34ba8:	mov	x0, x20
   34bac:	mov	x1, x20
   34bb0:	mov	x2, x19
   34bb4:	mov	x26, x21
   34bb8:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   34bbc:	b	34bd4 <__gmpn_toom32_mul@@Base+0x380>
   34bc0:	mov	x0, x20
   34bc4:	mov	x1, x20
   34bc8:	mov	x2, x19
   34bcc:	mov	x26, x21
   34bd0:	bl	cb10 <__gmpn_rsh1add_n@plt>
   34bd4:	ldr	x21, [x24]
   34bd8:	add	x28, x20, x22, lsl #3
   34bdc:	mov	x0, x24
   34be0:	mov	x1, x20
   34be4:	mov	x2, x28
   34be8:	mov	x3, x22
   34bec:	bl	cc30 <__gmpn_add_n@plt>
   34bf0:	ldr	x8, [x20, x23]
   34bf4:	ldr	x9, [x28]
   34bf8:	str	x28, [sp, #40]
   34bfc:	add	x8, x8, x0
   34c00:	add	x8, x9, x8
   34c04:	str	x8, [x28]
   34c08:	ldr	x9, [x20, x23]
   34c0c:	add	x9, x9, x0
   34c10:	cmp	x8, x9
   34c14:	b.cs	34c30 <__gmpn_toom32_mul@@Base+0x3dc>  // b.hs, b.nlast
   34c18:	add	x8, x20, x25, lsl #3
   34c1c:	add	x8, x8, #0x10
   34c20:	ldr	x9, [x8]
   34c24:	adds	x9, x9, #0x1
   34c28:	str	x9, [x8], #8
   34c2c:	b.cs	34c20 <__gmpn_toom32_mul@@Base+0x3cc>  // b.hs, b.nlast
   34c30:	ldur	x23, [x29, #-48]
   34c34:	ldur	w8, [x29, #-12]
   34c38:	cbz	w8, 34c9c <__gmpn_toom32_mul@@Base+0x448>
   34c3c:	mov	x0, x20
   34c40:	mov	x1, x20
   34c44:	mov	x2, x19
   34c48:	mov	x3, x22
   34c4c:	bl	cc30 <__gmpn_add_n@plt>
   34c50:	ldur	x2, [x29, #-8]
   34c54:	mov	x4, x0
   34c58:	mov	x0, x24
   34c5c:	mov	x1, x24
   34c60:	mov	x3, x22
   34c64:	bl	d060 <__gmpn_add_nc@plt>
   34c68:	ldp	x2, x10, [sp, #8]
   34c6c:	add	x9, x0, x21
   34c70:	ldr	x8, [x20, x10]
   34c74:	adds	x8, x8, x9
   34c78:	str	x8, [x20, x10]
   34c7c:	b.cc	34cf8 <__gmpn_toom32_mul@@Base+0x4a4>  // b.lo, b.ul, b.last
   34c80:	add	x8, x20, x25, lsl #3
   34c84:	add	x8, x8, #0x10
   34c88:	ldr	x9, [x8]
   34c8c:	adds	x9, x9, #0x1
   34c90:	str	x9, [x8], #8
   34c94:	b.cs	34c88 <__gmpn_toom32_mul@@Base+0x434>  // b.hs, b.nlast
   34c98:	b	34cf8 <__gmpn_toom32_mul@@Base+0x4a4>
   34c9c:	mov	x0, x20
   34ca0:	mov	x1, x20
   34ca4:	mov	x2, x19
   34ca8:	mov	x3, x22
   34cac:	bl	c420 <__gmpn_sub_n@plt>
   34cb0:	ldur	x2, [x29, #-8]
   34cb4:	mov	x4, x0
   34cb8:	mov	x0, x24
   34cbc:	mov	x1, x24
   34cc0:	mov	x3, x22
   34cc4:	bl	c8f0 <__gmpn_sub_nc@plt>
   34cc8:	ldp	x2, x10, [sp, #8]
   34ccc:	add	x9, x0, x21
   34cd0:	ldr	x8, [x20, x10]
   34cd4:	subs	x8, x8, x9
   34cd8:	str	x8, [x20, x10]
   34cdc:	b.cs	34cf8 <__gmpn_toom32_mul@@Base+0x4a4>  // b.hs, b.nlast
   34ce0:	add	x8, x20, x25, lsl #3
   34ce4:	add	x8, x8, #0x10
   34ce8:	ldr	x9, [x8]
   34cec:	sub	x10, x9, #0x1
   34cf0:	str	x10, [x8], #8
   34cf4:	cbz	x9, 34ce8 <__gmpn_toom32_mul@@Base+0x494>
   34cf8:	ldr	x1, [sp, #56]
   34cfc:	mov	x0, x19
   34d00:	mov	x3, x22
   34d04:	bl	cb50 <__gmpn_mul_n@plt>
   34d08:	mov	x28, x19
   34d0c:	mov	x0, x26
   34d10:	cmp	x27, x23
   34d14:	b.le	34d28 <__gmpn_toom32_mul@@Base+0x4d4>
   34d18:	ldp	x3, x1, [sp, #24]
   34d1c:	mov	x2, x27
   34d20:	mov	x4, x23
   34d24:	b	34d34 <__gmpn_toom32_mul@@Base+0x4e0>
   34d28:	ldp	x1, x3, [sp, #24]
   34d2c:	mov	x2, x23
   34d30:	mov	x4, x27
   34d34:	bl	cea0 <__gmpn_mul@plt>
   34d38:	ldur	x21, [x29, #-8]
   34d3c:	mov	x2, x26
   34d40:	mov	x3, x22
   34d44:	mov	x27, x23
   34d48:	mov	x0, x21
   34d4c:	mov	x1, x21
   34d50:	bl	c420 <__gmpn_sub_n@plt>
   34d54:	ldur	x23, [x29, #-24]
   34d58:	mov	x25, x0
   34d5c:	mov	x0, x24
   34d60:	mov	x1, x24
   34d64:	ldr	x19, [x20, x23, lsl #3]
   34d68:	mov	x2, x28
   34d6c:	mov	x3, x22
   34d70:	mov	x4, x25
   34d74:	bl	c8f0 <__gmpn_sub_nc@plt>
   34d78:	ldr	x1, [sp, #40]
   34d7c:	mov	x4, x0
   34d80:	mov	x0, x26
   34d84:	mov	x2, x21
   34d88:	mov	x3, x22
   34d8c:	bl	c8f0 <__gmpn_sub_nc@plt>
   34d90:	ldr	x2, [sp, #48]
   34d94:	mov	x26, x0
   34d98:	mov	x0, x21
   34d9c:	mov	x1, x21
   34da0:	mov	x3, x20
   34da4:	mov	x4, x22
   34da8:	bl	c970 <__gmpn_add@plt>
   34dac:	ldur	x8, [x29, #-32]
   34db0:	add	x8, x8, x27
   34db4:	subs	x4, x8, x22
   34db8:	b.le	34e4c <__gmpn_toom32_mul@@Base+0x5f8>
   34dbc:	add	x8, x19, x25
   34dc0:	sub	x8, x8, x26
   34dc4:	add	x20, x28, x22, lsl #5
   34dc8:	add	x21, x8, x0
   34dcc:	mov	x0, x24
   34dd0:	mov	x1, x24
   34dd4:	mov	x2, x23
   34dd8:	mov	x3, x20
   34ddc:	bl	d340 <__gmpn_sub@plt>
   34de0:	subs	x8, x21, x0
   34de4:	b.mi	34e18 <__gmpn_toom32_mul@@Base+0x5c4>  // b.first
   34de8:	ldr	x9, [x20]
   34dec:	adds	x8, x9, x8
   34df0:	str	x8, [x20]
   34df4:	b.cc	34e4c <__gmpn_toom32_mul@@Base+0x5f8>  // b.lo, b.ul, b.last
   34df8:	ldur	x8, [x29, #-40]
   34dfc:	add	x8, x28, x8, lsl #5
   34e00:	add	x8, x8, #0x28
   34e04:	ldr	x9, [x8]
   34e08:	adds	x9, x9, #0x1
   34e0c:	str	x9, [x8], #8
   34e10:	b.cs	34e04 <__gmpn_toom32_mul@@Base+0x5b0>  // b.hs, b.nlast
   34e14:	b	34e4c <__gmpn_toom32_mul@@Base+0x5f8>
   34e18:	ldr	x9, [x20]
   34e1c:	neg	x10, x8
   34e20:	add	x8, x9, x8
   34e24:	cmp	x9, x10
   34e28:	str	x8, [x20]
   34e2c:	b.cs	34e4c <__gmpn_toom32_mul@@Base+0x5f8>  // b.hs, b.nlast
   34e30:	ldur	x8, [x29, #-40]
   34e34:	add	x8, x28, x8, lsl #5
   34e38:	add	x8, x8, #0x28
   34e3c:	ldr	x9, [x8]
   34e40:	sub	x10, x9, #0x1
   34e44:	str	x10, [x8], #8
   34e48:	cbz	x9, 34e3c <__gmpn_toom32_mul@@Base+0x5e8>
   34e4c:	ldp	x20, x19, [sp, #192]
   34e50:	ldp	x22, x21, [sp, #176]
   34e54:	ldp	x24, x23, [sp, #160]
   34e58:	ldp	x26, x25, [sp, #144]
   34e5c:	ldp	x28, x27, [sp, #128]
   34e60:	ldp	x29, x30, [sp, #112]
   34e64:	add	sp, sp, #0xd0
   34e68:	ret

0000000000034e6c <__gmpn_toom42_mul@@Base>:
   34e6c:	stp	x29, x30, [sp, #-96]!
   34e70:	stp	x28, x27, [sp, #16]
   34e74:	stp	x26, x25, [sp, #32]
   34e78:	stp	x24, x23, [sp, #48]
   34e7c:	stp	x22, x21, [sp, #64]
   34e80:	stp	x20, x19, [sp, #80]
   34e84:	mov	x29, sp
   34e88:	sub	sp, sp, #0x60
   34e8c:	add	x8, x2, #0x3
   34e90:	add	x9, x4, #0x1
   34e94:	cmp	x2, x4, lsl #1
   34e98:	asr	x8, x8, #2
   34e9c:	asr	x9, x9, #1
   34ea0:	mov	w10, #0x30                  	// #48
   34ea4:	csel	x23, x9, x8, lt  // lt = tstop
   34ea8:	mul	x8, x23, x10
   34eac:	mov	x20, x1
   34eb0:	add	x22, x23, x23, lsl #1
   34eb4:	add	x1, x8, #0x28
   34eb8:	mov	w8, #0x7f00                	// #32512
   34ebc:	mov	x27, x4
   34ec0:	sub	x24, x2, x22
   34ec4:	cmp	x1, x8
   34ec8:	stur	x5, [x29, #-96]
   34ecc:	stp	x3, x0, [x29, #-24]
   34ed0:	stur	xzr, [x29, #-8]
   34ed4:	b.hi	352b4 <__gmpn_toom42_mul@@Base+0x448>  // b.pmore
   34ed8:	add	x9, x1, #0xf
   34edc:	mov	x8, sp
   34ee0:	and	x9, x9, #0xfffffffffffffff0
   34ee4:	sub	x0, x8, x9
   34ee8:	mov	sp, x0
   34eec:	add	x8, x23, #0x1
   34ef0:	stur	x8, [x29, #-40]
   34ef4:	lsl	x8, x8, #3
   34ef8:	add	x21, x0, x8
   34efc:	add	x19, x21, x8
   34f00:	ldur	x5, [x29, #-16]
   34f04:	add	x28, x19, x8
   34f08:	add	x8, x28, x8
   34f0c:	stur	x8, [x29, #-72]
   34f10:	add	x8, x8, x23, lsl #3
   34f14:	mov	x1, x21
   34f18:	mov	x2, x20
   34f1c:	mov	x3, x23
   34f20:	mov	x4, x24
   34f24:	sub	x25, x27, x23
   34f28:	stp	x0, x8, [x29, #-56]
   34f2c:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   34f30:	add	x26, x20, x23, lsl #4
   34f34:	and	w8, w0, #0x1
   34f38:	add	x2, x20, x22, lsl #3
   34f3c:	mov	x0, x19
   34f40:	mov	x1, x26
   34f44:	mov	x3, x24
   34f48:	stur	w8, [x29, #-60]
   34f4c:	stur	x2, [x29, #-88]
   34f50:	bl	ce00 <__gmpn_addlsh1_n@plt>
   34f54:	subs	x2, x23, x24
   34f58:	mov	x22, x0
   34f5c:	stur	x24, [x29, #-32]
   34f60:	b.eq	34f7c <__gmpn_toom42_mul@@Base+0x110>  // b.none
   34f64:	lsl	x8, x24, #3
   34f68:	add	x0, x19, x8
   34f6c:	add	x1, x26, x8
   34f70:	mov	x3, x22
   34f74:	bl	c150 <__gmpn_add_1@plt>
   34f78:	mov	x22, x0
   34f7c:	lsl	x24, x23, #3
   34f80:	add	x1, x20, x24
   34f84:	mov	x0, x19
   34f88:	mov	x2, x19
   34f8c:	mov	x3, x23
   34f90:	bl	ce00 <__gmpn_addlsh1_n@plt>
   34f94:	add	x22, x0, x22, lsl #1
   34f98:	mov	x0, x19
   34f9c:	mov	x1, x20
   34fa0:	mov	x2, x19
   34fa4:	mov	x3, x23
   34fa8:	stur	x20, [x29, #-80]
   34fac:	bl	ce00 <__gmpn_addlsh1_n@plt>
   34fb0:	ldur	x20, [x29, #-24]
   34fb4:	add	x8, x0, x22, lsl #1
   34fb8:	subs	x26, x23, x25
   34fbc:	mov	x0, x28
   34fc0:	add	x22, x20, x24
   34fc4:	mov	x1, x20
   34fc8:	str	x8, [x19, x24]
   34fcc:	b.ne	35014 <__gmpn_toom42_mul@@Base+0x1a8>  // b.any
   34fd0:	mov	x2, x22
   34fd4:	mov	x3, x23
   34fd8:	bl	cc30 <__gmpn_add_n@plt>
   34fdc:	str	x0, [x28, x23, lsl #3]
   34fe0:	mov	x0, x20
   34fe4:	mov	x1, x22
   34fe8:	mov	x2, x23
   34fec:	bl	c570 <__gmpn_cmp@plt>
   34ff0:	ldur	x27, [x29, #-96]
   34ff4:	tbnz	w0, #31, 35070 <__gmpn_toom42_mul@@Base+0x204>
   34ff8:	ldur	x0, [x29, #-72]
   34ffc:	mov	x1, x20
   35000:	mov	x2, x22
   35004:	mov	x3, x23
   35008:	mov	x20, x0
   3500c:	bl	c420 <__gmpn_sub_n@plt>
   35010:	b	350e4 <__gmpn_toom42_mul@@Base+0x278>
   35014:	mov	x2, x23
   35018:	mov	x3, x22
   3501c:	mov	x4, x25
   35020:	bl	c970 <__gmpn_add@plt>
   35024:	str	x0, [x28, x23, lsl #3]
   35028:	add	x0, x20, x25, lsl #3
   3502c:	mov	x1, x26
   35030:	bl	c010 <__gmpn_zero_p@plt>
   35034:	cbz	w0, 3504c <__gmpn_toom42_mul@@Base+0x1e0>
   35038:	mov	x0, x20
   3503c:	mov	x1, x22
   35040:	mov	x2, x25
   35044:	bl	c570 <__gmpn_cmp@plt>
   35048:	tbnz	w0, #31, 35090 <__gmpn_toom42_mul@@Base+0x224>
   3504c:	ldur	x0, [x29, #-72]
   35050:	mov	x1, x20
   35054:	mov	x2, x23
   35058:	mov	x3, x22
   3505c:	mov	x4, x25
   35060:	mov	x20, x0
   35064:	bl	d340 <__gmpn_sub@plt>
   35068:	ldur	x27, [x29, #-96]
   3506c:	b	350e4 <__gmpn_toom42_mul@@Base+0x278>
   35070:	ldur	x0, [x29, #-72]
   35074:	mov	x1, x22
   35078:	mov	x2, x20
   3507c:	mov	x3, x23
   35080:	mov	x20, x0
   35084:	bl	c420 <__gmpn_sub_n@plt>
   35088:	ldur	w8, [x29, #-60]
   3508c:	b	350dc <__gmpn_toom42_mul@@Base+0x270>
   35090:	ldur	x0, [x29, #-72]
   35094:	mov	x1, x22
   35098:	mov	x2, x20
   3509c:	mov	x3, x25
   350a0:	bl	c420 <__gmpn_sub_n@plt>
   350a4:	cbz	x26, 350d0 <__gmpn_toom42_mul@@Base+0x264>
   350a8:	ldur	x11, [x29, #-56]
   350ac:	mov	w8, #0x18                  	// #24
   350b0:	lsl	x9, x27, #3
   350b4:	madd	x8, x23, x8, x9
   350b8:	lsl	x10, x23, #4
   350bc:	add	x8, x8, x11
   350c0:	add	x0, x8, #0x20
   350c4:	sub	x2, x10, x9
   350c8:	mov	w1, wzr
   350cc:	bl	c780 <memset@plt>
   350d0:	ldur	w8, [x29, #-60]
   350d4:	ldur	x27, [x29, #-96]
   350d8:	ldur	x20, [x29, #-72]
   350dc:	eor	w8, w8, #0x1
   350e0:	stur	w8, [x29, #-60]
   350e4:	ldp	x0, x2, [x29, #-48]
   350e8:	mov	x1, x28
   350ec:	mov	x3, x22
   350f0:	mov	x4, x25
   350f4:	lsl	x26, x23, #1
   350f8:	bl	c970 <__gmpn_add@plt>
   350fc:	mov	x0, x27
   35100:	mov	x1, x21
   35104:	mov	x2, x20
   35108:	mov	x3, x23
   3510c:	bl	cb50 <__gmpn_mul_n@plt>
   35110:	ldr	x8, [x21, x23, lsl #3]
   35114:	cbz	x8, 35130 <__gmpn_toom42_mul@@Base+0x2c4>
   35118:	add	x0, x27, x23, lsl #3
   3511c:	mov	x1, x0
   35120:	mov	x2, x20
   35124:	mov	x3, x23
   35128:	bl	cc30 <__gmpn_add_n@plt>
   3512c:	b	35134 <__gmpn_toom42_mul@@Base+0x2c8>
   35130:	mov	x0, xzr
   35134:	add	x20, x27, x26, lsl #3
   35138:	str	x0, [x20], #8
   3513c:	ldp	x2, x3, [x29, #-48]
   35140:	mov	x0, x20
   35144:	mov	x1, x19
   35148:	bl	cb50 <__gmpn_mul_n@plt>
   3514c:	ldur	x8, [x29, #-16]
   35150:	ldur	x4, [x29, #-32]
   35154:	add	x19, x8, x23, lsl #5
   35158:	cmp	x4, x25
   3515c:	mov	x0, x19
   35160:	b.le	35178 <__gmpn_toom42_mul@@Base+0x30c>
   35164:	ldur	x1, [x29, #-88]
   35168:	mov	x2, x4
   3516c:	mov	x3, x22
   35170:	mov	x4, x25
   35174:	b	35184 <__gmpn_toom42_mul@@Base+0x318>
   35178:	ldur	x3, [x29, #-88]
   3517c:	mov	x1, x22
   35180:	mov	x2, x25
   35184:	bl	cea0 <__gmpn_mul@plt>
   35188:	ldur	x8, [x29, #-16]
   3518c:	ldur	x22, [x29, #-56]
   35190:	ldr	x19, [x19]
   35194:	mov	x2, x28
   35198:	add	x21, x8, x26, lsl #3
   3519c:	mov	x0, x21
   351a0:	mov	x1, x22
   351a4:	mov	x3, x23
   351a8:	bl	cb50 <__gmpn_mul_n@plt>
   351ac:	ldr	x8, [x22, x23, lsl #3]
   351b0:	cmp	x8, #0x3
   351b4:	b.eq	351e8 <__gmpn_toom42_mul@@Base+0x37c>  // b.none
   351b8:	cmp	x8, #0x2
   351bc:	b.eq	3520c <__gmpn_toom42_mul@@Base+0x3a0>  // b.none
   351c0:	cmp	x8, #0x1
   351c4:	b.ne	3522c <__gmpn_toom42_mul@@Base+0x3c0>  // b.any
   351c8:	ldr	x22, [x28, x24]
   351cc:	add	x0, x21, x24
   351d0:	mov	x1, x0
   351d4:	mov	x2, x28
   351d8:	mov	x3, x23
   351dc:	bl	cc30 <__gmpn_add_n@plt>
   351e0:	add	x22, x0, x22
   351e4:	b	35230 <__gmpn_toom42_mul@@Base+0x3c4>
   351e8:	ldr	x8, [x28, x24]
   351ec:	add	x0, x21, x24
   351f0:	mov	w3, #0x3                   	// #3
   351f4:	mov	x1, x28
   351f8:	mov	x2, x23
   351fc:	add	x22, x8, x8, lsl #1
   35200:	bl	d5e0 <__gmpn_addmul_1@plt>
   35204:	add	x22, x22, x0
   35208:	b	35230 <__gmpn_toom42_mul@@Base+0x3c4>
   3520c:	ldr	x22, [x28, x24]
   35210:	add	x0, x21, x24
   35214:	mov	x1, x0
   35218:	mov	x2, x28
   3521c:	mov	x3, x23
   35220:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35224:	add	x22, x0, x22, lsl #1
   35228:	b	35230 <__gmpn_toom42_mul@@Base+0x3c4>
   3522c:	mov	x22, xzr
   35230:	ldur	x24, [x29, #-32]
   35234:	ldr	x8, [x28, x23, lsl #3]
   35238:	cbz	x8, 35254 <__gmpn_toom42_mul@@Base+0x3e8>
   3523c:	ldur	x2, [x29, #-56]
   35240:	add	x0, x21, x23, lsl #3
   35244:	mov	x1, x0
   35248:	mov	x3, x23
   3524c:	bl	cc30 <__gmpn_add_n@plt>
   35250:	add	x22, x0, x22
   35254:	str	x22, [x21, x26, lsl #3]
   35258:	ldp	x2, x21, [x29, #-24]
   3525c:	ldur	x1, [x29, #-80]
   35260:	mov	x3, x23
   35264:	mov	x0, x21
   35268:	bl	cb50 <__gmpn_mul_n@plt>
   3526c:	ldur	w5, [x29, #-60]
   35270:	add	x4, x24, x25
   35274:	mov	x0, x21
   35278:	mov	x1, x20
   3527c:	mov	x2, x27
   35280:	mov	x3, x23
   35284:	mov	x6, x19
   35288:	bl	cbe0 <__gmpn_toom_interpolate_5pts@plt>
   3528c:	ldur	x0, [x29, #-8]
   35290:	cbnz	x0, 352c0 <__gmpn_toom42_mul@@Base+0x454>
   35294:	mov	sp, x29
   35298:	ldp	x20, x19, [sp, #80]
   3529c:	ldp	x22, x21, [sp, #64]
   352a0:	ldp	x24, x23, [sp, #48]
   352a4:	ldp	x26, x25, [sp, #32]
   352a8:	ldp	x28, x27, [sp, #16]
   352ac:	ldp	x29, x30, [sp], #96
   352b0:	ret
   352b4:	sub	x0, x29, #0x8
   352b8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   352bc:	b	34eec <__gmpn_toom42_mul@@Base+0x80>
   352c0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   352c4:	b	35294 <__gmpn_toom42_mul@@Base+0x428>

00000000000352c8 <__gmpn_toom52_mul@@Base>:
   352c8:	sub	sp, sp, #0xe0
   352cc:	add	x8, x4, x4, lsl #2
   352d0:	stp	x26, x25, [sp, #160]
   352d4:	stp	x22, x21, [sp, #192]
   352d8:	stp	x20, x19, [sp, #208]
   352dc:	mov	x19, x5
   352e0:	mov	x26, x3
   352e4:	mov	x22, x1
   352e8:	cmp	x8, x2, lsl #1
   352ec:	mov	x20, x0
   352f0:	stp	x29, x30, [sp, #128]
   352f4:	stp	x28, x27, [sp, #144]
   352f8:	stp	x24, x23, [sp, #176]
   352fc:	add	x29, sp, #0x80
   35300:	b.le	35310 <__gmpn_toom52_mul@@Base+0x48>
   35304:	sub	x8, x4, #0x1
   35308:	asr	x8, x8, #1
   3530c:	b	35324 <__gmpn_toom52_mul@@Base+0x5c>
   35310:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   35314:	sub	x8, x2, #0x1
   35318:	movk	x9, #0xcccd
   3531c:	umulh	x8, x8, x9
   35320:	lsr	x8, x8, #2
   35324:	add	x23, x8, #0x1
   35328:	stur	x8, [x29, #-16]
   3532c:	add	x8, x23, x23, lsl #1
   35330:	add	x9, x19, x23, lsl #5
   35334:	lsl	x8, x8, #3
   35338:	lsl	x10, x23, #2
   3533c:	str	x9, [sp, #64]
   35340:	add	x1, x9, #0x20
   35344:	add	x9, x20, x8
   35348:	sub	x25, x2, x10
   3534c:	add	x8, x19, x8
   35350:	add	x28, x9, #0x18
   35354:	str	x4, [sp]
   35358:	sub	x24, x4, x23
   3535c:	add	x6, x8, #0x18
   35360:	mov	w2, #0x4                   	// #4
   35364:	mov	x0, x28
   35368:	mov	x3, x22
   3536c:	mov	x4, x23
   35370:	mov	x5, x25
   35374:	str	x10, [sp, #16]
   35378:	stp	x6, x1, [sp, #48]
   3537c:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   35380:	and	w8, w0, #0x2
   35384:	subs	x21, x23, x24
   35388:	add	x27, x26, x23, lsl #3
   3538c:	mov	x0, x20
   35390:	mov	x1, x26
   35394:	stur	w8, [x29, #-8]
   35398:	stp	x25, x22, [x29, #-40]
   3539c:	stur	x28, [x29, #-56]
   353a0:	b.ne	353e8 <__gmpn_toom52_mul@@Base+0x120>  // b.any
   353a4:	mov	x2, x27
   353a8:	mov	x3, x23
   353ac:	bl	cc30 <__gmpn_add_n@plt>
   353b0:	str	x0, [x20, x23, lsl #3]
   353b4:	mov	x0, x26
   353b8:	mov	x1, x27
   353bc:	mov	x2, x23
   353c0:	bl	c570 <__gmpn_cmp@plt>
   353c4:	add	x9, x19, x23, lsl #4
   353c8:	mov	w8, w0
   353cc:	add	x0, x9, #0x10
   353d0:	tbnz	w8, #31, 35444 <__gmpn_toom52_mul@@Base+0x17c>
   353d4:	mov	x1, x26
   353d8:	mov	x2, x27
   353dc:	mov	x3, x23
   353e0:	bl	c420 <__gmpn_sub_n@plt>
   353e4:	b	3543c <__gmpn_toom52_mul@@Base+0x174>
   353e8:	mov	x2, x23
   353ec:	mov	x3, x27
   353f0:	mov	x4, x24
   353f4:	bl	c970 <__gmpn_add@plt>
   353f8:	str	x0, [x20, x23, lsl #3]
   353fc:	add	x0, x26, x24, lsl #3
   35400:	mov	x1, x21
   35404:	bl	c010 <__gmpn_zero_p@plt>
   35408:	cbz	w0, 35420 <__gmpn_toom52_mul@@Base+0x158>
   3540c:	mov	x0, x26
   35410:	mov	x1, x27
   35414:	mov	x2, x24
   35418:	bl	c570 <__gmpn_cmp@plt>
   3541c:	tbnz	w0, #31, 356a4 <__gmpn_toom52_mul@@Base+0x3dc>
   35420:	add	x8, x19, x23, lsl #4
   35424:	add	x0, x8, #0x10
   35428:	mov	x1, x26
   3542c:	mov	x2, x23
   35430:	mov	x3, x27
   35434:	mov	x4, x24
   35438:	bl	d340 <__gmpn_sub@plt>
   3543c:	ldur	w22, [x29, #-8]
   35440:	b	3545c <__gmpn_toom52_mul@@Base+0x194>
   35444:	mov	x1, x27
   35448:	mov	x2, x26
   3544c:	mov	x3, x23
   35450:	bl	c420 <__gmpn_sub_n@plt>
   35454:	ldur	w22, [x29, #-8]
   35458:	orr	w22, w22, #0x1
   3545c:	ldur	x25, [x29, #-16]
   35460:	add	x8, x20, x23, lsl #4
   35464:	add	x0, x8, #0x10
   35468:	add	x2, x25, #0x2
   3546c:	mov	x1, x20
   35470:	mov	x3, x27
   35474:	mov	x4, x24
   35478:	lsl	x28, x23, #1
   3547c:	str	x0, [sp, #24]
   35480:	stp	x2, x8, [sp, #32]
   35484:	bl	c970 <__gmpn_add@plt>
   35488:	add	x8, x20, x23, lsl #3
   3548c:	add	x21, x8, #0x8
   35490:	stur	x26, [x29, #-24]
   35494:	stur	x27, [x29, #-48]
   35498:	stur	x21, [x29, #-8]
   3549c:	str	x24, [sp, #8]
   354a0:	tbnz	w22, #0, 354ec <__gmpn_toom52_mul@@Base+0x224>
   354a4:	add	x8, x19, x28, lsl #3
   354a8:	str	xzr, [x21, x23, lsl #3]
   354ac:	subs	x25, x23, x24
   354b0:	add	x21, x8, #0x10
   354b4:	b.ne	3551c <__gmpn_toom52_mul@@Base+0x254>  // b.any
   354b8:	mov	x25, x27
   354bc:	mov	x0, x21
   354c0:	mov	x1, x25
   354c4:	mov	x2, x23
   354c8:	mov	x27, x20
   354cc:	bl	c570 <__gmpn_cmp@plt>
   354d0:	tbnz	w0, #31, 356f4 <__gmpn_toom52_mul@@Base+0x42c>
   354d4:	ldur	x0, [x29, #-8]
   354d8:	mov	x1, x21
   354dc:	mov	x2, x25
   354e0:	mov	x3, x23
   354e4:	bl	c420 <__gmpn_sub_n@plt>
   354e8:	b	3555c <__gmpn_toom52_mul@@Base+0x294>
   354ec:	add	x8, x19, x28, lsl #3
   354f0:	mov	x3, x27
   354f4:	add	x1, x8, #0x10
   354f8:	mov	x0, x21
   354fc:	mov	x2, x23
   35500:	mov	x4, x24
   35504:	mov	x27, x20
   35508:	bl	c970 <__gmpn_add@plt>
   3550c:	str	x0, [x21, x23, lsl #3]
   35510:	ldr	x8, [sp, #16]
   35514:	eor	w20, w22, #0x2
   35518:	b	35564 <__gmpn_toom52_mul@@Base+0x29c>
   3551c:	add	x0, x21, x24, lsl #3
   35520:	mov	x1, x25
   35524:	bl	c010 <__gmpn_zero_p@plt>
   35528:	cbz	w0, 35540 <__gmpn_toom52_mul@@Base+0x278>
   3552c:	mov	x0, x21
   35530:	mov	x1, x27
   35534:	mov	x2, x24
   35538:	bl	c570 <__gmpn_cmp@plt>
   3553c:	tbnz	w0, #31, 3570c <__gmpn_toom52_mul@@Base+0x444>
   35540:	ldur	x0, [x29, #-8]
   35544:	mov	x3, x27
   35548:	mov	x1, x21
   3554c:	mov	x2, x23
   35550:	mov	x4, x24
   35554:	mov	x27, x20
   35558:	bl	d340 <__gmpn_sub@plt>
   3555c:	ldr	x8, [sp, #16]
   35560:	mov	w20, w22
   35564:	ldr	x22, [sp, #48]
   35568:	ldp	x21, x25, [x29, #-40]
   3556c:	lsl	x26, x8, #3
   35570:	add	x8, x27, x26
   35574:	add	x24, x8, #0x20
   35578:	mov	w2, #0x4                   	// #4
   3557c:	mov	x0, x24
   35580:	mov	x1, x22
   35584:	mov	x3, x25
   35588:	mov	x4, x23
   3558c:	mov	x5, x21
   35590:	mov	x6, x19
   35594:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   35598:	and	w8, w0, #0x1
   3559c:	add	x28, x19, x28, lsl #3
   355a0:	stur	x19, [x29, #-16]
   355a4:	mov	x0, x19
   355a8:	ldr	x19, [sp, #32]
   355ac:	eor	w8, w8, w20
   355b0:	add	x3, x28, #0x10
   355b4:	mov	x1, x22
   355b8:	mov	x2, x19
   355bc:	mov	x4, x23
   355c0:	str	w8, [sp, #16]
   355c4:	bl	cea0 <__gmpn_mul@plt>
   355c8:	ldr	x1, [sp, #56]
   355cc:	ldur	x2, [x29, #-8]
   355d0:	add	x28, x28, #0x8
   355d4:	mov	x0, x28
   355d8:	mov	x3, x19
   355dc:	bl	cb50 <__gmpn_mul_n@plt>
   355e0:	ldr	x8, [sp, #64]
   355e4:	ldur	x1, [x29, #-56]
   355e8:	ldr	x2, [sp, #24]
   355ec:	mov	x3, x19
   355f0:	add	x22, x8, #0x10
   355f4:	mov	x0, x22
   355f8:	bl	cb50 <__gmpn_mul_n@plt>
   355fc:	ldr	x0, [sp, #40]
   35600:	mov	x1, x24
   35604:	mov	x2, x27
   35608:	mov	x3, x19
   3560c:	bl	cb50 <__gmpn_mul_n@plt>
   35610:	ldr	x19, [sp, #8]
   35614:	mov	w8, #0x28                  	// #40
   35618:	mov	x20, x27
   3561c:	madd	x0, x23, x8, x27
   35620:	cmp	x21, x19
   35624:	add	x3, x25, x26
   35628:	b.le	35640 <__gmpn_toom52_mul@@Base+0x378>
   3562c:	mov	x1, x3
   35630:	ldur	x3, [x29, #-48]
   35634:	mov	x2, x21
   35638:	mov	x4, x19
   3563c:	b	3564c <__gmpn_toom52_mul@@Base+0x384>
   35640:	ldur	x1, [x29, #-48]
   35644:	mov	x2, x19
   35648:	mov	x4, x21
   3564c:	bl	cea0 <__gmpn_mul@plt>
   35650:	ldur	x2, [x29, #-24]
   35654:	mov	x0, x20
   35658:	mov	x1, x25
   3565c:	mov	x3, x23
   35660:	bl	cb50 <__gmpn_mul_n@plt>
   35664:	ldr	w2, [sp, #16]
   35668:	ldur	x3, [x29, #-16]
   3566c:	add	x6, x21, x19
   35670:	mov	x0, x20
   35674:	mov	x1, x23
   35678:	mov	x4, x28
   3567c:	mov	x5, x22
   35680:	bl	cac0 <__gmpn_toom_interpolate_6pts@plt>
   35684:	ldp	x20, x19, [sp, #208]
   35688:	ldp	x22, x21, [sp, #192]
   3568c:	ldp	x24, x23, [sp, #176]
   35690:	ldp	x26, x25, [sp, #160]
   35694:	ldp	x28, x27, [sp, #144]
   35698:	ldp	x29, x30, [sp, #128]
   3569c:	add	sp, sp, #0xe0
   356a0:	ret
   356a4:	add	x8, x19, x23, lsl #4
   356a8:	add	x22, x8, #0x10
   356ac:	mov	x0, x22
   356b0:	mov	x1, x27
   356b4:	mov	x2, x26
   356b8:	mov	x3, x24
   356bc:	bl	c420 <__gmpn_sub_n@plt>
   356c0:	ldur	x25, [x29, #-16]
   356c4:	cbz	x21, 356e8 <__gmpn_toom52_mul@@Base+0x420>
   356c8:	ldr	x9, [sp]
   356cc:	lsl	x8, x25, #1
   356d0:	add	x0, x22, x24, lsl #3
   356d4:	mov	w1, wzr
   356d8:	sub	x8, x8, x9
   356dc:	lsl	x8, x8, #3
   356e0:	add	x2, x8, #0x10
   356e4:	bl	c780 <memset@plt>
   356e8:	ldur	w22, [x29, #-8]
   356ec:	orr	w22, w22, #0x1
   356f0:	b	35460 <__gmpn_toom52_mul@@Base+0x198>
   356f4:	ldur	x0, [x29, #-8]
   356f8:	mov	x1, x25
   356fc:	mov	x2, x21
   35700:	mov	x3, x23
   35704:	bl	c420 <__gmpn_sub_n@plt>
   35708:	b	35510 <__gmpn_toom52_mul@@Base+0x248>
   3570c:	mov	x1, x27
   35710:	mov	x27, x20
   35714:	ldur	x20, [x29, #-8]
   35718:	mov	x2, x21
   3571c:	mov	x3, x24
   35720:	mov	x0, x20
   35724:	bl	c420 <__gmpn_sub_n@plt>
   35728:	cbz	x25, 35510 <__gmpn_toom52_mul@@Base+0x248>
   3572c:	ldur	x8, [x29, #-16]
   35730:	ldr	x9, [sp]
   35734:	add	x0, x20, x24, lsl #3
   35738:	mov	w1, wzr
   3573c:	lsl	x8, x8, #1
   35740:	sub	x8, x8, x9
   35744:	lsl	x8, x8, #3
   35748:	add	x2, x8, #0x10
   3574c:	bl	c780 <memset@plt>
   35750:	b	35510 <__gmpn_toom52_mul@@Base+0x248>

0000000000035754 <__gmpn_toom62_mul@@Base>:
   35754:	stp	x29, x30, [sp, #-96]!
   35758:	stp	x28, x27, [sp, #16]
   3575c:	stp	x26, x25, [sp, #32]
   35760:	stp	x24, x23, [sp, #48]
   35764:	stp	x22, x21, [sp, #64]
   35768:	stp	x20, x19, [sp, #80]
   3576c:	mov	x29, sp
   35770:	sub	sp, sp, #0xe0
   35774:	add	x8, x4, x4, lsl #1
   35778:	mov	x22, x1
   3577c:	cmp	x8, x2
   35780:	mov	x24, x0
   35784:	stur	x3, [x29, #-16]
   35788:	stur	x5, [x29, #-88]
   3578c:	b.le	3579c <__gmpn_toom62_mul@@Base+0x48>
   35790:	sub	x8, x4, #0x1
   35794:	asr	x20, x8, #1
   35798:	b	357b0 <__gmpn_toom62_mul@@Base+0x5c>
   3579c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   357a0:	sub	x8, x2, #0x1
   357a4:	movk	x9, #0xaaab
   357a8:	umulh	x8, x8, x9
   357ac:	lsr	x20, x8, #2
   357b0:	add	x21, x20, #0x1
   357b4:	add	x8, x20, #0x2
   357b8:	sub	x10, x4, x21
   357bc:	stp	x8, x10, [x29, #-40]
   357c0:	lsl	x8, x8, #3
   357c4:	lsl	x9, x21, #2
   357c8:	add	x8, x8, #0xf
   357cc:	stur	x9, [x29, #-192]
   357d0:	add	x19, x9, x21
   357d4:	mov	x9, sp
   357d8:	and	x8, x8, #0xfffffffffffffff0
   357dc:	sub	x25, x2, x19
   357e0:	sub	x0, x9, x8
   357e4:	stur	x4, [x29, #-216]
   357e8:	stur	x2, [x29, #-96]
   357ec:	mov	sp, x0
   357f0:	mov	x9, sp
   357f4:	sub	x1, x9, x8
   357f8:	mov	sp, x1
   357fc:	mov	x9, sp
   35800:	sub	x23, x9, x8
   35804:	mov	sp, x23
   35808:	mov	x9, sp
   3580c:	sub	x27, x9, x8
   35810:	mov	sp, x27
   35814:	mov	x9, sp
   35818:	sub	x28, x9, x8
   3581c:	mov	sp, x28
   35820:	mov	x9, sp
   35824:	sub	x9, x9, x8
   35828:	stur	x9, [x29, #-144]
   3582c:	mov	sp, x9
   35830:	lsl	x26, x21, #3
   35834:	add	x10, x26, #0xf
   35838:	mov	x9, sp
   3583c:	and	x10, x10, #0xfffffffffffffff0
   35840:	sub	x9, x9, x10
   35844:	stp	x9, x0, [x29, #-80]
   35848:	mov	sp, x9
   3584c:	mov	x9, sp
   35850:	sub	x9, x9, x8
   35854:	stur	x9, [x29, #-48]
   35858:	mov	sp, x9
   3585c:	mov	x9, sp
   35860:	sub	x9, x9, x8
   35864:	stur	x9, [x29, #-64]
   35868:	mov	sp, x9
   3586c:	mov	x9, sp
   35870:	sub	x8, x9, x8
   35874:	stur	x8, [x29, #-160]
   35878:	mov	sp, x8
   3587c:	mov	w2, #0x5                   	// #5
   35880:	mov	x3, x22
   35884:	mov	x4, x21
   35888:	mov	x5, x25
   3588c:	mov	x6, x24
   35890:	stur	x1, [x29, #-128]
   35894:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   35898:	stur	w0, [x29, #-116]
   3589c:	mov	w2, #0x5                   	// #5
   358a0:	mov	x0, x23
   358a4:	mov	x1, x27
   358a8:	mov	x3, x22
   358ac:	mov	x4, x21
   358b0:	mov	x5, x25
   358b4:	mov	x6, x24
   358b8:	stur	x23, [x29, #-152]
   358bc:	stur	x27, [x29, #-136]
   358c0:	mov	x23, x22
   358c4:	stur	x24, [x29, #-56]
   358c8:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   358cc:	stur	w0, [x29, #-180]
   358d0:	add	x1, x22, x26
   358d4:	mov	x0, x28
   358d8:	mov	x2, x22
   358dc:	mov	x3, x21
   358e0:	stur	x26, [x29, #-24]
   358e4:	bl	ce00 <__gmpn_addlsh1_n@plt>
   358e8:	mov	x24, x0
   358ec:	add	x1, x22, x21, lsl #4
   358f0:	mov	x0, x28
   358f4:	mov	x2, x28
   358f8:	mov	x3, x21
   358fc:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35900:	mov	w8, #0x18                  	// #24
   35904:	add	x24, x0, x24, lsl #1
   35908:	madd	x1, x21, x8, x22
   3590c:	mov	x0, x28
   35910:	mov	x2, x28
   35914:	mov	x3, x21
   35918:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3591c:	add	x24, x0, x24, lsl #1
   35920:	add	x1, x22, x21, lsl #5
   35924:	mov	x0, x28
   35928:	mov	x2, x28
   3592c:	mov	x3, x21
   35930:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35934:	cmp	x20, x25
   35938:	add	x26, x0, x24, lsl #1
   3593c:	add	x1, x22, x19, lsl #3
   35940:	mov	x0, x28
   35944:	mov	x2, x28
   35948:	stur	x20, [x29, #-200]
   3594c:	stp	x19, x25, [x29, #-112]
   35950:	b.ge	3596c <__gmpn_toom62_mul@@Base+0x218>  // b.tcont
   35954:	mov	x3, x21
   35958:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3595c:	ldur	x24, [x29, #-16]
   35960:	add	x8, x0, x26, lsl #1
   35964:	str	x8, [x28, x21, lsl #3]
   35968:	b	359dc <__gmpn_toom62_mul@@Base+0x288>
   3596c:	mov	x3, x25
   35970:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35974:	mov	x8, x25
   35978:	add	x25, x28, x25, lsl #3
   3597c:	mov	x24, x0
   35980:	sub	x2, x21, x8
   35984:	mov	w3, #0x1                   	// #1
   35988:	mov	x0, x25
   3598c:	mov	x1, x25
   35990:	bl	c2d0 <__gmpn_lshift@plt>
   35994:	add	x8, x0, x26, lsl #1
   35998:	str	x8, [x28, x21, lsl #3]
   3599c:	ldr	x8, [x25]
   359a0:	ldur	x10, [x29, #-200]
   359a4:	adds	x8, x8, x24
   359a8:	ldur	x24, [x29, #-16]
   359ac:	str	x8, [x25]
   359b0:	b.cc	359dc <__gmpn_toom62_mul@@Base+0x288>  // b.lo, b.ul, b.last
   359b4:	ldur	x8, [x29, #-96]
   359b8:	mov	w9, #0x28                  	// #40
   359bc:	lsl	x8, x8, #3
   359c0:	msub	x8, x10, x9, x8
   359c4:	add	x8, x8, x28
   359c8:	sub	x8, x8, #0x20
   359cc:	ldr	x9, [x8]
   359d0:	adds	x9, x9, #0x1
   359d4:	str	x9, [x8], #8
   359d8:	b.cs	359cc <__gmpn_toom62_mul@@Base+0x278>  // b.hs, b.nlast
   359dc:	ldur	x26, [x29, #-32]
   359e0:	ldur	x27, [x29, #-144]
   359e4:	ldp	x25, x22, [x29, #-88]
   359e8:	add	x20, x24, x21, lsl #3
   359ec:	subs	x19, x21, x26
   359f0:	mov	x0, x27
   359f4:	mov	x1, x24
   359f8:	stur	x19, [x29, #-208]
   359fc:	b.ne	35a44 <__gmpn_toom62_mul@@Base+0x2f0>  // b.any
   35a00:	mov	x2, x20
   35a04:	mov	x3, x21
   35a08:	bl	cc30 <__gmpn_add_n@plt>
   35a0c:	str	x0, [x27, x21, lsl #3]
   35a10:	mov	x0, x24
   35a14:	mov	x1, x20
   35a18:	mov	x2, x21
   35a1c:	bl	c570 <__gmpn_cmp@plt>
   35a20:	mov	x19, x22
   35a24:	tbnz	w0, #31, 35aac <__gmpn_toom62_mul@@Base+0x358>
   35a28:	mov	x0, x22
   35a2c:	mov	x1, x24
   35a30:	mov	x2, x20
   35a34:	mov	x3, x21
   35a38:	bl	c420 <__gmpn_sub_n@plt>
   35a3c:	mov	w26, wzr
   35a40:	b	35b18 <__gmpn_toom62_mul@@Base+0x3c4>
   35a44:	mov	x2, x21
   35a48:	mov	x3, x20
   35a4c:	mov	x4, x26
   35a50:	bl	c970 <__gmpn_add@plt>
   35a54:	str	x0, [x27, x21, lsl #3]
   35a58:	add	x0, x24, x26, lsl #3
   35a5c:	mov	x1, x19
   35a60:	bl	c010 <__gmpn_zero_p@plt>
   35a64:	cbz	w0, 35a7c <__gmpn_toom62_mul@@Base+0x328>
   35a68:	mov	x0, x24
   35a6c:	mov	x1, x20
   35a70:	mov	x2, x26
   35a74:	bl	c570 <__gmpn_cmp@plt>
   35a78:	tbnz	w0, #31, 35ac4 <__gmpn_toom62_mul@@Base+0x370>
   35a7c:	mov	x0, x22
   35a80:	mov	x1, x24
   35a84:	mov	x2, x21
   35a88:	mov	x3, x20
   35a8c:	mov	x4, x26
   35a90:	mov	x19, x22
   35a94:	bl	d340 <__gmpn_sub@plt>
   35a98:	ldur	x24, [x29, #-64]
   35a9c:	mov	x22, x26
   35aa0:	mov	w26, wzr
   35aa4:	mov	w8, wzr
   35aa8:	b	35b20 <__gmpn_toom62_mul@@Base+0x3cc>
   35aac:	mov	x0, x22
   35ab0:	mov	x1, x20
   35ab4:	mov	x2, x24
   35ab8:	mov	x3, x21
   35abc:	bl	c420 <__gmpn_sub_n@plt>
   35ac0:	b	35b14 <__gmpn_toom62_mul@@Base+0x3c0>
   35ac4:	mov	x0, x22
   35ac8:	mov	x1, x20
   35acc:	mov	x2, x24
   35ad0:	mov	x3, x26
   35ad4:	mov	x19, x22
   35ad8:	bl	c420 <__gmpn_sub_n@plt>
   35adc:	ldur	x8, [x29, #-208]
   35ae0:	cbz	x8, 35b14 <__gmpn_toom62_mul@@Base+0x3c0>
   35ae4:	ldur	x10, [x29, #-216]
   35ae8:	ldur	x11, [x29, #-200]
   35aec:	mov	w1, wzr
   35af0:	lsl	x8, x10, #3
   35af4:	lsl	x9, x11, #1
   35af8:	sub	x8, x8, x11, lsl #3
   35afc:	sub	x9, x9, x10
   35b00:	add	x8, x8, x19
   35b04:	lsl	x9, x9, #3
   35b08:	sub	x0, x8, #0x8
   35b0c:	add	x2, x9, #0x10
   35b10:	bl	c780 <memset@plt>
   35b14:	mov	w26, #0x2                   	// #2
   35b18:	ldur	x24, [x29, #-64]
   35b1c:	ldur	x22, [x29, #-32]
   35b20:	ldp	x0, x2, [x29, #-48]
   35b24:	lsl	x8, x21, #1
   35b28:	mov	x1, x27
   35b2c:	mov	x3, x20
   35b30:	mov	x4, x22
   35b34:	stur	x8, [x29, #-64]
   35b38:	bl	c970 <__gmpn_add@plt>
   35b3c:	stp	x20, x23, [x29, #-176]
   35b40:	cbz	w26, 35b7c <__gmpn_toom62_mul@@Base+0x428>
   35b44:	mov	x0, x24
   35b48:	mov	x1, x19
   35b4c:	mov	x2, x21
   35b50:	mov	x3, x20
   35b54:	mov	x4, x22
   35b58:	mov	w23, w26
   35b5c:	mov	x26, x25
   35b60:	bl	c970 <__gmpn_add@plt>
   35b64:	orr	w23, w23, #0x1
   35b68:	str	x0, [x24, x21, lsl #3]
   35b6c:	stur	w23, [x29, #-96]
   35b70:	ldur	x25, [x29, #-192]
   35b74:	mov	x23, x19
   35b78:	b	35c38 <__gmpn_toom62_mul@@Base+0x4e4>
   35b7c:	ldur	x8, [x29, #-200]
   35b80:	cmp	x8, x22
   35b84:	b.ge	35bbc <__gmpn_toom62_mul@@Base+0x468>  // b.tcont
   35b88:	mov	x0, x19
   35b8c:	mov	x1, x20
   35b90:	mov	x2, x21
   35b94:	bl	c570 <__gmpn_cmp@plt>
   35b98:	tbnz	w0, #31, 35c08 <__gmpn_toom62_mul@@Base+0x4b4>
   35b9c:	mov	x0, x24
   35ba0:	mov	x1, x19
   35ba4:	mov	x2, x20
   35ba8:	mov	x3, x21
   35bac:	stur	w26, [x29, #-96]
   35bb0:	mov	x26, x25
   35bb4:	bl	c420 <__gmpn_sub_n@plt>
   35bb8:	b	35c2c <__gmpn_toom62_mul@@Base+0x4d8>
   35bbc:	ldur	x1, [x29, #-208]
   35bc0:	add	x0, x19, x22, lsl #3
   35bc4:	bl	c010 <__gmpn_zero_p@plt>
   35bc8:	stur	w26, [x29, #-96]
   35bcc:	cbz	w0, 35be4 <__gmpn_toom62_mul@@Base+0x490>
   35bd0:	mov	x0, x19
   35bd4:	mov	x1, x20
   35bd8:	mov	x2, x22
   35bdc:	bl	c570 <__gmpn_cmp@plt>
   35be0:	tbnz	w0, #31, 35ebc <__gmpn_toom62_mul@@Base+0x768>
   35be4:	mov	x0, x24
   35be8:	mov	x1, x19
   35bec:	mov	x2, x21
   35bf0:	mov	x3, x20
   35bf4:	mov	x4, x22
   35bf8:	mov	x26, x25
   35bfc:	bl	d340 <__gmpn_sub@plt>
   35c00:	str	xzr, [x24, x21, lsl #3]
   35c04:	b	35b70 <__gmpn_toom62_mul@@Base+0x41c>
   35c08:	mov	x0, x24
   35c0c:	mov	x1, x20
   35c10:	mov	x2, x19
   35c14:	mov	x3, x21
   35c18:	mov	w22, w26
   35c1c:	mov	x26, x25
   35c20:	bl	c420 <__gmpn_sub_n@plt>
   35c24:	orr	w22, w22, #0x1
   35c28:	stur	w22, [x29, #-96]
   35c2c:	ldur	x25, [x29, #-192]
   35c30:	str	xzr, [x24, x21, lsl #3]
   35c34:	ldur	x23, [x29, #-80]
   35c38:	ldp	x19, x2, [x29, #-24]
   35c3c:	ldur	x20, [x29, #-160]
   35c40:	mov	x22, x24
   35c44:	mov	x1, x27
   35c48:	ldr	x24, [x27, x19]
   35c4c:	mov	x0, x20
   35c50:	mov	x3, x21
   35c54:	bl	cc30 <__gmpn_add_n@plt>
   35c58:	add	x8, x0, x24
   35c5c:	ldp	x2, x24, [x29, #-48]
   35c60:	ldur	x1, [x29, #-152]
   35c64:	mov	x0, x26
   35c68:	str	x8, [x20, x19]
   35c6c:	mov	x3, x24
   35c70:	bl	cb50 <__gmpn_mul_n@plt>
   35c74:	ldur	x8, [x29, #-64]
   35c78:	ldur	x1, [x29, #-136]
   35c7c:	mov	x2, x22
   35c80:	mov	x3, x24
   35c84:	add	x8, x26, x8, lsl #3
   35c88:	add	x0, x8, #0x8
   35c8c:	stur	x0, [x29, #-48]
   35c90:	bl	cb50 <__gmpn_mul_n@plt>
   35c94:	add	x8, x26, x25, lsl #3
   35c98:	add	x22, x8, #0x10
   35c9c:	mov	x0, x22
   35ca0:	mov	x1, x28
   35ca4:	mov	x2, x20
   35ca8:	mov	x3, x24
   35cac:	bl	cb50 <__gmpn_mul_n@plt>
   35cb0:	ldur	x24, [x29, #-128]
   35cb4:	add	x25, x21, x21, lsl #1
   35cb8:	add	x8, x26, x25, lsl #4
   35cbc:	add	x28, x8, #0x18
   35cc0:	mov	x0, x28
   35cc4:	mov	x1, x24
   35cc8:	mov	x2, x23
   35ccc:	mov	x3, x21
   35cd0:	bl	cb50 <__gmpn_mul_n@plt>
   35cd4:	ldr	x8, [x24, x19]
   35cd8:	stur	x22, [x29, #-136]
   35cdc:	cmp	x8, #0x2
   35ce0:	b.eq	35d04 <__gmpn_toom62_mul@@Base+0x5b0>  // b.none
   35ce4:	cmp	x8, #0x1
   35ce8:	b.ne	35d1c <__gmpn_toom62_mul@@Base+0x5c8>  // b.any
   35cec:	ldur	x2, [x29, #-80]
   35cf0:	add	x0, x28, x21, lsl #3
   35cf4:	mov	x1, x0
   35cf8:	mov	x3, x21
   35cfc:	bl	cc30 <__gmpn_add_n@plt>
   35d00:	b	35d20 <__gmpn_toom62_mul@@Base+0x5cc>
   35d04:	add	x0, x28, x21, lsl #3
   35d08:	mov	x1, x0
   35d0c:	mov	x2, x23
   35d10:	mov	x3, x21
   35d14:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35d18:	b	35d20 <__gmpn_toom62_mul@@Base+0x5cc>
   35d1c:	mov	x0, xzr
   35d20:	ldp	x22, x9, [x29, #-64]
   35d24:	ldur	x24, [x29, #-72]
   35d28:	ldur	x23, [x29, #-32]
   35d2c:	ldur	x20, [x29, #-176]
   35d30:	lsl	x8, x22, #3
   35d34:	add	x19, x9, x8
   35d38:	str	x0, [x28, x8]
   35d3c:	mov	x0, x19
   35d40:	mov	x1, x24
   35d44:	mov	x2, x27
   35d48:	mov	x3, x21
   35d4c:	bl	cb50 <__gmpn_mul_n@plt>
   35d50:	ldr	x24, [x24, x21, lsl #3]
   35d54:	cbz	x24, 35dd4 <__gmpn_toom62_mul@@Base+0x680>
   35d58:	cmp	x24, #0x2
   35d5c:	b.eq	35d8c <__gmpn_toom62_mul@@Base+0x638>  // b.none
   35d60:	cmp	x24, #0x1
   35d64:	b.ne	35db0 <__gmpn_toom62_mul@@Base+0x65c>  // b.any
   35d68:	ldur	x8, [x29, #-24]
   35d6c:	mov	x2, x27
   35d70:	mov	x3, x21
   35d74:	ldr	x24, [x27, x8]
   35d78:	add	x0, x19, x8
   35d7c:	mov	x1, x0
   35d80:	bl	cc30 <__gmpn_add_n@plt>
   35d84:	add	x24, x0, x24
   35d88:	b	35dd4 <__gmpn_toom62_mul@@Base+0x680>
   35d8c:	ldur	x8, [x29, #-24]
   35d90:	mov	x2, x27
   35d94:	mov	x3, x21
   35d98:	ldr	x24, [x27, x8]
   35d9c:	add	x0, x19, x8
   35da0:	mov	x1, x0
   35da4:	bl	ce00 <__gmpn_addlsh1_n@plt>
   35da8:	add	x24, x0, x24, lsl #1
   35dac:	b	35dd4 <__gmpn_toom62_mul@@Base+0x680>
   35db0:	ldur	x8, [x29, #-24]
   35db4:	mov	x1, x27
   35db8:	mov	x2, x21
   35dbc:	mov	x3, x24
   35dc0:	ldr	x26, [x27, x8]
   35dc4:	add	x0, x19, x8
   35dc8:	bl	d5e0 <__gmpn_addmul_1@plt>
   35dcc:	madd	x24, x26, x24, x0
   35dd0:	ldur	x26, [x29, #-88]
   35dd4:	ldur	w9, [x29, #-116]
   35dd8:	ldr	x8, [x27, x21, lsl #3]
   35ddc:	lsl	x25, x25, #1
   35de0:	and	w27, w9, #0x2
   35de4:	cbz	x8, 35e00 <__gmpn_toom62_mul@@Base+0x6ac>
   35de8:	ldur	x2, [x29, #-72]
   35dec:	add	x0, x19, x21, lsl #3
   35df0:	mov	x1, x0
   35df4:	mov	x3, x21
   35df8:	bl	cc30 <__gmpn_add_n@plt>
   35dfc:	add	x24, x0, x24
   35e00:	str	x24, [x19, x22, lsl #3]
   35e04:	ldur	x19, [x29, #-56]
   35e08:	ldur	x22, [x29, #-168]
   35e0c:	ldur	w8, [x29, #-180]
   35e10:	ldur	x2, [x29, #-16]
   35e14:	mov	x0, x19
   35e18:	mov	x1, x22
   35e1c:	mov	x3, x21
   35e20:	bfxil	w27, w8, #0, #1
   35e24:	bl	cb50 <__gmpn_mul_n@plt>
   35e28:	ldp	x8, x24, [x29, #-112]
   35e2c:	add	x0, x19, x25, lsl #3
   35e30:	cmp	x24, x23
   35e34:	add	x3, x22, x8, lsl #3
   35e38:	b.le	35e50 <__gmpn_toom62_mul@@Base+0x6fc>
   35e3c:	mov	x1, x3
   35e40:	mov	x2, x24
   35e44:	mov	x3, x20
   35e48:	mov	x4, x23
   35e4c:	b	35e5c <__gmpn_toom62_mul@@Base+0x708>
   35e50:	mov	x1, x20
   35e54:	mov	x2, x23
   35e58:	mov	x4, x24
   35e5c:	bl	cea0 <__gmpn_mul@plt>
   35e60:	ldur	w8, [x29, #-96]
   35e64:	ldur	x6, [x29, #-136]
   35e68:	add	x7, x24, x23
   35e6c:	eor	w2, w8, w27
   35e70:	ldur	x8, [x29, #-24]
   35e74:	add	x8, x26, x8, lsl #3
   35e78:	add	x8, x8, #0x20
   35e7c:	str	x8, [sp, #-16]!
   35e80:	ldur	x3, [x29, #-48]
   35e84:	mov	x0, x19
   35e88:	mov	x1, x21
   35e8c:	mov	x4, x28
   35e90:	mov	x5, x26
   35e94:	bl	c9c0 <__gmpn_toom_interpolate_7pts@plt>
   35e98:	add	sp, sp, #0x10
   35e9c:	mov	sp, x29
   35ea0:	ldp	x20, x19, [sp, #80]
   35ea4:	ldp	x22, x21, [sp, #64]
   35ea8:	ldp	x24, x23, [sp, #48]
   35eac:	ldp	x26, x25, [sp, #32]
   35eb0:	ldp	x28, x27, [sp, #16]
   35eb4:	ldp	x29, x30, [sp], #96
   35eb8:	ret
   35ebc:	mov	x0, x24
   35ec0:	mov	x1, x20
   35ec4:	mov	x2, x19
   35ec8:	mov	x3, x22
   35ecc:	mov	x26, x25
   35ed0:	bl	c420 <__gmpn_sub_n@plt>
   35ed4:	ldur	x8, [x29, #-40]
   35ed8:	cmp	x8, x22
   35edc:	b.eq	35f10 <__gmpn_toom62_mul@@Base+0x7bc>  // b.none
   35ee0:	ldur	x10, [x29, #-216]
   35ee4:	ldur	x11, [x29, #-200]
   35ee8:	mov	w1, wzr
   35eec:	lsl	x8, x10, #3
   35ef0:	lsl	x9, x11, #1
   35ef4:	sub	x8, x8, x11, lsl #3
   35ef8:	sub	x9, x9, x10
   35efc:	add	x8, x8, x24
   35f00:	lsl	x9, x9, #3
   35f04:	sub	x0, x8, #0x8
   35f08:	add	x2, x9, #0x18
   35f0c:	bl	c780 <memset@plt>
   35f10:	ldur	w8, [x29, #-96]
   35f14:	ldur	x25, [x29, #-192]
   35f18:	orr	w8, w8, #0x1
   35f1c:	stur	w8, [x29, #-96]
   35f20:	b	35c34 <__gmpn_toom62_mul@@Base+0x4e0>

0000000000035f24 <__gmpn_toom33_mul@@Base>:
   35f24:	sub	sp, sp, #0xd0
   35f28:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   35f2c:	add	x8, x2, #0x2
   35f30:	movk	x9, #0xaaab
   35f34:	umulh	x8, x8, x9
   35f38:	stp	x24, x23, [sp, #160]
   35f3c:	lsr	x23, x8, #1
   35f40:	stp	x26, x25, [sp, #144]
   35f44:	mov	x10, x4
   35f48:	lsl	x25, x23, #1
   35f4c:	stp	x29, x30, [sp, #112]
   35f50:	add	x29, sp, #0x70
   35f54:	add	x8, x5, x23, lsl #5
   35f58:	lsl	x9, x23, #4
   35f5c:	lsl	x24, x23, #3
   35f60:	stp	x2, x10, [sp, #24]
   35f64:	sub	x10, x10, x25
   35f68:	stp	x28, x27, [sp, #128]
   35f6c:	stur	x3, [x29, #-8]
   35f70:	sub	x4, x2, x25
   35f74:	stur	x10, [x29, #-16]
   35f78:	add	x28, x8, #0x20
   35f7c:	add	x11, x5, x9
   35f80:	add	x8, x0, x24
   35f84:	str	x0, [sp, #56]
   35f88:	add	x10, x0, x9
   35f8c:	add	x3, x1, x9
   35f90:	mov	x0, x5
   35f94:	mov	x2, x23
   35f98:	stp	x22, x21, [sp, #176]
   35f9c:	stp	x20, x19, [sp, #192]
   35fa0:	mov	x19, x5
   35fa4:	mov	x27, x1
   35fa8:	stur	x10, [x29, #-40]
   35fac:	str	x11, [sp, #16]
   35fb0:	add	x26, x11, #0x10
   35fb4:	add	x20, x8, #0x8
   35fb8:	stur	x3, [x29, #-48]
   35fbc:	stur	x4, [x29, #-24]
   35fc0:	bl	c970 <__gmpn_add@plt>
   35fc4:	add	x22, x27, x24
   35fc8:	mov	x21, x0
   35fcc:	mov	x0, x28
   35fd0:	mov	x1, x19
   35fd4:	mov	x2, x22
   35fd8:	mov	x3, x23
   35fdc:	bl	cc30 <__gmpn_add_n@plt>
   35fe0:	add	x8, x0, x21
   35fe4:	stur	x28, [x29, #-32]
   35fe8:	str	x8, [x28, x24]
   35fec:	cbnz	x21, 36004 <__gmpn_toom33_mul@@Base+0xe0>
   35ff0:	mov	x0, x19
   35ff4:	mov	x1, x22
   35ff8:	mov	x2, x23
   35ffc:	bl	c570 <__gmpn_cmp@plt>
   36000:	tbnz	w0, #31, 362ac <__gmpn_toom33_mul@@Base+0x388>
   36004:	mov	x0, x26
   36008:	mov	x1, x19
   3600c:	mov	x2, x22
   36010:	mov	x3, x23
   36014:	bl	c420 <__gmpn_sub_n@plt>
   36018:	sub	x8, x21, x0
   3601c:	str	wzr, [sp, #52]
   36020:	mov	x10, x26
   36024:	str	x8, [x10, x23, lsl #3]
   36028:	mov	w9, #0x18                  	// #24
   3602c:	ldp	x28, x22, [x29, #-32]
   36030:	madd	x21, x23, x9, x19
   36034:	ldur	x9, [x29, #-40]
   36038:	ldur	x1, [x29, #-48]
   3603c:	mov	x0, x20
   36040:	mov	x2, x28
   36044:	mov	x3, x22
   36048:	add	x26, x9, #0x10
   3604c:	str	x10, [sp, #8]
   36050:	bl	cc30 <__gmpn_add_n@plt>
   36054:	subs	x2, x23, x22
   36058:	mov	x3, x0
   3605c:	b.eq	36074 <__gmpn_toom33_mul@@Base+0x150>  // b.none
   36060:	lsl	x8, x22, #3
   36064:	add	x0, x20, x8
   36068:	add	x1, x28, x8
   3606c:	bl	c150 <__gmpn_add_1@plt>
   36070:	mov	x3, x0
   36074:	ldr	x8, [x28, x24]
   36078:	add	x22, x21, #0x18
   3607c:	mov	x0, x20
   36080:	mov	x1, x27
   36084:	add	x21, x8, x3
   36088:	mov	x2, x20
   3608c:	mov	x3, x23
   36090:	str	x27, [sp, #40]
   36094:	bl	d260 <__gmpn_rsblsh1_n@plt>
   36098:	add	x8, x0, x21, lsl #1
   3609c:	str	x8, [x20, x24]
   360a0:	ldp	x4, x27, [x29, #-16]
   360a4:	mov	x0, x19
   360a8:	mov	x2, x23
   360ac:	add	x21, x27, x25, lsl #3
   360b0:	mov	x1, x27
   360b4:	mov	x3, x21
   360b8:	bl	c970 <__gmpn_add@plt>
   360bc:	ldr	x28, [sp, #56]
   360c0:	add	x27, x27, x24
   360c4:	mov	x25, x0
   360c8:	mov	x1, x19
   360cc:	mov	x0, x28
   360d0:	mov	x2, x27
   360d4:	mov	x3, x23
   360d8:	bl	cc30 <__gmpn_add_n@plt>
   360dc:	add	x8, x0, x25
   360e0:	str	x8, [x28, x24]
   360e4:	mov	x28, x20
   360e8:	cbnz	x25, 36100 <__gmpn_toom33_mul@@Base+0x1dc>
   360ec:	mov	x0, x19
   360f0:	mov	x1, x27
   360f4:	mov	x2, x23
   360f8:	bl	c570 <__gmpn_cmp@plt>
   360fc:	tbnz	w0, #31, 362d0 <__gmpn_toom33_mul@@Base+0x3ac>
   36100:	mov	x0, x22
   36104:	mov	x1, x19
   36108:	mov	x2, x27
   3610c:	mov	x3, x23
   36110:	bl	c420 <__gmpn_sub_n@plt>
   36114:	sub	x8, x25, x0
   36118:	str	x8, [x22, x23, lsl #3]
   3611c:	ldur	x20, [x29, #-16]
   36120:	ldr	x2, [sp, #56]
   36124:	lsl	x8, x23, #2
   36128:	mov	x0, x26
   3612c:	mov	x1, x21
   36130:	mov	x3, x20
   36134:	str	x8, [sp]
   36138:	mov	x25, x2
   3613c:	bl	cc30 <__gmpn_add_n@plt>
   36140:	subs	x2, x23, x20
   36144:	mov	x3, x0
   36148:	b.eq	36160 <__gmpn_toom33_mul@@Base+0x23c>  // b.none
   3614c:	lsl	x8, x20, #3
   36150:	add	x0, x26, x8
   36154:	add	x1, x25, x8
   36158:	bl	c150 <__gmpn_add_1@plt>
   3615c:	mov	x3, x0
   36160:	ldr	x8, [x25, x24]
   36164:	ldur	x1, [x29, #-8]
   36168:	mov	x20, x25
   3616c:	mov	x0, x26
   36170:	add	x25, x8, x3
   36174:	mov	x2, x26
   36178:	mov	x3, x23
   3617c:	bl	d260 <__gmpn_rsblsh1_n@plt>
   36180:	add	x8, x0, x25, lsl #1
   36184:	mov	w9, #0x28                  	// #40
   36188:	ldr	x1, [sp, #8]
   3618c:	str	x8, [x26, x24]
   36190:	madd	x8, x23, x9, x19
   36194:	add	x27, x23, #0x1
   36198:	add	x25, x8, #0x28
   3619c:	mov	x0, x19
   361a0:	mov	x2, x27
   361a4:	mov	x3, x22
   361a8:	mov	x4, x27
   361ac:	mov	x5, x25
   361b0:	bl	d630 <__gmpn_toom22_mul@plt>
   361b4:	ldr	x8, [sp, #16]
   361b8:	mov	x1, x28
   361bc:	mov	x2, x27
   361c0:	mov	x3, x26
   361c4:	add	x0, x8, #0x8
   361c8:	mov	x4, x27
   361cc:	mov	x5, x25
   361d0:	str	x0, [sp, #56]
   361d4:	bl	d630 <__gmpn_toom22_mul@plt>
   361d8:	ldp	x9, x8, [sp, #24]
   361dc:	mov	x22, x20
   361e0:	cmp	x9, x8
   361e4:	ldr	x8, [sp]
   361e8:	add	x26, x20, x8, lsl #3
   361ec:	mov	x0, x26
   361f0:	b.le	36210 <__gmpn_toom33_mul@@Base+0x2ec>
   361f4:	ldp	x28, x20, [x29, #-24]
   361f8:	ldur	x1, [x29, #-48]
   361fc:	mov	x3, x21
   36200:	mov	x2, x28
   36204:	mov	x4, x20
   36208:	bl	cea0 <__gmpn_mul@plt>
   3620c:	b	36230 <__gmpn_toom33_mul@@Base+0x30c>
   36210:	ldur	x28, [x29, #-24]
   36214:	ldur	x1, [x29, #-48]
   36218:	mov	x3, x21
   3621c:	mov	x5, x25
   36220:	mov	x2, x28
   36224:	mov	x4, x28
   36228:	bl	d630 <__gmpn_toom22_mul@plt>
   3622c:	ldur	x20, [x29, #-16]
   36230:	ldp	x0, x1, [x29, #-40]
   36234:	ldp	x21, x24, [x26]
   36238:	mov	x2, x27
   3623c:	mov	x3, x22
   36240:	mov	x4, x27
   36244:	mov	x5, x25
   36248:	bl	d630 <__gmpn_toom22_mul@plt>
   3624c:	ldr	x1, [sp, #40]
   36250:	ldur	x3, [x29, #-8]
   36254:	mov	x0, x22
   36258:	mov	x2, x23
   3625c:	mov	x4, x23
   36260:	mov	x5, x25
   36264:	str	x24, [x26, #8]
   36268:	bl	d630 <__gmpn_toom22_mul@plt>
   3626c:	ldr	x1, [sp, #56]
   36270:	ldr	w5, [sp, #52]
   36274:	add	x4, x28, x20
   36278:	mov	x0, x22
   3627c:	mov	x2, x19
   36280:	mov	x3, x23
   36284:	mov	x6, x21
   36288:	bl	cbe0 <__gmpn_toom_interpolate_5pts@plt>
   3628c:	ldp	x20, x19, [sp, #192]
   36290:	ldp	x22, x21, [sp, #176]
   36294:	ldp	x24, x23, [sp, #160]
   36298:	ldp	x26, x25, [sp, #144]
   3629c:	ldp	x28, x27, [sp, #128]
   362a0:	ldp	x29, x30, [sp, #112]
   362a4:	add	sp, sp, #0xd0
   362a8:	ret
   362ac:	mov	x0, x26
   362b0:	mov	x1, x22
   362b4:	mov	x2, x19
   362b8:	mov	x3, x23
   362bc:	bl	c420 <__gmpn_sub_n@plt>
   362c0:	mov	w9, #0x1                   	// #1
   362c4:	mov	x8, xzr
   362c8:	str	w9, [sp, #52]
   362cc:	b	36020 <__gmpn_toom33_mul@@Base+0xfc>
   362d0:	mov	x0, x22
   362d4:	mov	x1, x27
   362d8:	mov	x2, x19
   362dc:	mov	x3, x23
   362e0:	bl	c420 <__gmpn_sub_n@plt>
   362e4:	ldr	w8, [sp, #52]
   362e8:	str	xzr, [x22, x23, lsl #3]
   362ec:	eor	w8, w8, #0x1
   362f0:	str	w8, [sp, #52]
   362f4:	b	3611c <__gmpn_toom33_mul@@Base+0x1f8>

00000000000362f8 <__gmpn_toom43_mul@@Base>:
   362f8:	sub	sp, sp, #0xe0
   362fc:	add	x8, x2, x2, lsl #1
   36300:	stp	x29, x30, [sp, #128]
   36304:	stp	x20, x19, [sp, #208]
   36308:	add	x29, sp, #0x80
   3630c:	mov	x19, x5
   36310:	mov	x10, x1
   36314:	cmp	x8, x4, lsl #2
   36318:	mov	x20, x0
   3631c:	stp	x28, x27, [sp, #144]
   36320:	stp	x26, x25, [sp, #160]
   36324:	stp	x24, x23, [sp, #176]
   36328:	stp	x22, x21, [sp, #192]
   3632c:	stur	x3, [x29, #-8]
   36330:	b.ge	3634c <__gmpn_toom43_mul@@Base+0x54>  // b.tcont
   36334:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   36338:	sub	x8, x4, #0x1
   3633c:	movk	x9, #0xaaab
   36340:	umulh	x8, x8, x9
   36344:	lsr	x8, x8, #1
   36348:	b	36354 <__gmpn_toom43_mul@@Base+0x5c>
   3634c:	sub	x8, x2, #0x1
   36350:	asr	x8, x8, #2
   36354:	add	x23, x8, #0x1
   36358:	lsl	x22, x23, #1
   3635c:	str	x8, [sp, #24]
   36360:	add	x8, x22, x23
   36364:	add	x9, x19, x23, lsl #5
   36368:	sub	x24, x4, x22
   3636c:	sub	x4, x2, x8
   36370:	stur	x8, [x29, #-40]
   36374:	lsl	x8, x8, #3
   36378:	stur	x9, [x29, #-56]
   3637c:	add	x1, x9, #0x20
   36380:	add	x9, x20, x8
   36384:	add	x8, x19, x8
   36388:	add	x0, x9, #0x18
   3638c:	add	x5, x8, #0x18
   36390:	mov	x2, x10
   36394:	mov	x3, x23
   36398:	stur	x0, [x29, #-48]
   3639c:	str	x1, [sp, #64]
   363a0:	stp	x4, x10, [x29, #-24]
   363a4:	str	x5, [sp, #56]
   363a8:	bl	cf30 <__gmpn_toom_eval_dgr3_pm2@plt>
   363ac:	ldur	x21, [x29, #-8]
   363b0:	lsl	x25, x23, #4
   363b4:	add	x8, x19, x25
   363b8:	lsl	x27, x23, #3
   363bc:	add	x28, x8, #0x10
   363c0:	str	w0, [sp, #32]
   363c4:	add	x1, x21, x27
   363c8:	mov	w3, #0x1                   	// #1
   363cc:	mov	x0, x28
   363d0:	mov	x2, x23
   363d4:	str	x8, [sp, #48]
   363d8:	str	x1, [sp, #40]
   363dc:	bl	c2d0 <__gmpn_lshift@plt>
   363e0:	str	x0, [x28, x27]
   363e4:	add	x1, x21, x25
   363e8:	mov	w3, #0x2                   	// #2
   363ec:	mov	x0, x19
   363f0:	mov	x2, x24
   363f4:	stur	x1, [x29, #-32]
   363f8:	bl	c2d0 <__gmpn_lshift@plt>
   363fc:	mov	x26, x0
   36400:	mov	x0, x19
   36404:	mov	x1, x19
   36408:	mov	x2, x21
   3640c:	mov	x3, x24
   36410:	bl	cc30 <__gmpn_add_n@plt>
   36414:	subs	x2, x23, x24
   36418:	add	x3, x0, x26
   3641c:	b.eq	36434 <__gmpn_toom43_mul@@Base+0x13c>  // b.none
   36420:	lsl	x8, x24, #3
   36424:	add	x0, x19, x8
   36428:	add	x1, x21, x8
   3642c:	bl	c150 <__gmpn_add_1@plt>
   36430:	mov	x3, x0
   36434:	ldr	w8, [sp, #32]
   36438:	str	x3, [x19, x27]
   3643c:	mov	x1, x19
   36440:	mov	x2, x28
   36444:	and	w21, w8, #0x2
   36448:	add	x8, x20, x22, lsl #3
   3644c:	str	x8, [sp, #32]
   36450:	add	x0, x8, #0x10
   36454:	ldr	x8, [sp, #24]
   36458:	lsl	x25, x23, #2
   3645c:	str	x0, [sp, #24]
   36460:	add	x26, x8, #0x2
   36464:	mov	x3, x26
   36468:	bl	cc30 <__gmpn_add_n@plt>
   3646c:	mov	x0, x19
   36470:	mov	x1, x28
   36474:	mov	x2, x26
   36478:	bl	c570 <__gmpn_cmp@plt>
   3647c:	add	x8, x20, x27
   36480:	add	x8, x8, #0x8
   36484:	str	x8, [sp, #16]
   36488:	tbnz	w0, #31, 364a4 <__gmpn_toom43_mul@@Base+0x1ac>
   3648c:	mov	x0, x8
   36490:	mov	x1, x19
   36494:	mov	x2, x28
   36498:	mov	x3, x26
   3649c:	bl	c420 <__gmpn_sub_n@plt>
   364a0:	b	364bc <__gmpn_toom43_mul@@Base+0x1c4>
   364a4:	mov	x0, x8
   364a8:	mov	x1, x28
   364ac:	mov	x2, x19
   364b0:	mov	x3, x26
   364b4:	bl	c420 <__gmpn_sub_n@plt>
   364b8:	eor	w21, w21, #0x2
   364bc:	ldr	x22, [sp, #56]
   364c0:	ldp	x4, x2, [x29, #-24]
   364c4:	add	x8, x20, x25, lsl #3
   364c8:	add	x0, x8, #0x20
   364cc:	mov	x1, x22
   364d0:	mov	x3, x23
   364d4:	mov	x5, x19
   364d8:	str	x0, [sp, #8]
   364dc:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   364e0:	ldur	x1, [x29, #-8]
   364e4:	ldur	x3, [x29, #-32]
   364e8:	and	w8, w0, #0x1
   364ec:	eor	w8, w8, w21
   364f0:	mov	x0, x28
   364f4:	mov	x2, x23
   364f8:	mov	x4, x24
   364fc:	str	w8, [sp, #56]
   36500:	bl	c970 <__gmpn_add@plt>
   36504:	ldr	x25, [sp, #40]
   36508:	mov	x21, x0
   3650c:	str	x0, [x28, x27]
   36510:	mov	x0, x20
   36514:	mov	x1, x28
   36518:	mov	x2, x25
   3651c:	mov	x3, x23
   36520:	bl	cc30 <__gmpn_add_n@plt>
   36524:	add	x8, x0, x21
   36528:	str	x8, [x20, x27]
   3652c:	ldr	x8, [x28, x27]
   36530:	cbnz	x8, 36548 <__gmpn_toom43_mul@@Base+0x250>
   36534:	mov	x0, x28
   36538:	mov	x1, x25
   3653c:	mov	x2, x23
   36540:	bl	c570 <__gmpn_cmp@plt>
   36544:	tbnz	w0, #31, 3665c <__gmpn_toom43_mul@@Base+0x364>
   36548:	mov	x0, x28
   3654c:	mov	x1, x28
   36550:	mov	x2, x25
   36554:	mov	x3, x23
   36558:	bl	c420 <__gmpn_sub_n@plt>
   3655c:	ldr	x8, [x28, x27]
   36560:	ldr	w21, [sp, #56]
   36564:	sub	x8, x8, x0
   36568:	str	x8, [x28, x27]
   3656c:	mov	x0, x19
   36570:	mov	x1, x22
   36574:	mov	x2, x28
   36578:	mov	x3, x26
   3657c:	bl	cb50 <__gmpn_mul_n@plt>
   36580:	ldr	x8, [sp, #48]
   36584:	ldr	x1, [sp, #64]
   36588:	ldr	x2, [sp, #16]
   3658c:	mov	x3, x26
   36590:	add	x22, x8, #0x8
   36594:	mov	x0, x22
   36598:	bl	cb50 <__gmpn_mul_n@plt>
   3659c:	ldp	x8, x1, [x29, #-56]
   365a0:	ldr	x2, [sp, #24]
   365a4:	mov	x3, x26
   365a8:	add	x27, x8, #0x10
   365ac:	mov	x0, x27
   365b0:	bl	cb50 <__gmpn_mul_n@plt>
   365b4:	ldr	x0, [sp, #32]
   365b8:	ldr	x1, [sp, #8]
   365bc:	mov	x2, x20
   365c0:	mov	x3, x26
   365c4:	bl	cb50 <__gmpn_mul_n@plt>
   365c8:	mov	w8, #0x28                  	// #40
   365cc:	ldp	x26, x25, [x29, #-24]
   365d0:	madd	x0, x23, x8, x20
   365d4:	ldur	x8, [x29, #-40]
   365d8:	cmp	x26, x24
   365dc:	add	x3, x25, x8, lsl #3
   365e0:	b.le	365f8 <__gmpn_toom43_mul@@Base+0x300>
   365e4:	mov	x1, x3
   365e8:	ldur	x3, [x29, #-32]
   365ec:	mov	x2, x26
   365f0:	mov	x4, x24
   365f4:	b	36604 <__gmpn_toom43_mul@@Base+0x30c>
   365f8:	ldur	x1, [x29, #-32]
   365fc:	mov	x2, x24
   36600:	mov	x4, x26
   36604:	bl	cea0 <__gmpn_mul@plt>
   36608:	ldur	x2, [x29, #-8]
   3660c:	mov	x0, x20
   36610:	mov	x1, x25
   36614:	mov	x3, x23
   36618:	bl	cb50 <__gmpn_mul_n@plt>
   3661c:	add	x6, x24, x26
   36620:	mov	x0, x20
   36624:	mov	x1, x23
   36628:	mov	w2, w21
   3662c:	mov	x3, x19
   36630:	mov	x4, x22
   36634:	mov	x5, x27
   36638:	bl	cac0 <__gmpn_toom_interpolate_6pts@plt>
   3663c:	ldp	x20, x19, [sp, #208]
   36640:	ldp	x22, x21, [sp, #192]
   36644:	ldp	x24, x23, [sp, #176]
   36648:	ldp	x26, x25, [sp, #160]
   3664c:	ldp	x28, x27, [sp, #144]
   36650:	ldp	x29, x30, [sp, #128]
   36654:	add	sp, sp, #0xe0
   36658:	ret
   3665c:	mov	x0, x28
   36660:	mov	x1, x25
   36664:	mov	x2, x28
   36668:	mov	x3, x23
   3666c:	bl	c420 <__gmpn_sub_n@plt>
   36670:	ldr	w21, [sp, #56]
   36674:	eor	w21, w21, #0x1
   36678:	b	3656c <__gmpn_toom43_mul@@Base+0x274>

000000000003667c <__gmpn_toom53_mul@@Base>:
   3667c:	stp	x29, x30, [sp, #-96]!
   36680:	stp	x28, x27, [sp, #16]
   36684:	stp	x26, x25, [sp, #32]
   36688:	stp	x24, x23, [sp, #48]
   3668c:	stp	x22, x21, [sp, #64]
   36690:	stp	x20, x19, [sp, #80]
   36694:	mov	x29, sp
   36698:	sub	sp, sp, #0x90
   3669c:	add	x8, x2, x2, lsl #1
   366a0:	add	x9, x4, x4, lsl #2
   366a4:	cmp	x8, x9
   366a8:	mov	w10, #0x5                   	// #5
   366ac:	mov	w11, #0x3                   	// #3
   366b0:	csel	x8, x4, x2, lt  // lt = tstop
   366b4:	csel	x9, x11, x10, lt  // lt = tstop
   366b8:	sub	x8, x8, #0x1
   366bc:	udiv	x8, x8, x9
   366c0:	add	x19, x8, #0x2
   366c4:	add	x21, x8, #0x1
   366c8:	add	x8, x19, x19, lsl #2
   366cc:	mov	x25, x1
   366d0:	lsl	x9, x21, #2
   366d4:	lsl	x10, x21, #1
   366d8:	lsl	x1, x8, #4
   366dc:	mov	w8, #0x7f00                	// #32512
   366e0:	mov	x20, x0
   366e4:	stur	x9, [x29, #-72]
   366e8:	sub	x24, x2, x9
   366ec:	sub	x9, x4, x10
   366f0:	cmp	x1, x8
   366f4:	stp	x10, x3, [x29, #-32]
   366f8:	stp	x9, x2, [x29, #-56]
   366fc:	stur	xzr, [x29, #-8]
   36700:	stur	x5, [x29, #-104]
   36704:	b.hi	36c2c <__gmpn_toom53_mul@@Base+0x5b0>  // b.pmore
   36708:	add	x9, x1, #0xf
   3670c:	mov	x8, sp
   36710:	and	x9, x9, #0xfffffffffffffff0
   36714:	sub	x0, x8, x9
   36718:	mov	sp, x0
   3671c:	lsl	x28, x19, #3
   36720:	add	x1, x0, x28
   36724:	add	x23, x1, x28
   36728:	add	x22, x23, x28
   3672c:	add	x27, x22, x28
   36730:	add	x26, x27, x28
   36734:	add	x8, x26, x28
   36738:	mov	w2, #0x4                   	// #4
   3673c:	mov	x3, x25
   36740:	mov	x4, x21
   36744:	mov	x5, x24
   36748:	mov	x6, x20
   3674c:	stur	x19, [x29, #-64]
   36750:	stp	x0, x8, [x29, #-96]
   36754:	add	x19, x8, x28
   36758:	stur	x1, [x29, #-128]
   3675c:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   36760:	and	w8, w0, #0x2
   36764:	mov	w2, #0x4                   	// #4
   36768:	mov	x0, x23
   3676c:	mov	x1, x22
   36770:	mov	x3, x25
   36774:	mov	x4, x21
   36778:	mov	x5, x24
   3677c:	mov	x6, x20
   36780:	stur	w8, [x29, #-12]
   36784:	stp	x23, x22, [x29, #-144]
   36788:	stur	x20, [x29, #-80]
   3678c:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   36790:	stur	w0, [x29, #-40]
   36794:	add	x1, x25, x21, lsl #3
   36798:	mov	x0, x27
   3679c:	mov	x2, x25
   367a0:	mov	x3, x21
   367a4:	bl	ce00 <__gmpn_addlsh1_n@plt>
   367a8:	ldur	x8, [x29, #-32]
   367ac:	mov	x22, x0
   367b0:	mov	x0, x27
   367b4:	mov	x2, x27
   367b8:	add	x1, x25, x8, lsl #3
   367bc:	mov	x3, x21
   367c0:	bl	ce00 <__gmpn_addlsh1_n@plt>
   367c4:	mov	w8, #0x18                  	// #24
   367c8:	add	x20, x0, x22, lsl #1
   367cc:	madd	x1, x21, x8, x25
   367d0:	mov	x0, x27
   367d4:	mov	x2, x27
   367d8:	mov	x3, x21
   367dc:	bl	ce00 <__gmpn_addlsh1_n@plt>
   367e0:	subs	x23, x21, x24
   367e4:	add	x20, x0, x20, lsl #1
   367e8:	stp	x24, x25, [x29, #-120]
   367ec:	b.le	36860 <__gmpn_toom53_mul@@Base+0x1e4>
   367f0:	ldur	x8, [x29, #-72]
   367f4:	mov	x0, x27
   367f8:	mov	x2, x27
   367fc:	mov	x3, x24
   36800:	add	x1, x25, x8, lsl #3
   36804:	bl	ce00 <__gmpn_addlsh1_n@plt>
   36808:	add	x25, x27, x24, lsl #3
   3680c:	mov	x22, x0
   36810:	mov	w3, #0x1                   	// #1
   36814:	mov	x0, x25
   36818:	mov	x1, x25
   3681c:	mov	x2, x23
   36820:	bl	c2d0 <__gmpn_lshift@plt>
   36824:	add	x8, x0, x20, lsl #1
   36828:	str	x8, [x27, x21, lsl #3]
   3682c:	ldr	x8, [x25]
   36830:	adds	x8, x8, x22
   36834:	str	x8, [x25]
   36838:	b.cc	36880 <__gmpn_toom53_mul@@Base+0x204>  // b.lo, b.ul, b.last
   3683c:	ldur	x8, [x29, #-96]
   36840:	ldur	x9, [x29, #-48]
   36844:	add	x8, x8, x9, lsl #3
   36848:	add	x8, x8, #0x28
   3684c:	ldr	x9, [x8]
   36850:	adds	x9, x9, #0x1
   36854:	str	x9, [x8], #8
   36858:	b.cs	3684c <__gmpn_toom53_mul@@Base+0x1d0>  // b.hs, b.nlast
   3685c:	b	36880 <__gmpn_toom53_mul@@Base+0x204>
   36860:	ldur	x8, [x29, #-72]
   36864:	mov	x0, x27
   36868:	mov	x2, x27
   3686c:	mov	x3, x21
   36870:	add	x1, x25, x8, lsl #3
   36874:	bl	ce00 <__gmpn_addlsh1_n@plt>
   36878:	add	x8, x0, x20, lsl #1
   3687c:	str	x8, [x27, x21, lsl #3]
   36880:	add	x8, x19, x28
   36884:	stur	x8, [x29, #-48]
   36888:	ldur	w8, [x29, #-12]
   3688c:	ldur	w9, [x29, #-40]
   36890:	ldur	x24, [x29, #-56]
   36894:	mov	x0, x26
   36898:	mov	x2, x21
   3689c:	bfxil	w8, w9, #0, #1
   368a0:	stur	w8, [x29, #-12]
   368a4:	ldp	x8, x25, [x29, #-32]
   368a8:	mov	x4, x24
   368ac:	add	x3, x25, x8, lsl #3
   368b0:	mov	x1, x25
   368b4:	stur	x3, [x29, #-40]
   368b8:	bl	c970 <__gmpn_add@plt>
   368bc:	mov	x23, x0
   368c0:	lsl	x20, x21, #3
   368c4:	str	x0, [x26, x21, lsl #3]
   368c8:	cbnz	x0, 368e4 <__gmpn_toom53_mul@@Base+0x268>
   368cc:	add	x22, x25, x21, lsl #3
   368d0:	mov	x0, x26
   368d4:	mov	x1, x22
   368d8:	mov	x2, x21
   368dc:	bl	c570 <__gmpn_cmp@plt>
   368e0:	tbnz	w0, #31, 36c00 <__gmpn_toom53_mul@@Base+0x584>
   368e4:	ldur	x22, [x29, #-88]
   368e8:	add	x2, x25, x20
   368ec:	mov	x1, x26
   368f0:	mov	x3, x21
   368f4:	mov	x0, x22
   368f8:	bl	c420 <__gmpn_sub_n@plt>
   368fc:	sub	x8, x23, x0
   36900:	str	x8, [x22, x20]
   36904:	ldur	x8, [x29, #-48]
   36908:	ldur	x22, [x29, #-80]
   3690c:	mov	x0, x26
   36910:	mov	x1, x26
   36914:	add	x23, x8, x28
   36918:	mov	x28, x25
   3691c:	add	x25, x25, x20
   36920:	mov	x2, x25
   36924:	mov	x3, x21
   36928:	bl	cc30 <__gmpn_add_n@plt>
   3692c:	ldr	x8, [x26, x20]
   36930:	ldur	x2, [x29, #-40]
   36934:	mov	x1, x28
   36938:	mov	x3, x24
   3693c:	add	x8, x8, x0
   36940:	mov	x0, x19
   36944:	str	x8, [x26, x20]
   36948:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3694c:	subs	x2, x21, x24
   36950:	mov	x3, x0
   36954:	b.le	3696c <__gmpn_toom53_mul@@Base+0x2f0>
   36958:	lsl	x8, x24, #3
   3695c:	add	x0, x19, x8
   36960:	add	x1, x28, x8
   36964:	bl	c150 <__gmpn_add_1@plt>
   36968:	mov	x3, x0
   3696c:	str	x3, [x19, x20]
   36970:	mov	w3, #0x1                   	// #1
   36974:	mov	x0, x22
   36978:	mov	x1, x25
   3697c:	mov	x2, x21
   36980:	bl	c2d0 <__gmpn_lshift@plt>
   36984:	ldur	x28, [x29, #-64]
   36988:	str	x0, [x22, x20]
   3698c:	mov	x0, x19
   36990:	mov	x1, x22
   36994:	mov	x2, x28
   36998:	bl	c570 <__gmpn_cmp@plt>
   3699c:	tbnz	w0, #31, 369b8 <__gmpn_toom53_mul@@Base+0x33c>
   369a0:	ldur	x0, [x29, #-48]
   369a4:	mov	x1, x19
   369a8:	mov	x2, x22
   369ac:	mov	x3, x28
   369b0:	bl	c420 <__gmpn_sub_n@plt>
   369b4:	b	369d8 <__gmpn_toom53_mul@@Base+0x35c>
   369b8:	ldur	x0, [x29, #-48]
   369bc:	mov	x1, x22
   369c0:	mov	x2, x19
   369c4:	mov	x3, x28
   369c8:	bl	c420 <__gmpn_sub_n@plt>
   369cc:	ldur	w8, [x29, #-12]
   369d0:	eor	w8, w8, #0x1
   369d4:	stur	w8, [x29, #-12]
   369d8:	mov	x0, x19
   369dc:	mov	x1, x19
   369e0:	mov	x2, x22
   369e4:	mov	x3, x28
   369e8:	bl	cc30 <__gmpn_add_n@plt>
   369ec:	ldur	x2, [x29, #-24]
   369f0:	mov	x0, x23
   369f4:	mov	x1, x25
   369f8:	mov	x3, x21
   369fc:	bl	ce00 <__gmpn_addlsh1_n@plt>
   36a00:	subs	x22, x21, x24
   36a04:	mov	x25, x0
   36a08:	mov	x0, x23
   36a0c:	b.le	36a6c <__gmpn_toom53_mul@@Base+0x3f0>
   36a10:	ldur	x1, [x29, #-40]
   36a14:	mov	x2, x23
   36a18:	mov	x3, x24
   36a1c:	bl	ce00 <__gmpn_addlsh1_n@plt>
   36a20:	add	x24, x23, x24, lsl #3
   36a24:	mov	x28, x0
   36a28:	mov	w3, #0x1                   	// #1
   36a2c:	mov	x0, x24
   36a30:	mov	x1, x24
   36a34:	mov	x2, x22
   36a38:	bl	c2d0 <__gmpn_lshift@plt>
   36a3c:	add	x8, x0, x25, lsl #1
   36a40:	str	x8, [x23, x21, lsl #3]
   36a44:	ldr	x8, [x24]
   36a48:	adds	x8, x8, x28
   36a4c:	ldur	x28, [x29, #-104]
   36a50:	str	x8, [x24]
   36a54:	b.cc	36a88 <__gmpn_toom53_mul@@Base+0x40c>  // b.lo, b.ul, b.last
   36a58:	ldr	x8, [x24, #8]!
   36a5c:	adds	x8, x8, #0x1
   36a60:	str	x8, [x24]
   36a64:	b.cs	36a58 <__gmpn_toom53_mul@@Base+0x3dc>  // b.hs, b.nlast
   36a68:	b	36a88 <__gmpn_toom53_mul@@Base+0x40c>
   36a6c:	ldur	x1, [x29, #-40]
   36a70:	mov	x2, x23
   36a74:	mov	x3, x21
   36a78:	bl	ce00 <__gmpn_addlsh1_n@plt>
   36a7c:	ldur	x28, [x29, #-104]
   36a80:	add	x8, x0, x25, lsl #1
   36a84:	str	x8, [x23, x21, lsl #3]
   36a88:	mov	x2, x19
   36a8c:	ldur	x19, [x29, #-64]
   36a90:	ldur	x1, [x29, #-144]
   36a94:	mov	x0, x28
   36a98:	mov	x3, x19
   36a9c:	bl	cb50 <__gmpn_mul_n@plt>
   36aa0:	ldur	x8, [x29, #-32]
   36aa4:	ldur	x1, [x29, #-136]
   36aa8:	ldur	x2, [x29, #-48]
   36aac:	mov	x3, x19
   36ab0:	lsl	x22, x8, #3
   36ab4:	add	x8, x28, x22
   36ab8:	add	x0, x8, #0x8
   36abc:	stur	x0, [x29, #-32]
   36ac0:	bl	cb50 <__gmpn_mul_n@plt>
   36ac4:	ldur	x8, [x29, #-72]
   36ac8:	mov	x1, x27
   36acc:	mov	x2, x23
   36ad0:	mov	x3, x19
   36ad4:	lsl	x24, x8, #3
   36ad8:	add	x8, x28, x24
   36adc:	add	x25, x8, #0x10
   36ae0:	mov	x0, x25
   36ae4:	bl	cb50 <__gmpn_mul_n@plt>
   36ae8:	add	x8, x21, x21, lsl #1
   36aec:	lsl	x27, x8, #4
   36af0:	ldur	x1, [x29, #-128]
   36af4:	ldur	x2, [x29, #-88]
   36af8:	add	x8, x28, x27
   36afc:	add	x23, x8, #0x18
   36b00:	str	xzr, [x23, x22]
   36b04:	ldr	x8, [x1, x20]
   36b08:	ldr	x9, [x2, x20]
   36b0c:	mov	x0, x23
   36b10:	orr	x8, x9, x8
   36b14:	cmp	x8, #0x0
   36b18:	cinc	x3, x21, ne  // ne = any
   36b1c:	bl	cb50 <__gmpn_mul_n@plt>
   36b20:	ldur	x8, [x29, #-80]
   36b24:	ldur	x1, [x29, #-96]
   36b28:	mov	x2, x26
   36b2c:	add	x0, x8, x22
   36b30:	str	xzr, [x0, x22]
   36b34:	mov	x22, x8
   36b38:	ldr	x8, [x1, x20]
   36b3c:	ldr	x9, [x26, x20]
   36b40:	orr	x8, x9, x8
   36b44:	cmp	x8, #0x0
   36b48:	cinc	x3, x21, ne  // ne = any
   36b4c:	bl	cb50 <__gmpn_mul_n@plt>
   36b50:	ldur	x20, [x29, #-112]
   36b54:	ldur	x2, [x29, #-24]
   36b58:	mov	x0, x22
   36b5c:	mov	x3, x21
   36b60:	mov	x1, x20
   36b64:	bl	cb50 <__gmpn_mul_n@plt>
   36b68:	add	x0, x22, x27
   36b6c:	ldur	x26, [x29, #-120]
   36b70:	ldur	x27, [x29, #-56]
   36b74:	add	x3, x20, x24
   36b78:	cmp	x26, x27
   36b7c:	b.le	36b94 <__gmpn_toom53_mul@@Base+0x518>
   36b80:	mov	x1, x3
   36b84:	ldur	x3, [x29, #-40]
   36b88:	mov	x2, x26
   36b8c:	mov	x4, x27
   36b90:	b	36ba0 <__gmpn_toom53_mul@@Base+0x524>
   36b94:	ldur	x1, [x29, #-40]
   36b98:	mov	x2, x27
   36b9c:	mov	x4, x26
   36ba0:	bl	cea0 <__gmpn_mul@plt>
   36ba4:	add	x8, x28, x21, lsl #6
   36ba8:	add	x8, x8, #0x20
   36bac:	add	x7, x26, x27
   36bb0:	str	x8, [sp, #-16]!
   36bb4:	ldur	w2, [x29, #-12]
   36bb8:	ldur	x3, [x29, #-32]
   36bbc:	mov	x0, x22
   36bc0:	mov	x1, x21
   36bc4:	mov	x4, x23
   36bc8:	mov	x5, x28
   36bcc:	mov	x6, x25
   36bd0:	bl	c9c0 <__gmpn_toom_interpolate_7pts@plt>
   36bd4:	add	sp, sp, #0x10
   36bd8:	ldur	x0, [x29, #-8]
   36bdc:	cbnz	x0, 36c38 <__gmpn_toom53_mul@@Base+0x5bc>
   36be0:	mov	sp, x29
   36be4:	ldp	x20, x19, [sp, #80]
   36be8:	ldp	x22, x21, [sp, #64]
   36bec:	ldp	x24, x23, [sp, #48]
   36bf0:	ldp	x26, x25, [sp, #32]
   36bf4:	ldp	x28, x27, [sp, #16]
   36bf8:	ldp	x29, x30, [sp], #96
   36bfc:	ret
   36c00:	ldur	x23, [x29, #-88]
   36c04:	mov	x1, x22
   36c08:	mov	x2, x26
   36c0c:	mov	x3, x21
   36c10:	mov	x0, x23
   36c14:	bl	c420 <__gmpn_sub_n@plt>
   36c18:	ldur	w8, [x29, #-12]
   36c1c:	str	xzr, [x23, x21, lsl #3]
   36c20:	eor	w8, w8, #0x2
   36c24:	stur	w8, [x29, #-12]
   36c28:	b	36904 <__gmpn_toom53_mul@@Base+0x288>
   36c2c:	sub	x0, x29, #0x8
   36c30:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   36c34:	b	3671c <__gmpn_toom53_mul@@Base+0xa0>
   36c38:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   36c3c:	b	36be0 <__gmpn_toom53_mul@@Base+0x564>

0000000000036c40 <__gmpn_toom54_mul@@Base>:
   36c40:	sub	sp, sp, #0xc0
   36c44:	add	x8, x4, x4, lsl #2
   36c48:	stp	x29, x30, [sp, #96]
   36c4c:	stp	x24, x23, [sp, #144]
   36c50:	stp	x20, x19, [sp, #176]
   36c54:	add	x29, sp, #0x60
   36c58:	mov	x24, x3
   36c5c:	mov	x19, x1
   36c60:	cmp	x8, x2, lsl #2
   36c64:	mov	x20, x0
   36c68:	stp	x28, x27, [sp, #112]
   36c6c:	stp	x26, x25, [sp, #128]
   36c70:	stp	x22, x21, [sp, #160]
   36c74:	stur	x5, [x29, #-32]
   36c78:	b.le	36c84 <__gmpn_toom54_mul@@Base+0x44>
   36c7c:	sub	x8, x4, #0x1
   36c80:	b	36c94 <__gmpn_toom54_mul@@Base+0x54>
   36c84:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   36c88:	sub	x8, x2, #0x1
   36c8c:	movk	x9, #0xcccd
   36c90:	umulh	x8, x8, x9
   36c94:	lsr	x23, x8, #2
   36c98:	add	x21, x23, #0x1
   36c9c:	add	x26, x21, x21, lsl #1
   36ca0:	mov	w8, #0x28                  	// #40
   36ca4:	lsl	x25, x26, #3
   36ca8:	lsl	x9, x21, #2
   36cac:	madd	x8, x21, x8, x20
   36cb0:	add	x27, x20, x25
   36cb4:	sub	x5, x2, x9
   36cb8:	sub	x28, x4, x26
   36cbc:	add	x0, x8, #0x10
   36cc0:	mov	w2, #0x4                   	// #4
   36cc4:	mov	w6, #0x2                   	// #2
   36cc8:	mov	x1, x27
   36ccc:	mov	x3, x19
   36cd0:	mov	x4, x21
   36cd4:	mov	x7, x20
   36cd8:	str	x9, [sp, #40]
   36cdc:	stur	x5, [x29, #-40]
   36ce0:	str	x28, [sp, #16]
   36ce4:	stur	x0, [x29, #-24]
   36ce8:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   36cec:	lsl	x8, x26, #1
   36cf0:	str	x8, [sp, #48]
   36cf4:	add	x8, x20, x26, lsl #4
   36cf8:	add	x9, x20, x21, lsl #5
   36cfc:	stur	x19, [x29, #-16]
   36d00:	str	x26, [sp, #32]
   36d04:	add	x26, x8, #0x18
   36d08:	add	x19, x9, #0x8
   36d0c:	mov	w22, w0
   36d10:	mov	w2, #0x3                   	// #3
   36d14:	mov	w6, #0x2                   	// #2
   36d18:	mov	x0, x26
   36d1c:	mov	x1, x19
   36d20:	mov	x3, x24
   36d24:	mov	x4, x21
   36d28:	mov	x5, x28
   36d2c:	mov	x7, x20
   36d30:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   36d34:	eor	w8, w0, w22
   36d38:	add	x22, x23, #0x2
   36d3c:	mov	x0, x20
   36d40:	mov	x1, x27
   36d44:	mov	x2, x19
   36d48:	mov	x3, x22
   36d4c:	stur	x24, [x29, #-8]
   36d50:	str	w8, [sp, #12]
   36d54:	bl	cb50 <__gmpn_mul_n@plt>
   36d58:	ldp	x28, x24, [x29, #-32]
   36d5c:	mov	x2, x26
   36d60:	mov	x3, x22
   36d64:	add	x8, x28, x25
   36d68:	add	x25, x8, #0x8
   36d6c:	mov	x0, x25
   36d70:	mov	x1, x24
   36d74:	bl	cb50 <__gmpn_mul_n@plt>
   36d78:	ldr	w3, [sp, #12]
   36d7c:	mov	w23, #0x1                   	// #1
   36d80:	bfi	x23, x21, #1, #63
   36d84:	mov	w5, #0x2                   	// #2
   36d88:	mov	w6, #0x4                   	// #4
   36d8c:	mov	x0, x25
   36d90:	mov	x1, x23
   36d94:	mov	x2, x20
   36d98:	mov	x4, x21
   36d9c:	str	x25, [sp, #24]
   36da0:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   36da4:	ldur	x3, [x29, #-16]
   36da8:	ldur	x5, [x29, #-40]
   36dac:	mov	w2, #0x4                   	// #4
   36db0:	mov	x0, x24
   36db4:	mov	x1, x27
   36db8:	mov	x4, x21
   36dbc:	mov	x6, x20
   36dc0:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   36dc4:	ldr	x25, [sp, #16]
   36dc8:	ldur	x2, [x29, #-8]
   36dcc:	mov	w24, w0
   36dd0:	mov	x0, x26
   36dd4:	mov	x1, x19
   36dd8:	mov	x3, x21
   36ddc:	mov	x4, x25
   36de0:	mov	x5, x20
   36de4:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   36de8:	eor	w8, w0, w24
   36dec:	mov	x0, x20
   36df0:	mov	x1, x27
   36df4:	mov	x2, x19
   36df8:	mov	x3, x22
   36dfc:	str	w8, [sp, #12]
   36e00:	bl	cb50 <__gmpn_mul_n@plt>
   36e04:	ldur	x24, [x29, #-24]
   36e08:	mov	x0, x28
   36e0c:	mov	x2, x26
   36e10:	mov	x3, x22
   36e14:	mov	x1, x24
   36e18:	bl	cb50 <__gmpn_mul_n@plt>
   36e1c:	ldr	w3, [sp, #12]
   36e20:	mov	x0, x28
   36e24:	mov	x1, x23
   36e28:	mov	x2, x20
   36e2c:	mov	x4, x21
   36e30:	mov	w5, wzr
   36e34:	mov	w6, wzr
   36e38:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   36e3c:	ldur	x28, [x29, #-40]
   36e40:	ldur	x3, [x29, #-16]
   36e44:	mov	w2, #0x4                   	// #4
   36e48:	mov	x0, x24
   36e4c:	mov	x1, x27
   36e50:	mov	x4, x21
   36e54:	mov	x5, x28
   36e58:	mov	x6, x20
   36e5c:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   36e60:	ldur	x2, [x29, #-8]
   36e64:	mov	w24, w0
   36e68:	mov	x0, x26
   36e6c:	mov	x1, x19
   36e70:	mov	x3, x21
   36e74:	mov	x4, x25
   36e78:	mov	x5, x20
   36e7c:	bl	cf30 <__gmpn_toom_eval_dgr3_pm2@plt>
   36e80:	eor	w24, w0, w24
   36e84:	mov	x0, x20
   36e88:	mov	x1, x27
   36e8c:	mov	x2, x19
   36e90:	mov	x3, x22
   36e94:	bl	cb50 <__gmpn_mul_n@plt>
   36e98:	mov	x3, x22
   36e9c:	ldp	x1, x22, [x29, #-24]
   36ea0:	mov	x0, x27
   36ea4:	mov	x2, x26
   36ea8:	bl	cb50 <__gmpn_mul_n@plt>
   36eac:	ldur	x19, [x29, #-8]
   36eb0:	mov	w5, #0x1                   	// #1
   36eb4:	mov	w6, #0x2                   	// #2
   36eb8:	mov	x0, x27
   36ebc:	mov	x1, x23
   36ec0:	mov	x2, x20
   36ec4:	mov	w3, w24
   36ec8:	mov	x4, x21
   36ecc:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   36ed0:	mov	x0, x20
   36ed4:	mov	x1, x22
   36ed8:	mov	x2, x19
   36edc:	mov	x3, x21
   36ee0:	bl	cb50 <__gmpn_mul_n@plt>
   36ee4:	mov	w8, #0x38                  	// #56
   36ee8:	cmp	x28, x25
   36eec:	madd	x0, x21, x8, x20
   36ef0:	b.le	36f10 <__gmpn_toom54_mul@@Base+0x2d0>
   36ef4:	ldr	x8, [sp, #40]
   36ef8:	mov	x2, x28
   36efc:	mov	x4, x25
   36f00:	add	x1, x22, x8, lsl #3
   36f04:	ldr	x8, [sp, #32]
   36f08:	add	x3, x19, x8, lsl #3
   36f0c:	b	36f28 <__gmpn_toom54_mul@@Base+0x2e8>
   36f10:	ldr	x8, [sp, #32]
   36f14:	mov	x2, x25
   36f18:	mov	x4, x28
   36f1c:	add	x1, x19, x8, lsl #3
   36f20:	ldr	x8, [sp, #40]
   36f24:	add	x3, x22, x8, lsl #3
   36f28:	bl	cea0 <__gmpn_mul@plt>
   36f2c:	ldr	x8, [sp, #48]
   36f30:	ldur	x3, [x29, #-32]
   36f34:	ldr	x2, [sp, #24]
   36f38:	add	x4, x28, x25
   36f3c:	mov	x0, x20
   36f40:	add	x8, x3, x8, lsl #3
   36f44:	add	x5, x8, #0x10
   36f48:	mov	x1, x21
   36f4c:	bl	c950 <__gmpn_toom_interpolate_8pts@plt>
   36f50:	ldp	x20, x19, [sp, #176]
   36f54:	ldp	x22, x21, [sp, #160]
   36f58:	ldp	x24, x23, [sp, #144]
   36f5c:	ldp	x26, x25, [sp, #128]
   36f60:	ldp	x28, x27, [sp, #112]
   36f64:	ldp	x29, x30, [sp, #96]
   36f68:	add	sp, sp, #0xc0
   36f6c:	ret

0000000000036f70 <__gmpn_toom63_mul@@Base>:
   36f70:	sub	sp, sp, #0xd0
   36f74:	lsl	x8, x4, #1
   36f78:	cmp	x8, x2
   36f7c:	mov	w9, #0x6                   	// #6
   36f80:	mov	w10, #0x3                   	// #3
   36f84:	csel	x8, x4, x2, gt
   36f88:	csel	x9, x10, x9, gt
   36f8c:	sub	x8, x8, #0x1
   36f90:	stp	x24, x23, [sp, #160]
   36f94:	udiv	x24, x8, x9
   36f98:	stp	x22, x21, [sp, #176]
   36f9c:	add	x21, x24, #0x1
   36fa0:	stp	x29, x30, [sp, #112]
   36fa4:	add	x29, sp, #0x70
   36fa8:	add	x8, x21, x21, lsl #2
   36fac:	lsl	x9, x21, #1
   36fb0:	stp	x26, x25, [sp, #144]
   36fb4:	stp	x20, x19, [sp, #192]
   36fb8:	stur	x5, [x29, #-8]
   36fbc:	mov	x20, x0
   36fc0:	sub	x5, x2, x8
   36fc4:	str	x8, [sp, #32]
   36fc8:	add	x8, x0, x8, lsl #3
   36fcc:	add	x25, x9, x21
   36fd0:	stp	x28, x27, [sp, #128]
   36fd4:	mov	x28, x3
   36fd8:	mov	x3, x1
   36fdc:	sub	x23, x4, x9
   36fe0:	add	x0, x8, #0x10
   36fe4:	add	x1, x20, x25, lsl #3
   36fe8:	mov	w2, #0x5                   	// #5
   36fec:	mov	w6, #0x2                   	// #2
   36ff0:	mov	x4, x21
   36ff4:	mov	x7, x20
   36ff8:	str	x9, [sp, #56]
   36ffc:	stp	x5, x0, [x29, #-48]
   37000:	stp	x1, x3, [x29, #-24]
   37004:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   37008:	lsl	x22, x21, #3
   3700c:	add	x26, x28, x22
   37010:	mov	w27, w0
   37014:	mov	w3, #0x2                   	// #2
   37018:	mov	x0, x20
   3701c:	mov	x1, x26
   37020:	mov	x2, x21
   37024:	bl	c2d0 <__gmpn_lshift@plt>
   37028:	lsl	x8, x25, #1
   3702c:	str	x8, [sp, #16]
   37030:	add	x8, x20, x25, lsl #4
   37034:	add	x19, x8, #0x18
   37038:	str	x0, [x20, x22]
   3703c:	add	x1, x28, x21, lsl #4
   37040:	mov	w3, #0x4                   	// #4
   37044:	mov	x0, x19
   37048:	mov	x2, x23
   3704c:	stur	x1, [x29, #-32]
   37050:	bl	c2d0 <__gmpn_lshift@plt>
   37054:	cmp	x21, x23
   37058:	str	x0, [x19, x23, lsl #3]
   3705c:	stp	x23, x26, [sp]
   37060:	str	x28, [sp, #48]
   37064:	b.ne	3708c <__gmpn_toom63_mul@@Base+0x11c>  // b.any
   37068:	mov	x0, x19
   3706c:	mov	x1, x19
   37070:	mov	x2, x28
   37074:	mov	x3, x21
   37078:	bl	cc30 <__gmpn_add_n@plt>
   3707c:	ldr	x8, [x19, x22]
   37080:	add	x8, x8, x0
   37084:	str	x8, [x19, x22]
   37088:	b	370a8 <__gmpn_toom63_mul@@Base+0x138>
   3708c:	add	x4, x23, #0x1
   37090:	mov	x0, x19
   37094:	mov	x1, x28
   37098:	mov	x2, x21
   3709c:	mov	x3, x19
   370a0:	bl	c970 <__gmpn_add@plt>
   370a4:	str	x0, [x19, x21, lsl #3]
   370a8:	add	x8, x20, x21, lsl #5
   370ac:	add	x22, x8, #0x8
   370b0:	add	x24, x24, #0x2
   370b4:	mov	x0, x22
   370b8:	mov	x1, x19
   370bc:	mov	x2, x20
   370c0:	mov	x3, x24
   370c4:	bl	373ec <__gmpn_toom63_mul@@Base+0x47c>
   370c8:	ldur	x26, [x29, #-24]
   370cc:	eor	w27, w0, w27
   370d0:	mov	x0, x20
   370d4:	mov	x2, x22
   370d8:	mov	x1, x26
   370dc:	mov	x3, x24
   370e0:	bl	cb50 <__gmpn_mul_n@plt>
   370e4:	ldur	x28, [x29, #-8]
   370e8:	mov	x2, x19
   370ec:	mov	x3, x24
   370f0:	add	x8, x28, x25, lsl #3
   370f4:	ldur	x25, [x29, #-40]
   370f8:	add	x23, x8, #0x8
   370fc:	mov	x0, x23
   37100:	mov	x1, x25
   37104:	bl	cb50 <__gmpn_mul_n@plt>
   37108:	ldr	x8, [sp, #56]
   3710c:	mov	w5, #0x2                   	// #2
   37110:	mov	w6, #0x4                   	// #4
   37114:	mov	x0, x23
   37118:	orr	x1, x8, #0x1
   3711c:	mov	x2, x20
   37120:	mov	w3, w27
   37124:	mov	x4, x21
   37128:	str	x23, [sp, #24]
   3712c:	str	x1, [sp, #56]
   37130:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37134:	ldur	x3, [x29, #-16]
   37138:	ldur	x5, [x29, #-48]
   3713c:	mov	w2, #0x5                   	// #5
   37140:	mov	x0, x25
   37144:	mov	x1, x26
   37148:	mov	x4, x21
   3714c:	mov	x6, x20
   37150:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   37154:	ldr	x8, [sp, #16]
   37158:	ldr	x23, [sp]
   3715c:	ldr	x1, [sp, #48]
   37160:	ldur	x3, [x29, #-32]
   37164:	add	x8, x28, x8, lsl #3
   37168:	add	x25, x8, #0x10
   3716c:	str	w0, [sp, #44]
   37170:	mov	x0, x25
   37174:	mov	x2, x21
   37178:	mov	x4, x23
   3717c:	bl	c970 <__gmpn_add@plt>
   37180:	ldr	x26, [sp, #8]
   37184:	mov	x27, x0
   37188:	mov	x0, x19
   3718c:	mov	x1, x25
   37190:	mov	x2, x26
   37194:	mov	x3, x21
   37198:	bl	cc30 <__gmpn_add_n@plt>
   3719c:	add	x8, x0, x27
   371a0:	str	x8, [x19, x21, lsl #3]
   371a4:	str	x25, [sp, #16]
   371a8:	cbnz	x27, 371c0 <__gmpn_toom63_mul@@Base+0x250>
   371ac:	mov	x0, x25
   371b0:	mov	x1, x26
   371b4:	mov	x2, x21
   371b8:	bl	c570 <__gmpn_cmp@plt>
   371bc:	tbnz	w0, #31, 373c4 <__gmpn_toom63_mul@@Base+0x454>
   371c0:	mov	x0, x22
   371c4:	mov	x1, x25
   371c8:	mov	x2, x26
   371cc:	mov	x3, x21
   371d0:	bl	c420 <__gmpn_sub_n@plt>
   371d4:	sub	x8, x27, x0
   371d8:	str	x8, [x22, x21, lsl #3]
   371dc:	ldur	x25, [x29, #-24]
   371e0:	mov	x0, x20
   371e4:	mov	x2, x22
   371e8:	mov	x3, x24
   371ec:	mov	x1, x25
   371f0:	bl	cb50 <__gmpn_mul_n@plt>
   371f4:	ldur	x28, [x29, #-8]
   371f8:	ldur	x27, [x29, #-40]
   371fc:	mov	x2, x19
   37200:	mov	x3, x24
   37204:	mov	x0, x28
   37208:	mov	x1, x27
   3720c:	bl	cb50 <__gmpn_mul_n@plt>
   37210:	ldr	x1, [sp, #56]
   37214:	ldr	w3, [sp, #44]
   37218:	mov	x0, x28
   3721c:	mov	x2, x20
   37220:	mov	x4, x21
   37224:	mov	w5, wzr
   37228:	mov	w6, wzr
   3722c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37230:	ldur	x28, [x29, #-48]
   37234:	ldur	x3, [x29, #-16]
   37238:	mov	w2, #0x5                   	// #5
   3723c:	mov	x0, x27
   37240:	mov	x1, x25
   37244:	mov	x4, x21
   37248:	mov	x5, x28
   3724c:	mov	x6, x20
   37250:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   37254:	mov	w27, w0
   37258:	mov	w3, #0x1                   	// #1
   3725c:	mov	x0, x20
   37260:	mov	x1, x26
   37264:	mov	x2, x21
   37268:	bl	c2d0 <__gmpn_lshift@plt>
   3726c:	ldur	x1, [x29, #-32]
   37270:	str	x0, [x20, x21, lsl #3]
   37274:	mov	w3, #0x2                   	// #2
   37278:	mov	x0, x19
   3727c:	mov	x2, x23
   37280:	bl	c2d0 <__gmpn_lshift@plt>
   37284:	cmp	x21, x23
   37288:	str	x0, [x19, x23, lsl #3]
   3728c:	b.ne	372b4 <__gmpn_toom63_mul@@Base+0x344>  // b.any
   37290:	ldr	x26, [sp, #48]
   37294:	mov	x0, x19
   37298:	mov	x1, x19
   3729c:	mov	x3, x21
   372a0:	mov	x2, x26
   372a4:	bl	cc30 <__gmpn_add_n@plt>
   372a8:	ldr	x8, [x19, x21, lsl #3]
   372ac:	add	x0, x8, x0
   372b0:	b	372d0 <__gmpn_toom63_mul@@Base+0x360>
   372b4:	ldr	x26, [sp, #48]
   372b8:	add	x4, x23, #0x1
   372bc:	mov	x0, x19
   372c0:	mov	x2, x21
   372c4:	mov	x1, x26
   372c8:	mov	x3, x19
   372cc:	bl	c970 <__gmpn_add@plt>
   372d0:	str	x0, [x19, x21, lsl #3]
   372d4:	mov	x0, x22
   372d8:	mov	x1, x19
   372dc:	mov	x2, x20
   372e0:	mov	x3, x24
   372e4:	bl	373ec <__gmpn_toom63_mul@@Base+0x47c>
   372e8:	eor	w25, w0, w27
   372ec:	ldur	x27, [x29, #-24]
   372f0:	mov	x0, x20
   372f4:	mov	x2, x22
   372f8:	mov	x3, x24
   372fc:	mov	x1, x27
   37300:	bl	cb50 <__gmpn_mul_n@plt>
   37304:	ldur	x1, [x29, #-40]
   37308:	mov	x0, x27
   3730c:	mov	x2, x19
   37310:	mov	x3, x24
   37314:	bl	cb50 <__gmpn_mul_n@plt>
   37318:	ldr	x1, [sp, #56]
   3731c:	mov	w5, #0x1                   	// #1
   37320:	mov	w6, #0x2                   	// #2
   37324:	mov	x0, x27
   37328:	mov	x2, x20
   3732c:	mov	w3, w25
   37330:	mov	x4, x21
   37334:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37338:	ldur	x19, [x29, #-16]
   3733c:	mov	x0, x20
   37340:	mov	x2, x26
   37344:	mov	x3, x21
   37348:	mov	x1, x19
   3734c:	bl	cb50 <__gmpn_mul_n@plt>
   37350:	mov	w8, #0x38                  	// #56
   37354:	madd	x0, x21, x8, x20
   37358:	ldr	x8, [sp, #32]
   3735c:	cmp	x28, x23
   37360:	add	x3, x19, x8, lsl #3
   37364:	b.le	3737c <__gmpn_toom63_mul@@Base+0x40c>
   37368:	mov	x1, x3
   3736c:	ldur	x3, [x29, #-32]
   37370:	mov	x2, x28
   37374:	mov	x4, x23
   37378:	b	37388 <__gmpn_toom63_mul@@Base+0x418>
   3737c:	ldur	x1, [x29, #-32]
   37380:	mov	x2, x23
   37384:	mov	x4, x28
   37388:	bl	cea0 <__gmpn_mul@plt>
   3738c:	ldp	x5, x2, [sp, #16]
   37390:	ldur	x3, [x29, #-8]
   37394:	add	x4, x28, x23
   37398:	mov	x0, x20
   3739c:	mov	x1, x21
   373a0:	bl	c950 <__gmpn_toom_interpolate_8pts@plt>
   373a4:	ldp	x20, x19, [sp, #192]
   373a8:	ldp	x22, x21, [sp, #176]
   373ac:	ldp	x24, x23, [sp, #160]
   373b0:	ldp	x26, x25, [sp, #144]
   373b4:	ldp	x28, x27, [sp, #128]
   373b8:	ldp	x29, x30, [sp, #112]
   373bc:	add	sp, sp, #0xd0
   373c0:	ret
   373c4:	mov	x0, x22
   373c8:	mov	x1, x26
   373cc:	mov	x2, x25
   373d0:	mov	x3, x21
   373d4:	bl	c420 <__gmpn_sub_n@plt>
   373d8:	ldr	w8, [sp, #44]
   373dc:	str	xzr, [x22, x21, lsl #3]
   373e0:	mvn	w8, w8
   373e4:	str	w8, [sp, #44]
   373e8:	b	371dc <__gmpn_toom63_mul@@Base+0x26c>
   373ec:	stp	x29, x30, [sp, #-48]!
   373f0:	stp	x22, x21, [sp, #16]
   373f4:	stp	x20, x19, [sp, #32]
   373f8:	mov	x29, sp
   373fc:	mov	x19, x3
   37400:	mov	x20, x2
   37404:	mov	x21, x1
   37408:	bl	37438 <__gmpn_toom63_mul@@Base+0x4c8>
   3740c:	mov	w22, w0
   37410:	mov	x0, x21
   37414:	mov	x1, x21
   37418:	mov	x2, x20
   3741c:	mov	x3, x19
   37420:	bl	cc30 <__gmpn_add_n@plt>
   37424:	mov	w0, w22
   37428:	ldp	x20, x19, [sp, #32]
   3742c:	ldp	x22, x21, [sp, #16]
   37430:	ldp	x29, x30, [sp], #48
   37434:	ret
   37438:	stp	x29, x30, [sp, #-16]!
   3743c:	cmp	x3, #0x1
   37440:	mov	x29, sp
   37444:	b.lt	37494 <__gmpn_toom63_mul@@Base+0x524>  // b.tstop
   37448:	mov	x8, x1
   3744c:	sub	x9, x1, #0x8
   37450:	sub	x10, x2, #0x8
   37454:	sub	x11, x0, #0x8
   37458:	lsl	x12, x3, #3
   3745c:	ldr	x13, [x9, x12]
   37460:	ldr	x12, [x10, x12]
   37464:	cmp	x13, x12
   37468:	b.ne	37488 <__gmpn_toom63_mul@@Base+0x518>  // b.any
   3746c:	sub	x12, x3, #0x1
   37470:	add	x13, x12, #0x1
   37474:	cmp	x13, #0x1
   37478:	str	xzr, [x11, x3, lsl #3]
   3747c:	mov	x3, x12
   37480:	b.gt	37458 <__gmpn_toom63_mul@@Base+0x4e8>
   37484:	b	37494 <__gmpn_toom63_mul@@Base+0x524>
   37488:	b.ls	374a0 <__gmpn_toom63_mul@@Base+0x530>  // b.plast
   3748c:	mov	x1, x8
   37490:	bl	c420 <__gmpn_sub_n@plt>
   37494:	mov	w0, wzr
   37498:	ldp	x29, x30, [sp], #16
   3749c:	ret
   374a0:	mov	x1, x2
   374a4:	mov	x2, x8
   374a8:	bl	c420 <__gmpn_sub_n@plt>
   374ac:	mov	w0, #0xffffffff            	// #-1
   374b0:	ldp	x29, x30, [sp], #16
   374b4:	ret

00000000000374b8 <__gmpn_toom44_mul@@Base>:
   374b8:	sub	sp, sp, #0xf0
   374bc:	stp	x26, x25, [sp, #176]
   374c0:	mov	x26, x2
   374c4:	stp	x20, x19, [sp, #224]
   374c8:	add	x19, x26, #0x3
   374cc:	stp	x22, x21, [sp, #208]
   374d0:	asr	x21, x19, #2
   374d4:	lsl	x10, x21, #1
   374d8:	add	x9, x5, x21, lsl #6
   374dc:	str	x10, [sp, #16]
   374e0:	add	x10, x10, x21
   374e4:	add	x8, x0, x21, lsl #3
   374e8:	sub	x25, x26, x10
   374ec:	add	x22, x9, #0x28
   374f0:	stp	x29, x30, [sp, #144]
   374f4:	stp	x28, x27, [sp, #160]
   374f8:	stp	x24, x23, [sp, #192]
   374fc:	add	x29, sp, #0x90
   37500:	mov	x23, x5
   37504:	mov	x28, x3
   37508:	mov	x2, x1
   3750c:	add	x1, x8, #0x8
   37510:	str	x4, [sp, #72]
   37514:	sub	x24, x4, x10
   37518:	mov	x3, x21
   3751c:	mov	x4, x25
   37520:	mov	x5, x22
   37524:	mov	x20, x0
   37528:	stp	x10, x2, [x29, #-24]
   3752c:	stur	x1, [x29, #-64]
   37530:	bl	cf30 <__gmpn_toom_eval_dgr3_pm2@plt>
   37534:	and	x8, x19, #0xfffffffffffffffc
   37538:	add	x9, x20, x21, lsl #4
   3753c:	str	x8, [sp, #32]
   37540:	add	x8, x20, x8, lsl #3
   37544:	add	x27, x8, #0x10
   37548:	add	x19, x9, #0x10
   3754c:	str	w0, [sp, #40]
   37550:	mov	x0, x27
   37554:	mov	x1, x19
   37558:	mov	x2, x28
   3755c:	mov	x3, x21
   37560:	mov	x4, x24
   37564:	mov	x5, x22
   37568:	str	x9, [sp, #56]
   3756c:	stp	x28, x24, [x29, #-48]
   37570:	bl	cf30 <__gmpn_toom_eval_dgr3_pm2@plt>
   37574:	add	x24, x21, #0x1
   37578:	str	w0, [sp, #28]
   3757c:	cmp	x26, #0xbc
   37580:	mov	x0, x23
   37584:	mov	x1, x20
   37588:	mov	x2, x24
   3758c:	mov	x3, x27
   37590:	mov	x4, x24
   37594:	mov	x5, x22
   37598:	stur	x26, [x29, #-8]
   3759c:	b.le	375a8 <__gmpn_toom44_mul@@Base+0xf0>
   375a0:	bl	c1e0 <__gmpn_toom33_mul@plt>
   375a4:	b	375ac <__gmpn_toom44_mul@@Base+0xf4>
   375a8:	bl	d630 <__gmpn_toom22_mul@plt>
   375ac:	ldr	x26, [sp, #16]
   375b0:	ldp	x28, x9, [x29, #-16]
   375b4:	ldur	x1, [x29, #-64]
   375b8:	mov	x2, x24
   375bc:	add	x8, x23, x26, lsl #3
   375c0:	cmp	x9, #0xbc
   375c4:	add	x0, x8, #0x8
   375c8:	mov	x3, x19
   375cc:	mov	x4, x24
   375d0:	mov	x5, x22
   375d4:	stur	x23, [x29, #-32]
   375d8:	str	x0, [sp, #64]
   375dc:	str	x19, [sp, #48]
   375e0:	b.le	375ec <__gmpn_toom44_mul@@Base+0x134>
   375e4:	bl	c1e0 <__gmpn_toom33_mul@plt>
   375e8:	b	375f0 <__gmpn_toom44_mul@@Base+0x138>
   375ec:	bl	d630 <__gmpn_toom22_mul@plt>
   375f0:	add	x1, x28, x21, lsl #3
   375f4:	mov	x0, x20
   375f8:	mov	x2, x28
   375fc:	mov	x3, x21
   37600:	bl	ce00 <__gmpn_addlsh1_n@plt>
   37604:	mov	x23, x0
   37608:	add	x1, x28, x26, lsl #3
   3760c:	mov	x0, x20
   37610:	mov	x2, x20
   37614:	mov	x3, x21
   37618:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3761c:	subs	x19, x21, x25
   37620:	mov	x8, x28
   37624:	add	x28, x0, x23, lsl #1
   37628:	stur	x25, [x29, #-56]
   3762c:	b.le	37698 <__gmpn_toom44_mul@@Base+0x1e0>
   37630:	ldur	x9, [x29, #-24]
   37634:	mov	x0, x20
   37638:	mov	x2, x20
   3763c:	mov	x3, x25
   37640:	add	x1, x8, x9, lsl #3
   37644:	bl	ce00 <__gmpn_addlsh1_n@plt>
   37648:	mov	x8, x25
   3764c:	add	x23, x20, x8, lsl #3
   37650:	mov	x25, x0
   37654:	mov	w3, #0x1                   	// #1
   37658:	mov	x0, x23
   3765c:	mov	x1, x23
   37660:	mov	x2, x19
   37664:	bl	c2d0 <__gmpn_lshift@plt>
   37668:	add	x8, x0, x28, lsl #1
   3766c:	str	x8, [x20, x21, lsl #3]
   37670:	ldr	x8, [x23]
   37674:	adds	x8, x8, x25
   37678:	str	x8, [x23]
   3767c:	b.cc	376b8 <__gmpn_toom44_mul@@Base+0x200>  // b.lo, b.ul, b.last
   37680:	add	x8, x23, #0x8
   37684:	ldr	x9, [x8]
   37688:	adds	x9, x9, #0x1
   3768c:	str	x9, [x8], #8
   37690:	b.cs	37684 <__gmpn_toom44_mul@@Base+0x1cc>  // b.hs, b.nlast
   37694:	b	376b8 <__gmpn_toom44_mul@@Base+0x200>
   37698:	ldur	x9, [x29, #-24]
   3769c:	mov	x0, x20
   376a0:	mov	x2, x20
   376a4:	mov	x3, x21
   376a8:	add	x1, x8, x9, lsl #3
   376ac:	bl	ce00 <__gmpn_addlsh1_n@plt>
   376b0:	add	x8, x0, x28, lsl #1
   376b4:	str	x8, [x20, x21, lsl #3]
   376b8:	ldur	x28, [x29, #-48]
   376bc:	mov	x0, x27
   376c0:	mov	x3, x21
   376c4:	add	x1, x28, x21, lsl #3
   376c8:	mov	x2, x28
   376cc:	bl	ce00 <__gmpn_addlsh1_n@plt>
   376d0:	mov	x23, x0
   376d4:	add	x1, x28, x26, lsl #3
   376d8:	mov	x0, x27
   376dc:	mov	x2, x27
   376e0:	mov	x3, x21
   376e4:	bl	ce00 <__gmpn_addlsh1_n@plt>
   376e8:	ldur	x25, [x29, #-40]
   376ec:	add	x26, x0, x23, lsl #1
   376f0:	subs	x19, x21, x25
   376f4:	b.le	37780 <__gmpn_toom44_mul@@Base+0x2c8>
   376f8:	ldur	x8, [x29, #-24]
   376fc:	mov	x0, x27
   37700:	mov	x2, x27
   37704:	mov	x3, x25
   37708:	add	x1, x28, x8, lsl #3
   3770c:	bl	ce00 <__gmpn_addlsh1_n@plt>
   37710:	ldur	x8, [x29, #-40]
   37714:	mov	x23, x0
   37718:	mov	w3, #0x1                   	// #1
   3771c:	mov	x2, x19
   37720:	add	x25, x27, x8, lsl #3
   37724:	mov	x0, x25
   37728:	mov	x1, x25
   3772c:	bl	c2d0 <__gmpn_lshift@plt>
   37730:	add	x8, x0, x26, lsl #1
   37734:	str	x8, [x27, x21, lsl #3]
   37738:	ldr	x8, [x25]
   3773c:	ldr	x10, [sp, #32]
   37740:	adds	x8, x8, x23
   37744:	str	x8, [x25]
   37748:	ldur	x25, [x29, #-40]
   3774c:	ldur	x23, [x29, #-56]
   37750:	b.cc	377a8 <__gmpn_toom44_mul@@Base+0x2f0>  // b.lo, b.ul, b.last
   37754:	ldr	x8, [sp, #72]
   37758:	ldur	x9, [x29, #-24]
   3775c:	add	x8, x8, x10
   37760:	sub	x8, x8, x9
   37764:	add	x8, x20, x8, lsl #3
   37768:	add	x8, x8, #0x18
   3776c:	ldr	x9, [x8]
   37770:	adds	x9, x9, #0x1
   37774:	str	x9, [x8], #8
   37778:	b.cs	3776c <__gmpn_toom44_mul@@Base+0x2b4>  // b.hs, b.nlast
   3777c:	b	377a8 <__gmpn_toom44_mul@@Base+0x2f0>
   37780:	ldur	x8, [x29, #-24]
   37784:	mov	x0, x27
   37788:	mov	x2, x27
   3778c:	mov	x3, x21
   37790:	add	x1, x28, x8, lsl #3
   37794:	bl	ce00 <__gmpn_addlsh1_n@plt>
   37798:	ldur	x23, [x29, #-56]
   3779c:	ldr	x10, [sp, #32]
   377a0:	add	x8, x0, x26, lsl #1
   377a4:	str	x8, [x27, x21, lsl #3]
   377a8:	ldr	w8, [sp, #40]
   377ac:	ldr	w9, [sp, #28]
   377b0:	mov	x1, x20
   377b4:	mov	x2, x24
   377b8:	mov	x3, x27
   377bc:	eor	w19, w9, w8
   377c0:	ldur	x8, [x29, #-32]
   377c4:	ldur	x9, [x29, #-8]
   377c8:	mov	x4, x24
   377cc:	mov	x5, x22
   377d0:	add	x8, x8, x10, lsl #3
   377d4:	cmp	x9, #0xbc
   377d8:	add	x0, x8, #0x10
   377dc:	str	x0, [sp, #40]
   377e0:	b.le	377ec <__gmpn_toom44_mul@@Base+0x334>
   377e4:	bl	c1e0 <__gmpn_toom33_mul@plt>
   377e8:	b	377f0 <__gmpn_toom44_mul@@Base+0x338>
   377ec:	bl	d630 <__gmpn_toom22_mul@plt>
   377f0:	ldur	x28, [x29, #-64]
   377f4:	ldur	x2, [x29, #-16]
   377f8:	and	w8, w19, #0x1
   377fc:	mov	x0, x20
   37800:	mov	x1, x28
   37804:	mov	x3, x21
   37808:	mov	x4, x23
   3780c:	mov	x5, x22
   37810:	str	w8, [sp, #28]
   37814:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   37818:	ldr	x26, [sp, #48]
   3781c:	ldur	x2, [x29, #-48]
   37820:	and	w19, w0, #0x2
   37824:	mov	x0, x27
   37828:	mov	x1, x26
   3782c:	mov	x3, x21
   37830:	mov	x4, x25
   37834:	mov	x5, x22
   37838:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   3783c:	ldur	x8, [x29, #-32]
   37840:	ldur	x9, [x29, #-8]
   37844:	add	x25, x21, x21, lsl #1
   37848:	mov	w23, w0
   3784c:	add	x8, x8, x25, lsl #4
   37850:	cmp	x9, #0xbc
   37854:	add	x0, x8, #0x18
   37858:	mov	x1, x28
   3785c:	mov	x2, x24
   37860:	mov	x3, x26
   37864:	mov	x4, x24
   37868:	mov	x5, x22
   3786c:	str	x0, [sp, #32]
   37870:	b.le	3787c <__gmpn_toom44_mul@@Base+0x3c4>
   37874:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37878:	b	37880 <__gmpn_toom44_mul@@Base+0x3c8>
   3787c:	bl	d630 <__gmpn_toom22_mul@plt>
   37880:	ldr	w8, [sp, #28]
   37884:	and	w23, w23, #0x2
   37888:	orr	w19, w19, w8
   3788c:	ldur	x8, [x29, #-8]
   37890:	cmp	x8, #0xbc
   37894:	b.le	378b8 <__gmpn_toom44_mul@@Base+0x400>
   37898:	ldr	x0, [sp, #56]
   3789c:	mov	x1, x20
   378a0:	mov	x2, x24
   378a4:	mov	x3, x27
   378a8:	mov	x4, x24
   378ac:	mov	x5, x22
   378b0:	bl	c1e0 <__gmpn_toom33_mul@plt>
   378b4:	b	378d4 <__gmpn_toom44_mul@@Base+0x41c>
   378b8:	ldr	x0, [sp, #56]
   378bc:	mov	x1, x20
   378c0:	mov	x2, x24
   378c4:	mov	x3, x27
   378c8:	mov	x4, x24
   378cc:	mov	x5, x22
   378d0:	bl	d630 <__gmpn_toom22_mul@plt>
   378d4:	ldur	x8, [x29, #-8]
   378d8:	ldur	x26, [x29, #-48]
   378dc:	eor	w24, w19, w23
   378e0:	mov	x0, x20
   378e4:	cmp	x8, #0xc0
   378e8:	b.le	37908 <__gmpn_toom44_mul@@Base+0x450>
   378ec:	ldur	x1, [x29, #-16]
   378f0:	mov	x2, x21
   378f4:	mov	x3, x26
   378f8:	mov	x4, x21
   378fc:	mov	x5, x22
   37900:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37904:	b	37920 <__gmpn_toom44_mul@@Base+0x468>
   37908:	ldur	x1, [x29, #-16]
   3790c:	mov	x2, x21
   37910:	mov	x3, x26
   37914:	mov	x4, x21
   37918:	mov	x5, x22
   3791c:	bl	d630 <__gmpn_toom22_mul@plt>
   37920:	ldr	x8, [sp, #72]
   37924:	ldur	x9, [x29, #-8]
   37928:	cmp	x9, x8
   3792c:	lsl	x8, x25, #1
   37930:	add	x0, x20, x8, lsl #3
   37934:	b.le	37960 <__gmpn_toom44_mul@@Base+0x4a8>
   37938:	ldp	x8, x9, [x29, #-24]
   3793c:	ldur	x19, [x29, #-56]
   37940:	ldur	x23, [x29, #-40]
   37944:	lsl	x8, x8, #3
   37948:	add	x1, x9, x8
   3794c:	add	x3, x26, x8
   37950:	mov	x2, x19
   37954:	mov	x4, x23
   37958:	bl	cea0 <__gmpn_mul@plt>
   3795c:	b	37998 <__gmpn_toom44_mul@@Base+0x4e0>
   37960:	ldp	x8, x9, [x29, #-24]
   37964:	ldur	x19, [x29, #-56]
   37968:	ldur	x23, [x29, #-40]
   3796c:	mov	x5, x22
   37970:	lsl	x8, x8, #3
   37974:	cmp	x19, #0x30
   37978:	add	x1, x9, x8
   3797c:	add	x3, x26, x8
   37980:	mov	x2, x19
   37984:	mov	x4, x19
   37988:	b.le	37994 <__gmpn_toom44_mul@@Base+0x4dc>
   3798c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37990:	b	37998 <__gmpn_toom44_mul@@Base+0x4e0>
   37994:	bl	d630 <__gmpn_toom22_mul@plt>
   37998:	ldr	x3, [sp, #64]
   3799c:	ldp	x4, x6, [sp, #32]
   379a0:	ldur	x5, [x29, #-32]
   379a4:	add	x7, x19, x23
   379a8:	mov	x0, x20
   379ac:	mov	x1, x21
   379b0:	mov	w2, w24
   379b4:	str	x22, [sp]
   379b8:	bl	c9c0 <__gmpn_toom_interpolate_7pts@plt>
   379bc:	ldp	x20, x19, [sp, #224]
   379c0:	ldp	x22, x21, [sp, #208]
   379c4:	ldp	x24, x23, [sp, #192]
   379c8:	ldp	x26, x25, [sp, #176]
   379cc:	ldp	x28, x27, [sp, #160]
   379d0:	ldp	x29, x30, [sp, #144]
   379d4:	add	sp, sp, #0xf0
   379d8:	ret

00000000000379dc <__gmpn_toom6h_mul@@Base>:
   379dc:	sub	sp, sp, #0xd0
   379e0:	add	x8, x2, x2, lsl #4
   379e4:	add	x9, x4, x4, lsl #3
   379e8:	stp	x29, x30, [sp, #112]
   379ec:	stp	x20, x19, [sp, #192]
   379f0:	add	x29, sp, #0x70
   379f4:	mov	x19, x5
   379f8:	cmp	x8, x9, lsl #1
   379fc:	mov	x20, x0
   37a00:	stp	x28, x27, [sp, #128]
   37a04:	stp	x26, x25, [sp, #144]
   37a08:	stp	x24, x23, [sp, #160]
   37a0c:	stp	x22, x21, [sp, #176]
   37a10:	stp	x1, x3, [x29, #-16]
   37a14:	b.ge	38310 <__gmpn_toom6h_mul@@Base+0x934>  // b.tcont
   37a18:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   37a1c:	sub	x8, x2, #0x1
   37a20:	movk	x9, #0xaaab
   37a24:	umulh	x8, x8, x9
   37a28:	lsr	x8, x8, #2
   37a2c:	add	x24, x8, #0x1
   37a30:	add	x8, x24, x24, lsl #2
   37a34:	str	wzr, [sp, #52]
   37a38:	mov	w14, #0x5                   	// #5
   37a3c:	sub	x5, x2, x8
   37a40:	sub	x26, x4, x8
   37a44:	mov	w28, #0x5                   	// #5
   37a48:	add	x8, x24, x24, lsl #3
   37a4c:	lsl	x23, x8, #3
   37a50:	ldur	x3, [x29, #-16]
   37a54:	mov	w9, #0x38                  	// #56
   37a58:	add	x8, x20, x23
   37a5c:	add	x21, x8, #0x10
   37a60:	madd	x27, x24, x9, x20
   37a64:	mov	w6, #0x1                   	// #1
   37a68:	mov	x0, x21
   37a6c:	mov	x1, x27
   37a70:	mov	w2, w14
   37a74:	mov	x4, x24
   37a78:	mov	x7, x20
   37a7c:	stur	x5, [x29, #-24]
   37a80:	stur	x14, [x29, #-48]
   37a84:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   37a88:	ldur	x3, [x29, #-8]
   37a8c:	add	x8, x19, x23
   37a90:	add	x9, x20, x24, lsl #6
   37a94:	add	x23, x8, #0x18
   37a98:	add	x25, x9, #0x8
   37a9c:	mov	w22, w0
   37aa0:	mov	w6, #0x1                   	// #1
   37aa4:	mov	x0, x23
   37aa8:	mov	x1, x25
   37aac:	mov	w2, w28
   37ab0:	mov	x4, x24
   37ab4:	mov	x5, x26
   37ab8:	mov	x7, x20
   37abc:	stur	x28, [x29, #-40]
   37ac0:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   37ac4:	mov	w8, #0x50                  	// #80
   37ac8:	eor	w28, w0, w22
   37acc:	cmp	x24, #0x2f
   37ad0:	add	x22, x24, #0x1
   37ad4:	madd	x8, x24, x8, x19
   37ad8:	stur	x26, [x29, #-32]
   37adc:	b.le	37b30 <__gmpn_toom6h_mul@@Base+0x154>
   37ae0:	cmp	x24, #0x50
   37ae4:	b.le	37b70 <__gmpn_toom6h_mul@@Base+0x194>
   37ae8:	add	x26, x8, #0x20
   37aec:	cmp	x24, #0xab
   37af0:	mov	x0, x20
   37af4:	mov	x1, x27
   37af8:	mov	x2, x22
   37afc:	mov	x3, x25
   37b00:	mov	x4, x22
   37b04:	mov	x5, x26
   37b08:	b.le	37bb0 <__gmpn_toom6h_mul@@Base+0x1d4>
   37b0c:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37b10:	mov	x0, x19
   37b14:	mov	x1, x21
   37b18:	mov	x2, x22
   37b1c:	mov	x3, x23
   37b20:	mov	x4, x22
   37b24:	mov	x5, x26
   37b28:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37b2c:	b	37bd0 <__gmpn_toom6h_mul@@Base+0x1f4>
   37b30:	add	x26, x8, #0x20
   37b34:	mov	x0, x20
   37b38:	mov	x1, x27
   37b3c:	mov	x2, x22
   37b40:	mov	x3, x25
   37b44:	mov	x4, x22
   37b48:	mov	x5, x26
   37b4c:	bl	d630 <__gmpn_toom22_mul@plt>
   37b50:	mov	x0, x19
   37b54:	mov	x1, x21
   37b58:	mov	x2, x22
   37b5c:	mov	x3, x23
   37b60:	mov	x4, x22
   37b64:	mov	x5, x26
   37b68:	bl	d630 <__gmpn_toom22_mul@plt>
   37b6c:	b	37bd0 <__gmpn_toom6h_mul@@Base+0x1f4>
   37b70:	add	x26, x8, #0x20
   37b74:	mov	x0, x20
   37b78:	mov	x1, x27
   37b7c:	mov	x2, x22
   37b80:	mov	x3, x25
   37b84:	mov	x4, x22
   37b88:	mov	x5, x26
   37b8c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37b90:	mov	x0, x19
   37b94:	mov	x1, x21
   37b98:	mov	x2, x22
   37b9c:	mov	x3, x23
   37ba0:	mov	x4, x22
   37ba4:	mov	x5, x26
   37ba8:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37bac:	b	37bd0 <__gmpn_toom6h_mul@@Base+0x1f4>
   37bb0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37bb4:	mov	x0, x19
   37bb8:	mov	x1, x21
   37bbc:	mov	x2, x22
   37bc0:	mov	x3, x23
   37bc4:	mov	x4, x22
   37bc8:	mov	x5, x26
   37bcc:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37bd0:	ldr	w6, [sp, #52]
   37bd4:	mov	w1, #0x1                   	// #1
   37bd8:	bfi	x1, x24, #1, #63
   37bdc:	mov	x0, x19
   37be0:	add	w5, w6, #0x1
   37be4:	mov	x2, x20
   37be8:	mov	w3, w28
   37bec:	mov	x4, x24
   37bf0:	str	x1, [sp, #56]
   37bf4:	str	w5, [sp, #36]
   37bf8:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37bfc:	ldp	x5, x3, [x29, #-24]
   37c00:	mov	x0, x21
   37c04:	mov	x1, x27
   37c08:	ldur	x2, [x29, #-48]
   37c0c:	mov	x4, x24
   37c10:	mov	x6, x20
   37c14:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   37c18:	ldur	x2, [x29, #-40]
   37c1c:	mov	w26, w0
   37c20:	mov	x0, x23
   37c24:	mov	x1, x25
   37c28:	cmp	w2, #0x3
   37c2c:	b.eq	38334 <__gmpn_toom6h_mul@@Base+0x958>  // b.none
   37c30:	ldur	x3, [x29, #-8]
   37c34:	ldur	x5, [x29, #-32]
   37c38:	mov	x4, x24
   37c3c:	mov	x6, x20
   37c40:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   37c44:	mov	w8, #0x50                  	// #80
   37c48:	cmp	x24, #0x2f
   37c4c:	eor	w28, w0, w26
   37c50:	madd	x8, x24, x8, x19
   37c54:	b.le	37cb0 <__gmpn_toom6h_mul@@Base+0x2d4>
   37c58:	cmp	x24, #0x50
   37c5c:	b.le	37cf8 <__gmpn_toom6h_mul@@Base+0x31c>
   37c60:	add	x26, x8, #0x20
   37c64:	cmp	x24, #0xab
   37c68:	mov	x0, x20
   37c6c:	mov	x1, x27
   37c70:	mov	x2, x22
   37c74:	mov	x3, x25
   37c78:	mov	x4, x22
   37c7c:	mov	x5, x26
   37c80:	b.le	37d40 <__gmpn_toom6h_mul@@Base+0x364>
   37c84:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37c88:	mov	w8, #0x18                  	// #24
   37c8c:	madd	x8, x24, x8, x19
   37c90:	add	x0, x8, #0x8
   37c94:	mov	x1, x21
   37c98:	mov	x2, x22
   37c9c:	mov	x3, x23
   37ca0:	mov	x4, x22
   37ca4:	mov	x5, x26
   37ca8:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37cac:	b	37d68 <__gmpn_toom6h_mul@@Base+0x38c>
   37cb0:	add	x26, x8, #0x20
   37cb4:	mov	x0, x20
   37cb8:	mov	x1, x27
   37cbc:	mov	x2, x22
   37cc0:	mov	x3, x25
   37cc4:	mov	x4, x22
   37cc8:	mov	x5, x26
   37ccc:	bl	d630 <__gmpn_toom22_mul@plt>
   37cd0:	mov	w8, #0x18                  	// #24
   37cd4:	madd	x8, x24, x8, x19
   37cd8:	add	x0, x8, #0x8
   37cdc:	mov	x1, x21
   37ce0:	mov	x2, x22
   37ce4:	mov	x3, x23
   37ce8:	mov	x4, x22
   37cec:	mov	x5, x26
   37cf0:	bl	d630 <__gmpn_toom22_mul@plt>
   37cf4:	b	37d68 <__gmpn_toom6h_mul@@Base+0x38c>
   37cf8:	add	x26, x8, #0x20
   37cfc:	mov	x0, x20
   37d00:	mov	x1, x27
   37d04:	mov	x2, x22
   37d08:	mov	x3, x25
   37d0c:	mov	x4, x22
   37d10:	mov	x5, x26
   37d14:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37d18:	mov	w8, #0x18                  	// #24
   37d1c:	madd	x8, x24, x8, x19
   37d20:	add	x0, x8, #0x8
   37d24:	mov	x1, x21
   37d28:	mov	x2, x22
   37d2c:	mov	x3, x23
   37d30:	mov	x4, x22
   37d34:	mov	x5, x26
   37d38:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37d3c:	b	37d68 <__gmpn_toom6h_mul@@Base+0x38c>
   37d40:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37d44:	mov	w8, #0x18                  	// #24
   37d48:	madd	x8, x24, x8, x19
   37d4c:	add	x0, x8, #0x8
   37d50:	mov	x1, x21
   37d54:	mov	x2, x22
   37d58:	mov	x3, x23
   37d5c:	mov	x4, x22
   37d60:	mov	x5, x26
   37d64:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37d68:	ldr	x1, [sp, #56]
   37d6c:	ldur	x26, [x29, #-24]
   37d70:	add	x8, x24, x24, lsl #1
   37d74:	str	x8, [sp, #16]
   37d78:	add	x8, x19, x8, lsl #3
   37d7c:	add	x0, x8, #0x8
   37d80:	mov	x2, x20
   37d84:	mov	w3, w28
   37d88:	mov	x4, x24
   37d8c:	mov	w5, wzr
   37d90:	mov	w6, wzr
   37d94:	str	x0, [sp, #40]
   37d98:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37d9c:	ldur	x3, [x29, #-16]
   37da0:	mov	w6, #0x2                   	// #2
   37da4:	mov	x0, x21
   37da8:	mov	x1, x27
   37dac:	ldur	x2, [x29, #-48]
   37db0:	mov	x4, x24
   37db4:	mov	x5, x26
   37db8:	mov	x7, x20
   37dbc:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   37dc0:	ldp	x2, x5, [x29, #-40]
   37dc4:	ldur	x3, [x29, #-8]
   37dc8:	mov	w26, w0
   37dcc:	mov	w6, #0x2                   	// #2
   37dd0:	mov	x0, x23
   37dd4:	mov	x1, x25
   37dd8:	mov	x4, x24
   37ddc:	mov	x7, x20
   37de0:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   37de4:	mov	w8, #0x50                  	// #80
   37de8:	cmp	x24, #0x2f
   37dec:	eor	w28, w0, w26
   37df0:	madd	x8, x24, x8, x19
   37df4:	b.le	37e50 <__gmpn_toom6h_mul@@Base+0x474>
   37df8:	cmp	x24, #0x50
   37dfc:	b.le	37e98 <__gmpn_toom6h_mul@@Base+0x4bc>
   37e00:	add	x26, x8, #0x20
   37e04:	cmp	x24, #0xab
   37e08:	mov	x0, x20
   37e0c:	mov	x1, x27
   37e10:	mov	x2, x22
   37e14:	mov	x3, x25
   37e18:	mov	x4, x22
   37e1c:	mov	x5, x26
   37e20:	b.le	37ee0 <__gmpn_toom6h_mul@@Base+0x504>
   37e24:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37e28:	mov	w8, #0x30                  	// #48
   37e2c:	madd	x8, x24, x8, x19
   37e30:	add	x0, x8, #0x10
   37e34:	mov	x1, x21
   37e38:	mov	x2, x22
   37e3c:	mov	x3, x23
   37e40:	mov	x4, x22
   37e44:	mov	x5, x26
   37e48:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37e4c:	b	37f08 <__gmpn_toom6h_mul@@Base+0x52c>
   37e50:	add	x26, x8, #0x20
   37e54:	mov	x0, x20
   37e58:	mov	x1, x27
   37e5c:	mov	x2, x22
   37e60:	mov	x3, x25
   37e64:	mov	x4, x22
   37e68:	mov	x5, x26
   37e6c:	bl	d630 <__gmpn_toom22_mul@plt>
   37e70:	mov	w8, #0x30                  	// #48
   37e74:	madd	x8, x24, x8, x19
   37e78:	add	x0, x8, #0x10
   37e7c:	mov	x1, x21
   37e80:	mov	x2, x22
   37e84:	mov	x3, x23
   37e88:	mov	x4, x22
   37e8c:	mov	x5, x26
   37e90:	bl	d630 <__gmpn_toom22_mul@plt>
   37e94:	b	37f08 <__gmpn_toom6h_mul@@Base+0x52c>
   37e98:	add	x26, x8, #0x20
   37e9c:	mov	x0, x20
   37ea0:	mov	x1, x27
   37ea4:	mov	x2, x22
   37ea8:	mov	x3, x25
   37eac:	mov	x4, x22
   37eb0:	mov	x5, x26
   37eb4:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37eb8:	mov	w8, #0x30                  	// #48
   37ebc:	madd	x8, x24, x8, x19
   37ec0:	add	x0, x8, #0x10
   37ec4:	mov	x1, x21
   37ec8:	mov	x2, x22
   37ecc:	mov	x3, x23
   37ed0:	mov	x4, x22
   37ed4:	mov	x5, x26
   37ed8:	bl	c1e0 <__gmpn_toom33_mul@plt>
   37edc:	b	37f08 <__gmpn_toom6h_mul@@Base+0x52c>
   37ee0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37ee4:	mov	w8, #0x30                  	// #48
   37ee8:	madd	x8, x24, x8, x19
   37eec:	add	x0, x8, #0x10
   37ef0:	mov	x1, x21
   37ef4:	mov	x2, x22
   37ef8:	mov	x3, x23
   37efc:	mov	x4, x22
   37f00:	mov	x5, x26
   37f04:	bl	c8b0 <__gmpn_toom44_mul@plt>
   37f08:	ldr	x1, [sp, #56]
   37f0c:	ldur	x26, [x29, #-24]
   37f10:	mov	w8, #0x30                  	// #48
   37f14:	madd	x8, x24, x8, x19
   37f18:	add	x0, x8, #0x10
   37f1c:	mov	w5, #0x2                   	// #2
   37f20:	mov	w6, #0x4                   	// #4
   37f24:	mov	x2, x20
   37f28:	mov	w3, w28
   37f2c:	mov	x4, x24
   37f30:	str	x0, [sp, #24]
   37f34:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   37f38:	ldur	x3, [x29, #-16]
   37f3c:	mov	w6, #0x2                   	// #2
   37f40:	mov	x0, x21
   37f44:	mov	x1, x27
   37f48:	ldur	x2, [x29, #-48]
   37f4c:	mov	x4, x24
   37f50:	mov	x5, x26
   37f54:	mov	x7, x20
   37f58:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   37f5c:	ldp	x2, x5, [x29, #-40]
   37f60:	ldur	x3, [x29, #-8]
   37f64:	mov	w26, w0
   37f68:	mov	w6, #0x2                   	// #2
   37f6c:	mov	x0, x23
   37f70:	mov	x1, x25
   37f74:	mov	x4, x24
   37f78:	mov	x7, x20
   37f7c:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   37f80:	mov	w8, #0x50                  	// #80
   37f84:	cmp	x24, #0x2f
   37f88:	eor	w28, w0, w26
   37f8c:	madd	x8, x24, x8, x19
   37f90:	str	x19, [sp, #8]
   37f94:	b.le	37fec <__gmpn_toom6h_mul@@Base+0x610>
   37f98:	cmp	x24, #0x50
   37f9c:	b.le	38030 <__gmpn_toom6h_mul@@Base+0x654>
   37fa0:	add	x26, x8, #0x20
   37fa4:	cmp	x24, #0xab
   37fa8:	mov	x0, x20
   37fac:	mov	x1, x27
   37fb0:	mov	x2, x22
   37fb4:	mov	x3, x25
   37fb8:	mov	x4, x22
   37fbc:	mov	x5, x26
   37fc0:	b.le	38074 <__gmpn_toom6h_mul@@Base+0x698>
   37fc4:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37fc8:	ldr	x19, [sp, #16]
   37fcc:	mov	x1, x21
   37fd0:	mov	x2, x22
   37fd4:	mov	x3, x23
   37fd8:	add	x0, x20, x19, lsl #3
   37fdc:	mov	x4, x22
   37fe0:	mov	x5, x26
   37fe4:	bl	cde0 <__gmpn_toom6h_mul@plt>
   37fe8:	b	38098 <__gmpn_toom6h_mul@@Base+0x6bc>
   37fec:	add	x26, x8, #0x20
   37ff0:	mov	x0, x20
   37ff4:	mov	x1, x27
   37ff8:	mov	x2, x22
   37ffc:	mov	x3, x25
   38000:	mov	x4, x22
   38004:	mov	x5, x26
   38008:	bl	d630 <__gmpn_toom22_mul@plt>
   3800c:	ldr	x19, [sp, #16]
   38010:	mov	x1, x21
   38014:	mov	x2, x22
   38018:	mov	x3, x23
   3801c:	add	x0, x20, x19, lsl #3
   38020:	mov	x4, x22
   38024:	mov	x5, x26
   38028:	bl	d630 <__gmpn_toom22_mul@plt>
   3802c:	b	38098 <__gmpn_toom6h_mul@@Base+0x6bc>
   38030:	add	x26, x8, #0x20
   38034:	mov	x0, x20
   38038:	mov	x1, x27
   3803c:	mov	x2, x22
   38040:	mov	x3, x25
   38044:	mov	x4, x22
   38048:	mov	x5, x26
   3804c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38050:	ldr	x19, [sp, #16]
   38054:	mov	x1, x21
   38058:	mov	x2, x22
   3805c:	mov	x3, x23
   38060:	add	x0, x20, x19, lsl #3
   38064:	mov	x4, x22
   38068:	mov	x5, x26
   3806c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38070:	b	38098 <__gmpn_toom6h_mul@@Base+0x6bc>
   38074:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38078:	ldr	x19, [sp, #16]
   3807c:	mov	x1, x21
   38080:	mov	x2, x22
   38084:	mov	x3, x23
   38088:	add	x0, x20, x19, lsl #3
   3808c:	mov	x4, x22
   38090:	mov	x5, x26
   38094:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38098:	ldr	w8, [sp, #36]
   3809c:	ldr	x1, [sp, #56]
   380a0:	add	x0, x20, x19, lsl #3
   380a4:	mov	x2, x20
   380a8:	lsl	w5, w8, #1
   380ac:	ldr	w8, [sp, #52]
   380b0:	mov	w3, w28
   380b4:	mov	x4, x24
   380b8:	lsl	w6, w8, #1
   380bc:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   380c0:	ldp	x5, x3, [x29, #-24]
   380c4:	mov	x0, x21
   380c8:	mov	x1, x27
   380cc:	ldur	x2, [x29, #-48]
   380d0:	mov	x4, x24
   380d4:	mov	x6, x20
   380d8:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   380dc:	ldp	x2, x5, [x29, #-40]
   380e0:	ldur	x3, [x29, #-8]
   380e4:	mov	w26, w0
   380e8:	mov	x0, x23
   380ec:	mov	x1, x25
   380f0:	mov	x4, x24
   380f4:	mov	x6, x20
   380f8:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   380fc:	cmp	x24, #0x2f
   38100:	eor	w28, w0, w26
   38104:	b.le	38164 <__gmpn_toom6h_mul@@Base+0x788>
   38108:	ldr	x19, [sp, #8]
   3810c:	mov	w8, #0x50                  	// #80
   38110:	cmp	x24, #0x50
   38114:	madd	x8, x24, x8, x19
   38118:	b.le	381b0 <__gmpn_toom6h_mul@@Base+0x7d4>
   3811c:	add	x26, x8, #0x20
   38120:	cmp	x24, #0xab
   38124:	mov	x0, x20
   38128:	mov	x1, x27
   3812c:	mov	x2, x22
   38130:	mov	x3, x25
   38134:	mov	x4, x22
   38138:	mov	x5, x26
   3813c:	b.le	381f0 <__gmpn_toom6h_mul@@Base+0x814>
   38140:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38144:	mov	x0, x27
   38148:	mov	x1, x21
   3814c:	mov	x2, x22
   38150:	mov	x3, x23
   38154:	mov	x4, x22
   38158:	mov	x5, x26
   3815c:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38160:	b	38210 <__gmpn_toom6h_mul@@Base+0x834>
   38164:	ldr	x19, [sp, #8]
   38168:	mov	w8, #0x50                  	// #80
   3816c:	mov	x0, x20
   38170:	mov	x1, x27
   38174:	madd	x8, x24, x8, x19
   38178:	add	x26, x8, #0x20
   3817c:	mov	x2, x22
   38180:	mov	x3, x25
   38184:	mov	x4, x22
   38188:	mov	x5, x26
   3818c:	bl	d630 <__gmpn_toom22_mul@plt>
   38190:	mov	x0, x27
   38194:	mov	x1, x21
   38198:	mov	x2, x22
   3819c:	mov	x3, x23
   381a0:	mov	x4, x22
   381a4:	mov	x5, x26
   381a8:	bl	d630 <__gmpn_toom22_mul@plt>
   381ac:	b	38210 <__gmpn_toom6h_mul@@Base+0x834>
   381b0:	add	x26, x8, #0x20
   381b4:	mov	x0, x20
   381b8:	mov	x1, x27
   381bc:	mov	x2, x22
   381c0:	mov	x3, x25
   381c4:	mov	x4, x22
   381c8:	mov	x5, x26
   381cc:	bl	c1e0 <__gmpn_toom33_mul@plt>
   381d0:	mov	x0, x27
   381d4:	mov	x1, x21
   381d8:	mov	x2, x22
   381dc:	mov	x3, x23
   381e0:	mov	x4, x22
   381e4:	mov	x5, x26
   381e8:	bl	c1e0 <__gmpn_toom33_mul@plt>
   381ec:	b	38210 <__gmpn_toom6h_mul@@Base+0x834>
   381f0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   381f4:	mov	x0, x27
   381f8:	mov	x1, x21
   381fc:	mov	x2, x22
   38200:	mov	x3, x23
   38204:	mov	x4, x22
   38208:	mov	x5, x26
   3820c:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38210:	ldr	x1, [sp, #56]
   38214:	mov	w5, #0x1                   	// #1
   38218:	mov	w6, #0x2                   	// #2
   3821c:	mov	x0, x27
   38220:	mov	x2, x20
   38224:	mov	w3, w28
   38228:	mov	x4, x24
   3822c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38230:	ldr	w21, [sp, #52]
   38234:	ldp	x25, x22, [x29, #-32]
   38238:	cmp	x24, #0x30
   3823c:	b.le	38274 <__gmpn_toom6h_mul@@Base+0x898>
   38240:	ldur	x26, [x29, #-40]
   38244:	cmp	x24, #0x51
   38248:	mov	x0, x20
   3824c:	b.le	38298 <__gmpn_toom6h_mul@@Base+0x8bc>
   38250:	cmp	x24, #0xac
   38254:	b.le	382b4 <__gmpn_toom6h_mul@@Base+0x8d8>
   38258:	ldp	x1, x3, [x29, #-16]
   3825c:	mov	x2, x24
   38260:	mov	x4, x24
   38264:	mov	x5, x23
   38268:	bl	cde0 <__gmpn_toom6h_mul@plt>
   3826c:	cbz	w21, 382cc <__gmpn_toom6h_mul@@Base+0x8f0>
   38270:	b	3834c <__gmpn_toom6h_mul@@Base+0x970>
   38274:	ldp	x1, x3, [x29, #-16]
   38278:	mov	x0, x20
   3827c:	mov	x2, x24
   38280:	mov	x4, x24
   38284:	mov	x5, x23
   38288:	bl	d630 <__gmpn_toom22_mul@plt>
   3828c:	ldur	x26, [x29, #-40]
   38290:	cbz	w21, 382cc <__gmpn_toom6h_mul@@Base+0x8f0>
   38294:	b	3834c <__gmpn_toom6h_mul@@Base+0x970>
   38298:	ldp	x1, x3, [x29, #-16]
   3829c:	mov	x2, x24
   382a0:	mov	x4, x24
   382a4:	mov	x5, x23
   382a8:	bl	c1e0 <__gmpn_toom33_mul@plt>
   382ac:	cbz	w21, 382cc <__gmpn_toom6h_mul@@Base+0x8f0>
   382b0:	b	3834c <__gmpn_toom6h_mul@@Base+0x970>
   382b4:	ldp	x1, x3, [x29, #-16]
   382b8:	mov	x2, x24
   382bc:	mov	x4, x24
   382c0:	mov	x5, x23
   382c4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   382c8:	cbnz	w21, 3834c <__gmpn_toom6h_mul@@Base+0x970>
   382cc:	ldr	x1, [sp, #24]
   382d0:	ldr	x2, [sp, #40]
   382d4:	add	x5, x25, x22
   382d8:	mov	x0, x20
   382dc:	mov	x3, x19
   382e0:	mov	x4, x24
   382e4:	mov	w6, w21
   382e8:	mov	x7, x23
   382ec:	bl	c0e0 <__gmpn_toom_interpolate_12pts@plt>
   382f0:	ldp	x20, x19, [sp, #192]
   382f4:	ldp	x22, x21, [sp, #176]
   382f8:	ldp	x24, x23, [sp, #160]
   382fc:	ldp	x26, x25, [sp, #144]
   38300:	ldp	x28, x27, [sp, #128]
   38304:	ldp	x29, x30, [sp, #112]
   38308:	add	sp, sp, #0xd0
   3830c:	ret
   38310:	mov	w9, #0x5a                  	// #90
   38314:	mov	w10, #0x77                  	// #119
   38318:	mul	x9, x2, x9
   3831c:	mul	x10, x4, x10
   38320:	cmp	x9, x10
   38324:	b.ge	3838c <__gmpn_toom6h_mul@@Base+0x9b0>  // b.tcont
   38328:	mov	w8, #0x6                   	// #6
   3832c:	mov	w9, #0x7                   	// #7
   38330:	b	38414 <__gmpn_toom6h_mul@@Base+0xa38>
   38334:	ldur	x2, [x29, #-8]
   38338:	ldur	x4, [x29, #-32]
   3833c:	mov	x3, x24
   38340:	mov	x5, x20
   38344:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   38348:	b	37c44 <__gmpn_toom6h_mul@@Base+0x268>
   3834c:	mov	w8, #0x58                  	// #88
   38350:	cmp	x22, x25
   38354:	madd	x0, x24, x8, x20
   38358:	b.le	383b0 <__gmpn_toom6h_mul@@Base+0x9d4>
   3835c:	ldur	x8, [x29, #-48]
   38360:	ldur	x10, [x29, #-16]
   38364:	sxtw	x9, w26
   38368:	mul	x9, x24, x9
   3836c:	sxtw	x8, w8
   38370:	mul	x8, x24, x8
   38374:	add	x1, x10, x8, lsl #3
   38378:	ldur	x8, [x29, #-8]
   3837c:	mov	x2, x22
   38380:	mov	x4, x25
   38384:	add	x3, x8, x9, lsl #3
   38388:	b	383dc <__gmpn_toom6h_mul@@Base+0xa00>
   3838c:	mov	w9, #0x55                  	// #85
   38390:	mov	w10, #0x7e                  	// #126
   38394:	mul	x9, x2, x9
   38398:	mul	x10, x4, x10
   3839c:	cmp	x9, x10
   383a0:	b.ge	383e4 <__gmpn_toom6h_mul@@Base+0xa08>  // b.tcont
   383a4:	mov	w8, #0x5                   	// #5
   383a8:	mov	w9, #0x7                   	// #7
   383ac:	b	38414 <__gmpn_toom6h_mul@@Base+0xa38>
   383b0:	ldur	x10, [x29, #-8]
   383b4:	sxtw	x8, w26
   383b8:	ldur	x9, [x29, #-48]
   383bc:	mul	x8, x24, x8
   383c0:	add	x1, x10, x8, lsl #3
   383c4:	ldur	x8, [x29, #-16]
   383c8:	sxtw	x9, w9
   383cc:	mul	x9, x24, x9
   383d0:	mov	x2, x25
   383d4:	add	x3, x8, x9, lsl #3
   383d8:	mov	x4, x22
   383dc:	bl	cea0 <__gmpn_mul@plt>
   383e0:	b	382cc <__gmpn_toom6h_mul@@Base+0x8f0>
   383e4:	add	x9, x2, x2, lsl #3
   383e8:	lsl	x9, x9, #1
   383ec:	add	x10, x4, x4, lsl #4
   383f0:	cmp	x9, x10, lsl #1
   383f4:	mov	w9, #0x8                   	// #8
   383f8:	b.ge	38404 <__gmpn_toom6h_mul@@Base+0xa28>  // b.tcont
   383fc:	mov	w8, #0x5                   	// #5
   38400:	b	38414 <__gmpn_toom6h_mul@@Base+0xa38>
   38404:	add	x10, x4, x4, lsl #3
   38408:	cmp	x8, x10, lsl #2
   3840c:	cinc	w9, w9, ge  // ge = tcont
   38410:	mov	w8, #0x4                   	// #4
   38414:	mov	w12, w9
   38418:	mul	x11, x8, x2
   3841c:	mul	x13, x12, x4
   38420:	cmp	x11, x13
   38424:	csel	x11, x4, x2, lt  // lt = tstop
   38428:	csel	x12, x8, x12, lt  // lt = tstop
   3842c:	sub	x11, x11, #0x1
   38430:	sub	w14, w9, #0x1
   38434:	udiv	x11, x11, x12
   38438:	eor	w10, w8, w9
   3843c:	sub	w28, w8, #0x1
   38440:	sxtw	x12, w14
   38444:	add	x24, x11, #0x1
   38448:	and	w13, w10, #0x1
   3844c:	msub	x5, x24, x12, x2
   38450:	msub	x26, x24, x28, x4
   38454:	str	w13, [sp, #52]
   38458:	tbz	w10, #0, 37a48 <__gmpn_toom6h_mul@@Base+0x6c>
   3845c:	cmp	x5, #0x0
   38460:	b.le	3847c <__gmpn_toom6h_mul@@Base+0xaa0>
   38464:	cmp	x26, #0x0
   38468:	b.gt	37a48 <__gmpn_toom6h_mul@@Base+0x6c>
   3846c:	sub	w28, w8, #0x2
   38470:	add	x26, x26, x24
   38474:	str	wzr, [sp, #52]
   38478:	b	37a48 <__gmpn_toom6h_mul@@Base+0x6c>
   3847c:	str	wzr, [sp, #52]
   38480:	sub	w14, w9, #0x2
   38484:	add	x5, x5, x24
   38488:	b	37a48 <__gmpn_toom6h_mul@@Base+0x6c>

000000000003848c <__gmpn_toom6_sqr@@Base>:
   3848c:	sub	sp, sp, #0x90
   38490:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   38494:	sub	x8, x2, #0x1
   38498:	movk	x9, #0xaaab
   3849c:	umulh	x8, x8, x9
   384a0:	stp	x20, x19, [sp, #128]
   384a4:	lsr	x19, x8, #2
   384a8:	stp	x22, x21, [sp, #112]
   384ac:	add	x22, x19, #0x1
   384b0:	add	x9, x22, x22, lsl #3
   384b4:	add	x8, x22, x22, lsl #2
   384b8:	lsl	x21, x9, #3
   384bc:	mov	w10, #0x38                  	// #56
   384c0:	sub	x5, x2, x8
   384c4:	add	x8, x0, x21
   384c8:	stp	x28, x27, [sp, #64]
   384cc:	stp	x26, x25, [sp, #80]
   384d0:	mov	x20, x0
   384d4:	add	x27, x8, #0x10
   384d8:	madd	x25, x22, x10, x0
   384dc:	stp	x24, x23, [sp, #96]
   384e0:	mov	x24, x3
   384e4:	mov	x3, x1
   384e8:	mov	w2, #0x5                   	// #5
   384ec:	mov	w6, #0x1                   	// #1
   384f0:	mov	x0, x27
   384f4:	mov	x1, x25
   384f8:	mov	x4, x22
   384fc:	mov	x7, x20
   38500:	stp	x29, x30, [sp, #48]
   38504:	add	x29, sp, #0x30
   38508:	mov	x23, x3
   3850c:	mov	x26, x5
   38510:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   38514:	add	x8, x24, x21
   38518:	add	x28, x19, #0x2
   3851c:	mov	x19, x24
   38520:	add	x24, x8, #0x18
   38524:	mov	x0, x20
   38528:	mov	x1, x25
   3852c:	mov	x2, x28
   38530:	mov	x3, x24
   38534:	bl	c190 <__gmpn_toom2_sqr@plt>
   38538:	mov	x0, x19
   3853c:	mov	x1, x27
   38540:	mov	x2, x28
   38544:	mov	x3, x24
   38548:	bl	c190 <__gmpn_toom2_sqr@plt>
   3854c:	mov	w1, #0x1                   	// #1
   38550:	bfi	x1, x22, #1, #63
   38554:	mov	w5, #0x1                   	// #1
   38558:	mov	x0, x19
   3855c:	mov	x2, x20
   38560:	mov	w3, wzr
   38564:	mov	x4, x22
   38568:	mov	w6, wzr
   3856c:	str	x1, [sp, #8]
   38570:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38574:	mov	w2, #0x5                   	// #5
   38578:	mov	x0, x27
   3857c:	mov	x1, x25
   38580:	mov	x3, x23
   38584:	mov	x4, x22
   38588:	mov	x5, x26
   3858c:	mov	x6, x20
   38590:	mov	x21, x26
   38594:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   38598:	mov	x0, x20
   3859c:	mov	x1, x25
   385a0:	mov	x2, x28
   385a4:	mov	x3, x24
   385a8:	bl	c190 <__gmpn_toom2_sqr@plt>
   385ac:	add	x8, x22, x22, lsl #1
   385b0:	lsl	x8, x8, #3
   385b4:	str	x8, [sp, #24]
   385b8:	add	x8, x19, x8
   385bc:	add	x26, x8, #0x8
   385c0:	mov	x0, x26
   385c4:	mov	x1, x27
   385c8:	mov	x2, x28
   385cc:	mov	x3, x24
   385d0:	stp	x19, x26, [x29, #-16]
   385d4:	bl	c190 <__gmpn_toom2_sqr@plt>
   385d8:	mov	x0, x26
   385dc:	ldr	x26, [sp, #8]
   385e0:	mov	x2, x20
   385e4:	mov	w3, wzr
   385e8:	mov	x4, x22
   385ec:	mov	x1, x26
   385f0:	mov	w5, wzr
   385f4:	mov	w6, wzr
   385f8:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   385fc:	mov	w2, #0x5                   	// #5
   38600:	mov	w6, #0x2                   	// #2
   38604:	mov	x0, x27
   38608:	mov	x1, x25
   3860c:	mov	x3, x23
   38610:	mov	x4, x22
   38614:	mov	x5, x21
   38618:	mov	x7, x20
   3861c:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   38620:	mov	x0, x20
   38624:	mov	x1, x25
   38628:	mov	x2, x28
   3862c:	mov	x3, x24
   38630:	bl	c190 <__gmpn_toom2_sqr@plt>
   38634:	mov	w8, #0x30                  	// #48
   38638:	madd	x8, x22, x8, x19
   3863c:	add	x19, x8, #0x10
   38640:	mov	x0, x19
   38644:	mov	x1, x27
   38648:	mov	x2, x28
   3864c:	mov	x3, x24
   38650:	str	x19, [sp, #16]
   38654:	bl	c190 <__gmpn_toom2_sqr@plt>
   38658:	mov	w5, #0x2                   	// #2
   3865c:	mov	w6, #0x4                   	// #4
   38660:	mov	x0, x19
   38664:	mov	x1, x26
   38668:	mov	x2, x20
   3866c:	mov	w3, wzr
   38670:	mov	x4, x22
   38674:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38678:	mov	w2, #0x5                   	// #5
   3867c:	mov	w6, #0x2                   	// #2
   38680:	mov	x0, x27
   38684:	mov	x1, x25
   38688:	mov	x3, x23
   3868c:	mov	x4, x22
   38690:	mov	x5, x21
   38694:	mov	x7, x20
   38698:	mov	x19, x23
   3869c:	mov	x23, x21
   386a0:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   386a4:	mov	x0, x20
   386a8:	mov	x1, x25
   386ac:	mov	x2, x28
   386b0:	mov	x3, x24
   386b4:	bl	c190 <__gmpn_toom2_sqr@plt>
   386b8:	ldr	x8, [sp, #24]
   386bc:	mov	x1, x27
   386c0:	mov	x2, x28
   386c4:	mov	x3, x24
   386c8:	add	x21, x20, x8
   386cc:	mov	x0, x21
   386d0:	bl	c190 <__gmpn_toom2_sqr@plt>
   386d4:	mov	w5, #0x2                   	// #2
   386d8:	mov	x0, x21
   386dc:	mov	x1, x26
   386e0:	mov	x2, x20
   386e4:	mov	w3, wzr
   386e8:	mov	x4, x22
   386ec:	mov	w6, wzr
   386f0:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   386f4:	mov	w2, #0x5                   	// #5
   386f8:	mov	x0, x27
   386fc:	mov	x1, x25
   38700:	mov	x3, x19
   38704:	mov	x4, x22
   38708:	mov	x5, x23
   3870c:	mov	x6, x20
   38710:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   38714:	mov	x0, x20
   38718:	mov	x1, x25
   3871c:	mov	x2, x28
   38720:	mov	x3, x24
   38724:	bl	c190 <__gmpn_toom2_sqr@plt>
   38728:	mov	x0, x25
   3872c:	mov	x1, x27
   38730:	mov	x2, x28
   38734:	mov	x3, x24
   38738:	bl	c190 <__gmpn_toom2_sqr@plt>
   3873c:	mov	w5, #0x1                   	// #1
   38740:	mov	w6, #0x2                   	// #2
   38744:	mov	x0, x25
   38748:	mov	x1, x26
   3874c:	mov	x2, x20
   38750:	mov	w3, wzr
   38754:	mov	x4, x22
   38758:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   3875c:	mov	x0, x20
   38760:	mov	x1, x19
   38764:	mov	x2, x22
   38768:	mov	x3, x24
   3876c:	bl	c190 <__gmpn_toom2_sqr@plt>
   38770:	ldr	x1, [sp, #16]
   38774:	ldp	x3, x2, [x29, #-16]
   38778:	lsl	x5, x23, #1
   3877c:	mov	x0, x20
   38780:	mov	x4, x22
   38784:	mov	w6, wzr
   38788:	mov	x7, x24
   3878c:	bl	c0e0 <__gmpn_toom_interpolate_12pts@plt>
   38790:	ldp	x20, x19, [sp, #128]
   38794:	ldp	x22, x21, [sp, #112]
   38798:	ldp	x24, x23, [sp, #96]
   3879c:	ldp	x26, x25, [sp, #80]
   387a0:	ldp	x28, x27, [sp, #64]
   387a4:	ldp	x29, x30, [sp, #48]
   387a8:	add	sp, sp, #0x90
   387ac:	ret

00000000000387b0 <__gmpn_toom8h_mul@@Base>:
   387b0:	sub	sp, sp, #0xe0
   387b4:	stp	x29, x30, [sp, #128]
   387b8:	stp	x20, x19, [sp, #208]
   387bc:	add	x29, sp, #0x80
   387c0:	mov	x19, x5
   387c4:	cmp	x2, x4
   387c8:	stp	x28, x27, [sp, #144]
   387cc:	stp	x26, x25, [sp, #160]
   387d0:	stp	x24, x23, [sp, #176]
   387d4:	stp	x22, x21, [sp, #192]
   387d8:	stp	x1, x3, [x29, #-16]
   387dc:	b.ne	396cc <__gmpn_toom8h_mul@@Base+0xf1c>  // b.any
   387e0:	sub	x8, x2, #0x1
   387e4:	asr	x8, x8, #3
   387e8:	add	x24, x8, #0x1
   387ec:	lsl	x8, x24, #3
   387f0:	sub	x8, x8, x24
   387f4:	sub	x5, x2, x8
   387f8:	sub	x26, x4, x8
   387fc:	mov	w8, #0x7                   	// #7
   38800:	mov	w13, wzr
   38804:	mov	w14, #0x7                   	// #7
   38808:	stur	x8, [x29, #-24]
   3880c:	mov	w8, #0xd                   	// #13
   38810:	mul	x21, x24, x8
   38814:	ldur	x3, [x29, #-16]
   38818:	mov	w9, #0x58                  	// #88
   3881c:	add	x8, x0, x21, lsl #3
   38820:	add	x27, x8, #0x10
   38824:	mov	x20, x0
   38828:	madd	x28, x24, x9, x0
   3882c:	mov	w6, #0x3                   	// #3
   38830:	mov	x0, x27
   38834:	mov	x1, x28
   38838:	mov	w2, w14
   3883c:	mov	x4, x24
   38840:	mov	x7, x20
   38844:	stp	x5, x26, [x29, #-40]
   38848:	str	w13, [sp, #60]
   3884c:	stur	x14, [x29, #-56]
   38850:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   38854:	add	x8, x24, x24, lsl #1
   38858:	lsl	x8, x8, #5
   3885c:	ldur	x3, [x29, #-8]
   38860:	add	x9, x19, x8
   38864:	add	x8, x20, x8
   38868:	add	x23, x9, #0x20
   3886c:	add	x25, x8, #0x8
   38870:	mov	w22, w0
   38874:	mov	w6, #0x3                   	// #3
   38878:	mov	x0, x23
   3887c:	mov	x1, x25
   38880:	ldur	x2, [x29, #-24]
   38884:	mov	x4, x24
   38888:	mov	x5, x26
   3888c:	mov	x7, x20
   38890:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   38894:	eor	w9, w0, w22
   38898:	cmp	x24, #0x2f
   3889c:	add	x22, x24, #0x1
   388a0:	stur	x21, [x29, #-48]
   388a4:	str	w9, [sp, #40]
   388a8:	b.le	38914 <__gmpn_toom8h_mul@@Base+0x164>
   388ac:	mov	x21, x27
   388b0:	cmp	x24, #0x50
   388b4:	b.le	38960 <__gmpn_toom8h_mul@@Base+0x1b0>
   388b8:	ldur	x8, [x29, #-48]
   388bc:	cmp	x24, #0xab
   388c0:	mov	x27, x28
   388c4:	add	x8, x19, x8, lsl #3
   388c8:	b.le	389ac <__gmpn_toom8h_mul@@Base+0x1fc>
   388cc:	add	x26, x8, #0x28
   388d0:	cmp	x24, #0xea
   388d4:	mov	x0, x20
   388d8:	mov	x1, x27
   388dc:	mov	x2, x22
   388e0:	mov	x3, x25
   388e4:	mov	x4, x22
   388e8:	mov	x5, x26
   388ec:	b.le	389ec <__gmpn_toom8h_mul@@Base+0x23c>
   388f0:	bl	cce0 <__gmpn_toom8h_mul@plt>
   388f4:	mov	x0, x19
   388f8:	mov	x1, x21
   388fc:	mov	x2, x22
   38900:	mov	x3, x23
   38904:	mov	x4, x22
   38908:	mov	x5, x26
   3890c:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38910:	b	38a0c <__gmpn_toom8h_mul@@Base+0x25c>
   38914:	add	x8, x19, x21, lsl #3
   38918:	add	x26, x8, #0x28
   3891c:	mov	x0, x20
   38920:	mov	x1, x28
   38924:	mov	x2, x22
   38928:	mov	x3, x25
   3892c:	mov	x4, x22
   38930:	mov	x5, x26
   38934:	bl	d630 <__gmpn_toom22_mul@plt>
   38938:	mov	x0, x19
   3893c:	mov	x1, x27
   38940:	mov	x2, x22
   38944:	mov	x3, x23
   38948:	mov	x4, x22
   3894c:	mov	x5, x26
   38950:	bl	d630 <__gmpn_toom22_mul@plt>
   38954:	mov	x21, x27
   38958:	mov	x27, x28
   3895c:	b	38a0c <__gmpn_toom8h_mul@@Base+0x25c>
   38960:	ldur	x8, [x29, #-48]
   38964:	mov	x0, x20
   38968:	mov	x1, x28
   3896c:	mov	x2, x22
   38970:	add	x8, x19, x8, lsl #3
   38974:	add	x26, x8, #0x28
   38978:	mov	x3, x25
   3897c:	mov	x4, x22
   38980:	mov	x5, x26
   38984:	mov	x27, x28
   38988:	bl	c1e0 <__gmpn_toom33_mul@plt>
   3898c:	mov	x0, x19
   38990:	mov	x1, x21
   38994:	mov	x2, x22
   38998:	mov	x3, x23
   3899c:	mov	x4, x22
   389a0:	mov	x5, x26
   389a4:	bl	c1e0 <__gmpn_toom33_mul@plt>
   389a8:	b	38a0c <__gmpn_toom8h_mul@@Base+0x25c>
   389ac:	add	x26, x8, #0x28
   389b0:	mov	x0, x20
   389b4:	mov	x1, x27
   389b8:	mov	x2, x22
   389bc:	mov	x3, x25
   389c0:	mov	x4, x22
   389c4:	mov	x5, x26
   389c8:	bl	c8b0 <__gmpn_toom44_mul@plt>
   389cc:	mov	x0, x19
   389d0:	mov	x1, x21
   389d4:	mov	x2, x22
   389d8:	mov	x3, x23
   389dc:	mov	x4, x22
   389e0:	mov	x5, x26
   389e4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   389e8:	b	38a0c <__gmpn_toom8h_mul@@Base+0x25c>
   389ec:	bl	cde0 <__gmpn_toom6h_mul@plt>
   389f0:	mov	x0, x19
   389f4:	mov	x1, x21
   389f8:	mov	x2, x22
   389fc:	mov	x3, x23
   38a00:	mov	x4, x22
   38a04:	mov	x5, x26
   38a08:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38a0c:	ldr	w6, [sp, #60]
   38a10:	ldr	w3, [sp, #40]
   38a14:	mov	w1, #0x1                   	// #1
   38a18:	bfi	x1, x24, #1, #63
   38a1c:	add	w8, w6, #0x1
   38a20:	bfi	w6, w6, #1, #1
   38a24:	add	w5, w6, #0x3
   38a28:	mov	x0, x19
   38a2c:	mov	x2, x20
   38a30:	mov	x4, x24
   38a34:	str	w8, [sp, #48]
   38a38:	str	x1, [sp, #64]
   38a3c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38a40:	ldur	x3, [x29, #-16]
   38a44:	ldur	x5, [x29, #-40]
   38a48:	mov	w6, #0x2                   	// #2
   38a4c:	mov	x0, x21
   38a50:	mov	x1, x27
   38a54:	ldur	x2, [x29, #-56]
   38a58:	mov	x4, x24
   38a5c:	mov	x7, x20
   38a60:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   38a64:	ldp	x5, x2, [x29, #-32]
   38a68:	ldur	x3, [x29, #-8]
   38a6c:	mov	w28, w0
   38a70:	mov	w6, #0x2                   	// #2
   38a74:	mov	x0, x23
   38a78:	mov	x1, x25
   38a7c:	mov	x4, x24
   38a80:	mov	x7, x20
   38a84:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   38a88:	cmp	x24, #0x2f
   38a8c:	eor	w28, w0, w28
   38a90:	b.le	38afc <__gmpn_toom8h_mul@@Base+0x34c>
   38a94:	ldur	x8, [x29, #-48]
   38a98:	cmp	x24, #0x50
   38a9c:	add	x8, x19, x8, lsl #3
   38aa0:	b.le	38b50 <__gmpn_toom8h_mul@@Base+0x3a0>
   38aa4:	cmp	x24, #0xab
   38aa8:	b.le	38b98 <__gmpn_toom8h_mul@@Base+0x3e8>
   38aac:	add	x26, x8, #0x28
   38ab0:	cmp	x24, #0xea
   38ab4:	mov	x0, x20
   38ab8:	mov	x1, x27
   38abc:	mov	x2, x22
   38ac0:	mov	x3, x25
   38ac4:	mov	x4, x22
   38ac8:	mov	x5, x26
   38acc:	b.le	38be0 <__gmpn_toom8h_mul@@Base+0x430>
   38ad0:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38ad4:	mov	w8, #0x18                  	// #24
   38ad8:	madd	x8, x24, x8, x19
   38adc:	add	x0, x8, #0x8
   38ae0:	mov	x1, x21
   38ae4:	mov	x2, x22
   38ae8:	mov	x3, x23
   38aec:	mov	x4, x22
   38af0:	mov	x5, x26
   38af4:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38af8:	b	38c08 <__gmpn_toom8h_mul@@Base+0x458>
   38afc:	ldur	x8, [x29, #-48]
   38b00:	mov	x0, x20
   38b04:	mov	x1, x27
   38b08:	mov	x2, x22
   38b0c:	add	x8, x19, x8, lsl #3
   38b10:	add	x26, x8, #0x28
   38b14:	mov	x3, x25
   38b18:	mov	x4, x22
   38b1c:	mov	x5, x26
   38b20:	bl	d630 <__gmpn_toom22_mul@plt>
   38b24:	mov	w8, #0x18                  	// #24
   38b28:	mov	x5, x26
   38b2c:	ldr	w26, [sp, #60]
   38b30:	madd	x8, x24, x8, x19
   38b34:	add	x0, x8, #0x8
   38b38:	mov	x1, x21
   38b3c:	mov	x2, x22
   38b40:	mov	x3, x23
   38b44:	mov	x4, x22
   38b48:	bl	d630 <__gmpn_toom22_mul@plt>
   38b4c:	b	38c0c <__gmpn_toom8h_mul@@Base+0x45c>
   38b50:	add	x26, x8, #0x28
   38b54:	mov	x0, x20
   38b58:	mov	x1, x27
   38b5c:	mov	x2, x22
   38b60:	mov	x3, x25
   38b64:	mov	x4, x22
   38b68:	mov	x5, x26
   38b6c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38b70:	mov	w8, #0x18                  	// #24
   38b74:	madd	x8, x24, x8, x19
   38b78:	add	x0, x8, #0x8
   38b7c:	mov	x1, x21
   38b80:	mov	x2, x22
   38b84:	mov	x3, x23
   38b88:	mov	x4, x22
   38b8c:	mov	x5, x26
   38b90:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38b94:	b	38c08 <__gmpn_toom8h_mul@@Base+0x458>
   38b98:	add	x26, x8, #0x28
   38b9c:	mov	x0, x20
   38ba0:	mov	x1, x27
   38ba4:	mov	x2, x22
   38ba8:	mov	x3, x25
   38bac:	mov	x4, x22
   38bb0:	mov	x5, x26
   38bb4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38bb8:	mov	w8, #0x18                  	// #24
   38bbc:	madd	x8, x24, x8, x19
   38bc0:	add	x0, x8, #0x8
   38bc4:	mov	x1, x21
   38bc8:	mov	x2, x22
   38bcc:	mov	x3, x23
   38bd0:	mov	x4, x22
   38bd4:	mov	x5, x26
   38bd8:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38bdc:	b	38c08 <__gmpn_toom8h_mul@@Base+0x458>
   38be0:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38be4:	mov	w8, #0x18                  	// #24
   38be8:	madd	x8, x24, x8, x19
   38bec:	add	x0, x8, #0x8
   38bf0:	mov	x1, x21
   38bf4:	mov	x2, x22
   38bf8:	mov	x3, x23
   38bfc:	mov	x4, x22
   38c00:	mov	x5, x26
   38c04:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38c08:	ldr	w26, [sp, #60]
   38c0c:	ldr	w8, [sp, #48]
   38c10:	ldr	x1, [sp, #64]
   38c14:	add	x9, x24, x24, lsl #1
   38c18:	lsl	w6, w26, #1
   38c1c:	lsl	w5, w8, #1
   38c20:	add	x8, x19, x9, lsl #3
   38c24:	add	x0, x8, #0x8
   38c28:	mov	x2, x20
   38c2c:	mov	w3, w28
   38c30:	mov	x4, x24
   38c34:	str	x9, [sp, #16]
   38c38:	str	x0, [sp, #40]
   38c3c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38c40:	ldur	x3, [x29, #-16]
   38c44:	ldur	x5, [x29, #-40]
   38c48:	mov	x0, x21
   38c4c:	mov	x1, x27
   38c50:	ldur	x2, [x29, #-56]
   38c54:	mov	x4, x24
   38c58:	mov	x6, x20
   38c5c:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   38c60:	ldp	x5, x2, [x29, #-32]
   38c64:	ldur	x3, [x29, #-8]
   38c68:	mov	w26, w0
   38c6c:	mov	x0, x23
   38c70:	mov	x1, x25
   38c74:	mov	x4, x24
   38c78:	mov	x6, x20
   38c7c:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   38c80:	cmp	x24, #0x2f
   38c84:	eor	w28, w0, w26
   38c88:	b.le	38cf4 <__gmpn_toom8h_mul@@Base+0x544>
   38c8c:	ldur	x8, [x29, #-48]
   38c90:	cmp	x24, #0x50
   38c94:	add	x8, x19, x8, lsl #3
   38c98:	b.le	38d44 <__gmpn_toom8h_mul@@Base+0x594>
   38c9c:	cmp	x24, #0xab
   38ca0:	b.le	38d8c <__gmpn_toom8h_mul@@Base+0x5dc>
   38ca4:	add	x26, x8, #0x28
   38ca8:	cmp	x24, #0xea
   38cac:	mov	x0, x20
   38cb0:	mov	x1, x27
   38cb4:	mov	x2, x22
   38cb8:	mov	x3, x25
   38cbc:	mov	x4, x22
   38cc0:	mov	x5, x26
   38cc4:	b.le	38dd4 <__gmpn_toom8h_mul@@Base+0x624>
   38cc8:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38ccc:	mov	w8, #0x30                  	// #48
   38cd0:	madd	x8, x24, x8, x19
   38cd4:	add	x0, x8, #0x10
   38cd8:	mov	x1, x21
   38cdc:	mov	x2, x22
   38ce0:	mov	x3, x23
   38ce4:	mov	x4, x22
   38ce8:	mov	x5, x26
   38cec:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38cf0:	b	38dfc <__gmpn_toom8h_mul@@Base+0x64c>
   38cf4:	ldur	x8, [x29, #-48]
   38cf8:	mov	x0, x20
   38cfc:	mov	x1, x27
   38d00:	mov	x2, x22
   38d04:	add	x8, x19, x8, lsl #3
   38d08:	add	x26, x8, #0x28
   38d0c:	mov	x3, x25
   38d10:	mov	x4, x22
   38d14:	mov	x5, x26
   38d18:	bl	d630 <__gmpn_toom22_mul@plt>
   38d1c:	mov	w8, #0x30                  	// #48
   38d20:	madd	x8, x24, x8, x19
   38d24:	add	x0, x8, #0x10
   38d28:	mov	x1, x21
   38d2c:	mov	x2, x22
   38d30:	mov	x3, x23
   38d34:	mov	x4, x22
   38d38:	mov	x5, x26
   38d3c:	bl	d630 <__gmpn_toom22_mul@plt>
   38d40:	b	38dfc <__gmpn_toom8h_mul@@Base+0x64c>
   38d44:	add	x26, x8, #0x28
   38d48:	mov	x0, x20
   38d4c:	mov	x1, x27
   38d50:	mov	x2, x22
   38d54:	mov	x3, x25
   38d58:	mov	x4, x22
   38d5c:	mov	x5, x26
   38d60:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38d64:	mov	w8, #0x30                  	// #48
   38d68:	madd	x8, x24, x8, x19
   38d6c:	add	x0, x8, #0x10
   38d70:	mov	x1, x21
   38d74:	mov	x2, x22
   38d78:	mov	x3, x23
   38d7c:	mov	x4, x22
   38d80:	mov	x5, x26
   38d84:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38d88:	b	38dfc <__gmpn_toom8h_mul@@Base+0x64c>
   38d8c:	add	x26, x8, #0x28
   38d90:	mov	x0, x20
   38d94:	mov	x1, x27
   38d98:	mov	x2, x22
   38d9c:	mov	x3, x25
   38da0:	mov	x4, x22
   38da4:	mov	x5, x26
   38da8:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38dac:	mov	w8, #0x30                  	// #48
   38db0:	madd	x8, x24, x8, x19
   38db4:	add	x0, x8, #0x10
   38db8:	mov	x1, x21
   38dbc:	mov	x2, x22
   38dc0:	mov	x3, x23
   38dc4:	mov	x4, x22
   38dc8:	mov	x5, x26
   38dcc:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38dd0:	b	38dfc <__gmpn_toom8h_mul@@Base+0x64c>
   38dd4:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38dd8:	mov	w8, #0x30                  	// #48
   38ddc:	madd	x8, x24, x8, x19
   38de0:	add	x0, x8, #0x10
   38de4:	mov	x1, x21
   38de8:	mov	x2, x22
   38dec:	mov	x3, x23
   38df0:	mov	x4, x22
   38df4:	mov	x5, x26
   38df8:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38dfc:	ldr	x1, [sp, #64]
   38e00:	ldur	x26, [x29, #-40]
   38e04:	mov	w8, #0x30                  	// #48
   38e08:	madd	x8, x24, x8, x19
   38e0c:	add	x0, x8, #0x10
   38e10:	mov	w5, #0x1                   	// #1
   38e14:	mov	w6, #0x2                   	// #2
   38e18:	mov	x2, x20
   38e1c:	mov	w3, w28
   38e20:	mov	x4, x24
   38e24:	str	x0, [sp, #32]
   38e28:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   38e2c:	ldur	x3, [x29, #-16]
   38e30:	mov	w6, #0x3                   	// #3
   38e34:	mov	x0, x21
   38e38:	mov	x1, x27
   38e3c:	ldur	x2, [x29, #-56]
   38e40:	mov	x4, x24
   38e44:	mov	x5, x26
   38e48:	mov	x7, x20
   38e4c:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   38e50:	ldp	x5, x2, [x29, #-32]
   38e54:	ldur	x3, [x29, #-8]
   38e58:	mov	w26, w0
   38e5c:	mov	w6, #0x3                   	// #3
   38e60:	mov	x0, x23
   38e64:	mov	x1, x25
   38e68:	mov	x4, x24
   38e6c:	mov	x7, x20
   38e70:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   38e74:	cmp	x24, #0x2f
   38e78:	eor	w28, w0, w26
   38e7c:	b.le	38ee8 <__gmpn_toom8h_mul@@Base+0x738>
   38e80:	ldur	x8, [x29, #-48]
   38e84:	cmp	x24, #0x50
   38e88:	add	x8, x19, x8, lsl #3
   38e8c:	b.le	38f38 <__gmpn_toom8h_mul@@Base+0x788>
   38e90:	cmp	x24, #0xab
   38e94:	b.le	38f80 <__gmpn_toom8h_mul@@Base+0x7d0>
   38e98:	add	x26, x8, #0x28
   38e9c:	cmp	x24, #0xea
   38ea0:	mov	x0, x20
   38ea4:	mov	x1, x27
   38ea8:	mov	x2, x22
   38eac:	mov	x3, x25
   38eb0:	mov	x4, x22
   38eb4:	mov	x5, x26
   38eb8:	b.le	38fc8 <__gmpn_toom8h_mul@@Base+0x818>
   38ebc:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38ec0:	mov	w8, #0x48                  	// #72
   38ec4:	madd	x8, x24, x8, x19
   38ec8:	add	x0, x8, #0x18
   38ecc:	mov	x1, x21
   38ed0:	mov	x2, x22
   38ed4:	mov	x3, x23
   38ed8:	mov	x4, x22
   38edc:	mov	x5, x26
   38ee0:	bl	cce0 <__gmpn_toom8h_mul@plt>
   38ee4:	b	38ff0 <__gmpn_toom8h_mul@@Base+0x840>
   38ee8:	ldur	x8, [x29, #-48]
   38eec:	mov	x0, x20
   38ef0:	mov	x1, x27
   38ef4:	mov	x2, x22
   38ef8:	add	x8, x19, x8, lsl #3
   38efc:	add	x26, x8, #0x28
   38f00:	mov	x3, x25
   38f04:	mov	x4, x22
   38f08:	mov	x5, x26
   38f0c:	bl	d630 <__gmpn_toom22_mul@plt>
   38f10:	mov	w8, #0x48                  	// #72
   38f14:	madd	x8, x24, x8, x19
   38f18:	add	x0, x8, #0x18
   38f1c:	mov	x1, x21
   38f20:	mov	x2, x22
   38f24:	mov	x3, x23
   38f28:	mov	x4, x22
   38f2c:	mov	x5, x26
   38f30:	bl	d630 <__gmpn_toom22_mul@plt>
   38f34:	b	38ff0 <__gmpn_toom8h_mul@@Base+0x840>
   38f38:	add	x26, x8, #0x28
   38f3c:	mov	x0, x20
   38f40:	mov	x1, x27
   38f44:	mov	x2, x22
   38f48:	mov	x3, x25
   38f4c:	mov	x4, x22
   38f50:	mov	x5, x26
   38f54:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38f58:	mov	w8, #0x48                  	// #72
   38f5c:	madd	x8, x24, x8, x19
   38f60:	add	x0, x8, #0x18
   38f64:	mov	x1, x21
   38f68:	mov	x2, x22
   38f6c:	mov	x3, x23
   38f70:	mov	x4, x22
   38f74:	mov	x5, x26
   38f78:	bl	c1e0 <__gmpn_toom33_mul@plt>
   38f7c:	b	38ff0 <__gmpn_toom8h_mul@@Base+0x840>
   38f80:	add	x26, x8, #0x28
   38f84:	mov	x0, x20
   38f88:	mov	x1, x27
   38f8c:	mov	x2, x22
   38f90:	mov	x3, x25
   38f94:	mov	x4, x22
   38f98:	mov	x5, x26
   38f9c:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38fa0:	mov	w8, #0x48                  	// #72
   38fa4:	madd	x8, x24, x8, x19
   38fa8:	add	x0, x8, #0x18
   38fac:	mov	x1, x21
   38fb0:	mov	x2, x22
   38fb4:	mov	x3, x23
   38fb8:	mov	x4, x22
   38fbc:	mov	x5, x26
   38fc0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   38fc4:	b	38ff0 <__gmpn_toom8h_mul@@Base+0x840>
   38fc8:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38fcc:	mov	w8, #0x48                  	// #72
   38fd0:	madd	x8, x24, x8, x19
   38fd4:	add	x0, x8, #0x18
   38fd8:	mov	x1, x21
   38fdc:	mov	x2, x22
   38fe0:	mov	x3, x23
   38fe4:	mov	x4, x22
   38fe8:	mov	x5, x26
   38fec:	bl	cde0 <__gmpn_toom6h_mul@plt>
   38ff0:	ldr	x1, [sp, #64]
   38ff4:	ldur	x26, [x29, #-40]
   38ff8:	mov	w8, #0x48                  	// #72
   38ffc:	madd	x8, x24, x8, x19
   39000:	add	x0, x8, #0x18
   39004:	mov	w5, #0x3                   	// #3
   39008:	mov	w6, #0x6                   	// #6
   3900c:	mov	x2, x20
   39010:	mov	w3, w28
   39014:	mov	x4, x24
   39018:	str	x0, [sp, #24]
   3901c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39020:	ldur	x3, [x29, #-16]
   39024:	mov	w6, #0x1                   	// #1
   39028:	mov	x0, x21
   3902c:	mov	x1, x27
   39030:	ldur	x2, [x29, #-56]
   39034:	mov	x4, x24
   39038:	mov	x5, x26
   3903c:	mov	x7, x20
   39040:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   39044:	ldp	x5, x2, [x29, #-32]
   39048:	ldur	x3, [x29, #-8]
   3904c:	mov	w26, w0
   39050:	mov	w6, #0x1                   	// #1
   39054:	mov	x0, x23
   39058:	mov	x1, x25
   3905c:	mov	x4, x24
   39060:	mov	x7, x20
   39064:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   39068:	cmp	x24, #0x2f
   3906c:	eor	w28, w0, w26
   39070:	str	x19, [sp, #8]
   39074:	b.le	390e4 <__gmpn_toom8h_mul@@Base+0x934>
   39078:	ldur	x8, [x29, #-48]
   3907c:	cmp	x24, #0x50
   39080:	b.le	39134 <__gmpn_toom8h_mul@@Base+0x984>
   39084:	cmp	x24, #0xab
   39088:	b.le	39180 <__gmpn_toom8h_mul@@Base+0x9d0>
   3908c:	mov	x9, x19
   39090:	add	x8, x9, x8, lsl #3
   39094:	add	x26, x8, #0x28
   39098:	mov	x19, x23
   3909c:	cmp	x24, #0xea
   390a0:	mov	x0, x20
   390a4:	mov	x1, x27
   390a8:	mov	x2, x22
   390ac:	mov	x3, x25
   390b0:	mov	x4, x22
   390b4:	mov	x5, x26
   390b8:	b.le	391cc <__gmpn_toom8h_mul@@Base+0xa1c>
   390bc:	bl	cce0 <__gmpn_toom8h_mul@plt>
   390c0:	ldr	x23, [sp, #16]
   390c4:	mov	x1, x21
   390c8:	mov	x2, x22
   390cc:	mov	x3, x19
   390d0:	add	x0, x20, x23, lsl #3
   390d4:	mov	x4, x22
   390d8:	mov	x5, x26
   390dc:	bl	cce0 <__gmpn_toom8h_mul@plt>
   390e0:	b	391f0 <__gmpn_toom8h_mul@@Base+0xa40>
   390e4:	ldur	x8, [x29, #-48]
   390e8:	mov	x0, x20
   390ec:	mov	x1, x27
   390f0:	mov	x2, x22
   390f4:	add	x8, x19, x8, lsl #3
   390f8:	add	x26, x8, #0x28
   390fc:	mov	x3, x25
   39100:	mov	x4, x22
   39104:	mov	x5, x26
   39108:	bl	d630 <__gmpn_toom22_mul@plt>
   3910c:	mov	x3, x23
   39110:	ldr	x23, [sp, #16]
   39114:	mov	x1, x21
   39118:	mov	x2, x22
   3911c:	mov	x4, x22
   39120:	add	x0, x20, x23, lsl #3
   39124:	mov	x5, x26
   39128:	mov	x19, x3
   3912c:	bl	d630 <__gmpn_toom22_mul@plt>
   39130:	b	391f0 <__gmpn_toom8h_mul@@Base+0xa40>
   39134:	add	x8, x19, x8, lsl #3
   39138:	add	x26, x8, #0x28
   3913c:	mov	x0, x20
   39140:	mov	x1, x27
   39144:	mov	x2, x22
   39148:	mov	x3, x25
   3914c:	mov	x4, x22
   39150:	mov	x5, x26
   39154:	bl	c1e0 <__gmpn_toom33_mul@plt>
   39158:	mov	x3, x23
   3915c:	ldr	x23, [sp, #16]
   39160:	mov	x1, x21
   39164:	mov	x2, x22
   39168:	mov	x4, x22
   3916c:	add	x0, x20, x23, lsl #3
   39170:	mov	x5, x26
   39174:	mov	x19, x3
   39178:	bl	c1e0 <__gmpn_toom33_mul@plt>
   3917c:	b	391f0 <__gmpn_toom8h_mul@@Base+0xa40>
   39180:	add	x8, x19, x8, lsl #3
   39184:	add	x26, x8, #0x28
   39188:	mov	x0, x20
   3918c:	mov	x1, x27
   39190:	mov	x2, x22
   39194:	mov	x3, x25
   39198:	mov	x4, x22
   3919c:	mov	x5, x26
   391a0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   391a4:	mov	x3, x23
   391a8:	ldr	x23, [sp, #16]
   391ac:	mov	x1, x21
   391b0:	mov	x2, x22
   391b4:	mov	x4, x22
   391b8:	add	x0, x20, x23, lsl #3
   391bc:	mov	x5, x26
   391c0:	mov	x19, x3
   391c4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   391c8:	b	391f0 <__gmpn_toom8h_mul@@Base+0xa40>
   391cc:	bl	cde0 <__gmpn_toom6h_mul@plt>
   391d0:	ldr	x23, [sp, #16]
   391d4:	mov	x1, x21
   391d8:	mov	x2, x22
   391dc:	mov	x3, x19
   391e0:	add	x0, x20, x23, lsl #3
   391e4:	mov	x4, x22
   391e8:	mov	x5, x26
   391ec:	bl	cde0 <__gmpn_toom6h_mul@plt>
   391f0:	ldr	w6, [sp, #60]
   391f4:	ldr	x1, [sp, #64]
   391f8:	ldr	w5, [sp, #48]
   391fc:	ldur	x26, [x29, #-40]
   39200:	add	x0, x20, x23, lsl #3
   39204:	mov	x2, x20
   39208:	mov	w3, w28
   3920c:	mov	x4, x24
   39210:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39214:	ldur	x3, [x29, #-16]
   39218:	mov	x0, x21
   3921c:	mov	x1, x27
   39220:	ldur	x2, [x29, #-56]
   39224:	mov	x4, x24
   39228:	mov	x5, x26
   3922c:	mov	x6, x20
   39230:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   39234:	ldur	x2, [x29, #-24]
   39238:	mov	w28, w0
   3923c:	mov	x23, x19
   39240:	mov	x0, x19
   39244:	cmp	w2, #0x3
   39248:	mov	x1, x25
   3924c:	b.eq	39704 <__gmpn_toom8h_mul@@Base+0xf54>  // b.none
   39250:	ldur	x3, [x29, #-8]
   39254:	ldur	x5, [x29, #-32]
   39258:	mov	x4, x24
   3925c:	mov	x6, x20
   39260:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   39264:	ldr	x19, [sp, #8]
   39268:	cmp	x24, #0x2f
   3926c:	eor	w28, w0, w28
   39270:	b.le	392dc <__gmpn_toom8h_mul@@Base+0xb2c>
   39274:	cmp	x24, #0x50
   39278:	b.le	3932c <__gmpn_toom8h_mul@@Base+0xb7c>
   3927c:	cmp	x24, #0xab
   39280:	b.le	3937c <__gmpn_toom8h_mul@@Base+0xbcc>
   39284:	ldur	x8, [x29, #-48]
   39288:	cmp	x24, #0xea
   3928c:	mov	x0, x20
   39290:	mov	x1, x27
   39294:	add	x8, x19, x8, lsl #3
   39298:	add	x8, x8, #0x28
   3929c:	mov	x2, x22
   392a0:	mov	x3, x25
   392a4:	mov	x4, x22
   392a8:	mov	x5, x8
   392ac:	str	x8, [sp, #48]
   392b0:	b.le	393cc <__gmpn_toom8h_mul@@Base+0xc1c>
   392b4:	bl	cce0 <__gmpn_toom8h_mul@plt>
   392b8:	ldr	x5, [sp, #48]
   392bc:	mov	w8, #0x38                  	// #56
   392c0:	madd	x0, x24, x8, x20
   392c4:	mov	x1, x21
   392c8:	mov	x2, x22
   392cc:	mov	x3, x23
   392d0:	mov	x4, x22
   392d4:	bl	cce0 <__gmpn_toom8h_mul@plt>
   392d8:	b	393f0 <__gmpn_toom8h_mul@@Base+0xc40>
   392dc:	ldur	x8, [x29, #-48]
   392e0:	mov	x0, x20
   392e4:	mov	x1, x27
   392e8:	mov	x2, x22
   392ec:	add	x8, x19, x8, lsl #3
   392f0:	add	x26, x8, #0x28
   392f4:	mov	x3, x25
   392f8:	mov	x4, x22
   392fc:	mov	x5, x26
   39300:	bl	d630 <__gmpn_toom22_mul@plt>
   39304:	mov	x5, x26
   39308:	ldur	x26, [x29, #-40]
   3930c:	mov	w8, #0x38                  	// #56
   39310:	madd	x0, x24, x8, x20
   39314:	mov	x1, x21
   39318:	mov	x2, x22
   3931c:	mov	x3, x23
   39320:	mov	x4, x22
   39324:	bl	d630 <__gmpn_toom22_mul@plt>
   39328:	b	393f0 <__gmpn_toom8h_mul@@Base+0xc40>
   3932c:	ldur	x8, [x29, #-48]
   39330:	mov	x0, x20
   39334:	mov	x1, x27
   39338:	mov	x2, x22
   3933c:	add	x8, x19, x8, lsl #3
   39340:	add	x26, x8, #0x28
   39344:	mov	x3, x25
   39348:	mov	x4, x22
   3934c:	mov	x5, x26
   39350:	bl	c1e0 <__gmpn_toom33_mul@plt>
   39354:	mov	x5, x26
   39358:	ldur	x26, [x29, #-40]
   3935c:	mov	w8, #0x38                  	// #56
   39360:	madd	x0, x24, x8, x20
   39364:	mov	x1, x21
   39368:	mov	x2, x22
   3936c:	mov	x3, x23
   39370:	mov	x4, x22
   39374:	bl	c1e0 <__gmpn_toom33_mul@plt>
   39378:	b	393f0 <__gmpn_toom8h_mul@@Base+0xc40>
   3937c:	ldur	x8, [x29, #-48]
   39380:	mov	x0, x20
   39384:	mov	x1, x27
   39388:	mov	x2, x22
   3938c:	add	x8, x19, x8, lsl #3
   39390:	add	x26, x8, #0x28
   39394:	mov	x3, x25
   39398:	mov	x4, x22
   3939c:	mov	x5, x26
   393a0:	bl	c8b0 <__gmpn_toom44_mul@plt>
   393a4:	mov	x5, x26
   393a8:	ldur	x26, [x29, #-40]
   393ac:	mov	w8, #0x38                  	// #56
   393b0:	madd	x0, x24, x8, x20
   393b4:	mov	x1, x21
   393b8:	mov	x2, x22
   393bc:	mov	x3, x23
   393c0:	mov	x4, x22
   393c4:	bl	c8b0 <__gmpn_toom44_mul@plt>
   393c8:	b	393f0 <__gmpn_toom8h_mul@@Base+0xc40>
   393cc:	bl	cde0 <__gmpn_toom6h_mul@plt>
   393d0:	ldr	x5, [sp, #48]
   393d4:	mov	w8, #0x38                  	// #56
   393d8:	madd	x0, x24, x8, x20
   393dc:	mov	x1, x21
   393e0:	mov	x2, x22
   393e4:	mov	x3, x23
   393e8:	mov	x4, x22
   393ec:	bl	cde0 <__gmpn_toom6h_mul@plt>
   393f0:	ldr	x1, [sp, #64]
   393f4:	mov	w8, #0x38                  	// #56
   393f8:	madd	x0, x24, x8, x20
   393fc:	mov	x2, x20
   39400:	mov	w3, w28
   39404:	mov	x4, x24
   39408:	mov	w5, wzr
   3940c:	mov	w6, wzr
   39410:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39414:	ldur	x3, [x29, #-16]
   39418:	mov	w6, #0x2                   	// #2
   3941c:	mov	x0, x21
   39420:	mov	x1, x27
   39424:	ldur	x2, [x29, #-56]
   39428:	mov	x4, x24
   3942c:	mov	x5, x26
   39430:	mov	x7, x20
   39434:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   39438:	ldp	x5, x2, [x29, #-32]
   3943c:	ldur	x3, [x29, #-8]
   39440:	mov	w26, w0
   39444:	mov	w6, #0x2                   	// #2
   39448:	mov	x0, x23
   3944c:	mov	x1, x25
   39450:	mov	x4, x24
   39454:	mov	x7, x20
   39458:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   3945c:	cmp	x24, #0x2f
   39460:	eor	w28, w0, w26
   39464:	b.le	394c8 <__gmpn_toom8h_mul@@Base+0xd18>
   39468:	ldur	x8, [x29, #-48]
   3946c:	cmp	x24, #0x50
   39470:	add	x8, x19, x8, lsl #3
   39474:	b.le	39510 <__gmpn_toom8h_mul@@Base+0xd60>
   39478:	cmp	x24, #0xab
   3947c:	b.le	39550 <__gmpn_toom8h_mul@@Base+0xda0>
   39480:	add	x26, x8, #0x28
   39484:	cmp	x24, #0xea
   39488:	mov	x0, x20
   3948c:	mov	x1, x27
   39490:	mov	x2, x22
   39494:	mov	x3, x25
   39498:	mov	x4, x22
   3949c:	mov	x5, x26
   394a0:	b.le	39590 <__gmpn_toom8h_mul@@Base+0xde0>
   394a4:	bl	cce0 <__gmpn_toom8h_mul@plt>
   394a8:	mov	x0, x27
   394ac:	mov	x1, x21
   394b0:	mov	x2, x22
   394b4:	mov	x3, x23
   394b8:	mov	x4, x22
   394bc:	mov	x5, x26
   394c0:	bl	cce0 <__gmpn_toom8h_mul@plt>
   394c4:	b	395b0 <__gmpn_toom8h_mul@@Base+0xe00>
   394c8:	ldur	x8, [x29, #-48]
   394cc:	mov	x0, x20
   394d0:	mov	x1, x27
   394d4:	mov	x2, x22
   394d8:	add	x8, x19, x8, lsl #3
   394dc:	add	x26, x8, #0x28
   394e0:	mov	x3, x25
   394e4:	mov	x4, x22
   394e8:	mov	x5, x26
   394ec:	bl	d630 <__gmpn_toom22_mul@plt>
   394f0:	mov	x0, x27
   394f4:	mov	x1, x21
   394f8:	mov	x2, x22
   394fc:	mov	x3, x23
   39500:	mov	x4, x22
   39504:	mov	x5, x26
   39508:	bl	d630 <__gmpn_toom22_mul@plt>
   3950c:	b	395b0 <__gmpn_toom8h_mul@@Base+0xe00>
   39510:	add	x26, x8, #0x28
   39514:	mov	x0, x20
   39518:	mov	x1, x27
   3951c:	mov	x2, x22
   39520:	mov	x3, x25
   39524:	mov	x4, x22
   39528:	mov	x5, x26
   3952c:	bl	c1e0 <__gmpn_toom33_mul@plt>
   39530:	mov	x0, x27
   39534:	mov	x1, x21
   39538:	mov	x2, x22
   3953c:	mov	x3, x23
   39540:	mov	x4, x22
   39544:	mov	x5, x26
   39548:	bl	c1e0 <__gmpn_toom33_mul@plt>
   3954c:	b	395b0 <__gmpn_toom8h_mul@@Base+0xe00>
   39550:	add	x26, x8, #0x28
   39554:	mov	x0, x20
   39558:	mov	x1, x27
   3955c:	mov	x2, x22
   39560:	mov	x3, x25
   39564:	mov	x4, x22
   39568:	mov	x5, x26
   3956c:	bl	c8b0 <__gmpn_toom44_mul@plt>
   39570:	mov	x0, x27
   39574:	mov	x1, x21
   39578:	mov	x2, x22
   3957c:	mov	x3, x23
   39580:	mov	x4, x22
   39584:	mov	x5, x26
   39588:	bl	c8b0 <__gmpn_toom44_mul@plt>
   3958c:	b	395b0 <__gmpn_toom8h_mul@@Base+0xe00>
   39590:	bl	cde0 <__gmpn_toom6h_mul@plt>
   39594:	mov	x0, x27
   39598:	mov	x1, x21
   3959c:	mov	x2, x22
   395a0:	mov	x3, x23
   395a4:	mov	x4, x22
   395a8:	mov	x5, x26
   395ac:	bl	cde0 <__gmpn_toom6h_mul@plt>
   395b0:	ldr	x1, [sp, #64]
   395b4:	ldr	w21, [sp, #60]
   395b8:	ldp	x22, x25, [x29, #-40]
   395bc:	ldr	x26, [sp, #40]
   395c0:	mov	w5, #0x2                   	// #2
   395c4:	mov	w6, #0x4                   	// #4
   395c8:	mov	x0, x27
   395cc:	mov	x2, x20
   395d0:	mov	w3, w28
   395d4:	mov	x4, x24
   395d8:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   395dc:	mov	x0, x20
   395e0:	cmp	x24, #0x30
   395e4:	b.le	3961c <__gmpn_toom8h_mul@@Base+0xe6c>
   395e8:	cmp	x24, #0x51
   395ec:	b.le	39638 <__gmpn_toom8h_mul@@Base+0xe88>
   395f0:	cmp	x24, #0xac
   395f4:	b.le	39654 <__gmpn_toom8h_mul@@Base+0xea4>
   395f8:	cmp	x24, #0xeb
   395fc:	b.le	39670 <__gmpn_toom8h_mul@@Base+0xec0>
   39600:	ldp	x1, x3, [x29, #-16]
   39604:	mov	x2, x24
   39608:	mov	x4, x24
   3960c:	mov	x5, x23
   39610:	bl	cce0 <__gmpn_toom8h_mul@plt>
   39614:	cbz	w21, 39688 <__gmpn_toom8h_mul@@Base+0xed8>
   39618:	b	3971c <__gmpn_toom8h_mul@@Base+0xf6c>
   3961c:	ldp	x1, x3, [x29, #-16]
   39620:	mov	x2, x24
   39624:	mov	x4, x24
   39628:	mov	x5, x23
   3962c:	bl	d630 <__gmpn_toom22_mul@plt>
   39630:	cbz	w21, 39688 <__gmpn_toom8h_mul@@Base+0xed8>
   39634:	b	3971c <__gmpn_toom8h_mul@@Base+0xf6c>
   39638:	ldp	x1, x3, [x29, #-16]
   3963c:	mov	x2, x24
   39640:	mov	x4, x24
   39644:	mov	x5, x23
   39648:	bl	c1e0 <__gmpn_toom33_mul@plt>
   3964c:	cbz	w21, 39688 <__gmpn_toom8h_mul@@Base+0xed8>
   39650:	b	3971c <__gmpn_toom8h_mul@@Base+0xf6c>
   39654:	ldp	x1, x3, [x29, #-16]
   39658:	mov	x2, x24
   3965c:	mov	x4, x24
   39660:	mov	x5, x23
   39664:	bl	c8b0 <__gmpn_toom44_mul@plt>
   39668:	cbz	w21, 39688 <__gmpn_toom8h_mul@@Base+0xed8>
   3966c:	b	3971c <__gmpn_toom8h_mul@@Base+0xf6c>
   39670:	ldp	x1, x3, [x29, #-16]
   39674:	mov	x2, x24
   39678:	mov	x4, x24
   3967c:	mov	x5, x23
   39680:	bl	cde0 <__gmpn_toom6h_mul@plt>
   39684:	cbnz	w21, 3971c <__gmpn_toom8h_mul@@Base+0xf6c>
   39688:	ldp	x1, x2, [sp, #24]
   3968c:	add	x6, x25, x22
   39690:	mov	x0, x20
   39694:	mov	x3, x26
   39698:	mov	x4, x19
   3969c:	mov	x5, x24
   396a0:	mov	w7, w21
   396a4:	str	x23, [sp]
   396a8:	bl	d530 <__gmpn_toom_interpolate_16pts@plt>
   396ac:	ldp	x20, x19, [sp, #208]
   396b0:	ldp	x22, x21, [sp, #192]
   396b4:	ldp	x24, x23, [sp, #176]
   396b8:	ldp	x26, x25, [sp, #160]
   396bc:	ldp	x28, x27, [sp, #144]
   396c0:	ldp	x29, x30, [sp, #128]
   396c4:	add	sp, sp, #0xe0
   396c8:	ret
   396cc:	add	x9, x2, x2, lsl #2
   396d0:	asr	x8, x4, #1
   396d4:	mov	w10, #0x15                  	// #21
   396d8:	lsl	x9, x9, #1
   396dc:	mul	x10, x8, x10
   396e0:	cmp	x9, x10
   396e4:	b.lt	387e0 <__gmpn_toom8h_mul@@Base+0x30>  // b.tstop
   396e8:	mov	w10, #0xd                   	// #13
   396ec:	mul	x10, x2, x10
   396f0:	cmp	x10, x4, lsl #4
   396f4:	b.ge	39790 <__gmpn_toom8h_mul@@Base+0xfe0>  // b.tcont
   396f8:	mov	w8, #0x8                   	// #8
   396fc:	mov	w9, #0x9                   	// #9
   39700:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   39704:	ldur	x2, [x29, #-8]
   39708:	ldur	x4, [x29, #-32]
   3970c:	mov	x3, x24
   39710:	mov	x5, x20
   39714:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   39718:	b	39264 <__gmpn_toom8h_mul@@Base+0xab4>
   3971c:	mov	w8, #0x78                  	// #120
   39720:	cmp	x22, x25
   39724:	madd	x0, x24, x8, x20
   39728:	b.le	39758 <__gmpn_toom8h_mul@@Base+0xfa8>
   3972c:	ldur	x8, [x29, #-56]
   39730:	ldp	x9, x10, [x29, #-24]
   39734:	mov	x2, x22
   39738:	mov	x4, x25
   3973c:	sxtw	x8, w8
   39740:	mul	x8, x24, x8
   39744:	add	x1, x10, x8, lsl #3
   39748:	ldur	x8, [x29, #-8]
   3974c:	sxtw	x9, w9
   39750:	mul	x9, x24, x9
   39754:	b	39784 <__gmpn_toom8h_mul@@Base+0xfd4>
   39758:	ldur	x8, [x29, #-24]
   3975c:	ldur	x10, [x29, #-8]
   39760:	ldur	x9, [x29, #-56]
   39764:	mov	x2, x25
   39768:	sxtw	x8, w8
   3976c:	mul	x8, x24, x8
   39770:	add	x1, x10, x8, lsl #3
   39774:	ldur	x8, [x29, #-16]
   39778:	sxtw	x9, w9
   3977c:	mul	x9, x24, x9
   39780:	mov	x4, x22
   39784:	add	x3, x8, x9, lsl #3
   39788:	bl	cea0 <__gmpn_mul@plt>
   3978c:	b	39688 <__gmpn_toom8h_mul@@Base+0xed8>
   39790:	mov	w10, #0x1b                  	// #27
   39794:	mul	x10, x8, x10
   39798:	cmp	x9, x10
   3979c:	b.ge	397ac <__gmpn_toom8h_mul@@Base+0xffc>  // b.tcont
   397a0:	mov	w8, #0x7                   	// #7
   397a4:	mov	w9, #0x9                   	// #9
   397a8:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   397ac:	add	x8, x8, x8, lsl #5
   397b0:	cmp	x9, x8
   397b4:	b.ge	397c4 <__gmpn_toom8h_mul@@Base+0x1014>  // b.tcont
   397b8:	mov	w8, #0x7                   	// #7
   397bc:	mov	w9, #0xa                   	// #10
   397c0:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   397c4:	lsl	x9, x4, #3
   397c8:	lsl	x8, x2, #2
   397cc:	sub	x9, x9, x4
   397d0:	cmp	x8, x9
   397d4:	b.ge	397e4 <__gmpn_toom8h_mul@@Base+0x1034>  // b.tcont
   397d8:	mov	w8, #0x6                   	// #6
   397dc:	mov	w9, #0xa                   	// #10
   397e0:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   397e4:	add	x9, x2, x2, lsl #1
   397e8:	mov	w10, #0xd                   	// #13
   397ec:	lsl	x9, x9, #1
   397f0:	mul	x10, x4, x10
   397f4:	cmp	x9, x10
   397f8:	b.ge	39808 <__gmpn_toom8h_mul@@Base+0x1058>  // b.tcont
   397fc:	mov	w8, #0x6                   	// #6
   39800:	mov	w9, #0xb                   	// #11
   39804:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   39808:	add	x9, x4, x4, lsl #3
   3980c:	cmp	x8, x9
   39810:	b.ge	39820 <__gmpn_toom8h_mul@@Base+0x1070>  // b.tcont
   39814:	mov	w8, #0x5                   	// #5
   39818:	mov	w9, #0xb                   	// #11
   3981c:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   39820:	lsl	x8, x2, #3
   39824:	sub	x8, x8, x2
   39828:	add	x9, x4, x4, lsl #2
   3982c:	cmp	x8, x9, lsl #2
   39830:	mov	w9, #0xc                   	// #12
   39834:	b.ge	39840 <__gmpn_toom8h_mul@@Base+0x1090>  // b.tcont
   39838:	mov	w8, #0x5                   	// #5
   3983c:	b	39858 <__gmpn_toom8h_mul@@Base+0x10a8>
   39840:	mov	w10, #0x1c                  	// #28
   39844:	add	x8, x2, x2, lsl #3
   39848:	mul	x10, x4, x10
   3984c:	cmp	x8, x10
   39850:	cinc	w9, w9, ge  // ge = tcont
   39854:	mov	w8, #0x4                   	// #4
   39858:	mov	w12, w9
   3985c:	mul	x11, x8, x2
   39860:	mul	x13, x12, x4
   39864:	cmp	x11, x13
   39868:	csel	x11, x4, x2, lt  // lt = tstop
   3986c:	csel	x12, x8, x12, lt  // lt = tstop
   39870:	sub	x11, x11, #0x1
   39874:	sub	w15, w8, #0x1
   39878:	udiv	x11, x11, x12
   3987c:	add	w10, w8, w9
   39880:	sub	w14, w9, #0x1
   39884:	sxtw	x12, w15
   39888:	add	x24, x11, #0x1
   3988c:	and	w13, w10, #0x1
   39890:	msub	x5, x24, x14, x2
   39894:	msub	x26, x24, x12, x4
   39898:	mov	x28, x15
   3989c:	stur	x15, [x29, #-24]
   398a0:	tbz	w10, #0, 3880c <__gmpn_toom8h_mul@@Base+0x5c>
   398a4:	cmp	x5, #0x0
   398a8:	b.le	398c4 <__gmpn_toom8h_mul@@Base+0x1114>
   398ac:	cmp	x26, #0x0
   398b0:	b.gt	3880c <__gmpn_toom8h_mul@@Base+0x5c>
   398b4:	mov	w13, wzr
   398b8:	sub	w8, w8, #0x2
   398bc:	add	x26, x26, x24
   398c0:	b	38808 <__gmpn_toom8h_mul@@Base+0x58>
   398c4:	mov	w13, wzr
   398c8:	sub	w14, w9, #0x2
   398cc:	add	x5, x5, x24
   398d0:	b	3880c <__gmpn_toom8h_mul@@Base+0x5c>

00000000000398d4 <__gmpn_toom8_sqr@@Base>:
   398d4:	sub	sp, sp, #0xa0
   398d8:	sub	x8, x2, #0x1
   398dc:	stp	x22, x21, [sp, #128]
   398e0:	asr	x22, x8, #3
   398e4:	add	x21, x22, #0x1
   398e8:	mov	w9, #0x68                  	// #104
   398ec:	lsl	x8, x21, #3
   398f0:	mov	w10, #0x58                  	// #88
   398f4:	madd	x9, x21, x9, x0
   398f8:	sub	x8, x8, x21
   398fc:	stp	x26, x25, [sp, #96]
   39900:	stp	x24, x23, [sp, #112]
   39904:	stp	x20, x19, [sp, #144]
   39908:	mov	x20, x0
   3990c:	add	x26, x9, #0x10
   39910:	sub	x23, x2, x8
   39914:	madd	x25, x21, x10, x0
   39918:	stp	x29, x30, [sp, #64]
   3991c:	add	x29, sp, #0x40
   39920:	mov	x19, x3
   39924:	mov	x24, x2
   39928:	mov	x3, x1
   3992c:	mov	w2, #0x7                   	// #7
   39930:	mov	w6, #0x3                   	// #3
   39934:	mov	x0, x26
   39938:	mov	x1, x25
   3993c:	mov	x4, x21
   39940:	mov	x5, x23
   39944:	mov	x7, x20
   39948:	stp	x28, x27, [sp, #80]
   3994c:	str	x8, [sp, #24]
   39950:	stur	x3, [x29, #-8]
   39954:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   39958:	mov	w8, #0x60                  	// #96
   3995c:	cmp	x24, #0x208
   39960:	add	x27, x22, #0x2
   39964:	madd	x8, x21, x8, x19
   39968:	b.le	399b4 <__gmpn_toom8_sqr@@Base+0xe0>
   3996c:	cmp	x24, #0x520
   39970:	b.le	399e4 <__gmpn_toom8_sqr@@Base+0x110>
   39974:	cmp	x24, #0x6e0
   39978:	b.le	39a14 <__gmpn_toom8_sqr@@Base+0x140>
   3997c:	add	x28, x8, #0x20
   39980:	cmp	x24, #0xa58
   39984:	mov	x0, x20
   39988:	mov	x1, x25
   3998c:	mov	x2, x27
   39990:	mov	x3, x28
   39994:	b.le	39a44 <__gmpn_toom8_sqr@@Base+0x170>
   39998:	bl	d690 <__gmpn_toom8_sqr@plt>
   3999c:	mov	x0, x19
   399a0:	mov	x1, x26
   399a4:	mov	x2, x27
   399a8:	mov	x3, x28
   399ac:	bl	d690 <__gmpn_toom8_sqr@plt>
   399b0:	b	39a5c <__gmpn_toom8_sqr@@Base+0x188>
   399b4:	add	x28, x8, #0x20
   399b8:	mov	x0, x20
   399bc:	mov	x1, x25
   399c0:	mov	x2, x27
   399c4:	mov	x3, x28
   399c8:	bl	c190 <__gmpn_toom2_sqr@plt>
   399cc:	mov	x0, x19
   399d0:	mov	x1, x26
   399d4:	mov	x2, x27
   399d8:	mov	x3, x28
   399dc:	bl	c190 <__gmpn_toom2_sqr@plt>
   399e0:	b	39a5c <__gmpn_toom8_sqr@@Base+0x188>
   399e4:	add	x28, x8, #0x20
   399e8:	mov	x0, x20
   399ec:	mov	x1, x25
   399f0:	mov	x2, x27
   399f4:	mov	x3, x28
   399f8:	bl	d480 <__gmpn_toom3_sqr@plt>
   399fc:	mov	x0, x19
   39a00:	mov	x1, x26
   39a04:	mov	x2, x27
   39a08:	mov	x3, x28
   39a0c:	bl	d480 <__gmpn_toom3_sqr@plt>
   39a10:	b	39a5c <__gmpn_toom8_sqr@@Base+0x188>
   39a14:	add	x28, x8, #0x20
   39a18:	mov	x0, x20
   39a1c:	mov	x1, x25
   39a20:	mov	x2, x27
   39a24:	mov	x3, x28
   39a28:	bl	c370 <__gmpn_toom4_sqr@plt>
   39a2c:	mov	x0, x19
   39a30:	mov	x1, x26
   39a34:	mov	x2, x27
   39a38:	mov	x3, x28
   39a3c:	bl	c370 <__gmpn_toom4_sqr@plt>
   39a40:	b	39a5c <__gmpn_toom8_sqr@@Base+0x188>
   39a44:	bl	d650 <__gmpn_toom6_sqr@plt>
   39a48:	mov	x0, x19
   39a4c:	mov	x1, x26
   39a50:	mov	x2, x27
   39a54:	mov	x3, x28
   39a58:	bl	d650 <__gmpn_toom6_sqr@plt>
   39a5c:	mov	w28, #0x1                   	// #1
   39a60:	bfi	x28, x21, #1, #63
   39a64:	mov	w5, #0x3                   	// #3
   39a68:	mov	x0, x19
   39a6c:	mov	x1, x28
   39a70:	mov	x2, x20
   39a74:	mov	w3, wzr
   39a78:	mov	x4, x21
   39a7c:	mov	w6, wzr
   39a80:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39a84:	ldur	x3, [x29, #-8]
   39a88:	mov	w2, #0x7                   	// #7
   39a8c:	mov	w6, #0x2                   	// #2
   39a90:	mov	x0, x26
   39a94:	mov	x1, x25
   39a98:	mov	x4, x21
   39a9c:	mov	x5, x23
   39aa0:	mov	x7, x20
   39aa4:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   39aa8:	mov	w8, #0x60                  	// #96
   39aac:	cmp	x24, #0x208
   39ab0:	madd	x8, x21, x8, x19
   39ab4:	stur	x23, [x29, #-16]
   39ab8:	b.le	39b0c <__gmpn_toom8_sqr@@Base+0x238>
   39abc:	cmp	x24, #0x520
   39ac0:	b.le	39b44 <__gmpn_toom8_sqr@@Base+0x270>
   39ac4:	cmp	x24, #0x6e0
   39ac8:	b.le	39b7c <__gmpn_toom8_sqr@@Base+0x2a8>
   39acc:	add	x22, x8, #0x20
   39ad0:	cmp	x24, #0xa58
   39ad4:	mov	x0, x20
   39ad8:	mov	x1, x25
   39adc:	mov	x2, x27
   39ae0:	mov	x3, x22
   39ae4:	b.le	39bb4 <__gmpn_toom8_sqr@@Base+0x2e0>
   39ae8:	bl	d690 <__gmpn_toom8_sqr@plt>
   39aec:	mov	w8, #0x18                  	// #24
   39af0:	madd	x8, x21, x8, x19
   39af4:	add	x0, x8, #0x8
   39af8:	mov	x1, x26
   39afc:	mov	x2, x27
   39b00:	mov	x3, x22
   39b04:	bl	d690 <__gmpn_toom8_sqr@plt>
   39b08:	b	39bd4 <__gmpn_toom8_sqr@@Base+0x300>
   39b0c:	add	x22, x8, #0x20
   39b10:	mov	x0, x20
   39b14:	mov	x1, x25
   39b18:	mov	x2, x27
   39b1c:	mov	x3, x22
   39b20:	bl	c190 <__gmpn_toom2_sqr@plt>
   39b24:	mov	w8, #0x18                  	// #24
   39b28:	madd	x8, x21, x8, x19
   39b2c:	add	x0, x8, #0x8
   39b30:	mov	x1, x26
   39b34:	mov	x2, x27
   39b38:	mov	x3, x22
   39b3c:	bl	c190 <__gmpn_toom2_sqr@plt>
   39b40:	b	39bd4 <__gmpn_toom8_sqr@@Base+0x300>
   39b44:	add	x22, x8, #0x20
   39b48:	mov	x0, x20
   39b4c:	mov	x1, x25
   39b50:	mov	x2, x27
   39b54:	mov	x3, x22
   39b58:	bl	d480 <__gmpn_toom3_sqr@plt>
   39b5c:	mov	w8, #0x18                  	// #24
   39b60:	madd	x8, x21, x8, x19
   39b64:	add	x0, x8, #0x8
   39b68:	mov	x1, x26
   39b6c:	mov	x2, x27
   39b70:	mov	x3, x22
   39b74:	bl	d480 <__gmpn_toom3_sqr@plt>
   39b78:	b	39bd4 <__gmpn_toom8_sqr@@Base+0x300>
   39b7c:	add	x22, x8, #0x20
   39b80:	mov	x0, x20
   39b84:	mov	x1, x25
   39b88:	mov	x2, x27
   39b8c:	mov	x3, x22
   39b90:	bl	c370 <__gmpn_toom4_sqr@plt>
   39b94:	mov	w8, #0x18                  	// #24
   39b98:	madd	x8, x21, x8, x19
   39b9c:	add	x0, x8, #0x8
   39ba0:	mov	x1, x26
   39ba4:	mov	x2, x27
   39ba8:	mov	x3, x22
   39bac:	bl	c370 <__gmpn_toom4_sqr@plt>
   39bb0:	b	39bd4 <__gmpn_toom8_sqr@@Base+0x300>
   39bb4:	bl	d650 <__gmpn_toom6_sqr@plt>
   39bb8:	mov	w8, #0x18                  	// #24
   39bbc:	madd	x8, x21, x8, x19
   39bc0:	add	x0, x8, #0x8
   39bc4:	mov	x1, x26
   39bc8:	mov	x2, x27
   39bcc:	mov	x3, x22
   39bd0:	bl	d650 <__gmpn_toom6_sqr@plt>
   39bd4:	add	x23, x21, x21, lsl #1
   39bd8:	add	x8, x19, x23, lsl #3
   39bdc:	add	x22, x8, #0x8
   39be0:	mov	w5, #0x2                   	// #2
   39be4:	mov	x0, x22
   39be8:	mov	x1, x28
   39bec:	mov	x2, x20
   39bf0:	mov	w3, wzr
   39bf4:	mov	x4, x21
   39bf8:	mov	w6, wzr
   39bfc:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39c00:	ldp	x5, x3, [x29, #-16]
   39c04:	mov	w2, #0x7                   	// #7
   39c08:	mov	x0, x26
   39c0c:	mov	x1, x25
   39c10:	mov	x4, x21
   39c14:	mov	x6, x20
   39c18:	bl	c6c0 <__gmpn_toom_eval_pm2@plt>
   39c1c:	mov	w8, #0x60                  	// #96
   39c20:	cmp	x24, #0x208
   39c24:	madd	x8, x21, x8, x19
   39c28:	str	x22, [sp, #16]
   39c2c:	b.le	39c80 <__gmpn_toom8_sqr@@Base+0x3ac>
   39c30:	cmp	x24, #0x520
   39c34:	b.le	39cb8 <__gmpn_toom8_sqr@@Base+0x3e4>
   39c38:	cmp	x24, #0x6e0
   39c3c:	b.le	39cf0 <__gmpn_toom8_sqr@@Base+0x41c>
   39c40:	add	x22, x8, #0x20
   39c44:	cmp	x24, #0xa58
   39c48:	mov	x0, x20
   39c4c:	mov	x1, x25
   39c50:	mov	x2, x27
   39c54:	mov	x3, x22
   39c58:	b.le	39d28 <__gmpn_toom8_sqr@@Base+0x454>
   39c5c:	bl	d690 <__gmpn_toom8_sqr@plt>
   39c60:	mov	w8, #0x30                  	// #48
   39c64:	madd	x8, x21, x8, x19
   39c68:	add	x0, x8, #0x10
   39c6c:	mov	x1, x26
   39c70:	mov	x2, x27
   39c74:	mov	x3, x22
   39c78:	bl	d690 <__gmpn_toom8_sqr@plt>
   39c7c:	b	39d48 <__gmpn_toom8_sqr@@Base+0x474>
   39c80:	add	x22, x8, #0x20
   39c84:	mov	x0, x20
   39c88:	mov	x1, x25
   39c8c:	mov	x2, x27
   39c90:	mov	x3, x22
   39c94:	bl	c190 <__gmpn_toom2_sqr@plt>
   39c98:	mov	w8, #0x30                  	// #48
   39c9c:	madd	x8, x21, x8, x19
   39ca0:	add	x0, x8, #0x10
   39ca4:	mov	x1, x26
   39ca8:	mov	x2, x27
   39cac:	mov	x3, x22
   39cb0:	bl	c190 <__gmpn_toom2_sqr@plt>
   39cb4:	b	39d48 <__gmpn_toom8_sqr@@Base+0x474>
   39cb8:	add	x22, x8, #0x20
   39cbc:	mov	x0, x20
   39cc0:	mov	x1, x25
   39cc4:	mov	x2, x27
   39cc8:	mov	x3, x22
   39ccc:	bl	d480 <__gmpn_toom3_sqr@plt>
   39cd0:	mov	w8, #0x30                  	// #48
   39cd4:	madd	x8, x21, x8, x19
   39cd8:	add	x0, x8, #0x10
   39cdc:	mov	x1, x26
   39ce0:	mov	x2, x27
   39ce4:	mov	x3, x22
   39ce8:	bl	d480 <__gmpn_toom3_sqr@plt>
   39cec:	b	39d48 <__gmpn_toom8_sqr@@Base+0x474>
   39cf0:	add	x22, x8, #0x20
   39cf4:	mov	x0, x20
   39cf8:	mov	x1, x25
   39cfc:	mov	x2, x27
   39d00:	mov	x3, x22
   39d04:	bl	c370 <__gmpn_toom4_sqr@plt>
   39d08:	mov	w8, #0x30                  	// #48
   39d0c:	madd	x8, x21, x8, x19
   39d10:	add	x0, x8, #0x10
   39d14:	mov	x1, x26
   39d18:	mov	x2, x27
   39d1c:	mov	x3, x22
   39d20:	bl	c370 <__gmpn_toom4_sqr@plt>
   39d24:	b	39d48 <__gmpn_toom8_sqr@@Base+0x474>
   39d28:	bl	d650 <__gmpn_toom6_sqr@plt>
   39d2c:	mov	w8, #0x30                  	// #48
   39d30:	madd	x8, x21, x8, x19
   39d34:	add	x0, x8, #0x10
   39d38:	mov	x1, x26
   39d3c:	mov	x2, x27
   39d40:	mov	x3, x22
   39d44:	bl	d650 <__gmpn_toom6_sqr@plt>
   39d48:	mov	w8, #0x30                  	// #48
   39d4c:	madd	x8, x21, x8, x19
   39d50:	add	x22, x8, #0x10
   39d54:	mov	w5, #0x1                   	// #1
   39d58:	mov	w6, #0x2                   	// #2
   39d5c:	mov	x0, x22
   39d60:	mov	x1, x28
   39d64:	mov	x2, x20
   39d68:	mov	w3, wzr
   39d6c:	mov	x4, x21
   39d70:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39d74:	ldp	x5, x3, [x29, #-16]
   39d78:	mov	w2, #0x7                   	// #7
   39d7c:	mov	w6, #0x3                   	// #3
   39d80:	mov	x0, x26
   39d84:	mov	x1, x25
   39d88:	mov	x4, x21
   39d8c:	mov	x7, x20
   39d90:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   39d94:	mov	w8, #0x60                  	// #96
   39d98:	cmp	x24, #0x208
   39d9c:	madd	x8, x21, x8, x19
   39da0:	stur	x22, [x29, #-24]
   39da4:	b.le	39df8 <__gmpn_toom8_sqr@@Base+0x524>
   39da8:	cmp	x24, #0x520
   39dac:	b.le	39e30 <__gmpn_toom8_sqr@@Base+0x55c>
   39db0:	cmp	x24, #0x6e0
   39db4:	b.le	39e68 <__gmpn_toom8_sqr@@Base+0x594>
   39db8:	add	x22, x8, #0x20
   39dbc:	cmp	x24, #0xa58
   39dc0:	mov	x0, x20
   39dc4:	mov	x1, x25
   39dc8:	mov	x2, x27
   39dcc:	mov	x3, x22
   39dd0:	b.le	39ea0 <__gmpn_toom8_sqr@@Base+0x5cc>
   39dd4:	bl	d690 <__gmpn_toom8_sqr@plt>
   39dd8:	mov	w8, #0x48                  	// #72
   39ddc:	madd	x8, x21, x8, x19
   39de0:	add	x0, x8, #0x18
   39de4:	mov	x1, x26
   39de8:	mov	x2, x27
   39dec:	mov	x3, x22
   39df0:	bl	d690 <__gmpn_toom8_sqr@plt>
   39df4:	b	39ec0 <__gmpn_toom8_sqr@@Base+0x5ec>
   39df8:	add	x22, x8, #0x20
   39dfc:	mov	x0, x20
   39e00:	mov	x1, x25
   39e04:	mov	x2, x27
   39e08:	mov	x3, x22
   39e0c:	bl	c190 <__gmpn_toom2_sqr@plt>
   39e10:	mov	w8, #0x48                  	// #72
   39e14:	madd	x8, x21, x8, x19
   39e18:	add	x0, x8, #0x18
   39e1c:	mov	x1, x26
   39e20:	mov	x2, x27
   39e24:	mov	x3, x22
   39e28:	bl	c190 <__gmpn_toom2_sqr@plt>
   39e2c:	b	39ec0 <__gmpn_toom8_sqr@@Base+0x5ec>
   39e30:	add	x22, x8, #0x20
   39e34:	mov	x0, x20
   39e38:	mov	x1, x25
   39e3c:	mov	x2, x27
   39e40:	mov	x3, x22
   39e44:	bl	d480 <__gmpn_toom3_sqr@plt>
   39e48:	mov	w8, #0x48                  	// #72
   39e4c:	madd	x8, x21, x8, x19
   39e50:	add	x0, x8, #0x18
   39e54:	mov	x1, x26
   39e58:	mov	x2, x27
   39e5c:	mov	x3, x22
   39e60:	bl	d480 <__gmpn_toom3_sqr@plt>
   39e64:	b	39ec0 <__gmpn_toom8_sqr@@Base+0x5ec>
   39e68:	add	x22, x8, #0x20
   39e6c:	mov	x0, x20
   39e70:	mov	x1, x25
   39e74:	mov	x2, x27
   39e78:	mov	x3, x22
   39e7c:	bl	c370 <__gmpn_toom4_sqr@plt>
   39e80:	mov	w8, #0x48                  	// #72
   39e84:	madd	x8, x21, x8, x19
   39e88:	add	x0, x8, #0x18
   39e8c:	mov	x1, x26
   39e90:	mov	x2, x27
   39e94:	mov	x3, x22
   39e98:	bl	c370 <__gmpn_toom4_sqr@plt>
   39e9c:	b	39ec0 <__gmpn_toom8_sqr@@Base+0x5ec>
   39ea0:	bl	d650 <__gmpn_toom6_sqr@plt>
   39ea4:	mov	w8, #0x48                  	// #72
   39ea8:	madd	x8, x21, x8, x19
   39eac:	add	x0, x8, #0x18
   39eb0:	mov	x1, x26
   39eb4:	mov	x2, x27
   39eb8:	mov	x3, x22
   39ebc:	bl	d650 <__gmpn_toom6_sqr@plt>
   39ec0:	mov	w8, #0x48                  	// #72
   39ec4:	madd	x8, x21, x8, x19
   39ec8:	add	x22, x8, #0x18
   39ecc:	mov	w5, #0x3                   	// #3
   39ed0:	mov	w6, #0x6                   	// #6
   39ed4:	mov	x0, x22
   39ed8:	mov	x1, x28
   39edc:	mov	x2, x20
   39ee0:	mov	w3, wzr
   39ee4:	mov	x4, x21
   39ee8:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   39eec:	ldp	x5, x3, [x29, #-16]
   39ef0:	mov	w2, #0x7                   	// #7
   39ef4:	mov	w6, #0x1                   	// #1
   39ef8:	mov	x0, x26
   39efc:	mov	x1, x25
   39f00:	mov	x4, x21
   39f04:	mov	x7, x20
   39f08:	bl	d2b0 <__gmpn_toom_eval_pm2rexp@plt>
   39f0c:	mov	w8, #0x60                  	// #96
   39f10:	cmp	x24, #0x208
   39f14:	madd	x8, x21, x8, x19
   39f18:	str	x22, [sp, #32]
   39f1c:	b.le	39f68 <__gmpn_toom8_sqr@@Base+0x694>
   39f20:	cmp	x24, #0x520
   39f24:	b.le	39f98 <__gmpn_toom8_sqr@@Base+0x6c4>
   39f28:	cmp	x24, #0x6e0
   39f2c:	b.le	39fc8 <__gmpn_toom8_sqr@@Base+0x6f4>
   39f30:	add	x22, x8, #0x20
   39f34:	cmp	x24, #0xa58
   39f38:	mov	x0, x20
   39f3c:	mov	x1, x25
   39f40:	mov	x2, x27
   39f44:	mov	x3, x22
   39f48:	b.le	39ff8 <__gmpn_toom8_sqr@@Base+0x724>
   39f4c:	bl	d690 <__gmpn_toom8_sqr@plt>
   39f50:	add	x0, x20, x23, lsl #3
   39f54:	mov	x1, x26
   39f58:	mov	x2, x27
   39f5c:	mov	x3, x22
   39f60:	bl	d690 <__gmpn_toom8_sqr@plt>
   39f64:	b	3a010 <__gmpn_toom8_sqr@@Base+0x73c>
   39f68:	add	x22, x8, #0x20
   39f6c:	mov	x0, x20
   39f70:	mov	x1, x25
   39f74:	mov	x2, x27
   39f78:	mov	x3, x22
   39f7c:	bl	c190 <__gmpn_toom2_sqr@plt>
   39f80:	add	x0, x20, x23, lsl #3
   39f84:	mov	x1, x26
   39f88:	mov	x2, x27
   39f8c:	mov	x3, x22
   39f90:	bl	c190 <__gmpn_toom2_sqr@plt>
   39f94:	b	3a010 <__gmpn_toom8_sqr@@Base+0x73c>
   39f98:	add	x22, x8, #0x20
   39f9c:	mov	x0, x20
   39fa0:	mov	x1, x25
   39fa4:	mov	x2, x27
   39fa8:	mov	x3, x22
   39fac:	bl	d480 <__gmpn_toom3_sqr@plt>
   39fb0:	add	x0, x20, x23, lsl #3
   39fb4:	mov	x1, x26
   39fb8:	mov	x2, x27
   39fbc:	mov	x3, x22
   39fc0:	bl	d480 <__gmpn_toom3_sqr@plt>
   39fc4:	b	3a010 <__gmpn_toom8_sqr@@Base+0x73c>
   39fc8:	add	x22, x8, #0x20
   39fcc:	mov	x0, x20
   39fd0:	mov	x1, x25
   39fd4:	mov	x2, x27
   39fd8:	mov	x3, x22
   39fdc:	bl	c370 <__gmpn_toom4_sqr@plt>
   39fe0:	add	x0, x20, x23, lsl #3
   39fe4:	mov	x1, x26
   39fe8:	mov	x2, x27
   39fec:	mov	x3, x22
   39ff0:	bl	c370 <__gmpn_toom4_sqr@plt>
   39ff4:	b	3a010 <__gmpn_toom8_sqr@@Base+0x73c>
   39ff8:	bl	d650 <__gmpn_toom6_sqr@plt>
   39ffc:	add	x0, x20, x23, lsl #3
   3a000:	mov	x1, x26
   3a004:	mov	x2, x27
   3a008:	mov	x3, x22
   3a00c:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a010:	add	x0, x20, x23, lsl #3
   3a014:	mov	w5, #0x1                   	// #1
   3a018:	mov	x1, x28
   3a01c:	mov	x2, x20
   3a020:	mov	w3, wzr
   3a024:	mov	x4, x21
   3a028:	mov	w6, wzr
   3a02c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   3a030:	ldp	x23, x3, [x29, #-16]
   3a034:	mov	w2, #0x7                   	// #7
   3a038:	mov	x0, x26
   3a03c:	mov	x1, x25
   3a040:	mov	x4, x21
   3a044:	mov	x5, x23
   3a048:	mov	x6, x20
   3a04c:	bl	d190 <__gmpn_toom_eval_pm1@plt>
   3a050:	mov	w8, #0x60                  	// #96
   3a054:	madd	x8, x21, x8, x19
   3a058:	cmp	x24, #0x208
   3a05c:	b.le	3a0b0 <__gmpn_toom8_sqr@@Base+0x7dc>
   3a060:	cmp	x24, #0x520
   3a064:	b.le	3a0e8 <__gmpn_toom8_sqr@@Base+0x814>
   3a068:	cmp	x24, #0x6e0
   3a06c:	b.le	3a120 <__gmpn_toom8_sqr@@Base+0x84c>
   3a070:	ldr	x22, [sp, #24]
   3a074:	add	x8, x8, #0x20
   3a078:	cmp	x24, #0xa58
   3a07c:	mov	x0, x20
   3a080:	mov	x1, x25
   3a084:	mov	x2, x27
   3a088:	mov	x3, x8
   3a08c:	str	x8, [sp, #8]
   3a090:	b.le	3a158 <__gmpn_toom8_sqr@@Base+0x884>
   3a094:	bl	d690 <__gmpn_toom8_sqr@plt>
   3a098:	ldr	x3, [sp, #8]
   3a09c:	add	x0, x20, x22, lsl #3
   3a0a0:	mov	x1, x26
   3a0a4:	mov	x2, x27
   3a0a8:	bl	d690 <__gmpn_toom8_sqr@plt>
   3a0ac:	b	3a170 <__gmpn_toom8_sqr@@Base+0x89c>
   3a0b0:	add	x22, x8, #0x20
   3a0b4:	mov	x0, x20
   3a0b8:	mov	x1, x25
   3a0bc:	mov	x2, x27
   3a0c0:	mov	x3, x22
   3a0c4:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a0c8:	ldr	x8, [sp, #24]
   3a0cc:	mov	x1, x26
   3a0d0:	mov	x2, x27
   3a0d4:	mov	x3, x22
   3a0d8:	add	x0, x20, x8, lsl #3
   3a0dc:	mov	x22, x8
   3a0e0:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a0e4:	b	3a170 <__gmpn_toom8_sqr@@Base+0x89c>
   3a0e8:	add	x22, x8, #0x20
   3a0ec:	mov	x0, x20
   3a0f0:	mov	x1, x25
   3a0f4:	mov	x2, x27
   3a0f8:	mov	x3, x22
   3a0fc:	bl	d480 <__gmpn_toom3_sqr@plt>
   3a100:	ldr	x8, [sp, #24]
   3a104:	mov	x1, x26
   3a108:	mov	x2, x27
   3a10c:	mov	x3, x22
   3a110:	add	x0, x20, x8, lsl #3
   3a114:	mov	x22, x8
   3a118:	bl	d480 <__gmpn_toom3_sqr@plt>
   3a11c:	b	3a170 <__gmpn_toom8_sqr@@Base+0x89c>
   3a120:	add	x22, x8, #0x20
   3a124:	mov	x0, x20
   3a128:	mov	x1, x25
   3a12c:	mov	x2, x27
   3a130:	mov	x3, x22
   3a134:	bl	c370 <__gmpn_toom4_sqr@plt>
   3a138:	ldr	x8, [sp, #24]
   3a13c:	mov	x1, x26
   3a140:	mov	x2, x27
   3a144:	mov	x3, x22
   3a148:	add	x0, x20, x8, lsl #3
   3a14c:	mov	x22, x8
   3a150:	bl	c370 <__gmpn_toom4_sqr@plt>
   3a154:	b	3a170 <__gmpn_toom8_sqr@@Base+0x89c>
   3a158:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a15c:	ldr	x3, [sp, #8]
   3a160:	add	x0, x20, x22, lsl #3
   3a164:	mov	x1, x26
   3a168:	mov	x2, x27
   3a16c:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a170:	add	x0, x20, x22, lsl #3
   3a174:	mov	x1, x28
   3a178:	mov	x2, x20
   3a17c:	mov	w3, wzr
   3a180:	mov	x4, x21
   3a184:	mov	w5, wzr
   3a188:	mov	w6, wzr
   3a18c:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   3a190:	ldur	x3, [x29, #-8]
   3a194:	mov	w2, #0x7                   	// #7
   3a198:	mov	w6, #0x2                   	// #2
   3a19c:	mov	x0, x26
   3a1a0:	mov	x1, x25
   3a1a4:	mov	x4, x21
   3a1a8:	mov	x5, x23
   3a1ac:	mov	x7, x20
   3a1b0:	bl	c4f0 <__gmpn_toom_eval_pm2exp@plt>
   3a1b4:	mov	w8, #0x60                  	// #96
   3a1b8:	madd	x8, x21, x8, x19
   3a1bc:	cmp	x24, #0x208
   3a1c0:	b.le	3a214 <__gmpn_toom8_sqr@@Base+0x940>
   3a1c4:	cmp	x24, #0x520
   3a1c8:	b.le	3a248 <__gmpn_toom8_sqr@@Base+0x974>
   3a1cc:	cmp	x24, #0x6e0
   3a1d0:	b.le	3a27c <__gmpn_toom8_sqr@@Base+0x9a8>
   3a1d4:	ldr	x22, [sp, #16]
   3a1d8:	add	x8, x8, #0x20
   3a1dc:	cmp	x24, #0xa58
   3a1e0:	mov	x0, x20
   3a1e4:	mov	x1, x25
   3a1e8:	mov	x2, x27
   3a1ec:	mov	x3, x8
   3a1f0:	str	x8, [sp, #24]
   3a1f4:	b.le	3a2b0 <__gmpn_toom8_sqr@@Base+0x9dc>
   3a1f8:	bl	d690 <__gmpn_toom8_sqr@plt>
   3a1fc:	ldr	x3, [sp, #24]
   3a200:	mov	x0, x25
   3a204:	mov	x1, x26
   3a208:	mov	x2, x27
   3a20c:	bl	d690 <__gmpn_toom8_sqr@plt>
   3a210:	b	3a2c8 <__gmpn_toom8_sqr@@Base+0x9f4>
   3a214:	add	x22, x8, #0x20
   3a218:	mov	x0, x20
   3a21c:	mov	x1, x25
   3a220:	mov	x2, x27
   3a224:	mov	x3, x22
   3a228:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a22c:	mov	x0, x25
   3a230:	mov	x1, x26
   3a234:	mov	x2, x27
   3a238:	mov	x3, x22
   3a23c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a240:	ldr	x22, [sp, #16]
   3a244:	b	3a2c8 <__gmpn_toom8_sqr@@Base+0x9f4>
   3a248:	add	x22, x8, #0x20
   3a24c:	mov	x0, x20
   3a250:	mov	x1, x25
   3a254:	mov	x2, x27
   3a258:	mov	x3, x22
   3a25c:	bl	d480 <__gmpn_toom3_sqr@plt>
   3a260:	mov	x0, x25
   3a264:	mov	x1, x26
   3a268:	mov	x2, x27
   3a26c:	mov	x3, x22
   3a270:	bl	d480 <__gmpn_toom3_sqr@plt>
   3a274:	ldr	x22, [sp, #16]
   3a278:	b	3a2c8 <__gmpn_toom8_sqr@@Base+0x9f4>
   3a27c:	add	x22, x8, #0x20
   3a280:	mov	x0, x20
   3a284:	mov	x1, x25
   3a288:	mov	x2, x27
   3a28c:	mov	x3, x22
   3a290:	bl	c370 <__gmpn_toom4_sqr@plt>
   3a294:	mov	x0, x25
   3a298:	mov	x1, x26
   3a29c:	mov	x2, x27
   3a2a0:	mov	x3, x22
   3a2a4:	bl	c370 <__gmpn_toom4_sqr@plt>
   3a2a8:	ldr	x22, [sp, #16]
   3a2ac:	b	3a2c8 <__gmpn_toom8_sqr@@Base+0x9f4>
   3a2b0:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a2b4:	ldr	x3, [sp, #24]
   3a2b8:	mov	x0, x25
   3a2bc:	mov	x1, x26
   3a2c0:	mov	x2, x27
   3a2c4:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a2c8:	mov	w5, #0x2                   	// #2
   3a2cc:	mov	w6, #0x4                   	// #4
   3a2d0:	mov	x0, x25
   3a2d4:	mov	x1, x28
   3a2d8:	mov	x2, x20
   3a2dc:	mov	w3, wzr
   3a2e0:	mov	x4, x21
   3a2e4:	bl	cb30 <__gmpn_toom_couple_handling@plt>
   3a2e8:	mov	w8, #0x60                  	// #96
   3a2ec:	madd	x8, x21, x8, x19
   3a2f0:	cmp	x24, #0x210
   3a2f4:	b.le	3a328 <__gmpn_toom8_sqr@@Base+0xa54>
   3a2f8:	cmp	x24, #0x528
   3a2fc:	b.le	3a340 <__gmpn_toom8_sqr@@Base+0xa6c>
   3a300:	cmp	x24, #0x6e8
   3a304:	b.le	3a358 <__gmpn_toom8_sqr@@Base+0xa84>
   3a308:	cmp	x24, #0xa60
   3a30c:	add	x3, x8, #0x20
   3a310:	mov	x0, x20
   3a314:	b.le	3a370 <__gmpn_toom8_sqr@@Base+0xa9c>
   3a318:	ldur	x1, [x29, #-8]
   3a31c:	mov	x2, x21
   3a320:	bl	d690 <__gmpn_toom8_sqr@plt>
   3a324:	b	3a37c <__gmpn_toom8_sqr@@Base+0xaa8>
   3a328:	ldur	x1, [x29, #-8]
   3a32c:	add	x3, x8, #0x20
   3a330:	mov	x0, x20
   3a334:	mov	x2, x21
   3a338:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a33c:	b	3a37c <__gmpn_toom8_sqr@@Base+0xaa8>
   3a340:	ldur	x1, [x29, #-8]
   3a344:	add	x3, x8, #0x20
   3a348:	mov	x0, x20
   3a34c:	mov	x2, x21
   3a350:	bl	d480 <__gmpn_toom3_sqr@plt>
   3a354:	b	3a37c <__gmpn_toom8_sqr@@Base+0xaa8>
   3a358:	ldur	x1, [x29, #-8]
   3a35c:	add	x3, x8, #0x20
   3a360:	mov	x0, x20
   3a364:	mov	x2, x21
   3a368:	bl	c370 <__gmpn_toom4_sqr@plt>
   3a36c:	b	3a37c <__gmpn_toom8_sqr@@Base+0xaa8>
   3a370:	ldur	x1, [x29, #-8]
   3a374:	mov	x2, x21
   3a378:	bl	d650 <__gmpn_toom6_sqr@plt>
   3a37c:	ldr	x1, [sp, #32]
   3a380:	ldur	x2, [x29, #-24]
   3a384:	mov	w8, #0x60                  	// #96
   3a388:	madd	x8, x21, x8, x19
   3a38c:	lsl	x6, x23, #1
   3a390:	add	x8, x8, #0x20
   3a394:	mov	x0, x20
   3a398:	mov	x3, x22
   3a39c:	mov	x4, x19
   3a3a0:	mov	x5, x21
   3a3a4:	mov	w7, wzr
   3a3a8:	str	x8, [sp]
   3a3ac:	bl	d530 <__gmpn_toom_interpolate_16pts@plt>
   3a3b0:	ldp	x20, x19, [sp, #144]
   3a3b4:	ldp	x22, x21, [sp, #128]
   3a3b8:	ldp	x24, x23, [sp, #112]
   3a3bc:	ldp	x26, x25, [sp, #96]
   3a3c0:	ldp	x28, x27, [sp, #80]
   3a3c4:	ldp	x29, x30, [sp, #64]
   3a3c8:	add	sp, sp, #0xa0
   3a3cc:	ret

000000000003a3d0 <__gmpn_toom_couple_handling@@Base>:
   3a3d0:	stp	x29, x30, [sp, #-64]!
   3a3d4:	stp	x22, x21, [sp, #32]
   3a3d8:	mov	x21, x0
   3a3dc:	stp	x24, x23, [sp, #16]
   3a3e0:	stp	x20, x19, [sp, #48]
   3a3e4:	mov	w23, w6
   3a3e8:	mov	w24, w5
   3a3ec:	mov	x19, x4
   3a3f0:	mov	x20, x2
   3a3f4:	mov	x22, x1
   3a3f8:	mov	x0, x2
   3a3fc:	mov	x1, x21
   3a400:	mov	x29, sp
   3a404:	cbz	w3, 3a414 <__gmpn_toom_couple_handling@@Base+0x44>
   3a408:	mov	x3, x22
   3a40c:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3a410:	b	3a41c <__gmpn_toom_couple_handling@@Base+0x4c>
   3a414:	mov	x3, x22
   3a418:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3a41c:	mov	x0, x21
   3a420:	mov	x1, x21
   3a424:	mov	x2, x20
   3a428:	mov	x3, x22
   3a42c:	cmp	w24, #0x1
   3a430:	b.ne	3a43c <__gmpn_toom_couple_handling@@Base+0x6c>  // b.any
   3a434:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3a438:	b	3a45c <__gmpn_toom_couple_handling@@Base+0x8c>
   3a43c:	bl	c420 <__gmpn_sub_n@plt>
   3a440:	cmp	w24, #0x1
   3a444:	b.lt	3a45c <__gmpn_toom_couple_handling@@Base+0x8c>  // b.tstop
   3a448:	mov	x0, x21
   3a44c:	mov	x1, x21
   3a450:	mov	x2, x22
   3a454:	mov	w3, w24
   3a458:	bl	c2f0 <__gmpn_rshift@plt>
   3a45c:	cmp	w23, #0x1
   3a460:	b.lt	3a478 <__gmpn_toom_couple_handling@@Base+0xa8>  // b.tstop
   3a464:	mov	x0, x20
   3a468:	mov	x1, x20
   3a46c:	mov	x2, x22
   3a470:	mov	w3, w23
   3a474:	bl	c2f0 <__gmpn_rshift@plt>
   3a478:	lsl	x23, x19, #3
   3a47c:	add	x0, x21, x23
   3a480:	sub	x3, x22, x19
   3a484:	mov	x1, x0
   3a488:	mov	x2, x20
   3a48c:	bl	cc30 <__gmpn_add_n@plt>
   3a490:	lsl	x8, x22, #3
   3a494:	mov	x3, x0
   3a498:	add	x0, x21, x8
   3a49c:	add	x8, x20, x8
   3a4a0:	sub	x1, x8, x23
   3a4a4:	mov	x2, x19
   3a4a8:	str	x3, [x0]
   3a4ac:	bl	c150 <__gmpn_add_1@plt>
   3a4b0:	ldp	x20, x19, [sp, #48]
   3a4b4:	ldp	x22, x21, [sp, #32]
   3a4b8:	ldp	x24, x23, [sp, #16]
   3a4bc:	ldp	x29, x30, [sp], #64
   3a4c0:	ret

000000000003a4c4 <__gmpn_toom2_sqr@@Base>:
   3a4c4:	sub	sp, sp, #0x70
   3a4c8:	stp	x20, x19, [sp, #96]
   3a4cc:	asr	x20, x2, #1
   3a4d0:	stp	x22, x21, [sp, #80]
   3a4d4:	sub	x22, x2, x20
   3a4d8:	stp	x26, x25, [sp, #48]
   3a4dc:	stp	x24, x23, [sp, #64]
   3a4e0:	mov	x26, x3
   3a4e4:	mov	x19, x2
   3a4e8:	mov	x24, x1
   3a4ec:	cmp	x20, x22
   3a4f0:	mov	x21, x0
   3a4f4:	stp	x29, x30, [sp, #16]
   3a4f8:	stp	x28, x27, [sp, #32]
   3a4fc:	add	x29, sp, #0x10
   3a500:	b.ne	3a52c <__gmpn_toom2_sqr@@Base+0x68>  // b.any
   3a504:	add	x25, x24, x22, lsl #3
   3a508:	mov	x0, x24
   3a50c:	mov	x1, x25
   3a510:	mov	x2, x22
   3a514:	bl	c570 <__gmpn_cmp@plt>
   3a518:	tbnz	w0, #31, 3a574 <__gmpn_toom2_sqr@@Base+0xb0>
   3a51c:	mov	x0, x21
   3a520:	mov	x1, x24
   3a524:	mov	x2, x25
   3a528:	b	3a580 <__gmpn_toom2_sqr@@Base+0xbc>
   3a52c:	ldr	x23, [x24, x20, lsl #3]
   3a530:	cbnz	x23, 3a54c <__gmpn_toom2_sqr@@Base+0x88>
   3a534:	add	x25, x24, x22, lsl #3
   3a538:	mov	x0, x24
   3a53c:	mov	x1, x25
   3a540:	mov	x2, x20
   3a544:	bl	c570 <__gmpn_cmp@plt>
   3a548:	tbnz	w0, #31, 3a5a8 <__gmpn_toom2_sqr@@Base+0xe4>
   3a54c:	add	x2, x24, x22, lsl #3
   3a550:	mov	x0, x21
   3a554:	mov	x1, x24
   3a558:	mov	x3, x20
   3a55c:	bl	c420 <__gmpn_sub_n@plt>
   3a560:	sub	x8, x23, x0
   3a564:	str	x8, [x21, x20, lsl #3]
   3a568:	cmp	x22, #0x11
   3a56c:	b.gt	3a590 <__gmpn_toom2_sqr@@Base+0xcc>
   3a570:	b	3a5c8 <__gmpn_toom2_sqr@@Base+0x104>
   3a574:	mov	x0, x21
   3a578:	mov	x1, x25
   3a57c:	mov	x2, x24
   3a580:	mov	x3, x22
   3a584:	bl	c420 <__gmpn_sub_n@plt>
   3a588:	cmp	x22, #0x11
   3a58c:	b.le	3a5c8 <__gmpn_toom2_sqr@@Base+0x104>
   3a590:	add	x3, x26, x22, lsl #4
   3a594:	mov	x0, x26
   3a598:	mov	x1, x21
   3a59c:	mov	x2, x22
   3a5a0:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a5a4:	b	3a5d8 <__gmpn_toom2_sqr@@Base+0x114>
   3a5a8:	mov	x0, x21
   3a5ac:	mov	x1, x25
   3a5b0:	mov	x2, x24
   3a5b4:	mov	x3, x20
   3a5b8:	bl	c420 <__gmpn_sub_n@plt>
   3a5bc:	str	xzr, [x21, x20, lsl #3]
   3a5c0:	cmp	x22, #0x11
   3a5c4:	b.gt	3a590 <__gmpn_toom2_sqr@@Base+0xcc>
   3a5c8:	mov	x0, x26
   3a5cc:	mov	x1, x21
   3a5d0:	mov	x2, x22
   3a5d4:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   3a5d8:	lsl	x23, x22, #1
   3a5dc:	add	x25, x21, x22, lsl #4
   3a5e0:	cmp	x19, #0x23
   3a5e4:	add	x1, x24, x22, lsl #3
   3a5e8:	b.le	3a600 <__gmpn_toom2_sqr@@Base+0x13c>
   3a5ec:	add	x3, x26, x23, lsl #3
   3a5f0:	mov	x0, x25
   3a5f4:	mov	x2, x20
   3a5f8:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a5fc:	b	3a60c <__gmpn_toom2_sqr@@Base+0x148>
   3a600:	mov	x0, x25
   3a604:	mov	x2, x20
   3a608:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   3a60c:	cmp	x22, #0x11
   3a610:	stp	x20, x23, [sp]
   3a614:	b.le	3a630 <__gmpn_toom2_sqr@@Base+0x16c>
   3a618:	add	x3, x26, x23, lsl #3
   3a61c:	mov	x0, x21
   3a620:	mov	x1, x24
   3a624:	mov	x2, x22
   3a628:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a62c:	b	3a640 <__gmpn_toom2_sqr@@Base+0x17c>
   3a630:	mov	x0, x21
   3a634:	mov	x1, x24
   3a638:	mov	x2, x22
   3a63c:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   3a640:	lsl	x24, x22, #3
   3a644:	add	x23, x21, x24
   3a648:	mov	x0, x25
   3a64c:	mov	x1, x23
   3a650:	mov	x2, x25
   3a654:	mov	x3, x22
   3a658:	bl	cc30 <__gmpn_add_n@plt>
   3a65c:	mov	x27, x0
   3a660:	mov	x0, x23
   3a664:	mov	x1, x25
   3a668:	mov	x2, x21
   3a66c:	mov	x3, x22
   3a670:	bl	cc30 <__gmpn_add_n@plt>
   3a674:	and	x8, x19, #0xfffffffffffffffe
   3a678:	mov	x28, x0
   3a67c:	add	x3, x25, x24
   3a680:	sub	x4, x8, x22
   3a684:	mov	x0, x25
   3a688:	mov	x1, x25
   3a68c:	mov	x2, x22
   3a690:	mov	x20, x19
   3a694:	bl	c970 <__gmpn_add@plt>
   3a698:	ldr	x3, [sp, #8]
   3a69c:	add	x19, x0, x27
   3a6a0:	mov	x0, x23
   3a6a4:	mov	x1, x23
   3a6a8:	mov	x2, x26
   3a6ac:	bl	c420 <__gmpn_sub_n@plt>
   3a6b0:	sub	x8, x19, x0
   3a6b4:	cmp	x8, #0x3
   3a6b8:	b.cs	3a750 <__gmpn_toom2_sqr@@Base+0x28c>  // b.hs, b.nlast
   3a6bc:	ldr	x9, [x25]
   3a6c0:	ldr	x12, [sp]
   3a6c4:	add	x10, x28, x27
   3a6c8:	adds	x9, x9, x10
   3a6cc:	str	x9, [x25]
   3a6d0:	b.cc	3a6f4 <__gmpn_toom2_sqr@@Base+0x230>  // b.lo, b.ul, b.last
   3a6d4:	lsl	x9, x20, #1
   3a6d8:	sub	x9, x9, x12, lsl #1
   3a6dc:	add	x9, x21, x9, lsl #3
   3a6e0:	add	x9, x9, #0x8
   3a6e4:	ldr	x10, [x9]
   3a6e8:	adds	x10, x10, #0x1
   3a6ec:	str	x10, [x9], #8
   3a6f0:	b.cs	3a6e4 <__gmpn_toom2_sqr@@Base+0x220>  // b.hs, b.nlast
   3a6f4:	mov	w9, #0x18                  	// #24
   3a6f8:	mul	x9, x22, x9
   3a6fc:	ldr	x10, [x21, x9]
   3a700:	adds	x8, x10, x8
   3a704:	str	x8, [x21, x9]
   3a708:	b.cc	3a730 <__gmpn_toom2_sqr@@Base+0x26c>  // b.lo, b.ul, b.last
   3a70c:	add	x8, x20, x20, lsl #1
   3a710:	add	x9, x12, x12, lsl #1
   3a714:	sub	x8, x8, x9
   3a718:	add	x8, x21, x8, lsl #3
   3a71c:	add	x8, x8, #0x8
   3a720:	ldr	x9, [x8]
   3a724:	adds	x9, x9, #0x1
   3a728:	str	x9, [x8], #8
   3a72c:	b.cs	3a720 <__gmpn_toom2_sqr@@Base+0x25c>  // b.hs, b.nlast
   3a730:	ldp	x20, x19, [sp, #96]
   3a734:	ldp	x22, x21, [sp, #80]
   3a738:	ldp	x24, x23, [sp, #64]
   3a73c:	ldp	x26, x25, [sp, #48]
   3a740:	ldp	x28, x27, [sp, #32]
   3a744:	ldp	x29, x30, [sp, #16]
   3a748:	add	sp, sp, #0x70
   3a74c:	ret
   3a750:	mov	x0, x25
   3a754:	mov	w1, wzr
   3a758:	mov	x2, x24
   3a75c:	bl	c780 <memset@plt>
   3a760:	b	3a730 <__gmpn_toom2_sqr@@Base+0x26c>

000000000003a764 <__gmpn_toom3_sqr@@Base>:
   3a764:	sub	sp, sp, #0x90
   3a768:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3a76c:	add	x8, x2, #0x2
   3a770:	movk	x9, #0xaaab
   3a774:	umulh	x8, x8, x9
   3a778:	stp	x22, x21, [sp, #112]
   3a77c:	lsr	x21, x8, #1
   3a780:	stp	x29, x30, [sp, #48]
   3a784:	stp	x20, x19, [sp, #128]
   3a788:	add	x29, sp, #0x30
   3a78c:	mov	x19, x3
   3a790:	lsl	x10, x21, #1
   3a794:	add	x8, x3, x21, lsl #5
   3a798:	lsl	x9, x21, #4
   3a79c:	lsl	x20, x21, #3
   3a7a0:	stp	x24, x23, [sp, #96]
   3a7a4:	str	x10, [sp, #24]
   3a7a8:	sub	x4, x2, x10
   3a7ac:	add	x24, x8, #0x20
   3a7b0:	add	x10, x3, x9
   3a7b4:	stur	x0, [x29, #-16]
   3a7b8:	add	x8, x0, x20
   3a7bc:	add	x3, x1, x9
   3a7c0:	mov	x0, x19
   3a7c4:	mov	x2, x21
   3a7c8:	stp	x28, x27, [sp, #64]
   3a7cc:	stp	x26, x25, [sp, #80]
   3a7d0:	mov	x23, x1
   3a7d4:	str	x10, [sp, #16]
   3a7d8:	add	x27, x10, #0x10
   3a7dc:	add	x26, x8, #0x8
   3a7e0:	stur	x3, [x29, #-8]
   3a7e4:	mov	x22, x4
   3a7e8:	bl	c970 <__gmpn_add@plt>
   3a7ec:	add	x25, x23, x20
   3a7f0:	mov	x28, x0
   3a7f4:	mov	x0, x24
   3a7f8:	mov	x1, x19
   3a7fc:	mov	x2, x25
   3a800:	mov	x3, x21
   3a804:	bl	cc30 <__gmpn_add_n@plt>
   3a808:	add	x8, x0, x28
   3a80c:	str	x8, [x24, x20]
   3a810:	cbnz	x28, 3a828 <__gmpn_toom3_sqr@@Base+0xc4>
   3a814:	mov	x0, x19
   3a818:	mov	x1, x25
   3a81c:	mov	x2, x21
   3a820:	bl	c570 <__gmpn_cmp@plt>
   3a824:	tbnz	w0, #31, 3a990 <__gmpn_toom3_sqr@@Base+0x22c>
   3a828:	mov	x0, x27
   3a82c:	mov	x1, x19
   3a830:	mov	x2, x25
   3a834:	mov	x3, x21
   3a838:	bl	c420 <__gmpn_sub_n@plt>
   3a83c:	sub	x8, x28, x0
   3a840:	ldur	x1, [x29, #-8]
   3a844:	lsl	x9, x21, #2
   3a848:	mov	x0, x26
   3a84c:	mov	x2, x24
   3a850:	mov	x3, x22
   3a854:	str	x9, [sp, #8]
   3a858:	str	x8, [x27, x21, lsl #3]
   3a85c:	bl	cc30 <__gmpn_add_n@plt>
   3a860:	subs	x2, x21, x22
   3a864:	mov	x3, x0
   3a868:	str	x22, [sp]
   3a86c:	b.eq	3a884 <__gmpn_toom3_sqr@@Base+0x120>  // b.none
   3a870:	lsl	x8, x22, #3
   3a874:	add	x0, x26, x8
   3a878:	add	x1, x24, x8
   3a87c:	bl	c150 <__gmpn_add_1@plt>
   3a880:	mov	x3, x0
   3a884:	ldr	x8, [x24, x20]
   3a888:	mov	x0, x26
   3a88c:	mov	x1, x23
   3a890:	mov	x2, x26
   3a894:	add	x25, x8, x3
   3a898:	mov	x3, x21
   3a89c:	bl	d260 <__gmpn_rsblsh1_n@plt>
   3a8a0:	add	x8, x0, x25, lsl #1
   3a8a4:	mov	w9, #0x28                  	// #40
   3a8a8:	str	x8, [x26, x20]
   3a8ac:	madd	x8, x21, x9, x19
   3a8b0:	add	x25, x21, #0x1
   3a8b4:	add	x28, x8, #0x28
   3a8b8:	mov	x0, x19
   3a8bc:	mov	x1, x27
   3a8c0:	mov	x2, x25
   3a8c4:	mov	x3, x28
   3a8c8:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a8cc:	ldr	x8, [sp, #16]
   3a8d0:	mov	x1, x26
   3a8d4:	mov	x2, x25
   3a8d8:	mov	x3, x28
   3a8dc:	add	x27, x8, #0x8
   3a8e0:	mov	x0, x27
   3a8e4:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a8e8:	ldp	x22, x1, [x29, #-16]
   3a8ec:	ldr	x8, [sp, #8]
   3a8f0:	stur	x19, [x29, #-8]
   3a8f4:	mov	x19, x24
   3a8f8:	ldr	x24, [sp]
   3a8fc:	add	x26, x22, x8, lsl #3
   3a900:	mov	x0, x26
   3a904:	mov	x3, x28
   3a908:	mov	x2, x24
   3a90c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a910:	ldr	x8, [x26]
   3a914:	mov	x20, x23
   3a918:	ldr	x23, [x26, #8]
   3a91c:	mov	x1, x19
   3a920:	str	x8, [sp, #16]
   3a924:	ldr	x8, [sp, #24]
   3a928:	mov	x2, x25
   3a92c:	mov	x3, x28
   3a930:	add	x0, x22, x8, lsl #3
   3a934:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a938:	mov	x0, x22
   3a93c:	mov	x1, x20
   3a940:	mov	x2, x21
   3a944:	mov	x3, x28
   3a948:	str	x23, [x26, #8]
   3a94c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3a950:	ldur	x2, [x29, #-8]
   3a954:	ldr	x6, [sp, #16]
   3a958:	lsl	x4, x24, #1
   3a95c:	mov	x0, x22
   3a960:	mov	x1, x27
   3a964:	mov	x3, x21
   3a968:	mov	w5, wzr
   3a96c:	bl	cbe0 <__gmpn_toom_interpolate_5pts@plt>
   3a970:	ldp	x20, x19, [sp, #128]
   3a974:	ldp	x22, x21, [sp, #112]
   3a978:	ldp	x24, x23, [sp, #96]
   3a97c:	ldp	x26, x25, [sp, #80]
   3a980:	ldp	x28, x27, [sp, #64]
   3a984:	ldp	x29, x30, [sp, #48]
   3a988:	add	sp, sp, #0x90
   3a98c:	ret
   3a990:	mov	x0, x27
   3a994:	mov	x1, x25
   3a998:	mov	x2, x19
   3a99c:	mov	x3, x21
   3a9a0:	bl	c420 <__gmpn_sub_n@plt>
   3a9a4:	mov	x8, xzr
   3a9a8:	b	3a840 <__gmpn_toom3_sqr@@Base+0xdc>

000000000003a9ac <__gmpn_toom4_sqr@@Base>:
   3a9ac:	sub	sp, sp, #0xa0
   3a9b0:	add	x8, x2, #0x3
   3a9b4:	stp	x22, x21, [sp, #128]
   3a9b8:	asr	x21, x8, #2
   3a9bc:	and	x8, x8, #0xfffffffffffffffc
   3a9c0:	add	x10, x21, x21, lsl #1
   3a9c4:	str	x8, [sp, #16]
   3a9c8:	add	x8, x0, x8, lsl #3
   3a9cc:	add	x9, x3, x21, lsl #6
   3a9d0:	stp	x26, x25, [sp, #96]
   3a9d4:	stp	x24, x23, [sp, #112]
   3a9d8:	mov	x24, x1
   3a9dc:	sub	x23, x2, x10
   3a9e0:	add	x25, x8, #0x10
   3a9e4:	add	x22, x9, #0x28
   3a9e8:	stp	x29, x30, [sp, #64]
   3a9ec:	stp	x20, x19, [sp, #144]
   3a9f0:	add	x29, sp, #0x40
   3a9f4:	mov	x19, x3
   3a9f8:	mov	x26, x2
   3a9fc:	mov	x1, x25
   3aa00:	mov	x2, x24
   3aa04:	mov	x3, x21
   3aa08:	mov	x4, x23
   3aa0c:	mov	x5, x22
   3aa10:	stp	x28, x27, [sp, #80]
   3aa14:	mov	x20, x0
   3aa18:	stur	x10, [x29, #-16]
   3aa1c:	bl	cf30 <__gmpn_toom_eval_dgr3_pm2@plt>
   3aa20:	add	x28, x21, #0x1
   3aa24:	cmp	x26, #0x104
   3aa28:	mov	x0, x19
   3aa2c:	mov	x1, x20
   3aa30:	mov	x2, x28
   3aa34:	mov	x3, x22
   3aa38:	b.le	3aa44 <__gmpn_toom4_sqr@@Base+0x98>
   3aa3c:	bl	d480 <__gmpn_toom3_sqr@plt>
   3aa40:	b	3aa48 <__gmpn_toom4_sqr@@Base+0x9c>
   3aa44:	bl	c190 <__gmpn_toom2_sqr@plt>
   3aa48:	lsl	x8, x21, #1
   3aa4c:	stur	x8, [x29, #-8]
   3aa50:	add	x8, x19, x21, lsl #4
   3aa54:	cmp	x26, #0x104
   3aa58:	add	x0, x8, #0x8
   3aa5c:	mov	x1, x25
   3aa60:	mov	x2, x28
   3aa64:	mov	x3, x22
   3aa68:	stur	x25, [x29, #-24]
   3aa6c:	stp	x0, x19, [sp, #24]
   3aa70:	b.le	3aa7c <__gmpn_toom4_sqr@@Base+0xd0>
   3aa74:	bl	d480 <__gmpn_toom3_sqr@plt>
   3aa78:	b	3aa80 <__gmpn_toom4_sqr@@Base+0xd4>
   3aa7c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3aa80:	add	x1, x24, x21, lsl #3
   3aa84:	mov	x0, x20
   3aa88:	mov	x2, x24
   3aa8c:	mov	x3, x21
   3aa90:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3aa94:	ldur	x8, [x29, #-8]
   3aa98:	mov	x27, x0
   3aa9c:	mov	x0, x20
   3aaa0:	mov	x2, x20
   3aaa4:	add	x1, x24, x8, lsl #3
   3aaa8:	mov	x3, x21
   3aaac:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3aab0:	subs	x25, x21, x23
   3aab4:	add	x19, x0, x27, lsl #1
   3aab8:	b.le	3ab28 <__gmpn_toom4_sqr@@Base+0x17c>
   3aabc:	ldur	x8, [x29, #-16]
   3aac0:	mov	x0, x20
   3aac4:	mov	x2, x20
   3aac8:	mov	x3, x23
   3aacc:	add	x1, x24, x8, lsl #3
   3aad0:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3aad4:	add	x27, x20, x23, lsl #3
   3aad8:	str	x0, [sp, #8]
   3aadc:	mov	w3, #0x1                   	// #1
   3aae0:	mov	x0, x27
   3aae4:	mov	x1, x27
   3aae8:	mov	x2, x25
   3aaec:	bl	c2d0 <__gmpn_lshift@plt>
   3aaf0:	add	x8, x0, x19, lsl #1
   3aaf4:	str	x8, [x20, x21, lsl #3]
   3aaf8:	ldr	x8, [x27]
   3aafc:	ldp	x9, x10, [sp, #8]
   3ab00:	ldur	x19, [x29, #-24]
   3ab04:	adds	x8, x8, x9
   3ab08:	str	x8, [x27]
   3ab0c:	b.cc	3ab50 <__gmpn_toom4_sqr@@Base+0x1a4>  // b.lo, b.ul, b.last
   3ab10:	add	x8, x27, #0x8
   3ab14:	ldr	x9, [x8]
   3ab18:	adds	x9, x9, #0x1
   3ab1c:	str	x9, [x8], #8
   3ab20:	b.cs	3ab14 <__gmpn_toom4_sqr@@Base+0x168>  // b.hs, b.nlast
   3ab24:	b	3ab50 <__gmpn_toom4_sqr@@Base+0x1a4>
   3ab28:	ldur	x8, [x29, #-16]
   3ab2c:	mov	x0, x20
   3ab30:	mov	x2, x20
   3ab34:	mov	x3, x21
   3ab38:	add	x1, x24, x8, lsl #3
   3ab3c:	bl	ce00 <__gmpn_addlsh1_n@plt>
   3ab40:	add	x8, x0, x19, lsl #1
   3ab44:	ldur	x19, [x29, #-24]
   3ab48:	ldr	x10, [sp, #16]
   3ab4c:	str	x8, [x20, x21, lsl #3]
   3ab50:	ldr	x27, [sp, #32]
   3ab54:	cmp	x26, #0x104
   3ab58:	mov	x1, x20
   3ab5c:	mov	x2, x28
   3ab60:	add	x8, x27, x10, lsl #3
   3ab64:	add	x0, x8, #0x10
   3ab68:	mov	x3, x22
   3ab6c:	str	x0, [sp, #16]
   3ab70:	b.le	3ab7c <__gmpn_toom4_sqr@@Base+0x1d0>
   3ab74:	bl	d480 <__gmpn_toom3_sqr@plt>
   3ab78:	b	3ab80 <__gmpn_toom4_sqr@@Base+0x1d4>
   3ab7c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3ab80:	mov	x0, x20
   3ab84:	mov	x1, x19
   3ab88:	mov	x2, x24
   3ab8c:	mov	x3, x21
   3ab90:	mov	x4, x23
   3ab94:	mov	x5, x22
   3ab98:	bl	c3d0 <__gmpn_toom_eval_dgr3_pm1@plt>
   3ab9c:	ldur	x8, [x29, #-8]
   3aba0:	cmp	x26, #0x104
   3aba4:	mov	x1, x20
   3aba8:	mov	x2, x28
   3abac:	add	x0, x20, x8, lsl #3
   3abb0:	mov	x3, x22
   3abb4:	b.le	3abc0 <__gmpn_toom4_sqr@@Base+0x214>
   3abb8:	bl	d480 <__gmpn_toom3_sqr@plt>
   3abbc:	b	3abc4 <__gmpn_toom4_sqr@@Base+0x218>
   3abc0:	bl	c190 <__gmpn_toom2_sqr@plt>
   3abc4:	add	x19, x21, x21, lsl #1
   3abc8:	add	x8, x27, x19, lsl #4
   3abcc:	add	x25, x8, #0x18
   3abd0:	cmp	x26, #0x104
   3abd4:	mov	x0, x25
   3abd8:	b.le	3abf0 <__gmpn_toom4_sqr@@Base+0x244>
   3abdc:	ldur	x1, [x29, #-24]
   3abe0:	mov	x2, x28
   3abe4:	mov	x3, x22
   3abe8:	bl	d480 <__gmpn_toom3_sqr@plt>
   3abec:	b	3ac00 <__gmpn_toom4_sqr@@Base+0x254>
   3abf0:	ldur	x1, [x29, #-24]
   3abf4:	mov	x2, x28
   3abf8:	mov	x3, x22
   3abfc:	bl	c190 <__gmpn_toom2_sqr@plt>
   3ac00:	cmp	x26, #0x108
   3ac04:	lsl	x19, x19, #1
   3ac08:	mov	x0, x20
   3ac0c:	mov	x1, x24
   3ac10:	mov	x2, x21
   3ac14:	mov	x3, x22
   3ac18:	b.le	3ac24 <__gmpn_toom4_sqr@@Base+0x278>
   3ac1c:	bl	d480 <__gmpn_toom3_sqr@plt>
   3ac20:	b	3ac28 <__gmpn_toom4_sqr@@Base+0x27c>
   3ac24:	bl	c190 <__gmpn_toom2_sqr@plt>
   3ac28:	ldur	x8, [x29, #-16]
   3ac2c:	add	x0, x20, x19, lsl #3
   3ac30:	cmp	x23, #0x42
   3ac34:	mov	x2, x23
   3ac38:	add	x1, x24, x8, lsl #3
   3ac3c:	mov	x3, x22
   3ac40:	b.le	3ac4c <__gmpn_toom4_sqr@@Base+0x2a0>
   3ac44:	bl	d480 <__gmpn_toom3_sqr@plt>
   3ac48:	b	3ac50 <__gmpn_toom4_sqr@@Base+0x2a4>
   3ac4c:	bl	c190 <__gmpn_toom2_sqr@plt>
   3ac50:	ldp	x6, x3, [sp, #16]
   3ac54:	lsl	x7, x23, #1
   3ac58:	mov	x0, x20
   3ac5c:	mov	x1, x21
   3ac60:	mov	w2, wzr
   3ac64:	mov	x4, x25
   3ac68:	mov	x5, x27
   3ac6c:	str	x22, [sp]
   3ac70:	bl	c9c0 <__gmpn_toom_interpolate_7pts@plt>
   3ac74:	ldp	x20, x19, [sp, #144]
   3ac78:	ldp	x22, x21, [sp, #128]
   3ac7c:	ldp	x24, x23, [sp, #112]
   3ac80:	ldp	x26, x25, [sp, #96]
   3ac84:	ldp	x28, x27, [sp, #80]
   3ac88:	ldp	x29, x30, [sp, #64]
   3ac8c:	add	sp, sp, #0xa0
   3ac90:	ret

000000000003ac94 <__gmpn_toom_eval_dgr3_pm1@@Base>:
   3ac94:	stp	x29, x30, [sp, #-80]!
   3ac98:	stp	x24, x23, [sp, #32]
   3ac9c:	mov	x24, x2
   3aca0:	stp	x22, x21, [sp, #48]
   3aca4:	mov	x21, x1
   3aca8:	add	x2, x2, x3, lsl #4
   3acac:	mov	x1, x24
   3acb0:	str	x25, [sp, #16]
   3acb4:	stp	x20, x19, [sp, #64]
   3acb8:	mov	x29, sp
   3acbc:	mov	x19, x5
   3acc0:	mov	x22, x4
   3acc4:	mov	x23, x3
   3acc8:	mov	x20, x0
   3accc:	bl	cc30 <__gmpn_add_n@plt>
   3acd0:	lsl	x25, x23, #3
   3acd4:	mov	w8, #0x18                  	// #24
   3acd8:	str	x0, [x20, x25]
   3acdc:	add	x1, x24, x25
   3ace0:	madd	x3, x23, x8, x24
   3ace4:	mov	x0, x19
   3ace8:	mov	x2, x23
   3acec:	mov	x4, x22
   3acf0:	bl	c970 <__gmpn_add@plt>
   3acf4:	add	x22, x23, #0x1
   3acf8:	str	x0, [x19, x25]
   3acfc:	mov	x0, x20
   3ad00:	mov	x1, x19
   3ad04:	mov	x2, x22
   3ad08:	bl	c570 <__gmpn_cmp@plt>
   3ad0c:	asr	w23, w0, #31
   3ad10:	tbnz	w0, #31, 3ad24 <__gmpn_toom_eval_dgr3_pm1@@Base+0x90>
   3ad14:	mov	x0, x21
   3ad18:	mov	x1, x20
   3ad1c:	mov	x2, x19
   3ad20:	b	3ad30 <__gmpn_toom_eval_dgr3_pm1@@Base+0x9c>
   3ad24:	mov	x0, x21
   3ad28:	mov	x1, x19
   3ad2c:	mov	x2, x20
   3ad30:	mov	x3, x22
   3ad34:	bl	c420 <__gmpn_sub_n@plt>
   3ad38:	mov	x0, x20
   3ad3c:	mov	x1, x20
   3ad40:	mov	x2, x19
   3ad44:	mov	x3, x22
   3ad48:	bl	cc30 <__gmpn_add_n@plt>
   3ad4c:	mov	w0, w23
   3ad50:	ldp	x20, x19, [sp, #64]
   3ad54:	ldp	x22, x21, [sp, #48]
   3ad58:	ldp	x24, x23, [sp, #32]
   3ad5c:	ldr	x25, [sp, #16]
   3ad60:	ldp	x29, x30, [sp], #80
   3ad64:	ret

000000000003ad68 <__gmpn_toom_eval_dgr3_pm2@@Base>:
   3ad68:	stp	x29, x30, [sp, #-80]!
   3ad6c:	str	x25, [sp, #16]
   3ad70:	mov	x25, x2
   3ad74:	stp	x22, x21, [sp, #48]
   3ad78:	mov	x21, x1
   3ad7c:	add	x2, x2, x3, lsl #4
   3ad80:	mov	x1, x25
   3ad84:	stp	x24, x23, [sp, #32]
   3ad88:	stp	x20, x19, [sp, #64]
   3ad8c:	mov	x29, sp
   3ad90:	mov	x19, x5
   3ad94:	mov	x23, x4
   3ad98:	mov	x22, x3
   3ad9c:	mov	x20, x0
   3ada0:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3ada4:	lsl	x8, x22, #3
   3ada8:	mov	w9, #0x18                  	// #24
   3adac:	add	x24, x25, x8
   3adb0:	str	x0, [x20, x8]
   3adb4:	madd	x2, x22, x9, x25
   3adb8:	mov	x0, x19
   3adbc:	mov	x1, x24
   3adc0:	mov	x3, x23
   3adc4:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3adc8:	subs	x2, x22, x23
   3adcc:	mov	x3, x0
   3add0:	b.le	3ade8 <__gmpn_toom_eval_dgr3_pm2@@Base+0x80>
   3add4:	lsl	x8, x23, #3
   3add8:	add	x0, x19, x8
   3addc:	add	x1, x24, x8
   3ade0:	bl	c150 <__gmpn_add_1@plt>
   3ade4:	mov	x3, x0
   3ade8:	str	x3, [x19, x22, lsl #3]
   3adec:	add	x22, x22, #0x1
   3adf0:	mov	w3, #0x1                   	// #1
   3adf4:	mov	x0, x19
   3adf8:	mov	x1, x19
   3adfc:	mov	x2, x22
   3ae00:	bl	c2d0 <__gmpn_lshift@plt>
   3ae04:	mov	x0, x20
   3ae08:	mov	x1, x19
   3ae0c:	mov	x2, x22
   3ae10:	bl	c570 <__gmpn_cmp@plt>
   3ae14:	asr	w23, w0, #31
   3ae18:	tbnz	w0, #31, 3ae2c <__gmpn_toom_eval_dgr3_pm2@@Base+0xc4>
   3ae1c:	mov	x0, x21
   3ae20:	mov	x1, x20
   3ae24:	mov	x2, x19
   3ae28:	b	3ae38 <__gmpn_toom_eval_dgr3_pm2@@Base+0xd0>
   3ae2c:	mov	x0, x21
   3ae30:	mov	x1, x19
   3ae34:	mov	x2, x20
   3ae38:	mov	x3, x22
   3ae3c:	bl	c420 <__gmpn_sub_n@plt>
   3ae40:	mov	x0, x20
   3ae44:	mov	x1, x20
   3ae48:	mov	x2, x19
   3ae4c:	mov	x3, x22
   3ae50:	bl	cc30 <__gmpn_add_n@plt>
   3ae54:	mov	w0, w23
   3ae58:	ldp	x20, x19, [sp, #64]
   3ae5c:	ldp	x22, x21, [sp, #48]
   3ae60:	ldp	x24, x23, [sp, #32]
   3ae64:	ldr	x25, [sp, #16]
   3ae68:	ldp	x29, x30, [sp], #80
   3ae6c:	ret

000000000003ae70 <__gmpn_toom_eval_pm1@@Base>:
   3ae70:	stp	x29, x30, [sp, #-96]!
   3ae74:	stp	x24, x23, [sp, #48]
   3ae78:	stp	x22, x21, [sp, #64]
   3ae7c:	mov	x24, x3
   3ae80:	mov	w23, w2
   3ae84:	mov	x21, x1
   3ae88:	add	x2, x3, x4, lsl #4
   3ae8c:	mov	x1, x3
   3ae90:	mov	x3, x4
   3ae94:	str	x27, [sp, #16]
   3ae98:	stp	x26, x25, [sp, #32]
   3ae9c:	stp	x20, x19, [sp, #80]
   3aea0:	mov	x29, sp
   3aea4:	mov	x19, x6
   3aea8:	mov	x22, x5
   3aeac:	mov	x25, x4
   3aeb0:	mov	x20, x0
   3aeb4:	bl	cc30 <__gmpn_add_n@plt>
   3aeb8:	cmp	w23, #0x5
   3aebc:	str	x0, [x20, x25, lsl #3]
   3aec0:	b.cc	3aef8 <__gmpn_toom_eval_pm1@@Base+0x88>  // b.lo, b.ul, b.last
   3aec4:	add	x26, x25, #0x1
   3aec8:	mov	w27, #0x4                   	// #4
   3aecc:	mov	w8, w27
   3aed0:	mul	x8, x8, x25
   3aed4:	add	x3, x24, x8, lsl #3
   3aed8:	mov	x0, x20
   3aedc:	mov	x1, x20
   3aee0:	mov	x2, x26
   3aee4:	mov	x4, x25
   3aee8:	bl	c970 <__gmpn_add@plt>
   3aeec:	add	w27, w27, #0x2
   3aef0:	cmp	w27, w23
   3aef4:	b.cc	3aecc <__gmpn_toom_eval_pm1@@Base+0x5c>  // b.lo, b.ul, b.last
   3aef8:	lsl	x26, x25, #3
   3aefc:	mov	w8, #0x18                  	// #24
   3af00:	add	x1, x24, x26
   3af04:	madd	x2, x25, x8, x24
   3af08:	mov	x0, x19
   3af0c:	mov	x3, x25
   3af10:	bl	cc30 <__gmpn_add_n@plt>
   3af14:	cmp	w23, #0x6
   3af18:	str	x0, [x19, x26]
   3af1c:	b.cc	3af54 <__gmpn_toom_eval_pm1@@Base+0xe4>  // b.lo, b.ul, b.last
   3af20:	add	x26, x25, #0x1
   3af24:	mov	w27, #0x5                   	// #5
   3af28:	mov	w8, w27
   3af2c:	mul	x8, x8, x25
   3af30:	add	x3, x24, x8, lsl #3
   3af34:	mov	x0, x19
   3af38:	mov	x1, x19
   3af3c:	mov	x2, x26
   3af40:	mov	x4, x25
   3af44:	bl	c970 <__gmpn_add@plt>
   3af48:	add	w27, w27, #0x2
   3af4c:	cmp	w27, w23
   3af50:	b.cc	3af28 <__gmpn_toom_eval_pm1@@Base+0xb8>  // b.lo, b.ul, b.last
   3af54:	mov	w8, w23
   3af58:	mul	x8, x8, x25
   3af5c:	add	x26, x25, #0x1
   3af60:	add	x3, x24, x8, lsl #3
   3af64:	tbnz	w23, #0, 3af74 <__gmpn_toom_eval_pm1@@Base+0x104>
   3af68:	mov	x0, x20
   3af6c:	mov	x1, x20
   3af70:	b	3af7c <__gmpn_toom_eval_pm1@@Base+0x10c>
   3af74:	mov	x0, x19
   3af78:	mov	x1, x19
   3af7c:	mov	x2, x26
   3af80:	mov	x4, x22
   3af84:	bl	c970 <__gmpn_add@plt>
   3af88:	mov	x0, x20
   3af8c:	mov	x1, x19
   3af90:	mov	x2, x26
   3af94:	bl	c570 <__gmpn_cmp@plt>
   3af98:	asr	w22, w0, #31
   3af9c:	tbnz	w0, #31, 3afb0 <__gmpn_toom_eval_pm1@@Base+0x140>
   3afa0:	mov	x0, x21
   3afa4:	mov	x1, x20
   3afa8:	mov	x2, x19
   3afac:	b	3afbc <__gmpn_toom_eval_pm1@@Base+0x14c>
   3afb0:	mov	x0, x21
   3afb4:	mov	x1, x19
   3afb8:	mov	x2, x20
   3afbc:	mov	x3, x26
   3afc0:	bl	c420 <__gmpn_sub_n@plt>
   3afc4:	mov	x0, x20
   3afc8:	mov	x1, x20
   3afcc:	mov	x2, x19
   3afd0:	mov	x3, x26
   3afd4:	bl	cc30 <__gmpn_add_n@plt>
   3afd8:	mov	w0, w22
   3afdc:	ldp	x20, x19, [sp, #80]
   3afe0:	ldp	x22, x21, [sp, #64]
   3afe4:	ldp	x24, x23, [sp, #48]
   3afe8:	ldp	x26, x25, [sp, #32]
   3afec:	ldr	x27, [sp, #16]
   3aff0:	ldp	x29, x30, [sp], #96
   3aff4:	ret

000000000003aff8 <__gmpn_toom_eval_pm2@@Base>:
   3aff8:	stp	x29, x30, [sp, #-96]!
   3affc:	sub	w8, w2, #0x2
   3b000:	mov	w9, w2
   3b004:	mul	x8, x8, x4
   3b008:	stp	x28, x27, [sp, #16]
   3b00c:	mul	x9, x9, x4
   3b010:	add	x27, x3, x8, lsl #3
   3b014:	stp	x24, x23, [sp, #48]
   3b018:	stp	x22, x21, [sp, #64]
   3b01c:	mov	x23, x3
   3b020:	mov	w24, w2
   3b024:	mov	x21, x1
   3b028:	add	x2, x3, x9, lsl #3
   3b02c:	mov	x1, x27
   3b030:	mov	x3, x5
   3b034:	stp	x26, x25, [sp, #32]
   3b038:	stp	x20, x19, [sp, #80]
   3b03c:	mov	x29, sp
   3b040:	mov	x19, x6
   3b044:	mov	x26, x5
   3b048:	mov	x22, x4
   3b04c:	mov	x20, x0
   3b050:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3b054:	subs	x2, x22, x26
   3b058:	mov	x25, x0
   3b05c:	b.eq	3b078 <__gmpn_toom_eval_pm2@@Base+0x80>  // b.none
   3b060:	lsl	x8, x26, #3
   3b064:	add	x0, x20, x8
   3b068:	add	x1, x27, x8
   3b06c:	mov	x3, x25
   3b070:	bl	c150 <__gmpn_add_1@plt>
   3b074:	mov	x25, x0
   3b078:	subs	w8, w24, #0x4
   3b07c:	b.mi	3b0bc <__gmpn_toom_eval_pm2@@Base+0xc4>  // b.first
   3b080:	sxtw	x8, w8
   3b084:	add	x27, x8, #0x2
   3b088:	mul	x8, x22, x8
   3b08c:	add	x26, x23, x8, lsl #3
   3b090:	neg	x28, x22, lsl #4
   3b094:	mov	x0, x20
   3b098:	mov	x1, x26
   3b09c:	mov	x2, x20
   3b0a0:	mov	x3, x22
   3b0a4:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3b0a8:	sub	x27, x27, #0x2
   3b0ac:	add	x25, x0, x25, lsl #2
   3b0b0:	cmp	x27, #0x1
   3b0b4:	add	x26, x26, x28
   3b0b8:	b.gt	3b094 <__gmpn_toom_eval_pm2@@Base+0x9c>
   3b0bc:	str	x25, [x20, x22, lsl #3]
   3b0c0:	sub	w25, w24, #0x1
   3b0c4:	sub	w8, w24, #0x3
   3b0c8:	mul	x8, x8, x22
   3b0cc:	mul	x9, x25, x22
   3b0d0:	add	x1, x23, x8, lsl #3
   3b0d4:	add	x2, x23, x9, lsl #3
   3b0d8:	mov	x0, x19
   3b0dc:	mov	x3, x22
   3b0e0:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3b0e4:	subs	w8, w24, #0x5
   3b0e8:	mov	x24, x0
   3b0ec:	b.mi	3b12c <__gmpn_toom_eval_pm2@@Base+0x134>  // b.first
   3b0f0:	sxtw	x8, w8
   3b0f4:	add	x26, x8, #0x2
   3b0f8:	mul	x8, x22, x8
   3b0fc:	add	x23, x23, x8, lsl #3
   3b100:	neg	x27, x22, lsl #4
   3b104:	mov	x0, x19
   3b108:	mov	x1, x23
   3b10c:	mov	x2, x19
   3b110:	mov	x3, x22
   3b114:	bl	cd60 <__gmpn_addlsh2_n@plt>
   3b118:	sub	x26, x26, #0x2
   3b11c:	add	x24, x0, x24, lsl #2
   3b120:	cmp	x26, #0x1
   3b124:	add	x23, x23, x27
   3b128:	b.gt	3b104 <__gmpn_toom_eval_pm2@@Base+0x10c>
   3b12c:	str	x24, [x19, x22, lsl #3]
   3b130:	add	x22, x22, #0x1
   3b134:	mov	w3, #0x1                   	// #1
   3b138:	tbnz	w25, #0, 3b148 <__gmpn_toom_eval_pm2@@Base+0x150>
   3b13c:	mov	x0, x20
   3b140:	mov	x1, x20
   3b144:	b	3b150 <__gmpn_toom_eval_pm2@@Base+0x158>
   3b148:	mov	x0, x19
   3b14c:	mov	x1, x19
   3b150:	mov	x2, x22
   3b154:	bl	c2d0 <__gmpn_lshift@plt>
   3b158:	mov	x0, x20
   3b15c:	mov	x1, x19
   3b160:	mov	x2, x22
   3b164:	and	w23, w25, #0x1
   3b168:	bl	c570 <__gmpn_cmp@plt>
   3b16c:	asr	w24, w0, #31
   3b170:	tbnz	w0, #31, 3b184 <__gmpn_toom_eval_pm2@@Base+0x18c>
   3b174:	mov	x0, x21
   3b178:	mov	x1, x20
   3b17c:	mov	x2, x19
   3b180:	b	3b190 <__gmpn_toom_eval_pm2@@Base+0x198>
   3b184:	mov	x0, x21
   3b188:	mov	x1, x19
   3b18c:	mov	x2, x20
   3b190:	mov	x3, x22
   3b194:	bl	c420 <__gmpn_sub_n@plt>
   3b198:	mov	x0, x20
   3b19c:	mov	x1, x20
   3b1a0:	mov	x2, x19
   3b1a4:	mov	x3, x22
   3b1a8:	bl	cc30 <__gmpn_add_n@plt>
   3b1ac:	sub	w8, w23, #0x1
   3b1b0:	eor	w0, w24, w8
   3b1b4:	ldp	x20, x19, [sp, #80]
   3b1b8:	ldp	x22, x21, [sp, #64]
   3b1bc:	ldp	x24, x23, [sp, #48]
   3b1c0:	ldp	x26, x25, [sp, #32]
   3b1c4:	ldp	x28, x27, [sp, #16]
   3b1c8:	ldp	x29, x30, [sp], #96
   3b1cc:	ret

000000000003b1d0 <__gmpn_toom_eval_pm2exp@@Base>:
   3b1d0:	sub	sp, sp, #0x70
   3b1d4:	stp	x28, x27, [sp, #32]
   3b1d8:	lsl	w27, w6, #1
   3b1dc:	stp	x29, x30, [sp, #16]
   3b1e0:	stp	x26, x25, [sp, #48]
   3b1e4:	stp	x24, x23, [sp, #64]
   3b1e8:	stp	x22, x21, [sp, #80]
   3b1ec:	stp	x20, x19, [sp, #96]
   3b1f0:	add	x29, sp, #0x10
   3b1f4:	mov	x26, x3
   3b1f8:	mov	w23, w2
   3b1fc:	mov	x21, x1
   3b200:	mov	x20, x0
   3b204:	add	x1, x3, x4, lsl #4
   3b208:	mov	x0, x7
   3b20c:	mov	x2, x4
   3b210:	mov	w3, w27
   3b214:	mov	x19, x7
   3b218:	str	x5, [sp]
   3b21c:	mov	x24, x4
   3b220:	stur	w6, [x29, #-4]
   3b224:	bl	c2d0 <__gmpn_lshift@plt>
   3b228:	lsl	x22, x24, #3
   3b22c:	str	x0, [x20, x22]
   3b230:	mov	x0, x20
   3b234:	mov	x1, x26
   3b238:	mov	x2, x19
   3b23c:	mov	x3, x24
   3b240:	bl	cc30 <__gmpn_add_n@plt>
   3b244:	ldr	x8, [x20, x22]
   3b248:	cmp	w23, #0x5
   3b24c:	add	x8, x8, x0
   3b250:	str	x8, [x20, x22]
   3b254:	b.cc	3b2bc <__gmpn_toom_eval_pm2exp@@Base+0xec>  // b.lo, b.ul, b.last
   3b258:	ldur	w8, [x29, #-4]
   3b25c:	mov	w25, #0x4                   	// #4
   3b260:	lsl	w28, w8, #2
   3b264:	mov	w8, w25
   3b268:	mul	x8, x8, x24
   3b26c:	add	x1, x26, x8, lsl #3
   3b270:	mov	x0, x19
   3b274:	mov	x2, x24
   3b278:	mov	w3, w28
   3b27c:	bl	c2d0 <__gmpn_lshift@plt>
   3b280:	ldr	x8, [x20, x22]
   3b284:	mov	x1, x20
   3b288:	mov	x2, x19
   3b28c:	mov	x3, x24
   3b290:	add	x8, x8, x0
   3b294:	mov	x0, x20
   3b298:	str	x8, [x20, x22]
   3b29c:	bl	cc30 <__gmpn_add_n@plt>
   3b2a0:	ldr	x8, [x20, x22]
   3b2a4:	add	w25, w25, #0x2
   3b2a8:	cmp	w25, w23
   3b2ac:	add	w28, w28, w27
   3b2b0:	add	x8, x8, x0
   3b2b4:	str	x8, [x20, x22]
   3b2b8:	b.cc	3b264 <__gmpn_toom_eval_pm2exp@@Base+0x94>  // b.lo, b.ul, b.last
   3b2bc:	ldur	w3, [x29, #-4]
   3b2c0:	add	x1, x26, x22
   3b2c4:	mov	x0, x19
   3b2c8:	mov	x2, x24
   3b2cc:	bl	c2d0 <__gmpn_lshift@plt>
   3b2d0:	cmp	w23, #0x4
   3b2d4:	str	x0, [x19, x22]
   3b2d8:	b.cc	3b340 <__gmpn_toom_eval_pm2exp@@Base+0x170>  // b.lo, b.ul, b.last
   3b2dc:	ldur	w8, [x29, #-4]
   3b2e0:	mov	w25, #0x3                   	// #3
   3b2e4:	add	w28, w27, w8
   3b2e8:	mov	w8, w25
   3b2ec:	mul	x8, x8, x24
   3b2f0:	add	x1, x26, x8, lsl #3
   3b2f4:	mov	x0, x21
   3b2f8:	mov	x2, x24
   3b2fc:	mov	w3, w28
   3b300:	bl	c2d0 <__gmpn_lshift@plt>
   3b304:	ldr	x8, [x19, x22]
   3b308:	mov	x1, x19
   3b30c:	mov	x2, x21
   3b310:	mov	x3, x24
   3b314:	add	x8, x8, x0
   3b318:	mov	x0, x19
   3b31c:	str	x8, [x19, x22]
   3b320:	bl	cc30 <__gmpn_add_n@plt>
   3b324:	ldr	x8, [x19, x22]
   3b328:	add	w25, w25, #0x2
   3b32c:	cmp	w25, w23
   3b330:	add	w28, w28, w27
   3b334:	add	x8, x8, x0
   3b338:	str	x8, [x19, x22]
   3b33c:	b.cc	3b2e8 <__gmpn_toom_eval_pm2exp@@Base+0x118>  // b.lo, b.ul, b.last
   3b340:	mov	w8, w23
   3b344:	mul	x8, x8, x24
   3b348:	add	x1, x26, x8, lsl #3
   3b34c:	ldur	w8, [x29, #-4]
   3b350:	ldr	x22, [sp]
   3b354:	mov	x0, x21
   3b358:	mul	w3, w8, w23
   3b35c:	mov	x2, x22
   3b360:	bl	c2d0 <__gmpn_lshift@plt>
   3b364:	add	x24, x24, #0x1
   3b368:	add	x4, x22, #0x1
   3b36c:	str	x0, [x21, x22, lsl #3]
   3b370:	tbnz	w23, #0, 3b380 <__gmpn_toom_eval_pm2exp@@Base+0x1b0>
   3b374:	mov	x0, x20
   3b378:	mov	x1, x20
   3b37c:	b	3b388 <__gmpn_toom_eval_pm2exp@@Base+0x1b8>
   3b380:	mov	x0, x19
   3b384:	mov	x1, x19
   3b388:	mov	x2, x24
   3b38c:	mov	x3, x21
   3b390:	bl	c970 <__gmpn_add@plt>
   3b394:	mov	x0, x20
   3b398:	mov	x1, x19
   3b39c:	mov	x2, x24
   3b3a0:	bl	c570 <__gmpn_cmp@plt>
   3b3a4:	asr	w22, w0, #31
   3b3a8:	tbnz	w0, #31, 3b3bc <__gmpn_toom_eval_pm2exp@@Base+0x1ec>
   3b3ac:	mov	x0, x21
   3b3b0:	mov	x1, x20
   3b3b4:	mov	x2, x19
   3b3b8:	b	3b3c8 <__gmpn_toom_eval_pm2exp@@Base+0x1f8>
   3b3bc:	mov	x0, x21
   3b3c0:	mov	x1, x19
   3b3c4:	mov	x2, x20
   3b3c8:	mov	x3, x24
   3b3cc:	bl	c420 <__gmpn_sub_n@plt>
   3b3d0:	mov	x0, x20
   3b3d4:	mov	x1, x20
   3b3d8:	mov	x2, x19
   3b3dc:	mov	x3, x24
   3b3e0:	bl	cc30 <__gmpn_add_n@plt>
   3b3e4:	mov	w0, w22
   3b3e8:	ldp	x20, x19, [sp, #96]
   3b3ec:	ldp	x22, x21, [sp, #80]
   3b3f0:	ldp	x24, x23, [sp, #64]
   3b3f4:	ldp	x26, x25, [sp, #48]
   3b3f8:	ldp	x28, x27, [sp, #32]
   3b3fc:	ldp	x29, x30, [sp, #16]
   3b400:	add	sp, sp, #0x70
   3b404:	ret

000000000003b408 <__gmpn_toom_eval_pm2rexp@@Base>:
   3b408:	sub	sp, sp, #0x80
   3b40c:	stp	x24, x23, [sp, #80]
   3b410:	mov	x24, x3
   3b414:	stp	x26, x25, [sp, #64]
   3b418:	stp	x22, x21, [sp, #96]
   3b41c:	mov	w26, w2
   3b420:	mov	x21, x1
   3b424:	mul	w3, w6, w2
   3b428:	mov	x1, x24
   3b42c:	mov	x2, x4
   3b430:	stp	x29, x30, [sp, #32]
   3b434:	stp	x28, x27, [sp, #48]
   3b438:	stp	x20, x19, [sp, #112]
   3b43c:	add	x29, sp, #0x20
   3b440:	mov	x19, x7
   3b444:	mov	w27, w6
   3b448:	mov	x25, x5
   3b44c:	mov	x23, x4
   3b450:	mov	x20, x0
   3b454:	bl	c2d0 <__gmpn_lshift@plt>
   3b458:	lsl	x28, x23, #3
   3b45c:	sub	w22, w26, #0x1
   3b460:	str	x0, [x20, x28]
   3b464:	add	x1, x24, x28
   3b468:	mul	w3, w22, w27
   3b46c:	mov	x0, x19
   3b470:	mov	x2, x23
   3b474:	bl	c2d0 <__gmpn_lshift@plt>
   3b478:	mov	w8, w26
   3b47c:	mul	x8, x8, x23
   3b480:	add	x2, x23, #0x1
   3b484:	add	x3, x24, x8, lsl #3
   3b488:	str	x0, [x19, x28]
   3b48c:	stp	x2, x21, [sp, #8]
   3b490:	tbnz	w26, #0, 3b4a8 <__gmpn_toom_eval_pm2rexp@@Base+0xa0>
   3b494:	mov	x0, x20
   3b498:	mov	x1, x20
   3b49c:	mov	x4, x25
   3b4a0:	bl	c970 <__gmpn_add@plt>
   3b4a4:	b	3b4e4 <__gmpn_toom_eval_pm2rexp@@Base+0xdc>
   3b4a8:	mov	x0, x19
   3b4ac:	mov	x1, x19
   3b4b0:	mov	x4, x25
   3b4b4:	bl	c970 <__gmpn_add@plt>
   3b4b8:	mov	w8, w22
   3b4bc:	mul	x8, x8, x23
   3b4c0:	add	x1, x24, x8, lsl #3
   3b4c4:	mov	x0, x20
   3b4c8:	mov	x2, x23
   3b4cc:	mov	w3, w27
   3b4d0:	mov	x4, x21
   3b4d4:	bl	3b608 <__gmpn_toom_eval_pm2rexp@@Base+0x200>
   3b4d8:	ldr	x8, [x20, x28]
   3b4dc:	add	x8, x8, x0
   3b4e0:	str	x8, [x20, x28]
   3b4e4:	ldr	x25, [sp, #16]
   3b4e8:	cmp	w22, #0x3
   3b4ec:	b.cc	3b590 <__gmpn_toom_eval_pm2rexp@@Base+0x188>  // b.lo, b.ul, b.last
   3b4f0:	lsl	w9, w27, #1
   3b4f4:	sub	w8, w26, #0x2
   3b4f8:	stur	w9, [x29, #-4]
   3b4fc:	sub	w9, w26, #0x3
   3b500:	mov	w10, w22
   3b504:	mov	w22, wzr
   3b508:	mul	w8, w27, w8
   3b50c:	mul	w27, w27, w9
   3b510:	mov	w21, #0x2                   	// #2
   3b514:	stur	w8, [x29, #-8]
   3b518:	mov	w8, w21
   3b51c:	mul	x8, x8, x23
   3b520:	add	x1, x24, x8, lsl #3
   3b524:	ldur	w8, [x29, #-8]
   3b528:	mov	x0, x20
   3b52c:	mov	x2, x23
   3b530:	mov	x4, x25
   3b534:	add	w3, w8, w22
   3b538:	mov	w26, w10
   3b53c:	bl	3b608 <__gmpn_toom_eval_pm2rexp@@Base+0x200>
   3b540:	ldr	x8, [x20, x28]
   3b544:	add	w9, w21, #0x1
   3b548:	mul	x9, x9, x23
   3b54c:	add	x1, x24, x9, lsl #3
   3b550:	add	x8, x8, x0
   3b554:	add	w3, w27, w22
   3b558:	mov	x0, x19
   3b55c:	mov	x2, x23
   3b560:	mov	x4, x25
   3b564:	str	x8, [x20, x28]
   3b568:	bl	3b608 <__gmpn_toom_eval_pm2rexp@@Base+0x200>
   3b56c:	ldr	x8, [x19, x28]
   3b570:	add	w21, w21, #0x2
   3b574:	mov	w10, w26
   3b578:	cmp	w21, w26
   3b57c:	add	x8, x8, x0
   3b580:	str	x8, [x19, x28]
   3b584:	ldur	w8, [x29, #-4]
   3b588:	sub	w22, w22, w8
   3b58c:	b.cc	3b518 <__gmpn_toom_eval_pm2rexp@@Base+0x110>  // b.lo, b.ul, b.last
   3b590:	ldr	x21, [sp, #8]
   3b594:	mov	x0, x20
   3b598:	mov	x1, x19
   3b59c:	mov	x2, x21
   3b5a0:	bl	c570 <__gmpn_cmp@plt>
   3b5a4:	asr	w23, w0, #31
   3b5a8:	tbnz	w0, #31, 3b5bc <__gmpn_toom_eval_pm2rexp@@Base+0x1b4>
   3b5ac:	ldr	x0, [sp, #16]
   3b5b0:	mov	x1, x20
   3b5b4:	mov	x2, x19
   3b5b8:	b	3b5c8 <__gmpn_toom_eval_pm2rexp@@Base+0x1c0>
   3b5bc:	ldr	x0, [sp, #16]
   3b5c0:	mov	x1, x19
   3b5c4:	mov	x2, x20
   3b5c8:	mov	x3, x21
   3b5cc:	bl	c420 <__gmpn_sub_n@plt>
   3b5d0:	mov	x0, x20
   3b5d4:	mov	x1, x20
   3b5d8:	mov	x2, x19
   3b5dc:	mov	x3, x21
   3b5e0:	bl	cc30 <__gmpn_add_n@plt>
   3b5e4:	mov	w0, w23
   3b5e8:	ldp	x20, x19, [sp, #112]
   3b5ec:	ldp	x22, x21, [sp, #96]
   3b5f0:	ldp	x24, x23, [sp, #80]
   3b5f4:	ldp	x26, x25, [sp, #64]
   3b5f8:	ldp	x28, x27, [sp, #48]
   3b5fc:	ldp	x29, x30, [sp, #32]
   3b600:	add	sp, sp, #0x80
   3b604:	ret
   3b608:	stp	x29, x30, [sp, #-48]!
   3b60c:	stp	x22, x21, [sp, #16]
   3b610:	mov	x21, x0
   3b614:	mov	x0, x4
   3b618:	stp	x20, x19, [sp, #32]
   3b61c:	mov	x29, sp
   3b620:	mov	x19, x4
   3b624:	mov	x20, x2
   3b628:	bl	c2d0 <__gmpn_lshift@plt>
   3b62c:	mov	x22, x0
   3b630:	mov	x0, x21
   3b634:	mov	x1, x21
   3b638:	mov	x2, x19
   3b63c:	mov	x3, x20
   3b640:	bl	cc30 <__gmpn_add_n@plt>
   3b644:	add	x0, x0, x22
   3b648:	ldp	x20, x19, [sp, #32]
   3b64c:	ldp	x22, x21, [sp, #16]
   3b650:	ldp	x29, x30, [sp], #48
   3b654:	ret

000000000003b658 <__gmpn_toom_interpolate_5pts@@Base>:
   3b658:	sub	sp, sp, #0x80
   3b65c:	stp	x24, x23, [sp, #80]
   3b660:	lsl	x24, x3, #3
   3b664:	add	x23, x0, x24
   3b668:	stp	x28, x27, [sp, #48]
   3b66c:	stp	x26, x25, [sp, #64]
   3b670:	mov	w27, #0x1                   	// #1
   3b674:	add	x25, x23, x24
   3b678:	stp	x29, x30, [sp, #32]
   3b67c:	stp	x22, x21, [sp, #96]
   3b680:	add	x29, sp, #0x20
   3b684:	bfi	x27, x3, #1, #63
   3b688:	add	x22, x25, x24
   3b68c:	stp	x20, x19, [sp, #112]
   3b690:	mov	w20, w5
   3b694:	mov	x19, x3
   3b698:	mov	x28, x2
   3b69c:	mov	x21, x1
   3b6a0:	lsl	x8, x3, #1
   3b6a4:	stur	x0, [x29, #-8]
   3b6a8:	add	x26, x22, x24
   3b6ac:	mov	x0, x1
   3b6b0:	mov	x3, x27
   3b6b4:	stp	x6, x4, [sp, #8]
   3b6b8:	str	x8, [sp]
   3b6bc:	cbz	w5, 3b6c8 <__gmpn_toom_interpolate_5pts@@Base+0x70>
   3b6c0:	bl	cc30 <__gmpn_add_n@plt>
   3b6c4:	b	3b6cc <__gmpn_toom_interpolate_5pts@@Base+0x74>
   3b6c8:	bl	c420 <__gmpn_sub_n@plt>
   3b6cc:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   3b6d0:	mov	x0, x21
   3b6d4:	mov	x1, x21
   3b6d8:	mov	x2, x27
   3b6dc:	mov	x4, xzr
   3b6e0:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3b6e4:	mov	x0, x28
   3b6e8:	mov	x1, x25
   3b6ec:	mov	x2, x28
   3b6f0:	mov	x3, x27
   3b6f4:	cbz	w20, 3b700 <__gmpn_toom_interpolate_5pts@@Base+0xa8>
   3b6f8:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3b6fc:	b	3b704 <__gmpn_toom_interpolate_5pts@@Base+0xac>
   3b700:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3b704:	ldur	x2, [x29, #-8]
   3b708:	ldr	x3, [sp]
   3b70c:	mov	x0, x25
   3b710:	mov	x1, x25
   3b714:	bl	c420 <__gmpn_sub_n@plt>
   3b718:	ldr	x8, [x26]
   3b71c:	mov	x1, x21
   3b720:	mov	x2, x25
   3b724:	mov	x3, x27
   3b728:	sub	x8, x8, x0
   3b72c:	mov	x0, x21
   3b730:	str	x8, [x26]
   3b734:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3b738:	mov	x0, x25
   3b73c:	mov	x1, x25
   3b740:	mov	x2, x28
   3b744:	mov	x3, x27
   3b748:	bl	c420 <__gmpn_sub_n@plt>
   3b74c:	mov	x0, x23
   3b750:	mov	x1, x23
   3b754:	mov	x2, x28
   3b758:	mov	x3, x27
   3b75c:	bl	cc30 <__gmpn_add_n@plt>
   3b760:	ldr	x8, [x22, #8]
   3b764:	adds	x8, x8, x0
   3b768:	str	x8, [x22, #8]
   3b76c:	b.cc	3b790 <__gmpn_toom_interpolate_5pts@@Base+0x138>  // b.lo, b.ul, b.last
   3b770:	ldur	x9, [x29, #-8]
   3b774:	mov	w8, #0x18                  	// #24
   3b778:	madd	x8, x19, x8, x9
   3b77c:	add	x8, x8, #0x10
   3b780:	ldr	x9, [x8]
   3b784:	adds	x9, x9, #0x1
   3b788:	str	x9, [x8], #8
   3b78c:	b.cs	3b780 <__gmpn_toom_interpolate_5pts@@Base+0x128>  // b.hs, b.nlast
   3b790:	ldr	x8, [x26]
   3b794:	mov	x0, x21
   3b798:	mov	x1, x21
   3b79c:	mov	x2, x26
   3b7a0:	str	x8, [sp]
   3b7a4:	ldp	x8, x20, [sp, #8]
   3b7a8:	mov	x3, x20
   3b7ac:	str	x8, [x26]
   3b7b0:	bl	c5b0 <__gmpn_sublsh1_n@plt>
   3b7b4:	lsl	x28, x20, #3
   3b7b8:	ldr	x8, [x21, x28]
   3b7bc:	subs	x8, x8, x0
   3b7c0:	str	x8, [x21, x28]
   3b7c4:	b.cs	3b7e0 <__gmpn_toom_interpolate_5pts@@Base+0x188>  // b.hs, b.nlast
   3b7c8:	add	x8, x21, x20, lsl #3
   3b7cc:	add	x8, x8, #0x8
   3b7d0:	ldr	x9, [x8]
   3b7d4:	sub	x10, x9, #0x1
   3b7d8:	str	x10, [x8], #8
   3b7dc:	cbz	x9, 3b7d0 <__gmpn_toom_interpolate_5pts@@Base+0x178>
   3b7e0:	add	x3, x19, #0x1
   3b7e4:	cmp	x3, x20
   3b7e8:	add	x2, x21, x19, lsl #3
   3b7ec:	mov	x0, x26
   3b7f0:	mov	x1, x26
   3b7f4:	b.ge	3b928 <__gmpn_toom_interpolate_5pts@@Base+0x2d0>  // b.tcont
   3b7f8:	bl	cc30 <__gmpn_add_n@plt>
   3b7fc:	lsl	x8, x27, #3
   3b800:	ldr	x9, [x22, x8]
   3b804:	adds	x9, x9, x0
   3b808:	str	x9, [x22, x8]
   3b80c:	b.cc	3b830 <__gmpn_toom_interpolate_5pts@@Base+0x1d8>  // b.lo, b.ul, b.last
   3b810:	ldur	x9, [x29, #-8]
   3b814:	mov	w8, #0x28                  	// #40
   3b818:	madd	x8, x19, x8, x9
   3b81c:	add	x8, x8, #0x10
   3b820:	ldr	x9, [x8]
   3b824:	adds	x9, x9, #0x1
   3b828:	str	x9, [x8], #8
   3b82c:	b.cs	3b820 <__gmpn_toom_interpolate_5pts@@Base+0x1c8>  // b.hs, b.nlast
   3b830:	ldr	x20, [sp, #16]
   3b834:	mov	x0, x25
   3b838:	mov	x1, x25
   3b83c:	mov	x2, x26
   3b840:	mov	x3, x20
   3b844:	bl	c420 <__gmpn_sub_n@plt>
   3b848:	ldr	x8, [sp]
   3b84c:	ldr	x27, [x26]
   3b850:	str	x8, [x26]
   3b854:	ldr	x8, [x25, x28]
   3b858:	subs	x8, x8, x0
   3b85c:	str	x8, [x25, x28]
   3b860:	b.cs	3b884 <__gmpn_toom_interpolate_5pts@@Base+0x22c>  // b.hs, b.nlast
   3b864:	ldur	x9, [x29, #-8]
   3b868:	add	x8, x20, x19, lsl #1
   3b86c:	add	x8, x9, x8, lsl #3
   3b870:	add	x8, x8, #0x8
   3b874:	ldr	x9, [x8]
   3b878:	sub	x10, x9, #0x1
   3b87c:	str	x10, [x8], #8
   3b880:	cbz	x9, 3b874 <__gmpn_toom_interpolate_5pts@@Base+0x21c>
   3b884:	mov	x0, x23
   3b888:	mov	x1, x23
   3b88c:	mov	x2, x21
   3b890:	mov	x3, x19
   3b894:	bl	c420 <__gmpn_sub_n@plt>
   3b898:	ldr	x8, [x23, x24]
   3b89c:	subs	x8, x8, x0
   3b8a0:	str	x8, [x23, x24]
   3b8a4:	b.cs	3b8c4 <__gmpn_toom_interpolate_5pts@@Base+0x26c>  // b.hs, b.nlast
   3b8a8:	ldur	x8, [x29, #-8]
   3b8ac:	add	x8, x8, x19, lsl #4
   3b8b0:	add	x8, x8, #0x8
   3b8b4:	ldr	x9, [x8]
   3b8b8:	sub	x10, x9, #0x1
   3b8bc:	str	x10, [x8], #8
   3b8c0:	cbz	x9, 3b8b4 <__gmpn_toom_interpolate_5pts@@Base+0x25c>
   3b8c4:	mov	x0, x22
   3b8c8:	mov	x1, x22
   3b8cc:	mov	x2, x21
   3b8d0:	mov	x3, x19
   3b8d4:	bl	cc30 <__gmpn_add_n@plt>
   3b8d8:	ldr	x8, [x22, x24]
   3b8dc:	add	x8, x8, x0
   3b8e0:	adds	x8, x8, x27
   3b8e4:	str	x8, [x22, x24]
   3b8e8:	b.cc	3b908 <__gmpn_toom_interpolate_5pts@@Base+0x2b0>  // b.lo, b.ul, b.last
   3b8ec:	ldur	x8, [x29, #-8]
   3b8f0:	add	x8, x8, x19, lsl #5
   3b8f4:	add	x8, x8, #0x8
   3b8f8:	ldr	x9, [x8]
   3b8fc:	adds	x9, x9, #0x1
   3b900:	str	x9, [x8], #8
   3b904:	b.cs	3b8f8 <__gmpn_toom_interpolate_5pts@@Base+0x2a0>  // b.hs, b.nlast
   3b908:	ldp	x20, x19, [sp, #112]
   3b90c:	ldp	x22, x21, [sp, #96]
   3b910:	ldp	x24, x23, [sp, #80]
   3b914:	ldp	x26, x25, [sp, #64]
   3b918:	ldp	x28, x27, [sp, #48]
   3b91c:	ldp	x29, x30, [sp, #32]
   3b920:	add	sp, sp, #0x80
   3b924:	ret
   3b928:	ldr	x3, [sp, #16]
   3b92c:	bl	cc30 <__gmpn_add_n@plt>
   3b930:	b	3b830 <__gmpn_toom_interpolate_5pts@@Base+0x1d8>

000000000003b934 <__gmpn_toom_interpolate_6pts@@Base>:
   3b934:	sub	sp, sp, #0x90
   3b938:	stp	x26, x25, [sp, #80]
   3b93c:	mov	w25, #0x1                   	// #1
   3b940:	stp	x29, x30, [sp, #48]
   3b944:	stp	x28, x27, [sp, #64]
   3b948:	stp	x24, x23, [sp, #96]
   3b94c:	stp	x22, x21, [sp, #112]
   3b950:	stp	x20, x19, [sp, #128]
   3b954:	add	x29, sp, #0x30
   3b958:	mov	x23, x5
   3b95c:	mov	x27, x4
   3b960:	mov	x22, x3
   3b964:	mov	w24, w2
   3b968:	mov	x20, x1
   3b96c:	mov	x19, x0
   3b970:	lsl	x28, x1, #1
   3b974:	bfi	x25, x1, #1, #63
   3b978:	mov	x0, x4
   3b97c:	mov	x1, x5
   3b980:	stur	x6, [x29, #-8]
   3b984:	tbnz	w2, #1, 3b998 <__gmpn_toom_interpolate_6pts@@Base+0x64>
   3b988:	mov	x2, x27
   3b98c:	mov	x3, x25
   3b990:	bl	c420 <__gmpn_sub_n@plt>
   3b994:	b	3b9a4 <__gmpn_toom_interpolate_6pts@@Base+0x70>
   3b998:	mov	x2, x27
   3b99c:	mov	x3, x25
   3b9a0:	bl	cc30 <__gmpn_add_n@plt>
   3b9a4:	mov	w3, #0x2                   	// #2
   3b9a8:	mov	x0, x27
   3b9ac:	mov	x1, x27
   3b9b0:	mov	x2, x25
   3b9b4:	bl	c2f0 <__gmpn_rshift@plt>
   3b9b8:	mov	x0, x23
   3b9bc:	mov	x1, x23
   3b9c0:	mov	x2, x19
   3b9c4:	mov	x3, x28
   3b9c8:	bl	c420 <__gmpn_sub_n@plt>
   3b9cc:	lsl	x21, x28, #3
   3b9d0:	ldr	x8, [x23, x21]
   3b9d4:	mov	w3, #0x1                   	// #1
   3b9d8:	mov	x1, x23
   3b9dc:	mov	x2, x25
   3b9e0:	sub	x8, x8, x0
   3b9e4:	mov	x0, x23
   3b9e8:	str	x8, [x23, x21]
   3b9ec:	bl	c2f0 <__gmpn_rshift@plt>
   3b9f0:	mov	x0, x23
   3b9f4:	mov	x1, x23
   3b9f8:	mov	x2, x27
   3b9fc:	mov	x3, x25
   3ba00:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3ba04:	add	x26, x19, x21
   3ba08:	mov	x0, x22
   3ba0c:	mov	x1, x26
   3ba10:	mov	x2, x22
   3ba14:	mov	x3, x25
   3ba18:	tbnz	w24, #0, 3ba24 <__gmpn_toom_interpolate_6pts@@Base+0xf0>
   3ba1c:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3ba20:	b	3ba28 <__gmpn_toom_interpolate_6pts@@Base+0xf4>
   3ba24:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3ba28:	mov	x0, x27
   3ba2c:	mov	x1, x27
   3ba30:	mov	x2, x22
   3ba34:	mov	x3, x25
   3ba38:	bl	c420 <__gmpn_sub_n@plt>
   3ba3c:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   3ba40:	mov	x0, x27
   3ba44:	mov	x1, x27
   3ba48:	mov	x2, x25
   3ba4c:	mov	x4, xzr
   3ba50:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3ba54:	mov	x0, x26
   3ba58:	mov	x1, x26
   3ba5c:	mov	x2, x22
   3ba60:	mov	x3, x25
   3ba64:	bl	c420 <__gmpn_sub_n@plt>
   3ba68:	mov	x0, x26
   3ba6c:	mov	x1, x26
   3ba70:	mov	x2, x19
   3ba74:	mov	x3, x28
   3ba78:	str	x28, [sp, #8]
   3ba7c:	bl	c420 <__gmpn_sub_n@plt>
   3ba80:	ldr	x8, [x26, x21]
   3ba84:	mov	x1, x23
   3ba88:	mov	x2, x26
   3ba8c:	mov	x3, x25
   3ba90:	sub	x8, x8, x0
   3ba94:	mov	x0, x23
   3ba98:	str	x8, [x26, x21]
   3ba9c:	bl	c420 <__gmpn_sub_n@plt>
   3baa0:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   3baa4:	mov	x0, x23
   3baa8:	mov	x1, x23
   3baac:	mov	x2, x25
   3bab0:	mov	x4, xzr
   3bab4:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3bab8:	add	x28, x19, x20, lsl #3
   3babc:	mov	x0, x28
   3bac0:	mov	x1, x28
   3bac4:	mov	x2, x22
   3bac8:	mov	x3, x25
   3bacc:	bl	cc30 <__gmpn_add_n@plt>
   3bad0:	mov	w8, #0x18                  	// #24
   3bad4:	madd	x25, x20, x8, x19
   3bad8:	ldr	x8, [x25, #8]
   3badc:	adds	x8, x8, x0
   3bae0:	str	x8, [x25, #8]
   3bae4:	b.cc	3bb04 <__gmpn_toom_interpolate_6pts@@Base+0x1d0>  // b.lo, b.ul, b.last
   3bae8:	mov	w8, #0x18                  	// #24
   3baec:	madd	x8, x20, x8, x19
   3baf0:	add	x8, x8, #0x10
   3baf4:	ldr	x9, [x8]
   3baf8:	adds	x9, x9, #0x1
   3bafc:	str	x9, [x8], #8
   3bb00:	b.cs	3baf4 <__gmpn_toom_interpolate_6pts@@Base+0x1c0>  // b.hs, b.nlast
   3bb04:	ldur	x24, [x29, #-8]
   3bb08:	mov	w8, #0x28                  	// #40
   3bb0c:	madd	x22, x20, x8, x19
   3bb10:	mov	x0, x27
   3bb14:	mov	x1, x27
   3bb18:	mov	x2, x22
   3bb1c:	mov	x3, x24
   3bb20:	bl	c2b0 <__gmpn_sublsh2_n@plt>
   3bb24:	lsl	x9, x24, #3
   3bb28:	ldr	x8, [x27, x9]
   3bb2c:	stur	x9, [x29, #-16]
   3bb30:	subs	x8, x8, x0
   3bb34:	str	x8, [x27, x9]
   3bb38:	b.cs	3bb58 <__gmpn_toom_interpolate_6pts@@Base+0x224>  // b.hs, b.nlast
   3bb3c:	ldur	x8, [x29, #-8]
   3bb40:	add	x8, x27, x8, lsl #3
   3bb44:	add	x8, x8, #0x8
   3bb48:	ldr	x9, [x8]
   3bb4c:	sub	x10, x9, #0x1
   3bb50:	str	x10, [x8], #8
   3bb54:	cbz	x9, 3bb48 <__gmpn_toom_interpolate_6pts@@Base+0x214>
   3bb58:	mov	x0, x28
   3bb5c:	mov	x1, x28
   3bb60:	mov	x2, x27
   3bb64:	mov	x3, x20
   3bb68:	bl	c420 <__gmpn_sub_n@plt>
   3bb6c:	ldr	x8, [x26]
   3bb70:	subs	x8, x8, x0
   3bb74:	str	x8, [x26]
   3bb78:	b.cs	3bb94 <__gmpn_toom_interpolate_6pts@@Base+0x260>  // b.hs, b.nlast
   3bb7c:	add	x8, x19, x20, lsl #4
   3bb80:	add	x8, x8, #0x8
   3bb84:	ldr	x9, [x8]
   3bb88:	sub	x10, x9, #0x1
   3bb8c:	str	x10, [x8], #8
   3bb90:	cbz	x9, 3bb84 <__gmpn_toom_interpolate_6pts@@Base+0x250>
   3bb94:	ldr	x8, [x26, x21]
   3bb98:	mov	x0, x25
   3bb9c:	mov	x1, x25
   3bba0:	mov	x2, x27
   3bba4:	mov	x3, x20
   3bba8:	str	x8, [sp, #24]
   3bbac:	bl	cc30 <__gmpn_add_n@plt>
   3bbb0:	ldr	x24, [x27, x21]
   3bbb4:	add	x28, x19, x20, lsl #5
   3bbb8:	lsl	x21, x20, #3
   3bbbc:	str	x0, [sp, #16]
   3bbc0:	add	x2, x27, x21
   3bbc4:	mov	x0, x28
   3bbc8:	mov	x1, x23
   3bbcc:	mov	x3, x20
   3bbd0:	bl	cc30 <__gmpn_add_n@plt>
   3bbd4:	add	x2, x23, x21
   3bbd8:	ldr	x8, [x2]
   3bbdc:	add	x9, x0, x24
   3bbe0:	adds	x8, x8, x9
   3bbe4:	str	x8, [x2]
   3bbe8:	b.cc	3bc04 <__gmpn_toom_interpolate_6pts@@Base+0x2d0>  // b.lo, b.ul, b.last
   3bbec:	add	x8, x23, x20, lsl #3
   3bbf0:	add	x8, x8, #0x8
   3bbf4:	ldr	x9, [x8]
   3bbf8:	adds	x9, x9, #0x1
   3bbfc:	str	x9, [x8], #8
   3bc00:	b.cs	3bbf4 <__gmpn_toom_interpolate_6pts@@Base+0x2c0>  // b.hs, b.nlast
   3bc04:	ldur	x27, [x29, #-8]
   3bc08:	cmp	x27, x20
   3bc0c:	b.le	3bdb8 <__gmpn_toom_interpolate_6pts@@Base+0x484>
   3bc10:	ldr	x8, [sp, #8]
   3bc14:	mov	x0, x22
   3bc18:	mov	x1, x22
   3bc1c:	mov	x3, x20
   3bc20:	ldr	x23, [x23, x8, lsl #3]
   3bc24:	bl	cc30 <__gmpn_add_n@plt>
   3bc28:	add	x23, x0, x23
   3bc2c:	ldp	x9, x8, [sp, #16]
   3bc30:	add	x3, x27, x20
   3bc34:	mov	x0, x26
   3bc38:	mov	x1, x26
   3bc3c:	mov	x2, x28
   3bc40:	add	x24, x9, x8
   3bc44:	bl	c420 <__gmpn_sub_n@plt>
   3bc48:	sub	x8, x27, #0x1
   3bc4c:	lsl	x8, x8, #3
   3bc50:	ldr	x9, [x22, x8]
   3bc54:	mov	w10, #0x1                   	// #1
   3bc58:	cmp	x27, x20
   3bc5c:	str	x10, [x22, x8]
   3bc60:	sub	x9, x9, #0x1
   3bc64:	b.le	3bc9c <__gmpn_toom_interpolate_6pts@@Base+0x368>
   3bc68:	subs	x10, x24, x23
   3bc6c:	b.ls	3bd00 <__gmpn_toom_interpolate_6pts@@Base+0x3cc>  // b.plast
   3bc70:	ldr	x11, [x28]
   3bc74:	adds	x10, x11, x10
   3bc78:	str	x10, [x28]
   3bc7c:	b.cc	3bd2c <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.lo, b.ul, b.last
   3bc80:	add	x10, x19, x20, lsl #5
   3bc84:	add	x10, x10, #0x8
   3bc88:	ldr	x11, [x10]
   3bc8c:	adds	x11, x11, #0x1
   3bc90:	str	x11, [x10], #8
   3bc94:	b.cs	3bc88 <__gmpn_toom_interpolate_6pts@@Base+0x354>  // b.hs, b.nlast
   3bc98:	b	3bd2c <__gmpn_toom_interpolate_6pts@@Base+0x3f8>
   3bc9c:	ldr	x10, [x28]
   3bca0:	adds	x10, x10, x24
   3bca4:	str	x10, [x28]
   3bca8:	b.cc	3bcc4 <__gmpn_toom_interpolate_6pts@@Base+0x390>  // b.lo, b.ul, b.last
   3bcac:	add	x10, x19, x20, lsl #5
   3bcb0:	add	x10, x10, #0x8
   3bcb4:	ldr	x11, [x10]
   3bcb8:	adds	x11, x11, #0x1
   3bcbc:	str	x11, [x10], #8
   3bcc0:	b.cs	3bcb4 <__gmpn_toom_interpolate_6pts@@Base+0x380>  // b.hs, b.nlast
   3bcc4:	ldur	x12, [x29, #-16]
   3bcc8:	add	x11, x0, x23
   3bccc:	ldr	x10, [x25, x12]
   3bcd0:	subs	x10, x10, x11
   3bcd4:	str	x10, [x25, x12]
   3bcd8:	b.cs	3bd8c <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.hs, b.nlast
   3bcdc:	add	x10, x20, x20, lsl #1
   3bce0:	add	x10, x27, x10
   3bce4:	add	x10, x19, x10, lsl #3
   3bce8:	add	x10, x10, #0x8
   3bcec:	ldr	x11, [x10]
   3bcf0:	sub	x12, x11, #0x1
   3bcf4:	str	x12, [x10], #8
   3bcf8:	cbz	x11, 3bcec <__gmpn_toom_interpolate_6pts@@Base+0x3b8>
   3bcfc:	b	3bd8c <__gmpn_toom_interpolate_6pts@@Base+0x458>
   3bd00:	ldr	x10, [x28]
   3bd04:	sub	x11, x23, x24
   3bd08:	subs	x10, x10, x11
   3bd0c:	str	x10, [x28]
   3bd10:	b.cs	3bd2c <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.hs, b.nlast
   3bd14:	add	x10, x19, x20, lsl #5
   3bd18:	add	x10, x10, #0x8
   3bd1c:	ldr	x11, [x10]
   3bd20:	sub	x12, x11, #0x1
   3bd24:	str	x12, [x10], #8
   3bd28:	cbz	x11, 3bd1c <__gmpn_toom_interpolate_6pts@@Base+0x3e8>
   3bd2c:	ldur	x11, [x29, #-16]
   3bd30:	ldr	x10, [x25, x11]
   3bd34:	subs	x10, x10, x0
   3bd38:	str	x10, [x25, x11]
   3bd3c:	b.cs	3bd60 <__gmpn_toom_interpolate_6pts@@Base+0x42c>  // b.hs, b.nlast
   3bd40:	add	x10, x20, x20, lsl #1
   3bd44:	add	x10, x27, x10
   3bd48:	add	x10, x19, x10, lsl #3
   3bd4c:	add	x10, x10, #0x8
   3bd50:	ldr	x11, [x10]
   3bd54:	sub	x12, x11, #0x1
   3bd58:	str	x12, [x10], #8
   3bd5c:	cbz	x11, 3bd50 <__gmpn_toom_interpolate_6pts@@Base+0x41c>
   3bd60:	ldr	x10, [x22, x21]
   3bd64:	adds	x10, x10, x23
   3bd68:	str	x10, [x22, x21]
   3bd6c:	b.cc	3bd8c <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.lo, b.ul, b.last
   3bd70:	mov	w10, #0x30                  	// #48
   3bd74:	madd	x10, x20, x10, x19
   3bd78:	add	x10, x10, #0x8
   3bd7c:	ldr	x11, [x10]
   3bd80:	adds	x11, x11, #0x1
   3bd84:	str	x11, [x10], #8
   3bd88:	b.cs	3bd7c <__gmpn_toom_interpolate_6pts@@Base+0x448>  // b.hs, b.nlast
   3bd8c:	ldr	x10, [x22, x8]
   3bd90:	add	x9, x9, x10
   3bd94:	str	x9, [x22, x8]
   3bd98:	ldp	x20, x19, [sp, #128]
   3bd9c:	ldp	x22, x21, [sp, #112]
   3bda0:	ldp	x24, x23, [sp, #96]
   3bda4:	ldp	x26, x25, [sp, #80]
   3bda8:	ldp	x28, x27, [sp, #64]
   3bdac:	ldp	x29, x30, [sp, #48]
   3bdb0:	add	sp, sp, #0x90
   3bdb4:	ret
   3bdb8:	mov	x0, x22
   3bdbc:	mov	x1, x22
   3bdc0:	mov	x3, x27
   3bdc4:	bl	cc30 <__gmpn_add_n@plt>
   3bdc8:	mov	x23, x0
   3bdcc:	b	3bc2c <__gmpn_toom_interpolate_6pts@@Base+0x2f8>

000000000003bdd0 <__gmpn_toom_interpolate_7pts@@Base>:
   3bdd0:	sub	sp, sp, #0x80
   3bdd4:	stp	x29, x30, [sp, #32]
   3bdd8:	add	x29, sp, #0x20
   3bddc:	stp	x28, x27, [sp, #48]
   3bde0:	stp	x22, x21, [sp, #96]
   3bde4:	ldr	x21, [x29, #96]
   3bde8:	mov	w27, #0x1                   	// #1
   3bdec:	bfi	x27, x1, #1, #63
   3bdf0:	stp	x26, x25, [sp, #64]
   3bdf4:	stp	x20, x19, [sp, #112]
   3bdf8:	mov	x26, x3
   3bdfc:	mov	w28, w2
   3be00:	mov	x19, x1
   3be04:	mov	x20, x0
   3be08:	lsl	x8, x1, #1
   3be0c:	mov	x0, x6
   3be10:	mov	x1, x6
   3be14:	mov	x2, x5
   3be18:	mov	x3, x27
   3be1c:	stp	x24, x23, [sp, #80]
   3be20:	mov	x24, x7
   3be24:	mov	x23, x6
   3be28:	mov	x22, x5
   3be2c:	mov	x25, x4
   3be30:	str	x8, [sp, #16]
   3be34:	bl	cc30 <__gmpn_add_n@plt>
   3be38:	mov	x0, x26
   3be3c:	str	w28, [sp, #4]
   3be40:	tbnz	w28, #0, 3be58 <__gmpn_toom_interpolate_7pts@@Base+0x88>
   3be44:	mov	x1, x22
   3be48:	mov	x2, x26
   3be4c:	mov	x3, x27
   3be50:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3be54:	b	3be68 <__gmpn_toom_interpolate_7pts@@Base+0x98>
   3be58:	mov	x1, x26
   3be5c:	mov	x2, x22
   3be60:	mov	x3, x27
   3be64:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3be68:	ldr	x28, [sp, #16]
   3be6c:	mov	x0, x22
   3be70:	mov	x1, x22
   3be74:	mov	x2, x27
   3be78:	mov	x3, x20
   3be7c:	mov	x4, x28
   3be80:	bl	d340 <__gmpn_sub@plt>
   3be84:	mov	x0, x22
   3be88:	mov	x1, x22
   3be8c:	mov	x2, x26
   3be90:	mov	x3, x27
   3be94:	bl	c420 <__gmpn_sub_n@plt>
   3be98:	mov	w3, #0x2                   	// #2
   3be9c:	mov	x0, x22
   3bea0:	mov	x1, x22
   3bea4:	mov	x2, x27
   3bea8:	bl	c2f0 <__gmpn_rshift@plt>
   3beac:	mov	w8, #0x30                  	// #48
   3beb0:	madd	x1, x19, x8, x20
   3beb4:	mov	w3, #0x4                   	// #4
   3beb8:	mov	x0, x21
   3bebc:	mov	x2, x24
   3bec0:	str	x1, [sp, #8]
   3bec4:	bl	c2d0 <__gmpn_lshift@plt>
   3bec8:	str	x0, [x21, x24, lsl #3]
   3becc:	add	x4, x24, #0x1
   3bed0:	mov	x0, x22
   3bed4:	mov	x1, x22
   3bed8:	mov	x2, x27
   3bedc:	mov	x3, x21
   3bee0:	stur	x24, [x29, #-8]
   3bee4:	bl	d340 <__gmpn_sub@plt>
   3bee8:	ldr	w8, [sp, #4]
   3beec:	add	x24, x20, x28, lsl #3
   3bef0:	tbnz	w8, #1, 3bf0c <__gmpn_toom_interpolate_7pts@@Base+0x13c>
   3bef4:	mov	x0, x25
   3bef8:	mov	x1, x24
   3befc:	mov	x2, x25
   3bf00:	mov	x3, x27
   3bf04:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3bf08:	b	3bf20 <__gmpn_toom_interpolate_7pts@@Base+0x150>
   3bf0c:	mov	x0, x25
   3bf10:	mov	x1, x25
   3bf14:	mov	x2, x24
   3bf18:	mov	x3, x27
   3bf1c:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3bf20:	mov	x0, x24
   3bf24:	mov	x1, x24
   3bf28:	mov	x2, x25
   3bf2c:	mov	x3, x27
   3bf30:	bl	c420 <__gmpn_sub_n@plt>
   3bf34:	mov	w3, #0x41                  	// #65
   3bf38:	mov	x0, x23
   3bf3c:	mov	x1, x24
   3bf40:	mov	x2, x27
   3bf44:	bl	cba0 <__gmpn_submul_1@plt>
   3bf48:	ldr	x3, [sp, #8]
   3bf4c:	ldur	x4, [x29, #-8]
   3bf50:	mov	x0, x24
   3bf54:	mov	x1, x24
   3bf58:	mov	x2, x27
   3bf5c:	bl	d340 <__gmpn_sub@plt>
   3bf60:	ldr	x28, [sp, #16]
   3bf64:	mov	x0, x24
   3bf68:	mov	x1, x24
   3bf6c:	mov	x2, x27
   3bf70:	mov	x3, x20
   3bf74:	mov	x4, x28
   3bf78:	bl	d340 <__gmpn_sub@plt>
   3bf7c:	mov	w3, #0x2d                  	// #45
   3bf80:	mov	x0, x23
   3bf84:	mov	x1, x24
   3bf88:	mov	x2, x27
   3bf8c:	bl	d5e0 <__gmpn_addmul_1@plt>
   3bf90:	mov	w3, #0x1                   	// #1
   3bf94:	mov	x0, x23
   3bf98:	mov	x1, x23
   3bf9c:	mov	x2, x27
   3bfa0:	bl	c2f0 <__gmpn_rshift@plt>
   3bfa4:	mov	x0, x22
   3bfa8:	mov	x1, x22
   3bfac:	mov	x2, x24
   3bfb0:	mov	x3, x27
   3bfb4:	bl	c420 <__gmpn_sub_n@plt>
   3bfb8:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   3bfbc:	mov	x0, x22
   3bfc0:	mov	x1, x22
   3bfc4:	mov	x2, x27
   3bfc8:	mov	x4, xzr
   3bfcc:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3bfd0:	mov	x0, x24
   3bfd4:	mov	x1, x24
   3bfd8:	mov	x2, x22
   3bfdc:	mov	x3, x27
   3bfe0:	bl	c420 <__gmpn_sub_n@plt>
   3bfe4:	mov	x0, x26
   3bfe8:	mov	x1, x23
   3bfec:	mov	x2, x26
   3bff0:	mov	x3, x27
   3bff4:	bl	c420 <__gmpn_sub_n@plt>
   3bff8:	mov	w3, #0x3                   	// #3
   3bffc:	mov	x0, x21
   3c000:	mov	x1, x25
   3c004:	mov	x2, x27
   3c008:	bl	c2d0 <__gmpn_lshift@plt>
   3c00c:	mov	x0, x23
   3c010:	mov	x1, x23
   3c014:	mov	x2, x21
   3c018:	mov	x3, x27
   3c01c:	bl	c420 <__gmpn_sub_n@plt>
   3c020:	mov	x4, #0x8e39                	// #36409
   3c024:	movk	x4, #0x38e3, lsl #16
   3c028:	movk	x4, #0xe38e, lsl #32
   3c02c:	mov	w3, #0x9                   	// #9
   3c030:	movk	x4, #0x8e38, lsl #48
   3c034:	mov	x0, x23
   3c038:	mov	x1, x23
   3c03c:	mov	x2, x27
   3c040:	mov	w5, wzr
   3c044:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3c048:	mov	x0, x25
   3c04c:	mov	x1, x25
   3c050:	mov	x2, x23
   3c054:	mov	x3, x27
   3c058:	bl	c420 <__gmpn_sub_n@plt>
   3c05c:	mov	x3, #0x1111111111111111    	// #1229782938247303441
   3c060:	mov	x0, x26
   3c064:	mov	x1, x26
   3c068:	mov	x2, x27
   3c06c:	mov	x4, xzr
   3c070:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3c074:	mov	x0, x26
   3c078:	mov	x1, x26
   3c07c:	mov	x2, x23
   3c080:	mov	x3, x27
   3c084:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3c088:	lsl	x21, x28, #3
   3c08c:	ldr	x8, [x26, x21]
   3c090:	mov	x0, x23
   3c094:	mov	x1, x23
   3c098:	mov	x2, x26
   3c09c:	and	x8, x8, #0x7fffffffffffffff
   3c0a0:	mov	x3, x27
   3c0a4:	str	x8, [x26, x21]
   3c0a8:	bl	c420 <__gmpn_sub_n@plt>
   3c0ac:	lsl	x28, x19, #3
   3c0b0:	add	x0, x20, x28
   3c0b4:	mov	x1, x0
   3c0b8:	mov	x2, x26
   3c0bc:	mov	x3, x27
   3c0c0:	bl	cc30 <__gmpn_add_n@plt>
   3c0c4:	add	x8, x24, x28
   3c0c8:	ldr	x9, [x8, #8]
   3c0cc:	adds	x9, x9, x0
   3c0d0:	str	x9, [x8, #8]
   3c0d4:	b.cc	3c0f4 <__gmpn_toom_interpolate_7pts@@Base+0x324>  // b.lo, b.ul, b.last
   3c0d8:	mov	w8, #0x18                  	// #24
   3c0dc:	madd	x8, x19, x8, x20
   3c0e0:	add	x8, x8, #0x10
   3c0e4:	ldr	x9, [x8]
   3c0e8:	adds	x9, x9, #0x1
   3c0ec:	str	x9, [x8], #8
   3c0f0:	b.cs	3c0e4 <__gmpn_toom_interpolate_7pts@@Base+0x314>  // b.hs, b.nlast
   3c0f4:	mov	w8, #0x18                  	// #24
   3c0f8:	madd	x0, x19, x8, x20
   3c0fc:	mov	x1, x0
   3c100:	mov	x2, x25
   3c104:	mov	x3, x19
   3c108:	bl	cc30 <__gmpn_add_n@plt>
   3c10c:	add	x1, x25, x19, lsl #3
   3c110:	ldr	x8, [x24, x21]
   3c114:	ldr	x9, [x1]
   3c118:	add	x8, x8, x0
   3c11c:	add	x8, x9, x8
   3c120:	str	x8, [x1]
   3c124:	ldr	x9, [x24, x21]
   3c128:	add	x9, x9, x0
   3c12c:	cmp	x8, x9
   3c130:	b.cs	3c148 <__gmpn_toom_interpolate_7pts@@Base+0x378>  // b.hs, b.nlast
   3c134:	add	x8, x1, #0x8
   3c138:	ldr	x9, [x8]
   3c13c:	adds	x9, x9, #0x1
   3c140:	str	x9, [x8], #8
   3c144:	b.cs	3c138 <__gmpn_toom_interpolate_7pts@@Base+0x368>  // b.hs, b.nlast
   3c148:	add	x0, x20, x19, lsl #5
   3c14c:	mov	x2, x22
   3c150:	mov	x3, x19
   3c154:	bl	cc30 <__gmpn_add_n@plt>
   3c158:	add	x1, x22, x19, lsl #3
   3c15c:	ldr	x8, [x25, x21]
   3c160:	ldr	x9, [x1]
   3c164:	ldur	x24, [x29, #-8]
   3c168:	add	x8, x8, x0
   3c16c:	add	x8, x9, x8
   3c170:	str	x8, [x1]
   3c174:	ldr	x9, [x25, x21]
   3c178:	add	x9, x9, x0
   3c17c:	cmp	x8, x9
   3c180:	b.cs	3c198 <__gmpn_toom_interpolate_7pts@@Base+0x3c8>  // b.hs, b.nlast
   3c184:	add	x8, x1, #0x8
   3c188:	ldr	x9, [x8]
   3c18c:	adds	x9, x9, #0x1
   3c190:	str	x9, [x8], #8
   3c194:	b.cs	3c188 <__gmpn_toom_interpolate_7pts@@Base+0x3b8>  // b.hs, b.nlast
   3c198:	mov	w8, #0x28                  	// #40
   3c19c:	madd	x0, x19, x8, x20
   3c1a0:	mov	x2, x23
   3c1a4:	mov	x3, x19
   3c1a8:	bl	cc30 <__gmpn_add_n@plt>
   3c1ac:	add	x2, x23, x19, lsl #3
   3c1b0:	ldr	x8, [x22, x21]
   3c1b4:	ldr	x9, [x2]
   3c1b8:	add	x8, x8, x0
   3c1bc:	add	x8, x9, x8
   3c1c0:	str	x8, [x2]
   3c1c4:	ldr	x9, [x22, x21]
   3c1c8:	add	x9, x9, x0
   3c1cc:	cmp	x8, x9
   3c1d0:	b.cs	3c1e8 <__gmpn_toom_interpolate_7pts@@Base+0x418>  // b.hs, b.nlast
   3c1d4:	add	x8, x2, #0x8
   3c1d8:	ldr	x9, [x8]
   3c1dc:	adds	x9, x9, #0x1
   3c1e0:	str	x9, [x8], #8
   3c1e4:	b.cs	3c1d8 <__gmpn_toom_interpolate_7pts@@Base+0x408>  // b.hs, b.nlast
   3c1e8:	add	x3, x19, #0x1
   3c1ec:	cmp	x3, x24
   3c1f0:	b.ge	3c238 <__gmpn_toom_interpolate_7pts@@Base+0x468>  // b.tcont
   3c1f4:	ldr	x0, [sp, #8]
   3c1f8:	mov	x1, x0
   3c1fc:	bl	cc30 <__gmpn_add_n@plt>
   3c200:	mov	w8, #0x38                  	// #56
   3c204:	madd	x8, x19, x8, x20
   3c208:	ldr	x9, [x8, #8]
   3c20c:	adds	x9, x9, x0
   3c210:	str	x9, [x8, #8]
   3c214:	b.cc	3c248 <__gmpn_toom_interpolate_7pts@@Base+0x478>  // b.lo, b.ul, b.last
   3c218:	mov	w8, #0x38                  	// #56
   3c21c:	madd	x8, x19, x8, x20
   3c220:	add	x8, x8, #0x10
   3c224:	ldr	x9, [x8]
   3c228:	adds	x9, x9, #0x1
   3c22c:	str	x9, [x8], #8
   3c230:	b.cs	3c224 <__gmpn_toom_interpolate_7pts@@Base+0x454>  // b.hs, b.nlast
   3c234:	b	3c248 <__gmpn_toom_interpolate_7pts@@Base+0x478>
   3c238:	ldr	x0, [sp, #8]
   3c23c:	mov	x3, x24
   3c240:	mov	x1, x0
   3c244:	bl	cc30 <__gmpn_add_n@plt>
   3c248:	ldp	x20, x19, [sp, #112]
   3c24c:	ldp	x22, x21, [sp, #96]
   3c250:	ldp	x24, x23, [sp, #80]
   3c254:	ldp	x26, x25, [sp, #64]
   3c258:	ldp	x28, x27, [sp, #48]
   3c25c:	ldp	x29, x30, [sp, #32]
   3c260:	add	sp, sp, #0x80
   3c264:	ret

000000000003c268 <__gmpn_toom_interpolate_8pts@@Base>:
   3c268:	sub	sp, sp, #0x90
   3c26c:	stp	x29, x30, [sp, #48]
   3c270:	stp	x28, x27, [sp, #64]
   3c274:	stp	x26, x25, [sp, #80]
   3c278:	stp	x24, x23, [sp, #96]
   3c27c:	stp	x22, x21, [sp, #112]
   3c280:	stp	x20, x19, [sp, #128]
   3c284:	add	x25, x2, x1, lsl #3
   3c288:	ldr	x8, [x0]
   3c28c:	ldr	x9, [x25]
   3c290:	mov	w10, #0x38                  	// #56
   3c294:	add	x29, sp, #0x30
   3c298:	mov	x23, x5
   3c29c:	sub	x8, x9, x8, lsr #4
   3c2a0:	str	x8, [x25]
   3c2a4:	ldr	x8, [x0]
   3c2a8:	mov	x26, x3
   3c2ac:	mov	x21, x2
   3c2b0:	mov	x19, x1
   3c2b4:	mov	x20, x0
   3c2b8:	cmp	x9, x8, lsr #4
   3c2bc:	madd	x8, x1, x10, x0
   3c2c0:	stur	x4, [x29, #-8]
   3c2c4:	str	x8, [sp, #24]
   3c2c8:	b.cs	3c2e0 <__gmpn_toom_interpolate_8pts@@Base+0x78>  // b.hs, b.nlast
   3c2cc:	add	x8, x25, #0x8
   3c2d0:	ldr	x9, [x8]
   3c2d4:	sub	x10, x9, #0x1
   3c2d8:	str	x10, [x8], #8
   3c2dc:	cbz	x9, 3c2d0 <__gmpn_toom_interpolate_8pts@@Base+0x68>
   3c2e0:	add	x8, x19, x19, lsl #1
   3c2e4:	str	x8, [sp, #16]
   3c2e8:	lsl	x8, x19, #1
   3c2ec:	add	x22, x20, #0x8
   3c2f0:	sub	x24, x8, #0x1
   3c2f4:	mov	w3, #0x3c                  	// #60
   3c2f8:	mov	x0, x25
   3c2fc:	mov	x1, x22
   3c300:	mov	x2, x24
   3c304:	mov	x4, x23
   3c308:	stur	x8, [x29, #-16]
   3c30c:	bl	3c784 <__gmpn_toom_interpolate_8pts@@Base+0x51c>
   3c310:	add	x8, x25, x19, lsl #4
   3c314:	ldur	x9, [x8, #-8]
   3c318:	str	x25, [sp, #8]
   3c31c:	subs	x9, x9, x0
   3c320:	stur	x9, [x8, #-8]
   3c324:	b.cs	3c338 <__gmpn_toom_interpolate_8pts@@Base+0xd0>  // b.hs, b.nlast
   3c328:	ldr	x9, [x8]
   3c32c:	sub	x10, x9, #0x1
   3c330:	str	x10, [x8], #8
   3c334:	cbz	x9, 3c328 <__gmpn_toom_interpolate_8pts@@Base+0xc0>
   3c338:	ldur	x25, [x29, #-8]
   3c33c:	ldp	x8, x1, [sp, #16]
   3c340:	mov	w3, #0xc                   	// #12
   3c344:	mov	x0, x21
   3c348:	mov	x2, x25
   3c34c:	mov	x4, x23
   3c350:	add	x28, x20, x8, lsl #3
   3c354:	bl	3c784 <__gmpn_toom_interpolate_8pts@@Base+0x51c>
   3c358:	lsl	x25, x25, #3
   3c35c:	ldr	x8, [x21, x25]
   3c360:	subs	x8, x8, x0
   3c364:	str	x8, [x21, x25]
   3c368:	b.cs	3c388 <__gmpn_toom_interpolate_8pts@@Base+0x120>  // b.hs, b.nlast
   3c36c:	ldur	x8, [x29, #-8]
   3c370:	add	x8, x21, x8, lsl #3
   3c374:	add	x8, x8, #0x8
   3c378:	ldr	x9, [x8]
   3c37c:	sub	x10, x9, #0x1
   3c380:	str	x10, [x8], #8
   3c384:	cbz	x9, 3c378 <__gmpn_toom_interpolate_8pts@@Base+0x110>
   3c388:	add	x27, x28, x19, lsl #3
   3c38c:	ldr	x8, [x20]
   3c390:	ldr	x9, [x27]
   3c394:	sub	x8, x9, x8, lsr #2
   3c398:	str	x8, [x27]
   3c39c:	ldr	x8, [x20]
   3c3a0:	cmp	x9, x8, lsr #2
   3c3a4:	b.cs	3c3c0 <__gmpn_toom_interpolate_8pts@@Base+0x158>  // b.hs, b.nlast
   3c3a8:	add	x8, x20, x19, lsl #5
   3c3ac:	add	x8, x8, #0x8
   3c3b0:	ldr	x9, [x8]
   3c3b4:	sub	x10, x9, #0x1
   3c3b8:	str	x10, [x8], #8
   3c3bc:	cbz	x9, 3c3b0 <__gmpn_toom_interpolate_8pts@@Base+0x148>
   3c3c0:	mov	w3, #0x3e                  	// #62
   3c3c4:	mov	x0, x27
   3c3c8:	mov	x1, x22
   3c3cc:	mov	x2, x24
   3c3d0:	mov	x4, x23
   3c3d4:	bl	3c784 <__gmpn_toom_interpolate_8pts@@Base+0x51c>
   3c3d8:	ldur	x8, [x29, #-16]
   3c3dc:	add	x8, x27, x8, lsl #3
   3c3e0:	ldur	x9, [x8, #-8]
   3c3e4:	subs	x9, x9, x0
   3c3e8:	stur	x9, [x8, #-8]
   3c3ec:	b.cs	3c400 <__gmpn_toom_interpolate_8pts@@Base+0x198>  // b.hs, b.nlast
   3c3f0:	ldr	x9, [x8]
   3c3f4:	sub	x10, x9, #0x1
   3c3f8:	str	x10, [x8], #8
   3c3fc:	cbz	x9, 3c3f0 <__gmpn_toom_interpolate_8pts@@Base+0x188>
   3c400:	ldr	x1, [sp, #24]
   3c404:	ldur	x2, [x29, #-8]
   3c408:	mov	w3, #0x6                   	// #6
   3c40c:	mov	x0, x28
   3c410:	mov	x4, x23
   3c414:	bl	3c784 <__gmpn_toom_interpolate_8pts@@Base+0x51c>
   3c418:	ldr	x8, [x28, x25]
   3c41c:	subs	x8, x8, x0
   3c420:	str	x8, [x28, x25]
   3c424:	b.cs	3c44c <__gmpn_toom_interpolate_8pts@@Base+0x1e4>  // b.hs, b.nlast
   3c428:	ldur	x9, [x29, #-8]
   3c42c:	add	x8, x19, x19, lsl #1
   3c430:	add	x8, x9, x8
   3c434:	add	x8, x20, x8, lsl #3
   3c438:	add	x8, x8, #0x8
   3c43c:	ldr	x9, [x8]
   3c440:	sub	x10, x9, #0x1
   3c444:	str	x10, [x8], #8
   3c448:	cbz	x9, 3c43c <__gmpn_toom_interpolate_8pts@@Base+0x1d4>
   3c44c:	ldur	x3, [x29, #-16]
   3c450:	add	x23, x26, x19, lsl #3
   3c454:	mov	x0, x23
   3c458:	mov	x1, x23
   3c45c:	mov	x2, x20
   3c460:	bl	c420 <__gmpn_sub_n@plt>
   3c464:	ldp	x8, x2, [sp, #16]
   3c468:	ldur	x3, [x29, #-8]
   3c46c:	mov	x1, x26
   3c470:	lsl	x9, x8, #3
   3c474:	ldr	x8, [x26, x9]
   3c478:	str	x9, [sp]
   3c47c:	sub	x8, x8, x0
   3c480:	mov	x0, x26
   3c484:	str	x8, [x26, x9]
   3c488:	bl	c420 <__gmpn_sub_n@plt>
   3c48c:	ldr	x8, [x26, x25]
   3c490:	subs	x8, x8, x0
   3c494:	str	x8, [x26, x25]
   3c498:	b.cs	3c4b8 <__gmpn_toom_interpolate_8pts@@Base+0x250>  // b.hs, b.nlast
   3c49c:	ldur	x8, [x29, #-8]
   3c4a0:	add	x8, x26, x8, lsl #3
   3c4a4:	add	x8, x8, #0x8
   3c4a8:	ldr	x9, [x8]
   3c4ac:	sub	x10, x9, #0x1
   3c4b0:	str	x10, [x8], #8
   3c4b4:	cbz	x9, 3c4a8 <__gmpn_toom_interpolate_8pts@@Base+0x240>
   3c4b8:	ldr	x8, [sp, #16]
   3c4bc:	mov	x0, x21
   3c4c0:	mov	x1, x21
   3c4c4:	mov	x2, x28
   3c4c8:	add	x22, x8, #0x1
   3c4cc:	mov	x3, x22
   3c4d0:	bl	c420 <__gmpn_sub_n@plt>
   3c4d4:	mov	w3, #0x2                   	// #2
   3c4d8:	mov	x0, x21
   3c4dc:	mov	x1, x21
   3c4e0:	mov	x2, x22
   3c4e4:	bl	c2f0 <__gmpn_rshift@plt>
   3c4e8:	mov	x0, x28
   3c4ec:	mov	x1, x28
   3c4f0:	mov	x2, x26
   3c4f4:	mov	x3, x22
   3c4f8:	bl	c420 <__gmpn_sub_n@plt>
   3c4fc:	mov	x0, x21
   3c500:	mov	x1, x21
   3c504:	mov	x2, x28
   3c508:	mov	x3, x22
   3c50c:	bl	c420 <__gmpn_sub_n@plt>
   3c510:	mov	x4, #0x4fa5                	// #20389
   3c514:	movk	x4, #0xa4fa, lsl #16
   3c518:	movk	x4, #0xfa4f, lsl #32
   3c51c:	mov	w3, #0x2d                  	// #45
   3c520:	movk	x4, #0x4fa4, lsl #48
   3c524:	mov	x0, x21
   3c528:	mov	x1, x21
   3c52c:	mov	x2, x22
   3c530:	mov	w5, wzr
   3c534:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3c538:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   3c53c:	mov	x0, x28
   3c540:	mov	x1, x28
   3c544:	mov	x2, x22
   3c548:	mov	x4, xzr
   3c54c:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3c550:	mov	x0, x28
   3c554:	mov	x1, x28
   3c558:	mov	x2, x21
   3c55c:	mov	x3, x22
   3c560:	bl	c2b0 <__gmpn_sublsh2_n@plt>
   3c564:	add	x22, x20, x19, lsl #3
   3c568:	mov	x0, x22
   3c56c:	mov	x1, x22
   3c570:	mov	x2, x26
   3c574:	mov	x3, x19
   3c578:	bl	cc30 <__gmpn_add_n@plt>
   3c57c:	mov	x24, x0
   3c580:	mov	x0, x22
   3c584:	mov	x1, x22
   3c588:	mov	x2, x28
   3c58c:	mov	x3, x19
   3c590:	bl	c420 <__gmpn_sub_n@plt>
   3c594:	ldr	x25, [sp, #8]
   3c598:	sub	x8, x24, x0
   3c59c:	cmp	x8, #0x1
   3c5a0:	b.lt	3c5bc <__gmpn_toom_interpolate_8pts@@Base+0x354>  // b.tstop
   3c5a4:	mov	x8, x23
   3c5a8:	ldr	x9, [x8]
   3c5ac:	adds	x9, x9, #0x1
   3c5b0:	str	x9, [x8], #8
   3c5b4:	b.cs	3c5a8 <__gmpn_toom_interpolate_8pts@@Base+0x340>  // b.hs, b.nlast
   3c5b8:	mov	x8, xzr
   3c5bc:	ldur	x9, [x29, #-16]
   3c5c0:	neg	x4, x8
   3c5c4:	mov	x1, x23
   3c5c8:	mov	x2, x27
   3c5cc:	lsl	x22, x9, #3
   3c5d0:	add	x0, x20, x22
   3c5d4:	mov	x3, x19
   3c5d8:	bl	c8f0 <__gmpn_sub_nc@plt>
   3c5dc:	add	x2, x26, x22
   3c5e0:	ldr	x8, [x2]
   3c5e4:	subs	x8, x8, x0
   3c5e8:	str	x8, [x2]
   3c5ec:	b.cs	3c608 <__gmpn_toom_interpolate_8pts@@Base+0x3a0>  // b.hs, b.nlast
   3c5f0:	add	x8, x26, x19, lsl #4
   3c5f4:	add	x8, x8, #0x8
   3c5f8:	ldr	x9, [x8]
   3c5fc:	sub	x10, x9, #0x1
   3c600:	str	x10, [x8], #8
   3c604:	cbz	x9, 3c5f8 <__gmpn_toom_interpolate_8pts@@Base+0x390>
   3c608:	add	x22, x19, #0x1
   3c60c:	mov	x0, x28
   3c610:	mov	x1, x28
   3c614:	mov	x3, x22
   3c618:	bl	cc30 <__gmpn_add_n@plt>
   3c61c:	ldur	x8, [x29, #-16]
   3c620:	mov	x23, x0
   3c624:	mov	x2, x21
   3c628:	mov	x3, x19
   3c62c:	add	x24, x28, x8, lsl #3
   3c630:	mov	x0, x24
   3c634:	mov	x1, x24
   3c638:	bl	cc30 <__gmpn_add_n@plt>
   3c63c:	ldr	x26, [sp]
   3c640:	mov	x1, x28
   3c644:	mov	x2, x24
   3c648:	mov	x3, x22
   3c64c:	ldr	x8, [x28, x26]
   3c650:	add	x8, x8, x0
   3c654:	mov	x0, x28
   3c658:	str	x8, [x28, x26]
   3c65c:	bl	c420 <__gmpn_sub_n@plt>
   3c660:	subs	x8, x23, x0
   3c664:	b.mi	3c768 <__gmpn_toom_interpolate_8pts@@Base+0x500>  // b.first
   3c668:	ldr	x9, [x27, #8]
   3c66c:	adds	x8, x9, x8
   3c670:	str	x8, [x27, #8]
   3c674:	b.cc	3c690 <__gmpn_toom_interpolate_8pts@@Base+0x428>  // b.lo, b.ul, b.last
   3c678:	add	x8, x20, x19, lsl #5
   3c67c:	add	x8, x8, #0x10
   3c680:	ldr	x9, [x8]
   3c684:	adds	x9, x9, #0x1
   3c688:	str	x9, [x8], #8
   3c68c:	b.cs	3c680 <__gmpn_toom_interpolate_8pts@@Base+0x418>  // b.hs, b.nlast
   3c690:	ldur	x22, [x29, #-16]
   3c694:	add	x0, x20, x19, lsl #5
   3c698:	mov	x1, x27
   3c69c:	mov	x2, x25
   3c6a0:	orr	x3, x22, #0x1
   3c6a4:	bl	c420 <__gmpn_sub_n@plt>
   3c6a8:	mov	w8, #0x30                  	// #48
   3c6ac:	madd	x0, x19, x8, x20
   3c6b0:	ldr	x3, [x0]
   3c6b4:	mov	x1, x25
   3c6b8:	mov	x2, x19
   3c6bc:	bl	c150 <__gmpn_add_1@plt>
   3c6c0:	add	x2, x21, x22, lsl #3
   3c6c4:	ldr	x8, [x2]
   3c6c8:	adds	x8, x8, x0
   3c6cc:	str	x8, [x2]
   3c6d0:	b.cc	3c6ec <__gmpn_toom_interpolate_8pts@@Base+0x484>  // b.lo, b.ul, b.last
   3c6d4:	add	x8, x21, x19, lsl #4
   3c6d8:	add	x8, x8, #0x8
   3c6dc:	ldr	x9, [x8]
   3c6e0:	adds	x9, x9, #0x1
   3c6e4:	str	x9, [x8], #8
   3c6e8:	b.cs	3c6dc <__gmpn_toom_interpolate_8pts@@Base+0x474>  // b.hs, b.nlast
   3c6ec:	ldr	x0, [sp, #24]
   3c6f0:	mov	x3, x19
   3c6f4:	mov	x1, x0
   3c6f8:	bl	cc30 <__gmpn_add_n@plt>
   3c6fc:	ldur	x8, [x29, #-8]
   3c700:	cmp	x8, x19
   3c704:	b.eq	3c748 <__gmpn_toom_interpolate_8pts@@Base+0x4e0>  // b.none
   3c708:	lsl	x8, x19, #6
   3c70c:	ldr	x9, [x21, x26]
   3c710:	ldr	x10, [x20, x8]
   3c714:	add	x9, x9, x0
   3c718:	add	x9, x10, x9
   3c71c:	str	x9, [x20, x8]
   3c720:	ldr	x8, [x21, x26]
   3c724:	add	x8, x8, x0
   3c728:	cmp	x9, x8
   3c72c:	b.cs	3c748 <__gmpn_toom_interpolate_8pts@@Base+0x4e0>  // b.hs, b.nlast
   3c730:	add	x8, x20, x19, lsl #6
   3c734:	add	x8, x8, #0x8
   3c738:	ldr	x9, [x8]
   3c73c:	adds	x9, x9, #0x1
   3c740:	str	x9, [x8], #8
   3c744:	b.cs	3c738 <__gmpn_toom_interpolate_8pts@@Base+0x4d0>  // b.hs, b.nlast
   3c748:	ldp	x20, x19, [sp, #128]
   3c74c:	ldp	x22, x21, [sp, #112]
   3c750:	ldp	x24, x23, [sp, #96]
   3c754:	ldp	x26, x25, [sp, #80]
   3c758:	ldp	x28, x27, [sp, #64]
   3c75c:	ldp	x29, x30, [sp, #48]
   3c760:	add	sp, sp, #0x90
   3c764:	ret
   3c768:	add	x8, x20, x19, lsl #5
   3c76c:	add	x8, x8, #0x8
   3c770:	ldr	x9, [x8]
   3c774:	sub	x10, x9, #0x1
   3c778:	str	x10, [x8], #8
   3c77c:	cbz	x9, 3c770 <__gmpn_toom_interpolate_8pts@@Base+0x508>
   3c780:	b	3c690 <__gmpn_toom_interpolate_8pts@@Base+0x428>
   3c784:	stp	x29, x30, [sp, #-48]!
   3c788:	stp	x22, x21, [sp, #16]
   3c78c:	mov	x21, x0
   3c790:	mov	x0, x4
   3c794:	stp	x20, x19, [sp, #32]
   3c798:	mov	x29, sp
   3c79c:	mov	x19, x4
   3c7a0:	mov	x20, x2
   3c7a4:	bl	c2d0 <__gmpn_lshift@plt>
   3c7a8:	mov	x22, x0
   3c7ac:	mov	x0, x21
   3c7b0:	mov	x1, x21
   3c7b4:	mov	x2, x19
   3c7b8:	mov	x3, x20
   3c7bc:	bl	c420 <__gmpn_sub_n@plt>
   3c7c0:	add	x0, x0, x22
   3c7c4:	ldp	x20, x19, [sp, #32]
   3c7c8:	ldp	x22, x21, [sp, #16]
   3c7cc:	ldp	x29, x30, [sp], #48
   3c7d0:	ret

000000000003c7d4 <__gmpn_toom_interpolate_12pts@@Base>:
   3c7d4:	sub	sp, sp, #0xb0
   3c7d8:	stp	x29, x30, [sp, #80]
   3c7dc:	stp	x28, x27, [sp, #96]
   3c7e0:	stp	x26, x25, [sp, #112]
   3c7e4:	stp	x24, x23, [sp, #128]
   3c7e8:	stp	x22, x21, [sp, #144]
   3c7ec:	stp	x20, x19, [sp, #160]
   3c7f0:	add	x29, sp, #0x50
   3c7f4:	mov	x22, x7
   3c7f8:	mov	x28, x5
   3c7fc:	mov	x20, x4
   3c800:	mov	x25, x1
   3c804:	mov	x19, x0
   3c808:	add	x27, x4, x4, lsl #1
   3c80c:	lsl	x24, x4, #3
   3c810:	stp	x2, x3, [x29, #-24]
   3c814:	stur	x27, [x29, #-32]
   3c818:	str	w6, [sp, #28]
   3c81c:	str	x24, [sp, #40]
   3c820:	cbz	w6, 3ca08 <__gmpn_toom_interpolate_12pts@@Base+0x234>
   3c824:	ldur	x23, [x29, #-24]
   3c828:	mov	w8, #0x58                  	// #88
   3c82c:	madd	x21, x20, x8, x19
   3c830:	mov	x2, x21
   3c834:	mov	x0, x23
   3c838:	mov	x1, x23
   3c83c:	mov	x3, x28
   3c840:	bl	c420 <__gmpn_sub_n@plt>
   3c844:	lsl	x24, x28, #3
   3c848:	ldr	x8, [x23, x24]
   3c84c:	subs	x8, x8, x0
   3c850:	str	x8, [x23, x24]
   3c854:	b.cs	3c874 <__gmpn_toom_interpolate_12pts@@Base+0xa0>  // b.hs, b.nlast
   3c858:	ldur	x8, [x29, #-24]
   3c85c:	add	x8, x8, x28, lsl #3
   3c860:	add	x8, x8, #0x8
   3c864:	ldr	x9, [x8]
   3c868:	sub	x10, x9, #0x1
   3c86c:	str	x10, [x8], #8
   3c870:	cbz	x9, 3c864 <__gmpn_toom_interpolate_12pts@@Base+0x90>
   3c874:	mov	w8, #0x38                  	// #56
   3c878:	madd	x23, x20, x8, x19
   3c87c:	mov	w3, #0xa                   	// #10
   3c880:	mov	x0, x23
   3c884:	mov	x1, x21
   3c888:	mov	x2, x28
   3c88c:	mov	x4, x22
   3c890:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3c894:	ldr	x8, [x23, x24]
   3c898:	subs	x8, x8, x0
   3c89c:	str	x8, [x23, x24]
   3c8a0:	b.cs	3c8c8 <__gmpn_toom_interpolate_12pts@@Base+0xf4>  // b.hs, b.nlast
   3c8a4:	ldr	x8, [sp, #40]
   3c8a8:	sub	x8, x8, x20
   3c8ac:	add	x8, x28, x8
   3c8b0:	add	x8, x19, x8, lsl #3
   3c8b4:	add	x8, x8, #0x8
   3c8b8:	ldr	x9, [x8]
   3c8bc:	sub	x10, x9, #0x1
   3c8c0:	str	x10, [x8], #8
   3c8c4:	cbz	x9, 3c8b8 <__gmpn_toom_interpolate_12pts@@Base+0xe4>
   3c8c8:	ldur	x10, [x29, #-16]
   3c8cc:	ldr	x8, [x21]
   3c8d0:	ldr	x9, [x10]
   3c8d4:	sub	x8, x9, x8, lsr #2
   3c8d8:	str	x8, [x10]
   3c8dc:	ldr	x8, [x21]
   3c8e0:	cmp	x9, x8, lsr #2
   3c8e4:	b.cs	3c900 <__gmpn_toom_interpolate_12pts@@Base+0x12c>  // b.hs, b.nlast
   3c8e8:	ldur	x8, [x29, #-16]
   3c8ec:	add	x8, x8, #0x8
   3c8f0:	ldr	x9, [x8]
   3c8f4:	sub	x10, x9, #0x1
   3c8f8:	str	x10, [x8], #8
   3c8fc:	cbz	x9, 3c8f0 <__gmpn_toom_interpolate_12pts@@Base+0x11c>
   3c900:	ldur	x23, [x29, #-16]
   3c904:	sub	x26, x28, #0x1
   3c908:	add	x1, x21, #0x8
   3c90c:	mov	w3, #0x3e                  	// #62
   3c910:	mov	x0, x23
   3c914:	mov	x2, x26
   3c918:	mov	x4, x22
   3c91c:	stur	x1, [x29, #-8]
   3c920:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3c924:	add	x8, x23, x28, lsl #3
   3c928:	ldur	x9, [x8, #-8]
   3c92c:	subs	x9, x9, x0
   3c930:	stur	x9, [x8, #-8]
   3c934:	b.cs	3c948 <__gmpn_toom_interpolate_12pts@@Base+0x174>  // b.hs, b.nlast
   3c938:	ldr	x9, [x8]
   3c93c:	sub	x10, x9, #0x1
   3c940:	str	x10, [x8], #8
   3c944:	cbz	x9, 3c938 <__gmpn_toom_interpolate_12pts@@Base+0x164>
   3c948:	mov	w3, #0x14                  	// #20
   3c94c:	mov	x0, x25
   3c950:	mov	x1, x21
   3c954:	mov	x2, x28
   3c958:	mov	x4, x22
   3c95c:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3c960:	ldr	x8, [x25, x24]
   3c964:	subs	x8, x8, x0
   3c968:	str	x8, [x25, x24]
   3c96c:	b.cs	3c988 <__gmpn_toom_interpolate_12pts@@Base+0x1b4>  // b.hs, b.nlast
   3c970:	add	x8, x25, x28, lsl #3
   3c974:	add	x8, x8, #0x8
   3c978:	ldr	x9, [x8]
   3c97c:	sub	x10, x9, #0x1
   3c980:	str	x10, [x8], #8
   3c984:	cbz	x9, 3c978 <__gmpn_toom_interpolate_12pts@@Base+0x1a4>
   3c988:	add	x27, x19, x27, lsl #3
   3c98c:	ldr	x8, [x21]
   3c990:	ldr	x9, [x27]
   3c994:	ldr	x24, [sp, #40]
   3c998:	sub	x8, x9, x8, lsr #4
   3c99c:	str	x8, [x27]
   3c9a0:	ldr	x8, [x21]
   3c9a4:	cmp	x9, x8, lsr #4
   3c9a8:	b.cs	3c9c8 <__gmpn_toom_interpolate_12pts@@Base+0x1f4>  // b.hs, b.nlast
   3c9ac:	mov	w8, #0x18                  	// #24
   3c9b0:	madd	x8, x20, x8, x19
   3c9b4:	add	x8, x8, #0x8
   3c9b8:	ldr	x9, [x8]
   3c9bc:	sub	x10, x9, #0x1
   3c9c0:	str	x10, [x8], #8
   3c9c4:	cbz	x9, 3c9b8 <__gmpn_toom_interpolate_12pts@@Base+0x1e4>
   3c9c8:	ldur	x1, [x29, #-8]
   3c9cc:	mov	w3, #0x3c                  	// #60
   3c9d0:	mov	x0, x27
   3c9d4:	mov	x2, x26
   3c9d8:	mov	x4, x22
   3c9dc:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3c9e0:	add	x8, x27, x28, lsl #3
   3c9e4:	ldur	x9, [x8, #-8]
   3c9e8:	ldur	x27, [x29, #-32]
   3c9ec:	subs	x9, x9, x0
   3c9f0:	stur	x9, [x8, #-8]
   3c9f4:	b.cs	3ca08 <__gmpn_toom_interpolate_12pts@@Base+0x234>  // b.hs, b.nlast
   3c9f8:	ldr	x9, [x8]
   3c9fc:	sub	x10, x9, #0x1
   3ca00:	str	x10, [x8], #8
   3ca04:	cbz	x9, 3c9f8 <__gmpn_toom_interpolate_12pts@@Base+0x224>
   3ca08:	lsl	x26, x27, #3
   3ca0c:	str	x28, [sp, #32]
   3ca10:	add	x28, x19, x26
   3ca14:	add	x0, x28, x24
   3ca18:	lsl	x2, x20, #1
   3ca1c:	mov	w3, #0x14                  	// #20
   3ca20:	mov	x1, x19
   3ca24:	mov	x4, x22
   3ca28:	add	x23, x27, #0x1
   3ca2c:	stur	x2, [x29, #-8]
   3ca30:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3ca34:	ldr	x8, [x28, x26]
   3ca38:	add	x27, x25, x24
   3ca3c:	sub	x8, x8, x0
   3ca40:	str	x8, [x28, x26]
   3ca44:	ldr	x8, [x19]
   3ca48:	ldr	x9, [x27]
   3ca4c:	sub	x8, x9, x8, lsr #4
   3ca50:	str	x8, [x27]
   3ca54:	ldr	x8, [x19]
   3ca58:	cmp	x9, x8, lsr #4
   3ca5c:	b.cs	3ca78 <__gmpn_toom_interpolate_12pts@@Base+0x2a4>  // b.hs, b.nlast
   3ca60:	add	x8, x25, x20, lsl #3
   3ca64:	add	x8, x8, #0x8
   3ca68:	ldr	x9, [x8]
   3ca6c:	sub	x10, x9, #0x1
   3ca70:	str	x10, [x8], #8
   3ca74:	cbz	x9, 3ca68 <__gmpn_toom_interpolate_12pts@@Base+0x294>
   3ca78:	ldur	x21, [x29, #-8]
   3ca7c:	add	x1, x19, #0x8
   3ca80:	mov	w3, #0x3c                  	// #60
   3ca84:	mov	x0, x27
   3ca88:	sub	x2, x21, #0x1
   3ca8c:	mov	x4, x22
   3ca90:	stp	x2, x1, [sp]
   3ca94:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3ca98:	add	x8, x27, x21, lsl #3
   3ca9c:	ldur	x9, [x8, #-8]
   3caa0:	str	x27, [sp, #16]
   3caa4:	subs	x9, x9, x0
   3caa8:	stur	x9, [x8, #-8]
   3caac:	b.cs	3cac0 <__gmpn_toom_interpolate_12pts@@Base+0x2ec>  // b.hs, b.nlast
   3cab0:	ldr	x9, [x8]
   3cab4:	sub	x10, x9, #0x1
   3cab8:	str	x10, [x8], #8
   3cabc:	cbz	x9, 3cab0 <__gmpn_toom_interpolate_12pts@@Base+0x2dc>
   3cac0:	mov	x0, x22
   3cac4:	mov	x1, x25
   3cac8:	mov	x2, x28
   3cacc:	mov	x3, x23
   3cad0:	bl	cc30 <__gmpn_add_n@plt>
   3cad4:	mov	x0, x28
   3cad8:	mov	x1, x28
   3cadc:	mov	x2, x25
   3cae0:	mov	x3, x23
   3cae4:	bl	c420 <__gmpn_sub_n@plt>
   3cae8:	ldp	x27, x2, [x29, #-16]
   3caec:	mov	w3, #0xa                   	// #10
   3caf0:	mov	x1, x19
   3caf4:	mov	x4, x25
   3caf8:	add	x0, x27, x24
   3cafc:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3cb00:	ldr	x8, [x27, x26]
   3cb04:	mov	w9, #0x38                  	// #56
   3cb08:	madd	x21, x20, x9, x19
   3cb0c:	add	x24, x21, x24
   3cb10:	sub	x8, x8, x0
   3cb14:	str	x8, [x27, x26]
   3cb18:	ldr	x8, [x19]
   3cb1c:	ldr	x9, [x24]
   3cb20:	sub	x8, x9, x8, lsr #2
   3cb24:	str	x8, [x24]
   3cb28:	ldr	x8, [x19]
   3cb2c:	cmp	x9, x8, lsr #2
   3cb30:	b.cs	3cb4c <__gmpn_toom_interpolate_12pts@@Base+0x378>  // b.hs, b.nlast
   3cb34:	add	x8, x19, x20, lsl #6
   3cb38:	add	x8, x8, #0x8
   3cb3c:	ldr	x9, [x8]
   3cb40:	sub	x10, x9, #0x1
   3cb44:	str	x10, [x8], #8
   3cb48:	cbz	x9, 3cb3c <__gmpn_toom_interpolate_12pts@@Base+0x368>
   3cb4c:	ldp	x2, x1, [sp]
   3cb50:	mov	w3, #0x3e                  	// #62
   3cb54:	mov	x0, x24
   3cb58:	mov	x4, x25
   3cb5c:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3cb60:	ldur	x8, [x29, #-8]
   3cb64:	add	x8, x24, x8, lsl #3
   3cb68:	ldur	x9, [x8, #-8]
   3cb6c:	subs	x9, x9, x0
   3cb70:	stur	x9, [x8, #-8]
   3cb74:	b.cs	3cb88 <__gmpn_toom_interpolate_12pts@@Base+0x3b4>  // b.hs, b.nlast
   3cb78:	ldr	x9, [x8]
   3cb7c:	sub	x10, x9, #0x1
   3cb80:	str	x10, [x8], #8
   3cb84:	cbz	x9, 3cb78 <__gmpn_toom_interpolate_12pts@@Base+0x3a4>
   3cb88:	ldur	x24, [x29, #-16]
   3cb8c:	mov	x0, x25
   3cb90:	mov	x2, x21
   3cb94:	mov	x3, x23
   3cb98:	mov	x1, x24
   3cb9c:	bl	c420 <__gmpn_sub_n@plt>
   3cba0:	mov	x0, x21
   3cba4:	mov	x1, x21
   3cba8:	mov	x2, x24
   3cbac:	mov	x3, x23
   3cbb0:	bl	cc30 <__gmpn_add_n@plt>
   3cbb4:	ldur	x24, [x29, #-24]
   3cbb8:	ldur	x3, [x29, #-8]
   3cbbc:	mov	x2, x19
   3cbc0:	add	x0, x24, x20, lsl #3
   3cbc4:	mov	x1, x0
   3cbc8:	str	x0, [sp, #8]
   3cbcc:	bl	c420 <__gmpn_sub_n@plt>
   3cbd0:	ldr	x8, [x24, x26]
   3cbd4:	mov	w3, #0x101                 	// #257
   3cbd8:	mov	x1, x25
   3cbdc:	mov	x2, x23
   3cbe0:	sub	x8, x8, x0
   3cbe4:	mov	x0, x28
   3cbe8:	str	x8, [x24, x26]
   3cbec:	bl	cba0 <__gmpn_submul_1@plt>
   3cbf0:	mov	x4, #0x771b                	// #30491
   3cbf4:	movk	x4, #0x53e3, lsl #16
   3cbf8:	movk	x4, #0xc705, lsl #32
   3cbfc:	mov	w3, #0xb13                 	// #2835
   3cc00:	movk	x4, #0x938c, lsl #48
   3cc04:	mov	w5, #0x2                   	// #2
   3cc08:	mov	x0, x28
   3cc0c:	mov	x1, x28
   3cc10:	mov	x2, x23
   3cc14:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3cc18:	ldr	x8, [x28, x26]
   3cc1c:	ldur	x10, [x29, #-32]
   3cc20:	lsr	x9, x8, #61
   3cc24:	cbz	x9, 3cc30 <__gmpn_toom_interpolate_12pts@@Base+0x45c>
   3cc28:	orr	x8, x8, #0xc000000000000000
   3cc2c:	str	x8, [x28, x10, lsl #3]
   3cc30:	mov	w3, #0x3c                  	// #60
   3cc34:	mov	x0, x25
   3cc38:	mov	x1, x28
   3cc3c:	mov	x2, x23
   3cc40:	bl	d5e0 <__gmpn_addmul_1@plt>
   3cc44:	mov	x3, #0x101010101010101     	// #72340172838076673
   3cc48:	mov	x0, x25
   3cc4c:	mov	x1, x25
   3cc50:	mov	x2, x23
   3cc54:	mov	x4, xzr
   3cc58:	bl	c410 <__gmpn_bdiv_dbm1c@plt>
   3cc5c:	ldp	x24, x27, [x29, #-24]
   3cc60:	mov	w3, #0x5                   	// #5
   3cc64:	mov	x0, x21
   3cc68:	mov	x2, x23
   3cc6c:	mov	x1, x24
   3cc70:	mov	x4, x27
   3cc74:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3cc78:	mov	w3, #0x64                  	// #100
   3cc7c:	mov	x0, x22
   3cc80:	mov	x1, x21
   3cc84:	mov	x2, x23
   3cc88:	bl	cba0 <__gmpn_submul_1@plt>
   3cc8c:	mov	w3, #0x9                   	// #9
   3cc90:	mov	x0, x22
   3cc94:	mov	x1, x24
   3cc98:	mov	x2, x23
   3cc9c:	mov	x4, x27
   3cca0:	bl	3cfa0 <__gmpn_toom_interpolate_12pts@@Base+0x7cc>
   3cca4:	mov	x4, #0x4c35                	// #19509
   3cca8:	movk	x4, #0x9f31, lsl #16
   3ccac:	movk	x4, #0xd44, lsl #32
   3ccb0:	mov	w3, #0xa61d                	// #42525
   3ccb4:	movk	x4, #0xe7b4, lsl #48
   3ccb8:	mov	x0, x22
   3ccbc:	mov	x1, x22
   3ccc0:	mov	x2, x23
   3ccc4:	mov	w5, wzr
   3ccc8:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3cccc:	mov	w3, #0xe1                  	// #225
   3ccd0:	mov	x0, x21
   3ccd4:	mov	x1, x22
   3ccd8:	mov	x2, x23
   3ccdc:	bl	cba0 <__gmpn_submul_1@plt>
   3cce0:	mov	x4, #0x8e39                	// #36409
   3cce4:	movk	x4, #0x38e3, lsl #16
   3cce8:	movk	x4, #0xe38e, lsl #32
   3ccec:	mov	w3, #0x9                   	// #9
   3ccf0:	movk	x4, #0x8e38, lsl #48
   3ccf4:	mov	w5, #0x2                   	// #2
   3ccf8:	mov	x0, x21
   3ccfc:	mov	x1, x21
   3cd00:	mov	x2, x23
   3cd04:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3cd08:	mov	x0, x24
   3cd0c:	mov	x1, x24
   3cd10:	mov	x2, x21
   3cd14:	mov	x3, x23
   3cd18:	bl	c420 <__gmpn_sub_n@plt>
   3cd1c:	mov	x0, x28
   3cd20:	mov	x1, x21
   3cd24:	mov	x2, x28
   3cd28:	mov	x3, x23
   3cd2c:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3cd30:	ldr	x8, [x28, x26]
   3cd34:	mov	x0, x21
   3cd38:	mov	x1, x21
   3cd3c:	mov	x2, x28
   3cd40:	and	x8, x8, #0x7fffffffffffffff
   3cd44:	mov	x3, x23
   3cd48:	str	x8, [x28, x26]
   3cd4c:	bl	c420 <__gmpn_sub_n@plt>
   3cd50:	mov	x0, x25
   3cd54:	mov	x1, x25
   3cd58:	mov	x2, x22
   3cd5c:	mov	x3, x23
   3cd60:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3cd64:	ldr	x8, [x25, x26]
   3cd68:	mov	x0, x24
   3cd6c:	mov	x1, x24
   3cd70:	mov	x2, x22
   3cd74:	and	x8, x8, #0x7fffffffffffffff
   3cd78:	mov	x3, x23
   3cd7c:	str	x8, [x25, x26]
   3cd80:	bl	c420 <__gmpn_sub_n@plt>
   3cd84:	mov	x0, x22
   3cd88:	mov	x1, x22
   3cd8c:	mov	x2, x25
   3cd90:	mov	x3, x23
   3cd94:	bl	c420 <__gmpn_sub_n@plt>
   3cd98:	ldr	x27, [sp, #40]
   3cd9c:	mov	x2, x25
   3cda0:	mov	x3, x20
   3cda4:	add	x0, x19, x27
   3cda8:	mov	x1, x0
   3cdac:	bl	cc30 <__gmpn_add_n@plt>
   3cdb0:	ldur	x8, [x29, #-8]
   3cdb4:	ldr	x1, [sp, #16]
   3cdb8:	mov	x3, x0
   3cdbc:	mov	x2, x20
   3cdc0:	lsl	x23, x8, #3
   3cdc4:	add	x0, x19, x23
   3cdc8:	bl	c150 <__gmpn_add_1@plt>
   3cdcc:	ldr	x24, [x25, x26]
   3cdd0:	mov	x4, x0
   3cdd4:	add	x2, x25, x23
   3cdd8:	mov	x0, x28
   3cddc:	mov	x1, x28
   3cde0:	mov	x3, x20
   3cde4:	bl	d060 <__gmpn_add_nc@plt>
   3cde8:	ldr	x8, [x28, x27]
   3cdec:	add	x9, x0, x24
   3cdf0:	adds	x8, x8, x9
   3cdf4:	str	x8, [x28, x27]
   3cdf8:	b.cc	3ce14 <__gmpn_toom_interpolate_12pts@@Base+0x640>  // b.lo, b.ul, b.last
   3cdfc:	add	x8, x19, x20, lsl #5
   3ce00:	add	x8, x8, #0x8
   3ce04:	ldr	x9, [x8]
   3ce08:	adds	x9, x9, #0x1
   3ce0c:	str	x9, [x8], #8
   3ce10:	b.cs	3ce04 <__gmpn_toom_interpolate_12pts@@Base+0x630>  // b.hs, b.nlast
   3ce14:	ldur	x24, [x29, #-24]
   3ce18:	mov	w8, #0x28                  	// #40
   3ce1c:	madd	x0, x20, x8, x19
   3ce20:	mov	x1, x0
   3ce24:	mov	x2, x24
   3ce28:	mov	x3, x20
   3ce2c:	bl	cc30 <__gmpn_add_n@plt>
   3ce30:	mov	w8, #0x30                  	// #48
   3ce34:	madd	x8, x20, x8, x19
   3ce38:	ldr	x9, [x8]
   3ce3c:	ldr	x1, [sp, #8]
   3ce40:	mov	x2, x20
   3ce44:	add	x3, x9, x0
   3ce48:	mov	x0, x8
   3ce4c:	str	x3, [x8]
   3ce50:	bl	c150 <__gmpn_add_1@plt>
   3ce54:	ldur	x25, [x29, #-32]
   3ce58:	ldur	x8, [x29, #-8]
   3ce5c:	mov	x4, x0
   3ce60:	mov	x0, x21
   3ce64:	ldr	x23, [x24, x25, lsl #3]
   3ce68:	add	x2, x24, x8, lsl #3
   3ce6c:	mov	x1, x21
   3ce70:	mov	x3, x20
   3ce74:	bl	d060 <__gmpn_add_nc@plt>
   3ce78:	lsl	x8, x20, #6
   3ce7c:	ldr	x9, [x19, x8]
   3ce80:	ldr	x21, [sp, #32]
   3ce84:	add	x10, x0, x23
   3ce88:	adds	x9, x9, x10
   3ce8c:	str	x9, [x19, x8]
   3ce90:	b.cc	3ceac <__gmpn_toom_interpolate_12pts@@Base+0x6d8>  // b.lo, b.ul, b.last
   3ce94:	add	x8, x19, x20, lsl #6
   3ce98:	add	x8, x8, #0x8
   3ce9c:	ldr	x9, [x8]
   3cea0:	adds	x9, x9, #0x1
   3cea4:	str	x9, [x8], #8
   3cea8:	b.cs	3ce9c <__gmpn_toom_interpolate_12pts@@Base+0x6c8>  // b.hs, b.nlast
   3ceac:	mov	w8, #0x48                  	// #72
   3ceb0:	madd	x0, x20, x8, x19
   3ceb4:	mov	x1, x0
   3ceb8:	mov	x2, x22
   3cebc:	mov	x3, x20
   3cec0:	bl	cc30 <__gmpn_add_n@plt>
   3cec4:	mov	w9, #0x50                  	// #80
   3cec8:	mov	x8, x0
   3cecc:	madd	x0, x20, x9, x19
   3ced0:	ldr	x9, [x0]
   3ced4:	add	x1, x22, x20, lsl #3
   3ced8:	add	x3, x9, x8
   3cedc:	ldr	w8, [sp, #28]
   3cee0:	str	x3, [x0]
   3cee4:	cbz	w8, 3cf58 <__gmpn_toom_interpolate_12pts@@Base+0x784>
   3cee8:	mov	x2, x20
   3ceec:	bl	c150 <__gmpn_add_1@plt>
   3cef0:	cmp	x21, x20
   3cef4:	mov	x4, x0
   3cef8:	b.le	3cf80 <__gmpn_toom_interpolate_12pts@@Base+0x7ac>
   3cefc:	mov	w8, #0x58                  	// #88
   3cf00:	madd	x0, x20, x8, x19
   3cf04:	ldur	x8, [x29, #-8]
   3cf08:	ldr	x21, [x22, x25, lsl #3]
   3cf0c:	mov	x1, x0
   3cf10:	mov	x3, x20
   3cf14:	add	x2, x22, x8, lsl #3
   3cf18:	bl	d060 <__gmpn_add_nc@plt>
   3cf1c:	add	x8, x20, x20, lsl #1
   3cf20:	lsl	x9, x8, #5
   3cf24:	ldr	x10, [x19, x9]
   3cf28:	add	x11, x0, x21
   3cf2c:	adds	x10, x10, x11
   3cf30:	str	x10, [x19, x9]
   3cf34:	b.cc	3cf60 <__gmpn_toom_interpolate_12pts@@Base+0x78c>  // b.lo, b.ul, b.last
   3cf38:	lsl	x8, x8, #2
   3cf3c:	add	x8, x19, x8, lsl #3
   3cf40:	add	x8, x8, #0x8
   3cf44:	ldr	x9, [x8]
   3cf48:	adds	x9, x9, #0x1
   3cf4c:	str	x9, [x8], #8
   3cf50:	b.cs	3cf44 <__gmpn_toom_interpolate_12pts@@Base+0x770>  // b.hs, b.nlast
   3cf54:	b	3cf60 <__gmpn_toom_interpolate_12pts@@Base+0x78c>
   3cf58:	mov	x2, x21
   3cf5c:	bl	c150 <__gmpn_add_1@plt>
   3cf60:	ldp	x20, x19, [sp, #160]
   3cf64:	ldp	x22, x21, [sp, #144]
   3cf68:	ldp	x24, x23, [sp, #128]
   3cf6c:	ldp	x26, x25, [sp, #112]
   3cf70:	ldp	x28, x27, [sp, #96]
   3cf74:	ldp	x29, x30, [sp, #80]
   3cf78:	add	sp, sp, #0xb0
   3cf7c:	ret
   3cf80:	mov	w8, #0x58                  	// #88
   3cf84:	madd	x0, x20, x8, x19
   3cf88:	ldur	x8, [x29, #-8]
   3cf8c:	mov	x1, x0
   3cf90:	mov	x3, x21
   3cf94:	add	x2, x22, x8, lsl #3
   3cf98:	bl	d060 <__gmpn_add_nc@plt>
   3cf9c:	b	3cf60 <__gmpn_toom_interpolate_12pts@@Base+0x78c>
   3cfa0:	stp	x29, x30, [sp, #-48]!
   3cfa4:	stp	x22, x21, [sp, #16]
   3cfa8:	mov	x21, x0
   3cfac:	mov	x0, x4
   3cfb0:	stp	x20, x19, [sp, #32]
   3cfb4:	mov	x29, sp
   3cfb8:	mov	x19, x4
   3cfbc:	mov	x20, x2
   3cfc0:	bl	c2d0 <__gmpn_lshift@plt>
   3cfc4:	mov	x22, x0
   3cfc8:	mov	x0, x21
   3cfcc:	mov	x1, x21
   3cfd0:	mov	x2, x19
   3cfd4:	mov	x3, x20
   3cfd8:	bl	c420 <__gmpn_sub_n@plt>
   3cfdc:	add	x0, x0, x22
   3cfe0:	ldp	x20, x19, [sp, #32]
   3cfe4:	ldp	x22, x21, [sp, #16]
   3cfe8:	ldp	x29, x30, [sp], #48
   3cfec:	ret

000000000003cff0 <__gmpn_toom_interpolate_16pts@@Base>:
   3cff0:	sub	sp, sp, #0xd0
   3cff4:	stp	x29, x30, [sp, #112]
   3cff8:	add	x29, sp, #0x70
   3cffc:	stp	x28, x27, [sp, #128]
   3d000:	ldr	x27, [x29, #96]
   3d004:	stp	x26, x25, [sp, #144]
   3d008:	stp	x24, x23, [sp, #160]
   3d00c:	stp	x20, x19, [sp, #192]
   3d010:	mov	x19, x5
   3d014:	mov	x24, x3
   3d018:	mov	x26, x2
   3d01c:	mov	x20, x0
   3d020:	add	x25, x5, x5, lsl #1
   3d024:	lsl	x8, x5, #3
   3d028:	stp	x22, x21, [sp, #176]
   3d02c:	stur	x6, [x29, #-48]
   3d030:	stp	x4, x1, [x29, #-24]
   3d034:	stur	x8, [x29, #-32]
   3d038:	str	w7, [sp, #36]
   3d03c:	stur	x27, [x29, #-8]
   3d040:	cbz	w7, 3d2ec <__gmpn_toom_interpolate_16pts@@Base+0x2fc>
   3d044:	ldur	x23, [x29, #-48]
   3d048:	mov	w8, #0x38                  	// #56
   3d04c:	mov	w9, #0x78                  	// #120
   3d050:	madd	x22, x19, x8, x20
   3d054:	madd	x21, x19, x9, x20
   3d058:	mov	x0, x22
   3d05c:	mov	x1, x22
   3d060:	mov	x2, x21
   3d064:	mov	x3, x23
   3d068:	bl	c420 <__gmpn_sub_n@plt>
   3d06c:	lsl	x28, x23, #3
   3d070:	ldr	x8, [x22, x28]
   3d074:	subs	x8, x8, x0
   3d078:	str	x8, [x22, x28]
   3d07c:	b.cs	3d0a4 <__gmpn_toom_interpolate_16pts@@Base+0xb4>  // b.hs, b.nlast
   3d080:	ldur	x8, [x29, #-32]
   3d084:	sub	x8, x8, x19
   3d088:	add	x8, x23, x8
   3d08c:	add	x8, x20, x8, lsl #3
   3d090:	add	x8, x8, #0x8
   3d094:	ldr	x9, [x8]
   3d098:	sub	x10, x9, #0x1
   3d09c:	str	x10, [x8], #8
   3d0a0:	cbz	x9, 3d094 <__gmpn_toom_interpolate_16pts@@Base+0xa4>
   3d0a4:	mov	w3, #0xe                   	// #14
   3d0a8:	mov	x0, x26
   3d0ac:	mov	x1, x21
   3d0b0:	mov	x2, x23
   3d0b4:	mov	x4, x27
   3d0b8:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d0bc:	ldr	x8, [x26, x28]
   3d0c0:	subs	x8, x8, x0
   3d0c4:	str	x8, [x26, x28]
   3d0c8:	b.cs	3d0e4 <__gmpn_toom_interpolate_16pts@@Base+0xf4>  // b.hs, b.nlast
   3d0cc:	add	x8, x26, x23, lsl #3
   3d0d0:	add	x8, x8, #0x8
   3d0d4:	ldr	x9, [x8]
   3d0d8:	sub	x10, x9, #0x1
   3d0dc:	str	x10, [x8], #8
   3d0e0:	cbz	x9, 3d0d4 <__gmpn_toom_interpolate_16pts@@Base+0xe4>
   3d0e4:	add	x22, x20, x25, lsl #3
   3d0e8:	ldr	x8, [x21]
   3d0ec:	ldr	x9, [x22]
   3d0f0:	sub	x8, x9, x8, lsr #2
   3d0f4:	str	x8, [x22]
   3d0f8:	ldr	x8, [x21]
   3d0fc:	cmp	x9, x8, lsr #2
   3d100:	b.cs	3d120 <__gmpn_toom_interpolate_16pts@@Base+0x130>  // b.hs, b.nlast
   3d104:	mov	w8, #0x18                  	// #24
   3d108:	madd	x8, x19, x8, x20
   3d10c:	add	x8, x8, #0x8
   3d110:	ldr	x9, [x8]
   3d114:	sub	x10, x9, #0x1
   3d118:	str	x10, [x8], #8
   3d11c:	cbz	x9, 3d110 <__gmpn_toom_interpolate_16pts@@Base+0x120>
   3d120:	ldur	x23, [x29, #-48]
   3d124:	ldur	x4, [x29, #-8]
   3d128:	add	x1, x21, #0x8
   3d12c:	mov	w3, #0x3e                  	// #62
   3d130:	sub	x27, x23, #0x1
   3d134:	mov	x0, x22
   3d138:	mov	x2, x27
   3d13c:	stur	x1, [x29, #-40]
   3d140:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d144:	add	x8, x22, x23, lsl #3
   3d148:	ldur	x9, [x8, #-8]
   3d14c:	subs	x9, x9, x0
   3d150:	stur	x9, [x8, #-8]
   3d154:	b.cs	3d168 <__gmpn_toom_interpolate_16pts@@Base+0x178>  // b.hs, b.nlast
   3d158:	ldr	x9, [x8]
   3d15c:	sub	x10, x9, #0x1
   3d160:	str	x10, [x8], #8
   3d164:	cbz	x9, 3d158 <__gmpn_toom_interpolate_16pts@@Base+0x168>
   3d168:	ldur	x23, [x29, #-48]
   3d16c:	ldur	x4, [x29, #-8]
   3d170:	mov	w8, #0x58                  	// #88
   3d174:	madd	x22, x19, x8, x20
   3d178:	mov	w3, #0x1c                  	// #28
   3d17c:	mov	x0, x22
   3d180:	mov	x1, x21
   3d184:	mov	x2, x23
   3d188:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d18c:	ldr	x8, [x22, x28]
   3d190:	subs	x8, x8, x0
   3d194:	str	x8, [x22, x28]
   3d198:	mov	x22, x23
   3d19c:	b.cs	3d1c0 <__gmpn_toom_interpolate_16pts@@Base+0x1d0>  // b.hs, b.nlast
   3d1a0:	mov	w8, #0xb                   	// #11
   3d1a4:	madd	x8, x19, x8, x22
   3d1a8:	add	x8, x20, x8, lsl #3
   3d1ac:	add	x8, x8, #0x8
   3d1b0:	ldr	x9, [x8]
   3d1b4:	sub	x10, x9, #0x1
   3d1b8:	str	x10, [x8], #8
   3d1bc:	cbz	x9, 3d1b0 <__gmpn_toom_interpolate_16pts@@Base+0x1c0>
   3d1c0:	ldr	x8, [x21]
   3d1c4:	ldr	x9, [x24]
   3d1c8:	sub	x8, x9, x8, lsr #4
   3d1cc:	str	x8, [x24]
   3d1d0:	ldr	x8, [x21]
   3d1d4:	cmp	x9, x8, lsr #4
   3d1d8:	b.cs	3d1f0 <__gmpn_toom_interpolate_16pts@@Base+0x200>  // b.hs, b.nlast
   3d1dc:	add	x8, x24, #0x8
   3d1e0:	ldr	x9, [x8]
   3d1e4:	sub	x10, x9, #0x1
   3d1e8:	str	x10, [x8], #8
   3d1ec:	cbz	x9, 3d1e0 <__gmpn_toom_interpolate_16pts@@Base+0x1f0>
   3d1f0:	ldur	x1, [x29, #-40]
   3d1f4:	ldur	x4, [x29, #-8]
   3d1f8:	mov	w3, #0x3c                  	// #60
   3d1fc:	mov	x0, x24
   3d200:	mov	x2, x27
   3d204:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d208:	add	x8, x24, x22, lsl #3
   3d20c:	ldur	x9, [x8, #-8]
   3d210:	subs	x9, x9, x0
   3d214:	stur	x9, [x8, #-8]
   3d218:	b.cs	3d22c <__gmpn_toom_interpolate_16pts@@Base+0x23c>  // b.hs, b.nlast
   3d21c:	ldr	x9, [x8]
   3d220:	sub	x10, x9, #0x1
   3d224:	str	x10, [x8], #8
   3d228:	cbz	x9, 3d21c <__gmpn_toom_interpolate_16pts@@Base+0x22c>
   3d22c:	ldp	x23, x4, [x29, #-16]
   3d230:	mov	w3, #0x2a                  	// #42
   3d234:	mov	x1, x21
   3d238:	mov	x2, x22
   3d23c:	mov	x0, x23
   3d240:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d244:	ldr	x8, [x23, x28]
   3d248:	subs	x8, x8, x0
   3d24c:	str	x8, [x23, x28]
   3d250:	b.cs	3d270 <__gmpn_toom_interpolate_16pts@@Base+0x280>  // b.hs, b.nlast
   3d254:	ldur	x8, [x29, #-16]
   3d258:	add	x8, x8, x22, lsl #3
   3d25c:	add	x8, x8, #0x8
   3d260:	ldr	x9, [x8]
   3d264:	sub	x10, x9, #0x1
   3d268:	str	x10, [x8], #8
   3d26c:	cbz	x9, 3d260 <__gmpn_toom_interpolate_16pts@@Base+0x270>
   3d270:	ldur	x10, [x29, #-24]
   3d274:	ldr	x8, [x21]
   3d278:	ldr	x9, [x10]
   3d27c:	sub	x8, x9, x8, lsr #6
   3d280:	str	x8, [x10]
   3d284:	ldr	x8, [x21]
   3d288:	cmp	x9, x8, lsr #6
   3d28c:	b.cs	3d2a8 <__gmpn_toom_interpolate_16pts@@Base+0x2b8>  // b.hs, b.nlast
   3d290:	ldur	x8, [x29, #-24]
   3d294:	add	x8, x8, #0x8
   3d298:	ldr	x9, [x8]
   3d29c:	sub	x10, x9, #0x1
   3d2a0:	str	x10, [x8], #8
   3d2a4:	cbz	x9, 3d298 <__gmpn_toom_interpolate_16pts@@Base+0x2a8>
   3d2a8:	ldur	x21, [x29, #-24]
   3d2ac:	mov	x2, x27
   3d2b0:	ldur	x27, [x29, #-8]
   3d2b4:	ldur	x1, [x29, #-40]
   3d2b8:	mov	w3, #0x3a                  	// #58
   3d2bc:	mov	x0, x21
   3d2c0:	mov	x4, x27
   3d2c4:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d2c8:	add	x8, x21, x22, lsl #3
   3d2cc:	ldur	x9, [x8, #-8]
   3d2d0:	subs	x9, x9, x0
   3d2d4:	stur	x9, [x8, #-8]
   3d2d8:	b.cs	3d2ec <__gmpn_toom_interpolate_16pts@@Base+0x2fc>  // b.hs, b.nlast
   3d2dc:	ldr	x9, [x8]
   3d2e0:	sub	x10, x9, #0x1
   3d2e4:	str	x10, [x8], #8
   3d2e8:	cbz	x9, 3d2dc <__gmpn_toom_interpolate_16pts@@Base+0x2ec>
   3d2ec:	ldur	x28, [x29, #-32]
   3d2f0:	lsl	x23, x19, #1
   3d2f4:	mov	w3, #0x1c                  	// #28
   3d2f8:	mov	x1, x20
   3d2fc:	add	x0, x24, x28
   3d300:	mov	x2, x23
   3d304:	mov	x4, x27
   3d308:	add	x21, x25, #0x1
   3d30c:	str	x0, [sp, #24]
   3d310:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d314:	lsl	x22, x25, #3
   3d318:	ldr	x8, [x24, x22]
   3d31c:	mov	w9, #0x58                  	// #88
   3d320:	madd	x9, x19, x9, x20
   3d324:	add	x27, x9, x28
   3d328:	sub	x8, x8, x0
   3d32c:	str	x8, [x24, x22]
   3d330:	stur	x9, [x29, #-40]
   3d334:	ldr	x8, [x20]
   3d338:	ldr	x9, [x27]
   3d33c:	sub	x8, x9, x8, lsr #4
   3d340:	str	x8, [x27]
   3d344:	ldr	x8, [x20]
   3d348:	cmp	x9, x8, lsr #4
   3d34c:	b.cs	3d36c <__gmpn_toom_interpolate_16pts@@Base+0x37c>  // b.hs, b.nlast
   3d350:	mov	w8, #0x60                  	// #96
   3d354:	madd	x8, x19, x8, x20
   3d358:	add	x8, x8, #0x8
   3d35c:	ldr	x9, [x8]
   3d360:	sub	x10, x9, #0x1
   3d364:	str	x10, [x8], #8
   3d368:	cbz	x9, 3d35c <__gmpn_toom_interpolate_16pts@@Base+0x36c>
   3d36c:	ldur	x4, [x29, #-8]
   3d370:	add	x28, x20, #0x8
   3d374:	sub	x2, x23, #0x1
   3d378:	mov	w3, #0x3c                  	// #60
   3d37c:	mov	x0, x27
   3d380:	mov	x1, x28
   3d384:	stp	x2, x25, [sp, #48]
   3d388:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d38c:	add	x8, x27, x23, lsl #3
   3d390:	ldur	x9, [x8, #-8]
   3d394:	subs	x9, x9, x0
   3d398:	stur	x9, [x8, #-8]
   3d39c:	b.cs	3d3b0 <__gmpn_toom_interpolate_16pts@@Base+0x3c0>  // b.hs, b.nlast
   3d3a0:	ldr	x9, [x8]
   3d3a4:	sub	x10, x9, #0x1
   3d3a8:	str	x10, [x8], #8
   3d3ac:	cbz	x9, 3d3a0 <__gmpn_toom_interpolate_16pts@@Base+0x3b0>
   3d3b0:	ldur	x25, [x29, #-40]
   3d3b4:	ldur	x0, [x29, #-8]
   3d3b8:	mov	x1, x24
   3d3bc:	mov	x3, x21
   3d3c0:	mov	x2, x25
   3d3c4:	bl	c420 <__gmpn_sub_n@plt>
   3d3c8:	mov	x0, x25
   3d3cc:	mov	x1, x25
   3d3d0:	mov	x2, x24
   3d3d4:	mov	x3, x21
   3d3d8:	bl	cc30 <__gmpn_add_n@plt>
   3d3dc:	ldur	x25, [x29, #-32]
   3d3e0:	add	x27, x20, x22
   3d3e4:	mov	w3, #0xe                   	// #14
   3d3e8:	mov	x1, x20
   3d3ec:	add	x0, x27, x25
   3d3f0:	mov	x2, x23
   3d3f4:	mov	x4, x24
   3d3f8:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d3fc:	ldr	x8, [x27, x22]
   3d400:	add	x25, x26, x25
   3d404:	sub	x8, x8, x0
   3d408:	str	x8, [x27, x22]
   3d40c:	ldr	x8, [x20]
   3d410:	ldr	x9, [x25]
   3d414:	sub	x8, x9, x8, lsr #2
   3d418:	str	x8, [x25]
   3d41c:	ldr	x8, [x20]
   3d420:	cmp	x9, x8, lsr #2
   3d424:	b.cs	3d440 <__gmpn_toom_interpolate_16pts@@Base+0x450>  // b.hs, b.nlast
   3d428:	add	x8, x26, x19, lsl #3
   3d42c:	add	x8, x8, #0x8
   3d430:	ldr	x9, [x8]
   3d434:	sub	x10, x9, #0x1
   3d438:	str	x10, [x8], #8
   3d43c:	cbz	x9, 3d430 <__gmpn_toom_interpolate_16pts@@Base+0x440>
   3d440:	ldr	x2, [sp, #48]
   3d444:	mov	w3, #0x3e                  	// #62
   3d448:	mov	x0, x25
   3d44c:	mov	x1, x28
   3d450:	mov	x4, x24
   3d454:	str	x28, [sp, #16]
   3d458:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d45c:	add	x8, x25, x23, lsl #3
   3d460:	ldur	x9, [x8, #-8]
   3d464:	str	x25, [sp, #8]
   3d468:	subs	x9, x9, x0
   3d46c:	stur	x9, [x8, #-8]
   3d470:	b.cs	3d484 <__gmpn_toom_interpolate_16pts@@Base+0x494>  // b.hs, b.nlast
   3d474:	ldr	x9, [x8]
   3d478:	sub	x10, x9, #0x1
   3d47c:	str	x10, [x8], #8
   3d480:	cbz	x9, 3d474 <__gmpn_toom_interpolate_16pts@@Base+0x484>
   3d484:	mov	x0, x24
   3d488:	mov	x1, x26
   3d48c:	mov	x2, x27
   3d490:	mov	x3, x21
   3d494:	bl	cc30 <__gmpn_add_n@plt>
   3d498:	mov	x0, x27
   3d49c:	mov	x1, x27
   3d4a0:	mov	x2, x26
   3d4a4:	mov	x3, x21
   3d4a8:	bl	c420 <__gmpn_sub_n@plt>
   3d4ac:	ldp	x28, x25, [x29, #-32]
   3d4b0:	mov	w3, #0x2a                  	// #42
   3d4b4:	mov	x1, x20
   3d4b8:	mov	x2, x23
   3d4bc:	add	x0, x25, x28
   3d4c0:	mov	x4, x26
   3d4c4:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d4c8:	ldr	x8, [x25, x22]
   3d4cc:	ldur	x9, [x29, #-16]
   3d4d0:	sub	x8, x8, x0
   3d4d4:	add	x10, x9, x28
   3d4d8:	str	x8, [x25, x22]
   3d4dc:	ldr	x8, [x20]
   3d4e0:	ldr	x9, [x10]
   3d4e4:	str	x10, [sp, #40]
   3d4e8:	sub	x8, x9, x8, lsr #6
   3d4ec:	str	x8, [x10]
   3d4f0:	ldr	x8, [x20]
   3d4f4:	cmp	x9, x8, lsr #6
   3d4f8:	b.cs	3d518 <__gmpn_toom_interpolate_16pts@@Base+0x528>  // b.hs, b.nlast
   3d4fc:	ldur	x8, [x29, #-16]
   3d500:	add	x8, x8, x19, lsl #3
   3d504:	add	x8, x8, #0x8
   3d508:	ldr	x9, [x8]
   3d50c:	sub	x10, x9, #0x1
   3d510:	str	x10, [x8], #8
   3d514:	cbz	x9, 3d508 <__gmpn_toom_interpolate_16pts@@Base+0x518>
   3d518:	stur	x24, [x29, #-32]
   3d51c:	ldp	x24, x2, [sp, #40]
   3d520:	ldr	x1, [sp, #16]
   3d524:	mov	w3, #0x3a                  	// #58
   3d528:	mov	x4, x26
   3d52c:	mov	x0, x24
   3d530:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d534:	add	x8, x24, x23, lsl #3
   3d538:	ldur	x9, [x8, #-8]
   3d53c:	subs	x9, x9, x0
   3d540:	stur	x9, [x8, #-8]
   3d544:	b.cs	3d558 <__gmpn_toom_interpolate_16pts@@Base+0x568>  // b.hs, b.nlast
   3d548:	ldr	x9, [x8]
   3d54c:	sub	x10, x9, #0x1
   3d550:	str	x10, [x8], #8
   3d554:	cbz	x9, 3d548 <__gmpn_toom_interpolate_16pts@@Base+0x558>
   3d558:	ldp	x24, x25, [x29, #-24]
   3d55c:	mov	x0, x26
   3d560:	mov	x3, x21
   3d564:	mov	x1, x24
   3d568:	mov	x2, x25
   3d56c:	bl	c420 <__gmpn_sub_n@plt>
   3d570:	mov	x0, x25
   3d574:	mov	x1, x25
   3d578:	mov	x2, x24
   3d57c:	mov	x3, x21
   3d580:	bl	cc30 <__gmpn_add_n@plt>
   3d584:	mov	w8, #0x38                  	// #56
   3d588:	madd	x28, x19, x8, x20
   3d58c:	add	x0, x28, x19, lsl #3
   3d590:	mov	x1, x0
   3d594:	mov	x2, x20
   3d598:	mov	x3, x23
   3d59c:	bl	c420 <__gmpn_sub_n@plt>
   3d5a0:	ldr	x8, [x28, x22]
   3d5a4:	ldur	x24, [x29, #-8]
   3d5a8:	mov	w3, #0x404                 	// #1028
   3d5ac:	mov	x1, x27
   3d5b0:	sub	x8, x8, x0
   3d5b4:	mov	x0, x24
   3d5b8:	mov	x2, x21
   3d5bc:	str	x8, [x28, x22]
   3d5c0:	bl	cba0 <__gmpn_submul_1@plt>
   3d5c4:	mov	w3, #0x514                 	// #1300
   3d5c8:	mov	x0, x26
   3d5cc:	mov	x1, x24
   3d5d0:	mov	x2, x21
   3d5d4:	bl	cba0 <__gmpn_submul_1@plt>
   3d5d8:	mov	w3, #0x1010                	// #4112
   3d5dc:	movk	w3, #0x10, lsl #16
   3d5e0:	mov	x0, x26
   3d5e4:	mov	x1, x27
   3d5e8:	mov	x2, x21
   3d5ec:	bl	cba0 <__gmpn_submul_1@plt>
   3d5f0:	mov	x4, #0x275b                	// #10075
   3d5f4:	mov	x3, #0xb0d3                	// #45267
   3d5f8:	movk	x4, #0x6864, lsl #16
   3d5fc:	movk	x3, #0x313f, lsl #16
   3d600:	movk	x4, #0x993a, lsl #32
   3d604:	movk	x3, #0xb, lsl #32
   3d608:	movk	x4, #0x6db, lsl #48
   3d60c:	mov	x0, x26
   3d610:	mov	x1, x26
   3d614:	mov	x2, x21
   3d618:	mov	w5, wzr
   3d61c:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d620:	mov	w3, #0xc403                	// #50179
   3d624:	movk	w3, #0xbf, lsl #16
   3d628:	mov	x0, x24
   3d62c:	mov	x1, x26
   3d630:	mov	x2, x21
   3d634:	bl	cba0 <__gmpn_submul_1@plt>
   3d638:	mov	x4, #0x771b                	// #30491
   3d63c:	movk	x4, #0x53e3, lsl #16
   3d640:	movk	x4, #0xc705, lsl #32
   3d644:	mov	w3, #0xb13                 	// #2835
   3d648:	movk	x4, #0x938c, lsl #48
   3d64c:	mov	w5, #0x6                   	// #6
   3d650:	mov	x0, x24
   3d654:	mov	x1, x24
   3d658:	mov	x2, x21
   3d65c:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d660:	ldr	x8, [x24, x22]
   3d664:	ldr	x25, [sp, #56]
   3d668:	lsr	x9, x8, #57
   3d66c:	cbz	x9, 3d678 <__gmpn_toom_interpolate_16pts@@Base+0x688>
   3d670:	orr	x8, x8, #0xfc00000000000000
   3d674:	str	x8, [x24, x25, lsl #3]
   3d678:	mov	w3, #0xfff                 	// #4095
   3d67c:	mov	x0, x27
   3d680:	mov	x1, x26
   3d684:	mov	x2, x21
   3d688:	str	x23, [sp, #48]
   3d68c:	str	x26, [sp, #16]
   3d690:	bl	cba0 <__gmpn_submul_1@plt>
   3d694:	mov	w3, #0xf0                  	// #240
   3d698:	mov	x0, x27
   3d69c:	mov	x1, x24
   3d6a0:	mov	x2, x21
   3d6a4:	bl	d5e0 <__gmpn_addmul_1@plt>
   3d6a8:	mov	x4, #0xfefefefefefefefe    	// #-72340172838076674
   3d6ac:	mov	w3, #0xff                  	// #255
   3d6b0:	movk	x4, #0xfeff
   3d6b4:	mov	w5, #0x2                   	// #2
   3d6b8:	mov	x0, x27
   3d6bc:	mov	x1, x27
   3d6c0:	mov	x2, x21
   3d6c4:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d6c8:	ldr	x8, [x27, x25, lsl #3]
   3d6cc:	lsr	x9, x8, #61
   3d6d0:	cbz	x9, 3d6dc <__gmpn_toom_interpolate_16pts@@Base+0x6ec>
   3d6d4:	orr	x8, x8, #0xc000000000000000
   3d6d8:	str	x8, [x27, x25, lsl #3]
   3d6dc:	ldp	x23, x24, [x29, #-32]
   3d6e0:	mov	w3, #0x7                   	// #7
   3d6e4:	mov	x1, x28
   3d6e8:	mov	x2, x21
   3d6ec:	mov	x0, x23
   3d6f0:	mov	x4, x24
   3d6f4:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d6f8:	ldur	x26, [x29, #-40]
   3d6fc:	mov	w3, #0xd                   	// #13
   3d700:	mov	x1, x28
   3d704:	mov	x2, x21
   3d708:	mov	x0, x26
   3d70c:	mov	x4, x24
   3d710:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d714:	mov	w3, #0x190                 	// #400
   3d718:	mov	x0, x26
   3d71c:	mov	x1, x23
   3d720:	mov	x2, x21
   3d724:	bl	cba0 <__gmpn_submul_1@plt>
   3d728:	ldur	x25, [x29, #-16]
   3d72c:	mov	w3, #0x13                  	// #19
   3d730:	mov	x1, x28
   3d734:	mov	x2, x21
   3d738:	mov	x0, x25
   3d73c:	mov	x4, x24
   3d740:	bl	3dbb0 <__gmpn_toom_interpolate_16pts@@Base+0xbc0>
   3d744:	mov	w3, #0x594                 	// #1428
   3d748:	mov	x0, x25
   3d74c:	mov	x1, x26
   3d750:	mov	x2, x21
   3d754:	bl	cba0 <__gmpn_submul_1@plt>
   3d758:	mov	w3, #0xb900                	// #47360
   3d75c:	movk	w3, #0x1, lsl #16
   3d760:	mov	x0, x25
   3d764:	mov	x1, x23
   3d768:	mov	x2, x21
   3d76c:	bl	cba0 <__gmpn_submul_1@plt>
   3d770:	mov	x4, #0xcb25                	// #52005
   3d774:	mov	x3, #0x58ad                	// #22701
   3d778:	movk	x4, #0x6fc4, lsl #16
   3d77c:	movk	x3, #0xd916, lsl #16
   3d780:	movk	x4, #0x9a07, lsl #32
   3d784:	movk	x3, #0xa, lsl #32
   3d788:	movk	x4, #0x1b64, lsl #48
   3d78c:	mov	x0, x25
   3d790:	mov	x1, x25
   3d794:	mov	x2, x21
   3d798:	mov	w5, wzr
   3d79c:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d7a0:	mov	w3, #0xa671                	// #42609
   3d7a4:	movk	w3, #0xe7, lsl #16
   3d7a8:	mov	x0, x26
   3d7ac:	mov	x1, x25
   3d7b0:	mov	x2, x21
   3d7b4:	bl	cba0 <__gmpn_submul_1@plt>
   3d7b8:	mov	x4, #0x4c35                	// #19509
   3d7bc:	movk	x4, #0x9f31, lsl #16
   3d7c0:	movk	x4, #0xd44, lsl #32
   3d7c4:	mov	w3, #0xa61d                	// #42525
   3d7c8:	movk	x4, #0xe7b4, lsl #48
   3d7cc:	mov	w5, #0x4                   	// #4
   3d7d0:	mov	x0, x26
   3d7d4:	mov	x1, x26
   3d7d8:	mov	x2, x21
   3d7dc:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d7e0:	mov	w3, #0xf81                 	// #3969
   3d7e4:	mov	x0, x23
   3d7e8:	mov	x1, x25
   3d7ec:	mov	x2, x21
   3d7f0:	bl	cba0 <__gmpn_submul_1@plt>
   3d7f4:	mov	w3, #0x384                 	// #900
   3d7f8:	mov	x0, x23
   3d7fc:	mov	x1, x26
   3d800:	mov	x2, x21
   3d804:	bl	cba0 <__gmpn_submul_1@plt>
   3d808:	mov	x4, #0x8e39                	// #36409
   3d80c:	movk	x4, #0x38e3, lsl #16
   3d810:	movk	x4, #0xe38e, lsl #32
   3d814:	mov	w3, #0x9                   	// #9
   3d818:	movk	x4, #0x8e38, lsl #48
   3d81c:	mov	w5, #0x4                   	// #4
   3d820:	mov	x0, x23
   3d824:	mov	x1, x23
   3d828:	mov	x2, x21
   3d82c:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   3d830:	mov	x0, x28
   3d834:	mov	x1, x28
   3d838:	mov	x2, x25
   3d83c:	mov	x3, x21
   3d840:	bl	c420 <__gmpn_sub_n@plt>
   3d844:	mov	x0, x28
   3d848:	mov	x1, x28
   3d84c:	mov	x2, x23
   3d850:	mov	x3, x21
   3d854:	bl	c420 <__gmpn_sub_n@plt>
   3d858:	mov	x0, x28
   3d85c:	mov	x1, x28
   3d860:	mov	x2, x26
   3d864:	mov	x3, x21
   3d868:	bl	c420 <__gmpn_sub_n@plt>
   3d86c:	mov	x0, x27
   3d870:	mov	x1, x26
   3d874:	mov	x2, x27
   3d878:	mov	x3, x21
   3d87c:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3d880:	ldr	x8, [x27, x22]
   3d884:	mov	x0, x26
   3d888:	mov	x1, x26
   3d88c:	mov	x2, x27
   3d890:	and	x8, x8, #0x7fffffffffffffff
   3d894:	mov	x3, x21
   3d898:	str	x8, [x27, x22]
   3d89c:	bl	c420 <__gmpn_sub_n@plt>
   3d8a0:	ldur	x24, [x29, #-8]
   3d8a4:	mov	x1, x23
   3d8a8:	mov	x3, x21
   3d8ac:	mov	x0, x24
   3d8b0:	mov	x2, x24
   3d8b4:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   3d8b8:	ldr	x8, [x24, x22]
   3d8bc:	mov	x0, x23
   3d8c0:	mov	x1, x23
   3d8c4:	mov	x2, x24
   3d8c8:	and	x8, x8, #0x7fffffffffffffff
   3d8cc:	mov	x3, x21
   3d8d0:	str	x8, [x24, x22]
   3d8d4:	bl	c420 <__gmpn_sub_n@plt>
   3d8d8:	ldr	x24, [sp, #16]
   3d8dc:	mov	x1, x25
   3d8e0:	mov	x3, x21
   3d8e4:	mov	x0, x24
   3d8e8:	mov	x2, x24
   3d8ec:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3d8f0:	ldr	x8, [x24, x22]
   3d8f4:	mov	x0, x25
   3d8f8:	mov	x1, x25
   3d8fc:	mov	x2, x24
   3d900:	and	x8, x8, #0x7fffffffffffffff
   3d904:	mov	x3, x21
   3d908:	str	x8, [x24, x22]
   3d90c:	bl	c420 <__gmpn_sub_n@plt>
   3d910:	add	x0, x20, x19, lsl #3
   3d914:	mov	x1, x0
   3d918:	mov	x2, x24
   3d91c:	mov	x3, x19
   3d920:	bl	cc30 <__gmpn_add_n@plt>
   3d924:	ldr	x23, [sp, #48]
   3d928:	ldr	x1, [sp, #8]
   3d92c:	mov	x3, x0
   3d930:	mov	x2, x19
   3d934:	lsl	x21, x23, #3
   3d938:	add	x0, x20, x21
   3d93c:	bl	c150 <__gmpn_add_1@plt>
   3d940:	ldr	x22, [x24, x22]
   3d944:	mov	x4, x0
   3d948:	add	x2, x24, x21
   3d94c:	mov	x0, x27
   3d950:	mov	x1, x27
   3d954:	mov	x3, x19
   3d958:	bl	d060 <__gmpn_add_nc@plt>
   3d95c:	lsl	x8, x19, #5
   3d960:	ldr	x9, [x20, x8]
   3d964:	add	x10, x0, x22
   3d968:	adds	x9, x9, x10
   3d96c:	str	x9, [x20, x8]
   3d970:	b.cc	3d98c <__gmpn_toom_interpolate_16pts@@Base+0x99c>  // b.lo, b.ul, b.last
   3d974:	add	x8, x20, x19, lsl #5
   3d978:	add	x8, x8, #0x8
   3d97c:	ldr	x9, [x8]
   3d980:	adds	x9, x9, #0x1
   3d984:	str	x9, [x8], #8
   3d988:	b.cs	3d97c <__gmpn_toom_interpolate_16pts@@Base+0x98c>  // b.hs, b.nlast
   3d98c:	ldur	x22, [x29, #-8]
   3d990:	mov	w8, #0x28                  	// #40
   3d994:	madd	x0, x19, x8, x20
   3d998:	mov	x1, x0
   3d99c:	mov	x2, x22
   3d9a0:	mov	x3, x19
   3d9a4:	bl	cc30 <__gmpn_add_n@plt>
   3d9a8:	mov	w8, #0x30                  	// #48
   3d9ac:	madd	x8, x19, x8, x20
   3d9b0:	ldr	x9, [x8]
   3d9b4:	add	x1, x22, x19, lsl #3
   3d9b8:	mov	x2, x19
   3d9bc:	add	x3, x9, x0
   3d9c0:	mov	x0, x8
   3d9c4:	str	x3, [x8]
   3d9c8:	bl	c150 <__gmpn_add_1@plt>
   3d9cc:	ldr	x25, [sp, #56]
   3d9d0:	mov	x4, x0
   3d9d4:	add	x2, x22, x23, lsl #3
   3d9d8:	mov	x0, x28
   3d9dc:	ldr	x21, [x22, x25, lsl #3]
   3d9e0:	mov	x1, x28
   3d9e4:	mov	x3, x19
   3d9e8:	bl	d060 <__gmpn_add_nc@plt>
   3d9ec:	lsl	x8, x19, #6
   3d9f0:	ldr	x9, [x20, x8]
   3d9f4:	ldur	x22, [x29, #-48]
   3d9f8:	add	x10, x0, x21
   3d9fc:	adds	x9, x9, x10
   3da00:	str	x9, [x20, x8]
   3da04:	b.cc	3da20 <__gmpn_toom_interpolate_16pts@@Base+0xa30>  // b.lo, b.ul, b.last
   3da08:	add	x8, x20, x19, lsl #6
   3da0c:	add	x8, x8, #0x8
   3da10:	ldr	x9, [x8]
   3da14:	adds	x9, x9, #0x1
   3da18:	str	x9, [x8], #8
   3da1c:	b.cs	3da10 <__gmpn_toom_interpolate_16pts@@Base+0xa20>  // b.hs, b.nlast
   3da20:	ldur	x23, [x29, #-32]
   3da24:	mov	w8, #0x48                  	// #72
   3da28:	madd	x0, x19, x8, x20
   3da2c:	mov	x1, x0
   3da30:	mov	x2, x23
   3da34:	mov	x3, x19
   3da38:	bl	cc30 <__gmpn_add_n@plt>
   3da3c:	mov	w8, #0x50                  	// #80
   3da40:	madd	x8, x19, x8, x20
   3da44:	ldr	x9, [x8]
   3da48:	ldr	x1, [sp, #24]
   3da4c:	mov	x2, x19
   3da50:	add	x3, x9, x0
   3da54:	mov	x0, x8
   3da58:	str	x3, [x8]
   3da5c:	bl	c150 <__gmpn_add_1@plt>
   3da60:	mov	x4, x0
   3da64:	ldr	x8, [sp, #48]
   3da68:	ldur	x0, [x29, #-40]
   3da6c:	ldr	x21, [x23, x25, lsl #3]
   3da70:	mov	x3, x19
   3da74:	add	x2, x23, x8, lsl #3
   3da78:	mov	x1, x0
   3da7c:	mov	x23, x8
   3da80:	bl	d060 <__gmpn_add_nc@plt>
   3da84:	mov	w8, #0x60                  	// #96
   3da88:	mul	x8, x19, x8
   3da8c:	ldr	x9, [x20, x8]
   3da90:	add	x10, x0, x21
   3da94:	adds	x9, x9, x10
   3da98:	str	x9, [x20, x8]
   3da9c:	b.cc	3dabc <__gmpn_toom_interpolate_16pts@@Base+0xacc>  // b.lo, b.ul, b.last
   3daa0:	mov	w8, #0x60                  	// #96
   3daa4:	madd	x8, x19, x8, x20
   3daa8:	add	x8, x8, #0x8
   3daac:	ldr	x9, [x8]
   3dab0:	adds	x9, x9, #0x1
   3dab4:	str	x9, [x8], #8
   3dab8:	b.cs	3daac <__gmpn_toom_interpolate_16pts@@Base+0xabc>  // b.hs, b.nlast
   3dabc:	ldur	x2, [x29, #-16]
   3dac0:	mov	w8, #0x68                  	// #104
   3dac4:	madd	x0, x19, x8, x20
   3dac8:	mov	x1, x0
   3dacc:	mov	x3, x19
   3dad0:	bl	cc30 <__gmpn_add_n@plt>
   3dad4:	mov	w8, #0x70                  	// #112
   3dad8:	madd	x8, x19, x8, x20
   3dadc:	ldr	x9, [x8]
   3dae0:	add	x3, x9, x0
   3dae4:	ldr	w9, [sp, #36]
   3dae8:	str	x3, [x8]
   3daec:	cbz	w9, 3db60 <__gmpn_toom_interpolate_16pts@@Base+0xb70>
   3daf0:	ldr	x1, [sp, #40]
   3daf4:	mov	x0, x8
   3daf8:	mov	x2, x19
   3dafc:	bl	c150 <__gmpn_add_1@plt>
   3db00:	cmp	x22, x19
   3db04:	mov	x4, x0
   3db08:	b.le	3db90 <__gmpn_toom_interpolate_16pts@@Base+0xba0>
   3db0c:	ldur	x9, [x29, #-16]
   3db10:	mov	w8, #0x78                  	// #120
   3db14:	madd	x0, x19, x8, x20
   3db18:	mov	x1, x0
   3db1c:	ldr	x21, [x9, x25, lsl #3]
   3db20:	add	x2, x9, x23, lsl #3
   3db24:	mov	x3, x19
   3db28:	bl	d060 <__gmpn_add_nc@plt>
   3db2c:	lsl	x8, x19, #7
   3db30:	ldr	x9, [x20, x8]
   3db34:	add	x10, x0, x21
   3db38:	adds	x9, x9, x10
   3db3c:	str	x9, [x20, x8]
   3db40:	b.cc	3db70 <__gmpn_toom_interpolate_16pts@@Base+0xb80>  // b.lo, b.ul, b.last
   3db44:	add	x8, x20, x19, lsl #7
   3db48:	add	x8, x8, #0x8
   3db4c:	ldr	x9, [x8]
   3db50:	adds	x9, x9, #0x1
   3db54:	str	x9, [x8], #8
   3db58:	b.cs	3db4c <__gmpn_toom_interpolate_16pts@@Base+0xb5c>  // b.hs, b.nlast
   3db5c:	b	3db70 <__gmpn_toom_interpolate_16pts@@Base+0xb80>
   3db60:	ldr	x1, [sp, #40]
   3db64:	mov	x0, x8
   3db68:	mov	x2, x22
   3db6c:	bl	c150 <__gmpn_add_1@plt>
   3db70:	ldp	x20, x19, [sp, #192]
   3db74:	ldp	x22, x21, [sp, #176]
   3db78:	ldp	x24, x23, [sp, #160]
   3db7c:	ldp	x26, x25, [sp, #144]
   3db80:	ldp	x28, x27, [sp, #128]
   3db84:	ldp	x29, x30, [sp, #112]
   3db88:	add	sp, sp, #0xd0
   3db8c:	ret
   3db90:	mov	w8, #0x78                  	// #120
   3db94:	madd	x0, x19, x8, x20
   3db98:	ldur	x8, [x29, #-16]
   3db9c:	mov	x1, x0
   3dba0:	mov	x3, x22
   3dba4:	add	x2, x8, x23, lsl #3
   3dba8:	bl	d060 <__gmpn_add_nc@plt>
   3dbac:	b	3db70 <__gmpn_toom_interpolate_16pts@@Base+0xb80>
   3dbb0:	stp	x29, x30, [sp, #-48]!
   3dbb4:	stp	x22, x21, [sp, #16]
   3dbb8:	mov	x21, x0
   3dbbc:	mov	x0, x4
   3dbc0:	stp	x20, x19, [sp, #32]
   3dbc4:	mov	x29, sp
   3dbc8:	mov	x19, x4
   3dbcc:	mov	x20, x2
   3dbd0:	bl	c2d0 <__gmpn_lshift@plt>
   3dbd4:	mov	x22, x0
   3dbd8:	mov	x0, x21
   3dbdc:	mov	x1, x21
   3dbe0:	mov	x2, x19
   3dbe4:	mov	x3, x20
   3dbe8:	bl	c420 <__gmpn_sub_n@plt>
   3dbec:	add	x0, x0, x22
   3dbf0:	ldp	x20, x19, [sp, #32]
   3dbf4:	ldp	x22, x21, [sp, #16]
   3dbf8:	ldp	x29, x30, [sp], #48
   3dbfc:	ret

000000000003dc00 <__gmpn_ni_invertappr@@Base>:
   3dc00:	stp	x29, x30, [sp, #-96]!
   3dc04:	stp	x28, x27, [sp, #16]
   3dc08:	stp	x26, x25, [sp, #32]
   3dc0c:	stp	x24, x23, [sp, #48]
   3dc10:	stp	x22, x21, [sp, #64]
   3dc14:	stp	x20, x19, [sp, #80]
   3dc18:	mov	x29, sp
   3dc1c:	sub	sp, sp, #0x1a0
   3dc20:	mov	x19, sp
   3dc24:	mov	x20, x3
   3dc28:	mov	x23, x2
   3dc2c:	mov	x22, x0
   3dc30:	mov	x24, xzr
   3dc34:	add	x8, x19, #0x58
   3dc38:	mov	x21, x2
   3dc3c:	asr	x9, x21, #1
   3dc40:	str	x21, [x8, x24, lsl #3]
   3dc44:	cmp	x21, #0x143
   3dc48:	add	x21, x9, #0x1
   3dc4c:	add	x24, x24, #0x1
   3dc50:	b.gt	3dc3c <__gmpn_ni_invertappr@@Base+0x3c>
   3dc54:	lsl	x8, x23, #3
   3dc58:	mvn	x9, x9
   3dc5c:	add	x25, x1, x8
   3dc60:	add	x10, x22, x8
   3dc64:	lsl	x8, x9, #3
   3dc68:	add	x0, x10, x8
   3dc6c:	add	x1, x25, x8
   3dc70:	mov	x2, x21
   3dc74:	mov	x3, x20
   3dc78:	str	x10, [x19, #64]
   3dc7c:	bl	3e0b8 <__gmpn_ni_invertappr@@Base+0x4b8>
   3dc80:	add	x0, x23, #0x1
   3dc84:	str	xzr, [x19, #80]
   3dc88:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   3dc8c:	asr	x8, x23, #1
   3dc90:	add	x2, x8, #0x1
   3dc94:	mov	x1, x23
   3dc98:	bl	3e1f4 <__gmpn_ni_invertappr@@Base+0x5f4>
   3dc9c:	lsl	x1, x0, #3
   3dca0:	mov	w8, #0x7f00                	// #32512
   3dca4:	cmp	x1, x8
   3dca8:	b.hi	3e0a0 <__gmpn_ni_invertappr@@Base+0x4a0>  // b.pmore
   3dcac:	add	x9, x1, #0xf
   3dcb0:	mov	x8, sp
   3dcb4:	and	x9, x9, #0xfffffffffffffff0
   3dcb8:	sub	x8, x8, x9
   3dcbc:	str	x8, [x19, #16]
   3dcc0:	mov	sp, x8
   3dcc4:	add	x8, x20, #0x8
   3dcc8:	stp	x8, x25, [x19, #32]
   3dccc:	add	x8, x22, x23, lsl #3
   3dcd0:	add	x8, x8, #0x8
   3dcd4:	str	x8, [x19, #24]
   3dcd8:	b	3dce8 <__gmpn_ni_invertappr@@Base+0xe8>
   3dcdc:	ldr	x25, [x19, #40]
   3dce0:	mov	x21, x23
   3dce4:	cbz	x24, 3e060 <__gmpn_ni_invertappr@@Base+0x460>
   3dce8:	sub	x24, x24, #0x1
   3dcec:	add	x8, x19, #0x58
   3dcf0:	ldr	x23, [x8, x24, lsl #3]
   3dcf4:	neg	x8, x21
   3dcf8:	str	x8, [x19, #72]
   3dcfc:	add	x0, x23, #0x1
   3dd00:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   3dd04:	add	x8, x23, x21
   3dd08:	cmp	x0, x8
   3dd0c:	ldr	x8, [x19, #64]
   3dd10:	mov	x22, x25
   3dd14:	sub	x25, x25, x23, lsl #3
   3dd18:	sub	x28, x8, x21, lsl #3
   3dd1c:	str	x28, [x19, #56]
   3dd20:	b.le	3dd5c <__gmpn_ni_invertappr@@Base+0x15c>
   3dd24:	mov	x0, x20
   3dd28:	mov	x1, x25
   3dd2c:	mov	x2, x23
   3dd30:	mov	x3, x28
   3dd34:	mov	x4, x21
   3dd38:	bl	cea0 <__gmpn_mul@plt>
   3dd3c:	add	x0, x20, x21, lsl #3
   3dd40:	sub	x8, x23, x21
   3dd44:	add	x3, x8, #0x1
   3dd48:	mov	x1, x0
   3dd4c:	mov	x2, x25
   3dd50:	bl	cc30 <__gmpn_add_n@plt>
   3dd54:	mov	w8, #0x1                   	// #1
   3dd58:	b	3de40 <__gmpn_ni_invertappr@@Base+0x240>
   3dd5c:	ldr	x6, [x19, #16]
   3dd60:	mov	x26, x0
   3dd64:	mov	x0, x20
   3dd68:	mov	x1, x26
   3dd6c:	mov	x2, x25
   3dd70:	mov	x3, x23
   3dd74:	mov	x4, x28
   3dd78:	mov	x5, x21
   3dd7c:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   3dd80:	add	x27, x20, x21, lsl #3
   3dd84:	sub	x28, x26, x21
   3dd88:	mov	x0, x27
   3dd8c:	mov	x1, x27
   3dd90:	mov	x2, x25
   3dd94:	mov	x3, x28
   3dd98:	bl	cc30 <__gmpn_add_n@plt>
   3dd9c:	sub	x3, x23, x28
   3dda0:	mov	x4, x0
   3dda4:	sub	x2, x22, x3, lsl #3
   3dda8:	mov	x0, x20
   3ddac:	mov	x1, x20
   3ddb0:	bl	d060 <__gmpn_add_nc@plt>
   3ddb4:	lsl	x8, x26, #3
   3ddb8:	add	x9, x27, x23, lsl #3
   3ddbc:	mov	w11, #0x1                   	// #1
   3ddc0:	str	x11, [x20, x8]
   3ddc4:	sub	x9, x9, x8
   3ddc8:	ldr	x10, [x9]
   3ddcc:	sub	x11, x11, x0
   3ddd0:	mov	w12, #0x1                   	// #1
   3ddd4:	subs	x10, x10, x11
   3ddd8:	str	x10, [x9]
   3dddc:	b.cs	3de00 <__gmpn_ni_invertappr@@Base+0x200>  // b.hs, b.nlast
   3dde0:	ldr	x10, [x19, #32]
   3dde4:	add	x9, x21, x23
   3dde8:	sub	x9, x9, x26
   3ddec:	add	x9, x10, x9, lsl #3
   3ddf0:	ldr	x10, [x9]
   3ddf4:	sub	x11, x10, #0x1
   3ddf8:	str	x11, [x9], #8
   3ddfc:	cbz	x10, 3ddf0 <__gmpn_ni_invertappr@@Base+0x1f0>
   3de00:	ldr	x9, [x20, x8]
   3de04:	ldr	x10, [x20]
   3de08:	ldr	x28, [x19, #56]
   3de0c:	add	x9, x9, x10
   3de10:	sub	x9, x9, #0x1
   3de14:	str	x9, [x20]
   3de18:	ldr	x8, [x20, x8]
   3de1c:	sub	x8, x12, x8
   3de20:	cmp	x10, x8
   3de24:	b.cs	3de3c <__gmpn_ni_invertappr@@Base+0x23c>  // b.hs, b.nlast
   3de28:	ldr	x8, [x19, #32]
   3de2c:	ldr	x9, [x8]
   3de30:	sub	x10, x9, #0x1
   3de34:	str	x10, [x8], #8
   3de38:	cbz	x9, 3de2c <__gmpn_ni_invertappr@@Base+0x22c>
   3de3c:	mov	x8, xzr
   3de40:	add	x26, x20, x23, lsl #3
   3de44:	neg	x9, x23
   3de48:	ldr	x22, [x26]
   3de4c:	str	x9, [x19, #48]
   3de50:	ldr	x9, [x19, #72]
   3de54:	cmp	x22, #0x1
   3de58:	lsl	x27, x9, #3
   3de5c:	b.hi	3de9c <__gmpn_ni_invertappr@@Base+0x29c>  // b.pmore
   3de60:	add	x28, x22, #0x1
   3de64:	cbz	x22, 3df20 <__gmpn_ni_invertappr@@Base+0x320>
   3de68:	mov	x0, x20
   3de6c:	mov	x1, x25
   3de70:	mov	x2, x23
   3de74:	bl	c570 <__gmpn_cmp@plt>
   3de78:	cmp	w0, #0x1
   3de7c:	b.lt	3df0c <__gmpn_ni_invertappr@@Base+0x30c>  // b.tstop
   3de80:	mov	x0, x20
   3de84:	mov	x1, x20
   3de88:	mov	x2, x25
   3de8c:	mov	x3, x23
   3de90:	bl	c5b0 <__gmpn_sublsh1_n@plt>
   3de94:	add	x28, x22, #0x2
   3de98:	b	3df20 <__gmpn_ni_invertappr@@Base+0x320>
   3de9c:	ldr	x9, [x20]
   3dea0:	subs	x8, x9, x8
   3dea4:	str	x8, [x20]
   3dea8:	b.cs	3dec0 <__gmpn_ni_invertappr@@Base+0x2c0>  // b.hs, b.nlast
   3deac:	ldr	x8, [x19, #32]
   3deb0:	ldr	x9, [x8]
   3deb4:	sub	x10, x9, #0x1
   3deb8:	str	x10, [x8], #8
   3debc:	cbz	x9, 3deb0 <__gmpn_ni_invertappr@@Base+0x2b0>
   3dec0:	ldr	x8, [x26]
   3dec4:	cmn	x8, #0x1
   3dec8:	b.eq	3def4 <__gmpn_ni_invertappr@@Base+0x2f4>  // b.none
   3decc:	mov	x8, x28
   3ded0:	ldr	x9, [x8]
   3ded4:	adds	x9, x9, #0x1
   3ded8:	str	x9, [x8], #8
   3dedc:	b.cs	3ded0 <__gmpn_ni_invertappr@@Base+0x2d0>  // b.hs, b.nlast
   3dee0:	mov	x0, x20
   3dee4:	mov	x1, x20
   3dee8:	mov	x2, x25
   3deec:	mov	x3, x23
   3def0:	bl	cc30 <__gmpn_add_n@plt>
   3def4:	add	x8, x20, x23, lsl #4
   3def8:	add	x0, x8, x27
   3defc:	add	x1, x26, x27
   3df00:	mov	x2, x21
   3df04:	bl	c3e0 <__gmpn_com@plt>
   3df08:	b	3dfcc <__gmpn_ni_invertappr@@Base+0x3cc>
   3df0c:	mov	x0, x20
   3df10:	mov	x1, x20
   3df14:	mov	x2, x25
   3df18:	mov	x3, x23
   3df1c:	bl	c420 <__gmpn_sub_n@plt>
   3df20:	mov	x0, x20
   3df24:	mov	x1, x25
   3df28:	mov	x2, x23
   3df2c:	bl	c570 <__gmpn_cmp@plt>
   3df30:	cmp	w0, #0x1
   3df34:	b.lt	3df54 <__gmpn_ni_invertappr@@Base+0x354>  // b.tstop
   3df38:	mov	x0, x26
   3df3c:	mov	x1, x20
   3df40:	mov	x2, x25
   3df44:	mov	x3, x23
   3df48:	bl	d260 <__gmpn_rsblsh1_n@plt>
   3df4c:	add	x28, x28, #0x1
   3df50:	b	3df98 <__gmpn_ni_invertappr@@Base+0x398>
   3df54:	ldr	x9, [x19, #40]
   3df58:	add	x8, x20, x23, lsl #4
   3df5c:	sub	x2, x23, x21
   3df60:	mov	x0, x20
   3df64:	add	x9, x9, x27
   3df68:	str	x9, [x19, #8]
   3df6c:	add	x9, x26, x27
   3df70:	mov	x1, x25
   3df74:	str	x9, [x19]
   3df78:	add	x22, x8, x27
   3df7c:	bl	c570 <__gmpn_cmp@plt>
   3df80:	ldp	x2, x1, [x19]
   3df84:	cmp	w0, #0x0
   3df88:	cset	w4, gt
   3df8c:	mov	x0, x22
   3df90:	mov	x3, x21
   3df94:	bl	c8f0 <__gmpn_sub_nc@plt>
   3df98:	ldr	x9, [x19, #56]
   3df9c:	ldr	x8, [x9]
   3dfa0:	subs	x8, x8, x28
   3dfa4:	mov	x28, x9
   3dfa8:	str	x8, [x9]
   3dfac:	b.cs	3dfcc <__gmpn_ni_invertappr@@Base+0x3cc>  // b.hs, b.nlast
   3dfb0:	ldr	x8, [x19, #24]
   3dfb4:	ldr	x9, [x19, #72]
   3dfb8:	add	x8, x8, x9, lsl #3
   3dfbc:	ldr	x9, [x8]
   3dfc0:	sub	x10, x9, #0x1
   3dfc4:	str	x10, [x8], #8
   3dfc8:	cbz	x9, 3dfbc <__gmpn_ni_invertappr@@Base+0x3bc>
   3dfcc:	add	x8, x20, x23, lsl #4
   3dfd0:	add	x22, x8, x27
   3dfd4:	mov	x0, x20
   3dfd8:	mov	x1, x22
   3dfdc:	mov	x2, x28
   3dfe0:	mov	x3, x21
   3dfe4:	bl	cb50 <__gmpn_mul_n@plt>
   3dfe8:	lsl	x25, x21, #3
   3dfec:	lsl	x28, x21, #1
   3dff0:	add	x0, x20, x25
   3dff4:	sub	x3, x28, x23
   3dff8:	mov	x1, x0
   3dffc:	mov	x2, x22
   3e000:	bl	cc30 <__gmpn_add_n@plt>
   3e004:	ldr	x8, [x19, #48]
   3e008:	add	x2, x26, x25
   3e00c:	ldr	x25, [x19, #64]
   3e010:	add	x22, x28, x21
   3e014:	lsl	x8, x8, #3
   3e018:	add	x9, x20, x22, lsl #3
   3e01c:	mov	x4, x0
   3e020:	add	x0, x25, x8
   3e024:	add	x1, x9, x8
   3e028:	sub	x3, x23, x21
   3e02c:	bl	d060 <__gmpn_add_nc@plt>
   3e030:	ldr	x8, [x25, x27]
   3e034:	adds	x8, x8, x0
   3e038:	str	x8, [x25, x27]
   3e03c:	b.cc	3dcdc <__gmpn_ni_invertappr@@Base+0xdc>  // b.lo, b.ul, b.last
   3e040:	ldr	x8, [x19, #24]
   3e044:	ldr	x9, [x19, #72]
   3e048:	add	x8, x8, x9, lsl #3
   3e04c:	ldr	x9, [x8]
   3e050:	adds	x9, x9, #0x1
   3e054:	str	x9, [x8], #8
   3e058:	b.cs	3e04c <__gmpn_ni_invertappr@@Base+0x44c>  // b.hs, b.nlast
   3e05c:	b	3dcdc <__gmpn_ni_invertappr@@Base+0xdc>
   3e060:	mvn	x8, x23
   3e064:	add	x8, x22, x8
   3e068:	ldr	x8, [x20, x8, lsl #3]
   3e06c:	ldr	x0, [x19, #80]
   3e070:	cmn	x8, #0x8
   3e074:	cset	w20, hi  // hi = pmore
   3e078:	cbnz	x0, 3e0b0 <__gmpn_ni_invertappr@@Base+0x4b0>
   3e07c:	mov	x0, x20
   3e080:	mov	sp, x29
   3e084:	ldp	x20, x19, [sp, #80]
   3e088:	ldp	x22, x21, [sp, #64]
   3e08c:	ldp	x24, x23, [sp, #48]
   3e090:	ldp	x26, x25, [sp, #32]
   3e094:	ldp	x28, x27, [sp, #16]
   3e098:	ldp	x29, x30, [sp], #96
   3e09c:	ret
   3e0a0:	add	x0, x19, #0x50
   3e0a4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   3e0a8:	str	x0, [x19, #16]
   3e0ac:	b	3dcc4 <__gmpn_ni_invertappr@@Base+0xc4>
   3e0b0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   3e0b4:	b	3e07c <__gmpn_ni_invertappr@@Base+0x47c>
   3e0b8:	stp	x29, x30, [sp, #-64]!
   3e0bc:	stp	x20, x19, [sp, #48]
   3e0c0:	mov	x20, x1
   3e0c4:	cmp	x2, #0x1
   3e0c8:	mov	x19, x0
   3e0cc:	stp	x24, x23, [sp, #16]
   3e0d0:	stp	x22, x21, [sp, #32]
   3e0d4:	mov	x29, sp
   3e0d8:	b.ne	3e0f4 <__gmpn_ni_invertappr@@Base+0x4f4>  // b.any
   3e0dc:	ldr	x0, [x20]
   3e0e0:	bl	d5d0 <__gmpn_invert_limb@plt>
   3e0e4:	mov	x8, x0
   3e0e8:	mov	x0, xzr
   3e0ec:	str	x8, [x19]
   3e0f0:	b	3e1e0 <__gmpn_ni_invertappr@@Base+0x5e0>
   3e0f4:	lsl	x23, x2, #3
   3e0f8:	mov	x22, x2
   3e0fc:	mov	w1, #0xff                  	// #255
   3e100:	mov	x0, x3
   3e104:	mov	x2, x23
   3e108:	mov	x21, x3
   3e10c:	bl	c780 <memset@plt>
   3e110:	add	x0, x21, x23
   3e114:	mov	x1, x20
   3e118:	mov	x2, x22
   3e11c:	bl	c3e0 <__gmpn_com@plt>
   3e120:	cmp	x22, #0x2
   3e124:	b.ne	3e148 <__gmpn_ni_invertappr@@Base+0x548>  // b.any
   3e128:	mov	w3, #0x4                   	// #4
   3e12c:	mov	x0, x19
   3e130:	mov	x1, xzr
   3e134:	mov	x2, x21
   3e138:	mov	x4, x20
   3e13c:	bl	c350 <__gmpn_divrem_2@plt>
   3e140:	mov	x0, xzr
   3e144:	b	3e1e0 <__gmpn_ni_invertappr@@Base+0x5e0>
   3e148:	add	x24, x20, x22, lsl #3
   3e14c:	ldur	x23, [x24, #-8]
   3e150:	mov	x0, x23
   3e154:	bl	d5d0 <__gmpn_invert_limb@plt>
   3e158:	ldur	x8, [x24, #-16]
   3e15c:	mul	x9, x0, x23
   3e160:	adds	x9, x9, x8
   3e164:	b.cc	3e180 <__gmpn_ni_invertappr@@Base+0x580>  // b.lo, b.ul, b.last
   3e168:	subs	x9, x9, x23
   3e16c:	cset	w10, cs  // cs = hs, nlast
   3e170:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   3e174:	mvn	x10, x10
   3e178:	add	x0, x10, x0
   3e17c:	sub	x9, x9, x11
   3e180:	umulh	x10, x8, x0
   3e184:	adds	x9, x10, x9
   3e188:	b.cc	3e1b0 <__gmpn_ni_invertappr@@Base+0x5b0>  // b.lo, b.ul, b.last
   3e18c:	cmp	x9, x23
   3e190:	sub	x5, x0, #0x1
   3e194:	b.cc	3e1b4 <__gmpn_ni_invertappr@@Base+0x5b4>  // b.lo, b.ul, b.last
   3e198:	mul	x10, x0, x8
   3e19c:	cmp	x9, x23
   3e1a0:	sub	x11, x0, #0x2
   3e1a4:	ccmp	x10, x8, #0x2, ls  // ls = plast
   3e1a8:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   3e1ac:	b	3e1b4 <__gmpn_ni_invertappr@@Base+0x5b4>
   3e1b0:	mov	x5, x0
   3e1b4:	lsl	x2, x22, #1
   3e1b8:	mov	x0, x19
   3e1bc:	mov	x1, x21
   3e1c0:	mov	x3, x20
   3e1c4:	mov	x4, x22
   3e1c8:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   3e1cc:	mov	w0, #0x1                   	// #1
   3e1d0:	ldr	x8, [x19]
   3e1d4:	sub	x9, x8, #0x1
   3e1d8:	str	x9, [x19], #8
   3e1dc:	cbz	x8, 3e1d0 <__gmpn_ni_invertappr@@Base+0x5d0>
   3e1e0:	ldp	x20, x19, [sp, #48]
   3e1e4:	ldp	x22, x21, [sp, #32]
   3e1e8:	ldp	x24, x23, [sp, #16]
   3e1ec:	ldp	x29, x30, [sp], #64
   3e1f0:	ret
   3e1f4:	asr	x8, x0, #1
   3e1f8:	cmp	x8, x2
   3e1fc:	csel	x9, x0, x8, lt  // lt = tstop
   3e200:	cmp	x8, x1
   3e204:	csel	x8, x9, xzr, lt  // lt = tstop
   3e208:	add	x8, x0, x8
   3e20c:	add	x0, x8, #0x4
   3e210:	ret

000000000003e214 <__gmpn_invertappr@@Base>:
   3e214:	stp	x29, x30, [sp, #-16]!
   3e218:	cmp	x2, #0xa2
   3e21c:	mov	x29, sp
   3e220:	b.le	3e230 <__gmpn_invertappr@@Base+0x1c>
   3e224:	bl	d360 <__gmpn_ni_invertappr@plt>
   3e228:	ldp	x29, x30, [sp], #16
   3e22c:	ret
   3e230:	bl	3e0b8 <__gmpn_ni_invertappr@@Base+0x4b8>
   3e234:	ldp	x29, x30, [sp], #16
   3e238:	ret

000000000003e23c <__gmpn_invert@@Base>:
   3e23c:	stp	x29, x30, [sp, #-64]!
   3e240:	stp	x20, x19, [sp, #48]
   3e244:	mov	x20, x1
   3e248:	cmp	x2, #0x1
   3e24c:	mov	x19, x0
   3e250:	stp	x24, x23, [sp, #16]
   3e254:	stp	x22, x21, [sp, #32]
   3e258:	mov	x29, sp
   3e25c:	b.ne	3e270 <__gmpn_invert@@Base+0x34>  // b.any
   3e260:	ldr	x0, [x20]
   3e264:	bl	d5d0 <__gmpn_invert_limb@plt>
   3e268:	str	x0, [x19]
   3e26c:	b	3e3d8 <__gmpn_invert@@Base+0x19c>
   3e270:	mov	x21, x3
   3e274:	mov	x22, x2
   3e278:	cmp	x2, #0xa1
   3e27c:	b.le	3e30c <__gmpn_invert@@Base+0xd0>
   3e280:	mov	x0, x19
   3e284:	mov	x1, x20
   3e288:	mov	x2, x22
   3e28c:	mov	x3, x21
   3e290:	bl	d360 <__gmpn_ni_invertappr@plt>
   3e294:	cbz	x0, 3e3d8 <__gmpn_invert@@Base+0x19c>
   3e298:	mov	x0, x21
   3e29c:	mov	x1, x19
   3e2a0:	mov	x2, x20
   3e2a4:	mov	x3, x22
   3e2a8:	bl	cb50 <__gmpn_mul_n@plt>
   3e2ac:	mov	x0, x21
   3e2b0:	mov	x1, x21
   3e2b4:	mov	x2, x20
   3e2b8:	mov	x3, x22
   3e2bc:	bl	cc30 <__gmpn_add_n@plt>
   3e2c0:	mov	x4, x0
   3e2c4:	cbz	x0, 3e2e0 <__gmpn_invert@@Base+0xa4>
   3e2c8:	add	x0, x21, x22, lsl #3
   3e2cc:	mov	x1, x0
   3e2d0:	mov	x2, x20
   3e2d4:	mov	x3, x22
   3e2d8:	bl	d060 <__gmpn_add_nc@plt>
   3e2dc:	mov	x4, x0
   3e2e0:	ldr	x8, [x19]
   3e2e4:	eor	x9, x4, #0x1
   3e2e8:	adds	x8, x8, x9
   3e2ec:	str	x8, [x19]
   3e2f0:	b.cc	3e3d8 <__gmpn_invert@@Base+0x19c>  // b.lo, b.ul, b.last
   3e2f4:	add	x8, x19, #0x8
   3e2f8:	ldr	x9, [x8]
   3e2fc:	adds	x9, x9, #0x1
   3e300:	str	x9, [x8], #8
   3e304:	b.cs	3e2f8 <__gmpn_invert@@Base+0xbc>  // b.hs, b.nlast
   3e308:	b	3e3d8 <__gmpn_invert@@Base+0x19c>
   3e30c:	lsl	x23, x22, #3
   3e310:	mov	w1, #0xff                  	// #255
   3e314:	mov	x0, x21
   3e318:	mov	x2, x23
   3e31c:	bl	c780 <memset@plt>
   3e320:	add	x0, x21, x23
   3e324:	mov	x1, x20
   3e328:	mov	x2, x22
   3e32c:	bl	c3e0 <__gmpn_com@plt>
   3e330:	cmp	x22, #0x2
   3e334:	b.ne	3e354 <__gmpn_invert@@Base+0x118>  // b.any
   3e338:	mov	w3, #0x4                   	// #4
   3e33c:	mov	x0, x19
   3e340:	mov	x1, xzr
   3e344:	mov	x2, x21
   3e348:	mov	x4, x20
   3e34c:	bl	c350 <__gmpn_divrem_2@plt>
   3e350:	b	3e3d8 <__gmpn_invert@@Base+0x19c>
   3e354:	add	x24, x20, x22, lsl #3
   3e358:	ldur	x23, [x24, #-8]
   3e35c:	mov	x0, x23
   3e360:	bl	d5d0 <__gmpn_invert_limb@plt>
   3e364:	ldur	x8, [x24, #-16]
   3e368:	mul	x9, x0, x23
   3e36c:	adds	x9, x9, x8
   3e370:	b.cc	3e38c <__gmpn_invert@@Base+0x150>  // b.lo, b.ul, b.last
   3e374:	subs	x9, x9, x23
   3e378:	cset	w10, cs  // cs = hs, nlast
   3e37c:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   3e380:	mvn	x10, x10
   3e384:	add	x0, x10, x0
   3e388:	sub	x9, x9, x11
   3e38c:	umulh	x10, x8, x0
   3e390:	adds	x9, x10, x9
   3e394:	b.cc	3e3bc <__gmpn_invert@@Base+0x180>  // b.lo, b.ul, b.last
   3e398:	cmp	x9, x23
   3e39c:	sub	x5, x0, #0x1
   3e3a0:	b.cc	3e3c0 <__gmpn_invert@@Base+0x184>  // b.lo, b.ul, b.last
   3e3a4:	mul	x10, x0, x8
   3e3a8:	cmp	x9, x23
   3e3ac:	sub	x11, x0, #0x2
   3e3b0:	ccmp	x10, x8, #0x2, ls  // ls = plast
   3e3b4:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   3e3b8:	b	3e3c0 <__gmpn_invert@@Base+0x184>
   3e3bc:	mov	x5, x0
   3e3c0:	lsl	x2, x22, #1
   3e3c4:	mov	x0, x19
   3e3c8:	mov	x1, x21
   3e3cc:	mov	x3, x20
   3e3d0:	mov	x4, x22
   3e3d4:	bl	d0b0 <__gmpn_sbpi1_div_q@plt>
   3e3d8:	ldp	x20, x19, [sp, #48]
   3e3dc:	ldp	x22, x21, [sp, #32]
   3e3e0:	ldp	x24, x23, [sp, #16]
   3e3e4:	ldp	x29, x30, [sp], #64
   3e3e8:	ret

000000000003e3ec <__gmpn_binvert_itch@@Base>:
   3e3ec:	stp	x29, x30, [sp, #-32]!
   3e3f0:	stp	x20, x19, [sp, #16]
   3e3f4:	mov	x29, sp
   3e3f8:	mov	x19, x0
   3e3fc:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   3e400:	add	x8, x19, #0x1
   3e404:	asr	x2, x8, #1
   3e408:	mov	x1, x19
   3e40c:	mov	x20, x0
   3e410:	bl	3e424 <__gmpn_binvert_itch@@Base+0x38>
   3e414:	add	x0, x0, x20
   3e418:	ldp	x20, x19, [sp, #16]
   3e41c:	ldp	x29, x30, [sp], #32
   3e420:	ret
   3e424:	asr	x8, x0, #1
   3e428:	cmp	x8, x2
   3e42c:	csel	x9, x0, x8, lt  // lt = tstop
   3e430:	cmp	x8, x1
   3e434:	csel	x8, x9, xzr, lt  // lt = tstop
   3e438:	add	x8, x0, x8
   3e43c:	add	x0, x8, #0x4
   3e440:	ret

000000000003e444 <__gmpn_binvert@@Base>:
   3e444:	sub	sp, sp, #0x1a0
   3e448:	stp	x24, x23, [sp, #368]
   3e44c:	stp	x22, x21, [sp, #384]
   3e450:	stp	x20, x19, [sp, #400]
   3e454:	mov	x19, x3
   3e458:	mov	x20, x2
   3e45c:	mov	x21, x1
   3e460:	cmp	x2, #0xc2
   3e464:	mov	x22, x0
   3e468:	mov	x24, sp
   3e46c:	stp	x29, x30, [sp, #320]
   3e470:	stp	x28, x27, [sp, #336]
   3e474:	stp	x26, x25, [sp, #352]
   3e478:	add	x29, sp, #0x140
   3e47c:	b.lt	3e510 <__gmpn_binvert@@Base+0xcc>  // b.tstop
   3e480:	mov	x8, x20
   3e484:	add	x9, x8, #0x1
   3e488:	asr	x23, x9, #1
   3e48c:	cmp	x8, #0x182
   3e490:	str	x8, [x24], #8
   3e494:	mov	x8, x23
   3e498:	b.gt	3e484 <__gmpn_binvert@@Base+0x40>
   3e49c:	cbz	x23, 3e4b0 <__gmpn_binvert@@Base+0x6c>
   3e4a0:	lsl	x2, x23, #3
   3e4a4:	mov	x0, x19
   3e4a8:	mov	w1, wzr
   3e4ac:	bl	c780 <memset@plt>
   3e4b0:	mov	w8, #0x1                   	// #1
   3e4b4:	str	x8, [x19]
   3e4b8:	ldr	x8, [x21]
   3e4bc:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   3e4c0:	ldr	x9, [x9, #3952]
   3e4c4:	orr	x11, xzr, #0xfffffffffffffffe
   3e4c8:	ubfx	x10, x8, #1, #7
   3e4cc:	cmp	x23, #0x5c
   3e4d0:	ldrb	w9, [x9, x10]
   3e4d4:	mov	w10, #0x2                   	// #2
   3e4d8:	mov	x0, x22
   3e4dc:	mov	x1, x19
   3e4e0:	msub	x12, x8, x9, x10
   3e4e4:	mul	x9, x12, x9
   3e4e8:	msub	x10, x9, x8, x10
   3e4ec:	mul	x9, x9, x10
   3e4f0:	madd	x8, x9, x8, x11
   3e4f4:	mul	x5, x8, x9
   3e4f8:	mov	x2, x23
   3e4fc:	mov	x3, x21
   3e500:	mov	x4, x23
   3e504:	b.le	3e51c <__gmpn_binvert@@Base+0xd8>
   3e508:	bl	cfe0 <__gmpn_dcpi1_bdiv_q@plt>
   3e50c:	b	3e520 <__gmpn_binvert@@Base+0xdc>
   3e510:	mov	x23, x20
   3e514:	cbnz	x23, 3e4a0 <__gmpn_binvert@@Base+0x5c>
   3e518:	b	3e4b0 <__gmpn_binvert@@Base+0x6c>
   3e51c:	bl	c680 <__gmpn_sbpi1_bdiv_q@plt>
   3e520:	mov	x0, x22
   3e524:	mov	x1, x22
   3e528:	mov	x2, x23
   3e52c:	bl	ce90 <__gmpn_neg@plt>
   3e530:	cmp	x23, x20
   3e534:	b.ge	3e5c4 <__gmpn_binvert@@Base+0x180>  // b.tcont
   3e538:	sub	x27, x24, #0x8
   3e53c:	ldr	x24, [x27], #-8
   3e540:	mov	x0, x24
   3e544:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   3e548:	mov	x25, x0
   3e54c:	add	x26, x19, x0, lsl #3
   3e550:	mov	x0, x19
   3e554:	mov	x1, x25
   3e558:	mov	x2, x21
   3e55c:	mov	x3, x24
   3e560:	mov	x4, x22
   3e564:	mov	x5, x23
   3e568:	mov	x6, x26
   3e56c:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   3e570:	add	x8, x24, x23
   3e574:	sub	x2, x8, x25
   3e578:	mov	w3, #0x1                   	// #1
   3e57c:	mov	x0, x26
   3e580:	mov	x1, x19
   3e584:	bl	caf0 <__gmpn_sub_1@plt>
   3e588:	lsl	x8, x23, #3
   3e58c:	add	x25, x22, x8
   3e590:	sub	x23, x24, x23
   3e594:	add	x2, x19, x8
   3e598:	mov	x0, x25
   3e59c:	mov	x1, x22
   3e5a0:	mov	x3, x23
   3e5a4:	bl	d090 <__gmpn_mullo_n@plt>
   3e5a8:	mov	x0, x25
   3e5ac:	mov	x1, x25
   3e5b0:	mov	x2, x23
   3e5b4:	bl	ce90 <__gmpn_neg@plt>
   3e5b8:	cmp	x24, x20
   3e5bc:	mov	x23, x24
   3e5c0:	b.lt	3e53c <__gmpn_binvert@@Base+0xf8>  // b.tstop
   3e5c4:	ldp	x20, x19, [sp, #400]
   3e5c8:	ldp	x22, x21, [sp, #384]
   3e5cc:	ldp	x24, x23, [sp, #368]
   3e5d0:	ldp	x26, x25, [sp, #352]
   3e5d4:	ldp	x28, x27, [sp, #336]
   3e5d8:	ldp	x29, x30, [sp, #320]
   3e5dc:	add	sp, sp, #0x1a0
   3e5e0:	ret

000000000003e5e4 <__gmpn_bc_mulmod_bnm1@@Base>:
   3e5e4:	stp	x29, x30, [sp, #-48]!
   3e5e8:	stp	x20, x19, [sp, #32]
   3e5ec:	mov	x19, x0
   3e5f0:	mov	x0, x4
   3e5f4:	str	x21, [sp, #16]
   3e5f8:	mov	x29, sp
   3e5fc:	mov	x20, x4
   3e600:	mov	x21, x3
   3e604:	bl	cb50 <__gmpn_mul_n@plt>
   3e608:	add	x2, x20, x21, lsl #3
   3e60c:	mov	x0, x19
   3e610:	mov	x1, x20
   3e614:	mov	x3, x21
   3e618:	bl	cc30 <__gmpn_add_n@plt>
   3e61c:	ldr	x8, [x19]
   3e620:	adds	x8, x8, x0
   3e624:	str	x8, [x19]
   3e628:	b.cc	3e640 <__gmpn_bc_mulmod_bnm1@@Base+0x5c>  // b.lo, b.ul, b.last
   3e62c:	add	x8, x19, #0x8
   3e630:	ldr	x9, [x8]
   3e634:	adds	x9, x9, #0x1
   3e638:	str	x9, [x8], #8
   3e63c:	b.cs	3e630 <__gmpn_bc_mulmod_bnm1@@Base+0x4c>  // b.hs, b.nlast
   3e640:	ldp	x20, x19, [sp, #32]
   3e644:	ldr	x21, [sp, #16]
   3e648:	ldp	x29, x30, [sp], #48
   3e64c:	ret

000000000003e650 <__gmpn_mulmod_bnm1@@Base>:
   3e650:	sub	sp, sp, #0x80
   3e654:	stp	x28, x27, [sp, #48]
   3e658:	stp	x26, x25, [sp, #64]
   3e65c:	stp	x22, x21, [sp, #96]
   3e660:	stp	x20, x19, [sp, #112]
   3e664:	mov	x20, x6
   3e668:	mov	x25, x5
   3e66c:	mov	x28, x4
   3e670:	mov	x22, x3
   3e674:	mov	x27, x2
   3e678:	mov	x21, x1
   3e67c:	cmp	x1, #0xa
   3e680:	mov	x19, x0
   3e684:	stp	x29, x30, [sp, #32]
   3e688:	stp	x24, x23, [sp, #80]
   3e68c:	add	x29, sp, #0x20
   3e690:	b.lt	3e890 <__gmpn_mulmod_bnm1@@Base+0x240>  // b.tstop
   3e694:	tbnz	w21, #0, 3e890 <__gmpn_mulmod_bnm1@@Base+0x240>
   3e698:	asr	x24, x21, #1
   3e69c:	cmp	x24, x22
   3e6a0:	stur	x28, [x29, #-8]
   3e6a4:	str	x22, [sp, #16]
   3e6a8:	b.ge	3e8b4 <__gmpn_mulmod_bnm1@@Base+0x264>  // b.tcont
   3e6ac:	add	x28, x27, x24, lsl #3
   3e6b0:	sub	x23, x22, x24
   3e6b4:	mov	x0, x20
   3e6b8:	mov	x1, x27
   3e6bc:	mov	x2, x24
   3e6c0:	mov	x3, x28
   3e6c4:	mov	x4, x23
   3e6c8:	bl	c970 <__gmpn_add@plt>
   3e6cc:	ldr	x8, [x20]
   3e6d0:	adds	x8, x8, x0
   3e6d4:	str	x8, [x20]
   3e6d8:	b.cc	3e6f0 <__gmpn_mulmod_bnm1@@Base+0xa0>  // b.lo, b.ul, b.last
   3e6dc:	add	x8, x20, #0x8
   3e6e0:	ldr	x9, [x8]
   3e6e4:	adds	x9, x9, #0x1
   3e6e8:	str	x9, [x8], #8
   3e6ec:	b.cs	3e6e0 <__gmpn_mulmod_bnm1@@Base+0x90>  // b.hs, b.nlast
   3e6f0:	cmp	x24, x25
   3e6f4:	add	x26, x20, x24, lsl #3
   3e6f8:	str	x25, [sp, #8]
   3e6fc:	b.ge	3e944 <__gmpn_mulmod_bnm1@@Base+0x2f4>  // b.tcont
   3e700:	ldur	x1, [x29, #-8]
   3e704:	sub	x4, x25, x24
   3e708:	mov	x0, x26
   3e70c:	mov	x2, x24
   3e710:	add	x3, x1, x24, lsl #3
   3e714:	bl	c970 <__gmpn_add@plt>
   3e718:	ldr	x8, [x26]
   3e71c:	adds	x8, x8, x0
   3e720:	str	x8, [x26]
   3e724:	b.cc	3e740 <__gmpn_mulmod_bnm1@@Base+0xf0>  // b.lo, b.ul, b.last
   3e728:	add	x8, x20, x24, lsl #3
   3e72c:	add	x8, x8, #0x8
   3e730:	ldr	x9, [x8]
   3e734:	adds	x9, x9, #0x1
   3e738:	str	x9, [x8], #8
   3e73c:	b.cs	3e730 <__gmpn_mulmod_bnm1@@Base+0xe0>  // b.hs, b.nlast
   3e740:	add	x6, x26, x24, lsl #3
   3e744:	mov	x5, x24
   3e748:	mov	x0, x19
   3e74c:	mov	x1, x24
   3e750:	mov	x2, x20
   3e754:	mov	x3, x24
   3e758:	mov	x4, x26
   3e75c:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   3e760:	and	x22, x21, #0xfffffffffffffffe
   3e764:	add	x25, x20, x22, lsl #3
   3e768:	add	x26, x25, #0x10
   3e76c:	mov	x0, x26
   3e770:	mov	x1, x27
   3e774:	mov	x2, x24
   3e778:	mov	x3, x28
   3e77c:	mov	x4, x23
   3e780:	bl	d340 <__gmpn_sub@plt>
   3e784:	add	x23, x26, x24, lsl #3
   3e788:	str	xzr, [x23]
   3e78c:	ldr	x8, [x25, #16]
   3e790:	adds	x8, x8, x0
   3e794:	str	x8, [x25, #16]
   3e798:	b.cc	3e7b0 <__gmpn_mulmod_bnm1@@Base+0x160>  // b.lo, b.ul, b.last
   3e79c:	add	x8, x25, #0x18
   3e7a0:	ldr	x9, [x8]
   3e7a4:	adds	x9, x9, #0x1
   3e7a8:	str	x9, [x8], #8
   3e7ac:	b.cs	3e7a0 <__gmpn_mulmod_bnm1@@Base+0x150>  // b.hs, b.nlast
   3e7b0:	ldr	x8, [x23]
   3e7b4:	ldr	x25, [sp, #8]
   3e7b8:	add	x27, x8, x24
   3e7bc:	cmp	x24, x25
   3e7c0:	b.ge	3e954 <__gmpn_mulmod_bnm1@@Base+0x304>  // b.tcont
   3e7c4:	ldur	x1, [x29, #-8]
   3e7c8:	add	x28, x23, #0x8
   3e7cc:	sub	x4, x25, x24
   3e7d0:	mov	x0, x28
   3e7d4:	add	x3, x1, x24, lsl #3
   3e7d8:	mov	x2, x24
   3e7dc:	bl	d340 <__gmpn_sub@plt>
   3e7e0:	lsl	x8, x21, #3
   3e7e4:	orr	x8, x8, #0x8
   3e7e8:	str	xzr, [x26, x8]
   3e7ec:	ldr	x8, [x23, #8]
   3e7f0:	adds	x8, x8, x0
   3e7f4:	str	x8, [x23, #8]
   3e7f8:	b.cc	3e818 <__gmpn_mulmod_bnm1@@Base+0x1c8>  // b.lo, b.ul, b.last
   3e7fc:	add	x8, x24, x22
   3e800:	add	x8, x20, x8, lsl #3
   3e804:	add	x8, x8, #0x20
   3e808:	ldr	x9, [x8]
   3e80c:	adds	x9, x9, #0x1
   3e810:	str	x9, [x8], #8
   3e814:	b.cs	3e808 <__gmpn_mulmod_bnm1@@Base+0x1b8>  // b.hs, b.nlast
   3e818:	ldr	x8, [x28, x24, lsl #3]
   3e81c:	add	x23, x8, x24
   3e820:	cmp	x21, #0x278
   3e824:	b.lt	3e964 <__gmpn_mulmod_bnm1@@Base+0x314>  // b.tstop
   3e828:	mov	x0, x24
   3e82c:	mov	w1, wzr
   3e830:	bl	cc90 <__gmpn_fft_best_k@plt>
   3e834:	mov	w8, #0xffffffff            	// #-1
   3e838:	lsl	w8, w8, w0
   3e83c:	mvn	w8, w8
   3e840:	sxtw	x9, w8
   3e844:	mov	w6, w0
   3e848:	tst	x24, x9
   3e84c:	b.eq	3e864 <__gmpn_mulmod_bnm1@@Base+0x214>  // b.none
   3e850:	sbfx	x9, x8, #1, #31
   3e854:	asr	w8, w8, #1
   3e858:	tst	x24, x9
   3e85c:	sub	w6, w6, #0x1
   3e860:	b.ne	3e850 <__gmpn_mulmod_bnm1@@Base+0x200>  // b.any
   3e864:	cmp	w6, #0x4
   3e868:	b.lt	3e970 <__gmpn_mulmod_bnm1@@Base+0x320>  // b.tstop
   3e86c:	mov	x0, x20
   3e870:	mov	x1, x24
   3e874:	mov	x2, x26
   3e878:	mov	x3, x27
   3e87c:	mov	x4, x28
   3e880:	mov	x5, x23
   3e884:	bl	c460 <__gmpn_mul_fft@plt>
   3e888:	str	x0, [x20, x24, lsl #3]
   3e88c:	b	3e994 <__gmpn_mulmod_bnm1@@Base+0x344>
   3e890:	cmp	x25, x21
   3e894:	b.lt	3e8e0 <__gmpn_mulmod_bnm1@@Base+0x290>  // b.tstop
   3e898:	mov	x0, x19
   3e89c:	mov	x1, x27
   3e8a0:	mov	x2, x28
   3e8a4:	mov	x3, x21
   3e8a8:	mov	x4, x20
   3e8ac:	bl	c450 <__gmpn_bc_mulmod_bnm1@plt>
   3e8b0:	b	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>
   3e8b4:	mov	x0, x19
   3e8b8:	mov	x1, x24
   3e8bc:	mov	x2, x27
   3e8c0:	mov	x3, x22
   3e8c4:	mov	x4, x28
   3e8c8:	mov	x5, x25
   3e8cc:	mov	x6, x20
   3e8d0:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   3e8d4:	mov	x26, x27
   3e8d8:	mov	x27, x22
   3e8dc:	b	3e958 <__gmpn_mulmod_bnm1@@Base+0x308>
   3e8e0:	add	x8, x25, x22
   3e8e4:	subs	x23, x8, x21
   3e8e8:	b.le	3eb40 <__gmpn_mulmod_bnm1@@Base+0x4f0>
   3e8ec:	mov	x0, x20
   3e8f0:	mov	x1, x27
   3e8f4:	mov	x2, x22
   3e8f8:	mov	x3, x28
   3e8fc:	mov	x4, x25
   3e900:	bl	cea0 <__gmpn_mul@plt>
   3e904:	add	x3, x20, x21, lsl #3
   3e908:	mov	x0, x19
   3e90c:	mov	x1, x20
   3e910:	mov	x2, x21
   3e914:	mov	x4, x23
   3e918:	bl	c970 <__gmpn_add@plt>
   3e91c:	ldr	x8, [x19]
   3e920:	adds	x8, x8, x0
   3e924:	str	x8, [x19]
   3e928:	b.cc	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>  // b.lo, b.ul, b.last
   3e92c:	add	x8, x19, #0x8
   3e930:	ldr	x9, [x8]
   3e934:	adds	x9, x9, #0x1
   3e938:	str	x9, [x8], #8
   3e93c:	b.cs	3e930 <__gmpn_mulmod_bnm1@@Base+0x2e0>  // b.hs, b.nlast
   3e940:	b	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>
   3e944:	mov	x6, x26
   3e948:	ldur	x26, [x29, #-8]
   3e94c:	mov	x5, x25
   3e950:	b	3e748 <__gmpn_mulmod_bnm1@@Base+0xf8>
   3e954:	ldur	x28, [x29, #-8]
   3e958:	mov	x23, x25
   3e95c:	cmp	x21, #0x278
   3e960:	b.ge	3e828 <__gmpn_mulmod_bnm1@@Base+0x1d8>  // b.tcont
   3e964:	mov	w6, wzr
   3e968:	cmp	w6, #0x4
   3e96c:	b.ge	3e86c <__gmpn_mulmod_bnm1@@Base+0x21c>  // b.tcont
   3e970:	ldur	x8, [x29, #-8]
   3e974:	mov	x0, x20
   3e978:	mov	x1, x26
   3e97c:	cmp	x28, x8
   3e980:	b.eq	3ead8 <__gmpn_mulmod_bnm1@@Base+0x488>  // b.none
   3e984:	mov	x2, x28
   3e988:	mov	x3, x24
   3e98c:	mov	x4, x20
   3e990:	bl	3eb5c <__gmpn_mulmod_bnm1@@Base+0x50c>
   3e994:	lsl	x22, x24, #3
   3e998:	ldr	x23, [x20, x22]
   3e99c:	mov	x0, x19
   3e9a0:	mov	x1, x19
   3e9a4:	mov	x2, x20
   3e9a8:	mov	x3, x24
   3e9ac:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3e9b0:	mov	x8, xzr
   3e9b4:	add	x9, x22, x19
   3e9b8:	add	x11, x0, x23
   3e9bc:	ldur	x10, [x9, #-8]
   3e9c0:	lsr	x12, x11, #1
   3e9c4:	lsl	x11, x11, #63
   3e9c8:	adds	x13, x10, x11
   3e9cc:	adc	x8, x12, x8
   3e9d0:	stur	x13, [x9, #-8]
   3e9d4:	ldr	x9, [x19]
   3e9d8:	ldr	x10, [sp, #16]
   3e9dc:	adds	x8, x9, x8
   3e9e0:	str	x8, [x19]
   3e9e4:	b.cc	3e9fc <__gmpn_mulmod_bnm1@@Base+0x3ac>  // b.lo, b.ul, b.last
   3e9e8:	add	x8, x19, #0x8
   3e9ec:	ldr	x9, [x8]
   3e9f0:	adds	x9, x9, #0x1
   3e9f4:	str	x9, [x8], #8
   3e9f8:	b.cs	3e9ec <__gmpn_mulmod_bnm1@@Base+0x39c>  // b.hs, b.nlast
   3e9fc:	add	x23, x25, x10
   3ea00:	cmp	x23, x21
   3ea04:	b.lt	3ea68 <__gmpn_mulmod_bnm1@@Base+0x418>  // b.tstop
   3ea08:	ldr	x21, [x20, x22]
   3ea0c:	add	x0, x19, x22
   3ea10:	mov	x1, x19
   3ea14:	mov	x2, x20
   3ea18:	mov	x3, x24
   3ea1c:	bl	c420 <__gmpn_sub_n@plt>
   3ea20:	ldr	x8, [x19]
   3ea24:	add	x9, x0, x21
   3ea28:	subs	x8, x8, x9
   3ea2c:	str	x8, [x19]
   3ea30:	b.cs	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>  // b.hs, b.nlast
   3ea34:	add	x8, x19, #0x8
   3ea38:	ldr	x9, [x8]
   3ea3c:	sub	x10, x9, #0x1
   3ea40:	str	x10, [x8], #8
   3ea44:	cbz	x9, 3ea38 <__gmpn_mulmod_bnm1@@Base+0x3e8>
   3ea48:	ldp	x20, x19, [sp, #112]
   3ea4c:	ldp	x22, x21, [sp, #96]
   3ea50:	ldp	x24, x23, [sp, #80]
   3ea54:	ldp	x26, x25, [sp, #64]
   3ea58:	ldp	x28, x27, [sp, #48]
   3ea5c:	ldp	x29, x30, [sp, #32]
   3ea60:	add	sp, sp, #0x80
   3ea64:	ret
   3ea68:	add	x0, x19, x22
   3ea6c:	sub	x3, x23, x24
   3ea70:	mov	x1, x19
   3ea74:	mov	x2, x20
   3ea78:	mov	x27, x10
   3ea7c:	mov	x26, x25
   3ea80:	neg	x25, x24
   3ea84:	bl	c420 <__gmpn_sub_n@plt>
   3ea88:	lsl	x8, x27, #3
   3ea8c:	lsl	x9, x26, #3
   3ea90:	add	x11, x20, x8
   3ea94:	ldr	x22, [x20, x22]
   3ea98:	lsl	x10, x25, #3
   3ea9c:	add	x8, x19, x8
   3eaa0:	add	x11, x11, x9
   3eaa4:	mov	x4, x0
   3eaa8:	add	x8, x8, x9
   3eaac:	add	x0, x11, x10
   3eab0:	add	x1, x8, x10
   3eab4:	sub	x3, x21, x23
   3eab8:	mov	x2, x0
   3eabc:	bl	c8f0 <__gmpn_sub_nc@plt>
   3eac0:	add	x3, x0, x22
   3eac4:	mov	x0, x19
   3eac8:	mov	x1, x19
   3eacc:	mov	x2, x23
   3ead0:	bl	caf0 <__gmpn_sub_1@plt>
   3ead4:	b	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>
   3ead8:	mov	x2, x27
   3eadc:	mov	x3, x28
   3eae0:	mov	x4, x23
   3eae4:	bl	cea0 <__gmpn_mul@plt>
   3eae8:	sub	x8, x27, x24
   3eaec:	add	x8, x8, x23
   3eaf0:	cmp	x8, x24
   3eaf4:	cset	w9, gt
   3eaf8:	add	x23, x20, x24, lsl #3
   3eafc:	sub	x4, x8, x9
   3eb00:	mov	x0, x20
   3eb04:	mov	x1, x20
   3eb08:	mov	x2, x24
   3eb0c:	mov	x3, x23
   3eb10:	bl	d340 <__gmpn_sub@plt>
   3eb14:	str	xzr, [x23]
   3eb18:	ldr	x8, [x20]
   3eb1c:	adds	x8, x8, x0
   3eb20:	str	x8, [x20]
   3eb24:	b.cc	3e994 <__gmpn_mulmod_bnm1@@Base+0x344>  // b.lo, b.ul, b.last
   3eb28:	add	x8, x20, #0x8
   3eb2c:	ldr	x9, [x8]
   3eb30:	adds	x9, x9, #0x1
   3eb34:	str	x9, [x8], #8
   3eb38:	b.cs	3eb2c <__gmpn_mulmod_bnm1@@Base+0x4dc>  // b.hs, b.nlast
   3eb3c:	b	3e994 <__gmpn_mulmod_bnm1@@Base+0x344>
   3eb40:	mov	x0, x19
   3eb44:	mov	x1, x27
   3eb48:	mov	x2, x22
   3eb4c:	mov	x3, x28
   3eb50:	mov	x4, x25
   3eb54:	bl	cea0 <__gmpn_mul@plt>
   3eb58:	b	3ea48 <__gmpn_mulmod_bnm1@@Base+0x3f8>
   3eb5c:	stp	x29, x30, [sp, #-64]!
   3eb60:	stp	x22, x21, [sp, #32]
   3eb64:	stp	x20, x19, [sp, #48]
   3eb68:	mov	x21, x3
   3eb6c:	mov	x19, x0
   3eb70:	add	x3, x3, #0x1
   3eb74:	mov	x0, x4
   3eb78:	str	x23, [sp, #16]
   3eb7c:	mov	x29, sp
   3eb80:	mov	x20, x4
   3eb84:	bl	cb50 <__gmpn_mul_n@plt>
   3eb88:	lsl	x8, x21, #4
   3eb8c:	ldr	x22, [x20, x8]
   3eb90:	lsl	x23, x21, #3
   3eb94:	add	x2, x20, x23
   3eb98:	mov	x0, x19
   3eb9c:	mov	x1, x20
   3eba0:	mov	x3, x21
   3eba4:	bl	c420 <__gmpn_sub_n@plt>
   3eba8:	str	xzr, [x19, x23]
   3ebac:	ldr	x8, [x19]
   3ebb0:	add	x9, x0, x22
   3ebb4:	adds	x8, x8, x9
   3ebb8:	str	x8, [x19]
   3ebbc:	b.cc	3ebd4 <__gmpn_mulmod_bnm1@@Base+0x584>  // b.lo, b.ul, b.last
   3ebc0:	add	x8, x19, #0x8
   3ebc4:	ldr	x9, [x8]
   3ebc8:	adds	x9, x9, #0x1
   3ebcc:	str	x9, [x8], #8
   3ebd0:	b.cs	3ebc4 <__gmpn_mulmod_bnm1@@Base+0x574>  // b.hs, b.nlast
   3ebd4:	ldp	x20, x19, [sp, #48]
   3ebd8:	ldp	x22, x21, [sp, #32]
   3ebdc:	ldr	x23, [sp, #16]
   3ebe0:	ldp	x29, x30, [sp], #64
   3ebe4:	ret

000000000003ebe8 <__gmpn_mulmod_bnm1_next_size@@Base>:
   3ebe8:	cmp	x0, #0xa
   3ebec:	b.lt	3ec40 <__gmpn_mulmod_bnm1_next_size@@Base+0x58>  // b.tstop
   3ebf0:	cmp	x0, #0x24
   3ebf4:	b.le	3ec44 <__gmpn_mulmod_bnm1_next_size@@Base+0x5c>
   3ebf8:	cmp	x0, #0x48
   3ebfc:	b.le	3ec50 <__gmpn_mulmod_bnm1_next_size@@Base+0x68>
   3ec00:	cmp	x0, #0x276
   3ec04:	b.le	3ec5c <__gmpn_mulmod_bnm1_next_size@@Base+0x74>
   3ec08:	stp	x29, x30, [sp, #-32]!
   3ec0c:	add	x8, x0, #0x1
   3ec10:	str	x19, [sp, #16]
   3ec14:	asr	x19, x8, #1
   3ec18:	mov	x0, x19
   3ec1c:	mov	w1, wzr
   3ec20:	mov	x29, sp
   3ec24:	bl	cc90 <__gmpn_fft_best_k@plt>
   3ec28:	mov	w1, w0
   3ec2c:	mov	x0, x19
   3ec30:	bl	d3b0 <__gmpn_fft_next_size@plt>
   3ec34:	ldr	x19, [sp, #16]
   3ec38:	lsl	x0, x0, #1
   3ec3c:	ldp	x29, x30, [sp], #32
   3ec40:	ret
   3ec44:	add	x8, x0, #0x1
   3ec48:	and	x0, x8, #0xfffffffffffffffe
   3ec4c:	ret
   3ec50:	add	x8, x0, #0x3
   3ec54:	and	x0, x8, #0xfffffffffffffffc
   3ec58:	ret
   3ec5c:	add	x8, x0, #0x7
   3ec60:	and	x0, x8, #0xfffffffffffffff8
   3ec64:	ret

000000000003ec68 <__gmpn_sqrmod_bnm1@@Base>:
   3ec68:	sub	sp, sp, #0x70
   3ec6c:	stp	x28, x27, [sp, #32]
   3ec70:	stp	x24, x23, [sp, #64]
   3ec74:	stp	x22, x21, [sp, #80]
   3ec78:	stp	x20, x19, [sp, #96]
   3ec7c:	mov	x20, x4
   3ec80:	mov	x27, x3
   3ec84:	mov	x24, x2
   3ec88:	mov	x21, x1
   3ec8c:	cmp	x1, #0xb
   3ec90:	mov	x19, x0
   3ec94:	stp	x29, x30, [sp, #16]
   3ec98:	stp	x26, x25, [sp, #48]
   3ec9c:	add	x29, sp, #0x10
   3eca0:	b.lt	3edbc <__gmpn_sqrmod_bnm1@@Base+0x154>  // b.tstop
   3eca4:	tbnz	w21, #0, 3edbc <__gmpn_sqrmod_bnm1@@Base+0x154>
   3eca8:	asr	x23, x21, #1
   3ecac:	cmp	x23, x27
   3ecb0:	lsl	x28, x23, #3
   3ecb4:	b.ge	3eddc <__gmpn_sqrmod_bnm1@@Base+0x174>  // b.tcont
   3ecb8:	add	x26, x24, x28
   3ecbc:	str	x27, [sp, #8]
   3ecc0:	sub	x27, x27, x23
   3ecc4:	mov	x0, x20
   3ecc8:	mov	x1, x24
   3eccc:	mov	x2, x23
   3ecd0:	mov	x3, x26
   3ecd4:	mov	x4, x27
   3ecd8:	add	x25, x20, x28
   3ecdc:	bl	c970 <__gmpn_add@plt>
   3ece0:	ldr	x8, [x20]
   3ece4:	adds	x8, x8, x0
   3ece8:	str	x8, [x20]
   3ecec:	b.cc	3ed04 <__gmpn_sqrmod_bnm1@@Base+0x9c>  // b.lo, b.ul, b.last
   3ecf0:	add	x8, x20, #0x8
   3ecf4:	ldr	x9, [x8]
   3ecf8:	adds	x9, x9, #0x1
   3ecfc:	str	x9, [x8], #8
   3ed00:	b.cs	3ecf4 <__gmpn_sqrmod_bnm1@@Base+0x8c>  // b.hs, b.nlast
   3ed04:	mov	x0, x19
   3ed08:	mov	x1, x23
   3ed0c:	mov	x2, x20
   3ed10:	mov	x3, x23
   3ed14:	mov	x4, x25
   3ed18:	bl	c080 <__gmpn_sqrmod_bnm1@plt>
   3ed1c:	and	x8, x21, #0xfffffffffffffffe
   3ed20:	add	x22, x20, x8, lsl #3
   3ed24:	add	x25, x22, #0x10
   3ed28:	mov	x0, x25
   3ed2c:	mov	x1, x24
   3ed30:	mov	x2, x23
   3ed34:	mov	x3, x26
   3ed38:	mov	x4, x27
   3ed3c:	bl	d340 <__gmpn_sub@plt>
   3ed40:	str	xzr, [x25, x23, lsl #3]
   3ed44:	ldr	x8, [x22, #16]
   3ed48:	adds	x8, x8, x0
   3ed4c:	str	x8, [x22, #16]
   3ed50:	b.cc	3ed68 <__gmpn_sqrmod_bnm1@@Base+0x100>  // b.lo, b.ul, b.last
   3ed54:	add	x8, x22, #0x18
   3ed58:	ldr	x9, [x8]
   3ed5c:	adds	x9, x9, #0x1
   3ed60:	str	x9, [x8], #8
   3ed64:	b.cs	3ed58 <__gmpn_sqrmod_bnm1@@Base+0xf0>  // b.hs, b.nlast
   3ed68:	ldr	x8, [x25, x23, lsl #3]
   3ed6c:	ldr	x27, [sp, #8]
   3ed70:	add	x26, x8, x23
   3ed74:	cmp	x21, #0x278
   3ed78:	b.lt	3ee04 <__gmpn_sqrmod_bnm1@@Base+0x19c>  // b.tstop
   3ed7c:	mov	w1, #0x1                   	// #1
   3ed80:	mov	x0, x23
   3ed84:	bl	cc90 <__gmpn_fft_best_k@plt>
   3ed88:	mov	w8, #0xffffffff            	// #-1
   3ed8c:	lsl	w8, w8, w0
   3ed90:	mvn	w8, w8
   3ed94:	sxtw	x9, w8
   3ed98:	mov	w6, w0
   3ed9c:	tst	x23, x9
   3eda0:	b.eq	3ee08 <__gmpn_sqrmod_bnm1@@Base+0x1a0>  // b.none
   3eda4:	sbfx	x9, x8, #1, #31
   3eda8:	asr	w8, w8, #1
   3edac:	tst	x23, x9
   3edb0:	sub	w6, w6, #0x1
   3edb4:	b.ne	3eda4 <__gmpn_sqrmod_bnm1@@Base+0x13c>  // b.any
   3edb8:	b	3ee08 <__gmpn_sqrmod_bnm1@@Base+0x1a0>
   3edbc:	cmp	x27, x21
   3edc0:	b.lt	3ef18 <__gmpn_sqrmod_bnm1@@Base+0x2b0>  // b.tstop
   3edc4:	mov	x0, x19
   3edc8:	mov	x1, x24
   3edcc:	mov	x2, x21
   3edd0:	mov	x3, x20
   3edd4:	bl	3f034 <__gmpn_sqrmod_bnm1@@Base+0x3cc>
   3edd8:	b	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>
   3eddc:	mov	x0, x19
   3ede0:	mov	x1, x23
   3ede4:	mov	x2, x24
   3ede8:	mov	x3, x27
   3edec:	mov	x4, x20
   3edf0:	bl	c080 <__gmpn_sqrmod_bnm1@plt>
   3edf4:	mov	x25, x24
   3edf8:	mov	x26, x27
   3edfc:	cmp	x21, #0x278
   3ee00:	b.ge	3ed7c <__gmpn_sqrmod_bnm1@@Base+0x114>  // b.tcont
   3ee04:	mov	w6, wzr
   3ee08:	mov	x0, x20
   3ee0c:	cmp	w6, #0x4
   3ee10:	b.lt	3ee34 <__gmpn_sqrmod_bnm1@@Base+0x1cc>  // b.tstop
   3ee14:	mov	x1, x23
   3ee18:	mov	x2, x25
   3ee1c:	mov	x3, x26
   3ee20:	mov	x4, x25
   3ee24:	mov	x5, x26
   3ee28:	bl	c460 <__gmpn_mul_fft@plt>
   3ee2c:	str	x0, [x20, x23, lsl #3]
   3ee30:	b	3ee4c <__gmpn_sqrmod_bnm1@@Base+0x1e4>
   3ee34:	cmp	x25, x24
   3ee38:	b.eq	3efc8 <__gmpn_sqrmod_bnm1@@Base+0x360>  // b.none
   3ee3c:	mov	x1, x25
   3ee40:	mov	x2, x23
   3ee44:	mov	x3, x20
   3ee48:	bl	3f0a0 <__gmpn_sqrmod_bnm1@@Base+0x438>
   3ee4c:	ldr	x22, [x20, x28]
   3ee50:	mov	x0, x19
   3ee54:	mov	x1, x19
   3ee58:	mov	x2, x20
   3ee5c:	mov	x3, x23
   3ee60:	bl	cb10 <__gmpn_rsh1add_n@plt>
   3ee64:	mov	x8, xzr
   3ee68:	add	x9, x28, x19
   3ee6c:	add	x11, x0, x22
   3ee70:	ldur	x10, [x9, #-8]
   3ee74:	lsr	x12, x11, #1
   3ee78:	lsl	x11, x11, #63
   3ee7c:	adds	x13, x10, x11
   3ee80:	adc	x8, x12, x8
   3ee84:	stur	x13, [x9, #-8]
   3ee88:	ldr	x9, [x19]
   3ee8c:	adds	x8, x9, x8
   3ee90:	str	x8, [x19]
   3ee94:	b.cc	3eeac <__gmpn_sqrmod_bnm1@@Base+0x244>  // b.lo, b.ul, b.last
   3ee98:	add	x8, x19, #0x8
   3ee9c:	ldr	x9, [x8]
   3eea0:	adds	x9, x9, #0x1
   3eea4:	str	x9, [x8], #8
   3eea8:	b.cs	3ee9c <__gmpn_sqrmod_bnm1@@Base+0x234>  // b.hs, b.nlast
   3eeac:	lsl	x22, x27, #1
   3eeb0:	cmp	x22, x21
   3eeb4:	b.lt	3ef74 <__gmpn_sqrmod_bnm1@@Base+0x30c>  // b.tstop
   3eeb8:	ldr	x21, [x20, x28]
   3eebc:	add	x0, x19, x28
   3eec0:	mov	x1, x19
   3eec4:	mov	x2, x20
   3eec8:	mov	x3, x23
   3eecc:	bl	c420 <__gmpn_sub_n@plt>
   3eed0:	ldr	x8, [x19]
   3eed4:	add	x9, x0, x21
   3eed8:	subs	x8, x8, x9
   3eedc:	str	x8, [x19]
   3eee0:	b.cs	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>  // b.hs, b.nlast
   3eee4:	add	x8, x19, #0x8
   3eee8:	ldr	x9, [x8]
   3eeec:	sub	x10, x9, #0x1
   3eef0:	str	x10, [x8], #8
   3eef4:	cbz	x9, 3eee8 <__gmpn_sqrmod_bnm1@@Base+0x280>
   3eef8:	ldp	x20, x19, [sp, #96]
   3eefc:	ldp	x22, x21, [sp, #80]
   3ef00:	ldp	x24, x23, [sp, #64]
   3ef04:	ldp	x26, x25, [sp, #48]
   3ef08:	ldp	x28, x27, [sp, #32]
   3ef0c:	ldp	x29, x30, [sp, #16]
   3ef10:	add	sp, sp, #0x70
   3ef14:	ret
   3ef18:	lsl	x8, x27, #1
   3ef1c:	subs	x23, x8, x21
   3ef20:	b.le	3f020 <__gmpn_sqrmod_bnm1@@Base+0x3b8>
   3ef24:	mov	x0, x20
   3ef28:	mov	x1, x24
   3ef2c:	mov	x2, x27
   3ef30:	bl	ca90 <__gmpn_sqr@plt>
   3ef34:	add	x3, x20, x21, lsl #3
   3ef38:	mov	x0, x19
   3ef3c:	mov	x1, x20
   3ef40:	mov	x2, x21
   3ef44:	mov	x4, x23
   3ef48:	bl	c970 <__gmpn_add@plt>
   3ef4c:	ldr	x8, [x19]
   3ef50:	adds	x8, x8, x0
   3ef54:	str	x8, [x19]
   3ef58:	b.cc	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>  // b.lo, b.ul, b.last
   3ef5c:	add	x8, x19, #0x8
   3ef60:	ldr	x9, [x8]
   3ef64:	adds	x9, x9, #0x1
   3ef68:	str	x9, [x8], #8
   3ef6c:	b.cs	3ef60 <__gmpn_sqrmod_bnm1@@Base+0x2f8>  // b.hs, b.nlast
   3ef70:	b	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>
   3ef74:	add	x0, x19, x28
   3ef78:	sub	x3, x22, x23
   3ef7c:	mov	x1, x19
   3ef80:	mov	x2, x20
   3ef84:	bl	c420 <__gmpn_sub_n@plt>
   3ef88:	lsl	x8, x22, #3
   3ef8c:	ldr	x23, [x20, x28]
   3ef90:	add	x9, x20, x8
   3ef94:	mov	x4, x0
   3ef98:	add	x8, x19, x8
   3ef9c:	sub	x0, x9, x28
   3efa0:	sub	x1, x8, x28
   3efa4:	sub	x3, x21, x22
   3efa8:	mov	x2, x0
   3efac:	bl	c8f0 <__gmpn_sub_nc@plt>
   3efb0:	add	x3, x0, x23
   3efb4:	mov	x0, x19
   3efb8:	mov	x1, x19
   3efbc:	mov	x2, x22
   3efc0:	bl	caf0 <__gmpn_sub_1@plt>
   3efc4:	b	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>
   3efc8:	mov	x1, x24
   3efcc:	mov	x2, x27
   3efd0:	bl	ca90 <__gmpn_sqr@plt>
   3efd4:	lsl	x8, x27, #1
   3efd8:	add	x24, x20, x23, lsl #3
   3efdc:	sub	x4, x8, x23
   3efe0:	mov	x0, x20
   3efe4:	mov	x1, x20
   3efe8:	mov	x2, x23
   3efec:	mov	x3, x24
   3eff0:	bl	d340 <__gmpn_sub@plt>
   3eff4:	str	xzr, [x24]
   3eff8:	ldr	x8, [x20]
   3effc:	adds	x8, x8, x0
   3f000:	str	x8, [x20]
   3f004:	b.cc	3ee4c <__gmpn_sqrmod_bnm1@@Base+0x1e4>  // b.lo, b.ul, b.last
   3f008:	add	x8, x20, #0x8
   3f00c:	ldr	x9, [x8]
   3f010:	adds	x9, x9, #0x1
   3f014:	str	x9, [x8], #8
   3f018:	b.cs	3f00c <__gmpn_sqrmod_bnm1@@Base+0x3a4>  // b.hs, b.nlast
   3f01c:	b	3ee4c <__gmpn_sqrmod_bnm1@@Base+0x1e4>
   3f020:	mov	x0, x19
   3f024:	mov	x1, x24
   3f028:	mov	x2, x27
   3f02c:	bl	ca90 <__gmpn_sqr@plt>
   3f030:	b	3eef8 <__gmpn_sqrmod_bnm1@@Base+0x290>
   3f034:	stp	x29, x30, [sp, #-48]!
   3f038:	stp	x20, x19, [sp, #32]
   3f03c:	mov	x19, x0
   3f040:	mov	x0, x3
   3f044:	str	x21, [sp, #16]
   3f048:	mov	x29, sp
   3f04c:	mov	x20, x3
   3f050:	mov	x21, x2
   3f054:	bl	ca90 <__gmpn_sqr@plt>
   3f058:	add	x2, x20, x21, lsl #3
   3f05c:	mov	x0, x19
   3f060:	mov	x1, x20
   3f064:	mov	x3, x21
   3f068:	bl	cc30 <__gmpn_add_n@plt>
   3f06c:	ldr	x8, [x19]
   3f070:	adds	x8, x8, x0
   3f074:	str	x8, [x19]
   3f078:	b.cc	3f090 <__gmpn_sqrmod_bnm1@@Base+0x428>  // b.lo, b.ul, b.last
   3f07c:	add	x8, x19, #0x8
   3f080:	ldr	x9, [x8]
   3f084:	adds	x9, x9, #0x1
   3f088:	str	x9, [x8], #8
   3f08c:	b.cs	3f080 <__gmpn_sqrmod_bnm1@@Base+0x418>  // b.hs, b.nlast
   3f090:	ldp	x20, x19, [sp, #32]
   3f094:	ldr	x21, [sp, #16]
   3f098:	ldp	x29, x30, [sp], #48
   3f09c:	ret
   3f0a0:	stp	x29, x30, [sp, #-64]!
   3f0a4:	stp	x22, x21, [sp, #32]
   3f0a8:	stp	x20, x19, [sp, #48]
   3f0ac:	mov	x21, x2
   3f0b0:	mov	x19, x0
   3f0b4:	add	x2, x2, #0x1
   3f0b8:	mov	x0, x3
   3f0bc:	str	x23, [sp, #16]
   3f0c0:	mov	x29, sp
   3f0c4:	mov	x20, x3
   3f0c8:	bl	ca90 <__gmpn_sqr@plt>
   3f0cc:	lsl	x8, x21, #4
   3f0d0:	ldr	x22, [x20, x8]
   3f0d4:	lsl	x23, x21, #3
   3f0d8:	add	x2, x20, x23
   3f0dc:	mov	x0, x19
   3f0e0:	mov	x1, x20
   3f0e4:	mov	x3, x21
   3f0e8:	bl	c420 <__gmpn_sub_n@plt>
   3f0ec:	str	xzr, [x19, x23]
   3f0f0:	ldr	x8, [x19]
   3f0f4:	add	x9, x0, x22
   3f0f8:	adds	x8, x8, x9
   3f0fc:	str	x8, [x19]
   3f100:	b.cc	3f118 <__gmpn_sqrmod_bnm1@@Base+0x4b0>  // b.lo, b.ul, b.last
   3f104:	add	x8, x19, #0x8
   3f108:	ldr	x9, [x8]
   3f10c:	adds	x9, x9, #0x1
   3f110:	str	x9, [x8], #8
   3f114:	b.cs	3f108 <__gmpn_sqrmod_bnm1@@Base+0x4a0>  // b.hs, b.nlast
   3f118:	ldp	x20, x19, [sp, #48]
   3f11c:	ldp	x22, x21, [sp, #32]
   3f120:	ldr	x23, [sp, #16]
   3f124:	ldp	x29, x30, [sp], #64
   3f128:	ret

000000000003f12c <__gmpn_sqrmod_bnm1_next_size@@Base>:
   3f12c:	cmp	x0, #0xb
   3f130:	b.lt	3f184 <__gmpn_sqrmod_bnm1_next_size@@Base+0x58>  // b.tstop
   3f134:	cmp	x0, #0x28
   3f138:	b.le	3f188 <__gmpn_sqrmod_bnm1_next_size@@Base+0x5c>
   3f13c:	cmp	x0, #0x50
   3f140:	b.le	3f194 <__gmpn_sqrmod_bnm1_next_size@@Base+0x68>
   3f144:	cmp	x0, #0x21e
   3f148:	b.le	3f1a0 <__gmpn_sqrmod_bnm1_next_size@@Base+0x74>
   3f14c:	stp	x29, x30, [sp, #-32]!
   3f150:	add	x8, x0, #0x1
   3f154:	str	x19, [sp, #16]
   3f158:	asr	x19, x8, #1
   3f15c:	mov	w1, #0x1                   	// #1
   3f160:	mov	x0, x19
   3f164:	mov	x29, sp
   3f168:	bl	cc90 <__gmpn_fft_best_k@plt>
   3f16c:	mov	w1, w0
   3f170:	mov	x0, x19
   3f174:	bl	d3b0 <__gmpn_fft_next_size@plt>
   3f178:	ldr	x19, [sp, #16]
   3f17c:	lsl	x0, x0, #1
   3f180:	ldp	x29, x30, [sp], #32
   3f184:	ret
   3f188:	add	x8, x0, #0x1
   3f18c:	and	x0, x8, #0xfffffffffffffffe
   3f190:	ret
   3f194:	add	x8, x0, #0x3
   3f198:	and	x0, x8, #0xfffffffffffffffc
   3f19c:	ret
   3f1a0:	add	x8, x0, #0x7
   3f1a4:	and	x0, x8, #0xfffffffffffffff8
   3f1a8:	ret

000000000003f1ac <__gmpn_div_qr_1@@Base>:
   3f1ac:	stp	x29, x30, [sp, #-80]!
   3f1b0:	stp	x24, x23, [sp, #32]
   3f1b4:	stp	x22, x21, [sp, #48]
   3f1b8:	stp	x20, x19, [sp, #64]
   3f1bc:	mov	x19, x4
   3f1c0:	mov	x22, x2
   3f1c4:	mov	x24, x1
   3f1c8:	mov	x20, x0
   3f1cc:	stp	x26, x25, [sp, #16]
   3f1d0:	mov	x29, sp
   3f1d4:	tbnz	x4, #63, 3f250 <__gmpn_div_qr_1@@Base+0xa4>
   3f1d8:	sub	x23, x3, #0x1
   3f1dc:	ldr	x25, [x22, x23, lsl #3]
   3f1e0:	clz	x21, x19
   3f1e4:	mov	x0, x20
   3f1e8:	mov	x1, x22
   3f1ec:	mov	x2, x23
   3f1f0:	mov	w3, w21
   3f1f4:	lsl	x19, x19, x21
   3f1f8:	lsl	x26, x25, x21
   3f1fc:	bl	c2d0 <__gmpn_lshift@plt>
   3f200:	neg	w9, w21
   3f204:	lsr	x10, x19, #32
   3f208:	lsr	x9, x25, x9
   3f20c:	udiv	x14, x9, x10
   3f210:	orr	x8, x26, x0
   3f214:	and	x11, x19, #0xffffffff
   3f218:	msub	w9, w14, w10, w9
   3f21c:	mul	x12, x14, x11
   3f220:	extr	x13, x9, x8, #32
   3f224:	cmp	x13, x12
   3f228:	b.cs	3f29c <__gmpn_div_qr_1@@Base+0xf0>  // b.hs, b.nlast
   3f22c:	add	x13, x13, x19
   3f230:	cmp	x13, x19
   3f234:	sub	x9, x14, #0x1
   3f238:	b.cc	3f2a0 <__gmpn_div_qr_1@@Base+0xf4>  // b.lo, b.ul, b.last
   3f23c:	cmp	x13, x12
   3f240:	b.cs	3f2a0 <__gmpn_div_qr_1@@Base+0xf4>  // b.hs, b.nlast
   3f244:	sub	x9, x14, #0x2
   3f248:	add	x13, x13, x19
   3f24c:	b	3f2a0 <__gmpn_div_qr_1@@Base+0xf4>
   3f250:	sub	x23, x3, #0x1
   3f254:	ldr	x8, [x22, x23, lsl #3]
   3f258:	cmp	x8, x19
   3f25c:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   3f260:	cset	w9, cs  // cs = hs, nlast
   3f264:	cmp	x3, #0xd
   3f268:	sub	x25, x8, x10
   3f26c:	str	x9, [x24]
   3f270:	b.le	3f3ac <__gmpn_div_qr_1@@Base+0x200>
   3f274:	mov	x0, x19
   3f278:	bl	d5d0 <__gmpn_invert_limb@plt>
   3f27c:	mov	x5, x0
   3f280:	mov	x0, x20
   3f284:	mov	x1, x22
   3f288:	mov	x2, x23
   3f28c:	mov	x3, x25
   3f290:	mov	x4, x19
   3f294:	bl	c5e0 <__gmpn_div_qr_1n_pi1@plt>
   3f298:	b	3f3bc <__gmpn_div_qr_1@@Base+0x210>
   3f29c:	mov	x9, x14
   3f2a0:	sub	x13, x13, x12
   3f2a4:	udiv	x12, x13, x10
   3f2a8:	msub	w13, w12, w10, w13
   3f2ac:	mul	x10, x12, x11
   3f2b0:	bfi	x8, x13, #32, #32
   3f2b4:	cmp	x8, x10
   3f2b8:	b.cs	3f2e0 <__gmpn_div_qr_1@@Base+0x134>  // b.hs, b.nlast
   3f2bc:	add	x8, x8, x19
   3f2c0:	cmp	x8, x19
   3f2c4:	sub	x11, x12, #0x1
   3f2c8:	b.cc	3f2e4 <__gmpn_div_qr_1@@Base+0x138>  // b.lo, b.ul, b.last
   3f2cc:	cmp	x8, x10
   3f2d0:	b.cs	3f2e4 <__gmpn_div_qr_1@@Base+0x138>  // b.hs, b.nlast
   3f2d4:	sub	x11, x12, #0x2
   3f2d8:	add	x8, x8, x19
   3f2dc:	b	3f2e4 <__gmpn_div_qr_1@@Base+0x138>
   3f2e0:	mov	x11, x12
   3f2e4:	sub	x25, x8, x10
   3f2e8:	orr	x8, x11, x9, lsl #32
   3f2ec:	str	x8, [x24]
   3f2f0:	mov	x22, x20
   3f2f4:	subs	x8, x23, #0x1
   3f2f8:	b.lt	3f3b8 <__gmpn_div_qr_1@@Base+0x20c>  // b.tstop
   3f2fc:	lsr	x9, x19, #32
   3f300:	and	x10, x19, #0xffffffff
   3f304:	b	3f328 <__gmpn_div_qr_1@@Base+0x17c>
   3f308:	mov	x15, x14
   3f30c:	orr	x12, x15, x12, lsl #32
   3f310:	add	x14, x8, #0x1
   3f314:	str	x12, [x20, x8, lsl #3]
   3f318:	sub	x8, x8, #0x1
   3f31c:	cmp	x14, #0x1
   3f320:	sub	x25, x11, x13
   3f324:	b.le	3f3b8 <__gmpn_div_qr_1@@Base+0x20c>
   3f328:	ldr	x11, [x22, x8, lsl #3]
   3f32c:	udiv	x15, x25, x9
   3f330:	msub	w12, w15, w9, w25
   3f334:	mul	x13, x15, x10
   3f338:	extr	x14, x12, x11, #32
   3f33c:	cmp	x14, x13
   3f340:	b.cs	3f368 <__gmpn_div_qr_1@@Base+0x1bc>  // b.hs, b.nlast
   3f344:	add	x14, x14, x19
   3f348:	cmp	x14, x19
   3f34c:	sub	x12, x15, #0x1
   3f350:	b.cc	3f36c <__gmpn_div_qr_1@@Base+0x1c0>  // b.lo, b.ul, b.last
   3f354:	cmp	x14, x13
   3f358:	b.cs	3f36c <__gmpn_div_qr_1@@Base+0x1c0>  // b.hs, b.nlast
   3f35c:	sub	x12, x15, #0x2
   3f360:	add	x14, x14, x19
   3f364:	b	3f36c <__gmpn_div_qr_1@@Base+0x1c0>
   3f368:	mov	x12, x15
   3f36c:	sub	x13, x14, x13
   3f370:	udiv	x14, x13, x9
   3f374:	msub	w15, w14, w9, w13
   3f378:	mul	x13, x14, x10
   3f37c:	bfi	x11, x15, #32, #32
   3f380:	cmp	x11, x13
   3f384:	b.cs	3f308 <__gmpn_div_qr_1@@Base+0x15c>  // b.hs, b.nlast
   3f388:	add	x11, x11, x19
   3f38c:	cmp	x11, x19
   3f390:	sub	x15, x14, #0x1
   3f394:	b.cc	3f30c <__gmpn_div_qr_1@@Base+0x160>  // b.lo, b.ul, b.last
   3f398:	cmp	x11, x13
   3f39c:	b.cs	3f30c <__gmpn_div_qr_1@@Base+0x160>  // b.hs, b.nlast
   3f3a0:	sub	x15, x14, #0x2
   3f3a4:	add	x11, x11, x19
   3f3a8:	b	3f30c <__gmpn_div_qr_1@@Base+0x160>
   3f3ac:	mov	x21, xzr
   3f3b0:	subs	x8, x23, #0x1
   3f3b4:	b.ge	3f2fc <__gmpn_div_qr_1@@Base+0x150>  // b.tcont
   3f3b8:	lsr	x0, x25, x21
   3f3bc:	ldp	x20, x19, [sp, #64]
   3f3c0:	ldp	x22, x21, [sp, #48]
   3f3c4:	ldp	x24, x23, [sp, #32]
   3f3c8:	ldp	x26, x25, [sp, #16]
   3f3cc:	ldp	x29, x30, [sp], #80
   3f3d0:	ret

000000000003f3d4 <__gmpn_div_qr_1n_pi1@@Base>:
   3f3d4:	str	x19, [sp, #-16]!
   3f3d8:	cmp	x2, #0x1
   3f3dc:	b.ne	3f424 <__gmpn_div_qr_1n_pi1@@Base+0x50>  // b.any
   3f3e0:	ldr	x8, [x1]
   3f3e4:	mul	x10, x5, x3
   3f3e8:	umulh	x9, x3, x5
   3f3ec:	add	x11, x3, #0x1
   3f3f0:	adds	x12, x10, x8
   3f3f4:	adc	x10, x9, x11
   3f3f8:	msub	x8, x10, x4, x8
   3f3fc:	cmp	x8, x12
   3f400:	csel	x9, x4, xzr, hi  // hi = pmore
   3f404:	cset	w11, hi  // hi = pmore
   3f408:	add	x8, x9, x8
   3f40c:	subs	x9, x8, x4
   3f410:	sub	x10, x10, x11
   3f414:	b.cc	3f5e0 <__gmpn_div_qr_1n_pi1@@Base+0x20c>  // b.lo, b.ul, b.last
   3f418:	add	x10, x10, #0x1
   3f41c:	mov	x8, x9
   3f420:	b	3f5e0 <__gmpn_div_qr_1n_pi1@@Base+0x20c>
   3f424:	umulh	x10, x5, x3
   3f428:	lsl	x11, x2, #3
   3f42c:	add	x10, x10, x3
   3f430:	sub	x11, x11, #0x8
   3f434:	sub	x15, x2, #0x2
   3f438:	ldr	x12, [x1, x11]
   3f43c:	str	x10, [x0, x11]
   3f440:	ldr	x10, [x1, x15, lsl #3]
   3f444:	mul	x9, x4, x5
   3f448:	mneg	x9, x9, x3
   3f44c:	mneg	x8, x4, x5
   3f450:	adds	x10, x10, x9
   3f454:	mov	w11, #0x2                   	// #2
   3f458:	umulh	x13, x8, x3
   3f45c:	cset	w9, cs  // cs = hs, nlast
   3f460:	adds	x12, x12, x13
   3f464:	cset	w13, cs  // cs = hs, nlast
   3f468:	csinc	x14, x11, xzr, cs  // cs = hs, nlast
   3f46c:	adds	x11, x12, x9
   3f470:	csel	x16, x13, x14, cc  // cc = lo, ul, last
   3f474:	subs	x17, x2, #0x3
   3f478:	mul	x18, x5, x3
   3f47c:	b.lt	3f548 <__gmpn_div_qr_1n_pi1@@Base+0x174>  // b.tstop
   3f480:	add	x12, x0, #0x10
   3f484:	add	x13, x0, x2, lsl #3
   3f488:	mov	w14, #0x2                   	// #2
   3f48c:	b	3f4c4 <__gmpn_div_qr_1n_pi1@@Base+0xf0>
   3f490:	ldr	x18, [x1, x15, lsl #3]
   3f494:	sub	x17, x15, #0x1
   3f498:	sub	x13, x13, #0x8
   3f49c:	adds	x10, x18, x10
   3f4a0:	cset	w18, cs  // cs = hs, nlast
   3f4a4:	adds	x11, x16, x11
   3f4a8:	cset	w16, cs  // cs = hs, nlast
   3f4ac:	csinc	x2, x14, xzr, cs  // cs = hs, nlast
   3f4b0:	adds	x11, x11, x18
   3f4b4:	csel	x16, x16, x2, cc  // cc = lo, ul, last
   3f4b8:	cmp	x15, #0x0
   3f4bc:	mov	x18, x9
   3f4c0:	b.le	3f54c <__gmpn_div_qr_1n_pi1@@Base+0x178>
   3f4c4:	neg	x3, x16
   3f4c8:	mov	x2, xzr
   3f4cc:	and	x7, x3, x5
   3f4d0:	adds	x19, x7, x11
   3f4d4:	adc	x16, x16, x2
   3f4d8:	umulh	x6, x11, x5
   3f4dc:	adds	x7, x19, x6
   3f4e0:	adc	x16, x16, x2
   3f4e4:	and	x3, x3, x8
   3f4e8:	adds	x6, x7, x18
   3f4ec:	adc	x16, x16, x2
   3f4f0:	adds	x18, x10, x3
   3f4f4:	cset	w10, cs  // cs = hs, nlast
   3f4f8:	csel	x3, x4, xzr, cs  // cs = hs, nlast
   3f4fc:	adds	x7, x6, x10
   3f500:	adc	x2, x16, x2
   3f504:	str	x7, [x0, x15, lsl #3]
   3f508:	mov	x15, x17
   3f50c:	lsl	x17, x17, #3
   3f510:	ldr	x6, [x12, x17]
   3f514:	mul	x9, x11, x5
   3f518:	umulh	x16, x11, x8
   3f51c:	mul	x10, x11, x8
   3f520:	sub	x11, x18, x3
   3f524:	adds	x18, x6, x2
   3f528:	str	x18, [x12, x17]
   3f52c:	b.cc	3f490 <__gmpn_div_qr_1n_pi1@@Base+0xbc>  // b.lo, b.ul, b.last
   3f530:	mov	x17, x13
   3f534:	ldr	x18, [x17]
   3f538:	adds	x18, x18, #0x1
   3f53c:	str	x18, [x17], #8
   3f540:	b.cs	3f534 <__gmpn_div_qr_1n_pi1@@Base+0x160>  // b.hs, b.nlast
   3f544:	b	3f490 <__gmpn_div_qr_1n_pi1@@Base+0xbc>
   3f548:	mov	x9, x18
   3f54c:	mov	x12, xzr
   3f550:	cmp	x16, #0x0
   3f554:	csel	x14, x4, x12, ne  // ne = any
   3f558:	mov	w8, #0x2                   	// #2
   3f55c:	sub	x11, x11, x14
   3f560:	cset	w14, ne  // ne = any
   3f564:	csinc	x8, x8, xzr, ne  // ne = any
   3f568:	cmp	x11, x4
   3f56c:	csel	x14, x14, x8, cc  // cc = lo, ul, last
   3f570:	csel	x8, x4, x12, cs  // cs = hs, nlast
   3f574:	sub	x8, x11, x8
   3f578:	umulh	x11, x8, x5
   3f57c:	mul	x15, x8, x5
   3f580:	add	x8, x8, #0x1
   3f584:	adds	x16, x15, x10
   3f588:	adc	x8, x11, x8
   3f58c:	msub	x10, x8, x4, x10
   3f590:	cmp	x10, x16
   3f594:	csel	x15, x4, x12, hi  // hi = pmore
   3f598:	ldr	x13, [x0, #8]
   3f59c:	cset	w11, hi  // hi = pmore
   3f5a0:	add	x10, x15, x10
   3f5a4:	sub	x8, x8, x11
   3f5a8:	cmp	x10, x4
   3f5ac:	cinc	x11, x8, cs  // cs = hs, nlast
   3f5b0:	csel	x8, x12, x4, cc  // cc = lo, ul, last
   3f5b4:	sub	x8, x10, x8
   3f5b8:	adds	x10, x9, x11
   3f5bc:	adc	x9, x14, x12
   3f5c0:	adds	x9, x13, x9
   3f5c4:	str	x9, [x0, #8]
   3f5c8:	b.cc	3f5e0 <__gmpn_div_qr_1n_pi1@@Base+0x20c>  // b.lo, b.ul, b.last
   3f5cc:	add	x9, x0, #0x10
   3f5d0:	ldr	x11, [x9]
   3f5d4:	adds	x11, x11, #0x1
   3f5d8:	str	x11, [x9], #8
   3f5dc:	b.cs	3f5d0 <__gmpn_div_qr_1n_pi1@@Base+0x1fc>  // b.hs, b.nlast
   3f5e0:	str	x10, [x0]
   3f5e4:	mov	x0, x8
   3f5e8:	ldr	x19, [sp], #16
   3f5ec:	ret

000000000003f5f0 <__gmpn_div_qr_2@@Base>:
   3f5f0:	stp	x29, x30, [sp, #-80]!
   3f5f4:	str	x25, [sp, #16]
   3f5f8:	stp	x24, x23, [sp, #32]
   3f5fc:	stp	x22, x21, [sp, #48]
   3f600:	stp	x20, x19, [sp, #64]
   3f604:	ldp	x23, x25, [x4]
   3f608:	mov	x19, x3
   3f60c:	mov	x20, x2
   3f610:	mov	x21, x1
   3f614:	mov	x22, x0
   3f618:	mov	x29, sp
   3f61c:	tbnz	x25, #63, 3f6d0 <__gmpn_div_qr_2@@Base+0xe0>
   3f620:	clz	x24, x25
   3f624:	neg	w9, w24
   3f628:	lsl	x8, x25, x24
   3f62c:	lsr	x9, x23, x9
   3f630:	orr	x25, x9, x8
   3f634:	mov	x0, x25
   3f638:	lsl	x23, x23, x24
   3f63c:	bl	d5d0 <__gmpn_invert_limb@plt>
   3f640:	mul	x8, x0, x25
   3f644:	adds	x8, x8, x23
   3f648:	b.cc	3f664 <__gmpn_div_qr_2@@Base+0x74>  // b.lo, b.ul, b.last
   3f64c:	subs	x8, x8, x25
   3f650:	cset	w9, cs  // cs = hs, nlast
   3f654:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   3f658:	mvn	x9, x9
   3f65c:	add	x0, x9, x0
   3f660:	sub	x8, x8, x10
   3f664:	umulh	x9, x23, x0
   3f668:	adds	x8, x9, x8
   3f66c:	b.cc	3f694 <__gmpn_div_qr_2@@Base+0xa4>  // b.lo, b.ul, b.last
   3f670:	cmp	x8, x25
   3f674:	sub	x7, x0, #0x1
   3f678:	b.cc	3f698 <__gmpn_div_qr_2@@Base+0xa8>  // b.lo, b.ul, b.last
   3f67c:	mul	x9, x0, x23
   3f680:	cmp	x8, x25
   3f684:	sub	x10, x0, #0x2
   3f688:	ccmp	x9, x23, #0x2, ls  // ls = plast
   3f68c:	csel	x7, x7, x10, cc  // cc = lo, ul, last
   3f690:	b	3f698 <__gmpn_div_qr_2@@Base+0xa8>
   3f694:	mov	x7, x0
   3f698:	mov	x0, x22
   3f69c:	mov	x1, x21
   3f6a0:	mov	x2, x20
   3f6a4:	mov	x3, x19
   3f6a8:	mov	x4, x25
   3f6ac:	mov	x5, x23
   3f6b0:	mov	w6, w24
   3f6b4:	bl	d0f0 <__gmpn_div_qr_2u_pi1@plt>
   3f6b8:	ldp	x20, x19, [sp, #64]
   3f6bc:	ldp	x22, x21, [sp, #48]
   3f6c0:	ldp	x24, x23, [sp, #32]
   3f6c4:	ldr	x25, [sp, #16]
   3f6c8:	ldp	x29, x30, [sp], #80
   3f6cc:	ret
   3f6d0:	mov	x0, x25
   3f6d4:	bl	d5d0 <__gmpn_invert_limb@plt>
   3f6d8:	mul	x8, x0, x25
   3f6dc:	adds	x8, x8, x23
   3f6e0:	b.cc	3f6fc <__gmpn_div_qr_2@@Base+0x10c>  // b.lo, b.ul, b.last
   3f6e4:	subs	x8, x8, x25
   3f6e8:	cset	w9, cs  // cs = hs, nlast
   3f6ec:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   3f6f0:	mvn	x9, x9
   3f6f4:	add	x0, x9, x0
   3f6f8:	sub	x8, x8, x10
   3f6fc:	umulh	x9, x23, x0
   3f700:	adds	x8, x9, x8
   3f704:	b.cc	3f72c <__gmpn_div_qr_2@@Base+0x13c>  // b.lo, b.ul, b.last
   3f708:	cmp	x8, x25
   3f70c:	sub	x6, x0, #0x1
   3f710:	b.cc	3f730 <__gmpn_div_qr_2@@Base+0x140>  // b.lo, b.ul, b.last
   3f714:	mul	x9, x0, x23
   3f718:	cmp	x8, x25
   3f71c:	sub	x10, x0, #0x2
   3f720:	ccmp	x9, x23, #0x2, ls  // ls = plast
   3f724:	csel	x6, x6, x10, cc  // cc = lo, ul, last
   3f728:	b	3f730 <__gmpn_div_qr_2@@Base+0x140>
   3f72c:	mov	x6, x0
   3f730:	mov	x0, x22
   3f734:	mov	x1, x21
   3f738:	mov	x2, x20
   3f73c:	mov	x3, x19
   3f740:	mov	x4, x25
   3f744:	mov	x5, x23
   3f748:	bl	cad0 <__gmpn_div_qr_2n_pi1@plt>
   3f74c:	b	3f6b8 <__gmpn_div_qr_2@@Base+0xc8>

000000000003f750 <__gmpn_div_qr_2n_pi1@@Base>:
   3f750:	add	x8, x2, x3, lsl #3
   3f754:	ldp	x10, x9, [x8, #-16]
   3f758:	cmp	x9, x4
   3f75c:	b.cc	3f76c <__gmpn_div_qr_2n_pi1@@Base+0x1c>  // b.lo, b.ul, b.last
   3f760:	b.hi	3f77c <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.pmore
   3f764:	cmp	x10, x5
   3f768:	b.cs	3f77c <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.hs, b.nlast
   3f76c:	mov	x8, xzr
   3f770:	subs	x11, x3, #0x3
   3f774:	b.ge	3f7bc <__gmpn_div_qr_2n_pi1@@Base+0x6c>  // b.tcont
   3f778:	b	3f82c <__gmpn_div_qr_2n_pi1@@Base+0xdc>
   3f77c:	subs	x11, x10, x5
   3f780:	sbc	x9, x9, x4
   3f784:	mov	w8, #0x1                   	// #1
   3f788:	mov	x10, x11
   3f78c:	subs	x11, x3, #0x3
   3f790:	b.lt	3f82c <__gmpn_div_qr_2n_pi1@@Base+0xdc>  // b.tstop
   3f794:	b	3f7bc <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   3f798:	cmp	x10, x5
   3f79c:	b.cs	3f7a8 <__gmpn_div_qr_2n_pi1@@Base+0x58>  // b.hs, b.nlast
   3f7a0:	cmp	x9, x4
   3f7a4:	b.ls	3f818 <__gmpn_div_qr_2n_pi1@@Base+0xc8>  // b.plast
   3f7a8:	subs	x13, x10, x5
   3f7ac:	sbc	x9, x9, x4
   3f7b0:	add	x12, x12, #0x1
   3f7b4:	mov	x10, x13
   3f7b8:	b	3f818 <__gmpn_div_qr_2n_pi1@@Base+0xc8>
   3f7bc:	mul	x13, x9, x6
   3f7c0:	ldr	x12, [x2, x11, lsl #3]
   3f7c4:	umulh	x14, x9, x6
   3f7c8:	adds	x15, x13, x10
   3f7cc:	adc	x9, x14, x9
   3f7d0:	msub	x10, x9, x4, x10
   3f7d4:	mul	x13, x9, x5
   3f7d8:	umulh	x14, x5, x9
   3f7dc:	subs	x16, x12, x5
   3f7e0:	sbc	x10, x10, x4
   3f7e4:	subs	x12, x16, x13
   3f7e8:	sbc	x13, x10, x14
   3f7ec:	cmp	x13, x15
   3f7f0:	cset	w10, cs  // cs = hs, nlast
   3f7f4:	csetm	x14, cs  // cs = hs, nlast
   3f7f8:	sub	x15, x9, x10
   3f7fc:	and	x9, x14, x5
   3f800:	and	x14, x14, x4
   3f804:	adds	x10, x12, x9
   3f808:	adc	x9, x13, x14
   3f80c:	cmp	x9, x4
   3f810:	add	x12, x15, #0x1
   3f814:	b.cs	3f798 <__gmpn_div_qr_2n_pi1@@Base+0x48>  // b.hs, b.nlast
   3f818:	sub	x13, x11, #0x1
   3f81c:	cmp	x11, #0x0
   3f820:	str	x12, [x0, x11, lsl #3]
   3f824:	mov	x11, x13
   3f828:	b.gt	3f7bc <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   3f82c:	mov	x0, x8
   3f830:	stp	x10, x9, [x1]
   3f834:	ret

000000000003f838 <__gmpn_div_qr_2u_pi1@@Base>:
   3f838:	add	x8, x2, x3, lsl #3
   3f83c:	ldp	x8, x12, [x8, #-16]
   3f840:	neg	w11, w6
   3f844:	mov	w10, #0x40                  	// #64
   3f848:	mov	w9, w6
   3f84c:	lsr	x13, x12, x11
   3f850:	lsl	x12, x12, x6
   3f854:	lsr	x11, x8, x11
   3f858:	lsl	x8, x8, x6
   3f85c:	orr	x11, x11, x12
   3f860:	mul	x12, x13, x7
   3f864:	umulh	x14, x13, x7
   3f868:	adds	x15, x12, x11
   3f86c:	adc	x12, x14, x13
   3f870:	msub	x11, x12, x4, x11
   3f874:	subs	x16, x8, x5
   3f878:	sbc	x8, x11, x4
   3f87c:	mul	x13, x12, x5
   3f880:	umulh	x14, x5, x12
   3f884:	subs	x11, x16, x13
   3f888:	sbc	x8, x8, x14
   3f88c:	cmp	x8, x15
   3f890:	cset	w13, cs  // cs = hs, nlast
   3f894:	csetm	x14, cs  // cs = hs, nlast
   3f898:	sub	x13, x12, x13
   3f89c:	and	x15, x14, x5
   3f8a0:	and	x14, x14, x4
   3f8a4:	adds	x12, x11, x15
   3f8a8:	adc	x11, x8, x14
   3f8ac:	sub	w10, w10, w6
   3f8b0:	cmp	x11, x4
   3f8b4:	add	x8, x13, #0x1
   3f8b8:	b.cs	3f984 <__gmpn_div_qr_2u_pi1@@Base+0x14c>  // b.hs, b.nlast
   3f8bc:	subs	x13, x3, #0x3
   3f8c0:	b.lt	3f968 <__gmpn_div_qr_2u_pi1@@Base+0x130>  // b.tstop
   3f8c4:	ldr	x14, [x2, x13, lsl #3]
   3f8c8:	mul	x15, x11, x7
   3f8cc:	umulh	x16, x11, x7
   3f8d0:	lsr	x17, x14, x10
   3f8d4:	orr	x12, x17, x12
   3f8d8:	lsl	x14, x14, x9
   3f8dc:	adds	x17, x15, x12
   3f8e0:	adc	x11, x16, x11
   3f8e4:	msub	x12, x11, x4, x12
   3f8e8:	mul	x15, x11, x5
   3f8ec:	umulh	x16, x5, x11
   3f8f0:	subs	x18, x14, x5
   3f8f4:	sbc	x12, x12, x4
   3f8f8:	subs	x14, x18, x15
   3f8fc:	sbc	x15, x12, x16
   3f900:	cmp	x15, x17
   3f904:	cset	w12, cs  // cs = hs, nlast
   3f908:	csetm	x16, cs  // cs = hs, nlast
   3f90c:	sub	x17, x11, x12
   3f910:	and	x11, x16, x5
   3f914:	and	x16, x16, x4
   3f918:	adds	x12, x14, x11
   3f91c:	adc	x11, x15, x16
   3f920:	cmp	x11, x4
   3f924:	add	x14, x17, #0x1
   3f928:	b.cs	3f944 <__gmpn_div_qr_2u_pi1@@Base+0x10c>  // b.hs, b.nlast
   3f92c:	sub	x15, x13, #0x1
   3f930:	cmp	x13, #0x0
   3f934:	str	x14, [x0, x13, lsl #3]
   3f938:	mov	x13, x15
   3f93c:	b.gt	3f8c4 <__gmpn_div_qr_2u_pi1@@Base+0x8c>
   3f940:	b	3f968 <__gmpn_div_qr_2u_pi1@@Base+0x130>
   3f944:	cmp	x12, x5
   3f948:	b.cs	3f954 <__gmpn_div_qr_2u_pi1@@Base+0x11c>  // b.hs, b.nlast
   3f94c:	cmp	x11, x4
   3f950:	b.ls	3f92c <__gmpn_div_qr_2u_pi1@@Base+0xf4>  // b.plast
   3f954:	subs	x15, x12, x5
   3f958:	sbc	x11, x11, x4
   3f95c:	add	x14, x14, #0x1
   3f960:	mov	x12, x15
   3f964:	b	3f92c <__gmpn_div_qr_2u_pi1@@Base+0xf4>
   3f968:	lsr	x12, x12, x9
   3f96c:	lsl	x10, x11, x10
   3f970:	lsr	x9, x11, x9
   3f974:	orr	x10, x10, x12
   3f978:	mov	x0, x8
   3f97c:	stp	x10, x9, [x1]
   3f980:	ret
   3f984:	cmp	x12, x5
   3f988:	b.cs	3f994 <__gmpn_div_qr_2u_pi1@@Base+0x15c>  // b.hs, b.nlast
   3f98c:	cmp	x11, x4
   3f990:	b.ls	3f8bc <__gmpn_div_qr_2u_pi1@@Base+0x84>  // b.plast
   3f994:	subs	x13, x12, x5
   3f998:	sbc	x11, x11, x4
   3f99c:	add	x8, x8, #0x1
   3f9a0:	mov	x12, x13
   3f9a4:	b	3f8bc <__gmpn_div_qr_2u_pi1@@Base+0x84>

000000000003f9a8 <__gmpn_sbpi1_div_q@@Base>:
   3f9a8:	sub	sp, sp, #0xe0
   3f9ac:	stp	x28, x27, [sp, #144]
   3f9b0:	sub	x27, x2, x4
   3f9b4:	add	x8, x27, #0x1
   3f9b8:	stp	x3, x8, [sp, #16]
   3f9bc:	subs	x8, x4, x8
   3f9c0:	stp	x26, x25, [sp, #160]
   3f9c4:	stp	x22, x21, [sp, #192]
   3f9c8:	add	x21, x1, x2, lsl #3
   3f9cc:	str	x8, [sp, #8]
   3f9d0:	add	x8, x3, x8, lsl #3
   3f9d4:	csinc	x26, x4, x27, le
   3f9d8:	stp	x20, x19, [sp, #208]
   3f9dc:	csel	x22, x8, x3, gt
   3f9e0:	sub	x20, x21, x26, lsl #3
   3f9e4:	stp	x29, x30, [sp, #128]
   3f9e8:	add	x29, sp, #0x80
   3f9ec:	mov	x19, x0
   3f9f0:	stp	x2, x1, [sp, #32]
   3f9f4:	mov	x0, x20
   3f9f8:	mov	x1, x22
   3f9fc:	mov	x2, x26
   3fa00:	stp	x24, x23, [sp, #176]
   3fa04:	stur	x5, [x29, #-24]
   3fa08:	mov	x23, x4
   3fa0c:	bl	c570 <__gmpn_cmp@plt>
   3fa10:	mov	w28, w0
   3fa14:	tbnz	w0, #31, 3fa2c <__gmpn_sbpi1_div_q@@Base+0x84>
   3fa18:	mov	x0, x20
   3fa1c:	mov	x1, x20
   3fa20:	mov	x2, x22
   3fa24:	mov	x3, x26
   3fa28:	bl	c420 <__gmpn_sub_n@plt>
   3fa2c:	sub	x8, x26, #0x2
   3fa30:	sub	x9, x26, #0x1
   3fa34:	ldr	x15, [x22, x9, lsl #3]
   3fa38:	stp	x8, x22, [x29, #-40]
   3fa3c:	ldr	x8, [x22, x8, lsl #3]
   3fa40:	str	x23, [sp, #64]
   3fa44:	stur	x26, [x29, #-56]
   3fa48:	stp	x8, x15, [x29, #-16]
   3fa4c:	ldur	x22, [x21, #-8]
   3fa50:	subs	x8, x27, x26
   3fa54:	b.mi	3fc84 <__gmpn_sbpi1_div_q@@Base+0x2dc>  // b.first
   3fa58:	stur	x9, [x29, #-48]
   3fa5c:	ldp	x9, x21, [sp, #32]
   3fa60:	str	w28, [sp, #52]
   3fa64:	sub	x28, x19, x23, lsl #3
   3fa68:	str	x27, [sp, #56]
   3fa6c:	lsl	x25, x9, #3
   3fa70:	mov	x9, x26
   3fa74:	add	x26, x8, #0x1
   3fa78:	mvn	x8, x9
   3fa7c:	add	x20, x21, x8, lsl #3
   3fa80:	ldur	x16, [x29, #-16]
   3fa84:	cmp	x22, x15
   3fa88:	add	x24, x21, x25
   3fa8c:	b.eq	3fb40 <__gmpn_sbpi1_div_q@@Base+0x198>  // b.none
   3fa90:	ldur	x10, [x29, #-24]
   3fa94:	ldp	x11, x8, [x24, #-24]
   3fa98:	mul	x9, x22, x10
   3fa9c:	umulh	x10, x22, x10
   3faa0:	adds	x12, x9, x8
   3faa4:	adc	x9, x10, x22
   3faa8:	msub	x8, x9, x15, x8
   3faac:	subs	x14, x11, x16
   3fab0:	sbc	x8, x8, x15
   3fab4:	mul	x10, x9, x16
   3fab8:	umulh	x13, x16, x9
   3fabc:	subs	x11, x14, x10
   3fac0:	sbc	x8, x8, x13
   3fac4:	cmp	x8, x12
   3fac8:	cset	w10, cs  // cs = hs, nlast
   3facc:	csetm	x12, cs  // cs = hs, nlast
   3fad0:	sub	x9, x9, x10
   3fad4:	and	x10, x16, x12
   3fad8:	and	x12, x15, x12
   3fadc:	adds	x27, x11, x10
   3fae0:	adc	x22, x8, x12
   3fae4:	cmp	x22, x15
   3fae8:	add	x19, x9, #0x1
   3faec:	b.cs	3fb70 <__gmpn_sbpi1_div_q@@Base+0x1c8>  // b.hs, b.nlast
   3faf0:	ldp	x2, x1, [x29, #-40]
   3faf4:	add	x23, x20, x25
   3faf8:	mov	x0, x23
   3fafc:	mov	x3, x19
   3fb00:	bl	cba0 <__gmpn_submul_1@plt>
   3fb04:	subs	x8, x27, x0
   3fb08:	cset	w9, cc  // cc = lo, ul, last
   3fb0c:	subs	x22, x22, x9
   3fb10:	stur	x8, [x24, #-24]
   3fb14:	b.cc	3fb9c <__gmpn_sbpi1_div_q@@Base+0x1f4>  // b.lo, b.ul, b.last
   3fb18:	ldur	x15, [x29, #-8]
   3fb1c:	sub	x26, x26, #0x1
   3fb20:	add	x8, x28, x25
   3fb24:	sub	x28, x28, #0x8
   3fb28:	sub	x21, x21, #0x8
   3fb2c:	cmp	x26, #0x0
   3fb30:	sub	x20, x20, #0x8
   3fb34:	stur	x19, [x8, #-8]
   3fb38:	b.gt	3fa80 <__gmpn_sbpi1_div_q@@Base+0xd8>
   3fb3c:	b	3fbc4 <__gmpn_sbpi1_div_q@@Base+0x21c>
   3fb40:	ldur	x8, [x24, #-16]
   3fb44:	cmp	x8, x16
   3fb48:	b.ne	3fa90 <__gmpn_sbpi1_div_q@@Base+0xe8>  // b.any
   3fb4c:	ldur	x1, [x29, #-32]
   3fb50:	ldur	x2, [x29, #-56]
   3fb54:	add	x0, x20, x25
   3fb58:	mov	x3, #0xffffffffffffffff    	// #-1
   3fb5c:	mov	x19, #0xffffffffffffffff    	// #-1
   3fb60:	bl	cba0 <__gmpn_submul_1@plt>
   3fb64:	ldur	x15, [x29, #-8]
   3fb68:	ldur	x22, [x24, #-16]
   3fb6c:	b	3fb1c <__gmpn_sbpi1_div_q@@Base+0x174>
   3fb70:	ldur	x8, [x29, #-16]
   3fb74:	cmp	x27, x8
   3fb78:	b.cs	3fb84 <__gmpn_sbpi1_div_q@@Base+0x1dc>  // b.hs, b.nlast
   3fb7c:	cmp	x22, x15
   3fb80:	b.ls	3faf0 <__gmpn_sbpi1_div_q@@Base+0x148>  // b.plast
   3fb84:	ldur	x9, [x29, #-16]
   3fb88:	subs	x8, x27, x9
   3fb8c:	sbc	x22, x22, x15
   3fb90:	add	x19, x19, #0x1
   3fb94:	mov	x27, x8
   3fb98:	b	3faf0 <__gmpn_sbpi1_div_q@@Base+0x148>
   3fb9c:	ldur	x2, [x29, #-32]
   3fba0:	ldur	x3, [x29, #-48]
   3fba4:	mov	x0, x23
   3fba8:	mov	x1, x23
   3fbac:	bl	cc30 <__gmpn_add_n@plt>
   3fbb0:	ldur	x15, [x29, #-8]
   3fbb4:	sub	x19, x19, #0x1
   3fbb8:	add	x8, x22, x15
   3fbbc:	add	x22, x8, x0
   3fbc0:	b	3fb1c <__gmpn_sbpi1_div_q@@Base+0x174>
   3fbc4:	add	x24, x28, x25
   3fbc8:	ldr	x27, [sp, #56]
   3fbcc:	ldr	w28, [sp, #52]
   3fbd0:	ldur	x26, [x29, #-56]
   3fbd4:	add	x8, x21, x25
   3fbd8:	sub	x20, x8, #0x10
   3fbdc:	cmp	x26, #0x2
   3fbe0:	b.lt	3fc94 <__gmpn_sbpi1_div_q@@Base+0x2ec>  // b.tstop
   3fbe4:	cmp	x22, x15
   3fbe8:	cset	w8, cs  // cs = hs, nlast
   3fbec:	cmp	x26, #0x2
   3fbf0:	b.ne	3fca0 <__gmpn_sbpi1_div_q@@Base+0x2f8>  // b.any
   3fbf4:	ldur	x26, [x29, #-32]
   3fbf8:	ldr	x25, [sp, #64]
   3fbfc:	mov	x19, x20
   3fc00:	sub	x11, x20, #0x8
   3fc04:	mov	x23, #0xffffffffffffffff    	// #-1
   3fc08:	cbnz	w8, 3fe78 <__gmpn_sbpi1_div_q@@Base+0x4d0>
   3fc0c:	ldp	x10, x16, [x29, #-24]
   3fc10:	ldr	x8, [x19]
   3fc14:	mov	x21, x11
   3fc18:	ldr	x11, [x11]
   3fc1c:	mul	x9, x22, x10
   3fc20:	umulh	x10, x22, x10
   3fc24:	adds	x12, x9, x8
   3fc28:	adc	x9, x10, x22
   3fc2c:	msub	x8, x9, x15, x8
   3fc30:	mul	x10, x9, x16
   3fc34:	umulh	x13, x16, x9
   3fc38:	subs	x14, x11, x16
   3fc3c:	sbc	x8, x8, x15
   3fc40:	subs	x11, x14, x10
   3fc44:	sbc	x10, x8, x13
   3fc48:	cmp	x10, x12
   3fc4c:	ldur	x26, [x29, #-56]
   3fc50:	cset	w8, cs  // cs = hs, nlast
   3fc54:	csetm	x12, cs  // cs = hs, nlast
   3fc58:	sub	x9, x9, x8
   3fc5c:	and	x13, x16, x12
   3fc60:	and	x12, x15, x12
   3fc64:	adds	x8, x11, x13
   3fc68:	adc	x22, x10, x12
   3fc6c:	cmp	x22, x15
   3fc70:	add	x20, x9, #0x1
   3fc74:	b.cs	40068 <__gmpn_sbpi1_div_q@@Base+0x6c0>  // b.hs, b.nlast
   3fc78:	str	x8, [x21]
   3fc7c:	str	x22, [x19]
   3fc80:	b	3fea4 <__gmpn_sbpi1_div_q@@Base+0x4fc>
   3fc84:	add	x24, x19, x27, lsl #3
   3fc88:	sub	x20, x21, #0x10
   3fc8c:	cmp	x26, #0x2
   3fc90:	b.ge	3fbe4 <__gmpn_sbpi1_div_q@@Base+0x23c>  // b.tcont
   3fc94:	ldr	x25, [sp, #64]
   3fc98:	mov	x23, #0xffffffffffffffff    	// #-1
   3fc9c:	b	3feac <__gmpn_sbpi1_div_q@@Base+0x504>
   3fca0:	mov	w9, #0x1                   	// #1
   3fca4:	str	w28, [sp, #52]
   3fca8:	sub	x9, x9, x26
   3fcac:	mov	x28, x26
   3fcb0:	ldur	x26, [x29, #-32]
   3fcb4:	str	x27, [sp, #56]
   3fcb8:	mov	x25, xzr
   3fcbc:	add	x9, x20, x9, lsl #3
   3fcc0:	mov	x27, #0xffffffffffffffff    	// #-1
   3fcc4:	stur	x9, [x29, #-40]
   3fcc8:	b	3fd1c <__gmpn_sbpi1_div_q@@Base+0x374>
   3fccc:	ldur	x0, [x29, #-40]
   3fcd0:	mov	x3, #0xffffffffffffffff    	// #-1
   3fcd4:	mov	x1, x26
   3fcd8:	mov	x2, x28
   3fcdc:	mov	x23, #0xffffffffffffffff    	// #-1
   3fce0:	bl	cba0 <__gmpn_submul_1@plt>
   3fce4:	cmp	x22, x0
   3fce8:	b.ne	3fdd0 <__gmpn_sbpi1_div_q@@Base+0x428>  // b.any
   3fcec:	ldur	x15, [x29, #-8]
   3fcf0:	ldr	x22, [x21]
   3fcf4:	and	x9, x27, x15
   3fcf8:	add	x8, x24, x25
   3fcfc:	cmp	x22, x9
   3fd00:	add	x26, x26, #0x8
   3fd04:	sub	x25, x25, #0x8
   3fd08:	stur	x23, [x8, #-8]
   3fd0c:	cset	w8, cs  // cs = hs, nlast
   3fd10:	cmp	x19, #0x1
   3fd14:	sub	x28, x28, #0x1
   3fd18:	b.le	3fe5c <__gmpn_sbpi1_div_q@@Base+0x4b4>
   3fd1c:	sub	x19, x28, #0x2
   3fd20:	add	x21, x20, x25
   3fd24:	tbnz	w8, #0, 3fccc <__gmpn_sbpi1_div_q@@Base+0x324>
   3fd28:	stur	x27, [x29, #-48]
   3fd2c:	ldp	x10, x16, [x29, #-24]
   3fd30:	ldp	x11, x8, [x21, #-8]
   3fd34:	mov	x27, x20
   3fd38:	mov	x20, x24
   3fd3c:	mul	x9, x22, x10
   3fd40:	umulh	x10, x22, x10
   3fd44:	adds	x12, x9, x8
   3fd48:	adc	x9, x10, x22
   3fd4c:	msub	x8, x9, x15, x8
   3fd50:	subs	x14, x11, x16
   3fd54:	sbc	x8, x8, x15
   3fd58:	mul	x10, x9, x16
   3fd5c:	umulh	x13, x16, x9
   3fd60:	subs	x11, x14, x10
   3fd64:	sbc	x8, x8, x13
   3fd68:	cmp	x8, x12
   3fd6c:	cset	w10, cs  // cs = hs, nlast
   3fd70:	csetm	x12, cs  // cs = hs, nlast
   3fd74:	sub	x9, x9, x10
   3fd78:	and	x10, x16, x12
   3fd7c:	and	x12, x15, x12
   3fd80:	adds	x24, x11, x10
   3fd84:	adc	x22, x8, x12
   3fd88:	cmp	x22, x15
   3fd8c:	add	x23, x9, #0x1
   3fd90:	b.cs	3fdf8 <__gmpn_sbpi1_div_q@@Base+0x450>  // b.hs, b.nlast
   3fd94:	ldur	x0, [x29, #-40]
   3fd98:	mov	x1, x26
   3fd9c:	mov	x2, x19
   3fda0:	mov	x3, x23
   3fda4:	bl	cba0 <__gmpn_submul_1@plt>
   3fda8:	subs	x8, x24, x0
   3fdac:	cset	w9, cc  // cc = lo, ul, last
   3fdb0:	subs	x22, x22, x9
   3fdb4:	stur	x8, [x21, #-8]
   3fdb8:	b.cc	3fe24 <__gmpn_sbpi1_div_q@@Base+0x47c>  // b.lo, b.ul, b.last
   3fdbc:	ldur	x15, [x29, #-8]
   3fdc0:	mov	x24, x20
   3fdc4:	mov	x20, x27
   3fdc8:	ldur	x27, [x29, #-48]
   3fdcc:	b	3fcf4 <__gmpn_sbpi1_div_q@@Base+0x34c>
   3fdd0:	and	x8, x0, x27
   3fdd4:	cmp	x22, x8
   3fdd8:	b.cs	3fe50 <__gmpn_sbpi1_div_q@@Base+0x4a8>  // b.hs, b.nlast
   3fddc:	ldur	x0, [x29, #-40]
   3fde0:	mov	x2, x26
   3fde4:	mov	x3, x28
   3fde8:	mov	x1, x0
   3fdec:	bl	cc30 <__gmpn_add_n@plt>
   3fdf0:	mov	x23, #0xfffffffffffffffe    	// #-2
   3fdf4:	b	3fcec <__gmpn_sbpi1_div_q@@Base+0x344>
   3fdf8:	ldur	x8, [x29, #-16]
   3fdfc:	cmp	x24, x8
   3fe00:	b.cs	3fe0c <__gmpn_sbpi1_div_q@@Base+0x464>  // b.hs, b.nlast
   3fe04:	cmp	x22, x15
   3fe08:	b.ls	3fd94 <__gmpn_sbpi1_div_q@@Base+0x3ec>  // b.plast
   3fe0c:	ldur	x9, [x29, #-16]
   3fe10:	subs	x8, x24, x9
   3fe14:	sbc	x22, x22, x15
   3fe18:	add	x23, x23, #0x1
   3fe1c:	mov	x24, x8
   3fe20:	b	3fd94 <__gmpn_sbpi1_div_q@@Base+0x3ec>
   3fe24:	ldur	x0, [x29, #-40]
   3fe28:	sub	x3, x28, #0x1
   3fe2c:	mov	x2, x26
   3fe30:	mov	x1, x0
   3fe34:	bl	cc30 <__gmpn_add_n@plt>
   3fe38:	ldur	x15, [x29, #-8]
   3fe3c:	sub	x23, x23, #0x1
   3fe40:	mov	x24, x20
   3fe44:	add	x8, x22, x15
   3fe48:	add	x22, x8, x0
   3fe4c:	b	3fdc4 <__gmpn_sbpi1_div_q@@Base+0x41c>
   3fe50:	mov	x27, xzr
   3fe54:	mov	x23, #0xffffffffffffffff    	// #-1
   3fe58:	b	3fcec <__gmpn_sbpi1_div_q@@Base+0x344>
   3fe5c:	mov	x23, x27
   3fe60:	add	x19, x20, x25
   3fe64:	add	x24, x24, x25
   3fe68:	ldp	x27, x25, [sp, #56]
   3fe6c:	ldr	w28, [sp, #52]
   3fe70:	sub	x11, x19, #0x8
   3fe74:	cbz	w8, 3fc0c <__gmpn_sbpi1_div_q@@Base+0x264>
   3fe78:	mov	w2, #0x2                   	// #2
   3fe7c:	mov	x3, #0xffffffffffffffff    	// #-1
   3fe80:	mov	x0, x11
   3fe84:	mov	x1, x26
   3fe88:	mov	x20, #0xffffffffffffffff    	// #-1
   3fe8c:	mov	x21, x11
   3fe90:	bl	cba0 <__gmpn_submul_1@plt>
   3fe94:	cmp	x22, x0
   3fe98:	b.ne	40090 <__gmpn_sbpi1_div_q@@Base+0x6e8>  // b.any
   3fe9c:	ldr	x22, [x19]
   3fea0:	ldur	x26, [x29, #-56]
   3fea4:	str	x20, [x24, #-8]!
   3fea8:	mov	x20, x21
   3feac:	ldr	x8, [x20, #8]
   3feb0:	cmp	x8, x22
   3feb4:	b.ne	40120 <__gmpn_sbpi1_div_q@@Base+0x778>  // b.any
   3feb8:	mvn	w8, w28
   3febc:	and	x9, x23, x25
   3fec0:	mov	x21, x24
   3fec4:	cmp	x22, x9
   3fec8:	lsr	w24, w8, #31
   3fecc:	b.cc	3fef8 <__gmpn_sbpi1_div_q@@Base+0x550>  // b.lo, b.ul, b.last
   3fed0:	mov	x19, x24
   3fed4:	mov	x0, x19
   3fed8:	ldp	x20, x19, [sp, #208]
   3fedc:	ldp	x22, x21, [sp, #192]
   3fee0:	ldp	x24, x23, [sp, #176]
   3fee4:	ldp	x26, x25, [sp, #160]
   3fee8:	ldp	x28, x27, [sp, #144]
   3feec:	ldp	x29, x30, [sp, #128]
   3fef0:	add	sp, sp, #0xe0
   3fef4:	ret
   3fef8:	cmp	x25, #0x3
   3fefc:	b.lt	3ff84 <__gmpn_sbpi1_div_q@@Base+0x5dc>  // b.tstop
   3ff00:	mov	x8, x26
   3ff04:	ldr	x26, [x20]
   3ff08:	subs	x23, x8, #0x3
   3ff0c:	stur	x20, [x29, #-8]
   3ff10:	b.lt	3ff88 <__gmpn_sbpi1_div_q@@Base+0x5e0>  // b.tstop
   3ff14:	sub	x19, x20, #0x8
   3ff18:	mov	w20, #0x1                   	// #1
   3ff1c:	b	3ff38 <__gmpn_sbpi1_div_q@@Base+0x590>
   3ff20:	sub	x22, x22, #0x1
   3ff24:	cmp	x23, #0x0
   3ff28:	sub	x23, x23, #0x1
   3ff2c:	add	x20, x20, #0x1
   3ff30:	sub	x19, x19, #0x8
   3ff34:	b.le	3ff88 <__gmpn_sbpi1_div_q@@Base+0x5e0>
   3ff38:	ldr	x3, [x21, x23, lsl #3]
   3ff3c:	ldur	x1, [x29, #-32]
   3ff40:	mov	x0, x19
   3ff44:	mov	x2, x20
   3ff48:	bl	cba0 <__gmpn_submul_1@plt>
   3ff4c:	subs	x26, x26, x0
   3ff50:	b.cs	3ff24 <__gmpn_sbpi1_div_q@@Base+0x57c>  // b.hs, b.nlast
   3ff54:	cbnz	x22, 3ff20 <__gmpn_sbpi1_div_q@@Base+0x578>
   3ff58:	mov	w3, #0x1                   	// #1
   3ff5c:	mov	x0, x21
   3ff60:	mov	x1, x21
   3ff64:	mov	x2, x27
   3ff68:	mov	x23, x27
   3ff6c:	bl	caf0 <__gmpn_sub_1@plt>
   3ff70:	cbnz	x0, 40138 <__gmpn_sbpi1_div_q@@Base+0x790>
   3ff74:	sub	x19, x24, x0
   3ff78:	mov	w20, #0x1                   	// #1
   3ff7c:	mov	x27, x23
   3ff80:	b	3ff94 <__gmpn_sbpi1_div_q@@Base+0x5ec>
   3ff84:	b	3ff98 <__gmpn_sbpi1_div_q@@Base+0x5f0>
   3ff88:	ldur	x8, [x29, #-8]
   3ff8c:	mov	w20, wzr
   3ff90:	str	x26, [x8]
   3ff94:	cbnz	w20, 40060 <__gmpn_sbpi1_div_q@@Base+0x6b8>
   3ff98:	ldr	x8, [sp, #24]
   3ff9c:	cmp	x8, x25
   3ffa0:	b.ge	4005c <__gmpn_sbpi1_div_q@@Base+0x6b4>  // b.tcont
   3ffa4:	tbnz	w28, #31, 3ffc8 <__gmpn_sbpi1_div_q@@Base+0x620>
   3ffa8:	ldr	x8, [sp, #40]
   3ffac:	ldp	x3, x2, [sp, #8]
   3ffb0:	add	x0, x8, x27, lsl #3
   3ffb4:	mov	x1, x0
   3ffb8:	bl	c420 <__gmpn_sub_n@plt>
   3ffbc:	cbz	x0, 3ffc8 <__gmpn_sbpi1_div_q@@Base+0x620>
   3ffc0:	cbz	x22, 400dc <__gmpn_sbpi1_div_q@@Base+0x734>
   3ffc4:	sub	x22, x22, #0x1
   3ffc8:	cbz	x27, 400d0 <__gmpn_sbpi1_div_q@@Base+0x728>
   3ffcc:	sub	x8, x25, x27
   3ffd0:	cmp	x8, #0x2
   3ffd4:	b.lt	4005c <__gmpn_sbpi1_div_q@@Base+0x6b4>  // b.tstop
   3ffd8:	ldr	x10, [sp, #40]
   3ffdc:	ldr	x11, [sp, #16]
   3ffe0:	lsl	x8, x25, #1
   3ffe4:	mov	w23, #0x1                   	// #1
   3ffe8:	add	x9, x10, x25, lsl #3
   3ffec:	sub	x25, x11, #0x10
   3fff0:	ldr	x11, [sp, #32]
   3fff4:	sub	x20, x9, #0x10
   3fff8:	sub	x26, x8, x11
   3fffc:	add	x8, x10, x26, lsl #3
   40000:	sub	x28, x8, #0x10
   40004:	b	40024 <__gmpn_sbpi1_div_q@@Base+0x67c>
   40008:	sub	x8, x26, #0x2
   4000c:	sub	x26, x26, #0x1
   40010:	sub	x20, x20, #0x8
   40014:	sub	x28, x28, #0x8
   40018:	cmp	x8, #0x0
   4001c:	add	x23, x23, #0x1
   40020:	b.le	4005c <__gmpn_sbpi1_div_q@@Base+0x6b4>
   40024:	ldr	x3, [x25, x26, lsl #3]
   40028:	mov	x0, x28
   4002c:	mov	x1, x21
   40030:	mov	x2, x27
   40034:	bl	cba0 <__gmpn_submul_1@plt>
   40038:	mov	x3, x0
   4003c:	mov	x0, x20
   40040:	mov	x1, x20
   40044:	mov	x2, x23
   40048:	bl	caf0 <__gmpn_sub_1@plt>
   4004c:	cbz	x0, 40008 <__gmpn_sbpi1_div_q@@Base+0x660>
   40050:	cbz	x22, 40100 <__gmpn_sbpi1_div_q@@Base+0x758>
   40054:	sub	x22, x22, #0x1
   40058:	b	40008 <__gmpn_sbpi1_div_q@@Base+0x660>
   4005c:	mov	w20, wzr
   40060:	cbnz	w20, 3fed4 <__gmpn_sbpi1_div_q@@Base+0x52c>
   40064:	b	3fed0 <__gmpn_sbpi1_div_q@@Base+0x528>
   40068:	cmp	x8, x16
   4006c:	b.cs	40078 <__gmpn_sbpi1_div_q@@Base+0x6d0>  // b.hs, b.nlast
   40070:	cmp	x22, x15
   40074:	b.ls	3fc78 <__gmpn_sbpi1_div_q@@Base+0x2d0>  // b.plast
   40078:	ldur	x10, [x29, #-16]
   4007c:	subs	x9, x8, x10
   40080:	sbc	x22, x22, x15
   40084:	add	x20, x20, #0x1
   40088:	mov	x8, x9
   4008c:	b	3fc78 <__gmpn_sbpi1_div_q@@Base+0x2d0>
   40090:	and	x8, x0, x23
   40094:	cmp	x22, x8
   40098:	b.cs	400c4 <__gmpn_sbpi1_div_q@@Base+0x71c>  // b.hs, b.nlast
   4009c:	ldr	x8, [x19]
   400a0:	mov	x20, #0xfffffffffffffffe    	// #-2
   400a4:	ldp	x9, x10, [x26]
   400a8:	mov	x13, x21
   400ac:	ldr	x11, [x21]
   400b0:	adds	x12, x11, x9
   400b4:	adc	x8, x8, x10
   400b8:	str	x8, [x19]
   400bc:	str	x12, [x21]
   400c0:	b	3fe9c <__gmpn_sbpi1_div_q@@Base+0x4f4>
   400c4:	mov	x23, xzr
   400c8:	mov	x20, #0xffffffffffffffff    	// #-1
   400cc:	b	3fe9c <__gmpn_sbpi1_div_q@@Base+0x4f4>
   400d0:	mov	w20, #0x1                   	// #1
   400d4:	mov	x19, x24
   400d8:	b	40060 <__gmpn_sbpi1_div_q@@Base+0x6b8>
   400dc:	cbz	x27, 400f4 <__gmpn_sbpi1_div_q@@Base+0x74c>
   400e0:	mov	x2, x27
   400e4:	mov	w3, #0x1                   	// #1
   400e8:	mov	x0, x21
   400ec:	mov	x1, x21
   400f0:	bl	caf0 <__gmpn_sub_1@plt>
   400f4:	sub	x19, x24, x0
   400f8:	mov	w20, #0x1                   	// #1
   400fc:	b	40060 <__gmpn_sbpi1_div_q@@Base+0x6b8>
   40100:	mov	w3, #0x1                   	// #1
   40104:	mov	x0, x21
   40108:	mov	x1, x21
   4010c:	mov	x2, x27
   40110:	mov	w20, #0x1                   	// #1
   40114:	bl	caf0 <__gmpn_sub_1@plt>
   40118:	mov	x19, x24
   4011c:	b	40060 <__gmpn_sbpi1_div_q@@Base+0x6b8>
   40120:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   40124:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   40128:	add	x0, x0, #0x1df
   4012c:	add	x2, x2, #0x1ed
   40130:	mov	w1, #0xc5                  	// #197
   40134:	bl	c850 <__gmp_assert_fail@plt>
   40138:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   4013c:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   40140:	add	x0, x0, #0x1df
   40144:	add	x2, x2, #0x1f9
   40148:	mov	w1, #0xf8                  	// #248
   4014c:	bl	c850 <__gmp_assert_fail@plt>

0000000000040150 <__gmpn_sbpi1_div_qr@@Base>:
   40150:	sub	sp, sp, #0xb0
   40154:	stp	x20, x19, [sp, #160]
   40158:	add	x20, x1, x2, lsl #3
   4015c:	stp	x26, x25, [sp, #112]
   40160:	sub	x25, x20, x4, lsl #3
   40164:	stp	x29, x30, [sp, #80]
   40168:	stp	x28, x27, [sp, #96]
   4016c:	stp	x24, x23, [sp, #128]
   40170:	add	x29, sp, #0x50
   40174:	mov	x23, x2
   40178:	mov	x24, x1
   4017c:	mov	x27, x0
   40180:	mov	x0, x25
   40184:	mov	x1, x3
   40188:	mov	x2, x4
   4018c:	stp	x22, x21, [sp, #144]
   40190:	stur	x5, [x29, #-16]
   40194:	mov	x22, x4
   40198:	mov	x19, x3
   4019c:	bl	c570 <__gmpn_cmp@plt>
   401a0:	mvn	w8, w0
   401a4:	lsr	w21, w8, #31
   401a8:	tbnz	w0, #31, 401c0 <__gmpn_sbpi1_div_qr@@Base+0x70>
   401ac:	mov	x0, x25
   401b0:	mov	x1, x25
   401b4:	mov	x2, x19
   401b8:	mov	x3, x22
   401bc:	bl	c420 <__gmpn_sub_n@plt>
   401c0:	stur	x19, [x29, #-8]
   401c4:	ldur	x19, [x20, #-8]
   401c8:	sub	x8, x23, x22
   401cc:	cmp	x8, #0x1
   401d0:	b.lt	40374 <__gmpn_sbpi1_div_qr@@Base+0x224>  // b.tstop
   401d4:	ldur	x10, [x29, #-8]
   401d8:	stp	x23, x21, [sp, #24]
   401dc:	sub	x11, x22, #0x2
   401e0:	sub	x12, x22, #0x1
   401e4:	ldr	x16, [x10, x12, lsl #3]
   401e8:	ldr	x10, [x10, x11, lsl #3]
   401ec:	mvn	x9, x22
   401f0:	lsl	x9, x9, #3
   401f4:	lsl	x28, x23, #3
   401f8:	add	x21, x27, x9
   401fc:	add	x23, x24, x9
   40200:	add	x25, x8, #0x1
   40204:	str	x22, [sp, #8]
   40208:	str	x12, [sp, #16]
   4020c:	str	x11, [sp, #40]
   40210:	stp	x16, x10, [x29, #-32]
   40214:	cmp	x19, x16
   40218:	add	x26, x24, x28
   4021c:	b.eq	402d8 <__gmpn_sbpi1_div_qr@@Base+0x188>  // b.none
   40220:	ldp	x15, x10, [x29, #-24]
   40224:	ldp	x11, x8, [x26, #-24]
   40228:	mul	x9, x19, x10
   4022c:	umulh	x10, x19, x10
   40230:	adds	x12, x9, x8
   40234:	adc	x9, x10, x19
   40238:	msub	x8, x9, x16, x8
   4023c:	subs	x14, x11, x15
   40240:	sbc	x8, x8, x16
   40244:	mul	x10, x9, x15
   40248:	umulh	x13, x15, x9
   4024c:	subs	x11, x14, x10
   40250:	sbc	x8, x8, x13
   40254:	cmp	x8, x12
   40258:	cset	w10, cs  // cs = hs, nlast
   4025c:	csetm	x12, cs  // cs = hs, nlast
   40260:	sub	x9, x9, x10
   40264:	and	x10, x15, x12
   40268:	and	x12, x16, x12
   4026c:	adds	x22, x11, x10
   40270:	adc	x19, x8, x12
   40274:	cmp	x19, x16
   40278:	add	x27, x9, #0x1
   4027c:	b.cs	4030c <__gmpn_sbpi1_div_qr@@Base+0x1bc>  // b.hs, b.nlast
   40280:	ldur	x1, [x29, #-8]
   40284:	ldr	x2, [sp, #40]
   40288:	mov	x20, x28
   4028c:	add	x28, x23, x28
   40290:	mov	x0, x28
   40294:	mov	x3, x27
   40298:	bl	cba0 <__gmpn_submul_1@plt>
   4029c:	subs	x8, x22, x0
   402a0:	cset	w9, cc  // cc = lo, ul, last
   402a4:	subs	x19, x19, x9
   402a8:	stur	x8, [x26, #-24]
   402ac:	b.cc	40338 <__gmpn_sbpi1_div_qr@@Base+0x1e8>  // b.lo, b.ul, b.last
   402b0:	ldur	x16, [x29, #-32]
   402b4:	mov	x28, x20
   402b8:	sub	x25, x25, #0x1
   402bc:	str	x27, [x21, x28]
   402c0:	sub	x21, x21, #0x8
   402c4:	sub	x24, x24, #0x8
   402c8:	cmp	x25, #0x1
   402cc:	sub	x23, x23, #0x8
   402d0:	b.gt	40214 <__gmpn_sbpi1_div_qr@@Base+0xc4>
   402d4:	b	40364 <__gmpn_sbpi1_div_qr@@Base+0x214>
   402d8:	ldur	x8, [x26, #-16]
   402dc:	ldur	x9, [x29, #-24]
   402e0:	cmp	x8, x9
   402e4:	b.ne	40220 <__gmpn_sbpi1_div_qr@@Base+0xd0>  // b.any
   402e8:	ldur	x1, [x29, #-8]
   402ec:	ldr	x2, [sp, #8]
   402f0:	add	x0, x23, x28
   402f4:	mov	x3, #0xffffffffffffffff    	// #-1
   402f8:	mov	x27, #0xffffffffffffffff    	// #-1
   402fc:	bl	cba0 <__gmpn_submul_1@plt>
   40300:	ldur	x16, [x29, #-32]
   40304:	ldur	x19, [x26, #-16]
   40308:	b	402b8 <__gmpn_sbpi1_div_qr@@Base+0x168>
   4030c:	ldur	x8, [x29, #-24]
   40310:	cmp	x22, x8
   40314:	b.cs	40320 <__gmpn_sbpi1_div_qr@@Base+0x1d0>  // b.hs, b.nlast
   40318:	cmp	x19, x16
   4031c:	b.ls	40280 <__gmpn_sbpi1_div_qr@@Base+0x130>  // b.plast
   40320:	ldur	x9, [x29, #-24]
   40324:	subs	x8, x22, x9
   40328:	sbc	x19, x19, x16
   4032c:	add	x27, x27, #0x1
   40330:	mov	x22, x8
   40334:	b	40280 <__gmpn_sbpi1_div_qr@@Base+0x130>
   40338:	ldur	x2, [x29, #-8]
   4033c:	ldr	x3, [sp, #16]
   40340:	mov	x0, x28
   40344:	mov	x1, x28
   40348:	bl	cc30 <__gmpn_add_n@plt>
   4034c:	ldur	x16, [x29, #-32]
   40350:	sub	x27, x27, #0x1
   40354:	mov	x28, x20
   40358:	add	x8, x19, x16
   4035c:	add	x19, x8, x0
   40360:	b	402b8 <__gmpn_sbpi1_div_qr@@Base+0x168>
   40364:	ldp	x8, x21, [sp, #24]
   40368:	add	x8, x24, x8, lsl #3
   4036c:	sub	x8, x8, #0x10
   40370:	b	40378 <__gmpn_sbpi1_div_qr@@Base+0x228>
   40374:	sub	x8, x20, #0x10
   40378:	str	x19, [x8, #8]
   4037c:	mov	x0, x21
   40380:	ldp	x20, x19, [sp, #160]
   40384:	ldp	x22, x21, [sp, #144]
   40388:	ldp	x24, x23, [sp, #128]
   4038c:	ldp	x26, x25, [sp, #112]
   40390:	ldp	x28, x27, [sp, #96]
   40394:	ldp	x29, x30, [sp, #80]
   40398:	add	sp, sp, #0xb0
   4039c:	ret

00000000000403a0 <__gmpn_sbpi1_divappr_q@@Base>:
   403a0:	sub	sp, sp, #0xa0
   403a4:	stp	x20, x19, [sp, #144]
   403a8:	sub	x20, x2, x4
   403ac:	add	x8, x20, #0x1
   403b0:	subs	x8, x4, x8
   403b4:	stp	x26, x25, [sp, #96]
   403b8:	add	x19, x1, x2, lsl #3
   403bc:	add	x8, x3, x8, lsl #3
   403c0:	csinc	x25, x4, x20, le
   403c4:	stp	x28, x27, [sp, #80]
   403c8:	stp	x24, x23, [sp, #112]
   403cc:	csel	x28, x8, x3, gt
   403d0:	sub	x24, x19, x25, lsl #3
   403d4:	stp	x29, x30, [sp, #64]
   403d8:	add	x29, sp, #0x40
   403dc:	mov	x26, x2
   403e0:	mov	x23, x1
   403e4:	str	x0, [sp, #24]
   403e8:	mov	x0, x24
   403ec:	mov	x1, x28
   403f0:	mov	x2, x25
   403f4:	stp	x22, x21, [sp, #128]
   403f8:	stur	x5, [x29, #-16]
   403fc:	mov	x27, x4
   40400:	bl	c570 <__gmpn_cmp@plt>
   40404:	str	w0, [sp, #12]
   40408:	tbnz	w0, #31, 40420 <__gmpn_sbpi1_divappr_q@@Base+0x80>
   4040c:	mov	x0, x24
   40410:	mov	x1, x24
   40414:	mov	x2, x28
   40418:	mov	x3, x25
   4041c:	bl	c420 <__gmpn_sub_n@plt>
   40420:	sub	x8, x25, #0x2
   40424:	sub	x10, x25, #0x1
   40428:	ldr	x15, [x28, x10, lsl #3]
   4042c:	ldr	x22, [x28, x8, lsl #3]
   40430:	ldur	x21, [x19, #-8]
   40434:	str	x8, [sp, #32]
   40438:	subs	x8, x20, x25
   4043c:	mov	x1, x28
   40440:	mov	x9, x25
   40444:	stur	x15, [x29, #-8]
   40448:	stur	x22, [x29, #-24]
   4044c:	b.mi	40660 <__gmpn_sbpi1_divappr_q@@Base+0x2c0>  // b.first
   40450:	str	x10, [sp]
   40454:	ldr	x10, [sp, #24]
   40458:	add	x22, x8, #0x1
   4045c:	mvn	x8, x9
   40460:	lsl	x25, x26, #3
   40464:	sub	x24, x10, x27, lsl #3
   40468:	add	x20, x23, x8, lsl #3
   4046c:	stp	x9, x1, [sp, #16]
   40470:	cmp	x21, x15
   40474:	add	x28, x23, x25
   40478:	b.eq	40530 <__gmpn_sbpi1_divappr_q@@Base+0x190>  // b.none
   4047c:	ldp	x16, x10, [x29, #-24]
   40480:	ldp	x11, x8, [x28, #-24]
   40484:	mul	x9, x21, x10
   40488:	umulh	x10, x21, x10
   4048c:	adds	x12, x9, x8
   40490:	adc	x9, x10, x21
   40494:	msub	x8, x9, x15, x8
   40498:	subs	x14, x11, x16
   4049c:	sbc	x8, x8, x15
   404a0:	mul	x10, x9, x16
   404a4:	umulh	x13, x16, x9
   404a8:	subs	x11, x14, x10
   404ac:	sbc	x8, x8, x13
   404b0:	cmp	x8, x12
   404b4:	cset	w10, cs  // cs = hs, nlast
   404b8:	csetm	x12, cs  // cs = hs, nlast
   404bc:	sub	x9, x9, x10
   404c0:	and	x10, x16, x12
   404c4:	and	x12, x15, x12
   404c8:	adds	x19, x11, x10
   404cc:	adc	x21, x8, x12
   404d0:	cmp	x21, x15
   404d4:	add	x26, x9, #0x1
   404d8:	b.cs	40560 <__gmpn_sbpi1_divappr_q@@Base+0x1c0>  // b.hs, b.nlast
   404dc:	ldr	x2, [sp, #32]
   404e0:	add	x27, x20, x25
   404e4:	mov	x0, x27
   404e8:	mov	x3, x26
   404ec:	bl	cba0 <__gmpn_submul_1@plt>
   404f0:	subs	x8, x19, x0
   404f4:	cset	w9, cc  // cc = lo, ul, last
   404f8:	subs	x21, x21, x9
   404fc:	stur	x8, [x28, #-24]
   40500:	b.cc	4058c <__gmpn_sbpi1_divappr_q@@Base+0x1ec>  // b.lo, b.ul, b.last
   40504:	ldur	x15, [x29, #-8]
   40508:	ldr	x1, [sp, #24]
   4050c:	sub	x22, x22, #0x1
   40510:	add	x8, x24, x25
   40514:	sub	x24, x24, #0x8
   40518:	sub	x23, x23, #0x8
   4051c:	cmp	x22, #0x0
   40520:	sub	x20, x20, #0x8
   40524:	stur	x26, [x8, #-8]
   40528:	b.gt	40470 <__gmpn_sbpi1_divappr_q@@Base+0xd0>
   4052c:	b	405b4 <__gmpn_sbpi1_divappr_q@@Base+0x214>
   40530:	ldur	x8, [x28, #-16]
   40534:	ldur	x9, [x29, #-24]
   40538:	cmp	x8, x9
   4053c:	b.ne	4047c <__gmpn_sbpi1_divappr_q@@Base+0xdc>  // b.any
   40540:	ldr	x2, [sp, #16]
   40544:	add	x0, x20, x25
   40548:	mov	x3, #0xffffffffffffffff    	// #-1
   4054c:	mov	x26, #0xffffffffffffffff    	// #-1
   40550:	bl	cba0 <__gmpn_submul_1@plt>
   40554:	ldur	x15, [x29, #-8]
   40558:	ldur	x21, [x28, #-16]
   4055c:	b	40508 <__gmpn_sbpi1_divappr_q@@Base+0x168>
   40560:	ldur	x8, [x29, #-24]
   40564:	cmp	x19, x8
   40568:	b.cs	40574 <__gmpn_sbpi1_divappr_q@@Base+0x1d4>  // b.hs, b.nlast
   4056c:	cmp	x21, x15
   40570:	b.ls	404dc <__gmpn_sbpi1_divappr_q@@Base+0x13c>  // b.plast
   40574:	ldur	x9, [x29, #-24]
   40578:	subs	x8, x19, x9
   4057c:	sbc	x21, x21, x15
   40580:	add	x26, x26, #0x1
   40584:	mov	x19, x8
   40588:	b	404dc <__gmpn_sbpi1_divappr_q@@Base+0x13c>
   4058c:	ldr	x2, [sp, #24]
   40590:	ldr	x3, [sp]
   40594:	mov	x0, x27
   40598:	mov	x1, x27
   4059c:	bl	cc30 <__gmpn_add_n@plt>
   405a0:	ldur	x15, [x29, #-8]
   405a4:	sub	x26, x26, #0x1
   405a8:	add	x8, x21, x15
   405ac:	add	x21, x8, x0
   405b0:	b	40508 <__gmpn_sbpi1_divappr_q@@Base+0x168>
   405b4:	ldur	x22, [x29, #-24]
   405b8:	ldr	x9, [sp, #16]
   405bc:	add	x8, x23, x25
   405c0:	add	x19, x24, x25
   405c4:	sub	x23, x8, #0x10
   405c8:	cmp	x9, #0x2
   405cc:	b.lt	40678 <__gmpn_sbpi1_divappr_q@@Base+0x2d8>  // b.tstop
   405d0:	cmp	x21, x15
   405d4:	cset	w8, cs  // cs = hs, nlast
   405d8:	cmp	x9, #0x2
   405dc:	b.ne	40680 <__gmpn_sbpi1_divappr_q@@Base+0x2e0>  // b.any
   405e0:	mov	x9, #0xffffffffffffffff    	// #-1
   405e4:	sub	x22, x23, #0x8
   405e8:	str	x9, [sp, #32]
   405ec:	cbnz	w8, 40840 <__gmpn_sbpi1_divappr_q@@Base+0x4a0>
   405f0:	ldp	x16, x10, [x29, #-24]
   405f4:	ldr	x8, [x23]
   405f8:	ldr	x11, [x22]
   405fc:	mul	x9, x21, x10
   40600:	umulh	x10, x21, x10
   40604:	adds	x12, x9, x8
   40608:	adc	x9, x10, x21
   4060c:	msub	x8, x9, x15, x8
   40610:	mul	x10, x9, x16
   40614:	umulh	x13, x16, x9
   40618:	subs	x14, x11, x16
   4061c:	sbc	x8, x8, x15
   40620:	subs	x11, x14, x10
   40624:	sbc	x10, x8, x13
   40628:	cmp	x10, x12
   4062c:	cset	w8, cs  // cs = hs, nlast
   40630:	csetm	x12, cs  // cs = hs, nlast
   40634:	sub	x9, x9, x8
   40638:	and	x13, x16, x12
   4063c:	and	x12, x15, x12
   40640:	adds	x8, x11, x13
   40644:	adc	x21, x10, x12
   40648:	cmp	x21, x15
   4064c:	add	x20, x9, #0x1
   40650:	b.cs	408bc <__gmpn_sbpi1_divappr_q@@Base+0x51c>  // b.hs, b.nlast
   40654:	str	x21, [x23]
   40658:	str	x8, [x22]
   4065c:	b	40880 <__gmpn_sbpi1_divappr_q@@Base+0x4e0>
   40660:	ldr	x8, [sp, #24]
   40664:	sub	x23, x19, #0x10
   40668:	add	x8, x8, x20, lsl #3
   4066c:	mov	x19, x8
   40670:	cmp	x9, #0x2
   40674:	b.ge	405d0 <__gmpn_sbpi1_divappr_q@@Base+0x230>  // b.tcont
   40678:	mov	x22, x23
   4067c:	b	40884 <__gmpn_sbpi1_divappr_q@@Base+0x4e4>
   40680:	mov	x2, x9
   40684:	mov	w9, #0x1                   	// #1
   40688:	sub	x9, x9, x2
   4068c:	add	x9, x23, x9, lsl #3
   40690:	mov	x27, xzr
   40694:	stp	x23, x9, [sp, #16]
   40698:	mov	x9, #0xffffffffffffffff    	// #-1
   4069c:	str	x9, [sp, #32]
   406a0:	b	40700 <__gmpn_sbpi1_divappr_q@@Base+0x360>
   406a4:	ldr	x0, [sp, #24]
   406a8:	mov	x3, #0xffffffffffffffff    	// #-1
   406ac:	mov	x25, #0xffffffffffffffff    	// #-1
   406b0:	mov	x28, x1
   406b4:	mov	x26, x2
   406b8:	bl	cba0 <__gmpn_submul_1@plt>
   406bc:	cmp	x21, x0
   406c0:	b.ne	407ac <__gmpn_sbpi1_divappr_q@@Base+0x40c>  // b.any
   406c4:	ldur	x15, [x29, #-8]
   406c8:	ldr	x21, [x20]
   406cc:	ldr	x9, [sp, #32]
   406d0:	add	x8, x19, x27
   406d4:	mov	x1, x28
   406d8:	mov	x2, x26
   406dc:	and	x9, x9, x15
   406e0:	cmp	x21, x9
   406e4:	add	x1, x28, #0x8
   406e8:	sub	x27, x27, #0x8
   406ec:	stur	x25, [x8, #-8]
   406f0:	cset	w8, cs  // cs = hs, nlast
   406f4:	cmp	x24, #0x1
   406f8:	sub	x2, x26, #0x1
   406fc:	b.le	40830 <__gmpn_sbpi1_divappr_q@@Base+0x490>
   40700:	sub	x24, x2, #0x2
   40704:	add	x20, x23, x27
   40708:	tbnz	w8, #0, 406a4 <__gmpn_sbpi1_divappr_q@@Base+0x304>
   4070c:	ldur	x10, [x29, #-16]
   40710:	ldp	x11, x8, [x20, #-8]
   40714:	mov	x23, x19
   40718:	mov	x26, x2
   4071c:	mul	x9, x21, x10
   40720:	umulh	x10, x21, x10
   40724:	adds	x12, x9, x8
   40728:	adc	x9, x10, x21
   4072c:	msub	x8, x9, x15, x8
   40730:	subs	x14, x11, x22
   40734:	sbc	x8, x8, x15
   40738:	mul	x10, x9, x22
   4073c:	umulh	x13, x22, x9
   40740:	subs	x11, x14, x10
   40744:	sbc	x8, x8, x13
   40748:	cmp	x8, x12
   4074c:	cset	w10, cs  // cs = hs, nlast
   40750:	csetm	x12, cs  // cs = hs, nlast
   40754:	sub	x9, x9, x10
   40758:	and	x10, x22, x12
   4075c:	and	x12, x15, x12
   40760:	adds	x19, x11, x10
   40764:	adc	x21, x8, x12
   40768:	cmp	x21, x15
   4076c:	add	x25, x9, #0x1
   40770:	b.cs	40800 <__gmpn_sbpi1_divappr_q@@Base+0x460>  // b.hs, b.nlast
   40774:	ldr	x0, [sp, #24]
   40778:	mov	x2, x24
   4077c:	mov	x3, x25
   40780:	mov	x28, x1
   40784:	bl	cba0 <__gmpn_submul_1@plt>
   40788:	subs	x8, x19, x0
   4078c:	cset	w9, cc  // cc = lo, ul, last
   40790:	subs	x21, x21, x9
   40794:	stur	x8, [x20, #-8]
   40798:	b.cc	407d8 <__gmpn_sbpi1_divappr_q@@Base+0x438>  // b.lo, b.ul, b.last
   4079c:	ldur	x15, [x29, #-8]
   407a0:	mov	x19, x23
   407a4:	ldr	x23, [sp, #16]
   407a8:	b	406cc <__gmpn_sbpi1_divappr_q@@Base+0x32c>
   407ac:	ldr	x8, [sp, #32]
   407b0:	and	x8, x0, x8
   407b4:	cmp	x21, x8
   407b8:	b.cs	40824 <__gmpn_sbpi1_divappr_q@@Base+0x484>  // b.hs, b.nlast
   407bc:	ldr	x0, [sp, #24]
   407c0:	mov	x2, x28
   407c4:	mov	x3, x26
   407c8:	mov	x1, x0
   407cc:	bl	cc30 <__gmpn_add_n@plt>
   407d0:	mov	x25, #0xfffffffffffffffe    	// #-2
   407d4:	b	406c4 <__gmpn_sbpi1_divappr_q@@Base+0x324>
   407d8:	ldr	x0, [sp, #24]
   407dc:	sub	x3, x26, #0x1
   407e0:	mov	x2, x28
   407e4:	mov	x1, x0
   407e8:	bl	cc30 <__gmpn_add_n@plt>
   407ec:	ldur	x15, [x29, #-8]
   407f0:	sub	x25, x25, #0x1
   407f4:	add	x8, x21, x15
   407f8:	add	x21, x8, x0
   407fc:	b	407a0 <__gmpn_sbpi1_divappr_q@@Base+0x400>
   40800:	cmp	x19, x22
   40804:	b.cs	40810 <__gmpn_sbpi1_divappr_q@@Base+0x470>  // b.hs, b.nlast
   40808:	cmp	x21, x15
   4080c:	b.ls	40774 <__gmpn_sbpi1_divappr_q@@Base+0x3d4>  // b.plast
   40810:	subs	x8, x19, x22
   40814:	sbc	x21, x21, x15
   40818:	add	x25, x25, #0x1
   4081c:	mov	x19, x8
   40820:	b	40774 <__gmpn_sbpi1_divappr_q@@Base+0x3d4>
   40824:	str	xzr, [sp, #32]
   40828:	mov	x25, #0xffffffffffffffff    	// #-1
   4082c:	b	406c4 <__gmpn_sbpi1_divappr_q@@Base+0x324>
   40830:	add	x23, x23, x27
   40834:	sub	x22, x23, #0x8
   40838:	add	x19, x19, x27
   4083c:	cbz	w8, 405f0 <__gmpn_sbpi1_divappr_q@@Base+0x250>
   40840:	mov	w2, #0x2                   	// #2
   40844:	mov	x3, #0xffffffffffffffff    	// #-1
   40848:	mov	x0, x22
   4084c:	mov	x24, x23
   40850:	mov	x23, x19
   40854:	mov	x20, #0xffffffffffffffff    	// #-1
   40858:	mov	x19, x1
   4085c:	bl	cba0 <__gmpn_submul_1@plt>
   40860:	cmp	x21, x0
   40864:	b.eq	40878 <__gmpn_sbpi1_divappr_q@@Base+0x4d8>  // b.none
   40868:	ldr	x8, [sp, #32]
   4086c:	and	x8, x0, x8
   40870:	cmp	x21, x8
   40874:	b.cc	408e4 <__gmpn_sbpi1_divappr_q@@Base+0x544>  // b.lo, b.ul, b.last
   40878:	ldr	x21, [x24]
   4087c:	mov	x19, x23
   40880:	stur	x20, [x19, #-8]
   40884:	ldr	x8, [x22, #8]
   40888:	cmp	x8, x21
   4088c:	b.ne	4090c <__gmpn_sbpi1_divappr_q@@Base+0x56c>  // b.any
   40890:	ldr	w8, [sp, #12]
   40894:	ldp	x20, x19, [sp, #144]
   40898:	ldp	x22, x21, [sp, #128]
   4089c:	ldp	x24, x23, [sp, #112]
   408a0:	ldp	x26, x25, [sp, #96]
   408a4:	ldp	x28, x27, [sp, #80]
   408a8:	ldp	x29, x30, [sp, #64]
   408ac:	mvn	w8, w8
   408b0:	lsr	w0, w8, #31
   408b4:	add	sp, sp, #0xa0
   408b8:	ret
   408bc:	cmp	x8, x16
   408c0:	b.cs	408cc <__gmpn_sbpi1_divappr_q@@Base+0x52c>  // b.hs, b.nlast
   408c4:	cmp	x21, x15
   408c8:	b.ls	40654 <__gmpn_sbpi1_divappr_q@@Base+0x2b4>  // b.plast
   408cc:	ldur	x10, [x29, #-24]
   408d0:	subs	x9, x8, x10
   408d4:	sbc	x21, x21, x15
   408d8:	add	x20, x20, #0x1
   408dc:	mov	x8, x9
   408e0:	b	40654 <__gmpn_sbpi1_divappr_q@@Base+0x2b4>
   408e4:	ldr	x8, [x24]
   408e8:	mov	x20, #0xfffffffffffffffe    	// #-2
   408ec:	mov	x13, x24
   408f0:	ldp	x9, x10, [x19]
   408f4:	ldr	x11, [x22]
   408f8:	adds	x12, x11, x9
   408fc:	adc	x8, x8, x10
   40900:	str	x8, [x24]
   40904:	str	x12, [x22]
   40908:	b	40878 <__gmpn_sbpi1_divappr_q@@Base+0x4d8>
   4090c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   40910:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   40914:	add	x0, x0, #0x201
   40918:	add	x2, x2, #0x1ed
   4091c:	mov	w1, #0xc3                  	// #195
   40920:	bl	c850 <__gmp_assert_fail@plt>

0000000000040924 <__gmpn_dcpi1_div_q@@Base>:
   40924:	stp	x29, x30, [sp, #-96]!
   40928:	stp	x28, x27, [sp, #16]
   4092c:	stp	x26, x25, [sp, #32]
   40930:	stp	x24, x23, [sp, #48]
   40934:	stp	x22, x21, [sp, #64]
   40938:	stp	x20, x19, [sp, #80]
   4093c:	mov	x29, sp
   40940:	sub	sp, sp, #0x10
   40944:	add	x27, x2, #0x1
   40948:	mov	x21, x1
   4094c:	lsl	x1, x27, #3
   40950:	mov	w8, #0x7f00                	// #32512
   40954:	mov	x26, x5
   40958:	mov	x22, x4
   4095c:	mov	x23, x3
   40960:	mov	x20, x2
   40964:	mov	x19, x0
   40968:	cmp	x1, x8
   4096c:	stur	xzr, [x29, #-8]
   40970:	b.hi	40ad4 <__gmpn_dcpi1_div_q@@Base+0x1b0>  // b.pmore
   40974:	add	x9, x1, #0xf
   40978:	mov	x8, sp
   4097c:	and	x9, x9, #0xfffffffffffffff0
   40980:	sub	x25, x8, x9
   40984:	mov	sp, x25
   40988:	add	x0, x25, #0x8
   4098c:	mov	x1, x21
   40990:	mov	x2, x20
   40994:	bl	cc10 <__gmpn_copyi@plt>
   40998:	sub	x24, x20, x22
   4099c:	lsl	x8, x24, #3
   409a0:	add	x1, x8, #0x8
   409a4:	mov	w8, #0x7f00                	// #32512
   409a8:	cmp	x1, x8
   409ac:	str	xzr, [x25]
   409b0:	b.hi	40ae4 <__gmpn_dcpi1_div_q@@Base+0x1c0>  // b.pmore
   409b4:	add	x9, x1, #0xf
   409b8:	mov	x8, sp
   409bc:	and	x9, x9, #0xfffffffffffffff0
   409c0:	sub	x28, x8, x9
   409c4:	mov	sp, x28
   409c8:	mov	x0, x28
   409cc:	mov	x1, x25
   409d0:	mov	x2, x27
   409d4:	mov	x3, x23
   409d8:	mov	x4, x22
   409dc:	mov	x5, x26
   409e0:	bl	c640 <__gmpn_dcpi1_divappr_q@plt>
   409e4:	ldr	x8, [x28]
   409e8:	mov	x26, x0
   409ec:	cbz	x8, 40a2c <__gmpn_dcpi1_div_q@@Base+0x108>
   409f0:	add	x1, x28, #0x8
   409f4:	mov	x0, x19
   409f8:	mov	x2, x24
   409fc:	bl	cc10 <__gmpn_copyi@plt>
   40a00:	ldur	x0, [x29, #-8]
   40a04:	cbnz	x0, 40ac0 <__gmpn_dcpi1_div_q@@Base+0x19c>
   40a08:	mov	x0, x26
   40a0c:	mov	sp, x29
   40a10:	ldp	x20, x19, [sp, #80]
   40a14:	ldp	x22, x21, [sp, #64]
   40a18:	ldp	x24, x23, [sp, #48]
   40a1c:	ldp	x26, x25, [sp, #32]
   40a20:	ldp	x28, x27, [sp, #16]
   40a24:	ldp	x29, x30, [sp], #96
   40a28:	ret
   40a2c:	cmp	x24, x22
   40a30:	add	x27, x28, #0x8
   40a34:	mov	x0, x25
   40a38:	b.le	40a58 <__gmpn_dcpi1_div_q@@Base+0x134>
   40a3c:	mov	x1, x27
   40a40:	mov	x2, x24
   40a44:	mov	x3, x23
   40a48:	mov	x4, x22
   40a4c:	bl	cea0 <__gmpn_mul@plt>
   40a50:	cbnz	x26, 40a70 <__gmpn_dcpi1_div_q@@Base+0x14c>
   40a54:	b	40a88 <__gmpn_dcpi1_div_q@@Base+0x164>
   40a58:	mov	x1, x23
   40a5c:	mov	x2, x22
   40a60:	mov	x3, x27
   40a64:	mov	x4, x24
   40a68:	bl	cea0 <__gmpn_mul@plt>
   40a6c:	cbz	x26, 40a88 <__gmpn_dcpi1_div_q@@Base+0x164>
   40a70:	add	x0, x25, x24, lsl #3
   40a74:	mov	x1, x0
   40a78:	mov	x2, x23
   40a7c:	mov	x3, x22
   40a80:	bl	cc30 <__gmpn_add_n@plt>
   40a84:	cbnz	x0, 40aa0 <__gmpn_dcpi1_div_q@@Base+0x17c>
   40a88:	mov	x0, x25
   40a8c:	mov	x1, x21
   40a90:	mov	x2, x20
   40a94:	bl	c570 <__gmpn_cmp@plt>
   40a98:	cmp	w0, #0x1
   40a9c:	b.lt	40ac8 <__gmpn_dcpi1_div_q@@Base+0x1a4>  // b.tstop
   40aa0:	mov	w3, #0x1                   	// #1
   40aa4:	mov	x0, x19
   40aa8:	mov	x1, x27
   40aac:	mov	x2, x24
   40ab0:	bl	caf0 <__gmpn_sub_1@plt>
   40ab4:	sub	x26, x26, x0
   40ab8:	ldur	x0, [x29, #-8]
   40abc:	cbz	x0, 40a08 <__gmpn_dcpi1_div_q@@Base+0xe4>
   40ac0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   40ac4:	b	40a08 <__gmpn_dcpi1_div_q@@Base+0xe4>
   40ac8:	mov	x0, x19
   40acc:	mov	x1, x27
   40ad0:	b	409f8 <__gmpn_dcpi1_div_q@@Base+0xd4>
   40ad4:	sub	x0, x29, #0x8
   40ad8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   40adc:	mov	x25, x0
   40ae0:	b	40988 <__gmpn_dcpi1_div_q@@Base+0x64>
   40ae4:	sub	x0, x29, #0x8
   40ae8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   40aec:	mov	x28, x0
   40af0:	b	409c8 <__gmpn_dcpi1_div_q@@Base+0xa4>

0000000000040af4 <__gmpn_dcpi1_div_qr_n@@Base>:
   40af4:	sub	sp, sp, #0x80
   40af8:	stp	x24, x23, [sp, #80]
   40afc:	asr	x23, x3, #1
   40b00:	stp	x26, x25, [sp, #64]
   40b04:	sub	x25, x3, x23
   40b08:	and	x8, x3, #0xfffffffffffffffe
   40b0c:	stp	x28, x27, [sp, #48]
   40b10:	stp	x22, x21, [sp, #96]
   40b14:	stp	x20, x19, [sp, #112]
   40b18:	mov	x26, x5
   40b1c:	mov	x19, x3
   40b20:	mov	x20, x2
   40b24:	mov	x21, x1
   40b28:	mov	x22, x0
   40b2c:	add	x27, x0, x23, lsl #3
   40b30:	cmp	x25, #0x29
   40b34:	add	x1, x1, x8, lsl #3
   40b38:	stp	x29, x30, [sp, #32]
   40b3c:	add	x29, sp, #0x20
   40b40:	stp	x8, x4, [sp, #8]
   40b44:	b.le	40b60 <__gmpn_dcpi1_div_qr_n@@Base+0x6c>
   40b48:	add	x2, x20, x23, lsl #3
   40b4c:	mov	x0, x27
   40b50:	mov	x3, x25
   40b54:	mov	x5, x26
   40b58:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   40b5c:	b	40b78 <__gmpn_dcpi1_div_qr_n@@Base+0x84>
   40b60:	ldr	x5, [x4]
   40b64:	lsl	x2, x25, #1
   40b68:	add	x3, x20, x23, lsl #3
   40b6c:	mov	x0, x27
   40b70:	mov	x4, x25
   40b74:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   40b78:	mov	x24, x0
   40b7c:	mov	x0, x26
   40b80:	mov	x1, x27
   40b84:	mov	x2, x25
   40b88:	mov	x3, x20
   40b8c:	mov	x4, x23
   40b90:	bl	cea0 <__gmpn_mul@plt>
   40b94:	add	x28, x21, x23, lsl #3
   40b98:	mov	x0, x28
   40b9c:	mov	x1, x28
   40ba0:	mov	x2, x26
   40ba4:	mov	x3, x19
   40ba8:	stur	x26, [x29, #-8]
   40bac:	bl	c420 <__gmpn_sub_n@plt>
   40bb0:	mov	x26, x0
   40bb4:	cbz	x24, 40bd0 <__gmpn_dcpi1_div_qr_n@@Base+0xdc>
   40bb8:	add	x0, x21, x19, lsl #3
   40bbc:	mov	x1, x0
   40bc0:	mov	x2, x20
   40bc4:	mov	x3, x23
   40bc8:	bl	c420 <__gmpn_sub_n@plt>
   40bcc:	add	x26, x0, x26
   40bd0:	cbz	x26, 40c08 <__gmpn_dcpi1_div_qr_n@@Base+0x114>
   40bd4:	mov	w3, #0x1                   	// #1
   40bd8:	mov	x0, x27
   40bdc:	mov	x1, x27
   40be0:	mov	x2, x25
   40be4:	bl	caf0 <__gmpn_sub_1@plt>
   40be8:	sub	x24, x24, x0
   40bec:	mov	x0, x28
   40bf0:	mov	x1, x28
   40bf4:	mov	x2, x20
   40bf8:	mov	x3, x19
   40bfc:	bl	cc30 <__gmpn_add_n@plt>
   40c00:	subs	x26, x26, x0
   40c04:	b.ne	40bd4 <__gmpn_dcpi1_div_qr_n@@Base+0xe0>  // b.any
   40c08:	lsl	x8, x25, #3
   40c0c:	cmp	x19, #0x53
   40c10:	add	x1, x21, x8
   40c14:	add	x3, x20, x8
   40c18:	b.le	40c3c <__gmpn_dcpi1_div_qr_n@@Base+0x148>
   40c1c:	ldur	x26, [x29, #-8]
   40c20:	ldr	x4, [sp, #16]
   40c24:	mov	x0, x22
   40c28:	mov	x2, x3
   40c2c:	mov	x3, x23
   40c30:	mov	x5, x26
   40c34:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   40c38:	b	40c54 <__gmpn_dcpi1_div_qr_n@@Base+0x160>
   40c3c:	ldp	x2, x8, [sp, #8]
   40c40:	mov	x0, x22
   40c44:	mov	x4, x23
   40c48:	ldr	x5, [x8]
   40c4c:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   40c50:	ldur	x26, [x29, #-8]
   40c54:	mov	x27, x0
   40c58:	mov	x0, x26
   40c5c:	mov	x1, x20
   40c60:	mov	x2, x25
   40c64:	mov	x3, x22
   40c68:	mov	x4, x23
   40c6c:	bl	cea0 <__gmpn_mul@plt>
   40c70:	mov	x0, x21
   40c74:	mov	x1, x21
   40c78:	mov	x2, x26
   40c7c:	mov	x3, x19
   40c80:	bl	c420 <__gmpn_sub_n@plt>
   40c84:	mov	x26, x0
   40c88:	cbz	x27, 40ca4 <__gmpn_dcpi1_div_qr_n@@Base+0x1b0>
   40c8c:	mov	x0, x28
   40c90:	mov	x1, x28
   40c94:	mov	x2, x20
   40c98:	mov	x3, x25
   40c9c:	bl	c420 <__gmpn_sub_n@plt>
   40ca0:	add	x26, x0, x26
   40ca4:	cbz	x26, 40cd8 <__gmpn_dcpi1_div_qr_n@@Base+0x1e4>
   40ca8:	mov	w3, #0x1                   	// #1
   40cac:	mov	x0, x22
   40cb0:	mov	x1, x22
   40cb4:	mov	x2, x23
   40cb8:	bl	caf0 <__gmpn_sub_1@plt>
   40cbc:	mov	x0, x21
   40cc0:	mov	x1, x21
   40cc4:	mov	x2, x20
   40cc8:	mov	x3, x19
   40ccc:	bl	cc30 <__gmpn_add_n@plt>
   40cd0:	subs	x26, x26, x0
   40cd4:	b.ne	40ca8 <__gmpn_dcpi1_div_qr_n@@Base+0x1b4>  // b.any
   40cd8:	mov	x0, x24
   40cdc:	ldp	x20, x19, [sp, #112]
   40ce0:	ldp	x22, x21, [sp, #96]
   40ce4:	ldp	x24, x23, [sp, #80]
   40ce8:	ldp	x26, x25, [sp, #64]
   40cec:	ldp	x28, x27, [sp, #48]
   40cf0:	ldp	x29, x30, [sp, #32]
   40cf4:	add	sp, sp, #0x80
   40cf8:	ret

0000000000040cfc <__gmpn_dcpi1_div_qr@@Base>:
   40cfc:	stp	x29, x30, [sp, #-96]!
   40d00:	stp	x28, x27, [sp, #16]
   40d04:	stp	x26, x25, [sp, #32]
   40d08:	stp	x24, x23, [sp, #48]
   40d0c:	stp	x22, x21, [sp, #64]
   40d10:	stp	x20, x19, [sp, #80]
   40d14:	mov	x29, sp
   40d18:	sub	sp, sp, #0x40
   40d1c:	lsl	x25, x4, #3
   40d20:	mov	w8, #0x7f00                	// #32512
   40d24:	mov	x24, x5
   40d28:	mov	x19, x4
   40d2c:	mov	x20, x3
   40d30:	mov	x26, x2
   40d34:	cmp	x25, x8
   40d38:	stp	x1, x0, [x29, #-24]
   40d3c:	stur	xzr, [x29, #-8]
   40d40:	b.hi	411a4 <__gmpn_dcpi1_div_qr@@Base+0x4a8>  // b.pmore
   40d44:	add	x9, x25, #0xf
   40d48:	mov	x8, sp
   40d4c:	and	x9, x9, #0xfffffffffffffff0
   40d50:	sub	x27, x8, x9
   40d54:	mov	sp, x27
   40d58:	sub	x28, x26, x19
   40d5c:	cmp	x28, x19
   40d60:	add	x22, x20, x19, lsl #3
   40d64:	b.le	40ee8 <__gmpn_dcpi1_div_qr@@Base+0x1ec>
   40d68:	lsl	x8, x26, #1
   40d6c:	lsl	x9, x19, #1
   40d70:	lsl	x11, x26, #3
   40d74:	lsl	x10, x19, #4
   40d78:	stp	x27, x24, [x29, #-40]
   40d7c:	mov	x24, xzr
   40d80:	mov	x23, xzr
   40d84:	sub	x2, x8, x9
   40d88:	sub	x8, x11, x10
   40d8c:	sub	x24, x24, x19
   40d90:	add	x27, x28, x24
   40d94:	add	x23, x23, x25
   40d98:	sub	x2, x2, x9
   40d9c:	cmp	x27, x19
   40da0:	sub	x8, x8, x10
   40da4:	b.gt	40d8c <__gmpn_dcpi1_div_qr@@Base+0x90>
   40da8:	ldur	x10, [x29, #-16]
   40dac:	sub	x28, x23, x25
   40db0:	cmp	x27, #0x2
   40db4:	sub	x21, x9, x26
   40db8:	add	x0, x10, x23
   40dbc:	b.eq	40f6c <__gmpn_dcpi1_div_qr@@Base+0x270>  // b.none
   40dc0:	cmp	x27, #0x1
   40dc4:	b.ne	40f94 <__gmpn_dcpi1_div_qr@@Base+0x298>  // b.any
   40dc8:	ldur	x9, [x29, #-24]
   40dcc:	mov	x1, x20
   40dd0:	mov	x2, x19
   40dd4:	add	x21, x9, x23
   40dd8:	add	x8, x9, x25
   40ddc:	add	x27, x21, #0x8
   40de0:	add	x8, x8, x23
   40de4:	mov	x0, x27
   40de8:	stur	x8, [x29, #-48]
   40dec:	bl	c570 <__gmpn_cmp@plt>
   40df0:	mvn	w8, w0
   40df4:	lsr	w26, w8, #31
   40df8:	tbnz	w0, #31, 40e10 <__gmpn_dcpi1_div_qr@@Base+0x114>
   40dfc:	mov	x0, x27
   40e00:	mov	x1, x27
   40e04:	mov	x2, x20
   40e08:	mov	x3, x19
   40e0c:	bl	c420 <__gmpn_sub_n@plt>
   40e10:	ldur	x8, [x29, #-48]
   40e14:	ldr	x9, [x8]
   40e18:	ldur	x8, [x29, #-24]
   40e1c:	add	x8, x8, x19, lsl #3
   40e20:	add	x16, x8, x23
   40e24:	ldp	x8, x27, [x22, #-16]
   40e28:	ldur	x10, [x16, #-8]
   40e2c:	cmp	x9, x27
   40e30:	b.ne	40e3c <__gmpn_dcpi1_div_qr@@Base+0x140>  // b.any
   40e34:	cmp	x10, x8
   40e38:	b.eq	411e4 <__gmpn_dcpi1_div_qr@@Base+0x4e8>  // b.none
   40e3c:	ldur	x11, [x29, #-32]
   40e40:	ldur	x12, [x16, #-16]
   40e44:	ldr	x11, [x11]
   40e48:	mul	x13, x11, x9
   40e4c:	umulh	x11, x9, x11
   40e50:	adds	x14, x13, x10
   40e54:	adc	x9, x11, x9
   40e58:	msub	x10, x9, x27, x10
   40e5c:	subs	x15, x12, x8
   40e60:	sbc	x10, x10, x27
   40e64:	mul	x11, x9, x8
   40e68:	umulh	x13, x8, x9
   40e6c:	subs	x12, x15, x11
   40e70:	sbc	x10, x10, x13
   40e74:	cmp	x10, x14
   40e78:	cset	w11, cs  // cs = hs, nlast
   40e7c:	csetm	x13, cs  // cs = hs, nlast
   40e80:	sub	x9, x9, x11
   40e84:	and	x11, x8, x13
   40e88:	and	x13, x27, x13
   40e8c:	adds	x14, x12, x11
   40e90:	adc	x22, x10, x13
   40e94:	cmp	x22, x27
   40e98:	add	x3, x9, #0x1
   40e9c:	b.cs	411c0 <__gmpn_dcpi1_div_qr@@Base+0x4c4>  // b.hs, b.nlast
   40ea0:	cmp	x19, #0x3
   40ea4:	b.lt	4112c <__gmpn_dcpi1_div_qr@@Base+0x430>  // b.tstop
   40ea8:	sub	x2, x19, #0x2
   40eac:	mov	x0, x21
   40eb0:	mov	x1, x20
   40eb4:	stp	x3, x16, [x29, #-56]
   40eb8:	stur	x21, [x29, #-64]
   40ebc:	mov	x21, x14
   40ec0:	bl	cba0 <__gmpn_submul_1@plt>
   40ec4:	ldur	x16, [x29, #-48]
   40ec8:	subs	x8, x21, x0
   40ecc:	cset	w9, cc  // cc = lo, ul, last
   40ed0:	subs	x22, x22, x9
   40ed4:	stur	x8, [x16, #-16]
   40ed8:	b.cc	4120c <__gmpn_dcpi1_div_qr@@Base+0x510>  // b.lo, b.ul, b.last
   40edc:	ldur	x3, [x29, #-56]
   40ee0:	mov	x21, x20
   40ee4:	b	41134 <__gmpn_dcpi1_div_qr@@Base+0x438>
   40ee8:	ldur	x8, [x29, #-24]
   40eec:	lsl	x10, x28, #3
   40ef0:	cmp	x28, #0x29
   40ef4:	add	x9, x8, x26, lsl #3
   40ef8:	sub	x23, x9, x10
   40efc:	neg	x8, x28
   40f00:	sub	x1, x23, x10
   40f04:	b.le	40f24 <__gmpn_dcpi1_div_qr@@Base+0x228>
   40f08:	ldur	x0, [x29, #-16]
   40f0c:	add	x2, x22, x8, lsl #3
   40f10:	mov	x3, x28
   40f14:	mov	x4, x24
   40f18:	mov	x5, x27
   40f1c:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   40f20:	b	40f3c <__gmpn_dcpi1_div_qr@@Base+0x240>
   40f24:	ldr	x5, [x24]
   40f28:	ldur	x0, [x29, #-16]
   40f2c:	lsl	x2, x28, #1
   40f30:	add	x3, x22, x8, lsl #3
   40f34:	mov	x4, x28
   40f38:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   40f3c:	mov	x26, x0
   40f40:	cmp	x28, x19
   40f44:	b.eq	41178 <__gmpn_dcpi1_div_qr@@Base+0x47c>  // b.none
   40f48:	sub	x24, x19, x28
   40f4c:	mov	x0, x27
   40f50:	cmp	x28, x24
   40f54:	b.le	40fc0 <__gmpn_dcpi1_div_qr@@Base+0x2c4>
   40f58:	ldur	x1, [x29, #-16]
   40f5c:	mov	x2, x28
   40f60:	mov	x3, x20
   40f64:	mov	x4, x24
   40f68:	b	40fd0 <__gmpn_dcpi1_div_qr@@Base+0x2d4>
   40f6c:	ldur	x8, [x29, #-24]
   40f70:	sub	x4, x22, #0x10
   40f74:	mov	w3, #0x4                   	// #4
   40f78:	mov	x1, xzr
   40f7c:	add	x8, x8, x19, lsl #3
   40f80:	add	x8, x8, x23
   40f84:	sub	x2, x8, #0x10
   40f88:	stur	x0, [x29, #-48]
   40f8c:	bl	c350 <__gmpn_divrem_2@plt>
   40f90:	b	41064 <__gmpn_dcpi1_div_qr@@Base+0x368>
   40f94:	ldur	x9, [x29, #-24]
   40f98:	cmp	x27, #0x29
   40f9c:	stur	x0, [x29, #-48]
   40fa0:	sub	x1, x9, x8
   40fa4:	b.le	4104c <__gmpn_dcpi1_div_qr@@Base+0x350>
   40fa8:	ldp	x5, x4, [x29, #-40]
   40fac:	add	x8, x20, x21, lsl #3
   40fb0:	add	x2, x8, x23
   40fb4:	mov	x3, x27
   40fb8:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   40fbc:	b	41064 <__gmpn_dcpi1_div_qr@@Base+0x368>
   40fc0:	ldur	x3, [x29, #-16]
   40fc4:	mov	x1, x20
   40fc8:	mov	x2, x24
   40fcc:	mov	x4, x28
   40fd0:	bl	cea0 <__gmpn_mul@plt>
   40fd4:	sub	x23, x23, x19, lsl #3
   40fd8:	mov	x0, x23
   40fdc:	mov	x1, x23
   40fe0:	mov	x2, x27
   40fe4:	mov	x3, x19
   40fe8:	bl	c420 <__gmpn_sub_n@plt>
   40fec:	mov	x22, x0
   40ff0:	cbz	x26, 4100c <__gmpn_dcpi1_div_qr@@Base+0x310>
   40ff4:	add	x0, x23, x28, lsl #3
   40ff8:	mov	x1, x0
   40ffc:	mov	x2, x20
   41000:	mov	x3, x24
   41004:	bl	c420 <__gmpn_sub_n@plt>
   41008:	add	x22, x0, x22
   4100c:	cbz	x22, 41178 <__gmpn_dcpi1_div_qr@@Base+0x47c>
   41010:	ldur	x21, [x29, #-16]
   41014:	mov	w3, #0x1                   	// #1
   41018:	mov	x0, x21
   4101c:	mov	x1, x21
   41020:	mov	x2, x28
   41024:	bl	caf0 <__gmpn_sub_1@plt>
   41028:	sub	x26, x26, x0
   4102c:	mov	x0, x23
   41030:	mov	x1, x23
   41034:	mov	x2, x20
   41038:	mov	x3, x19
   4103c:	bl	cc30 <__gmpn_add_n@plt>
   41040:	subs	x22, x22, x0
   41044:	b.ne	41014 <__gmpn_dcpi1_div_qr@@Base+0x318>  // b.any
   41048:	b	41178 <__gmpn_dcpi1_div_qr@@Base+0x47c>
   4104c:	ldur	x8, [x29, #-32]
   41050:	mov	x4, x27
   41054:	ldr	x5, [x8]
   41058:	add	x8, x20, x21, lsl #3
   4105c:	add	x3, x8, x23
   41060:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   41064:	mov	x26, x0
   41068:	subs	x21, x21, x24
   4106c:	b.ne	41078 <__gmpn_dcpi1_div_qr@@Base+0x37c>  // b.any
   41070:	mov	x21, x20
   41074:	b	41140 <__gmpn_dcpi1_div_qr@@Base+0x444>
   41078:	cmp	x27, x21
   4107c:	b.le	41094 <__gmpn_dcpi1_div_qr@@Base+0x398>
   41080:	ldp	x1, x22, [x29, #-48]
   41084:	mov	x2, x27
   41088:	mov	x3, x20
   4108c:	mov	x4, x21
   41090:	b	410a4 <__gmpn_dcpi1_div_qr@@Base+0x3a8>
   41094:	ldp	x3, x22, [x29, #-48]
   41098:	mov	x1, x20
   4109c:	mov	x2, x21
   410a0:	mov	x4, x27
   410a4:	mov	x0, x22
   410a8:	bl	cea0 <__gmpn_mul@plt>
   410ac:	ldur	x8, [x29, #-24]
   410b0:	mov	x2, x22
   410b4:	mov	x3, x19
   410b8:	add	x23, x8, x23
   410bc:	mov	x0, x23
   410c0:	mov	x1, x23
   410c4:	bl	c420 <__gmpn_sub_n@plt>
   410c8:	mov	x22, x0
   410cc:	cbz	x26, 410e8 <__gmpn_dcpi1_div_qr@@Base+0x3ec>
   410d0:	add	x0, x23, x27, lsl #3
   410d4:	mov	x1, x0
   410d8:	mov	x2, x20
   410dc:	mov	x3, x21
   410e0:	bl	c420 <__gmpn_sub_n@plt>
   410e4:	add	x22, x0, x22
   410e8:	mov	x21, x20
   410ec:	cbz	x22, 41140 <__gmpn_dcpi1_div_qr@@Base+0x444>
   410f0:	ldur	x20, [x29, #-48]
   410f4:	mov	w3, #0x1                   	// #1
   410f8:	mov	x0, x20
   410fc:	mov	x1, x20
   41100:	mov	x2, x27
   41104:	bl	caf0 <__gmpn_sub_1@plt>
   41108:	sub	x26, x26, x0
   4110c:	mov	x0, x23
   41110:	mov	x1, x23
   41114:	mov	x2, x21
   41118:	mov	x3, x19
   4111c:	bl	cc30 <__gmpn_add_n@plt>
   41120:	subs	x22, x22, x0
   41124:	b.ne	410f4 <__gmpn_dcpi1_div_qr@@Base+0x3f8>  // b.any
   41128:	b	41140 <__gmpn_dcpi1_div_qr@@Base+0x444>
   4112c:	mov	x21, x20
   41130:	stur	x14, [x16, #-16]
   41134:	ldur	x8, [x29, #-16]
   41138:	stur	x22, [x16, #-8]
   4113c:	str	x3, [x8, x23]
   41140:	neg	x22, x24
   41144:	ldp	x23, x24, [x29, #-40]
   41148:	ldp	x27, x20, [x29, #-24]
   4114c:	add	x0, x20, x28
   41150:	add	x1, x27, x28
   41154:	mov	x2, x21
   41158:	mov	x3, x19
   4115c:	mov	x4, x24
   41160:	mov	x5, x23
   41164:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   41168:	sub	x22, x22, x19
   4116c:	cmp	x22, #0x0
   41170:	sub	x28, x28, x25
   41174:	b.gt	4114c <__gmpn_dcpi1_div_qr@@Base+0x450>
   41178:	ldur	x0, [x29, #-8]
   4117c:	cbnz	x0, 411b8 <__gmpn_dcpi1_div_qr@@Base+0x4bc>
   41180:	mov	x0, x26
   41184:	mov	sp, x29
   41188:	ldp	x20, x19, [sp, #80]
   4118c:	ldp	x22, x21, [sp, #64]
   41190:	ldp	x24, x23, [sp, #48]
   41194:	ldp	x26, x25, [sp, #32]
   41198:	ldp	x28, x27, [sp, #16]
   4119c:	ldp	x29, x30, [sp], #96
   411a0:	ret
   411a4:	sub	x0, x29, #0x8
   411a8:	mov	x1, x25
   411ac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   411b0:	mov	x27, x0
   411b4:	b	40d58 <__gmpn_dcpi1_div_qr@@Base+0x5c>
   411b8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   411bc:	b	41180 <__gmpn_dcpi1_div_qr@@Base+0x484>
   411c0:	cmp	x14, x8
   411c4:	b.cs	411d0 <__gmpn_dcpi1_div_qr@@Base+0x4d4>  // b.hs, b.nlast
   411c8:	cmp	x22, x27
   411cc:	b.ls	40ea0 <__gmpn_dcpi1_div_qr@@Base+0x1a4>  // b.plast
   411d0:	subs	x9, x14, x8
   411d4:	sbc	x22, x22, x27
   411d8:	add	x3, x3, #0x1
   411dc:	mov	x14, x9
   411e0:	b	40ea0 <__gmpn_dcpi1_div_qr@@Base+0x1a4>
   411e4:	mov	x3, #0xffffffffffffffff    	// #-1
   411e8:	mov	x0, x21
   411ec:	mov	x1, x20
   411f0:	mov	x2, x19
   411f4:	mov	x22, #0xffffffffffffffff    	// #-1
   411f8:	bl	cba0 <__gmpn_submul_1@plt>
   411fc:	ldur	x8, [x29, #-16]
   41200:	mov	x21, x20
   41204:	str	x22, [x8, x23]
   41208:	b	41140 <__gmpn_dcpi1_div_qr@@Base+0x444>
   4120c:	ldur	x0, [x29, #-64]
   41210:	sub	x3, x19, #0x1
   41214:	mov	x2, x20
   41218:	mov	x21, x20
   4121c:	mov	x1, x0
   41220:	bl	cc30 <__gmpn_add_n@plt>
   41224:	ldp	x3, x16, [x29, #-56]
   41228:	add	x8, x22, x27
   4122c:	add	x22, x8, x0
   41230:	cmp	x3, #0x0
   41234:	cset	w8, eq  // eq = none
   41238:	sub	x26, x26, x8
   4123c:	sub	x3, x3, #0x1
   41240:	b	41134 <__gmpn_dcpi1_div_qr@@Base+0x438>

0000000000041244 <__gmpn_dcpi1_divappr_q@@Base>:
   41244:	stp	x29, x30, [sp, #-96]!
   41248:	stp	x28, x27, [sp, #16]
   4124c:	stp	x26, x25, [sp, #32]
   41250:	stp	x24, x23, [sp, #48]
   41254:	stp	x22, x21, [sp, #64]
   41258:	stp	x20, x19, [sp, #80]
   4125c:	mov	x29, sp
   41260:	sub	sp, sp, #0x40
   41264:	sub	x26, x2, x4
   41268:	mov	x27, x2
   4126c:	mov	x22, x0
   41270:	cmp	x26, x4
   41274:	add	x28, x3, x4, lsl #3
   41278:	stur	x5, [x29, #-8]
   4127c:	b.ge	412e0 <__gmpn_dcpi1_divappr_q@@Base+0x9c>  // b.tcont
   41280:	add	x4, x26, #0x1
   41284:	lsl	x10, x4, #3
   41288:	add	x10, x10, #0xf
   4128c:	add	x9, x1, x27, lsl #3
   41290:	mov	x11, sp
   41294:	and	x10, x10, #0xfffffffffffffff0
   41298:	neg	x8, x26
   4129c:	sub	x9, x9, x26, lsl #3
   412a0:	sub	x20, x11, x10
   412a4:	mov	sp, x20
   412a8:	cmp	x26, #0x97
   412ac:	b.le	4146c <__gmpn_dcpi1_divappr_q@@Base+0x228>
   412b0:	mov	x11, sp
   412b4:	sub	x5, x11, x10
   412b8:	mov	sp, x5
   412bc:	mov	x3, x4
   412c0:	ldur	x4, [x29, #-8]
   412c4:	add	x8, x9, x8, lsl #3
   412c8:	mvn	x9, x26
   412cc:	sub	x1, x8, #0x10
   412d0:	add	x2, x28, x9, lsl #3
   412d4:	mov	x0, x20
   412d8:	bl	41768 <__gmpn_dcpi1_divappr_q@@Base+0x524>
   412dc:	b	41490 <__gmpn_dcpi1_divappr_q@@Base+0x24c>
   412e0:	lsl	x8, x27, #3
   412e4:	lsl	x9, x4, #4
   412e8:	sub	x8, x8, x9
   412ec:	mov	x20, x4
   412f0:	mov	x21, x3
   412f4:	mov	x23, xzr
   412f8:	mov	x24, xzr
   412fc:	lsl	x19, x4, #3
   41300:	add	x8, x8, #0x10
   41304:	add	x10, x26, #0x1
   41308:	sub	x23, x23, x20
   4130c:	add	x11, x10, x23
   41310:	add	x24, x24, x19
   41314:	cmp	x11, x20
   41318:	sub	x8, x8, x9
   4131c:	b.gt	41308 <__gmpn_dcpi1_divappr_q@@Base+0xc4>
   41320:	add	x9, x1, x19
   41324:	add	x9, x9, x24
   41328:	add	x10, x19, #0xf
   4132c:	sub	x11, x22, #0x8
   41330:	stp	x9, x1, [x29, #-32]
   41334:	mov	x9, sp
   41338:	and	x10, x10, #0xfffffffffffffff0
   4133c:	add	x22, x11, x24
   41340:	sub	x9, x9, x10
   41344:	stur	x9, [x29, #-16]
   41348:	mov	sp, x9
   4134c:	add	x9, x26, x23
   41350:	add	x26, x9, #0x1
   41354:	cmp	x26, #0x2
   41358:	b.eq	414a8 <__gmpn_dcpi1_divappr_q@@Base+0x264>  // b.none
   4135c:	cmp	x26, #0x1
   41360:	b.ne	414cc <__gmpn_dcpi1_divappr_q@@Base+0x288>  // b.any
   41364:	add	x26, x1, x24
   41368:	mov	x0, x26
   4136c:	mov	x1, x21
   41370:	mov	x2, x20
   41374:	stur	x11, [x29, #-40]
   41378:	bl	c570 <__gmpn_cmp@plt>
   4137c:	mvn	w8, w0
   41380:	lsr	w25, w8, #31
   41384:	tbnz	w0, #31, 4139c <__gmpn_dcpi1_divappr_q@@Base+0x158>
   41388:	mov	x0, x26
   4138c:	mov	x1, x26
   41390:	mov	x2, x21
   41394:	mov	x3, x20
   41398:	bl	c420 <__gmpn_sub_n@plt>
   4139c:	ldur	x8, [x29, #-32]
   413a0:	sub	x0, x26, #0x8
   413a4:	ldur	x9, [x8, #-8]
   413a8:	ldur	x8, [x29, #-24]
   413ac:	add	x8, x8, x20, lsl #3
   413b0:	add	x16, x8, x24
   413b4:	ldp	x8, x27, [x28, #-16]
   413b8:	ldur	x10, [x16, #-16]
   413bc:	cmp	x9, x27
   413c0:	b.ne	413cc <__gmpn_dcpi1_divappr_q@@Base+0x188>  // b.any
   413c4:	cmp	x10, x8
   413c8:	b.eq	41714 <__gmpn_dcpi1_divappr_q@@Base+0x4d0>  // b.none
   413cc:	ldur	x11, [x29, #-8]
   413d0:	ldur	x12, [x16, #-24]
   413d4:	ldr	x11, [x11]
   413d8:	mul	x13, x11, x9
   413dc:	umulh	x11, x9, x11
   413e0:	adds	x14, x13, x10
   413e4:	adc	x9, x11, x9
   413e8:	msub	x10, x9, x27, x10
   413ec:	subs	x15, x12, x8
   413f0:	sbc	x10, x10, x27
   413f4:	mul	x11, x9, x8
   413f8:	umulh	x13, x8, x9
   413fc:	subs	x12, x15, x11
   41400:	sbc	x10, x10, x13
   41404:	cmp	x10, x14
   41408:	cset	w11, cs  // cs = hs, nlast
   4140c:	csetm	x13, cs  // cs = hs, nlast
   41410:	sub	x9, x9, x11
   41414:	and	x11, x8, x13
   41418:	and	x13, x27, x13
   4141c:	adds	x26, x12, x11
   41420:	adc	x28, x10, x13
   41424:	cmp	x28, x27
   41428:	add	x3, x9, #0x1
   4142c:	b.cs	416f0 <__gmpn_dcpi1_divappr_q@@Base+0x4ac>  // b.hs, b.nlast
   41430:	cmp	x20, #0x3
   41434:	b.lt	41610 <__gmpn_dcpi1_divappr_q@@Base+0x3cc>  // b.tstop
   41438:	sub	x2, x20, #0x2
   4143c:	mov	x1, x21
   41440:	stp	x0, x3, [x29, #-64]
   41444:	stur	x16, [x29, #-48]
   41448:	bl	cba0 <__gmpn_submul_1@plt>
   4144c:	ldur	x16, [x29, #-48]
   41450:	subs	x8, x26, x0
   41454:	cset	w9, cc  // cc = lo, ul, last
   41458:	subs	x28, x28, x9
   4145c:	stur	x8, [x16, #-24]
   41460:	b.cc	41734 <__gmpn_dcpi1_divappr_q@@Base+0x4f0>  // b.lo, b.ul, b.last
   41464:	ldur	x3, [x29, #-56]
   41468:	b	41614 <__gmpn_dcpi1_divappr_q@@Base+0x3d0>
   4146c:	add	x8, x9, x8, lsl #3
   41470:	ldur	x9, [x29, #-8]
   41474:	sub	x1, x8, #0x10
   41478:	mvn	x8, x26
   4147c:	lsl	x2, x4, #1
   41480:	ldr	x5, [x9]
   41484:	add	x3, x28, x8, lsl #3
   41488:	mov	x0, x20
   4148c:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   41490:	mov	x25, x0
   41494:	add	x1, x20, #0x8
   41498:	mov	x0, x22
   4149c:	mov	x2, x26
   414a0:	bl	cc10 <__gmpn_copyi@plt>
   414a4:	b	416cc <__gmpn_dcpi1_divappr_q@@Base+0x488>
   414a8:	add	x8, x1, x20, lsl #3
   414ac:	add	x8, x8, x24
   414b0:	sub	x2, x8, #0x18
   414b4:	sub	x4, x28, #0x10
   414b8:	mov	w3, #0x4                   	// #4
   414bc:	mov	x0, x22
   414c0:	mov	x1, xzr
   414c4:	bl	c350 <__gmpn_divrem_2@plt>
   414c8:	b	4152c <__gmpn_dcpi1_divappr_q@@Base+0x2e8>
   414cc:	cmp	x26, #0x29
   414d0:	sub	x1, x1, x8
   414d4:	b.le	41500 <__gmpn_dcpi1_divappr_q@@Base+0x2bc>
   414d8:	lsl	x8, x20, #1
   414dc:	sub	x8, x8, x27
   414e0:	ldp	x5, x4, [x29, #-16]
   414e4:	add	x8, x21, x8, lsl #3
   414e8:	add	x8, x8, x24
   414ec:	sub	x2, x8, #0x8
   414f0:	mov	x0, x22
   414f4:	mov	x3, x26
   414f8:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   414fc:	b	4152c <__gmpn_dcpi1_divappr_q@@Base+0x2e8>
   41500:	ldur	x9, [x29, #-8]
   41504:	lsl	x8, x20, #1
   41508:	sub	x8, x8, x27
   4150c:	add	x8, x21, x8, lsl #3
   41510:	ldr	x5, [x9]
   41514:	add	x8, x8, x24
   41518:	lsl	x2, x26, #1
   4151c:	sub	x3, x8, #0x8
   41520:	mov	x0, x22
   41524:	mov	x4, x26
   41528:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   4152c:	mvn	x8, x27
   41530:	add	x8, x8, x20, lsl #1
   41534:	mov	x25, x0
   41538:	cmp	x8, x23
   4153c:	b.eq	41620 <__gmpn_dcpi1_divappr_q@@Base+0x3dc>  // b.none
   41540:	lsl	x8, x20, #1
   41544:	sub	x8, x8, x27
   41548:	mvn	x9, x23
   4154c:	add	x8, x9, x8
   41550:	cmp	x26, x8
   41554:	stur	x8, [x29, #-40]
   41558:	b.le	41578 <__gmpn_dcpi1_divappr_q@@Base+0x334>
   4155c:	ldur	x28, [x29, #-16]
   41560:	mov	x1, x22
   41564:	mov	x2, x26
   41568:	mov	x3, x21
   4156c:	mov	x0, x28
   41570:	mov	x4, x8
   41574:	b	41590 <__gmpn_dcpi1_divappr_q@@Base+0x34c>
   41578:	ldur	x28, [x29, #-16]
   4157c:	mov	x1, x21
   41580:	mov	x2, x8
   41584:	mov	x3, x22
   41588:	mov	x0, x28
   4158c:	mov	x4, x26
   41590:	bl	cea0 <__gmpn_mul@plt>
   41594:	ldur	x8, [x29, #-24]
   41598:	mov	x2, x28
   4159c:	mov	x3, x20
   415a0:	add	x8, x8, x24
   415a4:	sub	x27, x8, #0x8
   415a8:	mov	x0, x27
   415ac:	mov	x1, x27
   415b0:	bl	c420 <__gmpn_sub_n@plt>
   415b4:	mov	x28, x0
   415b8:	cbz	x25, 415d4 <__gmpn_dcpi1_divappr_q@@Base+0x390>
   415bc:	ldur	x3, [x29, #-40]
   415c0:	add	x0, x27, x26, lsl #3
   415c4:	mov	x1, x0
   415c8:	mov	x2, x21
   415cc:	bl	c420 <__gmpn_sub_n@plt>
   415d0:	add	x28, x0, x28
   415d4:	cbz	x28, 41620 <__gmpn_dcpi1_divappr_q@@Base+0x3dc>
   415d8:	mov	w3, #0x1                   	// #1
   415dc:	mov	x0, x22
   415e0:	mov	x1, x22
   415e4:	mov	x2, x26
   415e8:	bl	caf0 <__gmpn_sub_1@plt>
   415ec:	sub	x25, x25, x0
   415f0:	mov	x0, x27
   415f4:	mov	x1, x27
   415f8:	mov	x2, x21
   415fc:	mov	x3, x20
   41600:	bl	cc30 <__gmpn_add_n@plt>
   41604:	subs	x28, x28, x0
   41608:	b.ne	415d8 <__gmpn_dcpi1_divappr_q@@Base+0x394>  // b.any
   4160c:	b	41620 <__gmpn_dcpi1_divappr_q@@Base+0x3dc>
   41610:	stur	x26, [x16, #-24]
   41614:	ldur	x8, [x29, #-40]
   41618:	stur	x28, [x16, #-16]
   4161c:	str	x3, [x8, x24]
   41620:	neg	x23, x23
   41624:	cmp	x23, x20
   41628:	b.le	41678 <__gmpn_dcpi1_divappr_q@@Base+0x434>
   4162c:	ldp	x8, x27, [x29, #-24]
   41630:	ldur	x28, [x29, #-8]
   41634:	neg	x26, x20, lsl #3
   41638:	add	x8, x8, x24
   4163c:	sub	x24, x8, #0x8
   41640:	add	x22, x22, x26
   41644:	add	x24, x24, x26
   41648:	mov	x0, x22
   4164c:	mov	x1, x24
   41650:	mov	x2, x21
   41654:	mov	x3, x20
   41658:	mov	x4, x28
   4165c:	mov	x5, x27
   41660:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   41664:	sub	x23, x23, x20
   41668:	cmp	x23, x20
   4166c:	b.gt	41640 <__gmpn_dcpi1_divappr_q@@Base+0x3fc>
   41670:	add	x8, x24, x19
   41674:	b	41684 <__gmpn_dcpi1_divappr_q@@Base+0x440>
   41678:	ldur	x8, [x29, #-32]
   4167c:	ldur	x28, [x29, #-8]
   41680:	sub	x8, x8, #0x8
   41684:	mov	w9, #0x1                   	// #1
   41688:	ldur	x5, [x29, #-16]
   4168c:	ldr	x27, [x22]
   41690:	sub	x9, x9, x23
   41694:	add	x26, x22, x9, lsl #3
   41698:	sub	x8, x8, x19
   4169c:	sub	x1, x8, x19
   416a0:	mov	x0, x26
   416a4:	mov	x2, x21
   416a8:	mov	x3, x20
   416ac:	mov	x4, x28
   416b0:	sub	x24, x23, #0x1
   416b4:	bl	41768 <__gmpn_dcpi1_divappr_q@@Base+0x524>
   416b8:	add	x1, x26, #0x8
   416bc:	mov	x0, x26
   416c0:	mov	x2, x24
   416c4:	bl	cc10 <__gmpn_copyi@plt>
   416c8:	str	x27, [x22]
   416cc:	mov	x0, x25
   416d0:	mov	sp, x29
   416d4:	ldp	x20, x19, [sp, #80]
   416d8:	ldp	x22, x21, [sp, #64]
   416dc:	ldp	x24, x23, [sp, #48]
   416e0:	ldp	x26, x25, [sp, #32]
   416e4:	ldp	x28, x27, [sp, #16]
   416e8:	ldp	x29, x30, [sp], #96
   416ec:	ret
   416f0:	cmp	x26, x8
   416f4:	b.cs	41700 <__gmpn_dcpi1_divappr_q@@Base+0x4bc>  // b.hs, b.nlast
   416f8:	cmp	x28, x27
   416fc:	b.ls	41430 <__gmpn_dcpi1_divappr_q@@Base+0x1ec>  // b.plast
   41700:	subs	x9, x26, x8
   41704:	sbc	x28, x28, x27
   41708:	add	x3, x3, #0x1
   4170c:	mov	x26, x9
   41710:	b	41430 <__gmpn_dcpi1_divappr_q@@Base+0x1ec>
   41714:	mov	x3, #0xffffffffffffffff    	// #-1
   41718:	mov	x1, x21
   4171c:	mov	x2, x20
   41720:	mov	x26, #0xffffffffffffffff    	// #-1
   41724:	bl	cba0 <__gmpn_submul_1@plt>
   41728:	ldur	x8, [x29, #-40]
   4172c:	str	x26, [x8, x24]
   41730:	b	41620 <__gmpn_dcpi1_divappr_q@@Base+0x3dc>
   41734:	ldur	x0, [x29, #-64]
   41738:	sub	x3, x20, #0x1
   4173c:	mov	x2, x21
   41740:	mov	x1, x0
   41744:	bl	cc30 <__gmpn_add_n@plt>
   41748:	ldp	x3, x16, [x29, #-56]
   4174c:	add	x8, x28, x27
   41750:	add	x28, x8, x0
   41754:	cmp	x3, #0x0
   41758:	cset	w8, eq  // eq = none
   4175c:	sub	x25, x25, x8
   41760:	sub	x3, x3, #0x1
   41764:	b	41614 <__gmpn_dcpi1_divappr_q@@Base+0x3d0>
   41768:	sub	sp, sp, #0x80
   4176c:	stp	x20, x19, [sp, #112]
   41770:	asr	x20, x3, #1
   41774:	stp	x28, x27, [sp, #48]
   41778:	sub	x28, x3, x20
   4177c:	and	x8, x3, #0xfffffffffffffffe
   41780:	stp	x29, x30, [sp, #32]
   41784:	stp	x26, x25, [sp, #64]
   41788:	stp	x24, x23, [sp, #80]
   4178c:	stp	x22, x21, [sp, #96]
   41790:	add	x29, sp, #0x20
   41794:	mov	x23, x5
   41798:	mov	x21, x3
   4179c:	mov	x25, x2
   417a0:	mov	x27, x1
   417a4:	add	x19, x0, x20, lsl #3
   417a8:	cmp	x28, #0x29
   417ac:	add	x1, x1, x8, lsl #3
   417b0:	stur	x0, [x29, #-8]
   417b4:	stp	x8, x4, [sp, #8]
   417b8:	b.le	417d4 <__gmpn_dcpi1_divappr_q@@Base+0x590>
   417bc:	add	x2, x25, x20, lsl #3
   417c0:	mov	x0, x19
   417c4:	mov	x3, x28
   417c8:	mov	x5, x23
   417cc:	bl	d5c0 <__gmpn_dcpi1_div_qr_n@plt>
   417d0:	b	417ec <__gmpn_dcpi1_divappr_q@@Base+0x5a8>
   417d4:	ldr	x5, [x4]
   417d8:	lsl	x2, x28, #1
   417dc:	add	x3, x25, x20, lsl #3
   417e0:	mov	x0, x19
   417e4:	mov	x4, x28
   417e8:	bl	c7d0 <__gmpn_sbpi1_div_qr@plt>
   417ec:	mov	x24, x0
   417f0:	mov	x0, x23
   417f4:	mov	x1, x19
   417f8:	mov	x2, x28
   417fc:	mov	x3, x25
   41800:	mov	x4, x20
   41804:	bl	cea0 <__gmpn_mul@plt>
   41808:	add	x26, x27, x20, lsl #3
   4180c:	mov	x0, x26
   41810:	mov	x1, x26
   41814:	mov	x2, x23
   41818:	mov	x3, x21
   4181c:	bl	c420 <__gmpn_sub_n@plt>
   41820:	mov	x22, x0
   41824:	cbz	x24, 41840 <__gmpn_dcpi1_divappr_q@@Base+0x5fc>
   41828:	add	x0, x27, x21, lsl #3
   4182c:	mov	x1, x0
   41830:	mov	x2, x25
   41834:	mov	x3, x20
   41838:	bl	c420 <__gmpn_sub_n@plt>
   4183c:	add	x22, x0, x22
   41840:	cbz	x22, 41878 <__gmpn_dcpi1_divappr_q@@Base+0x634>
   41844:	mov	w3, #0x1                   	// #1
   41848:	mov	x0, x19
   4184c:	mov	x1, x19
   41850:	mov	x2, x28
   41854:	bl	caf0 <__gmpn_sub_1@plt>
   41858:	sub	x24, x24, x0
   4185c:	mov	x0, x26
   41860:	mov	x1, x26
   41864:	mov	x2, x25
   41868:	mov	x3, x21
   4186c:	bl	cc30 <__gmpn_add_n@plt>
   41870:	subs	x22, x22, x0
   41874:	b.ne	41844 <__gmpn_dcpi1_divappr_q@@Base+0x600>  // b.any
   41878:	lsl	x8, x28, #3
   4187c:	cmp	x21, #0x12f
   41880:	add	x1, x27, x8
   41884:	add	x3, x25, x8
   41888:	b.le	418b0 <__gmpn_dcpi1_divappr_q@@Base+0x66c>
   4188c:	ldur	x19, [x29, #-8]
   41890:	ldr	x4, [sp, #16]
   41894:	mov	x2, x3
   41898:	mov	x3, x20
   4189c:	mov	x0, x19
   418a0:	mov	x5, x23
   418a4:	bl	41768 <__gmpn_dcpi1_divappr_q@@Base+0x524>
   418a8:	cbnz	x0, 418cc <__gmpn_dcpi1_divappr_q@@Base+0x688>
   418ac:	b	418d4 <__gmpn_dcpi1_divappr_q@@Base+0x690>
   418b0:	ldp	x2, x8, [sp, #8]
   418b4:	ldur	x19, [x29, #-8]
   418b8:	mov	x4, x20
   418bc:	ldr	x5, [x8]
   418c0:	mov	x0, x19
   418c4:	bl	c880 <__gmpn_sbpi1_divappr_q@plt>
   418c8:	cbz	x0, 418d4 <__gmpn_dcpi1_divappr_q@@Base+0x690>
   418cc:	cmp	x21, #0x2
   418d0:	b.ge	418f8 <__gmpn_dcpi1_divappr_q@@Base+0x6b4>  // b.tcont
   418d4:	mov	x0, x24
   418d8:	ldp	x20, x19, [sp, #112]
   418dc:	ldp	x22, x21, [sp, #96]
   418e0:	ldp	x24, x23, [sp, #80]
   418e4:	ldp	x26, x25, [sp, #64]
   418e8:	ldp	x28, x27, [sp, #48]
   418ec:	ldp	x29, x30, [sp, #32]
   418f0:	add	sp, sp, #0x80
   418f4:	ret
   418f8:	cmp	x20, #0x1
   418fc:	csinc	x8, x20, xzr, gt
   41900:	lsl	x2, x8, #3
   41904:	mov	w1, #0xff                  	// #255
   41908:	mov	x0, x19
   4190c:	bl	c780 <memset@plt>
   41910:	b	418d4 <__gmpn_dcpi1_divappr_q@@Base+0x690>

0000000000041914 <__gmpn_mu_div_qr@@Base>:
   41914:	sub	sp, sp, #0x80
   41918:	stp	x22, x21, [sp, #96]
   4191c:	sub	x22, x3, x5
   41920:	add	x8, x22, #0x64
   41924:	stp	x28, x27, [sp, #48]
   41928:	stp	x26, x25, [sp, #64]
   4192c:	stp	x24, x23, [sp, #80]
   41930:	stp	x20, x19, [sp, #112]
   41934:	mov	x24, x6
   41938:	mov	x19, x5
   4193c:	mov	x25, x3
   41940:	mov	x27, x2
   41944:	cmp	x8, x5
   41948:	mov	x23, x0
   4194c:	stp	x29, x30, [sp, #32]
   41950:	add	x29, sp, #0x20
   41954:	b.ge	419d8 <__gmpn_mu_div_qr@@Base+0xc4>  // b.tcont
   41958:	mov	w20, #0x1                   	// #1
   4195c:	lsl	x8, x25, #3
   41960:	bfi	x20, x22, #1, #63
   41964:	add	x9, x4, x19, lsl #3
   41968:	add	x26, x22, #0x1
   4196c:	mvn	x10, x22
   41970:	add	x11, x1, x8
   41974:	add	x8, x27, x8
   41978:	lsl	x12, x20, #3
   4197c:	str	x1, [sp]
   41980:	sub	x1, x11, x12
   41984:	sub	x2, x8, x12
   41988:	mov	x21, x4
   4198c:	add	x4, x9, x10, lsl #3
   41990:	mov	x0, x23
   41994:	mov	x3, x20
   41998:	mov	x5, x26
   4199c:	mov	x6, x24
   419a0:	str	x1, [sp, #8]
   419a4:	bl	41ac4 <__gmpn_mu_div_qr@@Base+0x1b0>
   419a8:	sub	x28, x19, x26
   419ac:	str	x26, [sp, #16]
   419b0:	cmp	x28, x22
   419b4:	mov	x26, x0
   419b8:	mov	x0, x24
   419bc:	stur	x21, [x29, #-8]
   419c0:	b.le	419f8 <__gmpn_mu_div_qr@@Base+0xe4>
   419c4:	mov	x1, x21
   419c8:	mov	x2, x28
   419cc:	mov	x3, x23
   419d0:	mov	x4, x22
   419d4:	b	41a08 <__gmpn_mu_div_qr@@Base+0xf4>
   419d8:	mov	x0, x23
   419dc:	mov	x2, x27
   419e0:	mov	x3, x25
   419e4:	mov	x5, x19
   419e8:	mov	x6, x24
   419ec:	bl	41ac4 <__gmpn_mu_div_qr@@Base+0x1b0>
   419f0:	mov	x26, x0
   419f4:	b	41aa0 <__gmpn_mu_div_qr@@Base+0x18c>
   419f8:	mov	x1, x23
   419fc:	mov	x2, x22
   41a00:	mov	x3, x21
   41a04:	mov	x4, x28
   41a08:	bl	cea0 <__gmpn_mul@plt>
   41a0c:	ldr	x21, [sp]
   41a10:	neg	x8, x20
   41a14:	str	x8, [sp]
   41a18:	cbz	x26, 41a34 <__gmpn_mu_div_qr@@Base+0x120>
   41a1c:	ldur	x2, [x29, #-8]
   41a20:	add	x0, x24, x22, lsl #3
   41a24:	mov	x1, x0
   41a28:	mov	x3, x28
   41a2c:	bl	cc30 <__gmpn_add_n@plt>
   41a30:	b	41a38 <__gmpn_mu_div_qr@@Base+0x124>
   41a34:	mov	x0, xzr
   41a38:	add	x8, x24, x19, lsl #3
   41a3c:	stur	x0, [x8, #-8]
   41a40:	sub	x3, x25, x20
   41a44:	mov	x0, x21
   41a48:	mov	x1, x27
   41a4c:	mov	x2, x24
   41a50:	bl	c420 <__gmpn_sub_n@plt>
   41a54:	mov	x4, x0
   41a58:	ldp	x9, x0, [sp]
   41a5c:	ldr	x3, [sp, #16]
   41a60:	add	x8, x24, x25, lsl #3
   41a64:	add	x2, x8, x9, lsl #3
   41a68:	mov	x1, x0
   41a6c:	bl	c8f0 <__gmpn_sub_nc@plt>
   41a70:	cbz	x0, 41aa0 <__gmpn_mu_div_qr@@Base+0x18c>
   41a74:	mov	w3, #0x1                   	// #1
   41a78:	mov	x0, x23
   41a7c:	mov	x1, x23
   41a80:	mov	x2, x22
   41a84:	bl	caf0 <__gmpn_sub_1@plt>
   41a88:	ldur	x2, [x29, #-8]
   41a8c:	sub	x26, x26, x0
   41a90:	mov	x0, x21
   41a94:	mov	x1, x21
   41a98:	mov	x3, x19
   41a9c:	bl	cc30 <__gmpn_add_n@plt>
   41aa0:	mov	x0, x26
   41aa4:	ldp	x20, x19, [sp, #112]
   41aa8:	ldp	x22, x21, [sp, #96]
   41aac:	ldp	x24, x23, [sp, #80]
   41ab0:	ldp	x26, x25, [sp, #64]
   41ab4:	ldp	x28, x27, [sp, #48]
   41ab8:	ldp	x29, x30, [sp, #32]
   41abc:	add	sp, sp, #0x80
   41ac0:	ret
   41ac4:	sub	sp, sp, #0x70
   41ac8:	stp	x28, x27, [sp, #32]
   41acc:	stp	x26, x25, [sp, #48]
   41ad0:	stp	x24, x23, [sp, #64]
   41ad4:	mov	x28, x2
   41ad8:	mov	x24, x1
   41adc:	mov	x25, x0
   41ae0:	sub	x0, x3, x5
   41ae4:	mov	x1, x5
   41ae8:	mov	w2, wzr
   41aec:	stp	x29, x30, [sp, #16]
   41af0:	stp	x22, x21, [sp, #80]
   41af4:	stp	x20, x19, [sp, #96]
   41af8:	add	x29, sp, #0x10
   41afc:	mov	x19, x6
   41b00:	mov	x20, x5
   41b04:	mov	x21, x4
   41b08:	mov	x22, x3
   41b0c:	bl	41ff4 <__gmpn_mu_div_qr_itch@@Base+0x48>
   41b10:	add	x23, x19, x0, lsl #3
   41b14:	mov	x26, x0
   41b18:	cmp	x0, x20
   41b1c:	add	x27, x23, #0x8
   41b20:	b.ne	41b68 <__gmpn_mu_div_qr@@Base+0x254>  // b.any
   41b24:	add	x0, x27, #0x8
   41b28:	mov	x1, x21
   41b2c:	mov	x2, x26
   41b30:	bl	cc10 <__gmpn_copyi@plt>
   41b34:	add	x9, x27, x26, lsl #3
   41b38:	mov	w8, #0x1                   	// #1
   41b3c:	add	x2, x26, #0x1
   41b40:	add	x3, x9, #0x8
   41b44:	mov	x0, x19
   41b48:	mov	x1, x27
   41b4c:	str	x8, [x27]
   41b50:	bl	d230 <__gmpn_invertappr@plt>
   41b54:	add	x1, x19, #0x8
   41b58:	mov	x0, x19
   41b5c:	mov	x2, x26
   41b60:	bl	cc10 <__gmpn_copyi@plt>
   41b64:	b	41bcc <__gmpn_mu_div_qr@@Base+0x2b8>
   41b68:	str	x25, [sp, #8]
   41b6c:	mov	x25, x24
   41b70:	mov	x24, x28
   41b74:	add	x8, x21, x20, lsl #3
   41b78:	add	x28, x26, #0x1
   41b7c:	mvn	x9, x26
   41b80:	add	x1, x8, x9, lsl #3
   41b84:	mov	w3, #0x1                   	// #1
   41b88:	mov	x0, x27
   41b8c:	mov	x2, x28
   41b90:	bl	c150 <__gmpn_add_1@plt>
   41b94:	cbnz	x0, 41c14 <__gmpn_mu_div_qr@@Base+0x300>
   41b98:	add	x8, x27, x26, lsl #3
   41b9c:	add	x3, x8, #0x8
   41ba0:	mov	x0, x19
   41ba4:	mov	x1, x27
   41ba8:	mov	x2, x28
   41bac:	bl	d230 <__gmpn_invertappr@plt>
   41bb0:	add	x1, x19, #0x8
   41bb4:	mov	x0, x19
   41bb8:	mov	x2, x26
   41bbc:	bl	cc10 <__gmpn_copyi@plt>
   41bc0:	mov	x28, x24
   41bc4:	mov	x24, x25
   41bc8:	ldr	x25, [sp, #8]
   41bcc:	mov	x0, x25
   41bd0:	mov	x1, x24
   41bd4:	mov	x2, x28
   41bd8:	mov	x3, x22
   41bdc:	mov	x4, x21
   41be0:	mov	x5, x20
   41be4:	mov	x6, x19
   41be8:	mov	x7, x26
   41bec:	str	x23, [sp]
   41bf0:	bl	d250 <__gmpn_preinv_mu_div_qr@plt>
   41bf4:	ldp	x20, x19, [sp, #96]
   41bf8:	ldp	x22, x21, [sp, #80]
   41bfc:	ldp	x24, x23, [sp, #64]
   41c00:	ldp	x26, x25, [sp, #48]
   41c04:	ldp	x28, x27, [sp, #32]
   41c08:	ldp	x29, x30, [sp, #16]
   41c0c:	add	sp, sp, #0x70
   41c10:	ret
   41c14:	mov	x28, x24
   41c18:	mov	x24, x25
   41c1c:	ldr	x25, [sp, #8]
   41c20:	cbz	x26, 41bcc <__gmpn_mu_div_qr@@Base+0x2b8>
   41c24:	lsl	x2, x26, #3
   41c28:	mov	x0, x19
   41c2c:	mov	w1, wzr
   41c30:	bl	c780 <memset@plt>
   41c34:	b	41bcc <__gmpn_mu_div_qr@@Base+0x2b8>

0000000000041c38 <__gmpn_preinv_mu_div_qr@@Base>:
   41c38:	sub	sp, sp, #0xc0
   41c3c:	stp	x26, x25, [sp, #128]
   41c40:	sub	x26, x3, x5
   41c44:	stp	x20, x19, [sp, #176]
   41c48:	lsl	x20, x26, #3
   41c4c:	stp	x28, x27, [sp, #112]
   41c50:	add	x28, x2, x20
   41c54:	stp	x24, x23, [sp, #144]
   41c58:	mov	x23, x1
   41c5c:	mov	x24, x0
   41c60:	mov	x0, x28
   41c64:	mov	x1, x4
   41c68:	mov	x2, x5
   41c6c:	stp	x29, x30, [sp, #96]
   41c70:	stp	x22, x21, [sp, #160]
   41c74:	add	x29, sp, #0x60
   41c78:	mov	x19, x7
   41c7c:	mov	x25, x6
   41c80:	mov	x21, x5
   41c84:	mov	x22, x4
   41c88:	bl	c570 <__gmpn_cmp@plt>
   41c8c:	str	w0, [sp, #12]
   41c90:	tbnz	w0, #31, 41cb4 <__gmpn_preinv_mu_div_qr@@Base+0x7c>
   41c94:	mov	x0, x23
   41c98:	mov	x1, x28
   41c9c:	mov	x2, x22
   41ca0:	mov	x3, x21
   41ca4:	bl	c420 <__gmpn_sub_n@plt>
   41ca8:	cmp	x26, #0x1
   41cac:	b.ge	41ccc <__gmpn_preinv_mu_div_qr@@Base+0x94>  // b.tcont
   41cb0:	b	41f50 <__gmpn_preinv_mu_div_qr@@Base+0x318>
   41cb4:	mov	x0, x23
   41cb8:	mov	x1, x28
   41cbc:	mov	x2, x21
   41cc0:	bl	cc10 <__gmpn_copyi@plt>
   41cc4:	cmp	x26, #0x1
   41cc8:	b.lt	41f50 <__gmpn_preinv_mu_div_qr@@Base+0x318>  // b.tstop
   41ccc:	ldr	x8, [x29, #96]
   41cd0:	add	x27, x24, x20
   41cd4:	add	x9, x21, #0x1
   41cd8:	str	x9, [sp, #32]
   41cdc:	mov	x20, x8
   41ce0:	lsl	x8, x21, #3
   41ce4:	add	x9, x23, x8
   41ce8:	add	x8, x20, x8
   41cec:	str	x8, [sp, #24]
   41cf0:	add	x8, x20, #0x8
   41cf4:	str	x9, [sp, #48]
   41cf8:	str	x8, [sp, #16]
   41cfc:	stur	x20, [x29, #-8]
   41d00:	b	41d0c <__gmpn_preinv_mu_div_qr@@Base+0xd4>
   41d04:	cmp	x26, #0x0
   41d08:	b.le	41f50 <__gmpn_preinv_mu_div_qr@@Base+0x318>
   41d0c:	subs	x8, x19, x26
   41d10:	add	x8, x25, x8, lsl #3
   41d14:	csel	x25, x8, x25, gt
   41d18:	ldr	x8, [sp, #48]
   41d1c:	csel	x19, x26, x19, gt
   41d20:	stp	x28, x26, [x29, #-32]
   41d24:	lsl	x26, x19, #3
   41d28:	sub	x28, x8, x26
   41d2c:	mov	x0, x20
   41d30:	mov	x1, x28
   41d34:	mov	x2, x25
   41d38:	mov	x3, x19
   41d3c:	sub	x27, x27, x26
   41d40:	stur	x25, [x29, #-16]
   41d44:	bl	cb50 <__gmpn_mul_n@plt>
   41d48:	add	x24, x20, x26
   41d4c:	mov	x0, x27
   41d50:	mov	x1, x24
   41d54:	mov	x2, x28
   41d58:	mov	x3, x19
   41d5c:	bl	cc30 <__gmpn_add_n@plt>
   41d60:	cbnz	x0, 41f7c <__gmpn_preinv_mu_div_qr@@Base+0x344>
   41d64:	cmp	x19, #0x11
   41d68:	stur	x24, [x29, #-40]
   41d6c:	b.le	41e34 <__gmpn_preinv_mu_div_qr@@Base+0x1fc>
   41d70:	ldr	x0, [sp, #32]
   41d74:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   41d78:	mov	x24, x0
   41d7c:	add	x6, x20, x0, lsl #3
   41d80:	mov	x0, x20
   41d84:	mov	x1, x24
   41d88:	mov	x2, x22
   41d8c:	mov	x3, x21
   41d90:	mov	x4, x27
   41d94:	mov	x5, x19
   41d98:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   41d9c:	add	x8, x19, x21
   41da0:	sub	x20, x8, x24
   41da4:	cmp	x20, #0x1
   41da8:	b.lt	41e4c <__gmpn_preinv_mu_div_qr@@Base+0x214>  // b.tstop
   41dac:	ldr	x8, [sp, #48]
   41db0:	ldur	x25, [x29, #-8]
   41db4:	lsl	x9, x20, #3
   41db8:	mov	x3, x20
   41dbc:	sub	x2, x8, x9
   41dc0:	mov	x0, x25
   41dc4:	mov	x1, x25
   41dc8:	str	x9, [sp, #40]
   41dcc:	bl	c420 <__gmpn_sub_n@plt>
   41dd0:	ldr	x8, [sp, #40]
   41dd4:	mov	x3, x0
   41dd8:	sub	x2, x24, x20
   41ddc:	add	x0, x25, x8
   41de0:	mov	x1, x0
   41de4:	bl	caf0 <__gmpn_sub_1@plt>
   41de8:	ldr	x1, [sp, #24]
   41dec:	mov	x20, x0
   41df0:	sub	x2, x24, x21
   41df4:	mov	x0, x28
   41df8:	bl	c570 <__gmpn_cmp@plt>
   41dfc:	lsr	w8, w0, #31
   41e00:	cmp	x20, x8
   41e04:	b.hi	41f94 <__gmpn_preinv_mu_div_qr@@Base+0x35c>  // b.pmore
   41e08:	ldr	x9, [x25]
   41e0c:	sub	x8, x8, x20
   41e10:	adds	x8, x9, x8
   41e14:	str	x8, [x25]
   41e18:	b.cc	41e4c <__gmpn_preinv_mu_div_qr@@Base+0x214>  // b.lo, b.ul, b.last
   41e1c:	ldr	x8, [sp, #16]
   41e20:	ldr	x9, [x8]
   41e24:	adds	x9, x9, #0x1
   41e28:	str	x9, [x8], #8
   41e2c:	b.cs	41e20 <__gmpn_preinv_mu_div_qr@@Base+0x1e8>  // b.hs, b.nlast
   41e30:	b	41e4c <__gmpn_preinv_mu_div_qr@@Base+0x214>
   41e34:	mov	x0, x20
   41e38:	mov	x1, x22
   41e3c:	mov	x2, x21
   41e40:	mov	x3, x27
   41e44:	mov	x4, x19
   41e48:	bl	cea0 <__gmpn_mul@plt>
   41e4c:	ldur	x25, [x29, #-8]
   41e50:	subs	x8, x21, x19
   41e54:	ldr	x8, [x23, x8, lsl #3]
   41e58:	ldur	x28, [x29, #-32]
   41e5c:	ldr	x9, [x25, x21, lsl #3]
   41e60:	subs	x20, x21, x19
   41e64:	sub	x28, x28, x26
   41e68:	sub	x24, x8, x9
   41e6c:	b.ne	41e8c <__gmpn_preinv_mu_div_qr@@Base+0x254>  // b.any
   41e70:	mov	x0, x23
   41e74:	mov	x1, x28
   41e78:	mov	x2, x25
   41e7c:	mov	x3, x19
   41e80:	bl	c420 <__gmpn_sub_n@plt>
   41e84:	mov	x20, x0
   41e88:	b	41ecc <__gmpn_preinv_mu_div_qr@@Base+0x294>
   41e8c:	mov	x0, x25
   41e90:	mov	x1, x28
   41e94:	mov	x2, x25
   41e98:	mov	x3, x19
   41e9c:	bl	c420 <__gmpn_sub_n@plt>
   41ea0:	mov	x4, x0
   41ea4:	ldur	x0, [x29, #-40]
   41ea8:	mov	x1, x23
   41eac:	mov	x3, x20
   41eb0:	mov	x2, x0
   41eb4:	bl	c8f0 <__gmpn_sub_nc@plt>
   41eb8:	mov	x20, x0
   41ebc:	mov	x0, x23
   41ec0:	mov	x1, x25
   41ec4:	mov	x2, x21
   41ec8:	bl	cc10 <__gmpn_copyi@plt>
   41ecc:	ldp	x26, x25, [x29, #-24]
   41ed0:	subs	x20, x24, x20
   41ed4:	b.eq	41f08 <__gmpn_preinv_mu_div_qr@@Base+0x2d0>  // b.none
   41ed8:	mov	x8, x27
   41edc:	ldr	x9, [x8]
   41ee0:	adds	x9, x9, #0x1
   41ee4:	str	x9, [x8], #8
   41ee8:	b.cs	41edc <__gmpn_preinv_mu_div_qr@@Base+0x2a4>  // b.hs, b.nlast
   41eec:	mov	x0, x23
   41ef0:	mov	x1, x23
   41ef4:	mov	x2, x22
   41ef8:	mov	x3, x21
   41efc:	bl	c420 <__gmpn_sub_n@plt>
   41f00:	subs	x20, x20, x0
   41f04:	b.ne	41ed8 <__gmpn_preinv_mu_div_qr@@Base+0x2a0>  // b.any
   41f08:	mov	x0, x23
   41f0c:	mov	x1, x22
   41f10:	mov	x2, x21
   41f14:	sub	x26, x26, x19
   41f18:	bl	c570 <__gmpn_cmp@plt>
   41f1c:	ldur	x20, [x29, #-8]
   41f20:	tbnz	w0, #31, 41d04 <__gmpn_preinv_mu_div_qr@@Base+0xcc>
   41f24:	mov	x8, x27
   41f28:	ldr	x9, [x8]
   41f2c:	adds	x9, x9, #0x1
   41f30:	str	x9, [x8], #8
   41f34:	b.cs	41f28 <__gmpn_preinv_mu_div_qr@@Base+0x2f0>  // b.hs, b.nlast
   41f38:	mov	x0, x23
   41f3c:	mov	x1, x23
   41f40:	mov	x2, x22
   41f44:	mov	x3, x21
   41f48:	bl	c420 <__gmpn_sub_n@plt>
   41f4c:	b	41d04 <__gmpn_preinv_mu_div_qr@@Base+0xcc>
   41f50:	ldr	w8, [sp, #12]
   41f54:	ldp	x20, x19, [sp, #176]
   41f58:	ldp	x22, x21, [sp, #160]
   41f5c:	ldp	x24, x23, [sp, #144]
   41f60:	ldp	x26, x25, [sp, #128]
   41f64:	ldp	x28, x27, [sp, #112]
   41f68:	ldp	x29, x30, [sp, #96]
   41f6c:	mvn	w8, w8
   41f70:	lsr	w0, w8, #31
   41f74:	add	sp, sp, #0xc0
   41f78:	ret
   41f7c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   41f80:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   41f84:	add	x0, x0, #0x213
   41f88:	add	x2, x2, #0x1f9
   41f8c:	mov	w1, #0x118                 	// #280
   41f90:	bl	c850 <__gmp_assert_fail@plt>
   41f94:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   41f98:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   41f9c:	add	x0, x0, #0x213
   41fa0:	add	x2, x2, #0x21f
   41fa4:	mov	w1, #0x12c                 	// #300
   41fa8:	bl	c850 <__gmp_assert_fail@plt>

0000000000041fac <__gmpn_mu_div_qr_itch@@Base>:
   41fac:	stp	x29, x30, [sp, #-32]!
   41fb0:	sub	x0, x0, x1
   41fb4:	stp	x20, x19, [sp, #16]
   41fb8:	mov	x29, sp
   41fbc:	mov	x19, x1
   41fc0:	bl	41ff4 <__gmpn_mu_div_qr_itch@@Base+0x48>
   41fc4:	mov	x1, x19
   41fc8:	mov	x2, x0
   41fcc:	mov	x20, x0
   41fd0:	bl	c240 <__gmpn_preinv_mu_div_qr_itch@plt>
   41fd4:	add	x8, x20, x20, lsl #1
   41fd8:	add	x8, x8, #0x4
   41fdc:	cmp	x8, x0
   41fe0:	csel	x8, x8, x0, gt
   41fe4:	add	x0, x8, x20
   41fe8:	ldp	x20, x19, [sp, #16]
   41fec:	ldp	x29, x30, [sp], #32
   41ff0:	ret
   41ff4:	cbz	w2, 4200c <__gmpn_mu_div_qr_itch@@Base+0x60>
   41ff8:	cmp	x1, x0
   41ffc:	csel	x8, x1, x0, lt  // lt = tstop
   42000:	sub	x8, x8, #0x1
   42004:	sxtw	x9, w2
   42008:	b	42020 <__gmpn_mu_div_qr_itch@@Base+0x74>
   4200c:	cmp	x0, x1
   42010:	b.le	4202c <__gmpn_mu_div_qr_itch@@Base+0x80>
   42014:	sub	x8, x0, #0x1
   42018:	sdiv	x9, x8, x1
   4201c:	add	x9, x9, #0x1
   42020:	sdiv	x8, x8, x9
   42024:	add	x0, x8, #0x1
   42028:	ret
   4202c:	add	x8, x0, x0, lsl #1
   42030:	cmp	x8, x1
   42034:	b.le	42028 <__gmpn_mu_div_qr_itch@@Base+0x7c>
   42038:	sub	x8, x0, #0x1
   4203c:	cmp	x8, #0x0
   42040:	csel	x8, x0, x8, lt  // lt = tstop
   42044:	asr	x8, x8, #1
   42048:	b	42024 <__gmpn_mu_div_qr_itch@@Base+0x78>

000000000004204c <__gmpn_preinv_mu_div_qr_itch@@Base>:
   4204c:	stp	x29, x30, [sp, #-48]!
   42050:	add	x0, x1, #0x1
   42054:	str	x21, [sp, #16]
   42058:	stp	x20, x19, [sp, #32]
   4205c:	mov	x29, sp
   42060:	mov	x19, x2
   42064:	mov	x20, x1
   42068:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   4206c:	mov	x1, x20
   42070:	mov	x2, x19
   42074:	mov	x21, x0
   42078:	bl	42090 <__gmpn_preinv_mu_div_qr_itch@@Base+0x44>
   4207c:	add	x0, x0, x21
   42080:	ldp	x20, x19, [sp, #32]
   42084:	ldr	x21, [sp, #16]
   42088:	ldp	x29, x30, [sp], #48
   4208c:	ret
   42090:	asr	x8, x0, #1
   42094:	cmp	x8, x2
   42098:	csel	x9, x0, x8, lt  // lt = tstop
   4209c:	cmp	x8, x1
   420a0:	csel	x8, x9, xzr, lt  // lt = tstop
   420a4:	add	x8, x0, x8
   420a8:	add	x0, x8, #0x4
   420ac:	ret

00000000000420b0 <__gmpn_mu_divappr_q@@Base>:
   420b0:	stp	x29, x30, [sp, #-96]!
   420b4:	sub	x8, x2, x4
   420b8:	add	x9, x8, #0x1
   420bc:	stp	x24, x23, [sp, #48]
   420c0:	stp	x22, x21, [sp, #64]
   420c4:	stp	x20, x19, [sp, #80]
   420c8:	mov	x19, x5
   420cc:	mov	x23, x4
   420d0:	mov	x20, x3
   420d4:	mov	x21, x2
   420d8:	mov	x22, x1
   420dc:	cmp	x9, x4
   420e0:	mov	x24, x0
   420e4:	stp	x28, x27, [sp, #16]
   420e8:	stp	x26, x25, [sp, #32]
   420ec:	mov	x29, sp
   420f0:	b.ge	4210c <__gmpn_mu_divappr_q@@Base+0x5c>  // b.tcont
   420f4:	sub	x10, x23, x9
   420f8:	lsl	x11, x10, #3
   420fc:	sub	x21, x21, x10
   42100:	add	x22, x22, x11
   42104:	add	x20, x20, x11
   42108:	mov	x23, x9
   4210c:	mov	x0, x8
   42110:	mov	x1, x23
   42114:	mov	w2, wzr
   42118:	bl	42204 <__gmpn_mu_divappr_q@@Base+0x154>
   4211c:	add	x26, x19, x0, lsl #3
   42120:	mov	x25, x0
   42124:	cmp	x23, x0
   42128:	add	x27, x26, #0x8
   4212c:	b.ne	42160 <__gmpn_mu_divappr_q@@Base+0xb0>  // b.any
   42130:	add	x0, x27, #0x8
   42134:	mov	x1, x20
   42138:	mov	x2, x25
   4213c:	bl	cc10 <__gmpn_copyi@plt>
   42140:	mov	w8, #0x1                   	// #1
   42144:	add	x9, x27, x25, lsl #3
   42148:	add	x2, x25, #0x1
   4214c:	str	x8, [x27]
   42150:	add	x3, x9, #0x8
   42154:	mov	x0, x19
   42158:	mov	x1, x27
   4215c:	b	42198 <__gmpn_mu_divappr_q@@Base+0xe8>
   42160:	add	x8, x20, x23, lsl #3
   42164:	add	x28, x25, #0x1
   42168:	mvn	x9, x25
   4216c:	add	x1, x8, x9, lsl #3
   42170:	mov	w3, #0x1                   	// #1
   42174:	mov	x0, x27
   42178:	mov	x2, x28
   4217c:	bl	c150 <__gmpn_add_1@plt>
   42180:	cbnz	x0, 421ec <__gmpn_mu_divappr_q@@Base+0x13c>
   42184:	add	x8, x27, x25, lsl #3
   42188:	add	x3, x8, #0x8
   4218c:	mov	x0, x19
   42190:	mov	x1, x27
   42194:	mov	x2, x28
   42198:	bl	d230 <__gmpn_invertappr@plt>
   4219c:	add	x1, x19, #0x8
   421a0:	mov	x0, x19
   421a4:	mov	x2, x25
   421a8:	bl	cc10 <__gmpn_copyi@plt>
   421ac:	mov	x0, x24
   421b0:	mov	x1, x22
   421b4:	mov	x2, x21
   421b8:	mov	x3, x20
   421bc:	mov	x4, x23
   421c0:	mov	x5, x19
   421c4:	mov	x6, x25
   421c8:	mov	x7, x26
   421cc:	bl	4225c <__gmpn_mu_divappr_q@@Base+0x1ac>
   421d0:	ldp	x20, x19, [sp, #80]
   421d4:	ldp	x22, x21, [sp, #64]
   421d8:	ldp	x24, x23, [sp, #48]
   421dc:	ldp	x26, x25, [sp, #32]
   421e0:	ldp	x28, x27, [sp, #16]
   421e4:	ldp	x29, x30, [sp], #96
   421e8:	ret
   421ec:	cbz	x25, 421ac <__gmpn_mu_divappr_q@@Base+0xfc>
   421f0:	lsl	x2, x25, #3
   421f4:	mov	x0, x19
   421f8:	mov	w1, wzr
   421fc:	bl	c780 <memset@plt>
   42200:	b	421ac <__gmpn_mu_divappr_q@@Base+0xfc>
   42204:	cbz	w2, 4221c <__gmpn_mu_divappr_q@@Base+0x16c>
   42208:	cmp	x1, x0
   4220c:	csel	x8, x1, x0, lt  // lt = tstop
   42210:	sub	x8, x8, #0x1
   42214:	sxtw	x9, w2
   42218:	b	42230 <__gmpn_mu_divappr_q@@Base+0x180>
   4221c:	cmp	x0, x1
   42220:	b.le	4223c <__gmpn_mu_divappr_q@@Base+0x18c>
   42224:	sub	x8, x0, #0x1
   42228:	sdiv	x9, x8, x1
   4222c:	add	x9, x9, #0x1
   42230:	sdiv	x8, x8, x9
   42234:	add	x0, x8, #0x1
   42238:	ret
   4223c:	add	x8, x0, x0, lsl #1
   42240:	cmp	x8, x1
   42244:	b.le	42238 <__gmpn_mu_divappr_q@@Base+0x188>
   42248:	sub	x8, x0, #0x1
   4224c:	cmp	x8, #0x0
   42250:	csel	x8, x0, x8, lt  // lt = tstop
   42254:	asr	x8, x8, #1
   42258:	b	42234 <__gmpn_mu_divappr_q@@Base+0x184>
   4225c:	sub	sp, sp, #0xd0
   42260:	stp	x22, x21, [sp, #176]
   42264:	sub	x21, x2, x4
   42268:	stp	x20, x19, [sp, #192]
   4226c:	lsl	x20, x21, #3
   42270:	stp	x28, x27, [sp, #128]
   42274:	add	x28, x1, x20
   42278:	mov	x19, x0
   4227c:	mov	x0, x28
   42280:	mov	x1, x3
   42284:	mov	x2, x4
   42288:	stp	x29, x30, [sp, #112]
   4228c:	stp	x26, x25, [sp, #144]
   42290:	stp	x24, x23, [sp, #160]
   42294:	add	x29, sp, #0x70
   42298:	mov	x22, x7
   4229c:	mov	x23, x6
   422a0:	mov	x27, x5
   422a4:	mov	x25, x4
   422a8:	mov	x26, x3
   422ac:	bl	c570 <__gmpn_cmp@plt>
   422b0:	mvn	w8, w0
   422b4:	mov	w24, w0
   422b8:	lsr	w8, w8, #31
   422bc:	str	x8, [sp, #40]
   422c0:	tbnz	w0, #31, 42574 <__gmpn_mu_divappr_q@@Base+0x4c4>
   422c4:	mov	x0, x22
   422c8:	mov	x1, x28
   422cc:	mov	x2, x26
   422d0:	mov	x3, x25
   422d4:	bl	c420 <__gmpn_sub_n@plt>
   422d8:	cbz	x21, 42588 <__gmpn_mu_divappr_q@@Base+0x4d8>
   422dc:	stur	x27, [x29, #-8]
   422e0:	cmp	x21, #0x1
   422e4:	add	x27, x19, x20
   422e8:	str	w24, [sp, #20]
   422ec:	str	x20, [sp, #8]
   422f0:	str	x21, [sp, #32]
   422f4:	b.lt	42590 <__gmpn_mu_divappr_q@@Base+0x4e0>  // b.tstop
   422f8:	lsl	x8, x25, #3
   422fc:	add	x24, x22, x8
   42300:	add	x9, x25, #0x1
   42304:	add	x8, x24, x8
   42308:	stp	x8, x9, [sp, #48]
   4230c:	add	x8, x22, x9, lsl #3
   42310:	mov	x19, x21
   42314:	str	x8, [sp, #24]
   42318:	stur	x24, [x29, #-40]
   4231c:	b	42328 <__gmpn_mu_divappr_q@@Base+0x278>
   42320:	cmp	x19, #0x0
   42324:	b.le	42598 <__gmpn_mu_divappr_q@@Base+0x4e8>
   42328:	ldur	x2, [x29, #-8]
   4232c:	subs	x8, x23, x19
   42330:	csel	x23, x19, x23, gt
   42334:	lsl	x21, x23, #3
   42338:	stur	x28, [x29, #-16]
   4233c:	add	x8, x2, x8, lsl #3
   42340:	sub	x28, x24, x21
   42344:	csel	x2, x8, x2, gt
   42348:	mov	x0, x24
   4234c:	mov	x1, x28
   42350:	mov	x3, x23
   42354:	sub	x27, x27, x21
   42358:	stur	x2, [x29, #-8]
   4235c:	bl	cb50 <__gmpn_mul_n@plt>
   42360:	add	x1, x24, x21
   42364:	mov	x0, x27
   42368:	mov	x2, x28
   4236c:	mov	x3, x23
   42370:	stur	x1, [x29, #-32]
   42374:	bl	cc30 <__gmpn_add_n@plt>
   42378:	cbnz	x0, 42604 <__gmpn_mu_divappr_q@@Base+0x554>
   4237c:	subs	x19, x19, x23
   42380:	b.eq	42594 <__gmpn_mu_divappr_q@@Base+0x4e4>  // b.none
   42384:	cmp	x23, #0x11
   42388:	stur	x19, [x29, #-24]
   4238c:	b.le	42450 <__gmpn_mu_divappr_q@@Base+0x3a0>
   42390:	ldr	x0, [sp, #56]
   42394:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   42398:	mov	x20, x0
   4239c:	add	x6, x24, x0, lsl #3
   423a0:	mov	x0, x24
   423a4:	mov	x1, x20
   423a8:	mov	x2, x26
   423ac:	mov	x3, x25
   423b0:	mov	x4, x27
   423b4:	mov	x5, x23
   423b8:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   423bc:	add	x8, x23, x25
   423c0:	sub	x24, x8, x20
   423c4:	cmp	x24, #0x1
   423c8:	b.lt	42468 <__gmpn_mu_divappr_q@@Base+0x3b8>  // b.tstop
   423cc:	ldur	x19, [x29, #-40]
   423d0:	lsl	x8, x24, #3
   423d4:	mov	x3, x24
   423d8:	stur	x8, [x29, #-48]
   423dc:	sub	x2, x19, x8
   423e0:	mov	x0, x19
   423e4:	mov	x1, x19
   423e8:	bl	c420 <__gmpn_sub_n@plt>
   423ec:	ldur	x8, [x29, #-48]
   423f0:	mov	x3, x0
   423f4:	sub	x2, x20, x24
   423f8:	add	x0, x19, x8
   423fc:	mov	x1, x0
   42400:	bl	caf0 <__gmpn_sub_1@plt>
   42404:	ldr	x1, [sp, #48]
   42408:	mov	x24, x0
   4240c:	sub	x2, x20, x25
   42410:	mov	x0, x28
   42414:	bl	c570 <__gmpn_cmp@plt>
   42418:	lsr	w8, w0, #31
   4241c:	cmp	x24, x8
   42420:	b.hi	4261c <__gmpn_mu_divappr_q@@Base+0x56c>  // b.pmore
   42424:	ldr	x9, [x19]
   42428:	sub	x8, x8, x24
   4242c:	adds	x8, x9, x8
   42430:	str	x8, [x19]
   42434:	b.cc	42468 <__gmpn_mu_divappr_q@@Base+0x3b8>  // b.lo, b.ul, b.last
   42438:	ldr	x8, [sp, #24]
   4243c:	ldr	x9, [x8]
   42440:	adds	x9, x9, #0x1
   42444:	str	x9, [x8], #8
   42448:	b.cs	4243c <__gmpn_mu_divappr_q@@Base+0x38c>  // b.hs, b.nlast
   4244c:	b	42468 <__gmpn_mu_divappr_q@@Base+0x3b8>
   42450:	mov	x0, x24
   42454:	mov	x1, x26
   42458:	mov	x2, x25
   4245c:	mov	x3, x27
   42460:	mov	x4, x23
   42464:	bl	cea0 <__gmpn_mul@plt>
   42468:	ldur	x24, [x29, #-40]
   4246c:	subs	x8, x25, x23
   42470:	ldr	x8, [x22, x8, lsl #3]
   42474:	ldur	x28, [x29, #-16]
   42478:	ldr	x9, [x24, x25, lsl #3]
   4247c:	subs	x20, x25, x23
   42480:	sub	x28, x28, x21
   42484:	sub	x21, x8, x9
   42488:	b.ne	424b0 <__gmpn_mu_divappr_q@@Base+0x400>  // b.any
   4248c:	mov	x0, x22
   42490:	mov	x1, x28
   42494:	mov	x2, x24
   42498:	mov	x3, x23
   4249c:	bl	c420 <__gmpn_sub_n@plt>
   424a0:	mov	x20, x0
   424a4:	subs	x19, x21, x20
   424a8:	b.ne	424f8 <__gmpn_mu_divappr_q@@Base+0x448>  // b.any
   424ac:	b	4252c <__gmpn_mu_divappr_q@@Base+0x47c>
   424b0:	mov	x0, x24
   424b4:	mov	x1, x28
   424b8:	mov	x2, x24
   424bc:	mov	x3, x23
   424c0:	bl	c420 <__gmpn_sub_n@plt>
   424c4:	mov	x4, x0
   424c8:	ldur	x0, [x29, #-32]
   424cc:	mov	x1, x22
   424d0:	mov	x3, x20
   424d4:	mov	x2, x0
   424d8:	bl	c8f0 <__gmpn_sub_nc@plt>
   424dc:	mov	x20, x0
   424e0:	mov	x0, x22
   424e4:	mov	x1, x24
   424e8:	mov	x2, x25
   424ec:	bl	cc10 <__gmpn_copyi@plt>
   424f0:	subs	x19, x21, x20
   424f4:	b.eq	4252c <__gmpn_mu_divappr_q@@Base+0x47c>  // b.none
   424f8:	mov	x8, x27
   424fc:	ldr	x9, [x8]
   42500:	adds	x9, x9, #0x1
   42504:	str	x9, [x8], #8
   42508:	b.cs	424fc <__gmpn_mu_divappr_q@@Base+0x44c>  // b.hs, b.nlast
   4250c:	mov	x0, x22
   42510:	mov	x1, x22
   42514:	mov	x2, x26
   42518:	mov	x3, x25
   4251c:	bl	c420 <__gmpn_sub_n@plt>
   42520:	subs	x19, x19, x0
   42524:	b.ne	424f8 <__gmpn_mu_divappr_q@@Base+0x448>  // b.any
   42528:	mov	x20, x0
   4252c:	mov	x0, x22
   42530:	mov	x1, x26
   42534:	mov	x2, x25
   42538:	bl	c570 <__gmpn_cmp@plt>
   4253c:	ldur	x19, [x29, #-24]
   42540:	tbnz	w0, #31, 42320 <__gmpn_mu_divappr_q@@Base+0x270>
   42544:	mov	x8, x27
   42548:	ldr	x9, [x8]
   4254c:	adds	x9, x9, #0x1
   42550:	str	x9, [x8], #8
   42554:	b.cs	42548 <__gmpn_mu_divappr_q@@Base+0x498>  // b.hs, b.nlast
   42558:	mov	x0, x22
   4255c:	mov	x1, x22
   42560:	mov	x2, x26
   42564:	mov	x3, x25
   42568:	bl	c420 <__gmpn_sub_n@plt>
   4256c:	mov	x20, x0
   42570:	b	42320 <__gmpn_mu_divappr_q@@Base+0x270>
   42574:	mov	x0, x22
   42578:	mov	x1, x28
   4257c:	mov	x2, x25
   42580:	bl	cc10 <__gmpn_copyi@plt>
   42584:	cbnz	x21, 422dc <__gmpn_mu_divappr_q@@Base+0x22c>
   42588:	ldr	x0, [sp, #40]
   4258c:	b	425e4 <__gmpn_mu_divappr_q@@Base+0x534>
   42590:	b	42598 <__gmpn_mu_divappr_q@@Base+0x4e8>
   42594:	mov	x20, x0
   42598:	ldr	x2, [sp, #32]
   4259c:	mov	w3, #0x3                   	// #3
   425a0:	mov	x0, x27
   425a4:	mov	x1, x27
   425a8:	bl	c150 <__gmpn_add_1@plt>
   425ac:	ldr	x19, [sp, #40]
   425b0:	cmn	x20, x0
   425b4:	csinc	x0, x19, xzr, eq  // eq = none
   425b8:	b.eq	425e4 <__gmpn_mu_divappr_q@@Base+0x534>  // b.none
   425bc:	ldr	w8, [sp, #20]
   425c0:	tbnz	w8, #31, 425e4 <__gmpn_mu_divappr_q@@Base+0x534>
   425c4:	ldr	x8, [sp, #32]
   425c8:	cmp	x8, #0x1
   425cc:	b.lt	425e0 <__gmpn_mu_divappr_q@@Base+0x530>  // b.tstop
   425d0:	ldr	x2, [sp, #8]
   425d4:	mov	w1, #0xff                  	// #255
   425d8:	mov	x0, x27
   425dc:	bl	c780 <memset@plt>
   425e0:	mov	x0, x19
   425e4:	ldp	x20, x19, [sp, #192]
   425e8:	ldp	x22, x21, [sp, #176]
   425ec:	ldp	x24, x23, [sp, #160]
   425f0:	ldp	x26, x25, [sp, #144]
   425f4:	ldp	x28, x27, [sp, #128]
   425f8:	ldp	x29, x30, [sp, #112]
   425fc:	add	sp, sp, #0xd0
   42600:	ret
   42604:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   42608:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   4260c:	add	x0, x0, #0x228
   42610:	add	x2, x2, #0x1f9
   42614:	mov	w1, #0xd0                  	// #208
   42618:	bl	c850 <__gmp_assert_fail@plt>
   4261c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   42620:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   42624:	add	x0, x0, #0x228
   42628:	add	x2, x2, #0x21f
   4262c:	mov	w1, #0xe6                  	// #230
   42630:	bl	c850 <__gmp_assert_fail@plt>

0000000000042634 <__gmpn_mu_divappr_q_itch@@Base>:
   42634:	stp	x29, x30, [sp, #-48]!
   42638:	sub	x0, x0, x1
   4263c:	add	x8, x0, #0x1
   42640:	cmp	x8, x1
   42644:	stp	x20, x19, [sp, #32]
   42648:	csinc	x19, x1, x0, ge  // ge = tcont
   4264c:	mov	x1, x19
   42650:	str	x21, [sp, #16]
   42654:	mov	x29, sp
   42658:	bl	42204 <__gmpn_mu_divappr_q@@Base+0x154>
   4265c:	mov	x20, x0
   42660:	add	x0, x19, #0x1
   42664:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   42668:	mov	x1, x19
   4266c:	mov	x2, x20
   42670:	mov	x21, x0
   42674:	bl	426a4 <__gmpn_mu_divappr_q_itch@@Base+0x70>
   42678:	add	x8, x20, x20, lsl #1
   4267c:	add	x9, x21, x19
   42680:	add	x8, x8, #0x4
   42684:	add	x9, x9, x0
   42688:	cmp	x9, x8
   4268c:	csel	x8, x9, x8, gt
   42690:	add	x0, x8, x20
   42694:	ldp	x20, x19, [sp, #32]
   42698:	ldr	x21, [sp, #16]
   4269c:	ldp	x29, x30, [sp], #48
   426a0:	ret
   426a4:	asr	x8, x0, #1
   426a8:	cmp	x8, x2
   426ac:	csel	x9, x0, x8, lt  // lt = tstop
   426b0:	cmp	x8, x1
   426b4:	csel	x8, x9, xzr, lt  // lt = tstop
   426b8:	add	x8, x0, x8
   426bc:	add	x0, x8, #0x4
   426c0:	ret

00000000000426c4 <__gmpn_mu_div_q@@Base>:
   426c4:	sub	sp, sp, #0x80
   426c8:	stp	x20, x19, [sp, #112]
   426cc:	sub	x19, x2, x4
   426d0:	stp	x29, x30, [sp, #32]
   426d4:	add	x29, sp, #0x20
   426d8:	add	x20, x19, #0x1
   426dc:	stp	x28, x27, [sp, #48]
   426e0:	mov	x28, x1
   426e4:	mov	x27, x0
   426e8:	lsl	x1, x20, #3
   426ec:	sub	x0, x29, #0x8
   426f0:	stp	x26, x25, [sp, #64]
   426f4:	stp	x24, x23, [sp, #80]
   426f8:	stp	x22, x21, [sp, #96]
   426fc:	mov	x21, x5
   42700:	mov	x24, x4
   42704:	mov	x25, x3
   42708:	mov	x22, x2
   4270c:	stur	xzr, [x29, #-8]
   42710:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   42714:	cmp	x19, x24
   42718:	mov	x26, x0
   4271c:	b.ge	42770 <__gmpn_mu_div_q@@Base+0xac>  // b.tcont
   42720:	lsl	x9, x19, #1
   42724:	mov	x10, #0xfffffffffffffffe    	// #-2
   42728:	add	x8, x28, x22, lsl #3
   4272c:	add	x11, x25, x24, lsl #3
   42730:	mvn	x12, x19
   42734:	add	x2, x9, #0x2
   42738:	sub	x9, x10, x9
   4273c:	add	x1, x8, x9, lsl #3
   42740:	add	x3, x11, x12, lsl #3
   42744:	mov	x0, x26
   42748:	mov	x4, x20
   4274c:	mov	x5, x21
   42750:	bl	c8a0 <__gmpn_mu_divappr_q@plt>
   42754:	ldr	x8, [x26]
   42758:	mov	x21, x0
   4275c:	cmp	x8, #0x7
   42760:	b.cc	42850 <__gmpn_mu_div_q@@Base+0x18c>  // b.lo, b.ul, b.last
   42764:	add	x1, x26, #0x8
   42768:	mov	x0, x27
   4276c:	b	4281c <__gmpn_mu_div_q@@Base+0x158>
   42770:	add	x23, x22, #0x1
   42774:	lsl	x1, x23, #3
   42778:	sub	x0, x29, #0x8
   4277c:	stp	x27, x21, [sp]
   42780:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   42784:	add	x20, x0, #0x8
   42788:	mov	x27, x0
   4278c:	mov	x0, x20
   42790:	mov	x1, x28
   42794:	mov	x2, x22
   42798:	str	x28, [sp, #16]
   4279c:	bl	cc10 <__gmpn_copyi@plt>
   427a0:	add	x8, x20, x22, lsl #3
   427a4:	sub	x21, x8, x24, lsl #3
   427a8:	mov	x0, x21
   427ac:	mov	x1, x25
   427b0:	mov	x2, x24
   427b4:	str	xzr, [x27]
   427b8:	bl	c570 <__gmpn_cmp@plt>
   427bc:	mov	w20, w0
   427c0:	mvn	w28, w0
   427c4:	tbnz	w0, #31, 427dc <__gmpn_mu_div_q@@Base+0x118>
   427c8:	mov	x0, x21
   427cc:	mov	x1, x21
   427d0:	mov	x2, x25
   427d4:	mov	x3, x24
   427d8:	bl	c420 <__gmpn_sub_n@plt>
   427dc:	ldr	x5, [sp, #8]
   427e0:	mov	x0, x26
   427e4:	mov	x1, x27
   427e8:	mov	x2, x23
   427ec:	mov	x3, x25
   427f0:	mov	x4, x24
   427f4:	lsr	w21, w28, #31
   427f8:	bl	c8a0 <__gmpn_mu_divappr_q@plt>
   427fc:	cbz	x0, 42804 <__gmpn_mu_div_q@@Base+0x140>
   42800:	tbz	x19, #63, 4293c <__gmpn_mu_div_q@@Base+0x278>
   42804:	ldr	x8, [x26], #8
   42808:	ldr	x23, [sp, #16]
   4280c:	cmp	x8, #0x5
   42810:	b.cc	428bc <__gmpn_mu_div_q@@Base+0x1f8>  // b.lo, b.ul, b.last
   42814:	ldr	x0, [sp]
   42818:	mov	x1, x26
   4281c:	mov	x2, x19
   42820:	bl	cc10 <__gmpn_copyi@plt>
   42824:	ldur	x0, [x29, #-8]
   42828:	cbnz	x0, 42928 <__gmpn_mu_div_q@@Base+0x264>
   4282c:	mov	x0, x21
   42830:	ldp	x20, x19, [sp, #112]
   42834:	ldp	x22, x21, [sp, #96]
   42838:	ldp	x24, x23, [sp, #80]
   4283c:	ldp	x26, x25, [sp, #64]
   42840:	ldp	x28, x27, [sp, #48]
   42844:	ldp	x29, x30, [sp, #32]
   42848:	add	sp, sp, #0x80
   4284c:	ret
   42850:	lsl	x1, x22, #3
   42854:	sub	x0, x29, #0x8
   42858:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4285c:	add	x20, x26, #0x8
   42860:	mov	x1, x25
   42864:	mov	x2, x24
   42868:	mov	x3, x20
   4286c:	mov	x4, x19
   42870:	mov	x23, x0
   42874:	bl	cea0 <__gmpn_mul@plt>
   42878:	cbz	x21, 42894 <__gmpn_mu_div_q@@Base+0x1d0>
   4287c:	add	x0, x23, x19, lsl #3
   42880:	mov	x1, x0
   42884:	mov	x2, x25
   42888:	mov	x3, x24
   4288c:	bl	cc30 <__gmpn_add_n@plt>
   42890:	cbnz	x0, 428ac <__gmpn_mu_div_q@@Base+0x1e8>
   42894:	mov	x0, x23
   42898:	mov	x1, x28
   4289c:	mov	x2, x22
   428a0:	bl	c570 <__gmpn_cmp@plt>
   428a4:	cmp	w0, #0x1
   428a8:	b.lt	42930 <__gmpn_mu_div_q@@Base+0x26c>  // b.tstop
   428ac:	mov	w3, #0x1                   	// #1
   428b0:	mov	x0, x27
   428b4:	mov	x1, x20
   428b8:	b	42914 <__gmpn_mu_div_q@@Base+0x250>
   428bc:	mov	x0, x27
   428c0:	mov	x1, x26
   428c4:	mov	x2, x19
   428c8:	mov	x3, x25
   428cc:	mov	x4, x24
   428d0:	bl	cea0 <__gmpn_mul@plt>
   428d4:	tbnz	w20, #31, 428f0 <__gmpn_mu_div_q@@Base+0x22c>
   428d8:	add	x0, x27, x19, lsl #3
   428dc:	mov	x1, x0
   428e0:	mov	x2, x25
   428e4:	mov	x3, x24
   428e8:	bl	cc30 <__gmpn_add_n@plt>
   428ec:	cbnz	x0, 42908 <__gmpn_mu_div_q@@Base+0x244>
   428f0:	mov	x0, x27
   428f4:	mov	x1, x23
   428f8:	mov	x2, x22
   428fc:	bl	c570 <__gmpn_cmp@plt>
   42900:	cmp	w0, #0x1
   42904:	b.lt	42814 <__gmpn_mu_div_q@@Base+0x150>  // b.tstop
   42908:	ldr	x0, [sp]
   4290c:	mov	w3, #0x1                   	// #1
   42910:	mov	x1, x26
   42914:	mov	x2, x19
   42918:	bl	caf0 <__gmpn_sub_1@plt>
   4291c:	sub	x21, x21, x0
   42920:	ldur	x0, [x29, #-8]
   42924:	cbz	x0, 4282c <__gmpn_mu_div_q@@Base+0x168>
   42928:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   4292c:	b	4282c <__gmpn_mu_div_q@@Base+0x168>
   42930:	mov	x0, x27
   42934:	mov	x1, x20
   42938:	b	4281c <__gmpn_mu_div_q@@Base+0x158>
   4293c:	lsl	x8, x19, #3
   42940:	add	x2, x8, #0x8
   42944:	mov	w1, #0xff                  	// #255
   42948:	mov	x0, x26
   4294c:	bl	c780 <memset@plt>
   42950:	b	42804 <__gmpn_mu_div_q@@Base+0x140>

0000000000042954 <__gmpn_mu_div_q_itch@@Base>:
   42954:	stp	x29, x30, [sp, #-16]!
   42958:	sub	x8, x0, x1
   4295c:	lsl	x9, x8, #1
   42960:	cmp	x8, x1
   42964:	add	x9, x9, #0x2
   42968:	csinc	x1, x1, x8, ge  // ge = tcont
   4296c:	csinc	x0, x9, x0, lt  // lt = tstop
   42970:	mov	x29, sp
   42974:	bl	c220 <__gmpn_mu_divappr_q_itch@plt>
   42978:	ldp	x29, x30, [sp], #16
   4297c:	ret

0000000000042980 <__gmpn_bdiv_q_1@@Base>:
   42980:	rbit	x6, x3
   42984:	clz	x5, x6
   42988:	lsr	x3, x3, x5
   4298c:	adrp	x7, 69000 <__gmp_limbroots_table@@Base+0x11338>
   42990:	ubfx	x6, x3, #1, #7
   42994:	ldr	x7, [x7, #3952]
   42998:	ldrb	w6, [x7, x6]
   4299c:	ubfiz	x7, x6, #1, #8
   429a0:	umull	x6, w6, w6
   429a4:	msub	x6, x6, x3, x7
   429a8:	lsl	x7, x6, #1
   429ac:	mul	x6, x6, x6
   429b0:	msub	x6, x6, x3, x7
   429b4:	lsl	x7, x6, #1
   429b8:	mul	x6, x6, x6
   429bc:	msub	x4, x6, x3, x7
   429c0:	b	c650 <__gmpn_pi1_bdiv_q_1@plt>
   429c4:	nop

00000000000429c8 <__gmpn_pi1_bdiv_q_1@@Base>:
   429c8:	sub	x2, x2, #0x1
   429cc:	subs	x6, x6, x6
   429d0:	ldr	x9, [x1], #8
   429d4:	cbz	x5, 42a1c <__gmpn_pi1_bdiv_q_1@@Base+0x54>
   429d8:	lsr	x12, x9, x5
   429dc:	cbz	x2, 42a0c <__gmpn_pi1_bdiv_q_1@@Base+0x44>
   429e0:	neg	x8, x5
   429e4:	ldr	x9, [x1], #8
   429e8:	lsl	x7, x9, x8
   429ec:	orr	x7, x7, x12
   429f0:	sbcs	x6, x7, x6
   429f4:	mul	x7, x6, x4
   429f8:	str	x7, [x0], #8
   429fc:	lsr	x12, x9, x5
   42a00:	umulh	x6, x7, x3
   42a04:	sub	x2, x2, #0x1
   42a08:	cbnz	x2, 429e4 <__gmpn_pi1_bdiv_q_1@@Base+0x1c>
   42a0c:	sbcs	x6, x12, x6
   42a10:	mul	x6, x6, x4
   42a14:	str	x6, [x0]
   42a18:	ret
   42a1c:	mul	x5, x9, x4
   42a20:	str	x5, [x0], #8
   42a24:	cbz	x2, 42a44 <__gmpn_pi1_bdiv_q_1@@Base+0x7c>
   42a28:	ldr	x9, [x1], #8
   42a2c:	umulh	x5, x5, x3
   42a30:	sbcs	x5, x9, x5
   42a34:	mul	x5, x5, x4
   42a38:	str	x5, [x0], #8
   42a3c:	sub	x2, x2, #0x1
   42a40:	cbnz	x2, 42a28 <__gmpn_pi1_bdiv_q_1@@Base+0x60>
   42a44:	ret

0000000000042a48 <__gmpn_sbpi1_bdiv_q@@Base>:
   42a48:	stp	x29, x30, [sp, #-96]!
   42a4c:	stp	x26, x25, [sp, #32]
   42a50:	stp	x24, x23, [sp, #48]
   42a54:	stp	x22, x21, [sp, #64]
   42a58:	stp	x20, x19, [sp, #80]
   42a5c:	mov	x20, x5
   42a60:	mov	x23, x4
   42a64:	mov	x21, x3
   42a68:	mov	x22, x1
   42a6c:	subs	x26, x2, x4
   42a70:	mov	x19, x0
   42a74:	stp	x28, x27, [sp, #16]
   42a78:	mov	x29, sp
   42a7c:	b.le	42b18 <__gmpn_sbpi1_bdiv_q@@Base+0xd0>
   42a80:	ldr	x8, [x22]
   42a84:	mvn	x9, x23
   42a88:	add	x25, x9, x2
   42a8c:	mov	x0, x22
   42a90:	mul	x24, x8, x20
   42a94:	mov	x1, x21
   42a98:	mov	x2, x23
   42a9c:	mov	x3, x24
   42aa0:	bl	d5e0 <__gmpn_addmul_1@plt>
   42aa4:	cmp	x25, #0x1
   42aa8:	lsl	x25, x23, #3
   42aac:	mov	x27, xzr
   42ab0:	b.lt	42b00 <__gmpn_sbpi1_bdiv_q@@Base+0xb8>  // b.tstop
   42ab4:	mov	w28, #0x2                   	// #2
   42ab8:	str	x24, [x19], #8
   42abc:	ldr	x8, [x22, x25]
   42ac0:	adds	x9, x0, x27
   42ac4:	cset	w10, cs  // cs = hs, nlast
   42ac8:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   42acc:	adds	x8, x8, x9
   42ad0:	str	x8, [x22, x25]
   42ad4:	ldr	x8, [x22, #8]!
   42ad8:	mov	x1, x21
   42adc:	mov	x2, x23
   42ae0:	csel	x27, x10, x11, cc  // cc = lo, ul, last
   42ae4:	mul	x24, x8, x20
   42ae8:	mov	x0, x22
   42aec:	mov	x3, x24
   42af0:	bl	d5e0 <__gmpn_addmul_1@plt>
   42af4:	sub	x26, x26, #0x1
   42af8:	cmp	x26, #0x1
   42afc:	b.gt	42ab8 <__gmpn_sbpi1_bdiv_q@@Base+0x70>
   42b00:	str	x24, [x19], #8
   42b04:	ldr	x8, [x22, x25]
   42b08:	add	x9, x0, x27
   42b0c:	add	x8, x9, x8
   42b10:	str	x8, [x22, x25]
   42b14:	add	x22, x22, #0x8
   42b18:	ldr	x8, [x22]
   42b1c:	cmp	x23, #0x2
   42b20:	mul	x24, x8, x20
   42b24:	b.lt	42b54 <__gmpn_sbpi1_bdiv_q@@Base+0x10c>  // b.tstop
   42b28:	mov	x0, x22
   42b2c:	mov	x1, x21
   42b30:	mov	x2, x23
   42b34:	mov	x3, x24
   42b38:	bl	d5e0 <__gmpn_addmul_1@plt>
   42b3c:	str	x24, [x19], #8
   42b40:	ldr	x8, [x22, #8]!
   42b44:	cmp	x23, #0x2
   42b48:	sub	x23, x23, #0x1
   42b4c:	mul	x24, x8, x20
   42b50:	b.gt	42b28 <__gmpn_sbpi1_bdiv_q@@Base+0xe0>
   42b54:	str	x24, [x19]
   42b58:	ldp	x20, x19, [sp, #80]
   42b5c:	ldp	x22, x21, [sp, #64]
   42b60:	ldp	x24, x23, [sp, #48]
   42b64:	ldp	x26, x25, [sp, #32]
   42b68:	ldp	x28, x27, [sp, #16]
   42b6c:	ldp	x29, x30, [sp], #96
   42b70:	ret

0000000000042b74 <__gmpn_sbpi1_bdiv_qr@@Base>:
   42b74:	stp	x29, x30, [sp, #-96]!
   42b78:	cmp	x2, x4
   42b7c:	stp	x28, x27, [sp, #16]
   42b80:	stp	x26, x25, [sp, #32]
   42b84:	stp	x24, x23, [sp, #48]
   42b88:	stp	x22, x21, [sp, #64]
   42b8c:	stp	x20, x19, [sp, #80]
   42b90:	mov	x29, sp
   42b94:	b.eq	42c0c <__gmpn_sbpi1_bdiv_qr@@Base+0x98>  // b.none
   42b98:	mov	x19, x5
   42b9c:	mov	x20, x4
   42ba0:	mov	x21, x3
   42ba4:	mov	x22, x2
   42ba8:	mov	x23, x1
   42bac:	mov	x24, x0
   42bb0:	mov	x25, xzr
   42bb4:	lsl	x27, x4, #3
   42bb8:	mov	w28, #0x2                   	// #2
   42bbc:	ldr	x8, [x23]
   42bc0:	mov	x0, x23
   42bc4:	mov	x1, x21
   42bc8:	mov	x2, x20
   42bcc:	mul	x26, x8, x19
   42bd0:	mov	x3, x26
   42bd4:	bl	d5e0 <__gmpn_addmul_1@plt>
   42bd8:	str	x26, [x24], #8
   42bdc:	ldr	x9, [x23, x27]
   42be0:	adds	x8, x0, x25
   42be4:	sub	x22, x22, #0x1
   42be8:	cset	w10, cs  // cs = hs, nlast
   42bec:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   42bf0:	adds	x8, x9, x8
   42bf4:	csel	x25, x10, x11, cc  // cc = lo, ul, last
   42bf8:	str	x8, [x23, x27]
   42bfc:	cmp	x20, x22
   42c00:	add	x23, x23, #0x8
   42c04:	b.ne	42bbc <__gmpn_sbpi1_bdiv_qr@@Base+0x48>  // b.any
   42c08:	b	42c10 <__gmpn_sbpi1_bdiv_qr@@Base+0x9c>
   42c0c:	mov	x25, xzr
   42c10:	mov	x0, x25
   42c14:	ldp	x20, x19, [sp, #80]
   42c18:	ldp	x22, x21, [sp, #64]
   42c1c:	ldp	x24, x23, [sp, #48]
   42c20:	ldp	x26, x25, [sp, #32]
   42c24:	ldp	x28, x27, [sp, #16]
   42c28:	ldp	x29, x30, [sp], #96
   42c2c:	ret

0000000000042c30 <__gmpn_sbpi1_bdiv_r@@Base>:
   42c30:	stp	x29, x30, [sp, #-80]!
   42c34:	cmp	x1, x3
   42c38:	stp	x26, x25, [sp, #16]
   42c3c:	stp	x24, x23, [sp, #32]
   42c40:	stp	x22, x21, [sp, #48]
   42c44:	stp	x20, x19, [sp, #64]
   42c48:	mov	x29, sp
   42c4c:	b.eq	42cb8 <__gmpn_sbpi1_bdiv_r@@Base+0x88>  // b.none
   42c50:	mov	x19, x4
   42c54:	mov	x20, x3
   42c58:	mov	x21, x2
   42c5c:	mov	x22, x1
   42c60:	mov	x23, x0
   42c64:	mov	x24, xzr
   42c68:	lsl	x25, x3, #3
   42c6c:	mov	w26, #0x2                   	// #2
   42c70:	ldr	x8, [x23]
   42c74:	mov	x0, x23
   42c78:	mov	x1, x21
   42c7c:	mov	x2, x20
   42c80:	mul	x3, x8, x19
   42c84:	bl	d5e0 <__gmpn_addmul_1@plt>
   42c88:	ldr	x9, [x23, x25]
   42c8c:	adds	x8, x0, x24
   42c90:	sub	x22, x22, #0x1
   42c94:	cset	w10, cs  // cs = hs, nlast
   42c98:	csinc	x11, x26, xzr, cs  // cs = hs, nlast
   42c9c:	adds	x8, x8, x9
   42ca0:	csel	x24, x10, x11, cc  // cc = lo, ul, last
   42ca4:	str	x8, [x23, x25]
   42ca8:	cmp	x20, x22
   42cac:	add	x23, x23, #0x8
   42cb0:	b.ne	42c70 <__gmpn_sbpi1_bdiv_r@@Base+0x40>  // b.any
   42cb4:	b	42cbc <__gmpn_sbpi1_bdiv_r@@Base+0x8c>
   42cb8:	mov	x24, xzr
   42cbc:	mov	x0, x24
   42cc0:	ldp	x20, x19, [sp, #64]
   42cc4:	ldp	x22, x21, [sp, #48]
   42cc8:	ldp	x24, x23, [sp, #32]
   42ccc:	ldp	x26, x25, [sp, #16]
   42cd0:	ldp	x29, x30, [sp], #80
   42cd4:	ret

0000000000042cd8 <__gmpn_dcpi1_bdiv_q@@Base>:
   42cd8:	stp	x29, x30, [sp, #-96]!
   42cdc:	stp	x28, x27, [sp, #16]
   42ce0:	stp	x26, x25, [sp, #32]
   42ce4:	stp	x24, x23, [sp, #48]
   42ce8:	stp	x22, x21, [sp, #64]
   42cec:	stp	x20, x19, [sp, #80]
   42cf0:	mov	x29, sp
   42cf4:	sub	sp, sp, #0x30
   42cf8:	lsl	x10, x4, #3
   42cfc:	add	x9, x10, #0xf
   42d00:	mov	x8, sp
   42d04:	and	x9, x9, #0xfffffffffffffff0
   42d08:	mov	x21, x5
   42d0c:	mov	x28, x3
   42d10:	mov	x26, x2
   42d14:	mov	x19, x1
   42d18:	mov	x23, x0
   42d1c:	sub	x24, x8, x9
   42d20:	mov	sp, x24
   42d24:	cmp	x2, x4
   42d28:	b.le	42d84 <__gmpn_dcpi1_bdiv_q@@Base+0xac>
   42d2c:	mov	x22, x4
   42d30:	mov	x25, xzr
   42d34:	lsl	x20, x26, #3
   42d38:	lsl	x2, x26, #1
   42d3c:	lsl	x8, x4, #1
   42d40:	sub	x26, x26, x22
   42d44:	sub	x20, x20, x10
   42d48:	sub	x2, x2, x8
   42d4c:	cmp	x26, x22
   42d50:	add	x25, x25, x22
   42d54:	b.gt	42d40 <__gmpn_dcpi1_bdiv_q@@Base+0x68>
   42d58:	cmp	x26, #0x26
   42d5c:	mov	x0, x23
   42d60:	mov	x1, x19
   42d64:	stur	x10, [x29, #-40]
   42d68:	b.le	42da0 <__gmpn_dcpi1_bdiv_q@@Base+0xc8>
   42d6c:	mov	x2, x28
   42d70:	mov	x3, x26
   42d74:	mov	x4, x21
   42d78:	mov	x5, x24
   42d7c:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   42d80:	b	42db0 <__gmpn_dcpi1_bdiv_q@@Base+0xd8>
   42d84:	mov	x0, x23
   42d88:	mov	x1, x19
   42d8c:	cmp	x26, #0x5c
   42d90:	b.le	42dd8 <__gmpn_dcpi1_bdiv_q@@Base+0x100>
   42d94:	mov	x2, x28
   42d98:	mov	x3, x26
   42d9c:	b	42efc <__gmpn_dcpi1_bdiv_q@@Base+0x224>
   42da0:	mov	x3, x28
   42da4:	mov	x4, x26
   42da8:	mov	x5, x21
   42dac:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   42db0:	mov	x27, x0
   42db4:	subs	x4, x22, x26
   42db8:	b.eq	42e48 <__gmpn_dcpi1_bdiv_q@@Base+0x170>  // b.none
   42dbc:	cmp	x26, x4
   42dc0:	add	x3, x28, x20
   42dc4:	mov	x0, x24
   42dc8:	b.le	42df0 <__gmpn_dcpi1_bdiv_q@@Base+0x118>
   42dcc:	mov	x1, x23
   42dd0:	mov	x2, x26
   42dd4:	b	42e00 <__gmpn_dcpi1_bdiv_q@@Base+0x128>
   42dd8:	mov	x2, x26
   42ddc:	mov	x3, x28
   42de0:	mov	x4, x26
   42de4:	mov	x5, x21
   42de8:	bl	c680 <__gmpn_sbpi1_bdiv_q@plt>
   42dec:	b	42f08 <__gmpn_dcpi1_bdiv_q@@Base+0x230>
   42df0:	mov	x1, x3
   42df4:	mov	x2, x4
   42df8:	mov	x3, x23
   42dfc:	mov	x4, x26
   42e00:	bl	cea0 <__gmpn_mul@plt>
   42e04:	ldr	x8, [x24, x20]
   42e08:	adds	x8, x8, x27
   42e0c:	str	x8, [x24, x20]
   42e10:	b.cc	42e2c <__gmpn_dcpi1_bdiv_q@@Base+0x154>  // b.lo, b.ul, b.last
   42e14:	add	x8, x24, x20
   42e18:	add	x8, x8, #0x8
   42e1c:	ldr	x9, [x8]
   42e20:	adds	x9, x9, #0x1
   42e24:	str	x9, [x8], #8
   42e28:	b.cs	42e1c <__gmpn_dcpi1_bdiv_q@@Base+0x144>  // b.hs, b.nlast
   42e2c:	add	x0, x19, x20
   42e30:	mov	x1, x0
   42e34:	mov	x2, x25
   42e38:	mov	x3, x24
   42e3c:	mov	x4, x22
   42e40:	bl	c970 <__gmpn_add@plt>
   42e44:	mov	x27, xzr
   42e48:	ldur	x8, [x29, #-40]
   42e4c:	cmp	x25, x22
   42e50:	b.le	42eec <__gmpn_dcpi1_bdiv_q@@Base+0x214>
   42e54:	add	x9, x19, x8
   42e58:	stp	x9, x21, [x29, #-24]
   42e5c:	neg	x9, x22
   42e60:	stur	x19, [x29, #-8]
   42e64:	stur	x9, [x29, #-32]
   42e68:	ldur	x9, [x29, #-8]
   42e6c:	mov	x3, x27
   42e70:	mov	x19, x24
   42e74:	mov	x24, x28
   42e78:	add	x26, x9, x20
   42e7c:	ldur	x9, [x29, #-24]
   42e80:	add	x28, x23, x20
   42e84:	sub	x21, x25, x22
   42e88:	add	x0, x9, x20
   42e8c:	ldur	x9, [x29, #-32]
   42e90:	mov	x1, x0
   42e94:	add	x2, x9, x25
   42e98:	mov	x25, x8
   42e9c:	bl	c150 <__gmpn_add_1@plt>
   42ea0:	ldur	x4, [x29, #-16]
   42ea4:	mov	x0, x28
   42ea8:	mov	x28, x24
   42eac:	mov	x1, x26
   42eb0:	mov	x2, x28
   42eb4:	mov	x3, x22
   42eb8:	mov	x5, x19
   42ebc:	mov	x24, x19
   42ec0:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   42ec4:	mov	x8, x25
   42ec8:	mov	x27, x0
   42ecc:	cmp	x21, x22
   42ed0:	add	x20, x20, x25
   42ed4:	mov	x25, x21
   42ed8:	b.gt	42e68 <__gmpn_dcpi1_bdiv_q@@Base+0x190>
   42edc:	ldp	x21, x8, [x29, #-16]
   42ee0:	add	x0, x23, x20
   42ee4:	add	x1, x8, x20
   42ee8:	b	42ef4 <__gmpn_dcpi1_bdiv_q@@Base+0x21c>
   42eec:	add	x1, x19, x20
   42ef0:	add	x0, x23, x20
   42ef4:	mov	x2, x28
   42ef8:	mov	x3, x22
   42efc:	mov	x4, x21
   42f00:	mov	x5, x24
   42f04:	bl	42f28 <__gmpn_dcpi1_bdiv_q@@Base+0x250>
   42f08:	mov	sp, x29
   42f0c:	ldp	x20, x19, [sp, #80]
   42f10:	ldp	x22, x21, [sp, #64]
   42f14:	ldp	x24, x23, [sp, #48]
   42f18:	ldp	x26, x25, [sp, #32]
   42f1c:	ldp	x28, x27, [sp, #16]
   42f20:	ldp	x29, x30, [sp], #96
   42f24:	ret
   42f28:	stp	x29, x30, [sp, #-96]!
   42f2c:	stp	x26, x25, [sp, #32]
   42f30:	stp	x22, x21, [sp, #64]
   42f34:	stp	x20, x19, [sp, #80]
   42f38:	mov	x19, x4
   42f3c:	mov	x25, x3
   42f40:	mov	x20, x2
   42f44:	mov	x21, x1
   42f48:	cmp	x3, #0x5d
   42f4c:	mov	x22, x0
   42f50:	stp	x28, x27, [sp, #16]
   42f54:	stp	x24, x23, [sp, #48]
   42f58:	mov	x29, sp
   42f5c:	b.lt	43008 <__gmpn_dcpi1_bdiv_q@@Base+0x330>  // b.tstop
   42f60:	mov	x23, x5
   42f64:	b	42f7c <__gmpn_dcpi1_bdiv_q@@Base+0x2a4>
   42f68:	add	x22, x22, x28
   42f6c:	cmp	x24, #0x5c
   42f70:	add	x21, x21, x28
   42f74:	mov	x25, x24
   42f78:	b.le	4300c <__gmpn_dcpi1_bdiv_q@@Base+0x334>
   42f7c:	asr	x26, x25, #1
   42f80:	mov	x0, x22
   42f84:	mov	x1, x21
   42f88:	mov	x2, x20
   42f8c:	mov	x3, x26
   42f90:	mov	x4, x19
   42f94:	mov	x5, x23
   42f98:	sub	x24, x25, x26
   42f9c:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   42fa0:	lsl	x28, x24, #3
   42fa4:	mov	x27, x0
   42fa8:	add	x2, x20, x28
   42fac:	mov	x0, x23
   42fb0:	mov	x1, x22
   42fb4:	mov	x3, x26
   42fb8:	bl	d090 <__gmpn_mullo_n@plt>
   42fbc:	add	x0, x21, x28
   42fc0:	mov	x1, x0
   42fc4:	mov	x2, x23
   42fc8:	mov	x3, x26
   42fcc:	bl	cc30 <__gmpn_add_n@plt>
   42fd0:	cmp	x26, x24
   42fd4:	lsl	x28, x26, #3
   42fd8:	b.ge	42f68 <__gmpn_dcpi1_bdiv_q@@Base+0x290>  // b.tcont
   42fdc:	ldr	x3, [x20, x28]
   42fe0:	add	x0, x21, x28
   42fe4:	mov	x1, x22
   42fe8:	mov	x2, x26
   42fec:	bl	d5e0 <__gmpn_addmul_1@plt>
   42ff0:	add	x8, x21, x25, lsl #3
   42ff4:	ldur	x9, [x8, #-8]
   42ff8:	add	x10, x0, x27
   42ffc:	add	x9, x10, x9
   43000:	stur	x9, [x8, #-8]
   43004:	b	42f68 <__gmpn_dcpi1_bdiv_q@@Base+0x290>
   43008:	mov	x24, x25
   4300c:	mov	x0, x22
   43010:	mov	x1, x21
   43014:	mov	x2, x24
   43018:	mov	x3, x20
   4301c:	mov	x4, x24
   43020:	mov	x5, x19
   43024:	bl	c680 <__gmpn_sbpi1_bdiv_q@plt>
   43028:	ldp	x20, x19, [sp, #80]
   4302c:	ldp	x22, x21, [sp, #64]
   43030:	ldp	x24, x23, [sp, #48]
   43034:	ldp	x26, x25, [sp, #32]
   43038:	ldp	x28, x27, [sp, #16]
   4303c:	ldp	x29, x30, [sp], #96
   43040:	ret

0000000000043044 <__gmpn_dcpi1_bdiv_qr_n_itch@@Base>:
   43044:	ret

0000000000043048 <__gmpn_dcpi1_bdiv_qr_n@@Base>:
   43048:	sub	sp, sp, #0x70
   4304c:	stp	x24, x23, [sp, #64]
   43050:	asr	x23, x3, #1
   43054:	stp	x28, x27, [sp, #32]
   43058:	stp	x26, x25, [sp, #48]
   4305c:	stp	x22, x21, [sp, #80]
   43060:	stp	x20, x19, [sp, #96]
   43064:	mov	x19, x5
   43068:	mov	x25, x4
   4306c:	mov	x20, x3
   43070:	mov	x24, x2
   43074:	mov	x28, x1
   43078:	mov	x26, x0
   4307c:	cmp	x3, #0x4d
   43080:	sub	x22, x3, x23
   43084:	stp	x29, x30, [sp, #16]
   43088:	add	x29, sp, #0x10
   4308c:	b.le	430b0 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x68>
   43090:	mov	x0, x26
   43094:	mov	x1, x28
   43098:	mov	x2, x24
   4309c:	mov	x3, x23
   430a0:	mov	x4, x25
   430a4:	mov	x5, x19
   430a8:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   430ac:	b	430cc <__gmpn_dcpi1_bdiv_qr_n@@Base+0x84>
   430b0:	and	x2, x20, #0xfffffffffffffffe
   430b4:	mov	x0, x26
   430b8:	mov	x1, x28
   430bc:	mov	x3, x24
   430c0:	mov	x4, x23
   430c4:	mov	x5, x25
   430c8:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   430cc:	lsl	x21, x23, #3
   430d0:	mov	x27, x0
   430d4:	add	x1, x24, x21
   430d8:	mov	x0, x19
   430dc:	mov	x2, x22
   430e0:	mov	x3, x26
   430e4:	mov	x4, x23
   430e8:	bl	cea0 <__gmpn_mul@plt>
   430ec:	ldr	x8, [x19, x21]
   430f0:	adds	x8, x8, x27
   430f4:	str	x8, [x19, x21]
   430f8:	b.cc	43114 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xcc>  // b.lo, b.ul, b.last
   430fc:	add	x8, x19, x23, lsl #3
   43100:	add	x8, x8, #0x8
   43104:	ldr	x9, [x8]
   43108:	adds	x9, x9, #0x1
   4310c:	str	x9, [x8], #8
   43110:	b.cs	43104 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xbc>  // b.hs, b.nlast
   43114:	mov	x27, x28
   43118:	add	x28, x28, x21
   4311c:	add	x2, x22, x20
   43120:	mov	x0, x28
   43124:	mov	x1, x28
   43128:	mov	x3, x19
   4312c:	mov	x4, x20
   43130:	bl	c970 <__gmpn_add@plt>
   43134:	cmp	x22, #0x26
   43138:	add	x26, x26, x21
   4313c:	str	x0, [sp, #8]
   43140:	b.le	43164 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x11c>
   43144:	mov	x0, x26
   43148:	mov	x1, x28
   4314c:	mov	x2, x24
   43150:	mov	x3, x22
   43154:	mov	x4, x25
   43158:	mov	x5, x19
   4315c:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   43160:	b	43180 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x138>
   43164:	lsl	x2, x22, #1
   43168:	mov	x0, x26
   4316c:	mov	x1, x28
   43170:	mov	x3, x24
   43174:	mov	x4, x22
   43178:	mov	x5, x25
   4317c:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   43180:	lsl	x21, x22, #3
   43184:	mov	x25, x0
   43188:	add	x3, x24, x21
   4318c:	mov	x0, x19
   43190:	mov	x1, x26
   43194:	mov	x2, x22
   43198:	mov	x4, x23
   4319c:	bl	cea0 <__gmpn_mul@plt>
   431a0:	ldr	x8, [x19, x21]
   431a4:	adds	x8, x8, x25
   431a8:	str	x8, [x19, x21]
   431ac:	b.cc	431c8 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x180>  // b.lo, b.ul, b.last
   431b0:	add	x8, x19, x22, lsl #3
   431b4:	add	x8, x8, #0x8
   431b8:	ldr	x9, [x8]
   431bc:	adds	x9, x9, #0x1
   431c0:	str	x9, [x8], #8
   431c4:	b.cs	431b8 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x170>  // b.hs, b.nlast
   431c8:	add	x0, x27, x20, lsl #3
   431cc:	mov	x1, x0
   431d0:	mov	x2, x19
   431d4:	mov	x3, x20
   431d8:	bl	cc30 <__gmpn_add_n@plt>
   431dc:	ldr	x8, [sp, #8]
   431e0:	ldp	x20, x19, [sp, #96]
   431e4:	ldp	x22, x21, [sp, #80]
   431e8:	ldp	x24, x23, [sp, #64]
   431ec:	ldp	x26, x25, [sp, #48]
   431f0:	ldp	x28, x27, [sp, #32]
   431f4:	ldp	x29, x30, [sp, #16]
   431f8:	add	x0, x0, x8
   431fc:	add	sp, sp, #0x70
   43200:	ret

0000000000043204 <__gmpn_dcpi1_bdiv_qr@@Base>:
   43204:	stp	x29, x30, [sp, #-96]!
   43208:	stp	x28, x27, [sp, #16]
   4320c:	stp	x26, x25, [sp, #32]
   43210:	stp	x24, x23, [sp, #48]
   43214:	stp	x22, x21, [sp, #64]
   43218:	stp	x20, x19, [sp, #80]
   4321c:	mov	x29, sp
   43220:	sub	sp, sp, #0x30
   43224:	lsl	x10, x4, #3
   43228:	add	x9, x10, #0xf
   4322c:	mov	x8, sp
   43230:	and	x9, x9, #0xfffffffffffffff0
   43234:	mov	x19, x4
   43238:	mov	x25, x3
   4323c:	mov	x21, x2
   43240:	mov	x22, x1
   43244:	mov	x28, x0
   43248:	sub	x20, x8, x9
   4324c:	mov	sp, x20
   43250:	sub	x23, x2, x4
   43254:	cmp	x23, x4
   43258:	b.le	432b0 <__gmpn_dcpi1_bdiv_qr@@Base+0xac>
   4325c:	lsl	x8, x21, #3
   43260:	sub	x26, x8, x10
   43264:	mov	x27, x23
   43268:	stur	x21, [x29, #-16]
   4326c:	sub	x27, x27, x19
   43270:	mov	x21, x26
   43274:	cmp	x27, x19
   43278:	sub	x26, x26, x10
   4327c:	b.gt	4326c <__gmpn_dcpi1_bdiv_qr@@Base+0x68>
   43280:	cmp	x27, #0x26
   43284:	stur	x10, [x29, #-8]
   43288:	stp	x5, x28, [x29, #-40]
   4328c:	b.le	432d8 <__gmpn_dcpi1_bdiv_qr@@Base+0xd4>
   43290:	mov	x0, x28
   43294:	mov	x1, x22
   43298:	mov	x2, x25
   4329c:	mov	x3, x27
   432a0:	mov	x4, x5
   432a4:	mov	x5, x20
   432a8:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   432ac:	b	432f0 <__gmpn_dcpi1_bdiv_qr@@Base+0xec>
   432b0:	cmp	x23, #0x26
   432b4:	b.le	43310 <__gmpn_dcpi1_bdiv_qr@@Base+0x10c>
   432b8:	mov	x0, x28
   432bc:	mov	x1, x22
   432c0:	mov	x2, x25
   432c4:	mov	x3, x23
   432c8:	mov	x4, x5
   432cc:	mov	x5, x20
   432d0:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   432d4:	b	43328 <__gmpn_dcpi1_bdiv_qr@@Base+0x124>
   432d8:	lsl	x2, x27, #1
   432dc:	mov	x0, x28
   432e0:	mov	x1, x22
   432e4:	mov	x3, x25
   432e8:	mov	x4, x27
   432ec:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   432f0:	mov	x28, x0
   432f4:	cmp	x27, x19
   432f8:	b.ne	4333c <__gmpn_dcpi1_bdiv_qr@@Base+0x138>  // b.any
   432fc:	mov	x24, x25
   43300:	mov	x25, x19
   43304:	mov	x19, x20
   43308:	mov	x26, xzr
   4330c:	b	433ec <__gmpn_dcpi1_bdiv_qr@@Base+0x1e8>
   43310:	lsl	x2, x23, #1
   43314:	mov	x0, x28
   43318:	mov	x1, x22
   4331c:	mov	x3, x25
   43320:	mov	x4, x23
   43324:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   43328:	mov	x24, x0
   4332c:	cmp	x23, x19
   43330:	b.ne	43360 <__gmpn_dcpi1_bdiv_qr@@Base+0x15c>  // b.any
   43334:	mov	x0, xzr
   43338:	b	434e0 <__gmpn_dcpi1_bdiv_qr@@Base+0x2dc>
   4333c:	sub	x4, x19, x27
   43340:	cmp	x27, x4
   43344:	add	x3, x25, x27, lsl #3
   43348:	mov	x24, x25
   4334c:	mov	x0, x20
   43350:	b.le	43380 <__gmpn_dcpi1_bdiv_qr@@Base+0x17c>
   43354:	ldur	x1, [x29, #-32]
   43358:	mov	x2, x27
   4335c:	b	43390 <__gmpn_dcpi1_bdiv_qr@@Base+0x18c>
   43360:	sub	x4, x19, x23
   43364:	cmp	x23, x4
   43368:	add	x3, x25, x23, lsl #3
   4336c:	mov	x0, x20
   43370:	b.le	4347c <__gmpn_dcpi1_bdiv_qr@@Base+0x278>
   43374:	mov	x1, x28
   43378:	mov	x2, x23
   4337c:	b	4348c <__gmpn_dcpi1_bdiv_qr@@Base+0x288>
   43380:	mov	x1, x3
   43384:	ldur	x3, [x29, #-32]
   43388:	mov	x2, x4
   4338c:	mov	x4, x27
   43390:	bl	cea0 <__gmpn_mul@plt>
   43394:	lsl	x8, x27, #3
   43398:	ldr	x9, [x20, x8]
   4339c:	mov	x25, x19
   433a0:	adds	x9, x9, x28
   433a4:	str	x9, [x20, x8]
   433a8:	b.cc	433c4 <__gmpn_dcpi1_bdiv_qr@@Base+0x1c0>  // b.lo, b.ul, b.last
   433ac:	add	x8, x20, #0x8
   433b0:	ldr	x9, [x8, x26]
   433b4:	adds	x9, x9, #0x1
   433b8:	str	x9, [x8, x26]
   433bc:	add	x8, x8, #0x8
   433c0:	b.cs	433b0 <__gmpn_dcpi1_bdiv_qr@@Base+0x1ac>  // b.hs, b.nlast
   433c4:	ldur	x8, [x29, #-16]
   433c8:	add	x0, x22, x27, lsl #3
   433cc:	mov	x1, x0
   433d0:	mov	x3, x20
   433d4:	sub	x2, x8, x27
   433d8:	mov	x4, x25
   433dc:	mov	x19, x20
   433e0:	bl	c970 <__gmpn_add@plt>
   433e4:	mov	x26, x0
   433e8:	mov	x28, xzr
   433ec:	ldur	x9, [x29, #-8]
   433f0:	sub	x23, x23, x27
   433f4:	mov	x20, x25
   433f8:	sub	x8, x22, x9
   433fc:	stp	x8, x22, [x29, #-24]
   43400:	ldur	x8, [x29, #-32]
   43404:	mov	x22, x19
   43408:	ldur	x19, [x29, #-40]
   4340c:	sub	x8, x8, x9
   43410:	stur	x8, [x29, #-32]
   43414:	ldur	x8, [x29, #-32]
   43418:	mov	x2, x23
   4341c:	mov	x3, x28
   43420:	add	x25, x8, x21
   43424:	ldur	x8, [x29, #-24]
   43428:	add	x27, x8, x21
   4342c:	ldur	x8, [x29, #-16]
   43430:	add	x0, x8, x21
   43434:	mov	x1, x0
   43438:	bl	c150 <__gmpn_add_1@plt>
   4343c:	add	x26, x0, x26
   43440:	mov	x0, x25
   43444:	mov	x1, x27
   43448:	mov	x2, x24
   4344c:	mov	x3, x20
   43450:	mov	x4, x19
   43454:	mov	x5, x22
   43458:	bl	d4e0 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4345c:	ldur	x8, [x29, #-8]
   43460:	sub	x23, x23, x20
   43464:	mov	x28, x0
   43468:	cmp	x23, #0x0
   4346c:	add	x21, x21, x8
   43470:	b.gt	43414 <__gmpn_dcpi1_bdiv_qr@@Base+0x210>
   43474:	add	x0, x26, x28
   43478:	b	434e4 <__gmpn_dcpi1_bdiv_qr@@Base+0x2e0>
   4347c:	mov	x1, x3
   43480:	mov	x2, x4
   43484:	mov	x3, x28
   43488:	mov	x4, x23
   4348c:	bl	cea0 <__gmpn_mul@plt>
   43490:	lsl	x8, x23, #3
   43494:	ldr	x9, [x20, x8]
   43498:	adds	x9, x9, x24
   4349c:	str	x9, [x20, x8]
   434a0:	b.cc	434c4 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c0>  // b.lo, b.ul, b.last
   434a4:	lsl	x8, x21, #3
   434a8:	sub	x8, x8, x19, lsl #3
   434ac:	add	x8, x8, x20
   434b0:	add	x8, x8, #0x8
   434b4:	ldr	x9, [x8]
   434b8:	adds	x9, x9, #0x1
   434bc:	str	x9, [x8], #8
   434c0:	b.cs	434b4 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b0>  // b.hs, b.nlast
   434c4:	add	x0, x22, x23, lsl #3
   434c8:	mov	x1, x0
   434cc:	mov	x2, x19
   434d0:	mov	x3, x20
   434d4:	mov	x4, x19
   434d8:	bl	c970 <__gmpn_add@plt>
   434dc:	mov	x24, xzr
   434e0:	add	x0, x24, x0
   434e4:	mov	sp, x29
   434e8:	ldp	x20, x19, [sp, #80]
   434ec:	ldp	x22, x21, [sp, #64]
   434f0:	ldp	x24, x23, [sp, #48]
   434f4:	ldp	x26, x25, [sp, #32]
   434f8:	ldp	x28, x27, [sp, #16]
   434fc:	ldp	x29, x30, [sp], #96
   43500:	ret

0000000000043504 <__gmpn_mu_bdiv_q@@Base>:
   43504:	stp	x29, x30, [sp, #-32]!
   43508:	stp	x20, x19, [sp, #16]
   4350c:	mov	x29, sp
   43510:	mov	x19, x2
   43514:	mov	x20, x0
   43518:	bl	43538 <__gmpn_mu_bdiv_q@@Base+0x34>
   4351c:	mov	x0, x20
   43520:	mov	x1, x20
   43524:	mov	x2, x19
   43528:	bl	ce90 <__gmpn_neg@plt>
   4352c:	ldp	x20, x19, [sp, #16]
   43530:	ldp	x29, x30, [sp], #32
   43534:	ret
   43538:	sub	sp, sp, #0xc0
   4353c:	stp	x29, x30, [sp, #96]
   43540:	stp	x28, x27, [sp, #112]
   43544:	stp	x26, x25, [sp, #128]
   43548:	stp	x22, x21, [sp, #160]
   4354c:	stp	x20, x19, [sp, #176]
   43550:	add	x29, sp, #0x60
   43554:	mov	x26, x5
   43558:	mov	x28, x3
   4355c:	mov	x22, x2
   43560:	mov	x27, x1
   43564:	cmp	x2, x4
   43568:	mov	x20, x0
   4356c:	stp	x24, x23, [sp, #144]
   43570:	stur	x5, [x29, #-24]
   43574:	b.le	4377c <__gmpn_mu_bdiv_q@@Base+0x278>
   43578:	sub	x8, x22, #0x1
   4357c:	sdiv	x9, x8, x4
   43580:	add	x9, x9, #0x1
   43584:	sdiv	x19, x8, x9
   43588:	add	x25, x19, #0x1
   4358c:	add	x21, x26, x25, lsl #3
   43590:	mov	x0, x26
   43594:	mov	x1, x28
   43598:	mov	x2, x25
   4359c:	mov	x3, x21
   435a0:	mov	x23, x4
   435a4:	bl	cef0 <__gmpn_binvert@plt>
   435a8:	mov	x0, x21
   435ac:	mov	x1, x27
   435b0:	mov	x2, x23
   435b4:	bl	cc10 <__gmpn_copyi@plt>
   435b8:	mov	x0, x20
   435bc:	mov	x1, x21
   435c0:	mov	x2, x26
   435c4:	mov	x3, x25
   435c8:	add	x24, x27, x23, lsl #3
   435cc:	bl	d090 <__gmpn_mullo_n@plt>
   435d0:	sub	x22, x22, x25
   435d4:	cmp	x22, x25
   435d8:	lsl	x11, x23, #3
   435dc:	stur	x21, [x29, #-8]
   435e0:	stur	x19, [x29, #-32]
   435e4:	str	x11, [sp]
   435e8:	b.le	43844 <__gmpn_mu_bdiv_q@@Base+0x340>
   435ec:	add	x8, x25, x23
   435f0:	stur	x28, [x29, #-40]
   435f4:	add	x28, x21, x11
   435f8:	str	x8, [sp, #40]
   435fc:	lsl	x8, x25, #3
   43600:	sub	x9, x23, x25
   43604:	add	x12, x21, x8
   43608:	add	x8, x28, x8
   4360c:	str	x9, [sp, #32]
   43610:	mvn	x9, x19
   43614:	stp	x8, x12, [sp, #16]
   43618:	add	x8, x28, x11
   4361c:	add	x10, x19, x23
   43620:	stur	x8, [x29, #-16]
   43624:	add	x8, x28, x9, lsl #3
   43628:	str	x8, [sp, #48]
   4362c:	add	x8, x26, x10, lsl #4
   43630:	mov	w27, wzr
   43634:	add	x8, x8, #0x18
   43638:	str	x8, [sp, #8]
   4363c:	b	43688 <__gmpn_mu_bdiv_q@@Base+0x184>
   43640:	ldur	x21, [x29, #-8]
   43644:	ldr	x0, [sp, #48]
   43648:	ldur	x2, [x29, #-16]
   4364c:	sxtw	x4, w27
   43650:	mov	x1, x24
   43654:	mov	x3, x25
   43658:	bl	c8f0 <__gmpn_sub_nc@plt>
   4365c:	mov	x27, x0
   43660:	mov	x0, x20
   43664:	mov	x1, x21
   43668:	mov	x2, x19
   4366c:	mov	x3, x25
   43670:	add	x24, x24, x25, lsl #3
   43674:	bl	d090 <__gmpn_mullo_n@plt>
   43678:	ldur	x19, [x29, #-32]
   4367c:	sub	x22, x22, x25
   43680:	cmp	x22, x25
   43684:	b.le	4386c <__gmpn_mu_bdiv_q@@Base+0x368>
   43688:	cmp	x19, #0x10
   4368c:	b.le	43718 <__gmpn_mu_bdiv_q@@Base+0x214>
   43690:	mov	x0, x23
   43694:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43698:	ldur	x2, [x29, #-40]
   4369c:	mov	x26, x0
   436a0:	add	x21, x28, x0, lsl #3
   436a4:	mov	x0, x28
   436a8:	mov	x1, x26
   436ac:	mov	x3, x23
   436b0:	mov	x4, x20
   436b4:	mov	x5, x25
   436b8:	mov	x6, x21
   436bc:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   436c0:	ldr	x8, [sp, #40]
   436c4:	sub	x19, x8, x26
   436c8:	cmp	x19, #0x1
   436cc:	b.lt	43730 <__gmpn_mu_bdiv_q@@Base+0x22c>  // b.tstop
   436d0:	ldur	x2, [x29, #-8]
   436d4:	mov	x0, x21
   436d8:	mov	x1, x28
   436dc:	mov	x3, x19
   436e0:	bl	c420 <__gmpn_sub_n@plt>
   436e4:	lsl	x8, x19, #3
   436e8:	ldr	x9, [x28, x8]
   436ec:	sxtw	x10, w0
   436f0:	subs	x9, x9, x10
   436f4:	str	x9, [x28, x8]
   436f8:	b.cs	43730 <__gmpn_mu_bdiv_q@@Base+0x22c>  // b.hs, b.nlast
   436fc:	ldr	x8, [sp, #8]
   43700:	sub	x8, x8, x26, lsl #3
   43704:	ldr	x9, [x8]
   43708:	sub	x10, x9, #0x1
   4370c:	str	x10, [x8], #8
   43710:	cbz	x9, 43704 <__gmpn_mu_bdiv_q@@Base+0x200>
   43714:	b	43730 <__gmpn_mu_bdiv_q@@Base+0x22c>
   43718:	ldur	x1, [x29, #-40]
   4371c:	mov	x0, x28
   43720:	mov	x2, x23
   43724:	mov	x3, x20
   43728:	mov	x4, x25
   4372c:	bl	cea0 <__gmpn_mul@plt>
   43730:	ldur	x19, [x29, #-24]
   43734:	cmp	x25, x23
   43738:	add	x20, x20, x25, lsl #3
   4373c:	b.eq	43640 <__gmpn_mu_bdiv_q@@Base+0x13c>  // b.none
   43740:	ldur	x21, [x29, #-8]
   43744:	ldp	x2, x1, [sp, #16]
   43748:	ldr	x3, [sp, #32]
   4374c:	mov	x0, x21
   43750:	bl	c420 <__gmpn_sub_n@plt>
   43754:	add	w27, w27, w0
   43758:	cmp	w27, #0x2
   4375c:	b.ne	43644 <__gmpn_mu_bdiv_q@@Base+0x140>  // b.any
   43760:	ldur	x8, [x29, #-16]
   43764:	ldr	x9, [x8]
   43768:	adds	x9, x9, #0x1
   4376c:	str	x9, [x8], #8
   43770:	b.cs	43764 <__gmpn_mu_bdiv_q@@Base+0x260>  // b.hs, b.nlast
   43774:	mov	w27, #0x1                   	// #1
   43778:	b	43644 <__gmpn_mu_bdiv_q@@Base+0x140>
   4377c:	asr	x21, x22, #1
   43780:	sub	x24, x22, x21
   43784:	add	x23, x26, x24, lsl #3
   43788:	mov	x0, x26
   4378c:	mov	x1, x28
   43790:	mov	x2, x24
   43794:	mov	x3, x23
   43798:	bl	cef0 <__gmpn_binvert@plt>
   4379c:	mov	x0, x20
   437a0:	mov	x1, x27
   437a4:	mov	x2, x26
   437a8:	mov	x3, x24
   437ac:	bl	d090 <__gmpn_mullo_n@plt>
   437b0:	cmp	x24, #0x11
   437b4:	b.le	43928 <__gmpn_mu_bdiv_q@@Base+0x424>
   437b8:	mov	x0, x22
   437bc:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   437c0:	mov	x25, x0
   437c4:	add	x6, x23, x0, lsl #3
   437c8:	mov	x0, x23
   437cc:	mov	x1, x25
   437d0:	mov	x2, x28
   437d4:	mov	x3, x22
   437d8:	mov	x4, x20
   437dc:	mov	x5, x24
   437e0:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   437e4:	add	x8, x24, x22
   437e8:	sub	x2, x8, x25
   437ec:	cmp	x2, #0x1
   437f0:	b.lt	43940 <__gmpn_mu_bdiv_q@@Base+0x43c>  // b.tstop
   437f4:	mov	x0, x23
   437f8:	mov	x1, x27
   437fc:	lsl	x26, x2, #3
   43800:	bl	c570 <__gmpn_cmp@plt>
   43804:	ldr	x8, [x23, x26]
   43808:	lsr	w9, w0, #31
   4380c:	subs	x8, x8, x9
   43810:	str	x8, [x23, x26]
   43814:	ldur	x26, [x29, #-24]
   43818:	b.cs	43940 <__gmpn_mu_bdiv_q@@Base+0x43c>  // b.hs, b.nlast
   4381c:	add	x8, x22, x22, lsl #1
   43820:	sub	x8, x8, x25
   43824:	sub	x8, x8, x21, lsl #1
   43828:	add	x8, x26, x8, lsl #3
   4382c:	add	x8, x8, #0x8
   43830:	ldr	x9, [x8]
   43834:	sub	x10, x9, #0x1
   43838:	str	x10, [x8], #8
   4383c:	cbz	x9, 43830 <__gmpn_mu_bdiv_q@@Base+0x32c>
   43840:	b	43940 <__gmpn_mu_bdiv_q@@Base+0x43c>
   43844:	mov	w27, wzr
   43848:	cmp	x19, #0x10
   4384c:	b.gt	43878 <__gmpn_mu_bdiv_q@@Base+0x374>
   43850:	add	x0, x21, x23, lsl #3
   43854:	mov	x1, x28
   43858:	mov	x2, x23
   4385c:	mov	x3, x20
   43860:	mov	x4, x25
   43864:	bl	cea0 <__gmpn_mul@plt>
   43868:	b	43910 <__gmpn_mu_bdiv_q@@Base+0x40c>
   4386c:	ldur	x28, [x29, #-40]
   43870:	cmp	x19, #0x10
   43874:	b.le	43850 <__gmpn_mu_bdiv_q@@Base+0x34c>
   43878:	mov	x0, x23
   4387c:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43880:	add	x26, x21, x23, lsl #3
   43884:	mov	x19, x0
   43888:	mov	x2, x28
   4388c:	add	x28, x26, x0, lsl #3
   43890:	mov	x0, x26
   43894:	mov	x1, x19
   43898:	mov	x3, x23
   4389c:	mov	x4, x20
   438a0:	mov	x5, x25
   438a4:	mov	x6, x28
   438a8:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   438ac:	add	x8, x25, x23
   438b0:	sub	x21, x8, x19
   438b4:	cmp	x21, #0x1
   438b8:	b.lt	43910 <__gmpn_mu_bdiv_q@@Base+0x40c>  // b.tstop
   438bc:	ldur	x2, [x29, #-8]
   438c0:	mov	x0, x28
   438c4:	mov	x1, x26
   438c8:	mov	x3, x21
   438cc:	bl	c420 <__gmpn_sub_n@plt>
   438d0:	lsl	x8, x21, #3
   438d4:	ldr	x9, [x26, x8]
   438d8:	sxtw	x10, w0
   438dc:	subs	x9, x9, x10
   438e0:	str	x9, [x26, x8]
   438e4:	b.cs	43910 <__gmpn_mu_bdiv_q@@Base+0x40c>  // b.hs, b.nlast
   438e8:	ldp	x8, x9, [x29, #-32]
   438ec:	add	x8, x8, x23
   438f0:	lsl	x8, x8, #1
   438f4:	sub	x8, x8, x19
   438f8:	add	x8, x9, x8, lsl #3
   438fc:	add	x8, x8, #0x18
   43900:	ldr	x9, [x8]
   43904:	sub	x10, x9, #0x1
   43908:	str	x10, [x8], #8
   4390c:	cbz	x9, 43900 <__gmpn_mu_bdiv_q@@Base+0x3fc>
   43910:	subs	x21, x25, x23
   43914:	add	x19, x20, x25, lsl #3
   43918:	b.ne	4396c <__gmpn_mu_bdiv_q@@Base+0x468>  // b.any
   4391c:	ldp	x10, x25, [x29, #-32]
   43920:	ldr	x11, [sp]
   43924:	b	439b4 <__gmpn_mu_bdiv_q@@Base+0x4b0>
   43928:	mov	x0, x23
   4392c:	mov	x1, x28
   43930:	mov	x2, x22
   43934:	mov	x3, x20
   43938:	mov	x4, x24
   4393c:	bl	cea0 <__gmpn_mul@plt>
   43940:	lsl	x22, x24, #3
   43944:	add	x1, x27, x22
   43948:	add	x2, x23, x22
   4394c:	mov	x0, x23
   43950:	mov	x3, x21
   43954:	bl	c420 <__gmpn_sub_n@plt>
   43958:	add	x0, x20, x22
   4395c:	mov	x1, x23
   43960:	mov	x2, x26
   43964:	mov	x3, x21
   43968:	b	439e8 <__gmpn_mu_bdiv_q@@Base+0x4e4>
   4396c:	ldur	x0, [x29, #-8]
   43970:	lsl	x8, x25, #3
   43974:	sub	x3, x23, x25
   43978:	add	x20, x0, x23, lsl #3
   4397c:	add	x1, x0, x8
   43980:	add	x2, x20, x8
   43984:	bl	c420 <__gmpn_sub_n@plt>
   43988:	ldp	x10, x25, [x29, #-32]
   4398c:	ldr	x11, [sp]
   43990:	add	w27, w27, w0
   43994:	cmp	w27, #0x2
   43998:	b.ne	439b4 <__gmpn_mu_bdiv_q@@Base+0x4b0>  // b.any
   4399c:	add	x8, x20, x23, lsl #3
   439a0:	ldr	x9, [x8]
   439a4:	adds	x9, x9, #0x1
   439a8:	str	x9, [x8], #8
   439ac:	b.cs	439a0 <__gmpn_mu_bdiv_q@@Base+0x49c>  // b.hs, b.nlast
   439b0:	mov	w27, #0x1                   	// #1
   439b4:	ldur	x20, [x29, #-8]
   439b8:	mvn	x9, x10
   439bc:	add	x3, x21, x22
   439c0:	sxtw	x4, w27
   439c4:	add	x8, x20, x11
   439c8:	add	x0, x8, x9, lsl #3
   439cc:	add	x2, x8, x11
   439d0:	mov	x1, x24
   439d4:	bl	c8f0 <__gmpn_sub_nc@plt>
   439d8:	mov	x0, x19
   439dc:	mov	x1, x20
   439e0:	mov	x2, x25
   439e4:	mov	x3, x22
   439e8:	bl	d090 <__gmpn_mullo_n@plt>
   439ec:	ldp	x20, x19, [sp, #176]
   439f0:	ldp	x22, x21, [sp, #160]
   439f4:	ldp	x24, x23, [sp, #144]
   439f8:	ldp	x26, x25, [sp, #128]
   439fc:	ldp	x28, x27, [sp, #112]
   43a00:	ldp	x29, x30, [sp, #96]
   43a04:	add	sp, sp, #0xc0
   43a08:	ret

0000000000043a0c <__gmpn_mu_bdiv_q_itch@@Base>:
   43a0c:	stp	x29, x30, [sp, #-48]!
   43a10:	str	x21, [sp, #16]
   43a14:	mov	x21, x0
   43a18:	cmp	x0, x1
   43a1c:	stp	x20, x19, [sp, #32]
   43a20:	mov	x29, sp
   43a24:	b.le	43a64 <__gmpn_mu_bdiv_q_itch@@Base+0x58>
   43a28:	sub	x8, x21, #0x1
   43a2c:	sdiv	x9, x8, x1
   43a30:	add	x9, x9, #0x1
   43a34:	sdiv	x8, x8, x9
   43a38:	mov	x20, x1
   43a3c:	cmp	x8, #0x10
   43a40:	add	x19, x8, #0x1
   43a44:	b.le	43a8c <__gmpn_mu_bdiv_q_itch@@Base+0x80>
   43a48:	mov	x0, x20
   43a4c:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43a50:	mov	x1, x20
   43a54:	mov	x2, x19
   43a58:	mov	x21, x0
   43a5c:	bl	43ad0 <__gmpn_mu_bdiv_q_itch@@Base+0xc4>
   43a60:	b	43a94 <__gmpn_mu_bdiv_q_itch@@Base+0x88>
   43a64:	sub	x19, x21, x21, asr #1
   43a68:	cmp	x19, #0x11
   43a6c:	b.le	43aa0 <__gmpn_mu_bdiv_q_itch@@Base+0x94>
   43a70:	mov	x0, x21
   43a74:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43a78:	mov	x1, x21
   43a7c:	mov	x2, x19
   43a80:	mov	x20, x0
   43a84:	bl	43ad0 <__gmpn_mu_bdiv_q_itch@@Base+0xc4>
   43a88:	b	43aa8 <__gmpn_mu_bdiv_q_itch@@Base+0x9c>
   43a8c:	mov	x0, xzr
   43a90:	add	x21, x19, x20
   43a94:	add	x8, x21, x20
   43a98:	add	x20, x8, x0
   43a9c:	b	43aac <__gmpn_mu_bdiv_q_itch@@Base+0xa0>
   43aa0:	mov	x0, xzr
   43aa4:	add	x20, x19, x21
   43aa8:	add	x20, x0, x20
   43aac:	mov	x0, x19
   43ab0:	bl	d3c0 <__gmpn_binvert_itch@plt>
   43ab4:	cmp	x20, x0
   43ab8:	csel	x8, x20, x0, gt
   43abc:	add	x0, x8, x19
   43ac0:	ldp	x20, x19, [sp, #32]
   43ac4:	ldr	x21, [sp, #16]
   43ac8:	ldp	x29, x30, [sp], #48
   43acc:	ret
   43ad0:	asr	x8, x0, #1
   43ad4:	cmp	x8, x2
   43ad8:	csel	x9, x0, x8, lt  // lt = tstop
   43adc:	cmp	x8, x1
   43ae0:	csel	x8, x9, xzr, lt  // lt = tstop
   43ae4:	add	x8, x0, x8
   43ae8:	add	x0, x8, #0x4
   43aec:	ret

0000000000043af0 <__gmpn_mu_bdiv_qr@@Base>:
   43af0:	stp	x29, x30, [sp, #-64]!
   43af4:	stp	x24, x23, [sp, #16]
   43af8:	stp	x22, x21, [sp, #32]
   43afc:	stp	x20, x19, [sp, #48]
   43b00:	mov	x29, sp
   43b04:	mov	x19, x5
   43b08:	mov	x20, x4
   43b0c:	mov	x23, x3
   43b10:	mov	x21, x1
   43b14:	mov	x24, x0
   43b18:	bl	43b60 <__gmpn_mu_bdiv_qr@@Base+0x70>
   43b1c:	mov	x22, x0
   43b20:	sub	x2, x23, x19
   43b24:	mov	x0, x24
   43b28:	mov	x1, x24
   43b2c:	bl	ce90 <__gmpn_neg@plt>
   43b30:	cbz	x0, 43b4c <__gmpn_mu_bdiv_qr@@Base+0x5c>
   43b34:	mov	x0, x21
   43b38:	mov	x1, x21
   43b3c:	mov	x2, x20
   43b40:	mov	x3, x19
   43b44:	bl	cc30 <__gmpn_add_n@plt>
   43b48:	sub	x0, x0, x22
   43b4c:	ldp	x20, x19, [sp, #48]
   43b50:	ldp	x22, x21, [sp, #32]
   43b54:	ldp	x24, x23, [sp, #16]
   43b58:	ldp	x29, x30, [sp], #64
   43b5c:	ret
   43b60:	sub	sp, sp, #0xc0
   43b64:	stp	x22, x21, [sp, #160]
   43b68:	sub	x21, x3, x5
   43b6c:	stp	x29, x30, [sp, #96]
   43b70:	stp	x26, x25, [sp, #128]
   43b74:	stp	x24, x23, [sp, #144]
   43b78:	stp	x20, x19, [sp, #176]
   43b7c:	add	x29, sp, #0x60
   43b80:	mov	x20, x6
   43b84:	mov	x19, x5
   43b88:	mov	x22, x4
   43b8c:	mov	x26, x2
   43b90:	cmp	x21, x5
   43b94:	mov	x24, x0
   43b98:	stp	x28, x27, [sp, #112]
   43b9c:	stp	x5, x1, [x29, #-16]
   43ba0:	stp	x4, x6, [x29, #-32]
   43ba4:	b.le	43d8c <__gmpn_mu_bdiv_qr@@Base+0x29c>
   43ba8:	sub	x8, x21, #0x1
   43bac:	sdiv	x9, x8, x19
   43bb0:	add	x9, x9, #0x1
   43bb4:	sdiv	x27, x8, x9
   43bb8:	add	x28, x27, #0x1
   43bbc:	add	x25, x20, x28, lsl #3
   43bc0:	mov	x23, x1
   43bc4:	mov	x0, x20
   43bc8:	mov	x1, x22
   43bcc:	mov	x2, x28
   43bd0:	mov	x3, x25
   43bd4:	bl	cef0 <__gmpn_binvert@plt>
   43bd8:	mov	x0, x23
   43bdc:	mov	x1, x26
   43be0:	mov	x2, x19
   43be4:	bl	cc10 <__gmpn_copyi@plt>
   43be8:	cmp	x21, x28
   43bec:	add	x26, x26, x19, lsl #3
   43bf0:	stur	x27, [x29, #-40]
   43bf4:	b.le	43e6c <__gmpn_mu_bdiv_qr@@Base+0x37c>
   43bf8:	add	x8, x28, x19
   43bfc:	str	x8, [sp, #40]
   43c00:	lsl	x8, x28, #3
   43c04:	sub	x9, x19, x28
   43c08:	mov	x11, x27
   43c0c:	str	x9, [sp, #32]
   43c10:	lsl	x9, x19, #3
   43c14:	add	x12, x23, x8
   43c18:	add	x8, x25, x8
   43c1c:	mvn	x10, x11
   43c20:	add	x11, x19, x11, lsl #1
   43c24:	stp	x8, x12, [sp, #16]
   43c28:	add	x8, x23, x9
   43c2c:	add	x22, x25, x9
   43c30:	add	x9, x20, x11, lsl #3
   43c34:	add	x8, x8, x10, lsl #3
   43c38:	mov	x27, xzr
   43c3c:	str	x8, [sp, #48]
   43c40:	add	x8, x9, #0x18
   43c44:	str	x8, [sp, #8]
   43c48:	b	43c7c <__gmpn_mu_bdiv_qr@@Base+0x18c>
   43c4c:	ldur	x23, [x29, #-8]
   43c50:	ldur	x20, [x29, #-24]
   43c54:	ldr	x0, [sp, #48]
   43c58:	mov	x1, x26
   43c5c:	mov	x2, x22
   43c60:	mov	x3, x28
   43c64:	mov	x4, x27
   43c68:	bl	c8f0 <__gmpn_sub_nc@plt>
   43c6c:	mov	x27, x0
   43c70:	cmp	x21, x28
   43c74:	add	x26, x26, x28, lsl #3
   43c78:	b.le	43e70 <__gmpn_mu_bdiv_qr@@Base+0x380>
   43c7c:	mov	x0, x24
   43c80:	mov	x1, x23
   43c84:	mov	x2, x20
   43c88:	mov	x3, x28
   43c8c:	bl	d090 <__gmpn_mullo_n@plt>
   43c90:	ldur	x8, [x29, #-40]
   43c94:	cmp	x8, #0x10
   43c98:	b.le	43d20 <__gmpn_mu_bdiv_qr@@Base+0x230>
   43c9c:	mov	x0, x19
   43ca0:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43ca4:	ldur	x2, [x29, #-32]
   43ca8:	mov	x20, x0
   43cac:	add	x23, x25, x0, lsl #3
   43cb0:	mov	x0, x25
   43cb4:	mov	x1, x20
   43cb8:	mov	x3, x19
   43cbc:	mov	x4, x24
   43cc0:	mov	x5, x28
   43cc4:	mov	x6, x23
   43cc8:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   43ccc:	ldr	x8, [sp, #40]
   43cd0:	sub	x19, x8, x20
   43cd4:	cmp	x19, #0x1
   43cd8:	b.lt	43d38 <__gmpn_mu_bdiv_qr@@Base+0x248>  // b.tstop
   43cdc:	ldur	x2, [x29, #-8]
   43ce0:	mov	x0, x23
   43ce4:	mov	x1, x25
   43ce8:	mov	x3, x19
   43cec:	bl	c420 <__gmpn_sub_n@plt>
   43cf0:	lsl	x8, x19, #3
   43cf4:	ldr	x9, [x25, x8]
   43cf8:	subs	x9, x9, x0
   43cfc:	str	x9, [x25, x8]
   43d00:	b.cs	43d38 <__gmpn_mu_bdiv_qr@@Base+0x248>  // b.hs, b.nlast
   43d04:	ldr	x8, [sp, #8]
   43d08:	sub	x8, x8, x20, lsl #3
   43d0c:	ldr	x9, [x8]
   43d10:	sub	x10, x9, #0x1
   43d14:	str	x10, [x8], #8
   43d18:	cbz	x9, 43d0c <__gmpn_mu_bdiv_qr@@Base+0x21c>
   43d1c:	b	43d38 <__gmpn_mu_bdiv_qr@@Base+0x248>
   43d20:	ldur	x1, [x29, #-32]
   43d24:	mov	x0, x25
   43d28:	mov	x2, x19
   43d2c:	mov	x3, x24
   43d30:	mov	x4, x28
   43d34:	bl	cea0 <__gmpn_mul@plt>
   43d38:	ldur	x19, [x29, #-16]
   43d3c:	add	x24, x24, x28, lsl #3
   43d40:	sub	x21, x21, x28
   43d44:	cmp	x28, x19
   43d48:	b.eq	43c4c <__gmpn_mu_bdiv_qr@@Base+0x15c>  // b.none
   43d4c:	ldur	x23, [x29, #-8]
   43d50:	ldp	x2, x1, [sp, #16]
   43d54:	ldr	x3, [sp, #32]
   43d58:	mov	x0, x23
   43d5c:	bl	c420 <__gmpn_sub_n@plt>
   43d60:	ldur	x20, [x29, #-24]
   43d64:	add	x27, x0, x27
   43d68:	cmp	x27, #0x2
   43d6c:	b.ne	43c54 <__gmpn_mu_bdiv_qr@@Base+0x164>  // b.any
   43d70:	mov	x8, x22
   43d74:	ldr	x9, [x8]
   43d78:	adds	x9, x9, #0x1
   43d7c:	str	x9, [x8], #8
   43d80:	b.cs	43d74 <__gmpn_mu_bdiv_qr@@Base+0x284>  // b.hs, b.nlast
   43d84:	mov	w27, #0x1                   	// #1
   43d88:	b	43c54 <__gmpn_mu_bdiv_qr@@Base+0x164>
   43d8c:	asr	x25, x21, #1
   43d90:	sub	x27, x21, x25
   43d94:	add	x28, x20, x27, lsl #3
   43d98:	stur	x3, [x29, #-40]
   43d9c:	mov	x0, x20
   43da0:	mov	x1, x22
   43da4:	mov	x2, x27
   43da8:	mov	x3, x28
   43dac:	bl	cef0 <__gmpn_binvert@plt>
   43db0:	mov	x0, x24
   43db4:	mov	x1, x26
   43db8:	mov	x2, x20
   43dbc:	mov	x3, x27
   43dc0:	bl	d090 <__gmpn_mullo_n@plt>
   43dc4:	cmp	x27, #0x11
   43dc8:	b.le	43f28 <__gmpn_mu_bdiv_qr@@Base+0x438>
   43dcc:	mov	x0, x19
   43dd0:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43dd4:	mov	x2, x22
   43dd8:	mov	x22, x0
   43ddc:	add	x20, x28, x0, lsl #3
   43de0:	mov	x0, x28
   43de4:	mov	x1, x22
   43de8:	mov	x3, x19
   43dec:	mov	x4, x24
   43df0:	mov	x5, x27
   43df4:	mov	x6, x20
   43df8:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   43dfc:	add	x8, x27, x19
   43e00:	sub	x19, x8, x22
   43e04:	cmp	x19, #0x1
   43e08:	b.lt	43f40 <__gmpn_mu_bdiv_qr@@Base+0x450>  // b.tstop
   43e0c:	mov	x0, x20
   43e10:	mov	x1, x28
   43e14:	mov	x2, x26
   43e18:	mov	x3, x19
   43e1c:	bl	c420 <__gmpn_sub_n@plt>
   43e20:	lsl	x8, x19, #3
   43e24:	ldr	x9, [x28, x8]
   43e28:	subs	x9, x9, x0
   43e2c:	str	x9, [x28, x8]
   43e30:	b.cs	43f40 <__gmpn_mu_bdiv_qr@@Base+0x450>  // b.hs, b.nlast
   43e34:	ldur	x8, [x29, #-40]
   43e38:	ldur	x9, [x29, #-16]
   43e3c:	lsl	x8, x8, #1
   43e40:	add	x9, x22, x9
   43e44:	sub	x8, x8, x9
   43e48:	ldur	x9, [x29, #-24]
   43e4c:	sub	x8, x8, x25, lsl #1
   43e50:	add	x8, x9, x8, lsl #3
   43e54:	add	x8, x8, #0x8
   43e58:	ldr	x9, [x8]
   43e5c:	sub	x10, x9, #0x1
   43e60:	str	x10, [x8], #8
   43e64:	cbz	x9, 43e58 <__gmpn_mu_bdiv_qr@@Base+0x368>
   43e68:	b	43f40 <__gmpn_mu_bdiv_qr@@Base+0x450>
   43e6c:	mov	x27, xzr
   43e70:	mov	x0, x24
   43e74:	mov	x1, x23
   43e78:	mov	x2, x20
   43e7c:	mov	x3, x21
   43e80:	bl	d090 <__gmpn_mullo_n@plt>
   43e84:	cmp	x21, #0x11
   43e88:	b.le	44014 <__gmpn_mu_bdiv_qr@@Base+0x524>
   43e8c:	mov	x0, x19
   43e90:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43e94:	ldur	x2, [x29, #-32]
   43e98:	mov	x22, x0
   43e9c:	add	x20, x25, x0, lsl #3
   43ea0:	mov	x0, x25
   43ea4:	mov	x1, x22
   43ea8:	mov	x3, x19
   43eac:	mov	x4, x24
   43eb0:	mov	x5, x21
   43eb4:	mov	x6, x20
   43eb8:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   43ebc:	add	x8, x21, x19
   43ec0:	sub	x19, x8, x22
   43ec4:	cmp	x19, #0x1
   43ec8:	b.lt	4402c <__gmpn_mu_bdiv_qr@@Base+0x53c>  // b.tstop
   43ecc:	mov	x0, x20
   43ed0:	mov	x1, x25
   43ed4:	mov	x2, x23
   43ed8:	mov	x3, x19
   43edc:	bl	c420 <__gmpn_sub_n@plt>
   43ee0:	lsl	x8, x19, #3
   43ee4:	ldr	x9, [x25, x8]
   43ee8:	subs	x9, x9, x0
   43eec:	str	x9, [x25, x8]
   43ef0:	b.cs	4402c <__gmpn_mu_bdiv_qr@@Base+0x53c>  // b.hs, b.nlast
   43ef4:	ldur	x8, [x29, #-40]
   43ef8:	ldur	x9, [x29, #-16]
   43efc:	add	x8, x21, x8
   43f00:	add	x8, x8, x9
   43f04:	ldur	x9, [x29, #-24]
   43f08:	sub	x8, x8, x22
   43f0c:	add	x8, x9, x8, lsl #3
   43f10:	add	x8, x8, #0x10
   43f14:	ldr	x9, [x8]
   43f18:	sub	x10, x9, #0x1
   43f1c:	str	x10, [x8], #8
   43f20:	cbz	x9, 43f14 <__gmpn_mu_bdiv_qr@@Base+0x424>
   43f24:	b	4402c <__gmpn_mu_bdiv_qr@@Base+0x53c>
   43f28:	mov	x0, x28
   43f2c:	mov	x1, x22
   43f30:	mov	x2, x19
   43f34:	mov	x3, x24
   43f38:	mov	x4, x27
   43f3c:	bl	cea0 <__gmpn_mul@plt>
   43f40:	ldp	x19, x20, [x29, #-16]
   43f44:	lsl	x8, x27, #3
   43f48:	add	x1, x26, x8
   43f4c:	add	x2, x28, x8
   43f50:	mov	x0, x20
   43f54:	mov	x3, x19
   43f58:	add	x23, x24, x8
   43f5c:	bl	c420 <__gmpn_sub_n@plt>
   43f60:	ldur	x2, [x29, #-24]
   43f64:	mov	x22, x0
   43f68:	mov	x0, x23
   43f6c:	mov	x1, x20
   43f70:	mov	x3, x25
   43f74:	bl	d090 <__gmpn_mullo_n@plt>
   43f78:	cmp	x21, #0x23
   43f7c:	b.le	44094 <__gmpn_mu_bdiv_qr@@Base+0x5a4>
   43f80:	mov	x0, x19
   43f84:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   43f88:	ldur	x2, [x29, #-32]
   43f8c:	mov	x24, x0
   43f90:	add	x20, x28, x0, lsl #3
   43f94:	mov	x0, x28
   43f98:	mov	x1, x24
   43f9c:	mov	x3, x19
   43fa0:	mov	x4, x23
   43fa4:	mov	x5, x25
   43fa8:	mov	x6, x20
   43fac:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   43fb0:	ldur	x23, [x29, #-8]
   43fb4:	add	x8, x25, x19
   43fb8:	sub	x19, x8, x24
   43fbc:	cmp	x19, #0x1
   43fc0:	b.lt	440b0 <__gmpn_mu_bdiv_qr@@Base+0x5c0>  // b.tstop
   43fc4:	mov	x0, x20
   43fc8:	mov	x1, x28
   43fcc:	mov	x2, x23
   43fd0:	mov	x3, x19
   43fd4:	bl	c420 <__gmpn_sub_n@plt>
   43fd8:	lsl	x8, x19, #3
   43fdc:	ldr	x9, [x28, x8]
   43fe0:	subs	x9, x9, x0
   43fe4:	str	x9, [x28, x8]
   43fe8:	b.cs	440b0 <__gmpn_mu_bdiv_qr@@Base+0x5c0>  // b.hs, b.nlast
   43fec:	ldur	x8, [x29, #-40]
   43ff0:	ldur	x9, [x29, #-24]
   43ff4:	sub	x8, x8, x24
   43ff8:	add	x8, x9, x8, lsl #3
   43ffc:	add	x8, x8, #0x8
   44000:	ldr	x9, [x8]
   44004:	sub	x10, x9, #0x1
   44008:	str	x10, [x8], #8
   4400c:	cbz	x9, 44000 <__gmpn_mu_bdiv_qr@@Base+0x510>
   44010:	b	440b0 <__gmpn_mu_bdiv_qr@@Base+0x5c0>
   44014:	ldur	x1, [x29, #-32]
   44018:	mov	x0, x25
   4401c:	mov	x2, x19
   44020:	mov	x3, x24
   44024:	mov	x4, x21
   44028:	bl	cea0 <__gmpn_mul@plt>
   4402c:	ldur	x19, [x29, #-16]
   44030:	cmp	x21, x19
   44034:	b.eq	44074 <__gmpn_mu_bdiv_qr@@Base+0x584>  // b.none
   44038:	lsl	x8, x21, #3
   4403c:	add	x1, x23, x8
   44040:	add	x2, x25, x8
   44044:	sub	x3, x19, x21
   44048:	mov	x0, x23
   4404c:	bl	c420 <__gmpn_sub_n@plt>
   44050:	add	x27, x0, x27
   44054:	cmp	x27, #0x2
   44058:	b.ne	44074 <__gmpn_mu_bdiv_qr@@Base+0x584>  // b.any
   4405c:	add	x8, x25, x19, lsl #3
   44060:	mov	w27, #0x1                   	// #1
   44064:	ldr	x9, [x8]
   44068:	adds	x9, x9, #0x1
   4406c:	str	x9, [x8], #8
   44070:	b.cs	44064 <__gmpn_mu_bdiv_qr@@Base+0x574>  // b.hs, b.nlast
   44074:	lsl	x8, x19, #3
   44078:	add	x9, x23, x8
   4407c:	sub	x0, x9, x21, lsl #3
   44080:	add	x2, x25, x8
   44084:	mov	x1, x26
   44088:	mov	x3, x21
   4408c:	mov	x4, x27
   44090:	b	44108 <__gmpn_mu_bdiv_qr@@Base+0x618>
   44094:	ldur	x1, [x29, #-32]
   44098:	mov	x0, x28
   4409c:	mov	x2, x19
   440a0:	mov	x3, x23
   440a4:	mov	x4, x25
   440a8:	bl	cea0 <__gmpn_mul@plt>
   440ac:	ldur	x23, [x29, #-8]
   440b0:	ldp	x8, x19, [x29, #-24]
   440b4:	add	x1, x23, x25, lsl #3
   440b8:	mov	x0, x23
   440bc:	add	x2, x8, x21, lsl #3
   440c0:	sub	x3, x19, x25
   440c4:	bl	c420 <__gmpn_sub_n@plt>
   440c8:	add	x4, x0, x22
   440cc:	cmp	x4, #0x2
   440d0:	b.ne	440ec <__gmpn_mu_bdiv_qr@@Base+0x5fc>  // b.any
   440d4:	add	x8, x28, x19, lsl #3
   440d8:	mov	w4, #0x1                   	// #1
   440dc:	ldr	x9, [x8]
   440e0:	adds	x9, x9, #0x1
   440e4:	str	x9, [x8], #8
   440e8:	b.cs	440dc <__gmpn_mu_bdiv_qr@@Base+0x5ec>  // b.hs, b.nlast
   440ec:	lsl	x8, x19, #3
   440f0:	add	x9, x23, x8
   440f4:	add	x10, x26, x8
   440f8:	sub	x0, x9, x25, lsl #3
   440fc:	add	x1, x10, x27, lsl #3
   44100:	add	x2, x28, x8
   44104:	mov	x3, x25
   44108:	bl	c8f0 <__gmpn_sub_nc@plt>
   4410c:	ldp	x20, x19, [sp, #176]
   44110:	ldp	x22, x21, [sp, #160]
   44114:	ldp	x24, x23, [sp, #144]
   44118:	ldp	x26, x25, [sp, #128]
   4411c:	ldp	x28, x27, [sp, #112]
   44120:	ldp	x29, x30, [sp, #96]
   44124:	add	sp, sp, #0xc0
   44128:	ret

000000000004412c <__gmpn_mu_bdiv_qr_itch@@Base>:
   4412c:	stp	x29, x30, [sp, #-48]!
   44130:	sub	x8, x0, x1
   44134:	stp	x20, x19, [sp, #32]
   44138:	mov	x20, x1
   4413c:	cmp	x8, x1
   44140:	stp	x22, x21, [sp, #16]
   44144:	mov	x29, sp
   44148:	b.le	44174 <__gmpn_mu_bdiv_qr_itch@@Base+0x48>
   4414c:	sub	x8, x8, #0x1
   44150:	sdiv	x9, x8, x20
   44154:	add	x9, x9, #0x1
   44158:	sdiv	x8, x8, x9
   4415c:	add	x19, x8, #0x1
   44160:	cmp	x19, #0x11
   44164:	b.gt	44180 <__gmpn_mu_bdiv_qr_itch@@Base+0x54>
   44168:	mov	x22, xzr
   4416c:	add	x21, x19, x20
   44170:	b	4419c <__gmpn_mu_bdiv_qr_itch@@Base+0x70>
   44174:	sub	x19, x8, x8, asr #1
   44178:	cmp	x19, #0x11
   4417c:	b.le	44168 <__gmpn_mu_bdiv_qr_itch@@Base+0x3c>
   44180:	mov	x0, x20
   44184:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   44188:	mov	x1, x20
   4418c:	mov	x2, x19
   44190:	mov	x21, x0
   44194:	bl	441c4 <__gmpn_mu_bdiv_qr_itch@@Base+0x98>
   44198:	mov	x22, x0
   4419c:	mov	x0, x19
   441a0:	bl	d3c0 <__gmpn_binvert_itch@plt>
   441a4:	add	x8, x21, x22
   441a8:	cmp	x8, x0
   441ac:	csel	x8, x8, x0, gt
   441b0:	add	x0, x8, x19
   441b4:	ldp	x20, x19, [sp, #32]
   441b8:	ldp	x22, x21, [sp, #16]
   441bc:	ldp	x29, x30, [sp], #48
   441c0:	ret
   441c4:	asr	x8, x0, #1
   441c8:	cmp	x8, x2
   441cc:	csel	x9, x0, x8, lt  // lt = tstop
   441d0:	cmp	x8, x1
   441d4:	csel	x8, x9, xzr, lt  // lt = tstop
   441d8:	add	x8, x0, x8
   441dc:	add	x0, x8, #0x4
   441e0:	ret

00000000000441e4 <__gmpn_bdiv_q@@Base>:
   441e4:	stp	x29, x30, [sp, #-64]!
   441e8:	str	x23, [sp, #16]
   441ec:	stp	x22, x21, [sp, #32]
   441f0:	stp	x20, x19, [sp, #48]
   441f4:	mov	x21, x5
   441f8:	mov	x19, x4
   441fc:	mov	x20, x3
   44200:	mov	x22, x2
   44204:	cmp	x4, #0x5c
   44208:	mov	x23, x0
   4420c:	mov	x29, sp
   44210:	b.le	44238 <__gmpn_bdiv_q@@Base+0x54>
   44214:	cmp	x19, #0x39b
   44218:	b.le	44294 <__gmpn_bdiv_q@@Base+0xb0>
   4421c:	mov	x0, x23
   44220:	mov	x2, x22
   44224:	mov	x3, x20
   44228:	mov	x4, x19
   4422c:	mov	x5, x21
   44230:	bl	d5a0 <__gmpn_mu_bdiv_q@plt>
   44234:	b	442ec <__gmpn_bdiv_q@@Base+0x108>
   44238:	mov	x0, x21
   4423c:	mov	x2, x22
   44240:	bl	cc10 <__gmpn_copyi@plt>
   44244:	ldr	x8, [x20]
   44248:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4424c:	ldr	x9, [x9, #3952]
   44250:	mov	x0, x23
   44254:	ubfx	x10, x8, #1, #7
   44258:	mov	x1, x21
   4425c:	ldrb	w9, [x9, x10]
   44260:	mov	w10, #0x2                   	// #2
   44264:	mov	x2, x22
   44268:	mov	x3, x20
   4426c:	msub	x11, x8, x9, x10
   44270:	mul	x9, x11, x9
   44274:	msub	x10, x9, x8, x10
   44278:	mul	x9, x9, x10
   4427c:	orr	x10, xzr, #0xfffffffffffffffe
   44280:	madd	x8, x9, x8, x10
   44284:	mul	x5, x8, x9
   44288:	mov	x4, x19
   4428c:	bl	c680 <__gmpn_sbpi1_bdiv_q@plt>
   44290:	b	442ec <__gmpn_bdiv_q@@Base+0x108>
   44294:	mov	x0, x21
   44298:	mov	x2, x22
   4429c:	bl	cc10 <__gmpn_copyi@plt>
   442a0:	ldr	x8, [x20]
   442a4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   442a8:	ldr	x9, [x9, #3952]
   442ac:	mov	x0, x23
   442b0:	ubfx	x10, x8, #1, #7
   442b4:	mov	x1, x21
   442b8:	ldrb	w9, [x9, x10]
   442bc:	mov	w10, #0x2                   	// #2
   442c0:	mov	x2, x22
   442c4:	mov	x3, x20
   442c8:	msub	x11, x8, x9, x10
   442cc:	mul	x9, x11, x9
   442d0:	msub	x10, x9, x8, x10
   442d4:	mul	x9, x9, x10
   442d8:	orr	x10, xzr, #0xfffffffffffffffe
   442dc:	madd	x8, x9, x8, x10
   442e0:	mul	x5, x8, x9
   442e4:	mov	x4, x19
   442e8:	bl	cfe0 <__gmpn_dcpi1_bdiv_q@plt>
   442ec:	ldp	x20, x19, [sp, #48]
   442f0:	ldp	x22, x21, [sp, #32]
   442f4:	ldr	x23, [sp, #16]
   442f8:	ldp	x29, x30, [sp], #64
   442fc:	ret

0000000000044300 <__gmpn_bdiv_q_itch@@Base>:
   44300:	cmp	x1, #0x39c
   44304:	b.lt	44318 <__gmpn_bdiv_q_itch@@Base+0x18>  // b.tstop
   44308:	stp	x29, x30, [sp, #-16]!
   4430c:	mov	x29, sp
   44310:	bl	ca30 <__gmpn_mu_bdiv_q_itch@plt>
   44314:	ldp	x29, x30, [sp], #16
   44318:	ret

000000000004431c <__gmpn_bdiv_qr@@Base>:
   4431c:	stp	x29, x30, [sp, #-64]!
   44320:	stp	x24, x23, [sp, #16]
   44324:	stp	x22, x21, [sp, #32]
   44328:	stp	x20, x19, [sp, #48]
   4432c:	mov	x20, x6
   44330:	mov	x19, x5
   44334:	mov	x23, x4
   44338:	mov	x22, x3
   4433c:	mov	x21, x1
   44340:	cmp	x5, #0x27
   44344:	mov	x24, x0
   44348:	mov	x29, sp
   4434c:	b.lt	44388 <__gmpn_bdiv_qr@@Base+0x6c>  // b.tstop
   44350:	sub	x8, x22, x19
   44354:	cmp	x8, #0x26
   44358:	b.le	44388 <__gmpn_bdiv_qr@@Base+0x6c>
   4435c:	cmp	x19, #0x326
   44360:	b.le	443e8 <__gmpn_bdiv_qr@@Base+0xcc>
   44364:	mov	x0, x24
   44368:	mov	x1, x21
   4436c:	mov	x3, x22
   44370:	mov	x4, x23
   44374:	mov	x5, x19
   44378:	mov	x6, x20
   4437c:	bl	ce70 <__gmpn_mu_bdiv_qr@plt>
   44380:	mov	x23, x0
   44384:	b	4445c <__gmpn_bdiv_qr@@Base+0x140>
   44388:	mov	x0, x20
   4438c:	mov	x1, x2
   44390:	mov	x2, x22
   44394:	bl	cc10 <__gmpn_copyi@plt>
   44398:	ldr	x8, [x23]
   4439c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   443a0:	ldr	x9, [x9, #3952]
   443a4:	mov	x0, x24
   443a8:	ubfx	x10, x8, #1, #7
   443ac:	mov	x1, x20
   443b0:	ldrb	w9, [x9, x10]
   443b4:	mov	w10, #0x2                   	// #2
   443b8:	mov	x2, x22
   443bc:	mov	x3, x23
   443c0:	msub	x11, x8, x9, x10
   443c4:	mul	x9, x11, x9
   443c8:	msub	x10, x9, x8, x10
   443cc:	mul	x9, x9, x10
   443d0:	orr	x10, xzr, #0xfffffffffffffffe
   443d4:	madd	x8, x9, x8, x10
   443d8:	mul	x5, x8, x9
   443dc:	mov	x4, x19
   443e0:	bl	c9d0 <__gmpn_sbpi1_bdiv_qr@plt>
   443e4:	b	44444 <__gmpn_bdiv_qr@@Base+0x128>
   443e8:	mov	x0, x20
   443ec:	mov	x1, x2
   443f0:	mov	x2, x22
   443f4:	bl	cc10 <__gmpn_copyi@plt>
   443f8:	ldr	x8, [x23]
   443fc:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   44400:	ldr	x9, [x9, #3952]
   44404:	mov	x0, x24
   44408:	ubfx	x10, x8, #1, #7
   4440c:	mov	x1, x20
   44410:	ldrb	w9, [x9, x10]
   44414:	mov	w10, #0x2                   	// #2
   44418:	mov	x2, x22
   4441c:	mov	x3, x23
   44420:	msub	x11, x8, x9, x10
   44424:	mul	x9, x11, x9
   44428:	msub	x10, x9, x8, x10
   4442c:	mul	x9, x9, x10
   44430:	orr	x10, xzr, #0xfffffffffffffffe
   44434:	madd	x8, x9, x8, x10
   44438:	mul	x5, x8, x9
   4443c:	mov	x4, x19
   44440:	bl	c770 <__gmpn_dcpi1_bdiv_qr@plt>
   44444:	add	x8, x20, x22, lsl #3
   44448:	mov	x23, x0
   4444c:	sub	x1, x8, x19, lsl #3
   44450:	mov	x0, x21
   44454:	mov	x2, x19
   44458:	bl	cc10 <__gmpn_copyi@plt>
   4445c:	mov	x0, x23
   44460:	ldp	x20, x19, [sp, #48]
   44464:	ldp	x22, x21, [sp, #32]
   44468:	ldp	x24, x23, [sp, #16]
   4446c:	ldp	x29, x30, [sp], #64
   44470:	ret

0000000000044474 <__gmpn_bdiv_qr_itch@@Base>:
   44474:	cmp	x1, #0x327
   44478:	b.lt	4448c <__gmpn_bdiv_qr_itch@@Base+0x18>  // b.tstop
   4447c:	stp	x29, x30, [sp, #-16]!
   44480:	mov	x29, sp
   44484:	bl	d350 <__gmpn_mu_bdiv_qr_itch@plt>
   44488:	ldp	x29, x30, [sp], #16
   4448c:	ret

0000000000044490 <__gmpn_broot_invm1@@Base>:
   44490:	stp	x29, x30, [sp, #-96]!
   44494:	stp	x28, x27, [sp, #16]
   44498:	stp	x26, x25, [sp, #32]
   4449c:	stp	x24, x23, [sp, #48]
   444a0:	stp	x22, x21, [sp, #64]
   444a4:	stp	x20, x19, [sp, #80]
   444a8:	mov	x29, sp
   444ac:	sub	sp, sp, #0x440
   444b0:	mov	x25, x1
   444b4:	lsl	x1, x2, #5
   444b8:	mov	w8, #0x7f00                	// #32512
   444bc:	mov	x19, sp
   444c0:	mov	x20, x3
   444c4:	mov	x24, x2
   444c8:	mov	x21, x0
   444cc:	cmp	x1, x8
   444d0:	str	xzr, [x19, #32]
   444d4:	b.hi	44724 <__gmpn_broot_invm1@@Base+0x294>  // b.pmore
   444d8:	add	x9, x1, #0xf
   444dc:	mov	x8, sp
   444e0:	and	x9, x9, #0xfffffffffffffff0
   444e4:	sub	x27, x8, x9
   444e8:	mov	sp, x27
   444ec:	add	x28, x27, x24, lsl #3
   444f0:	sub	x8, x20, #0x1
   444f4:	add	x2, x19, #0x30
   444f8:	mov	w3, #0x1                   	// #1
   444fc:	mov	x0, x27
   44500:	mov	x1, x25
   44504:	mov	x4, x24
   44508:	mov	x5, x28
   4450c:	str	x8, [x19, #48]
   44510:	bl	c520 <__gmpn_powlo@plt>
   44514:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   44518:	ldr	w8, [x25]
   4451c:	ldr	x10, [x10, #3952]
   44520:	ubfx	x9, x20, #1, #7
   44524:	mov	w11, #0x2                   	// #2
   44528:	ldr	x23, [x27]
   4452c:	ldrb	w9, [x10, x9]
   44530:	lsl	w10, w8, #1
   44534:	eor	w8, w10, w8, lsl #2
   44538:	and	w8, w8, w20, lsl #2
   4453c:	msub	x10, x9, x20, x11
   44540:	mul	x9, x10, x9
   44544:	msub	x10, x9, x20, x11
   44548:	and	x8, x8, #0x8
   4454c:	mul	x9, x9, x10
   44550:	orr	x26, x8, #0x1
   44554:	msub	x8, x9, x20, x11
   44558:	and	x1, x20, #0x7f
   4455c:	mov	x0, x26
   44560:	add	x22, x20, #0x1
   44564:	mul	x25, x9, x8
   44568:	bl	4474c <__gmpn_broot_invm1@@Base+0x2bc>
   4456c:	msub	x8, x0, x23, x22
   44570:	mul	x8, x8, x26
   44574:	mul	x26, x8, x25
   44578:	and	x1, x20, #0x7fff
   4457c:	mov	x0, x26
   44580:	bl	4474c <__gmpn_broot_invm1@@Base+0x2bc>
   44584:	msub	x8, x0, x23, x22
   44588:	mul	x8, x25, x8
   4458c:	mul	x26, x8, x26
   44590:	mov	x0, x26
   44594:	mov	x1, x20
   44598:	bl	4474c <__gmpn_broot_invm1@@Base+0x2bc>
   4459c:	msub	x8, x0, x23, x22
   445a0:	mul	x8, x25, x8
   445a4:	mul	x26, x8, x26
   445a8:	mov	x0, x26
   445ac:	mov	x1, x20
   445b0:	bl	4474c <__gmpn_broot_invm1@@Base+0x2bc>
   445b4:	msub	x8, x0, x23, x22
   445b8:	mul	x8, x25, x8
   445bc:	mul	x8, x8, x26
   445c0:	cmp	x24, #0x1
   445c4:	str	x8, [x21]
   445c8:	b.eq	446fc <__gmpn_broot_invm1@@Base+0x26c>  // b.none
   445cc:	mov	w1, #0x8                   	// #8
   445d0:	lsr	x8, x20, #1
   445d4:	bfi	x1, x24, #4, #60
   445d8:	mov	w9, #0x7f00                	// #32512
   445dc:	add	x8, x8, #0x1
   445e0:	cmp	x1, x9
   445e4:	str	x8, [x19, #40]
   445e8:	b.hi	4473c <__gmpn_broot_invm1@@Base+0x2ac>  // b.pmore
   445ec:	add	x9, x1, #0xf
   445f0:	mov	x8, sp
   445f4:	and	x9, x9, #0xfffffffffffffff0
   445f8:	sub	x26, x8, x9
   445fc:	mov	sp, x26
   44600:	mov	x13, x20
   44604:	mov	x20, x21
   44608:	cmp	x24, #0x2
   4460c:	b.lt	4464c <__gmpn_broot_invm1@@Base+0x1bc>  // b.tstop
   44610:	mov	w9, wzr
   44614:	add	x8, x19, #0x38
   44618:	mov	x10, x24
   4461c:	add	x11, x10, #0x1
   44620:	add	x12, x10, #0x2
   44624:	cmp	x11, #0x0
   44628:	csinc	x11, x12, x10, lt  // lt = tstop
   4462c:	add	w22, w9, #0x1
   44630:	asr	x11, x11, #1
   44634:	cmp	x10, #0x2
   44638:	str	x10, [x8, w9, uxtw #3]
   4463c:	mov	x10, x11
   44640:	mov	w9, w22
   44644:	b.gt	4461c <__gmpn_broot_invm1@@Base+0x18c>
   44648:	b	44650 <__gmpn_broot_invm1@@Base+0x1c0>
   4464c:	mov	w22, wzr
   44650:	stp	x28, x13, [x19, #8]
   44654:	str	x27, [x19, #24]
   44658:	cbz	w22, 446fc <__gmpn_broot_invm1@@Base+0x26c>
   4465c:	sub	w8, w22, #0x1
   44660:	add	x9, x19, #0x38
   44664:	mov	x21, x25
   44668:	add	x24, x26, x24, lsl #3
   4466c:	add	x25, x9, w8, uxtw #3
   44670:	mov	w27, #0x1                   	// #1
   44674:	mov	x0, x24
   44678:	mov	x1, x20
   4467c:	mov	x2, x27
   44680:	bl	ca90 <__gmpn_sqr@plt>
   44684:	ldr	x28, [x25], #-8
   44688:	ldr	x5, [x19, #8]
   4468c:	add	x2, x19, #0x28
   44690:	mov	w3, #0x1                   	// #1
   44694:	mov	x0, x26
   44698:	mov	x1, x24
   4469c:	mov	x4, x28
   446a0:	bl	c520 <__gmpn_powlo@plt>
   446a4:	ldr	x2, [x19, #24]
   446a8:	mov	x0, x24
   446ac:	mov	x1, x26
   446b0:	mov	x3, x28
   446b4:	bl	d090 <__gmpn_mullo_n@plt>
   446b8:	ldr	x3, [x19, #16]
   446bc:	lsl	x8, x27, #3
   446c0:	add	x23, x20, x8
   446c4:	sub	x27, x28, x27
   446c8:	add	x1, x24, x8
   446cc:	mov	x0, x23
   446d0:	mov	x2, x27
   446d4:	mov	x4, x21
   446d8:	mov	w5, wzr
   446dc:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   446e0:	mov	x0, x23
   446e4:	mov	x1, x23
   446e8:	mov	x2, x27
   446ec:	bl	ce90 <__gmpn_neg@plt>
   446f0:	subs	w22, w22, #0x1
   446f4:	mov	x27, x28
   446f8:	b.ne	44674 <__gmpn_broot_invm1@@Base+0x1e4>  // b.any
   446fc:	ldr	x0, [x19, #32]
   44700:	cbnz	x0, 44734 <__gmpn_broot_invm1@@Base+0x2a4>
   44704:	mov	sp, x29
   44708:	ldp	x20, x19, [sp, #80]
   4470c:	ldp	x22, x21, [sp, #64]
   44710:	ldp	x24, x23, [sp, #48]
   44714:	ldp	x26, x25, [sp, #32]
   44718:	ldp	x28, x27, [sp, #16]
   4471c:	ldp	x29, x30, [sp], #96
   44720:	ret
   44724:	add	x0, x19, #0x20
   44728:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4472c:	mov	x27, x0
   44730:	b	444ec <__gmpn_broot_invm1@@Base+0x5c>
   44734:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   44738:	b	44704 <__gmpn_broot_invm1@@Base+0x274>
   4473c:	add	x0, x19, #0x20
   44740:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   44744:	mov	x26, x0
   44748:	b	44600 <__gmpn_broot_invm1@@Base+0x170>
   4474c:	cbz	x1, 44774 <__gmpn_broot_invm1@@Base+0x2e4>
   44750:	mov	x8, x0
   44754:	mov	w0, #0x1                   	// #1
   44758:	tst	x1, #0x1
   4475c:	csinc	x9, x8, xzr, ne  // ne = any
   44760:	lsr	x1, x1, #1
   44764:	mul	x0, x9, x0
   44768:	mul	x8, x8, x8
   4476c:	cbnz	x1, 44758 <__gmpn_broot_invm1@@Base+0x2c8>
   44770:	ret
   44774:	mov	w0, #0x1                   	// #1
   44778:	ret

000000000004477c <__gmpn_broot@@Base>:
   4477c:	stp	x29, x30, [sp, #-64]!
   44780:	stp	x22, x21, [sp, #32]
   44784:	stp	x20, x19, [sp, #48]
   44788:	mov	x19, x2
   4478c:	mov	x20, x1
   44790:	cmp	x3, #0x1
   44794:	mov	x21, x0
   44798:	str	x23, [sp, #16]
   4479c:	mov	x29, sp
   447a0:	b.ne	447b8 <__gmpn_broot@@Base+0x3c>  // b.any
   447a4:	mov	x0, x21
   447a8:	mov	x1, x20
   447ac:	mov	x2, x19
   447b0:	bl	cc10 <__gmpn_copyi@plt>
   447b4:	b	44814 <__gmpn_broot@@Base+0x98>
   447b8:	lsl	x1, x19, #3
   447bc:	mov	w8, #0x7f00                	// #32512
   447c0:	mov	x22, x3
   447c4:	cmp	x1, x8
   447c8:	str	xzr, [x29, #24]
   447cc:	b.hi	4482c <__gmpn_broot@@Base+0xb0>  // b.pmore
   447d0:	add	x9, x1, #0xf
   447d4:	mov	x8, sp
   447d8:	and	x9, x9, #0xfffffffffffffff0
   447dc:	sub	x23, x8, x9
   447e0:	mov	sp, x23
   447e4:	mov	x0, x23
   447e8:	mov	x1, x20
   447ec:	mov	x2, x19
   447f0:	mov	x3, x22
   447f4:	bl	cae0 <__gmpn_broot_invm1@plt>
   447f8:	mov	x0, x21
   447fc:	mov	x1, x23
   44800:	mov	x2, x20
   44804:	mov	x3, x19
   44808:	bl	d090 <__gmpn_mullo_n@plt>
   4480c:	ldr	x0, [x29, #24]
   44810:	cbnz	x0, 4483c <__gmpn_broot@@Base+0xc0>
   44814:	mov	sp, x29
   44818:	ldp	x20, x19, [sp, #48]
   4481c:	ldp	x22, x21, [sp, #32]
   44820:	ldr	x23, [sp, #16]
   44824:	ldp	x29, x30, [sp], #64
   44828:	ret
   4482c:	add	x0, x29, #0x18
   44830:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   44834:	mov	x23, x0
   44838:	b	447e4 <__gmpn_broot@@Base+0x68>
   4483c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   44840:	b	44814 <__gmpn_broot@@Base+0x98>

0000000000044844 <__gmpn_brootinv@@Base>:
   44844:	stp	x29, x30, [sp, #-96]!
   44848:	stp	x28, x27, [sp, #16]
   4484c:	stp	x26, x25, [sp, #32]
   44850:	stp	x24, x23, [sp, #48]
   44854:	stp	x22, x21, [sp, #64]
   44858:	stp	x20, x19, [sp, #80]
   4485c:	mov	x29, sp
   44860:	sub	sp, sp, #0x230
   44864:	adrp	x11, 69000 <__gmp_limbroots_table@@Base+0x11338>
   44868:	ldr	x11, [x11, #3952]
   4486c:	add	x8, x2, #0x3
   44870:	ubfx	x10, x3, #1, #7
   44874:	asr	x8, x8, #1
   44878:	lsr	x9, x3, #1
   4487c:	stp	x8, x1, [sp, #8]
   44880:	ldrb	w8, [x11, x10]
   44884:	add	x24, x9, #0x1
   44888:	stur	x24, [x29, #-16]
   4488c:	mov	w12, #0x2                   	// #2
   44890:	ldr	x27, [x1]
   44894:	msub	x9, x8, x3, x12
   44898:	mul	x8, x9, x8
   4489c:	msub	x9, x8, x3, x12
   448a0:	mul	x8, x8, x9
   448a4:	lsl	w9, w27, #1
   448a8:	eor	w9, w9, w27, lsl #2
   448ac:	msub	x10, x8, x3, x12
   448b0:	and	w9, w9, w24, lsl #3
   448b4:	mul	x23, x8, x10
   448b8:	and	x8, x9, #0x8
   448bc:	eor	x25, x8, x27
   448c0:	mov	x22, x0
   448c4:	and	x1, x24, #0x3f
   448c8:	mov	x0, x25
   448cc:	mov	x19, x4
   448d0:	mov	x20, x3
   448d4:	mov	x26, x2
   448d8:	lsl	x28, x24, #1
   448dc:	bl	44a88 <__gmpn_brootinv@@Base+0x244>
   448e0:	mul	x8, x0, x27
   448e4:	neg	x8, x8
   448e8:	madd	x8, x28, x25, x8
   448ec:	mul	x25, x23, x8
   448f0:	and	x1, x24, #0x3fff
   448f4:	mov	x0, x25
   448f8:	bl	44a88 <__gmpn_brootinv@@Base+0x244>
   448fc:	mul	x8, x0, x27
   44900:	neg	x8, x8
   44904:	madd	x8, x28, x25, x8
   44908:	mul	x25, x8, x23
   4490c:	mov	w21, #0x10                  	// #16
   44910:	mov	x0, x25
   44914:	mov	x1, x24
   44918:	bl	44a88 <__gmpn_brootinv@@Base+0x244>
   4491c:	mul	x8, x0, x27
   44920:	neg	x8, x8
   44924:	lsl	w21, w21, #1
   44928:	madd	x8, x28, x25, x8
   4492c:	cmp	w21, #0x40
   44930:	mul	x25, x8, x23
   44934:	b.cc	44910 <__gmpn_brootinv@@Base+0xcc>  // b.lo, b.ul, b.last
   44938:	ldr	x21, [sp, #16]
   4493c:	cmp	x26, #0x1
   44940:	str	x25, [x22]
   44944:	b.eq	44a68 <__gmpn_brootinv@@Base+0x224>  // b.none
   44948:	ldr	x8, [sp, #8]
   4494c:	add	x24, x19, x26, lsl #3
   44950:	cmp	x26, #0x2
   44954:	add	x25, x24, x8, lsl #3
   44958:	b.ne	44964 <__gmpn_brootinv@@Base+0x120>  // b.any
   4495c:	mov	w8, wzr
   44960:	b	44984 <__gmpn_brootinv@@Base+0x140>
   44964:	mov	x8, xzr
   44968:	add	x9, sp, #0x18
   4496c:	add	x10, x26, #0x1
   44970:	str	x26, [x9, x8, lsl #3]
   44974:	asr	x26, x10, #1
   44978:	cmp	x26, #0x2
   4497c:	add	x8, x8, #0x1
   44980:	b.ne	4496c <__gmpn_brootinv@@Base+0x128>  // b.any
   44984:	mov	w9, #0x2                   	// #2
   44988:	add	x10, sp, #0x18
   4498c:	sxtw	x28, w8
   44990:	mov	w26, #0x1                   	// #1
   44994:	str	x9, [x10, w8, uxtw #3]
   44998:	b	449c8 <__gmpn_brootinv@@Base+0x184>
   4499c:	bl	ce90 <__gmpn_neg@plt>
   449a0:	mov	x0, x22
   449a4:	mov	x1, x19
   449a8:	mov	x2, x26
   449ac:	mov	x3, x20
   449b0:	mov	x4, x23
   449b4:	mov	w5, wzr
   449b8:	bl	c650 <__gmpn_pi1_bdiv_q_1@plt>
   449bc:	cmp	x28, #0x0
   449c0:	sub	x28, x28, #0x1
   449c4:	b.le	44a68 <__gmpn_brootinv@@Base+0x224>
   449c8:	mov	x0, x19
   449cc:	mov	x1, x22
   449d0:	mov	x2, x26
   449d4:	bl	ca90 <__gmpn_sqr@plt>
   449d8:	ldur	x8, [x29, #-16]
   449dc:	mov	x0, x24
   449e0:	mov	x1, x22
   449e4:	mov	x2, x26
   449e8:	lsl	x3, x8, #1
   449ec:	bl	d670 <__gmpn_mul_1@plt>
   449f0:	str	x0, [x24, x26, lsl #3]
   449f4:	add	x8, sp, #0x18
   449f8:	ldr	x26, [x8, x28, lsl #3]
   449fc:	sub	x2, x29, #0x10
   44a00:	mov	w3, #0x1                   	// #1
   44a04:	mov	x0, x22
   44a08:	mov	x1, x19
   44a0c:	mov	x4, x26
   44a10:	mov	x5, x25
   44a14:	bl	c520 <__gmpn_powlo@plt>
   44a18:	mov	x0, x19
   44a1c:	mov	x1, x21
   44a20:	mov	x2, x22
   44a24:	mov	x3, x26
   44a28:	bl	d090 <__gmpn_mullo_n@plt>
   44a2c:	add	x8, x26, #0x3
   44a30:	asr	x27, x8, #1
   44a34:	mov	x0, x19
   44a38:	mov	x1, x24
   44a3c:	mov	x2, x19
   44a40:	mov	x3, x27
   44a44:	bl	c420 <__gmpn_sub_n@plt>
   44a48:	subs	x2, x26, x27
   44a4c:	b.le	449a0 <__gmpn_brootinv@@Base+0x15c>
   44a50:	mov	x8, x0
   44a54:	add	x0, x19, x27, lsl #3
   44a58:	mov	x1, x0
   44a5c:	cbz	x8, 4499c <__gmpn_brootinv@@Base+0x158>
   44a60:	bl	c3e0 <__gmpn_com@plt>
   44a64:	b	449a0 <__gmpn_brootinv@@Base+0x15c>
   44a68:	add	sp, sp, #0x230
   44a6c:	ldp	x20, x19, [sp, #80]
   44a70:	ldp	x22, x21, [sp, #64]
   44a74:	ldp	x24, x23, [sp, #48]
   44a78:	ldp	x26, x25, [sp, #32]
   44a7c:	ldp	x28, x27, [sp, #16]
   44a80:	ldp	x29, x30, [sp], #96
   44a84:	ret
   44a88:	mov	x8, x0
   44a8c:	mov	w0, #0x1                   	// #1
   44a90:	mul	x8, x8, x8
   44a94:	tst	x1, #0x1
   44a98:	csinc	x9, x8, xzr, ne  // ne = any
   44a9c:	lsr	x1, x1, #1
   44aa0:	mul	x0, x9, x0
   44aa4:	cbnz	x1, 44a90 <__gmpn_brootinv@@Base+0x24c>
   44aa8:	ret

0000000000044aac <__gmpn_bsqrt@@Base>:
   44aac:	stp	x29, x30, [sp, #-48]!
   44ab0:	stp	x22, x21, [sp, #16]
   44ab4:	stp	x20, x19, [sp, #32]
   44ab8:	mov	x19, x3
   44abc:	lsr	x22, x2, #6
   44ac0:	mov	x21, x0
   44ac4:	add	x3, x3, x22, lsl #3
   44ac8:	mov	x0, x19
   44acc:	mov	x29, sp
   44ad0:	mov	x20, x1
   44ad4:	bl	c8e0 <__gmpn_bsqrtinv@plt>
   44ad8:	mov	x0, x21
   44adc:	mov	x1, x19
   44ae0:	mov	x2, x20
   44ae4:	mov	x3, x22
   44ae8:	bl	d090 <__gmpn_mullo_n@plt>
   44aec:	ldp	x20, x19, [sp, #32]
   44af0:	ldp	x22, x21, [sp, #16]
   44af4:	ldp	x29, x30, [sp], #48
   44af8:	ret

0000000000044afc <__gmpn_bsqrtinv@@Base>:
   44afc:	stp	x29, x30, [sp, #-80]!
   44b00:	stp	x28, x25, [sp, #16]
   44b04:	stp	x24, x23, [sp, #32]
   44b08:	stp	x22, x21, [sp, #48]
   44b0c:	stp	x20, x19, [sp, #64]
   44b10:	mov	x29, sp
   44b14:	sub	sp, sp, #0x210
   44b18:	mov	w8, #0x1                   	// #1
   44b1c:	str	x8, [x0]
   44b20:	ldr	x8, [x1]
   44b24:	cmp	x2, #0x1
   44b28:	b.ne	44b3c <__gmpn_bsqrtinv@@Base+0x40>  // b.any
   44b2c:	and	x8, x8, #0x3
   44b30:	cmp	x8, #0x1
   44b34:	b.eq	44c28 <__gmpn_bsqrtinv@@Base+0x12c>  // b.none
   44b38:	b	44b64 <__gmpn_bsqrtinv@@Base+0x68>
   44b3c:	and	x8, x8, #0x7
   44b40:	cmp	x8, #0x1
   44b44:	b.ne	44b64 <__gmpn_bsqrtinv@@Base+0x68>  // b.any
   44b48:	mov	x19, x1
   44b4c:	mov	x20, x0
   44b50:	mov	x21, x3
   44b54:	cmp	x2, #0x2
   44b58:	b.ne	44b6c <__gmpn_bsqrtinv@@Base+0x70>  // b.any
   44b5c:	mov	w8, wzr
   44b60:	b	44b90 <__gmpn_bsqrtinv@@Base+0x94>
   44b64:	mov	w0, wzr
   44b68:	b	44c2c <__gmpn_bsqrtinv@@Base+0x130>
   44b6c:	mov	x8, xzr
   44b70:	add	x9, sp, #0x8
   44b74:	mov	x10, x2
   44b78:	str	x10, [x9, x8, lsl #3]
   44b7c:	add	x10, x10, #0x2
   44b80:	lsr	x10, x10, #1
   44b84:	cmp	x10, #0x2
   44b88:	add	x8, x8, #0x1
   44b8c:	b.ne	44b78 <__gmpn_bsqrtinv@@Base+0x7c>  // b.any
   44b90:	cmp	w8, #0x1
   44b94:	b.lt	44c28 <__gmpn_bsqrtinv@@Base+0x12c>  // b.tstop
   44b98:	lsr	x9, x2, #3
   44b9c:	and	x9, x9, #0x1ffffffffffffff8
   44ba0:	sxtw	x24, w8
   44ba4:	add	x8, sp, #0x8
   44ba8:	add	x9, x9, x21
   44bac:	add	x22, x9, #0x8
   44bb0:	sub	x25, x8, #0x8
   44bb4:	ldr	x8, [x25, x24, lsl #3]
   44bb8:	mov	x0, x21
   44bbc:	mov	x1, x20
   44bc0:	lsr	x8, x8, #6
   44bc4:	add	x23, x8, #0x1
   44bc8:	mov	x2, x23
   44bcc:	bl	cbb0 <__gmpn_sqrlo@plt>
   44bd0:	mov	x0, x22
   44bd4:	mov	x1, x20
   44bd8:	mov	x2, x21
   44bdc:	mov	x3, x23
   44be0:	bl	d090 <__gmpn_mullo_n@plt>
   44be4:	mov	w3, #0x3                   	// #3
   44be8:	mov	x0, x21
   44bec:	mov	x1, x20
   44bf0:	mov	x2, x23
   44bf4:	bl	d670 <__gmpn_mul_1@plt>
   44bf8:	mov	x0, x20
   44bfc:	mov	x1, x19
   44c00:	mov	x2, x22
   44c04:	mov	x3, x23
   44c08:	bl	d090 <__gmpn_mullo_n@plt>
   44c0c:	mov	x0, x20
   44c10:	mov	x1, x21
   44c14:	mov	x2, x20
   44c18:	mov	x3, x23
   44c1c:	bl	c9f0 <__gmpn_rsh1sub_n@plt>
   44c20:	subs	x24, x24, #0x1
   44c24:	b.gt	44bb4 <__gmpn_bsqrtinv@@Base+0xb8>
   44c28:	mov	w0, #0x1                   	// #1
   44c2c:	add	sp, sp, #0x210
   44c30:	ldp	x20, x19, [sp, #64]
   44c34:	ldp	x22, x21, [sp, #48]
   44c38:	ldp	x24, x23, [sp, #32]
   44c3c:	ldp	x28, x25, [sp, #16]
   44c40:	ldp	x29, x30, [sp], #80
   44c44:	ret

0000000000044c48 <__gmpn_divexact@@Base>:
   44c48:	stp	x29, x30, [sp, #-96]!
   44c4c:	stp	x26, x25, [sp, #32]
   44c50:	stp	x24, x23, [sp, #48]
   44c54:	stp	x22, x21, [sp, #64]
   44c58:	stp	x20, x19, [sp, #80]
   44c5c:	mov	x23, x3
   44c60:	ldr	x3, [x3]
   44c64:	mov	x21, x4
   44c68:	mov	x20, x1
   44c6c:	mov	x19, x0
   44c70:	str	x27, [sp, #16]
   44c74:	mov	x29, sp
   44c78:	cbnz	x3, 44c90 <__gmpn_divexact@@Base+0x48>
   44c7c:	ldr	x3, [x23, #8]!
   44c80:	add	x20, x20, #0x8
   44c84:	sub	x21, x21, #0x1
   44c88:	sub	x2, x2, #0x1
   44c8c:	cbz	x3, 44c7c <__gmpn_divexact@@Base+0x34>
   44c90:	cmp	x21, #0x1
   44c94:	b.ne	44ca8 <__gmpn_divexact@@Base+0x60>  // b.any
   44c98:	mov	x0, x19
   44c9c:	mov	x1, x20
   44ca0:	bl	c910 <__gmpn_divexact_1@plt>
   44ca4:	b	44dac <__gmpn_divexact@@Base+0x164>
   44ca8:	sub	x8, x2, x21
   44cac:	rbit	x9, x3
   44cb0:	clz	x24, x9
   44cb4:	add	x22, x8, #0x1
   44cb8:	str	xzr, [x29, #24]
   44cbc:	cbz	w24, 44d44 <__gmpn_divexact@@Base+0xfc>
   44cc0:	cmp	x21, x22
   44cc4:	csinc	x27, x21, x22, le
   44cc8:	lsl	x1, x27, #3
   44ccc:	mov	w8, #0x7f00                	// #32512
   44cd0:	cmp	x1, x8
   44cd4:	add	x25, x22, #0x1
   44cd8:	b.hi	44de4 <__gmpn_divexact@@Base+0x19c>  // b.pmore
   44cdc:	add	x9, x1, #0xf
   44ce0:	mov	x8, sp
   44ce4:	and	x9, x9, #0xfffffffffffffff0
   44ce8:	sub	x26, x8, x9
   44cec:	mov	sp, x26
   44cf0:	mov	x0, x26
   44cf4:	mov	x1, x23
   44cf8:	mov	x2, x27
   44cfc:	mov	w3, w24
   44d00:	bl	c2f0 <__gmpn_rshift@plt>
   44d04:	lsl	x1, x25, #3
   44d08:	mov	w8, #0x7f00                	// #32512
   44d0c:	cmp	x1, x8
   44d10:	b.hi	44df4 <__gmpn_divexact@@Base+0x1ac>  // b.pmore
   44d14:	add	x9, x1, #0xf
   44d18:	mov	x8, sp
   44d1c:	and	x9, x9, #0xfffffffffffffff0
   44d20:	sub	x27, x8, x9
   44d24:	mov	sp, x27
   44d28:	mov	x0, x27
   44d2c:	mov	x1, x20
   44d30:	mov	x2, x25
   44d34:	mov	w3, w24
   44d38:	bl	c2f0 <__gmpn_rshift@plt>
   44d3c:	mov	x23, x26
   44d40:	mov	x20, x27
   44d44:	cmp	x21, x22
   44d48:	csel	x21, x22, x21, gt
   44d4c:	mov	x0, x22
   44d50:	mov	x1, x21
   44d54:	bl	d560 <__gmpn_bdiv_q_itch@plt>
   44d58:	lsl	x1, x0, #3
   44d5c:	mov	w8, #0x7f00                	// #32512
   44d60:	cmp	x1, x8
   44d64:	b.hi	44dcc <__gmpn_divexact@@Base+0x184>  // b.pmore
   44d68:	add	x9, x1, #0xf
   44d6c:	mov	x8, sp
   44d70:	and	x9, x9, #0xfffffffffffffff0
   44d74:	sub	x5, x8, x9
   44d78:	mov	sp, x5
   44d7c:	mov	x0, x19
   44d80:	mov	x1, x20
   44d84:	mov	x2, x22
   44d88:	mov	x3, x23
   44d8c:	mov	x4, x21
   44d90:	bl	cbf0 <__gmpn_bdiv_q@plt>
   44d94:	ldr	x0, [x29, #24]
   44d98:	cbnz	x0, 44ddc <__gmpn_divexact@@Base+0x194>
   44d9c:	mov	x0, x19
   44da0:	mov	x1, x19
   44da4:	mov	x2, x22
   44da8:	bl	ce90 <__gmpn_neg@plt>
   44dac:	mov	sp, x29
   44db0:	ldp	x20, x19, [sp, #80]
   44db4:	ldp	x22, x21, [sp, #64]
   44db8:	ldp	x24, x23, [sp, #48]
   44dbc:	ldp	x26, x25, [sp, #32]
   44dc0:	ldr	x27, [sp, #16]
   44dc4:	ldp	x29, x30, [sp], #96
   44dc8:	ret
   44dcc:	add	x0, x29, #0x18
   44dd0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   44dd4:	mov	x5, x0
   44dd8:	b	44d7c <__gmpn_divexact@@Base+0x134>
   44ddc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   44de0:	b	44d9c <__gmpn_divexact@@Base+0x154>
   44de4:	add	x0, x29, #0x18
   44de8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   44dec:	mov	x26, x0
   44df0:	b	44cf0 <__gmpn_divexact@@Base+0xa8>
   44df4:	add	x0, x29, #0x18
   44df8:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   44dfc:	mov	x27, x0
   44e00:	b	44d28 <__gmpn_divexact@@Base+0xe0>
   44e04:	nop
   44e08:	nop
   44e0c:	nop

0000000000044e10 <__gmpn_bdiv_dbm1c@@Base>:
   44e10:	ldr	x5, [x1], #8
   44e14:	ands	x6, x2, #0x3
   44e18:	b.eq	44e38 <__gmpn_bdiv_dbm1c@@Base+0x28>  // b.none
   44e1c:	cmp	x6, #0x2
   44e20:	b.cc	44e48 <__gmpn_bdiv_dbm1c@@Base+0x38>  // b.lo, b.ul, b.last
   44e24:	b.eq	44e60 <__gmpn_bdiv_dbm1c@@Base+0x50>  // b.none
   44e28:	mul	x12, x5, x3
   44e2c:	umulh	x13, x5, x3
   44e30:	ldr	x5, [x1], #8
   44e34:	b	44eb0 <__gmpn_bdiv_dbm1c@@Base+0xa0>
   44e38:	mul	x10, x5, x3
   44e3c:	umulh	x11, x5, x3
   44e40:	ldr	x5, [x1], #8
   44e44:	b	44e98 <__gmpn_bdiv_dbm1c@@Base+0x88>
   44e48:	subs	x2, x2, #0x1
   44e4c:	mul	x12, x5, x3
   44e50:	umulh	x13, x5, x3
   44e54:	b.ls	44ee4 <__gmpn_bdiv_dbm1c@@Base+0xd4>  // b.plast
   44e58:	ldr	x5, [x1], #8
   44e5c:	b	44e80 <__gmpn_bdiv_dbm1c@@Base+0x70>
   44e60:	mul	x10, x5, x3
   44e64:	umulh	x11, x5, x3
   44e68:	ldr	x5, [x1], #8
   44e6c:	b	44ec8 <__gmpn_bdiv_dbm1c@@Base+0xb8>
   44e70:	ldr	x5, [x1], #8
   44e74:	subs	x4, x4, x10
   44e78:	str	x4, [x0], #8
   44e7c:	sbc	x4, x4, x11
   44e80:	mul	x10, x5, x3
   44e84:	umulh	x11, x5, x3
   44e88:	ldr	x5, [x1], #8
   44e8c:	subs	x4, x4, x12
   44e90:	str	x4, [x0], #8
   44e94:	sbc	x4, x4, x13
   44e98:	mul	x12, x5, x3
   44e9c:	umulh	x13, x5, x3
   44ea0:	ldr	x5, [x1], #8
   44ea4:	subs	x4, x4, x10
   44ea8:	str	x4, [x0], #8
   44eac:	sbc	x4, x4, x11
   44eb0:	mul	x10, x5, x3
   44eb4:	umulh	x11, x5, x3
   44eb8:	ldr	x5, [x1], #8
   44ebc:	subs	x4, x4, x12
   44ec0:	str	x4, [x0], #8
   44ec4:	sbc	x4, x4, x13
   44ec8:	subs	x2, x2, #0x4
   44ecc:	mul	x12, x5, x3
   44ed0:	umulh	x13, x5, x3
   44ed4:	b.hi	44e70 <__gmpn_bdiv_dbm1c@@Base+0x60>  // b.pmore
   44ed8:	subs	x4, x4, x10
   44edc:	str	x4, [x0], #8
   44ee0:	sbc	x4, x4, x11
   44ee4:	subs	x4, x4, x12
   44ee8:	str	x4, [x0]
   44eec:	sbc	x0, x4, x13
   44ef0:	ret

0000000000044ef4 <__gmpn_redc_1@@Base>:
   44ef4:	stp	x29, x30, [sp, #-64]!
   44ef8:	stp	x22, x21, [sp, #32]
   44efc:	stp	x20, x19, [sp, #48]
   44f00:	mov	x19, x3
   44f04:	mov	x20, x1
   44f08:	cmp	x3, #0x1
   44f0c:	mov	x21, x0
   44f10:	stp	x24, x23, [sp, #16]
   44f14:	mov	x29, sp
   44f18:	b.lt	44f50 <__gmpn_redc_1@@Base+0x5c>  // b.tstop
   44f1c:	mov	x22, x4
   44f20:	mov	x23, x2
   44f24:	add	x24, x19, #0x1
   44f28:	ldr	x8, [x20]
   44f2c:	mov	x0, x20
   44f30:	mov	x1, x23
   44f34:	mov	x2, x19
   44f38:	mul	x3, x8, x22
   44f3c:	bl	d5e0 <__gmpn_addmul_1@plt>
   44f40:	sub	x24, x24, #0x1
   44f44:	cmp	x24, #0x1
   44f48:	str	x0, [x20], #8
   44f4c:	b.gt	44f28 <__gmpn_redc_1@@Base+0x34>
   44f50:	sub	x2, x20, x19, lsl #3
   44f54:	mov	x0, x21
   44f58:	mov	x1, x20
   44f5c:	mov	x3, x19
   44f60:	bl	cc30 <__gmpn_add_n@plt>
   44f64:	ldp	x20, x19, [sp, #48]
   44f68:	ldp	x22, x21, [sp, #32]
   44f6c:	ldp	x24, x23, [sp, #16]
   44f70:	ldp	x29, x30, [sp], #64
   44f74:	ret

0000000000044f78 <__gmpn_redc_2@@Base>:
   44f78:	sub	sp, sp, #0x60
   44f7c:	stp	x24, x23, [sp, #48]
   44f80:	stp	x22, x21, [sp, #64]
   44f84:	stp	x20, x19, [sp, #80]
   44f88:	mov	x22, x4
   44f8c:	mov	x19, x3
   44f90:	mov	x23, x2
   44f94:	mov	x20, x1
   44f98:	mov	x21, x0
   44f9c:	stp	x29, x30, [sp, #16]
   44fa0:	stp	x26, x25, [sp, #32]
   44fa4:	add	x29, sp, #0x10
   44fa8:	tbz	w19, #0, 44fcc <__gmpn_redc_2@@Base+0x54>
   44fac:	ldr	x8, [x20]
   44fb0:	ldr	x9, [x22]
   44fb4:	mov	x0, x20
   44fb8:	mov	x1, x23
   44fbc:	mov	x2, x19
   44fc0:	mul	x3, x9, x8
   44fc4:	bl	d5e0 <__gmpn_addmul_1@plt>
   44fc8:	str	x0, [x20], #8
   44fcc:	cmp	x19, #0x2
   44fd0:	b.lt	45030 <__gmpn_redc_2@@Base+0xb8>  // b.tstop
   44fd4:	add	x24, x19, #0x2
   44fd8:	lsl	x25, x19, #3
   44fdc:	ldp	x8, x11, [x22]
   44fe0:	ldp	x9, x10, [x20]
   44fe4:	umulh	x12, x8, x9
   44fe8:	mov	x3, sp
   44fec:	mov	x0, x20
   44ff0:	mul	x13, x9, x8
   44ff4:	madd	x8, x10, x8, x12
   44ff8:	madd	x8, x11, x9, x8
   44ffc:	stp	x13, x8, [sp]
   45000:	ldr	x26, [x20, x25]
   45004:	mov	x1, x23
   45008:	mov	x2, x19
   4500c:	bl	45060 <__gmpn_redc_2@@Base+0xe8>
   45010:	str	x0, [x20, #8]
   45014:	ldr	x8, [x20, x25]
   45018:	sub	x24, x24, #0x2
   4501c:	cmp	x24, #0x3
   45020:	str	x8, [x20]
   45024:	str	x26, [x20, x25]
   45028:	add	x20, x20, #0x10
   4502c:	b.gt	44fdc <__gmpn_redc_2@@Base+0x64>
   45030:	sub	x2, x20, x19, lsl #3
   45034:	mov	x0, x21
   45038:	mov	x1, x20
   4503c:	mov	x3, x19
   45040:	bl	cc30 <__gmpn_add_n@plt>
   45044:	ldp	x20, x19, [sp, #80]
   45048:	ldp	x22, x21, [sp, #64]
   4504c:	ldp	x24, x23, [sp, #48]
   45050:	ldp	x26, x25, [sp, #32]
   45054:	ldp	x29, x30, [sp, #16]
   45058:	add	sp, sp, #0x60
   4505c:	ret
   45060:	stp	x29, x30, [sp, #-48]!
   45064:	stp	x22, x21, [sp, #16]
   45068:	stp	x20, x19, [sp, #32]
   4506c:	mov	x19, x3
   45070:	ldr	x3, [x3]
   45074:	mov	x29, sp
   45078:	mov	x20, x2
   4507c:	mov	x21, x1
   45080:	mov	x22, x0
   45084:	bl	d5e0 <__gmpn_addmul_1@plt>
   45088:	str	x0, [x22, x20, lsl #3]
   4508c:	ldr	x3, [x19, #8]
   45090:	add	x0, x22, #0x8
   45094:	mov	x1, x21
   45098:	mov	x2, x20
   4509c:	bl	d5e0 <__gmpn_addmul_1@plt>
   450a0:	ldp	x20, x19, [sp, #32]
   450a4:	ldp	x22, x21, [sp, #16]
   450a8:	ldp	x29, x30, [sp], #48
   450ac:	ret

00000000000450b0 <__gmpn_redc_n@@Base>:
   450b0:	stp	x29, x30, [sp, #-96]!
   450b4:	stp	x22, x21, [sp, #64]
   450b8:	mov	x29, sp
   450bc:	mov	x21, x0
   450c0:	mov	x0, x3
   450c4:	str	x27, [sp, #16]
   450c8:	stp	x26, x25, [sp, #32]
   450cc:	stp	x24, x23, [sp, #48]
   450d0:	stp	x20, x19, [sp, #80]
   450d4:	mov	x25, x4
   450d8:	mov	x19, x3
   450dc:	mov	x20, x2
   450e0:	mov	x22, x1
   450e4:	str	xzr, [x29, #24]
   450e8:	bl	ca10 <__gmpn_mulmod_bnm1_next_size@plt>
   450ec:	mov	x1, x19
   450f0:	mov	x2, x19
   450f4:	mov	x23, x0
   450f8:	add	x24, x0, x19
   450fc:	bl	45244 <__gmpn_redc_n@@Base+0x194>
   45100:	add	x8, x24, x0
   45104:	lsl	x1, x8, #3
   45108:	mov	w8, #0x7f00                	// #32512
   4510c:	cmp	x1, x8
   45110:	b.hi	45214 <__gmpn_redc_n@@Base+0x164>  // b.pmore
   45114:	add	x9, x1, #0xf
   45118:	mov	x8, sp
   4511c:	and	x9, x9, #0xfffffffffffffff0
   45120:	sub	x24, x8, x9
   45124:	mov	sp, x24
   45128:	mov	x0, x24
   4512c:	mov	x1, x22
   45130:	mov	x2, x25
   45134:	mov	x3, x19
   45138:	bl	d090 <__gmpn_mullo_n@plt>
   4513c:	add	x25, x24, x19, lsl #3
   45140:	add	x26, x25, x23, lsl #3
   45144:	mov	x0, x25
   45148:	mov	x1, x23
   4514c:	mov	x2, x24
   45150:	mov	x3, x19
   45154:	mov	x4, x20
   45158:	mov	x5, x19
   4515c:	mov	x6, x26
   45160:	bl	cb40 <__gmpn_mulmod_bnm1@plt>
   45164:	lsl	x27, x19, #1
   45168:	subs	x3, x27, x23
   4516c:	b.le	4522c <__gmpn_redc_n@@Base+0x17c>
   45170:	mov	x0, x26
   45174:	mov	x1, x25
   45178:	mov	x2, x22
   4517c:	bl	c420 <__gmpn_sub_n@plt>
   45180:	add	x8, x25, x27, lsl #3
   45184:	sub	x8, x8, x23, lsl #3
   45188:	ldr	x9, [x8]
   4518c:	subs	x9, x9, x0
   45190:	str	x9, [x8]
   45194:	b.cs	451bc <__gmpn_redc_n@@Base+0x10c>  // b.hs, b.nlast
   45198:	mov	w8, #0x18                  	// #24
   4519c:	mul	x8, x19, x8
   451a0:	sub	x8, x8, x23, lsl #3
   451a4:	add	x8, x8, x24
   451a8:	add	x8, x8, #0x8
   451ac:	ldr	x9, [x8]
   451b0:	sub	x10, x9, #0x1
   451b4:	str	x10, [x8], #8
   451b8:	cbz	x9, 451ac <__gmpn_redc_n@@Base+0xfc>
   451bc:	lsl	x8, x19, #3
   451c0:	add	x1, x22, x8
   451c4:	add	x2, x25, x8
   451c8:	mov	x0, x21
   451cc:	mov	x3, x19
   451d0:	bl	c420 <__gmpn_sub_n@plt>
   451d4:	cbz	x0, 451ec <__gmpn_redc_n@@Base+0x13c>
   451d8:	mov	x0, x21
   451dc:	mov	x1, x21
   451e0:	mov	x2, x20
   451e4:	mov	x3, x19
   451e8:	bl	cc30 <__gmpn_add_n@plt>
   451ec:	ldr	x0, [x29, #24]
   451f0:	cbnz	x0, 45224 <__gmpn_redc_n@@Base+0x174>
   451f4:	mov	sp, x29
   451f8:	ldp	x20, x19, [sp, #80]
   451fc:	ldp	x22, x21, [sp, #64]
   45200:	ldp	x24, x23, [sp, #48]
   45204:	ldp	x26, x25, [sp, #32]
   45208:	ldr	x27, [sp, #16]
   4520c:	ldp	x29, x30, [sp], #96
   45210:	ret
   45214:	add	x0, x29, #0x18
   45218:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4521c:	mov	x24, x0
   45220:	b	45128 <__gmpn_redc_n@@Base+0x78>
   45224:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   45228:	b	451f4 <__gmpn_redc_n@@Base+0x144>
   4522c:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   45230:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   45234:	add	x0, x0, #0x237
   45238:	add	x2, x2, #0x240
   4523c:	mov	w1, #0x46                  	// #70
   45240:	bl	c850 <__gmp_assert_fail@plt>
   45244:	asr	x8, x0, #1
   45248:	cmp	x8, x2
   4524c:	csel	x9, x0, x8, lt  // lt = tstop
   45250:	cmp	x8, x1
   45254:	csel	x8, x9, xzr, lt  // lt = tstop
   45258:	add	x8, x0, x8
   4525c:	add	x0, x8, #0x4
   45260:	ret

0000000000045264 <__gmpn_powm@@Base>:
   45264:	stp	x29, x30, [sp, #-96]!
   45268:	stp	x28, x27, [sp, #16]
   4526c:	stp	x26, x25, [sp, #32]
   45270:	stp	x24, x23, [sp, #48]
   45274:	stp	x22, x21, [sp, #64]
   45278:	stp	x20, x19, [sp, #80]
   4527c:	mov	x29, sp
   45280:	sub	sp, sp, #0x40
   45284:	stp	x2, xzr, [x29, #-32]
   45288:	add	x8, x3, x4, lsl #3
   4528c:	ldur	x8, [x8, #-8]
   45290:	lsl	x9, x4, #6
   45294:	mov	x21, x0
   45298:	mov	x22, x7
   4529c:	clz	x8, x8
   452a0:	sub	x0, x9, x8
   452a4:	mov	x19, x6
   452a8:	mov	x20, x5
   452ac:	mov	x23, x3
   452b0:	mov	x28, x1
   452b4:	stur	x0, [x29, #-40]
   452b8:	bl	45ad0 <__gmpn_powm@@Base+0x86c>
   452bc:	mov	w25, w0
   452c0:	cmp	x19, #0x2a
   452c4:	lsl	x1, x19, #3
   452c8:	stur	x1, [x29, #-56]
   452cc:	b.le	45308 <__gmpn_powm@@Base+0xa4>
   452d0:	mov	w8, #0x7f00                	// #32512
   452d4:	cmp	x1, x8
   452d8:	b.hi	45ac0 <__gmpn_powm@@Base+0x85c>  // b.pmore
   452dc:	add	x9, x1, #0xf
   452e0:	mov	x8, sp
   452e4:	and	x9, x9, #0xfffffffffffffff0
   452e8:	sub	x26, x8, x9
   452ec:	mov	sp, x26
   452f0:	mov	x0, x26
   452f4:	mov	x1, x20
   452f8:	mov	x2, x19
   452fc:	mov	x3, x22
   45300:	bl	cef0 <__gmpn_binvert@plt>
   45304:	b	45344 <__gmpn_powm@@Base+0xe0>
   45308:	ldr	x8, [x20]
   4530c:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   45310:	ldr	x9, [x9, #3952]
   45314:	sub	x26, x29, #0x10
   45318:	ubfx	x10, x8, #1, #7
   4531c:	ldrb	w9, [x9, x10]
   45320:	mov	w10, #0x2                   	// #2
   45324:	msub	x11, x8, x9, x10
   45328:	mul	x9, x11, x9
   4532c:	msub	x10, x9, x8, x10
   45330:	mul	x9, x9, x10
   45334:	orr	x10, xzr, #0xfffffffffffffffe
   45338:	madd	x8, x9, x8, x10
   4533c:	mul	x8, x8, x9
   45340:	stur	x8, [x29, #-16]
   45344:	sub	w27, w25, #0x1
   45348:	lsl	x8, x19, x27
   4534c:	lsl	x1, x8, #3
   45350:	mov	w8, #0x7f00                	// #32512
   45354:	cmp	x1, x8
   45358:	b.hi	45aa8 <__gmpn_powm@@Base+0x844>  // b.pmore
   4535c:	add	x9, x1, #0xf
   45360:	mov	x8, sp
   45364:	and	x9, x9, #0xfffffffffffffff0
   45368:	sub	x24, x8, x9
   4536c:	mov	sp, x24
   45370:	ldur	x2, [x29, #-32]
   45374:	mov	x0, x24
   45378:	mov	x1, x28
   4537c:	mov	x3, x20
   45380:	mov	x4, x19
   45384:	bl	45af8 <__gmpn_powm@@Base+0x894>
   45388:	mov	x0, x22
   4538c:	mov	x1, x24
   45390:	mov	x2, x19
   45394:	stur	x24, [x29, #-32]
   45398:	bl	ca90 <__gmpn_sqr@plt>
   4539c:	cmp	x19, #0x2a
   453a0:	b.le	453c0 <__gmpn_powm@@Base+0x15c>
   453a4:	mov	x0, x21
   453a8:	mov	x1, x22
   453ac:	mov	x2, x20
   453b0:	mov	x3, x19
   453b4:	mov	x4, x26
   453b8:	bl	c980 <__gmpn_redc_n@plt>
   453bc:	b	453f0 <__gmpn_powm@@Base+0x18c>
   453c0:	ldr	x4, [x26]
   453c4:	mov	x0, x21
   453c8:	mov	x1, x22
   453cc:	mov	x2, x20
   453d0:	mov	x3, x19
   453d4:	bl	d2c0 <__gmpn_redc_1@plt>
   453d8:	cbz	x0, 453f0 <__gmpn_powm@@Base+0x18c>
   453dc:	mov	x0, x21
   453e0:	mov	x1, x21
   453e4:	mov	x2, x20
   453e8:	mov	x3, x19
   453ec:	bl	c420 <__gmpn_sub_n@plt>
   453f0:	mov	w8, #0xffffffff            	// #-1
   453f4:	lsl	w8, w8, w27
   453f8:	cmn	w8, #0x2
   453fc:	b.gt	454d4 <__gmpn_powm@@Base+0x270>
   45400:	ldur	x24, [x29, #-32]
   45404:	mvn	w8, w8
   45408:	sxtw	x8, w8
   4540c:	add	x27, x8, #0x1
   45410:	b	4545c <__gmpn_powm@@Base+0x1f8>
   45414:	ldr	x8, [x24]
   45418:	ldr	x9, [x21]
   4541c:	umulh	x10, x8, x9
   45420:	mul	x8, x9, x8
   45424:	stp	x8, x10, [x22]
   45428:	ldr	x9, [x26]
   4542c:	ldr	x11, [x20]
   45430:	cmp	x8, #0x0
   45434:	mul	x9, x9, x8
   45438:	umulh	x9, x11, x9
   4543c:	cinc	x8, x9, ne  // ne = any
   45440:	adds	x8, x8, x10
   45444:	csel	x9, x11, xzr, cs  // cs = hs, nlast
   45448:	sub	x8, x8, x9
   4544c:	str	x8, [x24, #8]!
   45450:	sub	x27, x27, #0x1
   45454:	cmp	x27, #0x1
   45458:	b.le	454d4 <__gmpn_powm@@Base+0x270>
   4545c:	cmp	x19, #0x1
   45460:	b.eq	45414 <__gmpn_powm@@Base+0x1b0>  // b.none
   45464:	mov	x0, x22
   45468:	mov	x1, x24
   4546c:	mov	x2, x21
   45470:	mov	x3, x19
   45474:	bl	cb50 <__gmpn_mul_n@plt>
   45478:	cmp	x19, #0x2a
   4547c:	add	x24, x24, x19, lsl #3
   45480:	b.le	454a0 <__gmpn_powm@@Base+0x23c>
   45484:	mov	x0, x24
   45488:	mov	x1, x22
   4548c:	mov	x2, x20
   45490:	mov	x3, x19
   45494:	mov	x4, x26
   45498:	bl	c980 <__gmpn_redc_n@plt>
   4549c:	b	45450 <__gmpn_powm@@Base+0x1ec>
   454a0:	ldr	x4, [x26]
   454a4:	mov	x0, x24
   454a8:	mov	x1, x22
   454ac:	mov	x2, x20
   454b0:	mov	x3, x19
   454b4:	bl	d2c0 <__gmpn_redc_1@plt>
   454b8:	cbz	x0, 45450 <__gmpn_powm@@Base+0x1ec>
   454bc:	mov	x0, x24
   454c0:	mov	x1, x24
   454c4:	mov	x2, x20
   454c8:	mov	x3, x19
   454cc:	bl	c420 <__gmpn_sub_n@plt>
   454d0:	b	45450 <__gmpn_powm@@Base+0x1ec>
   454d4:	ldur	x24, [x29, #-40]
   454d8:	mov	x0, x23
   454dc:	mov	w2, w25
   454e0:	mov	x1, x24
   454e4:	bl	45be8 <__gmpn_powm@@Base+0x984>
   454e8:	sxtw	x27, w25
   454ec:	rbit	x8, x0
   454f0:	subs	x9, x24, x27
   454f4:	clz	x8, x8
   454f8:	ldur	x24, [x29, #-32]
   454fc:	csel	x9, xzr, x9, cc  // cc = lo, ul, last
   45500:	lsr	x10, x0, x8
   45504:	add	x28, x8, x9
   45508:	lsr	x8, x10, #1
   4550c:	mul	x8, x8, x19
   45510:	add	x1, x24, x8, lsl #3
   45514:	mov	x0, x21
   45518:	mov	x2, x19
   4551c:	bl	cc10 <__gmpn_copyi@plt>
   45520:	cmp	x19, #0x1
   45524:	b.ne	4563c <__gmpn_powm@@Base+0x3d8>  // b.any
   45528:	cbnz	x28, 45570 <__gmpn_powm@@Base+0x30c>
   4552c:	b	459e0 <__gmpn_powm@@Base+0x77c>
   45530:	ldr	x9, [x21]
   45534:	umulh	x10, x9, x9
   45538:	mov	x28, x8
   4553c:	mul	x9, x9, x9
   45540:	stp	x9, x10, [x22]
   45544:	ldr	x11, [x26]
   45548:	ldr	x12, [x20]
   4554c:	cmp	x9, #0x0
   45550:	mul	x11, x11, x9
   45554:	umulh	x11, x12, x11
   45558:	cinc	x9, x11, ne  // ne = any
   4555c:	adds	x9, x9, x10
   45560:	csel	x10, x12, xzr, cs  // cs = hs, nlast
   45564:	sub	x9, x9, x10
   45568:	str	x9, [x21]
   4556c:	cbz	x8, 459e0 <__gmpn_powm@@Base+0x77c>
   45570:	sub	x8, x28, #0x1
   45574:	lsr	x9, x8, #3
   45578:	and	x9, x9, #0x1ffffffffffffff8
   4557c:	ldr	x9, [x23, x9]
   45580:	lsr	x9, x9, x8
   45584:	tbz	w9, #0, 45530 <__gmpn_powm@@Base+0x2cc>
   45588:	mov	x0, x23
   4558c:	mov	x1, x28
   45590:	mov	w2, w25
   45594:	bl	45be8 <__gmpn_powm@@Base+0x984>
   45598:	ldr	x8, [x26]
   4559c:	subs	x9, x28, x27
   455a0:	rbit	x10, x0
   455a4:	csel	w11, w28, w25, cc  // cc = lo, ul, last
   455a8:	csel	x12, xzr, x9, cc  // cc = lo, ul, last
   455ac:	clz	x9, x10
   455b0:	add	x28, x9, x12
   455b4:	sub	w10, w9, w11
   455b8:	ldr	x11, [x21]
   455bc:	umulh	x12, x11, x11
   455c0:	mul	x11, x11, x11
   455c4:	stp	x11, x12, [x22]
   455c8:	ldr	x13, [x20]
   455cc:	mul	x14, x8, x11
   455d0:	cmp	x11, #0x0
   455d4:	umulh	x11, x13, x14
   455d8:	cinc	x11, x11, ne  // ne = any
   455dc:	adds	x11, x11, x12
   455e0:	csel	x12, x13, xzr, cs  // cs = hs, nlast
   455e4:	sub	x11, x11, x12
   455e8:	adds	w10, w10, #0x1
   455ec:	str	x11, [x21]
   455f0:	b.cc	455b8 <__gmpn_powm@@Base+0x354>  // b.lo, b.ul, b.last
   455f4:	lsr	x9, x0, x9
   455f8:	lsr	x9, x9, #1
   455fc:	mul	x9, x9, x19
   45600:	ldr	x9, [x24, x9, lsl #3]
   45604:	umulh	x10, x11, x9
   45608:	mul	x9, x9, x11
   4560c:	stp	x9, x10, [x22]
   45610:	ldr	x11, [x20]
   45614:	mul	x8, x9, x8
   45618:	umulh	x8, x11, x8
   4561c:	cmp	x9, #0x0
   45620:	cinc	x8, x8, ne  // ne = any
   45624:	adds	x8, x8, x10
   45628:	csel	x9, x11, xzr, cs  // cs = hs, nlast
   4562c:	sub	x8, x8, x9
   45630:	str	x8, [x21]
   45634:	cbnz	x28, 45570 <__gmpn_powm@@Base+0x30c>
   45638:	b	459e0 <__gmpn_powm@@Base+0x77c>
   4563c:	cmp	x19, #0xd
   45640:	stp	x27, x25, [x29, #-48]
   45644:	b.le	45748 <__gmpn_powm@@Base+0x4e4>
   45648:	cmp	x19, #0x2a
   4564c:	b.le	45898 <__gmpn_powm@@Base+0x634>
   45650:	cbnz	x28, 45688 <__gmpn_powm@@Base+0x424>
   45654:	b	459e0 <__gmpn_powm@@Base+0x77c>
   45658:	mov	x0, x22
   4565c:	mov	x1, x21
   45660:	mov	x2, x19
   45664:	bl	ca90 <__gmpn_sqr@plt>
   45668:	mov	x0, x21
   4566c:	mov	x1, x22
   45670:	mov	x2, x20
   45674:	mov	x3, x19
   45678:	mov	x4, x26
   4567c:	bl	c980 <__gmpn_redc_n@plt>
   45680:	mov	x28, x24
   45684:	cbz	x24, 459e0 <__gmpn_powm@@Base+0x77c>
   45688:	sub	x24, x28, #0x1
   4568c:	lsr	x8, x24, #3
   45690:	and	x8, x8, #0x1ffffffffffffff8
   45694:	ldr	x8, [x23, x8]
   45698:	lsr	x8, x8, x24
   4569c:	tbz	w8, #0, 45658 <__gmpn_powm@@Base+0x3f4>
   456a0:	mov	x0, x23
   456a4:	mov	x1, x28
   456a8:	mov	w2, w25
   456ac:	bl	45be8 <__gmpn_powm@@Base+0x984>
   456b0:	subs	x8, x28, x27
   456b4:	rbit	x9, x0
   456b8:	csel	w10, w28, w25, cc  // cc = lo, ul, last
   456bc:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   456c0:	clz	x25, x9
   456c4:	mov	x24, x0
   456c8:	add	x28, x25, x8
   456cc:	sub	w27, w25, w10
   456d0:	mov	x0, x22
   456d4:	mov	x1, x21
   456d8:	mov	x2, x19
   456dc:	bl	ca90 <__gmpn_sqr@plt>
   456e0:	mov	x0, x21
   456e4:	mov	x1, x22
   456e8:	mov	x2, x20
   456ec:	mov	x3, x19
   456f0:	mov	x4, x26
   456f4:	bl	c980 <__gmpn_redc_n@plt>
   456f8:	adds	w27, w27, #0x1
   456fc:	b.cc	456d0 <__gmpn_powm@@Base+0x46c>  // b.lo, b.ul, b.last
   45700:	ldur	x9, [x29, #-32]
   45704:	lsr	x8, x24, x25
   45708:	lsr	x8, x8, #1
   4570c:	mul	x8, x8, x19
   45710:	add	x2, x9, x8, lsl #3
   45714:	mov	x0, x22
   45718:	mov	x1, x21
   4571c:	mov	x3, x19
   45720:	bl	cb50 <__gmpn_mul_n@plt>
   45724:	mov	x0, x21
   45728:	mov	x1, x22
   4572c:	mov	x2, x20
   45730:	mov	x3, x19
   45734:	mov	x4, x26
   45738:	bl	c980 <__gmpn_redc_n@plt>
   4573c:	ldp	x27, x25, [x29, #-48]
   45740:	cbnz	x28, 45688 <__gmpn_powm@@Base+0x424>
   45744:	b	459e0 <__gmpn_powm@@Base+0x77c>
   45748:	cbnz	x28, 45758 <__gmpn_powm@@Base+0x4f4>
   4574c:	b	459e0 <__gmpn_powm@@Base+0x77c>
   45750:	ldp	x27, x25, [x29, #-48]
   45754:	cbz	x28, 459e0 <__gmpn_powm@@Base+0x77c>
   45758:	sub	x8, x28, #0x1
   4575c:	lsr	x9, x8, #3
   45760:	and	x9, x9, #0x1ffffffffffffff8
   45764:	ldr	x9, [x23, x9]
   45768:	lsr	x9, x9, x8
   4576c:	tbz	w9, #0, 4584c <__gmpn_powm@@Base+0x5e8>
   45770:	mov	x0, x23
   45774:	mov	x1, x28
   45778:	mov	w2, w25
   4577c:	bl	45be8 <__gmpn_powm@@Base+0x984>
   45780:	subs	x8, x28, x27
   45784:	rbit	x9, x0
   45788:	csel	w10, w28, w25, cc  // cc = lo, ul, last
   4578c:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   45790:	clz	x27, x9
   45794:	mov	x24, x0
   45798:	add	x28, x27, x8
   4579c:	sub	w25, w27, w10
   457a0:	b	457ac <__gmpn_powm@@Base+0x548>
   457a4:	adds	w25, w25, #0x1
   457a8:	b.cs	457f0 <__gmpn_powm@@Base+0x58c>  // b.hs, b.nlast
   457ac:	mov	x0, x22
   457b0:	mov	x1, x21
   457b4:	mov	x2, x19
   457b8:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   457bc:	ldr	x4, [x26]
   457c0:	mov	x0, x21
   457c4:	mov	x1, x22
   457c8:	mov	x2, x20
   457cc:	mov	x3, x19
   457d0:	bl	d2c0 <__gmpn_redc_1@plt>
   457d4:	cbz	x0, 457a4 <__gmpn_powm@@Base+0x540>
   457d8:	mov	x0, x21
   457dc:	mov	x1, x21
   457e0:	mov	x2, x20
   457e4:	mov	x3, x19
   457e8:	bl	c420 <__gmpn_sub_n@plt>
   457ec:	b	457a4 <__gmpn_powm@@Base+0x540>
   457f0:	ldur	x9, [x29, #-32]
   457f4:	lsr	x8, x24, x27
   457f8:	lsr	x8, x8, #1
   457fc:	mul	x8, x8, x19
   45800:	add	x3, x9, x8, lsl #3
   45804:	mov	x0, x22
   45808:	mov	x1, x21
   4580c:	mov	x2, x19
   45810:	mov	x4, x19
   45814:	bl	c6e0 <__gmpn_mul_basecase@plt>
   45818:	ldr	x4, [x26]
   4581c:	mov	x0, x21
   45820:	mov	x1, x22
   45824:	mov	x2, x20
   45828:	mov	x3, x19
   4582c:	bl	d2c0 <__gmpn_redc_1@plt>
   45830:	cbz	x0, 45750 <__gmpn_powm@@Base+0x4ec>
   45834:	mov	x0, x21
   45838:	mov	x1, x21
   4583c:	mov	x2, x20
   45840:	mov	x3, x19
   45844:	bl	c420 <__gmpn_sub_n@plt>
   45848:	b	45750 <__gmpn_powm@@Base+0x4ec>
   4584c:	mov	x0, x22
   45850:	mov	x1, x21
   45854:	mov	x2, x19
   45858:	mov	x28, x8
   4585c:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   45860:	ldr	x4, [x26]
   45864:	mov	x0, x21
   45868:	mov	x1, x22
   4586c:	mov	x2, x20
   45870:	mov	x3, x19
   45874:	bl	d2c0 <__gmpn_redc_1@plt>
   45878:	cbz	x0, 45890 <__gmpn_powm@@Base+0x62c>
   4587c:	mov	x0, x21
   45880:	mov	x1, x21
   45884:	mov	x2, x20
   45888:	mov	x3, x19
   4588c:	bl	c420 <__gmpn_sub_n@plt>
   45890:	cbnz	x28, 45758 <__gmpn_powm@@Base+0x4f4>
   45894:	b	459e0 <__gmpn_powm@@Base+0x77c>
   45898:	cbnz	x28, 458a8 <__gmpn_powm@@Base+0x644>
   4589c:	b	459e0 <__gmpn_powm@@Base+0x77c>
   458a0:	ldp	x27, x25, [x29, #-48]
   458a4:	cbz	x28, 459e0 <__gmpn_powm@@Base+0x77c>
   458a8:	sub	x8, x28, #0x1
   458ac:	lsr	x9, x8, #3
   458b0:	and	x9, x9, #0x1ffffffffffffff8
   458b4:	ldr	x9, [x23, x9]
   458b8:	lsr	x9, x9, x8
   458bc:	tbz	w9, #0, 45998 <__gmpn_powm@@Base+0x734>
   458c0:	mov	x0, x23
   458c4:	mov	x1, x28
   458c8:	mov	w2, w25
   458cc:	bl	45be8 <__gmpn_powm@@Base+0x984>
   458d0:	subs	x8, x28, x27
   458d4:	rbit	x9, x0
   458d8:	csel	w10, w28, w25, cc  // cc = lo, ul, last
   458dc:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   458e0:	clz	x27, x9
   458e4:	mov	x24, x0
   458e8:	add	x28, x27, x8
   458ec:	sub	w25, w27, w10
   458f0:	b	458fc <__gmpn_powm@@Base+0x698>
   458f4:	adds	w25, w25, #0x1
   458f8:	b.cs	45940 <__gmpn_powm@@Base+0x6dc>  // b.hs, b.nlast
   458fc:	mov	x0, x22
   45900:	mov	x1, x21
   45904:	mov	x2, x19
   45908:	bl	ca90 <__gmpn_sqr@plt>
   4590c:	ldr	x4, [x26]
   45910:	mov	x0, x21
   45914:	mov	x1, x22
   45918:	mov	x2, x20
   4591c:	mov	x3, x19
   45920:	bl	d2c0 <__gmpn_redc_1@plt>
   45924:	cbz	x0, 458f4 <__gmpn_powm@@Base+0x690>
   45928:	mov	x0, x21
   4592c:	mov	x1, x21
   45930:	mov	x2, x20
   45934:	mov	x3, x19
   45938:	bl	c420 <__gmpn_sub_n@plt>
   4593c:	b	458f4 <__gmpn_powm@@Base+0x690>
   45940:	ldur	x9, [x29, #-32]
   45944:	lsr	x8, x24, x27
   45948:	lsr	x8, x8, #1
   4594c:	mul	x8, x8, x19
   45950:	add	x2, x9, x8, lsl #3
   45954:	mov	x0, x22
   45958:	mov	x1, x21
   4595c:	mov	x3, x19
   45960:	bl	cb50 <__gmpn_mul_n@plt>
   45964:	ldr	x4, [x26]
   45968:	mov	x0, x21
   4596c:	mov	x1, x22
   45970:	mov	x2, x20
   45974:	mov	x3, x19
   45978:	bl	d2c0 <__gmpn_redc_1@plt>
   4597c:	cbz	x0, 458a0 <__gmpn_powm@@Base+0x63c>
   45980:	mov	x0, x21
   45984:	mov	x1, x21
   45988:	mov	x2, x20
   4598c:	mov	x3, x19
   45990:	bl	c420 <__gmpn_sub_n@plt>
   45994:	b	458a0 <__gmpn_powm@@Base+0x63c>
   45998:	mov	x0, x22
   4599c:	mov	x1, x21
   459a0:	mov	x2, x19
   459a4:	mov	x28, x8
   459a8:	bl	ca90 <__gmpn_sqr@plt>
   459ac:	ldr	x4, [x26]
   459b0:	mov	x0, x21
   459b4:	mov	x1, x22
   459b8:	mov	x2, x20
   459bc:	mov	x3, x19
   459c0:	bl	d2c0 <__gmpn_redc_1@plt>
   459c4:	cbz	x0, 459dc <__gmpn_powm@@Base+0x778>
   459c8:	mov	x0, x21
   459cc:	mov	x1, x21
   459d0:	mov	x2, x20
   459d4:	mov	x3, x19
   459d8:	bl	c420 <__gmpn_sub_n@plt>
   459dc:	cbnz	x28, 458a8 <__gmpn_powm@@Base+0x644>
   459e0:	mov	x0, x22
   459e4:	mov	x1, x21
   459e8:	mov	x2, x19
   459ec:	bl	cc10 <__gmpn_copyi@plt>
   459f0:	cbz	x19, 45a04 <__gmpn_powm@@Base+0x7a0>
   459f4:	ldur	x2, [x29, #-56]
   459f8:	mov	w1, wzr
   459fc:	add	x0, x22, x2
   45a00:	bl	c780 <memset@plt>
   45a04:	cmp	x19, #0x2a
   45a08:	b.le	45a28 <__gmpn_powm@@Base+0x7c4>
   45a0c:	mov	x0, x21
   45a10:	mov	x1, x22
   45a14:	mov	x2, x20
   45a18:	mov	x3, x19
   45a1c:	mov	x4, x26
   45a20:	bl	c980 <__gmpn_redc_n@plt>
   45a24:	b	45a58 <__gmpn_powm@@Base+0x7f4>
   45a28:	ldr	x4, [x26]
   45a2c:	mov	x0, x21
   45a30:	mov	x1, x22
   45a34:	mov	x2, x20
   45a38:	mov	x3, x19
   45a3c:	bl	d2c0 <__gmpn_redc_1@plt>
   45a40:	cbz	x0, 45a58 <__gmpn_powm@@Base+0x7f4>
   45a44:	mov	x0, x21
   45a48:	mov	x1, x21
   45a4c:	mov	x2, x20
   45a50:	mov	x3, x19
   45a54:	bl	c420 <__gmpn_sub_n@plt>
   45a58:	mov	x0, x21
   45a5c:	mov	x1, x20
   45a60:	mov	x2, x19
   45a64:	bl	c570 <__gmpn_cmp@plt>
   45a68:	tbnz	w0, #31, 45a80 <__gmpn_powm@@Base+0x81c>
   45a6c:	mov	x0, x21
   45a70:	mov	x1, x21
   45a74:	mov	x2, x20
   45a78:	mov	x3, x19
   45a7c:	bl	c420 <__gmpn_sub_n@plt>
   45a80:	ldur	x0, [x29, #-24]
   45a84:	cbnz	x0, 45ab8 <__gmpn_powm@@Base+0x854>
   45a88:	mov	sp, x29
   45a8c:	ldp	x20, x19, [sp, #80]
   45a90:	ldp	x22, x21, [sp, #64]
   45a94:	ldp	x24, x23, [sp, #48]
   45a98:	ldp	x26, x25, [sp, #32]
   45a9c:	ldp	x28, x27, [sp, #16]
   45aa0:	ldp	x29, x30, [sp], #96
   45aa4:	ret
   45aa8:	sub	x0, x29, #0x18
   45aac:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   45ab0:	mov	x24, x0
   45ab4:	b	45370 <__gmpn_powm@@Base+0x10c>
   45ab8:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   45abc:	b	45a88 <__gmpn_powm@@Base+0x824>
   45ac0:	sub	x0, x29, #0x18
   45ac4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   45ac8:	mov	x26, x0
   45acc:	b	452f0 <__gmpn_powm@@Base+0x8c>
   45ad0:	adrp	x9, 50000 <__gmpn_bases@@Base+0x2f98>
   45ad4:	mov	x8, x0
   45ad8:	mov	x0, xzr
   45adc:	add	x9, x9, #0x250
   45ae0:	add	x10, x9, x0, lsl #3
   45ae4:	ldr	x10, [x10, #8]
   45ae8:	add	x0, x0, #0x1
   45aec:	cmp	x10, x8
   45af0:	b.cc	45ae0 <__gmpn_powm@@Base+0x87c>  // b.lo, b.ul, b.last
   45af4:	ret
   45af8:	stp	x29, x30, [sp, #-80]!
   45afc:	stp	x26, x25, [sp, #16]
   45b00:	stp	x24, x23, [sp, #32]
   45b04:	stp	x22, x21, [sp, #48]
   45b08:	stp	x20, x19, [sp, #64]
   45b0c:	mov	x29, sp
   45b10:	sub	sp, sp, #0x10
   45b14:	add	x21, x4, x2
   45b18:	add	x8, x2, x21
   45b1c:	lsl	x8, x8, #3
   45b20:	mov	x24, x1
   45b24:	add	x1, x8, #0x8
   45b28:	mov	w8, #0x7f00                	// #32512
   45b2c:	mov	x19, x4
   45b30:	mov	x20, x3
   45b34:	mov	x23, x2
   45b38:	mov	x22, x0
   45b3c:	cmp	x1, x8
   45b40:	stur	xzr, [x29, #-8]
   45b44:	b.hi	45bc8 <__gmpn_powm@@Base+0x964>  // b.pmore
   45b48:	add	x9, x1, #0xf
   45b4c:	mov	x8, sp
   45b50:	and	x9, x9, #0xfffffffffffffff0
   45b54:	sub	x25, x8, x9
   45b58:	mov	sp, x25
   45b5c:	add	x26, x25, x21, lsl #3
   45b60:	cbz	x19, 45b74 <__gmpn_powm@@Base+0x910>
   45b64:	lsl	x2, x19, #3
   45b68:	mov	x0, x25
   45b6c:	mov	w1, wzr
   45b70:	bl	c780 <memset@plt>
   45b74:	add	x0, x25, x19, lsl #3
   45b78:	mov	x1, x24
   45b7c:	mov	x2, x23
   45b80:	bl	cc10 <__gmpn_copyi@plt>
   45b84:	mov	x0, x26
   45b88:	mov	x1, x22
   45b8c:	mov	x2, xzr
   45b90:	mov	x3, x25
   45b94:	mov	x4, x21
   45b98:	mov	x5, x20
   45b9c:	mov	x6, x19
   45ba0:	bl	c030 <__gmpn_tdiv_qr@plt>
   45ba4:	ldur	x0, [x29, #-8]
   45ba8:	cbnz	x0, 45be0 <__gmpn_powm@@Base+0x97c>
   45bac:	mov	sp, x29
   45bb0:	ldp	x20, x19, [sp, #64]
   45bb4:	ldp	x22, x21, [sp, #48]
   45bb8:	ldp	x24, x23, [sp, #32]
   45bbc:	ldp	x26, x25, [sp, #16]
   45bc0:	ldp	x29, x30, [sp], #80
   45bc4:	ret
   45bc8:	sub	x0, x29, #0x8
   45bcc:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   45bd0:	mov	x25, x0
   45bd4:	add	x26, x25, x21, lsl #3
   45bd8:	cbnz	x19, 45b64 <__gmpn_powm@@Base+0x900>
   45bdc:	b	45b74 <__gmpn_powm@@Base+0x910>
   45be0:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   45be4:	b	45bac <__gmpn_powm@@Base+0x948>
   45be8:	sxtw	x8, w2
   45bec:	cmp	x8, x1
   45bf0:	b.ls	45bfc <__gmpn_powm@@Base+0x998>  // b.plast
   45bf4:	ldr	x8, [x0]
   45bf8:	b	45c34 <__gmpn_powm@@Base+0x9d0>
   45bfc:	sub	x8, x1, x8
   45c00:	lsr	x9, x8, #6
   45c04:	ldr	x11, [x0, x9, lsl #3]
   45c08:	and	x10, x8, #0x3f
   45c0c:	mov	w12, #0x40                  	// #64
   45c10:	sub	w10, w12, w10
   45c14:	cmp	w10, w2
   45c18:	lsr	x8, x11, x8
   45c1c:	b.ge	45c30 <__gmpn_powm@@Base+0x9cc>  // b.tcont
   45c20:	add	x9, x0, x9, lsl #3
   45c24:	ldr	x9, [x9, #8]
   45c28:	lsl	x9, x9, x10
   45c2c:	add	x8, x9, x8
   45c30:	mov	w1, w2
   45c34:	mov	x9, #0xffffffffffffffff    	// #-1
   45c38:	lsl	x9, x9, x1
   45c3c:	bic	x0, x8, x9
   45c40:	ret

0000000000045c44 <__gmpn_powlo@@Base>:
   45c44:	stp	x29, x30, [sp, #-96]!
   45c48:	stp	x28, x27, [sp, #16]
   45c4c:	stp	x26, x25, [sp, #32]
   45c50:	stp	x24, x23, [sp, #48]
   45c54:	stp	x22, x21, [sp, #64]
   45c58:	stp	x20, x19, [sp, #80]
   45c5c:	mov	x29, sp
   45c60:	sub	sp, sp, #0x20
   45c64:	stur	xzr, [x29, #-8]
   45c68:	add	x8, x2, x3, lsl #3
   45c6c:	ldur	x8, [x8, #-8]
   45c70:	lsl	x9, x3, #6
   45c74:	mov	x20, x0
   45c78:	mov	x24, x5
   45c7c:	clz	x8, x8
   45c80:	sub	x25, x9, x8
   45c84:	mov	x0, x25
   45c88:	mov	x19, x4
   45c8c:	mov	x23, x2
   45c90:	mov	x26, x1
   45c94:	bl	45f1c <__gmpn_powlo@@Base+0x2d8>
   45c98:	mov	w22, w0
   45c9c:	cmp	w0, #0x2
   45ca0:	stur	w0, [x29, #-12]
   45ca4:	b.cc	45d78 <__gmpn_powlo@@Base+0x134>  // b.lo, b.ul, b.last
   45ca8:	sub	w22, w22, #0x1
   45cac:	lsl	x8, x19, x22
   45cb0:	lsl	x1, x8, #3
   45cb4:	mov	w8, #0x7f00                	// #32512
   45cb8:	mov	x21, x23
   45cbc:	cmp	x1, x8
   45cc0:	b.hi	45f0c <__gmpn_powlo@@Base+0x2c8>  // b.pmore
   45cc4:	add	x9, x1, #0xf
   45cc8:	mov	x8, sp
   45ccc:	and	x9, x9, #0xfffffffffffffff0
   45cd0:	sub	x28, x8, x9
   45cd4:	mov	sp, x28
   45cd8:	mov	x0, x28
   45cdc:	mov	x1, x26
   45ce0:	mov	x2, x19
   45ce4:	bl	cc10 <__gmpn_copyi@plt>
   45ce8:	mov	x0, x24
   45cec:	mov	x1, x26
   45cf0:	mov	x2, x19
   45cf4:	bl	cbb0 <__gmpn_sqrlo@plt>
   45cf8:	mov	w8, #0xffffffff            	// #-1
   45cfc:	lsl	w8, w8, w22
   45d00:	mvn	w8, w8
   45d04:	sxtw	x22, w8
   45d08:	lsl	x23, x19, #3
   45d0c:	mov	x1, x28
   45d10:	add	x26, x1, x23
   45d14:	mov	x0, x26
   45d18:	mov	x2, x24
   45d1c:	mov	x3, x19
   45d20:	bl	d090 <__gmpn_mullo_n@plt>
   45d24:	subs	x22, x22, #0x1
   45d28:	mov	x1, x26
   45d2c:	b.ne	45d10 <__gmpn_powlo@@Base+0xcc>  // b.any
   45d30:	ldur	w22, [x29, #-12]
   45d34:	mov	x0, x21
   45d38:	mov	x1, x25
   45d3c:	mov	x23, x21
   45d40:	mov	w2, w22
   45d44:	bl	45f40 <__gmpn_powlo@@Base+0x2fc>
   45d48:	rbit	x8, x0
   45d4c:	sub	x9, x25, w22, uxtw
   45d50:	clz	x8, x8
   45d54:	add	x26, x9, x8
   45d58:	lsr	x8, x0, x8
   45d5c:	lsr	x8, x8, #1
   45d60:	mul	x8, x8, x19
   45d64:	add	x1, x28, x8, lsl #3
   45d68:	mov	x0, x20
   45d6c:	mov	x2, x19
   45d70:	bl	cc10 <__gmpn_copyi@plt>
   45d74:	b	45da0 <__gmpn_powlo@@Base+0x15c>
   45d78:	add	x28, x24, x19, lsl #3
   45d7c:	mov	x0, x28
   45d80:	mov	x1, x26
   45d84:	mov	x2, x19
   45d88:	bl	cc10 <__gmpn_copyi@plt>
   45d8c:	mov	x0, x20
   45d90:	mov	x1, x26
   45d94:	mov	x2, x19
   45d98:	bl	cc10 <__gmpn_copyi@plt>
   45d9c:	sub	x26, x25, #0x1
   45da0:	mov	w27, wzr
   45da4:	mov	w8, w22
   45da8:	stp	x28, x8, [x29, #-32]
   45dac:	b	45dd4 <__gmpn_powlo@@Base+0x190>
   45db0:	mov	x0, x20
   45db4:	mov	x1, x25
   45db8:	mov	x2, x19
   45dbc:	bl	cbb0 <__gmpn_sqrlo@plt>
   45dc0:	cmp	w27, #0x0
   45dc4:	cset	w27, eq  // eq = none
   45dc8:	mov	x26, x22
   45dcc:	mov	x24, x25
   45dd0:	cbz	x22, 45ec8 <__gmpn_powlo@@Base+0x284>
   45dd4:	sub	x22, x26, #0x1
   45dd8:	lsr	x8, x22, #3
   45ddc:	and	x8, x8, #0x1ffffffffffffff8
   45de0:	ldr	x8, [x23, x8]
   45de4:	mov	x25, x20
   45de8:	mov	x20, x24
   45dec:	lsr	x8, x8, x22
   45df0:	tbz	w8, #0, 45db0 <__gmpn_powlo@@Base+0x16c>
   45df4:	ldur	w2, [x29, #-12]
   45df8:	mov	x0, x23
   45dfc:	mov	x1, x26
   45e00:	bl	45f40 <__gmpn_powlo@@Base+0x2fc>
   45e04:	ldur	x9, [x29, #-24]
   45e08:	rbit	x8, x0
   45e0c:	clz	x22, x8
   45e10:	mov	x24, x0
   45e14:	cmp	x26, x9
   45e18:	csel	x9, x9, x26, hi  // hi = pmore
   45e1c:	sub	w21, w9, w22
   45e20:	cmp	w21, #0x2
   45e24:	sub	x26, x26, x9
   45e28:	b.cc	45e6c <__gmpn_powlo@@Base+0x228>  // b.lo, b.ul, b.last
   45e2c:	mov	x28, x23
   45e30:	mov	w23, w21
   45e34:	mov	x0, x20
   45e38:	mov	x1, x25
   45e3c:	mov	x2, x19
   45e40:	bl	cbb0 <__gmpn_sqrlo@plt>
   45e44:	mov	x0, x25
   45e48:	mov	x1, x20
   45e4c:	mov	x2, x19
   45e50:	bl	cbb0 <__gmpn_sqrlo@plt>
   45e54:	sub	w23, w23, #0x2
   45e58:	cmp	w23, #0x1
   45e5c:	b.hi	45e34 <__gmpn_powlo@@Base+0x1f0>  // b.pmore
   45e60:	mov	x23, x28
   45e64:	ldur	x28, [x29, #-32]
   45e68:	and	w21, w21, #0x1
   45e6c:	add	x26, x22, x26
   45e70:	lsr	x22, x24, x22
   45e74:	cbz	w21, 45e94 <__gmpn_powlo@@Base+0x250>
   45e78:	mov	x0, x20
   45e7c:	mov	x1, x25
   45e80:	mov	x2, x19
   45e84:	bl	cbb0 <__gmpn_sqrlo@plt>
   45e88:	mov	x24, x20
   45e8c:	mov	x20, x25
   45e90:	b	45ea4 <__gmpn_powlo@@Base+0x260>
   45e94:	cmp	w27, #0x0
   45e98:	cset	w27, eq  // eq = none
   45e9c:	mov	x24, x25
   45ea0:	mov	x25, x20
   45ea4:	lsr	x8, x22, #1
   45ea8:	mul	x8, x8, x19
   45eac:	add	x2, x28, x8, lsl #3
   45eb0:	mov	x0, x25
   45eb4:	mov	x1, x24
   45eb8:	mov	x3, x19
   45ebc:	bl	d090 <__gmpn_mullo_n@plt>
   45ec0:	mov	x25, x24
   45ec4:	cbnz	x26, 45dd4 <__gmpn_powlo@@Base+0x190>
   45ec8:	cbz	w27, 45edc <__gmpn_powlo@@Base+0x298>
   45ecc:	mov	x0, x25
   45ed0:	mov	x1, x20
   45ed4:	mov	x2, x19
   45ed8:	bl	cc10 <__gmpn_copyi@plt>
   45edc:	ldur	x0, [x29, #-8]
   45ee0:	cbnz	x0, 45f04 <__gmpn_powlo@@Base+0x2c0>
   45ee4:	mov	sp, x29
   45ee8:	ldp	x20, x19, [sp, #80]
   45eec:	ldp	x22, x21, [sp, #64]
   45ef0:	ldp	x24, x23, [sp, #48]
   45ef4:	ldp	x26, x25, [sp, #32]
   45ef8:	ldp	x28, x27, [sp, #16]
   45efc:	ldp	x29, x30, [sp], #96
   45f00:	ret
   45f04:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   45f08:	b	45ee4 <__gmpn_powlo@@Base+0x2a0>
   45f0c:	sub	x0, x29, #0x8
   45f10:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   45f14:	mov	x28, x0
   45f18:	b	45cd8 <__gmpn_powlo@@Base+0x94>
   45f1c:	adrp	x9, 50000 <__gmpn_bases@@Base+0x2f98>
   45f20:	mov	w8, wzr
   45f24:	add	x9, x9, #0x2a8
   45f28:	ldr	x10, [x9, w8, uxtw #3]
   45f2c:	add	w8, w8, #0x1
   45f30:	cmp	x10, x0
   45f34:	b.cc	45f28 <__gmpn_powlo@@Base+0x2e4>  // b.lo, b.ul, b.last
   45f38:	mov	w0, w8
   45f3c:	ret
   45f40:	mov	w8, w2
   45f44:	cmp	x8, x1
   45f48:	b.ls	45f54 <__gmpn_powlo@@Base+0x310>  // b.plast
   45f4c:	ldr	x9, [x0]
   45f50:	b	45f8c <__gmpn_powlo@@Base+0x348>
   45f54:	sub	x9, x1, x8
   45f58:	lsr	x10, x9, #6
   45f5c:	ldr	x12, [x0, x10, lsl #3]
   45f60:	and	x11, x9, #0x3f
   45f64:	mov	w13, #0x40                  	// #64
   45f68:	sub	w11, w13, w11
   45f6c:	cmp	w11, w2
   45f70:	lsr	x9, x12, x9
   45f74:	b.cs	45f88 <__gmpn_powlo@@Base+0x344>  // b.hs, b.nlast
   45f78:	add	x10, x0, x10, lsl #3
   45f7c:	ldr	x10, [x10, #8]
   45f80:	lsl	x10, x10, x11
   45f84:	add	x9, x10, x9
   45f88:	mov	x1, x8
   45f8c:	mov	x8, #0xffffffffffffffff    	// #-1
   45f90:	lsl	x8, x8, x1
   45f94:	bic	x0, x9, x8
   45f98:	ret

0000000000045f9c <__gmpn_sec_powm@@Base>:
   45f9c:	sub	sp, sp, #0xa0
   45fa0:	stp	x29, x30, [sp, #64]
   45fa4:	stp	x22, x21, [sp, #128]
   45fa8:	add	x29, sp, #0x40
   45fac:	mov	x21, x0
   45fb0:	mov	x0, x4
   45fb4:	stp	x28, x27, [sp, #80]
   45fb8:	stp	x26, x25, [sp, #96]
   45fbc:	stp	x24, x23, [sp, #112]
   45fc0:	stp	x20, x19, [sp, #144]
   45fc4:	mov	x27, x7
   45fc8:	mov	x19, x6
   45fcc:	mov	x20, x5
   45fd0:	stp	x2, x3, [x29, #-24]
   45fd4:	str	x1, [sp, #32]
   45fd8:	str	x4, [sp]
   45fdc:	bl	46368 <__gmpn_sec_powm@@Base+0x3cc>
   45fe0:	ldr	x8, [x20]
   45fe4:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   45fe8:	ldr	x9, [x9, #3952]
   45fec:	lsl	x24, x19, #3
   45ff0:	ubfx	x10, x8, #1, #7
   45ff4:	add	x28, x27, x24
   45ff8:	ldrb	w9, [x9, x10]
   45ffc:	mov	w10, #0x2                   	// #2
   46000:	mov	w23, w0
   46004:	orr	x11, xzr, #0xfffffffffffffffe
   46008:	msub	x13, x8, x9, x10
   4600c:	mul	x9, x13, x9
   46010:	msub	x10, x9, x8, x10
   46014:	mul	x9, x9, x10
   46018:	mov	w26, #0x1                   	// #1
   4601c:	mov	x5, x28
   46020:	lsl	x12, x19, x23
   46024:	madd	x8, x9, x8, x11
   46028:	str	x26, [x5], #8
   4602c:	mov	w2, #0x1                   	// #1
   46030:	mov	x0, x27
   46034:	mov	x1, x28
   46038:	mov	x3, x20
   4603c:	mov	x4, x19
   46040:	add	x22, x27, x12, lsl #3
   46044:	mul	x25, x8, x9
   46048:	stur	x27, [x29, #-8]
   4604c:	bl	46390 <__gmpn_sec_powm@@Base+0x3f4>
   46050:	ldr	x1, [sp, #32]
   46054:	ldur	x2, [x29, #-24]
   46058:	add	x5, x28, x24
   4605c:	mov	x0, x28
   46060:	mov	x3, x20
   46064:	mov	x4, x19
   46068:	str	x24, [sp, #24]
   4606c:	bl	46390 <__gmpn_sec_powm@@Base+0x3f4>
   46070:	lsl	w8, w26, w23
   46074:	cmp	w8, #0x3
   46078:	stur	x23, [x29, #-24]
   4607c:	str	x8, [sp, #8]
   46080:	b.lt	46174 <__gmpn_sec_powm@@Base+0x1d8>  // b.tstop
   46084:	ldr	x8, [sp, #8]
   46088:	ldur	x27, [x29, #-8]
   4608c:	add	x9, x19, x19, lsl #1
   46090:	lsl	x9, x9, #3
   46094:	sub	w8, w8, #0x2
   46098:	sxtw	x8, w8
   4609c:	str	x9, [sp, #32]
   460a0:	lsl	x9, x19, #4
   460a4:	add	x26, x8, #0x2
   460a8:	mov	x24, x28
   460ac:	str	x9, [sp, #16]
   460b0:	b	46158 <__gmpn_sec_powm@@Base+0x1bc>
   460b4:	mov	x3, x24
   460b8:	mov	x4, x19
   460bc:	bl	c6e0 <__gmpn_mul_basecase@plt>
   460c0:	ldr	x8, [sp, #16]
   460c4:	mov	x1, x22
   460c8:	mov	x2, x20
   460cc:	mov	x3, x19
   460d0:	add	x23, x27, x8
   460d4:	mov	x0, x23
   460d8:	mov	x4, x25
   460dc:	bl	d2c0 <__gmpn_redc_1@plt>
   460e0:	mov	x1, x23
   460e4:	mov	x2, x23
   460e8:	mov	x3, x20
   460ec:	mov	x4, x19
   460f0:	bl	c160 <__gmpn_cnd_sub_n@plt>
   460f4:	mov	x0, x22
   460f8:	mov	x1, x23
   460fc:	mov	x2, x19
   46100:	mov	x3, x28
   46104:	mov	x4, x19
   46108:	bl	c6e0 <__gmpn_mul_basecase@plt>
   4610c:	ldr	x8, [sp, #32]
   46110:	mov	x1, x22
   46114:	mov	x2, x20
   46118:	mov	x3, x19
   4611c:	add	x27, x27, x8
   46120:	mov	x0, x27
   46124:	mov	x4, x25
   46128:	bl	d2c0 <__gmpn_redc_1@plt>
   4612c:	mov	x1, x27
   46130:	mov	x2, x27
   46134:	mov	x3, x20
   46138:	mov	x4, x19
   4613c:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46140:	ldr	x8, [sp, #24]
   46144:	sub	x26, x26, #0x2
   46148:	cmp	x26, #0x2
   4614c:	mov	x27, x23
   46150:	add	x24, x24, x8
   46154:	b.le	46174 <__gmpn_sec_powm@@Base+0x1d8>
   46158:	mov	x0, x22
   4615c:	mov	x1, x24
   46160:	mov	x2, x19
   46164:	cmp	x19, #0x11
   46168:	b.gt	460b4 <__gmpn_sec_powm@@Base+0x118>
   4616c:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   46170:	b	460c0 <__gmpn_sec_powm@@Base+0x124>
   46174:	ldur	x28, [x29, #-24]
   46178:	ldr	x23, [sp]
   4617c:	sxtw	x26, w28
   46180:	cmp	x26, x23
   46184:	b.hi	46350 <__gmpn_sec_powm@@Base+0x3b4>  // b.pmore
   46188:	ldur	x0, [x29, #-16]
   4618c:	mov	x1, x23
   46190:	mov	w2, w28
   46194:	bl	4642c <__gmpn_sec_powm@@Base+0x490>
   46198:	ldr	x8, [sp, #8]
   4619c:	ldur	x1, [x29, #-8]
   461a0:	mov	x4, x0
   461a4:	mov	x0, x21
   461a8:	sxtw	x3, w8
   461ac:	mov	x2, x19
   461b0:	sub	x27, x23, x26
   461b4:	str	x3, [sp, #32]
   461b8:	bl	c630 <__gmpn_sec_tabselect@plt>
   461bc:	cbz	x27, 462b0 <__gmpn_sec_powm@@Base+0x314>
   461c0:	add	x24, x22, x19, lsl #4
   461c4:	b	46234 <__gmpn_sec_powm@@Base+0x298>
   461c8:	ldur	x1, [x29, #-8]
   461cc:	ldr	x3, [sp, #32]
   461d0:	mov	x0, x24
   461d4:	mov	x2, x19
   461d8:	mov	x4, x23
   461dc:	bl	c630 <__gmpn_sec_tabselect@plt>
   461e0:	mov	x0, x22
   461e4:	mov	x1, x21
   461e8:	mov	x2, x19
   461ec:	mov	x3, x24
   461f0:	mov	x4, x19
   461f4:	bl	c6e0 <__gmpn_mul_basecase@plt>
   461f8:	mov	x0, x21
   461fc:	mov	x1, x22
   46200:	mov	x2, x20
   46204:	mov	x3, x19
   46208:	mov	x4, x25
   4620c:	bl	d2c0 <__gmpn_redc_1@plt>
   46210:	mov	x1, x21
   46214:	mov	x2, x21
   46218:	mov	x3, x20
   4621c:	mov	x4, x19
   46220:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46224:	ldur	x28, [x29, #-24]
   46228:	subs	x8, x27, x26
   4622c:	csel	x27, xzr, x8, cc  // cc = lo, ul, last
   46230:	b.ls	462b0 <__gmpn_sec_powm@@Base+0x314>  // b.plast
   46234:	ldur	x0, [x29, #-16]
   46238:	mov	x1, x27
   4623c:	mov	w2, w28
   46240:	bl	4642c <__gmpn_sec_powm@@Base+0x490>
   46244:	cmp	x27, x26
   46248:	mov	x23, x0
   4624c:	csel	w28, w27, w28, cc  // cc = lo, ul, last
   46250:	b	46294 <__gmpn_sec_powm@@Base+0x2f8>
   46254:	mov	x3, x21
   46258:	mov	x4, x19
   4625c:	bl	c6e0 <__gmpn_mul_basecase@plt>
   46260:	mov	x0, x21
   46264:	mov	x1, x22
   46268:	mov	x2, x20
   4626c:	mov	x3, x19
   46270:	mov	x4, x25
   46274:	bl	d2c0 <__gmpn_redc_1@plt>
   46278:	mov	x1, x21
   4627c:	mov	x2, x21
   46280:	mov	x3, x20
   46284:	mov	x4, x19
   46288:	bl	c160 <__gmpn_cnd_sub_n@plt>
   4628c:	subs	w28, w28, #0x1
   46290:	b.eq	461c8 <__gmpn_sec_powm@@Base+0x22c>  // b.none
   46294:	mov	x0, x22
   46298:	mov	x1, x21
   4629c:	mov	x2, x19
   462a0:	cmp	x19, #0x11
   462a4:	b.gt	46254 <__gmpn_sec_powm@@Base+0x2b8>
   462a8:	bl	c2e0 <__gmpn_sqr_basecase@plt>
   462ac:	b	46260 <__gmpn_sec_powm@@Base+0x2c4>
   462b0:	mov	x0, x22
   462b4:	mov	x1, x21
   462b8:	mov	x2, x19
   462bc:	bl	cc10 <__gmpn_copyi@plt>
   462c0:	cbz	x19, 462d4 <__gmpn_sec_powm@@Base+0x338>
   462c4:	ldr	x2, [sp, #24]
   462c8:	mov	w1, wzr
   462cc:	add	x0, x22, x2
   462d0:	bl	c780 <memset@plt>
   462d4:	mov	x0, x21
   462d8:	mov	x1, x22
   462dc:	mov	x2, x20
   462e0:	mov	x3, x19
   462e4:	mov	x4, x25
   462e8:	bl	d2c0 <__gmpn_redc_1@plt>
   462ec:	mov	x1, x21
   462f0:	mov	x2, x21
   462f4:	mov	x3, x20
   462f8:	mov	x4, x19
   462fc:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46300:	mov	x0, x22
   46304:	mov	x1, x21
   46308:	mov	x2, x20
   4630c:	mov	x3, x19
   46310:	bl	c420 <__gmpn_sub_n@plt>
   46314:	cmp	w0, #0x0
   46318:	cset	w0, eq  // eq = none
   4631c:	mov	x1, x21
   46320:	mov	x2, x21
   46324:	mov	x3, x20
   46328:	mov	x4, x19
   4632c:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46330:	ldp	x20, x19, [sp, #144]
   46334:	ldp	x22, x21, [sp, #128]
   46338:	ldp	x24, x23, [sp, #112]
   4633c:	ldp	x26, x25, [sp, #96]
   46340:	ldp	x28, x27, [sp, #80]
   46344:	ldp	x29, x30, [sp, #64]
   46348:	add	sp, sp, #0xa0
   4634c:	ret
   46350:	adrp	x0, 50000 <__gmpn_bases@@Base+0x2f98>
   46354:	adrp	x2, 50000 <__gmpn_bases@@Base+0x2f98>
   46358:	add	x0, x0, #0x2f8
   4635c:	add	x2, x2, #0x303
   46360:	mov	w1, #0x12a                 	// #298
   46364:	bl	c850 <__gmp_assert_fail@plt>
   46368:	adrp	x9, 50000 <__gmpn_bases@@Base+0x2f98>
   4636c:	mov	x8, x0
   46370:	mov	x0, xzr
   46374:	add	x9, x9, #0x318
   46378:	add	x10, x9, x0, lsl #3
   4637c:	ldr	x10, [x10, #8]
   46380:	add	x0, x0, #0x1
   46384:	cmp	x10, x8
   46388:	b.cc	46378 <__gmpn_sec_powm@@Base+0x3dc>  // b.lo, b.ul, b.last
   4638c:	ret
   46390:	stp	x29, x30, [sp, #-80]!
   46394:	str	x25, [sp, #16]
   46398:	stp	x24, x23, [sp, #32]
   4639c:	stp	x22, x21, [sp, #48]
   463a0:	stp	x20, x19, [sp, #64]
   463a4:	mov	x20, x5
   463a8:	mov	x19, x4
   463ac:	mov	x22, x3
   463b0:	mov	x23, x2
   463b4:	mov	x25, x1
   463b8:	mov	x21, x0
   463bc:	lsl	x24, x4, #3
   463c0:	mov	x29, sp
   463c4:	cbz	x4, 463d8 <__gmpn_sec_powm@@Base+0x43c>
   463c8:	mov	x0, x20
   463cc:	mov	w1, wzr
   463d0:	mov	x2, x24
   463d4:	bl	c780 <memset@plt>
   463d8:	add	x0, x20, x24
   463dc:	mov	x1, x25
   463e0:	mov	x2, x23
   463e4:	bl	cc10 <__gmpn_copyi@plt>
   463e8:	add	x8, x20, x23, lsl #3
   463ec:	add	x1, x19, x23
   463f0:	add	x4, x8, x24
   463f4:	mov	x0, x20
   463f8:	mov	x2, x22
   463fc:	mov	x3, x19
   46400:	bl	c290 <__gmpn_sec_div_r@plt>
   46404:	mov	x0, x21
   46408:	mov	x1, x20
   4640c:	mov	x2, x19
   46410:	bl	cc10 <__gmpn_copyi@plt>
   46414:	ldp	x20, x19, [sp, #64]
   46418:	ldp	x22, x21, [sp, #48]
   4641c:	ldp	x24, x23, [sp, #32]
   46420:	ldr	x25, [sp, #16]
   46424:	ldp	x29, x30, [sp], #80
   46428:	ret
   4642c:	sxtw	x8, w2
   46430:	cmp	x8, x1
   46434:	b.ls	46440 <__gmpn_sec_powm@@Base+0x4a4>  // b.plast
   46438:	ldr	x8, [x0]
   4643c:	b	46478 <__gmpn_sec_powm@@Base+0x4dc>
   46440:	sub	x8, x1, x8
   46444:	lsr	x9, x8, #6
   46448:	ldr	x11, [x0, x9, lsl #3]
   4644c:	and	x10, x8, #0x3f
   46450:	mov	w12, #0x40                  	// #64
   46454:	sub	w10, w12, w10
   46458:	cmp	w10, w2
   4645c:	lsr	x8, x11, x8
   46460:	b.ge	46474 <__gmpn_sec_powm@@Base+0x4d8>  // b.tcont
   46464:	add	x9, x0, x9, lsl #3
   46468:	ldr	x9, [x9, #8]
   4646c:	lsl	x9, x9, x10
   46470:	add	x8, x9, x8
   46474:	mov	w1, w2
   46478:	mov	x9, #0xffffffffffffffff    	// #-1
   4647c:	lsl	x9, x9, x1
   46480:	bic	x0, x8, x9
   46484:	ret

0000000000046488 <__gmpn_sec_powm_itch@@Base>:
   46488:	stp	x29, x30, [sp, #-32]!
   4648c:	stp	x20, x19, [sp, #16]
   46490:	mov	x20, x0
   46494:	mov	x0, x1
   46498:	mov	x29, sp
   4649c:	mov	x19, x2
   464a0:	bl	46368 <__gmpn_sec_powm@@Base+0x3cc>
   464a4:	add	x8, x19, x20
   464a8:	lsl	x9, x19, x0
   464ac:	add	x8, x8, x19, lsl #1
   464b0:	lsl	x8, x8, #1
   464b4:	add	x9, x9, x19, lsl #2
   464b8:	ldp	x20, x19, [sp, #16]
   464bc:	add	x8, x8, #0x2
   464c0:	cmp	x9, x8
   464c4:	csel	x0, x9, x8, gt
   464c8:	ldp	x29, x30, [sp], #32
   464cc:	ret

00000000000464d0 <__gmpn_sec_mul@@Base>:
   464d0:	stp	x29, x30, [sp, #-16]!
   464d4:	mov	x29, sp
   464d8:	bl	c6e0 <__gmpn_mul_basecase@plt>
   464dc:	ldp	x29, x30, [sp], #16
   464e0:	ret

00000000000464e4 <__gmpn_sec_mul_itch@@Base>:
   464e4:	mov	x0, xzr
   464e8:	ret

00000000000464ec <__gmpn_sec_sqr@@Base>:
   464ec:	stp	x29, x30, [sp, #-16]!
   464f0:	mov	x3, x1
   464f4:	mov	x4, x2
   464f8:	mov	x29, sp
   464fc:	bl	c6e0 <__gmpn_mul_basecase@plt>
   46500:	ldp	x29, x30, [sp], #16
   46504:	ret

0000000000046508 <__gmpn_sec_sqr_itch@@Base>:
   46508:	mov	x0, xzr
   4650c:	ret

0000000000046510 <__gmpn_sec_div_qr_itch@@Base>:
   46510:	add	x8, x0, x0, lsl #1
   46514:	add	x0, x8, #0x4
   46518:	ret

000000000004651c <__gmpn_sec_div_qr@@Base>:
   4651c:	sub	sp, sp, #0x70
   46520:	stp	x29, x30, [sp, #16]
   46524:	stp	x28, x27, [sp, #32]
   46528:	stp	x26, x25, [sp, #48]
   4652c:	stp	x24, x23, [sp, #64]
   46530:	stp	x22, x21, [sp, #80]
   46534:	stp	x20, x19, [sp, #96]
   46538:	sub	x26, x4, #0x1
   4653c:	ldr	x8, [x3, x26, lsl #3]
   46540:	mov	x21, x5
   46544:	mov	x19, x4
   46548:	mov	x25, x3
   4654c:	mov	x22, x2
   46550:	mov	x20, x1
   46554:	clz	x23, x8
   46558:	mov	x24, x0
   4655c:	add	x29, sp, #0x10
   46560:	cbz	w23, 46608 <__gmpn_sec_div_qr@@Base+0xec>
   46564:	mov	x0, x21
   46568:	mov	x1, x25
   4656c:	mov	x2, x19
   46570:	mov	w3, w23
   46574:	bl	c2d0 <__gmpn_lshift@plt>
   46578:	lsl	x28, x19, #3
   4657c:	add	x25, x21, x28
   46580:	mov	x0, x25
   46584:	mov	x1, x20
   46588:	mov	x2, x22
   4658c:	mov	w3, w23
   46590:	bl	c2d0 <__gmpn_lshift@plt>
   46594:	str	x24, [sp, #8]
   46598:	lsl	x24, x22, #3
   4659c:	str	x0, [x25, x24]
   465a0:	ldr	x8, [x21, x26, lsl #3]
   465a4:	add	x26, x22, #0x1
   465a8:	cmn	x8, #0x1
   465ac:	cinc	x0, x8, ne  // ne = any
   465b0:	bl	d5d0 <__gmpn_invert_limb@plt>
   465b4:	add	x27, x25, x28
   465b8:	add	x8, x21, x26, lsl #3
   465bc:	mov	x5, x0
   465c0:	add	x6, x8, x28
   465c4:	mov	x0, x27
   465c8:	mov	x1, x25
   465cc:	mov	x2, x26
   465d0:	mov	x3, x21
   465d4:	mov	x4, x19
   465d8:	bl	d0c0 <__gmpn_sec_pi1_div_qr@plt>
   465dc:	ldr	x0, [sp, #8]
   465e0:	sub	x2, x22, x19
   465e4:	mov	x1, x27
   465e8:	bl	cc10 <__gmpn_copyi@plt>
   465ec:	ldr	x21, [x25, x24]
   465f0:	mov	x0, x20
   465f4:	mov	x1, x25
   465f8:	mov	x2, x19
   465fc:	mov	w3, w23
   46600:	bl	c2f0 <__gmpn_rshift@plt>
   46604:	b	46638 <__gmpn_sec_div_qr@@Base+0x11c>
   46608:	cmn	x8, #0x1
   4660c:	cinc	x0, x8, ne  // ne = any
   46610:	bl	d5d0 <__gmpn_invert_limb@plt>
   46614:	mov	x5, x0
   46618:	mov	x0, x24
   4661c:	mov	x1, x20
   46620:	mov	x2, x22
   46624:	mov	x3, x25
   46628:	mov	x4, x19
   4662c:	mov	x6, x21
   46630:	bl	d0c0 <__gmpn_sec_pi1_div_qr@plt>
   46634:	mov	x21, x0
   46638:	mov	x0, x21
   4663c:	ldp	x20, x19, [sp, #96]
   46640:	ldp	x22, x21, [sp, #80]
   46644:	ldp	x24, x23, [sp, #64]
   46648:	ldp	x26, x25, [sp, #48]
   4664c:	ldp	x28, x27, [sp, #32]
   46650:	ldp	x29, x30, [sp, #16]
   46654:	add	sp, sp, #0x70
   46658:	ret

000000000004665c <__gmpn_sec_div_r_itch@@Base>:
   4665c:	add	x8, x0, x1, lsl #1
   46660:	add	x0, x8, #0x2
   46664:	ret

0000000000046668 <__gmpn_sec_div_r@@Base>:
   46668:	stp	x29, x30, [sp, #-80]!
   4666c:	stp	x26, x25, [sp, #16]
   46670:	stp	x24, x23, [sp, #32]
   46674:	stp	x22, x21, [sp, #48]
   46678:	stp	x20, x19, [sp, #64]
   4667c:	sub	x25, x3, #0x1
   46680:	ldr	x8, [x2, x25, lsl #3]
   46684:	mov	x20, x4
   46688:	mov	x19, x3
   4668c:	mov	x24, x2
   46690:	mov	x23, x1
   46694:	clz	x22, x8
   46698:	mov	x21, x0
   4669c:	mov	x29, sp
   466a0:	cbz	w22, 46724 <__gmpn_sec_div_r@@Base+0xbc>
   466a4:	mov	x0, x20
   466a8:	mov	x1, x24
   466ac:	mov	x2, x19
   466b0:	mov	w3, w22
   466b4:	bl	c2d0 <__gmpn_lshift@plt>
   466b8:	lsl	x26, x19, #3
   466bc:	add	x24, x20, x26
   466c0:	mov	x0, x24
   466c4:	mov	x1, x21
   466c8:	mov	x2, x23
   466cc:	mov	w3, w22
   466d0:	bl	c2d0 <__gmpn_lshift@plt>
   466d4:	str	x0, [x24, x23, lsl #3]
   466d8:	ldr	x8, [x20, x25, lsl #3]
   466dc:	add	x23, x23, #0x1
   466e0:	cmn	x8, #0x1
   466e4:	cinc	x0, x8, ne  // ne = any
   466e8:	bl	d5d0 <__gmpn_invert_limb@plt>
   466ec:	add	x8, x20, x23, lsl #3
   466f0:	mov	x4, x0
   466f4:	add	x5, x8, x26
   466f8:	mov	x0, x24
   466fc:	mov	x1, x23
   46700:	mov	x2, x20
   46704:	mov	x3, x19
   46708:	bl	c9b0 <__gmpn_sec_pi1_div_r@plt>
   4670c:	mov	x0, x21
   46710:	mov	x1, x24
   46714:	mov	x2, x19
   46718:	mov	w3, w22
   4671c:	bl	c2f0 <__gmpn_rshift@plt>
   46720:	b	4674c <__gmpn_sec_div_r@@Base+0xe4>
   46724:	cmn	x8, #0x1
   46728:	cinc	x0, x8, ne  // ne = any
   4672c:	bl	d5d0 <__gmpn_invert_limb@plt>
   46730:	mov	x4, x0
   46734:	mov	x0, x21
   46738:	mov	x1, x23
   4673c:	mov	x2, x24
   46740:	mov	x3, x19
   46744:	mov	x5, x20
   46748:	bl	c9b0 <__gmpn_sec_pi1_div_r@plt>
   4674c:	ldp	x20, x19, [sp, #64]
   46750:	ldp	x22, x21, [sp, #48]
   46754:	ldp	x24, x23, [sp, #32]
   46758:	ldp	x26, x25, [sp, #16]
   4675c:	ldp	x29, x30, [sp], #80
   46760:	ret

0000000000046764 <__gmpn_sec_pi1_div_qr@@Base>:
   46764:	sub	sp, sp, #0xa0
   46768:	stp	x29, x30, [sp, #64]
   4676c:	stp	x28, x27, [sp, #80]
   46770:	stp	x26, x25, [sp, #96]
   46774:	stp	x20, x19, [sp, #144]
   46778:	add	x29, sp, #0x40
   4677c:	mov	x28, x4
   46780:	mov	x19, x3
   46784:	mov	x26, x1
   46788:	subs	x20, x2, x4
   4678c:	stp	x24, x23, [sp, #112]
   46790:	stp	x22, x21, [sp, #128]
   46794:	stur	x6, [x29, #-8]
   46798:	b.ne	467d4 <__gmpn_sec_pi1_div_qr@@Base+0x70>  // b.any
   4679c:	mov	x0, x26
   467a0:	mov	x1, x26
   467a4:	mov	x2, x19
   467a8:	mov	x3, x28
   467ac:	bl	c420 <__gmpn_sub_n@plt>
   467b0:	mov	x1, x26
   467b4:	mov	x2, x26
   467b8:	mov	x3, x19
   467bc:	mov	x4, x28
   467c0:	mov	x21, x0
   467c4:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   467c8:	mov	w8, #0x1                   	// #1
   467cc:	sub	x0, x8, x21
   467d0:	b	469c0 <__gmpn_sec_pi1_div_qr@@Base+0x25c>
   467d4:	ldur	x21, [x29, #-8]
   467d8:	mov	x22, x2
   467dc:	mov	x23, x0
   467e0:	mov	w3, #0x20                  	// #32
   467e4:	mov	x0, x21
   467e8:	mov	x1, x19
   467ec:	mov	x2, x28
   467f0:	mov	x27, x5
   467f4:	bl	c2d0 <__gmpn_lshift@plt>
   467f8:	add	x12, x28, #0x1
   467fc:	add	x8, x21, x22, lsl #3
   46800:	cmp	x20, #0x1
   46804:	add	x24, x21, x12, lsl #3
   46808:	add	x25, x8, #0x8
   4680c:	add	x10, x26, x20, lsl #3
   46810:	str	x0, [x21, x28, lsl #3]
   46814:	b.lt	468e4 <__gmpn_sec_pi1_div_qr@@Base+0x180>  // b.tstop
   46818:	ldur	x11, [x29, #-8]
   4681c:	lsl	x9, x22, #1
   46820:	lsl	x8, x22, #3
   46824:	sub	x9, x9, x28
   46828:	stp	x25, x24, [sp]
   4682c:	stp	x23, x20, [sp, #16]
   46830:	mov	x23, xzr
   46834:	mov	x21, xzr
   46838:	add	x24, x20, #0x1
   4683c:	add	x13, x11, x8
   46840:	add	x9, x11, x9, lsl #3
   46844:	add	x25, x26, x8
   46848:	stp	x13, x19, [x29, #-24]
   4684c:	str	x9, [sp, #32]
   46850:	mov	x26, x28
   46854:	add	x28, x25, x23
   46858:	ldur	x8, [x28, #-8]
   4685c:	add	x9, x10, x23
   46860:	sub	x22, x9, #0x8
   46864:	ldur	x1, [x29, #-8]
   46868:	extr	x8, x21, x8, #32
   4686c:	umulh	x9, x8, x27
   46870:	add	x3, x8, x9
   46874:	ldr	x8, [sp, #32]
   46878:	mov	x0, x22
   4687c:	mov	x2, x12
   46880:	mov	x20, x10
   46884:	str	x3, [x8, x23]
   46888:	mov	x19, x12
   4688c:	bl	cba0 <__gmpn_submul_1@plt>
   46890:	ldur	x21, [x28, #-8]
   46894:	umulh	x8, x21, x27
   46898:	mov	x0, x22
   4689c:	mov	x2, x26
   468a0:	add	x3, x8, x21
   468a4:	ldp	x8, x1, [x29, #-24]
   468a8:	mov	x28, x26
   468ac:	str	x3, [x8, x23]
   468b0:	bl	cba0 <__gmpn_submul_1@plt>
   468b4:	sub	x24, x24, #0x1
   468b8:	mov	x12, x19
   468bc:	mov	x10, x20
   468c0:	sub	x21, x21, x0
   468c4:	cmp	x24, #0x1
   468c8:	sub	x23, x23, #0x8
   468cc:	b.gt	46850 <__gmpn_sec_pi1_div_qr@@Base+0xec>
   468d0:	add	x10, x10, x23
   468d4:	ldur	x19, [x29, #-16]
   468d8:	ldp	x23, x20, [sp, #16]
   468dc:	ldp	x25, x24, [sp]
   468e0:	b	468e8 <__gmpn_sec_pi1_div_qr@@Base+0x184>
   468e4:	mov	x21, xzr
   468e8:	ldr	x8, [x24]
   468ec:	cmp	x21, #0x0
   468f0:	cset	w0, ne  // ne = any
   468f4:	mov	x1, x10
   468f8:	cinc	x8, x8, ne  // ne = any
   468fc:	mov	x2, x10
   46900:	mov	x3, x19
   46904:	mov	x4, x28
   46908:	str	x8, [x24]
   4690c:	mov	x26, x10
   46910:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46914:	mov	x22, x0
   46918:	mov	x0, x26
   4691c:	mov	x1, x26
   46920:	mov	x2, x19
   46924:	mov	x3, x28
   46928:	bl	c420 <__gmpn_sub_n@plt>
   4692c:	ldr	x8, [x24]
   46930:	sub	x9, x22, x21
   46934:	add	x0, x0, x9
   46938:	mov	x1, x26
   4693c:	sub	x8, x8, x0
   46940:	add	x8, x8, #0x1
   46944:	mov	x2, x26
   46948:	mov	x3, x19
   4694c:	mov	x4, x28
   46950:	str	x8, [x24]
   46954:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46958:	mov	x0, x26
   4695c:	mov	x1, x26
   46960:	mov	x2, x19
   46964:	mov	x3, x28
   46968:	bl	c420 <__gmpn_sub_n@plt>
   4696c:	ldr	x8, [x24]
   46970:	mov	x1, x26
   46974:	mov	x2, x26
   46978:	mov	x3, x19
   4697c:	sub	x8, x8, x0
   46980:	add	x8, x8, #0x1
   46984:	mov	x4, x28
   46988:	str	x8, [x24]
   4698c:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46990:	mov	w3, #0x20                  	// #32
   46994:	mov	x0, x25
   46998:	mov	x1, x25
   4699c:	mov	x2, x20
   469a0:	bl	c2d0 <__gmpn_lshift@plt>
   469a4:	mov	x19, x0
   469a8:	mov	x0, x23
   469ac:	mov	x1, x25
   469b0:	mov	x2, x24
   469b4:	mov	x3, x20
   469b8:	bl	cc30 <__gmpn_add_n@plt>
   469bc:	add	x0, x0, x19
   469c0:	ldp	x20, x19, [sp, #144]
   469c4:	ldp	x22, x21, [sp, #128]
   469c8:	ldp	x24, x23, [sp, #112]
   469cc:	ldp	x26, x25, [sp, #96]
   469d0:	ldp	x28, x27, [sp, #80]
   469d4:	ldp	x29, x30, [sp, #64]
   469d8:	add	sp, sp, #0xa0
   469dc:	ret

00000000000469e0 <__gmpn_sec_pi1_div_r@@Base>:
   469e0:	stp	x29, x30, [sp, #-96]!
   469e4:	stp	x28, x27, [sp, #16]
   469e8:	stp	x24, x23, [sp, #48]
   469ec:	stp	x20, x19, [sp, #80]
   469f0:	mov	x19, x3
   469f4:	mov	x20, x2
   469f8:	subs	x28, x1, x3
   469fc:	mov	x24, x0
   46a00:	stp	x26, x25, [sp, #32]
   46a04:	stp	x22, x21, [sp, #64]
   46a08:	mov	x29, sp
   46a0c:	b.ne	46a30 <__gmpn_sec_pi1_div_r@@Base+0x50>  // b.any
   46a10:	mov	x0, x24
   46a14:	mov	x1, x24
   46a18:	mov	x2, x20
   46a1c:	mov	x3, x19
   46a20:	bl	c420 <__gmpn_sub_n@plt>
   46a24:	mov	x1, x24
   46a28:	mov	x2, x24
   46a2c:	b	46b3c <__gmpn_sec_pi1_div_r@@Base+0x15c>
   46a30:	mov	x25, x1
   46a34:	mov	w3, #0x20                  	// #32
   46a38:	mov	x0, x5
   46a3c:	mov	x1, x20
   46a40:	mov	x2, x19
   46a44:	mov	x21, x5
   46a48:	mov	x22, x4
   46a4c:	bl	c2d0 <__gmpn_lshift@plt>
   46a50:	cmp	x28, #0x1
   46a54:	mov	x26, xzr
   46a58:	str	x0, [x21, x19, lsl #3]
   46a5c:	b.lt	46acc <__gmpn_sec_pi1_div_r@@Base+0xec>  // b.tstop
   46a60:	add	x23, x19, #0x1
   46a64:	add	x25, x24, x25, lsl #3
   46a68:	neg	x27, x19, lsl #3
   46a6c:	add	x28, x28, #0x1
   46a70:	add	x8, x25, x27
   46a74:	ldr	x9, [x25, #-8]!
   46a78:	sub	x24, x8, #0x8
   46a7c:	mov	x0, x24
   46a80:	mov	x1, x21
   46a84:	extr	x8, x26, x9, #32
   46a88:	umulh	x9, x8, x22
   46a8c:	add	x3, x8, x9
   46a90:	mov	x2, x23
   46a94:	bl	cba0 <__gmpn_submul_1@plt>
   46a98:	ldr	x26, [x25]
   46a9c:	umulh	x8, x26, x22
   46aa0:	mov	x0, x24
   46aa4:	mov	x1, x20
   46aa8:	add	x3, x8, x26
   46aac:	mov	x2, x19
   46ab0:	bl	cba0 <__gmpn_submul_1@plt>
   46ab4:	sub	x28, x28, #0x1
   46ab8:	cmp	x28, #0x1
   46abc:	sub	x26, x26, x0
   46ac0:	b.gt	46a70 <__gmpn_sec_pi1_div_r@@Base+0x90>
   46ac4:	sub	x21, x25, x19, lsl #3
   46ac8:	b	46ad0 <__gmpn_sec_pi1_div_r@@Base+0xf0>
   46acc:	add	x21, x24, x28, lsl #3
   46ad0:	cmp	x26, #0x0
   46ad4:	cset	w0, ne  // ne = any
   46ad8:	mov	x1, x21
   46adc:	mov	x2, x21
   46ae0:	mov	x3, x20
   46ae4:	mov	x4, x19
   46ae8:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46aec:	mov	x22, x0
   46af0:	mov	x0, x21
   46af4:	mov	x1, x21
   46af8:	mov	x2, x20
   46afc:	mov	x3, x19
   46b00:	bl	c420 <__gmpn_sub_n@plt>
   46b04:	sub	x8, x22, x26
   46b08:	add	x0, x8, x0
   46b0c:	mov	x1, x21
   46b10:	mov	x2, x21
   46b14:	mov	x3, x20
   46b18:	mov	x4, x19
   46b1c:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46b20:	mov	x0, x21
   46b24:	mov	x1, x21
   46b28:	mov	x2, x20
   46b2c:	mov	x3, x19
   46b30:	bl	c420 <__gmpn_sub_n@plt>
   46b34:	mov	x1, x21
   46b38:	mov	x2, x21
   46b3c:	mov	x3, x20
   46b40:	mov	x4, x19
   46b44:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46b48:	ldp	x20, x19, [sp, #80]
   46b4c:	ldp	x22, x21, [sp, #64]
   46b50:	ldp	x24, x23, [sp, #48]
   46b54:	ldp	x26, x25, [sp, #32]
   46b58:	ldp	x28, x27, [sp, #16]
   46b5c:	ldp	x29, x30, [sp], #96
   46b60:	ret

0000000000046b64 <__gmpn_sec_add_1_itch@@Base>:
   46b64:	ret

0000000000046b68 <__gmpn_sec_add_1@@Base>:
   46b68:	stp	x29, x30, [sp, #-48]!
   46b6c:	stp	x22, x21, [sp, #16]
   46b70:	stp	x20, x19, [sp, #32]
   46b74:	mov	x19, x4
   46b78:	mov	x20, x2
   46b7c:	mov	x21, x1
   46b80:	mov	x22, x0
   46b84:	cmp	x2, #0x1
   46b88:	mov	x29, sp
   46b8c:	str	x3, [x4]
   46b90:	b.eq	46ba8 <__gmpn_sec_add_1@@Base+0x40>  // b.none
   46b94:	lsl	x8, x20, #3
   46b98:	add	x0, x19, #0x8
   46b9c:	sub	x2, x8, #0x8
   46ba0:	mov	w1, wzr
   46ba4:	bl	c780 <memset@plt>
   46ba8:	mov	x0, x22
   46bac:	mov	x1, x21
   46bb0:	mov	x2, x19
   46bb4:	mov	x3, x20
   46bb8:	bl	cc30 <__gmpn_add_n@plt>
   46bbc:	ldp	x20, x19, [sp, #32]
   46bc0:	ldp	x22, x21, [sp, #16]
   46bc4:	ldp	x29, x30, [sp], #48
   46bc8:	ret

0000000000046bcc <__gmpn_sec_sub_1_itch@@Base>:
   46bcc:	ret

0000000000046bd0 <__gmpn_sec_sub_1@@Base>:
   46bd0:	stp	x29, x30, [sp, #-48]!
   46bd4:	stp	x22, x21, [sp, #16]
   46bd8:	stp	x20, x19, [sp, #32]
   46bdc:	mov	x19, x4
   46be0:	mov	x20, x2
   46be4:	mov	x21, x1
   46be8:	mov	x22, x0
   46bec:	cmp	x2, #0x1
   46bf0:	mov	x29, sp
   46bf4:	str	x3, [x4]
   46bf8:	b.eq	46c10 <__gmpn_sec_sub_1@@Base+0x40>  // b.none
   46bfc:	lsl	x8, x20, #3
   46c00:	add	x0, x19, #0x8
   46c04:	sub	x2, x8, #0x8
   46c08:	mov	w1, wzr
   46c0c:	bl	c780 <memset@plt>
   46c10:	mov	x0, x22
   46c14:	mov	x1, x21
   46c18:	mov	x2, x19
   46c1c:	mov	x3, x20
   46c20:	bl	c420 <__gmpn_sub_n@plt>
   46c24:	ldp	x20, x19, [sp, #32]
   46c28:	ldp	x22, x21, [sp, #16]
   46c2c:	ldp	x29, x30, [sp], #48
   46c30:	ret

0000000000046c34 <__gmpn_sec_invert_itch@@Base>:
   46c34:	lsl	x0, x0, #2
   46c38:	ret

0000000000046c3c <__gmpn_sec_invert@@Base>:
   46c3c:	sub	sp, sp, #0x70
   46c40:	stp	x26, x25, [sp, #48]
   46c44:	add	x25, x5, x3, lsl #4
   46c48:	stp	x24, x23, [sp, #64]
   46c4c:	mov	x24, x0
   46c50:	mov	w8, #0x1                   	// #1
   46c54:	mov	x0, x25
   46c58:	stp	x29, x30, [sp, #16]
   46c5c:	stp	x28, x27, [sp, #32]
   46c60:	stp	x22, x21, [sp, #80]
   46c64:	stp	x20, x19, [sp, #96]
   46c68:	mov	x23, x1
   46c6c:	str	x8, [x0], #8
   46c70:	sub	x1, x3, #0x1
   46c74:	add	x29, sp, #0x10
   46c78:	mov	x20, x5
   46c7c:	mov	x21, x4
   46c80:	mov	x19, x3
   46c84:	mov	x22, x2
   46c88:	bl	d100 <__gmpn_zero@plt>
   46c8c:	add	x26, x20, x19, lsl #3
   46c90:	mov	x0, x26
   46c94:	mov	x1, x22
   46c98:	mov	x2, x19
   46c9c:	bl	cc10 <__gmpn_copyi@plt>
   46ca0:	mov	x0, x24
   46ca4:	mov	x1, x19
   46ca8:	bl	d100 <__gmpn_zero@plt>
   46cac:	mov	w8, #0x18                  	// #24
   46cb0:	madd	x27, x19, x8, x20
   46cb4:	mov	w3, #0x1                   	// #1
   46cb8:	mov	x0, x27
   46cbc:	mov	x1, x22
   46cc0:	mov	x2, x19
   46cc4:	bl	c2f0 <__gmpn_rshift@plt>
   46cc8:	mov	w3, #0x1                   	// #1
   46ccc:	mov	x0, x27
   46cd0:	mov	x1, x27
   46cd4:	mov	x2, x19
   46cd8:	mov	x4, x20
   46cdc:	str	x20, [sp, #8]
   46ce0:	bl	c4e0 <__gmpn_sec_add_1@plt>
   46ce4:	cbz	x21, 46dc4 <__gmpn_sec_invert@@Base+0x188>
   46ce8:	ldr	x8, [x23]
   46cec:	mov	x1, x23
   46cf0:	mov	x2, x23
   46cf4:	mov	x3, x26
   46cf8:	and	x28, x8, #0x1
   46cfc:	mov	x0, x28
   46d00:	mov	x4, x19
   46d04:	sub	x21, x21, #0x1
   46d08:	mov	x20, x22
   46d0c:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46d10:	mov	x1, x26
   46d14:	mov	x2, x26
   46d18:	mov	x3, x23
   46d1c:	mov	x4, x19
   46d20:	mov	x22, x0
   46d24:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46d28:	ldr	x4, [sp, #8]
   46d2c:	mov	w0, w22
   46d30:	mov	x1, x23
   46d34:	mov	x2, x23
   46d38:	mov	x3, x19
   46d3c:	bl	46df0 <__gmpn_sec_invert@@Base+0x1b4>
   46d40:	mov	x0, x22
   46d44:	mov	x1, x25
   46d48:	mov	x2, x24
   46d4c:	mov	x3, x19
   46d50:	bl	c7f0 <__gmpn_cnd_swap@plt>
   46d54:	mov	x0, x28
   46d58:	mov	x1, x25
   46d5c:	mov	x2, x25
   46d60:	mov	x3, x24
   46d64:	mov	x4, x19
   46d68:	mov	x22, x20
   46d6c:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46d70:	mov	x1, x25
   46d74:	mov	x2, x25
   46d78:	mov	x3, x20
   46d7c:	mov	x4, x19
   46d80:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46d84:	mov	w3, #0x1                   	// #1
   46d88:	mov	x0, x23
   46d8c:	mov	x1, x23
   46d90:	mov	x2, x19
   46d94:	bl	c2f0 <__gmpn_rshift@plt>
   46d98:	mov	w3, #0x1                   	// #1
   46d9c:	mov	x0, x25
   46da0:	mov	x1, x25
   46da4:	mov	x2, x19
   46da8:	bl	c2f0 <__gmpn_rshift@plt>
   46dac:	mov	x1, x25
   46db0:	mov	x2, x25
   46db4:	mov	x3, x27
   46db8:	mov	x4, x19
   46dbc:	bl	d6b0 <__gmpn_cnd_add_n@plt>
   46dc0:	cbnz	x21, 46ce8 <__gmpn_sec_invert@@Base+0xac>
   46dc4:	mov	x0, x26
   46dc8:	mov	x1, x19
   46dcc:	bl	46e58 <__gmpn_sec_invert@@Base+0x21c>
   46dd0:	ldp	x20, x19, [sp, #96]
   46dd4:	ldp	x22, x21, [sp, #80]
   46dd8:	ldp	x24, x23, [sp, #64]
   46ddc:	ldp	x26, x25, [sp, #48]
   46de0:	ldp	x28, x27, [sp, #32]
   46de4:	ldp	x29, x30, [sp, #16]
   46de8:	add	sp, sp, #0x70
   46dec:	ret
   46df0:	stp	x29, x30, [sp, #-64]!
   46df4:	stp	x20, x19, [sp, #48]
   46df8:	mov	x20, x3
   46dfc:	str	x23, [sp, #16]
   46e00:	stp	x22, x21, [sp, #32]
   46e04:	mov	x21, x2
   46e08:	mov	x22, x1
   46e0c:	mov	w23, w0
   46e10:	mov	w3, #0x1                   	// #1
   46e14:	mov	x0, x4
   46e18:	mov	x1, x2
   46e1c:	mov	x2, x20
   46e20:	mov	x29, sp
   46e24:	mov	x19, x4
   46e28:	bl	c2d0 <__gmpn_lshift@plt>
   46e2c:	sxtw	x0, w23
   46e30:	mov	x1, x22
   46e34:	mov	x2, x21
   46e38:	mov	x3, x19
   46e3c:	mov	x4, x20
   46e40:	bl	c160 <__gmpn_cnd_sub_n@plt>
   46e44:	ldp	x20, x19, [sp, #48]
   46e48:	ldp	x22, x21, [sp, #32]
   46e4c:	ldr	x23, [sp, #16]
   46e50:	ldp	x29, x30, [sp], #64
   46e54:	ret
   46e58:	ldr	x8, [x0]
   46e5c:	cmp	x1, #0x2
   46e60:	eor	x8, x8, #0x1
   46e64:	b.lt	46e80 <__gmpn_sec_invert@@Base+0x244>  // b.tstop
   46e68:	sub	x9, x0, #0x8
   46e6c:	ldr	x10, [x9, x1, lsl #3]
   46e70:	cmp	x1, #0x2
   46e74:	sub	x1, x1, #0x1
   46e78:	orr	x8, x10, x8
   46e7c:	b.gt	46e6c <__gmpn_sec_invert@@Base+0x230>
   46e80:	cmp	x8, #0x0
   46e84:	cset	w0, eq  // eq = none
   46e88:	ret

0000000000046e8c <__gmpn_trialdiv@@Base>:
   46e8c:	stp	x29, x30, [sp, #-96]!
   46e90:	stp	x26, x25, [sp, #32]
   46e94:	stp	x24, x23, [sp, #48]
   46e98:	stp	x22, x21, [sp, #64]
   46e9c:	stp	x20, x19, [sp, #80]
   46ea0:	ldrsw	x23, [x3]
   46ea4:	str	x27, [sp, #16]
   46ea8:	mov	x29, sp
   46eac:	cmp	w23, #0xc6
   46eb0:	b.hi	46f4c <__gmpn_trialdiv@@Base+0xc0>  // b.pmore
   46eb4:	adrp	x25, 54000 <__gmp_jacobi_table@@Base+0x3ef1>
   46eb8:	adrp	x26, 50000 <__gmpn_bases@@Base+0x2f98>
   46ebc:	mov	x19, x3
   46ec0:	mov	x20, x2
   46ec4:	mov	x21, x1
   46ec8:	mov	x22, x0
   46ecc:	mov	w24, #0x48                  	// #72
   46ed0:	add	x25, x25, #0x200
   46ed4:	add	x26, x26, #0x350
   46ed8:	madd	x27, x23, x24, x25
   46edc:	ldr	x8, [x27]
   46ee0:	ldr	x9, [x27, #16]
   46ee4:	add	x3, x27, #0x8
   46ee8:	mov	x0, x22
   46eec:	mov	x1, x21
   46ef0:	lsl	x2, x8, x9
   46ef4:	bl	d5f0 <__gmpn_mod_1s_4p@plt>
   46ef8:	ldr	w8, [x27, #64]
   46efc:	lsr	x9, x8, #24
   46f00:	cbz	w9, 46f34 <__gmpn_trialdiv@@Base+0xa8>
   46f04:	and	x8, x8, #0xffffff
   46f08:	add	x8, x26, x8, lsl #4
   46f0c:	mvn	x10, x9
   46f10:	add	x11, x8, #0x8
   46f14:	ldp	x8, x12, [x11, #-8]
   46f18:	mul	x13, x8, x0
   46f1c:	cmp	x13, x12
   46f20:	b.ls	46f54 <__gmpn_trialdiv@@Base+0xc8>  // b.plast
   46f24:	add	x10, x10, #0x1
   46f28:	cmn	x10, #0x2
   46f2c:	add	x11, x11, #0x10
   46f30:	b.le	46f14 <__gmpn_trialdiv@@Base+0x88>
   46f34:	sub	x20, x20, x9
   46f38:	cmp	x20, #0x1
   46f3c:	b.lt	46f4c <__gmpn_trialdiv@@Base+0xc0>  // b.tstop
   46f40:	add	x23, x23, #0x1
   46f44:	cmp	x23, #0xc7
   46f48:	b.cc	46ed8 <__gmpn_trialdiv@@Base+0x4c>  // b.lo, b.ul, b.last
   46f4c:	mov	x8, xzr
   46f50:	b	46f58 <__gmpn_trialdiv@@Base+0xcc>
   46f54:	str	w23, [x19]
   46f58:	ldp	x20, x19, [sp, #80]
   46f5c:	ldp	x22, x21, [sp, #64]
   46f60:	ldp	x24, x23, [sp, #48]
   46f64:	ldp	x26, x25, [sp, #32]
   46f68:	ldr	x27, [sp, #16]
   46f6c:	mov	x0, x8
   46f70:	ldp	x29, x30, [sp], #96
   46f74:	ret

0000000000046f78 <__gmpn_remove@@Base>:
   46f78:	stp	x29, x30, [sp, #-96]!
   46f7c:	stp	x28, x27, [sp, #16]
   46f80:	stp	x26, x25, [sp, #32]
   46f84:	stp	x24, x23, [sp, #48]
   46f88:	stp	x22, x21, [sp, #64]
   46f8c:	stp	x20, x19, [sp, #80]
   46f90:	mov	x29, sp
   46f94:	sub	sp, sp, #0x360
   46f98:	add	x20, x3, #0x1
   46f9c:	add	x8, x20, x5
   46fa0:	cmp	x8, #0x0
   46fa4:	cinc	x8, x8, lt  // lt = tstop
   46fa8:	lsr	x8, x8, #1
   46fac:	add	x8, x8, x20, lsl #1
   46fb0:	mov	x24, x1
   46fb4:	lsl	x1, x8, #3
   46fb8:	mov	w8, #0x7f00                	// #32512
   46fbc:	mov	x19, sp
   46fc0:	mov	x26, x5
   46fc4:	mov	x21, x4
   46fc8:	mov	x23, x3
   46fcc:	mov	x27, x2
   46fd0:	cmp	x1, x8
   46fd4:	str	x6, [x19, #32]
   46fd8:	str	x0, [x19]
   46fdc:	str	xzr, [x19, #48]
   46fe0:	b.hi	472ac <__gmpn_remove@@Base+0x334>  // b.pmore
   46fe4:	add	x9, x1, #0xf
   46fe8:	mov	x8, sp
   46fec:	and	x9, x9, #0xfffffffffffffff0
   46ff0:	sub	x25, x8, x9
   46ff4:	mov	sp, x25
   46ff8:	lsl	x8, x20, #3
   46ffc:	add	x28, x25, x8
   47000:	add	x8, x28, x8
   47004:	mov	x0, x25
   47008:	mov	x1, x27
   4700c:	mov	x2, x23
   47010:	str	x8, [x19, #40]
   47014:	bl	cc10 <__gmpn_copyi@plt>
   47018:	cmp	x23, x26
   4701c:	mov	x22, xzr
   47020:	str	x24, [x19, #8]
   47024:	b.lt	4715c <__gmpn_remove@@Base+0x1e4>  // b.tstop
   47028:	mov	x8, x25
   4702c:	b	47064 <__gmpn_remove@@Base+0xec>
   47030:	add	x27, x27, x26, lsl #3
   47034:	mov	x0, x27
   47038:	mov	x1, x21
   4703c:	mov	x2, x26
   47040:	bl	ca90 <__gmpn_sqr@plt>
   47044:	ldr	x8, [x27, x24, lsl #3]
   47048:	mov	x22, x20
   4704c:	mov	x21, x27
   47050:	cmp	x8, #0x0
   47054:	cinc	x26, x24, ne  // ne = any
   47058:	cmp	x23, x26
   4705c:	mov	x8, x25
   47060:	b.lt	47154 <__gmpn_remove@@Base+0x1dc>  // b.tstop
   47064:	ldr	x20, [x19, #40]
   47068:	mov	x25, x28
   4706c:	add	x3, x23, #0x1
   47070:	mov	x0, x25
   47074:	mov	x1, x20
   47078:	mov	x2, x8
   4707c:	mov	x4, x21
   47080:	mov	x5, x26
   47084:	mov	x28, x8
   47088:	str	xzr, [x8, x23, lsl #3]
   4708c:	bl	472c4 <__gmpn_remove@@Base+0x34c>
   47090:	mov	x0, x20
   47094:	mov	x1, x26
   47098:	bl	c010 <__gmpn_zero_p@plt>
   4709c:	cbnz	w0, 470b4 <__gmpn_remove@@Base+0x13c>
   470a0:	ldr	x0, [x19, #40]
   470a4:	mov	x1, x21
   470a8:	mov	x2, x26
   470ac:	bl	c570 <__gmpn_cmp@plt>
   470b0:	cbnz	w0, 4729c <__gmpn_remove@@Base+0x324>
   470b4:	sub	x20, x23, x26
   470b8:	add	x2, x20, #0x1
   470bc:	mov	x0, x25
   470c0:	mov	x1, x25
   470c4:	bl	ce90 <__gmpn_neg@plt>
   470c8:	ldr	x8, [x25, x20, lsl #3]
   470cc:	lsl	x9, x22, #3
   470d0:	mov	w10, #0x4                   	// #4
   470d4:	add	x11, x19, #0x1c8
   470d8:	cmp	x8, #0x0
   470dc:	ldr	x8, [x19, #32]
   470e0:	lsl	x10, x10, x22
   470e4:	str	x21, [x11, x9]
   470e8:	add	x11, x19, #0x38
   470ec:	str	x26, [x11, x9]
   470f0:	sub	x9, x10, #0x1
   470f4:	cinc	x23, x20, ne  // ne = any
   470f8:	cmp	x9, x8
   470fc:	add	x20, x22, #0x1
   47100:	b.hi	47154 <__gmpn_remove@@Base+0x1dc>  // b.pmore
   47104:	lsl	x8, x26, #1
   47108:	sub	x24, x8, #0x1
   4710c:	cmp	x24, x23
   47110:	b.gt	47154 <__gmpn_remove@@Base+0x1dc>
   47114:	cbnz	x22, 47030 <__gmpn_remove@@Base+0xb8>
   47118:	lsl	x8, x23, #3
   4711c:	add	x1, x8, #0x190
   47120:	mov	w8, #0x7f00                	// #32512
   47124:	cmp	x1, x8
   47128:	b.hi	47144 <__gmpn_remove@@Base+0x1cc>  // b.pmore
   4712c:	add	x9, x1, #0xf
   47130:	mov	x8, sp
   47134:	and	x9, x9, #0xfffffffffffffff0
   47138:	sub	x27, x8, x9
   4713c:	mov	sp, x27
   47140:	b	47034 <__gmpn_remove@@Base+0xbc>
   47144:	add	x0, x19, #0x30
   47148:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4714c:	mov	x27, x0
   47150:	b	47034 <__gmpn_remove@@Base+0xbc>
   47154:	mov	x22, x20
   47158:	ldr	x24, [x19, #8]
   4715c:	mov	x8, #0xffffffffffffffff    	// #-1
   47160:	lsl	x8, x8, x22
   47164:	cmp	x22, #0x1
   47168:	mvn	x21, x8
   4716c:	b.lt	47254 <__gmpn_remove@@Base+0x2dc>  // b.tstop
   47170:	stp	x28, x25, [x19, #16]
   47174:	add	x25, x23, #0x1
   47178:	sub	x22, x22, #0x1
   4717c:	b	47190 <__gmpn_remove@@Base+0x218>
   47180:	add	x8, x22, #0x1
   47184:	sub	x22, x22, #0x1
   47188:	cmp	x8, #0x1
   4718c:	b.le	47244 <__gmpn_remove@@Base+0x2cc>
   47190:	add	x8, x19, #0x38
   47194:	ldr	x27, [x8, x22, lsl #3]
   47198:	subs	x20, x23, x27
   4719c:	b.lt	47180 <__gmpn_remove@@Base+0x208>  // b.tstop
   471a0:	mov	w8, #0x1                   	// #1
   471a4:	lsl	x8, x8, x22
   471a8:	add	x26, x8, x21
   471ac:	ldr	x8, [x19, #32]
   471b0:	cmp	x26, x8
   471b4:	b.hi	47180 <__gmpn_remove@@Base+0x208>  // b.pmore
   471b8:	add	x8, x19, #0x1c8
   471bc:	ldr	x28, [x8, x22, lsl #3]
   471c0:	ldr	x24, [x19, #40]
   471c4:	ldp	x0, x2, [x19, #16]
   471c8:	mov	x3, x25
   471cc:	mov	x4, x28
   471d0:	mov	x1, x24
   471d4:	mov	x5, x27
   471d8:	str	xzr, [x2, x23, lsl #3]
   471dc:	bl	472c4 <__gmpn_remove@@Base+0x34c>
   471e0:	mov	x0, x24
   471e4:	mov	x1, x27
   471e8:	bl	c010 <__gmpn_zero_p@plt>
   471ec:	cbnz	w0, 47204 <__gmpn_remove@@Base+0x28c>
   471f0:	ldr	x0, [x19, #40]
   471f4:	mov	x1, x28
   471f8:	mov	x2, x27
   471fc:	bl	c570 <__gmpn_cmp@plt>
   47200:	cbnz	w0, 47180 <__gmpn_remove@@Base+0x208>
   47204:	ldr	x27, [x19, #16]
   47208:	add	x2, x20, #0x1
   4720c:	mov	x0, x27
   47210:	mov	x1, x27
   47214:	bl	ce90 <__gmpn_neg@plt>
   47218:	ldr	x8, [x27, x20, lsl #3]
   4721c:	ldr	x24, [x19, #8]
   47220:	ldr	x28, [x19, #24]
   47224:	add	x9, x22, #0x1
   47228:	cmp	x8, #0x0
   4722c:	cinc	x23, x20, ne  // ne = any
   47230:	cmp	x9, #0x1
   47234:	mov	x25, x27
   47238:	mov	x21, x26
   4723c:	b.gt	47170 <__gmpn_remove@@Base+0x1f8>
   47240:	b	4725c <__gmpn_remove@@Base+0x2e4>
   47244:	ldr	x27, [x19, #24]
   47248:	ldr	x24, [x19, #8]
   4724c:	mov	x26, x21
   47250:	b	4725c <__gmpn_remove@@Base+0x2e4>
   47254:	mov	x26, x21
   47258:	mov	x27, x25
   4725c:	ldr	x0, [x19]
   47260:	mov	x1, x27
   47264:	mov	x2, x23
   47268:	bl	cc10 <__gmpn_copyi@plt>
   4726c:	str	x23, [x24]
   47270:	ldr	x0, [x19, #48]
   47274:	cbnz	x0, 472bc <__gmpn_remove@@Base+0x344>
   47278:	mov	x0, x26
   4727c:	mov	sp, x29
   47280:	ldp	x20, x19, [sp, #80]
   47284:	ldp	x22, x21, [sp, #64]
   47288:	ldp	x24, x23, [sp, #48]
   4728c:	ldp	x26, x25, [sp, #32]
   47290:	ldp	x28, x27, [sp, #16]
   47294:	ldp	x29, x30, [sp], #96
   47298:	ret
   4729c:	mov	x8, x25
   472a0:	mov	x25, x28
   472a4:	mov	x28, x8
   472a8:	b	47158 <__gmpn_remove@@Base+0x1e0>
   472ac:	add	x0, x19, #0x30
   472b0:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   472b4:	mov	x25, x0
   472b8:	b	46ff8 <__gmpn_remove@@Base+0x80>
   472bc:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   472c0:	b	47278 <__gmpn_remove@@Base+0x300>
   472c4:	stp	x29, x30, [sp, #-64]!
   472c8:	stp	x24, x23, [sp, #16]
   472cc:	stp	x22, x21, [sp, #32]
   472d0:	stp	x20, x19, [sp, #48]
   472d4:	mov	x29, sp
   472d8:	sub	sp, sp, #0x10
   472dc:	mov	x23, x1
   472e0:	mov	x24, x0
   472e4:	mov	x0, x3
   472e8:	mov	x1, x5
   472ec:	mov	x19, x5
   472f0:	mov	x20, x4
   472f4:	mov	x21, x3
   472f8:	mov	x22, x2
   472fc:	stur	xzr, [x29, #-8]
   47300:	bl	ca70 <__gmpn_bdiv_qr_itch@plt>
   47304:	lsl	x1, x0, #3
   47308:	mov	w8, #0x7f00                	// #32512
   4730c:	cmp	x1, x8
   47310:	b.hi	47364 <__gmpn_remove@@Base+0x3ec>  // b.pmore
   47314:	add	x9, x1, #0xf
   47318:	mov	x8, sp
   4731c:	and	x9, x9, #0xfffffffffffffff0
   47320:	sub	x6, x8, x9
   47324:	mov	sp, x6
   47328:	mov	x0, x24
   4732c:	mov	x1, x23
   47330:	mov	x2, x22
   47334:	mov	x3, x21
   47338:	mov	x4, x20
   4733c:	mov	x5, x19
   47340:	bl	d120 <__gmpn_bdiv_qr@plt>
   47344:	ldur	x0, [x29, #-8]
   47348:	cbnz	x0, 47374 <__gmpn_remove@@Base+0x3fc>
   4734c:	mov	sp, x29
   47350:	ldp	x20, x19, [sp, #48]
   47354:	ldp	x22, x21, [sp, #32]
   47358:	ldp	x24, x23, [sp, #16]
   4735c:	ldp	x29, x30, [sp], #64
   47360:	ret
   47364:	sub	x0, x29, #0x8
   47368:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4736c:	mov	x6, x0
   47370:	b	47328 <__gmpn_remove@@Base+0x3b0>
   47374:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   47378:	b	4734c <__gmpn_remove@@Base+0x3d4>
   4737c:	nop

0000000000047380 <__gmpn_and_n@@Base>:
   47380:	lsr	x18, x3, #2
   47384:	tbz	w3, #0, 473cc <__gmpn_and_n@@Base+0x4c>
   47388:	ldr	x7, [x1]
   4738c:	ldr	x11, [x2]
   47390:	and	x15, x7, x11
   47394:	str	x15, [x0], #8
   47398:	tbnz	w3, #1, 473b4 <__gmpn_and_n@@Base+0x34>
   4739c:	cbz	x18, 4742c <__gmpn_and_n@@Base+0xac>
   473a0:	ldp	x4, x5, [x1, #8]
   473a4:	ldp	x8, x9, [x2, #8]
   473a8:	sub	x1, x1, #0x8
   473ac:	sub	x2, x2, #0x8
   473b0:	b	47404 <__gmpn_and_n@@Base+0x84>
   473b4:	ldp	x6, x7, [x1, #8]
   473b8:	ldp	x10, x11, [x2, #8]
   473bc:	add	x1, x1, #0x8
   473c0:	add	x2, x2, #0x8
   473c4:	cbz	x18, 47420 <__gmpn_and_n@@Base+0xa0>
   473c8:	b	473f0 <__gmpn_and_n@@Base+0x70>
   473cc:	tbnz	w3, #1, 473dc <__gmpn_and_n@@Base+0x5c>
   473d0:	ldp	x4, x5, [x1], #-16
   473d4:	ldp	x8, x9, [x2], #-16
   473d8:	b	47404 <__gmpn_and_n@@Base+0x84>
   473dc:	ldp	x6, x7, [x1]
   473e0:	ldp	x10, x11, [x2]
   473e4:	cbz	x18, 47420 <__gmpn_and_n@@Base+0xa0>
   473e8:	nop
   473ec:	nop
   473f0:	ldp	x4, x5, [x1, #16]
   473f4:	ldp	x8, x9, [x2, #16]
   473f8:	and	x12, x6, x10
   473fc:	and	x13, x7, x11
   47400:	stp	x12, x13, [x0], #16
   47404:	ldp	x6, x7, [x1, #32]!
   47408:	ldp	x10, x11, [x2, #32]!
   4740c:	and	x12, x4, x8
   47410:	and	x13, x5, x9
   47414:	stp	x12, x13, [x0], #16
   47418:	sub	x18, x18, #0x1
   4741c:	cbnz	x18, 473f0 <__gmpn_and_n@@Base+0x70>
   47420:	and	x12, x6, x10
   47424:	and	x13, x7, x11
   47428:	stp	x12, x13, [x0]
   4742c:	ret

0000000000047430 <__gmpn_andn_n@@Base>:
   47430:	lsr	x18, x3, #2
   47434:	tbz	w3, #0, 4747c <__gmpn_andn_n@@Base+0x4c>
   47438:	ldr	x7, [x1]
   4743c:	ldr	x11, [x2]
   47440:	bic	x15, x7, x11
   47444:	str	x15, [x0], #8
   47448:	tbnz	w3, #1, 47464 <__gmpn_andn_n@@Base+0x34>
   4744c:	cbz	x18, 474dc <__gmpn_andn_n@@Base+0xac>
   47450:	ldp	x4, x5, [x1, #8]
   47454:	ldp	x8, x9, [x2, #8]
   47458:	sub	x1, x1, #0x8
   4745c:	sub	x2, x2, #0x8
   47460:	b	474b4 <__gmpn_andn_n@@Base+0x84>
   47464:	ldp	x6, x7, [x1, #8]
   47468:	ldp	x10, x11, [x2, #8]
   4746c:	add	x1, x1, #0x8
   47470:	add	x2, x2, #0x8
   47474:	cbz	x18, 474d0 <__gmpn_andn_n@@Base+0xa0>
   47478:	b	474a0 <__gmpn_andn_n@@Base+0x70>
   4747c:	tbnz	w3, #1, 4748c <__gmpn_andn_n@@Base+0x5c>
   47480:	ldp	x4, x5, [x1], #-16
   47484:	ldp	x8, x9, [x2], #-16
   47488:	b	474b4 <__gmpn_andn_n@@Base+0x84>
   4748c:	ldp	x6, x7, [x1]
   47490:	ldp	x10, x11, [x2]
   47494:	cbz	x18, 474d0 <__gmpn_andn_n@@Base+0xa0>
   47498:	nop
   4749c:	nop
   474a0:	ldp	x4, x5, [x1, #16]
   474a4:	ldp	x8, x9, [x2, #16]
   474a8:	bic	x12, x6, x10
   474ac:	bic	x13, x7, x11
   474b0:	stp	x12, x13, [x0], #16
   474b4:	ldp	x6, x7, [x1, #32]!
   474b8:	ldp	x10, x11, [x2, #32]!
   474bc:	bic	x12, x4, x8
   474c0:	bic	x13, x5, x9
   474c4:	stp	x12, x13, [x0], #16
   474c8:	sub	x18, x18, #0x1
   474cc:	cbnz	x18, 474a0 <__gmpn_andn_n@@Base+0x70>
   474d0:	bic	x12, x6, x10
   474d4:	bic	x13, x7, x11
   474d8:	stp	x12, x13, [x0]
   474dc:	ret

00000000000474e0 <__gmpn_nand_n@@Base>:
   474e0:	lsr	x18, x3, #2
   474e4:	tbz	w3, #0, 47530 <__gmpn_nand_n@@Base+0x50>
   474e8:	ldr	x7, [x1]
   474ec:	ldr	x11, [x2]
   474f0:	and	x15, x7, x11
   474f4:	mvn	x15, x15
   474f8:	str	x15, [x0], #8
   474fc:	tbnz	w3, #1, 47518 <__gmpn_nand_n@@Base+0x38>
   47500:	cbz	x18, 475a4 <__gmpn_nand_n@@Base+0xc4>
   47504:	ldp	x4, x5, [x1, #8]
   47508:	ldp	x8, x9, [x2, #8]
   4750c:	sub	x1, x1, #0x8
   47510:	sub	x2, x2, #0x8
   47514:	b	4756c <__gmpn_nand_n@@Base+0x8c>
   47518:	ldp	x6, x7, [x1, #8]
   4751c:	ldp	x10, x11, [x2, #8]
   47520:	add	x1, x1, #0x8
   47524:	add	x2, x2, #0x8
   47528:	cbz	x18, 47590 <__gmpn_nand_n@@Base+0xb0>
   4752c:	b	47550 <__gmpn_nand_n@@Base+0x70>
   47530:	tbnz	w3, #1, 47540 <__gmpn_nand_n@@Base+0x60>
   47534:	ldp	x4, x5, [x1], #-16
   47538:	ldp	x8, x9, [x2], #-16
   4753c:	b	4756c <__gmpn_nand_n@@Base+0x8c>
   47540:	ldp	x6, x7, [x1]
   47544:	ldp	x10, x11, [x2]
   47548:	cbz	x18, 47590 <__gmpn_nand_n@@Base+0xb0>
   4754c:	nop
   47550:	ldp	x4, x5, [x1, #16]
   47554:	ldp	x8, x9, [x2, #16]
   47558:	and	x12, x6, x10
   4755c:	and	x13, x7, x11
   47560:	mvn	x12, x12
   47564:	mvn	x13, x13
   47568:	stp	x12, x13, [x0], #16
   4756c:	ldp	x6, x7, [x1, #32]!
   47570:	ldp	x10, x11, [x2, #32]!
   47574:	and	x12, x4, x8
   47578:	and	x13, x5, x9
   4757c:	mvn	x12, x12
   47580:	mvn	x13, x13
   47584:	stp	x12, x13, [x0], #16
   47588:	sub	x18, x18, #0x1
   4758c:	cbnz	x18, 47550 <__gmpn_nand_n@@Base+0x70>
   47590:	and	x12, x6, x10
   47594:	and	x13, x7, x11
   47598:	mvn	x12, x12
   4759c:	mvn	x13, x13
   475a0:	stp	x12, x13, [x0]
   475a4:	ret
   475a8:	nop
   475ac:	nop

00000000000475b0 <__gmpn_ior_n@@Base>:
   475b0:	lsr	x18, x3, #2
   475b4:	tbz	w3, #0, 475fc <__gmpn_ior_n@@Base+0x4c>
   475b8:	ldr	x7, [x1]
   475bc:	ldr	x11, [x2]
   475c0:	orr	x15, x7, x11
   475c4:	str	x15, [x0], #8
   475c8:	tbnz	w3, #1, 475e4 <__gmpn_ior_n@@Base+0x34>
   475cc:	cbz	x18, 4765c <__gmpn_ior_n@@Base+0xac>
   475d0:	ldp	x4, x5, [x1, #8]
   475d4:	ldp	x8, x9, [x2, #8]
   475d8:	sub	x1, x1, #0x8
   475dc:	sub	x2, x2, #0x8
   475e0:	b	47634 <__gmpn_ior_n@@Base+0x84>
   475e4:	ldp	x6, x7, [x1, #8]
   475e8:	ldp	x10, x11, [x2, #8]
   475ec:	add	x1, x1, #0x8
   475f0:	add	x2, x2, #0x8
   475f4:	cbz	x18, 47650 <__gmpn_ior_n@@Base+0xa0>
   475f8:	b	47620 <__gmpn_ior_n@@Base+0x70>
   475fc:	tbnz	w3, #1, 4760c <__gmpn_ior_n@@Base+0x5c>
   47600:	ldp	x4, x5, [x1], #-16
   47604:	ldp	x8, x9, [x2], #-16
   47608:	b	47634 <__gmpn_ior_n@@Base+0x84>
   4760c:	ldp	x6, x7, [x1]
   47610:	ldp	x10, x11, [x2]
   47614:	cbz	x18, 47650 <__gmpn_ior_n@@Base+0xa0>
   47618:	nop
   4761c:	nop
   47620:	ldp	x4, x5, [x1, #16]
   47624:	ldp	x8, x9, [x2, #16]
   47628:	orr	x12, x6, x10
   4762c:	orr	x13, x7, x11
   47630:	stp	x12, x13, [x0], #16
   47634:	ldp	x6, x7, [x1, #32]!
   47638:	ldp	x10, x11, [x2, #32]!
   4763c:	orr	x12, x4, x8
   47640:	orr	x13, x5, x9
   47644:	stp	x12, x13, [x0], #16
   47648:	sub	x18, x18, #0x1
   4764c:	cbnz	x18, 47620 <__gmpn_ior_n@@Base+0x70>
   47650:	orr	x12, x6, x10
   47654:	orr	x13, x7, x11
   47658:	stp	x12, x13, [x0]
   4765c:	ret

0000000000047660 <__gmpn_iorn_n@@Base>:
   47660:	lsr	x18, x3, #2
   47664:	tbz	w3, #0, 476ac <__gmpn_iorn_n@@Base+0x4c>
   47668:	ldr	x7, [x1]
   4766c:	ldr	x11, [x2]
   47670:	orn	x15, x7, x11
   47674:	str	x15, [x0], #8
   47678:	tbnz	w3, #1, 47694 <__gmpn_iorn_n@@Base+0x34>
   4767c:	cbz	x18, 4770c <__gmpn_iorn_n@@Base+0xac>
   47680:	ldp	x4, x5, [x1, #8]
   47684:	ldp	x8, x9, [x2, #8]
   47688:	sub	x1, x1, #0x8
   4768c:	sub	x2, x2, #0x8
   47690:	b	476e4 <__gmpn_iorn_n@@Base+0x84>
   47694:	ldp	x6, x7, [x1, #8]
   47698:	ldp	x10, x11, [x2, #8]
   4769c:	add	x1, x1, #0x8
   476a0:	add	x2, x2, #0x8
   476a4:	cbz	x18, 47700 <__gmpn_iorn_n@@Base+0xa0>
   476a8:	b	476d0 <__gmpn_iorn_n@@Base+0x70>
   476ac:	tbnz	w3, #1, 476bc <__gmpn_iorn_n@@Base+0x5c>
   476b0:	ldp	x4, x5, [x1], #-16
   476b4:	ldp	x8, x9, [x2], #-16
   476b8:	b	476e4 <__gmpn_iorn_n@@Base+0x84>
   476bc:	ldp	x6, x7, [x1]
   476c0:	ldp	x10, x11, [x2]
   476c4:	cbz	x18, 47700 <__gmpn_iorn_n@@Base+0xa0>
   476c8:	nop
   476cc:	nop
   476d0:	ldp	x4, x5, [x1, #16]
   476d4:	ldp	x8, x9, [x2, #16]
   476d8:	orn	x12, x6, x10
   476dc:	orn	x13, x7, x11
   476e0:	stp	x12, x13, [x0], #16
   476e4:	ldp	x6, x7, [x1, #32]!
   476e8:	ldp	x10, x11, [x2, #32]!
   476ec:	orn	x12, x4, x8
   476f0:	orn	x13, x5, x9
   476f4:	stp	x12, x13, [x0], #16
   476f8:	sub	x18, x18, #0x1
   476fc:	cbnz	x18, 476d0 <__gmpn_iorn_n@@Base+0x70>
   47700:	orn	x12, x6, x10
   47704:	orn	x13, x7, x11
   47708:	stp	x12, x13, [x0]
   4770c:	ret

0000000000047710 <__gmpn_nior_n@@Base>:
   47710:	lsr	x18, x3, #2
   47714:	tbz	w3, #0, 47760 <__gmpn_nior_n@@Base+0x50>
   47718:	ldr	x7, [x1]
   4771c:	ldr	x11, [x2]
   47720:	orr	x15, x7, x11
   47724:	mvn	x15, x15
   47728:	str	x15, [x0], #8
   4772c:	tbnz	w3, #1, 47748 <__gmpn_nior_n@@Base+0x38>
   47730:	cbz	x18, 477d4 <__gmpn_nior_n@@Base+0xc4>
   47734:	ldp	x4, x5, [x1, #8]
   47738:	ldp	x8, x9, [x2, #8]
   4773c:	sub	x1, x1, #0x8
   47740:	sub	x2, x2, #0x8
   47744:	b	4779c <__gmpn_nior_n@@Base+0x8c>
   47748:	ldp	x6, x7, [x1, #8]
   4774c:	ldp	x10, x11, [x2, #8]
   47750:	add	x1, x1, #0x8
   47754:	add	x2, x2, #0x8
   47758:	cbz	x18, 477c0 <__gmpn_nior_n@@Base+0xb0>
   4775c:	b	47780 <__gmpn_nior_n@@Base+0x70>
   47760:	tbnz	w3, #1, 47770 <__gmpn_nior_n@@Base+0x60>
   47764:	ldp	x4, x5, [x1], #-16
   47768:	ldp	x8, x9, [x2], #-16
   4776c:	b	4779c <__gmpn_nior_n@@Base+0x8c>
   47770:	ldp	x6, x7, [x1]
   47774:	ldp	x10, x11, [x2]
   47778:	cbz	x18, 477c0 <__gmpn_nior_n@@Base+0xb0>
   4777c:	nop
   47780:	ldp	x4, x5, [x1, #16]
   47784:	ldp	x8, x9, [x2, #16]
   47788:	orr	x12, x6, x10
   4778c:	orr	x13, x7, x11
   47790:	mvn	x12, x12
   47794:	mvn	x13, x13
   47798:	stp	x12, x13, [x0], #16
   4779c:	ldp	x6, x7, [x1, #32]!
   477a0:	ldp	x10, x11, [x2, #32]!
   477a4:	orr	x12, x4, x8
   477a8:	orr	x13, x5, x9
   477ac:	mvn	x12, x12
   477b0:	mvn	x13, x13
   477b4:	stp	x12, x13, [x0], #16
   477b8:	sub	x18, x18, #0x1
   477bc:	cbnz	x18, 47780 <__gmpn_nior_n@@Base+0x70>
   477c0:	orr	x12, x6, x10
   477c4:	orr	x13, x7, x11
   477c8:	mvn	x12, x12
   477cc:	mvn	x13, x13
   477d0:	stp	x12, x13, [x0]
   477d4:	ret
   477d8:	nop
   477dc:	nop

00000000000477e0 <__gmpn_xor_n@@Base>:
   477e0:	lsr	x18, x3, #2
   477e4:	tbz	w3, #0, 4782c <__gmpn_xor_n@@Base+0x4c>
   477e8:	ldr	x7, [x1]
   477ec:	ldr	x11, [x2]
   477f0:	eor	x15, x7, x11
   477f4:	str	x15, [x0], #8
   477f8:	tbnz	w3, #1, 47814 <__gmpn_xor_n@@Base+0x34>
   477fc:	cbz	x18, 4788c <__gmpn_xor_n@@Base+0xac>
   47800:	ldp	x4, x5, [x1, #8]
   47804:	ldp	x8, x9, [x2, #8]
   47808:	sub	x1, x1, #0x8
   4780c:	sub	x2, x2, #0x8
   47810:	b	47864 <__gmpn_xor_n@@Base+0x84>
   47814:	ldp	x6, x7, [x1, #8]
   47818:	ldp	x10, x11, [x2, #8]
   4781c:	add	x1, x1, #0x8
   47820:	add	x2, x2, #0x8
   47824:	cbz	x18, 47880 <__gmpn_xor_n@@Base+0xa0>
   47828:	b	47850 <__gmpn_xor_n@@Base+0x70>
   4782c:	tbnz	w3, #1, 4783c <__gmpn_xor_n@@Base+0x5c>
   47830:	ldp	x4, x5, [x1], #-16
   47834:	ldp	x8, x9, [x2], #-16
   47838:	b	47864 <__gmpn_xor_n@@Base+0x84>
   4783c:	ldp	x6, x7, [x1]
   47840:	ldp	x10, x11, [x2]
   47844:	cbz	x18, 47880 <__gmpn_xor_n@@Base+0xa0>
   47848:	nop
   4784c:	nop
   47850:	ldp	x4, x5, [x1, #16]
   47854:	ldp	x8, x9, [x2, #16]
   47858:	eor	x12, x6, x10
   4785c:	eor	x13, x7, x11
   47860:	stp	x12, x13, [x0], #16
   47864:	ldp	x6, x7, [x1, #32]!
   47868:	ldp	x10, x11, [x2, #32]!
   4786c:	eor	x12, x4, x8
   47870:	eor	x13, x5, x9
   47874:	stp	x12, x13, [x0], #16
   47878:	sub	x18, x18, #0x1
   4787c:	cbnz	x18, 47850 <__gmpn_xor_n@@Base+0x70>
   47880:	eor	x12, x6, x10
   47884:	eor	x13, x7, x11
   47888:	stp	x12, x13, [x0]
   4788c:	ret

0000000000047890 <__gmpn_xnor_n@@Base>:
   47890:	lsr	x18, x3, #2
   47894:	tbz	w3, #0, 478dc <__gmpn_xnor_n@@Base+0x4c>
   47898:	ldr	x7, [x1]
   4789c:	ldr	x11, [x2]
   478a0:	eon	x15, x7, x11
   478a4:	str	x15, [x0], #8
   478a8:	tbnz	w3, #1, 478c4 <__gmpn_xnor_n@@Base+0x34>
   478ac:	cbz	x18, 4793c <__gmpn_xnor_n@@Base+0xac>
   478b0:	ldp	x4, x5, [x1, #8]
   478b4:	ldp	x8, x9, [x2, #8]
   478b8:	sub	x1, x1, #0x8
   478bc:	sub	x2, x2, #0x8
   478c0:	b	47914 <__gmpn_xnor_n@@Base+0x84>
   478c4:	ldp	x6, x7, [x1, #8]
   478c8:	ldp	x10, x11, [x2, #8]
   478cc:	add	x1, x1, #0x8
   478d0:	add	x2, x2, #0x8
   478d4:	cbz	x18, 47930 <__gmpn_xnor_n@@Base+0xa0>
   478d8:	b	47900 <__gmpn_xnor_n@@Base+0x70>
   478dc:	tbnz	w3, #1, 478ec <__gmpn_xnor_n@@Base+0x5c>
   478e0:	ldp	x4, x5, [x1], #-16
   478e4:	ldp	x8, x9, [x2], #-16
   478e8:	b	47914 <__gmpn_xnor_n@@Base+0x84>
   478ec:	ldp	x6, x7, [x1]
   478f0:	ldp	x10, x11, [x2]
   478f4:	cbz	x18, 47930 <__gmpn_xnor_n@@Base+0xa0>
   478f8:	nop
   478fc:	nop
   47900:	ldp	x4, x5, [x1, #16]
   47904:	ldp	x8, x9, [x2, #16]
   47908:	eon	x12, x6, x10
   4790c:	eon	x13, x7, x11
   47910:	stp	x12, x13, [x0], #16
   47914:	ldp	x6, x7, [x1, #32]!
   47918:	ldp	x10, x11, [x2, #32]!
   4791c:	eon	x12, x4, x8
   47920:	eon	x13, x5, x9
   47924:	stp	x12, x13, [x0], #16
   47928:	sub	x18, x18, #0x1
   4792c:	cbnz	x18, 47900 <__gmpn_xnor_n@@Base+0x70>
   47930:	eon	x12, x6, x10
   47934:	eon	x13, x7, x11
   47938:	stp	x12, x13, [x0]
   4793c:	ret

0000000000047940 <__gmpn_copyi@@Base>:
   47940:	cmp	x2, #0x3
   47944:	b.le	4798c <__gmpn_copyi@@Base+0x4c>
   47948:	tbz	w0, #3, 47958 <__gmpn_copyi@@Base+0x18>
   4794c:	ld1	{v22.1d}, [x1], #8
   47950:	sub	x2, x2, #0x1
   47954:	st1	{v22.1d}, [x0], #8
   47958:	ld1	{v26.2d}, [x1], #16
   4795c:	sub	x2, x2, #0x6
   47960:	tbnz	x2, #63, 47988 <__gmpn_copyi@@Base+0x48>
   47964:	nop
   47968:	nop
   4796c:	nop
   47970:	ld1	{v22.2d}, [x1], #16
   47974:	st1	{v26.2d}, [x0], #16
   47978:	ld1	{v26.2d}, [x1], #16
   4797c:	st1	{v22.2d}, [x0], #16
   47980:	sub	x2, x2, #0x4
   47984:	tbz	x2, #63, 47970 <__gmpn_copyi@@Base+0x30>
   47988:	st1	{v26.2d}, [x0], #16
   4798c:	tbz	w2, #1, 47998 <__gmpn_copyi@@Base+0x58>
   47990:	ld1	{v22.2d}, [x1], #16
   47994:	st1	{v22.2d}, [x0], #16
   47998:	tbz	w2, #0, 479a4 <__gmpn_copyi@@Base+0x64>
   4799c:	ld1	{v22.1d}, [x1]
   479a0:	st1	{v22.1d}, [x0]
   479a4:	ret
   479a8:	nop
   479ac:	nop

00000000000479b0 <__gmpn_copyd@@Base>:
   479b0:	add	x0, x0, x2, lsl #3
   479b4:	add	x1, x1, x2, lsl #3
   479b8:	cmp	x2, #0x3
   479bc:	b.le	47a20 <__gmpn_copyd@@Base+0x70>
   479c0:	tbz	w0, #3, 479d8 <__gmpn_copyd@@Base+0x28>
   479c4:	sub	x1, x1, #0x8
   479c8:	ld1	{v22.1d}, [x1]
   479cc:	sub	x2, x2, #0x1
   479d0:	sub	x0, x0, #0x8
   479d4:	st1	{v22.1d}, [x0]
   479d8:	sub	x1, x1, #0x10
   479dc:	ld1	{v26.2d}, [x1]
   479e0:	sub	x2, x2, #0x6
   479e4:	sub	x0, x0, #0x10
   479e8:	tbnz	x2, #63, 47a1c <__gmpn_copyd@@Base+0x6c>
   479ec:	sub	x1, x1, #0x10
   479f0:	mov	x12, #0xfffffffffffffff0    	// #-16
   479f4:	nop
   479f8:	nop
   479fc:	nop
   47a00:	ld1	{v22.2d}, [x1], x12
   47a04:	st1	{v26.2d}, [x0], x12
   47a08:	ld1	{v26.2d}, [x1], x12
   47a0c:	st1	{v22.2d}, [x0], x12
   47a10:	sub	x2, x2, #0x4
   47a14:	tbz	x2, #63, 47a00 <__gmpn_copyd@@Base+0x50>
   47a18:	add	x1, x1, #0x10
   47a1c:	st1	{v26.2d}, [x0]
   47a20:	tbz	w2, #1, 47a34 <__gmpn_copyd@@Base+0x84>
   47a24:	sub	x1, x1, #0x10
   47a28:	ld1	{v22.2d}, [x1]
   47a2c:	sub	x0, x0, #0x10
   47a30:	st1	{v22.2d}, [x0]
   47a34:	tbz	w2, #0, 47a48 <__gmpn_copyd@@Base+0x98>
   47a38:	sub	x1, x1, #0x8
   47a3c:	ld1	{v22.1d}, [x1]
   47a40:	sub	x0, x0, #0x8
   47a44:	st1	{v22.1d}, [x0]
   47a48:	ret

0000000000047a4c <__gmpn_zero@@Base>:
   47a4c:	cbz	x1, 47a68 <__gmpn_zero@@Base+0x1c>
   47a50:	stp	x29, x30, [sp, #-16]!
   47a54:	lsl	x2, x1, #3
   47a58:	mov	w1, wzr
   47a5c:	mov	x29, sp
   47a60:	bl	c780 <memset@plt>
   47a64:	ldp	x29, x30, [sp], #16
   47a68:	ret
   47a6c:	nop

0000000000047a70 <__gmpn_sec_tabselect@@Base>:
   47a70:	dup	v7.2d, x4
   47a74:	mov	x10, #0x1                   	// #1
   47a78:	dup	v6.2d, x10
   47a7c:	subs	x6, x2, #0x4
   47a80:	b.mi	47ad0 <__gmpn_sec_tabselect@@Base+0x60>  // b.first
   47a84:	mov	x5, x3
   47a88:	mov	x12, x1
   47a8c:	movi	v5.16b, #0x0
   47a90:	movi	v2.16b, #0x0
   47a94:	movi	v3.16b, #0x0
   47a98:	nop
   47a9c:	nop
   47aa0:	cmeq	v4.2d, v5.2d, v7.2d
   47aa4:	ld1	{v0.2d, v1.2d}, [x1]
   47aa8:	add	v5.2d, v5.2d, v6.2d
   47aac:	bit	v2.16b, v0.16b, v4.16b
   47ab0:	bit	v3.16b, v1.16b, v4.16b
   47ab4:	add	x1, x1, x2, lsl #3
   47ab8:	sub	x5, x5, #0x1
   47abc:	cbnz	x5, 47aa0 <__gmpn_sec_tabselect@@Base+0x30>
   47ac0:	st1	{v2.2d, v3.2d}, [x0], #32
   47ac4:	add	x1, x12, #0x20
   47ac8:	subs	x6, x6, #0x4
   47acc:	b.pl	47a84 <__gmpn_sec_tabselect@@Base+0x14>  // b.nfrst
   47ad0:	tbz	w2, #1, 47b14 <__gmpn_sec_tabselect@@Base+0xa4>
   47ad4:	mov	x5, x3
   47ad8:	mov	x12, x1
   47adc:	movi	v5.16b, #0x0
   47ae0:	movi	v2.16b, #0x0
   47ae4:	nop
   47ae8:	nop
   47aec:	nop
   47af0:	cmeq	v4.2d, v5.2d, v7.2d
   47af4:	ld1	{v0.2d}, [x1]
   47af8:	add	v5.2d, v5.2d, v6.2d
   47afc:	bit	v2.16b, v0.16b, v4.16b
   47b00:	add	x1, x1, x2, lsl #3
   47b04:	sub	x5, x5, #0x1
   47b08:	cbnz	x5, 47af0 <__gmpn_sec_tabselect@@Base+0x80>
   47b0c:	st1	{v2.2d}, [x0], #16
   47b10:	add	x1, x12, #0x10
   47b14:	tbz	w2, #0, 47b54 <__gmpn_sec_tabselect@@Base+0xe4>
   47b18:	mov	x5, x3
   47b1c:	mov	x12, x1
   47b20:	movi	v5.16b, #0x0
   47b24:	movi	v2.16b, #0x0
   47b28:	nop
   47b2c:	nop
   47b30:	cmeq	v4.2d, v5.2d, v7.2d
   47b34:	ld1	{v0.1d}, [x1]
   47b38:	add	v5.2d, v5.2d, v6.2d
   47b3c:	bit	v2.8b, v0.8b, v4.8b
   47b40:	add	x1, x1, x2, lsl #3
   47b44:	sub	x5, x5, #0x1
   47b48:	cbnz	x5, 47b30 <__gmpn_sec_tabselect@@Base+0xc0>
   47b4c:	st1	{v2.1d}, [x0], #8
   47b50:	add	x1, x12, #0x8
   47b54:	ret

0000000000047b58 <__gmpn_invert_limb@@Base>:
   47b58:	lsr	x2, x0, #54
   47b5c:	adrp	x1, 57000 <__gmp_jacobi_table@@Base+0x6ef1>
   47b60:	and	x2, x2, #0x1fe
   47b64:	add	x1, x1, #0xd08
   47b68:	ldrh	w3, [x1, x2]
   47b6c:	lsr	x4, x0, #24
   47b70:	add	x4, x4, #0x1
   47b74:	ubfiz	x2, x3, #11, #16
   47b78:	umull	x3, w3, w3
   47b7c:	mul	x3, x3, x4
   47b80:	sub	x2, x2, #0x1
   47b84:	sub	x2, x2, x3, lsr #40
   47b88:	lsl	x3, x2, #60
   47b8c:	mul	x1, x2, x2
   47b90:	msub	x1, x1, x4, x3
   47b94:	lsl	x2, x2, #13
   47b98:	add	x1, x2, x1, lsr #47
   47b9c:	and	x2, x0, #0x1
   47ba0:	neg	x3, x2
   47ba4:	and	x3, x3, x1, lsr #1
   47ba8:	add	x2, x2, x0, lsr #1
   47bac:	msub	x2, x1, x2, x3
   47bb0:	umulh	x2, x2, x1
   47bb4:	lsl	x1, x1, #31
   47bb8:	add	x1, x1, x2, lsr #1
   47bbc:	mul	x3, x1, x0
   47bc0:	umulh	x2, x1, x0
   47bc4:	adds	x4, x3, x0
   47bc8:	adc	x0, x2, x0
   47bcc:	sub	x0, x1, x0
   47bd0:	ret
   47bd4:	nop
   47bd8:	nop
   47bdc:	nop

0000000000047be0 <__gmpn_sqr_diag_addlsh1@@Base>:
   47be0:	ldr	x15, [x2], #8
   47be4:	lsr	x18, x3, #1
   47be8:	tbz	w3, #0, 47c04 <__gmpn_sqr_diag_addlsh1@@Base+0x24>
   47bec:	adds	x7, xzr, xzr
   47bf0:	mul	x12, x15, x15
   47bf4:	ldr	x16, [x2], #8
   47bf8:	ldp	x4, x5, [x1], #16
   47bfc:	umulh	x11, x15, x15
   47c00:	b	47c44 <__gmpn_sqr_diag_addlsh1@@Base+0x64>
   47c04:	adds	x5, xzr, xzr
   47c08:	mul	x12, x15, x15
   47c0c:	ldr	x17, [x2], #16
   47c10:	ldp	x6, x7, [x1], #32
   47c14:	umulh	x11, x15, x15
   47c18:	sub	x18, x18, #0x1
   47c1c:	cbz	x18, 47c70 <__gmpn_sqr_diag_addlsh1@@Base+0x90>
   47c20:	extr	x9, x6, x5, #63
   47c24:	mul	x10, x17, x17
   47c28:	ldur	x16, [x2, #-8]
   47c2c:	adcs	x13, x9, x11
   47c30:	ldp	x4, x5, [x1, #-16]
   47c34:	umulh	x11, x17, x17
   47c38:	extr	x8, x7, x6, #63
   47c3c:	stp	x12, x13, [x0], #16
   47c40:	adcs	x12, x8, x10
   47c44:	extr	x9, x4, x7, #63
   47c48:	mul	x10, x16, x16
   47c4c:	ldr	x17, [x2], #16
   47c50:	adcs	x13, x9, x11
   47c54:	ldp	x6, x7, [x1], #32
   47c58:	umulh	x11, x16, x16
   47c5c:	extr	x8, x5, x4, #63
   47c60:	stp	x12, x13, [x0], #16
   47c64:	adcs	x12, x8, x10
   47c68:	sub	x18, x18, #0x1
   47c6c:	cbnz	x18, 47c20 <__gmpn_sqr_diag_addlsh1@@Base+0x40>
   47c70:	extr	x9, x6, x5, #63
   47c74:	mul	x10, x17, x17
   47c78:	adcs	x13, x9, x11
   47c7c:	umulh	x11, x17, x17
   47c80:	extr	x8, x7, x6, #63
   47c84:	stp	x12, x13, [x0]
   47c88:	adcs	x12, x8, x10
   47c8c:	extr	x9, xzr, x7, #63
   47c90:	adcs	x13, x9, x11
   47c94:	stp	x12, x13, [x0, #16]
   47c98:	ret
   47c9c:	nop

0000000000047ca0 <__gmpn_addlsh1_n@@Base>:
   47ca0:	lsr	x18, x3, #2
   47ca4:	tbz	w3, #0, 47d0c <__gmpn_addlsh1_n@@Base+0x6c>
   47ca8:	ldr	x5, [x1]
   47cac:	tbnz	w3, #1, 47cec <__gmpn_addlsh1_n@@Base+0x4c>
   47cb0:	ldr	x11, [x2]
   47cb4:	cbz	x18, 47cd4 <__gmpn_addlsh1_n@@Base+0x34>
   47cb8:	ldp	x8, x9, [x2, #8]
   47cbc:	lsl	x13, x11, #1
   47cc0:	adds	x15, x13, x5
   47cc4:	str	x15, [x0], #8
   47cc8:	sub	x1, x1, #0x18
   47ccc:	sub	x2, x2, #0x8
   47cd0:	b	47d4c <__gmpn_addlsh1_n@@Base+0xac>
   47cd4:	lsl	x13, x11, #1
   47cd8:	adds	x15, x13, x5
   47cdc:	str	x15, [x0]
   47ce0:	lsr	x0, x11, #63
   47ce4:	adc	x0, x0, xzr
   47ce8:	ret
   47cec:	ldr	x9, [x2]
   47cf0:	ldp	x10, x11, [x2, #8]!
   47cf4:	lsl	x13, x9, #1
   47cf8:	adds	x17, x13, x5
   47cfc:	str	x17, [x0], #8
   47d00:	sub	x1, x1, #0x8
   47d04:	cbz	x18, 47d70 <__gmpn_addlsh1_n@@Base+0xd0>
   47d08:	b	47d30 <__gmpn_addlsh1_n@@Base+0x90>
   47d0c:	tbnz	w3, #1, 47d20 <__gmpn_addlsh1_n@@Base+0x80>
   47d10:	adds	x11, xzr, xzr
   47d14:	ldp	x8, x9, [x2], #-16
   47d18:	sub	x1, x1, #0x20
   47d1c:	b	47d4c <__gmpn_addlsh1_n@@Base+0xac>
   47d20:	adds	x9, xzr, xzr
   47d24:	ldp	x10, x11, [x2]
   47d28:	sub	x1, x1, #0x10
   47d2c:	cbz	x18, 47d70 <__gmpn_addlsh1_n@@Base+0xd0>
   47d30:	ldp	x4, x5, [x1, #16]
   47d34:	extr	x12, x10, x9, #63
   47d38:	ldp	x8, x9, [x2, #16]
   47d3c:	extr	x13, x11, x10, #63
   47d40:	adcs	x14, x12, x4
   47d44:	adcs	x15, x13, x5
   47d48:	stp	x14, x15, [x0], #16
   47d4c:	ldp	x4, x5, [x1, #32]!
   47d50:	extr	x12, x8, x11, #63
   47d54:	ldp	x10, x11, [x2, #32]!
   47d58:	extr	x13, x9, x8, #63
   47d5c:	adcs	x16, x12, x4
   47d60:	adcs	x17, x13, x5
   47d64:	stp	x16, x17, [x0], #16
   47d68:	sub	x18, x18, #0x1
   47d6c:	cbnz	x18, 47d30 <__gmpn_addlsh1_n@@Base+0x90>
   47d70:	ldp	x4, x5, [x1, #16]
   47d74:	extr	x12, x10, x9, #63
   47d78:	extr	x13, x11, x10, #63
   47d7c:	adcs	x14, x12, x4
   47d80:	adcs	x15, x13, x5
   47d84:	stp	x14, x15, [x0]
   47d88:	lsr	x0, x11, #63
   47d8c:	adc	x0, x0, xzr
   47d90:	ret
   47d94:	nop
   47d98:	nop
   47d9c:	nop

0000000000047da0 <__gmpn_sublsh1_n@@Base>:
   47da0:	lsr	x18, x3, #2
   47da4:	tbz	w3, #0, 47e0c <__gmpn_sublsh1_n@@Base+0x6c>
   47da8:	ldr	x5, [x1]
   47dac:	tbnz	w3, #1, 47dec <__gmpn_sublsh1_n@@Base+0x4c>
   47db0:	ldr	x11, [x2]
   47db4:	cbz	x18, 47dd4 <__gmpn_sublsh1_n@@Base+0x34>
   47db8:	ldp	x8, x9, [x2, #8]
   47dbc:	lsl	x13, x11, #1
   47dc0:	subs	x15, x5, x13
   47dc4:	str	x15, [x0], #8
   47dc8:	sub	x1, x1, #0x18
   47dcc:	sub	x2, x2, #0x8
   47dd0:	b	47e4c <__gmpn_sublsh1_n@@Base+0xac>
   47dd4:	lsl	x13, x11, #1
   47dd8:	subs	x15, x5, x13
   47ddc:	str	x15, [x0]
   47de0:	lsr	x0, x11, #63
   47de4:	cinc	x0, x0, cc  // cc = lo, ul, last
   47de8:	ret
   47dec:	ldr	x9, [x2]
   47df0:	ldp	x10, x11, [x2, #8]!
   47df4:	lsl	x13, x9, #1
   47df8:	subs	x17, x5, x13
   47dfc:	str	x17, [x0], #8
   47e00:	sub	x1, x1, #0x8
   47e04:	cbz	x18, 47e70 <__gmpn_sublsh1_n@@Base+0xd0>
   47e08:	b	47e30 <__gmpn_sublsh1_n@@Base+0x90>
   47e0c:	tbnz	w3, #1, 47e20 <__gmpn_sublsh1_n@@Base+0x80>
   47e10:	negs	x11, xzr
   47e14:	ldp	x8, x9, [x2], #-16
   47e18:	sub	x1, x1, #0x20
   47e1c:	b	47e4c <__gmpn_sublsh1_n@@Base+0xac>
   47e20:	negs	x9, xzr
   47e24:	ldp	x10, x11, [x2]
   47e28:	sub	x1, x1, #0x10
   47e2c:	cbz	x18, 47e70 <__gmpn_sublsh1_n@@Base+0xd0>
   47e30:	ldp	x4, x5, [x1, #16]
   47e34:	extr	x12, x10, x9, #63
   47e38:	ldp	x8, x9, [x2, #16]
   47e3c:	extr	x13, x11, x10, #63
   47e40:	sbcs	x14, x4, x12
   47e44:	sbcs	x15, x5, x13
   47e48:	stp	x14, x15, [x0], #16
   47e4c:	ldp	x4, x5, [x1, #32]!
   47e50:	extr	x12, x8, x11, #63
   47e54:	ldp	x10, x11, [x2, #32]!
   47e58:	extr	x13, x9, x8, #63
   47e5c:	sbcs	x16, x4, x12
   47e60:	sbcs	x17, x5, x13
   47e64:	stp	x16, x17, [x0], #16
   47e68:	sub	x18, x18, #0x1
   47e6c:	cbnz	x18, 47e30 <__gmpn_sublsh1_n@@Base+0x90>
   47e70:	ldp	x4, x5, [x1, #16]
   47e74:	extr	x12, x10, x9, #63
   47e78:	extr	x13, x11, x10, #63
   47e7c:	sbcs	x14, x4, x12
   47e80:	sbcs	x15, x5, x13
   47e84:	stp	x14, x15, [x0]
   47e88:	lsr	x0, x11, #63
   47e8c:	cinc	x0, x0, cc  // cc = lo, ul, last
   47e90:	ret
   47e94:	nop
   47e98:	nop
   47e9c:	nop

0000000000047ea0 <__gmpn_rsblsh1_n@@Base>:
   47ea0:	lsr	x18, x3, #2
   47ea4:	tbz	w3, #0, 47f0c <__gmpn_rsblsh1_n@@Base+0x6c>
   47ea8:	ldr	x5, [x1]
   47eac:	tbnz	w3, #1, 47eec <__gmpn_rsblsh1_n@@Base+0x4c>
   47eb0:	ldr	x11, [x2]
   47eb4:	cbz	x18, 47ed4 <__gmpn_rsblsh1_n@@Base+0x34>
   47eb8:	ldp	x8, x9, [x2, #8]
   47ebc:	lsl	x13, x11, #1
   47ec0:	subs	x15, x13, x5
   47ec4:	str	x15, [x0], #8
   47ec8:	sub	x1, x1, #0x18
   47ecc:	sub	x2, x2, #0x8
   47ed0:	b	47f4c <__gmpn_rsblsh1_n@@Base+0xac>
   47ed4:	lsl	x13, x11, #1
   47ed8:	subs	x15, x13, x5
   47edc:	str	x15, [x0]
   47ee0:	lsr	x0, x11, #63
   47ee4:	sbc	x0, x0, xzr
   47ee8:	ret
   47eec:	ldr	x9, [x2]
   47ef0:	ldp	x10, x11, [x2, #8]!
   47ef4:	lsl	x13, x9, #1
   47ef8:	subs	x17, x13, x5
   47efc:	str	x17, [x0], #8
   47f00:	sub	x1, x1, #0x8
   47f04:	cbz	x18, 47f70 <__gmpn_rsblsh1_n@@Base+0xd0>
   47f08:	b	47f30 <__gmpn_rsblsh1_n@@Base+0x90>
   47f0c:	tbnz	w3, #1, 47f20 <__gmpn_rsblsh1_n@@Base+0x80>
   47f10:	negs	x11, xzr
   47f14:	ldp	x8, x9, [x2], #-16
   47f18:	sub	x1, x1, #0x20
   47f1c:	b	47f4c <__gmpn_rsblsh1_n@@Base+0xac>
   47f20:	negs	x9, xzr
   47f24:	ldp	x10, x11, [x2]
   47f28:	sub	x1, x1, #0x10
   47f2c:	cbz	x18, 47f70 <__gmpn_rsblsh1_n@@Base+0xd0>
   47f30:	ldp	x4, x5, [x1, #16]
   47f34:	extr	x12, x10, x9, #63
   47f38:	ldp	x8, x9, [x2, #16]
   47f3c:	extr	x13, x11, x10, #63
   47f40:	sbcs	x14, x12, x4
   47f44:	sbcs	x15, x13, x5
   47f48:	stp	x14, x15, [x0], #16
   47f4c:	ldp	x4, x5, [x1, #32]!
   47f50:	extr	x12, x8, x11, #63
   47f54:	ldp	x10, x11, [x2, #32]!
   47f58:	extr	x13, x9, x8, #63
   47f5c:	sbcs	x16, x12, x4
   47f60:	sbcs	x17, x13, x5
   47f64:	stp	x16, x17, [x0], #16
   47f68:	sub	x18, x18, #0x1
   47f6c:	cbnz	x18, 47f30 <__gmpn_rsblsh1_n@@Base+0x90>
   47f70:	ldp	x4, x5, [x1, #16]
   47f74:	extr	x12, x10, x9, #63
   47f78:	extr	x13, x11, x10, #63
   47f7c:	sbcs	x14, x12, x4
   47f80:	sbcs	x15, x13, x5
   47f84:	stp	x14, x15, [x0]
   47f88:	lsr	x0, x11, #63
   47f8c:	sbc	x0, x0, xzr
   47f90:	ret
   47f94:	nop
   47f98:	nop
   47f9c:	nop

0000000000047fa0 <__gmpn_rsh1add_n@@Base>:
   47fa0:	lsr	x18, x3, #2
   47fa4:	tbz	w3, #0, 4804c <__gmpn_rsh1add_n@@Base+0xac>
   47fa8:	ldr	x5, [x1], #8
   47fac:	ldr	x9, [x2], #8
   47fb0:	tbnz	w3, #1, 48008 <__gmpn_rsh1add_n@@Base+0x68>
   47fb4:	adds	x13, x5, x9
   47fb8:	and	x10, x13, #0x1
   47fbc:	cbz	x18, 47ff4 <__gmpn_rsh1add_n@@Base+0x54>
   47fc0:	ldp	x4, x5, [x1], #48
   47fc4:	ldp	x8, x9, [x2], #48
   47fc8:	adcs	x14, x4, x8
   47fcc:	adcs	x15, x5, x9
   47fd0:	ldp	x4, x5, [x1, #-32]
   47fd4:	ldp	x8, x9, [x2, #-32]
   47fd8:	extr	x17, x14, x13, #1
   47fdc:	adcs	x12, x4, x8
   47fe0:	adcs	x13, x5, x9
   47fe4:	str	x17, [x0], #24
   47fe8:	sub	x18, x18, #0x1
   47fec:	cbz	x18, 480f0 <__gmpn_rsh1add_n@@Base+0x150>
   47ff0:	b	480b0 <__gmpn_rsh1add_n@@Base+0x110>
   47ff4:	cset	x14, cs  // cs = hs, nlast
   47ff8:	extr	x17, x14, x13, #1
   47ffc:	str	x17, [x0]
   48000:	mov	x0, x10
   48004:	ret
   48008:	adds	x15, x5, x9
   4800c:	and	x10, x15, #0x1
   48010:	ldp	x4, x5, [x1], #32
   48014:	ldp	x8, x9, [x2], #32
   48018:	adcs	x12, x4, x8
   4801c:	adcs	x13, x5, x9
   48020:	cbz	x18, 48040 <__gmpn_rsh1add_n@@Base+0xa0>
   48024:	ldp	x4, x5, [x1, #-16]
   48028:	ldp	x8, x9, [x2, #-16]
   4802c:	extr	x17, x12, x15, #1
   48030:	adcs	x14, x4, x8
   48034:	adcs	x15, x5, x9
   48038:	str	x17, [x0], #8
   4803c:	b	480cc <__gmpn_rsh1add_n@@Base+0x12c>
   48040:	extr	x17, x12, x15, #1
   48044:	str	x17, [x0], #8
   48048:	b	480fc <__gmpn_rsh1add_n@@Base+0x15c>
   4804c:	tbz	w3, #1, 4807c <__gmpn_rsh1add_n@@Base+0xdc>
   48050:	ldp	x4, x5, [x1], #32
   48054:	ldp	x8, x9, [x2], #32
   48058:	adds	x12, x4, x8
   4805c:	adcs	x13, x5, x9
   48060:	and	x10, x12, #0x1
   48064:	cbz	x18, 480fc <__gmpn_rsh1add_n@@Base+0x15c>
   48068:	ldp	x4, x5, [x1, #-16]
   4806c:	ldp	x8, x9, [x2, #-16]
   48070:	adcs	x14, x4, x8
   48074:	adcs	x15, x5, x9
   48078:	b	480cc <__gmpn_rsh1add_n@@Base+0x12c>
   4807c:	ldp	x4, x5, [x1], #48
   48080:	ldp	x8, x9, [x2], #48
   48084:	adds	x14, x4, x8
   48088:	adcs	x15, x5, x9
   4808c:	and	x10, x14, #0x1
   48090:	ldp	x4, x5, [x1, #-32]
   48094:	ldp	x8, x9, [x2, #-32]
   48098:	adcs	x12, x4, x8
   4809c:	adcs	x13, x5, x9
   480a0:	add	x0, x0, #0x10
   480a4:	sub	x18, x18, #0x1
   480a8:	cbz	x18, 480f0 <__gmpn_rsh1add_n@@Base+0x150>
   480ac:	nop
   480b0:	ldp	x4, x5, [x1, #-16]
   480b4:	ldp	x8, x9, [x2, #-16]
   480b8:	extr	x16, x15, x14, #1
   480bc:	extr	x17, x12, x15, #1
   480c0:	adcs	x14, x4, x8
   480c4:	adcs	x15, x5, x9
   480c8:	stp	x16, x17, [x0, #-16]
   480cc:	ldp	x4, x5, [x1], #32
   480d0:	ldp	x8, x9, [x2], #32
   480d4:	extr	x16, x13, x12, #1
   480d8:	extr	x17, x14, x13, #1
   480dc:	adcs	x12, x4, x8
   480e0:	adcs	x13, x5, x9
   480e4:	stp	x16, x17, [x0], #32
   480e8:	sub	x18, x18, #0x1
   480ec:	cbnz	x18, 480b0 <__gmpn_rsh1add_n@@Base+0x110>
   480f0:	extr	x16, x15, x14, #1
   480f4:	extr	x17, x12, x15, #1
   480f8:	stp	x16, x17, [x0, #-16]
   480fc:	cset	x14, cs  // cs = hs, nlast
   48100:	extr	x16, x13, x12, #1
   48104:	extr	x17, x14, x13, #1
   48108:	stp	x16, x17, [x0]
   4810c:	mov	x0, x10
   48110:	ret
   48114:	nop
   48118:	nop
   4811c:	nop

0000000000048120 <__gmpn_rsh1sub_n@@Base>:
   48120:	lsr	x18, x3, #2
   48124:	tbz	w3, #0, 481cc <__gmpn_rsh1sub_n@@Base+0xac>
   48128:	ldr	x5, [x1], #8
   4812c:	ldr	x9, [x2], #8
   48130:	tbnz	w3, #1, 48188 <__gmpn_rsh1sub_n@@Base+0x68>
   48134:	subs	x13, x5, x9
   48138:	and	x10, x13, #0x1
   4813c:	cbz	x18, 48174 <__gmpn_rsh1sub_n@@Base+0x54>
   48140:	ldp	x4, x5, [x1], #48
   48144:	ldp	x8, x9, [x2], #48
   48148:	sbcs	x14, x4, x8
   4814c:	sbcs	x15, x5, x9
   48150:	ldp	x4, x5, [x1, #-32]
   48154:	ldp	x8, x9, [x2, #-32]
   48158:	extr	x17, x14, x13, #1
   4815c:	sbcs	x12, x4, x8
   48160:	sbcs	x13, x5, x9
   48164:	str	x17, [x0], #24
   48168:	sub	x18, x18, #0x1
   4816c:	cbz	x18, 48270 <__gmpn_rsh1sub_n@@Base+0x150>
   48170:	b	48230 <__gmpn_rsh1sub_n@@Base+0x110>
   48174:	cset	x14, cc  // cc = lo, ul, last
   48178:	extr	x17, x14, x13, #1
   4817c:	str	x17, [x0]
   48180:	mov	x0, x10
   48184:	ret
   48188:	subs	x15, x5, x9
   4818c:	and	x10, x15, #0x1
   48190:	ldp	x4, x5, [x1], #32
   48194:	ldp	x8, x9, [x2], #32
   48198:	sbcs	x12, x4, x8
   4819c:	sbcs	x13, x5, x9
   481a0:	cbz	x18, 481c0 <__gmpn_rsh1sub_n@@Base+0xa0>
   481a4:	ldp	x4, x5, [x1, #-16]
   481a8:	ldp	x8, x9, [x2, #-16]
   481ac:	extr	x17, x12, x15, #1
   481b0:	sbcs	x14, x4, x8
   481b4:	sbcs	x15, x5, x9
   481b8:	str	x17, [x0], #8
   481bc:	b	4824c <__gmpn_rsh1sub_n@@Base+0x12c>
   481c0:	extr	x17, x12, x15, #1
   481c4:	str	x17, [x0], #8
   481c8:	b	4827c <__gmpn_rsh1sub_n@@Base+0x15c>
   481cc:	tbz	w3, #1, 481fc <__gmpn_rsh1sub_n@@Base+0xdc>
   481d0:	ldp	x4, x5, [x1], #32
   481d4:	ldp	x8, x9, [x2], #32
   481d8:	subs	x12, x4, x8
   481dc:	sbcs	x13, x5, x9
   481e0:	and	x10, x12, #0x1
   481e4:	cbz	x18, 4827c <__gmpn_rsh1sub_n@@Base+0x15c>
   481e8:	ldp	x4, x5, [x1, #-16]
   481ec:	ldp	x8, x9, [x2, #-16]
   481f0:	sbcs	x14, x4, x8
   481f4:	sbcs	x15, x5, x9
   481f8:	b	4824c <__gmpn_rsh1sub_n@@Base+0x12c>
   481fc:	ldp	x4, x5, [x1], #48
   48200:	ldp	x8, x9, [x2], #48
   48204:	subs	x14, x4, x8
   48208:	sbcs	x15, x5, x9
   4820c:	and	x10, x14, #0x1
   48210:	ldp	x4, x5, [x1, #-32]
   48214:	ldp	x8, x9, [x2, #-32]
   48218:	sbcs	x12, x4, x8
   4821c:	sbcs	x13, x5, x9
   48220:	add	x0, x0, #0x10
   48224:	sub	x18, x18, #0x1
   48228:	cbz	x18, 48270 <__gmpn_rsh1sub_n@@Base+0x150>
   4822c:	nop
   48230:	ldp	x4, x5, [x1, #-16]
   48234:	ldp	x8, x9, [x2, #-16]
   48238:	extr	x16, x15, x14, #1
   4823c:	extr	x17, x12, x15, #1
   48240:	sbcs	x14, x4, x8
   48244:	sbcs	x15, x5, x9
   48248:	stp	x16, x17, [x0, #-16]
   4824c:	ldp	x4, x5, [x1], #32
   48250:	ldp	x8, x9, [x2], #32
   48254:	extr	x16, x13, x12, #1
   48258:	extr	x17, x14, x13, #1
   4825c:	sbcs	x12, x4, x8
   48260:	sbcs	x13, x5, x9
   48264:	stp	x16, x17, [x0], #32
   48268:	sub	x18, x18, #0x1
   4826c:	cbnz	x18, 48230 <__gmpn_rsh1sub_n@@Base+0x110>
   48270:	extr	x16, x15, x14, #1
   48274:	extr	x17, x12, x15, #1
   48278:	stp	x16, x17, [x0, #-16]
   4827c:	cset	x14, cc  // cc = lo, ul, last
   48280:	extr	x16, x13, x12, #1
   48284:	extr	x17, x14, x13, #1
   48288:	stp	x16, x17, [x0]
   4828c:	mov	x0, x10
   48290:	ret
   48294:	nop
   48298:	nop
   4829c:	nop

00000000000482a0 <__gmpn_addlsh2_n@@Base>:
   482a0:	lsr	x18, x3, #2
   482a4:	tbz	w3, #0, 4830c <__gmpn_addlsh2_n@@Base+0x6c>
   482a8:	ldr	x5, [x1]
   482ac:	tbnz	w3, #1, 482ec <__gmpn_addlsh2_n@@Base+0x4c>
   482b0:	ldr	x11, [x2]
   482b4:	cbz	x18, 482d4 <__gmpn_addlsh2_n@@Base+0x34>
   482b8:	ldp	x8, x9, [x2, #8]
   482bc:	lsl	x13, x11, #2
   482c0:	adds	x15, x13, x5
   482c4:	str	x15, [x0], #8
   482c8:	sub	x1, x1, #0x18
   482cc:	sub	x2, x2, #0x8
   482d0:	b	4834c <__gmpn_addlsh2_n@@Base+0xac>
   482d4:	lsl	x13, x11, #2
   482d8:	adds	x15, x13, x5
   482dc:	str	x15, [x0]
   482e0:	lsr	x0, x11, #62
   482e4:	adc	x0, x0, xzr
   482e8:	ret
   482ec:	ldr	x9, [x2]
   482f0:	ldp	x10, x11, [x2, #8]!
   482f4:	lsl	x13, x9, #2
   482f8:	adds	x17, x13, x5
   482fc:	str	x17, [x0], #8
   48300:	sub	x1, x1, #0x8
   48304:	cbz	x18, 48370 <__gmpn_addlsh2_n@@Base+0xd0>
   48308:	b	48330 <__gmpn_addlsh2_n@@Base+0x90>
   4830c:	tbnz	w3, #1, 48320 <__gmpn_addlsh2_n@@Base+0x80>
   48310:	adds	x11, xzr, xzr
   48314:	ldp	x8, x9, [x2], #-16
   48318:	sub	x1, x1, #0x20
   4831c:	b	4834c <__gmpn_addlsh2_n@@Base+0xac>
   48320:	adds	x9, xzr, xzr
   48324:	ldp	x10, x11, [x2]
   48328:	sub	x1, x1, #0x10
   4832c:	cbz	x18, 48370 <__gmpn_addlsh2_n@@Base+0xd0>
   48330:	ldp	x4, x5, [x1, #16]
   48334:	extr	x12, x10, x9, #62
   48338:	ldp	x8, x9, [x2, #16]
   4833c:	extr	x13, x11, x10, #62
   48340:	adcs	x14, x12, x4
   48344:	adcs	x15, x13, x5
   48348:	stp	x14, x15, [x0], #16
   4834c:	ldp	x4, x5, [x1, #32]!
   48350:	extr	x12, x8, x11, #62
   48354:	ldp	x10, x11, [x2, #32]!
   48358:	extr	x13, x9, x8, #62
   4835c:	adcs	x16, x12, x4
   48360:	adcs	x17, x13, x5
   48364:	stp	x16, x17, [x0], #16
   48368:	sub	x18, x18, #0x1
   4836c:	cbnz	x18, 48330 <__gmpn_addlsh2_n@@Base+0x90>
   48370:	ldp	x4, x5, [x1, #16]
   48374:	extr	x12, x10, x9, #62
   48378:	extr	x13, x11, x10, #62
   4837c:	adcs	x14, x12, x4
   48380:	adcs	x15, x13, x5
   48384:	stp	x14, x15, [x0]
   48388:	lsr	x0, x11, #62
   4838c:	adc	x0, x0, xzr
   48390:	ret
   48394:	nop
   48398:	nop
   4839c:	nop

00000000000483a0 <__gmpn_sublsh2_n@@Base>:
   483a0:	lsr	x18, x3, #2
   483a4:	tbz	w3, #0, 4840c <__gmpn_sublsh2_n@@Base+0x6c>
   483a8:	ldr	x5, [x1]
   483ac:	tbnz	w3, #1, 483ec <__gmpn_sublsh2_n@@Base+0x4c>
   483b0:	ldr	x11, [x2]
   483b4:	cbz	x18, 483d4 <__gmpn_sublsh2_n@@Base+0x34>
   483b8:	ldp	x8, x9, [x2, #8]
   483bc:	lsl	x13, x11, #2
   483c0:	subs	x15, x5, x13
   483c4:	str	x15, [x0], #8
   483c8:	sub	x1, x1, #0x18
   483cc:	sub	x2, x2, #0x8
   483d0:	b	4844c <__gmpn_sublsh2_n@@Base+0xac>
   483d4:	lsl	x13, x11, #2
   483d8:	subs	x15, x5, x13
   483dc:	str	x15, [x0]
   483e0:	lsr	x0, x11, #62
   483e4:	cinc	x0, x0, cc  // cc = lo, ul, last
   483e8:	ret
   483ec:	ldr	x9, [x2]
   483f0:	ldp	x10, x11, [x2, #8]!
   483f4:	lsl	x13, x9, #2
   483f8:	subs	x17, x5, x13
   483fc:	str	x17, [x0], #8
   48400:	sub	x1, x1, #0x8
   48404:	cbz	x18, 48470 <__gmpn_sublsh2_n@@Base+0xd0>
   48408:	b	48430 <__gmpn_sublsh2_n@@Base+0x90>
   4840c:	tbnz	w3, #1, 48420 <__gmpn_sublsh2_n@@Base+0x80>
   48410:	negs	x11, xzr
   48414:	ldp	x8, x9, [x2], #-16
   48418:	sub	x1, x1, #0x20
   4841c:	b	4844c <__gmpn_sublsh2_n@@Base+0xac>
   48420:	negs	x9, xzr
   48424:	ldp	x10, x11, [x2]
   48428:	sub	x1, x1, #0x10
   4842c:	cbz	x18, 48470 <__gmpn_sublsh2_n@@Base+0xd0>
   48430:	ldp	x4, x5, [x1, #16]
   48434:	extr	x12, x10, x9, #62
   48438:	ldp	x8, x9, [x2, #16]
   4843c:	extr	x13, x11, x10, #62
   48440:	sbcs	x14, x4, x12
   48444:	sbcs	x15, x5, x13
   48448:	stp	x14, x15, [x0], #16
   4844c:	ldp	x4, x5, [x1, #32]!
   48450:	extr	x12, x8, x11, #62
   48454:	ldp	x10, x11, [x2, #32]!
   48458:	extr	x13, x9, x8, #62
   4845c:	sbcs	x16, x4, x12
   48460:	sbcs	x17, x5, x13
   48464:	stp	x16, x17, [x0], #16
   48468:	sub	x18, x18, #0x1
   4846c:	cbnz	x18, 48430 <__gmpn_sublsh2_n@@Base+0x90>
   48470:	ldp	x4, x5, [x1, #16]
   48474:	extr	x12, x10, x9, #62
   48478:	extr	x13, x11, x10, #62
   4847c:	sbcs	x14, x4, x12
   48480:	sbcs	x15, x5, x13
   48484:	stp	x14, x15, [x0]
   48488:	lsr	x0, x11, #62
   4848c:	cinc	x0, x0, cc  // cc = lo, ul, last
   48490:	ret
   48494:	nop
   48498:	nop
   4849c:	nop

00000000000484a0 <__gmpn_rsblsh2_n@@Base>:
   484a0:	lsr	x18, x3, #2
   484a4:	tbz	w3, #0, 4850c <__gmpn_rsblsh2_n@@Base+0x6c>
   484a8:	ldr	x5, [x1]
   484ac:	tbnz	w3, #1, 484ec <__gmpn_rsblsh2_n@@Base+0x4c>
   484b0:	ldr	x11, [x2]
   484b4:	cbz	x18, 484d4 <__gmpn_rsblsh2_n@@Base+0x34>
   484b8:	ldp	x8, x9, [x2, #8]
   484bc:	lsl	x13, x11, #2
   484c0:	subs	x15, x13, x5
   484c4:	str	x15, [x0], #8
   484c8:	sub	x1, x1, #0x18
   484cc:	sub	x2, x2, #0x8
   484d0:	b	4854c <__gmpn_rsblsh2_n@@Base+0xac>
   484d4:	lsl	x13, x11, #2
   484d8:	subs	x15, x13, x5
   484dc:	str	x15, [x0]
   484e0:	lsr	x0, x11, #62
   484e4:	sbc	x0, x0, xzr
   484e8:	ret
   484ec:	ldr	x9, [x2]
   484f0:	ldp	x10, x11, [x2, #8]!
   484f4:	lsl	x13, x9, #2
   484f8:	subs	x17, x13, x5
   484fc:	str	x17, [x0], #8
   48500:	sub	x1, x1, #0x8
   48504:	cbz	x18, 48570 <__gmpn_rsblsh2_n@@Base+0xd0>
   48508:	b	48530 <__gmpn_rsblsh2_n@@Base+0x90>
   4850c:	tbnz	w3, #1, 48520 <__gmpn_rsblsh2_n@@Base+0x80>
   48510:	negs	x11, xzr
   48514:	ldp	x8, x9, [x2], #-16
   48518:	sub	x1, x1, #0x20
   4851c:	b	4854c <__gmpn_rsblsh2_n@@Base+0xac>
   48520:	negs	x9, xzr
   48524:	ldp	x10, x11, [x2]
   48528:	sub	x1, x1, #0x10
   4852c:	cbz	x18, 48570 <__gmpn_rsblsh2_n@@Base+0xd0>
   48530:	ldp	x4, x5, [x1, #16]
   48534:	extr	x12, x10, x9, #62
   48538:	ldp	x8, x9, [x2, #16]
   4853c:	extr	x13, x11, x10, #62
   48540:	sbcs	x14, x12, x4
   48544:	sbcs	x15, x13, x5
   48548:	stp	x14, x15, [x0], #16
   4854c:	ldp	x4, x5, [x1, #32]!
   48550:	extr	x12, x8, x11, #62
   48554:	ldp	x10, x11, [x2, #32]!
   48558:	extr	x13, x9, x8, #62
   4855c:	sbcs	x16, x12, x4
   48560:	sbcs	x17, x13, x5
   48564:	stp	x16, x17, [x0], #16
   48568:	sub	x18, x18, #0x1
   4856c:	cbnz	x18, 48530 <__gmpn_rsblsh2_n@@Base+0x90>
   48570:	ldp	x4, x5, [x1, #16]
   48574:	extr	x12, x10, x9, #62
   48578:	extr	x13, x11, x10, #62
   4857c:	sbcs	x14, x12, x4
   48580:	sbcs	x15, x13, x5
   48584:	stp	x14, x15, [x0]
   48588:	lsr	x0, x11, #62
   4858c:	sbc	x0, x0, xzr
   48590:	ret

0000000000048594 <__gmpn_add_n_sub_n@@Base>:
   48594:	stp	x29, x30, [sp, #-96]!
   48598:	stp	x28, x27, [sp, #16]
   4859c:	stp	x26, x25, [sp, #32]
   485a0:	stp	x24, x23, [sp, #48]
   485a4:	stp	x22, x21, [sp, #64]
   485a8:	stp	x20, x19, [sp, #80]
   485ac:	mov	x29, sp
   485b0:	sub	sp, sp, #0x560
   485b4:	mov	x19, x4
   485b8:	mov	x20, x3
   485bc:	mov	x21, x2
   485c0:	mov	x22, x0
   485c4:	cmp	x0, x2
   485c8:	mov	x23, x1
   485cc:	b.eq	48658 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   485d0:	cmp	x22, x20
   485d4:	b.eq	48658 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   485d8:	cmp	x19, #0x1
   485dc:	b.lt	48778 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   485e0:	mov	x27, xzr
   485e4:	mov	x25, xzr
   485e8:	mov	x24, xzr
   485ec:	mov	x8, x19
   485f0:	subs	x28, x8, #0xaa
   485f4:	mov	w9, #0xaa                  	// #170
   485f8:	csel	x26, x8, x9, lt  // lt = tstop
   485fc:	mov	x0, x22
   48600:	mov	x1, x21
   48604:	mov	x2, x20
   48608:	mov	x3, x26
   4860c:	mov	x4, x24
   48610:	bl	d060 <__gmpn_add_nc@plt>
   48614:	mov	x24, x0
   48618:	mov	x0, x23
   4861c:	mov	x1, x21
   48620:	mov	x2, x20
   48624:	mov	x3, x26
   48628:	mov	x4, x25
   4862c:	bl	c8f0 <__gmpn_sub_nc@plt>
   48630:	add	x27, x27, #0xaa
   48634:	mov	x25, x0
   48638:	add	x23, x23, #0x550
   4863c:	add	x20, x20, #0x550
   48640:	add	x21, x21, #0x550
   48644:	cmp	x27, x19
   48648:	add	x22, x22, #0x550
   4864c:	mov	x8, x28
   48650:	b.lt	485f0 <__gmpn_add_n_sub_n@@Base+0x5c>  // b.tstop
   48654:	b	48780 <__gmpn_add_n_sub_n@@Base+0x1ec>
   48658:	cmp	x23, x21
   4865c:	b.eq	486e8 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   48660:	cmp	x23, x20
   48664:	b.eq	486e8 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   48668:	cmp	x19, #0x1
   4866c:	b.lt	48778 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   48670:	mov	x27, xzr
   48674:	mov	x25, xzr
   48678:	mov	x24, xzr
   4867c:	mov	x8, x19
   48680:	subs	x28, x8, #0xaa
   48684:	mov	w9, #0xaa                  	// #170
   48688:	csel	x26, x8, x9, lt  // lt = tstop
   4868c:	mov	x0, x23
   48690:	mov	x1, x21
   48694:	mov	x2, x20
   48698:	mov	x3, x26
   4869c:	mov	x4, x25
   486a0:	bl	c8f0 <__gmpn_sub_nc@plt>
   486a4:	mov	x25, x0
   486a8:	mov	x0, x22
   486ac:	mov	x1, x21
   486b0:	mov	x2, x20
   486b4:	mov	x3, x26
   486b8:	mov	x4, x24
   486bc:	bl	d060 <__gmpn_add_nc@plt>
   486c0:	add	x27, x27, #0xaa
   486c4:	mov	x24, x0
   486c8:	add	x22, x22, #0x550
   486cc:	add	x20, x20, #0x550
   486d0:	add	x21, x21, #0x550
   486d4:	cmp	x27, x19
   486d8:	add	x23, x23, #0x550
   486dc:	mov	x8, x28
   486e0:	b.lt	48680 <__gmpn_add_n_sub_n@@Base+0xec>  // b.tstop
   486e4:	b	48780 <__gmpn_add_n_sub_n@@Base+0x1ec>
   486e8:	cmp	x19, #0x1
   486ec:	b.lt	48778 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   486f0:	mov	x27, xzr
   486f4:	mov	x25, xzr
   486f8:	mov	x24, xzr
   486fc:	mov	x8, x19
   48700:	subs	x28, x8, #0xaa
   48704:	mov	w9, #0xaa                  	// #170
   48708:	csel	x26, x8, x9, lt  // lt = tstop
   4870c:	add	x0, sp, #0x8
   48710:	mov	x1, x21
   48714:	mov	x2, x20
   48718:	mov	x3, x26
   4871c:	mov	x4, x24
   48720:	bl	d060 <__gmpn_add_nc@plt>
   48724:	mov	x24, x0
   48728:	mov	x0, x23
   4872c:	mov	x1, x21
   48730:	mov	x2, x20
   48734:	mov	x3, x26
   48738:	mov	x4, x25
   4873c:	bl	c8f0 <__gmpn_sub_nc@plt>
   48740:	mov	x25, x0
   48744:	add	x1, sp, #0x8
   48748:	mov	x0, x22
   4874c:	mov	x2, x26
   48750:	bl	cc10 <__gmpn_copyi@plt>
   48754:	add	x27, x27, #0xaa
   48758:	add	x22, x22, #0x550
   4875c:	add	x23, x23, #0x550
   48760:	add	x20, x20, #0x550
   48764:	cmp	x27, x19
   48768:	add	x21, x21, #0x550
   4876c:	mov	x8, x28
   48770:	b.lt	48700 <__gmpn_add_n_sub_n@@Base+0x16c>  // b.tstop
   48774:	b	48780 <__gmpn_add_n_sub_n@@Base+0x1ec>
   48778:	mov	x24, xzr
   4877c:	mov	x25, xzr
   48780:	add	x0, x25, x24, lsl #1
   48784:	add	sp, sp, #0x560
   48788:	ldp	x20, x19, [sp, #80]
   4878c:	ldp	x22, x21, [sp, #64]
   48790:	ldp	x24, x23, [sp, #48]
   48794:	ldp	x26, x25, [sp, #32]
   48798:	ldp	x28, x27, [sp, #16]
   4879c:	ldp	x29, x30, [sp], #96
   487a0:	ret

00000000000487a4 <__gmp_asprintf@@Base>:
   487a4:	sub	sp, sp, #0x100
   487a8:	stp	x29, x30, [sp, #240]
   487ac:	add	x29, sp, #0xf0
   487b0:	mov	x8, #0xffffffffffffffd0    	// #-48
   487b4:	mov	x9, sp
   487b8:	sub	x10, x29, #0x70
   487bc:	movk	x8, #0xff80, lsl #32
   487c0:	add	x11, x29, #0x10
   487c4:	add	x9, x9, #0x80
   487c8:	add	x10, x10, #0x30
   487cc:	stp	x9, x8, [x29, #-16]
   487d0:	stp	x11, x10, [x29, #-32]
   487d4:	stp	x2, x3, [x29, #-112]
   487d8:	stp	x4, x5, [x29, #-96]
   487dc:	stp	x6, x7, [x29, #-80]
   487e0:	stp	q1, q2, [sp, #16]
   487e4:	str	q0, [sp]
   487e8:	ldp	q0, q1, [x29, #-32]
   487ec:	sub	x2, x29, #0x40
   487f0:	stp	q3, q4, [sp, #48]
   487f4:	stp	q5, q6, [sp, #80]
   487f8:	str	q7, [sp, #112]
   487fc:	stp	q0, q1, [x29, #-64]
   48800:	bl	c670 <__gmp_vasprintf@plt>
   48804:	ldp	x29, x30, [sp, #240]
   48808:	add	sp, sp, #0x100
   4880c:	ret

0000000000048810 <__gmp_asprintf_memory@@Base>:
   48810:	stp	x29, x30, [sp, #-48]!
   48814:	str	x21, [sp, #16]
   48818:	stp	x20, x19, [sp, #32]
   4881c:	ldp	x9, x8, [x0, #16]
   48820:	mov	x19, x0
   48824:	mov	x20, x2
   48828:	mov	x21, x1
   4882c:	add	x9, x9, x2
   48830:	cmp	x8, x9
   48834:	mov	x29, sp
   48838:	b.hi	48860 <__gmp_asprintf_memory@@Base+0x50>  // b.pmore
   4883c:	lsl	x2, x9, #1
   48840:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   48844:	ldr	x9, [x9, #3792]
   48848:	str	x2, [x19, #24]
   4884c:	ldr	x0, [x19, #8]
   48850:	mov	x1, x8
   48854:	ldr	x9, [x9]
   48858:	blr	x9
   4885c:	str	x0, [x19, #8]
   48860:	ldp	x8, x9, [x19, #8]
   48864:	mov	x1, x21
   48868:	mov	x2, x20
   4886c:	add	x0, x8, x9
   48870:	bl	bff0 <memcpy@plt>
   48874:	ldr	x8, [x19, #16]
   48878:	mov	w0, w20
   4887c:	ldr	x21, [sp, #16]
   48880:	add	x8, x8, x20
   48884:	str	x8, [x19, #16]
   48888:	ldp	x20, x19, [sp, #32]
   4888c:	ldp	x29, x30, [sp], #48
   48890:	ret

0000000000048894 <__gmp_asprintf_reps@@Base>:
   48894:	stp	x29, x30, [sp, #-48]!
   48898:	stp	x22, x21, [sp, #16]
   4889c:	stp	x20, x19, [sp, #32]
   488a0:	ldp	x9, x8, [x0, #16]
   488a4:	mov	w20, w2
   488a8:	sxtw	x21, w20
   488ac:	mov	x19, x0
   488b0:	add	x9, x9, x21
   488b4:	cmp	x8, x9
   488b8:	mov	w22, w1
   488bc:	mov	x29, sp
   488c0:	b.hi	488e8 <__gmp_asprintf_reps@@Base+0x54>  // b.pmore
   488c4:	lsl	x2, x9, #1
   488c8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   488cc:	ldr	x9, [x9, #3792]
   488d0:	str	x2, [x19, #24]
   488d4:	ldr	x0, [x19, #8]
   488d8:	mov	x1, x8
   488dc:	ldr	x9, [x9]
   488e0:	blr	x9
   488e4:	str	x0, [x19, #8]
   488e8:	ldp	x8, x9, [x19, #8]
   488ec:	mov	w1, w22
   488f0:	mov	x2, x21
   488f4:	add	x0, x8, x9
   488f8:	bl	c780 <memset@plt>
   488fc:	ldr	x8, [x19, #16]
   48900:	mov	w0, w20
   48904:	add	x8, x8, x21
   48908:	str	x8, [x19, #16]
   4890c:	ldp	x20, x19, [sp, #32]
   48910:	ldp	x22, x21, [sp, #16]
   48914:	ldp	x29, x30, [sp], #48
   48918:	ret

000000000004891c <__gmp_asprintf_final@@Base>:
   4891c:	stp	x29, x30, [sp, #-32]!
   48920:	str	x19, [sp, #16]
   48924:	mov	x19, x0
   48928:	ldr	x0, [x0, #8]
   4892c:	ldr	x8, [x19, #16]
   48930:	mov	x29, sp
   48934:	strb	wzr, [x0, x8]
   48938:	ldp	x8, x1, [x19, #16]
   4893c:	add	x2, x8, #0x1
   48940:	cmp	x1, x2
   48944:	b.eq	48958 <__gmp_asprintf_final@@Base+0x3c>  // b.none
   48948:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4894c:	ldr	x8, [x8, #3792]
   48950:	ldr	x8, [x8]
   48954:	blr	x8
   48958:	ldr	x8, [x19]
   4895c:	ldr	x19, [sp, #16]
   48960:	str	x0, [x8]
   48964:	mov	w0, wzr
   48968:	ldp	x29, x30, [sp], #32
   4896c:	ret

0000000000048970 <__gmp_doprnt@@Base>:
   48970:	sub	sp, sp, #0x170
   48974:	stp	x29, x30, [sp, #272]
   48978:	stp	x28, x27, [sp, #288]
   4897c:	stp	x26, x25, [sp, #304]
   48980:	stp	x24, x23, [sp, #320]
   48984:	stp	x22, x21, [sp, #336]
   48988:	stp	x20, x19, [sp, #352]
   4898c:	stp	x1, x0, [sp, #24]
   48990:	ldp	q1, q0, [x3]
   48994:	add	x29, sp, #0x110
   48998:	mov	x0, x2
   4899c:	mov	x23, x2
   489a0:	stp	q1, q0, [x29, #-48]
   489a4:	bl	c090 <strlen@plt>
   489a8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   489ac:	ldr	x8, [x8, #3840]
   489b0:	add	x19, x0, #0x1
   489b4:	mov	x0, x19
   489b8:	ldr	x8, [x8]
   489bc:	blr	x8
   489c0:	mov	x1, x23
   489c4:	mov	x2, x19
   489c8:	mov	x20, x0
   489cc:	str	x19, [sp, #16]
   489d0:	bl	bff0 <memcpy@plt>
   489d4:	ldp	q0, q1, [x29, #-48]
   489d8:	mov	w1, #0x25                  	// #37
   489dc:	mov	x0, x20
   489e0:	stp	q0, q1, [x29, #-112]
   489e4:	bl	cf70 <strchr@plt>
   489e8:	str	x20, [sp, #8]
   489ec:	cbz	x0, 49224 <__gmp_doprnt@@Base+0x8b4>
   489f0:	add	x8, sp, #0x68
   489f4:	adrp	x25, 57000 <__gmp_jacobi_table@@Base+0x6ef1>
   489f8:	mov	x27, x0
   489fc:	add	x19, x8, #0x30
   48a00:	add	x8, x8, #0x1c
   48a04:	add	x25, x25, #0xf08
   48a08:	stp	xzr, x20, [sp, #40]
   48a0c:	str	x8, [sp, #56]
   48a10:	b	48a3c <__gmp_doprnt@@Base+0xcc>
   48a14:	ldur	w8, [x29, #-20]
   48a18:	tbnz	w8, #31, 48dd0 <__gmp_doprnt@@Base+0x460>
   48a1c:	ldur	x8, [x29, #-48]
   48a20:	add	x8, x8, #0x8
   48a24:	stur	x8, [x29, #-48]
   48a28:	mov	w1, #0x25                  	// #37
   48a2c:	mov	x0, x26
   48a30:	bl	cf70 <strchr@plt>
   48a34:	mov	x27, x0
   48a38:	cbz	x0, 49228 <__gmp_doprnt@@Base+0x8b8>
   48a3c:	mov	w8, #0xa                   	// #10
   48a40:	str	x8, [sp, #104]
   48a44:	adrp	x8, 58000 <__gmp_limbroots_table@@Base+0x338>
   48a48:	add	x8, x8, #0xd
   48a4c:	str	x8, [sp, #112]
   48a50:	mov	w8, #0x20                  	// #32
   48a54:	ldp	q0, q1, [x29, #-48]
   48a58:	strb	w8, [sp, #124]
   48a5c:	mov	x8, #0x2                   	// #2
   48a60:	movk	x8, #0x6, lsl #32
   48a64:	str	x8, [sp, #128]
   48a68:	mov	w8, #0x2                   	// #2
   48a6c:	mov	w28, wzr
   48a70:	mov	w23, wzr
   48a74:	add	x26, x27, #0x1
   48a78:	str	x8, [sp, #136]
   48a7c:	mov	w8, #0x1                   	// #1
   48a80:	mov	x20, x19
   48a84:	stp	q0, q1, [x29, #-80]
   48a88:	str	wzr, [sp, #120]
   48a8c:	str	w8, [sp, #144]
   48a90:	strb	wzr, [sp, #148]
   48a94:	str	wzr, [sp, #152]
   48a98:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48a9c:	mov	w23, w21
   48aa0:	mov	x8, x26
   48aa4:	ldrb	w21, [x26], #1
   48aa8:	sub	w9, w21, #0x20
   48aac:	cmp	w9, #0x5a
   48ab0:	b.hi	48a28 <__gmp_doprnt@@Base+0xb8>  // b.pmore
   48ab4:	adr	x10, 48a28 <__gmp_doprnt@@Base+0xb8>
   48ab8:	ldrb	w11, [x25, x9]
   48abc:	add	x10, x10, x11, lsl #2
   48ac0:	br	x10
   48ac4:	mov	w22, wzr
   48ac8:	mov	x24, x26
   48acc:	mov	w8, #0xa                   	// #10
   48ad0:	madd	w8, w22, w8, w21
   48ad4:	ldrb	w21, [x24]
   48ad8:	mov	x26, x24
   48adc:	sub	w22, w8, #0x30
   48ae0:	tbnz	w21, #7, 48af8 <__gmp_doprnt@@Base+0x188>
   48ae4:	add	x24, x26, #0x1
   48ae8:	bl	cca0 <__ctype_b_loc@plt>
   48aec:	ldr	x8, [x0]
   48af0:	ldrh	w8, [x8, x21, lsl #1]
   48af4:	tbnz	w8, #11, 48acc <__gmp_doprnt@@Base+0x15c>
   48af8:	str	w22, [x20]
   48afc:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b00:	strb	w21, [sp, #148]
   48b04:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b08:	mov	w8, #0x3                   	// #3
   48b0c:	str	w8, [sp, #136]
   48b10:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b14:	ldursw	x8, [x29, #-24]
   48b18:	tbz	w8, #31, 48b2c <__gmp_doprnt@@Base+0x1bc>
   48b1c:	add	w9, w8, #0x8
   48b20:	cmp	w9, #0x0
   48b24:	stur	w9, [x29, #-24]
   48b28:	b.le	48be4 <__gmp_doprnt@@Base+0x274>
   48b2c:	ldur	x8, [x29, #-48]
   48b30:	add	x9, x8, #0x8
   48b34:	stur	x9, [x29, #-48]
   48b38:	ldr	w8, [x8]
   48b3c:	cmp	x20, x19
   48b40:	b.eq	48bac <__gmp_doprnt@@Base+0x23c>  // b.none
   48b44:	bic	w8, w8, w8, asr #31
   48b48:	str	w8, [sp, #132]
   48b4c:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b50:	mov	w8, #0x1                   	// #1
   48b54:	str	w8, [sp, #128]
   48b58:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b5c:	ldr	x20, [sp, #56]
   48b60:	mov	w8, #0xffffffff            	// #-1
   48b64:	str	w8, [sp, #132]
   48b68:	mov	w28, #0x1                   	// #1
   48b6c:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b70:	cmp	x20, x19
   48b74:	b.eq	48bc4 <__gmp_doprnt@@Base+0x254>  // b.none
   48b78:	str	wzr, [x20]
   48b7c:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b80:	mov	w23, #0x6c                  	// #108
   48b84:	strb	w23, [x8]
   48b88:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48b8c:	cmp	w23, #0x68
   48b90:	mov	w23, #0x48                  	// #72
   48b94:	b.eq	48aa0 <__gmp_doprnt@@Base+0x130>  // b.none
   48b98:	b	48a9c <__gmp_doprnt@@Base+0x12c>
   48b9c:	cmp	w23, #0x6c
   48ba0:	mov	w23, #0x4c                  	// #76
   48ba4:	b.eq	48aa0 <__gmp_doprnt@@Base+0x130>  // b.none
   48ba8:	b	48a9c <__gmp_doprnt@@Base+0x12c>
   48bac:	tbz	w8, #31, 48bbc <__gmp_doprnt@@Base+0x24c>
   48bb0:	mov	w9, #0x1                   	// #1
   48bb4:	neg	w8, w8
   48bb8:	str	w9, [sp, #128]
   48bbc:	str	w8, [sp, #152]
   48bc0:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48bc4:	ldr	w8, [sp, #128]
   48bc8:	mov	w9, #0x30                  	// #48
   48bcc:	strb	w9, [sp, #124]
   48bd0:	cmp	w8, #0x2
   48bd4:	b.ne	48aa0 <__gmp_doprnt@@Base+0x130>  // b.any
   48bd8:	mov	w8, #0x3                   	// #3
   48bdc:	str	w8, [sp, #128]
   48be0:	b	48aa0 <__gmp_doprnt@@Base+0x130>
   48be4:	ldur	x9, [x29, #-40]
   48be8:	add	x8, x9, x8
   48bec:	b	48b38 <__gmp_doprnt@@Base+0x1c8>
   48bf0:	adrp	x8, 58000 <__gmp_limbroots_table@@Base+0x338>
   48bf4:	add	x8, x8, #0x1d
   48bf8:	mov	w9, #0xfffffff0            	// #-16
   48bfc:	b	48c54 <__gmp_doprnt@@Base+0x2e4>
   48c00:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   48c04:	mov	w8, #0xfffffff6            	// #-10
   48c08:	add	x9, x9, #0x24
   48c0c:	str	w8, [sp, #104]
   48c10:	str	x9, [sp, #112]
   48c14:	mov	w8, #0x2                   	// #2
   48c18:	b	48c80 <__gmp_doprnt@@Base+0x310>
   48c1c:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   48c20:	mov	w8, #0xfffffff6            	// #-10
   48c24:	add	x9, x9, #0x24
   48c28:	str	w8, [sp, #104]
   48c2c:	str	x9, [sp, #112]
   48c30:	mov	w8, #0x3                   	// #3
   48c34:	str	w8, [sp, #108]
   48c38:	str	wzr, [sp, #144]
   48c3c:	b	48c84 <__gmp_doprnt@@Base+0x314>
   48c40:	mov	w8, #0xfffffff0            	// #-16
   48c44:	b	48d7c <__gmp_doprnt@@Base+0x40c>
   48c48:	adrp	x8, 58000 <__gmp_limbroots_table@@Base+0x338>
   48c4c:	add	x8, x8, #0x16
   48c50:	mov	w9, #0x10                  	// #16
   48c54:	str	x8, [sp, #112]
   48c58:	mov	w10, #0x2                   	// #2
   48c5c:	mov	w8, #0x1                   	// #1
   48c60:	stp	w9, w10, [sp, #104]
   48c64:	str	w8, [sp, #120]
   48c68:	cbnz	w28, 48c74 <__gmp_doprnt@@Base+0x304>
   48c6c:	mov	w9, #0xffffffff            	// #-1
   48c70:	str	w9, [sp, #132]
   48c74:	str	w8, [sp, #136]
   48c78:	b	48c98 <__gmp_doprnt@@Base+0x328>
   48c7c:	mov	w8, #0x1                   	// #1
   48c80:	str	w8, [sp, #108]
   48c84:	ldr	w8, [sp, #136]
   48c88:	cmp	w8, #0x3
   48c8c:	b.ne	48ca0 <__gmp_doprnt@@Base+0x330>  // b.any
   48c90:	mov	w8, #0x1                   	// #1
   48c94:	str	w8, [sp, #140]
   48c98:	mov	w8, #0x1                   	// #1
   48c9c:	str	w8, [sp, #144]
   48ca0:	and	w8, w23, #0xff
   48ca4:	cmp	w8, #0x4c
   48ca8:	b.eq	48cf4 <__gmp_doprnt@@Base+0x384>  // b.none
   48cac:	cmp	w8, #0x46
   48cb0:	b.ne	48a14 <__gmp_doprnt@@Base+0xa4>  // b.any
   48cb4:	ldr	x1, [sp, #48]
   48cb8:	cmp	x27, x1
   48cbc:	b.eq	48de4 <__gmp_doprnt@@Base+0x474>  // b.none
   48cc0:	strb	wzr, [x27]
   48cc4:	ldp	x0, x8, [sp, #24]
   48cc8:	ldp	q0, q1, [x29, #-112]
   48ccc:	add	x2, sp, #0x40
   48cd0:	ldr	x8, [x8]
   48cd4:	stp	q0, q1, [sp, #64]
   48cd8:	blr	x8
   48cdc:	ldr	x20, [sp, #40]
   48ce0:	cmn	w0, #0x1
   48ce4:	csel	w8, wzr, w0, eq  // eq = none
   48ce8:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   48cec:	add	w20, w8, w20
   48cf0:	b	48de8 <__gmp_doprnt@@Base+0x478>
   48cf4:	ldur	w8, [x29, #-20]
   48cf8:	tbz	w8, #31, 48d0c <__gmp_doprnt@@Base+0x39c>
   48cfc:	add	w8, w8, #0x10
   48d00:	cmp	w8, #0x1
   48d04:	stur	w8, [x29, #-20]
   48d08:	b.lt	48a28 <__gmp_doprnt@@Base+0xb8>  // b.tstop
   48d0c:	ldur	x8, [x29, #-48]
   48d10:	add	x8, x8, #0xf
   48d14:	and	x8, x8, #0xfffffffffffffff0
   48d18:	add	x8, x8, #0x10
   48d1c:	b	48a24 <__gmp_doprnt@@Base+0xb4>
   48d20:	ldr	x1, [sp, #48]
   48d24:	cmp	x27, x1
   48d28:	b.eq	48f0c <__gmp_doprnt@@Base+0x59c>  // b.none
   48d2c:	strb	wzr, [x27]
   48d30:	ldp	x0, x8, [sp, #24]
   48d34:	ldp	q0, q1, [x29, #-112]
   48d38:	add	x2, sp, #0x40
   48d3c:	ldr	x8, [x8]
   48d40:	stp	q0, q1, [sp, #64]
   48d44:	blr	x8
   48d48:	ldr	x20, [sp, #40]
   48d4c:	cmn	w0, #0x1
   48d50:	mov	w8, #0x18                  	// #24
   48d54:	csel	w9, wzr, w0, eq  // eq = none
   48d58:	csel	w8, w8, wzr, eq  // eq = none
   48d5c:	add	w20, w9, w20
   48d60:	b.eq	491f8 <__gmp_doprnt@@Base+0x888>  // b.none
   48d64:	ldursw	x8, [x29, #-24]
   48d68:	tbz	w8, #31, 48f28 <__gmp_doprnt@@Base+0x5b8>
   48d6c:	b	48f18 <__gmp_doprnt@@Base+0x5a8>
   48d70:	mov	w8, #0x8                   	// #8
   48d74:	b	48d7c <__gmp_doprnt@@Base+0x40c>
   48d78:	mov	w8, #0x10                  	// #16
   48d7c:	str	w8, [sp, #104]
   48d80:	cbnz	w28, 48d8c <__gmp_doprnt@@Base+0x41c>
   48d84:	mov	w8, #0xffffffff            	// #-1
   48d88:	str	w8, [sp, #132]
   48d8c:	and	w8, w23, #0xff
   48d90:	sub	w8, w8, #0x4c
   48d94:	cmp	w8, #0x2e
   48d98:	b.hi	48db4 <__gmp_doprnt@@Base+0x444>  // b.pmore
   48d9c:	adrp	x9, 57000 <__gmp_jacobi_table@@Base+0x6ef1>
   48da0:	add	x9, x9, #0xfde
   48da4:	adr	x10, 48db4 <__gmp_doprnt@@Base+0x444>
   48da8:	ldrb	w11, [x9, x8]
   48dac:	add	x10, x10, x11, lsl #2
   48db0:	br	x10
   48db4:	ldur	w8, [x29, #-24]
   48db8:	tbz	w8, #31, 48a1c <__gmp_doprnt@@Base+0xac>
   48dbc:	add	w8, w8, #0x8
   48dc0:	cmp	w8, #0x1
   48dc4:	stur	w8, [x29, #-24]
   48dc8:	b.ge	48a1c <__gmp_doprnt@@Base+0xac>  // b.tcont
   48dcc:	b	48a28 <__gmp_doprnt@@Base+0xb8>
   48dd0:	add	w8, w8, #0x10
   48dd4:	cmp	w8, #0x1
   48dd8:	stur	w8, [x29, #-20]
   48ddc:	b.ge	48a1c <__gmp_doprnt@@Base+0xac>  // b.tcont
   48de0:	b	48a28 <__gmp_doprnt@@Base+0xb8>
   48de4:	ldr	x20, [sp, #40]
   48de8:	mov	w0, #0x10000               	// #65536
   48dec:	bl	c560 <nl_langinfo@plt>
   48df0:	ldursw	x8, [x29, #-24]
   48df4:	mov	x3, x0
   48df8:	tbz	w8, #31, 48e0c <__gmp_doprnt@@Base+0x49c>
   48dfc:	add	w9, w8, #0x8
   48e00:	cmp	w9, #0x0
   48e04:	stur	w9, [x29, #-24]
   48e08:	b.le	48f74 <__gmp_doprnt@@Base+0x604>
   48e0c:	ldur	x8, [x29, #-48]
   48e10:	add	x9, x8, #0x8
   48e14:	stur	x9, [x29, #-48]
   48e18:	ldr	x4, [x8]
   48e1c:	ldp	x1, x0, [sp, #24]
   48e20:	add	x2, sp, #0x68
   48e24:	bl	d410 <__gmp_doprnt_mpf2@plt>
   48e28:	cmn	w0, #0x1
   48e2c:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   48e30:	ldp	q0, q1, [x29, #-48]
   48e34:	add	w20, w0, w20
   48e38:	b	49200 <__gmp_doprnt@@Base+0x890>
   48e3c:	ldr	x1, [sp, #48]
   48e40:	cmp	x27, x1
   48e44:	b.eq	48f80 <__gmp_doprnt@@Base+0x610>  // b.none
   48e48:	strb	wzr, [x27]
   48e4c:	ldp	x0, x8, [sp, #24]
   48e50:	ldp	q0, q1, [x29, #-112]
   48e54:	add	x2, sp, #0x40
   48e58:	ldr	x8, [x8]
   48e5c:	stp	q0, q1, [sp, #64]
   48e60:	blr	x8
   48e64:	ldr	x22, [sp, #40]
   48e68:	cmn	w0, #0x1
   48e6c:	mov	w8, #0x18                  	// #24
   48e70:	csel	w9, wzr, w0, eq  // eq = none
   48e74:	csel	w8, w8, wzr, eq  // eq = none
   48e78:	add	w22, w9, w22
   48e7c:	b.eq	49104 <__gmp_doprnt@@Base+0x794>  // b.none
   48e80:	ldursw	x8, [x29, #-24]
   48e84:	tbz	w8, #31, 48f9c <__gmp_doprnt@@Base+0x62c>
   48e88:	b	48f8c <__gmp_doprnt@@Base+0x61c>
   48e8c:	ldr	x1, [sp, #48]
   48e90:	cmp	x27, x1
   48e94:	b.eq	4903c <__gmp_doprnt@@Base+0x6cc>  // b.none
   48e98:	strb	wzr, [x27]
   48e9c:	ldp	x0, x8, [sp, #24]
   48ea0:	ldp	q0, q1, [x29, #-112]
   48ea4:	add	x2, sp, #0x40
   48ea8:	ldr	x8, [x8]
   48eac:	stp	q0, q1, [sp, #64]
   48eb0:	blr	x8
   48eb4:	ldr	x22, [sp, #40]
   48eb8:	cmn	w0, #0x1
   48ebc:	csel	w8, wzr, w0, eq  // eq = none
   48ec0:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   48ec4:	add	w22, w8, w22
   48ec8:	b	49040 <__gmp_doprnt@@Base+0x6d0>
   48ecc:	ldr	x1, [sp, #48]
   48ed0:	cmp	x27, x1
   48ed4:	b.eq	49078 <__gmp_doprnt@@Base+0x708>  // b.none
   48ed8:	strb	wzr, [x27]
   48edc:	ldp	x0, x8, [sp, #24]
   48ee0:	ldp	q0, q1, [x29, #-112]
   48ee4:	add	x2, sp, #0x40
   48ee8:	ldr	x8, [x8]
   48eec:	stp	q0, q1, [sp, #64]
   48ef0:	blr	x8
   48ef4:	ldr	x22, [sp, #40]
   48ef8:	cmn	w0, #0x1
   48efc:	csel	w8, wzr, w0, eq  // eq = none
   48f00:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   48f04:	add	w22, w8, w22
   48f08:	b	4907c <__gmp_doprnt@@Base+0x70c>
   48f0c:	ldr	x20, [sp, #40]
   48f10:	ldursw	x8, [x29, #-24]
   48f14:	tbz	w8, #31, 48f28 <__gmp_doprnt@@Base+0x5b8>
   48f18:	add	w9, w8, #0x8
   48f1c:	cmp	w9, #0x0
   48f20:	stur	w9, [x29, #-24]
   48f24:	b.le	49114 <__gmp_doprnt@@Base+0x7a4>
   48f28:	ldur	x9, [x29, #-48]
   48f2c:	add	x8, x9, #0x8
   48f30:	stur	x8, [x29, #-48]
   48f34:	and	w8, w23, #0xff
   48f38:	cmp	w8, #0x7a
   48f3c:	mov	w8, wzr
   48f40:	b.hi	491f8 <__gmp_doprnt@@Base+0x888>  // b.pmore
   48f44:	ldr	x0, [x9]
   48f48:	adrp	x10, 57000 <__gmp_jacobi_table@@Base+0x6ef1>
   48f4c:	and	x9, x23, #0xff
   48f50:	add	x10, x10, #0xf63
   48f54:	adr	x11, 48f64 <__gmp_doprnt@@Base+0x5f4>
   48f58:	ldrb	w12, [x10, x9]
   48f5c:	add	x11, x11, x12, lsl #2
   48f60:	br	x11
   48f64:	sxtw	x9, w20
   48f68:	mov	w8, wzr
   48f6c:	str	x9, [x0]
   48f70:	b	491f8 <__gmp_doprnt@@Base+0x888>
   48f74:	ldur	x9, [x29, #-40]
   48f78:	add	x8, x9, x8
   48f7c:	b	48e18 <__gmp_doprnt@@Base+0x4a8>
   48f80:	ldr	x22, [sp, #40]
   48f84:	ldursw	x8, [x29, #-24]
   48f88:	tbz	w8, #31, 48f9c <__gmp_doprnt@@Base+0x62c>
   48f8c:	add	w9, w8, #0x8
   48f90:	cmp	w9, #0x0
   48f94:	stur	w9, [x29, #-24]
   48f98:	b.le	49138 <__gmp_doprnt@@Base+0x7c8>
   48f9c:	ldur	x8, [x29, #-48]
   48fa0:	add	x9, x8, #0x8
   48fa4:	stur	x9, [x29, #-48]
   48fa8:	ldr	x8, [x8]
   48fac:	ldursw	x9, [x29, #-24]
   48fb0:	str	x8, [sp, #72]
   48fb4:	tbz	w9, #31, 48fc8 <__gmp_doprnt@@Base+0x658>
   48fb8:	add	w10, w9, #0x8
   48fbc:	cmp	w10, #0x0
   48fc0:	stur	w10, [x29, #-24]
   48fc4:	b.le	49144 <__gmp_doprnt@@Base+0x7d4>
   48fc8:	ldur	x9, [x29, #-48]
   48fcc:	add	x10, x9, #0x8
   48fd0:	stur	x10, [x29, #-48]
   48fd4:	ldr	x10, [x9]
   48fd8:	mov	x11, #0xffffffff00000000    	// #-4294967296
   48fdc:	lsl	x9, x10, #32
   48fe0:	sxtw	x10, w10
   48fe4:	cmp	x9, x11
   48fe8:	cneg	x10, x10, le
   48fec:	sub	x11, x8, #0x8
   48ff0:	mov	x8, x10
   48ff4:	subs	x10, x10, #0x1
   48ff8:	b.lt	49004 <__gmp_doprnt@@Base+0x694>  // b.tstop
   48ffc:	ldr	x12, [x11, x8, lsl #3]
   49000:	cbz	x12, 48ff0 <__gmp_doprnt@@Base+0x680>
   49004:	ldr	w1, [sp, #104]
   49008:	mov	x11, #0xffffffff00000000    	// #-4294967296
   4900c:	neg	w10, w8
   49010:	cmp	x9, x11
   49014:	csel	x8, x8, x10, gt
   49018:	add	x2, sp, #0x40
   4901c:	mov	x0, xzr
   49020:	str	w8, [sp, #68]
   49024:	bl	c500 <__gmpz_get_str@plt>
   49028:	mov	x23, x0
   4902c:	mov	w8, #0x1d                  	// #29
   49030:	cmp	w8, #0x1d
   49034:	b.eq	490b4 <__gmp_doprnt@@Base+0x744>  // b.none
   49038:	b	492c0 <__gmp_doprnt@@Base+0x950>
   4903c:	ldr	x22, [sp, #40]
   49040:	ldursw	x8, [x29, #-24]
   49044:	ldr	w1, [sp, #104]
   49048:	tbz	w8, #31, 4905c <__gmp_doprnt@@Base+0x6ec>
   4904c:	add	w9, w8, #0x8
   49050:	cmp	w9, #0x0
   49054:	stur	w9, [x29, #-24]
   49058:	b.le	49120 <__gmp_doprnt@@Base+0x7b0>
   4905c:	ldur	x8, [x29, #-48]
   49060:	add	x9, x8, #0x8
   49064:	stur	x9, [x29, #-48]
   49068:	ldr	x2, [x8]
   4906c:	mov	x0, xzr
   49070:	bl	c280 <__gmpq_get_str@plt>
   49074:	b	490b0 <__gmp_doprnt@@Base+0x740>
   49078:	ldr	x22, [sp, #40]
   4907c:	ldursw	x8, [x29, #-24]
   49080:	ldr	w1, [sp, #104]
   49084:	tbz	w8, #31, 49098 <__gmp_doprnt@@Base+0x728>
   49088:	add	w9, w8, #0x8
   4908c:	cmp	w9, #0x0
   49090:	stur	w9, [x29, #-24]
   49094:	b.le	4912c <__gmp_doprnt@@Base+0x7bc>
   49098:	ldur	x8, [x29, #-48]
   4909c:	add	x9, x8, #0x8
   490a0:	stur	x9, [x29, #-48]
   490a4:	ldr	x2, [x8]
   490a8:	mov	x0, xzr
   490ac:	bl	c500 <__gmpz_get_str@plt>
   490b0:	mov	x23, x0
   490b4:	ldp	x1, x0, [sp, #24]
   490b8:	add	x2, sp, #0x68
   490bc:	mov	x3, x23
   490c0:	bl	cee0 <__gmp_doprnt_integer@plt>
   490c4:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   490c8:	ldr	x8, [x8, #4016]
   490cc:	mov	w20, w0
   490d0:	mov	x0, x23
   490d4:	ldr	x21, [x8]
   490d8:	bl	c090 <strlen@plt>
   490dc:	add	x1, x0, #0x1
   490e0:	mov	x0, x23
   490e4:	str	x23, [sp]
   490e8:	blr	x21
   490ec:	cmn	w20, #0x1
   490f0:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   490f4:	ldp	q0, q1, [x29, #-48]
   490f8:	add	w22, w20, w22
   490fc:	str	x22, [sp, #40]
   49100:	b	49204 <__gmp_doprnt@@Base+0x894>
   49104:	ldr	x23, [sp]
   49108:	cmp	w8, #0x1d
   4910c:	b.eq	490b4 <__gmp_doprnt@@Base+0x744>  // b.none
   49110:	b	492c0 <__gmp_doprnt@@Base+0x950>
   49114:	ldur	x9, [x29, #-40]
   49118:	add	x9, x9, x8
   4911c:	b	48f34 <__gmp_doprnt@@Base+0x5c4>
   49120:	ldur	x9, [x29, #-40]
   49124:	add	x8, x9, x8
   49128:	b	49068 <__gmp_doprnt@@Base+0x6f8>
   4912c:	ldur	x9, [x29, #-40]
   49130:	add	x8, x9, x8
   49134:	b	490a4 <__gmp_doprnt@@Base+0x734>
   49138:	ldur	x9, [x29, #-40]
   4913c:	add	x8, x9, x8
   49140:	b	48fa8 <__gmp_doprnt@@Base+0x638>
   49144:	ldur	x10, [x29, #-40]
   49148:	add	x9, x10, x9
   4914c:	b	48fd4 <__gmp_doprnt@@Base+0x664>
   49150:	mov	w8, wzr
   49154:	str	w20, [x0]
   49158:	b	491f8 <__gmp_doprnt@@Base+0x888>
   4915c:	sxtw	x1, w20
   49160:	bl	c7b0 <__gmpf_set_si@plt>
   49164:	b	491e8 <__gmp_doprnt@@Base+0x878>
   49168:	mov	w8, wzr
   4916c:	strb	w20, [x0]
   49170:	b	491f8 <__gmp_doprnt@@Base+0x888>
   49174:	ldursw	x8, [x29, #-24]
   49178:	tbz	w8, #31, 4918c <__gmp_doprnt@@Base+0x81c>
   4917c:	add	w9, w8, #0x8
   49180:	cmp	w9, #0x0
   49184:	stur	w9, [x29, #-24]
   49188:	b.le	49210 <__gmp_doprnt@@Base+0x8a0>
   4918c:	ldur	x8, [x29, #-48]
   49190:	add	x9, x8, #0x8
   49194:	stur	x9, [x29, #-48]
   49198:	ldr	x8, [x8]
   4919c:	cbz	x8, 491f8 <__gmp_doprnt@@Base+0x888>
   491a0:	cmp	x8, #0x0
   491a4:	cneg	x8, x8, mi  // mi = first
   491a8:	sxtw	x9, w20
   491ac:	cmp	x8, #0x1
   491b0:	str	x9, [x0]
   491b4:	b.eq	491e8 <__gmp_doprnt@@Base+0x878>  // b.none
   491b8:	lsl	x8, x8, #3
   491bc:	add	x0, x0, #0x8
   491c0:	sub	x2, x8, #0x8
   491c4:	mov	w1, wzr
   491c8:	bl	c780 <memset@plt>
   491cc:	b	491e8 <__gmp_doprnt@@Base+0x878>
   491d0:	sxtw	x1, w20
   491d4:	mov	w2, #0x1                   	// #1
   491d8:	bl	cd30 <__gmpq_set_si@plt>
   491dc:	b	491e8 <__gmp_doprnt@@Base+0x878>
   491e0:	sxtw	x1, w20
   491e4:	bl	d450 <__gmpz_set_si@plt>
   491e8:	mov	w8, wzr
   491ec:	b	491f8 <__gmp_doprnt@@Base+0x888>
   491f0:	mov	w8, wzr
   491f4:	strh	w20, [x0]
   491f8:	cbnz	w8, 492c0 <__gmp_doprnt@@Base+0x950>
   491fc:	ldp	q0, q1, [x29, #-48]
   49200:	str	x20, [sp, #40]
   49204:	stp	q0, q1, [x29, #-112]
   49208:	str	x26, [sp, #48]
   4920c:	b	48a28 <__gmp_doprnt@@Base+0xb8>
   49210:	ldur	x9, [x29, #-40]
   49214:	add	x8, x9, x8
   49218:	ldr	x8, [x8]
   4921c:	cbnz	x8, 491a0 <__gmp_doprnt@@Base+0x830>
   49220:	b	491f8 <__gmp_doprnt@@Base+0x888>
   49224:	stp	xzr, x20, [sp, #40]
   49228:	ldr	x1, [sp, #48]
   4922c:	ldrb	w8, [x1]
   49230:	cbz	w8, 49264 <__gmp_doprnt@@Base+0x8f4>
   49234:	ldp	x0, x8, [sp, #24]
   49238:	ldp	q0, q1, [x29, #-112]
   4923c:	add	x2, sp, #0x40
   49240:	ldr	x8, [x8]
   49244:	stp	q0, q1, [sp, #64]
   49248:	blr	x8
   4924c:	ldr	x19, [sp, #40]
   49250:	cmn	w0, #0x1
   49254:	csel	w8, wzr, w0, eq  // eq = none
   49258:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   4925c:	add	w19, w8, w19
   49260:	b	49268 <__gmp_doprnt@@Base+0x8f8>
   49264:	ldr	x19, [sp, #40]
   49268:	ldr	x8, [sp, #32]
   4926c:	ldr	x8, [x8, #24]
   49270:	cbz	x8, 49288 <__gmp_doprnt@@Base+0x918>
   49274:	ldr	x0, [sp, #24]
   49278:	blr	x8
   4927c:	cmn	w0, #0x1
   49280:	b.ne	49288 <__gmp_doprnt@@Base+0x918>  // b.any
   49284:	mov	w19, #0xffffffff            	// #-1
   49288:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4928c:	ldr	x8, [x8, #4016]
   49290:	ldp	x0, x1, [sp, #8]
   49294:	ldr	x8, [x8]
   49298:	blr	x8
   4929c:	mov	w0, w19
   492a0:	ldp	x20, x19, [sp, #352]
   492a4:	ldp	x22, x21, [sp, #336]
   492a8:	ldp	x24, x23, [sp, #320]
   492ac:	ldp	x26, x25, [sp, #304]
   492b0:	ldp	x28, x27, [sp, #288]
   492b4:	ldp	x29, x30, [sp, #272]
   492b8:	add	sp, sp, #0x170
   492bc:	ret
   492c0:	cmp	w8, #0x18
   492c4:	b.eq	49284 <__gmp_doprnt@@Base+0x914>  // b.none
   492c8:	b	4929c <__gmp_doprnt@@Base+0x92c>

00000000000492cc <__gmp_doprnt_mpf2@@Base>:
   492cc:	sub	sp, sp, #0x110
   492d0:	stp	x29, x30, [sp, #176]
   492d4:	stp	x28, x27, [sp, #192]
   492d8:	stp	x26, x25, [sp, #208]
   492dc:	stp	x24, x23, [sp, #224]
   492e0:	stp	x22, x21, [sp, #240]
   492e4:	stp	x20, x19, [sp, #256]
   492e8:	stp	x0, x1, [sp, #64]
   492ec:	ldr	w19, [x2, #28]
   492f0:	ldr	w8, [x2, #4]
   492f4:	mov	x25, x3
   492f8:	mov	x20, x2
   492fc:	add	x29, sp, #0xb0
   49300:	tbnz	w19, #31, 49358 <__gmp_doprnt_mpf2@@Base+0x8c>
   49304:	cmp	w8, #0x2
   49308:	b.eq	4939c <__gmp_doprnt_mpf2@@Base+0xd0>  // b.none
   4930c:	cmp	w8, #0x1
   49310:	b.ne	493a4 <__gmp_doprnt_mpf2@@Base+0xd8>  // b.any
   49314:	ldr	w8, [x20]
   49318:	mov	w10, #0x28                  	// #40
   4931c:	ldr	x9, [x4, #8]
   49320:	cmp	w8, #0x0
   49324:	cneg	w8, w8, mi  // mi = first
   49328:	umull	x8, w8, w10
   4932c:	adrp	x10, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49330:	ldr	x10, [x10, #3936]
   49334:	ldr	w8, [x10, x8]
   49338:	lsr	x10, x9, #63
   4933c:	eor	w10, w10, #0x1
   49340:	add	w8, w10, w8
   49344:	madd	w8, w8, w9, w19
   49348:	add	w8, w8, #0x3
   4934c:	cmp	w8, #0x1
   49350:	csinc	w8, w8, wzr, gt
   49354:	b	493b4 <__gmp_doprnt_mpf2@@Base+0xe8>
   49358:	cmp	w8, #0x3
   4935c:	b.ne	493b0 <__gmp_doprnt_mpf2@@Base+0xe4>  // b.any
   49360:	ldr	w11, [x20]
   49364:	adrp	x12, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49368:	ldrsw	x9, [x4]
   4936c:	ldr	x12, [x12, #3936]
   49370:	mov	w10, #0x28                  	// #40
   49374:	cmp	w11, #0x0
   49378:	mov	w8, wzr
   4937c:	madd	x9, x9, x10, x12
   49380:	cneg	w10, w11, mi  // mi = first
   49384:	ldr	x9, [x9, #8]
   49388:	sub	w10, w10, #0x1
   4938c:	sbfiz	x10, x10, #6, #32
   49390:	umulh	x9, x9, x10
   49394:	add	w19, w9, #0x2
   49398:	b	493b4 <__gmp_doprnt_mpf2@@Base+0xe8>
   4939c:	add	w8, w19, #0x1
   493a0:	b	493b4 <__gmp_doprnt_mpf2@@Base+0xe8>
   493a4:	cmp	w19, #0x1
   493a8:	csinc	w8, w19, wzr, gt
   493ac:	b	493b4 <__gmp_doprnt_mpf2@@Base+0xe8>
   493b0:	mov	w8, wzr
   493b4:	ldr	w2, [x20]
   493b8:	sxtw	x3, w8
   493bc:	sub	x1, x29, #0x10
   493c0:	mov	x0, xzr
   493c4:	bl	cff0 <__gmpf_get_str@plt>
   493c8:	mov	x28, x0
   493cc:	bl	c090 <strlen@plt>
   493d0:	mov	x10, x28
   493d4:	ldrb	w9, [x20, #44]
   493d8:	ldrb	w11, [x10], #1
   493dc:	ldr	w8, [x20, #4]
   493e0:	stp	x0, x28, [sp, #48]
   493e4:	cmp	w11, #0x2d
   493e8:	cset	w23, eq  // eq = none
   493ec:	csel	x10, x10, x28, eq  // eq = none
   493f0:	csel	w21, w11, w9, eq  // eq = none
   493f4:	cmp	w8, #0x2
   493f8:	sub	w22, w0, w23
   493fc:	str	x10, [sp, #32]
   49400:	b.eq	4948c <__gmp_doprnt_mpf2@@Base+0x1c0>  // b.none
   49404:	cmp	w8, #0x1
   49408:	b.ne	494a0 <__gmp_doprnt_mpf2@@Base+0x1d4>  // b.any
   4940c:	tbz	w19, #31, 49424 <__gmp_doprnt_mpf2@@Base+0x158>
   49410:	ldur	x8, [x29, #-16]
   49414:	sxtw	x9, w22
   49418:	sub	x8, x9, x8
   4941c:	cmp	x8, #0x0
   49420:	csel	w19, w8, wzr, gt
   49424:	ldur	w26, [x29, #-16]
   49428:	adds	w27, w19, w26
   4942c:	b.mi	49520 <__gmp_doprnt_mpf2@@Base+0x254>  // b.first
   49430:	cmp	w22, w27
   49434:	b.le	495f8 <__gmp_doprnt_mpf2@@Base+0x32c>
   49438:	ldr	w8, [x20]
   4943c:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   49440:	adrp	x10, 4c000 <__gmp_randclear_mt@@Base+0x18>
   49444:	add	x9, x9, #0x2d
   49448:	add	x10, x10, #0x79d
   4944c:	cmp	w8, #0x0
   49450:	mov	x24, x25
   49454:	csel	x22, x10, x9, ge  // ge = tcont
   49458:	cneg	w25, w8, mi  // mi = first
   4945c:	bl	cca0 <__ctype_b_loc@plt>
   49460:	ldr	x9, [sp, #32]
   49464:	ldr	x8, [x0]
   49468:	ldrb	w9, [x9, w27, sxtw]
   4946c:	ldrh	w10, [x8, x9, lsl #1]
   49470:	tbnz	w10, #11, 49528 <__gmp_doprnt_mpf2@@Base+0x25c>
   49474:	tst	w10, #0x200
   49478:	mov	w10, #0xffffffa9            	// #-87
   4947c:	mov	w11, #0xffffffc9            	// #-55
   49480:	csel	w10, w11, w10, eq  // eq = none
   49484:	add	w9, w10, w9
   49488:	b	4952c <__gmp_doprnt_mpf2@@Base+0x260>
   4948c:	tbz	w19, #31, 494bc <__gmp_doprnt_mpf2@@Base+0x1f0>
   49490:	sub	w8, w22, #0x1
   49494:	cmp	w22, #0x0
   49498:	csel	w19, w8, wzr, gt
   4949c:	b	494bc <__gmp_doprnt_mpf2@@Base+0x1f0>
   494a0:	ldur	x8, [x29, #-16]
   494a4:	cmn	x8, #0x3
   494a8:	b.lt	494bc <__gmp_doprnt_mpf2@@Base+0x1f0>  // b.tstop
   494ac:	cmp	w19, #0x1
   494b0:	csinc	w9, w19, wzr, gt
   494b4:	cmp	x8, x9
   494b8:	b.le	495f8 <__gmp_doprnt_mpf2@@Base+0x32c>
   494bc:	ldur	x8, [x29, #-16]
   494c0:	ldr	w9, [x20, #16]
   494c4:	cmp	w22, #0x1
   494c8:	csinc	w28, w22, wzr, lt  // lt = tstop
   494cc:	cmp	w28, #0x0
   494d0:	sub	x8, x8, w28, sxtw
   494d4:	ldr	x2, [x20, #8]
   494d8:	cset	w24, eq  // eq = none
   494dc:	cmp	w9, #0x0
   494e0:	lsl	x9, x8, #2
   494e4:	csel	x8, x8, x9, eq  // eq = none
   494e8:	mov	w10, #0x2d                  	// #45
   494ec:	mov	w9, #0x2b                  	// #43
   494f0:	cmp	x8, #0x0
   494f4:	cneg	x4, x8, mi  // mi = first
   494f8:	csel	w3, w9, w10, ge  // ge = tcont
   494fc:	add	x0, sp, #0x54
   49500:	mov	w1, #0x4a                  	// #74
   49504:	sub	w22, w22, w28
   49508:	bl	c440 <snprintf@plt>
   4950c:	mov	w23, w0
   49510:	mov	w27, wzr
   49514:	ldr	w8, [x20, #40]
   49518:	cbnz	w8, 49624 <__gmp_doprnt_mpf2@@Base+0x358>
   4951c:	b	49660 <__gmp_doprnt_mpf2@@Base+0x394>
   49520:	mov	w22, wzr
   49524:	b	495f4 <__gmp_doprnt_mpf2@@Base+0x328>
   49528:	sub	w9, w9, #0x30
   4952c:	add	w10, w25, #0x1
   49530:	cmp	w9, w10, lsr #1
   49534:	sxtw	x9, w27
   49538:	b.ge	49568 <__gmp_doprnt_mpf2@@Base+0x29c>  // b.tcont
   4953c:	ldr	x8, [sp, #32]
   49540:	mov	x25, x24
   49544:	sub	x8, x8, #0x1
   49548:	subs	x10, x9, #0x1
   4954c:	mov	w22, w9
   49550:	b.lt	495f0 <__gmp_doprnt_mpf2@@Base+0x324>  // b.tstop
   49554:	ldrb	w9, [x8, x9]
   49558:	cmp	w9, #0x30
   4955c:	mov	x9, x10
   49560:	b.eq	49548 <__gmp_doprnt_mpf2@@Base+0x27c>  // b.none
   49564:	b	495f0 <__gmp_doprnt_mpf2@@Base+0x324>
   49568:	ldr	x14, [sp, #32]
   4956c:	add	w11, w19, w26
   49570:	mov	x15, x22
   49574:	mov	w10, #0xffffffa9            	// #-87
   49578:	add	w22, w11, #0x1
   4957c:	mov	w11, #0xffffffc9            	// #-55
   49580:	b	495a8 <__gmp_doprnt_mpf2@@Base+0x2dc>
   49584:	tst	w13, #0x200
   49588:	csel	w13, w11, w10, eq  // eq = none
   4958c:	add	w12, w13, w12
   49590:	sxtw	x12, w12
   49594:	add	x12, x12, #0x1
   49598:	sub	x9, x9, #0x1
   4959c:	cmp	w12, w25
   495a0:	sub	w22, w22, #0x1
   495a4:	b.ne	495c4 <__gmp_doprnt_mpf2@@Base+0x2f8>  // b.any
   495a8:	cbz	x9, 495d4 <__gmp_doprnt_mpf2@@Base+0x308>
   495ac:	add	x12, x14, x9
   495b0:	ldurb	w12, [x12, #-1]
   495b4:	ldrh	w13, [x8, x12, lsl #1]
   495b8:	tbz	w13, #11, 49584 <__gmp_doprnt_mpf2@@Base+0x2b8>
   495bc:	sub	w12, w12, #0x30
   495c0:	b	49590 <__gmp_doprnt_mpf2@@Base+0x2c4>
   495c4:	ldrb	w8, [x15, x12]
   495c8:	mov	x25, x24
   495cc:	strb	w8, [x14, x9]
   495d0:	b	495f0 <__gmp_doprnt_mpf2@@Base+0x324>
   495d4:	mov	w8, #0x31                  	// #49
   495d8:	strb	w8, [x28, x23]
   495dc:	ldur	x8, [x29, #-16]
   495e0:	mov	w22, #0x1                   	// #1
   495e4:	mov	x25, x24
   495e8:	add	x8, x8, #0x1
   495ec:	stur	x8, [x29, #-16]
   495f0:	cbnz	w22, 495f8 <__gmp_doprnt_mpf2@@Base+0x32c>
   495f4:	stur	xzr, [x29, #-16]
   495f8:	ldur	x8, [x29, #-16]
   495fc:	cmp	x8, #0x0
   49600:	b.le	49648 <__gmp_doprnt_mpf2@@Base+0x37c>
   49604:	cmp	x8, w22, sxtw
   49608:	csel	w28, w22, w8, gt
   4960c:	mov	w27, wzr
   49610:	mov	w23, wzr
   49614:	sub	w24, w8, w28
   49618:	sub	w22, w22, w28
   4961c:	ldr	w8, [x20, #40]
   49620:	cbz	w8, 49660 <__gmp_doprnt_mpf2@@Base+0x394>
   49624:	ldr	w8, [x20, #4]
   49628:	add	w9, w28, w24
   4962c:	add	w10, w27, w22
   49630:	cmp	w8, #0x3
   49634:	csneg	w8, wzr, w9, ne  // ne = any
   49638:	sub	w9, w19, w10
   4963c:	add	w8, w9, w8
   49640:	bic	w10, w8, w8, asr #31
   49644:	b	49664 <__gmp_doprnt_mpf2@@Base+0x398>
   49648:	mov	w28, wzr
   4964c:	mov	w23, wzr
   49650:	neg	w27, w8
   49654:	mov	w24, #0x1                   	// #1
   49658:	ldr	w8, [x20, #40]
   4965c:	cbnz	w8, 49624 <__gmp_doprnt_mpf2@@Base+0x358>
   49660:	mov	w10, wzr
   49664:	add	w8, w27, w22
   49668:	cmn	w8, w10
   4966c:	b.ne	49678 <__gmp_doprnt_mpf2@@Base+0x3ac>  // b.any
   49670:	ldr	w8, [x20, #36]
   49674:	cbz	w8, 4998c <__gmp_doprnt_mpf2@@Base+0x6c0>
   49678:	mov	x0, x25
   4967c:	mov	w19, w10
   49680:	bl	c090 <strlen@plt>
   49684:	mov	w10, w19
   49688:	ldr	w8, [x20, #32]
   4968c:	and	w19, w21, #0xff
   49690:	str	x25, [sp, #16]
   49694:	cmp	w8, #0x1
   49698:	b.eq	496ac <__gmp_doprnt_mpf2@@Base+0x3e0>  // b.none
   4969c:	cmp	w8, #0x3
   496a0:	b.ne	496d8 <__gmp_doprnt_mpf2@@Base+0x40c>  // b.any
   496a4:	orr	w8, w28, w22
   496a8:	cbz	w8, 496d8 <__gmp_doprnt_mpf2@@Base+0x40c>
   496ac:	ldr	w8, [x20]
   496b0:	cmn	w8, #0x10
   496b4:	b.eq	49994 <__gmp_doprnt_mpf2@@Base+0x6c8>  // b.none
   496b8:	cmp	w8, #0x8
   496bc:	b.eq	499a4 <__gmp_doprnt_mpf2@@Base+0x6d8>  // b.none
   496c0:	cmp	w8, #0x10
   496c4:	b.ne	496d8 <__gmp_doprnt_mpf2@@Base+0x40c>  // b.any
   496c8:	adrp	x26, 58000 <__gmp_limbroots_table@@Base+0x338>
   496cc:	mov	w21, #0x2                   	// #2
   496d0:	add	x26, x26, #0x52
   496d4:	b	496e0 <__gmp_doprnt_mpf2@@Base+0x414>
   496d8:	mov	x26, xzr
   496dc:	mov	w21, wzr
   496e0:	cmp	w19, #0x0
   496e4:	csetm	w9, ne  // ne = any
   496e8:	sub	w9, w9, w22
   496ec:	sub	w9, w9, w27
   496f0:	sub	w9, w9, w24
   496f4:	sub	w9, w9, w28
   496f8:	ldr	w8, [x20, #48]
   496fc:	sub	w9, w9, w23
   49700:	sub	w9, w9, w10
   49704:	str	w10, [sp, #24]
   49708:	sub	w9, w9, w0
   4970c:	ldr	w10, [x20, #24]
   49710:	sub	w9, w9, w21
   49714:	add	w8, w9, w8
   49718:	cmp	w8, #0x0
   4971c:	str	w8, [sp, #44]
   49720:	csel	w8, w10, wzr, gt
   49724:	str	x23, [sp, #8]
   49728:	cmp	w8, #0x2
   4972c:	mov	w23, wzr
   49730:	str	w8, [sp, #28]
   49734:	b.ne	49768 <__gmp_doprnt_mpf2@@Base+0x49c>  // b.any
   49738:	mov	w25, w24
   4973c:	mov	x24, x0
   49740:	ldp	x8, x0, [sp, #64]
   49744:	ldrb	w1, [x20, #20]
   49748:	ldr	w2, [sp, #44]
   4974c:	ldr	x8, [x8, #16]
   49750:	blr	x8
   49754:	cmn	w0, #0x1
   49758:	csel	w23, wzr, w0, eq  // eq = none
   4975c:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49760:	mov	x0, x24
   49764:	mov	w24, w25
   49768:	str	x0, [sp]
   4976c:	cbz	w19, 49794 <__gmp_doprnt_mpf2@@Base+0x4c8>
   49770:	ldp	x8, x0, [sp, #64]
   49774:	mov	w2, #0x1                   	// #1
   49778:	mov	w1, w19
   4977c:	ldr	x8, [x8, #16]
   49780:	blr	x8
   49784:	cmn	w0, #0x1
   49788:	csel	w8, wzr, w0, eq  // eq = none
   4978c:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49790:	add	w23, w8, w23
   49794:	cbz	w21, 497bc <__gmp_doprnt_mpf2@@Base+0x4f0>
   49798:	ldp	x8, x0, [sp, #64]
   4979c:	mov	x1, x26
   497a0:	mov	x2, x21
   497a4:	ldr	x8, [x8, #8]
   497a8:	blr	x8
   497ac:	cmn	w0, #0x1
   497b0:	csel	w8, wzr, w0, eq  // eq = none
   497b4:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   497b8:	add	w23, w8, w23
   497bc:	ldr	w8, [sp, #28]
   497c0:	cmp	w8, #0x3
   497c4:	b.ne	497ec <__gmp_doprnt_mpf2@@Base+0x520>  // b.any
   497c8:	ldp	x8, x0, [sp, #64]
   497cc:	ldrb	w1, [x20, #20]
   497d0:	ldr	w2, [sp, #44]
   497d4:	ldr	x8, [x8, #16]
   497d8:	blr	x8
   497dc:	cmn	w0, #0x1
   497e0:	csel	w8, wzr, w0, eq  // eq = none
   497e4:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   497e8:	add	w23, w8, w23
   497ec:	ldp	x8, x0, [sp, #64]
   497f0:	ldr	x1, [sp, #32]
   497f4:	sxtw	x21, w28
   497f8:	mov	x2, x21
   497fc:	ldr	x8, [x8, #8]
   49800:	blr	x8
   49804:	cmn	w0, #0x1
   49808:	csel	w8, wzr, w0, eq  // eq = none
   4980c:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49810:	add	w19, w8, w23
   49814:	cbz	w24, 4983c <__gmp_doprnt_mpf2@@Base+0x570>
   49818:	ldp	x8, x0, [sp, #64]
   4981c:	mov	w2, w24
   49820:	mov	w1, #0x30                  	// #48
   49824:	ldr	x8, [x8, #16]
   49828:	blr	x8
   4982c:	cmn	w0, #0x1
   49830:	csel	w8, wzr, w0, eq  // eq = none
   49834:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49838:	add	w19, w8, w19
   4983c:	ldr	x9, [sp]
   49840:	cbz	w9, 49868 <__gmp_doprnt_mpf2@@Base+0x59c>
   49844:	ldp	x8, x0, [sp, #64]
   49848:	ldr	x1, [sp, #16]
   4984c:	sxtw	x2, w9
   49850:	ldr	x8, [x8, #8]
   49854:	blr	x8
   49858:	cmn	w0, #0x1
   4985c:	csel	w8, wzr, w0, eq  // eq = none
   49860:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49864:	add	w19, w8, w19
   49868:	cbz	w27, 49890 <__gmp_doprnt_mpf2@@Base+0x5c4>
   4986c:	ldp	x8, x0, [sp, #64]
   49870:	mov	w1, #0x30                  	// #48
   49874:	mov	w2, w27
   49878:	ldr	x8, [x8, #16]
   4987c:	blr	x8
   49880:	cmn	w0, #0x1
   49884:	csel	w8, wzr, w0, eq  // eq = none
   49888:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   4988c:	add	w19, w8, w19
   49890:	ldr	x9, [sp, #32]
   49894:	ldr	w2, [sp, #24]
   49898:	cbz	w22, 498c4 <__gmp_doprnt_mpf2@@Base+0x5f8>
   4989c:	ldp	x8, x0, [sp, #64]
   498a0:	add	x1, x9, x21
   498a4:	sxtw	x2, w22
   498a8:	ldr	x8, [x8, #8]
   498ac:	blr	x8
   498b0:	cmn	w0, #0x1
   498b4:	csel	w8, wzr, w0, eq  // eq = none
   498b8:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   498bc:	ldr	w2, [sp, #24]
   498c0:	add	w19, w8, w19
   498c4:	cbz	w2, 498e8 <__gmp_doprnt_mpf2@@Base+0x61c>
   498c8:	ldp	x8, x0, [sp, #64]
   498cc:	mov	w1, #0x30                  	// #48
   498d0:	ldr	x8, [x8, #16]
   498d4:	blr	x8
   498d8:	cmn	w0, #0x1
   498dc:	csel	w8, wzr, w0, eq  // eq = none
   498e0:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   498e4:	add	w19, w8, w19
   498e8:	ldr	x9, [sp, #8]
   498ec:	cbz	w9, 49914 <__gmp_doprnt_mpf2@@Base+0x648>
   498f0:	ldp	x8, x0, [sp, #64]
   498f4:	sxtw	x2, w9
   498f8:	add	x1, sp, #0x54
   498fc:	ldr	x8, [x8, #8]
   49900:	blr	x8
   49904:	cmn	w0, #0x1
   49908:	csel	w8, wzr, w0, eq  // eq = none
   4990c:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49910:	add	w19, w8, w19
   49914:	ldr	w8, [sp, #28]
   49918:	cmp	w8, #0x1
   4991c:	b.ne	4994c <__gmp_doprnt_mpf2@@Base+0x680>  // b.any
   49920:	ldp	x8, x0, [sp, #64]
   49924:	ldrb	w1, [x20, #20]
   49928:	ldr	w2, [sp, #44]
   4992c:	ldr	x8, [x8, #16]
   49930:	blr	x8
   49934:	cmn	w0, #0x1
   49938:	csel	w8, wzr, w0, eq  // eq = none
   4993c:	b.eq	49948 <__gmp_doprnt_mpf2@@Base+0x67c>  // b.none
   49940:	add	w19, w8, w19
   49944:	b	4994c <__gmp_doprnt_mpf2@@Base+0x680>
   49948:	mov	w19, #0xffffffff            	// #-1
   4994c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49950:	ldp	x9, x0, [sp, #48]
   49954:	ldr	x8, [x8, #4016]
   49958:	add	w9, w9, #0x1
   4995c:	ldr	x8, [x8]
   49960:	sxtw	x1, w9
   49964:	blr	x8
   49968:	mov	w0, w19
   4996c:	ldp	x20, x19, [sp, #256]
   49970:	ldp	x22, x21, [sp, #240]
   49974:	ldp	x24, x23, [sp, #224]
   49978:	ldp	x26, x25, [sp, #208]
   4997c:	ldp	x28, x27, [sp, #192]
   49980:	ldp	x29, x30, [sp, #176]
   49984:	add	sp, sp, #0x110
   49988:	ret
   4998c:	mov	x0, xzr
   49990:	b	49688 <__gmp_doprnt_mpf2@@Base+0x3bc>
   49994:	adrp	x26, 58000 <__gmp_limbroots_table@@Base+0x338>
   49998:	mov	w21, #0x2                   	// #2
   4999c:	add	x26, x26, #0x55
   499a0:	b	496e0 <__gmp_doprnt_mpf2@@Base+0x414>
   499a4:	adrp	x26, 58000 <__gmp_limbroots_table@@Base+0x338>
   499a8:	mov	w21, #0x1                   	// #1
   499ac:	add	x26, x26, #0x294
   499b0:	b	496e0 <__gmp_doprnt_mpf2@@Base+0x414>

00000000000499b4 <__gmp_doprnt_integer@@Base>:
   499b4:	sub	sp, sp, #0x90
   499b8:	stp	x29, x30, [sp, #48]
   499bc:	add	x29, sp, #0x30
   499c0:	stp	x28, x27, [sp, #64]
   499c4:	stp	x26, x25, [sp, #80]
   499c8:	stp	x24, x23, [sp, #96]
   499cc:	stp	x22, x21, [sp, #112]
   499d0:	stp	x20, x19, [sp, #128]
   499d4:	stur	x1, [x29, #-8]
   499d8:	mov	x9, x3
   499dc:	ldrb	w8, [x2, #44]
   499e0:	ldrb	w10, [x9], #1
   499e4:	mov	x20, x2
   499e8:	stur	x0, [x29, #-16]
   499ec:	cmp	w10, #0x2d
   499f0:	csel	w19, w10, w8, eq  // eq = none
   499f4:	csel	x8, x3, x9, ne  // ne = any
   499f8:	ldrb	w8, [x8]
   499fc:	csel	x22, x9, x3, eq  // eq = none
   49a00:	tst	w19, #0xff
   49a04:	cset	w23, ne  // ne = any
   49a08:	cmp	w8, #0x30
   49a0c:	b.ne	49a1c <__gmp_doprnt_integer@@Base+0x68>  // b.any
   49a10:	ldr	w8, [x20, #28]
   49a14:	cmp	w8, #0x0
   49a18:	cinc	x22, x22, eq  // eq = none
   49a1c:	mov	x0, x22
   49a20:	bl	c090 <strlen@plt>
   49a24:	mov	x21, x0
   49a28:	mov	w1, #0x2f                  	// #47
   49a2c:	mov	x0, x22
   49a30:	bl	cf70 <strchr@plt>
   49a34:	ldr	w8, [x20, #32]
   49a38:	stur	w23, [x29, #-20]
   49a3c:	cmp	w8, #0x2
   49a40:	b.ne	49a54 <__gmp_doprnt_integer@@Base+0xa0>  // b.any
   49a44:	str	xzr, [sp, #16]
   49a48:	mov	w27, wzr
   49a4c:	cbnz	x0, 49ab4 <__gmp_doprnt_integer@@Base+0x100>
   49a50:	b	49ad0 <__gmp_doprnt_integer@@Base+0x11c>
   49a54:	ldr	w9, [x20]
   49a58:	cmn	w9, #0x10
   49a5c:	b.eq	49a88 <__gmp_doprnt_integer@@Base+0xd4>  // b.none
   49a60:	cmp	w9, #0x8
   49a64:	b.eq	49aa0 <__gmp_doprnt_integer@@Base+0xec>  // b.none
   49a68:	cmp	w9, #0x10
   49a6c:	b.ne	49a44 <__gmp_doprnt_integer@@Base+0x90>  // b.any
   49a70:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   49a74:	mov	w27, #0x2                   	// #2
   49a78:	add	x9, x9, #0x52
   49a7c:	str	x9, [sp, #16]
   49a80:	cbnz	x0, 49ab4 <__gmp_doprnt_integer@@Base+0x100>
   49a84:	b	49ad0 <__gmp_doprnt_integer@@Base+0x11c>
   49a88:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   49a8c:	mov	w27, #0x2                   	// #2
   49a90:	add	x9, x9, #0x55
   49a94:	str	x9, [sp, #16]
   49a98:	cbnz	x0, 49ab4 <__gmp_doprnt_integer@@Base+0x100>
   49a9c:	b	49ad0 <__gmp_doprnt_integer@@Base+0x11c>
   49aa0:	adrp	x9, 58000 <__gmp_limbroots_table@@Base+0x338>
   49aa4:	mov	w27, #0x1                   	// #1
   49aa8:	add	x9, x9, #0x294
   49aac:	str	x9, [sp, #16]
   49ab0:	cbz	x0, 49ad0 <__gmp_doprnt_integer@@Base+0x11c>
   49ab4:	cmp	w8, #0x3
   49ab8:	mov	w25, w27
   49abc:	b.ne	49ad4 <__gmp_doprnt_integer@@Base+0x120>  // b.any
   49ac0:	ldrb	w9, [x0, #1]
   49ac4:	mov	w25, w27
   49ac8:	cmp	w9, #0x30
   49acc:	b.ne	49ad4 <__gmp_doprnt_integer@@Base+0x120>  // b.any
   49ad0:	mov	w25, wzr
   49ad4:	cmp	w8, #0x3
   49ad8:	and	w19, w19, #0xff
   49adc:	str	x0, [sp, #8]
   49ae0:	b.ne	49af0 <__gmp_doprnt_integer@@Base+0x13c>  // b.any
   49ae4:	ldrb	w8, [x22]
   49ae8:	cmp	w8, #0x30
   49aec:	csel	w27, wzr, w27, eq  // eq = none
   49af0:	ldp	w10, w9, [x20, #24]
   49af4:	cmp	w19, #0x0
   49af8:	ldr	w8, [x20, #48]
   49afc:	cinc	w11, w21, ne  // ne = any
   49b00:	add	w11, w25, w11
   49b04:	add	w11, w11, w27
   49b08:	sub	w23, w9, w21
   49b0c:	bic	w28, w23, w23, asr #31
   49b10:	sub	w8, w8, w11
   49b14:	sub	w26, w8, w28
   49b18:	cmp	w26, #0x0
   49b1c:	str	x21, [sp]
   49b20:	csel	w21, w10, wzr, gt
   49b24:	cmp	w21, #0x2
   49b28:	mov	w24, wzr
   49b2c:	b.ne	49b50 <__gmp_doprnt_integer@@Base+0x19c>  // b.any
   49b30:	ldp	x8, x0, [x29, #-16]
   49b34:	ldrb	w1, [x20, #20]
   49b38:	mov	w2, w26
   49b3c:	ldr	x8, [x8, #16]
   49b40:	blr	x8
   49b44:	cmn	w0, #0x1
   49b48:	csel	w24, wzr, w0, eq  // eq = none
   49b4c:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49b50:	cbz	w19, 49b78 <__gmp_doprnt_integer@@Base+0x1c4>
   49b54:	ldp	x8, x0, [x29, #-16]
   49b58:	ldur	w2, [x29, #-20]
   49b5c:	mov	w1, w19
   49b60:	ldr	x8, [x8, #16]
   49b64:	blr	x8
   49b68:	cmn	w0, #0x1
   49b6c:	csel	w8, wzr, w0, eq  // eq = none
   49b70:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49b74:	add	w24, w8, w24
   49b78:	cbz	w27, 49ba0 <__gmp_doprnt_integer@@Base+0x1ec>
   49b7c:	ldp	x8, x0, [x29, #-16]
   49b80:	ldr	x1, [sp, #16]
   49b84:	mov	x2, x27
   49b88:	ldr	x8, [x8, #8]
   49b8c:	blr	x8
   49b90:	cmn	w0, #0x1
   49b94:	csel	w8, wzr, w0, eq  // eq = none
   49b98:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49b9c:	add	w24, w8, w24
   49ba0:	cmp	w23, #0x1
   49ba4:	b.lt	49bcc <__gmp_doprnt_integer@@Base+0x218>  // b.tstop
   49ba8:	ldp	x8, x0, [x29, #-16]
   49bac:	mov	w1, #0x30                  	// #48
   49bb0:	mov	w2, w28
   49bb4:	ldr	x8, [x8, #16]
   49bb8:	blr	x8
   49bbc:	cmn	w0, #0x1
   49bc0:	csel	w8, wzr, w0, eq  // eq = none
   49bc4:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49bc8:	add	w24, w8, w24
   49bcc:	cmp	w21, #0x3
   49bd0:	b.ne	49bf8 <__gmp_doprnt_integer@@Base+0x244>  // b.any
   49bd4:	ldp	x8, x0, [x29, #-16]
   49bd8:	ldrb	w1, [x20, #20]
   49bdc:	mov	w2, w26
   49be0:	ldr	x8, [x8, #16]
   49be4:	blr	x8
   49be8:	cmn	w0, #0x1
   49bec:	csel	w8, wzr, w0, eq  // eq = none
   49bf0:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49bf4:	add	w24, w8, w24
   49bf8:	cbz	w25, 49c64 <__gmp_doprnt_integer@@Base+0x2b0>
   49bfc:	ldp	x8, x0, [x29, #-16]
   49c00:	ldr	x9, [sp, #8]
   49c04:	mov	x1, x22
   49c08:	ldr	x8, [x8, #8]
   49c0c:	sub	x9, x9, x22
   49c10:	add	x23, x9, #0x1
   49c14:	sxtw	x19, w23
   49c18:	mov	x2, x19
   49c1c:	blr	x8
   49c20:	cmn	w0, #0x1
   49c24:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49c28:	mov	w27, w0
   49c2c:	ldp	x8, x0, [x29, #-16]
   49c30:	ldr	x1, [sp, #16]
   49c34:	mov	x2, x25
   49c38:	ldr	x8, [x8, #8]
   49c3c:	blr	x8
   49c40:	cmn	w0, #0x1
   49c44:	csel	w8, wzr, w0, eq  // eq = none
   49c48:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49c4c:	ldr	x10, [sp]
   49c50:	add	w9, w27, w24
   49c54:	add	x22, x22, x19
   49c58:	add	w24, w9, w8
   49c5c:	sub	x10, x10, x23
   49c60:	b	49c68 <__gmp_doprnt_integer@@Base+0x2b4>
   49c64:	ldr	x10, [sp]
   49c68:	ldp	x8, x0, [x29, #-16]
   49c6c:	sxtw	x2, w10
   49c70:	mov	x1, x22
   49c74:	ldr	x8, [x8, #8]
   49c78:	blr	x8
   49c7c:	cmn	w0, #0x1
   49c80:	csel	w8, wzr, w0, eq  // eq = none
   49c84:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49c88:	cmp	w21, #0x1
   49c8c:	add	w19, w8, w24
   49c90:	b.ne	49cc0 <__gmp_doprnt_integer@@Base+0x30c>  // b.any
   49c94:	ldp	x8, x0, [x29, #-16]
   49c98:	ldrb	w1, [x20, #20]
   49c9c:	mov	w2, w26
   49ca0:	ldr	x8, [x8, #16]
   49ca4:	blr	x8
   49ca8:	cmn	w0, #0x1
   49cac:	csel	w8, wzr, w0, eq  // eq = none
   49cb0:	b.eq	49cbc <__gmp_doprnt_integer@@Base+0x308>  // b.none
   49cb4:	add	w19, w8, w19
   49cb8:	b	49cc0 <__gmp_doprnt_integer@@Base+0x30c>
   49cbc:	mov	w19, #0xffffffff            	// #-1
   49cc0:	mov	w0, w19
   49cc4:	ldp	x20, x19, [sp, #128]
   49cc8:	ldp	x22, x21, [sp, #112]
   49ccc:	ldp	x24, x23, [sp, #96]
   49cd0:	ldp	x26, x25, [sp, #80]
   49cd4:	ldp	x28, x27, [sp, #64]
   49cd8:	ldp	x29, x30, [sp, #48]
   49cdc:	add	sp, sp, #0x90
   49ce0:	ret

0000000000049ce4 <__gmp_fprintf@@Base>:
   49ce4:	sub	sp, sp, #0x100
   49ce8:	stp	x29, x30, [sp, #240]
   49cec:	add	x29, sp, #0xf0
   49cf0:	mov	x9, #0xffffffffffffffd0    	// #-48
   49cf4:	mov	x10, sp
   49cf8:	sub	x11, x29, #0x70
   49cfc:	movk	x9, #0xff80, lsl #32
   49d00:	add	x12, x29, #0x10
   49d04:	add	x10, x10, #0x80
   49d08:	add	x11, x11, #0x30
   49d0c:	stp	x10, x9, [x29, #-16]
   49d10:	stp	x12, x11, [x29, #-32]
   49d14:	stp	x2, x3, [x29, #-112]
   49d18:	stp	x4, x5, [x29, #-96]
   49d1c:	stp	x6, x7, [x29, #-80]
   49d20:	stp	q1, q2, [sp, #16]
   49d24:	str	q0, [sp]
   49d28:	ldp	q0, q1, [x29, #-32]
   49d2c:	mov	x8, x1
   49d30:	mov	x1, x0
   49d34:	stp	q3, q4, [sp, #48]
   49d38:	stp	q5, q6, [sp, #80]
   49d3c:	str	q7, [sp, #112]
   49d40:	stp	q0, q1, [x29, #-64]
   49d44:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49d48:	ldr	x0, [x0, #3816]
   49d4c:	sub	x3, x29, #0x40
   49d50:	mov	x2, x8
   49d54:	bl	d200 <__gmp_doprnt@plt>
   49d58:	ldp	x29, x30, [sp, #240]
   49d5c:	add	sp, sp, #0x100
   49d60:	ret

0000000000049d64 <__gmp_obstack_printf@@Base>:
   49d64:	sub	sp, sp, #0x100
   49d68:	stp	x29, x30, [sp, #240]
   49d6c:	add	x29, sp, #0xf0
   49d70:	mov	x9, #0xffffffffffffffd0    	// #-48
   49d74:	mov	x10, sp
   49d78:	sub	x11, x29, #0x70
   49d7c:	movk	x9, #0xff80, lsl #32
   49d80:	add	x12, x29, #0x10
   49d84:	add	x10, x10, #0x80
   49d88:	add	x11, x11, #0x30
   49d8c:	stp	x10, x9, [x29, #-16]
   49d90:	stp	x12, x11, [x29, #-32]
   49d94:	stp	x2, x3, [x29, #-112]
   49d98:	stp	x4, x5, [x29, #-96]
   49d9c:	stp	x6, x7, [x29, #-80]
   49da0:	stp	q1, q2, [sp, #16]
   49da4:	str	q0, [sp]
   49da8:	ldp	q0, q1, [x29, #-32]
   49dac:	mov	x8, x1
   49db0:	mov	x1, x0
   49db4:	stp	q3, q4, [sp, #48]
   49db8:	stp	q5, q6, [sp, #80]
   49dbc:	str	q7, [sp, #112]
   49dc0:	stp	q0, q1, [x29, #-64]
   49dc4:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49dc8:	ldr	x0, [x0, #4048]
   49dcc:	sub	x3, x29, #0x40
   49dd0:	mov	x2, x8
   49dd4:	bl	d200 <__gmp_doprnt@plt>
   49dd8:	ldp	x29, x30, [sp, #240]
   49ddc:	add	sp, sp, #0x100
   49de0:	ret

0000000000049de4 <__gmp_obstack_vprintf@@Base>:
   49de4:	sub	sp, sp, #0x30
   49de8:	stp	x29, x30, [sp, #32]
   49dec:	ldp	q1, q0, [x2]
   49df0:	mov	x8, x1
   49df4:	mov	x1, x0
   49df8:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49dfc:	stp	q1, q0, [sp]
   49e00:	ldr	x0, [x0, #4048]
   49e04:	mov	x3, sp
   49e08:	mov	x2, x8
   49e0c:	add	x29, sp, #0x20
   49e10:	bl	d200 <__gmp_doprnt@plt>
   49e14:	ldp	x29, x30, [sp, #32]
   49e18:	add	sp, sp, #0x30
   49e1c:	ret
   49e20:	stp	x29, x30, [sp, #-48]!
   49e24:	stp	x22, x21, [sp, #16]
   49e28:	stp	x20, x19, [sp, #32]
   49e2c:	ldp	x8, x9, [x0, #24]
   49e30:	sxtw	x21, w2
   49e34:	mov	x19, x0
   49e38:	mov	x20, x2
   49e3c:	add	x8, x8, x21
   49e40:	cmp	x8, x9
   49e44:	mov	x22, x1
   49e48:	mov	x29, sp
   49e4c:	b.ls	49e5c <__gmp_obstack_vprintf@@Base+0x78>  // b.plast
   49e50:	mov	x0, x19
   49e54:	mov	w1, w20
   49e58:	bl	d210 <_obstack_newchunk@plt>
   49e5c:	ldr	x0, [x19, #24]
   49e60:	mov	x1, x22
   49e64:	mov	x2, x21
   49e68:	bl	bff0 <memcpy@plt>
   49e6c:	ldr	x8, [x19, #24]
   49e70:	mov	w0, w20
   49e74:	add	x8, x8, x21
   49e78:	str	x8, [x19, #24]
   49e7c:	ldp	x20, x19, [sp, #32]
   49e80:	ldp	x22, x21, [sp, #16]
   49e84:	ldp	x29, x30, [sp], #48
   49e88:	ret
   49e8c:	stp	x29, x30, [sp, #-48]!
   49e90:	stp	x22, x21, [sp, #16]
   49e94:	stp	x20, x19, [sp, #32]
   49e98:	ldp	x9, x8, [x0, #24]
   49e9c:	mov	w19, w2
   49ea0:	sxtw	x21, w19
   49ea4:	mov	x20, x0
   49ea8:	sub	x8, x8, x9
   49eac:	cmp	x8, x21
   49eb0:	mov	w22, w1
   49eb4:	mov	x29, sp
   49eb8:	b.ge	49ec8 <__gmp_obstack_vprintf@@Base+0xe4>  // b.tcont
   49ebc:	mov	x0, x20
   49ec0:	mov	w1, w19
   49ec4:	bl	d210 <_obstack_newchunk@plt>
   49ec8:	ldr	x0, [x20, #24]
   49ecc:	mov	w1, w22
   49ed0:	mov	x2, x21
   49ed4:	add	x8, x0, x21
   49ed8:	str	x8, [x20, #24]
   49edc:	bl	c780 <memset@plt>
   49ee0:	mov	w0, w19
   49ee4:	ldp	x20, x19, [sp, #32]
   49ee8:	ldp	x22, x21, [sp, #16]
   49eec:	ldp	x29, x30, [sp], #48
   49ef0:	ret

0000000000049ef4 <__gmp_printf@@Base>:
   49ef4:	sub	sp, sp, #0x120
   49ef8:	stp	x29, x30, [sp, #256]
   49efc:	add	x29, sp, #0x100
   49f00:	mov	x9, #0xffffffffffffffc8    	// #-56
   49f04:	mov	x10, sp
   49f08:	sub	x11, x29, #0x78
   49f0c:	str	x28, [sp, #272]
   49f10:	stp	x1, x2, [x29, #-120]
   49f14:	stp	x3, x4, [x29, #-104]
   49f18:	stp	x5, x6, [x29, #-88]
   49f1c:	stur	x7, [x29, #-72]
   49f20:	stp	q0, q1, [sp]
   49f24:	stp	q2, q3, [sp, #32]
   49f28:	stp	q4, q5, [sp, #64]
   49f2c:	movk	x9, #0xff80, lsl #32
   49f30:	add	x12, x29, #0x20
   49f34:	adrp	x13, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49f38:	add	x10, x10, #0x80
   49f3c:	add	x11, x11, #0x38
   49f40:	ldr	x13, [x13, #3856]
   49f44:	stp	x10, x9, [x29, #-16]
   49f48:	stp	x12, x11, [x29, #-32]
   49f4c:	ldp	q0, q1, [x29, #-32]
   49f50:	mov	x8, x0
   49f54:	stp	q6, q7, [sp, #96]
   49f58:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   49f5c:	stp	q0, q1, [x29, #-64]
   49f60:	ldr	x1, [x13]
   49f64:	ldr	x0, [x0, #3816]
   49f68:	sub	x3, x29, #0x40
   49f6c:	mov	x2, x8
   49f70:	bl	d200 <__gmp_doprnt@plt>
   49f74:	ldr	x28, [sp, #272]
   49f78:	ldp	x29, x30, [sp, #256]
   49f7c:	add	sp, sp, #0x120
   49f80:	ret
   49f84:	stp	x29, x30, [sp, #-16]!
   49f88:	mov	x8, x1
   49f8c:	mov	x3, x0
   49f90:	mov	w1, #0x1                   	// #1
   49f94:	mov	x0, x8
   49f98:	mov	x29, sp
   49f9c:	bl	d000 <fwrite@plt>
   49fa0:	ldp	x29, x30, [sp], #16
   49fa4:	ret
   49fa8:	sub	sp, sp, #0x140
   49fac:	stp	x20, x19, [sp, #304]
   49fb0:	mov	w19, w2
   49fb4:	sxtw	x8, w19
   49fb8:	stp	x22, x21, [sp, #288]
   49fbc:	cmp	x8, #0x100
   49fc0:	mov	w21, #0x100                 	// #256
   49fc4:	mov	x20, x0
   49fc8:	csel	x2, x8, x21, cc  // cc = lo, ul, last
   49fcc:	mov	x0, sp
   49fd0:	stp	x29, x30, [sp, #256]
   49fd4:	stp	x28, x23, [sp, #272]
   49fd8:	add	x29, sp, #0x100
   49fdc:	bl	c780 <memset@plt>
   49fe0:	cmp	w19, #0x1
   49fe4:	b.lt	4a020 <__gmp_printf@@Base+0x12c>  // b.tstop
   49fe8:	mov	w22, w19
   49fec:	subs	w23, w22, #0x100
   49ff0:	csel	w2, w22, w21, cc  // cc = lo, ul, last
   49ff4:	mov	x0, sp
   49ff8:	mov	w1, #0x1                   	// #1
   49ffc:	mov	x3, x20
   4a000:	bl	d000 <fwrite@plt>
   4a004:	cmn	w0, #0x1
   4a008:	b.eq	4a01c <__gmp_printf@@Base+0x128>  // b.none
   4a00c:	cmp	w22, #0x101
   4a010:	mov	w22, w23
   4a014:	b.ge	49fec <__gmp_printf@@Base+0xf8>  // b.tcont
   4a018:	b	4a020 <__gmp_printf@@Base+0x12c>
   4a01c:	mov	w19, #0xffffffff            	// #-1
   4a020:	mov	w0, w19
   4a024:	ldp	x20, x19, [sp, #304]
   4a028:	ldp	x22, x21, [sp, #288]
   4a02c:	ldp	x28, x23, [sp, #272]
   4a030:	ldp	x29, x30, [sp, #256]
   4a034:	add	sp, sp, #0x140
   4a038:	ret

000000000004a03c <__gmp_snprintf@@Base>:
   4a03c:	sub	sp, sp, #0x120
   4a040:	stp	x29, x30, [sp, #256]
   4a044:	add	x29, sp, #0x100
   4a048:	mov	x8, #0xffffffffffffffd8    	// #-40
   4a04c:	mov	x9, sp
   4a050:	sub	x10, x29, #0x78
   4a054:	movk	x8, #0xff80, lsl #32
   4a058:	add	x11, x29, #0x20
   4a05c:	add	x9, x9, #0x80
   4a060:	add	x10, x10, #0x28
   4a064:	stp	x9, x8, [x29, #-32]
   4a068:	stp	x11, x10, [x29, #-48]
   4a06c:	stp	x3, x4, [x29, #-120]
   4a070:	stp	x5, x6, [x29, #-104]
   4a074:	stur	x7, [x29, #-88]
   4a078:	stp	q1, q2, [sp, #16]
   4a07c:	str	q0, [sp]
   4a080:	ldp	q0, q1, [x29, #-48]
   4a084:	str	x28, [sp, #272]
   4a088:	stp	q3, q4, [sp, #48]
   4a08c:	stp	q5, q6, [sp, #80]
   4a090:	str	q7, [sp, #112]
   4a094:	stp	x0, x1, [x29, #-16]
   4a098:	stp	q0, q1, [x29, #-80]
   4a09c:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a0a0:	ldr	x0, [x0, #4024]
   4a0a4:	sub	x1, x29, #0x10
   4a0a8:	sub	x3, x29, #0x50
   4a0ac:	bl	d200 <__gmp_doprnt@plt>
   4a0b0:	ldr	x28, [sp, #272]
   4a0b4:	ldp	x29, x30, [sp, #256]
   4a0b8:	add	sp, sp, #0x120
   4a0bc:	ret
   4a0c0:	sub	sp, sp, #0x90
   4a0c4:	stp	x29, x30, [sp, #64]
   4a0c8:	stp	x24, x23, [sp, #96]
   4a0cc:	stp	x22, x21, [sp, #112]
   4a0d0:	stp	x20, x19, [sp, #128]
   4a0d4:	ldr	x23, [x0, #8]
   4a0d8:	mov	x19, x2
   4a0dc:	mov	x20, x1
   4a0e0:	str	x25, [sp, #80]
   4a0e4:	cmp	x23, #0x2
   4a0e8:	add	x29, sp, #0x40
   4a0ec:	b.cc	4a158 <__gmp_snprintf@@Base+0x11c>  // b.lo, b.ul, b.last
   4a0f0:	ldp	q1, q0, [x19]
   4a0f4:	mov	x21, x0
   4a0f8:	mov	x3, sp
   4a0fc:	mov	x1, x23
   4a100:	stp	q1, q0, [sp, #32]
   4a104:	ldr	x0, [x0]
   4a108:	mov	x2, x20
   4a10c:	stp	q1, q0, [sp]
   4a110:	bl	d2e0 <vsnprintf@plt>
   4a114:	cmn	w0, #0x1
   4a118:	b.eq	4a1c4 <__gmp_snprintf@@Base+0x188>  // b.none
   4a11c:	mov	w22, w0
   4a120:	ldp	x11, x10, [x21]
   4a124:	sxtw	x8, w22
   4a128:	sub	x9, x23, #0x1
   4a12c:	cmp	x9, x8
   4a130:	csel	x12, x8, x9, hi  // hi = pmore
   4a134:	cmp	x9, x8
   4a138:	sub	x8, x10, x12
   4a13c:	add	x9, x11, x12
   4a140:	stp	x9, x8, [x21]
   4a144:	b.ne	4a1c8 <__gmp_snprintf@@Base+0x18c>  // b.any
   4a148:	cmp	w22, #0x80
   4a14c:	mov	w8, #0x80                  	// #128
   4a150:	csel	w21, w22, w8, gt
   4a154:	b	4a15c <__gmp_snprintf@@Base+0x120>
   4a158:	mov	w21, #0x80                  	// #128
   4a15c:	adrp	x24, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a160:	adrp	x25, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a164:	ldr	x24, [x24, #3840]
   4a168:	ldr	x25, [x25, #4016]
   4a16c:	ldr	x8, [x24]
   4a170:	lsl	x21, x21, #1
   4a174:	mov	x0, x21
   4a178:	blr	x8
   4a17c:	ldp	q0, q1, [x19]
   4a180:	mov	x3, sp
   4a184:	mov	x1, x21
   4a188:	mov	x2, x20
   4a18c:	stp	q0, q1, [sp, #32]
   4a190:	ldp	q0, q1, [sp, #32]
   4a194:	mov	x23, x0
   4a198:	stp	q0, q1, [sp]
   4a19c:	bl	d2e0 <vsnprintf@plt>
   4a1a0:	ldr	x8, [x25]
   4a1a4:	mov	w22, w0
   4a1a8:	mov	x0, x23
   4a1ac:	mov	x1, x21
   4a1b0:	blr	x8
   4a1b4:	sub	x8, x21, #0x1
   4a1b8:	cmp	x8, w22, sxtw
   4a1bc:	b.eq	4a16c <__gmp_snprintf@@Base+0x130>  // b.none
   4a1c0:	b	4a1c8 <__gmp_snprintf@@Base+0x18c>
   4a1c4:	mov	w22, #0xffffffff            	// #-1
   4a1c8:	mov	w0, w22
   4a1cc:	ldp	x20, x19, [sp, #128]
   4a1d0:	ldp	x22, x21, [sp, #112]
   4a1d4:	ldp	x24, x23, [sp, #96]
   4a1d8:	ldr	x25, [sp, #80]
   4a1dc:	ldp	x29, x30, [sp, #64]
   4a1e0:	add	sp, sp, #0x90
   4a1e4:	ret
   4a1e8:	stp	x29, x30, [sp, #-48]!
   4a1ec:	stp	x20, x19, [sp, #32]
   4a1f0:	ldr	x8, [x0, #8]
   4a1f4:	mov	x19, x2
   4a1f8:	str	x21, [sp, #16]
   4a1fc:	mov	x29, sp
   4a200:	cmp	x8, #0x2
   4a204:	b.cc	4a234 <__gmp_snprintf@@Base+0x1f8>  // b.lo, b.ul, b.last
   4a208:	mov	x20, x0
   4a20c:	ldr	x0, [x0]
   4a210:	sub	x8, x8, #0x1
   4a214:	cmp	x8, x19
   4a218:	csel	x21, x8, x19, cc  // cc = lo, ul, last
   4a21c:	mov	x2, x21
   4a220:	bl	bff0 <memcpy@plt>
   4a224:	ldp	x8, x9, [x20]
   4a228:	add	x8, x8, x21
   4a22c:	sub	x9, x9, x21
   4a230:	stp	x8, x9, [x20]
   4a234:	mov	w0, w19
   4a238:	ldp	x20, x19, [sp, #32]
   4a23c:	ldr	x21, [sp, #16]
   4a240:	ldp	x29, x30, [sp], #48
   4a244:	ret
   4a248:	stp	x29, x30, [sp, #-48]!
   4a24c:	stp	x20, x19, [sp, #32]
   4a250:	ldr	x8, [x0, #8]
   4a254:	mov	w19, w2
   4a258:	str	x21, [sp, #16]
   4a25c:	mov	x29, sp
   4a260:	cmp	x8, #0x2
   4a264:	b.cc	4a298 <__gmp_snprintf@@Base+0x25c>  // b.lo, b.ul, b.last
   4a268:	mov	x20, x0
   4a26c:	sub	x8, x8, #0x1
   4a270:	ldr	x0, [x0]
   4a274:	sxtw	x9, w19
   4a278:	cmp	x8, x9
   4a27c:	csel	x21, x8, x9, cc  // cc = lo, ul, last
   4a280:	mov	x2, x21
   4a284:	bl	c780 <memset@plt>
   4a288:	ldp	x8, x9, [x20]
   4a28c:	add	x8, x8, x21
   4a290:	sub	x9, x9, x21
   4a294:	stp	x8, x9, [x20]
   4a298:	mov	w0, w19
   4a29c:	ldp	x20, x19, [sp, #32]
   4a2a0:	ldr	x21, [sp, #16]
   4a2a4:	ldp	x29, x30, [sp], #48
   4a2a8:	ret
   4a2ac:	ldr	x8, [x0, #8]
   4a2b0:	cbz	x8, 4a2bc <__gmp_snprintf@@Base+0x280>
   4a2b4:	ldr	x8, [x0]
   4a2b8:	strb	wzr, [x8]
   4a2bc:	mov	w0, wzr
   4a2c0:	ret

000000000004a2c4 <__gmp_sprintf@@Base>:
   4a2c4:	sub	sp, sp, #0x120
   4a2c8:	stp	x29, x30, [sp, #256]
   4a2cc:	add	x29, sp, #0x100
   4a2d0:	mov	x10, #0xffffffffffffffd0    	// #-48
   4a2d4:	mov	x11, sp
   4a2d8:	add	x12, sp, #0x80
   4a2dc:	movk	x10, #0xff80, lsl #32
   4a2e0:	add	x13, x29, #0x20
   4a2e4:	add	x11, x11, #0x80
   4a2e8:	add	x12, x12, #0x30
   4a2ec:	sub	x9, x29, #0x28
   4a2f0:	stp	x11, x10, [x29, #-24]
   4a2f4:	stp	x13, x12, [x29, #-40]
   4a2f8:	stp	q1, q2, [sp, #16]
   4a2fc:	str	q0, [sp]
   4a300:	ldp	q0, q1, [x9]
   4a304:	str	x28, [sp, #272]
   4a308:	stp	x2, x3, [sp, #128]
   4a30c:	stp	x4, x5, [sp, #144]
   4a310:	stp	x6, x7, [sp, #160]
   4a314:	stp	q3, q4, [sp, #48]
   4a318:	stp	q5, q6, [sp, #80]
   4a31c:	str	q7, [sp, #112]
   4a320:	stur	x0, [x29, #-8]
   4a324:	stp	q0, q1, [x29, #-80]
   4a328:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a32c:	ldr	x0, [x0, #3944]
   4a330:	mov	x8, x1
   4a334:	sub	x1, x29, #0x8
   4a338:	sub	x3, x29, #0x50
   4a33c:	mov	x2, x8
   4a340:	bl	d200 <__gmp_doprnt@plt>
   4a344:	ldr	x28, [sp, #272]
   4a348:	ldp	x29, x30, [sp, #256]
   4a34c:	add	sp, sp, #0x120
   4a350:	ret
   4a354:	sub	sp, sp, #0x40
   4a358:	stp	x29, x30, [sp, #32]
   4a35c:	stp	x20, x19, [sp, #48]
   4a360:	ldr	x20, [x0]
   4a364:	ldp	q1, q0, [x2]
   4a368:	mov	x19, x0
   4a36c:	mov	x2, sp
   4a370:	mov	x0, x20
   4a374:	add	x29, sp, #0x20
   4a378:	stp	q1, q0, [sp]
   4a37c:	bl	d0e0 <vsprintf@plt>
   4a380:	mov	x0, x20
   4a384:	bl	c090 <strlen@plt>
   4a388:	add	x8, x20, w0, sxtw
   4a38c:	str	x8, [x19]
   4a390:	ldp	x20, x19, [sp, #48]
   4a394:	ldp	x29, x30, [sp, #32]
   4a398:	add	sp, sp, #0x40
   4a39c:	ret
   4a3a0:	stp	x29, x30, [sp, #-32]!
   4a3a4:	ldr	x8, [x0]
   4a3a8:	str	x19, [sp, #16]
   4a3ac:	mov	x29, sp
   4a3b0:	mov	x19, x2
   4a3b4:	add	x9, x8, x2
   4a3b8:	str	x9, [x0]
   4a3bc:	mov	x0, x8
   4a3c0:	bl	bff0 <memcpy@plt>
   4a3c4:	mov	w0, w19
   4a3c8:	ldr	x19, [sp, #16]
   4a3cc:	ldp	x29, x30, [sp], #32
   4a3d0:	ret
   4a3d4:	stp	x29, x30, [sp, #-32]!
   4a3d8:	ldr	x8, [x0]
   4a3dc:	str	x19, [sp, #16]
   4a3e0:	mov	w19, w2
   4a3e4:	sxtw	x2, w19
   4a3e8:	add	x9, x8, x2
   4a3ec:	str	x9, [x0]
   4a3f0:	mov	x0, x8
   4a3f4:	mov	x29, sp
   4a3f8:	bl	c780 <memset@plt>
   4a3fc:	mov	w0, w19
   4a400:	ldr	x19, [sp, #16]
   4a404:	ldp	x29, x30, [sp], #32
   4a408:	ret
   4a40c:	ldr	x8, [x0]
   4a410:	mov	w0, wzr
   4a414:	strb	wzr, [x8]
   4a418:	ret
   4a41c:	sub	sp, sp, #0x80
   4a420:	stp	x29, x30, [sp, #64]
   4a424:	str	x23, [sp, #80]
   4a428:	stp	x22, x21, [sp, #96]
   4a42c:	stp	x20, x19, [sp, #112]
   4a430:	adrp	x23, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a434:	ldr	x23, [x23, #3792]
   4a438:	mov	x20, x2
   4a43c:	mov	x21, x1
   4a440:	mov	x19, x0
   4a444:	mov	w8, #0x100                 	// #256
   4a448:	add	x29, sp, #0x40
   4a44c:	ldp	x9, x1, [x19, #16]
   4a450:	add	x8, x9, x8
   4a454:	cmp	x1, x8
   4a458:	b.hi	4a474 <__gmp_sprintf@@Base+0x1b0>  // b.pmore
   4a45c:	lsl	x2, x8, #1
   4a460:	str	x2, [x19, #24]
   4a464:	ldr	x8, [x23]
   4a468:	ldr	x0, [x19, #8]
   4a46c:	blr	x8
   4a470:	str	x0, [x19, #8]
   4a474:	ldp	q1, q0, [x20]
   4a478:	ldp	x9, x8, [x19, #16]
   4a47c:	mov	x3, sp
   4a480:	mov	x2, x21
   4a484:	stp	q1, q0, [sp, #32]
   4a488:	ldp	x10, x11, [x19, #8]
   4a48c:	sub	x22, x8, x9
   4a490:	mov	x1, x22
   4a494:	stp	q1, q0, [sp]
   4a498:	add	x0, x10, x11
   4a49c:	bl	d2e0 <vsnprintf@plt>
   4a4a0:	sub	w8, w22, #0x1
   4a4a4:	cmn	w0, #0x1
   4a4a8:	csel	w8, w8, w0, eq  // eq = none
   4a4ac:	sxtw	x0, w8
   4a4b0:	sub	x8, x22, #0x1
   4a4b4:	cmp	x8, x0
   4a4b8:	b.hi	4a4d4 <__gmp_sprintf@@Base+0x210>  // b.pmore
   4a4bc:	add	w10, w0, #0x2
   4a4c0:	lsl	x9, x22, #1
   4a4c4:	sxtw	x10, w10
   4a4c8:	cmp	x8, x0
   4a4cc:	csel	x8, x9, x10, eq  // eq = none
   4a4d0:	b	4a44c <__gmp_sprintf@@Base+0x188>
   4a4d4:	ldr	x8, [x19, #16]
   4a4d8:	ldr	x23, [sp, #80]
   4a4dc:	add	x8, x8, x0
   4a4e0:	str	x8, [x19, #16]
   4a4e4:	ldp	x20, x19, [sp, #112]
   4a4e8:	ldp	x22, x21, [sp, #96]
   4a4ec:	ldp	x29, x30, [sp, #64]
   4a4f0:	add	sp, sp, #0x80
   4a4f4:	ret

000000000004a4f8 <__gmp_vasprintf@@Base>:
   4a4f8:	sub	sp, sp, #0x60
   4a4fc:	stp	x29, x30, [sp, #64]
   4a500:	stp	x20, x19, [sp, #80]
   4a504:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a508:	ldr	x8, [x8, #3840]
   4a50c:	mov	w9, #0x100                 	// #256
   4a510:	str	x0, [sp, #32]
   4a514:	mov	w0, #0x100                 	// #256
   4a518:	ldr	x8, [x8]
   4a51c:	add	x29, sp, #0x40
   4a520:	mov	x19, x2
   4a524:	mov	x20, x1
   4a528:	str	x9, [sp, #56]
   4a52c:	blr	x8
   4a530:	stp	x0, xzr, [sp, #40]
   4a534:	ldp	q1, q0, [x19]
   4a538:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a53c:	add	x1, sp, #0x20
   4a540:	mov	x3, sp
   4a544:	stp	q1, q0, [sp]
   4a548:	ldr	x0, [x0, #3864]
   4a54c:	mov	x2, x20
   4a550:	bl	d200 <__gmp_doprnt@plt>
   4a554:	ldp	x20, x19, [sp, #80]
   4a558:	ldp	x29, x30, [sp, #64]
   4a55c:	add	sp, sp, #0x60
   4a560:	ret

000000000004a564 <__gmp_vfprintf@@Base>:
   4a564:	sub	sp, sp, #0x30
   4a568:	stp	x29, x30, [sp, #32]
   4a56c:	ldp	q1, q0, [x2]
   4a570:	mov	x8, x1
   4a574:	mov	x1, x0
   4a578:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a57c:	stp	q1, q0, [sp]
   4a580:	ldr	x0, [x0, #3816]
   4a584:	mov	x3, sp
   4a588:	mov	x2, x8
   4a58c:	add	x29, sp, #0x20
   4a590:	bl	d200 <__gmp_doprnt@plt>
   4a594:	ldp	x29, x30, [sp, #32]
   4a598:	add	sp, sp, #0x30
   4a59c:	ret

000000000004a5a0 <__gmp_vprintf@@Base>:
   4a5a0:	sub	sp, sp, #0x30
   4a5a4:	stp	x29, x30, [sp, #32]
   4a5a8:	ldp	q0, q1, [x1]
   4a5ac:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a5b0:	ldr	x8, [x8, #3856]
   4a5b4:	mov	x2, x0
   4a5b8:	stp	q0, q1, [sp]
   4a5bc:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a5c0:	ldr	x1, [x8]
   4a5c4:	ldr	x0, [x0, #3816]
   4a5c8:	mov	x3, sp
   4a5cc:	add	x29, sp, #0x20
   4a5d0:	bl	d200 <__gmp_doprnt@plt>
   4a5d4:	ldp	x29, x30, [sp, #32]
   4a5d8:	add	sp, sp, #0x30
   4a5dc:	ret

000000000004a5e0 <__gmp_vsnprintf@@Base>:
   4a5e0:	sub	sp, sp, #0x40
   4a5e4:	stp	x29, x30, [sp, #48]
   4a5e8:	add	x29, sp, #0x30
   4a5ec:	stp	x0, x1, [x29, #-16]
   4a5f0:	ldp	q1, q0, [x3]
   4a5f4:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a5f8:	sub	x1, x29, #0x10
   4a5fc:	mov	x3, sp
   4a600:	stp	q1, q0, [sp]
   4a604:	ldr	x0, [x0, #4024]
   4a608:	bl	d200 <__gmp_doprnt@plt>
   4a60c:	ldp	x29, x30, [sp, #48]
   4a610:	add	sp, sp, #0x40
   4a614:	ret

000000000004a618 <__gmp_vsprintf@@Base>:
   4a618:	sub	sp, sp, #0x40
   4a61c:	stp	x29, x30, [sp, #48]
   4a620:	add	x29, sp, #0x30
   4a624:	stur	x0, [x29, #-8]
   4a628:	ldp	q1, q0, [x2]
   4a62c:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a630:	mov	x8, x1
   4a634:	sub	x1, x29, #0x8
   4a638:	stp	q1, q0, [sp]
   4a63c:	ldr	x0, [x0, #3944]
   4a640:	mov	x3, sp
   4a644:	mov	x2, x8
   4a648:	bl	d200 <__gmp_doprnt@plt>
   4a64c:	ldp	x29, x30, [sp, #48]
   4a650:	add	sp, sp, #0x40
   4a654:	ret

000000000004a658 <__gmp_doscan@@Base>:
   4a658:	sub	sp, sp, #0xb0
   4a65c:	stp	x29, x30, [sp, #80]
   4a660:	stp	x28, x27, [sp, #96]
   4a664:	stp	x26, x25, [sp, #112]
   4a668:	stp	x24, x23, [sp, #128]
   4a66c:	stp	x22, x21, [sp, #144]
   4a670:	stp	x20, x19, [sp, #160]
   4a674:	ldp	q1, q0, [x3]
   4a678:	mov	x23, x0
   4a67c:	mov	x0, x2
   4a680:	add	x29, sp, #0x50
   4a684:	mov	x24, x2
   4a688:	mov	x22, x1
   4a68c:	stp	q1, q0, [sp, #32]
   4a690:	bl	c090 <strlen@plt>
   4a694:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4a698:	ldr	x8, [x8, #3840]
   4a69c:	add	x19, x0, #0x4
   4a6a0:	mov	x0, x19
   4a6a4:	ldr	x8, [x8]
   4a6a8:	blr	x8
   4a6ac:	ldrb	w25, [x24]
   4a6b0:	mov	x20, x0
   4a6b4:	str	x19, [sp, #8]
   4a6b8:	cbz	w25, 4aa7c <__gmp_doscan@@Base+0x424>
   4a6bc:	bl	cca0 <__ctype_b_loc@plt>
   4a6c0:	adrp	x28, 58000 <__gmp_limbroots_table@@Base+0x338>
   4a6c4:	mov	w27, wzr
   4a6c8:	mov	w21, wzr
   4a6cc:	add	x28, x28, #0x58
   4a6d0:	mov	w12, #0x1                   	// #1
   4a6d4:	mov	w19, #0xa                   	// #10
   4a6d8:	str	x0, [sp, #16]
   4a6dc:	b	4a700 <__gmp_doscan@@Base+0xa8>
   4a6e0:	add	w8, w26, w27
   4a6e4:	add	w27, w8, w0
   4a6e8:	ldur	w8, [x29, #-12]
   4a6ec:	ldrb	w25, [x24]
   4a6f0:	mov	w12, #0x1                   	// #1
   4a6f4:	cmp	w8, #0x0
   4a6f8:	cinc	w21, w21, eq  // eq = none
   4a6fc:	cbz	w25, 4aa80 <__gmp_doscan@@Base+0x428>
   4a700:	sxtw	x26, w27
   4a704:	ldr	x8, [sp, #16]
   4a708:	and	x9, x25, #0xff
   4a70c:	mov	x1, x24
   4a710:	add	x24, x24, #0x1
   4a714:	ldr	x8, [x8]
   4a718:	ldrh	w9, [x8, x9, lsl #1]
   4a71c:	tbnz	w9, #13, 4a96c <__gmp_doscan@@Base+0x314>
   4a720:	and	w9, w25, #0xff
   4a724:	cmp	w9, #0x25
   4a728:	b.ne	4a890 <__gmp_doscan@@Base+0x238>  // b.any
   4a72c:	stur	wzr, [x29, #-4]
   4a730:	stur	xzr, [x29, #-16]
   4a734:	sturb	wzr, [x29, #-8]
   4a738:	b	4a740 <__gmp_doscan@@Base+0xe8>
   4a73c:	sturb	w25, [x29, #-8]
   4a740:	mov	x9, x24
   4a744:	ldrb	w25, [x24], #1
   4a748:	cmp	w25, #0x7a
   4a74c:	b.hi	4a874 <__gmp_doscan@@Base+0x21c>  // b.pmore
   4a750:	adr	x10, 4a73c <__gmp_doscan@@Base+0xe4>
   4a754:	ldrb	w11, [x28, x25]
   4a758:	add	x10, x10, x11, lsl #2
   4a75c:	br	x10
   4a760:	mov	x24, x9
   4a764:	stur	wzr, [x29, #-4]
   4a768:	ldur	w9, [x29, #-4]
   4a76c:	mul	w9, w9, w19
   4a770:	add	w9, w9, w25, uxtb
   4a774:	sub	w9, w9, #0x30
   4a778:	stur	w9, [x29, #-4]
   4a77c:	ldrb	w25, [x24, #1]!
   4a780:	ldrh	w9, [x8, x25, lsl #1]
   4a784:	tbnz	w9, #11, 4a768 <__gmp_doscan@@Base+0x110>
   4a788:	b	4a740 <__gmp_doscan@@Base+0xe8>
   4a78c:	stur	w12, [x29, #-12]
   4a790:	b	4a740 <__gmp_doscan@@Base+0xe8>
   4a794:	ldurb	w9, [x29, #-8]
   4a798:	cmp	w9, #0x68
   4a79c:	b.ne	4a73c <__gmp_doscan@@Base+0xe4>  // b.any
   4a7a0:	mov	w9, #0x48                  	// #72
   4a7a4:	sturb	w9, [x29, #-8]
   4a7a8:	b	4a740 <__gmp_doscan@@Base+0xe8>
   4a7ac:	ldurb	w9, [x29, #-8]
   4a7b0:	cmp	w9, #0x6c
   4a7b4:	b.ne	4a73c <__gmp_doscan@@Base+0xe4>  // b.any
   4a7b8:	mov	w9, #0x4c                  	// #76
   4a7bc:	sturb	w9, [x29, #-8]
   4a7c0:	b	4a740 <__gmp_doscan@@Base+0xe8>
   4a7c4:	ldur	w8, [x29, #-12]
   4a7c8:	cbnz	w8, 4a874 <__gmp_doscan@@Base+0x21c>
   4a7cc:	ldrsw	x8, [sp, #56]
   4a7d0:	tbz	w8, #31, 4a7e4 <__gmp_doscan@@Base+0x18c>
   4a7d4:	add	w9, w8, #0x8
   4a7d8:	cmp	w9, #0x0
   4a7dc:	str	w9, [sp, #56]
   4a7e0:	b.le	4a824 <__gmp_doscan@@Base+0x1cc>
   4a7e4:	ldr	x8, [sp, #32]
   4a7e8:	add	x9, x8, #0x8
   4a7ec:	str	x9, [sp, #32]
   4a7f0:	ldurb	w9, [x29, #-8]
   4a7f4:	ldr	x0, [x8]
   4a7f8:	sub	w8, w9, #0x46
   4a7fc:	cmp	w8, #0x34
   4a800:	b.hi	4a830 <__gmp_doscan@@Base+0x1d8>  // b.pmore
   4a804:	adrp	x11, 58000 <__gmp_limbroots_table@@Base+0x338>
   4a808:	add	x11, x11, #0xd3
   4a80c:	adr	x9, 4a81c <__gmp_doscan@@Base+0x1c4>
   4a810:	ldrb	w10, [x11, x8]
   4a814:	add	x9, x9, x10, lsl #2
   4a818:	br	x9
   4a81c:	str	x26, [x0]
   4a820:	b	4a874 <__gmp_doscan@@Base+0x21c>
   4a824:	ldr	x9, [sp, #40]
   4a828:	add	x8, x9, x8
   4a82c:	b	4a7f0 <__gmp_doscan@@Base+0x198>
   4a830:	cbnz	w9, 4a874 <__gmp_doscan@@Base+0x21c>
   4a834:	str	w27, [x0]
   4a838:	b	4a874 <__gmp_doscan@@Base+0x21c>
   4a83c:	mov	x1, x26
   4a840:	bl	c7b0 <__gmpf_set_si@plt>
   4a844:	b	4a868 <__gmp_doscan@@Base+0x210>
   4a848:	strb	w27, [x0]
   4a84c:	b	4a874 <__gmp_doscan@@Base+0x21c>
   4a850:	mov	w2, #0x1                   	// #1
   4a854:	mov	x1, x26
   4a858:	bl	cd30 <__gmpq_set_si@plt>
   4a85c:	b	4a868 <__gmp_doscan@@Base+0x210>
   4a860:	mov	x1, x26
   4a864:	bl	d450 <__gmpz_set_si@plt>
   4a868:	mov	w12, #0x1                   	// #1
   4a86c:	b	4a874 <__gmp_doscan@@Base+0x21c>
   4a870:	strh	w27, [x0]
   4a874:	ldrb	w25, [x24]
   4a878:	cbnz	w25, 4a704 <__gmp_doscan@@Base+0xac>
   4a87c:	b	4aa80 <__gmp_doscan@@Base+0x428>
   4a880:	mov	w8, #0x10                  	// #16
   4a884:	b	4a8d4 <__gmp_doscan@@Base+0x27c>
   4a888:	stur	w19, [x29, #-16]
   4a88c:	b	4a8d8 <__gmp_doscan@@Base+0x280>
   4a890:	ldr	x8, [x23, #16]
   4a894:	mov	x0, x22
   4a898:	blr	x8
   4a89c:	cmp	w0, w25, uxtb
   4a8a0:	b.ne	4aac8 <__gmp_doscan@@Base+0x470>  // b.any
   4a8a4:	add	w27, w27, #0x1
   4a8a8:	mov	w12, #0x1                   	// #1
   4a8ac:	b	4a980 <__gmp_doscan@@Base+0x328>
   4a8b0:	ldrb	w8, [x9, #1]
   4a8b4:	cmp	w8, #0x5e
   4a8b8:	b.ne	4a98c <__gmp_doscan@@Base+0x334>  // b.any
   4a8bc:	ldrb	w8, [x9, #2]
   4a8c0:	add	x24, x9, #0x3
   4a8c4:	cmp	w8, #0x5d
   4a8c8:	b.eq	4a9a4 <__gmp_doscan@@Base+0x34c>  // b.none
   4a8cc:	b	4a998 <__gmp_doscan@@Base+0x340>
   4a8d0:	mov	w8, #0x8                   	// #8
   4a8d4:	stur	w8, [x29, #-16]
   4a8d8:	ldurb	w8, [x29, #-8]
   4a8dc:	sub	w8, w8, #0x46
   4a8e0:	cmp	w8, #0x14
   4a8e4:	b.hi	4a9b0 <__gmp_doscan@@Base+0x358>  // b.pmore
   4a8e8:	mov	w9, #0x801                 	// #2049
   4a8ec:	lsl	w8, w12, w8
   4a8f0:	movk	w9, #0x10, lsl #16
   4a8f4:	tst	w8, w9
   4a8f8:	b.eq	4a9b0 <__gmp_doscan@@Base+0x358>  // b.none
   4a8fc:	mov	x0, x23
   4a900:	mov	x1, x22
   4a904:	bl	4aae8 <__gmp_doscan@@Base+0x490>
   4a908:	ldur	w8, [x29, #-12]
   4a90c:	mov	w26, w0
   4a910:	cbz	w8, 4a91c <__gmp_doscan@@Base+0x2c4>
   4a914:	mov	x3, xzr
   4a918:	b	4a944 <__gmp_doscan@@Base+0x2ec>
   4a91c:	ldrsw	x8, [sp, #56]
   4a920:	tbz	w8, #31, 4a934 <__gmp_doscan@@Base+0x2dc>
   4a924:	add	w9, w8, #0x8
   4a928:	cmp	w9, #0x0
   4a92c:	str	w9, [sp, #56]
   4a930:	b.le	4aa64 <__gmp_doscan@@Base+0x40c>
   4a934:	ldr	x8, [sp, #32]
   4a938:	add	x9, x8, #0x8
   4a93c:	str	x9, [sp, #32]
   4a940:	ldr	x3, [x8]
   4a944:	sub	x2, x29, #0x10
   4a948:	mov	x0, x23
   4a94c:	mov	x1, x22
   4a950:	bl	4ab4c <__gmp_doscan@@Base+0x4f4>
   4a954:	cmn	w0, #0x2
   4a958:	str	w0, [sp, #28]
   4a95c:	b.eq	4aabc <__gmp_doscan@@Base+0x464>  // b.none
   4a960:	cmn	w0, #0x1
   4a964:	b.ne	4a6e0 <__gmp_doscan@@Base+0x88>  // b.any
   4a968:	b	4aa80 <__gmp_doscan@@Base+0x428>
   4a96c:	mov	x0, x23
   4a970:	mov	x1, x22
   4a974:	bl	4aae8 <__gmp_doscan@@Base+0x490>
   4a978:	mov	w12, #0x1                   	// #1
   4a97c:	add	w27, w0, w27
   4a980:	ldrb	w25, [x24]
   4a984:	cbnz	w25, 4a700 <__gmp_doscan@@Base+0xa8>
   4a988:	b	4aa80 <__gmp_doscan@@Base+0x428>
   4a98c:	add	x24, x9, #0x2
   4a990:	cmp	w8, #0x5d
   4a994:	b.eq	4a9a4 <__gmp_doscan@@Base+0x34c>  // b.none
   4a998:	cmp	w8, #0x5d
   4a99c:	b.eq	4a9b0 <__gmp_doscan@@Base+0x358>  // b.none
   4a9a0:	cbz	w8, 4aa80 <__gmp_doscan@@Base+0x428>
   4a9a4:	ldrb	w8, [x24], #1
   4a9a8:	cmp	w8, #0x5d
   4a9ac:	b.ne	4a9a0 <__gmp_doscan@@Base+0x348>  // b.any
   4a9b0:	sub	x26, x24, x1
   4a9b4:	mov	x0, x20
   4a9b8:	mov	x2, x26
   4a9bc:	bl	bff0 <memcpy@plt>
   4a9c0:	add	x8, x20, x26
   4a9c4:	mov	w9, #0x6e25                	// #28197
   4a9c8:	strh	w9, [x8]
   4a9cc:	strb	wzr, [x8, #2]
   4a9d0:	ldur	w8, [x29, #-12]
   4a9d4:	mov	w9, #0xffffffff            	// #-1
   4a9d8:	str	w9, [sp, #28]
   4a9dc:	cbz	w8, 4a9fc <__gmp_doscan@@Base+0x3a4>
   4a9e0:	ldr	x8, [x23]
   4a9e4:	add	x2, sp, #0x1c
   4a9e8:	mov	x0, x22
   4a9ec:	mov	x1, x20
   4a9f0:	mov	x3, xzr
   4a9f4:	blr	x8
   4a9f8:	b	4aa3c <__gmp_doscan@@Base+0x3e4>
   4a9fc:	ldrsw	x8, [sp, #56]
   4aa00:	tbz	w8, #31, 4aa14 <__gmp_doscan@@Base+0x3bc>
   4aa04:	add	w9, w8, #0x8
   4aa08:	cmp	w9, #0x0
   4aa0c:	str	w9, [sp, #56]
   4aa10:	b.le	4aa70 <__gmp_doscan@@Base+0x418>
   4aa14:	ldr	x8, [sp, #32]
   4aa18:	add	x9, x8, #0x8
   4aa1c:	str	x9, [sp, #32]
   4aa20:	ldr	x2, [x8]
   4aa24:	ldr	x8, [x23]
   4aa28:	add	x3, sp, #0x1c
   4aa2c:	mov	x0, x22
   4aa30:	mov	x1, x20
   4aa34:	blr	x8
   4aa38:	cbz	w0, 4aa80 <__gmp_doscan@@Base+0x428>
   4aa3c:	cmn	w0, #0x1
   4aa40:	b.eq	4aabc <__gmp_doscan@@Base+0x464>  // b.none
   4aa44:	ldr	w1, [sp, #28]
   4aa48:	cmn	w1, #0x1
   4aa4c:	b.eq	4aa80 <__gmp_doscan@@Base+0x428>  // b.none
   4aa50:	ldr	x8, [x23, #8]
   4aa54:	mov	x0, x22
   4aa58:	add	w27, w1, w27
   4aa5c:	blr	x8
   4aa60:	b	4a6e8 <__gmp_doscan@@Base+0x90>
   4aa64:	ldr	x9, [sp, #40]
   4aa68:	add	x8, x9, x8
   4aa6c:	b	4a940 <__gmp_doscan@@Base+0x2e8>
   4aa70:	ldr	x9, [sp, #40]
   4aa74:	add	x8, x9, x8
   4aa78:	b	4aa20 <__gmp_doscan@@Base+0x3c8>
   4aa7c:	mov	w21, wzr
   4aa80:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4aa84:	ldr	x8, [x8, #4016]
   4aa88:	ldr	x1, [sp, #8]
   4aa8c:	mov	x0, x20
   4aa90:	ldr	x8, [x8]
   4aa94:	blr	x8
   4aa98:	mov	w0, w21
   4aa9c:	ldp	x20, x19, [sp, #160]
   4aaa0:	ldp	x22, x21, [sp, #144]
   4aaa4:	ldp	x24, x23, [sp, #128]
   4aaa8:	ldp	x26, x25, [sp, #112]
   4aaac:	ldp	x28, x27, [sp, #96]
   4aab0:	ldp	x29, x30, [sp, #80]
   4aab4:	add	sp, sp, #0xb0
   4aab8:	ret
   4aabc:	cbnz	w21, 4aa80 <__gmp_doscan@@Base+0x428>
   4aac0:	mov	w21, #0xffffffff            	// #-1
   4aac4:	b	4aa80 <__gmp_doscan@@Base+0x428>
   4aac8:	ldr	x8, [x23, #24]
   4aacc:	mov	x1, x22
   4aad0:	mov	w26, w0
   4aad4:	blr	x8
   4aad8:	cbnz	w21, 4aa80 <__gmp_doscan@@Base+0x428>
   4aadc:	cmn	w26, #0x1
   4aae0:	b.eq	4aac0 <__gmp_doscan@@Base+0x468>  // b.none
   4aae4:	b	4aa80 <__gmp_doscan@@Base+0x428>
   4aae8:	stp	x29, x30, [sp, #-48]!
   4aaec:	stp	x22, x21, [sp, #16]
   4aaf0:	stp	x20, x19, [sp, #32]
   4aaf4:	mov	x20, x1
   4aaf8:	mov	x21, x0
   4aafc:	mov	w19, #0xffffffff            	// #-1
   4ab00:	mov	x29, sp
   4ab04:	ldr	x8, [x21, #16]
   4ab08:	mov	x0, x20
   4ab0c:	blr	x8
   4ab10:	mov	w22, w0
   4ab14:	bl	cca0 <__ctype_b_loc@plt>
   4ab18:	ldr	x8, [x0]
   4ab1c:	add	w19, w19, #0x1
   4ab20:	ldrh	w8, [x8, w22, sxtw #1]
   4ab24:	tbnz	w8, #13, 4ab04 <__gmp_doscan@@Base+0x4ac>
   4ab28:	ldr	x8, [x21, #24]
   4ab2c:	mov	w0, w22
   4ab30:	mov	x1, x20
   4ab34:	blr	x8
   4ab38:	mov	w0, w19
   4ab3c:	ldp	x20, x19, [sp, #32]
   4ab40:	ldp	x22, x21, [sp, #16]
   4ab44:	ldp	x29, x30, [sp], #48
   4ab48:	ret
   4ab4c:	sub	sp, sp, #0xb0
   4ab50:	stp	x29, x30, [sp, #80]
   4ab54:	stp	x28, x27, [sp, #96]
   4ab58:	stp	x26, x25, [sp, #112]
   4ab5c:	stp	x24, x23, [sp, #128]
   4ab60:	stp	x22, x21, [sp, #144]
   4ab64:	stp	x20, x19, [sp, #160]
   4ab68:	ldr	x8, [x0, #16]
   4ab6c:	mov	x19, x0
   4ab70:	mov	x0, x1
   4ab74:	add	x29, sp, #0x50
   4ab78:	str	x3, [sp, #16]
   4ab7c:	mov	x26, x2
   4ab80:	mov	x28, x1
   4ab84:	blr	x8
   4ab88:	cmn	w0, #0x1
   4ab8c:	b.eq	4b064 <__gmp_doscan@@Base+0xa0c>  // b.none
   4ab90:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4ab94:	ldr	w8, [x26, #12]
   4ab98:	ldr	w24, [x26]
   4ab9c:	ldr	x9, [x9, #3840]
   4aba0:	mov	w10, #0x7ffffffe            	// #2147483646
   4aba4:	cmp	w8, #0x0
   4aba8:	mov	w23, w0
   4abac:	ldr	x9, [x9]
   4abb0:	csel	w8, w10, w8, eq  // eq = none
   4abb4:	mov	w0, #0x200                 	// #512
   4abb8:	stur	w8, [x29, #-12]
   4abbc:	mov	w22, #0x200                 	// #512
   4abc0:	blr	x9
   4abc4:	mov	x21, x0
   4abc8:	mov	x27, xzr
   4abcc:	mov	w8, #0x1                   	// #1
   4abd0:	mov	w20, #0x8                   	// #8
   4abd4:	mov	w25, #0x1                   	// #1
   4abd8:	str	xzr, [sp]
   4abdc:	str	wzr, [sp, #12]
   4abe0:	stp	w8, wzr, [sp, #24]
   4abe4:	stur	x19, [x29, #-24]
   4abe8:	str	x26, [sp, #32]
   4abec:	cmp	w23, #0x2b
   4abf0:	b.eq	4ac40 <__gmp_doscan@@Base+0x5e8>  // b.none
   4abf4:	cmp	w23, #0x2d
   4abf8:	b.ne	4ac60 <__gmp_doscan@@Base+0x608>  // b.any
   4abfc:	cmp	x27, x22
   4ac00:	b.cc	4ac30 <__gmp_doscan@@Base+0x5d8>  // b.lo, b.ul, b.last
   4ac04:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4ac08:	ldr	x8, [x8, #3792]
   4ac0c:	add	x26, x22, #0x200
   4ac10:	mov	x0, x21
   4ac14:	mov	x1, x22
   4ac18:	ldr	x8, [x8]
   4ac1c:	mov	x2, x26
   4ac20:	blr	x8
   4ac24:	mov	x22, x26
   4ac28:	ldr	x26, [sp, #32]
   4ac2c:	mov	x21, x0
   4ac30:	add	x8, x27, #0x1
   4ac34:	mov	w9, #0x2d                  	// #45
   4ac38:	strb	w9, [x21, x27]
   4ac3c:	mov	x27, x8
   4ac40:	ldur	w8, [x29, #-12]
   4ac44:	cmp	w25, w8
   4ac48:	add	w25, w25, #0x1
   4ac4c:	b.ge	4b144 <__gmp_doscan@@Base+0xaec>  // b.tcont
   4ac50:	ldr	x8, [x19, #16]
   4ac54:	mov	x0, x28
   4ac58:	blr	x8
   4ac5c:	mov	w23, w0
   4ac60:	cbz	w24, 4ae30 <__gmp_doscan@@Base+0x7d8>
   4ac64:	stur	wzr, [x29, #-36]
   4ac68:	bl	cca0 <__ctype_b_loc@plt>
   4ac6c:	ldur	w9, [x29, #-36]
   4ac70:	mov	x26, x0
   4ac74:	mov	x20, x27
   4ac78:	ldr	x8, [x26]
   4ac7c:	cmp	w24, #0x10
   4ac80:	ldrh	w8, [x8, w23, sxtw #1]
   4ac84:	b.ne	4ac90 <__gmp_doscan@@Base+0x638>  // b.any
   4ac88:	tbnz	w8, #12, 4aca8 <__gmp_doscan@@Base+0x650>
   4ac8c:	b	4ad0c <__gmp_doscan@@Base+0x6b4>
   4ac90:	tbz	w8, #11, 4ad0c <__gmp_doscan@@Base+0x6b4>
   4ac94:	cmp	w24, #0x8
   4ac98:	b.ne	4aca8 <__gmp_doscan@@Base+0x650>  // b.any
   4ac9c:	orr	w8, w23, #0x1
   4aca0:	cmp	w8, #0x39
   4aca4:	b.eq	4ad0c <__gmp_doscan@@Base+0x6b4>  // b.none
   4aca8:	cmp	x20, x22
   4acac:	b.cc	4acd8 <__gmp_doscan@@Base+0x680>  // b.lo, b.ul, b.last
   4acb0:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4acb4:	ldr	x8, [x8, #3792]
   4acb8:	add	x27, x22, #0x200
   4acbc:	mov	x0, x21
   4acc0:	mov	x1, x22
   4acc4:	ldr	x8, [x8]
   4acc8:	mov	x2, x27
   4accc:	blr	x8
   4acd0:	mov	x21, x0
   4acd4:	mov	x22, x27
   4acd8:	ldur	w8, [x29, #-12]
   4acdc:	add	x27, x20, #0x1
   4ace0:	strb	w23, [x21, x20]
   4ace4:	cmp	w25, w8
   4ace8:	add	w25, w25, #0x1
   4acec:	b.ge	4b048 <__gmp_doscan@@Base+0x9f0>  // b.tcont
   4acf0:	ldr	x8, [x19, #16]
   4acf4:	mov	x0, x28
   4acf8:	blr	x8
   4acfc:	mov	w23, w0
   4ad00:	mov	w9, #0x1                   	// #1
   4ad04:	mov	x20, x27
   4ad08:	b	4ac78 <__gmp_doscan@@Base+0x620>
   4ad0c:	ldr	w8, [sp, #24]
   4ad10:	cbz	w8, 4b13c <__gmp_doscan@@Base+0xae4>
   4ad14:	ldr	w8, [sp, #28]
   4ad18:	stur	w9, [x29, #-36]
   4ad1c:	cbnz	w8, 4aef4 <__gmp_doscan@@Base+0x89c>
   4ad20:	ldr	x8, [sp, #32]
   4ad24:	ldrb	w8, [x8, #8]
   4ad28:	cmp	w8, #0x46
   4ad2c:	b.ne	4aef4 <__gmp_doscan@@Base+0x89c>  // b.any
   4ad30:	mov	w0, #0x10000               	// #65536
   4ad34:	bl	c560 <nl_langinfo@plt>
   4ad38:	ldrb	w8, [x0]
   4ad3c:	cmp	w23, w8
   4ad40:	b.ne	4add4 <__gmp_doscan@@Base+0x77c>  // b.any
   4ad44:	mov	x8, xzr
   4ad48:	add	x19, x0, #0x1
   4ad4c:	stur	x28, [x29, #-32]
   4ad50:	mov	x28, x8
   4ad54:	add	x8, x20, x8
   4ad58:	cmp	x8, x22
   4ad5c:	b.cc	4ad88 <__gmp_doscan@@Base+0x730>  // b.lo, b.ul, b.last
   4ad60:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4ad64:	ldr	x8, [x8, #3792]
   4ad68:	add	x27, x22, #0x200
   4ad6c:	mov	x0, x21
   4ad70:	mov	x1, x22
   4ad74:	ldr	x8, [x8]
   4ad78:	mov	x2, x27
   4ad7c:	blr	x8
   4ad80:	mov	x21, x0
   4ad84:	mov	x22, x27
   4ad88:	ldur	w9, [x29, #-12]
   4ad8c:	add	x8, x21, x20
   4ad90:	add	w27, w25, #0x1
   4ad94:	strb	w23, [x8, x28]
   4ad98:	cmp	w25, w9
   4ad9c:	b.ge	4ade0 <__gmp_doscan@@Base+0x788>  // b.tcont
   4ada0:	ldp	x0, x8, [x29, #-32]
   4ada4:	ldr	x8, [x8, #16]
   4ada8:	blr	x8
   4adac:	ldrb	w8, [x19, x28]
   4adb0:	mov	w23, w0
   4adb4:	cbz	w8, 4adec <__gmp_doscan@@Base+0x794>
   4adb8:	cmp	w23, w8
   4adbc:	add	x8, x28, #0x1
   4adc0:	mov	w25, w27
   4adc4:	b.eq	4ad50 <__gmp_doscan@@Base+0x6f8>  // b.none
   4adc8:	mov	w8, #0x34                  	// #52
   4adcc:	str	wzr, [sp, #28]
   4add0:	b	4adf8 <__gmp_doscan@@Base+0x7a0>
   4add4:	mov	w8, wzr
   4add8:	str	wzr, [sp, #28]
   4addc:	b	4ae08 <__gmp_doscan@@Base+0x7b0>
   4ade0:	str	wzr, [sp, #28]
   4ade4:	mov	w8, #0xe                   	// #14
   4ade8:	b	4adf8 <__gmp_doscan@@Base+0x7a0>
   4adec:	mov	w8, #0x1                   	// #1
   4adf0:	str	w8, [sp, #28]
   4adf4:	mov	w8, #0x1f                  	// #31
   4adf8:	add	x9, x20, x28
   4adfc:	ldp	x28, x19, [x29, #-32]
   4ae00:	add	x20, x9, #0x1
   4ae04:	mov	w25, w27
   4ae08:	ldur	w9, [x29, #-36]
   4ae0c:	cmp	w8, #0x1f
   4ae10:	b.eq	4ac78 <__gmp_doscan@@Base+0x620>  // b.none
   4ae14:	cmp	w8, #0x34
   4ae18:	b.eq	4b1a0 <__gmp_doscan@@Base+0xb48>  // b.none
   4ae1c:	cmp	w8, #0xe
   4ae20:	b.eq	4b13c <__gmp_doscan@@Base+0xae4>  // b.none
   4ae24:	ldr	x26, [sp, #32]
   4ae28:	cbz	w8, 4aef8 <__gmp_doscan@@Base+0x8a0>
   4ae2c:	b	4b064 <__gmp_doscan@@Base+0xa0c>
   4ae30:	cmp	w23, #0x30
   4ae34:	b.ne	4aed0 <__gmp_doscan@@Base+0x878>  // b.any
   4ae38:	ldrb	w8, [x26, #8]
   4ae3c:	cmp	w8, #0x46
   4ae40:	mov	w8, #0xa                   	// #10
   4ae44:	csel	w24, w8, w20, eq  // eq = none
   4ae48:	cmp	x27, x22
   4ae4c:	b.cc	4ae78 <__gmp_doscan@@Base+0x820>  // b.lo, b.ul, b.last
   4ae50:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4ae54:	ldr	x8, [x8, #3792]
   4ae58:	add	x23, x22, #0x200
   4ae5c:	mov	x0, x21
   4ae60:	mov	x1, x22
   4ae64:	ldr	x8, [x8]
   4ae68:	mov	x2, x23
   4ae6c:	blr	x8
   4ae70:	mov	x21, x0
   4ae74:	mov	x22, x23
   4ae78:	ldur	w8, [x29, #-12]
   4ae7c:	add	x20, x27, #0x1
   4ae80:	mov	w23, #0x30                  	// #48
   4ae84:	add	w19, w25, #0x1
   4ae88:	cmp	w25, w8
   4ae8c:	strb	w23, [x21, x27]
   4ae90:	b.ge	4b180 <__gmp_doscan@@Base+0xb28>  // b.tcont
   4ae94:	ldur	x8, [x29, #-24]
   4ae98:	mov	x0, x28
   4ae9c:	ldr	x8, [x8, #16]
   4aea0:	blr	x8
   4aea4:	orr	w8, w0, #0x20
   4aea8:	mov	w23, w0
   4aeac:	cmp	w8, #0x78
   4aeb0:	b.ne	4aedc <__gmp_doscan@@Base+0x884>  // b.any
   4aeb4:	ldrb	w8, [x26, #8]
   4aeb8:	cmp	w8, #0x46
   4aebc:	b.ne	4af24 <__gmp_doscan@@Base+0x8cc>  // b.any
   4aec0:	ldur	x19, [x29, #-24]
   4aec4:	mov	w8, #0x1                   	// #1
   4aec8:	str	w8, [sp, #12]
   4aecc:	b	4af68 <__gmp_doscan@@Base+0x910>
   4aed0:	stur	wzr, [x29, #-36]
   4aed4:	mov	w24, #0xa                   	// #10
   4aed8:	b	4ac68 <__gmp_doscan@@Base+0x610>
   4aedc:	mov	w25, w19
   4aee0:	ldur	x19, [x29, #-24]
   4aee4:	mov	w8, #0x1                   	// #1
   4aee8:	stur	w8, [x29, #-36]
   4aeec:	mov	x27, x20
   4aef0:	b	4ac68 <__gmp_doscan@@Base+0x610>
   4aef4:	ldr	x26, [sp, #32]
   4aef8:	ldrb	w8, [x26, #8]
   4aefc:	cmp	w8, #0x46
   4af00:	b.ne	4afb0 <__gmp_doscan@@Base+0x958>  // b.any
   4af04:	ldr	w10, [sp, #12]
   4af08:	orr	w9, w23, #0x20
   4af0c:	cbz	w10, 4af94 <__gmp_doscan@@Base+0x93c>
   4af10:	cmp	w9, #0x70
   4af14:	b.ne	4af94 <__gmp_doscan@@Base+0x93c>  // b.any
   4af18:	mov	w24, #0xa                   	// #10
   4af1c:	str	x20, [sp]
   4af20:	b	4afa4 <__gmp_doscan@@Base+0x94c>
   4af24:	cmp	x20, x22
   4af28:	b.cc	4af58 <__gmp_doscan@@Base+0x900>  // b.lo, b.ul, b.last
   4af2c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4af30:	ldr	x8, [x8, #3792]
   4af34:	add	x26, x22, #0x200
   4af38:	mov	x0, x21
   4af3c:	mov	x1, x22
   4af40:	ldr	x8, [x8]
   4af44:	mov	x2, x26
   4af48:	blr	x8
   4af4c:	mov	x22, x26
   4af50:	ldr	x26, [sp, #32]
   4af54:	mov	x21, x0
   4af58:	ldur	x19, [x29, #-24]
   4af5c:	add	x8, x27, #0x2
   4af60:	strb	w23, [x21, x20]
   4af64:	mov	x20, x8
   4af68:	ldur	w8, [x29, #-12]
   4af6c:	add	w25, w25, #0x2
   4af70:	cmp	w25, w8
   4af74:	b.gt	4b190 <__gmp_doscan@@Base+0xb38>
   4af78:	ldr	x8, [x19, #16]
   4af7c:	mov	x0, x28
   4af80:	blr	x8
   4af84:	mov	w23, w0
   4af88:	mov	w24, #0x10                  	// #16
   4af8c:	mov	x27, x20
   4af90:	b	4ac64 <__gmp_doscan@@Base+0x60c>
   4af94:	cmp	w9, #0x65
   4af98:	b.ne	4afb0 <__gmp_doscan@@Base+0x958>  // b.any
   4af9c:	ldr	w9, [sp, #12]
   4afa0:	cbnz	w9, 4afb0 <__gmp_doscan@@Base+0x958>
   4afa4:	ldur	w9, [x29, #-36]
   4afa8:	cbnz	w9, 4afd0 <__gmp_doscan@@Base+0x978>
   4afac:	b	4b1a0 <__gmp_doscan@@Base+0xb48>
   4afb0:	cmp	w23, #0x2f
   4afb4:	b.ne	4b20c <__gmp_doscan@@Base+0xbb4>  // b.any
   4afb8:	ldur	w9, [x29, #-36]
   4afbc:	cmp	w8, #0x51
   4afc0:	b.ne	4b210 <__gmp_doscan@@Base+0xbb8>  // b.any
   4afc4:	cbz	w9, 4b19c <__gmp_doscan@@Base+0xb44>
   4afc8:	ldr	w24, [x26]
   4afcc:	mov	w9, wzr
   4afd0:	cmp	x20, x22
   4afd4:	b.cc	4b00c <__gmp_doscan@@Base+0x9b4>  // b.lo, b.ul, b.last
   4afd8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4afdc:	ldr	x8, [x8, #3792]
   4afe0:	add	x26, x22, #0x200
   4afe4:	mov	x0, x21
   4afe8:	mov	x1, x22
   4afec:	ldr	x8, [x8]
   4aff0:	mov	x2, x26
   4aff4:	mov	w21, w9
   4aff8:	blr	x8
   4affc:	mov	x22, x26
   4b000:	ldr	x26, [sp, #32]
   4b004:	mov	w9, w21
   4b008:	mov	x21, x0
   4b00c:	ldur	w8, [x29, #-12]
   4b010:	add	x27, x20, #0x1
   4b014:	strb	w23, [x21, x20]
   4b018:	cmp	w25, w8
   4b01c:	add	w25, w25, #0x1
   4b020:	b.ge	4b050 <__gmp_doscan@@Base+0x9f8>  // b.tcont
   4b024:	ldr	x8, [x19, #16]
   4b028:	mov	x0, x28
   4b02c:	blr	x8
   4b030:	mov	w23, w0
   4b034:	str	wzr, [sp, #24]
   4b038:	mov	w20, #0x8                   	// #8
   4b03c:	cmp	w23, #0x2b
   4b040:	b.ne	4abf4 <__gmp_doscan@@Base+0x59c>  // b.any
   4b044:	b	4ac40 <__gmp_doscan@@Base+0x5e8>
   4b048:	mov	w9, #0x1                   	// #1
   4b04c:	ldr	x26, [sp, #32]
   4b050:	cbz	w9, 4b1a0 <__gmp_doscan@@Base+0xb48>
   4b054:	ldr	w8, [x26, #4]
   4b058:	cbz	w8, 4b06c <__gmp_doscan@@Base+0xa14>
   4b05c:	mov	w19, wzr
   4b060:	b	4b1a4 <__gmp_doscan@@Base+0xb4c>
   4b064:	mov	w0, #0xfffffffe            	// #-2
   4b068:	b	4b1ec <__gmp_doscan@@Base+0xb94>
   4b06c:	cmp	x27, x22
   4b070:	b.cc	4b0a0 <__gmp_doscan@@Base+0xa48>  // b.lo, b.ul, b.last
   4b074:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b078:	ldr	x8, [x8, #3792]
   4b07c:	add	x26, x22, #0x200
   4b080:	mov	x0, x21
   4b084:	mov	x1, x22
   4b088:	ldr	x8, [x8]
   4b08c:	mov	x2, x26
   4b090:	blr	x8
   4b094:	mov	x22, x26
   4b098:	ldr	x26, [sp, #32]
   4b09c:	mov	x21, x0
   4b0a0:	strb	wzr, [x21, x27]
   4b0a4:	ldrb	w8, [x26, #8]
   4b0a8:	ldr	x0, [sp, #16]
   4b0ac:	cmp	w8, #0x5a
   4b0b0:	b.eq	4b11c <__gmp_doscan@@Base+0xac4>  // b.none
   4b0b4:	cmp	w8, #0x51
   4b0b8:	b.eq	4b12c <__gmp_doscan@@Base+0xad4>  // b.none
   4b0bc:	cmp	w8, #0x46
   4b0c0:	b.ne	4b05c <__gmp_doscan@@Base+0xa04>  // b.any
   4b0c4:	ldr	x8, [sp]
   4b0c8:	cbz	x8, 4b14c <__gmp_doscan@@Base+0xaf4>
   4b0cc:	add	x24, x21, x8
   4b0d0:	ldr	w8, [sp, #12]
   4b0d4:	mov	w9, #0xa                   	// #10
   4b0d8:	mov	x1, x21
   4b0dc:	strb	wzr, [x24], #1
   4b0e0:	cmp	w8, #0x0
   4b0e4:	mov	w8, #0x10                  	// #16
   4b0e8:	csel	w2, w9, w8, eq  // eq = none
   4b0ec:	mov	x19, x0
   4b0f0:	bl	c310 <__gmpf_set_str@plt>
   4b0f4:	sub	x1, x29, #0x8
   4b0f8:	mov	w2, #0xa                   	// #10
   4b0fc:	mov	x0, x24
   4b100:	bl	cd20 <strtol@plt>
   4b104:	mov	x2, x0
   4b108:	tbnz	x0, #63, 4b16c <__gmp_doscan@@Base+0xb14>
   4b10c:	mov	x0, x19
   4b110:	mov	x1, x19
   4b114:	bl	cf60 <__gmpf_mul_2exp@plt>
   4b118:	b	4b05c <__gmp_doscan@@Base+0xa04>
   4b11c:	ldr	w2, [x26]
   4b120:	mov	x1, x21
   4b124:	bl	c210 <__gmpz_set_str@plt>
   4b128:	b	4b05c <__gmp_doscan@@Base+0xa04>
   4b12c:	ldr	w2, [x26]
   4b130:	mov	x1, x21
   4b134:	bl	c110 <__gmpq_set_str@plt>
   4b138:	b	4b05c <__gmp_doscan@@Base+0xa04>
   4b13c:	mov	x27, x20
   4b140:	b	4b04c <__gmp_doscan@@Base+0x9f4>
   4b144:	mov	w9, wzr
   4b148:	b	4b050 <__gmp_doscan@@Base+0x9f8>
   4b14c:	ldr	w8, [sp, #12]
   4b150:	mov	w9, #0xa                   	// #10
   4b154:	mov	x1, x21
   4b158:	cmp	w8, #0x0
   4b15c:	mov	w8, #0x10                  	// #16
   4b160:	csel	w2, w9, w8, eq  // eq = none
   4b164:	bl	c310 <__gmpf_set_str@plt>
   4b168:	b	4b05c <__gmp_doscan@@Base+0xa04>
   4b16c:	neg	x2, x2
   4b170:	mov	x0, x19
   4b174:	mov	x1, x19
   4b178:	bl	d6a0 <__gmpf_div_2exp@plt>
   4b17c:	b	4b05c <__gmp_doscan@@Base+0xa04>
   4b180:	mov	w9, #0x1                   	// #1
   4b184:	mov	x27, x20
   4b188:	mov	w25, w19
   4b18c:	b	4b050 <__gmp_doscan@@Base+0x9f8>
   4b190:	mov	w9, wzr
   4b194:	mov	x27, x20
   4b198:	b	4b050 <__gmp_doscan@@Base+0x9f8>
   4b19c:	mov	w23, #0x2f                  	// #47
   4b1a0:	mov	w19, #0x1                   	// #1
   4b1a4:	ldur	w8, [x29, #-12]
   4b1a8:	add	w8, w8, #0x1
   4b1ac:	cmp	w25, w8
   4b1b0:	b.eq	4b1c8 <__gmp_doscan@@Base+0xb70>  // b.none
   4b1b4:	ldur	x8, [x29, #-24]
   4b1b8:	mov	w0, w23
   4b1bc:	mov	x1, x28
   4b1c0:	ldr	x8, [x8, #24]
   4b1c4:	blr	x8
   4b1c8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b1cc:	ldr	x8, [x8, #4016]
   4b1d0:	mov	x0, x21
   4b1d4:	mov	x1, x22
   4b1d8:	ldr	x8, [x8]
   4b1dc:	blr	x8
   4b1e0:	sub	w8, w25, #0x1
   4b1e4:	cmp	w19, #0x0
   4b1e8:	csinv	w0, w8, wzr, eq  // eq = none
   4b1ec:	ldp	x20, x19, [sp, #160]
   4b1f0:	ldp	x22, x21, [sp, #144]
   4b1f4:	ldp	x24, x23, [sp, #128]
   4b1f8:	ldp	x26, x25, [sp, #112]
   4b1fc:	ldp	x28, x27, [sp, #96]
   4b200:	ldp	x29, x30, [sp, #80]
   4b204:	add	sp, sp, #0xb0
   4b208:	ret
   4b20c:	ldur	w9, [x29, #-36]
   4b210:	mov	x27, x20
   4b214:	b	4b050 <__gmp_doscan@@Base+0x9f8>

000000000004b218 <__gmp_fscanf@@Base>:
   4b218:	sub	sp, sp, #0x100
   4b21c:	stp	x29, x30, [sp, #240]
   4b220:	add	x29, sp, #0xf0
   4b224:	mov	x9, #0xffffffffffffffd0    	// #-48
   4b228:	mov	x10, sp
   4b22c:	sub	x11, x29, #0x70
   4b230:	movk	x9, #0xff80, lsl #32
   4b234:	add	x12, x29, #0x10
   4b238:	add	x10, x10, #0x80
   4b23c:	add	x11, x11, #0x30
   4b240:	stp	x10, x9, [x29, #-16]
   4b244:	stp	x12, x11, [x29, #-32]
   4b248:	stp	x2, x3, [x29, #-112]
   4b24c:	stp	x4, x5, [x29, #-96]
   4b250:	stp	x6, x7, [x29, #-80]
   4b254:	stp	q1, q2, [sp, #16]
   4b258:	str	q0, [sp]
   4b25c:	ldp	q0, q1, [x29, #-32]
   4b260:	mov	x8, x1
   4b264:	mov	x1, x0
   4b268:	stp	q3, q4, [sp, #48]
   4b26c:	stp	q5, q6, [sp, #80]
   4b270:	str	q7, [sp, #112]
   4b274:	stp	q0, q1, [x29, #-64]
   4b278:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b27c:	ldr	x0, [x0, #4064]
   4b280:	sub	x3, x29, #0x40
   4b284:	mov	x2, x8
   4b288:	bl	c230 <__gmp_doscan@plt>
   4b28c:	ldp	x29, x30, [sp, #240]
   4b290:	add	sp, sp, #0x100
   4b294:	ret
   4b298:	ret

000000000004b29c <__gmp_scanf@@Base>:
   4b29c:	sub	sp, sp, #0x120
   4b2a0:	stp	x29, x30, [sp, #256]
   4b2a4:	add	x29, sp, #0x100
   4b2a8:	mov	x9, #0xffffffffffffffc8    	// #-56
   4b2ac:	mov	x10, sp
   4b2b0:	sub	x11, x29, #0x78
   4b2b4:	str	x28, [sp, #272]
   4b2b8:	stp	x1, x2, [x29, #-120]
   4b2bc:	stp	x3, x4, [x29, #-104]
   4b2c0:	stp	x5, x6, [x29, #-88]
   4b2c4:	stur	x7, [x29, #-72]
   4b2c8:	stp	q0, q1, [sp]
   4b2cc:	stp	q2, q3, [sp, #32]
   4b2d0:	stp	q4, q5, [sp, #64]
   4b2d4:	movk	x9, #0xff80, lsl #32
   4b2d8:	add	x12, x29, #0x20
   4b2dc:	adrp	x13, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b2e0:	add	x10, x10, #0x80
   4b2e4:	add	x11, x11, #0x38
   4b2e8:	ldr	x13, [x13, #3888]
   4b2ec:	stp	x10, x9, [x29, #-16]
   4b2f0:	stp	x12, x11, [x29, #-32]
   4b2f4:	ldp	q0, q1, [x29, #-32]
   4b2f8:	mov	x8, x0
   4b2fc:	stp	q6, q7, [sp, #96]
   4b300:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b304:	stp	q0, q1, [x29, #-64]
   4b308:	ldr	x1, [x13]
   4b30c:	ldr	x0, [x0, #4064]
   4b310:	sub	x3, x29, #0x40
   4b314:	mov	x2, x8
   4b318:	bl	c230 <__gmp_doscan@plt>
   4b31c:	ldr	x28, [sp, #272]
   4b320:	ldp	x29, x30, [sp, #256]
   4b324:	add	sp, sp, #0x120
   4b328:	ret

000000000004b32c <__gmp_sscanf@@Base>:
   4b32c:	sub	sp, sp, #0x120
   4b330:	stp	x29, x30, [sp, #256]
   4b334:	add	x29, sp, #0x100
   4b338:	mov	x10, #0xffffffffffffffd0    	// #-48
   4b33c:	mov	x11, sp
   4b340:	add	x12, sp, #0x80
   4b344:	movk	x10, #0xff80, lsl #32
   4b348:	add	x13, x29, #0x20
   4b34c:	add	x11, x11, #0x80
   4b350:	add	x12, x12, #0x30
   4b354:	sub	x9, x29, #0x28
   4b358:	stp	x11, x10, [x29, #-24]
   4b35c:	stp	x13, x12, [x29, #-40]
   4b360:	stp	q1, q2, [sp, #16]
   4b364:	str	q0, [sp]
   4b368:	ldp	q0, q1, [x9]
   4b36c:	str	x28, [sp, #272]
   4b370:	stp	x2, x3, [sp, #128]
   4b374:	stp	x4, x5, [sp, #144]
   4b378:	stp	x6, x7, [sp, #160]
   4b37c:	stp	q3, q4, [sp, #48]
   4b380:	stp	q5, q6, [sp, #80]
   4b384:	str	q7, [sp, #112]
   4b388:	stur	x0, [x29, #-8]
   4b38c:	stp	q0, q1, [x29, #-80]
   4b390:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b394:	ldr	x0, [x0, #3984]
   4b398:	mov	x8, x1
   4b39c:	sub	x1, x29, #0x8
   4b3a0:	sub	x3, x29, #0x50
   4b3a4:	mov	x2, x8
   4b3a8:	bl	c230 <__gmp_doscan@plt>
   4b3ac:	ldr	x28, [sp, #272]
   4b3b0:	ldp	x29, x30, [sp, #256]
   4b3b4:	add	sp, sp, #0x120
   4b3b8:	ret
   4b3bc:	sub	sp, sp, #0xe0
   4b3c0:	stp	x29, x30, [sp, #208]
   4b3c4:	add	x29, sp, #0xd0
   4b3c8:	mov	x9, #0xffffffffffffffd0    	// #-48
   4b3cc:	mov	x10, sp
   4b3d0:	sub	x11, x29, #0x50
   4b3d4:	movk	x9, #0xff80, lsl #32
   4b3d8:	add	x12, x29, #0x10
   4b3dc:	mov	x8, #0xffffffffffffffd0    	// #-48
   4b3e0:	add	x10, x10, #0x80
   4b3e4:	add	x11, x11, #0x30
   4b3e8:	stp	x2, x3, [x29, #-80]
   4b3ec:	stp	x4, x5, [x29, #-64]
   4b3f0:	stp	x6, x7, [x29, #-48]
   4b3f4:	stp	q1, q2, [sp, #16]
   4b3f8:	stp	q3, q4, [sp, #48]
   4b3fc:	str	q0, [sp]
   4b400:	stp	q5, q6, [sp, #80]
   4b404:	str	q7, [sp, #112]
   4b408:	stp	x10, x9, [x29, #-16]
   4b40c:	stp	x12, x11, [x29, #-32]
   4b410:	tbz	w8, #31, 4b424 <__gmp_sscanf@@Base+0xf8>
   4b414:	add	w9, w8, #0x8
   4b418:	cmp	w9, #0x0
   4b41c:	stur	w9, [x29, #-8]
   4b420:	b.le	4b470 <__gmp_sscanf@@Base+0x144>
   4b424:	ldur	x9, [x29, #-32]
   4b428:	add	x8, x9, #0x8
   4b42c:	stur	x8, [x29, #-32]
   4b430:	ldursw	x8, [x29, #-8]
   4b434:	ldr	x2, [x9]
   4b438:	tbz	w8, #31, 4b44c <__gmp_sscanf@@Base+0x120>
   4b43c:	add	w9, w8, #0x8
   4b440:	cmp	w9, #0x0
   4b444:	stur	w9, [x29, #-8]
   4b448:	b.le	4b47c <__gmp_sscanf@@Base+0x150>
   4b44c:	ldur	x8, [x29, #-32]
   4b450:	add	x9, x8, #0x8
   4b454:	stur	x9, [x29, #-32]
   4b458:	ldr	x3, [x8]
   4b45c:	ldr	x0, [x0]
   4b460:	bl	d2d0 <__isoc99_sscanf@plt>
   4b464:	ldp	x29, x30, [sp, #208]
   4b468:	add	sp, sp, #0xe0
   4b46c:	ret
   4b470:	ldur	x9, [x29, #-24]
   4b474:	add	x9, x9, x8
   4b478:	b	4b430 <__gmp_sscanf@@Base+0x104>
   4b47c:	ldur	x9, [x29, #-24]
   4b480:	add	x8, x9, x8
   4b484:	b	4b458 <__gmp_sscanf@@Base+0x12c>
   4b488:	ldr	x8, [x0]
   4b48c:	add	x8, x8, w1, sxtw
   4b490:	str	x8, [x0]
   4b494:	ret
   4b498:	ldr	x9, [x0]
   4b49c:	mov	x8, x0
   4b4a0:	ldrb	w0, [x9]
   4b4a4:	cbz	w0, 4b4b4 <__gmp_sscanf@@Base+0x188>
   4b4a8:	add	x9, x9, #0x1
   4b4ac:	str	x9, [x8]
   4b4b0:	ret
   4b4b4:	mov	w0, #0xffffffff            	// #-1
   4b4b8:	ret
   4b4bc:	cmn	w0, #0x1
   4b4c0:	b.eq	4b4d0 <__gmp_sscanf@@Base+0x1a4>  // b.none
   4b4c4:	ldr	x8, [x1]
   4b4c8:	sub	x8, x8, #0x1
   4b4cc:	str	x8, [x1]
   4b4d0:	ret

000000000004b4d4 <__gmp_vfscanf@@Base>:
   4b4d4:	sub	sp, sp, #0x30
   4b4d8:	stp	x29, x30, [sp, #32]
   4b4dc:	ldp	q1, q0, [x2]
   4b4e0:	mov	x8, x1
   4b4e4:	mov	x1, x0
   4b4e8:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b4ec:	stp	q1, q0, [sp]
   4b4f0:	ldr	x0, [x0, #4064]
   4b4f4:	mov	x3, sp
   4b4f8:	mov	x2, x8
   4b4fc:	add	x29, sp, #0x20
   4b500:	bl	c230 <__gmp_doscan@plt>
   4b504:	ldp	x29, x30, [sp, #32]
   4b508:	add	sp, sp, #0x30
   4b50c:	ret

000000000004b510 <__gmp_vscanf@@Base>:
   4b510:	sub	sp, sp, #0x30
   4b514:	stp	x29, x30, [sp, #32]
   4b518:	ldp	q0, q1, [x1]
   4b51c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b520:	ldr	x8, [x8, #3888]
   4b524:	mov	x2, x0
   4b528:	stp	q0, q1, [sp]
   4b52c:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b530:	ldr	x1, [x8]
   4b534:	ldr	x0, [x0, #4064]
   4b538:	mov	x3, sp
   4b53c:	add	x29, sp, #0x20
   4b540:	bl	c230 <__gmp_doscan@plt>
   4b544:	ldp	x29, x30, [sp, #32]
   4b548:	add	sp, sp, #0x30
   4b54c:	ret

000000000004b550 <__gmp_vsscanf@@Base>:
   4b550:	sub	sp, sp, #0x40
   4b554:	stp	x29, x30, [sp, #48]
   4b558:	add	x29, sp, #0x30
   4b55c:	stur	x0, [x29, #-8]
   4b560:	ldp	q1, q0, [x2]
   4b564:	adrp	x0, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b568:	mov	x8, x1
   4b56c:	sub	x1, x29, #0x8
   4b570:	stp	q1, q0, [sp]
   4b574:	ldr	x0, [x0, #3984]
   4b578:	mov	x3, sp
   4b57c:	mov	x2, x8
   4b580:	bl	c230 <__gmp_doscan@plt>
   4b584:	ldp	x29, x30, [sp, #48]
   4b588:	add	sp, sp, #0x40
   4b58c:	ret

000000000004b590 <__gmp_randinit@@Base>:
   4b590:	sub	sp, sp, #0xe0
   4b594:	stp	x29, x30, [sp, #208]
   4b598:	add	x29, sp, #0xd0
   4b59c:	mov	x8, #0xffffffffffffffd0    	// #-48
   4b5a0:	mov	x9, sp
   4b5a4:	sub	x10, x29, #0x50
   4b5a8:	movk	x8, #0xff80, lsl #32
   4b5ac:	add	x11, x29, #0x10
   4b5b0:	add	x9, x9, #0x80
   4b5b4:	add	x10, x10, #0x30
   4b5b8:	stp	x2, x3, [x29, #-80]
   4b5bc:	stp	x4, x5, [x29, #-64]
   4b5c0:	stp	x6, x7, [x29, #-48]
   4b5c4:	stp	q1, q2, [sp, #16]
   4b5c8:	stp	q3, q4, [sp, #48]
   4b5cc:	str	q0, [sp]
   4b5d0:	stp	q5, q6, [sp, #80]
   4b5d4:	str	q7, [sp, #112]
   4b5d8:	stp	x9, x8, [x29, #-16]
   4b5dc:	stp	x11, x10, [x29, #-32]
   4b5e0:	cbz	w1, 4b600 <__gmp_randinit@@Base+0x70>
   4b5e4:	mov	w8, #0x1                   	// #1
   4b5e8:	adrp	x9, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b5ec:	ldr	x9, [x9, #3896]
   4b5f0:	ldr	w10, [x9]
   4b5f4:	orr	w8, w10, w8
   4b5f8:	str	w8, [x9]
   4b5fc:	b	4b630 <__gmp_randinit@@Base+0xa0>
   4b600:	ldursw	x8, [x29, #-8]
   4b604:	tbz	w8, #31, 4b618 <__gmp_randinit@@Base+0x88>
   4b608:	add	w9, w8, #0x8
   4b60c:	cmp	w9, #0x0
   4b610:	stur	w9, [x29, #-8]
   4b614:	b.le	4b644 <__gmp_randinit@@Base+0xb4>
   4b618:	ldur	x8, [x29, #-32]
   4b61c:	add	x9, x8, #0x8
   4b620:	stur	x9, [x29, #-32]
   4b624:	ldr	x1, [x8]
   4b628:	bl	d370 <__gmp_randinit_lc_2exp_size@plt>
   4b62c:	cbz	w0, 4b63c <__gmp_randinit@@Base+0xac>
   4b630:	ldp	x29, x30, [sp, #208]
   4b634:	add	sp, sp, #0xe0
   4b638:	ret
   4b63c:	mov	w8, #0x8                   	// #8
   4b640:	b	4b5e8 <__gmp_randinit@@Base+0x58>
   4b644:	ldur	x9, [x29, #-24]
   4b648:	add	x8, x9, x8
   4b64c:	b	4b624 <__gmp_randinit@@Base+0x94>

000000000004b650 <__gmp_randclear@@Base>:
   4b650:	stp	x29, x30, [sp, #-16]!
   4b654:	ldr	x8, [x0, #24]
   4b658:	mov	x29, sp
   4b65c:	ldr	x8, [x8, #16]
   4b660:	blr	x8
   4b664:	ldp	x29, x30, [sp], #16
   4b668:	ret

000000000004b66c <__gmp_randinit_default@@Base>:
   4b66c:	stp	x29, x30, [sp, #-16]!
   4b670:	mov	x29, sp
   4b674:	bl	ccc0 <__gmp_randinit_mt@plt>
   4b678:	ldp	x29, x30, [sp], #16
   4b67c:	ret

000000000004b680 <__gmp_randinit_set@@Base>:
   4b680:	stp	x29, x30, [sp, #-16]!
   4b684:	ldr	x8, [x1, #24]
   4b688:	mov	x29, sp
   4b68c:	ldr	x8, [x8, #24]
   4b690:	blr	x8
   4b694:	ldp	x29, x30, [sp], #16
   4b698:	ret

000000000004b69c <__gmp_randinit_lc_2exp_size@@Base>:
   4b69c:	sub	sp, sp, #0x30
   4b6a0:	stp	x20, x19, [sp, #32]
   4b6a4:	adrp	x20, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b6a8:	mov	x19, x0
   4b6ac:	add	x20, x20, #0xae0
   4b6b0:	mov	w8, #0x20                  	// #32
   4b6b4:	stp	x29, x30, [sp, #16]
   4b6b8:	add	x29, sp, #0x10
   4b6bc:	cmp	x1, x8, lsr #1
   4b6c0:	b.ls	4b6d8 <__gmp_randinit_lc_2exp_size@@Base+0x3c>  // b.plast
   4b6c4:	ldr	x8, [x20, #8]
   4b6c8:	add	x20, x20, #0x18
   4b6cc:	cbnz	x8, 4b6bc <__gmp_randinit_lc_2exp_size@@Base+0x20>
   4b6d0:	mov	w0, wzr
   4b6d4:	b	4b708 <__gmp_randinit_lc_2exp_size@@Base+0x6c>
   4b6d8:	ldur	x1, [x20, #-8]
   4b6dc:	mov	x0, sp
   4b6e0:	mov	w2, #0x10                  	// #16
   4b6e4:	bl	d270 <__gmpz_init_set_str@plt>
   4b6e8:	ldr	x2, [x20]
   4b6ec:	ldur	x3, [x20, #-16]
   4b6f0:	mov	x1, sp
   4b6f4:	mov	x0, x19
   4b6f8:	bl	d110 <__gmp_randinit_lc_2exp@plt>
   4b6fc:	mov	x0, sp
   4b700:	bl	cd10 <__gmpz_clear@plt>
   4b704:	mov	w0, #0x1                   	// #1
   4b708:	ldp	x20, x19, [sp, #32]
   4b70c:	ldp	x29, x30, [sp, #16]
   4b710:	add	sp, sp, #0x30
   4b714:	ret

000000000004b718 <__gmp_randinit_lc_2exp@@Base>:
   4b718:	stp	x29, x30, [sp, #-64]!
   4b71c:	stp	x24, x23, [sp, #16]
   4b720:	stp	x22, x21, [sp, #32]
   4b724:	stp	x20, x19, [sp, #48]
   4b728:	mov	x29, sp
   4b72c:	cbz	x3, 4b80c <__gmp_randinit_lc_2exp@@Base+0xf4>
   4b730:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b734:	ldr	x8, [x8, #3840]
   4b738:	mov	x23, x0
   4b73c:	add	x9, x3, #0x3f
   4b740:	mov	w0, #0x38                  	// #56
   4b744:	ldr	x8, [x8]
   4b748:	mov	x19, x3
   4b74c:	mov	x20, x2
   4b750:	mov	x22, x1
   4b754:	lsr	x24, x9, #6
   4b758:	blr	x8
   4b75c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4b760:	add	x8, x8, #0xc80
   4b764:	mov	x1, x19
   4b768:	mov	x21, x0
   4b76c:	str	x0, [x23, #8]
   4b770:	str	x8, [x23, #24]
   4b774:	bl	d300 <__gmpz_init2@plt>
   4b778:	cbz	x24, 4b78c <__gmp_randinit_lc_2exp@@Base+0x74>
   4b77c:	ldr	x0, [x21, #8]
   4b780:	lsl	x2, x24, #3
   4b784:	mov	w1, wzr
   4b788:	bl	c780 <memset@plt>
   4b78c:	ldr	x8, [x21, #8]
   4b790:	add	x23, x21, #0x10
   4b794:	str	w24, [x21, #4]
   4b798:	mov	w24, #0x1                   	// #1
   4b79c:	mov	x0, x23
   4b7a0:	str	x24, [x8]
   4b7a4:	bl	d430 <__gmpz_init@plt>
   4b7a8:	mov	x0, x23
   4b7ac:	mov	x1, x22
   4b7b0:	mov	x2, x19
   4b7b4:	bl	d290 <__gmpz_fdiv_r_2exp@plt>
   4b7b8:	ldr	w8, [x21, #20]
   4b7bc:	cbnz	w8, 4b7d8 <__gmp_randinit_lc_2exp@@Base+0xc0>
   4b7c0:	ldr	w8, [x21, #16]
   4b7c4:	str	w24, [x21, #20]
   4b7c8:	cmp	w8, #0x0
   4b7cc:	b.le	4b7fc <__gmp_randinit_lc_2exp@@Base+0xe4>
   4b7d0:	ldr	x0, [x21, #24]
   4b7d4:	str	xzr, [x0]
   4b7d8:	cmp	x20, #0x0
   4b7dc:	cset	w8, ne  // ne = any
   4b7e0:	stp	x8, x20, [x21, #32]
   4b7e4:	str	x19, [x21, #48]
   4b7e8:	ldp	x20, x19, [sp, #48]
   4b7ec:	ldp	x22, x21, [sp, #32]
   4b7f0:	ldp	x24, x23, [sp, #16]
   4b7f4:	ldp	x29, x30, [sp], #64
   4b7f8:	ret
   4b7fc:	mov	w1, #0x1                   	// #1
   4b800:	mov	x0, x23
   4b804:	bl	c1c0 <__gmpz_realloc@plt>
   4b808:	b	4b7d4 <__gmp_randinit_lc_2exp@@Base+0xbc>
   4b80c:	adrp	x0, 58000 <__gmp_limbroots_table@@Base+0x338>
   4b810:	adrp	x2, 58000 <__gmp_limbroots_table@@Base+0x338>
   4b814:	add	x0, x0, #0x280
   4b818:	add	x2, x2, #0x28b
   4b81c:	mov	w1, #0x12d                 	// #301
   4b820:	bl	c850 <__gmp_assert_fail@plt>
   4b824:	stp	x29, x30, [sp, #-32]!
   4b828:	stp	x20, x19, [sp, #16]
   4b82c:	ldr	x19, [x0, #8]
   4b830:	mov	x29, sp
   4b834:	ldr	x2, [x19, #48]
   4b838:	mov	x0, x19
   4b83c:	add	x8, x2, #0x3f
   4b840:	lsr	x20, x8, #6
   4b844:	bl	d290 <__gmpz_fdiv_r_2exp@plt>
   4b848:	ldrsw	x8, [x19, #4]
   4b84c:	subs	x9, x20, x8
   4b850:	b.eq	4b868 <__gmp_randinit_lc_2exp@@Base+0x150>  // b.none
   4b854:	ldr	x10, [x19, #8]
   4b858:	lsl	x2, x9, #3
   4b85c:	mov	w1, wzr
   4b860:	add	x0, x10, x8, lsl #3
   4b864:	bl	c780 <memset@plt>
   4b868:	str	w20, [x19, #4]
   4b86c:	ldp	x20, x19, [sp, #16]
   4b870:	ldp	x29, x30, [sp], #32
   4b874:	ret
   4b878:	stp	x29, x30, [sp, #-96]!
   4b87c:	stp	x28, x27, [sp, #16]
   4b880:	stp	x26, x25, [sp, #32]
   4b884:	stp	x24, x23, [sp, #48]
   4b888:	stp	x22, x21, [sp, #64]
   4b88c:	stp	x20, x19, [sp, #80]
   4b890:	mov	x29, sp
   4b894:	sub	sp, sp, #0x10
   4b898:	ldr	x8, [x0, #8]
   4b89c:	stp	x1, xzr, [x29, #-16]
   4b8a0:	mov	x19, x2
   4b8a4:	mov	x21, x0
   4b8a8:	ldr	x24, [x8, #48]
   4b8ac:	lsr	x25, x24, #1
   4b8b0:	add	w8, w25, #0x3f
   4b8b4:	add	w9, w25, #0x7e
   4b8b8:	cmp	w8, #0x0
   4b8bc:	csel	w23, w9, w8, lt  // lt = tstop
   4b8c0:	asr	w8, w23, #6
   4b8c4:	sbfiz	x8, x8, #3, #32
   4b8c8:	mov	w9, #0x7f00                	// #32512
   4b8cc:	cmp	x8, x9
   4b8d0:	b.hi	4b9dc <__gmp_randinit_lc_2exp@@Base+0x2c4>  // b.pmore
   4b8d4:	add	x8, x8, #0xf
   4b8d8:	mov	x9, sp
   4b8dc:	and	x8, x8, #0xfffffffffffffff0
   4b8e0:	sub	x22, x9, x8
   4b8e4:	mov	sp, x22
   4b8e8:	cmp	x19, w25, sxtw
   4b8ec:	b.cs	4b9f4 <__gmp_randinit_lc_2exp@@Base+0x2dc>  // b.hs, b.nlast
   4b8f0:	mov	x26, xzr
   4b8f4:	cmp	x26, x19
   4b8f8:	b.eq	4b998 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   4b8fc:	ldur	x11, [x29, #-16]
   4b900:	sub	w9, w19, w26
   4b904:	lsr	x8, x26, #3
   4b908:	add	w10, w9, #0x3f
   4b90c:	and	x8, x8, #0x1ffffffffffffff8
   4b910:	add	w9, w9, #0x7e
   4b914:	cmp	w10, #0x0
   4b918:	add	x23, x11, x8
   4b91c:	csel	w8, w9, w10, lt  // lt = tstop
   4b920:	mov	x0, x22
   4b924:	mov	x1, x21
   4b928:	sbfx	x24, x8, #6, #26
   4b92c:	bl	4bb5c <__gmp_randinit_lc_2exp@@Base+0x444>
   4b930:	ands	x21, x26, #0x3f
   4b934:	b.eq	4b9c0 <__gmp_randinit_lc_2exp@@Base+0x2a8>  // b.none
   4b938:	ldr	x20, [x23]
   4b93c:	mov	x0, x23
   4b940:	mov	x1, x22
   4b944:	mov	x2, x24
   4b948:	mov	w3, w21
   4b94c:	bl	c2d0 <__gmpn_lshift@plt>
   4b950:	ldr	x8, [x23]
   4b954:	sub	x9, x26, x21
   4b958:	add	x9, x9, x24, lsl #6
   4b95c:	cmp	x9, x19
   4b960:	orr	x8, x8, x20
   4b964:	str	x8, [x23]
   4b968:	b.cs	4b970 <__gmp_randinit_lc_2exp@@Base+0x258>  // b.hs, b.nlast
   4b96c:	str	x0, [x23, x24, lsl #3]
   4b970:	ands	x8, x19, #0x3f
   4b974:	b.eq	4b998 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   4b978:	ldur	x12, [x29, #-16]
   4b97c:	lsr	x9, x19, #3
   4b980:	and	x9, x9, #0x1ffffffffffffff8
   4b984:	mov	x11, #0xffffffffffffffff    	// #-1
   4b988:	ldr	x10, [x12, x9]
   4b98c:	lsl	x8, x11, x8
   4b990:	bic	x8, x10, x8
   4b994:	str	x8, [x12, x9]
   4b998:	ldur	x0, [x29, #-8]
   4b99c:	cbnz	x0, 4ba94 <__gmp_randinit_lc_2exp@@Base+0x37c>
   4b9a0:	mov	sp, x29
   4b9a4:	ldp	x20, x19, [sp, #80]
   4b9a8:	ldp	x22, x21, [sp, #64]
   4b9ac:	ldp	x24, x23, [sp, #48]
   4b9b0:	ldp	x26, x25, [sp, #32]
   4b9b4:	ldp	x28, x27, [sp, #16]
   4b9b8:	ldp	x29, x30, [sp], #96
   4b9bc:	ret
   4b9c0:	mov	x0, x23
   4b9c4:	mov	x1, x22
   4b9c8:	mov	x2, x24
   4b9cc:	bl	cc10 <__gmpn_copyi@plt>
   4b9d0:	ands	x8, x19, #0x3f
   4b9d4:	b.ne	4b978 <__gmp_randinit_lc_2exp@@Base+0x260>  // b.any
   4b9d8:	b	4b998 <__gmp_randinit_lc_2exp@@Base+0x280>
   4b9dc:	sub	x0, x29, #0x8
   4b9e0:	mov	x1, x8
   4b9e4:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4b9e8:	mov	x22, x0
   4b9ec:	cmp	x19, w25, sxtw
   4b9f0:	b.cc	4b8f0 <__gmp_randinit_lc_2exp@@Base+0x1d8>  // b.lo, b.ul, b.last
   4b9f4:	add	w8, w25, #0x3f
   4b9f8:	cmp	w25, #0x0
   4b9fc:	csel	w8, w8, w25, lt  // lt = tstop
   4ba00:	and	w8, w8, #0xffffffc0
   4ba04:	sub	w8, w25, w8
   4ba08:	mov	x26, xzr
   4ba0c:	sbfx	x23, x23, #6, #26
   4ba10:	sxtw	x27, w8
   4ba14:	sbfx	x28, x24, #1, #32
   4ba18:	b	4ba38 <__gmp_randinit_lc_2exp@@Base+0x320>
   4ba1c:	mov	x0, x24
   4ba20:	mov	x1, x21
   4ba24:	bl	4bb5c <__gmp_randinit_lc_2exp@@Base+0x444>
   4ba28:	add	x26, x26, x28
   4ba2c:	add	x8, x28, x26
   4ba30:	cmp	x8, x19
   4ba34:	b.hi	4b8f4 <__gmp_randinit_lc_2exp@@Base+0x1dc>  // b.pmore
   4ba38:	ldur	x9, [x29, #-16]
   4ba3c:	lsr	x8, x26, #3
   4ba40:	and	x8, x8, #0x1ffffffffffffff8
   4ba44:	ands	x25, x26, #0x3f
   4ba48:	add	x24, x9, x8
   4ba4c:	b.eq	4ba1c <__gmp_randinit_lc_2exp@@Base+0x304>  // b.none
   4ba50:	mov	x0, x22
   4ba54:	mov	x1, x21
   4ba58:	bl	4bb5c <__gmp_randinit_lc_2exp@@Base+0x444>
   4ba5c:	ldr	x20, [x24]
   4ba60:	mov	x0, x24
   4ba64:	mov	x1, x22
   4ba68:	mov	x2, x23
   4ba6c:	mov	w3, w25
   4ba70:	bl	c2d0 <__gmpn_lshift@plt>
   4ba74:	ldr	x8, [x24]
   4ba78:	add	x9, x25, x27
   4ba7c:	cmp	x9, #0x41
   4ba80:	orr	x8, x8, x20
   4ba84:	str	x8, [x24]
   4ba88:	b.cc	4ba28 <__gmp_randinit_lc_2exp@@Base+0x310>  // b.lo, b.ul, b.last
   4ba8c:	str	x0, [x24, x23, lsl #3]
   4ba90:	b	4ba28 <__gmp_randinit_lc_2exp@@Base+0x310>
   4ba94:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   4ba98:	b	4b9a0 <__gmp_randinit_lc_2exp@@Base+0x288>
   4ba9c:	stp	x29, x30, [sp, #-32]!
   4baa0:	str	x19, [sp, #16]
   4baa4:	ldr	x19, [x0, #8]
   4baa8:	mov	x29, sp
   4baac:	mov	x0, x19
   4bab0:	bl	cd10 <__gmpz_clear@plt>
   4bab4:	add	x0, x19, #0x10
   4bab8:	bl	cd10 <__gmpz_clear@plt>
   4babc:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4bac0:	ldr	x8, [x8, #4016]
   4bac4:	mov	w1, #0x38                  	// #56
   4bac8:	mov	x0, x19
   4bacc:	ldr	x8, [x8]
   4bad0:	blr	x8
   4bad4:	ldr	x19, [sp, #16]
   4bad8:	ldp	x29, x30, [sp], #32
   4badc:	ret
   4bae0:	stp	x29, x30, [sp, #-48]!
   4bae4:	str	x21, [sp, #16]
   4bae8:	stp	x20, x19, [sp, #32]
   4baec:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4baf0:	ldr	x19, [x1, #8]
   4baf4:	ldr	x8, [x8, #3840]
   4baf8:	mov	x20, x0
   4bafc:	mov	w0, #0x38                  	// #56
   4bb00:	mov	x29, sp
   4bb04:	ldr	x8, [x8]
   4bb08:	blr	x8
   4bb0c:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4bb10:	add	x8, x8, #0xc80
   4bb14:	mov	x1, x19
   4bb18:	mov	x21, x0
   4bb1c:	str	x0, [x20, #8]
   4bb20:	str	x8, [x20, #24]
   4bb24:	bl	c0b0 <__gmpz_init_set@plt>
   4bb28:	add	x0, x21, #0x10
   4bb2c:	add	x1, x19, #0x10
   4bb30:	bl	c0b0 <__gmpz_init_set@plt>
   4bb34:	ldr	x8, [x19, #32]
   4bb38:	str	x8, [x21, #32]
   4bb3c:	ldr	x8, [x19, #40]
   4bb40:	str	x8, [x21, #40]
   4bb44:	ldr	x8, [x19, #48]
   4bb48:	str	x8, [x21, #48]
   4bb4c:	ldp	x20, x19, [sp, #32]
   4bb50:	ldr	x21, [sp, #16]
   4bb54:	ldp	x29, x30, [sp], #48
   4bb58:	ret
   4bb5c:	stp	x29, x30, [sp, #-96]!
   4bb60:	stp	x28, x27, [sp, #16]
   4bb64:	stp	x26, x25, [sp, #32]
   4bb68:	stp	x24, x23, [sp, #48]
   4bb6c:	stp	x22, x21, [sp, #64]
   4bb70:	stp	x20, x19, [sp, #80]
   4bb74:	mov	x29, sp
   4bb78:	sub	sp, sp, #0x10
   4bb7c:	ldr	x27, [x1, #8]
   4bb80:	ldr	x26, [x27, #48]
   4bb84:	ldrsw	x22, [x27, #4]
   4bb88:	ldrsw	x23, [x27, #20]
   4bb8c:	ldr	x25, [x27, #8]
   4bb90:	ldr	x24, [x27, #24]
   4bb94:	add	x8, x26, #0x3f
   4bb98:	add	x28, x23, x22
   4bb9c:	lsr	x20, x8, #6
   4bba0:	cmp	x28, x20
   4bba4:	stp	x0, xzr, [x29, #-16]
   4bba8:	b.ge	4bbfc <__gmp_randinit_lc_2exp@@Base+0x4e4>  // b.tcont
   4bbac:	add	x19, x20, #0x1
   4bbb0:	lsr	x8, x8, #11
   4bbb4:	cmp	x8, #0x7e
   4bbb8:	lsl	x1, x19, #3
   4bbbc:	b.hi	4bd14 <__gmp_randinit_lc_2exp@@Base+0x5fc>  // b.pmore
   4bbc0:	add	x9, x1, #0xf
   4bbc4:	mov	x8, sp
   4bbc8:	and	x9, x9, #0x7ffffffffffffff0
   4bbcc:	sub	x21, x8, x9
   4bbd0:	mov	sp, x21
   4bbd4:	cmp	x19, x28
   4bbd8:	b.eq	4bc24 <__gmp_randinit_lc_2exp@@Base+0x50c>  // b.none
   4bbdc:	add	x8, x23, x22
   4bbe0:	add	x0, x21, x8, lsl #3
   4bbe4:	sub	x8, x20, x8
   4bbe8:	lsl	x8, x8, #3
   4bbec:	add	x2, x8, #0x8
   4bbf0:	mov	w1, wzr
   4bbf4:	bl	c780 <memset@plt>
   4bbf8:	b	4bc24 <__gmp_randinit_lc_2exp@@Base+0x50c>
   4bbfc:	lsl	x8, x28, #3
   4bc00:	add	x1, x8, #0x8
   4bc04:	mov	w8, #0x7f00                	// #32512
   4bc08:	cmp	x1, x8
   4bc0c:	b.hi	4bd2c <__gmp_randinit_lc_2exp@@Base+0x614>  // b.pmore
   4bc10:	add	x9, x1, #0xf
   4bc14:	mov	x8, sp
   4bc18:	and	x9, x9, #0xfffffffffffffff0
   4bc1c:	sub	x21, x8, x9
   4bc20:	mov	sp, x21
   4bc24:	mov	x0, x21
   4bc28:	mov	x1, x25
   4bc2c:	mov	x2, x22
   4bc30:	mov	x3, x24
   4bc34:	mov	x4, x23
   4bc38:	bl	cea0 <__gmpn_mul@plt>
   4bc3c:	ldr	x22, [x27, #32]
   4bc40:	cbz	x22, 4bc7c <__gmp_randinit_lc_2exp@@Base+0x564>
   4bc44:	add	x2, x27, #0x28
   4bc48:	mov	x0, x21
   4bc4c:	mov	x1, x21
   4bc50:	mov	x3, x22
   4bc54:	bl	cc30 <__gmpn_add_n@plt>
   4bc58:	cbz	x0, 4bc7c <__gmp_randinit_lc_2exp@@Base+0x564>
   4bc5c:	cmp	x22, x20
   4bc60:	b.ge	4bc7c <__gmp_randinit_lc_2exp@@Base+0x564>  // b.tcont
   4bc64:	lsl	x8, x22, #3
   4bc68:	ldr	x9, [x21, x8]
   4bc6c:	add	x22, x22, #0x1
   4bc70:	adds	x9, x9, #0x1
   4bc74:	str	x9, [x21, x8]
   4bc78:	b.cs	4bc5c <__gmp_randinit_lc_2exp@@Base+0x544>  // b.hs, b.nlast
   4bc7c:	lsr	x8, x26, #3
   4bc80:	and	x8, x8, #0x1ffffffffffffff8
   4bc84:	ldr	x9, [x21, x8]
   4bc88:	mov	x10, #0xffffffffffffffff    	// #-1
   4bc8c:	lsl	x10, x10, x26
   4bc90:	mov	x1, x21
   4bc94:	bic	x9, x9, x10
   4bc98:	str	x9, [x21, x8]
   4bc9c:	ldr	x0, [x27, #8]
   4bca0:	mov	x2, x20
   4bca4:	bl	cc10 <__gmpn_copyi@plt>
   4bca8:	lsr	x19, x26, #7
   4bcac:	sub	x2, x20, x19
   4bcb0:	cmp	x2, #0x1
   4bcb4:	b.lt	4bce4 <__gmp_randinit_lc_2exp@@Base+0x5cc>  // b.tstop
   4bcb8:	ubfx	w3, w26, #1, #6
   4bcbc:	add	x1, x21, x19, lsl #3
   4bcc0:	cbz	w3, 4bcdc <__gmp_randinit_lc_2exp@@Base+0x5c4>
   4bcc4:	mov	x0, x21
   4bcc8:	bl	c2f0 <__gmpn_rshift@plt>
   4bccc:	ldur	x0, [x29, #-16]
   4bcd0:	add	x2, x19, #0x1
   4bcd4:	mov	x1, x21
   4bcd8:	b	4bce0 <__gmp_randinit_lc_2exp@@Base+0x5c8>
   4bcdc:	ldur	x0, [x29, #-16]
   4bce0:	bl	cc10 <__gmpn_copyi@plt>
   4bce4:	ldur	x0, [x29, #-8]
   4bce8:	cbnz	x0, 4bd0c <__gmp_randinit_lc_2exp@@Base+0x5f4>
   4bcec:	mov	sp, x29
   4bcf0:	ldp	x20, x19, [sp, #80]
   4bcf4:	ldp	x22, x21, [sp, #64]
   4bcf8:	ldp	x24, x23, [sp, #48]
   4bcfc:	ldp	x26, x25, [sp, #32]
   4bd00:	ldp	x28, x27, [sp, #16]
   4bd04:	ldp	x29, x30, [sp], #96
   4bd08:	ret
   4bd0c:	bl	c020 <__gmp_tmp_reentrant_free@plt>
   4bd10:	b	4bcec <__gmp_randinit_lc_2exp@@Base+0x5d4>
   4bd14:	sub	x0, x29, #0x8
   4bd18:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4bd1c:	mov	x21, x0
   4bd20:	cmp	x19, x28
   4bd24:	b.ne	4bbdc <__gmp_randinit_lc_2exp@@Base+0x4c4>  // b.any
   4bd28:	b	4bc24 <__gmp_randinit_lc_2exp@@Base+0x50c>
   4bd2c:	sub	x0, x29, #0x8
   4bd30:	bl	cd80 <__gmp_tmp_reentrant_alloc@plt>
   4bd34:	mov	x21, x0
   4bd38:	b	4bc24 <__gmp_randinit_lc_2exp@@Base+0x50c>

000000000004bd3c <__gmp_mt_recalc_buffer@@Base>:
   4bd3c:	mov	w8, #0xb0df                	// #45279
   4bd40:	mov	x9, xzr
   4bd44:	movk	w8, #0x9908, lsl #16
   4bd48:	add	x10, x0, x9
   4bd4c:	ldp	w11, w12, [x10]
   4bd50:	ldr	w13, [x10, #1588]
   4bd54:	add	x9, x9, #0x4
   4bd58:	cmp	x9, #0x38c
   4bd5c:	and	w11, w11, #0x80000000
   4bd60:	and	w14, w12, #0x7ffffffe
   4bd64:	sbfx	w12, w12, #0, #1
   4bd68:	orr	w11, w14, w11
   4bd6c:	and	w12, w12, w8
   4bd70:	eor	w11, w13, w11, lsr #1
   4bd74:	eor	w11, w11, w12
   4bd78:	str	w11, [x10]
   4bd7c:	b.ne	4bd48 <__gmp_mt_recalc_buffer@@Base+0xc>  // b.any
   4bd80:	mov	x9, xzr
   4bd84:	add	x10, x0, x9
   4bd88:	ldr	w11, [x10, #908]
   4bd8c:	ldr	w12, [x10, #912]
   4bd90:	ldr	w13, [x10]
   4bd94:	add	x9, x9, #0x4
   4bd98:	and	w11, w11, #0x80000000
   4bd9c:	and	w14, w12, #0x7ffffffe
   4bda0:	sbfx	w12, w12, #0, #1
   4bda4:	orr	w11, w14, w11
   4bda8:	and	w12, w12, w8
   4bdac:	eor	w11, w13, w11, lsr #1
   4bdb0:	eor	w11, w11, w12
   4bdb4:	cmp	x9, #0x630
   4bdb8:	str	w11, [x10, #908]
   4bdbc:	b.ne	4bd84 <__gmp_mt_recalc_buffer@@Base+0x48>  // b.any
   4bdc0:	ldr	w9, [x0, #2492]
   4bdc4:	ldr	w10, [x0]
   4bdc8:	ldr	w11, [x0, #1584]
   4bdcc:	and	w9, w9, #0x80000000
   4bdd0:	and	w12, w10, #0x7ffffffe
   4bdd4:	sbfx	w10, w10, #0, #1
   4bdd8:	orr	w9, w12, w9
   4bddc:	eor	w9, w11, w9, lsr #1
   4bde0:	and	w8, w10, w8
   4bde4:	eor	w8, w9, w8
   4bde8:	str	w8, [x0, #2492]
   4bdec:	ret

000000000004bdf0 <__gmp_randget_mt@@Base>:
   4bdf0:	stp	x29, x30, [sp, #-80]!
   4bdf4:	stp	x26, x25, [sp, #16]
   4bdf8:	stp	x24, x23, [sp, #32]
   4bdfc:	stp	x22, x21, [sp, #48]
   4be00:	stp	x20, x19, [sp, #64]
   4be04:	ldr	x20, [x0, #8]
   4be08:	mov	w23, #0x5680                	// #22144
   4be0c:	mov	x19, x1
   4be10:	mov	w21, #0xefc60000            	// #-272236544
   4be14:	movk	w23, #0x9d2c, lsl #16
   4be18:	lsr	x22, x2, #6
   4be1c:	and	w24, w2, #0x3f
   4be20:	mov	x29, sp
   4be24:	cbz	x22, 4becc <__gmp_randget_mt@@Base+0xdc>
   4be28:	mov	x25, x19
   4be2c:	mov	x26, x22
   4be30:	b	4be70 <__gmp_randget_mt@@Base+0x80>
   4be34:	ldrsw	x8, [x20, #2496]
   4be38:	subs	x26, x26, #0x1
   4be3c:	add	w9, w8, #0x1
   4be40:	str	w9, [x20, #2496]
   4be44:	ldr	w8, [x20, x8, lsl #2]
   4be48:	ldr	x9, [x25]
   4be4c:	eor	w8, w8, w8, lsr #11
   4be50:	and	w10, w23, w8, lsl #7
   4be54:	eor	w8, w10, w8
   4be58:	and	w10, w21, w8, lsl #15
   4be5c:	eor	w8, w10, w8
   4be60:	eor	w8, w8, w8, lsr #18
   4be64:	orr	x8, x9, x8, lsl #32
   4be68:	str	x8, [x25], #8
   4be6c:	b.eq	4becc <__gmp_randget_mt@@Base+0xdc>  // b.none
   4be70:	ldr	w8, [x20, #2496]
   4be74:	cmp	w8, #0x26f
   4be78:	b.le	4be88 <__gmp_randget_mt@@Base+0x98>
   4be7c:	mov	x0, x20
   4be80:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4be84:	str	wzr, [x20, #2496]
   4be88:	ldrsw	x8, [x20, #2496]
   4be8c:	add	w9, w8, #0x1
   4be90:	str	w9, [x20, #2496]
   4be94:	ldr	w9, [x20, x8, lsl #2]
   4be98:	cmp	w8, #0x26f
   4be9c:	eor	w9, w9, w9, lsr #11
   4bea0:	and	w10, w23, w9, lsl #7
   4bea4:	eor	w9, w10, w9
   4bea8:	and	w10, w21, w9, lsl #15
   4beac:	eor	w9, w10, w9
   4beb0:	eor	w9, w9, w9, lsr #18
   4beb4:	str	x9, [x25]
   4beb8:	b.lt	4be34 <__gmp_randget_mt@@Base+0x44>  // b.tstop
   4bebc:	mov	x0, x20
   4bec0:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4bec4:	str	wzr, [x20, #2496]
   4bec8:	b	4be34 <__gmp_randget_mt@@Base+0x44>
   4becc:	cbz	w24, 4bfd0 <__gmp_randget_mt@@Base+0x1e0>
   4bed0:	ldr	w8, [x20, #2496]
   4bed4:	cmp	w24, #0x1f
   4bed8:	b.hi	4bf2c <__gmp_randget_mt@@Base+0x13c>  // b.pmore
   4bedc:	cmp	w8, #0x270
   4bee0:	b.lt	4bef0 <__gmp_randget_mt@@Base+0x100>  // b.tstop
   4bee4:	mov	x0, x20
   4bee8:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4beec:	str	wzr, [x20, #2496]
   4bef0:	ldrsw	x8, [x20, #2496]
   4bef4:	add	w9, w8, #0x1
   4bef8:	str	w9, [x20, #2496]
   4befc:	ldr	w8, [x20, x8, lsl #2]
   4bf00:	mov	x9, #0xffffffffffffffff    	// #-1
   4bf04:	lsl	x9, x9, x24
   4bf08:	eor	w8, w8, w8, lsr #11
   4bf0c:	and	w10, w23, w8, lsl #7
   4bf10:	eor	w8, w10, w8
   4bf14:	and	w10, w21, w8, lsl #15
   4bf18:	eor	w8, w10, w8
   4bf1c:	eor	w8, w8, w8, lsr #18
   4bf20:	bic	x8, x8, x9
   4bf24:	str	x8, [x19, x22, lsl #3]
   4bf28:	b	4bfd0 <__gmp_randget_mt@@Base+0x1e0>
   4bf2c:	cmp	w8, #0x270
   4bf30:	b.lt	4bf40 <__gmp_randget_mt@@Base+0x150>  // b.tstop
   4bf34:	mov	x0, x20
   4bf38:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4bf3c:	str	wzr, [x20, #2496]
   4bf40:	ldrsw	x8, [x20, #2496]
   4bf44:	cmp	w24, #0x21
   4bf48:	add	w9, w8, #0x1
   4bf4c:	str	w9, [x20, #2496]
   4bf50:	ldr	w9, [x20, x8, lsl #2]
   4bf54:	eor	w9, w9, w9, lsr #11
   4bf58:	and	w10, w23, w9, lsl #7
   4bf5c:	eor	w9, w10, w9
   4bf60:	and	w10, w21, w9, lsl #15
   4bf64:	eor	w9, w10, w9
   4bf68:	eor	w9, w9, w9, lsr #18
   4bf6c:	str	x9, [x19, x22, lsl #3]
   4bf70:	b.cc	4bfd0 <__gmp_randget_mt@@Base+0x1e0>  // b.lo, b.ul, b.last
   4bf74:	cmp	w8, #0x26f
   4bf78:	b.lt	4bf88 <__gmp_randget_mt@@Base+0x198>  // b.tstop
   4bf7c:	mov	x0, x20
   4bf80:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4bf84:	str	wzr, [x20, #2496]
   4bf88:	ldrsw	x8, [x20, #2496]
   4bf8c:	lsl	x11, x22, #3
   4bf90:	sub	w9, w24, #0x20
   4bf94:	add	w10, w8, #0x1
   4bf98:	str	w10, [x20, #2496]
   4bf9c:	ldr	w8, [x20, x8, lsl #2]
   4bfa0:	ldr	x12, [x19, x11]
   4bfa4:	mov	x10, #0xffffffffffffffff    	// #-1
   4bfa8:	lsl	x9, x10, x9
   4bfac:	eor	w8, w8, w8, lsr #11
   4bfb0:	and	w13, w23, w8, lsl #7
   4bfb4:	eor	w8, w13, w8
   4bfb8:	and	w13, w21, w8, lsl #15
   4bfbc:	eor	w8, w13, w8
   4bfc0:	eor	w8, w8, w8, lsr #18
   4bfc4:	bic	w8, w8, w9
   4bfc8:	orr	x8, x12, x8, lsl #32
   4bfcc:	str	x8, [x19, x11]
   4bfd0:	ldp	x20, x19, [sp, #64]
   4bfd4:	ldp	x22, x21, [sp, #48]
   4bfd8:	ldp	x24, x23, [sp, #32]
   4bfdc:	ldp	x26, x25, [sp, #16]
   4bfe0:	ldp	x29, x30, [sp], #80
   4bfe4:	ret

000000000004bfe8 <__gmp_randclear_mt@@Base>:
   4bfe8:	stp	x29, x30, [sp, #-16]!
   4bfec:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4bff0:	ldr	x8, [x8, #4016]
   4bff4:	ldrsw	x9, [x0]
   4bff8:	ldr	x0, [x0, #8]
   4bffc:	mov	x29, sp
   4c000:	ldr	x8, [x8]
   4c004:	lsl	x1, x9, #3
   4c008:	blr	x8
   4c00c:	ldp	x29, x30, [sp], #16
   4c010:	ret

000000000004c014 <__gmp_randiset_mt@@Base>:
   4c014:	stp	x29, x30, [sp, #-32]!
   4c018:	stp	x20, x19, [sp, #16]
   4c01c:	ldr	x8, [x1, #24]
   4c020:	mov	x20, x0
   4c024:	mov	x29, sp
   4c028:	mov	x19, x1
   4c02c:	str	x8, [x0, #24]
   4c030:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4c034:	ldr	x8, [x8, #3840]
   4c038:	mov	w0, #0x9c8                 	// #2504
   4c03c:	ldr	x8, [x8]
   4c040:	blr	x8
   4c044:	mov	w8, #0x139                 	// #313
   4c048:	str	x0, [x20, #8]
   4c04c:	str	w8, [x20]
   4c050:	ldr	x8, [x19, #8]
   4c054:	mov	x9, xzr
   4c058:	ldr	w10, [x8, x9]
   4c05c:	str	w10, [x0, x9]
   4c060:	add	x9, x9, #0x4
   4c064:	cmp	x9, #0x9c0
   4c068:	b.ne	4c058 <__gmp_randiset_mt@@Base+0x44>  // b.any
   4c06c:	ldr	w8, [x8, #2496]
   4c070:	str	w8, [x0, #2496]
   4c074:	ldp	x20, x19, [sp, #16]
   4c078:	ldp	x29, x30, [sp], #32
   4c07c:	ret

000000000004c080 <__gmp_randinit_mt_noseed@@Base>:
   4c080:	stp	x29, x30, [sp, #-32]!
   4c084:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4c088:	add	x8, x8, #0xca0
   4c08c:	stp	x20, x19, [sp, #16]
   4c090:	str	x8, [x0, #24]
   4c094:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4c098:	ldr	x8, [x8, #3840]
   4c09c:	mov	x19, x0
   4c0a0:	mov	w0, #0x9c8                 	// #2504
   4c0a4:	mov	x29, sp
   4c0a8:	ldr	x8, [x8]
   4c0ac:	blr	x8
   4c0b0:	adrp	x1, 58000 <__gmp_limbroots_table@@Base+0x338>
   4c0b4:	mov	w8, #0x139                 	// #313
   4c0b8:	add	x1, x1, #0x298
   4c0bc:	mov	w2, #0x9c0                 	// #2496
   4c0c0:	mov	x20, x0
   4c0c4:	str	x0, [x19, #8]
   4c0c8:	str	w8, [x19]
   4c0cc:	bl	bff0 <memcpy@plt>
   4c0d0:	mov	w8, #0x80                  	// #128
   4c0d4:	str	w8, [x20, #2496]
   4c0d8:	ldp	x20, x19, [sp, #16]
   4c0dc:	ldp	x29, x30, [sp], #32
   4c0e0:	ret

000000000004c0e4 <__gmp_randinit_mt@@Base>:
   4c0e4:	stp	x29, x30, [sp, #-32]!
   4c0e8:	str	x19, [sp, #16]
   4c0ec:	mov	x29, sp
   4c0f0:	mov	x19, x0
   4c0f4:	bl	c060 <__gmp_randinit_mt_noseed@plt>
   4c0f8:	adrp	x8, 69000 <__gmp_limbroots_table@@Base+0x11338>
   4c0fc:	add	x8, x8, #0xcc0
   4c100:	str	x8, [x19, #24]
   4c104:	ldr	x19, [sp, #16]
   4c108:	ldp	x29, x30, [sp], #32
   4c10c:	ret
   4c110:	sub	sp, sp, #0x50
   4c114:	stp	x29, x30, [sp, #48]
   4c118:	stp	x20, x19, [sp, #64]
   4c11c:	ldr	x19, [x0, #8]
   4c120:	mov	x20, x1
   4c124:	add	x0, sp, #0x18
   4c128:	mov	w1, #0x4de2                	// #19938
   4c12c:	add	x29, sp, #0x30
   4c130:	bl	d300 <__gmpz_init2@plt>
   4c134:	add	x0, sp, #0x8
   4c138:	mov	w1, #0x4de1                	// #19937
   4c13c:	bl	d300 <__gmpz_init2@plt>
   4c140:	add	x0, sp, #0x18
   4c144:	mov	w1, #0x4de1                	// #19937
   4c148:	bl	c470 <__gmpz_setbit@plt>
   4c14c:	add	x0, sp, #0x18
   4c150:	add	x1, sp, #0x18
   4c154:	mov	w2, #0x4e3b                	// #20027
   4c158:	bl	c270 <__gmpz_sub_ui@plt>
   4c15c:	add	x0, sp, #0x8
   4c160:	add	x2, sp, #0x18
   4c164:	mov	x1, x20
   4c168:	bl	cfc0 <__gmpz_mod@plt>
   4c16c:	add	x0, sp, #0x18
   4c170:	bl	cd10 <__gmpz_clear@plt>
   4c174:	add	x0, sp, #0x8
   4c178:	add	x1, sp, #0x8
   4c17c:	mov	w2, #0x2                   	// #2
   4c180:	bl	ca60 <__gmpz_add_ui@plt>
   4c184:	add	x0, sp, #0x8
   4c188:	bl	4c23c <__gmp_randinit_mt@@Base+0x158>
   4c18c:	add	x0, sp, #0x8
   4c190:	mov	w1, #0x4de0                	// #19936
   4c194:	bl	c5f0 <__gmpz_tstbit@plt>
   4c198:	cmp	w0, #0x0
   4c19c:	cset	w8, ne  // ne = any
   4c1a0:	lsl	w8, w8, #31
   4c1a4:	mov	x20, x19
   4c1a8:	add	x0, sp, #0x8
   4c1ac:	mov	w1, #0x4de0                	// #19936
   4c1b0:	str	w8, [x20], #4
   4c1b4:	bl	cb00 <__gmpz_clrbit@plt>
   4c1b8:	sub	x1, x29, #0x8
   4c1bc:	add	x6, sp, #0x8
   4c1c0:	mov	w2, #0xffffffff            	// #-1
   4c1c4:	mov	w3, #0x4                   	// #4
   4c1c8:	mov	x0, x20
   4c1cc:	mov	w4, wzr
   4c1d0:	mov	x5, xzr
   4c1d4:	bl	d400 <__gmpz_export@plt>
   4c1d8:	add	x0, sp, #0x8
   4c1dc:	bl	cd10 <__gmpz_clear@plt>
   4c1e0:	ldur	x8, [x29, #-8]
   4c1e4:	add	x9, x8, #0x1
   4c1e8:	cmp	x9, #0x26f
   4c1ec:	stur	x9, [x29, #-8]
   4c1f0:	b.hi	4c210 <__gmp_randinit_mt@@Base+0x12c>  // b.pmore
   4c1f4:	add	x0, x19, x9, lsl #2
   4c1f8:	mov	w9, #0x9bc                 	// #2492
   4c1fc:	sub	x2, x9, x8, lsl #2
   4c200:	mov	w1, wzr
   4c204:	bl	c780 <memset@plt>
   4c208:	mov	w8, #0x270                 	// #624
   4c20c:	stur	x8, [x29, #-8]
   4c210:	mov	w20, #0x3                   	// #3
   4c214:	mov	x0, x19
   4c218:	bl	d640 <__gmp_mt_recalc_buffer@plt>
   4c21c:	subs	w20, w20, #0x1
   4c220:	b.ne	4c214 <__gmp_randinit_mt@@Base+0x130>  // b.any
   4c224:	mov	w8, #0x80                  	// #128
   4c228:	str	w8, [x19, #2496]
   4c22c:	ldp	x20, x19, [sp, #64]
   4c230:	ldp	x29, x30, [sp, #48]
   4c234:	add	sp, sp, #0x50
   4c238:	ret
   4c23c:	sub	sp, sp, #0x50
   4c240:	stp	x20, x19, [sp, #64]
   4c244:	mov	x19, x0
   4c248:	add	x0, sp, #0x10
   4c24c:	mov	w1, #0x4de1                	// #19937
   4c250:	stp	x29, x30, [sp, #32]
   4c254:	str	x21, [sp, #48]
   4c258:	add	x29, sp, #0x20
   4c25c:	bl	d300 <__gmpz_init2@plt>
   4c260:	mov	x0, sp
   4c264:	mov	x1, x19
   4c268:	bl	c0b0 <__gmpz_init_set@plt>
   4c26c:	mov	w21, #0x8124                	// #33060
   4c270:	mov	w20, #0x20000000            	// #536870912
   4c274:	movk	w21, #0x4011, lsl #16
   4c278:	b	4c284 <__gmp_randinit_mt@@Base+0x1a0>
   4c27c:	lsr	x20, x20, #1
   4c280:	cbz	x20, 4c2f0 <__gmp_randinit_mt@@Base+0x20c>
   4c284:	mov	x0, x19
   4c288:	mov	x1, x19
   4c28c:	mov	x2, x19
   4c290:	bl	c620 <__gmpz_mul@plt>
   4c294:	b	4c2b8 <__gmp_randinit_mt@@Base+0x1d4>
   4c298:	mov	w2, #0x4de1                	// #19937
   4c29c:	mov	x0, x19
   4c2a0:	mov	x1, x19
   4c2a4:	bl	c000 <__gmpz_tdiv_r_2exp@plt>
   4c2a8:	add	x1, sp, #0x10
   4c2ac:	mov	w2, #0x4e37                	// #20023
   4c2b0:	mov	x0, x19
   4c2b4:	bl	d500 <__gmpz_addmul_ui@plt>
   4c2b8:	add	x0, sp, #0x10
   4c2bc:	mov	w2, #0x4de1                	// #19937
   4c2c0:	mov	x1, x19
   4c2c4:	bl	ce30 <__gmpz_tdiv_q_2exp@plt>
   4c2c8:	ldr	w8, [sp, #20]
   4c2cc:	cbnz	w8, 4c298 <__gmp_randinit_mt@@Base+0x1b4>
   4c2d0:	tst	x21, x20
   4c2d4:	b.eq	4c27c <__gmp_randinit_mt@@Base+0x198>  // b.none
   4c2d8:	mov	x2, sp
   4c2dc:	mov	x0, x19
   4c2e0:	mov	x1, x19
   4c2e4:	eor	x21, x21, x20
   4c2e8:	bl	c620 <__gmpz_mul@plt>
   4c2ec:	b	4c2b8 <__gmp_randinit_mt@@Base+0x1d4>
   4c2f0:	add	x0, sp, #0x10
   4c2f4:	bl	cd10 <__gmpz_clear@plt>
   4c2f8:	mov	x0, sp
   4c2fc:	bl	cd10 <__gmpz_clear@plt>
   4c300:	ldp	x20, x19, [sp, #64]
   4c304:	ldr	x21, [sp, #48]
   4c308:	ldp	x29, x30, [sp, #32]
   4c30c:	add	sp, sp, #0x50
   4c310:	ret

000000000004c314 <__gmp_randseed@@Base>:
   4c314:	stp	x29, x30, [sp, #-16]!
   4c318:	ldr	x8, [x0, #24]
   4c31c:	mov	x29, sp
   4c320:	ldr	x8, [x8]
   4c324:	blr	x8
   4c328:	ldp	x29, x30, [sp], #16
   4c32c:	ret

000000000004c330 <__gmp_randseed_ui@@Base>:
   4c330:	sub	sp, sp, #0x30
   4c334:	add	x8, sp, #0x8
   4c338:	cmp	x1, #0x0
   4c33c:	str	x1, [sp, #8]
   4c340:	str	x8, [sp, #24]
   4c344:	cset	w8, ne  // ne = any
   4c348:	add	x1, sp, #0x10
   4c34c:	stp	x29, x30, [sp, #32]
   4c350:	add	x29, sp, #0x20
   4c354:	str	w8, [sp, #20]
   4c358:	bl	d180 <__gmp_randseed@plt>
   4c35c:	ldp	x29, x30, [sp, #32]
   4c360:	add	sp, sp, #0x30
   4c364:	ret

000000000004c368 <__gmp_urandomb_ui@@Base>:
   4c368:	sub	sp, sp, #0x20
   4c36c:	stp	x29, x30, [sp, #16]
   4c370:	str	xzr, [sp, #8]
   4c374:	ldr	x8, [x0, #24]
   4c378:	cmp	x1, #0x40
   4c37c:	mov	w9, #0x40                  	// #64
   4c380:	csel	x2, x1, x9, cc  // cc = lo, ul, last
   4c384:	ldr	x8, [x8, #8]
   4c388:	add	x1, sp, #0x8
   4c38c:	add	x29, sp, #0x10
   4c390:	blr	x8
   4c394:	ldr	x0, [sp, #8]
   4c398:	ldp	x29, x30, [sp, #16]
   4c39c:	add	sp, sp, #0x20
   4c3a0:	ret

000000000004c3a4 <__gmp_urandomm_ui@@Base>:
   4c3a4:	sub	sp, sp, #0x40
   4c3a8:	stp	x29, x30, [sp, #16]
   4c3ac:	stp	x22, x21, [sp, #32]
   4c3b0:	stp	x20, x19, [sp, #48]
   4c3b4:	add	x29, sp, #0x10
   4c3b8:	cbz	x1, 4c428 <__gmp_urandomm_ui@@Base+0x84>
   4c3bc:	sub	x9, x1, #0x1
   4c3c0:	tst	x1, x9
   4c3c4:	clz	x8, x1
   4c3c8:	csetm	x9, eq  // eq = none
   4c3cc:	sub	x8, x9, x8
   4c3d0:	mov	x19, x1
   4c3d4:	mov	x20, x0
   4c3d8:	str	xzr, [sp, #8]
   4c3dc:	add	x21, x8, #0x40
   4c3e0:	mov	w22, #0x50                  	// #80
   4c3e4:	ldr	x8, [x20, #24]
   4c3e8:	add	x1, sp, #0x8
   4c3ec:	mov	x0, x20
   4c3f0:	mov	x2, x21
   4c3f4:	ldr	x8, [x8, #8]
   4c3f8:	blr	x8
   4c3fc:	ldr	x0, [sp, #8]
   4c400:	subs	x8, x0, x19
   4c404:	b.cc	4c414 <__gmp_urandomm_ui@@Base+0x70>  // b.lo, b.ul, b.last
   4c408:	subs	w22, w22, #0x1
   4c40c:	b.ne	4c3e4 <__gmp_urandomm_ui@@Base+0x40>  // b.any
   4c410:	mov	x0, x8
   4c414:	ldp	x20, x19, [sp, #48]
   4c418:	ldp	x22, x21, [sp, #32]
   4c41c:	ldp	x29, x30, [sp, #16]
   4c420:	add	sp, sp, #0x40
   4c424:	ret
   4c428:	bl	c100 <__gmp_divide_by_zero@plt>

Disassembly of section .fini:

000000000004c42c <.fini>:
   4c42c:	stp	x29, x30, [sp, #-16]!
   4c430:	mov	x29, sp
   4c434:	ldp	x29, x30, [sp], #16
   4c438:	ret
