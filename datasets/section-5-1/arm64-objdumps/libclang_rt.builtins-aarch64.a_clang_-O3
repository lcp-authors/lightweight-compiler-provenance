In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_clang_-O3:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	stp	q1, q0, [sp, #-32]!
   4:	ldp	x9, x8, [sp, #16]
   8:	mov	x13, #0x7fff000000000000    	// #9223090561878065152
   c:	mov	w0, #0x1                   	// #1
  10:	and	x12, x8, #0x7fffffffffffffff
  14:	cmp	x9, #0x0
  18:	cset	w10, ne  // ne = any
  1c:	cmp	x12, x13
  20:	cset	w11, hi  // hi = pmore
  24:	csel	w10, w10, w11, eq  // eq = none
  28:	tbnz	w10, #0, cc <__cmptf2+0xcc>
  2c:	ldp	x11, x10, [sp]
  30:	and	x14, x10, #0x7fffffffffffffff
  34:	cmp	x11, #0x0
  38:	cset	w15, ne  // ne = any
  3c:	cmp	x14, x13
  40:	cset	w13, hi  // hi = pmore
  44:	csel	w13, w15, w13, eq  // eq = none
  48:	tbnz	w13, #0, cc <__cmptf2+0xcc>
  4c:	orr	x13, x11, x9
  50:	orr	x12, x14, x12
  54:	orr	x12, x13, x12
  58:	cbz	x12, 88 <__cmptf2+0x88>
  5c:	tst	x10, x8
  60:	b.lt	94 <__cmptf2+0x94>  // b.tstop
  64:	cmp	x9, x11
  68:	cset	w12, cc  // cc = lo, ul, last
  6c:	cmp	x8, x10
  70:	cset	w13, lt  // lt = tstop
  74:	csel	w12, w12, w13, eq  // eq = none
  78:	tbz	w12, #0, b8 <__cmptf2+0xb8>
  7c:	mov	w0, #0xffffffff            	// #-1
  80:	add	sp, sp, #0x20
  84:	ret
  88:	mov	w0, wzr
  8c:	add	sp, sp, #0x20
  90:	ret
  94:	cmp	x9, x11
  98:	cset	w12, hi  // hi = pmore
  9c:	cmp	x8, x10
  a0:	cset	w13, gt
  a4:	csel	w12, w12, w13, eq  // eq = none
  a8:	tbz	w12, #0, b8 <__cmptf2+0xb8>
  ac:	mov	w0, #0xffffffff            	// #-1
  b0:	add	sp, sp, #0x20
  b4:	ret
  b8:	eor	x9, x9, x11
  bc:	eor	x8, x8, x10
  c0:	orr	x8, x9, x8
  c4:	cmp	x8, #0x0
  c8:	cset	w0, ne  // ne = any
  cc:	add	sp, sp, #0x20
  d0:	ret

00000000000000d4 <__getf2>:
  d4:	stp	q1, q0, [sp, #-32]!
  d8:	ldp	x9, x8, [sp, #16]
  dc:	mov	x13, #0x7fff000000000000    	// #9223090561878065152
  e0:	mov	w0, #0xffffffff            	// #-1
  e4:	and	x12, x8, #0x7fffffffffffffff
  e8:	cmp	x9, #0x0
  ec:	cset	w10, ne  // ne = any
  f0:	cmp	x12, x13
  f4:	cset	w11, hi  // hi = pmore
  f8:	csel	w10, w10, w11, eq  // eq = none
  fc:	tbnz	w10, #0, 1a0 <__getf2+0xcc>
 100:	ldp	x11, x10, [sp]
 104:	and	x14, x10, #0x7fffffffffffffff
 108:	cmp	x11, #0x0
 10c:	cset	w15, ne  // ne = any
 110:	cmp	x14, x13
 114:	cset	w13, hi  // hi = pmore
 118:	csel	w13, w15, w13, eq  // eq = none
 11c:	tbnz	w13, #0, 1a0 <__getf2+0xcc>
 120:	orr	x13, x11, x9
 124:	orr	x12, x14, x12
 128:	orr	x12, x13, x12
 12c:	cbz	x12, 15c <__getf2+0x88>
 130:	tst	x10, x8
 134:	b.lt	168 <__getf2+0x94>  // b.tstop
 138:	cmp	x9, x11
 13c:	cset	w12, cc  // cc = lo, ul, last
 140:	cmp	x8, x10
 144:	cset	w13, lt  // lt = tstop
 148:	csel	w12, w12, w13, eq  // eq = none
 14c:	tbz	w12, #0, 18c <__getf2+0xb8>
 150:	mov	w0, #0xffffffff            	// #-1
 154:	add	sp, sp, #0x20
 158:	ret
 15c:	mov	w0, wzr
 160:	add	sp, sp, #0x20
 164:	ret
 168:	cmp	x9, x11
 16c:	cset	w12, hi  // hi = pmore
 170:	cmp	x8, x10
 174:	cset	w13, gt
 178:	csel	w12, w12, w13, eq  // eq = none
 17c:	tbz	w12, #0, 18c <__getf2+0xb8>
 180:	mov	w0, #0xffffffff            	// #-1
 184:	add	sp, sp, #0x20
 188:	ret
 18c:	eor	x9, x9, x11
 190:	eor	x8, x8, x10
 194:	orr	x8, x9, x8
 198:	cmp	x8, #0x0
 19c:	cset	w0, ne  // ne = any
 1a0:	add	sp, sp, #0x20
 1a4:	ret

00000000000001a8 <__unordtf2>:
 1a8:	stp	q1, q0, [sp, #-32]!
 1ac:	ldp	x8, x9, [sp, #16]
 1b0:	ldp	x10, x11, [sp], #32
 1b4:	mov	x12, #0x7fff000000000000    	// #9223090561878065152
 1b8:	and	x9, x9, #0x7fffffffffffffff
 1bc:	cmp	x8, #0x0
 1c0:	and	x8, x11, #0x7fffffffffffffff
 1c4:	cset	w11, ne  // ne = any
 1c8:	cmp	x9, x12
 1cc:	cset	w9, hi  // hi = pmore
 1d0:	csel	w9, w11, w9, eq  // eq = none
 1d4:	cmp	x10, #0x0
 1d8:	cset	w10, ne  // ne = any
 1dc:	cmp	x8, x12
 1e0:	cset	w8, hi  // hi = pmore
 1e4:	csel	w8, w10, w8, eq  // eq = none
 1e8:	orr	w0, w9, w8
 1ec:	ret

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	fmov	x11, d0
   4:	and	x9, x11, #0x7fffffffffffffff
   8:	mov	x8, #0xfff0000000000000    	// #-4503599627370496
   c:	add	x8, x9, x8
  10:	lsr	x8, x8, #53
  14:	cmp	x8, #0x3fe
  18:	and	x8, x11, #0x8000000000000000
  1c:	b.hi	3c <__extenddftf2+0x3c>  // b.pmore
  20:	mov	x11, #0x3c00000000000000    	// #4323455642275676160
  24:	lsl	x10, x9, #60
  28:	add	x9, x11, x9, lsr #4
  2c:	orr	x8, x9, x8
  30:	stp	x10, x8, [sp, #-16]!
  34:	ldr	q0, [sp], #16
  38:	ret
  3c:	lsr	x10, x9, #52
  40:	cmp	x10, #0x7ff
  44:	b.cc	64 <__extenddftf2+0x64>  // b.lo, b.ul, b.last
  48:	lsr	x9, x11, #4
  4c:	lsl	x10, x11, #60
  50:	orr	x9, x9, #0x7fff000000000000
  54:	orr	x8, x9, x8
  58:	stp	x10, x8, [sp, #-16]!
  5c:	ldr	q0, [sp], #16
  60:	ret
  64:	cbz	x9, b8 <__extenddftf2+0xb8>
  68:	clz	x11, x9
  6c:	add	w10, w11, #0x31
  70:	neg	x13, x10
  74:	cmp	x10, #0x0
  78:	lsl	x14, x9, x10
  7c:	lsr	x13, x9, x13
  80:	lsl	x9, x9, x10
  84:	sub	x10, x10, #0x40
  88:	csel	x13, xzr, x13, eq  // eq = none
  8c:	cmp	x10, #0x0
  90:	mov	w12, #0x3c0c                	// #15372
  94:	csel	x13, x14, x13, ge  // ge = tcont
  98:	csel	x10, xzr, x9, ge  // ge = tcont
  9c:	eor	x9, x13, #0x1000000000000
  a0:	sub	w11, w12, w11
  a4:	orr	x9, x9, x11, lsl #48
  a8:	orr	x8, x9, x8
  ac:	stp	x10, x8, [sp, #-16]!
  b0:	ldr	q0, [sp], #16
  b4:	ret
  b8:	mov	x10, xzr
  bc:	orr	x8, x9, x8
  c0:	stp	x10, x8, [sp, #-16]!
  c4:	ldr	q0, [sp], #16
  c8:	ret

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	fmov	w11, s0
   4:	and	w9, w11, #0x7fffffff
   8:	sub	w8, w9, #0x800, lsl #12
   c:	lsr	w8, w8, #24
  10:	cmp	w8, #0x7e
  14:	and	w8, w11, #0x80000000
  18:	b.hi	38 <__extendsftf2+0x38>  // b.pmore
  1c:	mov	x11, #0x3f80000000000000    	// #4575657221408423936
  20:	mov	x10, xzr
  24:	add	x9, x11, x9, lsl #25
  28:	orr	x8, x9, x8, lsl #32
  2c:	stp	x10, x8, [sp, #-16]!
  30:	ldr	q0, [sp], #16
  34:	ret
  38:	lsr	w10, w9, #23
  3c:	cmp	w10, #0xff
  40:	b.cc	64 <__extendsftf2+0x64>  // b.lo, b.ul, b.last
  44:	mov	w9, w11
  48:	lsl	x9, x9, #25
  4c:	mov	x10, xzr
  50:	orr	x9, x9, #0x7fff000000000000
  54:	orr	x8, x9, x8, lsl #32
  58:	stp	x10, x8, [sp, #-16]!
  5c:	ldr	q0, [sp], #16
  60:	ret
  64:	cbz	w9, b8 <__extendsftf2+0xb8>
  68:	clz	w11, w9
  6c:	add	w10, w11, #0x51
  70:	neg	x13, x10
  74:	cmp	x10, #0x0
  78:	lsl	x14, x9, x10
  7c:	lsr	x13, x9, x13
  80:	lsl	x9, x9, x10
  84:	sub	x10, x10, #0x40
  88:	csel	x13, xzr, x13, eq  // eq = none
  8c:	cmp	x10, #0x0
  90:	mov	w12, #0x3f89                	// #16265
  94:	csel	x13, x14, x13, ge  // ge = tcont
  98:	csel	x10, xzr, x9, ge  // ge = tcont
  9c:	eor	x9, x13, #0x1000000000000
  a0:	sub	w11, w12, w11
  a4:	orr	x9, x9, x11, lsl #48
  a8:	orr	x8, x9, x8, lsl #32
  ac:	stp	x10, x8, [sp, #-16]!
  b0:	ldr	q0, [sp], #16
  b4:	ret
  b8:	mov	x10, xzr
  bc:	mov	x9, xzr
  c0:	orr	x8, x9, x8, lsl #32
  c4:	stp	x10, x8, [sp, #-16]!
  c8:	ldr	q0, [sp], #16
  cc:	ret

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w10, #0x3fff                	// #16383
   c:	ubfx	x9, x8, #48, #15
  10:	cmp	w9, w10
  14:	b.cs	24 <__fixtfdi+0x24>  // b.hs, b.nlast
  18:	mov	x0, xzr
  1c:	add	sp, sp, #0x10
  20:	ret
  24:	mov	w10, #0xffffc001            	// #-16383
  28:	add	w10, w9, w10
  2c:	cmp	w10, #0x40
  30:	b.cc	48 <__fixtfdi+0x48>  // b.lo, b.ul, b.last
  34:	cmp	x8, #0x0
  38:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  3c:	cinv	x0, x8, ge  // ge = tcont
  40:	add	sp, sp, #0x10
  44:	ret
  48:	ldr	x10, [sp], #16
  4c:	mov	w12, #0x406f                	// #16495
  50:	mov	x11, #0x1000000000000       	// #281474976710656
  54:	sub	w9, w12, w9
  58:	bfxil	x11, x8, #0, #48
  5c:	neg	x12, x9
  60:	cmp	x9, #0x0
  64:	lsl	x12, x11, x12
  68:	lsr	x11, x11, x9
  6c:	lsr	x10, x10, x9
  70:	sub	x9, x9, #0x40
  74:	csel	x12, xzr, x12, eq  // eq = none
  78:	cmp	x9, #0x0
  7c:	orr	x9, x10, x12
  80:	csel	x9, x11, x9, ge  // ge = tcont
  84:	cmp	x8, #0x0
  88:	cneg	x0, x9, lt  // lt = tstop
  8c:	ret

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w10, #0x3fff                	// #16383
   c:	ubfx	x9, x8, #48, #15
  10:	cmp	w9, w10
  14:	b.cs	24 <__fixtfsi+0x24>  // b.hs, b.nlast
  18:	mov	w0, wzr
  1c:	add	sp, sp, #0x10
  20:	ret
  24:	mov	w10, #0xffffc001            	// #-16383
  28:	add	w10, w9, w10
  2c:	cmp	w10, #0x20
  30:	b.cc	48 <__fixtfsi+0x48>  // b.lo, b.ul, b.last
  34:	cmp	x8, #0x0
  38:	mov	w8, #0x80000000            	// #-2147483648
  3c:	cinv	w0, w8, ge  // ge = tcont
  40:	add	sp, sp, #0x10
  44:	ret
  48:	ldr	x10, [sp], #16
  4c:	mov	w12, #0x406f                	// #16495
  50:	mov	x11, #0x1000000000000       	// #281474976710656
  54:	sub	w9, w12, w9
  58:	bfxil	x11, x8, #0, #48
  5c:	neg	x12, x9
  60:	cmp	x9, #0x0
  64:	lsl	x12, x11, x12
  68:	lsr	x11, x11, x9
  6c:	lsr	x10, x10, x9
  70:	sub	x9, x9, #0x40
  74:	csel	x12, xzr, x12, eq  // eq = none
  78:	cmp	x9, #0x0
  7c:	orr	x9, x10, x12
  80:	csel	x9, x11, x9, ge  // ge = tcont
  84:	cmp	x8, #0x0
  88:	cneg	w0, w9, lt  // lt = tstop
  8c:	ret

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x12, [sp, #8]
   8:	mov	w8, #0x1                   	// #1
   c:	mov	w10, #0x3fff                	// #16383
  10:	cmp	x12, #0x0
  14:	ubfx	x13, x12, #48, #15
  18:	csetm	x9, lt  // lt = tstop
  1c:	cneg	x8, x8, lt  // lt = tstop
  20:	cmp	w13, w10
  24:	b.cs	38 <__fixtfti+0x38>  // b.hs, b.nlast
  28:	mov	x0, xzr
  2c:	mov	x1, xzr
  30:	add	sp, sp, #0x10
  34:	ret
  38:	mov	w10, #0xffffc001            	// #-16383
  3c:	add	w10, w13, w10
  40:	cmp	w10, #0x80
  44:	b.cc	64 <__fixtfti+0x64>  // b.lo, b.ul, b.last
  48:	mov	x8, #0xffffffffffffffff    	// #-1
  4c:	cmp	x12, #0x0
  50:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  54:	eor	x0, x8, x12, asr #63
  58:	cinv	x1, x9, lt  // lt = tstop
  5c:	add	sp, sp, #0x10
  60:	ret
  64:	ldr	x10, [sp]
  68:	mov	x11, #0x1000000000000       	// #281474976710656
  6c:	mov	w14, #0x406e                	// #16494
  70:	cmp	w13, w14
  74:	bfxil	x11, x12, #0, #48
  78:	b.hi	cc <__fixtfti+0xcc>  // b.pmore
  7c:	mov	w12, #0x406f                	// #16495
  80:	sub	w12, w12, w13
  84:	neg	x13, x12
  88:	cmp	x12, #0x0
  8c:	lsr	x14, x11, x12
  90:	sub	x15, x12, #0x40
  94:	lsr	x10, x10, x12
  98:	lsr	x12, x11, x12
  9c:	lsl	x11, x11, x13
  a0:	csel	x11, xzr, x11, eq  // eq = none
  a4:	cmp	x15, #0x0
  a8:	orr	x10, x10, x11
  ac:	csel	x10, x12, x10, ge  // ge = tcont
  b0:	umulh	x11, x10, x8
  b4:	csel	x13, xzr, x14, ge  // ge = tcont
  b8:	madd	x9, x10, x9, x11
  bc:	madd	x1, x13, x8, x9
  c0:	mul	x0, x10, x8
  c4:	add	sp, sp, #0x10
  c8:	ret
  cc:	mov	w12, #0xffffbf91            	// #-16495
  d0:	add	w12, w13, w12
  d4:	neg	x13, x12
  d8:	cmp	x12, #0x0
  dc:	lsl	x11, x11, x12
  e0:	lsl	x14, x10, x12
  e4:	lsr	x13, x10, x13
  e8:	lsl	x10, x10, x12
  ec:	sub	x12, x12, #0x40
  f0:	csel	x13, xzr, x13, eq  // eq = none
  f4:	cmp	x12, #0x0
  f8:	csel	x10, xzr, x10, ge  // ge = tcont
  fc:	orr	x11, x13, x11
 100:	umulh	x12, x10, x8
 104:	csel	x11, x14, x11, ge  // ge = tcont
 108:	madd	x9, x10, x9, x12
 10c:	madd	x1, x11, x8, x9
 110:	mul	x0, x10, x8
 114:	add	sp, sp, #0x10
 118:	ret

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	x0, xzr
   c:	tbnz	x8, #63, 78 <__fixunstfdi+0x78>
  10:	ubfx	x9, x8, #48, #15
  14:	mov	w10, #0x3fff                	// #16383
  18:	cmp	w9, w10
  1c:	b.cc	78 <__fixunstfdi+0x78>  // b.lo, b.ul, b.last
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x3f
  2c:	b.ls	3c <__fixunstfdi+0x3c>  // b.plast
  30:	mov	x0, #0xffffffffffffffff    	// #-1
  34:	add	sp, sp, #0x10
  38:	ret
  3c:	ldr	x10, [sp]
  40:	mov	x11, #0x1000000000000       	// #281474976710656
  44:	mov	w12, #0x406f                	// #16495
  48:	bfxil	x11, x8, #0, #48
  4c:	sub	w8, w12, w9
  50:	neg	x9, x8
  54:	cmp	x8, #0x0
  58:	lsl	x9, x11, x9
  5c:	lsr	x12, x11, x8
  60:	sub	x13, x8, #0x40
  64:	csel	x9, xzr, x9, eq  // eq = none
  68:	lsr	x8, x10, x8
  6c:	orr	x8, x8, x9
  70:	cmp	x13, #0x0
  74:	csel	x0, x12, x8, ge  // ge = tcont
  78:	add	sp, sp, #0x10
  7c:	ret

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x8, [sp, #8]
   8:	mov	w0, wzr
   c:	tbnz	x8, #63, 78 <__fixunstfsi+0x78>
  10:	ubfx	x9, x8, #48, #15
  14:	mov	w10, #0x3fff                	// #16383
  18:	cmp	w9, w10
  1c:	b.cc	78 <__fixunstfsi+0x78>  // b.lo, b.ul, b.last
  20:	mov	w10, #0xffffc001            	// #-16383
  24:	add	w10, w9, w10
  28:	cmp	w10, #0x1f
  2c:	b.ls	3c <__fixunstfsi+0x3c>  // b.plast
  30:	mov	w0, #0xffffffff            	// #-1
  34:	add	sp, sp, #0x10
  38:	ret
  3c:	ldr	x10, [sp]
  40:	mov	x11, #0x1000000000000       	// #281474976710656
  44:	mov	w12, #0x406f                	// #16495
  48:	bfxil	x11, x8, #0, #48
  4c:	sub	w8, w12, w9
  50:	neg	x9, x8
  54:	cmp	x8, #0x0
  58:	lsl	x9, x11, x9
  5c:	lsr	x12, x11, x8
  60:	sub	x13, x8, #0x40
  64:	csel	x9, xzr, x9, eq  // eq = none
  68:	lsr	x8, x10, x8
  6c:	orr	x8, x8, x9
  70:	cmp	x13, #0x0
  74:	csel	x0, x12, x8, ge  // ge = tcont
  78:	add	sp, sp, #0x10
  7c:	ret

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	str	q0, [sp, #-16]!
   4:	ldr	x11, [sp, #8]
   8:	mov	x0, xzr
   c:	tbnz	x11, #63, 44 <__fixunstfti+0x44>
  10:	ubfx	x10, x11, #48, #15
  14:	mov	w8, #0x3fff                	// #16383
  18:	cmp	w10, w8
  1c:	mov	x1, x0
  20:	b.cc	e0 <__fixunstfti+0xe0>  // b.lo, b.ul, b.last
  24:	mov	w8, #0xffffc001            	// #-16383
  28:	add	w8, w10, w8
  2c:	cmp	w8, #0x7f
  30:	b.ls	50 <__fixunstfti+0x50>  // b.plast
  34:	mov	x0, #0xffffffffffffffff    	// #-1
  38:	mov	x1, #0xffffffffffffffff    	// #-1
  3c:	add	sp, sp, #0x10
  40:	ret
  44:	mov	x1, x0
  48:	add	sp, sp, #0x10
  4c:	ret
  50:	ldr	x8, [sp]
  54:	mov	x9, #0x1000000000000       	// #281474976710656
  58:	mov	w12, #0x406e                	// #16494
  5c:	cmp	w10, w12
  60:	bfxil	x9, x11, #0, #48
  64:	b.hi	a8 <__fixunstfti+0xa8>  // b.pmore
  68:	mov	w11, #0x406f                	// #16495
  6c:	sub	w10, w11, w10
  70:	neg	x11, x10
  74:	cmp	x10, #0x0
  78:	lsl	x11, x9, x11
  7c:	sub	x13, x10, #0x40
  80:	lsr	x8, x8, x10
  84:	csel	x11, xzr, x11, eq  // eq = none
  88:	lsr	x12, x9, x10
  8c:	cmp	x13, #0x0
  90:	orr	x8, x8, x11
  94:	lsr	x9, x9, x10
  98:	csel	x1, xzr, x12, ge  // ge = tcont
  9c:	csel	x0, x9, x8, ge  // ge = tcont
  a0:	add	sp, sp, #0x10
  a4:	ret
  a8:	mov	w11, #0xffffbf91            	// #-16495
  ac:	add	w10, w10, w11
  b0:	neg	x11, x10
  b4:	cmp	x10, #0x0
  b8:	lsr	x11, x8, x11
  bc:	sub	x13, x10, #0x40
  c0:	lsl	x9, x9, x10
  c4:	csel	x11, xzr, x11, eq  // eq = none
  c8:	lsl	x12, x8, x10
  cc:	cmp	x13, #0x0
  d0:	orr	x9, x11, x9
  d4:	lsl	x8, x8, x10
  d8:	csel	x0, xzr, x12, ge  // ge = tcont
  dc:	csel	x1, x8, x9, ge  // ge = tcont
  e0:	add	sp, sp, #0x10
  e4:	ret

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	x0, 64 <__floatditf+0x64>
   8:	cmp	x0, #0x0
   c:	cneg	x10, x0, mi  // mi = first
  10:	clz	x11, x10
  14:	mov	w9, #0x403e                	// #16446
  18:	add	w12, w11, #0x31
  1c:	sub	w9, w9, w11
  20:	neg	x11, x12
  24:	cmp	x12, #0x0
  28:	lsl	x13, x10, x12
  2c:	sub	x14, x12, #0x40
  30:	lsl	x12, x10, x12
  34:	lsr	x10, x10, x11
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x14, #0x0
  40:	csel	x10, x13, x10, ge  // ge = tcont
  44:	eor	x10, x10, #0x1000000000000
  48:	and	x8, x0, #0x8000000000000000
  4c:	add	x9, x10, x9, lsl #48
  50:	csel	x11, xzr, x12, ge  // ge = tcont
  54:	orr	x8, x9, x8
  58:	stp	x11, x8, [sp]
  5c:	ldr	q0, [sp], #16
  60:	ret
  64:	adrp	x8, 0 <__floatditf>
  68:	ldr	q0, [x8]
  6c:	add	sp, sp, #0x10
  70:	ret

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	w0, 64 <__floatsitf+0x64>
   8:	cmp	w0, #0x0
   c:	cneg	w10, w0, mi  // mi = first
  10:	clz	w11, w10
  14:	mov	w9, #0x401e                	// #16414
  18:	add	w12, w11, #0x51
  1c:	sub	w9, w9, w11
  20:	neg	x11, x12
  24:	cmp	x12, #0x0
  28:	lsl	x13, x10, x12
  2c:	sub	x14, x12, #0x40
  30:	lsl	x12, x10, x12
  34:	lsr	x10, x10, x11
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x14, #0x0
  40:	csel	x10, x13, x10, ge  // ge = tcont
  44:	eor	x10, x10, #0x1000000000000
  48:	and	w8, w0, #0x80000000
  4c:	add	x9, x10, x9, lsl #48
  50:	csel	x11, xzr, x12, ge  // ge = tcont
  54:	orr	x8, x9, x8, lsl #32
  58:	stp	x11, x8, [sp]
  5c:	ldr	q0, [sp], #16
  60:	ret
  64:	adrp	x8, 0 <__floatsitf>
  68:	ldr	q0, [x8]
  6c:	add	sp, sp, #0x10
  70:	ret

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	sub	sp, sp, #0x40
   4:	stp	x29, x30, [sp, #16]
   8:	str	x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	add	x29, sp, #0x10
  14:	orr	x8, x0, x1
  18:	cbz	x8, 70 <__floattitf+0x70>
  1c:	asr	x21, x1, #63
  20:	eor	x9, x21, x0
  24:	eor	x8, x21, x1
  28:	subs	x19, x9, x21
  2c:	sbcs	x20, x8, x21
  30:	mov	x0, x19
  34:	mov	x1, x20
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x72
  4c:	sub	w8, w8, w0
  50:	b.lt	7c <__floattitf+0x7c>  // b.tstop
  54:	cmp	w9, #0x73
  58:	b.eq	130 <__floattitf+0x130>  // b.none
  5c:	cmp	w9, #0x72
  60:	b.ne	b4 <__floattitf+0xb4>  // b.any
  64:	extr	x20, x20, x19, #63
  68:	lsl	x19, x19, #1
  6c:	b	130 <__floattitf+0x130>
  70:	adrp	x8, 0 <__floattitf>
  74:	ldr	q0, [x8]
  78:	b	184 <__floattitf+0x184>
  7c:	sub	w9, w0, #0xf
  80:	neg	x10, x9
  84:	cmp	x9, #0x0
  88:	lsr	x10, x19, x10
  8c:	lsl	x11, x20, x9
  90:	sub	x13, x9, #0x40
  94:	csel	x10, xzr, x10, eq  // eq = none
  98:	lsl	x12, x19, x9
  9c:	lsl	x9, x19, x9
  a0:	cmp	x13, #0x0
  a4:	orr	x10, x10, x11
  a8:	csel	x10, x12, x10, ge  // ge = tcont
  ac:	csel	x11, xzr, x9, ge  // ge = tcont
  b0:	b	168 <__floattitf+0x168>
  b4:	mov	w10, #0xd                   	// #13
  b8:	sub	w10, w10, w0
  bc:	neg	x13, x10
  c0:	cmp	x10, #0x0
  c4:	sub	x14, x10, #0x40
  c8:	lsl	x13, x20, x13
  cc:	add	w11, w0, #0x73
  d0:	csel	x13, xzr, x13, eq  // eq = none
  d4:	cmp	x14, #0x0
  d8:	lsr	x14, x19, x10
  dc:	neg	x12, x11
  e0:	orr	x13, x14, x13
  e4:	lsr	x14, x20, x10
  e8:	lsr	x10, x20, x10
  ec:	lsl	x15, x20, x11
  f0:	csel	x10, x10, x13, ge  // ge = tcont
  f4:	lsr	x12, x19, x12
  f8:	csel	x20, xzr, x14, ge  // ge = tcont
  fc:	cmp	x11, #0x0
 100:	lsl	x13, x19, x11
 104:	lsl	x16, x19, x11
 108:	sub	x11, x11, #0x40
 10c:	csel	x12, xzr, x12, eq  // eq = none
 110:	cmp	x11, #0x0
 114:	orr	x11, x12, x15
 118:	csel	x11, x13, x11, ge  // ge = tcont
 11c:	csel	x12, xzr, x16, ge  // ge = tcont
 120:	orr	x11, x12, x11
 124:	cmp	x11, #0x0
 128:	cset	w11, ne  // ne = any
 12c:	orr	x19, x10, x11
 130:	ubfx	x10, x19, #2, #1
 134:	orr	x10, x10, x19
 138:	adds	x10, x10, #0x1
 13c:	adcs	x12, x20, xzr
 140:	mov	w11, #0x2                   	// #2
 144:	tst	x12, #0x8000000000000
 148:	cinc	x11, x11, ne  // ne = any
 14c:	lsl	x13, x12, #1
 150:	eor	x15, x11, #0x3f
 154:	lsr	x14, x10, x11
 158:	asr	x10, x12, x11
 15c:	lsl	x11, x13, x15
 160:	orr	x11, x14, x11
 164:	csel	w8, w8, w9, eq  // eq = none
 168:	mov	w12, #0x3fff                	// #16383
 16c:	and	x9, x21, #0x8000000000000000
 170:	add	w8, w8, w12
 174:	orr	x8, x9, x8, lsl #48
 178:	bfxil	x8, x10, #0, #48
 17c:	stp	x11, x8, [sp]
 180:	ldr	q0, [sp]
 184:	ldp	x20, x19, [sp, #48]
 188:	ldr	x21, [sp, #32]
 18c:	ldp	x29, x30, [sp, #16]
 190:	add	sp, sp, #0x40
 194:	ret

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	x0, 5c <__floatunditf+0x5c>
   8:	clz	x8, x0
   c:	mov	w9, #0x70                  	// #112
  10:	eor	w8, w8, #0x3f
  14:	mov	w10, #0x3fff                	// #16383
  18:	sub	w9, w9, w8
  1c:	add	w8, w8, w10
  20:	neg	x10, x9
  24:	cmp	x9, #0x0
  28:	sub	x12, x9, #0x40
  2c:	lsr	x10, x0, x10
  30:	lsl	x11, x0, x9
  34:	lsl	x9, x0, x9
  38:	csel	x10, xzr, x10, eq  // eq = none
  3c:	cmp	x12, #0x0
  40:	csel	x9, x9, x10, ge  // ge = tcont
  44:	eor	x9, x9, #0x1000000000000
  48:	csel	x11, xzr, x11, ge  // ge = tcont
  4c:	add	x8, x9, x8, lsl #48
  50:	stp	x11, x8, [sp]
  54:	ldr	q0, [sp], #16
  58:	ret
  5c:	adrp	x8, 0 <__floatunditf>
  60:	ldr	q0, [x8]
  64:	add	sp, sp, #0x10
  68:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	sub	sp, sp, #0x10
   4:	cbz	w0, 60 <__floatunsitf+0x60>
   8:	clz	w8, w0
   c:	mov	w9, #0x70                  	// #112
  10:	eor	w8, w8, #0x1f
  14:	mov	w11, #0x3fff                	// #16383
  18:	sub	w9, w9, w8
  1c:	mov	w10, w0
  20:	add	w8, w8, w11
  24:	neg	x11, x9
  28:	cmp	x9, #0x0
  2c:	lsl	x12, x10, x9
  30:	sub	x13, x9, #0x40
  34:	lsl	x9, x10, x9
  38:	lsr	x10, x10, x11
  3c:	csel	x10, xzr, x10, eq  // eq = none
  40:	cmp	x13, #0x0
  44:	csel	x9, x9, x10, ge  // ge = tcont
  48:	eor	x9, x9, #0x1000000000000
  4c:	csel	x11, xzr, x12, ge  // ge = tcont
  50:	add	x8, x9, x8, lsl #48
  54:	stp	x11, x8, [sp]
  58:	ldr	q0, [sp], #16
  5c:	ret
  60:	adrp	x8, 0 <__floatunsitf>
  64:	ldr	q0, [x8]
  68:	add	sp, sp, #0x10
  6c:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	orr	x8, x0, x1
  14:	cbz	x8, 58 <__floatuntitf+0x58>
  18:	mov	x20, x1
  1c:	mov	x19, x0
  20:	bl	0 <__clzti2>
  24:	mov	w8, #0x80                  	// #128
  28:	sub	w9, w8, w0
  2c:	mov	w8, #0x7f                  	// #127
  30:	cmp	w9, #0x72
  34:	sub	w8, w8, w0
  38:	b.lt	70 <__floatuntitf+0x70>  // b.tstop
  3c:	cmp	w9, #0x73
  40:	b.eq	124 <__floatuntitf+0x124>  // b.none
  44:	cmp	w9, #0x72
  48:	b.ne	a8 <__floatuntitf+0xa8>  // b.any
  4c:	extr	x20, x20, x19, #63
  50:	lsl	x19, x19, #1
  54:	b	124 <__floatuntitf+0x124>
  58:	adrp	x8, 0 <__floatuntitf>
  5c:	ldr	q0, [x8]
  60:	ldp	x20, x19, [sp, #32]
  64:	ldp	x29, x30, [sp, #16]
  68:	add	sp, sp, #0x30
  6c:	ret
  70:	sub	w9, w0, #0xf
  74:	neg	x10, x9
  78:	cmp	x9, #0x0
  7c:	lsr	x10, x19, x10
  80:	lsl	x11, x20, x9
  84:	sub	x13, x9, #0x40
  88:	csel	x10, xzr, x10, eq  // eq = none
  8c:	lsl	x12, x19, x9
  90:	lsl	x9, x19, x9
  94:	cmp	x13, #0x0
  98:	orr	x10, x10, x11
  9c:	csel	x10, x12, x10, ge  // ge = tcont
  a0:	csel	x11, xzr, x9, ge  // ge = tcont
  a4:	b	15c <__floatuntitf+0x15c>
  a8:	mov	w10, #0xd                   	// #13
  ac:	sub	w10, w10, w0
  b0:	neg	x13, x10
  b4:	cmp	x10, #0x0
  b8:	sub	x14, x10, #0x40
  bc:	lsl	x13, x20, x13
  c0:	add	w11, w0, #0x73
  c4:	csel	x13, xzr, x13, eq  // eq = none
  c8:	cmp	x14, #0x0
  cc:	lsr	x14, x19, x10
  d0:	neg	x12, x11
  d4:	orr	x13, x14, x13
  d8:	lsr	x14, x20, x10
  dc:	lsr	x10, x20, x10
  e0:	lsl	x15, x20, x11
  e4:	csel	x10, x10, x13, ge  // ge = tcont
  e8:	lsr	x12, x19, x12
  ec:	csel	x20, xzr, x14, ge  // ge = tcont
  f0:	cmp	x11, #0x0
  f4:	lsl	x13, x19, x11
  f8:	lsl	x16, x19, x11
  fc:	sub	x11, x11, #0x40
 100:	csel	x12, xzr, x12, eq  // eq = none
 104:	cmp	x11, #0x0
 108:	orr	x11, x12, x15
 10c:	csel	x11, x13, x11, ge  // ge = tcont
 110:	csel	x12, xzr, x16, ge  // ge = tcont
 114:	orr	x11, x12, x11
 118:	cmp	x11, #0x0
 11c:	cset	w11, ne  // ne = any
 120:	orr	x19, x10, x11
 124:	ubfx	x10, x19, #2, #1
 128:	orr	x10, x10, x19
 12c:	adds	x10, x10, #0x1
 130:	adcs	x12, x20, xzr
 134:	mov	w11, #0x2                   	// #2
 138:	tst	x12, #0x8000000000000
 13c:	cinc	x11, x11, ne  // ne = any
 140:	lsl	x13, x12, #1
 144:	eor	x15, x11, #0x3f
 148:	lsr	x14, x10, x11
 14c:	lsr	x10, x12, x11
 150:	lsl	x11, x13, x15
 154:	orr	x11, x14, x11
 158:	csel	w8, w8, w9, eq  // eq = none
 15c:	mov	w9, #0x3fff                	// #16383
 160:	add	w8, w8, w9
 164:	bfi	x10, x8, #48, #16
 168:	stp	x11, x10, [sp]
 16c:	ldr	q0, [sp]
 170:	ldp	x20, x19, [sp, #32]
 174:	ldp	x29, x30, [sp, #16]
 178:	add	sp, sp, #0x30
 17c:	ret

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x28, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	sub	sp, sp, #0x2d0
  14:	str	q1, [sp, #192]
  18:	mov	v1.16b, v2.16b
  1c:	stp	q2, q3, [sp, #160]
  20:	str	q0, [sp, #144]
  24:	bl	0 <__multf3>
  28:	str	q0, [sp, #80]
  2c:	ldp	q1, q0, [sp, #176]
  30:	bl	0 <__multf3>
  34:	str	q0, [sp, #64]
  38:	ldr	q0, [sp, #144]
  3c:	ldr	q1, [sp, #176]
  40:	bl	0 <__multf3>
  44:	str	q0, [sp, #48]
  48:	ldr	q0, [sp, #192]
  4c:	ldr	q1, [sp, #160]
  50:	bl	0 <__multf3>
  54:	str	q0, [sp, #32]
  58:	ldp	q1, q0, [sp, #64]
  5c:	bl	0 <__subtf3>
  60:	str	q0, [sp, #112]
  64:	ldp	q0, q1, [sp, #32]
  68:	bl	0 <__addtf3>
  6c:	str	q0, [sp, #96]
  70:	ldr	q0, [sp, #112]
  74:	mov	v1.16b, v0.16b
  78:	bl	0 <__unordtf2>
  7c:	cbz	w0, f8 <__multc3+0xf8>
  80:	ldr	q0, [sp, #96]
  84:	mov	v1.16b, v0.16b
  88:	bl	0 <__unordtf2>
  8c:	cbz	w0, f8 <__multc3+0xf8>
  90:	ldr	q0, [sp, #144]
  94:	stur	q0, [x29, #-32]
  98:	ldurb	w8, [x29, #-17]
  9c:	and	w8, w8, #0x7f
  a0:	sturb	w8, [x29, #-17]
  a4:	ldr	q0, [sp, #192]
  a8:	stur	q0, [x29, #-16]
  ac:	ldurb	w8, [x29, #-1]
  b0:	and	w8, w8, #0x7f
  b4:	sturb	w8, [x29, #-1]
  b8:	ldp	q2, q0, [x29, #-32]
  bc:	adrp	x8, 0 <__multc3>
  c0:	ldr	q1, [x8]
  c4:	stp	q2, q0, [sp]
  c8:	mov	v0.16b, v2.16b
  cc:	str	q1, [sp, #128]
  d0:	bl	0 <__eqtf2>
  d4:	adrp	x19, 0 <__multc3>
  d8:	add	x19, x19, #0x0
  dc:	cbz	w0, 100 <__multc3+0x100>
  e0:	ldr	q0, [sp, #16]
  e4:	ldr	q1, [sp, #128]
  e8:	bl	0 <__netf2>
  ec:	cbz	w0, 100 <__multc3+0x100>
  f0:	mov	w20, wzr
  f4:	b	1fc <__multc3+0x1fc>
  f8:	ldr	q1, [sp, #96]
  fc:	b	548 <__multc3+0x548>
 100:	ldr	q0, [sp, #16]
 104:	ldr	q1, [sp, #128]
 108:	bl	0 <__eqtf2>
 10c:	ldr	q0, [sp]
 110:	ldr	q1, [sp, #128]
 114:	cmp	w0, #0x0
 118:	cset	w20, eq  // eq = none
 11c:	bl	0 <__eqtf2>
 120:	cmp	w0, #0x0
 124:	cset	w8, eq  // eq = none
 128:	ldr	q0, [x19, w8, uxtw #4]
 12c:	ldr	q1, [sp, #144]
 130:	adrp	x10, 0 <__multc3>
 134:	stp	q0, q1, [x29, #-64]
 138:	ldurb	w8, [x29, #-33]
 13c:	ldurb	w9, [x29, #-49]
 140:	ldr	q0, [x19, w20, uxtw #4]
 144:	bfxil	w8, w9, #0, #7
 148:	sturb	w8, [x29, #-49]
 14c:	ldur	q1, [x29, #-64]
 150:	str	q1, [sp, #144]
 154:	ldr	q1, [sp, #192]
 158:	stp	q0, q1, [x29, #-96]
 15c:	ldurb	w8, [x29, #-65]
 160:	ldurb	w9, [x29, #-81]
 164:	ldr	q1, [x10]
 168:	bfxil	w8, w9, #0, #7
 16c:	sturb	w8, [x29, #-81]
 170:	ldur	q0, [x29, #-96]
 174:	str	q0, [sp, #192]
 178:	ldr	q0, [sp, #160]
 17c:	stur	q0, [x29, #-144]
 180:	str	q1, [sp, #16]
 184:	stur	q1, [x29, #-160]
 188:	ldurb	w8, [x29, #-129]
 18c:	ldurb	w9, [x29, #-145]
 190:	bfxil	w8, w9, #0, #7
 194:	sturb	w8, [x29, #-145]
 198:	ldur	q1, [x29, #-160]
 19c:	str	q1, [sp]
 1a0:	mov	v1.16b, v0.16b
 1a4:	bl	0 <__unordtf2>
 1a8:	cmp	w0, #0x0
 1ac:	b.eq	1b8 <__multc3+0x1b8>  // b.none
 1b0:	ldr	q0, [sp]
 1b4:	str	q0, [sp, #160]
 1b8:	ldr	q0, [sp, #176]
 1bc:	stur	q0, [x29, #-112]
 1c0:	ldr	q1, [sp, #16]
 1c4:	ldurb	w8, [x29, #-97]
 1c8:	stur	q1, [x29, #-128]
 1cc:	ldurb	w9, [x29, #-113]
 1d0:	bfxil	w8, w9, #0, #7
 1d4:	sturb	w8, [x29, #-113]
 1d8:	ldur	q1, [x29, #-128]
 1dc:	str	q1, [sp, #16]
 1e0:	mov	v1.16b, v0.16b
 1e4:	bl	0 <__unordtf2>
 1e8:	cmp	w0, #0x0
 1ec:	b.eq	1f8 <__multc3+0x1f8>  // b.none
 1f0:	ldr	q0, [sp, #16]
 1f4:	str	q0, [sp, #176]
 1f8:	mov	w20, #0x1                   	// #1
 1fc:	ldr	q0, [sp, #160]
 200:	ldr	q1, [sp, #128]
 204:	stur	q0, [x29, #-192]
 208:	ldurb	w8, [x29, #-177]
 20c:	and	w8, w8, #0x7f
 210:	sturb	w8, [x29, #-177]
 214:	ldur	q0, [x29, #-192]
 218:	str	q0, [sp, #16]
 21c:	ldr	q0, [sp, #176]
 220:	stur	q0, [x29, #-176]
 224:	ldurb	w8, [x29, #-161]
 228:	and	w8, w8, #0x7f
 22c:	sturb	w8, [x29, #-161]
 230:	ldur	q0, [x29, #-176]
 234:	str	q0, [sp]
 238:	bl	0 <__eqtf2>
 23c:	cbz	w0, 3fc <__multc3+0x3fc>
 240:	ldr	q0, [sp, #16]
 244:	ldr	q1, [sp, #128]
 248:	bl	0 <__netf2>
 24c:	cbz	w0, 3fc <__multc3+0x3fc>
 250:	cbnz	w20, 4ec <__multc3+0x4ec>
 254:	ldr	q0, [sp, #80]
 258:	stur	q0, [x29, #-256]
 25c:	ldurb	w8, [x29, #-241]
 260:	and	w8, w8, #0x7f
 264:	sturb	w8, [x29, #-241]
 268:	ldur	q0, [x29, #-256]
 26c:	str	q0, [sp, #16]
 270:	ldr	q0, [sp, #64]
 274:	stur	q0, [x29, #-240]
 278:	ldurb	w8, [x29, #-225]
 27c:	and	w8, w8, #0x7f
 280:	sturb	w8, [x29, #-225]
 284:	ldur	q0, [x29, #-240]
 288:	str	q0, [sp, #64]
 28c:	ldr	q0, [sp, #48]
 290:	stur	q0, [x29, #-224]
 294:	ldurb	w8, [x29, #-209]
 298:	and	w8, w8, #0x7f
 29c:	sturb	w8, [x29, #-209]
 2a0:	ldur	q0, [x29, #-224]
 2a4:	str	q0, [sp, #80]
 2a8:	ldr	q0, [sp, #32]
 2ac:	stur	q0, [x29, #-208]
 2b0:	ldurb	w8, [x29, #-193]
 2b4:	and	w8, w8, #0x7f
 2b8:	sturb	w8, [x29, #-193]
 2bc:	ldur	q0, [x29, #-208]
 2c0:	ldr	q1, [sp, #128]
 2c4:	bl	0 <__eqtf2>
 2c8:	cbz	w0, 300 <__multc3+0x300>
 2cc:	ldr	q0, [sp, #80]
 2d0:	ldr	q1, [sp, #128]
 2d4:	bl	0 <__eqtf2>
 2d8:	cbz	w0, 300 <__multc3+0x300>
 2dc:	ldr	q0, [sp, #16]
 2e0:	ldr	q1, [sp, #128]
 2e4:	bl	0 <__eqtf2>
 2e8:	cbz	w0, 300 <__multc3+0x300>
 2ec:	ldr	q0, [sp, #64]
 2f0:	ldr	q1, [sp, #128]
 2f4:	bl	0 <__netf2>
 2f8:	ldr	q1, [sp, #96]
 2fc:	cbnz	w0, 548 <__multc3+0x548>
 300:	adrp	x8, 0 <__multc3>
 304:	ldr	q1, [x8]
 308:	ldr	q0, [sp, #144]
 30c:	str	q1, [sp, #112]
 310:	stp	q1, q0, [sp, #336]
 314:	ldrb	w8, [sp, #367]
 318:	ldrb	w9, [sp, #351]
 31c:	bfxil	w8, w9, #0, #7
 320:	strb	w8, [sp, #351]
 324:	ldr	q1, [sp, #336]
 328:	str	q1, [sp, #96]
 32c:	mov	v1.16b, v0.16b
 330:	bl	0 <__unordtf2>
 334:	cmp	w0, #0x0
 338:	b.eq	344 <__multc3+0x344>  // b.none
 33c:	ldr	q0, [sp, #96]
 340:	str	q0, [sp, #144]
 344:	ldr	q0, [sp, #192]
 348:	ldr	q1, [sp, #112]
 34c:	stp	q1, q0, [sp, #368]
 350:	ldrb	w8, [sp, #399]
 354:	ldrb	w9, [sp, #383]
 358:	bfxil	w8, w9, #0, #7
 35c:	strb	w8, [sp, #383]
 360:	ldr	q1, [sp, #368]
 364:	str	q1, [sp, #96]
 368:	mov	v1.16b, v0.16b
 36c:	bl	0 <__unordtf2>
 370:	cmp	w0, #0x0
 374:	b.eq	380 <__multc3+0x380>  // b.none
 378:	ldr	q0, [sp, #96]
 37c:	str	q0, [sp, #192]
 380:	ldr	q0, [sp, #160]
 384:	ldr	q1, [sp, #112]
 388:	stp	q1, q0, [sp, #400]
 38c:	ldrb	w8, [sp, #431]
 390:	ldrb	w9, [sp, #415]
 394:	bfxil	w8, w9, #0, #7
 398:	strb	w8, [sp, #415]
 39c:	ldr	q1, [sp, #400]
 3a0:	str	q1, [sp, #96]
 3a4:	mov	v1.16b, v0.16b
 3a8:	bl	0 <__unordtf2>
 3ac:	cmp	w0, #0x0
 3b0:	b.eq	3bc <__multc3+0x3bc>  // b.none
 3b4:	ldr	q0, [sp, #96]
 3b8:	str	q0, [sp, #160]
 3bc:	ldr	q0, [sp, #176]
 3c0:	ldr	q1, [sp, #112]
 3c4:	stp	q1, q0, [sp, #432]
 3c8:	ldrb	w8, [sp, #463]
 3cc:	ldrb	w9, [sp, #447]
 3d0:	bfxil	w8, w9, #0, #7
 3d4:	strb	w8, [sp, #447]
 3d8:	ldr	q1, [sp, #432]
 3dc:	str	q1, [sp, #112]
 3e0:	mov	v1.16b, v0.16b
 3e4:	bl	0 <__unordtf2>
 3e8:	cmp	w0, #0x0
 3ec:	b.eq	4ec <__multc3+0x4ec>  // b.none
 3f0:	ldr	q0, [sp, #112]
 3f4:	str	q0, [sp, #176]
 3f8:	b	4ec <__multc3+0x4ec>
 3fc:	ldr	q0, [sp]
 400:	ldr	q1, [sp, #128]
 404:	bl	0 <__eqtf2>
 408:	ldr	q0, [sp, #16]
 40c:	ldr	q1, [sp, #128]
 410:	cmp	w0, #0x0
 414:	cset	w20, eq  // eq = none
 418:	bl	0 <__eqtf2>
 41c:	cmp	w0, #0x0
 420:	cset	w8, eq  // eq = none
 424:	ldr	q0, [x19, w8, uxtw #4]
 428:	ldr	q1, [sp, #160]
 42c:	adrp	x10, 0 <__multc3>
 430:	stp	q0, q1, [sp, #304]
 434:	ldrb	w8, [sp, #335]
 438:	ldrb	w9, [sp, #319]
 43c:	ldr	q0, [x19, w20, uxtw #4]
 440:	bfxil	w8, w9, #0, #7
 444:	strb	w8, [sp, #319]
 448:	ldr	q1, [sp, #304]
 44c:	str	q1, [sp, #160]
 450:	ldr	q1, [sp, #176]
 454:	stp	q0, q1, [sp, #272]
 458:	ldrb	w8, [sp, #303]
 45c:	ldrb	w9, [sp, #287]
 460:	ldr	q1, [x10]
 464:	bfxil	w8, w9, #0, #7
 468:	strb	w8, [sp, #287]
 46c:	ldr	q0, [sp, #272]
 470:	str	q1, [sp, #112]
 474:	str	q0, [sp, #176]
 478:	ldr	q0, [sp, #144]
 47c:	stp	q1, q0, [sp, #208]
 480:	ldrb	w8, [sp, #239]
 484:	ldrb	w9, [sp, #223]
 488:	bfxil	w8, w9, #0, #7
 48c:	strb	w8, [sp, #223]
 490:	ldr	q1, [sp, #208]
 494:	str	q1, [sp, #96]
 498:	mov	v1.16b, v0.16b
 49c:	bl	0 <__unordtf2>
 4a0:	cmp	w0, #0x0
 4a4:	b.eq	4b0 <__multc3+0x4b0>  // b.none
 4a8:	ldr	q0, [sp, #96]
 4ac:	str	q0, [sp, #144]
 4b0:	ldr	q0, [sp, #192]
 4b4:	ldr	q1, [sp, #112]
 4b8:	stp	q1, q0, [sp, #240]
 4bc:	ldrb	w8, [sp, #271]
 4c0:	ldrb	w9, [sp, #255]
 4c4:	bfxil	w8, w9, #0, #7
 4c8:	strb	w8, [sp, #255]
 4cc:	ldr	q1, [sp, #240]
 4d0:	str	q1, [sp, #112]
 4d4:	mov	v1.16b, v0.16b
 4d8:	bl	0 <__unordtf2>
 4dc:	cmp	w0, #0x0
 4e0:	b.eq	4ec <__multc3+0x4ec>  // b.none
 4e4:	ldr	q0, [sp, #112]
 4e8:	str	q0, [sp, #192]
 4ec:	ldp	q1, q0, [sp, #144]
 4f0:	bl	0 <__multf3>
 4f4:	str	q0, [sp, #112]
 4f8:	ldp	q0, q1, [sp, #176]
 4fc:	bl	0 <__multf3>
 500:	mov	v1.16b, v0.16b
 504:	ldr	q0, [sp, #112]
 508:	bl	0 <__subtf3>
 50c:	ldr	q1, [sp, #128]
 510:	bl	0 <__multf3>
 514:	str	q0, [sp, #112]
 518:	ldr	q0, [sp, #176]
 51c:	ldr	q1, [sp, #144]
 520:	bl	0 <__multf3>
 524:	str	q0, [sp, #176]
 528:	ldr	q0, [sp, #160]
 52c:	ldr	q1, [sp, #192]
 530:	bl	0 <__multf3>
 534:	ldr	q1, [sp, #176]
 538:	bl	0 <__addtf3>
 53c:	ldr	q1, [sp, #128]
 540:	bl	0 <__multf3>
 544:	mov	v1.16b, v0.16b
 548:	ldr	q0, [sp, #112]
 54c:	add	sp, sp, #0x2d0
 550:	ldp	x20, x19, [sp, #32]
 554:	ldr	x28, [sp, #16]
 558:	ldp	x29, x30, [sp], #48
 55c:	ret

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	str	q0, [sp, #-16]!
   4:	ldp	x9, x8, [sp]
   8:	mov	x11, #0xc3ff000000000000    	// #-4323737117252386816
   c:	mov	x12, #0xbc01000000000000    	// #-4899634919602388992
  10:	and	x10, x8, #0x7fffffffffffffff
  14:	add	x11, x10, x11
  18:	add	x12, x10, x12
  1c:	cmp	x11, x12
  20:	b.cs	4c <__trunctfdf2+0x4c>  // b.hs, b.nlast
  24:	mov	x11, #0x1                   	// #1
  28:	and	x10, x9, #0xfffffffffffffff
  2c:	movk	x11, #0x800, lsl #48
  30:	cmp	x10, x11
  34:	extr	x9, x8, x9, #60
  38:	b.cc	78 <__trunctfdf2+0x78>  // b.lo, b.ul, b.last
  3c:	mov	x10, #0x1                   	// #1
  40:	movk	x10, #0x4000, lsl #48
  44:	add	x9, x9, x10
  48:	b	170 <__trunctfdf2+0x170>
  4c:	cmp	x9, #0x0
  50:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
  54:	cset	w12, eq  // eq = none
  58:	cmp	x10, x11
  5c:	cset	w11, cc  // cc = lo, ul, last
  60:	csel	w11, w12, w11, eq  // eq = none
  64:	tbnz	w11, #0, 94 <__trunctfdf2+0x94>
  68:	extr	x10, x8, x9, #60
  6c:	mov	x9, #0x7ff8000000000000    	// #9221120237041090560
  70:	bfxil	x9, x10, #0, #51
  74:	b	170 <__trunctfdf2+0x170>
  78:	mov	x11, #0x4000000000000000    	// #4611686018427387904
  7c:	eor	x10, x10, #0x800000000000000
  80:	add	x9, x9, x11
  84:	cbnz	x10, 170 <__trunctfdf2+0x170>
  88:	and	x10, x9, #0x1
  8c:	add	x9, x10, x9
  90:	b	170 <__trunctfdf2+0x170>
  94:	mov	x11, #0x43feffffffffffff    	// #4899634919602388991
  98:	cmp	x10, x11
  9c:	b.ls	a8 <__trunctfdf2+0xa8>  // b.plast
  a0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  a4:	b	170 <__trunctfdf2+0x170>
  a8:	lsr	x10, x10, #48
  ac:	mov	w11, #0x3b91                	// #15249
  b0:	cmp	w10, w11
  b4:	b.cs	c0 <__trunctfdf2+0xc0>  // b.hs, b.nlast
  b8:	mov	x9, xzr
  bc:	b	170 <__trunctfdf2+0x170>
  c0:	mov	w12, #0x3c01                	// #15361
  c4:	mov	w13, #0xffffc47f            	// #-15233
  c8:	sub	w12, w12, w10
  cc:	add	w10, w10, w13
  d0:	mov	x11, #0x1000000000000       	// #281474976710656
  d4:	neg	x13, x10
  d8:	bfxil	x11, x8, #0, #48
  dc:	cmp	x10, #0x0
  e0:	sub	x15, x10, #0x40
  e4:	lsr	x13, x9, x13
  e8:	csel	x13, xzr, x13, eq  // eq = none
  ec:	cmp	x15, #0x0
  f0:	lsl	x15, x11, x10
  f4:	lsl	x14, x9, x10
  f8:	lsl	x10, x9, x10
  fc:	orr	x13, x13, x15
 100:	csel	x10, x10, x13, ge  // ge = tcont
 104:	csel	x14, xzr, x14, ge  // ge = tcont
 108:	orr	x10, x14, x10
 10c:	neg	x15, x12
 110:	cmp	x10, #0x0
 114:	lsl	x15, x11, x15
 118:	cset	w10, ne  // ne = any
 11c:	cmp	x12, #0x0
 120:	lsr	x13, x11, x12
 124:	sub	x14, x12, #0x40
 128:	lsr	x9, x9, x12
 12c:	lsr	x11, x11, x12
 130:	csel	x12, xzr, x15, eq  // eq = none
 134:	cmp	x14, #0x0
 138:	orr	x9, x9, x12
 13c:	csel	x9, x11, x9, ge  // ge = tcont
 140:	and	x11, x9, #0xfffffffffffffff
 144:	orr	x10, x11, x10
 148:	mov	x11, #0x1                   	// #1
 14c:	csel	x13, xzr, x13, ge  // ge = tcont
 150:	movk	x11, #0x800, lsl #48
 154:	cmp	x10, x11
 158:	extr	x9, x13, x9, #60
 15c:	b.cc	168 <__trunctfdf2+0x168>  // b.lo, b.ul, b.last
 160:	add	x9, x9, #0x1
 164:	b	170 <__trunctfdf2+0x170>
 168:	eor	x10, x10, #0x800000000000000
 16c:	cbz	x10, 88 <__trunctfdf2+0x88>
 170:	and	x8, x8, #0x8000000000000000
 174:	orr	x8, x9, x8
 178:	fmov	d0, x8
 17c:	add	sp, sp, #0x10
 180:	ret

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	str	q0, [sp, #-16]!
   4:	ldp	x9, x8, [sp]
   8:	mov	x11, #0xc07f000000000000    	// #-4575938696385134592
   c:	mov	x12, #0xbf81000000000000    	// #-4647433340469641216
  10:	and	x10, x8, #0x7fffffffffffffff
  14:	add	x11, x10, x11
  18:	add	x12, x10, x12
  1c:	cmp	x11, x12
  20:	b.cs	60 <__trunctfsf2+0x60>  // b.hs, b.nlast
  24:	cmp	x9, #0x0
  28:	ubfx	x11, x8, #24, #1
  2c:	and	x10, x8, #0x1ffffff
  30:	mov	w12, #0x1000000             	// #16777216
  34:	cset	w13, eq  // eq = none
  38:	cmp	x11, #0x0
  3c:	cset	w11, eq  // eq = none
  40:	cmp	x10, x12
  44:	csel	w12, w13, w11, eq  // eq = none
  48:	lsr	x11, x8, #25
  4c:	tbnz	w12, #0, 88 <__trunctfsf2+0x88>
  50:	mov	w9, #0x1                   	// #1
  54:	movk	w9, #0x4000, lsl #16
  58:	add	w9, w11, w9
  5c:	b	19c <__trunctfsf2+0x19c>
  60:	cmp	x9, #0x0
  64:	mov	x11, #0x7fff000000000000    	// #9223090561878065152
  68:	cset	w12, eq  // eq = none
  6c:	cmp	x10, x11
  70:	cset	w11, cc  // cc = lo, ul, last
  74:	csel	w11, w12, w11, eq  // eq = none
  78:	tbnz	w11, #0, a8 <__trunctfsf2+0xa8>
  7c:	ubfx	x9, x8, #25, #22
  80:	orr	w9, w9, #0x7fc00000
  84:	b	19c <__trunctfsf2+0x19c>
  88:	mov	w12, #0x40000000            	// #1073741824
  8c:	eor	x10, x10, #0x1000000
  90:	orr	x10, x9, x10
  94:	add	w9, w11, w12
  98:	cbnz	x10, 19c <__trunctfsf2+0x19c>
  9c:	and	w10, w9, #0x1
  a0:	add	w9, w10, w9
  a4:	b	19c <__trunctfsf2+0x19c>
  a8:	mov	x11, #0x407effffffffffff    	// #4647433340469641215
  ac:	cmp	x10, x11
  b0:	b.ls	bc <__trunctfsf2+0xbc>  // b.plast
  b4:	mov	w9, #0x7f800000            	// #2139095040
  b8:	b	19c <__trunctfsf2+0x19c>
  bc:	lsr	x10, x10, #48
  c0:	mov	w11, #0x3f11                	// #16145
  c4:	cmp	w10, w11
  c8:	b.cs	d4 <__trunctfsf2+0xd4>  // b.hs, b.nlast
  cc:	mov	w9, wzr
  d0:	b	19c <__trunctfsf2+0x19c>
  d4:	mov	w12, #0x3f81                	// #16257
  d8:	mov	w13, #0xffffc0ff            	// #-16129
  dc:	sub	w12, w12, w10
  e0:	add	w10, w10, w13
  e4:	mov	x11, #0x1000000000000       	// #281474976710656
  e8:	neg	x13, x10
  ec:	bfxil	x11, x8, #0, #48
  f0:	cmp	x10, #0x0
  f4:	sub	x15, x10, #0x40
  f8:	lsr	x13, x9, x13
  fc:	csel	x13, xzr, x13, eq  // eq = none
 100:	cmp	x15, #0x0
 104:	lsl	x15, x11, x10
 108:	orr	x13, x13, x15
 10c:	lsl	x15, x9, x10
 110:	lsl	x10, x9, x10
 114:	csel	x10, x10, x13, ge  // ge = tcont
 118:	csel	x15, xzr, x15, ge  // ge = tcont
 11c:	orr	x10, x15, x10
 120:	neg	x13, x12
 124:	cmp	x10, #0x0
 128:	lsl	x13, x11, x13
 12c:	cset	w16, ne  // ne = any
 130:	cmp	x12, #0x0
 134:	lsr	x9, x9, x12
 138:	lsr	x15, x11, x12
 13c:	lsr	x11, x11, x12
 140:	sub	x10, x12, #0x40
 144:	csel	x12, xzr, x13, eq  // eq = none
 148:	cmp	x10, #0x0
 14c:	orr	x9, x9, x12
 150:	csel	x9, x15, x9, ge  // ge = tcont
 154:	csel	x12, xzr, x11, ge  // ge = tcont
 158:	orr	x11, x9, x16
 15c:	ubfx	x13, x12, #24, #1
 160:	cmp	x11, #0x0
 164:	mov	w14, #0x1000000             	// #16777216
 168:	and	x10, x12, #0x1ffffff
 16c:	cset	w9, eq  // eq = none
 170:	cmp	x13, #0x0
 174:	cset	w13, eq  // eq = none
 178:	cmp	x10, x14
 17c:	csel	w13, w9, w13, eq  // eq = none
 180:	lsr	x9, x12, #25
 184:	tbnz	w13, #0, 190 <__trunctfsf2+0x190>
 188:	add	w9, w9, #0x1
 18c:	b	19c <__trunctfsf2+0x19c>
 190:	eor	x10, x10, #0x1000000
 194:	orr	x10, x11, x10
 198:	cbz	x10, 9c <__trunctfsf2+0x9c>
 19c:	lsr	x8, x8, #32
 1a0:	and	w8, w8, #0x80000000
 1a4:	orr	w8, w9, w8
 1a8:	fmov	s0, w8
 1ac:	add	sp, sp, #0x10
 1b0:	ret

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	18 <__absvdi2+0x18>  // b.none
   c:	cmp	x0, #0x0
  10:	cneg	x0, x0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvdi2>
  24:	adrp	x2, 0 <__absvdi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	18 <__absvsi2+0x18>  // b.none
   c:	cmp	w0, #0x0
  10:	cneg	w0, w0, mi  // mi = first
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__absvsi2>
  24:	adrp	x2, 0 <__absvsi2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x16                  	// #22
  34:	bl	0 <__compilerrt_abort_impl>

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 24 <__absvti2+0x24>
   c:	negs	x8, x0
  10:	ngcs	x9, x1
  14:	cmp	x1, #0x0
  18:	csel	x0, x8, x0, lt  // lt = tstop
  1c:	csel	x1, x9, x1, lt  // lt = tstop
  20:	ret
  24:	stp	x29, x30, [sp, #-16]!
  28:	mov	x29, sp
  2c:	adrp	x0, 0 <__absvti2>
  30:	adrp	x2, 0 <__absvti2>
  34:	add	x0, x0, #0x0
  38:	add	x2, x2, #0x0
  3c:	mov	w1, #0x18                  	// #24
  40:	bl	0 <__compilerrt_abort_impl>

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	fmov	x8, d0
  14:	and	x10, x8, #0x7fffffffffffffff
  18:	mov	x12, #0xfffffffffffffffe    	// #-2
  1c:	fmov	x9, d1
  20:	sub	x11, x10, #0x1
  24:	movk	x12, #0x7fef, lsl #48
  28:	cmp	x11, x12
  2c:	and	x11, x9, #0x7fffffffffffffff
  30:	b.hi	bc <__adddf3+0xbc>  // b.pmore
  34:	sub	x13, x11, #0x1
  38:	cmp	x13, x12
  3c:	b.hi	bc <__adddf3+0xbc>  // b.pmore
  40:	cmp	x11, x10
  44:	csel	x10, x8, x9, hi  // hi = pmore
  48:	csel	x9, x9, x8, hi  // hi = pmore
  4c:	ubfx	x8, x9, #52, #11
  50:	ubfx	x13, x10, #52, #11
  54:	and	x11, x9, #0xfffffffffffff
  58:	and	x12, x10, #0xfffffffffffff
  5c:	cbz	w8, ec <__adddf3+0xec>
  60:	cbz	w13, 108 <__adddf3+0x108>
  64:	lsl	x12, x12, #3
  68:	eor	x10, x9, x10
  6c:	lsl	x11, x11, #3
  70:	subs	w13, w8, w13
  74:	orr	x12, x12, #0x80000000000000
  78:	b.eq	9c <__adddf3+0x9c>  // b.none
  7c:	cmp	w13, #0x3f
  80:	b.hi	124 <__adddf3+0x124>  // b.pmore
  84:	neg	x14, x13
  88:	lsl	x14, x12, x14
  8c:	cmp	x14, #0x0
  90:	cset	w14, ne  // ne = any
  94:	lsr	x12, x12, x13
  98:	orr	x12, x12, x14
  9c:	orr	x11, x11, #0x80000000000000
  a0:	tbnz	x10, #63, 130 <__adddf3+0x130>
  a4:	add	x10, x12, x11
  a8:	tbz	x10, #56, 150 <__adddf3+0x150>
  ac:	and	x11, x10, #0x1
  b0:	orr	x10, x11, x10, lsr #1
  b4:	add	w8, w8, #0x1
  b8:	b	150 <__adddf3+0x150>
  bc:	mov	x12, #0x1                   	// #1
  c0:	movk	x12, #0x7ff0, lsl #48
  c4:	cmp	x10, x12
  c8:	b.cc	d8 <__adddf3+0xd8>  // b.lo, b.ul, b.last
  cc:	orr	x8, x8, #0x8000000000000
  d0:	fmov	d1, x8
  d4:	b	250 <__adddf3+0x250>
  d8:	cmp	x11, x12
  dc:	b.cc	1d8 <__adddf3+0x1d8>  // b.lo, b.ul, b.last
  e0:	orr	x8, x9, #0x8000000000000
  e4:	fmov	d1, x8
  e8:	b	250 <__adddf3+0x250>
  ec:	clz	x8, x11
  f0:	mov	w14, #0xfffffff5            	// #-11
  f4:	mov	w15, #0xc                   	// #12
  f8:	add	w14, w8, w14
  fc:	lsl	x11, x11, x14
 100:	sub	w8, w15, w8
 104:	cbnz	w13, 64 <__adddf3+0x64>
 108:	clz	x13, x12
 10c:	mov	w14, #0xfffffff5            	// #-11
 110:	mov	w15, #0xc                   	// #12
 114:	add	w14, w13, w14
 118:	lsl	x12, x12, x14
 11c:	sub	w13, w15, w13
 120:	b	64 <__adddf3+0x64>
 124:	mov	w12, #0x1                   	// #1
 128:	orr	x11, x11, #0x80000000000000
 12c:	tbz	x10, #63, a4 <__adddf3+0xa4>
 130:	subs	x10, x11, x12
 134:	b.eq	200 <__adddf3+0x200>  // b.none
 138:	lsr	x11, x10, #55
 13c:	cbnz	x11, 150 <__adddf3+0x150>
 140:	clz	x11, x10
 144:	sub	w11, w11, #0x8
 148:	lsl	x10, x10, x11
 14c:	sub	w8, w8, w11
 150:	cmp	w8, #0x7ff
 154:	and	x21, x9, #0x8000000000000000
 158:	b.lt	168 <__adddf3+0x168>  // b.tstop
 15c:	orr	x8, x21, #0x7ff0000000000000
 160:	fmov	d1, x8
 164:	b	250 <__adddf3+0x250>
 168:	cmp	w8, #0x0
 16c:	b.gt	198 <__adddf3+0x198>
 170:	mov	w9, #0x1                   	// #1
 174:	sub	w9, w9, w8
 178:	sxtw	x11, w9
 17c:	neg	x11, x11
 180:	lsl	x11, x10, x11
 184:	cmp	x11, #0x0
 188:	cset	w11, ne  // ne = any
 18c:	lsr	x9, x10, x9
 190:	mov	w8, wzr
 194:	orr	x10, x9, x11
 198:	ubfx	x9, x10, #3, #52
 19c:	orr	x9, x9, x21
 1a0:	and	w20, w10, #0x7
 1a4:	orr	x19, x9, x8, lsl #52
 1a8:	bl	0 <__fe_getround>
 1ac:	cmp	w0, #0x2
 1b0:	b.eq	220 <__adddf3+0x220>  // b.none
 1b4:	cmp	w0, #0x1
 1b8:	b.eq	22c <__adddf3+0x22c>  // b.none
 1bc:	cbnz	w0, 244 <__adddf3+0x244>
 1c0:	cmp	w20, #0x4
 1c4:	cinc	x19, x19, hi  // hi = pmore
 1c8:	b.ne	244 <__adddf3+0x244>  // b.any
 1cc:	and	x8, x19, #0x1
 1d0:	add	x19, x8, x19
 1d4:	b	248 <__adddf3+0x248>
 1d8:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
 1dc:	cmp	x10, x12
 1e0:	b.ne	208 <__adddf3+0x208>  // b.any
 1e4:	eor	x8, x9, x8
 1e8:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
 1ec:	mov	x10, #0x7ff8000000000000    	// #9221120237041090560
 1f0:	cmp	x8, x9
 1f4:	fmov	d1, x10
 1f8:	fcsel	d1, d1, d0, eq  // eq = none
 1fc:	b	250 <__adddf3+0x250>
 200:	fmov	d1, xzr
 204:	b	250 <__adddf3+0x250>
 208:	cmp	x11, x12
 20c:	b.eq	250 <__adddf3+0x250>  // b.none
 210:	cbz	x10, 264 <__adddf3+0x264>
 214:	mov	v1.16b, v0.16b
 218:	cbnz	x11, 40 <__adddf3+0x40>
 21c:	b	250 <__adddf3+0x250>
 220:	cmp	x21, #0x0
 224:	cset	w8, eq  // eq = none
 228:	b	234 <__adddf3+0x234>
 22c:	cmp	x21, #0x0
 230:	cset	w8, ne  // ne = any
 234:	cmp	w20, #0x0
 238:	cset	w9, ne  // ne = any
 23c:	and	w8, w8, w9
 240:	add	x19, x19, x8
 244:	cbz	w20, 24c <__adddf3+0x24c>
 248:	bl	0 <__fe_raise_inexact>
 24c:	fmov	d1, x19
 250:	ldp	x20, x19, [sp, #32]
 254:	ldr	x21, [sp, #16]
 258:	mov	v0.16b, v1.16b
 25c:	ldp	x29, x30, [sp], #48
 260:	ret
 264:	cbnz	x11, 250 <__adddf3+0x250>
 268:	and	x8, x9, x8
 26c:	fmov	d1, x8
 270:	b	250 <__adddf3+0x250>

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	fmov	w8, s0
  14:	and	w10, w8, #0x7fffffff
  18:	mov	w12, #0xfffe                	// #65534
  1c:	fmov	w9, s1
  20:	sub	w11, w10, #0x1
  24:	movk	w12, #0x7f7f, lsl #16
  28:	cmp	w11, w12
  2c:	and	w11, w9, #0x7fffffff
  30:	b.hi	f4 <__addsf3+0xf4>  // b.pmore
  34:	sub	w13, w11, #0x1
  38:	cmp	w13, w12
  3c:	b.hi	f4 <__addsf3+0xf4>  // b.pmore
  40:	cmp	w11, w10
  44:	csel	w13, w8, w9, hi  // hi = pmore
  48:	csel	w9, w9, w8, hi  // hi = pmore
  4c:	and	w15, w9, #0x7fffff
  50:	mov	w11, #0x9                   	// #9
  54:	mov	w12, #0xff                  	// #255
  58:	clz	w17, w15
  5c:	ubfx	w8, w9, #23, #8
  60:	and	w16, w13, #0x7fffff
  64:	tst	w12, w9, lsr #23
  68:	sub	w10, w11, w17
  6c:	sub	w17, w17, #0x8
  70:	csel	w8, w10, w8, eq  // eq = none
  74:	csel	w17, w17, wzr, eq  // eq = none
  78:	tst	w12, w13, lsr #23
  7c:	clz	w12, w16
  80:	ubfx	w14, w13, #23, #8
  84:	eor	w10, w9, w13
  88:	sub	w13, w12, #0x8
  8c:	csel	w13, w13, wzr, eq  // eq = none
  90:	sub	w11, w11, w12
  94:	lsl	w13, w16, w13
  98:	lsl	w12, w15, w17
  9c:	csel	w11, w11, w14, eq  // eq = none
  a0:	lsl	w14, w13, #3
  a4:	lsl	w12, w12, #3
  a8:	subs	w13, w8, w11
  ac:	orr	w11, w14, #0x4000000
  b0:	b.eq	d4 <__addsf3+0xd4>  // b.none
  b4:	cmp	w13, #0x1f
  b8:	b.hi	124 <__addsf3+0x124>  // b.pmore
  bc:	neg	w14, w13
  c0:	lsl	w14, w11, w14
  c4:	cmp	w14, #0x0
  c8:	cset	w14, ne  // ne = any
  cc:	lsr	w11, w11, w13
  d0:	orr	w11, w11, w14
  d4:	orr	w12, w12, #0x4000000
  d8:	tbnz	w10, #31, 130 <__addsf3+0x130>
  dc:	add	w10, w11, w12
  e0:	tbz	w10, #27, 150 <__addsf3+0x150>
  e4:	and	w11, w10, #0x1
  e8:	orr	w10, w11, w10, lsr #1
  ec:	add	w8, w8, #0x1
  f0:	b	150 <__addsf3+0x150>
  f4:	mov	w12, #0x1                   	// #1
  f8:	movk	w12, #0x7f80, lsl #16
  fc:	cmp	w10, w12
 100:	b.cc	110 <__addsf3+0x110>  // b.lo, b.ul, b.last
 104:	orr	w8, w8, #0x400000
 108:	fmov	s1, w8
 10c:	b	24c <__addsf3+0x24c>
 110:	cmp	w11, w12
 114:	b.cc	1d4 <__addsf3+0x1d4>  // b.lo, b.ul, b.last
 118:	orr	w8, w9, #0x400000
 11c:	fmov	s1, w8
 120:	b	24c <__addsf3+0x24c>
 124:	mov	w11, #0x1                   	// #1
 128:	orr	w12, w12, #0x4000000
 12c:	tbz	w10, #31, dc <__addsf3+0xdc>
 130:	subs	w10, w12, w11
 134:	b.eq	1fc <__addsf3+0x1fc>  // b.none
 138:	lsr	w11, w10, #26
 13c:	cbnz	w11, 150 <__addsf3+0x150>
 140:	clz	w11, w10
 144:	sub	w11, w11, #0x5
 148:	lsl	w10, w10, w11
 14c:	sub	w8, w8, w11
 150:	cmp	w8, #0xff
 154:	and	w21, w9, #0x80000000
 158:	b.lt	168 <__addsf3+0x168>  // b.tstop
 15c:	orr	w8, w21, #0x7f800000
 160:	fmov	s1, w8
 164:	b	24c <__addsf3+0x24c>
 168:	cmp	w8, #0x0
 16c:	b.gt	194 <__addsf3+0x194>
 170:	mov	w9, #0x1                   	// #1
 174:	add	w11, w8, #0x1f
 178:	sub	w8, w9, w8
 17c:	lsl	w9, w10, w11
 180:	cmp	w9, #0x0
 184:	cset	w9, ne  // ne = any
 188:	lsr	w8, w10, w8
 18c:	orr	w10, w8, w9
 190:	mov	w8, wzr
 194:	ubfx	w9, w10, #3, #23
 198:	orr	w8, w21, w8, lsl #23
 19c:	and	w20, w10, #0x7
 1a0:	orr	w19, w8, w9
 1a4:	bl	0 <__fe_getround>
 1a8:	cmp	w0, #0x2
 1ac:	b.eq	21c <__addsf3+0x21c>  // b.none
 1b0:	cmp	w0, #0x1
 1b4:	b.eq	228 <__addsf3+0x228>  // b.none
 1b8:	cbnz	w0, 240 <__addsf3+0x240>
 1bc:	cmp	w20, #0x4
 1c0:	cinc	w19, w19, hi  // hi = pmore
 1c4:	b.ne	240 <__addsf3+0x240>  // b.any
 1c8:	and	w8, w19, #0x1
 1cc:	add	w19, w8, w19
 1d0:	b	244 <__addsf3+0x244>
 1d4:	mov	w12, #0x7f800000            	// #2139095040
 1d8:	cmp	w10, w12
 1dc:	b.ne	204 <__addsf3+0x204>  // b.any
 1e0:	eor	w8, w9, w8
 1e4:	mov	w9, #0x80000000            	// #-2147483648
 1e8:	mov	w10, #0x7fc00000            	// #2143289344
 1ec:	cmp	w8, w9
 1f0:	fmov	s1, w10
 1f4:	fcsel	s1, s1, s0, eq  // eq = none
 1f8:	b	24c <__addsf3+0x24c>
 1fc:	fmov	s1, wzr
 200:	b	24c <__addsf3+0x24c>
 204:	cmp	w11, w12
 208:	b.eq	24c <__addsf3+0x24c>  // b.none
 20c:	cbz	w10, 260 <__addsf3+0x260>
 210:	mov	v1.16b, v0.16b
 214:	cbnz	w11, 40 <__addsf3+0x40>
 218:	b	24c <__addsf3+0x24c>
 21c:	cmp	w21, #0x0
 220:	cset	w8, eq  // eq = none
 224:	b	230 <__addsf3+0x230>
 228:	cmp	w21, #0x0
 22c:	cset	w8, ne  // ne = any
 230:	cmp	w20, #0x0
 234:	cset	w9, ne  // ne = any
 238:	and	w8, w8, w9
 23c:	add	w19, w19, w8
 240:	cbz	w20, 248 <__addsf3+0x248>
 244:	bl	0 <__fe_raise_inexact>
 248:	fmov	s1, w19
 24c:	ldp	x20, x19, [sp, #32]
 250:	ldr	x21, [sp, #16]
 254:	mov	v0.16b, v1.16b
 258:	ldp	x29, x30, [sp], #48
 25c:	ret
 260:	cbnz	w11, 24c <__addsf3+0x24c>
 264:	and	w8, w9, w8
 268:	fmov	s1, w8
 26c:	b	24c <__addsf3+0x24c>

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	sub	sp, sp, #0xa0
   4:	stp	x29, x30, [sp, #112]
   8:	stp	x22, x21, [sp, #128]
   c:	stp	x20, x19, [sp, #144]
  10:	add	x29, sp, #0x70
  14:	stp	q1, q0, [x29, #-32]
  18:	ldp	x10, x8, [x29, #-16]
  1c:	mov	x15, #0xffffffffffffffff    	// #-1
  20:	ldp	x13, x9, [x29, #-32]
  24:	mov	x14, #0x7ffeffffffffffff    	// #9223090561878065151
  28:	and	x12, x8, #0x7fffffffffffffff
  2c:	subs	x16, x10, #0x1
  30:	adcs	x17, x12, x15
  34:	cmn	x16, #0x1
  38:	cset	w16, eq  // eq = none
  3c:	cmp	x17, x14
  40:	cset	w17, hi  // hi = pmore
  44:	and	x11, x9, #0x7fffffffffffffff
  48:	csel	w17, w16, w17, eq  // eq = none
  4c:	subs	x16, x13, #0x1
  50:	adcs	x15, x11, x15
  54:	tbnz	w17, #0, 174 <__addtf3+0x174>
  58:	cmn	x16, #0x1
  5c:	cset	w16, ne  // ne = any
  60:	cmp	x15, x14
  64:	cset	w14, cc  // cc = lo, ul, last
  68:	csel	w14, w16, w14, eq  // eq = none
  6c:	tbz	w14, #0, 174 <__addtf3+0x174>
  70:	cmp	x13, x10
  74:	cset	w14, hi  // hi = pmore
  78:	cmp	x11, x12
  7c:	cset	w11, hi  // hi = pmore
  80:	csel	w11, w14, w11, eq  // eq = none
  84:	cmp	w11, #0x0
  88:	csel	x11, x8, x9, ne  // ne = any
  8c:	csel	x8, x9, x8, ne  // ne = any
  90:	csel	x12, x10, x13, ne  // ne = any
  94:	csel	x10, x13, x10, ne  // ne = any
  98:	ubfx	x9, x8, #48, #15
  9c:	ubfx	x14, x11, #48, #15
  a0:	and	x13, x8, #0xffffffffffff
  a4:	cbz	w9, 1c8 <__addtf3+0x1c8>
  a8:	and	x15, x11, #0xffffffffffff
  ac:	cbz	w14, 224 <__addtf3+0x224>
  b0:	extr	x16, x15, x12, #61
  b4:	eor	x11, x8, x11
  b8:	extr	x13, x13, x10, #61
  bc:	lsl	x15, x12, #3
  c0:	subs	w14, w9, w14
  c4:	orr	x12, x16, #0x8000000000000
  c8:	b.eq	144 <__addtf3+0x144>  // b.none
  cc:	cmp	w14, #0x7f
  d0:	b.hi	27c <__addtf3+0x27c>  // b.pmore
  d4:	mov	w16, #0x80                  	// #128
  d8:	subs	x16, x16, x14
  dc:	neg	x16, x16
  e0:	neg	x17, x14
  e4:	mov	w18, #0x40                  	// #64
  e8:	lsr	x16, x15, x16
  ec:	lsr	x0, x15, x14
  f0:	sub	x18, x18, x14
  f4:	csel	x16, xzr, x16, eq  // eq = none
  f8:	lsl	x15, x15, x17
  fc:	lsl	x17, x12, x17
 100:	cmp	x18, #0x0
 104:	orr	x16, x16, x17
 108:	csel	x16, x15, x16, ge  // ge = tcont
 10c:	csel	x15, xzr, x15, ge  // ge = tcont
 110:	orr	x15, x15, x16
 114:	cmp	x15, #0x0
 118:	cset	w15, ne  // ne = any
 11c:	cmp	x14, #0x0
 120:	lsr	x18, x12, x14
 124:	sub	x16, x14, #0x40
 128:	lsr	x12, x12, x14
 12c:	csel	x14, xzr, x17, eq  // eq = none
 130:	cmp	x16, #0x0
 134:	orr	x14, x0, x14
 138:	csel	x14, x18, x14, ge  // ge = tcont
 13c:	csel	x12, xzr, x12, ge  // ge = tcont
 140:	orr	x15, x14, x15
 144:	lsl	x14, x10, #3
 148:	orr	x10, x13, #0x8000000000000
 14c:	tbnz	x11, #63, 290 <__addtf3+0x290>
 150:	adds	x11, x15, x14
 154:	adcs	x10, x12, x10
 158:	tbz	x10, #52, 2f8 <__addtf3+0x2f8>
 15c:	and	x12, x11, #0x1
 160:	extr	x11, x10, x11, #1
 164:	lsr	x10, x10, #1
 168:	orr	x11, x11, x12
 16c:	add	w9, w9, #0x1
 170:	b	2f8 <__addtf3+0x2f8>
 174:	cmp	x10, #0x0
 178:	mov	x14, #0x7fff000000000000    	// #9223090561878065152
 17c:	cset	w15, eq  // eq = none
 180:	cmp	x12, x14
 184:	cset	w16, cc  // cc = lo, ul, last
 188:	csel	w15, w15, w16, eq  // eq = none
 18c:	tbnz	w15, #0, 1a0 <__addtf3+0x1a0>
 190:	orr	x8, x8, #0x800000000000
 194:	stp	x10, x8, [sp]
 198:	ldr	q1, [sp]
 19c:	b	49c <__addtf3+0x49c>
 1a0:	cmp	x13, #0x0
 1a4:	cset	w15, eq  // eq = none
 1a8:	cmp	x11, x14
 1ac:	cset	w14, cc  // cc = lo, ul, last
 1b0:	csel	w14, w15, w14, eq  // eq = none
 1b4:	tbnz	w14, #0, 400 <__addtf3+0x400>
 1b8:	orr	x8, x9, #0x800000000000
 1bc:	stp	x13, x8, [sp, #16]
 1c0:	ldr	q1, [sp, #16]
 1c4:	b	49c <__addtf3+0x49c>
 1c8:	cmp	x13, #0x0
 1cc:	csel	x16, x10, x13, eq  // eq = none
 1d0:	cset	w15, eq  // eq = none
 1d4:	clz	x16, x16
 1d8:	add	w15, w16, w15, lsl #6
 1dc:	sub	w16, w15, #0xf
 1e0:	neg	x17, x16
 1e4:	cmp	x16, #0x0
 1e8:	lsl	x18, x10, x16
 1ec:	lsl	x13, x13, x16
 1f0:	lsr	x17, x10, x17
 1f4:	lsl	x10, x10, x16
 1f8:	sub	x16, x16, #0x40
 1fc:	csel	x17, xzr, x17, eq  // eq = none
 200:	cmp	x16, #0x0
 204:	mov	w9, #0x10                  	// #16
 208:	csel	x16, xzr, x18, ge  // ge = tcont
 20c:	orr	x13, x17, x13
 210:	csel	x13, x10, x13, ge  // ge = tcont
 214:	sub	w9, w9, w15
 218:	mov	x10, x16
 21c:	and	x15, x11, #0xffffffffffff
 220:	cbnz	w14, b0 <__addtf3+0xb0>
 224:	cmp	x15, #0x0
 228:	csel	x17, x12, x15, eq  // eq = none
 22c:	cset	w16, eq  // eq = none
 230:	clz	x17, x17
 234:	add	w16, w17, w16, lsl #6
 238:	sub	w17, w16, #0xf
 23c:	neg	x18, x17
 240:	cmp	x17, #0x0
 244:	lsl	x0, x12, x17
 248:	lsl	x15, x15, x17
 24c:	lsr	x18, x12, x18
 250:	lsl	x12, x12, x17
 254:	sub	x17, x17, #0x40
 258:	csel	x18, xzr, x18, eq  // eq = none
 25c:	cmp	x17, #0x0
 260:	mov	w14, #0x10                  	// #16
 264:	csel	x17, xzr, x0, ge  // ge = tcont
 268:	orr	x15, x18, x15
 26c:	csel	x15, x12, x15, ge  // ge = tcont
 270:	sub	w14, w14, w16
 274:	mov	x12, x17
 278:	b	b0 <__addtf3+0xb0>
 27c:	mov	x12, xzr
 280:	mov	w15, #0x1                   	// #1
 284:	lsl	x14, x10, #3
 288:	orr	x10, x13, #0x8000000000000
 28c:	tbz	x11, #63, 150 <__addtf3+0x150>
 290:	subs	x11, x14, x15
 294:	sbcs	x10, x10, x12
 298:	orr	x12, x11, x10
 29c:	cbz	x12, 434 <__addtf3+0x434>
 2a0:	lsr	x12, x10, #51
 2a4:	cbnz	x12, 2f8 <__addtf3+0x2f8>
 2a8:	cmp	x10, #0x0
 2ac:	csel	x13, x11, x10, eq  // eq = none
 2b0:	cset	w12, eq  // eq = none
 2b4:	clz	x13, x13
 2b8:	add	w12, w13, w12, lsl #6
 2bc:	sub	w12, w12, #0xc
 2c0:	neg	x13, x12
 2c4:	cmp	x12, #0x0
 2c8:	sub	x15, x12, #0x40
 2cc:	lsr	x13, x11, x13
 2d0:	lsl	x14, x11, x12
 2d4:	lsl	x10, x10, x12
 2d8:	csel	x13, xzr, x13, eq  // eq = none
 2dc:	cmp	x15, #0x0
 2e0:	lsl	x11, x11, x12
 2e4:	csel	x14, xzr, x14, ge  // ge = tcont
 2e8:	orr	x10, x13, x10
 2ec:	csel	x10, x11, x10, ge  // ge = tcont
 2f0:	sub	w9, w9, w12
 2f4:	mov	x11, x14
 2f8:	mov	w12, #0x7fff                	// #32767
 2fc:	cmp	w9, w12
 300:	and	x21, x8, #0x8000000000000000
 304:	b.lt	318 <__addtf3+0x318>  // b.tstop
 308:	orr	x8, x21, #0x7fff000000000000
 30c:	stp	xzr, x8, [sp, #48]
 310:	ldr	q1, [sp, #48]
 314:	b	49c <__addtf3+0x49c>
 318:	cmp	w9, #0x0
 31c:	b.gt	3ac <__addtf3+0x3ac>
 320:	mov	w8, #0x1                   	// #1
 324:	sub	w8, w8, w9
 328:	mov	w12, #0x80                  	// #128
 32c:	sxtw	x9, w8
 330:	subs	x12, x12, x9
 334:	mov	w13, #0x40                  	// #64
 338:	neg	x12, x12
 33c:	sub	x13, x13, x9
 340:	neg	x9, x9
 344:	lsr	x12, x11, x12
 348:	csel	x12, xzr, x12, eq  // eq = none
 34c:	cmp	x13, #0x0
 350:	lsl	x13, x10, x9
 354:	orr	x12, x12, x13
 358:	lsl	x9, x11, x9
 35c:	csel	x12, x9, x12, ge  // ge = tcont
 360:	csel	x9, xzr, x9, ge  // ge = tcont
 364:	orr	x9, x9, x12
 368:	neg	x14, x8
 36c:	cmp	x9, #0x0
 370:	lsl	x14, x10, x14
 374:	cset	w9, ne  // ne = any
 378:	cmp	x8, #0x0
 37c:	lsr	x15, x10, x8
 380:	sub	x13, x8, #0x40
 384:	lsr	x11, x11, x8
 388:	lsr	x10, x10, x8
 38c:	csel	x8, xzr, x14, eq  // eq = none
 390:	cmp	x13, #0x0
 394:	orr	x8, x11, x8
 398:	csel	x12, xzr, x15, ge  // ge = tcont
 39c:	csel	x8, x10, x8, ge  // ge = tcont
 3a0:	orr	x11, x8, x9
 3a4:	mov	x10, x12
 3a8:	mov	w9, wzr
 3ac:	ubfx	x8, x10, #3, #48
 3b0:	orr	x8, x8, x21
 3b4:	and	w22, w11, #0x7
 3b8:	extr	x19, x10, x11, #3
 3bc:	orr	x20, x8, x9, lsl #48
 3c0:	bl	0 <__fe_getround>
 3c4:	cmp	w0, #0x2
 3c8:	b.eq	464 <__addtf3+0x464>  // b.none
 3cc:	cmp	w0, #0x1
 3d0:	b.eq	470 <__addtf3+0x470>  // b.none
 3d4:	cbnz	w0, 48c <__addtf3+0x48c>
 3d8:	cmp	w22, #0x4
 3dc:	cset	w8, hi  // hi = pmore
 3e0:	adds	x19, x19, x8
 3e4:	adcs	x20, x20, xzr
 3e8:	cmp	w22, #0x4
 3ec:	b.ne	48c <__addtf3+0x48c>  // b.any
 3f0:	and	x8, x19, #0x1
 3f4:	adds	x19, x8, x19
 3f8:	adcs	x20, x20, xzr
 3fc:	b	490 <__addtf3+0x490>
 400:	eor	x14, x12, #0x7fff000000000000
 404:	orr	x14, x10, x14
 408:	cbnz	x14, 440 <__addtf3+0x440>
 40c:	eor	x8, x9, x8
 410:	eor	x10, x13, x10
 414:	eor	x8, x8, #0x8000000000000000
 418:	orr	x8, x10, x8
 41c:	cmp	x8, #0x0
 420:	b.ne	42c <__addtf3+0x42c>  // b.any
 424:	adrp	x8, 0 <__addtf3>
 428:	ldr	q0, [x8]
 42c:	mov	v1.16b, v0.16b
 430:	b	49c <__addtf3+0x49c>
 434:	adrp	x8, 0 <__addtf3>
 438:	ldr	q1, [x8]
 43c:	b	49c <__addtf3+0x49c>
 440:	eor	x14, x11, #0x7fff000000000000
 444:	orr	x14, x13, x14
 448:	cbz	x14, 49c <__addtf3+0x49c>
 44c:	orr	x14, x10, x12
 450:	cbz	x14, 4b4 <__addtf3+0x4b4>
 454:	orr	x14, x13, x11
 458:	mov	v1.16b, v0.16b
 45c:	cbnz	x14, 70 <__addtf3+0x70>
 460:	b	49c <__addtf3+0x49c>
 464:	cmp	x21, #0x0
 468:	cset	w8, eq  // eq = none
 46c:	b	478 <__addtf3+0x478>
 470:	cmp	x21, #0x0
 474:	cset	w8, ne  // ne = any
 478:	cmp	w22, #0x0
 47c:	cset	w9, ne  // ne = any
 480:	and	w8, w8, w9
 484:	adds	x19, x19, x8
 488:	adcs	x20, x20, xzr
 48c:	cbz	w22, 494 <__addtf3+0x494>
 490:	bl	0 <__fe_raise_inexact>
 494:	stp	x19, x20, [x29, #-48]
 498:	ldur	q1, [x29, #-48]
 49c:	ldp	x20, x19, [sp, #144]
 4a0:	ldp	x22, x21, [sp, #128]
 4a4:	ldp	x29, x30, [sp, #112]
 4a8:	mov	v0.16b, v1.16b
 4ac:	add	sp, sp, #0xa0
 4b0:	ret
 4b4:	orr	x11, x13, x11
 4b8:	cbnz	x11, 49c <__addtf3+0x49c>
 4bc:	and	x10, x13, x10
 4c0:	and	x8, x9, x8
 4c4:	stp	x10, x8, [sp, #32]
 4c8:	ldr	q1, [sp, #32]
 4cc:	b	49c <__addtf3+0x49c>

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	add	x0, x1, x0
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__addvdi3+0x34>
  18:	b.ge	38 <__addvdi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvdi3>
  20:	adrp	x2, 0 <__addvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvdi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvdi3>
  44:	adrp	x2, 0 <__addvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	add	w0, w1, w0
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__addvsi3+0x34>
  18:	b.ge	38 <__addvsi3+0x38>  // b.tcont
  1c:	adrp	x0, 0 <__addvsi3>
  20:	adrp	x2, 0 <__addvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.ge	40 <__addvsi3+0x40>  // b.tcont
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__addvsi3>
  44:	adrp	x2, 0 <__addvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	adds	x0, x2, x0
  10:	mov	x8, x1
  14:	adcs	x1, x3, x1
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__addvti3+0x4c>
  20:	cset	w9, cs  // cs = hs, nlast
  24:	cmp	x1, x8
  28:	cset	w8, ge  // ge = tcont
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__addvti3+0x60>
  34:	adrp	x0, 0 <__addvti3>
  38:	adrp	x2, 0 <__addvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, cc  // cc = lo, ul, last
  50:	cmp	x1, x8
  54:	cset	w8, lt  // lt = tstop
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__addvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__addvti3>
  6c:	adrp	x2, 0 <__addvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	tbnz	w1, #5, 24 <__ashldi3+0x24>
   4:	cbz	w1, 30 <__ashldi3+0x30>
   8:	lsr	x8, x0, #32
   c:	neg	w9, w1
  10:	lsr	w9, w0, w9
  14:	lsl	w8, w8, w1
  18:	lsl	w0, w0, w1
  1c:	orr	w8, w8, w9
  20:	b	2c <__ashldi3+0x2c>
  24:	lsl	w8, w0, w1
  28:	mov	x0, xzr
  2c:	bfi	x0, x8, #32, #32
  30:	ret

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbnz	w2, #6, 24 <__ashlti3+0x24>
   4:	cbz	w2, 34 <__ashlti3+0x34>
   8:	neg	w9, w2
   c:	lsr	x9, x0, x9
  10:	lsl	x10, x1, x2
  14:	mov	x8, xzr
  18:	lsl	x0, x0, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__ashlti3+0x30>
  24:	mov	x8, xzr
  28:	lsl	x9, x0, x2
  2c:	mov	x0, xzr
  30:	orr	x1, x9, x8
  34:	ret

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	lsr	x9, x0, #32
   4:	tbnz	w1, #5, 24 <__ashrdi3+0x24>
   8:	cbz	w1, 34 <__ashrdi3+0x34>
   c:	neg	w10, w1
  10:	asr	w8, w9, w1
  14:	lsl	w9, w9, w10
  18:	lsr	w10, w0, w1
  1c:	orr	w9, w9, w10
  20:	b	2c <__ashrdi3+0x2c>
  24:	asr	w8, w9, #31
  28:	asr	w9, w9, w1
  2c:	mov	w0, w9
  30:	bfi	x0, x8, #32, #32
  34:	ret

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	mov	x8, x1
   4:	tbnz	w2, #6, 24 <__ashrti3+0x24>
   8:	cbz	w2, 30 <__ashrti3+0x30>
   c:	neg	w9, w2
  10:	asr	x1, x8, x2
  14:	lsl	x8, x8, x9
  18:	lsr	x9, x0, x2
  1c:	orr	x0, x8, x9
  20:	ret
  24:	asr	x1, x8, #63
  28:	asr	x0, x8, x2
  2c:	ret
  30:	mov	x1, x8
  34:	ret

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w8, #0x0
   8:	csel	w8, w0, w8, eq  // eq = none
   c:	cset	w9, eq  // eq = none
  10:	clz	w8, w8
  14:	add	w0, w8, w9, lsl #5
  18:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	cmp	w0, #0x10, lsl #12
   4:	cset	w11, cc  // cc = lo, ul, last
   8:	mov	w8, #0x10                  	// #16
   c:	lsl	w11, w11, #4
  10:	sub	w8, w8, w11
  14:	lsr	w8, w0, w8
  18:	tst	w8, #0xff00
  1c:	cset	w12, eq  // eq = none
  20:	mov	w9, #0x8                   	// #8
  24:	bfi	w11, w12, #3, #1
  28:	lsl	w12, w12, #3
  2c:	sub	w9, w9, w12
  30:	lsr	w8, w8, w9
  34:	tst	w8, #0xf0
  38:	cset	w9, eq  // eq = none
  3c:	mov	w10, #0x4                   	// #4
  40:	bfi	w11, w9, #2, #1
  44:	lsl	w9, w9, #2
  48:	sub	w9, w10, w9
  4c:	lsr	w8, w8, w9
  50:	tst	w8, #0xc
  54:	cset	w9, eq  // eq = none
  58:	mov	w12, #0x2                   	// #2
  5c:	bfi	w11, w9, #1, #1
  60:	lsl	w9, w9, #1
  64:	sub	w9, w12, w9
  68:	mov	w10, #0x1                   	// #1
  6c:	lsr	w8, w8, w9
  70:	sub	w9, w12, w8
  74:	bic	w8, w10, w8, lsr #1
  78:	neg	w8, w8
  7c:	and	w8, w9, w8
  80:	add	w0, w8, w11
  84:	ret

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csel	x9, x0, x1, eq  // eq = none
   8:	cset	w8, eq  // eq = none
   c:	clz	x9, x9
  10:	add	w0, w9, w8, lsl #6
  14:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.ge	18 <__cmpdi2+0x18>  // b.tcont
  10:	mov	w0, wzr
  14:	ret
  18:	b.le	24 <__cmpdi2+0x24>
  1c:	mov	w0, #0x2                   	// #2
  20:	ret
  24:	cmp	w0, w1
  28:	b.cs	34 <__cmpdi2+0x34>  // b.hs, b.nlast
  2c:	mov	w0, wzr
  30:	ret
  34:	mov	w8, #0x1                   	// #1
  38:	cinc	w0, w8, hi  // hi = pmore
  3c:	ret

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	cmp	x1, x3
   4:	b.ge	10 <__cmpti2+0x10>  // b.tcont
   8:	mov	w0, wzr
   c:	ret
  10:	b.le	1c <__cmpti2+0x1c>
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	x0, x2
  20:	b.cs	2c <__cmpti2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	fmov	x8, d0
   4:	and	x10, x8, #0x7fffffffffffffff
   8:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
   c:	cmp	x10, x12
  10:	mov	w0, #0x1                   	// #1
  14:	b.hi	64 <__cmpdf2+0x64>  // b.pmore
  18:	fmov	x9, d1
  1c:	and	x11, x9, #0x7fffffffffffffff
  20:	cmp	x11, x12
  24:	b.hi	64 <__cmpdf2+0x64>  // b.pmore
  28:	orr	x10, x11, x10
  2c:	cbz	x10, 48 <__cmpdf2+0x48>
  30:	tst	x9, x8
  34:	b.lt	50 <__cmpdf2+0x50>  // b.tstop
  38:	cmp	x8, x9
  3c:	b.ge	60 <__cmpdf2+0x60>  // b.tcont
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ret
  48:	mov	w0, wzr
  4c:	ret
  50:	cmp	x8, x9
  54:	b.le	60 <__cmpdf2+0x60>
  58:	mov	w0, #0xffffffff            	// #-1
  5c:	ret
  60:	cset	w0, ne  // ne = any
  64:	ret

0000000000000068 <__gedf2>:
  68:	fmov	x8, d0
  6c:	and	x10, x8, #0x7fffffffffffffff
  70:	mov	x12, #0x7ff0000000000000    	// #9218868437227405312
  74:	cmp	x10, x12
  78:	mov	w0, #0xffffffff            	// #-1
  7c:	b.hi	cc <__gedf2+0x64>  // b.pmore
  80:	fmov	x9, d1
  84:	and	x11, x9, #0x7fffffffffffffff
  88:	cmp	x11, x12
  8c:	b.hi	cc <__gedf2+0x64>  // b.pmore
  90:	orr	x10, x11, x10
  94:	cbz	x10, b0 <__gedf2+0x48>
  98:	tst	x9, x8
  9c:	b.lt	b8 <__gedf2+0x50>  // b.tstop
  a0:	cmp	x8, x9
  a4:	b.ge	c8 <__gedf2+0x60>  // b.tcont
  a8:	mov	w0, #0xffffffff            	// #-1
  ac:	ret
  b0:	mov	w0, wzr
  b4:	ret
  b8:	cmp	x8, x9
  bc:	b.le	c8 <__gedf2+0x60>
  c0:	mov	w0, #0xffffffff            	// #-1
  c4:	ret
  c8:	cset	w0, ne  // ne = any
  cc:	ret

00000000000000d0 <__unorddf2>:
  d0:	fmov	x8, d0
  d4:	and	x8, x8, #0x7fffffffffffffff
  d8:	fmov	x9, d1
  dc:	mov	x10, #0x7ff0000000000000    	// #9218868437227405312
  e0:	and	x9, x9, #0x7fffffffffffffff
  e4:	cmp	x8, x10
  e8:	cset	w8, hi  // hi = pmore
  ec:	cmp	x9, x10
  f0:	cset	w9, hi  // hi = pmore
  f4:	orr	w0, w8, w9
  f8:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	fmov	w8, s0
   4:	and	w10, w8, #0x7fffffff
   8:	mov	w12, #0x7f800000            	// #2139095040
   c:	cmp	w10, w12
  10:	mov	w0, #0x1                   	// #1
  14:	b.hi	64 <__cmpsf2+0x64>  // b.pmore
  18:	fmov	w9, s1
  1c:	and	w11, w9, #0x7fffffff
  20:	cmp	w11, w12
  24:	b.hi	64 <__cmpsf2+0x64>  // b.pmore
  28:	orr	w10, w11, w10
  2c:	cbz	w10, 48 <__cmpsf2+0x48>
  30:	tst	w9, w8
  34:	b.lt	50 <__cmpsf2+0x50>  // b.tstop
  38:	cmp	w8, w9
  3c:	b.ge	60 <__cmpsf2+0x60>  // b.tcont
  40:	mov	w0, #0xffffffff            	// #-1
  44:	ret
  48:	mov	w0, wzr
  4c:	ret
  50:	cmp	w8, w9
  54:	b.le	60 <__cmpsf2+0x60>
  58:	mov	w0, #0xffffffff            	// #-1
  5c:	ret
  60:	cset	w0, ne  // ne = any
  64:	ret

0000000000000068 <__gesf2>:
  68:	fmov	w8, s0
  6c:	and	w10, w8, #0x7fffffff
  70:	mov	w12, #0x7f800000            	// #2139095040
  74:	cmp	w10, w12
  78:	mov	w0, #0xffffffff            	// #-1
  7c:	b.hi	cc <__gesf2+0x64>  // b.pmore
  80:	fmov	w9, s1
  84:	and	w11, w9, #0x7fffffff
  88:	cmp	w11, w12
  8c:	b.hi	cc <__gesf2+0x64>  // b.pmore
  90:	orr	w10, w11, w10
  94:	cbz	w10, b0 <__gesf2+0x48>
  98:	tst	w9, w8
  9c:	b.lt	b8 <__gesf2+0x50>  // b.tstop
  a0:	cmp	w8, w9
  a4:	b.ge	c8 <__gesf2+0x60>  // b.tcont
  a8:	mov	w0, #0xffffffff            	// #-1
  ac:	ret
  b0:	mov	w0, wzr
  b4:	ret
  b8:	cmp	w8, w9
  bc:	b.le	c8 <__gesf2+0x60>
  c0:	mov	w0, #0xffffffff            	// #-1
  c4:	ret
  c8:	cset	w0, ne  // ne = any
  cc:	ret

00000000000000d0 <__unordsf2>:
  d0:	fmov	w8, s0
  d4:	and	w8, w8, #0x7fffffff
  d8:	fmov	w9, s1
  dc:	mov	w10, #0x7f800000            	// #2139095040
  e0:	and	w9, w9, #0x7fffffff
  e4:	cmp	w8, w10
  e8:	cset	w8, hi  // hi = pmore
  ec:	cmp	w9, w10
  f0:	cset	w9, hi  // hi = pmore
  f4:	orr	w0, w8, w9
  f8:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	lsr	x8, x0, #32
   4:	cmp	w0, #0x0
   8:	csel	w8, w8, w0, eq  // eq = none
   c:	rbit	w8, w8
  10:	cset	w9, eq  // eq = none
  14:	clz	w8, w8
  18:	add	w0, w8, w9, lsl #5
  1c:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	cset	w9, eq  // eq = none
   8:	lsl	w9, w9, #4
   c:	lsr	w10, w0, w9
  10:	tst	w10, #0xff
  14:	cset	w11, eq  // eq = none
  18:	bfi	w9, w11, #3, #1
  1c:	lsl	w11, w11, #3
  20:	lsr	w10, w10, w11
  24:	tst	w10, #0xf
  28:	cset	w11, eq  // eq = none
  2c:	bfi	w9, w11, #2, #1
  30:	lsl	w11, w11, #2
  34:	lsr	w10, w10, w11
  38:	tst	w10, #0x3
  3c:	cset	w11, eq  // eq = none
  40:	bfi	w9, w11, #1, #1
  44:	lsl	w11, w11, #1
  48:	lsr	w10, w10, w11
  4c:	mov	w8, #0x2                   	// #2
  50:	mvn	w11, w10
  54:	ubfx	w10, w10, #1, #1
  58:	sub	w8, w8, w10
  5c:	and	w10, w11, #0x1
  60:	neg	w10, w10
  64:	and	w8, w8, w10
  68:	add	w0, w8, w9
  6c:	ret

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csel	x9, x1, x0, eq  // eq = none
   8:	rbit	x9, x9
   c:	cset	w8, eq  // eq = none
  10:	clz	x9, x9
  14:	add	w0, w9, w8, lsl #6
  18:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	str	q1, [sp, #32]
  1c:	fabs	d1, d2
  20:	fabs	d4, d3
  24:	fmaxnm	d1, d1, d4
  28:	fmov	x8, d1
  2c:	ubfx	x9, x8, #52, #11
  30:	cmp	w9, #0x7ff
  34:	stur	q0, [x29, #-16]
  38:	b.ne	50 <__divdc3+0x50>  // b.any
  3c:	cmn	x8, #0x1
  40:	fccmp	d1, d1, #0x1, le
  44:	fneg	d0, d1
  48:	fcsel	d9, d1, d0, vs
  4c:	b	90 <__divdc3+0x90>
  50:	fcmp	d1, #0.0
  54:	b.ne	64 <__divdc3+0x64>  // b.any
  58:	mov	x8, #0xfff0000000000000    	// #-4503599627370496
  5c:	fmov	d9, x8
  60:	b	90 <__divdc3+0x90>
  64:	cbz	w9, 70 <__divdc3+0x70>
  68:	sub	w8, w9, #0x3ff
  6c:	b	8c <__divdc3+0x8c>
  70:	and	x8, x8, #0x7fffffffffffffff
  74:	clz	x9, x8
  78:	sub	w10, w9, #0xb
  7c:	lsl	x8, x8, x10
  80:	ubfx	x8, x8, #52, #11
  84:	sub	w8, w8, w9
  88:	sub	w8, w8, #0x3f4
  8c:	scvtf	d9, w8
  90:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  94:	fabs	d10, d9
  98:	fmov	d0, x8
  9c:	fcmp	d10, d0
  a0:	mov	w19, wzr
  a4:	b.eq	e0 <__divdc3+0xe0>  // b.none
  a8:	b.vs	e0 <__divdc3+0xe0>
  ac:	b	b0 <__divdc3+0xb0>
  b0:	fcvtzs	w8, d9
  b4:	neg	w19, w8
  b8:	mov	v0.16b, v2.16b
  bc:	mov	w0, w19
  c0:	str	q3, [sp]
  c4:	bl	0 <scalbn>
  c8:	str	q0, [sp, #16]
  cc:	ldr	q0, [sp]
  d0:	mov	w0, w19
  d4:	bl	0 <scalbn>
  d8:	ldr	q2, [sp, #16]
  dc:	mov	v3.16b, v0.16b
  e0:	ldur	q4, [x29, #-16]
  e4:	ldr	q5, [sp, #32]
  e8:	fmul	d0, d2, d2
  ec:	fmul	d1, d3, d3
  f0:	fmul	d4, d2, d4
  f4:	fmul	d5, d3, d5
  f8:	fadd	d11, d0, d1
  fc:	fadd	d0, d4, d5
 100:	fdiv	d0, d0, d11
 104:	mov	w0, w19
 108:	stp	q3, q2, [sp]
 10c:	bl	0 <scalbn>
 110:	mov	v8.16b, v0.16b
 114:	ldp	q1, q0, [sp, #16]
 118:	ldr	q2, [sp]
 11c:	mov	w0, w19
 120:	fmul	d0, d1, d0
 124:	ldur	q1, [x29, #-16]
 128:	fmul	d1, d2, d1
 12c:	fsub	d0, d0, d1
 130:	fdiv	d0, d0, d11
 134:	bl	0 <scalbn>
 138:	fcmp	d8, d8
 13c:	mov	v1.16b, v0.16b
 140:	b.vc	2f8 <__divdc3+0x2f8>
 144:	fcmp	d1, d1
 148:	b.vc	2f8 <__divdc3+0x2f8>
 14c:	fcmp	d11, #0.0
 150:	b.ne	16c <__divdc3+0x16c>  // b.any
 154:	ldur	q0, [x29, #-16]
 158:	fcmp	d0, d0
 15c:	b.vc	2d0 <__divdc3+0x2d0>
 160:	ldr	q0, [sp, #32]
 164:	fcmp	d0, d0
 168:	b.vc	2d0 <__divdc3+0x2d0>
 16c:	ldur	q0, [x29, #-16]
 170:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 174:	ldr	q7, [sp]
 178:	fabs	d3, d0
 17c:	ldp	q6, q0, [sp, #16]
 180:	fabs	d4, d0
 184:	fmov	d0, x8
 188:	fcmp	d3, d0
 18c:	fabs	d2, d6
 190:	cset	w8, eq  // eq = none
 194:	fcmp	d4, d0
 198:	cset	w9, eq  // eq = none
 19c:	fcmp	d2, d0
 1a0:	fabs	d0, d7
 1a4:	b.eq	22c <__divdc3+0x22c>  // b.none
 1a8:	b.vs	22c <__divdc3+0x22c>
 1ac:	b	1b0 <__divdc3+0x1b0>
 1b0:	orr	w8, w8, w9
 1b4:	cbz	w8, 22c <__divdc3+0x22c>
 1b8:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1bc:	fmov	d5, x8
 1c0:	fcmp	d0, d5
 1c4:	b.eq	22c <__divdc3+0x22c>  // b.none
 1c8:	b.vs	22c <__divdc3+0x22c>
 1cc:	b	1d0 <__divdc3+0x1d0>
 1d0:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 1d4:	fmov	d5, x8
 1d8:	fmov	d0, xzr
 1dc:	fmov	d1, #1.000000000000000000e+00
 1e0:	fcmp	d3, d5
 1e4:	fcsel	d3, d1, d0, eq  // eq = none
 1e8:	fcmp	d4, d5
 1ec:	ldur	q4, [x29, #-16]
 1f0:	fcsel	d0, d1, d0, eq  // eq = none
 1f4:	ldr	q1, [sp, #32]
 1f8:	movi	v2.2d, #0x0
 1fc:	fneg	v2.2d, v2.2d
 200:	bit	v3.16b, v4.16b, v2.16b
 204:	bit	v0.16b, v1.16b, v2.16b
 208:	fmul	d1, d3, d6
 20c:	fmul	d2, d3, d7
 210:	fmul	d3, d0, d7
 214:	fmul	d0, d0, d6
 218:	fadd	d1, d1, d3
 21c:	fsub	d0, d0, d2
 220:	fmul	d8, d1, d5
 224:	fmul	d1, d0, d5
 228:	b	2f8 <__divdc3+0x2f8>
 22c:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 230:	fmov	d5, x8
 234:	fcmp	d4, d5
 238:	b.eq	2f8 <__divdc3+0x2f8>  // b.none
 23c:	b.vs	2f8 <__divdc3+0x2f8>
 240:	b	244 <__divdc3+0x244>
 244:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 248:	fmov	d4, x8
 24c:	fcmp	d3, d4
 250:	b.eq	2f8 <__divdc3+0x2f8>  // b.none
 254:	b.vs	2f8 <__divdc3+0x2f8>
 258:	b	25c <__divdc3+0x25c>
 25c:	fcmp	d9, #0.0
 260:	b.le	2f8 <__divdc3+0x2f8>
 264:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 268:	fmov	d3, x8
 26c:	fcmp	d10, d3
 270:	b.ne	2f8 <__divdc3+0x2f8>  // b.any
 274:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 278:	fmov	d5, x8
 27c:	fmov	d1, xzr
 280:	fmov	d3, #1.000000000000000000e+00
 284:	movi	v4.2d, #0x0
 288:	fcmp	d2, d5
 28c:	fneg	v2.2d, v4.2d
 290:	fcsel	d4, d3, d1, eq  // eq = none
 294:	fcmp	d0, d5
 298:	bit	v4.16b, v6.16b, v2.16b
 29c:	ldur	q6, [x29, #-16]
 2a0:	ldr	q5, [sp, #32]
 2a4:	fcsel	d0, d3, d1, eq  // eq = none
 2a8:	bit	v0.16b, v7.16b, v2.16b
 2ac:	fmul	d2, d4, d6
 2b0:	fmul	d3, d4, d5
 2b4:	fmul	d4, d0, d5
 2b8:	fmul	d0, d0, d6
 2bc:	fadd	d2, d2, d4
 2c0:	fsub	d0, d3, d0
 2c4:	fmul	d8, d2, d1
 2c8:	fmul	d1, d0, d1
 2cc:	b	2f8 <__divdc3+0x2f8>
 2d0:	ldr	q2, [sp, #16]
 2d4:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 2d8:	movi	v0.2d, #0x0
 2dc:	fmov	d1, x8
 2e0:	fneg	v0.2d, v0.2d
 2e4:	bit	v1.16b, v2.16b, v0.16b
 2e8:	ldur	q0, [x29, #-16]
 2ec:	fmul	d8, d1, d0
 2f0:	ldr	q0, [sp, #32]
 2f4:	fmul	d1, d1, d0
 2f8:	mov	v0.16b, v8.16b
 2fc:	ldr	x19, [sp, #112]
 300:	ldp	x29, x30, [sp, #96]
 304:	ldp	d9, d8, [sp, #80]
 308:	ldp	d11, d10, [sp, #64]
 30c:	add	sp, sp, #0x80
 310:	ret

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	fmov	x13, d0
   4:	ubfx	x12, x13, #52, #11
   8:	fmov	x11, d1
   c:	eor	x8, x11, x13
  10:	sub	w10, w12, #0x1
  14:	ubfx	x14, x11, #52, #11
  18:	and	x9, x13, #0xfffffffffffff
  1c:	and	x8, x8, #0x8000000000000000
  20:	cmp	w10, #0x7fd
  24:	and	x10, x11, #0xfffffffffffff
  28:	b.hi	128 <__divdf3+0x128>  // b.pmore
  2c:	sub	w15, w14, #0x1
  30:	cmp	w15, #0x7fe
  34:	b.cs	128 <__divdf3+0x128>  // b.hs, b.nlast
  38:	mov	w11, wzr
  3c:	orr	x13, x10, #0x10000000000000
  40:	sub	w12, w12, w14
  44:	mov	w14, #0xf333                	// #62259
  48:	movk	w14, #0x7504, lsl #16
  4c:	lsr	x16, x13, #21
  50:	ubfx	x17, x13, #21, #32
  54:	sub	w14, w14, w16
  58:	mul	x16, x14, x17
  5c:	lsr	x16, x16, #32
  60:	neg	w16, w16
  64:	umull	x14, w16, w14
  68:	ubfx	x14, x14, #31, #32
  6c:	mul	x16, x14, x17
  70:	lsr	x16, x16, #32
  74:	neg	w16, w16
  78:	mul	x14, x16, x14
  7c:	ubfx	x14, x14, #31, #32
  80:	mul	x16, x14, x17
  84:	lsr	x16, x16, #32
  88:	neg	w16, w16
  8c:	mul	x14, x16, x14
  90:	lsr	x14, x14, #31
  94:	lsl	w10, w10, #11
  98:	sub	w14, w14, #0x1
  9c:	mul	x17, x14, x17
  a0:	umull	x10, w14, w10
  a4:	add	x10, x17, x10, lsr #32
  a8:	neg	x10, x10
  ac:	lsr	x17, x10, #32
  b0:	and	x10, x10, #0xffffffff
  b4:	mul	x17, x17, x14
  b8:	mul	x10, x10, x14
  bc:	add	x10, x17, x10, lsr #32
  c0:	lsr	x16, x9, #30
  c4:	sub	x10, x10, #0x2
  c8:	lsl	w15, w9, #2
  cc:	and	x16, x16, #0xffffffff
  d0:	and	x14, x10, #0xffffffff
  d4:	lsr	x10, x10, #32
  d8:	orr	x16, x16, #0x400000
  dc:	mul	x17, x14, x15
  e0:	mul	x15, x10, x15
  e4:	mul	x14, x14, x16
  e8:	mul	x10, x10, x16
  ec:	and	x16, x15, #0xfffffffc
  f0:	add	x16, x16, x17, lsr #32
  f4:	add	x10, x10, x14, lsr #32
  f8:	add	x14, x16, w14, uxtw
  fc:	add	x10, x10, x15, lsr #32
 100:	add	x10, x10, x14, lsr #32
 104:	lsr	x14, x10, #53
 108:	add	w11, w11, w12
 10c:	cbnz	x14, 148 <__divdf3+0x148>
 110:	lsl	x9, x9, #53
 114:	msub	x9, x10, x13, x9
 118:	sub	w11, w11, #0x1
 11c:	cmp	w11, #0x400
 120:	b.ge	15c <__divdf3+0x15c>  // b.tcont
 124:	b	168 <__divdf3+0x168>
 128:	mov	x16, #0x1                   	// #1
 12c:	and	x15, x13, #0x7fffffffffffffff
 130:	movk	x16, #0x7ff0, lsl #48
 134:	cmp	x15, x16
 138:	b.cc	180 <__divdf3+0x180>  // b.lo, b.ul, b.last
 13c:	orr	x8, x13, #0x8000000000000
 140:	fmov	d0, x8
 144:	ret
 148:	lsr	x10, x10, #1
 14c:	lsl	x9, x9, #52
 150:	msub	x9, x10, x13, x9
 154:	cmp	w11, #0x400
 158:	b.lt	168 <__divdf3+0x168>  // b.tstop
 15c:	orr	x8, x8, #0x7ff0000000000000
 160:	fmov	d0, x8
 164:	ret
 168:	cmn	w11, #0x3ff
 16c:	add	w11, w11, #0x3ff
 170:	b.gt	198 <__divdf3+0x198>
 174:	cbz	w11, 204 <__divdf3+0x204>
 178:	fmov	d0, x8
 17c:	ret
 180:	and	x13, x11, #0x7fffffffffffffff
 184:	cmp	x13, x16
 188:	b.cc	1b0 <__divdf3+0x1b0>  // b.lo, b.ul, b.last
 18c:	orr	x8, x11, #0x8000000000000
 190:	fmov	d0, x8
 194:	ret
 198:	cmp	x13, x9, lsl #1
 19c:	bfi	x10, x11, #52, #12
 1a0:	cinc	x9, x10, cc  // cc = lo, ul, last
 1a4:	orr	x8, x9, x8
 1a8:	fmov	d0, x8
 1ac:	ret
 1b0:	mov	x11, #0x7ff0000000000000    	// #9218868437227405312
 1b4:	cmp	x15, x11
 1b8:	b.ne	1d0 <__divdf3+0x1d0>  // b.any
 1bc:	cmp	x13, x11
 1c0:	b.ne	15c <__divdf3+0x15c>  // b.any
 1c4:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 1c8:	fmov	d0, x8
 1cc:	ret
 1d0:	cmp	x13, x11
 1d4:	b.eq	178 <__divdf3+0x178>  // b.none
 1d8:	cbz	x15, 218 <__divdf3+0x218>
 1dc:	cbz	x13, 15c <__divdf3+0x15c>
 1e0:	lsr	x11, x15, #52
 1e4:	cbnz	x11, 230 <__divdf3+0x230>
 1e8:	clz	x11, x9
 1ec:	mov	w15, #0xfffffff5            	// #-11
 1f0:	mov	w16, #0xc                   	// #12
 1f4:	add	w15, w11, w15
 1f8:	lsl	x9, x9, x15
 1fc:	sub	w11, w16, w11
 200:	b	234 <__divdf3+0x234>
 204:	cmp	x13, x9, lsl #1
 208:	and	x9, x10, #0xfffffffffffff
 20c:	cinc	x9, x9, cc  // cc = lo, ul, last
 210:	tbz	x9, #52, 178 <__divdf3+0x178>
 214:	b	1a4 <__divdf3+0x1a4>
 218:	fmov	d0, x8
 21c:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 220:	cmp	x13, #0x0
 224:	fmov	d1, x8
 228:	fcsel	d0, d0, d1, ne  // ne = any
 22c:	ret
 230:	mov	w11, wzr
 234:	lsr	x13, x13, #52
 238:	cbnz	x13, 3c <__divdf3+0x3c>
 23c:	clz	x13, x10
 240:	mov	w15, #0xfffffff5            	// #-11
 244:	add	w15, w13, w15
 248:	add	w11, w13, w11
 24c:	lsl	x10, x10, x15
 250:	sub	w11, w11, #0xc
 254:	b	3c <__divdf3+0x3c>

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x8, x0, #63
  10:	asr	x9, x1, #63
  14:	eor	x10, x8, x0
  18:	eor	x11, x9, x1
  1c:	sub	x0, x10, x8
  20:	sub	x1, x11, x9
  24:	mov	x2, xzr
  28:	eor	x19, x9, x8
  2c:	bl	0 <__udivmoddi4>
  30:	eor	x8, x0, x19
  34:	sub	x0, x8, x19
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	x20, x1
  18:	mov	x21, x0
  1c:	bl	0 <__divdi3>
  20:	msub	x8, x0, x20, x21
  24:	str	x8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__divsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	sub	sp, sp, #0x80
   4:	stp	d11, d10, [sp, #64]
   8:	stp	d9, d8, [sp, #80]
   c:	stp	x29, x30, [sp, #96]
  10:	str	x19, [sp, #112]
  14:	add	x29, sp, #0x40
  18:	str	d1, [sp, #32]
  1c:	fabs	s1, s2
  20:	fabs	s4, s3
  24:	fmaxnm	s1, s1, s4
  28:	fmov	w8, s1
  2c:	ubfx	w9, w8, #23, #8
  30:	cmp	w9, #0xff
  34:	stur	d0, [x29, #-16]
  38:	b.ne	50 <__divsc3+0x50>  // b.any
  3c:	cmn	w8, #0x1
  40:	fccmp	s1, s1, #0x1, le
  44:	fneg	s0, s1
  48:	fcsel	s9, s1, s0, vs
  4c:	b	90 <__divsc3+0x90>
  50:	fcmp	s1, #0.0
  54:	b.ne	64 <__divsc3+0x64>  // b.any
  58:	mov	w8, #0xff800000            	// #-8388608
  5c:	fmov	s9, w8
  60:	b	90 <__divsc3+0x90>
  64:	cbz	w9, 70 <__divsc3+0x70>
  68:	sub	w8, w9, #0x7f
  6c:	b	8c <__divsc3+0x8c>
  70:	and	w8, w8, #0x7fffffff
  74:	clz	w9, w8
  78:	sub	w10, w9, #0x8
  7c:	lsl	w8, w8, w10
  80:	ubfx	w8, w8, #23, #8
  84:	sub	w8, w8, w9
  88:	sub	w8, w8, #0x77
  8c:	scvtf	s9, w8
  90:	mov	w8, #0x7f800000            	// #2139095040
  94:	fabs	s10, s9
  98:	fmov	s0, w8
  9c:	fcmp	s10, s0
  a0:	mov	w19, wzr
  a4:	b.eq	e0 <__divsc3+0xe0>  // b.none
  a8:	b.vs	e0 <__divsc3+0xe0>
  ac:	b	b0 <__divsc3+0xb0>
  b0:	fcvtzs	w8, s9
  b4:	neg	w19, w8
  b8:	mov	v0.16b, v2.16b
  bc:	mov	w0, w19
  c0:	str	q3, [sp]
  c4:	bl	0 <scalbnf>
  c8:	str	d0, [sp, #16]
  cc:	ldr	q0, [sp]
  d0:	mov	w0, w19
  d4:	bl	0 <scalbnf>
  d8:	ldr	q2, [sp, #16]
  dc:	mov	v3.16b, v0.16b
  e0:	ldur	q4, [x29, #-16]
  e4:	ldr	q5, [sp, #32]
  e8:	fmul	s0, s2, s2
  ec:	fmul	s1, s3, s3
  f0:	fmul	s4, s2, s4
  f4:	fmul	s5, s3, s5
  f8:	fadd	s11, s0, s1
  fc:	fadd	s0, s4, s5
 100:	fdiv	s0, s0, s11
 104:	mov	w0, w19
 108:	stp	q3, q2, [sp]
 10c:	bl	0 <scalbnf>
 110:	mov	v8.16b, v0.16b
 114:	ldp	q1, q0, [sp, #16]
 118:	ldr	q2, [sp]
 11c:	mov	w0, w19
 120:	fmul	s0, s1, s0
 124:	ldur	q1, [x29, #-16]
 128:	fmul	s1, s2, s1
 12c:	fsub	s0, s0, s1
 130:	fdiv	s0, s0, s11
 134:	bl	0 <scalbnf>
 138:	fcmp	s8, s8
 13c:	mov	v1.16b, v0.16b
 140:	b.vc	2ec <__divsc3+0x2ec>
 144:	fcmp	s1, s1
 148:	b.vc	2ec <__divsc3+0x2ec>
 14c:	fcmp	s11, #0.0
 150:	b.ne	16c <__divsc3+0x16c>  // b.any
 154:	ldur	q0, [x29, #-16]
 158:	fcmp	s0, s0
 15c:	b.vc	2c8 <__divsc3+0x2c8>
 160:	ldr	q0, [sp, #32]
 164:	fcmp	s0, s0
 168:	b.vc	2c8 <__divsc3+0x2c8>
 16c:	ldur	q0, [x29, #-16]
 170:	mov	w8, #0x7f800000            	// #2139095040
 174:	ldr	q7, [sp]
 178:	fmov	s2, w8
 17c:	fabs	s3, s0
 180:	ldp	q6, q0, [sp, #16]
 184:	fcmp	s3, s2
 188:	cset	w8, eq  // eq = none
 18c:	fabs	s4, s0
 190:	fabs	s0, s6
 194:	fcmp	s4, s2
 198:	cset	w9, eq  // eq = none
 19c:	fcmp	s0, s2
 1a0:	fabs	s2, s7
 1a4:	b.eq	228 <__divsc3+0x228>  // b.none
 1a8:	b.vs	228 <__divsc3+0x228>
 1ac:	b	1b0 <__divsc3+0x1b0>
 1b0:	orr	w8, w8, w9
 1b4:	cbz	w8, 228 <__divsc3+0x228>
 1b8:	mov	w8, #0x7f800000            	// #2139095040
 1bc:	fmov	s5, w8
 1c0:	fcmp	s2, s5
 1c4:	b.eq	228 <__divsc3+0x228>  // b.none
 1c8:	b.vs	228 <__divsc3+0x228>
 1cc:	b	1d0 <__divsc3+0x1d0>
 1d0:	mov	w8, #0x7f800000            	// #2139095040
 1d4:	fmov	s5, w8
 1d8:	fmov	s0, wzr
 1dc:	fmov	s1, #1.000000000000000000e+00
 1e0:	fcmp	s3, s5
 1e4:	fcsel	s3, s1, s0, eq  // eq = none
 1e8:	fcmp	s4, s5
 1ec:	ldur	q4, [x29, #-16]
 1f0:	fcsel	s0, s1, s0, eq  // eq = none
 1f4:	ldr	q1, [sp, #32]
 1f8:	movi	v2.4s, #0x80, lsl #24
 1fc:	bit	v3.16b, v4.16b, v2.16b
 200:	bit	v0.16b, v1.16b, v2.16b
 204:	fmul	s1, s3, s6
 208:	fmul	s2, s3, s7
 20c:	fmul	s3, s0, s7
 210:	fmul	s0, s0, s6
 214:	fadd	s1, s1, s3
 218:	fsub	s0, s0, s2
 21c:	fmul	s8, s1, s5
 220:	fmul	s1, s0, s5
 224:	b	2ec <__divsc3+0x2ec>
 228:	mov	w8, #0x7f800000            	// #2139095040
 22c:	fmov	s5, w8
 230:	fcmp	s4, s5
 234:	b.eq	2ec <__divsc3+0x2ec>  // b.none
 238:	b.vs	2ec <__divsc3+0x2ec>
 23c:	b	240 <__divsc3+0x240>
 240:	mov	w8, #0x7f800000            	// #2139095040
 244:	fmov	s4, w8
 248:	fcmp	s3, s4
 24c:	b.eq	2ec <__divsc3+0x2ec>  // b.none
 250:	b.vs	2ec <__divsc3+0x2ec>
 254:	b	258 <__divsc3+0x258>
 258:	fcmp	s9, #0.0
 25c:	b.le	2ec <__divsc3+0x2ec>
 260:	mov	w8, #0x7f800000            	// #2139095040
 264:	fmov	s3, w8
 268:	fcmp	s10, s3
 26c:	b.ne	2ec <__divsc3+0x2ec>  // b.any
 270:	mov	w8, #0x7f800000            	// #2139095040
 274:	fmov	s5, w8
 278:	fmov	s1, wzr
 27c:	fmov	s3, #1.000000000000000000e+00
 280:	fcmp	s0, s5
 284:	fcsel	s0, s3, s1, eq  // eq = none
 288:	fcmp	s2, s5
 28c:	movi	v4.4s, #0x80, lsl #24
 290:	fcsel	s2, s3, s1, eq  // eq = none
 294:	bit	v0.16b, v6.16b, v4.16b
 298:	bit	v2.16b, v7.16b, v4.16b
 29c:	ldur	q5, [x29, #-16]
 2a0:	ldr	q4, [sp, #32]
 2a4:	fmul	s3, s0, s5
 2a8:	fmul	s0, s0, s4
 2ac:	fmul	s4, s2, s4
 2b0:	fmul	s2, s2, s5
 2b4:	fadd	s3, s3, s4
 2b8:	fsub	s0, s0, s2
 2bc:	fmul	s8, s3, s1
 2c0:	fmul	s1, s0, s1
 2c4:	b	2ec <__divsc3+0x2ec>
 2c8:	ldr	q2, [sp, #16]
 2cc:	mov	w8, #0x7f800000            	// #2139095040
 2d0:	movi	v0.4s, #0x80, lsl #24
 2d4:	fmov	s1, w8
 2d8:	bit	v1.16b, v2.16b, v0.16b
 2dc:	ldur	q0, [x29, #-16]
 2e0:	fmul	s8, s1, s0
 2e4:	ldr	q0, [sp, #32]
 2e8:	fmul	s1, s1, s0
 2ec:	mov	v0.16b, v8.16b
 2f0:	ldr	x19, [sp, #112]
 2f4:	ldp	x29, x30, [sp, #96]
 2f8:	ldp	d9, d8, [sp, #80]
 2fc:	ldp	d11, d10, [sp, #64]
 300:	add	sp, sp, #0x80
 304:	ret

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	fmov	w13, s0
   4:	ubfx	w10, w13, #23, #8
   8:	fmov	w15, s1
   c:	eor	w8, w15, w13
  10:	sub	w12, w10, #0x1
  14:	ubfx	w11, w15, #23, #8
  18:	and	w9, w13, #0x7fffff
  1c:	and	w8, w8, #0x80000000
  20:	cmp	w12, #0xfd
  24:	and	w12, w15, #0x7fffff
  28:	b.hi	c8 <__divsf3+0xc8>  // b.pmore
  2c:	sub	w14, w11, #0x1
  30:	cmp	w14, #0xfe
  34:	b.cs	c8 <__divsf3+0xc8>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	orr	w12, w12, #0x800000
  40:	sub	w11, w10, w11
  44:	mov	w10, #0xf333                	// #62259
  48:	movk	w10, #0x7504, lsl #16
  4c:	lsl	w14, w12, #8
  50:	sub	w10, w10, w14
  54:	umull	x15, w10, w14
  58:	lsr	x15, x15, #32
  5c:	neg	w15, w15
  60:	umull	x10, w15, w10
  64:	ubfx	x10, x10, #31, #32
  68:	mul	x15, x10, x14
  6c:	lsr	x15, x15, #32
  70:	neg	w15, w15
  74:	mul	x10, x15, x10
  78:	ubfx	x10, x10, #31, #32
  7c:	mul	x14, x10, x14
  80:	lsr	x14, x14, #32
  84:	neg	w14, w14
  88:	mul	x10, x14, x10
  8c:	lsl	w14, w9, #1
  90:	lsr	x10, x10, #31
  94:	sub	w10, w10, #0x2
  98:	orr	w14, w14, #0x1000000
  9c:	umull	x10, w10, w14
  a0:	lsr	x14, x10, #56
  a4:	add	w11, w13, w11
  a8:	cbnz	w14, e8 <__divsf3+0xe8>
  ac:	lsr	x10, x10, #32
  b0:	lsl	w9, w9, #24
  b4:	msub	w9, w12, w10, w9
  b8:	sub	w11, w11, #0x1
  bc:	cmp	w11, #0x80
  c0:	b.ge	fc <__divsf3+0xfc>  // b.tcont
  c4:	b	108 <__divsf3+0x108>
  c8:	mov	w16, #0x1                   	// #1
  cc:	and	w14, w13, #0x7fffffff
  d0:	movk	w16, #0x7f80, lsl #16
  d4:	cmp	w14, w16
  d8:	b.cc	120 <__divsf3+0x120>  // b.lo, b.ul, b.last
  dc:	orr	w8, w13, #0x400000
  e0:	fmov	s0, w8
  e4:	ret
  e8:	lsr	x10, x10, #33
  ec:	lsl	w9, w9, #23
  f0:	msub	w9, w12, w10, w9
  f4:	cmp	w11, #0x80
  f8:	b.lt	108 <__divsf3+0x108>  // b.tstop
  fc:	orr	w8, w8, #0x7f800000
 100:	fmov	s0, w8
 104:	ret
 108:	cmn	w11, #0x7f
 10c:	add	w11, w11, #0x7f
 110:	b.gt	138 <__divsf3+0x138>
 114:	cbz	w11, 1c0 <__divsf3+0x1c0>
 118:	fmov	s0, w8
 11c:	ret
 120:	and	w13, w15, #0x7fffffff
 124:	cmp	w13, w16
 128:	b.cc	150 <__divsf3+0x150>  // b.lo, b.ul, b.last
 12c:	orr	w8, w15, #0x400000
 130:	fmov	s0, w8
 134:	ret
 138:	cmp	w12, w9, lsl #1
 13c:	bfi	w10, w11, #23, #9
 140:	cinc	w9, w10, cc  // cc = lo, ul, last
 144:	orr	w8, w9, w8
 148:	fmov	s0, w8
 14c:	ret
 150:	mov	w15, #0x7f800000            	// #2139095040
 154:	cmp	w14, w15
 158:	b.ne	170 <__divsf3+0x170>  // b.any
 15c:	cmp	w13, w15
 160:	b.ne	fc <__divsf3+0xfc>  // b.any
 164:	mov	w8, #0x7fc00000            	// #2143289344
 168:	fmov	s0, w8
 16c:	ret
 170:	cmp	w13, w15
 174:	b.eq	118 <__divsf3+0x118>  // b.none
 178:	cbz	w14, 1d4 <__divsf3+0x1d4>
 17c:	cbz	w13, fc <__divsf3+0xfc>
 180:	clz	w15, w9
 184:	mov	w16, #0x9                   	// #9
 188:	cmp	w14, #0x800, lsl #12
 18c:	lsr	w14, w13, #23
 190:	sub	w13, w15, #0x8
 194:	sub	w15, w16, w15
 198:	csel	w13, w13, wzr, cc  // cc = lo, ul, last
 19c:	lsl	w9, w9, w13
 1a0:	csel	w13, w15, wzr, cc  // cc = lo, ul, last
 1a4:	cbnz	w14, 3c <__divsf3+0x3c>
 1a8:	clz	w14, w12
 1ac:	sub	w15, w14, #0x8
 1b0:	add	w13, w14, w13
 1b4:	lsl	w12, w12, w15
 1b8:	sub	w13, w13, #0x9
 1bc:	b	3c <__divsf3+0x3c>
 1c0:	cmp	w12, w9, lsl #1
 1c4:	and	w9, w10, #0x7fffff
 1c8:	cinc	w9, w9, cc  // cc = lo, ul, last
 1cc:	tbz	w9, #23, 118 <__divsf3+0x118>
 1d0:	b	144 <__divsf3+0x144>
 1d4:	fmov	s0, w8
 1d8:	mov	w8, #0x7fc00000            	// #2143289344
 1dc:	cmp	w13, #0x0
 1e0:	fmov	s1, w8
 1e4:	fcsel	s0, s0, s1, ne  // ne = any
 1e8:	ret

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	asr	w8, w0, #31
   4:	asr	w9, w1, #31
   8:	eor	w10, w8, w0
   c:	eor	w11, w9, w1
  10:	sub	w10, w10, w8
  14:	sub	w11, w11, w9
  18:	eor	w8, w9, w8
  1c:	udiv	w9, w10, w11
  20:	eor	w9, w9, w8
  24:	sub	w0, w9, w8
  28:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x28, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	sub	sp, sp, #0x200
  18:	str	q1, [sp, #144]
  1c:	str	q2, [sp, #208]
  20:	stur	q2, [x29, #-48]
  24:	ldurb	w8, [x29, #-33]
  28:	and	w8, w8, #0x7f
  2c:	sturb	w8, [x29, #-33]
  30:	ldur	q2, [x29, #-48]
  34:	str	q3, [sp, #192]
  38:	stur	q3, [x29, #-32]
  3c:	ldurb	w8, [x29, #-17]
  40:	and	w8, w8, #0x7f
  44:	sturb	w8, [x29, #-17]
  48:	ldur	q1, [x29, #-32]
  4c:	str	q0, [sp, #160]
  50:	mov	v0.16b, v2.16b
  54:	bl	0 <fmaxl>
  58:	stur	q0, [x29, #-16]
  5c:	ldur	x21, [x29, #-8]
  60:	mov	w8, #0x7fff                	// #32767
  64:	adrp	x20, 0 <__divtc3>
  68:	ubfx	x22, x21, #48, #15
  6c:	cmp	w22, w8
  70:	b.ne	b4 <__divtc3+0xb4>  // b.any
  74:	cmp	x21, #0x0
  78:	mov	v1.16b, v0.16b
  7c:	cset	w19, ge  // ge = tcont
  80:	str	q0, [sp, #176]
  84:	bl	0 <__unordtf2>
  88:	cmp	w0, #0x0
  8c:	cset	w8, ne  // ne = any
  90:	orr	w19, w8, w19
  94:	adrp	x8, 0 <__divtc3>
  98:	ldr	q0, [x8]
  9c:	ldr	q1, [sp, #176]
  a0:	bl	0 <__subtf3>
  a4:	cmp	w19, #0x0
  a8:	b.eq	138 <__divtc3+0x138>  // b.none
  ac:	ldr	q0, [sp, #176]
  b0:	b	138 <__divtc3+0x138>
  b4:	ldr	q1, [x20]
  b8:	ldur	x19, [x29, #-16]
  bc:	bl	0 <__eqtf2>
  c0:	cbnz	w0, d0 <__divtc3+0xd0>
  c4:	adrp	x8, 0 <__divtc3>
  c8:	ldr	q0, [x8]
  cc:	b	138 <__divtc3+0x138>
  d0:	cbz	w22, e0 <__divtc3+0xe0>
  d4:	mov	w8, #0xffffc001            	// #-16383
  d8:	add	w0, w22, w8
  dc:	b	134 <__divtc3+0x134>
  e0:	and	x8, x21, #0x7fffffffffffffff
  e4:	tst	x21, #0x7fffffffffffffff
  e8:	csel	x10, x19, x8, eq  // eq = none
  ec:	cset	w9, eq  // eq = none
  f0:	clz	x10, x10
  f4:	add	w9, w10, w9, lsl #6
  f8:	sub	w10, w9, #0xf
  fc:	neg	x11, x10
 100:	cmp	x10, #0x0
 104:	lsr	x11, x19, x11
 108:	lsl	x8, x8, x10
 10c:	lsl	x12, x19, x10
 110:	sub	x10, x10, #0x40
 114:	csel	x11, xzr, x11, eq  // eq = none
 118:	cmp	x10, #0x0
 11c:	orr	x8, x11, x8
 120:	csel	x8, x12, x8, ge  // ge = tcont
 124:	ubfx	x8, x8, #48, #15
 128:	sub	w8, w8, w9
 12c:	mov	w9, #0xffffc010            	// #-16368
 130:	add	w0, w8, w9
 134:	bl	0 <__floatsitf>
 138:	str	q0, [sp, #64]
 13c:	stur	q0, [x29, #-64]
 140:	ldurb	w8, [x29, #-49]
 144:	and	w8, w8, #0x7f
 148:	sturb	w8, [x29, #-49]
 14c:	adrp	x8, 0 <__divtc3>
 150:	ldur	q0, [x29, #-64]
 154:	ldr	q1, [x8]
 158:	str	q0, [sp, #80]
 15c:	str	q1, [sp, #176]
 160:	bl	0 <__eqtf2>
 164:	ldr	q0, [sp, #80]
 168:	ldr	q1, [sp, #176]
 16c:	cmp	w0, #0x0
 170:	cset	w19, eq  // eq = none
 174:	bl	0 <__unordtf2>
 178:	cmp	w0, #0x0
 17c:	cset	w8, ne  // ne = any
 180:	orr	w8, w8, w19
 184:	cbnz	w8, 1b8 <__divtc3+0x1b8>
 188:	ldr	q0, [sp, #64]
 18c:	bl	0 <__fixtfsi>
 190:	ldr	q0, [sp, #208]
 194:	neg	w19, w0
 198:	mov	w0, w19
 19c:	bl	0 <scalbnl>
 1a0:	str	q0, [sp, #208]
 1a4:	ldr	q0, [sp, #192]
 1a8:	mov	w0, w19
 1ac:	bl	0 <scalbnl>
 1b0:	str	q0, [sp, #192]
 1b4:	b	1bc <__divtc3+0x1bc>
 1b8:	mov	w19, wzr
 1bc:	ldr	q0, [sp, #208]
 1c0:	mov	v1.16b, v0.16b
 1c4:	bl	0 <__multf3>
 1c8:	str	q0, [sp, #128]
 1cc:	ldr	q0, [sp, #192]
 1d0:	mov	v1.16b, v0.16b
 1d4:	bl	0 <__multf3>
 1d8:	mov	v1.16b, v0.16b
 1dc:	ldr	q0, [sp, #128]
 1e0:	bl	0 <__addtf3>
 1e4:	str	q0, [sp, #112]
 1e8:	ldr	q0, [sp, #208]
 1ec:	ldr	q1, [sp, #160]
 1f0:	bl	0 <__multf3>
 1f4:	str	q0, [sp, #128]
 1f8:	ldr	q0, [sp, #192]
 1fc:	ldr	q1, [sp, #144]
 200:	bl	0 <__multf3>
 204:	mov	v1.16b, v0.16b
 208:	ldr	q0, [sp, #128]
 20c:	bl	0 <__addtf3>
 210:	ldr	q1, [sp, #112]
 214:	bl	0 <__divtf3>
 218:	mov	w0, w19
 21c:	bl	0 <scalbnl>
 220:	str	q0, [sp, #128]
 224:	ldr	q0, [sp, #208]
 228:	ldr	q1, [sp, #144]
 22c:	bl	0 <__multf3>
 230:	str	q0, [sp, #96]
 234:	ldr	q0, [sp, #192]
 238:	ldr	q1, [sp, #160]
 23c:	bl	0 <__multf3>
 240:	mov	v1.16b, v0.16b
 244:	ldr	q0, [sp, #96]
 248:	bl	0 <__subtf3>
 24c:	ldr	q1, [sp, #112]
 250:	bl	0 <__divtf3>
 254:	mov	w0, w19
 258:	bl	0 <scalbnl>
 25c:	str	q0, [sp, #96]
 260:	ldr	q0, [sp, #128]
 264:	mov	v1.16b, v0.16b
 268:	bl	0 <__unordtf2>
 26c:	cbz	w0, 618 <__divtc3+0x618>
 270:	ldr	q0, [sp, #96]
 274:	mov	v1.16b, v0.16b
 278:	bl	0 <__unordtf2>
 27c:	cbz	w0, 618 <__divtc3+0x618>
 280:	ldr	q1, [x20]
 284:	ldr	q0, [sp, #112]
 288:	str	q1, [sp, #48]
 28c:	bl	0 <__netf2>
 290:	cbnz	w0, 2b4 <__divtc3+0x2b4>
 294:	ldr	q0, [sp, #160]
 298:	mov	v1.16b, v0.16b
 29c:	bl	0 <__unordtf2>
 2a0:	cbz	w0, 624 <__divtc3+0x624>
 2a4:	ldr	q0, [sp, #144]
 2a8:	mov	v1.16b, v0.16b
 2ac:	bl	0 <__unordtf2>
 2b0:	cbz	w0, 624 <__divtc3+0x624>
 2b4:	ldr	q0, [sp, #160]
 2b8:	stur	q0, [x29, #-160]
 2bc:	ldurb	w8, [x29, #-145]
 2c0:	and	w8, w8, #0x7f
 2c4:	sturb	w8, [x29, #-145]
 2c8:	ldur	q0, [x29, #-160]
 2cc:	ldr	q1, [sp, #176]
 2d0:	str	q0, [sp]
 2d4:	bl	0 <__eqtf2>
 2d8:	ldr	q0, [sp, #144]
 2dc:	cmp	w0, #0x0
 2e0:	cset	w19, eq  // eq = none
 2e4:	stur	q0, [x29, #-144]
 2e8:	ldurb	w8, [x29, #-129]
 2ec:	and	w8, w8, #0x7f
 2f0:	sturb	w8, [x29, #-129]
 2f4:	ldur	q0, [x29, #-144]
 2f8:	ldr	q1, [sp, #176]
 2fc:	str	q0, [sp, #32]
 300:	bl	0 <__eqtf2>
 304:	ldr	q0, [sp, #208]
 308:	cmp	w0, #0x0
 30c:	cset	w21, eq  // eq = none
 310:	stur	q0, [x29, #-128]
 314:	ldurb	w8, [x29, #-113]
 318:	and	w8, w8, #0x7f
 31c:	sturb	w8, [x29, #-113]
 320:	ldur	q1, [x29, #-128]
 324:	ldr	q0, [sp, #192]
 328:	str	q1, [sp, #16]
 32c:	stur	q0, [x29, #-112]
 330:	ldurb	w8, [x29, #-97]
 334:	and	w8, w8, #0x7f
 338:	sturb	w8, [x29, #-97]
 33c:	ldur	q0, [x29, #-112]
 340:	str	q0, [sp, #112]
 344:	mov	v0.16b, v1.16b
 348:	ldr	q1, [sp, #176]
 34c:	bl	0 <__eqtf2>
 350:	ldr	q0, [sp, #16]
 354:	ldr	q1, [sp, #176]
 358:	cmp	w0, #0x0
 35c:	cset	w22, eq  // eq = none
 360:	bl	0 <__unordtf2>
 364:	cmp	w0, #0x0
 368:	cset	w8, ne  // ne = any
 36c:	orr	w8, w8, w22
 370:	cbnz	w8, 49c <__divtc3+0x49c>
 374:	orr	w8, w19, w21
 378:	cbz	w8, 49c <__divtc3+0x49c>
 37c:	ldr	q0, [sp, #112]
 380:	ldr	q1, [sp, #176]
 384:	bl	0 <__eqtf2>
 388:	ldr	q0, [sp, #112]
 38c:	ldr	q1, [sp, #176]
 390:	cmp	w0, #0x0
 394:	cset	w19, eq  // eq = none
 398:	bl	0 <__unordtf2>
 39c:	cmp	w0, #0x0
 3a0:	cset	w8, ne  // ne = any
 3a4:	orr	w8, w8, w19
 3a8:	cbnz	w8, 49c <__divtc3+0x49c>
 3ac:	ldr	q0, [sp]
 3b0:	ldr	q1, [sp, #176]
 3b4:	bl	0 <__eqtf2>
 3b8:	adrp	x8, 0 <__divtc3>
 3bc:	ldr	q1, [x8]
 3c0:	ldr	q0, [sp, #48]
 3c4:	cmp	w0, #0x0
 3c8:	b.ne	3d0 <__divtc3+0x3d0>  // b.any
 3cc:	mov	v0.16b, v1.16b
 3d0:	str	q1, [sp, #128]
 3d4:	ldr	q1, [sp, #160]
 3d8:	stp	q0, q1, [x29, #-192]
 3dc:	ldurb	w8, [x29, #-161]
 3e0:	ldurb	w9, [x29, #-177]
 3e4:	bfxil	w8, w9, #0, #7
 3e8:	sturb	w8, [x29, #-177]
 3ec:	ldur	q0, [x29, #-192]
 3f0:	ldr	q1, [sp, #176]
 3f4:	str	q0, [sp, #160]
 3f8:	ldr	q0, [sp, #32]
 3fc:	bl	0 <__eqtf2>
 400:	cmp	w0, #0x0
 404:	b.ne	410 <__divtc3+0x410>  // b.any
 408:	ldr	q0, [sp, #128]
 40c:	str	q0, [sp, #48]
 410:	ldr	q0, [sp, #144]
 414:	stur	q0, [x29, #-208]
 418:	ldr	q0, [sp, #48]
 41c:	ldurb	w8, [x29, #-193]
 420:	stur	q0, [x29, #-224]
 424:	ldurb	w9, [x29, #-209]
 428:	bfxil	w8, w9, #0, #7
 42c:	sturb	w8, [x29, #-209]
 430:	ldur	q0, [x29, #-224]
 434:	ldr	q1, [sp, #208]
 438:	str	q0, [sp, #144]
 43c:	ldr	q0, [sp, #160]
 440:	bl	0 <__multf3>
 444:	str	q0, [sp, #128]
 448:	ldr	q0, [sp, #144]
 44c:	ldr	q1, [sp, #192]
 450:	bl	0 <__multf3>
 454:	mov	v1.16b, v0.16b
 458:	ldr	q0, [sp, #128]
 45c:	bl	0 <__addtf3>
 460:	ldr	q1, [sp, #176]
 464:	bl	0 <__multf3>
 468:	str	q0, [sp, #128]
 46c:	ldr	q0, [sp, #144]
 470:	ldr	q1, [sp, #208]
 474:	bl	0 <__multf3>
 478:	str	q0, [sp, #208]
 47c:	ldr	q0, [sp, #160]
 480:	ldr	q1, [sp, #192]
 484:	bl	0 <__multf3>
 488:	mov	v1.16b, v0.16b
 48c:	ldr	q0, [sp, #208]
 490:	bl	0 <__subtf3>
 494:	ldr	q1, [sp, #176]
 498:	b	660 <__divtc3+0x660>
 49c:	ldr	q0, [sp, #32]
 4a0:	ldr	q1, [sp, #176]
 4a4:	bl	0 <__eqtf2>
 4a8:	ldr	q0, [sp, #32]
 4ac:	ldr	q1, [sp, #176]
 4b0:	cmp	w0, #0x0
 4b4:	cset	w19, eq  // eq = none
 4b8:	bl	0 <__unordtf2>
 4bc:	cmp	w0, #0x0
 4c0:	cset	w8, ne  // ne = any
 4c4:	orr	w8, w8, w19
 4c8:	cbnz	w8, 618 <__divtc3+0x618>
 4cc:	ldr	q0, [sp]
 4d0:	ldr	q1, [sp, #176]
 4d4:	bl	0 <__eqtf2>
 4d8:	ldr	q0, [sp]
 4dc:	ldr	q1, [sp, #176]
 4e0:	cmp	w0, #0x0
 4e4:	cset	w19, eq  // eq = none
 4e8:	bl	0 <__unordtf2>
 4ec:	cmp	w0, #0x0
 4f0:	cset	w8, ne  // ne = any
 4f4:	orr	w8, w8, w19
 4f8:	cbnz	w8, 618 <__divtc3+0x618>
 4fc:	ldr	q1, [x20]
 500:	ldr	q0, [sp, #64]
 504:	str	q1, [sp, #64]
 508:	bl	0 <__gttf2>
 50c:	cmp	w0, #0x0
 510:	b.le	618 <__divtc3+0x618>
 514:	ldr	q0, [sp, #80]
 518:	ldr	q1, [sp, #176]
 51c:	bl	0 <__netf2>
 520:	ldr	q0, [sp, #128]
 524:	ldr	q1, [sp, #96]
 528:	cbnz	w0, 66c <__divtc3+0x66c>
 52c:	ldr	q0, [sp, #16]
 530:	ldr	q1, [sp, #176]
 534:	bl	0 <__eqtf2>
 538:	adrp	x8, 0 <__divtc3>
 53c:	ldr	q1, [x8]
 540:	ldr	q0, [sp, #64]
 544:	cmp	w0, #0x0
 548:	b.ne	550 <__divtc3+0x550>  // b.any
 54c:	mov	v0.16b, v1.16b
 550:	str	q1, [sp, #128]
 554:	ldr	q1, [sp, #208]
 558:	stp	q0, q1, [sp, #224]
 55c:	ldrb	w8, [sp, #255]
 560:	ldrb	w9, [sp, #239]
 564:	ldr	q1, [sp, #176]
 568:	bfxil	w8, w9, #0, #7
 56c:	strb	w8, [sp, #239]
 570:	ldr	q0, [sp, #224]
 574:	str	q0, [sp, #208]
 578:	ldr	q0, [sp, #112]
 57c:	bl	0 <__eqtf2>
 580:	ldr	q0, [sp, #64]
 584:	cmp	w0, #0x0
 588:	b.ne	590 <__divtc3+0x590>  // b.any
 58c:	ldr	q0, [sp, #128]
 590:	ldr	q1, [sp, #192]
 594:	stur	q1, [x29, #-240]
 598:	str	q0, [sp, #256]
 59c:	ldurb	w8, [x29, #-225]
 5a0:	ldrb	w9, [sp, #271]
 5a4:	ldr	q1, [sp, #160]
 5a8:	bfxil	w8, w9, #0, #7
 5ac:	strb	w8, [sp, #271]
 5b0:	ldr	q0, [sp, #256]
 5b4:	str	q0, [sp, #192]
 5b8:	ldr	q0, [sp, #208]
 5bc:	bl	0 <__multf3>
 5c0:	str	q0, [sp, #176]
 5c4:	ldr	q0, [sp, #192]
 5c8:	ldr	q1, [sp, #144]
 5cc:	bl	0 <__multf3>
 5d0:	mov	v1.16b, v0.16b
 5d4:	ldr	q0, [sp, #176]
 5d8:	bl	0 <__addtf3>
 5dc:	ldr	q1, [sp, #64]
 5e0:	bl	0 <__multf3>
 5e4:	str	q0, [sp, #128]
 5e8:	ldr	q0, [sp, #208]
 5ec:	ldr	q1, [sp, #144]
 5f0:	bl	0 <__multf3>
 5f4:	str	q0, [sp, #208]
 5f8:	ldr	q0, [sp, #192]
 5fc:	ldr	q1, [sp, #160]
 600:	bl	0 <__multf3>
 604:	mov	v1.16b, v0.16b
 608:	ldr	q0, [sp, #208]
 60c:	bl	0 <__subtf3>
 610:	ldr	q1, [sp, #64]
 614:	b	660 <__divtc3+0x660>
 618:	ldr	q0, [sp, #128]
 61c:	ldr	q1, [sp, #96]
 620:	b	66c <__divtc3+0x66c>
 624:	ldr	q0, [sp, #208]
 628:	stur	q0, [x29, #-80]
 62c:	ldr	q0, [sp, #176]
 630:	ldurb	w8, [x29, #-65]
 634:	stur	q0, [x29, #-96]
 638:	ldurb	w9, [x29, #-81]
 63c:	bfxil	w8, w9, #0, #7
 640:	sturb	w8, [x29, #-81]
 644:	ldur	q0, [x29, #-96]
 648:	ldr	q1, [sp, #160]
 64c:	str	q0, [sp, #208]
 650:	bl	0 <__multf3>
 654:	str	q0, [sp, #128]
 658:	ldr	q0, [sp, #208]
 65c:	ldr	q1, [sp, #144]
 660:	bl	0 <__multf3>
 664:	mov	v1.16b, v0.16b
 668:	ldr	q0, [sp, #128]
 66c:	add	sp, sp, #0x200
 670:	ldp	x20, x19, [sp, #48]
 674:	ldp	x22, x21, [sp, #32]
 678:	ldr	x28, [sp, #16]
 67c:	ldp	x29, x30, [sp], #64
 680:	ret

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x8, x1, #63
  10:	eor	x11, x8, x0
  14:	asr	x9, x3, #63
  18:	eor	x10, x8, x1
  1c:	subs	x0, x11, x8
  20:	eor	x13, x9, x2
  24:	sbcs	x1, x10, x8
  28:	eor	x12, x9, x3
  2c:	subs	x2, x13, x9
  30:	sbcs	x3, x12, x9
  34:	mov	x4, xzr
  38:	eor	x19, x9, x8
  3c:	bl	0 <__udivmodti4>
  40:	eor	x9, x0, x19
  44:	eor	x8, x1, x19
  48:	subs	x0, x9, x19
  4c:	sbcs	x1, x8, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #32
  58:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtf3>:
   0:	sub	sp, sp, #0xe0
   4:	stp	x22, x21, [sp, #192]
   8:	stp	x20, x19, [sp, #208]
   c:	stp	q1, q0, [sp, #160]
  10:	ldp	x11, x12, [sp, #176]
  14:	ldp	x9, x17, [sp, #160]
  18:	mov	w10, #0x7ffd                	// #32765
  1c:	ubfx	x15, x12, #48, #15
  20:	eor	x8, x17, x12
  24:	sub	w14, w15, #0x1
  28:	and	x13, x12, #0xffffffffffff
  2c:	ubfx	x16, x17, #48, #15
  30:	and	x8, x8, #0x8000000000000000
  34:	cmp	w14, w10
  38:	and	x10, x17, #0xffffffffffff
  3c:	b.hi	3a0 <__divtf3+0x3a0>  // b.pmore
  40:	sub	w14, w16, #0x1
  44:	mov	w18, #0x7ffe                	// #32766
  48:	cmp	w14, w18
  4c:	b.cs	3a0 <__divtf3+0x3a0>  // b.hs, b.nlast
  50:	mov	w14, wzr
  54:	sub	w15, w15, w16
  58:	mov	x16, #0x6484                	// #25732
  5c:	movk	x16, #0xf9de, lsl #16
  60:	orr	x12, x10, #0x1000000000000
  64:	movk	x16, #0xf333, lsl #32
  68:	movk	x16, #0x7504, lsl #48
  6c:	lsr	x1, x13, #30
  70:	extr	x4, x12, x9, #49
  74:	extr	x13, x13, x11, #62
  78:	and	x1, x1, #0xffffffff
  7c:	sub	x16, x16, x4
  80:	and	x5, x13, #0xffffffff
  84:	orr	x13, x1, #0x40000
  88:	umulh	x1, x4, x16
  8c:	neg	x19, x1
  90:	mneg	x1, x1, x16
  94:	umulh	x16, x19, x16
  98:	extr	x16, x16, x1, #63
  9c:	umulh	x1, x16, x4
  a0:	neg	x19, x1
  a4:	mneg	x1, x1, x16
  a8:	umulh	x16, x16, x19
  ac:	extr	x16, x16, x1, #63
  b0:	umulh	x1, x16, x4
  b4:	neg	x19, x1
  b8:	mneg	x1, x1, x16
  bc:	umulh	x16, x16, x19
  c0:	extr	x16, x16, x1, #63
  c4:	umulh	x1, x16, x4
  c8:	neg	x19, x1
  cc:	mneg	x1, x1, x16
  d0:	umulh	x16, x16, x19
  d4:	extr	x16, x16, x1, #63
  d8:	umulh	x1, x16, x4
  dc:	lsr	x6, x4, #32
  e0:	and	x7, x4, #0xffffffff
  e4:	neg	x4, x1
  e8:	mneg	x1, x1, x16
  ec:	umulh	x16, x16, x4
  f0:	extr	x16, x16, x1, #63
  f4:	sub	x16, x16, #0x1
  f8:	lsr	x1, x16, #32
  fc:	and	x16, x16, #0xffffffff
 100:	mul	x4, x1, x7
 104:	mul	x19, x16, x6
 108:	adds	x4, x19, x4
 10c:	mul	x7, x16, x7
 110:	adcs	x19, xzr, xzr
 114:	lsl	w17, w9, #15
 118:	ubfx	x18, x9, #17, #32
 11c:	adds	x7, x7, x4, lsl #32
 120:	mul	x20, x1, x17
 124:	mul	x21, x16, x18
 128:	extr	x4, x19, x4, #32
 12c:	adcs	x19, xzr, xzr
 130:	madd	x4, x1, x6, x4
 134:	adds	x6, x21, x20
 138:	mul	x17, x16, x17
 13c:	add	x4, x19, x4
 140:	adcs	x19, xzr, xzr
 144:	cmn	x17, x6, lsl #32
 148:	extr	x17, x19, x6, #32
 14c:	adcs	x6, xzr, xzr
 150:	madd	x17, x1, x18, x17
 154:	add	x17, x6, x17
 158:	adds	x17, x7, x17
 15c:	adcs	x18, x4, xzr
 160:	negs	x17, x17
 164:	ngcs	x18, x18
 168:	lsr	x6, x18, #32
 16c:	and	x18, x18, #0xffffffff
 170:	lsr	x4, x17, #32
 174:	and	x17, x17, #0xffffffff
 178:	mul	x20, x18, x1
 17c:	mul	x21, x16, x6
 180:	mul	x7, x17, x1
 184:	mul	x19, x4, x16
 188:	mul	x17, x17, x16
 18c:	mul	x16, x18, x16
 190:	adds	x18, x20, x21
 194:	adcs	x20, xzr, xzr
 198:	adds	x7, x19, x7
 19c:	adcs	x19, xzr, xzr
 1a0:	cmn	x17, x7, lsl #32
 1a4:	extr	x17, x19, x7, #32
 1a8:	adcs	x7, xzr, xzr
 1ac:	extr	x20, x20, x18, #32
 1b0:	madd	x17, x4, x1, x17
 1b4:	adds	x16, x16, x18, lsl #32
 1b8:	madd	x6, x1, x6, x20
 1bc:	add	x17, x7, x17
 1c0:	adcs	x18, xzr, xzr
 1c4:	add	x18, x18, x6
 1c8:	adds	x16, x16, x17
 1cc:	adcs	x17, x18, xzr
 1d0:	subs	x16, x16, #0x2
 1d4:	mov	x0, #0xffffffffffffffff    	// #-1
 1d8:	ubfx	x2, x11, #30, #32
 1dc:	lsl	w3, w11, #2
 1e0:	lsr	x18, x16, #32
 1e4:	and	x16, x16, #0xffffffff
 1e8:	adcs	x17, x17, x0
 1ec:	mul	x1, x18, x13
 1f0:	mul	x4, x16, x13
 1f4:	mul	x6, x18, x5
 1f8:	mul	x7, x16, x5
 1fc:	mul	x19, x18, x2
 200:	mul	x16, x16, x2
 204:	mul	x18, x18, x3
 208:	adds	x16, x18, x16
 20c:	adcs	x18, xzr, xzr
 210:	lsr	x0, x17, #32
 214:	and	x17, x17, #0xffffffff
 218:	adds	x7, x19, x7
 21c:	mul	x20, x17, x13
 220:	mul	x21, x5, x0
 224:	mul	x5, x17, x5
 228:	mul	x22, x2, x0
 22c:	mul	x2, x17, x2
 230:	mul	x17, x17, x3
 234:	extr	x16, x18, x16, #32
 238:	adcs	x18, xzr, xzr
 23c:	adds	x17, x7, x17
 240:	mul	x3, x3, x0
 244:	adcs	x18, x18, xzr
 248:	adds	x3, x3, x4
 24c:	adcs	x4, xzr, xzr
 250:	adds	x3, x3, x6
 254:	adcs	x4, x4, xzr
 258:	adds	x2, x3, x2
 25c:	adcs	x3, x4, xzr
 260:	adds	x4, x20, x21
 264:	adcs	x6, xzr, xzr
 268:	adds	x16, x17, x16
 26c:	extr	x17, x6, x4, #32
 270:	adcs	x6, xzr, xzr
 274:	cmn	x16, x2, lsl #32
 278:	adcs	x16, x6, xzr
 27c:	extr	x3, x3, x2, #32
 280:	adds	x2, x5, x22
 284:	adcs	x5, xzr, xzr
 288:	adds	x1, x2, x1
 28c:	adcs	x2, x5, xzr
 290:	madd	x13, x13, x0, x2
 294:	adds	x0, x1, x4, lsl #32
 298:	adcs	x13, x13, x17
 29c:	adds	x17, x0, x18
 2a0:	adcs	x13, x13, xzr
 2a4:	adds	x17, x17, x3
 2a8:	adcs	x18, x13, xzr
 2ac:	adds	x13, x17, x16
 2b0:	adcs	x16, x18, xzr
 2b4:	lsr	x17, x16, #49
 2b8:	add	w14, w14, w15
 2bc:	cbnz	x17, 3dc <__divtf3+0x3dc>
 2c0:	extr	x3, x16, x13, #32
 2c4:	lsr	x15, x16, #32
 2c8:	and	x18, x10, #0xffffffff
 2cc:	lsr	x0, x9, #32
 2d0:	and	x1, x9, #0xffffffff
 2d4:	and	x5, x13, #0xffffffff
 2d8:	and	x3, x3, #0xffffffff
 2dc:	lsr	x17, x12, #32
 2e0:	and	x2, x16, #0xffffffff
 2e4:	mul	w15, w9, w15
 2e8:	mul	x18, x5, x18
 2ec:	mul	x6, x5, x0
 2f0:	mul	x7, x3, x1
 2f4:	madd	w15, w13, w17, w15
 2f8:	madd	x17, x2, x1, x18
 2fc:	adds	x18, x7, x6
 300:	lsr	x4, x13, #32
 304:	mul	x5, x5, x1
 308:	madd	w15, w16, w0, w15
 30c:	madd	x17, x3, x0, x17
 310:	adcs	x0, xzr, xzr
 314:	lsl	x11, x11, #49
 318:	madd	w10, w4, w10, w15
 31c:	extr	x15, x0, x18, #32
 320:	negs	x0, x5
 324:	add	x15, x17, x15
 328:	sbcs	x11, x11, xzr
 32c:	add	x15, x15, x10, lsl #32
 330:	subs	x10, x0, x18, lsl #32
 334:	sbcs	x11, x11, xzr
 338:	sub	x11, x11, x15
 33c:	sub	w14, w14, #0x1
 340:	cmp	w14, #0x4, lsl #12
 344:	b.ge	468 <__divtf3+0x468>  // b.tcont
 348:	mov	w15, #0x3fff                	// #16383
 34c:	mov	w17, #0xffffc001            	// #-16383
 350:	cmp	w14, w17
 354:	add	w14, w14, w15
 358:	b.gt	4bc <__divtf3+0x4bc>
 35c:	cbnz	w14, 388 <__divtf3+0x388>
 360:	extr	x11, x11, x10, #63
 364:	cmp	x9, x10, lsl #1
 368:	cset	w9, cc  // cc = lo, ul, last
 36c:	cmp	x11, x12
 370:	cset	w10, hi  // hi = pmore
 374:	csel	w9, w9, w10, eq  // eq = none
 378:	and	x10, x16, #0xffffffffffff
 37c:	adds	x9, x13, x9
 380:	adcs	x10, x10, xzr
 384:	tbnz	x10, #48, 5dc <__divtf3+0x5dc>
 388:	stp	xzr, x8, [sp, #96]
 38c:	ldr	q0, [sp, #96]
 390:	ldp	x20, x19, [sp, #208]
 394:	ldp	x22, x21, [sp, #192]
 398:	add	sp, sp, #0xe0
 39c:	ret
 3a0:	and	x14, x12, #0x7fffffffffffffff
 3a4:	cmp	x11, #0x0
 3a8:	mov	x18, #0x7fff000000000000    	// #9223090561878065152
 3ac:	cset	w0, eq  // eq = none
 3b0:	cmp	x14, x18
 3b4:	cset	w1, cc  // cc = lo, ul, last
 3b8:	csel	w0, w0, w1, eq  // eq = none
 3bc:	tbnz	w0, #0, 484 <__divtf3+0x484>
 3c0:	orr	x8, x12, #0x800000000000
 3c4:	stp	x11, x8, [sp]
 3c8:	ldr	q0, [sp]
 3cc:	ldp	x20, x19, [sp, #208]
 3d0:	ldp	x22, x21, [sp, #192]
 3d4:	add	sp, sp, #0xe0
 3d8:	ret
 3dc:	lsr	x15, x13, #1
 3e0:	extr	x17, x16, x13, #33
 3e4:	extr	x13, x16, x13, #1
 3e8:	lsr	x18, x16, #33
 3ec:	lsr	x3, x9, #32
 3f0:	and	x4, x9, #0xffffffff
 3f4:	and	x5, x17, #0xffffffff
 3f8:	and	x6, x13, #0xffffffff
 3fc:	ubfx	x0, x16, #1, #32
 400:	lsr	x1, x12, #32
 404:	mul	w18, w9, w18
 408:	mul	x7, x5, x4
 40c:	mul	x19, x6, x3
 410:	lsr	x16, x16, #1
 414:	mul	x0, x0, x4
 418:	madd	w15, w15, w1, w18
 41c:	adds	x18, x19, x7
 420:	and	x2, x10, #0xffffffff
 424:	mul	x4, x6, x4
 428:	madd	x0, x5, x3, x0
 42c:	madd	w15, w16, w3, w15
 430:	adcs	x1, xzr, xzr
 434:	lsl	x11, x11, #48
 438:	madd	x0, x6, x2, x0
 43c:	madd	w10, w17, w10, w15
 440:	extr	x15, x1, x18, #32
 444:	negs	x17, x4
 448:	add	x15, x0, x15
 44c:	sbcs	x11, x11, xzr
 450:	add	x15, x15, x10, lsl #32
 454:	subs	x10, x17, x18, lsl #32
 458:	sbcs	x11, x11, xzr
 45c:	sub	x11, x11, x15
 460:	cmp	w14, #0x4, lsl #12
 464:	b.lt	348 <__divtf3+0x348>  // b.tstop
 468:	orr	x8, x8, #0x7fff000000000000
 46c:	stp	xzr, x8, [sp, #80]
 470:	ldr	q0, [sp, #80]
 474:	ldp	x20, x19, [sp, #208]
 478:	ldp	x22, x21, [sp, #192]
 47c:	add	sp, sp, #0xe0
 480:	ret
 484:	and	x12, x17, #0x7fffffffffffffff
 488:	cmp	x9, #0x0
 48c:	cset	w0, eq  // eq = none
 490:	cmp	x12, x18
 494:	cset	w18, cc  // cc = lo, ul, last
 498:	csel	w18, w0, w18, eq  // eq = none
 49c:	tbnz	w18, #0, 4fc <__divtf3+0x4fc>
 4a0:	orr	x8, x17, #0x800000000000
 4a4:	stp	x9, x8, [sp, #16]
 4a8:	ldr	q0, [sp, #16]
 4ac:	ldp	x20, x19, [sp, #208]
 4b0:	ldp	x22, x21, [sp, #192]
 4b4:	add	sp, sp, #0xe0
 4b8:	ret
 4bc:	extr	x11, x11, x10, #63
 4c0:	cmp	x9, x10, lsl #1
 4c4:	cset	w9, ls  // ls = plast
 4c8:	cmp	x11, x12
 4cc:	cset	w10, cs  // cs = hs, nlast
 4d0:	csel	w9, w9, w10, eq  // eq = none
 4d4:	bfi	x16, x14, #48, #16
 4d8:	adds	x9, x13, x9
 4dc:	adcs	x10, x16, xzr
 4e0:	orr	x8, x10, x8
 4e4:	stp	x9, x8, [sp, #128]
 4e8:	ldr	q0, [sp, #128]
 4ec:	ldp	x20, x19, [sp, #208]
 4f0:	ldp	x22, x21, [sp, #192]
 4f4:	add	sp, sp, #0xe0
 4f8:	ret
 4fc:	eor	x17, x14, #0x7fff000000000000
 500:	orr	x17, x11, x17
 504:	cbnz	x17, 52c <__divtf3+0x52c>
 508:	eor	x10, x12, #0x7fff000000000000
 50c:	orr	x9, x9, x10
 510:	cbnz	x9, 550 <__divtf3+0x550>
 514:	adrp	x8, 0 <__divtf3>
 518:	ldr	q0, [x8]
 51c:	ldp	x20, x19, [sp, #208]
 520:	ldp	x22, x21, [sp, #192]
 524:	add	sp, sp, #0xe0
 528:	ret
 52c:	eor	x17, x12, #0x7fff000000000000
 530:	orr	x17, x9, x17
 534:	cbnz	x17, 56c <__divtf3+0x56c>
 538:	stp	xzr, x8, [sp, #48]
 53c:	ldr	q0, [sp, #48]
 540:	ldp	x20, x19, [sp, #208]
 544:	ldp	x22, x21, [sp, #192]
 548:	add	sp, sp, #0xe0
 54c:	ret
 550:	orr	x8, x8, #0x7fff000000000000
 554:	stp	xzr, x8, [sp, #32]
 558:	ldr	q0, [sp, #32]
 55c:	ldp	x20, x19, [sp, #208]
 560:	ldp	x22, x21, [sp, #192]
 564:	add	sp, sp, #0xe0
 568:	ret
 56c:	orr	x17, x11, x14
 570:	cbz	x17, 5f8 <__divtf3+0x5f8>
 574:	orr	x17, x9, x12
 578:	cbz	x17, 61c <__divtf3+0x61c>
 57c:	lsr	x14, x14, #48
 580:	cbnz	x14, 638 <__divtf3+0x638>
 584:	cmp	x13, #0x0
 588:	csel	x18, x11, x13, eq  // eq = none
 58c:	cset	w17, eq  // eq = none
 590:	clz	x18, x18
 594:	add	w17, w18, w17, lsl #6
 598:	sub	w18, w17, #0xf
 59c:	neg	x0, x18
 5a0:	cmp	x18, #0x0
 5a4:	lsl	x1, x11, x18
 5a8:	lsl	x13, x13, x18
 5ac:	lsr	x0, x11, x0
 5b0:	lsl	x11, x11, x18
 5b4:	sub	x18, x18, #0x40
 5b8:	csel	x0, xzr, x0, eq  // eq = none
 5bc:	cmp	x18, #0x0
 5c0:	mov	w14, #0x10                  	// #16
 5c4:	csel	x18, xzr, x1, ge  // ge = tcont
 5c8:	orr	x13, x0, x13
 5cc:	csel	x13, x11, x13, ge  // ge = tcont
 5d0:	sub	w14, w14, w17
 5d4:	mov	x11, x18
 5d8:	b	63c <__divtf3+0x63c>
 5dc:	orr	x8, x10, x8
 5e0:	stp	x9, x8, [sp, #112]
 5e4:	ldr	q0, [sp, #112]
 5e8:	ldp	x20, x19, [sp, #208]
 5ec:	ldp	x22, x21, [sp, #192]
 5f0:	add	sp, sp, #0xe0
 5f4:	ret
 5f8:	orr	x9, x9, x12
 5fc:	cmp	x9, #0x0
 600:	stp	xzr, x8, [sp, #64]
 604:	b.eq	514 <__divtf3+0x514>  // b.none
 608:	ldr	q0, [sp, #64]
 60c:	ldp	x20, x19, [sp, #208]
 610:	ldp	x22, x21, [sp, #192]
 614:	add	sp, sp, #0xe0
 618:	ret
 61c:	orr	x8, x8, #0x7fff000000000000
 620:	stp	xzr, x8, [sp, #144]
 624:	ldr	q0, [sp, #144]
 628:	ldp	x20, x19, [sp, #208]
 62c:	ldp	x22, x21, [sp, #192]
 630:	add	sp, sp, #0xe0
 634:	ret
 638:	mov	w14, wzr
 63c:	lsr	x12, x12, #48
 640:	cbnz	x12, 54 <__divtf3+0x54>
 644:	cmp	x10, #0x0
 648:	csel	x17, x9, x10, eq  // eq = none
 64c:	cset	w12, eq  // eq = none
 650:	clz	x17, x17
 654:	add	w12, w17, w12, lsl #6
 658:	sub	w17, w12, #0xf
 65c:	add	w12, w12, w14
 660:	neg	x14, x17
 664:	cmp	x17, #0x0
 668:	lsl	x18, x9, x17
 66c:	sub	x0, x17, #0x40
 670:	lsl	x10, x10, x17
 674:	lsl	x17, x9, x17
 678:	lsr	x9, x9, x14
 67c:	csel	x9, xzr, x9, eq  // eq = none
 680:	cmp	x0, #0x0
 684:	csel	x18, xzr, x18, ge  // ge = tcont
 688:	orr	x9, x9, x10
 68c:	csel	x10, x17, x9, ge  // ge = tcont
 690:	sub	w14, w12, #0x10
 694:	mov	x9, x18
 698:	b	54 <__divtf3+0x54>

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	fmov	w10, s0
   4:	and	w9, w10, #0x7fffffff
   8:	sub	w8, w9, #0x800, lsl #12
   c:	lsr	w8, w8, #24
  10:	cmp	w8, #0x7e
  14:	and	w8, w10, #0x80000000
  18:	b.hi	30 <__extendsfdf2+0x30>  // b.pmore
  1c:	mov	x10, #0x3800000000000000    	// #4035225266123964416
  20:	add	x9, x10, x9, lsl #29
  24:	orr	x8, x9, x8, lsl #32
  28:	fmov	d0, x8
  2c:	ret
  30:	lsr	w11, w9, #23
  34:	cmp	w11, #0xff
  38:	b.cc	54 <__extendsfdf2+0x54>  // b.lo, b.ul, b.last
  3c:	mov	w9, w10
  40:	lsl	x9, x9, #29
  44:	orr	x9, x9, #0x7ff0000000000000
  48:	orr	x8, x9, x8, lsl #32
  4c:	fmov	d0, x8
  50:	ret
  54:	cbz	w9, 80 <__extendsfdf2+0x80>
  58:	clz	w10, w9
  5c:	add	w12, w10, #0x15
  60:	mov	w11, #0x389                 	// #905
  64:	lsl	x9, x9, x12
  68:	eor	x9, x9, #0x10000000000000
  6c:	sub	w10, w11, w10
  70:	orr	x9, x9, x10, lsl #52
  74:	orr	x8, x9, x8, lsl #32
  78:	fmov	d0, x8
  7c:	ret
  80:	mov	x9, xzr
  84:	orr	x8, x9, x8, lsl #32
  88:	fmov	d0, x8
  8c:	ret

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	and	w9, w0, #0x7fff
   4:	sub	w8, w9, #0x400
   8:	ubfx	w8, w8, #11, #5
   c:	cmp	w8, #0xe
  10:	and	w8, w0, #0xffff8000
  14:	b.hi	2c <__extendhfsf2+0x2c>  // b.pmore
  18:	mov	w10, #0x38000000            	// #939524096
  1c:	add	w9, w10, w9, lsl #13
  20:	orr	w8, w9, w8, lsl #16
  24:	fmov	s0, w8
  28:	ret
  2c:	lsr	w10, w9, #10
  30:	cmp	w10, #0x1f
  34:	b.cc	4c <__extendhfsf2+0x4c>  // b.lo, b.ul, b.last
  38:	lsl	w9, w9, #13
  3c:	orr	w9, w9, #0x7f800000
  40:	orr	w8, w9, w8, lsl #16
  44:	fmov	s0, w8
  48:	ret
  4c:	cbz	w9, 20 <__extendhfsf2+0x20>
  50:	clz	w10, w9
  54:	sub	w12, w10, #0x8
  58:	mov	w11, #0x43000000            	// #1124073472
  5c:	lsl	w9, w9, w12
  60:	eor	w9, w9, #0x800000
  64:	sub	w10, w11, w10, lsl #23
  68:	orr	w9, w9, w10
  6c:	orr	w8, w9, w8, lsl #16
  70:	fmov	s0, w8
  74:	ret

0000000000000078 <__gnu_h2f_ieee>:
  78:	b	0 <__extendhfsf2>

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbz	w0, 14 <__ffsdi2+0x14>
   4:	rbit	w8, w0
   8:	clz	w8, w8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	lsr	x8, x0, #32
  18:	cbz	w8, 2c <__ffsdi2+0x2c>
  1c:	rbit	w8, w8
  20:	clz	w8, w8
  24:	add	w0, w8, #0x21
  28:	ret
  2c:	mov	w0, wzr
  30:	ret

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	rbit	w8, w0
   4:	clz	w8, w8
   8:	cmp	w0, #0x0
   c:	csinc	w0, wzr, w8, eq  // eq = none
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbz	x0, 14 <__ffsti2+0x14>
   4:	rbit	x8, x0
   8:	clz	x8, x8
   c:	add	w0, w8, #0x1
  10:	ret
  14:	cbz	x1, 28 <__ffsti2+0x28>
  18:	rbit	x8, x1
  1c:	clz	x8, x8
  20:	add	w0, w8, #0x41
  24:	ret
  28:	mov	w0, wzr
  2c:	ret

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	fcmp	d0, #0.0
   4:	b.pl	24 <__fixdfdi+0x24>  // b.nfrst
   8:	stp	x29, x30, [sp, #-16]!
   c:	mov	x29, sp
  10:	fneg	d0, d0
  14:	bl	0 <__fixunsdfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunsdfdi>

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	fmov	x8, d0
   4:	ubfx	x9, x8, #52, #11
   8:	subs	w10, w9, #0x3ff
   c:	b.cs	18 <__fixdfsi+0x18>  // b.hs, b.nlast
  10:	mov	w0, wzr
  14:	ret
  18:	cmp	w10, #0x20
  1c:	b.cc	30 <__fixdfsi+0x30>  // b.lo, b.ul, b.last
  20:	cmp	x8, #0x0
  24:	mov	w8, #0x80000000            	// #-2147483648
  28:	cinv	w0, w8, ge  // ge = tcont
  2c:	ret
  30:	mov	x10, #0x10000000000000      	// #4503599627370496
  34:	mov	w11, #0x433                 	// #1075
  38:	bfxil	x10, x8, #0, #52
  3c:	sub	w9, w11, w9
  40:	lsr	x9, x10, x9
  44:	cmp	x8, #0x0
  48:	cneg	w0, w9, lt  // lt = tstop
  4c:	ret

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	fmov	x12, d0
   4:	cmp	x12, #0x0
   8:	mov	x8, #0xffffffffffffffff    	// #-1
   c:	ubfx	x11, x12, #52, #11
  10:	cneg	x8, x8, ge  // ge = tcont
  14:	subs	w9, w11, #0x3ff
  18:	b.cs	28 <__fixdfti+0x28>  // b.hs, b.nlast
  1c:	mov	x0, xzr
  20:	mov	x1, xzr
  24:	ret
  28:	cmp	w9, #0x80
  2c:	b.cc	44 <__fixdfti+0x44>  // b.lo, b.ul, b.last
  30:	cmp	x12, #0x0
  34:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
  38:	cinv	x1, x8, ge  // ge = tcont
  3c:	csetm	x0, ge  // ge = tcont
  40:	ret
  44:	mov	x10, #0x10000000000000      	// #4503599627370496
  48:	asr	x9, x12, #63
  4c:	cmp	w11, #0x432
  50:	bfxil	x10, x12, #0, #52
  54:	b.hi	74 <__fixdfti+0x74>  // b.pmore
  58:	mov	w12, #0x433                 	// #1075
  5c:	sub	w11, w12, w11
  60:	lsr	x10, x10, x11
  64:	umulh	x11, x8, x10
  68:	madd	x1, x9, x10, x11
  6c:	mul	x0, x8, x10
  70:	ret
  74:	sub	w11, w11, #0x433
  78:	neg	x12, x11
  7c:	cmp	x11, #0x0
  80:	lsl	x13, x10, x11
  84:	sub	x14, x11, #0x40
  88:	lsl	x11, x10, x11
  8c:	lsr	x10, x10, x12
  90:	csel	x10, xzr, x10, eq  // eq = none
  94:	cmp	x14, #0x0
  98:	csel	x11, xzr, x11, ge  // ge = tcont
  9c:	umulh	x12, x11, x8
  a0:	csel	x10, x13, x10, ge  // ge = tcont
  a4:	madd	x9, x11, x9, x12
  a8:	madd	x1, x10, x8, x9
  ac:	mul	x0, x11, x8
  b0:	ret

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	fcmp	s0, #0.0
   4:	b.pl	24 <__fixsfdi+0x24>  // b.nfrst
   8:	stp	x29, x30, [sp, #-16]!
   c:	mov	x29, sp
  10:	fneg	s0, s0
  14:	bl	0 <__fixunssfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunssfdi>

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	fmov	w10, s0
   4:	cmp	w10, #0x0
   8:	mov	w8, #0x1                   	// #1
   c:	ubfx	w9, w10, #23, #8
  10:	cneg	w8, w8, lt  // lt = tstop
  14:	subs	w11, w9, #0x7f
  18:	b.cs	24 <__fixsfsi+0x24>  // b.hs, b.nlast
  1c:	mov	w0, wzr
  20:	ret
  24:	cmp	w11, #0x20
  28:	b.cc	3c <__fixsfsi+0x3c>  // b.lo, b.ul, b.last
  2c:	cmp	w10, #0x0
  30:	mov	w8, #0x7fffffff            	// #2147483647
  34:	cinv	w0, w8, lt  // lt = tstop
  38:	ret
  3c:	mov	w11, #0x800000              	// #8388608
  40:	cmp	w9, #0x95
  44:	bfxil	w11, w10, #0, #23
  48:	b.hi	60 <__fixsfsi+0x60>  // b.pmore
  4c:	mov	w10, #0x96                  	// #150
  50:	sub	w9, w10, w9
  54:	lsr	w9, w11, w9
  58:	mul	w0, w9, w8
  5c:	ret
  60:	sub	w9, w9, #0x96
  64:	lsl	w9, w11, w9
  68:	mul	w0, w9, w8
  6c:	ret

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	fmov	w11, s0
   4:	cmp	w11, #0x0
   8:	mov	w8, #0x1                   	// #1
   c:	ubfx	w9, w11, #23, #8
  10:	cneg	w10, w8, lt  // lt = tstop
  14:	subs	w8, w9, #0x7f
  18:	b.cs	28 <__fixsfti+0x28>  // b.hs, b.nlast
  1c:	mov	x0, xzr
  20:	mov	x1, xzr
  24:	ret
  28:	cmp	w8, #0x80
  2c:	b.cc	44 <__fixsfti+0x44>  // b.lo, b.ul, b.last
  30:	cmp	w11, #0x0
  34:	mov	x8, #0x7fffffffffffffff    	// #9223372036854775807
  38:	cinv	x1, x8, lt  // lt = tstop
  3c:	csetm	x0, ge  // ge = tcont
  40:	ret
  44:	mov	w8, #0x800000              	// #8388608
  48:	cmp	w9, #0x95
  4c:	bfxil	w8, w11, #0, #23
  50:	sxtw	x10, w10
  54:	b.hi	70 <__fixsfti+0x70>  // b.pmore
  58:	mov	w11, #0x96                  	// #150
  5c:	sub	w9, w11, w9
  60:	lsr	w8, w8, w9
  64:	smulh	x1, x8, x10
  68:	mul	x0, x8, x10
  6c:	ret
  70:	sub	w9, w9, #0x96
  74:	neg	x12, x9
  78:	cmp	x9, #0x0
  7c:	lsl	x13, x8, x9
  80:	sub	x14, x9, #0x40
  84:	lsl	x9, x8, x9
  88:	lsr	x8, x8, x12
  8c:	csel	x8, xzr, x8, eq  // eq = none
  90:	cmp	x14, #0x0
  94:	csel	x9, xzr, x9, ge  // ge = tcont
  98:	asr	x11, x10, #63
  9c:	umulh	x12, x9, x10
  a0:	csel	x8, x13, x8, ge  // ge = tcont
  a4:	madd	x11, x9, x11, x12
  a8:	madd	x1, x8, x10, x11
  ac:	mul	x0, x9, x10
  b0:	ret

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	fcmp	d0, #0.0
   4:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fmov	d1, x8
  10:	fmul	d1, d0, d1
  14:	fcvtzu	w8, d1
  18:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  1c:	ucvtf	d1, w8
  20:	fmov	d2, x9
  24:	fmul	d1, d1, d2
  28:	fadd	d0, d0, d1
  2c:	fcvtzu	w0, d0
  30:	bfi	x0, x8, #32, #32
  34:	ret
  38:	mov	x0, xzr
  3c:	ret

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	mov	w0, wzr
   4:	fmov	x8, d0
   8:	tbnz	x8, #63, 3c <__fixunsdfsi+0x3c>
   c:	ubfx	x9, x8, #52, #11
  10:	subs	w10, w9, #0x3ff
  14:	b.cc	3c <__fixunsdfsi+0x3c>  // b.lo, b.ul, b.last
  18:	cmp	w10, #0x1f
  1c:	b.ls	28 <__fixunsdfsi+0x28>  // b.plast
  20:	mov	w0, #0xffffffff            	// #-1
  24:	ret
  28:	mov	x10, #0x10000000000000      	// #4503599627370496
  2c:	mov	w11, #0x433                 	// #1075
  30:	bfxil	x10, x8, #0, #52
  34:	sub	w8, w11, w9
  38:	lsr	x0, x10, x8
  3c:	ret

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	mov	x0, xzr
   4:	fmov	x10, d0
   8:	tbnz	x10, #63, 30 <__fixunsdfti+0x30>
   c:	ubfx	x9, x10, #52, #11
  10:	subs	w8, w9, #0x3ff
  14:	mov	x1, x0
  18:	b.cc	88 <__fixunsdfti+0x88>  // b.lo, b.ul, b.last
  1c:	cmp	w8, #0x7f
  20:	b.ls	38 <__fixunsdfti+0x38>  // b.plast
  24:	mov	x0, #0xffffffffffffffff    	// #-1
  28:	mov	x1, #0xffffffffffffffff    	// #-1
  2c:	ret
  30:	mov	x1, x0
  34:	ret
  38:	mov	x8, #0x10000000000000      	// #4503599627370496
  3c:	cmp	w9, #0x432
  40:	bfxil	x8, x10, #0, #52
  44:	b.hi	5c <__fixunsdfti+0x5c>  // b.pmore
  48:	mov	w10, #0x433                 	// #1075
  4c:	sub	w9, w10, w9
  50:	mov	x1, xzr
  54:	lsr	x0, x8, x9
  58:	ret
  5c:	sub	w9, w9, #0x433
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsl	x11, x8, x9
  6c:	sub	x12, x9, #0x40
  70:	lsl	x9, x8, x9
  74:	lsr	x8, x8, x10
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	cmp	x12, #0x0
  80:	csel	x1, x11, x8, ge  // ge = tcont
  84:	csel	x0, xzr, x9, ge  // ge = tcont
  88:	ret

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	fcmp	s0, #0.0
   4:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   8:	mov	x8, #0x3df0000000000000    	// #4463067230724161536
   c:	fcvt	d0, s0
  10:	fmov	d1, x8
  14:	fmul	d1, d0, d1
  18:	fcvtzu	w8, d1
  1c:	mov	x9, #0xc1f0000000000000    	// #-4472074429978902528
  20:	ucvtf	d1, w8
  24:	fmov	d2, x9
  28:	fmul	d1, d1, d2
  2c:	fadd	d0, d0, d1
  30:	fcvtzu	w0, d0
  34:	bfi	x0, x8, #32, #32
  38:	ret
  3c:	mov	x0, xzr
  40:	ret

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	mov	w0, wzr
   4:	fmov	w9, s0
   8:	tbnz	w9, #31, 50 <__fixunssfsi+0x50>
   c:	ubfx	w8, w9, #23, #8
  10:	subs	w10, w8, #0x7f
  14:	b.cc	50 <__fixunssfsi+0x50>  // b.lo, b.ul, b.last
  18:	cmp	w10, #0x1f
  1c:	b.ls	28 <__fixunssfsi+0x28>  // b.plast
  20:	mov	w0, #0xffffffff            	// #-1
  24:	ret
  28:	mov	w10, #0x800000              	// #8388608
  2c:	cmp	w8, #0x95
  30:	bfxil	w10, w9, #0, #23
  34:	b.hi	48 <__fixunssfsi+0x48>  // b.pmore
  38:	mov	w9, #0x96                  	// #150
  3c:	sub	w8, w9, w8
  40:	lsr	w0, w10, w8
  44:	ret
  48:	sub	w8, w8, #0x96
  4c:	lsl	w0, w10, w8
  50:	ret

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	mov	x0, xzr
   4:	fmov	w10, s0
   8:	tbnz	w10, #31, 30 <__fixunssfti+0x30>
   c:	ubfx	w9, w10, #23, #8
  10:	subs	w8, w9, #0x7f
  14:	mov	x1, x0
  18:	b.cc	88 <__fixunssfti+0x88>  // b.lo, b.ul, b.last
  1c:	cmp	w8, #0x7f
  20:	b.ls	38 <__fixunssfti+0x38>  // b.plast
  24:	mov	x0, #0xffffffffffffffff    	// #-1
  28:	mov	x1, #0xffffffffffffffff    	// #-1
  2c:	ret
  30:	mov	x1, x0
  34:	ret
  38:	mov	w8, #0x800000              	// #8388608
  3c:	cmp	w9, #0x95
  40:	bfxil	w8, w10, #0, #23
  44:	b.hi	5c <__fixunssfti+0x5c>  // b.pmore
  48:	mov	w10, #0x96                  	// #150
  4c:	sub	w9, w10, w9
  50:	mov	x1, xzr
  54:	lsr	w0, w8, w9
  58:	ret
  5c:	sub	w9, w9, #0x96
  60:	neg	x10, x9
  64:	cmp	x9, #0x0
  68:	lsl	x11, x8, x9
  6c:	sub	x12, x9, #0x40
  70:	lsl	x9, x8, x9
  74:	lsr	x8, x8, x10
  78:	csel	x8, xzr, x8, eq  // eq = none
  7c:	cmp	x12, #0x0
  80:	csel	x1, x11, x8, ge  // ge = tcont
  84:	csel	x0, xzr, x9, ge  // ge = tcont
  88:	ret

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	lsr	x8, x0, #32
   4:	mov	x9, #0x41f0000000000000    	// #4751297606875873280
   8:	mov	x10, #0x4330000000000000    	// #4841369599423283200
   c:	mov	x11, #0xc330000000000000    	// #-4382002437431492608
  10:	scvtf	d0, w8
  14:	fmov	d1, x9
  18:	fmul	d0, d0, d1
  1c:	bfxil	x10, x0, #0, #32
  20:	fmov	d1, x11
  24:	fadd	d0, d0, d1
  28:	fmov	d1, x10
  2c:	fadd	d0, d0, d1
  30:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	cbz	x0, 44 <__floatdisf+0x44>
   4:	asr	x8, x0, #63
   8:	eor	x9, x8, x0
   c:	sub	x9, x9, x8
  10:	clz	x11, x9
  14:	mov	w8, #0x3f                  	// #63
  18:	cmp	w11, #0x27
  1c:	sub	w8, w8, w11
  20:	b.hi	4c <__floatdisf+0x4c>  // b.pmore
  24:	mov	w10, #0x40                  	// #64
  28:	sub	w10, w10, w11
  2c:	cmp	w10, #0x1a
  30:	b.eq	74 <__floatdisf+0x74>  // b.none
  34:	cmp	w10, #0x19
  38:	b.ne	58 <__floatdisf+0x58>  // b.any
  3c:	lsl	x9, x9, #1
  40:	b	74 <__floatdisf+0x74>
  44:	fmov	s0, wzr
  48:	ret
  4c:	sub	w10, w11, #0x28
  50:	lsl	x9, x9, x10
  54:	b	94 <__floatdisf+0x94>
  58:	mov	w12, #0x26                  	// #38
  5c:	lsl	x13, x9, x11
  60:	sub	w11, w12, w11
  64:	tst	x13, #0x3fffffffff
  68:	lsr	x9, x9, x11
  6c:	cset	w11, ne  // ne = any
  70:	orr	x9, x9, x11
  74:	ubfx	x11, x9, #2, #1
  78:	orr	x9, x11, x9
  7c:	add	x9, x9, #0x1
  80:	mov	w12, #0x2                   	// #2
  84:	tst	x9, #0x4000000
  88:	cinc	x11, x12, ne  // ne = any
  8c:	asr	x9, x9, x11
  90:	csel	w8, w8, w10, eq  // eq = none
  94:	lsr	x10, x0, #32
  98:	mov	w11, #0x3f800000            	// #1065353216
  9c:	and	w10, w10, #0x80000000
  a0:	add	w8, w11, w8, lsl #23
  a4:	bfxil	w10, w9, #0, #23
  a8:	orr	w8, w10, w8
  ac:	fmov	s0, w8
  b0:	ret

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	cbz	w0, 38 <__floatsidf+0x38>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	clz	w11, w10
  10:	add	w12, w11, #0x15
  14:	mov	w9, #0x41e                 	// #1054
  18:	lsl	x10, x10, x12
  1c:	eor	x10, x10, #0x10000000000000
  20:	sub	w9, w9, w11
  24:	and	w8, w0, #0x80000000
  28:	add	x9, x10, x9, lsl #52
  2c:	orr	x8, x9, x8, lsl #32
  30:	fmov	d0, x8
  34:	ret
  38:	fmov	d0, xzr
  3c:	ret

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	cbz	w0, 2c <__floatsisf+0x2c>
   4:	cmp	w0, #0x0
   8:	cneg	w10, w0, mi  // mi = first
   c:	and	w8, w0, #0x80000000
  10:	lsr	w11, w10, #24
  14:	clz	w9, w10
  18:	cbnz	w11, 34 <__floatsisf+0x34>
  1c:	sub	w11, w9, #0x8
  20:	lsl	w10, w10, w11
  24:	eor	w10, w10, #0x800000
  28:	b	64 <__floatsisf+0x64>
  2c:	fmov	s0, wzr
  30:	ret
  34:	mov	w11, #0x8                   	// #8
  38:	add	w12, w9, #0x18
  3c:	sub	w11, w11, w9
  40:	mov	w13, #0x80000000            	// #-2147483648
  44:	lsl	w12, w10, w12
  48:	lsr	w10, w10, w11
  4c:	eor	w10, w10, #0x800000
  50:	cmp	w12, w13
  54:	cinc	w10, w10, hi  // hi = pmore
  58:	and	w11, w10, #0x1
  5c:	csel	w11, w11, wzr, eq  // eq = none
  60:	add	w10, w11, w10
  64:	sub	w9, w10, w9, lsl #23
  68:	mov	w10, #0x4f000000            	// #1325400064
  6c:	add	w9, w9, w10
  70:	orr	w8, w9, w8
  74:	fmov	s0, w8
  78:	ret

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattidf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	asr	x8, x1, #63
  1c:	eor	x10, x8, x0
  20:	eor	x9, x8, x1
  24:	subs	x20, x10, x8
  28:	sbcs	x21, x9, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x36
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattidf+0x78>  // b.tstop
  54:	cmp	w9, #0x37
  58:	b.eq	10c <__floattidf+0x10c>  // b.none
  5c:	cmp	w9, #0x36
  60:	b.ne	90 <__floattidf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattidf+0x10c>
  70:	fmov	d0, xzr
  74:	ret
  78:	sub	w9, w0, #0x4b
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	124 <__floattidf+0x124>
  90:	mov	w10, #0x49                  	// #73
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x37
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	x10, #55, 12c <__floattidf+0x12c>
 120:	extr	x10, x11, x10, #2
 124:	lsr	x11, x10, #32
 128:	b	138 <__floattidf+0x138>
 12c:	extr	x10, x11, x10, #3
 130:	lsr	x11, x10, #32
 134:	mov	w8, w9
 138:	lsr	x9, x19, #32
 13c:	mov	w12, #0x3ff00000            	// #1072693248
 140:	and	w9, w9, #0x80000000
 144:	add	w8, w12, w8, lsl #20
 148:	bfxil	w9, w11, #0, #20
 14c:	ldp	x20, x19, [sp, #32]
 150:	ldr	x21, [sp, #16]
 154:	orr	w8, w9, w8
 158:	bfi	x10, x8, #32, #32
 15c:	fmov	d0, x10
 160:	ldp	x29, x30, [sp], #48
 164:	ret

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 70 <__floattisf+0x70>
   8:	stp	x29, x30, [sp, #-48]!
   c:	str	x21, [sp, #16]
  10:	stp	x20, x19, [sp, #32]
  14:	mov	x29, sp
  18:	asr	x8, x1, #63
  1c:	eor	x10, x8, x0
  20:	eor	x9, x8, x1
  24:	subs	x20, x10, x8
  28:	sbcs	x21, x9, x8
  2c:	mov	x19, x1
  30:	mov	x0, x20
  34:	mov	x1, x21
  38:	bl	0 <__clzti2>
  3c:	mov	w8, #0x80                  	// #128
  40:	sub	w9, w8, w0
  44:	mov	w8, #0x7f                  	// #127
  48:	cmp	w9, #0x19
  4c:	sub	w8, w8, w0
  50:	b.lt	78 <__floattisf+0x78>  // b.tstop
  54:	cmp	w9, #0x1a
  58:	b.eq	10c <__floattisf+0x10c>  // b.none
  5c:	cmp	w9, #0x19
  60:	b.ne	90 <__floattisf+0x90>  // b.any
  64:	extr	x21, x21, x20, #63
  68:	lsl	x20, x20, #1
  6c:	b	10c <__floattisf+0x10c>
  70:	fmov	s0, wzr
  74:	ret
  78:	sub	w9, w0, #0x68
  7c:	lsl	x10, x20, x9
  80:	sub	x9, x9, #0x40
  84:	cmp	x9, #0x0
  88:	csel	x10, xzr, x10, ge  // ge = tcont
  8c:	b	134 <__floattisf+0x134>
  90:	mov	w10, #0x66                  	// #102
  94:	sub	w10, w10, w0
  98:	neg	x13, x10
  9c:	cmp	x10, #0x0
  a0:	sub	x14, x10, #0x40
  a4:	lsl	x13, x21, x13
  a8:	add	w11, w0, #0x1a
  ac:	csel	x13, xzr, x13, eq  // eq = none
  b0:	cmp	x14, #0x0
  b4:	lsr	x14, x20, x10
  b8:	neg	x12, x11
  bc:	orr	x13, x14, x13
  c0:	lsr	x14, x21, x10
  c4:	lsr	x10, x21, x10
  c8:	lsl	x15, x21, x11
  cc:	csel	x10, x10, x13, ge  // ge = tcont
  d0:	lsr	x12, x20, x12
  d4:	csel	x21, xzr, x14, ge  // ge = tcont
  d8:	cmp	x11, #0x0
  dc:	lsl	x13, x20, x11
  e0:	lsl	x16, x20, x11
  e4:	sub	x11, x11, #0x40
  e8:	csel	x12, xzr, x12, eq  // eq = none
  ec:	cmp	x11, #0x0
  f0:	orr	x11, x12, x15
  f4:	csel	x11, x13, x11, ge  // ge = tcont
  f8:	csel	x12, xzr, x16, ge  // ge = tcont
  fc:	orr	x11, x12, x11
 100:	cmp	x11, #0x0
 104:	cset	w11, ne  // ne = any
 108:	orr	x20, x10, x11
 10c:	ubfx	x10, x20, #2, #1
 110:	orr	x10, x10, x20
 114:	adds	x10, x10, #0x1
 118:	adcs	x11, x21, xzr
 11c:	tbnz	w10, #26, 128 <__floattisf+0x128>
 120:	lsr	x10, x10, #2
 124:	b	134 <__floattisf+0x134>
 128:	extr	x8, x11, x10, #2
 12c:	lsr	x10, x8, #1
 130:	mov	w8, w9
 134:	lsr	x9, x19, #32
 138:	mov	w11, #0x3f800000            	// #1065353216
 13c:	and	w9, w9, #0x80000000
 140:	ldp	x20, x19, [sp, #32]
 144:	ldr	x21, [sp, #16]
 148:	add	w8, w11, w8, lsl #23
 14c:	bfxil	w9, w10, #0, #23
 150:	orr	w8, w9, w8
 154:	fmov	s0, w8
 158:	ldp	x29, x30, [sp], #48
 15c:	ret

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	mov	x8, #0x4530000000000000    	// #4985484787499139072
   4:	mov	x10, #0x100000              	// #1048576
   8:	mov	x9, #0x4330000000000000    	// #4841369599423283200
   c:	movk	x10, #0xc530, lsl #48
  10:	bfxil	x8, x0, #32, #32
  14:	bfxil	x9, x0, #0, #32
  18:	fmov	d0, x10
  1c:	fmov	d1, x8
  20:	fadd	d0, d1, d0
  24:	fmov	d1, x9
  28:	fadd	d0, d0, d1
  2c:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	cbz	x0, 34 <__floatundisf+0x34>
   4:	clz	x10, x0
   8:	cmp	w10, #0x27
   c:	eor	w8, w10, #0x3f
  10:	b.hi	3c <__floatundisf+0x3c>  // b.pmore
  14:	mov	w9, #0x40                  	// #64
  18:	sub	w9, w9, w10
  1c:	cmp	w9, #0x1a
  20:	b.eq	64 <__floatundisf+0x64>  // b.none
  24:	cmp	w9, #0x19
  28:	b.ne	48 <__floatundisf+0x48>  // b.any
  2c:	lsl	x0, x0, #1
  30:	b	64 <__floatundisf+0x64>
  34:	fmov	s0, wzr
  38:	ret
  3c:	sub	w9, w10, #0x28
  40:	lsl	x10, x0, x9
  44:	b	84 <__floatundisf+0x84>
  48:	mov	w11, #0x26                  	// #38
  4c:	lsl	x12, x0, x10
  50:	sub	w10, w11, w10
  54:	tst	x12, #0x3fffffffff
  58:	lsr	x10, x0, x10
  5c:	cset	w11, ne  // ne = any
  60:	orr	x0, x10, x11
  64:	ubfx	x10, x0, #2, #1
  68:	orr	x10, x10, x0
  6c:	add	x10, x10, #0x1
  70:	mov	w11, #0x2                   	// #2
  74:	tst	x10, #0x4000000
  78:	cinc	x11, x11, ne  // ne = any
  7c:	lsr	x10, x10, x11
  80:	csel	w8, w8, w9, eq  // eq = none
  84:	bfi	w10, w8, #23, #9
  88:	mov	w8, #0x3f800000            	// #1065353216
  8c:	add	w8, w10, w8
  90:	fmov	s0, w8
  94:	ret

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	cbz	w0, 30 <__floatunsidf+0x30>
   4:	clz	w8, w0
   8:	mov	w9, #0x34                  	// #52
   c:	eor	w8, w8, #0x1f
  10:	mov	w10, w0
  14:	sub	w9, w9, w8
  18:	lsl	x9, x10, x9
  1c:	eor	x9, x9, #0x10000000000000
  20:	add	w8, w8, #0x3ff
  24:	add	x8, x9, x8, lsl #52
  28:	fmov	d0, x8
  2c:	ret
  30:	fmov	d0, xzr
  34:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	cbz	w0, 28 <__floatunsisf+0x28>
   4:	clz	w8, w0
   8:	eor	w8, w8, #0x1f
   c:	subs	w9, w8, #0x17
  10:	b.hi	30 <__floatunsisf+0x30>  // b.pmore
  14:	mov	w9, #0x17                  	// #23
  18:	sub	w9, w9, w8
  1c:	lsl	w9, w0, w9
  20:	eor	w9, w9, #0x800000
  24:	b	5c <__floatunsisf+0x5c>
  28:	fmov	s0, wzr
  2c:	ret
  30:	mov	w10, #0x37                  	// #55
  34:	sub	w10, w10, w8
  38:	lsr	w9, w0, w9
  3c:	lsl	w10, w0, w10
  40:	mov	w11, #0x80000000            	// #-2147483648
  44:	eor	w9, w9, #0x800000
  48:	cmp	w10, w11
  4c:	cinc	w9, w9, hi  // hi = pmore
  50:	and	w10, w9, #0x1
  54:	csel	w10, w10, wzr, eq  // eq = none
  58:	add	w9, w10, w9
  5c:	add	w8, w9, w8, lsl #23
  60:	mov	w9, #0x3f800000            	// #1065353216
  64:	add	w8, w8, w9
  68:	fmov	s0, w8
  6c:	ret

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 54 <__floatuntidf+0x54>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x36
  30:	sub	w8, w8, w0
  34:	b.lt	5c <__floatuntidf+0x5c>  // b.tstop
  38:	cmp	w9, #0x37
  3c:	b.eq	f4 <__floatuntidf+0xf4>  // b.none
  40:	cmp	w9, #0x36
  44:	b.ne	78 <__floatuntidf+0x78>  // b.any
  48:	extr	x20, x20, x19, #63
  4c:	lsl	x19, x19, #1
  50:	b	f4 <__floatuntidf+0xf4>
  54:	fmov	d0, xzr
  58:	ret
  5c:	sub	w9, w0, #0x4b
  60:	lsl	x10, x19, x9
  64:	sub	x9, x9, #0x40
  68:	cmp	x9, #0x0
  6c:	csel	x10, xzr, x10, ge  // ge = tcont
  70:	lsr	x11, x10, #32
  74:	b	128 <__floatuntidf+0x128>
  78:	mov	w10, #0x49                  	// #73
  7c:	sub	w10, w10, w0
  80:	neg	x13, x10
  84:	cmp	x10, #0x0
  88:	sub	x14, x10, #0x40
  8c:	lsl	x13, x20, x13
  90:	add	w11, w0, #0x37
  94:	csel	x13, xzr, x13, eq  // eq = none
  98:	cmp	x14, #0x0
  9c:	lsr	x14, x19, x10
  a0:	neg	x12, x11
  a4:	orr	x13, x14, x13
  a8:	lsr	x14, x20, x10
  ac:	lsr	x10, x20, x10
  b0:	lsl	x15, x20, x11
  b4:	csel	x10, x10, x13, ge  // ge = tcont
  b8:	lsr	x12, x19, x12
  bc:	csel	x20, xzr, x14, ge  // ge = tcont
  c0:	cmp	x11, #0x0
  c4:	lsl	x13, x19, x11
  c8:	lsl	x16, x19, x11
  cc:	sub	x11, x11, #0x40
  d0:	csel	x12, xzr, x12, eq  // eq = none
  d4:	cmp	x11, #0x0
  d8:	orr	x11, x12, x15
  dc:	csel	x11, x13, x11, ge  // ge = tcont
  e0:	csel	x12, xzr, x16, ge  // ge = tcont
  e4:	orr	x11, x12, x11
  e8:	cmp	x11, #0x0
  ec:	cset	w11, ne  // ne = any
  f0:	orr	x19, x10, x11
  f4:	ubfx	x10, x19, #2, #1
  f8:	orr	x10, x10, x19
  fc:	adds	x11, x10, #0x1
 100:	adcs	x12, x20, xzr
 104:	tbnz	x11, #55, 118 <__floatuntidf+0x118>
 108:	extr	x10, x12, x11, #2
 10c:	lsr	x11, x11, #34
 110:	bfi	w11, w12, #30, #2
 114:	b	128 <__floatuntidf+0x128>
 118:	extr	x10, x12, x11, #3
 11c:	lsr	x11, x11, #35
 120:	bfi	w11, w12, #29, #3
 124:	mov	w8, w9
 128:	bfi	w11, w8, #20, #12
 12c:	mov	w8, #0x3ff00000            	// #1072693248
 130:	ldp	x20, x19, [sp, #16]
 134:	add	w8, w11, w8
 138:	bfi	x10, x8, #32, #32
 13c:	fmov	d0, x10
 140:	ldp	x29, x30, [sp], #32
 144:	ret

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	orr	x8, x0, x1
   4:	cbz	x8, 64 <__floatuntisf+0x64>
   8:	stp	x29, x30, [sp, #-32]!
   c:	stp	x20, x19, [sp, #16]
  10:	mov	x29, sp
  14:	mov	x20, x1
  18:	mov	x19, x0
  1c:	bl	0 <__clzti2>
  20:	mov	w8, #0x80                  	// #128
  24:	sub	w9, w8, w0
  28:	mov	w8, #0x7f                  	// #127
  2c:	cmp	w9, #0x19
  30:	sub	w8, w8, w0
  34:	b.lt	6c <__floatuntisf+0x6c>  // b.tstop
  38:	cmp	w9, #0x1a
  3c:	b.eq	4c <__floatuntisf+0x4c>  // b.none
  40:	cmp	w9, #0x19
  44:	b.ne	84 <__floatuntisf+0x84>  // b.any
  48:	lsl	x19, x19, #1
  4c:	ubfx	x10, x19, #2, #1
  50:	orr	x10, x10, x19
  54:	adds	x10, x10, #0x1
  58:	tbnz	w10, #26, 108 <__floatuntisf+0x108>
  5c:	lsr	x10, x10, #2
  60:	b	110 <__floatuntisf+0x110>
  64:	fmov	s0, wzr
  68:	ret
  6c:	sub	w9, w0, #0x68
  70:	lsl	x10, x19, x9
  74:	sub	x9, x9, #0x40
  78:	cmp	x9, #0x0
  7c:	csel	x10, xzr, x10, ge  // ge = tcont
  80:	b	110 <__floatuntisf+0x110>
  84:	mov	w10, #0x66                  	// #102
  88:	sub	w10, w10, w0
  8c:	neg	x12, x10
  90:	cmp	x10, #0x0
  94:	sub	x13, x10, #0x40
  98:	lsl	x12, x20, x12
  9c:	add	w11, w0, #0x1a
  a0:	csel	x12, xzr, x12, eq  // eq = none
  a4:	cmp	x13, #0x0
  a8:	lsr	x13, x19, x10
  ac:	orr	x12, x13, x12
  b0:	neg	x13, x11
  b4:	lsr	x10, x20, x10
  b8:	csel	x10, x10, x12, ge  // ge = tcont
  bc:	lsr	x13, x19, x13
  c0:	cmp	x11, #0x0
  c4:	lsl	x14, x20, x11
  c8:	lsl	x12, x19, x11
  cc:	lsl	x15, x19, x11
  d0:	sub	x11, x11, #0x40
  d4:	csel	x13, xzr, x13, eq  // eq = none
  d8:	cmp	x11, #0x0
  dc:	orr	x11, x13, x14
  e0:	csel	x11, x12, x11, ge  // ge = tcont
  e4:	csel	x12, xzr, x15, ge  // ge = tcont
  e8:	orr	x11, x12, x11
  ec:	cmp	x11, #0x0
  f0:	cset	w11, ne  // ne = any
  f4:	orr	x19, x10, x11
  f8:	ubfx	x10, x19, #2, #1
  fc:	orr	x10, x10, x19
 100:	adds	x10, x10, #0x1
 104:	tbz	w10, #26, 5c <__floatuntisf+0x5c>
 108:	lsr	x10, x10, #3
 10c:	mov	w8, w9
 110:	ldp	x20, x19, [sp, #16]
 114:	bfi	w10, w8, #23, #9
 118:	mov	w8, #0x3f800000            	// #1065353216
 11c:	add	w8, w10, w8
 120:	fmov	s0, w8
 124:	ldp	x29, x30, [sp], #32
 128:	ret

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	lsr	x8, x0, #32
   4:	tbnz	w1, #5, 28 <__lshrdi3+0x28>
   8:	cbz	w1, 38 <__lshrdi3+0x38>
   c:	neg	w10, w1
  10:	lsr	w9, w8, w1
  14:	lsr	w11, w0, w1
  18:	lsl	w8, w8, w10
  1c:	orr	w8, w8, w11
  20:	lsl	x9, x9, #32
  24:	b	30 <__lshrdi3+0x30>
  28:	mov	x9, xzr
  2c:	lsr	w8, w8, w1
  30:	mov	w8, w8
  34:	orr	x0, x9, x8
  38:	ret

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbnz	w2, #6, 24 <__lshrti3+0x24>
   4:	cbz	w2, 34 <__lshrti3+0x34>
   8:	neg	w9, w2
   c:	lsl	x9, x1, x9
  10:	lsr	x10, x0, x2
  14:	mov	x8, xzr
  18:	lsr	x1, x1, x2
  1c:	orr	x9, x9, x10
  20:	b	30 <__lshrti3+0x30>
  24:	mov	x8, xzr
  28:	lsr	x9, x1, x2
  2c:	mov	x1, xzr
  30:	orr	x0, x8, x9
  34:	ret

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	asr	x19, x0, #63
  10:	cmp	x1, #0x0
  14:	eor	x8, x19, x0
  18:	cneg	x1, x1, mi  // mi = first
  1c:	sub	x0, x8, x19
  20:	add	x2, x29, #0x18
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x8, [x29, #24]
  2c:	eor	x8, x8, x19
  30:	sub	x0, x8, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #32
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	sub	sp, sp, #0x30
   4:	stp	x29, x30, [sp, #16]
   8:	str	x19, [sp, #32]
   c:	add	x29, sp, #0x10
  10:	negs	x8, x2
  14:	asr	x19, x1, #63
  18:	ngcs	x9, x3
  1c:	eor	x11, x19, x0
  20:	cmp	x3, #0x0
  24:	eor	x10, x19, x1
  28:	csel	x2, x8, x2, lt  // lt = tstop
  2c:	csel	x3, x9, x3, lt  // lt = tstop
  30:	subs	x0, x11, x19
  34:	sbcs	x1, x10, x19
  38:	mov	x4, sp
  3c:	bl	0 <__udivmodti4>
  40:	ldp	x9, x8, [sp]
  44:	ldp	x29, x30, [sp, #16]
  48:	eor	x9, x9, x19
  4c:	eor	x8, x8, x19
  50:	subs	x0, x9, x19
  54:	sbcs	x1, x8, x19
  58:	ldr	x19, [sp, #32]
  5c:	add	sp, sp, #0x30
  60:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d7, d0, d2
   4:	fmul	d6, d1, d3
   8:	fmul	d16, d0, d3
   c:	fmul	d17, d1, d2
  10:	fsub	d4, d7, d6
  14:	fcmp	d4, d4
  18:	fadd	d5, d17, d16
  1c:	b.vc	12c <__muldc3+0x12c>
  20:	fcmp	d5, d5
  24:	b.vc	12c <__muldc3+0x12c>
  28:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  2c:	fabs	d19, d0
  30:	fmov	d18, x8
  34:	fcmp	d19, d18
  38:	fabs	d18, d1
  3c:	b.eq	58 <__muldc3+0x58>  // b.none
  40:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  44:	fmov	d20, x8
  48:	fcmp	d18, d20
  4c:	b.eq	58 <__muldc3+0x58>  // b.none
  50:	mov	w8, wzr
  54:	b	b0 <__muldc3+0xb0>
  58:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  5c:	fmov	d23, x8
  60:	fmov	d20, xzr
  64:	fmov	d21, #1.000000000000000000e+00
  68:	movi	v22.2d, #0x0
  6c:	fcmp	d19, d23
  70:	fcsel	d19, d21, d20, eq  // eq = none
  74:	fcmp	d18, d23
  78:	fmov	d18, xzr
  7c:	fneg	v22.2d, v22.2d
  80:	bit	v18.16b, v2.16b, v22.16b
  84:	bit	v19.16b, v0.16b, v22.16b
  88:	fcsel	d0, d21, d20, eq  // eq = none
  8c:	fcmp	d2, d2
  90:	bit	v20.16b, v3.16b, v22.16b
  94:	bit	v0.16b, v1.16b, v22.16b
  98:	fcsel	d2, d18, d2, vs
  9c:	fcmp	d3, d3
  a0:	fcsel	d3, d20, d3, vs
  a4:	mov	w8, #0x1                   	// #1
  a8:	mov	v1.16b, v0.16b
  ac:	mov	v0.16b, v19.16b
  b0:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  b4:	fabs	d18, d3
  b8:	fmov	d19, x9
  bc:	fcmp	d18, d19
  c0:	fabs	d19, d2
  c4:	b.eq	138 <__muldc3+0x138>  // b.none
  c8:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  cc:	fmov	d20, x9
  d0:	fcmp	d19, d20
  d4:	b.eq	138 <__muldc3+0x138>  // b.none
  d8:	cbnz	w8, 18c <__muldc3+0x18c>
  dc:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  e0:	fabs	d17, d17
  e4:	fmov	d18, x8
  e8:	fcmp	d17, d18
  ec:	b.eq	1c0 <__muldc3+0x1c0>  // b.none
  f0:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  f4:	fabs	d16, d16
  f8:	fmov	d17, x8
  fc:	fcmp	d16, d17
 100:	b.eq	1c0 <__muldc3+0x1c0>  // b.none
 104:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 108:	fabs	d7, d7
 10c:	fmov	d16, x8
 110:	fcmp	d7, d16
 114:	b.eq	1c0 <__muldc3+0x1c0>  // b.none
 118:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 11c:	fabs	d6, d6
 120:	fmov	d7, x8
 124:	fcmp	d6, d7
 128:	b.eq	1c0 <__muldc3+0x1c0>  // b.none
 12c:	mov	v0.16b, v4.16b
 130:	mov	v1.16b, v5.16b
 134:	ret
 138:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 13c:	fmov	d7, x8
 140:	fmov	d4, xzr
 144:	fmov	d5, #1.000000000000000000e+00
 148:	movi	v6.2d, #0x0
 14c:	fcmp	d19, d7
 150:	fcsel	d16, d5, d4, eq  // eq = none
 154:	fcmp	d18, d7
 158:	fmov	d7, xzr
 15c:	fneg	v6.2d, v6.2d
 160:	bit	v7.16b, v0.16b, v6.16b
 164:	bit	v16.16b, v2.16b, v6.16b
 168:	fcsel	d2, d5, d4, eq  // eq = none
 16c:	fcmp	d0, d0
 170:	bit	v4.16b, v1.16b, v6.16b
 174:	bit	v2.16b, v3.16b, v6.16b
 178:	fcsel	d0, d7, d0, vs
 17c:	fcmp	d1, d1
 180:	fcsel	d1, d4, d1, vs
 184:	mov	v3.16b, v2.16b
 188:	mov	v2.16b, v16.16b
 18c:	fmul	d4, d2, d0
 190:	fmul	d5, d3, d1
 194:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
 198:	fmul	d0, d3, d0
 19c:	fmul	d1, d2, d1
 1a0:	fsub	d2, d4, d5
 1a4:	fmov	d3, x8
 1a8:	fadd	d0, d1, d0
 1ac:	fmul	d4, d2, d3
 1b0:	fmul	d5, d0, d3
 1b4:	mov	v0.16b, v4.16b
 1b8:	mov	v1.16b, v5.16b
 1bc:	ret
 1c0:	movi	v5.2d, #0x0
 1c4:	fmov	d6, xzr
 1c8:	fneg	v5.2d, v5.2d
 1cc:	fmov	d7, xzr
 1d0:	bit	v6.16b, v0.16b, v5.16b
 1d4:	fcmp	d0, d0
 1d8:	fmov	d16, xzr
 1dc:	bit	v7.16b, v1.16b, v5.16b
 1e0:	fcsel	d0, d6, d0, vs
 1e4:	fcmp	d1, d1
 1e8:	fmov	d4, xzr
 1ec:	bit	v16.16b, v2.16b, v5.16b
 1f0:	fcsel	d1, d7, d1, vs
 1f4:	fcmp	d2, d2
 1f8:	bit	v4.16b, v3.16b, v5.16b
 1fc:	fcsel	d2, d16, d2, vs
 200:	fcmp	d3, d3
 204:	fcsel	d3, d4, d3, vs
 208:	b	18c <__muldc3+0x18c>

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	fmov	x14, d0
   4:	ubfx	x9, x14, #52, #11
   8:	fmov	x13, d1
   c:	eor	x8, x13, x14
  10:	sub	w11, w9, #0x1
  14:	ubfx	x10, x13, #52, #11
  18:	and	x12, x14, #0xfffffffffffff
  1c:	and	x8, x8, #0x8000000000000000
  20:	cmp	w11, #0x7fd
  24:	and	x11, x13, #0xfffffffffffff
  28:	b.hi	b0 <__muldf3+0xb0>  // b.pmore
  2c:	sub	w15, w10, #0x1
  30:	cmp	w15, #0x7fe
  34:	b.cs	b0 <__muldf3+0xb0>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	and	x15, x12, #0xffffffff
  40:	lsr	x12, x12, #32
  44:	lsl	w14, w11, #11
  48:	mov	w16, #0x80000000            	// #-2147483648
  4c:	add	w17, w9, w10
  50:	orr	x10, x12, #0x100000
  54:	bfxil	x16, x11, #21, #31
  58:	mul	x12, x14, x10
  5c:	mul	x9, x14, x15
  60:	mul	x11, x16, x15
  64:	mul	x14, x16, x10
  68:	and	x15, x12, #0xfffff800
  6c:	and	x10, x9, #0xfffff800
  70:	add	x12, x14, x12, lsr #32
  74:	add	x9, x15, x9, lsr #32
  78:	add	x14, x12, x11, lsr #32
  7c:	add	x12, x9, w11, uxtw
  80:	add	w11, w17, w13
  84:	bfi	x10, x12, #32, #32
  88:	add	x9, x14, x12, lsr #32
  8c:	sub	w11, w11, #0x3ff
  90:	tbnz	x9, #52, d0 <__muldf3+0xd0>
  94:	lsr	x12, x12, #31
  98:	bfi	x12, x9, #1, #63
  9c:	lsl	x10, x10, #1
  a0:	mov	x9, x12
  a4:	cmp	w11, #0x7ff
  a8:	b.lt	dc <__muldf3+0xdc>  // b.tstop
  ac:	b	12c <__muldf3+0x12c>
  b0:	mov	x16, #0x1                   	// #1
  b4:	and	x15, x14, #0x7fffffffffffffff
  b8:	movk	x16, #0x7ff0, lsl #48
  bc:	cmp	x15, x16
  c0:	b.cc	104 <__muldf3+0x104>  // b.lo, b.ul, b.last
  c4:	orr	x8, x14, #0x8000000000000
  c8:	fmov	d0, x8
  cc:	ret
  d0:	add	w11, w11, #0x1
  d4:	cmp	w11, #0x7ff
  d8:	b.ge	12c <__muldf3+0x12c>  // b.tcont
  dc:	cmp	w11, #0x0
  e0:	b.le	138 <__muldf3+0x138>
  e4:	bfi	x9, x11, #52, #12
  e8:	mov	x11, #0x8000000000000001    	// #-9223372036854775807
  ec:	cmp	x10, x11
  f0:	orr	x8, x9, x8
  f4:	b.cc	17c <__muldf3+0x17c>  // b.lo, b.ul, b.last
  f8:	add	x8, x8, #0x1
  fc:	fmov	d0, x8
 100:	ret
 104:	and	x14, x13, #0x7fffffffffffffff
 108:	cmp	x14, x16
 10c:	b.cc	11c <__muldf3+0x11c>  // b.lo, b.ul, b.last
 110:	orr	x8, x13, #0x8000000000000
 114:	fmov	d0, x8
 118:	ret
 11c:	mov	x13, #0x7ff0000000000000    	// #9218868437227405312
 120:	cmp	x15, x13
 124:	b.ne	198 <__muldf3+0x198>  // b.any
 128:	cbz	x14, 1a4 <__muldf3+0x1a4>
 12c:	orr	x8, x8, #0x7ff0000000000000
 130:	fmov	d0, x8
 134:	ret
 138:	mov	w12, #0x1                   	// #1
 13c:	sub	w11, w12, w11
 140:	cmp	w11, #0x3f
 144:	b.hi	190 <__muldf3+0x190>  // b.pmore
 148:	neg	x12, x11
 14c:	lsr	x13, x10, x11
 150:	lsl	x10, x10, x12
 154:	lsl	x12, x9, x12
 158:	cmp	x10, #0x0
 15c:	orr	x10, x12, x13
 160:	cset	w12, ne  // ne = any
 164:	orr	x10, x10, x12
 168:	lsr	x9, x9, x11
 16c:	mov	x11, #0x8000000000000001    	// #-9223372036854775807
 170:	cmp	x10, x11
 174:	orr	x8, x9, x8
 178:	b.cs	f8 <__muldf3+0xf8>  // b.hs, b.nlast
 17c:	mov	x11, #0x8000000000000000    	// #-9223372036854775808
 180:	cmp	x10, x11
 184:	b.ne	190 <__muldf3+0x190>  // b.any
 188:	and	x9, x9, #0x1
 18c:	add	x8, x8, x9
 190:	fmov	d0, x8
 194:	ret
 198:	cmp	x14, x13
 19c:	b.ne	1b0 <__muldf3+0x1b0>  // b.any
 1a0:	cbnz	x15, 12c <__muldf3+0x12c>
 1a4:	mov	x8, #0x7ff8000000000000    	// #9221120237041090560
 1a8:	fmov	d0, x8
 1ac:	ret
 1b0:	cbz	x15, 190 <__muldf3+0x190>
 1b4:	cbz	x14, 190 <__muldf3+0x190>
 1b8:	lsr	x13, x15, #52
 1bc:	cbnz	x13, 1dc <__muldf3+0x1dc>
 1c0:	clz	x13, x12
 1c4:	mov	w15, #0xfffffff5            	// #-11
 1c8:	mov	w16, #0xc                   	// #12
 1cc:	add	w15, w13, w15
 1d0:	lsl	x12, x12, x15
 1d4:	sub	w13, w16, w13
 1d8:	b	1e0 <__muldf3+0x1e0>
 1dc:	mov	w13, wzr
 1e0:	lsr	x14, x14, #52
 1e4:	cbnz	x14, 3c <__muldf3+0x3c>
 1e8:	clz	x14, x11
 1ec:	mov	w15, #0xfffffff5            	// #-11
 1f0:	add	w15, w14, w15
 1f4:	sub	w13, w13, w14
 1f8:	lsl	x11, x11, x15
 1fc:	add	w13, w13, #0xc
 200:	b	3c <__muldf3+0x3c>

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	and	w10, w0, #0xffff
   4:	and	w11, w1, #0xffff
   8:	ubfx	x12, x0, #16, #16
   c:	ubfx	x13, x1, #16, #16
  10:	mul	w14, w11, w10
  14:	mul	w11, w11, w12
  18:	mul	w10, w13, w10
  1c:	mul	w12, w13, w12
  20:	add	w11, w11, w14, lsr #16
  24:	lsr	x8, x0, #32
  28:	add	w10, w10, w11, uxth
  2c:	add	w11, w12, w11, lsr #16
  30:	lsr	x9, x1, #32
  34:	mul	w8, w8, w1
  38:	bfi	w14, w10, #16, #16
  3c:	add	w10, w11, w10, lsr #16
  40:	bfi	x14, x10, #32, #32
  44:	madd	w8, w9, w0, w8
  48:	add	x0, x14, x8, lsl #32
  4c:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   4:	mov	x8, x0
   8:	cmp	x0, x9
   c:	mul	x0, x1, x0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulodi4+0x24>  // b.any
  18:	cmp	x1, #0x2
  1c:	b.cs	90 <__mulodi4+0x90>  // b.hs, b.nlast
  20:	ret
  24:	cmp	x1, x9
  28:	b.ne	38 <__mulodi4+0x38>  // b.any
  2c:	cmp	x8, #0x2
  30:	b.cc	20 <__mulodi4+0x20>  // b.lo, b.ul, b.last
  34:	b	90 <__mulodi4+0x90>
  38:	asr	x9, x8, #63
  3c:	eor	x8, x9, x8
  40:	sub	x8, x8, x9
  44:	cmp	x8, #0x2
  48:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  4c:	asr	x11, x1, #63
  50:	eor	x10, x11, x1
  54:	sub	x10, x10, x11
  58:	cmp	x10, #0x2
  5c:	b.lt	20 <__mulodi4+0x20>  // b.tstop
  60:	cmp	x9, x11
  64:	b.ne	7c <__mulodi4+0x7c>  // b.any
  68:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  6c:	udiv	x9, x9, x10
  70:	cmp	x8, x9
  74:	b.le	20 <__mulodi4+0x20>
  78:	b	90 <__mulodi4+0x90>
  7c:	neg	x9, x10
  80:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  84:	sdiv	x9, x10, x9
  88:	cmp	x8, x9
  8c:	b.le	20 <__mulodi4+0x20>
  90:	mov	w8, #0x1                   	// #1
  94:	str	w8, [x2]
  98:	ret

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w9, #0x80000000            	// #-2147483648
   4:	mov	w8, w0
   8:	cmp	w0, w9
   c:	mul	w0, w1, w0
  10:	str	wzr, [x2]
  14:	b.ne	24 <__mulosi4+0x24>  // b.any
  18:	cmp	w1, #0x2
  1c:	b.cs	90 <__mulosi4+0x90>  // b.hs, b.nlast
  20:	ret
  24:	cmp	w1, w9
  28:	b.ne	38 <__mulosi4+0x38>  // b.any
  2c:	cmp	w8, #0x2
  30:	b.cc	20 <__mulosi4+0x20>  // b.lo, b.ul, b.last
  34:	b	90 <__mulosi4+0x90>
  38:	asr	w9, w8, #31
  3c:	eor	w8, w9, w8
  40:	sub	w8, w8, w9
  44:	cmp	w8, #0x2
  48:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  4c:	asr	w11, w1, #31
  50:	eor	w10, w11, w1
  54:	sub	w10, w10, w11
  58:	cmp	w10, #0x2
  5c:	b.lt	20 <__mulosi4+0x20>  // b.tstop
  60:	cmp	w9, w11
  64:	b.ne	7c <__mulosi4+0x7c>  // b.any
  68:	mov	w9, #0x7fffffff            	// #2147483647
  6c:	udiv	w9, w9, w10
  70:	cmp	w8, w9
  74:	b.le	20 <__mulosi4+0x20>
  78:	b	90 <__mulosi4+0x90>
  7c:	neg	w9, w10
  80:	mov	w10, #0x80000000            	// #-2147483648
  84:	sdiv	w9, w10, w9
  88:	cmp	w8, w9
  8c:	b.le	20 <__mulosi4+0x20>
  90:	mov	w8, #0x1                   	// #1
  94:	str	w8, [x2]
  98:	ret

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	str	x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	umulh	x8, x2, x0
  18:	eor	x9, x1, #0x8000000000000000
  1c:	madd	x8, x2, x1, x8
  20:	mov	x19, x4
  24:	orr	x9, x0, x9
  28:	madd	x20, x3, x0, x8
  2c:	mul	x21, x2, x0
  30:	str	wzr, [x4]
  34:	cbnz	x9, 50 <__muloti4+0x50>
  38:	cmp	x2, #0x2
  3c:	cset	w8, cc  // cc = lo, ul, last
  40:	cmp	x3, #0x0
  44:	csel	w8, w8, wzr, eq  // eq = none
  48:	tbz	w8, #0, 114 <__muloti4+0x114>
  4c:	b	11c <__muloti4+0x11c>
  50:	eor	x8, x3, #0x8000000000000000
  54:	orr	x8, x2, x8
  58:	cbnz	x8, 74 <__muloti4+0x74>
  5c:	cmp	x0, #0x2
  60:	cset	w8, cc  // cc = lo, ul, last
  64:	cmp	x1, #0x0
  68:	csel	w8, w8, wzr, eq  // eq = none
  6c:	tbz	w8, #0, 114 <__muloti4+0x114>
  70:	b	11c <__muloti4+0x11c>
  74:	asr	x8, x1, #63
  78:	eor	x11, x8, x0
  7c:	asr	x9, x3, #63
  80:	eor	x10, x8, x1
  84:	subs	x23, x11, x8
  88:	eor	x13, x9, x2
  8c:	sbcs	x22, x10, x8
  90:	eor	x12, x9, x3
  94:	subs	x2, x13, x9
  98:	sbcs	x3, x12, x9
  9c:	cmp	x23, #0x2
  a0:	cset	w10, cc  // cc = lo, ul, last
  a4:	cmp	x22, #0x0
  a8:	cset	w11, lt  // lt = tstop
  ac:	csel	w10, w10, w11, eq  // eq = none
  b0:	tbnz	w10, #0, 11c <__muloti4+0x11c>
  b4:	cmp	x2, #0x2
  b8:	cset	w10, cc  // cc = lo, ul, last
  bc:	cmp	x3, #0x0
  c0:	cset	w11, lt  // lt = tstop
  c4:	csel	w10, w10, w11, eq  // eq = none
  c8:	tbnz	w10, #0, 11c <__muloti4+0x11c>
  cc:	eor	x8, x8, x9
  d0:	orr	x8, x8, x8
  d4:	cbnz	x8, e8 <__muloti4+0xe8>
  d8:	mov	x0, #0xffffffffffffffff    	// #-1
  dc:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  e0:	bl	0 <__udivti3>
  e4:	b	fc <__muloti4+0xfc>
  e8:	negs	x2, x2
  ec:	ngcs	x3, x3
  f0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
  f4:	mov	x0, xzr
  f8:	bl	0 <__divti3>
  fc:	cmp	x23, x0
 100:	cset	w8, ls  // ls = plast
 104:	cmp	x22, x1
 108:	cset	w9, le
 10c:	csel	w8, w8, w9, eq  // eq = none
 110:	tbnz	w8, #0, 11c <__muloti4+0x11c>
 114:	mov	w8, #0x1                   	// #1
 118:	str	w8, [x19]
 11c:	mov	x0, x21
 120:	mov	x1, x20
 124:	ldp	x20, x19, [sp, #48]
 128:	ldp	x22, x21, [sp, #32]
 12c:	ldr	x23, [sp, #16]
 130:	ldp	x29, x30, [sp], #64
 134:	ret

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s7, s0, s2
   4:	fmul	s6, s1, s3
   8:	fmul	s16, s0, s3
   c:	fmul	s17, s1, s2
  10:	fsub	s4, s7, s6
  14:	fcmp	s4, s4
  18:	fadd	s5, s17, s16
  1c:	b.vc	128 <__mulsc3+0x128>
  20:	fcmp	s5, s5
  24:	b.vc	128 <__mulsc3+0x128>
  28:	mov	w8, #0x7f800000            	// #2139095040
  2c:	fabs	s19, s0
  30:	fmov	s18, w8
  34:	fcmp	s19, s18
  38:	fabs	s18, s1
  3c:	b.eq	58 <__mulsc3+0x58>  // b.none
  40:	mov	w8, #0x7f800000            	// #2139095040
  44:	fmov	s20, w8
  48:	fcmp	s18, s20
  4c:	b.eq	58 <__mulsc3+0x58>  // b.none
  50:	mov	w8, wzr
  54:	b	ac <__mulsc3+0xac>
  58:	mov	w8, #0x7f800000            	// #2139095040
  5c:	fmov	s23, w8
  60:	fmov	s20, wzr
  64:	fmov	s21, #1.000000000000000000e+00
  68:	fcmp	s19, s23
  6c:	movi	v22.4s, #0x80, lsl #24
  70:	fcsel	s19, s21, s20, eq  // eq = none
  74:	fcmp	s18, s23
  78:	fmov	s18, wzr
  7c:	bit	v18.16b, v2.16b, v22.16b
  80:	bit	v19.16b, v0.16b, v22.16b
  84:	fcsel	s0, s21, s20, eq  // eq = none
  88:	fcmp	s2, s2
  8c:	bit	v20.16b, v3.16b, v22.16b
  90:	bit	v0.16b, v1.16b, v22.16b
  94:	fcsel	s2, s18, s2, vs
  98:	fcmp	s3, s3
  9c:	fcsel	s3, s20, s3, vs
  a0:	mov	w8, #0x1                   	// #1
  a4:	mov	v1.16b, v0.16b
  a8:	mov	v0.16b, v19.16b
  ac:	mov	w9, #0x7f800000            	// #2139095040
  b0:	fabs	s18, s3
  b4:	fmov	s19, w9
  b8:	fcmp	s18, s19
  bc:	fabs	s19, s2
  c0:	b.eq	134 <__mulsc3+0x134>  // b.none
  c4:	mov	w9, #0x7f800000            	// #2139095040
  c8:	fmov	s20, w9
  cc:	fcmp	s19, s20
  d0:	b.eq	134 <__mulsc3+0x134>  // b.none
  d4:	cbnz	w8, 184 <__mulsc3+0x184>
  d8:	mov	w8, #0x7f800000            	// #2139095040
  dc:	fabs	s17, s17
  e0:	fmov	s18, w8
  e4:	fcmp	s17, s18
  e8:	b.eq	1b8 <__mulsc3+0x1b8>  // b.none
  ec:	mov	w8, #0x7f800000            	// #2139095040
  f0:	fabs	s16, s16
  f4:	fmov	s17, w8
  f8:	fcmp	s16, s17
  fc:	b.eq	1b8 <__mulsc3+0x1b8>  // b.none
 100:	mov	w8, #0x7f800000            	// #2139095040
 104:	fabs	s7, s7
 108:	fmov	s16, w8
 10c:	fcmp	s7, s16
 110:	b.eq	1b8 <__mulsc3+0x1b8>  // b.none
 114:	mov	w8, #0x7f800000            	// #2139095040
 118:	fabs	s6, s6
 11c:	fmov	s7, w8
 120:	fcmp	s6, s7
 124:	b.eq	1b8 <__mulsc3+0x1b8>  // b.none
 128:	mov	v0.16b, v4.16b
 12c:	mov	v1.16b, v5.16b
 130:	ret
 134:	mov	w8, #0x7f800000            	// #2139095040
 138:	fmov	s7, w8
 13c:	fmov	s4, wzr
 140:	fmov	s5, #1.000000000000000000e+00
 144:	fcmp	s19, s7
 148:	movi	v6.4s, #0x80, lsl #24
 14c:	fcsel	s16, s5, s4, eq  // eq = none
 150:	fcmp	s18, s7
 154:	fmov	s7, wzr
 158:	bit	v7.16b, v0.16b, v6.16b
 15c:	bit	v16.16b, v2.16b, v6.16b
 160:	fcsel	s2, s5, s4, eq  // eq = none
 164:	fcmp	s0, s0
 168:	bit	v4.16b, v1.16b, v6.16b
 16c:	bit	v2.16b, v3.16b, v6.16b
 170:	fcsel	s0, s7, s0, vs
 174:	fcmp	s1, s1
 178:	fcsel	s1, s4, s1, vs
 17c:	mov	v3.16b, v2.16b
 180:	mov	v2.16b, v16.16b
 184:	fmul	s4, s2, s0
 188:	fmul	s5, s3, s1
 18c:	mov	w8, #0x7f800000            	// #2139095040
 190:	fmul	s0, s3, s0
 194:	fmul	s1, s2, s1
 198:	fsub	s2, s4, s5
 19c:	fmov	s3, w8
 1a0:	fadd	s0, s1, s0
 1a4:	fmul	s4, s2, s3
 1a8:	fmul	s5, s0, s3
 1ac:	mov	v0.16b, v4.16b
 1b0:	mov	v1.16b, v5.16b
 1b4:	ret
 1b8:	movi	v5.4s, #0x80, lsl #24
 1bc:	fmov	s6, wzr
 1c0:	fmov	s7, wzr
 1c4:	bit	v6.16b, v0.16b, v5.16b
 1c8:	fcmp	s0, s0
 1cc:	fmov	s16, wzr
 1d0:	bit	v7.16b, v1.16b, v5.16b
 1d4:	fcsel	s0, s6, s0, vs
 1d8:	fcmp	s1, s1
 1dc:	fmov	s4, wzr
 1e0:	bit	v16.16b, v2.16b, v5.16b
 1e4:	fcsel	s1, s7, s1, vs
 1e8:	fcmp	s2, s2
 1ec:	bit	v4.16b, v3.16b, v5.16b
 1f0:	fcsel	s2, s16, s2, vs
 1f4:	fcmp	s3, s3
 1f8:	fcsel	s3, s4, s3, vs
 1fc:	b	184 <__mulsc3+0x184>

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	fmov	w13, s0
   4:	ubfx	w9, w13, #23, #8
   8:	fmov	w15, s1
   c:	eor	w8, w15, w13
  10:	sub	w12, w9, #0x1
  14:	ubfx	w10, w15, #23, #8
  18:	and	w11, w13, #0x7fffff
  1c:	and	w8, w8, #0x80000000
  20:	cmp	w12, #0xfd
  24:	and	w12, w15, #0x7fffff
  28:	b.hi	74 <__mulsf3+0x74>  // b.pmore
  2c:	sub	w14, w10, #0x1
  30:	cmp	w14, #0xfe
  34:	b.cs	74 <__mulsf3+0x74>  // b.hs, b.nlast
  38:	mov	w13, wzr
  3c:	lsl	w12, w12, #8
  40:	orr	w11, w11, #0x800000
  44:	add	w14, w9, w10
  48:	orr	w9, w12, #0x80000000
  4c:	umull	x10, w9, w11
  50:	add	w11, w14, w13
  54:	lsr	x9, x10, #32
  58:	sub	w11, w11, #0x7f
  5c:	tbnz	w9, #23, 94 <__mulsf3+0x94>
  60:	extr	w9, w9, w10, #31
  64:	lsl	w10, w10, #1
  68:	cmp	w11, #0xff
  6c:	b.lt	a0 <__mulsf3+0xa0>  // b.tstop
  70:	b	f0 <__mulsf3+0xf0>
  74:	mov	w16, #0x1                   	// #1
  78:	and	w14, w13, #0x7fffffff
  7c:	movk	w16, #0x7f80, lsl #16
  80:	cmp	w14, w16
  84:	b.cc	c8 <__mulsf3+0xc8>  // b.lo, b.ul, b.last
  88:	orr	w8, w13, #0x400000
  8c:	fmov	s0, w8
  90:	ret
  94:	add	w11, w11, #0x1
  98:	cmp	w11, #0xff
  9c:	b.ge	f0 <__mulsf3+0xf0>  // b.tcont
  a0:	cmp	w11, #0x0
  a4:	b.le	fc <__mulsf3+0xfc>
  a8:	bfi	w9, w11, #23, #9
  ac:	mov	w11, #0x80000001            	// #-2147483647
  b0:	cmp	w10, w11
  b4:	orr	w8, w9, w8
  b8:	b.cc	140 <__mulsf3+0x140>  // b.lo, b.ul, b.last
  bc:	add	w8, w8, #0x1
  c0:	fmov	s0, w8
  c4:	ret
  c8:	and	w13, w15, #0x7fffffff
  cc:	cmp	w13, w16
  d0:	b.cc	e0 <__mulsf3+0xe0>  // b.lo, b.ul, b.last
  d4:	orr	w8, w15, #0x400000
  d8:	fmov	s0, w8
  dc:	ret
  e0:	mov	w15, #0x7f800000            	// #2139095040
  e4:	cmp	w14, w15
  e8:	b.ne	15c <__mulsf3+0x15c>  // b.any
  ec:	cbz	w13, 168 <__mulsf3+0x168>
  f0:	orr	w8, w8, #0x7f800000
  f4:	fmov	s0, w8
  f8:	ret
  fc:	mov	w12, #0x1                   	// #1
 100:	sub	w12, w12, w11
 104:	cmp	w12, #0x1f
 108:	b.hi	154 <__mulsf3+0x154>  // b.pmore
 10c:	add	w11, w11, #0x1f
 110:	lsr	w13, w10, w12
 114:	lsl	w10, w10, w11
 118:	lsl	w11, w9, w11
 11c:	cmp	w10, #0x0
 120:	orr	w10, w11, w13
 124:	cset	w11, ne  // ne = any
 128:	orr	w10, w10, w11
 12c:	lsr	w9, w9, w12
 130:	mov	w11, #0x80000001            	// #-2147483647
 134:	cmp	w10, w11
 138:	orr	w8, w9, w8
 13c:	b.cs	bc <__mulsf3+0xbc>  // b.hs, b.nlast
 140:	mov	w11, #0x80000000            	// #-2147483648
 144:	cmp	w10, w11
 148:	b.ne	154 <__mulsf3+0x154>  // b.any
 14c:	and	w9, w9, #0x1
 150:	add	w8, w8, w9
 154:	fmov	s0, w8
 158:	ret
 15c:	cmp	w13, w15
 160:	b.ne	174 <__mulsf3+0x174>  // b.any
 164:	cbnz	w14, f0 <__mulsf3+0xf0>
 168:	mov	w8, #0x7fc00000            	// #2143289344
 16c:	fmov	s0, w8
 170:	ret
 174:	cbz	w14, 154 <__mulsf3+0x154>
 178:	cbz	w13, 154 <__mulsf3+0x154>
 17c:	clz	w15, w11
 180:	mov	w16, #0x9                   	// #9
 184:	cmp	w14, #0x800, lsl #12
 188:	lsr	w14, w13, #23
 18c:	sub	w13, w15, #0x8
 190:	sub	w15, w16, w15
 194:	csel	w13, w13, wzr, cc  // cc = lo, ul, last
 198:	lsl	w11, w11, w13
 19c:	csel	w13, w15, wzr, cc  // cc = lo, ul, last
 1a0:	cbnz	w14, 3c <__mulsf3+0x3c>
 1a4:	clz	w14, w12
 1a8:	sub	w15, w14, #0x8
 1ac:	sub	w13, w13, w14
 1b0:	lsl	w12, w12, w15
 1b4:	add	w13, w13, #0x9
 1b8:	b	3c <__mulsf3+0x3c>

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x9, x0, #0xffffffff
   4:	and	x10, x2, #0xffffffff
   8:	lsr	x11, x0, #32
   c:	lsr	x12, x2, #32
  10:	mul	x13, x1, x2
  14:	mul	x8, x10, x9
  18:	mul	x10, x10, x11
  1c:	mul	x9, x12, x9
  20:	madd	x13, x3, x0, x13
  24:	add	x10, x10, x8, lsr #32
  28:	madd	x11, x12, x11, x13
  2c:	add	x9, x9, w10, uxtw
  30:	add	x10, x11, x10, lsr #32
  34:	bfi	x8, x9, #32, #32
  38:	add	x1, x10, x9, lsr #32
  3c:	mov	x0, x8
  40:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	sub	sp, sp, #0xc0
   4:	str	x19, [sp, #176]
   8:	stp	q1, q0, [sp, #144]
   c:	ldp	x12, x2, [sp, #160]
  10:	ldp	x14, x3, [sp, #144]
  14:	mov	w11, #0x7ffd                	// #32765
  18:	ubfx	x9, x2, #48, #15
  1c:	eor	x8, x3, x2
  20:	and	x1, x3, #0xffffffffffff
  24:	sub	w16, w9, #0x1
  28:	and	x13, x2, #0xffffffffffff
  2c:	extr	x15, x2, x12, #32
  30:	ubfx	x0, x2, #32, #16
  34:	ubfx	x10, x3, #48, #15
  38:	extr	x17, x3, x14, #17
  3c:	and	x8, x8, #0x8000000000000000
  40:	extr	x18, x1, x14, #49
  44:	cmp	w16, w11
  48:	ubfx	x11, x3, #17, #31
  4c:	b.hi	198 <__multf3+0x198>  // b.pmore
  50:	sub	w16, w10, #0x1
  54:	mov	w4, #0x7ffe                	// #32766
  58:	cmp	w16, w4
  5c:	b.cs	198 <__multf3+0x198>  // b.hs, b.nlast
  60:	mov	w16, wzr
  64:	orr	x0, x0, #0x10000
  68:	mov	w1, #0x80000000            	// #-2147483648
  6c:	and	x17, x17, #0xffffffff
  70:	lsl	w14, w14, #15
  74:	and	x13, x13, #0xffffffff
  78:	and	x15, x15, #0xffffffff
  7c:	and	x12, x12, #0xffffffff
  80:	and	x18, x18, #0xffffffff
  84:	add	w9, w9, w10
  88:	bfxil	x1, x11, #0, #31
  8c:	mul	x2, x17, x0
  90:	mul	x4, x17, x13
  94:	mul	x7, x17, x15
  98:	mul	x19, x14, x15
  9c:	mul	x17, x17, x12
  a0:	mul	x3, x18, x13
  a4:	mul	x5, x14, x13
  a8:	mul	x6, x18, x15
  ac:	add	w16, w9, w16
  b0:	mul	x9, x1, x13
  b4:	mul	x13, x1, x15
  b8:	adds	x15, x17, x19
  bc:	adcs	x17, xzr, xzr
  c0:	adds	x5, x7, x5
  c4:	mul	x10, x18, x0
  c8:	mul	x18, x18, x12
  cc:	adcs	x7, xzr, xzr
  d0:	adds	x18, x5, x18
  d4:	mul	x11, x14, x0
  d8:	adcs	x5, x7, xzr
  dc:	adds	x11, x4, x11
  e0:	adcs	x4, xzr, xzr
  e4:	adds	x11, x11, x6
  e8:	mul	x14, x14, x12
  ec:	mul	x12, x1, x12
  f0:	adcs	x4, x4, xzr
  f4:	adds	x11, x11, x12
  f8:	adcs	x12, x4, xzr
  fc:	adds	x9, x9, x10
 100:	adcs	x4, xzr, xzr
 104:	adds	x10, x14, x15, lsl #32
 108:	extr	x17, x17, x15, #32
 10c:	adcs	x15, xzr, xzr
 110:	adds	x17, x18, x17
 114:	adcs	x18, xzr, xzr
 118:	extr	x12, x12, x11, #32
 11c:	adds	x11, x17, x11, lsl #32
 120:	adcs	x17, x18, xzr
 124:	add	x11, x15, x11
 128:	adds	x15, x3, x2
 12c:	adcs	x18, xzr, xzr
 130:	adds	x13, x15, x13
 134:	adcs	x15, x18, xzr
 138:	adds	x13, x13, x5
 13c:	adcs	x15, x15, xzr
 140:	extr	x14, x4, x9, #32
 144:	madd	x15, x1, x0, x15
 148:	adds	x9, x13, x9, lsl #32
 14c:	adcs	x13, x15, x14
 150:	adds	x9, x9, x12
 154:	adcs	x12, x13, xzr
 158:	adds	x9, x9, x17
 15c:	mov	w13, #0xffffc001            	// #-16383
 160:	adcs	x12, x12, xzr
 164:	add	w13, w16, w13
 168:	tbnz	x12, #48, 1d0 <__multf3+0x1d0>
 16c:	extr	x12, x12, x9, #63
 170:	extr	x9, x9, x11, #63
 174:	extr	x11, x11, x10, #63
 178:	lsl	x10, x10, #1
 17c:	mov	w14, #0x7fff                	// #32767
 180:	cmp	w13, w14
 184:	b.ge	1e0 <__multf3+0x1e0>  // b.tcont
 188:	cmp	w13, #0x0
 18c:	b.le	258 <__multf3+0x258>
 190:	bfi	x12, x13, #48, #16
 194:	b	360 <__multf3+0x360>
 198:	and	x16, x2, #0x7fffffffffffffff
 19c:	cmp	x12, #0x0
 1a0:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 1a4:	cset	w5, eq  // eq = none
 1a8:	cmp	x16, x4
 1ac:	cset	w6, cc  // cc = lo, ul, last
 1b0:	csel	w5, w5, w6, eq  // eq = none
 1b4:	tbnz	w5, #0, 1f8 <__multf3+0x1f8>
 1b8:	orr	x8, x2, #0x800000000000
 1bc:	stp	x12, x8, [sp]
 1c0:	ldr	q0, [sp]
 1c4:	ldr	x19, [sp, #176]
 1c8:	add	sp, sp, #0xc0
 1cc:	ret
 1d0:	add	w13, w13, #0x1
 1d4:	mov	w14, #0x7fff                	// #32767
 1d8:	cmp	w13, w14
 1dc:	b.lt	188 <__multf3+0x188>  // b.tstop
 1e0:	orr	x8, x8, #0x7fff000000000000
 1e4:	stp	xzr, x8, [sp, #80]
 1e8:	ldr	q0, [sp, #80]
 1ec:	ldr	x19, [sp, #176]
 1f0:	add	sp, sp, #0xc0
 1f4:	ret
 1f8:	and	x2, x3, #0x7fffffffffffffff
 1fc:	cmp	x14, #0x0
 200:	cset	w5, eq  // eq = none
 204:	cmp	x2, x4
 208:	cset	w4, cc  // cc = lo, ul, last
 20c:	csel	w4, w5, w4, eq  // eq = none
 210:	tbnz	w4, #0, 22c <__multf3+0x22c>
 214:	orr	x8, x3, #0x800000000000
 218:	stp	x14, x8, [sp, #16]
 21c:	ldr	q0, [sp, #16]
 220:	ldr	x19, [sp, #176]
 224:	add	sp, sp, #0xc0
 228:	ret
 22c:	eor	x3, x16, #0x7fff000000000000
 230:	orr	x3, x12, x3
 234:	cbnz	x3, 27c <__multf3+0x27c>
 238:	orr	x9, x14, x2
 23c:	cbz	x9, 3b8 <__multf3+0x3b8>
 240:	orr	x8, x8, #0x7fff000000000000
 244:	stp	xzr, x8, [sp, #32]
 248:	ldr	q0, [sp, #32]
 24c:	ldr	x19, [sp, #176]
 250:	add	sp, sp, #0xc0
 254:	ret
 258:	mov	w14, #0x1                   	// #1
 25c:	sub	w13, w14, w13
 260:	cmp	w13, #0x7f
 264:	b.ls	2a8 <__multf3+0x2a8>  // b.plast
 268:	stp	xzr, x8, [sp, #96]
 26c:	ldr	q0, [sp, #96]
 270:	ldr	x19, [sp, #176]
 274:	add	sp, sp, #0xc0
 278:	ret
 27c:	eor	x3, x2, #0x7fff000000000000
 280:	orr	x3, x14, x3
 284:	cbnz	x3, 3cc <__multf3+0x3cc>
 288:	orr	x9, x12, x16
 28c:	cbz	x9, 3b8 <__multf3+0x3b8>
 290:	orr	x8, x8, #0x7fff000000000000
 294:	stp	xzr, x8, [sp, #48]
 298:	ldr	q0, [sp, #48]
 29c:	ldr	x19, [sp, #176]
 2a0:	add	sp, sp, #0xc0
 2a4:	ret
 2a8:	mov	w14, #0x80                  	// #128
 2ac:	mov	w16, #0x40                  	// #64
 2b0:	neg	x15, x13
 2b4:	sub	x14, x14, x13
 2b8:	sub	x16, x16, x13
 2bc:	lsl	x0, x9, x15
 2c0:	neg	x1, x14
 2c4:	cmp	x16, #0x0
 2c8:	csel	x2, xzr, x0, ge  // ge = tcont
 2cc:	cmp	x14, #0x0
 2d0:	lsr	x14, x10, x1
 2d4:	lsr	x1, x9, x1
 2d8:	lsr	x18, x10, x13
 2dc:	csel	x14, xzr, x14, eq  // eq = none
 2e0:	csel	x1, xzr, x1, eq  // eq = none
 2e4:	cmp	x16, #0x0
 2e8:	lsl	x16, x12, x15
 2ec:	lsl	x10, x10, x15
 2f0:	lsl	x15, x11, x15
 2f4:	orr	x14, x14, x15
 2f8:	csel	x14, x10, x14, ge  // ge = tcont
 2fc:	csel	x10, xzr, x10, ge  // ge = tcont
 300:	orr	x1, x1, x16
 304:	orr	x10, x10, x14
 308:	csel	x0, x0, x1, ge  // ge = tcont
 30c:	cmp	x10, #0x0
 310:	cset	w10, ne  // ne = any
 314:	cmp	x13, #0x0
 318:	sub	x17, x13, #0x40
 31c:	lsr	x1, x11, x13
 320:	lsr	x11, x11, x13
 324:	lsr	x9, x9, x13
 328:	lsr	x14, x12, x13
 32c:	lsr	x12, x12, x13
 330:	csel	x13, xzr, x15, eq  // eq = none
 334:	csel	x15, xzr, x16, eq  // eq = none
 338:	cmp	x17, #0x0
 33c:	orr	x13, x18, x13
 340:	csel	x13, x1, x13, ge  // ge = tcont
 344:	csel	x11, xzr, x11, ge  // ge = tcont
 348:	orr	x9, x9, x15
 34c:	orr	x13, x2, x13
 350:	orr	x11, x0, x11
 354:	csel	x9, x14, x9, ge  // ge = tcont
 358:	orr	x10, x13, x10
 35c:	csel	x12, xzr, x12, ge  // ge = tcont
 360:	cmp	x10, #0x0
 364:	mov	x13, #0x8000000000000000    	// #-9223372036854775808
 368:	cset	w14, eq  // eq = none
 36c:	cmp	x11, #0x0
 370:	cset	w15, ge  // ge = tcont
 374:	cmp	x11, x13
 378:	csel	w13, w14, w15, eq  // eq = none
 37c:	orr	x8, x12, x8
 380:	tbnz	w13, #0, 38c <__multf3+0x38c>
 384:	adds	x9, x9, #0x1
 388:	b	3a0 <__multf3+0x3a0>
 38c:	eor	x11, x11, #0x8000000000000000
 390:	orr	x10, x10, x11
 394:	cbnz	x10, 3a4 <__multf3+0x3a4>
 398:	and	x10, x9, #0x1
 39c:	adds	x9, x9, x10
 3a0:	adcs	x8, x8, xzr
 3a4:	stp	x9, x8, [sp, #112]
 3a8:	ldr	q0, [sp, #112]
 3ac:	ldr	x19, [sp, #176]
 3b0:	add	sp, sp, #0xc0
 3b4:	ret
 3b8:	adrp	x8, 0 <__multf3>
 3bc:	ldr	q0, [x8]
 3c0:	ldr	x19, [sp, #176]
 3c4:	add	sp, sp, #0xc0
 3c8:	ret
 3cc:	orr	x3, x12, x16
 3d0:	cbz	x3, 444 <__multf3+0x444>
 3d4:	orr	x3, x14, x2
 3d8:	cbz	x3, 458 <__multf3+0x458>
 3dc:	lsr	x16, x16, #48
 3e0:	cbnz	x16, 46c <__multf3+0x46c>
 3e4:	cmp	x13, #0x0
 3e8:	csel	x0, x12, x13, eq  // eq = none
 3ec:	cset	w16, eq  // eq = none
 3f0:	clz	x0, x0
 3f4:	add	w16, w0, w16, lsl #6
 3f8:	mov	w15, #0x10                  	// #16
 3fc:	sub	w0, w16, #0xf
 400:	sub	w16, w15, w16
 404:	neg	x15, x0
 408:	cmp	x0, #0x0
 40c:	lsl	x3, x12, x0
 410:	sub	x4, x0, #0x40
 414:	lsl	x13, x13, x0
 418:	lsl	x0, x12, x0
 41c:	lsr	x12, x12, x15
 420:	csel	x12, xzr, x12, eq  // eq = none
 424:	cmp	x4, #0x0
 428:	orr	x12, x12, x13
 42c:	csel	x3, xzr, x3, ge  // ge = tcont
 430:	csel	x13, x0, x12, ge  // ge = tcont
 434:	extr	x15, x13, x3, #32
 438:	lsr	x0, x13, #32
 43c:	mov	x12, x3
 440:	b	470 <__multf3+0x470>
 444:	stp	xzr, x8, [sp, #128]
 448:	ldr	q0, [sp, #128]
 44c:	ldr	x19, [sp, #176]
 450:	add	sp, sp, #0xc0
 454:	ret
 458:	stp	xzr, x8, [sp, #64]
 45c:	ldr	q0, [sp, #64]
 460:	ldr	x19, [sp, #176]
 464:	add	sp, sp, #0xc0
 468:	ret
 46c:	mov	w16, wzr
 470:	lsr	x2, x2, #48
 474:	cbnz	x2, 64 <__multf3+0x64>
 478:	cmp	x1, #0x0
 47c:	csel	x17, x14, x1, eq  // eq = none
 480:	cset	w11, eq  // eq = none
 484:	clz	x17, x17
 488:	add	w11, w17, w11, lsl #6
 48c:	sub	w17, w11, #0xf
 490:	sub	w11, w16, w11
 494:	neg	x18, x17
 498:	cmp	x17, #0x0
 49c:	add	w16, w11, #0x10
 4a0:	lsr	x11, x14, x18
 4a4:	sub	x3, x17, #0x40
 4a8:	lsl	x1, x1, x17
 4ac:	csel	x11, xzr, x11, eq  // eq = none
 4b0:	lsl	x2, x14, x17
 4b4:	lsl	x17, x14, x17
 4b8:	cmp	x3, #0x0
 4bc:	orr	x11, x11, x1
 4c0:	csel	x14, xzr, x2, ge  // ge = tcont
 4c4:	csel	x18, x17, x11, ge  // ge = tcont
 4c8:	extr	x17, x18, x14, #17
 4cc:	lsr	x11, x18, #17
 4d0:	extr	x18, x18, x14, #49
 4d4:	b	64 <__multf3+0x64>

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   c:	cmp	x0, x8
  10:	b.ne	28 <__mulvdi3+0x28>  // b.any
  14:	cmp	x1, #0x1
  18:	b.hi	bc <__mulvdi3+0xbc>  // b.pmore
  1c:	lsl	x0, x1, #63
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	cmp	x1, x8
  2c:	b.ne	44 <__mulvdi3+0x44>  // b.any
  30:	cmp	x0, #0x1
  34:	b.hi	d4 <__mulvdi3+0xd4>  // b.pmore
  38:	lsl	x0, x0, #63
  3c:	ldp	x29, x30, [sp], #16
  40:	ret
  44:	asr	x9, x0, #63
  48:	eor	x8, x9, x0
  4c:	sub	x8, x8, x9
  50:	cmp	x8, #0x2
  54:	b.lt	b0 <__mulvdi3+0xb0>  // b.tstop
  58:	asr	x11, x1, #63
  5c:	eor	x10, x11, x1
  60:	sub	x10, x10, x11
  64:	cmp	x10, #0x2
  68:	b.lt	b0 <__mulvdi3+0xb0>  // b.tstop
  6c:	cmp	x9, x11
  70:	b.ne	9c <__mulvdi3+0x9c>  // b.any
  74:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
  78:	udiv	x9, x9, x10
  7c:	cmp	x8, x9
  80:	b.le	b0 <__mulvdi3+0xb0>
  84:	adrp	x0, 0 <__mulvdi3>
  88:	adrp	x2, 0 <__mulvdi3>
  8c:	add	x0, x0, #0x0
  90:	add	x2, x2, #0x0
  94:	mov	w1, #0x29                  	// #41
  98:	bl	0 <__compilerrt_abort_impl>
  9c:	neg	x9, x10
  a0:	mov	x10, #0x8000000000000000    	// #-9223372036854775808
  a4:	sdiv	x9, x10, x9
  a8:	cmp	x8, x9
  ac:	b.gt	ec <__mulvdi3+0xec>
  b0:	mul	x0, x1, x0
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret
  bc:	adrp	x0, 0 <__mulvdi3>
  c0:	adrp	x2, 0 <__mulvdi3>
  c4:	add	x0, x0, #0x0
  c8:	add	x2, x2, #0x0
  cc:	mov	w1, #0x1a                  	// #26
  d0:	bl	0 <__compilerrt_abort_impl>
  d4:	adrp	x0, 0 <__mulvdi3>
  d8:	adrp	x2, 0 <__mulvdi3>
  dc:	add	x0, x0, #0x0
  e0:	add	x2, x2, #0x0
  e4:	mov	w1, #0x1f                  	// #31
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x0, 0 <__mulvdi3>
  f0:	adrp	x2, 0 <__mulvdi3>
  f4:	add	x0, x0, #0x0
  f8:	add	x2, x2, #0x0
  fc:	mov	w1, #0x2c                  	// #44
 100:	bl	0 <__compilerrt_abort_impl>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, #0x80000000            	// #-2147483648
   c:	cmp	w0, w8
  10:	b.ne	28 <__mulvsi3+0x28>  // b.any
  14:	cmp	w1, #0x1
  18:	b.hi	bc <__mulvsi3+0xbc>  // b.pmore
  1c:	lsl	w0, w1, #31
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	cmp	w1, w8
  2c:	b.ne	44 <__mulvsi3+0x44>  // b.any
  30:	cmp	w0, #0x1
  34:	b.hi	d4 <__mulvsi3+0xd4>  // b.pmore
  38:	lsl	w0, w0, #31
  3c:	ldp	x29, x30, [sp], #16
  40:	ret
  44:	asr	w9, w0, #31
  48:	eor	w8, w9, w0
  4c:	sub	w8, w8, w9
  50:	cmp	w8, #0x2
  54:	b.lt	b0 <__mulvsi3+0xb0>  // b.tstop
  58:	asr	w11, w1, #31
  5c:	eor	w10, w11, w1
  60:	sub	w10, w10, w11
  64:	cmp	w10, #0x2
  68:	b.lt	b0 <__mulvsi3+0xb0>  // b.tstop
  6c:	cmp	w9, w11
  70:	b.ne	9c <__mulvsi3+0x9c>  // b.any
  74:	mov	w9, #0x7fffffff            	// #2147483647
  78:	udiv	w9, w9, w10
  7c:	cmp	w8, w9
  80:	b.le	b0 <__mulvsi3+0xb0>
  84:	adrp	x0, 0 <__mulvsi3>
  88:	adrp	x2, 0 <__mulvsi3>
  8c:	add	x0, x0, #0x0
  90:	add	x2, x2, #0x0
  94:	mov	w1, #0x29                  	// #41
  98:	bl	0 <__compilerrt_abort_impl>
  9c:	neg	w9, w10
  a0:	mov	w10, #0x80000000            	// #-2147483648
  a4:	sdiv	w9, w10, w9
  a8:	cmp	w8, w9
  ac:	b.gt	ec <__mulvsi3+0xec>
  b0:	mul	w0, w1, w0
  b4:	ldp	x29, x30, [sp], #16
  b8:	ret
  bc:	adrp	x0, 0 <__mulvsi3>
  c0:	adrp	x2, 0 <__mulvsi3>
  c4:	add	x0, x0, #0x0
  c8:	add	x2, x2, #0x0
  cc:	mov	w1, #0x1a                  	// #26
  d0:	bl	0 <__compilerrt_abort_impl>
  d4:	adrp	x0, 0 <__mulvsi3>
  d8:	adrp	x2, 0 <__mulvsi3>
  dc:	add	x0, x0, #0x0
  e0:	add	x2, x2, #0x0
  e4:	mov	w1, #0x1f                  	// #31
  e8:	bl	0 <__compilerrt_abort_impl>
  ec:	adrp	x0, 0 <__mulvsi3>
  f0:	adrp	x2, 0 <__mulvsi3>
  f4:	add	x0, x0, #0x0
  f8:	add	x2, x2, #0x0
  fc:	mov	w1, #0x2c                  	// #44
 100:	bl	0 <__compilerrt_abort_impl>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	stp	x24, x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	eor	x8, x1, #0x8000000000000000
  18:	mov	x21, x3
  1c:	orr	x8, x0, x8
  20:	mov	x20, x2
  24:	cbnz	x8, 4c <__mulvti3+0x4c>
  28:	cmp	x20, #0x1
  2c:	cset	w8, hi  // hi = pmore
  30:	cmp	x21, #0x0
  34:	cset	w9, ne  // ne = any
  38:	csel	w8, w8, w9, eq  // eq = none
  3c:	tbnz	w8, #0, 174 <__mulvti3+0x174>
  40:	mov	x0, xzr
  44:	lsl	x1, x20, #63
  48:	b	160 <__mulvti3+0x160>
  4c:	eor	x8, x21, #0x8000000000000000
  50:	mov	x22, x1
  54:	mov	x19, x0
  58:	orr	x8, x20, x8
  5c:	cbnz	x8, 84 <__mulvti3+0x84>
  60:	cmp	x19, #0x1
  64:	cset	w8, hi  // hi = pmore
  68:	cmp	x22, #0x0
  6c:	cset	w9, ne  // ne = any
  70:	csel	w8, w8, w9, eq  // eq = none
  74:	tbnz	w8, #0, 18c <__mulvti3+0x18c>
  78:	mov	x0, xzr
  7c:	lsl	x1, x19, #63
  80:	b	160 <__mulvti3+0x160>
  84:	asr	x8, x22, #63
  88:	eor	x11, x8, x19
  8c:	asr	x9, x21, #63
  90:	eor	x10, x8, x22
  94:	subs	x24, x11, x8
  98:	eor	x13, x9, x20
  9c:	sbcs	x23, x10, x8
  a0:	eor	x12, x9, x21
  a4:	subs	x2, x13, x9
  a8:	sbcs	x3, x12, x9
  ac:	cmp	x24, #0x2
  b0:	cset	w10, cc  // cc = lo, ul, last
  b4:	cmp	x23, #0x0
  b8:	cset	w11, lt  // lt = tstop
  bc:	csel	w10, w10, w11, eq  // eq = none
  c0:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  c4:	cmp	x2, #0x2
  c8:	cset	w10, cc  // cc = lo, ul, last
  cc:	cmp	x3, #0x0
  d0:	cset	w11, lt  // lt = tstop
  d4:	csel	w10, w10, w11, eq  // eq = none
  d8:	tbnz	w10, #0, 150 <__mulvti3+0x150>
  dc:	eor	x8, x8, x9
  e0:	orr	x8, x8, x8
  e4:	cbnz	x8, 124 <__mulvti3+0x124>
  e8:	mov	x0, #0xffffffffffffffff    	// #-1
  ec:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
  f0:	bl	0 <__udivti3>
  f4:	cmp	x24, x0
  f8:	cset	w8, ls  // ls = plast
  fc:	cmp	x23, x1
 100:	cset	w9, le
 104:	csel	w8, w8, w9, eq  // eq = none
 108:	tbnz	w8, #0, 150 <__mulvti3+0x150>
 10c:	adrp	x0, 0 <__mulvti3>
 110:	adrp	x2, 0 <__mulvti3>
 114:	add	x0, x0, #0x0
 118:	add	x2, x2, #0x0
 11c:	mov	w1, #0x2b                  	// #43
 120:	bl	0 <__compilerrt_abort_impl>
 124:	negs	x2, x2
 128:	ngcs	x3, x3
 12c:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 130:	mov	x0, xzr
 134:	bl	0 <__divti3>
 138:	cmp	x24, x0
 13c:	cset	w8, ls  // ls = plast
 140:	cmp	x23, x1
 144:	cset	w9, le
 148:	csel	w8, w8, w9, eq  // eq = none
 14c:	tbz	w8, #0, 1a4 <__mulvti3+0x1a4>
 150:	umulh	x8, x20, x19
 154:	madd	x8, x20, x22, x8
 158:	madd	x1, x21, x19, x8
 15c:	mul	x0, x20, x19
 160:	ldp	x20, x19, [sp, #48]
 164:	ldp	x22, x21, [sp, #32]
 168:	ldp	x24, x23, [sp, #16]
 16c:	ldp	x29, x30, [sp], #64
 170:	ret
 174:	adrp	x0, 0 <__mulvti3>
 178:	adrp	x2, 0 <__mulvti3>
 17c:	add	x0, x0, #0x0
 180:	add	x2, x2, #0x0
 184:	mov	w1, #0x1c                  	// #28
 188:	bl	0 <__compilerrt_abort_impl>
 18c:	adrp	x0, 0 <__mulvti3>
 190:	adrp	x2, 0 <__mulvti3>
 194:	add	x0, x0, #0x0
 198:	add	x2, x2, #0x0
 19c:	mov	w1, #0x21                  	// #33
 1a0:	bl	0 <__compilerrt_abort_impl>
 1a4:	adrp	x0, 0 <__mulvti3>
 1a8:	adrp	x2, 0 <__mulvti3>
 1ac:	add	x0, x0, #0x0
 1b0:	add	x2, x2, #0x0
 1b4:	mov	w1, #0x2e                  	// #46
 1b8:	bl	0 <__compilerrt_abort_impl>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	fneg	d0, d0
   4:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	fneg	s0, s0
   4:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngcs	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x8, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x8
   8:	b.eq	14 <__negvdi2+0x14>  // b.none
   c:	neg	x0, x0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvdi2>
  20:	adrp	x2, 0 <__negvdi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w8, #0x80000000            	// #-2147483648
   4:	cmp	w0, w8
   8:	b.eq	14 <__negvsi2+0x14>  // b.none
   c:	neg	w0, w0
  10:	ret
  14:	stp	x29, x30, [sp, #-16]!
  18:	mov	x29, sp
  1c:	adrp	x0, 0 <__negvsi2>
  20:	adrp	x2, 0 <__negvsi2>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x16                  	// #22
  30:	bl	0 <__compilerrt_abort_impl>

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	eor	x8, x1, #0x8000000000000000
   4:	orr	x8, x0, x8
   8:	cbz	x8, 18 <__negvti2+0x18>
   c:	negs	x0, x0
  10:	ngcs	x1, x1
  14:	ret
  18:	stp	x29, x30, [sp, #-16]!
  1c:	mov	x29, sp
  20:	adrp	x0, 0 <__negvti2>
  24:	adrp	x2, 0 <__negvti2>
  28:	add	x0, x0, #0x0
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x18                  	// #24
  34:	bl	0 <__compilerrt_abort_impl>

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	lsr	x8, x0, #32
   4:	eor	w0, w8, w0
   8:	b	0 <__paritysi2>

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w8, w0, w0, lsr #16
   4:	eor	w8, w8, w8, lsr #8
   8:	eor	w8, w8, w8, lsr #4
   c:	and	w8, w8, #0xf
  10:	mov	w9, #0x6996                	// #27030
  14:	lsr	w8, w9, w8
  18:	and	w0, w8, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x0, x1, x0
   4:	b	0 <__paritydi2>

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x8, x0, #1
   4:	and	x8, x8, #0x5555555555555555
   8:	sub	x8, x0, x8
   c:	lsr	x9, x8, #2
  10:	and	x9, x9, #0x3333333333333333
  14:	and	x8, x8, #0x3333333333333333
  18:	add	x8, x9, x8
  1c:	add	x8, x8, x8, lsr #4
  20:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  24:	lsr	x9, x8, #32
  28:	add	w8, w9, w8
  2c:	add	w8, w8, w8, lsr #16
  30:	add	w8, w8, w8, lsr #8
  34:	and	w0, w8, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w8, w0, #1
   4:	and	w8, w8, #0x55555555
   8:	sub	w8, w0, w8
   c:	lsr	w9, w8, #2
  10:	and	w9, w9, #0x33333333
  14:	and	w8, w8, #0x33333333
  18:	add	w8, w9, w8
  1c:	add	w8, w8, w8, lsr #4
  20:	and	w8, w8, #0xf0f0f0f
  24:	add	w8, w8, w8, lsr #16
  28:	add	w8, w8, w8, lsr #8
  2c:	and	w0, w8, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	lsr	x8, x0, #1
   4:	lsr	x9, x1, #1
   8:	and	x8, x8, #0x5555555555555555
   c:	and	x9, x9, #0x5555555555555555
  10:	subs	x8, x0, x8
  14:	sbcs	x9, x1, x9
  18:	lsr	x10, x8, #2
  1c:	lsr	x11, x9, #2
  20:	and	x8, x8, #0x3333333333333333
  24:	and	x10, x10, #0x3333333333333333
  28:	and	x9, x9, #0x3333333333333333
  2c:	and	x11, x11, #0x3333333333333333
  30:	add	x8, x10, x8
  34:	add	x9, x11, x9
  38:	add	x8, x8, x8, lsr #4
  3c:	add	x9, x9, x9, lsr #4
  40:	and	x8, x8, #0xf0f0f0f0f0f0f0f
  44:	and	x9, x9, #0xf0f0f0f0f0f0f0f
  48:	add	x8, x9, x8
  4c:	lsr	x9, x8, #32
  50:	add	w8, w9, w8
  54:	add	w8, w8, w8, lsr #16
  58:	add	w8, w8, w8, lsr #8
  5c:	and	w0, w8, #0xff
  60:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	tst	w0, #0x1
   4:	fmov	d1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	d2, d1, d0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	44 <__powidf2+0x44>  // b.lo, b.ul, b.last
  18:	mov	w8, w0
  1c:	cmp	w8, #0x0
  20:	cinc	w8, w8, lt  // lt = tstop
  24:	fmul	d0, d0, d0
  28:	asr	w8, w8, #1
  2c:	fmul	d3, d0, d2
  30:	tst	w8, #0x1
  34:	add	w9, w8, #0x1
  38:	fcsel	d2, d2, d3, eq  // eq = none
  3c:	cmp	w9, #0x2
  40:	b.hi	1c <__powidf2+0x1c>  // b.pmore
  44:	fdiv	d0, d1, d2
  48:	cmp	w0, #0x0
  4c:	fcsel	d0, d0, d2, lt  // lt = tstop
  50:	ret

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	tst	w0, #0x1
   4:	fmov	s1, #1.000000000000000000e+00
   8:	add	w8, w0, #0x1
   c:	fcsel	s2, s1, s0, eq  // eq = none
  10:	cmp	w8, #0x3
  14:	b.cc	44 <__powisf2+0x44>  // b.lo, b.ul, b.last
  18:	mov	w8, w0
  1c:	cmp	w8, #0x0
  20:	cinc	w8, w8, lt  // lt = tstop
  24:	fmul	s0, s0, s0
  28:	asr	w8, w8, #1
  2c:	fmul	s3, s0, s2
  30:	tst	w8, #0x1
  34:	add	w9, w8, #0x1
  38:	fcsel	s2, s2, s3, eq  // eq = none
  3c:	cmp	w9, #0x2
  40:	b.hi	1c <__powisf2+0x1c>  // b.pmore
  44:	fdiv	s0, s1, s2
  48:	cmp	w0, #0x0
  4c:	fcsel	s0, s0, s2, lt  // lt = tstop
  50:	ret

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	fneg	d1, d1
   4:	b	0 <__adddf3>

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	fneg	s1, s1
   4:	b	0 <__addsf3>

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x8, x0
   c:	sub	x0, x0, x1
  10:	cmp	x0, x8
  14:	tbnz	x1, #63, 34 <__subvdi3+0x34>
  18:	b.le	38 <__subvdi3+0x38>
  1c:	adrp	x0, 0 <__subvdi3>
  20:	adrp	x2, 0 <__subvdi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvdi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvdi3>
  44:	adrp	x2, 0 <__subvdi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	w8, w0
   c:	sub	w0, w0, w1
  10:	cmp	w0, w8
  14:	tbnz	w1, #31, 34 <__subvsi3+0x34>
  18:	b.le	38 <__subvsi3+0x38>
  1c:	adrp	x0, 0 <__subvsi3>
  20:	adrp	x2, 0 <__subvsi3>
  24:	add	x0, x0, #0x0
  28:	add	x2, x2, #0x0
  2c:	mov	w1, #0x17                  	// #23
  30:	bl	0 <__compilerrt_abort_impl>
  34:	b.le	40 <__subvsi3+0x40>
  38:	ldp	x29, x30, [sp], #16
  3c:	ret
  40:	adrp	x0, 0 <__subvsi3>
  44:	adrp	x2, 0 <__subvsi3>
  48:	add	x0, x0, #0x0
  4c:	add	x2, x2, #0x0
  50:	mov	w1, #0x1a                  	// #26
  54:	bl	0 <__compilerrt_abort_impl>

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	mov	x9, x0
   c:	subs	x0, x0, x2
  10:	mov	x8, x1
  14:	sbcs	x1, x1, x3
  18:	cmp	x0, x9
  1c:	tbnz	x3, #63, 4c <__subvti3+0x4c>
  20:	cset	w9, ls  // ls = plast
  24:	cmp	x1, x8
  28:	cset	w8, le
  2c:	csel	w8, w9, w8, eq  // eq = none
  30:	tbnz	w8, #0, 60 <__subvti3+0x60>
  34:	adrp	x0, 0 <__subvti3>
  38:	adrp	x2, 0 <__subvti3>
  3c:	add	x0, x0, #0x0
  40:	add	x2, x2, #0x0
  44:	mov	w1, #0x19                  	// #25
  48:	bl	0 <__compilerrt_abort_impl>
  4c:	cset	w9, hi  // hi = pmore
  50:	cmp	x1, x8
  54:	cset	w8, gt
  58:	csel	w8, w9, w8, eq  // eq = none
  5c:	tbz	w8, #0, 68 <__subvti3+0x68>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	adrp	x0, 0 <__subvti3>
  6c:	adrp	x2, 0 <__subvti3>
  70:	add	x0, x0, #0x0
  74:	add	x2, x2, #0x0
  78:	mov	w1, #0x1c                  	// #28
  7c:	bl	0 <__compilerrt_abort_impl>

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	sub	sp, sp, #0x20
   4:	str	q1, [sp, #16]
   8:	ldp	x9, x8, [sp, #16]
   c:	eor	x8, x8, #0x8000000000000000
  10:	stp	x9, x8, [sp]
  14:	ldr	q1, [sp], #32
  18:	b	0 <__addtf3>

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	fmov	x8, d0
   4:	and	x9, x8, #0x7fffffffffffffff
   8:	mov	x10, #0xc0f0000000000000    	// #-4544132024016830464
   c:	mov	x11, #0xbf10000000000000    	// #-4679240012837945344
  10:	add	x10, x9, x10
  14:	add	x11, x9, x11
  18:	cmp	x10, x11
  1c:	b.cs	50 <__truncdfhf2+0x50>  // b.hs, b.nlast
  20:	mov	x9, #0x1                   	// #1
  24:	and	x10, x8, #0x3ffffffffff
  28:	movk	x9, #0x200, lsl #32
  2c:	cmp	x10, x9
  30:	lsr	x9, x8, #42
  34:	b.cc	78 <__truncdfhf2+0x78>  // b.lo, b.ul, b.last
  38:	mov	w10, #0x4001                	// #16385
  3c:	add	w9, w9, w10
  40:	lsr	x8, x8, #48
  44:	and	w8, w8, #0x8000
  48:	orr	w0, w9, w8
  4c:	ret
  50:	mov	x10, #0x1                   	// #1
  54:	movk	x10, #0x7ff0, lsl #48
  58:	cmp	x9, x10
  5c:	b.cc	a0 <__truncdfhf2+0xa0>  // b.lo, b.ul, b.last
  60:	ubfx	x9, x8, #42, #9
  64:	orr	w9, w9, #0x7e00
  68:	lsr	x8, x8, #48
  6c:	and	w8, w8, #0x8000
  70:	orr	w0, w9, w8
  74:	ret
  78:	mov	x11, #0x20000000000         	// #2199023255552
  7c:	cmp	x10, x11
  80:	add	w9, w9, #0x4, lsl #12
  84:	b.ne	40 <__truncdfhf2+0x40>  // b.any
  88:	and	w10, w9, #0x1
  8c:	add	w9, w10, w9
  90:	lsr	x8, x8, #48
  94:	and	w8, w8, #0x8000
  98:	orr	w0, w9, w8
  9c:	ret
  a0:	lsr	x9, x9, #52
  a4:	cmp	x9, #0x40e
  a8:	b.ls	c0 <__truncdfhf2+0xc0>  // b.plast
  ac:	mov	w9, #0x7c00                	// #31744
  b0:	lsr	x8, x8, #48
  b4:	and	w8, w8, #0x8000
  b8:	orr	w0, w9, w8
  bc:	ret
  c0:	cmp	w9, #0x3bd
  c4:	b.cs	dc <__truncdfhf2+0xdc>  // b.hs, b.nlast
  c8:	mov	w9, wzr
  cc:	lsr	x8, x8, #48
  d0:	and	w8, w8, #0x8000
  d4:	orr	w0, w9, w8
  d8:	ret
  dc:	mov	x10, #0x10000000000000      	// #4503599627370496
  e0:	mov	w11, #0x3f1                 	// #1009
  e4:	sub	w12, w9, #0x3b1
  e8:	bfxil	x10, x8, #0, #52
  ec:	sub	w9, w11, w9
  f0:	lsl	x11, x10, x12
  f4:	lsr	x10, x10, x9
  f8:	cmp	x11, #0x0
  fc:	and	x9, x10, #0x3ffffffffff
 100:	cset	w11, ne  // ne = any
 104:	orr	x11, x9, x11
 108:	mov	x9, #0x1                   	// #1
 10c:	movk	x9, #0x200, lsl #32
 110:	cmp	x11, x9
 114:	lsr	x9, x10, #42
 118:	b.cc	130 <__truncdfhf2+0x130>  // b.lo, b.ul, b.last
 11c:	add	w9, w9, #0x1
 120:	lsr	x8, x8, #48
 124:	and	w8, w8, #0x8000
 128:	orr	w0, w9, w8
 12c:	ret
 130:	mov	x12, #0x20000000000         	// #2199023255552
 134:	cmp	x11, x12
 138:	b.ne	40 <__truncdfhf2+0x40>  // b.any
 13c:	ubfx	x10, x10, #42, #1
 140:	add	w9, w10, w9
 144:	lsr	x8, x8, #48
 148:	and	w8, w8, #0x8000
 14c:	orr	w0, w9, w8
 150:	ret

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	fmov	x8, d0
   4:	and	x9, x8, #0x7fffffffffffffff
   8:	mov	x10, #0xc7f0000000000000    	// #-4039728865751334912
   c:	mov	x11, #0xb810000000000000    	// #-5183643171103440896
  10:	add	x10, x9, x10
  14:	add	x11, x9, x11
  18:	cmp	x10, x11
  1c:	b.cs	48 <__truncdfsf2+0x48>  // b.hs, b.nlast
  20:	mov	w10, #0x1                   	// #1
  24:	and	x9, x8, #0x1fffffff
  28:	movk	w10, #0x1000, lsl #16
  2c:	cmp	x9, x10
  30:	lsr	x10, x8, #29
  34:	b.cc	64 <__truncdfsf2+0x64>  // b.lo, b.ul, b.last
  38:	mov	w9, #0x1                   	// #1
  3c:	movk	w9, #0x4000, lsl #16
  40:	add	w9, w10, w9
  44:	b	fc <__truncdfsf2+0xfc>
  48:	mov	x10, #0x1                   	// #1
  4c:	movk	x10, #0x7ff0, lsl #48
  50:	cmp	x9, x10
  54:	b.cc	84 <__truncdfsf2+0x84>  // b.lo, b.ul, b.last
  58:	ubfx	x9, x8, #29, #22
  5c:	orr	w9, w9, #0x7fc00000
  60:	b	fc <__truncdfsf2+0xfc>
  64:	mov	w11, #0x40000000            	// #1073741824
  68:	mov	w12, #0x10000000            	// #268435456
  6c:	cmp	x9, x12
  70:	add	w9, w10, w11
  74:	b.ne	fc <__truncdfsf2+0xfc>  // b.any
  78:	and	w10, w9, #0x1
  7c:	add	w9, w10, w9
  80:	b	fc <__truncdfsf2+0xfc>
  84:	lsr	x9, x9, #52
  88:	cmp	x9, #0x47e
  8c:	b.ls	98 <__truncdfsf2+0x98>  // b.plast
  90:	mov	w9, #0x7f800000            	// #2139095040
  94:	b	fc <__truncdfsf2+0xfc>
  98:	cmp	w9, #0x34d
  9c:	b.cs	a8 <__truncdfsf2+0xa8>  // b.hs, b.nlast
  a0:	mov	w9, wzr
  a4:	b	fc <__truncdfsf2+0xfc>
  a8:	mov	x10, #0x10000000000000      	// #4503599627370496
  ac:	mov	w11, #0x381                 	// #897
  b0:	sub	w12, w9, #0x341
  b4:	bfxil	x10, x8, #0, #52
  b8:	sub	w9, w11, w9
  bc:	lsl	x11, x10, x12
  c0:	lsr	x9, x10, x9
  c4:	cmp	x11, #0x0
  c8:	and	x10, x9, #0x1fffffff
  cc:	cset	w11, ne  // ne = any
  d0:	orr	x10, x10, x11
  d4:	mov	w11, #0x1                   	// #1
  d8:	movk	w11, #0x1000, lsl #16
  dc:	cmp	x10, x11
  e0:	lsr	x9, x9, #29
  e4:	b.cc	f0 <__truncdfsf2+0xf0>  // b.lo, b.ul, b.last
  e8:	add	w9, w9, #0x1
  ec:	b	fc <__truncdfsf2+0xfc>
  f0:	mov	w11, #0x10000000            	// #268435456
  f4:	cmp	x10, x11
  f8:	b.eq	78 <__truncdfsf2+0x78>  // b.none
  fc:	lsr	x8, x8, #32
 100:	and	w8, w8, #0x80000000
 104:	orr	w8, w9, w8
 108:	fmov	s0, w8
 10c:	ret

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	fmov	w8, s0
   4:	and	w9, w8, #0x7fffffff
   8:	mov	w10, #0xc7800000            	// #-947912704
   c:	mov	w11, #0xb8800000            	// #-1199570944
  10:	add	w10, w9, w10
  14:	add	w11, w9, w11
  18:	cmp	w10, w11
  1c:	b.cs	54 <__truncsfhf2+0x54>  // b.hs, b.nlast
  20:	ubfx	w10, w8, #13, #16
  24:	and	w9, w8, #0x1fff
  28:	cmp	w9, #0x1, lsl #12
  2c:	sub	w9, w10, #0x1c, lsl #12
  30:	b.ls	7c <__truncsfhf2+0x7c>  // b.plast
  34:	mov	w9, #0x4000                	// #16384
  38:	movk	w9, #0xfffe, lsl #16
  3c:	add	w9, w9, w10
  40:	add	w9, w9, #0x1
  44:	lsr	w8, w8, #16
  48:	and	w8, w8, #0x8000
  4c:	orr	w0, w9, w8
  50:	ret
  54:	mov	w10, #0x1                   	// #1
  58:	movk	w10, #0x7f80, lsl #16
  5c:	cmp	w9, w10
  60:	b.cc	98 <__truncsfhf2+0x98>  // b.lo, b.ul, b.last
  64:	mov	w9, #0x7e00                	// #32256
  68:	bfxil	w9, w8, #13, #9
  6c:	lsr	w8, w8, #16
  70:	and	w8, w8, #0x8000
  74:	orr	w0, w9, w8
  78:	ret
  7c:	b.ne	44 <__truncsfhf2+0x44>  // b.any
  80:	and	w10, w9, #0x1
  84:	add	w9, w10, w9, uxth
  88:	lsr	w8, w8, #16
  8c:	and	w8, w8, #0x8000
  90:	orr	w0, w9, w8
  94:	ret
  98:	lsr	w10, w9, #23
  9c:	cmp	w10, #0x8e
  a0:	b.ls	b8 <__truncsfhf2+0xb8>  // b.plast
  a4:	mov	w9, #0x7c00                	// #31744
  a8:	lsr	w8, w8, #16
  ac:	and	w8, w8, #0x8000
  b0:	orr	w0, w9, w8
  b4:	ret
  b8:	lsr	w9, w9, #24
  bc:	cmp	w9, #0x2d
  c0:	b.cs	d8 <__truncsfhf2+0xd8>  // b.hs, b.nlast
  c4:	mov	w9, wzr
  c8:	lsr	w8, w8, #16
  cc:	and	w8, w8, #0x8000
  d0:	orr	w0, w9, w8
  d4:	ret
  d8:	mov	w9, #0x800000              	// #8388608
  dc:	mov	w11, #0x71                  	// #113
  e0:	sub	w12, w10, #0x51
  e4:	bfxil	w9, w8, #0, #23
  e8:	sub	w10, w11, w10
  ec:	lsl	w11, w9, w12
  f0:	lsr	w10, w9, w10
  f4:	cmp	w11, #0x0
  f8:	cset	w9, ne  // ne = any
  fc:	and	w11, w10, #0x1fff
 100:	orr	w9, w11, w9
 104:	cmp	w9, #0x1, lsl #12
 108:	lsr	w9, w10, #13
 10c:	b.ls	124 <__truncsfhf2+0x124>  // b.plast
 110:	add	w9, w9, #0x1
 114:	lsr	w8, w8, #16
 118:	and	w8, w8, #0x8000
 11c:	orr	w0, w9, w8
 120:	ret
 124:	b.ne	44 <__truncsfhf2+0x44>  // b.any
 128:	ubfx	w10, w10, #13, #1
 12c:	add	w9, w9, w10
 130:	lsr	w8, w8, #16
 134:	and	w8, w8, #0x8000
 138:	orr	w0, w9, w8
 13c:	ret

0000000000000140 <__gnu_f2h_ieee>:
 140:	b	0 <__truncsfhf2>

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cmp	w8, w9
   c:	b.cs	18 <__ucmpdi2+0x18>  // b.hs, b.nlast
  10:	mov	w0, wzr
  14:	ret
  18:	b.ls	24 <__ucmpdi2+0x24>  // b.plast
  1c:	mov	w0, #0x2                   	// #2
  20:	ret
  24:	cmp	w0, w1
  28:	b.cs	34 <__ucmpdi2+0x34>  // b.hs, b.nlast
  2c:	mov	w0, wzr
  30:	ret
  34:	mov	w8, #0x1                   	// #1
  38:	cinc	w0, w8, hi  // hi = pmore
  3c:	ret

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	cmp	x1, x3
   4:	b.cs	10 <__ucmpti2+0x10>  // b.hs, b.nlast
   8:	mov	w0, wzr
   c:	ret
  10:	b.ls	1c <__ucmpti2+0x1c>  // b.plast
  14:	mov	w0, #0x2                   	// #2
  18:	ret
  1c:	cmp	x0, x2
  20:	b.cs	2c <__ucmpti2+0x2c>  // b.hs, b.nlast
  24:	mov	w0, wzr
  28:	ret
  2c:	mov	w8, #0x1                   	// #1
  30:	cinc	w0, w8, hi  // hi = pmore
  34:	ret

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	mov	x2, xzr
   4:	b	0 <__udivmoddi4>

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	lsr	x8, x0, #32
   4:	lsr	x9, x1, #32
   8:	cbz	w8, 38 <__udivmoddi4+0x38>
   c:	cbz	w1, 50 <__udivmoddi4+0x50>
  10:	cbz	w9, 8c <__udivmoddi4+0x8c>
  14:	clz	w9, w9
  18:	clz	w10, w8
  1c:	sub	w12, w9, w10
  20:	cmp	w12, #0x20
  24:	b.cc	f0 <__udivmoddi4+0xf0>  // b.lo, b.ul, b.last
  28:	cbz	x2, 1f8 <__udivmoddi4+0x1f8>
  2c:	str	x0, [x2]
  30:	mov	x0, xzr
  34:	ret
  38:	cbz	w9, d8 <__udivmoddi4+0xd8>
  3c:	cbz	x2, 1f8 <__udivmoddi4+0x1f8>
  40:	and	x8, x0, #0xffffffff
  44:	mov	x0, xzr
  48:	str	x8, [x2]
  4c:	ret
  50:	cbz	w9, 124 <__udivmoddi4+0x124>
  54:	cbz	w0, 1c8 <__udivmoddi4+0x1c8>
  58:	sub	w10, w9, #0x1
  5c:	tst	w9, w10
  60:	b.ne	1e0 <__udivmoddi4+0x1e0>  // b.any
  64:	cbz	x2, 7c <__udivmoddi4+0x7c>
  68:	mov	w10, #0xffffffff            	// #-1
  6c:	add	x10, x9, x10
  70:	and	w10, w10, w8
  74:	bfi	x0, x10, #32, #32
  78:	str	x0, [x2]
  7c:	rbit	w9, w9
  80:	clz	w9, w9
  84:	lsr	w0, w8, w9
  88:	ret
  8c:	sub	w9, w1, #0x1
  90:	tst	w1, w9
  94:	b.ne	134 <__udivmoddi4+0x134>  // b.any
  98:	cbz	x2, ac <__udivmoddi4+0xac>
  9c:	mov	w9, #0xffffffff            	// #-1
  a0:	add	x9, x1, x9
  a4:	and	w9, w0, w9
  a8:	str	x9, [x2]
  ac:	cmp	w1, #0x1
  b0:	b.eq	1c4 <__udivmoddi4+0x1c4>  // b.none
  b4:	rbit	w9, w1
  b8:	clz	w9, w9
  bc:	neg	w11, w9
  c0:	lsr	w10, w8, w9
  c4:	lsl	w8, w8, w11
  c8:	lsr	w9, w0, w9
  cc:	orr	w0, w8, w9
  d0:	bfi	x0, x10, #32, #32
  d4:	ret
  d8:	udiv	w8, w0, w1
  dc:	cbz	x2, e8 <__udivmoddi4+0xe8>
  e0:	msub	w9, w8, w1, w0
  e4:	str	x9, [x2]
  e8:	mov	x0, x8
  ec:	ret
  f0:	add	w9, w12, #0x1
  f4:	cmp	w9, #0x20
  f8:	b.eq	14c <__udivmoddi4+0x14c>  // b.none
  fc:	mov	w13, #0x1f                  	// #31
 100:	sub	w12, w13, w12
 104:	lsr	w10, w8, w9
 108:	lsr	w14, w0, w9
 10c:	lsl	w13, w0, w12
 110:	lsl	w8, w8, w12
 114:	mov	w11, wzr
 118:	orr	w8, w8, w14
 11c:	mov	w0, w13
 120:	b	154 <__udivmoddi4+0x154>
 124:	mov	x0, xzr
 128:	cbz	x2, 1c4 <__udivmoddi4+0x1c4>
 12c:	str	xzr, [x2]
 130:	ret
 134:	clz	w9, w1
 138:	clz	w10, w8
 13c:	sub	w9, w9, w10
 140:	add	w9, w9, #0x21
 144:	cmp	w9, #0x20
 148:	b.ne	200 <__udivmoddi4+0x200>  // b.any
 14c:	mov	w10, wzr
 150:	mov	w11, wzr
 154:	mov	w13, wzr
 158:	mov	w12, w0
 15c:	extr	w10, w10, w8, #31
 160:	extr	w8, w8, w12, #31
 164:	bfi	x8, x10, #32, #32
 168:	mvn	x10, x8
 16c:	add	x10, x10, x1
 170:	asr	x10, x10, #63
 174:	extr	w12, w12, w11, #31
 178:	orr	w11, w13, w11, lsl #1
 17c:	and	w13, w10, #0x1
 180:	and	x10, x10, x1
 184:	sub	x8, x8, x10
 188:	subs	w9, w9, #0x1
 18c:	lsr	x10, x8, #32
 190:	b.ne	15c <__udivmoddi4+0x15c>  // b.any
 194:	mov	w9, w11
 198:	lsl	x9, x9, #1
 19c:	lsl	x11, x12, #33
 1a0:	and	x12, x9, #0x100000000
 1a4:	cbz	x2, 1b4 <__udivmoddi4+0x1b4>
 1a8:	mov	w8, w8
 1ac:	bfi	x8, x10, #32, #32
 1b0:	str	x8, [x2]
 1b4:	and	x8, x9, #0xfffffffe
 1b8:	orr	x9, x12, x11
 1bc:	orr	x8, x9, x8
 1c0:	orr	x0, x8, x13
 1c4:	ret
 1c8:	udiv	w0, w8, w9
 1cc:	cbz	x2, 1c4 <__udivmoddi4+0x1c4>
 1d0:	msub	w8, w0, w9, w8
 1d4:	lsl	x8, x8, #32
 1d8:	str	x8, [x2]
 1dc:	ret
 1e0:	clz	w9, w9
 1e4:	clz	w10, w8
 1e8:	sub	w10, w9, w10
 1ec:	cmp	w10, #0x1f
 1f0:	b.cc	22c <__udivmoddi4+0x22c>  // b.lo, b.ul, b.last
 1f4:	cbnz	x2, 2c <__udivmoddi4+0x2c>
 1f8:	mov	x0, xzr
 1fc:	ret
 200:	b.cs	258 <__udivmoddi4+0x258>  // b.hs, b.nlast
 204:	neg	w13, w9
 208:	lsr	w10, w8, w9
 20c:	lsl	w12, w0, w13
 210:	lsl	w8, w8, w13
 214:	lsr	w13, w0, w9
 218:	orr	w8, w8, w13
 21c:	cbz	w9, 278 <__udivmoddi4+0x278>
 220:	mov	w11, wzr
 224:	mov	w0, w12
 228:	b	154 <__udivmoddi4+0x154>
 22c:	mov	w12, #0x1f                  	// #31
 230:	add	w9, w10, #0x1
 234:	sub	w12, w12, w10
 238:	lsr	w10, w8, w9
 23c:	lsr	w13, w0, w9
 240:	lsl	w14, w0, w12
 244:	lsl	w8, w8, w12
 248:	mov	w11, wzr
 24c:	orr	w8, w8, w13
 250:	mov	w0, w14
 254:	b	154 <__udivmoddi4+0x154>
 258:	neg	w12, w9
 25c:	lsr	w13, w0, w9
 260:	lsl	w11, w0, w12
 264:	lsl	w12, w8, w12
 268:	mov	w10, wzr
 26c:	orr	w0, w12, w13
 270:	lsr	w8, w8, w9
 274:	b	154 <__udivmoddi4+0x154>
 278:	mov	x9, xzr
 27c:	mov	x13, xzr
 280:	lsl	x11, x12, #33
 284:	and	x12, x9, #0x100000000
 288:	cbnz	x2, 1a8 <__udivmoddi4+0x1a8>
 28c:	b	1b4 <__udivmoddi4+0x1b4>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	str	x21, [sp, #16]
   8:	stp	x20, x19, [sp, #32]
   c:	mov	x29, sp
  10:	mov	x19, x2
  14:	mov	w20, w1
  18:	mov	w21, w0
  1c:	bl	0 <__udivsi3>
  20:	msub	w8, w0, w20, w21
  24:	str	w8, [x19]
  28:	ldp	x20, x19, [sp, #32]
  2c:	ldr	x21, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	cbz	x1, 2c <__udivmodti4+0x2c>
   4:	cbz	x2, 44 <__udivmodti4+0x44>
   8:	cbz	x3, 78 <__udivmodti4+0x78>
   c:	clz	x8, x3
  10:	clz	x9, x1
  14:	sub	w11, w8, w9
  18:	cmp	w11, #0x40
  1c:	b.cc	d4 <__udivmodti4+0xd4>  // b.lo, b.ul, b.last
  20:	cbz	x4, 1e8 <__udivmodti4+0x1e8>
  24:	stp	x0, x1, [x4]
  28:	b	1e8 <__udivmodti4+0x1e8>
  2c:	cbz	x3, b8 <__udivmodti4+0xb8>
  30:	cbz	x4, 1e8 <__udivmodti4+0x1e8>
  34:	mov	x1, xzr
  38:	stp	x0, xzr, [x4]
  3c:	mov	x0, xzr
  40:	ret
  44:	cbz	x3, 104 <__udivmodti4+0x104>
  48:	cbz	x0, 1b8 <__udivmodti4+0x1b8>
  4c:	sub	x8, x3, #0x1
  50:	tst	x3, x8
  54:	b.ne	1d0 <__udivmodti4+0x1d0>  // b.any
  58:	cbz	x4, 64 <__udivmodti4+0x64>
  5c:	and	x8, x8, x1
  60:	stp	x0, x8, [x4]
  64:	rbit	x8, x3
  68:	clz	x8, x8
  6c:	lsr	x0, x1, x8
  70:	mov	x1, xzr
  74:	ret
  78:	sub	x8, x2, #0x1
  7c:	tst	x2, x8
  80:	b.ne	118 <__udivmodti4+0x118>  // b.any
  84:	cbz	x4, 90 <__udivmodti4+0x90>
  88:	and	x8, x8, x0
  8c:	stp	x8, xzr, [x4]
  90:	cmp	x2, #0x1
  94:	b.eq	1b4 <__udivmodti4+0x1b4>  // b.none
  98:	rbit	x8, x2
  9c:	clz	x8, x8
  a0:	neg	x9, x8
  a4:	lsl	x9, x1, x9
  a8:	lsr	x1, x1, x8
  ac:	lsr	x8, x0, x8
  b0:	orr	x0, x9, x8
  b4:	ret
  b8:	udiv	x8, x0, x2
  bc:	cbz	x4, c8 <__udivmodti4+0xc8>
  c0:	msub	x9, x8, x2, x0
  c4:	stp	x9, xzr, [x4]
  c8:	mov	x1, xzr
  cc:	mov	x0, x8
  d0:	ret
  d4:	add	w8, w11, #0x1
  d8:	cmp	w8, #0x40
  dc:	b.eq	130 <__udivmodti4+0x130>  // b.none
  e0:	mov	w12, #0x3f                  	// #63
  e4:	sub	w11, w12, w11
  e8:	lsr	x13, x0, x8
  ec:	lsl	x12, x1, x11
  f0:	mov	x10, xzr
  f4:	lsr	x9, x1, x8
  f8:	orr	x1, x12, x13
  fc:	lsl	x0, x0, x11
 100:	b	13c <__udivmodti4+0x13c>
 104:	cbz	x4, 1e8 <__udivmodti4+0x1e8>
 108:	mov	x0, xzr
 10c:	mov	x1, xzr
 110:	stp	xzr, xzr, [x4]
 114:	ret
 118:	clz	x8, x2
 11c:	clz	x9, x1
 120:	sub	w8, w8, w9
 124:	add	w8, w8, #0x41
 128:	cmp	w8, #0x40
 12c:	b.ne	1f4 <__udivmodti4+0x1f4>  // b.any
 130:	mov	x9, xzr
 134:	mov	x10, xzr
 138:	mov	w8, #0x40                  	// #64
 13c:	mov	w12, wzr
 140:	mov	x11, x0
 144:	extr	x13, x1, x11, #63
 148:	extr	x9, x9, x1, #63
 14c:	mov	w12, w12
 150:	mvn	x14, x13
 154:	extr	x11, x11, x10, #63
 158:	orr	x10, x12, x10, lsl #1
 15c:	mvn	x12, x9
 160:	cmn	x14, x2
 164:	adcs	x12, x12, x3
 168:	asr	x12, x12, #63
 16c:	and	x15, x12, x2
 170:	and	x14, x12, x3
 174:	subs	x1, x13, x15
 178:	sbcs	x9, x9, x14
 17c:	subs	w8, w8, #0x1
 180:	and	w12, w12, #0x1
 184:	b.ne	144 <__udivmodti4+0x144>  // b.any
 188:	mov	x8, xzr
 18c:	lsr	x13, x10, #63
 190:	lsl	x10, x10, #1
 194:	lsl	x11, x11, #1
 198:	and	x13, x13, #0x1
 19c:	cbz	x4, 1a4 <__udivmodti4+0x1a4>
 1a0:	stp	x1, x9, [x4]
 1a4:	and	x9, x10, #0xfffffffffffffffe
 1a8:	orr	x10, x13, x11
 1ac:	orr	x0, x9, x12
 1b0:	orr	x1, x10, x8
 1b4:	ret
 1b8:	udiv	x0, x1, x3
 1bc:	cbz	x4, 1c8 <__udivmodti4+0x1c8>
 1c0:	msub	x8, x0, x3, x1
 1c4:	stp	xzr, x8, [x4]
 1c8:	mov	x1, xzr
 1cc:	ret
 1d0:	clz	x8, x3
 1d4:	clz	x9, x1
 1d8:	sub	w9, w8, w9
 1dc:	cmp	w9, #0x3f
 1e0:	b.cc	220 <__udivmodti4+0x220>  // b.lo, b.ul, b.last
 1e4:	cbnz	x4, 24 <__udivmodti4+0x24>
 1e8:	mov	x0, xzr
 1ec:	mov	x1, xzr
 1f0:	ret
 1f4:	b.cs	248 <__udivmodti4+0x248>  // b.hs, b.nlast
 1f8:	neg	w10, w8
 1fc:	lsl	x11, x0, x10
 200:	lsl	x10, x1, x10
 204:	lsr	x12, x0, x8
 208:	lsr	x9, x1, x8
 20c:	orr	x1, x10, x12
 210:	mov	x10, xzr
 214:	cbz	w8, 268 <__udivmodti4+0x268>
 218:	mov	x0, x11
 21c:	b	13c <__udivmodti4+0x13c>
 220:	mov	w11, #0x3f                  	// #63
 224:	add	w8, w9, #0x1
 228:	sub	w11, w11, w9
 22c:	lsr	x12, x0, x8
 230:	lsl	x0, x0, x11
 234:	lsl	x11, x1, x11
 238:	mov	x10, xzr
 23c:	lsr	x9, x1, x8
 240:	orr	x1, x11, x12
 244:	b	13c <__udivmodti4+0x13c>
 248:	neg	w11, w8
 24c:	lsr	x12, x0, x8
 250:	lsl	x10, x0, x11
 254:	lsl	x11, x1, x11
 258:	mov	x9, xzr
 25c:	orr	x0, x11, x12
 260:	lsr	x1, x1, x8
 264:	b	13c <__udivmodti4+0x13c>
 268:	mov	x13, xzr
 26c:	mov	x12, xzr
 270:	mov	x8, xzr
 274:	lsl	x11, x11, #1
 278:	and	x13, x13, #0x1
 27c:	cbnz	x4, 1a0 <__udivmodti4+0x1a0>
 280:	b	1a4 <__udivmodti4+0x1a4>

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	mov	w8, w0
   4:	mov	w0, wzr
   8:	cbz	w8, 84 <__udivsi3+0x84>
   c:	cbz	w1, 84 <__udivsi3+0x84>
  10:	clz	w10, w1
  14:	clz	w9, w8
  18:	sub	w11, w10, w9
  1c:	cmp	w11, #0x1f
  20:	b.ls	2c <__udivsi3+0x2c>  // b.plast
  24:	mov	w0, wzr
  28:	ret
  2c:	b.ne	38 <__udivsi3+0x38>  // b.any
  30:	mov	w0, w8
  34:	ret
  38:	mov	w13, #0x1f                  	// #31
  3c:	add	w12, w11, #0x1
  40:	mvn	w14, w10
  44:	sub	w11, w13, w11
  48:	mov	w0, wzr
  4c:	lsr	w10, w8, w12
  50:	lsl	w8, w8, w11
  54:	add	w9, w14, w9
  58:	extr	w10, w10, w8, #31
  5c:	mvn	w11, w10
  60:	add	w11, w11, w1
  64:	asr	w11, w11, #31
  68:	orr	w8, w0, w8, lsl #1
  6c:	and	w0, w11, #0x1
  70:	and	w11, w11, w1
  74:	adds	w9, w9, #0x1
  78:	sub	w10, w10, w11
  7c:	b.cc	58 <__udivsi3+0x58>  // b.lo, b.ul, b.last
  80:	bfi	w0, w8, #1, #31
  84:	ret

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, xzr
   4:	b	0 <__udivmodti4>

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	add	x2, sp, #0x8
  10:	bl	0 <__udivmoddi4>
  14:	ldr	x0, [sp, #8]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	stp	x20, x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x20, x19, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	sub	sp, sp, #0x20
   4:	stp	x29, x30, [sp, #16]
   8:	add	x29, sp, #0x10
   c:	mov	x4, sp
  10:	bl	0 <__udivmodti4>
  14:	ldp	x0, x1, [sp]
  18:	ldp	x29, x30, [sp, #16]
  1c:	add	sp, sp, #0x20
  20:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__emutls_get_address>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	stp	x24, x23, [sp, #16]
   8:	stp	x22, x21, [sp, #32]
   c:	stp	x20, x19, [sp, #48]
  10:	mov	x29, sp
  14:	add	x8, x0, #0x10
  18:	ldar	x22, [x8]
  1c:	mov	x19, x0
  20:	cbz	x22, 84 <__emutls_get_address+0x84>
  24:	adrp	x23, 0 <__emutls_get_address>
  28:	ldr	w0, [x23]
  2c:	bl	0 <pthread_getspecific>
  30:	cbz	x0, e0 <__emutls_get_address+0xe0>
  34:	ldr	x21, [x0, #8]
  38:	mov	x20, x0
  3c:	cmp	x21, x22
  40:	b.cs	124 <__emutls_get_address+0x124>  // b.hs, b.nlast
  44:	add	x8, x22, #0x11
  48:	and	x8, x8, #0xfffffffffffffff0
  4c:	sub	x24, x8, #0x2
  50:	lsl	x8, x24, #3
  54:	add	x1, x8, #0x10
  58:	mov	x0, x20
  5c:	bl	0 <realloc>
  60:	cbz	x0, 1b4 <__emutls_get_address+0x1b4>
  64:	add	x8, x0, x21, lsl #3
  68:	sub	x9, x24, x21
  6c:	mov	x20, x0
  70:	add	x0, x8, #0x10
  74:	lsl	x2, x9, #3
  78:	mov	w1, wzr
  7c:	bl	0 <memset>
  80:	b	114 <__emutls_get_address+0x114>
  84:	adrp	x20, 0 <__emutls_get_address>
  88:	add	x20, x20, #0x0
  8c:	adrp	x1, 0 <__emutls_get_address>
  90:	add	x1, x1, #0x0
  94:	mov	x0, x20
  98:	bl	0 <pthread_once>
  9c:	add	x0, x20, #0x10
  a0:	bl	0 <pthread_mutex_lock>
  a4:	ldr	x22, [x19, #16]
  a8:	cbnz	x22, c4 <__emutls_get_address+0xc4>
  ac:	adrp	x8, 0 <__emutls_get_address>
  b0:	ldr	x9, [x8]
  b4:	add	x10, x19, #0x10
  b8:	add	x22, x9, #0x1
  bc:	str	x22, [x8]
  c0:	stlr	x22, [x10]
  c4:	adrp	x0, 0 <__emutls_get_address>
  c8:	add	x0, x0, #0x0
  cc:	bl	0 <pthread_mutex_unlock>
  d0:	adrp	x23, 0 <__emutls_get_address>
  d4:	ldr	w0, [x23]
  d8:	bl	0 <pthread_getspecific>
  dc:	cbnz	x0, 34 <__emutls_get_address+0x34>
  e0:	add	x8, x22, #0x11
  e4:	and	x8, x8, #0xfffffffffffffff0
  e8:	sub	x24, x8, #0x2
  ec:	lsl	x21, x24, #3
  f0:	add	x0, x21, #0x10
  f4:	bl	0 <malloc>
  f8:	cbz	x0, 1b4 <__emutls_get_address+0x1b4>
  fc:	mov	x20, x0
 100:	add	x0, x0, #0x10
 104:	mov	w1, wzr
 108:	mov	x2, x21
 10c:	bl	0 <memset>
 110:	str	xzr, [x20]
 114:	ldr	w0, [x23]
 118:	mov	x1, x20
 11c:	str	x24, [x20, #8]
 120:	bl	0 <pthread_setspecific>
 124:	sub	x8, x22, #0x1
 128:	add	x22, x20, x8, lsl #3
 12c:	ldr	x20, [x22, #16]!
 130:	cbnz	x20, 19c <__emutls_get_address+0x19c>
 134:	ldr	x8, [x19, #8]
 138:	mov	w9, #0x8                   	// #8
 13c:	cmp	x8, #0x8
 140:	csel	x20, x8, x9, hi  // hi = pmore
 144:	sub	x8, x20, #0x1
 148:	tst	x20, x8
 14c:	b.ne	1b4 <__emutls_get_address+0x1b4>  // b.any
 150:	ldr	x21, [x19]
 154:	add	x23, x20, #0x7
 158:	add	x0, x23, x21
 15c:	bl	0 <malloc>
 160:	cbz	x0, 1b4 <__emutls_get_address+0x1b4>
 164:	add	x8, x0, x23
 168:	neg	x9, x20
 16c:	and	x20, x8, x9
 170:	stur	x0, [x20, #-8]
 174:	ldr	x1, [x19, #24]
 178:	cbz	x1, 18c <__emutls_get_address+0x18c>
 17c:	mov	x0, x20
 180:	mov	x2, x21
 184:	bl	0 <memcpy>
 188:	b	198 <__emutls_get_address+0x198>
 18c:	mov	x0, x20
 190:	mov	x2, x21
 194:	bl	0 <memset>
 198:	str	x20, [x22]
 19c:	mov	x0, x20
 1a0:	ldp	x20, x19, [sp, #48]
 1a4:	ldp	x22, x21, [sp, #32]
 1a8:	ldp	x24, x23, [sp, #16]
 1ac:	ldp	x29, x30, [sp], #64
 1b0:	ret
 1b4:	bl	0 <abort>

00000000000001b8 <emutls_init>:
 1b8:	stp	x29, x30, [sp, #-16]!
 1bc:	mov	x29, sp
 1c0:	adrp	x0, 0 <__emutls_get_address>
 1c4:	adrp	x1, 0 <__emutls_get_address>
 1c8:	add	x0, x0, #0x0
 1cc:	add	x1, x1, #0x0
 1d0:	bl	0 <pthread_key_create>
 1d4:	cbnz	w0, 1e0 <emutls_init+0x28>
 1d8:	ldp	x29, x30, [sp], #16
 1dc:	ret
 1e0:	bl	0 <abort>

00000000000001e4 <emutls_key_destructor>:
 1e4:	stp	x29, x30, [sp, #-48]!
 1e8:	str	x21, [sp, #16]
 1ec:	stp	x20, x19, [sp, #32]
 1f0:	mov	x29, sp
 1f4:	ldr	x8, [x0]
 1f8:	mov	x19, x0
 1fc:	cbz	x8, 224 <emutls_key_destructor+0x40>
 200:	sub	x8, x8, #0x1
 204:	str	x8, [x19]
 208:	adrp	x8, 0 <__emutls_get_address>
 20c:	ldr	w0, [x8]
 210:	mov	x1, x19
 214:	ldp	x20, x19, [sp, #32]
 218:	ldr	x21, [sp, #16]
 21c:	ldp	x29, x30, [sp], #48
 220:	b	0 <pthread_setspecific>
 224:	ldr	x8, [x19, #8]
 228:	cbz	x8, 25c <emutls_key_destructor+0x78>
 22c:	mov	x20, xzr
 230:	add	x21, x19, #0x10
 234:	b	244 <emutls_key_destructor+0x60>
 238:	add	x20, x20, #0x1
 23c:	cmp	x20, x8
 240:	b.cs	25c <emutls_key_destructor+0x78>  // b.hs, b.nlast
 244:	ldr	x9, [x21, x20, lsl #3]
 248:	cbz	x9, 238 <emutls_key_destructor+0x54>
 24c:	ldur	x0, [x9, #-8]
 250:	bl	0 <free>
 254:	ldr	x8, [x19, #8]
 258:	b	238 <emutls_key_destructor+0x54>
 25c:	mov	x0, x19
 260:	ldp	x20, x19, [sp, #32]
 264:	ldr	x21, [sp, #16]
 268:	ldp	x29, x30, [sp], #48
 26c:	b	0 <free>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	neg	x8, x0
  1c:	add	x9, x19, x0
  20:	and	x0, x8, x19
  24:	ldr	x19, [sp, #16]
  28:	add	x9, x9, #0x30
  2c:	and	x8, x9, x8
  30:	sub	x1, x8, x0
  34:	mov	w2, #0x7                   	// #7
  38:	ldp	x29, x30, [sp], #32
  3c:	b	0 <mprotect>

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	str	x19, [sp, #16]
   8:	mov	x29, sp
   c:	adrp	x19, 0 <stderr>
  10:	ldr	x19, [x19]
  14:	mov	x4, x3
  18:	mov	x3, x2
  1c:	mov	x2, x1
  20:	ldr	x8, [x19]
  24:	mov	x1, x0
  28:	mov	x0, x8
  2c:	bl	0 <fprintf>
  30:	ldr	x0, [x19]
  34:	bl	0 <fflush>
  38:	adrp	x0, 0 <__eprintf>
  3c:	adrp	x2, 0 <__eprintf>
  40:	add	x0, x0, #0x0
  44:	add	x2, x2, #0x0
  48:	mov	w1, #0x1a                  	// #26
  4c:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__gcc_personality_v0>:
   0:	tbnz	w1, #0, 17c <__gcc_personality_v0+0x17c>
   4:	sub	sp, sp, #0x70
   8:	stp	x29, x30, [sp, #16]
   c:	str	x27, [sp, #32]
  10:	stp	x26, x25, [sp, #48]
  14:	stp	x24, x23, [sp, #64]
  18:	stp	x22, x21, [sp, #80]
  1c:	stp	x20, x19, [sp, #96]
  20:	add	x29, sp, #0x10
  24:	mov	x0, x4
  28:	mov	x19, x4
  2c:	mov	x20, x3
  30:	bl	0 <_Unwind_GetLanguageSpecificData>
  34:	str	x0, [x29, #24]
  38:	cbz	x0, 184 <__gcc_personality_v0+0x184>
  3c:	mov	x21, x0
  40:	mov	x0, x19
  44:	bl	0 <_Unwind_GetIP>
  48:	mov	x23, x0
  4c:	mov	x0, x19
  50:	bl	0 <_Unwind_GetRegionStart>
  54:	add	x8, x21, #0x1
  58:	str	x8, [x29, #24]
  5c:	ldrb	w1, [x21]
  60:	mov	x21, x0
  64:	cmp	w1, #0xff
  68:	b.eq	78 <__gcc_personality_v0+0x78>  // b.none
  6c:	add	x0, x29, #0x18
  70:	bl	1a8 <readEncodedPointer>
  74:	ldr	x8, [x29, #24]
  78:	add	x11, x8, #0x1
  7c:	str	x11, [x29, #24]
  80:	ldrb	w8, [x8]
  84:	mvn	x12, x21
  88:	cmp	w8, #0xff
  8c:	b.eq	9c <__gcc_personality_v0+0x9c>  // b.none
  90:	ldrsb	w8, [x11], #1
  94:	tbnz	w8, #31, 90 <__gcc_personality_v0+0x90>
  98:	str	x11, [x29, #24]
  9c:	add	x8, x11, #0x1
  a0:	str	x8, [x29, #24]
  a4:	ldrb	w22, [x11]
  a8:	mov	w10, wzr
  ac:	mov	x9, xzr
  b0:	add	x26, x23, x12
  b4:	ldrb	w11, [x8], #1
  b8:	and	w12, w11, #0x7f
  bc:	lsl	w12, w12, w10
  c0:	sxtw	x12, w12
  c4:	orr	x9, x9, x12
  c8:	add	w10, w10, #0x7
  cc:	tbnz	w11, #7, b4 <__gcc_personality_v0+0xb4>
  d0:	ands	x9, x9, #0xffffffff
  d4:	str	x8, [x29, #24]
  d8:	str	x8, [sp, #8]
  dc:	b.eq	184 <__gcc_personality_v0+0x184>  // b.none
  e0:	add	x27, x8, x9
  e4:	b	f0 <__gcc_personality_v0+0xf0>
  e8:	cmp	x8, x27
  ec:	b.cs	184 <__gcc_personality_v0+0x184>  // b.hs, b.nlast
  f0:	add	x0, sp, #0x8
  f4:	mov	w1, w22
  f8:	bl	1a8 <readEncodedPointer>
  fc:	mov	x24, x0
 100:	add	x0, sp, #0x8
 104:	mov	w1, w22
 108:	bl	1a8 <readEncodedPointer>
 10c:	mov	x25, x0
 110:	add	x0, sp, #0x8
 114:	mov	w1, w22
 118:	bl	1a8 <readEncodedPointer>
 11c:	ldr	x8, [sp, #8]
 120:	mov	x23, x0
 124:	ldrsb	w9, [x8], #1
 128:	tbnz	w9, #31, 124 <__gcc_personality_v0+0x124>
 12c:	str	x8, [sp, #8]
 130:	cbz	x23, e8 <__gcc_personality_v0+0xe8>
 134:	cmp	x24, x26
 138:	b.hi	e8 <__gcc_personality_v0+0xe8>  // b.pmore
 13c:	add	x9, x25, x24
 140:	cmp	x26, x9
 144:	b.cs	e8 <__gcc_personality_v0+0xe8>  // b.hs, b.nlast
 148:	mov	x0, x19
 14c:	mov	w1, wzr
 150:	mov	x2, x20
 154:	bl	0 <_Unwind_SetGR>
 158:	mov	w1, #0x1                   	// #1
 15c:	mov	x0, x19
 160:	mov	x2, xzr
 164:	bl	0 <_Unwind_SetGR>
 168:	add	x1, x23, x21
 16c:	mov	x0, x19
 170:	bl	0 <_Unwind_SetIP>
 174:	mov	w0, #0x7                   	// #7
 178:	b	188 <__gcc_personality_v0+0x188>
 17c:	mov	w0, #0x8                   	// #8
 180:	ret
 184:	mov	w0, #0x8                   	// #8
 188:	ldp	x20, x19, [sp, #96]
 18c:	ldp	x22, x21, [sp, #80]
 190:	ldp	x24, x23, [sp, #64]
 194:	ldp	x26, x25, [sp, #48]
 198:	ldr	x27, [sp, #32]
 19c:	ldp	x29, x30, [sp, #16]
 1a0:	add	sp, sp, #0x70
 1a4:	ret

00000000000001a8 <readEncodedPointer>:
 1a8:	stp	x29, x30, [sp, #-16]!
 1ac:	mov	x29, sp
 1b0:	and	w9, w1, #0xff
 1b4:	cmp	w9, #0xff
 1b8:	b.eq	1f8 <readEncodedPointer+0x50>  // b.none
 1bc:	and	w8, w9, #0xf
 1c0:	cmp	w8, #0xc
 1c4:	b.hi	2a4 <readEncodedPointer+0xfc>  // b.pmore
 1c8:	ldr	x10, [x0]
 1cc:	adrp	x11, 0 <__gcc_personality_v0>
 1d0:	add	x11, x11, #0x0
 1d4:	adr	x12, 1e4 <readEncodedPointer+0x3c>
 1d8:	ldrb	w13, [x11, x8]
 1dc:	add	x12, x12, x13, lsl #2
 1e0:	br	x12
 1e4:	mov	x11, x10
 1e8:	ldr	x8, [x11], #8
 1ec:	ubfx	w12, w9, #4, #3
 1f0:	cbnz	w12, 26c <readEncodedPointer+0xc4>
 1f4:	b	278 <readEncodedPointer+0xd0>
 1f8:	mov	x8, xzr
 1fc:	mov	x0, x8
 200:	ldp	x29, x30, [sp], #16
 204:	ret
 208:	mov	w12, wzr
 20c:	mov	x8, xzr
 210:	mov	x11, x10
 214:	ldrb	w13, [x11], #1
 218:	and	w14, w13, #0x7f
 21c:	lsl	w14, w14, w12
 220:	sxtw	x14, w14
 224:	orr	x8, x8, x14
 228:	add	w12, w12, #0x7
 22c:	tbnz	w13, #7, 214 <readEncodedPointer+0x6c>
 230:	b	264 <readEncodedPointer+0xbc>
 234:	mov	x11, x10
 238:	ldrsw	x8, [x11], #4
 23c:	ubfx	w12, w9, #4, #3
 240:	cbnz	w12, 26c <readEncodedPointer+0xc4>
 244:	b	278 <readEncodedPointer+0xd0>
 248:	mov	x11, x10
 24c:	ldr	w8, [x11], #4
 250:	ubfx	w12, w9, #4, #3
 254:	cbnz	w12, 26c <readEncodedPointer+0xc4>
 258:	b	278 <readEncodedPointer+0xd0>
 25c:	mov	x11, x10
 260:	ldrsh	x8, [x11], #2
 264:	ubfx	w12, w9, #4, #3
 268:	cbz	w12, 278 <readEncodedPointer+0xd0>
 26c:	cmp	w12, #0x1
 270:	b.ne	2bc <readEncodedPointer+0x114>  // b.any
 274:	add	x8, x8, x10
 278:	tbz	w9, #7, 280 <readEncodedPointer+0xd8>
 27c:	ldr	x8, [x8]
 280:	str	x11, [x0]
 284:	mov	x0, x8
 288:	ldp	x29, x30, [sp], #16
 28c:	ret
 290:	mov	x11, x10
 294:	ldrh	w8, [x11], #2
 298:	ubfx	w12, w9, #4, #3
 29c:	cbnz	w12, 26c <readEncodedPointer+0xc4>
 2a0:	b	278 <readEncodedPointer+0xd0>
 2a4:	adrp	x0, 0 <__gcc_personality_v0>
 2a8:	adrp	x2, 0 <__gcc_personality_v0>
 2ac:	add	x0, x0, #0x0
 2b0:	add	x2, x2, #0x0
 2b4:	mov	w1, #0x68                  	// #104
 2b8:	bl	0 <__compilerrt_abort_impl>
 2bc:	adrp	x0, 0 <__gcc_personality_v0>
 2c0:	adrp	x2, 0 <__gcc_personality_v0>
 2c4:	add	x0, x0, #0x0
 2c8:	add	x2, x2, #0x0
 2cc:	mov	w1, #0x7a                  	// #122
 2d0:	bl	0 <__compilerrt_abort_impl>

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	adrp	x8, 0 <__clear_cache>
   4:	ldr	x9, [x8]
   8:	cbz	x9, 14 <__clear_cache+0x14>
   c:	tbz	w9, #28, 20 <__clear_cache+0x20>
  10:	b	4c <__clear_cache+0x4c>
  14:	mrs	x9, ctr_el0
  18:	str	x9, [x8]
  1c:	tbnz	w9, #28, 4c <__clear_cache+0x4c>
  20:	ubfx	w9, w9, #16, #4
  24:	mov	w10, #0x4                   	// #4
  28:	lsl	w9, w10, w9
  2c:	neg	x10, x9
  30:	and	x10, x10, x0
  34:	cmp	x10, x1
  38:	b.cs	4c <__clear_cache+0x4c>  // b.hs, b.nlast
  3c:	dc	cvau, x10
  40:	add	x10, x10, x9
  44:	cmp	x10, x1
  48:	b.cc	3c <__clear_cache+0x3c>  // b.lo, b.ul, b.last
  4c:	dsb	ish
  50:	ldr	x8, [x8]
  54:	tbnz	w8, #29, 84 <__clear_cache+0x84>
  58:	and	w8, w8, #0xf
  5c:	mov	w9, #0x4                   	// #4
  60:	lsl	w8, w9, w8
  64:	neg	x9, x8
  68:	and	x9, x9, x0
  6c:	cmp	x9, x1
  70:	b.cs	84 <__clear_cache+0x84>  // b.hs, b.nlast
  74:	ic	ivau, x9
  78:	add	x9, x9, x8
  7c:	cmp	x9, x1
  80:	b.cc	74 <__clear_cache+0x74>  // b.lo, b.ul, b.last
  84:	isb
  88:	ret

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x8, fpcr
   4:	ubfx	x8, x8, #22, #2
   8:	sub	x8, x8, #0x1
   c:	cmp	x8, #0x2
  10:	b.hi	24 <__fe_getround+0x24>  // b.pmore
  14:	adrp	x9, 0 <__fe_getround>
  18:	add	x9, x9, #0x0
  1c:	ldr	w0, [x9, x8, lsl #2]
  20:	ret
  24:	mov	w0, wzr
  28:	ret

000000000000002c <__fe_raise_inexact>:
  2c:	mrs	x8, fpsr
  30:	mov	w0, wzr
  34:	orr	x8, x8, #0x10
  38:	msr	fpsr, x8
  3c:	ret
