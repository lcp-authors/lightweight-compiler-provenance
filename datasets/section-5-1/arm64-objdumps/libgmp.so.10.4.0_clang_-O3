
/home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgmp.so.10.4.0_clang_-O3:     file format elf64-littleaarch64


Disassembly of section .init:

000000000000be90 <.init>:
    be90:	stp	x29, x30, [sp, #-16]!
    be94:	mov	x29, sp
    be98:	bl	d4e0 <__gmpn_cnd_add_n@plt+0x10>
    be9c:	ldp	x29, x30, [sp], #16
    bea0:	ret

Disassembly of section .plt:

000000000000beb0 <memcpy@plt-0x20>:
    beb0:	stp	x16, x30, [sp, #-16]!
    beb4:	adrp	x16, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    beb8:	ldr	x17, [x16, #4088]
    bebc:	add	x16, x16, #0xff8
    bec0:	br	x17
    bec4:	nop
    bec8:	nop
    becc:	nop

000000000000bed0 <memcpy@plt>:
    bed0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bed4:	ldr	x17, [x16]
    bed8:	add	x16, x16, #0x0
    bedc:	br	x17

000000000000bee0 <__gmpz_tdiv_r_2exp@plt>:
    bee0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bee4:	ldr	x17, [x16, #8]
    bee8:	add	x16, x16, #0x8
    beec:	br	x17

000000000000bef0 <__gmp_tmp_reentrant_free@plt>:
    bef0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bef4:	ldr	x17, [x16, #16]
    bef8:	add	x16, x16, #0x10
    befc:	br	x17

000000000000bf00 <__gmpn_tdiv_qr@plt>:
    bf00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf04:	ldr	x17, [x16, #24]
    bf08:	add	x16, x16, #0x18
    bf0c:	br	x17

000000000000bf10 <__gmpq_cmp_ui@plt>:
    bf10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf14:	ldr	x17, [x16, #32]
    bf18:	add	x16, x16, #0x20
    bf1c:	br	x17

000000000000bf20 <__gmpz_scan1@plt>:
    bf20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf24:	ldr	x17, [x16, #40]
    bf28:	add	x16, x16, #0x28
    bf2c:	br	x17

000000000000bf30 <__gmp_randinit_mt_noseed@plt>:
    bf30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf34:	ldr	x17, [x16, #48]
    bf38:	add	x16, x16, #0x30
    bf3c:	br	x17

000000000000bf40 <__gmpn_get_d@plt>:
    bf40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf44:	ldr	x17, [x16, #56]
    bf48:	add	x16, x16, #0x38
    bf4c:	br	x17

000000000000bf50 <__gmpn_sqrmod_bnm1@plt>:
    bf50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf54:	ldr	x17, [x16, #64]
    bf58:	add	x16, x16, #0x40
    bf5c:	br	x17

000000000000bf60 <strlen@plt>:
    bf60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf64:	ldr	x17, [x16, #72]
    bf68:	add	x16, x16, #0x48
    bf6c:	br	x17

000000000000bf70 <__gmpf_add@plt>:
    bf70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf74:	ldr	x17, [x16, #80]
    bf78:	add	x16, x16, #0x50
    bf7c:	br	x17

000000000000bf80 <__gmpz_init_set@plt>:
    bf80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf84:	ldr	x17, [x16, #88]
    bf88:	add	x16, x16, #0x58
    bf8c:	br	x17

000000000000bf90 <__gmpn_gcd_1@plt>:
    bf90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bf94:	ldr	x17, [x16, #96]
    bf98:	add	x16, x16, #0x60
    bf9c:	br	x17

000000000000bfa0 <__gmpz_tdiv_ui@plt>:
    bfa0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bfa4:	ldr	x17, [x16, #104]
    bfa8:	add	x16, x16, #0x68
    bfac:	br	x17

000000000000bfb0 <__gmpn_toom_interpolate_12pts@plt>:
    bfb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bfb4:	ldr	x17, [x16, #112]
    bfb8:	add	x16, x16, #0x70
    bfbc:	br	x17

000000000000bfc0 <raise@plt>:
    bfc0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bfc4:	ldr	x17, [x16, #120]
    bfc8:	add	x16, x16, #0x78
    bfcc:	br	x17

000000000000bfd0 <__gmp_divide_by_zero@plt>:
    bfd0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bfd4:	ldr	x17, [x16, #128]
    bfd8:	add	x16, x16, #0x80
    bfdc:	br	x17

000000000000bfe0 <__gmpq_set_str@plt>:
    bfe0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bfe4:	ldr	x17, [x16, #136]
    bfe8:	add	x16, x16, #0x88
    bfec:	br	x17

000000000000bff0 <__gmpz_tdiv_qr@plt>:
    bff0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    bff4:	ldr	x17, [x16, #144]
    bff8:	add	x16, x16, #0x90
    bffc:	br	x17

000000000000c000 <__gmpn_copyd@plt>:
    c000:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c004:	ldr	x17, [x16, #152]
    c008:	add	x16, x16, #0x98
    c00c:	br	x17

000000000000c010 <__gmpn_matrix22_mul@plt>:
    c010:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c014:	ldr	x17, [x16, #160]
    c018:	add	x16, x16, #0xa0
    c01c:	br	x17

000000000000c020 <__gmpn_cnd_sub_n@plt>:
    c020:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c024:	ldr	x17, [x16, #168]
    c028:	add	x16, x16, #0xa8
    c02c:	br	x17

000000000000c030 <__gmpn_gcd_22@plt>:
    c030:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c034:	ldr	x17, [x16, #176]
    c038:	add	x16, x16, #0xb0
    c03c:	br	x17

000000000000c040 <__gmpz_tdiv_q@plt>:
    c040:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c044:	ldr	x17, [x16, #184]
    c048:	add	x16, x16, #0xb8
    c04c:	br	x17

000000000000c050 <__gmpn_toom2_sqr@plt>:
    c050:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c054:	ldr	x17, [x16, #192]
    c058:	add	x16, x16, #0xc0
    c05c:	br	x17

000000000000c060 <__gmpn_andn_n@plt>:
    c060:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c064:	ldr	x17, [x16, #200]
    c068:	add	x16, x16, #0xc8
    c06c:	br	x17

000000000000c070 <__gmpz_jacobi@plt>:
    c070:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c074:	ldr	x17, [x16, #208]
    c078:	add	x16, x16, #0xd0
    c07c:	br	x17

000000000000c080 <__gmpz_realloc@plt>:
    c080:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c084:	ldr	x17, [x16, #216]
    c088:	add	x16, x16, #0xd8
    c08c:	br	x17

000000000000c090 <__gmpn_set_str@plt>:
    c090:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c094:	ldr	x17, [x16, #224]
    c098:	add	x16, x16, #0xe0
    c09c:	br	x17

000000000000c0a0 <__gmpn_toom33_mul@plt>:
    c0a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0a4:	ldr	x17, [x16, #232]
    c0a8:	add	x16, x16, #0xe8
    c0ac:	br	x17

000000000000c0b0 <__gmpn_sqrlo_basecase@plt>:
    c0b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0b4:	ldr	x17, [x16, #240]
    c0b8:	add	x16, x16, #0xf0
    c0bc:	br	x17

000000000000c0c0 <__gmpz_gcdext@plt>:
    c0c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0c4:	ldr	x17, [x16, #248]
    c0c8:	add	x16, x16, #0xf8
    c0cc:	br	x17

000000000000c0d0 <__gmpz_set_str@plt>:
    c0d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0d4:	ldr	x17, [x16, #256]
    c0d8:	add	x16, x16, #0x100
    c0dc:	br	x17

000000000000c0e0 <__gmpn_mu_divappr_q_itch@plt>:
    c0e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0e4:	ldr	x17, [x16, #264]
    c0e8:	add	x16, x16, #0x108
    c0ec:	br	x17

000000000000c0f0 <__gmp_doscan@plt>:
    c0f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c0f4:	ldr	x17, [x16, #272]
    c0f8:	add	x16, x16, #0x110
    c0fc:	br	x17

000000000000c100 <__gmpz_cmpabs_ui@plt>:
    c100:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c104:	ldr	x17, [x16, #280]
    c108:	add	x16, x16, #0x118
    c10c:	br	x17

000000000000c110 <__gmpn_bc_set_str@plt>:
    c110:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c114:	ldr	x17, [x16, #288]
    c118:	add	x16, x16, #0x120
    c11c:	br	x17

000000000000c120 <__gmpz_sub_ui@plt>:
    c120:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c124:	ldr	x17, [x16, #296]
    c128:	add	x16, x16, #0x128
    c12c:	br	x17

000000000000c130 <__gmpq_get_str@plt>:
    c130:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c134:	ldr	x17, [x16, #304]
    c138:	add	x16, x16, #0x130
    c13c:	br	x17

000000000000c140 <__gmpn_sec_div_r@plt>:
    c140:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c144:	ldr	x17, [x16, #312]
    c148:	add	x16, x16, #0x138
    c14c:	br	x17

000000000000c150 <__gmpf_set@plt>:
    c150:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c154:	ldr	x17, [x16, #320]
    c158:	add	x16, x16, #0x140
    c15c:	br	x17

000000000000c160 <__gmpn_sublsh2_n@plt>:
    c160:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c164:	ldr	x17, [x16, #328]
    c168:	add	x16, x16, #0x148
    c16c:	br	x17

000000000000c170 <__gmpz_set_ui@plt>:
    c170:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c174:	ldr	x17, [x16, #336]
    c178:	add	x16, x16, #0x150
    c17c:	br	x17

000000000000c180 <__gmpn_lshift@plt>:
    c180:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c184:	ldr	x17, [x16, #344]
    c188:	add	x16, x16, #0x158
    c18c:	br	x17

000000000000c190 <__gmpn_sqr_basecase@plt>:
    c190:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c194:	ldr	x17, [x16, #352]
    c198:	add	x16, x16, #0x160
    c19c:	br	x17

000000000000c1a0 <__gmpn_rshift@plt>:
    c1a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1a4:	ldr	x17, [x16, #360]
    c1a8:	add	x16, x16, #0x168
    c1ac:	br	x17

000000000000c1b0 <__gmp_invalid_operation@plt>:
    c1b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1b4:	ldr	x17, [x16, #368]
    c1b8:	add	x16, x16, #0x170
    c1bc:	br	x17

000000000000c1c0 <__gmpf_set_str@plt>:
    c1c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1c4:	ldr	x17, [x16, #376]
    c1c8:	add	x16, x16, #0x178
    c1cc:	br	x17

000000000000c1d0 <__cxa_finalize@plt>:
    c1d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1d4:	ldr	x17, [x16, #384]
    c1d8:	add	x16, x16, #0x180
    c1dc:	br	x17

000000000000c1e0 <putc@plt>:
    c1e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1e4:	ldr	x17, [x16, #392]
    c1e8:	add	x16, x16, #0x188
    c1ec:	br	x17

000000000000c1f0 <__gmpf_sub_ui@plt>:
    c1f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c1f4:	ldr	x17, [x16, #400]
    c1f8:	add	x16, x16, #0x190
    c1fc:	br	x17

000000000000c200 <__gmpn_divrem_2@plt>:
    c200:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c204:	ldr	x17, [x16, #408]
    c208:	add	x16, x16, #0x198
    c20c:	br	x17

000000000000c210 <fputc@plt>:
    c210:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c214:	ldr	x17, [x16, #416]
    c218:	add	x16, x16, #0x1a0
    c21c:	br	x17

000000000000c220 <__gmpn_toom4_sqr@plt>:
    c220:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c224:	ldr	x17, [x16, #424]
    c228:	add	x16, x16, #0x1a8
    c22c:	br	x17

000000000000c230 <__gmpn_sec_powm_itch@plt>:
    c230:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c234:	ldr	x17, [x16, #432]
    c238:	add	x16, x16, #0x1b0
    c23c:	br	x17

000000000000c240 <__gmpn_perfect_power_p@plt>:
    c240:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c244:	ldr	x17, [x16, #440]
    c248:	add	x16, x16, #0x1b8
    c24c:	br	x17

000000000000c250 <__gmpn_mod_1_1p@plt>:
    c250:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c254:	ldr	x17, [x16, #448]
    c258:	add	x16, x16, #0x1c0
    c25c:	br	x17

000000000000c260 <__gmpz_sub@plt>:
    c260:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c264:	ldr	x17, [x16, #456]
    c268:	add	x16, x16, #0x1c8
    c26c:	br	x17

000000000000c270 <__gmpn_and_n@plt>:
    c270:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c274:	ldr	x17, [x16, #464]
    c278:	add	x16, x16, #0x1d0
    c27c:	br	x17

000000000000c280 <__gmpn_toom_eval_dgr3_pm1@plt>:
    c280:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c284:	ldr	x17, [x16, #472]
    c288:	add	x16, x16, #0x1d8
    c28c:	br	x17

000000000000c290 <__gmpn_com@plt>:
    c290:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c294:	ldr	x17, [x16, #480]
    c298:	add	x16, x16, #0x1e0
    c29c:	br	x17

000000000000c2a0 <__gmpn_rootrem@plt>:
    c2a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2a4:	ldr	x17, [x16, #488]
    c2a8:	add	x16, x16, #0x1e8
    c2ac:	br	x17

000000000000c2b0 <__gmpn_hgcd_step@plt>:
    c2b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2b4:	ldr	x17, [x16, #496]
    c2b8:	add	x16, x16, #0x1f0
    c2bc:	br	x17

000000000000c2c0 <__gmpn_bdiv_dbm1c@plt>:
    c2c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2c4:	ldr	x17, [x16, #504]
    c2c8:	add	x16, x16, #0x1f8
    c2cc:	br	x17

000000000000c2d0 <__gmpn_sub_n@plt>:
    c2d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2d4:	ldr	x17, [x16, #512]
    c2d8:	add	x16, x16, #0x200
    c2dc:	br	x17

000000000000c2e0 <__gmpn_mu_div_q@plt>:
    c2e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2e4:	ldr	x17, [x16, #520]
    c2e8:	add	x16, x16, #0x208
    c2ec:	br	x17

000000000000c2f0 <snprintf@plt>:
    c2f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c2f4:	ldr	x17, [x16, #528]
    c2f8:	add	x16, x16, #0x210
    c2fc:	br	x17

000000000000c300 <__gmpn_mul_fft@plt>:
    c300:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c304:	ldr	x17, [x16, #536]
    c308:	add	x16, x16, #0x218
    c30c:	br	x17

000000000000c310 <__gmpz_setbit@plt>:
    c310:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c314:	ldr	x17, [x16, #544]
    c318:	add	x16, x16, #0x220
    c31c:	br	x17

000000000000c320 <__gmpn_div_q@plt>:
    c320:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c324:	ldr	x17, [x16, #552]
    c328:	add	x16, x16, #0x228
    c32c:	br	x17

000000000000c330 <__gmpf_clear@plt>:
    c330:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c334:	ldr	x17, [x16, #560]
    c338:	add	x16, x16, #0x230
    c33c:	br	x17

000000000000c340 <__gmpz_n_pow_ui@plt>:
    c340:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c344:	ldr	x17, [x16, #568]
    c348:	add	x16, x16, #0x238
    c34c:	br	x17

000000000000c350 <__gmpf_get_prec@plt>:
    c350:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c354:	ldr	x17, [x16, #576]
    c358:	add	x16, x16, #0x240
    c35c:	br	x17

000000000000c360 <__gmpn_dc_set_str@plt>:
    c360:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c364:	ldr	x17, [x16, #584]
    c368:	add	x16, x16, #0x248
    c36c:	br	x17

000000000000c370 <__gmpz_powm@plt>:
    c370:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c374:	ldr	x17, [x16, #592]
    c378:	add	x16, x16, #0x250
    c37c:	br	x17

000000000000c380 <__gmpn_sec_add_1@plt>:
    c380:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c384:	ldr	x17, [x16, #600]
    c388:	add	x16, x16, #0x258
    c38c:	br	x17

000000000000c390 <__gmpn_toom_eval_pm2exp@plt>:
    c390:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c394:	ldr	x17, [x16, #608]
    c398:	add	x16, x16, #0x260
    c39c:	br	x17

000000000000c3a0 <__gmpz_get_str@plt>:
    c3a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3a4:	ldr	x17, [x16, #616]
    c3a8:	add	x16, x16, #0x268
    c3ac:	br	x17

000000000000c3b0 <__gmpn_dcpi1_div_qr@plt>:
    c3b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3b4:	ldr	x17, [x16, #624]
    c3b8:	add	x16, x16, #0x270
    c3bc:	br	x17

000000000000c3c0 <__gmpn_powlo@plt>:
    c3c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3c4:	ldr	x17, [x16, #632]
    c3c8:	add	x16, x16, #0x278
    c3cc:	br	x17

000000000000c3d0 <__gmpz_oddfac_1@plt>:
    c3d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3d4:	ldr	x17, [x16, #640]
    c3d8:	add	x16, x16, #0x280
    c3dc:	br	x17

000000000000c3e0 <__gmpn_mod_1@plt>:
    c3e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3e4:	ldr	x17, [x16, #648]
    c3e8:	add	x16, x16, #0x288
    c3ec:	br	x17

000000000000c3f0 <__gmpz_divexact@plt>:
    c3f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c3f4:	ldr	x17, [x16, #656]
    c3f8:	add	x16, x16, #0x290
    c3fc:	br	x17

000000000000c400 <nl_langinfo@plt>:
    c400:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c404:	ldr	x17, [x16, #664]
    c408:	add	x16, x16, #0x298
    c40c:	br	x17

000000000000c410 <malloc@plt>:
    c410:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c414:	ldr	x17, [x16, #672]
    c418:	add	x16, x16, #0x2a0
    c41c:	br	x17

000000000000c420 <__gmpz_set@plt>:
    c420:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c424:	ldr	x17, [x16, #680]
    c428:	add	x16, x16, #0x2a8
    c42c:	br	x17

000000000000c430 <__gmpn_divexact@plt>:
    c430:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c434:	ldr	x17, [x16, #688]
    c438:	add	x16, x16, #0x2b0
    c43c:	br	x17

000000000000c440 <__gmpn_sublsh1_n@plt>:
    c440:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c444:	ldr	x17, [x16, #696]
    c448:	add	x16, x16, #0x2b8
    c44c:	br	x17

000000000000c450 <__gmpz_fac_ui@plt>:
    c450:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c454:	ldr	x17, [x16, #704]
    c458:	add	x16, x16, #0x2c0
    c45c:	br	x17

000000000000c460 <__gmpn_mulmid_basecase@plt>:
    c460:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c464:	ldr	x17, [x16, #712]
    c468:	add	x16, x16, #0x2c8
    c46c:	br	x17

000000000000c470 <__gmpn_div_qr_1n_pi1@plt>:
    c470:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c474:	ldr	x17, [x16, #720]
    c478:	add	x16, x16, #0x2d0
    c47c:	br	x17

000000000000c480 <__gmpz_tstbit@plt>:
    c480:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c484:	ldr	x17, [x16, #728]
    c488:	add	x16, x16, #0x2d8
    c48c:	br	x17

000000000000c490 <__gmp_randclear@plt>:
    c490:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c494:	ldr	x17, [x16, #736]
    c498:	add	x16, x16, #0x2e0
    c49c:	br	x17

000000000000c4a0 <__gmpf_set_d@plt>:
    c4a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4a4:	ldr	x17, [x16, #744]
    c4a8:	add	x16, x16, #0x2e8
    c4ac:	br	x17

000000000000c4b0 <__gmpz_mul@plt>:
    c4b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4b4:	ldr	x17, [x16, #752]
    c4b8:	add	x16, x16, #0x2f0
    c4bc:	br	x17

000000000000c4c0 <__gmpn_sec_tabselect@plt>:
    c4c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4c4:	ldr	x17, [x16, #760]
    c4c8:	add	x16, x16, #0x2f8
    c4cc:	br	x17

000000000000c4d0 <__gmpn_dcpi1_divappr_q@plt>:
    c4d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4d4:	ldr	x17, [x16, #768]
    c4d8:	add	x16, x16, #0x300
    c4dc:	br	x17

000000000000c4e0 <__gmpn_pi1_bdiv_q_1@plt>:
    c4e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4e4:	ldr	x17, [x16, #776]
    c4e8:	add	x16, x16, #0x308
    c4ec:	br	x17

000000000000c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>:
    c4f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c4f4:	ldr	x17, [x16, #784]
    c4f8:	add	x16, x16, #0x310
    c4fc:	br	x17

000000000000c500 <__gmp_vasprintf@plt>:
    c500:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c504:	ldr	x17, [x16, #792]
    c508:	add	x16, x16, #0x318
    c50c:	br	x17

000000000000c510 <__gmpn_sbpi1_bdiv_q@plt>:
    c510:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c514:	ldr	x17, [x16, #800]
    c518:	add	x16, x16, #0x320
    c51c:	br	x17

000000000000c520 <__gmpz_lucas_mod@plt>:
    c520:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c524:	ldr	x17, [x16, #808]
    c528:	add	x16, x16, #0x328
    c52c:	br	x17

000000000000c530 <__gmpn_toom_eval_pm2@plt>:
    c530:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c534:	ldr	x17, [x16, #816]
    c538:	add	x16, x16, #0x330
    c53c:	br	x17

000000000000c540 <__gmpz_out_str@plt>:
    c540:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c544:	ldr	x17, [x16, #824]
    c548:	add	x16, x16, #0x338
    c54c:	br	x17

000000000000c550 <__gmpn_mul_basecase@plt>:
    c550:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c554:	ldr	x17, [x16, #832]
    c558:	add	x16, x16, #0x340
    c55c:	br	x17

000000000000c560 <__gmpn_gcdext@plt>:
    c560:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c564:	ldr	x17, [x16, #840]
    c568:	add	x16, x16, #0x348
    c56c:	br	x17

000000000000c570 <__isoc99_fscanf@plt>:
    c570:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c574:	ldr	x17, [x16, #848]
    c578:	add	x16, x16, #0x350
    c57c:	br	x17

000000000000c580 <__gmpz_swap@plt>:
    c580:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c584:	ldr	x17, [x16, #856]
    c588:	add	x16, x16, #0x358
    c58c:	br	x17

000000000000c590 <__gmpn_hgcd_itch@plt>:
    c590:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c594:	ldr	x17, [x16, #864]
    c598:	add	x16, x16, #0x360
    c59c:	br	x17

000000000000c5a0 <__gmpn_hgcd2@plt>:
    c5a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5a4:	ldr	x17, [x16, #872]
    c5a8:	add	x16, x16, #0x368
    c5ac:	br	x17

000000000000c5b0 <fgetc@plt>:
    c5b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5b4:	ldr	x17, [x16, #880]
    c5b8:	add	x16, x16, #0x370
    c5bc:	br	x17

000000000000c5c0 <__gmpz_mul_ui@plt>:
    c5c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5c4:	ldr	x17, [x16, #888]
    c5c8:	add	x16, x16, #0x378
    c5cc:	br	x17

000000000000c5d0 <__gmpn_sqrmod_bnm1_next_size@plt>:
    c5d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5d4:	ldr	x17, [x16, #896]
    c5d8:	add	x16, x16, #0x380
    c5dc:	br	x17

000000000000c5e0 <__gmpn_dcpi1_bdiv_qr@plt>:
    c5e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5e4:	ldr	x17, [x16, #904]
    c5e8:	add	x16, x16, #0x388
    c5ec:	br	x17

000000000000c5f0 <memset@plt>:
    c5f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c5f4:	ldr	x17, [x16, #912]
    c5f8:	add	x16, x16, #0x390
    c5fc:	br	x17

000000000000c600 <__gmpz_2fac_ui@plt>:
    c600:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c604:	ldr	x17, [x16, #920]
    c608:	add	x16, x16, #0x398
    c60c:	br	x17

000000000000c610 <__gmpn_trialdiv@plt>:
    c610:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c614:	ldr	x17, [x16, #928]
    c618:	add	x16, x16, #0x3a0
    c61c:	br	x17

000000000000c620 <__gmpf_set_si@plt>:
    c620:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c624:	ldr	x17, [x16, #936]
    c628:	add	x16, x16, #0x3a8
    c62c:	br	x17

000000000000c630 <__gmpn_add_err2_n@plt>:
    c630:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c634:	ldr	x17, [x16, #944]
    c638:	add	x16, x16, #0x3b0
    c63c:	br	x17

000000000000c640 <__gmpn_sbpi1_div_qr@plt>:
    c640:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c644:	ldr	x17, [x16, #952]
    c648:	add	x16, x16, #0x3b8
    c64c:	br	x17

000000000000c650 <__gmpz_fdiv_q_2exp@plt>:
    c650:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c654:	ldr	x17, [x16, #960]
    c658:	add	x16, x16, #0x3c0
    c65c:	br	x17

000000000000c660 <__gmpn_cnd_swap@plt>:
    c660:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c664:	ldr	x17, [x16, #968]
    c668:	add	x16, x16, #0x3c8
    c66c:	br	x17

000000000000c670 <__gmpf_cmp@plt>:
    c670:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c674:	ldr	x17, [x16, #976]
    c678:	add	x16, x16, #0x3d0
    c67c:	br	x17

000000000000c680 <__gmpn_mod_1s_4p_cps@plt>:
    c680:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c684:	ldr	x17, [x16, #984]
    c688:	add	x16, x16, #0x3d8
    c68c:	br	x17

000000000000c690 <__gmpn_scan0@plt>:
    c690:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c694:	ldr	x17, [x16, #992]
    c698:	add	x16, x16, #0x3e0
    c69c:	br	x17

000000000000c6a0 <__gmpf_set_ui@plt>:
    c6a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6a4:	ldr	x17, [x16, #1000]
    c6a8:	add	x16, x16, #0x3e8
    c6ac:	br	x17

000000000000c6b0 <__gmpn_brootinv@plt>:
    c6b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6b4:	ldr	x17, [x16, #1008]
    c6b8:	add	x16, x16, #0x3f0
    c6bc:	br	x17

000000000000c6c0 <__gmp_assert_fail@plt>:
    c6c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6c4:	ldr	x17, [x16, #1016]
    c6c8:	add	x16, x16, #0x3f8
    c6cc:	br	x17

000000000000c6d0 <__gmpn_preinv_mod_1@plt>:
    c6d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6d4:	ldr	x17, [x16, #1024]
    c6d8:	add	x16, x16, #0x400
    c6dc:	br	x17

000000000000c6e0 <__gmpz_mul_2exp@plt>:
    c6e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6e4:	ldr	x17, [x16, #1032]
    c6e8:	add	x16, x16, #0x408
    c6ec:	br	x17

000000000000c6f0 <__gmpn_sbpi1_divappr_q@plt>:
    c6f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c6f4:	ldr	x17, [x16, #1040]
    c6f8:	add	x16, x16, #0x410
    c6fc:	br	x17

000000000000c700 <__gmpn_mulmid@plt>:
    c700:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c704:	ldr	x17, [x16, #1048]
    c708:	add	x16, x16, #0x418
    c70c:	br	x17

000000000000c710 <__gmpn_mu_divappr_q@plt>:
    c710:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c714:	ldr	x17, [x16, #1056]
    c718:	add	x16, x16, #0x420
    c71c:	br	x17

000000000000c720 <__gmpn_toom44_mul@plt>:
    c720:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c724:	ldr	x17, [x16, #1064]
    c728:	add	x16, x16, #0x428
    c72c:	br	x17

000000000000c730 <__gmpn_jacobi_base@plt>:
    c730:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c734:	ldr	x17, [x16, #1072]
    c738:	add	x16, x16, #0x430
    c73c:	br	x17

000000000000c740 <__gmpn_toom63_mul@plt>:
    c740:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c744:	ldr	x17, [x16, #1080]
    c748:	add	x16, x16, #0x438
    c74c:	br	x17

000000000000c750 <__gmpn_bsqrtinv@plt>:
    c750:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c754:	ldr	x17, [x16, #1088]
    c758:	add	x16, x16, #0x440
    c75c:	br	x17

000000000000c760 <__gmpn_sub_nc@plt>:
    c760:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c764:	ldr	x17, [x16, #1096]
    c768:	add	x16, x16, #0x448
    c76c:	br	x17

000000000000c770 <__gmpn_divexact_1@plt>:
    c770:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c774:	ldr	x17, [x16, #1104]
    c778:	add	x16, x16, #0x450
    c77c:	br	x17

000000000000c780 <__gmpn_hgcd_matrix_mul_1@plt>:
    c780:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c784:	ldr	x17, [x16, #1112]
    c788:	add	x16, x16, #0x458
    c78c:	br	x17

000000000000c790 <__gmpn_toom42_mulmid@plt>:
    c790:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c794:	ldr	x17, [x16, #1120]
    c798:	add	x16, x16, #0x460
    c79c:	br	x17

000000000000c7a0 <__gmp_randinit_default@plt>:
    c7a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7a4:	ldr	x17, [x16, #1128]
    c7a8:	add	x16, x16, #0x468
    c7ac:	br	x17

000000000000c7b0 <__gmpn_toom_interpolate_8pts@plt>:
    c7b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7b4:	ldr	x17, [x16, #1136]
    c7b8:	add	x16, x16, #0x470
    c7bc:	br	x17

000000000000c7c0 <realloc@plt>:
    c7c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7c4:	ldr	x17, [x16, #1144]
    c7c8:	add	x16, x16, #0x478
    c7cc:	br	x17

000000000000c7d0 <__gmpn_redc_n@plt>:
    c7d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7d4:	ldr	x17, [x16, #1152]
    c7d8:	add	x16, x16, #0x480
    c7dc:	br	x17

000000000000c7e0 <__gmpn_modexact_1c_odd@plt>:
    c7e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7e4:	ldr	x17, [x16, #1160]
    c7e8:	add	x16, x16, #0x488
    c7ec:	br	x17

000000000000c7f0 <getc@plt>:
    c7f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c7f4:	ldr	x17, [x16, #1168]
    c7f8:	add	x16, x16, #0x490
    c7fc:	br	x17

000000000000c800 <__gmpn_sec_pi1_div_r@plt>:
    c800:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c804:	ldr	x17, [x16, #1176]
    c808:	add	x16, x16, #0x498
    c80c:	br	x17

000000000000c810 <__gmpn_toom_interpolate_7pts@plt>:
    c810:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c814:	ldr	x17, [x16, #1184]
    c818:	add	x16, x16, #0x4a0
    c81c:	br	x17

000000000000c820 <__gmpn_sbpi1_bdiv_qr@plt>:
    c820:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c824:	ldr	x17, [x16, #1192]
    c828:	add	x16, x16, #0x4a8
    c82c:	br	x17

000000000000c830 <__gmpn_hgcd_matrix_init@plt>:
    c830:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c834:	ldr	x17, [x16, #1200]
    c838:	add	x16, x16, #0x4b0
    c83c:	br	x17

000000000000c840 <__gmpn_rsh1sub_n@plt>:
    c840:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c844:	ldr	x17, [x16, #1208]
    c848:	add	x16, x16, #0x4b8
    c84c:	br	x17

000000000000c850 <__gmpn_toom32_mul@plt>:
    c850:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c854:	ldr	x17, [x16, #1216]
    c858:	add	x16, x16, #0x4c0
    c85c:	br	x17

000000000000c860 <__gmpn_mulmod_bnm1_next_size@plt>:
    c860:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c864:	ldr	x17, [x16, #1224]
    c868:	add	x16, x16, #0x4c8
    c86c:	br	x17

000000000000c870 <__gmpz_submul_ui@plt>:
    c870:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c874:	ldr	x17, [x16, #1232]
    c878:	add	x16, x16, #0x4d0
    c87c:	br	x17

000000000000c880 <__gmpn_mu_bdiv_q_itch@plt>:
    c880:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c884:	ldr	x17, [x16, #1240]
    c888:	add	x16, x16, #0x4d8
    c88c:	br	x17

000000000000c890 <__gmpz_set_d@plt>:
    c890:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c894:	ldr	x17, [x16, #1248]
    c898:	add	x16, x16, #0x4e0
    c89c:	br	x17

000000000000c8a0 <__gmpn_hgcd_matrix_adjust@plt>:
    c8a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8a4:	ldr	x17, [x16, #1256]
    c8a8:	add	x16, x16, #0x4e8
    c8ac:	br	x17

000000000000c8b0 <__gmpz_add_ui@plt>:
    c8b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8b4:	ldr	x17, [x16, #1264]
    c8b8:	add	x16, x16, #0x4f0
    c8bc:	br	x17

000000000000c8c0 <__gmpn_bdiv_qr_itch@plt>:
    c8c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8c4:	ldr	x17, [x16, #1272]
    c8c8:	add	x16, x16, #0x4f8
    c8cc:	br	x17

000000000000c8d0 <__gmon_start__@plt>:
    c8d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8d4:	ldr	x17, [x16, #1280]
    c8d8:	add	x16, x16, #0x500
    c8dc:	br	x17

000000000000c8e0 <__gmpn_sqr@plt>:
    c8e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8e4:	ldr	x17, [x16, #1288]
    c8e8:	add	x16, x16, #0x508
    c8ec:	br	x17

000000000000c8f0 <__gmpz_urandomm@plt>:
    c8f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c8f4:	ldr	x17, [x16, #1296]
    c8f8:	add	x16, x16, #0x510
    c8fc:	br	x17

000000000000c900 <abort@plt>:
    c900:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c904:	ldr	x17, [x16, #1304]
    c908:	add	x16, x16, #0x518
    c90c:	br	x17

000000000000c910 <__gmpn_toom_interpolate_6pts@plt>:
    c910:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c914:	ldr	x17, [x16, #1312]
    c918:	add	x16, x16, #0x520
    c91c:	br	x17

000000000000c920 <__gmpn_div_qr_2n_pi1@plt>:
    c920:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c924:	ldr	x17, [x16, #1320]
    c928:	add	x16, x16, #0x528
    c92c:	br	x17

000000000000c930 <__gmpn_broot_invm1@plt>:
    c930:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c934:	ldr	x17, [x16, #1328]
    c938:	add	x16, x16, #0x530
    c93c:	br	x17

000000000000c940 <__gmpz_clrbit@plt>:
    c940:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c944:	ldr	x17, [x16, #1336]
    c948:	add	x16, x16, #0x538
    c94c:	br	x17

000000000000c950 <__gmpn_rsh1add_n@plt>:
    c950:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c954:	ldr	x17, [x16, #1344]
    c958:	add	x16, x16, #0x540
    c95c:	br	x17

000000000000c960 <__gmpn_mu_div_qr@plt>:
    c960:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c964:	ldr	x17, [x16, #1352]
    c968:	add	x16, x16, #0x548
    c96c:	br	x17

000000000000c970 <__gmpn_toom_couple_handling@plt>:
    c970:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c974:	ldr	x17, [x16, #1360]
    c978:	add	x16, x16, #0x550
    c97c:	br	x17

000000000000c980 <__gmpn_mulmod_bnm1@plt>:
    c980:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c984:	ldr	x17, [x16, #1368]
    c988:	add	x16, x16, #0x558
    c98c:	br	x17

000000000000c990 <__gmpn_mul_n@plt>:
    c990:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c994:	ldr	x17, [x16, #1376]
    c998:	add	x16, x16, #0x560
    c99c:	br	x17

000000000000c9a0 <__gmpz_scan0@plt>:
    c9a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9a4:	ldr	x17, [x16, #1384]
    c9a8:	add	x16, x16, #0x568
    c9ac:	br	x17

000000000000c9b0 <puts@plt>:
    c9b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9b4:	ldr	x17, [x16, #1392]
    c9b8:	add	x16, x16, #0x570
    c9bc:	br	x17

000000000000c9c0 <__gmpz_stronglucas@plt>:
    c9c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9c4:	ldr	x17, [x16, #1400]
    c9c8:	add	x16, x16, #0x578
    c9cc:	br	x17

000000000000c9d0 <__gmpz_inp_str_nowhite@plt>:
    c9d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9d4:	ldr	x17, [x16, #1408]
    c9d8:	add	x16, x16, #0x580
    c9dc:	br	x17

000000000000c9e0 <__gmpn_submul_1@plt>:
    c9e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9e4:	ldr	x17, [x16, #1416]
    c9e8:	add	x16, x16, #0x588
    c9ec:	br	x17

000000000000c9f0 <__gmpn_sqrlo@plt>:
    c9f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    c9f4:	ldr	x17, [x16, #1424]
    c9f8:	add	x16, x16, #0x590
    c9fc:	br	x17

000000000000ca00 <__gmpz_divexact_gcd@plt>:
    ca00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca04:	ldr	x17, [x16, #1432]
    ca08:	add	x16, x16, #0x598
    ca0c:	br	x17

000000000000ca10 <__gmpz_ui_pow_ui@plt>:
    ca10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca14:	ldr	x17, [x16, #1440]
    ca18:	add	x16, x16, #0x5a0
    ca1c:	br	x17

000000000000ca20 <__gmpn_toom_interpolate_5pts@plt>:
    ca20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca24:	ldr	x17, [x16, #1448]
    ca28:	add	x16, x16, #0x5a8
    ca2c:	br	x17

000000000000ca30 <__gmpn_bdiv_q@plt>:
    ca30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca34:	ldr	x17, [x16, #1456]
    ca38:	add	x16, x16, #0x5b0
    ca3c:	br	x17

000000000000ca40 <__gmpn_toom53_mul@plt>:
    ca40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca44:	ldr	x17, [x16, #1464]
    ca48:	add	x16, x16, #0x5b8
    ca4c:	br	x17

000000000000ca50 <__gmpn_copyi@plt>:
    ca50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca54:	ldr	x17, [x16, #1472]
    ca58:	add	x16, x16, #0x5c0
    ca5c:	br	x17

000000000000ca60 <__gmpq_set_ui@plt>:
    ca60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca64:	ldr	x17, [x16, #1480]
    ca68:	add	x16, x16, #0x5c8
    ca6c:	br	x17

000000000000ca70 <__gmpn_add_n@plt>:
    ca70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca74:	ldr	x17, [x16, #1488]
    ca78:	add	x16, x16, #0x5d0
    ca7c:	br	x17

000000000000ca80 <__gmpz_tdiv_r@plt>:
    ca80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca84:	ldr	x17, [x16, #1496]
    ca88:	add	x16, x16, #0x5d8
    ca8c:	br	x17

000000000000ca90 <__gmpn_get_str@plt>:
    ca90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ca94:	ldr	x17, [x16, #1504]
    ca98:	add	x16, x16, #0x5e0
    ca9c:	br	x17

000000000000caa0 <__gmpn_dcpi1_div_q@plt>:
    caa0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    caa4:	ldr	x17, [x16, #1512]
    caa8:	add	x16, x16, #0x5e8
    caac:	br	x17

000000000000cab0 <__gmpn_jacobi_2@plt>:
    cab0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cab4:	ldr	x17, [x16, #1520]
    cab8:	add	x16, x16, #0x5f0
    cabc:	br	x17

000000000000cac0 <__gmpn_mod_1_1p_cps@plt>:
    cac0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cac4:	ldr	x17, [x16, #1528]
    cac8:	add	x16, x16, #0x5f8
    cacc:	br	x17

000000000000cad0 <__gmpn_fft_best_k@plt>:
    cad0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cad4:	ldr	x17, [x16, #1536]
    cad8:	add	x16, x16, #0x600
    cadc:	br	x17

000000000000cae0 <__ctype_b_loc@plt>:
    cae0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cae4:	ldr	x17, [x16, #1544]
    cae8:	add	x16, x16, #0x608
    caec:	br	x17

000000000000caf0 <__gmpn_hgcd2_jacobi@plt>:
    caf0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    caf4:	ldr	x17, [x16, #1552]
    caf8:	add	x16, x16, #0x610
    cafc:	br	x17

000000000000cb00 <__gmp_randinit_mt@plt>:
    cb00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb04:	ldr	x17, [x16, #1560]
    cb08:	add	x16, x16, #0x618
    cb0c:	br	x17

000000000000cb10 <__gmpn_compute_powtab@plt>:
    cb10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb14:	ldr	x17, [x16, #1568]
    cb18:	add	x16, x16, #0x620
    cb1c:	br	x17

000000000000cb20 <__gmpn_toom8h_mul@plt>:
    cb20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb24:	ldr	x17, [x16, #1576]
    cb28:	add	x16, x16, #0x628
    cb2c:	br	x17

000000000000cb30 <__gmpz_kronecker_ui@plt>:
    cb30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb34:	ldr	x17, [x16, #1584]
    cb38:	add	x16, x16, #0x630
    cb3c:	br	x17

000000000000cb40 <__gmpn_xor_n@plt>:
    cb40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb44:	ldr	x17, [x16, #1592]
    cb48:	add	x16, x16, #0x638
    cb4c:	br	x17

000000000000cb50 <__gmpz_clear@plt>:
    cb50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb54:	ldr	x17, [x16, #1600]
    cb58:	add	x16, x16, #0x640
    cb5c:	br	x17

000000000000cb60 <strtol@plt>:
    cb60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb64:	ldr	x17, [x16, #1608]
    cb68:	add	x16, x16, #0x648
    cb6c:	br	x17

000000000000cb70 <__gmpq_set_si@plt>:
    cb70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb74:	ldr	x17, [x16, #1616]
    cb78:	add	x16, x16, #0x650
    cb7c:	br	x17

000000000000cb80 <__gmpz_millerrabin@plt>:
    cb80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb84:	ldr	x17, [x16, #1624]
    cb88:	add	x16, x16, #0x658
    cb8c:	br	x17

000000000000cb90 <fread@plt>:
    cb90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cb94:	ldr	x17, [x16, #1632]
    cb98:	add	x16, x16, #0x660
    cb9c:	br	x17

000000000000cba0 <__gmpn_addlsh2_n@plt>:
    cba0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cba4:	ldr	x17, [x16, #1640]
    cba8:	add	x16, x16, #0x668
    cbac:	br	x17

000000000000cbb0 <__gmpz_mul_si@plt>:
    cbb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cbb4:	ldr	x17, [x16, #1648]
    cbb8:	add	x16, x16, #0x670
    cbbc:	br	x17

000000000000cbc0 <__gmp_tmp_reentrant_alloc@plt>:
    cbc0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cbc4:	ldr	x17, [x16, #1656]
    cbc8:	add	x16, x16, #0x678
    cbcc:	br	x17

000000000000cbd0 <__gmpz_invert@plt>:
    cbd0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cbd4:	ldr	x17, [x16, #1664]
    cbd8:	add	x16, x16, #0x680
    cbdc:	br	x17

000000000000cbe0 <__gmpn_rsblsh2_n@plt>:
    cbe0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cbe4:	ldr	x17, [x16, #1672]
    cbe8:	add	x16, x16, #0x688
    cbec:	br	x17

000000000000cbf0 <__gmpf_neg@plt>:
    cbf0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cbf4:	ldr	x17, [x16, #1680]
    cbf8:	add	x16, x16, #0x690
    cbfc:	br	x17

000000000000cc00 <__gmpn_ior_n@plt>:
    cc00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc04:	ldr	x17, [x16, #1688]
    cc08:	add	x16, x16, #0x698
    cc0c:	br	x17

000000000000cc10 <__gmpn_gcd@plt>:
    cc10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc14:	ldr	x17, [x16, #1696]
    cc18:	add	x16, x16, #0x6a0
    cc1c:	br	x17

000000000000cc20 <__gmpn_toom6h_mul@plt>:
    cc20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc24:	ldr	x17, [x16, #1704]
    cc28:	add	x16, x16, #0x6a8
    cc2c:	br	x17

000000000000cc30 <free@plt>:
    cc30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc34:	ldr	x17, [x16, #1712]
    cc38:	add	x16, x16, #0x6b0
    cc3c:	br	x17

000000000000cc40 <__gmpn_addlsh1_n@plt>:
    cc40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc44:	ldr	x17, [x16, #1720]
    cc48:	add	x16, x16, #0x6b8
    cc4c:	br	x17

000000000000cc50 <ungetc@plt>:
    cc50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc54:	ldr	x17, [x16, #1728]
    cc58:	add	x16, x16, #0x6c0
    cc5c:	br	x17

000000000000cc60 <__gmpn_sec_powm@plt>:
    cc60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc64:	ldr	x17, [x16, #1736]
    cc68:	add	x16, x16, #0x6c8
    cc6c:	br	x17

000000000000cc70 <__gmpz_tdiv_q_2exp@plt>:
    cc70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc74:	ldr	x17, [x16, #1744]
    cc78:	add	x16, x16, #0x6d0
    cc7c:	br	x17

000000000000cc80 <__gmp_nextprime@plt>:
    cc80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc84:	ldr	x17, [x16, #1752]
    cc88:	add	x16, x16, #0x6d8
    cc8c:	br	x17

000000000000cc90 <__gmpz_roinit_n@plt>:
    cc90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cc94:	ldr	x17, [x16, #1760]
    cc98:	add	x16, x16, #0x6e0
    cc9c:	br	x17

000000000000cca0 <__gmpn_nussbaumer_mul@plt>:
    cca0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cca4:	ldr	x17, [x16, #1768]
    cca8:	add	x16, x16, #0x6e8
    ccac:	br	x17

000000000000ccb0 <__gmpn_mu_bdiv_qr@plt>:
    ccb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ccb4:	ldr	x17, [x16, #1776]
    ccb8:	add	x16, x16, #0x6f0
    ccbc:	br	x17

000000000000ccc0 <__gmpn_mod_1s_2p_cps@plt>:
    ccc0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ccc4:	ldr	x17, [x16, #1784]
    ccc8:	add	x16, x16, #0x6f8
    cccc:	br	x17

000000000000ccd0 <__gmpn_mul@plt>:
    ccd0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ccd4:	ldr	x17, [x16, #1792]
    ccd8:	add	x16, x16, #0x700
    ccdc:	br	x17

000000000000cce0 <__gmpn_preinv_divrem_1@plt>:
    cce0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cce4:	ldr	x17, [x16, #1800]
    cce8:	add	x16, x16, #0x708
    ccec:	br	x17

000000000000ccf0 <__gmpn_add_err1_n@plt>:
    ccf0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ccf4:	ldr	x17, [x16, #1808]
    ccf8:	add	x16, x16, #0x710
    ccfc:	br	x17

000000000000cd00 <__gmpn_divrem_1@plt>:
    cd00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd04:	ldr	x17, [x16, #1816]
    cd08:	add	x16, x16, #0x718
    cd0c:	br	x17

000000000000cd10 <__gmp_doprnt_integer@plt>:
    cd10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd14:	ldr	x17, [x16, #1824]
    cd18:	add	x16, x16, #0x720
    cd1c:	br	x17

000000000000cd20 <__gmpn_binvert@plt>:
    cd20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd24:	ldr	x17, [x16, #1832]
    cd28:	add	x16, x16, #0x728
    cd2c:	br	x17

000000000000cd30 <__gmpf_mul@plt>:
    cd30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd34:	ldr	x17, [x16, #1840]
    cd38:	add	x16, x16, #0x730
    cd3c:	br	x17

000000000000cd40 <__gmpn_remove@plt>:
    cd40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd44:	ldr	x17, [x16, #1848]
    cd48:	add	x16, x16, #0x738
    cd4c:	br	x17

000000000000cd50 <__gmpn_hgcd_appr@plt>:
    cd50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd54:	ldr	x17, [x16, #1856]
    cd58:	add	x16, x16, #0x740
    cd5c:	br	x17

000000000000cd60 <__gmpn_toom_eval_dgr3_pm2@plt>:
    cd60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd64:	ldr	x17, [x16, #1864]
    cd68:	add	x16, x16, #0x748
    cd6c:	br	x17

000000000000cd70 <__gmpz_prodlimbs@plt>:
    cd70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd74:	ldr	x17, [x16, #1872]
    cd78:	add	x16, x16, #0x750
    cd7c:	br	x17

000000000000cd80 <__gmpn_popcount@plt>:
    cd80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd84:	ldr	x17, [x16, #1880]
    cd88:	add	x16, x16, #0x758
    cd8c:	br	x17

000000000000cd90 <__gmpf_mul_2exp@plt>:
    cd90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cd94:	ldr	x17, [x16, #1888]
    cd98:	add	x16, x16, #0x760
    cd9c:	br	x17

000000000000cda0 <strchr@plt>:
    cda0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cda4:	ldr	x17, [x16, #1896]
    cda8:	add	x16, x16, #0x768
    cdac:	br	x17

000000000000cdb0 <__gmp_assert_header@plt>:
    cdb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cdb4:	ldr	x17, [x16, #1904]
    cdb8:	add	x16, x16, #0x770
    cdbc:	br	x17

000000000000cdc0 <obstack_vprintf@plt>:
    cdc0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cdc4:	ldr	x17, [x16, #1912]
    cdc8:	add	x16, x16, #0x778
    cdcc:	br	x17

000000000000cdd0 <__gmpq_init@plt>:
    cdd0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cdd4:	ldr	x17, [x16, #1920]
    cdd8:	add	x16, x16, #0x780
    cddc:	br	x17

000000000000cde0 <__gmpn_hgcd@plt>:
    cde0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cde4:	ldr	x17, [x16, #1928]
    cde8:	add	x16, x16, #0x788
    cdec:	br	x17

000000000000cdf0 <__gmpz_mod@plt>:
    cdf0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cdf4:	ldr	x17, [x16, #1936]
    cdf8:	add	x16, x16, #0x790
    cdfc:	br	x17

000000000000ce00 <__gmpf_sub@plt>:
    ce00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce04:	ldr	x17, [x16, #1944]
    ce08:	add	x16, x16, #0x798
    ce0c:	br	x17

000000000000ce10 <__gmpn_dcpi1_bdiv_q@plt>:
    ce10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce14:	ldr	x17, [x16, #1952]
    ce18:	add	x16, x16, #0x7a0
    ce1c:	br	x17

000000000000ce20 <__gmpf_get_str@plt>:
    ce20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce24:	ldr	x17, [x16, #1960]
    ce28:	add	x16, x16, #0x7a8
    ce2c:	br	x17

000000000000ce30 <fwrite@plt>:
    ce30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce34:	ldr	x17, [x16, #1968]
    ce38:	add	x16, x16, #0x7b0
    ce3c:	br	x17

000000000000ce40 <__gmpn_hamdist@plt>:
    ce40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce44:	ldr	x17, [x16, #1976]
    ce48:	add	x16, x16, #0x7b8
    ce4c:	br	x17

000000000000ce50 <__gmpz_init_set_ui@plt>:
    ce50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce54:	ldr	x17, [x16, #1984]
    ce58:	add	x16, x16, #0x7c0
    ce5c:	br	x17

000000000000ce60 <__gmpf_init@plt>:
    ce60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce64:	ldr	x17, [x16, #1992]
    ce68:	add	x16, x16, #0x7c8
    ce6c:	br	x17

000000000000ce70 <__gmpz_cmp@plt>:
    ce70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce74:	ldr	x17, [x16, #2000]
    ce78:	add	x16, x16, #0x7d0
    ce7c:	br	x17

000000000000ce80 <__gmpn_mod_1s_2p@plt>:
    ce80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce84:	ldr	x17, [x16, #2008]
    ce88:	add	x16, x16, #0x7d8
    ce8c:	br	x17

000000000000ce90 <__gmpn_add_nc@plt>:
    ce90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ce94:	ldr	x17, [x16, #2016]
    ce98:	add	x16, x16, #0x7e0
    ce9c:	br	x17

000000000000cea0 <__gmpn_jacobi_n@plt>:
    cea0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cea4:	ldr	x17, [x16, #2024]
    cea8:	add	x16, x16, #0x7e8
    ceac:	br	x17

000000000000ceb0 <__gmpf_init2@plt>:
    ceb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ceb4:	ldr	x17, [x16, #2032]
    ceb8:	add	x16, x16, #0x7f0
    cebc:	br	x17

000000000000cec0 <__gmpn_mullo_n@plt>:
    cec0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cec4:	ldr	x17, [x16, #2040]
    cec8:	add	x16, x16, #0x7f8
    cecc:	br	x17

000000000000ced0 <__gmpf_div@plt>:
    ced0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    ced4:	ldr	x17, [x16, #2048]
    ced8:	add	x16, x16, #0x800
    cedc:	br	x17

000000000000cee0 <__gmpn_sbpi1_div_q@plt>:
    cee0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cee4:	ldr	x17, [x16, #2056]
    cee8:	add	x16, x16, #0x808
    ceec:	br	x17

000000000000cef0 <__gmpn_sec_pi1_div_qr@plt>:
    cef0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cef4:	ldr	x17, [x16, #2064]
    cef8:	add	x16, x16, #0x810
    cefc:	br	x17

000000000000cf00 <__gmpn_toom43_mul@plt>:
    cf00:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf04:	ldr	x17, [x16, #2072]
    cf08:	add	x16, x16, #0x818
    cf0c:	br	x17

000000000000cf10 <vsprintf@plt>:
    cf10:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf14:	ldr	x17, [x16, #2080]
    cf18:	add	x16, x16, #0x820
    cf1c:	br	x17

000000000000cf20 <__gmpn_div_qr_2u_pi1@plt>:
    cf20:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf24:	ldr	x17, [x16, #2088]
    cf28:	add	x16, x16, #0x828
    cf2c:	br	x17

000000000000cf30 <__gmpn_zero@plt>:
    cf30:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf34:	ldr	x17, [x16, #2096]
    cf38:	add	x16, x16, #0x830
    cf3c:	br	x17

000000000000cf40 <__gmp_randinit_lc_2exp@plt>:
    cf40:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf44:	ldr	x17, [x16, #2104]
    cf48:	add	x16, x16, #0x838
    cf4c:	br	x17

000000000000cf50 <__gmpn_bdiv_qr@plt>:
    cf50:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf54:	ldr	x17, [x16, #2112]
    cf58:	add	x16, x16, #0x840
    cf5c:	br	x17

000000000000cf60 <__gmpn_mod_34lsub1@plt>:
    cf60:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf64:	ldr	x17, [x16, #2120]
    cf68:	add	x16, x16, #0x848
    cf6c:	br	x17

000000000000cf70 <__gmpz_gcd@plt>:
    cf70:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf74:	ldr	x17, [x16, #2128]
    cf78:	add	x16, x16, #0x850
    cf7c:	br	x17

000000000000cf80 <__gmpz_aorsmul_1@plt>:
    cf80:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf84:	ldr	x17, [x16, #2136]
    cf88:	add	x16, x16, #0x858
    cf8c:	br	x17

000000000000cf90 <__gmpz_add@plt>:
    cf90:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cf94:	ldr	x17, [x16, #2144]
    cf98:	add	x16, x16, #0x860
    cf9c:	br	x17

000000000000cfa0 <__gmpn_hgcd_matrix_mul@plt>:
    cfa0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cfa4:	ldr	x17, [x16, #2152]
    cfa8:	add	x16, x16, #0x868
    cfac:	br	x17

000000000000cfb0 <__gmp_randseed@plt>:
    cfb0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cfb4:	ldr	x17, [x16, #2160]
    cfb8:	add	x16, x16, #0x870
    cfbc:	br	x17

000000000000cfc0 <__gmpn_toom_eval_pm1@plt>:
    cfc0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cfc4:	ldr	x17, [x16, #2168]
    cfc8:	add	x16, x16, #0x878
    cfcc:	br	x17

000000000000cfd0 <__gmpn_mu_div_q_itch@plt>:
    cfd0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cfd4:	ldr	x17, [x16, #2176]
    cfd8:	add	x16, x16, #0x880
    cfdc:	br	x17

000000000000cfe0 <__gmpq_mul@plt>:
    cfe0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cfe4:	ldr	x17, [x16, #2184]
    cfe8:	add	x16, x16, #0x888
    cfec:	br	x17

000000000000cff0 <__gmp_sqrt_of_negative@plt>:
    cff0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    cff4:	ldr	x17, [x16, #2192]
    cff8:	add	x16, x16, #0x890
    cffc:	br	x17

000000000000d000 <__gmpn_powm@plt>:
    d000:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d004:	ldr	x17, [x16, #2200]
    d008:	add	x16, x16, #0x898
    d00c:	br	x17

000000000000d010 <__gmpn_mu_div_qr_itch@plt>:
    d010:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d014:	ldr	x17, [x16, #2208]
    d018:	add	x16, x16, #0x8a0
    d01c:	br	x17

000000000000d020 <__gmpn_hgcd_matrix_update_q@plt>:
    d020:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d024:	ldr	x17, [x16, #2216]
    d028:	add	x16, x16, #0x8a8
    d02c:	br	x17

000000000000d030 <__gmp_doprnt@plt>:
    d030:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d034:	ldr	x17, [x16, #2224]
    d038:	add	x16, x16, #0x8b0
    d03c:	br	x17

000000000000d040 <_obstack_newchunk@plt>:
    d040:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d044:	ldr	x17, [x16, #2232]
    d048:	add	x16, x16, #0x8b8
    d04c:	br	x17

000000000000d050 <__gmpn_fib2m@plt>:
    d050:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d054:	ldr	x17, [x16, #2240]
    d058:	add	x16, x16, #0x8c0
    d05c:	br	x17

000000000000d060 <__gmpn_invertappr@plt>:
    d060:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d064:	ldr	x17, [x16, #2248]
    d068:	add	x16, x16, #0x8c8
    d06c:	br	x17

000000000000d070 <__gmpn_fib2_ui@plt>:
    d070:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d074:	ldr	x17, [x16, #2256]
    d078:	add	x16, x16, #0x8d0
    d07c:	br	x17

000000000000d080 <__gmpn_preinv_mu_div_qr@plt>:
    d080:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d084:	ldr	x17, [x16, #2264]
    d088:	add	x16, x16, #0x8d8
    d08c:	br	x17

000000000000d090 <__gmpn_rsblsh1_n@plt>:
    d090:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d094:	ldr	x17, [x16, #2272]
    d098:	add	x16, x16, #0x8e0
    d09c:	br	x17

000000000000d0a0 <__gmpz_init_set_str@plt>:
    d0a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0a4:	ldr	x17, [x16, #2280]
    d0a8:	add	x16, x16, #0x8e8
    d0ac:	br	x17

000000000000d0b0 <__gmpn_perfect_square_p@plt>:
    d0b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0b4:	ldr	x17, [x16, #2288]
    d0b8:	add	x16, x16, #0x8f0
    d0bc:	br	x17

000000000000d0c0 <__gmpz_fdiv_r_2exp@plt>:
    d0c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0c4:	ldr	x17, [x16, #2296]
    d0c8:	add	x16, x16, #0x8f8
    d0cc:	br	x17

000000000000d0d0 <__gmpz_inp_str@plt>:
    d0d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0d4:	ldr	x17, [x16, #2304]
    d0d8:	add	x16, x16, #0x900
    d0dc:	br	x17

000000000000d0e0 <__gmpn_toom_eval_pm2rexp@plt>:
    d0e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0e4:	ldr	x17, [x16, #2312]
    d0e8:	add	x16, x16, #0x908
    d0ec:	br	x17

000000000000d0f0 <__gmpn_redc_1@plt>:
    d0f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d0f4:	ldr	x17, [x16, #2320]
    d0f8:	add	x16, x16, #0x910
    d0fc:	br	x17

000000000000d100 <__isoc99_sscanf@plt>:
    d100:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d104:	ldr	x17, [x16, #2328]
    d108:	add	x16, x16, #0x918
    d10c:	br	x17

000000000000d110 <vsnprintf@plt>:
    d110:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d114:	ldr	x17, [x16, #2336]
    d118:	add	x16, x16, #0x920
    d11c:	br	x17

000000000000d120 <__gmpn_strongfibo@plt>:
    d120:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d124:	ldr	x17, [x16, #2344]
    d128:	add	x16, x16, #0x928
    d12c:	br	x17

000000000000d130 <__gmpz_init2@plt>:
    d130:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d134:	ldr	x17, [x16, #2352]
    d138:	add	x16, x16, #0x930
    d13c:	br	x17

000000000000d140 <__gmpn_gcdext_1@plt>:
    d140:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d144:	ldr	x17, [x16, #2360]
    d148:	add	x16, x16, #0x938
    d14c:	br	x17

000000000000d150 <__gmpn_scan1@plt>:
    d150:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d154:	ldr	x17, [x16, #2368]
    d158:	add	x16, x16, #0x940
    d15c:	br	x17

000000000000d160 <__gmpn_lshiftc@plt>:
    d160:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d164:	ldr	x17, [x16, #2376]
    d168:	add	x16, x16, #0x948
    d16c:	br	x17

000000000000d170 <__gmpn_mu_bdiv_qr_itch@plt>:
    d170:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d174:	ldr	x17, [x16, #2384]
    d178:	add	x16, x16, #0x950
    d17c:	br	x17

000000000000d180 <__gmpn_ni_invertappr@plt>:
    d180:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d184:	ldr	x17, [x16, #2392]
    d188:	add	x16, x16, #0x958
    d18c:	br	x17

000000000000d190 <__gmp_randinit_lc_2exp_size@plt>:
    d190:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d194:	ldr	x17, [x16, #2400]
    d198:	add	x16, x16, #0x960
    d19c:	br	x17

000000000000d1a0 <__gmp_init_primesieve@plt>:
    d1a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1a4:	ldr	x17, [x16, #2408]
    d1a8:	add	x16, x16, #0x968
    d1ac:	br	x17

000000000000d1b0 <__gmpn_gcdext_lehmer_n@plt>:
    d1b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1b4:	ldr	x17, [x16, #2416]
    d1b8:	add	x16, x16, #0x970
    d1bc:	br	x17

000000000000d1c0 <__gmpn_random2@plt>:
    d1c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1c4:	ldr	x17, [x16, #2424]
    d1c8:	add	x16, x16, #0x978
    d1cc:	br	x17

000000000000d1d0 <__gmpn_fft_next_size@plt>:
    d1d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1d4:	ldr	x17, [x16, #2432]
    d1d8:	add	x16, x16, #0x980
    d1dc:	br	x17

000000000000d1e0 <__gmpn_binvert_itch@plt>:
    d1e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1e4:	ldr	x17, [x16, #2440]
    d1e8:	add	x16, x16, #0x988
    d1ec:	br	x17

000000000000d1f0 <__gmpz_cmp_ui@plt>:
    d1f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d1f4:	ldr	x17, [x16, #2448]
    d1f8:	add	x16, x16, #0x990
    d1fc:	br	x17

000000000000d200 <__gmp_primesieve@plt>:
    d200:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d204:	ldr	x17, [x16, #2456]
    d208:	add	x16, x16, #0x998
    d20c:	br	x17

000000000000d210 <__gmpn_pow_1@plt>:
    d210:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d214:	ldr	x17, [x16, #2464]
    d218:	add	x16, x16, #0x9a0
    d21c:	br	x17

000000000000d220 <__gmpz_export@plt>:
    d220:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d224:	ldr	x17, [x16, #2472]
    d228:	add	x16, x16, #0x9a8
    d22c:	br	x17

000000000000d230 <__gmp_doprnt_mpf2@plt>:
    d230:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d234:	ldr	x17, [x16, #2480]
    d238:	add	x16, x16, #0x9b0
    d23c:	br	x17

000000000000d240 <__gmpn_mul_1c@plt>:
    d240:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d244:	ldr	x17, [x16, #2488]
    d248:	add	x16, x16, #0x9b8
    d24c:	br	x17

000000000000d250 <__gmpz_init@plt>:
    d250:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d254:	ldr	x17, [x16, #2496]
    d258:	add	x16, x16, #0x9c0
    d25c:	br	x17

000000000000d260 <__gmpz_sizeinbase@plt>:
    d260:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d264:	ldr	x17, [x16, #2504]
    d268:	add	x16, x16, #0x9c8
    d26c:	br	x17

000000000000d270 <__gmpz_set_si@plt>:
    d270:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d274:	ldr	x17, [x16, #2512]
    d278:	add	x16, x16, #0x9d0
    d27c:	br	x17

000000000000d280 <__gmp_extract_double@plt>:
    d280:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d284:	ldr	x17, [x16, #2520]
    d288:	add	x16, x16, #0x9d8
    d28c:	br	x17

000000000000d290 <__gmpn_mullo_basecase@plt>:
    d290:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d294:	ldr	x17, [x16, #2528]
    d298:	add	x16, x16, #0x9e0
    d29c:	br	x17

000000000000d2a0 <__gmpn_toom3_sqr@plt>:
    d2a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2a4:	ldr	x17, [x16, #2536]
    d2a8:	add	x16, x16, #0x9e8
    d2ac:	br	x17

000000000000d2b0 <__gmpn_gcd_subdiv_step@plt>:
    d2b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2b4:	ldr	x17, [x16, #2544]
    d2b8:	add	x16, x16, #0x9f0
    d2bc:	br	x17

000000000000d2c0 <__gmpz_powm_ui@plt>:
    d2c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2c4:	ldr	x17, [x16, #2552]
    d2c8:	add	x16, x16, #0x9f8
    d2cc:	br	x17

000000000000d2d0 <vfprintf@plt>:
    d2d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2d4:	ldr	x17, [x16, #2560]
    d2d8:	add	x16, x16, #0xa00
    d2dc:	br	x17

000000000000d2e0 <printf@plt>:
    d2e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2e4:	ldr	x17, [x16, #2568]
    d2e8:	add	x16, x16, #0xa08
    d2ec:	br	x17

000000000000d2f0 <__gmpn_hgcd_reduce@plt>:
    d2f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d2f4:	ldr	x17, [x16, #2576]
    d2f8:	add	x16, x16, #0xa10
    d2fc:	br	x17

000000000000d300 <__gmpn_dcpi1_bdiv_qr_n@plt>:
    d300:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d304:	ldr	x17, [x16, #2584]
    d308:	add	x16, x16, #0xa18
    d30c:	br	x17

000000000000d310 <putchar@plt>:
    d310:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d314:	ldr	x17, [x16, #2592]
    d318:	add	x16, x16, #0xa20
    d31c:	br	x17

000000000000d320 <__gmpz_addmul_ui@plt>:
    d320:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d324:	ldr	x17, [x16, #2600]
    d328:	add	x16, x16, #0xa28
    d32c:	br	x17

000000000000d330 <__gmpn_sqr_diag_addlsh1@plt>:
    d330:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d334:	ldr	x17, [x16, #2608]
    d338:	add	x16, x16, #0xa30
    d33c:	br	x17

000000000000d340 <__gmpn_gcd_11@plt>:
    d340:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d344:	ldr	x17, [x16, #2616]
    d348:	add	x16, x16, #0xa38
    d34c:	br	x17

000000000000d350 <__gmpn_toom_interpolate_16pts@plt>:
    d350:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d354:	ldr	x17, [x16, #2624]
    d358:	add	x16, x16, #0xa40
    d35c:	br	x17

000000000000d360 <__gmpn_divisible_p@plt>:
    d360:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d364:	ldr	x17, [x16, #2632]
    d368:	add	x16, x16, #0xa48
    d36c:	br	x17

000000000000d370 <__gmpn_sub_err2_n@plt>:
    d370:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d374:	ldr	x17, [x16, #2640]
    d378:	add	x16, x16, #0xa50
    d37c:	br	x17

000000000000d380 <__gmpn_bdiv_q_itch@plt>:
    d380:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d384:	ldr	x17, [x16, #2648]
    d388:	add	x16, x16, #0xa58
    d38c:	br	x17

000000000000d390 <__gmpn_hgcd_jacobi@plt>:
    d390:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d394:	ldr	x17, [x16, #2656]
    d398:	add	x16, x16, #0xa60
    d39c:	br	x17

000000000000d3a0 <__gmpn_divrem@plt>:
    d3a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3a4:	ldr	x17, [x16, #2664]
    d3a8:	add	x16, x16, #0xa68
    d3ac:	br	x17

000000000000d3b0 <__gmpn_sqrtrem@plt>:
    d3b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3b4:	ldr	x17, [x16, #2672]
    d3b8:	add	x16, x16, #0xa70
    d3bc:	br	x17

000000000000d3c0 <__gmpn_mu_bdiv_q@plt>:
    d3c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3c4:	ldr	x17, [x16, #2680]
    d3c8:	add	x16, x16, #0xa78
    d3cc:	br	x17

000000000000d3d0 <__gmp_exception@plt>:
    d3d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3d4:	ldr	x17, [x16, #2688]
    d3d8:	add	x16, x16, #0xa80
    d3dc:	br	x17

000000000000d3e0 <__gmpn_dcpi1_div_qr_n@plt>:
    d3e0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3e4:	ldr	x17, [x16, #2696]
    d3e8:	add	x16, x16, #0xa88
    d3ec:	br	x17

000000000000d3f0 <__gmpn_invert_limb@plt>:
    d3f0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d3f4:	ldr	x17, [x16, #2704]
    d3f8:	add	x16, x16, #0xa90
    d3fc:	br	x17

000000000000d400 <__gmpn_addmul_1@plt>:
    d400:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d404:	ldr	x17, [x16, #2712]
    d408:	add	x16, x16, #0xa98
    d40c:	br	x17

000000000000d410 <__gmpn_mod_1s_4p@plt>:
    d410:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d414:	ldr	x17, [x16, #2720]
    d418:	add	x16, x16, #0xaa0
    d41c:	br	x17

000000000000d420 <fprintf@plt>:
    d420:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d424:	ldr	x17, [x16, #2728]
    d428:	add	x16, x16, #0xaa8
    d42c:	br	x17

000000000000d430 <__gmpz_urandomb@plt>:
    d430:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d434:	ldr	x17, [x16, #2736]
    d438:	add	x16, x16, #0xab0
    d43c:	br	x17

000000000000d440 <__gmpn_hgcd_mul_matrix1_vector@plt>:
    d440:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d444:	ldr	x17, [x16, #2744]
    d448:	add	x16, x16, #0xab8
    d44c:	br	x17

000000000000d450 <__gmpn_toom22_mul@plt>:
    d450:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d454:	ldr	x17, [x16, #2752]
    d458:	add	x16, x16, #0xac0
    d45c:	br	x17

000000000000d460 <__gmp_mt_recalc_buffer@plt>:
    d460:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d464:	ldr	x17, [x16, #2760]
    d468:	add	x16, x16, #0xac8
    d46c:	br	x17

000000000000d470 <__gmpn_toom6_sqr@plt>:
    d470:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d474:	ldr	x17, [x16, #2768]
    d478:	add	x16, x16, #0xad0
    d47c:	br	x17

000000000000d480 <__gmpn_toom42_mul@plt>:
    d480:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d484:	ldr	x17, [x16, #2776]
    d488:	add	x16, x16, #0xad8
    d48c:	br	x17

000000000000d490 <__gmpn_mul_1@plt>:
    d490:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d494:	ldr	x17, [x16, #2784]
    d498:	add	x16, x16, #0xae0
    d49c:	br	x17

000000000000d4a0 <ferror@plt>:
    d4a0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d4a4:	ldr	x17, [x16, #2792]
    d4a8:	add	x16, x16, #0xae8
    d4ac:	br	x17

000000000000d4b0 <__gmpn_toom8_sqr@plt>:
    d4b0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d4b4:	ldr	x17, [x16, #2800]
    d4b8:	add	x16, x16, #0xaf0
    d4bc:	br	x17

000000000000d4c0 <__gmpf_div_2exp@plt>:
    d4c0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d4c4:	ldr	x17, [x16, #2808]
    d4c8:	add	x16, x16, #0xaf8
    d4cc:	br	x17

000000000000d4d0 <__gmpn_cnd_add_n@plt>:
    d4d0:	adrp	x16, 77000 <memcpy@GLIBC_2.17>
    d4d4:	ldr	x17, [x16, #2816]
    d4d8:	add	x16, x16, #0xb00
    d4dc:	br	x17

Disassembly of section .text:

000000000000d4e0 <__gmp_assert_header@@Base-0xd4>:
    d4e0:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d4e4:	ldr	x0, [x0, #3904]
    d4e8:	cbz	x0, d4f0 <__gmpn_cnd_add_n@plt+0x20>
    d4ec:	b	c8d0 <__gmon_start__@plt>
    d4f0:	ret
    d4f4:	nop
    d4f8:	adrp	x0, 77000 <memcpy@GLIBC_2.17>
    d4fc:	add	x0, x0, #0xb30
    d500:	adrp	x1, 77000 <memcpy@GLIBC_2.17>
    d504:	add	x1, x1, #0xb30
    d508:	cmp	x1, x0
    d50c:	b.eq	d524 <__gmpn_cnd_add_n@plt+0x54>  // b.none
    d510:	adrp	x1, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d514:	ldr	x1, [x1, #3784]
    d518:	cbz	x1, d524 <__gmpn_cnd_add_n@plt+0x54>
    d51c:	mov	x16, x1
    d520:	br	x16
    d524:	ret
    d528:	adrp	x0, 77000 <memcpy@GLIBC_2.17>
    d52c:	add	x0, x0, #0xb30
    d530:	adrp	x1, 77000 <memcpy@GLIBC_2.17>
    d534:	add	x1, x1, #0xb30
    d538:	sub	x1, x1, x0
    d53c:	lsr	x2, x1, #63
    d540:	add	x1, x2, x1, asr #3
    d544:	cmp	xzr, x1, asr #1
    d548:	asr	x1, x1, #1
    d54c:	b.eq	d564 <__gmpn_cnd_add_n@plt+0x94>  // b.none
    d550:	adrp	x2, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d554:	ldr	x2, [x2, #4056]
    d558:	cbz	x2, d564 <__gmpn_cnd_add_n@plt+0x94>
    d55c:	mov	x16, x2
    d560:	br	x16
    d564:	ret
    d568:	stp	x29, x30, [sp, #-32]!
    d56c:	mov	x29, sp
    d570:	str	x19, [sp, #16]
    d574:	adrp	x19, 77000 <memcpy@GLIBC_2.17>
    d578:	ldrb	w0, [x19, #2864]
    d57c:	cbnz	w0, d5a4 <__gmpn_cnd_add_n@plt+0xd4>
    d580:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d584:	ldr	x0, [x0, #3800]
    d588:	cbz	x0, d598 <__gmpn_cnd_add_n@plt+0xc8>
    d58c:	adrp	x0, 77000 <memcpy@GLIBC_2.17>
    d590:	ldr	x0, [x0, #2824]
    d594:	bl	c1d0 <__cxa_finalize@plt>
    d598:	bl	d4f8 <__gmpn_cnd_add_n@plt+0x28>
    d59c:	mov	w0, #0x1                   	// #1
    d5a0:	strb	w0, [x19, #2864]
    d5a4:	ldr	x19, [sp, #16]
    d5a8:	ldp	x29, x30, [sp], #32
    d5ac:	ret
    d5b0:	b	d528 <__gmpn_cnd_add_n@plt+0x58>

000000000000d5b4 <__gmp_assert_header@@Base>:
    d5b4:	stp	x29, x30, [sp, #-32]!
    d5b8:	stp	x20, x19, [sp, #16]
    d5bc:	mov	x29, sp
    d5c0:	cbz	x0, d5d0 <__gmp_assert_header@@Base+0x1c>
    d5c4:	ldrb	w8, [x0]
    d5c8:	mov	x2, x0
    d5cc:	cbnz	w8, d5dc <__gmp_assert_header@@Base+0x28>
    d5d0:	ldp	x20, x19, [sp, #16]
    d5d4:	ldp	x29, x30, [sp], #32
    d5d8:	ret
    d5dc:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d5e0:	ldr	x20, [x20, #3824]
    d5e4:	mov	w19, w1
    d5e8:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d5ec:	add	x1, x1, #0x920
    d5f0:	ldr	x0, [x20]
    d5f4:	bl	d420 <fprintf@plt>
    d5f8:	cmn	w19, #0x1
    d5fc:	b.eq	d5d0 <__gmp_assert_header@@Base+0x1c>  // b.none
    d600:	ldr	x0, [x20]
    d604:	mov	w2, w19
    d608:	ldp	x20, x19, [sp, #16]
    d60c:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d610:	add	x1, x1, #0x924
    d614:	ldp	x29, x30, [sp], #32
    d618:	b	d420 <fprintf@plt>

000000000000d61c <__gmp_assert_fail@@Base>:
    d61c:	stp	x29, x30, [sp, #-32]!
    d620:	str	x19, [sp, #16]
    d624:	mov	x29, sp
    d628:	mov	x19, x2
    d62c:	bl	cdb0 <__gmp_assert_header@plt>
    d630:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d634:	ldr	x8, [x8, #3824]
    d638:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d63c:	add	x1, x1, #0x929
    d640:	mov	x2, x19
    d644:	ldr	x0, [x8]
    d648:	bl	d420 <fprintf@plt>
    d64c:	bl	c900 <abort@plt>

000000000000d650 <__gmpn_divexact_by3@@Base>:
    d650:	stp	x29, x30, [sp, #-16]!
    d654:	mov	x3, #0x5555555555555555    	// #6148914691236517205
    d658:	mov	x4, xzr
    d65c:	mov	x29, sp
    d660:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
    d664:	and	x0, x0, #0x3
    d668:	ldp	x29, x30, [sp], #16
    d66c:	ret

000000000000d670 <__gmpn_divmod_1@@Base>:
    d670:	mov	x4, x3
    d674:	mov	x3, x2
    d678:	mov	x2, x1
    d67c:	mov	x1, xzr
    d680:	b	cd00 <__gmpn_divrem_1@plt>

000000000000d684 <__gmpz_legendre@@Base>:
    d684:	b	c070 <__gmpz_jacobi@plt>

000000000000d688 <__gmp_exception@@Base>:
    d688:	stp	x29, x30, [sp, #-16]!
    d68c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d690:	ldr	x8, [x8, #3896]
    d694:	mov	x29, sp
    d698:	ldr	w9, [x8]
    d69c:	orr	w9, w9, w0
    d6a0:	mov	w0, #0x8                   	// #8
    d6a4:	str	w9, [x8]
    d6a8:	bl	bfc0 <raise@plt>
    d6ac:	bl	c900 <abort@plt>

000000000000d6b0 <__gmp_sqrt_of_negative@@Base>:
    d6b0:	stp	x29, x30, [sp, #-16]!
    d6b4:	mov	w0, #0x4                   	// #4
    d6b8:	mov	x29, sp
    d6bc:	bl	d3d0 <__gmp_exception@plt>

000000000000d6c0 <__gmp_divide_by_zero@@Base>:
    d6c0:	stp	x29, x30, [sp, #-16]!
    d6c4:	mov	w0, #0x2                   	// #2
    d6c8:	mov	x29, sp
    d6cc:	bl	d3d0 <__gmp_exception@plt>

000000000000d6d0 <__gmp_extract_double@@Base>:
    d6d0:	fcmp	d0, #0.0
    d6d4:	b.ne	d6e8 <__gmp_extract_double@@Base+0x18>  // b.any
    d6d8:	mov	w8, wzr
    d6dc:	stp	xzr, xzr, [x0]
    d6e0:	mov	w0, w8
    d6e4:	ret
    d6e8:	fmov	x9, d0
    d6ec:	ubfx	x8, x9, #52, #11
    d6f0:	lsl	x9, x9, #11
    d6f4:	orr	x9, x9, #0x8000000000000000
    d6f8:	cbnz	x8, d70c <__gmp_extract_double@@Base+0x3c>
    d6fc:	mov	w8, #0x1                   	// #1
    d700:	lsl	x9, x9, #1
    d704:	sub	x8, x8, #0x1
    d708:	tbz	x9, #63, d700 <__gmp_extract_double@@Base+0x30>
    d70c:	add	x10, x8, #0xc02
    d710:	add	x11, x8, #0xc41
    d714:	cmp	x10, #0x0
    d718:	and	w8, w10, #0x3f
    d71c:	csel	x10, x11, x10, lt  // lt = tstop
    d720:	asr	x11, x10, #6
    d724:	cbz	w8, d744 <__gmp_extract_double@@Base+0x74>
    d728:	neg	w12, w8
    d72c:	lsl	x10, x9, x8
    d730:	lsr	x9, x9, x12
    d734:	sub	x8, x11, #0x3f
    d738:	stp	x10, x9, [x0]
    d73c:	mov	w0, w8
    d740:	ret
    d744:	mov	x10, xzr
    d748:	sub	x8, x11, #0x40
    d74c:	stp	x10, x9, [x0]
    d750:	mov	w0, w8
    d754:	ret

000000000000d758 <__gmp_invalid_operation@@Base>:
    d758:	stp	x29, x30, [sp, #-16]!
    d75c:	mov	w0, #0x8                   	// #8
    d760:	mov	x29, sp
    d764:	bl	bfc0 <raise@plt>
    d768:	bl	c900 <abort@plt>

000000000000d76c <__gmp_default_allocate@@Base>:
    d76c:	stp	x29, x30, [sp, #-32]!
    d770:	str	x19, [sp, #16]
    d774:	mov	x29, sp
    d778:	mov	x19, x0
    d77c:	bl	c410 <malloc@plt>
    d780:	cbz	x0, d790 <__gmp_default_allocate@@Base+0x24>
    d784:	ldr	x19, [sp, #16]
    d788:	ldp	x29, x30, [sp], #32
    d78c:	ret
    d790:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d794:	ldr	x8, [x8, #3824]
    d798:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d79c:	add	x1, x1, #0x946
    d7a0:	mov	x2, x19
    d7a4:	ldr	x0, [x8]
    d7a8:	bl	d420 <fprintf@plt>
    d7ac:	bl	c900 <abort@plt>

000000000000d7b0 <__gmp_default_reallocate@@Base>:
    d7b0:	stp	x29, x30, [sp, #-32]!
    d7b4:	stp	x20, x19, [sp, #16]
    d7b8:	mov	x20, x1
    d7bc:	mov	x1, x2
    d7c0:	mov	x29, sp
    d7c4:	mov	x19, x2
    d7c8:	bl	c7c0 <realloc@plt>
    d7cc:	cbz	x0, d7dc <__gmp_default_reallocate@@Base+0x2c>
    d7d0:	ldp	x20, x19, [sp, #16]
    d7d4:	ldp	x29, x30, [sp], #32
    d7d8:	ret
    d7dc:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d7e0:	ldr	x8, [x8, #3824]
    d7e4:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d7e8:	add	x1, x1, #0x971
    d7ec:	mov	x2, x20
    d7f0:	ldr	x0, [x8]
    d7f4:	mov	x3, x19
    d7f8:	bl	d420 <fprintf@plt>
    d7fc:	bl	c900 <abort@plt>

000000000000d800 <__gmp_default_free@@Base>:
    d800:	b	cc30 <free@plt>

000000000000d804 <__gmp_get_memory_functions@@Base>:
    d804:	cbz	x0, d818 <__gmp_get_memory_functions@@Base+0x14>
    d808:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d80c:	ldr	x8, [x8, #3840]
    d810:	ldr	x8, [x8]
    d814:	str	x8, [x0]
    d818:	cbz	x1, d82c <__gmp_get_memory_functions@@Base+0x28>
    d81c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d820:	ldr	x8, [x8, #3792]
    d824:	ldr	x8, [x8]
    d828:	str	x8, [x1]
    d82c:	cbz	x2, d840 <__gmp_get_memory_functions@@Base+0x3c>
    d830:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d834:	ldr	x8, [x8, #4016]
    d838:	ldr	x8, [x8]
    d83c:	str	x8, [x2]
    d840:	ret

000000000000d844 <__gmp_set_memory_functions@@Base>:
    d844:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d848:	ldr	x8, [x8, #4008]
    d84c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d850:	ldr	x9, [x9, #3840]
    d854:	cmp	x0, #0x0
    d858:	csel	x8, x8, x0, eq  // eq = none
    d85c:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d860:	str	x8, [x9]
    d864:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d868:	ldr	x8, [x8, #3912]
    d86c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d870:	ldr	x9, [x9, #4032]
    d874:	ldr	x10, [x10, #3792]
    d878:	cmp	x1, #0x0
    d87c:	csel	x8, x8, x1, eq  // eq = none
    d880:	cmp	x2, #0x0
    d884:	str	x8, [x10]
    d888:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    d88c:	ldr	x8, [x8, #4016]
    d890:	csel	x9, x9, x2, eq  // eq = none
    d894:	str	x9, [x8]
    d898:	ret

000000000000d89c <__gmp_nextprime@@Base>:
    d89c:	str	x19, [sp, #-16]!
    d8a0:	mov	x10, x0
    d8a4:	ldr	x3, [x10], #31
    d8a8:	mov	x18, #0x2493                	// #9363
    d8ac:	movk	x18, #0x9249, lsl #16
    d8b0:	mov	x16, #0xffffffffffffffe7    	// #-25
    d8b4:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    d8b8:	mov	x15, #0xcccccccccccccccc    	// #-3689348814741910324
    d8bc:	movk	x18, #0x4924, lsl #32
    d8c0:	adrp	x1, 59000 <__gmp_randget_mt@@Base+0x44c>
    d8c4:	add	x8, x0, #0x18
    d8c8:	add	x9, x0, #0x218
    d8cc:	add	x11, x0, #0x1d
    d8d0:	add	x12, x0, #0x1b
    d8d4:	movi	v0.2d, #0x0
    d8d8:	movk	x13, #0xaaab
    d8dc:	mov	w14, #0x1                   	// #1
    d8e0:	movk	x15, #0xcccd
    d8e4:	sub	x16, x16, x0
    d8e8:	mov	w17, #0x5                   	// #5
    d8ec:	movk	x18, #0x2492, lsl #48
    d8f0:	add	x1, x1, #0xc0e
    d8f4:	mov	w2, #0x30                  	// #48
    d8f8:	b	d904 <__gmp_nextprime@@Base+0x68>
    d8fc:	mov	x3, xzr
    d900:	str	xzr, [x0]
    d904:	add	x4, x0, x3
    d908:	ldrb	w4, [x4, #24]
    d90c:	add	x3, x3, #0x1
    d910:	cbnz	w4, d904 <__gmp_nextprime@@Base+0x68>
    d914:	cmp	x3, #0x201
    d918:	b.ne	dc10 <__gmp_nextprime@@Base+0x374>  // b.any
    d91c:	ldr	x3, [x0, #8]
    d920:	cmp	x3, #0x2
    d924:	b.ls	dc34 <__gmp_nextprime@@Base+0x398>  // b.plast
    d928:	stp	q0, q0, [x8, #480]
    d92c:	stp	q0, q0, [x8, #448]
    d930:	stp	q0, q0, [x8, #416]
    d934:	stp	q0, q0, [x8, #384]
    d938:	stp	q0, q0, [x8, #352]
    d93c:	stp	q0, q0, [x8, #320]
    d940:	stp	q0, q0, [x8, #288]
    d944:	stp	q0, q0, [x8, #256]
    d948:	stp	q0, q0, [x8, #224]
    d94c:	stp	q0, q0, [x8, #192]
    d950:	stp	q0, q0, [x8, #160]
    d954:	stp	q0, q0, [x8, #128]
    d958:	stp	q0, q0, [x8, #96]
    d95c:	stp	q0, q0, [x8, #64]
    d960:	stp	q0, q0, [x8, #32]
    d964:	stp	q0, q0, [x8]
    d968:	ldr	x5, [x0, #16]
    d96c:	add	x4, x3, #0x400
    d970:	str	x4, [x0, #8]
    d974:	add	x6, x5, #0x1
    d978:	mul	x7, x6, x6
    d97c:	add	x6, x3, #0x7ff
    d980:	cmp	x7, x6
    d984:	b.hi	d9a0 <__gmp_nextprime@@Base+0x104>  // b.pmore
    d988:	add	x7, x5, #0x2
    d98c:	mul	x7, x7, x7
    d990:	cmp	x7, x6
    d994:	add	x5, x5, #0x1
    d998:	b.ls	d988 <__gmp_nextprime@@Base+0xec>  // b.plast
    d99c:	str	x5, [x0, #16]
    d9a0:	add	x3, x3, #0x403
    d9a4:	lsr	x3, x3, #1
    d9a8:	umulh	x5, x3, x13
    d9ac:	lsr	x5, x5, #1
    d9b0:	add	x5, x5, x5, lsl #1
    d9b4:	subs	x3, x3, x5
    d9b8:	eor	x3, x3, #0x3
    d9bc:	csel	x3, xzr, x3, eq  // eq = none
    d9c0:	add	x4, x4, x3, lsl #1
    d9c4:	add	x5, x3, #0x3
    d9c8:	cmp	x4, #0x4
    d9cc:	csel	x3, x5, x3, cc  // cc = lo, ul, last
    d9d0:	add	x4, x12, x3
    d9d4:	cmp	x4, x9
    d9d8:	csel	x5, x4, x9, hi  // hi = pmore
    d9dc:	add	x5, x5, x16
    d9e0:	add	x6, x0, x3
    d9e4:	sub	x5, x5, x3
    d9e8:	cmp	x5, #0x3
    d9ec:	add	x3, x6, #0x18
    d9f0:	b.cc	da28 <__gmp_nextprime@@Base+0x18c>  // b.lo, b.ul, b.last
    d9f4:	umulh	x5, x5, x13
    d9f8:	lsr	x5, x5, #1
    d9fc:	add	x5, x5, #0x1
    da00:	and	x6, x5, #0x7ffffffffffffffe
    da04:	add	x7, x6, x6, lsl #1
    da08:	add	x3, x3, x7
    da0c:	mov	x7, x6
    da10:	sturb	w14, [x4, #-3]
    da14:	strb	w14, [x4], #6
    da18:	subs	x7, x7, #0x2
    da1c:	b.ne	da10 <__gmp_nextprime@@Base+0x174>  // b.any
    da20:	cmp	x5, x6
    da24:	b.eq	da34 <__gmp_nextprime@@Base+0x198>  // b.none
    da28:	strb	w14, [x3], #3
    da2c:	cmp	x3, x9
    da30:	b.cc	da28 <__gmp_nextprime@@Base+0x18c>  // b.lo, b.ul, b.last
    da34:	ldr	x3, [x0, #8]
    da38:	add	x4, x3, #0x5
    da3c:	lsr	x4, x4, #1
    da40:	umulh	x5, x4, x15
    da44:	lsr	x5, x5, #2
    da48:	add	x5, x5, x5, lsl #2
    da4c:	subs	x4, x4, x5
    da50:	sub	x4, x17, x4
    da54:	csel	x4, xzr, x4, eq  // eq = none
    da58:	add	x5, x3, x4, lsl #1
    da5c:	add	x6, x4, #0x5
    da60:	cmp	x5, #0x6
    da64:	csel	x5, x6, x4, cc  // cc = lo, ul, last
    da68:	cmp	x5, #0x1ff
    da6c:	b.gt	dad8 <__gmp_nextprime@@Base+0x23c>
    da70:	add	x4, x11, x5
    da74:	cmp	x4, x9
    da78:	csel	x6, x4, x9, hi  // hi = pmore
    da7c:	add	x6, x6, x16
    da80:	add	x3, x0, x5
    da84:	sub	x5, x6, x5
    da88:	cmp	x5, #0x5
    da8c:	add	x3, x3, #0x18
    da90:	b.cc	dac8 <__gmp_nextprime@@Base+0x22c>  // b.lo, b.ul, b.last
    da94:	umulh	x5, x5, x15
    da98:	lsr	x5, x5, #2
    da9c:	add	x5, x5, #0x1
    daa0:	and	x6, x5, #0x7ffffffffffffffe
    daa4:	add	x7, x6, x6, lsl #2
    daa8:	add	x3, x3, x7
    daac:	mov	x7, x6
    dab0:	sturb	w14, [x4, #-5]
    dab4:	strb	w14, [x4], #10
    dab8:	subs	x7, x7, #0x2
    dabc:	b.ne	dab0 <__gmp_nextprime@@Base+0x214>  // b.any
    dac0:	cmp	x5, x6
    dac4:	b.eq	dad4 <__gmp_nextprime@@Base+0x238>  // b.none
    dac8:	strb	w14, [x3], #5
    dacc:	cmp	x3, x9
    dad0:	b.cc	dac8 <__gmp_nextprime@@Base+0x22c>  // b.lo, b.ul, b.last
    dad4:	ldr	x3, [x0, #8]
    dad8:	add	x4, x3, #0x7
    dadc:	lsr	x4, x4, #1
    dae0:	umulh	x5, x4, x18
    dae4:	sub	x6, x4, x5
    dae8:	add	x5, x5, x6, lsr #1
    daec:	lsr	x5, x5, #2
    daf0:	sub	x5, x5, x5, lsl #3
    daf4:	adds	x4, x4, x5
    daf8:	eor	x4, x4, #0x7
    dafc:	csel	x4, xzr, x4, eq  // eq = none
    db00:	add	x3, x3, x4, lsl #1
    db04:	add	x5, x4, #0x7
    db08:	cmp	x3, #0x8
    db0c:	csel	x3, x5, x4, cc  // cc = lo, ul, last
    db10:	add	x4, x10, x3
    db14:	cmp	x4, x9
    db18:	csel	x5, x4, x9, hi  // hi = pmore
    db1c:	add	x5, x5, x16
    db20:	add	x6, x0, x3
    db24:	sub	x5, x5, x3
    db28:	cmp	x5, #0x6
    db2c:	add	x3, x6, #0x18
    db30:	b.ls	db74 <__gmp_nextprime@@Base+0x2d8>  // b.plast
    db34:	umulh	x6, x5, x18
    db38:	sub	x5, x5, x6
    db3c:	add	x5, x6, x5, lsr #1
    db40:	lsr	x5, x5, #2
    db44:	add	x5, x5, #0x1
    db48:	and	x6, x5, #0x7ffffffffffffffe
    db4c:	lsl	x7, x6, #3
    db50:	sub	x7, x7, x6
    db54:	add	x3, x3, x7
    db58:	mov	x7, x6
    db5c:	sturb	w14, [x4, #-7]
    db60:	strb	w14, [x4], #14
    db64:	subs	x7, x7, #0x2
    db68:	b.ne	db5c <__gmp_nextprime@@Base+0x2c0>  // b.any
    db6c:	cmp	x5, x6
    db70:	b.eq	db80 <__gmp_nextprime@@Base+0x2e4>  // b.none
    db74:	strb	w14, [x3], #7
    db78:	cmp	x3, x9
    db7c:	b.cc	db74 <__gmp_nextprime@@Base+0x2d8>  // b.lo, b.ul, b.last
    db80:	ldr	x4, [x0, #16]
    db84:	cmp	x4, #0xb
    db88:	b.cc	d8fc <__gmp_nextprime@@Base+0x60>  // b.lo, b.ul, b.last
    db8c:	mov	x5, xzr
    db90:	mov	w3, #0xb                   	// #11
    db94:	b	dbb8 <__gmp_nextprime@@Base+0x31c>
    db98:	ldrb	w6, [x1, x5]
    db9c:	add	x5, x5, #0x1
    dba0:	umulh	x7, x5, x13
    dba4:	add	x3, x3, x6
    dba8:	lsr	x6, x7, #5
    dbac:	cmp	x3, x4
    dbb0:	msub	x5, x6, x2, x5
    dbb4:	b.hi	d8fc <__gmp_nextprime@@Base+0x60>  // b.pmore
    dbb8:	ldr	x6, [x0, #8]
    dbbc:	add	x7, x6, x3
    dbc0:	lsr	x7, x7, #1
    dbc4:	udiv	x19, x7, x3
    dbc8:	msub	x7, x19, x3, x7
    dbcc:	sub	x19, x3, x7
    dbd0:	cmp	x7, #0x0
    dbd4:	csel	x7, xzr, x19, eq  // eq = none
    dbd8:	add	x6, x6, x7, lsl #1
    dbdc:	cmp	x6, x3
    dbe0:	csel	x6, xzr, x3, hi  // hi = pmore
    dbe4:	add	x6, x6, x7
    dbe8:	cmp	x6, #0x1ff
    dbec:	b.gt	db98 <__gmp_nextprime@@Base+0x2fc>
    dbf0:	add	x4, x0, x6
    dbf4:	add	x4, x4, #0x18
    dbf8:	strb	w14, [x4]
    dbfc:	add	x4, x4, x3
    dc00:	cmp	x4, x9
    dc04:	b.cc	dbf8 <__gmp_nextprime@@Base+0x35c>  // b.lo, b.ul, b.last
    dc08:	ldr	x4, [x0, #16]
    dc0c:	b	db98 <__gmp_nextprime@@Base+0x2fc>
    dc10:	ldr	x10, [x0, #8]
    dc14:	add	x9, x0, x3
    dc18:	sub	x8, x9, x8
    dc1c:	add	x9, x8, #0x18
    dc20:	add	x8, x10, x8, lsl #1
    dc24:	str	x9, [x0]
    dc28:	add	x0, x8, #0x2e
    dc2c:	ldr	x19, [sp], #16
    dc30:	ret
    dc34:	mov	x8, #0xfffffffffffffc03    	// #-1021
    dc38:	str	x8, [x0, #8]
    dc3c:	mov	w0, #0x2                   	// #2
    dc40:	ldr	x19, [sp], #16
    dc44:	ret

000000000000dc48 <__gmp_init_primesieve@@Base>:
    dc48:	mov	w8, #0x200                 	// #512
    dc4c:	stp	xzr, xzr, [x0, #8]
    dc50:	str	x8, [x0]
    dc54:	strb	wzr, [x0, #536]
    dc58:	ret

000000000000dc5c <__gmp_primesieve@@Base>:
    dc5c:	sub	sp, sp, #0x70
    dc60:	sub	x8, x1, #0x5
    dc64:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    dc68:	movk	x9, #0xaaab
    dc6c:	orr	x8, x8, #0x1
    dc70:	umulh	x9, x8, x9
    dc74:	stp	x22, x21, [sp, #80]
    dc78:	lsr	x21, x9, #7
    dc7c:	stp	x20, x19, [sp, #96]
    dc80:	mov	x19, x0
    dc84:	lsr	x10, x9, #1
    dc88:	cmp	x8, #0xc0, lsl #12
    dc8c:	add	x8, x21, #0x1
    dc90:	stp	x29, x30, [sp, #16]
    dc94:	stp	x28, x27, [sp, #32]
    dc98:	stp	x26, x25, [sp, #48]
    dc9c:	stp	x24, x23, [sp, #64]
    dca0:	add	x29, sp, #0x10
    dca4:	stp	x10, x8, [sp]
    dca8:	b.cc	e100 <__gmp_primesieve@@Base+0x4a4>  // b.lo, b.ul, b.last
    dcac:	mov	w28, #0x800                 	// #2048
    dcb0:	mov	x23, #0x184                 	// #388
    dcb4:	mov	x24, #0x2058                	// #8280
    dcb8:	mov	x25, #0x2120                	// #8480
    dcbc:	bfxil	x28, x8, #0, #11
    dcc0:	movk	x23, #0x4023, lsl #16
    dcc4:	movk	x24, #0x489, lsl #16
    dcc8:	movk	x25, #0x8840, lsl #16
    dccc:	mov	x27, #0x1244                	// #4676
    dcd0:	add	x8, x28, x28, lsl #1
    dcd4:	mov	w1, #0x1                   	// #1
    dcd8:	movk	x23, #0x180c, lsl #32
    dcdc:	movk	x24, #0x4a12, lsl #32
    dce0:	movk	x25, #0x210, lsl #32
    dce4:	movk	x27, #0x3068, lsl #16
    dce8:	bfi	x1, x8, #6, #14
    dcec:	mov	x0, x19
    dcf0:	movk	x23, #0x9402, lsl #48
    dcf4:	movk	x24, #0x8121, lsl #48
    dcf8:	movk	x25, #0x285, lsl #48
    dcfc:	movk	x27, #0xc81, lsl #32
    dd00:	mov	w22, #0x1                   	// #1
    dd04:	bl	e164 <__gmp_primesieve@@Base+0x508>
    dd08:	add	x8, x19, x28, lsl #3
    dd0c:	mov	w10, #0x6e                  	// #110
    dd10:	mov	w12, #0xb6                  	// #182
    dd14:	mov	w14, #0x40                  	// #64
    dd18:	b	dd2c <__gmp_primesieve@@Base+0xd0>
    dd1c:	add	x28, x28, #0x800
    dd20:	cmp	x28, x21
    dd24:	add	x8, x8, #0x4, lsl #12
    dd28:	b.hi	e108 <__gmp_primesieve@@Base+0x4ac>  // b.pmore
    dd2c:	mov	x9, #0x7905                	// #30981
    dd30:	lsl	x18, x28, #6
    dd34:	movk	x9, #0x904a, lsl #16
    dd38:	sub	x4, x18, #0x40
    dd3c:	movk	x9, #0x4a7, lsl #32
    dd40:	lsr	x3, x4, #1
    dd44:	movk	x9, #0x4a79, lsl #48
    dd48:	mov	x1, #0x2058                	// #8280
    dd4c:	umulh	x15, x3, x9
    dd50:	movk	x1, #0x489, lsl #16
    dd54:	mov	x2, #0x1244                	// #4676
    dd58:	lsr	x15, x15, #4
    dd5c:	movk	x1, #0x4a12, lsl #32
    dd60:	movk	x2, #0x3068, lsl #16
    dd64:	msub	x15, x15, x10, x4
    dd68:	movk	x1, #0x8121, lsl #48
    dd6c:	movk	x2, #0xc81, lsl #32
    dd70:	cbz	x15, dddc <__gmp_primesieve@@Base+0x180>
    dd74:	cmp	x15, #0x3f
    dd78:	b.hi	dda8 <__gmp_primesieve@@Base+0x14c>  // b.pmore
    dd7c:	neg	x16, x15
    dd80:	lsr	x17, x24, x15
    dd84:	lsl	x0, x27, x16
    dd88:	subs	x16, x15, #0x2e
    dd8c:	orr	x1, x0, x17
    dd90:	b.hi	ddcc <__gmp_primesieve@@Base+0x170>  // b.pmore
    dd94:	mov	w9, #0x2e                  	// #46
    dd98:	sub	x16, x9, x15
    dd9c:	lsl	x16, x24, x16
    dda0:	lsr	x15, x27, x15
    dda4:	b	ddc4 <__gmp_primesieve@@Base+0x168>
    dda8:	sub	x16, x10, x15
    ddac:	lsr	x17, x27, x15
    ddb0:	sub	x15, x15, #0x2e
    ddb4:	lsl	x0, x24, x16
    ddb8:	lsl	x16, x27, x16
    ddbc:	lsr	x15, x24, x15
    ddc0:	orr	x1, x0, x17
    ddc4:	orr	x2, x16, x15
    ddc8:	b	dddc <__gmp_primesieve@@Base+0x180>
    ddcc:	sub	x15, x10, x15
    ddd0:	lsl	x15, x24, x15
    ddd4:	orr	x1, x1, x15
    ddd8:	lsr	x2, x24, x16
    dddc:	mov	x9, #0x2d03                	// #11523
    dde0:	movk	x9, #0x2d0, lsl #16
    dde4:	movk	x9, #0xd02d, lsl #32
    dde8:	movk	x9, #0x2d02, lsl #48
    ddec:	umulh	x15, x3, x9
    ddf0:	lsr	x15, x15, #4
    ddf4:	mov	x3, #0x184                 	// #388
    ddf8:	msub	x6, x15, x12, x4
    ddfc:	mov	x4, #0x2120                	// #8480
    de00:	movk	x3, #0x4023, lsl #16
    de04:	movk	x4, #0x8840, lsl #16
    de08:	mov	x5, #0x4421                	// #17441
    de0c:	movk	x3, #0x180c, lsl #32
    de10:	movk	x4, #0x210, lsl #32
    de14:	movk	x5, #0x1008, lsl #16
    de18:	add	x0, x19, x28, lsl #3
    de1c:	movk	x3, #0x9402, lsl #48
    de20:	movk	x4, #0x285, lsl #48
    de24:	movk	x5, #0xa412, lsl #32
    de28:	cbz	x6, df54 <__gmp_primesieve@@Base+0x2f8>
    de2c:	subs	x15, x6, #0x40
    de30:	b.hi	de88 <__gmp_primesieve@@Base+0x22c>  // b.pmore
    de34:	mov	x9, #0x4421                	// #17441
    de38:	movk	x9, #0x1008, lsl #16
    de3c:	neg	x15, x6
    de40:	lsr	x16, x23, x6
    de44:	lsr	x17, x25, x6
    de48:	cmp	x6, #0x40
    de4c:	movk	x9, #0xa412, lsl #32
    de50:	lsl	x3, x25, x15
    de54:	lsl	x4, x9, x15
    de58:	csel	x16, xzr, x16, eq  // eq = none
    de5c:	csel	x17, xzr, x17, eq  // eq = none
    de60:	subs	x15, x6, #0x36
    de64:	orr	x3, x3, x16
    de68:	orr	x4, x4, x17
    de6c:	b.hi	dee4 <__gmp_primesieve@@Base+0x288>  // b.pmore
    de70:	mov	w11, #0x36                  	// #54
    de74:	sub	x15, x11, x6
    de78:	lsl	x15, x23, x15
    de7c:	lsr	x16, x9, x6
    de80:	orr	x5, x15, x16
    de84:	b	df54 <__gmp_primesieve@@Base+0x2f8>
    de88:	cmp	x6, #0x7f
    de8c:	b.hi	defc <__gmp_primesieve@@Base+0x2a0>  // b.pmore
    de90:	mov	x9, #0x4421                	// #17441
    de94:	movk	x9, #0x1008, lsl #16
    de98:	neg	x16, x6
    de9c:	movk	x9, #0xa412, lsl #32
    dea0:	lsr	x17, x25, x6
    dea4:	lsl	x3, x9, x16
    dea8:	subs	x16, x6, #0x76
    deac:	orr	x3, x17, x3
    deb0:	b.hi	df38 <__gmp_primesieve@@Base+0x2dc>  // b.pmore
    deb4:	lsr	x15, x9, x15
    deb8:	mov	w9, #0x76                  	// #118
    debc:	sub	x16, x9, x6
    dec0:	lsl	x17, x23, x16
    dec4:	cmp	x6, #0x76
    dec8:	orr	x4, x15, x17
    decc:	lsl	x5, x25, x16
    ded0:	b.eq	df54 <__gmp_primesieve@@Base+0x2f8>  // b.none
    ded4:	sub	x15, x6, #0x36
    ded8:	lsr	x15, x23, x15
    dedc:	orr	x5, x5, x15
    dee0:	b	df54 <__gmp_primesieve@@Base+0x2f8>
    dee4:	mov	w9, #0x76                  	// #118
    dee8:	sub	x16, x9, x6
    deec:	lsl	x16, x23, x16
    def0:	orr	x4, x4, x16
    def4:	lsr	x5, x23, x15
    def8:	b	df54 <__gmp_primesieve@@Base+0x2f8>
    defc:	mov	x9, #0x4421                	// #17441
    df00:	movk	x9, #0x1008, lsl #16
    df04:	sub	x15, x12, x6
    df08:	movk	x9, #0xa412, lsl #32
    df0c:	sub	x17, x6, #0x76
    df10:	lsr	x16, x9, x6
    df14:	lsl	x3, x23, x15
    df18:	lsl	x4, x25, x15
    df1c:	lsr	x5, x23, x17
    df20:	lsl	x15, x9, x15
    df24:	lsr	x17, x25, x17
    df28:	orr	x3, x3, x16
    df2c:	orr	x4, x4, x5
    df30:	orr	x5, x15, x17
    df34:	b	df54 <__gmp_primesieve@@Base+0x2f8>
    df38:	sub	x15, x12, x6
    df3c:	lsr	x17, x23, x16
    df40:	lsl	x4, x23, x15
    df44:	lsl	x15, x25, x15
    df48:	orr	x3, x3, x4
    df4c:	orr	x4, x15, x17
    df50:	lsr	x5, x25, x16
    df54:	mov	x6, xzr
    df58:	orr	x17, x2, x1, lsl #46
    df5c:	add	x15, x8, x6
    df60:	orr	x16, x3, x1
    df64:	extr	x1, x2, x1, #18
    df68:	lsr	x7, x4, #10
    df6c:	add	x6, x6, #0x10
    df70:	orr	x2, x4, x17
    df74:	lsr	x17, x17, #18
    df78:	extr	x4, x4, x3, #10
    df7c:	cmp	x6, #0x4, lsl #12
    df80:	stp	x16, x2, [x15]
    df84:	orr	x3, x5, x3, lsl #54
    df88:	mov	x2, x17
    df8c:	mov	x5, x7
    df90:	b.ne	df58 <__gmp_primesieve@@Base+0x2fc>  // b.any
    df94:	mov	w9, #0x1ffff               	// #131071
    df98:	mov	x1, xzr
    df9c:	add	x2, x18, x9
    dfa0:	mov	w15, #0x4                   	// #4
    dfa4:	mov	w3, #0x10                  	// #16
    dfa8:	b	dfbc <__gmp_primesieve@@Base+0x360>
    dfac:	ror	x9, x3, #63
    dfb0:	add	x1, x1, x3, lsr #63
    dfb4:	mov	x15, x4
    dfb8:	mov	x3, x9
    dfbc:	ldr	x16, [x19, x1, lsl #3]
    dfc0:	add	x4, x15, #0x1
    dfc4:	tst	x16, x3
    dfc8:	b.ne	dfac <__gmp_primesieve@@Base+0x350>  // b.any
    dfcc:	add	x7, x4, x4, lsl #1
    dfd0:	and	x30, x4, #0x1
    dfd4:	add	x15, x15, #0x2
    dfd8:	add	x16, x7, x30
    dfdc:	neg	x17, x30
    dfe0:	add	x5, x16, #0x2
    dfe4:	and	x15, x15, x17
    dfe8:	madd	x15, x5, x4, x15
    dfec:	sub	x15, x15, #0x1
    dff0:	cmp	x15, x2
    dff4:	b.gt	dd1c <__gmp_primesieve@@Base+0xc0>
    dff8:	add	x16, x16, #0x1
    dffc:	lsl	x5, x16, #1
    e000:	add	x16, x5, #0x3f
    e004:	cmp	x5, #0x0
    e008:	csel	x16, x16, x5, lt  // lt = tstop
    e00c:	and	x16, x16, #0xffffffffffffffc0
    e010:	cmp	x15, x18
    e014:	sub	x6, x5, x16
    e018:	b.ge	e030 <__gmp_primesieve@@Base+0x3d4>  // b.tcont
    e01c:	mvn	x16, x15
    e020:	add	x16, x18, x16
    e024:	sdiv	x16, x16, x5
    e028:	add	x16, x16, #0x1
    e02c:	madd	x15, x16, x5, x15
    e030:	sub	x15, x15, x18
    e034:	cmp	x15, #0x20, lsl #12
    e038:	b.ge	e084 <__gmp_primesieve@@Base+0x428>  // b.tcont
    e03c:	sub	w20, w14, w6
    e040:	lsl	x17, x22, x15
    e044:	and	x16, x6, #0xfffffffe
    e048:	and	x20, x20, #0xfffffffe
    e04c:	add	x26, x15, #0x3f
    e050:	cmp	x15, #0x0
    e054:	csel	x26, x26, x15, lt  // lt = tstop
    e058:	asr	x26, x26, #6
    e05c:	lsl	x26, x26, #3
    e060:	ldr	x11, [x0, x26]
    e064:	lsl	x9, x17, x16
    e068:	lsr	x13, x17, x20
    e06c:	add	x15, x15, x5
    e070:	cmp	x15, #0x20, lsl #12
    e074:	orr	x11, x11, x17
    e078:	orr	x17, x9, x13
    e07c:	str	x11, [x0, x26]
    e080:	b.lt	e04c <__gmp_primesieve@@Base+0x3f0>  // b.tstop
    e084:	add	x9, x7, #0x6
    e088:	madd	x15, x9, x4, x30
    e08c:	cmp	x15, x18
    e090:	b.ge	e0a8 <__gmp_primesieve@@Base+0x44c>  // b.tcont
    e094:	mvn	x9, x15
    e098:	add	x9, x18, x9
    e09c:	sdiv	x9, x9, x5
    e0a0:	add	x9, x9, #0x1
    e0a4:	madd	x15, x9, x5, x15
    e0a8:	sub	x15, x15, x18
    e0ac:	cmp	x15, #0x20, lsl #12
    e0b0:	b.ge	dfac <__gmp_primesieve@@Base+0x350>  // b.tcont
    e0b4:	sub	w9, w14, w6
    e0b8:	lsl	x17, x22, x15
    e0bc:	and	x16, x6, #0xfffffffe
    e0c0:	and	x6, x9, #0xfffffffe
    e0c4:	add	x9, x15, #0x3f
    e0c8:	cmp	x15, #0x0
    e0cc:	csel	x9, x9, x15, lt  // lt = tstop
    e0d0:	asr	x9, x9, #6
    e0d4:	lsl	x9, x9, #3
    e0d8:	ldr	x13, [x0, x9]
    e0dc:	lsl	x11, x17, x16
    e0e0:	lsr	x7, x17, x6
    e0e4:	add	x15, x15, x5
    e0e8:	cmp	x15, #0x20, lsl #12
    e0ec:	orr	x13, x13, x17
    e0f0:	orr	x17, x11, x7
    e0f4:	str	x13, [x0, x9]
    e0f8:	b.lt	e0c4 <__gmp_primesieve@@Base+0x468>  // b.tstop
    e0fc:	b	dfac <__gmp_primesieve@@Base+0x350>
    e100:	mov	x0, x19
    e104:	bl	e164 <__gmp_primesieve@@Base+0x508>
    e108:	ldr	x8, [sp]
    e10c:	add	w8, w8, #0x1
    e110:	ands	x8, x8, #0x3f
    e114:	b.eq	e130 <__gmp_primesieve@@Base+0x4d4>  // b.none
    e118:	lsl	x9, x21, #3
    e11c:	ldr	x10, [x19, x9]
    e120:	mov	x11, #0xffffffffffffffff    	// #-1
    e124:	lsl	x8, x11, x8
    e128:	orr	x8, x10, x8
    e12c:	str	x8, [x19, x9]
    e130:	ldr	x1, [sp, #8]
    e134:	mov	x0, x19
    e138:	lsl	x20, x1, #6
    e13c:	bl	cd80 <__gmpn_popcount@plt>
    e140:	sub	x0, x20, x0
    e144:	ldp	x20, x19, [sp, #96]
    e148:	ldp	x22, x21, [sp, #80]
    e14c:	ldp	x24, x23, [sp, #64]
    e150:	ldp	x26, x25, [sp, #48]
    e154:	ldp	x28, x27, [sp, #32]
    e158:	ldp	x29, x30, [sp, #16]
    e15c:	add	sp, sp, #0x70
    e160:	ret
    e164:	sub	x8, x1, #0x5
    e168:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    e16c:	movk	x9, #0xaaab
    e170:	orr	x10, x8, #0x1
    e174:	umulh	x11, x8, x9
    e178:	umulh	x8, x10, x9
    e17c:	cmp	x10, #0xc0
    e180:	lsr	x8, x8, #1
    e184:	lsr	x9, x11, #7
    e188:	b.cc	e220 <__gmp_primesieve@@Base+0x5c4>  // b.lo, b.ul, b.last
    e18c:	mov	x11, #0x2120                	// #8480
    e190:	mov	x12, #0x184                 	// #388
    e194:	mov	x14, #0x2058                	// #8280
    e198:	mov	x13, #0x4421                	// #17441
    e19c:	movk	x11, #0x8840, lsl #16
    e1a0:	movk	x12, #0x4023, lsl #16
    e1a4:	mov	x16, #0x1244                	// #4676
    e1a8:	movk	x14, #0x489, lsl #16
    e1ac:	movk	x13, #0x1008, lsl #16
    e1b0:	movk	x11, #0x210, lsl #32
    e1b4:	movk	x12, #0x180c, lsl #32
    e1b8:	movk	x16, #0x3068, lsl #16
    e1bc:	movk	x14, #0x4a12, lsl #32
    e1c0:	add	x10, x0, #0x8
    e1c4:	movk	x13, #0xa412, lsl #32
    e1c8:	movk	x11, #0x285, lsl #48
    e1cc:	movk	x12, #0x9402, lsl #48
    e1d0:	movk	x16, #0xc81, lsl #32
    e1d4:	movk	x14, #0x8121, lsl #48
    e1d8:	mov	x15, x9
    e1dc:	orr	x17, x12, x14
    e1e0:	cmp	x15, #0x1
    e1e4:	str	x17, [x10]
    e1e8:	b.eq	e220 <__gmp_primesieve@@Base+0x5c4>  // b.none
    e1ec:	orr	x17, x16, x14, lsl #46
    e1f0:	extr	x14, x16, x14, #18
    e1f4:	lsr	x18, x11, #10
    e1f8:	orr	x16, x11, x17
    e1fc:	lsr	x17, x17, #18
    e200:	subs	x15, x15, #0x2
    e204:	extr	x11, x11, x12, #10
    e208:	orr	x12, x13, x12, lsl #54
    e20c:	str	x16, [x10, #8]
    e210:	add	x10, x10, #0x10
    e214:	mov	x16, x17
    e218:	mov	x13, x18
    e21c:	b.ne	e1dc <__gmp_primesieve@@Base+0x580>  // b.any
    e220:	mov	x11, #0x8480                	// #33920
    e224:	movk	x11, #0x6912, lsl #16
    e228:	movk	x11, #0xc9e0, lsl #32
    e22c:	add	w10, w8, #0x1
    e230:	movk	x11, #0x3294, lsl #48
    e234:	ands	x10, x10, #0x3f
    e238:	str	x11, [x0]
    e23c:	b.eq	e258 <__gmp_primesieve@@Base+0x5fc>  // b.none
    e240:	lsl	x9, x9, #3
    e244:	ldr	x11, [x0, x9]
    e248:	mov	x12, #0xffffffffffffffff    	// #-1
    e24c:	lsl	x10, x12, x10
    e250:	orr	x10, x11, x10
    e254:	str	x10, [x0, x9]
    e258:	cmp	x1, #0xd3
    e25c:	b.cc	e374 <__gmp_primesieve@@Base+0x718>  // b.lo, b.ul, b.last
    e260:	mov	x9, xzr
    e264:	mov	w14, #0x4                   	// #4
    e268:	mov	w11, #0x10                  	// #16
    e26c:	mov	w10, #0x1                   	// #1
    e270:	mov	w12, #0x40                  	// #64
    e274:	b	e288 <__gmp_primesieve@@Base+0x62c>
    e278:	ror	x14, x11, #63
    e27c:	add	x9, x9, x11, lsr #63
    e280:	mov	x11, x14
    e284:	mov	x14, x13
    e288:	ldr	x13, [x0, x9, lsl #3]
    e28c:	tst	x13, x11
    e290:	add	x13, x14, #0x1
    e294:	b.ne	e278 <__gmp_primesieve@@Base+0x61c>  // b.any
    e298:	add	x17, x13, x13, lsl #1
    e29c:	and	x18, x13, #0x1
    e2a0:	add	x15, x14, #0x2
    e2a4:	add	x14, x17, x18
    e2a8:	neg	x16, x18
    e2ac:	add	x1, x14, #0x2
    e2b0:	and	x15, x15, x16
    e2b4:	madd	x15, x1, x13, x15
    e2b8:	sub	x1, x15, #0x1
    e2bc:	cmp	x1, x8
    e2c0:	b.gt	e374 <__gmp_primesieve@@Base+0x718>
    e2c4:	add	x14, x14, #0x1
    e2c8:	lsl	x14, x14, #1
    e2cc:	add	x15, x14, #0x3f
    e2d0:	cmp	x14, #0x0
    e2d4:	csel	x15, x15, x14, lt  // lt = tstop
    e2d8:	and	x15, x15, #0xffffffffffffffc0
    e2dc:	sub	x16, x14, x15
    e2e0:	lsl	x2, x10, x1
    e2e4:	and	x15, x16, #0xfffffffe
    e2e8:	sub	w16, w12, w16
    e2ec:	add	x3, x1, #0x3f
    e2f0:	cmp	x1, #0x0
    e2f4:	csel	x3, x3, x1, lt  // lt = tstop
    e2f8:	asr	x3, x3, #6
    e2fc:	lsl	x3, x3, #3
    e300:	ldr	x5, [x0, x3]
    e304:	lsl	x4, x2, x15
    e308:	lsr	x6, x2, x16
    e30c:	add	x1, x1, x14
    e310:	orr	x2, x5, x2
    e314:	cmp	x1, x8
    e318:	str	x2, [x0, x3]
    e31c:	orr	x2, x4, x6
    e320:	b.le	e2ec <__gmp_primesieve@@Base+0x690>
    e324:	add	x17, x17, #0x6
    e328:	madd	x17, x17, x13, x18
    e32c:	cmp	x17, x8
    e330:	b.gt	e278 <__gmp_primesieve@@Base+0x61c>
    e334:	lsl	x18, x10, x17
    e338:	add	x1, x17, #0x3f
    e33c:	cmp	x17, #0x0
    e340:	csel	x1, x1, x17, lt  // lt = tstop
    e344:	asr	x1, x1, #6
    e348:	lsl	x1, x1, #3
    e34c:	ldr	x3, [x0, x1]
    e350:	lsl	x2, x18, x15
    e354:	lsr	x4, x18, x16
    e358:	add	x17, x17, x14
    e35c:	orr	x18, x3, x18
    e360:	cmp	x17, x8
    e364:	str	x18, [x0, x1]
    e368:	orr	x18, x2, x4
    e36c:	b.le	e338 <__gmp_primesieve@@Base+0x6dc>
    e370:	b	e278 <__gmp_primesieve@@Base+0x61c>
    e374:	ret

000000000000e378 <__gmp_tmp_reentrant_alloc@@Base>:
    e378:	stp	x29, x30, [sp, #-32]!
    e37c:	stp	x20, x19, [sp, #16]
    e380:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e384:	ldr	x8, [x8, #3840]
    e388:	add	x20, x1, #0x10
    e38c:	mov	x19, x0
    e390:	mov	x0, x20
    e394:	ldr	x8, [x8]
    e398:	mov	x29, sp
    e39c:	blr	x8
    e3a0:	str	x20, [x0, #8]
    e3a4:	ldr	x9, [x19]
    e3a8:	add	x8, x0, #0x10
    e3ac:	str	x9, [x0]
    e3b0:	str	x0, [x19]
    e3b4:	ldp	x20, x19, [sp, #16]
    e3b8:	mov	x0, x8
    e3bc:	ldp	x29, x30, [sp], #32
    e3c0:	ret

000000000000e3c4 <__gmp_tmp_reentrant_free@@Base>:
    e3c4:	stp	x29, x30, [sp, #-32]!
    e3c8:	stp	x20, x19, [sp, #16]
    e3cc:	mov	x29, sp
    e3d0:	cbz	x0, e3f0 <__gmp_tmp_reentrant_free@@Base+0x2c>
    e3d4:	adrp	x19, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e3d8:	ldr	x19, [x19, #4016]
    e3dc:	ldr	x8, [x19]
    e3e0:	ldp	x20, x1, [x0]
    e3e4:	blr	x8
    e3e8:	mov	x0, x20
    e3ec:	cbnz	x20, e3dc <__gmp_tmp_reentrant_free@@Base+0x18>
    e3f0:	ldp	x20, x19, [sp, #16]
    e3f4:	ldp	x29, x30, [sp], #32
    e3f8:	ret

000000000000e3fc <__gmpf_init@@Base>:
    e3fc:	stp	x29, x30, [sp, #-32]!
    e400:	str	x19, [sp, #16]
    e404:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e408:	ldr	x8, [x8, #3960]
    e40c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e410:	mov	x19, x0
    e414:	mov	x29, sp
    e418:	ldr	x8, [x8]
    e41c:	str	xzr, [x0, #8]
    e420:	stp	w8, wzr, [x0]
    e424:	ldr	x9, [x9, #3840]
    e428:	lsl	x8, x8, #3
    e42c:	add	x0, x8, #0x8
    e430:	ldr	x9, [x9]
    e434:	blr	x9
    e438:	str	x0, [x19, #16]
    e43c:	ldr	x19, [sp, #16]
    e440:	ldp	x29, x30, [sp], #32
    e444:	ret

000000000000e448 <__gmpf_init2@@Base>:
    e448:	stp	x29, x30, [sp, #-32]!
    e44c:	cmp	x1, #0x35
    e450:	mov	w8, #0x35                  	// #53
    e454:	csel	x8, x1, x8, hi  // hi = pmore
    e458:	add	x8, x8, #0x7f
    e45c:	lsr	x8, x8, #6
    e460:	str	x19, [sp, #16]
    e464:	str	xzr, [x0, #8]
    e468:	stp	w8, wzr, [x0]
    e46c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e470:	ldr	x9, [x9, #3840]
    e474:	lsl	x8, x8, #3
    e478:	mov	x19, x0
    e47c:	add	x0, x8, #0x8
    e480:	ldr	x9, [x9]
    e484:	mov	x29, sp
    e488:	blr	x9
    e48c:	str	x0, [x19, #16]
    e490:	ldr	x19, [sp, #16]
    e494:	ldp	x29, x30, [sp], #32
    e498:	ret

000000000000e49c <__gmpf_inits@@Base>:
    e49c:	sub	sp, sp, #0xf0
    e4a0:	stp	x29, x30, [sp, #224]
    e4a4:	add	x29, sp, #0xe0
    e4a8:	mov	x8, #0xffffffffffffffc8    	// #-56
    e4ac:	mov	x9, sp
    e4b0:	sub	x10, x29, #0x58
    e4b4:	movk	x8, #0xff80, lsl #32
    e4b8:	add	x11, x29, #0x10
    e4bc:	add	x9, x9, #0x80
    e4c0:	add	x10, x10, #0x38
    e4c4:	stp	x1, x2, [x29, #-88]
    e4c8:	stp	x3, x4, [x29, #-72]
    e4cc:	stp	x5, x6, [x29, #-56]
    e4d0:	stur	x7, [x29, #-40]
    e4d4:	stp	q0, q1, [sp]
    e4d8:	stp	q2, q3, [sp, #32]
    e4dc:	stp	q4, q5, [sp, #64]
    e4e0:	stp	q6, q7, [sp, #96]
    e4e4:	stp	x9, x8, [x29, #-16]
    e4e8:	stp	x11, x10, [x29, #-32]
    e4ec:	b	e504 <__gmpf_inits@@Base+0x68>
    e4f0:	ldur	x8, [x29, #-32]
    e4f4:	add	x9, x8, #0x8
    e4f8:	stur	x9, [x29, #-32]
    e4fc:	ldr	x0, [x8]
    e500:	cbz	x0, e530 <__gmpf_inits@@Base+0x94>
    e504:	bl	ce60 <__gmpf_init@plt>
    e508:	ldursw	x8, [x29, #-8]
    e50c:	tbz	w8, #31, e4f0 <__gmpf_inits@@Base+0x54>
    e510:	add	w9, w8, #0x8
    e514:	cmn	w8, #0x8
    e518:	stur	w9, [x29, #-8]
    e51c:	b.gt	e4f0 <__gmpf_inits@@Base+0x54>
    e520:	ldur	x9, [x29, #-24]
    e524:	add	x8, x9, x8
    e528:	ldr	x0, [x8]
    e52c:	cbnz	x0, e504 <__gmpf_inits@@Base+0x68>
    e530:	ldp	x29, x30, [sp, #224]
    e534:	add	sp, sp, #0xf0
    e538:	ret

000000000000e53c <__gmpf_set@@Base>:
    e53c:	ldrsw	x10, [x1, #4]
    e540:	ldrsw	x9, [x0]
    e544:	ldp	x12, x11, [x1, #8]
    e548:	ldr	x8, [x0, #16]
    e54c:	cmp	x10, #0x0
    e550:	add	x13, x9, #0x1
    e554:	str	x12, [x0, #8]
    e558:	cneg	x12, x10, mi  // mi = first
    e55c:	subs	x13, x12, x13
    e560:	add	x13, x11, x13, lsl #3
    e564:	csinc	x2, x12, x9, le
    e568:	csel	x1, x13, x11, gt
    e56c:	neg	w9, w2
    e570:	cmp	w10, #0x0
    e574:	csel	x9, x2, x9, ge  // ge = tcont
    e578:	str	w9, [x0, #4]
    e57c:	mov	x0, x8
    e580:	b	ca50 <__gmpn_copyi@plt>

000000000000e584 <__gmpf_set_ui@@Base>:
    e584:	ldr	x8, [x0, #16]
    e588:	cmp	x1, #0x0
    e58c:	cset	w9, ne  // ne = any
    e590:	str	x1, [x8]
    e594:	str	w9, [x0, #4]
    e598:	str	x9, [x0, #8]
    e59c:	ret

000000000000e5a0 <__gmpf_set_si@@Base>:
    e5a0:	ldr	x8, [x0, #16]
    e5a4:	cmp	x1, #0x0
    e5a8:	cset	w10, ne  // ne = any
    e5ac:	csetm	x11, ne  // ne = any
    e5b0:	cneg	x9, x1, mi  // mi = first
    e5b4:	csel	x11, x10, x11, ge  // ge = tcont
    e5b8:	str	x9, [x8]
    e5bc:	str	x10, [x0, #8]
    e5c0:	str	w11, [x0, #4]
    e5c4:	ret

000000000000e5c8 <__gmpf_set_str@@Base>:
    e5c8:	stp	x29, x30, [sp, #-96]!
    e5cc:	stp	x28, x27, [sp, #16]
    e5d0:	stp	x26, x25, [sp, #32]
    e5d4:	stp	x24, x23, [sp, #48]
    e5d8:	stp	x22, x21, [sp, #64]
    e5dc:	stp	x20, x19, [sp, #80]
    e5e0:	mov	x29, sp
    e5e4:	sub	sp, sp, #0x40
    e5e8:	mov	x19, x0
    e5ec:	mov	w0, #0x10000               	// #65536
    e5f0:	mov	w21, w2
    e5f4:	mov	x20, x1
    e5f8:	bl	c400 <nl_langinfo@plt>
    e5fc:	mov	x22, x0
    e600:	bl	bf60 <strlen@plt>
    e604:	mov	x25, x0
    e608:	bl	cae0 <__ctype_b_loc@plt>
    e60c:	ldr	x9, [x0]
    e610:	mov	x23, x0
    e614:	ldrb	w8, [x20], #1
    e618:	ldrh	w10, [x9, x8, lsl #1]
    e61c:	tbnz	w10, #13, e614 <__gmpf_set_str@@Base+0x4c>
    e620:	cmp	w8, #0x2d
    e624:	b.ne	e634 <__gmpf_set_str@@Base+0x6c>  // b.any
    e628:	ldrb	w8, [x20]
    e62c:	mov	w28, #0x1                   	// #1
    e630:	b	e63c <__gmpf_set_str@@Base+0x74>
    e634:	mov	w28, wzr
    e638:	sub	x20, x20, #0x1
    e63c:	cmp	w21, #0x0
    e640:	mov	w9, #0xa                   	// #10
    e644:	adrp	x26, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e648:	csel	w10, w9, w21, eq  // eq = none
    e64c:	ldr	x26, [x26, #3920]
    e650:	cmp	w10, #0x0
    e654:	cneg	w21, w10, mi  // mi = first
    e658:	csel	w27, w9, w10, lt  // lt = tstop
    e65c:	cmp	w21, #0x25
    e660:	b.lt	e670 <__gmpf_set_str@@Base+0xa8>  // b.tstop
    e664:	cmp	w21, #0x3e
    e668:	b.gt	e998 <__gmpf_set_str@@Base+0x3d0>
    e66c:	add	x26, x26, #0xd0
    e670:	ldrb	w9, [x26, w8, uxtw]
    e674:	cmp	w21, w9
    e678:	b.gt	e6c4 <__gmpf_set_str@@Base+0xfc>
    e67c:	cbz	x25, e6b4 <__gmpf_set_str@@Base+0xec>
    e680:	ldrb	w9, [x22]
    e684:	cmp	w8, w9
    e688:	b.ne	e998 <__gmpf_set_str@@Base+0x3d0>  // b.any
    e68c:	add	x8, x22, #0x1
    e690:	sub	x9, x25, #0x1
    e694:	add	x10, x20, #0x1
    e698:	cbz	x9, e6b4 <__gmpf_set_str@@Base+0xec>
    e69c:	ldrb	w11, [x10], #1
    e6a0:	ldrb	w12, [x8], #1
    e6a4:	sub	x9, x9, #0x1
    e6a8:	cmp	w11, w12
    e6ac:	b.eq	e698 <__gmpf_set_str@@Base+0xd0>  // b.none
    e6b0:	b	e998 <__gmpf_set_str@@Base+0x3d0>
    e6b4:	ldrb	w8, [x20, x25]
    e6b8:	ldrb	w8, [x26, x8]
    e6bc:	cmp	w21, w8
    e6c0:	b.le	e998 <__gmpf_set_str@@Base+0x3d0>
    e6c4:	mov	x0, x20
    e6c8:	bl	bf60 <strlen@plt>
    e6cc:	cmp	w21, #0xb
    e6d0:	mov	x24, x0
    e6d4:	sub	x9, x20, #0x1
    e6d8:	stur	x27, [x29, #-32]
    e6dc:	b.ge	e720 <__gmpf_set_str@@Base+0x158>  // b.tcont
    e6e0:	mov	x11, #0x21                  	// #33
    e6e4:	mov	w10, #0x1                   	// #1
    e6e8:	movk	x11, #0x20, lsl #32
    e6ec:	mov	x12, x24
    e6f0:	subs	x8, x12, #0x1
    e6f4:	b.eq	e74c <__gmpf_set_str@@Base+0x184>  // b.none
    e6f8:	ldrb	w12, [x9, x12]
    e6fc:	sub	w13, w12, #0x40
    e700:	cmp	w13, #0x25
    e704:	mov	x12, x8
    e708:	b.hi	e6f0 <__gmpf_set_str@@Base+0x128>  // b.pmore
    e70c:	lsl	x12, x10, x13
    e710:	tst	x12, x11
    e714:	mov	x12, x8
    e718:	b.eq	e6f0 <__gmpf_set_str@@Base+0x128>  // b.none
    e71c:	b	e73c <__gmpf_set_str@@Base+0x174>
    e720:	mov	x10, x24
    e724:	subs	x8, x10, #0x1
    e728:	b.eq	e74c <__gmpf_set_str@@Base+0x184>  // b.none
    e72c:	ldrb	w10, [x9, x10]
    e730:	cmp	w10, #0x40
    e734:	mov	x10, x8
    e738:	b.ne	e724 <__gmpf_set_str@@Base+0x15c>  // b.any
    e73c:	add	x9, x8, #0x1
    e740:	add	x27, x20, x9
    e744:	mov	x24, x8
    e748:	b	e750 <__gmpf_set_str@@Base+0x188>
    e74c:	mov	x27, xzr
    e750:	add	x1, x24, #0x1
    e754:	mov	w8, #0x7f01                	// #32513
    e758:	cmp	x1, x8
    e75c:	stp	x19, xzr, [x29, #-16]
    e760:	stur	w28, [x29, #-20]
    e764:	b.cs	e844 <__gmpf_set_str@@Base+0x27c>  // b.hs, b.nlast
    e768:	add	x9, x1, #0xf
    e76c:	mov	x8, sp
    e770:	and	x9, x9, #0xfffffffffffffff0
    e774:	sub	x1, x8, x9
    e778:	mov	sp, x1
    e77c:	cbz	x24, e838 <__gmpf_set_str@@Base+0x270>
    e780:	sub	x8, x25, #0x1
    e784:	mov	x9, xzr
    e788:	cbz	x25, e85c <__gmpf_set_str@@Base+0x294>
    e78c:	mov	x28, xzr
    e790:	mov	w10, wzr
    e794:	mov	x19, xzr
    e798:	add	x11, x22, #0x1
    e79c:	mov	w12, #0x1                   	// #1
    e7a0:	mov	x25, x1
    e7a4:	b	e7c8 <__gmpf_set_str@@Base+0x200>
    e7a8:	cbnz	x28, e990 <__gmpf_set_str@@Base+0x3c8>
    e7ac:	add	x20, x20, x8
    e7b0:	add	x9, x9, x8
    e7b4:	mov	x28, x25
    e7b8:	add	x9, x9, #0x1
    e7bc:	cmp	x9, x24
    e7c0:	add	x20, x20, #0x1
    e7c4:	b.cs	e8a0 <__gmpf_set_str@@Base+0x2d8>  // b.hs, b.nlast
    e7c8:	ldrb	w13, [x20]
    e7cc:	ldr	x14, [x23]
    e7d0:	ldrh	w14, [x14, x13, lsl #1]
    e7d4:	tbnz	w14, #13, e7b8 <__gmpf_set_str@@Base+0x1f0>
    e7d8:	ldrb	w14, [x22]
    e7dc:	cmp	w13, w14
    e7e0:	b.ne	e808 <__gmpf_set_str@@Base+0x240>  // b.any
    e7e4:	add	x14, x20, #0x1
    e7e8:	mov	x15, x8
    e7ec:	mov	x16, x11
    e7f0:	cbz	x15, e7a8 <__gmpf_set_str@@Base+0x1e0>
    e7f4:	ldrb	w17, [x14], #1
    e7f8:	ldrb	w18, [x16], #1
    e7fc:	sub	x15, x15, #0x1
    e800:	cmp	w17, w18
    e804:	b.eq	e7f0 <__gmpf_set_str@@Base+0x228>  // b.none
    e808:	ldrb	w13, [x26, x13]
    e80c:	cmp	w21, w13
    e810:	b.le	e990 <__gmpf_set_str@@Base+0x3c8>
    e814:	cmp	w13, #0x0
    e818:	strb	w13, [x25]
    e81c:	cset	w13, ne  // ne = any
    e820:	orr	w10, w10, w13
    e824:	add	x25, x25, w10, sxtw
    e828:	cbz	x28, e7b8 <__gmpf_set_str@@Base+0x1f0>
    e82c:	sub	w13, w12, w10
    e830:	add	x19, x19, w13, sxtw
    e834:	b	e7b8 <__gmpf_set_str@@Base+0x1f0>
    e838:	mov	x19, xzr
    e83c:	mov	x28, xzr
    e840:	b	e89c <__gmpf_set_str@@Base+0x2d4>
    e844:	sub	x0, x29, #0x8
    e848:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    e84c:	mov	x1, x0
    e850:	sub	x8, x25, #0x1
    e854:	mov	x9, xzr
    e858:	cbnz	x25, e78c <__gmpf_set_str@@Base+0x1c4>
    e85c:	ldr	x10, [x23]
    e860:	mov	x28, xzr
    e864:	b	e878 <__gmpf_set_str@@Base+0x2b0>
    e868:	add	x9, x9, #0x1
    e86c:	cmp	x9, x24
    e870:	add	x20, x20, #0x1
    e874:	b.cs	e898 <__gmpf_set_str@@Base+0x2d0>  // b.hs, b.nlast
    e878:	ldrb	w11, [x20]
    e87c:	ldrh	w11, [x10, x11, lsl #1]
    e880:	tbnz	w11, #13, e868 <__gmpf_set_str@@Base+0x2a0>
    e884:	cbnz	x28, e990 <__gmpf_set_str@@Base+0x3c8>
    e888:	add	x20, x20, x8
    e88c:	add	x9, x9, x8
    e890:	mov	x28, x1
    e894:	b	e868 <__gmpf_set_str@@Base+0x2a0>
    e898:	mov	x19, xzr
    e89c:	mov	x25, x1
    e8a0:	subs	x22, x25, x1
    e8a4:	b.eq	e9a0 <__gmpf_set_str@@Base+0x3d8>  // b.none
    e8a8:	ldur	x8, [x29, #-16]
    e8ac:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    e8b0:	mov	w10, #0x28                  	// #40
    e8b4:	ldrsw	x8, [x8]
    e8b8:	ldr	x9, [x9, #3936]
    e8bc:	add	x20, x8, #0x1
    e8c0:	umaddl	x9, w21, w10, x9
    e8c4:	ldr	x9, [x9, #16]
    e8c8:	umulh	x8, x9, x22
    e8cc:	and	x8, x8, #0x1ffffffffffffff8
    e8d0:	mov	w9, #0x7ef0                	// #32496
    e8d4:	cmp	x8, x9
    e8d8:	add	x8, x8, #0x10
    e8dc:	b.hi	ed6c <__gmpf_set_str@@Base+0x7a4>  // b.pmore
    e8e0:	add	x8, x8, #0xf
    e8e4:	mov	x9, sp
    e8e8:	and	x8, x8, #0x7ffffffffffffff0
    e8ec:	sub	x23, x9, x8
    e8f0:	mov	sp, x23
    e8f4:	mov	x0, x23
    e8f8:	mov	x2, x22
    e8fc:	mov	w3, w21
    e900:	bl	c090 <__gmpn_set_str@plt>
    e904:	subs	x8, x0, x20
    e908:	add	x9, x23, x8, lsl #3
    e90c:	mov	x24, x0
    e910:	csel	x2, x20, x0, gt
    e914:	csel	x1, x9, x23, gt
    e918:	csel	x14, x8, xzr, gt
    e91c:	cbz	x27, e9b0 <__gmpf_set_str@@Base+0x3e8>
    e920:	ldrb	w9, [x27]
    e924:	cmp	w9, #0x2d
    e928:	cset	w10, eq  // eq = none
    e92c:	csetm	x8, eq  // eq = none
    e930:	cmp	w9, #0x2b
    e934:	cset	w9, eq  // eq = none
    e938:	orr	w12, w10, w9
    e93c:	add	x11, x27, x12
    e940:	ldrb	w9, [x11]
    e944:	ldur	x10, [x29, #-32]
    e948:	ldrb	w9, [x26, x9]
    e94c:	sxtw	x10, w10
    e950:	cmp	x9, x10
    e954:	b.ge	e990 <__gmpf_set_str@@Base+0x3c8>  // b.tcont
    e958:	ldrb	w11, [x11, #1]
    e95c:	ldrb	w11, [x26, x11]
    e960:	cmp	x11, x10
    e964:	b.ge	e984 <__gmpf_set_str@@Base+0x3bc>  // b.tcont
    e968:	add	x12, x12, x27
    e96c:	add	x12, x12, #0x2
    e970:	ldrb	w13, [x12], #1
    e974:	madd	x9, x9, x10, x11
    e978:	ldrb	w11, [x26, x13]
    e97c:	cmp	x11, x10
    e980:	b.lt	e970 <__gmpf_set_str@@Base+0x3a8>  // b.tstop
    e984:	eor	x9, x9, x8
    e988:	sub	x8, x9, x8
    e98c:	b	e9b4 <__gmpf_set_str@@Base+0x3ec>
    e990:	ldur	x0, [x29, #-8]
    e994:	cbnz	x0, ed88 <__gmpf_set_str@@Base+0x7c0>
    e998:	mov	w0, #0xffffffff            	// #-1
    e99c:	b	ed3c <__gmpf_set_str@@Base+0x774>
    e9a0:	ldur	x8, [x29, #-16]
    e9a4:	str	wzr, [x8, #4]
    e9a8:	str	xzr, [x8, #8]
    e9ac:	b	ed30 <__gmpf_set_str@@Base+0x768>
    e9b0:	mov	x8, xzr
    e9b4:	sub	x9, x19, x28
    e9b8:	add	x9, x9, x25
    e9bc:	cmp	x28, #0x0
    e9c0:	csel	x9, xzr, x9, eq  // eq = none
    e9c4:	subs	x28, x8, x9
    e9c8:	cneg	x27, x28, mi  // mi = first
    e9cc:	cbz	x27, eaa4 <__gmpf_set_str@@Base+0x4dc>
    e9d0:	add	x22, x20, #0x1
    e9d4:	stp	x14, x1, [x29, #-40]
    e9d8:	lsl	x1, x22, #5
    e9dc:	mov	w8, #0x7f00                	// #32512
    e9e0:	cmp	x1, x8
    e9e4:	mov	w24, w21
    e9e8:	stur	x2, [x29, #-48]
    e9ec:	b.hi	ed90 <__gmpf_set_str@@Base+0x7c8>  // b.pmore
    e9f0:	add	x9, x1, #0xf
    e9f4:	mov	x8, sp
    e9f8:	and	x9, x9, #0xfffffffffffffff0
    e9fc:	sub	x26, x8, x9
    ea00:	mov	sp, x26
    ea04:	clz	x9, x27
    ea08:	cmp	x9, #0x3f
    ea0c:	str	x24, [x26]
    ea10:	stp	x22, x26, [x29, #-64]
    ea14:	b.ne	ead0 <__gmpf_set_str@@Base+0x508>  // b.any
    ea18:	mov	x8, xzr
    ea1c:	mov	x19, xzr
    ea20:	mov	w25, #0x1                   	// #1
    ea24:	subs	x9, x25, x20
    ea28:	add	x10, x26, x9, lsl #3
    ea2c:	csel	x9, x9, xzr, gt
    ea30:	add	x27, x9, x19
    ea34:	csel	x9, x10, x26, gt
    ea38:	ldur	x26, [x29, #-56]
    ea3c:	csel	x24, x20, x25, gt
    ea40:	add	x1, x9, x8, lsl #3
    ea44:	mov	x2, x24
    ea48:	mov	x0, x26
    ea4c:	bl	ca50 <__gmpn_copyi@plt>
    ea50:	tbnz	x28, #63, eb80 <__gmpf_set_str@@Base+0x5b8>
    ea54:	ldur	x4, [x29, #-48]
    ea58:	mov	w8, #0x7f00                	// #32512
    ea5c:	add	x19, x24, x4
    ea60:	lsl	x1, x19, #3
    ea64:	cmp	x1, x8
    ea68:	b.hi	eda0 <__gmpf_set_str@@Base+0x7d8>  // b.pmore
    ea6c:	add	x9, x1, #0xf
    ea70:	mov	x8, sp
    ea74:	and	x9, x9, #0xfffffffffffffff0
    ea78:	sub	x25, x8, x9
    ea7c:	mov	sp, x25
    ea80:	ldur	x22, [x29, #-16]
    ea84:	ldur	w23, [x29, #-20]
    ea88:	mov	x0, x25
    ea8c:	cmp	x24, x4
    ea90:	b.le	ec00 <__gmpf_set_str@@Base+0x638>
    ea94:	ldur	x3, [x29, #-32]
    ea98:	mov	x1, x26
    ea9c:	mov	x2, x24
    eaa0:	b	ec10 <__gmpf_set_str@@Base+0x648>
    eaa4:	ldur	x19, [x29, #-16]
    eaa8:	mov	x20, x2
    eaac:	ldr	x0, [x19, #16]
    eab0:	bl	ca50 <__gmpn_copyi@plt>
    eab4:	ldur	w9, [x29, #-20]
    eab8:	neg	w8, w20
    eabc:	str	x24, [x19, #8]
    eac0:	cmp	w9, #0x0
    eac4:	csel	x8, x20, x8, eq  // eq = none
    eac8:	str	w8, [x19, #4]
    eacc:	b	ed30 <__gmpf_set_str@@Base+0x768>
    ead0:	lsl	x10, x22, #1
    ead4:	mov	w11, #0x3e                  	// #62
    ead8:	mov	w12, #0x3f                  	// #63
    eadc:	mov	x19, xzr
    eae0:	mov	x8, xzr
    eae4:	add	x21, x26, x10, lsl #3
    eae8:	sub	w23, w11, w9
    eaec:	sub	w22, w12, w9
    eaf0:	mov	w25, #0x1                   	// #1
    eaf4:	mov	x9, x26
    eaf8:	b	eb10 <__gmpf_set_str@@Base+0x548>
    eafc:	sub	w22, w22, #0x1
    eb00:	cmp	w22, #0x0
    eb04:	sub	x23, x23, #0x1
    eb08:	mov	x9, x26
    eb0c:	b.le	ea24 <__gmpf_set_str@@Base+0x45c>
    eb10:	mov	x26, x21
    eb14:	add	x1, x9, x8, lsl #3
    eb18:	mov	x0, x26
    eb1c:	mov	x2, x25
    eb20:	mov	x21, x9
    eb24:	bl	c8e0 <__gmpn_sqr@plt>
    eb28:	add	x8, x26, x25, lsl #4
    eb2c:	ldur	x8, [x8, #-8]
    eb30:	lsl	x9, x25, #1
    eb34:	cmp	x8, #0x0
    eb38:	cset	w8, eq  // eq = none
    eb3c:	sub	x9, x9, x8
    eb40:	subs	x8, x9, x20
    eb44:	csel	x8, x8, xzr, gt
    eb48:	csel	x25, x20, x9, gt
    eb4c:	lsr	x9, x27, x23
    eb50:	add	x19, x8, x19, lsl #1
    eb54:	tbz	w9, #0, eafc <__gmpf_set_str@@Base+0x534>
    eb58:	add	x1, x26, x8, lsl #3
    eb5c:	mov	x0, x26
    eb60:	mov	x2, x25
    eb64:	mov	x3, x24
    eb68:	bl	d490 <__gmpn_mul_1@plt>
    eb6c:	cmp	x0, #0x0
    eb70:	mov	x8, xzr
    eb74:	str	x0, [x26, x25, lsl #3]
    eb78:	cinc	x25, x25, ne  // ne = any
    eb7c:	b	eafc <__gmpf_set_str@@Base+0x534>
    eb80:	ldur	x10, [x29, #-48]
    eb84:	cmp	x24, x10
    eb88:	b.le	ec4c <__gmpf_set_str@@Base+0x684>
    eb8c:	lsl	x19, x24, #3
    eb90:	add	x1, x19, #0x8
    eb94:	mov	w8, #0x7f00                	// #32512
    eb98:	cmp	x1, x8
    eb9c:	b.hi	edcc <__gmpf_set_str@@Base+0x804>  // b.pmore
    eba0:	add	x9, x1, #0xf
    eba4:	mov	x8, sp
    eba8:	and	x9, x9, #0xfffffffffffffff0
    ebac:	sub	x25, x8, x9
    ebb0:	mov	sp, x25
    ebb4:	ldur	x22, [x29, #-16]
    ebb8:	ldur	w23, [x29, #-20]
    ebbc:	subs	x21, x24, x10
    ebc0:	b.eq	ebd8 <__gmpf_set_str@@Base+0x610>  // b.none
    ebc4:	sub	x2, x19, x10, lsl #3
    ebc8:	mov	x0, x25
    ebcc:	mov	w1, wzr
    ebd0:	bl	c5f0 <memset@plt>
    ebd4:	ldur	x10, [x29, #-48]
    ebd8:	ldur	x1, [x29, #-32]
    ebdc:	add	x8, x25, x24, lsl #3
    ebe0:	sub	x0, x8, x10, lsl #3
    ebe4:	mov	x2, x10
    ebe8:	bl	ca50 <__gmpn_copyi@plt>
    ebec:	ldur	x8, [x29, #-40]
    ebf0:	mov	x10, x24
    ebf4:	sub	x8, x8, x21
    ebf8:	stp	x8, x25, [x29, #-40]
    ebfc:	b	ec54 <__gmpf_set_str@@Base+0x68c>
    ec00:	ldur	x1, [x29, #-32]
    ec04:	mov	x2, x4
    ec08:	mov	x3, x26
    ec0c:	mov	x4, x24
    ec10:	bl	ccd0 <__gmpn_mul@plt>
    ec14:	add	x8, x25, x19, lsl #3
    ec18:	ldur	x8, [x8, #-8]
    ec1c:	ldur	x9, [x29, #-40]
    ec20:	cmp	x8, #0x0
    ec24:	cset	w8, eq  // eq = none
    ec28:	sub	x8, x19, x8
    ec2c:	add	x10, x27, x9
    ec30:	subs	x9, x8, x20
    ec34:	add	x19, x10, x8
    ec38:	b.le	ec44 <__gmpf_set_str@@Base+0x67c>
    ec3c:	add	x25, x25, x9, lsl #3
    ec40:	b	ed0c <__gmpf_set_str@@Base+0x744>
    ec44:	mov	x20, x8
    ec48:	b	ed0c <__gmpf_set_str@@Base+0x744>
    ec4c:	ldur	x22, [x29, #-16]
    ec50:	ldur	w23, [x29, #-20]
    ec54:	add	x8, x26, x24, lsl #3
    ec58:	ldur	x8, [x8, #-8]
    ec5c:	tbnz	x8, #63, eca8 <__gmpf_set_str@@Base+0x6e0>
    ec60:	clz	x25, x8
    ec64:	mov	x0, x26
    ec68:	mov	x1, x26
    ec6c:	mov	x2, x24
    ec70:	mov	w3, w25
    ec74:	mov	x19, x10
    ec78:	bl	c180 <__gmpn_lshift@plt>
    ec7c:	ldur	x21, [x29, #-32]
    ec80:	mov	x2, x19
    ec84:	mov	w3, w25
    ec88:	mov	x0, x21
    ec8c:	mov	x1, x21
    ec90:	bl	c180 <__gmpn_lshift@plt>
    ec94:	cbz	x0, eca4 <__gmpf_set_str@@Base+0x6dc>
    ec98:	add	x10, x19, #0x1
    ec9c:	str	x0, [x21, x19, lsl #3]
    eca0:	b	eca8 <__gmpf_set_str@@Base+0x6e0>
    eca4:	mov	x10, x19
    eca8:	ldur	x8, [x29, #-64]
    ecac:	lsl	x1, x8, #3
    ecb0:	mov	w8, #0x7f00                	// #32512
    ecb4:	cmp	x1, x8
    ecb8:	b.hi	edb4 <__gmpf_set_str@@Base+0x7ec>  // b.pmore
    ecbc:	add	x9, x1, #0xf
    ecc0:	mov	x8, sp
    ecc4:	and	x9, x9, #0xfffffffffffffff0
    ecc8:	sub	x25, x8, x9
    eccc:	mov	sp, x25
    ecd0:	ldur	x2, [x29, #-32]
    ecd4:	sub	x19, x10, x24
    ecd8:	sub	x1, x20, x19
    ecdc:	mov	x0, x25
    ece0:	mov	x3, x10
    ece4:	mov	x4, x26
    ece8:	mov	x5, x24
    ecec:	bl	d3a0 <__gmpn_divrem@plt>
    ecf0:	ldur	x8, [x29, #-40]
    ecf4:	sub	x8, x8, x27
    ecf8:	add	x8, x19, x8
    ecfc:	add	x19, x8, x0
    ed00:	cbz	x0, ed0c <__gmpf_set_str@@Base+0x744>
    ed04:	str	x0, [x25, x20, lsl #3]
    ed08:	add	x25, x25, #0x8
    ed0c:	ldr	x0, [x22, #16]
    ed10:	mov	x1, x25
    ed14:	mov	x2, x20
    ed18:	bl	ca50 <__gmpn_copyi@plt>
    ed1c:	neg	w8, w20
    ed20:	cmp	w23, #0x0
    ed24:	csel	x8, x20, x8, eq  // eq = none
    ed28:	str	w8, [x22, #4]
    ed2c:	str	x19, [x22, #8]
    ed30:	ldur	x8, [x29, #-8]
    ed34:	mov	w0, wzr
    ed38:	cbnz	x8, ed5c <__gmpf_set_str@@Base+0x794>
    ed3c:	mov	sp, x29
    ed40:	ldp	x20, x19, [sp, #80]
    ed44:	ldp	x22, x21, [sp, #64]
    ed48:	ldp	x24, x23, [sp, #48]
    ed4c:	ldp	x26, x25, [sp, #32]
    ed50:	ldp	x28, x27, [sp, #16]
    ed54:	ldp	x29, x30, [sp], #96
    ed58:	ret
    ed5c:	mov	x0, x8
    ed60:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    ed64:	mov	w0, wzr
    ed68:	b	ed3c <__gmpf_set_str@@Base+0x774>
    ed6c:	sub	x0, x29, #0x8
    ed70:	mov	x23, x1
    ed74:	mov	x1, x8
    ed78:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed7c:	mov	x1, x23
    ed80:	mov	x23, x0
    ed84:	b	e8f4 <__gmpf_set_str@@Base+0x32c>
    ed88:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    ed8c:	b	e998 <__gmpf_set_str@@Base+0x3d0>
    ed90:	sub	x0, x29, #0x8
    ed94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed98:	mov	x26, x0
    ed9c:	b	ea04 <__gmpf_set_str@@Base+0x43c>
    eda0:	sub	x0, x29, #0x8
    eda4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    eda8:	ldur	x4, [x29, #-48]
    edac:	mov	x25, x0
    edb0:	b	ea80 <__gmpf_set_str@@Base+0x4b8>
    edb4:	sub	x0, x29, #0x8
    edb8:	mov	x19, x10
    edbc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    edc0:	mov	x10, x19
    edc4:	mov	x25, x0
    edc8:	b	ecd0 <__gmpf_set_str@@Base+0x708>
    edcc:	sub	x0, x29, #0x8
    edd0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    edd4:	ldur	x10, [x29, #-48]
    edd8:	mov	x25, x0
    eddc:	ldur	x22, [x29, #-16]
    ede0:	ldur	w23, [x29, #-20]
    ede4:	subs	x21, x24, x10
    ede8:	b.ne	ebc4 <__gmpf_set_str@@Base+0x5fc>  // b.any
    edec:	b	ebd8 <__gmpf_set_str@@Base+0x610>

000000000000edf0 <__gmpf_set_d@@Base>:
    edf0:	stp	x29, x30, [sp, #-32]!
    edf4:	fmov	x8, d0
    edf8:	mvn	x8, x8
    edfc:	tst	x8, #0x7ff0000000000000
    ee00:	str	x19, [sp, #16]
    ee04:	mov	x29, sp
    ee08:	b.eq	ee64 <__gmpf_set_d@@Base+0x74>  // b.none
    ee0c:	mov	x19, x0
    ee10:	fcmp	d0, #0.0
    ee14:	b.eq	ee4c <__gmpf_set_d@@Base+0x5c>  // b.none
    ee18:	ldr	x0, [x19, #16]
    ee1c:	fneg	d1, d0
    ee20:	mov	w8, #0x2                   	// #2
    ee24:	mov	w9, #0xfffffffe            	// #-2
    ee28:	fcsel	d0, d0, d1, ge  // ge = tcont
    ee2c:	csel	w8, w9, w8, mi  // mi = first
    ee30:	str	w8, [x19, #4]
    ee34:	bl	d280 <__gmp_extract_double@plt>
    ee38:	sxtw	x8, w0
    ee3c:	str	x8, [x19, #8]
    ee40:	ldr	x19, [sp, #16]
    ee44:	ldp	x29, x30, [sp], #32
    ee48:	ret
    ee4c:	mov	x8, xzr
    ee50:	str	wzr, [x19, #4]
    ee54:	str	x8, [x19, #8]
    ee58:	ldr	x19, [sp, #16]
    ee5c:	ldp	x29, x30, [sp], #32
    ee60:	ret
    ee64:	bl	c1b0 <__gmp_invalid_operation@plt>

000000000000ee68 <__gmpf_set_z@@Base>:
    ee68:	ldrsw	x10, [x1, #4]
    ee6c:	ldrsw	x9, [x0]
    ee70:	ldr	x11, [x1, #8]
    ee74:	ldr	x8, [x0, #16]
    ee78:	cmp	x10, #0x0
    ee7c:	add	x12, x9, #0x1
    ee80:	cneg	x13, x10, mi  // mi = first
    ee84:	subs	x12, x13, x12
    ee88:	add	x12, x11, x12, lsl #3
    ee8c:	csinc	x2, x13, x9, le
    ee90:	csel	x1, x12, x11, gt
    ee94:	neg	w9, w2
    ee98:	cmp	w10, #0x0
    ee9c:	csel	x9, x2, x9, ge  // ge = tcont
    eea0:	str	x13, [x0, #8]
    eea4:	str	w9, [x0, #4]
    eea8:	mov	x0, x8
    eeac:	b	ca50 <__gmpn_copyi@plt>

000000000000eeb0 <__gmpf_init_set@@Base>:
    eeb0:	stp	x29, x30, [sp, #-48]!
    eeb4:	stp	x22, x21, [sp, #16]
    eeb8:	stp	x20, x19, [sp, #32]
    eebc:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    eec0:	ldr	x8, [x8, #3960]
    eec4:	mov	x20, x0
    eec8:	mov	x29, sp
    eecc:	mov	x19, x1
    eed0:	ldr	x21, [x8]
    eed4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    eed8:	ldr	x8, [x8, #3840]
    eedc:	add	x22, x21, #0x1
    eee0:	lsl	x0, x22, #3
    eee4:	ldr	x8, [x8]
    eee8:	blr	x8
    eeec:	str	x0, [x20, #16]
    eef0:	str	w21, [x20]
    eef4:	ldrsw	x8, [x19, #4]
    eef8:	ldp	x9, x10, [x19, #8]
    eefc:	cmp	x8, #0x0
    ef00:	str	x9, [x20, #8]
    ef04:	cneg	x9, x8, mi  // mi = first
    ef08:	subs	x11, x9, x22
    ef0c:	add	x11, x10, x11, lsl #3
    ef10:	csinc	x2, x9, x21, le
    ef14:	csel	x1, x11, x10, gt
    ef18:	neg	w9, w2
    ef1c:	cmp	w8, #0x0
    ef20:	csel	x8, x2, x9, ge  // ge = tcont
    ef24:	str	w8, [x20, #4]
    ef28:	ldp	x20, x19, [sp, #32]
    ef2c:	ldp	x22, x21, [sp, #16]
    ef30:	ldp	x29, x30, [sp], #48
    ef34:	b	ca50 <__gmpn_copyi@plt>

000000000000ef38 <__gmpf_init_set_ui@@Base>:
    ef38:	stp	x29, x30, [sp, #-32]!
    ef3c:	stp	x20, x19, [sp, #16]
    ef40:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    ef44:	ldr	x8, [x8, #3960]
    ef48:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    ef4c:	mov	x20, x0
    ef50:	mov	x29, sp
    ef54:	ldr	x8, [x8]
    ef58:	mov	x19, x1
    ef5c:	str	w8, [x0]
    ef60:	ldr	x9, [x9, #3840]
    ef64:	lsl	x8, x8, #3
    ef68:	add	x0, x8, #0x8
    ef6c:	ldr	x9, [x9]
    ef70:	blr	x9
    ef74:	cmp	x19, #0x0
    ef78:	cset	w8, ne  // ne = any
    ef7c:	str	x0, [x20, #16]
    ef80:	str	x19, [x0]
    ef84:	str	w8, [x20, #4]
    ef88:	str	x8, [x20, #8]
    ef8c:	ldp	x20, x19, [sp, #16]
    ef90:	ldp	x29, x30, [sp], #32
    ef94:	ret

000000000000ef98 <__gmpf_init_set_si@@Base>:
    ef98:	stp	x29, x30, [sp, #-32]!
    ef9c:	stp	x20, x19, [sp, #16]
    efa0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    efa4:	ldr	x8, [x8, #3960]
    efa8:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    efac:	mov	x20, x0
    efb0:	mov	x29, sp
    efb4:	ldr	x8, [x8]
    efb8:	mov	x19, x1
    efbc:	str	w8, [x0]
    efc0:	ldr	x9, [x9, #3840]
    efc4:	lsl	x8, x8, #3
    efc8:	add	x0, x8, #0x8
    efcc:	ldr	x9, [x9]
    efd0:	blr	x9
    efd4:	cmp	x19, #0x0
    efd8:	cneg	x8, x19, mi  // mi = first
    efdc:	cset	w9, ne  // ne = any
    efe0:	csetm	x10, ne  // ne = any
    efe4:	str	x0, [x20, #16]
    efe8:	str	x8, [x0]
    efec:	csel	x8, x9, x10, ge  // ge = tcont
    eff0:	str	x9, [x20, #8]
    eff4:	str	w8, [x20, #4]
    eff8:	ldp	x20, x19, [sp, #16]
    effc:	ldp	x29, x30, [sp], #32
    f000:	ret

000000000000f004 <__gmpf_init_set_str@@Base>:
    f004:	stp	x29, x30, [sp, #-48]!
    f008:	str	x21, [sp, #16]
    f00c:	stp	x20, x19, [sp, #32]
    f010:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f014:	ldr	x8, [x8, #3960]
    f018:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f01c:	mov	x21, x0
    f020:	mov	x29, sp
    f024:	ldr	x8, [x8]
    f028:	str	xzr, [x0, #8]
    f02c:	mov	w19, w2
    f030:	mov	x20, x1
    f034:	stp	w8, wzr, [x0]
    f038:	ldr	x9, [x9, #3840]
    f03c:	lsl	x8, x8, #3
    f040:	add	x0, x8, #0x8
    f044:	ldr	x9, [x9]
    f048:	blr	x9
    f04c:	str	x0, [x21, #16]
    f050:	mov	x0, x21
    f054:	mov	x1, x20
    f058:	mov	w2, w19
    f05c:	ldp	x20, x19, [sp, #32]
    f060:	ldr	x21, [sp, #16]
    f064:	ldp	x29, x30, [sp], #48
    f068:	b	c1c0 <__gmpf_set_str@plt>

000000000000f06c <__gmpf_init_set_d@@Base>:
    f06c:	str	d8, [sp, #-32]!
    f070:	stp	x29, x30, [sp, #8]
    f074:	str	x19, [sp, #24]
    f078:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f07c:	ldr	x8, [x8, #3960]
    f080:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f084:	mov	x19, x0
    f088:	mov	x29, sp
    f08c:	ldr	x8, [x8]
    f090:	mov	v8.16b, v0.16b
    f094:	str	w8, [x0]
    f098:	ldr	x9, [x9, #3840]
    f09c:	lsl	x8, x8, #3
    f0a0:	add	x0, x8, #0x8
    f0a4:	ldr	x9, [x9]
    f0a8:	blr	x9
    f0ac:	str	x0, [x19, #16]
    f0b0:	mov	x0, x19
    f0b4:	ldr	x19, [sp, #24]
    f0b8:	ldp	x29, x30, [sp, #8]
    f0bc:	mov	v0.16b, v8.16b
    f0c0:	ldr	d8, [sp], #32
    f0c4:	b	c4a0 <__gmpf_set_d@plt>

000000000000f0c8 <__gmpf_clear@@Base>:
    f0c8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f0cc:	ldr	x8, [x8, #4016]
    f0d0:	ldrsw	x9, [x0]
    f0d4:	ldr	x0, [x0, #16]
    f0d8:	ldr	x2, [x8]
    f0dc:	lsl	x8, x9, #3
    f0e0:	add	x1, x8, #0x8
    f0e4:	br	x2

000000000000f0e8 <__gmpf_clears@@Base>:
    f0e8:	sub	sp, sp, #0x100
    f0ec:	stp	x29, x30, [sp, #224]
    f0f0:	add	x29, sp, #0xe0
    f0f4:	mov	x8, #0xffffffffffffffc8    	// #-56
    f0f8:	mov	x9, sp
    f0fc:	sub	x10, x29, #0x58
    f100:	movk	x8, #0xff80, lsl #32
    f104:	add	x11, x29, #0x20
    f108:	add	x9, x9, #0x80
    f10c:	add	x10, x10, #0x38
    f110:	str	x19, [sp, #240]
    f114:	stp	x1, x2, [x29, #-88]
    f118:	stp	x3, x4, [x29, #-72]
    f11c:	stp	x5, x6, [x29, #-56]
    f120:	stur	x7, [x29, #-40]
    f124:	stp	q0, q1, [sp]
    f128:	stp	q2, q3, [sp, #32]
    f12c:	stp	q4, q5, [sp, #64]
    f130:	stp	q6, q7, [sp, #96]
    f134:	stp	x9, x8, [x29, #-16]
    f138:	stp	x11, x10, [x29, #-32]
    f13c:	adrp	x19, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f140:	ldr	x19, [x19, #4016]
    f144:	b	f15c <__gmpf_clears@@Base+0x74>
    f148:	ldur	x8, [x29, #-32]
    f14c:	add	x9, x8, #0x8
    f150:	stur	x9, [x29, #-32]
    f154:	ldr	x0, [x8]
    f158:	cbz	x0, f19c <__gmpf_clears@@Base+0xb4>
    f15c:	ldrsw	x8, [x0]
    f160:	ldr	x9, [x19]
    f164:	ldr	x0, [x0, #16]
    f168:	lsl	x8, x8, #3
    f16c:	add	x1, x8, #0x8
    f170:	blr	x9
    f174:	ldursw	x8, [x29, #-8]
    f178:	tbz	w8, #31, f148 <__gmpf_clears@@Base+0x60>
    f17c:	add	w9, w8, #0x8
    f180:	cmn	w8, #0x8
    f184:	stur	w9, [x29, #-8]
    f188:	b.gt	f148 <__gmpf_clears@@Base+0x60>
    f18c:	ldur	x9, [x29, #-24]
    f190:	add	x8, x9, x8
    f194:	ldr	x0, [x8]
    f198:	cbnz	x0, f15c <__gmpf_clears@@Base+0x74>
    f19c:	ldr	x19, [sp, #240]
    f1a0:	ldp	x29, x30, [sp, #224]
    f1a4:	add	sp, sp, #0x100
    f1a8:	ret

000000000000f1ac <__gmpf_get_str@@Base>:
    f1ac:	stp	x29, x30, [sp, #-96]!
    f1b0:	stp	x28, x27, [sp, #16]
    f1b4:	stp	x26, x25, [sp, #32]
    f1b8:	stp	x24, x23, [sp, #48]
    f1bc:	stp	x22, x21, [sp, #64]
    f1c0:	stp	x20, x19, [sp, #80]
    f1c4:	mov	x29, sp
    f1c8:	sub	sp, sp, #0x60
    f1cc:	ldr	w8, [x4, #4]
    f1d0:	ldp	x11, x22, [x4, #8]
    f1d4:	mov	x20, x4
    f1d8:	mov	w21, w2
    f1dc:	cmp	w8, #0x0
    f1e0:	mov	x23, x1
    f1e4:	cneg	w25, w8, mi  // mi = first
    f1e8:	cmp	w2, #0x2
    f1ec:	mov	x24, x0
    f1f0:	b.lt	f208 <__gmpf_get_str@@Base+0x5c>  // b.tstop
    f1f4:	cmp	w21, #0x25
    f1f8:	b.ge	f218 <__gmpf_get_str@@Base+0x6c>  // b.tcont
    f1fc:	adrp	x19, 59000 <__gmp_randget_mt@@Base+0x44c>
    f200:	add	x19, x19, #0xc7d
    f204:	b	f238 <__gmpf_get_str@@Base+0x8c>
    f208:	cmn	w21, #0x2
    f20c:	b.le	f224 <__gmpf_get_str@@Base+0x78>
    f210:	mov	w21, #0xa                   	// #10
    f214:	b	f230 <__gmpf_get_str@@Base+0x84>
    f218:	cmp	w21, #0x3e
    f21c:	b.le	f230 <__gmpf_get_str@@Base+0x84>
    f220:	b	f6fc <__gmpf_get_str@@Base+0x550>
    f224:	cmn	w21, #0x24
    f228:	b.lt	f6fc <__gmpf_get_str@@Base+0x550>  // b.tstop
    f22c:	neg	w21, w21
    f230:	adrp	x19, 59000 <__gmp_randget_mt@@Base+0x44c>
    f234:	add	x19, x19, #0xc3e
    f238:	adrp	x26, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f23c:	ldr	x26, [x26, #3936]
    f240:	ldrsw	x9, [x20]
    f244:	mov	w8, #0x28                  	// #40
    f248:	umaddl	x8, w21, w8, x26
    f24c:	ldr	x28, [x8, #8]
    f250:	lsl	x8, x9, #6
    f254:	sub	x8, x8, #0x40
    f258:	umulh	x8, x28, x8
    f25c:	add	x8, x8, #0x2
    f260:	sub	x9, x3, #0x1
    f264:	cmp	x9, x8
    f268:	csel	x9, x3, x8, cc  // cc = lo, ul, last
    f26c:	stur	x9, [x29, #-24]
    f270:	cbz	x24, f42c <__gmpf_get_str@@Base+0x280>
    f274:	mov	x27, xzr
    f278:	cbz	w25, f454 <__gmpf_get_str@@Base+0x2a8>
    f27c:	ldur	x10, [x29, #-24]
    f280:	mov	w8, #0x7f00                	// #32512
    f284:	mov	w2, w21
    f288:	stur	xzr, [x29, #-8]
    f28c:	add	x1, x10, #0x83
    f290:	cmp	x1, x8
    f294:	stp	x23, x20, [x29, #-56]
    f298:	stur	x27, [x29, #-72]
    f29c:	b.hi	f468 <__gmpf_get_str@@Base+0x2bc>  // b.pmore
    f2a0:	add	x9, x1, #0xf
    f2a4:	mov	x8, sp
    f2a8:	and	x9, x9, #0xfffffffffffffff0
    f2ac:	sub	x23, x8, x9
    f2b0:	mov	sp, x23
    f2b4:	mov	w8, #0x28                  	// #40
    f2b8:	madd	x8, x2, x8, x26
    f2bc:	ldr	x8, [x8, #16]
    f2c0:	umulh	x8, x8, x10
    f2c4:	ubfx	x21, x8, #3, #58
    f2c8:	add	x20, x21, #0x2
    f2cc:	subs	x8, x25, x20
    f2d0:	lsl	x9, x20, #1
    f2d4:	add	x8, x22, x8, lsl #3
    f2d8:	csel	x27, x20, x25, hi  // hi = pmore
    f2dc:	add	x25, x9, #0x4
    f2e0:	csel	x8, x8, x22, hi  // hi = pmore
    f2e4:	cmp	x21, #0x3f4
    f2e8:	lsl	x1, x25, #4
    f2ec:	stur	x8, [x29, #-64]
    f2f0:	stur	x24, [x29, #-80]
    f2f4:	b.hi	f48c <__gmpf_get_str@@Base+0x2e0>  // b.pmore
    f2f8:	add	x9, x1, #0xf
    f2fc:	mov	x8, sp
    f300:	and	x9, x9, #0xfffffffffffffff0
    f304:	sub	x26, x8, x9
    f308:	mov	sp, x26
    f30c:	subs	x24, x11, x20
    f310:	add	x25, x26, x25, lsl #3
    f314:	stp	x11, x2, [x29, #-40]
    f318:	b.le	f4b8 <__gmpf_get_str@@Base+0x30c>
    f31c:	add	x4, x21, #0x3
    f320:	sub	x1, x29, #0x10
    f324:	mov	x0, x26
    f328:	mov	x5, x25
    f32c:	lsl	x8, x24, #6
    f330:	umulh	x3, x28, x8
    f334:	stur	x3, [x29, #-88]
    f338:	bl	f730 <__gmpf_get_str@@Base+0x584>
    f33c:	ldur	x22, [x29, #-16]
    f340:	sub	x8, x24, x22
    f344:	add	x21, x8, x20
    f348:	lsl	x1, x21, #3
    f34c:	mov	w8, #0x7f00                	// #32512
    f350:	cmp	x1, x8
    f354:	mov	x20, x0
    f358:	b.hi	f704 <__gmpf_get_str@@Base+0x558>  // b.pmore
    f35c:	add	x9, x1, #0xf
    f360:	mov	x8, sp
    f364:	and	x9, x9, #0xfffffffffffffff0
    f368:	sub	x28, x8, x9
    f36c:	mov	sp, x28
    f370:	ldur	x8, [x29, #-40]
    f374:	subs	x24, x21, x27
    f378:	b.eq	f394 <__gmpf_get_str@@Base+0x1e8>  // b.none
    f37c:	sub	x8, x8, x22
    f380:	sub	x8, x8, x27
    f384:	lsl	x2, x8, #3
    f388:	mov	x0, x28
    f38c:	mov	w1, wzr
    f390:	bl	c5f0 <memset@plt>
    f394:	ldur	x1, [x29, #-64]
    f398:	add	x0, x28, x24, lsl #3
    f39c:	mov	x2, x27
    f3a0:	bl	ca50 <__gmpn_copyi@plt>
    f3a4:	lsl	x1, x20, #3
    f3a8:	mov	w8, #0x7f00                	// #32512
    f3ac:	cmp	x1, x8
    f3b0:	b.hi	f720 <__gmpf_get_str@@Base+0x574>  // b.pmore
    f3b4:	add	x9, x1, #0xf
    f3b8:	mov	x8, sp
    f3bc:	and	x9, x9, #0xfffffffffffffff0
    f3c0:	sub	x1, x8, x9
    f3c4:	mov	sp, x1
    f3c8:	ldur	x24, [x29, #-80]
    f3cc:	ldp	x27, x22, [x29, #-56]
    f3d0:	mov	x0, x25
    f3d4:	mov	x2, xzr
    f3d8:	mov	x3, x28
    f3dc:	mov	x4, x21
    f3e0:	mov	x5, x26
    f3e4:	mov	x6, x20
    f3e8:	bl	bf00 <__gmpn_tdiv_qr@plt>
    f3ec:	sub	x8, x21, x20
    f3f0:	ldr	x9, [x25, x8, lsl #3]
    f3f4:	mov	x0, x23
    f3f8:	ldur	x1, [x29, #-32]
    f3fc:	mov	x2, x25
    f400:	cmp	x9, #0x0
    f404:	cset	w9, eq  // eq = none
    f408:	sub	x8, x8, x9
    f40c:	add	x3, x8, #0x1
    f410:	bl	ca90 <__gmpn_get_str@plt>
    f414:	ldur	x8, [x29, #-88]
    f418:	add	x8, x0, x8
    f41c:	ldur	x11, [x29, #-24]
    f420:	cmp	x0, x11
    f424:	b.hi	f5a4 <__gmpf_get_str@@Base+0x3f8>  // b.pmore
    f428:	b	f60c <__gmpf_get_str@@Base+0x460>
    f42c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f430:	ldr	x8, [x8, #3840]
    f434:	add	x27, x9, #0x2
    f438:	mov	x0, x27
    f43c:	mov	x24, x11
    f440:	ldr	x8, [x8]
    f444:	blr	x8
    f448:	mov	x11, x24
    f44c:	mov	x24, x0
    f450:	cbnz	w25, f27c <__gmpf_get_str@@Base+0xd0>
    f454:	mov	x21, xzr
    f458:	str	xzr, [x23]
    f45c:	strb	wzr, [x24]
    f460:	cbnz	x27, f68c <__gmpf_get_str@@Base+0x4e0>
    f464:	b	f6b4 <__gmpf_get_str@@Base+0x508>
    f468:	sub	x0, x29, #0x8
    f46c:	mov	x20, x2
    f470:	mov	x21, x11
    f474:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f478:	ldur	x10, [x29, #-24]
    f47c:	mov	x11, x21
    f480:	mov	x2, x20
    f484:	mov	x23, x0
    f488:	b	f2b4 <__gmpf_get_str@@Base+0x108>
    f48c:	sub	x0, x29, #0x8
    f490:	mov	x22, x2
    f494:	mov	x24, x11
    f498:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f49c:	mov	x11, x24
    f4a0:	mov	x2, x22
    f4a4:	mov	x26, x0
    f4a8:	subs	x24, x11, x20
    f4ac:	add	x25, x26, x25, lsl #3
    f4b0:	stp	x11, x2, [x29, #-40]
    f4b4:	b.gt	f31c <__gmpf_get_str@@Base+0x170>
    f4b8:	sub	x8, x20, x11
    f4bc:	lsl	x8, x8, #6
    f4c0:	umulh	x28, x28, x8
    f4c4:	add	x4, x21, #0x3
    f4c8:	sub	x1, x29, #0x10
    f4cc:	mov	x0, x26
    f4d0:	mov	x3, x28
    f4d4:	mov	x5, x25
    f4d8:	bl	f730 <__gmpf_get_str@@Base+0x584>
    f4dc:	mov	x22, x0
    f4e0:	cmp	x27, x0
    f4e4:	b.le	f500 <__gmpf_get_str@@Base+0x354>
    f4e8:	ldur	x1, [x29, #-64]
    f4ec:	mov	x0, x25
    f4f0:	mov	x2, x27
    f4f4:	mov	x3, x26
    f4f8:	mov	x4, x22
    f4fc:	b	f514 <__gmpf_get_str@@Base+0x368>
    f500:	ldur	x3, [x29, #-64]
    f504:	mov	x0, x25
    f508:	mov	x1, x26
    f50c:	mov	x2, x22
    f510:	mov	x4, x27
    f514:	bl	ccd0 <__gmpn_mul@plt>
    f518:	add	x8, x22, x27
    f51c:	add	x9, x25, x8, lsl #3
    f520:	ldur	x9, [x9, #-8]
    f524:	ldur	x11, [x29, #-40]
    f528:	ldur	x10, [x29, #-16]
    f52c:	ldur	x24, [x29, #-80]
    f530:	cmp	x9, #0x0
    f534:	sub	x11, x27, x11
    f538:	cset	w9, eq  // eq = none
    f53c:	subs	x20, x11, x10
    f540:	sub	x22, x8, x9
    f544:	b.pl	f57c <__gmpf_get_str@@Base+0x3d0>  // b.nfrst
    f548:	neg	x8, x20
    f54c:	sub	x0, x25, x20, lsl #3
    f550:	mov	x1, x25
    f554:	mov	x2, x22
    f558:	lsl	x27, x8, #3
    f55c:	bl	c000 <__gmpn_copyd@plt>
    f560:	add	x8, x26, x21, lsl #4
    f564:	add	x0, x8, #0x40
    f568:	mov	w1, wzr
    f56c:	mov	x2, x27
    f570:	bl	c5f0 <memset@plt>
    f574:	sub	x22, x22, x20
    f578:	mov	x20, xzr
    f57c:	add	x2, x25, x20, lsl #3
    f580:	sub	x3, x22, x20
    f584:	mov	x0, x23
    f588:	ldur	x1, [x29, #-32]
    f58c:	bl	ca90 <__gmpn_get_str@plt>
    f590:	ldp	x27, x22, [x29, #-56]
    f594:	sub	x8, x0, x28
    f598:	ldur	x11, [x29, #-24]
    f59c:	cmp	x0, x11
    f5a0:	b.ls	f60c <__gmpf_get_str@@Base+0x460>  // b.plast
    f5a4:	ldrb	w9, [x23, x11]
    f5a8:	ldur	x12, [x29, #-32]
    f5ac:	cmp	w12, w9, lsl #1
    f5b0:	b.gt	f60c <__gmpf_get_str@@Base+0x460>
    f5b4:	add	x9, x11, x23
    f5b8:	ldurb	w10, [x9, #-1]
    f5bc:	add	w10, w10, #0x1
    f5c0:	cmp	w12, w10, uxtb
    f5c4:	sturb	w10, [x9, #-1]
    f5c8:	b.ne	f5f8 <__gmpf_get_str@@Base+0x44c>  // b.any
    f5cc:	mov	x9, x11
    f5d0:	subs	x0, x9, #0x1
    f5d4:	b.eq	f600 <__gmpf_get_str@@Base+0x454>  // b.none
    f5d8:	add	x9, x23, x9
    f5dc:	ldurb	w10, [x9, #-2]
    f5e0:	add	w10, w10, #0x1
    f5e4:	cmp	w12, w10, uxtb
    f5e8:	sturb	w10, [x9, #-2]
    f5ec:	mov	x9, x0
    f5f0:	b.eq	f5d0 <__gmpf_get_str@@Base+0x424>  // b.none
    f5f4:	b	f60c <__gmpf_get_str@@Base+0x460>
    f5f8:	mov	x0, x11
    f5fc:	b	f60c <__gmpf_get_str@@Base+0x460>
    f600:	mov	w0, #0x1                   	// #1
    f604:	strb	w0, [x23]
    f608:	add	x8, x8, #0x1
    f60c:	cmp	x11, x0
    f610:	csel	x9, x0, x11, hi  // hi = pmore
    f614:	sub	x10, x23, #0x1
    f618:	cbz	x9, f65c <__gmpf_get_str@@Base+0x4b0>
    f61c:	ldrb	w11, [x10, x9]
    f620:	sub	x12, x9, #0x1
    f624:	mov	x9, x12
    f628:	cbz	w11, f618 <__gmpf_get_str@@Base+0x46c>
    f62c:	ldr	w9, [x22, #4]
    f630:	mov	x10, xzr
    f634:	add	x21, x12, #0x1
    f638:	lsr	x9, x9, #31
    f63c:	add	x11, x24, x9
    f640:	ldrb	w12, [x23, x10]
    f644:	ldrb	w12, [x19, x12]
    f648:	strb	w12, [x11, x10]
    f64c:	add	x10, x10, #0x1
    f650:	cmp	x21, x10
    f654:	b.ne	f640 <__gmpf_get_str@@Base+0x494>  // b.any
    f658:	b	f668 <__gmpf_get_str@@Base+0x4bc>
    f65c:	ldr	w9, [x22, #4]
    f660:	mov	x21, xzr
    f664:	lsr	x9, x9, #31
    f668:	add	x9, x24, x9
    f66c:	strb	wzr, [x9, x21]
    f670:	str	x8, [x27]
    f674:	ldr	w8, [x22, #4]
    f678:	tbnz	w8, #31, f6d8 <__gmpf_get_str@@Base+0x52c>
    f67c:	ldur	x27, [x29, #-72]
    f680:	ldur	x0, [x29, #-8]
    f684:	cbnz	x0, f6f0 <__gmpf_get_str@@Base+0x544>
    f688:	cbz	x27, f6b4 <__gmpf_get_str@@Base+0x508>
    f68c:	add	x2, x21, #0x1
    f690:	cmp	x27, x2
    f694:	b.eq	f6b4 <__gmpf_get_str@@Base+0x508>  // b.none
    f698:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f69c:	ldr	x8, [x8, #3792]
    f6a0:	mov	x0, x24
    f6a4:	mov	x1, x27
    f6a8:	ldr	x8, [x8]
    f6ac:	blr	x8
    f6b0:	mov	x24, x0
    f6b4:	mov	x0, x24
    f6b8:	mov	sp, x29
    f6bc:	ldp	x20, x19, [sp, #80]
    f6c0:	ldp	x22, x21, [sp, #64]
    f6c4:	ldp	x24, x23, [sp, #48]
    f6c8:	ldp	x26, x25, [sp, #32]
    f6cc:	ldp	x28, x27, [sp, #16]
    f6d0:	ldp	x29, x30, [sp], #96
    f6d4:	ret
    f6d8:	mov	w8, #0x2d                  	// #45
    f6dc:	add	x21, x21, #0x1
    f6e0:	strb	w8, [x24]
    f6e4:	ldur	x27, [x29, #-72]
    f6e8:	ldur	x0, [x29, #-8]
    f6ec:	cbz	x0, f688 <__gmpf_get_str@@Base+0x4dc>
    f6f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    f6f4:	cbnz	x27, f68c <__gmpf_get_str@@Base+0x4e0>
    f6f8:	b	f6b4 <__gmpf_get_str@@Base+0x508>
    f6fc:	mov	x24, xzr
    f700:	b	f6b4 <__gmpf_get_str@@Base+0x508>
    f704:	sub	x0, x29, #0x8
    f708:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f70c:	mov	x28, x0
    f710:	ldur	x8, [x29, #-40]
    f714:	subs	x24, x21, x27
    f718:	b.ne	f37c <__gmpf_get_str@@Base+0x1d0>  // b.any
    f71c:	b	f394 <__gmpf_get_str@@Base+0x1e8>
    f720:	sub	x0, x29, #0x8
    f724:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f728:	mov	x1, x0
    f72c:	b	f3c8 <__gmpf_get_str@@Base+0x21c>
    f730:	sub	sp, sp, #0x70
    f734:	stp	x20, x19, [sp, #96]
    f738:	mov	x20, x0
    f73c:	stp	x29, x30, [sp, #16]
    f740:	stp	x28, x27, [sp, #32]
    f744:	stp	x26, x25, [sp, #48]
    f748:	stp	x24, x23, [sp, #64]
    f74c:	stp	x22, x21, [sp, #80]
    f750:	add	x29, sp, #0x10
    f754:	cbz	x3, f7c0 <__gmpf_get_str@@Base+0x614>
    f758:	clz	x9, x3
    f75c:	mov	x21, x4
    f760:	mov	x23, x3
    f764:	mov	x24, x2
    f768:	cmp	x9, #0x3f
    f76c:	str	x2, [x20]
    f770:	str	x1, [sp, #8]
    f774:	b.ne	f7f4 <__gmpf_get_str@@Base+0x648>  // b.any
    f778:	mov	x8, xzr
    f77c:	mov	x27, xzr
    f780:	mov	w25, #0x1                   	// #1
    f784:	mov	x26, x20
    f788:	subs	x9, x25, x21
    f78c:	add	x10, x26, x9, lsl #3
    f790:	csel	x10, x10, x26, gt
    f794:	csel	x9, x9, xzr, gt
    f798:	add	x1, x10, x8, lsl #3
    f79c:	csel	x21, x21, x25, gt
    f7a0:	cmp	x1, x20
    f7a4:	add	x19, x9, x27
    f7a8:	b.eq	f7b8 <__gmpf_get_str@@Base+0x60c>  // b.none
    f7ac:	mov	x0, x20
    f7b0:	mov	x2, x21
    f7b4:	bl	ca50 <__gmpn_copyi@plt>
    f7b8:	ldr	x1, [sp, #8]
    f7bc:	b	f7cc <__gmpf_get_str@@Base+0x620>
    f7c0:	mov	w21, #0x1                   	// #1
    f7c4:	mov	x19, xzr
    f7c8:	str	x21, [x20]
    f7cc:	str	x19, [x1]
    f7d0:	mov	x0, x21
    f7d4:	ldp	x20, x19, [sp, #96]
    f7d8:	ldp	x22, x21, [sp, #80]
    f7dc:	ldp	x24, x23, [sp, #64]
    f7e0:	ldp	x26, x25, [sp, #48]
    f7e4:	ldp	x28, x27, [sp, #32]
    f7e8:	ldp	x29, x30, [sp, #16]
    f7ec:	add	sp, sp, #0x70
    f7f0:	ret
    f7f4:	mov	w10, #0x3e                  	// #62
    f7f8:	mov	w11, #0x3f                  	// #63
    f7fc:	mov	x22, x5
    f800:	mov	x27, xzr
    f804:	mov	x8, xzr
    f808:	sub	w28, w10, w9
    f80c:	sub	w19, w11, w9
    f810:	mov	w25, #0x1                   	// #1
    f814:	mov	x9, x20
    f818:	b	f830 <__gmpf_get_str@@Base+0x684>
    f81c:	sub	w19, w19, #0x1
    f820:	cmp	w19, #0x0
    f824:	sub	x28, x28, #0x1
    f828:	mov	x9, x26
    f82c:	b.le	f788 <__gmpf_get_str@@Base+0x5dc>
    f830:	mov	x26, x22
    f834:	add	x1, x9, x8, lsl #3
    f838:	mov	x0, x26
    f83c:	mov	x2, x25
    f840:	mov	x22, x9
    f844:	bl	c8e0 <__gmpn_sqr@plt>
    f848:	add	x8, x26, x25, lsl #4
    f84c:	ldur	x8, [x8, #-8]
    f850:	lsl	x9, x25, #1
    f854:	lsr	x10, x23, x28
    f858:	cmp	x8, #0x0
    f85c:	cset	w8, eq  // eq = none
    f860:	sub	x9, x9, x8
    f864:	subs	x8, x9, x21
    f868:	csel	x8, x8, xzr, gt
    f86c:	add	x27, x8, x27, lsl #1
    f870:	csel	x25, x21, x9, gt
    f874:	tbz	w10, #0, f81c <__gmpf_get_str@@Base+0x670>
    f878:	add	x1, x26, x8, lsl #3
    f87c:	mov	x0, x26
    f880:	mov	x2, x25
    f884:	mov	x3, x24
    f888:	bl	d490 <__gmpn_mul_1@plt>
    f88c:	cmp	x0, #0x0
    f890:	mov	x8, xzr
    f894:	str	x0, [x26, x25, lsl #3]
    f898:	cinc	x25, x25, ne  // ne = any
    f89c:	b	f81c <__gmpf_get_str@@Base+0x670>

000000000000f8a0 <__gmpf_dump@@Base>:
    f8a0:	sub	sp, sp, #0x30
    f8a4:	mov	x4, x0
    f8a8:	add	x1, sp, #0x8
    f8ac:	mov	w2, #0xa                   	// #10
    f8b0:	mov	x0, xzr
    f8b4:	mov	x3, xzr
    f8b8:	stp	x29, x30, [sp, #16]
    f8bc:	stp	x20, x19, [sp, #32]
    f8c0:	add	x29, sp, #0x10
    f8c4:	bl	ce20 <__gmpf_get_str@plt>
    f8c8:	ldrb	w8, [x0]
    f8cc:	mov	x19, x0
    f8d0:	cmp	w8, #0x2d
    f8d4:	b.ne	f8ec <__gmpf_dump@@Base+0x4c>  // b.any
    f8d8:	ldr	x2, [sp, #8]
    f8dc:	adrp	x0, 59000 <__gmp_randget_mt@@Base+0x44c>
    f8e0:	add	x1, x19, #0x1
    f8e4:	add	x0, x0, #0xca2
    f8e8:	b	f8fc <__gmpf_dump@@Base+0x5c>
    f8ec:	ldr	x2, [sp, #8]
    f8f0:	adrp	x0, 59000 <__gmp_randget_mt@@Base+0x44c>
    f8f4:	add	x0, x0, #0xca3
    f8f8:	mov	x1, x19
    f8fc:	bl	d2e0 <printf@plt>
    f900:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    f904:	ldr	x8, [x8, #4016]
    f908:	mov	x0, x19
    f90c:	ldr	x20, [x8]
    f910:	bl	bf60 <strlen@plt>
    f914:	add	x1, x0, #0x1
    f918:	mov	x0, x19
    f91c:	blr	x20
    f920:	ldp	x20, x19, [sp, #32]
    f924:	ldp	x29, x30, [sp, #16]
    f928:	add	sp, sp, #0x30
    f92c:	ret

000000000000f930 <__gmpf_size@@Base>:
    f930:	ldr	w8, [x0, #4]
    f934:	cmp	w8, #0x0
    f938:	cneg	w0, w8, mi  // mi = first
    f93c:	ret

000000000000f940 <__gmpf_eq@@Base>:
    f940:	ldrsw	x10, [x0, #4]
    f944:	ldrsw	x9, [x1, #4]
    f948:	eor	w11, w9, w10
    f94c:	tbnz	w11, #31, fa60 <__gmpf_eq@@Base+0x120>
    f950:	orr	w11, w9, w10
    f954:	cmp	w11, #0x0
    f958:	mov	x8, x0
    f95c:	cset	w0, eq  // eq = none
    f960:	cbz	w10, fa94 <__gmpf_eq@@Base+0x154>
    f964:	cbz	w9, fa94 <__gmpf_eq@@Base+0x154>
    f968:	ldr	x11, [x8, #8]
    f96c:	ldr	x12, [x1, #8]
    f970:	cmp	x11, x12
    f974:	b.ne	fa60 <__gmpf_eq@@Base+0x120>  // b.any
    f978:	ldr	x8, [x8, #16]
    f97c:	cmp	x10, #0x0
    f980:	ldr	x12, [x1, #16]
    f984:	cneg	x11, x10, mi  // mi = first
    f988:	add	x13, x8, x11, lsl #3
    f98c:	cmp	x9, #0x0
    f990:	mov	x10, x13
    f994:	ldr	x8, [x10, #-8]!
    f998:	cneg	x9, x9, mi  // mi = first
    f99c:	add	x16, x12, x9, lsl #3
    f9a0:	ldur	x12, [x16, #-8]
    f9a4:	clz	x8, x8
    f9a8:	mov	w14, #0x3f                  	// #63
    f9ac:	sub	w14, w14, w8
    f9b0:	lsr	x12, x12, x14
    f9b4:	cmp	x12, #0x1
    f9b8:	b.ne	fa60 <__gmpf_eq@@Base+0x120>  // b.any
    f9bc:	add	x8, x8, x2
    f9c0:	add	x12, x8, #0x3f
    f9c4:	lsr	x12, x12, #6
    f9c8:	cmp	x11, x12
    f9cc:	csel	x11, x11, x12, lt  // lt = tstop
    f9d0:	cmp	x9, x12
    f9d4:	csel	x12, x9, x12, lt  // lt = tstop
    f9d8:	cmp	x11, x12
    f9dc:	csel	x15, x11, x12, lt  // lt = tstop
    f9e0:	add	x9, x11, x12
    f9e4:	lsl	x14, x15, #3
    f9e8:	sub	x9, x9, x15
    f9ec:	sub	x13, x13, x14
    f9f0:	sub	x14, x16, x14
    f9f4:	sub	x16, x16, #0x8
    f9f8:	mov	x17, x15
    f9fc:	cmp	x17, #0x2
    fa00:	b.lt	fa1c <__gmpf_eq@@Base+0xdc>  // b.tstop
    fa04:	ldr	x18, [x10], #-8
    fa08:	ldr	x0, [x16], #-8
    fa0c:	sub	x17, x17, #0x1
    fa10:	cmp	x18, x0
    fa14:	b.eq	f9fc <__gmpf_eq@@Base+0xbc>  // b.none
    fa18:	b	fa60 <__gmpf_eq@@Base+0x120>
    fa1c:	ldr	x16, [x13]
    fa20:	ldr	x17, [x14]
    fa24:	subs	x10, x9, x15
    fa28:	b.eq	fa68 <__gmpf_eq@@Base+0x128>  // b.none
    fa2c:	cmp	x16, x17
    fa30:	b.ne	fa60 <__gmpf_eq@@Base+0x120>  // b.any
    fa34:	cmp	x11, x12
    fa38:	csel	x11, x13, x14, gt
    fa3c:	neg	x15, x10
    fa40:	sub	x12, x11, #0x8
    fa44:	cmp	x10, #0x2
    fa48:	b.lt	fa70 <__gmpf_eq@@Base+0x130>  // b.tstop
    fa4c:	ldr	x13, [x12], #-8
    fa50:	mov	w0, wzr
    fa54:	sub	x10, x10, #0x1
    fa58:	cbz	x13, fa44 <__gmpf_eq@@Base+0x104>
    fa5c:	b	fa94 <__gmpf_eq@@Base+0x154>
    fa60:	mov	w0, wzr
    fa64:	ret
    fa68:	eor	x10, x17, x16
    fa6c:	b	fa74 <__gmpf_eq@@Base+0x134>
    fa70:	ldr	x10, [x11, x15, lsl #3]
    fa74:	sub	x8, x8, x9, lsl #6
    fa78:	add	x8, x8, #0x40
    fa7c:	mov	w9, #0x40                  	// #64
    fa80:	subs	x8, x9, x8
    fa84:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
    fa88:	lsr	x8, x10, x8
    fa8c:	cmp	x8, #0x0
    fa90:	cset	w0, eq  // eq = none
    fa94:	ret

000000000000fa98 <__gmpf_reldiff@@Base>:
    fa98:	stp	x29, x30, [sp, #-48]!
    fa9c:	str	x21, [sp, #16]
    faa0:	stp	x20, x19, [sp, #32]
    faa4:	mov	x29, sp
    faa8:	sub	sp, sp, #0x20
    faac:	ldr	w8, [x1, #4]
    fab0:	mov	x21, x2
    fab4:	mov	x19, x0
    fab8:	cbz	w8, fb50 <__gmpf_reldiff@@Base+0xb8>
    fabc:	str	xzr, [x29, #24]
    fac0:	ldr	w9, [x19]
    fac4:	cmp	w8, #0x0
    fac8:	cneg	w8, w8, mi  // mi = first
    facc:	mov	x20, x1
    fad0:	add	w8, w9, w8
    fad4:	sbfiz	x9, x8, #3, #32
    fad8:	add	x1, x9, #0x8
    fadc:	mov	w9, #0x7f00                	// #32512
    fae0:	cmp	x1, x9
    fae4:	stur	w8, [x29, #-24]
    fae8:	b.hi	fb74 <__gmpf_reldiff@@Base+0xdc>  // b.pmore
    faec:	add	x9, x1, #0xf
    faf0:	mov	x8, sp
    faf4:	and	x9, x9, #0xfffffffffffffff0
    faf8:	sub	x0, x8, x9
    fafc:	mov	sp, x0
    fb00:	stur	x0, [x29, #-8]
    fb04:	sub	x0, x29, #0x18
    fb08:	mov	x1, x20
    fb0c:	mov	x2, x21
    fb10:	bl	ce00 <__gmpf_sub@plt>
    fb14:	ldur	w8, [x29, #-20]
    fb18:	sub	x1, x29, #0x18
    fb1c:	mov	x0, x19
    fb20:	mov	x2, x20
    fb24:	cmp	w8, #0x0
    fb28:	cneg	w8, w8, mi  // mi = first
    fb2c:	stur	w8, [x29, #-20]
    fb30:	bl	ced0 <__gmpf_div@plt>
    fb34:	ldr	x0, [x29, #24]
    fb38:	cbnz	x0, fb80 <__gmpf_reldiff@@Base+0xe8>
    fb3c:	mov	sp, x29
    fb40:	ldp	x20, x19, [sp, #32]
    fb44:	ldr	x21, [sp, #16]
    fb48:	ldp	x29, x30, [sp], #48
    fb4c:	ret
    fb50:	ldr	w8, [x21, #4]
    fb54:	mov	x0, x19
    fb58:	cmp	w8, #0x0
    fb5c:	cset	w1, ne  // ne = any
    fb60:	mov	sp, x29
    fb64:	ldp	x20, x19, [sp, #32]
    fb68:	ldr	x21, [sp, #16]
    fb6c:	ldp	x29, x30, [sp], #48
    fb70:	b	c6a0 <__gmpf_set_ui@plt>
    fb74:	add	x0, x29, #0x18
    fb78:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    fb7c:	b	fb00 <__gmpf_reldiff@@Base+0x68>
    fb80:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    fb84:	b	fb3c <__gmpf_reldiff@@Base+0xa4>

000000000000fb88 <__gmpf_sqrt@@Base>:
    fb88:	stp	x29, x30, [sp, #-80]!
    fb8c:	stp	x26, x25, [sp, #16]
    fb90:	stp	x24, x23, [sp, #32]
    fb94:	stp	x22, x21, [sp, #48]
    fb98:	stp	x20, x19, [sp, #64]
    fb9c:	mov	x29, sp
    fba0:	sub	sp, sp, #0x10
    fba4:	ldrsw	x20, [x1, #4]
    fba8:	mov	x19, x0
    fbac:	cmp	w20, #0x0
    fbb0:	b.le	fc24 <__gmpf_sqrt@@Base+0x9c>
    fbb4:	stur	xzr, [x29, #-8]
    fbb8:	ldp	x8, x22, [x1, #8]
    fbbc:	ldrsw	x9, [x19]
    fbc0:	mov	w10, #0x7f00                	// #32512
    fbc4:	and	x24, x8, #0x1
    fbc8:	lsl	x25, x9, #1
    fbcc:	add	x8, x24, x8
    fbd0:	sub	x21, x25, x24
    fbd4:	cmp	x8, #0x0
    fbd8:	lsl	x1, x21, #3
    fbdc:	cinc	x8, x8, lt  // lt = tstop
    fbe0:	cmp	x1, x10
    fbe4:	asr	x8, x8, #1
    fbe8:	str	w9, [x19, #4]
    fbec:	str	x8, [x19, #8]
    fbf0:	b.hi	fc34 <__gmpf_sqrt@@Base+0xac>  // b.pmore
    fbf4:	add	x9, x1, #0xf
    fbf8:	mov	x8, sp
    fbfc:	and	x9, x9, #0xfffffffffffffff0
    fc00:	sub	x23, x8, x9
    fc04:	mov	sp, x23
    fc08:	subs	x26, x21, x20
    fc0c:	b.ge	fc48 <__gmpf_sqrt@@Base+0xc0>  // b.tcont
    fc10:	sub	x8, x20, x21
    fc14:	add	x1, x22, x8, lsl #3
    fc18:	mov	x0, x23
    fc1c:	mov	x2, x21
    fc20:	b	fc70 <__gmpf_sqrt@@Base+0xe8>
    fc24:	tbnz	w20, #31, fcb4 <__gmpf_sqrt@@Base+0x12c>
    fc28:	str	wzr, [x19, #4]
    fc2c:	str	xzr, [x19, #8]
    fc30:	b	fc90 <__gmpf_sqrt@@Base+0x108>
    fc34:	sub	x0, x29, #0x8
    fc38:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    fc3c:	mov	x23, x0
    fc40:	subs	x26, x21, x20
    fc44:	b.lt	fc10 <__gmpf_sqrt@@Base+0x88>  // b.tstop
    fc48:	b.eq	fc64 <__gmpf_sqrt@@Base+0xdc>  // b.none
    fc4c:	sub	x8, x25, x20
    fc50:	sub	x8, x8, x24
    fc54:	lsl	x2, x8, #3
    fc58:	mov	x0, x23
    fc5c:	mov	w1, wzr
    fc60:	bl	c5f0 <memset@plt>
    fc64:	add	x0, x23, x26, lsl #3
    fc68:	mov	x1, x22
    fc6c:	mov	x2, x20
    fc70:	bl	ca50 <__gmpn_copyi@plt>
    fc74:	ldr	x0, [x19, #16]
    fc78:	mov	x1, xzr
    fc7c:	mov	x2, x23
    fc80:	mov	x3, x21
    fc84:	bl	d3b0 <__gmpn_sqrtrem@plt>
    fc88:	ldur	x0, [x29, #-8]
    fc8c:	cbnz	x0, fcac <__gmpf_sqrt@@Base+0x124>
    fc90:	mov	sp, x29
    fc94:	ldp	x20, x19, [sp, #64]
    fc98:	ldp	x22, x21, [sp, #48]
    fc9c:	ldp	x24, x23, [sp, #32]
    fca0:	ldp	x26, x25, [sp, #16]
    fca4:	ldp	x29, x30, [sp], #80
    fca8:	ret
    fcac:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    fcb0:	b	fc90 <__gmpf_sqrt@@Base+0x108>
    fcb4:	bl	cff0 <__gmp_sqrt_of_negative@plt>

000000000000fcb8 <__gmpf_random2@@Base>:
    fcb8:	sub	sp, sp, #0x40
    fcbc:	cmp	x1, #0x0
    fcc0:	stp	x20, x19, [sp, #48]
    fcc4:	cneg	x8, x1, mi  // mi = first
    fcc8:	mov	x19, x0
    fccc:	stp	x29, x30, [sp, #16]
    fcd0:	stp	x22, x21, [sp, #32]
    fcd4:	add	x29, sp, #0x10
    fcd8:	cbz	x8, fd70 <__gmpf_random2@@Base+0xb8>
    fcdc:	ldrsw	x9, [x19]
    fce0:	ldr	x0, [x19, #16]
    fce4:	mov	x20, x1
    fce8:	mov	x21, x2
    fcec:	add	x10, x9, #0x1
    fcf0:	cmp	x8, x10
    fcf4:	csinc	x22, x8, x9, le
    fcf8:	mov	x1, x22
    fcfc:	bl	d1c0 <__gmpn_random2@plt>
    fd00:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fd04:	ldr	x8, [x8, #4040]
    fd08:	ldrb	w9, [x8]
    fd0c:	cbnz	w9, fd24 <__gmpf_random2@@Base+0x6c>
    fd10:	mov	w9, #0x1                   	// #1
    fd14:	strb	w9, [x8]
    fd18:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fd1c:	ldr	x0, [x0, #3976]
    fd20:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
    fd24:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fd28:	ldr	x0, [x0, #3976]
    fd2c:	add	x1, sp, #0x8
    fd30:	mov	w2, #0x40                  	// #64
    fd34:	ldr	x8, [x0, #24]
    fd38:	ldr	x8, [x8, #8]
    fd3c:	blr	x8
    fd40:	ldr	x8, [sp, #8]
    fd44:	cmp	x21, #0x0
    fd48:	mov	w9, #0x1                   	// #1
    fd4c:	cneg	x10, x21, mi  // mi = first
    fd50:	bfi	x9, x10, #1, #63
    fd54:	udiv	x11, x8, x9
    fd58:	msub	x8, x11, x9, x8
    fd5c:	cmp	x20, #0x0
    fd60:	sub	x8, x8, x10
    fd64:	str	x8, [x19, #8]
    fd68:	cneg	w8, w22, lt  // lt = tstop
    fd6c:	b	fd74 <__gmpf_random2@@Base+0xbc>
    fd70:	str	xzr, [x19, #8]
    fd74:	str	w8, [x19, #4]
    fd78:	ldp	x20, x19, [sp, #48]
    fd7c:	ldp	x22, x21, [sp, #32]
    fd80:	ldp	x29, x30, [sp, #16]
    fd84:	add	sp, sp, #0x40
    fd88:	ret

000000000000fd8c <__gmpf_inp_str@@Base>:
    fd8c:	sub	sp, sp, #0x70
    fd90:	stp	x29, x30, [sp, #16]
    fd94:	add	x29, sp, #0x10
    fd98:	stp	x28, x27, [sp, #32]
    fd9c:	stp	x26, x25, [sp, #48]
    fda0:	stp	x24, x23, [sp, #64]
    fda4:	stp	x22, x21, [sp, #80]
    fda8:	stp	x20, x19, [sp, #96]
    fdac:	stur	w2, [x29, #-4]
    fdb0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fdb4:	ldr	x8, [x8, #3888]
    fdb8:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fdbc:	mov	x20, x0
    fdc0:	cmp	x1, #0x0
    fdc4:	ldr	x8, [x8]
    fdc8:	ldr	x9, [x9, #3840]
    fdcc:	mov	w0, #0x64                  	// #100
    fdd0:	csel	x22, x8, x1, eq  // eq = none
    fdd4:	ldr	x9, [x9]
    fdd8:	blr	x9
    fddc:	mov	x21, x0
    fde0:	mov	x27, #0xffffffffffffffff    	// #-1
    fde4:	mov	x0, x22
    fde8:	bl	c7f0 <getc@plt>
    fdec:	mov	w24, w0
    fdf0:	bl	cae0 <__ctype_b_loc@plt>
    fdf4:	ldr	x8, [x0]
    fdf8:	add	x27, x27, #0x1
    fdfc:	ldrh	w8, [x8, w24, sxtw #1]
    fe00:	tbnz	w8, #13, fde4 <__gmpf_inp_str@@Base+0x58>
    fe04:	adrp	x19, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fe08:	ldr	x19, [x19, #3792]
    fe0c:	mov	x25, x0
    fe10:	mov	x28, xzr
    fe14:	mov	w23, #0x64                  	// #100
    fe18:	cmp	x28, x23
    fe1c:	b.cc	fe44 <__gmpf_inp_str@@Base+0xb8>  // b.lo, b.ul, b.last
    fe20:	ldr	x8, [x19]
    fe24:	add	x9, x23, x23, lsl #1
    fe28:	lsr	x26, x9, #1
    fe2c:	mov	x0, x21
    fe30:	mov	x1, x23
    fe34:	mov	x2, x26
    fe38:	blr	x8
    fe3c:	mov	x21, x0
    fe40:	mov	x23, x26
    fe44:	cmn	w24, #0x1
    fe48:	b.eq	fe7c <__gmpf_inp_str@@Base+0xf0>  // b.none
    fe4c:	ldr	x8, [x25]
    fe50:	ldrh	w8, [x8, w24, sxtw #1]
    fe54:	tbnz	w8, #13, fe7c <__gmpf_inp_str@@Base+0xf0>
    fe58:	mov	x0, x22
    fe5c:	add	x26, x28, #0x1
    fe60:	strb	w24, [x21, x28]
    fe64:	bl	c7f0 <getc@plt>
    fe68:	mov	w24, w0
    fe6c:	mov	x28, x26
    fe70:	cmp	x28, x23
    fe74:	b.cs	fe20 <__gmpf_inp_str@@Base+0x94>  // b.hs, b.nlast
    fe78:	b	fe44 <__gmpf_inp_str@@Base+0xb8>
    fe7c:	mov	w0, w24
    fe80:	mov	x1, x22
    fe84:	bl	cc50 <ungetc@plt>
    fe88:	cmp	x28, x23
    fe8c:	b.cc	feb4 <__gmpf_inp_str@@Base+0x128>  // b.lo, b.ul, b.last
    fe90:	ldr	x8, [x19]
    fe94:	add	x9, x23, x23, lsl #1
    fe98:	lsr	x22, x9, #1
    fe9c:	mov	x0, x21
    fea0:	mov	x1, x23
    fea4:	mov	x2, x22
    fea8:	blr	x8
    feac:	mov	x21, x0
    feb0:	mov	x23, x22
    feb4:	ldur	w2, [x29, #-4]
    feb8:	mov	x0, x20
    febc:	mov	x1, x21
    fec0:	strb	wzr, [x21, x28]
    fec4:	bl	c1c0 <__gmpf_set_str@plt>
    fec8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    fecc:	ldr	x8, [x8, #4016]
    fed0:	mov	w19, w0
    fed4:	mov	x0, x21
    fed8:	mov	x1, x23
    fedc:	ldr	x8, [x8]
    fee0:	blr	x8
    fee4:	add	x8, x27, x28
    fee8:	cmn	w19, #0x1
    feec:	ldp	x20, x19, [sp, #96]
    fef0:	ldp	x22, x21, [sp, #80]
    fef4:	ldp	x24, x23, [sp, #64]
    fef8:	ldp	x26, x25, [sp, #48]
    fefc:	ldp	x28, x27, [sp, #32]
    ff00:	ldp	x29, x30, [sp, #16]
    ff04:	csel	x0, xzr, x8, eq  // eq = none
    ff08:	add	sp, sp, #0x70
    ff0c:	ret

000000000000ff10 <__gmpf_out_str@@Base>:
    ff10:	stp	x29, x30, [sp, #-80]!
    ff14:	str	x25, [sp, #16]
    ff18:	stp	x24, x23, [sp, #32]
    ff1c:	stp	x22, x21, [sp, #48]
    ff20:	stp	x20, x19, [sp, #64]
    ff24:	mov	x29, sp
    ff28:	sub	sp, sp, #0x10
    ff2c:	cmp	w1, #0x0
    ff30:	mov	w8, #0xa                   	// #10
    ff34:	mov	x22, x3
    ff38:	mov	x23, x2
    ff3c:	csel	w19, w8, w1, eq  // eq = none
    ff40:	stur	xzr, [x29, #-8]
    ff44:	cbnz	x2, ff70 <__gmpf_out_str@@Base+0x60>
    ff48:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    ff4c:	ldr	x8, [x8, #3936]
    ff50:	ldrsw	x10, [x22]
    ff54:	mov	w9, #0x28                  	// #40
    ff58:	smaddl	x8, w19, w9, x8
    ff5c:	ldr	x8, [x8, #8]
    ff60:	lsl	x9, x10, #6
    ff64:	sub	x9, x9, #0x40
    ff68:	umulh	x8, x8, x9
    ff6c:	add	x23, x8, #0x2
    ff70:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
    ff74:	ldr	x8, [x8, #3856]
    ff78:	cmp	x0, #0x0
    ff7c:	add	x1, x23, #0x2
    ff80:	mov	w9, #0x7f00                	// #32512
    ff84:	ldr	x8, [x8]
    ff88:	csel	x20, x8, x0, eq  // eq = none
    ff8c:	cmp	x1, x9
    ff90:	b.hi	100ac <__gmpf_out_str@@Base+0x19c>  // b.pmore
    ff94:	add	x9, x1, #0xf
    ff98:	mov	x8, sp
    ff9c:	and	x9, x9, #0xfffffffffffffff0
    ffa0:	sub	x21, x8, x9
    ffa4:	mov	sp, x21
    ffa8:	add	x1, x29, #0x18
    ffac:	mov	x0, x21
    ffb0:	mov	w2, w19
    ffb4:	mov	x3, x23
    ffb8:	mov	x4, x22
    ffbc:	bl	ce20 <__gmpf_get_str@plt>
    ffc0:	mov	x0, x21
    ffc4:	bl	bf60 <strlen@plt>
    ffc8:	ldrb	w8, [x21]
    ffcc:	mov	x22, x0
    ffd0:	cmp	w8, #0x2d
    ffd4:	b.ne	fff4 <__gmpf_out_str@@Base+0xe4>  // b.any
    ffd8:	mov	w0, #0x2d                  	// #45
    ffdc:	mov	x1, x20
    ffe0:	add	x21, x21, #0x1
    ffe4:	bl	c210 <fputc@plt>
    ffe8:	sub	x22, x22, #0x1
    ffec:	mov	w25, #0x2                   	// #2
    fff0:	b	fff8 <__gmpf_out_str@@Base+0xe8>
    fff4:	mov	w25, #0x1                   	// #1
    fff8:	mov	w0, #0x10000               	// #65536
    fffc:	bl	c400 <nl_langinfo@plt>
   10000:	mov	x23, x0
   10004:	bl	bf60 <strlen@plt>
   10008:	mov	x24, x0
   1000c:	mov	w0, #0x30                  	// #48
   10010:	mov	x1, x20
   10014:	bl	c1e0 <putc@plt>
   10018:	mov	w1, #0x1                   	// #1
   1001c:	mov	x0, x23
   10020:	mov	x2, x24
   10024:	mov	x3, x20
   10028:	bl	ce30 <fwrite@plt>
   1002c:	mov	w1, #0x1                   	// #1
   10030:	mov	x0, x21
   10034:	mov	x2, x22
   10038:	mov	x3, x20
   1003c:	bl	ce30 <fwrite@plt>
   10040:	ldr	x2, [x29, #24]
   10044:	adrp	x8, 59000 <__gmp_randget_mt@@Base+0x44c>
   10048:	adrp	x9, 59000 <__gmp_randget_mt@@Base+0x44c>
   1004c:	add	x8, x8, #0xcb2
   10050:	add	x9, x9, #0xcad
   10054:	cmp	w19, #0xb
   10058:	mov	x21, x0
   1005c:	csel	x1, x9, x8, lt  // lt = tstop
   10060:	mov	x0, x20
   10064:	bl	d420 <fprintf@plt>
   10068:	mov	w8, w0
   1006c:	ldur	x0, [x29, #-8]
   10070:	add	x9, x25, x24
   10074:	add	x9, x9, x21
   10078:	add	x19, x9, w8, sxtw
   1007c:	cbnz	x0, 100bc <__gmpf_out_str@@Base+0x1ac>
   10080:	mov	x0, x20
   10084:	bl	d4a0 <ferror@plt>
   10088:	cmp	w0, #0x0
   1008c:	csel	x0, x19, xzr, eq  // eq = none
   10090:	mov	sp, x29
   10094:	ldp	x20, x19, [sp, #64]
   10098:	ldp	x22, x21, [sp, #48]
   1009c:	ldp	x24, x23, [sp, #32]
   100a0:	ldr	x25, [sp, #16]
   100a4:	ldp	x29, x30, [sp], #80
   100a8:	ret
   100ac:	sub	x0, x29, #0x8
   100b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   100b4:	mov	x21, x0
   100b8:	b	ffa8 <__gmpf_out_str@@Base+0x98>
   100bc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   100c0:	b	10080 <__gmpf_out_str@@Base+0x170>

00000000000100c4 <__gmpf_add@@Base>:
   100c4:	stp	x29, x30, [sp, #-96]!
   100c8:	stp	x28, x27, [sp, #16]
   100cc:	stp	x26, x25, [sp, #32]
   100d0:	stp	x24, x23, [sp, #48]
   100d4:	stp	x22, x21, [sp, #64]
   100d8:	stp	x20, x19, [sp, #80]
   100dc:	mov	x29, sp
   100e0:	sub	sp, sp, #0x30
   100e4:	ldr	w20, [x1, #4]
   100e8:	mov	x19, x0
   100ec:	cbz	w20, 1023c <__gmpf_add@@Base+0x178>
   100f0:	ldr	w8, [x2, #4]
   100f4:	cbz	w8, 10238 <__gmpf_add@@Base+0x174>
   100f8:	eor	w9, w8, w20
   100fc:	tbnz	w9, #31, 10254 <__gmpf_add@@Base+0x190>
   10100:	stur	xzr, [x29, #-24]
   10104:	ldr	x9, [x1, #8]
   10108:	ldr	x10, [x2, #8]
   1010c:	ldrsw	x28, [x19]
   10110:	ldr	x0, [x19, #16]
   10114:	mov	w11, #0x7f00                	// #32512
   10118:	cmp	x9, x10
   1011c:	csel	x10, x1, x2, lt  // lt = tstop
   10120:	csel	x12, x2, x1, lt  // lt = tstop
   10124:	csel	w9, w20, w8, lt  // lt = tstop
   10128:	csel	w8, w8, w20, lt  // lt = tstop
   1012c:	ldp	x14, x13, [x12, #8]
   10130:	ldp	x23, x12, [x10, #8]
   10134:	sxtw	x8, w8
   10138:	sxtw	x9, w9
   1013c:	cmp	x8, #0x0
   10140:	cneg	x8, x8, mi  // mi = first
   10144:	cmp	x9, #0x0
   10148:	cneg	x9, x9, mi  // mi = first
   1014c:	subs	x10, x8, x28
   10150:	sub	x27, x14, x23
   10154:	add	x10, x13, x10, lsl #3
   10158:	csel	x22, x28, x8, gt
   1015c:	add	x8, x27, x9
   10160:	csel	x24, x10, x13, gt
   10164:	subs	x8, x8, x28
   10168:	sub	x10, x28, x27
   1016c:	add	x8, x12, x8, lsl #3
   10170:	lsl	x1, x28, #3
   10174:	mov	x21, x14
   10178:	csel	x25, x10, x9, gt
   1017c:	csel	x26, x8, x12, gt
   10180:	cmp	x1, x11
   10184:	b.hi	10520 <__gmpf_add@@Base+0x45c>  // b.pmore
   10188:	add	x9, x1, #0xf
   1018c:	mov	x8, sp
   10190:	and	x9, x9, #0xfffffffffffffff0
   10194:	sub	x9, x8, x9
   10198:	mov	sp, x9
   1019c:	cmp	x27, x28
   101a0:	b.ge	1053c <__gmpf_add@@Base+0x478>  // b.tcont
   101a4:	subs	x8, x22, x27
   101a8:	add	x10, x25, x27
   101ac:	stp	x10, x0, [x29, #-40]
   101b0:	b.le	10274 <__gmpf_add@@Base+0x1b0>
   101b4:	subs	x28, x10, x22
   101b8:	stur	x9, [x29, #-48]
   101bc:	b.le	102c8 <__gmpf_add@@Base+0x204>
   101c0:	mov	x0, x9
   101c4:	mov	x1, x26
   101c8:	mov	x2, x28
   101cc:	bl	ca50 <__gmpn_copyi@plt>
   101d0:	ldur	x1, [x29, #-48]
   101d4:	subs	x25, x22, x27
   101d8:	add	x27, x1, x28, lsl #3
   101dc:	b.eq	1034c <__gmpf_add@@Base+0x288>  // b.none
   101e0:	add	x2, x26, x28, lsl #3
   101e4:	mov	x0, x27
   101e8:	mov	x1, x24
   101ec:	mov	x3, x25
   101f0:	bl	ca70 <__gmpn_add_n@plt>
   101f4:	cbz	x0, 10368 <__gmpf_add@@Base+0x2a4>
   101f8:	ldp	x1, x28, [x29, #-48]
   101fc:	mov	w26, #0x1                   	// #1
   10200:	add	x8, x23, x28
   10204:	lsl	x8, x8, #3
   10208:	mov	x23, x21
   1020c:	sub	x8, x8, x21, lsl #3
   10210:	ldur	x21, [x29, #-32]
   10214:	add	x8, x1, x8
   10218:	cmp	x25, x22
   1021c:	b.ge	104b0 <__gmpf_add@@Base+0x3ec>  // b.tcont
   10220:	ldr	x9, [x24, x25, lsl #3]
   10224:	add	x25, x25, #0x1
   10228:	adds	x9, x9, #0x1
   1022c:	str	x9, [x8], #8
   10230:	b.cs	10218 <__gmpf_add@@Base+0x154>  // b.hs, b.nlast
   10234:	b	10374 <__gmpf_add@@Base+0x2b0>
   10238:	mov	x2, x1
   1023c:	cmp	x2, x19
   10240:	b.eq	10570 <__gmpf_add@@Base+0x4ac>  // b.none
   10244:	mov	x0, x19
   10248:	mov	x1, x2
   1024c:	bl	c150 <__gmpf_set@plt>
   10250:	b	10570 <__gmpf_add@@Base+0x4ac>
   10254:	neg	w8, w8
   10258:	stur	w8, [x29, #-20]
   1025c:	ldur	q0, [x2, #8]
   10260:	sub	x2, x29, #0x18
   10264:	mov	x0, x19
   10268:	stur	q0, [x29, #-16]
   1026c:	bl	ce00 <__gmpf_sub@plt>
   10270:	b	10570 <__gmpf_add@@Base+0x4ac>
   10274:	mov	x0, x9
   10278:	mov	x1, x26
   1027c:	mov	x2, x25
   10280:	sub	x23, x10, x22
   10284:	mov	x28, x9
   10288:	bl	ca50 <__gmpn_copyi@plt>
   1028c:	subs	x8, x27, x22
   10290:	b.eq	102a4 <__gmpf_add@@Base+0x1e0>  // b.none
   10294:	add	x0, x28, x25, lsl #3
   10298:	lsl	x2, x8, #3
   1029c:	mov	w1, wzr
   102a0:	bl	c5f0 <memset@plt>
   102a4:	add	x0, x28, x23, lsl #3
   102a8:	mov	x1, x24
   102ac:	mov	x2, x22
   102b0:	bl	ca50 <__gmpn_copyi@plt>
   102b4:	mov	x23, x21
   102b8:	mov	x1, x28
   102bc:	ldp	x28, x21, [x29, #-40]
   102c0:	mov	x26, xzr
   102c4:	b	104b0 <__gmpf_add@@Base+0x3ec>
   102c8:	sub	x27, x8, x25
   102cc:	mov	x0, x9
   102d0:	mov	x1, x24
   102d4:	mov	x2, x27
   102d8:	bl	ca50 <__gmpn_copyi@plt>
   102dc:	sub	x28, x22, x27
   102e0:	cbz	x25, 10358 <__gmpf_add@@Base+0x294>
   102e4:	lsl	x8, x27, #3
   102e8:	ldur	x27, [x29, #-48]
   102ec:	add	x1, x24, x8
   102f0:	mov	x2, x26
   102f4:	mov	x3, x25
   102f8:	add	x0, x27, x8
   102fc:	bl	ca70 <__gmpn_add_n@plt>
   10300:	cbz	x0, 10428 <__gmpf_add@@Base+0x364>
   10304:	add	x8, x23, x22
   10308:	lsl	x9, x8, #3
   1030c:	mov	x23, x21
   10310:	sub	x8, x8, x21
   10314:	sub	x9, x9, x21, lsl #3
   10318:	ldur	x21, [x29, #-32]
   1031c:	add	x8, x24, x8, lsl #3
   10320:	add	x9, x27, x9
   10324:	mov	w26, #0x1                   	// #1
   10328:	mov	x1, x27
   1032c:	cmp	x25, x28
   10330:	b.ge	104ac <__gmpf_add@@Base+0x3e8>  // b.tcont
   10334:	ldr	x10, [x8], #8
   10338:	add	x25, x25, #0x1
   1033c:	adds	x10, x10, #0x1
   10340:	str	x10, [x9], #8
   10344:	b.cs	1032c <__gmpf_add@@Base+0x268>  // b.hs, b.nlast
   10348:	b	10434 <__gmpf_add@@Base+0x370>
   1034c:	mov	x23, x21
   10350:	ldp	x28, x21, [x29, #-40]
   10354:	b	10374 <__gmpf_add@@Base+0x2b0>
   10358:	mov	x23, x21
   1035c:	ldur	x21, [x29, #-32]
   10360:	ldur	x1, [x29, #-48]
   10364:	b	10434 <__gmpf_add@@Base+0x370>
   10368:	mov	x23, x21
   1036c:	ldp	x28, x21, [x29, #-40]
   10370:	ldur	x1, [x29, #-48]
   10374:	cmp	x27, x24
   10378:	mov	x26, xzr
   1037c:	b.eq	104b0 <__gmpf_add@@Base+0x3ec>  // b.none
   10380:	subs	x8, x22, x25
   10384:	b.le	104b0 <__gmpf_add@@Base+0x3ec>
   10388:	cmp	x8, #0x4
   1038c:	b.cc	10408 <__gmpf_add@@Base+0x344>  // b.lo, b.ul, b.last
   10390:	lsl	x10, x25, #3
   10394:	lsl	x9, x28, #3
   10398:	lsl	x11, x22, #3
   1039c:	add	x12, x10, x9
   103a0:	sub	x12, x12, x11
   103a4:	add	x12, x1, x12
   103a8:	add	x11, x24, x11
   103ac:	cmp	x12, x11
   103b0:	b.cs	103c4 <__gmpf_add@@Base+0x300>  // b.hs, b.nlast
   103b4:	add	x9, x1, x9
   103b8:	add	x11, x24, x10
   103bc:	cmp	x9, x11
   103c0:	b.hi	10408 <__gmpf_add@@Base+0x344>  // b.pmore
   103c4:	add	x11, x10, x24
   103c8:	add	x12, x10, x28, lsl #3
   103cc:	add	x10, x11, #0x10
   103d0:	sub	x11, x12, x22, lsl #3
   103d4:	and	x9, x8, #0xfffffffffffffffc
   103d8:	add	x11, x11, x1
   103dc:	add	x25, x25, x9
   103e0:	add	x11, x11, #0x10
   103e4:	mov	x12, x9
   103e8:	ldp	q0, q1, [x10, #-16]
   103ec:	subs	x12, x12, #0x4
   103f0:	add	x10, x10, #0x20
   103f4:	stp	q0, q1, [x11, #-16]
   103f8:	add	x11, x11, #0x20
   103fc:	b.ne	103e8 <__gmpf_add@@Base+0x324>  // b.any
   10400:	cmp	x8, x9
   10404:	b.eq	102c0 <__gmpf_add@@Base+0x1fc>  // b.none
   10408:	sub	x8, x25, x22
   1040c:	add	x9, x1, x28, lsl #3
   10410:	add	x10, x24, x25, lsl #3
   10414:	ldr	x11, [x10], #8
   10418:	str	x11, [x9, x8, lsl #3]
   1041c:	adds	x8, x8, #0x1
   10420:	b.cc	10414 <__gmpf_add@@Base+0x350>  // b.lo, b.ul, b.last
   10424:	b	102c0 <__gmpf_add@@Base+0x1fc>
   10428:	mov	x23, x21
   1042c:	ldur	x21, [x29, #-32]
   10430:	mov	x1, x27
   10434:	cmp	x24, x1
   10438:	mov	x26, xzr
   1043c:	b.eq	104ac <__gmpf_add@@Base+0x3e8>  // b.none
   10440:	subs	x9, x28, x25
   10444:	b.le	104ac <__gmpf_add@@Base+0x3e8>
   10448:	cmp	x9, #0x4
   1044c:	lsl	x8, x22, #3
   10450:	b.cc	10484 <__gmpf_add@@Base+0x3c0>  // b.lo, b.ul, b.last
   10454:	add	x10, x8, x25, lsl #3
   10458:	sub	x10, x10, x28, lsl #3
   1045c:	add	x10, x1, x10
   10460:	add	x11, x24, x8
   10464:	cmp	x10, x11
   10468:	add	x11, x25, x22
   1046c:	b.cs	104cc <__gmpf_add@@Base+0x408>  // b.hs, b.nlast
   10470:	sub	x12, x11, x28
   10474:	add	x10, x1, x8
   10478:	add	x12, x24, x12, lsl #3
   1047c:	cmp	x10, x12
   10480:	b.ls	104cc <__gmpf_add@@Base+0x408>  // b.plast
   10484:	ldur	x14, [x29, #-40]
   10488:	sub	x9, x25, x14
   1048c:	add	x10, x1, x8
   10490:	add	x8, x24, x8
   10494:	lsl	x11, x9, #3
   10498:	ldr	x12, [x8, x11]
   1049c:	adds	x9, x9, #0x1
   104a0:	str	x12, [x10, x11]
   104a4:	b.cc	10494 <__gmpf_add@@Base+0x3d0>  // b.lo, b.ul, b.last
   104a8:	mov	x26, xzr
   104ac:	mov	x28, x22
   104b0:	mov	x0, x21
   104b4:	mov	x2, x28
   104b8:	bl	ca50 <__gmpn_copyi@plt>
   104bc:	add	x22, x26, x28
   104c0:	add	x23, x26, x23
   104c4:	str	x26, [x21, x28, lsl #3]
   104c8:	b	10554 <__gmpf_add@@Base+0x490>
   104cc:	ldur	x14, [x29, #-40]
   104d0:	lsl	x12, x25, #3
   104d4:	add	x12, x12, x22, lsl #3
   104d8:	and	x10, x9, #0xfffffffffffffffc
   104dc:	sub	x11, x11, x14
   104e0:	sub	x12, x12, x14, lsl #3
   104e4:	add	x11, x24, x11, lsl #3
   104e8:	add	x12, x12, x1
   104ec:	add	x25, x25, x10
   104f0:	add	x11, x11, #0x10
   104f4:	add	x12, x12, #0x10
   104f8:	mov	x13, x10
   104fc:	ldp	q0, q1, [x11, #-16]
   10500:	subs	x13, x13, #0x4
   10504:	add	x11, x11, #0x20
   10508:	stp	q0, q1, [x12, #-16]
   1050c:	add	x12, x12, #0x20
   10510:	b.ne	104fc <__gmpf_add@@Base+0x438>  // b.any
   10514:	cmp	x9, x10
   10518:	b.ne	10488 <__gmpf_add@@Base+0x3c4>  // b.any
   1051c:	b	104a8 <__gmpf_add@@Base+0x3e4>
   10520:	stur	x0, [x29, #-32]
   10524:	sub	x0, x29, #0x18
   10528:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1052c:	mov	x9, x0
   10530:	ldur	x0, [x29, #-32]
   10534:	cmp	x27, x28
   10538:	b.lt	101a4 <__gmpf_add@@Base+0xe0>  // b.tstop
   1053c:	cmp	x0, x24
   10540:	b.eq	10550 <__gmpf_add@@Base+0x48c>  // b.none
   10544:	mov	x1, x24
   10548:	mov	x2, x22
   1054c:	bl	ca50 <__gmpn_copyi@plt>
   10550:	mov	x23, x21
   10554:	neg	w8, w22
   10558:	cmp	w20, #0x0
   1055c:	csel	x8, x8, x22, lt  // lt = tstop
   10560:	str	w8, [x19, #4]
   10564:	str	x23, [x19, #8]
   10568:	ldur	x0, [x29, #-24]
   1056c:	cbnz	x0, 10590 <__gmpf_add@@Base+0x4cc>
   10570:	mov	sp, x29
   10574:	ldp	x20, x19, [sp, #80]
   10578:	ldp	x22, x21, [sp, #64]
   1057c:	ldp	x24, x23, [sp, #48]
   10580:	ldp	x26, x25, [sp, #32]
   10584:	ldp	x28, x27, [sp, #16]
   10588:	ldp	x29, x30, [sp], #96
   1058c:	ret
   10590:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   10594:	b	10570 <__gmpf_add@@Base+0x4ac>

0000000000010598 <__gmpf_add_ui@@Base>:
   10598:	sub	sp, sp, #0x70
   1059c:	stp	x29, x30, [sp, #32]
   105a0:	stp	x24, x23, [sp, #64]
   105a4:	stp	x22, x21, [sp, #80]
   105a8:	stp	x20, x19, [sp, #96]
   105ac:	mov	x23, x1
   105b0:	ldrsw	x22, [x1, #4]
   105b4:	ldr	x1, [x1, #16]
   105b8:	ldr	x24, [x23, #8]
   105bc:	mov	x21, x2
   105c0:	cmp	w22, #0x0
   105c4:	mov	x19, x0
   105c8:	str	x25, [sp, #48]
   105cc:	add	x29, sp, #0x20
   105d0:	b.le	10624 <__gmpf_add_ui@@Base+0x8c>
   105d4:	ldr	x20, [x19, #16]
   105d8:	ldrsw	x8, [x19]
   105dc:	cbz	x21, 105f0 <__gmpf_add_ui@@Base+0x58>
   105e0:	subs	x25, x24, #0x1
   105e4:	b.lt	10654 <__gmpf_add_ui@@Base+0xbc>  // b.tstop
   105e8:	cmp	x24, x8
   105ec:	b.le	106e4 <__gmpf_add_ui@@Base+0x14c>
   105f0:	cmp	x23, x19
   105f4:	b.eq	108e4 <__gmpf_add_ui@@Base+0x34c>  // b.none
   105f8:	cmp	w22, w8
   105fc:	csinc	x21, x22, x8, le
   10600:	add	x8, x1, x22, lsl #3
   10604:	sub	x1, x8, x21, lsl #3
   10608:	mov	x0, x20
   1060c:	mov	x2, x21
   10610:	bl	ca50 <__gmpn_copyi@plt>
   10614:	str	w21, [x19, #4]
   10618:	ldr	x8, [x23, #8]
   1061c:	str	x8, [x19, #8]
   10620:	b	108e4 <__gmpf_add_ui@@Base+0x34c>
   10624:	cbz	w22, 106c0 <__gmpf_add_ui@@Base+0x128>
   10628:	neg	w8, w22
   1062c:	stp	x24, x1, [sp, #16]
   10630:	add	x1, sp, #0x8
   10634:	mov	x0, x19
   10638:	mov	x2, x21
   1063c:	str	w8, [sp, #12]
   10640:	bl	c1f0 <__gmpf_sub_ui@plt>
   10644:	ldr	w8, [x19, #4]
   10648:	neg	w8, w8
   1064c:	str	w8, [x19, #4]
   10650:	b	108e4 <__gmpf_add_ui@@Base+0x34c>
   10654:	neg	x9, x24
   10658:	cmp	x9, x8
   1065c:	b.ge	10728 <__gmpf_add_ui@@Base+0x190>  // b.tcont
   10660:	add	x10, x8, x24
   10664:	sub	x9, x22, x24
   10668:	sub	x10, x22, x10
   1066c:	cmp	x9, x8
   10670:	add	x8, x10, #0x1
   10674:	add	x8, x1, x8, lsl #3
   10678:	csinc	x9, xzr, x10, lt  // lt = tstop
   1067c:	csel	x1, x1, x8, lt  // lt = tstop
   10680:	cmp	x20, x1
   10684:	sub	x22, x22, x9
   10688:	b.eq	10698 <__gmpf_add_ui@@Base+0x100>  // b.none
   1068c:	mov	x0, x20
   10690:	mov	x2, x22
   10694:	bl	ca50 <__gmpn_copyi@plt>
   10698:	cbz	x24, 106ac <__gmpf_add_ui@@Base+0x114>
   1069c:	add	x0, x20, x22, lsl #3
   106a0:	neg	x2, x24, lsl #3
   106a4:	mov	w1, wzr
   106a8:	bl	c5f0 <memset@plt>
   106ac:	sub	x8, x22, x24
   106b0:	mov	w9, #0x1                   	// #1
   106b4:	str	x21, [x20, x8, lsl #3]
   106b8:	add	w8, w8, #0x1
   106bc:	b	108dc <__gmpf_add_ui@@Base+0x344>
   106c0:	mov	x0, x19
   106c4:	mov	x1, x21
   106c8:	ldp	x20, x19, [sp, #96]
   106cc:	ldp	x22, x21, [sp, #80]
   106d0:	ldp	x24, x23, [sp, #64]
   106d4:	ldr	x25, [sp, #48]
   106d8:	ldp	x29, x30, [sp, #32]
   106dc:	add	sp, sp, #0x70
   106e0:	b	c6a0 <__gmpf_set_ui@plt>
   106e4:	cmp	x24, x22
   106e8:	b.le	1073c <__gmpf_add_ui@@Base+0x1a4>
   106ec:	add	x8, x20, x24, lsl #3
   106f0:	sub	x0, x8, x22, lsl #3
   106f4:	mov	x2, x22
   106f8:	bl	c000 <__gmpn_copyd@plt>
   106fc:	mvn	x8, x22
   10700:	adds	x8, x24, x8
   10704:	str	x21, [x20]
   10708:	b.eq	1071c <__gmpf_add_ui@@Base+0x184>  // b.none
   1070c:	add	x0, x20, #0x8
   10710:	lsl	x2, x8, #3
   10714:	mov	w1, wzr
   10718:	bl	c5f0 <memset@plt>
   1071c:	str	w24, [x19, #4]
   10720:	str	x24, [x19, #8]
   10724:	b	108e4 <__gmpf_add_ui@@Base+0x34c>
   10728:	mov	w8, #0x1                   	// #1
   1072c:	str	x21, [x20]
   10730:	str	w8, [x19, #4]
   10734:	str	x8, [x19, #8]
   10738:	b	108e4 <__gmpf_add_ui@@Base+0x34c>
   1073c:	sub	x9, x22, x8
   10740:	cmp	w22, w8
   10744:	add	x9, x1, x9, lsl #3
   10748:	csel	w8, w22, w8, lt  // lt = tstop
   1074c:	csel	x22, x9, x1, gt
   10750:	cmp	x20, x22
   10754:	sxtw	x23, w8
   10758:	b.eq	1076c <__gmpf_add_ui@@Base+0x1d4>  // b.none
   1075c:	sub	x2, x23, x24
   10760:	mov	x0, x20
   10764:	mov	x1, x22
   10768:	bl	ca50 <__gmpn_copyi@plt>
   1076c:	lsl	x11, x23, #3
   10770:	lsl	x10, x24, #3
   10774:	add	x9, x22, x11
   10778:	sub	x8, x9, x10
   1077c:	ldr	x12, [x8]
   10780:	add	x8, x20, x11
   10784:	sub	x13, x8, x10
   10788:	adds	x12, x12, x21
   1078c:	str	x12, [x13]
   10790:	b.cc	10870 <__gmpf_add_ui@@Base+0x2d8>  // b.lo, b.ul, b.last
   10794:	sub	x14, x20, x10
   10798:	sub	x13, x22, x10
   1079c:	mov	w12, #0x1                   	// #1
   107a0:	mov	w10, #0x1                   	// #1
   107a4:	cmp	x10, x24
   107a8:	b.ge	108d0 <__gmpf_add_ui@@Base+0x338>  // b.tcont
   107ac:	add	x15, x13, x11
   107b0:	ldr	x15, [x15, #8]
   107b4:	add	x16, x14, x11
   107b8:	add	x10, x10, #0x1
   107bc:	add	x14, x14, #0x8
   107c0:	add	x13, x13, #0x8
   107c4:	adds	x15, x15, #0x1
   107c8:	sub	x25, x25, #0x1
   107cc:	str	x15, [x16, #8]
   107d0:	b.cs	107a4 <__gmpf_add_ui@@Base+0x20c>  // b.hs, b.nlast
   107d4:	cmp	x22, x20
   107d8:	mov	x12, xzr
   107dc:	b.eq	108d0 <__gmpf_add_ui@@Base+0x338>  // b.none
   107e0:	subs	x15, x24, x10
   107e4:	b.le	108d0 <__gmpf_add_ui@@Base+0x338>
   107e8:	cmp	x15, #0x4
   107ec:	b.cc	10854 <__gmpf_add_ui@@Base+0x2bc>  // b.lo, b.ul, b.last
   107f0:	add	x12, x14, x11
   107f4:	add	x12, x12, #0x8
   107f8:	cmp	x12, x9
   107fc:	b.cs	10810 <__gmpf_add_ui@@Base+0x278>  // b.hs, b.nlast
   10800:	add	x12, x13, x11
   10804:	add	x12, x12, #0x8
   10808:	cmp	x12, x8
   1080c:	b.cc	10854 <__gmpf_add_ui@@Base+0x2bc>  // b.lo, b.ul, b.last
   10810:	add	x12, x14, x11
   10814:	add	x13, x13, x11
   10818:	sub	x14, x24, x10
   1081c:	and	x16, x25, #0xfffffffffffffffc
   10820:	and	x11, x15, #0xfffffffffffffffc
   10824:	add	x12, x12, #0x18
   10828:	add	x13, x13, #0x18
   1082c:	add	x10, x16, x10
   10830:	and	x14, x14, #0xfffffffffffffffc
   10834:	ldp	q0, q1, [x13, #-16]
   10838:	subs	x14, x14, #0x4
   1083c:	add	x13, x13, #0x20
   10840:	stp	q0, q1, [x12, #-16]
   10844:	add	x12, x12, #0x20
   10848:	b.ne	10834 <__gmpf_add_ui@@Base+0x29c>  // b.any
   1084c:	cmp	x15, x11
   10850:	b.eq	108cc <__gmpf_add_ui@@Base+0x334>  // b.none
   10854:	sub	x10, x10, x24
   10858:	lsl	x11, x10, #3
   1085c:	ldr	x12, [x9, x11]
   10860:	adds	x10, x10, #0x1
   10864:	str	x12, [x8, x11]
   10868:	b.cc	10858 <__gmpf_add_ui@@Base+0x2c0>  // b.lo, b.ul, b.last
   1086c:	b	108cc <__gmpf_add_ui@@Base+0x334>
   10870:	cmp	x24, #0x2
   10874:	mov	x12, xzr
   10878:	b.lt	108d0 <__gmpf_add_ui@@Base+0x338>  // b.tstop
   1087c:	cmp	x22, x20
   10880:	b.eq	108d0 <__gmpf_add_ui@@Base+0x338>  // b.none
   10884:	cmp	x25, #0x4
   10888:	b.cc	108b0 <__gmpf_add_ui@@Base+0x318>  // b.lo, b.ul, b.last
   1088c:	sub	x10, x23, x24
   10890:	lsl	x10, x10, #3
   10894:	add	x11, x10, #0x8
   10898:	add	x12, x20, x11
   1089c:	cmp	x12, x9
   108a0:	b.cs	10900 <__gmpf_add_ui@@Base+0x368>  // b.hs, b.nlast
   108a4:	add	x11, x22, x11
   108a8:	cmp	x11, x8
   108ac:	b.cs	10900 <__gmpf_add_ui@@Base+0x368>  // b.hs, b.nlast
   108b0:	mov	w10, #0x1                   	// #1
   108b4:	sub	x10, x10, x24
   108b8:	lsl	x11, x10, #3
   108bc:	ldr	x12, [x9, x11]
   108c0:	adds	x10, x10, #0x1
   108c4:	str	x12, [x8, x11]
   108c8:	b.cc	108b8 <__gmpf_add_ui@@Base+0x320>  // b.lo, b.ul, b.last
   108cc:	mov	x12, xzr
   108d0:	str	x12, [x8]
   108d4:	add	w8, w23, w12
   108d8:	add	x9, x12, x24
   108dc:	str	w8, [x19, #4]
   108e0:	str	x9, [x19, #8]
   108e4:	ldp	x20, x19, [sp, #96]
   108e8:	ldp	x22, x21, [sp, #80]
   108ec:	ldp	x24, x23, [sp, #64]
   108f0:	ldr	x25, [sp, #48]
   108f4:	ldp	x29, x30, [sp, #32]
   108f8:	add	sp, sp, #0x70
   108fc:	ret
   10900:	and	x11, x25, #0xfffffffffffffffc
   10904:	add	x13, x10, #0x18
   10908:	orr	x10, x11, #0x1
   1090c:	add	x12, x22, x13
   10910:	add	x13, x20, x13
   10914:	mov	x14, x11
   10918:	ldp	q0, q1, [x12, #-16]
   1091c:	subs	x14, x14, #0x4
   10920:	add	x12, x12, #0x20
   10924:	stp	q0, q1, [x13, #-16]
   10928:	add	x13, x13, #0x20
   1092c:	b.ne	10918 <__gmpf_add_ui@@Base+0x380>  // b.any
   10930:	cmp	x25, x11
   10934:	b.eq	108cc <__gmpf_add_ui@@Base+0x334>  // b.none
   10938:	b	108b4 <__gmpf_add_ui@@Base+0x31c>

000000000001093c <__gmpf_sub@@Base>:
   1093c:	stp	x29, x30, [sp, #-96]!
   10940:	stp	x28, x27, [sp, #16]
   10944:	stp	x26, x25, [sp, #32]
   10948:	stp	x24, x23, [sp, #48]
   1094c:	stp	x22, x21, [sp, #64]
   10950:	stp	x20, x19, [sp, #80]
   10954:	mov	x29, sp
   10958:	sub	sp, sp, #0x70
   1095c:	ldr	w8, [x1, #4]
   10960:	mov	x25, x0
   10964:	cbz	w8, 10b04 <__gmpf_sub@@Base+0x1c8>
   10968:	ldr	w9, [x2, #4]
   1096c:	cbz	w9, 10b14 <__gmpf_sub@@Base+0x1d8>
   10970:	eor	w10, w9, w8
   10974:	tbnz	w10, #31, 10b28 <__gmpf_sub@@Base+0x1ec>
   10978:	stur	xzr, [x29, #-24]
   1097c:	ldr	x10, [x1, #8]
   10980:	ldr	x11, [x2, #8]
   10984:	ldrsw	x12, [x25]
   10988:	ldr	x18, [x25, #16]
   1098c:	cmp	x10, x11
   10990:	cset	w10, lt  // lt = tstop
   10994:	csel	x13, x1, x2, lt  // lt = tstop
   10998:	csel	x14, x2, x1, lt  // lt = tstop
   1099c:	csel	w11, w8, w9, lt  // lt = tstop
   109a0:	csel	w9, w9, w8, lt  // lt = tstop
   109a4:	eor	w24, w10, w8, lsr #31
   109a8:	ldp	x4, x23, [x14, #8]
   109ac:	ldp	x5, x8, [x13, #8]
   109b0:	sxtw	x9, w9
   109b4:	sxtw	x10, w11
   109b8:	cmp	x9, #0x0
   109bc:	cneg	x9, x9, mi  // mi = first
   109c0:	cmp	x10, #0x0
   109c4:	sub	x0, x4, x5
   109c8:	cneg	x10, x10, mi  // mi = first
   109cc:	cmp	x0, #0x1
   109d0:	add	x21, x12, #0x1
   109d4:	b.gt	109fc <__gmpf_sub@@Base+0xc0>
   109d8:	cbz	x0, 10b5c <__gmpf_sub@@Base+0x220>
   109dc:	add	x11, x23, x9, lsl #3
   109e0:	ldur	x11, [x11, #-8]
   109e4:	cmp	x11, #0x1
   109e8:	b.ne	109fc <__gmpf_sub@@Base+0xc0>  // b.any
   109ec:	add	x11, x8, x10, lsl #3
   109f0:	ldur	x11, [x11, #-8]
   109f4:	cmn	x11, #0x1
   109f8:	b.eq	10d00 <__gmpf_sub@@Base+0x3c4>  // b.none
   109fc:	mov	x20, x4
   10a00:	mov	x19, x10
   10a04:	subs	x10, x9, x21
   10a08:	add	x10, x23, x10, lsl #3
   10a0c:	add	x11, x19, x0
   10a10:	csel	x27, x21, x9, gt
   10a14:	csel	x14, x10, x23, gt
   10a18:	subs	x9, x21, x0
   10a1c:	subs	x10, x11, x21
   10a20:	add	x10, x8, x10, lsl #3
   10a24:	csel	x22, x9, x19, gt
   10a28:	csel	x26, x10, x8, gt
   10a2c:	cmp	x21, x0
   10a30:	b.le	10b48 <__gmpf_sub@@Base+0x20c>
   10a34:	lsl	x1, x21, #3
   10a38:	mov	w8, #0x7f00                	// #32512
   10a3c:	cmp	x1, x8
   10a40:	stp	x20, x27, [x29, #-40]
   10a44:	b.hi	11668 <__gmpf_sub@@Base+0xd2c>  // b.pmore
   10a48:	add	x9, x1, #0xf
   10a4c:	mov	x8, sp
   10a50:	and	x9, x9, #0xfffffffffffffff0
   10a54:	sub	x1, x8, x9
   10a58:	mov	sp, x1
   10a5c:	cbz	x22, 10ad0 <__gmpf_sub@@Base+0x194>
   10a60:	add	x8, x19, x4
   10a64:	ldur	x13, [x29, #-32]
   10a68:	sub	x8, x8, x5
   10a6c:	cmp	x8, x21
   10a70:	csel	x19, x8, x21, lt  // lt = tstop
   10a74:	add	x11, x5, x19
   10a78:	lsl	x10, x13, #3
   10a7c:	lsl	x9, x4, #3
   10a80:	lsl	x8, x19, #3
   10a84:	lsl	x11, x11, #3
   10a88:	sub	x12, x8, x10
   10a8c:	sub	x8, x10, x8
   10a90:	sub	x10, x13, x19
   10a94:	sub	x11, x11, x9
   10a98:	add	x21, x1, x12
   10a9c:	add	x20, x1, x8
   10aa0:	add	x28, x1, x11
   10aa4:	add	x27, x14, x10, lsl #3
   10aa8:	ldr	x8, [x26]
   10aac:	cbnz	x8, 10bb4 <__gmpf_sub@@Base+0x278>
   10ab0:	add	x26, x26, #0x8
   10ab4:	sub	x22, x22, #0x1
   10ab8:	sub	x21, x21, #0x8
   10abc:	sub	x28, x28, #0x8
   10ac0:	sub	x19, x19, #0x1
   10ac4:	add	x20, x20, #0x8
   10ac8:	add	x27, x27, #0x8
   10acc:	cbnz	x22, 10aa8 <__gmpf_sub@@Base+0x16c>
   10ad0:	ldur	x27, [x29, #-32]
   10ad4:	mov	x0, x18
   10ad8:	mov	x1, x14
   10adc:	mov	x2, x27
   10ae0:	bl	ca50 <__gmpn_copyi@plt>
   10ae4:	ldur	x20, [x29, #-40]
   10ae8:	ldur	x0, [x29, #-24]
   10aec:	cbz	x0, 10cc8 <__gmpf_sub@@Base+0x38c>
   10af0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   10af4:	cbnz	x27, 10ccc <__gmpf_sub@@Base+0x390>
   10af8:	str	wzr, [x25, #4]
   10afc:	str	xzr, [x25, #8]
   10b00:	b	10ce0 <__gmpf_sub@@Base+0x3a4>
   10b04:	mov	x0, x25
   10b08:	mov	x1, x2
   10b0c:	bl	cbf0 <__gmpf_neg@plt>
   10b10:	b	10ce0 <__gmpf_sub@@Base+0x3a4>
   10b14:	cmp	x25, x1
   10b18:	b.eq	10ce0 <__gmpf_sub@@Base+0x3a4>  // b.none
   10b1c:	mov	x0, x25
   10b20:	bl	c150 <__gmpf_set@plt>
   10b24:	b	10ce0 <__gmpf_sub@@Base+0x3a4>
   10b28:	neg	w8, w9
   10b2c:	stur	w8, [x29, #-20]
   10b30:	ldur	q0, [x2, #8]
   10b34:	sub	x2, x29, #0x18
   10b38:	mov	x0, x25
   10b3c:	stur	q0, [x29, #-16]
   10b40:	bl	bf70 <__gmpf_add@plt>
   10b44:	b	10ce0 <__gmpf_sub@@Base+0x3a4>
   10b48:	cmp	x18, x14
   10b4c:	b.eq	10cc0 <__gmpf_sub@@Base+0x384>  // b.none
   10b50:	mov	x0, x18
   10b54:	mov	x1, x14
   10b58:	b	10cb8 <__gmpf_sub@@Base+0x37c>
   10b5c:	add	x14, x8, x10, lsl #3
   10b60:	add	x15, x23, x9, lsl #3
   10b64:	mov	x12, xzr
   10b68:	sub	x11, x10, x9
   10b6c:	sub	x13, x9, x10
   10b70:	sub	x14, x14, #0x8
   10b74:	sub	x15, x15, #0x8
   10b78:	lsl	x16, x12, #3
   10b7c:	ldr	x17, [x15, x16]
   10b80:	ldr	x16, [x14, x16]
   10b84:	cmp	x17, x16
   10b88:	add	x16, x9, x12
   10b8c:	b.ne	10c30 <__gmpf_sub@@Base+0x2f4>  // b.any
   10b90:	sub	x16, x16, #0x1
   10b94:	cbz	x16, 10c78 <__gmpf_sub@@Base+0x33c>
   10b98:	sub	x12, x12, #0x1
   10b9c:	cmn	x10, x12
   10ba0:	b.ne	10b78 <__gmpf_sub@@Base+0x23c>  // b.any
   10ba4:	mov	x9, x10
   10ba8:	mov	x11, x13
   10bac:	mov	x8, x23
   10bb0:	b	10c7c <__gmpf_sub@@Base+0x340>
   10bb4:	ldur	x10, [x29, #-32]
   10bb8:	stur	w24, [x29, #-52]
   10bbc:	stur	x25, [x29, #-48]
   10bc0:	cbz	x10, 10c04 <__gmpf_sub@@Base+0x2c8>
   10bc4:	ldur	x3, [x29, #-32]
   10bc8:	mov	x24, x14
   10bcc:	add	x10, x5, x3
   10bd0:	lsl	x10, x10, #3
   10bd4:	sub	x9, x10, x9
   10bd8:	add	x25, x1, x9
   10bdc:	sub	x23, x3, #0x1
   10be0:	ldr	x9, [x24]
   10be4:	cbnz	x9, 10d24 <__gmpf_sub@@Base+0x3e8>
   10be8:	add	x24, x24, #0x8
   10bec:	sub	x3, x3, #0x1
   10bf0:	add	x21, x21, #0x8
   10bf4:	sub	x20, x20, #0x8
   10bf8:	sub	x25, x25, #0x8
   10bfc:	sub	x23, x23, #0x1
   10c00:	cbnz	x3, 10be0 <__gmpf_sub@@Base+0x2a4>
   10c04:	mov	x0, x18
   10c08:	mov	x1, x26
   10c0c:	mov	x2, x22
   10c10:	bl	ca50 <__gmpn_copyi@plt>
   10c14:	ldur	w24, [x29, #-52]
   10c18:	ldp	x25, x20, [x29, #-48]
   10c1c:	mov	x27, x22
   10c20:	eor	w24, w24, #0x1
   10c24:	ldur	x0, [x29, #-24]
   10c28:	cbz	x0, 10cc8 <__gmpf_sub@@Base+0x38c>
   10c2c:	b	10af0 <__gmpf_sub@@Base+0x1b4>
   10c30:	add	x9, x10, x12
   10c34:	csel	x19, x16, x9, cc  // cc = lo, ul, last
   10c38:	csel	x13, x23, x8, cc  // cc = lo, ul, last
   10c3c:	csel	x23, x8, x23, cc  // cc = lo, ul, last
   10c40:	csel	x9, x9, x16, cc  // cc = lo, ul, last
   10c44:	sub	x10, x19, #0x1
   10c48:	add	x8, x23, x9, lsl #3
   10c4c:	ldr	x11, [x13, x10, lsl #3]
   10c50:	ldur	x8, [x8, #-8]
   10c54:	cset	w14, cc  // cc = lo, ul, last
   10c58:	eor	w24, w24, w14
   10c5c:	add	x11, x11, #0x1
   10c60:	cmp	x8, x11
   10c64:	add	x11, x4, x12
   10c68:	mov	x8, x13
   10c6c:	mov	x20, x11
   10c70:	b.ne	10a04 <__gmpf_sub@@Base+0xc8>  // b.any
   10c74:	b	10e10 <__gmpf_sub@@Base+0x4d4>
   10c78:	eor	w24, w24, #0x1
   10c7c:	sub	x20, x4, x9
   10c80:	cbz	x11, 10ca4 <__gmpf_sub@@Base+0x368>
   10c84:	sub	x9, x20, x11
   10c88:	sub	x10, x8, #0x8
   10c8c:	ldr	x12, [x10, x11, lsl #3]
   10c90:	cbnz	x12, 10ca4 <__gmpf_sub@@Base+0x368>
   10c94:	sub	x11, x11, #0x1
   10c98:	sub	x20, x20, #0x1
   10c9c:	cbnz	x11, 10c8c <__gmpf_sub@@Base+0x350>
   10ca0:	mov	x20, x9
   10ca4:	subs	x9, x11, x21
   10ca8:	add	x9, x8, x9, lsl #3
   10cac:	csel	x27, x21, x11, gt
   10cb0:	csel	x1, x9, x8, gt
   10cb4:	mov	x0, x18
   10cb8:	mov	x2, x27
   10cbc:	bl	ca50 <__gmpn_copyi@plt>
   10cc0:	ldur	x0, [x29, #-24]
   10cc4:	cbnz	x0, 10af0 <__gmpf_sub@@Base+0x1b4>
   10cc8:	cbz	x27, 10af8 <__gmpf_sub@@Base+0x1bc>
   10ccc:	neg	w8, w27
   10cd0:	cmp	w24, #0x0
   10cd4:	csel	x8, x27, x8, eq  // eq = none
   10cd8:	str	w8, [x25, #4]
   10cdc:	str	x20, [x25, #8]
   10ce0:	mov	sp, x29
   10ce4:	ldp	x20, x19, [sp, #80]
   10ce8:	ldp	x22, x21, [sp, #64]
   10cec:	ldp	x24, x23, [sp, #48]
   10cf0:	ldp	x26, x25, [sp, #32]
   10cf4:	ldp	x28, x27, [sp, #16]
   10cf8:	ldp	x29, x30, [sp], #96
   10cfc:	ret
   10d00:	cmp	x9, #0x2
   10d04:	b.lt	10e0c <__gmpf_sub@@Base+0x4d0>  // b.tstop
   10d08:	add	x11, x23, x9, lsl #3
   10d0c:	ldur	x12, [x11, #-16]
   10d10:	mov	x11, x4
   10d14:	mov	x20, x4
   10d18:	mov	x19, x10
   10d1c:	cbnz	x12, 10a04 <__gmpf_sub@@Base+0xc8>
   10d20:	b	10e10 <__gmpf_sub@@Base+0x4d4>
   10d24:	subs	x9, x3, x0
   10d28:	stur	x1, [x29, #-72]
   10d2c:	b.le	10eec <__gmpf_sub@@Base+0x5b0>
   10d30:	cbz	x0, 11088 <__gmpf_sub@@Base+0x74c>
   10d34:	add	x10, x22, x0
   10d38:	subs	x23, x10, x3
   10d3c:	stur	x14, [x29, #-88]
   10d40:	stur	x18, [x29, #-64]
   10d44:	b.le	11238 <__gmpf_sub@@Base+0x8fc>
   10d48:	neg	x8, x8
   10d4c:	subs	x2, x23, #0x1
   10d50:	stur	x10, [x29, #-96]
   10d54:	str	x8, [x1]
   10d58:	b.eq	10d8c <__gmpf_sub@@Base+0x450>  // b.none
   10d5c:	mov	x27, x0
   10d60:	add	x0, x1, #0x8
   10d64:	add	x1, x26, #0x8
   10d68:	mov	x19, x3
   10d6c:	mov	x20, x4
   10d70:	mov	x25, x5
   10d74:	bl	c290 <__gmpn_com@plt>
   10d78:	ldur	x1, [x29, #-72]
   10d7c:	mov	x0, x27
   10d80:	mov	x5, x25
   10d84:	mov	x4, x20
   10d88:	mov	x3, x19
   10d8c:	ldp	x25, x20, [x29, #-48]
   10d90:	subs	x8, x3, x0
   10d94:	add	x19, x1, x23, lsl #3
   10d98:	b.eq	114b8 <__gmpf_sub@@Base+0xb7c>  // b.none
   10d9c:	add	x2, x26, x23, lsl #3
   10da0:	mov	x0, x19
   10da4:	mov	x1, x24
   10da8:	stur	x3, [x29, #-80]
   10dac:	mov	x3, x8
   10db0:	mov	x26, x4
   10db4:	mov	x27, x5
   10db8:	mov	x23, x8
   10dbc:	bl	c2d0 <__gmpn_sub_n@plt>
   10dc0:	ldp	x3, x1, [x29, #-80]
   10dc4:	ldur	x15, [x29, #-64]
   10dc8:	mov	x5, x27
   10dcc:	mov	x4, x26
   10dd0:	cbz	x0, 114c0 <__gmpf_sub@@Base+0xb84>
   10dd4:	ldur	x9, [x29, #-32]
   10dd8:	ldur	x10, [x29, #-88]
   10ddc:	sub	x8, x5, x4
   10de0:	add	x9, x10, x9, lsl #3
   10de4:	add	x10, x3, x8
   10de8:	cmp	x10, x3
   10dec:	b.ge	11528 <__gmpf_sub@@Base+0xbec>  // b.tcont
   10df0:	ldr	x10, [x9, x8, lsl #3]
   10df4:	add	x8, x8, #0x1
   10df8:	sub	x11, x10, #0x1
   10dfc:	str	x11, [x28], #8
   10e00:	cbz	x10, 10de4 <__gmpf_sub@@Base+0x4a8>
   10e04:	add	x23, x3, x8
   10e08:	b	114c0 <__gmpf_sub@@Base+0xb84>
   10e0c:	mov	x11, x4
   10e10:	lsl	x12, x10, #3
   10e14:	mov	w13, #0x8                   	// #8
   10e18:	lsl	x1, x21, #3
   10e1c:	add	x14, x12, x8
   10e20:	mov	w27, w24
   10e24:	mov	x0, xzr
   10e28:	sub	x2, x13, x12
   10e2c:	sub	x16, x14, #0x8
   10e30:	add	x17, x23, x9, lsl #3
   10e34:	mov	x3, x1
   10e38:	stur	x18, [x29, #-64]
   10e3c:	add	x18, x9, x0
   10e40:	mov	x15, x2
   10e44:	mov	x13, x0
   10e48:	mov	x14, x3
   10e4c:	add	x12, x10, x0
   10e50:	sub	x24, x18, #0x1
   10e54:	cbz	x12, 10e80 <__gmpf_sub@@Base+0x544>
   10e58:	cbz	x24, 10e80 <__gmpf_sub@@Base+0x544>
   10e5c:	add	x0, x17, x13, lsl #3
   10e60:	ldur	x0, [x0, #-16]
   10e64:	cbnz	x0, 10e80 <__gmpf_sub@@Base+0x544>
   10e68:	ldr	x3, [x16, x13, lsl #3]
   10e6c:	add	x2, x15, #0x8
   10e70:	sub	x0, x13, #0x1
   10e74:	cmn	x3, #0x1
   10e78:	add	x3, x14, #0x8
   10e7c:	b.eq	10e3c <__gmpf_sub@@Base+0x500>  // b.none
   10e80:	add	x16, x11, x13
   10e84:	sub	x28, x16, #0x1
   10e88:	cbz	x24, 10ea8 <__gmpf_sub@@Base+0x56c>
   10e8c:	mov	x20, x25
   10e90:	cmp	x18, x21
   10e94:	b.le	10fd4 <__gmpf_sub@@Base+0x698>
   10e98:	add	x9, x23, x9, lsl #3
   10e9c:	sub	x24, x21, #0x1
   10ea0:	sub	x23, x9, x14
   10ea4:	b	10fd4 <__gmpf_sub@@Base+0x698>
   10ea8:	cbz	x12, 10fac <__gmpf_sub@@Base+0x670>
   10eac:	mov	x9, xzr
   10eb0:	sub	x14, x8, x15
   10eb4:	sub	x28, x28, x12
   10eb8:	add	x12, x10, x13
   10ebc:	ldr	x15, [x14]
   10ec0:	cmn	x15, #0x1
   10ec4:	b.ne	10fb8 <__gmpf_sub@@Base+0x67c>  // b.any
   10ec8:	add	x15, x12, x9
   10ecc:	sub	x9, x9, #0x1
   10ed0:	cmp	x15, #0x1
   10ed4:	sub	x14, x14, #0x8
   10ed8:	b.ne	10ebc <__gmpf_sub@@Base+0x580>  // b.any
   10edc:	mov	x20, x25
   10ee0:	mov	x12, xzr
   10ee4:	mov	x24, xzr
   10ee8:	b	10fd4 <__gmpf_sub@@Base+0x698>
   10eec:	add	x25, x22, x0
   10ef0:	mov	x20, x18
   10ef4:	neg	x8, x8
   10ef8:	subs	x2, x22, #0x1
   10efc:	sub	x19, x25, x3
   10f00:	str	x8, [x1]
   10f04:	b.eq	10f30 <__gmpf_sub@@Base+0x5f4>  // b.none
   10f08:	add	x0, x1, #0x8
   10f0c:	add	x1, x26, #0x8
   10f10:	mov	x26, x3
   10f14:	mov	x27, x4
   10f18:	mov	x28, x5
   10f1c:	bl	c290 <__gmpn_com@plt>
   10f20:	ldur	x1, [x29, #-72]
   10f24:	mov	x5, x28
   10f28:	mov	x4, x27
   10f2c:	mov	x3, x26
   10f30:	cmp	x22, x19
   10f34:	b.ge	10f6c <__gmpf_sub@@Base+0x630>  // b.tcont
   10f38:	add	x8, x3, x5
   10f3c:	sub	x8, x4, x8
   10f40:	add	x0, x1, x22, lsl #3
   10f44:	lsl	x2, x8, #3
   10f48:	mov	w1, #0xff                  	// #255
   10f4c:	mov	x26, x3
   10f50:	mov	x27, x4
   10f54:	mov	x28, x5
   10f58:	bl	c5f0 <memset@plt>
   10f5c:	ldur	x1, [x29, #-72]
   10f60:	mov	x5, x28
   10f64:	mov	x4, x27
   10f68:	mov	x3, x26
   10f6c:	ldr	x8, [x24]
   10f70:	add	x10, x1, x19, lsl #3
   10f74:	sub	x9, x8, #0x1
   10f78:	str	x9, [x10]
   10f7c:	cbz	x8, 110e0 <__gmpf_sub@@Base+0x7a4>
   10f80:	cmp	x3, #0x2
   10f84:	mov	x15, x20
   10f88:	b.lt	11468 <__gmpf_sub@@Base+0xb2c>  // b.tstop
   10f8c:	ldur	x20, [x29, #-40]
   10f90:	cmp	x10, x24
   10f94:	b.eq	115a0 <__gmpf_sub@@Base+0xc64>  // b.none
   10f98:	sub	x9, x3, #0x1
   10f9c:	cmp	x9, #0x4
   10fa0:	b.cs	11474 <__gmpf_sub@@Base+0xb38>  // b.hs, b.nlast
   10fa4:	mov	w8, #0x1                   	// #1
   10fa8:	b	11588 <__gmpf_sub@@Base+0xc4c>
   10fac:	mov	x20, x25
   10fb0:	mov	x24, xzr
   10fb4:	b	10fd4 <__gmpf_sub@@Base+0x698>
   10fb8:	add	x10, x10, x13
   10fbc:	add	x11, x11, x13
   10fc0:	add	x12, x10, x9
   10fc4:	add	x9, x11, x9
   10fc8:	mov	x20, x25
   10fcc:	mov	x24, xzr
   10fd0:	sub	x28, x9, #0x1
   10fd4:	sub	x9, x21, #0x1
   10fd8:	cmp	x12, x21
   10fdc:	sub	x11, x12, x9
   10fe0:	mov	w10, #0x7f00                	// #32512
   10fe4:	csel	x22, x12, x9, lt  // lt = tstop
   10fe8:	add	x9, x8, x11, lsl #3
   10fec:	csel	x25, x8, x9, lt  // lt = tstop
   10ff0:	cmp	x1, x10
   10ff4:	b.hi	116a4 <__gmpf_sub@@Base+0xd68>  // b.pmore
   10ff8:	add	x9, x1, #0xf
   10ffc:	mov	x8, sp
   11000:	and	x9, x9, #0xfffffffffffffff0
   11004:	sub	x1, x8, x9
   11008:	mov	sp, x1
   1100c:	cbz	x22, 116b4 <__gmpf_sub@@Base+0xd78>
   11010:	cbz	x24, 11038 <__gmpf_sub@@Base+0x6fc>
   11014:	subs	x26, x24, x22
   11018:	b.ge	11050 <__gmpf_sub@@Base+0x714>  // b.tcont
   1101c:	ldr	x8, [x25]
   11020:	sub	x19, x22, x24
   11024:	cbz	x8, 11368 <__gmpf_sub@@Base+0xa2c>
   11028:	mov	x9, x1
   1102c:	mov	x10, x25
   11030:	mov	x11, x19
   11034:	b	113a4 <__gmpf_sub@@Base+0xa68>
   11038:	ldr	x8, [x25]
   1103c:	cbz	x8, 111c4 <__gmpf_sub@@Base+0x888>
   11040:	mov	x9, x1
   11044:	mov	x10, x22
   11048:	mov	w24, w27
   1104c:	b	11204 <__gmpf_sub@@Base+0x8c8>
   11050:	mov	x0, x1
   11054:	mov	x21, x1
   11058:	mov	x1, x23
   1105c:	mov	x2, x26
   11060:	bl	ca50 <__gmpn_copyi@plt>
   11064:	lsl	x8, x26, #3
   11068:	add	x0, x21, x8
   1106c:	add	x1, x23, x8
   11070:	mov	x2, x25
   11074:	mov	x3, x22
   11078:	mov	x19, x21
   1107c:	bl	c2d0 <__gmpn_sub_n@plt>
   11080:	mov	x22, x24
   11084:	b	113e8 <__gmpf_sub@@Base+0xaac>
   11088:	mov	x20, x18
   1108c:	subs	x25, x3, x22
   11090:	b.ge	11404 <__gmpf_sub@@Base+0xac8>  // b.tcont
   11094:	sub	x19, x22, x3
   11098:	neg	x8, x8
   1109c:	subs	x2, x19, #0x1
   110a0:	str	x8, [x1]
   110a4:	b.eq	110c0 <__gmpf_sub@@Base+0x784>  // b.none
   110a8:	add	x0, x1, #0x8
   110ac:	add	x1, x26, #0x8
   110b0:	mov	x21, x3
   110b4:	bl	c290 <__gmpn_com@plt>
   110b8:	ldur	x1, [x29, #-72]
   110bc:	mov	x3, x21
   110c0:	lsl	x8, x19, #3
   110c4:	add	x0, x1, x8
   110c8:	add	x2, x26, x8
   110cc:	mov	w4, #0x1                   	// #1
   110d0:	mov	x1, x24
   110d4:	bl	c760 <__gmpn_sub_nc@plt>
   110d8:	ldur	x1, [x29, #-72]
   110dc:	b	1143c <__gmpf_sub@@Base+0xb00>
   110e0:	mov	x15, x20
   110e4:	ldur	x20, [x29, #-40]
   110e8:	mov	x9, xzr
   110ec:	mov	w8, #0x1                   	// #1
   110f0:	cmp	x8, x3
   110f4:	b.ge	115a0 <__gmpf_sub@@Base+0xc64>  // b.tcont
   110f8:	add	x11, x24, x9
   110fc:	ldr	x11, [x11, #8]
   11100:	add	x12, x21, x9
   11104:	add	x8, x8, #0x1
   11108:	add	x9, x9, #0x8
   1110c:	sub	x13, x11, #0x1
   11110:	sub	x23, x23, #0x1
   11114:	str	x13, [x12, #8]
   11118:	cbz	x11, 110f0 <__gmpf_sub@@Base+0x7b4>
   1111c:	cmp	x10, x24
   11120:	b.eq	115a0 <__gmpf_sub@@Base+0xc64>  // b.none
   11124:	subs	x10, x3, x8
   11128:	b.le	115a0 <__gmpf_sub@@Base+0xc64>
   1112c:	cmp	x10, #0x4
   11130:	b.cc	111a8 <__gmpf_sub@@Base+0x86c>  // b.lo, b.ul, b.last
   11134:	add	x11, x21, x9
   11138:	add	x11, x11, #0x8
   1113c:	add	x12, x24, x3, lsl #3
   11140:	cmp	x11, x12
   11144:	b.cs	11164 <__gmpf_sub@@Base+0x828>  // b.hs, b.nlast
   11148:	add	x11, x22, x4
   1114c:	add	x12, x24, x9
   11150:	sub	x11, x11, x5
   11154:	add	x11, x1, x11, lsl #3
   11158:	add	x12, x12, #0x8
   1115c:	cmp	x11, x12
   11160:	b.hi	111a8 <__gmpf_sub@@Base+0x86c>  // b.pmore
   11164:	sub	x13, x3, x8
   11168:	add	x11, x21, x9
   1116c:	add	x12, x24, x9
   11170:	and	x14, x23, #0xfffffffffffffffc
   11174:	and	x9, x10, #0xfffffffffffffffc
   11178:	add	x11, x11, #0x18
   1117c:	add	x12, x12, #0x18
   11180:	add	x8, x14, x8
   11184:	and	x13, x13, #0xfffffffffffffffc
   11188:	ldp	q0, q1, [x12, #-16]
   1118c:	add	x12, x12, #0x20
   11190:	subs	x13, x13, #0x4
   11194:	stp	q0, q1, [x11, #-16]
   11198:	add	x11, x11, #0x20
   1119c:	b.ne	11188 <__gmpf_sub@@Base+0x84c>  // b.any
   111a0:	cmp	x10, x9
   111a4:	b.eq	115a0 <__gmpf_sub@@Base+0xc64>  // b.none
   111a8:	lsl	x9, x8, #3
   111ac:	ldr	x10, [x24, x9]
   111b0:	add	x8, x8, #0x1
   111b4:	cmp	x3, x8
   111b8:	str	x10, [x21, x9]
   111bc:	b.ne	111a8 <__gmpf_sub@@Base+0x86c>  // b.any
   111c0:	b	115a0 <__gmpf_sub@@Base+0xc64>
   111c4:	ldur	x15, [x29, #-64]
   111c8:	mov	x9, xzr
   111cc:	mov	x10, xzr
   111d0:	sub	x11, x22, #0x1
   111d4:	mov	w24, w27
   111d8:	cmp	x11, x10
   111dc:	str	xzr, [x1, x10, lsl #3]
   111e0:	b.eq	1144c <__gmpf_sub@@Base+0xb10>  // b.none
   111e4:	add	x8, x25, x10, lsl #3
   111e8:	ldr	x8, [x8, #8]
   111ec:	add	x10, x10, #0x1
   111f0:	sub	x9, x9, #0x8
   111f4:	cbz	x8, 111d8 <__gmpf_sub@@Base+0x89c>
   111f8:	sub	x10, x22, x10
   111fc:	sub	x25, x25, x9
   11200:	sub	x9, x1, x9
   11204:	neg	x8, x8
   11208:	subs	x2, x10, #0x1
   1120c:	str	x8, [x9]
   11210:	b.eq	11228 <__gmpf_sub@@Base+0x8ec>  // b.none
   11214:	add	x0, x9, #0x8
   11218:	mov	x19, x1
   1121c:	add	x1, x25, #0x8
   11220:	bl	c290 <__gmpn_com@plt>
   11224:	mov	x1, x19
   11228:	ldur	x15, [x29, #-64]
   1122c:	mov	x25, x20
   11230:	mov	x20, x28
   11234:	b	115c8 <__gmpf_sub@@Base+0xc8c>
   11238:	sub	x23, x9, x22
   1123c:	mov	x0, x1
   11240:	mov	x1, x24
   11244:	mov	x2, x23
   11248:	mov	x21, x3
   1124c:	stur	x4, [x29, #-96]
   11250:	mov	x28, x5
   11254:	bl	ca50 <__gmpn_copyi@plt>
   11258:	ldur	x9, [x29, #-72]
   1125c:	lsl	x8, x23, #3
   11260:	add	x1, x24, x8
   11264:	mov	x2, x26
   11268:	add	x0, x9, x8
   1126c:	mov	x3, x22
   11270:	stur	x21, [x29, #-80]
   11274:	stur	x23, [x29, #-104]
   11278:	sub	x21, x21, x23
   1127c:	bl	c2d0 <__gmpn_sub_n@plt>
   11280:	ldur	x13, [x29, #-96]
   11284:	ldp	x1, x15, [x29, #-72]
   11288:	mov	x9, x22
   1128c:	cbz	x0, 112cc <__gmpf_sub@@Base+0x990>
   11290:	ldur	x9, [x29, #-32]
   11294:	ldp	x10, x12, [x29, #-88]
   11298:	mov	x8, xzr
   1129c:	add	x9, x28, x9
   112a0:	sub	x9, x9, x13
   112a4:	add	x9, x10, x9, lsl #3
   112a8:	add	x10, x22, x8
   112ac:	cmp	x10, x21
   112b0:	b.ge	11540 <__gmpf_sub@@Base+0xc04>  // b.tcont
   112b4:	ldr	x10, [x9, x8, lsl #3]
   112b8:	add	x8, x8, #0x1
   112bc:	sub	x11, x10, #0x1
   112c0:	str	x11, [x25], #8
   112c4:	cbz	x10, 112a8 <__gmpf_sub@@Base+0x96c>
   112c8:	add	x9, x22, x8
   112cc:	cmp	x24, x1
   112d0:	b.eq	115b0 <__gmpf_sub@@Base+0xc74>  // b.none
   112d4:	ldur	x25, [x29, #-48]
   112d8:	ldur	x14, [x29, #-80]
   112dc:	cmp	x9, x21
   112e0:	b.ge	115bc <__gmpf_sub@@Base+0xc80>  // b.tcont
   112e4:	add	x8, x22, x13
   112e8:	sub	x8, x8, x9
   112ec:	sub	x8, x8, x28
   112f0:	cmp	x8, #0x4
   112f4:	b.cc	11334 <__gmpf_sub@@Base+0x9f8>  // b.lo, b.ul, b.last
   112f8:	ldur	x10, [x29, #-104]
   112fc:	add	x11, x9, x10
   11300:	lsl	x10, x14, #3
   11304:	add	x11, x1, x11, lsl #3
   11308:	add	x12, x24, x10
   1130c:	cmp	x11, x12
   11310:	b.cs	11628 <__gmpf_sub@@Base+0xcec>  // b.hs, b.nlast
   11314:	add	x11, x9, x14
   11318:	add	x11, x11, x28
   1131c:	sub	x11, x11, x22
   11320:	sub	x11, x11, x13
   11324:	add	x10, x1, x10
   11328:	add	x11, x24, x11, lsl #3
   1132c:	cmp	x10, x11
   11330:	b.ls	11628 <__gmpf_sub@@Base+0xcec>  // b.plast
   11334:	ldur	w24, [x29, #-52]
   11338:	mov	x10, x9
   1133c:	sub	x8, x19, x10
   11340:	lsl	x10, x10, #3
   11344:	add	x9, x20, x10
   11348:	add	x10, x27, x10
   1134c:	ldr	x11, [x10], #8
   11350:	subs	x8, x8, #0x1
   11354:	str	x11, [x9], #8
   11358:	b.ne	1134c <__gmpf_sub@@Base+0xa10>  // b.any
   1135c:	ldur	x20, [x29, #-40]
   11360:	mov	x22, x14
   11364:	b	115c8 <__gmpf_sub@@Base+0xc8c>
   11368:	mvn	x8, x24
   1136c:	mov	x9, xzr
   11370:	mov	x10, xzr
   11374:	add	x11, x8, x22
   11378:	cmp	x11, x10
   1137c:	str	xzr, [x1, x10, lsl #3]
   11380:	b.eq	114b0 <__gmpf_sub@@Base+0xb74>  // b.none
   11384:	add	x8, x25, x10, lsl #3
   11388:	ldr	x8, [x8, #8]
   1138c:	add	x10, x10, #0x1
   11390:	sub	x9, x9, #0x8
   11394:	cbz	x8, 11378 <__gmpf_sub@@Base+0xa3c>
   11398:	sub	x11, x19, x10
   1139c:	sub	x10, x25, x9
   113a0:	sub	x9, x1, x9
   113a4:	neg	x8, x8
   113a8:	subs	x2, x11, #0x1
   113ac:	str	x8, [x9]
   113b0:	b.eq	113c8 <__gmpf_sub@@Base+0xa8c>  // b.none
   113b4:	add	x0, x9, #0x8
   113b8:	mov	x21, x1
   113bc:	add	x1, x10, #0x8
   113c0:	bl	c290 <__gmpn_com@plt>
   113c4:	mov	x1, x21
   113c8:	mov	w4, #0x1                   	// #1
   113cc:	lsl	x8, x19, #3
   113d0:	mov	x19, x1
   113d4:	add	x0, x1, x8
   113d8:	add	x2, x25, x8
   113dc:	mov	x1, x23
   113e0:	mov	x3, x24
   113e4:	bl	c760 <__gmpn_sub_nc@plt>
   113e8:	ldur	x15, [x29, #-64]
   113ec:	mov	x25, x20
   113f0:	mov	w24, w27
   113f4:	mov	x1, x19
   113f8:	cbz	x0, 11450 <__gmpf_sub@@Base+0xb14>
   113fc:	mov	x20, x28
   11400:	b	115c8 <__gmpf_sub@@Base+0xc8c>
   11404:	mov	x0, x1
   11408:	mov	x1, x24
   1140c:	mov	x2, x25
   11410:	mov	x19, x3
   11414:	bl	ca50 <__gmpn_copyi@plt>
   11418:	ldur	x9, [x29, #-72]
   1141c:	lsl	x8, x25, #3
   11420:	add	x1, x24, x8
   11424:	mov	x2, x26
   11428:	add	x0, x9, x8
   1142c:	mov	x3, x22
   11430:	bl	c2d0 <__gmpn_sub_n@plt>
   11434:	ldur	x1, [x29, #-72]
   11438:	mov	x22, x19
   1143c:	ldur	w24, [x29, #-52]
   11440:	mov	x15, x20
   11444:	ldp	x25, x20, [x29, #-48]
   11448:	b	115c8 <__gmpf_sub@@Base+0xc8c>
   1144c:	mov	x25, x20
   11450:	mov	w8, #0x1                   	// #1
   11454:	mov	x20, x28
   11458:	str	x8, [x1, x22, lsl #3]
   1145c:	add	x22, x22, #0x1
   11460:	add	x20, x28, #0x1
   11464:	b	116ec <__gmpf_sub@@Base+0xdb0>
   11468:	mov	x22, x25
   1146c:	ldur	x25, [x29, #-48]
   11470:	b	115c0 <__gmpf_sub@@Base+0xc84>
   11474:	add	x8, x22, x4
   11478:	sub	x10, x8, x3
   1147c:	sub	x10, x10, x5
   11480:	add	x10, x1, x10, lsl #3
   11484:	add	x10, x10, #0x8
   11488:	add	x11, x24, x3, lsl #3
   1148c:	cmp	x10, x11
   11490:	b.cs	1154c <__gmpf_sub@@Base+0xc10>  // b.hs, b.nlast
   11494:	sub	x8, x8, x5
   11498:	add	x8, x1, x8, lsl #3
   1149c:	add	x10, x24, #0x8
   114a0:	cmp	x8, x10
   114a4:	b.ls	1154c <__gmpf_sub@@Base+0xc10>  // b.plast
   114a8:	mov	w8, #0x1                   	// #1
   114ac:	b	11588 <__gmpf_sub@@Base+0xc4c>
   114b0:	mov	x4, xzr
   114b4:	b	113cc <__gmpf_sub@@Base+0xa90>
   114b8:	ldur	x15, [x29, #-64]
   114bc:	mov	x23, xzr
   114c0:	cmp	x19, x24
   114c4:	b.eq	11528 <__gmpf_sub@@Base+0xbec>  // b.none
   114c8:	subs	x9, x3, x23
   114cc:	b.le	11528 <__gmpf_sub@@Base+0xbec>
   114d0:	cmp	x9, #0x4
   114d4:	b.cc	1150c <__gmpf_sub@@Base+0xbd0>  // b.lo, b.ul, b.last
   114d8:	add	x8, x22, x4
   114dc:	sub	x10, x8, x3
   114e0:	sub	x10, x10, x5
   114e4:	add	x10, x23, x10
   114e8:	add	x10, x1, x10, lsl #3
   114ec:	add	x11, x24, x3, lsl #3
   114f0:	cmp	x10, x11
   114f4:	b.cs	115f0 <__gmpf_sub@@Base+0xcb4>  // b.hs, b.nlast
   114f8:	sub	x8, x8, x5
   114fc:	add	x8, x1, x8, lsl #3
   11500:	add	x10, x24, x23, lsl #3
   11504:	cmp	x8, x10
   11508:	b.ls	115f0 <__gmpf_sub@@Base+0xcb4>  // b.plast
   1150c:	mov	x8, x23
   11510:	add	x9, x21, x8, lsl #3
   11514:	ldr	x10, [x24, x8, lsl #3]
   11518:	add	x8, x8, #0x1
   1151c:	cmp	x3, x8
   11520:	str	x10, [x9], #8
   11524:	b.ne	11514 <__gmpf_sub@@Base+0xbd8>  // b.any
   11528:	ldr	x8, [x19]
   1152c:	sub	x9, x8, #0x1
   11530:	str	x9, [x19], #8
   11534:	cbz	x8, 11528 <__gmpf_sub@@Base+0xbec>
   11538:	ldur	x22, [x29, #-96]
   1153c:	b	115a8 <__gmpf_sub@@Base+0xc6c>
   11540:	ldur	x25, [x29, #-48]
   11544:	mov	x22, x12
   11548:	b	115c0 <__gmpf_sub@@Base+0xc84>
   1154c:	and	x10, x9, #0xfffffffffffffffc
   11550:	mov	x11, xzr
   11554:	orr	x8, x10, #0x1
   11558:	and	x12, x23, #0xfffffffffffffffc
   1155c:	add	x13, x24, x11
   11560:	ldur	q0, [x13, #8]
   11564:	ldur	q1, [x13, #24]
   11568:	add	x13, x21, x11
   1156c:	subs	x12, x12, #0x4
   11570:	add	x11, x11, #0x20
   11574:	stur	q0, [x13, #8]
   11578:	stur	q1, [x13, #24]
   1157c:	b.ne	1155c <__gmpf_sub@@Base+0xc20>  // b.any
   11580:	cmp	x9, x10
   11584:	b.eq	115a0 <__gmpf_sub@@Base+0xc64>  // b.none
   11588:	lsl	x9, x8, #3
   1158c:	ldr	x10, [x24, x9]
   11590:	add	x8, x8, #0x1
   11594:	cmp	x3, x8
   11598:	str	x10, [x21, x9]
   1159c:	b.ne	11588 <__gmpf_sub@@Base+0xc4c>  // b.any
   115a0:	mov	x22, x25
   115a4:	ldur	x25, [x29, #-48]
   115a8:	ldur	w24, [x29, #-52]
   115ac:	b	115c8 <__gmpf_sub@@Base+0xc8c>
   115b0:	ldur	x22, [x29, #-80]
   115b4:	ldur	x25, [x29, #-48]
   115b8:	b	115c0 <__gmpf_sub@@Base+0xc84>
   115bc:	mov	x22, x14
   115c0:	ldur	w24, [x29, #-52]
   115c4:	ldur	x20, [x29, #-40]
   115c8:	cbz	x22, 116ec <__gmpf_sub@@Base+0xdb0>
   115cc:	sub	x8, x20, x22
   115d0:	add	x9, x1, x22, lsl #3
   115d4:	ldur	x9, [x9, #-8]
   115d8:	cbnz	x9, 116ec <__gmpf_sub@@Base+0xdb0>
   115dc:	sub	x22, x22, #0x1
   115e0:	sub	x20, x20, #0x1
   115e4:	cbnz	x22, 115d0 <__gmpf_sub@@Base+0xc94>
   115e8:	mov	x20, x8
   115ec:	b	116ec <__gmpf_sub@@Base+0xdb0>
   115f0:	and	x10, x9, #0xfffffffffffffffc
   115f4:	add	x8, x23, x10
   115f8:	lsl	x11, x23, #3
   115fc:	mov	x12, x10
   11600:	add	x13, x24, x11
   11604:	ldp	q0, q1, [x13]
   11608:	add	x13, x21, x11
   1160c:	subs	x12, x12, #0x4
   11610:	add	x11, x11, #0x20
   11614:	stp	q0, q1, [x13]
   11618:	b.ne	11600 <__gmpf_sub@@Base+0xcc4>  // b.any
   1161c:	cmp	x9, x10
   11620:	b.eq	11528 <__gmpf_sub@@Base+0xbec>  // b.none
   11624:	b	11510 <__gmpf_sub@@Base+0xbd4>
   11628:	and	x11, x8, #0xfffffffffffffffc
   1162c:	sub	x12, x19, x9
   11630:	add	x10, x9, x11
   11634:	and	x12, x12, #0xfffffffffffffffc
   11638:	lsl	x9, x9, #3
   1163c:	add	x13, x27, x9
   11640:	ldp	q0, q1, [x13]
   11644:	add	x13, x20, x9
   11648:	subs	x12, x12, #0x4
   1164c:	add	x9, x9, #0x20
   11650:	stp	q0, q1, [x13]
   11654:	b.ne	1163c <__gmpf_sub@@Base+0xd00>  // b.any
   11658:	ldur	w24, [x29, #-52]
   1165c:	cmp	x8, x11
   11660:	b.ne	1133c <__gmpf_sub@@Base+0xa00>  // b.any
   11664:	b	1135c <__gmpf_sub@@Base+0xa20>
   11668:	stur	x0, [x29, #-48]
   1166c:	sub	x0, x29, #0x18
   11670:	mov	x20, x18
   11674:	mov	x23, x4
   11678:	mov	x27, x5
   1167c:	mov	x28, x14
   11680:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11684:	mov	x1, x0
   11688:	ldur	x0, [x29, #-48]
   1168c:	mov	x14, x28
   11690:	mov	x5, x27
   11694:	mov	x4, x23
   11698:	mov	x18, x20
   1169c:	cbnz	x22, 10a60 <__gmpf_sub@@Base+0x124>
   116a0:	b	10ad0 <__gmpf_sub@@Base+0x194>
   116a4:	sub	x0, x29, #0x18
   116a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   116ac:	mov	x1, x0
   116b0:	cbnz	x22, 11010 <__gmpf_sub@@Base+0x6d4>
   116b4:	mov	x0, x1
   116b8:	mov	x19, x1
   116bc:	mov	x1, x23
   116c0:	mov	x2, x24
   116c4:	bl	ca50 <__gmpn_copyi@plt>
   116c8:	ldur	x15, [x29, #-64]
   116cc:	mov	w8, #0x1                   	// #1
   116d0:	add	x28, x28, #0x1
   116d4:	mov	x1, x19
   116d8:	add	x22, x24, #0x1
   116dc:	str	x8, [x19, x24, lsl #3]
   116e0:	mov	x25, x20
   116e4:	mov	x20, x28
   116e8:	mov	w24, w27
   116ec:	mov	x0, x15
   116f0:	mov	x2, x22
   116f4:	bl	ca50 <__gmpn_copyi@plt>
   116f8:	mov	x27, x22
   116fc:	ldur	x0, [x29, #-24]
   11700:	cbz	x0, 10cc8 <__gmpf_sub@@Base+0x38c>
   11704:	b	10af0 <__gmpf_sub@@Base+0x1b4>

0000000000011708 <__gmpf_sub_ui@@Base>:
   11708:	sub	sp, sp, #0x30
   1170c:	stp	x29, x30, [sp, #32]
   11710:	add	x29, sp, #0x20
   11714:	cbz	x2, 11740 <__gmpf_sub_ui@@Base+0x38>
   11718:	str	x2, [sp]
   1171c:	mov	w8, #0x1                   	// #1
   11720:	mov	x9, sp
   11724:	add	x2, sp, #0x8
   11728:	str	w8, [sp, #12]
   1172c:	stp	x8, x9, [sp, #16]
   11730:	bl	ce00 <__gmpf_sub@plt>
   11734:	ldp	x29, x30, [sp, #32]
   11738:	add	sp, sp, #0x30
   1173c:	ret
   11740:	bl	c150 <__gmpf_set@plt>
   11744:	ldp	x29, x30, [sp, #32]
   11748:	add	sp, sp, #0x30
   1174c:	ret

0000000000011750 <__gmpf_ui_sub@@Base>:
   11750:	sub	sp, sp, #0x30
   11754:	stp	x29, x30, [sp, #32]
   11758:	add	x29, sp, #0x20
   1175c:	cbz	x1, 11788 <__gmpf_ui_sub@@Base+0x38>
   11760:	str	x1, [sp]
   11764:	mov	w8, #0x1                   	// #1
   11768:	mov	x9, sp
   1176c:	add	x1, sp, #0x8
   11770:	str	w8, [sp, #12]
   11774:	stp	x8, x9, [sp, #16]
   11778:	bl	ce00 <__gmpf_sub@plt>
   1177c:	ldp	x29, x30, [sp, #32]
   11780:	add	sp, sp, #0x30
   11784:	ret
   11788:	mov	x1, x2
   1178c:	bl	cbf0 <__gmpf_neg@plt>
   11790:	ldp	x29, x30, [sp, #32]
   11794:	add	sp, sp, #0x30
   11798:	ret

000000000001179c <__gmpf_mul@@Base>:
   1179c:	stp	x29, x30, [sp, #-96]!
   117a0:	stp	x28, x27, [sp, #16]
   117a4:	stp	x26, x25, [sp, #32]
   117a8:	stp	x24, x23, [sp, #48]
   117ac:	stp	x22, x21, [sp, #64]
   117b0:	stp	x20, x19, [sp, #80]
   117b4:	mov	x29, sp
   117b8:	sub	sp, sp, #0x10
   117bc:	ldrsw	x27, [x0]
   117c0:	ldrsw	x8, [x1, #4]
   117c4:	mov	x20, x1
   117c8:	mov	x19, x0
   117cc:	mov	x21, x2
   117d0:	cmp	x1, x2
   117d4:	b.eq	11870 <__gmpf_mul@@Base+0xd4>  // b.none
   117d8:	ldrsw	x9, [x21, #4]
   117dc:	ldr	x10, [x20, #16]
   117e0:	cmp	x8, #0x0
   117e4:	ldr	x11, [x21, #16]
   117e8:	cneg	x12, x8, mi  // mi = first
   117ec:	cmp	x9, #0x0
   117f0:	cneg	x13, x9, mi  // mi = first
   117f4:	subs	x14, x12, x27
   117f8:	add	x14, x10, x14, lsl #3
   117fc:	csel	x23, x27, x12, gt
   11800:	csel	x24, x14, x10, gt
   11804:	subs	x10, x13, x27
   11808:	add	x10, x11, x10, lsl #3
   1180c:	csel	x26, x10, x11, gt
   11810:	csel	x25, x27, x13, gt
   11814:	cbz	x23, 118dc <__gmpf_mul@@Base+0x140>
   11818:	cbz	x25, 118dc <__gmpf_mul@@Base+0x140>
   1181c:	eor	w8, w9, w8
   11820:	add	x28, x25, x23
   11824:	sxtw	x8, w8
   11828:	stp	x8, xzr, [x29, #-16]
   1182c:	lsl	x1, x28, #3
   11830:	mov	w8, #0x7f00                	// #32512
   11834:	cmp	x1, x8
   11838:	b.hi	118e8 <__gmpf_mul@@Base+0x14c>  // b.pmore
   1183c:	add	x9, x1, #0xf
   11840:	mov	x8, sp
   11844:	and	x9, x9, #0xfffffffffffffff0
   11848:	sub	x22, x8, x9
   1184c:	mov	sp, x22
   11850:	mov	x0, x22
   11854:	cmp	x23, x25
   11858:	b.ge	11900 <__gmpf_mul@@Base+0x164>  // b.tcont
   1185c:	mov	x1, x26
   11860:	mov	x2, x25
   11864:	mov	x3, x24
   11868:	mov	x4, x23
   1186c:	b	11910 <__gmpf_mul@@Base+0x174>
   11870:	ldr	x9, [x20, #16]
   11874:	cmp	x8, #0x0
   11878:	cneg	x8, x8, mi  // mi = first
   1187c:	subs	x10, x8, x27
   11880:	add	x10, x9, x10, lsl #3
   11884:	csel	x23, x27, x8, gt
   11888:	csel	x24, x10, x9, gt
   1188c:	cbz	x23, 118dc <__gmpf_mul@@Base+0x140>
   11890:	lsl	x1, x23, #4
   11894:	mov	w8, #0x7f00                	// #32512
   11898:	cmp	x1, x8
   1189c:	lsl	x28, x23, #1
   118a0:	stur	xzr, [x29, #-8]
   118a4:	b.hi	11998 <__gmpf_mul@@Base+0x1fc>  // b.pmore
   118a8:	add	x9, x1, #0xf
   118ac:	mov	x8, sp
   118b0:	and	x9, x9, #0xfffffffffffffff0
   118b4:	sub	x22, x8, x9
   118b8:	mov	sp, x22
   118bc:	mov	x0, x22
   118c0:	mov	x1, x24
   118c4:	mov	x2, x23
   118c8:	bl	c8e0 <__gmpn_sqr@plt>
   118cc:	add	x8, x22, x28, lsl #3
   118d0:	ldur	x0, [x8, #-8]
   118d4:	mov	x25, xzr
   118d8:	b	11918 <__gmpf_mul@@Base+0x17c>
   118dc:	str	wzr, [x19, #4]
   118e0:	str	xzr, [x19, #8]
   118e4:	b	11970 <__gmpf_mul@@Base+0x1d4>
   118e8:	sub	x0, x29, #0x8
   118ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   118f0:	mov	x22, x0
   118f4:	mov	x0, x22
   118f8:	cmp	x23, x25
   118fc:	b.lt	1185c <__gmpf_mul@@Base+0xc0>  // b.tstop
   11900:	mov	x1, x24
   11904:	mov	x2, x23
   11908:	mov	x3, x26
   1190c:	mov	x4, x25
   11910:	bl	ccd0 <__gmpn_mul@plt>
   11914:	ldur	x25, [x29, #-16]
   11918:	cmp	x0, #0x0
   1191c:	cset	w24, eq  // eq = none
   11920:	add	x8, x27, #0x1
   11924:	ldr	x0, [x19, #16]
   11928:	sub	x9, x28, x24
   1192c:	subs	x8, x9, x8
   11930:	add	x8, x22, x8, lsl #3
   11934:	csinc	x23, x9, x27, le
   11938:	csel	x1, x8, x22, gt
   1193c:	mov	x2, x23
   11940:	bl	ca50 <__gmpn_copyi@plt>
   11944:	ldr	x8, [x20, #8]
   11948:	ldr	x9, [x21, #8]
   1194c:	neg	w10, w23
   11950:	cmp	x25, #0x0
   11954:	sub	x8, x8, x24
   11958:	csel	x10, x23, x10, ge  // ge = tcont
   1195c:	add	x8, x8, x9
   11960:	str	x8, [x19, #8]
   11964:	str	w10, [x19, #4]
   11968:	ldur	x0, [x29, #-8]
   1196c:	cbnz	x0, 11990 <__gmpf_mul@@Base+0x1f4>
   11970:	mov	sp, x29
   11974:	ldp	x20, x19, [sp, #80]
   11978:	ldp	x22, x21, [sp, #64]
   1197c:	ldp	x24, x23, [sp, #48]
   11980:	ldp	x26, x25, [sp, #32]
   11984:	ldp	x28, x27, [sp, #16]
   11988:	ldp	x29, x30, [sp], #96
   1198c:	ret
   11990:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11994:	b	11970 <__gmpf_mul@@Base+0x1d4>
   11998:	sub	x0, x29, #0x8
   1199c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   119a0:	mov	x22, x0
   119a4:	b	118bc <__gmpf_mul@@Base+0x120>

00000000000119a8 <__gmpf_mul_ui@@Base>:
   119a8:	stp	x29, x30, [sp, #-64]!
   119ac:	stp	x20, x19, [sp, #48]
   119b0:	mov	x19, x0
   119b4:	str	x23, [sp, #16]
   119b8:	stp	x22, x21, [sp, #32]
   119bc:	mov	x29, sp
   119c0:	cbz	x2, 11a90 <__gmpf_mul_ui@@Base+0xe8>
   119c4:	ldr	w8, [x1, #4]
   119c8:	mov	x20, x1
   119cc:	cbz	w8, 11a90 <__gmpf_mul_ui@@Base+0xe8>
   119d0:	ldrsw	x21, [x19]
   119d4:	sxtw	x23, w8
   119d8:	cmp	w23, #0x0
   119dc:	ldr	x1, [x20, #16]
   119e0:	cneg	x9, x23, lt  // lt = tstop
   119e4:	sub	x8, x9, x21
   119e8:	mov	x3, x2
   119ec:	cmp	x8, #0x1
   119f0:	b.lt	11a3c <__gmpf_mul_ui@@Base+0x94>  // b.tstop
   119f4:	add	x9, x1, x8, lsl #3
   119f8:	ldur	x9, [x9, #-8]
   119fc:	sub	x10, x1, #0x10
   11a00:	mov	x11, x8
   11a04:	umulh	x4, x9, x3
   11a08:	sub	x12, x11, #0x1
   11a0c:	cmp	x12, #0x1
   11a10:	b.lt	11a34 <__gmpf_mul_ui@@Base+0x8c>  // b.tstop
   11a14:	mul	x13, x9, x3
   11a18:	ldr	x9, [x10, x11, lsl #3]
   11a1c:	umulh	x11, x9, x3
   11a20:	adds	x11, x11, x13
   11a24:	cinc	x4, x4, cs  // cs = hs, nlast
   11a28:	cmn	x11, #0x1
   11a2c:	mov	x11, x12
   11a30:	b.eq	11a08 <__gmpf_mul_ui@@Base+0x60>  // b.none
   11a34:	add	x1, x1, x8, lsl #3
   11a38:	b	11a44 <__gmpf_mul_ui@@Base+0x9c>
   11a3c:	mov	x4, xzr
   11a40:	mov	x21, x9
   11a44:	ldr	x22, [x19, #16]
   11a48:	mov	x2, x21
   11a4c:	mov	x0, x22
   11a50:	bl	d240 <__gmpn_mul_1c@plt>
   11a54:	str	x0, [x22, x21, lsl #3]
   11a58:	ldr	x8, [x20, #8]
   11a5c:	cmp	x0, #0x0
   11a60:	cinc	x9, x21, ne  // ne = any
   11a64:	neg	w10, w9
   11a68:	cinc	x8, x8, ne  // ne = any
   11a6c:	cmp	w23, #0x0
   11a70:	str	x8, [x19, #8]
   11a74:	csel	x8, x9, x10, ge  // ge = tcont
   11a78:	str	w8, [x19, #4]
   11a7c:	ldp	x20, x19, [sp, #48]
   11a80:	ldp	x22, x21, [sp, #32]
   11a84:	ldr	x23, [sp, #16]
   11a88:	ldp	x29, x30, [sp], #64
   11a8c:	ret
   11a90:	str	wzr, [x19, #4]
   11a94:	str	xzr, [x19, #8]
   11a98:	b	11a7c <__gmpf_mul_ui@@Base+0xd4>

0000000000011a9c <__gmpf_div@@Base>:
   11a9c:	stp	x29, x30, [sp, #-96]!
   11aa0:	stp	x28, x27, [sp, #16]
   11aa4:	stp	x26, x25, [sp, #32]
   11aa8:	stp	x24, x23, [sp, #48]
   11aac:	stp	x22, x21, [sp, #64]
   11ab0:	stp	x20, x19, [sp, #80]
   11ab4:	mov	x29, sp
   11ab8:	sub	sp, sp, #0x30
   11abc:	ldrsw	x27, [x2, #4]
   11ac0:	cbz	w27, 11d00 <__gmpf_div@@Base+0x264>
   11ac4:	ldrsw	x28, [x1, #4]
   11ac8:	mov	x19, x0
   11acc:	cbz	w28, 11b5c <__gmpf_div@@Base+0xc0>
   11ad0:	ldr	x8, [x2, #8]
   11ad4:	cmp	x28, #0x0
   11ad8:	ldrsw	x26, [x19]
   11adc:	cneg	x11, x28, mi  // mi = first
   11ae0:	cmp	x27, #0x0
   11ae4:	cneg	x21, x27, mi  // mi = first
   11ae8:	ldr	x20, [x19, #16]
   11aec:	ldp	x23, x10, [x1, #8]
   11af0:	stp	x8, xzr, [x29, #-16]
   11af4:	sub	x8, x21, x11
   11af8:	ldr	x24, [x2, #16]
   11afc:	add	x8, x8, x26
   11b00:	neg	x9, x8
   11b04:	and	x9, x9, x8, asr #63
   11b08:	cmp	x8, #0x0
   11b0c:	add	x12, x10, x9, lsl #3
   11b10:	sub	x22, x11, x9
   11b14:	b.gt	11b68 <__gmpf_div@@Base+0xcc>
   11b18:	cmp	x20, x10
   11b1c:	b.eq	11b68 <__gmpf_div@@Base+0xcc>  // b.none
   11b20:	lsl	x8, x22, #3
   11b24:	add	x1, x8, #0x8
   11b28:	mov	w8, #0x7f00                	// #32512
   11b2c:	cmp	x1, x8
   11b30:	b.hi	11ca4 <__gmpf_div@@Base+0x208>  // b.pmore
   11b34:	add	x9, x1, #0xf
   11b38:	mov	x8, sp
   11b3c:	and	x9, x9, #0xfffffffffffffff0
   11b40:	sub	x25, x8, x9
   11b44:	mov	sp, x25
   11b48:	eor	w27, w27, w28
   11b4c:	cmp	x20, x24
   11b50:	add	x28, x26, #0x1
   11b54:	b.ne	11c28 <__gmpf_div@@Base+0x18c>  // b.any
   11b58:	b	11bf0 <__gmpf_div@@Base+0x154>
   11b5c:	str	wzr, [x19, #4]
   11b60:	str	xzr, [x19, #8]
   11b64:	b	11c7c <__gmpf_div@@Base+0x1e0>
   11b68:	add	x10, x21, x26
   11b6c:	stp	x10, x23, [x29, #-32]
   11b70:	lsl	x10, x10, #3
   11b74:	add	x1, x10, #0x8
   11b78:	mov	w10, #0x7f00                	// #32512
   11b7c:	cmp	x1, x10
   11b80:	add	x23, x9, x8
   11b84:	b.hi	11cd4 <__gmpf_div@@Base+0x238>  // b.pmore
   11b88:	add	x9, x1, #0xf
   11b8c:	mov	x8, sp
   11b90:	and	x9, x9, #0xfffffffffffffff0
   11b94:	sub	x25, x8, x9
   11b98:	mov	sp, x25
   11b9c:	cbz	x23, 11bc8 <__gmpf_div@@Base+0x12c>
   11ba0:	lsl	x2, x23, #3
   11ba4:	mov	x0, x25
   11ba8:	mov	w1, wzr
   11bac:	stur	x26, [x29, #-40]
   11bb0:	mov	x26, x22
   11bb4:	mov	x22, x12
   11bb8:	bl	c5f0 <memset@plt>
   11bbc:	mov	x12, x22
   11bc0:	mov	x22, x26
   11bc4:	ldur	x26, [x29, #-40]
   11bc8:	add	x0, x25, x23, lsl #3
   11bcc:	mov	x1, x12
   11bd0:	mov	x2, x22
   11bd4:	bl	ca50 <__gmpn_copyi@plt>
   11bd8:	ldp	x22, x23, [x29, #-32]
   11bdc:	mov	x12, x25
   11be0:	eor	w27, w27, w28
   11be4:	cmp	x20, x24
   11be8:	add	x28, x26, #0x1
   11bec:	b.ne	11c28 <__gmpf_div@@Base+0x18c>  // b.any
   11bf0:	cmp	x21, #0xfe0
   11bf4:	lsl	x1, x21, #3
   11bf8:	stur	x12, [x29, #-24]
   11bfc:	b.hi	11cf0 <__gmpf_div@@Base+0x254>  // b.pmore
   11c00:	add	x9, x1, #0xf
   11c04:	mov	x8, sp
   11c08:	and	x9, x9, #0xfffffffffffffff0
   11c0c:	sub	x24, x8, x9
   11c10:	mov	sp, x24
   11c14:	mov	x0, x24
   11c18:	mov	x1, x20
   11c1c:	mov	x2, x21
   11c20:	bl	ca50 <__gmpn_copyi@plt>
   11c24:	ldur	x12, [x29, #-24]
   11c28:	mov	x0, x20
   11c2c:	mov	x1, x12
   11c30:	mov	x2, x22
   11c34:	mov	x3, x24
   11c38:	mov	x4, x21
   11c3c:	mov	x5, x25
   11c40:	bl	c320 <__gmpn_div_q@plt>
   11c44:	ldr	x8, [x20, x26, lsl #3]
   11c48:	ldp	x9, x0, [x29, #-16]
   11c4c:	cmp	x8, #0x0
   11c50:	cset	w8, eq  // eq = none
   11c54:	sub	x9, x23, x9
   11c58:	sub	x10, x28, x8
   11c5c:	cmp	w27, #0x0
   11c60:	sub	x8, x9, x8
   11c64:	neg	w9, w10
   11c68:	add	x8, x8, #0x1
   11c6c:	csel	x9, x10, x9, ge  // ge = tcont
   11c70:	str	w9, [x19, #4]
   11c74:	str	x8, [x19, #8]
   11c78:	cbnz	x0, 11c9c <__gmpf_div@@Base+0x200>
   11c7c:	mov	sp, x29
   11c80:	ldp	x20, x19, [sp, #80]
   11c84:	ldp	x22, x21, [sp, #64]
   11c88:	ldp	x24, x23, [sp, #48]
   11c8c:	ldp	x26, x25, [sp, #32]
   11c90:	ldp	x28, x27, [sp, #16]
   11c94:	ldp	x29, x30, [sp], #96
   11c98:	ret
   11c9c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11ca0:	b	11c7c <__gmpf_div@@Base+0x1e0>
   11ca4:	sub	x0, x29, #0x8
   11ca8:	mov	x25, x22
   11cac:	mov	x22, x12
   11cb0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11cb4:	mov	x12, x22
   11cb8:	mov	x22, x25
   11cbc:	mov	x25, x0
   11cc0:	eor	w27, w27, w28
   11cc4:	cmp	x20, x24
   11cc8:	add	x28, x26, #0x1
   11ccc:	b.ne	11c28 <__gmpf_div@@Base+0x18c>  // b.any
   11cd0:	b	11bf0 <__gmpf_div@@Base+0x154>
   11cd4:	sub	x0, x29, #0x8
   11cd8:	mov	x25, x12
   11cdc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11ce0:	mov	x12, x25
   11ce4:	mov	x25, x0
   11ce8:	cbnz	x23, 11ba0 <__gmpf_div@@Base+0x104>
   11cec:	b	11bc8 <__gmpf_div@@Base+0x12c>
   11cf0:	sub	x0, x29, #0x8
   11cf4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11cf8:	mov	x24, x0
   11cfc:	b	11c14 <__gmpf_div@@Base+0x178>
   11d00:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000011d04 <__gmpf_div_ui@@Base>:
   11d04:	stp	x29, x30, [sp, #-96]!
   11d08:	stp	x28, x27, [sp, #16]
   11d0c:	stp	x26, x25, [sp, #32]
   11d10:	stp	x24, x23, [sp, #48]
   11d14:	stp	x22, x21, [sp, #64]
   11d18:	stp	x20, x19, [sp, #80]
   11d1c:	mov	x29, sp
   11d20:	sub	sp, sp, #0x10
   11d24:	cbz	x2, 11e5c <__gmpf_div_ui@@Base+0x158>
   11d28:	ldrsw	x27, [x1, #4]
   11d2c:	mov	x20, x1
   11d30:	mov	x19, x0
   11d34:	cbz	w27, 11d98 <__gmpf_div_ui@@Base+0x94>
   11d38:	ldrsw	x28, [x19]
   11d3c:	stur	xzr, [x29, #-8]
   11d40:	ldr	x23, [x19, #16]
   11d44:	ldr	x24, [x20, #16]
   11d48:	lsl	x8, x28, #3
   11d4c:	cmp	w27, #0x0
   11d50:	add	x1, x8, #0x10
   11d54:	mov	w8, #0x7f00                	// #32512
   11d58:	mov	x21, x2
   11d5c:	cneg	x25, x27, lt  // lt = tstop
   11d60:	cmp	x1, x8
   11d64:	add	x22, x28, #0x1
   11d68:	b.hi	11da4 <__gmpf_div_ui@@Base+0xa0>  // b.pmore
   11d6c:	add	x9, x1, #0xf
   11d70:	mov	x8, sp
   11d74:	and	x9, x9, #0xfffffffffffffff0
   11d78:	sub	x26, x8, x9
   11d7c:	mov	sp, x26
   11d80:	subs	x8, x25, x22
   11d84:	b.le	11db8 <__gmpf_div_ui@@Base+0xb4>
   11d88:	add	x24, x24, x8, lsl #3
   11d8c:	mov	x25, x22
   11d90:	mov	x0, x26
   11d94:	b	11ddc <__gmpf_div_ui@@Base+0xd8>
   11d98:	str	wzr, [x19, #4]
   11d9c:	str	xzr, [x19, #8]
   11da0:	b	11e34 <__gmpf_div_ui@@Base+0x130>
   11da4:	sub	x0, x29, #0x8
   11da8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11dac:	mov	x26, x0
   11db0:	subs	x8, x25, x22
   11db4:	b.gt	11d88 <__gmpf_div_ui@@Base+0x84>
   11db8:	stur	x23, [x29, #-16]
   11dbc:	subs	x23, x22, x25
   11dc0:	b.eq	11dd4 <__gmpf_div_ui@@Base+0xd0>  // b.none
   11dc4:	lsl	x2, x23, #3
   11dc8:	mov	x0, x26
   11dcc:	mov	w1, wzr
   11dd0:	bl	c5f0 <memset@plt>
   11dd4:	add	x0, x26, x23, lsl #3
   11dd8:	ldur	x23, [x29, #-16]
   11ddc:	mov	x1, x24
   11de0:	mov	x2, x25
   11de4:	bl	ca50 <__gmpn_copyi@plt>
   11de8:	mov	x0, x23
   11dec:	mov	x1, xzr
   11df0:	mov	x2, x26
   11df4:	mov	x3, x22
   11df8:	mov	x4, x21
   11dfc:	bl	cd00 <__gmpn_divrem_1@plt>
   11e00:	ldr	x8, [x23, x28, lsl #3]
   11e04:	ldr	x9, [x20, #8]
   11e08:	cmp	x8, #0x0
   11e0c:	cset	w8, eq  // eq = none
   11e10:	sub	x10, x22, x8
   11e14:	cmp	w27, #0x0
   11e18:	sub	x8, x9, x8
   11e1c:	neg	w9, w10
   11e20:	csel	x9, x10, x9, ge  // ge = tcont
   11e24:	str	w9, [x19, #4]
   11e28:	str	x8, [x19, #8]
   11e2c:	ldur	x0, [x29, #-8]
   11e30:	cbnz	x0, 11e54 <__gmpf_div_ui@@Base+0x150>
   11e34:	mov	sp, x29
   11e38:	ldp	x20, x19, [sp, #80]
   11e3c:	ldp	x22, x21, [sp, #64]
   11e40:	ldp	x24, x23, [sp, #48]
   11e44:	ldp	x26, x25, [sp, #32]
   11e48:	ldp	x28, x27, [sp, #16]
   11e4c:	ldp	x29, x30, [sp], #96
   11e50:	ret
   11e54:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11e58:	b	11e34 <__gmpf_div_ui@@Base+0x130>
   11e5c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000011e60 <__gmpf_cmp_z@@Base>:
   11e60:	sub	sp, sp, #0x30
   11e64:	stp	x29, x30, [sp, #32]
   11e68:	ldrsw	x8, [x1, #4]
   11e6c:	add	x29, sp, #0x20
   11e70:	cmp	x8, #0x0
   11e74:	str	w8, [sp, #12]
   11e78:	cneg	x8, x8, mi  // mi = first
   11e7c:	str	x8, [sp, #16]
   11e80:	ldr	x8, [x1, #8]
   11e84:	add	x1, sp, #0x8
   11e88:	str	x8, [sp, #24]
   11e8c:	bl	c670 <__gmpf_cmp@plt>
   11e90:	ldp	x29, x30, [sp, #32]
   11e94:	add	sp, sp, #0x30
   11e98:	ret

0000000000011e9c <__gmpf_cmp@@Base>:
   11e9c:	ldrsw	x12, [x0, #4]
   11ea0:	ldrsw	x9, [x1, #4]
   11ea4:	mov	w10, #0x1                   	// #1
   11ea8:	mov	x8, x0
   11eac:	cmp	w12, #0x0
   11eb0:	eor	w11, w9, w12
   11eb4:	cneg	w0, w10, lt  // lt = tstop
   11eb8:	tbnz	w11, #31, 11edc <__gmpf_cmp@@Base+0x40>
   11ebc:	cbz	w12, 11ee0 <__gmpf_cmp@@Base+0x44>
   11ec0:	cbz	w9, 11eec <__gmpf_cmp@@Base+0x50>
   11ec4:	ldr	x10, [x8, #8]
   11ec8:	ldr	x11, [x1, #8]
   11ecc:	cmp	x10, x11
   11ed0:	b.gt	11edc <__gmpf_cmp@@Base+0x40>
   11ed4:	b.ge	11ef4 <__gmpf_cmp@@Base+0x58>  // b.tcont
   11ed8:	neg	w0, w0
   11edc:	ret
   11ee0:	cmp	w9, #0x0
   11ee4:	csetm	w0, ne  // ne = any
   11ee8:	ret
   11eec:	mov	w0, #0x1                   	// #1
   11ef0:	ret
   11ef4:	ldr	x10, [x8, #16]
   11ef8:	ldr	x11, [x1, #16]
   11efc:	cmp	w12, #0x0
   11f00:	cneg	x8, x12, lt  // lt = tstop
   11f04:	ldr	x13, [x10]
   11f08:	cmp	x9, #0x0
   11f0c:	cneg	x9, x9, mi  // mi = first
   11f10:	cbnz	x13, 11f20 <__gmpf_cmp@@Base+0x84>
   11f14:	ldr	x12, [x10, #8]!
   11f18:	sub	x8, x8, #0x1
   11f1c:	cbz	x12, 11f14 <__gmpf_cmp@@Base+0x78>
   11f20:	ldr	x12, [x11]
   11f24:	cbnz	x12, 11f34 <__gmpf_cmp@@Base+0x98>
   11f28:	ldr	x12, [x11, #8]!
   11f2c:	sub	x9, x9, #0x1
   11f30:	cbz	x12, 11f28 <__gmpf_cmp@@Base+0x8c>
   11f34:	cmp	x8, x9
   11f38:	b.le	11f68 <__gmpf_cmp@@Base+0xcc>
   11f3c:	add	x8, x10, x8, lsl #3
   11f40:	sub	x11, x11, #0x8
   11f44:	sub	x8, x8, #0x8
   11f48:	subs	x10, x9, #0x1
   11f4c:	b.lt	11edc <__gmpf_cmp@@Base+0x40>  // b.tstop
   11f50:	ldr	x12, [x8], #-8
   11f54:	ldr	x9, [x11, x9, lsl #3]
   11f58:	cmp	x12, x9
   11f5c:	mov	x9, x10
   11f60:	b.eq	11f48 <__gmpf_cmp@@Base+0xac>  // b.none
   11f64:	b	11fc4 <__gmpf_cmp@@Base+0x128>
   11f68:	cmp	x9, x8
   11f6c:	b.le	11f9c <__gmpf_cmp@@Base+0x100>
   11f70:	add	x9, x11, x9, lsl #3
   11f74:	sub	x9, x9, #0x8
   11f78:	sub	x10, x10, #0x8
   11f7c:	subs	x11, x8, #0x1
   11f80:	b.lt	11ed8 <__gmpf_cmp@@Base+0x3c>  // b.tstop
   11f84:	ldr	x8, [x10, x8, lsl #3]
   11f88:	ldr	x12, [x9], #-8
   11f8c:	cmp	x8, x12
   11f90:	mov	x8, x11
   11f94:	b.eq	11f7c <__gmpf_cmp@@Base+0xe0>  // b.none
   11f98:	b	11fc4 <__gmpf_cmp@@Base+0x128>
   11f9c:	sub	x9, x11, #0x8
   11fa0:	sub	x10, x10, #0x8
   11fa4:	subs	x11, x8, #0x1
   11fa8:	b.lt	11fcc <__gmpf_cmp@@Base+0x130>  // b.tstop
   11fac:	lsl	x8, x8, #3
   11fb0:	ldr	x12, [x10, x8]
   11fb4:	ldr	x8, [x9, x8]
   11fb8:	cmp	x12, x8
   11fbc:	mov	x8, x11
   11fc0:	b.eq	11fa4 <__gmpf_cmp@@Base+0x108>  // b.none
   11fc4:	b.ls	11ed8 <__gmpf_cmp@@Base+0x3c>  // b.plast
   11fc8:	b	11edc <__gmpf_cmp@@Base+0x40>
   11fcc:	mov	w0, wzr
   11fd0:	ret

0000000000011fd4 <__gmpf_cmp_d@@Base>:
   11fd4:	sub	sp, sp, #0x50
   11fd8:	fmov	x8, d0
   11fdc:	mvn	x9, x8
   11fe0:	tst	x9, #0x7ff0000000000000
   11fe4:	stp	x29, x30, [sp, #48]
   11fe8:	str	x19, [sp, #64]
   11fec:	add	x29, sp, #0x30
   11ff0:	b.eq	12060 <__gmpf_cmp_d@@Base+0x8c>  // b.none
   11ff4:	mov	x19, x0
   11ff8:	fcmp	d0, #0.0
   11ffc:	b.ne	12014 <__gmpf_cmp_d@@Base+0x40>  // b.any
   12000:	ldr	w0, [x19, #4]
   12004:	ldr	x19, [sp, #64]
   12008:	ldp	x29, x30, [sp, #48]
   1200c:	add	sp, sp, #0x50
   12010:	ret
   12014:	sub	x8, x29, #0x10
   12018:	mov	w9, #0xfffffffe            	// #-2
   1201c:	mov	w10, #0x2                   	// #2
   12020:	fneg	d1, d0
   12024:	str	x8, [sp, #24]
   12028:	csel	w8, w10, w9, ge  // ge = tcont
   1202c:	fcsel	d0, d0, d1, ge  // ge = tcont
   12030:	sub	x0, x29, #0x10
   12034:	str	w8, [sp, #12]
   12038:	bl	d280 <__gmp_extract_double@plt>
   1203c:	sxtw	x8, w0
   12040:	add	x1, sp, #0x8
   12044:	mov	x0, x19
   12048:	str	x8, [sp, #16]
   1204c:	bl	c670 <__gmpf_cmp@plt>
   12050:	ldr	x19, [sp, #64]
   12054:	ldp	x29, x30, [sp, #48]
   12058:	add	sp, sp, #0x50
   1205c:	ret
   12060:	tst	x8, #0xfffffffffffff
   12064:	b.ne	12084 <__gmpf_cmp_d@@Base+0xb0>  // b.any
   12068:	fcmp	d0, #0.0
   1206c:	mov	w8, #0xffffffff            	// #-1
   12070:	csinc	w0, w8, wzr, pl  // pl = nfrst
   12074:	ldr	x19, [sp, #64]
   12078:	ldp	x29, x30, [sp, #48]
   1207c:	add	sp, sp, #0x50
   12080:	ret
   12084:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000012088 <__gmpf_cmp_ui@@Base>:
   12088:	ldrsw	x8, [x0, #4]
   1208c:	tbnz	w8, #31, 120d8 <__gmpf_cmp_ui@@Base+0x50>
   12090:	cbz	x1, 120e0 <__gmpf_cmp_ui@@Base+0x58>
   12094:	ldr	x9, [x0, #8]
   12098:	cmp	x9, #0x1
   1209c:	b.ne	120ec <__gmpf_cmp_ui@@Base+0x64>  // b.any
   120a0:	ldr	x9, [x0, #16]
   120a4:	sub	x8, x8, #0x1
   120a8:	ldr	x10, [x9, x8, lsl #3]
   120ac:	cmp	x10, x1
   120b0:	b.ne	120f8 <__gmpf_cmp_ui@@Base+0x70>  // b.any
   120b4:	ldr	x10, [x9]
   120b8:	cbnz	x10, 120cc <__gmpf_cmp_ui@@Base+0x44>
   120bc:	add	x9, x9, #0x8
   120c0:	ldr	x10, [x9], #8
   120c4:	sub	x8, x8, #0x1
   120c8:	cbz	x10, 120c0 <__gmpf_cmp_ui@@Base+0x38>
   120cc:	cmp	x8, #0x0
   120d0:	cset	w0, gt
   120d4:	ret
   120d8:	mov	w0, #0xffffffff            	// #-1
   120dc:	ret
   120e0:	cmp	w8, #0x0
   120e4:	cset	w0, ne  // ne = any
   120e8:	ret
   120ec:	mov	w8, #0xffffffff            	// #-1
   120f0:	cneg	w0, w8, ge  // ge = tcont
   120f4:	ret
   120f8:	mov	w8, #0xffffffff            	// #-1
   120fc:	cneg	w0, w8, cs  // cs = hs, nlast
   12100:	ret

0000000000012104 <__gmpf_cmp_si@@Base>:
   12104:	ldrsw	x10, [x0, #4]
   12108:	lsr	x9, x1, #63
   1210c:	cmp	w9, w10, lsr #31
   12110:	b.ne	12180 <__gmpf_cmp_si@@Base+0x7c>  // b.any
   12114:	cbz	w10, 12190 <__gmpf_cmp_si@@Base+0x8c>
   12118:	mov	x8, x0
   1211c:	mov	w0, #0x1                   	// #1
   12120:	cbz	x1, 1217c <__gmpf_cmp_si@@Base+0x78>
   12124:	ldr	x11, [x8, #8]
   12128:	cmp	w10, #0x0
   1212c:	cneg	w9, w0, lt  // lt = tstop
   12130:	cmp	x1, #0x0
   12134:	cneg	x12, x1, mi  // mi = first
   12138:	cmp	x11, #0x1
   1213c:	b.ne	1219c <__gmpf_cmp_si@@Base+0x98>  // b.any
   12140:	ldr	x11, [x8, #16]
   12144:	cmp	w10, #0x0
   12148:	cneg	x8, x10, lt  // lt = tstop
   1214c:	sub	x8, x8, #0x1
   12150:	ldr	x10, [x11, x8, lsl #3]
   12154:	cmp	x10, x12
   12158:	b.ne	121a4 <__gmpf_cmp_si@@Base+0xa0>  // b.any
   1215c:	ldr	x10, [x11]
   12160:	cbnz	x10, 12174 <__gmpf_cmp_si@@Base+0x70>
   12164:	add	x10, x11, #0x8
   12168:	ldr	x11, [x10], #8
   1216c:	sub	x8, x8, #0x1
   12170:	cbz	x11, 12168 <__gmpf_cmp_si@@Base+0x64>
   12174:	cmp	x8, #0x0
   12178:	csel	w0, w9, wzr, gt
   1217c:	ret
   12180:	cmp	w10, #0x0
   12184:	mov	w8, #0x1                   	// #1
   12188:	cneg	w0, w8, lt  // lt = tstop
   1218c:	ret
   12190:	cmp	x1, #0x0
   12194:	csetm	w0, ne  // ne = any
   12198:	ret
   1219c:	cneg	w0, w9, lt  // lt = tstop
   121a0:	ret
   121a4:	cneg	w0, w9, cc  // cc = lo, ul, last
   121a8:	ret

00000000000121ac <__gmpf_mul_2exp@@Base>:
   121ac:	stp	x29, x30, [sp, #-80]!
   121b0:	stp	x24, x23, [sp, #32]
   121b4:	stp	x22, x21, [sp, #48]
   121b8:	stp	x20, x19, [sp, #64]
   121bc:	ldrsw	x24, [x1, #4]
   121c0:	mov	x19, x0
   121c4:	str	x25, [sp, #16]
   121c8:	mov	x29, sp
   121cc:	cbz	w24, 1229c <__gmpf_mul_2exp@@Base+0xf0>
   121d0:	ldr	x21, [x19, #16]
   121d4:	ldrsw	x22, [x19]
   121d8:	ldp	x25, x1, [x1, #8]
   121dc:	cmp	w24, #0x0
   121e0:	mov	x20, x2
   121e4:	cneg	x23, x24, lt  // lt = tstop
   121e8:	ands	x3, x2, #0x3f
   121ec:	b.eq	1221c <__gmpf_mul_2exp@@Base+0x70>  // b.none
   121f0:	subs	x8, x23, x22
   121f4:	b.le	1224c <__gmpf_mul_2exp@@Base+0xa0>
   121f8:	add	x1, x1, x8, lsl #3
   121fc:	mov	w8, #0x40                  	// #64
   12200:	add	x0, x21, #0x8
   12204:	sub	w3, w8, w3
   12208:	mov	x2, x22
   1220c:	bl	c1a0 <__gmpn_rshift@plt>
   12210:	str	x0, [x21]
   12214:	ldr	x0, [x21, x22, lsl #3]
   12218:	b	12260 <__gmpf_mul_2exp@@Base+0xb4>
   1221c:	add	x8, x22, #0x1
   12220:	subs	x8, x23, x8
   12224:	add	x8, x1, x8, lsl #3
   12228:	csel	x1, x8, x1, gt
   1222c:	csinc	x22, x23, x22, le
   12230:	cmp	x21, x1
   12234:	b.eq	12244 <__gmpf_mul_2exp@@Base+0x98>  // b.none
   12238:	mov	x0, x21
   1223c:	mov	x2, x22
   12240:	bl	ca50 <__gmpn_copyi@plt>
   12244:	add	x8, x25, x20, lsr #6
   12248:	b	12270 <__gmpf_mul_2exp@@Base+0xc4>
   1224c:	mov	x0, x21
   12250:	mov	x2, x23
   12254:	bl	c180 <__gmpn_lshift@plt>
   12258:	mov	x22, x23
   1225c:	str	x0, [x21, x23, lsl #3]
   12260:	cmp	x0, #0x0
   12264:	add	x8, x25, x20, lsr #6
   12268:	cinc	x22, x22, ne  // ne = any
   1226c:	cinc	x8, x8, ne  // ne = any
   12270:	str	x8, [x19, #8]
   12274:	neg	w8, w22
   12278:	cmp	w24, #0x0
   1227c:	csel	x8, x22, x8, ge  // ge = tcont
   12280:	str	w8, [x19, #4]
   12284:	ldp	x20, x19, [sp, #64]
   12288:	ldp	x22, x21, [sp, #48]
   1228c:	ldp	x24, x23, [sp, #32]
   12290:	ldr	x25, [sp, #16]
   12294:	ldp	x29, x30, [sp], #80
   12298:	ret
   1229c:	str	wzr, [x19, #4]
   122a0:	str	xzr, [x19, #8]
   122a4:	b	12284 <__gmpf_mul_2exp@@Base+0xd8>

00000000000122a8 <__gmpf_div_2exp@@Base>:
   122a8:	stp	x29, x30, [sp, #-80]!
   122ac:	stp	x24, x23, [sp, #32]
   122b0:	stp	x22, x21, [sp, #48]
   122b4:	stp	x20, x19, [sp, #64]
   122b8:	ldrsw	x24, [x1, #4]
   122bc:	mov	x19, x0
   122c0:	str	x25, [sp, #16]
   122c4:	mov	x29, sp
   122c8:	cbz	w24, 123a4 <__gmpf_div_2exp@@Base+0xfc>
   122cc:	ldr	x21, [x19, #16]
   122d0:	ldrsw	x22, [x19]
   122d4:	ldp	x25, x1, [x1, #8]
   122d8:	cmp	w24, #0x0
   122dc:	mov	x20, x2
   122e0:	cneg	x23, x24, lt  // lt = tstop
   122e4:	ands	x3, x2, #0x3f
   122e8:	b.eq	12310 <__gmpf_div_2exp@@Base+0x68>  // b.none
   122ec:	subs	x8, x23, x22
   122f0:	b.le	12340 <__gmpf_div_2exp@@Base+0x98>
   122f4:	add	x1, x1, x8, lsl #3
   122f8:	add	x0, x21, #0x8
   122fc:	mov	x2, x22
   12300:	bl	c1a0 <__gmpn_rshift@plt>
   12304:	str	x0, [x21]
   12308:	ldr	x0, [x21, x22, lsl #3]
   1230c:	b	1235c <__gmpf_div_2exp@@Base+0xb4>
   12310:	add	x8, x22, #0x1
   12314:	subs	x8, x23, x8
   12318:	add	x8, x1, x8, lsl #3
   1231c:	csel	x1, x8, x1, gt
   12320:	csinc	x22, x23, x22, le
   12324:	cmp	x21, x1
   12328:	b.eq	12338 <__gmpf_div_2exp@@Base+0x90>  // b.none
   1232c:	mov	x0, x21
   12330:	mov	x2, x22
   12334:	bl	ca50 <__gmpn_copyi@plt>
   12338:	sub	x8, x25, x20, lsr #6
   1233c:	b	12378 <__gmpf_div_2exp@@Base+0xd0>
   12340:	mov	w8, #0x40                  	// #64
   12344:	sub	w3, w8, w3
   12348:	mov	x0, x21
   1234c:	mov	x2, x23
   12350:	bl	c180 <__gmpn_lshift@plt>
   12354:	mov	x22, x23
   12358:	str	x0, [x21, x23, lsl #3]
   1235c:	lsr	x8, x20, #6
   12360:	sub	x9, x25, x8
   12364:	mvn	x8, x8
   12368:	cmp	x0, #0x0
   1236c:	add	x8, x25, x8
   12370:	cinc	x22, x22, ne  // ne = any
   12374:	csel	x8, x8, x9, eq  // eq = none
   12378:	str	x8, [x19, #8]
   1237c:	neg	w8, w22
   12380:	cmp	w24, #0x0
   12384:	csel	x8, x22, x8, ge  // ge = tcont
   12388:	str	w8, [x19, #4]
   1238c:	ldp	x20, x19, [sp, #64]
   12390:	ldp	x22, x21, [sp, #48]
   12394:	ldp	x24, x23, [sp, #32]
   12398:	ldr	x25, [sp, #16]
   1239c:	ldp	x29, x30, [sp], #80
   123a0:	ret
   123a4:	str	wzr, [x19, #4]
   123a8:	str	xzr, [x19, #8]
   123ac:	b	1238c <__gmpf_div_2exp@@Base+0xe4>

00000000000123b0 <__gmpf_abs@@Base>:
   123b0:	stp	x29, x30, [sp, #-48]!
   123b4:	stp	x20, x19, [sp, #32]
   123b8:	ldr	w8, [x1, #4]
   123bc:	mov	x19, x0
   123c0:	str	x21, [sp, #16]
   123c4:	mov	x29, sp
   123c8:	cmp	w8, #0x0
   123cc:	cneg	w20, w8, mi  // mi = first
   123d0:	cmp	x0, x1
   123d4:	b.eq	1240c <__gmpf_abs@@Base+0x5c>  // b.none
   123d8:	ldrsw	x8, [x19]
   123dc:	ldr	x9, [x1, #16]
   123e0:	ldr	x0, [x19, #16]
   123e4:	mov	x21, x1
   123e8:	add	x10, x8, #0x1
   123ec:	subs	x10, x20, x10
   123f0:	add	x10, x9, x10, lsl #3
   123f4:	csinc	x20, x20, x8, le
   123f8:	csel	x1, x10, x9, gt
   123fc:	mov	x2, x20
   12400:	bl	ca50 <__gmpn_copyi@plt>
   12404:	ldr	x8, [x21, #8]
   12408:	str	x8, [x19, #8]
   1240c:	str	w20, [x19, #4]
   12410:	ldp	x20, x19, [sp, #32]
   12414:	ldr	x21, [sp, #16]
   12418:	ldp	x29, x30, [sp], #48
   1241c:	ret

0000000000012420 <__gmpf_neg@@Base>:
   12420:	stp	x29, x30, [sp, #-48]!
   12424:	stp	x20, x19, [sp, #32]
   12428:	ldrsw	x8, [x1, #4]
   1242c:	str	x21, [sp, #16]
   12430:	mov	x19, x0
   12434:	cmp	x0, x1
   12438:	neg	x21, x8
   1243c:	mov	x29, sp
   12440:	b.eq	12484 <__gmpf_neg@@Base+0x64>  // b.none
   12444:	ldrsw	x9, [x19]
   12448:	ldr	x10, [x1, #16]
   1244c:	cmp	w8, #0x1
   12450:	ldr	x0, [x19, #16]
   12454:	cneg	x11, x21, ge  // ge = tcont
   12458:	add	x12, x9, #0x1
   1245c:	subs	x12, x11, x12
   12460:	add	x12, x10, x12, lsl #3
   12464:	mov	x20, x1
   12468:	csinc	x2, x11, x9, le
   1246c:	csel	x1, x12, x10, gt
   12470:	cmp	w8, #0x1
   12474:	cneg	x21, x2, ge  // ge = tcont
   12478:	bl	ca50 <__gmpn_copyi@plt>
   1247c:	ldr	x8, [x20, #8]
   12480:	str	x8, [x19, #8]
   12484:	str	w21, [x19, #4]
   12488:	ldp	x20, x19, [sp, #32]
   1248c:	ldr	x21, [sp, #16]
   12490:	ldp	x29, x30, [sp], #48
   12494:	ret

0000000000012498 <__gmpf_set_q@@Base>:
   12498:	stp	x29, x30, [sp, #-96]!
   1249c:	stp	x28, x27, [sp, #16]
   124a0:	stp	x26, x25, [sp, #32]
   124a4:	stp	x24, x23, [sp, #48]
   124a8:	stp	x22, x21, [sp, #64]
   124ac:	stp	x20, x19, [sp, #80]
   124b0:	mov	x29, sp
   124b4:	sub	sp, sp, #0x20
   124b8:	ldrsw	x27, [x1, #4]
   124bc:	mov	x19, x0
   124c0:	cbz	w27, 12568 <__gmpf_set_q@@Base+0xd0>
   124c4:	ldrsw	x20, [x1, #20]
   124c8:	ldr	x8, [x19, #16]
   124cc:	ldrsw	x22, [x19]
   124d0:	cmp	w27, #0x0
   124d4:	cneg	x24, x27, lt  // lt = tstop
   124d8:	stur	x8, [x29, #-24]
   124dc:	sub	x8, x24, x20
   124e0:	add	x8, x8, #0x1
   124e4:	add	x9, x22, #0x1
   124e8:	sub	x28, x9, x8
   124ec:	ldr	x25, [x1, #8]
   124f0:	ldr	x3, [x1, #24]
   124f4:	add	x23, x28, x24
   124f8:	stp	x8, xzr, [x29, #-16]
   124fc:	lsl	x8, x23, #3
   12500:	add	x1, x8, #0x8
   12504:	mov	w8, #0x7f00                	// #32512
   12508:	cmp	x1, x8
   1250c:	stur	x9, [x29, #-32]
   12510:	b.hi	12574 <__gmpf_set_q@@Base+0xdc>  // b.pmore
   12514:	add	x9, x1, #0xf
   12518:	mov	x8, sp
   1251c:	and	x9, x9, #0xfffffffffffffff0
   12520:	sub	x26, x8, x9
   12524:	mov	sp, x26
   12528:	cmp	x28, #0x1
   1252c:	b.lt	12590 <__gmpf_set_q@@Base+0xf8>  // b.tstop
   12530:	add	x8, x20, x22
   12534:	sub	x8, x8, x24
   12538:	lsl	x2, x8, #3
   1253c:	mov	x0, x26
   12540:	mov	w1, wzr
   12544:	mov	x21, x3
   12548:	bl	c5f0 <memset@plt>
   1254c:	add	x0, x26, x28, lsl #3
   12550:	mov	x1, x25
   12554:	mov	x2, x24
   12558:	bl	ca50 <__gmpn_copyi@plt>
   1255c:	mov	x3, x21
   12560:	mov	x1, x26
   12564:	b	12594 <__gmpf_set_q@@Base+0xfc>
   12568:	str	wzr, [x19, #4]
   1256c:	str	xzr, [x19, #8]
   12570:	b	125e0 <__gmpf_set_q@@Base+0x148>
   12574:	sub	x0, x29, #0x8
   12578:	mov	x21, x3
   1257c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   12580:	mov	x3, x21
   12584:	mov	x26, x0
   12588:	cmp	x28, #0x1
   1258c:	b.ge	12530 <__gmpf_set_q@@Base+0x98>  // b.tcont
   12590:	sub	x1, x25, x28, lsl #3
   12594:	ldur	x21, [x29, #-24]
   12598:	mov	x2, x23
   1259c:	mov	x4, x20
   125a0:	mov	x5, x26
   125a4:	mov	x0, x21
   125a8:	bl	c320 <__gmpn_div_q@plt>
   125ac:	ldr	x8, [x21, x22, lsl #3]
   125b0:	ldur	x9, [x29, #-32]
   125b4:	ldp	x10, x0, [x29, #-16]
   125b8:	cmp	x8, #0x0
   125bc:	cset	w8, eq  // eq = none
   125c0:	sub	x9, x9, x8
   125c4:	sub	x8, x10, x8
   125c8:	str	x8, [x19, #8]
   125cc:	neg	w8, w9
   125d0:	cmp	w27, #0x0
   125d4:	csel	x8, x9, x8, ge  // ge = tcont
   125d8:	str	w8, [x19, #4]
   125dc:	cbnz	x0, 12600 <__gmpf_set_q@@Base+0x168>
   125e0:	mov	sp, x29
   125e4:	ldp	x20, x19, [sp, #80]
   125e8:	ldp	x22, x21, [sp, #64]
   125ec:	ldp	x24, x23, [sp, #48]
   125f0:	ldp	x26, x25, [sp, #32]
   125f4:	ldp	x28, x27, [sp, #16]
   125f8:	ldp	x29, x30, [sp], #96
   125fc:	ret
   12600:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   12604:	b	125e0 <__gmpf_set_q@@Base+0x148>

0000000000012608 <__gmpf_get_d@@Base>:
   12608:	ldrsw	x2, [x0, #4]
   1260c:	cbz	w2, 12628 <__gmpf_get_d@@Base+0x20>
   12610:	ldp	x8, x0, [x0, #8]
   12614:	cmp	x2, #0x0
   12618:	cneg	x1, x2, mi  // mi = first
   1261c:	sub	x8, x8, x1
   12620:	lsl	x3, x8, #6
   12624:	b	bf40 <__gmpn_get_d@plt>
   12628:	fmov	d0, xzr
   1262c:	ret

0000000000012630 <__gmpf_get_d_2exp@@Base>:
   12630:	ldrsw	x2, [x1, #4]
   12634:	cbz	w2, 12668 <__gmpf_get_d_2exp@@Base+0x38>
   12638:	ldp	x9, x8, [x1, #8]
   1263c:	cmp	x2, #0x0
   12640:	cneg	x1, x2, mi  // mi = first
   12644:	add	x10, x8, x1, lsl #3
   12648:	ldur	x10, [x10, #-8]
   1264c:	lsl	x9, x9, #6
   12650:	clz	x10, x10
   12654:	sub	x9, x9, x10
   12658:	sub	x3, x10, x1, lsl #6
   1265c:	str	x9, [x0]
   12660:	mov	x0, x8
   12664:	b	bf40 <__gmpn_get_d@plt>
   12668:	fmov	d0, xzr
   1266c:	str	xzr, [x0]
   12670:	ret

0000000000012674 <__gmpf_set_default_prec@@Base>:
   12674:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   12678:	cmp	x0, #0x35
   1267c:	mov	w8, #0x35                  	// #53
   12680:	ldr	x9, [x9, #3960]
   12684:	csel	x8, x0, x8, hi  // hi = pmore
   12688:	add	x8, x8, #0x7f
   1268c:	lsr	x8, x8, #6
   12690:	str	x8, [x9]
   12694:	ret

0000000000012698 <__gmpf_set_prec@@Base>:
   12698:	stp	x29, x30, [sp, #-48]!
   1269c:	stp	x22, x21, [sp, #16]
   126a0:	stp	x20, x19, [sp, #32]
   126a4:	cmp	x1, #0x35
   126a8:	mov	w8, #0x35                  	// #53
   126ac:	ldrsw	x22, [x0]
   126b0:	csel	x8, x1, x8, hi  // hi = pmore
   126b4:	add	x8, x8, #0x7f
   126b8:	lsr	x8, x8, #6
   126bc:	cmp	x8, x22
   126c0:	mov	x29, sp
   126c4:	b.eq	12734 <__gmpf_set_prec@@Base+0x9c>  // b.none
   126c8:	ldrsw	x10, [x0, #4]
   126cc:	ldr	x21, [x0, #16]
   126d0:	add	x20, x8, #0x1
   126d4:	mov	x19, x0
   126d8:	cmp	x10, #0x0
   126dc:	cneg	x9, x10, mi  // mi = first
   126e0:	cmp	x9, x20
   126e4:	str	w8, [x0]
   126e8:	b.le	12710 <__gmpf_set_prec@@Base+0x78>
   126ec:	mvn	x11, x8
   126f0:	cmp	w10, #0x0
   126f4:	add	x9, x21, x9, lsl #3
   126f8:	csinv	x8, x20, x8, ge  // ge = tcont
   126fc:	add	x1, x9, x11, lsl #3
   12700:	mov	x0, x21
   12704:	mov	x2, x20
   12708:	str	w8, [x19, #4]
   1270c:	bl	ca50 <__gmpn_copyi@plt>
   12710:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   12714:	ldr	x8, [x8, #3792]
   12718:	lsl	x9, x22, #3
   1271c:	add	x1, x9, #0x8
   12720:	lsl	x2, x20, #3
   12724:	ldr	x8, [x8]
   12728:	mov	x0, x21
   1272c:	blr	x8
   12730:	str	x0, [x19, #16]
   12734:	ldp	x20, x19, [sp, #32]
   12738:	ldp	x22, x21, [sp, #16]
   1273c:	ldp	x29, x30, [sp], #48
   12740:	ret

0000000000012744 <__gmpf_set_prec_raw@@Base>:
   12744:	cmp	x1, #0x35
   12748:	mov	w8, #0x35                  	// #53
   1274c:	csel	x8, x1, x8, hi  // hi = pmore
   12750:	add	x8, x8, #0x7f
   12754:	lsr	x8, x8, #6
   12758:	str	w8, [x0]
   1275c:	ret

0000000000012760 <__gmpf_get_default_prec@@Base>:
   12760:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   12764:	ldr	x8, [x8, #3960]
   12768:	ldr	x8, [x8]
   1276c:	lsl	x8, x8, #6
   12770:	sub	x0, x8, #0x40
   12774:	ret

0000000000012778 <__gmpf_get_prec@@Base>:
   12778:	ldrsw	x8, [x0]
   1277c:	lsl	x8, x8, #6
   12780:	sub	x0, x8, #0x40
   12784:	ret

0000000000012788 <__gmpf_ui_div@@Base>:
   12788:	stp	x29, x30, [sp, #-96]!
   1278c:	stp	x28, x27, [sp, #16]
   12790:	stp	x26, x25, [sp, #32]
   12794:	stp	x24, x23, [sp, #48]
   12798:	stp	x22, x21, [sp, #64]
   1279c:	stp	x20, x19, [sp, #80]
   127a0:	mov	x29, sp
   127a4:	sub	sp, sp, #0x30
   127a8:	ldrsw	x27, [x2, #4]
   127ac:	cbz	w27, 12930 <__gmpf_ui_div@@Base+0x1a8>
   127b0:	mov	x11, x1
   127b4:	mov	x19, x0
   127b8:	cbz	x1, 128f0 <__gmpf_ui_div@@Base+0x168>
   127bc:	ldrsw	x12, [x19]
   127c0:	ldr	x21, [x19, #16]
   127c4:	ldp	x13, x23, [x2, #8]
   127c8:	cmp	w27, #0x0
   127cc:	cneg	x22, x27, lt  // lt = tstop
   127d0:	add	x10, x12, #0x1
   127d4:	add	x20, x22, x10
   127d8:	cmp	x21, x23
   127dc:	sub	x24, x20, #0x1
   127e0:	csel	x8, x22, xzr, eq  // eq = none
   127e4:	add	x9, x24, x22
   127e8:	add	x8, x9, x8
   127ec:	lsl	x1, x8, #3
   127f0:	mov	w8, #0x7f00                	// #32512
   127f4:	cmp	x1, x8
   127f8:	mov	w14, #0x2                   	// #2
   127fc:	stp	x10, xzr, [x29, #-16]
   12800:	stp	x12, x11, [x29, #-32]
   12804:	b.hi	128fc <__gmpf_ui_div@@Base+0x174>  // b.pmore
   12808:	add	x9, x1, #0xf
   1280c:	mov	x8, sp
   12810:	and	x9, x9, #0xfffffffffffffff0
   12814:	sub	x25, x8, x9
   12818:	mov	sp, x25
   1281c:	sub	x28, x20, #0x2
   12820:	cmp	x21, x23
   12824:	add	x26, x25, x22, lsl #3
   12828:	b.ne	1284c <__gmpf_ui_div@@Base+0xc4>  // b.any
   1282c:	add	x23, x26, x24, lsl #3
   12830:	mov	x0, x23
   12834:	mov	x1, x21
   12838:	mov	x2, x22
   1283c:	mov	x20, x13
   12840:	bl	ca50 <__gmpn_copyi@plt>
   12844:	mov	w14, #0x2                   	// #2
   12848:	mov	x13, x20
   1284c:	ldur	x20, [x29, #-32]
   12850:	sub	x8, x14, x13
   12854:	stur	x8, [x29, #-40]
   12858:	cbz	x28, 12874 <__gmpf_ui_div@@Base+0xec>
   1285c:	add	x8, x22, x20
   12860:	lsl	x8, x8, #3
   12864:	add	x0, x25, x22, lsl #3
   12868:	sub	x2, x8, #0x8
   1286c:	mov	w1, wzr
   12870:	bl	c5f0 <memset@plt>
   12874:	ldur	x8, [x29, #-24]
   12878:	mov	x0, x21
   1287c:	mov	x1, x25
   12880:	mov	x2, xzr
   12884:	mov	x3, x26
   12888:	mov	x4, x24
   1288c:	mov	x5, x23
   12890:	mov	x6, x22
   12894:	str	x8, [x26, x28, lsl #3]
   12898:	bl	bf00 <__gmpn_tdiv_qr@plt>
   1289c:	ldr	x8, [x21, x20, lsl #3]
   128a0:	ldp	x9, x0, [x29, #-16]
   128a4:	ldur	x10, [x29, #-40]
   128a8:	cmp	x8, #0x0
   128ac:	cset	w8, eq  // eq = none
   128b0:	sub	x9, x9, x8
   128b4:	cmp	w27, #0x0
   128b8:	sub	x8, x10, x8
   128bc:	neg	w10, w9
   128c0:	csel	x9, x9, x10, ge  // ge = tcont
   128c4:	str	w9, [x19, #4]
   128c8:	str	x8, [x19, #8]
   128cc:	cbnz	x0, 12928 <__gmpf_ui_div@@Base+0x1a0>
   128d0:	mov	sp, x29
   128d4:	ldp	x20, x19, [sp, #80]
   128d8:	ldp	x22, x21, [sp, #64]
   128dc:	ldp	x24, x23, [sp, #48]
   128e0:	ldp	x26, x25, [sp, #32]
   128e4:	ldp	x28, x27, [sp, #16]
   128e8:	ldp	x29, x30, [sp], #96
   128ec:	ret
   128f0:	str	wzr, [x19, #4]
   128f4:	str	xzr, [x19, #8]
   128f8:	b	128d0 <__gmpf_ui_div@@Base+0x148>
   128fc:	sub	x0, x29, #0x8
   12900:	mov	x25, x13
   12904:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   12908:	mov	w14, #0x2                   	// #2
   1290c:	mov	x13, x25
   12910:	mov	x25, x0
   12914:	sub	x28, x20, #0x2
   12918:	cmp	x21, x23
   1291c:	add	x26, x25, x22, lsl #3
   12920:	b.ne	1284c <__gmpf_ui_div@@Base+0xc4>  // b.any
   12924:	b	1282c <__gmpf_ui_div@@Base+0xa4>
   12928:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1292c:	b	128d0 <__gmpf_ui_div@@Base+0x148>
   12930:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000012934 <__gmpf_sqrt_ui@@Base>:
   12934:	stp	x29, x30, [sp, #-64]!
   12938:	stp	x24, x23, [sp, #16]
   1293c:	stp	x22, x21, [sp, #32]
   12940:	stp	x20, x19, [sp, #48]
   12944:	mov	x29, sp
   12948:	sub	sp, sp, #0x10
   1294c:	mov	x20, x1
   12950:	cmp	x1, #0x1
   12954:	mov	x19, x0
   12958:	b.ls	129f4 <__gmpf_sqrt_ui@@Base+0xc0>  // b.plast
   1295c:	stur	xzr, [x29, #-8]
   12960:	ldr	w23, [x19]
   12964:	mov	w9, #0x7f00                	// #32512
   12968:	sbfiz	x8, x23, #1, #32
   1296c:	sub	x21, x8, #0x1
   12970:	lsl	x1, x21, #3
   12974:	cmp	x1, x9
   12978:	sub	x24, x8, #0x2
   1297c:	b.hi	12a08 <__gmpf_sqrt_ui@@Base+0xd4>  // b.pmore
   12980:	add	x9, x1, #0xf
   12984:	mov	x8, sp
   12988:	and	x9, x9, #0xfffffffffffffff0
   1298c:	sub	x22, x8, x9
   12990:	mov	sp, x22
   12994:	cbz	x24, 129b0 <__gmpf_sqrt_ui@@Base+0x7c>
   12998:	sxtw	x8, w23
   1299c:	lsl	x8, x8, #4
   129a0:	sub	x2, x8, #0x10
   129a4:	mov	x0, x22
   129a8:	mov	w1, wzr
   129ac:	bl	c5f0 <memset@plt>
   129b0:	str	x20, [x22, x24, lsl #3]
   129b4:	ldr	x0, [x19, #16]
   129b8:	mov	x1, xzr
   129bc:	mov	x2, x22
   129c0:	mov	x3, x21
   129c4:	bl	d3b0 <__gmpn_sqrtrem@plt>
   129c8:	mov	w8, #0x1                   	// #1
   129cc:	str	w23, [x19, #4]
   129d0:	str	x8, [x19, #8]
   129d4:	ldur	x0, [x29, #-8]
   129d8:	cbnz	x0, 12a1c <__gmpf_sqrt_ui@@Base+0xe8>
   129dc:	mov	sp, x29
   129e0:	ldp	x20, x19, [sp, #48]
   129e4:	ldp	x22, x21, [sp, #32]
   129e8:	ldp	x24, x23, [sp, #16]
   129ec:	ldp	x29, x30, [sp], #64
   129f0:	ret
   129f4:	ldr	x8, [x19, #16]
   129f8:	str	x20, [x19, #8]
   129fc:	str	w20, [x19, #4]
   12a00:	str	x20, [x8]
   12a04:	b	129dc <__gmpf_sqrt_ui@@Base+0xa8>
   12a08:	sub	x0, x29, #0x8
   12a0c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   12a10:	mov	x22, x0
   12a14:	cbnz	x24, 12998 <__gmpf_sqrt_ui@@Base+0x64>
   12a18:	b	129b0 <__gmpf_sqrt_ui@@Base+0x7c>
   12a1c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   12a20:	b	129dc <__gmpf_sqrt_ui@@Base+0xa8>

0000000000012a24 <__gmpf_ceil@@Base>:
   12a24:	mov	w2, #0x1                   	// #1
   12a28:	b	12a2c <__gmpf_ceil@@Base+0x8>
   12a2c:	ldrsw	x10, [x1, #4]
   12a30:	cbz	w10, 12ae0 <__gmpf_ceil@@Base+0xbc>
   12a34:	ldr	x11, [x1, #8]
   12a38:	ldr	x8, [x0, #16]
   12a3c:	mov	w9, w2
   12a40:	cmp	x11, #0x0
   12a44:	b.le	12ac4 <__gmpf_ceil@@Base+0xa0>
   12a48:	ldrsw	x14, [x0]
   12a4c:	str	x11, [x0, #8]
   12a50:	cmp	w10, #0x0
   12a54:	ldr	x12, [x1, #16]
   12a58:	cneg	x13, x10, lt  // lt = tstop
   12a5c:	cmp	x13, x11
   12a60:	csel	x15, x13, x11, lt  // lt = tstop
   12a64:	add	x16, x14, #0x1
   12a68:	cmp	x15, x16
   12a6c:	add	x11, x12, x13, lsl #3
   12a70:	csinc	x2, x15, x14, lt  // lt = tstop
   12a74:	eor	w9, w10, w9
   12a78:	sub	x1, x11, x2, lsl #3
   12a7c:	tbnz	w9, #31, 12aa4 <__gmpf_ceil@@Base+0x80>
   12a80:	cmp	x12, x1
   12a84:	b.eq	12aa4 <__gmpf_ceil@@Base+0x80>  // b.none
   12a88:	lsl	x9, x13, #3
   12a8c:	sub	x9, x9, x2, lsl #3
   12a90:	mov	x14, x12
   12a94:	ldr	x15, [x14], #8
   12a98:	cbnz	x15, 12aec <__gmpf_ceil@@Base+0xc8>
   12a9c:	subs	x9, x9, #0x8
   12aa0:	b.ne	12a94 <__gmpf_ceil@@Base+0x70>  // b.any
   12aa4:	neg	x9, x2
   12aa8:	cmp	w10, #0x0
   12aac:	csel	x9, x2, x9, ge  // ge = tcont
   12ab0:	cmp	x8, x1
   12ab4:	str	w9, [x0, #4]
   12ab8:	b.eq	12ae8 <__gmpf_ceil@@Base+0xc4>  // b.none
   12abc:	mov	x0, x8
   12ac0:	b	ca50 <__gmpn_copyi@plt>
   12ac4:	eor	w10, w10, w9
   12ac8:	tbnz	w10, #31, 12ae0 <__gmpf_ceil@@Base+0xbc>
   12acc:	mov	w10, #0x1                   	// #1
   12ad0:	str	x10, [x8]
   12ad4:	str	x10, [x0, #8]
   12ad8:	str	w9, [x0, #4]
   12adc:	ret
   12ae0:	str	wzr, [x0, #4]
   12ae4:	str	xzr, [x0, #8]
   12ae8:	ret
   12aec:	ldr	x9, [x1]
   12af0:	adds	x9, x9, #0x1
   12af4:	str	x9, [x8]
   12af8:	b.cc	12bd0 <__gmpf_ceil@@Base+0x1ac>  // b.lo, b.ul, b.last
   12afc:	mov	x13, xzr
   12b00:	sub	x12, x2, #0x1
   12b04:	mov	w9, #0x1                   	// #1
   12b08:	cmp	x9, x2
   12b0c:	b.ge	12c30 <__gmpf_ceil@@Base+0x20c>  // b.tcont
   12b10:	add	x14, x1, x13
   12b14:	ldr	x14, [x14, #8]
   12b18:	add	x15, x8, x13
   12b1c:	add	x9, x9, #0x1
   12b20:	add	x13, x13, #0x8
   12b24:	adds	x14, x14, #0x1
   12b28:	sub	x12, x12, #0x1
   12b2c:	str	x14, [x15, #8]
   12b30:	b.cs	12b08 <__gmpf_ceil@@Base+0xe4>  // b.hs, b.nlast
   12b34:	cmp	x1, x8
   12b38:	b.eq	12c44 <__gmpf_ceil@@Base+0x220>  // b.none
   12b3c:	subs	x14, x2, x9
   12b40:	b.le	12c44 <__gmpf_ceil@@Base+0x220>
   12b44:	cmp	x14, #0x4
   12b48:	b.cc	12bb4 <__gmpf_ceil@@Base+0x190>  // b.lo, b.ul, b.last
   12b4c:	add	x15, x8, x13
   12b50:	add	x15, x15, #0x8
   12b54:	cmp	x15, x11
   12b58:	b.cs	12b70 <__gmpf_ceil@@Base+0x14c>  // b.hs, b.nlast
   12b5c:	add	x16, x1, x13
   12b60:	add	x15, x8, x2, lsl #3
   12b64:	add	x16, x16, #0x8
   12b68:	cmp	x16, x15
   12b6c:	b.cc	12bb4 <__gmpf_ceil@@Base+0x190>  // b.lo, b.ul, b.last
   12b70:	sub	x16, x2, x9
   12b74:	add	x15, x8, x13
   12b78:	add	x17, x1, x13
   12b7c:	and	x18, x12, #0xfffffffffffffffc
   12b80:	and	x13, x14, #0xfffffffffffffffc
   12b84:	add	x12, x15, #0x18
   12b88:	add	x15, x17, #0x18
   12b8c:	add	x9, x18, x9
   12b90:	and	x16, x16, #0xfffffffffffffffc
   12b94:	ldp	q0, q1, [x15, #-16]
   12b98:	add	x15, x15, #0x20
   12b9c:	subs	x16, x16, #0x4
   12ba0:	stp	q0, q1, [x12, #-16]
   12ba4:	add	x12, x12, #0x20
   12ba8:	b.ne	12b94 <__gmpf_ceil@@Base+0x170>  // b.any
   12bac:	cmp	x14, x13
   12bb0:	b.eq	12c44 <__gmpf_ceil@@Base+0x220>  // b.none
   12bb4:	sub	x12, x9, x2
   12bb8:	add	x8, x8, x9, lsl #3
   12bbc:	ldr	x9, [x11, x12, lsl #3]
   12bc0:	adds	x12, x12, #0x1
   12bc4:	str	x9, [x8], #8
   12bc8:	b.cc	12bbc <__gmpf_ceil@@Base+0x198>  // b.lo, b.ul, b.last
   12bcc:	b	12c44 <__gmpf_ceil@@Base+0x220>
   12bd0:	cmp	x2, #0x2
   12bd4:	b.lt	12c44 <__gmpf_ceil@@Base+0x220>  // b.tstop
   12bd8:	cmp	x1, x8
   12bdc:	b.eq	12c44 <__gmpf_ceil@@Base+0x220>  // b.none
   12be0:	sub	x9, x2, #0x1
   12be4:	cmp	x9, #0x4
   12be8:	b.cc	12c10 <__gmpf_ceil@@Base+0x1ec>  // b.lo, b.ul, b.last
   12bec:	add	x14, x8, #0x8
   12bf0:	sub	x13, x13, x2
   12bf4:	cmp	x14, x11
   12bf8:	add	x12, x12, x13, lsl #3
   12bfc:	b.cs	12c58 <__gmpf_ceil@@Base+0x234>  // b.hs, b.nlast
   12c00:	add	x13, x8, x2, lsl #3
   12c04:	add	x14, x12, #0x8
   12c08:	cmp	x14, x13
   12c0c:	b.cs	12c58 <__gmpf_ceil@@Base+0x234>  // b.hs, b.nlast
   12c10:	mov	w12, #0x1                   	// #1
   12c14:	sub	x9, x12, x2
   12c18:	add	x8, x8, x12, lsl #3
   12c1c:	ldr	x12, [x11, x9, lsl #3]
   12c20:	adds	x9, x9, #0x1
   12c24:	str	x12, [x8], #8
   12c28:	b.cc	12c1c <__gmpf_ceil@@Base+0x1f8>  // b.lo, b.ul, b.last
   12c2c:	b	12c44 <__gmpf_ceil@@Base+0x220>
   12c30:	mov	w2, #0x1                   	// #1
   12c34:	str	x2, [x8]
   12c38:	ldr	x8, [x0, #8]
   12c3c:	add	x8, x8, #0x1
   12c40:	str	x8, [x0, #8]
   12c44:	neg	w8, w2
   12c48:	cmp	w10, #0x0
   12c4c:	csel	x8, x2, x8, ge  // ge = tcont
   12c50:	str	w8, [x0, #4]
   12c54:	ret
   12c58:	and	x13, x9, #0xfffffffffffffffc
   12c5c:	add	x14, x12, #0x18
   12c60:	orr	x12, x13, #0x1
   12c64:	add	x15, x8, #0x18
   12c68:	mov	x16, x13
   12c6c:	ldp	q0, q1, [x14, #-16]
   12c70:	add	x14, x14, #0x20
   12c74:	subs	x16, x16, #0x4
   12c78:	stp	q0, q1, [x15, #-16]
   12c7c:	add	x15, x15, #0x20
   12c80:	b.ne	12c6c <__gmpf_ceil@@Base+0x248>  // b.any
   12c84:	cmp	x9, x13
   12c88:	b.eq	12c44 <__gmpf_ceil@@Base+0x220>  // b.none
   12c8c:	b	12c14 <__gmpf_ceil@@Base+0x1f0>

0000000000012c90 <__gmpf_floor@@Base>:
   12c90:	mov	w2, #0xffffffff            	// #-1
   12c94:	b	12a2c <__gmpf_ceil@@Base+0x8>

0000000000012c98 <__gmpf_trunc@@Base>:
   12c98:	ldr	x9, [x1, #8]
   12c9c:	cmp	x9, #0x1
   12ca0:	b.lt	12d04 <__gmpf_trunc@@Base+0x6c>  // b.tstop
   12ca4:	ldr	w8, [x1, #4]
   12ca8:	cbz	w8, 12d04 <__gmpf_trunc@@Base+0x6c>
   12cac:	sxtw	x10, w8
   12cb0:	ldrsw	x11, [x0]
   12cb4:	cmp	w10, #0x0
   12cb8:	ldr	x12, [x1, #16]
   12cbc:	cneg	x13, x10, lt  // lt = tstop
   12cc0:	cmp	x13, x9
   12cc4:	str	x9, [x0, #8]
   12cc8:	ldr	x8, [x0, #16]
   12ccc:	csel	x9, x13, x9, lt  // lt = tstop
   12cd0:	add	x14, x11, #0x1
   12cd4:	cmp	x9, x14
   12cd8:	add	x12, x12, x13, lsl #3
   12cdc:	csinc	x2, x9, x11, lt  // lt = tstop
   12ce0:	cmp	w10, #0x0
   12ce4:	neg	w9, w2
   12ce8:	sub	x1, x12, x2, lsl #3
   12cec:	csel	x9, x2, x9, ge  // ge = tcont
   12cf0:	cmp	x8, x1
   12cf4:	str	w9, [x0, #4]
   12cf8:	b.eq	12d0c <__gmpf_trunc@@Base+0x74>  // b.none
   12cfc:	mov	x0, x8
   12d00:	b	ca50 <__gmpn_copyi@plt>
   12d04:	str	wzr, [x0, #4]
   12d08:	str	xzr, [x0, #8]
   12d0c:	ret

0000000000012d10 <__gmpf_pow_ui@@Base>:
   12d10:	sub	sp, sp, #0x60
   12d14:	stp	x22, x21, [sp, #64]
   12d18:	stp	x20, x19, [sp, #80]
   12d1c:	mov	x21, x2
   12d20:	mov	x20, x1
   12d24:	cmp	x2, #0x1
   12d28:	mov	x19, x0
   12d2c:	stp	x29, x30, [sp, #32]
   12d30:	str	x23, [sp, #48]
   12d34:	add	x29, sp, #0x20
   12d38:	b.hi	12d50 <__gmpf_pow_ui@@Base+0x40>  // b.pmore
   12d3c:	cbz	x21, 12e10 <__gmpf_pow_ui@@Base+0x100>
   12d40:	mov	x0, x19
   12d44:	mov	x1, x20
   12d48:	bl	c150 <__gmpf_set@plt>
   12d4c:	b	12e1c <__gmpf_pow_ui@@Base+0x10c>
   12d50:	mov	x0, x19
   12d54:	clz	x23, x21
   12d58:	bl	c350 <__gmpf_get_prec@plt>
   12d5c:	sub	x8, x0, x23
   12d60:	add	x1, x8, #0x3f
   12d64:	add	x0, sp, #0x8
   12d68:	bl	ceb0 <__gmpf_init2@plt>
   12d6c:	add	x0, sp, #0x8
   12d70:	mov	x1, x20
   12d74:	bl	c150 <__gmpf_set@plt>
   12d78:	cmp	w23, #0x3d
   12d7c:	b.hi	12dd0 <__gmpf_pow_ui@@Base+0xc0>  // b.pmore
   12d80:	mov	w8, #0x3e                  	// #62
   12d84:	mov	w9, #0x3f                  	// #63
   12d88:	sub	w22, w8, w23
   12d8c:	sub	w23, w9, w23
   12d90:	b	12da4 <__gmpf_pow_ui@@Base+0x94>
   12d94:	sub	w23, w23, #0x1
   12d98:	cmp	w23, #0x1
   12d9c:	sub	x22, x22, #0x1
   12da0:	b.le	12dd0 <__gmpf_pow_ui@@Base+0xc0>
   12da4:	add	x0, sp, #0x8
   12da8:	add	x1, sp, #0x8
   12dac:	add	x2, sp, #0x8
   12db0:	bl	cd30 <__gmpf_mul@plt>
   12db4:	lsr	x8, x21, x22
   12db8:	tbz	w8, #0, 12d94 <__gmpf_pow_ui@@Base+0x84>
   12dbc:	add	x0, sp, #0x8
   12dc0:	add	x1, sp, #0x8
   12dc4:	mov	x2, x20
   12dc8:	bl	cd30 <__gmpf_mul@plt>
   12dcc:	b	12d94 <__gmpf_pow_ui@@Base+0x84>
   12dd0:	tbnz	w21, #0, 12de4 <__gmpf_pow_ui@@Base+0xd4>
   12dd4:	add	x1, sp, #0x8
   12dd8:	add	x2, sp, #0x8
   12ddc:	mov	x0, x19
   12de0:	b	12e00 <__gmpf_pow_ui@@Base+0xf0>
   12de4:	add	x0, sp, #0x8
   12de8:	add	x1, sp, #0x8
   12dec:	add	x2, sp, #0x8
   12df0:	bl	cd30 <__gmpf_mul@plt>
   12df4:	add	x1, sp, #0x8
   12df8:	mov	x0, x19
   12dfc:	mov	x2, x20
   12e00:	bl	cd30 <__gmpf_mul@plt>
   12e04:	add	x0, sp, #0x8
   12e08:	bl	c330 <__gmpf_clear@plt>
   12e0c:	b	12e1c <__gmpf_pow_ui@@Base+0x10c>
   12e10:	mov	w1, #0x1                   	// #1
   12e14:	mov	x0, x19
   12e18:	bl	c6a0 <__gmpf_set_ui@plt>
   12e1c:	ldp	x20, x19, [sp, #80]
   12e20:	ldp	x22, x21, [sp, #64]
   12e24:	ldr	x23, [sp, #48]
   12e28:	ldp	x29, x30, [sp, #32]
   12e2c:	add	sp, sp, #0x60
   12e30:	ret

0000000000012e34 <__gmpf_urandomb@@Base>:
   12e34:	stp	x29, x30, [sp, #-48]!
   12e38:	stp	x22, x21, [sp, #16]
   12e3c:	stp	x20, x19, [sp, #32]
   12e40:	ldrsw	x8, [x0]
   12e44:	add	x9, x2, #0x3f
   12e48:	ldr	x10, [x1, #24]
   12e4c:	lsr	x9, x9, #6
   12e50:	add	x11, x8, #0x1
   12e54:	cmp	x9, x11
   12e58:	cset	w12, gt
   12e5c:	cmp	x9, #0x0
   12e60:	cset	w13, eq  // eq = none
   12e64:	ldr	x21, [x0, #16]
   12e68:	orr	w12, w13, w12
   12e6c:	ldr	x10, [x10, #8]
   12e70:	lsl	x11, x11, #6
   12e74:	cmp	w12, #0x0
   12e78:	csel	x22, x11, x2, ne  // ne = any
   12e7c:	mov	x19, x0
   12e80:	mov	x0, x1
   12e84:	mov	x1, x21
   12e88:	mov	x2, x22
   12e8c:	mov	x29, sp
   12e90:	csinc	x20, x9, x8, eq  // eq = none
   12e94:	blr	x10
   12e98:	ands	x8, x22, #0x3f
   12e9c:	b.eq	12eb8 <__gmpf_urandomb@@Base+0x84>  // b.none
   12ea0:	mov	w9, #0x40                  	// #64
   12ea4:	sub	w3, w9, w8
   12ea8:	mov	x0, x21
   12eac:	mov	x1, x21
   12eb0:	mov	x2, x20
   12eb4:	bl	c180 <__gmpn_lshift@plt>
   12eb8:	cbz	x20, 12ee8 <__gmpf_urandomb@@Base+0xb4>
   12ebc:	add	x10, x21, x20, lsl #3
   12ec0:	mov	x9, xzr
   12ec4:	neg	x8, x20
   12ec8:	sub	x10, x10, #0x8
   12ecc:	ldr	x11, [x10, x9, lsl #3]
   12ed0:	cbnz	x11, 12ef4 <__gmpf_urandomb@@Base+0xc0>
   12ed4:	sub	x9, x9, #0x1
   12ed8:	cmn	x20, x9
   12edc:	b.ne	12ecc <__gmpf_urandomb@@Base+0x98>  // b.any
   12ee0:	mov	x10, xzr
   12ee4:	b	12efc <__gmpf_urandomb@@Base+0xc8>
   12ee8:	mov	x8, xzr
   12eec:	mov	x10, xzr
   12ef0:	b	12efc <__gmpf_urandomb@@Base+0xc8>
   12ef4:	add	x10, x20, x9
   12ef8:	mov	x8, x9
   12efc:	str	x8, [x19, #8]
   12f00:	str	w10, [x19, #4]
   12f04:	ldp	x20, x19, [sp, #32]
   12f08:	ldp	x22, x21, [sp, #16]
   12f0c:	ldp	x29, x30, [sp], #48
   12f10:	ret

0000000000012f14 <__gmpf_swap@@Base>:
   12f14:	ldr	x8, [x1]
   12f18:	ldr	x9, [x0]
   12f1c:	str	x8, [x0]
   12f20:	str	x9, [x1]
   12f24:	ldur	q0, [x1, #8]
   12f28:	ldur	q1, [x0, #8]
   12f2c:	stur	q0, [x0, #8]
   12f30:	stur	q1, [x1, #8]
   12f34:	ret

0000000000012f38 <__gmpf_fits_sint_p@@Base>:
   12f38:	ldr	x8, [x0, #8]
   12f3c:	cmp	x8, #0x1
   12f40:	b.lt	12f78 <__gmpf_fits_sint_p@@Base+0x40>  // b.tstop
   12f44:	cmp	x8, #0x1
   12f48:	b.ne	12f80 <__gmpf_fits_sint_p@@Base+0x48>  // b.any
   12f4c:	ldrsw	x8, [x0, #4]
   12f50:	ldr	x9, [x0, #16]
   12f54:	cmp	w8, #0x0
   12f58:	cneg	x8, x8, lt  // lt = tstop
   12f5c:	add	x8, x9, x8, lsl #3
   12f60:	ldur	x8, [x8, #-8]
   12f64:	mov	w9, #0x7fffffff            	// #2147483647
   12f68:	cinc	x9, x9, lt  // lt = tstop
   12f6c:	cmp	x8, x9
   12f70:	cset	w0, ls  // ls = plast
   12f74:	ret
   12f78:	mov	w0, #0x1                   	// #1
   12f7c:	ret
   12f80:	mov	w0, wzr
   12f84:	ret

0000000000012f88 <__gmpf_fits_slong_p@@Base>:
   12f88:	ldr	x8, [x0, #8]
   12f8c:	cmp	x8, #0x1
   12f90:	b.lt	12fc8 <__gmpf_fits_slong_p@@Base+0x40>  // b.tstop
   12f94:	cmp	x8, #0x1
   12f98:	b.ne	12fd0 <__gmpf_fits_slong_p@@Base+0x48>  // b.any
   12f9c:	ldrsw	x8, [x0, #4]
   12fa0:	ldr	x9, [x0, #16]
   12fa4:	cmp	w8, #0x0
   12fa8:	cneg	x8, x8, lt  // lt = tstop
   12fac:	add	x8, x9, x8, lsl #3
   12fb0:	ldur	x8, [x8, #-8]
   12fb4:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   12fb8:	cinv	x9, x9, lt  // lt = tstop
   12fbc:	cmp	x8, x9
   12fc0:	cset	w0, ls  // ls = plast
   12fc4:	ret
   12fc8:	mov	w0, #0x1                   	// #1
   12fcc:	ret
   12fd0:	mov	w0, wzr
   12fd4:	ret

0000000000012fd8 <__gmpf_fits_sshort_p@@Base>:
   12fd8:	ldr	x8, [x0, #8]
   12fdc:	cmp	x8, #0x1
   12fe0:	b.lt	13018 <__gmpf_fits_sshort_p@@Base+0x40>  // b.tstop
   12fe4:	cmp	x8, #0x1
   12fe8:	b.ne	13020 <__gmpf_fits_sshort_p@@Base+0x48>  // b.any
   12fec:	ldrsw	x8, [x0, #4]
   12ff0:	ldr	x9, [x0, #16]
   12ff4:	cmp	w8, #0x0
   12ff8:	cneg	x8, x8, lt  // lt = tstop
   12ffc:	add	x8, x9, x8, lsl #3
   13000:	ldur	x8, [x8, #-8]
   13004:	mov	w9, #0x7fff                	// #32767
   13008:	cinc	x9, x9, lt  // lt = tstop
   1300c:	cmp	x8, x9
   13010:	cset	w0, ls  // ls = plast
   13014:	ret
   13018:	mov	w0, #0x1                   	// #1
   1301c:	ret
   13020:	mov	w0, wzr
   13024:	ret

0000000000013028 <__gmpf_fits_uint_p@@Base>:
   13028:	ldr	x9, [x0, #8]
   1302c:	cmp	x9, #0x1
   13030:	b.lt	13064 <__gmpf_fits_uint_p@@Base+0x3c>  // b.tstop
   13034:	mov	x8, x0
   13038:	cmp	x9, #0x1
   1303c:	mov	w0, wzr
   13040:	b.ne	13060 <__gmpf_fits_uint_p@@Base+0x38>  // b.any
   13044:	ldr	w9, [x8, #4]
   13048:	tbnz	w9, #31, 13060 <__gmpf_fits_uint_p@@Base+0x38>
   1304c:	ldr	x8, [x8, #16]
   13050:	add	x8, x8, x9, lsl #3
   13054:	ldur	w8, [x8, #-4]
   13058:	cmp	w8, #0x0
   1305c:	cset	w0, eq  // eq = none
   13060:	ret
   13064:	mov	w0, #0x1                   	// #1
   13068:	ret

000000000001306c <__gmpf_fits_ulong_p@@Base>:
   1306c:	ldr	x8, [x0, #8]
   13070:	cmp	x8, #0x1
   13074:	b.lt	1308c <__gmpf_fits_ulong_p@@Base+0x20>  // b.tstop
   13078:	ldr	w9, [x0, #4]
   1307c:	tbnz	w9, #31, 13094 <__gmpf_fits_ulong_p@@Base+0x28>
   13080:	cmp	x8, #0x1
   13084:	cset	w0, eq  // eq = none
   13088:	ret
   1308c:	mov	w0, #0x1                   	// #1
   13090:	ret
   13094:	mov	w0, wzr
   13098:	ret

000000000001309c <__gmpf_fits_ushort_p@@Base>:
   1309c:	ldr	x9, [x0, #8]
   130a0:	cmp	x9, #0x1
   130a4:	b.lt	130d8 <__gmpf_fits_ushort_p@@Base+0x3c>  // b.tstop
   130a8:	mov	x8, x0
   130ac:	cmp	x9, #0x1
   130b0:	mov	w0, wzr
   130b4:	b.ne	130d4 <__gmpf_fits_ushort_p@@Base+0x38>  // b.any
   130b8:	ldr	w9, [x8, #4]
   130bc:	tbnz	w9, #31, 130d4 <__gmpf_fits_ushort_p@@Base+0x38>
   130c0:	ldr	x8, [x8, #16]
   130c4:	add	x8, x8, x9, lsl #3
   130c8:	ldur	x8, [x8, #-8]
   130cc:	cmp	x8, #0x10, lsl #12
   130d0:	cset	w0, cc  // cc = lo, ul, last
   130d4:	ret
   130d8:	mov	w0, #0x1                   	// #1
   130dc:	ret

00000000000130e0 <__gmpf_get_si@@Base>:
   130e0:	ldr	x9, [x0, #8]
   130e4:	cmp	x9, #0x1
   130e8:	b.lt	1311c <__gmpf_get_si@@Base+0x3c>  // b.tstop
   130ec:	ldrsw	x8, [x0, #4]
   130f0:	cmp	x8, #0x0
   130f4:	cneg	x10, x8, mi  // mi = first
   130f8:	subs	x9, x10, x9
   130fc:	b.ge	13124 <__gmpf_get_si@@Base+0x44>  // b.tcont
   13100:	mov	x9, xzr
   13104:	cmp	w8, #0x1
   13108:	b.ge	13134 <__gmpf_get_si@@Base+0x54>  // b.tcont
   1310c:	sub	x8, x9, #0x1
   13110:	orr	x8, x8, #0x8000000000000000
   13114:	eor	x0, x8, #0x7fffffffffffffff
   13118:	ret
   1311c:	mov	x0, xzr
   13120:	ret
   13124:	ldr	x10, [x0, #16]
   13128:	ldr	x9, [x10, x9, lsl #3]
   1312c:	cmp	w8, #0x1
   13130:	b.lt	1310c <__gmpf_get_si@@Base+0x2c>  // b.tstop
   13134:	and	x0, x9, #0x7fffffffffffffff
   13138:	ret

000000000001313c <__gmpf_get_ui@@Base>:
   1313c:	ldr	x8, [x0, #8]
   13140:	cmp	x8, #0x1
   13144:	b.lt	1315c <__gmpf_get_ui@@Base+0x20>  // b.tstop
   13148:	ldrsw	x9, [x0, #4]
   1314c:	cmp	x9, #0x0
   13150:	cneg	x9, x9, mi  // mi = first
   13154:	subs	x8, x9, x8
   13158:	b.ge	13164 <__gmpf_get_ui@@Base+0x28>  // b.tcont
   1315c:	mov	x0, xzr
   13160:	ret
   13164:	ldr	x9, [x0, #16]
   13168:	ldr	x0, [x9, x8, lsl #3]
   1316c:	ret

0000000000013170 <__gmpf_integer_p@@Base>:
   13170:	ldr	x8, [x0, #8]
   13174:	ldrsw	x9, [x0, #4]
   13178:	cmp	x8, #0x0
   1317c:	b.le	131b0 <__gmpf_integer_p@@Base+0x40>
   13180:	ldr	x10, [x0, #16]
   13184:	cmp	x9, #0x0
   13188:	cneg	x9, x9, mi  // mi = first
   1318c:	ldr	x11, [x10]
   13190:	cbnz	x11, 131a4 <__gmpf_integer_p@@Base+0x34>
   13194:	add	x10, x10, #0x8
   13198:	ldr	x11, [x10], #8
   1319c:	sub	x9, x9, #0x1
   131a0:	cbz	x11, 13198 <__gmpf_integer_p@@Base+0x28>
   131a4:	cmp	x9, x8
   131a8:	cset	w0, le
   131ac:	ret
   131b0:	cmp	w9, #0x0
   131b4:	cset	w0, eq  // eq = none
   131b8:	ret

00000000000131bc <__gmpz_abs@@Base>:
   131bc:	stp	x29, x30, [sp, #-48]!
   131c0:	stp	x20, x19, [sp, #32]
   131c4:	ldr	w8, [x1, #4]
   131c8:	mov	x19, x0
   131cc:	str	x21, [sp, #16]
   131d0:	mov	x29, sp
   131d4:	cmp	w8, #0x0
   131d8:	cneg	w20, w8, mi  // mi = first
   131dc:	cmp	x1, x0
   131e0:	b.eq	13204 <__gmpz_abs@@Base+0x48>  // b.none
   131e4:	ldrsw	x8, [x19]
   131e8:	mov	x21, x1
   131ec:	cmp	x20, x8
   131f0:	b.gt	13218 <__gmpz_abs@@Base+0x5c>
   131f4:	ldr	x0, [x19, #8]
   131f8:	ldr	x1, [x21, #8]
   131fc:	mov	x2, x20
   13200:	bl	ca50 <__gmpn_copyi@plt>
   13204:	str	w20, [x19, #4]
   13208:	ldp	x20, x19, [sp, #32]
   1320c:	ldr	x21, [sp, #16]
   13210:	ldp	x29, x30, [sp], #48
   13214:	ret
   13218:	mov	x0, x19
   1321c:	mov	x1, x20
   13220:	bl	c080 <__gmpz_realloc@plt>
   13224:	b	131f8 <__gmpz_abs@@Base+0x3c>

0000000000013228 <__gmpz_add@@Base>:
   13228:	stp	x29, x30, [sp, #-80]!
   1322c:	stp	x26, x25, [sp, #16]
   13230:	stp	x24, x23, [sp, #32]
   13234:	stp	x22, x21, [sp, #48]
   13238:	stp	x20, x19, [sp, #64]
   1323c:	ldrsw	x8, [x1, #4]
   13240:	ldrsw	x9, [x2, #4]
   13244:	ldrsw	x10, [x0]
   13248:	mov	x19, x0
   1324c:	cmp	x8, #0x0
   13250:	cneg	x11, x8, mi  // mi = first
   13254:	cmp	x9, #0x0
   13258:	cneg	x12, x9, mi  // mi = first
   1325c:	cmp	x11, x12
   13260:	csel	x20, x12, x11, lt  // lt = tstop
   13264:	csel	x23, x11, x12, lt  // lt = tstop
   13268:	csel	x25, x8, x9, lt  // lt = tstop
   1326c:	csel	x24, x9, x8, lt  // lt = tstop
   13270:	csel	x26, x1, x2, lt  // lt = tstop
   13274:	csel	x22, x2, x1, lt  // lt = tstop
   13278:	cmp	x20, x10
   1327c:	mov	x29, sp
   13280:	b.ge	13388 <__gmpz_add@@Base+0x160>  // b.tcont
   13284:	ldr	x21, [x19, #8]
   13288:	ldr	x22, [x22, #8]
   1328c:	ldr	x2, [x26, #8]
   13290:	eor	x8, x24, x25
   13294:	tbnz	x8, #63, 133a8 <__gmpz_add@@Base+0x180>
   13298:	cbz	x23, 132d4 <__gmpz_add@@Base+0xac>
   1329c:	mov	x0, x21
   132a0:	mov	x1, x22
   132a4:	mov	x3, x23
   132a8:	bl	ca70 <__gmpn_add_n@plt>
   132ac:	cbz	x0, 132d4 <__gmpz_add@@Base+0xac>
   132b0:	mov	w9, #0x1                   	// #1
   132b4:	cmp	x23, x20
   132b8:	b.ge	13374 <__gmpz_add@@Base+0x14c>  // b.tcont
   132bc:	lsl	x8, x23, #3
   132c0:	ldr	x10, [x22, x8]
   132c4:	add	x23, x23, #0x1
   132c8:	adds	x10, x10, #0x1
   132cc:	str	x10, [x21, x8]
   132d0:	b.cs	132b4 <__gmpz_add@@Base+0x8c>  // b.hs, b.nlast
   132d4:	cmp	x21, x22
   132d8:	mov	x9, xzr
   132dc:	b.eq	13374 <__gmpz_add@@Base+0x14c>  // b.none
   132e0:	subs	x8, x20, x23
   132e4:	b.le	13374 <__gmpz_add@@Base+0x14c>
   132e8:	cmp	x8, #0x4
   132ec:	b.cc	13350 <__gmpz_add@@Base+0x128>  // b.lo, b.ul, b.last
   132f0:	lsl	x10, x23, #3
   132f4:	lsl	x9, x20, #3
   132f8:	add	x11, x21, x10
   132fc:	add	x12, x22, x9
   13300:	cmp	x11, x12
   13304:	b.cs	13318 <__gmpz_add@@Base+0xf0>  // b.hs, b.nlast
   13308:	add	x9, x21, x9
   1330c:	add	x11, x22, x10
   13310:	cmp	x11, x9
   13314:	b.cc	13350 <__gmpz_add@@Base+0x128>  // b.lo, b.ul, b.last
   13318:	and	x9, x8, #0xfffffffffffffffc
   1331c:	add	x11, x10, #0x10
   13320:	add	x23, x23, x9
   13324:	add	x10, x22, x11
   13328:	add	x11, x21, x11
   1332c:	mov	x12, x9
   13330:	ldp	q0, q1, [x10, #-16]
   13334:	add	x10, x10, #0x20
   13338:	subs	x12, x12, #0x4
   1333c:	stp	q0, q1, [x11, #-16]
   13340:	add	x11, x11, #0x20
   13344:	b.ne	13330 <__gmpz_add@@Base+0x108>  // b.any
   13348:	cmp	x8, x9
   1334c:	b.eq	13370 <__gmpz_add@@Base+0x148>  // b.none
   13350:	lsl	x10, x23, #3
   13354:	sub	x8, x20, x23
   13358:	add	x9, x21, x10
   1335c:	add	x10, x22, x10
   13360:	ldr	x11, [x10], #8
   13364:	subs	x8, x8, #0x1
   13368:	str	x11, [x9], #8
   1336c:	b.ne	13360 <__gmpz_add@@Base+0x138>  // b.any
   13370:	mov	x9, xzr
   13374:	add	x8, x9, x20
   13378:	cmp	x24, #0x0
   1337c:	cneg	x8, x8, lt  // lt = tstop
   13380:	str	x9, [x21, x20, lsl #3]
   13384:	b	134f8 <__gmpz_add@@Base+0x2d0>
   13388:	add	x1, x20, #0x1
   1338c:	mov	x0, x19
   13390:	bl	c080 <__gmpz_realloc@plt>
   13394:	mov	x21, x0
   13398:	ldr	x22, [x22, #8]
   1339c:	ldr	x2, [x26, #8]
   133a0:	eor	x8, x24, x25
   133a4:	tbz	x8, #63, 13298 <__gmpz_add@@Base+0x70>
   133a8:	cmp	x20, x23
   133ac:	b.ne	13408 <__gmpz_add@@Base+0x1e0>  // b.any
   133b0:	sub	x8, x20, #0x1
   133b4:	add	x9, x8, #0x1
   133b8:	cmp	x9, #0x1
   133bc:	b.lt	133dc <__gmpz_add@@Base+0x1b4>  // b.tstop
   133c0:	lsl	x9, x8, #3
   133c4:	ldr	x10, [x22, x9]
   133c8:	ldr	x9, [x2, x9]
   133cc:	sub	x8, x8, #0x1
   133d0:	cmp	x10, x9
   133d4:	b.eq	133b4 <__gmpz_add@@Base+0x18c>  // b.none
   133d8:	b.ls	13514 <__gmpz_add@@Base+0x2ec>  // b.plast
   133dc:	mov	x0, x21
   133e0:	mov	x1, x22
   133e4:	mov	x3, x20
   133e8:	bl	c2d0 <__gmpn_sub_n@plt>
   133ec:	sub	x8, x21, #0x8
   133f0:	mov	x9, x20
   133f4:	subs	x20, x20, #0x1
   133f8:	b.lt	134f0 <__gmpz_add@@Base+0x2c8>  // b.tstop
   133fc:	ldr	x10, [x8, x9, lsl #3]
   13400:	cbz	x10, 133f0 <__gmpz_add@@Base+0x1c8>
   13404:	b	134f0 <__gmpz_add@@Base+0x2c8>
   13408:	cbz	x23, 13440 <__gmpz_add@@Base+0x218>
   1340c:	mov	x0, x21
   13410:	mov	x1, x22
   13414:	mov	x3, x23
   13418:	bl	c2d0 <__gmpn_sub_n@plt>
   1341c:	cbz	x0, 13440 <__gmpz_add@@Base+0x218>
   13420:	cmp	x23, x20
   13424:	b.ge	134d8 <__gmpz_add@@Base+0x2b0>  // b.tcont
   13428:	lsl	x8, x23, #3
   1342c:	ldr	x9, [x22, x8]
   13430:	add	x23, x23, #0x1
   13434:	sub	x10, x9, #0x1
   13438:	str	x10, [x21, x8]
   1343c:	cbz	x9, 13420 <__gmpz_add@@Base+0x1f8>
   13440:	cmp	x21, x22
   13444:	b.eq	134d8 <__gmpz_add@@Base+0x2b0>  // b.none
   13448:	subs	x8, x20, x23
   1344c:	b.le	134d8 <__gmpz_add@@Base+0x2b0>
   13450:	cmp	x8, #0x4
   13454:	b.cc	134b8 <__gmpz_add@@Base+0x290>  // b.lo, b.ul, b.last
   13458:	lsl	x10, x23, #3
   1345c:	lsl	x9, x20, #3
   13460:	add	x11, x21, x10
   13464:	add	x12, x22, x9
   13468:	cmp	x11, x12
   1346c:	b.cs	13480 <__gmpz_add@@Base+0x258>  // b.hs, b.nlast
   13470:	add	x9, x21, x9
   13474:	add	x11, x22, x10
   13478:	cmp	x11, x9
   1347c:	b.cc	134b8 <__gmpz_add@@Base+0x290>  // b.lo, b.ul, b.last
   13480:	and	x9, x8, #0xfffffffffffffffc
   13484:	add	x11, x10, #0x10
   13488:	add	x23, x23, x9
   1348c:	add	x10, x22, x11
   13490:	add	x11, x21, x11
   13494:	mov	x12, x9
   13498:	ldp	q0, q1, [x10, #-16]
   1349c:	add	x10, x10, #0x20
   134a0:	subs	x12, x12, #0x4
   134a4:	stp	q0, q1, [x11, #-16]
   134a8:	add	x11, x11, #0x20
   134ac:	b.ne	13498 <__gmpz_add@@Base+0x270>  // b.any
   134b0:	cmp	x8, x9
   134b4:	b.eq	134d8 <__gmpz_add@@Base+0x2b0>  // b.none
   134b8:	lsl	x10, x23, #3
   134bc:	sub	x8, x20, x23
   134c0:	add	x9, x21, x10
   134c4:	add	x10, x22, x10
   134c8:	ldr	x11, [x10], #8
   134cc:	subs	x8, x8, #0x1
   134d0:	str	x11, [x9], #8
   134d4:	b.ne	134c8 <__gmpz_add@@Base+0x2a0>  // b.any
   134d8:	sub	x8, x21, #0x8
   134dc:	mov	x9, x20
   134e0:	subs	x20, x20, #0x1
   134e4:	b.lt	134f0 <__gmpz_add@@Base+0x2c8>  // b.tstop
   134e8:	ldr	x10, [x8, x9, lsl #3]
   134ec:	cbz	x10, 134dc <__gmpz_add@@Base+0x2b4>
   134f0:	cmp	x24, #0x0
   134f4:	cneg	x8, x9, lt  // lt = tstop
   134f8:	str	w8, [x19, #4]
   134fc:	ldp	x20, x19, [sp, #64]
   13500:	ldp	x22, x21, [sp, #48]
   13504:	ldp	x24, x23, [sp, #32]
   13508:	ldp	x26, x25, [sp, #16]
   1350c:	ldp	x29, x30, [sp], #80
   13510:	ret
   13514:	mov	x0, x21
   13518:	mov	x1, x2
   1351c:	mov	x2, x22
   13520:	mov	x3, x20
   13524:	bl	c2d0 <__gmpn_sub_n@plt>
   13528:	sub	x8, x21, #0x8
   1352c:	mov	x9, x20
   13530:	subs	x20, x20, #0x1
   13534:	b.lt	13540 <__gmpz_add@@Base+0x318>  // b.tstop
   13538:	ldr	x10, [x8, x9, lsl #3]
   1353c:	cbz	x10, 1352c <__gmpz_add@@Base+0x304>
   13540:	cmp	x24, #0x0
   13544:	cneg	x8, x9, ge  // ge = tcont
   13548:	b	134f8 <__gmpz_add@@Base+0x2d0>

000000000001354c <__gmpz_add_ui@@Base>:
   1354c:	stp	x29, x30, [sp, #-64]!
   13550:	stp	x22, x21, [sp, #32]
   13554:	stp	x20, x19, [sp, #48]
   13558:	str	x23, [sp, #16]
   1355c:	ldrsw	x23, [x1, #4]
   13560:	mov	x20, x2
   13564:	mov	x19, x0
   13568:	mov	x29, sp
   1356c:	cbz	w23, 13690 <__gmpz_add_ui@@Base+0x144>
   13570:	ldrsw	x8, [x19]
   13574:	cmp	w23, #0x0
   13578:	cneg	x22, x23, lt  // lt = tstop
   1357c:	mov	x21, x1
   13580:	cmp	x22, x8
   13584:	b.ge	13760 <__gmpz_add_ui@@Base+0x214>  // b.tcont
   13588:	ldr	x0, [x19, #8]
   1358c:	ldr	x8, [x21, #8]
   13590:	tbnz	w23, #31, 13774 <__gmpz_add_ui@@Base+0x228>
   13594:	ldr	x9, [x8]
   13598:	adds	x9, x9, x20
   1359c:	str	x9, [x0]
   135a0:	b.cc	136b0 <__gmpz_add_ui@@Base+0x164>  // b.lo, b.ul, b.last
   135a4:	mov	x11, xzr
   135a8:	sub	x10, x22, #0x1
   135ac:	mov	w12, #0x1                   	// #1
   135b0:	mov	w9, #0x1                   	// #1
   135b4:	cmp	x9, x22
   135b8:	b.ge	1371c <__gmpz_add_ui@@Base+0x1d0>  // b.tcont
   135bc:	add	x13, x8, x11
   135c0:	ldr	x13, [x13, #8]
   135c4:	add	x14, x0, x11
   135c8:	add	x9, x9, #0x1
   135cc:	add	x11, x11, #0x8
   135d0:	adds	x13, x13, #0x1
   135d4:	sub	x10, x10, #0x1
   135d8:	str	x13, [x14, #8]
   135dc:	b.cs	135b4 <__gmpz_add_ui@@Base+0x68>  // b.hs, b.nlast
   135e0:	cmp	x8, x0
   135e4:	mov	x12, xzr
   135e8:	b.eq	1371c <__gmpz_add_ui@@Base+0x1d0>  // b.none
   135ec:	subs	x13, x22, x9
   135f0:	b.le	1371c <__gmpz_add_ui@@Base+0x1d0>
   135f4:	cmp	x13, #0x4
   135f8:	b.cc	1366c <__gmpz_add_ui@@Base+0x120>  // b.lo, b.ul, b.last
   135fc:	add	x14, x0, x11
   13600:	lsl	x12, x22, #3
   13604:	add	x14, x14, #0x8
   13608:	add	x15, x8, x12
   1360c:	cmp	x14, x15
   13610:	b.cs	13628 <__gmpz_add_ui@@Base+0xdc>  // b.hs, b.nlast
   13614:	add	x14, x8, x11
   13618:	add	x12, x0, x12
   1361c:	add	x14, x14, #0x8
   13620:	cmp	x14, x12
   13624:	b.cc	1366c <__gmpz_add_ui@@Base+0x120>  // b.lo, b.ul, b.last
   13628:	sub	x14, x22, x9
   1362c:	add	x12, x0, x11
   13630:	add	x15, x8, x11
   13634:	and	x16, x10, #0xfffffffffffffffc
   13638:	and	x11, x13, #0xfffffffffffffffc
   1363c:	add	x10, x12, #0x18
   13640:	add	x12, x15, #0x18
   13644:	add	x9, x16, x9
   13648:	and	x14, x14, #0xfffffffffffffffc
   1364c:	ldp	q0, q1, [x12, #-16]
   13650:	add	x12, x12, #0x20
   13654:	subs	x14, x14, #0x4
   13658:	stp	q0, q1, [x10, #-16]
   1365c:	add	x10, x10, #0x20
   13660:	b.ne	1364c <__gmpz_add_ui@@Base+0x100>  // b.any
   13664:	cmp	x13, x11
   13668:	b.eq	13718 <__gmpz_add_ui@@Base+0x1cc>  // b.none
   1366c:	lsl	x11, x9, #3
   13670:	sub	x10, x22, x9
   13674:	add	x9, x0, x11
   13678:	add	x8, x8, x11
   1367c:	ldr	x11, [x8], #8
   13680:	subs	x10, x10, #0x1
   13684:	str	x11, [x9], #8
   13688:	b.ne	1367c <__gmpz_add_ui@@Base+0x130>  // b.any
   1368c:	b	13718 <__gmpz_add_ui@@Base+0x1cc>
   13690:	ldr	w8, [x19]
   13694:	cmp	w8, #0x0
   13698:	b.le	13950 <__gmpz_add_ui@@Base+0x404>
   1369c:	ldr	x0, [x19, #8]
   136a0:	cmp	x20, #0x0
   136a4:	str	x20, [x0]
   136a8:	cset	w8, ne  // ne = any
   136ac:	b	13900 <__gmpz_add_ui@@Base+0x3b4>
   136b0:	cmp	x22, #0x2
   136b4:	mov	x12, xzr
   136b8:	b.lt	1371c <__gmpz_add_ui@@Base+0x1d0>  // b.tstop
   136bc:	cmp	x8, x0
   136c0:	b.eq	1371c <__gmpz_add_ui@@Base+0x1d0>  // b.none
   136c4:	sub	x9, x22, #0x1
   136c8:	cmp	x9, #0x4
   136cc:	b.cc	136f4 <__gmpz_add_ui@@Base+0x1a8>  // b.lo, b.ul, b.last
   136d0:	lsl	x10, x22, #3
   136d4:	add	x11, x0, #0x8
   136d8:	add	x12, x8, x10
   136dc:	cmp	x11, x12
   136e0:	b.cs	13728 <__gmpz_add_ui@@Base+0x1dc>  // b.hs, b.nlast
   136e4:	add	x10, x0, x10
   136e8:	add	x11, x8, #0x8
   136ec:	cmp	x11, x10
   136f0:	b.cs	13728 <__gmpz_add_ui@@Base+0x1dc>  // b.hs, b.nlast
   136f4:	mov	w10, #0x1                   	// #1
   136f8:	lsl	x11, x10, #3
   136fc:	sub	x9, x22, x10
   13700:	add	x10, x0, x11
   13704:	add	x8, x8, x11
   13708:	ldr	x11, [x8], #8
   1370c:	subs	x9, x9, #0x1
   13710:	str	x11, [x10], #8
   13714:	b.ne	13708 <__gmpz_add_ui@@Base+0x1bc>  // b.any
   13718:	mov	x12, xzr
   1371c:	str	x12, [x0, x22, lsl #3]
   13720:	add	x8, x12, x22
   13724:	b	13900 <__gmpz_add_ui@@Base+0x3b4>
   13728:	and	x11, x9, #0xfffffffffffffffc
   1372c:	add	x12, x8, #0x18
   13730:	orr	x10, x11, #0x1
   13734:	add	x13, x0, #0x18
   13738:	mov	x14, x11
   1373c:	ldp	q0, q1, [x12, #-16]
   13740:	add	x12, x12, #0x20
   13744:	subs	x14, x14, #0x4
   13748:	stp	q0, q1, [x13, #-16]
   1374c:	add	x13, x13, #0x20
   13750:	b.ne	1373c <__gmpz_add_ui@@Base+0x1f0>  // b.any
   13754:	cmp	x9, x11
   13758:	b.eq	13718 <__gmpz_add_ui@@Base+0x1cc>  // b.none
   1375c:	b	136f8 <__gmpz_add_ui@@Base+0x1ac>
   13760:	add	x1, x22, #0x1
   13764:	mov	x0, x19
   13768:	bl	c080 <__gmpz_realloc@plt>
   1376c:	ldr	x8, [x21, #8]
   13770:	tbz	w23, #31, 13594 <__gmpz_add_ui@@Base+0x48>
   13774:	ldr	x10, [x8]
   13778:	subs	x9, x22, #0x1
   1377c:	b.ne	13798 <__gmpz_add_ui@@Base+0x24c>  // b.any
   13780:	subs	x8, x10, x20
   13784:	b.cs	13884 <__gmpz_add_ui@@Base+0x338>  // b.hs, b.nlast
   13788:	sub	x8, x20, x10
   1378c:	str	x8, [x0]
   13790:	mov	w8, #0x1                   	// #1
   13794:	b	13900 <__gmpz_add_ui@@Base+0x3b4>
   13798:	subs	x10, x10, x20
   1379c:	str	x10, [x0]
   137a0:	b.cs	1388c <__gmpz_add_ui@@Base+0x340>  // b.hs, b.nlast
   137a4:	mov	x11, xzr
   137a8:	mov	w10, #0x1                   	// #1
   137ac:	cmp	x10, x22
   137b0:	b.ge	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.tcont
   137b4:	add	x12, x8, x11
   137b8:	ldr	x12, [x12, #8]
   137bc:	add	x13, x0, x11
   137c0:	add	x10, x10, #0x1
   137c4:	add	x11, x11, #0x8
   137c8:	sub	x14, x12, #0x1
   137cc:	sub	x9, x9, #0x1
   137d0:	str	x14, [x13, #8]
   137d4:	cbz	x12, 137ac <__gmpz_add_ui@@Base+0x260>
   137d8:	cmp	x8, x0
   137dc:	b.eq	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.none
   137e0:	subs	x12, x22, x10
   137e4:	b.le	138ec <__gmpz_add_ui@@Base+0x3a0>
   137e8:	cmp	x12, #0x4
   137ec:	b.cc	13860 <__gmpz_add_ui@@Base+0x314>  // b.lo, b.ul, b.last
   137f0:	add	x14, x0, x11
   137f4:	lsl	x13, x22, #3
   137f8:	add	x14, x14, #0x8
   137fc:	add	x15, x8, x13
   13800:	cmp	x14, x15
   13804:	b.cs	1381c <__gmpz_add_ui@@Base+0x2d0>  // b.hs, b.nlast
   13808:	add	x14, x8, x11
   1380c:	add	x13, x0, x13
   13810:	add	x14, x14, #0x8
   13814:	cmp	x14, x13
   13818:	b.cc	13860 <__gmpz_add_ui@@Base+0x314>  // b.lo, b.ul, b.last
   1381c:	sub	x14, x22, x10
   13820:	add	x13, x0, x11
   13824:	add	x15, x8, x11
   13828:	and	x16, x9, #0xfffffffffffffffc
   1382c:	and	x11, x12, #0xfffffffffffffffc
   13830:	add	x9, x13, #0x18
   13834:	add	x13, x15, #0x18
   13838:	add	x10, x16, x10
   1383c:	and	x14, x14, #0xfffffffffffffffc
   13840:	ldp	q0, q1, [x13, #-16]
   13844:	add	x13, x13, #0x20
   13848:	subs	x14, x14, #0x4
   1384c:	stp	q0, q1, [x9, #-16]
   13850:	add	x9, x9, #0x20
   13854:	b.ne	13840 <__gmpz_add_ui@@Base+0x2f4>  // b.any
   13858:	cmp	x12, x11
   1385c:	b.eq	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.none
   13860:	lsl	x11, x10, #3
   13864:	sub	x9, x22, x10
   13868:	add	x10, x0, x11
   1386c:	add	x8, x8, x11
   13870:	ldr	x11, [x8], #8
   13874:	subs	x9, x9, #0x1
   13878:	str	x11, [x10], #8
   1387c:	b.ne	13870 <__gmpz_add_ui@@Base+0x324>  // b.any
   13880:	b	138ec <__gmpz_add_ui@@Base+0x3a0>
   13884:	str	x8, [x0]
   13888:	b	138ec <__gmpz_add_ui@@Base+0x3a0>
   1388c:	cmp	x22, #0x2
   13890:	b.lt	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.tstop
   13894:	cmp	x8, x0
   13898:	b.eq	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.none
   1389c:	cmp	x9, #0x4
   138a0:	b.cc	138c8 <__gmpz_add_ui@@Base+0x37c>  // b.lo, b.ul, b.last
   138a4:	lsl	x10, x22, #3
   138a8:	add	x11, x0, #0x8
   138ac:	add	x12, x8, x10
   138b0:	cmp	x11, x12
   138b4:	b.cs	13918 <__gmpz_add_ui@@Base+0x3cc>  // b.hs, b.nlast
   138b8:	add	x10, x0, x10
   138bc:	add	x11, x8, #0x8
   138c0:	cmp	x11, x10
   138c4:	b.cs	13918 <__gmpz_add_ui@@Base+0x3cc>  // b.hs, b.nlast
   138c8:	mov	w10, #0x1                   	// #1
   138cc:	lsl	x11, x10, #3
   138d0:	sub	x9, x22, x10
   138d4:	add	x10, x0, x11
   138d8:	add	x8, x8, x11
   138dc:	ldr	x11, [x8], #8
   138e0:	subs	x9, x9, #0x1
   138e4:	str	x11, [x10], #8
   138e8:	b.ne	138dc <__gmpz_add_ui@@Base+0x390>  // b.any
   138ec:	add	x8, x0, x22, lsl #3
   138f0:	ldur	x8, [x8, #-8]
   138f4:	cmp	x8, #0x0
   138f8:	cset	w8, eq  // eq = none
   138fc:	sub	x8, x8, x22
   13900:	str	w8, [x19, #4]
   13904:	ldp	x20, x19, [sp, #48]
   13908:	ldp	x22, x21, [sp, #32]
   1390c:	ldr	x23, [sp, #16]
   13910:	ldp	x29, x30, [sp], #64
   13914:	ret
   13918:	and	x11, x9, #0xfffffffffffffffc
   1391c:	add	x12, x8, #0x18
   13920:	orr	x10, x11, #0x1
   13924:	add	x13, x0, #0x18
   13928:	mov	x14, x11
   1392c:	ldp	q0, q1, [x12, #-16]
   13930:	add	x12, x12, #0x20
   13934:	subs	x14, x14, #0x4
   13938:	stp	q0, q1, [x13, #-16]
   1393c:	add	x13, x13, #0x20
   13940:	b.ne	1392c <__gmpz_add_ui@@Base+0x3e0>  // b.any
   13944:	cmp	x9, x11
   13948:	b.eq	138ec <__gmpz_add_ui@@Base+0x3a0>  // b.none
   1394c:	b	138cc <__gmpz_add_ui@@Base+0x380>
   13950:	mov	w1, #0x1                   	// #1
   13954:	mov	x0, x19
   13958:	bl	c080 <__gmpz_realloc@plt>
   1395c:	b	136a0 <__gmpz_add_ui@@Base+0x154>

0000000000013960 <__gmpz_addmul@@Base>:
   13960:	mov	x3, xzr
   13964:	b	13968 <__gmpz_addmul@@Base+0x8>
   13968:	stp	x29, x30, [sp, #-96]!
   1396c:	stp	x28, x27, [sp, #16]
   13970:	stp	x26, x25, [sp, #32]
   13974:	stp	x24, x23, [sp, #48]
   13978:	stp	x22, x21, [sp, #64]
   1397c:	stp	x20, x19, [sp, #80]
   13980:	mov	x29, sp
   13984:	sub	sp, sp, #0x10
   13988:	ldrsw	x8, [x1, #4]
   1398c:	cbz	w8, 13d70 <__gmpz_addmul@@Base+0x410>
   13990:	ldr	w9, [x2, #4]
   13994:	cbz	w9, 13d70 <__gmpz_addmul@@Base+0x410>
   13998:	sxtw	x9, w9
   1399c:	cmp	x9, #0x0
   139a0:	cneg	x10, x9, mi  // mi = first
   139a4:	cmp	x8, #0x0
   139a8:	cneg	x11, x8, mi  // mi = first
   139ac:	cmp	x10, x11
   139b0:	csel	x10, x8, x9, gt
   139b4:	csel	x8, x9, x8, gt
   139b8:	csel	x28, x1, x2, gt
   139bc:	csel	x23, x2, x1, gt
   139c0:	cmp	x10, #0x0
   139c4:	cneg	x22, x10, mi  // mi = first
   139c8:	mov	x19, x0
   139cc:	cmp	x22, #0x1
   139d0:	eor	x3, x10, x3
   139d4:	b.ne	139f0 <__gmpz_addmul@@Base+0x90>  // b.any
   139d8:	ldr	x8, [x28, #8]
   139dc:	mov	x0, x19
   139e0:	mov	x1, x23
   139e4:	ldr	x2, [x8]
   139e8:	bl	cf80 <__gmpz_aorsmul_1@plt>
   139ec:	b	13d70 <__gmpz_addmul@@Base+0x410>
   139f0:	ldpsw	x10, x11, [x19]
   139f4:	cmp	x8, #0x0
   139f8:	cneg	x24, x8, mi  // mi = first
   139fc:	add	x27, x24, x22
   13a00:	cmp	x11, #0x0
   13a04:	cneg	x26, x11, mi  // mi = first
   13a08:	cmp	x26, x27
   13a0c:	csel	x9, x26, x27, gt
   13a10:	cmp	x9, x10
   13a14:	eor	x21, x3, x8
   13a18:	b.ge	13d1c <__gmpz_addmul@@Base+0x3bc>  // b.tcont
   13a1c:	ldr	x20, [x19, #8]
   13a20:	eor	x25, x21, x11
   13a24:	cbz	w11, 13d3c <__gmpz_addmul@@Base+0x3dc>
   13a28:	cmp	x27, #0xfe0
   13a2c:	lsl	x1, x27, #3
   13a30:	stp	x11, xzr, [x29, #-16]
   13a34:	b.hi	13d90 <__gmpz_addmul@@Base+0x430>  // b.pmore
   13a38:	add	x9, x1, #0xf
   13a3c:	mov	x8, sp
   13a40:	and	x9, x9, #0xfffffffffffffff0
   13a44:	sub	x21, x8, x9
   13a48:	mov	sp, x21
   13a4c:	ldr	x1, [x23, #8]
   13a50:	ldr	x3, [x28, #8]
   13a54:	mov	x0, x21
   13a58:	mov	x2, x24
   13a5c:	mov	x4, x22
   13a60:	bl	ccd0 <__gmpn_mul@plt>
   13a64:	cmp	x0, #0x0
   13a68:	cset	w8, eq  // eq = none
   13a6c:	sub	x8, x27, x8
   13a70:	cmp	x26, x8
   13a74:	tbnz	x25, #63, 13b80 <__gmpz_addmul@@Base+0x220>
   13a78:	csel	x23, x26, x8, lt  // lt = tstop
   13a7c:	csel	x24, x8, x26, lt  // lt = tstop
   13a80:	csel	x22, x21, x20, lt  // lt = tstop
   13a84:	cbz	x23, 13ad0 <__gmpz_addmul@@Base+0x170>
   13a88:	cmp	x26, x8
   13a8c:	csel	x2, x20, x21, lt  // lt = tstop
   13a90:	mov	x0, x20
   13a94:	mov	x1, x22
   13a98:	mov	x3, x23
   13a9c:	bl	ca70 <__gmpn_add_n@plt>
   13aa0:	cbz	x0, 13ad0 <__gmpz_addmul@@Base+0x170>
   13aa4:	ldur	x13, [x29, #-16]
   13aa8:	mov	w9, #0x1                   	// #1
   13aac:	cmp	x23, x24
   13ab0:	b.ge	13b74 <__gmpz_addmul@@Base+0x214>  // b.tcont
   13ab4:	lsl	x8, x23, #3
   13ab8:	ldr	x10, [x22, x8]
   13abc:	add	x23, x23, #0x1
   13ac0:	adds	x10, x10, #0x1
   13ac4:	str	x10, [x20, x8]
   13ac8:	b.cs	13aac <__gmpz_addmul@@Base+0x14c>  // b.hs, b.nlast
   13acc:	b	13ad4 <__gmpz_addmul@@Base+0x174>
   13ad0:	ldur	x13, [x29, #-16]
   13ad4:	cmp	x20, x22
   13ad8:	mov	x9, xzr
   13adc:	b.eq	13b74 <__gmpz_addmul@@Base+0x214>  // b.none
   13ae0:	subs	x8, x24, x23
   13ae4:	b.le	13b74 <__gmpz_addmul@@Base+0x214>
   13ae8:	cmp	x8, #0x4
   13aec:	b.cc	13b50 <__gmpz_addmul@@Base+0x1f0>  // b.lo, b.ul, b.last
   13af0:	lsl	x10, x23, #3
   13af4:	lsl	x9, x24, #3
   13af8:	add	x11, x20, x10
   13afc:	add	x12, x22, x9
   13b00:	cmp	x11, x12
   13b04:	b.cs	13b18 <__gmpz_addmul@@Base+0x1b8>  // b.hs, b.nlast
   13b08:	add	x9, x20, x9
   13b0c:	add	x11, x22, x10
   13b10:	cmp	x11, x9
   13b14:	b.cc	13b50 <__gmpz_addmul@@Base+0x1f0>  // b.lo, b.ul, b.last
   13b18:	and	x9, x8, #0xfffffffffffffffc
   13b1c:	add	x11, x10, #0x10
   13b20:	add	x23, x23, x9
   13b24:	add	x10, x22, x11
   13b28:	add	x11, x20, x11
   13b2c:	mov	x12, x9
   13b30:	ldp	q0, q1, [x10, #-16]
   13b34:	add	x10, x10, #0x20
   13b38:	subs	x12, x12, #0x4
   13b3c:	stp	q0, q1, [x11, #-16]
   13b40:	add	x11, x11, #0x20
   13b44:	b.ne	13b30 <__gmpz_addmul@@Base+0x1d0>  // b.any
   13b48:	cmp	x8, x9
   13b4c:	b.eq	13b70 <__gmpz_addmul@@Base+0x210>  // b.none
   13b50:	lsl	x10, x23, #3
   13b54:	sub	x8, x24, x23
   13b58:	add	x9, x20, x10
   13b5c:	add	x10, x22, x10
   13b60:	ldr	x11, [x10], #8
   13b64:	subs	x8, x8, #0x1
   13b68:	str	x11, [x9], #8
   13b6c:	b.ne	13b60 <__gmpz_addmul@@Base+0x200>  // b.any
   13b70:	mov	x9, xzr
   13b74:	str	x9, [x20, x24, lsl #3]
   13b78:	add	x8, x9, x24
   13b7c:	b	13ce8 <__gmpz_addmul@@Base+0x388>
   13b80:	ldur	x13, [x29, #-16]
   13b84:	neg	x9, x13
   13b88:	b.ge	13ba4 <__gmpz_addmul@@Base+0x244>  // b.tcont
   13b8c:	mov	x22, x26
   13b90:	mov	x13, x9
   13b94:	mov	x2, x20
   13b98:	mov	x26, x8
   13b9c:	cbnz	x22, 13bfc <__gmpz_addmul@@Base+0x29c>
   13ba0:	b	13c38 <__gmpz_addmul@@Base+0x2d8>
   13ba4:	b.ne	13bec <__gmpz_addmul@@Base+0x28c>  // b.any
   13ba8:	sub	x8, x21, #0x8
   13bac:	mov	x10, x26
   13bb0:	subs	x11, x10, #0x1
   13bb4:	b.lt	13bd8 <__gmpz_addmul@@Base+0x278>  // b.tstop
   13bb8:	lsl	x10, x10, #3
   13bbc:	add	x12, x20, x10
   13bc0:	ldur	x12, [x12, #-8]
   13bc4:	ldr	x10, [x8, x10]
   13bc8:	cmp	x12, x10
   13bcc:	mov	x10, x11
   13bd0:	b.eq	13bb0 <__gmpz_addmul@@Base+0x250>  // b.none
   13bd4:	b.ls	13d08 <__gmpz_addmul@@Base+0x3a8>  // b.plast
   13bd8:	mov	x22, x26
   13bdc:	mov	x2, x21
   13be0:	mov	x21, x20
   13be4:	cbnz	x22, 13bfc <__gmpz_addmul@@Base+0x29c>
   13be8:	b	13c38 <__gmpz_addmul@@Base+0x2d8>
   13bec:	mov	x22, x8
   13bf0:	mov	x2, x21
   13bf4:	mov	x21, x20
   13bf8:	cbz	x22, 13c38 <__gmpz_addmul@@Base+0x2d8>
   13bfc:	mov	x0, x20
   13c00:	mov	x1, x21
   13c04:	mov	x3, x22
   13c08:	mov	x23, x13
   13c0c:	bl	c2d0 <__gmpn_sub_n@plt>
   13c10:	mov	x13, x23
   13c14:	cbz	x0, 13c38 <__gmpz_addmul@@Base+0x2d8>
   13c18:	cmp	x22, x26
   13c1c:	b.ge	13cd0 <__gmpz_addmul@@Base+0x370>  // b.tcont
   13c20:	lsl	x8, x22, #3
   13c24:	ldr	x9, [x21, x8]
   13c28:	add	x22, x22, #0x1
   13c2c:	sub	x10, x9, #0x1
   13c30:	str	x10, [x20, x8]
   13c34:	cbz	x9, 13c18 <__gmpz_addmul@@Base+0x2b8>
   13c38:	cmp	x20, x21
   13c3c:	b.eq	13cd0 <__gmpz_addmul@@Base+0x370>  // b.none
   13c40:	subs	x8, x26, x22
   13c44:	b.le	13cd0 <__gmpz_addmul@@Base+0x370>
   13c48:	cmp	x8, #0x4
   13c4c:	b.cc	13cb0 <__gmpz_addmul@@Base+0x350>  // b.lo, b.ul, b.last
   13c50:	lsl	x10, x22, #3
   13c54:	lsl	x9, x26, #3
   13c58:	add	x11, x20, x10
   13c5c:	add	x12, x21, x9
   13c60:	cmp	x11, x12
   13c64:	b.cs	13c78 <__gmpz_addmul@@Base+0x318>  // b.hs, b.nlast
   13c68:	add	x9, x20, x9
   13c6c:	add	x11, x21, x10
   13c70:	cmp	x11, x9
   13c74:	b.cc	13cb0 <__gmpz_addmul@@Base+0x350>  // b.lo, b.ul, b.last
   13c78:	and	x9, x8, #0xfffffffffffffffc
   13c7c:	add	x11, x10, #0x10
   13c80:	add	x22, x22, x9
   13c84:	add	x10, x21, x11
   13c88:	add	x11, x20, x11
   13c8c:	mov	x12, x9
   13c90:	ldp	q0, q1, [x10, #-16]
   13c94:	add	x10, x10, #0x20
   13c98:	subs	x12, x12, #0x4
   13c9c:	stp	q0, q1, [x11, #-16]
   13ca0:	add	x11, x11, #0x20
   13ca4:	b.ne	13c90 <__gmpz_addmul@@Base+0x330>  // b.any
   13ca8:	cmp	x8, x9
   13cac:	b.eq	13cd0 <__gmpz_addmul@@Base+0x370>  // b.none
   13cb0:	lsl	x10, x22, #3
   13cb4:	sub	x8, x26, x22
   13cb8:	add	x9, x20, x10
   13cbc:	add	x10, x21, x10
   13cc0:	ldr	x11, [x10], #8
   13cc4:	subs	x8, x8, #0x1
   13cc8:	str	x11, [x9], #8
   13ccc:	b.ne	13cc0 <__gmpz_addmul@@Base+0x360>  // b.any
   13cd0:	sub	x9, x20, #0x8
   13cd4:	mov	x8, x26
   13cd8:	subs	x26, x26, #0x1
   13cdc:	b.lt	13ce8 <__gmpz_addmul@@Base+0x388>  // b.tstop
   13ce0:	ldr	x10, [x9, x8, lsl #3]
   13ce4:	cbz	x10, 13cd4 <__gmpz_addmul@@Base+0x374>
   13ce8:	neg	w9, w8
   13cec:	cmp	x13, #0x0
   13cf0:	csel	x8, x8, x9, ge  // ge = tcont
   13cf4:	str	w8, [x19, #4]
   13cf8:	ldur	x0, [x29, #-8]
   13cfc:	cbz	x0, 13d70 <__gmpz_addmul@@Base+0x410>
   13d00:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   13d04:	b	13d70 <__gmpz_addmul@@Base+0x410>
   13d08:	mov	x22, x26
   13d0c:	mov	x13, x9
   13d10:	mov	x2, x20
   13d14:	cbnz	x22, 13bfc <__gmpz_addmul@@Base+0x29c>
   13d18:	b	13c38 <__gmpz_addmul@@Base+0x2d8>
   13d1c:	add	x1, x9, #0x1
   13d20:	mov	x0, x19
   13d24:	mov	x20, x11
   13d28:	bl	c080 <__gmpz_realloc@plt>
   13d2c:	mov	x11, x20
   13d30:	mov	x20, x0
   13d34:	eor	x25, x21, x11
   13d38:	cbnz	w11, 13a28 <__gmpz_addmul@@Base+0xc8>
   13d3c:	ldr	x1, [x23, #8]
   13d40:	ldr	x3, [x28, #8]
   13d44:	mov	x0, x20
   13d48:	mov	x2, x24
   13d4c:	mov	x4, x22
   13d50:	bl	ccd0 <__gmpn_mul@plt>
   13d54:	cmp	x0, #0x0
   13d58:	cset	w8, eq  // eq = none
   13d5c:	sub	x8, x27, x8
   13d60:	neg	w9, w8
   13d64:	cmp	x25, #0x0
   13d68:	csel	x8, x8, x9, ge  // ge = tcont
   13d6c:	str	w8, [x19, #4]
   13d70:	mov	sp, x29
   13d74:	ldp	x20, x19, [sp, #80]
   13d78:	ldp	x22, x21, [sp, #64]
   13d7c:	ldp	x24, x23, [sp, #48]
   13d80:	ldp	x26, x25, [sp, #32]
   13d84:	ldp	x28, x27, [sp, #16]
   13d88:	ldp	x29, x30, [sp], #96
   13d8c:	ret
   13d90:	sub	x0, x29, #0x8
   13d94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   13d98:	mov	x21, x0
   13d9c:	b	13a4c <__gmpz_addmul@@Base+0xec>

0000000000013da0 <__gmpz_submul@@Base>:
   13da0:	mov	x3, #0xffffffffffffffff    	// #-1
   13da4:	b	13968 <__gmpz_addmul@@Base+0x8>

0000000000013da8 <__gmpz_aorsmul_1@@Base>:
   13da8:	sub	sp, sp, #0x70
   13dac:	stp	x29, x30, [sp, #16]
   13db0:	stp	x28, x27, [sp, #32]
   13db4:	stp	x26, x25, [sp, #48]
   13db8:	stp	x24, x23, [sp, #64]
   13dbc:	stp	x22, x21, [sp, #80]
   13dc0:	stp	x20, x19, [sp, #96]
   13dc4:	add	x29, sp, #0x10
   13dc8:	cbz	x2, 140b4 <__gmpz_aorsmul_1@@Base+0x30c>
   13dcc:	ldr	w8, [x1, #4]
   13dd0:	mov	x26, x1
   13dd4:	cbz	w8, 140b4 <__gmpz_aorsmul_1@@Base+0x30c>
   13dd8:	ldrsw	x25, [x0, #4]
   13ddc:	sxtw	x8, w8
   13de0:	cmp	x8, #0x0
   13de4:	mov	x22, x2
   13de8:	mov	x27, x0
   13dec:	eor	x21, x8, x3
   13df0:	cneg	x24, x8, mi  // mi = first
   13df4:	cbz	w25, 13e74 <__gmpz_aorsmul_1@@Base+0xcc>
   13df8:	cmp	x25, #0x0
   13dfc:	ldrsw	x8, [x27]
   13e00:	cneg	x23, x25, mi  // mi = first
   13e04:	cmp	x24, x23
   13e08:	csel	x20, x23, x24, lt  // lt = tstop
   13e0c:	eor	x19, x21, x25
   13e10:	cmp	x20, x8
   13e14:	add	x8, x20, #0x1
   13e18:	str	x27, [sp, #8]
   13e1c:	b.ge	13f0c <__gmpz_aorsmul_1@@Base+0x164>  // b.tcont
   13e20:	ldr	x21, [x27, #8]
   13e24:	ldr	x26, [x26, #8]
   13e28:	subs	x27, x24, x23
   13e2c:	csel	x28, x23, x24, gt
   13e30:	tbnz	x19, #63, 13f34 <__gmpz_aorsmul_1@@Base+0x18c>
   13e34:	mov	x0, x21
   13e38:	mov	x1, x26
   13e3c:	mov	x2, x28
   13e40:	mov	x3, x22
   13e44:	bl	d400 <__gmpn_addmul_1@plt>
   13e48:	mov	x4, x0
   13e4c:	cmp	x27, #0x1
   13e50:	add	x21, x21, x28, lsl #3
   13e54:	b.lt	13eac <__gmpz_aorsmul_1@@Base+0x104>  // b.tstop
   13e58:	add	x1, x26, x28, lsl #3
   13e5c:	mov	x0, x21
   13e60:	mov	x2, x27
   13e64:	mov	x3, x22
   13e68:	bl	d240 <__gmpn_mul_1c@plt>
   13e6c:	mov	x4, x0
   13e70:	b	13ef8 <__gmpz_aorsmul_1@@Base+0x150>
   13e74:	ldrsw	x8, [x27]
   13e78:	cmp	x24, x8
   13e7c:	b.ge	140d4 <__gmpz_aorsmul_1@@Base+0x32c>  // b.tcont
   13e80:	ldr	x20, [x27, #8]
   13e84:	ldr	x1, [x26, #8]
   13e88:	mov	x0, x20
   13e8c:	mov	x2, x24
   13e90:	mov	x3, x22
   13e94:	bl	d490 <__gmpn_mul_1@plt>
   13e98:	cmp	x0, #0x0
   13e9c:	str	x0, [x20, x24, lsl #3]
   13ea0:	cinc	x8, x24, ne  // ne = any
   13ea4:	mov	x25, x21
   13ea8:	b	140a4 <__gmpz_aorsmul_1@@Base+0x2fc>
   13eac:	tbnz	x27, #63, 13eb8 <__gmpz_aorsmul_1@@Base+0x110>
   13eb0:	mov	x27, xzr
   13eb4:	b	13ef8 <__gmpz_aorsmul_1@@Base+0x150>
   13eb8:	ldr	x8, [x21]
   13ebc:	neg	x27, x27
   13ec0:	adds	x8, x8, x4
   13ec4:	str	x8, [x21]
   13ec8:	b.cc	13ef4 <__gmpz_aorsmul_1@@Base+0x14c>  // b.lo, b.ul, b.last
   13ecc:	mov	w4, #0x1                   	// #1
   13ed0:	mov	w8, #0x1                   	// #1
   13ed4:	cmp	x8, x27
   13ed8:	b.ge	13ef8 <__gmpz_aorsmul_1@@Base+0x150>  // b.tcont
   13edc:	lsl	x9, x8, #3
   13ee0:	ldr	x10, [x21, x9]
   13ee4:	add	x8, x8, #0x1
   13ee8:	adds	x10, x10, #0x1
   13eec:	str	x10, [x21, x9]
   13ef0:	b.cs	13ed4 <__gmpz_aorsmul_1@@Base+0x12c>  // b.hs, b.nlast
   13ef4:	mov	x4, xzr
   13ef8:	str	x4, [x21, x27, lsl #3]
   13efc:	ldr	x27, [sp, #8]
   13f00:	cmp	x4, #0x0
   13f04:	cinc	x8, x20, ne  // ne = any
   13f08:	b	140a4 <__gmpz_aorsmul_1@@Base+0x2fc>
   13f0c:	mov	x0, x27
   13f10:	mov	x1, x8
   13f14:	mov	x21, x8
   13f18:	bl	c080 <__gmpz_realloc@plt>
   13f1c:	mov	x8, x21
   13f20:	mov	x21, x0
   13f24:	ldr	x26, [x26, #8]
   13f28:	subs	x27, x24, x23
   13f2c:	csel	x28, x23, x24, gt
   13f30:	tbz	x19, #63, 13e34 <__gmpz_aorsmul_1@@Base+0x8c>
   13f34:	mov	x0, x21
   13f38:	mov	x1, x26
   13f3c:	mov	x2, x28
   13f40:	mov	x3, x22
   13f44:	str	x8, [sp]
   13f48:	neg	x19, x25
   13f4c:	bl	c9e0 <__gmpn_submul_1@plt>
   13f50:	subs	x28, x24, x23
   13f54:	mov	x27, x0
   13f58:	b.le	13fb0 <__gmpz_aorsmul_1@@Base+0x208>
   13f5c:	mov	x0, x21
   13f60:	mov	x1, x21
   13f64:	mov	x2, x23
   13f68:	bl	c290 <__gmpn_com@plt>
   13f6c:	ldr	x8, [x21]
   13f70:	adds	x8, x8, #0x1
   13f74:	str	x8, [x21]
   13f78:	b.cc	13fa4 <__gmpz_aorsmul_1@@Base+0x1fc>  // b.lo, b.ul, b.last
   13f7c:	mov	w8, #0x1                   	// #1
   13f80:	mov	w9, #0x1                   	// #1
   13f84:	cmp	x9, x23
   13f88:	b.ge	14034 <__gmpz_aorsmul_1@@Base+0x28c>  // b.tcont
   13f8c:	lsl	x10, x9, #3
   13f90:	ldr	x11, [x21, x10]
   13f94:	add	x9, x9, #0x1
   13f98:	adds	x11, x11, #0x1
   13f9c:	str	x11, [x21, x10]
   13fa0:	b.cs	13f84 <__gmpz_aorsmul_1@@Base+0x1dc>  // b.hs, b.nlast
   13fa4:	mov	x25, x19
   13fa8:	mov	x8, xzr
   13fac:	b	14038 <__gmpz_aorsmul_1@@Base+0x290>
   13fb0:	b.ne	13ff0 <__gmpz_aorsmul_1@@Base+0x248>  // b.any
   13fb4:	cbz	x27, 14070 <__gmpz_aorsmul_1@@Base+0x2c8>
   13fb8:	sub	x8, x27, #0x1
   13fbc:	mov	x0, x21
   13fc0:	mov	x1, x21
   13fc4:	mov	x2, x20
   13fc8:	str	x8, [x21, x20, lsl #3]
   13fcc:	bl	c290 <__gmpn_com@plt>
   13fd0:	mov	x8, x21
   13fd4:	ldr	x9, [x8]
   13fd8:	adds	x9, x9, #0x1
   13fdc:	str	x9, [x8], #8
   13fe0:	b.cs	13fd4 <__gmpz_aorsmul_1@@Base+0x22c>  // b.hs, b.nlast
   13fe4:	ldp	x20, x27, [sp]
   13fe8:	mov	x25, x19
   13fec:	b	1408c <__gmpz_aorsmul_1@@Base+0x2e4>
   13ff0:	add	x8, x21, x24, lsl #3
   13ff4:	ldr	x9, [x8]
   13ff8:	subs	x9, x9, x27
   13ffc:	str	x9, [x8]
   14000:	b.cs	14070 <__gmpz_aorsmul_1@@Base+0x2c8>  // b.hs, b.nlast
   14004:	sub	x9, x23, x24
   14008:	mov	w27, #0x1                   	// #1
   1400c:	mov	w10, #0x1                   	// #1
   14010:	cmp	x10, x9
   14014:	b.ge	13fb8 <__gmpz_aorsmul_1@@Base+0x210>  // b.tcont
   14018:	lsl	x11, x10, #3
   1401c:	ldr	x12, [x8, x11]
   14020:	add	x10, x10, #0x1
   14024:	sub	x13, x12, #0x1
   14028:	str	x13, [x8, x11]
   1402c:	cbz	x12, 14010 <__gmpz_aorsmul_1@@Base+0x268>
   14030:	b	14070 <__gmpz_aorsmul_1@@Base+0x2c8>
   14034:	mov	x25, x19
   14038:	adds	x19, x8, x27
   1403c:	lsl	x8, x23, #3
   14040:	cinc	x9, x19, eq  // eq = none
   14044:	add	x23, x21, x8
   14048:	sub	x4, x9, #0x1
   1404c:	add	x1, x26, x8
   14050:	mov	x0, x23
   14054:	mov	x2, x28
   14058:	mov	x3, x22
   1405c:	bl	d240 <__gmpn_mul_1c@plt>
   14060:	cmp	x0, #0x0
   14064:	str	x0, [x21, x20, lsl #3]
   14068:	cinc	x20, x20, ne  // ne = any
   1406c:	cbz	x19, 14078 <__gmpz_aorsmul_1@@Base+0x2d0>
   14070:	ldr	x27, [sp, #8]
   14074:	b	1408c <__gmpz_aorsmul_1@@Base+0x2e4>
   14078:	ldr	x27, [sp, #8]
   1407c:	ldr	x8, [x23]
   14080:	sub	x9, x8, #0x1
   14084:	str	x9, [x23], #8
   14088:	cbz	x8, 1407c <__gmpz_aorsmul_1@@Base+0x2d4>
   1408c:	sub	x9, x21, #0x8
   14090:	mov	x8, x20
   14094:	subs	x20, x20, #0x1
   14098:	b.lt	140a4 <__gmpz_aorsmul_1@@Base+0x2fc>  // b.tstop
   1409c:	ldr	x10, [x9, x8, lsl #3]
   140a0:	cbz	x10, 14090 <__gmpz_aorsmul_1@@Base+0x2e8>
   140a4:	neg	w9, w8
   140a8:	cmp	x25, #0x0
   140ac:	csel	x8, x8, x9, ge  // ge = tcont
   140b0:	str	w8, [x27, #4]
   140b4:	ldp	x20, x19, [sp, #96]
   140b8:	ldp	x22, x21, [sp, #80]
   140bc:	ldp	x24, x23, [sp, #64]
   140c0:	ldp	x26, x25, [sp, #48]
   140c4:	ldp	x28, x27, [sp, #32]
   140c8:	ldp	x29, x30, [sp, #16]
   140cc:	add	sp, sp, #0x70
   140d0:	ret
   140d4:	add	x1, x24, #0x1
   140d8:	mov	x0, x27
   140dc:	bl	c080 <__gmpz_realloc@plt>
   140e0:	mov	x20, x0
   140e4:	b	13e84 <__gmpz_aorsmul_1@@Base+0xdc>

00000000000140e8 <__gmpz_addmul_ui@@Base>:
   140e8:	mov	x3, xzr
   140ec:	b	cf80 <__gmpz_aorsmul_1@plt>

00000000000140f0 <__gmpz_submul_ui@@Base>:
   140f0:	mov	x3, #0xffffffffffffffff    	// #-1
   140f4:	b	cf80 <__gmpz_aorsmul_1@plt>

00000000000140f8 <__gmpz_and@@Base>:
   140f8:	stp	x29, x30, [sp, #-96]!
   140fc:	stp	x26, x25, [sp, #32]
   14100:	stp	x24, x23, [sp, #48]
   14104:	stp	x22, x21, [sp, #64]
   14108:	stp	x20, x19, [sp, #80]
   1410c:	ldr	w8, [x1, #4]
   14110:	ldr	w9, [x2, #4]
   14114:	str	x27, [sp, #16]
   14118:	mov	x19, x0
   1411c:	mov	x29, sp
   14120:	cmp	w8, w9
   14124:	csel	x10, x1, x2, lt  // lt = tstop
   14128:	csel	x11, x2, x1, lt  // lt = tstop
   1412c:	ldr	x23, [x11, #8]
   14130:	ldr	x22, [x10, #8]
   14134:	csel	w10, w8, w9, lt  // lt = tstop
   14138:	sxtw	x27, w10
   1413c:	csel	w8, w9, w8, lt  // lt = tstop
   14140:	tbnz	w10, #31, 141b8 <__gmpz_and@@Base+0xc0>
   14144:	sub	x9, x27, #0x1
   14148:	add	w8, w27, #0x1
   1414c:	add	x10, x9, #0x1
   14150:	cmp	x10, #0x1
   14154:	b.lt	1425c <__gmpz_and@@Base+0x164>  // b.tstop
   14158:	lsl	x10, x9, #3
   1415c:	ldr	x11, [x23, x10]
   14160:	ldr	x10, [x22, x10]
   14164:	sub	x9, x9, #0x1
   14168:	sub	w8, w8, #0x1
   1416c:	and	x10, x10, x11
   14170:	cbz	x10, 1414c <__gmpz_and@@Base+0x54>
   14174:	ldrsw	x10, [x19]
   14178:	add	x20, x9, #0x2
   1417c:	str	w8, [x19, #4]
   14180:	cmp	x20, x10
   14184:	b.gt	14698 <__gmpz_and@@Base+0x5a0>
   14188:	ldr	x0, [x19, #8]
   1418c:	mov	x1, x23
   14190:	mov	x2, x22
   14194:	mov	x3, x20
   14198:	mov	sp, x29
   1419c:	ldp	x20, x19, [sp, #80]
   141a0:	ldp	x22, x21, [sp, #64]
   141a4:	ldp	x24, x23, [sp, #48]
   141a8:	ldp	x26, x25, [sp, #32]
   141ac:	ldr	x27, [sp, #16]
   141b0:	ldp	x29, x30, [sp], #96
   141b4:	b	c270 <__gmpn_and_n@plt>
   141b8:	sxtw	x20, w8
   141bc:	neg	x21, x27
   141c0:	str	xzr, [x29, #24]
   141c4:	tbnz	w20, #31, 14264 <__gmpz_and@@Base+0x16c>
   141c8:	cmp	x21, #0xfe0
   141cc:	lsl	x25, x21, #3
   141d0:	b.hi	146a8 <__gmpz_and@@Base+0x5b0>  // b.pmore
   141d4:	add	x9, x25, #0xf
   141d8:	mov	x8, sp
   141dc:	and	x9, x9, #0xfffffffffffffff0
   141e0:	sub	x24, x8, x9
   141e4:	mov	sp, x24
   141e8:	ldr	x8, [x22]
   141ec:	sub	x9, x8, #0x1
   141f0:	str	x9, [x24]
   141f4:	cbz	x8, 146c8 <__gmpz_and@@Base+0x5d0>
   141f8:	cmn	w27, #0x2
   141fc:	b.gt	147a8 <__gmpz_and@@Base+0x6b0>
   14200:	cmp	x22, x24
   14204:	b.eq	147a8 <__gmpz_and@@Base+0x6b0>  // b.none
   14208:	cmn	w27, #0x5
   1420c:	b.hi	14230 <__gmpz_and@@Base+0x138>  // b.pmore
   14210:	add	x8, x24, #0x8
   14214:	add	x9, x22, x21, lsl #3
   14218:	cmp	x8, x9
   1421c:	b.cs	143ec <__gmpz_and@@Base+0x2f4>  // b.hs, b.nlast
   14220:	sub	x8, x24, x27, lsl #3
   14224:	add	x9, x22, #0x8
   14228:	cmp	x8, x9
   1422c:	b.ls	143ec <__gmpz_and@@Base+0x2f4>  // b.plast
   14230:	mov	w8, #0x1                   	// #1
   14234:	add	x9, x8, x27
   14238:	lsl	x10, x8, #3
   1423c:	neg	x8, x9
   14240:	add	x9, x24, x10
   14244:	add	x10, x22, x10
   14248:	ldr	x11, [x10], #8
   1424c:	subs	x8, x8, #0x1
   14250:	str	x11, [x9], #8
   14254:	b.ne	14248 <__gmpz_and@@Base+0x150>  // b.any
   14258:	b	147a8 <__gmpz_and@@Base+0x6b0>
   1425c:	str	wzr, [x19, #4]
   14260:	b	14848 <__gmpz_and@@Base+0x750>
   14264:	add	x8, x20, x27
   14268:	neg	x1, x8, lsl #3
   1426c:	mov	w8, #0x7f00                	// #32512
   14270:	cmp	x1, x8
   14274:	neg	x24, x20
   14278:	b.hi	14870 <__gmpz_and@@Base+0x778>  // b.pmore
   1427c:	add	x9, x1, #0xf
   14280:	mov	x8, sp
   14284:	and	x9, x9, #0xfffffffffffffff0
   14288:	sub	x25, x8, x9
   1428c:	mov	sp, x25
   14290:	ldr	x8, [x23]
   14294:	add	x26, x25, x24, lsl #3
   14298:	sub	x9, x8, #0x1
   1429c:	str	x9, [x25]
   142a0:	cbz	x8, 14308 <__gmpz_and@@Base+0x210>
   142a4:	cmn	w20, #0x2
   142a8:	b.gt	14460 <__gmpz_and@@Base+0x368>
   142ac:	cmp	x23, x25
   142b0:	b.eq	14460 <__gmpz_and@@Base+0x368>  // b.none
   142b4:	cmn	w20, #0x5
   142b8:	b.hi	142dc <__gmpz_and@@Base+0x1e4>  // b.pmore
   142bc:	add	x8, x25, #0x8
   142c0:	add	x9, x23, x24, lsl #3
   142c4:	cmp	x8, x9
   142c8:	b.cs	14428 <__gmpz_and@@Base+0x330>  // b.hs, b.nlast
   142cc:	sub	x8, x25, x20, lsl #3
   142d0:	add	x9, x23, #0x8
   142d4:	cmp	x8, x9
   142d8:	b.ls	14428 <__gmpz_and@@Base+0x330>  // b.plast
   142dc:	mov	w8, #0x1                   	// #1
   142e0:	add	x9, x8, x20
   142e4:	lsl	x10, x8, #3
   142e8:	neg	x8, x9
   142ec:	add	x9, x25, x10
   142f0:	add	x10, x23, x10
   142f4:	ldr	x11, [x10], #8
   142f8:	subs	x8, x8, #0x1
   142fc:	str	x11, [x9], #8
   14300:	b.ne	142f4 <__gmpz_and@@Base+0x1fc>  // b.any
   14304:	b	14460 <__gmpz_and@@Base+0x368>
   14308:	mov	x10, xzr
   1430c:	mvn	x9, x20
   14310:	mov	w8, #0x1                   	// #1
   14314:	cmp	x8, x24
   14318:	b.ge	14460 <__gmpz_and@@Base+0x368>  // b.tcont
   1431c:	add	x11, x23, x10
   14320:	ldr	x11, [x11, #8]
   14324:	add	x12, x25, x10
   14328:	add	x8, x8, #0x1
   1432c:	add	x10, x10, #0x8
   14330:	sub	x13, x11, #0x1
   14334:	sub	x9, x9, #0x1
   14338:	str	x13, [x12, #8]
   1433c:	cbz	x11, 14314 <__gmpz_and@@Base+0x21c>
   14340:	cmp	x23, x25
   14344:	b.eq	14460 <__gmpz_and@@Base+0x368>  // b.none
   14348:	cmp	x8, x24
   1434c:	b.ge	14460 <__gmpz_and@@Base+0x368>  // b.tcont
   14350:	sub	x11, x24, x8
   14354:	cmp	x11, #0x4
   14358:	b.cc	143c4 <__gmpz_and@@Base+0x2cc>  // b.lo, b.ul, b.last
   1435c:	add	x12, x25, x10
   14360:	add	x12, x12, #0x8
   14364:	add	x13, x23, x24, lsl #3
   14368:	cmp	x12, x13
   1436c:	b.cs	14384 <__gmpz_and@@Base+0x28c>  // b.hs, b.nlast
   14370:	add	x13, x23, x10
   14374:	sub	x12, x25, x20, lsl #3
   14378:	add	x13, x13, #0x8
   1437c:	cmp	x12, x13
   14380:	b.hi	143c4 <__gmpz_and@@Base+0x2cc>  // b.pmore
   14384:	add	x12, x25, x10
   14388:	add	x13, x23, x10
   1438c:	and	x10, x11, #0xfffffffffffffffc
   14390:	and	x14, x9, #0xfffffffffffffffc
   14394:	add	x9, x12, #0x18
   14398:	add	x12, x13, #0x18
   1439c:	add	x8, x14, x8
   143a0:	mov	x13, x10
   143a4:	ldp	q0, q1, [x12, #-16]
   143a8:	add	x12, x12, #0x20
   143ac:	subs	x13, x13, #0x4
   143b0:	stp	q0, q1, [x9, #-16]
   143b4:	add	x9, x9, #0x20
   143b8:	b.ne	143a4 <__gmpz_and@@Base+0x2ac>  // b.any
   143bc:	cmp	x11, x10
   143c0:	b.eq	14460 <__gmpz_and@@Base+0x368>  // b.none
   143c4:	add	x9, x8, x20
   143c8:	lsl	x10, x8, #3
   143cc:	neg	x8, x9
   143d0:	add	x9, x25, x10
   143d4:	add	x10, x23, x10
   143d8:	ldr	x11, [x10], #8
   143dc:	subs	x8, x8, #0x1
   143e0:	str	x11, [x9], #8
   143e4:	b.ne	143d8 <__gmpz_and@@Base+0x2e0>  // b.any
   143e8:	b	14460 <__gmpz_and@@Base+0x368>
   143ec:	mvn	x9, x27
   143f0:	and	x10, x9, #0xfffffffffffffffc
   143f4:	add	x11, x22, #0x18
   143f8:	orr	x8, x10, #0x1
   143fc:	add	x12, x24, #0x18
   14400:	mov	x13, x10
   14404:	ldp	q0, q1, [x11, #-16]
   14408:	add	x11, x11, #0x20
   1440c:	subs	x13, x13, #0x4
   14410:	stp	q0, q1, [x12, #-16]
   14414:	add	x12, x12, #0x20
   14418:	b.ne	14404 <__gmpz_and@@Base+0x30c>  // b.any
   1441c:	cmp	x10, x9
   14420:	b.eq	147a8 <__gmpz_and@@Base+0x6b0>  // b.none
   14424:	b	14234 <__gmpz_and@@Base+0x13c>
   14428:	mvn	x9, x20
   1442c:	and	x10, x9, #0xfffffffffffffffc
   14430:	add	x11, x23, #0x18
   14434:	orr	x8, x10, #0x1
   14438:	add	x12, x25, #0x18
   1443c:	mov	x13, x10
   14440:	ldp	q0, q1, [x11, #-16]
   14444:	add	x11, x11, #0x20
   14448:	subs	x13, x13, #0x4
   1444c:	stp	q0, q1, [x12, #-16]
   14450:	add	x12, x12, #0x20
   14454:	b.ne	14440 <__gmpz_and@@Base+0x348>  // b.any
   14458:	cmp	x10, x9
   1445c:	b.ne	142e0 <__gmpz_and@@Base+0x1e8>  // b.any
   14460:	ldr	x8, [x22]
   14464:	sub	x9, x8, #0x1
   14468:	str	x9, [x26]
   1446c:	cbz	x8, 144e4 <__gmpz_and@@Base+0x3ec>
   14470:	cmn	w27, #0x2
   14474:	b.gt	14624 <__gmpz_and@@Base+0x52c>
   14478:	cmp	x22, x26
   1447c:	b.eq	14624 <__gmpz_and@@Base+0x52c>  // b.none
   14480:	cmn	w27, #0x5
   14484:	b.hi	144b4 <__gmpz_and@@Base+0x3bc>  // b.pmore
   14488:	lsl	x8, x20, #3
   1448c:	sub	x9, x25, x8
   14490:	add	x9, x9, #0x8
   14494:	add	x10, x22, x21, lsl #3
   14498:	cmp	x9, x10
   1449c:	b.cs	145e8 <__gmpz_and@@Base+0x4f0>  // b.hs, b.nlast
   144a0:	add	x8, x8, x27, lsl #3
   144a4:	sub	x8, x25, x8
   144a8:	add	x9, x22, #0x8
   144ac:	cmp	x8, x9
   144b0:	b.ls	145e8 <__gmpz_and@@Base+0x4f0>  // b.plast
   144b4:	mov	w8, #0x1                   	// #1
   144b8:	add	x9, x8, x27
   144bc:	lsl	x10, x8, #3
   144c0:	neg	x8, x9
   144c4:	sub	x9, x10, x20, lsl #3
   144c8:	add	x9, x25, x9
   144cc:	add	x10, x22, x10
   144d0:	ldr	x11, [x10], #8
   144d4:	subs	x8, x8, #0x1
   144d8:	str	x11, [x9], #8
   144dc:	b.ne	144d0 <__gmpz_and@@Base+0x3d8>  // b.any
   144e0:	b	14624 <__gmpz_and@@Base+0x52c>
   144e4:	mov	w8, #0x20                  	// #32
   144e8:	sub	x11, x8, x20, lsl #3
   144ec:	add	x8, x11, x25
   144f0:	mov	x9, xzr
   144f4:	mvn	x10, x27
   144f8:	sub	x12, x8, #0x20
   144fc:	mov	w8, #0x1                   	// #1
   14500:	cmp	x8, x21
   14504:	b.ge	14624 <__gmpz_and@@Base+0x52c>  // b.tcont
   14508:	add	x13, x22, x9
   1450c:	ldr	x13, [x13, #8]
   14510:	add	x14, x12, x9
   14514:	add	x8, x8, #0x1
   14518:	add	x9, x9, #0x8
   1451c:	sub	x15, x13, #0x1
   14520:	sub	x10, x10, #0x1
   14524:	str	x15, [x14, #8]
   14528:	cbz	x13, 14500 <__gmpz_and@@Base+0x408>
   1452c:	cmp	x22, x26
   14530:	b.eq	14624 <__gmpz_and@@Base+0x52c>  // b.none
   14534:	cmp	x8, x21
   14538:	b.ge	14624 <__gmpz_and@@Base+0x52c>  // b.tcont
   1453c:	sub	x12, x21, x8
   14540:	cmp	x12, #0x4
   14544:	b.cc	145bc <__gmpz_and@@Base+0x4c4>  // b.lo, b.ul, b.last
   14548:	add	x13, x11, x25
   1454c:	add	x13, x13, x9
   14550:	sub	x13, x13, #0x18
   14554:	add	x14, x22, x21, lsl #3
   14558:	cmp	x13, x14
   1455c:	b.cs	14578 <__gmpz_and@@Base+0x480>  // b.hs, b.nlast
   14560:	add	x13, x27, x20
   14564:	add	x14, x22, x9
   14568:	sub	x13, x25, x13, lsl #3
   1456c:	add	x14, x14, #0x8
   14570:	cmp	x13, x14
   14574:	b.hi	145bc <__gmpz_and@@Base+0x4c4>  // b.pmore
   14578:	add	x13, x11, x25
   1457c:	add	x14, x22, x9
   14580:	and	x11, x12, #0xfffffffffffffffc
   14584:	and	x15, x10, #0xfffffffffffffffc
   14588:	add	x10, x13, x9
   1458c:	add	x9, x14, #0x18
   14590:	sub	x10, x10, #0x8
   14594:	add	x8, x15, x8
   14598:	mov	x13, x11
   1459c:	ldp	q0, q1, [x9, #-16]
   145a0:	add	x9, x9, #0x20
   145a4:	subs	x13, x13, #0x4
   145a8:	stp	q0, q1, [x10, #-16]
   145ac:	add	x10, x10, #0x20
   145b0:	b.ne	1459c <__gmpz_and@@Base+0x4a4>  // b.any
   145b4:	cmp	x12, x11
   145b8:	b.eq	14624 <__gmpz_and@@Base+0x52c>  // b.none
   145bc:	add	x9, x8, x27
   145c0:	lsl	x10, x8, #3
   145c4:	neg	x8, x9
   145c8:	sub	x9, x10, x20, lsl #3
   145cc:	add	x9, x25, x9
   145d0:	add	x10, x22, x10
   145d4:	ldr	x11, [x10], #8
   145d8:	subs	x8, x8, #0x1
   145dc:	str	x11, [x9], #8
   145e0:	b.ne	145d4 <__gmpz_and@@Base+0x4dc>  // b.any
   145e4:	b	14624 <__gmpz_and@@Base+0x52c>
   145e8:	mvn	x9, x27
   145ec:	sub	x12, x25, x20, lsl #3
   145f0:	and	x11, x9, #0xfffffffffffffffc
   145f4:	add	x10, x22, #0x18
   145f8:	orr	x8, x11, #0x1
   145fc:	add	x12, x12, #0x18
   14600:	mov	x13, x11
   14604:	ldp	q0, q1, [x10, #-16]
   14608:	add	x10, x10, #0x20
   1460c:	subs	x13, x13, #0x4
   14610:	stp	q0, q1, [x12, #-16]
   14614:	add	x12, x12, #0x20
   14618:	b.ne	14604 <__gmpz_and@@Base+0x50c>  // b.any
   1461c:	cmp	x11, x9
   14620:	b.ne	144b8 <__gmpz_and@@Base+0x3c0>  // b.any
   14624:	ldrsw	x8, [x19]
   14628:	mov	w9, #0x1                   	// #1
   1462c:	sub	x1, x9, x27
   14630:	cmp	x1, x8
   14634:	b.gt	14880 <__gmpz_and@@Base+0x788>
   14638:	ldr	x22, [x19, #8]
   1463c:	lsl	x8, x24, #3
   14640:	add	x0, x22, x8
   14644:	add	x1, x26, x8
   14648:	sub	x2, x20, x27
   1464c:	bl	ca50 <__gmpn_copyi@plt>
   14650:	mov	x0, x22
   14654:	mov	x1, x25
   14658:	mov	x2, x26
   1465c:	mov	x3, x24
   14660:	bl	cc00 <__gmpn_ior_n@plt>
   14664:	ldr	x0, [x29, #24]
   14668:	cbnz	x0, 14890 <__gmpz_and@@Base+0x798>
   1466c:	mov	x8, x22
   14670:	str	xzr, [x22, x21, lsl #3]
   14674:	ldr	x9, [x8]
   14678:	adds	x9, x9, #0x1
   1467c:	str	x9, [x8], #8
   14680:	b.cs	14674 <__gmpz_and@@Base+0x57c>  // b.hs, b.nlast
   14684:	lsl	x8, x21, #3
   14688:	ldr	w8, [x22, x8]
   1468c:	sub	w8, w27, w8
   14690:	str	w8, [x19, #4]
   14694:	b	14848 <__gmpz_and@@Base+0x750>
   14698:	mov	x0, x19
   1469c:	mov	x1, x20
   146a0:	bl	c080 <__gmpz_realloc@plt>
   146a4:	b	1418c <__gmpz_and@@Base+0x94>
   146a8:	add	x0, x29, #0x18
   146ac:	mov	x1, x25
   146b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   146b4:	mov	x24, x0
   146b8:	ldr	x8, [x22]
   146bc:	sub	x9, x8, #0x1
   146c0:	str	x9, [x24]
   146c4:	cbnz	x8, 141f8 <__gmpz_and@@Base+0x100>
   146c8:	mov	x10, xzr
   146cc:	mvn	x9, x27
   146d0:	mov	w8, #0x1                   	// #1
   146d4:	cmp	x8, x21
   146d8:	b.ge	147a8 <__gmpz_and@@Base+0x6b0>  // b.tcont
   146dc:	add	x11, x22, x10
   146e0:	ldr	x11, [x11, #8]
   146e4:	add	x12, x24, x10
   146e8:	add	x8, x8, #0x1
   146ec:	add	x10, x10, #0x8
   146f0:	sub	x13, x11, #0x1
   146f4:	sub	x9, x9, #0x1
   146f8:	str	x13, [x12, #8]
   146fc:	cbz	x11, 146d4 <__gmpz_and@@Base+0x5dc>
   14700:	cmp	x22, x24
   14704:	b.eq	147a8 <__gmpz_and@@Base+0x6b0>  // b.none
   14708:	cmp	x8, x21
   1470c:	b.ge	147a8 <__gmpz_and@@Base+0x6b0>  // b.tcont
   14710:	sub	x11, x21, x8
   14714:	cmp	x11, #0x4
   14718:	b.cc	14784 <__gmpz_and@@Base+0x68c>  // b.lo, b.ul, b.last
   1471c:	add	x12, x24, x10
   14720:	add	x12, x12, #0x8
   14724:	add	x13, x22, x21, lsl #3
   14728:	cmp	x12, x13
   1472c:	b.cs	14744 <__gmpz_and@@Base+0x64c>  // b.hs, b.nlast
   14730:	add	x13, x22, x10
   14734:	sub	x12, x24, x27, lsl #3
   14738:	add	x13, x13, #0x8
   1473c:	cmp	x12, x13
   14740:	b.hi	14784 <__gmpz_and@@Base+0x68c>  // b.pmore
   14744:	add	x12, x24, x10
   14748:	add	x13, x22, x10
   1474c:	and	x10, x11, #0xfffffffffffffffc
   14750:	and	x14, x9, #0xfffffffffffffffc
   14754:	add	x9, x12, #0x18
   14758:	add	x12, x13, #0x18
   1475c:	add	x8, x14, x8
   14760:	mov	x13, x10
   14764:	ldp	q0, q1, [x12, #-16]
   14768:	add	x12, x12, #0x20
   1476c:	subs	x13, x13, #0x4
   14770:	stp	q0, q1, [x9, #-16]
   14774:	add	x9, x9, #0x20
   14778:	b.ne	14764 <__gmpz_and@@Base+0x66c>  // b.any
   1477c:	cmp	x11, x10
   14780:	b.eq	147a8 <__gmpz_and@@Base+0x6b0>  // b.none
   14784:	add	x9, x8, x27
   14788:	lsl	x10, x8, #3
   1478c:	neg	x8, x9
   14790:	add	x9, x24, x10
   14794:	add	x10, x22, x10
   14798:	ldr	x11, [x10], #8
   1479c:	subs	x8, x8, #0x1
   147a0:	str	x11, [x9], #8
   147a4:	b.ne	14798 <__gmpz_and@@Base+0x6a0>  // b.any
   147a8:	cmp	x20, x21
   147ac:	b.le	147e8 <__gmpz_and@@Base+0x6f0>
   147b0:	ldr	w8, [x19]
   147b4:	cmp	w20, w8
   147b8:	b.gt	14898 <__gmpz_and@@Base+0x7a0>
   147bc:	ldr	x22, [x19, #8]
   147c0:	mov	x0, x22
   147c4:	mov	x1, x23
   147c8:	mov	x2, x24
   147cc:	mov	x3, x21
   147d0:	bl	c060 <__gmpn_andn_n@plt>
   147d4:	add	x0, x22, x25
   147d8:	add	x1, x23, x25
   147dc:	add	x2, x20, x27
   147e0:	bl	ca50 <__gmpn_copyi@plt>
   147e4:	b	1483c <__gmpz_and@@Base+0x744>
   147e8:	sub	x8, x24, #0x8
   147ec:	subs	x9, x20, #0x1
   147f0:	b.lt	14838 <__gmpz_and@@Base+0x740>  // b.tstop
   147f4:	lsl	x10, x20, #3
   147f8:	add	x11, x23, x10
   147fc:	ldur	x11, [x11, #-8]
   14800:	ldr	x10, [x8, x10]
   14804:	mov	x20, x9
   14808:	bics	xzr, x11, x10
   1480c:	b.eq	147ec <__gmpz_and@@Base+0x6f4>  // b.none
   14810:	ldrsw	x8, [x19]
   14814:	add	x20, x9, #0x1
   14818:	cmp	x20, x8
   1481c:	b.gt	148ac <__gmpz_and@@Base+0x7b4>
   14820:	ldr	x0, [x19, #8]
   14824:	mov	x1, x23
   14828:	mov	x2, x24
   1482c:	mov	x3, x20
   14830:	bl	c060 <__gmpn_andn_n@plt>
   14834:	b	1483c <__gmpz_and@@Base+0x744>
   14838:	mov	x20, xzr
   1483c:	str	w20, [x19, #4]
   14840:	ldr	x0, [x29, #24]
   14844:	cbnz	x0, 14868 <__gmpz_and@@Base+0x770>
   14848:	mov	sp, x29
   1484c:	ldp	x20, x19, [sp, #80]
   14850:	ldp	x22, x21, [sp, #64]
   14854:	ldp	x24, x23, [sp, #48]
   14858:	ldp	x26, x25, [sp, #32]
   1485c:	ldr	x27, [sp, #16]
   14860:	ldp	x29, x30, [sp], #96
   14864:	ret
   14868:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1486c:	b	14848 <__gmpz_and@@Base+0x750>
   14870:	add	x0, x29, #0x18
   14874:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   14878:	mov	x25, x0
   1487c:	b	14290 <__gmpz_and@@Base+0x198>
   14880:	mov	x0, x19
   14884:	bl	c080 <__gmpz_realloc@plt>
   14888:	mov	x22, x0
   1488c:	b	1463c <__gmpz_and@@Base+0x544>
   14890:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   14894:	b	1466c <__gmpz_and@@Base+0x574>
   14898:	mov	x0, x19
   1489c:	mov	x1, x20
   148a0:	bl	c080 <__gmpz_realloc@plt>
   148a4:	mov	x22, x0
   148a8:	b	147c0 <__gmpz_and@@Base+0x6c8>
   148ac:	mov	x0, x19
   148b0:	mov	x1, x20
   148b4:	bl	c080 <__gmpz_realloc@plt>
   148b8:	b	14824 <__gmpz_and@@Base+0x72c>

00000000000148bc <__gmpz_array_init@@Base>:
   148bc:	stp	x29, x30, [sp, #-48]!
   148c0:	stp	x22, x21, [sp, #16]
   148c4:	stp	x20, x19, [sp, #32]
   148c8:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   148cc:	ldr	x9, [x9, #3840]
   148d0:	add	x8, x2, #0x3f
   148d4:	cmp	x2, #0x0
   148d8:	csel	x8, x8, x2, lt  // lt = tstop
   148dc:	asr	x22, x8, #6
   148e0:	ldr	x8, [x9]
   148e4:	add	x21, x22, #0x1
   148e8:	mul	x9, x1, x21
   148ec:	mov	x19, x0
   148f0:	lsl	x0, x9, #3
   148f4:	mov	x29, sp
   148f8:	mov	x20, x1
   148fc:	blr	x8
   14900:	cmp	x20, #0x1
   14904:	b.lt	14998 <__gmpz_array_init@@Base+0xdc>  // b.tstop
   14908:	add	w8, w22, #0x2
   1490c:	cmp	x20, #0x1
   14910:	lsl	x9, x22, #3
   14914:	b.ne	14920 <__gmpz_array_init@@Base+0x64>  // b.any
   14918:	mov	x10, xzr
   1491c:	b	14968 <__gmpz_array_init@@Base+0xac>
   14920:	and	x10, x20, #0xfffffffffffffffe
   14924:	lsl	x12, x22, #4
   14928:	add	x11, x9, #0x8
   1492c:	add	x12, x12, #0x10
   14930:	add	x13, x19, #0x10
   14934:	mov	x14, x0
   14938:	mov	x15, x10
   1493c:	add	x16, x14, x11
   14940:	stp	w8, wzr, [x13, #-16]
   14944:	stp	w8, wzr, [x13]
   14948:	stur	x14, [x13, #-8]
   1494c:	subs	x15, x15, #0x2
   14950:	add	x14, x14, x12
   14954:	str	x16, [x13, #8]
   14958:	add	x13, x13, #0x20
   1495c:	b.ne	1493c <__gmpz_array_init@@Base+0x80>  // b.any
   14960:	cmp	x10, x20
   14964:	b.eq	14998 <__gmpz_array_init@@Base+0xdc>  // b.none
   14968:	add	x12, x19, x10, lsl #4
   1496c:	mul	x13, x10, x21
   14970:	sub	x11, x20, x10
   14974:	add	x10, x12, #0x4
   14978:	add	x12, x0, x13, lsl #3
   1497c:	add	x9, x9, #0x8
   14980:	stp	w8, wzr, [x10, #-4]
   14984:	stur	x12, [x10, #4]
   14988:	subs	x11, x11, #0x1
   1498c:	add	x10, x10, #0x10
   14990:	add	x12, x12, x9
   14994:	b.ne	14980 <__gmpz_array_init@@Base+0xc4>  // b.any
   14998:	ldp	x20, x19, [sp, #32]
   1499c:	ldp	x22, x21, [sp, #16]
   149a0:	ldp	x29, x30, [sp], #48
   149a4:	ret

00000000000149a8 <__gmpz_bin_ui@@Base>:
   149a8:	sub	sp, sp, #0x70
   149ac:	stp	x29, x30, [sp, #48]
   149b0:	stp	x22, x21, [sp, #80]
   149b4:	stp	x20, x19, [sp, #96]
   149b8:	ldr	w8, [x1, #4]
   149bc:	mov	x20, x2
   149c0:	mov	x21, x1
   149c4:	mov	x19, x0
   149c8:	str	x23, [sp, #64]
   149cc:	add	x29, sp, #0x30
   149d0:	tbnz	w8, #31, 14a30 <__gmpz_bin_ui@@Base+0x88>
   149d4:	mov	x0, x21
   149d8:	mov	x1, x20
   149dc:	bl	d1f0 <__gmpz_cmp_ui@plt>
   149e0:	tbnz	w0, #31, 14b84 <__gmpz_bin_ui@@Base+0x1dc>
   149e4:	sub	x0, x29, #0x10
   149e8:	bl	d250 <__gmpz_init@plt>
   149ec:	sub	x0, x29, #0x10
   149f0:	mov	x1, x21
   149f4:	mov	x2, x20
   149f8:	bl	c120 <__gmpz_sub_ui@plt>
   149fc:	mov	x23, xzr
   14a00:	sub	x0, x29, #0x10
   14a04:	mov	x1, x20
   14a08:	bl	d1f0 <__gmpz_cmp_ui@plt>
   14a0c:	tbnz	w0, #31, 14a68 <__gmpz_bin_ui@@Base+0xc0>
   14a10:	cmp	x20, #0x1
   14a14:	b.hi	14a90 <__gmpz_bin_ui@@Base+0xe8>  // b.pmore
   14a18:	cbz	x20, 14b8c <__gmpz_bin_ui@@Base+0x1e4>
   14a1c:	sub	x1, x29, #0x10
   14a20:	mov	w2, #0x1                   	// #1
   14a24:	mov	x0, x19
   14a28:	bl	c8b0 <__gmpz_add_ui@plt>
   14a2c:	b	14d28 <__gmpz_bin_ui@@Base+0x380>
   14a30:	sub	x0, x29, #0x10
   14a34:	bl	d250 <__gmpz_init@plt>
   14a38:	sub	x0, x29, #0x10
   14a3c:	mov	w2, #0x1                   	// #1
   14a40:	mov	x1, x21
   14a44:	bl	c8b0 <__gmpz_add_ui@plt>
   14a48:	ldur	w8, [x29, #-12]
   14a4c:	and	x23, x20, #0x1
   14a50:	neg	w8, w8
   14a54:	stur	w8, [x29, #-12]
   14a58:	sub	x0, x29, #0x10
   14a5c:	mov	x1, x20
   14a60:	bl	d1f0 <__gmpz_cmp_ui@plt>
   14a64:	tbz	w0, #31, 14a10 <__gmpz_bin_ui@@Base+0x68>
   14a68:	ldur	x8, [x29, #-8]
   14a6c:	ldur	w22, [x29, #-12]
   14a70:	sub	x0, x29, #0x10
   14a74:	mov	x1, x20
   14a78:	ldr	x21, [x8]
   14a7c:	bl	c170 <__gmpz_set_ui@plt>
   14a80:	cbz	w22, 14b8c <__gmpz_bin_ui@@Base+0x1e4>
   14a84:	mov	x20, x21
   14a88:	cmp	x20, #0x1
   14a8c:	b.ls	14a18 <__gmpz_bin_ui@@Base+0x70>  // b.plast
   14a90:	add	x0, sp, #0x10
   14a94:	bl	d250 <__gmpz_init@plt>
   14a98:	mov	x0, sp
   14a9c:	bl	d250 <__gmpz_init@plt>
   14aa0:	ldp	w8, w21, [x29, #-16]
   14aa4:	sxtw	x21, w21
   14aa8:	add	x1, x21, #0x2
   14aac:	cmp	w1, w8
   14ab0:	b.gt	14d5c <__gmpz_bin_ui@@Base+0x3b4>
   14ab4:	ldur	x0, [x29, #-8]
   14ab8:	add	x8, x0, x21, lsl #3
   14abc:	stp	xzr, xzr, [x8]
   14ac0:	ldur	x8, [x29, #-8]
   14ac4:	mov	x9, x8
   14ac8:	ldr	x10, [x9]
   14acc:	adds	x10, x10, #0x1
   14ad0:	str	x10, [x9], #8
   14ad4:	b.cs	14ac8 <__gmpz_bin_ui@@Base+0x120>  // b.hs, b.nlast
   14ad8:	ldursw	x9, [x29, #-12]
   14adc:	ldr	x8, [x8, x9, lsl #3]
   14ae0:	str	wzr, [sp, #20]
   14ae4:	cmp	x8, #0x0
   14ae8:	cinc	w8, w9, ne  // ne = any
   14aec:	stur	w8, [x29, #-12]
   14af0:	tbz	w20, #0, 14b2c <__gmpz_bin_ui@@Base+0x184>
   14af4:	add	x0, sp, #0x10
   14af8:	sub	x1, x29, #0x10
   14afc:	bl	c420 <__gmpz_set@plt>
   14b00:	ldur	x8, [x29, #-8]
   14b04:	mov	x9, x8
   14b08:	ldr	x10, [x9]
   14b0c:	adds	x10, x10, #0x1
   14b10:	str	x10, [x9], #8
   14b14:	b.cs	14b08 <__gmpz_bin_ui@@Base+0x160>  // b.hs, b.nlast
   14b18:	ldursw	x9, [x29, #-12]
   14b1c:	ldr	x8, [x8, x9, lsl #3]
   14b20:	cmp	x8, #0x0
   14b24:	cinc	w8, w9, ne  // ne = any
   14b28:	stur	w8, [x29, #-12]
   14b2c:	lsr	x21, x20, #1
   14b30:	sub	x1, x29, #0x10
   14b34:	mov	x3, sp
   14b38:	mov	x0, x19
   14b3c:	mov	x2, x21
   14b40:	bl	14d90 <__gmpz_bin_ui@@Base+0x3e8>
   14b44:	ldp	w8, w22, [x19]
   14b48:	sxtw	x22, w22
   14b4c:	add	x1, x22, #0x2
   14b50:	cmp	w1, w8
   14b54:	b.gt	14d68 <__gmpz_bin_ui@@Base+0x3c0>
   14b58:	ldr	x0, [x19, #8]
   14b5c:	add	x8, x0, x22, lsl #3
   14b60:	stp	xzr, xzr, [x8]
   14b64:	tbz	w20, #1, 14bfc <__gmpz_bin_ui@@Base+0x254>
   14b68:	ldr	w8, [sp, #20]
   14b6c:	cbz	w8, 14bb0 <__gmpz_bin_ui@@Base+0x208>
   14b70:	add	x0, sp, #0x10
   14b74:	add	x1, sp, #0x10
   14b78:	mov	x2, x19
   14b7c:	bl	c4b0 <__gmpz_mul@plt>
   14b80:	b	14bbc <__gmpz_bin_ui@@Base+0x214>
   14b84:	str	wzr, [x19, #4]
   14b88:	b	14d44 <__gmpz_bin_ui@@Base+0x39c>
   14b8c:	ldr	w8, [x19]
   14b90:	mov	w9, #0x1                   	// #1
   14b94:	str	w9, [x19, #4]
   14b98:	cmp	w8, #0x0
   14b9c:	b.le	14d80 <__gmpz_bin_ui@@Base+0x3d8>
   14ba0:	ldr	x0, [x19, #8]
   14ba4:	mov	w8, #0x1                   	// #1
   14ba8:	str	x8, [x0]
   14bac:	b	14d28 <__gmpz_bin_ui@@Base+0x380>
   14bb0:	add	x0, sp, #0x10
   14bb4:	mov	x1, x19
   14bb8:	bl	c420 <__gmpz_set@plt>
   14bbc:	ldr	x8, [x19, #8]
   14bc0:	sub	x10, x21, #0x1
   14bc4:	ldr	x9, [x8]
   14bc8:	adds	x9, x9, x10
   14bcc:	str	x9, [x8]
   14bd0:	b.cc	14be8 <__gmpz_bin_ui@@Base+0x240>  // b.lo, b.ul, b.last
   14bd4:	add	x9, x8, #0x8
   14bd8:	ldr	x10, [x9]
   14bdc:	adds	x10, x10, #0x1
   14be0:	str	x10, [x9], #8
   14be4:	b.cs	14bd8 <__gmpz_bin_ui@@Base+0x230>  // b.hs, b.nlast
   14be8:	ldrsw	x9, [x19, #4]
   14bec:	ldr	x8, [x8, x9, lsl #3]
   14bf0:	cmp	x8, #0x0
   14bf4:	cinc	w8, w9, ne  // ne = any
   14bf8:	str	w8, [x19, #4]
   14bfc:	lsr	x22, x20, #2
   14c00:	cbz	x22, 14cb0 <__gmpz_bin_ui@@Base+0x308>
   14c04:	mov	x0, sp
   14c08:	sub	x3, x29, #0x10
   14c0c:	mov	x1, x19
   14c10:	mov	x2, x22
   14c14:	bl	14d90 <__gmpz_bin_ui@@Base+0x3e8>
   14c18:	ldr	w8, [sp, #20]
   14c1c:	cbz	w8, 14c3c <__gmpz_bin_ui@@Base+0x294>
   14c20:	add	x0, sp, #0x10
   14c24:	add	x1, sp, #0x10
   14c28:	mov	x2, sp
   14c2c:	bl	c4b0 <__gmpz_mul@plt>
   14c30:	cmp	x20, #0x8
   14c34:	b.cs	14c50 <__gmpz_bin_ui@@Base+0x2a8>  // b.hs, b.nlast
   14c38:	b	14cb0 <__gmpz_bin_ui@@Base+0x308>
   14c3c:	add	x0, sp, #0x10
   14c40:	mov	x1, sp
   14c44:	bl	c420 <__gmpz_set@plt>
   14c48:	cmp	x20, #0x8
   14c4c:	b.cc	14cb0 <__gmpz_bin_ui@@Base+0x308>  // b.lo, b.ul, b.last
   14c50:	ldr	x8, [x19, #8]
   14c54:	ldr	x9, [x8]
   14c58:	subs	x9, x9, x22
   14c5c:	str	x9, [x8]
   14c60:	b.cs	14c78 <__gmpz_bin_ui@@Base+0x2d0>  // b.hs, b.nlast
   14c64:	add	x9, x8, #0x8
   14c68:	ldr	x10, [x9]
   14c6c:	sub	x11, x10, #0x1
   14c70:	str	x11, [x9], #8
   14c74:	cbz	x10, 14c68 <__gmpz_bin_ui@@Base+0x2c0>
   14c78:	ldr	w9, [x19, #4]
   14c7c:	sub	x3, x22, #0x1
   14c80:	add	x0, sp, #0x10
   14c84:	mov	x2, sp
   14c88:	sub	w10, w9, #0x1
   14c8c:	ldr	x8, [x8, w10, sxtw #3]
   14c90:	sub	x5, x29, #0x10
   14c94:	mov	x1, x19
   14c98:	mov	x4, xzr
   14c9c:	cmp	x8, #0x0
   14ca0:	cset	w8, eq  // eq = none
   14ca4:	sub	w8, w9, w8
   14ca8:	str	w8, [x19, #4]
   14cac:	bl	14ec4 <__gmpz_bin_ui@@Base+0x51c>
   14cb0:	and	x8, x21, #0x5555555555555555
   14cb4:	sub	x8, x20, x8
   14cb8:	lsr	x10, x8, #2
   14cbc:	and	x8, x8, #0x3333333333333333
   14cc0:	and	x10, x10, #0x3333333333333333
   14cc4:	add	x8, x10, x8
   14cc8:	add	x8, x8, x8, lsr #4
   14ccc:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   14cd0:	add	x8, x8, x8, lsr #8
   14cd4:	add	x8, x8, x8, lsr #16
   14cd8:	sub	x9, x20, x21
   14cdc:	lsr	x10, x8, #32
   14ce0:	add	w8, w10, w8
   14ce4:	sub	x9, x9, x22
   14ce8:	sub	x2, x9, w8, uxtb
   14cec:	add	x0, sp, #0x10
   14cf0:	add	x1, sp, #0x10
   14cf4:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   14cf8:	mov	x0, sp
   14cfc:	mov	x1, x20
   14d00:	mov	w2, wzr
   14d04:	bl	c3d0 <__gmpz_oddfac_1@plt>
   14d08:	add	x1, sp, #0x10
   14d0c:	mov	x2, sp
   14d10:	mov	x0, x19
   14d14:	bl	c3f0 <__gmpz_divexact@plt>
   14d18:	add	x0, sp, #0x10
   14d1c:	bl	cb50 <__gmpz_clear@plt>
   14d20:	mov	x0, sp
   14d24:	bl	cb50 <__gmpz_clear@plt>
   14d28:	sub	x0, x29, #0x10
   14d2c:	bl	cb50 <__gmpz_clear@plt>
   14d30:	ldr	w8, [x19, #4]
   14d34:	neg	w9, w23
   14d38:	eor	w8, w8, w9
   14d3c:	add	w8, w8, w23
   14d40:	str	w8, [x19, #4]
   14d44:	ldp	x20, x19, [sp, #96]
   14d48:	ldp	x22, x21, [sp, #80]
   14d4c:	ldr	x23, [sp, #64]
   14d50:	ldp	x29, x30, [sp, #48]
   14d54:	add	sp, sp, #0x70
   14d58:	ret
   14d5c:	sub	x0, x29, #0x10
   14d60:	bl	c080 <__gmpz_realloc@plt>
   14d64:	b	14ab8 <__gmpz_bin_ui@@Base+0x110>
   14d68:	mov	x0, x19
   14d6c:	bl	c080 <__gmpz_realloc@plt>
   14d70:	add	x8, x0, x22, lsl #3
   14d74:	stp	xzr, xzr, [x8]
   14d78:	tbnz	w20, #1, 14b68 <__gmpz_bin_ui@@Base+0x1c0>
   14d7c:	b	14bfc <__gmpz_bin_ui@@Base+0x254>
   14d80:	mov	w1, #0x1                   	// #1
   14d84:	mov	x0, x19
   14d88:	bl	c080 <__gmpz_realloc@plt>
   14d8c:	b	14ba4 <__gmpz_bin_ui@@Base+0x1fc>
   14d90:	sub	sp, sp, #0x40
   14d94:	stp	x20, x19, [sp, #48]
   14d98:	sub	x20, x2, #0x1
   14d9c:	mov	x19, x0
   14da0:	mov	x0, x3
   14da4:	mov	x2, x20
   14da8:	stp	x29, x30, [sp, #16]
   14dac:	stp	x22, x21, [sp, #32]
   14db0:	add	x29, sp, #0x10
   14db4:	mov	x21, x3
   14db8:	mov	x22, x1
   14dbc:	bl	c8b0 <__gmpz_add_ui@plt>
   14dc0:	mov	x0, x19
   14dc4:	mov	x1, x21
   14dc8:	mov	x2, x21
   14dcc:	bl	c4b0 <__gmpz_mul@plt>
   14dd0:	mov	x0, x19
   14dd4:	mov	x1, x19
   14dd8:	mov	x2, x22
   14ddc:	bl	cf90 <__gmpz_add@plt>
   14de0:	ldrsw	x21, [x19, #4]
   14de4:	ldr	x22, [x19, #8]
   14de8:	mov	w3, #0x1                   	// #1
   14dec:	mov	x2, x21
   14df0:	mov	x0, x22
   14df4:	mov	x1, x22
   14df8:	bl	c1a0 <__gmpn_rshift@plt>
   14dfc:	add	x8, x22, x21, lsl #3
   14e00:	ldur	x8, [x8, #-8]
   14e04:	ldr	w9, [x19, #4]
   14e08:	mov	x10, #0x100000000           	// #4294967296
   14e0c:	cmp	x8, #0x0
   14e10:	cset	w8, eq  // eq = none
   14e14:	sub	w8, w9, w8
   14e18:	cmp	x20, x10
   14e1c:	str	w8, [x19, #4]
   14e20:	b.hi	14e88 <__gmpz_bin_ui@@Base+0x4e0>  // b.pmore
   14e24:	ldr	x9, [x19, #8]
   14e28:	and	x10, x20, #0x1
   14e2c:	add	x10, x10, x20
   14e30:	lsr	x12, x20, #1
   14e34:	ldr	x11, [x9]
   14e38:	mul	x10, x10, x12
   14e3c:	subs	x10, x11, x10
   14e40:	str	x10, [x9]
   14e44:	b.cs	14e5c <__gmpz_bin_ui@@Base+0x4b4>  // b.hs, b.nlast
   14e48:	add	x10, x9, #0x8
   14e4c:	ldr	x11, [x10]
   14e50:	sub	x12, x11, #0x1
   14e54:	str	x12, [x10], #8
   14e58:	cbz	x11, 14e4c <__gmpz_bin_ui@@Base+0x4a4>
   14e5c:	sub	w10, w8, #0x1
   14e60:	ldr	x9, [x9, w10, sxtw #3]
   14e64:	cmp	x9, #0x0
   14e68:	cset	w9, eq  // eq = none
   14e6c:	sub	w8, w8, w9
   14e70:	str	w8, [x19, #4]
   14e74:	ldp	x20, x19, [sp, #48]
   14e78:	ldp	x22, x21, [sp, #32]
   14e7c:	ldp	x29, x30, [sp, #16]
   14e80:	add	sp, sp, #0x40
   14e84:	ret
   14e88:	and	x8, x20, #0x1
   14e8c:	add	x1, x8, x20
   14e90:	mov	x0, sp
   14e94:	bl	ce50 <__gmpz_init_set_ui@plt>
   14e98:	lsr	x2, x20, #1
   14e9c:	mov	x0, sp
   14ea0:	mov	x1, sp
   14ea4:	bl	c5c0 <__gmpz_mul_ui@plt>
   14ea8:	mov	x2, sp
   14eac:	mov	x0, x19
   14eb0:	mov	x1, x19
   14eb4:	bl	c260 <__gmpz_sub@plt>
   14eb8:	mov	x0, sp
   14ebc:	bl	cb50 <__gmpz_clear@plt>
   14ec0:	b	14e74 <__gmpz_bin_ui@@Base+0x4cc>
   14ec4:	sub	sp, sp, #0x60
   14ec8:	sub	x8, x3, x4
   14ecc:	stp	x26, x25, [sp, #32]
   14ed0:	stp	x22, x21, [sp, #64]
   14ed4:	stp	x20, x19, [sp, #80]
   14ed8:	mov	x19, x4
   14edc:	mov	x25, x3
   14ee0:	mov	x21, x2
   14ee4:	mov	x22, x1
   14ee8:	cmp	x8, #0x4
   14eec:	mov	x20, x0
   14ef0:	stp	x29, x30, [sp, #16]
   14ef4:	stp	x24, x23, [sp, #48]
   14ef8:	add	x29, sp, #0x10
   14efc:	b.ls	15014 <__gmpz_bin_ui@@Base+0x66c>  // b.plast
   14f00:	add	x8, x19, x25
   14f04:	lsr	x24, x8, #1
   14f08:	add	x26, x24, #0x1
   14f0c:	mov	x0, x20
   14f10:	mov	x1, x22
   14f14:	mov	x2, x21
   14f18:	mov	x3, x25
   14f1c:	mov	x4, x26
   14f20:	mov	x23, x5
   14f24:	bl	14ec4 <__gmpz_bin_ui@@Base+0x51c>
   14f28:	ldr	x8, [x22, #8]
   14f2c:	mov	w10, #0x2                   	// #2
   14f30:	bfi	x10, x26, #2, #62
   14f34:	lsl	x2, x26, #2
   14f38:	ldr	x9, [x8]
   14f3c:	adds	x9, x9, x10
   14f40:	str	x9, [x8]
   14f44:	b.cc	14f5c <__gmpz_bin_ui@@Base+0x5b4>  // b.lo, b.ul, b.last
   14f48:	add	x9, x8, #0x8
   14f4c:	ldr	x10, [x9]
   14f50:	adds	x10, x10, #0x1
   14f54:	str	x10, [x9], #8
   14f58:	b.cs	14f4c <__gmpz_bin_ui@@Base+0x5a4>  // b.hs, b.nlast
   14f5c:	ldrsw	x9, [x22, #4]
   14f60:	mov	x0, x21
   14f64:	mov	x1, x22
   14f68:	ldr	x8, [x8, x9, lsl #3]
   14f6c:	cmp	x8, #0x0
   14f70:	cinc	w8, w9, ne  // ne = any
   14f74:	str	w8, [x22, #4]
   14f78:	bl	d320 <__gmpz_addmul_ui@plt>
   14f7c:	ldr	x8, [x21, #8]
   14f80:	ldr	x9, [x8]
   14f84:	sub	x10, x9, x26
   14f88:	cmp	x9, x24
   14f8c:	str	x10, [x8]
   14f90:	b.hi	14fa8 <__gmpz_bin_ui@@Base+0x600>  // b.pmore
   14f94:	add	x9, x8, #0x8
   14f98:	ldr	x10, [x9]
   14f9c:	sub	x11, x10, #0x1
   14fa0:	str	x11, [x9], #8
   14fa4:	cbz	x10, 14f98 <__gmpz_bin_ui@@Base+0x5f0>
   14fa8:	ldr	w9, [x21, #4]
   14fac:	sub	w10, w9, #0x1
   14fb0:	ldr	x8, [x8, w10, sxtw #3]
   14fb4:	cmp	x8, #0x0
   14fb8:	cset	w8, eq  // eq = none
   14fbc:	sub	w8, w9, w8
   14fc0:	str	w8, [x21, #4]
   14fc4:	cbz	x23, 15094 <__gmpz_bin_ui@@Base+0x6ec>
   14fc8:	mov	x0, x23
   14fcc:	mov	x1, x21
   14fd0:	str	wzr, [sp]
   14fd4:	bl	c420 <__gmpz_set@plt>
   14fd8:	b	150a4 <__gmpz_bin_ui@@Base+0x6fc>
   14fdc:	ldr	w9, [x21, #4]
   14fe0:	mov	x0, x20
   14fe4:	mov	x1, x20
   14fe8:	mov	x2, x21
   14fec:	sub	w10, w9, #0x1
   14ff0:	ldr	x8, [x8, w10, sxtw #3]
   14ff4:	cmp	x8, #0x0
   14ff8:	cset	w8, eq  // eq = none
   14ffc:	sub	w8, w9, w8
   15000:	str	w8, [x21, #4]
   15004:	bl	c4b0 <__gmpz_mul@plt>
   15008:	sub	x25, x25, #0x1
   1500c:	cmp	x25, x19
   15010:	b.ls	150d8 <__gmpz_bin_ui@@Base+0x730>  // b.plast
   15014:	ldr	x8, [x22, #8]
   15018:	mov	w10, #0x2                   	// #2
   1501c:	bfi	x10, x25, #2, #62
   15020:	lsl	x2, x25, #2
   15024:	ldr	x9, [x8]
   15028:	adds	x9, x9, x10
   1502c:	str	x9, [x8]
   15030:	b.cc	15048 <__gmpz_bin_ui@@Base+0x6a0>  // b.lo, b.ul, b.last
   15034:	add	x9, x8, #0x8
   15038:	ldr	x10, [x9]
   1503c:	adds	x10, x10, #0x1
   15040:	str	x10, [x9], #8
   15044:	b.cs	15038 <__gmpz_bin_ui@@Base+0x690>  // b.hs, b.nlast
   15048:	ldrsw	x9, [x22, #4]
   1504c:	mov	x0, x21
   15050:	mov	x1, x22
   15054:	ldr	x8, [x8, x9, lsl #3]
   15058:	cmp	x8, #0x0
   1505c:	cinc	w8, w9, ne  // ne = any
   15060:	str	w8, [x22, #4]
   15064:	bl	d320 <__gmpz_addmul_ui@plt>
   15068:	ldr	x8, [x21, #8]
   1506c:	ldr	x9, [x8]
   15070:	subs	x9, x9, x25
   15074:	str	x9, [x8]
   15078:	b.cs	14fdc <__gmpz_bin_ui@@Base+0x634>  // b.hs, b.nlast
   1507c:	add	x9, x8, #0x8
   15080:	ldr	x10, [x9]
   15084:	sub	x11, x10, #0x1
   15088:	str	x11, [x9], #8
   1508c:	cbz	x10, 15080 <__gmpz_bin_ui@@Base+0x6d8>
   15090:	b	14fdc <__gmpz_bin_ui@@Base+0x634>
   15094:	mov	x0, sp
   15098:	mov	x1, x21
   1509c:	mov	x23, sp
   150a0:	bl	bf80 <__gmpz_init_set@plt>
   150a4:	mov	x0, x23
   150a8:	mov	x1, x22
   150ac:	mov	x2, x21
   150b0:	mov	x3, x24
   150b4:	mov	x4, x19
   150b8:	mov	x5, xzr
   150bc:	bl	14ec4 <__gmpz_bin_ui@@Base+0x51c>
   150c0:	mov	x0, x20
   150c4:	mov	x1, x20
   150c8:	mov	x2, x23
   150cc:	bl	c4b0 <__gmpz_mul@plt>
   150d0:	mov	x0, sp
   150d4:	bl	cb50 <__gmpz_clear@plt>
   150d8:	ldp	x20, x19, [sp, #80]
   150dc:	ldp	x22, x21, [sp, #64]
   150e0:	ldp	x24, x23, [sp, #48]
   150e4:	ldp	x26, x25, [sp, #32]
   150e8:	ldp	x29, x30, [sp, #16]
   150ec:	add	sp, sp, #0x60
   150f0:	ret

00000000000150f4 <__gmpz_bin_uiui@@Base>:
   150f4:	stp	x29, x30, [sp, #-32]!
   150f8:	stp	x20, x19, [sp, #16]
   150fc:	subs	x8, x1, x2
   15100:	mov	x19, x0
   15104:	mov	x29, sp
   15108:	b.cc	1523c <__gmpz_bin_uiui@@Base+0x148>  // b.lo, b.ul, b.last
   1510c:	cmp	x8, x2
   15110:	csel	x2, x2, x8, hi  // hi = pmore
   15114:	cmp	x2, #0x1
   15118:	b.hi	1514c <__gmpz_bin_uiui@@Base+0x58>  // b.pmore
   1511c:	ldr	w8, [x19]
   15120:	cmp	x2, #0x0
   15124:	csinc	x20, x1, xzr, ne  // ne = any
   15128:	cmp	w8, #0x0
   1512c:	b.le	151c8 <__gmpz_bin_uiui@@Base+0xd4>
   15130:	ldr	x0, [x19, #8]
   15134:	mov	w8, #0x1                   	// #1
   15138:	str	x20, [x0]
   1513c:	str	w8, [x19, #4]
   15140:	ldp	x20, x19, [sp, #16]
   15144:	ldp	x29, x30, [sp], #32
   15148:	ret
   1514c:	cmp	x1, #0x43
   15150:	b.hi	151d8 <__gmpz_bin_uiui@@Base+0xe4>  // b.pmore
   15154:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15158:	adrp	x10, 59000 <__gmp_randget_mt@@Base+0x44c>
   1515c:	sub	w11, w1, w2
   15160:	ldr	x8, [x8, #3848]
   15164:	sub	w9, w2, #0x2
   15168:	add	x10, x10, #0xcb8
   1516c:	sub	w13, w11, #0x2
   15170:	ldr	x9, [x10, w9, uxtw #3]
   15174:	ldr	x10, [x10, w13, uxtw #3]
   15178:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1517c:	ldr	x13, [x13, #3992]
   15180:	ubfx	x12, x1, #1, #31
   15184:	ldr	x8, [x8, x1, lsl #3]
   15188:	ubfx	x14, x2, #1, #31
   1518c:	sub	w12, w12, #0x1
   15190:	sub	w14, w14, #0x1
   15194:	lsr	w11, w11, #1
   15198:	ldrb	w12, [x13, w12, uxtw]
   1519c:	ldrb	w14, [x13, w14, uxtw]
   151a0:	sub	w11, w11, #0x1
   151a4:	ldrb	w11, [x13, w11, uxtw]
   151a8:	mul	x8, x9, x8
   151ac:	ldr	w9, [x19]
   151b0:	mul	x8, x8, x10
   151b4:	sub	w10, w12, w14
   151b8:	sub	w10, w10, w11
   151bc:	cmp	w9, #0x0
   151c0:	lsl	x20, x8, x10
   151c4:	b.gt	15130 <__gmpz_bin_uiui@@Base+0x3c>
   151c8:	mov	w1, #0x1                   	// #1
   151cc:	mov	x0, x19
   151d0:	bl	c080 <__gmpz_realloc@plt>
   151d4:	b	15134 <__gmpz_bin_uiui@@Base+0x40>
   151d8:	cmp	x2, #0x19
   151dc:	b.hi	151f0 <__gmpz_bin_uiui@@Base+0xfc>  // b.pmore
   151e0:	mov	x0, x19
   151e4:	ldp	x20, x19, [sp, #16]
   151e8:	ldp	x29, x30, [sp], #32
   151ec:	b	1524c <__gmpz_bin_uiui@@Base+0x158>
   151f0:	cmp	x2, #0x46
   151f4:	b.hi	15208 <__gmpz_bin_uiui@@Base+0x114>  // b.pmore
   151f8:	mov	x0, x19
   151fc:	ldp	x20, x19, [sp, #16]
   15200:	ldp	x29, x30, [sp], #32
   15204:	b	15470 <__gmpz_bin_uiui@@Base+0x37c>
   15208:	cmp	x2, #0x200
   1520c:	b.cc	1522c <__gmpz_bin_uiui@@Base+0x138>  // b.lo, b.ul, b.last
   15210:	lsr	x8, x1, #4
   15214:	cmp	x2, x8
   15218:	b.ls	1522c <__gmpz_bin_uiui@@Base+0x138>  // b.plast
   1521c:	mov	x0, x19
   15220:	ldp	x20, x19, [sp, #16]
   15224:	ldp	x29, x30, [sp], #32
   15228:	b	15644 <__gmpz_bin_uiui@@Base+0x550>
   1522c:	mov	x0, x19
   15230:	ldp	x20, x19, [sp, #16]
   15234:	ldp	x29, x30, [sp], #32
   15238:	b	15af0 <__gmpz_bin_uiui@@Base+0x9fc>
   1523c:	str	wzr, [x19, #4]
   15240:	ldp	x20, x19, [sp, #16]
   15244:	ldp	x29, x30, [sp], #32
   15248:	ret
   1524c:	sub	sp, sp, #0x70
   15250:	stp	x29, x30, [sp, #16]
   15254:	stp	x28, x27, [sp, #32]
   15258:	stp	x26, x25, [sp, #48]
   1525c:	stp	x24, x23, [sp, #64]
   15260:	stp	x22, x21, [sp, #80]
   15264:	stp	x20, x19, [sp, #96]
   15268:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1526c:	ldr	x9, [x9, #3880]
   15270:	mov	x20, x2
   15274:	mov	x21, x0
   15278:	mov	w8, #0x9                   	// #9
   1527c:	add	x29, sp, #0x10
   15280:	sub	w10, w8, #0x2
   15284:	ldr	x10, [x9, w10, uxtw #3]
   15288:	sub	w8, w8, #0x1
   1528c:	cmp	x10, x1
   15290:	b.cc	15280 <__gmpz_bin_uiui@@Base+0x18c>  // b.lo, b.ul, b.last
   15294:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15298:	ldr	x10, [x10, #3992]
   1529c:	cmp	w8, #0x8
   152a0:	mov	w9, #0x8                   	// #8
   152a4:	csel	w26, w8, w9, cc  // cc = lo, ul, last
   152a8:	add	x8, x10, x20, lsr #1
   152ac:	ldurb	w24, [x8, #-1]
   152b0:	sub	x8, x1, x20
   152b4:	cmp	x26, x20
   152b8:	add	x22, x8, #0x1
   152bc:	b.cs	153d4 <__gmpz_bin_uiui@@Base+0x2e0>  // b.hs, b.nlast
   152c0:	clz	x8, x1
   152c4:	mov	w9, #0x40                  	// #64
   152c8:	sub	x8, x9, x8
   152cc:	ldrsw	x9, [x21]
   152d0:	mul	x8, x8, x20
   152d4:	lsr	x8, x8, #6
   152d8:	add	x1, x8, #0x3
   152dc:	cmp	x1, x9
   152e0:	str	x21, [sp, #8]
   152e4:	b.gt	15450 <__gmpz_bin_uiui@@Base+0x35c>
   152e8:	ldr	x21, [x21, #8]
   152ec:	adrp	x27, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   152f0:	sub	w19, w26, #0x1
   152f4:	add	x27, x27, #0x9a0
   152f8:	ldr	x8, [x27, w19, uxtw #3]
   152fc:	mov	x0, x22
   15300:	blr	x8
   15304:	adrp	x28, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   15308:	add	x28, x28, #0x3f
   1530c:	ldrb	w8, [x28, w19, uxtw]
   15310:	add	x23, x22, x26
   15314:	sub	w19, w20, w26
   15318:	mov	w2, #0x1                   	// #1
   1531c:	sub	w22, w24, w8
   15320:	str	x0, [x21]
   15324:	cmp	w26, w19
   15328:	csel	w26, w26, w19, cc  // cc = lo, ul, last
   1532c:	sub	w25, w26, #0x1
   15330:	ldr	x8, [x27, w25, uxtw #3]
   15334:	mov	x0, x23
   15338:	mov	x24, x2
   1533c:	blr	x8
   15340:	ldrb	w8, [x28, w25, uxtw]
   15344:	mov	x3, x0
   15348:	mov	x0, x21
   1534c:	mov	x1, x21
   15350:	mov	x2, x24
   15354:	add	x23, x23, x26
   15358:	sub	w22, w22, w8
   1535c:	bl	d490 <__gmpn_mul_1@plt>
   15360:	cmp	x0, #0x0
   15364:	cinc	x2, x24, ne  // ne = any
   15368:	subs	w19, w19, w26
   1536c:	str	x0, [x21, x24, lsl #3]
   15370:	b.ne	15324 <__gmpz_bin_uiui@@Base+0x230>  // b.any
   15374:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15378:	ldr	x9, [x9, #3848]
   1537c:	adrp	x10, 59000 <__gmp_randget_mt@@Base+0x44c>
   15380:	lsl	x8, x20, #3
   15384:	add	x10, x10, #0xcb8
   15388:	ldr	x3, [x9, x8]
   1538c:	add	x8, x8, x10
   15390:	ldur	x4, [x8, #-16]
   15394:	mov	x25, x0
   15398:	mov	x0, x21
   1539c:	mov	x1, x21
   153a0:	mov	w5, w22
   153a4:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   153a8:	cmp	x25, #0x0
   153ac:	cinc	x9, x24, ne  // ne = any
   153b0:	add	x9, x21, x9, lsl #3
   153b4:	ldr	x21, [sp, #8]
   153b8:	cinc	w8, w24, ne  // ne = any
   153bc:	add	w8, w8, #0x1
   153c0:	sub	x9, x9, #0x8
   153c4:	ldr	x10, [x9], #-8
   153c8:	sub	w8, w8, #0x1
   153cc:	cbz	x10, 153c4 <__gmpz_bin_uiui@@Base+0x2d0>
   153d0:	b	1542c <__gmpz_bin_uiui@@Base+0x338>
   153d4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   153d8:	sub	x19, x20, #0x1
   153dc:	add	x8, x8, #0x9a0
   153e0:	ldr	x8, [x8, x19, lsl #3]
   153e4:	mov	x0, x22
   153e8:	blr	x8
   153ec:	adrp	x8, 59000 <__gmp_randget_mt@@Base+0x44c>
   153f0:	add	x8, x8, #0xcb8
   153f4:	adrp	x9, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   153f8:	add	x9, x9, #0x3f
   153fc:	add	x8, x8, x20, lsl #3
   15400:	ldrb	w9, [x9, x19]
   15404:	ldur	x8, [x8, #-16]
   15408:	ldr	w10, [x21]
   1540c:	sub	w9, w24, w9
   15410:	mul	x8, x8, x0
   15414:	cmp	w10, #0x0
   15418:	lsr	x19, x8, x9
   1541c:	b.le	15460 <__gmpz_bin_uiui@@Base+0x36c>
   15420:	ldr	x0, [x21, #8]
   15424:	str	x19, [x0]
   15428:	mov	w8, #0x1                   	// #1
   1542c:	str	w8, [x21, #4]
   15430:	ldp	x20, x19, [sp, #96]
   15434:	ldp	x22, x21, [sp, #80]
   15438:	ldp	x24, x23, [sp, #64]
   1543c:	ldp	x26, x25, [sp, #48]
   15440:	ldp	x28, x27, [sp, #32]
   15444:	ldp	x29, x30, [sp, #16]
   15448:	add	sp, sp, #0x70
   1544c:	ret
   15450:	mov	x0, x21
   15454:	bl	c080 <__gmpz_realloc@plt>
   15458:	mov	x21, x0
   1545c:	b	152ec <__gmpz_bin_uiui@@Base+0x1f8>
   15460:	mov	w1, #0x1                   	// #1
   15464:	mov	x0, x21
   15468:	bl	c080 <__gmpz_realloc@plt>
   1546c:	b	15424 <__gmpz_bin_uiui@@Base+0x330>
   15470:	sub	sp, sp, #0x190
   15474:	stp	x20, x19, [sp, #384]
   15478:	lsr	x20, x2, #1
   1547c:	stp	x22, x21, [sp, #368]
   15480:	mov	x21, x2
   15484:	mov	x22, x1
   15488:	mov	x19, x0
   1548c:	cmp	x2, #0x33
   15490:	mov	x2, x20
   15494:	stp	x29, x30, [sp, #320]
   15498:	str	x28, [sp, #336]
   1549c:	stp	x24, x23, [sp, #352]
   154a0:	add	x29, sp, #0x140
   154a4:	b.hi	154e8 <__gmpz_bin_uiui@@Base+0x3f4>  // b.pmore
   154a8:	bl	1524c <__gmpz_bin_uiui@@Base+0x158>
   154ac:	sub	x24, x22, x20
   154b0:	cmp	x24, #0x43
   154b4:	sub	x21, x21, x20
   154b8:	b.ls	154fc <__gmpz_bin_uiui@@Base+0x408>  // b.plast
   154bc:	mov	w8, #0x26                  	// #38
   154c0:	add	x9, sp, #0x10
   154c4:	cmp	x21, #0x19
   154c8:	mov	x0, sp
   154cc:	mov	x1, x24
   154d0:	mov	x2, x21
   154d4:	str	w8, [sp]
   154d8:	str	x9, [sp, #8]
   154dc:	b.hi	15598 <__gmpz_bin_uiui@@Base+0x4a4>  // b.pmore
   154e0:	bl	1524c <__gmpz_bin_uiui@@Base+0x158>
   154e4:	b	1559c <__gmpz_bin_uiui@@Base+0x4a8>
   154e8:	bl	15470 <__gmpz_bin_uiui@@Base+0x37c>
   154ec:	sub	x24, x22, x20
   154f0:	cmp	x24, #0x43
   154f4:	sub	x21, x21, x20
   154f8:	b.hi	154bc <__gmpz_bin_uiui@@Base+0x3c8>  // b.pmore
   154fc:	ldp	w8, w23, [x19]
   15500:	sxtw	x23, w23
   15504:	cmp	w23, w8
   15508:	b.ge	15630 <__gmpz_bin_uiui@@Base+0x53c>  // b.tcont
   1550c:	ldr	x22, [x19, #8]
   15510:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15514:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15518:	ldr	x8, [x8, #3848]
   1551c:	adrp	x10, 59000 <__gmp_randget_mt@@Base+0x44c>
   15520:	sub	w11, w24, w21
   15524:	ldr	x13, [x13, #3992]
   15528:	sub	w9, w21, #0x2
   1552c:	add	x10, x10, #0xcb8
   15530:	sub	w14, w11, #0x2
   15534:	ubfx	x12, x24, #1, #31
   15538:	ldr	x9, [x10, w9, uxtw #3]
   1553c:	ldr	x10, [x10, w14, uxtw #3]
   15540:	ubfx	x14, x21, #1, #31
   15544:	sub	w12, w12, #0x1
   15548:	sub	w14, w14, #0x1
   1554c:	lsr	w11, w11, #1
   15550:	ldr	x8, [x8, x24, lsl #3]
   15554:	ldrb	w12, [x13, w12, uxtw]
   15558:	ldrb	w14, [x13, w14, uxtw]
   1555c:	sub	w11, w11, #0x1
   15560:	ldrb	w11, [x13, w11, uxtw]
   15564:	mul	x8, x9, x8
   15568:	sub	w9, w12, w14
   1556c:	mul	x8, x8, x10
   15570:	sub	w9, w9, w11
   15574:	lsl	x3, x8, x9
   15578:	mov	x0, x22
   1557c:	mov	x1, x22
   15580:	mov	x2, x23
   15584:	bl	d490 <__gmpn_mul_1@plt>
   15588:	cmp	x0, #0x0
   1558c:	str	x0, [x22, x23, lsl #3]
   15590:	cinc	x23, x23, ne  // ne = any
   15594:	b	155b4 <__gmpz_bin_uiui@@Base+0x4c0>
   15598:	bl	15470 <__gmpz_bin_uiui@@Base+0x37c>
   1559c:	mov	x2, sp
   155a0:	mov	x0, x19
   155a4:	mov	x1, x19
   155a8:	bl	c4b0 <__gmpz_mul@plt>
   155ac:	ldr	x22, [x19, #8]
   155b0:	ldrsw	x23, [x19, #4]
   155b4:	adrp	x11, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   155b8:	sub	x8, x21, #0xd
   155bc:	adrp	x9, 59000 <__gmp_randget_mt@@Base+0x44c>
   155c0:	adrp	x10, 59000 <__gmp_randget_mt@@Base+0x44c>
   155c4:	add	x11, x11, #0x28
   155c8:	add	x9, x9, #0xeb8
   155cc:	add	x10, x10, #0xf70
   155d0:	lsl	x12, x8, #3
   155d4:	ldrb	w8, [x11, x8]
   155d8:	ldr	x3, [x9, x12]
   155dc:	ldr	x4, [x10, x12]
   155e0:	cmp	x21, x20
   155e4:	cset	w9, ne  // ne = any
   155e8:	sub	w5, w8, w9
   155ec:	mov	x0, x22
   155f0:	mov	x1, x22
   155f4:	mov	x2, x23
   155f8:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   155fc:	sub	x8, x22, #0x8
   15600:	ldr	x9, [x8, x23, lsl #3]
   15604:	sub	x23, x23, #0x1
   15608:	cbz	x9, 15600 <__gmpz_bin_uiui@@Base+0x50c>
   1560c:	add	w8, w23, #0x1
   15610:	str	w8, [x19, #4]
   15614:	ldp	x20, x19, [sp, #384]
   15618:	ldp	x22, x21, [sp, #368]
   1561c:	ldp	x24, x23, [sp, #352]
   15620:	ldr	x28, [sp, #336]
   15624:	ldp	x29, x30, [sp, #320]
   15628:	add	sp, sp, #0x190
   1562c:	ret
   15630:	add	x1, x23, #0x1
   15634:	mov	x0, x19
   15638:	bl	c080 <__gmpz_realloc@plt>
   1563c:	mov	x22, x0
   15640:	b	15510 <__gmpz_bin_uiui@@Base+0x41c>
   15644:	stp	x29, x30, [sp, #-64]!
   15648:	sub	x8, x1, #0x5
   1564c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   15650:	movk	x9, #0xaaab
   15654:	orr	x8, x8, #0x1
   15658:	str	x23, [sp, #16]
   1565c:	umulh	x23, x8, x9
   15660:	lsr	x9, x23, #4
   15664:	lsr	x8, x8, #11
   15668:	and	x9, x9, #0xffffffffffffff8
   1566c:	stp	x22, x21, [sp, #32]
   15670:	stp	x20, x19, [sp, #48]
   15674:	mov	x29, sp
   15678:	mov	x21, x2
   1567c:	mov	x22, x1
   15680:	mov	x19, x0
   15684:	cmp	x8, #0x17c
   15688:	add	x1, x9, #0x8
   1568c:	str	xzr, [x29, #24]
   15690:	b.hi	15a8c <__gmpz_bin_uiui@@Base+0x998>  // b.pmore
   15694:	add	x9, x1, #0xf
   15698:	mov	x8, sp
   1569c:	and	x9, x9, #0x3ffffffffffffff0
   156a0:	sub	x20, x8, x9
   156a4:	mov	sp, x20
   156a8:	mov	x0, x20
   156ac:	mov	x1, x22
   156b0:	lsr	x23, x23, #1
   156b4:	bl	d200 <__gmp_primesieve@plt>
   156b8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   156bc:	ldr	x8, [x8, #3880]
   156c0:	mov	w10, #0x9                   	// #9
   156c4:	sub	w9, w10, #0x2
   156c8:	ldr	x9, [x8, w9, uxtw #3]
   156cc:	sub	w10, w10, #0x1
   156d0:	cmp	x9, x22
   156d4:	b.cc	156c4 <__gmpz_bin_uiui@@Base+0x5d0>  // b.lo, b.ul, b.last
   156d8:	add	x9, x0, #0x1
   156dc:	mov	w10, w10
   156e0:	udiv	x10, x9, x10
   156e4:	lsl	x10, x10, #3
   156e8:	add	x10, x10, #0x8
   156ec:	mov	w11, #0x9                   	// #9
   156f0:	sub	w12, w11, #0x2
   156f4:	ldr	x12, [x8, w12, uxtw #3]
   156f8:	sub	w11, w11, #0x1
   156fc:	cmp	x12, x22
   15700:	b.cc	156f0 <__gmpz_bin_uiui@@Base+0x5fc>  // b.lo, b.ul, b.last
   15704:	mov	w8, w11
   15708:	udiv	x8, x9, x8
   1570c:	mov	w11, #0x7f00                	// #32512
   15710:	lsl	x8, x8, #3
   15714:	cmp	x10, x11
   15718:	add	x1, x8, #0x8
   1571c:	b.hi	15a9c <__gmpz_bin_uiui@@Base+0x9a8>  // b.pmore
   15720:	add	x9, x1, #0xf
   15724:	mov	x8, sp
   15728:	and	x9, x9, #0xfffffffffffffff0
   1572c:	sub	x1, x8, x9
   15730:	mov	sp, x1
   15734:	lsr	x9, x21, #1
   15738:	and	x9, x9, #0x5555555555555555
   1573c:	lsr	x12, x22, #1
   15740:	sub	x9, x21, x9
   15744:	mov	x8, #0xffffffffffffffff    	// #-1
   15748:	sub	x10, x22, x21
   1574c:	and	x13, x12, #0x5555555555555555
   15750:	lsr	x14, x9, #2
   15754:	udiv	x11, x8, x22
   15758:	lsr	x8, x10, #1
   1575c:	sub	x13, x22, x13
   15760:	and	x9, x9, #0x3333333333333333
   15764:	and	x14, x14, #0x3333333333333333
   15768:	and	x8, x8, #0x5555555555555555
   1576c:	add	x9, x14, x9
   15770:	lsr	x14, x13, #2
   15774:	sub	x8, x10, x8
   15778:	and	x13, x13, #0x3333333333333333
   1577c:	and	x14, x14, #0x3333333333333333
   15780:	add	x13, x14, x13
   15784:	lsr	x14, x8, #2
   15788:	and	x8, x8, #0x3333333333333333
   1578c:	and	x14, x14, #0x3333333333333333
   15790:	add	x9, x9, x9, lsr #4
   15794:	add	x8, x14, x8
   15798:	add	x13, x13, x13, lsr #4
   1579c:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   157a0:	add	x8, x8, x8, lsr #4
   157a4:	and	x13, x13, #0xf0f0f0f0f0f0f0f
   157a8:	add	x9, x9, x9, lsr #8
   157ac:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   157b0:	add	x13, x13, x13, lsr #8
   157b4:	add	x9, x9, x9, lsr #16
   157b8:	add	x8, x8, x8, lsr #8
   157bc:	add	x13, x13, x13, lsr #16
   157c0:	lsr	x14, x9, #32
   157c4:	add	x8, x8, x8, lsr #16
   157c8:	add	w9, w14, w9
   157cc:	lsr	x14, x13, #32
   157d0:	add	w13, w14, w13
   157d4:	lsr	x14, x8, #32
   157d8:	and	x9, x9, #0xff
   157dc:	add	w8, w14, w8
   157e0:	sub	x9, x9, w13, uxtb
   157e4:	add	x8, x9, w8, uxtb
   157e8:	mov	w9, #0x1                   	// #1
   157ec:	lsl	x8, x9, x8
   157f0:	cmp	x8, x11
   157f4:	b.ls	15808 <__gmpz_bin_uiui@@Base+0x714>  // b.plast
   157f8:	str	x8, [x1]
   157fc:	mov	w9, #0x1                   	// #1
   15800:	mov	w8, #0x1                   	// #1
   15804:	b	1580c <__gmpz_bin_uiui@@Base+0x718>
   15808:	mov	x9, xzr
   1580c:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   15810:	mov	x14, xzr
   15814:	movk	x13, #0xaaab
   15818:	mov	x16, x21
   1581c:	mov	x15, x22
   15820:	umulh	x17, x16, x13
   15824:	lsr	x17, x17, #1
   15828:	add	x18, x17, x17, lsl #1
   1582c:	sub	x16, x16, x18
   15830:	umulh	x18, x15, x13
   15834:	lsr	x18, x18, #1
   15838:	add	x14, x16, x14
   1583c:	add	x16, x18, x18, lsl #1
   15840:	sub	x16, x15, x16
   15844:	cmp	x16, x14
   15848:	add	x16, x8, x8, lsl #1
   1584c:	cset	w14, cc  // cc = lo, ul, last
   15850:	csel	x8, x16, x8, cc  // cc = lo, ul, last
   15854:	cmp	x15, #0x8
   15858:	mov	x16, x17
   1585c:	mov	x15, x18
   15860:	b.hi	15820 <__gmpz_bin_uiui@@Base+0x72c>  // b.pmore
   15864:	clz	x14, x22
   15868:	mov	w15, #0x40                  	// #64
   1586c:	sub	w14, w15, w14
   15870:	mov	w17, #0x1                   	// #1
   15874:	asr	w14, w14, #1
   15878:	lsl	x15, x17, x14
   1587c:	lsr	x14, x22, x14
   15880:	add	x14, x15, x14
   15884:	lsr	x14, x14, #1
   15888:	mov	x15, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1588c:	sub	x14, x14, #0x5
   15890:	movk	x15, #0xaaab
   15894:	orr	x14, x14, #0x1
   15898:	umulh	x14, x14, x15
   1589c:	mov	x13, xzr
   158a0:	mov	x16, xzr
   158a4:	lsr	x18, x14, #1
   158a8:	mov	w2, #0x7                   	// #7
   158ac:	b	158c8 <__gmpz_bin_uiui@@Base+0x7d4>
   158b0:	ror	x0, x17, #63
   158b4:	add	x13, x13, x17, lsr #63
   158b8:	cmp	x14, x18
   158bc:	add	x2, x15, #0x3
   158c0:	mov	x17, x0
   158c4:	b.cs	1594c <__gmpz_bin_uiui@@Base+0x858>  // b.hs, b.nlast
   158c8:	ldr	x0, [x20, x13, lsl #3]
   158cc:	mov	x14, x16
   158d0:	mov	x15, x2
   158d4:	add	x16, x16, #0x1
   158d8:	tst	x0, x17
   158dc:	b.ne	158b0 <__gmpz_bin_uiui@@Base+0x7bc>  // b.any
   158e0:	add	x0, x16, x16, lsl #1
   158e4:	and	x2, x16, #0x1
   158e8:	cmp	x8, x11
   158ec:	add	x2, x0, x2
   158f0:	b.ls	15904 <__gmpz_bin_uiui@@Base+0x810>  // b.plast
   158f4:	add	x0, x9, #0x1
   158f8:	str	x8, [x1, x9, lsl #3]
   158fc:	mov	x9, x0
   15900:	mov	w8, #0x1                   	// #1
   15904:	mov	x0, xzr
   15908:	add	x2, x2, #0x1
   1590c:	mov	x3, x22
   15910:	mov	x4, x21
   15914:	udiv	x5, x4, x2
   15918:	udiv	x6, x3, x2
   1591c:	msub	x4, x5, x2, x4
   15920:	msub	x3, x6, x2, x3
   15924:	add	x0, x4, x0
   15928:	cmp	x3, x0
   1592c:	csinc	x3, x2, xzr, cc  // cc = lo, ul, last
   15930:	cset	w0, cc  // cc = lo, ul, last
   15934:	cmp	x6, x2
   15938:	mul	x8, x3, x8
   1593c:	mov	x3, x6
   15940:	mov	x4, x5
   15944:	b.cs	15914 <__gmpz_bin_uiui@@Base+0x820>  // b.hs, b.nlast
   15948:	b	158b0 <__gmpz_bin_uiui@@Base+0x7bc>
   1594c:	sub	x12, x12, #0x5
   15950:	mov	x17, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   15954:	orr	x12, x12, #0x1
   15958:	movk	x17, #0xaaab
   1595c:	umulh	x12, x12, x17
   15960:	lsl	x16, x11, #1
   15964:	lsr	x12, x12, #1
   15968:	b	15988 <__gmpz_bin_uiui@@Base+0x894>
   1596c:	mul	x8, x17, x8
   15970:	add	x14, x14, #0x1
   15974:	add	x13, x13, x0, lsr #63
   15978:	ror	x0, x0, #63
   1597c:	cmp	x14, x12
   15980:	add	x15, x15, #0x3
   15984:	b.cs	159d0 <__gmpz_bin_uiui@@Base+0x8dc>  // b.hs, b.nlast
   15988:	ldr	x17, [x20, x13, lsl #3]
   1598c:	tst	x17, x0
   15990:	b.ne	15970 <__gmpz_bin_uiui@@Base+0x87c>  // b.any
   15994:	and	x17, x14, #0x1
   15998:	add	x17, x15, x17
   1599c:	udiv	x18, x22, x17
   159a0:	udiv	x2, x21, x17
   159a4:	msub	x18, x18, x17, x22
   159a8:	msub	x2, x2, x17, x21
   159ac:	cmp	x18, x2
   159b0:	b.cs	15970 <__gmpz_bin_uiui@@Base+0x87c>  // b.hs, b.nlast
   159b4:	cmp	x8, x16
   159b8:	b.ls	1596c <__gmpz_bin_uiui@@Base+0x878>  // b.plast
   159bc:	add	x18, x9, #0x1
   159c0:	str	x8, [x1, x9, lsl #3]
   159c4:	mov	x9, x18
   159c8:	mov	x8, x17
   159cc:	b	15970 <__gmpz_bin_uiui@@Base+0x87c>
   159d0:	sub	x10, x10, #0x5
   159d4:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   159d8:	movk	x12, #0xaaab
   159dc:	orr	x10, x10, #0x1
   159e0:	umulh	x10, x10, x12
   159e4:	lsr	x12, x10, #1
   159e8:	mov	w13, #0x1                   	// #1
   159ec:	add	x14, x12, #0x1
   159f0:	add	x15, x12, x12, lsl #1
   159f4:	and	x11, x11, #0x7fffffffffffffff
   159f8:	add	x10, x12, #0x2
   159fc:	lsr	x12, x14, #6
   15a00:	lsl	x14, x13, x14
   15a04:	add	x13, x15, #0x7
   15a08:	b	15a28 <__gmpz_bin_uiui@@Base+0x934>
   15a0c:	mul	x8, x15, x8
   15a10:	add	x12, x12, x14, lsr #63
   15a14:	ror	x14, x14, #63
   15a18:	cmp	x10, x23
   15a1c:	add	x10, x10, #0x1
   15a20:	add	x13, x13, #0x3
   15a24:	b.hi	15a58 <__gmpz_bin_uiui@@Base+0x964>  // b.pmore
   15a28:	ldr	x15, [x20, x12, lsl #3]
   15a2c:	tst	x15, x14
   15a30:	b.ne	15a10 <__gmpz_bin_uiui@@Base+0x91c>  // b.any
   15a34:	and	x15, x10, #0x1
   15a38:	cmp	x8, x11
   15a3c:	add	x15, x13, x15
   15a40:	b.ls	15a0c <__gmpz_bin_uiui@@Base+0x918>  // b.plast
   15a44:	add	x16, x9, #0x1
   15a48:	str	x8, [x1, x9, lsl #3]
   15a4c:	mov	x9, x16
   15a50:	mov	x8, x15
   15a54:	b	15a10 <__gmpz_bin_uiui@@Base+0x91c>
   15a58:	cbz	x9, 15aac <__gmpz_bin_uiui@@Base+0x9b8>
   15a5c:	add	x2, x9, #0x1
   15a60:	mov	x0, x19
   15a64:	str	x8, [x1, x9, lsl #3]
   15a68:	bl	cd70 <__gmpz_prodlimbs@plt>
   15a6c:	ldr	x0, [x29, #24]
   15a70:	cbnz	x0, 15ad0 <__gmpz_bin_uiui@@Base+0x9dc>
   15a74:	mov	sp, x29
   15a78:	ldp	x20, x19, [sp, #48]
   15a7c:	ldp	x22, x21, [sp, #32]
   15a80:	ldr	x23, [sp, #16]
   15a84:	ldp	x29, x30, [sp], #64
   15a88:	ret
   15a8c:	add	x0, x29, #0x18
   15a90:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15a94:	mov	x20, x0
   15a98:	b	156a8 <__gmpz_bin_uiui@@Base+0x5b4>
   15a9c:	add	x0, x29, #0x18
   15aa0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15aa4:	mov	x1, x0
   15aa8:	b	15734 <__gmpz_bin_uiui@@Base+0x640>
   15aac:	ldr	w9, [x19]
   15ab0:	cmp	w9, #0x0
   15ab4:	b.le	15ad8 <__gmpz_bin_uiui@@Base+0x9e4>
   15ab8:	ldr	x0, [x19, #8]
   15abc:	str	x8, [x0]
   15ac0:	mov	w8, #0x1                   	// #1
   15ac4:	str	w8, [x19, #4]
   15ac8:	ldr	x0, [x29, #24]
   15acc:	cbz	x0, 15a74 <__gmpz_bin_uiui@@Base+0x980>
   15ad0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   15ad4:	b	15a74 <__gmpz_bin_uiui@@Base+0x980>
   15ad8:	mov	w1, #0x1                   	// #1
   15adc:	mov	x0, x19
   15ae0:	mov	x20, x8
   15ae4:	bl	c080 <__gmpz_realloc@plt>
   15ae8:	mov	x8, x20
   15aec:	b	15abc <__gmpz_bin_uiui@@Base+0x9c8>
   15af0:	stp	x29, x30, [sp, #-96]!
   15af4:	stp	x28, x27, [sp, #16]
   15af8:	stp	x26, x25, [sp, #32]
   15afc:	stp	x24, x23, [sp, #48]
   15b00:	stp	x22, x21, [sp, #64]
   15b04:	stp	x20, x19, [sp, #80]
   15b08:	mov	x29, sp
   15b0c:	sub	sp, sp, #0x40
   15b10:	lsr	x8, x1, #6
   15b14:	add	x8, x8, x8, lsl #1
   15b18:	add	x10, x8, #0x3
   15b1c:	cmp	x8, #0x27
   15b20:	lsr	x8, x10, #1
   15b24:	mov	w9, #0x27                  	// #39
   15b28:	add	x8, x8, #0x13
   15b2c:	csel	x8, x9, x8, cc  // cc = lo, ul, last
   15b30:	cmp	x8, x2
   15b34:	csel	x19, x8, x2, lt  // lt = tstop
   15b38:	lsl	x8, x19, #3
   15b3c:	mov	x21, x1
   15b40:	add	x1, x8, #0xb0
   15b44:	mov	w8, #0x7f00                	// #32512
   15b48:	mov	x22, x2
   15b4c:	cmp	x1, x8
   15b50:	stur	x0, [x29, #-48]
   15b54:	stur	xzr, [x29, #-8]
   15b58:	b.hi	15f24 <__gmpz_bin_uiui@@Base+0xe30>  // b.pmore
   15b5c:	add	x9, x1, #0xf
   15b60:	mov	x8, sp
   15b64:	and	x9, x9, #0xfffffffffffffff0
   15b68:	sub	x20, x8, x9
   15b6c:	mov	sp, x20
   15b70:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15b74:	ldr	x8, [x8, #3880]
   15b78:	add	x9, x19, #0x1
   15b7c:	mov	w11, #0x9                   	// #9
   15b80:	sub	w10, w11, #0x2
   15b84:	ldr	x10, [x8, w10, uxtw #3]
   15b88:	sub	w11, w11, #0x1
   15b8c:	cmp	x10, x21
   15b90:	b.cc	15b80 <__gmpz_bin_uiui@@Base+0xa8c>  // b.lo, b.ul, b.last
   15b94:	add	x24, x20, x9, lsl #3
   15b98:	mov	w27, #0x9                   	// #9
   15b9c:	stur	w11, [x29, #-36]
   15ba0:	sub	w9, w27, #0x2
   15ba4:	ldr	x9, [x8, w9, uxtw #3]
   15ba8:	sub	w27, w27, #0x1
   15bac:	cmp	x9, x22
   15bb0:	b.cc	15ba0 <__gmpz_bin_uiui@@Base+0xaac>  // b.lo, b.ul, b.last
   15bb4:	mov	x9, #0x41ef                	// #16879
   15bb8:	movk	x9, #0x7ec2, lsl #16
   15bbc:	stur	x21, [x29, #-56]
   15bc0:	sub	x10, x21, x22
   15bc4:	movk	x9, #0x8186, lsl #32
   15bc8:	adrp	x21, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15bcc:	mov	w23, #0x1                   	// #1
   15bd0:	movk	x9, #0x3352, lsl #48
   15bd4:	mov	w8, #0x1a                  	// #26
   15bd8:	add	x21, x21, #0x9a0
   15bdc:	add	x25, x10, #0x1
   15be0:	mov	w28, #0x1                   	// #1
   15be4:	stur	x10, [x29, #-64]
   15be8:	str	x23, [x20]
   15bec:	stp	x24, x22, [x29, #-32]
   15bf0:	sub	x10, x22, x8
   15bf4:	add	x11, x10, #0x1
   15bf8:	mov	w12, w27
   15bfc:	cmp	x11, x12
   15c00:	csinc	x19, x12, x10, hi  // hi = pmore
   15c04:	str	x9, [x24]
   15c08:	cbz	x19, 15c84 <__gmpz_bin_uiui@@Base+0xb90>
   15c0c:	mov	w27, #0x1                   	// #1
   15c10:	mov	x26, x8
   15c14:	sub	w8, w19, #0x1
   15c18:	ldr	x8, [x21, w8, uxtw #3]
   15c1c:	mov	x0, x26
   15c20:	blr	x8
   15c24:	rbit	x8, x0
   15c28:	clz	x8, x8
   15c2c:	and	x19, x19, #0xffffffff
   15c30:	lsr	x3, x0, x8
   15c34:	mov	x0, x24
   15c38:	mov	x1, x24
   15c3c:	mov	x2, x27
   15c40:	add	x26, x19, x26
   15c44:	bl	d490 <__gmpn_mul_1@plt>
   15c48:	sub	x8, x22, x26
   15c4c:	cmp	x0, #0x0
   15c50:	add	x9, x8, #0x1
   15c54:	str	x0, [x24, x27, lsl #3]
   15c58:	cinc	x27, x27, ne  // ne = any
   15c5c:	cmp	x19, x9
   15c60:	csinc	x19, x19, x8, cc  // cc = lo, ul, last
   15c64:	cmp	x27, #0x13
   15c68:	b.hi	15c70 <__gmpz_bin_uiui@@Base+0xb7c>  // b.pmore
   15c6c:	cbnz	x19, 15c14 <__gmpz_bin_uiui@@Base+0xb20>
   15c70:	mov	w8, w19
   15c74:	stur	w19, [x29, #-12]
   15c78:	subs	w22, w26, w28
   15c7c:	b.ne	15c98 <__gmpz_bin_uiui@@Base+0xba4>  // b.any
   15c80:	b	15cf0 <__gmpz_bin_uiui@@Base+0xbfc>
   15c84:	stur	wzr, [x29, #-12]
   15c88:	mov	x26, x8
   15c8c:	mov	w27, #0x1                   	// #1
   15c90:	subs	w22, w26, w28
   15c94:	b.eq	15cf0 <__gmpz_bin_uiui@@Base+0xbfc>  // b.none
   15c98:	ldur	w24, [x29, #-36]
   15c9c:	adrp	x28, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15ca0:	add	x28, x28, #0x9a0
   15ca4:	cmp	w24, w22
   15ca8:	csel	w21, w24, w22, cc  // cc = lo, ul, last
   15cac:	sub	w8, w21, #0x1
   15cb0:	ldr	x8, [x28, w8, uxtw #3]
   15cb4:	mov	x0, x25
   15cb8:	blr	x8
   15cbc:	rbit	x8, x0
   15cc0:	clz	x8, x8
   15cc4:	lsr	x3, x0, x8
   15cc8:	mov	x0, x20
   15ccc:	mov	x1, x20
   15cd0:	mov	x2, x23
   15cd4:	add	x25, x25, x21
   15cd8:	bl	d490 <__gmpn_mul_1@plt>
   15cdc:	cmp	x0, #0x0
   15ce0:	str	x0, [x20, x23, lsl #3]
   15ce4:	cinc	x23, x23, ne  // ne = any
   15ce8:	subs	w22, w22, w21
   15cec:	b.ne	15ca4 <__gmpz_bin_uiui@@Base+0xbb0>  // b.any
   15cf0:	ldur	x24, [x29, #-32]
   15cf4:	add	x9, x20, x23, lsl #3
   15cf8:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15cfc:	ldur	x9, [x9, #-8]
   15d00:	ldr	x8, [x24]
   15d04:	add	x10, x24, x27, lsl #3
   15d08:	ldur	x10, [x10, #-8]
   15d0c:	ldr	x13, [x13, #3952]
   15d10:	ubfx	x12, x8, #1, #7
   15d14:	sub	x11, x23, x27
   15d18:	cmp	x9, x10
   15d1c:	ldrb	w12, [x13, x12]
   15d20:	mov	w10, #0x2                   	// #2
   15d24:	cinc	x23, x11, cs  // cs = hs, nlast
   15d28:	cmp	x27, x23
   15d2c:	msub	x9, x8, x12, x10
   15d30:	mul	x9, x9, x12
   15d34:	msub	x10, x9, x8, x10
   15d38:	mul	x9, x9, x10
   15d3c:	orr	x10, xzr, #0xfffffffffffffffe
   15d40:	madd	x8, x9, x8, x10
   15d44:	csel	x4, x27, x23, lt  // lt = tstop
   15d48:	mul	x5, x8, x9
   15d4c:	mov	x0, x20
   15d50:	mov	x1, x20
   15d54:	mov	x2, x23
   15d58:	mov	x3, x24
   15d5c:	bl	c510 <__gmpn_sbpi1_bdiv_q@plt>
   15d60:	ldr	x10, [x20]
   15d64:	cbz	x10, 15d84 <__gmpz_bin_uiui@@Base+0xc90>
   15d68:	ldur	x22, [x29, #-24]
   15d6c:	ldur	w27, [x29, #-12]
   15d70:	adrp	x21, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15d74:	mov	x8, x20
   15d78:	mov	x9, x23
   15d7c:	add	x21, x21, #0x9a0
   15d80:	b	15db0 <__gmpz_bin_uiui@@Base+0xcbc>
   15d84:	ldur	x22, [x29, #-24]
   15d88:	ldur	w27, [x29, #-12]
   15d8c:	adrp	x21, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   15d90:	mov	x9, x23
   15d94:	mov	x8, x20
   15d98:	add	x21, x21, #0x9a0
   15d9c:	subs	x9, x9, #0x1
   15da0:	str	xzr, [x8]
   15da4:	b.eq	15dcc <__gmpz_bin_uiui@@Base+0xcd8>  // b.none
   15da8:	ldr	x10, [x8, #8]!
   15dac:	cbz	x10, 15d9c <__gmpz_bin_uiui@@Base+0xca8>
   15db0:	neg	x10, x10
   15db4:	subs	x2, x9, #0x1
   15db8:	str	x10, [x8]
   15dbc:	b.eq	15dcc <__gmpz_bin_uiui@@Base+0xcd8>  // b.none
   15dc0:	add	x0, x8, #0x8
   15dc4:	mov	x1, x0
   15dc8:	bl	c290 <__gmpn_com@plt>
   15dcc:	cbz	w27, 15df8 <__gmpz_bin_uiui@@Base+0xd04>
   15dd0:	sub	w8, w19, #0x1
   15dd4:	ldr	x8, [x21, w8, uxtw #3]
   15dd8:	mov	x0, x26
   15ddc:	blr	x8
   15de0:	rbit	x9, x0
   15de4:	clz	x9, x9
   15de8:	add	x8, x26, w19, uxtw
   15dec:	lsr	x9, x0, x9
   15df0:	mov	x28, x26
   15df4:	b	15bf0 <__gmpz_bin_uiui@@Base+0xafc>
   15df8:	ldp	x9, x11, [x29, #-64]
   15dfc:	lsr	x8, x9, #1
   15e00:	and	x8, x8, #0x5555555555555555
   15e04:	lsr	x10, x11, #1
   15e08:	sub	x8, x9, x8
   15e0c:	lsr	x9, x22, #1
   15e10:	and	x10, x10, #0x5555555555555555
   15e14:	and	x9, x9, #0x5555555555555555
   15e18:	sub	x10, x11, x10
   15e1c:	lsr	x11, x8, #2
   15e20:	sub	x9, x22, x9
   15e24:	and	x8, x8, #0x3333333333333333
   15e28:	and	x11, x11, #0x3333333333333333
   15e2c:	add	x8, x11, x8
   15e30:	lsr	x11, x9, #2
   15e34:	and	x9, x9, #0x3333333333333333
   15e38:	and	x11, x11, #0x3333333333333333
   15e3c:	add	x9, x11, x9
   15e40:	lsr	x11, x10, #2
   15e44:	and	x10, x10, #0x3333333333333333
   15e48:	and	x11, x11, #0x3333333333333333
   15e4c:	add	x8, x8, x8, lsr #4
   15e50:	add	x10, x11, x10
   15e54:	add	x9, x9, x9, lsr #4
   15e58:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   15e5c:	add	x10, x10, x10, lsr #4
   15e60:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   15e64:	add	x8, x8, x8, lsr #8
   15e68:	and	x10, x10, #0xf0f0f0f0f0f0f0f
   15e6c:	add	x9, x9, x9, lsr #8
   15e70:	add	x8, x8, x8, lsr #16
   15e74:	add	x10, x10, x10, lsr #8
   15e78:	add	x9, x9, x9, lsr #16
   15e7c:	lsr	x11, x8, #32
   15e80:	add	x10, x10, x10, lsr #16
   15e84:	add	w8, w11, w8
   15e88:	lsr	x11, x9, #32
   15e8c:	add	w9, w11, w9
   15e90:	lsr	x11, x10, #32
   15e94:	and	w9, w9, #0xff
   15e98:	add	w10, w11, w10
   15e9c:	sub	w9, w9, w10, uxtb
   15ea0:	adds	w3, w9, w8, uxtb
   15ea4:	b.eq	15ec4 <__gmpz_bin_uiui@@Base+0xdd0>  // b.none
   15ea8:	mov	x0, x20
   15eac:	mov	x1, x20
   15eb0:	mov	x2, x23
   15eb4:	bl	c180 <__gmpn_lshift@plt>
   15eb8:	cmp	x0, #0x0
   15ebc:	str	x0, [x20, x23, lsl #3]
   15ec0:	cinc	x23, x23, ne  // ne = any
   15ec4:	ldur	x19, [x29, #-48]
   15ec8:	add	x8, x20, x23, lsl #3
   15ecc:	ldur	x8, [x8, #-8]
   15ed0:	ldrsw	x9, [x19]
   15ed4:	cmp	x8, #0x0
   15ed8:	cset	w8, eq  // eq = none
   15edc:	sub	x21, x23, x8
   15ee0:	cmp	x21, x9
   15ee4:	b.gt	15f34 <__gmpz_bin_uiui@@Base+0xe40>
   15ee8:	ldr	x0, [x19, #8]
   15eec:	mov	x1, x20
   15ef0:	mov	x2, x21
   15ef4:	str	w21, [x19, #4]
   15ef8:	bl	ca50 <__gmpn_copyi@plt>
   15efc:	ldur	x0, [x29, #-8]
   15f00:	cbnz	x0, 15f44 <__gmpz_bin_uiui@@Base+0xe50>
   15f04:	mov	sp, x29
   15f08:	ldp	x20, x19, [sp, #80]
   15f0c:	ldp	x22, x21, [sp, #64]
   15f10:	ldp	x24, x23, [sp, #48]
   15f14:	ldp	x26, x25, [sp, #32]
   15f18:	ldp	x28, x27, [sp, #16]
   15f1c:	ldp	x29, x30, [sp], #96
   15f20:	ret
   15f24:	sub	x0, x29, #0x8
   15f28:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15f2c:	mov	x20, x0
   15f30:	b	15b70 <__gmpz_bin_uiui@@Base+0xa7c>
   15f34:	mov	x0, x19
   15f38:	mov	x1, x21
   15f3c:	bl	c080 <__gmpz_realloc@plt>
   15f40:	b	15eec <__gmpz_bin_uiui@@Base+0xdf8>
   15f44:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   15f48:	b	15f04 <__gmpz_bin_uiui@@Base+0xe10>
   15f4c:	ret
   15f50:	add	x9, x0, #0x1
   15f54:	orr	x8, x0, #0x1
   15f58:	lsr	x9, x9, #1
   15f5c:	mul	x0, x9, x8
   15f60:	ret
   15f64:	add	x8, x0, #0x1
   15f68:	mul	x8, x8, x0
   15f6c:	lsr	x8, x8, #1
   15f70:	add	x9, x0, #0x2
   15f74:	mul	x0, x8, x9
   15f78:	ret
   15f7c:	add	x8, x0, #0x3
   15f80:	mul	x8, x8, x0
   15f84:	lsr	x8, x8, #1
   15f88:	add	x9, x8, #0x1
   15f8c:	mul	x0, x9, x8
   15f90:	ret
   15f94:	add	x8, x0, #0x3
   15f98:	mul	x8, x8, x0
   15f9c:	add	x9, x0, #0x4
   15fa0:	lsr	x8, x8, #1
   15fa4:	mul	x9, x8, x9
   15fa8:	add	x8, x8, #0x1
   15fac:	mul	x0, x9, x8
   15fb0:	ret
   15fb4:	add	x8, x0, #0x5
   15fb8:	mul	x8, x8, x0
   15fbc:	add	x9, x8, #0x5
   15fc0:	mul	x9, x9, x9
   15fc4:	lsr	x9, x9, #3
   15fc8:	lsr	x8, x8, #1
   15fcc:	mul	x0, x9, x8
   15fd0:	ret
   15fd4:	add	x8, x0, #0x5
   15fd8:	mul	x8, x8, x0
   15fdc:	add	x9, x0, #0x6
   15fe0:	add	x10, x8, #0x5
   15fe4:	mul	x8, x8, x9
   15fe8:	mul	x9, x10, x10
   15fec:	lsr	x9, x9, #3
   15ff0:	lsr	x8, x8, #1
   15ff4:	mul	x0, x9, x8
   15ff8:	ret
   15ffc:	add	x8, x0, #0x7
   16000:	mul	x8, x8, x0
   16004:	add	x9, x8, #0xa
   16008:	mul	x9, x9, x8
   1600c:	lsr	x9, x9, #3
   16010:	add	x8, x8, x9
   16014:	add	x8, x8, #0x9
   16018:	mul	x0, x8, x9
   1601c:	ret

0000000000016020 <__gmpz_cdiv_q@@Base>:
   16020:	stp	x29, x30, [sp, #-64]!
   16024:	str	x23, [sp, #16]
   16028:	stp	x22, x21, [sp, #32]
   1602c:	stp	x20, x19, [sp, #48]
   16030:	mov	x29, sp
   16034:	sub	sp, sp, #0x10
   16038:	ldrsw	x22, [x2, #4]
   1603c:	ldr	w23, [x1, #4]
   16040:	mov	x20, x2
   16044:	mov	x21, x1
   16048:	cmp	x22, #0x0
   1604c:	cneg	x8, x22, mi  // mi = first
   16050:	mov	x19, x0
   16054:	cmp	x8, #0xfe0
   16058:	lsl	x1, x8, #3
   1605c:	str	xzr, [x29, #24]
   16060:	stur	w8, [x29, #-16]
   16064:	b.hi	160d4 <__gmpz_cdiv_q@@Base+0xb4>  // b.pmore
   16068:	add	x9, x1, #0xf
   1606c:	mov	x8, sp
   16070:	and	x9, x9, #0xfffffffffffffff0
   16074:	sub	x0, x8, x9
   16078:	mov	sp, x0
   1607c:	stur	x0, [x29, #-8]
   16080:	sub	x1, x29, #0x10
   16084:	mov	x0, x19
   16088:	mov	x2, x21
   1608c:	mov	x3, x20
   16090:	bl	bff0 <__gmpz_tdiv_qr@plt>
   16094:	eor	w8, w22, w23
   16098:	tbnz	w8, #31, 160b4 <__gmpz_cdiv_q@@Base+0x94>
   1609c:	ldur	w8, [x29, #-12]
   160a0:	cbz	w8, 160b4 <__gmpz_cdiv_q@@Base+0x94>
   160a4:	mov	w2, #0x1                   	// #1
   160a8:	mov	x0, x19
   160ac:	mov	x1, x19
   160b0:	bl	c8b0 <__gmpz_add_ui@plt>
   160b4:	ldr	x0, [x29, #24]
   160b8:	cbnz	x0, 160e0 <__gmpz_cdiv_q@@Base+0xc0>
   160bc:	mov	sp, x29
   160c0:	ldp	x20, x19, [sp, #48]
   160c4:	ldp	x22, x21, [sp, #32]
   160c8:	ldr	x23, [sp, #16]
   160cc:	ldp	x29, x30, [sp], #64
   160d0:	ret
   160d4:	add	x0, x29, #0x18
   160d8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   160dc:	b	1607c <__gmpz_cdiv_q@@Base+0x5c>
   160e0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   160e4:	b	160bc <__gmpz_cdiv_q@@Base+0x9c>

00000000000160e8 <__gmpz_cdiv_q_ui@@Base>:
   160e8:	stp	x29, x30, [sp, #-64]!
   160ec:	stp	x24, x23, [sp, #16]
   160f0:	stp	x22, x21, [sp, #32]
   160f4:	stp	x20, x19, [sp, #48]
   160f8:	mov	x29, sp
   160fc:	cbz	x2, 161b8 <__gmpz_cdiv_q_ui@@Base+0xd0>
   16100:	ldrsw	x24, [x1, #4]
   16104:	mov	x23, x1
   16108:	mov	x19, x0
   1610c:	cbz	w24, 16184 <__gmpz_cdiv_q_ui@@Base+0x9c>
   16110:	ldrsw	x8, [x19]
   16114:	cmp	w24, #0x0
   16118:	cneg	x21, x24, lt  // lt = tstop
   1611c:	mov	x20, x2
   16120:	cmp	x21, x8
   16124:	b.gt	161a4 <__gmpz_cdiv_q_ui@@Base+0xbc>
   16128:	ldr	x22, [x19, #8]
   1612c:	ldr	x2, [x23, #8]
   16130:	mov	x0, x22
   16134:	mov	x1, xzr
   16138:	mov	x3, x21
   1613c:	mov	x4, x20
   16140:	bl	cd00 <__gmpn_divrem_1@plt>
   16144:	tbnz	w24, #31, 16164 <__gmpz_cdiv_q_ui@@Base+0x7c>
   16148:	cbz	x0, 16164 <__gmpz_cdiv_q_ui@@Base+0x7c>
   1614c:	mov	x8, x22
   16150:	ldr	x9, [x8]
   16154:	adds	x9, x9, #0x1
   16158:	str	x9, [x8], #8
   1615c:	b.cs	16150 <__gmpz_cdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   16160:	sub	x0, x20, x0
   16164:	add	x8, x22, x21, lsl #3
   16168:	ldur	x8, [x8, #-8]
   1616c:	cmp	x8, #0x0
   16170:	cset	w8, eq  // eq = none
   16174:	sub	w8, w21, w8
   16178:	cmp	w24, #0x0
   1617c:	cneg	w8, w8, lt  // lt = tstop
   16180:	b	1618c <__gmpz_cdiv_q_ui@@Base+0xa4>
   16184:	mov	w8, wzr
   16188:	mov	x0, xzr
   1618c:	str	w8, [x19, #4]
   16190:	ldp	x20, x19, [sp, #48]
   16194:	ldp	x22, x21, [sp, #32]
   16198:	ldp	x24, x23, [sp, #16]
   1619c:	ldp	x29, x30, [sp], #64
   161a0:	ret
   161a4:	mov	x0, x19
   161a8:	mov	x1, x21
   161ac:	bl	c080 <__gmpz_realloc@plt>
   161b0:	mov	x22, x0
   161b4:	b	1612c <__gmpz_cdiv_q_ui@@Base+0x44>
   161b8:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000161bc <__gmpz_cdiv_qr@@Base>:
   161bc:	stp	x29, x30, [sp, #-64]!
   161c0:	str	x23, [sp, #16]
   161c4:	stp	x22, x21, [sp, #32]
   161c8:	stp	x20, x19, [sp, #48]
   161cc:	mov	x29, sp
   161d0:	sub	sp, sp, #0x10
   161d4:	ldrsw	x23, [x3, #4]
   161d8:	mov	x20, x3
   161dc:	mov	x22, x2
   161e0:	mov	x19, x1
   161e4:	mov	x21, x0
   161e8:	cmp	x0, x3
   161ec:	str	xzr, [x29, #24]
   161f0:	b.eq	161fc <__gmpz_cdiv_qr@@Base+0x40>  // b.none
   161f4:	cmp	x19, x20
   161f8:	b.ne	1623c <__gmpz_cdiv_qr@@Base+0x80>  // b.any
   161fc:	cmp	x23, #0x0
   16200:	cneg	x8, x23, mi  // mi = first
   16204:	cmp	x8, #0xfe0
   16208:	lsl	x1, x8, #3
   1620c:	stur	w8, [x29, #-16]
   16210:	b.hi	162ac <__gmpz_cdiv_qr@@Base+0xf0>  // b.pmore
   16214:	add	x9, x1, #0xf
   16218:	mov	x8, sp
   1621c:	and	x9, x9, #0xfffffffffffffff0
   16220:	sub	x0, x8, x9
   16224:	mov	sp, x0
   16228:	stur	x0, [x29, #-8]
   1622c:	sub	x0, x29, #0x10
   16230:	mov	x1, x20
   16234:	bl	c420 <__gmpz_set@plt>
   16238:	sub	x20, x29, #0x10
   1623c:	ldr	w8, [x22, #4]
   16240:	mov	x0, x21
   16244:	mov	x1, x19
   16248:	mov	x2, x22
   1624c:	mov	x3, x20
   16250:	eor	w23, w8, w23
   16254:	bl	bff0 <__gmpz_tdiv_qr@plt>
   16258:	tbnz	w23, #31, 16284 <__gmpz_cdiv_qr@@Base+0xc8>
   1625c:	ldr	w8, [x19, #4]
   16260:	cbz	w8, 16284 <__gmpz_cdiv_qr@@Base+0xc8>
   16264:	mov	w2, #0x1                   	// #1
   16268:	mov	x0, x21
   1626c:	mov	x1, x21
   16270:	bl	c8b0 <__gmpz_add_ui@plt>
   16274:	mov	x0, x19
   16278:	mov	x1, x19
   1627c:	mov	x2, x20
   16280:	bl	c260 <__gmpz_sub@plt>
   16284:	ldr	x0, [x29, #24]
   16288:	cbnz	x0, 162a4 <__gmpz_cdiv_qr@@Base+0xe8>
   1628c:	mov	sp, x29
   16290:	ldp	x20, x19, [sp, #48]
   16294:	ldp	x22, x21, [sp, #32]
   16298:	ldr	x23, [sp, #16]
   1629c:	ldp	x29, x30, [sp], #64
   162a0:	ret
   162a4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   162a8:	b	1628c <__gmpz_cdiv_qr@@Base+0xd0>
   162ac:	add	x0, x29, #0x18
   162b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   162b4:	b	16228 <__gmpz_cdiv_qr@@Base+0x6c>

00000000000162b8 <__gmpz_cdiv_qr_ui@@Base>:
   162b8:	stp	x29, x30, [sp, #-80]!
   162bc:	str	x25, [sp, #16]
   162c0:	stp	x24, x23, [sp, #32]
   162c4:	stp	x22, x21, [sp, #48]
   162c8:	stp	x20, x19, [sp, #64]
   162cc:	mov	x29, sp
   162d0:	cbz	x3, 163dc <__gmpz_cdiv_qr_ui@@Base+0x124>
   162d4:	ldrsw	x25, [x2, #4]
   162d8:	mov	x22, x2
   162dc:	mov	x20, x1
   162e0:	mov	x19, x0
   162e4:	cbz	w25, 16360 <__gmpz_cdiv_qr_ui@@Base+0xa8>
   162e8:	ldrsw	x8, [x19]
   162ec:	cmp	w25, #0x0
   162f0:	cneg	x21, x25, lt  // lt = tstop
   162f4:	mov	x24, x3
   162f8:	cmp	x21, x8
   162fc:	b.gt	163b8 <__gmpz_cdiv_qr_ui@@Base+0x100>
   16300:	ldr	x23, [x19, #8]
   16304:	ldr	x2, [x22, #8]
   16308:	mov	x0, x23
   1630c:	mov	x1, xzr
   16310:	mov	x3, x21
   16314:	mov	x4, x24
   16318:	bl	cd00 <__gmpn_divrem_1@plt>
   1631c:	mov	x22, x0
   16320:	cbz	x0, 16374 <__gmpz_cdiv_qr_ui@@Base+0xbc>
   16324:	tbnz	w25, #31, 16340 <__gmpz_cdiv_qr_ui@@Base+0x88>
   16328:	mov	x8, x23
   1632c:	ldr	x9, [x8]
   16330:	adds	x9, x9, #0x1
   16334:	str	x9, [x8], #8
   16338:	b.cs	1632c <__gmpz_cdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   1633c:	sub	x22, x24, x22
   16340:	ldr	w8, [x20]
   16344:	cmp	w8, #0x0
   16348:	b.le	163cc <__gmpz_cdiv_qr_ui@@Base+0x114>
   1634c:	ldr	x0, [x20, #8]
   16350:	cmp	x22, #0x0
   16354:	csetm	w8, ne  // ne = any
   16358:	str	x22, [x0]
   1635c:	b	16378 <__gmpz_cdiv_qr_ui@@Base+0xc0>
   16360:	mov	w8, wzr
   16364:	mov	x22, xzr
   16368:	str	wzr, [x19, #4]
   1636c:	mov	x19, x20
   16370:	b	16398 <__gmpz_cdiv_qr_ui@@Base+0xe0>
   16374:	mov	w8, wzr
   16378:	str	w8, [x20, #4]
   1637c:	add	x8, x23, x21, lsl #3
   16380:	ldur	x8, [x8, #-8]
   16384:	cmp	x8, #0x0
   16388:	cset	w8, eq  // eq = none
   1638c:	sub	w8, w21, w8
   16390:	cmp	w25, #0x0
   16394:	cneg	w8, w8, lt  // lt = tstop
   16398:	str	w8, [x19, #4]
   1639c:	mov	x0, x22
   163a0:	ldp	x20, x19, [sp, #64]
   163a4:	ldp	x22, x21, [sp, #48]
   163a8:	ldp	x24, x23, [sp, #32]
   163ac:	ldr	x25, [sp, #16]
   163b0:	ldp	x29, x30, [sp], #80
   163b4:	ret
   163b8:	mov	x0, x19
   163bc:	mov	x1, x21
   163c0:	bl	c080 <__gmpz_realloc@plt>
   163c4:	mov	x23, x0
   163c8:	b	16304 <__gmpz_cdiv_qr_ui@@Base+0x4c>
   163cc:	mov	w1, #0x1                   	// #1
   163d0:	mov	x0, x20
   163d4:	bl	c080 <__gmpz_realloc@plt>
   163d8:	b	16350 <__gmpz_cdiv_qr_ui@@Base+0x98>
   163dc:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000163e0 <__gmpz_cdiv_r@@Base>:
   163e0:	stp	x29, x30, [sp, #-48]!
   163e4:	stp	x22, x21, [sp, #16]
   163e8:	stp	x20, x19, [sp, #32]
   163ec:	mov	x29, sp
   163f0:	sub	sp, sp, #0x20
   163f4:	ldrsw	x22, [x2, #4]
   163f8:	mov	x19, x2
   163fc:	mov	x21, x1
   16400:	mov	x20, x0
   16404:	cmp	x0, x2
   16408:	stur	xzr, [x29, #-24]
   1640c:	b.ne	16450 <__gmpz_cdiv_r@@Base+0x70>  // b.any
   16410:	cmp	x22, #0x0
   16414:	cneg	x8, x22, mi  // mi = first
   16418:	cmp	x8, #0xfe0
   1641c:	lsl	x1, x8, #3
   16420:	stur	w8, [x29, #-16]
   16424:	b.hi	164a8 <__gmpz_cdiv_r@@Base+0xc8>  // b.pmore
   16428:	add	x9, x1, #0xf
   1642c:	mov	x8, sp
   16430:	and	x9, x9, #0xfffffffffffffff0
   16434:	sub	x0, x8, x9
   16438:	mov	sp, x0
   1643c:	stur	x0, [x29, #-8]
   16440:	sub	x0, x29, #0x10
   16444:	mov	x1, x19
   16448:	bl	c420 <__gmpz_set@plt>
   1644c:	sub	x19, x29, #0x10
   16450:	mov	x0, x20
   16454:	mov	x1, x21
   16458:	mov	x2, x19
   1645c:	bl	ca80 <__gmpz_tdiv_r@plt>
   16460:	ldr	w8, [x21, #4]
   16464:	eor	w8, w8, w22
   16468:	tbnz	w8, #31, 16484 <__gmpz_cdiv_r@@Base+0xa4>
   1646c:	ldr	w8, [x20, #4]
   16470:	cbz	w8, 16484 <__gmpz_cdiv_r@@Base+0xa4>
   16474:	mov	x0, x20
   16478:	mov	x1, x20
   1647c:	mov	x2, x19
   16480:	bl	c260 <__gmpz_sub@plt>
   16484:	ldur	x0, [x29, #-24]
   16488:	cbnz	x0, 164a0 <__gmpz_cdiv_r@@Base+0xc0>
   1648c:	mov	sp, x29
   16490:	ldp	x20, x19, [sp, #32]
   16494:	ldp	x22, x21, [sp, #16]
   16498:	ldp	x29, x30, [sp], #48
   1649c:	ret
   164a0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   164a4:	b	1648c <__gmpz_cdiv_r@@Base+0xac>
   164a8:	sub	x0, x29, #0x18
   164ac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   164b0:	b	1643c <__gmpz_cdiv_r@@Base+0x5c>

00000000000164b4 <__gmpz_cdiv_r_ui@@Base>:
   164b4:	stp	x29, x30, [sp, #-48]!
   164b8:	str	x21, [sp, #16]
   164bc:	stp	x20, x19, [sp, #32]
   164c0:	mov	x29, sp
   164c4:	cbz	x2, 16544 <__gmpz_cdiv_r_ui@@Base+0x90>
   164c8:	ldrsw	x21, [x1, #4]
   164cc:	mov	x19, x0
   164d0:	cbz	w21, 16514 <__gmpz_cdiv_r_ui@@Base+0x60>
   164d4:	ldr	x0, [x1, #8]
   164d8:	cmp	w21, #0x0
   164dc:	cneg	x1, x21, lt  // lt = tstop
   164e0:	mov	x20, x2
   164e4:	bl	c3e0 <__gmpn_mod_1@plt>
   164e8:	cbz	x0, 16514 <__gmpz_cdiv_r_ui@@Base+0x60>
   164ec:	ldr	w8, [x19]
   164f0:	sub	x9, x20, x0
   164f4:	cmp	w21, #0x0
   164f8:	csel	x20, x9, x0, ge  // ge = tcont
   164fc:	cmp	w8, #0x0
   16500:	b.le	16534 <__gmpz_cdiv_r_ui@@Base+0x80>
   16504:	ldr	x0, [x19, #8]
   16508:	mov	w8, #0xffffffff            	// #-1
   1650c:	str	x20, [x0]
   16510:	b	1651c <__gmpz_cdiv_r_ui@@Base+0x68>
   16514:	mov	w8, wzr
   16518:	mov	x20, xzr
   1651c:	str	w8, [x19, #4]
   16520:	mov	x0, x20
   16524:	ldp	x20, x19, [sp, #32]
   16528:	ldr	x21, [sp, #16]
   1652c:	ldp	x29, x30, [sp], #48
   16530:	ret
   16534:	mov	w1, #0x1                   	// #1
   16538:	mov	x0, x19
   1653c:	bl	c080 <__gmpz_realloc@plt>
   16540:	b	16508 <__gmpz_cdiv_r_ui@@Base+0x54>
   16544:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000016548 <__gmpz_cdiv_ui@@Base>:
   16548:	stp	x29, x30, [sp, #-32]!
   1654c:	stp	x20, x19, [sp, #16]
   16550:	mov	x29, sp
   16554:	cbz	x1, 165a8 <__gmpz_cdiv_ui@@Base+0x60>
   16558:	ldrsw	x20, [x0, #4]
   1655c:	cbz	w20, 16598 <__gmpz_cdiv_ui@@Base+0x50>
   16560:	ldr	x0, [x0, #8]
   16564:	mov	x19, x1
   16568:	cmp	w20, #0x0
   1656c:	cneg	x1, x20, lt  // lt = tstop
   16570:	mov	x2, x19
   16574:	bl	c3e0 <__gmpn_mod_1@plt>
   16578:	cmp	x0, #0x0
   1657c:	mov	w9, #0xffffffff            	// #-1
   16580:	sub	x8, x19, x0
   16584:	ccmp	w20, w9, #0x4, ne  // ne = any
   16588:	csel	x0, x8, x0, gt
   1658c:	ldp	x20, x19, [sp, #16]
   16590:	ldp	x29, x30, [sp], #32
   16594:	ret
   16598:	mov	x0, xzr
   1659c:	ldp	x20, x19, [sp, #16]
   165a0:	ldp	x29, x30, [sp], #32
   165a4:	ret
   165a8:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000165ac <__gmpz_cdiv_q_2exp@@Base>:
   165ac:	mov	w3, #0x1                   	// #1
   165b0:	b	165b4 <__gmpz_cdiv_q_2exp@@Base+0x8>
   165b4:	stp	x29, x30, [sp, #-80]!
   165b8:	stp	x26, x25, [sp, #16]
   165bc:	stp	x24, x23, [sp, #32]
   165c0:	stp	x22, x21, [sp, #48]
   165c4:	stp	x20, x19, [sp, #64]
   165c8:	ldrsw	x25, [x1, #4]
   165cc:	ldrsw	x8, [x0]
   165d0:	lsr	x26, x2, #6
   165d4:	mov	x19, x0
   165d8:	cmp	x25, #0x0
   165dc:	cneg	x9, x25, mi  // mi = first
   165e0:	sub	x20, x9, x26
   165e4:	cmp	x20, #0x0
   165e8:	mov	w23, w3
   165ec:	mov	x29, sp
   165f0:	b.le	1667c <__gmpz_cdiv_q_2exp@@Base+0xd0>
   165f4:	mov	x22, x2
   165f8:	mov	x24, x1
   165fc:	cmp	x20, x8
   16600:	b.ge	1672c <__gmpz_cdiv_q_2exp@@Base+0x180>  // b.tcont
   16604:	ldr	x21, [x19, #8]
   16608:	ldr	x9, [x24, #8]
   1660c:	eor	w8, w25, w23
   16610:	mov	x23, xzr
   16614:	tbnz	w8, #31, 16634 <__gmpz_cdiv_q_2exp@@Base+0x88>
   16618:	cbz	x26, 16634 <__gmpz_cdiv_q_2exp@@Base+0x88>
   1661c:	mov	x10, xzr
   16620:	ldr	x23, [x9, x10, lsl #3]
   16624:	add	x10, x10, #0x1
   16628:	cmp	x10, x26
   1662c:	b.cs	16634 <__gmpz_cdiv_q_2exp@@Base+0x88>  // b.hs, b.nlast
   16630:	cbz	x23, 16620 <__gmpz_cdiv_q_2exp@@Base+0x74>
   16634:	ands	x3, x22, #0x3f
   16638:	add	x1, x9, x26, lsl #3
   1663c:	b.eq	166a4 <__gmpz_cdiv_q_2exp@@Base+0xf8>  // b.none
   16640:	mov	w9, #0xffffffff            	// #-1
   16644:	eor	w8, w9, w8, asr #31
   16648:	mov	x0, x21
   1664c:	mov	x2, x20
   16650:	sxtw	x22, w8
   16654:	bl	c1a0 <__gmpn_rshift@plt>
   16658:	add	x8, x21, x20, lsl #3
   1665c:	ldur	x8, [x8, #-8]
   16660:	and	x9, x0, x22
   16664:	orr	x23, x9, x23
   16668:	cmp	x8, #0x0
   1666c:	cset	w8, eq  // eq = none
   16670:	sub	x20, x20, x8
   16674:	cbnz	x23, 166b4 <__gmpz_cdiv_q_2exp@@Base+0x108>
   16678:	b	16708 <__gmpz_cdiv_q_2exp@@Base+0x15c>
   1667c:	cmp	w8, #0x0
   16680:	b.le	16750 <__gmpz_cdiv_q_2exp@@Base+0x1a4>
   16684:	ldr	x0, [x19, #8]
   16688:	eor	w9, w25, w23
   1668c:	cmp	w9, #0x0
   16690:	mov	w8, #0x1                   	// #1
   16694:	ccmp	w25, #0x0, #0x4, ge  // ge = tcont
   16698:	str	x8, [x0]
   1669c:	csel	w8, wzr, w23, eq  // eq = none
   166a0:	b	16710 <__gmpz_cdiv_q_2exp@@Base+0x164>
   166a4:	mov	x0, x21
   166a8:	mov	x2, x20
   166ac:	bl	ca50 <__gmpn_copyi@plt>
   166b0:	cbz	x23, 16708 <__gmpz_cdiv_q_2exp@@Base+0x15c>
   166b4:	cbz	x20, 16700 <__gmpz_cdiv_q_2exp@@Base+0x154>
   166b8:	ldr	x8, [x21]
   166bc:	adds	x8, x8, #0x1
   166c0:	str	x8, [x21]
   166c4:	b.cc	166f0 <__gmpz_cdiv_q_2exp@@Base+0x144>  // b.lo, b.ul, b.last
   166c8:	mov	w8, #0x1                   	// #1
   166cc:	mov	w9, #0x1                   	// #1
   166d0:	cmp	x9, x20
   166d4:	b.ge	166f4 <__gmpz_cdiv_q_2exp@@Base+0x148>  // b.tcont
   166d8:	lsl	x10, x9, #3
   166dc:	ldr	x11, [x21, x10]
   166e0:	add	x9, x9, #0x1
   166e4:	adds	x11, x11, #0x1
   166e8:	str	x11, [x21, x10]
   166ec:	b.cs	166d0 <__gmpz_cdiv_q_2exp@@Base+0x124>  // b.hs, b.nlast
   166f0:	mov	x8, xzr
   166f4:	str	x8, [x21, x20, lsl #3]
   166f8:	add	x20, x8, x20
   166fc:	b	16708 <__gmpz_cdiv_q_2exp@@Base+0x15c>
   16700:	mov	w20, #0x1                   	// #1
   16704:	str	x20, [x21]
   16708:	cmp	w25, #0x0
   1670c:	cneg	w8, w20, lt  // lt = tstop
   16710:	str	w8, [x19, #4]
   16714:	ldp	x20, x19, [sp, #64]
   16718:	ldp	x22, x21, [sp, #48]
   1671c:	ldp	x24, x23, [sp, #32]
   16720:	ldp	x26, x25, [sp, #16]
   16724:	ldp	x29, x30, [sp], #80
   16728:	ret
   1672c:	add	x1, x20, #0x1
   16730:	mov	x0, x19
   16734:	bl	c080 <__gmpz_realloc@plt>
   16738:	mov	x21, x0
   1673c:	ldr	x9, [x24, #8]
   16740:	eor	w8, w25, w23
   16744:	mov	x23, xzr
   16748:	tbz	w8, #31, 16618 <__gmpz_cdiv_q_2exp@@Base+0x6c>
   1674c:	b	16634 <__gmpz_cdiv_q_2exp@@Base+0x88>
   16750:	mov	w1, #0x1                   	// #1
   16754:	mov	x0, x19
   16758:	bl	c080 <__gmpz_realloc@plt>
   1675c:	b	16688 <__gmpz_cdiv_q_2exp@@Base+0xdc>

0000000000016760 <__gmpz_fdiv_q_2exp@@Base>:
   16760:	mov	w3, #0xffffffff            	// #-1
   16764:	b	165b4 <__gmpz_cdiv_q_2exp@@Base+0x8>

0000000000016768 <__gmpz_cdiv_r_2exp@@Base>:
   16768:	mov	w3, #0x1                   	// #1
   1676c:	b	16770 <__gmpz_cdiv_r_2exp@@Base+0x8>
   16770:	stp	x29, x30, [sp, #-80]!
   16774:	stp	x26, x25, [sp, #16]
   16778:	stp	x24, x23, [sp, #32]
   1677c:	stp	x22, x21, [sp, #48]
   16780:	stp	x20, x19, [sp, #64]
   16784:	ldr	w26, [x1, #4]
   16788:	mov	x19, x0
   1678c:	mov	x29, sp
   16790:	cbz	w26, 16910 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   16794:	mov	x21, x1
   16798:	ldr	x1, [x1, #8]
   1679c:	sxtw	x22, w26
   167a0:	cmp	x22, #0x0
   167a4:	lsr	x23, x2, #6
   167a8:	and	x24, x2, #0x3f
   167ac:	eor	w8, w26, w3
   167b0:	cneg	x25, x22, mi  // mi = first
   167b4:	tbnz	w8, #31, 16828 <__gmpz_cdiv_r_2exp@@Base+0xc0>
   167b8:	cmp	x25, x23
   167bc:	b.le	167f4 <__gmpz_cdiv_r_2exp@@Base+0x8c>
   167c0:	cbz	x23, 167e0 <__gmpz_cdiv_r_2exp@@Base+0x78>
   167c4:	mov	x8, x1
   167c8:	mov	x9, x23
   167cc:	ldr	x10, [x8]
   167d0:	cbnz	x10, 167f4 <__gmpz_cdiv_r_2exp@@Base+0x8c>
   167d4:	subs	x9, x9, #0x1
   167d8:	add	x8, x8, #0x8
   167dc:	b.ne	167cc <__gmpz_cdiv_r_2exp@@Base+0x64>  // b.any
   167e0:	ldr	x8, [x1, x23, lsl #3]
   167e4:	mov	x9, #0xffffffffffffffff    	// #-1
   167e8:	lsl	x9, x9, x24
   167ec:	bics	xzr, x8, x9
   167f0:	b.eq	1690c <__gmpz_cdiv_r_2exp@@Base+0x1a4>  // b.none
   167f4:	ldrsw	x8, [x19]
   167f8:	add	x20, x23, #0x1
   167fc:	cmp	x23, x8
   16800:	b.ge	16938 <__gmpz_cdiv_r_2exp@@Base+0x1d0>  // b.tcont
   16804:	ldr	x21, [x19, #8]
   16808:	ldr	x10, [x1]
   1680c:	cmp	x25, x23
   16810:	neg	x22, x22
   16814:	csel	x25, x20, x25, gt
   16818:	cbz	x10, 16864 <__gmpz_cdiv_r_2exp@@Base+0xfc>
   1681c:	mov	x8, x21
   16820:	mov	x9, x25
   16824:	b	16884 <__gmpz_cdiv_r_2exp@@Base+0x11c>
   16828:	cmp	x19, x21
   1682c:	b.eq	1692c <__gmpz_cdiv_r_2exp@@Base+0x1c4>  // b.none
   16830:	ldrsw	x8, [x19]
   16834:	cmp	x25, x23
   16838:	csinc	x20, x25, x23, le
   1683c:	cmp	x20, x8
   16840:	b.gt	16950 <__gmpz_cdiv_r_2exp@@Base+0x1e8>
   16844:	ldr	x21, [x19, #8]
   16848:	mov	x0, x21
   1684c:	mov	x2, x20
   16850:	bl	ca50 <__gmpn_copyi@plt>
   16854:	cmp	x25, x23
   16858:	mov	x1, x21
   1685c:	b.gt	168c0 <__gmpz_cdiv_r_2exp@@Base+0x158>
   16860:	b	16910 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   16864:	mov	x9, x25
   16868:	mov	x8, x21
   1686c:	subs	x9, x9, #0x1
   16870:	str	xzr, [x8]
   16874:	b.eq	168a0 <__gmpz_cdiv_r_2exp@@Base+0x138>  // b.none
   16878:	ldr	x10, [x1, #8]!
   1687c:	add	x8, x8, #0x8
   16880:	cbz	x10, 1686c <__gmpz_cdiv_r_2exp@@Base+0x104>
   16884:	neg	x10, x10
   16888:	subs	x2, x9, #0x1
   1688c:	str	x10, [x8]
   16890:	b.eq	168a0 <__gmpz_cdiv_r_2exp@@Base+0x138>  // b.none
   16894:	add	x0, x8, #0x8
   16898:	add	x1, x1, #0x8
   1689c:	bl	c290 <__gmpn_com@plt>
   168a0:	cmp	x25, x23
   168a4:	b.hi	168bc <__gmpz_cdiv_r_2exp@@Base+0x154>  // b.pmore
   168a8:	sub	x8, x20, x25
   168ac:	add	x0, x21, x25, lsl #3
   168b0:	lsl	x2, x8, #3
   168b4:	mov	w1, #0xff                  	// #255
   168b8:	bl	c5f0 <memset@plt>
   168bc:	mov	x1, x21
   168c0:	lsl	x8, x23, #3
   168c4:	ldr	x9, [x1, x8]
   168c8:	mov	x10, #0xffffffffffffffff    	// #-1
   168cc:	lsl	x10, x10, x24
   168d0:	bics	x9, x9, x10
   168d4:	str	x9, [x1, x8]
   168d8:	b.eq	168e4 <__gmpz_cdiv_r_2exp@@Base+0x17c>  // b.none
   168dc:	mov	x8, x23
   168e0:	b	168fc <__gmpz_cdiv_r_2exp@@Base+0x194>
   168e4:	sub	x9, x1, #0x8
   168e8:	subs	x8, x23, #0x1
   168ec:	b.lt	1690c <__gmpz_cdiv_r_2exp@@Base+0x1a4>  // b.tstop
   168f0:	ldr	x10, [x9, x23, lsl #3]
   168f4:	mov	x23, x8
   168f8:	cbz	x10, 168e8 <__gmpz_cdiv_r_2exp@@Base+0x180>
   168fc:	mvn	w9, w8
   16900:	cmp	x22, #0x0
   16904:	csinc	w26, w9, w8, lt  // lt = tstop
   16908:	b	16910 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   1690c:	mov	w26, wzr
   16910:	str	w26, [x19, #4]
   16914:	ldp	x20, x19, [sp, #64]
   16918:	ldp	x22, x21, [sp, #48]
   1691c:	ldp	x24, x23, [sp, #32]
   16920:	ldp	x26, x25, [sp, #16]
   16924:	ldp	x29, x30, [sp], #80
   16928:	ret
   1692c:	cmp	x25, x23
   16930:	b.gt	168c0 <__gmpz_cdiv_r_2exp@@Base+0x158>
   16934:	b	16914 <__gmpz_cdiv_r_2exp@@Base+0x1ac>
   16938:	mov	x0, x19
   1693c:	mov	x1, x20
   16940:	bl	c080 <__gmpz_realloc@plt>
   16944:	ldr	x1, [x21, #8]
   16948:	mov	x21, x0
   1694c:	b	16808 <__gmpz_cdiv_r_2exp@@Base+0xa0>
   16950:	mov	x0, x19
   16954:	mov	x21, x1
   16958:	mov	x1, x20
   1695c:	bl	c080 <__gmpz_realloc@plt>
   16960:	mov	x1, x21
   16964:	mov	x21, x0
   16968:	b	16848 <__gmpz_cdiv_r_2exp@@Base+0xe0>

000000000001696c <__gmpz_fdiv_r_2exp@@Base>:
   1696c:	mov	w3, #0xffffffff            	// #-1
   16970:	b	16770 <__gmpz_cdiv_r_2exp@@Base+0x8>

0000000000016974 <__gmpz_clear@@Base>:
   16974:	ldrsw	x8, [x0]
   16978:	cbz	w8, 16994 <__gmpz_clear@@Base+0x20>
   1697c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   16980:	ldr	x9, [x9, #4016]
   16984:	ldr	x0, [x0, #8]
   16988:	lsl	x1, x8, #3
   1698c:	ldr	x2, [x9]
   16990:	br	x2
   16994:	ret

0000000000016998 <__gmpz_clears@@Base>:
   16998:	sub	sp, sp, #0x100
   1699c:	stp	x29, x30, [sp, #224]
   169a0:	add	x29, sp, #0xe0
   169a4:	mov	x8, #0xffffffffffffffc8    	// #-56
   169a8:	mov	x9, sp
   169ac:	sub	x10, x29, #0x58
   169b0:	movk	x8, #0xff80, lsl #32
   169b4:	add	x11, x29, #0x20
   169b8:	add	x9, x9, #0x80
   169bc:	add	x10, x10, #0x38
   169c0:	str	x19, [sp, #240]
   169c4:	stp	x1, x2, [x29, #-88]
   169c8:	stp	x3, x4, [x29, #-72]
   169cc:	stp	x5, x6, [x29, #-56]
   169d0:	stur	x7, [x29, #-40]
   169d4:	stp	q0, q1, [sp]
   169d8:	stp	q2, q3, [sp, #32]
   169dc:	stp	q4, q5, [sp, #64]
   169e0:	stp	q6, q7, [sp, #96]
   169e4:	stp	x9, x8, [x29, #-16]
   169e8:	stp	x11, x10, [x29, #-32]
   169ec:	adrp	x19, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   169f0:	ldr	x19, [x19, #4016]
   169f4:	b	16a0c <__gmpz_clears@@Base+0x74>
   169f8:	ldur	x8, [x29, #-32]
   169fc:	add	x9, x8, #0x8
   16a00:	stur	x9, [x29, #-32]
   16a04:	ldr	x0, [x8]
   16a08:	cbz	x0, 16a4c <__gmpz_clears@@Base+0xb4>
   16a0c:	ldrsw	x8, [x0]
   16a10:	cbz	w8, 16a24 <__gmpz_clears@@Base+0x8c>
   16a14:	ldr	x9, [x19]
   16a18:	ldr	x0, [x0, #8]
   16a1c:	lsl	x1, x8, #3
   16a20:	blr	x9
   16a24:	ldursw	x8, [x29, #-8]
   16a28:	tbz	w8, #31, 169f8 <__gmpz_clears@@Base+0x60>
   16a2c:	add	w9, w8, #0x8
   16a30:	cmn	w8, #0x8
   16a34:	stur	w9, [x29, #-8]
   16a38:	b.gt	169f8 <__gmpz_clears@@Base+0x60>
   16a3c:	ldur	x9, [x29, #-24]
   16a40:	add	x8, x9, x8
   16a44:	ldr	x0, [x8]
   16a48:	cbnz	x0, 16a0c <__gmpz_clears@@Base+0x74>
   16a4c:	ldr	x19, [sp, #240]
   16a50:	ldp	x29, x30, [sp, #224]
   16a54:	add	sp, sp, #0x100
   16a58:	ret

0000000000016a5c <__gmpz_clrbit@@Base>:
   16a5c:	sub	sp, sp, #0x40
   16a60:	stp	x29, x30, [sp, #16]
   16a64:	stp	x20, x19, [sp, #48]
   16a68:	ldrsw	x8, [x0, #4]
   16a6c:	ldr	x19, [x0, #8]
   16a70:	mov	w9, #0x1                   	// #1
   16a74:	str	x21, [sp, #32]
   16a78:	lsr	x20, x1, #6
   16a7c:	lsl	x21, x9, x1
   16a80:	add	x29, sp, #0x10
   16a84:	tbnz	w8, #31, 16ad4 <__gmpz_clrbit@@Base+0x78>
   16a88:	cmp	x20, x8
   16a8c:	b.ge	16bac <__gmpz_clrbit@@Base+0x150>  // b.tcont
   16a90:	lsl	x9, x20, #3
   16a94:	ldr	x10, [x19, x9]
   16a98:	bics	xzr, x10, x21
   16a9c:	bic	x11, x10, x21
   16aa0:	cinc	x10, x20, eq  // eq = none
   16aa4:	cmp	x10, x8
   16aa8:	str	x11, [x19, x9]
   16aac:	b.ne	16bac <__gmpz_clrbit@@Base+0x150>  // b.any
   16ab0:	sub	x8, x19, #0x8
   16ab4:	subs	x9, x20, #0x1
   16ab8:	b.lt	16bf8 <__gmpz_clrbit@@Base+0x19c>  // b.tstop
   16abc:	ldr	x10, [x8, x20, lsl #3]
   16ac0:	mov	x20, x9
   16ac4:	cbz	x10, 16ab4 <__gmpz_clrbit@@Base+0x58>
   16ac8:	add	x8, x9, #0x1
   16acc:	str	w8, [x0, #4]
   16ad0:	b	16bac <__gmpz_clrbit@@Base+0x150>
   16ad4:	neg	x9, x8
   16ad8:	cmp	x20, x9
   16adc:	b.ge	16b10 <__gmpz_clrbit@@Base+0xb4>  // b.tcont
   16ae0:	mov	x10, xzr
   16ae4:	ldr	x11, [x19, x10, lsl #3]
   16ae8:	add	x10, x10, #0x1
   16aec:	cbz	x11, 16ae4 <__gmpz_clrbit@@Base+0x88>
   16af0:	sub	x11, x10, #0x1
   16af4:	cmp	x20, x11
   16af8:	b.ls	16b44 <__gmpz_clrbit@@Base+0xe8>  // b.plast
   16afc:	lsl	x8, x20, #3
   16b00:	ldr	x9, [x19, x8]
   16b04:	orr	x9, x9, x21
   16b08:	str	x9, [x19, x8]
   16b0c:	b	16bac <__gmpz_clrbit@@Base+0x150>
   16b10:	ldrsw	x10, [x0]
   16b14:	cmp	x20, x10
   16b18:	b.ge	16bc0 <__gmpz_clrbit@@Base+0x164>  // b.tcont
   16b1c:	mvn	w10, w20
   16b20:	adds	x8, x20, x8
   16b24:	str	w10, [x0, #4]
   16b28:	b.eq	16b3c <__gmpz_clrbit@@Base+0xe0>  // b.none
   16b2c:	add	x0, x19, x9, lsl #3
   16b30:	lsl	x2, x8, #3
   16b34:	mov	w1, wzr
   16b38:	bl	c5f0 <memset@plt>
   16b3c:	str	x21, [x19, x20, lsl #3]
   16b40:	b	16bac <__gmpz_clrbit@@Base+0x150>
   16b44:	add	x11, x20, #0x1
   16b48:	cmp	x11, x10
   16b4c:	b.ne	16bac <__gmpz_clrbit@@Base+0x150>  // b.any
   16b50:	lsl	x10, x20, #3
   16b54:	ldr	x11, [x19, x10]
   16b58:	sub	x11, x11, #0x1
   16b5c:	orr	x11, x11, x21
   16b60:	adds	x11, x11, #0x1
   16b64:	str	x11, [x19, x10]
   16b68:	b.cc	16bac <__gmpz_clrbit@@Base+0x150>  // b.lo, b.ul, b.last
   16b6c:	ldrsw	x10, [x0]
   16b70:	mov	w11, #0x1                   	// #1
   16b74:	sub	x1, x11, x8
   16b78:	cmp	x1, x10
   16b7c:	b.gt	16c04 <__gmpz_clrbit@@Base+0x1a8>
   16b80:	add	x10, x19, x20, lsl #3
   16b84:	add	x10, x10, #0x8
   16b88:	str	xzr, [x19, x9, lsl #3]
   16b8c:	ldr	x11, [x10]
   16b90:	adds	x11, x11, #0x1
   16b94:	str	x11, [x10], #8
   16b98:	b.cs	16b8c <__gmpz_clrbit@@Base+0x130>  // b.hs, b.nlast
   16b9c:	lsl	x9, x9, #3
   16ba0:	ldr	w9, [x19, x9]
   16ba4:	sub	w8, w8, w9
   16ba8:	str	w8, [x0, #4]
   16bac:	ldp	x20, x19, [sp, #48]
   16bb0:	ldr	x21, [sp, #32]
   16bb4:	ldp	x29, x30, [sp, #16]
   16bb8:	add	sp, sp, #0x40
   16bbc:	ret
   16bc0:	add	x1, x20, #0x1
   16bc4:	str	x0, [x29, #24]
   16bc8:	str	x8, [sp, #8]
   16bcc:	mov	x19, x9
   16bd0:	bl	c080 <__gmpz_realloc@plt>
   16bd4:	mov	x9, x19
   16bd8:	ldr	x8, [sp, #8]
   16bdc:	mov	x19, x0
   16be0:	ldr	x0, [x29, #24]
   16be4:	mvn	w10, w20
   16be8:	adds	x8, x20, x8
   16bec:	str	w10, [x0, #4]
   16bf0:	b.ne	16b2c <__gmpz_clrbit@@Base+0xd0>  // b.any
   16bf4:	b	16b3c <__gmpz_clrbit@@Base+0xe0>
   16bf8:	mov	x8, xzr
   16bfc:	str	w8, [x0, #4]
   16c00:	b	16bac <__gmpz_clrbit@@Base+0x150>
   16c04:	str	x0, [x29, #24]
   16c08:	mov	x19, x8
   16c0c:	mov	x21, x9
   16c10:	bl	c080 <__gmpz_realloc@plt>
   16c14:	mov	x8, x19
   16c18:	mov	x19, x0
   16c1c:	ldr	x0, [x29, #24]
   16c20:	mov	x9, x21
   16c24:	b	16b80 <__gmpz_clrbit@@Base+0x124>

0000000000016c28 <__gmpz_cmp@@Base>:
   16c28:	ldrsw	x9, [x0, #4]
   16c2c:	ldrsw	x10, [x1, #4]
   16c30:	mov	x8, x0
   16c34:	subs	x0, x9, x10
   16c38:	b.eq	16c40 <__gmpz_cmp@@Base+0x18>  // b.none
   16c3c:	ret
   16c40:	ldr	x8, [x8, #8]
   16c44:	ldr	x10, [x1, #8]
   16c48:	cmp	w9, #0x0
   16c4c:	cneg	x11, x9, lt  // lt = tstop
   16c50:	sub	x8, x8, #0x8
   16c54:	sub	x10, x10, #0x8
   16c58:	subs	x12, x11, #0x1
   16c5c:	b.lt	16c8c <__gmpz_cmp@@Base+0x64>  // b.tstop
   16c60:	lsl	x11, x11, #3
   16c64:	ldr	x13, [x8, x11]
   16c68:	ldr	x11, [x10, x11]
   16c6c:	cmp	x13, x11
   16c70:	mov	x11, x12
   16c74:	b.eq	16c58 <__gmpz_cmp@@Base+0x30>  // b.none
   16c78:	mov	w8, #0x1                   	// #1
   16c7c:	cneg	w8, w8, ls  // ls = plast
   16c80:	cmp	w9, #0x0
   16c84:	cneg	w0, w8, lt  // lt = tstop
   16c88:	ret
   16c8c:	mov	w8, wzr
   16c90:	cmp	w9, #0x0
   16c94:	cneg	w0, w8, lt  // lt = tstop
   16c98:	ret

0000000000016c9c <__gmpz_cmp_d@@Base>:
   16c9c:	sub	sp, sp, #0x40
   16ca0:	fmov	x8, d0
   16ca4:	mvn	x9, x8
   16ca8:	tst	x9, #0x7ff0000000000000
   16cac:	stp	x29, x30, [sp, #16]
   16cb0:	str	x21, [sp, #32]
   16cb4:	stp	x20, x19, [sp, #48]
   16cb8:	add	x29, sp, #0x10
   16cbc:	b.eq	16ddc <__gmpz_cmp_d@@Base+0x140>  // b.none
   16cc0:	mov	x19, x0
   16cc4:	ldr	w0, [x0, #4]
   16cc8:	fcmp	d0, #0.0
   16ccc:	b.ne	16ce4 <__gmpz_cmp_d@@Base+0x48>  // b.any
   16cd0:	ldp	x20, x19, [sp, #48]
   16cd4:	ldr	x21, [sp, #32]
   16cd8:	ldp	x29, x30, [sp, #16]
   16cdc:	add	sp, sp, #0x40
   16ce0:	ret
   16ce4:	cbz	w0, 16de4 <__gmpz_cmp_d@@Base+0x148>
   16ce8:	sxtw	x21, w0
   16cec:	tbnz	w0, #31, 16d10 <__gmpz_cmp_d@@Base+0x74>
   16cf0:	mov	w20, #0x1                   	// #1
   16cf4:	fcmp	d0, #0.0
   16cf8:	mov	w0, #0x1                   	// #1
   16cfc:	b.mi	16cd0 <__gmpz_cmp_d@@Base+0x34>  // b.first
   16d00:	fmov	d1, #1.000000000000000000e+00
   16d04:	fcmp	d0, d1
   16d08:	b.mi	16d30 <__gmpz_cmp_d@@Base+0x94>  // b.first
   16d0c:	b	16d38 <__gmpz_cmp_d@@Base+0x9c>
   16d10:	fcmp	d0, #0.0
   16d14:	b.ge	16d7c <__gmpz_cmp_d@@Base+0xe0>  // b.tcont
   16d18:	fneg	d0, d0
   16d1c:	neg	x21, x21
   16d20:	mov	w20, #0xffffffff            	// #-1
   16d24:	fmov	d1, #1.000000000000000000e+00
   16d28:	fcmp	d0, d1
   16d2c:	b.pl	16d38 <__gmpz_cmp_d@@Base+0x9c>  // b.nfrst
   16d30:	mov	w0, w20
   16d34:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16d38:	mov	x0, sp
   16d3c:	bl	d280 <__gmp_extract_double@plt>
   16d40:	sxtw	x8, w0
   16d44:	cmp	x21, x8
   16d48:	b.ne	16d84 <__gmpz_cmp_d@@Base+0xe8>  // b.any
   16d4c:	ldr	x8, [x19, #8]
   16d50:	ldr	x10, [sp, #8]
   16d54:	add	x9, x8, x21, lsl #3
   16d58:	ldur	x9, [x9, #-8]
   16d5c:	cmp	x9, x10
   16d60:	b.ne	16dcc <__gmpz_cmp_d@@Base+0x130>  // b.any
   16d64:	cmp	x21, #0x1
   16d68:	b.ne	16d8c <__gmpz_cmp_d@@Base+0xf0>  // b.any
   16d6c:	ldr	x8, [sp]
   16d70:	cmp	x8, #0x0
   16d74:	csneg	w0, wzr, w20, eq  // eq = none
   16d78:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16d7c:	mov	w0, #0xffffffff            	// #-1
   16d80:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16d84:	cneg	w0, w20, lt  // lt = tstop
   16d88:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16d8c:	add	x9, x8, x21, lsl #3
   16d90:	ldur	x9, [x9, #-16]
   16d94:	ldr	x10, [sp]
   16d98:	cmp	x9, x10
   16d9c:	b.ne	16dcc <__gmpz_cmp_d@@Base+0x130>  // b.any
   16da0:	cmp	x21, #0x3
   16da4:	b.lt	16dd4 <__gmpz_cmp_d@@Base+0x138>  // b.tstop
   16da8:	sub	x8, x8, #0x18
   16dac:	ldr	x9, [x8, x21, lsl #3]
   16db0:	cbnz	x9, 16d30 <__gmpz_cmp_d@@Base+0x94>
   16db4:	sub	x9, x21, #0x3
   16db8:	mov	w0, wzr
   16dbc:	sub	x21, x21, #0x1
   16dc0:	cmp	x9, #0x1
   16dc4:	b.ge	16dac <__gmpz_cmp_d@@Base+0x110>  // b.tcont
   16dc8:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16dcc:	cneg	w0, w20, cc  // cc = lo, ul, last
   16dd0:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16dd4:	mov	w0, wzr
   16dd8:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16ddc:	tst	x8, #0xfffffffffffff
   16de0:	b.ne	16df4 <__gmpz_cmp_d@@Base+0x158>  // b.any
   16de4:	fcmp	d0, #0.0
   16de8:	mov	w8, #0xffffffff            	// #-1
   16dec:	csinc	w0, w8, wzr, pl  // pl = nfrst
   16df0:	b	16cd0 <__gmpz_cmp_d@@Base+0x34>
   16df4:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000016df8 <__gmpz_cmp_si@@Base>:
   16df8:	ldr	w8, [x0, #4]
   16dfc:	cmp	x1, #0x0
   16e00:	asr	x9, x1, #63
   16e04:	cinc	x9, x9, gt
   16e08:	cbz	w8, 16e38 <__gmpz_cmp_si@@Base+0x40>
   16e0c:	sxtw	x10, w8
   16e10:	cmp	x9, x10
   16e14:	b.ne	16e38 <__gmpz_cmp_si@@Base+0x40>  // b.any
   16e18:	ldr	x9, [x0, #8]
   16e1c:	cmp	x1, #0x0
   16e20:	cneg	x10, x1, mi  // mi = first
   16e24:	ldr	x9, [x9]
   16e28:	cmp	x9, x10
   16e2c:	b.ne	16e40 <__gmpz_cmp_si@@Base+0x48>  // b.any
   16e30:	mov	w0, wzr
   16e34:	ret
   16e38:	sub	w0, w8, w9
   16e3c:	ret
   16e40:	cneg	w0, w8, ls  // ls = plast
   16e44:	ret

0000000000016e48 <__gmpz_cmp_ui@@Base>:
   16e48:	ldr	w8, [x0, #4]
   16e4c:	cmp	w8, #0x1
   16e50:	b.eq	16e64 <__gmpz_cmp_ui@@Base+0x1c>  // b.none
   16e54:	cbnz	w8, 16e7c <__gmpz_cmp_ui@@Base+0x34>
   16e58:	cmp	x1, #0x0
   16e5c:	csetm	w0, ne  // ne = any
   16e60:	ret
   16e64:	ldr	x8, [x0, #8]
   16e68:	ldr	x8, [x8]
   16e6c:	cmp	x8, x1
   16e70:	b.ls	16e8c <__gmpz_cmp_ui@@Base+0x44>  // b.plast
   16e74:	mov	w0, #0x1                   	// #1
   16e78:	ret
   16e7c:	cmp	w8, #0x1
   16e80:	mov	w8, #0xffffffff            	// #-1
   16e84:	cneg	w0, w8, ge  // ge = tcont
   16e88:	ret
   16e8c:	csetm	w0, cc  // cc = lo, ul, last
   16e90:	ret

0000000000016e94 <__gmpz_cmpabs@@Base>:
   16e94:	ldr	w9, [x0, #4]
   16e98:	ldr	w10, [x1, #4]
   16e9c:	mov	x8, x0
   16ea0:	cmp	w9, #0x0
   16ea4:	cneg	w9, w9, mi  // mi = first
   16ea8:	cmp	w10, #0x0
   16eac:	cneg	w10, w10, mi  // mi = first
   16eb0:	subs	x0, x9, x10
   16eb4:	b.eq	16ebc <__gmpz_cmpabs@@Base+0x28>  // b.none
   16eb8:	ret
   16ebc:	ldr	x8, [x8, #8]
   16ec0:	ldr	x10, [x1, #8]
   16ec4:	sub	x8, x8, #0x8
   16ec8:	sub	x10, x10, #0x8
   16ecc:	subs	x11, x9, #0x1
   16ed0:	b.lt	16ef8 <__gmpz_cmpabs@@Base+0x64>  // b.tstop
   16ed4:	lsl	x9, x9, #3
   16ed8:	ldr	x12, [x8, x9]
   16edc:	ldr	x9, [x10, x9]
   16ee0:	cmp	x12, x9
   16ee4:	mov	x9, x11
   16ee8:	b.eq	16ecc <__gmpz_cmpabs@@Base+0x38>  // b.none
   16eec:	mov	w8, #0x1                   	// #1
   16ef0:	cneg	w0, w8, ls  // ls = plast
   16ef4:	ret
   16ef8:	mov	w0, wzr
   16efc:	ret

0000000000016f00 <__gmpz_cmpabs_d@@Base>:
   16f00:	sub	sp, sp, #0x30
   16f04:	fmov	x8, d0
   16f08:	mvn	x9, x8
   16f0c:	tst	x9, #0x7ff0000000000000
   16f10:	stp	x29, x30, [sp, #16]
   16f14:	stp	x20, x19, [sp, #32]
   16f18:	add	x29, sp, #0x10
   16f1c:	b.eq	17054 <__gmpz_cmpabs_d@@Base+0x154>  // b.none
   16f20:	ldrsw	x8, [x0, #4]
   16f24:	mov	x19, x0
   16f28:	fcmp	d0, #0.0
   16f2c:	b.ne	16f48 <__gmpz_cmpabs_d@@Base+0x48>  // b.any
   16f30:	cmp	w8, #0x0
   16f34:	cset	w0, ne  // ne = any
   16f38:	ldp	x20, x19, [sp, #32]
   16f3c:	ldp	x29, x30, [sp, #16]
   16f40:	add	sp, sp, #0x30
   16f44:	ret
   16f48:	cbz	w8, 1705c <__gmpz_cmpabs_d@@Base+0x15c>
   16f4c:	cmp	x8, #0x0
   16f50:	fneg	d1, d0
   16f54:	cneg	x20, x8, mi  // mi = first
   16f58:	fcmp	d0, #0.0
   16f5c:	fcsel	d0, d0, d1, ge  // ge = tcont
   16f60:	fmov	d1, #1.000000000000000000e+00
   16f64:	fcmp	d0, d1
   16f68:	b.pl	16f80 <__gmpz_cmpabs_d@@Base+0x80>  // b.nfrst
   16f6c:	mov	w0, #0x1                   	// #1
   16f70:	ldp	x20, x19, [sp, #32]
   16f74:	ldp	x29, x30, [sp, #16]
   16f78:	add	sp, sp, #0x30
   16f7c:	ret
   16f80:	mov	x0, sp
   16f84:	bl	d280 <__gmp_extract_double@plt>
   16f88:	sxtw	x8, w0
   16f8c:	cmp	x20, x8
   16f90:	b.ne	16fd0 <__gmpz_cmpabs_d@@Base+0xd0>  // b.any
   16f94:	ldr	x8, [x19, #8]
   16f98:	ldr	x10, [sp, #8]
   16f9c:	add	x9, x8, x20, lsl #3
   16fa0:	ldur	x9, [x9, #-8]
   16fa4:	cmp	x9, x10
   16fa8:	b.ne	17028 <__gmpz_cmpabs_d@@Base+0x128>  // b.any
   16fac:	cmp	x20, #0x1
   16fb0:	b.ne	16fe8 <__gmpz_cmpabs_d@@Base+0xe8>  // b.any
   16fb4:	ldr	x8, [sp]
   16fb8:	cmp	x8, #0x0
   16fbc:	csetm	w0, ne  // ne = any
   16fc0:	ldp	x20, x19, [sp, #32]
   16fc4:	ldp	x29, x30, [sp, #16]
   16fc8:	add	sp, sp, #0x30
   16fcc:	ret
   16fd0:	mov	w8, #0xffffffff            	// #-1
   16fd4:	cneg	w0, w8, ge  // ge = tcont
   16fd8:	ldp	x20, x19, [sp, #32]
   16fdc:	ldp	x29, x30, [sp, #16]
   16fe0:	add	sp, sp, #0x30
   16fe4:	ret
   16fe8:	add	x9, x8, x20, lsl #3
   16fec:	ldur	x9, [x9, #-16]
   16ff0:	ldr	x10, [sp]
   16ff4:	cmp	x9, x10
   16ff8:	b.ne	17028 <__gmpz_cmpabs_d@@Base+0x128>  // b.any
   16ffc:	cmp	x20, #0x3
   17000:	b.lt	17040 <__gmpz_cmpabs_d@@Base+0x140>  // b.tstop
   17004:	sub	x8, x8, #0x18
   17008:	ldr	x9, [x8, x20, lsl #3]
   1700c:	cbnz	x9, 16f6c <__gmpz_cmpabs_d@@Base+0x6c>
   17010:	sub	x9, x20, #0x3
   17014:	mov	w0, wzr
   17018:	sub	x20, x20, #0x1
   1701c:	cmp	x9, #0x1
   17020:	b.ge	17008 <__gmpz_cmpabs_d@@Base+0x108>  // b.tcont
   17024:	b	16f38 <__gmpz_cmpabs_d@@Base+0x38>
   17028:	mov	w8, #0xffffffff            	// #-1
   1702c:	cneg	w0, w8, cs  // cs = hs, nlast
   17030:	ldp	x20, x19, [sp, #32]
   17034:	ldp	x29, x30, [sp, #16]
   17038:	add	sp, sp, #0x30
   1703c:	ret
   17040:	mov	w0, wzr
   17044:	ldp	x20, x19, [sp, #32]
   17048:	ldp	x29, x30, [sp, #16]
   1704c:	add	sp, sp, #0x30
   17050:	ret
   17054:	tst	x8, #0xfffffffffffff
   17058:	b.ne	17070 <__gmpz_cmpabs_d@@Base+0x170>  // b.any
   1705c:	mov	w0, #0xffffffff            	// #-1
   17060:	ldp	x20, x19, [sp, #32]
   17064:	ldp	x29, x30, [sp, #16]
   17068:	add	sp, sp, #0x30
   1706c:	ret
   17070:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000017074 <__gmpz_cmpabs_ui@@Base>:
   17074:	ldrsw	x8, [x0, #4]
   17078:	cbz	w8, 170a4 <__gmpz_cmpabs_ui@@Base+0x30>
   1707c:	cmp	x8, #0x0
   17080:	cneg	x8, x8, mi  // mi = first
   17084:	cmp	x8, #0x1
   17088:	b.ne	1709c <__gmpz_cmpabs_ui@@Base+0x28>  // b.any
   1708c:	ldr	x8, [x0, #8]
   17090:	ldr	x8, [x8]
   17094:	cmp	x8, x1
   17098:	b.ls	170b0 <__gmpz_cmpabs_ui@@Base+0x3c>  // b.plast
   1709c:	mov	w0, #0x1                   	// #1
   170a0:	ret
   170a4:	cmp	x1, #0x0
   170a8:	csetm	w0, ne  // ne = any
   170ac:	ret
   170b0:	csetm	w0, cc  // cc = lo, ul, last
   170b4:	ret

00000000000170b8 <__gmpz_com@@Base>:
   170b8:	stp	x29, x30, [sp, #-48]!
   170bc:	stp	x22, x21, [sp, #16]
   170c0:	stp	x20, x19, [sp, #32]
   170c4:	ldrsw	x22, [x1, #4]
   170c8:	mov	x21, x1
   170cc:	mov	x19, x0
   170d0:	mov	x29, sp
   170d4:	tbnz	w22, #31, 171ec <__gmpz_com@@Base+0x134>
   170d8:	ldr	w8, [x19]
   170dc:	cbz	w22, 17478 <__gmpz_com@@Base+0x3c0>
   170e0:	cmp	w22, w8
   170e4:	b.ge	17494 <__gmpz_com@@Base+0x3dc>  // b.tcont
   170e8:	ldr	x0, [x19, #8]
   170ec:	ldr	x8, [x21, #8]
   170f0:	ldr	x9, [x8]
   170f4:	adds	x9, x9, #0x1
   170f8:	str	x9, [x0]
   170fc:	b.cc	1727c <__gmpz_com@@Base+0x1c4>  // b.lo, b.ul, b.last
   17100:	mov	x11, xzr
   17104:	sub	x10, x22, #0x1
   17108:	mov	w12, #0x1                   	// #1
   1710c:	mov	w9, #0x1                   	// #1
   17110:	cmp	x9, x22
   17114:	b.ge	172e8 <__gmpz_com@@Base+0x230>  // b.tcont
   17118:	add	x13, x8, x11
   1711c:	ldr	x13, [x13, #8]
   17120:	add	x14, x0, x11
   17124:	add	x9, x9, #0x1
   17128:	add	x11, x11, #0x8
   1712c:	adds	x13, x13, #0x1
   17130:	sub	x10, x10, #0x1
   17134:	str	x13, [x14, #8]
   17138:	b.cs	17110 <__gmpz_com@@Base+0x58>  // b.hs, b.nlast
   1713c:	cmp	x8, x0
   17140:	mov	x12, xzr
   17144:	b.eq	172e8 <__gmpz_com@@Base+0x230>  // b.none
   17148:	cmp	x9, x22
   1714c:	b.ge	172e8 <__gmpz_com@@Base+0x230>  // b.tcont
   17150:	sub	x12, x22, x9
   17154:	cmp	x12, #0x4
   17158:	b.cc	171c8 <__gmpz_com@@Base+0x110>  // b.lo, b.ul, b.last
   1715c:	add	x14, x0, x11
   17160:	lsl	x13, x22, #3
   17164:	add	x14, x14, #0x8
   17168:	add	x15, x8, x13
   1716c:	cmp	x14, x15
   17170:	b.cs	17188 <__gmpz_com@@Base+0xd0>  // b.hs, b.nlast
   17174:	add	x14, x8, x11
   17178:	add	x13, x0, x13
   1717c:	add	x14, x14, #0x8
   17180:	cmp	x14, x13
   17184:	b.cc	171c8 <__gmpz_com@@Base+0x110>  // b.lo, b.ul, b.last
   17188:	add	x13, x0, x11
   1718c:	add	x14, x8, x11
   17190:	and	x11, x12, #0xfffffffffffffffc
   17194:	and	x15, x10, #0xfffffffffffffffc
   17198:	add	x10, x13, #0x18
   1719c:	add	x13, x14, #0x18
   171a0:	add	x9, x15, x9
   171a4:	mov	x14, x11
   171a8:	ldp	q0, q1, [x13, #-16]
   171ac:	add	x13, x13, #0x20
   171b0:	subs	x14, x14, #0x4
   171b4:	stp	q0, q1, [x10, #-16]
   171b8:	add	x10, x10, #0x20
   171bc:	b.ne	171a8 <__gmpz_com@@Base+0xf0>  // b.any
   171c0:	cmp	x12, x11
   171c4:	b.eq	172e4 <__gmpz_com@@Base+0x22c>  // b.none
   171c8:	lsl	x11, x9, #3
   171cc:	sub	x10, x22, x9
   171d0:	add	x9, x0, x11
   171d4:	add	x8, x8, x11
   171d8:	ldr	x11, [x8], #8
   171dc:	subs	x10, x10, #0x1
   171e0:	str	x11, [x9], #8
   171e4:	b.ne	171d8 <__gmpz_com@@Base+0x120>  // b.any
   171e8:	b	172e4 <__gmpz_com@@Base+0x22c>
   171ec:	ldrsw	x8, [x19]
   171f0:	neg	x20, x22
   171f4:	cmp	x20, x8
   171f8:	b.gt	174a4 <__gmpz_com@@Base+0x3ec>
   171fc:	ldr	x0, [x19, #8]
   17200:	ldr	x8, [x21, #8]
   17204:	ldr	x9, [x8]
   17208:	sub	x10, x9, #0x1
   1720c:	str	x10, [x0]
   17210:	cbz	x9, 172f8 <__gmpz_com@@Base+0x240>
   17214:	cmn	w22, #0x2
   17218:	b.gt	17450 <__gmpz_com@@Base+0x398>
   1721c:	cmp	x8, x0
   17220:	b.eq	17450 <__gmpz_com@@Base+0x398>  // b.none
   17224:	cmn	w22, #0x5
   17228:	b.hi	17250 <__gmpz_com@@Base+0x198>  // b.pmore
   1722c:	lsl	x9, x20, #3
   17230:	add	x10, x0, #0x8
   17234:	add	x11, x8, x9
   17238:	cmp	x10, x11
   1723c:	b.cs	17418 <__gmpz_com@@Base+0x360>  // b.hs, b.nlast
   17240:	add	x9, x0, x9
   17244:	add	x10, x8, #0x8
   17248:	cmp	x10, x9
   1724c:	b.cs	17418 <__gmpz_com@@Base+0x360>  // b.hs, b.nlast
   17250:	mov	w9, #0x1                   	// #1
   17254:	add	x10, x9, x22
   17258:	lsl	x11, x9, #3
   1725c:	neg	x9, x10
   17260:	add	x10, x0, x11
   17264:	add	x8, x8, x11
   17268:	ldr	x11, [x8], #8
   1726c:	subs	x9, x9, #0x1
   17270:	str	x11, [x10], #8
   17274:	b.ne	17268 <__gmpz_com@@Base+0x1b0>  // b.any
   17278:	b	17450 <__gmpz_com@@Base+0x398>
   1727c:	cmp	w22, #0x2
   17280:	mov	x12, xzr
   17284:	b.lt	172e8 <__gmpz_com@@Base+0x230>  // b.tstop
   17288:	cmp	x8, x0
   1728c:	b.eq	172e8 <__gmpz_com@@Base+0x230>  // b.none
   17290:	sub	x9, x22, #0x1
   17294:	cmp	x9, #0x4
   17298:	b.cc	172c0 <__gmpz_com@@Base+0x208>  // b.lo, b.ul, b.last
   1729c:	lsl	x10, x22, #3
   172a0:	add	x11, x0, #0x8
   172a4:	add	x12, x8, x10
   172a8:	cmp	x11, x12
   172ac:	b.cs	173e0 <__gmpz_com@@Base+0x328>  // b.hs, b.nlast
   172b0:	add	x10, x0, x10
   172b4:	add	x11, x8, #0x8
   172b8:	cmp	x11, x10
   172bc:	b.cs	173e0 <__gmpz_com@@Base+0x328>  // b.hs, b.nlast
   172c0:	mov	w10, #0x1                   	// #1
   172c4:	lsl	x11, x10, #3
   172c8:	sub	x9, x22, x10
   172cc:	add	x10, x0, x11
   172d0:	add	x8, x8, x11
   172d4:	ldr	x11, [x8], #8
   172d8:	subs	x9, x9, #0x1
   172dc:	str	x11, [x10], #8
   172e0:	b.ne	172d4 <__gmpz_com@@Base+0x21c>  // b.any
   172e4:	mov	x12, xzr
   172e8:	add	w8, w22, w12
   172ec:	str	x12, [x0, x22, lsl #3]
   172f0:	neg	w8, w8
   172f4:	b	17464 <__gmpz_com@@Base+0x3ac>
   172f8:	mov	x11, xzr
   172fc:	mvn	x10, x22
   17300:	mov	w9, #0x1                   	// #1
   17304:	cmp	x9, x20
   17308:	b.ge	17450 <__gmpz_com@@Base+0x398>  // b.tcont
   1730c:	add	x12, x8, x11
   17310:	ldr	x12, [x12, #8]
   17314:	add	x13, x0, x11
   17318:	add	x9, x9, #0x1
   1731c:	add	x11, x11, #0x8
   17320:	sub	x14, x12, #0x1
   17324:	sub	x10, x10, #0x1
   17328:	str	x14, [x13, #8]
   1732c:	cbz	x12, 17304 <__gmpz_com@@Base+0x24c>
   17330:	cmp	x8, x0
   17334:	b.eq	17450 <__gmpz_com@@Base+0x398>  // b.none
   17338:	cmp	x9, x20
   1733c:	b.ge	17450 <__gmpz_com@@Base+0x398>  // b.tcont
   17340:	sub	x12, x20, x9
   17344:	cmp	x12, #0x4
   17348:	b.cc	173b8 <__gmpz_com@@Base+0x300>  // b.lo, b.ul, b.last
   1734c:	add	x14, x0, x11
   17350:	lsl	x13, x20, #3
   17354:	add	x14, x14, #0x8
   17358:	add	x15, x8, x13
   1735c:	cmp	x14, x15
   17360:	b.cs	17378 <__gmpz_com@@Base+0x2c0>  // b.hs, b.nlast
   17364:	add	x14, x8, x11
   17368:	add	x13, x0, x13
   1736c:	add	x14, x14, #0x8
   17370:	cmp	x14, x13
   17374:	b.cc	173b8 <__gmpz_com@@Base+0x300>  // b.lo, b.ul, b.last
   17378:	add	x13, x0, x11
   1737c:	add	x14, x8, x11
   17380:	and	x11, x12, #0xfffffffffffffffc
   17384:	and	x15, x10, #0xfffffffffffffffc
   17388:	add	x10, x13, #0x18
   1738c:	add	x13, x14, #0x18
   17390:	add	x9, x15, x9
   17394:	mov	x14, x11
   17398:	ldp	q0, q1, [x13, #-16]
   1739c:	add	x13, x13, #0x20
   173a0:	subs	x14, x14, #0x4
   173a4:	stp	q0, q1, [x10, #-16]
   173a8:	add	x10, x10, #0x20
   173ac:	b.ne	17398 <__gmpz_com@@Base+0x2e0>  // b.any
   173b0:	cmp	x12, x11
   173b4:	b.eq	17450 <__gmpz_com@@Base+0x398>  // b.none
   173b8:	add	x10, x9, x22
   173bc:	lsl	x11, x9, #3
   173c0:	neg	x9, x10
   173c4:	add	x10, x0, x11
   173c8:	add	x8, x8, x11
   173cc:	ldr	x11, [x8], #8
   173d0:	subs	x9, x9, #0x1
   173d4:	str	x11, [x10], #8
   173d8:	b.ne	173cc <__gmpz_com@@Base+0x314>  // b.any
   173dc:	b	17450 <__gmpz_com@@Base+0x398>
   173e0:	and	x11, x9, #0xfffffffffffffffc
   173e4:	add	x12, x8, #0x18
   173e8:	orr	x10, x11, #0x1
   173ec:	add	x13, x0, #0x18
   173f0:	mov	x14, x11
   173f4:	ldp	q0, q1, [x12, #-16]
   173f8:	add	x12, x12, #0x20
   173fc:	subs	x14, x14, #0x4
   17400:	stp	q0, q1, [x13, #-16]
   17404:	add	x13, x13, #0x20
   17408:	b.ne	173f4 <__gmpz_com@@Base+0x33c>  // b.any
   1740c:	cmp	x9, x11
   17410:	b.eq	172e4 <__gmpz_com@@Base+0x22c>  // b.none
   17414:	b	172c4 <__gmpz_com@@Base+0x20c>
   17418:	mvn	x10, x22
   1741c:	and	x11, x10, #0xfffffffffffffffc
   17420:	add	x12, x8, #0x18
   17424:	orr	x9, x11, #0x1
   17428:	add	x13, x0, #0x18
   1742c:	mov	x14, x11
   17430:	ldp	q0, q1, [x12, #-16]
   17434:	add	x12, x12, #0x20
   17438:	subs	x14, x14, #0x4
   1743c:	stp	q0, q1, [x13, #-16]
   17440:	add	x13, x13, #0x20
   17444:	b.ne	17430 <__gmpz_com@@Base+0x378>  // b.any
   17448:	cmp	x11, x10
   1744c:	b.ne	17254 <__gmpz_com@@Base+0x19c>  // b.any
   17450:	mvn	x8, x22
   17454:	ldr	x8, [x0, x8, lsl #3]
   17458:	cmp	x8, #0x0
   1745c:	csetm	w8, eq  // eq = none
   17460:	sub	w8, w8, w22
   17464:	str	w8, [x19, #4]
   17468:	ldp	x20, x19, [sp, #32]
   1746c:	ldp	x22, x21, [sp, #16]
   17470:	ldp	x29, x30, [sp], #48
   17474:	ret
   17478:	cmp	w8, #0x0
   1747c:	b.le	174b4 <__gmpz_com@@Base+0x3fc>
   17480:	ldr	x0, [x19, #8]
   17484:	mov	w8, #0x1                   	// #1
   17488:	str	x8, [x0]
   1748c:	mov	w8, #0xffffffff            	// #-1
   17490:	b	17464 <__gmpz_com@@Base+0x3ac>
   17494:	add	x1, x22, #0x1
   17498:	mov	x0, x19
   1749c:	bl	c080 <__gmpz_realloc@plt>
   174a0:	b	170ec <__gmpz_com@@Base+0x34>
   174a4:	mov	x0, x19
   174a8:	mov	x1, x20
   174ac:	bl	c080 <__gmpz_realloc@plt>
   174b0:	b	17200 <__gmpz_com@@Base+0x148>
   174b4:	mov	w1, #0x1                   	// #1
   174b8:	mov	x0, x19
   174bc:	bl	c080 <__gmpz_realloc@plt>
   174c0:	b	17484 <__gmpz_com@@Base+0x3cc>

00000000000174c4 <__gmpz_combit@@Base>:
   174c4:	stp	x29, x30, [sp, #-64]!
   174c8:	stp	x22, x21, [sp, #32]
   174cc:	stp	x20, x19, [sp, #48]
   174d0:	ldrsw	x8, [x0, #4]
   174d4:	ldr	x20, [x0, #8]
   174d8:	lsr	x22, x1, #6
   174dc:	mov	w9, #0x1                   	// #1
   174e0:	add	x21, x22, #0x1
   174e4:	str	x23, [sp, #16]
   174e8:	cmp	x21, x8
   174ec:	lsl	x23, x9, x1
   174f0:	mov	x29, sp
   174f4:	b.ge	1750c <__gmpz_combit@@Base+0x48>  // b.tcont
   174f8:	lsl	x8, x22, #3
   174fc:	ldr	x9, [x20, x8]
   17500:	eor	x9, x9, x23
   17504:	str	x9, [x20, x8]
   17508:	b	175d4 <__gmpz_combit@@Base+0x110>
   1750c:	neg	x9, x8
   17510:	mov	x19, x0
   17514:	cmp	x22, x9
   17518:	b.ge	17548 <__gmpz_combit@@Base+0x84>  // b.tcont
   1751c:	cbz	x22, 17538 <__gmpz_combit@@Base+0x74>
   17520:	lsl	x10, x22, #3
   17524:	sub	x11, x20, #0x8
   17528:	ldr	x12, [x11, x10]
   1752c:	cbnz	x12, 17548 <__gmpz_combit@@Base+0x84>
   17530:	subs	x10, x10, #0x8
   17534:	b.ne	17528 <__gmpz_combit@@Base+0x64>  // b.any
   17538:	ldr	x10, [x20, x22, lsl #3]
   1753c:	sub	x11, x23, #0x1
   17540:	tst	x10, x11
   17544:	b.eq	175e8 <__gmpz_combit@@Base+0x124>  // b.none
   17548:	cmp	x8, #0x0
   1754c:	cneg	x9, x8, mi  // mi = first
   17550:	cmp	x22, x9
   17554:	b.ge	17598 <__gmpz_combit@@Base+0xd4>  // b.tcont
   17558:	lsl	x10, x22, #3
   1755c:	ldr	x11, [x20, x10]
   17560:	eor	x11, x11, x23
   17564:	cmp	x11, #0x0
   17568:	cinc	x12, x22, eq  // eq = none
   1756c:	cmp	x12, x9
   17570:	str	x11, [x20, x10]
   17574:	b.ne	175d4 <__gmpz_combit@@Base+0x110>  // b.any
   17578:	sub	x9, x20, #0x8
   1757c:	subs	x10, x22, #0x1
   17580:	b.lt	176a8 <__gmpz_combit@@Base+0x1e4>  // b.tstop
   17584:	ldr	x11, [x9, x22, lsl #3]
   17588:	mov	x22, x10
   1758c:	cbz	x11, 1757c <__gmpz_combit@@Base+0xb8>
   17590:	add	x9, x10, #0x1
   17594:	b	176ac <__gmpz_combit@@Base+0x1e8>
   17598:	ldrsw	x8, [x19]
   1759c:	cmp	x22, x8
   175a0:	b.ge	17684 <__gmpz_combit@@Base+0x1c0>  // b.tcont
   175a4:	subs	x8, x22, x9
   175a8:	b.eq	175bc <__gmpz_combit@@Base+0xf8>  // b.none
   175ac:	add	x0, x20, x9, lsl #3
   175b0:	lsl	x2, x8, #3
   175b4:	mov	w1, wzr
   175b8:	bl	c5f0 <memset@plt>
   175bc:	str	x23, [x20, x22, lsl #3]
   175c0:	ldr	w8, [x19, #4]
   175c4:	mvn	w9, w22
   175c8:	cmp	w8, #0x0
   175cc:	csel	x8, x21, x9, ge  // ge = tcont
   175d0:	str	w8, [x19, #4]
   175d4:	ldp	x20, x19, [sp, #48]
   175d8:	ldp	x22, x21, [sp, #32]
   175dc:	ldr	x23, [sp, #16]
   175e0:	ldp	x29, x30, [sp], #64
   175e4:	ret
   175e8:	tst	x10, x23
   175ec:	b.eq	17648 <__gmpz_combit@@Base+0x184>  // b.none
   175f0:	ldrsw	x10, [x19]
   175f4:	mov	w11, #0x1                   	// #1
   175f8:	sub	x1, x11, x8
   175fc:	cmp	x1, x10
   17600:	b.gt	176bc <__gmpz_combit@@Base+0x1f8>
   17604:	str	xzr, [x20, x9, lsl #3]
   17608:	lsl	x10, x22, #3
   1760c:	ldr	x11, [x20, x10]
   17610:	adds	x11, x11, x23
   17614:	str	x11, [x20, x10]
   17618:	b.cc	17634 <__gmpz_combit@@Base+0x170>  // b.lo, b.ul, b.last
   1761c:	add	x10, x20, x22, lsl #3
   17620:	add	x10, x10, #0x8
   17624:	ldr	x11, [x10]
   17628:	adds	x11, x11, #0x1
   1762c:	str	x11, [x10], #8
   17630:	b.cs	17624 <__gmpz_combit@@Base+0x160>  // b.hs, b.nlast
   17634:	lsl	x9, x9, #3
   17638:	ldr	w9, [x20, x9]
   1763c:	sub	w8, w8, w9
   17640:	str	w8, [x19, #4]
   17644:	b	175d4 <__gmpz_combit@@Base+0x110>
   17648:	subs	x9, x10, x23
   1764c:	str	x9, [x20, x22, lsl #3]
   17650:	b.cs	1766c <__gmpz_combit@@Base+0x1a8>  // b.hs, b.nlast
   17654:	add	x9, x20, x22, lsl #3
   17658:	add	x9, x9, #0x8
   1765c:	ldr	x10, [x9]
   17660:	sub	x11, x10, #0x1
   17664:	str	x11, [x9], #8
   17668:	cbz	x10, 1765c <__gmpz_combit@@Base+0x198>
   1766c:	mvn	x9, x8
   17670:	ldr	x9, [x20, x9, lsl #3]
   17674:	cmp	x9, #0x0
   17678:	cinc	w8, w8, eq  // eq = none
   1767c:	str	w8, [x19, #4]
   17680:	b	175d4 <__gmpz_combit@@Base+0x110>
   17684:	mov	x0, x19
   17688:	mov	x1, x21
   1768c:	mov	x20, x9
   17690:	bl	c080 <__gmpz_realloc@plt>
   17694:	mov	x9, x20
   17698:	mov	x20, x0
   1769c:	subs	x8, x22, x9
   176a0:	b.ne	175ac <__gmpz_combit@@Base+0xe8>  // b.any
   176a4:	b	175bc <__gmpz_combit@@Base+0xf8>
   176a8:	mov	x9, xzr
   176ac:	neg	w10, w9
   176b0:	cmp	w8, #0x0
   176b4:	csel	x8, x9, x10, ge  // ge = tcont
   176b8:	b	175d0 <__gmpz_combit@@Base+0x10c>
   176bc:	mov	x0, x19
   176c0:	mov	x20, x8
   176c4:	mov	x21, x9
   176c8:	bl	c080 <__gmpz_realloc@plt>
   176cc:	mov	x9, x21
   176d0:	mov	x8, x20
   176d4:	mov	x20, x0
   176d8:	b	17604 <__gmpz_combit@@Base+0x140>

00000000000176dc <__gmpz_congruent_p@@Base>:
   176dc:	stp	x29, x30, [sp, #-80]!
   176e0:	str	x25, [sp, #16]
   176e4:	stp	x24, x23, [sp, #32]
   176e8:	stp	x22, x21, [sp, #48]
   176ec:	stp	x20, x19, [sp, #64]
   176f0:	mov	x29, sp
   176f4:	sub	sp, sp, #0x10
   176f8:	ldrsw	x8, [x2, #4]
   176fc:	cbz	w8, 17a00 <__gmpz_congruent_p@@Base+0x324>
   17700:	ldr	w9, [x0, #4]
   17704:	ldr	w10, [x1, #4]
   17708:	cmp	x8, #0x0
   1770c:	cneg	x21, x8, mi  // mi = first
   17710:	cmp	w9, #0x0
   17714:	cneg	w8, w9, mi  // mi = first
   17718:	cmp	w10, #0x0
   1771c:	cneg	w9, w10, mi  // mi = first
   17720:	cmp	w8, w9
   17724:	csel	x11, x1, x0, lt  // lt = tstop
   17728:	ldrsw	x8, [x11, #4]
   1772c:	csel	x10, x0, x1, lt  // lt = tstop
   17730:	ldr	x20, [x2, #8]
   17734:	ldrsw	x9, [x10, #4]
   17738:	ldr	x22, [x11, #8]
   1773c:	cmp	x8, #0x0
   17740:	cneg	x19, x8, mi  // mi = first
   17744:	cbz	w9, 1778c <__gmpz_congruent_p@@Base+0xb0>
   17748:	ldr	x2, [x10, #8]
   1774c:	ldr	x25, [x20]
   17750:	ldr	x10, [x22]
   17754:	eor	w8, w9, w8
   17758:	ldr	x23, [x2]
   1775c:	cmp	x9, #0x0
   17760:	cneg	x24, x9, mi  // mi = first
   17764:	neg	x9, x25
   17768:	cmp	w8, #0x0
   1776c:	and	x9, x25, x9
   17770:	cneg	x10, x10, lt  // lt = tstop
   17774:	sub	x9, x9, #0x1
   17778:	sub	x10, x10, x23
   1777c:	tst	x9, x10
   17780:	b.eq	177a8 <__gmpz_congruent_p@@Base+0xcc>  // b.none
   17784:	mov	w19, wzr
   17788:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   1778c:	mov	x0, x22
   17790:	mov	x1, x19
   17794:	mov	x2, x20
   17798:	mov	x3, x21
   1779c:	bl	d360 <__gmpn_divisible_p@plt>
   177a0:	mov	w19, w0
   177a4:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   177a8:	cmp	x24, #0x1
   177ac:	b.ne	177f8 <__gmpz_congruent_p@@Base+0x11c>  // b.any
   177b0:	cmp	x21, #0x1
   177b4:	b.ne	177e0 <__gmpz_congruent_p@@Base+0x104>  // b.any
   177b8:	tbz	w8, #31, 17980 <__gmpz_congruent_p@@Base+0x2a4>
   177bc:	subs	x8, x25, x23
   177c0:	b.cs	1797c <__gmpz_congruent_p@@Base+0x2a0>  // b.hs, b.nlast
   177c4:	clz	x8, x25
   177c8:	lsl	x8, x25, x8
   177cc:	cmp	x23, x8
   177d0:	cset	w9, hi  // hi = pmore
   177d4:	lsl	x8, x8, x9
   177d8:	sub	x23, x8, x23
   177dc:	b	17980 <__gmpz_congruent_p@@Base+0x2a4>
   177e0:	cmp	x21, #0x2
   177e4:	b.ne	177f8 <__gmpz_congruent_p@@Base+0x11c>  // b.any
   177e8:	cbz	x25, 177f8 <__gmpz_congruent_p@@Base+0x11c>
   177ec:	ldr	x10, [x20, #8]
   177f0:	cmp	x10, x9
   177f4:	b.ls	1795c <__gmpz_congruent_p@@Base+0x280>  // b.plast
   177f8:	lsl	x25, x19, #3
   177fc:	cmp	x19, #0xfdf
   17800:	add	x1, x25, #0x8
   17804:	str	xzr, [x29, #24]
   17808:	b.hi	17a10 <__gmpz_congruent_p@@Base+0x334>  // b.pmore
   1780c:	add	x10, x1, #0xf
   17810:	mov	x9, sp
   17814:	and	x10, x10, #0xfffffffffffffff0
   17818:	sub	x23, x9, x10
   1781c:	mov	sp, x23
   17820:	tbnz	w8, #31, 17a30 <__gmpz_congruent_p@@Base+0x354>
   17824:	cmp	x19, x24
   17828:	b.le	17900 <__gmpz_congruent_p@@Base+0x224>
   1782c:	cbz	x24, 17864 <__gmpz_congruent_p@@Base+0x188>
   17830:	mov	x0, x23
   17834:	mov	x1, x22
   17838:	mov	x3, x24
   1783c:	bl	c2d0 <__gmpn_sub_n@plt>
   17840:	cbz	x0, 17864 <__gmpz_congruent_p@@Base+0x188>
   17844:	cmp	x24, x19
   17848:	b.ge	17940 <__gmpz_congruent_p@@Base+0x264>  // b.tcont
   1784c:	lsl	x8, x24, #3
   17850:	ldr	x9, [x22, x8]
   17854:	add	x24, x24, #0x1
   17858:	sub	x10, x9, #0x1
   1785c:	str	x10, [x23, x8]
   17860:	cbz	x9, 17844 <__gmpz_congruent_p@@Base+0x168>
   17864:	cmp	x22, x23
   17868:	b.eq	17940 <__gmpz_congruent_p@@Base+0x264>  // b.none
   1786c:	subs	x8, x19, x24
   17870:	b.le	17940 <__gmpz_congruent_p@@Base+0x264>
   17874:	cmp	x8, #0x4
   17878:	b.cc	178dc <__gmpz_congruent_p@@Base+0x200>  // b.lo, b.ul, b.last
   1787c:	lsl	x10, x24, #3
   17880:	add	x9, x23, x10
   17884:	add	x11, x22, x25
   17888:	cmp	x9, x11
   1788c:	b.cs	178a0 <__gmpz_congruent_p@@Base+0x1c4>  // b.hs, b.nlast
   17890:	add	x9, x23, x25
   17894:	add	x11, x22, x10
   17898:	cmp	x9, x11
   1789c:	b.hi	178dc <__gmpz_congruent_p@@Base+0x200>  // b.pmore
   178a0:	and	x9, x8, #0xfffffffffffffffc
   178a4:	add	x11, x10, x22
   178a8:	add	x12, x10, x23
   178ac:	add	x24, x24, x9
   178b0:	add	x10, x11, #0x10
   178b4:	add	x11, x12, #0x10
   178b8:	mov	x12, x9
   178bc:	ldp	q0, q1, [x10, #-16]
   178c0:	add	x10, x10, #0x20
   178c4:	subs	x12, x12, #0x4
   178c8:	stp	q0, q1, [x11, #-16]
   178cc:	add	x11, x11, #0x20
   178d0:	b.ne	178bc <__gmpz_congruent_p@@Base+0x1e0>  // b.any
   178d4:	cmp	x8, x9
   178d8:	b.eq	17940 <__gmpz_congruent_p@@Base+0x264>  // b.none
   178dc:	lsl	x10, x24, #3
   178e0:	sub	x8, x19, x24
   178e4:	add	x9, x23, x10
   178e8:	add	x10, x22, x10
   178ec:	ldr	x11, [x10], #8
   178f0:	subs	x8, x8, #0x1
   178f4:	str	x11, [x9], #8
   178f8:	b.ne	178ec <__gmpz_congruent_p@@Base+0x210>  // b.any
   178fc:	b	17940 <__gmpz_congruent_p@@Base+0x264>
   17900:	sub	x8, x19, #0x1
   17904:	add	x9, x8, #0x1
   17908:	cmp	x9, #0x1
   1790c:	b.lt	1782c <__gmpz_congruent_p@@Base+0x150>  // b.tstop
   17910:	lsl	x9, x8, #3
   17914:	ldr	x10, [x22, x9]
   17918:	ldr	x9, [x2, x9]
   1791c:	sub	x8, x8, #0x1
   17920:	cmp	x10, x9
   17924:	b.eq	17904 <__gmpz_congruent_p@@Base+0x228>  // b.none
   17928:	b.hi	1782c <__gmpz_congruent_p@@Base+0x150>  // b.pmore
   1792c:	mov	x0, x23
   17930:	mov	x1, x2
   17934:	mov	x2, x22
   17938:	mov	x3, x19
   1793c:	bl	c2d0 <__gmpn_sub_n@plt>
   17940:	sub	x8, x23, #0x8
   17944:	mov	x1, x19
   17948:	subs	x19, x19, #0x1
   1794c:	b.lt	17b14 <__gmpz_congruent_p@@Base+0x438>  // b.tstop
   17950:	ldr	x9, [x8, x1, lsl #3]
   17954:	cbz	x9, 17944 <__gmpz_congruent_p@@Base+0x268>
   17958:	b	17b14 <__gmpz_congruent_p@@Base+0x438>
   1795c:	rbit	x9, x25
   17960:	clz	x9, x9
   17964:	lsr	x11, x25, x9
   17968:	neg	x9, x9
   1796c:	lsl	x9, x10, x9
   17970:	orr	x25, x9, x11
   17974:	tbz	w8, #31, 17980 <__gmpz_congruent_p@@Base+0x2a4>
   17978:	b	177bc <__gmpz_congruent_p@@Base+0xe0>
   1797c:	mov	x23, x8
   17980:	cmp	x19, #0x28
   17984:	b.lt	179ac <__gmpz_congruent_p@@Base+0x2d0>  // b.tstop
   17988:	mov	x0, x22
   1798c:	mov	x1, x19
   17990:	mov	x2, x25
   17994:	bl	c3e0 <__gmpn_mod_1@plt>
   17998:	cmp	x23, x25
   1799c:	b.cs	179ec <__gmpz_congruent_p@@Base+0x310>  // b.hs, b.nlast
   179a0:	cmp	x0, x23
   179a4:	cset	w19, eq  // eq = none
   179a8:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   179ac:	rbit	x8, x25
   179b0:	clz	x8, x8
   179b4:	tst	x25, #0x1
   179b8:	csel	x8, x8, xzr, eq  // eq = none
   179bc:	lsr	x20, x25, x8
   179c0:	mov	x0, x22
   179c4:	mov	x1, x19
   179c8:	mov	x2, x20
   179cc:	mov	x3, x23
   179d0:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   179d4:	cmp	x0, #0x0
   179d8:	cset	w8, eq  // eq = none
   179dc:	cmp	x0, x20
   179e0:	cset	w9, eq  // eq = none
   179e4:	orr	w19, w8, w9
   179e8:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   179ec:	udiv	x8, x23, x25
   179f0:	msub	x8, x8, x25, x23
   179f4:	cmp	x0, x8
   179f8:	cset	w19, eq  // eq = none
   179fc:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   17a00:	bl	ce70 <__gmpz_cmp@plt>
   17a04:	cmp	w0, #0x0
   17a08:	cset	w19, eq  // eq = none
   17a0c:	b	17b30 <__gmpz_congruent_p@@Base+0x454>
   17a10:	add	x0, x29, #0x18
   17a14:	stur	x2, [x29, #-8]
   17a18:	mov	w23, w8
   17a1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   17a20:	ldur	x2, [x29, #-8]
   17a24:	mov	w8, w23
   17a28:	mov	x23, x0
   17a2c:	tbz	w8, #31, 17824 <__gmpz_congruent_p@@Base+0x148>
   17a30:	cbz	x24, 17a6c <__gmpz_congruent_p@@Base+0x390>
   17a34:	mov	x0, x23
   17a38:	mov	x1, x22
   17a3c:	mov	x3, x24
   17a40:	bl	ca70 <__gmpn_add_n@plt>
   17a44:	cbz	x0, 17a6c <__gmpz_congruent_p@@Base+0x390>
   17a48:	mov	w9, #0x1                   	// #1
   17a4c:	cmp	x24, x19
   17a50:	b.ge	17b0c <__gmpz_congruent_p@@Base+0x430>  // b.tcont
   17a54:	lsl	x8, x24, #3
   17a58:	ldr	x10, [x22, x8]
   17a5c:	add	x24, x24, #0x1
   17a60:	adds	x10, x10, #0x1
   17a64:	str	x10, [x23, x8]
   17a68:	b.cs	17a4c <__gmpz_congruent_p@@Base+0x370>  // b.hs, b.nlast
   17a6c:	cmp	x22, x23
   17a70:	mov	x9, xzr
   17a74:	b.eq	17b0c <__gmpz_congruent_p@@Base+0x430>  // b.none
   17a78:	subs	x8, x19, x24
   17a7c:	b.le	17b0c <__gmpz_congruent_p@@Base+0x430>
   17a80:	cmp	x8, #0x4
   17a84:	b.cc	17ae8 <__gmpz_congruent_p@@Base+0x40c>  // b.lo, b.ul, b.last
   17a88:	lsl	x10, x24, #3
   17a8c:	add	x9, x23, x10
   17a90:	add	x11, x22, x25
   17a94:	cmp	x9, x11
   17a98:	b.cs	17aac <__gmpz_congruent_p@@Base+0x3d0>  // b.hs, b.nlast
   17a9c:	add	x9, x23, x25
   17aa0:	add	x11, x22, x10
   17aa4:	cmp	x9, x11
   17aa8:	b.hi	17ae8 <__gmpz_congruent_p@@Base+0x40c>  // b.pmore
   17aac:	and	x9, x8, #0xfffffffffffffffc
   17ab0:	add	x11, x10, x22
   17ab4:	add	x12, x10, x23
   17ab8:	add	x24, x24, x9
   17abc:	add	x10, x11, #0x10
   17ac0:	add	x11, x12, #0x10
   17ac4:	mov	x12, x9
   17ac8:	ldp	q0, q1, [x10, #-16]
   17acc:	add	x10, x10, #0x20
   17ad0:	subs	x12, x12, #0x4
   17ad4:	stp	q0, q1, [x11, #-16]
   17ad8:	add	x11, x11, #0x20
   17adc:	b.ne	17ac8 <__gmpz_congruent_p@@Base+0x3ec>  // b.any
   17ae0:	cmp	x8, x9
   17ae4:	b.eq	17b08 <__gmpz_congruent_p@@Base+0x42c>  // b.none
   17ae8:	lsl	x10, x24, #3
   17aec:	sub	x8, x19, x24
   17af0:	add	x9, x23, x10
   17af4:	add	x10, x22, x10
   17af8:	ldr	x11, [x10], #8
   17afc:	subs	x8, x8, #0x1
   17b00:	str	x11, [x9], #8
   17b04:	b.ne	17af8 <__gmpz_congruent_p@@Base+0x41c>  // b.any
   17b08:	mov	x9, xzr
   17b0c:	add	x1, x9, x19
   17b10:	str	x9, [x23, x19, lsl #3]
   17b14:	mov	x0, x23
   17b18:	mov	x2, x20
   17b1c:	mov	x3, x21
   17b20:	bl	d360 <__gmpn_divisible_p@plt>
   17b24:	ldr	x8, [x29, #24]
   17b28:	mov	w19, w0
   17b2c:	cbnz	x8, 17b50 <__gmpz_congruent_p@@Base+0x474>
   17b30:	mov	w0, w19
   17b34:	mov	sp, x29
   17b38:	ldp	x20, x19, [sp, #64]
   17b3c:	ldp	x22, x21, [sp, #48]
   17b40:	ldp	x24, x23, [sp, #32]
   17b44:	ldr	x25, [sp, #16]
   17b48:	ldp	x29, x30, [sp], #80
   17b4c:	ret
   17b50:	mov	x0, x8
   17b54:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   17b58:	b	17b30 <__gmpz_congruent_p@@Base+0x454>

0000000000017b5c <__gmpz_congruent_2exp_p@@Base>:
   17b5c:	ldrsw	x13, [x0, #4]
   17b60:	ldrsw	x15, [x1, #4]
   17b64:	cmp	x13, #0x0
   17b68:	cneg	x8, x13, mi  // mi = first
   17b6c:	cmp	x15, #0x0
   17b70:	cneg	x9, x15, mi  // mi = first
   17b74:	cmp	x8, x9
   17b78:	csel	x11, x9, x8, lt  // lt = tstop
   17b7c:	csel	x12, x8, x9, lt  // lt = tstop
   17b80:	csel	x8, x1, x0, lt  // lt = tstop
   17b84:	ldr	x10, [x8, #8]
   17b88:	mov	x8, #0xffffffffffffffff    	// #-1
   17b8c:	lsl	x8, x8, x2
   17b90:	csel	x14, x0, x1, lt  // lt = tstop
   17b94:	lsr	x9, x2, #6
   17b98:	mvn	x8, x8
   17b9c:	cbz	x12, 17c7c <__gmpz_congruent_2exp_p@@Base+0x120>
   17ba0:	ldr	x14, [x14, #8]
   17ba4:	eor	w13, w15, w13
   17ba8:	tbnz	w13, #31, 17be4 <__gmpz_congruent_2exp_p@@Base+0x88>
   17bac:	cmp	x12, x9
   17bb0:	sub	x13, x14, #0x8
   17bb4:	csel	x16, x12, x9, cc  // cc = lo, ul, last
   17bb8:	sub	x15, x10, #0x8
   17bbc:	subs	x17, x16, #0x1
   17bc0:	b.lt	17c58 <__gmpz_congruent_2exp_p@@Base+0xfc>  // b.tstop
   17bc4:	lsl	x16, x16, #3
   17bc8:	ldr	x18, [x15, x16]
   17bcc:	ldr	x16, [x13, x16]
   17bd0:	cmp	x18, x16
   17bd4:	mov	x16, x17
   17bd8:	b.eq	17bbc <__gmpz_congruent_2exp_p@@Base+0x60>  // b.none
   17bdc:	mov	w0, wzr
   17be0:	ret
   17be4:	mov	x15, xzr
   17be8:	and	w13, w2, #0x3f
   17bec:	lsl	x17, x15, #3
   17bf0:	ldr	x16, [x10, x17]
   17bf4:	ldr	x17, [x14, x17]
   17bf8:	cmp	x9, x15
   17bfc:	add	x17, x17, x16
   17c00:	b.eq	17cbc <__gmpz_congruent_2exp_p@@Base+0x160>  // b.none
   17c04:	cbnz	x17, 17cfc <__gmpz_congruent_2exp_p@@Base+0x1a0>
   17c08:	add	x15, x15, #0x1
   17c0c:	cbz	x16, 17bec <__gmpz_congruent_2exp_p@@Base+0x90>
   17c10:	cmp	x15, x12
   17c14:	b.cs	17c48 <__gmpz_congruent_2exp_p@@Base+0xec>  // b.hs, b.nlast
   17c18:	lsl	x16, x15, #3
   17c1c:	ldr	x17, [x10, x16]
   17c20:	ldr	x16, [x14, x16]
   17c24:	cmp	x15, x9
   17c28:	eor	x16, x16, x17
   17c2c:	b.cs	17d04 <__gmpz_congruent_2exp_p@@Base+0x1a8>  // b.hs, b.nlast
   17c30:	cmn	x16, #0x1
   17c34:	b.ne	17cfc <__gmpz_congruent_2exp_p@@Base+0x1a0>  // b.any
   17c38:	add	x15, x15, #0x1
   17c3c:	cmp	x15, x12
   17c40:	b.cc	17c18 <__gmpz_congruent_2exp_p@@Base+0xbc>  // b.lo, b.ul, b.last
   17c44:	mov	x15, x12
   17c48:	cmp	x11, x9
   17c4c:	b.cs	17cc8 <__gmpz_congruent_2exp_p@@Base+0x16c>  // b.hs, b.nlast
   17c50:	mov	w0, wzr
   17c54:	ret
   17c58:	cmp	x12, x9
   17c5c:	b.ls	17c7c <__gmpz_congruent_2exp_p@@Base+0x120>  // b.plast
   17c60:	lsl	x9, x9, #3
   17c64:	ldr	x10, [x10, x9]
   17c68:	ldr	x9, [x14, x9]
   17c6c:	sub	x9, x10, x9
   17c70:	tst	x9, x8
   17c74:	cset	w0, eq  // eq = none
   17c78:	ret
   17c7c:	cmp	x11, x9
   17c80:	b.ls	17cb0 <__gmpz_congruent_2exp_p@@Base+0x154>  // b.plast
   17c84:	cmp	x12, x9
   17c88:	b.cs	17ca0 <__gmpz_congruent_2exp_p@@Base+0x144>  // b.hs, b.nlast
   17c8c:	ldr	x11, [x10, x12, lsl #3]
   17c90:	cbnz	x11, 17cfc <__gmpz_congruent_2exp_p@@Base+0x1a0>
   17c94:	add	x12, x12, #0x1
   17c98:	cmp	x12, x9
   17c9c:	b.cc	17c8c <__gmpz_congruent_2exp_p@@Base+0x130>  // b.lo, b.ul, b.last
   17ca0:	ldr	x9, [x10, x9, lsl #3]
   17ca4:	tst	x9, x8
   17ca8:	cset	w0, eq  // eq = none
   17cac:	ret
   17cb0:	cmp	x11, x12
   17cb4:	cset	w0, eq  // eq = none
   17cb8:	ret
   17cbc:	tst	x17, x8
   17cc0:	cset	w0, eq  // eq = none
   17cc4:	ret
   17cc8:	cmp	x15, x9
   17ccc:	b.cs	17cf0 <__gmpz_congruent_2exp_p@@Base+0x194>  // b.hs, b.nlast
   17cd0:	sub	x12, x9, x15
   17cd4:	add	x14, x10, x15, lsl #3
   17cd8:	ldr	x15, [x14]
   17cdc:	cmn	x15, #0x1
   17ce0:	b.ne	17cfc <__gmpz_congruent_2exp_p@@Base+0x1a0>  // b.any
   17ce4:	subs	x12, x12, #0x1
   17ce8:	add	x14, x14, #0x8
   17cec:	b.ne	17cd8 <__gmpz_congruent_2exp_p@@Base+0x17c>  // b.any
   17cf0:	cbz	w13, 17d10 <__gmpz_congruent_2exp_p@@Base+0x1b4>
   17cf4:	cmp	x11, x9
   17cf8:	b.ne	17d18 <__gmpz_congruent_2exp_p@@Base+0x1bc>  // b.any
   17cfc:	mov	w0, wzr
   17d00:	ret
   17d04:	bics	xzr, x8, x16
   17d08:	cset	w0, eq  // eq = none
   17d0c:	ret
   17d10:	mov	w0, #0x1                   	// #1
   17d14:	ret
   17d18:	ldr	x9, [x10, x9, lsl #3]
   17d1c:	add	x9, x9, #0x1
   17d20:	tst	x9, x8
   17d24:	cset	w0, eq  // eq = none
   17d28:	ret

0000000000017d2c <__gmpz_congruent_ui_p@@Base>:
   17d2c:	stp	x29, x30, [sp, #-32]!
   17d30:	stp	x20, x19, [sp, #16]
   17d34:	mov	x20, x1
   17d38:	mov	x29, sp
   17d3c:	cbz	x2, 17e70 <__gmpz_congruent_ui_p@@Base+0x144>
   17d40:	ldrsw	x1, [x0, #4]
   17d44:	mov	x19, x2
   17d48:	cbz	w1, 17da4 <__gmpz_congruent_ui_p@@Base+0x78>
   17d4c:	tbz	w1, #31, 17d74 <__gmpz_congruent_ui_p@@Base+0x48>
   17d50:	subs	x8, x19, x20
   17d54:	neg	x1, x1
   17d58:	b.cs	17df8 <__gmpz_congruent_ui_p@@Base+0xcc>  // b.hs, b.nlast
   17d5c:	clz	x8, x19
   17d60:	lsl	x8, x19, x8
   17d64:	cmp	x8, x20
   17d68:	cset	w9, cc  // cc = lo, ul, last
   17d6c:	lsl	x8, x8, x9
   17d70:	sub	x20, x8, x20
   17d74:	ldr	x0, [x0, #8]
   17d78:	cmp	x1, #0x28
   17d7c:	b.lt	17e08 <__gmpz_congruent_ui_p@@Base+0xdc>  // b.tstop
   17d80:	mov	x2, x19
   17d84:	bl	c3e0 <__gmpn_mod_1@plt>
   17d88:	cmp	x20, x19
   17d8c:	b.cs	17ddc <__gmpz_congruent_ui_p@@Base+0xb0>  // b.hs, b.nlast
   17d90:	cmp	x0, x20
   17d94:	cset	w0, eq  // eq = none
   17d98:	ldp	x20, x19, [sp, #16]
   17d9c:	ldp	x29, x30, [sp], #32
   17da0:	ret
   17da4:	cmp	x19, x20
   17da8:	b.ls	17dc0 <__gmpz_congruent_ui_p@@Base+0x94>  // b.plast
   17dac:	cmp	x20, #0x0
   17db0:	cset	w0, eq  // eq = none
   17db4:	ldp	x20, x19, [sp, #16]
   17db8:	ldp	x29, x30, [sp], #32
   17dbc:	ret
   17dc0:	udiv	x8, x20, x19
   17dc4:	msub	x8, x8, x19, x20
   17dc8:	cmp	x8, #0x0
   17dcc:	cset	w0, eq  // eq = none
   17dd0:	ldp	x20, x19, [sp, #16]
   17dd4:	ldp	x29, x30, [sp], #32
   17dd8:	ret
   17ddc:	udiv	x8, x20, x19
   17de0:	msub	x8, x8, x19, x20
   17de4:	cmp	x0, x8
   17de8:	cset	w0, eq  // eq = none
   17dec:	ldp	x20, x19, [sp, #16]
   17df0:	ldp	x29, x30, [sp], #32
   17df4:	ret
   17df8:	mov	x20, x8
   17dfc:	ldr	x0, [x0, #8]
   17e00:	cmp	x1, #0x28
   17e04:	b.ge	17d80 <__gmpz_congruent_ui_p@@Base+0x54>  // b.tcont
   17e08:	tbnz	w19, #0, 17e44 <__gmpz_congruent_ui_p@@Base+0x118>
   17e0c:	ldr	x8, [x0]
   17e10:	neg	x9, x19
   17e14:	and	x9, x9, x19
   17e18:	sub	x9, x9, #0x1
   17e1c:	sub	x8, x8, x20
   17e20:	tst	x8, x9
   17e24:	b.eq	17e38 <__gmpz_congruent_ui_p@@Base+0x10c>  // b.none
   17e28:	mov	w0, wzr
   17e2c:	ldp	x20, x19, [sp, #16]
   17e30:	ldp	x29, x30, [sp], #32
   17e34:	ret
   17e38:	rbit	x8, x19
   17e3c:	clz	x8, x8
   17e40:	lsr	x19, x19, x8
   17e44:	mov	x2, x19
   17e48:	mov	x3, x20
   17e4c:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   17e50:	cmp	x0, #0x0
   17e54:	cset	w8, eq  // eq = none
   17e58:	cmp	x0, x19
   17e5c:	cset	w9, eq  // eq = none
   17e60:	orr	w0, w8, w9
   17e64:	ldp	x20, x19, [sp, #16]
   17e68:	ldp	x29, x30, [sp], #32
   17e6c:	ret
   17e70:	mov	x1, x20
   17e74:	bl	d1f0 <__gmpz_cmp_ui@plt>
   17e78:	cmp	w0, #0x0
   17e7c:	cset	w0, eq  // eq = none
   17e80:	ldp	x20, x19, [sp, #16]
   17e84:	ldp	x29, x30, [sp], #32
   17e88:	ret

0000000000017e8c <__gmpz_divexact@@Base>:
   17e8c:	stp	x29, x30, [sp, #-80]!
   17e90:	stp	x24, x23, [sp, #32]
   17e94:	stp	x22, x21, [sp, #48]
   17e98:	stp	x20, x19, [sp, #64]
   17e9c:	ldr	w8, [x1, #4]
   17ea0:	ldr	w9, [x2, #4]
   17ea4:	mov	x19, x0
   17ea8:	str	x25, [sp, #16]
   17eac:	cmp	w8, #0x0
   17eb0:	cneg	w23, w8, mi  // mi = first
   17eb4:	cmp	w9, #0x0
   17eb8:	cneg	w22, w9, mi  // mi = first
   17ebc:	cmp	w23, w22
   17ec0:	mov	x29, sp
   17ec4:	b.cs	17ed0 <__gmpz_divexact@@Base+0x44>  // b.hs, b.nlast
   17ec8:	str	wzr, [x19, #4]
   17ecc:	b	17fa8 <__gmpz_divexact@@Base+0x11c>
   17ed0:	sub	x25, x23, x22
   17ed4:	mov	x20, x1
   17ed8:	mov	x21, x2
   17edc:	cmp	x19, x1
   17ee0:	add	x1, x25, #0x1
   17ee4:	str	xzr, [x29, #24]
   17ee8:	b.eq	17f08 <__gmpz_divexact@@Base+0x7c>  // b.none
   17eec:	cmp	x19, x21
   17ef0:	b.eq	17f08 <__gmpz_divexact@@Base+0x7c>  // b.none
   17ef4:	ldrsw	x8, [x19]
   17ef8:	cmp	x25, x8
   17efc:	b.ge	17fdc <__gmpz_divexact@@Base+0x150>  // b.tcont
   17f00:	ldr	x24, [x19, #8]
   17f04:	b	17f2c <__gmpz_divexact@@Base+0xa0>
   17f08:	lsl	x1, x1, #3
   17f0c:	mov	w8, #0x7f00                	// #32512
   17f10:	cmp	x1, x8
   17f14:	b.hi	17fec <__gmpz_divexact@@Base+0x160>  // b.pmore
   17f18:	add	x9, x1, #0xf
   17f1c:	mov	x8, sp
   17f20:	and	x9, x9, #0xfffffffffffffff0
   17f24:	sub	x24, x8, x9
   17f28:	mov	sp, x24
   17f2c:	ldr	x1, [x20, #8]
   17f30:	ldr	x3, [x21, #8]
   17f34:	mov	x0, x24
   17f38:	mov	x2, x23
   17f3c:	mov	x4, x22
   17f40:	bl	c430 <__gmpn_divexact@plt>
   17f44:	add	x22, x25, #0x1
   17f48:	mov	x23, x25
   17f4c:	cmp	x22, #0x1
   17f50:	b.lt	17f60 <__gmpz_divexact@@Base+0xd4>  // b.tstop
   17f54:	ldr	x8, [x24, x23, lsl #3]
   17f58:	sub	x25, x23, #0x1
   17f5c:	cbz	x8, 17f44 <__gmpz_divexact@@Base+0xb8>
   17f60:	ldr	x0, [x19, #8]
   17f64:	cmp	x24, x0
   17f68:	b.eq	17f84 <__gmpz_divexact@@Base+0xf8>  // b.none
   17f6c:	ldrsw	x8, [x19]
   17f70:	cmp	x22, x8
   17f74:	b.gt	17fcc <__gmpz_divexact@@Base+0x140>
   17f78:	mov	x1, x24
   17f7c:	mov	x2, x22
   17f80:	bl	ca50 <__gmpn_copyi@plt>
   17f84:	ldr	w8, [x20, #4]
   17f88:	ldr	w9, [x21, #4]
   17f8c:	eor	w8, w9, w8
   17f90:	mvn	w9, w23
   17f94:	cmp	w8, #0x0
   17f98:	csel	x8, x22, x9, ge  // ge = tcont
   17f9c:	str	w8, [x19, #4]
   17fa0:	ldr	x0, [x29, #24]
   17fa4:	cbnz	x0, 17fc4 <__gmpz_divexact@@Base+0x138>
   17fa8:	mov	sp, x29
   17fac:	ldp	x20, x19, [sp, #64]
   17fb0:	ldp	x22, x21, [sp, #48]
   17fb4:	ldp	x24, x23, [sp, #32]
   17fb8:	ldr	x25, [sp, #16]
   17fbc:	ldp	x29, x30, [sp], #80
   17fc0:	ret
   17fc4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   17fc8:	b	17fa8 <__gmpz_divexact@@Base+0x11c>
   17fcc:	mov	x0, x19
   17fd0:	mov	x1, x22
   17fd4:	bl	c080 <__gmpz_realloc@plt>
   17fd8:	b	17f78 <__gmpz_divexact@@Base+0xec>
   17fdc:	mov	x0, x19
   17fe0:	bl	c080 <__gmpz_realloc@plt>
   17fe4:	mov	x24, x0
   17fe8:	b	17f2c <__gmpz_divexact@@Base+0xa0>
   17fec:	add	x0, x29, #0x18
   17ff0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   17ff4:	mov	x24, x0
   17ff8:	b	17f2c <__gmpz_divexact@@Base+0xa0>

0000000000017ffc <__gmpz_divexact_gcd@@Base>:
   17ffc:	stp	x29, x30, [sp, #-64]!
   18000:	stp	x22, x21, [sp, #32]
   18004:	stp	x20, x19, [sp, #48]
   18008:	ldr	w8, [x1, #4]
   1800c:	mov	x19, x0
   18010:	str	x23, [sp, #16]
   18014:	mov	x29, sp
   18018:	cbz	w8, 18084 <__gmpz_divexact_gcd@@Base+0x88>
   1801c:	ldr	w8, [x2, #4]
   18020:	cmp	w8, #0x1
   18024:	b.ne	1808c <__gmpz_divexact_gcd@@Base+0x90>  // b.any
   18028:	ldr	x8, [x2, #8]
   1802c:	ldr	x20, [x8]
   18030:	tbnz	w20, #0, 1804c <__gmpz_divexact_gcd@@Base+0x50>
   18034:	rbit	x8, x20
   18038:	clz	x2, x8
   1803c:	mov	x0, x19
   18040:	lsr	x20, x20, x2
   18044:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   18048:	mov	x1, x19
   1804c:	cmp	x20, #0x5
   18050:	b.eq	180a4 <__gmpz_divexact_gcd@@Base+0xa8>  // b.none
   18054:	cmp	x20, #0x3
   18058:	b.eq	180cc <__gmpz_divexact_gcd@@Base+0xd0>  // b.none
   1805c:	cmp	x20, #0x1
   18060:	b.ne	18120 <__gmpz_divexact_gcd@@Base+0x124>  // b.any
   18064:	cmp	x1, x19
   18068:	b.eq	18174 <__gmpz_divexact_gcd@@Base+0x178>  // b.none
   1806c:	mov	x0, x19
   18070:	ldp	x20, x19, [sp, #48]
   18074:	ldp	x22, x21, [sp, #32]
   18078:	ldr	x23, [sp, #16]
   1807c:	ldp	x29, x30, [sp], #64
   18080:	b	c420 <__gmpz_set@plt>
   18084:	str	wzr, [x19, #4]
   18088:	b	18174 <__gmpz_divexact_gcd@@Base+0x178>
   1808c:	mov	x0, x19
   18090:	ldp	x20, x19, [sp, #48]
   18094:	ldp	x22, x21, [sp, #32]
   18098:	ldr	x23, [sp, #16]
   1809c:	ldp	x29, x30, [sp], #64
   180a0:	b	c3f0 <__gmpz_divexact@plt>
   180a4:	ldrsw	x22, [x1, #4]
   180a8:	ldrsw	x8, [x19]
   180ac:	cmp	x22, #0x0
   180b0:	cneg	x20, x22, mi  // mi = first
   180b4:	cmp	x20, x8
   180b8:	b.gt	18188 <__gmpz_divexact_gcd@@Base+0x18c>
   180bc:	ldr	x21, [x19, #8]
   180c0:	ldr	x1, [x1, #8]
   180c4:	mov	x3, #0x3333333333333333    	// #3689348814741910323
   180c8:	b	180f0 <__gmpz_divexact_gcd@@Base+0xf4>
   180cc:	ldrsw	x22, [x1, #4]
   180d0:	ldrsw	x8, [x19]
   180d4:	cmp	x22, #0x0
   180d8:	cneg	x20, x22, mi  // mi = first
   180dc:	cmp	x20, x8
   180e0:	b.gt	181a4 <__gmpz_divexact_gcd@@Base+0x1a8>
   180e4:	ldr	x21, [x19, #8]
   180e8:	ldr	x1, [x1, #8]
   180ec:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   180f0:	mov	x0, x21
   180f4:	mov	x2, x20
   180f8:	mov	x4, xzr
   180fc:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   18100:	add	x8, x21, x20, lsl #3
   18104:	ldur	x8, [x8, #-8]
   18108:	cmp	x8, #0x0
   1810c:	cset	w8, eq  // eq = none
   18110:	sub	x8, x20, x8
   18114:	neg	w9, w8
   18118:	cmp	w22, #0x0
   1811c:	b	1816c <__gmpz_divexact_gcd@@Base+0x170>
   18120:	ldrsw	x23, [x1, #4]
   18124:	ldrsw	x8, [x19]
   18128:	cmp	x23, #0x0
   1812c:	cneg	x21, x23, mi  // mi = first
   18130:	cmp	x21, x8
   18134:	b.gt	181c0 <__gmpz_divexact_gcd@@Base+0x1c4>
   18138:	ldr	x22, [x19, #8]
   1813c:	ldr	x1, [x1, #8]
   18140:	mov	x0, x22
   18144:	mov	x2, x21
   18148:	mov	x3, x20
   1814c:	bl	c770 <__gmpn_divexact_1@plt>
   18150:	add	x8, x22, x21, lsl #3
   18154:	ldur	x8, [x8, #-8]
   18158:	cmp	x8, #0x0
   1815c:	cset	w8, eq  // eq = none
   18160:	sub	x8, x21, x8
   18164:	neg	w9, w8
   18168:	cmp	w23, #0x0
   1816c:	csel	x8, x8, x9, gt
   18170:	str	w8, [x19, #4]
   18174:	ldp	x20, x19, [sp, #48]
   18178:	ldp	x22, x21, [sp, #32]
   1817c:	ldr	x23, [sp, #16]
   18180:	ldp	x29, x30, [sp], #64
   18184:	ret
   18188:	mov	x0, x19
   1818c:	mov	x21, x1
   18190:	mov	x1, x20
   18194:	bl	c080 <__gmpz_realloc@plt>
   18198:	mov	x1, x21
   1819c:	mov	x21, x0
   181a0:	b	180c0 <__gmpz_divexact_gcd@@Base+0xc4>
   181a4:	mov	x0, x19
   181a8:	mov	x21, x1
   181ac:	mov	x1, x20
   181b0:	bl	c080 <__gmpz_realloc@plt>
   181b4:	mov	x1, x21
   181b8:	mov	x21, x0
   181bc:	b	180e8 <__gmpz_divexact_gcd@@Base+0xec>
   181c0:	mov	x0, x19
   181c4:	mov	x22, x1
   181c8:	mov	x1, x21
   181cc:	bl	c080 <__gmpz_realloc@plt>
   181d0:	mov	x1, x22
   181d4:	mov	x22, x0
   181d8:	b	1813c <__gmpz_divexact_gcd@@Base+0x140>

00000000000181dc <__gmpz_divexact_ui@@Base>:
   181dc:	stp	x29, x30, [sp, #-64]!
   181e0:	stp	x24, x23, [sp, #16]
   181e4:	stp	x22, x21, [sp, #32]
   181e8:	stp	x20, x19, [sp, #48]
   181ec:	mov	x29, sp
   181f0:	cbz	x2, 18284 <__gmpz_divexact_ui@@Base+0xa8>
   181f4:	ldrsw	x24, [x1, #4]
   181f8:	mov	x21, x1
   181fc:	mov	x19, x0
   18200:	cbz	w24, 18254 <__gmpz_divexact_ui@@Base+0x78>
   18204:	ldrsw	x8, [x19]
   18208:	cmp	w24, #0x0
   1820c:	cneg	x22, x24, lt  // lt = tstop
   18210:	mov	x20, x2
   18214:	cmp	x22, x8
   18218:	b.gt	18270 <__gmpz_divexact_ui@@Base+0x94>
   1821c:	ldr	x23, [x19, #8]
   18220:	ldr	x1, [x21, #8]
   18224:	mov	x0, x23
   18228:	mov	x2, x22
   1822c:	mov	x3, x20
   18230:	bl	c770 <__gmpn_divexact_1@plt>
   18234:	add	x8, x23, x22, lsl #3
   18238:	ldur	x8, [x8, #-8]
   1823c:	cmp	x8, #0x0
   18240:	cset	w8, eq  // eq = none
   18244:	sub	w8, w22, w8
   18248:	cmp	w24, #0x0
   1824c:	cneg	w8, w8, lt  // lt = tstop
   18250:	b	18258 <__gmpz_divexact_ui@@Base+0x7c>
   18254:	mov	w8, wzr
   18258:	str	w8, [x19, #4]
   1825c:	ldp	x20, x19, [sp, #48]
   18260:	ldp	x22, x21, [sp, #32]
   18264:	ldp	x24, x23, [sp, #16]
   18268:	ldp	x29, x30, [sp], #64
   1826c:	ret
   18270:	mov	x0, x19
   18274:	mov	x1, x22
   18278:	bl	c080 <__gmpz_realloc@plt>
   1827c:	mov	x23, x0
   18280:	b	18220 <__gmpz_divexact_ui@@Base+0x44>
   18284:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000018288 <__gmpz_divisible_p@@Base>:
   18288:	ldrsw	x8, [x1, #4]
   1828c:	ldrsw	x9, [x0, #4]
   18290:	cbz	w8, 182b0 <__gmpz_divisible_p@@Base+0x28>
   18294:	ldr	x0, [x0, #8]
   18298:	ldr	x2, [x1, #8]
   1829c:	cmp	x9, #0x0
   182a0:	cneg	x1, x9, mi  // mi = first
   182a4:	cmp	x8, #0x0
   182a8:	cneg	x3, x8, mi  // mi = first
   182ac:	b	d360 <__gmpn_divisible_p@plt>
   182b0:	cmp	w9, #0x0
   182b4:	cset	w0, eq  // eq = none
   182b8:	ret

00000000000182bc <__gmpz_divisible_ui_p@@Base>:
   182bc:	stp	x29, x30, [sp, #-16]!
   182c0:	ldrsw	x9, [x0, #4]
   182c4:	mov	x8, x0
   182c8:	mov	x29, sp
   182cc:	cmp	x9, #0x0
   182d0:	cset	w10, eq  // eq = none
   182d4:	cmp	x1, #0x0
   182d8:	cset	w11, ne  // ne = any
   182dc:	orr	w0, w10, w11
   182e0:	cbz	x1, 1834c <__gmpz_divisible_ui_p@@Base+0x90>
   182e4:	cbz	w9, 1834c <__gmpz_divisible_ui_p@@Base+0x90>
   182e8:	ldr	x0, [x8, #8]
   182ec:	cmp	x9, #0x0
   182f0:	mov	x2, x1
   182f4:	cneg	x1, x9, mi  // mi = first
   182f8:	cmp	x1, #0x28
   182fc:	b.lt	18308 <__gmpz_divisible_ui_p@@Base+0x4c>  // b.tstop
   18300:	bl	c3e0 <__gmpn_mod_1@plt>
   18304:	b	18344 <__gmpz_divisible_ui_p@@Base+0x88>
   18308:	tbnz	w2, #0, 1833c <__gmpz_divisible_ui_p@@Base+0x80>
   1830c:	ldr	x8, [x0]
   18310:	neg	x9, x2
   18314:	and	x9, x9, x2
   18318:	sub	x9, x9, #0x1
   1831c:	tst	x8, x9
   18320:	b.eq	18330 <__gmpz_divisible_ui_p@@Base+0x74>  // b.none
   18324:	mov	w0, wzr
   18328:	ldp	x29, x30, [sp], #16
   1832c:	ret
   18330:	rbit	x8, x2
   18334:	clz	x8, x8
   18338:	lsr	x2, x2, x8
   1833c:	mov	x3, xzr
   18340:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   18344:	cmp	x0, #0x0
   18348:	cset	w0, eq  // eq = none
   1834c:	ldp	x29, x30, [sp], #16
   18350:	ret

0000000000018354 <__gmpz_divisible_2exp_p@@Base>:
   18354:	ldr	w8, [x0, #4]
   18358:	cmp	w8, #0x0
   1835c:	cneg	w9, w8, mi  // mi = first
   18360:	lsr	x8, x1, #6
   18364:	cmp	x8, x9
   18368:	b.cs	183a8 <__gmpz_divisible_2exp_p@@Base+0x54>  // b.hs, b.nlast
   1836c:	ldr	x9, [x0, #8]
   18370:	cbz	x8, 18390 <__gmpz_divisible_2exp_p@@Base+0x3c>
   18374:	mov	x10, x9
   18378:	mov	x11, x8
   1837c:	ldr	x12, [x10]
   18380:	cbnz	x12, 183b4 <__gmpz_divisible_2exp_p@@Base+0x60>
   18384:	subs	x11, x11, #0x1
   18388:	add	x10, x10, #0x8
   1838c:	b.ne	1837c <__gmpz_divisible_2exp_p@@Base+0x28>  // b.any
   18390:	ldr	x8, [x9, x8, lsl #3]
   18394:	mov	x9, #0xffffffffffffffff    	// #-1
   18398:	lsl	x9, x9, x1
   1839c:	bics	xzr, x8, x9
   183a0:	cset	w0, eq  // eq = none
   183a4:	ret
   183a8:	cmp	w9, #0x0
   183ac:	cset	w0, eq  // eq = none
   183b0:	ret
   183b4:	mov	w0, wzr
   183b8:	ret

00000000000183bc <__gmpz_dump@@Base>:
   183bc:	stp	x29, x30, [sp, #-32]!
   183c0:	mov	x2, x0
   183c4:	mov	w1, #0xa                   	// #10
   183c8:	mov	x0, xzr
   183cc:	str	x19, [sp, #16]
   183d0:	mov	x29, sp
   183d4:	bl	c3a0 <__gmpz_get_str@plt>
   183d8:	mov	x19, x0
   183dc:	bl	c9b0 <puts@plt>
   183e0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   183e4:	ldr	x8, [x8, #4016]
   183e8:	ldr	x0, [x8]
   183ec:	str	x0, [x29, #24]
   183f0:	mov	x0, x19
   183f4:	bl	bf60 <strlen@plt>
   183f8:	add	x1, x0, #0x1
   183fc:	mov	x0, x19
   18400:	ldr	x2, [x29, #24]
   18404:	ldr	x19, [sp, #16]
   18408:	ldp	x29, x30, [sp], #32
   1840c:	br	x2

0000000000018410 <__gmpz_export@@Base>:
   18410:	sub	sp, sp, #0x70
   18414:	stp	x29, x30, [sp, #16]
   18418:	stp	x28, x27, [sp, #32]
   1841c:	stp	x26, x25, [sp, #48]
   18420:	stp	x24, x23, [sp, #64]
   18424:	stp	x22, x21, [sp, #80]
   18428:	stp	x20, x19, [sp, #96]
   1842c:	ldrsw	x9, [x6, #4]
   18430:	cmp	x1, #0x0
   18434:	add	x8, sp, #0x8
   18438:	mov	x19, x0
   1843c:	csel	x8, x8, x1, eq  // eq = none
   18440:	add	x29, sp, #0x10
   18444:	cbz	w9, 184a0 <__gmpz_export@@Base+0x90>
   18448:	ldr	x21, [x6, #8]
   1844c:	cmp	x9, #0x0
   18450:	cneg	x26, x9, mi  // mi = first
   18454:	lsl	x10, x3, #3
   18458:	add	x9, x21, x26, lsl #3
   1845c:	ldur	x9, [x9, #-8]
   18460:	sub	x27, x10, x5
   18464:	add	x10, x27, x26, lsl #6
   18468:	mov	x24, x5
   1846c:	clz	x9, x9
   18470:	mvn	x9, x9
   18474:	add	x28, x9, x10
   18478:	mov	w25, w4
   1847c:	mov	x22, x3
   18480:	mov	w23, w2
   18484:	udiv	x20, x28, x27
   18488:	str	x20, [x8]
   1848c:	cbz	x19, 184a8 <__gmpz_export@@Base+0x98>
   18490:	cmp	w25, #0x0
   18494:	csinv	w11, w25, wzr, ne  // ne = any
   18498:	cbnz	x24, 18694 <__gmpz_export@@Base+0x284>
   1849c:	b	184cc <__gmpz_export@@Base+0xbc>
   184a0:	str	xzr, [x8]
   184a4:	b	186c0 <__gmpz_export@@Base+0x2b0>
   184a8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   184ac:	ldr	x8, [x8, #3840]
   184b0:	mul	x0, x20, x22
   184b4:	ldr	x8, [x8]
   184b8:	blr	x8
   184bc:	mov	x19, x0
   184c0:	cmp	w25, #0x0
   184c4:	csinv	w11, w25, wzr, ne  // ne = any
   184c8:	cbnz	x24, 18694 <__gmpz_export@@Base+0x284>
   184cc:	cmp	x22, #0x8
   184d0:	b.ne	18694 <__gmpz_export@@Base+0x284>  // b.any
   184d4:	and	x8, x19, #0x7
   184d8:	cbnz	x8, 18694 <__gmpz_export@@Base+0x284>
   184dc:	and	w8, w11, w23
   184e0:	cmn	w8, #0x1
   184e4:	b.eq	18544 <__gmpz_export@@Base+0x134>  // b.none
   184e8:	cmp	w23, #0x1
   184ec:	b.ne	18558 <__gmpz_export@@Base+0x148>  // b.any
   184f0:	cmn	w11, #0x1
   184f4:	b.ne	18558 <__gmpz_export@@Base+0x148>  // b.any
   184f8:	cmp	x20, #0x1
   184fc:	b.lt	186c0 <__gmpz_export@@Base+0x2b0>  // b.tstop
   18500:	cmp	x20, #0x4
   18504:	add	x10, x21, x20, lsl #3
   18508:	b.cc	18520 <__gmpz_export@@Base+0x110>  // b.lo, b.ul, b.last
   1850c:	cmp	x19, x10
   18510:	b.cs	18904 <__gmpz_export@@Base+0x4f4>  // b.hs, b.nlast
   18514:	add	x8, x19, x20, lsl #3
   18518:	cmp	x8, x21
   1851c:	b.ls	18904 <__gmpz_export@@Base+0x4f4>  // b.plast
   18520:	mov	x8, xzr
   18524:	mov	x9, x19
   18528:	sub	x10, x10, #0x8
   1852c:	ldr	x11, [x10], #-8
   18530:	add	x8, x8, #0x1
   18534:	cmp	x8, x20
   18538:	str	x11, [x9], #8
   1853c:	b.lt	1852c <__gmpz_export@@Base+0x11c>  // b.tstop
   18540:	b	186c0 <__gmpz_export@@Base+0x2b0>
   18544:	mov	x0, x19
   18548:	mov	x1, x21
   1854c:	mov	x2, x20
   18550:	bl	ca50 <__gmpn_copyi@plt>
   18554:	b	186c0 <__gmpz_export@@Base+0x2b0>
   18558:	cmn	w23, #0x1
   1855c:	b.ne	185f8 <__gmpz_export@@Base+0x1e8>  // b.any
   18560:	cmp	w11, #0x1
   18564:	b.ne	185f8 <__gmpz_export@@Base+0x1e8>  // b.any
   18568:	cmp	x20, #0x1
   1856c:	b.lt	186c0 <__gmpz_export@@Base+0x2b0>  // b.tstop
   18570:	cmp	x20, #0x1
   18574:	b.eq	18594 <__gmpz_export@@Base+0x184>  // b.none
   18578:	lsl	x8, x20, #3
   1857c:	add	x9, x21, x8
   18580:	cmp	x19, x9
   18584:	b.cs	18954 <__gmpz_export@@Base+0x544>  // b.hs, b.nlast
   18588:	add	x8, x19, x8
   1858c:	cmp	x8, x21
   18590:	b.ls	18954 <__gmpz_export@@Base+0x544>  // b.plast
   18594:	mov	x8, xzr
   18598:	mov	x9, x21
   1859c:	mov	x10, x19
   185a0:	ldr	x11, [x9], #8
   185a4:	add	x8, x8, #0x1
   185a8:	cmp	x8, x20
   185ac:	lsl	x14, x11, #40
   185b0:	and	x14, x14, #0xff000000000000
   185b4:	lsr	x13, x11, #16
   185b8:	bfi	x14, x11, #56, #8
   185bc:	lsr	x12, x11, #24
   185c0:	bfi	x14, x13, #40, #8
   185c4:	lsr	x13, x11, #8
   185c8:	and	x13, x13, #0xff000000
   185cc:	bfi	x14, x12, #32, #8
   185d0:	orr	x13, x14, x13
   185d4:	lsr	x14, x11, #40
   185d8:	and	x12, x12, #0xff0000
   185dc:	and	x14, x14, #0xff00
   185e0:	orr	x12, x13, x12
   185e4:	orr	x12, x12, x14
   185e8:	add	x11, x12, x11, lsr #56
   185ec:	str	x11, [x10], #8
   185f0:	b.lt	185a0 <__gmpz_export@@Base+0x190>  // b.tstop
   185f4:	b	186c0 <__gmpz_export@@Base+0x2b0>
   185f8:	cmp	w23, #0x1
   185fc:	b.ne	18694 <__gmpz_export@@Base+0x284>  // b.any
   18600:	cmp	w11, #0x1
   18604:	b.ne	18694 <__gmpz_export@@Base+0x284>  // b.any
   18608:	cmp	x20, #0x1
   1860c:	b.lt	186c0 <__gmpz_export@@Base+0x2b0>  // b.tstop
   18610:	cmp	x20, #0x1
   18614:	add	x11, x21, x20, lsl #3
   18618:	b.eq	18630 <__gmpz_export@@Base+0x220>  // b.none
   1861c:	cmp	x19, x11
   18620:	b.cs	189f8 <__gmpz_export@@Base+0x5e8>  // b.hs, b.nlast
   18624:	add	x8, x19, x20, lsl #3
   18628:	cmp	x8, x21
   1862c:	b.ls	189f8 <__gmpz_export@@Base+0x5e8>  // b.plast
   18630:	mov	x8, xzr
   18634:	mov	x9, x19
   18638:	sub	x10, x11, #0x8
   1863c:	ldr	x11, [x10], #-8
   18640:	add	x8, x8, #0x1
   18644:	cmp	x8, x20
   18648:	lsl	x14, x11, #40
   1864c:	and	x14, x14, #0xff000000000000
   18650:	lsr	x13, x11, #16
   18654:	bfi	x14, x11, #56, #8
   18658:	lsr	x12, x11, #24
   1865c:	bfi	x14, x13, #40, #8
   18660:	lsr	x13, x11, #8
   18664:	and	x13, x13, #0xff000000
   18668:	bfi	x14, x12, #32, #8
   1866c:	orr	x13, x14, x13
   18670:	lsr	x14, x11, #40
   18674:	and	x12, x12, #0xff0000
   18678:	and	x14, x14, #0xff00
   1867c:	orr	x12, x13, x12
   18680:	orr	x12, x12, x14
   18684:	add	x11, x12, x11, lsr #56
   18688:	str	x11, [x9], #8
   1868c:	b.lt	1863c <__gmpz_export@@Base+0x22c>  // b.tstop
   18690:	b	186c0 <__gmpz_export@@Base+0x2b0>
   18694:	sub	x8, x20, #0x1
   18698:	cmp	w23, #0x0
   1869c:	mul	x8, x8, x22
   186a0:	sub	x12, x22, #0x1
   186a4:	cneg	x9, x22, ge  // ge = tcont
   186a8:	csel	x10, x8, xzr, ge  // ge = tcont
   186ac:	cmp	w11, #0x0
   186b0:	cneg	x13, x22, lt  // lt = tstop
   186b4:	csel	x12, x12, xzr, ge  // ge = tcont
   186b8:	cmp	x27, x28
   186bc:	b.ls	186e4 <__gmpz_export@@Base+0x2d4>  // b.plast
   186c0:	mov	x0, x19
   186c4:	ldp	x20, x19, [sp, #96]
   186c8:	ldp	x22, x21, [sp, #80]
   186cc:	ldp	x24, x23, [sp, #64]
   186d0:	ldp	x26, x25, [sp, #48]
   186d4:	ldp	x28, x27, [sp, #32]
   186d8:	ldp	x29, x30, [sp, #16]
   186dc:	add	sp, sp, #0x70
   186e0:	ret
   186e4:	mov	x14, xzr
   186e8:	and	w8, w27, #0x7
   186ec:	mov	x16, #0xffffffffffffffff    	// #-1
   186f0:	add	x9, x13, x9
   186f4:	add	x13, x19, x10
   186f8:	sub	x11, x14, w11, sxtw
   186fc:	mov	w14, #0x40                  	// #64
   18700:	lsl	x16, x16, x8
   18704:	lsr	x15, x27, #3
   18708:	add	x10, x21, x26, lsl #3
   1870c:	add	x12, x13, x12
   18710:	mvn	x13, x16
   18714:	sub	w14, w14, w8
   18718:	cbz	x15, 1884c <__gmpz_export@@Base+0x43c>
   1871c:	mov	w16, wzr
   18720:	mov	x17, xzr
   18724:	mov	x0, xzr
   18728:	add	x18, x15, #0x1
   1872c:	mov	w1, #0x8                   	// #8
   18730:	b	18744 <__gmpz_export@@Base+0x334>
   18734:	add	x17, x17, #0x1
   18738:	cmp	x17, x20
   1873c:	add	x12, x12, x9
   18740:	b.cs	186c0 <__gmpz_export@@Base+0x2b0>  // b.hs, b.nlast
   18744:	mov	x2, x15
   18748:	b	18768 <__gmpz_export@@Base+0x358>
   1874c:	strb	w0, [x12]
   18750:	lsr	x0, x0, #8
   18754:	mov	w3, #0xfffffff8            	// #-8
   18758:	add	w16, w16, w3
   1875c:	subs	x2, x2, #0x1
   18760:	add	x12, x12, x11
   18764:	b.eq	187b0 <__gmpz_export@@Base+0x3a0>  // b.none
   18768:	cmp	w16, #0x7
   1876c:	b.gt	1874c <__gmpz_export@@Base+0x33c>
   18770:	cmp	x21, x10
   18774:	b.eq	18780 <__gmpz_export@@Base+0x370>  // b.none
   18778:	ldr	x3, [x21], #8
   1877c:	b	18788 <__gmpz_export@@Base+0x378>
   18780:	mov	x3, xzr
   18784:	mov	x21, x10
   18788:	lsl	x4, x3, x16
   1878c:	sub	w5, w1, w16
   18790:	orr	w4, w4, w0
   18794:	lsr	x0, x3, x5
   18798:	strb	w4, [x12]
   1879c:	mov	w3, #0x38                  	// #56
   187a0:	add	w16, w16, w3
   187a4:	subs	x2, x2, #0x1
   187a8:	add	x12, x12, x11
   187ac:	b.ne	18768 <__gmpz_export@@Base+0x358>  // b.any
   187b0:	cbz	w8, 187cc <__gmpz_export@@Base+0x3bc>
   187b4:	cmp	w16, w8
   187b8:	b.ge	187dc <__gmpz_export@@Base+0x3cc>  // b.tcont
   187bc:	cmp	x21, x10
   187c0:	b.eq	18800 <__gmpz_export@@Base+0x3f0>  // b.none
   187c4:	ldr	x2, [x21], #8
   187c8:	b	18808 <__gmpz_export@@Base+0x3f8>
   187cc:	mov	x2, x15
   187d0:	cmp	x2, x22
   187d4:	b.cs	18734 <__gmpz_export@@Base+0x324>  // b.hs, b.nlast
   187d8:	b	18834 <__gmpz_export@@Base+0x424>
   187dc:	and	w2, w0, w13
   187e0:	lsr	x0, x0, x8
   187e4:	strb	w2, [x12]
   187e8:	sub	w16, w16, w8
   187ec:	add	x12, x12, x11
   187f0:	mov	x2, x18
   187f4:	cmp	x2, x22
   187f8:	b.cs	18734 <__gmpz_export@@Base+0x324>  // b.hs, b.nlast
   187fc:	b	18834 <__gmpz_export@@Base+0x424>
   18800:	mov	x2, xzr
   18804:	mov	x21, x10
   18808:	lsl	x3, x2, x16
   1880c:	sub	w4, w8, w16
   18810:	orr	w3, w3, w0
   18814:	lsr	x0, x2, x4
   18818:	and	w2, w3, w13
   1881c:	add	w16, w14, w16
   18820:	strb	w2, [x12]
   18824:	add	x12, x12, x11
   18828:	mov	x2, x18
   1882c:	cmp	x2, x22
   18830:	b.cs	18734 <__gmpz_export@@Base+0x324>  // b.hs, b.nlast
   18834:	sub	x2, x22, x2
   18838:	strb	wzr, [x12]
   1883c:	subs	x2, x2, #0x1
   18840:	add	x12, x12, x11
   18844:	b.ne	18838 <__gmpz_export@@Base+0x428>  // b.any
   18848:	b	18734 <__gmpz_export@@Base+0x324>
   1884c:	mov	x16, xzr
   18850:	mov	x17, xzr
   18854:	b	18868 <__gmpz_export@@Base+0x458>
   18858:	add	x16, x16, #0x1
   1885c:	cmp	x16, x20
   18860:	add	x12, x12, x9
   18864:	b.cs	186c0 <__gmpz_export@@Base+0x2b0>  // b.hs, b.nlast
   18868:	cbz	w8, 18884 <__gmpz_export@@Base+0x474>
   1886c:	cmp	w15, w8
   18870:	b.ge	18894 <__gmpz_export@@Base+0x484>  // b.tcont
   18874:	cmp	x21, x10
   18878:	b.eq	188b8 <__gmpz_export@@Base+0x4a8>  // b.none
   1887c:	ldr	x18, [x21], #8
   18880:	b	188c0 <__gmpz_export@@Base+0x4b0>
   18884:	mov	x18, xzr
   18888:	cmp	x18, x22
   1888c:	b.cs	18858 <__gmpz_export@@Base+0x448>  // b.hs, b.nlast
   18890:	b	188ec <__gmpz_export@@Base+0x4dc>
   18894:	and	w18, w17, w13
   18898:	lsr	x17, x17, x8
   1889c:	strb	w18, [x12]
   188a0:	sub	w15, w15, w8
   188a4:	add	x12, x12, x11
   188a8:	mov	w18, #0x1                   	// #1
   188ac:	cmp	x18, x22
   188b0:	b.cs	18858 <__gmpz_export@@Base+0x448>  // b.hs, b.nlast
   188b4:	b	188ec <__gmpz_export@@Base+0x4dc>
   188b8:	mov	x18, xzr
   188bc:	mov	x21, x10
   188c0:	lsl	x0, x18, x15
   188c4:	sub	w1, w8, w15
   188c8:	orr	w0, w0, w17
   188cc:	lsr	x17, x18, x1
   188d0:	and	w18, w0, w13
   188d4:	add	w15, w14, w15
   188d8:	strb	w18, [x12]
   188dc:	add	x12, x12, x11
   188e0:	mov	w18, #0x1                   	// #1
   188e4:	cmp	x18, x22
   188e8:	b.cs	18858 <__gmpz_export@@Base+0x448>  // b.hs, b.nlast
   188ec:	sub	x18, x22, x18
   188f0:	strb	wzr, [x12]
   188f4:	subs	x18, x18, #0x1
   188f8:	add	x12, x12, x11
   188fc:	b.ne	188f0 <__gmpz_export@@Base+0x4e0>  // b.any
   18900:	b	18858 <__gmpz_export@@Base+0x448>
   18904:	and	x8, x20, #0xfffffffffffffffc
   18908:	lsl	x9, x8, #3
   1890c:	mov	x11, xzr
   18910:	sub	x12, x10, #0x8
   18914:	sub	x10, x10, x9
   18918:	add	x9, x19, x9
   1891c:	add	x13, x19, #0x10
   18920:	sub	x14, x12, x11, lsl #3
   18924:	ldur	q0, [x14, #-8]
   18928:	ldur	q1, [x14, #-24]
   1892c:	add	x11, x11, #0x4
   18930:	cmp	x11, x8
   18934:	ext	v0.16b, v0.16b, v0.16b, #8
   18938:	ext	v1.16b, v1.16b, v1.16b, #8
   1893c:	stp	q0, q1, [x13, #-16]
   18940:	add	x13, x13, #0x20
   18944:	b.ne	18920 <__gmpz_export@@Base+0x510>  // b.any
   18948:	cmp	x20, x8
   1894c:	b.ne	18528 <__gmpz_export@@Base+0x118>  // b.any
   18950:	b	186c0 <__gmpz_export@@Base+0x2b0>
   18954:	and	x8, x20, #0xfffffffffffffffe
   18958:	lsl	x10, x8, #3
   1895c:	mov	x11, xzr
   18960:	mov	x12, xzr
   18964:	movi	v0.2d, #0xff000000000000
   18968:	movi	v1.2d, #0xff0000000000
   1896c:	movi	v2.2d, #0xff00000000
   18970:	movi	v3.2d, #0xff000000
   18974:	movi	v4.2d, #0xff0000
   18978:	add	x9, x21, x10
   1897c:	add	x10, x19, x10
   18980:	movi	v5.2d, #0xff00
   18984:	ldr	q6, [x21, x11]
   18988:	add	x12, x12, #0x2
   1898c:	cmp	x12, x8
   18990:	shl	v16.2d, v6.2d, #40
   18994:	shl	v7.2d, v6.2d, #56
   18998:	and	v16.16b, v16.16b, v0.16b
   1899c:	orr	v7.16b, v16.16b, v7.16b
   189a0:	shl	v16.2d, v6.2d, #24
   189a4:	and	v16.16b, v16.16b, v1.16b
   189a8:	orr	v7.16b, v7.16b, v16.16b
   189ac:	shl	v16.2d, v6.2d, #8
   189b0:	and	v16.16b, v16.16b, v2.16b
   189b4:	orr	v7.16b, v7.16b, v16.16b
   189b8:	ushr	v16.2d, v6.2d, #8
   189bc:	and	v16.16b, v16.16b, v3.16b
   189c0:	orr	v7.16b, v7.16b, v16.16b
   189c4:	ushr	v16.2d, v6.2d, #24
   189c8:	and	v16.16b, v16.16b, v4.16b
   189cc:	orr	v7.16b, v7.16b, v16.16b
   189d0:	ushr	v16.2d, v6.2d, #40
   189d4:	and	v16.16b, v16.16b, v5.16b
   189d8:	orr	v7.16b, v7.16b, v16.16b
   189dc:	usra	v7.2d, v6.2d, #56
   189e0:	str	q7, [x19, x11]
   189e4:	add	x11, x11, #0x10
   189e8:	b.ne	18984 <__gmpz_export@@Base+0x574>  // b.any
   189ec:	cmp	x20, x8
   189f0:	b.ne	185a0 <__gmpz_export@@Base+0x190>  // b.any
   189f4:	b	186c0 <__gmpz_export@@Base+0x2b0>
   189f8:	and	x8, x20, #0xfffffffffffffffe
   189fc:	lsl	x9, x8, #3
   18a00:	mov	x10, xzr
   18a04:	sub	x12, x11, #0x10
   18a08:	movi	v0.2d, #0xff000000000000
   18a0c:	movi	v1.2d, #0xff0000000000
   18a10:	movi	v2.2d, #0xff00000000
   18a14:	movi	v3.2d, #0xff000000
   18a18:	movi	v4.2d, #0xff0000
   18a1c:	sub	x11, x11, x9
   18a20:	add	x9, x19, x9
   18a24:	movi	v5.2d, #0xff00
   18a28:	mov	x13, x19
   18a2c:	sub	x14, x12, x10, lsl #3
   18a30:	ldr	q6, [x14]
   18a34:	add	x10, x10, #0x2
   18a38:	cmp	x10, x8
   18a3c:	ext	v6.16b, v6.16b, v6.16b, #8
   18a40:	shl	v16.2d, v6.2d, #40
   18a44:	shl	v7.2d, v6.2d, #56
   18a48:	and	v16.16b, v16.16b, v0.16b
   18a4c:	orr	v7.16b, v16.16b, v7.16b
   18a50:	shl	v16.2d, v6.2d, #24
   18a54:	and	v16.16b, v16.16b, v1.16b
   18a58:	orr	v7.16b, v7.16b, v16.16b
   18a5c:	shl	v16.2d, v6.2d, #8
   18a60:	and	v16.16b, v16.16b, v2.16b
   18a64:	orr	v7.16b, v7.16b, v16.16b
   18a68:	ushr	v16.2d, v6.2d, #8
   18a6c:	and	v16.16b, v16.16b, v3.16b
   18a70:	orr	v7.16b, v7.16b, v16.16b
   18a74:	ushr	v16.2d, v6.2d, #24
   18a78:	and	v16.16b, v16.16b, v4.16b
   18a7c:	orr	v7.16b, v7.16b, v16.16b
   18a80:	ushr	v16.2d, v6.2d, #40
   18a84:	and	v16.16b, v16.16b, v5.16b
   18a88:	orr	v7.16b, v7.16b, v16.16b
   18a8c:	usra	v7.2d, v6.2d, #56
   18a90:	str	q7, [x13], #16
   18a94:	b.ne	18a2c <__gmpz_export@@Base+0x61c>  // b.any
   18a98:	cmp	x20, x8
   18a9c:	b.ne	18638 <__gmpz_export@@Base+0x228>  // b.any
   18aa0:	b	186c0 <__gmpz_export@@Base+0x2b0>

0000000000018aa4 <__gmpz_mfac_uiui@@Base>:
   18aa4:	stp	x29, x30, [sp, #-64]!
   18aa8:	str	x23, [sp, #16]
   18aac:	stp	x22, x21, [sp, #32]
   18ab0:	stp	x20, x19, [sp, #48]
   18ab4:	mov	x29, sp
   18ab8:	sub	sp, sp, #0x20
   18abc:	mov	x20, x1
   18ac0:	subs	x8, x1, #0x3
   18ac4:	mov	x19, x0
   18ac8:	b.cc	18b38 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   18acc:	sub	x9, x2, #0x1
   18ad0:	mov	x22, x2
   18ad4:	cmp	x8, x9
   18ad8:	b.cc	18b38 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   18adc:	add	x0, x29, #0x18
   18ae0:	mov	w1, #0x1                   	// #1
   18ae4:	mov	x2, x22
   18ae8:	str	x20, [x29, #24]
   18aec:	bl	bf90 <__gmpn_gcd_1@plt>
   18af0:	mov	x21, x0
   18af4:	cmp	x0, #0x2
   18af8:	b.cc	18b04 <__gmpz_mfac_uiui@@Base+0x60>  // b.lo, b.ul, b.last
   18afc:	udiv	x20, x20, x21
   18b00:	udiv	x22, x22, x21
   18b04:	cmp	x22, #0x2
   18b08:	b.hi	18b60 <__gmpz_mfac_uiui@@Base+0xbc>  // b.pmore
   18b0c:	cmp	x22, #0x1
   18b10:	b.ne	18bc0 <__gmpz_mfac_uiui@@Base+0x11c>  // b.any
   18b14:	cmp	x21, #0x3
   18b18:	b.cc	18cb4 <__gmpz_mfac_uiui@@Base+0x210>  // b.lo, b.ul, b.last
   18b1c:	sub	x0, x29, #0x10
   18b20:	bl	d250 <__gmpz_init@plt>
   18b24:	sub	x0, x29, #0x10
   18b28:	mov	x1, x20
   18b2c:	bl	c450 <__gmpz_fac_ui@plt>
   18b30:	str	x20, [x29, #24]
   18b34:	b	18d80 <__gmpz_mfac_uiui@@Base+0x2dc>
   18b38:	ldr	w8, [x19]
   18b3c:	cmp	x20, #0x0
   18b40:	cinc	x20, x20, eq  // eq = none
   18b44:	cmp	w8, #0x0
   18b48:	b.le	18cec <__gmpz_mfac_uiui@@Base+0x248>
   18b4c:	ldr	x0, [x19, #8]
   18b50:	mov	w8, #0x1                   	// #1
   18b54:	str	x20, [x0]
   18b58:	str	w8, [x19, #4]
   18b5c:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>
   18b60:	udiv	x8, x20, x22
   18b64:	cmp	x21, #0x2
   18b68:	add	x9, x8, #0x1
   18b6c:	sub	x8, x20, x22
   18b70:	str	x9, [x29, #24]
   18b74:	b.cc	18bec <__gmpz_mfac_uiui@@Base+0x148>  // b.lo, b.ul, b.last
   18b78:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   18b7c:	ldr	x10, [x10, #3880]
   18b80:	mov	w11, #0x9                   	// #9
   18b84:	sub	w12, w11, #0x2
   18b88:	ldr	x12, [x10, w12, uxtw #3]
   18b8c:	sub	w11, w11, #0x1
   18b90:	cmp	x12, x8
   18b94:	b.cc	18b84 <__gmpz_mfac_uiui@@Base+0xe0>  // b.lo, b.ul, b.last
   18b98:	ldrsw	x12, [x19]
   18b9c:	mov	w11, w11
   18ba0:	udiv	x11, x9, x11
   18ba4:	add	x11, x11, #0x2
   18ba8:	cmp	x11, x12
   18bac:	b.hi	18cfc <__gmpz_mfac_uiui@@Base+0x258>  // b.pmore
   18bb0:	ldr	x23, [x19, #8]
   18bb4:	cmp	x8, x22
   18bb8:	b.hi	18c70 <__gmpz_mfac_uiui@@Base+0x1cc>  // b.pmore
   18bbc:	b	18d4c <__gmpz_mfac_uiui@@Base+0x2a8>
   18bc0:	cmp	x21, #0x2
   18bc4:	b.cc	18ccc <__gmpz_mfac_uiui@@Base+0x228>  // b.lo, b.ul, b.last
   18bc8:	sub	x0, x29, #0x10
   18bcc:	bl	d250 <__gmpz_init@plt>
   18bd0:	sub	x0, x29, #0x10
   18bd4:	mov	x1, x20
   18bd8:	bl	c600 <__gmpz_2fac_ui@plt>
   18bdc:	lsr	x8, x20, #1
   18be0:	add	x8, x8, #0x1
   18be4:	str	x8, [x29, #24]
   18be8:	b	18d80 <__gmpz_mfac_uiui@@Base+0x2dc>
   18bec:	stur	xzr, [x29, #-32]
   18bf0:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   18bf4:	ldr	x10, [x10, #3880]
   18bf8:	mov	w11, #0x9                   	// #9
   18bfc:	sub	w12, w11, #0x2
   18c00:	ldr	x12, [x10, w12, uxtw #3]
   18c04:	sub	w11, w11, #0x1
   18c08:	cmp	x12, x8
   18c0c:	b.cc	18bfc <__gmpz_mfac_uiui@@Base+0x158>  // b.lo, b.ul, b.last
   18c10:	mov	w11, w11
   18c14:	udiv	x11, x9, x11
   18c18:	lsl	x11, x11, #3
   18c1c:	add	x11, x11, #0x10
   18c20:	mov	w12, #0x9                   	// #9
   18c24:	sub	w13, w12, #0x2
   18c28:	ldr	x13, [x10, w13, uxtw #3]
   18c2c:	sub	w12, w12, #0x1
   18c30:	cmp	x13, x8
   18c34:	b.cc	18c24 <__gmpz_mfac_uiui@@Base+0x180>  // b.lo, b.ul, b.last
   18c38:	mov	w10, w12
   18c3c:	udiv	x9, x9, x10
   18c40:	mov	w12, #0x7f00                	// #32512
   18c44:	lsl	x9, x9, #3
   18c48:	cmp	x11, x12
   18c4c:	add	x1, x9, #0x10
   18c50:	b.hi	18d30 <__gmpz_mfac_uiui@@Base+0x28c>  // b.pmore
   18c54:	add	x10, x1, #0xf
   18c58:	mov	x9, sp
   18c5c:	and	x10, x10, #0xfffffffffffffff0
   18c60:	sub	x23, x9, x10
   18c64:	mov	sp, x23
   18c68:	cmp	x8, x22
   18c6c:	b.ls	18d4c <__gmpz_mfac_uiui@@Base+0x2a8>  // b.plast
   18c70:	mov	x10, xzr
   18c74:	mov	x9, x8
   18c78:	b	18c8c <__gmpz_mfac_uiui@@Base+0x1e8>
   18c7c:	mul	x20, x9, x20
   18c80:	sub	x9, x9, x22
   18c84:	cmp	x9, x22
   18c88:	b.ls	18d54 <__gmpz_mfac_uiui@@Base+0x2b0>  // b.plast
   18c8c:	umulh	x11, x8, x20
   18c90:	cbz	x11, 18c7c <__gmpz_mfac_uiui@@Base+0x1d8>
   18c94:	add	x11, x10, #0x1
   18c98:	str	x20, [x23, x10, lsl #3]
   18c9c:	mov	x20, x9
   18ca0:	mov	x10, x11
   18ca4:	sub	x9, x9, x22
   18ca8:	cmp	x9, x22
   18cac:	b.hi	18c8c <__gmpz_mfac_uiui@@Base+0x1e8>  // b.pmore
   18cb0:	b	18d54 <__gmpz_mfac_uiui@@Base+0x2b0>
   18cb4:	cmp	x21, #0x2
   18cb8:	b.ne	18cdc <__gmpz_mfac_uiui@@Base+0x238>  // b.any
   18cbc:	lsl	x1, x20, #1
   18cc0:	mov	x0, x19
   18cc4:	bl	c600 <__gmpz_2fac_ui@plt>
   18cc8:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>
   18ccc:	mov	x0, x19
   18cd0:	mov	x1, x20
   18cd4:	bl	c600 <__gmpz_2fac_ui@plt>
   18cd8:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>
   18cdc:	mov	x0, x19
   18ce0:	mov	x1, x20
   18ce4:	bl	c450 <__gmpz_fac_ui@plt>
   18ce8:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>
   18cec:	mov	w1, #0x1                   	// #1
   18cf0:	mov	x0, x19
   18cf4:	bl	c080 <__gmpz_realloc@plt>
   18cf8:	b	18b50 <__gmpz_mfac_uiui@@Base+0xac>
   18cfc:	mov	w11, #0x9                   	// #9
   18d00:	sub	w12, w11, #0x2
   18d04:	ldr	x12, [x10, w12, uxtw #3]
   18d08:	sub	w11, w11, #0x1
   18d0c:	cmp	x12, x8
   18d10:	b.cc	18d00 <__gmpz_mfac_uiui@@Base+0x25c>  // b.lo, b.ul, b.last
   18d14:	mov	w10, w11
   18d18:	udiv	x9, x9, x10
   18d1c:	add	x1, x9, #0x2
   18d20:	mov	x0, x19
   18d24:	mov	x23, x8
   18d28:	bl	c080 <__gmpz_realloc@plt>
   18d2c:	b	18d3c <__gmpz_mfac_uiui@@Base+0x298>
   18d30:	sub	x0, x29, #0x20
   18d34:	mov	x23, x8
   18d38:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   18d3c:	mov	x8, x23
   18d40:	mov	x23, x0
   18d44:	cmp	x8, x22
   18d48:	b.hi	18c70 <__gmpz_mfac_uiui@@Base+0x1cc>  // b.pmore
   18d4c:	mov	x10, xzr
   18d50:	mov	x9, x8
   18d54:	add	x8, x23, x10, lsl #3
   18d58:	add	x22, x10, #0x2
   18d5c:	cmp	x21, #0x2
   18d60:	stp	x9, x20, [x8]
   18d64:	b.cc	18dbc <__gmpz_mfac_uiui@@Base+0x318>  // b.lo, b.ul, b.last
   18d68:	sub	x0, x29, #0x10
   18d6c:	bl	d250 <__gmpz_init@plt>
   18d70:	sub	x0, x29, #0x10
   18d74:	mov	x1, x23
   18d78:	mov	x2, x22
   18d7c:	bl	cd70 <__gmpz_prodlimbs@plt>
   18d80:	sub	x0, x29, #0x20
   18d84:	bl	d250 <__gmpz_init@plt>
   18d88:	ldr	x2, [x29, #24]
   18d8c:	sub	x0, x29, #0x20
   18d90:	mov	x1, x21
   18d94:	bl	ca10 <__gmpz_ui_pow_ui@plt>
   18d98:	sub	x1, x29, #0x20
   18d9c:	sub	x2, x29, #0x10
   18da0:	mov	x0, x19
   18da4:	bl	c4b0 <__gmpz_mul@plt>
   18da8:	sub	x0, x29, #0x20
   18dac:	bl	cb50 <__gmpz_clear@plt>
   18db0:	sub	x0, x29, #0x10
   18db4:	bl	cb50 <__gmpz_clear@plt>
   18db8:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>
   18dbc:	mov	x0, x19
   18dc0:	mov	x1, x23
   18dc4:	mov	x2, x22
   18dc8:	bl	cd70 <__gmpz_prodlimbs@plt>
   18dcc:	ldur	x0, [x29, #-32]
   18dd0:	cbnz	x0, 18dec <__gmpz_mfac_uiui@@Base+0x348>
   18dd4:	mov	sp, x29
   18dd8:	ldp	x20, x19, [sp, #48]
   18ddc:	ldp	x22, x21, [sp, #32]
   18de0:	ldr	x23, [sp, #16]
   18de4:	ldp	x29, x30, [sp], #64
   18de8:	ret
   18dec:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   18df0:	b	18dd4 <__gmpz_mfac_uiui@@Base+0x330>

0000000000018df4 <__gmpz_2fac_ui@@Base>:
   18df4:	stp	x29, x30, [sp, #-32]!
   18df8:	stp	x20, x19, [sp, #16]
   18dfc:	mov	x19, x0
   18e00:	mov	x29, sp
   18e04:	tbnz	w1, #0, 18e2c <__gmpz_2fac_ui@@Base+0x38>
   18e08:	sub	x8, x1, #0x1
   18e0c:	cmp	x8, #0x50
   18e10:	lsr	x8, x1, #1
   18e14:	b.hi	18e74 <__gmpz_2fac_ui@@Base+0x80>  // b.pmore
   18e18:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   18e1c:	ldr	x9, [x9, #3992]
   18e20:	add	x9, x8, x9
   18e24:	ldurb	w20, [x9, #-1]
   18e28:	b	18ea8 <__gmpz_2fac_ui@@Base+0xb4>
   18e2c:	cmp	x1, #0x21
   18e30:	b.hi	18ed4 <__gmpz_2fac_ui@@Base+0xe0>  // b.pmore
   18e34:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   18e38:	ldr	w9, [x19]
   18e3c:	ldr	x10, [x10, #3928]
   18e40:	lsl	x8, x1, #2
   18e44:	and	x8, x8, #0xfffffffffffffff8
   18e48:	cmp	w9, #0x0
   18e4c:	ldr	x20, [x10, x8]
   18e50:	b.le	18fb8 <__gmpz_2fac_ui@@Base+0x1c4>
   18e54:	ldr	x0, [x19, #8]
   18e58:	mov	w8, #0x1                   	// #1
   18e5c:	str	x20, [x0]
   18e60:	str	w8, [x19, #4]
   18e64:	mov	sp, x29
   18e68:	ldp	x20, x19, [sp, #16]
   18e6c:	ldp	x29, x30, [sp], #32
   18e70:	ret
   18e74:	and	x9, x8, #0x5555555555555555
   18e78:	sub	x9, x1, x9
   18e7c:	lsr	x10, x9, #2
   18e80:	and	x10, x10, #0x3333333333333333
   18e84:	and	x9, x9, #0x3333333333333333
   18e88:	add	x9, x10, x9
   18e8c:	add	x9, x9, x9, lsr #4
   18e90:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   18e94:	add	x9, x9, x9, lsr #8
   18e98:	add	x9, x9, x9, lsr #16
   18e9c:	lsr	x10, x9, #32
   18ea0:	add	w9, w10, w9
   18ea4:	sub	x20, x1, w9, uxtb
   18ea8:	mov	x0, x19
   18eac:	mov	x1, x8
   18eb0:	mov	w2, wzr
   18eb4:	bl	c3d0 <__gmpz_oddfac_1@plt>
   18eb8:	mov	x0, x19
   18ebc:	mov	x1, x19
   18ec0:	mov	x2, x20
   18ec4:	mov	sp, x29
   18ec8:	ldp	x20, x19, [sp, #16]
   18ecc:	ldp	x29, x30, [sp], #32
   18ed0:	b	c6e0 <__gmpz_mul_2exp@plt>
   18ed4:	cmp	x1, #0x1d7
   18ed8:	b.ls	18ef4 <__gmpz_2fac_ui@@Base+0x100>  // b.plast
   18edc:	mov	w2, #0x1                   	// #1
   18ee0:	mov	x0, x19
   18ee4:	mov	sp, x29
   18ee8:	ldp	x20, x19, [sp, #16]
   18eec:	ldp	x29, x30, [sp], #32
   18ef0:	b	c3d0 <__gmpz_oddfac_1@plt>
   18ef4:	mov	w9, #0xaaab                	// #43691
   18ef8:	and	w8, w1, #0xffff
   18efc:	movk	w9, #0xaaaa, lsl #16
   18f00:	umull	x8, w8, w9
   18f04:	lsr	x8, x8, #32
   18f08:	and	w8, w8, #0xfff8
   18f0c:	add	w8, w8, #0x8
   18f10:	and	w8, w8, #0xfff8
   18f14:	add	w8, w8, #0xf
   18f18:	and	x8, x8, #0x1fff0
   18f1c:	mov	x9, sp
   18f20:	sub	x8, x9, x8
   18f24:	mov	sp, x8
   18f28:	mov	x9, #0xd941                	// #55617
   18f2c:	movk	x9, #0xc030, lsl #16
   18f30:	movk	x9, #0x2099, lsl #32
   18f34:	movk	x9, #0x57e2, lsl #48
   18f38:	sub	x10, x1, #0x2
   18f3c:	cmp	x10, #0x22
   18f40:	str	x9, [x8]
   18f44:	mov	w9, #0x1                   	// #1
   18f48:	b.cc	18f94 <__gmpz_2fac_ui@@Base+0x1a0>  // b.lo, b.ul, b.last
   18f4c:	mov	x11, #0x3869                	// #14441
   18f50:	movk	x11, #0xfba9, lsl #16
   18f54:	movk	x11, #0xd8f2, lsl #32
   18f58:	movk	x11, #0x8a, lsl #48
   18f5c:	b	18f70 <__gmpz_2fac_ui@@Base+0x17c>
   18f60:	mul	x1, x10, x1
   18f64:	sub	x10, x10, #0x2
   18f68:	cmp	x10, #0x21
   18f6c:	b.ls	18f94 <__gmpz_2fac_ui@@Base+0x1a0>  // b.plast
   18f70:	cmp	x1, x11
   18f74:	b.cc	18f60 <__gmpz_2fac_ui@@Base+0x16c>  // b.lo, b.ul, b.last
   18f78:	add	x12, x9, #0x1
   18f7c:	str	x1, [x8, x9, lsl #3]
   18f80:	mov	x1, x10
   18f84:	mov	x9, x12
   18f88:	sub	x10, x10, #0x2
   18f8c:	cmp	x10, #0x21
   18f90:	b.hi	18f70 <__gmpz_2fac_ui@@Base+0x17c>  // b.pmore
   18f94:	add	x2, x9, #0x1
   18f98:	str	x1, [x8, x9, lsl #3]
   18f9c:	mov	x0, x19
   18fa0:	mov	x1, x8
   18fa4:	bl	cd70 <__gmpz_prodlimbs@plt>
   18fa8:	mov	sp, x29
   18fac:	ldp	x20, x19, [sp, #16]
   18fb0:	ldp	x29, x30, [sp], #32
   18fb4:	ret
   18fb8:	mov	w1, #0x1                   	// #1
   18fbc:	mov	x0, x19
   18fc0:	bl	c080 <__gmpz_realloc@plt>
   18fc4:	b	18e58 <__gmpz_2fac_ui@@Base+0x64>

0000000000018fc8 <__gmpz_fac_ui@@Base>:
   18fc8:	stp	x29, x30, [sp, #-32]!
   18fcc:	stp	x20, x19, [sp, #16]
   18fd0:	mov	x20, x1
   18fd4:	cmp	x1, #0x14
   18fd8:	mov	x19, x0
   18fdc:	mov	x29, sp
   18fe0:	b.hi	1901c <__gmpz_fac_ui@@Base+0x54>  // b.pmore
   18fe4:	adrp	x9, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   18fe8:	ldr	w8, [x19]
   18fec:	add	x9, x9, #0x48
   18ff0:	ldr	x20, [x9, x20, lsl #3]
   18ff4:	cmp	w8, #0x0
   18ff8:	b.le	1916c <__gmpz_fac_ui@@Base+0x1a4>
   18ffc:	ldr	x0, [x19, #8]
   19000:	mov	w8, #0x1                   	// #1
   19004:	str	x20, [x0]
   19008:	str	w8, [x19, #4]
   1900c:	mov	sp, x29
   19010:	ldp	x20, x19, [sp, #16]
   19014:	ldp	x29, x30, [sp], #32
   19018:	ret
   1901c:	cmp	x20, #0x17
   19020:	b.ls	19054 <__gmpz_fac_ui@@Base+0x8c>  // b.plast
   19024:	mov	x0, x19
   19028:	mov	x1, x20
   1902c:	mov	w2, wzr
   19030:	bl	c3d0 <__gmpz_oddfac_1@plt>
   19034:	cmp	x20, #0x51
   19038:	lsr	x8, x20, #1
   1903c:	b.hi	190f8 <__gmpz_fac_ui@@Base+0x130>  // b.pmore
   19040:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   19044:	ldr	x9, [x9, #3992]
   19048:	add	x8, x8, x9
   1904c:	ldurb	w2, [x8, #-1]
   19050:	b	1912c <__gmpz_fac_ui@@Base+0x164>
   19054:	sub	w8, w20, #0x15
   19058:	mov	w9, #0xcccd                	// #52429
   1905c:	and	w8, w8, #0xff
   19060:	movk	w9, #0xcccc, lsl #16
   19064:	umull	x8, w8, w9
   19068:	lsr	x8, x8, #32
   1906c:	and	w8, w8, #0xf8
   19070:	add	w8, w8, #0x10
   19074:	and	w8, w8, #0xf8
   19078:	add	w8, w8, #0xf
   1907c:	and	x8, x8, #0x1f0
   19080:	mov	x9, sp
   19084:	sub	x1, x9, x8
   19088:	mov	sp, x1
   1908c:	mov	x8, #0x82b40000            	// #2192834560
   19090:	movk	x8, #0x677c, lsl #32
   19094:	sub	x9, x20, #0x1
   19098:	movk	x8, #0x21c3, lsl #48
   1909c:	cmp	x9, #0x15
   190a0:	str	x8, [x1]
   190a4:	b.cc	19144 <__gmpz_fac_ui@@Base+0x17c>  // b.lo, b.ul, b.last
   190a8:	mov	x10, #0x3d71                	// #15729
   190ac:	movk	x10, #0xd70a, lsl #16
   190b0:	movk	x10, #0x70a3, lsl #32
   190b4:	mov	w8, #0x1                   	// #1
   190b8:	movk	x10, #0xa3d, lsl #48
   190bc:	b	190d0 <__gmpz_fac_ui@@Base+0x108>
   190c0:	mul	x20, x9, x20
   190c4:	sub	x9, x9, #0x1
   190c8:	cmp	x9, #0x14
   190cc:	b.ls	1914c <__gmpz_fac_ui@@Base+0x184>  // b.plast
   190d0:	cmp	x20, x10
   190d4:	b.cc	190c0 <__gmpz_fac_ui@@Base+0xf8>  // b.lo, b.ul, b.last
   190d8:	add	x11, x8, #0x1
   190dc:	str	x20, [x1, x8, lsl #3]
   190e0:	mov	x8, x11
   190e4:	mov	x20, x9
   190e8:	sub	x9, x9, #0x1
   190ec:	cmp	x9, #0x14
   190f0:	b.hi	190d0 <__gmpz_fac_ui@@Base+0x108>  // b.pmore
   190f4:	b	1914c <__gmpz_fac_ui@@Base+0x184>
   190f8:	and	x8, x8, #0x5555555555555555
   190fc:	sub	x8, x20, x8
   19100:	lsr	x9, x8, #2
   19104:	and	x9, x9, #0x3333333333333333
   19108:	and	x8, x8, #0x3333333333333333
   1910c:	add	x8, x9, x8
   19110:	add	x8, x8, x8, lsr #4
   19114:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   19118:	add	x8, x8, x8, lsr #8
   1911c:	add	x8, x8, x8, lsr #16
   19120:	lsr	x9, x8, #32
   19124:	add	w8, w9, w8
   19128:	sub	x2, x20, w8, uxtb
   1912c:	mov	x0, x19
   19130:	mov	x1, x19
   19134:	mov	sp, x29
   19138:	ldp	x20, x19, [sp, #16]
   1913c:	ldp	x29, x30, [sp], #32
   19140:	b	c6e0 <__gmpz_mul_2exp@plt>
   19144:	mov	w20, #0x15                  	// #21
   19148:	mov	w8, #0x1                   	// #1
   1914c:	add	x2, x8, #0x1
   19150:	mov	x0, x19
   19154:	str	x20, [x1, x8, lsl #3]
   19158:	bl	cd70 <__gmpz_prodlimbs@plt>
   1915c:	mov	sp, x29
   19160:	ldp	x20, x19, [sp, #16]
   19164:	ldp	x29, x30, [sp], #32
   19168:	ret
   1916c:	mov	w1, #0x1                   	// #1
   19170:	mov	x0, x19
   19174:	bl	c080 <__gmpz_realloc@plt>
   19178:	b	19000 <__gmpz_fac_ui@@Base+0x38>

000000000001917c <__gmpz_oddfac_1@@Base>:
   1917c:	stp	x29, x30, [sp, #-96]!
   19180:	stp	x28, x27, [sp, #16]
   19184:	stp	x26, x25, [sp, #32]
   19188:	stp	x24, x23, [sp, #48]
   1918c:	stp	x22, x21, [sp, #64]
   19190:	stp	x20, x19, [sp, #80]
   19194:	mov	x29, sp
   19198:	sub	sp, sp, #0x30
   1919c:	mov	x20, x1
   191a0:	cmp	x1, #0x19
   191a4:	mov	x19, x0
   191a8:	stur	w2, [x29, #-36]
   191ac:	b.hi	191dc <__gmpz_oddfac_1@@Base+0x60>  // b.pmore
   191b0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   191b4:	ldr	w8, [x19]
   191b8:	ldr	x9, [x9, #3848]
   191bc:	cmp	w8, #0x0
   191c0:	ldr	x20, [x9, x20, lsl #3]
   191c4:	b.le	197bc <__gmpz_oddfac_1@@Base+0x640>
   191c8:	ldr	x0, [x19, #8]
   191cc:	mov	w8, #0x1                   	// #1
   191d0:	str	x20, [x0]
   191d4:	str	w8, [x19, #4]
   191d8:	b	1979c <__gmpz_oddfac_1@@Base+0x620>
   191dc:	cmp	x20, #0x23
   191e0:	b.cs	19234 <__gmpz_oddfac_1@@Base+0xb8>  // b.hs, b.nlast
   191e4:	ldr	w8, [x19]
   191e8:	cmp	w8, #0x1
   191ec:	b.le	197cc <__gmpz_oddfac_1@@Base+0x650>
   191f0:	ldr	x0, [x19, #8]
   191f4:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   191f8:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   191fc:	ldr	x9, [x9, #3928]
   19200:	ldr	x10, [x10, #3848]
   19204:	lsl	x8, x20, #2
   19208:	sub	x11, x8, #0x4
   1920c:	and	x8, x8, #0xfffffffffffffff8
   19210:	and	x11, x11, #0xfffffffffffffff8
   19214:	ldr	x8, [x10, x8]
   19218:	ldr	x9, [x9, x11]
   1921c:	mov	w10, #0x2                   	// #2
   19220:	umulh	x11, x9, x8
   19224:	mul	x8, x8, x9
   19228:	stp	x8, x11, [x0]
   1922c:	str	w10, [x19, #4]
   19230:	b	1979c <__gmpz_oddfac_1@@Base+0x620>
   19234:	cmp	x20, #0xec
   19238:	b.cc	1925c <__gmpz_oddfac_1@@Base+0xe0>  // b.lo, b.ul, b.last
   1923c:	mov	w25, wzr
   19240:	mov	x8, x20
   19244:	lsr	x13, x8, #1
   19248:	cmp	x8, #0x1d7
   1924c:	add	w25, w25, #0x1
   19250:	mov	x8, x13
   19254:	b.hi	19244 <__gmpz_oddfac_1@@Base+0xc8>  // b.pmore
   19258:	b	19264 <__gmpz_oddfac_1@@Base+0xe8>
   1925c:	mov	w25, wzr
   19260:	mov	x13, x20
   19264:	mov	w9, #0x4925                	// #18725
   19268:	and	w8, w13, #0xff
   1926c:	movk	w9, #0x2492, lsl #16
   19270:	umull	x9, w8, w9
   19274:	lsr	x9, x9, #32
   19278:	sub	w8, w8, w9
   1927c:	add	w8, w9, w8, lsr #1
   19280:	lsl	w8, w8, #1
   19284:	and	w8, w8, #0x7f8
   19288:	add	w8, w8, #0x17
   1928c:	and	x8, x8, #0x7f0
   19290:	mov	x9, sp
   19294:	sub	x1, x9, x8
   19298:	mov	sp, x1
   1929c:	mov	x10, #0x70d0                	// #28880
   192a0:	mov	x12, #0xd941                	// #55617
   192a4:	movk	x10, #0xf752, lsl #16
   192a8:	movk	x12, #0xc030, lsl #16
   192ac:	movk	x10, #0xb1e5, lsl #32
   192b0:	movk	x12, #0x2099, lsl #32
   192b4:	mov	x8, xzr
   192b8:	movk	x10, #0x115, lsl #48
   192bc:	mov	w9, #0x1                   	// #1
   192c0:	movk	x12, #0x57e2, lsl #48
   192c4:	b	192d8 <__gmpz_oddfac_1@@Base+0x15c>
   192c8:	lsl	x10, x10, #1
   192cc:	cmp	x11, #0x45
   192d0:	lsr	x13, x11, #1
   192d4:	b.ls	19324 <__gmpz_oddfac_1@@Base+0x1a8>  // b.plast
   192d8:	mov	x11, x13
   192dc:	str	x12, [x1, x8, lsl #3]
   192e0:	add	x8, x8, #0x1
   192e4:	mov	w13, #0x23                  	// #35
   192e8:	b	192fc <__gmpz_oddfac_1@@Base+0x180>
   192ec:	mul	x9, x9, x13
   192f0:	add	x13, x13, #0x2
   192f4:	cmp	x13, x11
   192f8:	b.hi	192c8 <__gmpz_oddfac_1@@Base+0x14c>  // b.pmore
   192fc:	cmp	x9, x10
   19300:	b.ls	192ec <__gmpz_oddfac_1@@Base+0x170>  // b.plast
   19304:	add	x14, x8, #0x1
   19308:	str	x9, [x1, x8, lsl #3]
   1930c:	mov	x8, x14
   19310:	mov	x9, x13
   19314:	add	x13, x13, #0x2
   19318:	cmp	x13, x11
   1931c:	b.ls	192fc <__gmpz_oddfac_1@@Base+0x180>  // b.plast
   19320:	b	192c8 <__gmpz_oddfac_1@@Base+0x14c>
   19324:	lsl	x12, x13, #2
   19328:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1932c:	adrp	x14, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   19330:	ldr	x13, [x13, #3928]
   19334:	ldr	x14, [x14, #3848]
   19338:	lsl	x11, x11, #1
   1933c:	sub	x12, x12, #0x4
   19340:	and	x11, x11, #0xfffffffffffffff8
   19344:	and	x12, x12, #0xfffffffffffffff8
   19348:	ldr	x12, [x13, x12]
   1934c:	ldr	x11, [x14, x11]
   19350:	add	x10, x1, x8, lsl #3
   19354:	add	x2, x8, #0x3
   19358:	mov	x0, x19
   1935c:	stp	x9, x12, [x10]
   19360:	str	x11, [x10, #16]
   19364:	bl	cd70 <__gmpz_prodlimbs@plt>
   19368:	cbz	w25, 1979c <__gmpz_oddfac_1@@Base+0x620>
   1936c:	lsr	x8, x20, #6
   19370:	add	x21, x8, #0x4
   19374:	cmp	x8, #0xfdc
   19378:	lsl	x1, x21, #3
   1937c:	stur	xzr, [x29, #-24]
   19380:	stur	w21, [x29, #-16]
   19384:	b.hi	197dc <__gmpz_oddfac_1@@Base+0x660>  // b.pmore
   19388:	add	x9, x1, #0xf
   1938c:	mov	x8, sp
   19390:	and	x9, x9, #0x7ffffffffffffff0
   19394:	sub	x0, x8, x9
   19398:	mov	sp, x0
   1939c:	lsl	x8, x21, #2
   193a0:	and	x8, x8, #0x1ffffffffffffff8
   193a4:	add	x8, x0, x8
   193a8:	add	x22, x8, #0x8
   193ac:	stur	x0, [x29, #-8]
   193b0:	sub	x1, x20, #0x1
   193b4:	mov	x0, x22
   193b8:	bl	d200 <__gmp_primesieve@plt>
   193bc:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   193c0:	ldr	x9, [x9, #3880]
   193c4:	mov	w8, #0x9                   	// #9
   193c8:	sub	w10, w8, #0x2
   193cc:	ldr	x10, [x9, w10, uxtw #3]
   193d0:	sub	w8, w8, #0x1
   193d4:	cmp	x10, x20
   193d8:	b.cc	193c8 <__gmpz_oddfac_1@@Base+0x24c>  // b.lo, b.ul, b.last
   193dc:	add	x9, x0, #0x1
   193e0:	mov	w8, w8
   193e4:	udiv	x8, x9, x8
   193e8:	lsl	x8, x8, #3
   193ec:	add	x1, x8, #0x8
   193f0:	mov	w8, #0x7f00                	// #32512
   193f4:	cmp	x1, x8
   193f8:	b.hi	197e8 <__gmpz_oddfac_1@@Base+0x66c>  // b.pmore
   193fc:	add	x9, x1, #0xf
   19400:	mov	x8, sp
   19404:	and	x9, x9, #0xfffffffffffffff0
   19408:	sub	x23, x8, x9
   1940c:	mov	sp, x23
   19410:	mov	x21, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   19414:	mov	w3, #0x1                   	// #1
   19418:	movk	x21, #0xaaab
   1941c:	sub	w28, w25, #0x1
   19420:	lsr	x12, x20, x28
   19424:	and	x8, x12, #0x1
   19428:	neg	x8, x8
   1942c:	and	x11, x12, #0xfffffffffffffffe
   19430:	and	x8, x12, x8
   19434:	sub	x10, x11, #0x1
   19438:	orr	x9, x8, #0x1
   1943c:	mov	x8, #0xffffffffffffffff    	// #-1
   19440:	udiv	x8, x8, x10
   19444:	cmp	x9, x8
   19448:	b.ls	1945c <__gmpz_oddfac_1@@Base+0x2e0>  // b.plast
   1944c:	str	x9, [x23]
   19450:	mov	w10, #0x1                   	// #1
   19454:	mov	w9, #0x1                   	// #1
   19458:	b	19460 <__gmpz_oddfac_1@@Base+0x2e4>
   1945c:	mov	x10, xzr
   19460:	mov	x13, x11
   19464:	umulh	x14, x13, x21
   19468:	lsr	x14, x14, #1
   1946c:	add	x15, x9, x9, lsl #1
   19470:	tst	x14, #0x1
   19474:	csel	x9, x9, x15, eq  // eq = none
   19478:	cmp	x13, #0x8
   1947c:	mov	x13, x14
   19480:	b.hi	19464 <__gmpz_oddfac_1@@Base+0x2e8>  // b.pmore
   19484:	clz	x14, x11
   19488:	mov	w15, #0x40                  	// #64
   1948c:	sub	w14, w15, w14
   19490:	asr	w14, w14, #1
   19494:	lsl	x15, x3, x14
   19498:	lsr	x14, x11, x14
   1949c:	add	x14, x15, x14
   194a0:	lsr	x14, x14, #1
   194a4:	sub	x14, x14, #0x5
   194a8:	orr	x14, x14, #0x1
   194ac:	umulh	x14, x14, x21
   194b0:	mov	x16, xzr
   194b4:	mov	x13, xzr
   194b8:	mov	w1, #0x7                   	// #7
   194bc:	lsr	x18, x14, #1
   194c0:	mov	w0, #0x1                   	// #1
   194c4:	b	194e0 <__gmpz_oddfac_1@@Base+0x364>
   194c8:	ror	x17, x0, #63
   194cc:	add	x13, x13, x0, lsr #63
   194d0:	cmp	x14, x18
   194d4:	add	x1, x15, #0x3
   194d8:	mov	x0, x17
   194dc:	b.cs	19540 <__gmpz_oddfac_1@@Base+0x3c4>  // b.hs, b.nlast
   194e0:	ldr	x17, [x22, x13, lsl #3]
   194e4:	mov	x14, x16
   194e8:	mov	x15, x1
   194ec:	add	x16, x16, #0x1
   194f0:	tst	x17, x0
   194f4:	b.ne	194c8 <__gmpz_oddfac_1@@Base+0x34c>  // b.any
   194f8:	add	x17, x16, x16, lsl #1
   194fc:	and	x1, x16, #0x1
   19500:	cmp	x9, x8
   19504:	add	x17, x17, x1
   19508:	b.ls	1951c <__gmpz_oddfac_1@@Base+0x3a0>  // b.plast
   1950c:	add	x1, x10, #0x1
   19510:	str	x9, [x23, x10, lsl #3]
   19514:	mov	x10, x1
   19518:	mov	w9, #0x1                   	// #1
   1951c:	add	x17, x17, #0x1
   19520:	mov	x1, x11
   19524:	udiv	x1, x1, x17
   19528:	tst	x1, #0x1
   1952c:	csinc	x2, x17, xzr, ne  // ne = any
   19530:	cmp	x1, x17
   19534:	mul	x9, x2, x9
   19538:	b.cs	19524 <__gmpz_oddfac_1@@Base+0x3a8>  // b.hs, b.nlast
   1953c:	b	194c8 <__gmpz_oddfac_1@@Base+0x34c>
   19540:	umulh	x18, x11, x21
   19544:	lsr	x18, x18, #1
   19548:	sub	x18, x18, #0x5
   1954c:	orr	x18, x18, #0x1
   19550:	umulh	x18, x18, x21
   19554:	add	x16, x8, x8, lsl #1
   19558:	lsr	x18, x18, #1
   1955c:	b	1957c <__gmpz_oddfac_1@@Base+0x400>
   19560:	mul	x9, x9, x0
   19564:	add	x14, x14, #0x1
   19568:	add	x13, x13, x17, lsr #63
   1956c:	ror	x17, x17, #63
   19570:	cmp	x14, x18
   19574:	add	x15, x15, #0x3
   19578:	b.cs	195b4 <__gmpz_oddfac_1@@Base+0x438>  // b.hs, b.nlast
   1957c:	ldr	x0, [x22, x13, lsl #3]
   19580:	tst	x0, x17
   19584:	b.ne	19564 <__gmpz_oddfac_1@@Base+0x3e8>  // b.any
   19588:	and	x0, x14, #0x1
   1958c:	add	x0, x15, x0
   19590:	udiv	x1, x11, x0
   19594:	tbz	w1, #0, 19564 <__gmpz_oddfac_1@@Base+0x3e8>
   19598:	cmp	x9, x16
   1959c:	b.ls	19560 <__gmpz_oddfac_1@@Base+0x3e4>  // b.plast
   195a0:	add	x1, x10, #0x1
   195a4:	str	x9, [x23, x10, lsl #3]
   195a8:	mov	x10, x1
   195ac:	mov	x9, x0
   195b0:	b	19564 <__gmpz_oddfac_1@@Base+0x3e8>
   195b4:	lsr	x12, x12, #1
   195b8:	sub	x12, x12, #0x5
   195bc:	orr	x12, x12, #0x1
   195c0:	umulh	x12, x12, x21
   195c4:	sub	x11, x11, #0x5
   195c8:	lsr	x12, x12, #1
   195cc:	umulh	x11, x11, x21
   195d0:	add	x14, x12, #0x1
   195d4:	add	x16, x12, x12, lsl #1
   195d8:	lsr	x11, x11, #1
   195dc:	add	x13, x12, #0x2
   195e0:	lsr	x12, x14, #6
   195e4:	lsl	x15, x3, x14
   195e8:	add	x14, x16, #0x7
   195ec:	b	1960c <__gmpz_oddfac_1@@Base+0x490>
   195f0:	mul	x9, x16, x9
   195f4:	add	x12, x12, x15, lsr #63
   195f8:	ror	x15, x15, #63
   195fc:	cmp	x13, x11
   19600:	add	x13, x13, #0x1
   19604:	add	x14, x14, #0x3
   19608:	b.hi	1963c <__gmpz_oddfac_1@@Base+0x4c0>  // b.pmore
   1960c:	ldr	x16, [x22, x12, lsl #3]
   19610:	tst	x16, x15
   19614:	b.ne	195f4 <__gmpz_oddfac_1@@Base+0x478>  // b.any
   19618:	and	x16, x13, #0x1
   1961c:	cmp	x9, x8
   19620:	add	x16, x14, x16
   19624:	b.ls	195f0 <__gmpz_oddfac_1@@Base+0x474>  // b.plast
   19628:	add	x17, x10, #0x1
   1962c:	str	x9, [x23, x10, lsl #3]
   19630:	mov	x10, x17
   19634:	mov	x9, x16
   19638:	b	195f4 <__gmpz_oddfac_1@@Base+0x478>
   1963c:	cbz	x10, 19740 <__gmpz_oddfac_1@@Base+0x5c4>
   19640:	add	x2, x10, #0x1
   19644:	sub	x0, x29, #0x10
   19648:	mov	x1, x23
   1964c:	str	x9, [x23, x10, lsl #3]
   19650:	bl	cd70 <__gmpz_prodlimbs@plt>
   19654:	stur	xzr, [x29, #-32]
   19658:	ldur	w8, [x29, #-36]
   1965c:	ldrsw	x24, [x19, #4]
   19660:	cmp	w25, w8
   19664:	b.ne	196a0 <__gmpz_oddfac_1@@Base+0x524>  // b.any
   19668:	lsl	x1, x24, #3
   1966c:	mov	w8, #0x7f00                	// #32512
   19670:	cmp	x1, x8
   19674:	b.hi	19774 <__gmpz_oddfac_1@@Base+0x5f8>  // b.pmore
   19678:	add	x9, x1, #0xf
   1967c:	mov	x8, sp
   19680:	and	x9, x9, #0xfffffffffffffff0
   19684:	sub	x25, x8, x9
   19688:	mov	sp, x25
   1968c:	ldr	x1, [x19, #8]
   19690:	mov	x0, x25
   19694:	mov	x2, x24
   19698:	bl	ca50 <__gmpn_copyi@plt>
   1969c:	b	196ec <__gmpz_oddfac_1@@Base+0x570>
   196a0:	lsl	x1, x24, #4
   196a4:	mov	w8, #0x7f00                	// #32512
   196a8:	cmp	x1, x8
   196ac:	lsl	x26, x24, #1
   196b0:	b.hi	19784 <__gmpz_oddfac_1@@Base+0x608>  // b.pmore
   196b4:	add	x9, x1, #0xf
   196b8:	mov	x8, sp
   196bc:	and	x9, x9, #0xfffffffffffffff0
   196c0:	sub	x25, x8, x9
   196c4:	mov	sp, x25
   196c8:	ldr	x1, [x19, #8]
   196cc:	mov	x0, x25
   196d0:	mov	x2, x24
   196d4:	bl	c8e0 <__gmpn_sqr@plt>
   196d8:	add	x8, x25, x26, lsl #3
   196dc:	ldur	x8, [x8, #-8]
   196e0:	cmp	x8, #0x0
   196e4:	cset	w8, eq  // eq = none
   196e8:	sub	x24, x26, x8
   196ec:	ldursw	x27, [x29, #-12]
   196f0:	ldrsw	x8, [x19]
   196f4:	add	x26, x24, x27
   196f8:	cmp	x26, x8
   196fc:	b.gt	19750 <__gmpz_oddfac_1@@Base+0x5d4>
   19700:	ldr	x0, [x19, #8]
   19704:	ldur	x3, [x29, #-8]
   19708:	mov	x1, x25
   1970c:	mov	x2, x24
   19710:	mov	x4, x27
   19714:	bl	ccd0 <__gmpn_mul@plt>
   19718:	cmp	x0, #0x0
   1971c:	cset	w8, eq  // eq = none
   19720:	sub	w8, w26, w8
   19724:	str	w8, [x19, #4]
   19728:	ldur	x0, [x29, #-32]
   1972c:	cbnz	x0, 19760 <__gmpz_oddfac_1@@Base+0x5e4>
   19730:	mov	w25, w28
   19734:	mov	w3, #0x1                   	// #1
   19738:	cbnz	w28, 1941c <__gmpz_oddfac_1@@Base+0x2a0>
   1973c:	b	19794 <__gmpz_oddfac_1@@Base+0x618>
   19740:	ldur	x8, [x29, #-8]
   19744:	str	x9, [x8]
   19748:	stur	w3, [x29, #-12]
   1974c:	b	19654 <__gmpz_oddfac_1@@Base+0x4d8>
   19750:	mov	x0, x19
   19754:	mov	x1, x26
   19758:	bl	c080 <__gmpz_realloc@plt>
   1975c:	b	19704 <__gmpz_oddfac_1@@Base+0x588>
   19760:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19764:	mov	w25, w28
   19768:	mov	w3, #0x1                   	// #1
   1976c:	cbnz	w28, 1941c <__gmpz_oddfac_1@@Base+0x2a0>
   19770:	b	19794 <__gmpz_oddfac_1@@Base+0x618>
   19774:	sub	x0, x29, #0x20
   19778:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1977c:	mov	x25, x0
   19780:	b	1968c <__gmpz_oddfac_1@@Base+0x510>
   19784:	sub	x0, x29, #0x20
   19788:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1978c:	mov	x25, x0
   19790:	b	196c8 <__gmpz_oddfac_1@@Base+0x54c>
   19794:	ldur	x0, [x29, #-24]
   19798:	cbnz	x0, 197f8 <__gmpz_oddfac_1@@Base+0x67c>
   1979c:	mov	sp, x29
   197a0:	ldp	x20, x19, [sp, #80]
   197a4:	ldp	x22, x21, [sp, #64]
   197a8:	ldp	x24, x23, [sp, #48]
   197ac:	ldp	x26, x25, [sp, #32]
   197b0:	ldp	x28, x27, [sp, #16]
   197b4:	ldp	x29, x30, [sp], #96
   197b8:	ret
   197bc:	mov	w1, #0x1                   	// #1
   197c0:	mov	x0, x19
   197c4:	bl	c080 <__gmpz_realloc@plt>
   197c8:	b	191cc <__gmpz_oddfac_1@@Base+0x50>
   197cc:	mov	w1, #0x2                   	// #2
   197d0:	mov	x0, x19
   197d4:	bl	c080 <__gmpz_realloc@plt>
   197d8:	b	191f4 <__gmpz_oddfac_1@@Base+0x78>
   197dc:	sub	x0, x29, #0x18
   197e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   197e4:	b	1939c <__gmpz_oddfac_1@@Base+0x220>
   197e8:	sub	x0, x29, #0x18
   197ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   197f0:	mov	x23, x0
   197f4:	b	19410 <__gmpz_oddfac_1@@Base+0x294>
   197f8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   197fc:	b	1979c <__gmpz_oddfac_1@@Base+0x620>

0000000000019800 <__gmpz_prodlimbs@@Base>:
   19800:	stp	x29, x30, [sp, #-64]!
   19804:	stp	x24, x23, [sp, #16]
   19808:	stp	x22, x21, [sp, #32]
   1980c:	stp	x20, x19, [sp, #48]
   19810:	mov	x29, sp
   19814:	sub	sp, sp, #0x30
   19818:	mov	x20, x1
   1981c:	cmp	x2, #0xd
   19820:	mov	x19, x0
   19824:	b.le	198c0 <__gmpz_prodlimbs@@Base+0xc0>
   19828:	lsr	x21, x2, #1
   1982c:	sub	x22, x2, x21
   19830:	lsl	x1, x22, #3
   19834:	mov	w8, #0x7f00                	// #32512
   19838:	cmp	x1, x8
   1983c:	stur	xzr, [x29, #-40]
   19840:	stur	w22, [x29, #-32]
   19844:	b.hi	19948 <__gmpz_prodlimbs@@Base+0x148>  // b.pmore
   19848:	add	x9, x1, #0xf
   1984c:	mov	x8, sp
   19850:	and	x9, x9, #0xfffffffffffffff0
   19854:	sub	x0, x8, x9
   19858:	mov	sp, x0
   1985c:	stur	x0, [x29, #-24]
   19860:	add	x1, x20, x21, lsl #3
   19864:	sub	x0, x29, #0x20
   19868:	mov	x2, x22
   1986c:	stur	x1, [x29, #-8]
   19870:	stur	w22, [x29, #-16]
   19874:	bl	cd70 <__gmpz_prodlimbs@plt>
   19878:	mov	x22, x0
   1987c:	sub	x0, x29, #0x10
   19880:	mov	x1, x20
   19884:	mov	x2, x21
   19888:	bl	cd70 <__gmpz_prodlimbs@plt>
   1988c:	ldrsw	x8, [x19]
   19890:	add	x20, x0, x22
   19894:	mov	x21, x0
   19898:	cmp	x20, x8
   1989c:	b.gt	19954 <__gmpz_prodlimbs@@Base+0x154>
   198a0:	ldr	x0, [x19, #8]
   198a4:	cmp	x21, x22
   198a8:	b.ge	19968 <__gmpz_prodlimbs@@Base+0x168>  // b.tcont
   198ac:	ldur	x1, [x29, #-24]
   198b0:	ldur	x3, [x29, #-8]
   198b4:	mov	x2, x22
   198b8:	mov	x4, x21
   198bc:	b	19978 <__gmpz_prodlimbs@@Base+0x178>
   198c0:	cmp	x2, #0x3
   198c4:	b.lt	1990c <__gmpz_prodlimbs@@Base+0x10c>  // b.tstop
   198c8:	mov	x22, xzr
   198cc:	sub	x23, x2, #0x1
   198d0:	sub	x24, x2, #0x2
   198d4:	mov	w21, #0x1                   	// #1
   198d8:	add	x8, x20, x22, lsl #3
   198dc:	ldr	x3, [x8, #8]
   198e0:	mov	x0, x20
   198e4:	mov	x1, x20
   198e8:	mov	x2, x21
   198ec:	bl	d490 <__gmpn_mul_1@plt>
   198f0:	cmp	x0, #0x0
   198f4:	add	x22, x22, #0x1
   198f8:	str	x0, [x20, x21, lsl #3]
   198fc:	cinc	x21, x21, ne  // ne = any
   19900:	cmp	x24, x22
   19904:	b.ne	198d8 <__gmpz_prodlimbs@@Base+0xd8>  // b.any
   19908:	b	19914 <__gmpz_prodlimbs@@Base+0x114>
   1990c:	mov	w21, #0x1                   	// #1
   19910:	mov	w23, #0x1                   	// #1
   19914:	ldrsw	x8, [x19]
   19918:	cmp	x21, x8
   1991c:	b.ge	199b4 <__gmpz_prodlimbs@@Base+0x1b4>  // b.tcont
   19920:	ldr	x22, [x19, #8]
   19924:	ldr	x3, [x20, x23, lsl #3]
   19928:	mov	x0, x22
   1992c:	mov	x1, x20
   19930:	mov	x2, x21
   19934:	bl	d490 <__gmpn_mul_1@plt>
   19938:	cmp	x0, #0x0
   1993c:	str	x0, [x22, x21, lsl #3]
   19940:	cinc	x8, x21, ne  // ne = any
   19944:	b	19994 <__gmpz_prodlimbs@@Base+0x194>
   19948:	sub	x0, x29, #0x28
   1994c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19950:	b	1985c <__gmpz_prodlimbs@@Base+0x5c>
   19954:	mov	x0, x19
   19958:	mov	x1, x20
   1995c:	bl	c080 <__gmpz_realloc@plt>
   19960:	cmp	x21, x22
   19964:	b.lt	198ac <__gmpz_prodlimbs@@Base+0xac>  // b.tstop
   19968:	ldur	x1, [x29, #-8]
   1996c:	ldur	x3, [x29, #-24]
   19970:	mov	x2, x21
   19974:	mov	x4, x22
   19978:	bl	ccd0 <__gmpn_mul@plt>
   1997c:	mov	x21, x0
   19980:	ldur	x0, [x29, #-40]
   19984:	cbnz	x0, 199c8 <__gmpz_prodlimbs@@Base+0x1c8>
   19988:	cmp	x21, #0x0
   1998c:	cset	w8, eq  // eq = none
   19990:	sub	x8, x20, x8
   19994:	str	w8, [x19, #4]
   19998:	sxtw	x0, w8
   1999c:	mov	sp, x29
   199a0:	ldp	x20, x19, [sp, #48]
   199a4:	ldp	x22, x21, [sp, #32]
   199a8:	ldp	x24, x23, [sp, #16]
   199ac:	ldp	x29, x30, [sp], #64
   199b0:	ret
   199b4:	add	x1, x21, #0x1
   199b8:	mov	x0, x19
   199bc:	bl	c080 <__gmpz_realloc@plt>
   199c0:	mov	x22, x0
   199c4:	b	19924 <__gmpz_prodlimbs@@Base+0x124>
   199c8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   199cc:	b	19988 <__gmpz_prodlimbs@@Base+0x188>

00000000000199d0 <__gmpz_fdiv_q_ui@@Base>:
   199d0:	stp	x29, x30, [sp, #-64]!
   199d4:	stp	x24, x23, [sp, #16]
   199d8:	stp	x22, x21, [sp, #32]
   199dc:	stp	x20, x19, [sp, #48]
   199e0:	mov	x29, sp
   199e4:	cbz	x2, 19aa0 <__gmpz_fdiv_q_ui@@Base+0xd0>
   199e8:	ldrsw	x24, [x1, #4]
   199ec:	mov	x23, x1
   199f0:	mov	x19, x0
   199f4:	cbz	w24, 19a6c <__gmpz_fdiv_q_ui@@Base+0x9c>
   199f8:	ldrsw	x8, [x19]
   199fc:	cmp	w24, #0x0
   19a00:	cneg	x21, x24, lt  // lt = tstop
   19a04:	mov	x20, x2
   19a08:	cmp	x21, x8
   19a0c:	b.gt	19a8c <__gmpz_fdiv_q_ui@@Base+0xbc>
   19a10:	ldr	x22, [x19, #8]
   19a14:	ldr	x2, [x23, #8]
   19a18:	mov	x0, x22
   19a1c:	mov	x1, xzr
   19a20:	mov	x3, x21
   19a24:	mov	x4, x20
   19a28:	bl	cd00 <__gmpn_divrem_1@plt>
   19a2c:	tbz	w24, #31, 19a4c <__gmpz_fdiv_q_ui@@Base+0x7c>
   19a30:	cbz	x0, 19a4c <__gmpz_fdiv_q_ui@@Base+0x7c>
   19a34:	mov	x8, x22
   19a38:	ldr	x9, [x8]
   19a3c:	adds	x9, x9, #0x1
   19a40:	str	x9, [x8], #8
   19a44:	b.cs	19a38 <__gmpz_fdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   19a48:	sub	x0, x20, x0
   19a4c:	add	x8, x22, x21, lsl #3
   19a50:	ldur	x8, [x8, #-8]
   19a54:	cmp	x8, #0x0
   19a58:	cset	w8, eq  // eq = none
   19a5c:	sub	w8, w21, w8
   19a60:	cmp	w24, #0x0
   19a64:	cneg	w8, w8, lt  // lt = tstop
   19a68:	b	19a74 <__gmpz_fdiv_q_ui@@Base+0xa4>
   19a6c:	mov	w8, wzr
   19a70:	mov	x0, xzr
   19a74:	str	w8, [x19, #4]
   19a78:	ldp	x20, x19, [sp, #48]
   19a7c:	ldp	x22, x21, [sp, #32]
   19a80:	ldp	x24, x23, [sp, #16]
   19a84:	ldp	x29, x30, [sp], #64
   19a88:	ret
   19a8c:	mov	x0, x19
   19a90:	mov	x1, x21
   19a94:	bl	c080 <__gmpz_realloc@plt>
   19a98:	mov	x22, x0
   19a9c:	b	19a14 <__gmpz_fdiv_q_ui@@Base+0x44>
   19aa0:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019aa4 <__gmpz_fdiv_qr@@Base>:
   19aa4:	stp	x29, x30, [sp, #-64]!
   19aa8:	str	x23, [sp, #16]
   19aac:	stp	x22, x21, [sp, #32]
   19ab0:	stp	x20, x19, [sp, #48]
   19ab4:	mov	x29, sp
   19ab8:	sub	sp, sp, #0x10
   19abc:	ldrsw	x23, [x3, #4]
   19ac0:	mov	x20, x3
   19ac4:	mov	x22, x2
   19ac8:	mov	x19, x1
   19acc:	mov	x21, x0
   19ad0:	cmp	x0, x3
   19ad4:	str	xzr, [x29, #24]
   19ad8:	b.eq	19ae4 <__gmpz_fdiv_qr@@Base+0x40>  // b.none
   19adc:	cmp	x19, x20
   19ae0:	b.ne	19b24 <__gmpz_fdiv_qr@@Base+0x80>  // b.any
   19ae4:	cmp	x23, #0x0
   19ae8:	cneg	x8, x23, mi  // mi = first
   19aec:	cmp	x8, #0xfe0
   19af0:	lsl	x1, x8, #3
   19af4:	stur	w8, [x29, #-16]
   19af8:	b.hi	19b94 <__gmpz_fdiv_qr@@Base+0xf0>  // b.pmore
   19afc:	add	x9, x1, #0xf
   19b00:	mov	x8, sp
   19b04:	and	x9, x9, #0xfffffffffffffff0
   19b08:	sub	x0, x8, x9
   19b0c:	mov	sp, x0
   19b10:	stur	x0, [x29, #-8]
   19b14:	sub	x0, x29, #0x10
   19b18:	mov	x1, x20
   19b1c:	bl	c420 <__gmpz_set@plt>
   19b20:	sub	x20, x29, #0x10
   19b24:	ldr	w8, [x22, #4]
   19b28:	mov	x0, x21
   19b2c:	mov	x1, x19
   19b30:	mov	x2, x22
   19b34:	mov	x3, x20
   19b38:	eor	w23, w8, w23
   19b3c:	bl	bff0 <__gmpz_tdiv_qr@plt>
   19b40:	tbz	w23, #31, 19b6c <__gmpz_fdiv_qr@@Base+0xc8>
   19b44:	ldr	w8, [x19, #4]
   19b48:	cbz	w8, 19b6c <__gmpz_fdiv_qr@@Base+0xc8>
   19b4c:	mov	w2, #0x1                   	// #1
   19b50:	mov	x0, x21
   19b54:	mov	x1, x21
   19b58:	bl	c120 <__gmpz_sub_ui@plt>
   19b5c:	mov	x0, x19
   19b60:	mov	x1, x19
   19b64:	mov	x2, x20
   19b68:	bl	cf90 <__gmpz_add@plt>
   19b6c:	ldr	x0, [x29, #24]
   19b70:	cbnz	x0, 19b8c <__gmpz_fdiv_qr@@Base+0xe8>
   19b74:	mov	sp, x29
   19b78:	ldp	x20, x19, [sp, #48]
   19b7c:	ldp	x22, x21, [sp, #32]
   19b80:	ldr	x23, [sp, #16]
   19b84:	ldp	x29, x30, [sp], #64
   19b88:	ret
   19b8c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19b90:	b	19b74 <__gmpz_fdiv_qr@@Base+0xd0>
   19b94:	add	x0, x29, #0x18
   19b98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19b9c:	b	19b10 <__gmpz_fdiv_qr@@Base+0x6c>

0000000000019ba0 <__gmpz_fdiv_qr_ui@@Base>:
   19ba0:	stp	x29, x30, [sp, #-80]!
   19ba4:	str	x25, [sp, #16]
   19ba8:	stp	x24, x23, [sp, #32]
   19bac:	stp	x22, x21, [sp, #48]
   19bb0:	stp	x20, x19, [sp, #64]
   19bb4:	mov	x29, sp
   19bb8:	cbz	x3, 19cc4 <__gmpz_fdiv_qr_ui@@Base+0x124>
   19bbc:	ldrsw	x25, [x2, #4]
   19bc0:	mov	x22, x2
   19bc4:	mov	x20, x1
   19bc8:	mov	x19, x0
   19bcc:	cbz	w25, 19c48 <__gmpz_fdiv_qr_ui@@Base+0xa8>
   19bd0:	ldrsw	x8, [x19]
   19bd4:	cmp	w25, #0x0
   19bd8:	cneg	x21, x25, lt  // lt = tstop
   19bdc:	mov	x24, x3
   19be0:	cmp	x21, x8
   19be4:	b.gt	19ca0 <__gmpz_fdiv_qr_ui@@Base+0x100>
   19be8:	ldr	x23, [x19, #8]
   19bec:	ldr	x2, [x22, #8]
   19bf0:	mov	x0, x23
   19bf4:	mov	x1, xzr
   19bf8:	mov	x3, x21
   19bfc:	mov	x4, x24
   19c00:	bl	cd00 <__gmpn_divrem_1@plt>
   19c04:	mov	x22, x0
   19c08:	cbz	x0, 19c5c <__gmpz_fdiv_qr_ui@@Base+0xbc>
   19c0c:	tbz	w25, #31, 19c28 <__gmpz_fdiv_qr_ui@@Base+0x88>
   19c10:	mov	x8, x23
   19c14:	ldr	x9, [x8]
   19c18:	adds	x9, x9, #0x1
   19c1c:	str	x9, [x8], #8
   19c20:	b.cs	19c14 <__gmpz_fdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   19c24:	sub	x22, x24, x22
   19c28:	ldr	w8, [x20]
   19c2c:	cmp	w8, #0x0
   19c30:	b.le	19cb4 <__gmpz_fdiv_qr_ui@@Base+0x114>
   19c34:	ldr	x0, [x20, #8]
   19c38:	cmp	x22, #0x0
   19c3c:	cset	w8, ne  // ne = any
   19c40:	str	x22, [x0]
   19c44:	b	19c60 <__gmpz_fdiv_qr_ui@@Base+0xc0>
   19c48:	mov	w8, wzr
   19c4c:	mov	x22, xzr
   19c50:	str	wzr, [x19, #4]
   19c54:	mov	x19, x20
   19c58:	b	19c80 <__gmpz_fdiv_qr_ui@@Base+0xe0>
   19c5c:	mov	w8, wzr
   19c60:	str	w8, [x20, #4]
   19c64:	add	x8, x23, x21, lsl #3
   19c68:	ldur	x8, [x8, #-8]
   19c6c:	cmp	x8, #0x0
   19c70:	cset	w8, eq  // eq = none
   19c74:	sub	w8, w21, w8
   19c78:	cmp	w25, #0x0
   19c7c:	cneg	w8, w8, lt  // lt = tstop
   19c80:	str	w8, [x19, #4]
   19c84:	mov	x0, x22
   19c88:	ldp	x20, x19, [sp, #64]
   19c8c:	ldp	x22, x21, [sp, #48]
   19c90:	ldp	x24, x23, [sp, #32]
   19c94:	ldr	x25, [sp, #16]
   19c98:	ldp	x29, x30, [sp], #80
   19c9c:	ret
   19ca0:	mov	x0, x19
   19ca4:	mov	x1, x21
   19ca8:	bl	c080 <__gmpz_realloc@plt>
   19cac:	mov	x23, x0
   19cb0:	b	19bec <__gmpz_fdiv_qr_ui@@Base+0x4c>
   19cb4:	mov	w1, #0x1                   	// #1
   19cb8:	mov	x0, x20
   19cbc:	bl	c080 <__gmpz_realloc@plt>
   19cc0:	b	19c38 <__gmpz_fdiv_qr_ui@@Base+0x98>
   19cc4:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019cc8 <__gmpz_fdiv_r@@Base>:
   19cc8:	stp	x29, x30, [sp, #-48]!
   19ccc:	stp	x22, x21, [sp, #16]
   19cd0:	stp	x20, x19, [sp, #32]
   19cd4:	mov	x29, sp
   19cd8:	sub	sp, sp, #0x20
   19cdc:	ldrsw	x22, [x2, #4]
   19ce0:	mov	x19, x2
   19ce4:	mov	x21, x1
   19ce8:	mov	x20, x0
   19cec:	cmp	x0, x2
   19cf0:	stur	xzr, [x29, #-24]
   19cf4:	b.ne	19d38 <__gmpz_fdiv_r@@Base+0x70>  // b.any
   19cf8:	cmp	x22, #0x0
   19cfc:	cneg	x8, x22, mi  // mi = first
   19d00:	cmp	x8, #0xfe0
   19d04:	lsl	x1, x8, #3
   19d08:	stur	w8, [x29, #-16]
   19d0c:	b.hi	19d90 <__gmpz_fdiv_r@@Base+0xc8>  // b.pmore
   19d10:	add	x9, x1, #0xf
   19d14:	mov	x8, sp
   19d18:	and	x9, x9, #0xfffffffffffffff0
   19d1c:	sub	x0, x8, x9
   19d20:	mov	sp, x0
   19d24:	stur	x0, [x29, #-8]
   19d28:	sub	x0, x29, #0x10
   19d2c:	mov	x1, x19
   19d30:	bl	c420 <__gmpz_set@plt>
   19d34:	sub	x19, x29, #0x10
   19d38:	mov	x0, x20
   19d3c:	mov	x1, x21
   19d40:	mov	x2, x19
   19d44:	bl	ca80 <__gmpz_tdiv_r@plt>
   19d48:	ldr	w8, [x21, #4]
   19d4c:	eor	w8, w8, w22
   19d50:	tbz	w8, #31, 19d6c <__gmpz_fdiv_r@@Base+0xa4>
   19d54:	ldr	w8, [x20, #4]
   19d58:	cbz	w8, 19d6c <__gmpz_fdiv_r@@Base+0xa4>
   19d5c:	mov	x0, x20
   19d60:	mov	x1, x20
   19d64:	mov	x2, x19
   19d68:	bl	cf90 <__gmpz_add@plt>
   19d6c:	ldur	x0, [x29, #-24]
   19d70:	cbnz	x0, 19d88 <__gmpz_fdiv_r@@Base+0xc0>
   19d74:	mov	sp, x29
   19d78:	ldp	x20, x19, [sp, #32]
   19d7c:	ldp	x22, x21, [sp, #16]
   19d80:	ldp	x29, x30, [sp], #48
   19d84:	ret
   19d88:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19d8c:	b	19d74 <__gmpz_fdiv_r@@Base+0xac>
   19d90:	sub	x0, x29, #0x18
   19d94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19d98:	b	19d24 <__gmpz_fdiv_r@@Base+0x5c>

0000000000019d9c <__gmpz_fdiv_r_ui@@Base>:
   19d9c:	stp	x29, x30, [sp, #-48]!
   19da0:	str	x21, [sp, #16]
   19da4:	stp	x20, x19, [sp, #32]
   19da8:	mov	x29, sp
   19dac:	cbz	x2, 19e2c <__gmpz_fdiv_r_ui@@Base+0x90>
   19db0:	ldrsw	x21, [x1, #4]
   19db4:	mov	x19, x0
   19db8:	cbz	w21, 19dfc <__gmpz_fdiv_r_ui@@Base+0x60>
   19dbc:	ldr	x0, [x1, #8]
   19dc0:	cmp	x21, #0x0
   19dc4:	cneg	x1, x21, mi  // mi = first
   19dc8:	mov	x20, x2
   19dcc:	bl	c3e0 <__gmpn_mod_1@plt>
   19dd0:	cbz	x0, 19dfc <__gmpz_fdiv_r_ui@@Base+0x60>
   19dd4:	ldr	w8, [x19]
   19dd8:	sub	x9, x20, x0
   19ddc:	cmp	w21, #0x0
   19de0:	csel	x20, x9, x0, lt  // lt = tstop
   19de4:	cmp	w8, #0x0
   19de8:	b.le	19e1c <__gmpz_fdiv_r_ui@@Base+0x80>
   19dec:	ldr	x0, [x19, #8]
   19df0:	mov	w8, #0x1                   	// #1
   19df4:	str	x20, [x0]
   19df8:	b	19e04 <__gmpz_fdiv_r_ui@@Base+0x68>
   19dfc:	mov	w8, wzr
   19e00:	mov	x20, xzr
   19e04:	str	w8, [x19, #4]
   19e08:	mov	x0, x20
   19e0c:	ldp	x20, x19, [sp, #32]
   19e10:	ldr	x21, [sp, #16]
   19e14:	ldp	x29, x30, [sp], #48
   19e18:	ret
   19e1c:	mov	w1, #0x1                   	// #1
   19e20:	mov	x0, x19
   19e24:	bl	c080 <__gmpz_realloc@plt>
   19e28:	b	19df0 <__gmpz_fdiv_r_ui@@Base+0x54>
   19e2c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019e30 <__gmpz_fdiv_q@@Base>:
   19e30:	stp	x29, x30, [sp, #-64]!
   19e34:	str	x23, [sp, #16]
   19e38:	stp	x22, x21, [sp, #32]
   19e3c:	stp	x20, x19, [sp, #48]
   19e40:	mov	x29, sp
   19e44:	sub	sp, sp, #0x10
   19e48:	ldrsw	x22, [x2, #4]
   19e4c:	ldr	w23, [x1, #4]
   19e50:	mov	x20, x2
   19e54:	mov	x21, x1
   19e58:	cmp	x22, #0x0
   19e5c:	cneg	x8, x22, mi  // mi = first
   19e60:	mov	x19, x0
   19e64:	cmp	x8, #0xfe0
   19e68:	lsl	x1, x8, #3
   19e6c:	str	xzr, [x29, #24]
   19e70:	stur	w8, [x29, #-16]
   19e74:	b.hi	19ee4 <__gmpz_fdiv_q@@Base+0xb4>  // b.pmore
   19e78:	add	x9, x1, #0xf
   19e7c:	mov	x8, sp
   19e80:	and	x9, x9, #0xfffffffffffffff0
   19e84:	sub	x0, x8, x9
   19e88:	mov	sp, x0
   19e8c:	stur	x0, [x29, #-8]
   19e90:	sub	x1, x29, #0x10
   19e94:	mov	x0, x19
   19e98:	mov	x2, x21
   19e9c:	mov	x3, x20
   19ea0:	bl	bff0 <__gmpz_tdiv_qr@plt>
   19ea4:	eor	w8, w22, w23
   19ea8:	tbz	w8, #31, 19ec4 <__gmpz_fdiv_q@@Base+0x94>
   19eac:	ldur	w8, [x29, #-12]
   19eb0:	cbz	w8, 19ec4 <__gmpz_fdiv_q@@Base+0x94>
   19eb4:	mov	w2, #0x1                   	// #1
   19eb8:	mov	x0, x19
   19ebc:	mov	x1, x19
   19ec0:	bl	c120 <__gmpz_sub_ui@plt>
   19ec4:	ldr	x0, [x29, #24]
   19ec8:	cbnz	x0, 19ef0 <__gmpz_fdiv_q@@Base+0xc0>
   19ecc:	mov	sp, x29
   19ed0:	ldp	x20, x19, [sp, #48]
   19ed4:	ldp	x22, x21, [sp, #32]
   19ed8:	ldr	x23, [sp, #16]
   19edc:	ldp	x29, x30, [sp], #64
   19ee0:	ret
   19ee4:	add	x0, x29, #0x18
   19ee8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19eec:	b	19e8c <__gmpz_fdiv_q@@Base+0x5c>
   19ef0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19ef4:	b	19ecc <__gmpz_fdiv_q@@Base+0x9c>

0000000000019ef8 <__gmpz_fdiv_ui@@Base>:
   19ef8:	stp	x29, x30, [sp, #-32]!
   19efc:	stp	x20, x19, [sp, #16]
   19f00:	mov	x29, sp
   19f04:	cbz	x1, 19f54 <__gmpz_fdiv_ui@@Base+0x5c>
   19f08:	ldrsw	x20, [x0, #4]
   19f0c:	cbz	w20, 19f44 <__gmpz_fdiv_ui@@Base+0x4c>
   19f10:	ldr	x0, [x0, #8]
   19f14:	mov	x19, x1
   19f18:	cmp	x20, #0x0
   19f1c:	cneg	x1, x20, mi  // mi = first
   19f20:	mov	x2, x19
   19f24:	bl	c3e0 <__gmpn_mod_1@plt>
   19f28:	cmp	x0, #0x0
   19f2c:	sub	x8, x19, x0
   19f30:	ccmp	w20, #0x0, #0x0, ne  // ne = any
   19f34:	csel	x0, x8, x0, lt  // lt = tstop
   19f38:	ldp	x20, x19, [sp, #16]
   19f3c:	ldp	x29, x30, [sp], #32
   19f40:	ret
   19f44:	mov	x0, xzr
   19f48:	ldp	x20, x19, [sp, #16]
   19f4c:	ldp	x29, x30, [sp], #32
   19f50:	ret
   19f54:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019f58 <__gmpz_fib_ui@@Base>:
   19f58:	stp	x29, x30, [sp, #-96]!
   19f5c:	stp	x20, x19, [sp, #80]
   19f60:	mov	x20, x1
   19f64:	cmp	x1, #0x5d
   19f68:	mov	x19, x0
   19f6c:	str	x27, [sp, #16]
   19f70:	stp	x26, x25, [sp, #32]
   19f74:	stp	x24, x23, [sp, #48]
   19f78:	stp	x22, x21, [sp, #64]
   19f7c:	mov	x29, sp
   19f80:	b.hi	19fb8 <__gmpz_fib_ui@@Base+0x60>  // b.pmore
   19f84:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   19f88:	ldr	x8, [x8, #3808]
   19f8c:	ldr	w9, [x19]
   19f90:	add	x8, x8, x20, lsl #3
   19f94:	ldr	x21, [x8, #8]
   19f98:	cmp	w9, #0x0
   19f9c:	b.le	1a13c <__gmpz_fib_ui@@Base+0x1e4>
   19fa0:	ldr	x0, [x19, #8]
   19fa4:	cmp	x20, #0x0
   19fa8:	cset	w8, ne  // ne = any
   19fac:	str	x21, [x0]
   19fb0:	str	w8, [x19, #4]
   19fb4:	b	1a11c <__gmpz_fib_ui@@Base+0x1c4>
   19fb8:	lsr	x8, x20, #6
   19fbc:	mov	w9, #0x17                  	// #23
   19fc0:	mul	x22, x8, x9
   19fc4:	ldrsw	x8, [x19]
   19fc8:	lsr	x9, x22, #6
   19fcc:	add	x23, x9, #0x5
   19fd0:	lsl	x1, x23, #1
   19fd4:	cmp	x1, x8
   19fd8:	lsr	x24, x20, #1
   19fdc:	b.gt	1a14c <__gmpz_fib_ui@@Base+0x1f4>
   19fe0:	ldr	x21, [x19, #8]
   19fe4:	lsr	x8, x22, #8
   19fe8:	cmp	x8, #0x1fa
   19fec:	lsl	x1, x23, #4
   19ff0:	str	xzr, [x29, #24]
   19ff4:	b.hi	1a15c <__gmpz_fib_ui@@Base+0x204>  // b.pmore
   19ff8:	add	x9, x1, #0xf
   19ffc:	mov	x8, sp
   1a000:	and	x9, x9, #0x7ffffffffffffff0
   1a004:	sub	x22, x8, x9
   1a008:	mov	sp, x22
   1a00c:	add	x23, x22, x23, lsl #3
   1a010:	mov	x0, x22
   1a014:	mov	x1, x23
   1a018:	mov	x2, x24
   1a01c:	bl	d070 <__gmpn_fib2_ui@plt>
   1a020:	mov	x24, x0
   1a024:	tbnz	w20, #0, 1a064 <__gmpz_fib_ui@@Base+0x10c>
   1a028:	mov	x0, x23
   1a02c:	mov	x1, x22
   1a030:	mov	x2, x23
   1a034:	mov	x3, x24
   1a038:	bl	cc40 <__gmpn_addlsh1_n@plt>
   1a03c:	cmp	x0, #0x0
   1a040:	str	x0, [x23, x24, lsl #3]
   1a044:	cinc	x2, x24, ne  // ne = any
   1a048:	mov	x0, x21
   1a04c:	mov	x1, x23
   1a050:	mov	x3, x22
   1a054:	mov	x4, x24
   1a058:	add	x25, x2, x24
   1a05c:	bl	ccd0 <__gmpn_mul@plt>
   1a060:	b	1a0f0 <__gmpz_fib_ui@@Base+0x198>
   1a064:	mov	w3, #0x1                   	// #1
   1a068:	mov	x0, x21
   1a06c:	mov	x1, x22
   1a070:	mov	x2, x24
   1a074:	bl	c180 <__gmpn_lshift@plt>
   1a078:	mov	x25, x0
   1a07c:	mov	x0, x22
   1a080:	mov	x1, x21
   1a084:	mov	x2, x23
   1a088:	mov	x3, x24
   1a08c:	bl	ca70 <__gmpn_add_n@plt>
   1a090:	adds	x8, x0, x25
   1a094:	lsl	x27, x24, #3
   1a098:	mov	x0, x23
   1a09c:	mov	x1, x21
   1a0a0:	mov	x2, x23
   1a0a4:	mov	x3, x24
   1a0a8:	str	x8, [x22, x27]
   1a0ac:	cinc	x26, x24, ne  // ne = any
   1a0b0:	bl	c2d0 <__gmpn_sub_n@plt>
   1a0b4:	sub	x8, x25, x0
   1a0b8:	add	x4, x8, x24
   1a0bc:	mov	x0, x21
   1a0c0:	mov	x1, x22
   1a0c4:	mov	x2, x26
   1a0c8:	mov	x3, x23
   1a0cc:	str	x8, [x23, x27]
   1a0d0:	add	x25, x26, x4
   1a0d4:	bl	ccd0 <__gmpn_mul@plt>
   1a0d8:	ldr	x8, [x21]
   1a0dc:	tst	x20, #0x2
   1a0e0:	mov	w9, #0x2                   	// #2
   1a0e4:	cneg	x9, x9, ne  // ne = any
   1a0e8:	add	x8, x8, x9
   1a0ec:	str	x8, [x21]
   1a0f0:	cmp	x0, #0x0
   1a0f4:	cset	w8, eq  // eq = none
   1a0f8:	sub	x8, x25, x8
   1a0fc:	add	x9, x21, x8, lsl #3
   1a100:	ldur	x9, [x9, #-8]
   1a104:	cmp	x9, #0x0
   1a108:	cset	w9, eq  // eq = none
   1a10c:	sub	w8, w8, w9
   1a110:	str	w8, [x19, #4]
   1a114:	ldr	x0, [x29, #24]
   1a118:	cbnz	x0, 1a16c <__gmpz_fib_ui@@Base+0x214>
   1a11c:	mov	sp, x29
   1a120:	ldp	x20, x19, [sp, #80]
   1a124:	ldp	x22, x21, [sp, #64]
   1a128:	ldp	x24, x23, [sp, #48]
   1a12c:	ldp	x26, x25, [sp, #32]
   1a130:	ldr	x27, [sp, #16]
   1a134:	ldp	x29, x30, [sp], #96
   1a138:	ret
   1a13c:	mov	w1, #0x1                   	// #1
   1a140:	mov	x0, x19
   1a144:	bl	c080 <__gmpz_realloc@plt>
   1a148:	b	19fa4 <__gmpz_fib_ui@@Base+0x4c>
   1a14c:	mov	x0, x19
   1a150:	bl	c080 <__gmpz_realloc@plt>
   1a154:	mov	x21, x0
   1a158:	b	19fe4 <__gmpz_fib_ui@@Base+0x8c>
   1a15c:	add	x0, x29, #0x18
   1a160:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a164:	mov	x22, x0
   1a168:	b	1a00c <__gmpz_fib_ui@@Base+0xb4>
   1a16c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1a170:	b	1a11c <__gmpz_fib_ui@@Base+0x1c4>

000000000001a174 <__gmpz_fib2_ui@@Base>:
   1a174:	stp	x29, x30, [sp, #-64]!
   1a178:	stp	x22, x21, [sp, #32]
   1a17c:	stp	x20, x19, [sp, #48]
   1a180:	mov	x20, x2
   1a184:	mov	x19, x1
   1a188:	cmp	x2, #0x5d
   1a18c:	mov	x21, x0
   1a190:	str	x23, [sp, #16]
   1a194:	mov	x29, sp
   1a198:	b.hi	1a1f0 <__gmpz_fib2_ui@@Base+0x7c>  // b.pmore
   1a19c:	adrp	x22, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1a1a0:	ldr	x22, [x22, #3808]
   1a1a4:	ldr	w8, [x21]
   1a1a8:	add	x9, x22, x20, lsl #3
   1a1ac:	ldr	x23, [x9, #8]
   1a1b0:	cmp	w8, #0x0
   1a1b4:	b.le	1a264 <__gmpz_fib2_ui@@Base+0xf0>
   1a1b8:	ldr	x0, [x21, #8]
   1a1bc:	cmp	x20, #0x0
   1a1c0:	cset	w8, ne  // ne = any
   1a1c4:	str	x23, [x0]
   1a1c8:	str	w8, [x21, #4]
   1a1cc:	ldr	w8, [x19]
   1a1d0:	ldr	x21, [x22, x20, lsl #3]
   1a1d4:	cmp	w8, #0x0
   1a1d8:	b.le	1a274 <__gmpz_fib2_ui@@Base+0x100>
   1a1dc:	ldr	x0, [x19, #8]
   1a1e0:	cmp	x20, #0x1
   1a1e4:	str	x21, [x0]
   1a1e8:	cset	w8, ne  // ne = any
   1a1ec:	b	1a24c <__gmpz_fib2_ui@@Base+0xd8>
   1a1f0:	lsr	x8, x20, #5
   1a1f4:	mov	w9, #0x17                  	// #23
   1a1f8:	ldrsw	x10, [x21]
   1a1fc:	mul	x8, x8, x9
   1a200:	lsr	x8, x8, #6
   1a204:	add	x23, x8, #0x4
   1a208:	cmp	x23, x10
   1a20c:	b.gt	1a284 <__gmpz_fib2_ui@@Base+0x110>
   1a210:	ldr	x22, [x21, #8]
   1a214:	ldrsw	x8, [x19]
   1a218:	cmp	x23, x8
   1a21c:	b.gt	1a2a0 <__gmpz_fib2_ui@@Base+0x12c>
   1a220:	ldr	x23, [x19, #8]
   1a224:	mov	x0, x22
   1a228:	mov	x1, x23
   1a22c:	mov	x2, x20
   1a230:	bl	d070 <__gmpn_fib2_ui@plt>
   1a234:	str	w0, [x21, #4]
   1a238:	add	x8, x23, x0, lsl #3
   1a23c:	ldur	x8, [x8, #-8]
   1a240:	cmp	x8, #0x0
   1a244:	cset	w8, eq  // eq = none
   1a248:	sub	w8, w0, w8
   1a24c:	str	w8, [x19, #4]
   1a250:	ldp	x20, x19, [sp, #48]
   1a254:	ldp	x22, x21, [sp, #32]
   1a258:	ldr	x23, [sp, #16]
   1a25c:	ldp	x29, x30, [sp], #64
   1a260:	ret
   1a264:	mov	w1, #0x1                   	// #1
   1a268:	mov	x0, x21
   1a26c:	bl	c080 <__gmpz_realloc@plt>
   1a270:	b	1a1bc <__gmpz_fib2_ui@@Base+0x48>
   1a274:	mov	w1, #0x1                   	// #1
   1a278:	mov	x0, x19
   1a27c:	bl	c080 <__gmpz_realloc@plt>
   1a280:	b	1a1e0 <__gmpz_fib2_ui@@Base+0x6c>
   1a284:	mov	x0, x21
   1a288:	mov	x1, x23
   1a28c:	bl	c080 <__gmpz_realloc@plt>
   1a290:	mov	x22, x0
   1a294:	ldrsw	x8, [x19]
   1a298:	cmp	x23, x8
   1a29c:	b.le	1a220 <__gmpz_fib2_ui@@Base+0xac>
   1a2a0:	mov	x0, x19
   1a2a4:	mov	x1, x23
   1a2a8:	bl	c080 <__gmpz_realloc@plt>
   1a2ac:	mov	x23, x0
   1a2b0:	b	1a224 <__gmpz_fib2_ui@@Base+0xb0>

000000000001a2b4 <__gmpz_fits_sint_p@@Base>:
   1a2b4:	ldr	x8, [x0, #8]
   1a2b8:	ldr	w9, [x0, #4]
   1a2bc:	ldr	x8, [x8]
   1a2c0:	cmn	w9, #0x1
   1a2c4:	b.eq	1a2e4 <__gmpz_fits_sint_p@@Base+0x30>  // b.none
   1a2c8:	cbz	w9, 1a2f4 <__gmpz_fits_sint_p@@Base+0x40>
   1a2cc:	cmp	w9, #0x1
   1a2d0:	b.ne	1a2fc <__gmpz_fits_sint_p@@Base+0x48>  // b.any
   1a2d4:	lsr	x8, x8, #31
   1a2d8:	cmp	x8, #0x0
   1a2dc:	cset	w0, eq  // eq = none
   1a2e0:	ret
   1a2e4:	mov	w9, #0x80000001            	// #-2147483647
   1a2e8:	cmp	x8, x9
   1a2ec:	cset	w0, cc  // cc = lo, ul, last
   1a2f0:	ret
   1a2f4:	mov	w0, #0x1                   	// #1
   1a2f8:	ret
   1a2fc:	mov	w0, wzr
   1a300:	ret

000000000001a304 <__gmpz_fits_slong_p@@Base>:
   1a304:	ldr	x8, [x0, #8]
   1a308:	ldr	w9, [x0, #4]
   1a30c:	ldr	x8, [x8]
   1a310:	cmn	w9, #0x1
   1a314:	b.eq	1a330 <__gmpz_fits_slong_p@@Base+0x2c>  // b.none
   1a318:	cbz	w9, 1a340 <__gmpz_fits_slong_p@@Base+0x3c>
   1a31c:	cmp	w9, #0x1
   1a320:	b.ne	1a348 <__gmpz_fits_slong_p@@Base+0x44>  // b.any
   1a324:	lsr	x8, x8, #63
   1a328:	eor	w0, w8, #0x1
   1a32c:	ret
   1a330:	mov	x9, #0x8000000000000001    	// #-9223372036854775807
   1a334:	cmp	x8, x9
   1a338:	cset	w0, cc  // cc = lo, ul, last
   1a33c:	ret
   1a340:	mov	w0, #0x1                   	// #1
   1a344:	ret
   1a348:	mov	w0, wzr
   1a34c:	ret

000000000001a350 <__gmpz_fits_sshort_p@@Base>:
   1a350:	ldr	x8, [x0, #8]
   1a354:	ldr	w9, [x0, #4]
   1a358:	ldr	x8, [x8]
   1a35c:	cmn	w9, #0x1
   1a360:	b.eq	1a37c <__gmpz_fits_sshort_p@@Base+0x2c>  // b.none
   1a364:	cbz	w9, 1a388 <__gmpz_fits_sshort_p@@Base+0x38>
   1a368:	cmp	w9, #0x1
   1a36c:	b.ne	1a390 <__gmpz_fits_sshort_p@@Base+0x40>  // b.any
   1a370:	cmp	x8, #0x8, lsl #12
   1a374:	cset	w0, cc  // cc = lo, ul, last
   1a378:	ret
   1a37c:	cmp	x8, #0x8, lsl #12
   1a380:	cset	w0, ls  // ls = plast
   1a384:	ret
   1a388:	mov	w0, #0x1                   	// #1
   1a38c:	ret
   1a390:	mov	w0, wzr
   1a394:	ret

000000000001a398 <__gmpz_fits_uint_p@@Base>:
   1a398:	ldr	w8, [x0, #4]
   1a39c:	cbz	w8, 1a3bc <__gmpz_fits_uint_p@@Base+0x24>
   1a3a0:	cmp	w8, #0x1
   1a3a4:	b.ne	1a3c4 <__gmpz_fits_uint_p@@Base+0x2c>  // b.any
   1a3a8:	ldr	x8, [x0, #8]
   1a3ac:	ldr	w8, [x8, #4]
   1a3b0:	cmp	w8, #0x0
   1a3b4:	cset	w0, eq  // eq = none
   1a3b8:	ret
   1a3bc:	mov	w0, #0x1                   	// #1
   1a3c0:	ret
   1a3c4:	mov	w0, wzr
   1a3c8:	ret

000000000001a3cc <__gmpz_fits_ulong_p@@Base>:
   1a3cc:	ldr	w8, [x0, #4]
   1a3d0:	cmp	w8, #0x2
   1a3d4:	cset	w0, cc  // cc = lo, ul, last
   1a3d8:	ret

000000000001a3dc <__gmpz_fits_ushort_p@@Base>:
   1a3dc:	ldr	w8, [x0, #4]
   1a3e0:	cbz	w8, 1a400 <__gmpz_fits_ushort_p@@Base+0x24>
   1a3e4:	cmp	w8, #0x1
   1a3e8:	b.ne	1a408 <__gmpz_fits_ushort_p@@Base+0x2c>  // b.any
   1a3ec:	ldr	x8, [x0, #8]
   1a3f0:	ldr	x8, [x8]
   1a3f4:	cmp	x8, #0x10, lsl #12
   1a3f8:	cset	w0, cc  // cc = lo, ul, last
   1a3fc:	ret
   1a400:	mov	w0, #0x1                   	// #1
   1a404:	ret
   1a408:	mov	w0, wzr
   1a40c:	ret

000000000001a410 <__gmpz_gcd@@Base>:
   1a410:	stp	x29, x30, [sp, #-96]!
   1a414:	stp	x26, x25, [sp, #32]
   1a418:	stp	x24, x23, [sp, #48]
   1a41c:	stp	x22, x21, [sp, #64]
   1a420:	stp	x20, x19, [sp, #80]
   1a424:	ldr	w8, [x1, #4]
   1a428:	ldr	w9, [x2, #4]
   1a42c:	ldr	x22, [x2, #8]
   1a430:	mov	x19, x0
   1a434:	cmp	w8, #0x0
   1a438:	cneg	w21, w8, mi  // mi = first
   1a43c:	cmp	w9, #0x0
   1a440:	cneg	w20, w9, mi  // mi = first
   1a444:	str	x27, [sp, #16]
   1a448:	mov	x29, sp
   1a44c:	cbz	w21, 1a478 <__gmpz_gcd@@Base+0x68>
   1a450:	ldr	x23, [x1, #8]
   1a454:	cbz	w20, 1a4a4 <__gmpz_gcd@@Base+0x94>
   1a458:	cmp	w21, #0x1
   1a45c:	b.ne	1a4d0 <__gmpz_gcd@@Base+0xc0>  // b.any
   1a460:	mov	w8, #0x1                   	// #1
   1a464:	str	w8, [x19, #4]
   1a468:	ldr	x2, [x23]
   1a46c:	mov	x0, x22
   1a470:	mov	x1, x20
   1a474:	b	1a4ec <__gmpz_gcd@@Base+0xdc>
   1a478:	cmp	x19, x2
   1a47c:	str	w20, [x19, #4]
   1a480:	b.eq	1a7b0 <__gmpz_gcd@@Base+0x3a0>  // b.none
   1a484:	ldrsw	x8, [x19]
   1a488:	cmp	x20, x8
   1a48c:	b.gt	1a598 <__gmpz_gcd@@Base+0x188>
   1a490:	ldr	x0, [x19, #8]
   1a494:	mov	x1, x22
   1a498:	mov	x2, x20
   1a49c:	bl	ca50 <__gmpn_copyi@plt>
   1a4a0:	b	1a7b0 <__gmpz_gcd@@Base+0x3a0>
   1a4a4:	cmp	x19, x1
   1a4a8:	str	w21, [x19, #4]
   1a4ac:	b.eq	1a7b0 <__gmpz_gcd@@Base+0x3a0>  // b.none
   1a4b0:	ldrsw	x8, [x19]
   1a4b4:	cmp	x21, x8
   1a4b8:	b.gt	1a5a8 <__gmpz_gcd@@Base+0x198>
   1a4bc:	ldr	x0, [x19, #8]
   1a4c0:	mov	x1, x23
   1a4c4:	mov	x2, x21
   1a4c8:	bl	ca50 <__gmpn_copyi@plt>
   1a4cc:	b	1a7b0 <__gmpz_gcd@@Base+0x3a0>
   1a4d0:	cmp	w20, #0x1
   1a4d4:	b.ne	1a50c <__gmpz_gcd@@Base+0xfc>  // b.any
   1a4d8:	mov	w8, #0x1                   	// #1
   1a4dc:	str	w8, [x19, #4]
   1a4e0:	ldr	x2, [x22]
   1a4e4:	mov	x0, x23
   1a4e8:	mov	x1, x21
   1a4ec:	bl	bf90 <__gmpn_gcd_1@plt>
   1a4f0:	ldr	w8, [x19]
   1a4f4:	mov	x20, x0
   1a4f8:	cmp	w8, #0x0
   1a4fc:	b.le	1a584 <__gmpz_gcd@@Base+0x174>
   1a500:	ldr	x8, [x19, #8]
   1a504:	str	x20, [x8]
   1a508:	b	1a7b0 <__gmpz_gcd@@Base+0x3a0>
   1a50c:	sub	x25, x23, #0x8
   1a510:	str	xzr, [x29, #24]
   1a514:	ldr	x8, [x25, #8]!
   1a518:	cbz	x8, 1a514 <__gmpz_gcd@@Base+0x104>
   1a51c:	sub	x9, x25, x23
   1a520:	asr	x27, x9, #3
   1a524:	sub	x21, x21, x27
   1a528:	rbit	x8, x8
   1a52c:	lsl	x1, x21, #3
   1a530:	mov	w9, #0x7f00                	// #32512
   1a534:	cmp	x1, x9
   1a538:	clz	x24, x8
   1a53c:	b.hi	1a5b8 <__gmpz_gcd@@Base+0x1a8>  // b.pmore
   1a540:	add	x9, x1, #0xf
   1a544:	mov	x8, sp
   1a548:	and	x9, x9, #0xfffffffffffffff0
   1a54c:	sub	x23, x8, x9
   1a550:	mov	sp, x23
   1a554:	mov	x0, x23
   1a558:	mov	x1, x25
   1a55c:	mov	x2, x21
   1a560:	cbz	x24, 1a5d4 <__gmpz_gcd@@Base+0x1c4>
   1a564:	mov	w3, w24
   1a568:	bl	c1a0 <__gmpn_rshift@plt>
   1a56c:	add	x8, x23, x21, lsl #3
   1a570:	ldur	x8, [x8, #-8]
   1a574:	cmp	x8, #0x0
   1a578:	cset	w8, eq  // eq = none
   1a57c:	sub	x21, x21, x8
   1a580:	b	1a5d8 <__gmpz_gcd@@Base+0x1c8>
   1a584:	mov	w1, #0x1                   	// #1
   1a588:	mov	x0, x19
   1a58c:	bl	c080 <__gmpz_realloc@plt>
   1a590:	str	x20, [x0]
   1a594:	b	1a7b0 <__gmpz_gcd@@Base+0x3a0>
   1a598:	mov	x0, x19
   1a59c:	mov	x1, x20
   1a5a0:	bl	c080 <__gmpz_realloc@plt>
   1a5a4:	b	1a494 <__gmpz_gcd@@Base+0x84>
   1a5a8:	mov	x0, x19
   1a5ac:	mov	x1, x21
   1a5b0:	bl	c080 <__gmpz_realloc@plt>
   1a5b4:	b	1a4c0 <__gmpz_gcd@@Base+0xb0>
   1a5b8:	add	x0, x29, #0x18
   1a5bc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a5c0:	mov	x23, x0
   1a5c4:	mov	x0, x23
   1a5c8:	mov	x1, x25
   1a5cc:	mov	x2, x21
   1a5d0:	cbnz	x24, 1a564 <__gmpz_gcd@@Base+0x154>
   1a5d4:	bl	ca50 <__gmpn_copyi@plt>
   1a5d8:	sub	x1, x22, #0x8
   1a5dc:	ldr	x8, [x1, #8]!
   1a5e0:	cbz	x8, 1a5dc <__gmpz_gcd@@Base+0x1cc>
   1a5e4:	sub	x9, x1, x22
   1a5e8:	asr	x26, x9, #3
   1a5ec:	sub	x25, x20, x26
   1a5f0:	rbit	x10, x8
   1a5f4:	lsl	x8, x25, #3
   1a5f8:	mov	w9, #0x7f00                	// #32512
   1a5fc:	cmp	x8, x9
   1a600:	clz	x22, x10
   1a604:	b.hi	1a650 <__gmpz_gcd@@Base+0x240>  // b.pmore
   1a608:	add	x8, x8, #0xf
   1a60c:	mov	x9, sp
   1a610:	and	x8, x8, #0xfffffffffffffff0
   1a614:	sub	x20, x9, x8
   1a618:	mov	sp, x20
   1a61c:	mov	x0, x20
   1a620:	mov	x2, x25
   1a624:	cbz	x22, 1a674 <__gmpz_gcd@@Base+0x264>
   1a628:	mov	w3, w22
   1a62c:	bl	c1a0 <__gmpn_rshift@plt>
   1a630:	add	x8, x20, x25, lsl #3
   1a634:	ldur	x8, [x8, #-8]
   1a638:	cmp	x8, #0x0
   1a63c:	cset	w8, eq  // eq = none
   1a640:	sub	x25, x25, x8
   1a644:	cmp	x27, x26
   1a648:	b.le	1a680 <__gmpz_gcd@@Base+0x270>
   1a64c:	b	1a68c <__gmpz_gcd@@Base+0x27c>
   1a650:	add	x0, x29, #0x18
   1a654:	mov	x20, x1
   1a658:	mov	x1, x8
   1a65c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a660:	mov	x1, x20
   1a664:	mov	x20, x0
   1a668:	mov	x0, x20
   1a66c:	mov	x2, x25
   1a670:	cbnz	x22, 1a628 <__gmpz_gcd@@Base+0x218>
   1a674:	bl	ca50 <__gmpn_copyi@plt>
   1a678:	cmp	x27, x26
   1a67c:	b.gt	1a68c <__gmpz_gcd@@Base+0x27c>
   1a680:	b.ge	1a698 <__gmpz_gcd@@Base+0x288>  // b.tcont
   1a684:	mov	x26, x27
   1a688:	mov	x22, x24
   1a68c:	cmp	x21, x25
   1a690:	b.ge	1a6ac <__gmpz_gcd@@Base+0x29c>  // b.tcont
   1a694:	b	1a6c8 <__gmpz_gcd@@Base+0x2b8>
   1a698:	cmp	x24, x22
   1a69c:	csel	x22, x24, x22, cc  // cc = lo, ul, last
   1a6a0:	mov	x26, x27
   1a6a4:	cmp	x21, x25
   1a6a8:	b.lt	1a6c8 <__gmpz_gcd@@Base+0x2b8>  // b.tstop
   1a6ac:	b.ne	1a74c <__gmpz_gcd@@Base+0x33c>  // b.any
   1a6b0:	lsl	x8, x21, #3
   1a6b4:	sub	x8, x8, #0x8
   1a6b8:	ldr	x9, [x23, x8]
   1a6bc:	ldr	x8, [x20, x8]
   1a6c0:	cmp	x9, x8
   1a6c4:	b.cs	1a74c <__gmpz_gcd@@Base+0x33c>  // b.hs, b.nlast
   1a6c8:	mov	x0, x20
   1a6cc:	mov	x1, x20
   1a6d0:	mov	x2, x25
   1a6d4:	mov	x3, x23
   1a6d8:	mov	x4, x21
   1a6dc:	bl	cc10 <__gmpn_gcd@plt>
   1a6e0:	mov	x23, x0
   1a6e4:	add	x21, x0, x26
   1a6e8:	cbz	x22, 1a770 <__gmpz_gcd@@Base+0x360>
   1a6ec:	add	x8, x20, x23, lsl #3
   1a6f0:	ldur	x8, [x8, #-8]
   1a6f4:	neg	x9, x22
   1a6f8:	ldrsw	x10, [x19]
   1a6fc:	lsr	x8, x8, x9
   1a700:	cmp	x8, #0x0
   1a704:	cinc	x21, x21, ne  // ne = any
   1a708:	cmp	x21, x10
   1a70c:	b.gt	1a7d8 <__gmpz_gcd@@Base+0x3c8>
   1a710:	ldr	x24, [x19, #8]
   1a714:	cbz	x26, 1a728 <__gmpz_gcd@@Base+0x318>
   1a718:	lsl	x2, x26, #3
   1a71c:	mov	x0, x24
   1a720:	mov	w1, wzr
   1a724:	bl	c5f0 <memset@plt>
   1a728:	add	x24, x24, x26, lsl #3
   1a72c:	mov	x0, x24
   1a730:	mov	x1, x20
   1a734:	mov	x2, x23
   1a738:	mov	w3, w22
   1a73c:	bl	c180 <__gmpn_lshift@plt>
   1a740:	cbz	x0, 1a7a4 <__gmpz_gcd@@Base+0x394>
   1a744:	str	x0, [x24, x23, lsl #3]
   1a748:	b	1a7a4 <__gmpz_gcd@@Base+0x394>
   1a74c:	mov	x0, x20
   1a750:	mov	x1, x23
   1a754:	mov	x2, x21
   1a758:	mov	x3, x20
   1a75c:	mov	x4, x25
   1a760:	bl	cc10 <__gmpn_gcd@plt>
   1a764:	mov	x23, x0
   1a768:	add	x21, x0, x26
   1a76c:	cbnz	x22, 1a6ec <__gmpz_gcd@@Base+0x2dc>
   1a770:	ldrsw	x8, [x19]
   1a774:	cmp	x21, x8
   1a778:	b.gt	1a7f0 <__gmpz_gcd@@Base+0x3e0>
   1a77c:	ldr	x22, [x19, #8]
   1a780:	cbz	x26, 1a794 <__gmpz_gcd@@Base+0x384>
   1a784:	lsl	x2, x26, #3
   1a788:	mov	x0, x22
   1a78c:	mov	w1, wzr
   1a790:	bl	c5f0 <memset@plt>
   1a794:	add	x0, x22, x26, lsl #3
   1a798:	mov	x1, x20
   1a79c:	mov	x2, x23
   1a7a0:	bl	ca50 <__gmpn_copyi@plt>
   1a7a4:	str	w21, [x19, #4]
   1a7a8:	ldr	x0, [x29, #24]
   1a7ac:	cbnz	x0, 1a7d0 <__gmpz_gcd@@Base+0x3c0>
   1a7b0:	mov	sp, x29
   1a7b4:	ldp	x20, x19, [sp, #80]
   1a7b8:	ldp	x22, x21, [sp, #64]
   1a7bc:	ldp	x24, x23, [sp, #48]
   1a7c0:	ldp	x26, x25, [sp, #32]
   1a7c4:	ldr	x27, [sp, #16]
   1a7c8:	ldp	x29, x30, [sp], #96
   1a7cc:	ret
   1a7d0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1a7d4:	b	1a7b0 <__gmpz_gcd@@Base+0x3a0>
   1a7d8:	mov	x0, x19
   1a7dc:	mov	x1, x21
   1a7e0:	bl	c080 <__gmpz_realloc@plt>
   1a7e4:	mov	x24, x0
   1a7e8:	cbnz	x26, 1a718 <__gmpz_gcd@@Base+0x308>
   1a7ec:	b	1a728 <__gmpz_gcd@@Base+0x318>
   1a7f0:	mov	x0, x19
   1a7f4:	mov	x1, x21
   1a7f8:	bl	c080 <__gmpz_realloc@plt>
   1a7fc:	mov	x22, x0
   1a800:	cbnz	x26, 1a784 <__gmpz_gcd@@Base+0x374>
   1a804:	b	1a794 <__gmpz_gcd@@Base+0x384>

000000000001a808 <__gmpz_gcd_ui@@Base>:
   1a808:	stp	x29, x30, [sp, #-48]!
   1a80c:	stp	x22, x21, [sp, #16]
   1a810:	stp	x20, x19, [sp, #32]
   1a814:	ldr	w8, [x1, #4]
   1a818:	mov	x20, x2
   1a81c:	mov	x19, x0
   1a820:	mov	x29, sp
   1a824:	cmp	w8, #0x0
   1a828:	cneg	w21, w8, mi  // mi = first
   1a82c:	cbz	w21, 1a84c <__gmpz_gcd_ui@@Base+0x44>
   1a830:	mov	x22, x1
   1a834:	cbz	x20, 1a874 <__gmpz_gcd_ui@@Base+0x6c>
   1a838:	ldr	x0, [x22, #8]
   1a83c:	mov	x1, x21
   1a840:	mov	x2, x20
   1a844:	bl	bf90 <__gmpn_gcd_1@plt>
   1a848:	mov	x20, x0
   1a84c:	cbz	x19, 1a8b0 <__gmpz_gcd_ui@@Base+0xa8>
   1a850:	ldr	w8, [x19]
   1a854:	cmp	w8, #0x0
   1a858:	b.le	1a8c4 <__gmpz_gcd_ui@@Base+0xbc>
   1a85c:	ldr	x0, [x19, #8]
   1a860:	cmp	x20, #0x0
   1a864:	cset	w8, ne  // ne = any
   1a868:	str	x20, [x0]
   1a86c:	str	w8, [x19, #4]
   1a870:	b	1a8b0 <__gmpz_gcd_ui@@Base+0xa8>
   1a874:	cbz	x19, 1a8a0 <__gmpz_gcd_ui@@Base+0x98>
   1a878:	cmp	x22, x19
   1a87c:	b.eq	1a89c <__gmpz_gcd_ui@@Base+0x94>  // b.none
   1a880:	ldrsw	x8, [x19]
   1a884:	cmp	x21, x8
   1a888:	b.gt	1a8d4 <__gmpz_gcd_ui@@Base+0xcc>
   1a88c:	ldr	x0, [x19, #8]
   1a890:	ldr	x1, [x22, #8]
   1a894:	mov	x2, x21
   1a898:	bl	ca50 <__gmpn_copyi@plt>
   1a89c:	str	w21, [x19, #4]
   1a8a0:	ldr	x8, [x22, #8]
   1a8a4:	cmp	w21, #0x1
   1a8a8:	ldr	x8, [x8]
   1a8ac:	csel	x20, x8, xzr, eq  // eq = none
   1a8b0:	mov	x0, x20
   1a8b4:	ldp	x20, x19, [sp, #32]
   1a8b8:	ldp	x22, x21, [sp, #16]
   1a8bc:	ldp	x29, x30, [sp], #48
   1a8c0:	ret
   1a8c4:	mov	w1, #0x1                   	// #1
   1a8c8:	mov	x0, x19
   1a8cc:	bl	c080 <__gmpz_realloc@plt>
   1a8d0:	b	1a860 <__gmpz_gcd_ui@@Base+0x58>
   1a8d4:	mov	x0, x19
   1a8d8:	mov	x1, x21
   1a8dc:	bl	c080 <__gmpz_realloc@plt>
   1a8e0:	b	1a88c <__gmpz_gcd_ui@@Base+0x84>

000000000001a8e4 <__gmpz_gcdext@@Base>:
   1a8e4:	stp	x29, x30, [sp, #-96]!
   1a8e8:	stp	x28, x27, [sp, #16]
   1a8ec:	stp	x26, x25, [sp, #32]
   1a8f0:	stp	x24, x23, [sp, #48]
   1a8f4:	stp	x22, x21, [sp, #64]
   1a8f8:	stp	x20, x19, [sp, #80]
   1a8fc:	mov	x29, sp
   1a900:	sub	sp, sp, #0x50
   1a904:	ldr	w8, [x3, #4]
   1a908:	ldr	w9, [x4, #4]
   1a90c:	mov	x19, x0
   1a910:	cmp	w8, #0x0
   1a914:	cneg	w8, w8, mi  // mi = first
   1a918:	cmp	w9, #0x0
   1a91c:	cneg	w9, w9, mi  // mi = first
   1a920:	cmp	w8, w9
   1a924:	csel	w26, w8, w9, cc  // cc = lo, ul, last
   1a928:	csel	w23, w9, w8, cc  // cc = lo, ul, last
   1a92c:	csel	x21, x3, x4, cc  // cc = lo, ul, last
   1a930:	csel	x24, x4, x3, cc  // cc = lo, ul, last
   1a934:	csel	x25, x1, x2, cc  // cc = lo, ul, last
   1a938:	csel	x20, x2, x1, cc  // cc = lo, ul, last
   1a93c:	cbz	w26, 1aaa0 <__gmpz_gcdext@@Base+0x1bc>
   1a940:	add	x8, x26, x26, lsl #1
   1a944:	add	x8, x23, x8
   1a948:	add	x8, x8, #0x1
   1a94c:	cmp	x8, #0xfe0
   1a950:	lsl	x1, x8, #3
   1a954:	stur	xzr, [x29, #-16]
   1a958:	stur	x25, [x29, #-72]
   1a95c:	b.hi	1ab20 <__gmpz_gcdext@@Base+0x23c>  // b.pmore
   1a960:	add	x9, x1, #0xf
   1a964:	mov	x8, sp
   1a968:	and	x9, x9, #0xfffffffff0
   1a96c:	sub	x22, x8, x9
   1a970:	mov	sp, x22
   1a974:	lsl	x8, x26, #3
   1a978:	add	x27, x22, x8
   1a97c:	ldr	x1, [x24, #8]
   1a980:	add	x9, x27, x8
   1a984:	add	x28, x9, #0x8
   1a988:	add	x25, x28, x8
   1a98c:	mov	x0, x25
   1a990:	mov	x2, x23
   1a994:	bl	ca50 <__gmpn_copyi@plt>
   1a998:	ldr	x1, [x21, #8]
   1a99c:	mov	x0, x28
   1a9a0:	mov	x2, x26
   1a9a4:	bl	ca50 <__gmpn_copyi@plt>
   1a9a8:	sub	x2, x29, #0x8
   1a9ac:	mov	x0, x22
   1a9b0:	mov	x1, x27
   1a9b4:	mov	x3, x25
   1a9b8:	mov	x4, x23
   1a9bc:	mov	x5, x28
   1a9c0:	mov	x6, x26
   1a9c4:	bl	c560 <__gmpn_gcdext@plt>
   1a9c8:	ldur	x8, [x29, #-8]
   1a9cc:	ldr	w9, [x24, #4]
   1a9d0:	ldur	x25, [x29, #-72]
   1a9d4:	mov	x26, x0
   1a9d8:	cmp	x8, #0x0
   1a9dc:	cneg	x28, x8, mi  // mi = first
   1a9e0:	cmp	w9, #0x0
   1a9e4:	cneg	x8, x8, lt  // lt = tstop
   1a9e8:	stur	x8, [x29, #-8]
   1a9ec:	cbz	x25, 1aa44 <__gmpz_gcdext@@Base+0x160>
   1a9f0:	stur	w8, [x29, #-60]
   1a9f4:	add	x8, x27, x28, lsl #3
   1a9f8:	add	w9, w23, w28
   1a9fc:	stur	x8, [x29, #-24]
   1aa00:	add	w8, w9, #0x1
   1aa04:	sub	x0, x29, #0x20
   1aa08:	sub	x1, x29, #0x40
   1aa0c:	mov	x2, x24
   1aa10:	stur	x22, [x29, #-40]
   1aa14:	stur	w26, [x29, #-44]
   1aa18:	stur	x27, [x29, #-56]
   1aa1c:	stur	w8, [x29, #-32]
   1aa20:	bl	c4b0 <__gmpz_mul@plt>
   1aa24:	sub	x0, x29, #0x20
   1aa28:	sub	x1, x29, #0x30
   1aa2c:	sub	x2, x29, #0x20
   1aa30:	bl	c260 <__gmpz_sub@plt>
   1aa34:	sub	x1, x29, #0x20
   1aa38:	mov	x0, x25
   1aa3c:	mov	x2, x21
   1aa40:	bl	c3f0 <__gmpz_divexact@plt>
   1aa44:	cbz	x20, 1aa6c <__gmpz_gcdext@@Base+0x188>
   1aa48:	ldrsw	x8, [x20]
   1aa4c:	cmp	x28, x8
   1aa50:	b.gt	1ab30 <__gmpz_gcdext@@Base+0x24c>
   1aa54:	ldr	x0, [x20, #8]
   1aa58:	mov	x1, x27
   1aa5c:	mov	x2, x28
   1aa60:	bl	ca50 <__gmpn_copyi@plt>
   1aa64:	ldur	x8, [x29, #-8]
   1aa68:	str	w8, [x20, #4]
   1aa6c:	cbz	x19, 1aa90 <__gmpz_gcdext@@Base+0x1ac>
   1aa70:	ldrsw	x8, [x19]
   1aa74:	cmp	x26, x8
   1aa78:	b.gt	1ab40 <__gmpz_gcdext@@Base+0x25c>
   1aa7c:	ldr	x0, [x19, #8]
   1aa80:	mov	x1, x22
   1aa84:	mov	x2, x26
   1aa88:	bl	ca50 <__gmpn_copyi@plt>
   1aa8c:	str	w26, [x19, #4]
   1aa90:	ldur	x0, [x29, #-16]
   1aa94:	cbz	x0, 1ab00 <__gmpz_gcdext@@Base+0x21c>
   1aa98:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1aa9c:	b	1ab00 <__gmpz_gcdext@@Base+0x21c>
   1aaa0:	ldr	w8, [x24, #4]
   1aaa4:	cmp	w23, #0x0
   1aaa8:	cset	w9, ne  // ne = any
   1aaac:	cmp	w8, #0x0
   1aab0:	csinv	w22, w9, wzr, ge  // ge = tcont
   1aab4:	cbz	x19, 1aad8 <__gmpz_gcdext@@Base+0x1f4>
   1aab8:	ldrsw	x8, [x19]
   1aabc:	cmp	x23, x8
   1aac0:	b.gt	1ab50 <__gmpz_gcdext@@Base+0x26c>
   1aac4:	ldr	x0, [x19, #8]
   1aac8:	ldr	x1, [x24, #8]
   1aacc:	mov	x2, x23
   1aad0:	bl	ca50 <__gmpn_copyi@plt>
   1aad4:	str	w23, [x19, #4]
   1aad8:	cbz	x25, 1aae0 <__gmpz_gcdext@@Base+0x1fc>
   1aadc:	str	wzr, [x25, #4]
   1aae0:	cbz	x20, 1ab00 <__gmpz_gcdext@@Base+0x21c>
   1aae4:	ldr	w8, [x20]
   1aae8:	str	w22, [x20, #4]
   1aaec:	cmp	w8, #0x0
   1aaf0:	b.le	1ab60 <__gmpz_gcdext@@Base+0x27c>
   1aaf4:	ldr	x0, [x20, #8]
   1aaf8:	mov	w8, #0x1                   	// #1
   1aafc:	str	x8, [x0]
   1ab00:	mov	sp, x29
   1ab04:	ldp	x20, x19, [sp, #80]
   1ab08:	ldp	x22, x21, [sp, #64]
   1ab0c:	ldp	x24, x23, [sp, #48]
   1ab10:	ldp	x26, x25, [sp, #32]
   1ab14:	ldp	x28, x27, [sp, #16]
   1ab18:	ldp	x29, x30, [sp], #96
   1ab1c:	ret
   1ab20:	sub	x0, x29, #0x10
   1ab24:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1ab28:	mov	x22, x0
   1ab2c:	b	1a974 <__gmpz_gcdext@@Base+0x90>
   1ab30:	mov	x0, x20
   1ab34:	mov	x1, x28
   1ab38:	bl	c080 <__gmpz_realloc@plt>
   1ab3c:	b	1aa58 <__gmpz_gcdext@@Base+0x174>
   1ab40:	mov	x0, x19
   1ab44:	mov	x1, x26
   1ab48:	bl	c080 <__gmpz_realloc@plt>
   1ab4c:	b	1aa80 <__gmpz_gcdext@@Base+0x19c>
   1ab50:	mov	x0, x19
   1ab54:	mov	x1, x23
   1ab58:	bl	c080 <__gmpz_realloc@plt>
   1ab5c:	b	1aac8 <__gmpz_gcdext@@Base+0x1e4>
   1ab60:	mov	w1, #0x1                   	// #1
   1ab64:	mov	x0, x20
   1ab68:	bl	c080 <__gmpz_realloc@plt>
   1ab6c:	b	1aaf8 <__gmpz_gcdext@@Base+0x214>

000000000001ab70 <__gmpz_get_d@@Base>:
   1ab70:	ldrsw	x2, [x0, #4]
   1ab74:	cbz	w2, 1ab8c <__gmpz_get_d@@Base+0x1c>
   1ab78:	ldr	x0, [x0, #8]
   1ab7c:	cmp	x2, #0x0
   1ab80:	cneg	x1, x2, mi  // mi = first
   1ab84:	mov	x3, xzr
   1ab88:	b	bf40 <__gmpn_get_d@plt>
   1ab8c:	fmov	d0, xzr
   1ab90:	ret

000000000001ab94 <__gmpz_get_d_2exp@@Base>:
   1ab94:	ldrsw	x2, [x1, #4]
   1ab98:	cbz	w2, 1abcc <__gmpz_get_d_2exp@@Base+0x38>
   1ab9c:	ldr	x8, [x1, #8]
   1aba0:	cmp	x2, #0x0
   1aba4:	cneg	x1, x2, mi  // mi = first
   1aba8:	lsl	x10, x1, #6
   1abac:	add	x9, x8, x1, lsl #3
   1abb0:	ldur	x9, [x9, #-8]
   1abb4:	clz	x9, x9
   1abb8:	sub	x9, x10, x9
   1abbc:	neg	x3, x9
   1abc0:	str	x9, [x0]
   1abc4:	mov	x0, x8
   1abc8:	b	bf40 <__gmpn_get_d@plt>
   1abcc:	fmov	d0, xzr
   1abd0:	str	xzr, [x0]
   1abd4:	ret

000000000001abd8 <__gmpz_get_si@@Base>:
   1abd8:	ldr	x8, [x0, #8]
   1abdc:	ldr	w9, [x0, #4]
   1abe0:	ldr	x8, [x8]
   1abe4:	cmp	w9, #0x1
   1abe8:	b.lt	1abf4 <__gmpz_get_si@@Base+0x1c>  // b.tstop
   1abec:	and	x0, x8, #0x7fffffffffffffff
   1abf0:	ret
   1abf4:	tbnz	w9, #31, 1ac00 <__gmpz_get_si@@Base+0x28>
   1abf8:	mov	x0, xzr
   1abfc:	ret
   1ac00:	sub	x8, x8, #0x1
   1ac04:	orr	x8, x8, #0x8000000000000000
   1ac08:	eor	x0, x8, #0x7fffffffffffffff
   1ac0c:	ret

000000000001ac10 <__gmpz_get_str@@Base>:
   1ac10:	stp	x29, x30, [sp, #-80]!
   1ac14:	stp	x24, x23, [sp, #32]
   1ac18:	stp	x22, x21, [sp, #48]
   1ac1c:	stp	x20, x19, [sp, #64]
   1ac20:	ldrsw	x20, [x2, #4]
   1ac24:	mov	x22, x2
   1ac28:	mov	w21, w1
   1ac2c:	cmp	w1, #0x2
   1ac30:	mov	x19, x0
   1ac34:	str	x25, [sp, #16]
   1ac38:	mov	x29, sp
   1ac3c:	b.lt	1acac <__gmpz_get_str@@Base+0x9c>  // b.tstop
   1ac40:	cmp	w21, #0x25
   1ac44:	b.ge	1acc8 <__gmpz_get_str@@Base+0xb8>  // b.tcont
   1ac48:	adrp	x25, 59000 <__gmp_randget_mt@@Base+0x44c>
   1ac4c:	add	x25, x25, #0xc7d
   1ac50:	cbnz	x19, 1acdc <__gmpz_get_str@@Base+0xcc>
   1ac54:	cmp	x20, #0x0
   1ac58:	cneg	x8, x20, mi  // mi = first
   1ac5c:	cbz	x8, 1adf8 <__gmpz_get_str@@Base+0x1e8>
   1ac60:	ldr	x9, [x22, #8]
   1ac64:	adrp	x11, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ac68:	sub	w10, w21, #0x1
   1ac6c:	tst	w21, w10
   1ac70:	add	x9, x9, x8, lsl #3
   1ac74:	ldur	x9, [x9, #-8]
   1ac78:	ldr	x11, [x11, #3936]
   1ac7c:	lsl	x8, x8, #6
   1ac80:	mov	w10, #0x28                  	// #40
   1ac84:	clz	x9, x9
   1ac88:	sub	x8, x8, x9
   1ac8c:	mov	w9, w21
   1ac90:	madd	x9, x9, x10, x11
   1ac94:	b.ne	1ae00 <__gmpz_get_str@@Base+0x1f0>  // b.any
   1ac98:	ldrsw	x9, [x9, #24]
   1ac9c:	add	x8, x8, x9
   1aca0:	sub	x8, x8, #0x1
   1aca4:	udiv	x8, x8, x9
   1aca8:	b	1ae10 <__gmpz_get_str@@Base+0x200>
   1acac:	cmn	w21, #0x2
   1acb0:	b.le	1addc <__gmpz_get_str@@Base+0x1cc>
   1acb4:	mov	w21, #0xa                   	// #10
   1acb8:	adrp	x25, 59000 <__gmp_randget_mt@@Base+0x44c>
   1acbc:	add	x25, x25, #0xc3e
   1acc0:	cbnz	x19, 1acdc <__gmpz_get_str@@Base+0xcc>
   1acc4:	b	1ac54 <__gmpz_get_str@@Base+0x44>
   1acc8:	cmp	w21, #0x3e
   1accc:	b.gt	1ae4c <__gmpz_get_str@@Base+0x23c>
   1acd0:	adrp	x25, 59000 <__gmp_randget_mt@@Base+0x44c>
   1acd4:	add	x25, x25, #0xc3e
   1acd8:	cbz	x19, 1ac54 <__gmpz_get_str@@Base+0x44>
   1acdc:	mov	x23, xzr
   1ace0:	mov	x24, x19
   1ace4:	tbz	w20, #31, 1acf8 <__gmpz_get_str@@Base+0xe8>
   1ace8:	mov	w8, #0x2d                  	// #45
   1acec:	mov	x24, x19
   1acf0:	strb	w8, [x24], #1
   1acf4:	neg	x20, x20
   1acf8:	str	xzr, [x29, #24]
   1acfc:	ldr	x2, [x22, #8]
   1ad00:	sub	w8, w21, #0x1
   1ad04:	tst	w21, w8
   1ad08:	b.eq	1ad48 <__gmpz_get_str@@Base+0x138>  // b.none
   1ad0c:	lsl	x8, x20, #3
   1ad10:	orr	x1, x8, #0x8
   1ad14:	mov	w8, #0x7f00                	// #32512
   1ad18:	cmp	x1, x8
   1ad1c:	b.hi	1ae54 <__gmpz_get_str@@Base+0x244>  // b.pmore
   1ad20:	add	x9, x1, #0xf
   1ad24:	mov	x8, sp
   1ad28:	and	x9, x9, #0xfffffffffffffff0
   1ad2c:	sub	x22, x8, x9
   1ad30:	mov	sp, x22
   1ad34:	mov	x0, x22
   1ad38:	mov	x1, x2
   1ad3c:	mov	x2, x20
   1ad40:	bl	ca50 <__gmpn_copyi@plt>
   1ad44:	mov	x2, x22
   1ad48:	mov	x0, x24
   1ad4c:	mov	w1, w21
   1ad50:	mov	x3, x20
   1ad54:	bl	ca90 <__gmpn_get_str@plt>
   1ad58:	mov	x20, x0
   1ad5c:	cbz	x0, 1ad7c <__gmpz_get_str@@Base+0x16c>
   1ad60:	mov	x8, x20
   1ad64:	mov	x9, x24
   1ad68:	ldrb	w10, [x9]
   1ad6c:	subs	x8, x8, #0x1
   1ad70:	ldrb	w10, [x25, x10]
   1ad74:	strb	w10, [x9], #1
   1ad78:	b.ne	1ad68 <__gmpz_get_str@@Base+0x158>  // b.any
   1ad7c:	strb	wzr, [x24, x20]
   1ad80:	ldr	x0, [x29, #24]
   1ad84:	cbnz	x0, 1ae40 <__gmpz_get_str@@Base+0x230>
   1ad88:	cbz	x23, 1adbc <__gmpz_get_str@@Base+0x1ac>
   1ad8c:	sub	x8, x24, x19
   1ad90:	add	x8, x8, x20
   1ad94:	add	x2, x8, #0x1
   1ad98:	cmp	x23, x2
   1ad9c:	b.eq	1adbc <__gmpz_get_str@@Base+0x1ac>  // b.none
   1ada0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ada4:	ldr	x8, [x8, #3792]
   1ada8:	mov	x0, x19
   1adac:	mov	x1, x23
   1adb0:	ldr	x8, [x8]
   1adb4:	blr	x8
   1adb8:	mov	x19, x0
   1adbc:	mov	x0, x19
   1adc0:	mov	sp, x29
   1adc4:	ldp	x20, x19, [sp, #64]
   1adc8:	ldp	x22, x21, [sp, #48]
   1adcc:	ldp	x24, x23, [sp, #32]
   1add0:	ldr	x25, [sp, #16]
   1add4:	ldp	x29, x30, [sp], #80
   1add8:	ret
   1addc:	cmn	w21, #0x24
   1ade0:	b.lt	1ae4c <__gmpz_get_str@@Base+0x23c>  // b.tstop
   1ade4:	neg	w21, w21
   1ade8:	adrp	x25, 59000 <__gmp_randget_mt@@Base+0x44c>
   1adec:	add	x25, x25, #0xc3e
   1adf0:	cbnz	x19, 1acdc <__gmpz_get_str@@Base+0xcc>
   1adf4:	b	1ac54 <__gmpz_get_str@@Base+0x44>
   1adf8:	mov	w8, #0x1                   	// #1
   1adfc:	b	1ae10 <__gmpz_get_str@@Base+0x200>
   1ae00:	ldr	x9, [x9, #8]
   1ae04:	add	x9, x9, #0x1
   1ae08:	umulh	x8, x9, x8
   1ae0c:	add	x8, x8, #0x1
   1ae10:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ae14:	ldr	x9, [x9, #3840]
   1ae18:	ubfx	x10, x20, #31, #1
   1ae1c:	add	w10, w10, #0x1
   1ae20:	add	x23, x8, x10
   1ae24:	ldr	x9, [x9]
   1ae28:	mov	x0, x23
   1ae2c:	blr	x9
   1ae30:	mov	x19, x0
   1ae34:	mov	x24, x19
   1ae38:	tbz	w20, #31, 1acf8 <__gmpz_get_str@@Base+0xe8>
   1ae3c:	b	1ace8 <__gmpz_get_str@@Base+0xd8>
   1ae40:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1ae44:	cbnz	x23, 1ad8c <__gmpz_get_str@@Base+0x17c>
   1ae48:	b	1adbc <__gmpz_get_str@@Base+0x1ac>
   1ae4c:	mov	x19, xzr
   1ae50:	b	1adbc <__gmpz_get_str@@Base+0x1ac>
   1ae54:	add	x0, x29, #0x18
   1ae58:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1ae5c:	ldr	x2, [x22, #8]
   1ae60:	mov	x22, x0
   1ae64:	b	1ad34 <__gmpz_get_str@@Base+0x124>

000000000001ae68 <__gmpz_get_ui@@Base>:
   1ae68:	ldr	x8, [x0, #8]
   1ae6c:	ldr	w9, [x0, #4]
   1ae70:	ldr	x8, [x8]
   1ae74:	cmp	w9, #0x0
   1ae78:	csel	x0, xzr, x8, eq  // eq = none
   1ae7c:	ret

000000000001ae80 <__gmpz_getlimbn@@Base>:
   1ae80:	tbnz	x1, #63, 1aea4 <__gmpz_getlimbn@@Base+0x24>
   1ae84:	ldr	w8, [x0, #4]
   1ae88:	cmp	w8, #0x0
   1ae8c:	cneg	w8, w8, mi  // mi = first
   1ae90:	cmp	x8, x1
   1ae94:	b.le	1aea4 <__gmpz_getlimbn@@Base+0x24>
   1ae98:	ldr	x8, [x0, #8]
   1ae9c:	ldr	x0, [x8, x1, lsl #3]
   1aea0:	ret
   1aea4:	mov	x0, xzr
   1aea8:	ret

000000000001aeac <__gmpz_hamdist@@Base>:
   1aeac:	stp	x29, x30, [sp, #-80]!
   1aeb0:	stp	x24, x23, [sp, #32]
   1aeb4:	stp	x22, x21, [sp, #48]
   1aeb8:	stp	x20, x19, [sp, #64]
   1aebc:	ldrsw	x12, [x0, #4]
   1aec0:	ldrsw	x9, [x1, #4]
   1aec4:	ldr	x8, [x0, #8]
   1aec8:	ldr	x10, [x1, #8]
   1aecc:	str	x25, [sp, #16]
   1aed0:	mov	x29, sp
   1aed4:	tbnz	w12, #31, 1af1c <__gmpz_hamdist@@Base+0x70>
   1aed8:	tbnz	w9, #31, 1af20 <__gmpz_hamdist@@Base+0x74>
   1aedc:	cmp	w12, w9
   1aee0:	csel	w11, w12, w9, lt  // lt = tstop
   1aee4:	csel	w13, w12, w9, gt
   1aee8:	sxtw	x19, w11
   1aeec:	sxtw	x21, w13
   1aef0:	csel	x20, x10, x8, lt  // lt = tstop
   1aef4:	cbz	w11, 1af28 <__gmpz_hamdist@@Base+0x7c>
   1aef8:	cmp	w12, w9
   1aefc:	csel	x1, x8, x10, lt  // lt = tstop
   1af00:	mov	x0, x20
   1af04:	mov	x2, x19
   1af08:	bl	ce40 <__gmpn_hamdist@plt>
   1af0c:	mov	x22, x0
   1af10:	subs	x1, x21, x19
   1af14:	b.ne	1af34 <__gmpz_hamdist@@Base+0x88>  // b.any
   1af18:	b	1b0d4 <__gmpz_hamdist@@Base+0x228>
   1af1c:	tbnz	w9, #31, 1af3c <__gmpz_hamdist@@Base+0x90>
   1af20:	mov	x22, #0xffffffffffffffff    	// #-1
   1af24:	b	1b0d4 <__gmpz_hamdist@@Base+0x228>
   1af28:	mov	x22, xzr
   1af2c:	subs	x1, x21, x19
   1af30:	b.eq	1b0d4 <__gmpz_hamdist@@Base+0x228>  // b.none
   1af34:	add	x0, x20, x19, lsl #3
   1af38:	b	1b0cc <__gmpz_hamdist@@Base+0x220>
   1af3c:	mov	x11, xzr
   1af40:	neg	x24, x12
   1af44:	neg	x19, x9
   1af48:	ldr	x12, [x8, x11]
   1af4c:	ldr	x9, [x10, x11]
   1af50:	sub	x24, x24, #0x1
   1af54:	sub	x19, x19, #0x1
   1af58:	cbnz	x12, 1af78 <__gmpz_hamdist@@Base+0xcc>
   1af5c:	add	x11, x11, #0x8
   1af60:	cbz	x9, 1af48 <__gmpz_hamdist@@Base+0x9c>
   1af64:	add	x20, x10, x11
   1af68:	add	x21, x8, x11
   1af6c:	mov	x12, x9
   1af70:	mov	x9, xzr
   1af74:	b	1af94 <__gmpz_hamdist@@Base+0xe8>
   1af78:	mov	x0, x24
   1af7c:	add	x10, x10, x11
   1af80:	add	x8, x8, x11
   1af84:	add	x21, x10, #0x8
   1af88:	add	x20, x8, #0x8
   1af8c:	mov	x24, x19
   1af90:	mov	x19, x0
   1af94:	neg	x8, x12
   1af98:	neg	x10, x9
   1af9c:	eor	x8, x10, x8
   1afa0:	lsr	x10, x8, #1
   1afa4:	and	x10, x10, #0x5555555555555555
   1afa8:	sub	x8, x8, x10
   1afac:	lsr	x10, x8, #2
   1afb0:	and	x10, x10, #0x3333333333333333
   1afb4:	and	x8, x8, #0x3333333333333333
   1afb8:	add	x8, x10, x8
   1afbc:	add	x8, x8, x8, lsr #4
   1afc0:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   1afc4:	add	x8, x8, x8, lsr #8
   1afc8:	add	x8, x8, x8, lsr #16
   1afcc:	lsr	x10, x8, #32
   1afd0:	add	w8, w10, w8
   1afd4:	and	x22, x8, #0xff
   1afd8:	cbnz	x9, 1b080 <__gmpz_hamdist@@Base+0x1d4>
   1afdc:	mov	w8, #0x40                  	// #64
   1afe0:	sub	x25, x8, x22
   1afe4:	mov	w8, #0x1                   	// #1
   1afe8:	ldr	x23, [x21], #8
   1afec:	sub	x25, x25, #0x40
   1aff0:	sub	x8, x8, #0x1
   1aff4:	cbz	x23, 1afe8 <__gmpz_hamdist@@Base+0x13c>
   1aff8:	neg	x9, x8
   1affc:	cmp	x9, x19
   1b000:	csneg	x22, x19, x8, ge  // ge = tcont
   1b004:	add	x24, x24, x8
   1b008:	cbz	x22, 1b02c <__gmpz_hamdist@@Base+0x180>
   1b00c:	mov	x0, x20
   1b010:	mov	x1, x22
   1b014:	bl	cd80 <__gmpn_popcount@plt>
   1b018:	add	x8, x0, x25
   1b01c:	sub	x19, x19, x22
   1b020:	neg	x8, x8
   1b024:	add	x20, x20, x22, lsl #3
   1b028:	b	1b030 <__gmpz_hamdist@@Base+0x184>
   1b02c:	neg	x8, x25
   1b030:	sub	x24, x24, #0x1
   1b034:	sub	x9, x23, #0x1
   1b038:	cbz	x19, 1b048 <__gmpz_hamdist@@Base+0x19c>
   1b03c:	ldr	x10, [x20], #8
   1b040:	sub	x19, x19, #0x1
   1b044:	eor	x9, x10, x9
   1b048:	lsr	x10, x9, #1
   1b04c:	and	x10, x10, #0x5555555555555555
   1b050:	sub	x9, x9, x10
   1b054:	lsr	x10, x9, #2
   1b058:	and	x10, x10, #0x3333333333333333
   1b05c:	and	x9, x9, #0x3333333333333333
   1b060:	add	x9, x10, x9
   1b064:	add	x9, x9, x9, lsr #4
   1b068:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   1b06c:	add	x9, x9, x9, lsr #8
   1b070:	add	x9, x9, x9, lsr #16
   1b074:	lsr	x10, x9, #32
   1b078:	add	w9, w10, w9
   1b07c:	add	x22, x8, w9, uxtb
   1b080:	cmp	x19, x24
   1b084:	csel	x23, x19, x24, lt  // lt = tstop
   1b088:	cbz	x23, 1b0b4 <__gmpz_hamdist@@Base+0x208>
   1b08c:	mov	x0, x20
   1b090:	mov	x1, x21
   1b094:	mov	x2, x23
   1b098:	bl	ce40 <__gmpn_hamdist@plt>
   1b09c:	lsl	x8, x23, #3
   1b0a0:	add	x22, x0, x22
   1b0a4:	sub	x19, x19, x23
   1b0a8:	sub	x24, x24, x23
   1b0ac:	add	x20, x20, x8
   1b0b0:	add	x21, x21, x8
   1b0b4:	cbnz	x19, 1b0c4 <__gmpz_hamdist@@Base+0x218>
   1b0b8:	mov	x19, x24
   1b0bc:	mov	x20, x21
   1b0c0:	cbz	x24, 1b0d4 <__gmpz_hamdist@@Base+0x228>
   1b0c4:	mov	x0, x20
   1b0c8:	mov	x1, x19
   1b0cc:	bl	cd80 <__gmpn_popcount@plt>
   1b0d0:	add	x22, x0, x22
   1b0d4:	mov	x0, x22
   1b0d8:	ldp	x20, x19, [sp, #64]
   1b0dc:	ldp	x22, x21, [sp, #48]
   1b0e0:	ldp	x24, x23, [sp, #32]
   1b0e4:	ldr	x25, [sp, #16]
   1b0e8:	ldp	x29, x30, [sp], #80
   1b0ec:	ret

000000000001b0f0 <__gmpz_import@@Base>:
   1b0f0:	stp	x29, x30, [sp, #-96]!
   1b0f4:	stp	x26, x25, [sp, #32]
   1b0f8:	stp	x24, x23, [sp, #48]
   1b0fc:	stp	x22, x21, [sp, #64]
   1b100:	stp	x20, x19, [sp, #80]
   1b104:	lsl	x8, x3, #3
   1b108:	ldrsw	x9, [x0]
   1b10c:	str	x27, [sp, #16]
   1b110:	sub	x27, x8, x5
   1b114:	orr	x8, xzr, #0x3f
   1b118:	madd	x8, x27, x1, x8
   1b11c:	lsr	x20, x8, #6
   1b120:	mov	x22, x6
   1b124:	mov	x25, x5
   1b128:	mov	w26, w4
   1b12c:	mov	x23, x3
   1b130:	mov	x21, x1
   1b134:	mov	x19, x0
   1b138:	cmp	x20, x9
   1b13c:	mov	w24, w2
   1b140:	mov	x29, sp
   1b144:	b.gt	1b2b0 <__gmpz_import@@Base+0x1c0>
   1b148:	ldr	x0, [x19, #8]
   1b14c:	cmp	w26, #0x0
   1b150:	csinv	w10, w26, wzr, ne  // ne = any
   1b154:	cbz	x25, 1b2c8 <__gmpz_import@@Base+0x1d8>
   1b158:	sub	x11, x21, #0x1
   1b15c:	add	x8, x27, #0x7
   1b160:	cmp	w24, #0x0
   1b164:	mul	x11, x11, x23
   1b168:	sub	x14, x23, #0x1
   1b16c:	lsr	x8, x8, #3
   1b170:	cneg	x9, x23, ge  // ge = tcont
   1b174:	csel	x12, x11, xzr, ge  // ge = tcont
   1b178:	cmp	w10, #0x0
   1b17c:	cneg	x13, x8, lt  // lt = tstop
   1b180:	csel	x11, x14, xzr, ge  // ge = tcont
   1b184:	cbz	x21, 1b4f8 <__gmpz_import@@Base+0x408>
   1b188:	and	w8, w27, #0x7
   1b18c:	mov	x14, #0xffffffffffffffff    	// #-1
   1b190:	add	x12, x22, x12
   1b194:	sxtw	x16, w10
   1b198:	lsl	x10, x14, x8
   1b19c:	lsr	x15, x27, #3
   1b1a0:	add	x9, x13, x9
   1b1a4:	add	x13, x12, x11
   1b1a8:	mvn	x10, x10
   1b1ac:	neg	x11, x16
   1b1b0:	cbz	x15, 1b254 <__gmpz_import@@Base+0x164>
   1b1b4:	mov	w14, wzr
   1b1b8:	mov	x17, xzr
   1b1bc:	mov	x12, xzr
   1b1c0:	b	1b1dc <__gmpz_import@@Base+0xec>
   1b1c4:	add	x13, x13, x16
   1b1c8:	add	x13, x13, x11
   1b1cc:	add	x17, x17, #0x1
   1b1d0:	cmp	x17, x21
   1b1d4:	add	x13, x13, x9
   1b1d8:	b.eq	1b2a4 <__gmpz_import@@Base+0x1b4>  // b.none
   1b1dc:	mov	x18, x15
   1b1e0:	b	1b1fc <__gmpz_import@@Base+0x10c>
   1b1e4:	str	x12, [x0], #8
   1b1e8:	neg	w12, w14
   1b1ec:	lsr	x12, x1, x12
   1b1f0:	mov	w14, w2
   1b1f4:	subs	x18, x18, #0x1
   1b1f8:	b.eq	1b220 <__gmpz_import@@Base+0x130>  // b.none
   1b1fc:	ldrb	w1, [x13]
   1b200:	add	x13, x13, x11
   1b204:	subs	w2, w14, #0x38
   1b208:	lsl	x3, x1, x14
   1b20c:	orr	x12, x3, x12
   1b210:	b.ge	1b1e4 <__gmpz_import@@Base+0xf4>  // b.tcont
   1b214:	add	w14, w14, #0x8
   1b218:	subs	x18, x18, #0x1
   1b21c:	b.ne	1b1fc <__gmpz_import@@Base+0x10c>  // b.any
   1b220:	cbz	w8, 1b1c4 <__gmpz_import@@Base+0xd4>
   1b224:	ldrb	w18, [x13]
   1b228:	and	x18, x18, x10
   1b22c:	lsl	x2, x18, x14
   1b230:	add	w14, w14, w8
   1b234:	subs	w1, w14, #0x40
   1b238:	orr	x12, x2, x12
   1b23c:	b.lt	1b1c8 <__gmpz_import@@Base+0xd8>  // b.tstop
   1b240:	str	x12, [x0], #8
   1b244:	sub	w12, w8, w1
   1b248:	lsr	x12, x18, x12
   1b24c:	mov	w14, w1
   1b250:	b	1b1c8 <__gmpz_import@@Base+0xd8>
   1b254:	cbz	w8, 1b4f8 <__gmpz_import@@Base+0x408>
   1b258:	mov	w14, wzr
   1b25c:	mov	x12, xzr
   1b260:	b	1b270 <__gmpz_import@@Base+0x180>
   1b264:	subs	x21, x21, #0x1
   1b268:	add	x13, x13, x9
   1b26c:	b.eq	1b2a4 <__gmpz_import@@Base+0x1b4>  // b.none
   1b270:	ldrb	w15, [x13]
   1b274:	add	x13, x13, x11
   1b278:	and	x15, x15, x10
   1b27c:	lsl	x17, x15, x14
   1b280:	add	w14, w14, w8
   1b284:	subs	w16, w14, #0x40
   1b288:	orr	x12, x17, x12
   1b28c:	b.lt	1b264 <__gmpz_import@@Base+0x174>  // b.tstop
   1b290:	str	x12, [x0], #8
   1b294:	sub	w12, w8, w16
   1b298:	lsr	x12, x15, x12
   1b29c:	mov	w14, w16
   1b2a0:	b	1b264 <__gmpz_import@@Base+0x174>
   1b2a4:	cbz	w14, 1b4f8 <__gmpz_import@@Base+0x408>
   1b2a8:	str	x12, [x0]
   1b2ac:	b	1b4f8 <__gmpz_import@@Base+0x408>
   1b2b0:	mov	x0, x19
   1b2b4:	mov	x1, x20
   1b2b8:	bl	c080 <__gmpz_realloc@plt>
   1b2bc:	cmp	w26, #0x0
   1b2c0:	csinv	w10, w26, wzr, ne  // ne = any
   1b2c4:	cbnz	x25, 1b158 <__gmpz_import@@Base+0x68>
   1b2c8:	cmn	w24, #0x1
   1b2cc:	cset	w8, eq  // eq = none
   1b2d0:	cmp	x23, #0x8
   1b2d4:	cset	w9, eq  // eq = none
   1b2d8:	and	w9, w8, w9
   1b2dc:	cmp	w9, #0x1
   1b2e0:	and	x8, x22, #0x7
   1b2e4:	b.ne	1b304 <__gmpz_import@@Base+0x214>  // b.any
   1b2e8:	cmn	w10, #0x1
   1b2ec:	b.ne	1b304 <__gmpz_import@@Base+0x214>  // b.any
   1b2f0:	cbnz	x8, 1b304 <__gmpz_import@@Base+0x214>
   1b2f4:	mov	x1, x22
   1b2f8:	mov	x2, x21
   1b2fc:	bl	ca50 <__gmpn_copyi@plt>
   1b300:	b	1b4f8 <__gmpz_import@@Base+0x408>
   1b304:	cmp	w10, #0x1
   1b308:	cset	w11, ne  // ne = any
   1b30c:	eor	w9, w9, #0x1
   1b310:	orr	w9, w9, w11
   1b314:	tbnz	w9, #0, 1b3ac <__gmpz_import@@Base+0x2bc>
   1b318:	cbnz	x8, 1b3ac <__gmpz_import@@Base+0x2bc>
   1b31c:	cmp	x21, #0x1
   1b320:	b.lt	1b4f8 <__gmpz_import@@Base+0x408>  // b.tstop
   1b324:	cmp	x21, #0x1
   1b328:	b.eq	1b348 <__gmpz_import@@Base+0x258>  // b.none
   1b32c:	lsl	x8, x21, #3
   1b330:	add	x9, x22, x8
   1b334:	cmp	x9, x0
   1b338:	b.ls	1b418 <__gmpz_import@@Base+0x328>  // b.plast
   1b33c:	add	x8, x0, x8
   1b340:	cmp	x8, x22
   1b344:	b.ls	1b418 <__gmpz_import@@Base+0x328>  // b.plast
   1b348:	mov	x10, xzr
   1b34c:	mov	x8, x0
   1b350:	mov	x9, x22
   1b354:	sub	x10, x21, x10
   1b358:	ldr	x11, [x9], #8
   1b35c:	subs	x10, x10, #0x1
   1b360:	lsl	x14, x11, #40
   1b364:	and	x14, x14, #0xff000000000000
   1b368:	lsr	x13, x11, #16
   1b36c:	bfi	x14, x11, #56, #8
   1b370:	lsr	x12, x11, #24
   1b374:	bfi	x14, x13, #40, #8
   1b378:	lsr	x13, x11, #8
   1b37c:	and	x13, x13, #0xff000000
   1b380:	bfi	x14, x12, #32, #8
   1b384:	orr	x13, x14, x13
   1b388:	lsr	x14, x11, #40
   1b38c:	and	x12, x12, #0xff0000
   1b390:	and	x14, x14, #0xff00
   1b394:	orr	x12, x13, x12
   1b398:	orr	x12, x12, x14
   1b39c:	add	x11, x12, x11, lsr #56
   1b3a0:	str	x11, [x8], #8
   1b3a4:	b.ne	1b358 <__gmpz_import@@Base+0x268>  // b.any
   1b3a8:	b	1b4f8 <__gmpz_import@@Base+0x408>
   1b3ac:	cmp	w24, #0x1
   1b3b0:	b.ne	1b158 <__gmpz_import@@Base+0x68>  // b.any
   1b3b4:	cmp	x23, #0x8
   1b3b8:	b.ne	1b158 <__gmpz_import@@Base+0x68>  // b.any
   1b3bc:	cmn	w10, #0x1
   1b3c0:	b.ne	1b158 <__gmpz_import@@Base+0x68>  // b.any
   1b3c4:	cbnz	x8, 1b158 <__gmpz_import@@Base+0x68>
   1b3c8:	cmp	x21, #0x1
   1b3cc:	b.lt	1b4f8 <__gmpz_import@@Base+0x408>  // b.tstop
   1b3d0:	cmp	x21, #0x4
   1b3d4:	add	x8, x22, x21, lsl #3
   1b3d8:	b.cc	1b3f8 <__gmpz_import@@Base+0x308>  // b.lo, b.ul, b.last
   1b3dc:	lsl	x9, x21, #3
   1b3e0:	add	x10, x22, x9
   1b3e4:	cmp	x10, x0
   1b3e8:	b.ls	1b4b0 <__gmpz_import@@Base+0x3c0>  // b.plast
   1b3ec:	add	x9, x0, x9
   1b3f0:	cmp	x9, x22
   1b3f4:	b.ls	1b4b0 <__gmpz_import@@Base+0x3c0>  // b.plast
   1b3f8:	mov	x9, xzr
   1b3fc:	sub	x8, x8, #0x8
   1b400:	sub	x9, x21, x9
   1b404:	ldr	x10, [x8], #-8
   1b408:	subs	x9, x9, #0x1
   1b40c:	str	x10, [x0], #8
   1b410:	b.ne	1b404 <__gmpz_import@@Base+0x314>  // b.any
   1b414:	b	1b4f8 <__gmpz_import@@Base+0x408>
   1b418:	and	x10, x21, #0xfffffffffffffffe
   1b41c:	lsl	x9, x10, #3
   1b420:	movi	v0.2d, #0xff000000000000
   1b424:	movi	v1.2d, #0xff0000000000
   1b428:	movi	v2.2d, #0xff00000000
   1b42c:	movi	v3.2d, #0xff000000
   1b430:	movi	v4.2d, #0xff0000
   1b434:	add	x8, x0, x9
   1b438:	add	x9, x22, x9
   1b43c:	movi	v5.2d, #0xff00
   1b440:	mov	x11, x10
   1b444:	ldr	q6, [x22], #16
   1b448:	subs	x11, x11, #0x2
   1b44c:	shl	v16.2d, v6.2d, #40
   1b450:	shl	v7.2d, v6.2d, #56
   1b454:	and	v16.16b, v16.16b, v0.16b
   1b458:	orr	v7.16b, v16.16b, v7.16b
   1b45c:	shl	v16.2d, v6.2d, #24
   1b460:	and	v16.16b, v16.16b, v1.16b
   1b464:	orr	v7.16b, v7.16b, v16.16b
   1b468:	shl	v16.2d, v6.2d, #8
   1b46c:	and	v16.16b, v16.16b, v2.16b
   1b470:	orr	v7.16b, v7.16b, v16.16b
   1b474:	ushr	v16.2d, v6.2d, #8
   1b478:	and	v16.16b, v16.16b, v3.16b
   1b47c:	orr	v7.16b, v7.16b, v16.16b
   1b480:	ushr	v16.2d, v6.2d, #24
   1b484:	and	v16.16b, v16.16b, v4.16b
   1b488:	orr	v7.16b, v7.16b, v16.16b
   1b48c:	ushr	v16.2d, v6.2d, #40
   1b490:	and	v16.16b, v16.16b, v5.16b
   1b494:	orr	v7.16b, v7.16b, v16.16b
   1b498:	usra	v7.2d, v6.2d, #56
   1b49c:	str	q7, [x0], #16
   1b4a0:	b.ne	1b444 <__gmpz_import@@Base+0x354>  // b.any
   1b4a4:	cmp	x10, x21
   1b4a8:	b.ne	1b354 <__gmpz_import@@Base+0x264>  // b.any
   1b4ac:	b	1b4f8 <__gmpz_import@@Base+0x408>
   1b4b0:	and	x9, x21, #0xfffffffffffffffc
   1b4b4:	add	x11, x22, x21, lsl #3
   1b4b8:	lsl	x12, x9, #3
   1b4bc:	add	x10, x0, #0x10
   1b4c0:	sub	x8, x8, x12
   1b4c4:	add	x0, x0, x12
   1b4c8:	sub	x11, x11, #0x10
   1b4cc:	mov	x12, x9
   1b4d0:	ldp	q1, q0, [x11, #-16]
   1b4d4:	subs	x12, x12, #0x4
   1b4d8:	sub	x11, x11, #0x20
   1b4dc:	ext	v0.16b, v0.16b, v0.16b, #8
   1b4e0:	ext	v1.16b, v1.16b, v1.16b, #8
   1b4e4:	stp	q0, q1, [x10, #-16]
   1b4e8:	add	x10, x10, #0x20
   1b4ec:	b.ne	1b4d0 <__gmpz_import@@Base+0x3e0>  // b.any
   1b4f0:	cmp	x9, x21
   1b4f4:	b.ne	1b3fc <__gmpz_import@@Base+0x30c>  // b.any
   1b4f8:	ldr	x8, [x19, #8]
   1b4fc:	sub	x8, x8, #0x8
   1b500:	subs	x9, x20, #0x1
   1b504:	b.lt	1b51c <__gmpz_import@@Base+0x42c>  // b.tstop
   1b508:	ldr	x10, [x8, x20, lsl #3]
   1b50c:	mov	x20, x9
   1b510:	cbz	x10, 1b500 <__gmpz_import@@Base+0x410>
   1b514:	add	x8, x9, #0x1
   1b518:	b	1b520 <__gmpz_import@@Base+0x430>
   1b51c:	mov	x8, xzr
   1b520:	str	w8, [x19, #4]
   1b524:	ldp	x20, x19, [sp, #80]
   1b528:	ldp	x22, x21, [sp, #64]
   1b52c:	ldp	x24, x23, [sp, #48]
   1b530:	ldp	x26, x25, [sp, #32]
   1b534:	ldr	x27, [sp, #16]
   1b538:	ldp	x29, x30, [sp], #96
   1b53c:	ret

000000000001b540 <__gmpz_init@@Base>:
   1b540:	adrp	x8, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1b544:	add	x8, x8, #0xf0
   1b548:	stp	xzr, x8, [x0]
   1b54c:	ret

000000000001b550 <__gmpz_init2@@Base>:
   1b550:	stp	x29, x30, [sp, #-32]!
   1b554:	cmp	x1, #0x0
   1b558:	cset	w8, ne  // ne = any
   1b55c:	sub	x8, x1, x8
   1b560:	mov	x9, #0x1fffffffc0          	// #137438953408
   1b564:	cmp	x8, x9
   1b568:	stp	x20, x19, [sp, #16]
   1b56c:	mov	x29, sp
   1b570:	b.cs	1b5a8 <__gmpz_init2@@Base+0x58>  // b.hs, b.nlast
   1b574:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b578:	ldr	x9, [x9, #3840]
   1b57c:	lsr	x8, x8, #6
   1b580:	add	x20, x8, #0x1
   1b584:	mov	x19, x0
   1b588:	ldr	x9, [x9]
   1b58c:	lsl	x0, x20, #3
   1b590:	blr	x9
   1b594:	str	x0, [x19, #8]
   1b598:	stp	w20, wzr, [x19]
   1b59c:	ldp	x20, x19, [sp, #16]
   1b5a0:	ldp	x29, x30, [sp], #32
   1b5a4:	ret
   1b5a8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b5ac:	ldr	x8, [x8, #3824]
   1b5b0:	adrp	x0, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1b5b4:	add	x0, x0, #0xf8
   1b5b8:	mov	w1, #0x1a                  	// #26
   1b5bc:	ldr	x3, [x8]
   1b5c0:	mov	w2, #0x1                   	// #1
   1b5c4:	bl	ce30 <fwrite@plt>
   1b5c8:	bl	c900 <abort@plt>

000000000001b5cc <__gmpz_inits@@Base>:
   1b5cc:	sub	sp, sp, #0xe0
   1b5d0:	mov	x8, #0xffffffffffffffc8    	// #-56
   1b5d4:	mov	x9, sp
   1b5d8:	movk	x8, #0xff80, lsl #32
   1b5dc:	add	x9, x9, #0x80
   1b5e0:	add	x10, sp, #0x88
   1b5e4:	stp	x9, x8, [sp, #208]
   1b5e8:	adrp	x8, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1b5ec:	add	x11, sp, #0xe0
   1b5f0:	add	x10, x10, #0x38
   1b5f4:	add	x8, x8, #0x118
   1b5f8:	stp	x1, x2, [sp, #136]
   1b5fc:	stp	x3, x4, [sp, #152]
   1b600:	stp	x5, x6, [sp, #168]
   1b604:	stp	q0, q1, [sp]
   1b608:	stp	q2, q3, [sp, #32]
   1b60c:	stp	q4, q5, [sp, #64]
   1b610:	stp	q6, q7, [sp, #96]
   1b614:	str	x10, [sp, #200]
   1b618:	stp	x7, x11, [sp, #184]
   1b61c:	b	1b634 <__gmpz_inits@@Base+0x68>
   1b620:	ldr	x9, [sp, #192]
   1b624:	add	x10, x9, #0x8
   1b628:	str	x10, [sp, #192]
   1b62c:	ldr	x0, [x9]
   1b630:	cbz	x0, 1b660 <__gmpz_inits@@Base+0x94>
   1b634:	stp	xzr, x8, [x0]
   1b638:	ldrsw	x9, [sp, #216]
   1b63c:	tbz	w9, #31, 1b620 <__gmpz_inits@@Base+0x54>
   1b640:	add	w10, w9, #0x8
   1b644:	cmn	w9, #0x8
   1b648:	str	w10, [sp, #216]
   1b64c:	b.gt	1b620 <__gmpz_inits@@Base+0x54>
   1b650:	ldr	x10, [sp, #200]
   1b654:	add	x9, x10, x9
   1b658:	ldr	x0, [x9]
   1b65c:	cbnz	x0, 1b634 <__gmpz_inits@@Base+0x68>
   1b660:	add	sp, sp, #0xe0
   1b664:	ret

000000000001b668 <__gmpz_inp_raw@@Base>:
   1b668:	sub	sp, sp, #0x50
   1b66c:	stp	x29, x30, [sp, #16]
   1b670:	stp	x24, x23, [sp, #32]
   1b674:	stp	x22, x21, [sp, #48]
   1b678:	stp	x20, x19, [sp, #64]
   1b67c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b680:	ldr	x8, [x8, #3888]
   1b684:	cmp	x1, #0x0
   1b688:	add	x29, sp, #0x10
   1b68c:	mov	x19, x0
   1b690:	ldr	x8, [x8]
   1b694:	sub	x0, x29, #0x4
   1b698:	mov	w2, #0x1                   	// #1
   1b69c:	csel	x23, x8, x1, eq  // eq = none
   1b6a0:	mov	w1, #0x4                   	// #4
   1b6a4:	mov	x3, x23
   1b6a8:	bl	cb90 <fread@plt>
   1b6ac:	cmp	x0, #0x1
   1b6b0:	b.ne	1b7f0 <__gmpz_inp_raw@@Base+0x188>  // b.any
   1b6b4:	ldur	w8, [x29, #-4]
   1b6b8:	lsl	x8, x8, #32
   1b6bc:	rev	x8, x8
   1b6c0:	lsr	x10, x8, #31
   1b6c4:	orr	x9, x8, #0xffffffff00000000
   1b6c8:	cmp	x10, #0x0
   1b6cc:	csel	x24, x8, x9, eq  // eq = none
   1b6d0:	cmp	x24, #0x0
   1b6d4:	cneg	x20, x24, mi  // mi = first
   1b6d8:	lsl	x8, x20, #3
   1b6dc:	add	x8, x8, #0x3f
   1b6e0:	lsr	x21, x8, #6
   1b6e4:	cbz	x21, 1b7f8 <__gmpz_inp_raw@@Base+0x190>
   1b6e8:	ldrsw	x8, [x19]
   1b6ec:	cmp	x21, x8
   1b6f0:	b.gt	1b828 <__gmpz_inp_raw@@Base+0x1c0>
   1b6f4:	ldr	x22, [x19, #8]
   1b6f8:	add	x8, x22, x21, lsl #3
   1b6fc:	sub	x0, x8, x20
   1b700:	mov	w2, #0x1                   	// #1
   1b704:	mov	x1, x20
   1b708:	mov	x3, x23
   1b70c:	str	xzr, [x22]
   1b710:	bl	cb90 <fread@plt>
   1b714:	cmp	x0, #0x1
   1b718:	mov	x0, xzr
   1b71c:	b.ne	1b810 <__gmpz_inp_raw@@Base+0x1a8>  // b.any
   1b720:	add	x8, x21, #0x1
   1b724:	lsr	x8, x8, #1
   1b728:	cbz	x8, 1b7d0 <__gmpz_inp_raw@@Base+0x168>
   1b72c:	add	x9, x22, x21, lsl #3
   1b730:	sub	x9, x9, #0x8
   1b734:	mov	x10, x22
   1b738:	ldr	x11, [x9]
   1b73c:	ldr	x12, [x10]
   1b740:	subs	x8, x8, #0x1
   1b744:	lsl	x15, x11, #40
   1b748:	and	x15, x15, #0xff000000000000
   1b74c:	lsr	x14, x11, #16
   1b750:	bfi	x15, x11, #56, #8
   1b754:	bfi	x15, x14, #40, #8
   1b758:	lsl	x14, x12, #40
   1b75c:	lsr	x13, x11, #24
   1b760:	lsr	x16, x11, #8
   1b764:	and	x14, x14, #0xff000000000000
   1b768:	lsr	x17, x12, #16
   1b76c:	bfi	x14, x12, #56, #8
   1b770:	and	x16, x16, #0xff000000
   1b774:	bfi	x15, x13, #32, #8
   1b778:	bfi	x14, x17, #40, #8
   1b77c:	lsr	x17, x12, #24
   1b780:	orr	x15, x15, x16
   1b784:	lsr	x16, x12, #8
   1b788:	and	x16, x16, #0xff000000
   1b78c:	bfi	x14, x17, #32, #8
   1b790:	and	x13, x13, #0xff0000
   1b794:	orr	x14, x14, x16
   1b798:	orr	x13, x15, x13
   1b79c:	and	x15, x17, #0xff0000
   1b7a0:	orr	x14, x14, x15
   1b7a4:	lsr	x15, x11, #40
   1b7a8:	and	x15, x15, #0xff00
   1b7ac:	orr	x13, x13, x15
   1b7b0:	lsr	x15, x12, #40
   1b7b4:	and	x15, x15, #0xff00
   1b7b8:	orr	x14, x14, x15
   1b7bc:	add	x11, x13, x11, lsr #56
   1b7c0:	add	x12, x14, x12, lsr #56
   1b7c4:	str	x11, [x10], #8
   1b7c8:	str	x12, [x9], #-8
   1b7cc:	b.ne	1b738 <__gmpz_inp_raw@@Base+0xd0>  // b.any
   1b7d0:	sub	x8, x22, #0x8
   1b7d4:	subs	x9, x21, #0x1
   1b7d8:	b.lt	1b7f8 <__gmpz_inp_raw@@Base+0x190>  // b.tstop
   1b7dc:	ldr	x10, [x8, x21, lsl #3]
   1b7e0:	mov	x21, x9
   1b7e4:	cbz	x10, 1b7d4 <__gmpz_inp_raw@@Base+0x16c>
   1b7e8:	add	x8, x9, #0x1
   1b7ec:	b	1b7fc <__gmpz_inp_raw@@Base+0x194>
   1b7f0:	mov	x0, xzr
   1b7f4:	b	1b810 <__gmpz_inp_raw@@Base+0x1a8>
   1b7f8:	mov	x8, xzr
   1b7fc:	neg	w9, w8
   1b800:	cmp	x24, #0x0
   1b804:	csel	x8, x8, x9, ge  // ge = tcont
   1b808:	add	x0, x20, #0x4
   1b80c:	str	w8, [x19, #4]
   1b810:	ldp	x20, x19, [sp, #64]
   1b814:	ldp	x22, x21, [sp, #48]
   1b818:	ldp	x24, x23, [sp, #32]
   1b81c:	ldp	x29, x30, [sp, #16]
   1b820:	add	sp, sp, #0x50
   1b824:	ret
   1b828:	mov	x0, x19
   1b82c:	mov	x1, x21
   1b830:	bl	c080 <__gmpz_realloc@plt>
   1b834:	mov	x22, x0
   1b838:	b	1b6f8 <__gmpz_inp_raw@@Base+0x90>

000000000001b83c <__gmpz_inp_str@@Base>:
   1b83c:	stp	x29, x30, [sp, #-64]!
   1b840:	str	x23, [sp, #16]
   1b844:	stp	x22, x21, [sp, #32]
   1b848:	stp	x20, x19, [sp, #48]
   1b84c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b850:	ldr	x8, [x8, #3888]
   1b854:	cmp	x1, #0x0
   1b858:	mov	w19, w2
   1b85c:	mov	x21, x0
   1b860:	ldr	x8, [x8]
   1b864:	mov	x20, xzr
   1b868:	mov	x29, sp
   1b86c:	csel	x22, x8, x1, eq  // eq = none
   1b870:	mov	x0, x22
   1b874:	bl	c7f0 <getc@plt>
   1b878:	mov	w23, w0
   1b87c:	add	x20, x20, #0x1
   1b880:	bl	cae0 <__ctype_b_loc@plt>
   1b884:	ldr	x8, [x0]
   1b888:	ldrh	w8, [x8, w23, sxtw #1]
   1b88c:	tbnz	w8, #13, 1b870 <__gmpz_inp_str@@Base+0x34>
   1b890:	mov	x0, x21
   1b894:	mov	x1, x22
   1b898:	mov	w2, w19
   1b89c:	mov	w3, w23
   1b8a0:	mov	x4, x20
   1b8a4:	ldp	x20, x19, [sp, #48]
   1b8a8:	ldp	x22, x21, [sp, #32]
   1b8ac:	ldr	x23, [sp, #16]
   1b8b0:	ldp	x29, x30, [sp], #64
   1b8b4:	b	c9d0 <__gmpz_inp_str_nowhite@plt>

000000000001b8b8 <__gmpz_inp_str_nowhite@@Base>:
   1b8b8:	sub	sp, sp, #0x70
   1b8bc:	stp	x29, x30, [sp, #16]
   1b8c0:	stp	x28, x27, [sp, #32]
   1b8c4:	stp	x26, x25, [sp, #48]
   1b8c8:	stp	x24, x23, [sp, #64]
   1b8cc:	stp	x22, x21, [sp, #80]
   1b8d0:	stp	x20, x19, [sp, #96]
   1b8d4:	adrp	x28, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b8d8:	ldr	x28, [x28, #3920]
   1b8dc:	mov	x20, x4
   1b8e0:	mov	w23, w3
   1b8e4:	mov	w22, w2
   1b8e8:	mov	x21, x1
   1b8ec:	mov	x26, x0
   1b8f0:	cmp	w2, #0x25
   1b8f4:	add	x29, sp, #0x10
   1b8f8:	b.lt	1b908 <__gmpz_inp_str_nowhite@@Base+0x50>  // b.tstop
   1b8fc:	cmp	w22, #0x3e
   1b900:	b.gt	1b98c <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b904:	add	x28, x28, #0xd0
   1b908:	cmp	w23, #0x2d
   1b90c:	b.ne	1b930 <__gmpz_inp_str_nowhite@@Base+0x78>  // b.any
   1b910:	mov	x0, x21
   1b914:	bl	c7f0 <getc@plt>
   1b918:	mov	w23, w0
   1b91c:	add	x20, x20, #0x1
   1b920:	mov	w27, #0x1                   	// #1
   1b924:	cmn	w23, #0x1
   1b928:	b.ne	1b93c <__gmpz_inp_str_nowhite@@Base+0x84>  // b.any
   1b92c:	b	1b98c <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b930:	mov	w27, wzr
   1b934:	cmn	w23, #0x1
   1b938:	b.eq	1b98c <__gmpz_inp_str_nowhite@@Base+0xd4>  // b.none
   1b93c:	ldrb	w8, [x28, w23, sxtw]
   1b940:	cmp	w22, #0x0
   1b944:	mov	w9, #0xa                   	// #10
   1b948:	csel	w9, w9, w22, eq  // eq = none
   1b94c:	cmp	w9, w8
   1b950:	b.le	1b98c <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b954:	cbnz	w22, 1b998 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b958:	cmp	w23, #0x30
   1b95c:	b.ne	1b994 <__gmpz_inp_str_nowhite@@Base+0xdc>  // b.any
   1b960:	mov	x0, x21
   1b964:	bl	c7f0 <getc@plt>
   1b968:	orr	w8, w0, #0x20
   1b96c:	cmp	w8, #0x78
   1b970:	b.ne	1bb10 <__gmpz_inp_str_nowhite@@Base+0x258>  // b.any
   1b974:	mov	x0, x21
   1b978:	bl	c7f0 <getc@plt>
   1b97c:	mov	w23, w0
   1b980:	add	x20, x20, #0x2
   1b984:	mov	w22, #0x10                  	// #16
   1b988:	b	1b998 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b98c:	mov	x20, xzr
   1b990:	b	1baec <__gmpz_inp_str_nowhite@@Base+0x234>
   1b994:	mov	w22, #0xa                   	// #10
   1b998:	cmp	w23, #0x30
   1b99c:	b.ne	1b9b8 <__gmpz_inp_str_nowhite@@Base+0x100>  // b.any
   1b9a0:	mov	x0, x21
   1b9a4:	bl	c7f0 <getc@plt>
   1b9a8:	cmp	w0, #0x30
   1b9ac:	add	x20, x20, #0x1
   1b9b0:	b.eq	1b9a0 <__gmpz_inp_str_nowhite@@Base+0xe8>  // b.none
   1b9b4:	mov	w23, w0
   1b9b8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1b9bc:	ldr	x8, [x8, #3840]
   1b9c0:	mov	w0, #0x64                  	// #100
   1b9c4:	mov	w24, #0x64                  	// #100
   1b9c8:	ldr	x8, [x8]
   1b9cc:	blr	x8
   1b9d0:	cmn	w23, #0x1
   1b9d4:	mov	x25, x0
   1b9d8:	b.eq	1ba54 <__gmpz_inp_str_nowhite@@Base+0x19c>  // b.none
   1b9dc:	str	x26, [sp, #8]
   1b9e0:	mov	x26, xzr
   1b9e4:	mov	w24, #0x64                  	// #100
   1b9e8:	str	w27, [sp, #4]
   1b9ec:	b	1ba10 <__gmpz_inp_str_nowhite@@Base+0x158>
   1b9f0:	mov	x0, x21
   1b9f4:	add	x19, x26, #0x1
   1b9f8:	strb	w27, [x25, x26]
   1b9fc:	bl	c7f0 <getc@plt>
   1ba00:	mov	w23, w0
   1ba04:	cmn	w0, #0x1
   1ba08:	mov	x26, x19
   1ba0c:	b.eq	1ba60 <__gmpz_inp_str_nowhite@@Base+0x1a8>  // b.none
   1ba10:	ldrb	w27, [x28, w23, sxtw]
   1ba14:	cmp	w22, w27
   1ba18:	b.le	1ba5c <__gmpz_inp_str_nowhite@@Base+0x1a4>
   1ba1c:	cmp	x26, x24
   1ba20:	b.cc	1b9f0 <__gmpz_inp_str_nowhite@@Base+0x138>  // b.lo, b.ul, b.last
   1ba24:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ba28:	ldr	x8, [x8, #3792]
   1ba2c:	add	x9, x24, x24, lsl #1
   1ba30:	lsr	x23, x9, #1
   1ba34:	mov	x0, x25
   1ba38:	ldr	x8, [x8]
   1ba3c:	mov	x1, x24
   1ba40:	mov	x2, x23
   1ba44:	blr	x8
   1ba48:	mov	x25, x0
   1ba4c:	mov	x24, x23
   1ba50:	b	1b9f0 <__gmpz_inp_str_nowhite@@Base+0x138>
   1ba54:	mov	x19, xzr
   1ba58:	b	1ba68 <__gmpz_inp_str_nowhite@@Base+0x1b0>
   1ba5c:	mov	x19, x26
   1ba60:	ldr	x26, [sp, #8]
   1ba64:	ldr	w27, [sp, #4]
   1ba68:	mov	w0, w23
   1ba6c:	mov	x1, x21
   1ba70:	bl	cc50 <ungetc@plt>
   1ba74:	add	x8, x20, x19
   1ba78:	sub	x20, x8, #0x1
   1ba7c:	cbz	x19, 1bacc <__gmpz_inp_str_nowhite@@Base+0x214>
   1ba80:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ba84:	ldr	x8, [x8, #3936]
   1ba88:	mov	w9, #0x28                  	// #40
   1ba8c:	smaddl	x8, w22, w9, x8
   1ba90:	ldr	x8, [x8, #16]
   1ba94:	ldrsw	x9, [x26]
   1ba98:	umulh	x8, x8, x19
   1ba9c:	ubfx	x8, x8, #3, #58
   1baa0:	add	x1, x8, #0x2
   1baa4:	cmp	x1, x9
   1baa8:	b.gt	1bb40 <__gmpz_inp_str_nowhite@@Base+0x288>
   1baac:	ldr	x0, [x26, #8]
   1bab0:	mov	x1, x25
   1bab4:	mov	x2, x19
   1bab8:	mov	w3, w22
   1babc:	bl	c090 <__gmpn_set_str@plt>
   1bac0:	cmp	w27, #0x0
   1bac4:	cneg	w8, w0, ne  // ne = any
   1bac8:	b	1bad0 <__gmpz_inp_str_nowhite@@Base+0x218>
   1bacc:	mov	w8, wzr
   1bad0:	str	w8, [x26, #4]
   1bad4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1bad8:	ldr	x8, [x8, #4016]
   1badc:	mov	x0, x25
   1bae0:	mov	x1, x24
   1bae4:	ldr	x8, [x8]
   1bae8:	blr	x8
   1baec:	mov	x0, x20
   1baf0:	ldp	x20, x19, [sp, #96]
   1baf4:	ldp	x22, x21, [sp, #80]
   1baf8:	ldp	x24, x23, [sp, #64]
   1bafc:	ldp	x26, x25, [sp, #48]
   1bb00:	ldp	x28, x27, [sp, #32]
   1bb04:	ldp	x29, x30, [sp, #16]
   1bb08:	add	sp, sp, #0x70
   1bb0c:	ret
   1bb10:	cmp	w8, #0x62
   1bb14:	b.ne	1bb30 <__gmpz_inp_str_nowhite@@Base+0x278>  // b.any
   1bb18:	mov	x0, x21
   1bb1c:	bl	c7f0 <getc@plt>
   1bb20:	mov	w23, w0
   1bb24:	add	x20, x20, #0x2
   1bb28:	mov	w22, #0x2                   	// #2
   1bb2c:	b	1b998 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1bb30:	mov	w23, w0
   1bb34:	add	x20, x20, #0x1
   1bb38:	mov	w22, #0x8                   	// #8
   1bb3c:	b	1b998 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1bb40:	mov	x0, x26
   1bb44:	bl	c080 <__gmpz_realloc@plt>
   1bb48:	b	1baac <__gmpz_inp_str_nowhite@@Base+0x1f4>

000000000001bb4c <__gmpz_invert@@Base>:
   1bb4c:	stp	x29, x30, [sp, #-64]!
   1bb50:	stp	x24, x23, [sp, #16]
   1bb54:	stp	x22, x21, [sp, #32]
   1bb58:	stp	x20, x19, [sp, #48]
   1bb5c:	mov	x29, sp
   1bb60:	sub	sp, sp, #0x30
   1bb64:	ldr	w8, [x1, #4]
   1bb68:	ldr	w9, [x2, #4]
   1bb6c:	mov	x19, x2
   1bb70:	mov	x21, x1
   1bb74:	cmp	w8, #0x0
   1bb78:	cneg	w8, w8, mi  // mi = first
   1bb7c:	cmp	w9, #0x0
   1bb80:	cneg	w9, w9, mi  // mi = first
   1bb84:	cmp	x8, x9
   1bb88:	csel	x8, x8, x9, hi  // hi = pmore
   1bb8c:	add	x24, x8, #0x1
   1bb90:	mov	x20, x0
   1bb94:	cmp	x8, #0xfdf
   1bb98:	lsl	x23, x24, #3
   1bb9c:	stur	xzr, [x29, #-40]
   1bba0:	stur	w24, [x29, #-16]
   1bba4:	b.hi	1bc9c <__gmpz_invert@@Base+0x150>  // b.pmore
   1bba8:	add	x9, x23, #0xf
   1bbac:	mov	x8, sp
   1bbb0:	and	x9, x9, #0x1ffffffff0
   1bbb4:	sub	x8, x8, x9
   1bbb8:	mov	sp, x8
   1bbbc:	stur	x8, [x29, #-8]
   1bbc0:	mov	x8, sp
   1bbc4:	sub	x22, x29, #0x20
   1bbc8:	sub	x0, x8, x9
   1bbcc:	stur	w24, [x29, #-32]
   1bbd0:	mov	sp, x0
   1bbd4:	stur	x0, [x29, #-24]
   1bbd8:	sub	x0, x29, #0x10
   1bbdc:	mov	x1, x22
   1bbe0:	mov	x2, xzr
   1bbe4:	mov	x3, x21
   1bbe8:	mov	x4, x19
   1bbec:	bl	c0c0 <__gmpz_gcdext@plt>
   1bbf0:	ldur	w8, [x29, #-12]
   1bbf4:	cmp	w8, #0x1
   1bbf8:	b.ne	1bc2c <__gmpz_invert@@Base+0xe0>  // b.any
   1bbfc:	ldur	x8, [x29, #-8]
   1bc00:	ldr	x8, [x8]
   1bc04:	cmp	x8, #0x1
   1bc08:	b.ne	1bc2c <__gmpz_invert@@Base+0xe0>  // b.any
   1bc0c:	ldur	w8, [x29, #-28]
   1bc10:	tbnz	w8, #31, 1bc40 <__gmpz_invert@@Base+0xf4>
   1bc14:	mov	x0, x20
   1bc18:	mov	x1, x22
   1bc1c:	bl	c420 <__gmpz_set@plt>
   1bc20:	ldur	x0, [x29, #-40]
   1bc24:	cbz	x0, 1bc60 <__gmpz_invert@@Base+0x114>
   1bc28:	b	1bc94 <__gmpz_invert@@Base+0x148>
   1bc2c:	ldur	x0, [x29, #-40]
   1bc30:	cbz	x0, 1bc64 <__gmpz_invert@@Base+0x118>
   1bc34:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1bc38:	mov	w0, wzr
   1bc3c:	b	1bc64 <__gmpz_invert@@Base+0x118>
   1bc40:	ldr	w8, [x19, #4]
   1bc44:	tbnz	w8, #31, 1bc7c <__gmpz_invert@@Base+0x130>
   1bc48:	mov	x0, x20
   1bc4c:	mov	x1, x22
   1bc50:	mov	x2, x19
   1bc54:	bl	cf90 <__gmpz_add@plt>
   1bc58:	ldur	x0, [x29, #-40]
   1bc5c:	cbnz	x0, 1bc94 <__gmpz_invert@@Base+0x148>
   1bc60:	mov	w0, #0x1                   	// #1
   1bc64:	mov	sp, x29
   1bc68:	ldp	x20, x19, [sp, #48]
   1bc6c:	ldp	x22, x21, [sp, #32]
   1bc70:	ldp	x24, x23, [sp, #16]
   1bc74:	ldp	x29, x30, [sp], #64
   1bc78:	ret
   1bc7c:	mov	x0, x20
   1bc80:	mov	x1, x22
   1bc84:	mov	x2, x19
   1bc88:	bl	c260 <__gmpz_sub@plt>
   1bc8c:	ldur	x0, [x29, #-40]
   1bc90:	cbz	x0, 1bc60 <__gmpz_invert@@Base+0x114>
   1bc94:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1bc98:	b	1bc60 <__gmpz_invert@@Base+0x114>
   1bc9c:	sub	x0, x29, #0x28
   1bca0:	mov	x1, x23
   1bca4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1bca8:	stur	x0, [x29, #-8]
   1bcac:	sub	x0, x29, #0x28
   1bcb0:	mov	x1, x23
   1bcb4:	sub	x22, x29, #0x20
   1bcb8:	stur	w24, [x29, #-32]
   1bcbc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1bcc0:	b	1bbd4 <__gmpz_invert@@Base+0x88>

000000000001bcc4 <__gmpz_ior@@Base>:
   1bcc4:	stp	x29, x30, [sp, #-80]!
   1bcc8:	stp	x26, x25, [sp, #16]
   1bccc:	stp	x24, x23, [sp, #32]
   1bcd0:	stp	x22, x21, [sp, #48]
   1bcd4:	stp	x20, x19, [sp, #64]
   1bcd8:	mov	x29, sp
   1bcdc:	sub	sp, sp, #0x10
   1bce0:	ldr	w9, [x1, #4]
   1bce4:	ldr	w10, [x2, #4]
   1bce8:	ldr	x22, [x0, #8]
   1bcec:	mov	x19, x0
   1bcf0:	cmp	w9, w10
   1bcf4:	csel	x8, x2, x1, lt  // lt = tstop
   1bcf8:	ldr	x21, [x8, #8]
   1bcfc:	csel	w11, w9, w10, lt  // lt = tstop
   1bd00:	csel	w24, w10, w9, lt  // lt = tstop
   1bd04:	sxtw	x23, w11
   1bd08:	sxtw	x20, w24
   1bd0c:	csel	x26, x1, x2, lt  // lt = tstop
   1bd10:	tbnz	w11, #31, 1bd60 <__gmpz_ior@@Base+0x9c>
   1bd14:	cmp	x22, x21
   1bd18:	mov	x0, x21
   1bd1c:	b.eq	1bd44 <__gmpz_ior@@Base+0x80>  // b.none
   1bd20:	ldr	w8, [x19]
   1bd24:	cmp	w24, w8
   1bd28:	b.gt	1c3dc <__gmpz_ior@@Base+0x718>
   1bd2c:	lsl	x8, x23, #3
   1bd30:	add	x0, x22, x8
   1bd34:	add	x1, x21, x8
   1bd38:	sub	x2, x20, x23
   1bd3c:	bl	ca50 <__gmpn_copyi@plt>
   1bd40:	mov	x0, x22
   1bd44:	cbz	w23, 1bd58 <__gmpz_ior@@Base+0x94>
   1bd48:	ldr	x2, [x26, #8]
   1bd4c:	mov	x1, x21
   1bd50:	mov	x3, x23
   1bd54:	bl	cc00 <__gmpn_ior_n@plt>
   1bd58:	str	w24, [x19, #4]
   1bd5c:	b	1c4a4 <__gmpz_ior@@Base+0x7e0>
   1bd60:	stur	xzr, [x29, #-8]
   1bd64:	tbnz	w24, #31, 1be10 <__gmpz_ior@@Base+0x14c>
   1bd68:	ldrsw	x9, [x19]
   1bd6c:	neg	x25, x23
   1bd70:	cmp	x25, x9
   1bd74:	b.gt	1c3f0 <__gmpz_ior@@Base+0x72c>
   1bd78:	cmp	x25, #0xfe0
   1bd7c:	lsl	x1, x25, #3
   1bd80:	b.hi	1c414 <__gmpz_ior@@Base+0x750>  // b.pmore
   1bd84:	add	x9, x1, #0xf
   1bd88:	mov	x8, sp
   1bd8c:	and	x9, x9, #0xfffffffffffffff0
   1bd90:	sub	x24, x8, x9
   1bd94:	mov	sp, x24
   1bd98:	ldr	x8, [x26, #8]
   1bd9c:	ldr	x9, [x8]
   1bda0:	sub	x10, x9, #0x1
   1bda4:	str	x10, [x24]
   1bda8:	cbz	x9, 1beac <__gmpz_ior@@Base+0x1e8>
   1bdac:	cmn	w23, #0x2
   1bdb0:	b.gt	1c0ac <__gmpz_ior@@Base+0x3e8>
   1bdb4:	cmp	x8, x24
   1bdb8:	b.eq	1c0ac <__gmpz_ior@@Base+0x3e8>  // b.none
   1bdbc:	cmn	w23, #0x5
   1bdc0:	b.hi	1bde4 <__gmpz_ior@@Base+0x120>  // b.pmore
   1bdc4:	add	x9, x24, #0x8
   1bdc8:	add	x10, x8, x25, lsl #3
   1bdcc:	cmp	x9, x10
   1bdd0:	b.cs	1c074 <__gmpz_ior@@Base+0x3b0>  // b.hs, b.nlast
   1bdd4:	sub	x9, x24, x23, lsl #3
   1bdd8:	add	x10, x8, #0x8
   1bddc:	cmp	x9, x10
   1bde0:	b.ls	1c074 <__gmpz_ior@@Base+0x3b0>  // b.plast
   1bde4:	mov	w9, #0x1                   	// #1
   1bde8:	add	x10, x9, x23
   1bdec:	lsl	x11, x9, #3
   1bdf0:	neg	x9, x10
   1bdf4:	add	x10, x24, x11
   1bdf8:	add	x8, x8, x11
   1bdfc:	ldr	x11, [x8], #8
   1be00:	subs	x9, x9, #0x1
   1be04:	str	x11, [x10], #8
   1be08:	b.ne	1bdfc <__gmpz_ior@@Base+0x138>  // b.any
   1be0c:	b	1c0ac <__gmpz_ior@@Base+0x3e8>
   1be10:	neg	x22, x20
   1be14:	cmp	x22, #0x7f0
   1be18:	lsl	x1, x22, #4
   1be1c:	b.hi	1c424 <__gmpz_ior@@Base+0x760>  // b.pmore
   1be20:	add	x9, x1, #0xf
   1be24:	mov	x8, sp
   1be28:	and	x9, x9, #0xfffffffffffffff0
   1be2c:	sub	x1, x8, x9
   1be30:	mov	sp, x1
   1be34:	ldr	x8, [x21]
   1be38:	add	x2, x1, x22, lsl #3
   1be3c:	sub	x9, x8, #0x1
   1be40:	str	x9, [x1]
   1be44:	cbz	x8, 1bf90 <__gmpz_ior@@Base+0x2cc>
   1be48:	cmn	w24, #0x2
   1be4c:	b.gt	1c1c0 <__gmpz_ior@@Base+0x4fc>
   1be50:	cmp	x21, x1
   1be54:	b.eq	1c1c0 <__gmpz_ior@@Base+0x4fc>  // b.none
   1be58:	cmn	w24, #0x5
   1be5c:	b.hi	1be80 <__gmpz_ior@@Base+0x1bc>  // b.pmore
   1be60:	add	x8, x1, #0x8
   1be64:	add	x9, x21, x22, lsl #3
   1be68:	cmp	x8, x9
   1be6c:	b.cs	1c188 <__gmpz_ior@@Base+0x4c4>  // b.hs, b.nlast
   1be70:	sub	x8, x1, x20, lsl #3
   1be74:	add	x9, x21, #0x8
   1be78:	cmp	x8, x9
   1be7c:	b.ls	1c188 <__gmpz_ior@@Base+0x4c4>  // b.plast
   1be80:	mov	w8, #0x1                   	// #1
   1be84:	add	x9, x8, x20
   1be88:	lsl	x10, x8, #3
   1be8c:	neg	x8, x9
   1be90:	add	x9, x1, x10
   1be94:	add	x10, x21, x10
   1be98:	ldr	x11, [x10], #8
   1be9c:	subs	x8, x8, #0x1
   1bea0:	str	x11, [x9], #8
   1bea4:	b.ne	1be98 <__gmpz_ior@@Base+0x1d4>  // b.any
   1bea8:	b	1c1c0 <__gmpz_ior@@Base+0x4fc>
   1beac:	mov	x11, xzr
   1beb0:	mvn	x10, x23
   1beb4:	mov	w9, #0x1                   	// #1
   1beb8:	cmp	x9, x25
   1bebc:	b.ge	1c0ac <__gmpz_ior@@Base+0x3e8>  // b.tcont
   1bec0:	add	x12, x8, x11
   1bec4:	ldr	x12, [x12, #8]
   1bec8:	add	x13, x24, x11
   1becc:	add	x9, x9, #0x1
   1bed0:	add	x11, x11, #0x8
   1bed4:	sub	x14, x12, #0x1
   1bed8:	sub	x10, x10, #0x1
   1bedc:	str	x14, [x13, #8]
   1bee0:	cbz	x12, 1beb8 <__gmpz_ior@@Base+0x1f4>
   1bee4:	cmp	x8, x24
   1bee8:	b.eq	1c0ac <__gmpz_ior@@Base+0x3e8>  // b.none
   1beec:	cmp	x9, x25
   1bef0:	b.ge	1c0ac <__gmpz_ior@@Base+0x3e8>  // b.tcont
   1bef4:	sub	x12, x25, x9
   1bef8:	cmp	x12, #0x4
   1befc:	b.cc	1bf68 <__gmpz_ior@@Base+0x2a4>  // b.lo, b.ul, b.last
   1bf00:	add	x13, x24, x11
   1bf04:	add	x13, x13, #0x8
   1bf08:	add	x14, x8, x25, lsl #3
   1bf0c:	cmp	x13, x14
   1bf10:	b.cs	1bf28 <__gmpz_ior@@Base+0x264>  // b.hs, b.nlast
   1bf14:	add	x14, x8, x11
   1bf18:	sub	x13, x24, x23, lsl #3
   1bf1c:	add	x14, x14, #0x8
   1bf20:	cmp	x13, x14
   1bf24:	b.hi	1bf68 <__gmpz_ior@@Base+0x2a4>  // b.pmore
   1bf28:	add	x13, x24, x11
   1bf2c:	add	x14, x8, x11
   1bf30:	and	x11, x12, #0xfffffffffffffffc
   1bf34:	and	x15, x10, #0xfffffffffffffffc
   1bf38:	add	x10, x13, #0x18
   1bf3c:	add	x13, x14, #0x18
   1bf40:	add	x9, x15, x9
   1bf44:	mov	x14, x11
   1bf48:	ldp	q0, q1, [x13, #-16]
   1bf4c:	add	x13, x13, #0x20
   1bf50:	subs	x14, x14, #0x4
   1bf54:	stp	q0, q1, [x10, #-16]
   1bf58:	add	x10, x10, #0x20
   1bf5c:	b.ne	1bf48 <__gmpz_ior@@Base+0x284>  // b.any
   1bf60:	cmp	x12, x11
   1bf64:	b.eq	1c0ac <__gmpz_ior@@Base+0x3e8>  // b.none
   1bf68:	add	x10, x9, x23
   1bf6c:	lsl	x11, x9, #3
   1bf70:	neg	x9, x10
   1bf74:	add	x10, x24, x11
   1bf78:	add	x8, x8, x11
   1bf7c:	ldr	x11, [x8], #8
   1bf80:	subs	x9, x9, #0x1
   1bf84:	str	x11, [x10], #8
   1bf88:	b.ne	1bf7c <__gmpz_ior@@Base+0x2b8>  // b.any
   1bf8c:	b	1c0ac <__gmpz_ior@@Base+0x3e8>
   1bf90:	mov	x10, xzr
   1bf94:	mvn	x9, x20
   1bf98:	mov	w8, #0x1                   	// #1
   1bf9c:	cmp	x8, x22
   1bfa0:	b.ge	1c1c0 <__gmpz_ior@@Base+0x4fc>  // b.tcont
   1bfa4:	add	x11, x21, x10
   1bfa8:	ldr	x11, [x11, #8]
   1bfac:	add	x12, x1, x10
   1bfb0:	add	x8, x8, #0x1
   1bfb4:	add	x10, x10, #0x8
   1bfb8:	sub	x13, x11, #0x1
   1bfbc:	sub	x9, x9, #0x1
   1bfc0:	str	x13, [x12, #8]
   1bfc4:	cbz	x11, 1bf9c <__gmpz_ior@@Base+0x2d8>
   1bfc8:	cmp	x21, x1
   1bfcc:	b.eq	1c1c0 <__gmpz_ior@@Base+0x4fc>  // b.none
   1bfd0:	cmp	x8, x22
   1bfd4:	b.ge	1c1c0 <__gmpz_ior@@Base+0x4fc>  // b.tcont
   1bfd8:	sub	x11, x22, x8
   1bfdc:	cmp	x11, #0x4
   1bfe0:	b.cc	1c04c <__gmpz_ior@@Base+0x388>  // b.lo, b.ul, b.last
   1bfe4:	add	x12, x1, x10
   1bfe8:	add	x12, x12, #0x8
   1bfec:	add	x13, x21, x22, lsl #3
   1bff0:	cmp	x12, x13
   1bff4:	b.cs	1c00c <__gmpz_ior@@Base+0x348>  // b.hs, b.nlast
   1bff8:	add	x13, x21, x10
   1bffc:	sub	x12, x1, x20, lsl #3
   1c000:	add	x13, x13, #0x8
   1c004:	cmp	x12, x13
   1c008:	b.hi	1c04c <__gmpz_ior@@Base+0x388>  // b.pmore
   1c00c:	add	x12, x1, x10
   1c010:	add	x13, x21, x10
   1c014:	and	x10, x11, #0xfffffffffffffffc
   1c018:	and	x14, x9, #0xfffffffffffffffc
   1c01c:	add	x9, x12, #0x18
   1c020:	add	x12, x13, #0x18
   1c024:	add	x8, x14, x8
   1c028:	mov	x13, x10
   1c02c:	ldp	q0, q1, [x12, #-16]
   1c030:	add	x12, x12, #0x20
   1c034:	subs	x13, x13, #0x4
   1c038:	stp	q0, q1, [x9, #-16]
   1c03c:	add	x9, x9, #0x20
   1c040:	b.ne	1c02c <__gmpz_ior@@Base+0x368>  // b.any
   1c044:	cmp	x11, x10
   1c048:	b.eq	1c1c0 <__gmpz_ior@@Base+0x4fc>  // b.none
   1c04c:	add	x9, x8, x20
   1c050:	lsl	x10, x8, #3
   1c054:	neg	x8, x9
   1c058:	add	x9, x1, x10
   1c05c:	add	x10, x21, x10
   1c060:	ldr	x11, [x10], #8
   1c064:	subs	x8, x8, #0x1
   1c068:	str	x11, [x9], #8
   1c06c:	b.ne	1c060 <__gmpz_ior@@Base+0x39c>  // b.any
   1c070:	b	1c1c0 <__gmpz_ior@@Base+0x4fc>
   1c074:	mvn	x10, x23
   1c078:	and	x11, x10, #0xfffffffffffffffc
   1c07c:	add	x12, x8, #0x18
   1c080:	orr	x9, x11, #0x1
   1c084:	add	x13, x24, #0x18
   1c088:	mov	x14, x11
   1c08c:	ldp	q0, q1, [x12, #-16]
   1c090:	add	x12, x12, #0x20
   1c094:	subs	x14, x14, #0x4
   1c098:	stp	q0, q1, [x13, #-16]
   1c09c:	add	x13, x13, #0x20
   1c0a0:	b.ne	1c08c <__gmpz_ior@@Base+0x3c8>  // b.any
   1c0a4:	cmp	x11, x10
   1c0a8:	b.ne	1bde8 <__gmpz_ior@@Base+0x124>  // b.any
   1c0ac:	mvn	x8, x23
   1c0b0:	ldr	x8, [x24, x8, lsl #3]
   1c0b4:	cmp	x8, #0x0
   1c0b8:	csetm	x8, eq  // eq = none
   1c0bc:	sub	x23, x8, x23
   1c0c0:	subs	x2, x23, x20
   1c0c4:	b.le	1c0e4 <__gmpz_ior@@Base+0x420>
   1c0c8:	lsl	x8, x20, #3
   1c0cc:	add	x0, x22, x8
   1c0d0:	add	x1, x24, x8
   1c0d4:	bl	ca50 <__gmpn_copyi@plt>
   1c0d8:	cbz	x23, 1c168 <__gmpz_ior@@Base+0x4a4>
   1c0dc:	cbnz	x20, 1c114 <__gmpz_ior@@Base+0x450>
   1c0e0:	b	1c128 <__gmpz_ior@@Base+0x464>
   1c0e4:	sub	x8, x21, #0x8
   1c0e8:	sub	x9, x24, #0x8
   1c0ec:	subs	x10, x23, #0x1
   1c0f0:	b.lt	1c160 <__gmpz_ior@@Base+0x49c>  // b.tstop
   1c0f4:	lsl	x11, x23, #3
   1c0f8:	ldr	x12, [x8, x11]
   1c0fc:	ldr	x11, [x9, x11]
   1c100:	mov	x23, x10
   1c104:	bics	xzr, x11, x12
   1c108:	b.eq	1c0ec <__gmpz_ior@@Base+0x428>  // b.none
   1c10c:	add	x23, x10, #0x1
   1c110:	mov	x20, x23
   1c114:	mov	x0, x22
   1c118:	mov	x1, x24
   1c11c:	mov	x2, x21
   1c120:	mov	x3, x20
   1c124:	bl	c060 <__gmpn_andn_n@plt>
   1c128:	ldr	x8, [x22]
   1c12c:	adds	x8, x8, #0x1
   1c130:	str	x8, [x22]
   1c134:	b.cc	1c180 <__gmpz_ior@@Base+0x4bc>  // b.lo, b.ul, b.last
   1c138:	mov	w8, #0x1                   	// #1
   1c13c:	cmp	x8, x23
   1c140:	b.ge	1c174 <__gmpz_ior@@Base+0x4b0>  // b.tcont
   1c144:	lsl	x9, x8, #3
   1c148:	ldr	x10, [x22, x9]
   1c14c:	add	x8, x8, #0x1
   1c150:	adds	x10, x10, #0x1
   1c154:	str	x10, [x22, x9]
   1c158:	b.cs	1c13c <__gmpz_ior@@Base+0x478>  // b.hs, b.nlast
   1c15c:	b	1c180 <__gmpz_ior@@Base+0x4bc>
   1c160:	mov	x20, x23
   1c164:	cbnz	x23, 1c0dc <__gmpz_ior@@Base+0x418>
   1c168:	mov	w23, #0x1                   	// #1
   1c16c:	str	x23, [x22]
   1c170:	b	1c180 <__gmpz_ior@@Base+0x4bc>
   1c174:	mov	w8, #0x1                   	// #1
   1c178:	str	x8, [x22, x23, lsl #3]
   1c17c:	add	x23, x23, #0x1
   1c180:	neg	w8, w23
   1c184:	b	1c498 <__gmpz_ior@@Base+0x7d4>
   1c188:	mvn	x9, x20
   1c18c:	and	x10, x9, #0xfffffffffffffffc
   1c190:	add	x11, x21, #0x18
   1c194:	orr	x8, x10, #0x1
   1c198:	add	x12, x1, #0x18
   1c19c:	mov	x13, x10
   1c1a0:	ldp	q0, q1, [x11, #-16]
   1c1a4:	add	x11, x11, #0x20
   1c1a8:	subs	x13, x13, #0x4
   1c1ac:	stp	q0, q1, [x12, #-16]
   1c1b0:	add	x12, x12, #0x20
   1c1b4:	b.ne	1c1a0 <__gmpz_ior@@Base+0x4dc>  // b.any
   1c1b8:	cmp	x10, x9
   1c1bc:	b.ne	1be84 <__gmpz_ior@@Base+0x1c0>  // b.any
   1c1c0:	ldr	x8, [x26, #8]
   1c1c4:	ldr	x9, [x8]
   1c1c8:	sub	x10, x9, #0x1
   1c1cc:	str	x10, [x2]
   1c1d0:	cbz	x9, 1c240 <__gmpz_ior@@Base+0x57c>
   1c1d4:	cmn	w24, #0x2
   1c1d8:	b.gt	1c378 <__gmpz_ior@@Base+0x6b4>
   1c1dc:	cmp	x8, x2
   1c1e0:	b.eq	1c378 <__gmpz_ior@@Base+0x6b4>  // b.none
   1c1e4:	cmn	w24, #0x5
   1c1e8:	b.hi	1c210 <__gmpz_ior@@Base+0x54c>  // b.pmore
   1c1ec:	sub	x13, x1, x20, lsl #3
   1c1f0:	add	x9, x13, #0x8
   1c1f4:	add	x10, x8, x22, lsl #3
   1c1f8:	cmp	x9, x10
   1c1fc:	b.cs	1c340 <__gmpz_ior@@Base+0x67c>  // b.hs, b.nlast
   1c200:	sub	x9, x1, x20, lsl #4
   1c204:	add	x10, x8, #0x8
   1c208:	cmp	x9, x10
   1c20c:	b.ls	1c340 <__gmpz_ior@@Base+0x67c>  // b.plast
   1c210:	mov	w9, #0x1                   	// #1
   1c214:	add	x10, x9, x20
   1c218:	lsl	x11, x9, #3
   1c21c:	neg	x9, x10
   1c220:	sub	x10, x11, x20, lsl #3
   1c224:	add	x10, x1, x10
   1c228:	add	x8, x8, x11
   1c22c:	ldr	x11, [x8], #8
   1c230:	subs	x9, x9, #0x1
   1c234:	str	x11, [x10], #8
   1c238:	b.ne	1c22c <__gmpz_ior@@Base+0x568>  // b.any
   1c23c:	b	1c378 <__gmpz_ior@@Base+0x6b4>
   1c240:	mov	w9, #0x20                  	// #32
   1c244:	sub	x12, x9, x20, lsl #3
   1c248:	add	x9, x12, x1
   1c24c:	mov	x10, xzr
   1c250:	mvn	x11, x20
   1c254:	sub	x13, x9, #0x20
   1c258:	mov	w9, #0x1                   	// #1
   1c25c:	cmp	x9, x22
   1c260:	b.ge	1c378 <__gmpz_ior@@Base+0x6b4>  // b.tcont
   1c264:	add	x14, x8, x10
   1c268:	ldr	x14, [x14, #8]
   1c26c:	add	x15, x13, x10
   1c270:	add	x9, x9, #0x1
   1c274:	add	x10, x10, #0x8
   1c278:	sub	x16, x14, #0x1
   1c27c:	sub	x11, x11, #0x1
   1c280:	str	x16, [x15, #8]
   1c284:	cbz	x14, 1c25c <__gmpz_ior@@Base+0x598>
   1c288:	cmp	x8, x2
   1c28c:	b.eq	1c378 <__gmpz_ior@@Base+0x6b4>  // b.none
   1c290:	cmp	x9, x22
   1c294:	b.ge	1c378 <__gmpz_ior@@Base+0x6b4>  // b.tcont
   1c298:	sub	x13, x22, x9
   1c29c:	cmp	x13, #0x4
   1c2a0:	b.cc	1c314 <__gmpz_ior@@Base+0x650>  // b.lo, b.ul, b.last
   1c2a4:	add	x14, x12, x1
   1c2a8:	add	x14, x14, x10
   1c2ac:	sub	x14, x14, #0x18
   1c2b0:	add	x15, x8, x22, lsl #3
   1c2b4:	cmp	x14, x15
   1c2b8:	b.cs	1c2d0 <__gmpz_ior@@Base+0x60c>  // b.hs, b.nlast
   1c2bc:	add	x15, x8, x10
   1c2c0:	sub	x14, x1, x20, lsl #4
   1c2c4:	add	x15, x15, #0x8
   1c2c8:	cmp	x14, x15
   1c2cc:	b.hi	1c314 <__gmpz_ior@@Base+0x650>  // b.pmore
   1c2d0:	add	x14, x12, x1
   1c2d4:	add	x15, x8, x10
   1c2d8:	and	x12, x13, #0xfffffffffffffffc
   1c2dc:	and	x16, x11, #0xfffffffffffffffc
   1c2e0:	add	x11, x14, x10
   1c2e4:	add	x10, x15, #0x18
   1c2e8:	sub	x11, x11, #0x8
   1c2ec:	add	x9, x16, x9
   1c2f0:	mov	x14, x12
   1c2f4:	ldp	q0, q1, [x10, #-16]
   1c2f8:	add	x10, x10, #0x20
   1c2fc:	subs	x14, x14, #0x4
   1c300:	stp	q0, q1, [x11, #-16]
   1c304:	add	x11, x11, #0x20
   1c308:	b.ne	1c2f4 <__gmpz_ior@@Base+0x630>  // b.any
   1c30c:	cmp	x13, x12
   1c310:	b.eq	1c378 <__gmpz_ior@@Base+0x6b4>  // b.none
   1c314:	add	x10, x9, x20
   1c318:	lsl	x11, x9, #3
   1c31c:	neg	x9, x10
   1c320:	sub	x10, x11, x20, lsl #3
   1c324:	add	x10, x1, x10
   1c328:	add	x8, x8, x11
   1c32c:	ldr	x11, [x8], #8
   1c330:	subs	x9, x9, #0x1
   1c334:	str	x11, [x10], #8
   1c338:	b.ne	1c32c <__gmpz_ior@@Base+0x668>  // b.any
   1c33c:	b	1c378 <__gmpz_ior@@Base+0x6b4>
   1c340:	mvn	x10, x20
   1c344:	and	x11, x10, #0xfffffffffffffffc
   1c348:	add	x12, x8, #0x18
   1c34c:	orr	x9, x11, #0x1
   1c350:	add	x13, x13, #0x18
   1c354:	mov	x14, x11
   1c358:	ldp	q0, q1, [x12, #-16]
   1c35c:	add	x12, x12, #0x20
   1c360:	subs	x14, x14, #0x4
   1c364:	stp	q0, q1, [x13, #-16]
   1c368:	add	x13, x13, #0x20
   1c36c:	b.ne	1c358 <__gmpz_ior@@Base+0x694>  // b.any
   1c370:	cmp	x11, x10
   1c374:	b.ne	1c214 <__gmpz_ior@@Base+0x550>  // b.any
   1c378:	sub	x8, x1, x20, lsl #3
   1c37c:	sub	x9, x1, x20, lsl #4
   1c380:	mov	x10, xzr
   1c384:	sub	x8, x8, #0x8
   1c388:	sub	x9, x9, #0x8
   1c38c:	add	x21, x22, x10
   1c390:	mov	x23, x10
   1c394:	cmp	x21, #0x1
   1c398:	b.lt	1c3b4 <__gmpz_ior@@Base+0x6f0>  // b.tstop
   1c39c:	lsl	x10, x23, #3
   1c3a0:	ldr	x11, [x8, x10]
   1c3a4:	ldr	x10, [x9, x10]
   1c3a8:	and	x11, x10, x11
   1c3ac:	sub	x10, x23, #0x1
   1c3b0:	cbz	x11, 1c38c <__gmpz_ior@@Base+0x6c8>
   1c3b4:	ldrsw	x8, [x19]
   1c3b8:	cmp	x21, x8
   1c3bc:	b.ge	1c434 <__gmpz_ior@@Base+0x770>  // b.tcont
   1c3c0:	ldr	x22, [x19, #8]
   1c3c4:	cmp	x20, x23
   1c3c8:	b.ne	1c464 <__gmpz_ior@@Base+0x7a0>  // b.any
   1c3cc:	mov	w8, #0x1                   	// #1
   1c3d0:	str	x8, [x22]
   1c3d4:	mov	w8, #0xffffffff            	// #-1
   1c3d8:	b	1c498 <__gmpz_ior@@Base+0x7d4>
   1c3dc:	mov	x0, x19
   1c3e0:	mov	x1, x20
   1c3e4:	bl	c080 <__gmpz_realloc@plt>
   1c3e8:	mov	x22, x0
   1c3ec:	b	1bd2c <__gmpz_ior@@Base+0x68>
   1c3f0:	mov	x0, x19
   1c3f4:	mov	x1, x25
   1c3f8:	mov	x21, x8
   1c3fc:	bl	c080 <__gmpz_realloc@plt>
   1c400:	ldr	x21, [x21, #8]
   1c404:	mov	x22, x0
   1c408:	cmp	x25, #0xfe0
   1c40c:	lsl	x1, x25, #3
   1c410:	b.ls	1bd84 <__gmpz_ior@@Base+0xc0>  // b.plast
   1c414:	sub	x0, x29, #0x8
   1c418:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1c41c:	mov	x24, x0
   1c420:	b	1bd98 <__gmpz_ior@@Base+0xd4>
   1c424:	sub	x0, x29, #0x8
   1c428:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1c42c:	mov	x1, x0
   1c430:	b	1be34 <__gmpz_ior@@Base+0x170>
   1c434:	add	x8, x22, x23
   1c438:	add	x8, x8, #0x1
   1c43c:	mov	x0, x19
   1c440:	mov	x22, x1
   1c444:	mov	x1, x8
   1c448:	mov	x24, x2
   1c44c:	bl	c080 <__gmpz_realloc@plt>
   1c450:	mov	x2, x24
   1c454:	mov	x1, x22
   1c458:	mov	x22, x0
   1c45c:	cmp	x20, x23
   1c460:	b.eq	1c3cc <__gmpz_ior@@Base+0x708>  // b.none
   1c464:	mov	x0, x22
   1c468:	mov	x3, x21
   1c46c:	bl	c270 <__gmpn_and_n@plt>
   1c470:	sub	x8, x22, x20, lsl #3
   1c474:	str	xzr, [x8, x23, lsl #3]
   1c478:	ldr	x9, [x22]
   1c47c:	adds	x9, x9, #0x1
   1c480:	str	x9, [x22], #8
   1c484:	b.cs	1c478 <__gmpz_ior@@Base+0x7b4>  // b.hs, b.nlast
   1c488:	lsl	x9, x23, #3
   1c48c:	ldr	w8, [x8, x9]
   1c490:	sub	w8, w20, w8
   1c494:	sub	w8, w8, w23
   1c498:	str	w8, [x19, #4]
   1c49c:	ldur	x0, [x29, #-8]
   1c4a0:	cbnz	x0, 1c4c0 <__gmpz_ior@@Base+0x7fc>
   1c4a4:	mov	sp, x29
   1c4a8:	ldp	x20, x19, [sp, #64]
   1c4ac:	ldp	x22, x21, [sp, #48]
   1c4b0:	ldp	x24, x23, [sp, #32]
   1c4b4:	ldp	x26, x25, [sp, #16]
   1c4b8:	ldp	x29, x30, [sp], #80
   1c4bc:	ret
   1c4c0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1c4c4:	b	1c4a4 <__gmpz_ior@@Base+0x7e0>

000000000001c4c8 <__gmpz_init_set@@Base>:
   1c4c8:	stp	x29, x30, [sp, #-48]!
   1c4cc:	stp	x22, x21, [sp, #16]
   1c4d0:	stp	x20, x19, [sp, #32]
   1c4d4:	ldrsw	x22, [x1, #4]
   1c4d8:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1c4dc:	mov	x20, x0
   1c4e0:	mov	x29, sp
   1c4e4:	cmp	x22, #0x0
   1c4e8:	cneg	x21, x22, mi  // mi = first
   1c4ec:	cmp	x21, #0x1
   1c4f0:	csinc	x8, x21, xzr, gt
   1c4f4:	str	w8, [x0]
   1c4f8:	ldr	x9, [x9, #3840]
   1c4fc:	sbfiz	x0, x8, #3, #32
   1c500:	mov	x19, x1
   1c504:	ldr	x9, [x9]
   1c508:	blr	x9
   1c50c:	str	x0, [x20, #8]
   1c510:	ldr	x1, [x19, #8]
   1c514:	mov	x2, x21
   1c518:	bl	ca50 <__gmpn_copyi@plt>
   1c51c:	str	w22, [x20, #4]
   1c520:	ldp	x20, x19, [sp, #32]
   1c524:	ldp	x22, x21, [sp, #16]
   1c528:	ldp	x29, x30, [sp], #48
   1c52c:	ret

000000000001c530 <__gmpz_init_set_d@@Base>:
   1c530:	adrp	x8, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1c534:	add	x8, x8, #0x120
   1c538:	stp	xzr, x8, [x0]
   1c53c:	b	c890 <__gmpz_set_d@plt>

000000000001c540 <__gmpz_init_set_si@@Base>:
   1c540:	stp	x29, x30, [sp, #-32]!
   1c544:	mov	w8, #0x1                   	// #1
   1c548:	stp	x20, x19, [sp, #16]
   1c54c:	str	w8, [x0]
   1c550:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1c554:	ldr	x8, [x8, #3840]
   1c558:	mov	x19, x0
   1c55c:	mov	w0, #0x8                   	// #8
   1c560:	mov	x29, sp
   1c564:	ldr	x8, [x8]
   1c568:	mov	x20, x1
   1c56c:	blr	x8
   1c570:	cmp	x20, #0x0
   1c574:	cneg	x8, x20, mi  // mi = first
   1c578:	cset	w9, ne  // ne = any
   1c57c:	csetm	w10, ne  // ne = any
   1c580:	str	x0, [x19, #8]
   1c584:	str	x8, [x0]
   1c588:	csel	w8, w9, w10, ge  // ge = tcont
   1c58c:	str	w8, [x19, #4]
   1c590:	ldp	x20, x19, [sp, #16]
   1c594:	ldp	x29, x30, [sp], #32
   1c598:	ret

000000000001c59c <__gmpz_init_set_str@@Base>:
   1c59c:	adrp	x8, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1c5a0:	add	x8, x8, #0x128
   1c5a4:	stp	xzr, x8, [x0]
   1c5a8:	b	c0d0 <__gmpz_set_str@plt>

000000000001c5ac <__gmpz_init_set_ui@@Base>:
   1c5ac:	stp	x29, x30, [sp, #-32]!
   1c5b0:	mov	w8, #0x1                   	// #1
   1c5b4:	stp	x20, x19, [sp, #16]
   1c5b8:	str	w8, [x0]
   1c5bc:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1c5c0:	ldr	x8, [x8, #3840]
   1c5c4:	mov	x19, x0
   1c5c8:	mov	w0, #0x8                   	// #8
   1c5cc:	mov	x29, sp
   1c5d0:	ldr	x8, [x8]
   1c5d4:	mov	x20, x1
   1c5d8:	blr	x8
   1c5dc:	cmp	x20, #0x0
   1c5e0:	cset	w8, ne  // ne = any
   1c5e4:	str	x0, [x19, #8]
   1c5e8:	str	x20, [x0]
   1c5ec:	str	w8, [x19, #4]
   1c5f0:	ldp	x20, x19, [sp, #16]
   1c5f4:	ldp	x29, x30, [sp], #32
   1c5f8:	ret

000000000001c5fc <__gmpz_jacobi@@Base>:
   1c5fc:	stp	x29, x30, [sp, #-80]!
   1c600:	stp	x26, x25, [sp, #16]
   1c604:	stp	x24, x23, [sp, #32]
   1c608:	stp	x22, x21, [sp, #48]
   1c60c:	stp	x20, x19, [sp, #64]
   1c610:	mov	x29, sp
   1c614:	sub	sp, sp, #0x10
   1c618:	ldr	x8, [x0, #8]
   1c61c:	ldrsw	x10, [x0, #4]
   1c620:	ldrsw	x13, [x1, #4]
   1c624:	ldr	x9, [x8]
   1c628:	cbz	w13, 1c720 <__gmpz_jacobi@@Base+0x124>
   1c62c:	ldr	x3, [x1, #8]
   1c630:	ldr	x11, [x3]
   1c634:	cbz	w10, 1c73c <__gmpz_jacobi@@Base+0x140>
   1c638:	orr	w12, w11, w9
   1c63c:	tbz	w12, #0, 1c758 <__gmpz_jacobi@@Base+0x15c>
   1c640:	and	w12, w10, w13
   1c644:	cmp	w13, #0x0
   1c648:	lsr	w12, w12, #30
   1c64c:	cneg	x4, x13, lt  // lt = tstop
   1c650:	cbz	x11, 1c8b4 <__gmpz_jacobi@@Base+0x2b8>
   1c654:	rbit	x13, x11
   1c658:	clz	x20, x13
   1c65c:	and	w12, w12, #0x2
   1c660:	cmp	x4, #0x2
   1c664:	lsr	x19, x11, x20
   1c668:	b.lt	1c698 <__gmpz_jacobi@@Base+0x9c>  // b.tstop
   1c66c:	cbz	w20, 1c698 <__gmpz_jacobi@@Base+0x9c>
   1c670:	ldr	x11, [x3, #8]
   1c674:	neg	x13, x20
   1c678:	cmp	x4, #0x2
   1c67c:	lsl	x13, x11, x13
   1c680:	orr	x19, x13, x19
   1c684:	b.ne	1c698 <__gmpz_jacobi@@Base+0x9c>  // b.any
   1c688:	lsr	x11, x11, x20
   1c68c:	cmp	x11, #0x0
   1c690:	mov	w11, #0x1                   	// #1
   1c694:	cinc	x4, x11, ne  // ne = any
   1c698:	and	w11, w19, w10, asr #31
   1c69c:	cmp	w10, #0x0
   1c6a0:	eor	w26, w11, w12
   1c6a4:	cneg	x10, x10, lt  // lt = tstop
   1c6a8:	cbz	x9, 1c8c4 <__gmpz_jacobi@@Base+0x2c8>
   1c6ac:	cmp	x10, x4
   1c6b0:	b.ge	1c77c <__gmpz_jacobi@@Base+0x180>  // b.tcont
   1c6b4:	rbit	x11, x9
   1c6b8:	clz	x20, x11
   1c6bc:	cmp	x10, #0x2
   1c6c0:	lsr	x21, x9, x20
   1c6c4:	b.lt	1c6f4 <__gmpz_jacobi@@Base+0xf8>  // b.tstop
   1c6c8:	cbz	w20, 1c6f4 <__gmpz_jacobi@@Base+0xf8>
   1c6cc:	ldr	x9, [x8, #8]
   1c6d0:	neg	x11, x20
   1c6d4:	cmp	x10, #0x2
   1c6d8:	lsl	x11, x9, x11
   1c6dc:	orr	x21, x11, x21
   1c6e0:	b.ne	1c6f4 <__gmpz_jacobi@@Base+0xf8>  // b.any
   1c6e4:	lsr	x9, x9, x20
   1c6e8:	cmp	x9, #0x0
   1c6ec:	mov	w9, #0x1                   	// #1
   1c6f0:	cinc	x10, x9, ne  // ne = any
   1c6f4:	and	w9, w21, w19
   1c6f8:	eor	w26, w26, w9
   1c6fc:	mov	x22, x10
   1c700:	mov	x23, x8
   1c704:	cmp	x22, #0x1
   1c708:	b.eq	1c79c <__gmpz_jacobi@@Base+0x1a0>  // b.none
   1c70c:	cmp	x4, x22, lsl #1
   1c710:	stur	xzr, [x29, #-8]
   1c714:	b.ge	1c7c4 <__gmpz_jacobi@@Base+0x1c8>  // b.tcont
   1c718:	lsl	x1, x22, #4
   1c71c:	b	1c7cc <__gmpz_jacobi@@Base+0x1d0>
   1c720:	cmp	w10, #0x1
   1c724:	b.eq	1c730 <__gmpz_jacobi@@Base+0x134>  // b.none
   1c728:	cmn	w10, #0x1
   1c72c:	b.ne	1c758 <__gmpz_jacobi@@Base+0x15c>  // b.any
   1c730:	cmp	x9, #0x1
   1c734:	cset	w19, eq  // eq = none
   1c738:	b	1c75c <__gmpz_jacobi@@Base+0x160>
   1c73c:	cmp	w13, #0x1
   1c740:	b.eq	1c74c <__gmpz_jacobi@@Base+0x150>  // b.none
   1c744:	cmn	w13, #0x1
   1c748:	b.ne	1c758 <__gmpz_jacobi@@Base+0x15c>  // b.any
   1c74c:	cmp	x11, #0x1
   1c750:	cset	w19, eq  // eq = none
   1c754:	b	1c75c <__gmpz_jacobi@@Base+0x160>
   1c758:	mov	w19, wzr
   1c75c:	mov	w0, w19
   1c760:	mov	sp, x29
   1c764:	ldp	x20, x19, [sp, #64]
   1c768:	ldp	x22, x21, [sp, #48]
   1c76c:	ldp	x24, x23, [sp, #32]
   1c770:	ldp	x26, x25, [sp, #16]
   1c774:	ldp	x29, x30, [sp], #80
   1c778:	ret
   1c77c:	mov	x21, x19
   1c780:	mov	x19, x9
   1c784:	mov	x22, x4
   1c788:	mov	x4, x10
   1c78c:	mov	x23, x3
   1c790:	mov	x3, x8
   1c794:	cmp	x22, #0x1
   1c798:	b.ne	1c70c <__gmpz_jacobi@@Base+0x110>  // b.any
   1c79c:	lsr	x8, x19, #1
   1c7a0:	eor	w8, w8, w19
   1c7a4:	and	w8, w8, w20, lsl #1
   1c7a8:	cmp	x21, #0x1
   1c7ac:	eor	w20, w8, w26
   1c7b0:	b.ne	1c85c <__gmpz_jacobi@@Base+0x260>  // b.any
   1c7b4:	and	w8, w20, #0x2
   1c7b8:	mov	w9, #0x1                   	// #1
   1c7bc:	sub	w19, w9, w8
   1c7c0:	b	1c75c <__gmpz_jacobi@@Base+0x160>
   1c7c4:	lsl	x8, x4, #3
   1c7c8:	add	x1, x8, #0x8
   1c7cc:	mov	w8, #0x7f00                	// #32512
   1c7d0:	cmp	x1, x8
   1c7d4:	b.hi	1c8d4 <__gmpz_jacobi@@Base+0x2d8>  // b.pmore
   1c7d8:	add	x9, x1, #0xf
   1c7dc:	mov	x8, sp
   1c7e0:	and	x9, x9, #0xfffffffffffffff0
   1c7e4:	sub	x24, x8, x9
   1c7e8:	mov	sp, x24
   1c7ec:	cmp	x4, x22
   1c7f0:	add	x25, x24, x22, lsl #3
   1c7f4:	b.le	1c8fc <__gmpz_jacobi@@Base+0x300>
   1c7f8:	mov	x0, x25
   1c7fc:	mov	x1, x24
   1c800:	mov	x2, xzr
   1c804:	mov	x5, x23
   1c808:	mov	x6, x22
   1c80c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   1c810:	cbz	w20, 1c910 <__gmpz_jacobi@@Base+0x314>
   1c814:	lsr	x8, x19, #1
   1c818:	eor	w8, w8, w19
   1c81c:	and	w8, w8, w20, lsl #1
   1c820:	mov	x0, x25
   1c824:	mov	x1, x23
   1c828:	mov	x2, x22
   1c82c:	mov	w3, w20
   1c830:	eor	w26, w8, w26
   1c834:	bl	c1a0 <__gmpn_rshift@plt>
   1c838:	lsl	x8, x22, #3
   1c83c:	sub	x8, x8, #0x8
   1c840:	ldr	x9, [x24, x8]
   1c844:	ldr	x8, [x25, x8]
   1c848:	orr	x8, x8, x9
   1c84c:	cmp	x8, #0x0
   1c850:	cset	w8, eq  // eq = none
   1c854:	sub	x22, x22, x8
   1c858:	b	1c920 <__gmpz_jacobi@@Base+0x324>
   1c85c:	cmp	x4, #0x2
   1c860:	b.lt	1c89c <__gmpz_jacobi@@Base+0x2a0>  // b.tstop
   1c864:	cmp	x4, #0x28
   1c868:	b.lt	1c880 <__gmpz_jacobi@@Base+0x284>  // b.tstop
   1c86c:	mov	x0, x3
   1c870:	mov	x1, x4
   1c874:	mov	x2, x21
   1c878:	bl	c3e0 <__gmpn_mod_1@plt>
   1c87c:	b	1c898 <__gmpz_jacobi@@Base+0x29c>
   1c880:	mov	x0, x3
   1c884:	mov	x1, x4
   1c888:	mov	x2, x21
   1c88c:	mov	x3, xzr
   1c890:	eor	w20, w20, w21
   1c894:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c898:	mov	x19, x0
   1c89c:	mov	x0, x19
   1c8a0:	mov	x1, x21
   1c8a4:	mov	w2, w20
   1c8a8:	bl	c730 <__gmpn_jacobi_base@plt>
   1c8ac:	mov	w19, w0
   1c8b0:	b	1c75c <__gmpz_jacobi@@Base+0x160>
   1c8b4:	ldr	x11, [x3, #8]!
   1c8b8:	sub	x4, x4, #0x1
   1c8bc:	cbnz	x11, 1c654 <__gmpz_jacobi@@Base+0x58>
   1c8c0:	b	1c8b4 <__gmpz_jacobi@@Base+0x2b8>
   1c8c4:	ldr	x9, [x8, #8]!
   1c8c8:	sub	x10, x10, #0x1
   1c8cc:	cbnz	x9, 1c6ac <__gmpz_jacobi@@Base+0xb0>
   1c8d0:	b	1c8c4 <__gmpz_jacobi@@Base+0x2c8>
   1c8d4:	sub	x0, x29, #0x8
   1c8d8:	mov	x24, x3
   1c8dc:	mov	x25, x4
   1c8e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1c8e4:	mov	x4, x25
   1c8e8:	mov	x3, x24
   1c8ec:	mov	x24, x0
   1c8f0:	cmp	x4, x22
   1c8f4:	add	x25, x24, x22, lsl #3
   1c8f8:	b.gt	1c7f8 <__gmpz_jacobi@@Base+0x1fc>
   1c8fc:	mov	x0, x24
   1c900:	mov	x1, x3
   1c904:	mov	x2, x22
   1c908:	bl	ca50 <__gmpn_copyi@plt>
   1c90c:	cbnz	w20, 1c814 <__gmpz_jacobi@@Base+0x218>
   1c910:	mov	x0, x25
   1c914:	mov	x1, x23
   1c918:	mov	x2, x22
   1c91c:	bl	ca50 <__gmpn_copyi@plt>
   1c920:	ldr	w8, [x24]
   1c924:	and	w3, w21, #0x2
   1c928:	bfxil	w3, w26, #1, #1
   1c92c:	mov	x0, x24
   1c930:	bfi	w3, w8, #2, #2
   1c934:	mov	x1, x25
   1c938:	mov	x2, x22
   1c93c:	bl	cea0 <__gmpn_jacobi_n@plt>
   1c940:	ldur	x8, [x29, #-8]
   1c944:	mov	w19, w0
   1c948:	cbz	x8, 1c75c <__gmpz_jacobi@@Base+0x160>
   1c94c:	mov	x0, x8
   1c950:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1c954:	b	1c75c <__gmpz_jacobi@@Base+0x160>

000000000001c958 <__gmpz_si_kronecker@@Base>:
   1c958:	stp	x29, x30, [sp, #-48]!
   1c95c:	stp	x20, x19, [sp, #32]
   1c960:	ldrsw	x11, [x1, #4]
   1c964:	mov	x8, x0
   1c968:	str	x21, [sp, #16]
   1c96c:	mov	x29, sp
   1c970:	cbz	w11, 1c9ac <__gmpz_si_kronecker@@Base+0x54>
   1c974:	ldr	x0, [x1, #8]
   1c978:	lsr	x10, x8, #63
   1c97c:	and	w9, w10, w11, lsr #31
   1c980:	cmp	x11, #0x0
   1c984:	ldr	x20, [x0]
   1c988:	lsl	w9, w9, #1
   1c98c:	cneg	x1, x11, mi  // mi = first
   1c990:	tbnz	w20, #0, 1c9d0 <__gmpz_si_kronecker@@Base+0x78>
   1c994:	tbnz	w8, #0, 1ca28 <__gmpz_si_kronecker@@Base+0xd0>
   1c998:	mov	w0, wzr
   1c99c:	ldp	x20, x19, [sp, #32]
   1c9a0:	ldr	x21, [sp, #16]
   1c9a4:	ldp	x29, x30, [sp], #48
   1c9a8:	ret
   1c9ac:	cmp	x8, #0x1
   1c9b0:	cset	w9, eq  // eq = none
   1c9b4:	cmn	x8, #0x1
   1c9b8:	cset	w8, eq  // eq = none
   1c9bc:	orr	w0, w9, w8
   1c9c0:	ldp	x20, x19, [sp, #32]
   1c9c4:	ldr	x21, [sp, #16]
   1c9c8:	ldp	x29, x30, [sp], #48
   1c9cc:	ret
   1c9d0:	and	w10, w20, w10, lsl #1
   1c9d4:	cmp	x8, #0x0
   1c9d8:	cneg	x19, x8, mi  // mi = first
   1c9dc:	eor	w21, w9, w10
   1c9e0:	tbnz	w19, #0, 1ca04 <__gmpz_si_kronecker@@Base+0xac>
   1c9e4:	cbz	x19, 1caa0 <__gmpz_si_kronecker@@Base+0x148>
   1c9e8:	rbit	x8, x8
   1c9ec:	lsr	x9, x20, #1
   1c9f0:	clz	x8, x8
   1c9f4:	eor	w9, w9, w20
   1c9f8:	lsr	x19, x19, x8
   1c9fc:	and	w8, w9, w8, lsl #1
   1ca00:	eor	w21, w8, w21
   1ca04:	cmp	x19, #0x1
   1ca08:	b.ne	1ca60 <__gmpz_si_kronecker@@Base+0x108>  // b.any
   1ca0c:	and	w8, w21, #0x2
   1ca10:	mov	w9, #0x1                   	// #1
   1ca14:	sub	w0, w9, w8
   1ca18:	ldp	x20, x19, [sp, #32]
   1ca1c:	ldr	x21, [sp, #16]
   1ca20:	ldp	x29, x30, [sp], #48
   1ca24:	ret
   1ca28:	cbz	x20, 1cac4 <__gmpz_si_kronecker@@Base+0x16c>
   1ca2c:	tbnz	w20, #0, 1ca48 <__gmpz_si_kronecker@@Base+0xf0>
   1ca30:	mov	x11, #0x8000000000000000    	// #-9223372036854775808
   1ca34:	cmp	x20, x11
   1ca38:	b.eq	1cad4 <__gmpz_si_kronecker@@Base+0x17c>  // b.none
   1ca3c:	rbit	x11, x20
   1ca40:	clz	x11, x11
   1ca44:	lsr	x20, x20, x11
   1ca48:	and	w10, w20, w10, lsl #1
   1ca4c:	cmp	x8, #0x0
   1ca50:	eor	w21, w9, w10
   1ca54:	cneg	x19, x8, mi  // mi = first
   1ca58:	cmp	x19, #0x1
   1ca5c:	b.eq	1ca0c <__gmpz_si_kronecker@@Base+0xb4>  // b.none
   1ca60:	cmp	x1, #0x28
   1ca64:	b.lt	1ca74 <__gmpz_si_kronecker@@Base+0x11c>  // b.tstop
   1ca68:	mov	x2, x19
   1ca6c:	bl	c3e0 <__gmpn_mod_1@plt>
   1ca70:	b	1ca84 <__gmpz_si_kronecker@@Base+0x12c>
   1ca74:	mov	x2, x19
   1ca78:	mov	x3, xzr
   1ca7c:	eor	w21, w21, w19
   1ca80:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1ca84:	and	w8, w20, w19
   1ca88:	eor	w2, w21, w8
   1ca8c:	mov	x1, x19
   1ca90:	ldp	x20, x19, [sp, #32]
   1ca94:	ldr	x21, [sp, #16]
   1ca98:	ldp	x29, x30, [sp], #48
   1ca9c:	b	c730 <__gmpn_jacobi_base@plt>
   1caa0:	cmp	x1, #0x1
   1caa4:	cset	w8, eq  // eq = none
   1caa8:	cmp	x20, #0x1
   1caac:	cset	w9, eq  // eq = none
   1cab0:	and	w0, w8, w9
   1cab4:	ldp	x20, x19, [sp, #32]
   1cab8:	ldr	x21, [sp, #16]
   1cabc:	ldp	x29, x30, [sp], #48
   1cac0:	ret
   1cac4:	ldr	x20, [x0, #8]!
   1cac8:	sub	x1, x1, #0x1
   1cacc:	cbnz	x20, 1ca2c <__gmpz_si_kronecker@@Base+0xd4>
   1cad0:	b	1cac4 <__gmpz_si_kronecker@@Base+0x16c>
   1cad4:	cmp	x1, #0x1
   1cad8:	b.ne	1caec <__gmpz_si_kronecker@@Base+0x194>  // b.any
   1cadc:	eor	w8, w8, w8, lsr #1
   1cae0:	and	w8, w8, #0x2
   1cae4:	eor	w8, w9, w8
   1cae8:	b	1ca10 <__gmpz_si_kronecker@@Base+0xb8>
   1caec:	ldr	x11, [x0, #8]
   1caf0:	lsl	x20, x11, #1
   1caf4:	b	1ca48 <__gmpz_si_kronecker@@Base+0xf0>

000000000001caf8 <__gmpz_ui_kronecker@@Base>:
   1caf8:	stp	x29, x30, [sp, #-48]!
   1cafc:	stp	x20, x19, [sp, #32]
   1cb00:	ldr	w8, [x1, #4]
   1cb04:	mov	x19, x0
   1cb08:	str	x21, [sp, #16]
   1cb0c:	mov	x29, sp
   1cb10:	cmp	w8, #0x0
   1cb14:	cneg	w8, w8, mi  // mi = first
   1cb18:	cbz	w8, 1cb40 <__gmpz_ui_kronecker@@Base+0x48>
   1cb1c:	ldr	x0, [x1, #8]
   1cb20:	ldr	x20, [x0]
   1cb24:	tbnz	w20, #0, 1cb58 <__gmpz_ui_kronecker@@Base+0x60>
   1cb28:	tbnz	w19, #0, 1cb98 <__gmpz_ui_kronecker@@Base+0xa0>
   1cb2c:	mov	w0, wzr
   1cb30:	ldp	x20, x19, [sp, #32]
   1cb34:	ldr	x21, [sp, #16]
   1cb38:	ldp	x29, x30, [sp], #48
   1cb3c:	ret
   1cb40:	cmp	x19, #0x1
   1cb44:	cset	w0, eq  // eq = none
   1cb48:	ldp	x20, x19, [sp, #32]
   1cb4c:	ldr	x21, [sp, #16]
   1cb50:	ldp	x29, x30, [sp], #48
   1cb54:	ret
   1cb58:	cbz	x19, 1cbf0 <__gmpz_ui_kronecker@@Base+0xf8>
   1cb5c:	tbnz	w19, #0, 1cbc8 <__gmpz_ui_kronecker@@Base+0xd0>
   1cb60:	rbit	x9, x19
   1cb64:	lsr	x10, x20, #1
   1cb68:	clz	x9, x9
   1cb6c:	eor	w10, w10, w20
   1cb70:	lsr	x19, x19, x9
   1cb74:	and	w21, w10, w9, lsl #1
   1cb78:	cmp	x19, #0x1
   1cb7c:	b.eq	1cbd4 <__gmpz_ui_kronecker@@Base+0xdc>  // b.none
   1cb80:	cmp	w8, #0x28
   1cb84:	sxtw	x1, w8
   1cb88:	b.lt	1cc14 <__gmpz_ui_kronecker@@Base+0x11c>  // b.tstop
   1cb8c:	mov	x2, x19
   1cb90:	bl	c3e0 <__gmpn_mod_1@plt>
   1cb94:	b	1cc24 <__gmpz_ui_kronecker@@Base+0x12c>
   1cb98:	cbz	x20, 1cc40 <__gmpz_ui_kronecker@@Base+0x148>
   1cb9c:	tbnz	w20, #0, 1cbc8 <__gmpz_ui_kronecker@@Base+0xd0>
   1cba0:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   1cba4:	cmp	x20, x9
   1cba8:	b.eq	1cc50 <__gmpz_ui_kronecker@@Base+0x158>  // b.none
   1cbac:	rbit	x9, x20
   1cbb0:	clz	x9, x9
   1cbb4:	mov	w21, wzr
   1cbb8:	lsr	x20, x20, x9
   1cbbc:	cmp	x19, #0x1
   1cbc0:	b.eq	1cbd4 <__gmpz_ui_kronecker@@Base+0xdc>  // b.none
   1cbc4:	b	1cb80 <__gmpz_ui_kronecker@@Base+0x88>
   1cbc8:	mov	w21, wzr
   1cbcc:	cmp	x19, #0x1
   1cbd0:	b.ne	1cb80 <__gmpz_ui_kronecker@@Base+0x88>  // b.any
   1cbd4:	and	w8, w21, #0x2
   1cbd8:	mov	w9, #0x1                   	// #1
   1cbdc:	sub	w0, w9, w8
   1cbe0:	ldp	x20, x19, [sp, #32]
   1cbe4:	ldr	x21, [sp, #16]
   1cbe8:	ldp	x29, x30, [sp], #48
   1cbec:	ret
   1cbf0:	cmp	w8, #0x1
   1cbf4:	cset	w8, eq  // eq = none
   1cbf8:	cmp	x20, #0x1
   1cbfc:	cset	w9, eq  // eq = none
   1cc00:	and	w0, w8, w9
   1cc04:	ldp	x20, x19, [sp, #32]
   1cc08:	ldr	x21, [sp, #16]
   1cc0c:	ldp	x29, x30, [sp], #48
   1cc10:	ret
   1cc14:	mov	x2, x19
   1cc18:	mov	x3, xzr
   1cc1c:	eor	w21, w21, w19
   1cc20:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1cc24:	and	w8, w19, w20
   1cc28:	eor	w2, w21, w8
   1cc2c:	mov	x1, x19
   1cc30:	ldp	x20, x19, [sp, #32]
   1cc34:	ldr	x21, [sp, #16]
   1cc38:	ldp	x29, x30, [sp], #48
   1cc3c:	b	c730 <__gmpn_jacobi_base@plt>
   1cc40:	ldr	x20, [x0, #8]!
   1cc44:	sub	w8, w8, #0x1
   1cc48:	cbnz	x20, 1cb9c <__gmpz_ui_kronecker@@Base+0xa4>
   1cc4c:	b	1cc40 <__gmpz_ui_kronecker@@Base+0x148>
   1cc50:	cmp	w8, #0x1
   1cc54:	b.ne	1cc64 <__gmpz_ui_kronecker@@Base+0x16c>  // b.any
   1cc58:	eor	w8, w19, w19, lsr #1
   1cc5c:	and	w8, w8, #0x2
   1cc60:	b	1cbd8 <__gmpz_ui_kronecker@@Base+0xe0>
   1cc64:	ldr	x9, [x0, #8]
   1cc68:	mov	w21, wzr
   1cc6c:	lsl	x20, x9, #1
   1cc70:	cmp	x19, #0x1
   1cc74:	b.eq	1cbd4 <__gmpz_ui_kronecker@@Base+0xdc>  // b.none
   1cc78:	b	1cb80 <__gmpz_ui_kronecker@@Base+0x88>

000000000001cc7c <__gmpz_kronecker_si@@Base>:
   1cc7c:	stp	x29, x30, [sp, #-32]!
   1cc80:	stp	x20, x19, [sp, #16]
   1cc84:	ldrsw	x8, [x0, #4]
   1cc88:	mov	x29, sp
   1cc8c:	cbz	w8, 1ccf8 <__gmpz_kronecker_si@@Base+0x7c>
   1cc90:	ldr	x0, [x0, #8]
   1cc94:	ubfx	x9, x8, #31, #1
   1cc98:	lsr	x10, x1, #63
   1cc9c:	and	w10, w9, w10
   1cca0:	cmp	x1, #0x0
   1cca4:	cneg	x19, x1, mi  // mi = first
   1cca8:	lsl	w10, w10, #1
   1ccac:	tbnz	w19, #0, 1ccd8 <__gmpz_kronecker_si@@Base+0x5c>
   1ccb0:	ldr	x11, [x0]
   1ccb4:	cbz	x19, 1cd3c <__gmpz_kronecker_si@@Base+0xc0>
   1ccb8:	tbz	w11, #0, 1cd60 <__gmpz_kronecker_si@@Base+0xe4>
   1ccbc:	rbit	x12, x1
   1ccc0:	lsr	x13, x11, #1
   1ccc4:	clz	x12, x12
   1ccc8:	eor	w11, w13, w11
   1cccc:	and	w11, w11, w12, lsl #1
   1ccd0:	lsr	x19, x19, x12
   1ccd4:	eor	w10, w11, w10
   1ccd8:	cmp	x19, #0x1
   1ccdc:	b.ne	1cd18 <__gmpz_kronecker_si@@Base+0x9c>  // b.any
   1cce0:	and	w8, w10, #0x2
   1cce4:	mov	w9, #0x1                   	// #1
   1cce8:	sub	w0, w9, w8
   1ccec:	ldp	x20, x19, [sp, #16]
   1ccf0:	ldp	x29, x30, [sp], #32
   1ccf4:	ret
   1ccf8:	cmp	x1, #0x1
   1ccfc:	cset	w8, eq  // eq = none
   1cd00:	cmn	x1, #0x1
   1cd04:	cset	w9, eq  // eq = none
   1cd08:	orr	w0, w8, w9
   1cd0c:	ldp	x20, x19, [sp, #16]
   1cd10:	ldp	x29, x30, [sp], #32
   1cd14:	ret
   1cd18:	cmp	x8, #0x0
   1cd1c:	and	w9, w19, w9, lsl #1
   1cd20:	cneg	x1, x8, mi  // mi = first
   1cd24:	cmp	x1, #0x28
   1cd28:	eor	w20, w9, w10
   1cd2c:	b.lt	1cd70 <__gmpz_kronecker_si@@Base+0xf4>  // b.tstop
   1cd30:	mov	x2, x19
   1cd34:	bl	c3e0 <__gmpn_mod_1@plt>
   1cd38:	b	1cd80 <__gmpz_kronecker_si@@Base+0x104>
   1cd3c:	cmp	w8, #0x1
   1cd40:	b.eq	1cd4c <__gmpz_kronecker_si@@Base+0xd0>  // b.none
   1cd44:	cmn	w8, #0x1
   1cd48:	b.ne	1cd60 <__gmpz_kronecker_si@@Base+0xe4>  // b.any
   1cd4c:	cmp	x11, #0x1
   1cd50:	cset	w0, eq  // eq = none
   1cd54:	ldp	x20, x19, [sp, #16]
   1cd58:	ldp	x29, x30, [sp], #32
   1cd5c:	ret
   1cd60:	mov	w0, wzr
   1cd64:	ldp	x20, x19, [sp, #16]
   1cd68:	ldp	x29, x30, [sp], #32
   1cd6c:	ret
   1cd70:	mov	x2, x19
   1cd74:	mov	x3, xzr
   1cd78:	eor	w20, w20, w19
   1cd7c:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1cd80:	mov	x1, x19
   1cd84:	mov	w2, w20
   1cd88:	ldp	x20, x19, [sp, #16]
   1cd8c:	ldp	x29, x30, [sp], #32
   1cd90:	b	c730 <__gmpn_jacobi_base@plt>

000000000001cd94 <__gmpz_kronecker_ui@@Base>:
   1cd94:	stp	x29, x30, [sp, #-32]!
   1cd98:	stp	x20, x19, [sp, #16]
   1cd9c:	ldrsw	x8, [x0, #4]
   1cda0:	mov	x19, x1
   1cda4:	mov	x29, sp
   1cda8:	cbz	w8, 1ce08 <__gmpz_kronecker_ui@@Base+0x74>
   1cdac:	ldr	x0, [x0, #8]
   1cdb0:	tbnz	w19, #0, 1ce1c <__gmpz_kronecker_ui@@Base+0x88>
   1cdb4:	ldr	x9, [x0]
   1cdb8:	cbz	x19, 1ce44 <__gmpz_kronecker_ui@@Base+0xb0>
   1cdbc:	tbz	w9, #0, 1ce68 <__gmpz_kronecker_ui@@Base+0xd4>
   1cdc0:	rbit	x10, x19
   1cdc4:	lsr	x11, x9, #1
   1cdc8:	clz	x10, x10
   1cdcc:	eor	w9, w11, w9
   1cdd0:	lsr	x19, x19, x10
   1cdd4:	and	w9, w9, w10, lsl #1
   1cdd8:	and	w10, w19, w8, lsr #30
   1cddc:	and	w10, w10, #0x2
   1cde0:	eor	w20, w9, w10
   1cde4:	cmp	x19, #0x1
   1cde8:	b.eq	1ce2c <__gmpz_kronecker_ui@@Base+0x98>  // b.none
   1cdec:	cmp	x8, #0x0
   1cdf0:	cneg	x1, x8, mi  // mi = first
   1cdf4:	cmp	x1, #0x28
   1cdf8:	b.lt	1ce78 <__gmpz_kronecker_ui@@Base+0xe4>  // b.tstop
   1cdfc:	mov	x2, x19
   1ce00:	bl	c3e0 <__gmpn_mod_1@plt>
   1ce04:	b	1ce88 <__gmpz_kronecker_ui@@Base+0xf4>
   1ce08:	cmp	x19, #0x1
   1ce0c:	cset	w0, eq  // eq = none
   1ce10:	ldp	x20, x19, [sp, #16]
   1ce14:	ldp	x29, x30, [sp], #32
   1ce18:	ret
   1ce1c:	and	w9, w19, w8, lsr #30
   1ce20:	and	w20, w9, #0x2
   1ce24:	cmp	x19, #0x1
   1ce28:	b.ne	1cdec <__gmpz_kronecker_ui@@Base+0x58>  // b.any
   1ce2c:	and	w8, w20, #0x2
   1ce30:	mov	w9, #0x1                   	// #1
   1ce34:	sub	w0, w9, w8
   1ce38:	ldp	x20, x19, [sp, #16]
   1ce3c:	ldp	x29, x30, [sp], #32
   1ce40:	ret
   1ce44:	cmp	w8, #0x1
   1ce48:	b.eq	1ce54 <__gmpz_kronecker_ui@@Base+0xc0>  // b.none
   1ce4c:	cmn	w8, #0x1
   1ce50:	b.ne	1ce68 <__gmpz_kronecker_ui@@Base+0xd4>  // b.any
   1ce54:	cmp	x9, #0x1
   1ce58:	cset	w0, eq  // eq = none
   1ce5c:	ldp	x20, x19, [sp, #16]
   1ce60:	ldp	x29, x30, [sp], #32
   1ce64:	ret
   1ce68:	mov	w0, wzr
   1ce6c:	ldp	x20, x19, [sp, #16]
   1ce70:	ldp	x29, x30, [sp], #32
   1ce74:	ret
   1ce78:	mov	x2, x19
   1ce7c:	mov	x3, xzr
   1ce80:	eor	w20, w20, w19
   1ce84:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1ce88:	mov	x1, x19
   1ce8c:	mov	w2, w20
   1ce90:	ldp	x20, x19, [sp, #16]
   1ce94:	ldp	x29, x30, [sp], #32
   1ce98:	b	c730 <__gmpn_jacobi_base@plt>

000000000001ce9c <__gmpz_lcm@@Base>:
   1ce9c:	stp	x29, x30, [sp, #-64]!
   1cea0:	str	x23, [sp, #16]
   1cea4:	stp	x22, x21, [sp, #32]
   1cea8:	stp	x20, x19, [sp, #48]
   1ceac:	mov	x29, sp
   1ceb0:	sub	sp, sp, #0x10
   1ceb4:	ldrsw	x8, [x1, #4]
   1ceb8:	mov	x19, x0
   1cebc:	cbz	w8, 1cf70 <__gmpz_lcm@@Base+0xd4>
   1cec0:	ldr	w9, [x2, #4]
   1cec4:	mov	x20, x2
   1cec8:	cbz	w9, 1cf70 <__gmpz_lcm@@Base+0xd4>
   1cecc:	sxtw	x9, w9
   1ced0:	cmp	x8, #0x0
   1ced4:	cneg	x8, x8, mi  // mi = first
   1ced8:	cmp	x9, #0x0
   1cedc:	mov	x21, x1
   1cee0:	cneg	x9, x9, mi  // mi = first
   1cee4:	cmp	x8, #0x1
   1cee8:	b.eq	1cf78 <__gmpz_lcm@@Base+0xdc>  // b.none
   1ceec:	cmp	x9, #0x1
   1cef0:	b.eq	1cf78 <__gmpz_lcm@@Base+0xdc>  // b.none
   1cef4:	cmp	x8, #0xfe0
   1cef8:	lsl	x1, x8, #3
   1cefc:	str	xzr, [x29, #24]
   1cf00:	stur	w8, [x29, #-16]
   1cf04:	b.hi	1cff0 <__gmpz_lcm@@Base+0x154>  // b.pmore
   1cf08:	add	x9, x1, #0xf
   1cf0c:	mov	x8, sp
   1cf10:	and	x9, x9, #0xfffffffffffffff0
   1cf14:	sub	x0, x8, x9
   1cf18:	mov	sp, x0
   1cf1c:	stur	x0, [x29, #-8]
   1cf20:	sub	x0, x29, #0x10
   1cf24:	mov	x1, x21
   1cf28:	mov	x2, x20
   1cf2c:	bl	cf70 <__gmpz_gcd@plt>
   1cf30:	sub	x0, x29, #0x10
   1cf34:	sub	x2, x29, #0x10
   1cf38:	mov	x1, x21
   1cf3c:	bl	c3f0 <__gmpz_divexact@plt>
   1cf40:	sub	x1, x29, #0x10
   1cf44:	mov	x0, x19
   1cf48:	mov	x2, x20
   1cf4c:	bl	c4b0 <__gmpz_mul@plt>
   1cf50:	ldr	w8, [x19, #4]
   1cf54:	cmp	w8, #0x0
   1cf58:	cneg	w8, w8, mi  // mi = first
   1cf5c:	str	w8, [x19, #4]
   1cf60:	ldr	x0, [x29, #24]
   1cf64:	cbz	x0, 1cfd8 <__gmpz_lcm@@Base+0x13c>
   1cf68:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1cf6c:	b	1cfd8 <__gmpz_lcm@@Base+0x13c>
   1cf70:	str	wzr, [x19, #4]
   1cf74:	b	1cfd8 <__gmpz_lcm@@Base+0x13c>
   1cf78:	ldrsw	x10, [x19]
   1cf7c:	cmp	x8, #0x1
   1cf80:	csel	x22, x9, x8, eq  // eq = none
   1cf84:	csel	x23, x21, x20, eq  // eq = none
   1cf88:	csel	x20, x20, x21, eq  // eq = none
   1cf8c:	cmp	x22, x10
   1cf90:	b.ge	1cffc <__gmpz_lcm@@Base+0x160>  // b.tcont
   1cf94:	ldr	x8, [x23, #8]
   1cf98:	ldr	x20, [x20, #8]
   1cf9c:	mov	x1, x22
   1cfa0:	ldr	x21, [x8]
   1cfa4:	mov	x0, x20
   1cfa8:	mov	x2, x21
   1cfac:	bl	bf90 <__gmpn_gcd_1@plt>
   1cfb0:	ldr	x23, [x19, #8]
   1cfb4:	udiv	x3, x21, x0
   1cfb8:	mov	x1, x20
   1cfbc:	mov	x2, x22
   1cfc0:	mov	x0, x23
   1cfc4:	bl	d490 <__gmpn_mul_1@plt>
   1cfc8:	cmp	x0, #0x0
   1cfcc:	cinc	w8, w22, ne  // ne = any
   1cfd0:	str	x0, [x23, x22, lsl #3]
   1cfd4:	str	w8, [x19, #4]
   1cfd8:	mov	sp, x29
   1cfdc:	ldp	x20, x19, [sp, #48]
   1cfe0:	ldp	x22, x21, [sp, #32]
   1cfe4:	ldr	x23, [sp, #16]
   1cfe8:	ldp	x29, x30, [sp], #64
   1cfec:	ret
   1cff0:	add	x0, x29, #0x18
   1cff4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1cff8:	b	1cf1c <__gmpz_lcm@@Base+0x80>
   1cffc:	add	x1, x22, #0x1
   1d000:	mov	x0, x19
   1d004:	bl	c080 <__gmpz_realloc@plt>
   1d008:	b	1cf94 <__gmpz_lcm@@Base+0xf8>

000000000001d00c <__gmpz_lcm_ui@@Base>:
   1d00c:	stp	x29, x30, [sp, #-64]!
   1d010:	stp	x20, x19, [sp, #48]
   1d014:	mov	x19, x0
   1d018:	mov	w8, wzr
   1d01c:	str	x23, [sp, #16]
   1d020:	stp	x22, x21, [sp, #32]
   1d024:	mov	x29, sp
   1d028:	cbz	x2, 1d08c <__gmpz_lcm_ui@@Base+0x80>
   1d02c:	ldr	w9, [x1, #4]
   1d030:	mov	x22, x1
   1d034:	cbz	w9, 1d08c <__gmpz_lcm_ui@@Base+0x80>
   1d038:	ldrsw	x8, [x19]
   1d03c:	sxtw	x9, w9
   1d040:	cmp	x9, #0x0
   1d044:	cneg	x21, x9, mi  // mi = first
   1d048:	mov	x20, x2
   1d04c:	cmp	x21, x8
   1d050:	b.ge	1d0a4 <__gmpz_lcm_ui@@Base+0x98>  // b.tcont
   1d054:	ldr	x22, [x22, #8]
   1d058:	mov	x1, x21
   1d05c:	mov	x2, x20
   1d060:	mov	x0, x22
   1d064:	bl	bf90 <__gmpn_gcd_1@plt>
   1d068:	ldr	x23, [x19, #8]
   1d06c:	udiv	x3, x20, x0
   1d070:	mov	x1, x22
   1d074:	mov	x2, x21
   1d078:	mov	x0, x23
   1d07c:	bl	d490 <__gmpn_mul_1@plt>
   1d080:	cmp	x0, #0x0
   1d084:	cinc	w8, w21, ne  // ne = any
   1d088:	str	x0, [x23, x21, lsl #3]
   1d08c:	str	w8, [x19, #4]
   1d090:	ldp	x20, x19, [sp, #48]
   1d094:	ldp	x22, x21, [sp, #32]
   1d098:	ldr	x23, [sp, #16]
   1d09c:	ldp	x29, x30, [sp], #64
   1d0a0:	ret
   1d0a4:	add	x1, x21, #0x1
   1d0a8:	mov	x0, x19
   1d0ac:	bl	c080 <__gmpz_realloc@plt>
   1d0b0:	b	1d054 <__gmpz_lcm_ui@@Base+0x48>

000000000001d0b4 <__gmpz_limbs_finish@@Base>:
   1d0b4:	cmp	x1, #0x0
   1d0b8:	cneg	x9, x1, mi  // mi = first
   1d0bc:	mov	x8, x9
   1d0c0:	subs	x9, x9, #0x1
   1d0c4:	b.lt	1d0d8 <__gmpz_limbs_finish@@Base+0x24>  // b.tstop
   1d0c8:	ldr	x10, [x0, #8]
   1d0cc:	add	x10, x10, x8, lsl #3
   1d0d0:	ldur	x10, [x10, #-8]
   1d0d4:	cbz	x10, 1d0bc <__gmpz_limbs_finish@@Base+0x8>
   1d0d8:	neg	w9, w8
   1d0dc:	cmp	x1, #0x0
   1d0e0:	csel	x8, x9, x8, lt  // lt = tstop
   1d0e4:	str	w8, [x0, #4]
   1d0e8:	ret

000000000001d0ec <__gmpz_limbs_modify@@Base>:
   1d0ec:	ldrsw	x8, [x0]
   1d0f0:	cmp	x8, x1
   1d0f4:	b.lt	1d100 <__gmpz_limbs_modify@@Base+0x14>  // b.tstop
   1d0f8:	ldr	x0, [x0, #8]
   1d0fc:	ret
   1d100:	b	c080 <__gmpz_realloc@plt>

000000000001d104 <__gmpz_limbs_read@@Base>:
   1d104:	ldr	x0, [x0, #8]
   1d108:	ret

000000000001d10c <__gmpz_limbs_write@@Base>:
   1d10c:	ldrsw	x8, [x0]
   1d110:	cmp	x8, x1
   1d114:	b.lt	1d120 <__gmpz_limbs_write@@Base+0x14>  // b.tstop
   1d118:	ldr	x0, [x0, #8]
   1d11c:	ret
   1d120:	b	c080 <__gmpz_realloc@plt>

000000000001d124 <__gmpz_lucas_mod@@Base>:
   1d124:	stp	x29, x30, [sp, #-96]!
   1d128:	stp	x22, x21, [sp, #64]
   1d12c:	mov	x21, x1
   1d130:	mov	w1, #0x1                   	// #1
   1d134:	str	x27, [sp, #16]
   1d138:	stp	x26, x25, [sp, #32]
   1d13c:	stp	x24, x23, [sp, #48]
   1d140:	stp	x20, x19, [sp, #80]
   1d144:	mov	x29, sp
   1d148:	mov	x19, x6
   1d14c:	mov	x22, x5
   1d150:	mov	x20, x4
   1d154:	mov	x25, x3
   1d158:	mov	x23, x2
   1d15c:	mov	x26, x0
   1d160:	bl	c170 <__gmpz_set_ui@plt>
   1d164:	mov	w1, #0x2                   	// #2
   1d168:	mov	x0, x20
   1d16c:	bl	d260 <__gmpz_sizeinbase@plt>
   1d170:	sub	x27, x0, #0x2
   1d174:	cmp	x27, x25
   1d178:	b.cc	1d41c <__gmpz_lucas_mod@@Base+0x2f8>  // b.lo, b.ul, b.last
   1d17c:	mov	w1, #0x1                   	// #1
   1d180:	mov	x0, x21
   1d184:	bl	c170 <__gmpz_set_ui@plt>
   1d188:	cmp	x23, #0x0
   1d18c:	neg	x24, x23
   1d190:	b.gt	1d1c4 <__gmpz_lucas_mod@@Base+0xa0>
   1d194:	b	1d290 <__gmpz_lucas_mod@@Base+0x16c>
   1d198:	mov	x0, x21
   1d19c:	mov	x1, x22
   1d1a0:	mov	x2, x20
   1d1a4:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d1a8:	mov	x0, x26
   1d1ac:	mov	x1, x19
   1d1b0:	mov	x2, x20
   1d1b4:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d1b8:	sub	x27, x27, #0x1
   1d1bc:	cmp	x27, x25
   1d1c0:	b.cc	1d330 <__gmpz_lucas_mod@@Base+0x20c>  // b.lo, b.ul, b.last
   1d1c4:	mov	x0, x22
   1d1c8:	mov	x1, x21
   1d1cc:	mov	x2, x21
   1d1d0:	bl	c4b0 <__gmpz_mul@plt>
   1d1d4:	mov	x0, x21
   1d1d8:	mov	x1, x26
   1d1dc:	mov	x2, x21
   1d1e0:	bl	c260 <__gmpz_sub@plt>
   1d1e4:	mov	x0, x19
   1d1e8:	mov	x1, x21
   1d1ec:	mov	x2, x21
   1d1f0:	bl	c4b0 <__gmpz_mul@plt>
   1d1f4:	mov	x0, x21
   1d1f8:	mov	x1, x26
   1d1fc:	mov	x2, x26
   1d200:	bl	c4b0 <__gmpz_mul@plt>
   1d204:	mov	x0, x19
   1d208:	mov	x1, x22
   1d20c:	mov	x2, x19
   1d210:	bl	c260 <__gmpz_sub@plt>
   1d214:	mov	x0, x22
   1d218:	mov	x1, x21
   1d21c:	mov	x2, x23
   1d220:	bl	c870 <__gmpz_submul_ui@plt>
   1d224:	mov	x0, x20
   1d228:	mov	x1, x27
   1d22c:	bl	c480 <__gmpz_tstbit@plt>
   1d230:	cbz	w0, 1d198 <__gmpz_lucas_mod@@Base+0x74>
   1d234:	mov	x0, x19
   1d238:	mov	x1, x19
   1d23c:	mov	x2, x23
   1d240:	bl	cbb0 <__gmpz_mul_si@plt>
   1d244:	mov	x0, x19
   1d248:	mov	x1, x22
   1d24c:	mov	x2, x19
   1d250:	bl	c260 <__gmpz_sub@plt>
   1d254:	mov	x0, x22
   1d258:	mov	x1, x19
   1d25c:	bl	c580 <__gmpz_swap@plt>
   1d260:	b	1d198 <__gmpz_lucas_mod@@Base+0x74>
   1d264:	mov	x0, x21
   1d268:	mov	x1, x22
   1d26c:	mov	x2, x20
   1d270:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d274:	mov	x0, x26
   1d278:	mov	x1, x19
   1d27c:	mov	x2, x20
   1d280:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d284:	sub	x27, x27, #0x1
   1d288:	cmp	x27, x25
   1d28c:	b.cc	1d330 <__gmpz_lucas_mod@@Base+0x20c>  // b.lo, b.ul, b.last
   1d290:	mov	x0, x22
   1d294:	mov	x1, x21
   1d298:	mov	x2, x21
   1d29c:	bl	c4b0 <__gmpz_mul@plt>
   1d2a0:	mov	x0, x21
   1d2a4:	mov	x1, x26
   1d2a8:	mov	x2, x21
   1d2ac:	bl	c260 <__gmpz_sub@plt>
   1d2b0:	mov	x0, x19
   1d2b4:	mov	x1, x21
   1d2b8:	mov	x2, x21
   1d2bc:	bl	c4b0 <__gmpz_mul@plt>
   1d2c0:	mov	x0, x21
   1d2c4:	mov	x1, x26
   1d2c8:	mov	x2, x26
   1d2cc:	bl	c4b0 <__gmpz_mul@plt>
   1d2d0:	mov	x0, x19
   1d2d4:	mov	x1, x22
   1d2d8:	mov	x2, x19
   1d2dc:	bl	c260 <__gmpz_sub@plt>
   1d2e0:	mov	x0, x22
   1d2e4:	mov	x1, x21
   1d2e8:	mov	x2, x24
   1d2ec:	bl	d320 <__gmpz_addmul_ui@plt>
   1d2f0:	mov	x0, x20
   1d2f4:	mov	x1, x27
   1d2f8:	bl	c480 <__gmpz_tstbit@plt>
   1d2fc:	cbz	w0, 1d264 <__gmpz_lucas_mod@@Base+0x140>
   1d300:	mov	x0, x19
   1d304:	mov	x1, x19
   1d308:	mov	x2, x23
   1d30c:	bl	cbb0 <__gmpz_mul_si@plt>
   1d310:	mov	x0, x19
   1d314:	mov	x1, x22
   1d318:	mov	x2, x19
   1d31c:	bl	c260 <__gmpz_sub@plt>
   1d320:	mov	x0, x22
   1d324:	mov	x1, x19
   1d328:	bl	c580 <__gmpz_swap@plt>
   1d32c:	b	1d264 <__gmpz_lucas_mod@@Base+0x140>
   1d330:	ldr	w8, [x21, #4]
   1d334:	cbz	w8, 1d3dc <__gmpz_lucas_mod@@Base+0x2b8>
   1d338:	neg	x2, x23, lsl #1
   1d33c:	mov	x0, x22
   1d340:	mov	x1, x26
   1d344:	bl	cbb0 <__gmpz_mul_si@plt>
   1d348:	mov	x0, x22
   1d34c:	mov	x1, x21
   1d350:	mov	x2, x22
   1d354:	bl	cf90 <__gmpz_add@plt>
   1d358:	mov	x0, x26
   1d35c:	mov	x1, x22
   1d360:	mov	x2, x20
   1d364:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d368:	ldr	w8, [x26, #4]
   1d36c:	cmp	w8, #0x0
   1d370:	cset	w0, eq  // eq = none
   1d374:	cmp	x25, #0x2
   1d378:	b.cc	1d400 <__gmpz_lucas_mod@@Base+0x2dc>  // b.lo, b.ul, b.last
   1d37c:	cbz	w8, 1d400 <__gmpz_lucas_mod@@Base+0x2dc>
   1d380:	mov	x0, x19
   1d384:	mov	x1, x22
   1d388:	mov	x2, x22
   1d38c:	bl	c4b0 <__gmpz_mul@plt>
   1d390:	mov	x0, x22
   1d394:	mov	x1, x21
   1d398:	mov	x2, x21
   1d39c:	bl	c4b0 <__gmpz_mul@plt>
   1d3a0:	mov	x0, x19
   1d3a4:	mov	x1, x19
   1d3a8:	mov	x2, x22
   1d3ac:	bl	c260 <__gmpz_sub@plt>
   1d3b0:	mov	w2, #0x2                   	// #2
   1d3b4:	mov	x0, x19
   1d3b8:	mov	x1, x19
   1d3bc:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1d3c0:	mov	x0, x19
   1d3c4:	mov	x1, x22
   1d3c8:	cmp	x23, #0x1
   1d3cc:	b.lt	1d3e4 <__gmpz_lucas_mod@@Base+0x2c0>  // b.tstop
   1d3d0:	mov	x2, x23
   1d3d4:	bl	d320 <__gmpz_addmul_ui@plt>
   1d3d8:	b	1d3ec <__gmpz_lucas_mod@@Base+0x2c8>
   1d3dc:	mov	w0, #0x1                   	// #1
   1d3e0:	b	1d400 <__gmpz_lucas_mod@@Base+0x2dc>
   1d3e4:	mov	x2, x24
   1d3e8:	bl	c870 <__gmpz_submul_ui@plt>
   1d3ec:	mov	x0, x21
   1d3f0:	mov	x1, x19
   1d3f4:	mov	x2, x20
   1d3f8:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d3fc:	mov	w0, wzr
   1d400:	ldp	x20, x19, [sp, #80]
   1d404:	ldp	x22, x21, [sp, #64]
   1d408:	ldp	x24, x23, [sp, #48]
   1d40c:	ldp	x26, x25, [sp, #32]
   1d410:	ldr	x27, [sp, #16]
   1d414:	ldp	x29, x30, [sp], #96
   1d418:	ret
   1d41c:	mov	x0, x21
   1d420:	mov	x1, x23
   1d424:	bl	d270 <__gmpz_set_si@plt>
   1d428:	b	1d3fc <__gmpz_lucas_mod@@Base+0x2d8>

000000000001d42c <__gmpz_lucnum_ui@@Base>:
   1d42c:	stp	x29, x30, [sp, #-96]!
   1d430:	stp	x20, x19, [sp, #80]
   1d434:	mov	x20, x1
   1d438:	cmp	x1, #0x5c
   1d43c:	mov	x19, x0
   1d440:	str	x27, [sp, #16]
   1d444:	stp	x26, x25, [sp, #32]
   1d448:	stp	x24, x23, [sp, #48]
   1d44c:	stp	x22, x21, [sp, #64]
   1d450:	mov	x29, sp
   1d454:	b.hi	1d48c <__gmpz_lucnum_ui@@Base+0x60>  // b.pmore
   1d458:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1d45c:	ldr	x8, [x8, #3808]
   1d460:	ldr	w9, [x19]
   1d464:	add	x8, x8, x20, lsl #3
   1d468:	ldp	x8, x10, [x8]
   1d46c:	cmp	w9, #0x0
   1d470:	add	x20, x10, x8, lsl #1
   1d474:	b.le	1d6e4 <__gmpz_lucnum_ui@@Base+0x2b8>
   1d478:	ldr	x0, [x19, #8]
   1d47c:	mov	w8, #0x1                   	// #1
   1d480:	str	x20, [x0]
   1d484:	str	w8, [x19, #4]
   1d488:	b	1d64c <__gmpz_lucnum_ui@@Base+0x220>
   1d48c:	lsr	x8, x20, #5
   1d490:	mov	w9, #0x17                  	// #23
   1d494:	ldrsw	x10, [x19]
   1d498:	mul	x8, x8, x9
   1d49c:	lsr	x24, x8, #6
   1d4a0:	add	x22, x24, #0x6
   1d4a4:	cmp	x22, x10
   1d4a8:	b.gt	1d6f4 <__gmpz_lucnum_ui@@Base+0x2c8>
   1d4ac:	ldr	x21, [x19, #8]
   1d4b0:	mov	w23, #0xf6c0                	// #63168
   1d4b4:	movk	w23, #0x3, lsl #16
   1d4b8:	cmp	x24, #0xfda
   1d4bc:	lsl	x1, x22, #3
   1d4c0:	str	xzr, [x29, #24]
   1d4c4:	b.hi	1d708 <__gmpz_lucnum_ui@@Base+0x2dc>  // b.pmore
   1d4c8:	add	x9, x1, #0xf
   1d4cc:	mov	x8, sp
   1d4d0:	and	x9, x9, #0x7ffffffffffffff0
   1d4d4:	sub	x0, x8, x9
   1d4d8:	mov	sp, x0
   1d4dc:	mov	w26, wzr
   1d4e0:	mov	x22, x21
   1d4e4:	mov	x21, x0
   1d4e8:	lsr	x2, x20, #1
   1d4ec:	tbnz	w20, #0, 1d538 <__gmpz_lucnum_ui@@Base+0x10c>
   1d4f0:	cmp	x20, #0xb9
   1d4f4:	add	w26, w26, #0x1
   1d4f8:	mov	x0, x22
   1d4fc:	mov	x20, x2
   1d500:	b.hi	1d4e0 <__gmpz_lucnum_ui@@Base+0xb4>  // b.pmore
   1d504:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1d508:	ldr	x8, [x8, #3808]
   1d50c:	sbfiz	x9, x2, #3, #32
   1d510:	mov	w23, #0x1                   	// #1
   1d514:	mov	x20, x2
   1d518:	add	x10, x8, x2, lsl #3
   1d51c:	ldr	x8, [x8, x9]
   1d520:	ldr	x9, [x10, #8]
   1d524:	mov	x0, x21
   1d528:	add	x8, x9, x8, lsl #1
   1d52c:	str	x8, [x21]
   1d530:	mov	x21, x22
   1d534:	b	1d684 <__gmpz_lucnum_ui@@Base+0x258>
   1d538:	lsr	x8, x20, #6
   1d53c:	mov	w9, #0x17                  	// #23
   1d540:	mul	x8, x8, x9
   1d544:	lsr	x9, x8, #3
   1d548:	add	x10, x23, #0x80
   1d54c:	and	x9, x9, #0xffffffffffffff8
   1d550:	cmp	x8, x10
   1d554:	add	x1, x9, #0x20
   1d558:	b.cs	1d71c <__gmpz_lucnum_ui@@Base+0x2f0>  // b.hs, b.nlast
   1d55c:	add	x9, x1, #0xf
   1d560:	mov	x8, sp
   1d564:	and	x9, x9, #0x3ffffffffffffff0
   1d568:	sub	x23, x8, x9
   1d56c:	mov	sp, x23
   1d570:	mov	x0, x21
   1d574:	mov	x1, x23
   1d578:	bl	d070 <__gmpn_fib2_ui@plt>
   1d57c:	lsl	x27, x0, #3
   1d580:	add	x8, x27, x23
   1d584:	ldur	x8, [x8, #-8]
   1d588:	mov	x24, x0
   1d58c:	mov	x1, x23
   1d590:	mov	x2, x21
   1d594:	cmp	x8, #0x0
   1d598:	cset	w8, eq  // eq = none
   1d59c:	sub	x25, x0, x8
   1d5a0:	mov	x0, x21
   1d5a4:	mov	x3, x24
   1d5a8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   1d5ac:	cmp	x0, #0x0
   1d5b0:	cinc	x24, x24, ne  // ne = any
   1d5b4:	str	x0, [x21, x27]
   1d5b8:	mov	x0, x22
   1d5bc:	mov	x1, x21
   1d5c0:	mov	x2, x24
   1d5c4:	mov	x3, x23
   1d5c8:	mov	x4, x25
   1d5cc:	bl	ccd0 <__gmpn_mul@plt>
   1d5d0:	cmp	x0, #0x0
   1d5d4:	add	x8, x24, x25
   1d5d8:	cset	w9, eq  // eq = none
   1d5dc:	sub	x23, x8, x9
   1d5e0:	mov	x0, x22
   1d5e4:	mov	x1, x22
   1d5e8:	mov	x2, x22
   1d5ec:	mov	x3, x23
   1d5f0:	bl	cba0 <__gmpn_addlsh2_n@plt>
   1d5f4:	str	x0, [x22, x23, lsl #3]
   1d5f8:	ldr	x8, [x22]
   1d5fc:	cmp	x0, #0x0
   1d600:	cinc	x23, x23, ne  // ne = any
   1d604:	tbnz	w20, #1, 1d630 <__gmpz_lucnum_ui@@Base+0x204>
   1d608:	sub	x9, x8, #0x4
   1d60c:	cmp	x8, #0x3
   1d610:	str	x9, [x22]
   1d614:	b.hi	1d638 <__gmpz_lucnum_ui@@Base+0x20c>  // b.pmore
   1d618:	add	x8, x22, #0x8
   1d61c:	ldr	x9, [x8]
   1d620:	sub	x10, x9, #0x1
   1d624:	str	x10, [x8], #8
   1d628:	cbz	x9, 1d61c <__gmpz_lucnum_ui@@Base+0x1f0>
   1d62c:	b	1d638 <__gmpz_lucnum_ui@@Base+0x20c>
   1d630:	add	x8, x8, #0x4
   1d634:	str	x8, [x22]
   1d638:	mov	x0, x22
   1d63c:	cbnz	w26, 1d684 <__gmpz_lucnum_ui@@Base+0x258>
   1d640:	str	w23, [x19, #4]
   1d644:	ldr	x0, [x29, #24]
   1d648:	cbnz	x0, 1d714 <__gmpz_lucnum_ui@@Base+0x2e8>
   1d64c:	mov	sp, x29
   1d650:	ldp	x20, x19, [sp, #80]
   1d654:	ldp	x22, x21, [sp, #64]
   1d658:	ldp	x24, x23, [sp, #48]
   1d65c:	ldp	x26, x25, [sp, #32]
   1d660:	ldr	x27, [sp, #16]
   1d664:	ldp	x29, x30, [sp], #96
   1d668:	ret
   1d66c:	add	x8, x8, #0x2
   1d670:	mov	x20, xzr
   1d674:	str	x8, [x22]
   1d678:	subs	w26, w26, #0x1
   1d67c:	mov	x0, x22
   1d680:	b.eq	1d640 <__gmpz_lucnum_ui@@Base+0x214>  // b.none
   1d684:	mov	x22, x21
   1d688:	mov	x21, x0
   1d68c:	mov	x0, x22
   1d690:	mov	x1, x21
   1d694:	mov	x2, x23
   1d698:	bl	c8e0 <__gmpn_sqr@plt>
   1d69c:	add	x8, x22, x23, lsl #4
   1d6a0:	ldur	x9, [x8, #-8]
   1d6a4:	ldr	x8, [x22]
   1d6a8:	lsl	x10, x23, #1
   1d6ac:	cmp	x9, #0x0
   1d6b0:	cset	w9, eq  // eq = none
   1d6b4:	sub	x23, x10, x9
   1d6b8:	tbnz	w20, #0, 1d66c <__gmpz_lucnum_ui@@Base+0x240>
   1d6bc:	sub	x9, x8, #0x2
   1d6c0:	cmp	x8, #0x1
   1d6c4:	str	x9, [x22]
   1d6c8:	b.hi	1d678 <__gmpz_lucnum_ui@@Base+0x24c>  // b.pmore
   1d6cc:	add	x8, x22, #0x8
   1d6d0:	ldr	x9, [x8]
   1d6d4:	sub	x10, x9, #0x1
   1d6d8:	str	x10, [x8], #8
   1d6dc:	cbz	x9, 1d6d0 <__gmpz_lucnum_ui@@Base+0x2a4>
   1d6e0:	b	1d678 <__gmpz_lucnum_ui@@Base+0x24c>
   1d6e4:	mov	w1, #0x1                   	// #1
   1d6e8:	mov	x0, x19
   1d6ec:	bl	c080 <__gmpz_realloc@plt>
   1d6f0:	b	1d47c <__gmpz_lucnum_ui@@Base+0x50>
   1d6f4:	mov	x0, x19
   1d6f8:	mov	x1, x22
   1d6fc:	bl	c080 <__gmpz_realloc@plt>
   1d700:	mov	x21, x0
   1d704:	b	1d4b0 <__gmpz_lucnum_ui@@Base+0x84>
   1d708:	add	x0, x29, #0x18
   1d70c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d710:	b	1d4dc <__gmpz_lucnum_ui@@Base+0xb0>
   1d714:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d718:	b	1d64c <__gmpz_lucnum_ui@@Base+0x220>
   1d71c:	add	x0, x29, #0x18
   1d720:	mov	x23, x2
   1d724:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d728:	mov	x2, x23
   1d72c:	mov	x23, x0
   1d730:	b	1d570 <__gmpz_lucnum_ui@@Base+0x144>

000000000001d734 <__gmpz_lucnum2_ui@@Base>:
   1d734:	stp	x29, x30, [sp, #-80]!
   1d738:	stp	x22, x21, [sp, #48]
   1d73c:	stp	x20, x19, [sp, #64]
   1d740:	mov	x20, x2
   1d744:	mov	x19, x1
   1d748:	cmp	x2, #0x5c
   1d74c:	mov	x21, x0
   1d750:	str	x25, [sp, #16]
   1d754:	stp	x24, x23, [sp, #32]
   1d758:	mov	x29, sp
   1d75c:	b.hi	1d7c8 <__gmpz_lucnum2_ui@@Base+0x94>  // b.pmore
   1d760:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1d764:	ldr	x8, [x8, #3808]
   1d768:	ldr	w9, [x21]
   1d76c:	add	x8, x8, x20, lsl #3
   1d770:	ldp	x22, x23, [x8]
   1d774:	cmp	w9, #0x0
   1d778:	add	x24, x23, x22, lsl #1
   1d77c:	b.le	1d8a8 <__gmpz_lucnum2_ui@@Base+0x174>
   1d780:	ldr	x0, [x21, #8]
   1d784:	mov	w8, #0x1                   	// #1
   1d788:	str	x24, [x0]
   1d78c:	str	w8, [x21, #4]
   1d790:	ldr	w9, [x19]
   1d794:	lsl	x8, x23, #1
   1d798:	sub	x8, x8, x22
   1d79c:	cmp	x20, #0x0
   1d7a0:	csinc	x21, x8, xzr, ne  // ne = any
   1d7a4:	cmp	w9, #0x0
   1d7a8:	b.le	1d8b8 <__gmpz_lucnum2_ui@@Base+0x184>
   1d7ac:	ldr	x0, [x19, #8]
   1d7b0:	cmp	x20, #0x0
   1d7b4:	mov	w8, #0xffffffff            	// #-1
   1d7b8:	cneg	w8, w8, ne  // ne = any
   1d7bc:	str	x21, [x0]
   1d7c0:	str	w8, [x19, #4]
   1d7c4:	b	1d88c <__gmpz_lucnum2_ui@@Base+0x158>
   1d7c8:	lsr	x8, x20, #5
   1d7cc:	mov	w9, #0x17                  	// #23
   1d7d0:	mul	x8, x8, x9
   1d7d4:	lsr	x23, x8, #6
   1d7d8:	lsl	x8, x23, #3
   1d7dc:	cmp	x23, #0xfdc
   1d7e0:	add	x1, x8, #0x20
   1d7e4:	str	xzr, [x29, #24]
   1d7e8:	b.hi	1d8c8 <__gmpz_lucnum2_ui@@Base+0x194>  // b.pmore
   1d7ec:	add	x9, x1, #0xf
   1d7f0:	mov	x8, sp
   1d7f4:	and	x9, x9, #0x7ffffffffffffff0
   1d7f8:	sub	x22, x8, x9
   1d7fc:	mov	sp, x22
   1d800:	ldrsw	x8, [x21]
   1d804:	add	x24, x23, #0x5
   1d808:	cmp	x24, x8
   1d80c:	b.gt	1d8e4 <__gmpz_lucnum2_ui@@Base+0x1b0>
   1d810:	ldr	x23, [x21, #8]
   1d814:	ldrsw	x8, [x19]
   1d818:	cmp	x24, x8
   1d81c:	b.gt	1d900 <__gmpz_lucnum2_ui@@Base+0x1cc>
   1d820:	ldr	x24, [x19, #8]
   1d824:	mov	x0, x24
   1d828:	mov	x1, x22
   1d82c:	mov	x2, x20
   1d830:	bl	d070 <__gmpn_fib2_ui@plt>
   1d834:	mov	x20, x0
   1d838:	mov	x0, x23
   1d83c:	mov	x1, x24
   1d840:	mov	x2, x22
   1d844:	mov	x3, x20
   1d848:	bl	cc40 <__gmpn_addlsh1_n@plt>
   1d84c:	lsl	x25, x20, #3
   1d850:	cmp	x0, #0x0
   1d854:	str	x0, [x23, x25]
   1d858:	cinc	w8, w20, ne  // ne = any
   1d85c:	mov	x0, x24
   1d860:	mov	x1, x22
   1d864:	mov	x2, x24
   1d868:	mov	x3, x20
   1d86c:	str	w8, [x21, #4]
   1d870:	bl	d090 <__gmpn_rsblsh1_n@plt>
   1d874:	cmp	x0, #0x0
   1d878:	cinc	w8, w20, ne  // ne = any
   1d87c:	str	x0, [x24, x25]
   1d880:	str	w8, [x19, #4]
   1d884:	ldr	x0, [x29, #24]
   1d888:	cbnz	x0, 1d914 <__gmpz_lucnum2_ui@@Base+0x1e0>
   1d88c:	mov	sp, x29
   1d890:	ldp	x20, x19, [sp, #64]
   1d894:	ldp	x22, x21, [sp, #48]
   1d898:	ldp	x24, x23, [sp, #32]
   1d89c:	ldr	x25, [sp, #16]
   1d8a0:	ldp	x29, x30, [sp], #80
   1d8a4:	ret
   1d8a8:	mov	w1, #0x1                   	// #1
   1d8ac:	mov	x0, x21
   1d8b0:	bl	c080 <__gmpz_realloc@plt>
   1d8b4:	b	1d784 <__gmpz_lucnum2_ui@@Base+0x50>
   1d8b8:	mov	w1, #0x1                   	// #1
   1d8bc:	mov	x0, x19
   1d8c0:	bl	c080 <__gmpz_realloc@plt>
   1d8c4:	b	1d7b0 <__gmpz_lucnum2_ui@@Base+0x7c>
   1d8c8:	add	x0, x29, #0x18
   1d8cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d8d0:	mov	x22, x0
   1d8d4:	ldrsw	x8, [x21]
   1d8d8:	add	x24, x23, #0x5
   1d8dc:	cmp	x24, x8
   1d8e0:	b.le	1d810 <__gmpz_lucnum2_ui@@Base+0xdc>
   1d8e4:	mov	x0, x21
   1d8e8:	mov	x1, x24
   1d8ec:	bl	c080 <__gmpz_realloc@plt>
   1d8f0:	mov	x23, x0
   1d8f4:	ldrsw	x8, [x19]
   1d8f8:	cmp	x24, x8
   1d8fc:	b.le	1d820 <__gmpz_lucnum2_ui@@Base+0xec>
   1d900:	mov	x0, x19
   1d904:	mov	x1, x24
   1d908:	bl	c080 <__gmpz_realloc@plt>
   1d90c:	mov	x24, x0
   1d910:	b	1d824 <__gmpz_lucnum2_ui@@Base+0xf0>
   1d914:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d918:	b	1d88c <__gmpz_lucnum2_ui@@Base+0x158>

000000000001d91c <__gmpz_millerrabin@@Base>:
   1d91c:	stp	x29, x30, [sp, #-48]!
   1d920:	stp	x22, x21, [sp, #16]
   1d924:	stp	x20, x19, [sp, #32]
   1d928:	mov	x29, sp
   1d92c:	sub	sp, sp, #0x70
   1d930:	stur	xzr, [x29, #-104]
   1d934:	ldrsw	x8, [x0, #4]
   1d938:	mov	w20, w1
   1d93c:	mov	w9, #0x7f00                	// #32512
   1d940:	mov	x19, x0
   1d944:	add	x8, x8, #0x1
   1d948:	lsl	x1, x8, #3
   1d94c:	cmp	x1, x9
   1d950:	stur	w8, [x29, #-16]
   1d954:	b.hi	1db40 <__gmpz_millerrabin@@Base+0x224>  // b.pmore
   1d958:	add	x9, x1, #0xf
   1d95c:	mov	x8, sp
   1d960:	and	x9, x9, #0xfffffffffffffff0
   1d964:	sub	x0, x8, x9
   1d968:	mov	sp, x0
   1d96c:	stur	x0, [x29, #-8]
   1d970:	sub	x0, x29, #0x10
   1d974:	mov	w2, #0x1                   	// #1
   1d978:	mov	x1, x19
   1d97c:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1d980:	ldr	w8, [x19, #4]
   1d984:	mov	w10, #0x7f00                	// #32512
   1d988:	add	w9, w8, #0x1
   1d98c:	sbfiz	x1, x9, #3, #32
   1d990:	cmp	x1, x10
   1d994:	stur	w9, [x29, #-32]
   1d998:	b.hi	1db4c <__gmpz_millerrabin@@Base+0x230>  // b.pmore
   1d99c:	add	x10, x1, #0xf
   1d9a0:	mov	x9, sp
   1d9a4:	and	x10, x10, #0xfffffffffffffff0
   1d9a8:	sub	x0, x9, x10
   1d9ac:	mov	sp, x0
   1d9b0:	lsl	w9, w8, #1
   1d9b4:	sbfiz	x1, x9, #3, #32
   1d9b8:	mov	w10, #0x7f00                	// #32512
   1d9bc:	cmp	x1, x10
   1d9c0:	stur	x0, [x29, #-24]
   1d9c4:	stur	w9, [x29, #-48]
   1d9c8:	b.hi	1db5c <__gmpz_millerrabin@@Base+0x240>  // b.pmore
   1d9cc:	add	x10, x1, #0xf
   1d9d0:	mov	x9, sp
   1d9d4:	and	x10, x10, #0xfffffffffffffff0
   1d9d8:	sub	x0, x9, x10
   1d9dc:	mov	sp, x0
   1d9e0:	sbfiz	x1, x8, #3, #32
   1d9e4:	mov	w9, #0x7f00                	// #32512
   1d9e8:	cmp	x1, x9
   1d9ec:	stur	x0, [x29, #-40]
   1d9f0:	stur	w8, [x29, #-64]
   1d9f4:	b.hi	1db6c <__gmpz_millerrabin@@Base+0x250>  // b.pmore
   1d9f8:	add	x9, x1, #0xf
   1d9fc:	mov	x8, sp
   1da00:	and	x9, x9, #0xfffffffffffffff0
   1da04:	sub	x0, x8, x9
   1da08:	mov	sp, x0
   1da0c:	stur	x0, [x29, #-56]
   1da10:	sub	x0, x29, #0x10
   1da14:	mov	x1, xzr
   1da18:	bl	bf20 <__gmpz_scan1@plt>
   1da1c:	mov	x21, x0
   1da20:	sub	x0, x29, #0x40
   1da24:	sub	x1, x29, #0x10
   1da28:	mov	x2, x21
   1da2c:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1da30:	sub	x0, x29, #0x20
   1da34:	mov	w1, #0x2                   	// #2
   1da38:	add	x21, x21, #0x1
   1da3c:	bl	c170 <__gmpz_set_ui@plt>
   1da40:	sub	x1, x29, #0x20
   1da44:	sub	x2, x29, #0x30
   1da48:	sub	x3, x29, #0x40
   1da4c:	mov	x0, x19
   1da50:	mov	x4, x21
   1da54:	bl	1db80 <__gmpz_millerrabin@@Base+0x264>
   1da58:	cbz	w0, 1da98 <__gmpz_millerrabin@@Base+0x17c>
   1da5c:	sub	x1, x29, #0x20
   1da60:	sub	x2, x29, #0x30
   1da64:	mov	x0, x19
   1da68:	bl	c9c0 <__gmpz_stronglucas@plt>
   1da6c:	cbz	w0, 1da98 <__gmpz_millerrabin@@Base+0x17c>
   1da70:	ldr	x8, [x19, #8]
   1da74:	ldr	w9, [x19, #4]
   1da78:	ldr	x8, [x8]
   1da7c:	lsr	x8, x8, #46
   1da80:	cmp	x8, #0x13
   1da84:	cset	w8, cc  // cc = lo, ul, last
   1da88:	cmp	w9, w8
   1da8c:	b.ne	1dabc <__gmpz_millerrabin@@Base+0x1a0>  // b.any
   1da90:	mov	w20, #0x2                   	// #2
   1da94:	b	1da9c <__gmpz_millerrabin@@Base+0x180>
   1da98:	mov	w20, wzr
   1da9c:	ldur	x0, [x29, #-104]
   1daa0:	cbnz	x0, 1db78 <__gmpz_millerrabin@@Base+0x25c>
   1daa4:	mov	w0, w20
   1daa8:	mov	sp, x29
   1daac:	ldp	x20, x19, [sp, #32]
   1dab0:	ldp	x22, x21, [sp, #16]
   1dab4:	ldp	x29, x30, [sp], #48
   1dab8:	ret
   1dabc:	cmp	w20, #0x19
   1dac0:	b.lt	1db38 <__gmpz_millerrabin@@Base+0x21c>  // b.tstop
   1dac4:	sub	x0, x29, #0x10
   1dac8:	sub	x1, x29, #0x10
   1dacc:	mov	w2, #0x2                   	// #2
   1dad0:	sub	w22, w20, #0x18
   1dad4:	bl	c120 <__gmpz_sub_ui@plt>
   1dad8:	sub	x0, x29, #0x60
   1dadc:	bl	c7a0 <__gmp_randinit_default@plt>
   1dae0:	sub	x0, x29, #0x20
   1dae4:	sub	x1, x29, #0x60
   1dae8:	sub	x2, x29, #0x10
   1daec:	bl	c8f0 <__gmpz_urandomm@plt>
   1daf0:	sub	x0, x29, #0x20
   1daf4:	sub	x1, x29, #0x20
   1daf8:	mov	w2, #0x3                   	// #3
   1dafc:	bl	c8b0 <__gmpz_add_ui@plt>
   1db00:	sub	x1, x29, #0x20
   1db04:	sub	x2, x29, #0x30
   1db08:	sub	x3, x29, #0x40
   1db0c:	mov	x0, x19
   1db10:	mov	x4, x21
   1db14:	bl	1db80 <__gmpz_millerrabin@@Base+0x264>
   1db18:	cmp	w22, #0x2
   1db1c:	mov	w20, w0
   1db20:	b.lt	1db2c <__gmpz_millerrabin@@Base+0x210>  // b.tstop
   1db24:	sub	w22, w22, #0x1
   1db28:	cbnz	w20, 1dae0 <__gmpz_millerrabin@@Base+0x1c4>
   1db2c:	sub	x0, x29, #0x60
   1db30:	bl	c490 <__gmp_randclear@plt>
   1db34:	b	1da9c <__gmpz_millerrabin@@Base+0x180>
   1db38:	mov	w20, #0x1                   	// #1
   1db3c:	b	1da9c <__gmpz_millerrabin@@Base+0x180>
   1db40:	sub	x0, x29, #0x68
   1db44:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1db48:	b	1d96c <__gmpz_millerrabin@@Base+0x50>
   1db4c:	sub	x0, x29, #0x68
   1db50:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1db54:	ldr	w8, [x19, #4]
   1db58:	b	1d9b0 <__gmpz_millerrabin@@Base+0x94>
   1db5c:	sub	x0, x29, #0x68
   1db60:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1db64:	ldr	w8, [x19, #4]
   1db68:	b	1d9e0 <__gmpz_millerrabin@@Base+0xc4>
   1db6c:	sub	x0, x29, #0x68
   1db70:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1db74:	b	1da0c <__gmpz_millerrabin@@Base+0xf0>
   1db78:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1db7c:	b	1daa4 <__gmpz_millerrabin@@Base+0x188>
   1db80:	stp	x29, x30, [sp, #-64]!
   1db84:	stp	x22, x21, [sp, #32]
   1db88:	mov	x21, x0
   1db8c:	stp	x20, x19, [sp, #48]
   1db90:	mov	x20, x2
   1db94:	mov	x0, x2
   1db98:	mov	x2, x3
   1db9c:	mov	x3, x21
   1dba0:	str	x23, [sp, #16]
   1dba4:	mov	x29, sp
   1dba8:	mov	x19, x4
   1dbac:	bl	c370 <__gmpz_powm@plt>
   1dbb0:	mov	w1, #0x1                   	// #1
   1dbb4:	mov	x0, x20
   1dbb8:	mov	w22, #0x1                   	// #1
   1dbbc:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1dbc0:	cbz	w0, 1dcc0 <__gmpz_millerrabin@@Base+0x3a4>
   1dbc4:	ldrsw	x8, [x21, #4]
   1dbc8:	ldr	w9, [x20, #4]
   1dbcc:	cmp	w9, w8
   1dbd0:	b.ne	1dc18 <__gmpz_millerrabin@@Base+0x2fc>  // b.any
   1dbd4:	ldr	x10, [x20, #8]
   1dbd8:	ldr	x9, [x21, #8]
   1dbdc:	ldr	x11, [x10]
   1dbe0:	ldr	x12, [x9]
   1dbe4:	eor	x11, x11, #0x1
   1dbe8:	cmp	x11, x12
   1dbec:	b.ne	1dc18 <__gmpz_millerrabin@@Base+0x2fc>  // b.any
   1dbf0:	sub	x9, x9, #0x8
   1dbf4:	sub	x10, x10, #0x8
   1dbf8:	cmp	x8, #0x2
   1dbfc:	b.lt	1dcb4 <__gmpz_millerrabin@@Base+0x398>  // b.tstop
   1dc00:	lsl	x11, x8, #3
   1dc04:	ldr	x12, [x10, x11]
   1dc08:	ldr	x11, [x9, x11]
   1dc0c:	sub	x8, x8, #0x1
   1dc10:	cmp	x12, x11
   1dc14:	b.eq	1dbf8 <__gmpz_millerrabin@@Base+0x2dc>  // b.none
   1dc18:	cmp	x19, #0x2
   1dc1c:	b.cc	1dcbc <__gmpz_millerrabin@@Base+0x3a0>  // b.lo, b.ul, b.last
   1dc20:	mov	w23, #0x1                   	// #1
   1dc24:	mov	w2, #0x2                   	// #2
   1dc28:	mov	x0, x20
   1dc2c:	mov	x1, x20
   1dc30:	mov	x3, x21
   1dc34:	bl	d2c0 <__gmpz_powm_ui@plt>
   1dc38:	ldrsw	x8, [x21, #4]
   1dc3c:	ldr	w9, [x20, #4]
   1dc40:	cmp	w9, w8
   1dc44:	b.ne	1dc8c <__gmpz_millerrabin@@Base+0x370>  // b.any
   1dc48:	ldr	x10, [x20, #8]
   1dc4c:	ldr	x9, [x21, #8]
   1dc50:	ldr	x11, [x10]
   1dc54:	ldr	x12, [x9]
   1dc58:	eor	x11, x11, #0x1
   1dc5c:	cmp	x11, x12
   1dc60:	b.ne	1dc8c <__gmpz_millerrabin@@Base+0x370>  // b.any
   1dc64:	sub	x9, x9, #0x8
   1dc68:	sub	x10, x10, #0x8
   1dc6c:	cmp	x8, #0x2
   1dc70:	b.lt	1dcb4 <__gmpz_millerrabin@@Base+0x398>  // b.tstop
   1dc74:	lsl	x11, x8, #3
   1dc78:	ldr	x12, [x10, x11]
   1dc7c:	ldr	x11, [x9, x11]
   1dc80:	sub	x8, x8, #0x1
   1dc84:	cmp	x12, x11
   1dc88:	b.eq	1dc6c <__gmpz_millerrabin@@Base+0x350>  // b.none
   1dc8c:	mov	w1, #0x1                   	// #1
   1dc90:	mov	x0, x20
   1dc94:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1dc98:	cmp	w0, #0x1
   1dc9c:	mov	w22, wzr
   1dca0:	b.lt	1dcc0 <__gmpz_millerrabin@@Base+0x3a4>  // b.tstop
   1dca4:	add	x23, x23, #0x1
   1dca8:	cmp	x23, x19
   1dcac:	b.ne	1dc24 <__gmpz_millerrabin@@Base+0x308>  // b.any
   1dcb0:	b	1dcc0 <__gmpz_millerrabin@@Base+0x3a4>
   1dcb4:	mov	w22, #0x1                   	// #1
   1dcb8:	b	1dcc0 <__gmpz_millerrabin@@Base+0x3a4>
   1dcbc:	mov	w22, wzr
   1dcc0:	mov	w0, w22
   1dcc4:	ldp	x20, x19, [sp, #48]
   1dcc8:	ldp	x22, x21, [sp, #32]
   1dccc:	ldr	x23, [sp, #16]
   1dcd0:	ldp	x29, x30, [sp], #64
   1dcd4:	ret

000000000001dcd8 <__gmpz_mod@@Base>:
   1dcd8:	stp	x29, x30, [sp, #-48]!
   1dcdc:	stp	x22, x21, [sp, #16]
   1dce0:	stp	x20, x19, [sp, #32]
   1dce4:	mov	x29, sp
   1dce8:	sub	sp, sp, #0x20
   1dcec:	stur	xzr, [x29, #-24]
   1dcf0:	ldr	w8, [x2, #4]
   1dcf4:	mov	x22, x2
   1dcf8:	mov	x19, x0
   1dcfc:	mov	x20, x1
   1dd00:	cmp	w8, #0x0
   1dd04:	cneg	w21, w8, mi  // mi = first
   1dd08:	cmp	x0, x2
   1dd0c:	b.eq	1dd1c <__gmpz_mod@@Base+0x44>  // b.none
   1dd10:	ldr	x8, [x22, #8]
   1dd14:	stur	x8, [x29, #-8]
   1dd18:	b	1dd4c <__gmpz_mod@@Base+0x74>
   1dd1c:	cmp	w21, #0xfe0
   1dd20:	lsl	x1, x21, #3
   1dd24:	b.hi	1dd9c <__gmpz_mod@@Base+0xc4>  // b.pmore
   1dd28:	add	x9, x1, #0xf
   1dd2c:	mov	x8, sp
   1dd30:	and	x9, x9, #0xffffffff0
   1dd34:	sub	x0, x8, x9
   1dd38:	mov	sp, x0
   1dd3c:	stur	x0, [x29, #-8]
   1dd40:	ldr	x1, [x22, #8]
   1dd44:	mov	x2, x21
   1dd48:	bl	ca50 <__gmpn_copyi@plt>
   1dd4c:	sub	x2, x29, #0x10
   1dd50:	mov	x0, x19
   1dd54:	mov	x1, x20
   1dd58:	stur	w21, [x29, #-12]
   1dd5c:	bl	ca80 <__gmpz_tdiv_r@plt>
   1dd60:	ldr	w8, [x19, #4]
   1dd64:	tbz	w8, #31, 1dd78 <__gmpz_mod@@Base+0xa0>
   1dd68:	sub	x2, x29, #0x10
   1dd6c:	mov	x0, x19
   1dd70:	mov	x1, x19
   1dd74:	bl	cf90 <__gmpz_add@plt>
   1dd78:	ldur	x0, [x29, #-24]
   1dd7c:	cbnz	x0, 1dd94 <__gmpz_mod@@Base+0xbc>
   1dd80:	mov	sp, x29
   1dd84:	ldp	x20, x19, [sp, #32]
   1dd88:	ldp	x22, x21, [sp, #16]
   1dd8c:	ldp	x29, x30, [sp], #48
   1dd90:	ret
   1dd94:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1dd98:	b	1dd80 <__gmpz_mod@@Base+0xa8>
   1dd9c:	sub	x0, x29, #0x18
   1dda0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1dda4:	b	1dd3c <__gmpz_mod@@Base+0x64>

000000000001dda8 <__gmpz_mul@@Base>:
   1dda8:	stp	x29, x30, [sp, #-96]!
   1ddac:	stp	x28, x27, [sp, #16]
   1ddb0:	stp	x26, x25, [sp, #32]
   1ddb4:	stp	x24, x23, [sp, #48]
   1ddb8:	stp	x22, x21, [sp, #64]
   1ddbc:	stp	x20, x19, [sp, #80]
   1ddc0:	mov	x29, sp
   1ddc4:	sub	sp, sp, #0x10
   1ddc8:	ldrsw	x8, [x1, #4]
   1ddcc:	ldrsw	x9, [x2, #4]
   1ddd0:	mov	x19, x0
   1ddd4:	cmp	x8, #0x0
   1ddd8:	cneg	x10, x8, mi  // mi = first
   1dddc:	cmp	x9, #0x0
   1dde0:	cneg	x11, x9, mi  // mi = first
   1dde4:	cmp	x10, x11
   1dde8:	csel	x21, x10, x11, lt  // lt = tstop
   1ddec:	csel	x20, x11, x10, lt  // lt = tstop
   1ddf0:	csel	x23, x1, x2, lt  // lt = tstop
   1ddf4:	csel	x22, x2, x1, lt  // lt = tstop
   1ddf8:	cmp	x21, #0x1
   1ddfc:	eor	w27, w9, w8
   1de00:	b.eq	1de10 <__gmpz_mul@@Base+0x68>  // b.none
   1de04:	cbnz	x21, 1de58 <__gmpz_mul@@Base+0xb0>
   1de08:	str	wzr, [x19, #4]
   1de0c:	b	1e008 <__gmpz_mul@@Base+0x260>
   1de10:	ldrsw	x8, [x19]
   1de14:	cmp	x20, x8
   1de18:	b.ge	1e028 <__gmpz_mul@@Base+0x280>  // b.tcont
   1de1c:	ldr	x21, [x19, #8]
   1de20:	ldr	x8, [x23, #8]
   1de24:	ldr	x1, [x22, #8]
   1de28:	mov	x0, x21
   1de2c:	mov	x2, x20
   1de30:	ldr	x3, [x8]
   1de34:	bl	d490 <__gmpn_mul_1@plt>
   1de38:	cmp	x0, #0x0
   1de3c:	cinc	x8, x20, ne  // ne = any
   1de40:	neg	w9, w8
   1de44:	cmp	w27, #0x0
   1de48:	csel	x8, x8, x9, ge  // ge = tcont
   1de4c:	str	x0, [x21, x20, lsl #3]
   1de50:	str	w8, [x19, #4]
   1de54:	b	1e008 <__gmpz_mul@@Base+0x260>
   1de58:	ldrsw	x9, [x19]
   1de5c:	ldr	x22, [x22, #8]
   1de60:	ldr	x23, [x23, #8]
   1de64:	ldr	x24, [x19, #8]
   1de68:	add	x28, x20, x21
   1de6c:	cmp	x28, x9
   1de70:	stp	x9, xzr, [x29, #-16]
   1de74:	b.le	1dedc <__gmpz_mul@@Base+0x134>
   1de78:	cbz	w9, 1dea4 <__gmpz_mul@@Base+0xfc>
   1de7c:	cmp	x24, x22
   1de80:	b.eq	1dea8 <__gmpz_mul@@Base+0x100>  // b.none
   1de84:	cmp	x24, x23
   1de88:	b.eq	1dea8 <__gmpz_mul@@Base+0x100>  // b.none
   1de8c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1de90:	ldr	x8, [x8, #4016]
   1de94:	lsl	x1, x9, #3
   1de98:	mov	x0, x24
   1de9c:	ldr	x8, [x8]
   1dea0:	blr	x8
   1dea4:	mov	x24, xzr
   1dea8:	str	w28, [x19]
   1deac:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1deb0:	ldr	x8, [x8, #3840]
   1deb4:	lsl	x0, x28, #3
   1deb8:	ldr	x8, [x8]
   1debc:	blr	x8
   1dec0:	mov	x26, x22
   1dec4:	str	x0, [x19, #8]
   1dec8:	mov	x25, x23
   1decc:	mov	x22, x0
   1ded0:	cmp	x26, x25
   1ded4:	b.ne	1df4c <__gmpz_mul@@Base+0x1a4>  // b.any
   1ded8:	b	1dfac <__gmpz_mul@@Base+0x204>
   1dedc:	cmp	x24, x22
   1dee0:	b.eq	1df08 <__gmpz_mul@@Base+0x160>  // b.none
   1dee4:	cmp	x24, x23
   1dee8:	b.eq	1df68 <__gmpz_mul@@Base+0x1c0>  // b.none
   1deec:	mov	x26, x22
   1def0:	mov	x25, x23
   1def4:	mov	x22, x24
   1def8:	mov	x24, xzr
   1defc:	cmp	x26, x25
   1df00:	b.ne	1df4c <__gmpz_mul@@Base+0x1a4>  // b.any
   1df04:	b	1dfac <__gmpz_mul@@Base+0x204>
   1df08:	cmp	x20, #0xfe0
   1df0c:	lsl	x1, x20, #3
   1df10:	b.hi	1e044 <__gmpz_mul@@Base+0x29c>  // b.pmore
   1df14:	add	x9, x1, #0xf
   1df18:	mov	x8, sp
   1df1c:	and	x9, x9, #0xfffffffffffffff0
   1df20:	sub	x26, x8, x9
   1df24:	mov	sp, x26
   1df28:	cmp	x22, x23
   1df2c:	mov	x0, x26
   1df30:	mov	x1, x22
   1df34:	mov	x2, x20
   1df38:	csel	x25, x26, x23, eq  // eq = none
   1df3c:	bl	ca50 <__gmpn_copyi@plt>
   1df40:	mov	x24, xzr
   1df44:	cmp	x26, x25
   1df48:	b.eq	1dfac <__gmpz_mul@@Base+0x204>  // b.none
   1df4c:	mov	x0, x22
   1df50:	mov	x1, x26
   1df54:	mov	x2, x20
   1df58:	mov	x3, x25
   1df5c:	mov	x4, x21
   1df60:	bl	ccd0 <__gmpn_mul@plt>
   1df64:	b	1dfc4 <__gmpz_mul@@Base+0x21c>
   1df68:	cmp	x21, #0xfe0
   1df6c:	lsl	x1, x21, #3
   1df70:	b.hi	1e054 <__gmpz_mul@@Base+0x2ac>  // b.pmore
   1df74:	add	x9, x1, #0xf
   1df78:	mov	x8, sp
   1df7c:	and	x9, x9, #0xfffffffffffffff0
   1df80:	sub	x25, x8, x9
   1df84:	mov	sp, x25
   1df88:	mov	x0, x25
   1df8c:	mov	x1, x23
   1df90:	mov	x2, x21
   1df94:	bl	ca50 <__gmpn_copyi@plt>
   1df98:	mov	x24, xzr
   1df9c:	mov	x26, x22
   1dfa0:	mov	x22, x23
   1dfa4:	cmp	x26, x25
   1dfa8:	b.ne	1df4c <__gmpz_mul@@Base+0x1a4>  // b.any
   1dfac:	mov	x0, x22
   1dfb0:	mov	x1, x26
   1dfb4:	mov	x2, x20
   1dfb8:	bl	c8e0 <__gmpn_sqr@plt>
   1dfbc:	add	x8, x22, x28, lsl #3
   1dfc0:	ldur	x0, [x8, #-8]
   1dfc4:	cmp	x0, #0x0
   1dfc8:	cset	w8, eq  // eq = none
   1dfcc:	sub	x8, x28, x8
   1dfd0:	neg	w9, w8
   1dfd4:	cmp	w27, #0x0
   1dfd8:	csel	x8, x9, x8, lt  // lt = tstop
   1dfdc:	str	w8, [x19, #4]
   1dfe0:	cbz	x24, 1e000 <__gmpz_mul@@Base+0x258>
   1dfe4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1dfe8:	ldr	x8, [x8, #4016]
   1dfec:	ldur	x9, [x29, #-16]
   1dff0:	mov	x0, x24
   1dff4:	ldr	x8, [x8]
   1dff8:	lsl	x1, x9, #3
   1dffc:	blr	x8
   1e000:	ldur	x0, [x29, #-8]
   1e004:	cbnz	x0, 1e03c <__gmpz_mul@@Base+0x294>
   1e008:	mov	sp, x29
   1e00c:	ldp	x20, x19, [sp, #80]
   1e010:	ldp	x22, x21, [sp, #64]
   1e014:	ldp	x24, x23, [sp, #48]
   1e018:	ldp	x26, x25, [sp, #32]
   1e01c:	ldp	x28, x27, [sp, #16]
   1e020:	ldp	x29, x30, [sp], #96
   1e024:	ret
   1e028:	add	x1, x20, #0x1
   1e02c:	mov	x0, x19
   1e030:	bl	c080 <__gmpz_realloc@plt>
   1e034:	mov	x21, x0
   1e038:	b	1de20 <__gmpz_mul@@Base+0x78>
   1e03c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1e040:	b	1e008 <__gmpz_mul@@Base+0x260>
   1e044:	sub	x0, x29, #0x8
   1e048:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e04c:	mov	x26, x0
   1e050:	b	1df28 <__gmpz_mul@@Base+0x180>
   1e054:	sub	x0, x29, #0x8
   1e058:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e05c:	mov	x25, x0
   1e060:	b	1df88 <__gmpz_mul@@Base+0x1e0>

000000000001e064 <__gmpz_mul_2exp@@Base>:
   1e064:	stp	x29, x30, [sp, #-80]!
   1e068:	stp	x24, x23, [sp, #32]
   1e06c:	stp	x22, x21, [sp, #48]
   1e070:	stp	x20, x19, [sp, #64]
   1e074:	ldr	w8, [x1, #4]
   1e078:	mov	x20, x1
   1e07c:	mov	x19, x0
   1e080:	str	x25, [sp, #16]
   1e084:	cmp	w8, #0x0
   1e088:	cneg	w22, w8, mi  // mi = first
   1e08c:	mov	x29, sp
   1e090:	cbz	w22, 1e0dc <__gmpz_mul_2exp@@Base+0x78>
   1e094:	ldrsw	x8, [x19]
   1e098:	lsr	x25, x2, #6
   1e09c:	add	x24, x25, x22
   1e0a0:	mov	x23, x2
   1e0a4:	cmp	x24, x8
   1e0a8:	b.ge	1e128 <__gmpz_mul_2exp@@Base+0xc4>  // b.tcont
   1e0ac:	ldr	x21, [x19, #8]
   1e0b0:	ldr	x1, [x20, #8]
   1e0b4:	ands	x3, x23, #0x3f
   1e0b8:	add	x0, x21, x25, lsl #3
   1e0bc:	mov	x2, x22
   1e0c0:	b.eq	1e0e4 <__gmpz_mul_2exp@@Base+0x80>  // b.none
   1e0c4:	bl	c180 <__gmpn_lshift@plt>
   1e0c8:	cmp	x0, #0x0
   1e0cc:	str	x0, [x21, x24, lsl #3]
   1e0d0:	cinc	x24, x24, ne  // ne = any
   1e0d4:	cbnz	x25, 1e0ec <__gmpz_mul_2exp@@Base+0x88>
   1e0d8:	b	1e0fc <__gmpz_mul_2exp@@Base+0x98>
   1e0dc:	mov	x24, xzr
   1e0e0:	b	1e0fc <__gmpz_mul_2exp@@Base+0x98>
   1e0e4:	bl	c000 <__gmpn_copyd@plt>
   1e0e8:	cbz	x25, 1e0fc <__gmpz_mul_2exp@@Base+0x98>
   1e0ec:	lsl	x2, x25, #3
   1e0f0:	mov	x0, x21
   1e0f4:	mov	w1, wzr
   1e0f8:	bl	c5f0 <memset@plt>
   1e0fc:	ldr	w8, [x20, #4]
   1e100:	neg	w9, w24
   1e104:	ldr	x25, [sp, #16]
   1e108:	cmp	w8, #0x0
   1e10c:	csel	x8, x24, x9, ge  // ge = tcont
   1e110:	str	w8, [x19, #4]
   1e114:	ldp	x20, x19, [sp, #64]
   1e118:	ldp	x22, x21, [sp, #48]
   1e11c:	ldp	x24, x23, [sp, #32]
   1e120:	ldp	x29, x30, [sp], #80
   1e124:	ret
   1e128:	add	x1, x24, #0x1
   1e12c:	mov	x0, x19
   1e130:	bl	c080 <__gmpz_realloc@plt>
   1e134:	mov	x21, x0
   1e138:	b	1e0b0 <__gmpz_mul_2exp@@Base+0x4c>

000000000001e13c <__gmpz_mul_si@@Base>:
   1e13c:	stp	x29, x30, [sp, #-80]!
   1e140:	stp	x20, x19, [sp, #64]
   1e144:	mov	x19, x0
   1e148:	mov	w8, wzr
   1e14c:	str	x25, [sp, #16]
   1e150:	stp	x24, x23, [sp, #32]
   1e154:	stp	x22, x21, [sp, #48]
   1e158:	mov	x29, sp
   1e15c:	cbz	x2, 1e1c0 <__gmpz_mul_si@@Base+0x84>
   1e160:	ldr	w9, [x1, #4]
   1e164:	mov	x21, x1
   1e168:	cbz	w9, 1e1c0 <__gmpz_mul_si@@Base+0x84>
   1e16c:	ldrsw	x8, [x19]
   1e170:	sxtw	x25, w9
   1e174:	cmp	x25, #0x0
   1e178:	cneg	x22, x25, mi  // mi = first
   1e17c:	cmp	x2, #0x0
   1e180:	mov	x20, x2
   1e184:	cneg	x23, x2, mi  // mi = first
   1e188:	cmp	x22, x8
   1e18c:	b.ge	1e1dc <__gmpz_mul_si@@Base+0xa0>  // b.tcont
   1e190:	ldr	x24, [x19, #8]
   1e194:	ldr	x1, [x21, #8]
   1e198:	mov	x0, x24
   1e19c:	mov	x2, x22
   1e1a0:	mov	x3, x23
   1e1a4:	bl	d490 <__gmpn_mul_1@plt>
   1e1a8:	cmp	x0, #0x0
   1e1ac:	lsr	x8, x20, #63
   1e1b0:	cinc	w9, w22, ne  // ne = any
   1e1b4:	cmp	w8, w25, lsr #31
   1e1b8:	cneg	w8, w9, ne  // ne = any
   1e1bc:	str	x0, [x24, x22, lsl #3]
   1e1c0:	str	w8, [x19, #4]
   1e1c4:	ldp	x20, x19, [sp, #64]
   1e1c8:	ldp	x22, x21, [sp, #48]
   1e1cc:	ldp	x24, x23, [sp, #32]
   1e1d0:	ldr	x25, [sp, #16]
   1e1d4:	ldp	x29, x30, [sp], #80
   1e1d8:	ret
   1e1dc:	add	x1, x22, #0x1
   1e1e0:	mov	x0, x19
   1e1e4:	bl	c080 <__gmpz_realloc@plt>
   1e1e8:	mov	x24, x0
   1e1ec:	b	1e194 <__gmpz_mul_si@@Base+0x58>

000000000001e1f0 <__gmpz_mul_ui@@Base>:
   1e1f0:	stp	x29, x30, [sp, #-64]!
   1e1f4:	stp	x20, x19, [sp, #48]
   1e1f8:	mov	x19, x0
   1e1fc:	mov	w8, wzr
   1e200:	stp	x24, x23, [sp, #16]
   1e204:	stp	x22, x21, [sp, #32]
   1e208:	mov	x29, sp
   1e20c:	cbz	x2, 1e264 <__gmpz_mul_ui@@Base+0x74>
   1e210:	ldr	w9, [x1, #4]
   1e214:	mov	x21, x1
   1e218:	cbz	w9, 1e264 <__gmpz_mul_ui@@Base+0x74>
   1e21c:	ldrsw	x8, [x19]
   1e220:	sxtw	x24, w9
   1e224:	cmp	x24, #0x0
   1e228:	cneg	x22, x24, mi  // mi = first
   1e22c:	mov	x20, x2
   1e230:	cmp	x22, x8
   1e234:	b.ge	1e27c <__gmpz_mul_ui@@Base+0x8c>  // b.tcont
   1e238:	ldr	x23, [x19, #8]
   1e23c:	ldr	x1, [x21, #8]
   1e240:	mov	x0, x23
   1e244:	mov	x2, x22
   1e248:	mov	x3, x20
   1e24c:	bl	d490 <__gmpn_mul_1@plt>
   1e250:	cmp	x0, #0x0
   1e254:	cinc	w8, w22, ne  // ne = any
   1e258:	cmp	w24, #0x0
   1e25c:	cneg	w8, w8, lt  // lt = tstop
   1e260:	str	x0, [x23, x22, lsl #3]
   1e264:	str	w8, [x19, #4]
   1e268:	ldp	x20, x19, [sp, #48]
   1e26c:	ldp	x22, x21, [sp, #32]
   1e270:	ldp	x24, x23, [sp, #16]
   1e274:	ldp	x29, x30, [sp], #64
   1e278:	ret
   1e27c:	add	x1, x22, #0x1
   1e280:	mov	x0, x19
   1e284:	bl	c080 <__gmpz_realloc@plt>
   1e288:	mov	x23, x0
   1e28c:	b	1e23c <__gmpz_mul_ui@@Base+0x4c>

000000000001e290 <__gmpz_n_pow_ui@@Base>:
   1e290:	stp	x29, x30, [sp, #-96]!
   1e294:	stp	x28, x27, [sp, #16]
   1e298:	stp	x26, x25, [sp, #32]
   1e29c:	stp	x24, x23, [sp, #48]
   1e2a0:	stp	x22, x21, [sp, #64]
   1e2a4:	stp	x20, x19, [sp, #80]
   1e2a8:	mov	x29, sp
   1e2ac:	sub	sp, sp, #0x40
   1e2b0:	mov	x26, x0
   1e2b4:	cbz	x3, 1e380 <__gmpz_n_pow_ui@@Base+0xf0>
   1e2b8:	cbz	x2, 1e39c <__gmpz_n_pow_ui@@Base+0x10c>
   1e2bc:	ldr	x10, [x1]
   1e2c0:	ldr	x8, [x26, #8]
   1e2c4:	cmp	x2, #0x0
   1e2c8:	mov	x21, x3
   1e2cc:	cset	w11, lt  // lt = tstop
   1e2d0:	cneg	x23, x2, mi  // mi = first
   1e2d4:	mov	x9, xzr
   1e2d8:	mov	x22, x1
   1e2dc:	stur	w11, [x29, #-52]
   1e2e0:	cbnz	x10, 1e2f4 <__gmpz_n_pow_ui@@Base+0x64>
   1e2e4:	ldr	x10, [x22, #8]!
   1e2e8:	add	x9, x9, x21
   1e2ec:	sub	x23, x23, #0x1
   1e2f0:	cbz	x10, 1e2e4 <__gmpz_n_pow_ui@@Base+0x54>
   1e2f4:	rbit	x11, x10
   1e2f8:	clz	x25, x11
   1e2fc:	lsr	x24, x10, x25
   1e300:	mul	x10, x25, x21
   1e304:	cmp	x23, #0x2
   1e308:	add	x27, x9, x10, lsr #6
   1e30c:	and	x28, x10, #0x3f
   1e310:	stur	xzr, [x29, #-24]
   1e314:	b.eq	1e3a4 <__gmpz_n_pow_ui@@Base+0x114>  // b.none
   1e318:	cmp	x23, #0x1
   1e31c:	b.eq	1e3dc <__gmpz_n_pow_ui@@Base+0x14c>  // b.none
   1e320:	cmp	x8, x1
   1e324:	b.eq	1e32c <__gmpz_n_pow_ui@@Base+0x9c>  // b.none
   1e328:	cbz	w25, 1e470 <__gmpz_n_pow_ui@@Base+0x1e0>
   1e32c:	lsl	x1, x23, #3
   1e330:	mov	w8, #0x7f00                	// #32512
   1e334:	cmp	x1, x8
   1e338:	b.hi	1e44c <__gmpz_n_pow_ui@@Base+0x1bc>  // b.pmore
   1e33c:	add	x9, x1, #0xf
   1e340:	mov	x8, sp
   1e344:	and	x9, x9, #0xfffffffffffffff0
   1e348:	sub	x24, x8, x9
   1e34c:	mov	sp, x24
   1e350:	mov	x0, x24
   1e354:	mov	x1, x22
   1e358:	mov	x2, x23
   1e35c:	cbz	w25, 1e468 <__gmpz_n_pow_ui@@Base+0x1d8>
   1e360:	mov	w3, w25
   1e364:	bl	c1a0 <__gmpn_rshift@plt>
   1e368:	add	x8, x24, x23, lsl #3
   1e36c:	ldur	x8, [x8, #-8]
   1e370:	cmp	x8, #0x0
   1e374:	cset	w8, eq  // eq = none
   1e378:	sub	x23, x23, x8
   1e37c:	b	1e46c <__gmpz_n_pow_ui@@Base+0x1dc>
   1e380:	ldr	w8, [x26]
   1e384:	cmp	w8, #0x0
   1e388:	b.le	1e43c <__gmpz_n_pow_ui@@Base+0x1ac>
   1e38c:	ldr	x0, [x26, #8]
   1e390:	mov	w8, #0x1                   	// #1
   1e394:	str	x8, [x0]
   1e398:	b	1e748 <__gmpz_n_pow_ui@@Base+0x4b8>
   1e39c:	mov	w8, wzr
   1e3a0:	b	1e748 <__gmpz_n_pow_ui@@Base+0x4b8>
   1e3a4:	ldr	x8, [x22, #8]
   1e3a8:	neg	x9, x25
   1e3ac:	cmp	w25, #0x0
   1e3b0:	lsl	x9, x8, x9
   1e3b4:	csel	x9, xzr, x9, eq  // eq = none
   1e3b8:	lsr	x8, x8, x25
   1e3bc:	orr	x24, x9, x24
   1e3c0:	cbz	x8, 1e3dc <__gmpz_n_pow_ui@@Base+0x14c>
   1e3c4:	sub	x22, x29, #0x10
   1e3c8:	stp	x24, x8, [x29, #-16]
   1e3cc:	mov	w23, #0x2                   	// #2
   1e3d0:	mov	w25, #0x1                   	// #1
   1e3d4:	mov	x24, x8
   1e3d8:	b	1e47c <__gmpz_n_pow_ui@@Base+0x1ec>
   1e3dc:	mov	w25, #0x1                   	// #1
   1e3e0:	mov	x20, x21
   1e3e4:	lsr	x8, x24, #32
   1e3e8:	cbnz	x8, 1e434 <__gmpz_n_pow_ui@@Base+0x1a4>
   1e3ec:	tst	x20, #0x1
   1e3f0:	csinc	x8, x24, xzr, ne  // ne = any
   1e3f4:	lsr	x20, x20, #1
   1e3f8:	mul	x25, x8, x25
   1e3fc:	cbz	x20, 1e40c <__gmpz_n_pow_ui@@Base+0x17c>
   1e400:	mul	x24, x24, x24
   1e404:	lsr	x8, x24, #32
   1e408:	cbz	x8, 1e3ec <__gmpz_n_pow_ui@@Base+0x15c>
   1e40c:	mov	w23, #0x1                   	// #1
   1e410:	cbz	x28, 1e480 <__gmpz_n_pow_ui@@Base+0x1f0>
   1e414:	cmp	x25, #0x1
   1e418:	b.eq	1e480 <__gmpz_n_pow_ui@@Base+0x1f0>  // b.none
   1e41c:	neg	x8, x28
   1e420:	lsr	x8, x25, x8
   1e424:	cmp	x8, #0x0
   1e428:	csel	x8, x28, xzr, eq  // eq = none
   1e42c:	csel	x28, xzr, x28, eq  // eq = none
   1e430:	lsl	x25, x25, x8
   1e434:	mov	w23, #0x1                   	// #1
   1e438:	b	1e480 <__gmpz_n_pow_ui@@Base+0x1f0>
   1e43c:	mov	w1, #0x1                   	// #1
   1e440:	mov	x0, x26
   1e444:	bl	c080 <__gmpz_realloc@plt>
   1e448:	b	1e390 <__gmpz_n_pow_ui@@Base+0x100>
   1e44c:	sub	x0, x29, #0x18
   1e450:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e454:	mov	x24, x0
   1e458:	mov	x0, x24
   1e45c:	mov	x1, x22
   1e460:	mov	x2, x23
   1e464:	cbnz	w25, 1e360 <__gmpz_n_pow_ui@@Base+0xd0>
   1e468:	bl	ca50 <__gmpn_copyi@plt>
   1e46c:	mov	x22, x24
   1e470:	add	x8, x22, x23, lsl #3
   1e474:	ldur	x24, [x8, #-8]
   1e478:	mov	w25, #0x1                   	// #1
   1e47c:	mov	x20, x21
   1e480:	clz	x8, x24
   1e484:	lsl	x9, x23, #6
   1e488:	sub	x8, x9, x8
   1e48c:	mul	x8, x8, x20
   1e490:	ldrsw	x9, [x26]
   1e494:	lsr	x8, x8, #6
   1e498:	add	x19, x8, #0x5
   1e49c:	add	x1, x19, x27
   1e4a0:	cmp	x1, x9
   1e4a4:	stur	x26, [x29, #-32]
   1e4a8:	b.gt	1e76c <__gmpz_n_pow_ui@@Base+0x4dc>
   1e4ac:	ldr	x26, [x26, #8]
   1e4b0:	cbz	x27, 1e4c4 <__gmpz_n_pow_ui@@Base+0x234>
   1e4b4:	lsl	x2, x27, #3
   1e4b8:	mov	x0, x26
   1e4bc:	mov	w1, wzr
   1e4c0:	bl	c5f0 <memset@plt>
   1e4c4:	add	x12, x26, x27, lsl #3
   1e4c8:	stp	x28, x27, [x29, #-48]
   1e4cc:	cbz	x20, 1e544 <__gmpz_n_pow_ui@@Base+0x2b4>
   1e4d0:	cmp	x23, #0x2
   1e4d4:	mvn	w8, w20
   1e4d8:	cset	w9, lt  // lt = tstop
   1e4dc:	and	x8, x8, #0x1
   1e4e0:	orr	x8, x8, x9
   1e4e4:	lsr	x8, x19, x8
   1e4e8:	cmp	x8, #0xfe0
   1e4ec:	lsl	x1, x8, #3
   1e4f0:	b.hi	1e794 <__gmpz_n_pow_ui@@Base+0x504>  // b.pmore
   1e4f4:	add	x9, x1, #0xf
   1e4f8:	mov	x8, sp
   1e4fc:	and	x9, x9, #0x7ffffffffffffff0
   1e500:	sub	x27, x8, x9
   1e504:	mov	sp, x27
   1e508:	clz	x19, x20
   1e50c:	mov	w8, #0x3e                  	// #62
   1e510:	cmp	x23, #0x1
   1e514:	sub	w28, w8, w19
   1e518:	b.ne	1e558 <__gmpz_n_pow_ui@@Base+0x2c8>  // b.any
   1e51c:	tst	w28, #0x1
   1e520:	csel	x26, x27, x12, eq  // eq = none
   1e524:	cmp	w19, #0x3f
   1e528:	str	x24, [x26]
   1e52c:	b.ne	1e5b4 <__gmpz_n_pow_ui@@Base+0x324>  // b.any
   1e530:	mov	w27, #0x1                   	// #1
   1e534:	cmp	x25, #0x1
   1e538:	b.ne	1e640 <__gmpz_n_pow_ui@@Base+0x3b0>  // b.any
   1e53c:	ldur	w25, [x29, #-52]
   1e540:	b	1e708 <__gmpz_n_pow_ui@@Base+0x478>
   1e544:	str	x25, [x12]
   1e548:	ldur	w25, [x29, #-52]
   1e54c:	mov	w27, #0x1                   	// #1
   1e550:	mov	x26, x12
   1e554:	b	1e708 <__gmpz_n_pow_ui@@Base+0x478>
   1e558:	mov	w9, #0x6996                	// #27030
   1e55c:	mov	x8, xzr
   1e560:	movk	w9, #0x9669, lsl #16
   1e564:	mov	x10, x20
   1e568:	and	x11, x10, #0x1f
   1e56c:	sxtw	x8, w8
   1e570:	lsr	x11, x9, x11
   1e574:	lsr	x10, x10, #5
   1e578:	eor	x8, x8, x11
   1e57c:	cbnz	x10, 1e568 <__gmpz_n_pow_ui@@Base+0x2d8>
   1e580:	eor	w24, w28, w8
   1e584:	tst	w24, #0x1
   1e588:	csel	x26, x12, x27, eq  // eq = none
   1e58c:	mov	x0, x26
   1e590:	mov	x1, x22
   1e594:	mov	x2, x23
   1e598:	stur	x12, [x29, #-64]
   1e59c:	bl	ca50 <__gmpn_copyi@plt>
   1e5a0:	ldur	w25, [x29, #-52]
   1e5a4:	cmp	w19, #0x3f
   1e5a8:	b.ne	1e668 <__gmpz_n_pow_ui@@Base+0x3d8>  // b.any
   1e5ac:	mov	x27, x23
   1e5b0:	b	1e708 <__gmpz_n_pow_ui@@Base+0x478>
   1e5b4:	tst	w28, #0x1
   1e5b8:	mov	w8, #0x3f                  	// #63
   1e5bc:	csel	x22, x12, x27, eq  // eq = none
   1e5c0:	sub	w19, w8, w19
   1e5c4:	mov	w27, #0x1                   	// #1
   1e5c8:	mov	x0, x26
   1e5cc:	b	1e5e4 <__gmpz_n_pow_ui@@Base+0x354>
   1e5d0:	sub	w19, w19, #0x1
   1e5d4:	cmp	w19, #0x0
   1e5d8:	sub	x28, x28, #0x1
   1e5dc:	mov	x0, x26
   1e5e0:	b.le	1e534 <__gmpz_n_pow_ui@@Base+0x2a4>
   1e5e4:	mov	x26, x22
   1e5e8:	mov	x22, x0
   1e5ec:	mov	x0, x26
   1e5f0:	mov	x1, x22
   1e5f4:	mov	x2, x27
   1e5f8:	bl	c8e0 <__gmpn_sqr@plt>
   1e5fc:	add	x8, x26, x27, lsl #4
   1e600:	ldur	x8, [x8, #-8]
   1e604:	lsl	x9, x27, #1
   1e608:	lsr	x10, x20, x28
   1e60c:	cmp	x8, #0x0
   1e610:	cset	w8, eq  // eq = none
   1e614:	sub	x27, x9, x8
   1e618:	tbz	w10, #0, 1e5d0 <__gmpz_n_pow_ui@@Base+0x340>
   1e61c:	mov	x0, x26
   1e620:	mov	x1, x26
   1e624:	mov	x2, x27
   1e628:	mov	x3, x24
   1e62c:	bl	d490 <__gmpn_mul_1@plt>
   1e630:	cmp	x0, #0x0
   1e634:	str	x0, [x26, x27, lsl #3]
   1e638:	cinc	x27, x27, ne  // ne = any
   1e63c:	b	1e5d0 <__gmpz_n_pow_ui@@Base+0x340>
   1e640:	mov	x0, x26
   1e644:	mov	x1, x26
   1e648:	mov	x2, x27
   1e64c:	mov	x3, x25
   1e650:	bl	d490 <__gmpn_mul_1@plt>
   1e654:	ldur	w25, [x29, #-52]
   1e658:	cmp	x0, #0x0
   1e65c:	str	x0, [x26, x27, lsl #3]
   1e660:	cinc	x27, x27, ne  // ne = any
   1e664:	b	1e708 <__gmpz_n_pow_ui@@Base+0x478>
   1e668:	ldur	x9, [x29, #-64]
   1e66c:	tst	w24, #0x1
   1e670:	mov	w8, #0x3f                  	// #63
   1e674:	sub	w19, w8, w19
   1e678:	csel	x24, x27, x9, eq  // eq = none
   1e67c:	mov	x27, x23
   1e680:	b	1e6a0 <__gmpz_n_pow_ui@@Base+0x410>
   1e684:	mov	x0, x26
   1e688:	mov	x26, x24
   1e68c:	mov	x24, x0
   1e690:	sub	w19, w19, #0x1
   1e694:	cmp	w19, #0x0
   1e698:	sub	x28, x28, #0x1
   1e69c:	b.le	1e708 <__gmpz_n_pow_ui@@Base+0x478>
   1e6a0:	mov	x0, x24
   1e6a4:	mov	x1, x26
   1e6a8:	mov	x2, x27
   1e6ac:	bl	c8e0 <__gmpn_sqr@plt>
   1e6b0:	add	x8, x24, x27, lsl #4
   1e6b4:	ldur	x8, [x8, #-8]
   1e6b8:	lsl	x9, x27, #1
   1e6bc:	lsr	x10, x20, x28
   1e6c0:	cmp	x8, #0x0
   1e6c4:	cset	w8, eq  // eq = none
   1e6c8:	sub	x27, x9, x8
   1e6cc:	tbz	w10, #0, 1e684 <__gmpz_n_pow_ui@@Base+0x3f4>
   1e6d0:	mov	x0, x26
   1e6d4:	mov	x1, x24
   1e6d8:	mov	x2, x27
   1e6dc:	mov	x3, x22
   1e6e0:	mov	x4, x23
   1e6e4:	bl	ccd0 <__gmpn_mul@plt>
   1e6e8:	cmp	x0, #0x0
   1e6ec:	cset	w8, eq  // eq = none
   1e6f0:	add	x9, x27, x23
   1e6f4:	sub	x27, x9, x8
   1e6f8:	sub	w19, w19, #0x1
   1e6fc:	cmp	w19, #0x0
   1e700:	sub	x28, x28, #0x1
   1e704:	b.gt	1e6a0 <__gmpz_n_pow_ui@@Base+0x410>
   1e708:	ldur	x0, [x29, #-24]
   1e70c:	cbnz	x0, 1e780 <__gmpz_n_pow_ui@@Base+0x4f0>
   1e710:	ldur	x3, [x29, #-48]
   1e714:	and	w19, w21, w25
   1e718:	cbz	x3, 1e738 <__gmpz_n_pow_ui@@Base+0x4a8>
   1e71c:	mov	x0, x26
   1e720:	mov	x1, x26
   1e724:	mov	x2, x27
   1e728:	bl	c180 <__gmpn_lshift@plt>
   1e72c:	cmp	x0, #0x0
   1e730:	str	x0, [x26, x27, lsl #3]
   1e734:	cinc	x27, x27, ne  // ne = any
   1e738:	ldp	x8, x26, [x29, #-40]
   1e73c:	cmp	w19, #0x0
   1e740:	add	w8, w27, w8
   1e744:	cneg	w8, w8, ne  // ne = any
   1e748:	str	w8, [x26, #4]
   1e74c:	mov	sp, x29
   1e750:	ldp	x20, x19, [sp, #80]
   1e754:	ldp	x22, x21, [sp, #64]
   1e758:	ldp	x24, x23, [sp, #48]
   1e75c:	ldp	x26, x25, [sp, #32]
   1e760:	ldp	x28, x27, [sp, #16]
   1e764:	ldp	x29, x30, [sp], #96
   1e768:	ret
   1e76c:	mov	x0, x26
   1e770:	bl	c080 <__gmpz_realloc@plt>
   1e774:	mov	x26, x0
   1e778:	cbnz	x27, 1e4b4 <__gmpz_n_pow_ui@@Base+0x224>
   1e77c:	b	1e4c4 <__gmpz_n_pow_ui@@Base+0x234>
   1e780:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1e784:	ldur	x3, [x29, #-48]
   1e788:	and	w19, w21, w25
   1e78c:	cbnz	x3, 1e71c <__gmpz_n_pow_ui@@Base+0x48c>
   1e790:	b	1e738 <__gmpz_n_pow_ui@@Base+0x4a8>
   1e794:	sub	x0, x29, #0x18
   1e798:	mov	x19, x12
   1e79c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e7a0:	mov	x12, x19
   1e7a4:	mov	x27, x0
   1e7a8:	b	1e508 <__gmpz_n_pow_ui@@Base+0x278>

000000000001e7ac <__gmpz_neg@@Base>:
   1e7ac:	stp	x29, x30, [sp, #-48]!
   1e7b0:	stp	x22, x21, [sp, #16]
   1e7b4:	stp	x20, x19, [sp, #32]
   1e7b8:	ldrsw	x22, [x1, #4]
   1e7bc:	mov	x19, x0
   1e7c0:	cmp	x1, x0
   1e7c4:	mov	x29, sp
   1e7c8:	b.eq	1e7f4 <__gmpz_neg@@Base+0x48>  // b.none
   1e7cc:	ldrsw	x8, [x19]
   1e7d0:	cmp	x22, #0x0
   1e7d4:	cneg	x21, x22, mi  // mi = first
   1e7d8:	mov	x20, x1
   1e7dc:	cmp	x21, x8
   1e7e0:	b.gt	1e80c <__gmpz_neg@@Base+0x60>
   1e7e4:	ldr	x0, [x19, #8]
   1e7e8:	ldr	x1, [x20, #8]
   1e7ec:	mov	x2, x21
   1e7f0:	bl	ca50 <__gmpn_copyi@plt>
   1e7f4:	neg	w8, w22
   1e7f8:	str	w8, [x19, #4]
   1e7fc:	ldp	x20, x19, [sp, #32]
   1e800:	ldp	x22, x21, [sp, #16]
   1e804:	ldp	x29, x30, [sp], #48
   1e808:	ret
   1e80c:	mov	x0, x19
   1e810:	mov	x1, x21
   1e814:	bl	c080 <__gmpz_realloc@plt>
   1e818:	b	1e7e8 <__gmpz_neg@@Base+0x3c>

000000000001e81c <__gmpz_nextprime@@Base>:
   1e81c:	stp	x29, x30, [sp, #-80]!
   1e820:	stp	x20, x19, [sp, #64]
   1e824:	mov	x20, x1
   1e828:	mov	x19, x0
   1e82c:	mov	w1, #0x2                   	// #2
   1e830:	mov	x0, x20
   1e834:	str	x25, [sp, #16]
   1e838:	stp	x24, x23, [sp, #32]
   1e83c:	stp	x22, x21, [sp, #48]
   1e840:	mov	x29, sp
   1e844:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1e848:	tbnz	w0, #31, 1e9a4 <__gmpz_nextprime@@Base+0x188>
   1e84c:	mov	w2, #0x1                   	// #1
   1e850:	mov	x0, x19
   1e854:	mov	x1, x20
   1e858:	bl	c8b0 <__gmpz_add_ui@plt>
   1e85c:	mov	x0, x19
   1e860:	mov	x1, xzr
   1e864:	bl	c310 <__gmpz_setbit@plt>
   1e868:	mov	w1, #0x7                   	// #7
   1e86c:	mov	x0, x19
   1e870:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1e874:	cmp	w0, #0x1
   1e878:	b.lt	1e988 <__gmpz_nextprime@@Base+0x16c>  // b.tstop
   1e87c:	ldrsw	x8, [x19, #4]
   1e880:	ldr	x9, [x19, #8]
   1e884:	add	x9, x9, x8, lsl #3
   1e888:	ldur	x9, [x9, #-8]
   1e88c:	lsl	x8, x8, #6
   1e890:	clz	x9, x9
   1e894:	sub	x8, x8, x9
   1e898:	lsr	x9, x8, #1
   1e89c:	cmp	x8, #0x14d
   1e8a0:	mov	w8, #0xa6                  	// #166
   1e8a4:	csel	w21, w8, w9, hi  // hi = pmore
   1e8a8:	lsl	x8, x21, #1
   1e8ac:	add	x8, x8, #0xf
   1e8b0:	and	x8, x8, #0x3fffffff0
   1e8b4:	mov	x9, sp
   1e8b8:	sub	x22, x9, x8
   1e8bc:	mov	sp, x22
   1e8c0:	cbz	w21, 1e9c8 <__gmpz_nextprime@@Base+0x1ac>
   1e8c4:	adrp	x23, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   1e8c8:	mov	x25, xzr
   1e8cc:	mov	w20, #0x3                   	// #3
   1e8d0:	add	x23, x23, #0x130
   1e8d4:	mov	w24, #0xfffe                	// #65534
   1e8d8:	b	1e8f0 <__gmpz_nextprime@@Base+0xd4>
   1e8dc:	mov	x0, x19
   1e8e0:	mov	x1, x19
   1e8e4:	bl	c8b0 <__gmpz_add_ui@plt>
   1e8e8:	mov	x25, xzr
   1e8ec:	mov	w20, #0x3                   	// #3
   1e8f0:	mov	x0, x19
   1e8f4:	mov	x1, x20
   1e8f8:	bl	bfa0 <__gmpz_tdiv_ui@plt>
   1e8fc:	ldrb	w8, [x23, x25]
   1e900:	strh	w0, [x22, x25, lsl #1]
   1e904:	add	x25, x25, #0x1
   1e908:	cmp	x25, x21
   1e90c:	add	x20, x20, x8
   1e910:	b.ne	1e8f0 <__gmpz_nextprime@@Base+0xd4>  // b.any
   1e914:	mov	x2, xzr
   1e918:	mov	w20, wzr
   1e91c:	b	1e934 <__gmpz_nextprime@@Base+0x118>
   1e920:	mov	x2, xzr
   1e924:	cmp	w20, w24
   1e928:	add	w20, w20, #0x2
   1e92c:	add	x2, x2, #0x2
   1e930:	b.cs	1e8dc <__gmpz_nextprime@@Base+0xc0>  // b.hs, b.nlast
   1e934:	mov	x8, x21
   1e938:	mov	x9, x23
   1e93c:	mov	x10, x22
   1e940:	mov	w11, #0x3                   	// #3
   1e944:	ldrh	w12, [x10]
   1e948:	add	x12, x12, w20, uxtw
   1e94c:	udiv	x13, x12, x11
   1e950:	msub	x12, x13, x11, x12
   1e954:	cbz	x12, 1e924 <__gmpz_nextprime@@Base+0x108>
   1e958:	ldrb	w12, [x9], #1
   1e95c:	subs	x8, x8, #0x1
   1e960:	add	x10, x10, #0x2
   1e964:	add	x11, x11, x12
   1e968:	b.ne	1e944 <__gmpz_nextprime@@Base+0x128>  // b.any
   1e96c:	mov	x0, x19
   1e970:	mov	x1, x19
   1e974:	bl	c8b0 <__gmpz_add_ui@plt>
   1e978:	mov	w1, #0x19                  	// #25
   1e97c:	mov	x0, x19
   1e980:	bl	cb80 <__gmpz_millerrabin@plt>
   1e984:	cbz	w0, 1e920 <__gmpz_nextprime@@Base+0x104>
   1e988:	mov	sp, x29
   1e98c:	ldp	x20, x19, [sp, #64]
   1e990:	ldp	x22, x21, [sp, #48]
   1e994:	ldp	x24, x23, [sp, #32]
   1e998:	ldr	x25, [sp, #16]
   1e99c:	ldp	x29, x30, [sp], #80
   1e9a0:	ret
   1e9a4:	mov	w1, #0x2                   	// #2
   1e9a8:	mov	x0, x19
   1e9ac:	mov	sp, x29
   1e9b0:	ldp	x20, x19, [sp, #64]
   1e9b4:	ldp	x22, x21, [sp, #48]
   1e9b8:	ldp	x24, x23, [sp, #32]
   1e9bc:	ldr	x25, [sp, #16]
   1e9c0:	ldp	x29, x30, [sp], #80
   1e9c4:	b	c170 <__gmpz_set_ui@plt>
   1e9c8:	mov	x2, xzr
   1e9cc:	mov	w20, #0xfffe                	// #65534
   1e9d0:	mov	x0, x19
   1e9d4:	mov	x1, x19
   1e9d8:	bl	c8b0 <__gmpz_add_ui@plt>
   1e9dc:	mov	w1, #0x19                  	// #25
   1e9e0:	mov	x0, x19
   1e9e4:	bl	cb80 <__gmpz_millerrabin@plt>
   1e9e8:	cbnz	w0, 1e988 <__gmpz_nextprime@@Base+0x16c>
   1e9ec:	cmp	w21, w20
   1e9f0:	add	w21, w21, #0x2
   1e9f4:	mov	w2, #0x2                   	// #2
   1e9f8:	b.cc	1e9d0 <__gmpz_nextprime@@Base+0x1b4>  // b.lo, b.ul, b.last
   1e9fc:	mov	w2, #0x2                   	// #2
   1ea00:	mov	x0, x19
   1ea04:	mov	x1, x19
   1ea08:	bl	c8b0 <__gmpz_add_ui@plt>
   1ea0c:	mov	x2, xzr
   1ea10:	mov	w21, wzr
   1ea14:	b	1e9d0 <__gmpz_nextprime@@Base+0x1b4>

000000000001ea18 <__gmpz_out_raw@@Base>:
   1ea18:	stp	x29, x30, [sp, #-80]!
   1ea1c:	stp	x24, x23, [sp, #32]
   1ea20:	stp	x22, x21, [sp, #48]
   1ea24:	stp	x20, x19, [sp, #64]
   1ea28:	str	x25, [sp, #16]
   1ea2c:	ldrsw	x23, [x1, #4]
   1ea30:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ea34:	ldr	x8, [x8, #3840]
   1ea38:	mov	x21, x0
   1ea3c:	cmp	x23, #0x0
   1ea40:	cneg	x25, x23, mi  // mi = first
   1ea44:	ldr	x8, [x8]
   1ea48:	ubfiz	x24, x25, #3, #58
   1ea4c:	add	x19, x24, #0x8
   1ea50:	mov	x0, x19
   1ea54:	mov	x29, sp
   1ea58:	mov	x22, x1
   1ea5c:	blr	x8
   1ea60:	mov	x20, x0
   1ea64:	add	x0, x0, #0x8
   1ea68:	cbz	x24, 1eb5c <__gmpz_out_raw@@Base+0x144>
   1ea6c:	ldr	x8, [x22, #8]
   1ea70:	cmp	x25, #0x1
   1ea74:	csinc	x10, x25, xzr, gt
   1ea78:	cmp	x10, #0x2
   1ea7c:	add	x9, x0, x24
   1ea80:	b.cc	1eb68 <__gmpz_out_raw@@Base+0x150>  // b.lo, b.ul, b.last
   1ea84:	cmp	x25, #0x1
   1ea88:	csinc	x11, x25, xzr, lt  // lt = tstop
   1ea8c:	add	x12, x24, x11, lsl #3
   1ea90:	sub	x11, x25, x11
   1ea94:	sub	x12, x12, x25, lsl #3
   1ea98:	add	x11, x8, x11, lsl #3
   1ea9c:	add	x12, x20, x12
   1eaa0:	add	x11, x11, #0x8
   1eaa4:	cmp	x12, x11
   1eaa8:	b.cs	1eab4 <__gmpz_out_raw@@Base+0x9c>  // b.hs, b.nlast
   1eaac:	cmp	x9, x8
   1eab0:	b.hi	1eb68 <__gmpz_out_raw@@Base+0x150>  // b.pmore
   1eab4:	and	x11, x10, #0x7ffffffffffffffe
   1eab8:	lsl	x14, x11, #3
   1eabc:	sub	x12, x9, #0x10
   1eac0:	movi	v0.2d, #0xff000000000000
   1eac4:	movi	v1.2d, #0xff0000000000
   1eac8:	movi	v2.2d, #0xff00000000
   1eacc:	movi	v3.2d, #0xff000000
   1ead0:	movi	v4.2d, #0xff0000
   1ead4:	sub	x25, x25, x11
   1ead8:	add	x13, x8, x14
   1eadc:	sub	x9, x9, x14
   1eae0:	movi	v5.2d, #0xff00
   1eae4:	mov	x14, x11
   1eae8:	ldr	q6, [x8], #16
   1eaec:	subs	x14, x14, #0x2
   1eaf0:	shl	v16.2d, v6.2d, #40
   1eaf4:	shl	v7.2d, v6.2d, #56
   1eaf8:	and	v16.16b, v16.16b, v0.16b
   1eafc:	orr	v7.16b, v16.16b, v7.16b
   1eb00:	shl	v16.2d, v6.2d, #24
   1eb04:	and	v16.16b, v16.16b, v1.16b
   1eb08:	orr	v7.16b, v7.16b, v16.16b
   1eb0c:	shl	v16.2d, v6.2d, #8
   1eb10:	and	v16.16b, v16.16b, v2.16b
   1eb14:	orr	v7.16b, v7.16b, v16.16b
   1eb18:	ushr	v16.2d, v6.2d, #8
   1eb1c:	and	v16.16b, v16.16b, v3.16b
   1eb20:	orr	v7.16b, v7.16b, v16.16b
   1eb24:	ushr	v16.2d, v6.2d, #24
   1eb28:	and	v16.16b, v16.16b, v4.16b
   1eb2c:	orr	v7.16b, v7.16b, v16.16b
   1eb30:	ushr	v16.2d, v6.2d, #40
   1eb34:	and	v16.16b, v16.16b, v5.16b
   1eb38:	orr	v7.16b, v7.16b, v16.16b
   1eb3c:	usra	v7.2d, v6.2d, #56
   1eb40:	ext	v7.16b, v7.16b, v7.16b, #8
   1eb44:	str	q7, [x12], #-16
   1eb48:	b.ne	1eae8 <__gmpz_out_raw@@Base+0xd0>  // b.any
   1eb4c:	cmp	x10, x11
   1eb50:	b.ne	1eb64 <__gmpz_out_raw@@Base+0x14c>  // b.any
   1eb54:	mov	x11, v6.d[1]
   1eb58:	b	1ebc0 <__gmpz_out_raw@@Base+0x1a8>
   1eb5c:	mov	x8, xzr
   1eb60:	b	1ebd0 <__gmpz_out_raw@@Base+0x1b8>
   1eb64:	mov	x8, x13
   1eb68:	add	x10, x25, #0x1
   1eb6c:	ldr	x11, [x8], #8
   1eb70:	sub	x10, x10, #0x1
   1eb74:	cmp	x10, #0x1
   1eb78:	lsl	x14, x11, #40
   1eb7c:	and	x14, x14, #0xff000000000000
   1eb80:	lsr	x13, x11, #16
   1eb84:	bfi	x14, x11, #56, #8
   1eb88:	lsr	x12, x11, #24
   1eb8c:	bfi	x14, x13, #40, #8
   1eb90:	lsr	x13, x11, #8
   1eb94:	and	x13, x13, #0xff000000
   1eb98:	bfi	x14, x12, #32, #8
   1eb9c:	orr	x13, x14, x13
   1eba0:	lsr	x14, x11, #40
   1eba4:	and	x12, x12, #0xff0000
   1eba8:	and	x14, x14, #0xff00
   1ebac:	orr	x12, x13, x12
   1ebb0:	orr	x12, x12, x14
   1ebb4:	add	x12, x12, x11, lsr #56
   1ebb8:	str	x12, [x9, #-8]!
   1ebbc:	b.gt	1eb6c <__gmpz_out_raw@@Base+0x154>
   1ebc0:	clz	x8, x11
   1ebc4:	lsr	x8, x8, #3
   1ebc8:	add	x0, x9, x8
   1ebcc:	sub	x8, x24, x8
   1ebd0:	cmp	w23, #0x0
   1ebd4:	cneg	w9, w8, lt  // lt = tstop
   1ebd8:	rev	w9, w9
   1ebdc:	str	w9, [x0, #-4]!
   1ebe0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ebe4:	ldr	x9, [x9, #3856]
   1ebe8:	add	x22, x8, #0x4
   1ebec:	cmp	x21, #0x0
   1ebf0:	mov	w2, #0x1                   	// #1
   1ebf4:	ldr	x9, [x9]
   1ebf8:	mov	x1, x22
   1ebfc:	csel	x3, x9, x21, eq  // eq = none
   1ec00:	bl	ce30 <fwrite@plt>
   1ec04:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ec08:	ldr	x8, [x8, #4016]
   1ec0c:	cmp	x0, #0x1
   1ec10:	mov	x0, x20
   1ec14:	mov	x1, x19
   1ec18:	ldr	x8, [x8]
   1ec1c:	csel	x21, x22, xzr, eq  // eq = none
   1ec20:	blr	x8
   1ec24:	mov	x0, x21
   1ec28:	ldp	x20, x19, [sp, #64]
   1ec2c:	ldp	x22, x21, [sp, #48]
   1ec30:	ldp	x24, x23, [sp, #32]
   1ec34:	ldr	x25, [sp, #16]
   1ec38:	ldp	x29, x30, [sp], #80
   1ec3c:	ret

000000000001ec40 <__gmpz_out_str@@Base>:
   1ec40:	stp	x29, x30, [sp, #-80]!
   1ec44:	str	x25, [sp, #16]
   1ec48:	stp	x24, x23, [sp, #32]
   1ec4c:	stp	x22, x21, [sp, #48]
   1ec50:	stp	x20, x19, [sp, #64]
   1ec54:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ec58:	ldr	x8, [x8, #3856]
   1ec5c:	ldrsw	x21, [x2, #4]
   1ec60:	cmp	x0, #0x0
   1ec64:	mov	x23, x2
   1ec68:	ldr	x8, [x8]
   1ec6c:	mov	w20, w1
   1ec70:	mov	x29, sp
   1ec74:	csel	x19, x8, x0, eq  // eq = none
   1ec78:	cmp	w1, #0x2
   1ec7c:	b.lt	1ecac <__gmpz_out_str@@Base+0x6c>  // b.tstop
   1ec80:	cmp	w20, #0x25
   1ec84:	b.ge	1ecc8 <__gmpz_out_str@@Base+0x88>  // b.tcont
   1ec88:	adrp	x24, 59000 <__gmp_randget_mt@@Base+0x44c>
   1ec8c:	add	x24, x24, #0xc7d
   1ec90:	tbz	w21, #31, 1ecdc <__gmpz_out_str@@Base+0x9c>
   1ec94:	mov	w0, #0x2d                  	// #45
   1ec98:	mov	x1, x19
   1ec9c:	bl	c210 <fputc@plt>
   1eca0:	neg	x21, x21
   1eca4:	mov	w25, #0x1                   	// #1
   1eca8:	b	1ece0 <__gmpz_out_str@@Base+0xa0>
   1ecac:	cmn	w20, #0x2
   1ecb0:	b.le	1ee34 <__gmpz_out_str@@Base+0x1f4>
   1ecb4:	mov	w20, #0xa                   	// #10
   1ecb8:	adrp	x24, 59000 <__gmp_randget_mt@@Base+0x44c>
   1ecbc:	add	x24, x24, #0xc3e
   1ecc0:	tbz	w21, #31, 1ecdc <__gmpz_out_str@@Base+0x9c>
   1ecc4:	b	1ec94 <__gmpz_out_str@@Base+0x54>
   1ecc8:	cmp	w20, #0x3e
   1eccc:	b.gt	1ee7c <__gmpz_out_str@@Base+0x23c>
   1ecd0:	adrp	x24, 59000 <__gmp_randget_mt@@Base+0x44c>
   1ecd4:	add	x24, x24, #0xc3e
   1ecd8:	tbnz	w21, #31, 1ec94 <__gmpz_out_str@@Base+0x54>
   1ecdc:	mov	x25, xzr
   1ece0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   1ece4:	ldr	x8, [x8, #3936]
   1ece8:	mov	w9, #0x28                  	// #40
   1ecec:	str	xzr, [x29, #24]
   1ecf0:	umaddl	x8, w20, w9, x8
   1ecf4:	ldr	x8, [x8, #8]
   1ecf8:	lsl	x9, x21, #6
   1ecfc:	umulh	x8, x8, x9
   1ed00:	add	x1, x8, #0x3
   1ed04:	mov	w8, #0x7f00                	// #32512
   1ed08:	cmp	x1, x8
   1ed0c:	b.hi	1ee50 <__gmpz_out_str@@Base+0x210>  // b.pmore
   1ed10:	add	x9, x1, #0xf
   1ed14:	mov	x8, sp
   1ed18:	and	x9, x9, #0xfffffffffffffff0
   1ed1c:	sub	x22, x8, x9
   1ed20:	mov	sp, x22
   1ed24:	ldr	x2, [x23, #8]
   1ed28:	sub	w8, w20, #0x1
   1ed2c:	tst	w20, w8
   1ed30:	b.eq	1ed70 <__gmpz_out_str@@Base+0x130>  // b.none
   1ed34:	lsl	x8, x21, #3
   1ed38:	orr	x1, x8, #0x8
   1ed3c:	mov	w8, #0x7f00                	// #32512
   1ed40:	cmp	x1, x8
   1ed44:	b.hi	1ee84 <__gmpz_out_str@@Base+0x244>  // b.pmore
   1ed48:	add	x9, x1, #0xf
   1ed4c:	mov	x8, sp
   1ed50:	and	x9, x9, #0xfffffffffffffff0
   1ed54:	sub	x23, x8, x9
   1ed58:	mov	sp, x23
   1ed5c:	mov	x0, x23
   1ed60:	mov	x1, x2
   1ed64:	mov	x2, x21
   1ed68:	bl	ca50 <__gmpn_copyi@plt>
   1ed6c:	mov	x2, x23
   1ed70:	mov	x0, x22
   1ed74:	mov	w1, w20
   1ed78:	mov	x3, x21
   1ed7c:	bl	ca90 <__gmpn_get_str@plt>
   1ed80:	mov	x2, x0
   1ed84:	cbz	x0, 1ede8 <__gmpz_out_str@@Base+0x1a8>
   1ed88:	cmp	x2, #0x1
   1ed8c:	b.ne	1ed98 <__gmpz_out_str@@Base+0x158>  // b.any
   1ed90:	mov	x8, xzr
   1ed94:	b	1edcc <__gmpz_out_str@@Base+0x18c>
   1ed98:	and	x8, x2, #0xfffffffffffffffe
   1ed9c:	add	x9, x22, #0x1
   1eda0:	mov	x10, x8
   1eda4:	ldurb	w11, [x9, #-1]
   1eda8:	ldrb	w12, [x9]
   1edac:	subs	x10, x10, #0x2
   1edb0:	ldrb	w11, [x24, x11]
   1edb4:	ldrb	w12, [x24, x12]
   1edb8:	sturb	w11, [x9, #-1]
   1edbc:	strb	w12, [x9], #2
   1edc0:	b.ne	1eda4 <__gmpz_out_str@@Base+0x164>  // b.any
   1edc4:	cmp	x2, x8
   1edc8:	b.eq	1ede8 <__gmpz_out_str@@Base+0x1a8>  // b.none
   1edcc:	sub	x9, x2, x8
   1edd0:	add	x8, x22, x8
   1edd4:	ldrb	w10, [x8]
   1edd8:	subs	x9, x9, #0x1
   1eddc:	ldrb	w10, [x24, x10]
   1ede0:	strb	w10, [x8], #1
   1ede4:	b.ne	1edd4 <__gmpz_out_str@@Base+0x194>  // b.any
   1ede8:	mov	w1, #0x1                   	// #1
   1edec:	mov	x0, x22
   1edf0:	mov	x3, x19
   1edf4:	strb	wzr, [x22, x2]
   1edf8:	bl	ce30 <fwrite@plt>
   1edfc:	ldr	x8, [x29, #24]
   1ee00:	add	x20, x0, x25
   1ee04:	cbnz	x8, 1ee70 <__gmpz_out_str@@Base+0x230>
   1ee08:	mov	x0, x19
   1ee0c:	bl	d4a0 <ferror@plt>
   1ee10:	cmp	w0, #0x0
   1ee14:	csel	x0, x20, xzr, eq  // eq = none
   1ee18:	mov	sp, x29
   1ee1c:	ldp	x20, x19, [sp, #64]
   1ee20:	ldp	x22, x21, [sp, #48]
   1ee24:	ldp	x24, x23, [sp, #32]
   1ee28:	ldr	x25, [sp, #16]
   1ee2c:	ldp	x29, x30, [sp], #80
   1ee30:	ret
   1ee34:	cmn	w20, #0x24
   1ee38:	b.lt	1ee7c <__gmpz_out_str@@Base+0x23c>  // b.tstop
   1ee3c:	neg	w20, w20
   1ee40:	adrp	x24, 59000 <__gmp_randget_mt@@Base+0x44c>
   1ee44:	add	x24, x24, #0xc3e
   1ee48:	tbz	w21, #31, 1ecdc <__gmpz_out_str@@Base+0x9c>
   1ee4c:	b	1ec94 <__gmpz_out_str@@Base+0x54>
   1ee50:	add	x0, x29, #0x18
   1ee54:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1ee58:	mov	x22, x0
   1ee5c:	ldr	x2, [x23, #8]
   1ee60:	sub	w8, w20, #0x1
   1ee64:	tst	w20, w8
   1ee68:	b.ne	1ed34 <__gmpz_out_str@@Base+0xf4>  // b.any
   1ee6c:	b	1ed70 <__gmpz_out_str@@Base+0x130>
   1ee70:	mov	x0, x8
   1ee74:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1ee78:	b	1ee08 <__gmpz_out_str@@Base+0x1c8>
   1ee7c:	mov	x0, xzr
   1ee80:	b	1ee18 <__gmpz_out_str@@Base+0x1d8>
   1ee84:	add	x0, x29, #0x18
   1ee88:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1ee8c:	ldr	x2, [x23, #8]
   1ee90:	mov	x23, x0
   1ee94:	b	1ed5c <__gmpz_out_str@@Base+0x11c>

000000000001ee98 <__gmpz_perfect_power_p@@Base>:
   1ee98:	ldr	x8, [x0, #8]
   1ee9c:	ldrsw	x1, [x0, #4]
   1eea0:	mov	x0, x8
   1eea4:	b	c240 <__gmpn_perfect_power_p@plt>

000000000001eea8 <__gmpz_perfect_square_p@@Base>:
   1eea8:	ldr	w1, [x0, #4]
   1eeac:	cmp	w1, #0x1
   1eeb0:	b.lt	1eebc <__gmpz_perfect_square_p@@Base+0x14>  // b.tstop
   1eeb4:	ldr	x0, [x0, #8]
   1eeb8:	b	d0b0 <__gmpn_perfect_square_p@plt>
   1eebc:	mvn	w8, w1
   1eec0:	lsr	w0, w8, #31
   1eec4:	ret

000000000001eec8 <__gmpz_popcount@@Base>:
   1eec8:	ldr	w1, [x0, #4]
   1eecc:	cmp	w1, #0x1
   1eed0:	b.lt	1eedc <__gmpz_popcount@@Base+0x14>  // b.tstop
   1eed4:	ldr	x0, [x0, #8]
   1eed8:	b	cd80 <__gmpn_popcount@plt>
   1eedc:	sbfx	x0, x1, #31, #1
   1eee0:	ret

000000000001eee4 <__gmpz_pow_ui@@Base>:
   1eee4:	cmp	x2, #0x2
   1eee8:	b.eq	1ef04 <__gmpz_pow_ui@@Base+0x20>  // b.none
   1eeec:	mov	x3, x2
   1eef0:	cmp	x2, #0x1
   1eef4:	b.eq	1ef0c <__gmpz_pow_ui@@Base+0x28>  // b.none
   1eef8:	cbnz	x3, 1ef10 <__gmpz_pow_ui@@Base+0x2c>
   1eefc:	mov	w1, #0x1                   	// #1
   1ef00:	b	c170 <__gmpz_set_ui@plt>
   1ef04:	mov	x2, x1
   1ef08:	b	c4b0 <__gmpz_mul@plt>
   1ef0c:	b	c420 <__gmpz_set@plt>
   1ef10:	ldr	x8, [x1, #8]
   1ef14:	ldrsw	x2, [x1, #4]
   1ef18:	mov	x1, x8
   1ef1c:	b	c340 <__gmpz_n_pow_ui@plt>

000000000001ef20 <__gmpz_powm@@Base>:
   1ef20:	stp	x29, x30, [sp, #-96]!
   1ef24:	stp	x28, x27, [sp, #16]
   1ef28:	stp	x26, x25, [sp, #32]
   1ef2c:	stp	x24, x23, [sp, #48]
   1ef30:	stp	x22, x21, [sp, #64]
   1ef34:	stp	x20, x19, [sp, #80]
   1ef38:	mov	x29, sp
   1ef3c:	sub	sp, sp, #0x50
   1ef40:	ldr	w8, [x3, #4]
   1ef44:	cmp	w8, #0x0
   1ef48:	cneg	w20, w8, mi  // mi = first
   1ef4c:	cbz	w20, 1f97c <__gmpz_powm@@Base+0xa5c>
   1ef50:	ldr	x26, [x3, #8]
   1ef54:	stur	xzr, [x29, #-24]
   1ef58:	ldrsw	x22, [x2, #4]
   1ef5c:	mov	x19, x3
   1ef60:	mov	x21, x2
   1ef64:	mov	x25, x1
   1ef68:	mov	x23, x0
   1ef6c:	cmp	w22, #0x0
   1ef70:	b.le	1f554 <__gmpz_powm@@Base+0x634>
   1ef74:	ldr	w8, [x25, #4]
   1ef78:	cmp	w8, #0x0
   1ef7c:	cneg	w24, w8, mi  // mi = first
   1ef80:	cbz	w24, 1f5b0 <__gmpz_powm@@Base+0x690>
   1ef84:	ldr	x9, [x21, #8]
   1ef88:	cmp	x22, #0x1
   1ef8c:	b.ne	1ef9c <__gmpz_powm@@Base+0x7c>  // b.any
   1ef90:	ldr	x8, [x9]
   1ef94:	cmp	x8, #0x1
   1ef98:	b.eq	1f5f4 <__gmpz_powm@@Base+0x6d4>  // b.none
   1ef9c:	ldr	x8, [x26]
   1efa0:	mov	x28, xzr
   1efa4:	stur	x9, [x29, #-32]
   1efa8:	cbz	x8, 1f5c4 <__gmpz_powm@@Base+0x6a4>
   1efac:	sub	x27, x20, x28
   1efb0:	stur	x19, [x29, #-64]
   1efb4:	stur	x22, [x29, #-40]
   1efb8:	tbnz	w8, #0, 1f01c <__gmpz_powm@@Base+0xfc>
   1efbc:	lsl	x1, x27, #3
   1efc0:	mov	w9, #0x7f00                	// #32512
   1efc4:	cmp	x1, x9
   1efc8:	b.hi	1f64c <__gmpz_powm@@Base+0x72c>  // b.pmore
   1efcc:	add	x10, x1, #0xf
   1efd0:	mov	x9, sp
   1efd4:	and	x10, x10, #0xfffffffffffffff0
   1efd8:	sub	x21, x9, x10
   1efdc:	mov	sp, x21
   1efe0:	rbit	x8, x8
   1efe4:	clz	x3, x8
   1efe8:	mov	x0, x21
   1efec:	mov	x1, x26
   1eff0:	mov	x2, x27
   1eff4:	stur	x3, [x29, #-56]
   1eff8:	bl	c1a0 <__gmpn_rshift@plt>
   1effc:	add	x8, x21, x27, lsl #3
   1f000:	ldur	x8, [x8, #-8]
   1f004:	add	x28, x28, #0x1
   1f008:	mov	x26, x21
   1f00c:	cmp	x8, #0x0
   1f010:	cset	w8, eq  // eq = none
   1f014:	sub	x27, x27, x8
   1f018:	b	1f024 <__gmpz_powm@@Base+0x104>
   1f01c:	cbz	x28, 1f1d4 <__gmpz_powm@@Base+0x2b4>
   1f020:	stur	xzr, [x29, #-56]
   1f024:	cmp	x28, x27
   1f028:	csel	x0, x28, x27, gt
   1f02c:	bl	d1e0 <__gmpn_binvert_itch@plt>
   1f030:	lsl	x8, x20, #1
   1f034:	cmp	x0, x8
   1f038:	add	x9, x8, x20
   1f03c:	csel	x8, x0, x8, gt
   1f040:	mov	w22, #0x1                   	// #1
   1f044:	add	x8, x8, x9
   1f048:	lsl	x1, x8, #3
   1f04c:	mov	w8, #0x7f00                	// #32512
   1f050:	mov	x19, x26
   1f054:	cmp	x1, x8
   1f058:	b.hi	1f5e4 <__gmpz_powm@@Base+0x6c4>  // b.pmore
   1f05c:	add	x9, x1, #0xf
   1f060:	mov	x8, sp
   1f064:	and	x9, x9, #0xfffffffffffffff0
   1f068:	sub	x21, x8, x9
   1f06c:	mov	sp, x21
   1f070:	ldr	x26, [x25, #8]
   1f074:	ldp	x4, x3, [x29, #-40]
   1f078:	stur	x25, [x29, #-48]
   1f07c:	add	x25, x21, x20, lsl #3
   1f080:	mov	x0, x21
   1f084:	mov	x1, x26
   1f088:	mov	x2, x24
   1f08c:	mov	x5, x19
   1f090:	mov	x6, x27
   1f094:	mov	x7, x25
   1f098:	bl	d000 <__gmpn_powm@plt>
   1f09c:	cbz	w22, 1f390 <__gmpz_powm@@Base+0x470>
   1f0a0:	cmp	x28, x24
   1f0a4:	stur	x23, [x29, #-72]
   1f0a8:	b.le	1f104 <__gmpz_powm@@Base+0x1e4>
   1f0ac:	lsl	x22, x28, #3
   1f0b0:	mov	w8, #0x7f00                	// #32512
   1f0b4:	cmp	x22, x8
   1f0b8:	b.hi	1f678 <__gmpz_powm@@Base+0x758>  // b.pmore
   1f0bc:	add	x9, x22, #0xf
   1f0c0:	mov	x8, sp
   1f0c4:	and	x9, x9, #0xfffffffffffffff0
   1f0c8:	sub	x23, x8, x9
   1f0cc:	mov	sp, x23
   1f0d0:	mov	x0, x23
   1f0d4:	mov	x1, x26
   1f0d8:	mov	x2, x24
   1f0dc:	bl	ca50 <__gmpn_copyi@plt>
   1f0e0:	cmp	x28, x24
   1f0e4:	mov	x26, x23
   1f0e8:	b.eq	1f104 <__gmpz_powm@@Base+0x1e4>  // b.none
   1f0ec:	lsl	x8, x24, #3
   1f0f0:	add	x0, x23, x8
   1f0f4:	sub	x2, x22, x8
   1f0f8:	mov	w1, wzr
   1f0fc:	bl	c5f0 <memset@plt>
   1f100:	mov	x26, x23
   1f104:	ldr	x8, [x26]
   1f108:	ldur	x3, [x29, #-40]
   1f10c:	tbnz	w8, #0, 1f150 <__gmpz_powm@@Base+0x230>
   1f110:	cmp	x3, #0x2
   1f114:	b.ge	1f1f8 <__gmpz_powm@@Base+0x2d8>  // b.tcont
   1f118:	ldur	x10, [x29, #-32]
   1f11c:	ldur	x12, [x29, #-56]
   1f120:	ubfiz	w8, w8, #1, #3
   1f124:	mov	w9, #0x1213                	// #4627
   1f128:	ldr	x10, [x10]
   1f12c:	cmp	w12, #0x0
   1f130:	cset	w11, ne  // ne = any
   1f134:	lsr	w8, w9, w8
   1f138:	sub	x9, x28, x11
   1f13c:	and	w8, w8, #0x3
   1f140:	add	x9, x12, x9, lsl #6
   1f144:	mul	x8, x10, x8
   1f148:	cmp	x8, x9
   1f14c:	b.cs	1f1f8 <__gmpz_powm@@Base+0x2d8>  // b.hs, b.nlast
   1f150:	ldur	x2, [x29, #-32]
   1f154:	add	x5, x25, x28, lsl #3
   1f158:	mov	x0, x25
   1f15c:	mov	x1, x26
   1f160:	mov	x4, x28
   1f164:	bl	c3c0 <__gmpn_powlo@plt>
   1f168:	cmp	x28, x27
   1f16c:	mov	x26, x19
   1f170:	b.le	1f214 <__gmpz_powm@@Base+0x2f4>
   1f174:	ldur	x23, [x29, #-72]
   1f178:	lsl	x19, x28, #3
   1f17c:	mov	w8, #0x7f00                	// #32512
   1f180:	cmp	x19, x8
   1f184:	b.hi	1f68c <__gmpz_powm@@Base+0x76c>  // b.pmore
   1f188:	add	x9, x19, #0xf
   1f18c:	mov	x8, sp
   1f190:	and	x9, x9, #0xfffffffffffffff0
   1f194:	sub	x22, x8, x9
   1f198:	mov	sp, x22
   1f19c:	mov	x0, x22
   1f1a0:	mov	x1, x26
   1f1a4:	mov	x2, x27
   1f1a8:	bl	ca50 <__gmpn_copyi@plt>
   1f1ac:	cmp	x28, x27
   1f1b0:	mov	x26, x22
   1f1b4:	b.eq	1f218 <__gmpz_powm@@Base+0x2f8>  // b.none
   1f1b8:	lsl	x8, x27, #3
   1f1bc:	add	x0, x22, x8
   1f1c0:	sub	x2, x19, x8
   1f1c4:	mov	w1, wzr
   1f1c8:	bl	c5f0 <memset@plt>
   1f1cc:	mov	x26, x22
   1f1d0:	b	1f218 <__gmpz_powm@@Base+0x2f8>
   1f1d4:	mov	x0, x27
   1f1d8:	bl	d1e0 <__gmpn_binvert_itch@plt>
   1f1dc:	lsl	x8, x20, #1
   1f1e0:	cmp	x0, x8
   1f1e4:	mov	w22, wzr
   1f1e8:	csel	x8, x0, x8, gt
   1f1ec:	mov	x9, x20
   1f1f0:	stur	xzr, [x29, #-56]
   1f1f4:	b	1f044 <__gmpz_powm@@Base+0x124>
   1f1f8:	add	x0, x21, x20, lsl #3
   1f1fc:	lsl	x2, x28, #3
   1f200:	mov	w1, wzr
   1f204:	bl	c5f0 <memset@plt>
   1f208:	cmp	x28, x27
   1f20c:	mov	x26, x19
   1f210:	b.gt	1f174 <__gmpz_powm@@Base+0x254>
   1f214:	ldur	x23, [x29, #-72]
   1f218:	add	x24, x25, x20, lsl #3
   1f21c:	add	x19, x25, x20, lsl #4
   1f220:	mov	x0, x24
   1f224:	mov	x1, x26
   1f228:	mov	x2, x28
   1f22c:	mov	x3, x19
   1f230:	bl	cd20 <__gmpn_binvert@plt>
   1f234:	cmp	x28, x27
   1f238:	csel	x22, x28, x27, lt  // lt = tstop
   1f23c:	cbz	x22, 1f27c <__gmpz_powm@@Base+0x35c>
   1f240:	mov	x0, x25
   1f244:	mov	x1, x25
   1f248:	mov	x2, x21
   1f24c:	mov	x3, x22
   1f250:	bl	c2d0 <__gmpn_sub_n@plt>
   1f254:	cbz	x0, 1f27c <__gmpz_powm@@Base+0x35c>
   1f258:	add	x8, x21, x20, lsl #3
   1f25c:	cmp	x22, x28
   1f260:	b.ge	1f27c <__gmpz_powm@@Base+0x35c>  // b.tcont
   1f264:	lsl	x9, x22, #3
   1f268:	ldr	x10, [x8, x9]
   1f26c:	add	x22, x22, #0x1
   1f270:	sub	x11, x10, #0x1
   1f274:	str	x11, [x8, x9]
   1f278:	cbz	x10, 1f25c <__gmpz_powm@@Base+0x33c>
   1f27c:	mov	x0, x19
   1f280:	mov	x1, x24
   1f284:	mov	x2, x25
   1f288:	mov	x3, x28
   1f28c:	bl	cec0 <__gmpn_mullo_n@plt>
   1f290:	ldur	x11, [x29, #-56]
   1f294:	cbz	w11, 1f2b0 <__gmpz_powm@@Base+0x390>
   1f298:	add	x8, x19, x28, lsl #3
   1f29c:	ldur	x9, [x8, #-8]
   1f2a0:	mov	x10, #0xffffffffffffffff    	// #-1
   1f2a4:	lsl	x10, x10, x11
   1f2a8:	bic	x9, x9, x10
   1f2ac:	stur	x9, [x8, #-8]
   1f2b0:	mov	x0, x25
   1f2b4:	cmp	x28, x27
   1f2b8:	b.le	1f2d8 <__gmpz_powm@@Base+0x3b8>
   1f2bc:	mov	x1, x19
   1f2c0:	mov	x2, x28
   1f2c4:	mov	x3, x26
   1f2c8:	mov	x4, x27
   1f2cc:	bl	ccd0 <__gmpn_mul@plt>
   1f2d0:	cbnz	x27, 1f2f0 <__gmpz_powm@@Base+0x3d0>
   1f2d4:	b	1f32c <__gmpz_powm@@Base+0x40c>
   1f2d8:	mov	x1, x26
   1f2dc:	mov	x2, x27
   1f2e0:	mov	x3, x19
   1f2e4:	mov	x4, x28
   1f2e8:	bl	ccd0 <__gmpn_mul@plt>
   1f2ec:	cbz	x27, 1f32c <__gmpz_powm@@Base+0x40c>
   1f2f0:	mov	x0, x21
   1f2f4:	mov	x1, x25
   1f2f8:	mov	x2, x21
   1f2fc:	mov	x3, x27
   1f300:	bl	ca70 <__gmpn_add_n@plt>
   1f304:	cbz	x0, 1f32c <__gmpz_powm@@Base+0x40c>
   1f308:	add	x8, x21, x20, lsl #3
   1f30c:	cmp	x27, x20
   1f310:	b.ge	1f390 <__gmpz_powm@@Base+0x470>  // b.tcont
   1f314:	lsl	x9, x27, #3
   1f318:	ldr	x10, [x8, x9]
   1f31c:	add	x27, x27, #0x1
   1f320:	adds	x10, x10, #0x1
   1f324:	str	x10, [x21, x9]
   1f328:	b.cs	1f30c <__gmpz_powm@@Base+0x3ec>  // b.hs, b.nlast
   1f32c:	cmp	x25, x21
   1f330:	b.eq	1f390 <__gmpz_powm@@Base+0x470>  // b.none
   1f334:	cmp	x27, x20
   1f338:	b.ge	1f390 <__gmpz_powm@@Base+0x470>  // b.tcont
   1f33c:	sub	x8, x20, x27
   1f340:	cmp	x8, #0x4
   1f344:	b.cc	1f374 <__gmpz_powm@@Base+0x454>  // b.lo, b.ul, b.last
   1f348:	lsl	x9, x27, #3
   1f34c:	add	x10, x21, x9
   1f350:	add	x11, x21, x20, lsl #4
   1f354:	cmp	x10, x11
   1f358:	lsl	x10, x20, #3
   1f35c:	b.cs	1f524 <__gmpz_powm@@Base+0x604>  // b.hs, b.nlast
   1f360:	add	x9, x9, x10
   1f364:	add	x11, x21, x10
   1f368:	add	x9, x21, x9
   1f36c:	cmp	x9, x11
   1f370:	b.cs	1f524 <__gmpz_powm@@Base+0x604>  // b.hs, b.nlast
   1f374:	mov	x9, x27
   1f378:	sub	x8, x20, x9
   1f37c:	add	x9, x21, x9, lsl #3
   1f380:	ldr	x10, [x9, x20, lsl #3]
   1f384:	subs	x8, x8, #0x1
   1f388:	str	x10, [x9], #8
   1f38c:	b.ne	1f380 <__gmpz_powm@@Base+0x460>  // b.any
   1f390:	ldur	x11, [x29, #-48]
   1f394:	ldur	x12, [x29, #-32]
   1f398:	sub	x22, x21, #0x8
   1f39c:	mov	x8, x20
   1f3a0:	subs	x9, x8, #0x1
   1f3a4:	b.lt	1f3c4 <__gmpz_powm@@Base+0x4a4>  // b.tstop
   1f3a8:	ldr	x10, [x22, x8, lsl #3]
   1f3ac:	mov	x8, x9
   1f3b0:	cbz	x10, 1f3a0 <__gmpz_powm@@Base+0x480>
   1f3b4:	add	x24, x9, #0x1
   1f3b8:	ldrb	w8, [x12]
   1f3bc:	tbnz	w8, #0, 1f3d0 <__gmpz_powm@@Base+0x4b0>
   1f3c0:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f3c4:	mov	x24, xzr
   1f3c8:	ldrb	w8, [x12]
   1f3cc:	tbz	w8, #0, 1f4dc <__gmpz_powm@@Base+0x5bc>
   1f3d0:	cbz	x24, 1f4dc <__gmpz_powm@@Base+0x5bc>
   1f3d4:	ldr	w8, [x11, #4]
   1f3d8:	tbz	w8, #31, 1f4dc <__gmpz_powm@@Base+0x5bc>
   1f3dc:	ldur	x8, [x29, #-64]
   1f3e0:	mov	x0, x21
   1f3e4:	mov	x2, x21
   1f3e8:	mov	x3, x24
   1f3ec:	ldr	x19, [x8, #8]
   1f3f0:	mov	x1, x19
   1f3f4:	bl	c2d0 <__gmpn_sub_n@plt>
   1f3f8:	cbz	x0, 1f41c <__gmpz_powm@@Base+0x4fc>
   1f3fc:	cmp	x24, x20
   1f400:	b.ge	1f4bc <__gmpz_powm@@Base+0x59c>  // b.tcont
   1f404:	lsl	x8, x24, #3
   1f408:	ldr	x9, [x19, x8]
   1f40c:	add	x24, x24, #0x1
   1f410:	sub	x10, x9, #0x1
   1f414:	str	x10, [x21, x8]
   1f418:	cbz	x9, 1f3fc <__gmpz_powm@@Base+0x4dc>
   1f41c:	cmp	x19, x21
   1f420:	b.eq	1f4bc <__gmpz_powm@@Base+0x59c>  // b.none
   1f424:	cmp	x24, x20
   1f428:	b.ge	1f4bc <__gmpz_powm@@Base+0x59c>  // b.tcont
   1f42c:	sub	x8, x20, x24
   1f430:	cmp	x8, #0x4
   1f434:	b.cc	1f49c <__gmpz_powm@@Base+0x57c>  // b.lo, b.ul, b.last
   1f438:	lsl	x10, x24, #3
   1f43c:	lsl	x9, x20, #3
   1f440:	add	x11, x21, x10
   1f444:	add	x12, x19, x9
   1f448:	cmp	x11, x12
   1f44c:	b.cs	1f460 <__gmpz_powm@@Base+0x540>  // b.hs, b.nlast
   1f450:	add	x9, x21, x9
   1f454:	add	x11, x19, x10
   1f458:	cmp	x9, x11
   1f45c:	b.hi	1f49c <__gmpz_powm@@Base+0x57c>  // b.pmore
   1f460:	and	x9, x8, #0xfffffffffffffffc
   1f464:	add	x11, x10, x19
   1f468:	add	x12, x10, x21
   1f46c:	add	x24, x24, x9
   1f470:	add	x10, x11, #0x10
   1f474:	add	x11, x12, #0x10
   1f478:	mov	x12, x9
   1f47c:	ldp	q0, q1, [x10, #-16]
   1f480:	add	x10, x10, #0x20
   1f484:	subs	x12, x12, #0x4
   1f488:	stp	q0, q1, [x11, #-16]
   1f48c:	add	x11, x11, #0x20
   1f490:	b.ne	1f47c <__gmpz_powm@@Base+0x55c>  // b.any
   1f494:	cmp	x8, x9
   1f498:	b.eq	1f4bc <__gmpz_powm@@Base+0x59c>  // b.none
   1f49c:	lsl	x10, x24, #3
   1f4a0:	sub	x8, x20, x24
   1f4a4:	add	x9, x21, x10
   1f4a8:	add	x10, x19, x10
   1f4ac:	ldr	x11, [x10], #8
   1f4b0:	subs	x8, x8, #0x1
   1f4b4:	str	x11, [x9], #8
   1f4b8:	b.ne	1f4ac <__gmpz_powm@@Base+0x58c>  // b.any
   1f4bc:	subs	x8, x20, #0x1
   1f4c0:	b.lt	1f4d8 <__gmpz_powm@@Base+0x5b8>  // b.tstop
   1f4c4:	ldr	x9, [x22, x20, lsl #3]
   1f4c8:	mov	x20, x8
   1f4cc:	cbz	x9, 1f4bc <__gmpz_powm@@Base+0x59c>
   1f4d0:	add	x24, x8, #0x1
   1f4d4:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f4d8:	mov	x24, xzr
   1f4dc:	ldrsw	x8, [x23]
   1f4e0:	cmp	x24, x8
   1f4e4:	b.gt	1f5d4 <__gmpz_powm@@Base+0x6b4>
   1f4e8:	ldr	x0, [x23, #8]
   1f4ec:	mov	x1, x21
   1f4f0:	mov	x2, x24
   1f4f4:	str	w24, [x23, #4]
   1f4f8:	bl	ca50 <__gmpn_copyi@plt>
   1f4fc:	ldur	x0, [x29, #-24]
   1f500:	cbnz	x0, 1f5bc <__gmpz_powm@@Base+0x69c>
   1f504:	mov	sp, x29
   1f508:	ldp	x20, x19, [sp, #80]
   1f50c:	ldp	x22, x21, [sp, #64]
   1f510:	ldp	x24, x23, [sp, #48]
   1f514:	ldp	x26, x25, [sp, #32]
   1f518:	ldp	x28, x27, [sp, #16]
   1f51c:	ldp	x29, x30, [sp], #96
   1f520:	ret
   1f524:	and	x11, x8, #0xfffffffffffffffc
   1f528:	add	x9, x27, x11
   1f52c:	add	x12, x21, x27, lsl #3
   1f530:	mov	x13, x11
   1f534:	add	x14, x12, x10
   1f538:	ldp	q0, q1, [x14]
   1f53c:	subs	x13, x13, #0x4
   1f540:	stp	q0, q1, [x12], #32
   1f544:	b.ne	1f534 <__gmpz_powm@@Base+0x614>  // b.any
   1f548:	cmp	x8, x11
   1f54c:	b.eq	1f390 <__gmpz_powm@@Base+0x470>  // b.none
   1f550:	b	1f378 <__gmpz_powm@@Base+0x458>
   1f554:	cbz	w22, 1f660 <__gmpz_powm@@Base+0x740>
   1f558:	add	x8, x20, #0x1
   1f55c:	cmp	w20, #0xfdf
   1f560:	lsl	x1, x8, #3
   1f564:	stur	w8, [x29, #-16]
   1f568:	b.hi	1f7c0 <__gmpz_powm@@Base+0x8a0>  // b.pmore
   1f56c:	add	x9, x1, #0xf
   1f570:	mov	x8, sp
   1f574:	and	x9, x9, #0x1ffffffff0
   1f578:	sub	x0, x8, x9
   1f57c:	mov	sp, x0
   1f580:	stur	x0, [x29, #-8]
   1f584:	sub	x0, x29, #0x10
   1f588:	mov	x1, x25
   1f58c:	mov	x2, x19
   1f590:	bl	cbd0 <__gmpz_invert@plt>
   1f594:	cbz	w0, 1f97c <__gmpz_powm@@Base+0xa5c>
   1f598:	sub	x25, x29, #0x10
   1f59c:	neg	x22, x22
   1f5a0:	ldr	w8, [x25, #4]
   1f5a4:	cmp	w8, #0x0
   1f5a8:	cneg	w24, w8, mi  // mi = first
   1f5ac:	cbnz	w24, 1ef84 <__gmpz_powm@@Base+0x64>
   1f5b0:	str	wzr, [x23, #4]
   1f5b4:	ldur	x0, [x29, #-24]
   1f5b8:	cbz	x0, 1f504 <__gmpz_powm@@Base+0x5e4>
   1f5bc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1f5c0:	b	1f504 <__gmpz_powm@@Base+0x5e4>
   1f5c4:	ldr	x8, [x26, #8]!
   1f5c8:	add	x28, x28, #0x1
   1f5cc:	cbnz	x8, 1efac <__gmpz_powm@@Base+0x8c>
   1f5d0:	b	1f5c4 <__gmpz_powm@@Base+0x6a4>
   1f5d4:	mov	x0, x23
   1f5d8:	mov	x1, x24
   1f5dc:	bl	c080 <__gmpz_realloc@plt>
   1f5e0:	b	1f4e8 <__gmpz_powm@@Base+0x5c8>
   1f5e4:	sub	x0, x29, #0x18
   1f5e8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f5ec:	mov	x21, x0
   1f5f0:	b	1f070 <__gmpz_powm@@Base+0x150>
   1f5f4:	mov	x27, x25
   1f5f8:	cmp	w20, #0xfe0
   1f5fc:	lsl	x19, x20, #3
   1f600:	mov	x25, x23
   1f604:	b.hi	1f7cc <__gmpz_powm@@Base+0x8ac>  // b.pmore
   1f608:	add	x9, x19, #0xf
   1f60c:	mov	x8, sp
   1f610:	and	x9, x9, #0xffffffff0
   1f614:	sub	x21, x8, x9
   1f618:	mov	sp, x21
   1f61c:	ldr	x23, [x27, #8]
   1f620:	cmp	w24, w20
   1f624:	b.cs	1f7e8 <__gmpz_powm@@Base+0x8c8>  // b.hs, b.nlast
   1f628:	mov	x8, x27
   1f62c:	ldr	w8, [x27, #4]
   1f630:	tbnz	w8, #31, 1f6c4 <__gmpz_powm@@Base+0x7a4>
   1f634:	mov	x0, x21
   1f638:	mov	x1, x23
   1f63c:	mov	x2, x24
   1f640:	bl	ca50 <__gmpn_copyi@plt>
   1f644:	mov	x23, x25
   1f648:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f64c:	sub	x0, x29, #0x18
   1f650:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f654:	ldr	x8, [x26]
   1f658:	mov	x21, x0
   1f65c:	b	1efe0 <__gmpz_powm@@Base+0xc0>
   1f660:	cmp	w20, #0x1
   1f664:	b.ne	1f6a0 <__gmpz_powm@@Base+0x780>  // b.any
   1f668:	ldr	x8, [x26]
   1f66c:	cmp	x8, #0x1
   1f670:	cset	w8, ne  // ne = any
   1f674:	b	1f6a4 <__gmpz_powm@@Base+0x784>
   1f678:	sub	x0, x29, #0x18
   1f67c:	mov	x1, x22
   1f680:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f684:	mov	x23, x0
   1f688:	b	1f0d0 <__gmpz_powm@@Base+0x1b0>
   1f68c:	sub	x0, x29, #0x18
   1f690:	mov	x1, x19
   1f694:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f698:	mov	x22, x0
   1f69c:	b	1f19c <__gmpz_powm@@Base+0x27c>
   1f6a0:	mov	w8, #0x1                   	// #1
   1f6a4:	ldr	w9, [x23]
   1f6a8:	str	w8, [x23, #4]
   1f6ac:	cmp	w9, #0x0
   1f6b0:	b.le	1f960 <__gmpz_powm@@Base+0xa40>
   1f6b4:	ldr	x0, [x23, #8]
   1f6b8:	mov	w8, #0x1                   	// #1
   1f6bc:	str	x8, [x0]
   1f6c0:	b	1f504 <__gmpz_powm@@Base+0x5e4>
   1f6c4:	mov	x0, x21
   1f6c8:	mov	x1, x26
   1f6cc:	mov	x2, x23
   1f6d0:	mov	x3, x24
   1f6d4:	mov	x22, x26
   1f6d8:	bl	c2d0 <__gmpn_sub_n@plt>
   1f6dc:	mov	x11, x26
   1f6e0:	cbz	x0, 1f704 <__gmpz_powm@@Base+0x7e4>
   1f6e4:	cmp	x24, x20
   1f6e8:	b.cs	1f7a0 <__gmpz_powm@@Base+0x880>  // b.hs, b.nlast
   1f6ec:	lsl	x8, x24, #3
   1f6f0:	ldr	x9, [x11, x8]
   1f6f4:	add	x24, x24, #0x1
   1f6f8:	sub	x10, x9, #0x1
   1f6fc:	str	x10, [x21, x8]
   1f700:	cbz	x9, 1f6e4 <__gmpz_powm@@Base+0x7c4>
   1f704:	cmp	x11, x21
   1f708:	b.eq	1f7a0 <__gmpz_powm@@Base+0x880>  // b.none
   1f70c:	cmp	x24, x20
   1f710:	b.ge	1f7a0 <__gmpz_powm@@Base+0x880>  // b.tcont
   1f714:	sub	x8, x20, x24
   1f718:	cmp	x8, #0x4
   1f71c:	b.cc	1f780 <__gmpz_powm@@Base+0x860>  // b.lo, b.ul, b.last
   1f720:	lsl	x10, x24, #3
   1f724:	add	x9, x21, x10
   1f728:	add	x11, x22, x19
   1f72c:	cmp	x9, x11
   1f730:	b.cs	1f744 <__gmpz_powm@@Base+0x824>  // b.hs, b.nlast
   1f734:	add	x9, x21, x19
   1f738:	add	x11, x22, x10
   1f73c:	cmp	x9, x11
   1f740:	b.hi	1f780 <__gmpz_powm@@Base+0x860>  // b.pmore
   1f744:	and	x9, x8, #0xfffffffffffffffc
   1f748:	add	x11, x10, x22
   1f74c:	add	x12, x10, x21
   1f750:	add	x24, x24, x9
   1f754:	add	x10, x11, #0x10
   1f758:	add	x11, x12, #0x10
   1f75c:	mov	x12, x9
   1f760:	ldp	q0, q1, [x10, #-16]
   1f764:	add	x10, x10, #0x20
   1f768:	subs	x12, x12, #0x4
   1f76c:	stp	q0, q1, [x11, #-16]
   1f770:	add	x11, x11, #0x20
   1f774:	b.ne	1f760 <__gmpz_powm@@Base+0x840>  // b.any
   1f778:	cmp	x8, x9
   1f77c:	b.eq	1f7a0 <__gmpz_powm@@Base+0x880>  // b.none
   1f780:	lsl	x10, x24, #3
   1f784:	sub	x8, x20, x24
   1f788:	add	x9, x21, x10
   1f78c:	add	x10, x22, x10
   1f790:	ldr	x11, [x10], #8
   1f794:	subs	x8, x8, #0x1
   1f798:	str	x11, [x9], #8
   1f79c:	b.ne	1f790 <__gmpz_powm@@Base+0x870>  // b.any
   1f7a0:	sub	x8, x21, #0x8
   1f7a4:	ldr	x10, [x8, x20, lsl #3]
   1f7a8:	sub	x9, x20, #0x1
   1f7ac:	mov	x20, x9
   1f7b0:	cbz	x10, 1f7a4 <__gmpz_powm@@Base+0x884>
   1f7b4:	add	x24, x9, #0x1
   1f7b8:	mov	x23, x25
   1f7bc:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f7c0:	sub	x0, x29, #0x18
   1f7c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f7c8:	b	1f580 <__gmpz_powm@@Base+0x660>
   1f7cc:	sub	x0, x29, #0x18
   1f7d0:	mov	x1, x19
   1f7d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f7d8:	mov	x21, x0
   1f7dc:	ldr	x23, [x27, #8]
   1f7e0:	cmp	w24, w20
   1f7e4:	b.cc	1f628 <__gmpz_powm@@Base+0x708>  // b.lo, b.ul, b.last
   1f7e8:	sub	x8, x24, x20
   1f7ec:	lsl	x8, x8, #3
   1f7f0:	add	x1, x8, #0x8
   1f7f4:	mov	w8, #0x7f00                	// #32512
   1f7f8:	cmp	x1, x8
   1f7fc:	b.hi	1f970 <__gmpz_powm@@Base+0xa50>  // b.pmore
   1f800:	add	x9, x1, #0xf
   1f804:	mov	x8, sp
   1f808:	and	x9, x9, #0xfffffffffffffff0
   1f80c:	sub	x0, x8, x9
   1f810:	mov	sp, x0
   1f814:	mov	x1, x21
   1f818:	mov	x2, xzr
   1f81c:	mov	x3, x23
   1f820:	mov	x4, x24
   1f824:	mov	x5, x26
   1f828:	mov	x6, x20
   1f82c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   1f830:	sub	x22, x21, #0x8
   1f834:	mov	x9, x20
   1f838:	subs	x8, x9, #0x1
   1f83c:	b.lt	1f860 <__gmpz_powm@@Base+0x940>  // b.tstop
   1f840:	ldr	x10, [x22, x9, lsl #3]
   1f844:	mov	x9, x8
   1f848:	cbz	x10, 1f838 <__gmpz_powm@@Base+0x918>
   1f84c:	ldr	w9, [x27, #4]
   1f850:	add	x24, x8, #0x1
   1f854:	tbnz	w9, #31, 1f86c <__gmpz_powm@@Base+0x94c>
   1f858:	mov	x23, x25
   1f85c:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f860:	mov	x24, xzr
   1f864:	mov	x23, x25
   1f868:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f86c:	mov	x0, x21
   1f870:	mov	x1, x26
   1f874:	mov	x2, x21
   1f878:	mov	x3, x24
   1f87c:	bl	c2d0 <__gmpn_sub_n@plt>
   1f880:	mov	x11, x26
   1f884:	cbz	x0, 1f8a8 <__gmpz_powm@@Base+0x988>
   1f888:	cmp	x24, x20
   1f88c:	b.ge	1f944 <__gmpz_powm@@Base+0xa24>  // b.tcont
   1f890:	lsl	x8, x24, #3
   1f894:	ldr	x9, [x11, x8]
   1f898:	add	x24, x24, #0x1
   1f89c:	sub	x10, x9, #0x1
   1f8a0:	str	x10, [x21, x8]
   1f8a4:	cbz	x9, 1f888 <__gmpz_powm@@Base+0x968>
   1f8a8:	cmp	x11, x21
   1f8ac:	b.eq	1f944 <__gmpz_powm@@Base+0xa24>  // b.none
   1f8b0:	cmp	x24, x20
   1f8b4:	b.ge	1f944 <__gmpz_powm@@Base+0xa24>  // b.tcont
   1f8b8:	sub	x8, x20, x24
   1f8bc:	cmp	x8, #0x4
   1f8c0:	b.cc	1f924 <__gmpz_powm@@Base+0xa04>  // b.lo, b.ul, b.last
   1f8c4:	lsl	x10, x24, #3
   1f8c8:	add	x9, x21, x10
   1f8cc:	add	x11, x26, x19
   1f8d0:	cmp	x9, x11
   1f8d4:	b.cs	1f8e8 <__gmpz_powm@@Base+0x9c8>  // b.hs, b.nlast
   1f8d8:	add	x9, x21, x19
   1f8dc:	add	x11, x26, x10
   1f8e0:	cmp	x9, x11
   1f8e4:	b.hi	1f924 <__gmpz_powm@@Base+0xa04>  // b.pmore
   1f8e8:	and	x9, x8, #0xfffffffffffffffc
   1f8ec:	add	x11, x10, x26
   1f8f0:	add	x12, x10, x21
   1f8f4:	add	x24, x24, x9
   1f8f8:	add	x10, x11, #0x10
   1f8fc:	add	x11, x12, #0x10
   1f900:	mov	x12, x9
   1f904:	ldp	q0, q1, [x10, #-16]
   1f908:	add	x10, x10, #0x20
   1f90c:	subs	x12, x12, #0x4
   1f910:	stp	q0, q1, [x11, #-16]
   1f914:	add	x11, x11, #0x20
   1f918:	b.ne	1f904 <__gmpz_powm@@Base+0x9e4>  // b.any
   1f91c:	cmp	x8, x9
   1f920:	b.eq	1f944 <__gmpz_powm@@Base+0xa24>  // b.none
   1f924:	lsl	x10, x24, #3
   1f928:	sub	x8, x20, x24
   1f92c:	add	x9, x21, x10
   1f930:	add	x10, x26, x10
   1f934:	ldr	x11, [x10], #8
   1f938:	subs	x8, x8, #0x1
   1f93c:	str	x11, [x9], #8
   1f940:	b.ne	1f934 <__gmpz_powm@@Base+0xa14>  // b.any
   1f944:	ldr	x9, [x22, x20, lsl #3]
   1f948:	sub	x8, x20, #0x1
   1f94c:	mov	x20, x8
   1f950:	cbz	x9, 1f944 <__gmpz_powm@@Base+0xa24>
   1f954:	add	x24, x8, #0x1
   1f958:	mov	x23, x25
   1f95c:	b	1f4dc <__gmpz_powm@@Base+0x5bc>
   1f960:	mov	w1, #0x1                   	// #1
   1f964:	mov	x0, x23
   1f968:	bl	c080 <__gmpz_realloc@plt>
   1f96c:	b	1f6b8 <__gmpz_powm@@Base+0x798>
   1f970:	sub	x0, x29, #0x18
   1f974:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f978:	b	1f814 <__gmpz_powm@@Base+0x8f4>
   1f97c:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000001f980 <__gmpz_powm_sec@@Base>:
   1f980:	stp	x29, x30, [sp, #-96]!
   1f984:	stp	x28, x27, [sp, #16]
   1f988:	stp	x26, x25, [sp, #32]
   1f98c:	stp	x24, x23, [sp, #48]
   1f990:	stp	x22, x21, [sp, #64]
   1f994:	stp	x20, x19, [sp, #80]
   1f998:	mov	x29, sp
   1f99c:	sub	sp, sp, #0x10
   1f9a0:	ldr	w8, [x3, #4]
   1f9a4:	cmp	w8, #0x0
   1f9a8:	cneg	w20, w8, mi  // mi = first
   1f9ac:	cbz	w20, 1fc50 <__gmpz_powm_sec@@Base+0x2d0>
   1f9b0:	ldr	x25, [x3, #8]
   1f9b4:	mov	x23, x3
   1f9b8:	ldr	x8, [x25]
   1f9bc:	tbz	w8, #0, 1fc50 <__gmpz_powm_sec@@Base+0x2d0>
   1f9c0:	ldrsw	x9, [x2, #4]
   1f9c4:	mov	x22, x2
   1f9c8:	mov	x19, x0
   1f9cc:	cmp	w9, #0x0
   1f9d0:	b.le	1fbd8 <__gmpz_powm_sec@@Base+0x258>
   1f9d4:	ldr	w8, [x1, #4]
   1f9d8:	mov	x24, x1
   1f9dc:	cmp	w8, #0x0
   1f9e0:	cneg	w26, w8, mi  // mi = first
   1f9e4:	cbz	w26, 1fc10 <__gmpz_powm_sec@@Base+0x290>
   1f9e8:	lsl	x27, x9, #6
   1f9ec:	mov	x0, x26
   1f9f0:	mov	x1, x27
   1f9f4:	mov	x2, x20
   1f9f8:	stur	xzr, [x29, #-8]
   1f9fc:	bl	c230 <__gmpn_sec_powm_itch@plt>
   1fa00:	add	x8, x0, x20
   1fa04:	lsl	x1, x8, #3
   1fa08:	mov	w8, #0x7f00                	// #32512
   1fa0c:	cmp	x1, x8
   1fa10:	b.hi	1fc18 <__gmpz_powm_sec@@Base+0x298>  // b.pmore
   1fa14:	add	x9, x1, #0xf
   1fa18:	mov	x8, sp
   1fa1c:	and	x9, x9, #0xfffffffffffffff0
   1fa20:	sub	x21, x8, x9
   1fa24:	mov	sp, x21
   1fa28:	ldr	x28, [x22, #8]
   1fa2c:	ldr	x1, [x24, #8]
   1fa30:	add	x7, x21, x20, lsl #3
   1fa34:	mov	x0, x21
   1fa38:	mov	x2, x26
   1fa3c:	mov	x3, x28
   1fa40:	mov	x4, x27
   1fa44:	mov	x5, x25
   1fa48:	mov	x6, x20
   1fa4c:	bl	cc60 <__gmpn_sec_powm@plt>
   1fa50:	sub	x25, x21, #0x8
   1fa54:	mov	x8, x20
   1fa58:	subs	x9, x8, #0x1
   1fa5c:	b.lt	1fa7c <__gmpz_powm_sec@@Base+0xfc>  // b.tstop
   1fa60:	ldr	x10, [x25, x8, lsl #3]
   1fa64:	mov	x8, x9
   1fa68:	cbz	x10, 1fa58 <__gmpz_powm_sec@@Base+0xd8>
   1fa6c:	add	x22, x9, #0x1
   1fa70:	ldrb	w8, [x28]
   1fa74:	tbnz	w8, #0, 1fa88 <__gmpz_powm_sec@@Base+0x108>
   1fa78:	b	1fb90 <__gmpz_powm_sec@@Base+0x210>
   1fa7c:	mov	x22, xzr
   1fa80:	ldrb	w8, [x28]
   1fa84:	tbz	w8, #0, 1fb90 <__gmpz_powm_sec@@Base+0x210>
   1fa88:	cbz	x22, 1fb90 <__gmpz_powm_sec@@Base+0x210>
   1fa8c:	ldr	w8, [x24, #4]
   1fa90:	tbz	w8, #31, 1fb90 <__gmpz_powm_sec@@Base+0x210>
   1fa94:	ldr	x23, [x23, #8]
   1fa98:	mov	x0, x21
   1fa9c:	mov	x2, x21
   1faa0:	mov	x3, x22
   1faa4:	mov	x1, x23
   1faa8:	bl	c2d0 <__gmpn_sub_n@plt>
   1faac:	cbz	x0, 1fad0 <__gmpz_powm_sec@@Base+0x150>
   1fab0:	cmp	x22, x20
   1fab4:	b.ge	1fb70 <__gmpz_powm_sec@@Base+0x1f0>  // b.tcont
   1fab8:	lsl	x8, x22, #3
   1fabc:	ldr	x9, [x23, x8]
   1fac0:	add	x22, x22, #0x1
   1fac4:	sub	x10, x9, #0x1
   1fac8:	str	x10, [x21, x8]
   1facc:	cbz	x9, 1fab0 <__gmpz_powm_sec@@Base+0x130>
   1fad0:	cmp	x23, x21
   1fad4:	b.eq	1fb70 <__gmpz_powm_sec@@Base+0x1f0>  // b.none
   1fad8:	cmp	x22, x20
   1fadc:	b.ge	1fb70 <__gmpz_powm_sec@@Base+0x1f0>  // b.tcont
   1fae0:	sub	x8, x20, x22
   1fae4:	cmp	x8, #0x4
   1fae8:	b.cc	1fb50 <__gmpz_powm_sec@@Base+0x1d0>  // b.lo, b.ul, b.last
   1faec:	lsl	x10, x22, #3
   1faf0:	lsl	x9, x20, #3
   1faf4:	add	x11, x21, x10
   1faf8:	add	x12, x23, x9
   1fafc:	cmp	x11, x12
   1fb00:	b.cs	1fb14 <__gmpz_powm_sec@@Base+0x194>  // b.hs, b.nlast
   1fb04:	add	x9, x21, x9
   1fb08:	add	x11, x23, x10
   1fb0c:	cmp	x9, x11
   1fb10:	b.hi	1fb50 <__gmpz_powm_sec@@Base+0x1d0>  // b.pmore
   1fb14:	and	x9, x8, #0xfffffffffffffffc
   1fb18:	add	x11, x10, x23
   1fb1c:	add	x12, x10, x21
   1fb20:	add	x22, x22, x9
   1fb24:	add	x10, x11, #0x10
   1fb28:	add	x11, x12, #0x10
   1fb2c:	mov	x12, x9
   1fb30:	ldp	q0, q1, [x10, #-16]
   1fb34:	add	x10, x10, #0x20
   1fb38:	subs	x12, x12, #0x4
   1fb3c:	stp	q0, q1, [x11, #-16]
   1fb40:	add	x11, x11, #0x20
   1fb44:	b.ne	1fb30 <__gmpz_powm_sec@@Base+0x1b0>  // b.any
   1fb48:	cmp	x8, x9
   1fb4c:	b.eq	1fb70 <__gmpz_powm_sec@@Base+0x1f0>  // b.none
   1fb50:	lsl	x10, x22, #3
   1fb54:	sub	x8, x20, x22
   1fb58:	add	x9, x21, x10
   1fb5c:	add	x10, x23, x10
   1fb60:	ldr	x11, [x10], #8
   1fb64:	subs	x8, x8, #0x1
   1fb68:	str	x11, [x9], #8
   1fb6c:	b.ne	1fb60 <__gmpz_powm_sec@@Base+0x1e0>  // b.any
   1fb70:	subs	x8, x20, #0x1
   1fb74:	b.lt	1fb8c <__gmpz_powm_sec@@Base+0x20c>  // b.tstop
   1fb78:	ldr	x9, [x25, x20, lsl #3]
   1fb7c:	mov	x20, x8
   1fb80:	cbz	x9, 1fb70 <__gmpz_powm_sec@@Base+0x1f0>
   1fb84:	add	x22, x8, #0x1
   1fb88:	b	1fb90 <__gmpz_powm_sec@@Base+0x210>
   1fb8c:	mov	x22, xzr
   1fb90:	ldrsw	x8, [x19]
   1fb94:	cmp	x22, x8
   1fb98:	b.gt	1fc28 <__gmpz_powm_sec@@Base+0x2a8>
   1fb9c:	ldr	x0, [x19, #8]
   1fba0:	mov	x1, x21
   1fba4:	mov	x2, x22
   1fba8:	str	w22, [x19, #4]
   1fbac:	bl	ca50 <__gmpn_copyi@plt>
   1fbb0:	ldur	x0, [x29, #-8]
   1fbb4:	cbnz	x0, 1fc38 <__gmpz_powm_sec@@Base+0x2b8>
   1fbb8:	mov	sp, x29
   1fbbc:	ldp	x20, x19, [sp, #80]
   1fbc0:	ldp	x22, x21, [sp, #64]
   1fbc4:	ldp	x24, x23, [sp, #48]
   1fbc8:	ldp	x26, x25, [sp, #32]
   1fbcc:	ldp	x28, x27, [sp, #16]
   1fbd0:	ldp	x29, x30, [sp], #96
   1fbd4:	ret
   1fbd8:	cbnz	w9, 1fc50 <__gmpz_powm_sec@@Base+0x2d0>
   1fbdc:	ldr	w9, [x19]
   1fbe0:	cmp	w20, #0x1
   1fbe4:	cset	w10, ne  // ne = any
   1fbe8:	cmp	x8, #0x1
   1fbec:	cset	w8, ne  // ne = any
   1fbf0:	orr	w8, w10, w8
   1fbf4:	cmp	w9, #0x0
   1fbf8:	str	w8, [x19, #4]
   1fbfc:	b.le	1fc40 <__gmpz_powm_sec@@Base+0x2c0>
   1fc00:	ldr	x0, [x19, #8]
   1fc04:	mov	w8, #0x1                   	// #1
   1fc08:	str	x8, [x0]
   1fc0c:	b	1fbb8 <__gmpz_powm_sec@@Base+0x238>
   1fc10:	str	wzr, [x19, #4]
   1fc14:	b	1fbb8 <__gmpz_powm_sec@@Base+0x238>
   1fc18:	sub	x0, x29, #0x8
   1fc1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fc20:	mov	x21, x0
   1fc24:	b	1fa28 <__gmpz_powm_sec@@Base+0xa8>
   1fc28:	mov	x0, x19
   1fc2c:	mov	x1, x22
   1fc30:	bl	c080 <__gmpz_realloc@plt>
   1fc34:	b	1fb9c <__gmpz_powm_sec@@Base+0x21c>
   1fc38:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1fc3c:	b	1fbb8 <__gmpz_powm_sec@@Base+0x238>
   1fc40:	mov	w1, #0x1                   	// #1
   1fc44:	mov	x0, x19
   1fc48:	bl	c080 <__gmpz_realloc@plt>
   1fc4c:	b	1fc04 <__gmpz_powm_sec@@Base+0x284>
   1fc50:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000001fc54 <__gmpz_powm_ui@@Base>:
   1fc54:	stp	x29, x30, [sp, #-96]!
   1fc58:	stp	x28, x27, [sp, #16]
   1fc5c:	stp	x26, x25, [sp, #32]
   1fc60:	stp	x24, x23, [sp, #48]
   1fc64:	stp	x22, x21, [sp, #64]
   1fc68:	stp	x20, x19, [sp, #80]
   1fc6c:	mov	x29, sp
   1fc70:	sub	sp, sp, #0x40
   1fc74:	mov	x25, x3
   1fc78:	mov	x27, x2
   1fc7c:	mov	x26, x1
   1fc80:	cmp	x2, #0x13
   1fc84:	mov	x23, x0
   1fc88:	b.hi	1fcc0 <__gmpz_powm_ui@@Base+0x6c>  // b.pmore
   1fc8c:	ldr	w8, [x25, #4]
   1fc90:	cmp	w8, #0x0
   1fc94:	cneg	w22, w8, mi  // mi = first
   1fc98:	cbz	w22, 20210 <__gmpz_powm_ui@@Base+0x5bc>
   1fc9c:	ldr	x24, [x25, #8]
   1fca0:	cmp	x27, #0x1
   1fca4:	b.hi	1fce8 <__gmpz_powm_ui@@Base+0x94>  // b.pmore
   1fca8:	b.ne	1fd44 <__gmpz_powm_ui@@Base+0xf0>  // b.any
   1fcac:	mov	x0, x23
   1fcb0:	mov	x1, x26
   1fcb4:	mov	x2, x25
   1fcb8:	bl	cdf0 <__gmpz_mod@plt>
   1fcbc:	b	20198 <__gmpz_powm_ui@@Base+0x544>
   1fcc0:	sub	x8, x29, #0x8
   1fcc4:	mov	w9, #0x1                   	// #1
   1fcc8:	sub	x2, x29, #0x18
   1fccc:	mov	x0, x23
   1fcd0:	mov	x1, x26
   1fcd4:	mov	x3, x25
   1fcd8:	stp	x8, x27, [x29, #-16]
   1fcdc:	stur	w9, [x29, #-20]
   1fce0:	bl	c370 <__gmpz_powm@plt>
   1fce4:	b	20198 <__gmpz_powm_ui@@Base+0x544>
   1fce8:	stur	xzr, [x29, #-8]
   1fcec:	sub	x20, x22, #0x1
   1fcf0:	ldr	x8, [x24, x20, lsl #3]
   1fcf4:	clz	x28, x8
   1fcf8:	cbz	w28, 1fd34 <__gmpz_powm_ui@@Base+0xe0>
   1fcfc:	cmp	w22, #0xfe0
   1fd00:	lsl	x1, x22, #3
   1fd04:	b.hi	201c0 <__gmpz_powm_ui@@Base+0x56c>  // b.pmore
   1fd08:	add	x9, x1, #0xf
   1fd0c:	mov	x8, sp
   1fd10:	and	x9, x9, #0xffffffff0
   1fd14:	sub	x19, x8, x9
   1fd18:	mov	sp, x19
   1fd1c:	mov	x0, x19
   1fd20:	mov	x1, x24
   1fd24:	mov	x2, x22
   1fd28:	mov	w3, w28
   1fd2c:	bl	c180 <__gmpn_lshift@plt>
   1fd30:	mov	x24, x19
   1fd34:	cmp	w22, #0x1
   1fd38:	b.ne	1fd5c <__gmpz_powm_ui@@Base+0x108>  // b.any
   1fd3c:	mov	x21, xzr
   1fd40:	b	1fd64 <__gmpz_powm_ui@@Base+0x110>
   1fd44:	cmp	w22, #0x1
   1fd48:	b.ne	20018 <__gmpz_powm_ui@@Base+0x3c4>  // b.any
   1fd4c:	ldr	x8, [x24]
   1fd50:	cmp	x8, #0x1
   1fd54:	cset	w8, ne  // ne = any
   1fd58:	b	2001c <__gmpz_powm_ui@@Base+0x3c8>
   1fd5c:	add	x8, x24, x22, lsl #3
   1fd60:	ldur	x21, [x8, #-16]
   1fd64:	ldr	x19, [x24, x20, lsl #3]
   1fd68:	mov	x0, x19
   1fd6c:	bl	d3f0 <__gmpn_invert_limb@plt>
   1fd70:	mul	x8, x0, x19
   1fd74:	adds	x8, x8, x21
   1fd78:	b.cc	1fd94 <__gmpz_powm_ui@@Base+0x140>  // b.lo, b.ul, b.last
   1fd7c:	subs	x8, x8, x19
   1fd80:	cset	w9, cs  // cs = hs, nlast
   1fd84:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   1fd88:	mvn	x9, x9
   1fd8c:	add	x0, x9, x0
   1fd90:	sub	x8, x8, x10
   1fd94:	umulh	x9, x21, x0
   1fd98:	adds	x9, x9, x8
   1fd9c:	stp	x28, x27, [x29, #-40]
   1fda0:	b.cc	1fdc8 <__gmpz_powm_ui@@Base+0x174>  // b.lo, b.ul, b.last
   1fda4:	cmp	x9, x19
   1fda8:	sub	x8, x0, #0x1
   1fdac:	b.cc	1fdcc <__gmpz_powm_ui@@Base+0x178>  // b.lo, b.ul, b.last
   1fdb0:	mul	x10, x0, x21
   1fdb4:	cmp	x9, x19
   1fdb8:	sub	x11, x0, #0x2
   1fdbc:	ccmp	x10, x21, #0x2, ls  // ls = plast
   1fdc0:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   1fdc4:	b	1fdcc <__gmpz_powm_ui@@Base+0x178>
   1fdc8:	mov	x8, x0
   1fdcc:	stur	x8, [x29, #-24]
   1fdd0:	ldr	w8, [x26, #4]
   1fdd4:	ldr	x28, [x26, #8]
   1fdd8:	cmp	w8, #0x0
   1fddc:	cneg	w27, w8, mi  // mi = first
   1fde0:	cmp	w27, w22
   1fde4:	b.ls	1fe4c <__gmpz_powm_ui@@Base+0x1f8>  // b.plast
   1fde8:	cmp	w22, #0xfe0
   1fdec:	lsl	x1, x22, #3
   1fdf0:	b.hi	201f0 <__gmpz_powm_ui@@Base+0x59c>  // b.pmore
   1fdf4:	add	x9, x1, #0xf
   1fdf8:	mov	x8, sp
   1fdfc:	and	x9, x9, #0xffffffff0
   1fe00:	sub	x19, x8, x9
   1fe04:	mov	sp, x19
   1fe08:	sub	x5, x29, #0x18
   1fe0c:	mov	x0, x19
   1fe10:	mov	x1, x28
   1fe14:	mov	x2, x27
   1fe18:	mov	x3, x24
   1fe1c:	mov	x4, x22
   1fe20:	bl	20214 <__gmpz_powm_ui@@Base+0x5c0>
   1fe24:	sub	x8, x19, #0x8
   1fe28:	mov	x10, x22
   1fe2c:	subs	x9, x10, #0x1
   1fe30:	b.lt	20008 <__gmpz_powm_ui@@Base+0x3b4>  // b.tstop
   1fe34:	ldr	x11, [x8, x10, lsl #3]
   1fe38:	mov	x10, x9
   1fe3c:	cbz	x11, 1fe2c <__gmpz_powm_ui@@Base+0x1d8>
   1fe40:	add	x27, x9, #0x1
   1fe44:	mov	x28, x19
   1fe48:	b	1fe50 <__gmpz_powm_ui@@Base+0x1fc>
   1fe4c:	cbz	w27, 20008 <__gmpz_powm_ui@@Base+0x3b4>
   1fe50:	add	x20, x22, #0x1
   1fe54:	mov	w8, #0x1                   	// #1
   1fe58:	add	x9, x20, x22
   1fe5c:	bfi	x8, x22, #1, #32
   1fe60:	add	x8, x9, x8
   1fe64:	cmp	x8, #0xfe0
   1fe68:	lsl	x1, x8, #3
   1fe6c:	stp	x26, x23, [x29, #-56]
   1fe70:	stur	x25, [x29, #-64]
   1fe74:	b.hi	201d0 <__gmpz_powm_ui@@Base+0x57c>  // b.pmore
   1fe78:	add	x9, x1, #0xf
   1fe7c:	mov	x8, sp
   1fe80:	and	x9, x9, #0x7ffffffff0
   1fe84:	sub	x26, x8, x9
   1fe88:	mov	sp, x26
   1fe8c:	add	x19, x26, x22, lsl #3
   1fe90:	mov	x0, x26
   1fe94:	mov	x1, x28
   1fe98:	mov	x2, x27
   1fe9c:	add	x25, x19, x20, lsl #3
   1fea0:	bl	ca50 <__gmpn_copyi@plt>
   1fea4:	ldur	x9, [x29, #-32]
   1fea8:	mov	x20, x27
   1feac:	clz	x8, x9
   1feb0:	lsl	x21, x9, x8
   1feb4:	sub	w23, w8, #0x3f
   1feb8:	b	1fed4 <__gmpz_powm_ui@@Base+0x280>
   1febc:	mov	x0, x26
   1fec0:	mov	x1, x25
   1fec4:	mov	x2, x20
   1fec8:	bl	ca50 <__gmpn_copyi@plt>
   1fecc:	adds	w23, w23, #0x1
   1fed0:	b.cs	1ff98 <__gmpz_powm_ui@@Base+0x344>  // b.hs, b.nlast
   1fed4:	mov	x0, x25
   1fed8:	mov	x1, x26
   1fedc:	mov	x2, x20
   1fee0:	lsl	x21, x21, #1
   1fee4:	bl	c8e0 <__gmpn_sqr@plt>
   1fee8:	add	x8, x25, x20, lsl #4
   1feec:	ldur	x8, [x8, #-8]
   1fef0:	lsl	x9, x20, #1
   1fef4:	cmp	x8, #0x0
   1fef8:	cset	w8, eq  // eq = none
   1fefc:	sub	x20, x9, x8
   1ff00:	cmp	x20, x22
   1ff04:	b.lt	1ff28 <__gmpz_powm_ui@@Base+0x2d4>  // b.tstop
   1ff08:	sub	x4, x29, #0x18
   1ff0c:	mov	x0, x25
   1ff10:	mov	x1, x20
   1ff14:	mov	x2, x24
   1ff18:	mov	x3, x22
   1ff1c:	mov	x5, x19
   1ff20:	bl	202f8 <__gmpz_powm_ui@@Base+0x6a4>
   1ff24:	mov	x20, x22
   1ff28:	mov	x0, x26
   1ff2c:	mov	x1, x25
   1ff30:	mov	x2, x20
   1ff34:	bl	ca50 <__gmpn_copyi@plt>
   1ff38:	tbz	x21, #63, 1fecc <__gmpz_powm_ui@@Base+0x278>
   1ff3c:	mov	x0, x25
   1ff40:	mov	x1, x26
   1ff44:	mov	x2, x20
   1ff48:	mov	x3, x28
   1ff4c:	mov	x4, x27
   1ff50:	bl	ccd0 <__gmpn_mul@plt>
   1ff54:	add	x8, x20, x27
   1ff58:	add	x9, x25, x8, lsl #3
   1ff5c:	ldur	x9, [x9, #-8]
   1ff60:	cmp	x9, #0x0
   1ff64:	cset	w9, eq  // eq = none
   1ff68:	sub	x20, x8, x9
   1ff6c:	cmp	x20, x22
   1ff70:	b.lt	1febc <__gmpz_powm_ui@@Base+0x268>  // b.tstop
   1ff74:	sub	x4, x29, #0x18
   1ff78:	mov	x0, x25
   1ff7c:	mov	x1, x20
   1ff80:	mov	x2, x24
   1ff84:	mov	x3, x22
   1ff88:	mov	x5, x19
   1ff8c:	bl	202f8 <__gmpz_powm_ui@@Base+0x6a4>
   1ff90:	mov	x20, x22
   1ff94:	b	1febc <__gmpz_powm_ui@@Base+0x268>
   1ff98:	ldur	x27, [x29, #-40]
   1ff9c:	cbz	w27, 2003c <__gmpz_powm_ui@@Base+0x3e8>
   1ffa0:	mov	x0, x25
   1ffa4:	mov	x1, x26
   1ffa8:	mov	x2, x20
   1ffac:	mov	w3, w27
   1ffb0:	bl	c180 <__gmpn_lshift@plt>
   1ffb4:	ldur	x21, [x29, #-48]
   1ffb8:	ldur	x23, [x29, #-32]
   1ffbc:	cmp	x0, #0x0
   1ffc0:	str	x0, [x25, x20, lsl #3]
   1ffc4:	cinc	x20, x20, ne  // ne = any
   1ffc8:	cmp	x20, x22
   1ffcc:	b.lt	1fff0 <__gmpz_powm_ui@@Base+0x39c>  // b.tstop
   1ffd0:	sub	x4, x29, #0x18
   1ffd4:	mov	x0, x25
   1ffd8:	mov	x1, x20
   1ffdc:	mov	x2, x24
   1ffe0:	mov	x3, x22
   1ffe4:	mov	x5, x19
   1ffe8:	bl	202f8 <__gmpz_powm_ui@@Base+0x6a4>
   1ffec:	mov	x20, x22
   1fff0:	mov	x0, x26
   1fff4:	mov	x1, x25
   1fff8:	mov	x2, x20
   1fffc:	mov	w3, w27
   20000:	bl	c1a0 <__gmpn_rshift@plt>
   20004:	b	20044 <__gmpz_powm_ui@@Base+0x3f0>
   20008:	str	wzr, [x23, #4]
   2000c:	ldur	x0, [x29, #-8]
   20010:	cbz	x0, 20198 <__gmpz_powm_ui@@Base+0x544>
   20014:	b	201b8 <__gmpz_powm_ui@@Base+0x564>
   20018:	mov	w8, #0x1                   	// #1
   2001c:	ldr	w9, [x23]
   20020:	str	w8, [x23, #4]
   20024:	cmp	w9, #0x0
   20028:	b.le	20200 <__gmpz_powm_ui@@Base+0x5ac>
   2002c:	ldr	x0, [x23, #8]
   20030:	mov	w8, #0x1                   	// #1
   20034:	str	x8, [x0]
   20038:	b	20198 <__gmpz_powm_ui@@Base+0x544>
   2003c:	ldur	x21, [x29, #-48]
   20040:	ldur	x23, [x29, #-32]
   20044:	sub	x25, x26, #0x8
   20048:	mov	x24, x20
   2004c:	subs	x20, x20, #0x1
   20050:	b.lt	2005c <__gmpz_powm_ui@@Base+0x408>  // b.tstop
   20054:	ldr	x8, [x25, x24, lsl #3]
   20058:	cbz	x8, 20048 <__gmpz_powm_ui@@Base+0x3f4>
   2005c:	tbz	w23, #0, 20170 <__gmpz_powm_ui@@Base+0x51c>
   20060:	cbz	x24, 20170 <__gmpz_powm_ui@@Base+0x51c>
   20064:	ldur	x8, [x29, #-56]
   20068:	ldr	w8, [x8, #4]
   2006c:	tbz	w8, #31, 20170 <__gmpz_powm_ui@@Base+0x51c>
   20070:	ldur	x8, [x29, #-64]
   20074:	mov	x0, x26
   20078:	mov	x2, x26
   2007c:	mov	x3, x24
   20080:	ldr	x19, [x8, #8]
   20084:	mov	x1, x19
   20088:	bl	c2d0 <__gmpn_sub_n@plt>
   2008c:	cbz	x0, 200b0 <__gmpz_powm_ui@@Base+0x45c>
   20090:	cmp	x24, x22
   20094:	b.ge	20150 <__gmpz_powm_ui@@Base+0x4fc>  // b.tcont
   20098:	lsl	x8, x24, #3
   2009c:	ldr	x9, [x19, x8]
   200a0:	add	x24, x24, #0x1
   200a4:	sub	x10, x9, #0x1
   200a8:	str	x10, [x26, x8]
   200ac:	cbz	x9, 20090 <__gmpz_powm_ui@@Base+0x43c>
   200b0:	cmp	x19, x26
   200b4:	b.eq	20150 <__gmpz_powm_ui@@Base+0x4fc>  // b.none
   200b8:	cmp	x24, x22
   200bc:	b.ge	20150 <__gmpz_powm_ui@@Base+0x4fc>  // b.tcont
   200c0:	sub	x8, x22, x24
   200c4:	cmp	x8, #0x4
   200c8:	b.cc	20130 <__gmpz_powm_ui@@Base+0x4dc>  // b.lo, b.ul, b.last
   200cc:	lsl	x10, x24, #3
   200d0:	lsl	x9, x22, #3
   200d4:	add	x11, x26, x10
   200d8:	add	x12, x19, x9
   200dc:	cmp	x11, x12
   200e0:	b.cs	200f4 <__gmpz_powm_ui@@Base+0x4a0>  // b.hs, b.nlast
   200e4:	add	x9, x26, x9
   200e8:	add	x11, x19, x10
   200ec:	cmp	x9, x11
   200f0:	b.hi	20130 <__gmpz_powm_ui@@Base+0x4dc>  // b.pmore
   200f4:	and	x9, x8, #0xfffffffffffffffc
   200f8:	add	x11, x10, x19
   200fc:	add	x12, x10, x26
   20100:	add	x24, x24, x9
   20104:	add	x10, x11, #0x10
   20108:	add	x11, x12, #0x10
   2010c:	mov	x12, x9
   20110:	ldp	q0, q1, [x10, #-16]
   20114:	add	x10, x10, #0x20
   20118:	subs	x12, x12, #0x4
   2011c:	stp	q0, q1, [x11, #-16]
   20120:	add	x11, x11, #0x20
   20124:	b.ne	20110 <__gmpz_powm_ui@@Base+0x4bc>  // b.any
   20128:	cmp	x8, x9
   2012c:	b.eq	20150 <__gmpz_powm_ui@@Base+0x4fc>  // b.none
   20130:	lsl	x10, x24, #3
   20134:	sub	x8, x22, x24
   20138:	add	x9, x26, x10
   2013c:	add	x10, x19, x10
   20140:	ldr	x11, [x10], #8
   20144:	subs	x8, x8, #0x1
   20148:	str	x11, [x9], #8
   2014c:	b.ne	20140 <__gmpz_powm_ui@@Base+0x4ec>  // b.any
   20150:	subs	x8, x22, #0x1
   20154:	b.lt	2016c <__gmpz_powm_ui@@Base+0x518>  // b.tstop
   20158:	ldr	x9, [x25, x22, lsl #3]
   2015c:	mov	x22, x8
   20160:	cbz	x9, 20150 <__gmpz_powm_ui@@Base+0x4fc>
   20164:	add	x24, x8, #0x1
   20168:	b	20170 <__gmpz_powm_ui@@Base+0x51c>
   2016c:	mov	x24, xzr
   20170:	ldrsw	x8, [x21]
   20174:	cmp	x24, x8
   20178:	b.gt	201e0 <__gmpz_powm_ui@@Base+0x58c>
   2017c:	ldr	x0, [x21, #8]
   20180:	mov	x1, x26
   20184:	mov	x2, x24
   20188:	str	w24, [x21, #4]
   2018c:	bl	ca50 <__gmpn_copyi@plt>
   20190:	ldur	x0, [x29, #-8]
   20194:	cbnz	x0, 201b8 <__gmpz_powm_ui@@Base+0x564>
   20198:	mov	sp, x29
   2019c:	ldp	x20, x19, [sp, #80]
   201a0:	ldp	x22, x21, [sp, #64]
   201a4:	ldp	x24, x23, [sp, #48]
   201a8:	ldp	x26, x25, [sp, #32]
   201ac:	ldp	x28, x27, [sp, #16]
   201b0:	ldp	x29, x30, [sp], #96
   201b4:	ret
   201b8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   201bc:	b	20198 <__gmpz_powm_ui@@Base+0x544>
   201c0:	sub	x0, x29, #0x8
   201c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   201c8:	mov	x19, x0
   201cc:	b	1fd1c <__gmpz_powm_ui@@Base+0xc8>
   201d0:	sub	x0, x29, #0x8
   201d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   201d8:	mov	x26, x0
   201dc:	b	1fe8c <__gmpz_powm_ui@@Base+0x238>
   201e0:	mov	x0, x21
   201e4:	mov	x1, x24
   201e8:	bl	c080 <__gmpz_realloc@plt>
   201ec:	b	2017c <__gmpz_powm_ui@@Base+0x528>
   201f0:	sub	x0, x29, #0x8
   201f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   201f8:	mov	x19, x0
   201fc:	b	1fe08 <__gmpz_powm_ui@@Base+0x1b4>
   20200:	mov	w1, #0x1                   	// #1
   20204:	mov	x0, x23
   20208:	bl	c080 <__gmpz_realloc@plt>
   2020c:	b	20030 <__gmpz_powm_ui@@Base+0x3dc>
   20210:	bl	bfd0 <__gmp_divide_by_zero@plt>
   20214:	stp	x29, x30, [sp, #-80]!
   20218:	stp	x26, x25, [sp, #16]
   2021c:	stp	x24, x23, [sp, #32]
   20220:	stp	x22, x21, [sp, #48]
   20224:	stp	x20, x19, [sp, #64]
   20228:	mov	x29, sp
   2022c:	sub	sp, sp, #0x10
   20230:	mov	w8, #0x1                   	// #1
   20234:	bfi	x8, x2, #1, #63
   20238:	sub	x8, x8, x4
   2023c:	mov	x24, x1
   20240:	lsl	x1, x8, #3
   20244:	mov	w8, #0x7f00                	// #32512
   20248:	mov	x21, x5
   2024c:	mov	x19, x4
   20250:	mov	x22, x3
   20254:	mov	x23, x2
   20258:	mov	x20, x0
   2025c:	cmp	x1, x8
   20260:	stur	xzr, [x29, #-8]
   20264:	b.hi	202e0 <__gmpz_powm_ui@@Base+0x68c>  // b.pmore
   20268:	add	x9, x1, #0xf
   2026c:	mov	x8, sp
   20270:	and	x9, x9, #0xfffffffffffffff0
   20274:	sub	x25, x8, x9
   20278:	mov	sp, x25
   2027c:	mov	x0, x25
   20280:	mov	x1, x24
   20284:	mov	x2, x23
   20288:	add	x26, x25, x23, lsl #3
   2028c:	bl	ca50 <__gmpn_copyi@plt>
   20290:	mov	x0, x25
   20294:	mov	x1, x23
   20298:	mov	x2, x22
   2029c:	mov	x3, x19
   202a0:	mov	x4, x21
   202a4:	mov	x5, x26
   202a8:	bl	202f8 <__gmpz_powm_ui@@Base+0x6a4>
   202ac:	mov	x0, x20
   202b0:	mov	x1, x25
   202b4:	mov	x2, x19
   202b8:	bl	ca50 <__gmpn_copyi@plt>
   202bc:	ldur	x0, [x29, #-8]
   202c0:	cbnz	x0, 202f0 <__gmpz_powm_ui@@Base+0x69c>
   202c4:	mov	sp, x29
   202c8:	ldp	x20, x19, [sp, #64]
   202cc:	ldp	x22, x21, [sp, #48]
   202d0:	ldp	x24, x23, [sp, #32]
   202d4:	ldp	x26, x25, [sp, #16]
   202d8:	ldp	x29, x30, [sp], #80
   202dc:	ret
   202e0:	sub	x0, x29, #0x8
   202e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   202e8:	mov	x25, x0
   202ec:	b	2027c <__gmpz_powm_ui@@Base+0x628>
   202f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   202f4:	b	202c4 <__gmpz_powm_ui@@Base+0x670>
   202f8:	stp	x29, x30, [sp, #-80]!
   202fc:	stp	x24, x23, [sp, #32]
   20300:	stp	x22, x21, [sp, #48]
   20304:	stp	x20, x19, [sp, #64]
   20308:	mov	x23, x5
   2030c:	mov	x8, x4
   20310:	mov	x22, x2
   20314:	mov	x21, x1
   20318:	cmp	x3, #0x2
   2031c:	mov	x19, x0
   20320:	str	x25, [sp, #16]
   20324:	mov	x29, sp
   20328:	b.eq	2036c <__gmpz_powm_ui@@Base+0x718>  // b.none
   2032c:	mov	x20, x3
   20330:	cmp	x3, #0x1
   20334:	b.ne	2039c <__gmpz_powm_ui@@Base+0x748>  // b.any
   20338:	ldr	x4, [x22]
   2033c:	mov	x0, x23
   20340:	mov	x1, xzr
   20344:	mov	x2, x19
   20348:	mov	x3, x21
   2034c:	bl	cd00 <__gmpn_divrem_1@plt>
   20350:	str	x0, [x19]
   20354:	ldp	x20, x19, [sp, #64]
   20358:	ldp	x22, x21, [sp, #48]
   2035c:	ldp	x24, x23, [sp, #32]
   20360:	ldr	x25, [sp, #16]
   20364:	ldp	x29, x30, [sp], #80
   20368:	ret
   2036c:	ldp	x5, x4, [x22]
   20370:	ldr	x6, [x8]
   20374:	mov	x0, x23
   20378:	mov	x1, x19
   2037c:	mov	x2, x19
   20380:	mov	x3, x21
   20384:	ldp	x20, x19, [sp, #64]
   20388:	ldp	x22, x21, [sp, #48]
   2038c:	ldp	x24, x23, [sp, #32]
   20390:	ldr	x25, [sp, #16]
   20394:	ldp	x29, x30, [sp], #80
   20398:	b	c920 <__gmpn_div_qr_2n_pi1@plt>
   2039c:	cmp	x20, #0x2a
   203a0:	b.lt	20428 <__gmpz_powm_ui@@Base+0x7d4>  // b.tstop
   203a4:	sub	x9, x21, x20
   203a8:	cmp	x9, #0x29
   203ac:	b.le	20428 <__gmpz_powm_ui@@Base+0x7d4>
   203b0:	cmp	x21, #0x7cc
   203b4:	b.lt	203f8 <__gmpz_powm_ui@@Base+0x7a4>  // b.tstop
   203b8:	cmp	x20, #0x62
   203bc:	b.lt	203f8 <__gmpz_powm_ui@@Base+0x7a4>  // b.tstop
   203c0:	mov	x9, #0x200000000000        	// #35184372088832
   203c4:	mov	x10, #0x800000000000        	// #140737488355328
   203c8:	movk	x9, #0x409c, lsl #48
   203cc:	movk	x10, #0x4058, lsl #48
   203d0:	scvtf	d0, x20
   203d4:	scvtf	d1, x21
   203d8:	fmov	d2, x9
   203dc:	fmov	d3, x10
   203e0:	fmul	d2, d0, d2
   203e4:	fmul	d3, d1, d3
   203e8:	fadd	d2, d3, d2
   203ec:	fmul	d0, d1, d0
   203f0:	fcmp	d2, d0
   203f4:	b.le	20458 <__gmpz_powm_ui@@Base+0x804>
   203f8:	mov	x0, x23
   203fc:	mov	x1, x19
   20400:	mov	x2, x21
   20404:	mov	x3, x22
   20408:	mov	x4, x20
   2040c:	ldp	x20, x19, [sp, #64]
   20410:	ldp	x22, x21, [sp, #48]
   20414:	ldp	x24, x23, [sp, #32]
   20418:	ldr	x25, [sp, #16]
   2041c:	mov	x5, x8
   20420:	ldp	x29, x30, [sp], #80
   20424:	b	c3b0 <__gmpn_dcpi1_div_qr@plt>
   20428:	ldr	x5, [x8]
   2042c:	mov	x0, x23
   20430:	mov	x1, x19
   20434:	mov	x2, x21
   20438:	mov	x3, x22
   2043c:	mov	x4, x20
   20440:	ldp	x20, x19, [sp, #64]
   20444:	ldp	x22, x21, [sp, #48]
   20448:	ldp	x24, x23, [sp, #32]
   2044c:	ldr	x25, [sp, #16]
   20450:	ldp	x29, x30, [sp], #80
   20454:	b	c640 <__gmpn_sbpi1_div_qr@plt>
   20458:	mov	x0, x21
   2045c:	mov	x1, x20
   20460:	mov	w2, wzr
   20464:	str	xzr, [x29, #24]
   20468:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   2046c:	mov	x24, x0
   20470:	lsl	x1, x20, #3
   20474:	add	x0, x29, #0x18
   20478:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2047c:	mov	x25, x0
   20480:	lsl	x1, x24, #3
   20484:	add	x0, x29, #0x18
   20488:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2048c:	mov	x6, x0
   20490:	mov	x0, x23
   20494:	mov	x1, x25
   20498:	mov	x2, x19
   2049c:	mov	x3, x21
   204a0:	mov	x4, x22
   204a4:	mov	x5, x20
   204a8:	bl	c960 <__gmpn_mu_div_qr@plt>
   204ac:	mov	x0, x19
   204b0:	mov	x1, x25
   204b4:	mov	x2, x20
   204b8:	bl	ca50 <__gmpn_copyi@plt>
   204bc:	ldr	x0, [x29, #24]
   204c0:	cbz	x0, 20354 <__gmpz_powm_ui@@Base+0x700>
   204c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   204c8:	b	20354 <__gmpz_powm_ui@@Base+0x700>

00000000000204cc <__gmpz_primorial_ui@@Base>:
   204cc:	stp	x29, x30, [sp, #-48]!
   204d0:	stp	x20, x19, [sp, #32]
   204d4:	mov	x20, x1
   204d8:	cmp	x1, #0x4
   204dc:	mov	x19, x0
   204e0:	str	x21, [sp, #16]
   204e4:	mov	x29, sp
   204e8:	b.hi	2051c <__gmpz_primorial_ui@@Base+0x50>  // b.pmore
   204ec:	ldr	w8, [x19]
   204f0:	add	w9, w20, w20, lsl #1
   204f4:	mov	w10, #0x6c89                	// #27785
   204f8:	lsr	w9, w10, w9
   204fc:	cmp	w8, #0x0
   20500:	and	w20, w9, #0x7
   20504:	b.le	20668 <__gmpz_primorial_ui@@Base+0x19c>
   20508:	ldr	x0, [x19, #8]
   2050c:	mov	w8, #0x1                   	// #1
   20510:	str	x20, [x0]
   20514:	str	w8, [x19, #4]
   20518:	b	20634 <__gmpz_primorial_ui@@Base+0x168>
   2051c:	ldrsw	x9, [x19]
   20520:	lsr	x8, x20, #6
   20524:	add	x8, x8, x20, lsr #7
   20528:	cmp	x8, x9
   2052c:	b.ge	20678 <__gmpz_primorial_ui@@Base+0x1ac>  // b.tcont
   20530:	ldr	x21, [x19, #8]
   20534:	mov	x0, x21
   20538:	mov	x1, x20
   2053c:	bl	d200 <__gmp_primesieve@plt>
   20540:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20544:	ldr	x9, [x9, #3880]
   20548:	mov	w8, #0x9                   	// #9
   2054c:	sub	w10, w8, #0x2
   20550:	ldr	x10, [x9, w10, uxtw #3]
   20554:	sub	w8, w8, #0x1
   20558:	cmp	x10, x20
   2055c:	b.cc	2054c <__gmpz_primorial_ui@@Base+0x80>  // b.lo, b.ul, b.last
   20560:	add	x9, x0, #0x1
   20564:	mov	w8, w8
   20568:	udiv	x8, x9, x8
   2056c:	lsl	x8, x8, #3
   20570:	add	x1, x8, #0x8
   20574:	mov	w8, #0x7f00                	// #32512
   20578:	cmp	x1, x8
   2057c:	str	xzr, [x29, #24]
   20580:	b.hi	2068c <__gmpz_primorial_ui@@Base+0x1c0>  // b.pmore
   20584:	add	x9, x1, #0xf
   20588:	mov	x8, sp
   2058c:	and	x9, x9, #0xfffffffffffffff0
   20590:	sub	x1, x8, x9
   20594:	mov	sp, x1
   20598:	sub	x12, x20, #0x5
   2059c:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   205a0:	movk	x13, #0xaaab
   205a4:	orr	x12, x12, #0x1
   205a8:	umulh	x12, x12, x13
   205ac:	mov	x9, xzr
   205b0:	mov	x8, xzr
   205b4:	mov	x11, #0xffffffffffffffff    	// #-1
   205b8:	mov	w14, #0x1                   	// #1
   205bc:	mov	w10, #0x6                   	// #6
   205c0:	lsr	x12, x12, #1
   205c4:	mov	w13, #0x4                   	// #4
   205c8:	b	205e8 <__gmpz_primorial_ui@@Base+0x11c>
   205cc:	mul	x10, x15, x10
   205d0:	add	x11, x11, #0x1
   205d4:	add	x9, x9, x14, lsr #63
   205d8:	ror	x14, x14, #63
   205dc:	cmp	x11, x12
   205e0:	add	x13, x13, #0x3
   205e4:	b.cs	20618 <__gmpz_primorial_ui@@Base+0x14c>  // b.hs, b.nlast
   205e8:	ldr	x15, [x21, x9, lsl #3]
   205ec:	tst	x15, x14
   205f0:	b.ne	205d0 <__gmpz_primorial_ui@@Base+0x104>  // b.any
   205f4:	and	x15, x11, #0x1
   205f8:	umulh	x16, x20, x10
   205fc:	add	x15, x13, x15
   20600:	cbz	x16, 205cc <__gmpz_primorial_ui@@Base+0x100>
   20604:	add	x16, x8, #0x1
   20608:	str	x10, [x1, x8, lsl #3]
   2060c:	mov	x10, x15
   20610:	mov	x8, x16
   20614:	b	205d0 <__gmpz_primorial_ui@@Base+0x104>
   20618:	cbz	x8, 20648 <__gmpz_primorial_ui@@Base+0x17c>
   2061c:	add	x2, x8, #0x1
   20620:	mov	x0, x19
   20624:	str	x10, [x1, x8, lsl #3]
   20628:	bl	cd70 <__gmpz_prodlimbs@plt>
   2062c:	ldr	x0, [x29, #24]
   20630:	cbnz	x0, 20660 <__gmpz_primorial_ui@@Base+0x194>
   20634:	mov	sp, x29
   20638:	ldp	x20, x19, [sp, #32]
   2063c:	ldr	x21, [sp, #16]
   20640:	ldp	x29, x30, [sp], #48
   20644:	ret
   20648:	ldr	x8, [x19, #8]
   2064c:	mov	w9, #0x1                   	// #1
   20650:	str	x10, [x8]
   20654:	str	w9, [x19, #4]
   20658:	ldr	x0, [x29, #24]
   2065c:	cbz	x0, 20634 <__gmpz_primorial_ui@@Base+0x168>
   20660:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   20664:	b	20634 <__gmpz_primorial_ui@@Base+0x168>
   20668:	mov	w1, #0x1                   	// #1
   2066c:	mov	x0, x19
   20670:	bl	c080 <__gmpz_realloc@plt>
   20674:	b	2050c <__gmpz_primorial_ui@@Base+0x40>
   20678:	add	x1, x8, #0x1
   2067c:	mov	x0, x19
   20680:	bl	c080 <__gmpz_realloc@plt>
   20684:	mov	x21, x0
   20688:	b	20534 <__gmpz_primorial_ui@@Base+0x68>
   2068c:	add	x0, x29, #0x18
   20690:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   20694:	mov	x1, x0
   20698:	b	20598 <__gmpz_primorial_ui@@Base+0xcc>

000000000002069c <__gmpz_probab_prime_p@@Base>:
   2069c:	sub	sp, sp, #0xb0
   206a0:	stp	x20, x19, [sp, #160]
   206a4:	mov	w19, w1
   206a8:	mov	w1, #0x4240                	// #16960
   206ac:	movk	w1, #0xf, lsl #16
   206b0:	stp	x29, x30, [sp, #80]
   206b4:	str	x27, [sp, #96]
   206b8:	stp	x26, x25, [sp, #112]
   206bc:	stp	x24, x23, [sp, #128]
   206c0:	stp	x22, x21, [sp, #144]
   206c4:	add	x29, sp, #0x50
   206c8:	mov	x20, x0
   206cc:	bl	d1f0 <__gmpz_cmp_ui@plt>
   206d0:	cmp	w0, #0x0
   206d4:	b.gt	20708 <__gmpz_probab_prime_p@@Base+0x6c>
   206d8:	mov	w1, #0x4240                	// #16960
   206dc:	movk	w1, #0xf, lsl #16
   206e0:	mov	x0, x20
   206e4:	bl	c100 <__gmpz_cmpabs_ui@plt>
   206e8:	cmp	w0, #0x0
   206ec:	b.le	209dc <__gmpz_probab_prime_p@@Base+0x340>
   206f0:	ldr	x8, [x20, #8]
   206f4:	stur	x8, [x29, #-8]
   206f8:	ldr	w8, [x20, #4]
   206fc:	sub	x20, x29, #0x10
   20700:	neg	w8, w8
   20704:	stur	w8, [x29, #-12]
   20708:	ldr	x22, [x20, #8]
   2070c:	ldrsw	x21, [x20, #4]
   20710:	ldr	w8, [x22]
   20714:	cmp	x21, #0x0
   20718:	cset	w9, ne  // ne = any
   2071c:	tst	w8, w9
   20720:	b.eq	20b94 <__gmpz_probab_prime_p@@Base+0x4f8>  // b.none
   20724:	mov	x2, #0x4e1d                	// #19997
   20728:	movk	x2, #0x30e9, lsl #16
   2072c:	movk	x2, #0xf97c, lsl #32
   20730:	movk	x2, #0xe221, lsl #48
   20734:	cmp	w21, #0x14
   20738:	b.le	2074c <__gmpz_probab_prime_p@@Base+0xb0>
   2073c:	mov	x0, x22
   20740:	mov	x1, x21
   20744:	bl	c3e0 <__gmpn_mod_1@plt>
   20748:	b	20768 <__gmpz_probab_prime_p@@Base+0xcc>
   2074c:	mov	x3, #0xb36b                	// #45931
   20750:	movk	x3, #0xc938, lsl #16
   20754:	movk	x3, #0xe6cf, lsl #32
   20758:	movk	x3, #0x21cf, lsl #48
   2075c:	mov	x0, x22
   20760:	mov	x1, x21
   20764:	bl	c6d0 <__gmpn_preinv_mod_1@plt>
   20768:	mov	x9, #0x521d                	// #21021
   2076c:	movk	x9, #0x8c13, lsl #16
   20770:	mov	x10, #0x304e                	// #12366
   20774:	movk	x9, #0xb2b7, lsl #32
   20778:	movk	x10, #0xcade, lsl #16
   2077c:	movk	x9, #0x21cf, lsl #48
   20780:	movk	x10, #0x873e, lsl #32
   20784:	mul	x9, x0, x9
   20788:	movk	x10, #0x4d4, lsl #48
   2078c:	mov	x8, x0
   20790:	cmp	x9, x10
   20794:	mov	w0, wzr
   20798:	b.cc	20b98 <__gmpz_probab_prime_p@@Base+0x4fc>  // b.lo, b.ul, b.last
   2079c:	mov	x9, #0x7263                	// #29283
   207a0:	movk	x9, #0x3105, lsl #16
   207a4:	movk	x9, #0x82b9, lsl #32
   207a8:	movk	x9, #0x5c98, lsl #48
   207ac:	umulh	x9, x8, x9
   207b0:	sub	x10, x8, x9
   207b4:	add	x9, x9, x10, lsr #1
   207b8:	lsr	x9, x9, #5
   207bc:	mov	w10, #0x2f                  	// #47
   207c0:	msub	x9, x9, x10, x8
   207c4:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   207c8:	mov	x9, #0xa0bf                	// #41151
   207cc:	movk	x9, #0xe82f, lsl #16
   207d0:	movk	x9, #0xfa0b, lsl #32
   207d4:	movk	x9, #0xbe82, lsl #48
   207d8:	umulh	x9, x8, x9
   207dc:	lsr	x9, x9, #5
   207e0:	mov	w10, #0x2b                  	// #43
   207e4:	msub	x9, x9, x10, x8
   207e8:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   207ec:	mov	x9, #0xce0d                	// #52749
   207f0:	movk	x9, #0xe0c7, lsl #16
   207f4:	movk	x9, #0xc7c, lsl #32
   207f8:	movk	x9, #0xc7ce, lsl #48
   207fc:	umulh	x9, x8, x9
   20800:	lsr	x9, x9, #5
   20804:	mov	w10, #0x29                  	// #41
   20808:	msub	x9, x9, x10, x8
   2080c:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20810:	mov	x9, #0x7c8b                	// #31883
   20814:	movk	x9, #0xdd6, lsl #16
   20818:	movk	x9, #0xc8a6, lsl #32
   2081c:	movk	x9, #0xdd67, lsl #48
   20820:	umulh	x9, x8, x9
   20824:	lsr	x9, x9, #5
   20828:	mov	w10, #0x25                  	// #37
   2082c:	msub	x9, x9, x10, x8
   20830:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20834:	mov	x9, #0x4211                	// #16913
   20838:	movk	x9, #0x2108, lsl #16
   2083c:	movk	x9, #0x1084, lsl #32
   20840:	movk	x9, #0x842, lsl #48
   20844:	umulh	x9, x8, x9
   20848:	sub	x10, x8, x9
   2084c:	add	x9, x9, x10, lsr #1
   20850:	lsr	x9, x9, #4
   20854:	sub	x9, x9, x9, lsl #5
   20858:	add	x9, x8, x9
   2085c:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20860:	mov	x9, #0x611b                	// #24859
   20864:	movk	x9, #0xa7b9, lsl #16
   20868:	movk	x9, #0x9611, lsl #32
   2086c:	movk	x9, #0x1a7b, lsl #48
   20870:	umulh	x9, x8, x9
   20874:	sub	x10, x8, x9
   20878:	add	x9, x9, x10, lsr #1
   2087c:	lsr	x9, x9, #4
   20880:	mov	w10, #0x1d                  	// #29
   20884:	msub	x9, x9, x10, x8
   20888:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   2088c:	mov	x9, #0x42c9                	// #17097
   20890:	movk	x9, #0xb216, lsl #16
   20894:	movk	x9, #0x8590, lsl #32
   20898:	movk	x9, #0x642c, lsl #48
   2089c:	umulh	x9, x8, x9
   208a0:	sub	x10, x8, x9
   208a4:	add	x9, x9, x10, lsr #1
   208a8:	lsr	x9, x9, #4
   208ac:	mov	w10, #0x17                  	// #23
   208b0:	msub	x9, x9, x10, x8
   208b4:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   208b8:	mov	x9, #0x435f                	// #17247
   208bc:	movk	x9, #0xd79, lsl #16
   208c0:	movk	x9, #0x35e5, lsl #32
   208c4:	movk	x9, #0xd794, lsl #48
   208c8:	umulh	x9, x8, x9
   208cc:	lsr	x9, x9, #4
   208d0:	mov	w10, #0x13                  	// #19
   208d4:	msub	x9, x9, x10, x8
   208d8:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   208dc:	mov	x9, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   208e0:	movk	x9, #0xf0f1
   208e4:	umulh	x9, x8, x9
   208e8:	lsr	x9, x9, #4
   208ec:	add	x9, x9, x9, lsl #4
   208f0:	sub	x9, x8, x9
   208f4:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   208f8:	mov	x9, #0x4ec5                	// #20165
   208fc:	movk	x9, #0xc4ec, lsl #16
   20900:	movk	x9, #0xec4e, lsl #32
   20904:	movk	x9, #0x4ec4, lsl #48
   20908:	umulh	x9, x8, x9
   2090c:	lsr	x9, x9, #2
   20910:	mov	w10, #0xd                   	// #13
   20914:	msub	x9, x9, x10, x8
   20918:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   2091c:	mov	x9, #0x8ba3                	// #35747
   20920:	movk	x9, #0xba2e, lsl #16
   20924:	movk	x9, #0xa2e8, lsl #32
   20928:	movk	x9, #0x2e8b, lsl #48
   2092c:	umulh	x9, x8, x9
   20930:	lsr	x9, x9, #1
   20934:	mov	w10, #0xb                   	// #11
   20938:	msub	x9, x9, x10, x8
   2093c:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20940:	mov	x9, #0x2493                	// #9363
   20944:	movk	x9, #0x9249, lsl #16
   20948:	movk	x9, #0x4924, lsl #32
   2094c:	movk	x9, #0x2492, lsl #48
   20950:	umulh	x9, x8, x9
   20954:	sub	x10, x8, x9
   20958:	add	x9, x9, x10, lsr #1
   2095c:	lsr	x9, x9, #2
   20960:	sub	x9, x9, x9, lsl #3
   20964:	add	x9, x8, x9
   20968:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   2096c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   20970:	movk	x9, #0xaaab
   20974:	umulh	x9, x8, x9
   20978:	lsr	x9, x9, #1
   2097c:	add	x9, x9, x9, lsl #1
   20980:	sub	x9, x8, x9
   20984:	cbz	x9, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20988:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   2098c:	movk	x9, #0xcccd
   20990:	umulh	x9, x8, x9
   20994:	lsr	x9, x9, #2
   20998:	add	x9, x9, x9, lsl #2
   2099c:	sub	x8, x8, x9
   209a0:	cbz	x8, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   209a4:	mov	w1, #0x2                   	// #2
   209a8:	mov	x0, x20
   209ac:	bl	d260 <__gmpz_sizeinbase@plt>
   209b0:	cmp	x0, #0x3c
   209b4:	b.cc	20b48 <__gmpz_probab_prime_p@@Base+0x4ac>  // b.lo, b.ul, b.last
   209b8:	mov	x23, x0
   209bc:	cmp	w21, #0x27
   209c0:	b.le	20aa8 <__gmpz_probab_prime_p@@Base+0x40c>
   209c4:	add	x24, sp, #0x4
   209c8:	mov	w27, wzr
   209cc:	mov	w25, #0x3b                  	// #59
   209d0:	sub	x26, x24, #0x4
   209d4:	mov	w2, #0x1                   	// #1
   209d8:	b	20a40 <__gmpz_probab_prime_p@@Base+0x3a4>
   209dc:	ldr	x8, [x20, #8]
   209e0:	ldr	w9, [x20, #4]
   209e4:	ldr	x8, [x8]
   209e8:	cmp	w9, #0x0
   209ec:	csel	x8, xzr, x8, eq  // eq = none
   209f0:	cmp	x8, #0x1
   209f4:	cset	w9, hi  // hi = pmore
   209f8:	tst	x8, x9
   209fc:	b.eq	20b84 <__gmpz_probab_prime_p@@Base+0x4e8>  // b.none
   20a00:	mov	w9, #0x3                   	// #3
   20a04:	udiv	x10, x8, x9
   20a08:	cmp	x10, x9
   20a0c:	b.cc	20b8c <__gmpz_probab_prime_p@@Base+0x4f0>  // b.lo, b.ul, b.last
   20a10:	mul	x10, x10, x9
   20a14:	cmp	x10, x8
   20a18:	add	x9, x9, #0x2
   20a1c:	b.ne	20a04 <__gmpz_probab_prime_p@@Base+0x368>  // b.any
   20a20:	b	20b94 <__gmpz_probab_prime_p@@Base+0x4f8>
   20a24:	mul	x2, x2, x25
   20a28:	add	w8, w27, #0x1
   20a2c:	str	w25, [x24, w27, sxtw #2]
   20a30:	mov	w27, w8
   20a34:	add	x25, x25, #0x2
   20a38:	cmp	x25, x23
   20a3c:	b.cs	20b48 <__gmpz_probab_prime_p@@Base+0x4ac>  // b.hs, b.nlast
   20a40:	mov	w8, #0x3                   	// #3
   20a44:	udiv	x9, x25, x8
   20a48:	cmp	x9, x8
   20a4c:	b.cc	20a64 <__gmpz_probab_prime_p@@Base+0x3c8>  // b.lo, b.ul, b.last
   20a50:	mul	x9, x9, x8
   20a54:	cmp	x9, x25
   20a58:	add	x8, x8, #0x2
   20a5c:	b.ne	20a44 <__gmpz_probab_prime_p@@Base+0x3a8>  // b.any
   20a60:	b	20a34 <__gmpz_probab_prime_p@@Base+0x398>
   20a64:	umulh	x8, x2, x25
   20a68:	cbz	x8, 20a24 <__gmpz_probab_prime_p@@Base+0x388>
   20a6c:	mov	x0, x22
   20a70:	mov	x1, x21
   20a74:	bl	c3e0 <__gmpn_mod_1@plt>
   20a78:	sxtw	x9, w27
   20a7c:	subs	x10, x9, #0x1
   20a80:	b.lt	20a9c <__gmpz_probab_prime_p@@Base+0x400>  // b.tstop
   20a84:	ldr	w8, [x26, x9, lsl #2]
   20a88:	udiv	x9, x0, x8
   20a8c:	msub	x11, x9, x8, x0
   20a90:	mov	x9, x10
   20a94:	cbnz	x11, 20a7c <__gmpz_probab_prime_p@@Base+0x3e0>
   20a98:	b	20b58 <__gmpz_probab_prime_p@@Base+0x4bc>
   20a9c:	mov	w27, wzr
   20aa0:	mov	x2, x25
   20aa4:	b	20a28 <__gmpz_probab_prime_p@@Base+0x38c>
   20aa8:	add	x24, sp, #0x4
   20aac:	mov	w27, wzr
   20ab0:	mov	w25, #0x3b                  	// #59
   20ab4:	sub	x26, x24, #0x4
   20ab8:	mov	w2, #0x1                   	// #1
   20abc:	b	20adc <__gmpz_probab_prime_p@@Base+0x440>
   20ac0:	mul	x2, x2, x25
   20ac4:	add	w8, w27, #0x1
   20ac8:	str	w25, [x24, w27, sxtw #2]
   20acc:	mov	w27, w8
   20ad0:	add	x25, x25, #0x2
   20ad4:	cmp	x25, x23
   20ad8:	b.cs	20b48 <__gmpz_probab_prime_p@@Base+0x4ac>  // b.hs, b.nlast
   20adc:	mov	w8, #0x3                   	// #3
   20ae0:	udiv	x9, x25, x8
   20ae4:	cmp	x9, x8
   20ae8:	b.cc	20b00 <__gmpz_probab_prime_p@@Base+0x464>  // b.lo, b.ul, b.last
   20aec:	mul	x9, x9, x8
   20af0:	cmp	x9, x25
   20af4:	add	x8, x8, #0x2
   20af8:	b.ne	20ae0 <__gmpz_probab_prime_p@@Base+0x444>  // b.any
   20afc:	b	20ad0 <__gmpz_probab_prime_p@@Base+0x434>
   20b00:	umulh	x8, x2, x25
   20b04:	cbz	x8, 20ac0 <__gmpz_probab_prime_p@@Base+0x424>
   20b08:	mov	x0, x22
   20b0c:	mov	x1, x21
   20b10:	mov	x3, xzr
   20b14:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   20b18:	sxtw	x9, w27
   20b1c:	subs	x10, x9, #0x1
   20b20:	b.lt	20b3c <__gmpz_probab_prime_p@@Base+0x4a0>  // b.tstop
   20b24:	ldr	w8, [x26, x9, lsl #2]
   20b28:	udiv	x9, x0, x8
   20b2c:	msub	x11, x9, x8, x0
   20b30:	mov	x9, x10
   20b34:	cbnz	x11, 20b1c <__gmpz_probab_prime_p@@Base+0x480>
   20b38:	b	20b58 <__gmpz_probab_prime_p@@Base+0x4bc>
   20b3c:	mov	w27, wzr
   20b40:	mov	x2, x25
   20b44:	b	20ac4 <__gmpz_probab_prime_p@@Base+0x428>
   20b48:	mov	x0, x20
   20b4c:	mov	w1, w19
   20b50:	bl	cb80 <__gmpz_millerrabin@plt>
   20b54:	b	20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20b58:	mov	w2, w8
   20b5c:	mov	x0, x22
   20b60:	mov	x1, x21
   20b64:	bl	c3e0 <__gmpn_mod_1@plt>
   20b68:	cbz	x0, 20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20b6c:	adrp	x0, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   20b70:	adrp	x2, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   20b74:	add	x0, x0, #0x1d7
   20b78:	add	x2, x2, #0x1e2
   20b7c:	mov	w1, #0x83                  	// #131
   20b80:	bl	c6c0 <__gmp_assert_fail@plt>
   20b84:	cmp	x8, #0x2
   20b88:	b.ne	20b94 <__gmpz_probab_prime_p@@Base+0x4f8>  // b.any
   20b8c:	mov	w0, #0x2                   	// #2
   20b90:	b	20b98 <__gmpz_probab_prime_p@@Base+0x4fc>
   20b94:	mov	w0, wzr
   20b98:	ldp	x20, x19, [sp, #160]
   20b9c:	ldp	x22, x21, [sp, #144]
   20ba0:	ldp	x24, x23, [sp, #128]
   20ba4:	ldp	x26, x25, [sp, #112]
   20ba8:	ldr	x27, [sp, #96]
   20bac:	ldp	x29, x30, [sp, #80]
   20bb0:	add	sp, sp, #0xb0
   20bb4:	ret

0000000000020bb8 <__gmpz_random@@Base>:
   20bb8:	stp	x29, x30, [sp, #-32]!
   20bbc:	stp	x20, x19, [sp, #16]
   20bc0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20bc4:	ldr	x8, [x8, #4040]
   20bc8:	mov	x20, x1
   20bcc:	mov	x19, x0
   20bd0:	mov	x29, sp
   20bd4:	ldrb	w9, [x8]
   20bd8:	cbnz	w9, 20bf0 <__gmpz_random@@Base+0x38>
   20bdc:	mov	w9, #0x1                   	// #1
   20be0:	strb	w9, [x8]
   20be4:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20be8:	ldr	x0, [x0, #3976]
   20bec:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   20bf0:	adrp	x1, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20bf4:	ldr	x1, [x1, #3976]
   20bf8:	cmp	x20, #0x0
   20bfc:	cneg	x8, x20, mi  // mi = first
   20c00:	lsl	x2, x8, #6
   20c04:	mov	x0, x19
   20c08:	bl	d430 <__gmpz_urandomb@plt>
   20c0c:	tbnz	x20, #63, 20c1c <__gmpz_random@@Base+0x64>
   20c10:	ldp	x20, x19, [sp, #16]
   20c14:	ldp	x29, x30, [sp], #32
   20c18:	ret
   20c1c:	ldr	w8, [x19, #4]
   20c20:	neg	w8, w8
   20c24:	str	w8, [x19, #4]
   20c28:	ldp	x20, x19, [sp, #16]
   20c2c:	ldp	x29, x30, [sp], #32
   20c30:	ret

0000000000020c34 <__gmpz_random2@@Base>:
   20c34:	stp	x29, x30, [sp, #-48]!
   20c38:	cmp	x1, #0x0
   20c3c:	str	x21, [sp, #16]
   20c40:	stp	x20, x19, [sp, #32]
   20c44:	mov	x19, x1
   20c48:	mov	x20, x0
   20c4c:	cneg	x21, x1, mi  // mi = first
   20c50:	mov	x29, sp
   20c54:	cbz	x1, 20c70 <__gmpz_random2@@Base+0x3c>
   20c58:	ldrsw	x8, [x20]
   20c5c:	cmp	x21, x8
   20c60:	b.gt	20c84 <__gmpz_random2@@Base+0x50>
   20c64:	ldr	x0, [x20, #8]
   20c68:	mov	x1, x21
   20c6c:	bl	d1c0 <__gmpn_random2@plt>
   20c70:	str	w19, [x20, #4]
   20c74:	ldp	x20, x19, [sp, #32]
   20c78:	ldr	x21, [sp, #16]
   20c7c:	ldp	x29, x30, [sp], #48
   20c80:	ret
   20c84:	mov	x0, x20
   20c88:	mov	x1, x21
   20c8c:	bl	c080 <__gmpz_realloc@plt>
   20c90:	b	20c68 <__gmpz_random2@@Base+0x34>

0000000000020c94 <__gmpz_realloc@@Base>:
   20c94:	stp	x29, x30, [sp, #-32]!
   20c98:	cmp	x1, #0x1
   20c9c:	stp	x20, x19, [sp, #16]
   20ca0:	csinc	x20, x1, xzr, gt
   20ca4:	mov	w8, #0x80000000            	// #-2147483648
   20ca8:	cmp	x20, x8
   20cac:	mov	x29, sp
   20cb0:	b.ge	20d20 <__gmpz_realloc@@Base+0x8c>  // b.tcont
   20cb4:	ldrsw	x8, [x0]
   20cb8:	mov	x19, x0
   20cbc:	cbz	w8, 20cf8 <__gmpz_realloc@@Base+0x64>
   20cc0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20cc4:	ldr	x9, [x9, #3792]
   20cc8:	ldr	x0, [x19, #8]
   20ccc:	lsl	x1, x8, #3
   20cd0:	lsl	x2, x20, #3
   20cd4:	ldr	x9, [x9]
   20cd8:	blr	x9
   20cdc:	ldr	w8, [x19, #4]
   20ce0:	cmp	w8, #0x0
   20ce4:	cneg	w8, w8, mi  // mi = first
   20ce8:	cmp	x20, x8
   20cec:	b.ge	20d0c <__gmpz_realloc@@Base+0x78>  // b.tcont
   20cf0:	str	wzr, [x19, #4]
   20cf4:	b	20d0c <__gmpz_realloc@@Base+0x78>
   20cf8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20cfc:	ldr	x8, [x8, #3840]
   20d00:	lsl	x0, x20, #3
   20d04:	ldr	x8, [x8]
   20d08:	blr	x8
   20d0c:	str	x0, [x19, #8]
   20d10:	str	w20, [x19]
   20d14:	ldp	x20, x19, [sp, #16]
   20d18:	ldp	x29, x30, [sp], #32
   20d1c:	ret
   20d20:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20d24:	ldr	x8, [x8, #3824]
   20d28:	adrp	x0, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   20d2c:	add	x0, x0, #0xf8
   20d30:	mov	w1, #0x1a                  	// #26
   20d34:	ldr	x3, [x8]
   20d38:	mov	w2, #0x1                   	// #1
   20d3c:	bl	ce30 <fwrite@plt>
   20d40:	bl	c900 <abort@plt>

0000000000020d44 <__gmpz_realloc2@@Base>:
   20d44:	stp	x29, x30, [sp, #-32]!
   20d48:	cmp	x1, #0x0
   20d4c:	cset	w8, ne  // ne = any
   20d50:	sub	x8, x1, x8
   20d54:	mov	x9, #0x1fffffffc0          	// #137438953408
   20d58:	cmp	x8, x9
   20d5c:	stp	x20, x19, [sp, #16]
   20d60:	mov	x29, sp
   20d64:	b.cs	20dec <__gmpz_realloc2@@Base+0xa8>  // b.hs, b.nlast
   20d68:	ldrsw	x9, [x0]
   20d6c:	lsr	x8, x8, #6
   20d70:	mov	x19, x0
   20d74:	add	x20, x8, #0x1
   20d78:	cbz	w9, 20dc4 <__gmpz_realloc2@@Base+0x80>
   20d7c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20d80:	ldr	x8, [x8, #3792]
   20d84:	ldr	x0, [x19, #8]
   20d88:	lsl	x1, x9, #3
   20d8c:	lsl	x2, x20, #3
   20d90:	ldr	x8, [x8]
   20d94:	blr	x8
   20d98:	ldr	w8, [x19, #4]
   20d9c:	str	x0, [x19, #8]
   20da0:	cmp	w8, #0x0
   20da4:	cneg	w8, w8, mi  // mi = first
   20da8:	cmp	x20, x8
   20dac:	b.cs	20db4 <__gmpz_realloc2@@Base+0x70>  // b.hs, b.nlast
   20db0:	str	wzr, [x19, #4]
   20db4:	str	w20, [x19]
   20db8:	ldp	x20, x19, [sp, #16]
   20dbc:	ldp	x29, x30, [sp], #32
   20dc0:	ret
   20dc4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20dc8:	ldr	x8, [x8, #3840]
   20dcc:	lsl	x0, x20, #3
   20dd0:	ldr	x8, [x8]
   20dd4:	blr	x8
   20dd8:	str	x0, [x19, #8]
   20ddc:	str	w20, [x19]
   20de0:	ldp	x20, x19, [sp, #16]
   20de4:	ldp	x29, x30, [sp], #32
   20de8:	ret
   20dec:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   20df0:	ldr	x8, [x8, #3824]
   20df4:	adrp	x0, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   20df8:	add	x0, x0, #0xf8
   20dfc:	mov	w1, #0x1a                  	// #26
   20e00:	ldr	x3, [x8]
   20e04:	mov	w2, #0x1                   	// #1
   20e08:	bl	ce30 <fwrite@plt>
   20e0c:	bl	c900 <abort@plt>

0000000000020e10 <__gmpz_remove@@Base>:
   20e10:	stp	x29, x30, [sp, #-80]!
   20e14:	stp	x28, x25, [sp, #16]
   20e18:	stp	x24, x23, [sp, #32]
   20e1c:	stp	x22, x21, [sp, #48]
   20e20:	stp	x20, x19, [sp, #64]
   20e24:	mov	x29, sp
   20e28:	sub	sp, sp, #0x420
   20e2c:	ldr	x4, [x2, #8]
   20e30:	ldrsw	x9, [x2, #4]
   20e34:	ldrsw	x25, [x1, #4]
   20e38:	mov	x21, x1
   20e3c:	ldr	x8, [x4]
   20e40:	cmp	x9, #0x0
   20e44:	cneg	x22, x9, mi  // mi = first
   20e48:	mov	x19, x0
   20e4c:	cmp	x8, #0x1
   20e50:	cset	w10, eq  // eq = none
   20e54:	cbz	w25, 210c0 <__gmpz_remove@@Base+0x2b0>
   20e58:	cmp	x22, x10
   20e5c:	b.le	210c0 <__gmpz_remove@@Base+0x2b0>
   20e60:	mov	x20, x2
   20e64:	and	x24, x9, #0xffffffff
   20e68:	tbnz	w8, #0, 20eb8 <__gmpz_remove@@Base+0xa8>
   20e6c:	cmp	x8, #0x2
   20e70:	cset	w8, eq  // eq = none
   20e74:	cmp	x22, x8
   20e78:	b.ne	20f10 <__gmpz_remove@@Base+0x100>  // b.any
   20e7c:	mov	x0, x21
   20e80:	mov	x1, xzr
   20e84:	bl	bf20 <__gmpz_scan1@plt>
   20e88:	mov	x20, x0
   20e8c:	mov	x0, x19
   20e90:	mov	x1, x21
   20e94:	mov	x2, x20
   20e98:	bl	c650 <__gmpz_fdiv_q_2exp@plt>
   20e9c:	ubfx	x8, x24, #31, #1
   20ea0:	tst	x20, x8
   20ea4:	b.eq	20f5c <__gmpz_remove@@Base+0x14c>  // b.none
   20ea8:	ldr	w8, [x19, #4]
   20eac:	neg	w8, w8
   20eb0:	str	w8, [x19, #4]
   20eb4:	b	20f5c <__gmpz_remove@@Base+0x14c>
   20eb8:	cmp	x25, #0x0
   20ebc:	cneg	x23, x25, mi  // mi = first
   20ec0:	str	x23, [sp]
   20ec4:	ldrsw	x8, [x19]
   20ec8:	cmp	x23, x8
   20ecc:	b.gt	210d8 <__gmpz_remove@@Base+0x2c8>
   20ed0:	ldr	x0, [x19, #8]
   20ed4:	ldr	x2, [x21, #8]
   20ed8:	mov	x1, sp
   20edc:	mov	x6, #0xffffffffffffffff    	// #-1
   20ee0:	mov	x3, x23
   20ee4:	mov	x5, x22
   20ee8:	bl	cd40 <__gmpn_remove@plt>
   20eec:	ldr	x8, [sp]
   20ef0:	and	x9, x0, x24, lsr #31
   20ef4:	ubfx	x10, x25, #31, #1
   20ef8:	cmp	x9, x10
   20efc:	neg	w11, w8
   20f00:	csel	x8, x8, x11, eq  // eq = none
   20f04:	mov	x20, x0
   20f08:	str	w8, [x19, #4]
   20f0c:	b	20f5c <__gmpz_remove@@Base+0x14c>
   20f10:	sub	x0, x29, #0x20
   20f14:	bl	d250 <__gmpz_init@plt>
   20f18:	sub	x0, x29, #0x10
   20f1c:	bl	d250 <__gmpz_init@plt>
   20f20:	sub	x0, x29, #0x10
   20f24:	sub	x1, x29, #0x20
   20f28:	mov	x2, x21
   20f2c:	mov	x3, x20
   20f30:	bl	bff0 <__gmpz_tdiv_qr@plt>
   20f34:	ldur	w8, [x29, #-28]
   20f38:	cbz	w8, 20f7c <__gmpz_remove@@Base+0x16c>
   20f3c:	mov	x0, x19
   20f40:	mov	x1, x21
   20f44:	bl	c420 <__gmpz_set@plt>
   20f48:	mov	x20, xzr
   20f4c:	sub	x0, x29, #0x10
   20f50:	bl	cb50 <__gmpz_clear@plt>
   20f54:	sub	x0, x29, #0x20
   20f58:	bl	cb50 <__gmpz_clear@plt>
   20f5c:	mov	x0, x20
   20f60:	add	sp, sp, #0x420
   20f64:	ldp	x20, x19, [sp, #64]
   20f68:	ldp	x22, x21, [sp, #48]
   20f6c:	ldp	x24, x23, [sp, #32]
   20f70:	ldp	x28, x25, [sp, #16]
   20f74:	ldp	x29, x30, [sp], #80
   20f78:	ret
   20f7c:	mov	x0, sp
   20f80:	mov	x1, x20
   20f84:	mov	x22, sp
   20f88:	bl	bf80 <__gmpz_init_set@plt>
   20f8c:	sub	x1, x29, #0x10
   20f90:	mov	x0, x19
   20f94:	bl	c580 <__gmpz_swap@plt>
   20f98:	ldr	w8, [x19, #4]
   20f9c:	ldr	w9, [sp, #4]
   20fa0:	cmp	w8, #0x0
   20fa4:	cneg	w8, w8, mi  // mi = first
   20fa8:	cmp	w9, #0x0
   20fac:	cneg	w9, w9, mi  // mi = first
   20fb0:	lsl	w9, w9, #1
   20fb4:	sub	w9, w9, #0x1
   20fb8:	cmp	w8, w9
   20fbc:	b.ge	20fc8 <__gmpz_remove@@Base+0x1b8>  // b.tcont
   20fc0:	mov	w23, #0x1                   	// #1
   20fc4:	b	21050 <__gmpz_remove@@Base+0x240>
   20fc8:	add	x20, x22, #0x10
   20fcc:	mov	w23, #0x1                   	// #1
   20fd0:	mov	x0, x20
   20fd4:	sub	x21, x20, #0x10
   20fd8:	bl	d250 <__gmpz_init@plt>
   20fdc:	mov	x0, x20
   20fe0:	mov	x1, x21
   20fe4:	mov	x2, x21
   20fe8:	bl	c4b0 <__gmpz_mul@plt>
   20fec:	sub	x0, x29, #0x10
   20ff0:	sub	x1, x29, #0x20
   20ff4:	mov	x2, x19
   20ff8:	mov	x3, x20
   20ffc:	bl	bff0 <__gmpz_tdiv_qr@plt>
   21000:	ldur	w8, [x29, #-28]
   21004:	cbnz	w8, 21048 <__gmpz_remove@@Base+0x238>
   21008:	sub	x1, x29, #0x10
   2100c:	mov	x0, x19
   21010:	bl	c580 <__gmpz_swap@plt>
   21014:	ldr	w8, [x19, #4]
   21018:	ldr	w9, [x20, #4]
   2101c:	add	w23, w23, #0x1
   21020:	add	x20, x20, #0x10
   21024:	cmp	w8, #0x0
   21028:	cneg	w8, w8, mi  // mi = first
   2102c:	cmp	w9, #0x0
   21030:	cneg	w9, w9, mi  // mi = first
   21034:	lsl	w9, w9, #1
   21038:	sub	w9, w9, #0x1
   2103c:	cmp	w8, w9
   21040:	b.ge	20fd0 <__gmpz_remove@@Base+0x1c0>  // b.tcont
   21044:	b	21050 <__gmpz_remove@@Base+0x240>
   21048:	mov	x0, x20
   2104c:	bl	cb50 <__gmpz_clear@plt>
   21050:	mov	x8, #0xffffffffffffffff    	// #-1
   21054:	sub	w25, w23, #0x1
   21058:	lsl	x8, x8, x23
   2105c:	add	w24, w23, #0x1
   21060:	add	x21, x22, w25, sxtw #4
   21064:	mvn	x20, x8
   21068:	mov	w22, #0x1                   	// #1
   2106c:	b	2108c <__gmpz_remove@@Base+0x27c>
   21070:	mov	x0, x21
   21074:	bl	cb50 <__gmpz_clear@plt>
   21078:	sub	w24, w24, #0x1
   2107c:	sub	x21, x21, #0x10
   21080:	cmp	w24, #0x1
   21084:	sub	x25, x25, #0x1
   21088:	b.le	20f4c <__gmpz_remove@@Base+0x13c>
   2108c:	sub	x0, x29, #0x10
   21090:	sub	x1, x29, #0x20
   21094:	mov	x2, x19
   21098:	mov	x3, x21
   2109c:	bl	bff0 <__gmpz_tdiv_qr@plt>
   210a0:	ldur	w8, [x29, #-28]
   210a4:	cbnz	w8, 21070 <__gmpz_remove@@Base+0x260>
   210a8:	lsl	x8, x22, x25
   210ac:	sub	x1, x29, #0x10
   210b0:	mov	x0, x19
   210b4:	add	x20, x8, x20
   210b8:	bl	c580 <__gmpz_swap@plt>
   210bc:	b	21070 <__gmpz_remove@@Base+0x260>
   210c0:	cbz	x22, 210ec <__gmpz_remove@@Base+0x2dc>
   210c4:	mov	x0, x19
   210c8:	mov	x1, x21
   210cc:	bl	c420 <__gmpz_set@plt>
   210d0:	mov	x20, xzr
   210d4:	b	20f5c <__gmpz_remove@@Base+0x14c>
   210d8:	mov	x0, x19
   210dc:	mov	x1, x23
   210e0:	bl	c080 <__gmpz_realloc@plt>
   210e4:	ldr	x4, [x20, #8]
   210e8:	b	20ed4 <__gmpz_remove@@Base+0xc4>
   210ec:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000210f0 <__gmpz_roinit_n@@Base>:
   210f0:	cmp	x2, #0x0
   210f4:	cneg	x9, x2, mi  // mi = first
   210f8:	mov	x8, x9
   210fc:	subs	x9, x9, #0x1
   21100:	b.lt	21110 <__gmpz_roinit_n@@Base+0x20>  // b.tstop
   21104:	add	x10, x1, x8, lsl #3
   21108:	ldur	x10, [x10, #-8]
   2110c:	cbz	x10, 210f8 <__gmpz_roinit_n@@Base+0x8>
   21110:	neg	w9, w8
   21114:	cmp	x2, #0x0
   21118:	csel	x8, x9, x8, lt  // lt = tstop
   2111c:	stp	wzr, w8, [x0]
   21120:	str	x1, [x0, #8]
   21124:	ret

0000000000021128 <__gmpz_root@@Base>:
   21128:	stp	x29, x30, [sp, #-96]!
   2112c:	stp	x26, x25, [sp, #32]
   21130:	stp	x24, x23, [sp, #48]
   21134:	stp	x22, x21, [sp, #64]
   21138:	stp	x20, x19, [sp, #80]
   2113c:	ldr	w8, [x1, #4]
   21140:	mov	x22, x2
   21144:	mov	x20, x1
   21148:	mov	x19, x0
   2114c:	str	x27, [sp, #16]
   21150:	mov	x29, sp
   21154:	tbnz	w22, #0, 2115c <__gmpz_root@@Base+0x34>
   21158:	tbnz	w8, #31, 212bc <__gmpz_root@@Base+0x194>
   2115c:	cbz	x22, 212c0 <__gmpz_root@@Base+0x198>
   21160:	sxtw	x26, w8
   21164:	cbz	w26, 211b0 <__gmpz_root@@Base+0x88>
   21168:	cmp	w26, #0x0
   2116c:	cneg	x23, x26, lt  // lt = tstop
   21170:	sub	x8, x23, #0x1
   21174:	udiv	x27, x8, x22
   21178:	cmp	x20, x19
   2117c:	add	x21, x27, #0x1
   21180:	str	xzr, [x29, #24]
   21184:	b.eq	211c0 <__gmpz_root@@Base+0x98>  // b.none
   21188:	cbz	x19, 211c0 <__gmpz_root@@Base+0x98>
   2118c:	ldrsw	x8, [x19]
   21190:	cmp	x21, x8
   21194:	b.gt	21214 <__gmpz_root@@Base+0xec>
   21198:	ldr	x24, [x19, #8]
   2119c:	ldr	x25, [x20, #8]
   211a0:	mov	x0, x24
   211a4:	cmp	x22, #0x1
   211a8:	b.eq	211f4 <__gmpz_root@@Base+0xcc>  // b.none
   211ac:	b	21240 <__gmpz_root@@Base+0x118>
   211b0:	cbz	x19, 2120c <__gmpz_root@@Base+0xe4>
   211b4:	str	wzr, [x19, #4]
   211b8:	mov	w0, #0x1                   	// #1
   211bc:	b	21294 <__gmpz_root@@Base+0x16c>
   211c0:	lsl	x1, x21, #3
   211c4:	mov	w8, #0x7f00                	// #32512
   211c8:	cmp	x1, x8
   211cc:	b.hi	21224 <__gmpz_root@@Base+0xfc>  // b.pmore
   211d0:	add	x9, x1, #0xf
   211d4:	mov	x8, sp
   211d8:	and	x9, x9, #0xfffffffffffffff0
   211dc:	sub	x24, x8, x9
   211e0:	mov	sp, x24
   211e4:	ldr	x25, [x20, #8]
   211e8:	mov	x0, x24
   211ec:	cmp	x22, #0x1
   211f0:	b.ne	21240 <__gmpz_root@@Base+0x118>  // b.any
   211f4:	mov	x1, x25
   211f8:	mov	x2, x23
   211fc:	bl	ca50 <__gmpn_copyi@plt>
   21200:	mov	x22, xzr
   21204:	cbnz	x19, 2125c <__gmpz_root@@Base+0x134>
   21208:	b	21284 <__gmpz_root@@Base+0x15c>
   2120c:	mov	w0, #0x1                   	// #1
   21210:	b	21294 <__gmpz_root@@Base+0x16c>
   21214:	mov	x0, x19
   21218:	mov	x1, x21
   2121c:	bl	c080 <__gmpz_realloc@plt>
   21220:	b	2122c <__gmpz_root@@Base+0x104>
   21224:	add	x0, x29, #0x18
   21228:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2122c:	mov	x24, x0
   21230:	ldr	x25, [x20, #8]
   21234:	mov	x0, x24
   21238:	cmp	x22, #0x1
   2123c:	b.eq	211f4 <__gmpz_root@@Base+0xcc>  // b.none
   21240:	mov	x1, xzr
   21244:	mov	x2, x25
   21248:	mov	x3, x23
   2124c:	mov	x4, x22
   21250:	bl	c2a0 <__gmpn_rootrem@plt>
   21254:	mov	x22, x0
   21258:	cbz	x19, 21284 <__gmpz_root@@Base+0x15c>
   2125c:	mvn	w8, w27
   21260:	cmp	w26, #0x0
   21264:	csel	x8, x21, x8, ge  // ge = tcont
   21268:	cmp	x20, x19
   2126c:	str	w8, [x19, #4]
   21270:	b.ne	21284 <__gmpz_root@@Base+0x15c>  // b.any
   21274:	mov	x0, x25
   21278:	mov	x1, x24
   2127c:	mov	x2, x21
   21280:	bl	ca50 <__gmpn_copyi@plt>
   21284:	ldr	x0, [x29, #24]
   21288:	cbnz	x0, 212b4 <__gmpz_root@@Base+0x18c>
   2128c:	cmp	x22, #0x0
   21290:	cset	w0, eq  // eq = none
   21294:	mov	sp, x29
   21298:	ldp	x20, x19, [sp, #80]
   2129c:	ldp	x22, x21, [sp, #64]
   212a0:	ldp	x24, x23, [sp, #48]
   212a4:	ldp	x26, x25, [sp, #32]
   212a8:	ldr	x27, [sp, #16]
   212ac:	ldp	x29, x30, [sp], #96
   212b0:	ret
   212b4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   212b8:	b	2128c <__gmpz_root@@Base+0x164>
   212bc:	bl	cff0 <__gmp_sqrt_of_negative@plt>
   212c0:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000212c4 <__gmpz_rootrem@@Base>:
   212c4:	stp	x29, x30, [sp, #-96]!
   212c8:	stp	x28, x27, [sp, #16]
   212cc:	stp	x26, x25, [sp, #32]
   212d0:	stp	x24, x23, [sp, #48]
   212d4:	stp	x22, x21, [sp, #64]
   212d8:	stp	x20, x19, [sp, #80]
   212dc:	mov	x29, sp
   212e0:	sub	sp, sp, #0x10
   212e4:	ldr	w8, [x2, #4]
   212e8:	mov	x23, x3
   212ec:	mov	x20, x2
   212f0:	mov	x19, x1
   212f4:	mov	x21, x0
   212f8:	tbnz	w23, #0, 21300 <__gmpz_rootrem@@Base+0x3c>
   212fc:	tbnz	w8, #31, 214ec <__gmpz_rootrem@@Base+0x228>
   21300:	cbz	x23, 214f0 <__gmpz_rootrem@@Base+0x22c>
   21304:	sxtw	x28, w8
   21308:	cbz	w28, 2134c <__gmpz_rootrem@@Base+0x88>
   2130c:	cmp	w28, #0x0
   21310:	cneg	x24, x28, lt  // lt = tstop
   21314:	sub	x8, x24, #0x1
   21318:	udiv	x22, x8, x23
   2131c:	cmp	x20, x21
   21320:	add	x1, x22, #0x1
   21324:	stp	x1, xzr, [x29, #-16]
   21328:	b.eq	2135c <__gmpz_rootrem@@Base+0x98>  // b.none
   2132c:	cbz	x21, 2135c <__gmpz_rootrem@@Base+0x98>
   21330:	ldrsw	x8, [x21]
   21334:	cmp	x1, x8
   21338:	b.gt	213bc <__gmpz_rootrem@@Base+0xf8>
   2133c:	ldr	x25, [x21, #8]
   21340:	cmp	x20, x19
   21344:	b.ne	21388 <__gmpz_rootrem@@Base+0xc4>  // b.any
   21348:	b	213dc <__gmpz_rootrem@@Base+0x118>
   2134c:	cbz	x21, 21354 <__gmpz_rootrem@@Base+0x90>
   21350:	str	wzr, [x21, #4]
   21354:	str	wzr, [x19, #4]
   21358:	b	214ac <__gmpz_rootrem@@Base+0x1e8>
   2135c:	lsl	x1, x1, #3
   21360:	mov	w8, #0x7f00                	// #32512
   21364:	cmp	x1, x8
   21368:	b.hi	213c8 <__gmpz_rootrem@@Base+0x104>  // b.pmore
   2136c:	add	x9, x1, #0xf
   21370:	mov	x8, sp
   21374:	and	x9, x9, #0xfffffffffffffff0
   21378:	sub	x25, x8, x9
   2137c:	mov	sp, x25
   21380:	cmp	x20, x19
   21384:	b.eq	213dc <__gmpz_rootrem@@Base+0x118>  // b.none
   21388:	ldrsw	x8, [x19]
   2138c:	cmp	x24, x8
   21390:	b.gt	213ac <__gmpz_rootrem@@Base+0xe8>
   21394:	ldr	x26, [x19, #8]
   21398:	ldr	x27, [x20, #8]
   2139c:	mov	x0, x25
   213a0:	cmp	x23, #0x1
   213a4:	b.eq	2140c <__gmpz_rootrem@@Base+0x148>  // b.none
   213a8:	b	21444 <__gmpz_rootrem@@Base+0x180>
   213ac:	mov	x0, x19
   213b0:	mov	x1, x24
   213b4:	bl	c080 <__gmpz_realloc@plt>
   213b8:	b	21430 <__gmpz_rootrem@@Base+0x16c>
   213bc:	mov	x0, x21
   213c0:	bl	c080 <__gmpz_realloc@plt>
   213c4:	b	213d0 <__gmpz_rootrem@@Base+0x10c>
   213c8:	sub	x0, x29, #0x8
   213cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   213d0:	mov	x25, x0
   213d4:	cmp	x20, x19
   213d8:	b.ne	21388 <__gmpz_rootrem@@Base+0xc4>  // b.any
   213dc:	cmp	x24, #0xfe0
   213e0:	lsl	x1, x24, #3
   213e4:	b.hi	21428 <__gmpz_rootrem@@Base+0x164>  // b.pmore
   213e8:	add	x9, x1, #0xf
   213ec:	mov	x8, sp
   213f0:	and	x9, x9, #0xfffffffffffffff0
   213f4:	sub	x26, x8, x9
   213f8:	mov	sp, x26
   213fc:	ldr	x27, [x20, #8]
   21400:	mov	x0, x25
   21404:	cmp	x23, #0x1
   21408:	b.ne	21444 <__gmpz_rootrem@@Base+0x180>  // b.any
   2140c:	mov	x1, x27
   21410:	mov	x2, x24
   21414:	bl	ca50 <__gmpn_copyi@plt>
   21418:	mov	x23, xzr
   2141c:	ldur	x2, [x29, #-16]
   21420:	cbnz	x21, 21464 <__gmpz_rootrem@@Base+0x1a0>
   21424:	b	2147c <__gmpz_rootrem@@Base+0x1b8>
   21428:	sub	x0, x29, #0x8
   2142c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   21430:	mov	x26, x0
   21434:	ldr	x27, [x20, #8]
   21438:	mov	x0, x25
   2143c:	cmp	x23, #0x1
   21440:	b.eq	2140c <__gmpz_rootrem@@Base+0x148>  // b.none
   21444:	mov	x1, x26
   21448:	mov	x2, x27
   2144c:	mov	x3, x24
   21450:	mov	x4, x23
   21454:	bl	c2a0 <__gmpn_rootrem@plt>
   21458:	mov	x23, x0
   2145c:	ldur	x2, [x29, #-16]
   21460:	cbz	x21, 2147c <__gmpz_rootrem@@Base+0x1b8>
   21464:	mvn	w8, w22
   21468:	cmp	w28, #0x0
   2146c:	csel	x8, x2, x8, ge  // ge = tcont
   21470:	cmp	x20, x21
   21474:	str	w8, [x21, #4]
   21478:	b.eq	214cc <__gmpz_rootrem@@Base+0x208>  // b.none
   2147c:	cmp	x20, x19
   21480:	b.ne	21494 <__gmpz_rootrem@@Base+0x1d0>  // b.any
   21484:	mov	x0, x27
   21488:	mov	x1, x26
   2148c:	mov	x2, x23
   21490:	bl	ca50 <__gmpn_copyi@plt>
   21494:	neg	w8, w23
   21498:	cmp	w28, #0x0
   2149c:	csel	x8, x23, x8, ge  // ge = tcont
   214a0:	str	w8, [x19, #4]
   214a4:	ldur	x0, [x29, #-8]
   214a8:	cbnz	x0, 214e4 <__gmpz_rootrem@@Base+0x220>
   214ac:	mov	sp, x29
   214b0:	ldp	x20, x19, [sp, #80]
   214b4:	ldp	x22, x21, [sp, #64]
   214b8:	ldp	x24, x23, [sp, #48]
   214bc:	ldp	x26, x25, [sp, #32]
   214c0:	ldp	x28, x27, [sp, #16]
   214c4:	ldp	x29, x30, [sp], #96
   214c8:	ret
   214cc:	mov	x0, x27
   214d0:	mov	x1, x25
   214d4:	bl	ca50 <__gmpn_copyi@plt>
   214d8:	cmp	x20, x19
   214dc:	b.ne	21494 <__gmpz_rootrem@@Base+0x1d0>  // b.any
   214e0:	b	21484 <__gmpz_rootrem@@Base+0x1c0>
   214e4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   214e8:	b	214ac <__gmpz_rootrem@@Base+0x1e8>
   214ec:	bl	cff0 <__gmp_sqrt_of_negative@plt>
   214f0:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000214f4 <__gmpz_rrandomb@@Base>:
   214f4:	stp	x29, x30, [sp, #-96]!
   214f8:	stp	x24, x23, [sp, #48]
   214fc:	add	x24, x2, #0x3f
   21500:	stp	x20, x19, [sp, #80]
   21504:	mov	x19, x0
   21508:	lsr	x20, x24, #6
   2150c:	str	x27, [sp, #16]
   21510:	stp	x26, x25, [sp, #32]
   21514:	stp	x22, x21, [sp, #64]
   21518:	mov	x29, sp
   2151c:	cbz	x2, 2166c <__gmpz_rrandomb@@Base+0x178>
   21520:	ldrsw	x8, [x19]
   21524:	mov	x23, x2
   21528:	mov	x21, x1
   2152c:	cmp	x20, x8
   21530:	b.gt	2168c <__gmpz_rrandomb@@Base+0x198>
   21534:	ldr	x22, [x19, #8]
   21538:	neg	w9, w23
   2153c:	mov	x10, #0xffffffffffffffff    	// #-1
   21540:	sub	x8, x20, #0x1
   21544:	lsr	x9, x10, x9
   21548:	cmp	x24, #0x80
   2154c:	str	x9, [x22, x8, lsl #3]
   21550:	b.cc	21578 <__gmpz_rrandomb@@Base+0x84>  // b.lo, b.ul, b.last
   21554:	cmp	x20, #0x2
   21558:	mov	w9, #0x2                   	// #2
   2155c:	csel	x9, x20, x9, cc  // cc = lo, ul, last
   21560:	sub	x9, x9, #0x2
   21564:	sub	x8, x8, x9
   21568:	add	x0, x22, x9, lsl #3
   2156c:	lsl	x2, x8, #3
   21570:	mov	w1, #0xff                  	// #255
   21574:	bl	c5f0 <memset@plt>
   21578:	ldr	x8, [x21, #24]
   2157c:	add	x1, x29, #0x18
   21580:	mov	w2, #0x20                  	// #32
   21584:	mov	x0, x21
   21588:	ldr	x8, [x8, #8]
   2158c:	blr	x8
   21590:	ldr	x8, [x29, #24]
   21594:	add	x24, x22, #0x8
   21598:	mov	w26, #0x1                   	// #1
   2159c:	and	x8, x8, #0x3
   215a0:	add	x8, x8, #0x1
   215a4:	udiv	x8, x23, x8
   215a8:	cmp	w8, #0x0
   215ac:	cinc	w25, w8, eq  // eq = none
   215b0:	b	215bc <__gmpz_rrandomb@@Base+0xc8>
   215b4:	cmp	x27, x8
   215b8:	b.ls	2166c <__gmpz_rrandomb@@Base+0x178>  // b.plast
   215bc:	ldr	x8, [x21, #24]
   215c0:	add	x1, x29, #0x18
   215c4:	mov	w2, #0x20                  	// #32
   215c8:	mov	x0, x21
   215cc:	ldr	x8, [x8, #8]
   215d0:	blr	x8
   215d4:	ldr	x8, [x29, #24]
   215d8:	udiv	x9, x8, x25
   215dc:	msub	x8, x9, x25, x8
   215e0:	add	x8, x8, #0x1
   215e4:	subs	x8, x23, x8
   215e8:	csel	x27, xzr, x8, cc  // cc = lo, ul, last
   215ec:	b.ls	2166c <__gmpz_rrandomb@@Base+0x178>  // b.plast
   215f0:	lsr	x8, x27, #3
   215f4:	and	x8, x8, #0x1ffffffffffffff8
   215f8:	ldr	x9, [x22, x8]
   215fc:	lsl	x10, x26, x27
   21600:	add	x1, x29, #0x18
   21604:	mov	w2, #0x20                  	// #32
   21608:	eor	x9, x9, x10
   2160c:	str	x9, [x22, x8]
   21610:	ldr	x8, [x21, #24]
   21614:	mov	x0, x21
   21618:	ldr	x8, [x8, #8]
   2161c:	blr	x8
   21620:	ldr	x8, [x29, #24]
   21624:	udiv	x9, x8, x25
   21628:	msub	x8, x9, x25, x8
   2162c:	add	x8, x8, #0x1
   21630:	subs	x9, x27, x8
   21634:	csel	x23, xzr, x9, cc  // cc = lo, ul, last
   21638:	lsr	x9, x23, #6
   2163c:	lsl	x10, x9, #3
   21640:	ldr	x11, [x22, x10]
   21644:	lsl	x12, x26, x23
   21648:	adds	x11, x11, x12
   2164c:	str	x11, [x22, x10]
   21650:	b.cc	215b4 <__gmpz_rrandomb@@Base+0xc0>  // b.lo, b.ul, b.last
   21654:	add	x9, x24, x9, lsl #3
   21658:	ldr	x10, [x9]
   2165c:	adds	x10, x10, #0x1
   21660:	str	x10, [x9], #8
   21664:	b.cs	21658 <__gmpz_rrandomb@@Base+0x164>  // b.hs, b.nlast
   21668:	b	215b4 <__gmpz_rrandomb@@Base+0xc0>
   2166c:	str	w20, [x19, #4]
   21670:	ldp	x20, x19, [sp, #80]
   21674:	ldp	x22, x21, [sp, #64]
   21678:	ldp	x24, x23, [sp, #48]
   2167c:	ldp	x26, x25, [sp, #32]
   21680:	ldr	x27, [sp, #16]
   21684:	ldp	x29, x30, [sp], #96
   21688:	ret
   2168c:	mov	x0, x19
   21690:	mov	x1, x20
   21694:	bl	c080 <__gmpz_realloc@plt>
   21698:	mov	x22, x0
   2169c:	b	21538 <__gmpz_rrandomb@@Base+0x44>

00000000000216a0 <__gmpz_scan0@@Base>:
   216a0:	ldrsw	x13, [x0, #4]
   216a4:	lsr	x12, x1, #6
   216a8:	cmp	x13, #0x0
   216ac:	cneg	x10, x13, mi  // mi = first
   216b0:	cmp	x12, x10
   216b4:	b.ge	21710 <__gmpz_scan0@@Base+0x70>  // b.tcont
   216b8:	ldr	x8, [x0, #8]
   216bc:	add	x9, x8, x12, lsl #3
   216c0:	ldr	x11, [x9]
   216c4:	tbnz	w13, #31, 2171c <__gmpz_scan0@@Base+0x7c>
   216c8:	mov	x13, #0xffffffffffffffff    	// #-1
   216cc:	lsl	x13, x13, x1
   216d0:	orn	x11, x11, x13
   216d4:	cmn	x11, #0x1
   216d8:	b.ne	21708 <__gmpz_scan0@@Base+0x68>  // b.any
   216dc:	lsl	x11, x10, #3
   216e0:	sub	x11, x11, x12, lsl #3
   216e4:	sub	x12, x11, #0x8
   216e8:	cbz	x12, 21788 <__gmpz_scan0@@Base+0xe8>
   216ec:	ldr	x11, [x9, #8]
   216f0:	add	x13, x9, #0x8
   216f4:	sub	x12, x12, #0x8
   216f8:	mov	x9, x13
   216fc:	cmn	x11, #0x1
   21700:	b.eq	216e8 <__gmpz_scan0@@Base+0x48>  // b.none
   21704:	mov	x9, x13
   21708:	mvn	x11, x11
   2170c:	b	21774 <__gmpz_scan0@@Base+0xd4>
   21710:	cmp	w13, #0x0
   21714:	csinv	x0, x1, xzr, ge  // ge = tcont
   21718:	ret
   2171c:	add	x10, x8, x10, lsl #3
   21720:	sub	x13, x8, #0x8
   21724:	lsl	x12, x12, #3
   21728:	cbz	x12, 2174c <__gmpz_scan0@@Base+0xac>
   2172c:	ldr	x14, [x13, x12]
   21730:	sub	x12, x12, #0x8
   21734:	cbz	x14, 21728 <__gmpz_scan0@@Base+0x88>
   21738:	mov	x12, #0xffffffffffffffff    	// #-1
   2173c:	lsl	x12, x12, x1
   21740:	ands	x11, x11, x12
   21744:	b.ne	21774 <__gmpz_scan0@@Base+0xd4>  // b.any
   21748:	b	21760 <__gmpz_scan0@@Base+0xc0>
   2174c:	sub	x11, x11, #0x1
   21750:	mov	x12, #0xffffffffffffffff    	// #-1
   21754:	lsl	x12, x12, x1
   21758:	ands	x11, x11, x12
   2175c:	b.ne	21774 <__gmpz_scan0@@Base+0xd4>  // b.any
   21760:	add	x11, x9, #0x8
   21764:	cmp	x11, x10
   21768:	b.eq	21790 <__gmpz_scan0@@Base+0xf0>  // b.none
   2176c:	ldr	x11, [x9, #8]!
   21770:	cbz	x11, 2176c <__gmpz_scan0@@Base+0xcc>
   21774:	rbit	x10, x11
   21778:	clz	x10, x10
   2177c:	sub	x8, x9, x8
   21780:	add	x0, x10, x8, lsl #3
   21784:	ret
   21788:	lsl	x0, x10, #6
   2178c:	ret
   21790:	mov	x0, #0xffffffffffffffff    	// #-1
   21794:	ret

0000000000021798 <__gmpz_scan1@@Base>:
   21798:	ldrsw	x13, [x0, #4]
   2179c:	lsr	x11, x1, #6
   217a0:	cmp	x13, #0x0
   217a4:	cneg	x10, x13, mi  // mi = first
   217a8:	cmp	x11, x10
   217ac:	b.ge	217ec <__gmpz_scan1@@Base+0x54>  // b.tcont
   217b0:	ldr	x8, [x0, #8]
   217b4:	add	x9, x8, x11, lsl #3
   217b8:	cbz	x1, 21864 <__gmpz_scan1@@Base+0xcc>
   217bc:	ldr	x12, [x9]
   217c0:	tbnz	w13, #31, 217f8 <__gmpz_scan1@@Base+0x60>
   217c4:	mov	x11, #0xffffffffffffffff    	// #-1
   217c8:	lsl	x11, x11, x1
   217cc:	ands	x11, x12, x11
   217d0:	b.ne	2186c <__gmpz_scan1@@Base+0xd4>  // b.any
   217d4:	add	x10, x8, x10, lsl #3
   217d8:	sub	x10, x10, #0x8
   217dc:	cmp	x9, x10
   217e0:	b.ne	21860 <__gmpz_scan1@@Base+0xc8>  // b.any
   217e4:	mov	x0, #0xffffffffffffffff    	// #-1
   217e8:	ret
   217ec:	cmp	w13, #0x0
   217f0:	csinv	x0, x1, xzr, lt  // lt = tstop
   217f4:	ret
   217f8:	cbz	x11, 21814 <__gmpz_scan1@@Base+0x7c>
   217fc:	lsl	x13, x11, #3
   21800:	sub	x14, x8, #0x8
   21804:	ldr	x15, [x14, x13]
   21808:	cbnz	x15, 2181c <__gmpz_scan1@@Base+0x84>
   2180c:	subs	x13, x13, #0x8
   21810:	b.ne	21804 <__gmpz_scan1@@Base+0x6c>  // b.any
   21814:	cbz	x12, 21860 <__gmpz_scan1@@Base+0xc8>
   21818:	sub	x12, x12, #0x1
   2181c:	mov	x13, #0xffffffffffffffff    	// #-1
   21820:	lsl	x13, x13, x1
   21824:	orn	x12, x12, x13
   21828:	cmn	x12, #0x1
   2182c:	b.ne	21850 <__gmpz_scan1@@Base+0xb8>  // b.any
   21830:	lsl	x12, x10, #3
   21834:	sub	x11, x12, x11, lsl #3
   21838:	sub	x11, x11, #0x8
   2183c:	cbz	x11, 21858 <__gmpz_scan1@@Base+0xc0>
   21840:	ldr	x12, [x9, #8]!
   21844:	sub	x11, x11, #0x8
   21848:	cmn	x12, #0x1
   2184c:	b.eq	2183c <__gmpz_scan1@@Base+0xa4>  // b.none
   21850:	mvn	x11, x12
   21854:	b	2186c <__gmpz_scan1@@Base+0xd4>
   21858:	lsl	x0, x10, #6
   2185c:	ret
   21860:	add	x9, x9, #0x8
   21864:	ldr	x11, [x9]
   21868:	cbz	x11, 21860 <__gmpz_scan1@@Base+0xc8>
   2186c:	rbit	x10, x11
   21870:	clz	x10, x10
   21874:	sub	x8, x9, x8
   21878:	add	x0, x10, x8, lsl #3
   2187c:	ret

0000000000021880 <__gmpz_set@@Base>:
   21880:	stp	x29, x30, [sp, #-48]!
   21884:	stp	x22, x21, [sp, #16]
   21888:	stp	x20, x19, [sp, #32]
   2188c:	ldrsw	x22, [x1, #4]
   21890:	ldrsw	x8, [x0]
   21894:	mov	x20, x1
   21898:	mov	x19, x0
   2189c:	cmp	x22, #0x0
   218a0:	cneg	x21, x22, mi  // mi = first
   218a4:	cmp	x21, x8
   218a8:	mov	x29, sp
   218ac:	b.gt	218d4 <__gmpz_set@@Base+0x54>
   218b0:	ldr	x0, [x19, #8]
   218b4:	ldr	x1, [x20, #8]
   218b8:	mov	x2, x21
   218bc:	bl	ca50 <__gmpn_copyi@plt>
   218c0:	str	w22, [x19, #4]
   218c4:	ldp	x20, x19, [sp, #32]
   218c8:	ldp	x22, x21, [sp, #16]
   218cc:	ldp	x29, x30, [sp], #48
   218d0:	ret
   218d4:	mov	x0, x19
   218d8:	mov	x1, x21
   218dc:	bl	c080 <__gmpz_realloc@plt>
   218e0:	b	218b4 <__gmpz_set@@Base+0x34>

00000000000218e4 <__gmpz_set_d@@Base>:
   218e4:	sub	sp, sp, #0x50
   218e8:	str	d8, [sp, #16]
   218ec:	mov	v8.16b, v0.16b
   218f0:	fmov	x8, d8
   218f4:	mvn	x8, x8
   218f8:	tst	x8, #0x7ff0000000000000
   218fc:	stp	x29, x30, [sp, #32]
   21900:	stp	x22, x21, [sp, #48]
   21904:	stp	x20, x19, [sp, #64]
   21908:	add	x29, sp, #0x10
   2190c:	b.eq	219c4 <__gmpz_set_d@@Base+0xe0>  // b.none
   21910:	fneg	d0, d8
   21914:	fcmp	d8, #0.0
   21918:	mov	x19, x0
   2191c:	fcsel	d0, d8, d0, ge  // ge = tcont
   21920:	mov	x0, sp
   21924:	bl	d280 <__gmp_extract_double@plt>
   21928:	ldrsw	x8, [x19]
   2192c:	sxtw	x9, w0
   21930:	bic	x20, x9, x9, asr #63
   21934:	cmp	x20, x8
   21938:	b.gt	219ac <__gmpz_set_d@@Base+0xc8>
   2193c:	ldr	x21, [x19, #8]
   21940:	cbz	x20, 21984 <__gmpz_set_d@@Base+0xa0>
   21944:	cmp	x20, #0x1
   21948:	b.eq	2197c <__gmpz_set_d@@Base+0x98>  // b.none
   2194c:	cmp	x20, #0x2
   21950:	b.eq	21970 <__gmpz_set_d@@Base+0x8c>  // b.none
   21954:	lsl	x8, x20, #3
   21958:	sub	x22, x8, #0x10
   2195c:	mov	x0, x21
   21960:	mov	w1, wzr
   21964:	mov	x2, x22
   21968:	bl	c5f0 <memset@plt>
   2196c:	add	x21, x21, x22
   21970:	ldr	q0, [sp]
   21974:	str	q0, [x21]
   21978:	b	21984 <__gmpz_set_d@@Base+0xa0>
   2197c:	ldr	x8, [sp, #8]
   21980:	str	x8, [x21]
   21984:	neg	w8, w20
   21988:	fcmp	d8, #0.0
   2198c:	csel	x8, x8, x20, mi  // mi = first
   21990:	str	w8, [x19, #4]
   21994:	ldp	x20, x19, [sp, #64]
   21998:	ldp	x22, x21, [sp, #48]
   2199c:	ldp	x29, x30, [sp, #32]
   219a0:	ldr	d8, [sp, #16]
   219a4:	add	sp, sp, #0x50
   219a8:	ret
   219ac:	mov	x0, x19
   219b0:	mov	x1, x20
   219b4:	bl	c080 <__gmpz_realloc@plt>
   219b8:	mov	x21, x0
   219bc:	cbnz	x20, 21944 <__gmpz_set_d@@Base+0x60>
   219c0:	b	21984 <__gmpz_set_d@@Base+0xa0>
   219c4:	bl	c1b0 <__gmp_invalid_operation@plt>

00000000000219c8 <__gmpz_set_f@@Base>:
   219c8:	stp	x29, x30, [sp, #-64]!
   219cc:	stp	x24, x23, [sp, #16]
   219d0:	stp	x22, x21, [sp, #32]
   219d4:	stp	x20, x19, [sp, #48]
   219d8:	ldr	x19, [x1, #8]
   219dc:	mov	x22, x0
   219e0:	mov	x29, sp
   219e4:	cmp	x19, #0x0
   219e8:	b.le	21a48 <__gmpz_set_f@@Base+0x80>
   219ec:	ldrsw	x8, [x22]
   219f0:	mov	x21, x1
   219f4:	cmp	x19, x8
   219f8:	b.gt	21a88 <__gmpz_set_f@@Base+0xc0>
   219fc:	ldr	x20, [x22, #8]
   21a00:	ldrsw	x8, [x21, #4]
   21a04:	ldr	x21, [x21, #16]
   21a08:	neg	w9, w19
   21a0c:	cmp	w8, #0x0
   21a10:	csel	x9, x19, x9, ge  // ge = tcont
   21a14:	cmp	x8, #0x0
   21a18:	cneg	x23, x8, mi  // mi = first
   21a1c:	subs	x24, x19, x23
   21a20:	str	w9, [x22, #4]
   21a24:	b.le	21a60 <__gmpz_set_f@@Base+0x98>
   21a28:	b.eq	21a3c <__gmpz_set_f@@Base+0x74>  // b.none
   21a2c:	lsl	x2, x24, #3
   21a30:	mov	x0, x20
   21a34:	mov	w1, wzr
   21a38:	bl	c5f0 <memset@plt>
   21a3c:	add	x20, x20, x24, lsl #3
   21a40:	mov	x19, x23
   21a44:	b	21a68 <__gmpz_set_f@@Base+0xa0>
   21a48:	str	wzr, [x22, #4]
   21a4c:	ldp	x20, x19, [sp, #48]
   21a50:	ldp	x22, x21, [sp, #32]
   21a54:	ldp	x24, x23, [sp, #16]
   21a58:	ldp	x29, x30, [sp], #64
   21a5c:	ret
   21a60:	sub	x8, x23, x19
   21a64:	add	x21, x21, x8, lsl #3
   21a68:	mov	x0, x20
   21a6c:	mov	x1, x21
   21a70:	mov	x2, x19
   21a74:	ldp	x20, x19, [sp, #48]
   21a78:	ldp	x22, x21, [sp, #32]
   21a7c:	ldp	x24, x23, [sp, #16]
   21a80:	ldp	x29, x30, [sp], #64
   21a84:	b	ca50 <__gmpn_copyi@plt>
   21a88:	mov	x0, x22
   21a8c:	mov	x1, x19
   21a90:	bl	c080 <__gmpz_realloc@plt>
   21a94:	mov	x20, x0
   21a98:	b	21a00 <__gmpz_set_f@@Base+0x38>

0000000000021a9c <__gmpz_set_q@@Base>:
   21a9c:	add	x2, x1, #0x10
   21aa0:	b	c040 <__gmpz_tdiv_q@plt>

0000000000021aa4 <__gmpz_set_si@@Base>:
   21aa4:	stp	x29, x30, [sp, #-48]!
   21aa8:	stp	x20, x19, [sp, #32]
   21aac:	ldr	w8, [x0]
   21ab0:	cmp	x1, #0x0
   21ab4:	str	x21, [sp, #16]
   21ab8:	mov	x19, x0
   21abc:	mov	x20, x1
   21ac0:	cneg	x21, x1, mi  // mi = first
   21ac4:	cmp	w8, #0x0
   21ac8:	mov	x29, sp
   21acc:	b.le	21afc <__gmpz_set_si@@Base+0x58>
   21ad0:	ldr	x0, [x19, #8]
   21ad4:	cmp	x20, #0x0
   21ad8:	cset	w8, ne  // ne = any
   21adc:	csetm	w9, ne  // ne = any
   21ae0:	csel	w8, w8, w9, ge  // ge = tcont
   21ae4:	str	x21, [x0]
   21ae8:	str	w8, [x19, #4]
   21aec:	ldp	x20, x19, [sp, #32]
   21af0:	ldr	x21, [sp, #16]
   21af4:	ldp	x29, x30, [sp], #48
   21af8:	ret
   21afc:	mov	w1, #0x1                   	// #1
   21b00:	mov	x0, x19
   21b04:	bl	c080 <__gmpz_realloc@plt>
   21b08:	b	21ad4 <__gmpz_set_si@@Base+0x30>

0000000000021b0c <__gmpz_set_str@@Base>:
   21b0c:	stp	x29, x30, [sp, #-96]!
   21b10:	str	x27, [sp, #16]
   21b14:	stp	x26, x25, [sp, #32]
   21b18:	stp	x24, x23, [sp, #48]
   21b1c:	stp	x22, x21, [sp, #64]
   21b20:	stp	x20, x19, [sp, #80]
   21b24:	adrp	x26, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   21b28:	ldr	x26, [x26, #3920]
   21b2c:	mov	w20, w2
   21b30:	mov	x21, x1
   21b34:	mov	x19, x0
   21b38:	cmp	w2, #0x25
   21b3c:	mov	x29, sp
   21b40:	b.lt	21b50 <__gmpz_set_str@@Base+0x44>  // b.tstop
   21b44:	cmp	w20, #0x3e
   21b48:	b.gt	21c8c <__gmpz_set_str@@Base+0x180>
   21b4c:	add	x26, x26, #0xd0
   21b50:	bl	cae0 <__ctype_b_loc@plt>
   21b54:	ldr	x8, [x0]
   21b58:	mov	x22, x0
   21b5c:	ldrb	w27, [x21], #1
   21b60:	ldrh	w9, [x8, x27, lsl #1]
   21b64:	tbnz	w9, #13, 21b5c <__gmpz_set_str@@Base+0x50>
   21b68:	cmp	w27, #0x2d
   21b6c:	b.ne	21b7c <__gmpz_set_str@@Base+0x70>  // b.any
   21b70:	ldrb	w27, [x21], #1
   21b74:	mov	w25, #0x1                   	// #1
   21b78:	b	21b80 <__gmpz_set_str@@Base+0x74>
   21b7c:	mov	w25, wzr
   21b80:	ldrb	w9, [x26, w27, uxtw]
   21b84:	cmp	w20, #0x0
   21b88:	mov	w10, #0xa                   	// #10
   21b8c:	csel	w10, w10, w20, eq  // eq = none
   21b90:	cmp	w10, w9
   21b94:	b.le	21c8c <__gmpz_set_str@@Base+0x180>
   21b98:	cbnz	w20, 21bf8 <__gmpz_set_str@@Base+0xec>
   21b9c:	cmp	w27, #0x30
   21ba0:	b.ne	21bc8 <__gmpz_set_str@@Base+0xbc>  // b.any
   21ba4:	mov	x9, x21
   21ba8:	ldrb	w27, [x9], #1
   21bac:	orr	w10, w27, #0x20
   21bb0:	cmp	w10, #0x78
   21bb4:	b.ne	21bd0 <__gmpz_set_str@@Base+0xc4>  // b.any
   21bb8:	ldrb	w27, [x21, #1]
   21bbc:	add	x21, x21, #0x2
   21bc0:	mov	w20, #0x10                  	// #16
   21bc4:	b	21bf8 <__gmpz_set_str@@Base+0xec>
   21bc8:	mov	w20, #0xa                   	// #10
   21bcc:	b	21bf8 <__gmpz_set_str@@Base+0xec>
   21bd0:	cmp	w10, #0x62
   21bd4:	b.ne	21be8 <__gmpz_set_str@@Base+0xdc>  // b.any
   21bd8:	ldrb	w27, [x21, #1]
   21bdc:	add	x21, x21, #0x2
   21be0:	mov	w20, #0x2                   	// #2
   21be4:	b	21bf8 <__gmpz_set_str@@Base+0xec>
   21be8:	mov	w20, #0x8                   	// #8
   21bec:	mov	x21, x9
   21bf0:	b	21bf8 <__gmpz_set_str@@Base+0xec>
   21bf4:	ldrb	w27, [x21], #1
   21bf8:	cmp	w27, #0x30
   21bfc:	b.eq	21bf4 <__gmpz_set_str@@Base+0xe8>  // b.none
   21c00:	ldrh	w9, [x8, w27, uxtw #1]
   21c04:	tbnz	w9, #13, 21bf4 <__gmpz_set_str@@Base+0xe8>
   21c08:	cbz	w27, 21d04 <__gmpz_set_str@@Base+0x1f8>
   21c0c:	sub	x0, x21, #0x1
   21c10:	str	xzr, [x29, #24]
   21c14:	bl	bf60 <strlen@plt>
   21c18:	add	x1, x0, #0x1
   21c1c:	mov	w8, #0x7f01                	// #32513
   21c20:	mov	x24, x0
   21c24:	cmp	x1, x8
   21c28:	b.cs	21d2c <__gmpz_set_str@@Base+0x220>  // b.hs, b.nlast
   21c2c:	add	x9, x1, #0xf
   21c30:	mov	x8, sp
   21c34:	and	x9, x9, #0xfffffffffffffff0
   21c38:	sub	x23, x8, x9
   21c3c:	mov	sp, x23
   21c40:	mov	x8, x23
   21c44:	cbz	x24, 21c94 <__gmpz_set_str@@Base+0x188>
   21c48:	mov	x9, xzr
   21c4c:	mov	x8, x23
   21c50:	b	21c68 <__gmpz_set_str@@Base+0x15c>
   21c54:	strb	w10, [x8], #1
   21c58:	ldrb	w27, [x21, x9]
   21c5c:	add	x9, x9, #0x1
   21c60:	cmp	x24, x9
   21c64:	b.eq	21c94 <__gmpz_set_str@@Base+0x188>  // b.none
   21c68:	ldr	x10, [x22]
   21c6c:	ldrh	w10, [x10, w27, uxtw #1]
   21c70:	tbnz	w10, #13, 21c58 <__gmpz_set_str@@Base+0x14c>
   21c74:	mov	w10, w27
   21c78:	ldrb	w10, [x26, x10]
   21c7c:	cmp	w20, w10
   21c80:	b.gt	21c54 <__gmpz_set_str@@Base+0x148>
   21c84:	ldr	x0, [x29, #24]
   21c88:	cbnz	x0, 21d48 <__gmpz_set_str@@Base+0x23c>
   21c8c:	mov	w0, #0xffffffff            	// #-1
   21c90:	b	21d0c <__gmpz_set_str@@Base+0x200>
   21c94:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   21c98:	ldr	x9, [x9, #3936]
   21c9c:	mov	w10, #0x28                  	// #40
   21ca0:	sub	x21, x8, x23
   21ca4:	ldrsw	x8, [x19]
   21ca8:	smaddl	x9, w20, w10, x9
   21cac:	ldr	x9, [x9, #16]
   21cb0:	umulh	x9, x9, x21
   21cb4:	ubfx	x9, x9, #3, #58
   21cb8:	add	x1, x9, #0x2
   21cbc:	cmp	x1, x8
   21cc0:	b.gt	21d3c <__gmpz_set_str@@Base+0x230>
   21cc4:	ldr	x0, [x19, #8]
   21cc8:	mov	x1, x23
   21ccc:	mov	x2, x21
   21cd0:	mov	w3, w20
   21cd4:	bl	c090 <__gmpn_set_str@plt>
   21cd8:	neg	w8, w0
   21cdc:	cmp	w25, #0x0
   21ce0:	csel	x8, x0, x8, eq  // eq = none
   21ce4:	str	w8, [x19, #4]
   21ce8:	ldr	x8, [x29, #24]
   21cec:	mov	w0, wzr
   21cf0:	cbz	x8, 21d0c <__gmpz_set_str@@Base+0x200>
   21cf4:	mov	x0, x8
   21cf8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   21cfc:	mov	w0, wzr
   21d00:	b	21d0c <__gmpz_set_str@@Base+0x200>
   21d04:	mov	w0, wzr
   21d08:	str	wzr, [x19, #4]
   21d0c:	mov	sp, x29
   21d10:	ldp	x20, x19, [sp, #80]
   21d14:	ldp	x22, x21, [sp, #64]
   21d18:	ldp	x24, x23, [sp, #48]
   21d1c:	ldp	x26, x25, [sp, #32]
   21d20:	ldr	x27, [sp, #16]
   21d24:	ldp	x29, x30, [sp], #96
   21d28:	ret
   21d2c:	add	x0, x29, #0x18
   21d30:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   21d34:	mov	x23, x0
   21d38:	b	21c48 <__gmpz_set_str@@Base+0x13c>
   21d3c:	mov	x0, x19
   21d40:	bl	c080 <__gmpz_realloc@plt>
   21d44:	b	21cc4 <__gmpz_set_str@@Base+0x1b8>
   21d48:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   21d4c:	mov	w0, #0xffffffff            	// #-1
   21d50:	b	21d0c <__gmpz_set_str@@Base+0x200>

0000000000021d54 <__gmpz_set_ui@@Base>:
   21d54:	stp	x29, x30, [sp, #-32]!
   21d58:	stp	x20, x19, [sp, #16]
   21d5c:	ldr	w8, [x0]
   21d60:	mov	x19, x0
   21d64:	mov	x20, x1
   21d68:	mov	x29, sp
   21d6c:	cmp	w8, #0x0
   21d70:	b.le	21d94 <__gmpz_set_ui@@Base+0x40>
   21d74:	ldr	x0, [x19, #8]
   21d78:	cmp	x20, #0x0
   21d7c:	cset	w8, ne  // ne = any
   21d80:	str	x20, [x0]
   21d84:	str	w8, [x19, #4]
   21d88:	ldp	x20, x19, [sp, #16]
   21d8c:	ldp	x29, x30, [sp], #32
   21d90:	ret
   21d94:	mov	w1, #0x1                   	// #1
   21d98:	mov	x0, x19
   21d9c:	bl	c080 <__gmpz_realloc@plt>
   21da0:	b	21d78 <__gmpz_set_ui@@Base+0x24>

0000000000021da4 <__gmpz_setbit@@Base>:
   21da4:	stp	x29, x30, [sp, #-64]!
   21da8:	stp	x24, x23, [sp, #16]
   21dac:	stp	x22, x21, [sp, #32]
   21db0:	stp	x20, x19, [sp, #48]
   21db4:	ldrsw	x23, [x0, #4]
   21db8:	ldr	x20, [x0, #8]
   21dbc:	mov	w8, #0x1                   	// #1
   21dc0:	mov	x19, x0
   21dc4:	lsr	x22, x1, #6
   21dc8:	lsl	x24, x8, x1
   21dcc:	mov	x29, sp
   21dd0:	tbnz	w23, #31, 21df0 <__gmpz_setbit@@Base+0x4c>
   21dd4:	cmp	x22, x23
   21dd8:	b.ge	21e5c <__gmpz_setbit@@Base+0xb8>  // b.tcont
   21ddc:	lsl	x8, x22, #3
   21de0:	ldr	x9, [x20, x8]
   21de4:	orr	x9, x9, x24
   21de8:	str	x9, [x20, x8]
   21dec:	b	21e8c <__gmpz_setbit@@Base+0xe8>
   21df0:	neg	x8, x23
   21df4:	cmp	x22, x8
   21df8:	b.ge	21e8c <__gmpz_setbit@@Base+0xe8>  // b.tcont
   21dfc:	mov	x9, xzr
   21e00:	ldr	x10, [x20, x9, lsl #3]
   21e04:	add	x9, x9, #0x1
   21e08:	cbz	x10, 21e00 <__gmpz_setbit@@Base+0x5c>
   21e0c:	sub	x10, x9, #0x1
   21e10:	cmp	x22, x10
   21e14:	b.ls	21ea0 <__gmpz_setbit@@Base+0xfc>  // b.plast
   21e18:	lsl	x9, x22, #3
   21e1c:	ldr	x10, [x20, x9]
   21e20:	bics	xzr, x10, x24
   21e24:	bic	x11, x10, x24
   21e28:	cinc	x10, x22, eq  // eq = none
   21e2c:	cmp	x10, x8
   21e30:	str	x11, [x20, x9]
   21e34:	b.ne	21e8c <__gmpz_setbit@@Base+0xe8>  // b.any
   21e38:	sub	x8, x20, #0x8
   21e3c:	subs	x9, x22, #0x1
   21e40:	b.lt	21f20 <__gmpz_setbit@@Base+0x17c>  // b.tstop
   21e44:	ldr	x10, [x8, x22, lsl #3]
   21e48:	mov	x22, x9
   21e4c:	cbz	x10, 21e3c <__gmpz_setbit@@Base+0x98>
   21e50:	add	x8, x9, #0x1
   21e54:	neg	w8, w8
   21e58:	b	21ef8 <__gmpz_setbit@@Base+0x154>
   21e5c:	ldrsw	x8, [x19]
   21e60:	add	x21, x22, #0x1
   21e64:	cmp	x22, x8
   21e68:	b.ge	21f00 <__gmpz_setbit@@Base+0x15c>  // b.tcont
   21e6c:	subs	x8, x22, x23
   21e70:	str	w21, [x19, #4]
   21e74:	b.eq	21e88 <__gmpz_setbit@@Base+0xe4>  // b.none
   21e78:	add	x0, x20, x23, lsl #3
   21e7c:	lsl	x2, x8, #3
   21e80:	mov	w1, wzr
   21e84:	bl	c5f0 <memset@plt>
   21e88:	str	x24, [x20, x22, lsl #3]
   21e8c:	ldp	x20, x19, [sp, #48]
   21e90:	ldp	x22, x21, [sp, #32]
   21e94:	ldp	x24, x23, [sp, #16]
   21e98:	ldp	x29, x30, [sp], #64
   21e9c:	ret
   21ea0:	ldr	x8, [x20, x22, lsl #3]
   21ea4:	add	x10, x22, #0x1
   21ea8:	cmp	x10, x9
   21eac:	b.ne	21ec4 <__gmpz_setbit@@Base+0x120>  // b.any
   21eb0:	sub	x8, x8, #0x1
   21eb4:	bic	x8, x8, x24
   21eb8:	add	x8, x8, #0x1
   21ebc:	str	x8, [x20, x22, lsl #3]
   21ec0:	b	21e8c <__gmpz_setbit@@Base+0xe8>
   21ec4:	subs	x8, x8, x24
   21ec8:	str	x8, [x20, x22, lsl #3]
   21ecc:	b.cs	21ee8 <__gmpz_setbit@@Base+0x144>  // b.hs, b.nlast
   21ed0:	add	x8, x20, x22, lsl #3
   21ed4:	add	x8, x8, #0x8
   21ed8:	ldr	x9, [x8]
   21edc:	sub	x10, x9, #0x1
   21ee0:	str	x10, [x8], #8
   21ee4:	cbz	x9, 21ed8 <__gmpz_setbit@@Base+0x134>
   21ee8:	mvn	x8, x23
   21eec:	ldr	x8, [x20, x8, lsl #3]
   21ef0:	cmp	x8, #0x0
   21ef4:	cinc	w8, w23, eq  // eq = none
   21ef8:	str	w8, [x19, #4]
   21efc:	b	21e8c <__gmpz_setbit@@Base+0xe8>
   21f00:	mov	x0, x19
   21f04:	mov	x1, x21
   21f08:	bl	c080 <__gmpz_realloc@plt>
   21f0c:	mov	x20, x0
   21f10:	subs	x8, x22, x23
   21f14:	str	w21, [x19, #4]
   21f18:	b.ne	21e78 <__gmpz_setbit@@Base+0xd4>  // b.any
   21f1c:	b	21e88 <__gmpz_setbit@@Base+0xe4>
   21f20:	mov	x8, xzr
   21f24:	neg	w8, w8
   21f28:	b	21ef8 <__gmpz_setbit@@Base+0x154>

0000000000021f2c <__gmpz_size@@Base>:
   21f2c:	ldr	w8, [x0, #4]
   21f30:	cmp	w8, #0x0
   21f34:	cneg	w0, w8, mi  // mi = first
   21f38:	ret

0000000000021f3c <__gmpz_sizeinbase@@Base>:
   21f3c:	ldr	w8, [x0, #4]
   21f40:	cmp	w8, #0x0
   21f44:	cneg	w8, w8, mi  // mi = first
   21f48:	cbz	w8, 21f9c <__gmpz_sizeinbase@@Base+0x60>
   21f4c:	ldr	x9, [x0, #8]
   21f50:	sub	w10, w8, #0x1
   21f54:	adrp	x11, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   21f58:	mov	w8, w8
   21f5c:	ldr	x9, [x9, w10, sxtw #3]
   21f60:	ldr	x11, [x11, #3936]
   21f64:	sub	w10, w1, #0x1
   21f68:	lsl	x8, x8, #6
   21f6c:	clz	x9, x9
   21f70:	tst	w1, w10
   21f74:	sub	x8, x8, x9
   21f78:	sxtw	x9, w1
   21f7c:	mov	w10, #0x28                  	// #40
   21f80:	madd	x9, x9, x10, x11
   21f84:	b.ne	21fa4 <__gmpz_sizeinbase@@Base+0x68>  // b.any
   21f88:	ldrsw	x9, [x9, #24]
   21f8c:	add	x8, x8, x9
   21f90:	sub	x8, x8, #0x1
   21f94:	udiv	x0, x8, x9
   21f98:	ret
   21f9c:	mov	w0, #0x1                   	// #1
   21fa0:	ret
   21fa4:	ldr	x9, [x9, #8]
   21fa8:	add	x9, x9, #0x1
   21fac:	umulh	x8, x9, x8
   21fb0:	add	x0, x8, #0x1
   21fb4:	ret

0000000000021fb8 <__gmpz_sqrt@@Base>:
   21fb8:	stp	x29, x30, [sp, #-48]!
   21fbc:	stp	x22, x21, [sp, #16]
   21fc0:	stp	x20, x19, [sp, #32]
   21fc4:	mov	x29, sp
   21fc8:	sub	sp, sp, #0x10
   21fcc:	ldrsw	x19, [x1, #4]
   21fd0:	cmp	w19, #0x0
   21fd4:	b.le	22094 <__gmpz_sqrt@@Base+0xdc>
   21fd8:	add	x8, x19, #0x1
   21fdc:	add	x9, x19, #0x2
   21fe0:	cmp	x8, #0x0
   21fe4:	csinc	x8, x9, x19, lt  // lt = tstop
   21fe8:	asr	x21, x8, #1
   21fec:	str	w21, [x0, #4]
   21ff0:	ldr	x20, [x1, #8]
   21ff4:	cmp	x0, x1
   21ff8:	b.eq	2202c <__gmpz_sqrt@@Base+0x74>  // b.none
   21ffc:	ldrsw	x8, [x0]
   22000:	cmp	x21, x8
   22004:	b.gt	220a0 <__gmpz_sqrt@@Base+0xe8>
   22008:	ldr	x0, [x0, #8]
   2200c:	mov	x1, xzr
   22010:	mov	x2, x20
   22014:	mov	x3, x19
   22018:	mov	sp, x29
   2201c:	ldp	x20, x19, [sp, #32]
   22020:	ldp	x22, x21, [sp, #16]
   22024:	ldp	x29, x30, [sp], #48
   22028:	b	d3b0 <__gmpn_sqrtrem@plt>
   2202c:	lsl	x1, x21, #3
   22030:	mov	w8, #0x7f00                	// #32512
   22034:	cmp	x1, x8
   22038:	stur	xzr, [x29, #-8]
   2203c:	b.hi	220ac <__gmpz_sqrt@@Base+0xf4>  // b.pmore
   22040:	add	x9, x1, #0xf
   22044:	mov	x8, sp
   22048:	and	x9, x9, #0xfffffffffffffff0
   2204c:	sub	x22, x8, x9
   22050:	mov	sp, x22
   22054:	mov	x0, x22
   22058:	mov	x1, xzr
   2205c:	mov	x2, x20
   22060:	mov	x3, x19
   22064:	bl	d3b0 <__gmpn_sqrtrem@plt>
   22068:	mov	x0, x20
   2206c:	mov	x1, x22
   22070:	mov	x2, x21
   22074:	bl	ca50 <__gmpn_copyi@plt>
   22078:	ldur	x0, [x29, #-8]
   2207c:	cbnz	x0, 220bc <__gmpz_sqrt@@Base+0x104>
   22080:	mov	sp, x29
   22084:	ldp	x20, x19, [sp, #32]
   22088:	ldp	x22, x21, [sp, #16]
   2208c:	ldp	x29, x30, [sp], #48
   22090:	ret
   22094:	cbnz	w19, 220c4 <__gmpz_sqrt@@Base+0x10c>
   22098:	str	wzr, [x0, #4]
   2209c:	b	22080 <__gmpz_sqrt@@Base+0xc8>
   220a0:	mov	x1, x21
   220a4:	bl	c080 <__gmpz_realloc@plt>
   220a8:	b	2200c <__gmpz_sqrt@@Base+0x54>
   220ac:	sub	x0, x29, #0x8
   220b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   220b4:	mov	x22, x0
   220b8:	b	22054 <__gmpz_sqrt@@Base+0x9c>
   220bc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   220c0:	b	22080 <__gmpz_sqrt@@Base+0xc8>
   220c4:	bl	cff0 <__gmp_sqrt_of_negative@plt>

00000000000220c8 <__gmpz_sqrtrem@@Base>:
   220c8:	stp	x29, x30, [sp, #-80]!
   220cc:	stp	x24, x23, [sp, #32]
   220d0:	stp	x22, x21, [sp, #48]
   220d4:	stp	x20, x19, [sp, #64]
   220d8:	ldrsw	x20, [x2, #4]
   220dc:	str	x25, [sp, #16]
   220e0:	mov	x19, x1
   220e4:	mov	x25, x0
   220e8:	cmp	w20, #0x0
   220ec:	mov	x29, sp
   220f0:	b.le	221d4 <__gmpz_sqrtrem@@Base+0x10c>
   220f4:	ldr	w8, [x19]
   220f8:	mov	x21, x2
   220fc:	cmp	w20, w8
   22100:	b.gt	221e0 <__gmpz_sqrtrem@@Base+0x118>
   22104:	ldr	x22, [x19, #8]
   22108:	add	x8, x20, #0x1
   2210c:	add	x9, x20, #0x2
   22110:	cmp	x8, #0x0
   22114:	csinc	x8, x9, x20, lt  // lt = tstop
   22118:	asr	x24, x8, #1
   2211c:	str	w24, [x25, #4]
   22120:	ldr	x23, [x21, #8]
   22124:	cmp	x25, x21
   22128:	b.eq	22154 <__gmpz_sqrtrem@@Base+0x8c>  // b.none
   2212c:	ldrsw	x8, [x25]
   22130:	cmp	x24, x8
   22134:	b.gt	221f4 <__gmpz_sqrtrem@@Base+0x12c>
   22138:	ldr	x0, [x25, #8]
   2213c:	mov	x1, x22
   22140:	mov	x2, x23
   22144:	mov	x3, x20
   22148:	bl	d3b0 <__gmpn_sqrtrem@plt>
   2214c:	mov	x20, x0
   22150:	b	221b4 <__gmpz_sqrtrem@@Base+0xec>
   22154:	lsl	x1, x24, #3
   22158:	mov	w8, #0x7f00                	// #32512
   2215c:	cmp	x1, x8
   22160:	str	xzr, [x29, #24]
   22164:	b.hi	22204 <__gmpz_sqrtrem@@Base+0x13c>  // b.pmore
   22168:	add	x9, x1, #0xf
   2216c:	mov	x8, sp
   22170:	and	x9, x9, #0xfffffffffffffff0
   22174:	sub	x25, x8, x9
   22178:	mov	sp, x25
   2217c:	mov	x0, x25
   22180:	mov	x1, x22
   22184:	mov	x2, x23
   22188:	mov	x3, x20
   2218c:	bl	d3b0 <__gmpn_sqrtrem@plt>
   22190:	cmp	x19, x21
   22194:	mov	x20, x0
   22198:	b.eq	221ac <__gmpz_sqrtrem@@Base+0xe4>  // b.none
   2219c:	mov	x0, x23
   221a0:	mov	x1, x25
   221a4:	mov	x2, x24
   221a8:	bl	ca50 <__gmpn_copyi@plt>
   221ac:	ldr	x0, [x29, #24]
   221b0:	cbnz	x0, 22214 <__gmpz_sqrtrem@@Base+0x14c>
   221b4:	str	w20, [x19, #4]
   221b8:	mov	sp, x29
   221bc:	ldp	x20, x19, [sp, #64]
   221c0:	ldp	x22, x21, [sp, #48]
   221c4:	ldp	x24, x23, [sp, #32]
   221c8:	ldr	x25, [sp, #16]
   221cc:	ldp	x29, x30, [sp], #80
   221d0:	ret
   221d4:	cbnz	w20, 2221c <__gmpz_sqrtrem@@Base+0x154>
   221d8:	str	wzr, [x25, #4]
   221dc:	b	221b4 <__gmpz_sqrtrem@@Base+0xec>
   221e0:	mov	x0, x19
   221e4:	mov	x1, x20
   221e8:	bl	c080 <__gmpz_realloc@plt>
   221ec:	mov	x22, x0
   221f0:	b	22108 <__gmpz_sqrtrem@@Base+0x40>
   221f4:	mov	x0, x25
   221f8:	mov	x1, x24
   221fc:	bl	c080 <__gmpz_realloc@plt>
   22200:	b	2213c <__gmpz_sqrtrem@@Base+0x74>
   22204:	add	x0, x29, #0x18
   22208:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2220c:	mov	x25, x0
   22210:	b	2217c <__gmpz_sqrtrem@@Base+0xb4>
   22214:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   22218:	b	221b4 <__gmpz_sqrtrem@@Base+0xec>
   2221c:	bl	cff0 <__gmp_sqrt_of_negative@plt>

0000000000022220 <__gmpz_stronglucas@@Base>:
   22220:	sub	sp, sp, #0x70
   22224:	stp	x29, x30, [sp, #48]
   22228:	stp	x24, x23, [sp, #64]
   2222c:	stp	x22, x21, [sp, #80]
   22230:	stp	x20, x19, [sp, #96]
   22234:	ldr	w8, [x0, #4]
   22238:	mov	x19, x1
   2223c:	ldr	x1, [x0, #8]
   22240:	add	x29, sp, #0x30
   22244:	cmp	w8, #0x0
   22248:	mov	x20, x2
   2224c:	cneg	w2, w8, mi  // mi = first
   22250:	sub	x0, x29, #0x10
   22254:	bl	cc90 <__gmpz_roinit_n@plt>
   22258:	ldur	w22, [x29, #-12]
   2225c:	ldur	x21, [x29, #-8]
   22260:	sxtw	x23, w22
   22264:	mov	x0, x21
   22268:	mov	x1, x23
   2226c:	bl	cf60 <__gmpn_mod_34lsub1@plt>
   22270:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   22274:	movk	x8, #0xcccd
   22278:	umulh	x8, x0, x8
   2227c:	ubfx	x8, x8, #2, #30
   22280:	mov	x24, x0
   22284:	add	w8, w8, w8, lsl #2
   22288:	sub	w8, w24, w8
   2228c:	tbnz	w8, #1, 22338 <__gmpz_stronglucas@@Base+0x118>
   22290:	mov	x8, #0x2493                	// #9363
   22294:	movk	x8, #0x9249, lsl #16
   22298:	movk	x8, #0x4924, lsl #32
   2229c:	movk	x8, #0x2492, lsl #48
   222a0:	umulh	x8, x24, x8
   222a4:	sub	x9, x24, x8
   222a8:	add	x8, x8, x9, lsr #1
   222ac:	lsr	x8, x8, #2
   222b0:	sub	x8, x8, x8, lsl #3
   222b4:	add	x8, x24, x8
   222b8:	sub	x9, x8, #0x1
   222bc:	tst	x8, x9
   222c0:	b.ne	2234c <__gmpz_stronglucas@@Base+0x12c>  // b.any
   222c4:	sub	x0, x29, #0x10
   222c8:	mov	w1, #0xb                   	// #11
   222cc:	mov	w23, #0xb                   	// #11
   222d0:	bl	cb30 <__gmpz_kronecker_ui@plt>
   222d4:	cmn	w0, #0x1
   222d8:	b.eq	22350 <__gmpz_stronglucas@@Base+0x130>  // b.none
   222dc:	mov	x8, #0x4ec5                	// #20165
   222e0:	movk	x8, #0xc4ec, lsl #16
   222e4:	movk	x8, #0xec4e, lsl #32
   222e8:	movk	x8, #0x4ec4, lsl #48
   222ec:	umulh	x8, x24, x8
   222f0:	lsr	x8, x8, #2
   222f4:	mov	w23, #0xd                   	// #13
   222f8:	msub	w8, w8, w23, w24
   222fc:	sub	w8, w8, w8, lsr #3
   22300:	and	x8, x8, #0x7
   22304:	cmp	x8, #0x4
   22308:	b.hi	22350 <__gmpz_stronglucas@@Base+0x130>  // b.pmore
   2230c:	cmp	x8, #0x2
   22310:	b.eq	22350 <__gmpz_stronglucas@@Base+0x130>  // b.none
   22314:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   22318:	movk	x8, #0xaaab
   2231c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   22320:	madd	x8, x24, x8, x9
   22324:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   22328:	cmp	x8, x9
   2232c:	b.cs	22480 <__gmpz_stronglucas@@Base+0x260>  // b.hs, b.nlast
   22330:	mov	w23, #0xf                   	// #15
   22334:	b	22350 <__gmpz_stronglucas@@Base+0x130>
   22338:	ldr	x2, [x19, #8]
   2233c:	mov	x0, x21
   22340:	mov	x1, x23
   22344:	bl	d120 <__gmpn_strongfibo@plt>
   22348:	b	22468 <__gmpz_stronglucas@@Base+0x248>
   2234c:	mov	w23, #0x7                   	// #7
   22350:	lsr	x8, x23, #2
   22354:	neg	x9, x8
   22358:	tst	x23, #0x2
   2235c:	sub	x0, x29, #0x10
   22360:	mov	x1, xzr
   22364:	csinc	x22, x9, x8, eq  // eq = none
   22368:	bl	c9a0 <__gmpz_scan0@plt>
   2236c:	mov	x21, x0
   22370:	add	x0, sp, #0x10
   22374:	bl	d250 <__gmpz_init@plt>
   22378:	mov	x0, sp
   2237c:	bl	d250 <__gmpz_init@plt>
   22380:	sub	x4, x29, #0x10
   22384:	add	x5, sp, #0x10
   22388:	mov	x6, sp
   2238c:	mov	x0, x19
   22390:	mov	x1, x20
   22394:	mov	x2, x22
   22398:	mov	x3, x21
   2239c:	bl	c520 <__gmpz_lucas_mod@plt>
   223a0:	cbnz	w0, 22450 <__gmpz_stronglucas@@Base+0x230>
   223a4:	subs	x22, x21, #0x1
   223a8:	b.eq	225d0 <__gmpz_stronglucas@@Base+0x3b0>  // b.none
   223ac:	mov	x0, sp
   223b0:	mov	x1, x19
   223b4:	mov	x2, x19
   223b8:	bl	c4b0 <__gmpz_mul@plt>
   223bc:	mov	x0, sp
   223c0:	mov	w2, #0x2                   	// #2
   223c4:	mov	x1, x20
   223c8:	bl	c870 <__gmpz_submul_ui@plt>
   223cc:	mov	x1, sp
   223d0:	sub	x2, x29, #0x10
   223d4:	mov	x0, x19
   223d8:	bl	ca80 <__gmpz_tdiv_r@plt>
   223dc:	ldr	w8, [x19, #4]
   223e0:	cbz	w8, 2244c <__gmpz_stronglucas@@Base+0x22c>
   223e4:	mov	w21, #0x1                   	// #1
   223e8:	subs	x22, x22, #0x1
   223ec:	b.eq	225d0 <__gmpz_stronglucas@@Base+0x3b0>  // b.none
   223f0:	mov	x0, sp
   223f4:	mov	x1, x20
   223f8:	mov	x2, x20
   223fc:	bl	c4b0 <__gmpz_mul@plt>
   22400:	mov	x1, sp
   22404:	sub	x2, x29, #0x10
   22408:	mov	x0, x20
   2240c:	bl	ca80 <__gmpz_tdiv_r@plt>
   22410:	mov	x0, sp
   22414:	mov	x1, x19
   22418:	mov	x2, x19
   2241c:	bl	c4b0 <__gmpz_mul@plt>
   22420:	mov	x0, sp
   22424:	mov	w2, #0x2                   	// #2
   22428:	mov	x1, x20
   2242c:	bl	c870 <__gmpz_submul_ui@plt>
   22430:	mov	x1, sp
   22434:	sub	x2, x29, #0x10
   22438:	mov	x0, x19
   2243c:	bl	ca80 <__gmpz_tdiv_r@plt>
   22440:	ldr	w8, [x19, #4]
   22444:	cbnz	w8, 223e8 <__gmpz_stronglucas@@Base+0x1c8>
   22448:	b	22450 <__gmpz_stronglucas@@Base+0x230>
   2244c:	mov	w21, #0x1                   	// #1
   22450:	add	x0, sp, #0x10
   22454:	bl	cb50 <__gmpz_clear@plt>
   22458:	mov	x0, sp
   2245c:	bl	cb50 <__gmpz_clear@plt>
   22460:	cmp	x21, #0x0
   22464:	cset	w0, ne  // ne = any
   22468:	ldp	x20, x19, [sp, #96]
   2246c:	ldp	x22, x21, [sp, #80]
   22470:	ldp	x24, x23, [sp, #64]
   22474:	ldp	x29, x30, [sp, #48]
   22478:	add	sp, sp, #0x70
   2247c:	ret
   22480:	mov	x8, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   22484:	movk	x8, #0xf0f1
   22488:	umulh	x8, x24, x8
   2248c:	lsr	x8, x8, #4
   22490:	add	x8, x8, x8, lsl #4
   22494:	sub	x8, x24, x8
   22498:	sub	x9, x8, #0x1
   2249c:	tst	x8, x9
   224a0:	b.eq	224bc <__gmpz_stronglucas@@Base+0x29c>  // b.none
   224a4:	mov	w23, #0x11                  	// #17
   224a8:	mov	w9, #0x10                  	// #16
   224ac:	sub	x10, x23, x8
   224b0:	sub	x8, x9, x8
   224b4:	tst	x10, x8
   224b8:	b.ne	22350 <__gmpz_stronglucas@@Base+0x130>  // b.any
   224bc:	cmp	w22, #0x1
   224c0:	b.lt	225e0 <__gmpz_stronglucas@@Base+0x3c0>  // b.tstop
   224c4:	mov	x0, x21
   224c8:	mov	x1, x22
   224cc:	bl	d0b0 <__gmpn_perfect_square_p@plt>
   224d0:	cbnz	w0, 225ec <__gmpz_stronglucas@@Base+0x3cc>
   224d4:	cmp	w22, #0x2
   224d8:	b.eq	22514 <__gmpz_stronglucas@@Base+0x2f4>  // b.none
   224dc:	cmp	w22, #0x1
   224e0:	b.ne	22538 <__gmpz_stronglucas@@Base+0x318>  // b.any
   224e4:	ldr	x8, [x21]
   224e8:	mov	w9, #0x40                  	// #64
   224ec:	mov	w22, #0x1                   	// #1
   224f0:	clz	x10, x8
   224f4:	sub	w9, w9, w10
   224f8:	asr	w9, w9, #1
   224fc:	lsl	x10, x22, x9
   22500:	lsr	x8, x8, x9
   22504:	add	x8, x10, x8
   22508:	lsr	x24, x8, #1
   2250c:	str	x24, [sp, #16]
   22510:	b	22588 <__gmpz_stronglucas@@Base+0x368>
   22514:	add	x0, sp, #0x10
   22518:	mov	w3, #0x2                   	// #2
   2251c:	mov	x1, xzr
   22520:	mov	x2, x21
   22524:	bl	d3b0 <__gmpn_sqrtrem@plt>
   22528:	ldr	x24, [sp, #16]
   2252c:	ldur	x21, [x29, #-8]
   22530:	ldur	w22, [x29, #-12]
   22534:	b	22540 <__gmpz_stronglucas@@Base+0x320>
   22538:	mov	x24, #0xffffffffffffffff    	// #-1
   2253c:	str	x24, [sp, #16]
   22540:	cmp	w22, #0x28
   22544:	sxtw	x22, w22
   22548:	b.lt	22588 <__gmpz_stronglucas@@Base+0x368>  // b.tstop
   2254c:	mov	w23, #0x11                  	// #17
   22550:	cmp	x23, x24
   22554:	b.cs	225d8 <__gmpz_stronglucas@@Base+0x3b8>  // b.hs, b.nlast
   22558:	add	x23, x23, #0x2
   2255c:	mov	x0, x21
   22560:	mov	x1, x22
   22564:	mov	x2, x23
   22568:	bl	c3e0 <__gmpn_mod_1@plt>
   2256c:	cbz	x0, 22468 <__gmpz_stronglucas@@Base+0x248>
   22570:	mov	x1, x23
   22574:	mov	w2, wzr
   22578:	bl	c730 <__gmpn_jacobi_base@plt>
   2257c:	cmp	w0, #0x1
   22580:	b.eq	22550 <__gmpz_stronglucas@@Base+0x330>  // b.none
   22584:	b	22350 <__gmpz_stronglucas@@Base+0x130>
   22588:	mov	w23, #0x13                  	// #19
   2258c:	sub	x8, x23, #0x2
   22590:	cmp	x8, x24
   22594:	b.cs	225d8 <__gmpz_stronglucas@@Base+0x3b8>  // b.hs, b.nlast
   22598:	mov	x0, x21
   2259c:	mov	x1, x22
   225a0:	mov	x2, x23
   225a4:	mov	x3, xzr
   225a8:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   225ac:	cbz	x0, 22468 <__gmpz_stronglucas@@Base+0x248>
   225b0:	mov	x1, x23
   225b4:	mov	w2, w23
   225b8:	bl	c730 <__gmpn_jacobi_base@plt>
   225bc:	cmp	w0, #0x1
   225c0:	add	x23, x23, #0x2
   225c4:	b.eq	2258c <__gmpz_stronglucas@@Base+0x36c>  // b.none
   225c8:	sub	x23, x23, #0x2
   225cc:	b	22350 <__gmpz_stronglucas@@Base+0x130>
   225d0:	mov	x21, xzr
   225d4:	b	22450 <__gmpz_stronglucas@@Base+0x230>
   225d8:	mov	w0, #0x1                   	// #1
   225dc:	b	22468 <__gmpz_stronglucas@@Base+0x248>
   225e0:	mvn	w8, w22
   225e4:	lsr	w0, w8, #31
   225e8:	cbz	w0, 224d4 <__gmpz_stronglucas@@Base+0x2b4>
   225ec:	mov	w0, wzr
   225f0:	b	22468 <__gmpz_stronglucas@@Base+0x248>

00000000000225f4 <__gmpz_sub@@Base>:
   225f4:	stp	x29, x30, [sp, #-80]!
   225f8:	stp	x26, x25, [sp, #16]
   225fc:	stp	x24, x23, [sp, #32]
   22600:	stp	x22, x21, [sp, #48]
   22604:	stp	x20, x19, [sp, #64]
   22608:	ldrsw	x8, [x2, #4]
   2260c:	ldrsw	x9, [x1, #4]
   22610:	ldrsw	x10, [x0]
   22614:	mov	x19, x0
   22618:	neg	x11, x8
   2261c:	cmp	x9, #0x0
   22620:	cneg	x12, x9, mi  // mi = first
   22624:	cmp	x11, #0x0
   22628:	cneg	x11, x8, pl  // pl = nfrst
   2262c:	cmp	x12, x11
   22630:	csel	x20, x11, x12, lt  // lt = tstop
   22634:	csel	x23, x12, x11, lt  // lt = tstop
   22638:	csneg	x25, x9, x8, lt  // lt = tstop
   2263c:	csneg	x24, x9, x8, ge  // ge = tcont
   22640:	csel	x26, x1, x2, lt  // lt = tstop
   22644:	csel	x22, x2, x1, lt  // lt = tstop
   22648:	cmp	x20, x10
   2264c:	mov	x29, sp
   22650:	b.ge	22758 <__gmpz_sub@@Base+0x164>  // b.tcont
   22654:	ldr	x21, [x19, #8]
   22658:	ldr	x22, [x22, #8]
   2265c:	ldr	x2, [x26, #8]
   22660:	eor	x8, x24, x25
   22664:	tbnz	x8, #63, 22778 <__gmpz_sub@@Base+0x184>
   22668:	cbz	x23, 226a4 <__gmpz_sub@@Base+0xb0>
   2266c:	mov	x0, x21
   22670:	mov	x1, x22
   22674:	mov	x3, x23
   22678:	bl	ca70 <__gmpn_add_n@plt>
   2267c:	cbz	x0, 226a4 <__gmpz_sub@@Base+0xb0>
   22680:	mov	w9, #0x1                   	// #1
   22684:	cmp	x23, x20
   22688:	b.ge	22744 <__gmpz_sub@@Base+0x150>  // b.tcont
   2268c:	lsl	x8, x23, #3
   22690:	ldr	x10, [x22, x8]
   22694:	add	x23, x23, #0x1
   22698:	adds	x10, x10, #0x1
   2269c:	str	x10, [x21, x8]
   226a0:	b.cs	22684 <__gmpz_sub@@Base+0x90>  // b.hs, b.nlast
   226a4:	cmp	x21, x22
   226a8:	mov	x9, xzr
   226ac:	b.eq	22744 <__gmpz_sub@@Base+0x150>  // b.none
   226b0:	subs	x8, x20, x23
   226b4:	b.le	22744 <__gmpz_sub@@Base+0x150>
   226b8:	cmp	x8, #0x4
   226bc:	b.cc	22720 <__gmpz_sub@@Base+0x12c>  // b.lo, b.ul, b.last
   226c0:	lsl	x10, x23, #3
   226c4:	lsl	x9, x20, #3
   226c8:	add	x11, x21, x10
   226cc:	add	x12, x22, x9
   226d0:	cmp	x11, x12
   226d4:	b.cs	226e8 <__gmpz_sub@@Base+0xf4>  // b.hs, b.nlast
   226d8:	add	x9, x21, x9
   226dc:	add	x11, x22, x10
   226e0:	cmp	x11, x9
   226e4:	b.cc	22720 <__gmpz_sub@@Base+0x12c>  // b.lo, b.ul, b.last
   226e8:	and	x9, x8, #0xfffffffffffffffc
   226ec:	add	x11, x10, #0x10
   226f0:	add	x23, x23, x9
   226f4:	add	x10, x22, x11
   226f8:	add	x11, x21, x11
   226fc:	mov	x12, x9
   22700:	ldp	q0, q1, [x10, #-16]
   22704:	add	x10, x10, #0x20
   22708:	subs	x12, x12, #0x4
   2270c:	stp	q0, q1, [x11, #-16]
   22710:	add	x11, x11, #0x20
   22714:	b.ne	22700 <__gmpz_sub@@Base+0x10c>  // b.any
   22718:	cmp	x8, x9
   2271c:	b.eq	22740 <__gmpz_sub@@Base+0x14c>  // b.none
   22720:	lsl	x10, x23, #3
   22724:	sub	x8, x20, x23
   22728:	add	x9, x21, x10
   2272c:	add	x10, x22, x10
   22730:	ldr	x11, [x10], #8
   22734:	subs	x8, x8, #0x1
   22738:	str	x11, [x9], #8
   2273c:	b.ne	22730 <__gmpz_sub@@Base+0x13c>  // b.any
   22740:	mov	x9, xzr
   22744:	add	x8, x9, x20
   22748:	cmp	x24, #0x0
   2274c:	cneg	x8, x8, lt  // lt = tstop
   22750:	str	x9, [x21, x20, lsl #3]
   22754:	b	228c8 <__gmpz_sub@@Base+0x2d4>
   22758:	add	x1, x20, #0x1
   2275c:	mov	x0, x19
   22760:	bl	c080 <__gmpz_realloc@plt>
   22764:	mov	x21, x0
   22768:	ldr	x22, [x22, #8]
   2276c:	ldr	x2, [x26, #8]
   22770:	eor	x8, x24, x25
   22774:	tbz	x8, #63, 22668 <__gmpz_sub@@Base+0x74>
   22778:	cmp	x20, x23
   2277c:	b.ne	227d8 <__gmpz_sub@@Base+0x1e4>  // b.any
   22780:	sub	x8, x20, #0x1
   22784:	add	x9, x8, #0x1
   22788:	cmp	x9, #0x1
   2278c:	b.lt	227ac <__gmpz_sub@@Base+0x1b8>  // b.tstop
   22790:	lsl	x9, x8, #3
   22794:	ldr	x10, [x22, x9]
   22798:	ldr	x9, [x2, x9]
   2279c:	sub	x8, x8, #0x1
   227a0:	cmp	x10, x9
   227a4:	b.eq	22784 <__gmpz_sub@@Base+0x190>  // b.none
   227a8:	b.ls	228e4 <__gmpz_sub@@Base+0x2f0>  // b.plast
   227ac:	mov	x0, x21
   227b0:	mov	x1, x22
   227b4:	mov	x3, x20
   227b8:	bl	c2d0 <__gmpn_sub_n@plt>
   227bc:	sub	x8, x21, #0x8
   227c0:	mov	x9, x20
   227c4:	subs	x20, x20, #0x1
   227c8:	b.lt	228c0 <__gmpz_sub@@Base+0x2cc>  // b.tstop
   227cc:	ldr	x10, [x8, x9, lsl #3]
   227d0:	cbz	x10, 227c0 <__gmpz_sub@@Base+0x1cc>
   227d4:	b	228c0 <__gmpz_sub@@Base+0x2cc>
   227d8:	cbz	x23, 22810 <__gmpz_sub@@Base+0x21c>
   227dc:	mov	x0, x21
   227e0:	mov	x1, x22
   227e4:	mov	x3, x23
   227e8:	bl	c2d0 <__gmpn_sub_n@plt>
   227ec:	cbz	x0, 22810 <__gmpz_sub@@Base+0x21c>
   227f0:	cmp	x23, x20
   227f4:	b.ge	228a8 <__gmpz_sub@@Base+0x2b4>  // b.tcont
   227f8:	lsl	x8, x23, #3
   227fc:	ldr	x9, [x22, x8]
   22800:	add	x23, x23, #0x1
   22804:	sub	x10, x9, #0x1
   22808:	str	x10, [x21, x8]
   2280c:	cbz	x9, 227f0 <__gmpz_sub@@Base+0x1fc>
   22810:	cmp	x21, x22
   22814:	b.eq	228a8 <__gmpz_sub@@Base+0x2b4>  // b.none
   22818:	subs	x8, x20, x23
   2281c:	b.le	228a8 <__gmpz_sub@@Base+0x2b4>
   22820:	cmp	x8, #0x4
   22824:	b.cc	22888 <__gmpz_sub@@Base+0x294>  // b.lo, b.ul, b.last
   22828:	lsl	x10, x23, #3
   2282c:	lsl	x9, x20, #3
   22830:	add	x11, x21, x10
   22834:	add	x12, x22, x9
   22838:	cmp	x11, x12
   2283c:	b.cs	22850 <__gmpz_sub@@Base+0x25c>  // b.hs, b.nlast
   22840:	add	x9, x21, x9
   22844:	add	x11, x22, x10
   22848:	cmp	x11, x9
   2284c:	b.cc	22888 <__gmpz_sub@@Base+0x294>  // b.lo, b.ul, b.last
   22850:	and	x9, x8, #0xfffffffffffffffc
   22854:	add	x11, x10, #0x10
   22858:	add	x23, x23, x9
   2285c:	add	x10, x22, x11
   22860:	add	x11, x21, x11
   22864:	mov	x12, x9
   22868:	ldp	q0, q1, [x10, #-16]
   2286c:	add	x10, x10, #0x20
   22870:	subs	x12, x12, #0x4
   22874:	stp	q0, q1, [x11, #-16]
   22878:	add	x11, x11, #0x20
   2287c:	b.ne	22868 <__gmpz_sub@@Base+0x274>  // b.any
   22880:	cmp	x8, x9
   22884:	b.eq	228a8 <__gmpz_sub@@Base+0x2b4>  // b.none
   22888:	lsl	x10, x23, #3
   2288c:	sub	x8, x20, x23
   22890:	add	x9, x21, x10
   22894:	add	x10, x22, x10
   22898:	ldr	x11, [x10], #8
   2289c:	subs	x8, x8, #0x1
   228a0:	str	x11, [x9], #8
   228a4:	b.ne	22898 <__gmpz_sub@@Base+0x2a4>  // b.any
   228a8:	sub	x8, x21, #0x8
   228ac:	mov	x9, x20
   228b0:	subs	x20, x20, #0x1
   228b4:	b.lt	228c0 <__gmpz_sub@@Base+0x2cc>  // b.tstop
   228b8:	ldr	x10, [x8, x9, lsl #3]
   228bc:	cbz	x10, 228ac <__gmpz_sub@@Base+0x2b8>
   228c0:	cmp	x24, #0x0
   228c4:	cneg	x8, x9, lt  // lt = tstop
   228c8:	str	w8, [x19, #4]
   228cc:	ldp	x20, x19, [sp, #64]
   228d0:	ldp	x22, x21, [sp, #48]
   228d4:	ldp	x24, x23, [sp, #32]
   228d8:	ldp	x26, x25, [sp, #16]
   228dc:	ldp	x29, x30, [sp], #80
   228e0:	ret
   228e4:	mov	x0, x21
   228e8:	mov	x1, x2
   228ec:	mov	x2, x22
   228f0:	mov	x3, x20
   228f4:	bl	c2d0 <__gmpn_sub_n@plt>
   228f8:	sub	x8, x21, #0x8
   228fc:	mov	x9, x20
   22900:	subs	x20, x20, #0x1
   22904:	b.lt	22910 <__gmpz_sub@@Base+0x31c>  // b.tstop
   22908:	ldr	x10, [x8, x9, lsl #3]
   2290c:	cbz	x10, 228fc <__gmpz_sub@@Base+0x308>
   22910:	cmp	x24, #0x0
   22914:	cneg	x8, x9, ge  // ge = tcont
   22918:	b	228c8 <__gmpz_sub@@Base+0x2d4>

000000000002291c <__gmpz_sub_ui@@Base>:
   2291c:	stp	x29, x30, [sp, #-64]!
   22920:	stp	x22, x21, [sp, #32]
   22924:	stp	x20, x19, [sp, #48]
   22928:	str	x23, [sp, #16]
   2292c:	ldrsw	x23, [x1, #4]
   22930:	mov	x20, x2
   22934:	mov	x19, x0
   22938:	mov	x29, sp
   2293c:	cbz	w23, 22988 <__gmpz_sub_ui@@Base+0x6c>
   22940:	ldrsw	x8, [x19]
   22944:	cmp	x23, #0x0
   22948:	cneg	x22, x23, mi  // mi = first
   2294c:	mov	x21, x1
   22950:	cmp	x22, x8
   22954:	b.ge	22b4c <__gmpz_sub_ui@@Base+0x230>  // b.tcont
   22958:	ldr	x0, [x19, #8]
   2295c:	ldr	x8, [x21, #8]
   22960:	tbnz	w23, #31, 22b60 <__gmpz_sub_ui@@Base+0x244>
   22964:	ldr	x10, [x8]
   22968:	subs	x9, x22, #0x1
   2296c:	b.ne	229a8 <__gmpz_sub_ui@@Base+0x8c>  // b.any
   22970:	subs	x8, x10, x20
   22974:	b.cs	22a94 <__gmpz_sub_ui@@Base+0x178>  // b.hs, b.nlast
   22978:	sub	x8, x20, x10
   2297c:	str	x8, [x0]
   22980:	mov	x8, #0xffffffffffffffff    	// #-1
   22984:	b	22cd4 <__gmpz_sub_ui@@Base+0x3b8>
   22988:	ldr	w8, [x19]
   2298c:	cmp	w8, #0x0
   22990:	b.le	22d24 <__gmpz_sub_ui@@Base+0x408>
   22994:	ldr	x0, [x19, #8]
   22998:	cmp	x20, #0x0
   2299c:	str	x20, [x0]
   229a0:	csetm	w8, ne  // ne = any
   229a4:	b	22cd4 <__gmpz_sub_ui@@Base+0x3b8>
   229a8:	subs	x10, x10, x20
   229ac:	str	x10, [x0]
   229b0:	b.cs	22a9c <__gmpz_sub_ui@@Base+0x180>  // b.hs, b.nlast
   229b4:	mov	x11, xzr
   229b8:	mov	w10, #0x1                   	// #1
   229bc:	cmp	x10, x22
   229c0:	b.ge	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.tcont
   229c4:	add	x12, x8, x11
   229c8:	ldr	x12, [x12, #8]
   229cc:	add	x13, x0, x11
   229d0:	add	x10, x10, #0x1
   229d4:	add	x11, x11, #0x8
   229d8:	sub	x14, x12, #0x1
   229dc:	sub	x9, x9, #0x1
   229e0:	str	x14, [x13, #8]
   229e4:	cbz	x12, 229bc <__gmpz_sub_ui@@Base+0xa0>
   229e8:	cmp	x8, x0
   229ec:	b.eq	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.none
   229f0:	subs	x12, x22, x10
   229f4:	b.le	22afc <__gmpz_sub_ui@@Base+0x1e0>
   229f8:	cmp	x12, #0x4
   229fc:	b.cc	22a70 <__gmpz_sub_ui@@Base+0x154>  // b.lo, b.ul, b.last
   22a00:	add	x14, x0, x11
   22a04:	lsl	x13, x22, #3
   22a08:	add	x14, x14, #0x8
   22a0c:	add	x15, x8, x13
   22a10:	cmp	x14, x15
   22a14:	b.cs	22a2c <__gmpz_sub_ui@@Base+0x110>  // b.hs, b.nlast
   22a18:	add	x14, x8, x11
   22a1c:	add	x13, x0, x13
   22a20:	add	x14, x14, #0x8
   22a24:	cmp	x14, x13
   22a28:	b.cc	22a70 <__gmpz_sub_ui@@Base+0x154>  // b.lo, b.ul, b.last
   22a2c:	sub	x14, x22, x10
   22a30:	add	x13, x0, x11
   22a34:	add	x15, x8, x11
   22a38:	and	x16, x9, #0xfffffffffffffffc
   22a3c:	and	x11, x12, #0xfffffffffffffffc
   22a40:	add	x9, x13, #0x18
   22a44:	add	x13, x15, #0x18
   22a48:	add	x10, x16, x10
   22a4c:	and	x14, x14, #0xfffffffffffffffc
   22a50:	ldp	q0, q1, [x13, #-16]
   22a54:	add	x13, x13, #0x20
   22a58:	subs	x14, x14, #0x4
   22a5c:	stp	q0, q1, [x9, #-16]
   22a60:	add	x9, x9, #0x20
   22a64:	b.ne	22a50 <__gmpz_sub_ui@@Base+0x134>  // b.any
   22a68:	cmp	x12, x11
   22a6c:	b.eq	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.none
   22a70:	lsl	x11, x10, #3
   22a74:	sub	x9, x22, x10
   22a78:	add	x10, x0, x11
   22a7c:	add	x8, x8, x11
   22a80:	ldr	x11, [x8], #8
   22a84:	subs	x9, x9, #0x1
   22a88:	str	x11, [x10], #8
   22a8c:	b.ne	22a80 <__gmpz_sub_ui@@Base+0x164>  // b.any
   22a90:	b	22afc <__gmpz_sub_ui@@Base+0x1e0>
   22a94:	str	x8, [x0]
   22a98:	b	22afc <__gmpz_sub_ui@@Base+0x1e0>
   22a9c:	cmp	x22, #0x2
   22aa0:	b.lt	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.tstop
   22aa4:	cmp	x8, x0
   22aa8:	b.eq	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.none
   22aac:	cmp	x9, #0x4
   22ab0:	b.cc	22ad8 <__gmpz_sub_ui@@Base+0x1bc>  // b.lo, b.ul, b.last
   22ab4:	lsl	x10, x22, #3
   22ab8:	add	x11, x0, #0x8
   22abc:	add	x12, x8, x10
   22ac0:	cmp	x11, x12
   22ac4:	b.cs	22b14 <__gmpz_sub_ui@@Base+0x1f8>  // b.hs, b.nlast
   22ac8:	add	x10, x0, x10
   22acc:	add	x11, x8, #0x8
   22ad0:	cmp	x11, x10
   22ad4:	b.cs	22b14 <__gmpz_sub_ui@@Base+0x1f8>  // b.hs, b.nlast
   22ad8:	mov	w10, #0x1                   	// #1
   22adc:	lsl	x11, x10, #3
   22ae0:	sub	x9, x22, x10
   22ae4:	add	x10, x0, x11
   22ae8:	add	x8, x8, x11
   22aec:	ldr	x11, [x8], #8
   22af0:	subs	x9, x9, #0x1
   22af4:	str	x11, [x10], #8
   22af8:	b.ne	22aec <__gmpz_sub_ui@@Base+0x1d0>  // b.any
   22afc:	add	x8, x0, x22, lsl #3
   22b00:	ldur	x8, [x8, #-8]
   22b04:	cmp	x8, #0x0
   22b08:	cset	w8, eq  // eq = none
   22b0c:	sub	x8, x22, x8
   22b10:	b	22cd4 <__gmpz_sub_ui@@Base+0x3b8>
   22b14:	and	x11, x9, #0xfffffffffffffffc
   22b18:	add	x12, x8, #0x18
   22b1c:	orr	x10, x11, #0x1
   22b20:	add	x13, x0, #0x18
   22b24:	mov	x14, x11
   22b28:	ldp	q0, q1, [x12, #-16]
   22b2c:	add	x12, x12, #0x20
   22b30:	subs	x14, x14, #0x4
   22b34:	stp	q0, q1, [x13, #-16]
   22b38:	add	x13, x13, #0x20
   22b3c:	b.ne	22b28 <__gmpz_sub_ui@@Base+0x20c>  // b.any
   22b40:	cmp	x9, x11
   22b44:	b.eq	22afc <__gmpz_sub_ui@@Base+0x1e0>  // b.none
   22b48:	b	22adc <__gmpz_sub_ui@@Base+0x1c0>
   22b4c:	add	x1, x22, #0x1
   22b50:	mov	x0, x19
   22b54:	bl	c080 <__gmpz_realloc@plt>
   22b58:	ldr	x8, [x21, #8]
   22b5c:	tbz	w23, #31, 22964 <__gmpz_sub_ui@@Base+0x48>
   22b60:	ldr	x9, [x8]
   22b64:	adds	x9, x9, x20
   22b68:	str	x9, [x0]
   22b6c:	b.cc	22c5c <__gmpz_sub_ui@@Base+0x340>  // b.lo, b.ul, b.last
   22b70:	mov	x11, xzr
   22b74:	sub	x10, x22, #0x1
   22b78:	mov	w12, #0x1                   	// #1
   22b7c:	mov	w9, #0x1                   	// #1
   22b80:	cmp	x9, x22
   22b84:	b.ge	22cc8 <__gmpz_sub_ui@@Base+0x3ac>  // b.tcont
   22b88:	add	x13, x8, x11
   22b8c:	ldr	x13, [x13, #8]
   22b90:	add	x14, x0, x11
   22b94:	add	x9, x9, #0x1
   22b98:	add	x11, x11, #0x8
   22b9c:	adds	x13, x13, #0x1
   22ba0:	sub	x10, x10, #0x1
   22ba4:	str	x13, [x14, #8]
   22ba8:	b.cs	22b80 <__gmpz_sub_ui@@Base+0x264>  // b.hs, b.nlast
   22bac:	cmp	x8, x0
   22bb0:	mov	x12, xzr
   22bb4:	b.eq	22cc8 <__gmpz_sub_ui@@Base+0x3ac>  // b.none
   22bb8:	subs	x13, x22, x9
   22bbc:	b.le	22cc8 <__gmpz_sub_ui@@Base+0x3ac>
   22bc0:	cmp	x13, #0x4
   22bc4:	b.cc	22c38 <__gmpz_sub_ui@@Base+0x31c>  // b.lo, b.ul, b.last
   22bc8:	add	x14, x0, x11
   22bcc:	lsl	x12, x22, #3
   22bd0:	add	x14, x14, #0x8
   22bd4:	add	x15, x8, x12
   22bd8:	cmp	x14, x15
   22bdc:	b.cs	22bf4 <__gmpz_sub_ui@@Base+0x2d8>  // b.hs, b.nlast
   22be0:	add	x14, x8, x11
   22be4:	add	x12, x0, x12
   22be8:	add	x14, x14, #0x8
   22bec:	cmp	x14, x12
   22bf0:	b.cc	22c38 <__gmpz_sub_ui@@Base+0x31c>  // b.lo, b.ul, b.last
   22bf4:	sub	x14, x22, x9
   22bf8:	add	x12, x0, x11
   22bfc:	add	x15, x8, x11
   22c00:	and	x16, x10, #0xfffffffffffffffc
   22c04:	and	x11, x13, #0xfffffffffffffffc
   22c08:	add	x10, x12, #0x18
   22c0c:	add	x12, x15, #0x18
   22c10:	add	x9, x16, x9
   22c14:	and	x14, x14, #0xfffffffffffffffc
   22c18:	ldp	q0, q1, [x12, #-16]
   22c1c:	add	x12, x12, #0x20
   22c20:	subs	x14, x14, #0x4
   22c24:	stp	q0, q1, [x10, #-16]
   22c28:	add	x10, x10, #0x20
   22c2c:	b.ne	22c18 <__gmpz_sub_ui@@Base+0x2fc>  // b.any
   22c30:	cmp	x13, x11
   22c34:	b.eq	22cc4 <__gmpz_sub_ui@@Base+0x3a8>  // b.none
   22c38:	lsl	x11, x9, #3
   22c3c:	sub	x10, x22, x9
   22c40:	add	x9, x0, x11
   22c44:	add	x8, x8, x11
   22c48:	ldr	x11, [x8], #8
   22c4c:	subs	x10, x10, #0x1
   22c50:	str	x11, [x9], #8
   22c54:	b.ne	22c48 <__gmpz_sub_ui@@Base+0x32c>  // b.any
   22c58:	b	22cc4 <__gmpz_sub_ui@@Base+0x3a8>
   22c5c:	cmp	x22, #0x2
   22c60:	mov	x12, xzr
   22c64:	b.lt	22cc8 <__gmpz_sub_ui@@Base+0x3ac>  // b.tstop
   22c68:	cmp	x8, x0
   22c6c:	b.eq	22cc8 <__gmpz_sub_ui@@Base+0x3ac>  // b.none
   22c70:	sub	x9, x22, #0x1
   22c74:	cmp	x9, #0x4
   22c78:	b.cc	22ca0 <__gmpz_sub_ui@@Base+0x384>  // b.lo, b.ul, b.last
   22c7c:	lsl	x10, x22, #3
   22c80:	add	x11, x0, #0x8
   22c84:	add	x12, x8, x10
   22c88:	cmp	x11, x12
   22c8c:	b.cs	22cec <__gmpz_sub_ui@@Base+0x3d0>  // b.hs, b.nlast
   22c90:	add	x10, x0, x10
   22c94:	add	x11, x8, #0x8
   22c98:	cmp	x11, x10
   22c9c:	b.cs	22cec <__gmpz_sub_ui@@Base+0x3d0>  // b.hs, b.nlast
   22ca0:	mov	w10, #0x1                   	// #1
   22ca4:	lsl	x11, x10, #3
   22ca8:	sub	x9, x22, x10
   22cac:	add	x10, x0, x11
   22cb0:	add	x8, x8, x11
   22cb4:	ldr	x11, [x8], #8
   22cb8:	subs	x9, x9, #0x1
   22cbc:	str	x11, [x10], #8
   22cc0:	b.ne	22cb4 <__gmpz_sub_ui@@Base+0x398>  // b.any
   22cc4:	mov	x12, xzr
   22cc8:	add	x8, x22, x12
   22ccc:	str	x12, [x0, x22, lsl #3]
   22cd0:	neg	x8, x8
   22cd4:	str	w8, [x19, #4]
   22cd8:	ldp	x20, x19, [sp, #48]
   22cdc:	ldp	x22, x21, [sp, #32]
   22ce0:	ldr	x23, [sp, #16]
   22ce4:	ldp	x29, x30, [sp], #64
   22ce8:	ret
   22cec:	and	x11, x9, #0xfffffffffffffffc
   22cf0:	add	x12, x8, #0x18
   22cf4:	orr	x10, x11, #0x1
   22cf8:	add	x13, x0, #0x18
   22cfc:	mov	x14, x11
   22d00:	ldp	q0, q1, [x12, #-16]
   22d04:	add	x12, x12, #0x20
   22d08:	subs	x14, x14, #0x4
   22d0c:	stp	q0, q1, [x13, #-16]
   22d10:	add	x13, x13, #0x20
   22d14:	b.ne	22d00 <__gmpz_sub_ui@@Base+0x3e4>  // b.any
   22d18:	cmp	x9, x11
   22d1c:	b.eq	22cc4 <__gmpz_sub_ui@@Base+0x3a8>  // b.none
   22d20:	b	22ca4 <__gmpz_sub_ui@@Base+0x388>
   22d24:	mov	w1, #0x1                   	// #1
   22d28:	mov	x0, x19
   22d2c:	bl	c080 <__gmpz_realloc@plt>
   22d30:	b	22998 <__gmpz_sub_ui@@Base+0x7c>

0000000000022d34 <__gmpz_swap@@Base>:
   22d34:	ldr	x8, [x1]
   22d38:	ldr	x9, [x0]
   22d3c:	str	x8, [x0]
   22d40:	str	x9, [x1]
   22d44:	ldr	x8, [x0, #8]
   22d48:	ldr	x9, [x1, #8]
   22d4c:	str	x8, [x1, #8]
   22d50:	str	x9, [x0, #8]
   22d54:	ret

0000000000022d58 <__gmpz_tdiv_ui@@Base>:
   22d58:	stp	x29, x30, [sp, #-16]!
   22d5c:	mov	x29, sp
   22d60:	cbz	x1, 22d90 <__gmpz_tdiv_ui@@Base+0x38>
   22d64:	ldrsw	x8, [x0, #4]
   22d68:	cbz	w8, 22d84 <__gmpz_tdiv_ui@@Base+0x2c>
   22d6c:	ldr	x0, [x0, #8]
   22d70:	cmp	x8, #0x0
   22d74:	mov	x2, x1
   22d78:	cneg	x1, x8, mi  // mi = first
   22d7c:	ldp	x29, x30, [sp], #16
   22d80:	b	c3e0 <__gmpn_mod_1@plt>
   22d84:	mov	x0, xzr
   22d88:	ldp	x29, x30, [sp], #16
   22d8c:	ret
   22d90:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022d94 <__gmpz_tdiv_q@@Base>:
   22d94:	stp	x29, x30, [sp, #-96]!
   22d98:	stp	x28, x27, [sp, #16]
   22d9c:	stp	x26, x25, [sp, #32]
   22da0:	stp	x24, x23, [sp, #48]
   22da4:	stp	x22, x21, [sp, #64]
   22da8:	stp	x20, x19, [sp, #80]
   22dac:	mov	x29, sp
   22db0:	sub	sp, sp, #0x10
   22db4:	ldrsw	x27, [x1, #4]
   22db8:	ldrsw	x28, [x2, #4]
   22dbc:	cmp	x27, #0x0
   22dc0:	cneg	x20, x27, mi  // mi = first
   22dc4:	cmp	x28, #0x0
   22dc8:	cneg	x21, x28, mi  // mi = first
   22dcc:	cbz	x21, 22f3c <__gmpz_tdiv_q@@Base+0x1a8>
   22dd0:	mov	x19, x0
   22dd4:	sub	x22, x20, x21
   22dd8:	tbnz	x22, #63, 22ecc <__gmpz_tdiv_q@@Base+0x138>
   22ddc:	ldrsw	x8, [x19]
   22de0:	mov	x23, x1
   22de4:	mov	x25, x2
   22de8:	add	x1, x22, #0x1
   22dec:	cmp	x22, x8
   22df0:	stur	x1, [x29, #-16]
   22df4:	b.ge	22ef0 <__gmpz_tdiv_q@@Base+0x15c>  // b.tcont
   22df8:	ldr	x24, [x19, #8]
   22dfc:	stur	xzr, [x29, #-8]
   22e00:	ldr	x25, [x25, #8]
   22e04:	cmp	x25, x24
   22e08:	b.ne	22e3c <__gmpz_tdiv_q@@Base+0xa8>  // b.any
   22e0c:	cmp	x21, #0xfe0
   22e10:	lsl	x1, x21, #3
   22e14:	b.hi	22f2c <__gmpz_tdiv_q@@Base+0x198>  // b.pmore
   22e18:	add	x9, x1, #0xf
   22e1c:	mov	x8, sp
   22e20:	and	x9, x9, #0xfffffffffffffff0
   22e24:	sub	x25, x8, x9
   22e28:	mov	sp, x25
   22e2c:	mov	x0, x25
   22e30:	mov	x1, x24
   22e34:	mov	x2, x21
   22e38:	bl	ca50 <__gmpn_copyi@plt>
   22e3c:	lsl	x8, x20, #3
   22e40:	cmp	x20, #0xfdf
   22e44:	add	x1, x8, #0x8
   22e48:	b.hi	22f10 <__gmpz_tdiv_q@@Base+0x17c>  // b.pmore
   22e4c:	add	x9, x1, #0xf
   22e50:	mov	x8, sp
   22e54:	and	x9, x9, #0xfffffffffffffff0
   22e58:	sub	x26, x8, x9
   22e5c:	mov	sp, x26
   22e60:	ldr	x1, [x23, #8]
   22e64:	cmp	x1, x24
   22e68:	b.ne	22e80 <__gmpz_tdiv_q@@Base+0xec>  // b.any
   22e6c:	mov	x0, x26
   22e70:	mov	x1, x24
   22e74:	mov	x2, x20
   22e78:	bl	ca50 <__gmpn_copyi@plt>
   22e7c:	mov	x1, x26
   22e80:	mov	x0, x24
   22e84:	mov	x2, x20
   22e88:	mov	x3, x25
   22e8c:	mov	x4, x21
   22e90:	mov	x5, x26
   22e94:	bl	c320 <__gmpn_div_q@plt>
   22e98:	ldr	x8, [x24, x22, lsl #3]
   22e9c:	ldp	x10, x0, [x29, #-16]
   22ea0:	eor	w9, w28, w27
   22ea4:	cmp	x8, #0x0
   22ea8:	cset	w8, eq  // eq = none
   22eac:	sub	x8, x10, x8
   22eb0:	neg	w10, w8
   22eb4:	cmp	w9, #0x0
   22eb8:	csel	x8, x8, x10, ge  // ge = tcont
   22ebc:	str	w8, [x19, #4]
   22ec0:	cbz	x0, 22ed0 <__gmpz_tdiv_q@@Base+0x13c>
   22ec4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   22ec8:	b	22ed0 <__gmpz_tdiv_q@@Base+0x13c>
   22ecc:	str	wzr, [x19, #4]
   22ed0:	mov	sp, x29
   22ed4:	ldp	x20, x19, [sp, #80]
   22ed8:	ldp	x22, x21, [sp, #64]
   22edc:	ldp	x24, x23, [sp, #48]
   22ee0:	ldp	x26, x25, [sp, #32]
   22ee4:	ldp	x28, x27, [sp, #16]
   22ee8:	ldp	x29, x30, [sp], #96
   22eec:	ret
   22ef0:	mov	x0, x19
   22ef4:	bl	c080 <__gmpz_realloc@plt>
   22ef8:	mov	x24, x0
   22efc:	stur	xzr, [x29, #-8]
   22f00:	ldr	x25, [x25, #8]
   22f04:	cmp	x25, x24
   22f08:	b.ne	22e3c <__gmpz_tdiv_q@@Base+0xa8>  // b.any
   22f0c:	b	22e0c <__gmpz_tdiv_q@@Base+0x78>
   22f10:	sub	x0, x29, #0x8
   22f14:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22f18:	mov	x26, x0
   22f1c:	ldr	x1, [x23, #8]
   22f20:	cmp	x1, x24
   22f24:	b.ne	22e80 <__gmpz_tdiv_q@@Base+0xec>  // b.any
   22f28:	b	22e6c <__gmpz_tdiv_q@@Base+0xd8>
   22f2c:	sub	x0, x29, #0x8
   22f30:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22f34:	mov	x25, x0
   22f38:	b	22e2c <__gmpz_tdiv_q@@Base+0x98>
   22f3c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022f40 <__gmpz_tdiv_q_2exp@@Base>:
   22f40:	stp	x29, x30, [sp, #-80]!
   22f44:	stp	x24, x23, [sp, #32]
   22f48:	stp	x22, x21, [sp, #48]
   22f4c:	stp	x20, x19, [sp, #64]
   22f50:	ldrsw	x24, [x1, #4]
   22f54:	str	x25, [sp, #16]
   22f58:	lsr	x25, x2, #6
   22f5c:	mov	x19, x0
   22f60:	cmp	x24, #0x0
   22f64:	cneg	x8, x24, mi  // mi = first
   22f68:	sub	x20, x8, x25
   22f6c:	cmp	x20, #0x1
   22f70:	mov	x29, sp
   22f74:	b.lt	22fc4 <__gmpz_tdiv_q_2exp@@Base+0x84>  // b.tstop
   22f78:	ldrsw	x8, [x19]
   22f7c:	mov	x21, x2
   22f80:	mov	x22, x1
   22f84:	cmp	x20, x8
   22f88:	b.gt	22fcc <__gmpz_tdiv_q_2exp@@Base+0x8c>
   22f8c:	ldr	x23, [x19, #8]
   22f90:	ldr	x8, [x22, #8]
   22f94:	ands	x3, x21, #0x3f
   22f98:	add	x1, x8, x25, lsl #3
   22f9c:	b.eq	22fec <__gmpz_tdiv_q_2exp@@Base+0xac>  // b.none
   22fa0:	mov	x0, x23
   22fa4:	mov	x2, x20
   22fa8:	bl	c1a0 <__gmpn_rshift@plt>
   22fac:	add	x8, x23, x20, lsl #3
   22fb0:	ldur	x8, [x8, #-8]
   22fb4:	cmp	x8, #0x0
   22fb8:	cset	w8, eq  // eq = none
   22fbc:	sub	x20, x20, x8
   22fc0:	b	22ff8 <__gmpz_tdiv_q_2exp@@Base+0xb8>
   22fc4:	mov	x20, xzr
   22fc8:	b	22ff8 <__gmpz_tdiv_q_2exp@@Base+0xb8>
   22fcc:	mov	x0, x19
   22fd0:	mov	x1, x20
   22fd4:	bl	c080 <__gmpz_realloc@plt>
   22fd8:	mov	x23, x0
   22fdc:	ldr	x8, [x22, #8]
   22fe0:	ands	x3, x21, #0x3f
   22fe4:	add	x1, x8, x25, lsl #3
   22fe8:	b.ne	22fa0 <__gmpz_tdiv_q_2exp@@Base+0x60>  // b.any
   22fec:	mov	x0, x23
   22ff0:	mov	x2, x20
   22ff4:	bl	ca50 <__gmpn_copyi@plt>
   22ff8:	neg	w8, w20
   22ffc:	cmp	w24, #0x0
   23000:	csel	x8, x20, x8, ge  // ge = tcont
   23004:	str	w8, [x19, #4]
   23008:	ldp	x20, x19, [sp, #64]
   2300c:	ldp	x22, x21, [sp, #48]
   23010:	ldp	x24, x23, [sp, #32]
   23014:	ldr	x25, [sp, #16]
   23018:	ldp	x29, x30, [sp], #80
   2301c:	ret

0000000000023020 <__gmpz_tdiv_q_ui@@Base>:
   23020:	stp	x29, x30, [sp, #-64]!
   23024:	stp	x24, x23, [sp, #16]
   23028:	stp	x22, x21, [sp, #32]
   2302c:	stp	x20, x19, [sp, #48]
   23030:	mov	x29, sp
   23034:	cbz	x2, 230d0 <__gmpz_tdiv_q_ui@@Base+0xb0>
   23038:	ldrsw	x24, [x1, #4]
   2303c:	mov	x22, x1
   23040:	mov	x19, x0
   23044:	cbz	w24, 2309c <__gmpz_tdiv_q_ui@@Base+0x7c>
   23048:	ldrsw	x8, [x19]
   2304c:	cmp	w24, #0x0
   23050:	cneg	x21, x24, lt  // lt = tstop
   23054:	mov	x20, x2
   23058:	cmp	x21, x8
   2305c:	b.gt	230bc <__gmpz_tdiv_q_ui@@Base+0x9c>
   23060:	ldr	x23, [x19, #8]
   23064:	ldr	x2, [x22, #8]
   23068:	mov	x0, x23
   2306c:	mov	x1, xzr
   23070:	mov	x3, x21
   23074:	mov	x4, x20
   23078:	bl	cd00 <__gmpn_divrem_1@plt>
   2307c:	add	x8, x23, x21, lsl #3
   23080:	ldur	x8, [x8, #-8]
   23084:	cmp	x8, #0x0
   23088:	cset	w8, eq  // eq = none
   2308c:	sub	w8, w21, w8
   23090:	cmp	w24, #0x0
   23094:	cneg	w8, w8, lt  // lt = tstop
   23098:	b	230a4 <__gmpz_tdiv_q_ui@@Base+0x84>
   2309c:	mov	w8, wzr
   230a0:	mov	x0, xzr
   230a4:	str	w8, [x19, #4]
   230a8:	ldp	x20, x19, [sp, #48]
   230ac:	ldp	x22, x21, [sp, #32]
   230b0:	ldp	x24, x23, [sp, #16]
   230b4:	ldp	x29, x30, [sp], #64
   230b8:	ret
   230bc:	mov	x0, x19
   230c0:	mov	x1, x21
   230c4:	bl	c080 <__gmpz_realloc@plt>
   230c8:	mov	x23, x0
   230cc:	b	23064 <__gmpz_tdiv_q_ui@@Base+0x44>
   230d0:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000230d4 <__gmpz_tdiv_qr@@Base>:
   230d4:	stp	x29, x30, [sp, #-96]!
   230d8:	stp	x28, x27, [sp, #16]
   230dc:	stp	x26, x25, [sp, #32]
   230e0:	stp	x24, x23, [sp, #48]
   230e4:	stp	x22, x21, [sp, #64]
   230e8:	stp	x20, x19, [sp, #80]
   230ec:	mov	x29, sp
   230f0:	sub	sp, sp, #0x20
   230f4:	ldrsw	x24, [x2, #4]
   230f8:	ldrsw	x28, [x3, #4]
   230fc:	cmp	x24, #0x0
   23100:	cneg	x22, x24, mi  // mi = first
   23104:	cmp	x28, #0x0
   23108:	cneg	x21, x28, mi  // mi = first
   2310c:	cbz	x21, 23300 <__gmpz_tdiv_qr@@Base+0x22c>
   23110:	ldrsw	x8, [x1]
   23114:	mov	x26, x3
   23118:	mov	x27, x2
   2311c:	mov	x19, x1
   23120:	mov	x25, x0
   23124:	cmp	x21, x8
   23128:	sub	x20, x22, x21
   2312c:	b.gt	23274 <__gmpz_tdiv_qr@@Base+0x1a0>
   23130:	ldr	x23, [x19, #8]
   23134:	tbnz	x20, #63, 23288 <__gmpz_tdiv_qr@@Base+0x1b4>
   23138:	ldrsw	x8, [x25]
   2313c:	cmp	x20, x8
   23140:	add	x8, x20, #0x1
   23144:	stp	x25, x8, [x29, #-24]
   23148:	b.ge	232cc <__gmpz_tdiv_qr@@Base+0x1f8>  // b.tcont
   2314c:	ldr	x25, [x25, #8]
   23150:	stur	xzr, [x29, #-8]
   23154:	ldr	x26, [x26, #8]
   23158:	ldr	x27, [x27, #8]
   2315c:	stur	x28, [x29, #-32]
   23160:	cmp	x26, x23
   23164:	b.eq	23170 <__gmpz_tdiv_qr@@Base+0x9c>  // b.none
   23168:	cmp	x26, x25
   2316c:	b.ne	231a8 <__gmpz_tdiv_qr@@Base+0xd4>  // b.any
   23170:	cmp	x21, #0xfe0
   23174:	lsl	x1, x21, #3
   23178:	b.hi	232e0 <__gmpz_tdiv_qr@@Base+0x20c>  // b.pmore
   2317c:	add	x9, x1, #0xf
   23180:	mov	x8, sp
   23184:	and	x9, x9, #0xfffffffffffffff0
   23188:	sub	x28, x8, x9
   2318c:	mov	sp, x28
   23190:	mov	x0, x28
   23194:	mov	x1, x26
   23198:	mov	x2, x21
   2319c:	bl	ca50 <__gmpn_copyi@plt>
   231a0:	mov	x26, x28
   231a4:	ldur	x28, [x29, #-32]
   231a8:	cmp	x27, x23
   231ac:	b.eq	231b8 <__gmpz_tdiv_qr@@Base+0xe4>  // b.none
   231b0:	cmp	x27, x25
   231b4:	b.ne	231f0 <__gmpz_tdiv_qr@@Base+0x11c>  // b.any
   231b8:	cmp	x22, #0xfe0
   231bc:	lsl	x1, x22, #3
   231c0:	b.hi	232f0 <__gmpz_tdiv_qr@@Base+0x21c>  // b.pmore
   231c4:	add	x9, x1, #0xf
   231c8:	mov	x8, sp
   231cc:	and	x9, x9, #0xfffffffffffffff0
   231d0:	sub	x28, x8, x9
   231d4:	mov	sp, x28
   231d8:	mov	x0, x28
   231dc:	mov	x1, x27
   231e0:	mov	x2, x22
   231e4:	bl	ca50 <__gmpn_copyi@plt>
   231e8:	mov	x27, x28
   231ec:	ldur	x28, [x29, #-32]
   231f0:	mov	x0, x25
   231f4:	mov	x1, x23
   231f8:	mov	x2, xzr
   231fc:	mov	x3, x27
   23200:	mov	x4, x22
   23204:	mov	x5, x26
   23208:	mov	x6, x21
   2320c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   23210:	ldr	x8, [x25, x20, lsl #3]
   23214:	ldur	x9, [x29, #-16]
   23218:	sub	x10, x23, #0x8
   2321c:	cmp	x8, #0x0
   23220:	cset	w8, eq  // eq = none
   23224:	sub	x8, x9, x8
   23228:	mov	x9, x21
   2322c:	subs	x21, x21, #0x1
   23230:	b.lt	2323c <__gmpz_tdiv_qr@@Base+0x168>  // b.tstop
   23234:	ldr	x11, [x10, x9, lsl #3]
   23238:	cbz	x11, 23228 <__gmpz_tdiv_qr@@Base+0x154>
   2323c:	eor	w10, w28, w24
   23240:	cmp	w10, #0x0
   23244:	ldur	x10, [x29, #-24]
   23248:	neg	w11, w8
   2324c:	neg	w12, w9
   23250:	csel	x8, x8, x11, ge  // ge = tcont
   23254:	cmp	w24, #0x0
   23258:	str	w8, [x10, #4]
   2325c:	csel	x8, x9, x12, ge  // ge = tcont
   23260:	str	w8, [x19, #4]
   23264:	ldur	x0, [x29, #-8]
   23268:	cbz	x0, 232ac <__gmpz_tdiv_qr@@Base+0x1d8>
   2326c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   23270:	b	232ac <__gmpz_tdiv_qr@@Base+0x1d8>
   23274:	mov	x0, x19
   23278:	mov	x1, x21
   2327c:	bl	c080 <__gmpz_realloc@plt>
   23280:	mov	x23, x0
   23284:	tbz	x20, #63, 23138 <__gmpz_tdiv_qr@@Base+0x64>
   23288:	cmp	x27, x19
   2328c:	b.eq	232a8 <__gmpz_tdiv_qr@@Base+0x1d4>  // b.none
   23290:	ldr	x1, [x27, #8]
   23294:	mov	x0, x23
   23298:	mov	x2, x22
   2329c:	bl	ca50 <__gmpn_copyi@plt>
   232a0:	ldr	w8, [x27, #4]
   232a4:	str	w8, [x19, #4]
   232a8:	str	wzr, [x25, #4]
   232ac:	mov	sp, x29
   232b0:	ldp	x20, x19, [sp, #80]
   232b4:	ldp	x22, x21, [sp, #64]
   232b8:	ldp	x24, x23, [sp, #48]
   232bc:	ldp	x26, x25, [sp, #32]
   232c0:	ldp	x28, x27, [sp, #16]
   232c4:	ldp	x29, x30, [sp], #96
   232c8:	ret
   232cc:	ldur	x1, [x29, #-16]
   232d0:	mov	x0, x25
   232d4:	bl	c080 <__gmpz_realloc@plt>
   232d8:	mov	x25, x0
   232dc:	b	23150 <__gmpz_tdiv_qr@@Base+0x7c>
   232e0:	sub	x0, x29, #0x8
   232e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   232e8:	mov	x28, x0
   232ec:	b	23190 <__gmpz_tdiv_qr@@Base+0xbc>
   232f0:	sub	x0, x29, #0x8
   232f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   232f8:	mov	x28, x0
   232fc:	b	231d8 <__gmpz_tdiv_qr@@Base+0x104>
   23300:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000023304 <__gmpz_tdiv_qr_ui@@Base>:
   23304:	stp	x29, x30, [sp, #-80]!
   23308:	str	x25, [sp, #16]
   2330c:	stp	x24, x23, [sp, #32]
   23310:	stp	x22, x21, [sp, #48]
   23314:	stp	x20, x19, [sp, #64]
   23318:	mov	x29, sp
   2331c:	cbz	x3, 23410 <__gmpz_tdiv_qr_ui@@Base+0x10c>
   23320:	ldrsw	x25, [x2, #4]
   23324:	mov	x24, x2
   23328:	mov	x20, x1
   2332c:	mov	x19, x0
   23330:	cbz	w25, 23398 <__gmpz_tdiv_qr_ui@@Base+0x94>
   23334:	ldrsw	x8, [x19]
   23338:	cmp	w25, #0x0
   2333c:	cneg	x21, x25, lt  // lt = tstop
   23340:	mov	x22, x3
   23344:	cmp	x21, x8
   23348:	b.gt	233ec <__gmpz_tdiv_qr_ui@@Base+0xe8>
   2334c:	ldr	x23, [x19, #8]
   23350:	ldr	x2, [x24, #8]
   23354:	mov	x0, x23
   23358:	mov	x1, xzr
   2335c:	mov	x3, x21
   23360:	mov	x4, x22
   23364:	bl	cd00 <__gmpn_divrem_1@plt>
   23368:	mov	x22, x0
   2336c:	cbz	x0, 233ac <__gmpz_tdiv_qr_ui@@Base+0xa8>
   23370:	ldr	w8, [x20]
   23374:	cmp	w25, #0x0
   23378:	mov	w9, #0x1                   	// #1
   2337c:	cneg	w9, w9, lt  // lt = tstop
   23380:	cmp	w8, #0x0
   23384:	str	w9, [x20, #4]
   23388:	b.le	23400 <__gmpz_tdiv_qr_ui@@Base+0xfc>
   2338c:	ldr	x0, [x20, #8]
   23390:	str	x22, [x0]
   23394:	b	233b0 <__gmpz_tdiv_qr_ui@@Base+0xac>
   23398:	mov	w8, wzr
   2339c:	mov	x22, xzr
   233a0:	str	wzr, [x19, #4]
   233a4:	mov	x19, x20
   233a8:	b	233cc <__gmpz_tdiv_qr_ui@@Base+0xc8>
   233ac:	str	wzr, [x20, #4]
   233b0:	add	x8, x23, x21, lsl #3
   233b4:	ldur	x8, [x8, #-8]
   233b8:	cmp	x8, #0x0
   233bc:	cset	w8, eq  // eq = none
   233c0:	sub	w8, w21, w8
   233c4:	cmp	w25, #0x0
   233c8:	cneg	w8, w8, lt  // lt = tstop
   233cc:	str	w8, [x19, #4]
   233d0:	mov	x0, x22
   233d4:	ldp	x20, x19, [sp, #64]
   233d8:	ldp	x22, x21, [sp, #48]
   233dc:	ldp	x24, x23, [sp, #32]
   233e0:	ldr	x25, [sp, #16]
   233e4:	ldp	x29, x30, [sp], #80
   233e8:	ret
   233ec:	mov	x0, x19
   233f0:	mov	x1, x21
   233f4:	bl	c080 <__gmpz_realloc@plt>
   233f8:	mov	x23, x0
   233fc:	b	23350 <__gmpz_tdiv_qr_ui@@Base+0x4c>
   23400:	mov	w1, #0x1                   	// #1
   23404:	mov	x0, x20
   23408:	bl	c080 <__gmpz_realloc@plt>
   2340c:	b	23390 <__gmpz_tdiv_qr_ui@@Base+0x8c>
   23410:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000023414 <__gmpz_tdiv_r@@Base>:
   23414:	stp	x29, x30, [sp, #-80]!
   23418:	stp	x26, x25, [sp, #16]
   2341c:	stp	x24, x23, [sp, #32]
   23420:	stp	x22, x21, [sp, #48]
   23424:	stp	x20, x19, [sp, #64]
   23428:	mov	x29, sp
   2342c:	sub	sp, sp, #0x10
   23430:	ldrsw	x26, [x1, #4]
   23434:	ldr	w8, [x2, #4]
   23438:	cmp	x26, #0x0
   2343c:	cneg	x20, x26, mi  // mi = first
   23440:	cmp	w8, #0x0
   23444:	cneg	w21, w8, mi  // mi = first
   23448:	cbz	w21, 23628 <__gmpz_tdiv_r@@Base+0x214>
   2344c:	mov	x24, x1
   23450:	mov	x19, x0
   23454:	sub	x23, x20, x21
   23458:	tbnz	x23, #63, 234b4 <__gmpz_tdiv_r@@Base+0xa0>
   2345c:	ldrsw	x8, [x19]
   23460:	mov	x25, x2
   23464:	cmp	x21, x8
   23468:	b.gt	234e0 <__gmpz_tdiv_r@@Base+0xcc>
   2346c:	ldr	x22, [x19, #8]
   23470:	lsl	x8, x23, #3
   23474:	cmp	x23, #0xfdf
   23478:	add	x1, x8, #0x8
   2347c:	stur	xzr, [x29, #-8]
   23480:	b.hi	234f4 <__gmpz_tdiv_r@@Base+0xe0>  // b.pmore
   23484:	add	x9, x1, #0xf
   23488:	mov	x8, sp
   2348c:	and	x9, x9, #0xfffffffffffffff0
   23490:	sub	x23, x8, x9
   23494:	mov	sp, x23
   23498:	ldr	x25, [x25, #8]
   2349c:	ldr	x24, [x24, #8]
   234a0:	cmp	x25, x22
   234a4:	b.eq	23510 <__gmpz_tdiv_r@@Base+0xfc>  // b.none
   234a8:	cmp	x24, x22
   234ac:	b.ne	23578 <__gmpz_tdiv_r@@Base+0x164>  // b.any
   234b0:	b	23548 <__gmpz_tdiv_r@@Base+0x134>
   234b4:	cmp	x24, x19
   234b8:	b.eq	235d4 <__gmpz_tdiv_r@@Base+0x1c0>  // b.none
   234bc:	ldrsw	x8, [x19]
   234c0:	str	w26, [x19, #4]
   234c4:	cmp	x20, x8
   234c8:	b.gt	235f8 <__gmpz_tdiv_r@@Base+0x1e4>
   234cc:	ldr	x0, [x19, #8]
   234d0:	ldr	x1, [x24, #8]
   234d4:	mov	x2, x20
   234d8:	bl	ca50 <__gmpn_copyi@plt>
   234dc:	b	235d4 <__gmpz_tdiv_r@@Base+0x1c0>
   234e0:	mov	x0, x19
   234e4:	mov	x1, x21
   234e8:	bl	c080 <__gmpz_realloc@plt>
   234ec:	mov	x22, x0
   234f0:	b	23470 <__gmpz_tdiv_r@@Base+0x5c>
   234f4:	sub	x0, x29, #0x8
   234f8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   234fc:	mov	x23, x0
   23500:	ldr	x25, [x25, #8]
   23504:	ldr	x24, [x24, #8]
   23508:	cmp	x25, x22
   2350c:	b.ne	234a8 <__gmpz_tdiv_r@@Base+0x94>  // b.any
   23510:	cmp	w21, #0xfe0
   23514:	lsl	x1, x21, #3
   23518:	b.hi	23608 <__gmpz_tdiv_r@@Base+0x1f4>  // b.pmore
   2351c:	add	x9, x1, #0xf
   23520:	mov	x8, sp
   23524:	and	x9, x9, #0xffffffff0
   23528:	sub	x25, x8, x9
   2352c:	mov	sp, x25
   23530:	mov	x0, x25
   23534:	mov	x1, x22
   23538:	mov	x2, x21
   2353c:	bl	ca50 <__gmpn_copyi@plt>
   23540:	cmp	x24, x22
   23544:	b.ne	23578 <__gmpz_tdiv_r@@Base+0x164>  // b.any
   23548:	cmp	x20, #0xfe0
   2354c:	lsl	x1, x20, #3
   23550:	b.hi	23618 <__gmpz_tdiv_r@@Base+0x204>  // b.pmore
   23554:	add	x9, x1, #0xf
   23558:	mov	x8, sp
   2355c:	and	x9, x9, #0xfffffffffffffff0
   23560:	sub	x24, x8, x9
   23564:	mov	sp, x24
   23568:	mov	x0, x24
   2356c:	mov	x1, x22
   23570:	mov	x2, x20
   23574:	bl	ca50 <__gmpn_copyi@plt>
   23578:	mov	x0, x23
   2357c:	mov	x1, x22
   23580:	mov	x2, xzr
   23584:	mov	x3, x24
   23588:	mov	x4, x20
   2358c:	mov	x5, x25
   23590:	mov	x6, x21
   23594:	bl	bf00 <__gmpn_tdiv_qr@plt>
   23598:	sub	x8, x22, #0x8
   2359c:	subs	x9, x21, #0x1
   235a0:	b.lt	235b8 <__gmpz_tdiv_r@@Base+0x1a4>  // b.tstop
   235a4:	ldr	x10, [x8, x21, lsl #3]
   235a8:	mov	x21, x9
   235ac:	cbz	x10, 2359c <__gmpz_tdiv_r@@Base+0x188>
   235b0:	add	x8, x9, #0x1
   235b4:	b	235bc <__gmpz_tdiv_r@@Base+0x1a8>
   235b8:	mov	x8, xzr
   235bc:	neg	w9, w8
   235c0:	cmp	w26, #0x0
   235c4:	csel	x8, x8, x9, ge  // ge = tcont
   235c8:	str	w8, [x19, #4]
   235cc:	ldur	x0, [x29, #-8]
   235d0:	cbnz	x0, 235f0 <__gmpz_tdiv_r@@Base+0x1dc>
   235d4:	mov	sp, x29
   235d8:	ldp	x20, x19, [sp, #64]
   235dc:	ldp	x22, x21, [sp, #48]
   235e0:	ldp	x24, x23, [sp, #32]
   235e4:	ldp	x26, x25, [sp, #16]
   235e8:	ldp	x29, x30, [sp], #80
   235ec:	ret
   235f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   235f4:	b	235d4 <__gmpz_tdiv_r@@Base+0x1c0>
   235f8:	mov	x0, x19
   235fc:	mov	x1, x20
   23600:	bl	c080 <__gmpz_realloc@plt>
   23604:	b	234d0 <__gmpz_tdiv_r@@Base+0xbc>
   23608:	sub	x0, x29, #0x8
   2360c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   23610:	mov	x25, x0
   23614:	b	23530 <__gmpz_tdiv_r@@Base+0x11c>
   23618:	sub	x0, x29, #0x8
   2361c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   23620:	mov	x24, x0
   23624:	b	23568 <__gmpz_tdiv_r@@Base+0x154>
   23628:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000002362c <__gmpz_tdiv_r_2exp@@Base>:
   2362c:	stp	x29, x30, [sp, #-64]!
   23630:	stp	x22, x21, [sp, #32]
   23634:	stp	x20, x19, [sp, #48]
   23638:	ldr	w8, [x1, #4]
   2363c:	lsr	x21, x2, #6
   23640:	mov	x20, x1
   23644:	mov	x19, x0
   23648:	cmp	w8, #0x0
   2364c:	cneg	w22, w8, mi  // mi = first
   23650:	cmp	x21, x22
   23654:	str	x23, [sp, #16]
   23658:	mov	x29, sp
   2365c:	b.cs	2369c <__gmpz_tdiv_r_2exp@@Base+0x70>  // b.hs, b.nlast
   23660:	ldr	x8, [x20, #8]
   23664:	mov	x10, #0xffffffffffffffff    	// #-1
   23668:	lsl	x10, x10, x2
   2366c:	ldr	x9, [x8, x21, lsl #3]
   23670:	bics	x23, x9, x10
   23674:	b.eq	236b8 <__gmpz_tdiv_r_2exp@@Base+0x8c>  // b.none
   23678:	ldrsw	x8, [x19]
   2367c:	add	x22, x21, #0x1
   23680:	cmp	x21, x8
   23684:	b.ge	2373c <__gmpz_tdiv_r_2exp@@Base+0x110>  // b.tcont
   23688:	ldr	x8, [x19, #8]
   2368c:	str	x23, [x8, x21, lsl #3]
   23690:	cmp	x19, x20
   23694:	b.ne	236f4 <__gmpz_tdiv_r_2exp@@Base+0xc8>  // b.any
   23698:	b	23704 <__gmpz_tdiv_r_2exp@@Base+0xd8>
   2369c:	ldrsw	x8, [x19]
   236a0:	cmp	x22, x8
   236a4:	b.gt	2372c <__gmpz_tdiv_r_2exp@@Base+0x100>
   236a8:	mov	x21, x22
   236ac:	cmp	x19, x20
   236b0:	b.ne	236f4 <__gmpz_tdiv_r_2exp@@Base+0xc8>  // b.any
   236b4:	b	23704 <__gmpz_tdiv_r_2exp@@Base+0xd8>
   236b8:	sub	x8, x8, #0x8
   236bc:	subs	x9, x21, #0x1
   236c0:	b.lt	236d8 <__gmpz_tdiv_r_2exp@@Base+0xac>  // b.tstop
   236c4:	ldr	x10, [x8, x21, lsl #3]
   236c8:	mov	x21, x9
   236cc:	cbz	x10, 236bc <__gmpz_tdiv_r_2exp@@Base+0x90>
   236d0:	add	x21, x9, #0x1
   236d4:	b	236dc <__gmpz_tdiv_r_2exp@@Base+0xb0>
   236d8:	mov	x21, xzr
   236dc:	ldrsw	x8, [x19]
   236e0:	cmp	x21, x8
   236e4:	b.gt	2374c <__gmpz_tdiv_r_2exp@@Base+0x120>
   236e8:	mov	x22, x21
   236ec:	cmp	x19, x20
   236f0:	b.eq	23704 <__gmpz_tdiv_r_2exp@@Base+0xd8>  // b.none
   236f4:	ldr	x0, [x19, #8]
   236f8:	ldr	x1, [x20, #8]
   236fc:	mov	x2, x21
   23700:	bl	ca50 <__gmpn_copyi@plt>
   23704:	ldr	w8, [x20, #4]
   23708:	neg	w9, w22
   2370c:	ldr	x23, [sp, #16]
   23710:	cmp	w8, #0x0
   23714:	csel	x8, x22, x9, ge  // ge = tcont
   23718:	str	w8, [x19, #4]
   2371c:	ldp	x20, x19, [sp, #48]
   23720:	ldp	x22, x21, [sp, #32]
   23724:	ldp	x29, x30, [sp], #64
   23728:	ret
   2372c:	mov	x0, x19
   23730:	mov	x1, x22
   23734:	bl	c080 <__gmpz_realloc@plt>
   23738:	b	236a8 <__gmpz_tdiv_r_2exp@@Base+0x7c>
   2373c:	mov	x0, x19
   23740:	mov	x1, x22
   23744:	bl	c080 <__gmpz_realloc@plt>
   23748:	b	23688 <__gmpz_tdiv_r_2exp@@Base+0x5c>
   2374c:	mov	x0, x19
   23750:	mov	x1, x21
   23754:	bl	c080 <__gmpz_realloc@plt>
   23758:	mov	x22, x21
   2375c:	cmp	x19, x20
   23760:	b.ne	236f4 <__gmpz_tdiv_r_2exp@@Base+0xc8>  // b.any
   23764:	b	23704 <__gmpz_tdiv_r_2exp@@Base+0xd8>

0000000000023768 <__gmpz_tdiv_r_ui@@Base>:
   23768:	stp	x29, x30, [sp, #-48]!
   2376c:	str	x21, [sp, #16]
   23770:	stp	x20, x19, [sp, #32]
   23774:	mov	x29, sp
   23778:	cbz	x2, 237f4 <__gmpz_tdiv_r_ui@@Base+0x8c>
   2377c:	ldrsw	x21, [x1, #4]
   23780:	mov	x20, x0
   23784:	cbz	w21, 237c8 <__gmpz_tdiv_r_ui@@Base+0x60>
   23788:	ldr	x0, [x1, #8]
   2378c:	cmp	w21, #0x0
   23790:	cneg	x1, x21, lt  // lt = tstop
   23794:	bl	c3e0 <__gmpn_mod_1@plt>
   23798:	mov	x19, x0
   2379c:	cbz	x0, 237cc <__gmpz_tdiv_r_ui@@Base+0x64>
   237a0:	ldr	w8, [x20]
   237a4:	cmp	w21, #0x0
   237a8:	mov	w9, #0x1                   	// #1
   237ac:	cneg	w9, w9, lt  // lt = tstop
   237b0:	cmp	w8, #0x0
   237b4:	str	w9, [x20, #4]
   237b8:	b.le	237e4 <__gmpz_tdiv_r_ui@@Base+0x7c>
   237bc:	ldr	x0, [x20, #8]
   237c0:	str	x19, [x0]
   237c4:	b	237d0 <__gmpz_tdiv_r_ui@@Base+0x68>
   237c8:	mov	x19, xzr
   237cc:	str	wzr, [x20, #4]
   237d0:	mov	x0, x19
   237d4:	ldp	x20, x19, [sp, #32]
   237d8:	ldr	x21, [sp, #16]
   237dc:	ldp	x29, x30, [sp], #48
   237e0:	ret
   237e4:	mov	w1, #0x1                   	// #1
   237e8:	mov	x0, x20
   237ec:	bl	c080 <__gmpz_realloc@plt>
   237f0:	b	237c0 <__gmpz_tdiv_r_ui@@Base+0x58>
   237f4:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000237f8 <__gmpz_tstbit@@Base>:
   237f8:	ldr	w11, [x0, #4]
   237fc:	lsr	x10, x1, #6
   23800:	cmp	w11, #0x0
   23804:	cneg	w8, w11, mi  // mi = first
   23808:	cmp	x10, x8
   2380c:	b.cs	2382c <__gmpz_tstbit@@Base+0x34>  // b.hs, b.nlast
   23810:	ldr	x12, [x0, #8]
   23814:	ldr	x9, [x12, x10, lsl #3]
   23818:	mov	x8, x9
   2381c:	tbnz	w11, #31, 23834 <__gmpz_tstbit@@Base+0x3c>
   23820:	lsr	x8, x8, x1
   23824:	and	w0, w8, #0x1
   23828:	ret
   2382c:	lsr	w0, w11, #31
   23830:	ret
   23834:	neg	x8, x9
   23838:	sub	x11, x12, #0x8
   2383c:	lsl	x10, x10, #3
   23840:	cbz	x10, 23820 <__gmpz_tstbit@@Base+0x28>
   23844:	ldr	x12, [x11, x10]
   23848:	sub	x10, x10, #0x8
   2384c:	cbz	x12, 23840 <__gmpz_tstbit@@Base+0x48>
   23850:	mvn	x8, x9
   23854:	lsr	x8, x8, x1
   23858:	and	w0, w8, #0x1
   2385c:	ret

0000000000023860 <__gmpz_ui_pow_ui@@Base>:
   23860:	sub	sp, sp, #0x20
   23864:	cmp	x1, #0x0
   23868:	mov	x3, x2
   2386c:	str	x1, [sp, #8]
   23870:	cset	w2, ne  // ne = any
   23874:	add	x1, sp, #0x8
   23878:	stp	x29, x30, [sp, #16]
   2387c:	add	x29, sp, #0x10
   23880:	bl	c340 <__gmpz_n_pow_ui@plt>
   23884:	ldp	x29, x30, [sp, #16]
   23888:	add	sp, sp, #0x20
   2388c:	ret

0000000000023890 <__gmpz_ui_sub@@Base>:
   23890:	sub	sp, sp, #0x40
   23894:	stp	x29, x30, [sp, #16]
   23898:	stp	x22, x21, [sp, #32]
   2389c:	stp	x20, x19, [sp, #48]
   238a0:	ldrsw	x20, [x2, #4]
   238a4:	mov	x22, x2
   238a8:	mov	x21, x1
   238ac:	mov	x19, x0
   238b0:	cmp	w20, #0x2
   238b4:	add	x29, sp, #0x10
   238b8:	b.lt	239c4 <__gmpz_ui_sub@@Base+0x134>  // b.tstop
   238bc:	ldr	w8, [x19]
   238c0:	cmp	w20, w8
   238c4:	b.gt	23c7c <__gmpz_ui_sub@@Base+0x3ec>
   238c8:	ldr	x0, [x19, #8]
   238cc:	ldr	x8, [x22, #8]
   238d0:	ldr	x9, [x8]
   238d4:	subs	x9, x9, x21
   238d8:	str	x9, [x0]
   238dc:	b.cs	239fc <__gmpz_ui_sub@@Base+0x16c>  // b.hs, b.nlast
   238e0:	mov	x11, xzr
   238e4:	sub	x10, x20, #0x1
   238e8:	mov	w9, #0x1                   	// #1
   238ec:	cmp	x9, x20
   238f0:	b.ge	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.tcont
   238f4:	add	x12, x8, x11
   238f8:	ldr	x12, [x12, #8]
   238fc:	add	x13, x0, x11
   23900:	add	x9, x9, #0x1
   23904:	add	x11, x11, #0x8
   23908:	sub	x14, x12, #0x1
   2390c:	sub	x10, x10, #0x1
   23910:	str	x14, [x13, #8]
   23914:	cbz	x12, 238ec <__gmpz_ui_sub@@Base+0x5c>
   23918:	cmp	x8, x0
   2391c:	b.eq	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   23920:	cmp	x9, x20
   23924:	b.ge	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.tcont
   23928:	sub	x12, x20, x9
   2392c:	cmp	x12, #0x4
   23930:	b.cc	239a0 <__gmpz_ui_sub@@Base+0x110>  // b.lo, b.ul, b.last
   23934:	add	x14, x0, x11
   23938:	lsl	x13, x20, #3
   2393c:	add	x14, x14, #0x8
   23940:	add	x15, x8, x13
   23944:	cmp	x14, x15
   23948:	b.cs	23960 <__gmpz_ui_sub@@Base+0xd0>  // b.hs, b.nlast
   2394c:	add	x14, x8, x11
   23950:	add	x13, x0, x13
   23954:	add	x14, x14, #0x8
   23958:	cmp	x14, x13
   2395c:	b.cc	239a0 <__gmpz_ui_sub@@Base+0x110>  // b.lo, b.ul, b.last
   23960:	add	x13, x0, x11
   23964:	add	x14, x8, x11
   23968:	and	x11, x12, #0xfffffffffffffffc
   2396c:	and	x15, x10, #0xfffffffffffffffc
   23970:	add	x10, x13, #0x18
   23974:	add	x13, x14, #0x18
   23978:	add	x9, x15, x9
   2397c:	mov	x14, x11
   23980:	ldp	q0, q1, [x13, #-16]
   23984:	add	x13, x13, #0x20
   23988:	subs	x14, x14, #0x4
   2398c:	stp	q0, q1, [x10, #-16]
   23990:	add	x10, x10, #0x20
   23994:	b.ne	23980 <__gmpz_ui_sub@@Base+0xf0>  // b.any
   23998:	cmp	x12, x11
   2399c:	b.eq	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   239a0:	lsl	x11, x9, #3
   239a4:	sub	x10, x20, x9
   239a8:	add	x9, x0, x11
   239ac:	add	x8, x8, x11
   239b0:	ldr	x11, [x8], #8
   239b4:	subs	x10, x10, #0x1
   239b8:	str	x11, [x9], #8
   239bc:	b.ne	239b0 <__gmpz_ui_sub@@Base+0x120>  // b.any
   239c0:	b	23a58 <__gmpz_ui_sub@@Base+0x1c8>
   239c4:	tbnz	w20, #31, 23a70 <__gmpz_ui_sub@@Base+0x1e0>
   239c8:	ldr	x8, [x22, #8]
   239cc:	ldr	w9, [x19]
   239d0:	neg	x10, x20
   239d4:	ldr	x8, [x8]
   239d8:	cmp	w9, #0x0
   239dc:	and	x20, x8, x10
   239e0:	b.le	23c8c <__gmpz_ui_sub@@Base+0x3fc>
   239e4:	ldr	x0, [x19, #8]
   239e8:	subs	x8, x20, x21
   239ec:	b.ls	23ca0 <__gmpz_ui_sub@@Base+0x410>  // b.plast
   239f0:	str	x8, [x0]
   239f4:	mov	w8, #0xffffffff            	// #-1
   239f8:	b	23cac <__gmpz_ui_sub@@Base+0x41c>
   239fc:	cmp	x8, x0
   23a00:	b.eq	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   23a04:	sub	x9, x20, #0x1
   23a08:	cmp	x9, #0x4
   23a0c:	b.cc	23a34 <__gmpz_ui_sub@@Base+0x1a4>  // b.lo, b.ul, b.last
   23a10:	lsl	x10, x20, #3
   23a14:	add	x11, x0, #0x8
   23a18:	add	x12, x8, x10
   23a1c:	cmp	x11, x12
   23a20:	b.cs	23c08 <__gmpz_ui_sub@@Base+0x378>  // b.hs, b.nlast
   23a24:	add	x10, x0, x10
   23a28:	add	x11, x8, #0x8
   23a2c:	cmp	x11, x10
   23a30:	b.cs	23c08 <__gmpz_ui_sub@@Base+0x378>  // b.hs, b.nlast
   23a34:	mov	w10, #0x1                   	// #1
   23a38:	lsl	x11, x10, #3
   23a3c:	sub	x9, x20, x10
   23a40:	add	x10, x0, x11
   23a44:	add	x8, x8, x11
   23a48:	ldr	x11, [x8], #8
   23a4c:	subs	x9, x9, #0x1
   23a50:	str	x11, [x10], #8
   23a54:	b.ne	23a48 <__gmpz_ui_sub@@Base+0x1b8>  // b.any
   23a58:	add	x8, x0, x20, lsl #3
   23a5c:	ldur	x8, [x8, #-8]
   23a60:	cmp	x8, #0x0
   23a64:	cset	w8, eq  // eq = none
   23a68:	sub	w8, w8, w20
   23a6c:	b	23cac <__gmpz_ui_sub@@Base+0x41c>
   23a70:	ldrsw	x8, [x19]
   23a74:	mov	w9, #0x1                   	// #1
   23a78:	sub	x1, x9, x20
   23a7c:	cmp	x1, x8
   23a80:	neg	x8, x20
   23a84:	b.gt	23cc4 <__gmpz_ui_sub@@Base+0x434>
   23a88:	ldr	x0, [x19, #8]
   23a8c:	ldr	x9, [x22, #8]
   23a90:	ldr	x10, [x9]
   23a94:	adds	x10, x10, x21
   23a98:	str	x10, [x0]
   23a9c:	b.cc	23b90 <__gmpz_ui_sub@@Base+0x300>  // b.lo, b.ul, b.last
   23aa0:	mov	x12, xzr
   23aa4:	mvn	x11, x20
   23aa8:	mov	w13, #0x1                   	// #1
   23aac:	mov	w10, #0x1                   	// #1
   23ab0:	cmp	x10, x8
   23ab4:	b.ge	23bfc <__gmpz_ui_sub@@Base+0x36c>  // b.tcont
   23ab8:	add	x14, x9, x12
   23abc:	ldr	x14, [x14, #8]
   23ac0:	add	x15, x0, x12
   23ac4:	add	x10, x10, #0x1
   23ac8:	add	x12, x12, #0x8
   23acc:	adds	x14, x14, #0x1
   23ad0:	sub	x11, x11, #0x1
   23ad4:	str	x14, [x15, #8]
   23ad8:	b.cs	23ab0 <__gmpz_ui_sub@@Base+0x220>  // b.hs, b.nlast
   23adc:	cmp	x9, x0
   23ae0:	mov	x13, xzr
   23ae4:	b.eq	23bfc <__gmpz_ui_sub@@Base+0x36c>  // b.none
   23ae8:	cmp	x10, x8
   23aec:	b.ge	23bfc <__gmpz_ui_sub@@Base+0x36c>  // b.tcont
   23af0:	sub	x13, x8, x10
   23af4:	cmp	x13, #0x4
   23af8:	b.cc	23b68 <__gmpz_ui_sub@@Base+0x2d8>  // b.lo, b.ul, b.last
   23afc:	add	x15, x0, x12
   23b00:	lsl	x14, x8, #3
   23b04:	add	x15, x15, #0x8
   23b08:	add	x16, x9, x14
   23b0c:	cmp	x15, x16
   23b10:	b.cs	23b28 <__gmpz_ui_sub@@Base+0x298>  // b.hs, b.nlast
   23b14:	add	x15, x9, x12
   23b18:	add	x14, x0, x14
   23b1c:	add	x15, x15, #0x8
   23b20:	cmp	x15, x14
   23b24:	b.cc	23b68 <__gmpz_ui_sub@@Base+0x2d8>  // b.lo, b.ul, b.last
   23b28:	add	x14, x0, x12
   23b2c:	add	x15, x9, x12
   23b30:	and	x12, x13, #0xfffffffffffffffc
   23b34:	and	x16, x11, #0xfffffffffffffffc
   23b38:	add	x11, x14, #0x18
   23b3c:	add	x14, x15, #0x18
   23b40:	add	x10, x16, x10
   23b44:	mov	x15, x12
   23b48:	ldp	q0, q1, [x14, #-16]
   23b4c:	add	x14, x14, #0x20
   23b50:	subs	x15, x15, #0x4
   23b54:	stp	q0, q1, [x11, #-16]
   23b58:	add	x11, x11, #0x20
   23b5c:	b.ne	23b48 <__gmpz_ui_sub@@Base+0x2b8>  // b.any
   23b60:	cmp	x13, x12
   23b64:	b.eq	23bf8 <__gmpz_ui_sub@@Base+0x368>  // b.none
   23b68:	add	x11, x10, x20
   23b6c:	lsl	x12, x10, #3
   23b70:	neg	x10, x11
   23b74:	add	x11, x0, x12
   23b78:	add	x9, x9, x12
   23b7c:	ldr	x12, [x9], #8
   23b80:	subs	x10, x10, #0x1
   23b84:	str	x12, [x11], #8
   23b88:	b.ne	23b7c <__gmpz_ui_sub@@Base+0x2ec>  // b.any
   23b8c:	b	23bf8 <__gmpz_ui_sub@@Base+0x368>
   23b90:	cmn	w20, #0x2
   23b94:	mov	x13, xzr
   23b98:	b.gt	23bfc <__gmpz_ui_sub@@Base+0x36c>
   23b9c:	cmp	x9, x0
   23ba0:	b.eq	23bfc <__gmpz_ui_sub@@Base+0x36c>  // b.none
   23ba4:	cmn	w20, #0x5
   23ba8:	b.hi	23bd0 <__gmpz_ui_sub@@Base+0x340>  // b.pmore
   23bac:	lsl	x10, x8, #3
   23bb0:	add	x11, x0, #0x8
   23bb4:	add	x12, x9, x10
   23bb8:	cmp	x11, x12
   23bbc:	b.cs	23c40 <__gmpz_ui_sub@@Base+0x3b0>  // b.hs, b.nlast
   23bc0:	add	x10, x0, x10
   23bc4:	add	x11, x9, #0x8
   23bc8:	cmp	x11, x10
   23bcc:	b.cs	23c40 <__gmpz_ui_sub@@Base+0x3b0>  // b.hs, b.nlast
   23bd0:	mov	w10, #0x1                   	// #1
   23bd4:	add	x11, x10, x20
   23bd8:	lsl	x12, x10, #3
   23bdc:	neg	x10, x11
   23be0:	add	x11, x0, x12
   23be4:	add	x9, x9, x12
   23be8:	ldr	x12, [x9], #8
   23bec:	subs	x10, x10, #0x1
   23bf0:	str	x12, [x11], #8
   23bf4:	b.ne	23be8 <__gmpz_ui_sub@@Base+0x358>  // b.any
   23bf8:	mov	x13, xzr
   23bfc:	str	x13, [x0, x8, lsl #3]
   23c00:	sub	w8, w13, w20
   23c04:	b	23cac <__gmpz_ui_sub@@Base+0x41c>
   23c08:	and	x11, x9, #0xfffffffffffffffc
   23c0c:	add	x12, x8, #0x18
   23c10:	orr	x10, x11, #0x1
   23c14:	add	x13, x0, #0x18
   23c18:	mov	x14, x11
   23c1c:	ldp	q0, q1, [x12, #-16]
   23c20:	add	x12, x12, #0x20
   23c24:	subs	x14, x14, #0x4
   23c28:	stp	q0, q1, [x13, #-16]
   23c2c:	add	x13, x13, #0x20
   23c30:	b.ne	23c1c <__gmpz_ui_sub@@Base+0x38c>  // b.any
   23c34:	cmp	x9, x11
   23c38:	b.eq	23a58 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   23c3c:	b	23a38 <__gmpz_ui_sub@@Base+0x1a8>
   23c40:	mvn	x11, x20
   23c44:	and	x12, x11, #0xfffffffffffffffc
   23c48:	add	x13, x9, #0x18
   23c4c:	orr	x10, x12, #0x1
   23c50:	add	x14, x0, #0x18
   23c54:	mov	x15, x12
   23c58:	ldp	q0, q1, [x13, #-16]
   23c5c:	add	x13, x13, #0x20
   23c60:	subs	x15, x15, #0x4
   23c64:	stp	q0, q1, [x14, #-16]
   23c68:	add	x14, x14, #0x20
   23c6c:	b.ne	23c58 <__gmpz_ui_sub@@Base+0x3c8>  // b.any
   23c70:	cmp	x12, x11
   23c74:	b.eq	23bf8 <__gmpz_ui_sub@@Base+0x368>  // b.none
   23c78:	b	23bd4 <__gmpz_ui_sub@@Base+0x344>
   23c7c:	mov	x0, x19
   23c80:	mov	x1, x20
   23c84:	bl	c080 <__gmpz_realloc@plt>
   23c88:	b	238cc <__gmpz_ui_sub@@Base+0x3c>
   23c8c:	mov	w1, #0x1                   	// #1
   23c90:	mov	x0, x19
   23c94:	bl	c080 <__gmpz_realloc@plt>
   23c98:	subs	x8, x20, x21
   23c9c:	b.hi	239f0 <__gmpz_ui_sub@@Base+0x160>  // b.pmore
   23ca0:	subs	x8, x21, x20
   23ca4:	str	x8, [x0]
   23ca8:	cset	w8, ne  // ne = any
   23cac:	str	w8, [x19, #4]
   23cb0:	ldp	x20, x19, [sp, #48]
   23cb4:	ldp	x22, x21, [sp, #32]
   23cb8:	ldp	x29, x30, [sp, #16]
   23cbc:	add	sp, sp, #0x40
   23cc0:	ret
   23cc4:	mov	x0, x19
   23cc8:	str	x8, [sp, #8]
   23ccc:	bl	c080 <__gmpz_realloc@plt>
   23cd0:	ldr	x8, [sp, #8]
   23cd4:	b	23a8c <__gmpz_ui_sub@@Base+0x1fc>

0000000000023cd8 <__gmpz_urandomb@@Base>:
   23cd8:	stp	x29, x30, [sp, #-64]!
   23cdc:	stp	x22, x21, [sp, #32]
   23ce0:	stp	x20, x19, [sp, #48]
   23ce4:	ldrsw	x8, [x0]
   23ce8:	add	x9, x2, #0x3f
   23cec:	lsr	x20, x9, #6
   23cf0:	mov	x19, x0
   23cf4:	mov	x21, x2
   23cf8:	cmp	x20, x8
   23cfc:	mov	x22, x1
   23d00:	str	x23, [sp, #16]
   23d04:	mov	x29, sp
   23d08:	b.gt	23d64 <__gmpz_urandomb@@Base+0x8c>
   23d0c:	ldr	x23, [x19, #8]
   23d10:	ldr	x8, [x22, #24]
   23d14:	mov	x0, x22
   23d18:	mov	x1, x23
   23d1c:	mov	x2, x21
   23d20:	ldr	x8, [x8, #8]
   23d24:	blr	x8
   23d28:	sub	x8, x23, #0x8
   23d2c:	subs	x9, x20, #0x1
   23d30:	b.lt	23d48 <__gmpz_urandomb@@Base+0x70>  // b.tstop
   23d34:	ldr	x10, [x8, x20, lsl #3]
   23d38:	mov	x20, x9
   23d3c:	cbz	x10, 23d2c <__gmpz_urandomb@@Base+0x54>
   23d40:	add	x8, x9, #0x1
   23d44:	b	23d4c <__gmpz_urandomb@@Base+0x74>
   23d48:	mov	x8, xzr
   23d4c:	str	w8, [x19, #4]
   23d50:	ldp	x20, x19, [sp, #48]
   23d54:	ldp	x22, x21, [sp, #32]
   23d58:	ldr	x23, [sp, #16]
   23d5c:	ldp	x29, x30, [sp], #64
   23d60:	ret
   23d64:	mov	x0, x19
   23d68:	mov	x1, x20
   23d6c:	bl	c080 <__gmpz_realloc@plt>
   23d70:	mov	x23, x0
   23d74:	b	23d10 <__gmpz_urandomb@@Base+0x38>

0000000000023d78 <__gmpz_urandomm@@Base>:
   23d78:	stp	x29, x30, [sp, #-80]!
   23d7c:	stp	x26, x25, [sp, #16]
   23d80:	stp	x24, x23, [sp, #32]
   23d84:	stp	x22, x21, [sp, #48]
   23d88:	stp	x20, x19, [sp, #64]
   23d8c:	mov	x29, sp
   23d90:	sub	sp, sp, #0x10
   23d94:	ldr	w8, [x2, #4]
   23d98:	cmp	w8, #0x0
   23d9c:	cneg	w20, w8, mi  // mi = first
   23da0:	cbz	w20, 23f54 <__gmpz_urandomm@@Base+0x1dc>
   23da4:	ldr	x22, [x2, #8]
   23da8:	sub	x25, x20, #0x1
   23dac:	mov	x21, x1
   23db0:	mov	x19, x0
   23db4:	ldr	x8, [x22, x25, lsl #3]
   23db8:	sub	x9, x8, #0x1
   23dbc:	tst	x8, x9
   23dc0:	b.ne	23df0 <__gmpz_urandomm@@Base+0x78>  // b.any
   23dc4:	cmp	w20, #0x1
   23dc8:	b.eq	23de8 <__gmpz_urandomm@@Base+0x70>  // b.none
   23dcc:	sub	x9, x22, #0x10
   23dd0:	mov	x10, x20
   23dd4:	ldr	x11, [x9, x10, lsl #3]
   23dd8:	cbnz	x11, 23df0 <__gmpz_urandomm@@Base+0x78>
   23ddc:	sub	x10, x10, #0x1
   23de0:	cmp	x10, #0x1
   23de4:	b.ne	23dd4 <__gmpz_urandomm@@Base+0x5c>  // b.any
   23de8:	mov	x9, #0xffffffffffffffff    	// #-1
   23dec:	b	23df4 <__gmpz_urandomm@@Base+0x7c>
   23df0:	mov	x9, xzr
   23df4:	clz	x8, x8
   23df8:	lsl	x10, x20, #6
   23dfc:	sub	x8, x10, x8
   23e00:	adds	x23, x9, x8
   23e04:	b.eq	23ef4 <__gmpz_urandomm@@Base+0x17c>  // b.none
   23e08:	cmp	x19, x2
   23e0c:	stur	xzr, [x29, #-8]
   23e10:	b.ne	23e48 <__gmpz_urandomm@@Base+0xd0>  // b.any
   23e14:	cmp	w20, #0xfe0
   23e18:	lsl	x1, x20, #3
   23e1c:	b.hi	23f44 <__gmpz_urandomm@@Base+0x1cc>  // b.pmore
   23e20:	add	x9, x1, #0xf
   23e24:	mov	x8, sp
   23e28:	and	x9, x9, #0xffffffff0
   23e2c:	sub	x24, x8, x9
   23e30:	mov	sp, x24
   23e34:	mov	x0, x24
   23e38:	mov	x1, x22
   23e3c:	mov	x2, x20
   23e40:	bl	ca50 <__gmpn_copyi@plt>
   23e44:	mov	x22, x24
   23e48:	ldrsw	x8, [x19]
   23e4c:	cmp	x20, x8
   23e50:	b.gt	23f28 <__gmpz_urandomm@@Base+0x1b0>
   23e54:	ldr	x24, [x19, #8]
   23e58:	mov	w26, #0x50                  	// #80
   23e5c:	str	xzr, [x24, x25, lsl #3]
   23e60:	b	23e6c <__gmpz_urandomm@@Base+0xf4>
   23e64:	subs	w26, w26, #0x1
   23e68:	b.eq	23eb4 <__gmpz_urandomm@@Base+0x13c>  // b.none
   23e6c:	ldr	x8, [x21, #24]
   23e70:	mov	x0, x21
   23e74:	mov	x1, x24
   23e78:	mov	x2, x23
   23e7c:	ldr	x8, [x8, #8]
   23e80:	blr	x8
   23e84:	mov	x8, x25
   23e88:	add	x9, x8, #0x1
   23e8c:	cmp	x9, #0x1
   23e90:	b.lt	23e64 <__gmpz_urandomm@@Base+0xec>  // b.tstop
   23e94:	lsl	x9, x8, #3
   23e98:	ldr	x10, [x24, x9]
   23e9c:	ldr	x9, [x22, x9]
   23ea0:	sub	x8, x8, #0x1
   23ea4:	cmp	x10, x9
   23ea8:	b.eq	23e88 <__gmpz_urandomm@@Base+0x110>  // b.none
   23eac:	b.hi	23e64 <__gmpz_urandomm@@Base+0xec>  // b.pmore
   23eb0:	cbnz	w26, 23ec8 <__gmpz_urandomm@@Base+0x150>
   23eb4:	mov	x0, x24
   23eb8:	mov	x1, x24
   23ebc:	mov	x2, x22
   23ec0:	mov	x3, x20
   23ec4:	bl	c2d0 <__gmpn_sub_n@plt>
   23ec8:	sub	x8, x24, #0x8
   23ecc:	subs	x9, x20, #0x1
   23ed0:	b.lt	23efc <__gmpz_urandomm@@Base+0x184>  // b.tstop
   23ed4:	ldr	x10, [x8, x20, lsl #3]
   23ed8:	mov	x20, x9
   23edc:	cbz	x10, 23ecc <__gmpz_urandomm@@Base+0x154>
   23ee0:	add	x8, x9, #0x1
   23ee4:	str	w8, [x19, #4]
   23ee8:	ldur	x0, [x29, #-8]
   23eec:	cbz	x0, 23f0c <__gmpz_urandomm@@Base+0x194>
   23ef0:	b	23f3c <__gmpz_urandomm@@Base+0x1c4>
   23ef4:	str	wzr, [x19, #4]
   23ef8:	b	23f0c <__gmpz_urandomm@@Base+0x194>
   23efc:	mov	x8, xzr
   23f00:	str	w8, [x19, #4]
   23f04:	ldur	x0, [x29, #-8]
   23f08:	cbnz	x0, 23f3c <__gmpz_urandomm@@Base+0x1c4>
   23f0c:	mov	sp, x29
   23f10:	ldp	x20, x19, [sp, #64]
   23f14:	ldp	x22, x21, [sp, #48]
   23f18:	ldp	x24, x23, [sp, #32]
   23f1c:	ldp	x26, x25, [sp, #16]
   23f20:	ldp	x29, x30, [sp], #80
   23f24:	ret
   23f28:	mov	x0, x19
   23f2c:	mov	x1, x20
   23f30:	bl	c080 <__gmpz_realloc@plt>
   23f34:	mov	x24, x0
   23f38:	b	23e58 <__gmpz_urandomm@@Base+0xe0>
   23f3c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   23f40:	b	23f0c <__gmpz_urandomm@@Base+0x194>
   23f44:	sub	x0, x29, #0x8
   23f48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   23f4c:	mov	x24, x0
   23f50:	b	23e34 <__gmpz_urandomm@@Base+0xbc>
   23f54:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000023f58 <__gmpz_xor@@Base>:
   23f58:	stp	x29, x30, [sp, #-96]!
   23f5c:	stp	x28, x27, [sp, #16]
   23f60:	stp	x26, x25, [sp, #32]
   23f64:	stp	x24, x23, [sp, #48]
   23f68:	stp	x22, x21, [sp, #64]
   23f6c:	stp	x20, x19, [sp, #80]
   23f70:	mov	x29, sp
   23f74:	sub	sp, sp, #0x10
   23f78:	ldr	w8, [x1, #4]
   23f7c:	ldr	w9, [x2, #4]
   23f80:	ldr	x24, [x0, #8]
   23f84:	mov	x19, x0
   23f88:	cmp	w8, w9
   23f8c:	csel	x25, x2, x1, lt  // lt = tstop
   23f90:	ldr	x23, [x25, #8]
   23f94:	csel	w10, w8, w9, gt
   23f98:	csel	w8, w8, w9, lt  // lt = tstop
   23f9c:	sxtw	x20, w10
   23fa0:	sxtw	x21, w8
   23fa4:	csel	x27, x1, x2, lt  // lt = tstop
   23fa8:	tbnz	w8, #31, 24014 <__gmpz_xor@@Base+0xbc>
   23fac:	cmp	x24, x23
   23fb0:	mov	x22, x23
   23fb4:	b.eq	23fdc <__gmpz_xor@@Base+0x84>  // b.none
   23fb8:	ldr	w8, [x19]
   23fbc:	cmp	w20, w8
   23fc0:	b.gt	2467c <__gmpz_xor@@Base+0x724>
   23fc4:	lsl	x8, x21, #3
   23fc8:	add	x0, x24, x8
   23fcc:	add	x1, x23, x8
   23fd0:	sub	x2, x20, x21
   23fd4:	bl	ca50 <__gmpn_copyi@plt>
   23fd8:	mov	x22, x24
   23fdc:	cbz	w21, 23ff4 <__gmpz_xor@@Base+0x9c>
   23fe0:	ldr	x2, [x27, #8]
   23fe4:	mov	x0, x22
   23fe8:	mov	x1, x23
   23fec:	mov	x3, x21
   23ff0:	bl	cb40 <__gmpn_xor_n@plt>
   23ff4:	sub	x8, x22, #0x8
   23ff8:	mov	x9, x20
   23ffc:	subs	x20, x20, #0x1
   24000:	b.lt	2400c <__gmpz_xor@@Base+0xb4>  // b.tstop
   24004:	ldr	x10, [x8, x9, lsl #3]
   24008:	cbz	x10, 23ff8 <__gmpz_xor@@Base+0xa0>
   2400c:	str	w9, [x19, #4]
   24010:	b	2465c <__gmpz_xor@@Base+0x704>
   24014:	neg	x22, x21
   24018:	stur	xzr, [x29, #-8]
   2401c:	tbnz	w20, #31, 240cc <__gmpz_xor@@Base+0x174>
   24020:	ldrsw	x8, [x19]
   24024:	cmp	x22, x20
   24028:	csel	x28, x20, x22, lt  // lt = tstop
   2402c:	cmp	x28, x8
   24030:	b.ge	24690 <__gmpz_xor@@Base+0x738>  // b.tcont
   24034:	cmp	x22, #0xfe0
   24038:	lsl	x26, x22, #3
   2403c:	b.hi	246b0 <__gmpz_xor@@Base+0x758>  // b.pmore
   24040:	add	x9, x26, #0xf
   24044:	mov	x8, sp
   24048:	and	x9, x9, #0xfffffffffffffff0
   2404c:	sub	x25, x8, x9
   24050:	mov	sp, x25
   24054:	ldr	x8, [x27, #8]
   24058:	ldr	x9, [x8]
   2405c:	sub	x10, x9, #0x1
   24060:	str	x10, [x25]
   24064:	cbz	x9, 24170 <__gmpz_xor@@Base+0x218>
   24068:	cmn	w21, #0x2
   2406c:	b.gt	24370 <__gmpz_xor@@Base+0x418>
   24070:	cmp	x8, x25
   24074:	b.eq	24370 <__gmpz_xor@@Base+0x418>  // b.none
   24078:	cmn	w21, #0x5
   2407c:	b.hi	240a0 <__gmpz_xor@@Base+0x148>  // b.pmore
   24080:	add	x9, x25, #0x8
   24084:	add	x10, x8, x22, lsl #3
   24088:	cmp	x9, x10
   2408c:	b.cs	24338 <__gmpz_xor@@Base+0x3e0>  // b.hs, b.nlast
   24090:	sub	x9, x25, x21, lsl #3
   24094:	add	x10, x8, #0x8
   24098:	cmp	x9, x10
   2409c:	b.ls	24338 <__gmpz_xor@@Base+0x3e0>  // b.plast
   240a0:	mov	w9, #0x1                   	// #1
   240a4:	add	x10, x9, x21
   240a8:	lsl	x11, x9, #3
   240ac:	neg	x9, x10
   240b0:	add	x10, x25, x11
   240b4:	add	x8, x8, x11
   240b8:	ldr	x11, [x8], #8
   240bc:	subs	x9, x9, #0x1
   240c0:	str	x11, [x10], #8
   240c4:	b.ne	240b8 <__gmpz_xor@@Base+0x160>  // b.any
   240c8:	b	24370 <__gmpz_xor@@Base+0x418>
   240cc:	add	x8, x20, x21
   240d0:	neg	x1, x8, lsl #3
   240d4:	mov	w8, #0x7f00                	// #32512
   240d8:	cmp	x1, x8
   240dc:	neg	x24, x20
   240e0:	b.hi	246cc <__gmpz_xor@@Base+0x774>  // b.pmore
   240e4:	add	x9, x1, #0xf
   240e8:	mov	x8, sp
   240ec:	and	x9, x9, #0xfffffffffffffff0
   240f0:	sub	x25, x8, x9
   240f4:	mov	sp, x25
   240f8:	ldr	x8, [x23]
   240fc:	add	x26, x25, x24, lsl #3
   24100:	sub	x9, x8, #0x1
   24104:	str	x9, [x25]
   24108:	cbz	x8, 24254 <__gmpz_xor@@Base+0x2fc>
   2410c:	cmn	w20, #0x2
   24110:	b.gt	24434 <__gmpz_xor@@Base+0x4dc>
   24114:	cmp	x23, x25
   24118:	b.eq	24434 <__gmpz_xor@@Base+0x4dc>  // b.none
   2411c:	cmn	w20, #0x5
   24120:	b.hi	24144 <__gmpz_xor@@Base+0x1ec>  // b.pmore
   24124:	add	x8, x25, #0x8
   24128:	add	x9, x23, x24, lsl #3
   2412c:	cmp	x8, x9
   24130:	b.cs	243fc <__gmpz_xor@@Base+0x4a4>  // b.hs, b.nlast
   24134:	sub	x8, x25, x20, lsl #3
   24138:	add	x9, x23, #0x8
   2413c:	cmp	x8, x9
   24140:	b.ls	243fc <__gmpz_xor@@Base+0x4a4>  // b.plast
   24144:	mov	w8, #0x1                   	// #1
   24148:	add	x9, x8, x20
   2414c:	lsl	x10, x8, #3
   24150:	neg	x8, x9
   24154:	add	x9, x25, x10
   24158:	add	x10, x23, x10
   2415c:	ldr	x11, [x10], #8
   24160:	subs	x8, x8, #0x1
   24164:	str	x11, [x9], #8
   24168:	b.ne	2415c <__gmpz_xor@@Base+0x204>  // b.any
   2416c:	b	24434 <__gmpz_xor@@Base+0x4dc>
   24170:	mov	x11, xzr
   24174:	mvn	x10, x21
   24178:	mov	w9, #0x1                   	// #1
   2417c:	cmp	x9, x22
   24180:	b.ge	24370 <__gmpz_xor@@Base+0x418>  // b.tcont
   24184:	add	x12, x8, x11
   24188:	ldr	x12, [x12, #8]
   2418c:	add	x13, x25, x11
   24190:	add	x9, x9, #0x1
   24194:	add	x11, x11, #0x8
   24198:	sub	x14, x12, #0x1
   2419c:	sub	x10, x10, #0x1
   241a0:	str	x14, [x13, #8]
   241a4:	cbz	x12, 2417c <__gmpz_xor@@Base+0x224>
   241a8:	cmp	x8, x25
   241ac:	b.eq	24370 <__gmpz_xor@@Base+0x418>  // b.none
   241b0:	cmp	x9, x22
   241b4:	b.ge	24370 <__gmpz_xor@@Base+0x418>  // b.tcont
   241b8:	sub	x12, x22, x9
   241bc:	cmp	x12, #0x4
   241c0:	b.cc	2422c <__gmpz_xor@@Base+0x2d4>  // b.lo, b.ul, b.last
   241c4:	add	x13, x25, x11
   241c8:	add	x13, x13, #0x8
   241cc:	add	x14, x8, x22, lsl #3
   241d0:	cmp	x13, x14
   241d4:	b.cs	241ec <__gmpz_xor@@Base+0x294>  // b.hs, b.nlast
   241d8:	add	x14, x8, x11
   241dc:	sub	x13, x25, x21, lsl #3
   241e0:	add	x14, x14, #0x8
   241e4:	cmp	x13, x14
   241e8:	b.hi	2422c <__gmpz_xor@@Base+0x2d4>  // b.pmore
   241ec:	add	x13, x25, x11
   241f0:	add	x14, x8, x11
   241f4:	and	x11, x12, #0xfffffffffffffffc
   241f8:	and	x15, x10, #0xfffffffffffffffc
   241fc:	add	x10, x13, #0x18
   24200:	add	x13, x14, #0x18
   24204:	add	x9, x15, x9
   24208:	mov	x14, x11
   2420c:	ldp	q0, q1, [x13, #-16]
   24210:	add	x13, x13, #0x20
   24214:	subs	x14, x14, #0x4
   24218:	stp	q0, q1, [x10, #-16]
   2421c:	add	x10, x10, #0x20
   24220:	b.ne	2420c <__gmpz_xor@@Base+0x2b4>  // b.any
   24224:	cmp	x12, x11
   24228:	b.eq	24370 <__gmpz_xor@@Base+0x418>  // b.none
   2422c:	add	x10, x9, x21
   24230:	lsl	x11, x9, #3
   24234:	neg	x9, x10
   24238:	add	x10, x25, x11
   2423c:	add	x8, x8, x11
   24240:	ldr	x11, [x8], #8
   24244:	subs	x9, x9, #0x1
   24248:	str	x11, [x10], #8
   2424c:	b.ne	24240 <__gmpz_xor@@Base+0x2e8>  // b.any
   24250:	b	24370 <__gmpz_xor@@Base+0x418>
   24254:	mov	x10, xzr
   24258:	mvn	x9, x20
   2425c:	mov	w8, #0x1                   	// #1
   24260:	cmp	x8, x24
   24264:	b.ge	24434 <__gmpz_xor@@Base+0x4dc>  // b.tcont
   24268:	add	x11, x23, x10
   2426c:	ldr	x11, [x11, #8]
   24270:	add	x12, x25, x10
   24274:	add	x8, x8, #0x1
   24278:	add	x10, x10, #0x8
   2427c:	sub	x13, x11, #0x1
   24280:	sub	x9, x9, #0x1
   24284:	str	x13, [x12, #8]
   24288:	cbz	x11, 24260 <__gmpz_xor@@Base+0x308>
   2428c:	cmp	x23, x25
   24290:	b.eq	24434 <__gmpz_xor@@Base+0x4dc>  // b.none
   24294:	cmp	x8, x24
   24298:	b.ge	24434 <__gmpz_xor@@Base+0x4dc>  // b.tcont
   2429c:	sub	x11, x24, x8
   242a0:	cmp	x11, #0x4
   242a4:	b.cc	24310 <__gmpz_xor@@Base+0x3b8>  // b.lo, b.ul, b.last
   242a8:	add	x12, x25, x10
   242ac:	add	x12, x12, #0x8
   242b0:	add	x13, x23, x24, lsl #3
   242b4:	cmp	x12, x13
   242b8:	b.cs	242d0 <__gmpz_xor@@Base+0x378>  // b.hs, b.nlast
   242bc:	add	x13, x23, x10
   242c0:	sub	x12, x25, x20, lsl #3
   242c4:	add	x13, x13, #0x8
   242c8:	cmp	x12, x13
   242cc:	b.hi	24310 <__gmpz_xor@@Base+0x3b8>  // b.pmore
   242d0:	add	x12, x25, x10
   242d4:	add	x13, x23, x10
   242d8:	and	x10, x11, #0xfffffffffffffffc
   242dc:	and	x14, x9, #0xfffffffffffffffc
   242e0:	add	x9, x12, #0x18
   242e4:	add	x12, x13, #0x18
   242e8:	add	x8, x14, x8
   242ec:	mov	x13, x10
   242f0:	ldp	q0, q1, [x12, #-16]
   242f4:	add	x12, x12, #0x20
   242f8:	subs	x13, x13, #0x4
   242fc:	stp	q0, q1, [x9, #-16]
   24300:	add	x9, x9, #0x20
   24304:	b.ne	242f0 <__gmpz_xor@@Base+0x398>  // b.any
   24308:	cmp	x11, x10
   2430c:	b.eq	24434 <__gmpz_xor@@Base+0x4dc>  // b.none
   24310:	add	x9, x8, x20
   24314:	lsl	x10, x8, #3
   24318:	neg	x8, x9
   2431c:	add	x9, x25, x10
   24320:	add	x10, x23, x10
   24324:	ldr	x11, [x10], #8
   24328:	subs	x8, x8, #0x1
   2432c:	str	x11, [x9], #8
   24330:	b.ne	24324 <__gmpz_xor@@Base+0x3cc>  // b.any
   24334:	b	24434 <__gmpz_xor@@Base+0x4dc>
   24338:	mvn	x10, x21
   2433c:	and	x11, x10, #0xfffffffffffffffc
   24340:	add	x12, x8, #0x18
   24344:	orr	x9, x11, #0x1
   24348:	add	x13, x25, #0x18
   2434c:	mov	x14, x11
   24350:	ldp	q0, q1, [x12, #-16]
   24354:	add	x12, x12, #0x20
   24358:	subs	x14, x14, #0x4
   2435c:	stp	q0, q1, [x13, #-16]
   24360:	add	x13, x13, #0x20
   24364:	b.ne	24350 <__gmpz_xor@@Base+0x3f8>  // b.any
   24368:	cmp	x11, x10
   2436c:	b.ne	240a4 <__gmpz_xor@@Base+0x14c>  // b.any
   24370:	subs	x2, x22, x20
   24374:	b.le	24390 <__gmpz_xor@@Base+0x438>
   24378:	lsl	x8, x20, #3
   2437c:	add	x0, x24, x8
   24380:	add	x1, x25, x8
   24384:	bl	ca50 <__gmpn_copyi@plt>
   24388:	cbnz	w20, 243a4 <__gmpz_xor@@Base+0x44c>
   2438c:	b	243b8 <__gmpz_xor@@Base+0x460>
   24390:	add	x0, x24, x26
   24394:	add	x1, x23, x26
   24398:	add	x2, x20, x21
   2439c:	bl	ca50 <__gmpn_copyi@plt>
   243a0:	mov	x20, x22
   243a4:	mov	x0, x24
   243a8:	mov	x1, x23
   243ac:	mov	x2, x25
   243b0:	mov	x3, x20
   243b4:	bl	cb40 <__gmpn_xor_n@plt>
   243b8:	ldur	x0, [x29, #-8]
   243bc:	cbnz	x0, 246c4 <__gmpz_xor@@Base+0x76c>
   243c0:	mov	x8, x24
   243c4:	str	xzr, [x24, x28, lsl #3]
   243c8:	ldr	x9, [x8]
   243cc:	adds	x9, x9, #0x1
   243d0:	str	x9, [x8], #8
   243d4:	b.cs	243c8 <__gmpz_xor@@Base+0x470>  // b.hs, b.nlast
   243d8:	ldr	x8, [x24, x28, lsl #3]
   243dc:	add	x9, x8, x28
   243e0:	mvn	w8, w9
   243e4:	add	x9, x24, x9, lsl #3
   243e8:	sub	x9, x9, #0x8
   243ec:	ldr	x10, [x9], #-8
   243f0:	add	w8, w8, #0x1
   243f4:	cbz	x10, 243ec <__gmpz_xor@@Base+0x494>
   243f8:	b	24658 <__gmpz_xor@@Base+0x700>
   243fc:	mvn	x9, x20
   24400:	and	x10, x9, #0xfffffffffffffffc
   24404:	add	x11, x23, #0x18
   24408:	orr	x8, x10, #0x1
   2440c:	add	x12, x25, #0x18
   24410:	mov	x13, x10
   24414:	ldp	q0, q1, [x11, #-16]
   24418:	add	x11, x11, #0x20
   2441c:	subs	x13, x13, #0x4
   24420:	stp	q0, q1, [x12, #-16]
   24424:	add	x12, x12, #0x20
   24428:	b.ne	24414 <__gmpz_xor@@Base+0x4bc>  // b.any
   2442c:	cmp	x10, x9
   24430:	b.ne	24148 <__gmpz_xor@@Base+0x1f0>  // b.any
   24434:	ldr	x8, [x27, #8]
   24438:	ldr	x9, [x8]
   2443c:	sub	x10, x9, #0x1
   24440:	str	x10, [x26]
   24444:	cbz	x9, 244bc <__gmpz_xor@@Base+0x564>
   24448:	cmn	w21, #0x2
   2444c:	b.gt	245fc <__gmpz_xor@@Base+0x6a4>
   24450:	cmp	x8, x26
   24454:	b.eq	245fc <__gmpz_xor@@Base+0x6a4>  // b.none
   24458:	cmn	w21, #0x5
   2445c:	b.hi	2448c <__gmpz_xor@@Base+0x534>  // b.pmore
   24460:	lsl	x9, x20, #3
   24464:	sub	x10, x25, x9
   24468:	add	x10, x10, #0x8
   2446c:	add	x11, x8, x22, lsl #3
   24470:	cmp	x10, x11
   24474:	b.cs	245c0 <__gmpz_xor@@Base+0x668>  // b.hs, b.nlast
   24478:	add	x9, x9, x21, lsl #3
   2447c:	sub	x9, x25, x9
   24480:	add	x10, x8, #0x8
   24484:	cmp	x9, x10
   24488:	b.ls	245c0 <__gmpz_xor@@Base+0x668>  // b.plast
   2448c:	mov	w9, #0x1                   	// #1
   24490:	add	x10, x9, x21
   24494:	lsl	x11, x9, #3
   24498:	neg	x9, x10
   2449c:	sub	x10, x11, x20, lsl #3
   244a0:	add	x10, x25, x10
   244a4:	add	x8, x8, x11
   244a8:	ldr	x11, [x8], #8
   244ac:	subs	x9, x9, #0x1
   244b0:	str	x11, [x10], #8
   244b4:	b.ne	244a8 <__gmpz_xor@@Base+0x550>  // b.any
   244b8:	b	245fc <__gmpz_xor@@Base+0x6a4>
   244bc:	mov	w9, #0x20                  	// #32
   244c0:	sub	x12, x9, x20, lsl #3
   244c4:	add	x9, x12, x25
   244c8:	mov	x10, xzr
   244cc:	mvn	x11, x21
   244d0:	sub	x13, x9, #0x20
   244d4:	mov	w9, #0x1                   	// #1
   244d8:	cmp	x9, x22
   244dc:	b.ge	245fc <__gmpz_xor@@Base+0x6a4>  // b.tcont
   244e0:	add	x14, x8, x10
   244e4:	ldr	x14, [x14, #8]
   244e8:	add	x15, x13, x10
   244ec:	add	x9, x9, #0x1
   244f0:	add	x10, x10, #0x8
   244f4:	sub	x16, x14, #0x1
   244f8:	sub	x11, x11, #0x1
   244fc:	str	x16, [x15, #8]
   24500:	cbz	x14, 244d8 <__gmpz_xor@@Base+0x580>
   24504:	cmp	x8, x26
   24508:	b.eq	245fc <__gmpz_xor@@Base+0x6a4>  // b.none
   2450c:	cmp	x9, x22
   24510:	b.ge	245fc <__gmpz_xor@@Base+0x6a4>  // b.tcont
   24514:	sub	x13, x22, x9
   24518:	cmp	x13, #0x4
   2451c:	b.cc	24594 <__gmpz_xor@@Base+0x63c>  // b.lo, b.ul, b.last
   24520:	add	x14, x12, x25
   24524:	add	x14, x14, x10
   24528:	sub	x14, x14, #0x18
   2452c:	add	x15, x8, x22, lsl #3
   24530:	cmp	x14, x15
   24534:	b.cs	24550 <__gmpz_xor@@Base+0x5f8>  // b.hs, b.nlast
   24538:	add	x14, x21, x20
   2453c:	add	x15, x8, x10
   24540:	sub	x14, x25, x14, lsl #3
   24544:	add	x15, x15, #0x8
   24548:	cmp	x14, x15
   2454c:	b.hi	24594 <__gmpz_xor@@Base+0x63c>  // b.pmore
   24550:	add	x14, x12, x25
   24554:	add	x15, x8, x10
   24558:	and	x12, x13, #0xfffffffffffffffc
   2455c:	and	x16, x11, #0xfffffffffffffffc
   24560:	add	x11, x14, x10
   24564:	add	x10, x15, #0x18
   24568:	sub	x11, x11, #0x8
   2456c:	add	x9, x16, x9
   24570:	mov	x14, x12
   24574:	ldp	q0, q1, [x10, #-16]
   24578:	add	x10, x10, #0x20
   2457c:	subs	x14, x14, #0x4
   24580:	stp	q0, q1, [x11, #-16]
   24584:	add	x11, x11, #0x20
   24588:	b.ne	24574 <__gmpz_xor@@Base+0x61c>  // b.any
   2458c:	cmp	x13, x12
   24590:	b.eq	245fc <__gmpz_xor@@Base+0x6a4>  // b.none
   24594:	add	x10, x9, x21
   24598:	lsl	x11, x9, #3
   2459c:	neg	x9, x10
   245a0:	sub	x10, x11, x20, lsl #3
   245a4:	add	x10, x25, x10
   245a8:	add	x8, x8, x11
   245ac:	ldr	x11, [x8], #8
   245b0:	subs	x9, x9, #0x1
   245b4:	str	x11, [x10], #8
   245b8:	b.ne	245ac <__gmpz_xor@@Base+0x654>  // b.any
   245bc:	b	245fc <__gmpz_xor@@Base+0x6a4>
   245c0:	mvn	x10, x21
   245c4:	sub	x13, x25, x20, lsl #3
   245c8:	and	x12, x10, #0xfffffffffffffffc
   245cc:	add	x11, x8, #0x18
   245d0:	orr	x9, x12, #0x1
   245d4:	add	x13, x13, #0x18
   245d8:	mov	x14, x12
   245dc:	ldp	q0, q1, [x11, #-16]
   245e0:	add	x11, x11, #0x20
   245e4:	subs	x14, x14, #0x4
   245e8:	stp	q0, q1, [x13, #-16]
   245ec:	add	x13, x13, #0x20
   245f0:	b.ne	245dc <__gmpz_xor@@Base+0x684>  // b.any
   245f4:	cmp	x12, x10
   245f8:	b.ne	24490 <__gmpz_xor@@Base+0x538>  // b.any
   245fc:	ldrsw	x8, [x19]
   24600:	cmp	x22, x8
   24604:	b.gt	246dc <__gmpz_xor@@Base+0x784>
   24608:	ldr	x23, [x19, #8]
   2460c:	lsl	x8, x24, #3
   24610:	add	x0, x23, x8
   24614:	add	x1, x26, x8
   24618:	sub	x2, x20, x21
   2461c:	bl	ca50 <__gmpn_copyi@plt>
   24620:	mov	x0, x23
   24624:	mov	x1, x25
   24628:	mov	x2, x26
   2462c:	mov	x3, x24
   24630:	bl	cb40 <__gmpn_xor_n@plt>
   24634:	ldur	x0, [x29, #-8]
   24638:	cbnz	x0, 246f0 <__gmpz_xor@@Base+0x798>
   2463c:	sub	x9, x23, #0x8
   24640:	subs	x10, x22, #0x1
   24644:	mov	w8, w22
   24648:	b.lt	24658 <__gmpz_xor@@Base+0x700>  // b.tstop
   2464c:	ldr	x11, [x9, x22, lsl #3]
   24650:	mov	x22, x10
   24654:	cbz	x11, 24640 <__gmpz_xor@@Base+0x6e8>
   24658:	str	w8, [x19, #4]
   2465c:	mov	sp, x29
   24660:	ldp	x20, x19, [sp, #80]
   24664:	ldp	x22, x21, [sp, #64]
   24668:	ldp	x24, x23, [sp, #48]
   2466c:	ldp	x26, x25, [sp, #32]
   24670:	ldp	x28, x27, [sp, #16]
   24674:	ldp	x29, x30, [sp], #96
   24678:	ret
   2467c:	mov	x0, x19
   24680:	mov	x1, x20
   24684:	bl	c080 <__gmpz_realloc@plt>
   24688:	mov	x24, x0
   2468c:	b	23fc4 <__gmpz_xor@@Base+0x6c>
   24690:	add	x1, x28, #0x1
   24694:	mov	x0, x19
   24698:	bl	c080 <__gmpz_realloc@plt>
   2469c:	ldr	x23, [x25, #8]
   246a0:	mov	x24, x0
   246a4:	cmp	x22, #0xfe0
   246a8:	lsl	x26, x22, #3
   246ac:	b.ls	24040 <__gmpz_xor@@Base+0xe8>  // b.plast
   246b0:	sub	x0, x29, #0x8
   246b4:	mov	x1, x26
   246b8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   246bc:	mov	x25, x0
   246c0:	b	24054 <__gmpz_xor@@Base+0xfc>
   246c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   246c8:	b	243c0 <__gmpz_xor@@Base+0x468>
   246cc:	sub	x0, x29, #0x8
   246d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   246d4:	mov	x25, x0
   246d8:	b	240f8 <__gmpz_xor@@Base+0x1a0>
   246dc:	mov	x0, x19
   246e0:	mov	x1, x22
   246e4:	bl	c080 <__gmpz_realloc@plt>
   246e8:	mov	x23, x0
   246ec:	b	2460c <__gmpz_xor@@Base+0x6b4>
   246f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   246f4:	b	2463c <__gmpz_xor@@Base+0x6e4>

00000000000246f8 <__gmpq_abs@@Base>:
   246f8:	stp	x29, x30, [sp, #-64]!
   246fc:	stp	x22, x21, [sp, #32]
   24700:	stp	x20, x19, [sp, #48]
   24704:	ldr	w8, [x1, #4]
   24708:	mov	x19, x0
   2470c:	str	x23, [sp, #16]
   24710:	mov	x29, sp
   24714:	cmp	w8, #0x0
   24718:	cneg	w20, w8, mi  // mi = first
   2471c:	cmp	x0, x1
   24720:	b.eq	24770 <__gmpq_abs@@Base+0x78>  // b.none
   24724:	ldrsw	x8, [x19]
   24728:	ldr	w23, [x1, #20]
   2472c:	mov	x21, x1
   24730:	cmp	x20, x8
   24734:	sxtw	x22, w23
   24738:	b.gt	24788 <__gmpq_abs@@Base+0x90>
   2473c:	ldr	x0, [x19, #8]
   24740:	ldr	x1, [x21, #8]
   24744:	mov	x2, x20
   24748:	bl	ca50 <__gmpn_copyi@plt>
   2474c:	mov	x0, x19
   24750:	ldr	w8, [x0, #16]!
   24754:	cmp	w23, w8
   24758:	b.gt	24798 <__gmpq_abs@@Base+0xa0>
   2475c:	ldr	x0, [x19, #24]
   24760:	str	w23, [x19, #20]
   24764:	ldr	x1, [x21, #24]
   24768:	mov	x2, x22
   2476c:	bl	ca50 <__gmpn_copyi@plt>
   24770:	str	w20, [x19, #4]
   24774:	ldp	x20, x19, [sp, #48]
   24778:	ldp	x22, x21, [sp, #32]
   2477c:	ldr	x23, [sp, #16]
   24780:	ldp	x29, x30, [sp], #64
   24784:	ret
   24788:	mov	x0, x19
   2478c:	mov	x1, x20
   24790:	bl	c080 <__gmpz_realloc@plt>
   24794:	b	24740 <__gmpq_abs@@Base+0x48>
   24798:	mov	x1, x22
   2479c:	bl	c080 <__gmpz_realloc@plt>
   247a0:	b	24760 <__gmpq_abs@@Base+0x68>

00000000000247a4 <__gmpq_add@@Base>:
   247a4:	adrp	x3, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   247a8:	ldr	x3, [x3, #4000]
   247ac:	b	247b0 <__gmpq_add@@Base+0xc>
   247b0:	stp	x29, x30, [sp, #-96]!
   247b4:	stp	x28, x27, [sp, #16]
   247b8:	stp	x26, x25, [sp, #32]
   247bc:	stp	x24, x23, [sp, #48]
   247c0:	stp	x22, x21, [sp, #64]
   247c4:	stp	x20, x19, [sp, #80]
   247c8:	mov	x29, sp
   247cc:	sub	sp, sp, #0x50
   247d0:	ldr	w8, [x1, #4]
   247d4:	ldr	w9, [x2, #4]
   247d8:	ldrsw	x26, [x1, #20]
   247dc:	ldrsw	x25, [x2, #20]
   247e0:	cmp	w8, #0x0
   247e4:	cneg	w28, w8, mi  // mi = first
   247e8:	cmp	w9, #0x0
   247ec:	cneg	w27, w9, mi  // mi = first
   247f0:	cmp	x26, x25
   247f4:	csel	x8, x26, x25, lt  // lt = tstop
   247f8:	mov	x23, x1
   247fc:	mov	w10, #0x7f00                	// #32512
   24800:	lsl	x1, x8, #3
   24804:	mov	x21, x3
   24808:	mov	x22, x2
   2480c:	mov	x19, x0
   24810:	cmp	x1, x10
   24814:	stur	xzr, [x29, #-56]
   24818:	stur	w8, [x29, #-16]
   2481c:	b.hi	24a38 <__gmpq_add@@Base+0x294>  // b.pmore
   24820:	add	x9, x1, #0xf
   24824:	mov	x8, sp
   24828:	and	x9, x9, #0xfffffffffffffff0
   2482c:	sub	x0, x8, x9
   24830:	mov	sp, x0
   24834:	add	x25, x25, x28
   24838:	lsl	x1, x25, #3
   2483c:	mov	w8, #0x7f00                	// #32512
   24840:	add	x24, x23, #0x10
   24844:	add	x20, x22, #0x10
   24848:	cmp	x1, x8
   2484c:	stur	x0, [x29, #-8]
   24850:	stur	w25, [x29, #-32]
   24854:	b.hi	24a44 <__gmpq_add@@Base+0x2a0>  // b.pmore
   24858:	add	x9, x1, #0xf
   2485c:	mov	x8, sp
   24860:	and	x9, x9, #0xfffffffffffffff0
   24864:	sub	x0, x8, x9
   24868:	mov	sp, x0
   2486c:	add	x26, x27, x26
   24870:	lsl	x1, x26, #3
   24874:	mov	w8, #0x7f00                	// #32512
   24878:	cmp	x1, x8
   2487c:	stur	x0, [x29, #-24]
   24880:	stur	w26, [x29, #-48]
   24884:	b.hi	24a50 <__gmpq_add@@Base+0x2ac>  // b.pmore
   24888:	add	x9, x1, #0xf
   2488c:	mov	x8, sp
   24890:	and	x9, x9, #0xfffffffffffffff0
   24894:	sub	x0, x8, x9
   24898:	mov	sp, x0
   2489c:	stur	x0, [x29, #-40]
   248a0:	sub	x0, x29, #0x10
   248a4:	mov	x1, x24
   248a8:	mov	x2, x20
   248ac:	bl	cf70 <__gmpz_gcd@plt>
   248b0:	ldursw	x8, [x29, #-12]
   248b4:	cmp	w8, #0x1
   248b8:	b.ne	2490c <__gmpq_add@@Base+0x168>  // b.any
   248bc:	ldur	x9, [x29, #-8]
   248c0:	ldr	x9, [x9]
   248c4:	cmp	x9, #0x1
   248c8:	b.ne	2490c <__gmpq_add@@Base+0x168>  // b.any
   248cc:	sub	x0, x29, #0x20
   248d0:	mov	x1, x23
   248d4:	mov	x2, x20
   248d8:	bl	c4b0 <__gmpz_mul@plt>
   248dc:	sub	x0, x29, #0x30
   248e0:	mov	x1, x22
   248e4:	mov	x2, x24
   248e8:	bl	c4b0 <__gmpz_mul@plt>
   248ec:	sub	x1, x29, #0x20
   248f0:	sub	x2, x29, #0x30
   248f4:	mov	x0, x19
   248f8:	blr	x21
   248fc:	add	x0, x19, #0x10
   24900:	mov	x1, x24
   24904:	mov	x2, x20
   24908:	b	24a0c <__gmpq_add@@Base+0x268>
   2490c:	cmp	x25, x26
   24910:	csel	x9, x25, x26, gt
   24914:	sub	w10, w9, w8
   24918:	sub	x8, x9, x8
   2491c:	lsl	x8, x8, #3
   24920:	add	x1, x8, #0x10
   24924:	mov	w8, #0x7f00                	// #32512
   24928:	add	w9, w10, #0x2
   2492c:	cmp	x1, x8
   24930:	stur	w9, [x29, #-72]
   24934:	b.hi	24a64 <__gmpq_add@@Base+0x2c0>  // b.pmore
   24938:	add	x9, x1, #0xf
   2493c:	mov	x8, sp
   24940:	and	x9, x9, #0xfffffffffffffff0
   24944:	sub	x0, x8, x9
   24948:	mov	sp, x0
   2494c:	stur	x0, [x29, #-64]
   24950:	sub	x0, x29, #0x48
   24954:	sub	x2, x29, #0x10
   24958:	mov	x1, x20
   2495c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24960:	sub	x0, x29, #0x30
   24964:	sub	x2, x29, #0x10
   24968:	mov	x1, x24
   2496c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24970:	sub	x0, x29, #0x20
   24974:	sub	x2, x29, #0x48
   24978:	mov	x1, x23
   2497c:	bl	c4b0 <__gmpz_mul@plt>
   24980:	sub	x0, x29, #0x48
   24984:	sub	x2, x29, #0x30
   24988:	mov	x1, x22
   2498c:	bl	c4b0 <__gmpz_mul@plt>
   24990:	sub	x0, x29, #0x48
   24994:	sub	x1, x29, #0x20
   24998:	sub	x2, x29, #0x48
   2499c:	blr	x21
   249a0:	sub	x0, x29, #0x10
   249a4:	sub	x1, x29, #0x48
   249a8:	sub	x2, x29, #0x10
   249ac:	bl	cf70 <__gmpz_gcd@plt>
   249b0:	ldur	w8, [x29, #-12]
   249b4:	cmp	w8, #0x1
   249b8:	b.ne	249dc <__gmpq_add@@Base+0x238>  // b.any
   249bc:	ldur	x8, [x29, #-8]
   249c0:	ldr	x8, [x8]
   249c4:	cmp	x8, #0x1
   249c8:	b.ne	249dc <__gmpq_add@@Base+0x238>  // b.any
   249cc:	sub	x1, x29, #0x48
   249d0:	mov	x0, x19
   249d4:	bl	c420 <__gmpz_set@plt>
   249d8:	b	24a00 <__gmpq_add@@Base+0x25c>
   249dc:	sub	x1, x29, #0x48
   249e0:	sub	x2, x29, #0x10
   249e4:	mov	x0, x19
   249e8:	bl	ca00 <__gmpz_divexact_gcd@plt>
   249ec:	sub	x0, x29, #0x20
   249f0:	sub	x2, x29, #0x10
   249f4:	mov	x1, x20
   249f8:	bl	ca00 <__gmpz_divexact_gcd@plt>
   249fc:	sub	x20, x29, #0x20
   24a00:	add	x0, x19, #0x10
   24a04:	sub	x2, x29, #0x30
   24a08:	mov	x1, x20
   24a0c:	bl	c4b0 <__gmpz_mul@plt>
   24a10:	ldur	x0, [x29, #-56]
   24a14:	cbnz	x0, 24a5c <__gmpq_add@@Base+0x2b8>
   24a18:	mov	sp, x29
   24a1c:	ldp	x20, x19, [sp, #80]
   24a20:	ldp	x22, x21, [sp, #64]
   24a24:	ldp	x24, x23, [sp, #48]
   24a28:	ldp	x26, x25, [sp, #32]
   24a2c:	ldp	x28, x27, [sp, #16]
   24a30:	ldp	x29, x30, [sp], #96
   24a34:	ret
   24a38:	sub	x0, x29, #0x38
   24a3c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24a40:	b	24834 <__gmpq_add@@Base+0x90>
   24a44:	sub	x0, x29, #0x38
   24a48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24a4c:	b	2486c <__gmpq_add@@Base+0xc8>
   24a50:	sub	x0, x29, #0x38
   24a54:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24a58:	b	2489c <__gmpq_add@@Base+0xf8>
   24a5c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24a60:	b	24a18 <__gmpq_add@@Base+0x274>
   24a64:	sub	x0, x29, #0x38
   24a68:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24a6c:	b	2494c <__gmpq_add@@Base+0x1a8>

0000000000024a70 <__gmpq_sub@@Base>:
   24a70:	adrp	x3, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   24a74:	ldr	x3, [x3, #3832]
   24a78:	b	247b0 <__gmpq_add@@Base+0xc>

0000000000024a7c <__gmpq_canonicalize@@Base>:
   24a7c:	stp	x29, x30, [sp, #-32]!
   24a80:	stp	x20, x19, [sp, #16]
   24a84:	mov	x29, sp
   24a88:	sub	sp, sp, #0x20
   24a8c:	ldr	w8, [x0, #20]
   24a90:	mov	x19, x0
   24a94:	tbnz	w8, #31, 24aa4 <__gmpq_canonicalize@@Base+0x28>
   24a98:	cbz	w8, 24b88 <__gmpq_canonicalize@@Base+0x10c>
   24a9c:	ldr	w9, [x19, #4]
   24aa0:	b	24ab8 <__gmpq_canonicalize@@Base+0x3c>
   24aa4:	ldr	w9, [x19, #4]
   24aa8:	neg	w8, w8
   24aac:	str	w8, [x19, #20]
   24ab0:	neg	w9, w9
   24ab4:	str	w9, [x19, #4]
   24ab8:	cmp	w9, #0x0
   24abc:	cneg	w9, w9, mi  // mi = first
   24ac0:	cmp	w9, w8
   24ac4:	csel	w8, w9, w8, gt
   24ac8:	add	w9, w8, #0x1
   24acc:	add	x20, x19, #0x10
   24ad0:	cmp	w8, #0xfdf
   24ad4:	lsl	x1, x9, #3
   24ad8:	stur	xzr, [x29, #-24]
   24adc:	stur	w9, [x29, #-16]
   24ae0:	b.hi	24b7c <__gmpq_canonicalize@@Base+0x100>  // b.pmore
   24ae4:	add	x9, x1, #0xf
   24ae8:	mov	x8, sp
   24aec:	and	x9, x9, #0xffffffff0
   24af0:	sub	x0, x8, x9
   24af4:	mov	sp, x0
   24af8:	stur	x0, [x29, #-8]
   24afc:	sub	x0, x29, #0x10
   24b00:	mov	x1, x19
   24b04:	mov	x2, x20
   24b08:	bl	cf70 <__gmpz_gcd@plt>
   24b0c:	ldur	w8, [x29, #-12]
   24b10:	cmp	w8, #0x1
   24b14:	b.ne	24b40 <__gmpq_canonicalize@@Base+0xc4>  // b.any
   24b18:	ldur	x8, [x29, #-8]
   24b1c:	ldr	x8, [x8]
   24b20:	cmp	x8, #0x1
   24b24:	b.ne	24b40 <__gmpq_canonicalize@@Base+0xc4>  // b.any
   24b28:	ldur	x0, [x29, #-24]
   24b2c:	cbnz	x0, 24b68 <__gmpq_canonicalize@@Base+0xec>
   24b30:	mov	sp, x29
   24b34:	ldp	x20, x19, [sp, #16]
   24b38:	ldp	x29, x30, [sp], #32
   24b3c:	ret
   24b40:	sub	x2, x29, #0x10
   24b44:	mov	x0, x19
   24b48:	mov	x1, x19
   24b4c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b50:	sub	x2, x29, #0x10
   24b54:	mov	x0, x20
   24b58:	mov	x1, x20
   24b5c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b60:	ldur	x0, [x29, #-24]
   24b64:	cbz	x0, 24b30 <__gmpq_canonicalize@@Base+0xb4>
   24b68:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24b6c:	mov	sp, x29
   24b70:	ldp	x20, x19, [sp, #16]
   24b74:	ldp	x29, x30, [sp], #32
   24b78:	ret
   24b7c:	sub	x0, x29, #0x18
   24b80:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24b84:	b	24af8 <__gmpq_canonicalize@@Base+0x7c>
   24b88:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000024b8c <__gmpq_clear@@Base>:
   24b8c:	stp	x29, x30, [sp, #-32]!
   24b90:	stp	x20, x19, [sp, #16]
   24b94:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   24b98:	ldrsw	x8, [x0]
   24b9c:	ldr	x20, [x20, #4016]
   24ba0:	mov	x19, x0
   24ba4:	mov	x29, sp
   24ba8:	cbz	w8, 24bbc <__gmpq_clear@@Base+0x30>
   24bac:	ldr	x9, [x20]
   24bb0:	ldr	x0, [x19, #8]
   24bb4:	lsl	x1, x8, #3
   24bb8:	blr	x9
   24bbc:	ldrsw	x8, [x19, #16]
   24bc0:	cbz	w8, 24bdc <__gmpq_clear@@Base+0x50>
   24bc4:	ldr	x2, [x20]
   24bc8:	ldr	x0, [x19, #24]
   24bcc:	ldp	x20, x19, [sp, #16]
   24bd0:	lsl	x1, x8, #3
   24bd4:	ldp	x29, x30, [sp], #32
   24bd8:	br	x2
   24bdc:	ldp	x20, x19, [sp, #16]
   24be0:	ldp	x29, x30, [sp], #32
   24be4:	ret

0000000000024be8 <__gmpq_clears@@Base>:
   24be8:	sub	sp, sp, #0x100
   24bec:	stp	x29, x30, [sp, #224]
   24bf0:	add	x29, sp, #0xe0
   24bf4:	mov	x8, #0xffffffffffffffc8    	// #-56
   24bf8:	mov	x9, sp
   24bfc:	sub	x10, x29, #0x58
   24c00:	movk	x8, #0xff80, lsl #32
   24c04:	add	x11, x29, #0x20
   24c08:	add	x9, x9, #0x80
   24c0c:	add	x10, x10, #0x38
   24c10:	stp	x20, x19, [sp, #240]
   24c14:	stp	x1, x2, [x29, #-88]
   24c18:	stp	x3, x4, [x29, #-72]
   24c1c:	stp	x5, x6, [x29, #-56]
   24c20:	stur	x7, [x29, #-40]
   24c24:	stp	q0, q1, [sp]
   24c28:	stp	q2, q3, [sp, #32]
   24c2c:	stp	q4, q5, [sp, #64]
   24c30:	stp	q6, q7, [sp, #96]
   24c34:	stp	x9, x8, [x29, #-16]
   24c38:	stp	x11, x10, [x29, #-32]
   24c3c:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   24c40:	ldr	x20, [x20, #4016]
   24c44:	mov	x19, x0
   24c48:	b	24c60 <__gmpq_clears@@Base+0x78>
   24c4c:	ldur	x8, [x29, #-32]
   24c50:	add	x9, x8, #0x8
   24c54:	stur	x9, [x29, #-32]
   24c58:	ldr	x19, [x8]
   24c5c:	cbz	x19, 24cb8 <__gmpq_clears@@Base+0xd0>
   24c60:	ldrsw	x8, [x19]
   24c64:	cbz	w8, 24c78 <__gmpq_clears@@Base+0x90>
   24c68:	ldr	x9, [x20]
   24c6c:	ldr	x0, [x19, #8]
   24c70:	lsl	x1, x8, #3
   24c74:	blr	x9
   24c78:	ldrsw	x8, [x19, #16]
   24c7c:	cbz	w8, 24c90 <__gmpq_clears@@Base+0xa8>
   24c80:	ldr	x9, [x20]
   24c84:	ldr	x0, [x19, #24]
   24c88:	lsl	x1, x8, #3
   24c8c:	blr	x9
   24c90:	ldursw	x8, [x29, #-8]
   24c94:	tbz	w8, #31, 24c4c <__gmpq_clears@@Base+0x64>
   24c98:	add	w9, w8, #0x8
   24c9c:	cmn	w8, #0x8
   24ca0:	stur	w9, [x29, #-8]
   24ca4:	b.gt	24c4c <__gmpq_clears@@Base+0x64>
   24ca8:	ldur	x9, [x29, #-24]
   24cac:	add	x8, x9, x8
   24cb0:	ldr	x19, [x8]
   24cb4:	cbnz	x19, 24c60 <__gmpq_clears@@Base+0x78>
   24cb8:	ldp	x20, x19, [sp, #240]
   24cbc:	ldp	x29, x30, [sp, #224]
   24cc0:	add	sp, sp, #0x100
   24cc4:	ret

0000000000024cc8 <__gmpq_cmp@@Base>:
   24cc8:	add	x2, x1, #0x10
   24ccc:	b	24cd0 <__gmpq_cmp@@Base+0x8>
   24cd0:	stp	x29, x30, [sp, #-96]!
   24cd4:	str	x27, [sp, #16]
   24cd8:	stp	x26, x25, [sp, #32]
   24cdc:	stp	x24, x23, [sp, #48]
   24ce0:	stp	x22, x21, [sp, #64]
   24ce4:	stp	x20, x19, [sp, #80]
   24ce8:	mov	x29, sp
   24cec:	sub	sp, sp, #0x10
   24cf0:	ldr	w25, [x0, #4]
   24cf4:	ldrsw	x11, [x1, #4]
   24cf8:	cbz	w25, 24d98 <__gmpq_cmp@@Base+0xd0>
   24cfc:	cbz	w11, 24dbc <__gmpq_cmp@@Base+0xf4>
   24d00:	eor	w8, w11, w25
   24d04:	tbnz	w8, #31, 24dbc <__gmpq_cmp@@Base+0xf4>
   24d08:	ldrsw	x8, [x2, #4]
   24d0c:	ldr	x9, [x2, #8]
   24d10:	ldrsw	x21, [x0, #20]
   24d14:	ldr	x10, [x0, #24]
   24d18:	sxtw	x14, w25
   24d1c:	add	x9, x9, x8, lsl #3
   24d20:	ldur	x12, [x9, #-8]
   24d24:	add	x9, x10, x21, lsl #3
   24d28:	ldur	x13, [x9, #-8]
   24d2c:	cmp	x14, #0x0
   24d30:	orr	x9, x12, x8
   24d34:	cneg	x4, x14, mi  // mi = first
   24d38:	cmp	x9, #0x1
   24d3c:	cset	w10, eq  // eq = none
   24d40:	orr	x15, x13, x21
   24d44:	mov	x19, x1
   24d48:	mov	x20, x0
   24d4c:	cmp	x15, x10
   24d50:	b.ne	24da0 <__gmpq_cmp@@Base+0xd8>  // b.any
   24d54:	cmp	w25, w11
   24d58:	b.ne	24de0 <__gmpq_cmp@@Base+0x118>  // b.any
   24d5c:	ldr	x8, [x19, #8]
   24d60:	ldr	x9, [x20, #8]
   24d64:	sub	x8, x8, #0x8
   24d68:	sub	x9, x9, #0x8
   24d6c:	subs	x10, x4, #0x1
   24d70:	b.lt	24e04 <__gmpq_cmp@@Base+0x13c>  // b.tstop
   24d74:	lsl	x11, x4, #3
   24d78:	ldr	x12, [x9, x11]
   24d7c:	ldr	x11, [x8, x11]
   24d80:	mov	x4, x10
   24d84:	cmp	x12, x11
   24d88:	b.eq	24d6c <__gmpq_cmp@@Base+0xa4>  // b.none
   24d8c:	mov	w8, #0x1                   	// #1
   24d90:	cneg	w8, w8, ls  // ls = plast
   24d94:	b	24e08 <__gmpq_cmp@@Base+0x140>
   24d98:	neg	w0, w11
   24d9c:	b	24dc0 <__gmpq_cmp@@Base+0xf8>
   24da0:	cmp	x11, #0x0
   24da4:	cneg	x22, x11, mi  // mi = first
   24da8:	add	x26, x22, x21
   24dac:	add	x27, x4, x8
   24db0:	add	x11, x26, #0x1
   24db4:	cmp	x27, x11
   24db8:	b.le	24de8 <__gmpq_cmp@@Base+0x120>
   24dbc:	mov	w0, w25
   24dc0:	mov	sp, x29
   24dc4:	ldp	x20, x19, [sp, #80]
   24dc8:	ldp	x22, x21, [sp, #64]
   24dcc:	ldp	x24, x23, [sp, #48]
   24dd0:	ldp	x26, x25, [sp, #32]
   24dd4:	ldr	x27, [sp, #16]
   24dd8:	ldp	x29, x30, [sp], #96
   24ddc:	ret
   24de0:	sub	w0, w25, w11
   24de4:	b	24dc0 <__gmpq_cmp@@Base+0xf8>
   24de8:	add	x11, x26, x10
   24dec:	add	x15, x27, #0x1
   24df0:	cmp	x11, x15
   24df4:	neg	x11, x14
   24df8:	b.le	24e14 <__gmpq_cmp@@Base+0x14c>
   24dfc:	mov	w0, w11
   24e00:	b	24dc0 <__gmpq_cmp@@Base+0xf8>
   24e04:	mov	w8, wzr
   24e08:	cmp	w25, #0x0
   24e0c:	cneg	w0, w8, le
   24e10:	b	24dc0 <__gmpq_cmp@@Base+0xf8>
   24e14:	ldr	x23, [x20, #8]
   24e18:	ldr	x14, [x19, #8]
   24e1c:	lsl	x16, x27, #6
   24e20:	clz	x13, x13
   24e24:	add	x15, x23, x4, lsl #3
   24e28:	ldur	x15, [x15, #-8]
   24e2c:	add	x14, x14, x22, lsl #3
   24e30:	ldur	x14, [x14, #-8]
   24e34:	clz	x12, x12
   24e38:	clz	x15, x15
   24e3c:	sub	x15, x16, x15
   24e40:	lsl	x16, x26, #6
   24e44:	clz	x14, x14
   24e48:	sub	x14, x16, x14
   24e4c:	sub	x13, x14, x13
   24e50:	sub	x12, x15, x12
   24e54:	add	x14, x13, #0x1
   24e58:	cmp	x12, x14
   24e5c:	mov	w0, w25
   24e60:	b.hi	24dc0 <__gmpq_cmp@@Base+0xf8>  // b.pmore
   24e64:	add	x10, x13, x10
   24e68:	add	x12, x12, #0x1
   24e6c:	cmp	x10, x12
   24e70:	mov	w0, w11
   24e74:	b.hi	24dc0 <__gmpq_cmp@@Base+0xf8>  // b.pmore
   24e78:	cmp	x9, #0x1
   24e7c:	str	xzr, [x29, #24]
   24e80:	b.ne	24eb0 <__gmpq_cmp@@Base+0x1e8>  // b.any
   24e84:	lsl	x1, x26, #3
   24e88:	mov	w8, #0x7f00                	// #32512
   24e8c:	cmp	x1, x8
   24e90:	b.hi	24ef8 <__gmpq_cmp@@Base+0x230>  // b.pmore
   24e94:	add	x9, x1, #0xf
   24e98:	mov	x8, sp
   24e9c:	and	x9, x9, #0xfffffffffffffff0
   24ea0:	sub	x24, x8, x9
   24ea4:	mov	sp, x24
   24ea8:	mov	x8, #0xffffffffffffffff    	// #-1
   24eac:	b	24f60 <__gmpq_cmp@@Base+0x298>
   24eb0:	add	x9, x26, x27
   24eb4:	lsl	x1, x9, #3
   24eb8:	mov	w9, #0x7f00                	// #32512
   24ebc:	cmp	x1, x9
   24ec0:	b.hi	24f10 <__gmpq_cmp@@Base+0x248>  // b.pmore
   24ec4:	add	x10, x1, #0xf
   24ec8:	mov	x9, sp
   24ecc:	and	x10, x10, #0xfffffffffffffff0
   24ed0:	sub	x23, x9, x10
   24ed4:	mov	sp, x23
   24ed8:	cmp	x4, x8
   24edc:	add	x24, x23, x27, lsl #3
   24ee0:	b.ge	24f40 <__gmpq_cmp@@Base+0x278>  // b.tcont
   24ee4:	ldr	x1, [x2, #8]
   24ee8:	ldr	x3, [x20, #8]
   24eec:	mov	x0, x23
   24ef0:	mov	x2, x8
   24ef4:	b	24f54 <__gmpq_cmp@@Base+0x28c>
   24ef8:	add	x0, x29, #0x18
   24efc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24f00:	ldr	x23, [x20, #8]
   24f04:	mov	x24, x0
   24f08:	mov	x8, #0xffffffffffffffff    	// #-1
   24f0c:	b	24f60 <__gmpq_cmp@@Base+0x298>
   24f10:	add	x0, x29, #0x18
   24f14:	stur	x4, [x29, #-8]
   24f18:	mov	x23, x2
   24f1c:	mov	x24, x8
   24f20:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24f24:	ldur	x4, [x29, #-8]
   24f28:	mov	x8, x24
   24f2c:	mov	x2, x23
   24f30:	mov	x23, x0
   24f34:	cmp	x4, x8
   24f38:	add	x24, x23, x27, lsl #3
   24f3c:	b.lt	24ee4 <__gmpq_cmp@@Base+0x21c>  // b.tstop
   24f40:	ldr	x1, [x20, #8]
   24f44:	ldr	x3, [x2, #8]
   24f48:	mov	x0, x23
   24f4c:	mov	x2, x4
   24f50:	mov	x4, x8
   24f54:	bl	ccd0 <__gmpn_mul@plt>
   24f58:	cmp	x0, #0x0
   24f5c:	csetm	x8, eq  // eq = none
   24f60:	cmp	x22, x21
   24f64:	add	x27, x27, x8
   24f68:	b.ge	24f84 <__gmpq_cmp@@Base+0x2bc>  // b.tcont
   24f6c:	ldr	x1, [x20, #24]
   24f70:	ldr	x3, [x19, #8]
   24f74:	mov	x0, x24
   24f78:	mov	x2, x21
   24f7c:	mov	x4, x22
   24f80:	b	24f98 <__gmpq_cmp@@Base+0x2d0>
   24f84:	ldr	x1, [x19, #8]
   24f88:	ldr	x3, [x20, #24]
   24f8c:	mov	x0, x24
   24f90:	mov	x2, x22
   24f94:	mov	x4, x21
   24f98:	bl	ccd0 <__gmpn_mul@plt>
   24f9c:	sub	x8, x27, x26
   24fa0:	cmp	x0, #0x0
   24fa4:	cinc	x19, x8, eq  // eq = none
   24fa8:	cbnz	x19, 24fe4 <__gmpq_cmp@@Base+0x31c>
   24fac:	sub	x8, x24, #0x8
   24fb0:	sub	x9, x23, #0x8
   24fb4:	subs	x10, x27, #0x1
   24fb8:	b.lt	24fe0 <__gmpq_cmp@@Base+0x318>  // b.tstop
   24fbc:	lsl	x11, x27, #3
   24fc0:	ldr	x12, [x9, x11]
   24fc4:	ldr	x11, [x8, x11]
   24fc8:	mov	x27, x10
   24fcc:	cmp	x12, x11
   24fd0:	b.eq	24fb4 <__gmpq_cmp@@Base+0x2ec>  // b.none
   24fd4:	mov	w8, #0x1                   	// #1
   24fd8:	cneg	x19, x8, ls  // ls = plast
   24fdc:	b	24fe4 <__gmpq_cmp@@Base+0x31c>
   24fe0:	mov	x19, xzr
   24fe4:	ldr	x0, [x29, #24]
   24fe8:	cbnz	x0, 24ff8 <__gmpq_cmp@@Base+0x330>
   24fec:	cmp	w25, #0x0
   24ff0:	cneg	w0, w19, lt  // lt = tstop
   24ff4:	b	24dc0 <__gmpq_cmp@@Base+0xf8>
   24ff8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24ffc:	b	24fec <__gmpq_cmp@@Base+0x324>

0000000000025000 <__gmpq_cmp_z@@Base>:
   25000:	adrp	x2, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   25004:	add	x2, x2, #0x9e0
   25008:	b	24cd0 <__gmpq_cmp@@Base+0x8>

000000000002500c <__gmpq_cmp_si@@Base>:
   2500c:	tbnz	x1, #63, 25014 <__gmpq_cmp_si@@Base+0x8>
   25010:	b	bf10 <__gmpq_cmp_ui@plt>
   25014:	sub	sp, sp, #0x30
   25018:	stp	x29, x30, [sp, #32]
   2501c:	ldr	w8, [x0, #4]
   25020:	add	x29, sp, #0x20
   25024:	tbnz	w8, #31, 25038 <__gmpq_cmp_si@@Base+0x2c>
   25028:	mov	w0, #0x1                   	// #1
   2502c:	ldp	x29, x30, [sp, #32]
   25030:	add	sp, sp, #0x30
   25034:	ret
   25038:	neg	w8, w8
   2503c:	str	w8, [sp, #4]
   25040:	ldr	x8, [x0, #8]
   25044:	neg	x1, x1
   25048:	str	x8, [sp, #8]
   2504c:	ldr	w8, [x0, #20]
   25050:	str	w8, [sp, #20]
   25054:	ldr	x8, [x0, #24]
   25058:	mov	x0, sp
   2505c:	str	x8, [sp, #24]
   25060:	bl	bf10 <__gmpq_cmp_ui@plt>
   25064:	neg	w0, w0
   25068:	ldp	x29, x30, [sp, #32]
   2506c:	add	sp, sp, #0x30
   25070:	ret

0000000000025074 <__gmpq_cmp_ui@@Base>:
   25074:	stp	x29, x30, [sp, #-80]!
   25078:	str	x25, [sp, #16]
   2507c:	stp	x24, x23, [sp, #32]
   25080:	stp	x22, x21, [sp, #48]
   25084:	stp	x20, x19, [sp, #64]
   25088:	mov	x29, sp
   2508c:	cbz	x2, 251f8 <__gmpq_cmp_ui@@Base+0x184>
   25090:	mov	x22, x0
   25094:	ldr	w0, [x0, #4]
   25098:	mov	x20, x1
   2509c:	cbz	x1, 251b0 <__gmpq_cmp_ui@@Base+0x13c>
   250a0:	cmp	w0, #0x1
   250a4:	b.lt	250dc <__gmpq_cmp_ui@@Base+0x68>  // b.tstop
   250a8:	ldrsw	x21, [x22, #20]
   250ac:	cmp	x20, x2
   250b0:	sxtw	x19, w0
   250b4:	mov	x3, x2
   250b8:	cinc	x8, x21, hi  // hi = pmore
   250bc:	cmp	x8, x19
   250c0:	b.lt	251b0 <__gmpq_cmp_ui@@Base+0x13c>  // b.tstop
   250c4:	cmp	x3, x20
   250c8:	cinc	x8, x19, hi  // hi = pmore
   250cc:	cmp	x8, x21
   250d0:	b.ge	250e4 <__gmpq_cmp_ui@@Base+0x70>  // b.tcont
   250d4:	neg	w0, w0
   250d8:	b	251b0 <__gmpq_cmp_ui@@Base+0x13c>
   250dc:	mov	w0, #0xffffffff            	// #-1
   250e0:	b	251b0 <__gmpq_cmp_ui@@Base+0x13c>
   250e4:	add	x24, x19, #0x1
   250e8:	add	x8, x21, x24
   250ec:	lsl	x8, x8, #3
   250f0:	add	x1, x8, #0x8
   250f4:	mov	w8, #0x7f00                	// #32512
   250f8:	cmp	x1, x8
   250fc:	str	xzr, [x29, #24]
   25100:	b.hi	251cc <__gmpq_cmp_ui@@Base+0x158>  // b.pmore
   25104:	add	x9, x1, #0xf
   25108:	mov	x8, sp
   2510c:	and	x9, x9, #0xfffffffffffffff0
   25110:	sub	x23, x8, x9
   25114:	mov	sp, x23
   25118:	ldr	x1, [x22, #8]
   2511c:	mov	x0, x23
   25120:	mov	x2, x19
   25124:	add	x24, x23, x24, lsl #3
   25128:	bl	d490 <__gmpn_mul_1@plt>
   2512c:	str	x0, [x23, x19, lsl #3]
   25130:	ldr	x1, [x22, #24]
   25134:	cmp	x0, #0x0
   25138:	mov	x0, x24
   2513c:	mov	x2, x21
   25140:	mov	x3, x20
   25144:	cset	w25, ne  // ne = any
   25148:	cinc	x22, x19, ne  // ne = any
   2514c:	bl	d490 <__gmpn_mul_1@plt>
   25150:	cmp	x0, #0x0
   25154:	cset	w9, ne  // ne = any
   25158:	sub	w10, w22, w21
   2515c:	mov	x8, x0
   25160:	subs	w0, w10, w9
   25164:	str	x8, [x24, x21, lsl #3]
   25168:	b.ne	251a8 <__gmpq_cmp_ui@@Base+0x134>  // b.any
   2516c:	lsl	x8, x25, #3
   25170:	bfi	x8, x19, #4, #60
   25174:	add	x8, x23, x8
   25178:	sub	x9, x23, #0x8
   2517c:	subs	x10, x22, #0x1
   25180:	b.lt	251a4 <__gmpq_cmp_ui@@Base+0x130>  // b.tstop
   25184:	ldr	x11, [x9, x22, lsl #3]
   25188:	ldr	x12, [x8], #-8
   2518c:	mov	x22, x10
   25190:	cmp	x11, x12
   25194:	b.eq	2517c <__gmpq_cmp_ui@@Base+0x108>  // b.none
   25198:	mov	w8, #0x1                   	// #1
   2519c:	cneg	w0, w8, ls  // ls = plast
   251a0:	b	251a8 <__gmpq_cmp_ui@@Base+0x134>
   251a4:	mov	w0, wzr
   251a8:	ldr	x8, [x29, #24]
   251ac:	cbnz	x8, 251e4 <__gmpq_cmp_ui@@Base+0x170>
   251b0:	mov	sp, x29
   251b4:	ldp	x20, x19, [sp, #64]
   251b8:	ldp	x22, x21, [sp, #48]
   251bc:	ldp	x24, x23, [sp, #32]
   251c0:	ldr	x25, [sp, #16]
   251c4:	ldp	x29, x30, [sp], #80
   251c8:	ret
   251cc:	add	x0, x29, #0x18
   251d0:	mov	x23, x3
   251d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   251d8:	mov	x3, x23
   251dc:	mov	x23, x0
   251e0:	b	25118 <__gmpq_cmp_ui@@Base+0xa4>
   251e4:	mov	x19, x0
   251e8:	mov	x0, x8
   251ec:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   251f0:	mov	x0, x19
   251f4:	b	251b0 <__gmpq_cmp_ui@@Base+0x13c>
   251f8:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000251fc <__gmpq_div@@Base>:
   251fc:	stp	x29, x30, [sp, #-80]!
   25200:	str	x25, [sp, #16]
   25204:	stp	x24, x23, [sp, #32]
   25208:	stp	x22, x21, [sp, #48]
   2520c:	stp	x20, x19, [sp, #64]
   25210:	mov	x29, sp
   25214:	sub	sp, sp, #0x40
   25218:	ldrsw	x8, [x2, #4]
   2521c:	cbz	w8, 254bc <__gmpq_div@@Base+0x2c0>
   25220:	mov	x20, x2
   25224:	mov	x21, x1
   25228:	mov	x19, x0
   2522c:	cmp	x0, x2
   25230:	b.eq	2541c <__gmpq_div@@Base+0x220>  // b.none
   25234:	ldr	w9, [x21, #4]
   25238:	cmp	w9, #0x0
   2523c:	cneg	w22, w9, mi  // mi = first
   25240:	cbz	w22, 253bc <__gmpq_div@@Base+0x1c0>
   25244:	cmp	x8, #0x0
   25248:	cneg	x23, x8, mi  // mi = first
   2524c:	cmp	x23, x22
   25250:	csel	x8, x22, x23, gt
   25254:	cmp	x8, #0xfe0
   25258:	lsl	x1, x8, #3
   2525c:	str	xzr, [x29, #24]
   25260:	stur	w8, [x29, #-16]
   25264:	b.hi	25464 <__gmpq_div@@Base+0x268>  // b.pmore
   25268:	add	x9, x1, #0xf
   2526c:	mov	x8, sp
   25270:	and	x9, x9, #0xfffffffffffffff0
   25274:	sub	x0, x8, x9
   25278:	mov	sp, x0
   2527c:	cmp	x23, x22
   25280:	csel	x8, x22, x23, lt  // lt = tstop
   25284:	cmp	x8, #0xfe0
   25288:	lsl	x1, x8, #3
   2528c:	stur	x0, [x29, #-8]
   25290:	stur	w8, [x29, #-48]
   25294:	b.hi	25470 <__gmpq_div@@Base+0x274>  // b.pmore
   25298:	add	x9, x1, #0xf
   2529c:	mov	x8, sp
   252a0:	and	x9, x9, #0xfffffffffffffff0
   252a4:	sub	x0, x8, x9
   252a8:	mov	sp, x0
   252ac:	stur	x0, [x29, #-40]
   252b0:	ldrsw	x24, [x20, #20]
   252b4:	ldrsw	x25, [x21, #20]
   252b8:	mov	w9, #0x7f00                	// #32512
   252bc:	add	x23, x20, #0x10
   252c0:	add	x22, x21, #0x10
   252c4:	cmp	x25, x24
   252c8:	csel	x8, x25, x24, lt  // lt = tstop
   252cc:	lsl	x1, x8, #3
   252d0:	cmp	x1, x9
   252d4:	stur	w8, [x29, #-32]
   252d8:	b.hi	2547c <__gmpq_div@@Base+0x280>  // b.pmore
   252dc:	add	x9, x1, #0xf
   252e0:	mov	x8, sp
   252e4:	and	x9, x9, #0xfffffffffffffff0
   252e8:	sub	x0, x8, x9
   252ec:	mov	sp, x0
   252f0:	cmp	x25, x24
   252f4:	csel	x8, x25, x24, gt
   252f8:	lsl	x1, x8, #3
   252fc:	mov	w9, #0x7f00                	// #32512
   25300:	cmp	x1, x9
   25304:	stur	x0, [x29, #-24]
   25308:	stur	w8, [x29, #-64]
   2530c:	b.hi	25488 <__gmpq_div@@Base+0x28c>  // b.pmore
   25310:	add	x9, x1, #0xf
   25314:	mov	x8, sp
   25318:	and	x9, x9, #0xfffffffffffffff0
   2531c:	sub	x0, x8, x9
   25320:	mov	sp, x0
   25324:	stur	x0, [x29, #-56]
   25328:	sub	x0, x29, #0x10
   2532c:	mov	x1, x21
   25330:	mov	x2, x20
   25334:	bl	cf70 <__gmpz_gcd@plt>
   25338:	sub	x0, x29, #0x20
   2533c:	mov	x1, x23
   25340:	mov	x2, x22
   25344:	bl	cf70 <__gmpz_gcd@plt>
   25348:	sub	x0, x29, #0x30
   2534c:	sub	x2, x29, #0x10
   25350:	mov	x1, x21
   25354:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25358:	sub	x0, x29, #0x40
   2535c:	sub	x2, x29, #0x20
   25360:	mov	x1, x23
   25364:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25368:	sub	x1, x29, #0x30
   2536c:	sub	x2, x29, #0x40
   25370:	mov	x0, x19
   25374:	bl	c4b0 <__gmpz_mul@plt>
   25378:	sub	x0, x29, #0x30
   2537c:	sub	x2, x29, #0x10
   25380:	mov	x1, x20
   25384:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25388:	sub	x0, x29, #0x40
   2538c:	sub	x2, x29, #0x20
   25390:	mov	x1, x22
   25394:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25398:	add	x0, x19, #0x10
   2539c:	sub	x1, x29, #0x30
   253a0:	sub	x2, x29, #0x40
   253a4:	bl	c4b0 <__gmpz_mul@plt>
   253a8:	ldr	w8, [x19, #20]
   253ac:	tbnz	w8, #31, 253e4 <__gmpq_div@@Base+0x1e8>
   253b0:	ldr	x0, [x29, #24]
   253b4:	cbz	x0, 25400 <__gmpq_div@@Base+0x204>
   253b8:	b	25494 <__gmpq_div@@Base+0x298>
   253bc:	mov	x0, x19
   253c0:	ldr	w8, [x0, #16]!
   253c4:	cmp	w8, #0x0
   253c8:	stur	wzr, [x0, #-12]
   253cc:	b.le	2549c <__gmpq_div@@Base+0x2a0>
   253d0:	ldr	x0, [x19, #24]
   253d4:	mov	w8, #0x1                   	// #1
   253d8:	str	x8, [x0]
   253dc:	str	w8, [x19, #20]
   253e0:	b	25400 <__gmpq_div@@Base+0x204>
   253e4:	ldr	w9, [x19, #4]
   253e8:	neg	w8, w8
   253ec:	str	w8, [x19, #20]
   253f0:	neg	w8, w9
   253f4:	str	w8, [x19, #4]
   253f8:	ldr	x0, [x29, #24]
   253fc:	cbnz	x0, 25494 <__gmpq_div@@Base+0x298>
   25400:	mov	sp, x29
   25404:	ldp	x20, x19, [sp, #64]
   25408:	ldp	x22, x21, [sp, #48]
   2540c:	ldp	x24, x23, [sp, #32]
   25410:	ldr	x25, [sp, #16]
   25414:	ldp	x29, x30, [sp], #80
   25418:	ret
   2541c:	cmp	x21, x20
   25420:	b.eq	254a8 <__gmpq_div@@Base+0x2ac>  // b.none
   25424:	ldr	x9, [x20, #8]
   25428:	ldp	w12, w13, [x20, #16]
   2542c:	ldr	x10, [x20, #24]
   25430:	ldr	w11, [x20]
   25434:	cmp	w8, #0x0
   25438:	cneg	w8, w8, le
   2543c:	str	x9, [x20, #24]
   25440:	cneg	w9, w13, le
   25444:	mov	x0, x20
   25448:	mov	x1, x20
   2544c:	mov	x2, x21
   25450:	str	x10, [x20, #8]
   25454:	stp	w11, w8, [x20, #16]
   25458:	stp	w12, w9, [x20]
   2545c:	bl	cfe0 <__gmpq_mul@plt>
   25460:	b	25400 <__gmpq_div@@Base+0x204>
   25464:	add	x0, x29, #0x18
   25468:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2546c:	b	2527c <__gmpq_div@@Base+0x80>
   25470:	add	x0, x29, #0x18
   25474:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25478:	b	252ac <__gmpq_div@@Base+0xb0>
   2547c:	add	x0, x29, #0x18
   25480:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25484:	b	252f0 <__gmpq_div@@Base+0xf4>
   25488:	add	x0, x29, #0x18
   2548c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25490:	b	25324 <__gmpq_div@@Base+0x128>
   25494:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   25498:	b	25400 <__gmpq_div@@Base+0x204>
   2549c:	mov	w1, #0x1                   	// #1
   254a0:	bl	c080 <__gmpz_realloc@plt>
   254a4:	b	253d4 <__gmpq_div@@Base+0x1d8>
   254a8:	mov	w1, #0x1                   	// #1
   254ac:	mov	w2, #0x1                   	// #1
   254b0:	mov	x0, x20
   254b4:	bl	ca60 <__gmpq_set_ui@plt>
   254b8:	b	25400 <__gmpq_div@@Base+0x204>
   254bc:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000254c0 <__gmpq_get_d@@Base>:
   254c0:	str	d8, [sp, #-96]!
   254c4:	stp	x29, x30, [sp, #8]
   254c8:	str	x27, [sp, #24]
   254cc:	stp	x26, x25, [sp, #32]
   254d0:	stp	x24, x23, [sp, #48]
   254d4:	stp	x22, x21, [sp, #64]
   254d8:	stp	x20, x19, [sp, #80]
   254dc:	mov	x29, sp
   254e0:	sub	sp, sp, #0x20
   254e4:	ldrsw	x19, [x0, #4]
   254e8:	cbz	w19, 25618 <__gmpq_get_d@@Base+0x158>
   254ec:	ldrsw	x8, [x0, #20]
   254f0:	cmp	x19, #0x0
   254f4:	stur	xzr, [x29, #-32]
   254f8:	cneg	x24, x19, mi  // mi = first
   254fc:	cmp	x8, #0x0
   25500:	ldr	x25, [x0, #8]
   25504:	ldr	x21, [x0, #24]
   25508:	cneg	x22, x8, mi  // mi = first
   2550c:	mov	x9, #0xfffffffffffffffe    	// #-2
   25510:	sub	x8, x22, x24
   25514:	sub	x9, x9, x8
   25518:	cmn	x8, #0x1
   2551c:	lsl	x20, x9, #6
   25520:	b.lt	2557c <__gmpq_get_d@@Base+0xbc>  // b.tstop
   25524:	add	x27, x8, #0x2
   25528:	lsl	x8, x22, #3
   2552c:	cmp	x22, #0xfdd
   25530:	add	x1, x8, #0x18
   25534:	b.hi	25628 <__gmpq_get_d@@Base+0x168>  // b.pmore
   25538:	add	x9, x1, #0xf
   2553c:	mov	x8, sp
   25540:	and	x9, x9, #0xfffffffffffffff0
   25544:	sub	x23, x8, x9
   25548:	mov	sp, x23
   2554c:	add	x26, x22, #0x2
   25550:	sub	x8, x26, x24
   25554:	lsl	x2, x8, #3
   25558:	mov	x0, x23
   2555c:	mov	w1, wzr
   25560:	bl	c5f0 <memset@plt>
   25564:	add	x0, x23, x27, lsl #3
   25568:	mov	x1, x25
   2556c:	mov	x2, x24
   25570:	bl	ca50 <__gmpn_copyi@plt>
   25574:	mov	x24, x23
   25578:	b	255a8 <__gmpq_get_d@@Base+0xe8>
   2557c:	lsl	x8, x22, #3
   25580:	add	x24, x25, x9, lsl #3
   25584:	cmp	x22, #0xfdd
   25588:	add	x1, x8, #0x18
   2558c:	b.hi	25638 <__gmpq_get_d@@Base+0x178>  // b.pmore
   25590:	add	x9, x1, #0xf
   25594:	mov	x8, sp
   25598:	and	x9, x9, #0xfffffffffffffff0
   2559c:	sub	x23, x8, x9
   255a0:	mov	sp, x23
   255a4:	add	x26, x22, #0x2
   255a8:	sub	x0, x29, #0x18
   255ac:	mov	x1, x24
   255b0:	mov	x2, x26
   255b4:	mov	x3, x21
   255b8:	mov	x4, x22
   255bc:	mov	x5, x23
   255c0:	bl	c320 <__gmpn_div_q@plt>
   255c4:	ldur	x8, [x29, #-8]
   255c8:	sub	x0, x29, #0x18
   255cc:	mov	x2, x19
   255d0:	mov	x3, x20
   255d4:	cmp	x8, #0x0
   255d8:	mov	w8, #0x2                   	// #2
   255dc:	cinc	x1, x8, ne  // ne = any
   255e0:	bl	bf40 <__gmpn_get_d@plt>
   255e4:	ldur	x0, [x29, #-32]
   255e8:	mov	v8.16b, v0.16b
   255ec:	cbnz	x0, 25620 <__gmpq_get_d@@Base+0x160>
   255f0:	mov	v0.16b, v8.16b
   255f4:	mov	sp, x29
   255f8:	ldp	x20, x19, [sp, #80]
   255fc:	ldp	x22, x21, [sp, #64]
   25600:	ldp	x24, x23, [sp, #48]
   25604:	ldp	x26, x25, [sp, #32]
   25608:	ldr	x27, [sp, #24]
   2560c:	ldp	x29, x30, [sp, #8]
   25610:	ldr	d8, [sp], #96
   25614:	ret
   25618:	fmov	d8, xzr
   2561c:	b	255f0 <__gmpq_get_d@@Base+0x130>
   25620:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   25624:	b	255f0 <__gmpq_get_d@@Base+0x130>
   25628:	sub	x0, x29, #0x20
   2562c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25630:	mov	x23, x0
   25634:	b	2554c <__gmpq_get_d@@Base+0x8c>
   25638:	sub	x0, x29, #0x20
   2563c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25640:	mov	x23, x0
   25644:	b	255a4 <__gmpq_get_d@@Base+0xe4>

0000000000025648 <__gmpq_get_den@@Base>:
   25648:	stp	x29, x30, [sp, #-48]!
   2564c:	stp	x22, x21, [sp, #16]
   25650:	stp	x20, x19, [sp, #32]
   25654:	ldr	w22, [x1, #20]
   25658:	ldr	w8, [x0]
   2565c:	mov	x19, x1
   25660:	mov	x20, x0
   25664:	sxtw	x21, w22
   25668:	cmp	w22, w8
   2566c:	mov	x29, sp
   25670:	b.gt	25694 <__gmpq_get_den@@Base+0x4c>
   25674:	ldr	x0, [x20, #8]
   25678:	str	w22, [x20, #4]
   2567c:	ldr	x1, [x19, #24]
   25680:	mov	x2, x21
   25684:	ldp	x20, x19, [sp, #32]
   25688:	ldp	x22, x21, [sp, #16]
   2568c:	ldp	x29, x30, [sp], #48
   25690:	b	ca50 <__gmpn_copyi@plt>
   25694:	mov	x0, x20
   25698:	mov	x1, x21
   2569c:	bl	c080 <__gmpz_realloc@plt>
   256a0:	b	25678 <__gmpq_get_den@@Base+0x30>

00000000000256a4 <__gmpq_get_num@@Base>:
   256a4:	stp	x29, x30, [sp, #-48]!
   256a8:	stp	x22, x21, [sp, #16]
   256ac:	stp	x20, x19, [sp, #32]
   256b0:	ldrsw	x22, [x1, #4]
   256b4:	ldrsw	x8, [x0]
   256b8:	mov	x19, x1
   256bc:	mov	x20, x0
   256c0:	cmp	x22, #0x0
   256c4:	cneg	x21, x22, mi  // mi = first
   256c8:	cmp	x21, x8
   256cc:	mov	x29, sp
   256d0:	b.gt	256f4 <__gmpq_get_num@@Base+0x50>
   256d4:	ldr	x0, [x20, #8]
   256d8:	str	w22, [x20, #4]
   256dc:	ldr	x1, [x19, #8]
   256e0:	mov	x2, x21
   256e4:	ldp	x20, x19, [sp, #32]
   256e8:	ldp	x22, x21, [sp, #16]
   256ec:	ldp	x29, x30, [sp], #48
   256f0:	b	ca50 <__gmpn_copyi@plt>
   256f4:	mov	x0, x20
   256f8:	mov	x1, x21
   256fc:	bl	c080 <__gmpz_realloc@plt>
   25700:	b	256d8 <__gmpq_get_num@@Base+0x34>

0000000000025704 <__gmpq_get_str@@Base>:
   25704:	stp	x29, x30, [sp, #-64]!
   25708:	add	w8, w1, #0x24
   2570c:	cmp	w8, #0x62
   25710:	str	x23, [sp, #16]
   25714:	stp	x22, x21, [sp, #32]
   25718:	stp	x20, x19, [sp, #48]
   2571c:	mov	x29, sp
   25720:	b.ls	2572c <__gmpq_get_str@@Base+0x28>  // b.plast
   25724:	mov	x19, xzr
   25728:	b	2581c <__gmpq_get_str@@Base+0x118>
   2572c:	mov	x21, x2
   25730:	mov	w20, w1
   25734:	mov	x19, x0
   25738:	cbz	x0, 25744 <__gmpq_get_str@@Base+0x40>
   2573c:	mov	x22, xzr
   25740:	b	257ac <__gmpq_get_str@@Base+0xa8>
   25744:	cmp	w20, #0x0
   25748:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2574c:	cneg	w11, w20, mi  // mi = first
   25750:	mov	w8, #0xa                   	// #10
   25754:	ldr	x9, [x9, #3936]
   25758:	cmp	w11, #0x2
   2575c:	csel	w20, w8, w20, lt  // lt = tstop
   25760:	ldr	w11, [x21, #4]
   25764:	cmp	w20, #0x0
   25768:	mov	w10, #0x28                  	// #40
   2576c:	cneg	w8, w20, mi  // mi = first
   25770:	umaddl	x8, w8, w10, x9
   25774:	ldr	w9, [x21, #20]
   25778:	cmp	w11, #0x0
   2577c:	cneg	w10, w11, mi  // mi = first
   25780:	ldr	x8, [x8, #8]
   25784:	add	w9, w10, w9
   25788:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2578c:	ldr	x10, [x10, #3840]
   25790:	sbfiz	x9, x9, #6, #32
   25794:	umulh	x8, x8, x9
   25798:	add	x22, x8, #0x6
   2579c:	ldr	x10, [x10]
   257a0:	mov	x0, x22
   257a4:	blr	x10
   257a8:	mov	x19, x0
   257ac:	mov	x0, x19
   257b0:	mov	w1, w20
   257b4:	mov	x2, x21
   257b8:	bl	c3a0 <__gmpz_get_str@plt>
   257bc:	mov	x0, x19
   257c0:	bl	bf60 <strlen@plt>
   257c4:	ldr	w8, [x21, #20]
   257c8:	cmp	w8, #0x1
   257cc:	b.ne	257e0 <__gmpq_get_str@@Base+0xdc>  // b.any
   257d0:	ldr	x8, [x21, #24]
   257d4:	ldr	x8, [x8]
   257d8:	cmp	x8, #0x1
   257dc:	b.eq	2580c <__gmpq_get_str@@Base+0x108>  // b.none
   257e0:	add	x23, x0, #0x1
   257e4:	add	x2, x21, #0x10
   257e8:	mov	w8, #0x2f                  	// #47
   257ec:	add	x21, x19, x23
   257f0:	strb	w8, [x19, x0]
   257f4:	mov	x0, x21
   257f8:	mov	w1, w20
   257fc:	bl	c3a0 <__gmpz_get_str@plt>
   25800:	mov	x0, x21
   25804:	bl	bf60 <strlen@plt>
   25808:	add	x0, x0, x23
   2580c:	cbz	x22, 2581c <__gmpq_get_str@@Base+0x118>
   25810:	add	x2, x0, #0x1
   25814:	cmp	x22, x2
   25818:	b.ne	25834 <__gmpq_get_str@@Base+0x130>  // b.any
   2581c:	mov	x0, x19
   25820:	ldp	x20, x19, [sp, #48]
   25824:	ldp	x22, x21, [sp, #32]
   25828:	ldr	x23, [sp, #16]
   2582c:	ldp	x29, x30, [sp], #64
   25830:	ret
   25834:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   25838:	ldr	x8, [x8, #3792]
   2583c:	mov	x0, x19
   25840:	mov	x1, x22
   25844:	ldp	x20, x19, [sp, #48]
   25848:	ldr	x3, [x8]
   2584c:	ldp	x22, x21, [sp, #32]
   25850:	ldr	x23, [sp, #16]
   25854:	ldp	x29, x30, [sp], #64
   25858:	br	x3

000000000002585c <__gmpq_init@@Base>:
   2585c:	stp	x29, x30, [sp, #-32]!
   25860:	adrp	x8, 5a000 <__gmp_binvert_limb_table@@Base+0x478>
   25864:	stp	x20, x19, [sp, #16]
   25868:	add	x8, x8, #0x248
   2586c:	mov	w20, #0x1                   	// #1
   25870:	stp	xzr, x8, [x0]
   25874:	str	w20, [x0, #16]
   25878:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2587c:	ldr	x8, [x8, #3840]
   25880:	mov	x19, x0
   25884:	mov	w0, #0x8                   	// #8
   25888:	mov	x29, sp
   2588c:	ldr	x8, [x8]
   25890:	blr	x8
   25894:	str	x0, [x19, #24]
   25898:	str	x20, [x0]
   2589c:	str	w20, [x19, #20]
   258a0:	ldp	x20, x19, [sp, #16]
   258a4:	ldp	x29, x30, [sp], #32
   258a8:	ret

00000000000258ac <__gmpq_inits@@Base>:
   258ac:	sub	sp, sp, #0xf0
   258b0:	stp	x29, x30, [sp, #224]
   258b4:	add	x29, sp, #0xe0
   258b8:	mov	x8, #0xffffffffffffffc8    	// #-56
   258bc:	mov	x9, sp
   258c0:	sub	x10, x29, #0x58
   258c4:	movk	x8, #0xff80, lsl #32
   258c8:	add	x11, x29, #0x10
   258cc:	add	x9, x9, #0x80
   258d0:	add	x10, x10, #0x38
   258d4:	stp	x1, x2, [x29, #-88]
   258d8:	stp	x3, x4, [x29, #-72]
   258dc:	stp	x5, x6, [x29, #-56]
   258e0:	stur	x7, [x29, #-40]
   258e4:	stp	q0, q1, [sp]
   258e8:	stp	q2, q3, [sp, #32]
   258ec:	stp	q4, q5, [sp, #64]
   258f0:	stp	q6, q7, [sp, #96]
   258f4:	stp	x9, x8, [x29, #-16]
   258f8:	stp	x11, x10, [x29, #-32]
   258fc:	b	25914 <__gmpq_inits@@Base+0x68>
   25900:	ldur	x8, [x29, #-32]
   25904:	add	x9, x8, #0x8
   25908:	stur	x9, [x29, #-32]
   2590c:	ldr	x0, [x8]
   25910:	cbz	x0, 25940 <__gmpq_inits@@Base+0x94>
   25914:	bl	cdd0 <__gmpq_init@plt>
   25918:	ldursw	x8, [x29, #-8]
   2591c:	tbz	w8, #31, 25900 <__gmpq_inits@@Base+0x54>
   25920:	add	w9, w8, #0x8
   25924:	cmn	w8, #0x8
   25928:	stur	w9, [x29, #-8]
   2592c:	b.gt	25900 <__gmpq_inits@@Base+0x54>
   25930:	ldur	x9, [x29, #-24]
   25934:	add	x8, x9, x8
   25938:	ldr	x0, [x8]
   2593c:	cbnz	x0, 25914 <__gmpq_inits@@Base+0x68>
   25940:	ldp	x29, x30, [sp, #224]
   25944:	add	sp, sp, #0xf0
   25948:	ret

000000000002594c <__gmpq_inp_str@@Base>:
   2594c:	stp	x29, x30, [sp, #-64]!
   25950:	str	x23, [sp, #16]
   25954:	stp	x22, x21, [sp, #32]
   25958:	stp	x20, x19, [sp, #48]
   2595c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   25960:	ldr	x8, [x8, #3888]
   25964:	mov	x22, x0
   25968:	cmp	x1, #0x0
   2596c:	mov	w20, w2
   25970:	ldr	x8, [x8]
   25974:	ldr	w9, [x22, #16]!
   25978:	mov	x19, x0
   2597c:	mov	w10, #0x1                   	// #1
   25980:	csel	x21, x8, x1, eq  // eq = none
   25984:	cmp	w9, #0x0
   25988:	mov	x29, sp
   2598c:	str	w10, [x22, #4]
   25990:	b.le	25a28 <__gmpq_inp_str@@Base+0xdc>
   25994:	ldr	x0, [x19, #24]
   25998:	mov	w8, #0x1                   	// #1
   2599c:	str	x8, [x0]
   259a0:	mov	x0, x19
   259a4:	mov	x1, x21
   259a8:	mov	w2, w20
   259ac:	bl	d0d0 <__gmpz_inp_str@plt>
   259b0:	mov	x23, x0
   259b4:	cbz	x0, 25a10 <__gmpq_inp_str@@Base+0xc4>
   259b8:	mov	x0, x21
   259bc:	bl	c7f0 <getc@plt>
   259c0:	cmp	w0, #0x2f
   259c4:	b.ne	25a08 <__gmpq_inp_str@@Base+0xbc>  // b.any
   259c8:	mov	x0, x21
   259cc:	bl	c7f0 <getc@plt>
   259d0:	mov	w3, w0
   259d4:	add	x4, x23, #0x2
   259d8:	mov	x0, x22
   259dc:	mov	x1, x21
   259e0:	mov	w2, w20
   259e4:	bl	c9d0 <__gmpz_inp_str_nowhite@plt>
   259e8:	mov	x23, x0
   259ec:	cbnz	x0, 25a10 <__gmpq_inp_str@@Base+0xc4>
   259f0:	ldr	x8, [x19, #24]
   259f4:	mov	w9, #0x1                   	// #1
   259f8:	str	wzr, [x19, #4]
   259fc:	str	w9, [x19, #20]
   25a00:	str	x9, [x8]
   25a04:	b	25a10 <__gmpq_inp_str@@Base+0xc4>
   25a08:	mov	x1, x21
   25a0c:	bl	cc50 <ungetc@plt>
   25a10:	mov	x0, x23
   25a14:	ldp	x20, x19, [sp, #48]
   25a18:	ldp	x22, x21, [sp, #32]
   25a1c:	ldr	x23, [sp, #16]
   25a20:	ldp	x29, x30, [sp], #64
   25a24:	ret
   25a28:	mov	w1, #0x1                   	// #1
   25a2c:	mov	x0, x22
   25a30:	bl	c080 <__gmpz_realloc@plt>
   25a34:	b	25998 <__gmpq_inp_str@@Base+0x4c>

0000000000025a38 <__gmpq_inv@@Base>:
   25a38:	stp	x29, x30, [sp, #-64]!
   25a3c:	stp	x22, x21, [sp, #32]
   25a40:	stp	x20, x19, [sp, #48]
   25a44:	ldrsw	x20, [x1, #4]
   25a48:	ldrsw	x8, [x1, #20]
   25a4c:	mov	x19, x1
   25a50:	mov	x21, x0
   25a54:	str	x23, [sp, #16]
   25a58:	mov	x29, sp
   25a5c:	tbnz	w20, #31, 25a68 <__gmpq_inv@@Base+0x30>
   25a60:	cbnz	w20, 25a70 <__gmpq_inv@@Base+0x38>
   25a64:	bl	bfd0 <__gmp_divide_by_zero@plt>
   25a68:	neg	x20, x20
   25a6c:	neg	x8, x8
   25a70:	add	x22, x21, #0x10
   25a74:	cmp	x21, x19
   25a78:	str	w20, [x21, #20]
   25a7c:	str	w8, [x21, #4]
   25a80:	b.eq	25ad4 <__gmpq_inv@@Base+0x9c>  // b.none
   25a84:	ldrsw	x9, [x21]
   25a88:	cmp	x8, #0x0
   25a8c:	cneg	x23, x8, mi  // mi = first
   25a90:	cmp	x23, x9
   25a94:	b.gt	25b08 <__gmpq_inv@@Base+0xd0>
   25a98:	ldr	x0, [x21, #8]
   25a9c:	ldr	x1, [x19, #24]
   25aa0:	mov	x2, x23
   25aa4:	bl	ca50 <__gmpn_copyi@plt>
   25aa8:	ldrsw	x8, [x22]
   25aac:	cmp	x20, x8
   25ab0:	b.gt	25b18 <__gmpq_inv@@Base+0xe0>
   25ab4:	ldr	x0, [x21, #24]
   25ab8:	ldr	x1, [x19, #8]
   25abc:	mov	x2, x20
   25ac0:	ldp	x20, x19, [sp, #48]
   25ac4:	ldp	x22, x21, [sp, #32]
   25ac8:	ldr	x23, [sp, #16]
   25acc:	ldp	x29, x30, [sp], #64
   25ad0:	b	ca50 <__gmpn_copyi@plt>
   25ad4:	ldr	x8, [x19, #24]
   25ad8:	ldr	x9, [x19, #8]
   25adc:	ldr	x23, [sp, #16]
   25ae0:	str	x8, [x19, #8]
   25ae4:	str	x9, [x19, #24]
   25ae8:	ldr	w8, [x22]
   25aec:	ldr	w9, [x19]
   25af0:	str	w8, [x19]
   25af4:	str	w9, [x22]
   25af8:	ldp	x20, x19, [sp, #48]
   25afc:	ldp	x22, x21, [sp, #32]
   25b00:	ldp	x29, x30, [sp], #64
   25b04:	ret
   25b08:	mov	x0, x21
   25b0c:	mov	x1, x23
   25b10:	bl	c080 <__gmpz_realloc@plt>
   25b14:	b	25a9c <__gmpq_inv@@Base+0x64>
   25b18:	mov	x0, x22
   25b1c:	mov	x1, x20
   25b20:	bl	c080 <__gmpz_realloc@plt>
   25b24:	b	25ab8 <__gmpq_inv@@Base+0x80>

0000000000025b28 <__gmpq_mul_2exp@@Base>:
   25b28:	ldr	w3, [x1, #20]
   25b2c:	ldr	x4, [x1, #24]
   25b30:	mov	x8, x1
   25b34:	add	x1, x0, #0x10
   25b38:	mov	x5, x2
   25b3c:	mov	x2, x8
   25b40:	b	25b44 <__gmpq_mul_2exp@@Base+0x1c>
   25b44:	stp	x29, x30, [sp, #-96]!
   25b48:	stp	x28, x27, [sp, #16]
   25b4c:	stp	x26, x25, [sp, #32]
   25b50:	stp	x24, x23, [sp, #48]
   25b54:	stp	x22, x21, [sp, #64]
   25b58:	stp	x20, x19, [sp, #80]
   25b5c:	ldr	x27, [x4]
   25b60:	mov	w22, w3
   25b64:	sxtw	x8, w22
   25b68:	cmp	x8, #0x0
   25b6c:	cneg	x8, x8, mi  // mi = first
   25b70:	cmp	x27, #0x0
   25b74:	mov	x20, x5
   25b78:	mov	x19, x2
   25b7c:	mov	x21, x1
   25b80:	cset	w28, eq  // eq = none
   25b84:	cmp	x5, #0x40
   25b88:	mov	x23, x0
   25b8c:	mov	x24, x4
   25b90:	mov	x29, sp
   25b94:	b.cc	25bb8 <__gmpq_mul_2exp@@Base+0x90>  // b.lo, b.ul, b.last
   25b98:	cbnz	x27, 25bb8 <__gmpq_mul_2exp@@Base+0x90>
   25b9c:	ldr	x27, [x24, #8]!
   25ba0:	sub	x20, x20, #0x40
   25ba4:	cmp	x27, #0x0
   25ba8:	cset	w28, eq  // eq = none
   25bac:	cmp	x20, #0x40
   25bb0:	b.cc	25bb8 <__gmpq_mul_2exp@@Base+0x90>  // b.lo, b.ul, b.last
   25bb4:	cbz	x27, 25b9c <__gmpq_mul_2exp@@Base+0x74>
   25bb8:	ldrsw	x9, [x21]
   25bbc:	sub	x10, x24, x4
   25bc0:	sub	x25, x8, x10, asr #3
   25bc4:	cmp	x25, x9
   25bc8:	b.gt	25c20 <__gmpq_mul_2exp@@Base+0xf8>
   25bcc:	ldr	x26, [x21, #8]
   25bd0:	cbz	x20, 25c34 <__gmpq_mul_2exp@@Base+0x10c>
   25bd4:	tbnz	w27, #0, 25c34 <__gmpq_mul_2exp@@Base+0x10c>
   25bd8:	rbit	x8, x27
   25bdc:	clz	x8, x8
   25be0:	cmp	x8, x20
   25be4:	csel	x8, x8, x20, cc  // cc = lo, ul, last
   25be8:	cmp	w28, #0x0
   25bec:	csel	x27, x20, x8, ne  // ne = any
   25bf0:	mov	x0, x26
   25bf4:	mov	x1, x24
   25bf8:	mov	x2, x25
   25bfc:	mov	w3, w27
   25c00:	bl	c1a0 <__gmpn_rshift@plt>
   25c04:	add	x8, x26, x25, lsl #3
   25c08:	ldur	x8, [x8, #-8]
   25c0c:	sub	x20, x20, x27
   25c10:	cmp	x8, #0x0
   25c14:	cset	w8, eq  // eq = none
   25c18:	sub	x25, x25, x8
   25c1c:	b	25c4c <__gmpq_mul_2exp@@Base+0x124>
   25c20:	mov	x0, x21
   25c24:	mov	x1, x25
   25c28:	bl	c080 <__gmpz_realloc@plt>
   25c2c:	mov	x26, x0
   25c30:	cbnz	x20, 25bd4 <__gmpq_mul_2exp@@Base+0xac>
   25c34:	cmp	x24, x26
   25c38:	b.eq	25c4c <__gmpq_mul_2exp@@Base+0x124>  // b.none
   25c3c:	mov	x0, x26
   25c40:	mov	x1, x24
   25c44:	mov	x2, x25
   25c48:	bl	ca50 <__gmpn_copyi@plt>
   25c4c:	neg	w8, w25
   25c50:	cmp	w22, #0x0
   25c54:	csel	x8, x25, x8, ge  // ge = tcont
   25c58:	str	w8, [x21, #4]
   25c5c:	cbz	x20, 25c88 <__gmpq_mul_2exp@@Base+0x160>
   25c60:	mov	x0, x23
   25c64:	mov	x1, x19
   25c68:	mov	x2, x20
   25c6c:	ldp	x20, x19, [sp, #80]
   25c70:	ldp	x22, x21, [sp, #64]
   25c74:	ldp	x24, x23, [sp, #48]
   25c78:	ldp	x26, x25, [sp, #32]
   25c7c:	ldp	x28, x27, [sp, #16]
   25c80:	ldp	x29, x30, [sp], #96
   25c84:	b	c6e0 <__gmpz_mul_2exp@plt>
   25c88:	cmp	x23, x19
   25c8c:	b.eq	25cb4 <__gmpq_mul_2exp@@Base+0x18c>  // b.none
   25c90:	mov	x0, x23
   25c94:	mov	x1, x19
   25c98:	ldp	x20, x19, [sp, #80]
   25c9c:	ldp	x22, x21, [sp, #64]
   25ca0:	ldp	x24, x23, [sp, #48]
   25ca4:	ldp	x26, x25, [sp, #32]
   25ca8:	ldp	x28, x27, [sp, #16]
   25cac:	ldp	x29, x30, [sp], #96
   25cb0:	b	c420 <__gmpz_set@plt>
   25cb4:	ldp	x20, x19, [sp, #80]
   25cb8:	ldp	x22, x21, [sp, #64]
   25cbc:	ldp	x24, x23, [sp, #48]
   25cc0:	ldp	x26, x25, [sp, #32]
   25cc4:	ldp	x28, x27, [sp, #16]
   25cc8:	ldp	x29, x30, [sp], #96
   25ccc:	ret

0000000000025cd0 <__gmpq_div_2exp@@Base>:
   25cd0:	stp	x29, x30, [sp, #-16]!
   25cd4:	ldr	w3, [x1, #4]
   25cd8:	mov	x8, x0
   25cdc:	mov	x29, sp
   25ce0:	cbz	w3, 25d00 <__gmpq_div_2exp@@Base+0x30>
   25ce4:	ldr	x4, [x1, #8]
   25ce8:	mov	x5, x2
   25cec:	add	x0, x8, #0x10
   25cf0:	add	x2, x1, #0x10
   25cf4:	mov	x1, x8
   25cf8:	ldp	x29, x30, [sp], #16
   25cfc:	b	25b44 <__gmpq_mul_2exp@@Base+0x1c>
   25d00:	mov	x0, x8
   25d04:	ldr	w9, [x0, #16]!
   25d08:	mov	w10, #0x1                   	// #1
   25d0c:	cmp	w9, #0x0
   25d10:	stur	wzr, [x0, #-12]
   25d14:	str	w10, [x0, #4]
   25d18:	b.le	25d30 <__gmpq_div_2exp@@Base+0x60>
   25d1c:	ldr	x0, [x8, #24]
   25d20:	mov	w8, #0x1                   	// #1
   25d24:	str	x8, [x0]
   25d28:	ldp	x29, x30, [sp], #16
   25d2c:	ret
   25d30:	mov	w1, #0x1                   	// #1
   25d34:	bl	c080 <__gmpz_realloc@plt>
   25d38:	mov	w8, #0x1                   	// #1
   25d3c:	str	x8, [x0]
   25d40:	ldp	x29, x30, [sp], #16
   25d44:	ret

0000000000025d48 <__gmpq_mul@@Base>:
   25d48:	stp	x29, x30, [sp, #-96]!
   25d4c:	str	x27, [sp, #16]
   25d50:	stp	x26, x25, [sp, #32]
   25d54:	stp	x24, x23, [sp, #48]
   25d58:	stp	x22, x21, [sp, #64]
   25d5c:	stp	x20, x19, [sp, #80]
   25d60:	mov	x29, sp
   25d64:	sub	sp, sp, #0x40
   25d68:	mov	x20, x1
   25d6c:	cmp	x1, x2
   25d70:	mov	x19, x0
   25d74:	b.eq	25f08 <__gmpq_mul@@Base+0x1c0>  // b.none
   25d78:	ldr	w8, [x20, #4]
   25d7c:	ldr	w9, [x2, #4]
   25d80:	mov	x21, x2
   25d84:	cmp	w8, #0x0
   25d88:	cneg	w26, w8, mi  // mi = first
   25d8c:	cmp	w9, #0x0
   25d90:	cneg	w24, w9, mi  // mi = first
   25d94:	cbz	w26, 25f2c <__gmpq_mul@@Base+0x1e4>
   25d98:	cbz	w24, 25f2c <__gmpq_mul@@Base+0x1e4>
   25d9c:	ldrsw	x27, [x21, #20]
   25da0:	ldrsw	x25, [x20, #20]
   25da4:	mov	w9, #0x7f00                	// #32512
   25da8:	str	xzr, [x29, #24]
   25dac:	cmp	x26, x27
   25db0:	csel	x8, x26, x27, lt  // lt = tstop
   25db4:	lsl	x1, x8, #3
   25db8:	cmp	x1, x9
   25dbc:	stur	w8, [x29, #-16]
   25dc0:	b.hi	25f70 <__gmpq_mul@@Base+0x228>  // b.pmore
   25dc4:	add	x9, x1, #0xf
   25dc8:	mov	x8, sp
   25dcc:	and	x9, x9, #0xfffffffffffffff0
   25dd0:	sub	x0, x8, x9
   25dd4:	mov	sp, x0
   25dd8:	cmp	x24, x25
   25ddc:	csel	x8, x24, x25, lt  // lt = tstop
   25de0:	lsl	x1, x8, #3
   25de4:	mov	w9, #0x7f00                	// #32512
   25de8:	cmp	x1, x9
   25dec:	stur	x0, [x29, #-8]
   25df0:	stur	w8, [x29, #-32]
   25df4:	b.hi	25f7c <__gmpq_mul@@Base+0x234>  // b.pmore
   25df8:	add	x9, x1, #0xf
   25dfc:	mov	x8, sp
   25e00:	and	x9, x9, #0xfffffffffffffff0
   25e04:	sub	x0, x8, x9
   25e08:	mov	sp, x0
   25e0c:	cmp	x26, x27
   25e10:	csel	x8, x26, x27, gt
   25e14:	add	x22, x20, #0x10
   25e18:	add	x23, x21, #0x10
   25e1c:	cmp	x8, #0xfe0
   25e20:	lsl	x1, x8, #3
   25e24:	stur	x0, [x29, #-24]
   25e28:	stur	w8, [x29, #-48]
   25e2c:	b.hi	25f88 <__gmpq_mul@@Base+0x240>  // b.pmore
   25e30:	add	x9, x1, #0xf
   25e34:	mov	x8, sp
   25e38:	and	x9, x9, #0xfffffffffffffff0
   25e3c:	sub	x0, x8, x9
   25e40:	mov	sp, x0
   25e44:	cmp	x24, x25
   25e48:	csel	x8, x24, x25, gt
   25e4c:	cmp	x8, #0xfe0
   25e50:	lsl	x1, x8, #3
   25e54:	stur	x0, [x29, #-40]
   25e58:	stur	w8, [x29, #-64]
   25e5c:	b.hi	25f94 <__gmpq_mul@@Base+0x24c>  // b.pmore
   25e60:	add	x9, x1, #0xf
   25e64:	mov	x8, sp
   25e68:	and	x9, x9, #0xfffffffffffffff0
   25e6c:	sub	x0, x8, x9
   25e70:	mov	sp, x0
   25e74:	stur	x0, [x29, #-56]
   25e78:	sub	x0, x29, #0x10
   25e7c:	mov	x1, x20
   25e80:	mov	x2, x23
   25e84:	bl	cf70 <__gmpz_gcd@plt>
   25e88:	sub	x0, x29, #0x20
   25e8c:	mov	x1, x21
   25e90:	mov	x2, x22
   25e94:	bl	cf70 <__gmpz_gcd@plt>
   25e98:	sub	x0, x29, #0x30
   25e9c:	sub	x2, x29, #0x10
   25ea0:	mov	x1, x20
   25ea4:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25ea8:	sub	x0, x29, #0x40
   25eac:	sub	x2, x29, #0x20
   25eb0:	mov	x1, x21
   25eb4:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25eb8:	sub	x1, x29, #0x30
   25ebc:	sub	x2, x29, #0x40
   25ec0:	mov	x0, x19
   25ec4:	bl	c4b0 <__gmpz_mul@plt>
   25ec8:	sub	x0, x29, #0x30
   25ecc:	sub	x2, x29, #0x10
   25ed0:	mov	x1, x23
   25ed4:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25ed8:	sub	x0, x29, #0x40
   25edc:	sub	x2, x29, #0x20
   25ee0:	mov	x1, x22
   25ee4:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25ee8:	add	x0, x19, #0x10
   25eec:	sub	x1, x29, #0x30
   25ef0:	sub	x2, x29, #0x40
   25ef4:	bl	c4b0 <__gmpz_mul@plt>
   25ef8:	ldr	x0, [x29, #24]
   25efc:	cbz	x0, 25f50 <__gmpq_mul@@Base+0x208>
   25f00:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   25f04:	b	25f50 <__gmpq_mul@@Base+0x208>
   25f08:	mov	x0, x19
   25f0c:	mov	x1, x20
   25f10:	mov	x2, x20
   25f14:	bl	c4b0 <__gmpz_mul@plt>
   25f18:	add	x1, x20, #0x10
   25f1c:	add	x0, x19, #0x10
   25f20:	mov	x2, x1
   25f24:	bl	c4b0 <__gmpz_mul@plt>
   25f28:	b	25f50 <__gmpq_mul@@Base+0x208>
   25f2c:	mov	x0, x19
   25f30:	ldr	w8, [x0, #16]!
   25f34:	cmp	w8, #0x0
   25f38:	stur	wzr, [x0, #-12]
   25f3c:	b.le	25fa0 <__gmpq_mul@@Base+0x258>
   25f40:	ldr	x0, [x19, #24]
   25f44:	mov	w8, #0x1                   	// #1
   25f48:	str	x8, [x0]
   25f4c:	str	w8, [x19, #20]
   25f50:	mov	sp, x29
   25f54:	ldp	x20, x19, [sp, #80]
   25f58:	ldp	x22, x21, [sp, #64]
   25f5c:	ldp	x24, x23, [sp, #48]
   25f60:	ldp	x26, x25, [sp, #32]
   25f64:	ldr	x27, [sp, #16]
   25f68:	ldp	x29, x30, [sp], #96
   25f6c:	ret
   25f70:	add	x0, x29, #0x18
   25f74:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25f78:	b	25dd8 <__gmpq_mul@@Base+0x90>
   25f7c:	add	x0, x29, #0x18
   25f80:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25f84:	b	25e0c <__gmpq_mul@@Base+0xc4>
   25f88:	add	x0, x29, #0x18
   25f8c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25f90:	b	25e44 <__gmpq_mul@@Base+0xfc>
   25f94:	add	x0, x29, #0x18
   25f98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25f9c:	b	25e74 <__gmpq_mul@@Base+0x12c>
   25fa0:	mov	w1, #0x1                   	// #1
   25fa4:	bl	c080 <__gmpz_realloc@plt>
   25fa8:	b	25f44 <__gmpq_mul@@Base+0x1fc>

0000000000025fac <__gmpq_neg@@Base>:
   25fac:	stp	x29, x30, [sp, #-64]!
   25fb0:	stp	x22, x21, [sp, #32]
   25fb4:	stp	x20, x19, [sp, #48]
   25fb8:	ldrsw	x22, [x1, #4]
   25fbc:	mov	x19, x0
   25fc0:	cmp	x1, x0
   25fc4:	str	x23, [sp, #16]
   25fc8:	mov	x29, sp
   25fcc:	b.eq	26024 <__gmpq_neg@@Base+0x78>  // b.none
   25fd0:	ldrsw	x8, [x19]
   25fd4:	cmp	x22, #0x0
   25fd8:	cneg	x21, x22, mi  // mi = first
   25fdc:	mov	x20, x1
   25fe0:	cmp	x21, x8
   25fe4:	b.gt	26040 <__gmpq_neg@@Base+0x94>
   25fe8:	ldr	x0, [x19, #8]
   25fec:	ldr	x1, [x20, #8]
   25ff0:	mov	x2, x21
   25ff4:	bl	ca50 <__gmpn_copyi@plt>
   25ff8:	mov	x0, x19
   25ffc:	ldr	w23, [x20, #20]
   26000:	ldr	w8, [x0, #16]!
   26004:	sxtw	x21, w23
   26008:	cmp	w23, w8
   2600c:	b.gt	26050 <__gmpq_neg@@Base+0xa4>
   26010:	ldr	x0, [x19, #24]
   26014:	str	w23, [x19, #20]
   26018:	ldr	x1, [x20, #24]
   2601c:	mov	x2, x21
   26020:	bl	ca50 <__gmpn_copyi@plt>
   26024:	neg	w8, w22
   26028:	str	w8, [x19, #4]
   2602c:	ldp	x20, x19, [sp, #48]
   26030:	ldp	x22, x21, [sp, #32]
   26034:	ldr	x23, [sp, #16]
   26038:	ldp	x29, x30, [sp], #64
   2603c:	ret
   26040:	mov	x0, x19
   26044:	mov	x1, x21
   26048:	bl	c080 <__gmpz_realloc@plt>
   2604c:	b	25fec <__gmpq_neg@@Base+0x40>
   26050:	mov	x1, x21
   26054:	bl	c080 <__gmpz_realloc@plt>
   26058:	b	26014 <__gmpq_neg@@Base+0x68>

000000000002605c <__gmpq_out_str@@Base>:
   2605c:	stp	x29, x30, [sp, #-48]!
   26060:	stp	x22, x21, [sp, #16]
   26064:	stp	x20, x19, [sp, #32]
   26068:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2606c:	ldr	x8, [x8, #3856]
   26070:	cmp	x0, #0x0
   26074:	mov	x29, sp
   26078:	mov	x22, x2
   2607c:	ldr	x8, [x8]
   26080:	mov	w21, w1
   26084:	csel	x19, x8, x0, eq  // eq = none
   26088:	mov	x0, x19
   2608c:	bl	c540 <__gmpz_out_str@plt>
   26090:	add	x22, x22, #0x10
   26094:	mov	x20, x0
   26098:	mov	w1, #0x1                   	// #1
   2609c:	mov	x0, x22
   260a0:	bl	d1f0 <__gmpz_cmp_ui@plt>
   260a4:	cbz	w0, 260cc <__gmpq_out_str@@Base+0x70>
   260a8:	mov	w0, #0x2f                  	// #47
   260ac:	mov	x1, x19
   260b0:	bl	c1e0 <putc@plt>
   260b4:	mov	x0, x19
   260b8:	mov	w1, w21
   260bc:	mov	x2, x22
   260c0:	bl	c540 <__gmpz_out_str@plt>
   260c4:	add	x8, x20, x0
   260c8:	add	x20, x8, #0x1
   260cc:	mov	x0, x19
   260d0:	bl	d4a0 <ferror@plt>
   260d4:	cmp	w0, #0x0
   260d8:	csel	x0, x20, xzr, eq  // eq = none
   260dc:	ldp	x20, x19, [sp, #32]
   260e0:	ldp	x22, x21, [sp, #16]
   260e4:	ldp	x29, x30, [sp], #48
   260e8:	ret

00000000000260ec <__gmpq_set@@Base>:
   260ec:	stp	x29, x30, [sp, #-48]!
   260f0:	stp	x20, x19, [sp, #32]
   260f4:	ldrsw	x8, [x1, #4]
   260f8:	ldrsw	x9, [x0]
   260fc:	str	x21, [sp, #16]
   26100:	mov	x19, x1
   26104:	cmp	x8, #0x0
   26108:	cneg	x21, x8, mi  // mi = first
   2610c:	mov	x20, x0
   26110:	cmp	x21, x9
   26114:	mov	x29, sp
   26118:	str	w8, [x0, #4]
   2611c:	b.gt	26164 <__gmpq_set@@Base+0x78>
   26120:	ldr	x0, [x20, #8]
   26124:	ldr	x1, [x19, #8]
   26128:	mov	x2, x21
   2612c:	bl	ca50 <__gmpn_copyi@plt>
   26130:	mov	x0, x20
   26134:	ldrsw	x21, [x19, #20]
   26138:	ldr	w8, [x0, #16]!
   2613c:	cmp	w21, w8
   26140:	str	w21, [x0, #4]
   26144:	b.gt	26174 <__gmpq_set@@Base+0x88>
   26148:	ldr	x0, [x20, #24]
   2614c:	ldr	x1, [x19, #24]
   26150:	mov	x2, x21
   26154:	ldp	x20, x19, [sp, #32]
   26158:	ldr	x21, [sp, #16]
   2615c:	ldp	x29, x30, [sp], #48
   26160:	b	ca50 <__gmpn_copyi@plt>
   26164:	mov	x0, x20
   26168:	mov	x1, x21
   2616c:	bl	c080 <__gmpz_realloc@plt>
   26170:	b	26124 <__gmpq_set@@Base+0x38>
   26174:	mov	x1, x21
   26178:	bl	c080 <__gmpz_realloc@plt>
   2617c:	b	2614c <__gmpq_set@@Base+0x60>

0000000000026180 <__gmpq_set_den@@Base>:
   26180:	stp	x29, x30, [sp, #-32]!
   26184:	stp	x20, x19, [sp, #16]
   26188:	ldrsw	x9, [x1, #4]
   2618c:	mov	x8, x0
   26190:	ldrsw	x10, [x8, #16]!
   26194:	mov	x19, x1
   26198:	cmp	x9, #0x0
   2619c:	cneg	x20, x9, mi  // mi = first
   261a0:	cmp	x20, x10
   261a4:	mov	x29, sp
   261a8:	str	w9, [x8, #4]
   261ac:	b.gt	261c8 <__gmpq_set_den@@Base+0x48>
   261b0:	ldr	x0, [x0, #24]
   261b4:	ldr	x1, [x19, #8]
   261b8:	mov	x2, x20
   261bc:	ldp	x20, x19, [sp, #16]
   261c0:	ldp	x29, x30, [sp], #32
   261c4:	b	ca50 <__gmpn_copyi@plt>
   261c8:	mov	x0, x8
   261cc:	mov	x1, x20
   261d0:	bl	c080 <__gmpz_realloc@plt>
   261d4:	b	261b4 <__gmpq_set_den@@Base+0x34>

00000000000261d8 <__gmpq_set_num@@Base>:
   261d8:	stp	x29, x30, [sp, #-32]!
   261dc:	stp	x20, x19, [sp, #16]
   261e0:	ldrsw	x8, [x1, #4]
   261e4:	ldrsw	x9, [x0]
   261e8:	mov	x19, x1
   261ec:	mov	x29, sp
   261f0:	cmp	x8, #0x0
   261f4:	cneg	x20, x8, mi  // mi = first
   261f8:	cmp	x20, x9
   261fc:	str	w8, [x0, #4]
   26200:	b.gt	2621c <__gmpq_set_num@@Base+0x44>
   26204:	ldr	x0, [x0, #8]
   26208:	ldr	x1, [x19, #8]
   2620c:	mov	x2, x20
   26210:	ldp	x20, x19, [sp, #16]
   26214:	ldp	x29, x30, [sp], #32
   26218:	b	ca50 <__gmpn_copyi@plt>
   2621c:	mov	x1, x20
   26220:	bl	c080 <__gmpz_realloc@plt>
   26224:	b	26208 <__gmpq_set_num@@Base+0x30>

0000000000026228 <__gmpq_set_si@@Base>:
   26228:	stp	x29, x30, [sp, #-48]!
   2622c:	stp	x20, x19, [sp, #32]
   26230:	mov	x19, x0
   26234:	stp	x22, x21, [sp, #16]
   26238:	mov	x29, sp
   2623c:	cbz	x1, 26274 <__gmpq_set_si@@Base+0x4c>
   26240:	ldr	w8, [x19]
   26244:	cmp	x1, #0x0
   26248:	mov	x20, x2
   2624c:	mov	x21, x1
   26250:	cneg	x22, x1, mi  // mi = first
   26254:	cmp	w8, #0x0
   26258:	b.le	262c0 <__gmpq_set_si@@Base+0x98>
   2625c:	ldr	x0, [x19, #8]
   26260:	cmp	x21, #0x0
   26264:	mov	w8, #0x1                   	// #1
   26268:	cneg	w8, w8, le
   2626c:	str	x22, [x0]
   26270:	b	2627c <__gmpq_set_si@@Base+0x54>
   26274:	mov	w8, wzr
   26278:	mov	w20, #0x1                   	// #1
   2627c:	mov	x0, x19
   26280:	ldr	w9, [x0, #16]!
   26284:	cmp	w9, #0x0
   26288:	stur	w8, [x0, #-12]
   2628c:	b.le	262b4 <__gmpq_set_si@@Base+0x8c>
   26290:	ldr	x0, [x19, #24]
   26294:	cmp	x20, #0x0
   26298:	cset	w8, ne  // ne = any
   2629c:	str	x20, [x0]
   262a0:	str	w8, [x19, #20]
   262a4:	ldp	x20, x19, [sp, #32]
   262a8:	ldp	x22, x21, [sp, #16]
   262ac:	ldp	x29, x30, [sp], #48
   262b0:	ret
   262b4:	mov	w1, #0x1                   	// #1
   262b8:	bl	c080 <__gmpz_realloc@plt>
   262bc:	b	26294 <__gmpq_set_si@@Base+0x6c>
   262c0:	mov	w1, #0x1                   	// #1
   262c4:	mov	x0, x19
   262c8:	bl	c080 <__gmpz_realloc@plt>
   262cc:	b	26260 <__gmpq_set_si@@Base+0x38>

00000000000262d0 <__gmpq_set_str@@Base>:
   262d0:	stp	x29, x30, [sp, #-80]!
   262d4:	stp	x22, x21, [sp, #48]
   262d8:	mov	x21, x1
   262dc:	stp	x20, x19, [sp, #64]
   262e0:	mov	x20, x0
   262e4:	mov	w1, #0x2f                  	// #47
   262e8:	mov	x0, x21
   262ec:	str	x25, [sp, #16]
   262f0:	stp	x24, x23, [sp, #32]
   262f4:	mov	x29, sp
   262f8:	mov	w19, w2
   262fc:	bl	cda0 <strchr@plt>
   26300:	cbz	x0, 26384 <__gmpq_set_str@@Base+0xb4>
   26304:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   26308:	ldr	x8, [x8, #3840]
   2630c:	sub	x23, x0, x21
   26310:	add	x24, x23, #0x1
   26314:	mov	x22, x0
   26318:	ldr	x8, [x8]
   2631c:	mov	x0, x24
   26320:	blr	x8
   26324:	mov	x1, x21
   26328:	mov	x2, x23
   2632c:	mov	x25, x0
   26330:	bl	bed0 <memcpy@plt>
   26334:	mov	x0, x20
   26338:	mov	x1, x25
   2633c:	mov	w2, w19
   26340:	strb	wzr, [x25, x23]
   26344:	bl	c0d0 <__gmpz_set_str@plt>
   26348:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2634c:	ldr	x8, [x8, #4016]
   26350:	mov	w21, w0
   26354:	mov	x0, x25
   26358:	mov	x1, x24
   2635c:	ldr	x8, [x8]
   26360:	blr	x8
   26364:	cbz	w21, 263b4 <__gmpq_set_str@@Base+0xe4>
   26368:	mov	w0, w21
   2636c:	ldp	x20, x19, [sp, #64]
   26370:	ldp	x22, x21, [sp, #48]
   26374:	ldp	x24, x23, [sp, #32]
   26378:	ldr	x25, [sp, #16]
   2637c:	ldp	x29, x30, [sp], #80
   26380:	ret
   26384:	mov	x0, x20
   26388:	ldr	w8, [x0, #16]!
   2638c:	mov	w9, #0x1                   	// #1
   26390:	cmp	w8, #0x0
   26394:	str	w9, [x0, #4]
   26398:	b.le	263d8 <__gmpq_set_str@@Base+0x108>
   2639c:	ldr	x0, [x20, #24]
   263a0:	mov	w8, #0x1                   	// #1
   263a4:	str	x8, [x0]
   263a8:	mov	x0, x20
   263ac:	mov	x1, x21
   263b0:	b	263bc <__gmpq_set_str@@Base+0xec>
   263b4:	add	x0, x20, #0x10
   263b8:	add	x1, x22, #0x1
   263bc:	mov	w2, w19
   263c0:	ldp	x20, x19, [sp, #64]
   263c4:	ldp	x22, x21, [sp, #48]
   263c8:	ldp	x24, x23, [sp, #32]
   263cc:	ldr	x25, [sp, #16]
   263d0:	ldp	x29, x30, [sp], #80
   263d4:	b	c0d0 <__gmpz_set_str@plt>
   263d8:	mov	w1, #0x1                   	// #1
   263dc:	bl	c080 <__gmpz_realloc@plt>
   263e0:	b	263a0 <__gmpq_set_str@@Base+0xd0>

00000000000263e4 <__gmpq_set_ui@@Base>:
   263e4:	stp	x29, x30, [sp, #-48]!
   263e8:	stp	x20, x19, [sp, #32]
   263ec:	mov	x19, x0
   263f0:	str	x21, [sp, #16]
   263f4:	mov	x29, sp
   263f8:	cbz	x1, 26420 <__gmpq_set_ui@@Base+0x3c>
   263fc:	ldr	w8, [x19]
   26400:	mov	x20, x2
   26404:	mov	x21, x1
   26408:	cmp	w8, #0x0
   2640c:	b.le	2646c <__gmpq_set_ui@@Base+0x88>
   26410:	ldr	x0, [x19, #8]
   26414:	mov	w8, #0x1                   	// #1
   26418:	str	x21, [x0]
   2641c:	b	26428 <__gmpq_set_ui@@Base+0x44>
   26420:	mov	w8, wzr
   26424:	mov	w20, #0x1                   	// #1
   26428:	mov	x0, x19
   2642c:	ldr	w9, [x0, #16]!
   26430:	cmp	w9, #0x0
   26434:	stur	w8, [x0, #-12]
   26438:	b.le	26460 <__gmpq_set_ui@@Base+0x7c>
   2643c:	ldr	x0, [x19, #24]
   26440:	cmp	x20, #0x0
   26444:	cset	w8, ne  // ne = any
   26448:	str	x20, [x0]
   2644c:	str	w8, [x19, #20]
   26450:	ldp	x20, x19, [sp, #32]
   26454:	ldr	x21, [sp, #16]
   26458:	ldp	x29, x30, [sp], #48
   2645c:	ret
   26460:	mov	w1, #0x1                   	// #1
   26464:	bl	c080 <__gmpz_realloc@plt>
   26468:	b	26440 <__gmpq_set_ui@@Base+0x5c>
   2646c:	mov	w1, #0x1                   	// #1
   26470:	mov	x0, x19
   26474:	bl	c080 <__gmpz_realloc@plt>
   26478:	b	26414 <__gmpq_set_ui@@Base+0x30>

000000000002647c <__gmpq_equal@@Base>:
   2647c:	ldrsw	x9, [x0, #4]
   26480:	ldr	w8, [x1, #4]
   26484:	cmp	w9, w8
   26488:	b.ne	2650c <__gmpq_equal@@Base+0x90>  // b.any
   2648c:	ldrsw	x8, [x0, #20]
   26490:	ldr	w10, [x1, #20]
   26494:	cmp	w8, w10
   26498:	b.ne	2650c <__gmpq_equal@@Base+0x90>  // b.any
   2649c:	cmp	x9, #0x0
   264a0:	cneg	x9, x9, mi  // mi = first
   264a4:	cmp	x9, #0x1
   264a8:	b.lt	264d4 <__gmpq_equal@@Base+0x58>  // b.tstop
   264ac:	ldr	x10, [x0, #8]
   264b0:	ldr	x11, [x1, #8]
   264b4:	ldr	x12, [x10]
   264b8:	ldr	x13, [x11]
   264bc:	cmp	x12, x13
   264c0:	b.ne	2650c <__gmpq_equal@@Base+0x90>  // b.any
   264c4:	subs	x9, x9, #0x1
   264c8:	add	x11, x11, #0x8
   264cc:	add	x10, x10, #0x8
   264d0:	b.ne	264b4 <__gmpq_equal@@Base+0x38>  // b.any
   264d4:	cmp	w8, #0x1
   264d8:	b.lt	26504 <__gmpq_equal@@Base+0x88>  // b.tstop
   264dc:	ldr	x9, [x0, #24]
   264e0:	ldr	x10, [x1, #24]
   264e4:	ldr	x11, [x9]
   264e8:	ldr	x12, [x10]
   264ec:	cmp	x11, x12
   264f0:	b.ne	2650c <__gmpq_equal@@Base+0x90>  // b.any
   264f4:	subs	x8, x8, #0x1
   264f8:	add	x10, x10, #0x8
   264fc:	add	x9, x9, #0x8
   26500:	b.ne	264e4 <__gmpq_equal@@Base+0x68>  // b.any
   26504:	mov	w0, #0x1                   	// #1
   26508:	ret
   2650c:	mov	w0, wzr
   26510:	ret

0000000000026514 <__gmpq_set_z@@Base>:
   26514:	stp	x29, x30, [sp, #-48]!
   26518:	stp	x20, x19, [sp, #32]
   2651c:	ldrsw	x8, [x1, #4]
   26520:	ldrsw	x9, [x0]
   26524:	str	x21, [sp, #16]
   26528:	mov	x20, x1
   2652c:	cmp	x8, #0x0
   26530:	cneg	x21, x8, mi  // mi = first
   26534:	mov	x19, x0
   26538:	cmp	x21, x9
   2653c:	mov	x29, sp
   26540:	str	w8, [x0, #4]
   26544:	b.gt	26588 <__gmpq_set_z@@Base+0x74>
   26548:	ldr	x0, [x19, #8]
   2654c:	ldr	x1, [x20, #8]
   26550:	mov	x2, x21
   26554:	bl	ca50 <__gmpn_copyi@plt>
   26558:	mov	x0, x19
   2655c:	ldr	w8, [x0, #16]!
   26560:	cmp	w8, #0x0
   26564:	b.le	26598 <__gmpq_set_z@@Base+0x84>
   26568:	ldr	x0, [x19, #24]
   2656c:	mov	w8, #0x1                   	// #1
   26570:	str	x8, [x0]
   26574:	str	w8, [x19, #20]
   26578:	ldp	x20, x19, [sp, #32]
   2657c:	ldr	x21, [sp, #16]
   26580:	ldp	x29, x30, [sp], #48
   26584:	ret
   26588:	mov	x0, x19
   2658c:	mov	x1, x21
   26590:	bl	c080 <__gmpz_realloc@plt>
   26594:	b	2654c <__gmpq_set_z@@Base+0x38>
   26598:	mov	w1, #0x1                   	// #1
   2659c:	bl	c080 <__gmpz_realloc@plt>
   265a0:	b	2656c <__gmpq_set_z@@Base+0x58>

00000000000265a4 <__gmpq_set_d@@Base>:
   265a4:	sub	sp, sp, #0x70
   265a8:	stp	d9, d8, [sp, #16]
   265ac:	mov	v8.16b, v0.16b
   265b0:	fmov	x8, d8
   265b4:	mvn	x8, x8
   265b8:	tst	x8, #0x7ff0000000000000
   265bc:	stp	x29, x30, [sp, #32]
   265c0:	str	x25, [sp, #48]
   265c4:	stp	x24, x23, [sp, #64]
   265c8:	stp	x22, x21, [sp, #80]
   265cc:	stp	x20, x19, [sp, #96]
   265d0:	add	x29, sp, #0x10
   265d4:	b.eq	267f8 <__gmpq_set_d@@Base+0x254>  // b.none
   265d8:	fneg	d0, d8
   265dc:	fcmp	d8, #0.0
   265e0:	fcsel	d9, d8, d0, ge  // ge = tcont
   265e4:	mov	x19, x0
   265e8:	mov	x0, sp
   265ec:	mov	v0.16b, v9.16b
   265f0:	bl	d280 <__gmp_extract_double@plt>
   265f4:	cmp	w0, #0x1
   265f8:	sxtw	x20, w0
   265fc:	b.gt	26634 <__gmpq_set_d@@Base+0x90>
   26600:	fcmp	d9, #0.0
   26604:	b.ne	266c4 <__gmpq_set_d@@Base+0x120>  // b.any
   26608:	mov	x0, x19
   2660c:	ldr	w8, [x0, #16]!
   26610:	mov	w9, #0x1                   	// #1
   26614:	cmp	w8, #0x0
   26618:	stur	wzr, [x0, #-12]
   2661c:	str	w9, [x0, #4]
   26620:	b.le	26714 <__gmpq_set_d@@Base+0x170>
   26624:	ldr	x0, [x19, #24]
   26628:	mov	w8, #0x1                   	// #1
   2662c:	str	x8, [x0]
   26630:	b	266a4 <__gmpq_set_d@@Base+0x100>
   26634:	ldr	w8, [x19]
   26638:	cmp	w20, w8
   2663c:	b.gt	266ec <__gmpq_set_d@@Base+0x148>
   26640:	ldr	x21, [x19, #8]
   26644:	cmp	w20, #0x2
   26648:	b.eq	2666c <__gmpq_set_d@@Base+0xc8>  // b.none
   2664c:	subs	x22, x20, #0x2
   26650:	b.eq	26668 <__gmpq_set_d@@Base+0xc4>  // b.none
   26654:	lsl	x8, x20, #3
   26658:	sub	x2, x8, #0x10
   2665c:	mov	x0, x21
   26660:	mov	w1, wzr
   26664:	bl	c5f0 <memset@plt>
   26668:	add	x21, x21, x22, lsl #3
   2666c:	ldr	q0, [sp]
   26670:	mov	x0, x19
   26674:	str	q0, [x21]
   26678:	ldr	w8, [x0, #16]!
   2667c:	cmp	w8, #0x0
   26680:	b.le	26708 <__gmpq_set_d@@Base+0x164>
   26684:	ldr	x0, [x19, #24]
   26688:	mov	w25, #0x1                   	// #1
   2668c:	str	x25, [x0]
   26690:	neg	w8, w20
   26694:	fcmp	d8, #0.0
   26698:	csel	x8, x8, x20, mi  // mi = first
   2669c:	str	w25, [x19, #20]
   266a0:	str	w8, [x19, #4]
   266a4:	ldp	x20, x19, [sp, #96]
   266a8:	ldp	x22, x21, [sp, #80]
   266ac:	ldp	x24, x23, [sp, #64]
   266b0:	ldr	x25, [sp, #48]
   266b4:	ldp	x29, x30, [sp, #32]
   266b8:	ldp	d9, d8, [sp, #16]
   266bc:	add	sp, sp, #0x70
   266c0:	ret
   266c4:	ldr	w8, [x19]
   266c8:	cmp	w8, #0x1
   266cc:	b.le	26720 <__gmpq_set_d@@Base+0x17c>
   266d0:	ldr	x22, [x19, #8]
   266d4:	ldp	x9, x8, [sp]
   266d8:	cbz	x9, 26738 <__gmpq_set_d@@Base+0x194>
   266dc:	str	x8, [x22, #8]
   266e0:	mov	w21, #0x2                   	// #2
   266e4:	mov	x8, x9
   266e8:	b	2673c <__gmpq_set_d@@Base+0x198>
   266ec:	mov	x0, x19
   266f0:	mov	x1, x20
   266f4:	bl	c080 <__gmpz_realloc@plt>
   266f8:	mov	x21, x0
   266fc:	cmp	w20, #0x2
   26700:	b.ne	2664c <__gmpq_set_d@@Base+0xa8>  // b.any
   26704:	b	2666c <__gmpq_set_d@@Base+0xc8>
   26708:	mov	w1, #0x1                   	// #1
   2670c:	bl	c080 <__gmpz_realloc@plt>
   26710:	b	26688 <__gmpq_set_d@@Base+0xe4>
   26714:	mov	w1, #0x1                   	// #1
   26718:	bl	c080 <__gmpz_realloc@plt>
   2671c:	b	26628 <__gmpq_set_d@@Base+0x84>
   26720:	mov	w1, #0x2                   	// #2
   26724:	mov	x0, x19
   26728:	bl	c080 <__gmpz_realloc@plt>
   2672c:	mov	x22, x0
   26730:	ldp	x9, x8, [sp]
   26734:	cbnz	x9, 266dc <__gmpq_set_d@@Base+0x138>
   26738:	mov	w21, #0x1                   	// #1
   2673c:	str	x8, [x22]
   26740:	mov	x0, x19
   26744:	ldrsw	x8, [x0, #16]!
   26748:	sub	x24, x21, x20
   2674c:	add	x20, x24, #0x1
   26750:	cmp	x20, x8
   26754:	b.gt	267e0 <__gmpq_set_d@@Base+0x23c>
   26758:	ldr	x23, [x19, #24]
   2675c:	subs	x25, x20, #0x1
   26760:	b.eq	26774 <__gmpq_set_d@@Base+0x1d0>  // b.none
   26764:	lsl	x2, x24, #3
   26768:	mov	x0, x23
   2676c:	mov	w1, wzr
   26770:	bl	c5f0 <memset@plt>
   26774:	mov	w8, #0x1                   	// #1
   26778:	str	x8, [x23, x25, lsl #3]
   2677c:	ldr	x8, [x22]
   26780:	ldr	x9, [x23]
   26784:	orr	x8, x9, x8
   26788:	rbit	x8, x8
   2678c:	clz	x24, x8
   26790:	cbz	w24, 267d4 <__gmpq_set_d@@Base+0x230>
   26794:	mov	x0, x22
   26798:	mov	x1, x22
   2679c:	mov	x2, x21
   267a0:	mov	w3, w24
   267a4:	bl	c1a0 <__gmpn_rshift@plt>
   267a8:	add	x8, x22, x21, lsl #3
   267ac:	ldur	x8, [x8, #-8]
   267b0:	neg	x9, x24
   267b4:	mov	w10, #0x1                   	// #1
   267b8:	add	x11, x23, x20, lsl #3
   267bc:	cmp	x8, #0x0
   267c0:	lsl	x9, x10, x9
   267c4:	cset	w8, eq  // eq = none
   267c8:	sub	x20, x21, x8
   267cc:	stur	x9, [x11, #-16]
   267d0:	b	26690 <__gmpq_set_d@@Base+0xec>
   267d4:	mov	x25, x20
   267d8:	mov	x20, x21
   267dc:	b	26690 <__gmpq_set_d@@Base+0xec>
   267e0:	mov	x1, x20
   267e4:	bl	c080 <__gmpz_realloc@plt>
   267e8:	mov	x23, x0
   267ec:	subs	x25, x20, #0x1
   267f0:	b.ne	26764 <__gmpq_set_d@@Base+0x1c0>  // b.any
   267f4:	b	26774 <__gmpq_set_d@@Base+0x1d0>
   267f8:	bl	c1b0 <__gmp_invalid_operation@plt>

00000000000267fc <__gmpq_set_f@@Base>:
   267fc:	stp	x29, x30, [sp, #-96]!
   26800:	stp	x26, x25, [sp, #32]
   26804:	stp	x24, x23, [sp, #48]
   26808:	stp	x22, x21, [sp, #64]
   2680c:	stp	x20, x19, [sp, #80]
   26810:	ldrsw	x26, [x1, #4]
   26814:	mov	x19, x0
   26818:	str	x27, [sp, #16]
   2681c:	mov	x29, sp
   26820:	cbz	w26, 268d0 <__gmpq_set_f@@Base+0xd4>
   26824:	ldp	x21, x22, [x1, #8]
   26828:	cmp	w26, #0x0
   2682c:	cneg	x20, x26, lt  // lt = tstop
   26830:	ldr	x25, [x22]
   26834:	cbnz	x25, 26844 <__gmpq_set_f@@Base+0x48>
   26838:	ldr	x25, [x22, #8]!
   2683c:	sub	x20, x20, #0x1
   26840:	cbz	x25, 26838 <__gmpq_set_f@@Base+0x3c>
   26844:	subs	x27, x20, x21
   26848:	b.le	268fc <__gmpq_set_f@@Base+0x100>
   2684c:	ldrsw	x8, [x19]
   26850:	cmp	x20, x8
   26854:	b.gt	26970 <__gmpq_set_f@@Base+0x174>
   26858:	ldr	x24, [x19, #8]
   2685c:	mov	x0, x19
   26860:	ldrsw	x8, [x0, #16]!
   26864:	cmp	x27, x8
   26868:	b.ge	26990 <__gmpq_set_f@@Base+0x194>  // b.tcont
   2686c:	ldr	x23, [x19, #24]
   26870:	tbnz	w25, #0, 269a0 <__gmpq_set_f@@Base+0x1a4>
   26874:	rbit	x8, x25
   26878:	clz	x25, x8
   2687c:	mov	x0, x24
   26880:	mov	x1, x22
   26884:	mov	x2, x20
   26888:	mov	w3, w25
   2688c:	sub	x27, x27, #0x1
   26890:	bl	c1a0 <__gmpn_rshift@plt>
   26894:	sub	x8, x20, #0x1
   26898:	ldr	x9, [x24, x8, lsl #3]
   2689c:	cmp	x9, #0x0
   268a0:	cset	w9, eq  // eq = none
   268a4:	sub	x20, x20, x9
   268a8:	cbz	x27, 268c0 <__gmpq_set_f@@Base+0xc4>
   268ac:	sub	x8, x8, x21
   268b0:	lsl	x2, x8, #3
   268b4:	mov	x0, x23
   268b8:	mov	w1, wzr
   268bc:	bl	c5f0 <memset@plt>
   268c0:	sub	w8, w25, #0x1
   268c4:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   268c8:	lsr	x8, x9, x8
   268cc:	b	269c8 <__gmpq_set_f@@Base+0x1cc>
   268d0:	mov	x0, x19
   268d4:	ldr	w8, [x0, #16]!
   268d8:	mov	w9, #0x1                   	// #1
   268dc:	cmp	w8, #0x0
   268e0:	stur	wzr, [x0, #-12]
   268e4:	str	w9, [x0, #4]
   268e8:	b.le	26964 <__gmpq_set_f@@Base+0x168>
   268ec:	ldr	x0, [x19, #24]
   268f0:	mov	w8, #0x1                   	// #1
   268f4:	str	x8, [x0]
   268f8:	b	269e4 <__gmpq_set_f@@Base+0x1e8>
   268fc:	ldrsw	x8, [x19]
   26900:	cmp	x21, x8
   26904:	b.gt	26a00 <__gmpq_set_f@@Base+0x204>
   26908:	ldr	x23, [x19, #8]
   2690c:	cmp	x20, x21
   26910:	b.eq	26928 <__gmpq_set_f@@Base+0x12c>  // b.none
   26914:	sub	x8, x21, x20
   26918:	lsl	x2, x8, #3
   2691c:	mov	x0, x23
   26920:	mov	w1, wzr
   26924:	bl	c5f0 <memset@plt>
   26928:	add	x8, x23, x21, lsl #3
   2692c:	sub	x0, x8, x20, lsl #3
   26930:	mov	x1, x22
   26934:	mov	x2, x20
   26938:	bl	ca50 <__gmpn_copyi@plt>
   2693c:	mov	x0, x19
   26940:	ldr	w9, [x0, #16]!
   26944:	neg	w8, w21
   26948:	cmp	w26, #0x0
   2694c:	mov	w10, #0x1                   	// #1
   26950:	csel	x8, x21, x8, ge  // ge = tcont
   26954:	cmp	w9, #0x0
   26958:	stur	w8, [x0, #-12]
   2695c:	str	w10, [x0, #4]
   26960:	b.gt	268ec <__gmpq_set_f@@Base+0xf0>
   26964:	mov	w1, #0x1                   	// #1
   26968:	bl	c080 <__gmpz_realloc@plt>
   2696c:	b	268f0 <__gmpq_set_f@@Base+0xf4>
   26970:	mov	x0, x19
   26974:	mov	x1, x20
   26978:	bl	c080 <__gmpz_realloc@plt>
   2697c:	mov	x24, x0
   26980:	mov	x0, x19
   26984:	ldrsw	x8, [x0, #16]!
   26988:	cmp	x27, x8
   2698c:	b.lt	2686c <__gmpq_set_f@@Base+0x70>  // b.tstop
   26990:	add	x1, x27, #0x1
   26994:	bl	c080 <__gmpz_realloc@plt>
   26998:	mov	x23, x0
   2699c:	tbz	w25, #0, 26874 <__gmpq_set_f@@Base+0x78>
   269a0:	mov	x0, x24
   269a4:	mov	x1, x22
   269a8:	mov	x2, x20
   269ac:	bl	ca50 <__gmpn_copyi@plt>
   269b0:	cbz	x27, 269c4 <__gmpq_set_f@@Base+0x1c8>
   269b4:	lsl	x2, x27, #3
   269b8:	mov	x0, x23
   269bc:	mov	w1, wzr
   269c0:	bl	c5f0 <memset@plt>
   269c4:	mov	w8, #0x1                   	// #1
   269c8:	str	x8, [x23, x27, lsl #3]
   269cc:	neg	w8, w20
   269d0:	cmp	w26, #0x0
   269d4:	add	w9, w27, #0x1
   269d8:	csel	x8, x20, x8, ge  // ge = tcont
   269dc:	str	w8, [x19, #4]
   269e0:	str	w9, [x19, #20]
   269e4:	ldp	x20, x19, [sp, #80]
   269e8:	ldp	x22, x21, [sp, #64]
   269ec:	ldp	x24, x23, [sp, #48]
   269f0:	ldp	x26, x25, [sp, #32]
   269f4:	ldr	x27, [sp, #16]
   269f8:	ldp	x29, x30, [sp], #96
   269fc:	ret
   26a00:	mov	x0, x19
   26a04:	mov	x1, x21
   26a08:	bl	c080 <__gmpz_realloc@plt>
   26a0c:	mov	x23, x0
   26a10:	cmp	x20, x21
   26a14:	b.ne	26914 <__gmpq_set_f@@Base+0x118>  // b.any
   26a18:	b	26928 <__gmpq_set_f@@Base+0x12c>

0000000000026a1c <__gmpq_swap@@Base>:
   26a1c:	ldr	w8, [x1]
   26a20:	ldr	w9, [x0]
   26a24:	str	w8, [x0]
   26a28:	ldr	x8, [x1, #16]
   26a2c:	str	w9, [x1]
   26a30:	ldr	x9, [x0, #16]
   26a34:	ldr	w10, [x0, #4]
   26a38:	str	x8, [x0, #16]
   26a3c:	ldr	w8, [x1, #4]
   26a40:	str	w8, [x0, #4]
   26a44:	str	w10, [x1, #4]
   26a48:	str	x9, [x1, #16]
   26a4c:	ldr	x8, [x1, #8]
   26a50:	ldr	x9, [x0, #8]
   26a54:	str	x8, [x0, #8]
   26a58:	str	x9, [x1, #8]
   26a5c:	ldr	x8, [x1, #24]
   26a60:	ldr	x9, [x0, #24]
   26a64:	str	x8, [x0, #24]
   26a68:	str	x9, [x1, #24]
   26a6c:	ret

0000000000026a70 <__gmpn_add@@Base>:
   26a70:	stp	x29, x30, [sp, #-48]!
   26a74:	stp	x22, x21, [sp, #16]
   26a78:	stp	x20, x19, [sp, #32]
   26a7c:	mov	x21, x4
   26a80:	mov	x22, x2
   26a84:	mov	x19, x1
   26a88:	mov	x20, x0
   26a8c:	mov	x29, sp
   26a90:	cbz	x4, 26ad0 <__gmpn_add@@Base+0x60>
   26a94:	mov	x0, x20
   26a98:	mov	x1, x19
   26a9c:	mov	x2, x3
   26aa0:	mov	x3, x21
   26aa4:	bl	ca70 <__gmpn_add_n@plt>
   26aa8:	cbz	x0, 26ad0 <__gmpn_add@@Base+0x60>
   26aac:	mov	w0, #0x1                   	// #1
   26ab0:	cmp	x21, x22
   26ab4:	b.ge	26b74 <__gmpn_add@@Base+0x104>  // b.tcont
   26ab8:	lsl	x8, x21, #3
   26abc:	ldr	x9, [x19, x8]
   26ac0:	add	x21, x21, #0x1
   26ac4:	adds	x9, x9, #0x1
   26ac8:	str	x9, [x20, x8]
   26acc:	b.cs	26ab0 <__gmpn_add@@Base+0x40>  // b.hs, b.nlast
   26ad0:	cmp	x20, x19
   26ad4:	mov	x0, xzr
   26ad8:	b.eq	26b74 <__gmpn_add@@Base+0x104>  // b.none
   26adc:	cmp	x21, x22
   26ae0:	b.ge	26b74 <__gmpn_add@@Base+0x104>  // b.tcont
   26ae4:	sub	x8, x22, x21
   26ae8:	cmp	x8, #0x4
   26aec:	b.cc	26b50 <__gmpn_add@@Base+0xe0>  // b.lo, b.ul, b.last
   26af0:	lsl	x10, x21, #3
   26af4:	lsl	x9, x22, #3
   26af8:	add	x11, x20, x10
   26afc:	add	x12, x19, x9
   26b00:	cmp	x11, x12
   26b04:	b.cs	26b18 <__gmpn_add@@Base+0xa8>  // b.hs, b.nlast
   26b08:	add	x9, x20, x9
   26b0c:	add	x11, x19, x10
   26b10:	cmp	x11, x9
   26b14:	b.cc	26b50 <__gmpn_add@@Base+0xe0>  // b.lo, b.ul, b.last
   26b18:	and	x9, x8, #0xfffffffffffffffc
   26b1c:	add	x11, x10, #0x10
   26b20:	add	x21, x21, x9
   26b24:	add	x10, x19, x11
   26b28:	add	x11, x20, x11
   26b2c:	mov	x12, x9
   26b30:	ldp	q0, q1, [x10, #-16]
   26b34:	add	x10, x10, #0x20
   26b38:	subs	x12, x12, #0x4
   26b3c:	stp	q0, q1, [x11, #-16]
   26b40:	add	x11, x11, #0x20
   26b44:	b.ne	26b30 <__gmpn_add@@Base+0xc0>  // b.any
   26b48:	cmp	x8, x9
   26b4c:	b.eq	26b70 <__gmpn_add@@Base+0x100>  // b.none
   26b50:	lsl	x10, x21, #3
   26b54:	sub	x8, x22, x21
   26b58:	add	x9, x20, x10
   26b5c:	add	x10, x19, x10
   26b60:	ldr	x11, [x10], #8
   26b64:	subs	x8, x8, #0x1
   26b68:	str	x11, [x9], #8
   26b6c:	b.ne	26b60 <__gmpn_add@@Base+0xf0>  // b.any
   26b70:	mov	x0, xzr
   26b74:	ldp	x20, x19, [sp, #32]
   26b78:	ldp	x22, x21, [sp, #16]
   26b7c:	ldp	x29, x30, [sp], #48
   26b80:	ret

0000000000026b84 <__gmpn_add_1@@Base>:
   26b84:	ldr	x8, [x1]
   26b88:	adds	x8, x8, x3
   26b8c:	str	x8, [x0]
   26b90:	b.cc	26c84 <__gmpn_add_1@@Base+0x100>  // b.lo, b.ul, b.last
   26b94:	mov	x11, xzr
   26b98:	sub	x10, x2, #0x1
   26b9c:	mov	w8, #0x1                   	// #1
   26ba0:	mov	w9, #0x1                   	// #1
   26ba4:	cmp	x9, x2
   26ba8:	b.ge	26c7c <__gmpn_add_1@@Base+0xf8>  // b.tcont
   26bac:	add	x12, x1, x11
   26bb0:	ldr	x12, [x12, #8]
   26bb4:	add	x13, x0, x11
   26bb8:	add	x9, x9, #0x1
   26bbc:	add	x11, x11, #0x8
   26bc0:	adds	x12, x12, #0x1
   26bc4:	sub	x10, x10, #0x1
   26bc8:	str	x12, [x13, #8]
   26bcc:	b.cs	26ba4 <__gmpn_add_1@@Base+0x20>  // b.hs, b.nlast
   26bd0:	cmp	x1, x0
   26bd4:	mov	x8, xzr
   26bd8:	b.eq	26c7c <__gmpn_add_1@@Base+0xf8>  // b.none
   26bdc:	cmp	x9, x2
   26be0:	b.ge	26c7c <__gmpn_add_1@@Base+0xf8>  // b.tcont
   26be4:	sub	x8, x2, x9
   26be8:	cmp	x8, #0x4
   26bec:	b.cc	26c5c <__gmpn_add_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26bf0:	add	x13, x0, x11
   26bf4:	lsl	x12, x2, #3
   26bf8:	add	x13, x13, #0x8
   26bfc:	add	x14, x1, x12
   26c00:	cmp	x13, x14
   26c04:	b.cs	26c1c <__gmpn_add_1@@Base+0x98>  // b.hs, b.nlast
   26c08:	add	x13, x1, x11
   26c0c:	add	x12, x0, x12
   26c10:	add	x13, x13, #0x8
   26c14:	cmp	x13, x12
   26c18:	b.cc	26c5c <__gmpn_add_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26c1c:	add	x12, x0, x11
   26c20:	add	x13, x1, x11
   26c24:	and	x11, x8, #0xfffffffffffffffc
   26c28:	and	x14, x10, #0xfffffffffffffffc
   26c2c:	add	x10, x12, #0x18
   26c30:	add	x12, x13, #0x18
   26c34:	add	x9, x14, x9
   26c38:	mov	x13, x11
   26c3c:	ldp	q0, q1, [x12, #-16]
   26c40:	add	x12, x12, #0x20
   26c44:	subs	x13, x13, #0x4
   26c48:	stp	q0, q1, [x10, #-16]
   26c4c:	add	x10, x10, #0x20
   26c50:	b.ne	26c3c <__gmpn_add_1@@Base+0xb8>  // b.any
   26c54:	cmp	x8, x11
   26c58:	b.eq	26d24 <__gmpn_add_1@@Base+0x1a0>  // b.none
   26c5c:	lsl	x10, x9, #3
   26c60:	sub	x8, x2, x9
   26c64:	add	x9, x0, x10
   26c68:	add	x10, x1, x10
   26c6c:	ldr	x11, [x10], #8
   26c70:	subs	x8, x8, #0x1
   26c74:	str	x11, [x9], #8
   26c78:	b.ne	26c6c <__gmpn_add_1@@Base+0xe8>  // b.any
   26c7c:	mov	x0, x8
   26c80:	ret
   26c84:	cmp	x1, x0
   26c88:	mov	x8, xzr
   26c8c:	b.eq	26c7c <__gmpn_add_1@@Base+0xf8>  // b.none
   26c90:	cmp	x2, #0x2
   26c94:	b.lt	26c7c <__gmpn_add_1@@Base+0xf8>  // b.tstop
   26c98:	sub	x8, x2, #0x1
   26c9c:	cmp	x8, #0x4
   26ca0:	b.cc	26cc8 <__gmpn_add_1@@Base+0x144>  // b.lo, b.ul, b.last
   26ca4:	lsl	x9, x2, #3
   26ca8:	add	x10, x0, #0x8
   26cac:	add	x11, x1, x9
   26cb0:	cmp	x10, x11
   26cb4:	b.cs	26cf0 <__gmpn_add_1@@Base+0x16c>  // b.hs, b.nlast
   26cb8:	add	x9, x0, x9
   26cbc:	add	x10, x1, #0x8
   26cc0:	cmp	x10, x9
   26cc4:	b.cs	26cf0 <__gmpn_add_1@@Base+0x16c>  // b.hs, b.nlast
   26cc8:	mov	w9, #0x1                   	// #1
   26ccc:	lsl	x10, x9, #3
   26cd0:	sub	x8, x2, x9
   26cd4:	add	x9, x0, x10
   26cd8:	add	x10, x1, x10
   26cdc:	ldr	x11, [x10], #8
   26ce0:	subs	x8, x8, #0x1
   26ce4:	str	x11, [x9], #8
   26ce8:	b.ne	26cdc <__gmpn_add_1@@Base+0x158>  // b.any
   26cec:	b	26c7c <__gmpn_add_1@@Base+0xf8>
   26cf0:	and	x10, x8, #0xfffffffffffffffc
   26cf4:	add	x11, x1, #0x18
   26cf8:	orr	x9, x10, #0x1
   26cfc:	add	x12, x0, #0x18
   26d00:	mov	x13, x10
   26d04:	ldp	q0, q1, [x11, #-16]
   26d08:	add	x11, x11, #0x20
   26d0c:	subs	x13, x13, #0x4
   26d10:	stp	q0, q1, [x12, #-16]
   26d14:	add	x12, x12, #0x20
   26d18:	b.ne	26d04 <__gmpn_add_1@@Base+0x180>  // b.any
   26d1c:	cmp	x8, x10
   26d20:	b.ne	26ccc <__gmpn_add_1@@Base+0x148>  // b.any
   26d24:	mov	x8, xzr
   26d28:	mov	x0, x8
   26d2c:	ret

0000000000026d30 <__gmpn_add_nc@@Base>:
   26d30:	cmp	x4, #0x1
   26d34:	b	26d3c <__gmpn_add_n@@Base+0x4>

0000000000026d38 <__gmpn_add_n@@Base>:
   26d38:	cmn	xzr, xzr
   26d3c:	lsr	x18, x3, #2
   26d40:	tbz	w3, #0, 26d88 <__gmpn_add_n@@Base+0x50>
   26d44:	ldr	x7, [x1]
   26d48:	ldr	x11, [x2]
   26d4c:	adcs	x13, x7, x11
   26d50:	str	x13, [x0], #8
   26d54:	tbnz	w3, #1, 26d70 <__gmpn_add_n@@Base+0x38>
   26d58:	cbz	x18, 26dec <__gmpn_add_n@@Base+0xb4>
   26d5c:	ldp	x4, x5, [x1, #8]
   26d60:	ldp	x8, x9, [x2, #8]
   26d64:	sub	x1, x1, #0x8
   26d68:	sub	x2, x2, #0x8
   26d6c:	b	26dc4 <__gmpn_add_n@@Base+0x8c>
   26d70:	ldp	x6, x7, [x1, #8]
   26d74:	ldp	x10, x11, [x2, #8]
   26d78:	add	x1, x1, #0x8
   26d7c:	add	x2, x2, #0x8
   26d80:	cbz	x18, 26de0 <__gmpn_add_n@@Base+0xa8>
   26d84:	b	26db0 <__gmpn_add_n@@Base+0x78>
   26d88:	tbnz	w3, #1, 26da0 <__gmpn_add_n@@Base+0x68>
   26d8c:	ldp	x4, x5, [x1]
   26d90:	ldp	x8, x9, [x2]
   26d94:	sub	x1, x1, #0x10
   26d98:	sub	x2, x2, #0x10
   26d9c:	b	26dc4 <__gmpn_add_n@@Base+0x8c>
   26da0:	ldp	x6, x7, [x1]
   26da4:	ldp	x10, x11, [x2]
   26da8:	cbz	x18, 26de0 <__gmpn_add_n@@Base+0xa8>
   26dac:	nop
   26db0:	ldp	x4, x5, [x1, #16]
   26db4:	ldp	x8, x9, [x2, #16]
   26db8:	adcs	x12, x6, x10
   26dbc:	adcs	x13, x7, x11
   26dc0:	stp	x12, x13, [x0], #16
   26dc4:	ldp	x6, x7, [x1, #32]!
   26dc8:	ldp	x10, x11, [x2, #32]!
   26dcc:	adcs	x12, x4, x8
   26dd0:	adcs	x13, x5, x9
   26dd4:	stp	x12, x13, [x0], #16
   26dd8:	sub	x18, x18, #0x1
   26ddc:	cbnz	x18, 26db0 <__gmpn_add_n@@Base+0x78>
   26de0:	adcs	x12, x6, x10
   26de4:	adcs	x13, x7, x11
   26de8:	stp	x12, x13, [x0]
   26dec:	cset	x0, cs  // cs = hs, nlast
   26df0:	ret

0000000000026df4 <__gmpn_sub@@Base>:
   26df4:	stp	x29, x30, [sp, #-48]!
   26df8:	stp	x22, x21, [sp, #16]
   26dfc:	stp	x20, x19, [sp, #32]
   26e00:	mov	x21, x4
   26e04:	mov	x22, x2
   26e08:	mov	x19, x1
   26e0c:	mov	x20, x0
   26e10:	mov	x29, sp
   26e14:	cbz	x4, 26e54 <__gmpn_sub@@Base+0x60>
   26e18:	mov	x0, x20
   26e1c:	mov	x1, x19
   26e20:	mov	x2, x3
   26e24:	mov	x3, x21
   26e28:	bl	c2d0 <__gmpn_sub_n@plt>
   26e2c:	cbz	x0, 26e54 <__gmpn_sub@@Base+0x60>
   26e30:	mov	w0, #0x1                   	// #1
   26e34:	cmp	x21, x22
   26e38:	b.ge	26ef8 <__gmpn_sub@@Base+0x104>  // b.tcont
   26e3c:	lsl	x8, x21, #3
   26e40:	ldr	x9, [x19, x8]
   26e44:	add	x21, x21, #0x1
   26e48:	sub	x10, x9, #0x1
   26e4c:	str	x10, [x20, x8]
   26e50:	cbz	x9, 26e34 <__gmpn_sub@@Base+0x40>
   26e54:	cmp	x20, x19
   26e58:	mov	x0, xzr
   26e5c:	b.eq	26ef8 <__gmpn_sub@@Base+0x104>  // b.none
   26e60:	cmp	x21, x22
   26e64:	b.ge	26ef8 <__gmpn_sub@@Base+0x104>  // b.tcont
   26e68:	sub	x8, x22, x21
   26e6c:	cmp	x8, #0x4
   26e70:	b.cc	26ed4 <__gmpn_sub@@Base+0xe0>  // b.lo, b.ul, b.last
   26e74:	lsl	x10, x21, #3
   26e78:	lsl	x9, x22, #3
   26e7c:	add	x11, x20, x10
   26e80:	add	x12, x19, x9
   26e84:	cmp	x11, x12
   26e88:	b.cs	26e9c <__gmpn_sub@@Base+0xa8>  // b.hs, b.nlast
   26e8c:	add	x9, x20, x9
   26e90:	add	x11, x19, x10
   26e94:	cmp	x11, x9
   26e98:	b.cc	26ed4 <__gmpn_sub@@Base+0xe0>  // b.lo, b.ul, b.last
   26e9c:	and	x9, x8, #0xfffffffffffffffc
   26ea0:	add	x11, x10, #0x10
   26ea4:	add	x21, x21, x9
   26ea8:	add	x10, x19, x11
   26eac:	add	x11, x20, x11
   26eb0:	mov	x12, x9
   26eb4:	ldp	q0, q1, [x10, #-16]
   26eb8:	add	x10, x10, #0x20
   26ebc:	subs	x12, x12, #0x4
   26ec0:	stp	q0, q1, [x11, #-16]
   26ec4:	add	x11, x11, #0x20
   26ec8:	b.ne	26eb4 <__gmpn_sub@@Base+0xc0>  // b.any
   26ecc:	cmp	x8, x9
   26ed0:	b.eq	26ef4 <__gmpn_sub@@Base+0x100>  // b.none
   26ed4:	lsl	x10, x21, #3
   26ed8:	sub	x8, x22, x21
   26edc:	add	x9, x20, x10
   26ee0:	add	x10, x19, x10
   26ee4:	ldr	x11, [x10], #8
   26ee8:	subs	x8, x8, #0x1
   26eec:	str	x11, [x9], #8
   26ef0:	b.ne	26ee4 <__gmpn_sub@@Base+0xf0>  // b.any
   26ef4:	mov	x0, xzr
   26ef8:	ldp	x20, x19, [sp, #32]
   26efc:	ldp	x22, x21, [sp, #16]
   26f00:	ldp	x29, x30, [sp], #48
   26f04:	ret

0000000000026f08 <__gmpn_sub_1@@Base>:
   26f08:	ldr	x8, [x1]
   26f0c:	subs	x8, x8, x3
   26f10:	str	x8, [x0]
   26f14:	b.cs	27008 <__gmpn_sub_1@@Base+0x100>  // b.hs, b.nlast
   26f18:	mov	x11, xzr
   26f1c:	sub	x10, x2, #0x1
   26f20:	mov	w8, #0x1                   	// #1
   26f24:	mov	w9, #0x1                   	// #1
   26f28:	cmp	x9, x2
   26f2c:	b.ge	27000 <__gmpn_sub_1@@Base+0xf8>  // b.tcont
   26f30:	add	x12, x1, x11
   26f34:	ldr	x12, [x12, #8]
   26f38:	add	x13, x0, x11
   26f3c:	add	x9, x9, #0x1
   26f40:	add	x11, x11, #0x8
   26f44:	sub	x14, x12, #0x1
   26f48:	sub	x10, x10, #0x1
   26f4c:	str	x14, [x13, #8]
   26f50:	cbz	x12, 26f28 <__gmpn_sub_1@@Base+0x20>
   26f54:	cmp	x1, x0
   26f58:	mov	x8, xzr
   26f5c:	b.eq	27000 <__gmpn_sub_1@@Base+0xf8>  // b.none
   26f60:	cmp	x9, x2
   26f64:	b.ge	27000 <__gmpn_sub_1@@Base+0xf8>  // b.tcont
   26f68:	sub	x8, x2, x9
   26f6c:	cmp	x8, #0x4
   26f70:	b.cc	26fe0 <__gmpn_sub_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26f74:	add	x13, x0, x11
   26f78:	lsl	x12, x2, #3
   26f7c:	add	x13, x13, #0x8
   26f80:	add	x14, x1, x12
   26f84:	cmp	x13, x14
   26f88:	b.cs	26fa0 <__gmpn_sub_1@@Base+0x98>  // b.hs, b.nlast
   26f8c:	add	x13, x1, x11
   26f90:	add	x12, x0, x12
   26f94:	add	x13, x13, #0x8
   26f98:	cmp	x13, x12
   26f9c:	b.cc	26fe0 <__gmpn_sub_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26fa0:	add	x12, x0, x11
   26fa4:	add	x13, x1, x11
   26fa8:	and	x11, x8, #0xfffffffffffffffc
   26fac:	and	x14, x10, #0xfffffffffffffffc
   26fb0:	add	x10, x12, #0x18
   26fb4:	add	x12, x13, #0x18
   26fb8:	add	x9, x14, x9
   26fbc:	mov	x13, x11
   26fc0:	ldp	q0, q1, [x12, #-16]
   26fc4:	add	x12, x12, #0x20
   26fc8:	subs	x13, x13, #0x4
   26fcc:	stp	q0, q1, [x10, #-16]
   26fd0:	add	x10, x10, #0x20
   26fd4:	b.ne	26fc0 <__gmpn_sub_1@@Base+0xb8>  // b.any
   26fd8:	cmp	x8, x11
   26fdc:	b.eq	270a8 <__gmpn_sub_1@@Base+0x1a0>  // b.none
   26fe0:	lsl	x10, x9, #3
   26fe4:	sub	x8, x2, x9
   26fe8:	add	x9, x0, x10
   26fec:	add	x10, x1, x10
   26ff0:	ldr	x11, [x10], #8
   26ff4:	subs	x8, x8, #0x1
   26ff8:	str	x11, [x9], #8
   26ffc:	b.ne	26ff0 <__gmpn_sub_1@@Base+0xe8>  // b.any
   27000:	mov	x0, x8
   27004:	ret
   27008:	cmp	x1, x0
   2700c:	mov	x8, xzr
   27010:	b.eq	27000 <__gmpn_sub_1@@Base+0xf8>  // b.none
   27014:	cmp	x2, #0x2
   27018:	b.lt	27000 <__gmpn_sub_1@@Base+0xf8>  // b.tstop
   2701c:	sub	x8, x2, #0x1
   27020:	cmp	x8, #0x4
   27024:	b.cc	2704c <__gmpn_sub_1@@Base+0x144>  // b.lo, b.ul, b.last
   27028:	lsl	x9, x2, #3
   2702c:	add	x10, x0, #0x8
   27030:	add	x11, x1, x9
   27034:	cmp	x10, x11
   27038:	b.cs	27074 <__gmpn_sub_1@@Base+0x16c>  // b.hs, b.nlast
   2703c:	add	x9, x0, x9
   27040:	add	x10, x1, #0x8
   27044:	cmp	x10, x9
   27048:	b.cs	27074 <__gmpn_sub_1@@Base+0x16c>  // b.hs, b.nlast
   2704c:	mov	w9, #0x1                   	// #1
   27050:	lsl	x10, x9, #3
   27054:	sub	x8, x2, x9
   27058:	add	x9, x0, x10
   2705c:	add	x10, x1, x10
   27060:	ldr	x11, [x10], #8
   27064:	subs	x8, x8, #0x1
   27068:	str	x11, [x9], #8
   2706c:	b.ne	27060 <__gmpn_sub_1@@Base+0x158>  // b.any
   27070:	b	27000 <__gmpn_sub_1@@Base+0xf8>
   27074:	and	x10, x8, #0xfffffffffffffffc
   27078:	add	x11, x1, #0x18
   2707c:	orr	x9, x10, #0x1
   27080:	add	x12, x0, #0x18
   27084:	mov	x13, x10
   27088:	ldp	q0, q1, [x11, #-16]
   2708c:	add	x11, x11, #0x20
   27090:	subs	x13, x13, #0x4
   27094:	stp	q0, q1, [x12, #-16]
   27098:	add	x12, x12, #0x20
   2709c:	b.ne	27088 <__gmpn_sub_1@@Base+0x180>  // b.any
   270a0:	cmp	x8, x10
   270a4:	b.ne	27050 <__gmpn_sub_1@@Base+0x148>  // b.any
   270a8:	mov	x8, xzr
   270ac:	mov	x0, x8
   270b0:	ret
   270b4:	nop
   270b8:	nop
   270bc:	nop

00000000000270c0 <__gmpn_sub_nc@@Base>:
   270c0:	cmp	xzr, x4
   270c4:	b	270cc <__gmpn_sub_n@@Base+0x4>

00000000000270c8 <__gmpn_sub_n@@Base>:
   270c8:	cmp	xzr, xzr
   270cc:	lsr	x18, x3, #2
   270d0:	tbz	w3, #0, 27118 <__gmpn_sub_n@@Base+0x50>
   270d4:	ldr	x7, [x1]
   270d8:	ldr	x11, [x2]
   270dc:	sbcs	x13, x7, x11
   270e0:	str	x13, [x0], #8
   270e4:	tbnz	w3, #1, 27100 <__gmpn_sub_n@@Base+0x38>
   270e8:	cbz	x18, 2717c <__gmpn_sub_n@@Base+0xb4>
   270ec:	ldp	x4, x5, [x1, #8]
   270f0:	ldp	x8, x9, [x2, #8]
   270f4:	sub	x1, x1, #0x8
   270f8:	sub	x2, x2, #0x8
   270fc:	b	27154 <__gmpn_sub_n@@Base+0x8c>
   27100:	ldp	x6, x7, [x1, #8]
   27104:	ldp	x10, x11, [x2, #8]
   27108:	add	x1, x1, #0x8
   2710c:	add	x2, x2, #0x8
   27110:	cbz	x18, 27170 <__gmpn_sub_n@@Base+0xa8>
   27114:	b	27140 <__gmpn_sub_n@@Base+0x78>
   27118:	tbnz	w3, #1, 27130 <__gmpn_sub_n@@Base+0x68>
   2711c:	ldp	x4, x5, [x1]
   27120:	ldp	x8, x9, [x2]
   27124:	sub	x1, x1, #0x10
   27128:	sub	x2, x2, #0x10
   2712c:	b	27154 <__gmpn_sub_n@@Base+0x8c>
   27130:	ldp	x6, x7, [x1]
   27134:	ldp	x10, x11, [x2]
   27138:	cbz	x18, 27170 <__gmpn_sub_n@@Base+0xa8>
   2713c:	nop
   27140:	ldp	x4, x5, [x1, #16]
   27144:	ldp	x8, x9, [x2, #16]
   27148:	sbcs	x12, x6, x10
   2714c:	sbcs	x13, x7, x11
   27150:	stp	x12, x13, [x0], #16
   27154:	ldp	x6, x7, [x1, #32]!
   27158:	ldp	x10, x11, [x2, #32]!
   2715c:	sbcs	x12, x4, x8
   27160:	sbcs	x13, x5, x9
   27164:	stp	x12, x13, [x0], #16
   27168:	sub	x18, x18, #0x1
   2716c:	cbnz	x18, 27140 <__gmpn_sub_n@@Base+0x78>
   27170:	sbcs	x12, x6, x10
   27174:	sbcs	x13, x7, x11
   27178:	stp	x12, x13, [x0]
   2717c:	cset	x0, cc  // cc = lo, ul, last
   27180:	ret
   27184:	nop
   27188:	nop
   2718c:	nop

0000000000027190 <__gmpn_cnd_add_n@@Base>:
   27190:	cmp	x0, #0x1
   27194:	sbc	x0, x0, x0
   27198:	cmn	xzr, xzr
   2719c:	lsr	x18, x4, #2
   271a0:	tbz	w4, #0, 271ec <__gmpn_cnd_add_n@@Base+0x5c>
   271a4:	ldr	x13, [x3]
   271a8:	ldr	x11, [x2]
   271ac:	bic	x7, x13, x0
   271b0:	adcs	x9, x11, x7
   271b4:	str	x9, [x1]
   271b8:	tbnz	w4, #1, 271d8 <__gmpn_cnd_add_n@@Base+0x48>
   271bc:	cbz	x18, 27264 <__gmpn_cnd_add_n@@Base+0xd4>
   271c0:	ldp	x12, x13, [x3, #8]
   271c4:	ldp	x10, x11, [x2, #8]
   271c8:	sub	x2, x2, #0x8
   271cc:	sub	x3, x3, #0x8
   271d0:	sub	x1, x1, #0x18
   271d4:	b	2722c <__gmpn_cnd_add_n@@Base+0x9c>
   271d8:	ldp	x12, x13, [x3, #8]!
   271dc:	ldp	x10, x11, [x2, #8]!
   271e0:	sub	x1, x1, #0x8
   271e4:	cbz	x18, 27250 <__gmpn_cnd_add_n@@Base+0xc0>
   271e8:	b	27210 <__gmpn_cnd_add_n@@Base+0x80>
   271ec:	ldp	x12, x13, [x3]
   271f0:	ldp	x10, x11, [x2]
   271f4:	tbnz	w4, #1, 27208 <__gmpn_cnd_add_n@@Base+0x78>
   271f8:	sub	x2, x2, #0x10
   271fc:	sub	x3, x3, #0x10
   27200:	sub	x1, x1, #0x20
   27204:	b	2722c <__gmpn_cnd_add_n@@Base+0x9c>
   27208:	sub	x1, x1, #0x10
   2720c:	cbz	x18, 27250 <__gmpn_cnd_add_n@@Base+0xc0>
   27210:	bic	x6, x12, x0
   27214:	bic	x7, x13, x0
   27218:	ldp	x12, x13, [x3, #16]
   2721c:	adcs	x8, x10, x6
   27220:	adcs	x9, x11, x7
   27224:	ldp	x10, x11, [x2, #16]
   27228:	stp	x8, x9, [x1, #16]
   2722c:	bic	x6, x12, x0
   27230:	bic	x7, x13, x0
   27234:	ldp	x12, x13, [x3, #32]!
   27238:	adcs	x8, x10, x6
   2723c:	adcs	x9, x11, x7
   27240:	ldp	x10, x11, [x2, #32]!
   27244:	stp	x8, x9, [x1, #32]!
   27248:	sub	x18, x18, #0x1
   2724c:	cbnz	x18, 27210 <__gmpn_cnd_add_n@@Base+0x80>
   27250:	bic	x6, x12, x0
   27254:	bic	x7, x13, x0
   27258:	adcs	x8, x10, x6
   2725c:	adcs	x9, x11, x7
   27260:	stp	x8, x9, [x1, #16]
   27264:	cset	x0, cs  // cs = hs, nlast
   27268:	ret
   2726c:	nop

0000000000027270 <__gmpn_cnd_sub_n@@Base>:
   27270:	cmp	x0, #0x1
   27274:	sbc	x0, x0, x0
   27278:	cmp	xzr, xzr
   2727c:	lsr	x18, x4, #2
   27280:	tbz	w4, #0, 272cc <__gmpn_cnd_sub_n@@Base+0x5c>
   27284:	ldr	x13, [x3]
   27288:	ldr	x11, [x2]
   2728c:	bic	x7, x13, x0
   27290:	sbcs	x9, x11, x7
   27294:	str	x9, [x1]
   27298:	tbnz	w4, #1, 272b8 <__gmpn_cnd_sub_n@@Base+0x48>
   2729c:	cbz	x18, 27344 <__gmpn_cnd_sub_n@@Base+0xd4>
   272a0:	ldp	x12, x13, [x3, #8]
   272a4:	ldp	x10, x11, [x2, #8]
   272a8:	sub	x2, x2, #0x8
   272ac:	sub	x3, x3, #0x8
   272b0:	sub	x1, x1, #0x18
   272b4:	b	2730c <__gmpn_cnd_sub_n@@Base+0x9c>
   272b8:	ldp	x12, x13, [x3, #8]!
   272bc:	ldp	x10, x11, [x2, #8]!
   272c0:	sub	x1, x1, #0x8
   272c4:	cbz	x18, 27330 <__gmpn_cnd_sub_n@@Base+0xc0>
   272c8:	b	272f0 <__gmpn_cnd_sub_n@@Base+0x80>
   272cc:	ldp	x12, x13, [x3]
   272d0:	ldp	x10, x11, [x2]
   272d4:	tbnz	w4, #1, 272e8 <__gmpn_cnd_sub_n@@Base+0x78>
   272d8:	sub	x2, x2, #0x10
   272dc:	sub	x3, x3, #0x10
   272e0:	sub	x1, x1, #0x20
   272e4:	b	2730c <__gmpn_cnd_sub_n@@Base+0x9c>
   272e8:	sub	x1, x1, #0x10
   272ec:	cbz	x18, 27330 <__gmpn_cnd_sub_n@@Base+0xc0>
   272f0:	bic	x6, x12, x0
   272f4:	bic	x7, x13, x0
   272f8:	ldp	x12, x13, [x3, #16]
   272fc:	sbcs	x8, x10, x6
   27300:	sbcs	x9, x11, x7
   27304:	ldp	x10, x11, [x2, #16]
   27308:	stp	x8, x9, [x1, #16]
   2730c:	bic	x6, x12, x0
   27310:	bic	x7, x13, x0
   27314:	ldp	x12, x13, [x3, #32]!
   27318:	sbcs	x8, x10, x6
   2731c:	sbcs	x9, x11, x7
   27320:	ldp	x10, x11, [x2, #32]!
   27324:	stp	x8, x9, [x1, #32]!
   27328:	sub	x18, x18, #0x1
   2732c:	cbnz	x18, 272f0 <__gmpn_cnd_sub_n@@Base+0x80>
   27330:	bic	x6, x12, x0
   27334:	bic	x7, x13, x0
   27338:	sbcs	x8, x10, x6
   2733c:	sbcs	x9, x11, x7
   27340:	stp	x8, x9, [x1, #16]
   27344:	cset	x0, cc  // cc = lo, ul, last
   27348:	ret

000000000002734c <__gmpn_cnd_swap@@Base>:
   2734c:	sub	sp, sp, #0x10
   27350:	cmp	x0, #0x0
   27354:	csetm	x8, ne  // ne = any
   27358:	cmp	x3, #0x1
   2735c:	str	x8, [sp, #8]
   27360:	b.lt	27390 <__gmpn_cnd_swap@@Base+0x44>  // b.tstop
   27364:	ldr	x8, [x1]
   27368:	ldr	x9, [x2]
   2736c:	ldr	x10, [sp, #8]
   27370:	subs	x3, x3, #0x1
   27374:	eor	x11, x9, x8
   27378:	and	x10, x11, x10
   2737c:	eor	x8, x10, x8
   27380:	eor	x9, x10, x9
   27384:	str	x8, [x1], #8
   27388:	str	x9, [x2], #8
   2738c:	b.ne	27364 <__gmpn_cnd_swap@@Base+0x18>  // b.any
   27390:	add	sp, sp, #0x10
   27394:	ret

0000000000027398 <__gmpn_neg@@Base>:
   27398:	stp	x29, x30, [sp, #-16]!
   2739c:	ldr	x8, [x1]
   273a0:	mov	x29, sp
   273a4:	cbz	x8, 273dc <__gmpn_neg@@Base+0x44>
   273a8:	neg	x8, x8
   273ac:	subs	x2, x2, #0x1
   273b0:	str	x8, [x0]
   273b4:	b.eq	273c4 <__gmpn_neg@@Base+0x2c>  // b.none
   273b8:	add	x0, x0, #0x8
   273bc:	add	x1, x1, #0x8
   273c0:	bl	c290 <__gmpn_com@plt>
   273c4:	mov	w0, #0x1                   	// #1
   273c8:	ldp	x29, x30, [sp], #16
   273cc:	ret
   273d0:	ldr	x8, [x1, #8]!
   273d4:	add	x0, x0, #0x8
   273d8:	cbnz	x8, 273a8 <__gmpn_neg@@Base+0x10>
   273dc:	subs	x2, x2, #0x1
   273e0:	str	xzr, [x0]
   273e4:	b.ne	273d0 <__gmpn_neg@@Base+0x38>  // b.any
   273e8:	mov	x0, xzr
   273ec:	ldp	x29, x30, [sp], #16
   273f0:	ret
   273f4:	nop
   273f8:	nop
   273fc:	nop

0000000000027400 <__gmpn_com@@Base>:
   27400:	cmp	x2, #0x3
   27404:	b.le	27458 <__gmpn_com@@Base+0x58>
   27408:	tbz	w0, #3, 2741c <__gmpn_com@@Base+0x1c>
   2740c:	ld1	{v22.1d}, [x1], #8
   27410:	sub	x2, x2, #0x1
   27414:	mvn	v22.8b, v22.8b
   27418:	st1	{v22.1d}, [x0], #8
   2741c:	ld1	{v26.2d}, [x1], #16
   27420:	subs	x2, x2, #0x6
   27424:	b.lt	27450 <__gmpn_com@@Base+0x50>  // b.tstop
   27428:	nop
   2742c:	nop
   27430:	ld1	{v22.2d}, [x1], #16
   27434:	mvn	v26.16b, v26.16b
   27438:	st1	{v26.2d}, [x0], #16
   2743c:	ld1	{v26.2d}, [x1], #16
   27440:	mvn	v22.16b, v22.16b
   27444:	st1	{v22.2d}, [x0], #16
   27448:	subs	x2, x2, #0x4
   2744c:	b.ge	27430 <__gmpn_com@@Base+0x30>  // b.tcont
   27450:	mvn	v26.16b, v26.16b
   27454:	st1	{v26.2d}, [x0], #16
   27458:	tbz	w2, #1, 27468 <__gmpn_com@@Base+0x68>
   2745c:	ld1	{v22.2d}, [x1], #16
   27460:	mvn	v22.16b, v22.16b
   27464:	st1	{v22.2d}, [x0], #16
   27468:	tbz	w2, #0, 27478 <__gmpn_com@@Base+0x78>
   2746c:	ld1	{v22.1d}, [x1]
   27470:	mvn	v22.8b, v22.8b
   27474:	st1	{v22.1d}, [x0]
   27478:	ret
   2747c:	nop

0000000000027480 <__gmpn_mul_1c@@Base>:
   27480:	cmn	xzr, xzr
   27484:	b	2748c <__gmpn_mul_1@@Base+0x4>

0000000000027488 <__gmpn_mul_1@@Base>:
   27488:	adds	x4, xzr, xzr
   2748c:	lsr	x18, x2, #2
   27490:	tbnz	w2, #0, 274c0 <__gmpn_mul_1@@Base+0x38>
   27494:	mov	x11, x4
   27498:	tbz	w2, #1, 274dc <__gmpn_mul_1@@Base+0x54>
   2749c:	ldp	x4, x5, [x1]
   274a0:	mul	x8, x4, x3
   274a4:	umulh	x10, x4, x3
   274a8:	cbz	x18, 274b8 <__gmpn_mul_1@@Base+0x30>
   274ac:	ldp	x6, x7, [x1, #16]!
   274b0:	mul	x9, x5, x3
   274b4:	b	27528 <__gmpn_mul_1@@Base+0xa0>
   274b8:	mul	x9, x5, x3
   274bc:	b	2756c <__gmpn_mul_1@@Base+0xe4>
   274c0:	ldr	x7, [x1], #8
   274c4:	mul	x9, x7, x3
   274c8:	umulh	x11, x7, x3
   274cc:	adds	x9, x9, x4
   274d0:	str	x9, [x0], #8
   274d4:	tbnz	w2, #1, 2749c <__gmpn_mul_1@@Base+0x14>
   274d8:	cbz	x18, 2757c <__gmpn_mul_1@@Base+0xf4>
   274dc:	ldp	x6, x7, [x1]
   274e0:	mul	x8, x6, x3
   274e4:	umulh	x10, x6, x3
   274e8:	ldp	x4, x5, [x1, #16]
   274ec:	mul	x9, x7, x3
   274f0:	adcs	x12, x8, x11
   274f4:	umulh	x11, x7, x3
   274f8:	add	x0, x0, #0x10
   274fc:	sub	x18, x18, #0x1
   27500:	cbz	x18, 27558 <__gmpn_mul_1@@Base+0xd0>
   27504:	nop
   27508:	nop
   2750c:	nop
   27510:	mul	x8, x4, x3
   27514:	ldp	x6, x7, [x1, #32]!
   27518:	adcs	x13, x9, x10
   2751c:	umulh	x10, x4, x3
   27520:	mul	x9, x5, x3
   27524:	stp	x12, x13, [x0, #-16]
   27528:	adcs	x12, x8, x11
   2752c:	umulh	x11, x5, x3
   27530:	mul	x8, x6, x3
   27534:	ldp	x4, x5, [x1, #16]
   27538:	adcs	x13, x9, x10
   2753c:	umulh	x10, x6, x3
   27540:	mul	x9, x7, x3
   27544:	stp	x12, x13, [x0], #32
   27548:	adcs	x12, x8, x11
   2754c:	umulh	x11, x7, x3
   27550:	sub	x18, x18, #0x1
   27554:	cbnz	x18, 27510 <__gmpn_mul_1@@Base+0x88>
   27558:	mul	x8, x4, x3
   2755c:	adcs	x13, x9, x10
   27560:	umulh	x10, x4, x3
   27564:	mul	x9, x5, x3
   27568:	stp	x12, x13, [x0, #-16]
   2756c:	adcs	x12, x8, x11
   27570:	umulh	x11, x5, x3
   27574:	adcs	x13, x9, x10
   27578:	stp	x12, x13, [x0]
   2757c:	adc	x0, x11, xzr
   27580:	ret
   27584:	nop
   27588:	nop
   2758c:	nop

0000000000027590 <__gmpn_addmul_1@@Base>:
   27590:	adds	x15, xzr, xzr
   27594:	tbz	w2, #0, 275b4 <__gmpn_addmul_1@@Base+0x24>
   27598:	ldr	x4, [x1], #8
   2759c:	mul	x8, x4, x3
   275a0:	umulh	x12, x4, x3
   275a4:	ldr	x4, [x0]
   275a8:	adds	x8, x4, x8
   275ac:	cinc	x15, x12, cs  // cs = hs, nlast
   275b0:	str	x8, [x0], #8
   275b4:	tbz	w2, #1, 275ec <__gmpn_addmul_1@@Base+0x5c>
   275b8:	ldp	x4, x5, [x1], #16
   275bc:	mul	x8, x4, x3
   275c0:	umulh	x12, x4, x3
   275c4:	mul	x9, x5, x3
   275c8:	umulh	x13, x5, x3
   275cc:	adds	x8, x8, x15
   275d0:	adcs	x9, x9, x12
   275d4:	ldp	x4, x5, [x0]
   275d8:	adc	x15, x13, xzr
   275dc:	adds	x8, x4, x8
   275e0:	adcs	x9, x5, x9
   275e4:	cinc	x15, x15, cs  // cs = hs, nlast
   275e8:	stp	x8, x9, [x0], #16
   275ec:	lsr	x2, x2, #2
   275f0:	cbz	x2, 27600 <__gmpn_addmul_1@@Base+0x70>
   275f4:	ldp	x4, x5, [x1], #32
   275f8:	ldp	x6, x7, [x1, #-16]
   275fc:	b	27634 <__gmpn_addmul_1@@Base+0xa4>
   27600:	mov	x0, x15
   27604:	ret
   27608:	nop
   2760c:	nop
   27610:	ldp	x4, x5, [x1], #32
   27614:	ldp	x6, x7, [x1, #-16]
   27618:	adds	x8, x16, x8
   2761c:	adcs	x9, x17, x9
   27620:	stp	x8, x9, [x0], #32
   27624:	adcs	x10, x12, x10
   27628:	adcs	x11, x13, x11
   2762c:	stp	x10, x11, [x0, #-16]
   27630:	cinc	x15, x15, cs  // cs = hs, nlast
   27634:	sub	x2, x2, #0x1
   27638:	mul	x8, x4, x3
   2763c:	umulh	x12, x4, x3
   27640:	mul	x9, x5, x3
   27644:	umulh	x13, x5, x3
   27648:	adds	x8, x8, x15
   2764c:	mul	x10, x6, x3
   27650:	umulh	x14, x6, x3
   27654:	adcs	x9, x9, x12
   27658:	mul	x11, x7, x3
   2765c:	umulh	x15, x7, x3
   27660:	adcs	x10, x10, x13
   27664:	ldp	x16, x17, [x0]
   27668:	adcs	x11, x11, x14
   2766c:	ldp	x12, x13, [x0, #16]
   27670:	adc	x15, x15, xzr
   27674:	cbnz	x2, 27610 <__gmpn_addmul_1@@Base+0x80>
   27678:	adds	x8, x16, x8
   2767c:	adcs	x9, x17, x9
   27680:	adcs	x10, x12, x10
   27684:	adcs	x11, x13, x11
   27688:	stp	x8, x9, [x0]
   2768c:	stp	x10, x11, [x0, #16]
   27690:	cinc	x0, x15, cs  // cs = hs, nlast
   27694:	ret
   27698:	nop
   2769c:	nop

00000000000276a0 <__gmpn_submul_1@@Base>:
   276a0:	adds	x15, xzr, xzr
   276a4:	tbz	w2, #0, 276c4 <__gmpn_submul_1@@Base+0x24>
   276a8:	ldr	x4, [x1], #8
   276ac:	mul	x8, x4, x3
   276b0:	umulh	x12, x4, x3
   276b4:	ldr	x4, [x0]
   276b8:	subs	x8, x4, x8
   276bc:	cinc	x15, x12, cc  // cc = lo, ul, last
   276c0:	str	x8, [x0], #8
   276c4:	tbz	w2, #1, 276fc <__gmpn_submul_1@@Base+0x5c>
   276c8:	ldp	x4, x5, [x1], #16
   276cc:	mul	x8, x4, x3
   276d0:	umulh	x12, x4, x3
   276d4:	mul	x9, x5, x3
   276d8:	umulh	x13, x5, x3
   276dc:	adds	x8, x8, x15
   276e0:	adcs	x9, x9, x12
   276e4:	ldp	x4, x5, [x0]
   276e8:	adc	x15, x13, xzr
   276ec:	subs	x8, x4, x8
   276f0:	sbcs	x9, x5, x9
   276f4:	cinc	x15, x15, cc  // cc = lo, ul, last
   276f8:	stp	x8, x9, [x0], #16
   276fc:	lsr	x2, x2, #2
   27700:	cbz	x2, 27710 <__gmpn_submul_1@@Base+0x70>
   27704:	ldp	x4, x5, [x1], #32
   27708:	ldp	x6, x7, [x1, #-16]
   2770c:	b	27744 <__gmpn_submul_1@@Base+0xa4>
   27710:	mov	x0, x15
   27714:	ret
   27718:	nop
   2771c:	nop
   27720:	ldp	x4, x5, [x1], #32
   27724:	ldp	x6, x7, [x1, #-16]
   27728:	subs	x8, x16, x8
   2772c:	sbcs	x9, x17, x9
   27730:	stp	x8, x9, [x0], #32
   27734:	sbcs	x10, x12, x10
   27738:	sbcs	x11, x13, x11
   2773c:	stp	x10, x11, [x0, #-16]
   27740:	cinc	x15, x15, cc  // cc = lo, ul, last
   27744:	sub	x2, x2, #0x1
   27748:	mul	x8, x4, x3
   2774c:	umulh	x12, x4, x3
   27750:	mul	x9, x5, x3
   27754:	umulh	x13, x5, x3
   27758:	adds	x8, x8, x15
   2775c:	mul	x10, x6, x3
   27760:	umulh	x14, x6, x3
   27764:	adcs	x9, x9, x12
   27768:	mul	x11, x7, x3
   2776c:	umulh	x15, x7, x3
   27770:	adcs	x10, x10, x13
   27774:	ldp	x16, x17, [x0]
   27778:	adcs	x11, x11, x14
   2777c:	ldp	x12, x13, [x0, #16]
   27780:	adc	x15, x15, xzr
   27784:	cbnz	x2, 27720 <__gmpn_submul_1@@Base+0x80>
   27788:	subs	x8, x16, x8
   2778c:	sbcs	x9, x17, x9
   27790:	sbcs	x10, x12, x10
   27794:	sbcs	x11, x13, x11
   27798:	stp	x8, x9, [x0]
   2779c:	stp	x10, x11, [x0, #16]
   277a0:	cinc	x0, x15, cc  // cc = lo, ul, last
   277a4:	ret

00000000000277a8 <__gmpn_add_err1_n@@Base>:
   277a8:	mov	x8, xzr
   277ac:	mov	x9, xzr
   277b0:	sub	x10, x4, #0x8
   277b4:	ldr	x11, [x10, x5, lsl #3]
   277b8:	ldr	x12, [x1], #8
   277bc:	ldr	x13, [x2], #8
   277c0:	adds	x12, x13, x12
   277c4:	cset	w13, cs  // cs = hs, nlast
   277c8:	adds	x12, x12, x6
   277cc:	cset	w14, cs  // cs = hs, nlast
   277d0:	orr	w6, w13, w14
   277d4:	cmp	w6, #0x0
   277d8:	csel	x11, x11, xzr, ne  // ne = any
   277dc:	adds	x9, x11, x9
   277e0:	cinc	x8, x8, cs  // cs = hs, nlast
   277e4:	subs	x5, x5, #0x1
   277e8:	str	x12, [x0], #8
   277ec:	b.ne	277b4 <__gmpn_add_err1_n@@Base+0xc>  // b.any
   277f0:	mov	x0, x6
   277f4:	stp	x9, x8, [x3]
   277f8:	ret

00000000000277fc <__gmpn_add_err2_n@@Base>:
   277fc:	mov	x8, xzr
   27800:	mov	x9, xzr
   27804:	mov	x10, xzr
   27808:	mov	x11, xzr
   2780c:	sub	x12, x5, #0x8
   27810:	sub	x13, x4, #0x8
   27814:	lsl	x14, x6, #3
   27818:	ldr	x15, [x13, x14]
   2781c:	ldr	x14, [x12, x14]
   27820:	ldr	x16, [x1], #8
   27824:	ldr	x17, [x2], #8
   27828:	adds	x16, x17, x16
   2782c:	cset	w17, cs  // cs = hs, nlast
   27830:	adds	x16, x16, x7
   27834:	cset	w18, cs  // cs = hs, nlast
   27838:	orr	w7, w17, w18
   2783c:	sbfx	x17, x7, #0, #1
   27840:	and	x15, x15, x17
   27844:	and	x14, x14, x17
   27848:	adds	x11, x15, x11
   2784c:	cinc	x10, x10, cs  // cs = hs, nlast
   27850:	adds	x9, x14, x9
   27854:	cinc	x8, x8, cs  // cs = hs, nlast
   27858:	subs	x6, x6, #0x1
   2785c:	str	x16, [x0], #8
   27860:	b.ne	27814 <__gmpn_add_err2_n@@Base+0x18>  // b.any
   27864:	mov	x0, x7
   27868:	stp	x11, x10, [x3]
   2786c:	stp	x9, x8, [x3, #16]
   27870:	ret

0000000000027874 <__gmpn_add_err3_n@@Base>:
   27874:	str	x21, [sp, #-32]!
   27878:	mov	x8, x0
   2787c:	ldr	x0, [sp, #32]
   27880:	lsl	x16, x7, #3
   27884:	sub	x18, x16, #0x8
   27888:	mov	x14, xzr
   2788c:	mov	x9, xzr
   27890:	mov	x10, xzr
   27894:	mov	x11, xzr
   27898:	mov	x12, xzr
   2789c:	mov	x13, xzr
   278a0:	mov	x15, xzr
   278a4:	add	x16, x4, x18
   278a8:	add	x17, x5, x18
   278ac:	add	x18, x6, x18
   278b0:	stp	x20, x19, [sp, #16]
   278b4:	lsl	x5, x14, #3
   278b8:	ldr	x4, [x16], #-8
   278bc:	ldr	x6, [x17], #-8
   278c0:	ldr	x19, [x18], #-8
   278c4:	ldr	x20, [x1, x5]
   278c8:	ldr	x21, [x2, x5]
   278cc:	add	x14, x14, #0x1
   278d0:	adds	x20, x21, x20
   278d4:	cset	w21, cs  // cs = hs, nlast
   278d8:	adds	x20, x20, x0
   278dc:	cset	w0, cs  // cs = hs, nlast
   278e0:	orr	w0, w21, w0
   278e4:	sbfx	x21, x0, #0, #1
   278e8:	and	x4, x4, x21
   278ec:	and	x6, x6, x21
   278f0:	adds	x15, x4, x15
   278f4:	and	x19, x19, x21
   278f8:	cinc	x13, x13, cs  // cs = hs, nlast
   278fc:	adds	x12, x6, x12
   27900:	cinc	x11, x11, cs  // cs = hs, nlast
   27904:	adds	x10, x19, x10
   27908:	cinc	x9, x9, cs  // cs = hs, nlast
   2790c:	cmp	x7, x14
   27910:	str	x20, [x8, x5]
   27914:	b.ne	278b4 <__gmpn_add_err3_n@@Base+0x40>  // b.any
   27918:	stp	x15, x13, [x3]
   2791c:	stp	x12, x11, [x3, #16]
   27920:	stp	x10, x9, [x3, #32]
   27924:	ldp	x20, x19, [sp, #16]
   27928:	ldr	x21, [sp], #32
   2792c:	ret

0000000000027930 <__gmpn_sub_err1_n@@Base>:
   27930:	mov	x8, xzr
   27934:	mov	x9, xzr
   27938:	sub	x10, x4, #0x8
   2793c:	ldr	x11, [x10, x5, lsl #3]
   27940:	ldr	x12, [x1], #8
   27944:	ldr	x13, [x2], #8
   27948:	subs	x12, x12, x13
   2794c:	cset	w13, cc  // cc = lo, ul, last
   27950:	subs	x12, x12, x6
   27954:	cset	w14, cc  // cc = lo, ul, last
   27958:	orr	w6, w13, w14
   2795c:	cmp	w6, #0x0
   27960:	csel	x11, x11, xzr, ne  // ne = any
   27964:	adds	x9, x11, x9
   27968:	cinc	x8, x8, cs  // cs = hs, nlast
   2796c:	subs	x5, x5, #0x1
   27970:	str	x12, [x0], #8
   27974:	b.ne	2793c <__gmpn_sub_err1_n@@Base+0xc>  // b.any
   27978:	mov	x0, x6
   2797c:	stp	x9, x8, [x3]
   27980:	ret

0000000000027984 <__gmpn_sub_err2_n@@Base>:
   27984:	mov	x8, xzr
   27988:	mov	x9, xzr
   2798c:	mov	x10, xzr
   27990:	mov	x11, xzr
   27994:	sub	x12, x5, #0x8
   27998:	sub	x13, x4, #0x8
   2799c:	lsl	x14, x6, #3
   279a0:	ldr	x15, [x13, x14]
   279a4:	ldr	x14, [x12, x14]
   279a8:	ldr	x16, [x1], #8
   279ac:	ldr	x17, [x2], #8
   279b0:	subs	x16, x16, x17
   279b4:	cset	w17, cc  // cc = lo, ul, last
   279b8:	subs	x16, x16, x7
   279bc:	cset	w18, cc  // cc = lo, ul, last
   279c0:	orr	w7, w17, w18
   279c4:	sbfx	x17, x7, #0, #1
   279c8:	and	x15, x15, x17
   279cc:	and	x14, x14, x17
   279d0:	adds	x11, x15, x11
   279d4:	cinc	x10, x10, cs  // cs = hs, nlast
   279d8:	adds	x9, x14, x9
   279dc:	cinc	x8, x8, cs  // cs = hs, nlast
   279e0:	subs	x6, x6, #0x1
   279e4:	str	x16, [x0], #8
   279e8:	b.ne	2799c <__gmpn_sub_err2_n@@Base+0x18>  // b.any
   279ec:	mov	x0, x7
   279f0:	stp	x11, x10, [x3]
   279f4:	stp	x9, x8, [x3, #16]
   279f8:	ret

00000000000279fc <__gmpn_sub_err3_n@@Base>:
   279fc:	str	x21, [sp, #-32]!
   27a00:	mov	x8, x0
   27a04:	ldr	x0, [sp, #32]
   27a08:	lsl	x16, x7, #3
   27a0c:	sub	x18, x16, #0x8
   27a10:	mov	x14, xzr
   27a14:	mov	x9, xzr
   27a18:	mov	x10, xzr
   27a1c:	mov	x11, xzr
   27a20:	mov	x12, xzr
   27a24:	mov	x13, xzr
   27a28:	mov	x15, xzr
   27a2c:	add	x16, x4, x18
   27a30:	add	x17, x5, x18
   27a34:	add	x18, x6, x18
   27a38:	stp	x20, x19, [sp, #16]
   27a3c:	lsl	x5, x14, #3
   27a40:	ldr	x4, [x16], #-8
   27a44:	ldr	x6, [x17], #-8
   27a48:	ldr	x19, [x18], #-8
   27a4c:	ldr	x20, [x1, x5]
   27a50:	ldr	x21, [x2, x5]
   27a54:	add	x14, x14, #0x1
   27a58:	subs	x20, x20, x21
   27a5c:	cset	w21, cc  // cc = lo, ul, last
   27a60:	subs	x20, x20, x0
   27a64:	cset	w0, cc  // cc = lo, ul, last
   27a68:	orr	w0, w21, w0
   27a6c:	sbfx	x21, x0, #0, #1
   27a70:	and	x4, x4, x21
   27a74:	and	x6, x6, x21
   27a78:	adds	x15, x4, x15
   27a7c:	and	x19, x19, x21
   27a80:	cinc	x13, x13, cs  // cs = hs, nlast
   27a84:	adds	x12, x6, x12
   27a88:	cinc	x11, x11, cs  // cs = hs, nlast
   27a8c:	adds	x10, x19, x10
   27a90:	cinc	x9, x9, cs  // cs = hs, nlast
   27a94:	cmp	x7, x14
   27a98:	str	x20, [x8, x5]
   27a9c:	b.ne	27a3c <__gmpn_sub_err3_n@@Base+0x40>  // b.any
   27aa0:	stp	x15, x13, [x3]
   27aa4:	stp	x12, x11, [x3, #16]
   27aa8:	stp	x10, x9, [x3, #32]
   27aac:	ldp	x20, x19, [sp, #16]
   27ab0:	ldr	x21, [sp], #32
   27ab4:	ret
   27ab8:	nop
   27abc:	nop

0000000000027ac0 <__gmpn_lshift@@Base>:
   27ac0:	add	x16, x0, x2, lsl #3
   27ac4:	add	x1, x1, x2, lsl #3
   27ac8:	neg	x8, x3
   27acc:	lsr	x18, x2, #2
   27ad0:	tbz	w2, #0, 27b10 <__gmpn_lshift@@Base+0x50>
   27ad4:	ldur	x4, [x1, #-8]
   27ad8:	tbnz	w2, #1, 27b00 <__gmpn_lshift@@Base+0x40>
   27adc:	lsr	x0, x4, x8
   27ae0:	lsl	x2, x4, x3
   27ae4:	cbnz	x18, 27af0 <__gmpn_lshift@@Base+0x30>
   27ae8:	stur	x2, [x16, #-8]
   27aec:	ret
   27af0:	ldp	x4, x5, [x1, #-24]
   27af4:	sub	x1, x1, #0x8
   27af8:	add	x16, x16, #0x10
   27afc:	b	27b84 <__gmpn_lshift@@Base+0xc4>
   27b00:	lsr	x0, x4, x8
   27b04:	lsl	x2, x4, x3
   27b08:	ldp	x6, x7, [x1, #-24]!
   27b0c:	b	27ba8 <__gmpn_lshift@@Base+0xe8>
   27b10:	ldp	x4, x5, [x1, #-16]
   27b14:	tbz	w2, #1, 27b50 <__gmpn_lshift@@Base+0x90>
   27b18:	lsr	x0, x5, x8
   27b1c:	lsl	x13, x5, x3
   27b20:	lsr	x10, x4, x8
   27b24:	lsl	x2, x4, x3
   27b28:	cbnz	x18, 27b38 <__gmpn_lshift@@Base+0x78>
   27b2c:	orr	x10, x10, x13
   27b30:	stp	x2, x10, [x16, #-16]
   27b34:	ret
   27b38:	ldp	x4, x5, [x1, #-32]
   27b3c:	orr	x10, x10, x13
   27b40:	stur	x10, [x16, #-8]
   27b44:	sub	x1, x1, #0x10
   27b48:	add	x16, x16, #0x8
   27b4c:	b	27b84 <__gmpn_lshift@@Base+0xc4>
   27b50:	lsr	x0, x5, x8
   27b54:	lsl	x13, x5, x3
   27b58:	lsr	x10, x4, x8
   27b5c:	lsl	x2, x4, x3
   27b60:	ldp	x6, x7, [x1, #-32]!
   27b64:	orr	x10, x10, x13
   27b68:	str	x10, [x16, #-8]!
   27b6c:	b	27ba4 <__gmpn_lshift@@Base+0xe4>
   27b70:	ldp	x4, x5, [x1, #-16]
   27b74:	orr	x10, x10, x13
   27b78:	orr	x11, x12, x2
   27b7c:	stp	x10, x11, [x16, #-16]
   27b80:	lsl	x2, x6, x3
   27b84:	lsr	x10, x4, x8
   27b88:	lsl	x13, x5, x3
   27b8c:	lsr	x12, x5, x8
   27b90:	ldp	x6, x7, [x1, #-32]!
   27b94:	orr	x10, x10, x13
   27b98:	orr	x11, x12, x2
   27b9c:	stp	x10, x11, [x16, #-32]!
   27ba0:	lsl	x2, x4, x3
   27ba4:	sub	x18, x18, #0x1
   27ba8:	lsr	x10, x6, x8
   27bac:	lsl	x13, x7, x3
   27bb0:	lsr	x12, x7, x8
   27bb4:	cbnz	x18, 27b70 <__gmpn_lshift@@Base+0xb0>
   27bb8:	orr	x10, x10, x13
   27bbc:	orr	x11, x12, x2
   27bc0:	lsl	x2, x6, x3
   27bc4:	stp	x10, x11, [x16, #-16]
   27bc8:	stur	x2, [x16, #-24]
   27bcc:	ret

0000000000027bd0 <__gmpn_rshift@@Base>:
   27bd0:	mov	x16, x0
   27bd4:	neg	x8, x3
   27bd8:	lsr	x18, x2, #2
   27bdc:	tbz	w2, #0, 27c20 <__gmpn_rshift@@Base+0x50>
   27be0:	ldr	x5, [x1]
   27be4:	tbnz	w2, #1, 27c0c <__gmpn_rshift@@Base+0x3c>
   27be8:	lsl	x0, x5, x8
   27bec:	lsr	x2, x5, x3
   27bf0:	cbnz	x18, 27bfc <__gmpn_rshift@@Base+0x2c>
   27bf4:	str	x2, [x16]
   27bf8:	ret
   27bfc:	ldp	x4, x5, [x1, #8]
   27c00:	sub	x1, x1, #0x8
   27c04:	sub	x16, x16, #0x20
   27c08:	b	27c94 <__gmpn_rshift@@Base+0xc4>
   27c0c:	lsl	x0, x5, x8
   27c10:	lsr	x2, x5, x3
   27c14:	ldp	x6, x7, [x1, #8]!
   27c18:	sub	x16, x16, #0x10
   27c1c:	b	27cb8 <__gmpn_rshift@@Base+0xe8>
   27c20:	ldp	x4, x5, [x1]
   27c24:	tbz	w2, #1, 27c58 <__gmpn_rshift@@Base+0x88>
   27c28:	lsl	x0, x4, x8
   27c2c:	lsr	x13, x4, x3
   27c30:	lsl	x10, x5, x8
   27c34:	lsr	x2, x5, x3
   27c38:	cbnz	x18, 27c48 <__gmpn_rshift@@Base+0x78>
   27c3c:	orr	x10, x10, x13
   27c40:	stp	x10, x2, [x16]
   27c44:	ret
   27c48:	ldp	x4, x5, [x1, #16]
   27c4c:	orr	x10, x10, x13
   27c50:	str	x10, [x16], #-24
   27c54:	b	27c94 <__gmpn_rshift@@Base+0xc4>
   27c58:	lsl	x0, x4, x8
   27c5c:	lsr	x13, x4, x3
   27c60:	lsl	x10, x5, x8
   27c64:	lsr	x2, x5, x3
   27c68:	ldp	x6, x7, [x1, #16]!
   27c6c:	orr	x10, x10, x13
   27c70:	str	x10, [x16], #-8
   27c74:	b	27cb4 <__gmpn_rshift@@Base+0xe4>
   27c78:	nop
   27c7c:	nop
   27c80:	ldp	x4, x5, [x1, #16]
   27c84:	orr	x10, x10, x13
   27c88:	orr	x11, x12, x2
   27c8c:	stp	x11, x10, [x16, #16]
   27c90:	lsr	x2, x7, x3
   27c94:	lsl	x10, x5, x8
   27c98:	lsl	x12, x4, x8
   27c9c:	lsr	x13, x4, x3
   27ca0:	ldp	x6, x7, [x1, #32]!
   27ca4:	orr	x10, x10, x13
   27ca8:	orr	x11, x12, x2
   27cac:	stp	x11, x10, [x16, #32]!
   27cb0:	lsr	x2, x5, x3
   27cb4:	sub	x18, x18, #0x1
   27cb8:	lsl	x10, x7, x8
   27cbc:	lsl	x12, x6, x8
   27cc0:	lsr	x13, x6, x3
   27cc4:	cbnz	x18, 27c80 <__gmpn_rshift@@Base+0xb0>
   27cc8:	orr	x10, x10, x13
   27ccc:	orr	x11, x12, x2
   27cd0:	lsr	x2, x7, x3
   27cd4:	stp	x11, x10, [x16, #16]
   27cd8:	str	x2, [x16, #32]
   27cdc:	ret

0000000000027ce0 <__gmpn_divexact_1@@Base>:
   27ce0:	rbit	x8, x3
   27ce4:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   27ce8:	tst	x3, #0x1
   27cec:	ldr	x9, [x9, #3952]
   27cf0:	clz	x10, x8
   27cf4:	csel	x8, x10, xzr, eq  // eq = none
   27cf8:	lsr	x8, x3, x8
   27cfc:	ubfx	x11, x8, #1, #7
   27d00:	ldrb	w9, [x9, x11]
   27d04:	mov	w12, #0x2                   	// #2
   27d08:	msub	x11, x8, x9, x12
   27d0c:	mul	x9, x11, x9
   27d10:	msub	x13, x9, x8, x12
   27d14:	ldr	x11, [x1]
   27d18:	mul	x9, x9, x13
   27d1c:	msub	x12, x9, x8, x12
   27d20:	mul	x9, x9, x12
   27d24:	cbz	x10, 27d84 <__gmpn_divexact_1@@Base+0xa4>
   27d28:	tbnz	w3, #0, 27d84 <__gmpn_divexact_1@@Base+0xa4>
   27d2c:	cmp	x2, #0x2
   27d30:	b.lt	27dcc <__gmpn_divexact_1@@Base+0xec>  // b.tstop
   27d34:	mov	w14, #0x40                  	// #64
   27d38:	mov	x12, xzr
   27d3c:	sub	x13, x2, #0x1
   27d40:	sub	x14, x14, x10
   27d44:	add	x15, x1, #0x8
   27d48:	mov	x16, x0
   27d4c:	mov	x17, x11
   27d50:	ldr	x11, [x15], #8
   27d54:	lsr	x17, x17, x10
   27d58:	lsl	x18, x11, x14
   27d5c:	orr	x17, x18, x17
   27d60:	subs	x12, x17, x12
   27d64:	mul	x17, x12, x9
   27d68:	umulh	x12, x17, x8
   27d6c:	cinc	x12, x12, cc  // cc = lo, ul, last
   27d70:	subs	x13, x13, #0x1
   27d74:	str	x17, [x16], #8
   27d78:	mov	x17, x11
   27d7c:	b.ne	27d50 <__gmpn_divexact_1@@Base+0x70>  // b.any
   27d80:	b	27dd0 <__gmpn_divexact_1@@Base+0xf0>
   27d84:	mul	x10, x9, x11
   27d88:	cmp	x2, #0x2
   27d8c:	str	x10, [x0]
   27d90:	b.lt	27dc8 <__gmpn_divexact_1@@Base+0xe8>  // b.tstop
   27d94:	mov	x12, xzr
   27d98:	sub	x11, x2, #0x1
   27d9c:	add	x13, x0, #0x8
   27da0:	add	x14, x1, #0x8
   27da4:	ldr	x15, [x14], #8
   27da8:	umulh	x10, x10, x8
   27dac:	add	x10, x10, x12
   27db0:	subs	x10, x15, x10
   27db4:	mul	x10, x10, x9
   27db8:	cset	w12, cc  // cc = lo, ul, last
   27dbc:	subs	x11, x11, #0x1
   27dc0:	str	x10, [x13], #8
   27dc4:	b.ne	27da4 <__gmpn_divexact_1@@Base+0xc4>  // b.any
   27dc8:	ret
   27dcc:	mov	x12, xzr
   27dd0:	lsr	x8, x11, x10
   27dd4:	sub	x8, x8, x12
   27dd8:	mul	x8, x8, x9
   27ddc:	add	x9, x0, x2, lsl #3
   27de0:	stur	x8, [x9, #-8]
   27de4:	ret

0000000000027de8 <__gmpn_divexact_by3c@@Base>:
   27de8:	stp	x29, x30, [sp, #-16]!
   27dec:	mov	x8, #0x5555555555555555    	// #6148914691236517205
   27df0:	mul	x4, x3, x8
   27df4:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   27df8:	mov	x29, sp
   27dfc:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   27e00:	and	x0, x0, #0x3
   27e04:	ldp	x29, x30, [sp], #16
   27e08:	ret

0000000000027e0c <__gmpn_divisible_p@@Base>:
   27e0c:	stp	x29, x30, [sp, #-80]!
   27e10:	stp	x26, x25, [sp, #16]
   27e14:	stp	x24, x23, [sp, #32]
   27e18:	stp	x22, x21, [sp, #48]
   27e1c:	stp	x20, x19, [sp, #64]
   27e20:	mov	x29, sp
   27e24:	sub	sp, sp, #0x10
   27e28:	mov	x20, x1
   27e2c:	cmp	x1, x3
   27e30:	b.ge	27e40 <__gmpn_divisible_p@@Base+0x34>  // b.tcont
   27e34:	cmp	x20, #0x0
   27e38:	cset	w19, eq  // eq = none
   27e3c:	b	27ea4 <__gmpn_divisible_p@@Base+0x98>
   27e40:	mov	x21, x2
   27e44:	ldr	x2, [x2]
   27e48:	ldr	x8, [x0]
   27e4c:	mov	x19, x3
   27e50:	mov	x23, x0
   27e54:	cbz	x2, 27e9c <__gmpn_divisible_p@@Base+0x90>
   27e58:	neg	x9, x2
   27e5c:	and	x9, x2, x9
   27e60:	sub	x9, x9, #0x1
   27e64:	tst	x9, x8
   27e68:	b.ne	27ea0 <__gmpn_divisible_p@@Base+0x94>  // b.any
   27e6c:	cmp	x19, #0x1
   27e70:	b.ne	27ec4 <__gmpn_divisible_p@@Base+0xb8>  // b.any
   27e74:	cmp	x20, #0x28
   27e78:	b.lt	27f70 <__gmpn_divisible_p@@Base+0x164>  // b.tstop
   27e7c:	mov	x0, x23
   27e80:	mov	x1, x20
   27e84:	b	27fa8 <__gmpn_divisible_p@@Base+0x19c>
   27e88:	ldr	x8, [x23, #8]!
   27e8c:	ldr	x2, [x21, #8]!
   27e90:	sub	x20, x20, #0x1
   27e94:	sub	x19, x19, #0x1
   27e98:	cbnz	x2, 27e58 <__gmpn_divisible_p@@Base+0x4c>
   27e9c:	cbz	x8, 27e88 <__gmpn_divisible_p@@Base+0x7c>
   27ea0:	mov	w19, wzr
   27ea4:	mov	w0, w19
   27ea8:	mov	sp, x29
   27eac:	ldp	x20, x19, [sp, #64]
   27eb0:	ldp	x22, x21, [sp, #48]
   27eb4:	ldp	x24, x23, [sp, #32]
   27eb8:	ldp	x26, x25, [sp, #16]
   27ebc:	ldp	x29, x30, [sp], #80
   27ec0:	ret
   27ec4:	rbit	x8, x2
   27ec8:	cmp	x19, #0x2
   27ecc:	clz	x24, x8
   27ed0:	b.ne	27ee0 <__gmpn_divisible_p@@Base+0xd4>  // b.any
   27ed4:	ldr	x8, [x21, #8]
   27ed8:	cmp	x8, x9
   27edc:	b.ls	27f88 <__gmpn_divisible_p@@Base+0x17c>  // b.plast
   27ee0:	add	x26, x20, #0x1
   27ee4:	sub	x8, x20, x19
   27ee8:	add	x8, x8, x26
   27eec:	lsl	x8, x8, #3
   27ef0:	add	x1, x8, #0x8
   27ef4:	mov	w8, #0x7f00                	// #32512
   27ef8:	cmp	x1, x8
   27efc:	stur	xzr, [x29, #-8]
   27f00:	b.hi	27fc0 <__gmpn_divisible_p@@Base+0x1b4>  // b.pmore
   27f04:	add	x9, x1, #0xf
   27f08:	mov	x8, sp
   27f0c:	and	x9, x9, #0xfffffffffffffff0
   27f10:	sub	x22, x8, x9
   27f14:	mov	sp, x22
   27f18:	cbz	w24, 27fd0 <__gmpn_divisible_p@@Base+0x1c4>
   27f1c:	lsl	x1, x19, #3
   27f20:	mov	w8, #0x7f00                	// #32512
   27f24:	cmp	x1, x8
   27f28:	b.hi	28178 <__gmpn_divisible_p@@Base+0x36c>  // b.pmore
   27f2c:	add	x9, x1, #0xf
   27f30:	mov	x8, sp
   27f34:	and	x9, x9, #0xfffffffffffffff0
   27f38:	sub	x25, x8, x9
   27f3c:	mov	sp, x25
   27f40:	mov	x0, x25
   27f44:	mov	x1, x21
   27f48:	mov	x2, x19
   27f4c:	mov	w3, w24
   27f50:	bl	c1a0 <__gmpn_rshift@plt>
   27f54:	mov	x0, x22
   27f58:	mov	x1, x23
   27f5c:	mov	x2, x20
   27f60:	mov	w3, w24
   27f64:	bl	c1a0 <__gmpn_rshift@plt>
   27f68:	mov	x21, x25
   27f6c:	b	27fe0 <__gmpn_divisible_p@@Base+0x1d4>
   27f70:	rbit	x8, x2
   27f74:	clz	x8, x8
   27f78:	lsr	x2, x2, x8
   27f7c:	mov	x0, x23
   27f80:	mov	x1, x20
   27f84:	b	27fb0 <__gmpn_divisible_p@@Base+0x1a4>
   27f88:	neg	x10, x24
   27f8c:	lsr	x9, x2, x24
   27f90:	lsl	x8, x8, x10
   27f94:	cmp	x20, #0x27
   27f98:	orr	x2, x8, x9
   27f9c:	mov	x0, x23
   27fa0:	mov	x1, x20
   27fa4:	b.le	27fb0 <__gmpn_divisible_p@@Base+0x1a4>
   27fa8:	bl	c3e0 <__gmpn_mod_1@plt>
   27fac:	b	27fb8 <__gmpn_divisible_p@@Base+0x1ac>
   27fb0:	mov	x3, xzr
   27fb4:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   27fb8:	cmp	x0, #0x0
   27fbc:	b	27e38 <__gmpn_divisible_p@@Base+0x2c>
   27fc0:	sub	x0, x29, #0x8
   27fc4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27fc8:	mov	x22, x0
   27fcc:	cbnz	w24, 27f1c <__gmpn_divisible_p@@Base+0x110>
   27fd0:	mov	x0, x22
   27fd4:	mov	x1, x23
   27fd8:	mov	x2, x20
   27fdc:	bl	ca50 <__gmpn_copyi@plt>
   27fe0:	add	x8, x22, x20, lsl #3
   27fe4:	add	x9, x21, x19, lsl #3
   27fe8:	ldur	x8, [x8, #-8]
   27fec:	ldur	x9, [x9, #-8]
   27ff0:	cmp	x8, x9
   27ff4:	b.cs	28014 <__gmpn_divisible_p@@Base+0x208>  // b.hs, b.nlast
   27ff8:	cmp	x20, x19
   27ffc:	b.ne	2801c <__gmpn_divisible_p@@Base+0x210>  // b.any
   28000:	ldur	x0, [x29, #-8]
   28004:	cbz	x0, 27ea0 <__gmpn_divisible_p@@Base+0x94>
   28008:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2800c:	mov	w19, wzr
   28010:	b	27ea4 <__gmpn_divisible_p@@Base+0x98>
   28014:	str	xzr, [x22, x20, lsl #3]
   28018:	mov	x20, x26
   2801c:	add	x23, x22, x26, lsl #3
   28020:	cmp	x19, #0x27
   28024:	sub	x24, x20, x19
   28028:	b.lt	2808c <__gmpn_divisible_p@@Base+0x280>  // b.tstop
   2802c:	cmp	x24, #0x26
   28030:	b.le	2808c <__gmpn_divisible_p@@Base+0x280>
   28034:	cmp	x19, #0x326
   28038:	b.le	280dc <__gmpn_divisible_p@@Base+0x2d0>
   2803c:	mov	x0, x20
   28040:	mov	x1, x19
   28044:	bl	d170 <__gmpn_mu_bdiv_qr_itch@plt>
   28048:	lsl	x1, x0, #3
   2804c:	mov	w8, #0x7f00                	// #32512
   28050:	cmp	x1, x8
   28054:	b.hi	28188 <__gmpn_divisible_p@@Base+0x37c>  // b.pmore
   28058:	add	x9, x1, #0xf
   2805c:	mov	x8, sp
   28060:	and	x9, x9, #0xfffffffffffffff0
   28064:	sub	x6, x8, x9
   28068:	mov	sp, x6
   2806c:	mov	x0, x23
   28070:	mov	x1, x22
   28074:	mov	x2, x22
   28078:	mov	x3, x20
   2807c:	mov	x4, x21
   28080:	mov	x5, x19
   28084:	bl	ccb0 <__gmpn_mu_bdiv_qr@plt>
   28088:	b	2812c <__gmpn_divisible_p@@Base+0x320>
   2808c:	ldr	x8, [x21]
   28090:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   28094:	ldr	x9, [x9, #3952]
   28098:	mov	x0, x23
   2809c:	ubfx	x10, x8, #1, #7
   280a0:	mov	x1, x22
   280a4:	ldrb	w9, [x9, x10]
   280a8:	mov	w10, #0x2                   	// #2
   280ac:	mov	x2, x20
   280b0:	mov	x3, x21
   280b4:	msub	x11, x8, x9, x10
   280b8:	mul	x9, x11, x9
   280bc:	msub	x10, x9, x8, x10
   280c0:	mul	x9, x9, x10
   280c4:	orr	x10, xzr, #0xfffffffffffffffe
   280c8:	madd	x8, x9, x8, x10
   280cc:	mul	x5, x8, x9
   280d0:	mov	x4, x19
   280d4:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   280d8:	b	28128 <__gmpn_divisible_p@@Base+0x31c>
   280dc:	ldr	x8, [x21]
   280e0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   280e4:	ldr	x9, [x9, #3952]
   280e8:	mov	x0, x23
   280ec:	ubfx	x10, x8, #1, #7
   280f0:	mov	x1, x22
   280f4:	ldrb	w9, [x9, x10]
   280f8:	mov	w10, #0x2                   	// #2
   280fc:	mov	x2, x20
   28100:	mov	x3, x21
   28104:	msub	x11, x8, x9, x10
   28108:	mul	x9, x11, x9
   2810c:	msub	x10, x9, x8, x10
   28110:	mul	x9, x9, x10
   28114:	orr	x10, xzr, #0xfffffffffffffffe
   28118:	madd	x8, x9, x8, x10
   2811c:	mul	x5, x8, x9
   28120:	mov	x4, x19
   28124:	bl	c5e0 <__gmpn_dcpi1_bdiv_qr@plt>
   28128:	add	x22, x22, x24, lsl #3
   2812c:	sub	x8, x22, #0x8
   28130:	sub	x9, x21, #0x8
   28134:	subs	x10, x19, #0x1
   28138:	b.lt	28164 <__gmpn_divisible_p@@Base+0x358>  // b.tstop
   2813c:	lsl	x11, x19, #3
   28140:	ldr	x12, [x8, x11]
   28144:	ldr	x11, [x9, x11]
   28148:	mov	x19, x10
   2814c:	cmp	x12, x11
   28150:	b.eq	28134 <__gmpn_divisible_p@@Base+0x328>  // b.none
   28154:	mov	w19, wzr
   28158:	ldur	x0, [x29, #-8]
   2815c:	cbz	x0, 27ea4 <__gmpn_divisible_p@@Base+0x98>
   28160:	b	28170 <__gmpn_divisible_p@@Base+0x364>
   28164:	mov	w19, #0x1                   	// #1
   28168:	ldur	x0, [x29, #-8]
   2816c:	cbz	x0, 27ea4 <__gmpn_divisible_p@@Base+0x98>
   28170:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   28174:	b	27ea4 <__gmpn_divisible_p@@Base+0x98>
   28178:	sub	x0, x29, #0x8
   2817c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28180:	mov	x25, x0
   28184:	b	27f40 <__gmpn_divisible_p@@Base+0x134>
   28188:	sub	x0, x29, #0x8
   2818c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28190:	mov	x6, x0
   28194:	b	2806c <__gmpn_divisible_p@@Base+0x260>

0000000000028198 <__gmpn_divrem@@Base>:
   28198:	stp	x29, x30, [sp, #-96]!
   2819c:	stp	x28, x27, [sp, #16]
   281a0:	stp	x26, x25, [sp, #32]
   281a4:	stp	x24, x23, [sp, #48]
   281a8:	stp	x22, x21, [sp, #64]
   281ac:	stp	x20, x19, [sp, #80]
   281b0:	mov	x29, sp
   281b4:	sub	sp, sp, #0x10
   281b8:	mov	x21, x4
   281bc:	mov	x22, x3
   281c0:	mov	x20, x2
   281c4:	mov	x24, x1
   281c8:	cmp	x5, #0x2
   281cc:	mov	x19, x0
   281d0:	b.eq	2826c <__gmpn_divrem@@Base+0xd4>  // b.none
   281d4:	mov	x23, x5
   281d8:	cmp	x5, #0x1
   281dc:	b.ne	282a0 <__gmpn_divrem@@Base+0x108>  // b.any
   281e0:	add	x25, x22, x24
   281e4:	lsl	x1, x25, #3
   281e8:	mov	w8, #0x7f00                	// #32512
   281ec:	cmp	x1, x8
   281f0:	stur	xzr, [x29, #-8]
   281f4:	b.hi	2831c <__gmpn_divrem@@Base+0x184>  // b.pmore
   281f8:	add	x9, x1, #0xf
   281fc:	mov	x8, sp
   28200:	and	x9, x9, #0xfffffffffffffff0
   28204:	sub	x23, x8, x9
   28208:	mov	sp, x23
   2820c:	ldr	x4, [x21]
   28210:	mov	x0, x23
   28214:	mov	x1, x24
   28218:	mov	x2, x20
   2821c:	mov	x3, x22
   28220:	bl	cd00 <__gmpn_divrem_1@plt>
   28224:	str	x0, [x20]
   28228:	sub	x20, x25, #0x1
   2822c:	mov	x0, x19
   28230:	mov	x1, x23
   28234:	mov	x2, x20
   28238:	bl	ca50 <__gmpn_copyi@plt>
   2823c:	ldur	x0, [x29, #-8]
   28240:	ldr	x19, [x23, x20, lsl #3]
   28244:	cbnz	x0, 28314 <__gmpn_divrem@@Base+0x17c>
   28248:	mov	x0, x19
   2824c:	mov	sp, x29
   28250:	ldp	x20, x19, [sp, #80]
   28254:	ldp	x22, x21, [sp, #64]
   28258:	ldp	x24, x23, [sp, #48]
   2825c:	ldp	x26, x25, [sp, #32]
   28260:	ldp	x28, x27, [sp, #16]
   28264:	ldp	x29, x30, [sp], #96
   28268:	ret
   2826c:	mov	x0, x19
   28270:	mov	x1, x24
   28274:	mov	x2, x20
   28278:	mov	x3, x22
   2827c:	mov	x4, x21
   28280:	mov	sp, x29
   28284:	ldp	x20, x19, [sp, #80]
   28288:	ldp	x22, x21, [sp, #64]
   2828c:	ldp	x24, x23, [sp, #48]
   28290:	ldp	x26, x25, [sp, #32]
   28294:	ldp	x28, x27, [sp, #16]
   28298:	ldp	x29, x30, [sp], #96
   2829c:	b	c200 <__gmpn_divrem_2@plt>
   282a0:	stur	xzr, [x29, #-8]
   282a4:	cbnz	x24, 2832c <__gmpn_divrem@@Base+0x194>
   282a8:	sub	x24, x22, x23
   282ac:	lsl	x8, x24, #3
   282b0:	add	x1, x8, #0x8
   282b4:	mov	w8, #0x7f00                	// #32512
   282b8:	cmp	x1, x8
   282bc:	b.hi	283d0 <__gmpn_divrem@@Base+0x238>  // b.pmore
   282c0:	add	x9, x1, #0xf
   282c4:	mov	x8, sp
   282c8:	and	x9, x9, #0xfffffffffffffff0
   282cc:	sub	x25, x8, x9
   282d0:	mov	sp, x25
   282d4:	mov	x0, x25
   282d8:	mov	x1, x20
   282dc:	mov	x2, xzr
   282e0:	mov	x3, x20
   282e4:	mov	x4, x22
   282e8:	mov	x5, x21
   282ec:	mov	x6, x23
   282f0:	bl	bf00 <__gmpn_tdiv_qr@plt>
   282f4:	mov	x0, x19
   282f8:	mov	x1, x25
   282fc:	mov	x2, x24
   28300:	bl	ca50 <__gmpn_copyi@plt>
   28304:	add	x8, x25, x24, lsl #3
   28308:	ldur	x0, [x29, #-8]
   2830c:	ldr	x19, [x8]
   28310:	cbz	x0, 28248 <__gmpn_divrem@@Base+0xb0>
   28314:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   28318:	b	28248 <__gmpn_divrem@@Base+0xb0>
   2831c:	sub	x0, x29, #0x8
   28320:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28324:	mov	x23, x0
   28328:	b	2820c <__gmpn_divrem@@Base+0x74>
   2832c:	sub	x8, x22, x23
   28330:	add	x26, x22, x24
   28334:	add	x25, x8, x24
   28338:	add	x8, x26, x25
   2833c:	lsl	x8, x8, #3
   28340:	add	x1, x8, #0x8
   28344:	mov	w8, #0x7f00                	// #32512
   28348:	cmp	x1, x8
   2834c:	b.hi	283e0 <__gmpn_divrem@@Base+0x248>  // b.pmore
   28350:	add	x9, x1, #0xf
   28354:	mov	x8, sp
   28358:	and	x9, x9, #0xfffffffffffffff0
   2835c:	sub	x27, x8, x9
   28360:	mov	sp, x27
   28364:	lsl	x24, x24, #3
   28368:	mov	x0, x27
   2836c:	mov	w1, wzr
   28370:	mov	x2, x24
   28374:	bl	c5f0 <memset@plt>
   28378:	add	x0, x27, x24
   2837c:	mov	x1, x20
   28380:	mov	x2, x22
   28384:	add	x28, x27, x26, lsl #3
   28388:	bl	ca50 <__gmpn_copyi@plt>
   2838c:	mov	x0, x28
   28390:	mov	x1, x20
   28394:	mov	x2, xzr
   28398:	mov	x3, x27
   2839c:	mov	x4, x26
   283a0:	mov	x5, x21
   283a4:	mov	x6, x23
   283a8:	bl	bf00 <__gmpn_tdiv_qr@plt>
   283ac:	mov	x0, x19
   283b0:	mov	x1, x28
   283b4:	mov	x2, x25
   283b8:	bl	ca50 <__gmpn_copyi@plt>
   283bc:	add	x8, x28, x25, lsl #3
   283c0:	ldur	x0, [x29, #-8]
   283c4:	ldr	x19, [x8]
   283c8:	cbz	x0, 28248 <__gmpn_divrem@@Base+0xb0>
   283cc:	b	28314 <__gmpn_divrem@@Base+0x17c>
   283d0:	sub	x0, x29, #0x8
   283d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   283d8:	mov	x25, x0
   283dc:	b	282d4 <__gmpn_divrem@@Base+0x13c>
   283e0:	sub	x0, x29, #0x8
   283e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   283e8:	mov	x27, x0
   283ec:	b	28364 <__gmpn_divrem@@Base+0x1cc>

00000000000283f0 <__gmpn_divrem_1@@Base>:
   283f0:	stp	x29, x30, [sp, #-80]!
   283f4:	adds	x8, x3, x1
   283f8:	str	x25, [sp, #16]
   283fc:	stp	x24, x23, [sp, #32]
   28400:	stp	x22, x21, [sp, #48]
   28404:	stp	x20, x19, [sp, #64]
   28408:	mov	x29, sp
   2840c:	b.eq	28458 <__gmpn_divrem_1@@Base+0x68>  // b.none
   28410:	sub	x9, x8, #0x1
   28414:	mov	x20, x4
   28418:	mov	x21, x3
   2841c:	mov	x23, x2
   28420:	mov	x19, x1
   28424:	add	x24, x0, x9, lsl #3
   28428:	tbnz	x4, #63, 28514 <__gmpn_divrem_1@@Base+0x124>
   2842c:	cbz	x21, 28460 <__gmpn_divrem_1@@Base+0x70>
   28430:	sub	x10, x21, #0x1
   28434:	ldr	x22, [x23, x10, lsl #3]
   28438:	cmp	x22, x20
   2843c:	b.cs	28460 <__gmpn_divrem_1@@Base+0x70>  // b.hs, b.nlast
   28440:	str	xzr, [x24]
   28444:	cbz	x9, 28a38 <__gmpn_divrem_1@@Base+0x648>
   28448:	sub	x24, x24, #0x8
   2844c:	mov	x8, x9
   28450:	mov	x21, x10
   28454:	b	28464 <__gmpn_divrem_1@@Base+0x74>
   28458:	mov	x22, xzr
   2845c:	b	28a38 <__gmpn_divrem_1@@Base+0x648>
   28460:	mov	x22, xzr
   28464:	clz	x25, x20
   28468:	cmp	x8, #0x3
   2846c:	lsl	x20, x20, x25
   28470:	lsl	x22, x22, x25
   28474:	b.le	285f0 <__gmpn_divrem_1@@Base+0x200>
   28478:	mov	x0, x20
   2847c:	bl	d3f0 <__gmpn_invert_limb@plt>
   28480:	cbz	x21, 2889c <__gmpn_divrem_1@@Base+0x4ac>
   28484:	add	x8, x23, x21, lsl #3
   28488:	ldur	x12, [x8, #-8]
   2848c:	neg	x8, x25
   28490:	cmp	x21, #0x2
   28494:	lsr	x8, x12, x8
   28498:	orr	x11, x8, x22
   2849c:	b.lt	28854 <__gmpn_divrem_1@@Base+0x464>  // b.tstop
   284a0:	mov	w8, #0x40                  	// #64
   284a4:	sub	x8, x8, x25
   284a8:	sub	x9, x23, #0x10
   284ac:	ldr	x10, [x9, x21, lsl #3]
   284b0:	lsl	x12, x12, x25
   284b4:	umulh	x13, x11, x0
   284b8:	lsr	x14, x10, x8
   284bc:	orr	x12, x14, x12
   284c0:	mul	x14, x11, x0
   284c4:	add	x11, x11, #0x1
   284c8:	adds	x15, x14, x12
   284cc:	adc	x11, x13, x11
   284d0:	msub	x12, x11, x20, x12
   284d4:	cmp	x12, x15
   284d8:	cset	w14, hi  // hi = pmore
   284dc:	sub	x11, x11, x14
   284e0:	csel	x14, x20, xzr, hi  // hi = pmore
   284e4:	add	x12, x14, x12
   284e8:	cmp	x12, x20
   284ec:	sub	x13, x21, #0x2
   284f0:	cinc	x14, x11, cs  // cs = hs, nlast
   284f4:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   284f8:	sub	x21, x21, #0x1
   284fc:	cmp	x13, #0x0
   28500:	sub	x11, x12, x11
   28504:	str	x14, [x24], #-8
   28508:	mov	x12, x10
   2850c:	b.gt	284ac <__gmpn_divrem_1@@Base+0xbc>
   28510:	b	28858 <__gmpn_divrem_1@@Base+0x468>
   28514:	cbz	x21, 286dc <__gmpn_divrem_1@@Base+0x2ec>
   28518:	sub	x21, x21, #0x1
   2851c:	ldr	x8, [x23, x21, lsl #3]
   28520:	cmp	x8, x20
   28524:	cset	w10, cs  // cs = hs, nlast
   28528:	csel	x11, x20, xzr, cs  // cs = hs, nlast
   2852c:	str	x10, [x24], #-8
   28530:	sub	x22, x8, x11
   28534:	mov	x8, x9
   28538:	cmp	x8, #0x2
   2853c:	b.le	286e8 <__gmpn_divrem_1@@Base+0x2f8>
   28540:	mov	x0, x20
   28544:	bl	d3f0 <__gmpn_invert_limb@plt>
   28548:	cmp	x21, #0x1
   2854c:	b.lt	285a8 <__gmpn_divrem_1@@Base+0x1b8>  // b.tstop
   28550:	add	x8, x21, #0x1
   28554:	sub	x9, x23, #0x10
   28558:	ldr	x10, [x9, x8, lsl #3]
   2855c:	umulh	x11, x22, x0
   28560:	mul	x12, x22, x0
   28564:	add	x13, x22, #0x1
   28568:	adds	x14, x12, x10
   2856c:	adc	x11, x11, x13
   28570:	msub	x10, x11, x20, x10
   28574:	cmp	x10, x14
   28578:	csel	x13, x20, xzr, hi  // hi = pmore
   2857c:	cset	w12, hi  // hi = pmore
   28580:	add	x10, x13, x10
   28584:	sub	x11, x11, x12
   28588:	cmp	x10, x20
   2858c:	sub	x8, x8, #0x1
   28590:	csel	x12, xzr, x20, cc  // cc = lo, ul, last
   28594:	cinc	x11, x11, cs  // cs = hs, nlast
   28598:	cmp	x8, #0x1
   2859c:	sub	x22, x10, x12
   285a0:	str	x11, [x24], #-8
   285a4:	b.gt	28558 <__gmpn_divrem_1@@Base+0x168>
   285a8:	cmp	x19, #0x1
   285ac:	b.lt	28a38 <__gmpn_divrem_1@@Base+0x648>  // b.tstop
   285b0:	add	x8, x19, #0x1
   285b4:	umulh	x9, x22, x0
   285b8:	add	x9, x22, x9
   285bc:	add	x9, x9, #0x1
   285c0:	mul	x10, x22, x0
   285c4:	mneg	x11, x9, x20
   285c8:	cmp	x10, x11
   285cc:	cset	w10, cc  // cc = lo, ul, last
   285d0:	sub	x8, x8, #0x1
   285d4:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   285d8:	sub	x10, x9, x10
   285dc:	msub	x22, x9, x20, x11
   285e0:	cmp	x8, #0x1
   285e4:	str	x10, [x24], #-8
   285e8:	b.gt	285b4 <__gmpn_divrem_1@@Base+0x1c4>
   285ec:	b	28a38 <__gmpn_divrem_1@@Base+0x648>
   285f0:	cbz	x21, 28980 <__gmpn_divrem_1@@Base+0x590>
   285f4:	add	x8, x23, x21, lsl #3
   285f8:	ldur	x10, [x8, #-8]
   285fc:	neg	x8, x25
   28600:	cmp	x21, #0x1
   28604:	lsr	x8, x10, x8
   28608:	orr	x13, x8, x22
   2860c:	b.le	288e4 <__gmpn_divrem_1@@Base+0x4f4>
   28610:	mov	w11, #0x40                  	// #64
   28614:	lsr	x8, x20, #32
   28618:	and	x9, x20, #0xffffffff
   2861c:	sub	x11, x11, x25
   28620:	sub	x12, x23, #0x10
   28624:	b	28648 <__gmpn_divrem_1@@Base+0x258>
   28628:	mov	x17, x16
   2862c:	sub	x13, x13, x15
   28630:	orr	x14, x17, x14, lsl #32
   28634:	sub	x15, x21, #0x2
   28638:	sub	x21, x21, #0x1
   2863c:	cmp	x15, #0x0
   28640:	str	x14, [x24], #-8
   28644:	b.le	288ec <__gmpn_divrem_1@@Base+0x4fc>
   28648:	mov	x14, x10
   2864c:	ldr	x10, [x12, x21, lsl #3]
   28650:	udiv	x17, x13, x8
   28654:	lsl	x14, x14, x25
   28658:	msub	w16, w17, w8, w13
   2865c:	lsr	x13, x10, x11
   28660:	orr	x13, x13, x14
   28664:	mul	x15, x17, x9
   28668:	extr	x16, x16, x13, #32
   2866c:	cmp	x16, x15
   28670:	b.cs	28698 <__gmpn_divrem_1@@Base+0x2a8>  // b.hs, b.nlast
   28674:	add	x16, x16, x20
   28678:	cmp	x16, x20
   2867c:	sub	x14, x17, #0x1
   28680:	b.cc	2869c <__gmpn_divrem_1@@Base+0x2ac>  // b.lo, b.ul, b.last
   28684:	cmp	x16, x15
   28688:	b.cs	2869c <__gmpn_divrem_1@@Base+0x2ac>  // b.hs, b.nlast
   2868c:	sub	x14, x17, #0x2
   28690:	add	x16, x16, x20
   28694:	b	2869c <__gmpn_divrem_1@@Base+0x2ac>
   28698:	mov	x14, x17
   2869c:	sub	x15, x16, x15
   286a0:	udiv	x16, x15, x8
   286a4:	msub	w17, w16, w8, w15
   286a8:	mul	x15, x16, x9
   286ac:	bfi	x13, x17, #32, #32
   286b0:	cmp	x13, x15
   286b4:	b.cs	28628 <__gmpn_divrem_1@@Base+0x238>  // b.hs, b.nlast
   286b8:	add	x13, x13, x20
   286bc:	cmp	x13, x20
   286c0:	sub	x17, x16, #0x1
   286c4:	b.cc	2862c <__gmpn_divrem_1@@Base+0x23c>  // b.lo, b.ul, b.last
   286c8:	cmp	x13, x15
   286cc:	b.cs	2862c <__gmpn_divrem_1@@Base+0x23c>  // b.hs, b.nlast
   286d0:	sub	x17, x16, #0x2
   286d4:	add	x13, x13, x20
   286d8:	b	2862c <__gmpn_divrem_1@@Base+0x23c>
   286dc:	mov	x22, xzr
   286e0:	cmp	x8, #0x2
   286e4:	b.gt	28540 <__gmpn_divrem_1@@Base+0x150>
   286e8:	cmp	x21, #0x1
   286ec:	lsr	x8, x20, #32
   286f0:	b.lt	287a4 <__gmpn_divrem_1@@Base+0x3b4>  // b.tstop
   286f4:	and	x9, x20, #0xffffffff
   286f8:	add	x10, x21, #0x1
   286fc:	sub	x11, x23, #0x10
   28700:	b	28720 <__gmpn_divrem_1@@Base+0x330>
   28704:	mov	x16, x15
   28708:	sub	x22, x12, x14
   2870c:	orr	x12, x16, x13, lsl #32
   28710:	sub	x10, x10, #0x1
   28714:	cmp	x10, #0x1
   28718:	str	x12, [x24], #-8
   2871c:	b.le	287a4 <__gmpn_divrem_1@@Base+0x3b4>
   28720:	ldr	x12, [x11, x10, lsl #3]
   28724:	udiv	x16, x22, x8
   28728:	msub	w13, w16, w8, w22
   2872c:	mul	x14, x16, x9
   28730:	extr	x15, x13, x12, #32
   28734:	cmp	x15, x14
   28738:	b.cs	28760 <__gmpn_divrem_1@@Base+0x370>  // b.hs, b.nlast
   2873c:	add	x15, x15, x20
   28740:	cmp	x15, x20
   28744:	sub	x13, x16, #0x1
   28748:	b.cc	28764 <__gmpn_divrem_1@@Base+0x374>  // b.lo, b.ul, b.last
   2874c:	cmp	x15, x14
   28750:	b.cs	28764 <__gmpn_divrem_1@@Base+0x374>  // b.hs, b.nlast
   28754:	sub	x13, x16, #0x2
   28758:	add	x15, x15, x20
   2875c:	b	28764 <__gmpn_divrem_1@@Base+0x374>
   28760:	mov	x13, x16
   28764:	sub	x14, x15, x14
   28768:	udiv	x15, x14, x8
   2876c:	msub	w16, w15, w8, w14
   28770:	mul	x14, x15, x9
   28774:	bfi	x12, x16, #32, #32
   28778:	cmp	x12, x14
   2877c:	b.cs	28704 <__gmpn_divrem_1@@Base+0x314>  // b.hs, b.nlast
   28780:	add	x12, x12, x20
   28784:	cmp	x12, x20
   28788:	sub	x16, x15, #0x1
   2878c:	b.cc	28708 <__gmpn_divrem_1@@Base+0x318>  // b.lo, b.ul, b.last
   28790:	cmp	x12, x14
   28794:	b.cs	28708 <__gmpn_divrem_1@@Base+0x318>  // b.hs, b.nlast
   28798:	sub	x16, x15, #0x2
   2879c:	add	x12, x12, x20
   287a0:	b	28708 <__gmpn_divrem_1@@Base+0x318>
   287a4:	cmp	x19, #0x1
   287a8:	b.lt	28a38 <__gmpn_divrem_1@@Base+0x648>  // b.tstop
   287ac:	and	x9, x20, #0xffffffff
   287b0:	add	x10, x19, #0x1
   287b4:	b	287d4 <__gmpn_divrem_1@@Base+0x3e4>
   287b8:	mov	x15, x14
   287bc:	orr	x11, x15, x11, lsl #32
   287c0:	sub	x10, x10, #0x1
   287c4:	sub	x22, x13, x12
   287c8:	cmp	x10, #0x1
   287cc:	str	x11, [x24], #-8
   287d0:	b.le	28a38 <__gmpn_divrem_1@@Base+0x648>
   287d4:	udiv	x14, x22, x8
   287d8:	msub	w11, w14, w8, w22
   287dc:	mul	x12, x14, x9
   287e0:	lsl	x13, x11, #32
   287e4:	cmp	x13, x12
   287e8:	b.cs	28810 <__gmpn_divrem_1@@Base+0x420>  // b.hs, b.nlast
   287ec:	add	x13, x13, x20
   287f0:	cmp	x13, x20
   287f4:	sub	x11, x14, #0x1
   287f8:	b.cc	28814 <__gmpn_divrem_1@@Base+0x424>  // b.lo, b.ul, b.last
   287fc:	cmp	x13, x12
   28800:	b.cs	28814 <__gmpn_divrem_1@@Base+0x424>  // b.hs, b.nlast
   28804:	sub	x11, x14, #0x2
   28808:	add	x13, x13, x20
   2880c:	b	28814 <__gmpn_divrem_1@@Base+0x424>
   28810:	mov	x11, x14
   28814:	sub	x12, x13, x12
   28818:	udiv	x14, x12, x8
   2881c:	msub	w13, w14, w8, w12
   28820:	mul	x12, x14, x9
   28824:	lsl	x13, x13, #32
   28828:	cmp	x13, x12
   2882c:	b.cs	287b8 <__gmpn_divrem_1@@Base+0x3c8>  // b.hs, b.nlast
   28830:	add	x13, x13, x20
   28834:	cmp	x13, x20
   28838:	sub	x15, x14, #0x1
   2883c:	b.cc	287bc <__gmpn_divrem_1@@Base+0x3cc>  // b.lo, b.ul, b.last
   28840:	cmp	x13, x12
   28844:	b.cs	287bc <__gmpn_divrem_1@@Base+0x3cc>  // b.hs, b.nlast
   28848:	sub	x15, x14, #0x2
   2884c:	add	x13, x13, x20
   28850:	b	287bc <__gmpn_divrem_1@@Base+0x3cc>
   28854:	mov	x10, x12
   28858:	umulh	x8, x11, x0
   2885c:	mul	x9, x11, x0
   28860:	lsl	x10, x10, x25
   28864:	add	x11, x11, #0x1
   28868:	adds	x12, x9, x10
   2886c:	adc	x8, x8, x11
   28870:	msub	x9, x8, x20, x10
   28874:	cmp	x9, x12
   28878:	csel	x11, x20, xzr, hi  // hi = pmore
   2887c:	cset	w10, hi  // hi = pmore
   28880:	add	x9, x11, x9
   28884:	sub	x8, x8, x10
   28888:	cmp	x9, x20
   2888c:	cinc	x8, x8, cs  // cs = hs, nlast
   28890:	csel	x10, xzr, x20, cc  // cc = lo, ul, last
   28894:	sub	x22, x9, x10
   28898:	str	x8, [x24], #-8
   2889c:	cmp	x19, #0x1
   288a0:	b.lt	28a34 <__gmpn_divrem_1@@Base+0x644>  // b.tstop
   288a4:	add	x8, x19, #0x1
   288a8:	umulh	x9, x22, x0
   288ac:	add	x9, x22, x9
   288b0:	add	x9, x9, #0x1
   288b4:	mul	x10, x22, x0
   288b8:	mneg	x11, x9, x20
   288bc:	cmp	x10, x11
   288c0:	cset	w10, cc  // cc = lo, ul, last
   288c4:	sub	x8, x8, #0x1
   288c8:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   288cc:	sub	x10, x9, x10
   288d0:	msub	x22, x9, x20, x11
   288d4:	cmp	x8, #0x1
   288d8:	str	x10, [x24], #-8
   288dc:	b.gt	288a8 <__gmpn_divrem_1@@Base+0x4b8>
   288e0:	b	28a34 <__gmpn_divrem_1@@Base+0x644>
   288e4:	lsr	x8, x20, #32
   288e8:	and	x9, x20, #0xffffffff
   288ec:	udiv	x14, x13, x8
   288f0:	msub	w11, w14, w8, w13
   288f4:	lsl	x10, x10, x25
   288f8:	mul	x12, x14, x9
   288fc:	extr	x13, x11, x10, #32
   28900:	cmp	x13, x12
   28904:	b.cs	2892c <__gmpn_divrem_1@@Base+0x53c>  // b.hs, b.nlast
   28908:	add	x13, x13, x20
   2890c:	cmp	x13, x20
   28910:	sub	x11, x14, #0x1
   28914:	b.cc	28930 <__gmpn_divrem_1@@Base+0x540>  // b.lo, b.ul, b.last
   28918:	cmp	x13, x12
   2891c:	b.cs	28930 <__gmpn_divrem_1@@Base+0x540>  // b.hs, b.nlast
   28920:	sub	x11, x14, #0x2
   28924:	add	x13, x13, x20
   28928:	b	28930 <__gmpn_divrem_1@@Base+0x540>
   2892c:	mov	x11, x14
   28930:	sub	x13, x13, x12
   28934:	udiv	x12, x13, x8
   28938:	msub	w13, w12, w8, w13
   2893c:	mul	x8, x12, x9
   28940:	bfi	x10, x13, #32, #32
   28944:	cmp	x10, x8
   28948:	b.cs	28970 <__gmpn_divrem_1@@Base+0x580>  // b.hs, b.nlast
   2894c:	add	x10, x10, x20
   28950:	cmp	x10, x20
   28954:	sub	x9, x12, #0x1
   28958:	b.cc	28974 <__gmpn_divrem_1@@Base+0x584>  // b.lo, b.ul, b.last
   2895c:	cmp	x10, x8
   28960:	b.cs	28974 <__gmpn_divrem_1@@Base+0x584>  // b.hs, b.nlast
   28964:	sub	x9, x12, #0x2
   28968:	add	x10, x10, x20
   2896c:	b	28974 <__gmpn_divrem_1@@Base+0x584>
   28970:	mov	x9, x12
   28974:	sub	x22, x10, x8
   28978:	orr	x8, x9, x11, lsl #32
   2897c:	str	x8, [x24], #-8
   28980:	cmp	x19, #0x1
   28984:	b.lt	28a34 <__gmpn_divrem_1@@Base+0x644>  // b.tstop
   28988:	lsr	x8, x20, #32
   2898c:	and	x9, x20, #0xffffffff
   28990:	add	x10, x19, #0x1
   28994:	b	289b4 <__gmpn_divrem_1@@Base+0x5c4>
   28998:	mov	x15, x14
   2899c:	orr	x11, x15, x11, lsl #32
   289a0:	sub	x10, x10, #0x1
   289a4:	sub	x22, x13, x12
   289a8:	cmp	x10, #0x1
   289ac:	str	x11, [x24], #-8
   289b0:	b.le	28a34 <__gmpn_divrem_1@@Base+0x644>
   289b4:	udiv	x14, x22, x8
   289b8:	msub	w11, w14, w8, w22
   289bc:	mul	x12, x14, x9
   289c0:	lsl	x13, x11, #32
   289c4:	cmp	x13, x12
   289c8:	b.cs	289f0 <__gmpn_divrem_1@@Base+0x600>  // b.hs, b.nlast
   289cc:	add	x13, x13, x20
   289d0:	cmp	x13, x20
   289d4:	sub	x11, x14, #0x1
   289d8:	b.cc	289f4 <__gmpn_divrem_1@@Base+0x604>  // b.lo, b.ul, b.last
   289dc:	cmp	x13, x12
   289e0:	b.cs	289f4 <__gmpn_divrem_1@@Base+0x604>  // b.hs, b.nlast
   289e4:	sub	x11, x14, #0x2
   289e8:	add	x13, x13, x20
   289ec:	b	289f4 <__gmpn_divrem_1@@Base+0x604>
   289f0:	mov	x11, x14
   289f4:	sub	x12, x13, x12
   289f8:	udiv	x14, x12, x8
   289fc:	msub	w13, w14, w8, w12
   28a00:	mul	x12, x14, x9
   28a04:	lsl	x13, x13, #32
   28a08:	cmp	x13, x12
   28a0c:	b.cs	28998 <__gmpn_divrem_1@@Base+0x5a8>  // b.hs, b.nlast
   28a10:	add	x13, x13, x20
   28a14:	cmp	x13, x20
   28a18:	sub	x15, x14, #0x1
   28a1c:	b.cc	2899c <__gmpn_divrem_1@@Base+0x5ac>  // b.lo, b.ul, b.last
   28a20:	cmp	x13, x12
   28a24:	b.cs	2899c <__gmpn_divrem_1@@Base+0x5ac>  // b.hs, b.nlast
   28a28:	sub	x15, x14, #0x2
   28a2c:	add	x13, x13, x20
   28a30:	b	2899c <__gmpn_divrem_1@@Base+0x5ac>
   28a34:	lsr	x22, x22, x25
   28a38:	mov	x0, x22
   28a3c:	ldp	x20, x19, [sp, #64]
   28a40:	ldp	x22, x21, [sp, #48]
   28a44:	ldp	x24, x23, [sp, #32]
   28a48:	ldr	x25, [sp, #16]
   28a4c:	ldp	x29, x30, [sp], #80
   28a50:	ret

0000000000028a54 <__gmpn_divrem_2@@Base>:
   28a54:	stp	x29, x30, [sp, #-96]!
   28a58:	stp	x28, x27, [sp, #16]
   28a5c:	stp	x26, x25, [sp, #32]
   28a60:	stp	x24, x23, [sp, #48]
   28a64:	stp	x22, x21, [sp, #64]
   28a68:	stp	x20, x19, [sp, #80]
   28a6c:	add	x28, x2, x3, lsl #3
   28a70:	ldr	x27, [x28, #-16]!
   28a74:	ldp	x25, x20, [x4]
   28a78:	mov	x24, x3
   28a7c:	mov	x22, x2
   28a80:	ldr	x26, [x28, #8]
   28a84:	mov	x19, x1
   28a88:	mov	x23, x0
   28a8c:	mov	x29, sp
   28a90:	cmp	x26, x20
   28a94:	b.cc	28aa4 <__gmpn_divrem_2@@Base+0x50>  // b.lo, b.ul, b.last
   28a98:	b.hi	28aac <__gmpn_divrem_2@@Base+0x58>  // b.pmore
   28a9c:	cmp	x27, x25
   28aa0:	b.cs	28aac <__gmpn_divrem_2@@Base+0x58>  // b.hs, b.nlast
   28aa4:	mov	x21, xzr
   28aa8:	b	28abc <__gmpn_divrem_2@@Base+0x68>
   28aac:	subs	x8, x27, x25
   28ab0:	sbc	x26, x26, x20
   28ab4:	mov	w21, #0x1                   	// #1
   28ab8:	mov	x27, x8
   28abc:	mov	x0, x20
   28ac0:	bl	d3f0 <__gmpn_invert_limb@plt>
   28ac4:	mul	x8, x0, x20
   28ac8:	adds	x8, x8, x25
   28acc:	b.cc	28ae8 <__gmpn_divrem_2@@Base+0x94>  // b.lo, b.ul, b.last
   28ad0:	subs	x8, x8, x20
   28ad4:	cset	w9, cs  // cs = hs, nlast
   28ad8:	csel	x10, x20, xzr, cs  // cs = hs, nlast
   28adc:	mvn	x9, x9
   28ae0:	add	x0, x9, x0
   28ae4:	sub	x8, x8, x10
   28ae8:	umulh	x9, x25, x0
   28aec:	adds	x9, x9, x8
   28af0:	b.cc	28ba4 <__gmpn_divrem_2@@Base+0x150>  // b.lo, b.ul, b.last
   28af4:	cmp	x9, x20
   28af8:	sub	x8, x0, #0x1
   28afc:	b.cs	28bec <__gmpn_divrem_2@@Base+0x198>  // b.hs, b.nlast
   28b00:	subs	x9, x24, #0x3
   28b04:	b.lt	28bb0 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   28b08:	add	x10, x23, x19, lsl #3
   28b0c:	ldr	x11, [x22, x9, lsl #3]
   28b10:	mul	x12, x26, x8
   28b14:	umulh	x13, x26, x8
   28b18:	adds	x14, x12, x27
   28b1c:	adc	x12, x13, x26
   28b20:	msub	x13, x12, x20, x27
   28b24:	subs	x17, x11, x25
   28b28:	sbc	x11, x13, x20
   28b2c:	mul	x15, x12, x25
   28b30:	umulh	x16, x25, x12
   28b34:	subs	x13, x17, x15
   28b38:	sbc	x11, x11, x16
   28b3c:	cmp	x11, x14
   28b40:	cset	w14, cs  // cs = hs, nlast
   28b44:	csetm	x15, cs  // cs = hs, nlast
   28b48:	sub	x12, x12, x14
   28b4c:	and	x14, x25, x15
   28b50:	and	x15, x20, x15
   28b54:	adds	x27, x13, x14
   28b58:	adc	x26, x11, x15
   28b5c:	cmp	x26, x20
   28b60:	add	x11, x12, #0x1
   28b64:	b.cs	28b80 <__gmpn_divrem_2@@Base+0x12c>  // b.hs, b.nlast
   28b68:	sub	x12, x9, #0x1
   28b6c:	cmp	x9, #0x0
   28b70:	str	x11, [x10, x9, lsl #3]
   28b74:	mov	x9, x12
   28b78:	b.gt	28b0c <__gmpn_divrem_2@@Base+0xb8>
   28b7c:	b	28bbc <__gmpn_divrem_2@@Base+0x168>
   28b80:	cmp	x27, x25
   28b84:	b.cs	28b90 <__gmpn_divrem_2@@Base+0x13c>  // b.hs, b.nlast
   28b88:	cmp	x26, x20
   28b8c:	b.ls	28b68 <__gmpn_divrem_2@@Base+0x114>  // b.plast
   28b90:	subs	x12, x27, x25
   28b94:	sbc	x26, x26, x20
   28b98:	add	x11, x11, #0x1
   28b9c:	mov	x27, x12
   28ba0:	b	28b68 <__gmpn_divrem_2@@Base+0x114>
   28ba4:	mov	x8, x0
   28ba8:	subs	x9, x24, #0x3
   28bac:	b.ge	28b08 <__gmpn_divrem_2@@Base+0xb4>  // b.tcont
   28bb0:	cmp	x19, #0x1
   28bb4:	b.lt	28bc8 <__gmpn_divrem_2@@Base+0x174>  // b.tstop
   28bb8:	b	28c0c <__gmpn_divrem_2@@Base+0x1b8>
   28bbc:	mov	x28, x22
   28bc0:	cmp	x19, #0x1
   28bc4:	b.ge	28c0c <__gmpn_divrem_2@@Base+0x1b8>  // b.tcont
   28bc8:	stp	x27, x26, [x28]
   28bcc:	mov	x0, x21
   28bd0:	ldp	x20, x19, [sp, #80]
   28bd4:	ldp	x22, x21, [sp, #64]
   28bd8:	ldp	x24, x23, [sp, #48]
   28bdc:	ldp	x26, x25, [sp, #32]
   28be0:	ldp	x28, x27, [sp, #16]
   28be4:	ldp	x29, x30, [sp], #96
   28be8:	ret
   28bec:	mul	x10, x0, x25
   28bf0:	cmp	x9, x20
   28bf4:	sub	x11, x0, #0x2
   28bf8:	ccmp	x10, x25, #0x2, ls  // ls = plast
   28bfc:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   28c00:	subs	x9, x24, #0x3
   28c04:	b.lt	28bb0 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   28c08:	b	28b08 <__gmpn_divrem_2@@Base+0xb4>
   28c0c:	sub	x9, x23, #0x8
   28c10:	mov	x11, xzr
   28c14:	mul	x12, x26, x8
   28c18:	umulh	x13, x26, x8
   28c1c:	adds	x14, x12, x27
   28c20:	adc	x12, x13, x26
   28c24:	msub	x13, x12, x20, x27
   28c28:	subs	x17, x11, x25
   28c2c:	sbc	x11, x13, x20
   28c30:	mul	x15, x12, x25
   28c34:	umulh	x16, x25, x12
   28c38:	subs	x13, x17, x15
   28c3c:	sbc	x11, x11, x16
   28c40:	cmp	x11, x14
   28c44:	cset	w14, cs  // cs = hs, nlast
   28c48:	csetm	x15, cs  // cs = hs, nlast
   28c4c:	sub	x12, x12, x14
   28c50:	and	x14, x25, x15
   28c54:	and	x15, x20, x15
   28c58:	adds	x27, x13, x14
   28c5c:	adc	x26, x11, x15
   28c60:	sub	x10, x19, #0x1
   28c64:	cmp	x26, x20
   28c68:	add	x11, x12, #0x1
   28c6c:	b.cs	28c88 <__gmpn_divrem_2@@Base+0x234>  // b.hs, b.nlast
   28c70:	add	x12, x10, #0x1
   28c74:	cmp	x12, #0x1
   28c78:	str	x11, [x9, x19, lsl #3]
   28c7c:	mov	x19, x10
   28c80:	b.gt	28c10 <__gmpn_divrem_2@@Base+0x1bc>
   28c84:	b	28bc8 <__gmpn_divrem_2@@Base+0x174>
   28c88:	cmp	x27, x25
   28c8c:	b.cs	28c98 <__gmpn_divrem_2@@Base+0x244>  // b.hs, b.nlast
   28c90:	cmp	x26, x20
   28c94:	b.ls	28c70 <__gmpn_divrem_2@@Base+0x21c>  // b.plast
   28c98:	subs	x12, x27, x25
   28c9c:	sbc	x26, x26, x20
   28ca0:	add	x11, x11, #0x1
   28ca4:	mov	x27, x12
   28ca8:	b	28c70 <__gmpn_divrem_2@@Base+0x21c>

0000000000028cac <__gmpn_fib2_ui@@Base>:
   28cac:	stp	x29, x30, [sp, #-96]!
   28cb0:	stp	x24, x23, [sp, #48]
   28cb4:	stp	x22, x21, [sp, #64]
   28cb8:	stp	x20, x19, [sp, #80]
   28cbc:	mov	x19, x2
   28cc0:	mov	x20, x1
   28cc4:	cmp	x2, #0x5e
   28cc8:	mov	x21, x0
   28ccc:	mov	w24, #0x1                   	// #1
   28cd0:	str	x27, [sp, #16]
   28cd4:	stp	x26, x25, [sp, #32]
   28cd8:	mov	x29, sp
   28cdc:	b.cc	28cfc <__gmpn_fib2_ui@@Base+0x50>  // b.lo, b.ul, b.last
   28ce0:	mov	x8, x19
   28ce4:	lsr	x9, x8, #1
   28ce8:	cmp	x8, #0xbb
   28cec:	lsl	x24, x24, #1
   28cf0:	mov	x8, x9
   28cf4:	b.hi	28ce4 <__gmpn_fib2_ui@@Base+0x38>  // b.pmore
   28cf8:	b	28d00 <__gmpn_fib2_ui@@Base+0x54>
   28cfc:	mov	x9, x19
   28d00:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   28d04:	ldr	x8, [x8, #3808]
   28d08:	cmp	x24, #0x1
   28d0c:	add	x8, x8, x9, lsl #3
   28d10:	ldp	x9, x8, [x8]
   28d14:	str	x9, [x20]
   28d18:	str	x8, [x21]
   28d1c:	b.ne	28d28 <__gmpn_fib2_ui@@Base+0x7c>  // b.any
   28d20:	mov	w23, #0x1                   	// #1
   28d24:	b	28e80 <__gmpn_fib2_ui@@Base+0x1d4>
   28d28:	lsr	x8, x19, #5
   28d2c:	mov	w9, #0x17                  	// #23
   28d30:	mul	x8, x8, x9
   28d34:	lsr	x8, x8, #6
   28d38:	lsl	x9, x8, #3
   28d3c:	cmp	x8, #0xfdc
   28d40:	add	x1, x9, #0x20
   28d44:	str	xzr, [x29, #24]
   28d48:	b.hi	28ea4 <__gmpn_fib2_ui@@Base+0x1f8>  // b.pmore
   28d4c:	add	x9, x1, #0xf
   28d50:	mov	x8, sp
   28d54:	and	x9, x9, #0x7ffffffffffffff0
   28d58:	sub	x22, x8, x9
   28d5c:	mov	sp, x22
   28d60:	add	x25, x21, #0x8
   28d64:	mov	w23, #0x1                   	// #1
   28d68:	b	28d9c <__gmpn_fib2_ui@@Base+0xf0>
   28d6c:	mov	x0, x21
   28d70:	mov	x1, x21
   28d74:	mov	x2, x20
   28d78:	mov	x3, x23
   28d7c:	bl	c2d0 <__gmpn_sub_n@plt>
   28d80:	add	x8, x21, x23, lsl #3
   28d84:	ldur	x8, [x8, #-8]
   28d88:	cmp	x8, #0x0
   28d8c:	cset	w8, eq  // eq = none
   28d90:	sub	x23, x23, x8
   28d94:	cmp	x24, #0x1
   28d98:	b.eq	28e78 <__gmpn_fib2_ui@@Base+0x1cc>  // b.none
   28d9c:	mov	x0, x22
   28da0:	mov	x1, x21
   28da4:	mov	x2, x23
   28da8:	bl	c8e0 <__gmpn_sqr@plt>
   28dac:	mov	x0, x21
   28db0:	mov	x1, x20
   28db4:	mov	x2, x23
   28db8:	bl	c8e0 <__gmpn_sqr@plt>
   28dbc:	add	x8, x22, x23, lsl #4
   28dc0:	ldur	x8, [x8, #-8]
   28dc4:	lsl	x9, x23, #1
   28dc8:	mov	x0, x20
   28dcc:	mov	x1, x22
   28dd0:	cmp	x8, #0x0
   28dd4:	cset	w8, eq  // eq = none
   28dd8:	sub	x23, x9, x8
   28ddc:	mov	x2, x21
   28de0:	mov	x3, x23
   28de4:	bl	ca70 <__gmpn_add_n@plt>
   28de8:	lsl	x26, x23, #3
   28dec:	str	x0, [x20, x26]
   28df0:	ldr	x8, [x21]
   28df4:	tst	x24, x19
   28df8:	cset	w9, ne  // ne = any
   28dfc:	mov	x0, x21
   28e00:	orr	x8, x8, x9, lsl #1
   28e04:	mov	x1, x21
   28e08:	mov	x2, x22
   28e0c:	mov	x3, x23
   28e10:	cset	w27, eq  // eq = none
   28e14:	str	x8, [x21]
   28e18:	bl	cbe0 <__gmpn_rsblsh2_n@plt>
   28e1c:	str	x0, [x21, x26]
   28e20:	ldr	x8, [x21]
   28e24:	adds	x8, x8, w27, uxtw #1
   28e28:	str	x8, [x21]
   28e2c:	b.cc	28e44 <__gmpn_fib2_ui@@Base+0x198>  // b.lo, b.ul, b.last
   28e30:	mov	x8, x25
   28e34:	ldr	x9, [x8]
   28e38:	adds	x9, x9, #0x1
   28e3c:	str	x9, [x8], #8
   28e40:	b.cs	28e34 <__gmpn_fib2_ui@@Base+0x188>  // b.hs, b.nlast
   28e44:	ldr	x8, [x21, x23, lsl #3]
   28e48:	lsr	x24, x24, #1
   28e4c:	cmp	x8, #0x0
   28e50:	cinc	x23, x23, ne  // ne = any
   28e54:	tst	x24, x19
   28e58:	b.eq	28d6c <__gmpn_fib2_ui@@Base+0xc0>  // b.none
   28e5c:	mov	x0, x20
   28e60:	mov	x1, x21
   28e64:	mov	x2, x20
   28e68:	mov	x3, x23
   28e6c:	bl	c2d0 <__gmpn_sub_n@plt>
   28e70:	cmp	x24, #0x1
   28e74:	b.ne	28d9c <__gmpn_fib2_ui@@Base+0xf0>  // b.any
   28e78:	ldr	x0, [x29, #24]
   28e7c:	cbnz	x0, 28eb4 <__gmpn_fib2_ui@@Base+0x208>
   28e80:	mov	x0, x23
   28e84:	mov	sp, x29
   28e88:	ldp	x20, x19, [sp, #80]
   28e8c:	ldp	x22, x21, [sp, #64]
   28e90:	ldp	x24, x23, [sp, #48]
   28e94:	ldp	x26, x25, [sp, #32]
   28e98:	ldr	x27, [sp, #16]
   28e9c:	ldp	x29, x30, [sp], #96
   28ea0:	ret
   28ea4:	add	x0, x29, #0x18
   28ea8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28eac:	mov	x22, x0
   28eb0:	b	28d60 <__gmpn_fib2_ui@@Base+0xb4>
   28eb4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   28eb8:	b	28e80 <__gmpn_fib2_ui@@Base+0x1d4>

0000000000028ebc <__gmpn_fib2m@@Base>:
   28ebc:	stp	x29, x30, [sp, #-96]!
   28ec0:	stp	x28, x27, [sp, #16]
   28ec4:	stp	x26, x25, [sp, #32]
   28ec8:	stp	x24, x23, [sp, #48]
   28ecc:	stp	x22, x21, [sp, #64]
   28ed0:	stp	x20, x19, [sp, #80]
   28ed4:	mov	x29, sp
   28ed8:	sub	sp, sp, #0x40
   28edc:	mov	x11, #0x2c84                	// #11396
   28ee0:	movk	x11, #0x2164, lsl #16
   28ee4:	sub	x10, x3, #0x1
   28ee8:	movk	x11, #0x590b, lsl #32
   28eec:	ldr	x8, [x2, x10, lsl #3]
   28ef0:	mov	w9, #0x5c                  	// #92
   28ef4:	movk	x11, #0x2c8, lsl #48
   28ef8:	mul	x9, x5, x9
   28efc:	cmp	x5, x11
   28f00:	csinv	x9, x9, xzr, ls  // ls = plast
   28f04:	clz	x13, x8
   28f08:	clz	x14, x9
   28f0c:	mov	x19, x5
   28f10:	mov	x21, x1
   28f14:	subs	w11, w14, w13
   28f18:	mov	x23, x0
   28f1c:	stur	x4, [x29, #-56]
   28f20:	stur	x2, [x29, #-32]
   28f24:	b.cs	28f5c <__gmpn_fib2m@@Base+0xa0>  // b.hs, b.nlast
   28f28:	subs	x12, x3, #0x2
   28f2c:	b.lt	28f64 <__gmpn_fib2m@@Base+0xa8>  // b.tstop
   28f30:	ldur	x11, [x29, #-32]
   28f34:	sub	w10, w13, w14
   28f38:	lsl	x8, x8, x10
   28f3c:	ldr	x13, [x11, x12, lsl #3]
   28f40:	mov	w11, #0x40                  	// #64
   28f44:	sub	w11, w11, w10
   28f48:	neg	w10, w10
   28f4c:	lsr	x10, x13, x10
   28f50:	orr	x8, x10, x8
   28f54:	mov	x10, x12
   28f58:	b	28f68 <__gmpn_fib2m@@Base+0xac>
   28f5c:	lsr	x8, x8, x11
   28f60:	b	28f68 <__gmpn_fib2m@@Base+0xac>
   28f64:	mov	w11, wzr
   28f68:	lsl	x10, x10, #6
   28f6c:	cmp	x8, x9
   28f70:	add	x9, x10, w11, sxtw
   28f74:	cset	w10, hi  // hi = pmore
   28f78:	lsr	x24, x8, x10
   28f7c:	mov	x0, x23
   28f80:	mov	x1, x21
   28f84:	mov	x2, x24
   28f88:	cinc	x22, x9, hi  // hi = pmore
   28f8c:	bl	d070 <__gmpn_fib2_ui@plt>
   28f90:	mov	x25, x0
   28f94:	cmp	x0, x19
   28f98:	b.eq	28fc8 <__gmpn_fib2m@@Base+0x10c>  // b.none
   28f9c:	sub	x8, x19, x25
   28fa0:	lsl	x20, x25, #3
   28fa4:	lsl	x26, x8, #3
   28fa8:	add	x0, x23, x20
   28fac:	mov	w1, wzr
   28fb0:	mov	x2, x26
   28fb4:	bl	c5f0 <memset@plt>
   28fb8:	add	x0, x21, x20
   28fbc:	mov	w1, wzr
   28fc0:	mov	x2, x26
   28fc4:	bl	c5f0 <memset@plt>
   28fc8:	cbz	x22, 29264 <__gmpn_fib2m@@Base+0x3a8>
   28fcc:	cmp	x19, #0x2
   28fd0:	cset	w8, lt  // lt = tstop
   28fd4:	bfi	x8, x19, #1, #63
   28fd8:	lsl	x1, x8, #3
   28fdc:	mov	w8, #0x7f00                	// #32512
   28fe0:	and	w20, w24, #0x1
   28fe4:	cmp	x1, x8
   28fe8:	lsl	x28, x19, #1
   28fec:	stur	xzr, [x29, #-16]
   28ff0:	b.hi	292d4 <__gmpn_fib2m@@Base+0x418>  // b.pmore
   28ff4:	add	x9, x1, #0xf
   28ff8:	mov	x8, sp
   28ffc:	and	x9, x9, #0xfffffffffffffff0
   29000:	sub	x25, x8, x9
   29004:	mov	sp, x25
   29008:	ldur	x24, [x29, #-56]
   2900c:	orr	x8, x28, #0x1
   29010:	add	x9, x23, #0x8
   29014:	stur	x8, [x29, #-24]
   29018:	sub	x8, x8, #0x1
   2901c:	lsl	x27, x28, #3
   29020:	stur	x9, [x29, #-48]
   29024:	stur	x8, [x29, #-64]
   29028:	b	29088 <__gmpn_fib2m@@Base+0x1cc>
   2902c:	mov	x1, x23
   29030:	mov	x2, x21
   29034:	bl	c2d0 <__gmpn_sub_n@plt>
   29038:	stur	wzr, [x29, #-36]
   2903c:	mov	x28, x26
   29040:	ldur	x26, [x29, #-24]
   29044:	mov	x0, x25
   29048:	mov	x1, x23
   2904c:	mov	x2, xzr
   29050:	mov	x3, x23
   29054:	mov	x4, x26
   29058:	mov	x5, x24
   2905c:	mov	x6, x19
   29060:	bl	bf00 <__gmpn_tdiv_qr@plt>
   29064:	mov	x0, x25
   29068:	mov	x1, x21
   2906c:	mov	x2, xzr
   29070:	mov	x3, x21
   29074:	mov	x4, x26
   29078:	mov	x5, x24
   2907c:	mov	x6, x19
   29080:	bl	bf00 <__gmpn_tdiv_qr@plt>
   29084:	cbz	x22, 29254 <__gmpn_fib2m@@Base+0x398>
   29088:	mov	x0, x25
   2908c:	mov	x1, x23
   29090:	mov	x2, x19
   29094:	bl	c8e0 <__gmpn_sqr@plt>
   29098:	mov	x0, x23
   2909c:	mov	x1, x21
   290a0:	mov	x2, x19
   290a4:	bl	c8e0 <__gmpn_sqr@plt>
   290a8:	mov	x0, x21
   290ac:	mov	x1, x25
   290b0:	mov	x2, x23
   290b4:	mov	x3, x28
   290b8:	bl	ca70 <__gmpn_add_n@plt>
   290bc:	str	x0, [x21, x27]
   290c0:	ldr	x8, [x23]
   290c4:	lsl	w20, w20, #1
   290c8:	mov	x0, x23
   290cc:	mov	x1, x23
   290d0:	orr	x8, x8, x20
   290d4:	mov	x2, x25
   290d8:	mov	x3, x28
   290dc:	str	x8, [x23]
   290e0:	mov	x26, x28
   290e4:	bl	cbe0 <__gmpn_rsblsh2_n@plt>
   290e8:	add	x8, x0, #0x1
   290ec:	str	x8, [x23, x27]
   290f0:	ldr	x8, [x23]
   290f4:	eor	w9, w20, #0x2
   290f8:	adds	x8, x8, x9
   290fc:	str	x8, [x23]
   29100:	b.cc	29118 <__gmpn_fib2m@@Base+0x25c>  // b.lo, b.ul, b.last
   29104:	ldur	x8, [x29, #-48]
   29108:	ldr	x9, [x8]
   2910c:	adds	x9, x9, #0x1
   29110:	str	x9, [x8], #8
   29114:	b.cs	29108 <__gmpn_fib2m@@Base+0x24c>  // b.hs, b.nlast
   29118:	ldr	x8, [x23, x27]
   2911c:	sub	x22, x22, #0x1
   29120:	lsr	x9, x22, #3
   29124:	and	x9, x9, #0x1ffffffffffffff8
   29128:	sub	x10, x8, #0x1
   2912c:	str	x10, [x23, x27]
   29130:	ldur	x10, [x29, #-32]
   29134:	ldr	x9, [x10, x9]
   29138:	lsr	x9, x9, x22
   2913c:	tst	w9, #0x1
   29140:	and	w20, w9, #0x1
   29144:	csel	x28, x21, x23, ne  // ne = any
   29148:	cbz	x8, 2918c <__gmpn_fib2m@@Base+0x2d0>
   2914c:	ldur	x8, [x29, #-24]
   29150:	cmp	x8, #0x1
   29154:	b.lt	29038 <__gmpn_fib2m@@Base+0x17c>  // b.tstop
   29158:	ldur	x8, [x29, #-64]
   2915c:	lsl	x9, x8, #3
   29160:	ldr	x10, [x23, x9]
   29164:	ldr	x9, [x21, x9]
   29168:	cmp	x10, x9
   2916c:	b.ne	2921c <__gmpn_fib2m@@Base+0x360>  // b.any
   29170:	add	x9, x8, #0x1
   29174:	sub	x10, x8, #0x1
   29178:	cmp	x9, #0x1
   2917c:	str	xzr, [x28, x8, lsl #3]
   29180:	mov	x8, x10
   29184:	b.gt	2915c <__gmpn_fib2m@@Base+0x2a0>
   29188:	b	29038 <__gmpn_fib2m@@Base+0x17c>
   2918c:	ldr	x24, [x21, x27]
   29190:	cmp	w20, #0x0
   29194:	cset	w8, eq  // eq = none
   29198:	mov	x0, x28
   2919c:	mov	x1, x21
   291a0:	mov	x2, x23
   291a4:	mov	x3, x26
   291a8:	stur	w8, [x29, #-36]
   291ac:	bl	c2d0 <__gmpn_sub_n@plt>
   291b0:	sub	x8, x24, x0
   291b4:	add	x8, x8, #0x1
   291b8:	str	x8, [x28, x27]
   291bc:	cbz	w20, 29240 <__gmpn_fib2m@@Base+0x384>
   291c0:	ldr	x10, [x23]
   291c4:	ldur	x24, [x29, #-56]
   291c8:	mov	x8, x23
   291cc:	mov	x9, x26
   291d0:	mov	x28, x26
   291d4:	cbnz	x10, 291f4 <__gmpn_fib2m@@Base+0x338>
   291d8:	mov	x9, x28
   291dc:	mov	x8, x23
   291e0:	subs	x9, x9, #0x1
   291e4:	str	xzr, [x8]
   291e8:	b.eq	29248 <__gmpn_fib2m@@Base+0x38c>  // b.none
   291ec:	ldr	x10, [x8, #8]!
   291f0:	cbz	x10, 291e0 <__gmpn_fib2m@@Base+0x324>
   291f4:	neg	x10, x10
   291f8:	subs	x2, x9, #0x1
   291fc:	str	x10, [x8]
   29200:	b.eq	29210 <__gmpn_fib2m@@Base+0x354>  // b.none
   29204:	add	x0, x8, #0x8
   29208:	mov	x1, x0
   2920c:	bl	c290 <__gmpn_com@plt>
   29210:	mov	x8, xzr
   29214:	str	xzr, [x23, x28, lsl #3]
   29218:	b	29040 <__gmpn_fib2m@@Base+0x184>
   2921c:	add	x3, x8, #0x1
   29220:	mov	x0, x28
   29224:	b.hi	2902c <__gmpn_fib2m@@Base+0x170>  // b.pmore
   29228:	mov	x1, x21
   2922c:	mov	x2, x23
   29230:	bl	c2d0 <__gmpn_sub_n@plt>
   29234:	mov	w8, #0x1                   	// #1
   29238:	stur	w8, [x29, #-36]
   2923c:	b	2903c <__gmpn_fib2m@@Base+0x180>
   29240:	ldur	x24, [x29, #-56]
   29244:	b	2903c <__gmpn_fib2m@@Base+0x180>
   29248:	mov	w8, #0x1                   	// #1
   2924c:	str	x8, [x23, x28, lsl #3]
   29250:	b	29040 <__gmpn_fib2m@@Base+0x184>
   29254:	ldur	x0, [x29, #-16]
   29258:	cbnz	x0, 292e4 <__gmpn_fib2m@@Base+0x428>
   2925c:	ldur	w0, [x29, #-36]
   29260:	b	292b4 <__gmpn_fib2m@@Base+0x3f8>
   29264:	cmp	x25, x19
   29268:	b.ne	292b0 <__gmpn_fib2m@@Base+0x3f4>  // b.any
   2926c:	ldur	x20, [x29, #-56]
   29270:	sub	x0, x29, #0x10
   29274:	mov	x1, x23
   29278:	mov	x2, xzr
   2927c:	mov	x3, x23
   29280:	mov	x4, x19
   29284:	mov	x5, x20
   29288:	mov	x6, x19
   2928c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   29290:	sub	x0, x29, #0x10
   29294:	mov	x1, x21
   29298:	mov	x2, xzr
   2929c:	mov	x3, x21
   292a0:	mov	x4, x19
   292a4:	mov	x5, x20
   292a8:	mov	x6, x19
   292ac:	bl	bf00 <__gmpn_tdiv_qr@plt>
   292b0:	mov	w0, wzr
   292b4:	mov	sp, x29
   292b8:	ldp	x20, x19, [sp, #80]
   292bc:	ldp	x22, x21, [sp, #64]
   292c0:	ldp	x24, x23, [sp, #48]
   292c4:	ldp	x26, x25, [sp, #32]
   292c8:	ldp	x28, x27, [sp, #16]
   292cc:	ldp	x29, x30, [sp], #96
   292d0:	ret
   292d4:	sub	x0, x29, #0x10
   292d8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   292dc:	mov	x25, x0
   292e0:	b	29008 <__gmpn_fib2m@@Base+0x14c>
   292e4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   292e8:	b	2925c <__gmpn_fib2m@@Base+0x3a0>

00000000000292ec <__gmpn_mod_1@@Base>:
   292ec:	sub	sp, sp, #0x90
   292f0:	stp	x29, x30, [sp, #64]
   292f4:	str	x25, [sp, #80]
   292f8:	stp	x24, x23, [sp, #96]
   292fc:	stp	x22, x21, [sp, #112]
   29300:	stp	x20, x19, [sp, #128]
   29304:	add	x29, sp, #0x40
   29308:	cbz	x1, 29364 <__gmpn_mod_1@@Base+0x78>
   2930c:	mov	x20, x2
   29310:	mov	x19, x1
   29314:	mov	x21, x0
   29318:	tbnz	x2, #63, 295d0 <__gmpn_mod_1@@Base+0x2e4>
   2931c:	cmp	x19, #0x5
   29320:	b.le	2936c <__gmpn_mod_1@@Base+0x80>
   29324:	cmp	x19, #0x9
   29328:	b.le	29390 <__gmpn_mod_1@@Base+0xa4>
   2932c:	cmp	x19, #0x14
   29330:	b.lt	29590 <__gmpn_mod_1@@Base+0x2a4>  // b.tstop
   29334:	lsr	x8, x20, #62
   29338:	cbnz	x8, 29590 <__gmpn_mod_1@@Base+0x2a4>
   2933c:	add	x0, sp, #0x8
   29340:	mov	x1, x20
   29344:	bl	c680 <__gmpn_mod_1s_4p_cps@plt>
   29348:	ldr	x8, [sp, #16]
   2934c:	add	x3, sp, #0x8
   29350:	mov	x0, x21
   29354:	mov	x1, x19
   29358:	lsl	x2, x20, x8
   2935c:	bl	d410 <__gmpn_mod_1s_4p@plt>
   29360:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   29364:	mov	x0, xzr
   29368:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   2936c:	sub	x8, x19, #0x1
   29370:	ldr	x0, [x21, x8, lsl #3]
   29374:	cmp	x0, x20
   29378:	b.cs	293b8 <__gmpn_mod_1@@Base+0xcc>  // b.hs, b.nlast
   2937c:	cbz	x8, 295b4 <__gmpn_mod_1@@Base+0x2c8>
   29380:	add	x9, x21, x19, lsl #3
   29384:	ldur	x23, [x9, #-16]
   29388:	mov	x19, x8
   2938c:	b	293c0 <__gmpn_mod_1@@Base+0xd4>
   29390:	add	x0, sp, #0x8
   29394:	mov	x1, x20
   29398:	bl	cac0 <__gmpn_mod_1_1p_cps@plt>
   2939c:	ldr	x8, [sp, #16]
   293a0:	add	x3, sp, #0x8
   293a4:	mov	x0, x21
   293a8:	mov	x1, x19
   293ac:	lsl	x2, x20, x8
   293b0:	bl	c250 <__gmpn_mod_1_1p@plt>
   293b4:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   293b8:	mov	x23, x0
   293bc:	mov	x0, xzr
   293c0:	clz	x22, x20
   293c4:	mov	w8, #0x40                  	// #64
   293c8:	sub	x24, x8, x22
   293cc:	neg	x8, x22
   293d0:	lsl	x9, x0, x22
   293d4:	lsr	x8, x23, x8
   293d8:	lsl	x20, x20, x22
   293dc:	cmp	x19, #0x3
   293e0:	orr	x25, x9, x8
   293e4:	b.le	29484 <__gmpn_mod_1@@Base+0x198>
   293e8:	mov	x0, x20
   293ec:	bl	d3f0 <__gmpn_invert_limb@plt>
   293f0:	sub	x8, x21, #0x10
   293f4:	ldr	x9, [x8, x19, lsl #3]
   293f8:	lsl	x10, x23, x22
   293fc:	umulh	x11, x25, x0
   29400:	add	x13, x25, #0x1
   29404:	lsr	x12, x9, x24
   29408:	orr	x10, x12, x10
   2940c:	mul	x12, x25, x0
   29410:	adds	x14, x12, x10
   29414:	adc	x11, x11, x13
   29418:	msub	x10, x11, x20, x10
   2941c:	cmp	x10, x14
   29420:	csel	x11, x20, xzr, hi  // hi = pmore
   29424:	add	x10, x11, x10
   29428:	cmp	x10, x20
   2942c:	sub	x12, x19, #0x2
   29430:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   29434:	sub	x19, x19, #0x1
   29438:	cmp	x12, #0x0
   2943c:	sub	x25, x10, x11
   29440:	mov	x23, x9
   29444:	b.gt	293f4 <__gmpn_mod_1@@Base+0x108>
   29448:	umulh	x8, x25, x0
   2944c:	mul	x10, x25, x0
   29450:	lsl	x9, x9, x22
   29454:	add	x11, x25, #0x1
   29458:	adds	x12, x10, x9
   2945c:	adc	x8, x8, x11
   29460:	msub	x8, x8, x20, x9
   29464:	cmp	x8, x12
   29468:	csel	x9, x20, xzr, hi  // hi = pmore
   2946c:	add	x8, x9, x8
   29470:	cmp	x8, x20
   29474:	csel	x9, xzr, x20, cc  // cc = lo, ul, last
   29478:	sub	x8, x8, x9
   2947c:	lsr	x0, x8, x22
   29480:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   29484:	lsr	x8, x20, #32
   29488:	and	x9, x20, #0xffffffff
   2948c:	cmp	x19, #0x1
   29490:	b.le	29524 <__gmpn_mod_1@@Base+0x238>
   29494:	sub	x10, x21, #0x10
   29498:	b	294b0 <__gmpn_mod_1@@Base+0x1c4>
   2949c:	sub	x13, x19, #0x2
   294a0:	sub	x19, x19, #0x1
   294a4:	cmp	x13, #0x0
   294a8:	sub	x25, x11, x12
   294ac:	b.le	29524 <__gmpn_mod_1@@Base+0x238>
   294b0:	mov	x11, x23
   294b4:	ldr	x23, [x10, x19, lsl #3]
   294b8:	lsl	x11, x11, x22
   294bc:	udiv	x12, x25, x8
   294c0:	msub	w13, w12, w8, w25
   294c4:	lsr	x14, x23, x24
   294c8:	orr	x11, x14, x11
   294cc:	mul	x12, x12, x9
   294d0:	extr	x13, x13, x11, #32
   294d4:	cmp	x13, x12
   294d8:	b.cs	294f0 <__gmpn_mod_1@@Base+0x204>  // b.hs, b.nlast
   294dc:	add	x13, x13, x20
   294e0:	cmp	x13, x12
   294e4:	ccmp	x13, x20, #0x0, cc  // cc = lo, ul, last
   294e8:	csel	x14, x20, xzr, cs  // cs = hs, nlast
   294ec:	add	x13, x14, x13
   294f0:	sub	x12, x13, x12
   294f4:	udiv	x13, x12, x8
   294f8:	msub	w14, w13, w8, w12
   294fc:	mul	x12, x13, x9
   29500:	bfi	x11, x14, #32, #32
   29504:	cmp	x11, x12
   29508:	b.cs	2949c <__gmpn_mod_1@@Base+0x1b0>  // b.hs, b.nlast
   2950c:	add	x11, x11, x20
   29510:	cmp	x11, x12
   29514:	ccmp	x11, x20, #0x0, cc  // cc = lo, ul, last
   29518:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   2951c:	add	x11, x13, x11
   29520:	b	2949c <__gmpn_mod_1@@Base+0x1b0>
   29524:	udiv	x10, x25, x8
   29528:	msub	w12, w10, w8, w25
   2952c:	mul	x11, x10, x9
   29530:	lsl	x10, x23, x22
   29534:	extr	x12, x12, x10, #32
   29538:	cmp	x12, x11
   2953c:	b.cs	29554 <__gmpn_mod_1@@Base+0x268>  // b.hs, b.nlast
   29540:	add	x12, x12, x20
   29544:	cmp	x12, x11
   29548:	ccmp	x12, x20, #0x0, cc  // cc = lo, ul, last
   2954c:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   29550:	add	x12, x13, x12
   29554:	sub	x11, x12, x11
   29558:	udiv	x12, x11, x8
   2955c:	msub	w11, w12, w8, w11
   29560:	mul	x8, x12, x9
   29564:	bfi	x10, x11, #32, #32
   29568:	cmp	x10, x8
   2956c:	b.cs	29584 <__gmpn_mod_1@@Base+0x298>  // b.hs, b.nlast
   29570:	add	x9, x10, x20
   29574:	cmp	x9, x8
   29578:	ccmp	x9, x20, #0x0, cc  // cc = lo, ul, last
   2957c:	csel	x10, x20, xzr, cs  // cs = hs, nlast
   29580:	add	x10, x10, x9
   29584:	sub	x8, x10, x8
   29588:	lsr	x0, x8, x22
   2958c:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   29590:	add	x0, sp, #0x8
   29594:	mov	x1, x20
   29598:	bl	ccc0 <__gmpn_mod_1s_2p_cps@plt>
   2959c:	ldr	x8, [sp, #16]
   295a0:	add	x3, sp, #0x8
   295a4:	mov	x0, x21
   295a8:	mov	x1, x19
   295ac:	lsl	x2, x20, x8
   295b0:	bl	ce80 <__gmpn_mod_1s_2p@plt>
   295b4:	ldp	x20, x19, [sp, #128]
   295b8:	ldp	x22, x21, [sp, #112]
   295bc:	ldp	x24, x23, [sp, #96]
   295c0:	ldr	x25, [sp, #80]
   295c4:	ldp	x29, x30, [sp, #64]
   295c8:	add	sp, sp, #0x90
   295cc:	ret
   295d0:	cmp	x19, #0x7
   295d4:	b.le	295fc <__gmpn_mod_1@@Base+0x310>
   295d8:	add	x0, sp, #0x8
   295dc:	mov	x1, x20
   295e0:	bl	cac0 <__gmpn_mod_1_1p_cps@plt>
   295e4:	add	x3, sp, #0x8
   295e8:	mov	x0, x21
   295ec:	mov	x1, x19
   295f0:	mov	x2, x20
   295f4:	bl	c250 <__gmpn_mod_1_1p@plt>
   295f8:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   295fc:	add	x8, x21, x19, lsl #3
   29600:	ldur	x8, [x8, #-8]
   29604:	cmp	x8, x20
   29608:	csel	x9, xzr, x20, cc  // cc = lo, ul, last
   2960c:	cmp	x19, #0x1
   29610:	sub	x0, x8, x9
   29614:	b.eq	295b4 <__gmpn_mod_1@@Base+0x2c8>  // b.none
   29618:	mov	x22, x0
   2961c:	cmp	x19, #0x3
   29620:	b.le	29680 <__gmpn_mod_1@@Base+0x394>
   29624:	mov	x0, x20
   29628:	bl	d3f0 <__gmpn_invert_limb@plt>
   2962c:	mov	x8, x0
   29630:	sub	x9, x21, #0x10
   29634:	mov	x0, x22
   29638:	ldr	x10, [x9, x19, lsl #3]
   2963c:	umulh	x11, x0, x8
   29640:	mul	x12, x0, x8
   29644:	add	x13, x0, #0x1
   29648:	adds	x14, x12, x10
   2964c:	adc	x11, x11, x13
   29650:	msub	x10, x11, x20, x10
   29654:	cmp	x10, x14
   29658:	csel	x11, x20, xzr, hi  // hi = pmore
   2965c:	add	x10, x11, x10
   29660:	cmp	x10, x20
   29664:	sub	x12, x19, #0x2
   29668:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   2966c:	sub	x19, x19, #0x1
   29670:	cmp	x12, #0x0
   29674:	sub	x0, x10, x11
   29678:	b.gt	29638 <__gmpn_mod_1@@Base+0x34c>
   2967c:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   29680:	cmp	x19, #0x2
   29684:	b.lt	29714 <__gmpn_mod_1@@Base+0x428>  // b.tstop
   29688:	lsr	x8, x20, #32
   2968c:	and	x9, x20, #0xffffffff
   29690:	sub	x10, x21, #0x10
   29694:	mov	x0, x22
   29698:	b	296b0 <__gmpn_mod_1@@Base+0x3c4>
   2969c:	sub	x13, x19, #0x2
   296a0:	sub	x19, x19, #0x1
   296a4:	cmp	x13, #0x0
   296a8:	sub	x0, x11, x12
   296ac:	b.le	295b4 <__gmpn_mod_1@@Base+0x2c8>
   296b0:	ldr	x11, [x10, x19, lsl #3]
   296b4:	udiv	x12, x0, x8
   296b8:	msub	w13, w12, w8, w0
   296bc:	mul	x12, x12, x9
   296c0:	extr	x13, x13, x11, #32
   296c4:	cmp	x13, x12
   296c8:	b.cs	296e0 <__gmpn_mod_1@@Base+0x3f4>  // b.hs, b.nlast
   296cc:	add	x13, x13, x20
   296d0:	cmp	x13, x12
   296d4:	ccmp	x13, x20, #0x0, cc  // cc = lo, ul, last
   296d8:	csel	x14, x20, xzr, cs  // cs = hs, nlast
   296dc:	add	x13, x14, x13
   296e0:	sub	x12, x13, x12
   296e4:	udiv	x13, x12, x8
   296e8:	msub	w14, w13, w8, w12
   296ec:	mul	x12, x13, x9
   296f0:	bfi	x11, x14, #32, #32
   296f4:	cmp	x11, x12
   296f8:	b.cs	2969c <__gmpn_mod_1@@Base+0x3b0>  // b.hs, b.nlast
   296fc:	add	x11, x11, x20
   29700:	cmp	x11, x12
   29704:	ccmp	x11, x20, #0x0, cc  // cc = lo, ul, last
   29708:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   2970c:	add	x11, x13, x11
   29710:	b	2969c <__gmpn_mod_1@@Base+0x3b0>
   29714:	mov	x0, x22
   29718:	b	295b4 <__gmpn_mod_1@@Base+0x2c8>
   2971c:	nop

0000000000029720 <__gmpn_mod_34lsub1@@Base>:
   29720:	subs	x1, x1, #0x3
   29724:	mov	x8, #0x0                   	// #0
   29728:	b.lt	297c4 <__gmpn_mod_34lsub1@@Base+0xa4>  // b.tstop
   2972c:	ldp	x2, x3, [x0]
   29730:	ldr	x4, [x0, #16]
   29734:	add	x0, x0, #0x18
   29738:	subs	x1, x1, #0x3
   2973c:	b.lt	29768 <__gmpn_mod_34lsub1@@Base+0x48>  // b.tstop
   29740:	cmn	x0, #0x0
   29744:	ldp	x5, x6, [x0]
   29748:	ldr	x7, [x0, #16]
   2974c:	add	x0, x0, #0x18
   29750:	sub	x1, x1, #0x3
   29754:	adcs	x2, x2, x5
   29758:	adcs	x3, x3, x6
   2975c:	adcs	x4, x4, x7
   29760:	tbz	x1, #63, 29744 <__gmpn_mod_34lsub1@@Base+0x24>
   29764:	adc	x8, xzr, xzr
   29768:	cmn	x1, #0x2
   2976c:	mov	x5, #0x0                   	// #0
   29770:	b.cc	29778 <__gmpn_mod_34lsub1@@Base+0x58>  // b.lo, b.ul, b.last
   29774:	ldr	x5, [x0], #8
   29778:	mov	x6, #0x0                   	// #0
   2977c:	b.ls	29784 <__gmpn_mod_34lsub1@@Base+0x64>  // b.plast
   29780:	ldr	x6, [x0], #8
   29784:	adds	x2, x2, x5
   29788:	adcs	x3, x3, x6
   2978c:	adcs	x4, x4, xzr
   29790:	adc	x8, x8, xzr
   29794:	and	x0, x2, #0xffffffffffff
   29798:	add	x0, x0, x2, lsr #48
   2979c:	add	x0, x0, x8
   297a0:	lsl	x8, x3, #16
   297a4:	and	x1, x8, #0xffffffffffff
   297a8:	add	x0, x0, x1
   297ac:	add	x0, x0, x3, lsr #32
   297b0:	lsl	x8, x4, #32
   297b4:	and	x1, x8, #0xffffffffffff
   297b8:	add	x0, x0, x1
   297bc:	add	x0, x0, x4, lsr #16
   297c0:	ret
   297c4:	cmn	x1, #0x1
   297c8:	b.ne	297d8 <__gmpn_mod_34lsub1@@Base+0xb8>  // b.any
   297cc:	ldp	x2, x3, [x0]
   297d0:	mov	x4, #0x0                   	// #0
   297d4:	b	29794 <__gmpn_mod_34lsub1@@Base+0x74>
   297d8:	ldr	x2, [x0]
   297dc:	and	x0, x2, #0xffffffffffff
   297e0:	add	x0, x0, x2, lsr #48
   297e4:	ret

00000000000297e8 <__gmpn_modexact_1c_odd@@Base>:
   297e8:	subs	x8, x1, #0x1
   297ec:	b.ne	29814 <__gmpn_modexact_1c_odd@@Base+0x2c>  // b.any
   297f0:	ldr	x8, [x0]
   297f4:	subs	x9, x8, x3
   297f8:	b.ls	29884 <__gmpn_modexact_1c_odd@@Base+0x9c>  // b.plast
   297fc:	udiv	x8, x9, x2
   29800:	msub	x8, x8, x2, x9
   29804:	sub	x9, x2, x8
   29808:	cmp	x8, #0x0
   2980c:	csel	x0, xzr, x9, eq  // eq = none
   29810:	ret
   29814:	adrp	x11, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   29818:	ldr	x11, [x11, #3952]
   2981c:	ubfx	x10, x2, #1, #7
   29820:	mov	x9, xzr
   29824:	ldrb	w10, [x11, x10]
   29828:	mov	w11, #0x2                   	// #2
   2982c:	msub	x12, x10, x2, x11
   29830:	mul	x10, x12, x10
   29834:	msub	x12, x10, x2, x11
   29838:	mul	x10, x10, x12
   2983c:	msub	x11, x10, x2, x11
   29840:	mul	x10, x10, x11
   29844:	ldr	x11, [x0, x9, lsl #3]
   29848:	add	x9, x9, #0x1
   2984c:	subs	x11, x11, x3
   29850:	mul	x11, x11, x10
   29854:	umulh	x11, x11, x2
   29858:	cinc	x3, x11, cc  // cc = lo, ul, last
   2985c:	cmp	x9, x8
   29860:	b.lt	29844 <__gmpn_modexact_1c_odd@@Base+0x5c>  // b.tstop
   29864:	ldr	x8, [x0, x9, lsl #3]
   29868:	cmp	x8, x2
   2986c:	b.ls	29894 <__gmpn_modexact_1c_odd@@Base+0xac>  // b.plast
   29870:	subs	x8, x8, x3
   29874:	mul	x8, x8, x10
   29878:	umulh	x8, x8, x2
   2987c:	cinc	x0, x8, cc  // cc = lo, ul, last
   29880:	ret
   29884:	sub	x8, x3, x8
   29888:	udiv	x9, x8, x2
   2988c:	msub	x0, x9, x2, x8
   29890:	ret
   29894:	subs	x8, x3, x8
   29898:	csel	x9, x2, xzr, cc  // cc = lo, ul, last
   2989c:	add	x0, x8, x9
   298a0:	ret

00000000000298a4 <__gmpn_preinv_divrem_1@@Base>:
   298a4:	sub	x13, x3, #0x1
   298a8:	ldr	x12, [x2, x13, lsl #3]
   298ac:	add	x10, x13, x1
   298b0:	mov	w8, w6
   298b4:	lsl	x9, x4, x6
   298b8:	add	x10, x0, x10, lsl #3
   298bc:	cbz	w6, 298e4 <__gmpn_preinv_divrem_1@@Base+0x40>
   298c0:	cmp	x12, x4
   298c4:	b.cs	2995c <__gmpn_preinv_divrem_1@@Base+0xb8>  // b.hs, b.nlast
   298c8:	lsl	x11, x12, x8
   298cc:	str	xzr, [x10], #-8
   298d0:	cbz	x13, 29a30 <__gmpn_preinv_divrem_1@@Base+0x18c>
   298d4:	add	x12, x2, x3, lsl #3
   298d8:	ldur	x12, [x12, #-16]
   298dc:	mov	x3, x13
   298e0:	b	29960 <__gmpn_preinv_divrem_1@@Base+0xbc>
   298e4:	cmp	x12, x9
   298e8:	cset	w13, cs  // cs = hs, nlast
   298ec:	csel	x11, x9, xzr, cs  // cs = hs, nlast
   298f0:	cmp	x3, #0x1
   298f4:	sub	x11, x12, x11
   298f8:	str	x13, [x10], #-8
   298fc:	b.le	29a30 <__gmpn_preinv_divrem_1@@Base+0x18c>
   29900:	sub	x12, x2, #0x10
   29904:	ldr	x13, [x12, x3, lsl #3]
   29908:	umulh	x14, x11, x5
   2990c:	mul	x15, x11, x5
   29910:	add	x11, x11, #0x1
   29914:	adds	x16, x15, x13
   29918:	adc	x11, x14, x11
   2991c:	msub	x13, x11, x9, x13
   29920:	cmp	x13, x16
   29924:	cset	w15, hi  // hi = pmore
   29928:	sub	x11, x11, x15
   2992c:	csel	x15, x9, xzr, hi  // hi = pmore
   29930:	add	x13, x15, x13
   29934:	cmp	x13, x9
   29938:	sub	x14, x3, #0x2
   2993c:	csel	x15, xzr, x9, cc  // cc = lo, ul, last
   29940:	cinc	x16, x11, cs  // cs = hs, nlast
   29944:	sub	x3, x3, #0x1
   29948:	cmp	x14, #0x0
   2994c:	sub	x11, x13, x15
   29950:	str	x16, [x10], #-8
   29954:	b.gt	29904 <__gmpn_preinv_divrem_1@@Base+0x60>
   29958:	b	29a30 <__gmpn_preinv_divrem_1@@Base+0x18c>
   2995c:	mov	x11, xzr
   29960:	neg	w13, w6
   29964:	lsr	x13, x12, x13
   29968:	cmp	x3, #0x2
   2996c:	orr	x15, x13, x11
   29970:	b.lt	299e8 <__gmpn_preinv_divrem_1@@Base+0x144>  // b.tstop
   29974:	mov	w11, #0x40                  	// #64
   29978:	sub	w11, w11, w6
   2997c:	sub	x13, x2, #0x10
   29980:	ldr	x14, [x13, x3, lsl #3]
   29984:	lsl	x12, x12, x8
   29988:	umulh	x16, x15, x5
   2998c:	mul	x17, x15, x5
   29990:	lsr	x18, x14, x11
   29994:	add	x15, x15, #0x1
   29998:	orr	x12, x18, x12
   2999c:	adds	x0, x17, x12
   299a0:	adc	x15, x16, x15
   299a4:	msub	x12, x15, x9, x12
   299a8:	cmp	x12, x0
   299ac:	csel	x17, x9, xzr, hi  // hi = pmore
   299b0:	cset	w16, hi  // hi = pmore
   299b4:	add	x12, x17, x12
   299b8:	sub	x15, x15, x16
   299bc:	cmp	x12, x9
   299c0:	sub	x18, x3, #0x2
   299c4:	cinc	x16, x15, cs  // cs = hs, nlast
   299c8:	csel	x15, xzr, x9, cc  // cc = lo, ul, last
   299cc:	sub	x3, x3, #0x1
   299d0:	cmp	x18, #0x0
   299d4:	sub	x15, x12, x15
   299d8:	str	x16, [x10], #-8
   299dc:	mov	x12, x14
   299e0:	b.gt	29980 <__gmpn_preinv_divrem_1@@Base+0xdc>
   299e4:	b	299ec <__gmpn_preinv_divrem_1@@Base+0x148>
   299e8:	mov	x14, x12
   299ec:	umulh	x11, x15, x5
   299f0:	mul	x12, x15, x5
   299f4:	lsl	x13, x14, x8
   299f8:	add	x14, x15, #0x1
   299fc:	adds	x15, x12, x13
   29a00:	adc	x11, x11, x14
   29a04:	msub	x12, x11, x9, x13
   29a08:	cmp	x12, x15
   29a0c:	csel	x14, x9, xzr, hi  // hi = pmore
   29a10:	cset	w13, hi  // hi = pmore
   29a14:	add	x12, x14, x12
   29a18:	sub	x11, x11, x13
   29a1c:	cmp	x12, x9
   29a20:	cinc	x13, x11, cs  // cs = hs, nlast
   29a24:	csel	x11, xzr, x9, cc  // cc = lo, ul, last
   29a28:	sub	x11, x12, x11
   29a2c:	str	x13, [x10], #-8
   29a30:	cmp	x1, #0x1
   29a34:	b.lt	29a6c <__gmpn_preinv_divrem_1@@Base+0x1c8>  // b.tstop
   29a38:	umulh	x12, x11, x5
   29a3c:	mul	x13, x11, x5
   29a40:	add	x11, x11, x12
   29a44:	add	x11, x11, #0x1
   29a48:	mneg	x12, x11, x9
   29a4c:	cmp	x13, x12
   29a50:	cset	w12, cc  // cc = lo, ul, last
   29a54:	csel	x13, x9, xzr, cc  // cc = lo, ul, last
   29a58:	sub	x12, x11, x12
   29a5c:	msub	x11, x11, x9, x13
   29a60:	subs	x1, x1, #0x1
   29a64:	str	x12, [x10], #-8
   29a68:	b.ne	29a38 <__gmpn_preinv_divrem_1@@Base+0x194>  // b.any
   29a6c:	lsr	x0, x11, x8
   29a70:	ret

0000000000029a74 <__gmpn_preinv_mod_1@@Base>:
   29a74:	add	x9, x0, x1, lsl #3
   29a78:	ldur	x9, [x9, #-8]
   29a7c:	mov	x8, x0
   29a80:	cmp	x9, x2
   29a84:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29a88:	cmp	x1, #0x2
   29a8c:	sub	x0, x9, x10
   29a90:	b.lt	29adc <__gmpn_preinv_mod_1@@Base+0x68>  // b.tstop
   29a94:	sub	x8, x8, #0x10
   29a98:	ldr	x9, [x8, x1, lsl #3]
   29a9c:	umulh	x10, x0, x3
   29aa0:	mul	x11, x0, x3
   29aa4:	add	x12, x0, #0x1
   29aa8:	adds	x13, x11, x9
   29aac:	adc	x10, x10, x12
   29ab0:	msub	x9, x10, x2, x9
   29ab4:	cmp	x9, x13
   29ab8:	csel	x10, x2, xzr, hi  // hi = pmore
   29abc:	add	x9, x10, x9
   29ac0:	cmp	x9, x2
   29ac4:	sub	x11, x1, #0x2
   29ac8:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29acc:	sub	x1, x1, #0x1
   29ad0:	cmp	x11, #0x0
   29ad4:	sub	x0, x9, x10
   29ad8:	b.gt	29a98 <__gmpn_preinv_mod_1@@Base+0x24>
   29adc:	ret

0000000000029ae0 <__gmpn_dump@@Base>:
   29ae0:	stp	x29, x30, [sp, #-48]!
   29ae4:	stp	x20, x19, [sp, #32]
   29ae8:	sub	x20, x0, #0x8
   29aec:	stp	x22, x21, [sp, #16]
   29af0:	mov	x29, sp
   29af4:	subs	x21, x1, #0x1
   29af8:	b.lt	29b20 <__gmpn_dump@@Base+0x40>  // b.tstop
   29afc:	ldr	x8, [x20, x1, lsl #3]
   29b00:	mov	x1, x21
   29b04:	cbz	x8, 29af4 <__gmpn_dump@@Base+0x14>
   29b08:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   29b0c:	add	x0, x0, #0xd70
   29b10:	mov	x1, x8
   29b14:	bl	d2e0 <printf@plt>
   29b18:	cbnz	x21, 29b38 <__gmpn_dump@@Base+0x58>
   29b1c:	b	29b5c <__gmpn_dump@@Base+0x7c>
   29b20:	cbz	x1, 29b70 <__gmpn_dump@@Base+0x90>
   29b24:	add	x8, x0, x1, lsl #3
   29b28:	ldur	x1, [x8, #-8]
   29b2c:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   29b30:	add	x0, x0, #0xd70
   29b34:	bl	d2e0 <printf@plt>
   29b38:	adrp	x19, 5c000 <__gmpn_bases@@Base+0x1ab8>
   29b3c:	add	x19, x19, #0xd74
   29b40:	ldr	x2, [x20, x21, lsl #3]
   29b44:	mov	w1, #0x10                  	// #16
   29b48:	mov	x0, x19
   29b4c:	sub	x22, x21, #0x1
   29b50:	bl	d2e0 <printf@plt>
   29b54:	mov	x21, x22
   29b58:	cbnz	x22, 29b40 <__gmpn_dump@@Base+0x60>
   29b5c:	ldp	x20, x19, [sp, #32]
   29b60:	ldp	x22, x21, [sp, #16]
   29b64:	mov	w0, #0xa                   	// #10
   29b68:	ldp	x29, x30, [sp], #48
   29b6c:	b	d310 <putchar@plt>
   29b70:	ldp	x20, x19, [sp, #32]
   29b74:	ldp	x22, x21, [sp, #16]
   29b78:	adrp	x0, 65000 <__gmp_oddfac_table@@Base+0xf0>
   29b7c:	add	x0, x0, #0x8a3
   29b80:	ldp	x29, x30, [sp], #48
   29b84:	b	c9b0 <puts@plt>

0000000000029b88 <__gmpn_mod_1_1p_cps@@Base>:
   29b88:	stp	x29, x30, [sp, #-48]!
   29b8c:	str	x21, [sp, #16]
   29b90:	clz	x21, x1
   29b94:	stp	x20, x19, [sp, #32]
   29b98:	lsl	x20, x1, x21
   29b9c:	mov	x19, x0
   29ba0:	mov	x0, x20
   29ba4:	mov	x29, sp
   29ba8:	bl	d3f0 <__gmpn_invert_limb@plt>
   29bac:	stp	x0, x21, [x19]
   29bb0:	cbz	x21, 29bd4 <__gmpn_mod_1_1p_cps@@Base+0x4c>
   29bb4:	neg	x8, x21
   29bb8:	mov	w9, #0x1                   	// #1
   29bbc:	lsr	x8, x0, x8
   29bc0:	lsl	x9, x9, x21
   29bc4:	orr	x8, x8, x9
   29bc8:	mneg	x8, x20, x8
   29bcc:	lsr	x8, x8, x21
   29bd0:	str	x8, [x19, #16]
   29bd4:	mneg	x8, x20, x0
   29bd8:	str	x8, [x19, #24]
   29bdc:	ldp	x20, x19, [sp, #32]
   29be0:	ldr	x21, [sp, #16]
   29be4:	ldp	x29, x30, [sp], #48
   29be8:	ret

0000000000029bec <__gmpn_mod_1_1p@@Base>:
   29bec:	add	x10, x0, x1, lsl #3
   29bf0:	ldp	x9, x11, [x10, #-16]
   29bf4:	cmp	x1, #0x3
   29bf8:	b.lt	29c98 <__gmpn_mod_1_1p@@Base+0xac>  // b.tstop
   29bfc:	ldr	x8, [x3, #24]
   29c00:	ldur	x10, [x10, #-24]
   29c04:	umulh	x13, x11, x8
   29c08:	mov	w12, #0x2                   	// #2
   29c0c:	mul	x11, x8, x11
   29c10:	adds	x10, x10, x11
   29c14:	cset	w11, cs  // cs = hs, nlast
   29c18:	adds	x9, x13, x9
   29c1c:	cset	w13, cs  // cs = hs, nlast
   29c20:	csinc	x12, x12, xzr, cs  // cs = hs, nlast
   29c24:	adds	x9, x9, x11
   29c28:	csel	x11, x13, x12, cc  // cc = lo, ul, last
   29c2c:	cmp	x1, #0x3
   29c30:	neg	x13, x11
   29c34:	b.eq	29c8c <__gmpn_mod_1_1p@@Base+0xa0>  // b.none
   29c38:	sub	x11, x0, #0x20
   29c3c:	mov	w12, #0x2                   	// #2
   29c40:	ldr	x15, [x11, x1, lsl #3]
   29c44:	and	x13, x13, x8
   29c48:	adds	x10, x10, x13
   29c4c:	umulh	x14, x9, x8
   29c50:	mul	x9, x9, x8
   29c54:	csel	x13, x2, xzr, cs  // cs = hs, nlast
   29c58:	sub	x13, x10, x13
   29c5c:	adds	x10, x15, x9
   29c60:	cset	w9, cs  // cs = hs, nlast
   29c64:	adds	x13, x14, x13
   29c68:	cset	w14, cs  // cs = hs, nlast
   29c6c:	csinc	x15, x12, xzr, cs  // cs = hs, nlast
   29c70:	adds	x9, x13, x9
   29c74:	sub	x13, x1, #0x4
   29c78:	csel	x14, x14, x15, cc  // cc = lo, ul, last
   29c7c:	sub	x1, x1, #0x1
   29c80:	cmp	x13, #0x0
   29c84:	neg	x13, x14
   29c88:	b.gt	29c40 <__gmpn_mod_1_1p@@Base+0x54>
   29c8c:	and	x8, x13, x2
   29c90:	sub	x11, x9, x8
   29c94:	mov	x9, x10
   29c98:	ldr	x8, [x3, #8]
   29c9c:	cbz	w8, 29d04 <__gmpn_mod_1_1p@@Base+0x118>
   29ca0:	ldr	x10, [x3, #16]
   29ca4:	umulh	x13, x11, x10
   29ca8:	neg	w12, w8
   29cac:	mul	x10, x10, x11
   29cb0:	adds	x9, x10, x9
   29cb4:	cinc	x10, x13, cs  // cs = hs, nlast
   29cb8:	lsr	x11, x9, x12
   29cbc:	lsl	x10, x10, x8
   29cc0:	orr	x10, x10, x11
   29cc4:	lsl	x9, x9, x8
   29cc8:	ldr	x11, [x3]
   29ccc:	umulh	x12, x10, x11
   29cd0:	mul	x11, x11, x10
   29cd4:	add	x10, x10, #0x1
   29cd8:	adds	x13, x11, x9
   29cdc:	adc	x10, x12, x10
   29ce0:	msub	x9, x10, x2, x9
   29ce4:	cmp	x9, x13
   29ce8:	csel	x10, x2, xzr, hi  // hi = pmore
   29cec:	add	x9, x10, x9
   29cf0:	cmp	x9, x2
   29cf4:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29cf8:	sub	x9, x9, x10
   29cfc:	lsr	x0, x9, x8
   29d00:	ret
   29d04:	cmp	x11, x2
   29d08:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29d0c:	sub	x10, x11, x10
   29d10:	b	29cc8 <__gmpn_mod_1_1p@@Base+0xdc>

0000000000029d14 <__gmpn_mod_1s_2p_cps@@Base>:
   29d14:	stp	x29, x30, [sp, #-48]!
   29d18:	str	x21, [sp, #16]
   29d1c:	clz	x21, x1
   29d20:	stp	x20, x19, [sp, #32]
   29d24:	lsl	x20, x1, x21
   29d28:	mov	x19, x0
   29d2c:	mov	x0, x20
   29d30:	mov	x29, sp
   29d34:	bl	d3f0 <__gmpn_invert_limb@plt>
   29d38:	neg	x8, x21
   29d3c:	mov	w9, #0x1                   	// #1
   29d40:	lsr	x8, x0, x8
   29d44:	lsl	x9, x9, x21
   29d48:	orr	x8, x8, x9
   29d4c:	mul	x9, x8, x20
   29d50:	mneg	x8, x8, x20
   29d54:	lsr	x10, x8, x21
   29d58:	umulh	x8, x8, x0
   29d5c:	mvn	x8, x8
   29d60:	add	x8, x9, x8
   29d64:	mneg	x11, x9, x0
   29d68:	mul	x8, x8, x20
   29d6c:	cmp	x8, x11
   29d70:	csel	x9, x20, xzr, hi  // hi = pmore
   29d74:	add	x8, x9, x8
   29d78:	lsr	x9, x8, x21
   29d7c:	umulh	x11, x8, x0
   29d80:	mul	x12, x8, x0
   29d84:	add	x8, x8, x11
   29d88:	mvn	x8, x8
   29d8c:	mul	x8, x20, x8
   29d90:	cmp	x8, x12
   29d94:	stp	x10, x9, [x19, #16]
   29d98:	csel	x9, x20, xzr, hi  // hi = pmore
   29d9c:	add	x8, x9, x8
   29da0:	lsr	x8, x8, x21
   29da4:	stp	x0, x21, [x19]
   29da8:	str	x8, [x19, #32]
   29dac:	ldp	x20, x19, [sp, #32]
   29db0:	ldr	x21, [sp, #16]
   29db4:	ldp	x29, x30, [sp], #48
   29db8:	ret

0000000000029dbc <__gmpn_mod_1s_2p@@Base>:
   29dbc:	ldp	x8, x9, [x3, #16]
   29dc0:	ldr	x10, [x3, #32]
   29dc4:	tbnz	w1, #0, 29e70 <__gmpn_mod_1s_2p@@Base+0xb4>
   29dc8:	add	x12, x0, x1, lsl #3
   29dcc:	ldp	x12, x11, [x12, #-16]
   29dd0:	cmp	x1, #0x4
   29dd4:	b.lt	29e24 <__gmpn_mod_1s_2p@@Base+0x68>  // b.tstop
   29dd8:	add	x14, x0, x1, lsl #3
   29ddc:	ldp	x14, x15, [x14, #-32]
   29de0:	mov	x13, xzr
   29de4:	mul	x16, x15, x8
   29de8:	umulh	x15, x15, x8
   29dec:	adds	x17, x16, x14
   29df0:	adc	x13, x15, x13
   29df4:	mul	x14, x12, x9
   29df8:	umulh	x12, x12, x9
   29dfc:	mul	x15, x11, x10
   29e00:	umulh	x11, x11, x10
   29e04:	adds	x16, x17, x14
   29e08:	adc	x13, x13, x12
   29e0c:	sub	x14, x1, #0x4
   29e10:	adds	x12, x15, x16
   29e14:	adc	x11, x11, x13
   29e18:	sub	x1, x1, #0x2
   29e1c:	cmp	x14, #0x1
   29e20:	b.gt	29dd8 <__gmpn_mod_1s_2p@@Base+0x1c>
   29e24:	mul	x10, x11, x8
   29e28:	umulh	x8, x11, x8
   29e2c:	ldp	x11, x13, [x3]
   29e30:	mov	x9, xzr
   29e34:	adds	x14, x12, x10
   29e38:	adc	x9, x8, x9
   29e3c:	neg	w10, w13
   29e40:	lsl	x9, x9, x13
   29e44:	lsr	x10, x14, x10
   29e48:	orr	x9, x10, x9
   29e4c:	umulh	x10, x9, x11
   29e50:	mul	x11, x9, x11
   29e54:	add	x9, x9, #0x1
   29e58:	and	x8, x13, #0xffffffff
   29e5c:	lsl	x12, x14, x13
   29e60:	adds	x13, x11, x12
   29e64:	adc	x9, x10, x9
   29e68:	msub	x9, x9, x2, x12
   29e6c:	b	29ea8 <__gmpn_mod_1s_2p@@Base+0xec>
   29e70:	subs	x13, x1, #0x1
   29e74:	b.ne	29ed0 <__gmpn_mod_1s_2p@@Base+0x114>  // b.any
   29e78:	ldp	x11, x9, [x3]
   29e7c:	ldr	x10, [x0]
   29e80:	neg	w12, w9
   29e84:	and	x8, x9, #0xffffffff
   29e88:	lsl	x9, x10, x9
   29e8c:	lsr	x10, x10, x12
   29e90:	umulh	x12, x10, x11
   29e94:	mul	x11, x10, x11
   29e98:	add	x10, x10, #0x1
   29e9c:	adds	x13, x11, x9
   29ea0:	adc	x10, x12, x10
   29ea4:	msub	x9, x10, x2, x9
   29ea8:	cmp	x9, x13
   29eac:	cset	w10, hi  // hi = pmore
   29eb0:	cmp	w10, #0x0
   29eb4:	csel	x10, x2, xzr, ne  // ne = any
   29eb8:	add	x9, x10, x9
   29ebc:	cmp	x9, x2
   29ec0:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29ec4:	sub	x9, x9, x10
   29ec8:	lsr	x0, x9, x8
   29ecc:	ret
   29ed0:	add	x12, x0, x1, lsl #3
   29ed4:	ldp	x12, x15, [x12, #-24]
   29ed8:	ldr	x14, [x0, x13, lsl #3]
   29edc:	mov	x11, xzr
   29ee0:	mov	x1, x13
   29ee4:	mul	x17, x15, x8
   29ee8:	umulh	x15, x15, x8
   29eec:	adds	x18, x17, x12
   29ef0:	adc	x11, x15, x11
   29ef4:	mul	x16, x14, x9
   29ef8:	umulh	x14, x14, x9
   29efc:	adds	x12, x16, x18
   29f00:	adc	x11, x14, x11
   29f04:	cmp	x1, #0x4
   29f08:	b.ge	29dd8 <__gmpn_mod_1s_2p@@Base+0x1c>  // b.tcont
   29f0c:	b	29e24 <__gmpn_mod_1s_2p@@Base+0x68>

0000000000029f10 <__gmpn_mod_1s_3p_cps@@Base>:
   29f10:	stp	x29, x30, [sp, #-48]!
   29f14:	str	x21, [sp, #16]
   29f18:	clz	x21, x1
   29f1c:	stp	x20, x19, [sp, #32]
   29f20:	lsl	x20, x1, x21
   29f24:	mov	x19, x0
   29f28:	mov	x0, x20
   29f2c:	mov	x29, sp
   29f30:	bl	d3f0 <__gmpn_invert_limb@plt>
   29f34:	neg	x8, x21
   29f38:	mov	w9, #0x1                   	// #1
   29f3c:	lsr	x8, x0, x8
   29f40:	lsl	x9, x9, x21
   29f44:	orr	x8, x8, x9
   29f48:	mul	x9, x8, x20
   29f4c:	mneg	x8, x8, x20
   29f50:	lsr	x10, x8, x21
   29f54:	umulh	x8, x8, x0
   29f58:	mvn	x8, x8
   29f5c:	add	x8, x9, x8
   29f60:	mneg	x11, x9, x0
   29f64:	mul	x8, x8, x20
   29f68:	cmp	x8, x11
   29f6c:	csel	x9, x20, xzr, hi  // hi = pmore
   29f70:	add	x8, x9, x8
   29f74:	lsr	x9, x8, x21
   29f78:	umulh	x11, x8, x0
   29f7c:	stp	x10, x9, [x19, #16]
   29f80:	mul	x9, x8, x0
   29f84:	add	x8, x8, x11
   29f88:	mvn	x8, x8
   29f8c:	mul	x8, x20, x8
   29f90:	cmp	x8, x9
   29f94:	csel	x9, x20, xzr, hi  // hi = pmore
   29f98:	add	x8, x9, x8
   29f9c:	lsr	x9, x8, x21
   29fa0:	umulh	x10, x8, x0
   29fa4:	mul	x11, x8, x0
   29fa8:	add	x8, x8, x10
   29fac:	mvn	x8, x8
   29fb0:	mul	x8, x20, x8
   29fb4:	cmp	x8, x11
   29fb8:	csel	x10, x20, xzr, hi  // hi = pmore
   29fbc:	add	x8, x10, x8
   29fc0:	lsr	x8, x8, x21
   29fc4:	stp	x0, x21, [x19]
   29fc8:	stp	x9, x8, [x19, #32]
   29fcc:	ldp	x20, x19, [sp, #32]
   29fd0:	ldr	x21, [sp, #16]
   29fd4:	ldp	x29, x30, [sp], #48
   29fd8:	ret

0000000000029fdc <__gmpn_mod_1s_3p@@Base>:
   29fdc:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   29fe0:	ldp	x8, x9, [x3, #16]
   29fe4:	ldp	x10, x11, [x3, #32]
   29fe8:	movk	x12, #0xaaab
   29fec:	mul	x12, x1, x12
   29ff0:	lsr	x12, x12, #62
   29ff4:	cmp	w12, #0x2
   29ff8:	b.eq	2a048 <__gmpn_mod_1s_3p@@Base+0x6c>  // b.none
   29ffc:	cmp	w12, #0x1
   2a000:	b.eq	2a060 <__gmpn_mod_1s_3p@@Base+0x84>  // b.none
   2a004:	cbnz	w12, 2a07c <__gmpn_mod_1s_3p@@Base+0xa0>
   2a008:	add	x13, x0, x1, lsl #3
   2a00c:	ldp	x14, x13, [x13, #-16]
   2a010:	mov	x12, xzr
   2a014:	sub	x1, x1, #0x3
   2a018:	ldr	x15, [x0, x1, lsl #3]
   2a01c:	mul	x16, x14, x8
   2a020:	umulh	x14, x14, x8
   2a024:	adds	x18, x16, x15
   2a028:	adc	x12, x14, x12
   2a02c:	mul	x17, x13, x9
   2a030:	umulh	x14, x13, x9
   2a034:	adds	x13, x17, x18
   2a038:	adc	x12, x14, x12
   2a03c:	cmp	x1, #0x3
   2a040:	b.ge	2a084 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   2a044:	b	2a0e0 <__gmpn_mod_1s_3p@@Base+0x104>
   2a048:	sub	x1, x1, #0x1
   2a04c:	ldr	x13, [x0, x1, lsl #3]
   2a050:	mov	x12, xzr
   2a054:	cmp	x1, #0x3
   2a058:	b.ge	2a084 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   2a05c:	b	2a0e0 <__gmpn_mod_1s_3p@@Base+0x104>
   2a060:	add	x12, x0, x1, lsl #3
   2a064:	sub	x1, x1, #0x2
   2a068:	ldur	x12, [x12, #-8]
   2a06c:	ldr	x13, [x0, x1, lsl #3]
   2a070:	cmp	x1, #0x3
   2a074:	b.ge	2a084 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   2a078:	b	2a0e0 <__gmpn_mod_1s_3p@@Base+0x104>
   2a07c:	cmp	x1, #0x3
   2a080:	b.lt	2a0e0 <__gmpn_mod_1s_3p@@Base+0x104>  // b.tstop
   2a084:	add	x15, x0, x1, lsl #3
   2a088:	ldp	x18, x17, [x15, #-24]
   2a08c:	ldur	x15, [x15, #-8]
   2a090:	mov	x14, xzr
   2a094:	mul	x16, x13, x10
   2a098:	mul	x4, x17, x8
   2a09c:	umulh	x17, x17, x8
   2a0a0:	adds	x5, x4, x18
   2a0a4:	adc	x14, x17, x14
   2a0a8:	umulh	x13, x13, x10
   2a0ac:	mul	x17, x12, x11
   2a0b0:	umulh	x12, x12, x11
   2a0b4:	mul	x18, x15, x9
   2a0b8:	umulh	x15, x15, x9
   2a0bc:	adds	x4, x5, x18
   2a0c0:	adc	x14, x14, x15
   2a0c4:	adds	x15, x4, x16
   2a0c8:	adc	x14, x14, x13
   2a0cc:	adds	x13, x17, x15
   2a0d0:	adc	x12, x12, x14
   2a0d4:	cmp	x1, #0x5
   2a0d8:	sub	x1, x1, #0x3
   2a0dc:	b.gt	2a084 <__gmpn_mod_1s_3p@@Base+0xa8>
   2a0e0:	mul	x10, x12, x8
   2a0e4:	umulh	x8, x12, x8
   2a0e8:	ldp	x12, x11, [x3]
   2a0ec:	mov	x9, xzr
   2a0f0:	adds	x14, x13, x10
   2a0f4:	adc	x8, x8, x9
   2a0f8:	neg	w10, w11
   2a0fc:	lsl	x8, x8, x11
   2a100:	lsr	x10, x14, x10
   2a104:	orr	x8, x10, x8
   2a108:	umulh	x10, x8, x12
   2a10c:	mul	x12, x8, x12
   2a110:	add	x8, x8, #0x1
   2a114:	lsl	x13, x14, x11
   2a118:	adds	x14, x12, x13
   2a11c:	adc	x8, x10, x8
   2a120:	msub	x8, x8, x2, x13
   2a124:	cmp	x8, x14
   2a128:	csel	x10, x2, x9, hi  // hi = pmore
   2a12c:	add	x8, x10, x8
   2a130:	cmp	x8, x2
   2a134:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   2a138:	sub	x8, x8, x9
   2a13c:	lsr	x0, x8, x11
   2a140:	ret

000000000002a144 <__gmpn_mod_1s_4p_cps@@Base>:
   2a144:	stp	x29, x30, [sp, #-48]!
   2a148:	str	x21, [sp, #16]
   2a14c:	clz	x21, x1
   2a150:	stp	x20, x19, [sp, #32]
   2a154:	lsl	x20, x1, x21
   2a158:	mov	x19, x0
   2a15c:	mov	x0, x20
   2a160:	mov	x29, sp
   2a164:	bl	d3f0 <__gmpn_invert_limb@plt>
   2a168:	neg	x8, x21
   2a16c:	mov	w9, #0x1                   	// #1
   2a170:	lsr	x8, x0, x8
   2a174:	lsl	x9, x9, x21
   2a178:	orr	x8, x8, x9
   2a17c:	mul	x9, x8, x20
   2a180:	mneg	x8, x8, x20
   2a184:	lsr	x10, x8, x21
   2a188:	umulh	x8, x8, x0
   2a18c:	mvn	x8, x8
   2a190:	add	x8, x9, x8
   2a194:	mneg	x11, x9, x0
   2a198:	mul	x8, x8, x20
   2a19c:	cmp	x8, x11
   2a1a0:	csel	x9, x20, xzr, hi  // hi = pmore
   2a1a4:	add	x8, x9, x8
   2a1a8:	lsr	x9, x8, x21
   2a1ac:	umulh	x11, x8, x0
   2a1b0:	mul	x12, x8, x0
   2a1b4:	add	x8, x8, x11
   2a1b8:	mvn	x8, x8
   2a1bc:	mul	x8, x20, x8
   2a1c0:	cmp	x8, x12
   2a1c4:	stp	x10, x9, [x19, #16]
   2a1c8:	csel	x9, x20, xzr, hi  // hi = pmore
   2a1cc:	add	x8, x9, x8
   2a1d0:	lsr	x9, x8, x21
   2a1d4:	umulh	x10, x8, x0
   2a1d8:	mul	x11, x8, x0
   2a1dc:	add	x8, x8, x10
   2a1e0:	mvn	x8, x8
   2a1e4:	mul	x8, x20, x8
   2a1e8:	cmp	x8, x11
   2a1ec:	csel	x10, x20, xzr, hi  // hi = pmore
   2a1f0:	add	x8, x10, x8
   2a1f4:	lsr	x10, x8, x21
   2a1f8:	umulh	x11, x8, x0
   2a1fc:	mul	x12, x8, x0
   2a200:	add	x8, x8, x11
   2a204:	mvn	x8, x8
   2a208:	mul	x8, x20, x8
   2a20c:	cmp	x8, x12
   2a210:	stp	x9, x10, [x19, #32]
   2a214:	csel	x9, x20, xzr, hi  // hi = pmore
   2a218:	add	x8, x9, x8
   2a21c:	lsr	x8, x8, x21
   2a220:	stp	x0, x21, [x19]
   2a224:	str	x8, [x19, #48]
   2a228:	ldp	x20, x19, [sp, #32]
   2a22c:	ldr	x21, [sp, #16]
   2a230:	ldp	x29, x30, [sp], #48
   2a234:	ret

000000000002a238 <__gmpn_mod_1s_4p@@Base>:
   2a238:	ldp	x8, x9, [x3, #16]
   2a23c:	ldp	x10, x11, [x3, #32]
   2a240:	ldr	x12, [x3, #48]
   2a244:	adrp	x14, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2a248:	and	x13, x1, #0x3
   2a24c:	add	x14, x14, #0xd7a
   2a250:	adr	x15, 2a260 <__gmpn_mod_1s_4p@@Base+0x28>
   2a254:	ldrb	w16, [x14, x13]
   2a258:	add	x15, x15, x16, lsl #2
   2a25c:	br	x15
   2a260:	add	x14, x0, x1, lsl #3
   2a264:	ldp	x16, x18, [x14, #-24]
   2a268:	ldur	x14, [x14, #-8]
   2a26c:	mov	x13, xzr
   2a270:	sub	x15, x1, #0x4
   2a274:	ldr	x17, [x0, x15, lsl #3]
   2a278:	mul	x1, x16, x8
   2a27c:	umulh	x16, x16, x8
   2a280:	adds	x4, x1, x17
   2a284:	adc	x13, x16, x13
   2a288:	mul	x16, x18, x9
   2a28c:	umulh	x17, x18, x9
   2a290:	adds	x18, x4, x16
   2a294:	adc	x13, x13, x17
   2a298:	mul	x16, x14, x10
   2a29c:	umulh	x17, x14, x10
   2a2a0:	adds	x14, x16, x18
   2a2a4:	adc	x13, x17, x13
   2a2a8:	cmp	x15, #0x4
   2a2ac:	b.ge	2a324 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   2a2b0:	b	2a398 <__gmpn_mod_1s_4p@@Base+0x160>
   2a2b4:	add	x13, x0, x1, lsl #3
   2a2b8:	sub	x15, x1, #0x2
   2a2bc:	ldur	x13, [x13, #-8]
   2a2c0:	ldr	x14, [x0, x15, lsl #3]
   2a2c4:	cmp	x15, #0x4
   2a2c8:	b.ge	2a324 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   2a2cc:	b	2a398 <__gmpn_mod_1s_4p@@Base+0x160>
   2a2d0:	add	x14, x0, x1, lsl #3
   2a2d4:	ldp	x16, x14, [x14, #-16]
   2a2d8:	mov	x13, xzr
   2a2dc:	sub	x15, x1, #0x3
   2a2e0:	ldr	x17, [x0, x15, lsl #3]
   2a2e4:	mul	x18, x16, x8
   2a2e8:	umulh	x16, x16, x8
   2a2ec:	adds	x4, x18, x17
   2a2f0:	adc	x13, x16, x13
   2a2f4:	mul	x1, x14, x9
   2a2f8:	umulh	x16, x14, x9
   2a2fc:	adds	x14, x1, x4
   2a300:	adc	x13, x16, x13
   2a304:	cmp	x15, #0x4
   2a308:	b.ge	2a324 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   2a30c:	b	2a398 <__gmpn_mod_1s_4p@@Base+0x160>
   2a310:	sub	x15, x1, #0x1
   2a314:	ldr	x14, [x0, x15, lsl #3]
   2a318:	mov	x13, xzr
   2a31c:	cmp	x15, #0x4
   2a320:	b.lt	2a398 <__gmpn_mod_1s_4p@@Base+0x160>  // b.tstop
   2a324:	add	x16, x15, #0x4
   2a328:	add	x15, x0, x15, lsl #3
   2a32c:	sub	x15, x15, #0x10
   2a330:	ldp	x0, x18, [x15, #-16]
   2a334:	mov	x17, xzr
   2a338:	sub	x16, x16, #0x4
   2a33c:	mul	x1, x18, x8
   2a340:	umulh	x18, x18, x8
   2a344:	adds	x5, x1, x0
   2a348:	adc	x17, x18, x17
   2a34c:	ldp	x4, x18, [x15], #-32
   2a350:	umulh	x1, x4, x9
   2a354:	mul	x0, x4, x9
   2a358:	adds	x4, x5, x0
   2a35c:	adc	x17, x17, x1
   2a360:	mul	x0, x18, x10
   2a364:	umulh	x18, x18, x10
   2a368:	adds	x1, x4, x0
   2a36c:	adc	x17, x17, x18
   2a370:	mul	x18, x14, x11
   2a374:	umulh	x14, x14, x11
   2a378:	mul	x0, x13, x12
   2a37c:	umulh	x13, x13, x12
   2a380:	adds	x4, x1, x18
   2a384:	adc	x17, x17, x14
   2a388:	adds	x14, x0, x4
   2a38c:	adc	x13, x13, x17
   2a390:	cmp	x16, #0x7
   2a394:	b.gt	2a330 <__gmpn_mod_1s_4p@@Base+0xf8>
   2a398:	ldp	x12, x11, [x3]
   2a39c:	mul	x10, x13, x8
   2a3a0:	umulh	x8, x13, x8
   2a3a4:	mov	x9, xzr
   2a3a8:	adds	x13, x14, x10
   2a3ac:	adc	x8, x8, x9
   2a3b0:	neg	w10, w11
   2a3b4:	lsl	x8, x8, x11
   2a3b8:	lsr	x10, x13, x10
   2a3bc:	orr	x8, x10, x8
   2a3c0:	umulh	x10, x8, x12
   2a3c4:	mul	x12, x8, x12
   2a3c8:	add	x8, x8, #0x1
   2a3cc:	lsl	x13, x13, x11
   2a3d0:	adds	x14, x12, x13
   2a3d4:	adc	x8, x10, x8
   2a3d8:	msub	x8, x8, x2, x13
   2a3dc:	cmp	x8, x14
   2a3e0:	csel	x10, x2, x9, hi  // hi = pmore
   2a3e4:	add	x8, x10, x8
   2a3e8:	cmp	x8, x2
   2a3ec:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   2a3f0:	sub	x8, x8, x9
   2a3f4:	lsr	x0, x8, x11
   2a3f8:	ret
   2a3fc:	nop

000000000002a400 <__gmpn_lshiftc@@Base>:
   2a400:	add	x16, x0, x2, lsl #3
   2a404:	add	x1, x1, x2, lsl #3
   2a408:	neg	x8, x3
   2a40c:	lsr	x18, x2, #2
   2a410:	tbz	w2, #0, 2a454 <__gmpn_lshiftc@@Base+0x54>
   2a414:	ldur	x4, [x1, #-8]
   2a418:	tbnz	w2, #1, 2a444 <__gmpn_lshiftc@@Base+0x44>
   2a41c:	lsr	x0, x4, x8
   2a420:	lsl	x2, x4, x3
   2a424:	cbnz	x18, 2a434 <__gmpn_lshiftc@@Base+0x34>
   2a428:	mvn	x2, x2
   2a42c:	stur	x2, [x16, #-8]
   2a430:	ret
   2a434:	ldp	x4, x5, [x1, #-24]
   2a438:	sub	x1, x1, #0x8
   2a43c:	add	x16, x16, #0x10
   2a440:	b	2a4d4 <__gmpn_lshiftc@@Base+0xd4>
   2a444:	lsr	x0, x4, x8
   2a448:	lsl	x2, x4, x3
   2a44c:	ldp	x6, x7, [x1, #-24]!
   2a450:	b	2a4f8 <__gmpn_lshiftc@@Base+0xf8>
   2a454:	ldp	x4, x5, [x1, #-16]
   2a458:	tbz	w2, #1, 2a498 <__gmpn_lshiftc@@Base+0x98>
   2a45c:	lsr	x0, x5, x8
   2a460:	lsl	x13, x5, x3
   2a464:	lsr	x10, x4, x8
   2a468:	lsl	x2, x4, x3
   2a46c:	cbnz	x18, 2a480 <__gmpn_lshiftc@@Base+0x80>
   2a470:	eon	x10, x10, x13
   2a474:	mvn	x2, x2
   2a478:	stp	x2, x10, [x16, #-16]
   2a47c:	ret
   2a480:	ldp	x4, x5, [x1, #-32]
   2a484:	eon	x10, x10, x13
   2a488:	stur	x10, [x16, #-8]
   2a48c:	sub	x1, x1, #0x10
   2a490:	add	x16, x16, #0x8
   2a494:	b	2a4d4 <__gmpn_lshiftc@@Base+0xd4>
   2a498:	lsr	x0, x5, x8
   2a49c:	lsl	x13, x5, x3
   2a4a0:	lsr	x10, x4, x8
   2a4a4:	lsl	x2, x4, x3
   2a4a8:	ldp	x6, x7, [x1, #-32]!
   2a4ac:	eon	x10, x10, x13
   2a4b0:	str	x10, [x16, #-8]!
   2a4b4:	b	2a4f4 <__gmpn_lshiftc@@Base+0xf4>
   2a4b8:	nop
   2a4bc:	nop
   2a4c0:	ldp	x4, x5, [x1, #-16]
   2a4c4:	eon	x10, x10, x13
   2a4c8:	eon	x11, x12, x2
   2a4cc:	stp	x10, x11, [x16, #-16]
   2a4d0:	lsl	x2, x6, x3
   2a4d4:	lsr	x10, x4, x8
   2a4d8:	lsl	x13, x5, x3
   2a4dc:	lsr	x12, x5, x8
   2a4e0:	ldp	x6, x7, [x1, #-32]!
   2a4e4:	eon	x10, x10, x13
   2a4e8:	eon	x11, x12, x2
   2a4ec:	stp	x10, x11, [x16, #-32]!
   2a4f0:	lsl	x2, x4, x3
   2a4f4:	sub	x18, x18, #0x1
   2a4f8:	lsr	x10, x6, x8
   2a4fc:	lsl	x13, x7, x3
   2a500:	lsr	x12, x7, x8
   2a504:	cbnz	x18, 2a4c0 <__gmpn_lshiftc@@Base+0xc0>
   2a508:	eon	x10, x10, x13
   2a50c:	eon	x11, x12, x2
   2a510:	lsl	x2, x6, x3
   2a514:	stp	x10, x11, [x16, #-16]
   2a518:	mvn	x2, x2
   2a51c:	stur	x2, [x16, #-24]
   2a520:	ret

000000000002a524 <__gmpn_mul@@Base>:
   2a524:	stp	x29, x30, [sp, #-96]!
   2a528:	stp	x28, x27, [sp, #16]
   2a52c:	stp	x26, x25, [sp, #32]
   2a530:	stp	x24, x23, [sp, #48]
   2a534:	stp	x22, x21, [sp, #64]
   2a538:	stp	x20, x19, [sp, #80]
   2a53c:	mov	x29, sp
   2a540:	sub	sp, sp, #0xa0
   2a544:	mov	x19, x4
   2a548:	mov	x28, x3
   2a54c:	mov	x20, x2
   2a550:	mov	x23, x1
   2a554:	cmp	x2, #0xd
   2a558:	mov	x22, x0
   2a55c:	b.le	2a668 <__gmpn_mul@@Base+0x144>
   2a560:	cmp	x20, x19
   2a564:	b.ne	2a580 <__gmpn_mul@@Base+0x5c>  // b.any
   2a568:	mov	x0, x22
   2a56c:	mov	x1, x23
   2a570:	mov	x2, x28
   2a574:	mov	x3, x20
   2a578:	bl	c990 <__gmpn_mul_n@plt>
   2a57c:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2a580:	cmp	x19, #0xd
   2a584:	b.gt	2a6ac <__gmpn_mul@@Base+0x188>
   2a588:	cmp	x20, #0x1f5
   2a58c:	b.lt	2a668 <__gmpn_mul@@Base+0x144>  // b.tstop
   2a590:	cmp	x19, #0x1
   2a594:	b.eq	2a668 <__gmpn_mul@@Base+0x144>  // b.none
   2a598:	mov	w2, #0x1f4                 	// #500
   2a59c:	mov	x0, x22
   2a5a0:	mov	x1, x23
   2a5a4:	mov	x3, x28
   2a5a8:	mov	x4, x19
   2a5ac:	bl	c550 <__gmpn_mul_basecase@plt>
   2a5b0:	add	x24, x22, #0xfa0
   2a5b4:	sub	x0, x29, #0x70
   2a5b8:	mov	x1, x24
   2a5bc:	mov	x2, x19
   2a5c0:	bl	ca50 <__gmpn_copyi@plt>
   2a5c4:	sub	x21, x20, #0x1f4
   2a5c8:	cmp	x20, #0x3e9
   2a5cc:	add	x23, x23, #0xfa0
   2a5d0:	b.lt	2a864 <__gmpn_mul@@Base+0x340>  // b.tstop
   2a5d4:	add	x8, x22, x19, lsl #3
   2a5d8:	add	x25, x8, #0xfa8
   2a5dc:	lsl	x26, x19, #3
   2a5e0:	mov	x22, x24
   2a5e4:	b	2a614 <__gmpn_mul@@Base+0xf0>
   2a5e8:	add	x22, x22, #0xfa0
   2a5ec:	sub	x0, x29, #0x70
   2a5f0:	mov	x1, x22
   2a5f4:	mov	x2, x19
   2a5f8:	bl	ca50 <__gmpn_copyi@plt>
   2a5fc:	sub	x20, x21, #0x1f4
   2a600:	add	x23, x23, #0xfa0
   2a604:	cmp	x21, #0x3e8
   2a608:	add	x25, x25, #0xfa0
   2a60c:	mov	x21, x20
   2a610:	b.le	2a86c <__gmpn_mul@@Base+0x348>
   2a614:	mov	w2, #0x1f4                 	// #500
   2a618:	mov	x0, x22
   2a61c:	mov	x1, x23
   2a620:	mov	x3, x28
   2a624:	mov	x4, x19
   2a628:	bl	c550 <__gmpn_mul_basecase@plt>
   2a62c:	sub	x2, x29, #0x70
   2a630:	mov	x0, x22
   2a634:	mov	x1, x22
   2a638:	mov	x3, x19
   2a63c:	bl	ca70 <__gmpn_add_n@plt>
   2a640:	ldr	x8, [x22, x26]
   2a644:	adds	x8, x8, x0
   2a648:	str	x8, [x22, x26]
   2a64c:	b.cc	2a5e8 <__gmpn_mul@@Base+0xc4>  // b.lo, b.ul, b.last
   2a650:	mov	x8, x25
   2a654:	ldr	x9, [x8]
   2a658:	adds	x9, x9, #0x1
   2a65c:	str	x9, [x8], #8
   2a660:	b.cs	2a654 <__gmpn_mul@@Base+0x130>  // b.hs, b.nlast
   2a664:	b	2a5e8 <__gmpn_mul@@Base+0xc4>
   2a668:	mov	x0, x22
   2a66c:	mov	x1, x23
   2a670:	mov	x2, x20
   2a674:	mov	x3, x28
   2a678:	mov	x4, x19
   2a67c:	bl	c550 <__gmpn_mul_basecase@plt>
   2a680:	add	x8, x19, x20
   2a684:	add	x8, x22, x8, lsl #3
   2a688:	ldur	x0, [x8, #-8]
   2a68c:	mov	sp, x29
   2a690:	ldp	x20, x19, [sp, #80]
   2a694:	ldp	x22, x21, [sp, #64]
   2a698:	ldp	x24, x23, [sp, #48]
   2a69c:	ldp	x26, x25, [sp, #32]
   2a6a0:	ldp	x28, x27, [sp, #16]
   2a6a4:	ldp	x29, x30, [sp], #96
   2a6a8:	ret
   2a6ac:	cmp	x19, #0x30
   2a6b0:	stur	x28, [x29, #-120]
   2a6b4:	b.le	2a6f8 <__gmpn_mul@@Base+0x1d4>
   2a6b8:	add	x8, x19, x20
   2a6bc:	mov	w9, #0x1900                	// #6400
   2a6c0:	cmp	x8, x9
   2a6c4:	b.lt	2a758 <__gmpn_mul@@Base+0x234>  // b.tstop
   2a6c8:	add	x25, x19, x19, lsl #1
   2a6cc:	cmp	x25, #0xc7f
   2a6d0:	b.le	2a758 <__gmpn_mul@@Base+0x234>
   2a6d4:	cmp	x20, x19, lsl #3
   2a6d8:	b.ge	2ac64 <__gmpn_mul@@Base+0x740>  // b.tcont
   2a6dc:	mov	x0, x22
   2a6e0:	mov	x1, x23
   2a6e4:	mov	x2, x20
   2a6e8:	mov	x3, x28
   2a6ec:	mov	x4, x19
   2a6f0:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   2a6f4:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2a6f8:	add	x8, x19, x19, lsl #3
   2a6fc:	cmp	x8, #0x0
   2a700:	cinc	x8, x8, lt  // lt = tstop
   2a704:	lsl	x8, x8, #2
   2a708:	and	x8, x8, #0xfffffffffffffff8
   2a70c:	add	x8, x8, #0x40f
   2a710:	and	x8, x8, #0xfffffffffffffff0
   2a714:	mov	x9, sp
   2a718:	sub	x5, x9, x8
   2a71c:	mov	sp, x5
   2a720:	add	x26, x19, x19, lsl #1
   2a724:	cmp	x26, x20
   2a728:	b.le	2a88c <__gmpn_mul@@Base+0x368>
   2a72c:	lsl	x8, x20, #2
   2a730:	add	x9, x19, x19, lsl #2
   2a734:	cmp	x8, x9
   2a738:	b.ge	2a9bc <__gmpn_mul@@Base+0x498>  // b.tcont
   2a73c:	mov	x0, x22
   2a740:	mov	x1, x23
   2a744:	mov	x2, x20
   2a748:	mov	x3, x28
   2a74c:	mov	x4, x19
   2a750:	bl	d450 <__gmpn_toom22_mul@plt>
   2a754:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2a758:	cmp	x19, #0x52
   2a75c:	b.lt	2a7e8 <__gmpn_mul@@Base+0x2c4>  // b.tstop
   2a760:	add	x9, x20, x20, lsl #1
   2a764:	add	x9, x9, #0xc
   2a768:	cmp	x9, x19, lsl #2
   2a76c:	b.ge	2a7e8 <__gmpn_mul@@Base+0x2c4>  // b.tcont
   2a770:	cmp	x19, #0xac
   2a774:	stur	xzr, [x29, #-112]
   2a778:	b.le	2ad4c <__gmpn_mul@@Base+0x828>
   2a77c:	cmp	x19, #0xeb
   2a780:	b.le	2af78 <__gmpn_mul@@Base+0xa54>
   2a784:	mov	x9, #0x4925                	// #18725
   2a788:	movk	x9, #0x2492, lsl #16
   2a78c:	movk	x9, #0x9249, lsl #32
   2a790:	lsr	x8, x8, #1
   2a794:	movk	x9, #0x4924, lsl #48
   2a798:	umulh	x8, x8, x9
   2a79c:	mov	w10, #0x78                  	// #120
   2a7a0:	lsr	x8, x8, #1
   2a7a4:	mul	x8, x8, x10
   2a7a8:	add	x1, x8, #0xd68
   2a7ac:	mov	w8, #0x7f00                	// #32512
   2a7b0:	cmp	x1, x8
   2a7b4:	b.hi	2b054 <__gmpn_mul@@Base+0xb30>  // b.pmore
   2a7b8:	add	x9, x1, #0xf
   2a7bc:	mov	x8, sp
   2a7c0:	and	x9, x9, #0xfffffffffffffff0
   2a7c4:	sub	x5, x8, x9
   2a7c8:	mov	sp, x5
   2a7cc:	mov	x0, x22
   2a7d0:	mov	x1, x23
   2a7d4:	mov	x2, x20
   2a7d8:	mov	x3, x28
   2a7dc:	mov	x4, x19
   2a7e0:	bl	cb20 <__gmpn_toom8h_mul@plt>
   2a7e4:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2a7e8:	lsl	x8, x19, #5
   2a7ec:	add	x1, x8, #0x200
   2a7f0:	mov	w8, #0x7f00                	// #32512
   2a7f4:	cmp	x1, x8
   2a7f8:	stur	xzr, [x29, #-112]
   2a7fc:	b.hi	2b034 <__gmpn_mul@@Base+0xb10>  // b.pmore
   2a800:	add	x9, x1, #0xf
   2a804:	mov	x8, sp
   2a808:	and	x9, x9, #0xfffffffffffffff0
   2a80c:	sub	x8, x8, x9
   2a810:	stur	x8, [x29, #-128]
   2a814:	mov	sp, x8
   2a818:	lsl	x9, x20, #1
   2a81c:	add	x8, x19, x19, lsl #2
   2a820:	cmp	x9, x8
   2a824:	stur	x8, [x29, #-136]
   2a828:	b.ge	2a964 <__gmpn_mul@@Base+0x440>  // b.tcont
   2a82c:	add	x8, x20, x20, lsl #1
   2a830:	lsl	x11, x19, #3
   2a834:	lsl	x10, x8, #1
   2a838:	sub	x8, x11, x19
   2a83c:	cmp	x10, x8
   2a840:	b.ge	2aac4 <__gmpn_mul@@Base+0x5a0>  // b.tcont
   2a844:	ldur	x5, [x29, #-128]
   2a848:	mov	x0, x22
   2a84c:	mov	x1, x23
   2a850:	mov	x2, x20
   2a854:	mov	x3, x28
   2a858:	mov	x4, x19
   2a85c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   2a860:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2a864:	mov	x22, x24
   2a868:	mov	x20, x21
   2a86c:	mov	x0, x22
   2a870:	cmp	x20, x19
   2a874:	b.le	2a90c <__gmpn_mul@@Base+0x3e8>
   2a878:	mov	x1, x23
   2a87c:	mov	x2, x20
   2a880:	mov	x3, x28
   2a884:	mov	x4, x19
   2a888:	b	2a91c <__gmpn_mul@@Base+0x3f8>
   2a88c:	mov	x8, sp
   2a890:	sub	x21, x8, x19, lsl #5
   2a894:	mov	sp, x21
   2a898:	lsl	x27, x19, #1
   2a89c:	mov	x0, x22
   2a8a0:	mov	x1, x23
   2a8a4:	mov	x2, x27
   2a8a8:	mov	x3, x28
   2a8ac:	mov	x4, x19
   2a8b0:	stur	x5, [x29, #-128]
   2a8b4:	bl	d480 <__gmpn_toom42_mul@plt>
   2a8b8:	lsl	x8, x19, #4
   2a8bc:	sub	x20, x20, x27
   2a8c0:	add	x25, x22, x8
   2a8c4:	cmp	x20, x26
   2a8c8:	add	x23, x23, x8
   2a8cc:	stp	x8, x21, [x29, #-144]
   2a8d0:	b.ge	2a9e8 <__gmpn_mul@@Base+0x4c4>  // b.tcont
   2a8d4:	ldur	x5, [x29, #-128]
   2a8d8:	lsl	x8, x20, #2
   2a8dc:	add	x9, x19, x19, lsl #2
   2a8e0:	cmp	x8, x9
   2a8e4:	lsl	x24, x19, #3
   2a8e8:	b.ge	2aa98 <__gmpn_mul@@Base+0x574>  // b.tcont
   2a8ec:	ldur	x26, [x29, #-136]
   2a8f0:	ldur	x3, [x29, #-120]
   2a8f4:	mov	x1, x23
   2a8f8:	mov	x2, x20
   2a8fc:	mov	x0, x26
   2a900:	mov	x4, x19
   2a904:	bl	d450 <__gmpn_toom22_mul@plt>
   2a908:	b	2acf0 <__gmpn_mul@@Base+0x7cc>
   2a90c:	mov	x1, x28
   2a910:	mov	x2, x19
   2a914:	mov	x3, x23
   2a918:	mov	x4, x20
   2a91c:	bl	c550 <__gmpn_mul_basecase@plt>
   2a920:	sub	x2, x29, #0x70
   2a924:	mov	x0, x22
   2a928:	mov	x1, x22
   2a92c:	mov	x3, x19
   2a930:	bl	ca70 <__gmpn_add_n@plt>
   2a934:	lsl	x8, x19, #3
   2a938:	ldr	x9, [x22, x8]
   2a93c:	adds	x9, x9, x0
   2a940:	str	x9, [x22, x8]
   2a944:	b.cc	2a680 <__gmpn_mul@@Base+0x15c>  // b.lo, b.ul, b.last
   2a948:	add	x8, x22, x19, lsl #3
   2a94c:	add	x8, x8, #0x8
   2a950:	ldr	x9, [x8]
   2a954:	adds	x9, x9, #0x1
   2a958:	str	x9, [x8], #8
   2a95c:	b.cs	2a950 <__gmpn_mul@@Base+0x42c>  // b.hs, b.nlast
   2a960:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2a964:	mov	w8, #0x1c                  	// #28
   2a968:	mul	x8, x19, x8
   2a96c:	and	x1, x8, #0xfffffffffffffff8
   2a970:	mov	w8, #0x7f00                	// #32512
   2a974:	cmp	x1, x8
   2a978:	b.hi	2b044 <__gmpn_mul@@Base+0xb20>  // b.pmore
   2a97c:	add	x9, x1, #0xf
   2a980:	mov	x8, sp
   2a984:	and	x9, x9, #0xfffffffffffffff0
   2a988:	sub	x25, x8, x9
   2a98c:	mov	sp, x25
   2a990:	lsl	x27, x19, #1
   2a994:	cmp	x19, #0x4f
   2a998:	mov	x0, x22
   2a99c:	mov	x1, x23
   2a9a0:	mov	x2, x27
   2a9a4:	mov	x3, x28
   2a9a8:	mov	x4, x19
   2a9ac:	b.le	2aaf8 <__gmpn_mul@@Base+0x5d4>
   2a9b0:	ldur	x5, [x29, #-128]
   2a9b4:	bl	c740 <__gmpn_toom63_mul@plt>
   2a9b8:	b	2ab00 <__gmpn_mul@@Base+0x5dc>
   2a9bc:	lsl	x9, x19, #3
   2a9c0:	sub	x9, x9, x19
   2a9c4:	mov	x0, x22
   2a9c8:	mov	x1, x23
   2a9cc:	mov	x2, x20
   2a9d0:	mov	x3, x28
   2a9d4:	mov	x4, x19
   2a9d8:	cmp	x8, x9
   2a9dc:	b.ge	2ace4 <__gmpn_mul@@Base+0x7c0>  // b.tcont
   2a9e0:	bl	c850 <__gmpn_toom32_mul@plt>
   2a9e4:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2a9e8:	add	x8, x21, x19, lsl #3
   2a9ec:	ldur	x5, [x29, #-128]
   2a9f0:	stur	x8, [x29, #-152]
   2a9f4:	mov	w8, #0x18                  	// #24
   2a9f8:	madd	x8, x19, x8, x22
   2a9fc:	add	x28, x8, #0x8
   2aa00:	lsl	x21, x27, #3
   2aa04:	b	2aa28 <__gmpn_mul@@Base+0x504>
   2aa08:	ldur	x8, [x29, #-144]
   2aa0c:	ldur	x5, [x29, #-128]
   2aa10:	sub	x20, x20, x27
   2aa14:	add	x25, x25, x21
   2aa18:	add	x23, x23, x21
   2aa1c:	cmp	x20, x26
   2aa20:	add	x28, x28, x8
   2aa24:	b.lt	2a8d8 <__gmpn_mul@@Base+0x3b4>  // b.tstop
   2aa28:	ldur	x22, [x29, #-136]
   2aa2c:	ldur	x3, [x29, #-120]
   2aa30:	mov	x1, x23
   2aa34:	mov	x2, x27
   2aa38:	mov	x0, x22
   2aa3c:	mov	x4, x19
   2aa40:	bl	d480 <__gmpn_toom42_mul@plt>
   2aa44:	mov	x0, x25
   2aa48:	mov	x1, x25
   2aa4c:	mov	x2, x22
   2aa50:	mov	x3, x19
   2aa54:	bl	ca70 <__gmpn_add_n@plt>
   2aa58:	ldur	x1, [x29, #-152]
   2aa5c:	add	x24, x25, x19, lsl #3
   2aa60:	mov	x22, x0
   2aa64:	mov	x0, x24
   2aa68:	mov	x2, x27
   2aa6c:	bl	ca50 <__gmpn_copyi@plt>
   2aa70:	ldr	x8, [x24]
   2aa74:	adds	x8, x8, x22
   2aa78:	str	x8, [x24]
   2aa7c:	b.cc	2aa08 <__gmpn_mul@@Base+0x4e4>  // b.lo, b.ul, b.last
   2aa80:	mov	x8, x28
   2aa84:	ldr	x9, [x8]
   2aa88:	adds	x9, x9, #0x1
   2aa8c:	str	x9, [x8], #8
   2aa90:	b.cs	2aa84 <__gmpn_mul@@Base+0x560>  // b.hs, b.nlast
   2aa94:	b	2aa08 <__gmpn_mul@@Base+0x4e4>
   2aa98:	ldur	x26, [x29, #-136]
   2aa9c:	ldur	x3, [x29, #-120]
   2aaa0:	sub	x9, x24, x19
   2aaa4:	cmp	x8, x9
   2aaa8:	mov	x0, x26
   2aaac:	mov	x1, x23
   2aab0:	mov	x2, x20
   2aab4:	mov	x4, x19
   2aab8:	b.ge	2acec <__gmpn_mul@@Base+0x7c8>  // b.tcont
   2aabc:	bl	c850 <__gmpn_toom32_mul@plt>
   2aac0:	b	2acf0 <__gmpn_mul@@Base+0x7cc>
   2aac4:	add	x11, x19, x19, lsl #1
   2aac8:	cmp	x9, x11
   2aacc:	b.ge	2ad84 <__gmpn_mul@@Base+0x860>  // b.tcont
   2aad0:	cmp	x19, #0x50
   2aad4:	b.le	2ada4 <__gmpn_mul@@Base+0x880>
   2aad8:	ldur	x5, [x29, #-128]
   2aadc:	mov	x0, x22
   2aae0:	mov	x1, x23
   2aae4:	mov	x2, x20
   2aae8:	mov	x3, x28
   2aaec:	mov	x4, x19
   2aaf0:	bl	cf00 <__gmpn_toom43_mul@plt>
   2aaf4:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2aaf8:	ldur	x5, [x29, #-128]
   2aafc:	bl	d480 <__gmpn_toom42_mul@plt>
   2ab00:	ldur	x8, [x29, #-136]
   2ab04:	lsl	x28, x27, #3
   2ab08:	sub	x20, x20, x27
   2ab0c:	add	x26, x22, x28
   2ab10:	cmp	x8, x20, lsl #1
   2ab14:	add	x23, x23, x28
   2ab18:	b.le	2ab3c <__gmpn_mul@@Base+0x618>
   2ab1c:	mov	x0, x25
   2ab20:	cmp	x20, x19
   2ab24:	b.ge	2abf0 <__gmpn_mul@@Base+0x6cc>  // b.tcont
   2ab28:	ldur	x1, [x29, #-120]
   2ab2c:	mov	x2, x19
   2ab30:	mov	x3, x23
   2ab34:	mov	x4, x20
   2ab38:	b	2ac00 <__gmpn_mul@@Base+0x6dc>
   2ab3c:	add	x8, x25, x19, lsl #3
   2ab40:	stur	x8, [x29, #-144]
   2ab44:	mov	w8, #0x18                  	// #24
   2ab48:	madd	x8, x19, x8, x22
   2ab4c:	cmp	x19, #0x4f
   2ab50:	add	x21, x8, #0x8
   2ab54:	lsl	x8, x19, #4
   2ab58:	b.le	2adc4 <__gmpn_mul@@Base+0x8a0>
   2ab5c:	stur	x8, [x29, #-152]
   2ab60:	b	2ab84 <__gmpn_mul@@Base+0x660>
   2ab64:	ldur	x8, [x29, #-136]
   2ab68:	sub	x20, x20, x27
   2ab6c:	add	x26, x26, x28
   2ab70:	add	x23, x23, x28
   2ab74:	cmp	x8, x20, lsl #1
   2ab78:	ldur	x8, [x29, #-152]
   2ab7c:	add	x21, x21, x8
   2ab80:	b.gt	2ab1c <__gmpn_mul@@Base+0x5f8>
   2ab84:	ldp	x5, x3, [x29, #-128]
   2ab88:	mov	x0, x25
   2ab8c:	mov	x1, x23
   2ab90:	mov	x2, x27
   2ab94:	mov	x4, x19
   2ab98:	bl	c740 <__gmpn_toom63_mul@plt>
   2ab9c:	mov	x0, x26
   2aba0:	mov	x1, x26
   2aba4:	mov	x2, x25
   2aba8:	mov	x3, x19
   2abac:	bl	ca70 <__gmpn_add_n@plt>
   2abb0:	ldur	x1, [x29, #-144]
   2abb4:	add	x24, x26, x19, lsl #3
   2abb8:	mov	x22, x0
   2abbc:	mov	x0, x24
   2abc0:	mov	x2, x27
   2abc4:	bl	ca50 <__gmpn_copyi@plt>
   2abc8:	ldr	x8, [x24]
   2abcc:	adds	x8, x8, x22
   2abd0:	str	x8, [x24]
   2abd4:	b.cc	2ab64 <__gmpn_mul@@Base+0x640>  // b.lo, b.ul, b.last
   2abd8:	mov	x8, x21
   2abdc:	ldr	x9, [x8]
   2abe0:	adds	x9, x9, #0x1
   2abe4:	str	x9, [x8], #8
   2abe8:	b.cs	2abdc <__gmpn_mul@@Base+0x6b8>  // b.hs, b.nlast
   2abec:	b	2ab64 <__gmpn_mul@@Base+0x640>
   2abf0:	ldur	x3, [x29, #-120]
   2abf4:	mov	x1, x23
   2abf8:	mov	x2, x20
   2abfc:	mov	x4, x19
   2ac00:	bl	ccd0 <__gmpn_mul@plt>
   2ac04:	mov	x0, x26
   2ac08:	mov	x1, x26
   2ac0c:	mov	x2, x25
   2ac10:	mov	x3, x19
   2ac14:	bl	ca70 <__gmpn_add_n@plt>
   2ac18:	lsl	x23, x19, #3
   2ac1c:	add	x22, x26, x23
   2ac20:	mov	x21, x0
   2ac24:	add	x1, x25, x23
   2ac28:	mov	x0, x22
   2ac2c:	mov	x2, x20
   2ac30:	bl	ca50 <__gmpn_copyi@plt>
   2ac34:	ldr	x8, [x22]
   2ac38:	adds	x8, x8, x21
   2ac3c:	str	x8, [x22]
   2ac40:	b.cc	2ac5c <__gmpn_mul@@Base+0x738>  // b.lo, b.ul, b.last
   2ac44:	add	x8, x23, #0x8
   2ac48:	ldr	x9, [x26, x8]
   2ac4c:	adds	x9, x9, #0x1
   2ac50:	str	x9, [x26, x8]
   2ac54:	add	x8, x8, #0x8
   2ac58:	b.cs	2ac48 <__gmpn_mul@@Base+0x724>  // b.hs, b.nlast
   2ac5c:	mov	x22, x26
   2ac60:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2ac64:	lsl	x26, x19, #3
   2ac68:	add	x8, x26, x19
   2ac6c:	lsl	x8, x8, #2
   2ac70:	and	x1, x8, #0xfffffffffffffff8
   2ac74:	sub	x0, x29, #0x70
   2ac78:	stur	xzr, [x29, #-112]
   2ac7c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ac80:	mov	x24, x0
   2ac84:	mov	x0, x22
   2ac88:	mov	x1, x23
   2ac8c:	mov	x2, x25
   2ac90:	mov	x3, x28
   2ac94:	mov	x4, x19
   2ac98:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   2ac9c:	lsl	x21, x25, #3
   2aca0:	sub	x20, x20, x25
   2aca4:	sub	x9, x26, x19
   2aca8:	add	x8, x22, x21
   2acac:	cmp	x9, x20, lsl #1
   2acb0:	add	x23, x23, x21
   2acb4:	stur	x26, [x29, #-152]
   2acb8:	stur	x9, [x29, #-128]
   2acbc:	b.le	2ae58 <__gmpn_mul@@Base+0x934>
   2acc0:	mov	x22, x8
   2acc4:	mov	x0, x24
   2acc8:	cmp	x20, x19
   2accc:	b.ge	2af08 <__gmpn_mul@@Base+0x9e4>  // b.tcont
   2acd0:	mov	x1, x28
   2acd4:	mov	x2, x19
   2acd8:	mov	x3, x23
   2acdc:	mov	x4, x20
   2ace0:	b	2af18 <__gmpn_mul@@Base+0x9f4>
   2ace4:	bl	d480 <__gmpn_toom42_mul@plt>
   2ace8:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2acec:	bl	d480 <__gmpn_toom42_mul@plt>
   2acf0:	mov	x0, x25
   2acf4:	mov	x1, x25
   2acf8:	mov	x2, x26
   2acfc:	mov	x3, x19
   2ad00:	bl	ca70 <__gmpn_add_n@plt>
   2ad04:	add	x22, x25, x24
   2ad08:	mov	x21, x0
   2ad0c:	add	x1, x26, x24
   2ad10:	mov	x0, x22
   2ad14:	mov	x2, x20
   2ad18:	bl	ca50 <__gmpn_copyi@plt>
   2ad1c:	ldr	x8, [x22]
   2ad20:	adds	x8, x8, x21
   2ad24:	str	x8, [x22]
   2ad28:	b.cc	2ad44 <__gmpn_mul@@Base+0x820>  // b.lo, b.ul, b.last
   2ad2c:	add	x8, x24, #0x8
   2ad30:	ldr	x9, [x25, x8]
   2ad34:	adds	x9, x9, #0x1
   2ad38:	str	x9, [x25, x8]
   2ad3c:	add	x8, x8, #0x8
   2ad40:	b.cs	2ad30 <__gmpn_mul@@Base+0x80c>  // b.hs, b.nlast
   2ad44:	mov	x22, x25
   2ad48:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2ad4c:	mov	w8, #0x18                  	// #24
   2ad50:	mul	x8, x20, x8
   2ad54:	add	x8, x8, #0x20f
   2ad58:	and	x8, x8, #0xfffffffffffffff0
   2ad5c:	mov	x9, sp
   2ad60:	sub	x5, x9, x8
   2ad64:	mov	sp, x5
   2ad68:	mov	x0, x22
   2ad6c:	mov	x1, x23
   2ad70:	mov	x2, x20
   2ad74:	mov	x3, x28
   2ad78:	mov	x4, x19
   2ad7c:	bl	c720 <__gmpn_toom44_mul@plt>
   2ad80:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2ad84:	mov	w9, #0xb                   	// #11
   2ad88:	mul	x9, x19, x9
   2ad8c:	cmp	x10, x9
   2ad90:	b.ge	2afb8 <__gmpn_mul@@Base+0xa94>  // b.tcont
   2ad94:	cmp	x8, x20, lsl #2
   2ad98:	b.le	2afe0 <__gmpn_mul@@Base+0xabc>
   2ad9c:	cmp	x19, #0x4b
   2ada0:	b.gt	2afe8 <__gmpn_mul@@Base+0xac4>
   2ada4:	ldur	x5, [x29, #-128]
   2ada8:	mov	x0, x22
   2adac:	mov	x1, x23
   2adb0:	mov	x2, x20
   2adb4:	mov	x3, x28
   2adb8:	mov	x4, x19
   2adbc:	bl	c850 <__gmpn_toom32_mul@plt>
   2adc0:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2adc4:	stur	x8, [x29, #-152]
   2adc8:	b	2adec <__gmpn_mul@@Base+0x8c8>
   2adcc:	ldur	x8, [x29, #-136]
   2add0:	sub	x20, x20, x27
   2add4:	add	x26, x26, x28
   2add8:	add	x23, x23, x28
   2addc:	cmp	x8, x20, lsl #1
   2ade0:	ldur	x8, [x29, #-152]
   2ade4:	add	x21, x21, x8
   2ade8:	b.gt	2ab1c <__gmpn_mul@@Base+0x5f8>
   2adec:	ldp	x5, x3, [x29, #-128]
   2adf0:	mov	x0, x25
   2adf4:	mov	x1, x23
   2adf8:	mov	x2, x27
   2adfc:	mov	x4, x19
   2ae00:	bl	d480 <__gmpn_toom42_mul@plt>
   2ae04:	mov	x0, x26
   2ae08:	mov	x1, x26
   2ae0c:	mov	x2, x25
   2ae10:	mov	x3, x19
   2ae14:	bl	ca70 <__gmpn_add_n@plt>
   2ae18:	ldur	x1, [x29, #-144]
   2ae1c:	add	x24, x26, x19, lsl #3
   2ae20:	mov	x22, x0
   2ae24:	mov	x0, x24
   2ae28:	mov	x2, x27
   2ae2c:	bl	ca50 <__gmpn_copyi@plt>
   2ae30:	ldr	x8, [x24]
   2ae34:	adds	x8, x8, x22
   2ae38:	str	x8, [x24]
   2ae3c:	b.cc	2adcc <__gmpn_mul@@Base+0x8a8>  // b.lo, b.ul, b.last
   2ae40:	mov	x8, x21
   2ae44:	ldr	x9, [x8]
   2ae48:	adds	x9, x9, #0x1
   2ae4c:	str	x9, [x8], #8
   2ae50:	b.cs	2ae44 <__gmpn_mul@@Base+0x920>  // b.hs, b.nlast
   2ae54:	b	2adcc <__gmpn_mul@@Base+0x8a8>
   2ae58:	add	x9, x24, x19, lsl #3
   2ae5c:	stur	x9, [x29, #-136]
   2ae60:	add	x9, x22, x19, lsl #5
   2ae64:	add	x10, x19, x19, lsl #1
   2ae68:	add	x26, x9, #0x8
   2ae6c:	lsl	x9, x10, #3
   2ae70:	mov	x22, x8
   2ae74:	stur	x9, [x29, #-144]
   2ae78:	b	2ae9c <__gmpn_mul@@Base+0x978>
   2ae7c:	ldp	x8, x28, [x29, #-128]
   2ae80:	sub	x20, x20, x25
   2ae84:	add	x22, x22, x21
   2ae88:	add	x23, x23, x21
   2ae8c:	cmp	x8, x20, lsl #1
   2ae90:	ldur	x8, [x29, #-144]
   2ae94:	add	x26, x26, x8
   2ae98:	b.gt	2acc4 <__gmpn_mul@@Base+0x7a0>
   2ae9c:	mov	x0, x24
   2aea0:	mov	x1, x23
   2aea4:	mov	x2, x25
   2aea8:	mov	x3, x28
   2aeac:	mov	x4, x19
   2aeb0:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   2aeb4:	mov	x0, x22
   2aeb8:	mov	x1, x22
   2aebc:	mov	x2, x24
   2aec0:	mov	x3, x19
   2aec4:	bl	ca70 <__gmpn_add_n@plt>
   2aec8:	ldur	x1, [x29, #-136]
   2aecc:	add	x28, x22, x19, lsl #3
   2aed0:	mov	x27, x0
   2aed4:	mov	x0, x28
   2aed8:	mov	x2, x25
   2aedc:	bl	ca50 <__gmpn_copyi@plt>
   2aee0:	ldr	x8, [x28]
   2aee4:	adds	x8, x8, x27
   2aee8:	str	x8, [x28]
   2aeec:	b.cc	2ae7c <__gmpn_mul@@Base+0x958>  // b.lo, b.ul, b.last
   2aef0:	mov	x8, x26
   2aef4:	ldr	x9, [x8]
   2aef8:	adds	x9, x9, #0x1
   2aefc:	str	x9, [x8], #8
   2af00:	b.cs	2aef4 <__gmpn_mul@@Base+0x9d0>  // b.hs, b.nlast
   2af04:	b	2ae7c <__gmpn_mul@@Base+0x958>
   2af08:	mov	x1, x23
   2af0c:	mov	x2, x20
   2af10:	mov	x3, x28
   2af14:	mov	x4, x19
   2af18:	bl	ccd0 <__gmpn_mul@plt>
   2af1c:	mov	x0, x22
   2af20:	mov	x1, x22
   2af24:	mov	x2, x24
   2af28:	mov	x3, x19
   2af2c:	bl	ca70 <__gmpn_add_n@plt>
   2af30:	ldur	x8, [x29, #-152]
   2af34:	mov	x21, x0
   2af38:	mov	x2, x20
   2af3c:	add	x23, x22, x8
   2af40:	add	x1, x24, x8
   2af44:	mov	x0, x23
   2af48:	bl	ca50 <__gmpn_copyi@plt>
   2af4c:	ldr	x8, [x23]
   2af50:	adds	x8, x8, x21
   2af54:	str	x8, [x23]
   2af58:	b.cc	2b024 <__gmpn_mul@@Base+0xb00>  // b.lo, b.ul, b.last
   2af5c:	add	x8, x22, x19, lsl #3
   2af60:	add	x8, x8, #0x8
   2af64:	ldr	x9, [x8]
   2af68:	adds	x9, x9, #0x1
   2af6c:	str	x9, [x8], #8
   2af70:	b.cs	2af64 <__gmpn_mul@@Base+0xa40>  // b.hs, b.nlast
   2af74:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2af78:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   2af7c:	movk	x9, #0xcccd
   2af80:	umulh	x8, x8, x9
   2af84:	lsr	x8, x8, #3
   2af88:	mov	w9, #0x60                  	// #96
   2af8c:	mov	x10, sp
   2af90:	msub	x8, x8, x9, x10
   2af94:	sub	x5, x8, #0xc60
   2af98:	mov	sp, x5
   2af9c:	mov	x0, x22
   2afa0:	mov	x1, x23
   2afa4:	mov	x2, x20
   2afa8:	mov	x3, x28
   2afac:	mov	x4, x19
   2afb0:	bl	cc20 <__gmpn_toom6h_mul@plt>
   2afb4:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2afb8:	cmp	x19, #0x4f
   2afbc:	b.le	2b008 <__gmpn_mul@@Base+0xae4>
   2afc0:	ldur	x5, [x29, #-128]
   2afc4:	mov	x0, x22
   2afc8:	mov	x1, x23
   2afcc:	mov	x2, x20
   2afd0:	mov	x3, x28
   2afd4:	mov	x4, x19
   2afd8:	bl	c740 <__gmpn_toom63_mul@plt>
   2afdc:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2afe0:	cmp	x19, #0x50
   2afe4:	b.le	2b008 <__gmpn_mul@@Base+0xae4>
   2afe8:	ldur	x5, [x29, #-128]
   2afec:	mov	x0, x22
   2aff0:	mov	x1, x23
   2aff4:	mov	x2, x20
   2aff8:	mov	x3, x28
   2affc:	mov	x4, x19
   2b000:	bl	ca40 <__gmpn_toom53_mul@plt>
   2b004:	b	2b024 <__gmpn_mul@@Base+0xb00>
   2b008:	ldur	x5, [x29, #-128]
   2b00c:	mov	x0, x22
   2b010:	mov	x1, x23
   2b014:	mov	x2, x20
   2b018:	mov	x3, x28
   2b01c:	mov	x4, x19
   2b020:	bl	d480 <__gmpn_toom42_mul@plt>
   2b024:	ldur	x0, [x29, #-112]
   2b028:	cbz	x0, 2a680 <__gmpn_mul@@Base+0x15c>
   2b02c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2b030:	b	2a680 <__gmpn_mul@@Base+0x15c>
   2b034:	sub	x0, x29, #0x70
   2b038:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b03c:	stur	x0, [x29, #-128]
   2b040:	b	2a818 <__gmpn_mul@@Base+0x2f4>
   2b044:	sub	x0, x29, #0x70
   2b048:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b04c:	mov	x25, x0
   2b050:	b	2a990 <__gmpn_mul@@Base+0x46c>
   2b054:	sub	x0, x29, #0x70
   2b058:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b05c:	mov	x5, x0
   2b060:	b	2a7cc <__gmpn_mul@@Base+0x2a8>

000000000002b064 <__gmpn_fft_best_k@@Base>:
   2b064:	adrp	x8, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b068:	add	x8, x8, #0xde0
   2b06c:	mov	w9, #0x1d8                 	// #472
   2b070:	smaddl	x9, w1, w9, x8
   2b074:	ldr	w10, [x9], #4
   2b078:	lsr	w8, w10, #27
   2b07c:	ldr	w10, [x9], #4
   2b080:	and	x11, x10, #0x7ffffff
   2b084:	lsl	x11, x11, x8
   2b088:	cmp	x11, x0
   2b08c:	b.lt	2b078 <__gmpn_fft_best_k@@Base+0x14>  // b.tstop
   2b090:	mov	w0, w8
   2b094:	ret

000000000002b098 <__gmpn_fft_next_size@@Base>:
   2b098:	sub	x8, x0, #0x1
   2b09c:	asr	x8, x8, x1
   2b0a0:	add	x8, x8, #0x1
   2b0a4:	lsl	x0, x8, x1
   2b0a8:	ret

000000000002b0ac <__gmpn_mul_fft@@Base>:
   2b0ac:	sub	sp, sp, #0xd0
   2b0b0:	sub	x9, x1, #0x1
   2b0b4:	asr	x10, x9, x6
   2b0b8:	cmp	x2, x4
   2b0bc:	add	x10, x10, #0x1
   2b0c0:	cset	w8, eq  // eq = none
   2b0c4:	cmp	x3, x5
   2b0c8:	lsl	x10, x10, x6
   2b0cc:	stp	x29, x30, [sp, #112]
   2b0d0:	add	x29, sp, #0x70
   2b0d4:	cset	w9, eq  // eq = none
   2b0d8:	cmp	x10, x1
   2b0dc:	stp	x28, x27, [sp, #128]
   2b0e0:	stp	x26, x25, [sp, #144]
   2b0e4:	stp	x24, x23, [sp, #160]
   2b0e8:	stp	x22, x21, [sp, #176]
   2b0ec:	stp	x20, x19, [sp, #192]
   2b0f0:	stp	x2, x3, [x29, #-48]
   2b0f4:	b.ne	2b530 <__gmpn_mul_fft@@Base+0x484>  // b.any
   2b0f8:	add	w23, w6, #0x1
   2b0fc:	mov	x25, x1
   2b100:	mov	x24, x0
   2b104:	lsl	x21, x1, #6
   2b108:	sbfiz	x1, x23, #3, #32
   2b10c:	sub	x0, x29, #0x8
   2b110:	mov	x19, x5
   2b114:	mov	x27, x4
   2b118:	and	w26, w8, w9
   2b11c:	mov	w20, w6
   2b120:	stp	x6, xzr, [x29, #-16]
   2b124:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b128:	mov	w8, #0x8                   	// #8
   2b12c:	mov	x22, x0
   2b130:	lsl	x1, x8, x20
   2b134:	sub	x0, x29, #0x8
   2b138:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b13c:	ldur	x5, [x29, #-16]
   2b140:	stur	x24, [x29, #-32]
   2b144:	tbnz	w5, #31, 2b2f8 <__gmpn_mul_fft@@Base+0x24c>
   2b148:	mov	x9, xzr
   2b14c:	mov	w8, w23
   2b150:	mov	w10, #0x1                   	// #1
   2b154:	str	x0, [x22, x9, lsl #3]
   2b158:	lsl	x11, x10, x9
   2b15c:	add	x9, x9, #0x1
   2b160:	cmp	x8, x9
   2b164:	add	x0, x0, x11, lsl #2
   2b168:	b.ne	2b154 <__gmpn_mul_fft@@Base+0xa8>  // b.any
   2b16c:	ldr	x9, [x22]
   2b170:	cmp	w5, #0x0
   2b174:	str	wzr, [x9]
   2b178:	b.le	2b300 <__gmpn_mul_fft@@Base+0x254>
   2b17c:	mov	w9, #0x1                   	// #1
   2b180:	mov	w10, #0x1                   	// #1
   2b184:	b	2b198 <__gmpn_mul_fft@@Base+0xec>
   2b188:	add	x9, x9, #0x1
   2b18c:	cmp	x9, x8
   2b190:	lsl	w10, w10, #1
   2b194:	b.eq	2b2a4 <__gmpn_mul_fft@@Base+0x1f8>  // b.none
   2b198:	cbz	w10, 2b188 <__gmpn_mul_fft@@Base+0xdc>
   2b19c:	add	x12, x22, x9, lsl #3
   2b1a0:	ldr	x11, [x22, x9, lsl #3]
   2b1a4:	ldur	x12, [x12, #-8]
   2b1a8:	sxtw	x13, w10
   2b1ac:	cmp	w10, #0x8
   2b1b0:	mov	w14, w10
   2b1b4:	mov	x15, xzr
   2b1b8:	b.cs	2b1f8 <__gmpn_mul_fft@@Base+0x14c>  // b.hs, b.nlast
   2b1bc:	add	x16, x15, x13
   2b1c0:	sub	x13, x14, x15
   2b1c4:	lsl	x15, x15, #2
   2b1c8:	add	x14, x11, x16, lsl #2
   2b1cc:	add	x11, x11, x15
   2b1d0:	add	x12, x12, x15
   2b1d4:	ldr	w15, [x12], #4
   2b1d8:	mov	w16, #0x1                   	// #1
   2b1dc:	subs	x13, x13, #0x1
   2b1e0:	lsl	w17, w15, #1
   2b1e4:	bfi	w16, w15, #1, #31
   2b1e8:	str	w17, [x11], #4
   2b1ec:	str	w16, [x14], #4
   2b1f0:	b.ne	2b1d4 <__gmpn_mul_fft@@Base+0x128>  // b.any
   2b1f4:	b	2b188 <__gmpn_mul_fft@@Base+0xdc>
   2b1f8:	add	x18, x13, x14
   2b1fc:	lsl	x16, x14, #2
   2b200:	add	x1, x11, x18, lsl #2
   2b204:	add	x17, x11, x13, lsl #2
   2b208:	add	x0, x11, x16
   2b20c:	cmp	x11, x1
   2b210:	add	x2, x12, x16
   2b214:	cset	w3, cc  // cc = lo, ul, last
   2b218:	cmp	x17, x0
   2b21c:	cset	w4, cc  // cc = lo, ul, last
   2b220:	cmp	x11, x2
   2b224:	cset	w16, cc  // cc = lo, ul, last
   2b228:	cmp	x12, x0
   2b22c:	cset	w18, cc  // cc = lo, ul, last
   2b230:	cmp	x17, x2
   2b234:	cset	w17, cc  // cc = lo, ul, last
   2b238:	cmp	x12, x1
   2b23c:	and	w2, w3, w4
   2b240:	cset	w0, cc  // cc = lo, ul, last
   2b244:	tbnz	w2, #0, 2b1bc <__gmpn_mul_fft@@Base+0x110>
   2b248:	and	w16, w16, w18
   2b24c:	tbnz	w16, #0, 2b1bc <__gmpn_mul_fft@@Base+0x110>
   2b250:	and	w16, w17, w0
   2b254:	tbnz	w16, #0, 2b1bc <__gmpn_mul_fft@@Base+0x110>
   2b258:	and	x15, x14, #0xfffffff8
   2b25c:	add	x16, x12, #0x10
   2b260:	lsl	x17, x13, #2
   2b264:	mov	x18, x15
   2b268:	mov	x0, x11
   2b26c:	ldp	q0, q1, [x16, #-16]
   2b270:	add	x1, x0, x17
   2b274:	add	x16, x16, #0x20
   2b278:	subs	x18, x18, #0x8
   2b27c:	shl	v0.4s, v0.4s, #1
   2b280:	shl	v1.4s, v1.4s, #1
   2b284:	stp	q0, q1, [x0], #32
   2b288:	orr	v0.4s, #0x1
   2b28c:	orr	v1.4s, #0x1
   2b290:	stp	q0, q1, [x1]
   2b294:	b.ne	2b26c <__gmpn_mul_fft@@Base+0x1c0>  // b.any
   2b298:	cmp	x15, x14
   2b29c:	b.eq	2b188 <__gmpn_mul_fft@@Base+0xdc>  // b.none
   2b2a0:	b	2b1bc <__gmpn_mul_fft@@Base+0x110>
   2b2a4:	mov	w9, #0x1                   	// #1
   2b2a8:	asr	x8, x21, x20
   2b2ac:	lsl	x28, x9, x20
   2b2b0:	sub	x9, x8, #0x1
   2b2b4:	add	x10, x8, #0x3e
   2b2b8:	cmp	x9, #0x0
   2b2bc:	csel	x9, x10, x9, lt  // lt = tstop
   2b2c0:	asr	x9, x9, #6
   2b2c4:	mov	x15, x19
   2b2c8:	cmp	w5, #0x1
   2b2cc:	add	x19, x9, #0x1
   2b2d0:	mov	w9, #0x40                  	// #64
   2b2d4:	b.lt	2b32c <__gmpn_mul_fft@@Base+0x280>  // b.tstop
   2b2d8:	mov	w10, w5
   2b2dc:	mov	x11, x9
   2b2e0:	cmp	w10, #0x2
   2b2e4:	lsr	x9, x9, #1
   2b2e8:	b.lt	2b32c <__gmpn_mul_fft@@Base+0x280>  // b.tstop
   2b2ec:	sub	w10, w10, #0x1
   2b2f0:	tbz	w11, #1, 2b2dc <__gmpn_mul_fft@@Base+0x230>
   2b2f4:	b	2b32c <__gmpn_mul_fft@@Base+0x280>
   2b2f8:	ldr	x8, [x22]
   2b2fc:	str	wzr, [x8]
   2b300:	mov	w9, #0x1                   	// #1
   2b304:	asr	x8, x21, x20
   2b308:	lsl	x28, x9, x20
   2b30c:	sub	x9, x8, #0x1
   2b310:	add	x10, x8, #0x3e
   2b314:	cmp	x9, #0x0
   2b318:	csel	x9, x10, x9, lt  // lt = tstop
   2b31c:	asr	x9, x9, #6
   2b320:	mov	x15, x19
   2b324:	add	x19, x9, #0x1
   2b328:	mov	w9, #0x40                  	// #64
   2b32c:	lsl	x8, x8, #1
   2b330:	add	x8, x8, w5, sxtw
   2b334:	lsl	x9, x9, x20
   2b338:	add	x8, x8, #0x2
   2b33c:	sdiv	x8, x8, x9
   2b340:	add	x8, x8, #0x1
   2b344:	mul	x24, x8, x9
   2b348:	add	x8, x24, #0x3f
   2b34c:	cmp	x24, #0x0
   2b350:	mov	w10, #0x13c                 	// #316
   2b354:	mov	w11, #0x110                 	// #272
   2b358:	csel	x8, x8, x24, lt  // lt = tstop
   2b35c:	cmp	w26, #0x0
   2b360:	asr	x21, x8, #6
   2b364:	csel	x8, x11, x10, ne  // ne = any
   2b368:	cmp	x21, x8
   2b36c:	str	x27, [sp, #40]
   2b370:	b.ge	2b428 <__gmpn_mul_fft@@Base+0x37c>  // b.tcont
   2b374:	cmp	x21, x25
   2b378:	str	x15, [sp, #48]
   2b37c:	stur	w26, [x29, #-52]
   2b380:	stur	x25, [x29, #-24]
   2b384:	b.ge	2b548 <__gmpn_mul_fft@@Base+0x49c>  // b.tcont
   2b388:	add	x25, x21, #0x1
   2b38c:	lsl	x1, x25, #4
   2b390:	sub	x0, x29, #0x8
   2b394:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b398:	lsl	x8, x25, x20
   2b39c:	asr	x24, x24, x20
   2b3a0:	lsl	x20, x8, #3
   2b3a4:	mov	x23, x0
   2b3a8:	sub	x0, x29, #0x8
   2b3ac:	mov	x1, x20
   2b3b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3b4:	lsl	x26, x28, #3
   2b3b8:	mov	x27, x0
   2b3bc:	sub	x0, x29, #0x8
   2b3c0:	mov	x1, x26
   2b3c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3c8:	ldp	x4, x5, [x29, #-48]
   2b3cc:	mov	x25, x0
   2b3d0:	mov	x0, x27
   2b3d4:	mov	x1, x25
   2b3d8:	mov	x2, x28
   2b3dc:	mov	x3, x21
   2b3e0:	mov	x6, x19
   2b3e4:	mov	x7, x24
   2b3e8:	str	x23, [sp]
   2b3ec:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2b3f0:	ldur	w27, [x29, #-52]
   2b3f4:	cbz	w27, 2b484 <__gmpn_mul_fft@@Base+0x3d8>
   2b3f8:	sub	x8, x28, #0x1
   2b3fc:	madd	x8, x8, x19, x21
   2b400:	lsl	x8, x8, #3
   2b404:	add	x1, x8, #0x8
   2b408:	sub	x0, x29, #0x8
   2b40c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b410:	mov	x20, x0
   2b414:	sub	x0, x29, #0x8
   2b418:	mov	x1, x26
   2b41c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b420:	mov	x26, x0
   2b424:	b	2b4c8 <__gmpn_mul_fft@@Base+0x41c>
   2b428:	adrp	x8, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b42c:	add	x8, x8, #0xde0
   2b430:	mov	w9, #0x1d8                 	// #472
   2b434:	umaddl	x8, w26, w9, x8
   2b438:	ldr	w9, [x8], #4
   2b43c:	mov	w10, #0x1                   	// #1
   2b440:	mov	x11, x8
   2b444:	mov	w13, w9
   2b448:	lsr	w12, w13, #27
   2b44c:	ldr	w13, [x11], #4
   2b450:	and	x14, x13, #0x7ffffff
   2b454:	lsl	x14, x14, x12
   2b458:	cmp	x14, x21
   2b45c:	b.lt	2b448 <__gmpn_mul_fft@@Base+0x39c>  // b.tstop
   2b460:	lsl	x11, x10, x12
   2b464:	sub	x12, x11, #0x1
   2b468:	tst	x12, x21
   2b46c:	b.eq	2b374 <__gmpn_mul_fft@@Base+0x2c8>  // b.none
   2b470:	add	x12, x12, x21
   2b474:	neg	x11, x11
   2b478:	and	x21, x12, x11
   2b47c:	lsl	x24, x21, #6
   2b480:	b	2b440 <__gmpn_mul_fft@@Base+0x394>
   2b484:	sub	x0, x29, #0x8
   2b488:	mov	x1, x20
   2b48c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b490:	mov	x20, x0
   2b494:	sub	x0, x29, #0x8
   2b498:	mov	x1, x26
   2b49c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b4a0:	ldp	x4, x5, [sp, #40]
   2b4a4:	mov	x26, x0
   2b4a8:	mov	x0, x20
   2b4ac:	mov	x1, x26
   2b4b0:	mov	x2, x28
   2b4b4:	mov	x3, x21
   2b4b8:	mov	x6, x19
   2b4bc:	mov	x7, x24
   2b4c0:	str	x23, [sp]
   2b4c4:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2b4c8:	ldp	x0, x1, [x29, #-32]
   2b4cc:	ldur	x2, [x29, #-16]
   2b4d0:	mov	x3, x25
   2b4d4:	mov	x4, x26
   2b4d8:	mov	x5, x20
   2b4dc:	mov	x6, x21
   2b4e0:	mov	x7, x19
   2b4e4:	str	w27, [sp, #24]
   2b4e8:	stp	x22, x23, [sp, #8]
   2b4ec:	str	x24, [sp]
   2b4f0:	bl	2b9d0 <__gmpn_mul_fft@@Base+0x924>
   2b4f4:	ldur	x8, [x29, #-8]
   2b4f8:	mov	x19, x0
   2b4fc:	cbnz	x8, 2b524 <__gmpn_mul_fft@@Base+0x478>
   2b500:	mov	x0, x19
   2b504:	ldp	x20, x19, [sp, #192]
   2b508:	ldp	x22, x21, [sp, #176]
   2b50c:	ldp	x24, x23, [sp, #160]
   2b510:	ldp	x26, x25, [sp, #144]
   2b514:	ldp	x28, x27, [sp, #128]
   2b518:	ldp	x29, x30, [sp, #112]
   2b51c:	add	sp, sp, #0xd0
   2b520:	ret
   2b524:	mov	x0, x8
   2b528:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2b52c:	b	2b500 <__gmpn_mul_fft@@Base+0x454>
   2b530:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b534:	adrp	x2, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b538:	add	x0, x0, #0xd7e
   2b53c:	add	x2, x2, #0xd88
   2b540:	mov	w1, #0x365                 	// #869
   2b544:	bl	c6c0 <__gmp_assert_fail@plt>
   2b548:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b54c:	adrp	x2, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b550:	add	x0, x0, #0xd7e
   2b554:	add	x2, x2, #0xdab
   2b558:	mov	w1, #0x38b                 	// #907
   2b55c:	bl	c6c0 <__gmp_assert_fail@plt>
   2b560:	sub	sp, sp, #0x90
   2b564:	stp	x26, x25, [sp, #80]
   2b568:	mul	x26, x6, x2
   2b56c:	stp	x29, x30, [sp, #48]
   2b570:	stp	x28, x27, [sp, #64]
   2b574:	stp	x22, x21, [sp, #112]
   2b578:	stp	x20, x19, [sp, #128]
   2b57c:	add	x29, sp, #0x30
   2b580:	mov	x19, x5
   2b584:	mov	x28, x4
   2b588:	mov	x21, x3
   2b58c:	mov	x25, x0
   2b590:	cmp	x26, x5
   2b594:	stp	x24, x23, [sp, #96]
   2b598:	str	x1, [sp, #16]
   2b59c:	stp	x6, xzr, [x29, #-16]
   2b5a0:	str	x2, [sp, #24]
   2b5a4:	str	x7, [sp, #8]
   2b5a8:	b.ge	2b644 <__gmpn_mul_fft@@Base+0x598>  // b.tcont
   2b5ac:	sub	x22, x19, x26
   2b5b0:	add	x19, x26, #0x1
   2b5b4:	lsl	x1, x19, #3
   2b5b8:	sub	x0, x29, #0x8
   2b5bc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b5c0:	mov	x27, x0
   2b5c4:	subs	x20, x22, x26
   2b5c8:	add	x2, x28, x26, lsl #3
   2b5cc:	b.le	2b658 <__gmpn_mul_fft@@Base+0x5ac>
   2b5d0:	mov	x0, x27
   2b5d4:	mov	x1, x28
   2b5d8:	mov	x3, x26
   2b5dc:	bl	c2d0 <__gmpn_sub_n@plt>
   2b5e0:	mov	x22, x0
   2b5e4:	cmp	x20, x26
   2b5e8:	add	x28, x28, x26, lsl #4
   2b5ec:	b.le	2b75c <__gmpn_mul_fft@@Base+0x6b0>
   2b5f0:	mov	w8, wzr
   2b5f4:	mov	w23, wzr
   2b5f8:	lsl	x24, x26, #3
   2b5fc:	b	2b624 <__gmpn_mul_fft@@Base+0x578>
   2b600:	bl	ca70 <__gmpn_add_n@plt>
   2b604:	sub	x22, x22, x0
   2b608:	eor	w23, w23, #0x1
   2b60c:	sub	x20, x20, x26
   2b610:	cmp	w23, #0x0
   2b614:	cset	w8, ne  // ne = any
   2b618:	cmp	x20, x26
   2b61c:	add	x28, x28, x24
   2b620:	b.le	2b6c4 <__gmpn_mul_fft@@Base+0x618>
   2b624:	mov	x0, x27
   2b628:	mov	x1, x27
   2b62c:	mov	x2, x28
   2b630:	mov	x3, x26
   2b634:	tbz	w8, #0, 2b600 <__gmpn_mul_fft@@Base+0x554>
   2b638:	bl	c2d0 <__gmpn_sub_n@plt>
   2b63c:	add	x22, x0, x22
   2b640:	b	2b608 <__gmpn_mul_fft@@Base+0x55c>
   2b644:	mov	x27, x7
   2b648:	ldr	x8, [sp, #24]
   2b64c:	subs	x24, x8, #0x1
   2b650:	b.ge	2b83c <__gmpn_mul_fft@@Base+0x790>  // b.tcont
   2b654:	b	2b97c <__gmpn_mul_fft@@Base+0x8d0>
   2b658:	cbz	x22, 2b690 <__gmpn_mul_fft@@Base+0x5e4>
   2b65c:	mov	x0, x27
   2b660:	mov	x1, x28
   2b664:	mov	x3, x22
   2b668:	bl	c2d0 <__gmpn_sub_n@plt>
   2b66c:	cbz	x0, 2b690 <__gmpn_mul_fft@@Base+0x5e4>
   2b670:	cmp	x22, x26
   2b674:	b.ge	2b7e4 <__gmpn_mul_fft@@Base+0x738>  // b.tcont
   2b678:	lsl	x8, x22, #3
   2b67c:	ldr	x9, [x28, x8]
   2b680:	add	x22, x22, #0x1
   2b684:	sub	x10, x9, #0x1
   2b688:	str	x10, [x27, x8]
   2b68c:	cbz	x9, 2b670 <__gmpn_mul_fft@@Base+0x5c4>
   2b690:	cmp	x27, x28
   2b694:	mov	x8, xzr
   2b698:	b.eq	2b824 <__gmpn_mul_fft@@Base+0x778>  // b.none
   2b69c:	cmp	x22, x26
   2b6a0:	b.ge	2b824 <__gmpn_mul_fft@@Base+0x778>  // b.tcont
   2b6a4:	lsl	x8, x22, #3
   2b6a8:	lsl	x9, x26, #3
   2b6ac:	add	x0, x27, x8
   2b6b0:	add	x1, x28, x8
   2b6b4:	sub	x2, x9, x8
   2b6b8:	bl	bed0 <memcpy@plt>
   2b6bc:	mov	x8, xzr
   2b6c0:	b	2b824 <__gmpn_mul_fft@@Base+0x778>
   2b6c4:	cbz	w23, 2b75c <__gmpn_mul_fft@@Base+0x6b0>
   2b6c8:	cbz	x20, 2b708 <__gmpn_mul_fft@@Base+0x65c>
   2b6cc:	mov	x0, x27
   2b6d0:	mov	x1, x27
   2b6d4:	mov	x2, x28
   2b6d8:	mov	x3, x20
   2b6dc:	bl	c2d0 <__gmpn_sub_n@plt>
   2b6e0:	cbz	x0, 2b708 <__gmpn_mul_fft@@Base+0x65c>
   2b6e4:	mov	w8, #0x1                   	// #1
   2b6e8:	cmp	x20, x26
   2b6ec:	b.ge	2b70c <__gmpn_mul_fft@@Base+0x660>  // b.tcont
   2b6f0:	lsl	x9, x20, #3
   2b6f4:	ldr	x10, [x27, x9]
   2b6f8:	add	x20, x20, #0x1
   2b6fc:	sub	x11, x10, #0x1
   2b700:	str	x11, [x27, x9]
   2b704:	cbz	x10, 2b6e8 <__gmpn_mul_fft@@Base+0x63c>
   2b708:	mov	x8, xzr
   2b70c:	add	x9, x8, x22
   2b710:	tbz	x9, #63, 2b7a8 <__gmpn_mul_fft@@Base+0x6fc>
   2b714:	ldr	x10, [x27]
   2b718:	neg	x11, x9
   2b71c:	mov	x8, xzr
   2b720:	add	x9, x10, x9
   2b724:	cmp	x10, x11
   2b728:	str	x9, [x27]
   2b72c:	b.cs	2b824 <__gmpn_mul_fft@@Base+0x778>  // b.hs, b.nlast
   2b730:	mov	w8, #0x1                   	// #1
   2b734:	mov	w9, #0x1                   	// #1
   2b738:	cmp	x9, x26
   2b73c:	b.ge	2b824 <__gmpn_mul_fft@@Base+0x778>  // b.tcont
   2b740:	lsl	x10, x9, #3
   2b744:	ldr	x11, [x27, x10]
   2b748:	add	x9, x9, #0x1
   2b74c:	sub	x12, x11, #0x1
   2b750:	str	x12, [x27, x10]
   2b754:	cbz	x11, 2b738 <__gmpn_mul_fft@@Base+0x68c>
   2b758:	b	2b820 <__gmpn_mul_fft@@Base+0x774>
   2b75c:	cbz	x20, 2b79c <__gmpn_mul_fft@@Base+0x6f0>
   2b760:	mov	x0, x27
   2b764:	mov	x1, x27
   2b768:	mov	x2, x28
   2b76c:	mov	x3, x20
   2b770:	bl	ca70 <__gmpn_add_n@plt>
   2b774:	cbz	x0, 2b79c <__gmpn_mul_fft@@Base+0x6f0>
   2b778:	mov	w8, #0x1                   	// #1
   2b77c:	cmp	x20, x26
   2b780:	b.ge	2b7a0 <__gmpn_mul_fft@@Base+0x6f4>  // b.tcont
   2b784:	lsl	x9, x20, #3
   2b788:	ldr	x10, [x27, x9]
   2b78c:	add	x20, x20, #0x1
   2b790:	adds	x10, x10, #0x1
   2b794:	str	x10, [x27, x9]
   2b798:	b.cs	2b77c <__gmpn_mul_fft@@Base+0x6d0>  // b.hs, b.nlast
   2b79c:	mov	x8, xzr
   2b7a0:	sub	x9, x22, x8
   2b7a4:	tbnz	x9, #63, 2b714 <__gmpn_mul_fft@@Base+0x668>
   2b7a8:	ldr	x8, [x27]
   2b7ac:	adds	x8, x8, x9
   2b7b0:	str	x8, [x27]
   2b7b4:	b.cc	2b820 <__gmpn_mul_fft@@Base+0x774>  // b.lo, b.ul, b.last
   2b7b8:	mov	w8, #0x1                   	// #1
   2b7bc:	mov	w9, #0x1                   	// #1
   2b7c0:	cmp	x9, x26
   2b7c4:	b.ge	2b824 <__gmpn_mul_fft@@Base+0x778>  // b.tcont
   2b7c8:	lsl	x10, x9, #3
   2b7cc:	ldr	x11, [x27, x10]
   2b7d0:	add	x9, x9, #0x1
   2b7d4:	adds	x11, x11, #0x1
   2b7d8:	str	x11, [x27, x10]
   2b7dc:	b.cs	2b7c0 <__gmpn_mul_fft@@Base+0x714>  // b.hs, b.nlast
   2b7e0:	b	2b820 <__gmpn_mul_fft@@Base+0x774>
   2b7e4:	ldr	x8, [x27]
   2b7e8:	adds	x8, x8, #0x1
   2b7ec:	str	x8, [x27]
   2b7f0:	b.cc	2b820 <__gmpn_mul_fft@@Base+0x774>  // b.lo, b.ul, b.last
   2b7f4:	mov	w9, #0x1                   	// #1
   2b7f8:	cmp	x9, x26
   2b7fc:	b.ge	2b9a8 <__gmpn_mul_fft@@Base+0x8fc>  // b.tcont
   2b800:	lsl	x10, x9, #3
   2b804:	ldr	x11, [x27, x10]
   2b808:	mov	x8, xzr
   2b80c:	add	x9, x9, #0x1
   2b810:	adds	x11, x11, #0x1
   2b814:	str	x11, [x27, x10]
   2b818:	b.cs	2b7f8 <__gmpn_mul_fft@@Base+0x74c>  // b.hs, b.nlast
   2b81c:	b	2b824 <__gmpn_mul_fft@@Base+0x778>
   2b820:	mov	x8, xzr
   2b824:	str	x8, [x27, x26, lsl #3]
   2b828:	mov	x28, x27
   2b82c:	ldr	x27, [sp, #8]
   2b830:	ldr	x8, [sp, #24]
   2b834:	subs	x24, x8, #0x1
   2b838:	b.lt	2b97c <__gmpn_mul_fft@@Base+0x8d0>  // b.tstop
   2b83c:	ldr	x22, [x29, #96]
   2b840:	mov	x20, xzr
   2b844:	mov	x23, xzr
   2b848:	adds	x9, x21, #0x1
   2b84c:	b.cs	2b894 <__gmpn_mul_fft@@Base+0x7e8>  // b.hs, b.nlast
   2b850:	lsl	x8, x21, #3
   2b854:	str	x21, [sp]
   2b858:	add	x26, x8, #0x8
   2b85c:	mov	x21, x9
   2b860:	b	2b90c <__gmpn_mul_fft@@Base+0x860>
   2b864:	ldur	x8, [x29, #-16]
   2b868:	mov	x0, x25
   2b86c:	mov	x1, x22
   2b870:	mov	x2, x20
   2b874:	mov	x3, x21
   2b878:	add	x28, x28, x8, lsl #3
   2b87c:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2b880:	ldr	x8, [sp, #24]
   2b884:	add	x23, x23, #0x1
   2b888:	add	x20, x20, x27
   2b88c:	cmp	x8, x23
   2b890:	b.eq	2b97c <__gmpn_mul_fft@@Base+0x8d0>  // b.none
   2b894:	ldr	x8, [sp, #16]
   2b898:	cmp	x19, #0x1
   2b89c:	str	x25, [x8, x23, lsl #3]
   2b8a0:	b.lt	2b880 <__gmpn_mul_fft@@Base+0x7d4>  // b.tstop
   2b8a4:	ldur	x8, [x29, #-16]
   2b8a8:	mov	x0, x22
   2b8ac:	mov	x1, x28
   2b8b0:	cmp	x19, x8
   2b8b4:	ccmp	x23, x24, #0x0, ge  // ge = tcont
   2b8b8:	csel	x26, x8, x19, lt  // lt = tstop
   2b8bc:	mov	x2, x26
   2b8c0:	sub	x19, x19, x26
   2b8c4:	bl	ca50 <__gmpn_copyi@plt>
   2b8c8:	cbz	x26, 2b864 <__gmpn_mul_fft@@Base+0x7b8>
   2b8cc:	lsl	x8, x26, #3
   2b8d0:	add	x0, x22, x8
   2b8d4:	neg	x2, x8
   2b8d8:	mov	w1, wzr
   2b8dc:	bl	c5f0 <memset@plt>
   2b8e0:	b	2b864 <__gmpn_mul_fft@@Base+0x7b8>
   2b8e4:	mov	x0, x25
   2b8e8:	mov	w1, wzr
   2b8ec:	mov	x2, x26
   2b8f0:	bl	c5f0 <memset@plt>
   2b8f4:	ldr	x8, [sp, #24]
   2b8f8:	add	x23, x23, #0x1
   2b8fc:	add	x25, x25, x26
   2b900:	add	x20, x20, x27
   2b904:	cmp	x8, x23
   2b908:	b.eq	2b97c <__gmpn_mul_fft@@Base+0x8d0>  // b.none
   2b90c:	ldr	x8, [sp, #16]
   2b910:	cmp	x19, #0x0
   2b914:	str	x25, [x8, x23, lsl #3]
   2b918:	b.le	2b8e4 <__gmpn_mul_fft@@Base+0x838>
   2b91c:	ldur	x8, [x29, #-16]
   2b920:	mov	x0, x22
   2b924:	mov	x1, x28
   2b928:	cmp	x19, x8
   2b92c:	ccmp	x23, x24, #0x0, ge  // ge = tcont
   2b930:	csel	x27, x8, x19, lt  // lt = tstop
   2b934:	mov	x2, x27
   2b938:	sub	x19, x19, x27
   2b93c:	bl	ca50 <__gmpn_copyi@plt>
   2b940:	subs	x8, x21, x27
   2b944:	b.eq	2b958 <__gmpn_mul_fft@@Base+0x8ac>  // b.none
   2b948:	add	x0, x22, x27, lsl #3
   2b94c:	lsl	x2, x8, #3
   2b950:	mov	w1, wzr
   2b954:	bl	c5f0 <memset@plt>
   2b958:	ldur	x8, [x29, #-16]
   2b95c:	ldr	x3, [sp]
   2b960:	mov	x0, x25
   2b964:	mov	x1, x22
   2b968:	mov	x2, x20
   2b96c:	add	x28, x28, x8, lsl #3
   2b970:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2b974:	ldr	x27, [sp, #8]
   2b978:	b	2b8f4 <__gmpn_mul_fft@@Base+0x848>
   2b97c:	cbnz	x19, 2b9b8 <__gmpn_mul_fft@@Base+0x90c>
   2b980:	ldur	x0, [x29, #-8]
   2b984:	cbnz	x0, 2b9b0 <__gmpn_mul_fft@@Base+0x904>
   2b988:	ldp	x20, x19, [sp, #128]
   2b98c:	ldp	x22, x21, [sp, #112]
   2b990:	ldp	x24, x23, [sp, #96]
   2b994:	ldp	x26, x25, [sp, #80]
   2b998:	ldp	x28, x27, [sp, #64]
   2b99c:	ldp	x29, x30, [sp, #48]
   2b9a0:	add	sp, sp, #0x90
   2b9a4:	ret
   2b9a8:	mov	w8, #0x1                   	// #1
   2b9ac:	b	2b824 <__gmpn_mul_fft@@Base+0x778>
   2b9b0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2b9b4:	b	2b988 <__gmpn_mul_fft@@Base+0x8dc>
   2b9b8:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b9bc:	adrp	x2, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2b9c0:	add	x0, x0, #0xd7e
   2b9c4:	add	x2, x2, #0xdb7
   2b9c8:	mov	w1, #0x2e7                 	// #743
   2b9cc:	bl	c6c0 <__gmp_assert_fail@plt>
   2b9d0:	sub	sp, sp, #0x160
   2b9d4:	stp	x29, x30, [sp, #256]
   2b9d8:	add	x29, sp, #0x100
   2b9dc:	ldp	x10, x8, [x29, #96]
   2b9e0:	stp	x26, x25, [sp, #288]
   2b9e4:	ldr	x26, [x29, #112]
   2b9e8:	stp	x20, x19, [sp, #336]
   2b9ec:	ldr	w20, [x29, #120]
   2b9f0:	stp	x28, x27, [sp, #272]
   2b9f4:	mov	w27, w2
   2b9f8:	mov	w9, #0x1                   	// #1
   2b9fc:	add	x19, x8, w2, sxtw #3
   2ba00:	lsl	x25, x10, #1
   2ba04:	stp	x24, x23, [sp, #304]
   2ba08:	stp	x22, x21, [sp, #320]
   2ba0c:	stp	x7, x5, [x29, #-64]
   2ba10:	mov	x24, x6
   2ba14:	mov	x21, x4
   2ba18:	mov	x23, x3
   2ba1c:	mov	x22, x1
   2ba20:	str	x0, [sp, #48]
   2ba24:	lsl	x1, x9, x27
   2ba28:	mov	w5, #0x1                   	// #1
   2ba2c:	mov	x0, x3
   2ba30:	mov	x2, x19
   2ba34:	mov	x3, x25
   2ba38:	mov	x4, x6
   2ba3c:	mov	x6, x26
   2ba40:	stur	x10, [x29, #-72]
   2ba44:	stur	x1, [x29, #-24]
   2ba48:	bl	2cf34 <__gmpn_mul_fft@@Base+0x1e88>
   2ba4c:	cbnz	w20, 2ba70 <__gmpn_mul_fft@@Base+0x9c4>
   2ba50:	ldur	x1, [x29, #-24]
   2ba54:	mov	w5, #0x1                   	// #1
   2ba58:	mov	x0, x21
   2ba5c:	mov	x2, x19
   2ba60:	mov	x3, x25
   2ba64:	mov	x4, x24
   2ba68:	mov	x6, x26
   2ba6c:	bl	2cf34 <__gmpn_mul_fft@@Base+0x1e88>
   2ba70:	cmp	w20, #0x0
   2ba74:	csel	x11, x23, x21, ne  // ne = any
   2ba78:	cmp	x11, x23
   2ba7c:	mov	w8, #0x13c                 	// #316
   2ba80:	mov	w9, #0x110                 	// #272
   2ba84:	cset	w10, eq  // eq = none
   2ba88:	stur	w10, [x29, #-104]
   2ba8c:	csel	x10, x9, x8, eq  // eq = none
   2ba90:	cmp	x10, x24
   2ba94:	lsl	x15, x24, #3
   2ba98:	stur	xzr, [x29, #-16]
   2ba9c:	stp	x21, x24, [x29, #-40]
   2baa0:	stur	x23, [x29, #-80]
   2baa4:	stur	x11, [x29, #-48]
   2baa8:	stp	x22, x27, [sp, #56]
   2baac:	str	x15, [sp, #80]
   2bab0:	str	x25, [sp, #40]
   2bab4:	b.le	2bc18 <__gmpn_mul_fft@@Base+0xb6c>
   2bab8:	lsl	x1, x24, #4
   2babc:	sub	x0, x29, #0x10
   2bac0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bac4:	cmp	w27, #0x3f
   2bac8:	b.eq	2be9c <__gmpn_mul_fft@@Base+0xdf0>  // b.none
   2bacc:	ldur	x27, [x29, #-80]
   2bad0:	ldur	x8, [x29, #-48]
   2bad4:	mov	x19, x0
   2bad8:	lsl	x25, x24, #1
   2badc:	add	x20, x0, x24, lsl #3
   2bae0:	cmp	x8, x27
   2bae4:	mov	x22, xzr
   2bae8:	b.eq	2c5d0 <__gmpn_mul_fft@@Base+0x1524>  // b.none
   2baec:	ldur	x28, [x29, #-32]
   2baf0:	b	2bb0c <__gmpn_mul_fft@@Base+0xa60>
   2baf4:	mov	x8, xzr
   2baf8:	ldur	x9, [x29, #-24]
   2bafc:	add	x22, x22, #0x1
   2bb00:	str	x8, [x21, x28, lsl #3]
   2bb04:	cmp	x9, x22
   2bb08:	b.le	2be9c <__gmpn_mul_fft@@Base+0xdf0>
   2bb0c:	ldr	x21, [x27], #8
   2bb10:	ldur	x8, [x29, #-48]
   2bb14:	mov	x0, x19
   2bb18:	mov	x3, x28
   2bb1c:	mov	x2, x21
   2bb20:	ldr	x24, [x8], #8
   2bb24:	mov	x1, x24
   2bb28:	stur	x8, [x29, #-48]
   2bb2c:	bl	c990 <__gmpn_mul_n@plt>
   2bb30:	ldr	x8, [x21, x28, lsl #3]
   2bb34:	cbz	x8, 2bc00 <__gmpn_mul_fft@@Base+0xb54>
   2bb38:	mov	x0, x20
   2bb3c:	mov	x1, x20
   2bb40:	mov	x2, x24
   2bb44:	mov	x3, x28
   2bb48:	bl	ca70 <__gmpn_add_n@plt>
   2bb4c:	mov	x23, x0
   2bb50:	ldr	x8, [x24, x28, lsl #3]
   2bb54:	cbz	x8, 2bb78 <__gmpn_mul_fft@@Base+0xacc>
   2bb58:	mov	x0, x20
   2bb5c:	mov	x1, x20
   2bb60:	mov	x2, x21
   2bb64:	mov	x3, x28
   2bb68:	bl	ca70 <__gmpn_add_n@plt>
   2bb6c:	ldr	x8, [x21, x28, lsl #3]
   2bb70:	add	x9, x0, x23
   2bb74:	add	x23, x9, x8
   2bb78:	cbz	x23, 2bbb0 <__gmpn_mul_fft@@Base+0xb04>
   2bb7c:	ldr	x8, [x19]
   2bb80:	adds	x8, x8, x23
   2bb84:	str	x8, [x19]
   2bb88:	b.cc	2bbb0 <__gmpn_mul_fft@@Base+0xb04>  // b.lo, b.ul, b.last
   2bb8c:	mov	w8, #0x1                   	// #1
   2bb90:	cmp	x8, x25
   2bb94:	b.ge	2bbb0 <__gmpn_mul_fft@@Base+0xb04>  // b.tcont
   2bb98:	lsl	x9, x8, #3
   2bb9c:	ldr	x10, [x19, x9]
   2bba0:	add	x8, x8, #0x1
   2bba4:	adds	x10, x10, #0x1
   2bba8:	str	x10, [x19, x9]
   2bbac:	b.cs	2bb90 <__gmpn_mul_fft@@Base+0xae4>  // b.hs, b.nlast
   2bbb0:	mov	x0, x21
   2bbb4:	mov	x1, x19
   2bbb8:	mov	x2, x20
   2bbbc:	mov	x3, x28
   2bbc0:	bl	c2d0 <__gmpn_sub_n@plt>
   2bbc4:	cbz	x0, 2baf4 <__gmpn_mul_fft@@Base+0xa48>
   2bbc8:	ldr	x8, [x21]
   2bbcc:	adds	x8, x8, #0x1
   2bbd0:	str	x8, [x21]
   2bbd4:	b.cc	2baf4 <__gmpn_mul_fft@@Base+0xa48>  // b.lo, b.ul, b.last
   2bbd8:	mov	w8, #0x1                   	// #1
   2bbdc:	cmp	x8, x28
   2bbe0:	b.ge	2bc10 <__gmpn_mul_fft@@Base+0xb64>  // b.tcont
   2bbe4:	lsl	x9, x8, #3
   2bbe8:	ldr	x10, [x21, x9]
   2bbec:	add	x8, x8, #0x1
   2bbf0:	adds	x10, x10, #0x1
   2bbf4:	str	x10, [x21, x9]
   2bbf8:	b.cs	2bbdc <__gmpn_mul_fft@@Base+0xb30>  // b.hs, b.nlast
   2bbfc:	b	2baf4 <__gmpn_mul_fft@@Base+0xa48>
   2bc00:	mov	x23, xzr
   2bc04:	ldr	x8, [x24, x28, lsl #3]
   2bc08:	cbnz	x8, 2bb58 <__gmpn_mul_fft@@Base+0xaac>
   2bc0c:	b	2bb78 <__gmpn_mul_fft@@Base+0xacc>
   2bc10:	mov	w8, #0x1                   	// #1
   2bc14:	b	2baf8 <__gmpn_mul_fft@@Base+0xa4c>
   2bc18:	cmp	x11, x23
   2bc1c:	adrp	x8, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2bc20:	add	x8, x8, #0xde0
   2bc24:	cset	w9, eq  // eq = none
   2bc28:	mov	w11, #0x1d8                 	// #472
   2bc2c:	umaddl	x8, w9, w11, x8
   2bc30:	ldr	w9, [x8], #4
   2bc34:	mov	x11, x8
   2bc38:	mov	w12, w9
   2bc3c:	lsr	w19, w12, #27
   2bc40:	ldr	w12, [x11], #4
   2bc44:	and	x13, x12, #0x7ffffff
   2bc48:	lsl	x13, x13, x19
   2bc4c:	cmp	x13, x24
   2bc50:	b.lt	2bc3c <__gmpn_mul_fft@@Base+0xb90>  // b.tstop
   2bc54:	mov	w11, #0x1                   	// #1
   2bc58:	lsl	x16, x11, x19
   2bc5c:	sub	x11, x16, #0x1
   2bc60:	tst	x11, x24
   2bc64:	b.ne	2cc2c <__gmpn_mul_fft@@Base+0x1b80>  // b.any
   2bc68:	lsl	x12, x24, #6
   2bc6c:	cmp	x16, #0x40
   2bc70:	mov	w11, #0x40                  	// #64
   2bc74:	add	w13, w19, #0x2
   2bc78:	asr	x12, x12, x19
   2bc7c:	csel	x11, x16, x11, gt
   2bc80:	add	x12, x13, x12, lsl #1
   2bc84:	add	x12, x12, x11
   2bc88:	sdiv	x12, x12, x11
   2bc8c:	mul	x21, x12, x11
   2bc90:	add	x11, x21, #0x3f
   2bc94:	cmp	x21, #0x0
   2bc98:	csel	x11, x11, x21, lt  // lt = tstop
   2bc9c:	asr	x23, x11, #6
   2bca0:	cmp	x23, x10
   2bca4:	b.ge	2bf28 <__gmpn_mul_fft@@Base+0xe7c>  // b.tcont
   2bca8:	cmp	x23, x24
   2bcac:	b.ge	2cc44 <__gmpn_mul_fft@@Base+0x1b98>  // b.tcont
   2bcb0:	lsl	x20, x16, #3
   2bcb4:	asr	x8, x24, x19
   2bcb8:	sub	x0, x29, #0x10
   2bcbc:	mov	x1, x20
   2bcc0:	stur	x8, [x29, #-112]
   2bcc4:	str	x16, [sp, #120]
   2bcc8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bccc:	stur	x0, [x29, #-88]
   2bcd0:	sub	x0, x29, #0x10
   2bcd4:	mov	x1, x20
   2bcd8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bcdc:	add	x20, x23, #0x1
   2bce0:	lsl	x8, x20, #1
   2bce4:	lsl	x8, x8, x19
   2bce8:	mov	x28, x0
   2bcec:	lsl	x1, x8, #3
   2bcf0:	sub	x0, x29, #0x10
   2bcf4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bcf8:	str	x0, [sp, #128]
   2bcfc:	lsl	x1, x20, #4
   2bd00:	sub	x0, x29, #0x10
   2bd04:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bd08:	add	w25, w19, #0x1
   2bd0c:	stur	x0, [x29, #-96]
   2bd10:	lsl	w1, w25, #3
   2bd14:	sub	x0, x29, #0x10
   2bd18:	lsl	x24, x20, x19
   2bd1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bd20:	mov	w8, #0x8                   	// #8
   2bd24:	mov	x20, x0
   2bd28:	lsl	x1, x8, x19
   2bd2c:	sub	x0, x29, #0x10
   2bd30:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2bd34:	mov	x8, xzr
   2bd38:	mov	w9, #0x1                   	// #1
   2bd3c:	str	x0, [x20, x8, lsl #3]
   2bd40:	lsl	x10, x9, x8
   2bd44:	add	x8, x8, #0x1
   2bd48:	cmp	x25, x8
   2bd4c:	add	x0, x0, x10, lsl #2
   2bd50:	b.ne	2bd3c <__gmpn_mul_fft@@Base+0xc90>  // b.any
   2bd54:	ldr	x8, [x20]
   2bd58:	str	wzr, [x8]
   2bd5c:	cbz	w19, 2be88 <__gmpn_mul_fft@@Base+0xddc>
   2bd60:	mov	w8, #0x1                   	// #1
   2bd64:	mov	w9, #0x1                   	// #1
   2bd68:	b	2bd7c <__gmpn_mul_fft@@Base+0xcd0>
   2bd6c:	add	x8, x8, #0x1
   2bd70:	cmp	x8, x25
   2bd74:	lsl	w9, w9, #1
   2bd78:	b.eq	2be88 <__gmpn_mul_fft@@Base+0xddc>  // b.none
   2bd7c:	cbz	w9, 2bd6c <__gmpn_mul_fft@@Base+0xcc0>
   2bd80:	add	x11, x20, x8, lsl #3
   2bd84:	ldr	x10, [x20, x8, lsl #3]
   2bd88:	ldur	x11, [x11, #-8]
   2bd8c:	sxtw	x12, w9
   2bd90:	cmp	w9, #0x8
   2bd94:	mov	w13, w9
   2bd98:	mov	x14, xzr
   2bd9c:	b.cs	2bddc <__gmpn_mul_fft@@Base+0xd30>  // b.hs, b.nlast
   2bda0:	add	x15, x14, x12
   2bda4:	sub	x12, x13, x14
   2bda8:	lsl	x14, x14, #2
   2bdac:	add	x13, x10, x15, lsl #2
   2bdb0:	add	x10, x10, x14
   2bdb4:	add	x11, x11, x14
   2bdb8:	ldr	w14, [x11], #4
   2bdbc:	mov	w15, #0x1                   	// #1
   2bdc0:	subs	x12, x12, #0x1
   2bdc4:	lsl	w16, w14, #1
   2bdc8:	bfi	w15, w14, #1, #31
   2bdcc:	str	w16, [x10], #4
   2bdd0:	str	w15, [x13], #4
   2bdd4:	b.ne	2bdb8 <__gmpn_mul_fft@@Base+0xd0c>  // b.any
   2bdd8:	b	2bd6c <__gmpn_mul_fft@@Base+0xcc0>
   2bddc:	add	x17, x12, x13
   2bde0:	lsl	x15, x13, #2
   2bde4:	add	x0, x10, x17, lsl #2
   2bde8:	add	x16, x10, x12, lsl #2
   2bdec:	add	x18, x10, x15
   2bdf0:	cmp	x10, x0
   2bdf4:	add	x1, x11, x15
   2bdf8:	cset	w2, cc  // cc = lo, ul, last
   2bdfc:	cmp	x16, x18
   2be00:	cset	w3, cc  // cc = lo, ul, last
   2be04:	cmp	x10, x1
   2be08:	cset	w15, cc  // cc = lo, ul, last
   2be0c:	cmp	x11, x18
   2be10:	cset	w17, cc  // cc = lo, ul, last
   2be14:	cmp	x16, x1
   2be18:	cset	w16, cc  // cc = lo, ul, last
   2be1c:	cmp	x11, x0
   2be20:	and	w1, w2, w3
   2be24:	cset	w18, cc  // cc = lo, ul, last
   2be28:	tbnz	w1, #0, 2bda0 <__gmpn_mul_fft@@Base+0xcf4>
   2be2c:	and	w15, w15, w17
   2be30:	tbnz	w15, #0, 2bda0 <__gmpn_mul_fft@@Base+0xcf4>
   2be34:	and	w15, w16, w18
   2be38:	tbnz	w15, #0, 2bda0 <__gmpn_mul_fft@@Base+0xcf4>
   2be3c:	and	x14, x13, #0xfffffff8
   2be40:	add	x15, x11, #0x10
   2be44:	lsl	x16, x12, #2
   2be48:	mov	x17, x14
   2be4c:	mov	x18, x10
   2be50:	ldp	q0, q1, [x15, #-16]
   2be54:	add	x0, x18, x16
   2be58:	add	x15, x15, #0x20
   2be5c:	subs	x17, x17, #0x8
   2be60:	shl	v0.4s, v0.4s, #1
   2be64:	shl	v1.4s, v1.4s, #1
   2be68:	stp	q0, q1, [x18], #32
   2be6c:	orr	v0.4s, #0x1
   2be70:	orr	v1.4s, #0x1
   2be74:	stp	q0, q1, [x0]
   2be78:	b.ne	2be50 <__gmpn_mul_fft@@Base+0xda4>  // b.any
   2be7c:	cmp	x14, x13
   2be80:	b.eq	2bd6c <__gmpn_mul_fft@@Base+0xcc0>  // b.none
   2be84:	b	2bda0 <__gmpn_mul_fft@@Base+0xcf4>
   2be88:	ldur	x9, [x29, #-32]
   2be8c:	cmp	w27, #0x3f
   2be90:	str	x20, [sp, #112]
   2be94:	stur	x19, [x29, #-120]
   2be98:	b.ne	2bf70 <__gmpn_mul_fft@@Base+0xec4>  // b.any
   2be9c:	ldr	x8, [sp, #64]
   2bea0:	ldur	x0, [x29, #-16]
   2bea4:	sxtw	x21, w8
   2bea8:	cbnz	x0, 2cc24 <__gmpn_mul_fft@@Base+0x1b78>
   2beac:	ldur	x23, [x29, #-80]
   2beb0:	ldp	x27, x1, [x29, #-32]
   2beb4:	ldr	x2, [sp, #40]
   2beb8:	mov	x4, x26
   2bebc:	mov	x0, x23
   2bec0:	mov	x3, x27
   2bec4:	bl	2d1ec <__gmpn_mul_fft@@Base+0x2140>
   2bec8:	ldr	x28, [sp, #80]
   2becc:	lsl	x22, x27, #7
   2bed0:	sub	x19, x22, x21
   2bed4:	mov	x2, x19
   2bed8:	add	x8, x26, x28
   2bedc:	add	x20, x8, #0x8
   2bee0:	ldur	x8, [x29, #-40]
   2bee4:	mov	x0, x20
   2bee8:	mov	x3, x27
   2beec:	str	x20, [x8]
   2bef0:	ldr	x1, [x23]
   2bef4:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2bef8:	ldr	x8, [x20, x28]
   2befc:	ldr	x25, [sp, #56]
   2bf00:	cbz	x8, 2c174 <__gmpn_mul_fft@@Base+0x10c8>
   2bf04:	mov	x8, x20
   2bf08:	ldr	x9, [x8]
   2bf0c:	sub	x10, x9, #0x1
   2bf10:	str	x10, [x8], #8
   2bf14:	cbz	x9, 2bf08 <__gmpn_mul_fft@@Base+0xe5c>
   2bf18:	ldr	x8, [x20, x27, lsl #3]
   2bf1c:	cbz	x8, 2c158 <__gmpn_mul_fft@@Base+0x10ac>
   2bf20:	mov	x8, xzr
   2bf24:	b	2c170 <__gmpn_mul_fft@@Base+0x10c4>
   2bf28:	mov	w10, #0x1                   	// #1
   2bf2c:	mov	x11, x8
   2bf30:	mov	w13, w9
   2bf34:	lsr	w12, w13, #27
   2bf38:	ldr	w13, [x11], #4
   2bf3c:	and	x14, x13, #0x7ffffff
   2bf40:	lsl	x14, x14, x12
   2bf44:	cmp	x14, x23
   2bf48:	b.lt	2bf34 <__gmpn_mul_fft@@Base+0xe88>  // b.tstop
   2bf4c:	lsl	x11, x10, x12
   2bf50:	sub	x12, x11, #0x1
   2bf54:	tst	x12, x23
   2bf58:	b.eq	2bca8 <__gmpn_mul_fft@@Base+0xbfc>  // b.none
   2bf5c:	add	x12, x12, x23
   2bf60:	neg	x11, x11
   2bf64:	and	x23, x12, x11
   2bf68:	lsl	x21, x23, #6
   2bf6c:	b	2bf2c <__gmpn_mul_fft@@Base+0xe80>
   2bf70:	ldr	x8, [sp, #128]
   2bf74:	ldur	x10, [x29, #-120]
   2bf78:	add	x19, x8, x24, lsl #3
   2bf7c:	asr	x8, x21, x10
   2bf80:	str	x8, [sp, #104]
   2bf84:	ldur	x8, [x29, #-112]
   2bf88:	lsl	x8, x8, x10
   2bf8c:	add	x8, x8, #0x1
   2bf90:	stp	x19, x8, [sp, #88]
   2bf94:	cbz	x9, 2c6f0 <__gmpn_mul_fft@@Base+0x1644>
   2bf98:	ldur	x20, [x29, #-80]
   2bf9c:	ldur	x25, [x29, #-48]
   2bfa0:	ldur	x27, [x29, #-32]
   2bfa4:	mov	x22, xzr
   2bfa8:	str	x26, [sp, #72]
   2bfac:	b	2c038 <__gmpn_mul_fft@@Base+0xf8c>
   2bfb0:	mov	x1, x24
   2bfb4:	ldp	x5, x21, [sp, #96]
   2bfb8:	ldur	x24, [x29, #-112]
   2bfbc:	ldr	x4, [x20]
   2bfc0:	ldur	x8, [x29, #-96]
   2bfc4:	ldp	x2, x0, [sp, #120]
   2bfc8:	mov	x3, x23
   2bfcc:	mov	x6, x24
   2bfd0:	mov	x7, x21
   2bfd4:	str	x8, [sp]
   2bfd8:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2bfdc:	ldur	w8, [x29, #-104]
   2bfe0:	ldr	x0, [x20]
   2bfe4:	ldur	x27, [x29, #-32]
   2bfe8:	ldur	x3, [x29, #-88]
   2bfec:	str	w8, [sp, #24]
   2bff0:	ldur	x8, [x29, #-96]
   2bff4:	mov	x1, x27
   2bff8:	ldur	x2, [x29, #-120]
   2bffc:	mov	x4, x28
   2c000:	str	x8, [sp, #16]
   2c004:	ldr	x8, [sp, #112]
   2c008:	mov	x5, x19
   2c00c:	mov	x6, x23
   2c010:	mov	x7, x24
   2c014:	stp	x21, x8, [sp]
   2c018:	bl	2b9d0 <__gmpn_mul_fft@@Base+0x924>
   2c01c:	ldr	x8, [x20], #8
   2c020:	ldur	x9, [x29, #-24]
   2c024:	add	x22, x22, #0x1
   2c028:	add	x25, x25, #0x8
   2c02c:	str	x0, [x8, x27, lsl #3]
   2c030:	cmp	x9, x22
   2c034:	b.le	2be9c <__gmpn_mul_fft@@Base+0xdf0>
   2c038:	ldr	x21, [x20]
   2c03c:	ldur	x24, [x29, #-88]
   2c040:	ldr	x8, [x21, x27, lsl #3]
   2c044:	cbz	x8, 2c08c <__gmpn_mul_fft@@Base+0xfe0>
   2c048:	mov	x8, x21
   2c04c:	ldr	x9, [x8]
   2c050:	sub	x10, x9, #0x1
   2c054:	str	x10, [x8], #8
   2c058:	cbz	x9, 2c04c <__gmpn_mul_fft@@Base+0xfa0>
   2c05c:	ldur	x8, [x29, #-32]
   2c060:	ldr	x8, [x21, x8, lsl #3]
   2c064:	cbz	x8, 2c070 <__gmpn_mul_fft@@Base+0xfc4>
   2c068:	mov	x8, xzr
   2c06c:	b	2c084 <__gmpn_mul_fft@@Base+0xfd8>
   2c070:	ldr	x2, [sp, #80]
   2c074:	mov	x0, x21
   2c078:	mov	w1, wzr
   2c07c:	bl	c5f0 <memset@plt>
   2c080:	mov	w8, #0x1                   	// #1
   2c084:	ldur	x9, [x29, #-32]
   2c088:	str	x8, [x21, x9, lsl #3]
   2c08c:	ldur	x8, [x29, #-80]
   2c090:	ldur	x9, [x29, #-48]
   2c094:	cmp	x9, x8
   2c098:	b.eq	2bfb0 <__gmpn_mul_fft@@Base+0xf04>  // b.none
   2c09c:	ldr	x21, [x25]
   2c0a0:	ldur	x8, [x29, #-32]
   2c0a4:	ldr	x8, [x21, x8, lsl #3]
   2c0a8:	cbz	x8, 2c0f0 <__gmpn_mul_fft@@Base+0x1044>
   2c0ac:	mov	x8, x21
   2c0b0:	ldr	x9, [x8]
   2c0b4:	sub	x10, x9, #0x1
   2c0b8:	str	x10, [x8], #8
   2c0bc:	cbz	x9, 2c0b0 <__gmpn_mul_fft@@Base+0x1004>
   2c0c0:	ldur	x8, [x29, #-32]
   2c0c4:	ldr	x8, [x21, x8, lsl #3]
   2c0c8:	cbz	x8, 2c0d4 <__gmpn_mul_fft@@Base+0x1028>
   2c0cc:	mov	x8, xzr
   2c0d0:	b	2c0e8 <__gmpn_mul_fft@@Base+0x103c>
   2c0d4:	ldr	x2, [sp, #80]
   2c0d8:	mov	x0, x21
   2c0dc:	mov	w1, wzr
   2c0e0:	bl	c5f0 <memset@plt>
   2c0e4:	mov	w8, #0x1                   	// #1
   2c0e8:	ldur	x9, [x29, #-32]
   2c0ec:	str	x8, [x21, x9, lsl #3]
   2c0f0:	ldp	x27, x0, [sp, #120]
   2c0f4:	mov	x1, x24
   2c0f8:	ldp	x26, x21, [sp, #96]
   2c0fc:	ldur	x24, [x29, #-112]
   2c100:	ldr	x4, [x20]
   2c104:	ldur	x19, [x29, #-96]
   2c108:	mov	x2, x27
   2c10c:	mov	x3, x23
   2c110:	mov	x5, x26
   2c114:	mov	x6, x24
   2c118:	mov	x7, x21
   2c11c:	str	x19, [sp]
   2c120:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2c124:	ldr	x4, [x25]
   2c128:	str	x19, [sp]
   2c12c:	ldr	x19, [sp, #88]
   2c130:	mov	x1, x28
   2c134:	mov	x2, x27
   2c138:	mov	x3, x23
   2c13c:	mov	x0, x19
   2c140:	mov	x5, x26
   2c144:	mov	x6, x24
   2c148:	mov	x7, x21
   2c14c:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2c150:	ldr	x26, [sp, #72]
   2c154:	b	2bfdc <__gmpn_mul_fft@@Base+0xf30>
   2c158:	cbz	x27, 2c16c <__gmpn_mul_fft@@Base+0x10c0>
   2c15c:	mov	x0, x20
   2c160:	mov	w1, wzr
   2c164:	mov	x2, x28
   2c168:	bl	c5f0 <memset@plt>
   2c16c:	mov	w8, #0x1                   	// #1
   2c170:	str	x8, [x20, x27, lsl #3]
   2c174:	ldur	x8, [x29, #-24]
   2c178:	ldur	x14, [x29, #-40]
   2c17c:	cmp	x8, #0x2
   2c180:	b.lt	2c28c <__gmpn_mul_fft@@Base+0x11e0>  // b.tstop
   2c184:	cbz	x27, 2c218 <__gmpn_mul_fft@@Base+0x116c>
   2c188:	mov	w21, #0x1                   	// #1
   2c18c:	b	2c1bc <__gmpn_mul_fft@@Base+0x1110>
   2c190:	mov	x0, x20
   2c194:	mov	w1, wzr
   2c198:	mov	x2, x28
   2c19c:	bl	c5f0 <memset@plt>
   2c1a0:	mov	w8, #0x1                   	// #1
   2c1a4:	str	x8, [x20, x27, lsl #3]
   2c1a8:	ldur	x8, [x29, #-24]
   2c1ac:	ldur	x14, [x29, #-40]
   2c1b0:	add	x21, x21, #0x1
   2c1b4:	cmp	x21, x8
   2c1b8:	b.eq	2c28c <__gmpn_mul_fft@@Base+0x11e0>  // b.none
   2c1bc:	lsl	x8, x21, #3
   2c1c0:	add	x9, x23, x8
   2c1c4:	ldur	x20, [x9, #-8]
   2c1c8:	mov	x3, x27
   2c1cc:	str	x20, [x14, x8]
   2c1d0:	ldur	x8, [x29, #-24]
   2c1d4:	ldr	x1, [x9]
   2c1d8:	ldur	x9, [x29, #-72]
   2c1dc:	mov	x0, x20
   2c1e0:	sub	x8, x8, x21
   2c1e4:	msub	x2, x8, x9, x19
   2c1e8:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2c1ec:	ldr	x8, [x20, x27, lsl #3]
   2c1f0:	cbz	x8, 2c1a8 <__gmpn_mul_fft@@Base+0x10fc>
   2c1f4:	mov	x8, x20
   2c1f8:	ldr	x9, [x8]
   2c1fc:	sub	x10, x9, #0x1
   2c200:	str	x10, [x8], #8
   2c204:	cbz	x9, 2c1f8 <__gmpn_mul_fft@@Base+0x114c>
   2c208:	ldr	x8, [x20, x27, lsl #3]
   2c20c:	cbz	x8, 2c190 <__gmpn_mul_fft@@Base+0x10e4>
   2c210:	mov	x8, xzr
   2c214:	b	2c1a4 <__gmpn_mul_fft@@Base+0x10f8>
   2c218:	ldur	x8, [x29, #-24]
   2c21c:	mov	x9, x23
   2c220:	add	x24, x9, #0x8
   2c224:	sub	x23, x8, #0x1
   2c228:	ldur	x8, [x29, #-72]
   2c22c:	msub	x8, x8, x23, x22
   2c230:	sub	x19, x8, x21
   2c234:	add	x21, x14, #0x8
   2c238:	b	2c258 <__gmpn_mul_fft@@Base+0x11ac>
   2c23c:	ldur	x8, [x29, #-72]
   2c240:	ldur	x14, [x29, #-40]
   2c244:	subs	x23, x23, #0x1
   2c248:	add	x24, x24, #0x8
   2c24c:	add	x19, x19, x8
   2c250:	add	x21, x21, #0x8
   2c254:	b.eq	2c28c <__gmpn_mul_fft@@Base+0x11e0>  // b.none
   2c258:	ldur	x20, [x24, #-8]
   2c25c:	mov	x2, x19
   2c260:	mov	x3, xzr
   2c264:	str	x20, [x21]
   2c268:	ldr	x1, [x24]
   2c26c:	mov	x0, x20
   2c270:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2c274:	ldr	x8, [x20]
   2c278:	cbz	x8, 2c23c <__gmpn_mul_fft@@Base+0x1190>
   2c27c:	cmp	x8, #0x1
   2c280:	cset	w8, eq  // eq = none
   2c284:	str	x8, [x20]
   2c288:	b	2c23c <__gmpn_mul_fft@@Base+0x1190>
   2c28c:	adds	x8, x27, #0x1
   2c290:	stur	x8, [x29, #-80]
   2c294:	b.cs	2c2ac <__gmpn_mul_fft@@Base+0x1200>  // b.hs, b.nlast
   2c298:	add	x2, x28, #0x8
   2c29c:	mov	x0, x26
   2c2a0:	mov	w1, wzr
   2c2a4:	bl	c5f0 <memset@plt>
   2c2a8:	ldur	x14, [x29, #-40]
   2c2ac:	ldur	x8, [x29, #-24]
   2c2b0:	sub	x9, x8, #0x1
   2c2b4:	ldp	x8, x20, [x29, #-64]
   2c2b8:	stur	x9, [x29, #-72]
   2c2bc:	mul	x22, x9, x8
   2c2c0:	add	x19, x22, x27
   2c2c4:	adds	x24, x19, #0x1
   2c2c8:	b.cs	2c2e0 <__gmpn_mul_fft@@Base+0x1234>  // b.hs, b.nlast
   2c2cc:	lsl	x2, x24, #3
   2c2d0:	mov	x0, x20
   2c2d4:	mov	w1, wzr
   2c2d8:	bl	c5f0 <memset@plt>
   2c2dc:	ldur	x14, [x29, #-40]
   2c2e0:	ldr	x8, [sp, #64]
   2c2e4:	lsl	x23, x25, #1
   2c2e8:	cmp	w8, #0x3f
   2c2ec:	b.eq	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.none
   2c2f0:	ldur	x9, [x29, #-64]
   2c2f4:	stp	x23, x24, [x29, #-112]
   2c2f8:	mov	x28, x22
   2c2fc:	add	x23, x20, x22, lsl #3
   2c300:	lsl	x8, x9, #1
   2c304:	stp	x22, x8, [x29, #-96]
   2c308:	add	x8, x27, x22
   2c30c:	ldur	x27, [x29, #-72]
   2c310:	add	x8, x20, x8, lsl #3
   2c314:	neg	x25, x9, lsl #3
   2c318:	add	x21, x8, #0x10
   2c31c:	add	x24, x20, x24, lsl #3
   2c320:	stur	xzr, [x29, #-48]
   2c324:	stur	x19, [x29, #-120]
   2c328:	b	2c354 <__gmpn_mul_fft@@Base+0x12a8>
   2c32c:	ldur	x20, [x29, #-56]
   2c330:	ldur	x8, [x29, #-64]
   2c334:	cmp	x27, #0x0
   2c338:	sub	x27, x27, #0x1
   2c33c:	add	x21, x21, x25
   2c340:	sub	x19, x19, x8
   2c344:	sub	x28, x28, x8
   2c348:	add	x23, x23, x25
   2c34c:	add	x24, x24, x25
   2c350:	b.le	2c4e8 <__gmpn_mul_fft@@Base+0x143c>
   2c354:	ldur	x8, [x29, #-24]
   2c358:	ldp	x3, x9, [x29, #-80]
   2c35c:	add	x20, x20, x28, lsl #3
   2c360:	mov	x0, x20
   2c364:	sub	x8, x8, x27
   2c368:	and	x22, x8, x9
   2c36c:	ldr	x2, [x14, x22, lsl #3]
   2c370:	mov	x1, x20
   2c374:	bl	ca70 <__gmpn_add_n@plt>
   2c378:	cbz	x0, 2c3d0 <__gmpn_mul_fft@@Base+0x1324>
   2c37c:	ldur	x8, [x29, #-32]
   2c380:	add	x8, x20, x8, lsl #3
   2c384:	ldr	x9, [x8, #8]
   2c388:	adds	x9, x9, #0x1
   2c38c:	str	x9, [x8, #8]
   2c390:	b.cc	2c3d8 <__gmpn_mul_fft@@Base+0x132c>  // b.lo, b.ul, b.last
   2c394:	ldur	x8, [x29, #-96]
   2c398:	ldur	x14, [x29, #-40]
   2c39c:	mov	x9, xzr
   2c3a0:	sub	x8, x8, x28
   2c3a4:	add	x10, x9, #0x1
   2c3a8:	cmp	x10, x8
   2c3ac:	b.ge	2c3e4 <__gmpn_mul_fft@@Base+0x1338>  // b.tcont
   2c3b0:	lsl	x9, x9, #3
   2c3b4:	ldr	x11, [x21, x9]
   2c3b8:	adds	x11, x11, #0x1
   2c3bc:	str	x11, [x21, x9]
   2c3c0:	mov	x9, x10
   2c3c4:	b.cs	2c3a4 <__gmpn_mul_fft@@Base+0x12f8>  // b.hs, b.nlast
   2c3c8:	mov	x8, xzr
   2c3cc:	b	2c3e8 <__gmpn_mul_fft@@Base+0x133c>
   2c3d0:	ldur	x14, [x29, #-40]
   2c3d4:	b	2c3f4 <__gmpn_mul_fft@@Base+0x1348>
   2c3d8:	ldur	x14, [x29, #-40]
   2c3dc:	mov	x8, xzr
   2c3e0:	b	2c3e8 <__gmpn_mul_fft@@Base+0x133c>
   2c3e4:	mov	w8, #0x1                   	// #1
   2c3e8:	ldur	x9, [x29, #-48]
   2c3ec:	add	x9, x8, x9
   2c3f0:	stur	x9, [x29, #-48]
   2c3f4:	ldur	x9, [x29, #-88]
   2c3f8:	add	x8, x27, #0x1
   2c3fc:	str	x8, [x26, x9, lsl #3]
   2c400:	ldr	x8, [x14, x22, lsl #3]
   2c404:	ldur	x9, [x29, #-32]
   2c408:	add	x10, x9, #0x1
   2c40c:	cmp	x10, #0x1
   2c410:	b.lt	2c32c <__gmpn_mul_fft@@Base+0x1280>  // b.tstop
   2c414:	lsl	x10, x9, #3
   2c418:	ldr	x11, [x8, x10]
   2c41c:	ldr	x10, [x26, x10]
   2c420:	sub	x9, x9, #0x1
   2c424:	cmp	x11, x10
   2c428:	b.eq	2c408 <__gmpn_mul_fft@@Base+0x135c>  // b.none
   2c42c:	b.ls	2c32c <__gmpn_mul_fft@@Base+0x1280>  // b.plast
   2c430:	ldr	x8, [x20]
   2c434:	sub	x9, x8, #0x1
   2c438:	str	x9, [x20]
   2c43c:	cbz	x8, 2c44c <__gmpn_mul_fft@@Base+0x13a0>
   2c440:	ldur	x20, [x29, #-56]
   2c444:	mov	x8, xzr
   2c448:	b	2c488 <__gmpn_mul_fft@@Base+0x13dc>
   2c44c:	ldur	x8, [x29, #-104]
   2c450:	ldur	x20, [x29, #-56]
   2c454:	mov	w9, #0x1                   	// #1
   2c458:	sub	x8, x8, x28
   2c45c:	cmp	x9, x8
   2c460:	b.ge	2c484 <__gmpn_mul_fft@@Base+0x13d8>  // b.tcont
   2c464:	lsl	x10, x9, #3
   2c468:	ldr	x11, [x23, x10]
   2c46c:	add	x9, x9, #0x1
   2c470:	sub	x12, x11, #0x1
   2c474:	str	x12, [x23, x10]
   2c478:	cbz	x11, 2c45c <__gmpn_mul_fft@@Base+0x13b0>
   2c47c:	mov	x8, xzr
   2c480:	b	2c488 <__gmpn_mul_fft@@Base+0x13dc>
   2c484:	mov	x8, #0xffffffffffffffff    	// #-1
   2c488:	lsl	x9, x19, #3
   2c48c:	ldr	x10, [x20, x9]
   2c490:	ldur	x11, [x29, #-48]
   2c494:	add	x8, x8, x11
   2c498:	sub	x11, x10, #0x1
   2c49c:	str	x11, [x20, x9]
   2c4a0:	cbnz	x10, 2c4d0 <__gmpn_mul_fft@@Base+0x1424>
   2c4a4:	ldur	x9, [x29, #-104]
   2c4a8:	sub	x9, x9, x19
   2c4ac:	add	x11, x10, #0x1
   2c4b0:	cmp	x11, x9
   2c4b4:	b.ge	2c4e0 <__gmpn_mul_fft@@Base+0x1434>  // b.tcont
   2c4b8:	lsl	x10, x10, #3
   2c4bc:	ldr	x12, [x24, x10]
   2c4c0:	sub	x13, x12, #0x1
   2c4c4:	str	x13, [x24, x10]
   2c4c8:	mov	x10, x11
   2c4cc:	cbz	x12, 2c4ac <__gmpn_mul_fft@@Base+0x1400>
   2c4d0:	mov	x9, xzr
   2c4d4:	add	x8, x8, x9
   2c4d8:	stur	x8, [x29, #-48]
   2c4dc:	b	2c330 <__gmpn_mul_fft@@Base+0x1284>
   2c4e0:	mov	x9, #0xffffffffffffffff    	// #-1
   2c4e4:	b	2c4d4 <__gmpn_mul_fft@@Base+0x1428>
   2c4e8:	ldur	x8, [x29, #-48]
   2c4ec:	cmp	x8, #0x1
   2c4f0:	b.eq	2c564 <__gmpn_mul_fft@@Base+0x14b8>  // b.none
   2c4f4:	ldr	x25, [sp, #56]
   2c4f8:	ldur	x27, [x29, #-32]
   2c4fc:	ldr	x28, [sp, #80]
   2c500:	ldp	x24, x22, [x29, #-104]
   2c504:	ldp	x13, x23, [x29, #-120]
   2c508:	cmn	x8, #0x1
   2c50c:	b.ne	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.any
   2c510:	add	x8, x20, x24, lsl #3
   2c514:	sub	x8, x8, x25, lsl #3
   2c518:	ldr	x9, [x8]
   2c51c:	adds	x9, x9, #0x1
   2c520:	str	x9, [x8]
   2c524:	b.cc	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.lo, b.ul, b.last
   2c528:	add	x9, x27, x22
   2c52c:	sub	x9, x9, x25
   2c530:	add	x9, x20, x9, lsl #3
   2c534:	mov	x10, xzr
   2c538:	add	x9, x9, #0x10
   2c53c:	add	x11, x10, #0x1
   2c540:	cmp	x11, x25
   2c544:	b.ge	2c87c <__gmpn_mul_fft@@Base+0x17d0>  // b.tcont
   2c548:	lsl	x10, x10, #3
   2c54c:	ldr	x12, [x9, x10]
   2c550:	adds	x12, x12, #0x1
   2c554:	str	x12, [x9, x10]
   2c558:	mov	x10, x11
   2c55c:	b.cs	2c53c <__gmpn_mul_fft@@Base+0x1490>  // b.hs, b.nlast
   2c560:	b	2c8c4 <__gmpn_mul_fft@@Base+0x1818>
   2c564:	ldp	x23, x24, [x29, #-112]
   2c568:	ldr	x25, [sp, #56]
   2c56c:	ldur	x27, [x29, #-32]
   2c570:	ldr	x28, [sp, #80]
   2c574:	ldur	x22, [x29, #-96]
   2c578:	cmp	x24, x23
   2c57c:	add	x8, x20, x24, lsl #3
   2c580:	b.ge	2c828 <__gmpn_mul_fft@@Base+0x177c>  // b.tcont
   2c584:	sub	x8, x8, x25, lsl #3
   2c588:	ldr	x9, [x8]
   2c58c:	sub	x10, x9, #0x1
   2c590:	str	x10, [x8]
   2c594:	cbnz	x9, 2c8c4 <__gmpn_mul_fft@@Base+0x1818>
   2c598:	add	x8, x27, x22
   2c59c:	sub	x8, x8, x25
   2c5a0:	add	x8, x20, x8, lsl #3
   2c5a4:	add	x8, x8, #0x10
   2c5a8:	add	x10, x9, #0x1
   2c5ac:	cmp	x10, x25
   2c5b0:	b.ge	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.tcont
   2c5b4:	lsl	x9, x9, #3
   2c5b8:	ldr	x11, [x8, x9]
   2c5bc:	sub	x12, x11, #0x1
   2c5c0:	str	x12, [x8, x9]
   2c5c4:	mov	x9, x10
   2c5c8:	cbz	x11, 2c5a8 <__gmpn_mul_fft@@Base+0x14fc>
   2c5cc:	b	2c8c4 <__gmpn_mul_fft@@Base+0x1818>
   2c5d0:	mov	x24, x27
   2c5d4:	ldur	x27, [x29, #-32]
   2c5d8:	b	2c5f4 <__gmpn_mul_fft@@Base+0x1548>
   2c5dc:	mov	x8, xzr
   2c5e0:	ldur	x9, [x29, #-24]
   2c5e4:	add	x22, x22, #0x1
   2c5e8:	str	x8, [x21, x27, lsl #3]
   2c5ec:	cmp	x9, x22
   2c5f0:	b.le	2be9c <__gmpn_mul_fft@@Base+0xdf0>
   2c5f4:	ldr	x21, [x24], #8
   2c5f8:	mov	x0, x19
   2c5fc:	mov	x2, x27
   2c600:	mov	x1, x21
   2c604:	bl	c8e0 <__gmpn_sqr@plt>
   2c608:	ldr	x8, [x21, x27, lsl #3]
   2c60c:	cbz	x8, 2c6d8 <__gmpn_mul_fft@@Base+0x162c>
   2c610:	mov	x0, x20
   2c614:	mov	x1, x20
   2c618:	mov	x2, x21
   2c61c:	mov	x3, x27
   2c620:	bl	ca70 <__gmpn_add_n@plt>
   2c624:	mov	x23, x0
   2c628:	ldr	x8, [x21, x27, lsl #3]
   2c62c:	cbz	x8, 2c650 <__gmpn_mul_fft@@Base+0x15a4>
   2c630:	mov	x0, x20
   2c634:	mov	x1, x20
   2c638:	mov	x2, x21
   2c63c:	mov	x3, x27
   2c640:	bl	ca70 <__gmpn_add_n@plt>
   2c644:	ldr	x8, [x21, x27, lsl #3]
   2c648:	add	x9, x0, x23
   2c64c:	add	x23, x9, x8
   2c650:	cbz	x23, 2c688 <__gmpn_mul_fft@@Base+0x15dc>
   2c654:	ldr	x8, [x19]
   2c658:	adds	x8, x8, x23
   2c65c:	str	x8, [x19]
   2c660:	b.cc	2c688 <__gmpn_mul_fft@@Base+0x15dc>  // b.lo, b.ul, b.last
   2c664:	mov	w8, #0x1                   	// #1
   2c668:	cmp	x8, x25
   2c66c:	b.ge	2c688 <__gmpn_mul_fft@@Base+0x15dc>  // b.tcont
   2c670:	lsl	x9, x8, #3
   2c674:	ldr	x10, [x19, x9]
   2c678:	add	x8, x8, #0x1
   2c67c:	adds	x10, x10, #0x1
   2c680:	str	x10, [x19, x9]
   2c684:	b.cs	2c668 <__gmpn_mul_fft@@Base+0x15bc>  // b.hs, b.nlast
   2c688:	mov	x0, x21
   2c68c:	mov	x1, x19
   2c690:	mov	x2, x20
   2c694:	mov	x3, x27
   2c698:	bl	c2d0 <__gmpn_sub_n@plt>
   2c69c:	cbz	x0, 2c5dc <__gmpn_mul_fft@@Base+0x1530>
   2c6a0:	ldr	x8, [x21]
   2c6a4:	adds	x8, x8, #0x1
   2c6a8:	str	x8, [x21]
   2c6ac:	b.cc	2c5dc <__gmpn_mul_fft@@Base+0x1530>  // b.lo, b.ul, b.last
   2c6b0:	mov	w8, #0x1                   	// #1
   2c6b4:	cmp	x8, x27
   2c6b8:	b.ge	2c6e8 <__gmpn_mul_fft@@Base+0x163c>  // b.tcont
   2c6bc:	lsl	x9, x8, #3
   2c6c0:	ldr	x10, [x21, x9]
   2c6c4:	add	x8, x8, #0x1
   2c6c8:	adds	x10, x10, #0x1
   2c6cc:	str	x10, [x21, x9]
   2c6d0:	b.cs	2c6b4 <__gmpn_mul_fft@@Base+0x1608>  // b.hs, b.nlast
   2c6d4:	b	2c5dc <__gmpn_mul_fft@@Base+0x1530>
   2c6d8:	mov	x23, xzr
   2c6dc:	ldr	x8, [x21, x27, lsl #3]
   2c6e0:	cbnz	x8, 2c630 <__gmpn_mul_fft@@Base+0x1584>
   2c6e4:	b	2c650 <__gmpn_mul_fft@@Base+0x15a4>
   2c6e8:	mov	w8, #0x1                   	// #1
   2c6ec:	b	2c5e0 <__gmpn_mul_fft@@Base+0x1534>
   2c6f0:	ldur	x25, [x29, #-112]
   2c6f4:	mov	x20, xzr
   2c6f8:	b	2c77c <__gmpn_mul_fft@@Base+0x16d0>
   2c6fc:	ldp	x5, x24, [sp, #96]
   2c700:	ldp	x2, x0, [sp, #120]
   2c704:	mov	x1, x27
   2c708:	mov	x3, x23
   2c70c:	mov	x6, x25
   2c710:	mov	x7, x24
   2c714:	str	x19, [sp]
   2c718:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2c71c:	mov	x9, x19
   2c720:	ldr	x19, [sp, #88]
   2c724:	ldur	x22, [x29, #-80]
   2c728:	ldur	w8, [x29, #-104]
   2c72c:	lsl	x21, x20, #3
   2c730:	mov	x1, xzr
   2c734:	ldr	x0, [x22, x21]
   2c738:	str	w8, [sp, #24]
   2c73c:	ldr	x8, [sp, #112]
   2c740:	ldur	x2, [x29, #-120]
   2c744:	mov	x3, x27
   2c748:	mov	x4, x28
   2c74c:	mov	x5, x19
   2c750:	mov	x6, x23
   2c754:	mov	x7, x25
   2c758:	stp	x8, x9, [sp, #8]
   2c75c:	str	x24, [sp]
   2c760:	bl	2b9d0 <__gmpn_mul_fft@@Base+0x924>
   2c764:	ldur	x9, [x29, #-24]
   2c768:	ldr	x8, [x22, x21]
   2c76c:	add	x20, x20, #0x1
   2c770:	cmp	x9, x20
   2c774:	str	x0, [x8]
   2c778:	b.le	2be9c <__gmpn_mul_fft@@Base+0xdf0>
   2c77c:	ldur	x8, [x29, #-80]
   2c780:	ldr	x4, [x8, x20, lsl #3]
   2c784:	ldr	x8, [x4]
   2c788:	cbz	x8, 2c798 <__gmpn_mul_fft@@Base+0x16ec>
   2c78c:	cmp	x8, #0x1
   2c790:	cset	w8, eq  // eq = none
   2c794:	str	x8, [x4]
   2c798:	ldp	x27, x8, [x29, #-88]
   2c79c:	ldur	x21, [x29, #-48]
   2c7a0:	ldur	x19, [x29, #-96]
   2c7a4:	cmp	x21, x8
   2c7a8:	b.eq	2c6fc <__gmpn_mul_fft@@Base+0x1650>  // b.none
   2c7ac:	ldr	x8, [x21, x20, lsl #3]
   2c7b0:	ldr	x9, [x8]
   2c7b4:	cbz	x9, 2c7c4 <__gmpn_mul_fft@@Base+0x1718>
   2c7b8:	cmp	x9, #0x1
   2c7bc:	cset	w9, eq  // eq = none
   2c7c0:	str	x9, [x8]
   2c7c4:	ldp	x22, x0, [sp, #120]
   2c7c8:	mov	x1, x27
   2c7cc:	ldp	x27, x24, [sp, #96]
   2c7d0:	mov	x3, x23
   2c7d4:	mov	x2, x22
   2c7d8:	mov	x6, x25
   2c7dc:	mov	x5, x27
   2c7e0:	mov	x7, x24
   2c7e4:	str	x19, [sp]
   2c7e8:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2c7ec:	ldr	x4, [x21, x20, lsl #3]
   2c7f0:	str	x19, [sp]
   2c7f4:	mov	x21, x19
   2c7f8:	ldr	x19, [sp, #88]
   2c7fc:	mov	x5, x27
   2c800:	ldur	x27, [x29, #-88]
   2c804:	mov	x1, x28
   2c808:	mov	x0, x19
   2c80c:	mov	x2, x22
   2c810:	mov	x3, x23
   2c814:	mov	x6, x25
   2c818:	mov	x7, x24
   2c81c:	bl	2b560 <__gmpn_mul_fft@@Base+0x4b4>
   2c820:	mov	x9, x21
   2c824:	b	2c724 <__gmpn_mul_fft@@Base+0x1678>
   2c828:	sub	x8, x8, x23, lsl #3
   2c82c:	ldr	x9, [x8]
   2c830:	adds	x9, x9, #0x1
   2c834:	str	x9, [x8]
   2c838:	b.cc	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.lo, b.ul, b.last
   2c83c:	mov	w9, #0x1                   	// #1
   2c840:	b	2c858 <__gmpn_mul_fft@@Base+0x17ac>
   2c844:	ldr	x9, [x8]
   2c848:	adds	x9, x9, #0x1
   2c84c:	str	x9, [x8]
   2c850:	mov	w9, #0x1                   	// #1
   2c854:	b.cc	2c8c4 <__gmpn_mul_fft@@Base+0x1818>  // b.lo, b.ul, b.last
   2c858:	cmp	x9, x23
   2c85c:	b.ge	2c844 <__gmpn_mul_fft@@Base+0x1798>  // b.tcont
   2c860:	lsl	x10, x9, #3
   2c864:	ldr	x11, [x8, x10]
   2c868:	add	x9, x9, #0x1
   2c86c:	adds	x11, x11, #0x1
   2c870:	str	x11, [x8, x10]
   2c874:	b.cs	2c858 <__gmpn_mul_fft@@Base+0x17ac>  // b.hs, b.nlast
   2c878:	b	2c8c4 <__gmpn_mul_fft@@Base+0x1818>
   2c87c:	ldur	x9, [x8, #-8]
   2c880:	sub	x10, x9, #0x1
   2c884:	stur	x10, [x8, #-8]
   2c888:	cbnz	x9, 2c8b4 <__gmpn_mul_fft@@Base+0x1808>
   2c88c:	b	2c8a8 <__gmpn_mul_fft@@Base+0x17fc>
   2c890:	lsl	x9, x9, #3
   2c894:	ldr	x11, [x8, x9]
   2c898:	sub	x12, x11, #0x1
   2c89c:	str	x12, [x8, x9]
   2c8a0:	mov	x9, x10
   2c8a4:	cbnz	x11, 2c8b4 <__gmpn_mul_fft@@Base+0x1808>
   2c8a8:	add	x10, x9, #0x1
   2c8ac:	cmp	x10, x25
   2c8b0:	b.le	2c890 <__gmpn_mul_fft@@Base+0x17e4>
   2c8b4:	lsl	x8, x13, #3
   2c8b8:	ldr	x9, [x20, x8]
   2c8bc:	sub	x9, x9, #0x1
   2c8c0:	str	x9, [x20, x8]
   2c8c4:	sub	x19, x24, x23
   2c8c8:	cmp	x19, #0x1
   2c8cc:	b.lt	2c99c <__gmpn_mul_fft@@Base+0x18f0>  // b.tstop
   2c8d0:	ldr	x21, [sp, #48]
   2c8d4:	add	x2, x20, x23, lsl #3
   2c8d8:	mov	x1, x20
   2c8dc:	mov	x3, x19
   2c8e0:	mov	x0, x21
   2c8e4:	bl	ca70 <__gmpn_add_n@plt>
   2c8e8:	lsl	x8, x19, #3
   2c8ec:	ldr	x9, [x20, x8]
   2c8f0:	sub	x13, x25, x19
   2c8f4:	adds	x9, x9, x0
   2c8f8:	str	x9, [x21, x8]
   2c8fc:	b.cc	2c9bc <__gmpn_mul_fft@@Base+0x1910>  // b.lo, b.ul, b.last
   2c900:	add	x9, x23, x25
   2c904:	sub	x10, x22, x23
   2c908:	sub	x9, x9, x22
   2c90c:	lsl	x10, x10, #3
   2c910:	sub	x11, x9, x27
   2c914:	mov	x8, xzr
   2c918:	add	x12, x21, x10
   2c91c:	add	x10, x20, x10
   2c920:	sub	x9, x11, #0x2
   2c924:	mov	w20, #0x1                   	// #1
   2c928:	add	x8, x8, #0x1
   2c92c:	cmp	x8, x13
   2c930:	b.ge	2cb60 <__gmpn_mul_fft@@Base+0x1ab4>  // b.tcont
   2c934:	add	x14, x10, x28
   2c938:	ldr	x14, [x14, #16]
   2c93c:	add	x15, x12, x28
   2c940:	add	x12, x12, #0x8
   2c944:	add	x10, x10, #0x8
   2c948:	adds	x14, x14, #0x1
   2c94c:	sub	x9, x9, #0x1
   2c950:	str	x14, [x15, #16]
   2c954:	b.cs	2c928 <__gmpn_mul_fft@@Base+0x187c>  // b.hs, b.nlast
   2c958:	ldur	x14, [x29, #-56]
   2c95c:	mov	x20, xzr
   2c960:	cmp	x14, x21
   2c964:	b.eq	2cb60 <__gmpn_mul_fft@@Base+0x1ab4>  // b.none
   2c968:	add	x14, x8, #0x1
   2c96c:	cmp	x14, x13
   2c970:	mov	x19, x25
   2c974:	b.ge	2cb64 <__gmpn_mul_fft@@Base+0x1ab8>  // b.tcont
   2c978:	add	x13, x25, x25, lsl #1
   2c97c:	sub	x15, x13, x22
   2c980:	sub	x15, x15, x27
   2c984:	sub	x15, x15, x8
   2c988:	sub	x15, x15, #0x2
   2c98c:	cmp	x15, #0x4
   2c990:	b.cs	2cab0 <__gmpn_mul_fft@@Base+0x1a04>  // b.hs, b.nlast
   2c994:	ldur	x0, [x29, #-56]
   2c998:	b	2cb2c <__gmpn_mul_fft@@Base+0x1a80>
   2c99c:	ldr	x21, [sp, #48]
   2c9a0:	mov	x1, x20
   2c9a4:	mov	x2, x25
   2c9a8:	sub	x19, x24, x25
   2c9ac:	mov	x0, x21
   2c9b0:	bl	ca50 <__gmpn_copyi@plt>
   2c9b4:	mov	x20, xzr
   2c9b8:	b	2cb64 <__gmpn_mul_fft@@Base+0x1ab8>
   2c9bc:	cmp	x20, x21
   2c9c0:	mov	x20, xzr
   2c9c4:	b.eq	2cb60 <__gmpn_mul_fft@@Base+0x1ab4>  // b.none
   2c9c8:	cmp	x13, #0x2
   2c9cc:	mov	x19, x25
   2c9d0:	b.lt	2cb64 <__gmpn_mul_fft@@Base+0x1ab8>  // b.tstop
   2c9d4:	add	x8, x25, x25, lsl #1
   2c9d8:	sub	x9, x8, x22
   2c9dc:	sub	x9, x9, x27
   2c9e0:	sub	x9, x9, #0x2
   2c9e4:	cmp	x9, #0x4
   2c9e8:	b.cs	2c9f8 <__gmpn_mul_fft@@Base+0x194c>  // b.hs, b.nlast
   2c9ec:	ldur	x15, [x29, #-56]
   2c9f0:	mov	w10, #0x1                   	// #1
   2c9f4:	b	2ca7c <__gmpn_mul_fft@@Base+0x19d0>
   2c9f8:	add	x10, x22, x27
   2c9fc:	ldur	x15, [x29, #-56]
   2ca00:	sub	x10, x10, x25, lsl #1
   2ca04:	lsl	x10, x10, #3
   2ca08:	lsl	x11, x25, #3
   2ca0c:	add	x10, x10, #0x10
   2ca10:	add	x12, x21, x10
   2ca14:	add	x13, x15, x11
   2ca18:	cmp	x12, x13
   2ca1c:	b.cs	2ca38 <__gmpn_mul_fft@@Base+0x198c>  // b.hs, b.nlast
   2ca20:	add	x11, x21, x11
   2ca24:	add	x10, x15, x10
   2ca28:	cmp	x10, x11
   2ca2c:	b.cs	2ca38 <__gmpn_mul_fft@@Base+0x198c>  // b.hs, b.nlast
   2ca30:	mov	w10, #0x1                   	// #1
   2ca34:	b	2ca7c <__gmpn_mul_fft@@Base+0x19d0>
   2ca38:	add	x12, x27, x22
   2ca3c:	sub	x12, x12, x25, lsl #1
   2ca40:	lsl	x12, x12, #3
   2ca44:	and	x11, x9, #0xfffffffffffffffc
   2ca48:	add	x13, x12, #0x20
   2ca4c:	orr	x10, x11, #0x1
   2ca50:	add	x12, x15, x13
   2ca54:	add	x13, x21, x13
   2ca58:	mov	x14, x11
   2ca5c:	ldp	q0, q1, [x12, #-16]
   2ca60:	add	x12, x12, #0x20
   2ca64:	subs	x14, x14, #0x4
   2ca68:	stp	q0, q1, [x13, #-16]
   2ca6c:	add	x13, x13, #0x20
   2ca70:	b.ne	2ca5c <__gmpn_mul_fft@@Base+0x19b0>  // b.any
   2ca74:	cmp	x9, x11
   2ca78:	b.eq	2cb5c <__gmpn_mul_fft@@Base+0x1ab0>  // b.none
   2ca7c:	add	x9, x10, x27
   2ca80:	add	x9, x9, x22
   2ca84:	add	x9, x9, #0x1
   2ca88:	sub	x8, x9, x8
   2ca8c:	sub	x9, x9, x25, lsl #1
   2ca90:	lsl	x10, x9, #3
   2ca94:	add	x9, x21, x10
   2ca98:	add	x10, x15, x10
   2ca9c:	ldr	x11, [x10], #8
   2caa0:	adds	x8, x8, #0x1
   2caa4:	str	x11, [x9], #8
   2caa8:	b.cc	2ca9c <__gmpn_mul_fft@@Base+0x19f0>  // b.lo, b.ul, b.last
   2caac:	b	2cb5c <__gmpn_mul_fft@@Base+0x1ab0>
   2cab0:	ldur	x0, [x29, #-56]
   2cab4:	add	x17, x12, x28
   2cab8:	lsl	x16, x25, #3
   2cabc:	add	x17, x17, #0x10
   2cac0:	add	x18, x0, x16
   2cac4:	cmp	x17, x18
   2cac8:	b.cs	2cae0 <__gmpn_mul_fft@@Base+0x1a34>  // b.hs, b.nlast
   2cacc:	add	x17, x10, x28
   2cad0:	add	x16, x21, x16
   2cad4:	add	x17, x17, #0x10
   2cad8:	cmp	x17, x16
   2cadc:	b.cc	2cb2c <__gmpn_mul_fft@@Base+0x1a80>  // b.lo, b.ul, b.last
   2cae0:	add	x12, x12, x28
   2cae4:	sub	x11, x11, x8
   2cae8:	and	x16, x9, #0xfffffffffffffffc
   2caec:	add	x14, x10, x28
   2caf0:	add	x9, x12, #0x20
   2caf4:	sub	x12, x11, #0x2
   2caf8:	add	x8, x16, x8
   2cafc:	and	x10, x15, #0xfffffffffffffffc
   2cb00:	add	x11, x14, #0x20
   2cb04:	add	x14, x8, #0x1
   2cb08:	and	x8, x12, #0xfffffffffffffffc
   2cb0c:	ldp	q0, q1, [x11, #-16]
   2cb10:	add	x11, x11, #0x20
   2cb14:	subs	x8, x8, #0x4
   2cb18:	stp	q0, q1, [x9, #-16]
   2cb1c:	add	x9, x9, #0x20
   2cb20:	b.ne	2cb0c <__gmpn_mul_fft@@Base+0x1a60>  // b.any
   2cb24:	cmp	x15, x10
   2cb28:	b.eq	2cb5c <__gmpn_mul_fft@@Base+0x1ab0>  // b.none
   2cb2c:	add	x8, x14, x27
   2cb30:	add	x8, x8, x22
   2cb34:	add	x9, x8, #0x1
   2cb38:	sub	x8, x9, x13
   2cb3c:	sub	x9, x9, x25, lsl #1
   2cb40:	lsl	x10, x9, #3
   2cb44:	add	x9, x21, x10
   2cb48:	add	x10, x0, x10
   2cb4c:	ldr	x11, [x10], #8
   2cb50:	adds	x8, x8, #0x1
   2cb54:	str	x11, [x9], #8
   2cb58:	b.cc	2cb4c <__gmpn_mul_fft@@Base+0x1aa0>  // b.lo, b.ul, b.last
   2cb5c:	mov	x20, xzr
   2cb60:	mov	x19, x25
   2cb64:	ldur	x8, [x29, #-56]
   2cb68:	mov	x0, x21
   2cb6c:	mov	x1, x21
   2cb70:	mov	x3, x19
   2cb74:	add	x2, x8, x25, lsl #3
   2cb78:	bl	c2d0 <__gmpn_sub_n@plt>
   2cb7c:	add	x9, x21, x19, lsl #3
   2cb80:	ldr	x8, [x9]
   2cb84:	subs	x8, x8, x0
   2cb88:	str	x8, [x9]
   2cb8c:	b.cs	2cbbc <__gmpn_mul_fft@@Base+0x1b10>  // b.hs, b.nlast
   2cb90:	sub	x10, x25, x19
   2cb94:	mov	w8, #0x1                   	// #1
   2cb98:	mov	w11, #0x1                   	// #1
   2cb9c:	cmp	x11, x10
   2cba0:	b.ge	2cbc0 <__gmpn_mul_fft@@Base+0x1b14>  // b.tcont
   2cba4:	lsl	x12, x11, #3
   2cba8:	ldr	x13, [x9, x12]
   2cbac:	add	x11, x11, #0x1
   2cbb0:	sub	x14, x13, #0x1
   2cbb4:	str	x14, [x9, x12]
   2cbb8:	cbz	x13, 2cb9c <__gmpn_mul_fft@@Base+0x1af0>
   2cbbc:	mov	x8, xzr
   2cbc0:	subs	x0, x20, x8
   2cbc4:	b.pl	2cc04 <__gmpn_mul_fft@@Base+0x1b58>  // b.nfrst
   2cbc8:	ldr	x8, [x21]
   2cbcc:	adds	x8, x8, #0x1
   2cbd0:	str	x8, [x21]
   2cbd4:	b.cc	2cc00 <__gmpn_mul_fft@@Base+0x1b54>  // b.lo, b.ul, b.last
   2cbd8:	mov	w0, #0x1                   	// #1
   2cbdc:	mov	w8, #0x1                   	// #1
   2cbe0:	cmp	x8, x25
   2cbe4:	b.ge	2cc04 <__gmpn_mul_fft@@Base+0x1b58>  // b.tcont
   2cbe8:	lsl	x9, x8, #3
   2cbec:	ldr	x10, [x21, x9]
   2cbf0:	add	x8, x8, #0x1
   2cbf4:	adds	x10, x10, #0x1
   2cbf8:	str	x10, [x21, x9]
   2cbfc:	b.cs	2cbe0 <__gmpn_mul_fft@@Base+0x1b34>  // b.hs, b.nlast
   2cc00:	mov	x0, xzr
   2cc04:	ldp	x20, x19, [sp, #336]
   2cc08:	ldp	x22, x21, [sp, #320]
   2cc0c:	ldp	x24, x23, [sp, #304]
   2cc10:	ldp	x26, x25, [sp, #288]
   2cc14:	ldp	x28, x27, [sp, #272]
   2cc18:	ldp	x29, x30, [sp, #256]
   2cc1c:	add	sp, sp, #0x160
   2cc20:	ret
   2cc24:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2cc28:	b	2beac <__gmpn_mul_fft@@Base+0xe00>
   2cc2c:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2cc30:	adrp	x2, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2cc34:	add	x0, x0, #0xd7e
   2cc38:	add	x2, x2, #0xdbf
   2cc3c:	mov	w1, #0x1d9                 	// #473
   2cc40:	bl	c6c0 <__gmp_assert_fail@plt>
   2cc44:	adrp	x0, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2cc48:	adrp	x2, 5c000 <__gmpn_bases@@Base+0x1ab8>
   2cc4c:	add	x0, x0, #0xd7e
   2cc50:	add	x2, x2, #0xdd3
   2cc54:	mov	w1, #0x1ef                 	// #495
   2cc58:	bl	c6c0 <__gmp_assert_fail@plt>
   2cc5c:	stp	x29, x30, [sp, #-80]!
   2cc60:	stp	x24, x23, [sp, #32]
   2cc64:	lsr	x24, x2, #6
   2cc68:	stp	x22, x21, [sp, #48]
   2cc6c:	stp	x20, x19, [sp, #64]
   2cc70:	mov	x19, x3
   2cc74:	mov	x22, x1
   2cc78:	mov	x20, x0
   2cc7c:	subs	x21, x24, x3
   2cc80:	and	w23, w2, #0x3f
   2cc84:	add	x8, x1, x3, lsl #3
   2cc88:	str	x25, [sp, #16]
   2cc8c:	mov	x29, sp
   2cc90:	b.ge	2ccd0 <__gmpn_mul_fft@@Base+0x1c24>  // b.tcont
   2cc94:	sub	x1, x8, x24, lsl #3
   2cc98:	add	x2, x24, #0x1
   2cc9c:	mov	x0, x20
   2cca0:	cbz	w23, 2cd08 <__gmpn_mul_fft@@Base+0x1c5c>
   2cca4:	mov	w3, w23
   2cca8:	bl	d160 <__gmpn_lshiftc@plt>
   2ccac:	add	x0, x20, x24, lsl #3
   2ccb0:	ldr	x8, [x0]
   2ccb4:	sub	x2, x19, x24
   2ccb8:	mov	x1, x22
   2ccbc:	mov	w3, w23
   2ccc0:	mvn	x21, x8
   2ccc4:	bl	c180 <__gmpn_lshift@plt>
   2ccc8:	cbnz	x24, 2cd28 <__gmpn_mul_fft@@Base+0x1c7c>
   2cccc:	b	2ce40 <__gmpn_mul_fft@@Base+0x1d94>
   2ccd0:	sub	x1, x8, x21, lsl #3
   2ccd4:	cbz	w23, 2cd6c <__gmpn_mul_fft@@Base+0x1cc0>
   2ccd8:	add	x2, x21, #0x1
   2ccdc:	mov	x0, x20
   2cce0:	mov	w3, w23
   2cce4:	bl	c180 <__gmpn_lshift@plt>
   2cce8:	add	x0, x20, x21, lsl #3
   2ccec:	ldr	x25, [x0]
   2ccf0:	sub	x2, x19, x21
   2ccf4:	mov	x1, x22
   2ccf8:	mov	w3, w23
   2ccfc:	bl	d160 <__gmpn_lshiftc@plt>
   2cd00:	add	x8, x0, #0x1
   2cd04:	b	2cd90 <__gmpn_mul_fft@@Base+0x1ce4>
   2cd08:	bl	c290 <__gmpn_com@plt>
   2cd0c:	ldr	x21, [x22, x19, lsl #3]
   2cd10:	add	x0, x20, x24, lsl #3
   2cd14:	sub	x2, x19, x24
   2cd18:	mov	x1, x22
   2cd1c:	bl	ca50 <__gmpn_copyi@plt>
   2cd20:	mov	x0, xzr
   2cd24:	cbz	x24, 2ce40 <__gmpn_mul_fft@@Base+0x1d94>
   2cd28:	cbz	x0, 2ce00 <__gmpn_mul_fft@@Base+0x1d54>
   2cd2c:	sub	x0, x0, #0x1
   2cd30:	ldr	x8, [x20]
   2cd34:	subs	x8, x8, x0
   2cd38:	str	x8, [x20]
   2cd3c:	b.cs	2ce3c <__gmpn_mul_fft@@Base+0x1d90>  // b.hs, b.nlast
   2cd40:	mov	w0, #0x2                   	// #2
   2cd44:	mov	w8, #0x1                   	// #1
   2cd48:	cmp	x8, x24
   2cd4c:	b.cs	2ce40 <__gmpn_mul_fft@@Base+0x1d94>  // b.hs, b.nlast
   2cd50:	lsl	x9, x8, #3
   2cd54:	ldr	x10, [x20, x9]
   2cd58:	add	x8, x8, #0x1
   2cd5c:	sub	x11, x10, #0x1
   2cd60:	str	x11, [x20, x9]
   2cd64:	cbz	x10, 2cd48 <__gmpn_mul_fft@@Base+0x1c9c>
   2cd68:	b	2ce3c <__gmpn_mul_fft@@Base+0x1d90>
   2cd6c:	mov	x0, x20
   2cd70:	mov	x2, x21
   2cd74:	bl	ca50 <__gmpn_copyi@plt>
   2cd78:	ldr	x25, [x22, x19, lsl #3]
   2cd7c:	add	x0, x20, x21, lsl #3
   2cd80:	sub	x2, x19, x21
   2cd84:	mov	x1, x22
   2cd88:	bl	c290 <__gmpn_com@plt>
   2cd8c:	mov	w8, #0x1                   	// #1
   2cd90:	str	xzr, [x20, x19, lsl #3]
   2cd94:	ldr	x9, [x20]
   2cd98:	adds	x8, x9, x8
   2cd9c:	str	x8, [x20]
   2cda0:	b.cc	2cdb8 <__gmpn_mul_fft@@Base+0x1d0c>  // b.lo, b.ul, b.last
   2cda4:	add	x8, x20, #0x8
   2cda8:	ldr	x9, [x8]
   2cdac:	adds	x9, x9, #0x1
   2cdb0:	str	x9, [x8], #8
   2cdb4:	b.cs	2cda8 <__gmpn_mul_fft@@Base+0x1cfc>  // b.hs, b.nlast
   2cdb8:	adds	x9, x25, #0x1
   2cdbc:	cset	w8, cs  // cs = hs, nlast
   2cdc0:	add	x10, x20, x21, lsl #3
   2cdc4:	lsl	x11, x8, #3
   2cdc8:	ldr	x12, [x10, x11]
   2cdcc:	csinc	x9, x9, xzr, cc  // cc = lo, ul, last
   2cdd0:	adds	x9, x12, x9
   2cdd4:	str	x9, [x10, x11]
   2cdd8:	b.cc	2cf1c <__gmpn_mul_fft@@Base+0x1e70>  // b.lo, b.ul, b.last
   2cddc:	add	x8, x24, x8
   2cde0:	sub	x8, x8, x19
   2cde4:	add	x8, x20, x8, lsl #3
   2cde8:	add	x8, x8, #0x8
   2cdec:	ldr	x9, [x8]
   2cdf0:	adds	x9, x9, #0x1
   2cdf4:	str	x9, [x8], #8
   2cdf8:	b.cs	2cdec <__gmpn_mul_fft@@Base+0x1d40>  // b.hs, b.nlast
   2cdfc:	b	2cf1c <__gmpn_mul_fft@@Base+0x1e70>
   2ce00:	ldr	x8, [x20]
   2ce04:	adds	x8, x8, #0x1
   2ce08:	str	x8, [x20]
   2ce0c:	b.cc	2ce3c <__gmpn_mul_fft@@Base+0x1d90>  // b.lo, b.ul, b.last
   2ce10:	mov	w0, #0x1                   	// #1
   2ce14:	mov	w8, #0x1                   	// #1
   2ce18:	cmp	x8, x19
   2ce1c:	b.ge	2cd30 <__gmpn_mul_fft@@Base+0x1c84>  // b.tcont
   2ce20:	lsl	x9, x8, #3
   2ce24:	ldr	x10, [x20, x9]
   2ce28:	add	x8, x8, #0x1
   2ce2c:	adds	x10, x10, #0x1
   2ce30:	str	x10, [x20, x9]
   2ce34:	b.cs	2ce18 <__gmpn_mul_fft@@Base+0x1d6c>  // b.hs, b.nlast
   2ce38:	b	2ce40 <__gmpn_mul_fft@@Base+0x1d94>
   2ce3c:	mov	w0, #0x1                   	// #1
   2ce40:	add	x8, x20, x24, lsl #3
   2ce44:	ldr	x10, [x8]
   2ce48:	sub	x9, x19, x24
   2ce4c:	subs	x10, x10, x0
   2ce50:	str	x10, [x8]
   2ce54:	b.cs	2ce7c <__gmpn_mul_fft@@Base+0x1dd0>  // b.hs, b.nlast
   2ce58:	mov	w10, #0x1                   	// #1
   2ce5c:	cmp	x10, x9
   2ce60:	b.ge	2ce84 <__gmpn_mul_fft@@Base+0x1dd8>  // b.tcont
   2ce64:	lsl	x11, x10, #3
   2ce68:	ldr	x12, [x8, x11]
   2ce6c:	add	x10, x10, #0x1
   2ce70:	sub	x13, x12, #0x1
   2ce74:	str	x13, [x8, x11]
   2ce78:	cbz	x12, 2ce5c <__gmpn_mul_fft@@Base+0x1db0>
   2ce7c:	mov	x10, xzr
   2ce80:	b	2ce88 <__gmpn_mul_fft@@Base+0x1ddc>
   2ce84:	mov	x10, #0xffffffffffffffff    	// #-1
   2ce88:	str	x10, [x20, x19, lsl #3]
   2ce8c:	ldr	x10, [x8]
   2ce90:	subs	x10, x10, x21
   2ce94:	str	x10, [x8]
   2ce98:	b.cs	2cec4 <__gmpn_mul_fft@@Base+0x1e18>  // b.hs, b.nlast
   2ce9c:	mov	w10, #0x1                   	// #1
   2cea0:	mov	w11, #0x1                   	// #1
   2cea4:	cmp	x11, x9
   2cea8:	b.ge	2cec8 <__gmpn_mul_fft@@Base+0x1e1c>  // b.tcont
   2ceac:	lsl	x12, x11, #3
   2ceb0:	ldr	x13, [x8, x12]
   2ceb4:	add	x11, x11, #0x1
   2ceb8:	sub	x14, x13, #0x1
   2cebc:	str	x14, [x8, x12]
   2cec0:	cbz	x13, 2cea4 <__gmpn_mul_fft@@Base+0x1df8>
   2cec4:	mov	x10, xzr
   2cec8:	lsl	x8, x19, #3
   2cecc:	ldr	x9, [x20, x8]
   2ced0:	subs	x9, x9, x10
   2ced4:	str	x9, [x20, x8]
   2ced8:	b.pl	2cf1c <__gmpn_mul_fft@@Base+0x1e70>  // b.nfrst
   2cedc:	ldr	x8, [x20]
   2cee0:	adds	x8, x8, #0x1
   2cee4:	str	x8, [x20]
   2cee8:	b.cc	2cf14 <__gmpn_mul_fft@@Base+0x1e68>  // b.lo, b.ul, b.last
   2ceec:	mov	w8, #0x1                   	// #1
   2cef0:	mov	w9, #0x1                   	// #1
   2cef4:	cmp	x9, x19
   2cef8:	b.ge	2cf18 <__gmpn_mul_fft@@Base+0x1e6c>  // b.tcont
   2cefc:	lsl	x10, x9, #3
   2cf00:	ldr	x11, [x20, x10]
   2cf04:	add	x9, x9, #0x1
   2cf08:	adds	x11, x11, #0x1
   2cf0c:	str	x11, [x20, x10]
   2cf10:	b.cs	2cef4 <__gmpn_mul_fft@@Base+0x1e48>  // b.hs, b.nlast
   2cf14:	mov	x8, xzr
   2cf18:	str	x8, [x20, x19, lsl #3]
   2cf1c:	ldp	x20, x19, [sp, #64]
   2cf20:	ldp	x22, x21, [sp, #48]
   2cf24:	ldp	x24, x23, [sp, #32]
   2cf28:	ldr	x25, [sp, #16]
   2cf2c:	ldp	x29, x30, [sp], #80
   2cf30:	ret
   2cf34:	sub	sp, sp, #0x70
   2cf38:	stp	x26, x25, [sp, #48]
   2cf3c:	stp	x22, x21, [sp, #80]
   2cf40:	stp	x20, x19, [sp, #96]
   2cf44:	mov	x21, x6
   2cf48:	mov	x25, x5
   2cf4c:	mov	x19, x4
   2cf50:	cmp	x1, #0x2
   2cf54:	mov	x20, x0
   2cf58:	stp	x29, x30, [sp, #16]
   2cf5c:	stp	x28, x27, [sp, #32]
   2cf60:	stp	x24, x23, [sp, #64]
   2cf64:	add	x29, sp, #0x10
   2cf68:	b.ne	2cffc <__gmpn_mul_fft@@Base+0x1f50>  // b.any
   2cf6c:	ldr	x1, [x20]
   2cf70:	add	x22, x19, #0x1
   2cf74:	mov	x0, x21
   2cf78:	mov	x2, x22
   2cf7c:	bl	ca50 <__gmpn_copyi@plt>
   2cf80:	ldr	x0, [x20]
   2cf84:	lsl	x23, x25, #3
   2cf88:	ldr	x2, [x20, x23]
   2cf8c:	mov	x3, x22
   2cf90:	mov	x1, x0
   2cf94:	bl	ca70 <__gmpn_add_n@plt>
   2cf98:	ldr	x0, [x20, x23]
   2cf9c:	mov	x1, x21
   2cfa0:	mov	x3, x22
   2cfa4:	mov	x2, x0
   2cfa8:	bl	c2d0 <__gmpn_sub_n@plt>
   2cfac:	ldr	x8, [x20]
   2cfb0:	ldr	x9, [x8, x19, lsl #3]
   2cfb4:	cmp	x9, #0x2
   2cfb8:	b.cc	2d174 <__gmpn_mul_fft@@Base+0x20c8>  // b.lo, b.ul, b.last
   2cfbc:	ldr	x10, [x8]
   2cfc0:	sub	x9, x9, #0x1
   2cfc4:	subs	x9, x10, x9
   2cfc8:	str	x9, [x8]
   2cfcc:	mov	w9, #0x1                   	// #1
   2cfd0:	b.cs	2d170 <__gmpn_mul_fft@@Base+0x20c4>  // b.hs, b.nlast
   2cfd4:	mov	w10, #0x1                   	// #1
   2cfd8:	cmp	x10, x19
   2cfdc:	b.ge	2d16c <__gmpn_mul_fft@@Base+0x20c0>  // b.tcont
   2cfe0:	lsl	x11, x10, #3
   2cfe4:	ldr	x12, [x8, x11]
   2cfe8:	add	x10, x10, #0x1
   2cfec:	sub	x13, x12, #0x1
   2cff0:	str	x13, [x8, x11]
   2cff4:	cbz	x12, 2cfd8 <__gmpn_mul_fft@@Base+0x1f2c>
   2cff8:	b	2d170 <__gmpn_mul_fft@@Base+0x20c4>
   2cffc:	mov	x27, x2
   2d000:	ldr	x28, [x27], #-8
   2d004:	asr	x22, x1, #1
   2d008:	lsl	x24, x25, #1
   2d00c:	mov	x23, x3
   2d010:	mov	x26, x1
   2d014:	lsl	x3, x3, #1
   2d018:	mov	x0, x20
   2d01c:	mov	x1, x22
   2d020:	mov	x2, x27
   2d024:	mov	x4, x19
   2d028:	mov	x5, x24
   2d02c:	mov	x6, x21
   2d030:	str	x3, [sp]
   2d034:	bl	2cf34 <__gmpn_mul_fft@@Base+0x1e88>
   2d038:	ldr	x3, [sp]
   2d03c:	add	x0, x20, x25, lsl #3
   2d040:	mov	x1, x22
   2d044:	mov	x2, x27
   2d048:	mov	x4, x19
   2d04c:	mov	x5, x24
   2d050:	mov	x6, x21
   2d054:	stp	x24, x22, [sp]
   2d058:	bl	2cf34 <__gmpn_mul_fft@@Base+0x1e88>
   2d05c:	cmp	x26, #0x2
   2d060:	b.lt	2d1cc <__gmpn_mul_fft@@Base+0x2120>  // b.tstop
   2d064:	mov	x26, xzr
   2d068:	lsl	x27, x25, #3
   2d06c:	lsl	x22, x19, #3
   2d070:	b	2d094 <__gmpn_mul_fft@@Base+0x1fe8>
   2d074:	ldr	x8, [sp, #8]
   2d078:	add	x26, x26, #0x1
   2d07c:	add	x28, x28, #0x8
   2d080:	mov	x23, x24
   2d084:	cmp	x26, x8
   2d088:	ldr	x8, [sp]
   2d08c:	add	x20, x20, x8, lsl #3
   2d090:	b.ge	2d1cc <__gmpn_mul_fft@@Base+0x2120>  // b.tcont
   2d094:	ldrsw	x8, [x28]
   2d098:	ldr	x1, [x20, x27]
   2d09c:	mov	x0, x21
   2d0a0:	mov	x3, x19
   2d0a4:	mul	x2, x8, x23
   2d0a8:	mov	x24, x23
   2d0ac:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2d0b0:	ldr	x1, [x20]
   2d0b4:	ldr	x25, [x20, x27]
   2d0b8:	ldr	x9, [x21, x22]
   2d0bc:	mov	x2, x21
   2d0c0:	ldr	x8, [x1, x22]
   2d0c4:	mov	x0, x25
   2d0c8:	mov	x3, x19
   2d0cc:	sub	x23, x8, x9
   2d0d0:	bl	c2d0 <__gmpn_sub_n@plt>
   2d0d4:	sub	x8, x23, x0
   2d0d8:	neg	x9, x8
   2d0dc:	and	x9, x9, x8, asr #63
   2d0e0:	add	x8, x9, x8
   2d0e4:	str	x8, [x25, x22]
   2d0e8:	ldr	x8, [x25]
   2d0ec:	adds	x8, x8, x9
   2d0f0:	str	x8, [x25]
   2d0f4:	b.cc	2d10c <__gmpn_mul_fft@@Base+0x2060>  // b.lo, b.ul, b.last
   2d0f8:	add	x8, x25, #0x8
   2d0fc:	ldr	x9, [x8]
   2d100:	adds	x9, x9, #0x1
   2d104:	str	x9, [x8], #8
   2d108:	b.cs	2d0fc <__gmpn_mul_fft@@Base+0x2050>  // b.hs, b.nlast
   2d10c:	ldr	x25, [x20]
   2d110:	ldr	x9, [x21, x22]
   2d114:	mov	x2, x21
   2d118:	mov	x3, x19
   2d11c:	ldr	x8, [x25, x22]
   2d120:	mov	x0, x25
   2d124:	mov	x1, x25
   2d128:	add	x23, x9, x8
   2d12c:	bl	ca70 <__gmpn_add_n@plt>
   2d130:	adds	x8, x23, x0
   2d134:	sub	x9, x8, #0x1
   2d138:	csel	x9, xzr, x9, eq  // eq = none
   2d13c:	sub	x8, x8, x9
   2d140:	str	x8, [x25, x22]
   2d144:	ldr	x8, [x25]
   2d148:	subs	x8, x8, x9
   2d14c:	str	x8, [x25]
   2d150:	b.cs	2d074 <__gmpn_mul_fft@@Base+0x1fc8>  // b.hs, b.nlast
   2d154:	add	x8, x25, #0x8
   2d158:	ldr	x9, [x8]
   2d15c:	sub	x10, x9, #0x1
   2d160:	str	x10, [x8], #8
   2d164:	cbz	x9, 2d158 <__gmpn_mul_fft@@Base+0x20ac>
   2d168:	b	2d074 <__gmpn_mul_fft@@Base+0x1fc8>
   2d16c:	mov	x9, xzr
   2d170:	str	x9, [x8, x19, lsl #3]
   2d174:	cbz	x0, 2d1cc <__gmpn_mul_fft@@Base+0x2120>
   2d178:	ldr	x8, [x20, x25, lsl #3]
   2d17c:	mov	x9, xzr
   2d180:	ldr	x10, [x8, x19, lsl #3]
   2d184:	ldr	x11, [x8]
   2d188:	neg	x12, x10
   2d18c:	sub	x10, x11, x10
   2d190:	cmp	x10, x12
   2d194:	str	x10, [x8]
   2d198:	b.cs	2d1c8 <__gmpn_mul_fft@@Base+0x211c>  // b.hs, b.nlast
   2d19c:	mov	w9, #0x1                   	// #1
   2d1a0:	mov	w10, #0x1                   	// #1
   2d1a4:	cmp	x10, x19
   2d1a8:	b.ge	2d1c8 <__gmpn_mul_fft@@Base+0x211c>  // b.tcont
   2d1ac:	lsl	x11, x10, #3
   2d1b0:	ldr	x12, [x8, x11]
   2d1b4:	add	x10, x10, #0x1
   2d1b8:	adds	x12, x12, #0x1
   2d1bc:	str	x12, [x8, x11]
   2d1c0:	b.cs	2d1a4 <__gmpn_mul_fft@@Base+0x20f8>  // b.hs, b.nlast
   2d1c4:	mov	x9, xzr
   2d1c8:	str	x9, [x8, x19, lsl #3]
   2d1cc:	ldp	x20, x19, [sp, #96]
   2d1d0:	ldp	x22, x21, [sp, #80]
   2d1d4:	ldp	x24, x23, [sp, #64]
   2d1d8:	ldp	x26, x25, [sp, #48]
   2d1dc:	ldp	x28, x27, [sp, #32]
   2d1e0:	ldp	x29, x30, [sp, #16]
   2d1e4:	add	sp, sp, #0x70
   2d1e8:	ret
   2d1ec:	stp	x29, x30, [sp, #-96]!
   2d1f0:	stp	x22, x21, [sp, #64]
   2d1f4:	stp	x20, x19, [sp, #80]
   2d1f8:	mov	x21, x4
   2d1fc:	mov	x19, x3
   2d200:	cmp	x1, #0x2
   2d204:	mov	x20, x0
   2d208:	stp	x28, x27, [sp, #16]
   2d20c:	stp	x26, x25, [sp, #32]
   2d210:	stp	x24, x23, [sp, #48]
   2d214:	mov	x29, sp
   2d218:	b.ne	2d2a4 <__gmpn_mul_fft@@Base+0x21f8>  // b.any
   2d21c:	ldr	x1, [x20]
   2d220:	add	x22, x19, #0x1
   2d224:	mov	x0, x21
   2d228:	mov	x2, x22
   2d22c:	bl	ca50 <__gmpn_copyi@plt>
   2d230:	ldp	x0, x2, [x20]
   2d234:	mov	x3, x22
   2d238:	mov	x1, x0
   2d23c:	bl	ca70 <__gmpn_add_n@plt>
   2d240:	ldr	x0, [x20, #8]
   2d244:	mov	x1, x21
   2d248:	mov	x3, x22
   2d24c:	mov	x2, x0
   2d250:	bl	c2d0 <__gmpn_sub_n@plt>
   2d254:	ldr	x8, [x20]
   2d258:	ldr	x9, [x8, x19, lsl #3]
   2d25c:	cmp	x9, #0x2
   2d260:	b.cc	2d3e4 <__gmpn_mul_fft@@Base+0x2338>  // b.lo, b.ul, b.last
   2d264:	ldr	x10, [x8]
   2d268:	sub	x9, x9, #0x1
   2d26c:	subs	x9, x10, x9
   2d270:	str	x9, [x8]
   2d274:	mov	w9, #0x1                   	// #1
   2d278:	b.cs	2d3e0 <__gmpn_mul_fft@@Base+0x2334>  // b.hs, b.nlast
   2d27c:	mov	w10, #0x1                   	// #1
   2d280:	cmp	x10, x19
   2d284:	b.ge	2d3dc <__gmpn_mul_fft@@Base+0x2330>  // b.tcont
   2d288:	lsl	x11, x10, #3
   2d28c:	ldr	x12, [x8, x11]
   2d290:	add	x10, x10, #0x1
   2d294:	sub	x13, x12, #0x1
   2d298:	str	x13, [x8, x11]
   2d29c:	cbz	x12, 2d280 <__gmpn_mul_fft@@Base+0x21d4>
   2d2a0:	b	2d3e0 <__gmpn_mul_fft@@Base+0x2334>
   2d2a4:	asr	x23, x1, #1
   2d2a8:	lsl	x25, x2, #1
   2d2ac:	mov	x22, x2
   2d2b0:	mov	x24, x1
   2d2b4:	mov	x0, x20
   2d2b8:	mov	x1, x23
   2d2bc:	mov	x2, x25
   2d2c0:	mov	x3, x19
   2d2c4:	mov	x4, x21
   2d2c8:	bl	2d1ec <__gmpn_mul_fft@@Base+0x2140>
   2d2cc:	add	x0, x20, x23, lsl #3
   2d2d0:	mov	x1, x23
   2d2d4:	mov	x2, x25
   2d2d8:	mov	x3, x19
   2d2dc:	mov	x4, x21
   2d2e0:	bl	2d1ec <__gmpn_mul_fft@@Base+0x2140>
   2d2e4:	cmp	x24, #0x2
   2d2e8:	b.lt	2d43c <__gmpn_mul_fft@@Base+0x2390>  // b.tstop
   2d2ec:	mov	x25, xzr
   2d2f0:	lsl	x26, x23, #3
   2d2f4:	lsl	x27, x19, #3
   2d2f8:	b	2d30c <__gmpn_mul_fft@@Base+0x2260>
   2d2fc:	add	x25, x25, #0x1
   2d300:	cmp	x25, x23
   2d304:	add	x20, x20, #0x8
   2d308:	b.ge	2d43c <__gmpn_mul_fft@@Base+0x2390>  // b.tcont
   2d30c:	ldr	x1, [x20, x26]
   2d310:	mul	x2, x25, x22
   2d314:	mov	x0, x21
   2d318:	mov	x3, x19
   2d31c:	bl	2cc5c <__gmpn_mul_fft@@Base+0x1bb0>
   2d320:	ldr	x1, [x20]
   2d324:	ldr	x24, [x20, x26]
   2d328:	ldr	x9, [x21, x27]
   2d32c:	mov	x2, x21
   2d330:	ldr	x8, [x1, x27]
   2d334:	mov	x0, x24
   2d338:	mov	x3, x19
   2d33c:	sub	x28, x8, x9
   2d340:	bl	c2d0 <__gmpn_sub_n@plt>
   2d344:	sub	x8, x28, x0
   2d348:	neg	x9, x8
   2d34c:	and	x9, x9, x8, asr #63
   2d350:	add	x8, x9, x8
   2d354:	str	x8, [x24, x27]
   2d358:	ldr	x8, [x24]
   2d35c:	adds	x8, x8, x9
   2d360:	str	x8, [x24]
   2d364:	b.cc	2d37c <__gmpn_mul_fft@@Base+0x22d0>  // b.lo, b.ul, b.last
   2d368:	add	x8, x24, #0x8
   2d36c:	ldr	x9, [x8]
   2d370:	adds	x9, x9, #0x1
   2d374:	str	x9, [x8], #8
   2d378:	b.cs	2d36c <__gmpn_mul_fft@@Base+0x22c0>  // b.hs, b.nlast
   2d37c:	ldr	x24, [x20]
   2d380:	ldr	x9, [x21, x27]
   2d384:	mov	x2, x21
   2d388:	mov	x3, x19
   2d38c:	ldr	x8, [x24, x27]
   2d390:	mov	x0, x24
   2d394:	mov	x1, x24
   2d398:	add	x28, x9, x8
   2d39c:	bl	ca70 <__gmpn_add_n@plt>
   2d3a0:	adds	x8, x28, x0
   2d3a4:	sub	x9, x8, #0x1
   2d3a8:	csel	x9, xzr, x9, eq  // eq = none
   2d3ac:	sub	x8, x8, x9
   2d3b0:	str	x8, [x24, x27]
   2d3b4:	ldr	x8, [x24]
   2d3b8:	subs	x8, x8, x9
   2d3bc:	str	x8, [x24]
   2d3c0:	b.cs	2d2fc <__gmpn_mul_fft@@Base+0x2250>  // b.hs, b.nlast
   2d3c4:	add	x8, x24, #0x8
   2d3c8:	ldr	x9, [x8]
   2d3cc:	sub	x10, x9, #0x1
   2d3d0:	str	x10, [x8], #8
   2d3d4:	cbz	x9, 2d3c8 <__gmpn_mul_fft@@Base+0x231c>
   2d3d8:	b	2d2fc <__gmpn_mul_fft@@Base+0x2250>
   2d3dc:	mov	x9, xzr
   2d3e0:	str	x9, [x8, x19, lsl #3]
   2d3e4:	cbz	x0, 2d43c <__gmpn_mul_fft@@Base+0x2390>
   2d3e8:	ldr	x8, [x20, #8]
   2d3ec:	mov	x9, xzr
   2d3f0:	ldr	x10, [x8, x19, lsl #3]
   2d3f4:	ldr	x11, [x8]
   2d3f8:	neg	x12, x10
   2d3fc:	sub	x10, x11, x10
   2d400:	cmp	x10, x12
   2d404:	str	x10, [x8]
   2d408:	b.cs	2d438 <__gmpn_mul_fft@@Base+0x238c>  // b.hs, b.nlast
   2d40c:	mov	w9, #0x1                   	// #1
   2d410:	mov	w10, #0x1                   	// #1
   2d414:	cmp	x10, x19
   2d418:	b.ge	2d438 <__gmpn_mul_fft@@Base+0x238c>  // b.tcont
   2d41c:	lsl	x11, x10, #3
   2d420:	ldr	x12, [x8, x11]
   2d424:	add	x10, x10, #0x1
   2d428:	adds	x12, x12, #0x1
   2d42c:	str	x12, [x8, x11]
   2d430:	b.cs	2d414 <__gmpn_mul_fft@@Base+0x2368>  // b.hs, b.nlast
   2d434:	mov	x9, xzr
   2d438:	str	x9, [x8, x19, lsl #3]
   2d43c:	ldp	x20, x19, [sp, #80]
   2d440:	ldp	x22, x21, [sp, #64]
   2d444:	ldp	x24, x23, [sp, #48]
   2d448:	ldp	x26, x25, [sp, #32]
   2d44c:	ldp	x28, x27, [sp, #16]
   2d450:	ldp	x29, x30, [sp], #96
   2d454:	ret

000000000002d458 <__gmpn_mul_n@@Base>:
   2d458:	stp	x29, x30, [sp, #-32]!
   2d45c:	stp	x28, x19, [sp, #16]
   2d460:	mov	x29, sp
   2d464:	sub	sp, sp, #0x720
   2d468:	mov	x4, x3
   2d46c:	mov	x3, x2
   2d470:	cmp	x4, #0xd
   2d474:	mov	x19, sp
   2d478:	b.le	2d4b8 <__gmpn_mul_n@@Base+0x60>
   2d47c:	cmp	x4, #0x30
   2d480:	b.le	2d4cc <__gmpn_mul_n@@Base+0x74>
   2d484:	cmp	x4, #0x51
   2d488:	b.le	2d4e8 <__gmpn_mul_n@@Base+0x90>
   2d48c:	cmp	x4, #0xac
   2d490:	b.le	2d51c <__gmpn_mul_n@@Base+0xc4>
   2d494:	cmp	x4, #0xeb
   2d498:	b.le	2d550 <__gmpn_mul_n@@Base+0xf8>
   2d49c:	cmp	x4, #0xc7f
   2d4a0:	b.le	2d578 <__gmpn_mul_n@@Base+0x120>
   2d4a4:	mov	x2, x4
   2d4a8:	mov	sp, x29
   2d4ac:	ldp	x28, x19, [sp, #16]
   2d4b0:	ldp	x29, x30, [sp], #32
   2d4b4:	b	cca0 <__gmpn_nussbaumer_mul@plt>
   2d4b8:	mov	x2, x4
   2d4bc:	mov	sp, x29
   2d4c0:	ldp	x28, x19, [sp, #16]
   2d4c4:	ldp	x29, x30, [sp], #32
   2d4c8:	b	c550 <__gmpn_mul_basecase@plt>
   2d4cc:	add	x5, x19, #0x20
   2d4d0:	mov	x2, x4
   2d4d4:	bl	d450 <__gmpn_toom22_mul@plt>
   2d4d8:	mov	sp, x29
   2d4dc:	ldp	x28, x19, [sp, #16]
   2d4e0:	ldp	x29, x30, [sp], #32
   2d4e4:	ret
   2d4e8:	mov	w8, #0x18                  	// #24
   2d4ec:	mul	x8, x4, x8
   2d4f0:	add	x8, x8, #0x20f
   2d4f4:	and	x8, x8, #0xfffffffffffffff0
   2d4f8:	mov	x9, sp
   2d4fc:	sub	x5, x9, x8
   2d500:	mov	sp, x5
   2d504:	mov	x2, x4
   2d508:	bl	c0a0 <__gmpn_toom33_mul@plt>
   2d50c:	mov	sp, x29
   2d510:	ldp	x28, x19, [sp, #16]
   2d514:	ldp	x29, x30, [sp], #32
   2d518:	ret
   2d51c:	mov	w8, #0x18                  	// #24
   2d520:	mul	x8, x4, x8
   2d524:	add	x8, x8, #0x20f
   2d528:	and	x8, x8, #0xfffffffffffffff0
   2d52c:	mov	x9, sp
   2d530:	sub	x5, x9, x8
   2d534:	mov	sp, x5
   2d538:	mov	x2, x4
   2d53c:	bl	c720 <__gmpn_toom44_mul@plt>
   2d540:	mov	sp, x29
   2d544:	ldp	x28, x19, [sp, #16]
   2d548:	ldp	x29, x30, [sp], #32
   2d54c:	ret
   2d550:	mov	x8, sp
   2d554:	sub	x8, x8, x4, lsl #4
   2d558:	sub	x5, x8, #0xc00
   2d55c:	mov	sp, x5
   2d560:	mov	x2, x4
   2d564:	bl	cc20 <__gmpn_toom6h_mul@plt>
   2d568:	mov	sp, x29
   2d56c:	ldp	x28, x19, [sp, #16]
   2d570:	ldp	x29, x30, [sp], #32
   2d574:	ret
   2d578:	lsl	x8, x4, #4
   2d57c:	sub	x8, x8, x4
   2d580:	add	x8, x8, #0xcf0
   2d584:	and	x8, x8, #0xfffffffffffffff8
   2d588:	mov	w9, #0x7f00                	// #32512
   2d58c:	cmp	x8, x9
   2d590:	str	xzr, [x19, #32]
   2d594:	b.hi	2d5d0 <__gmpn_mul_n@@Base+0x178>  // b.pmore
   2d598:	add	x8, x8, #0xf
   2d59c:	mov	x9, sp
   2d5a0:	and	x8, x8, #0xfffffffffffffff0
   2d5a4:	sub	x5, x9, x8
   2d5a8:	mov	sp, x5
   2d5ac:	mov	x2, x4
   2d5b0:	bl	cb20 <__gmpn_toom8h_mul@plt>
   2d5b4:	ldr	x0, [x19, #32]
   2d5b8:	cbz	x0, 2d4d8 <__gmpn_mul_n@@Base+0x80>
   2d5bc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2d5c0:	mov	sp, x29
   2d5c4:	ldp	x28, x19, [sp, #16]
   2d5c8:	ldp	x29, x30, [sp], #32
   2d5cc:	ret
   2d5d0:	stp	x1, x0, [x19, #16]
   2d5d4:	add	x0, x19, #0x20
   2d5d8:	mov	x1, x8
   2d5dc:	stp	x3, x4, [x19]
   2d5e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2d5e4:	ldp	x4, x1, [x19, #8]
   2d5e8:	ldr	x3, [x19]
   2d5ec:	mov	x5, x0
   2d5f0:	ldr	x0, [x19, #24]
   2d5f4:	mov	x2, x4
   2d5f8:	bl	cb20 <__gmpn_toom8h_mul@plt>
   2d5fc:	ldr	x0, [x19, #32]
   2d600:	cbz	x0, 2d4d8 <__gmpn_mul_n@@Base+0x80>
   2d604:	b	2d5bc <__gmpn_mul_n@@Base+0x164>

000000000002d608 <__gmpn_sqr@@Base>:
   2d608:	stp	x29, x30, [sp, #-32]!
   2d60c:	stp	x28, x19, [sp, #16]
   2d610:	mov	x29, sp
   2d614:	sub	sp, sp, #0x840
   2d618:	mov	x8, x1
   2d61c:	cmp	x2, #0x11
   2d620:	mov	x19, sp
   2d624:	b.le	2d66c <__gmpn_sqr@@Base+0x64>
   2d628:	cmp	x2, #0x42
   2d62c:	b.le	2d680 <__gmpn_sqr@@Base+0x78>
   2d630:	cmp	x2, #0xa5
   2d634:	b.le	2d69c <__gmpn_sqr@@Base+0x94>
   2d638:	cmp	x2, #0xdd
   2d63c:	b.le	2d6d0 <__gmpn_sqr@@Base+0xc8>
   2d640:	cmp	x2, #0x14c
   2d644:	b.le	2d704 <__gmpn_sqr@@Base+0xfc>
   2d648:	cmp	x2, #0xa7f
   2d64c:	b.le	2d72c <__gmpn_sqr@@Base+0x124>
   2d650:	mov	x1, x8
   2d654:	mov	x3, x8
   2d658:	mov	x4, x2
   2d65c:	mov	sp, x29
   2d660:	ldp	x28, x19, [sp, #16]
   2d664:	ldp	x29, x30, [sp], #32
   2d668:	b	cca0 <__gmpn_nussbaumer_mul@plt>
   2d66c:	mov	x1, x8
   2d670:	mov	sp, x29
   2d674:	ldp	x28, x19, [sp, #16]
   2d678:	ldp	x29, x30, [sp], #32
   2d67c:	b	c190 <__gmpn_sqr_basecase@plt>
   2d680:	add	x3, x19, #0x20
   2d684:	mov	x1, x8
   2d688:	bl	c050 <__gmpn_toom2_sqr@plt>
   2d68c:	mov	sp, x29
   2d690:	ldp	x28, x19, [sp, #16]
   2d694:	ldp	x29, x30, [sp], #32
   2d698:	ret
   2d69c:	mov	w9, #0x18                  	// #24
   2d6a0:	mul	x9, x2, x9
   2d6a4:	add	x9, x9, #0x20f
   2d6a8:	and	x9, x9, #0xfffffffffffffff0
   2d6ac:	mov	x10, sp
   2d6b0:	sub	x3, x10, x9
   2d6b4:	mov	sp, x3
   2d6b8:	mov	x1, x8
   2d6bc:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   2d6c0:	mov	sp, x29
   2d6c4:	ldp	x28, x19, [sp, #16]
   2d6c8:	ldp	x29, x30, [sp], #32
   2d6cc:	ret
   2d6d0:	mov	w9, #0x18                  	// #24
   2d6d4:	mul	x9, x2, x9
   2d6d8:	add	x9, x9, #0x20f
   2d6dc:	and	x9, x9, #0xfffffffffffffff0
   2d6e0:	mov	x10, sp
   2d6e4:	sub	x3, x10, x9
   2d6e8:	mov	sp, x3
   2d6ec:	mov	x1, x8
   2d6f0:	bl	c220 <__gmpn_toom4_sqr@plt>
   2d6f4:	mov	sp, x29
   2d6f8:	ldp	x28, x19, [sp, #16]
   2d6fc:	ldp	x29, x30, [sp], #32
   2d700:	ret
   2d704:	mov	x9, sp
   2d708:	sub	x9, x9, x2, lsl #4
   2d70c:	sub	x3, x9, #0xc00
   2d710:	mov	sp, x3
   2d714:	mov	x1, x8
   2d718:	bl	d470 <__gmpn_toom6_sqr@plt>
   2d71c:	mov	sp, x29
   2d720:	ldp	x28, x19, [sp, #16]
   2d724:	ldp	x29, x30, [sp], #32
   2d728:	ret
   2d72c:	lsl	x9, x2, #4
   2d730:	sub	x9, x9, x2
   2d734:	add	x9, x9, #0xd50
   2d738:	and	x1, x9, #0xfffffffffffffff8
   2d73c:	mov	w9, #0x7f00                	// #32512
   2d740:	cmp	x1, x9
   2d744:	str	xzr, [x19, #32]
   2d748:	b.hi	2d784 <__gmpn_sqr@@Base+0x17c>  // b.pmore
   2d74c:	add	x10, x1, #0xf
   2d750:	mov	x9, sp
   2d754:	and	x10, x10, #0xfffffffffffffff0
   2d758:	sub	x3, x9, x10
   2d75c:	mov	sp, x3
   2d760:	mov	x1, x8
   2d764:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   2d768:	ldr	x0, [x19, #32]
   2d76c:	cbz	x0, 2d68c <__gmpn_sqr@@Base+0x84>
   2d770:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2d774:	mov	sp, x29
   2d778:	ldp	x28, x19, [sp, #16]
   2d77c:	ldp	x29, x30, [sp], #32
   2d780:	ret
   2d784:	stp	x2, x0, [x19, #16]
   2d788:	add	x0, x19, #0x20
   2d78c:	str	x8, [x19, #8]
   2d790:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2d794:	ldp	x8, x2, [x19, #8]
   2d798:	mov	x3, x0
   2d79c:	ldr	x0, [x19, #24]
   2d7a0:	mov	x1, x8
   2d7a4:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   2d7a8:	ldr	x0, [x19, #32]
   2d7ac:	cbz	x0, 2d68c <__gmpn_sqr@@Base+0x84>
   2d7b0:	b	2d770 <__gmpn_sqr@@Base+0x168>

000000000002d7b4 <__gmpn_mul_basecase@@Base>:
   2d7b4:	stp	x29, x30, [sp, #-64]!
   2d7b8:	stp	x22, x21, [sp, #32]
   2d7bc:	stp	x20, x19, [sp, #48]
   2d7c0:	str	x23, [sp, #16]
   2d7c4:	mov	x23, x3
   2d7c8:	ldr	x3, [x3]
   2d7cc:	mov	x29, sp
   2d7d0:	mov	x22, x4
   2d7d4:	mov	x19, x2
   2d7d8:	mov	x20, x1
   2d7dc:	mov	x21, x0
   2d7e0:	bl	d490 <__gmpn_mul_1@plt>
   2d7e4:	cmp	x22, #0x2
   2d7e8:	str	x0, [x21, x19, lsl #3]
   2d7ec:	b.lt	2d824 <__gmpn_mul_basecase@@Base+0x70>  // b.tstop
   2d7f0:	add	x21, x21, #0x8
   2d7f4:	add	x23, x23, #0x8
   2d7f8:	add	x22, x22, #0x1
   2d7fc:	ldr	x3, [x23], #8
   2d800:	mov	x0, x21
   2d804:	mov	x1, x20
   2d808:	mov	x2, x19
   2d80c:	bl	d400 <__gmpn_addmul_1@plt>
   2d810:	sub	x22, x22, #0x1
   2d814:	str	x0, [x21, x19, lsl #3]
   2d818:	cmp	x22, #0x2
   2d81c:	add	x21, x21, #0x8
   2d820:	b.gt	2d7fc <__gmpn_mul_basecase@@Base+0x48>
   2d824:	ldp	x20, x19, [sp, #48]
   2d828:	ldp	x22, x21, [sp, #32]
   2d82c:	ldr	x23, [sp, #16]
   2d830:	ldp	x29, x30, [sp], #64
   2d834:	ret

000000000002d838 <__gmpn_sqr_basecase@@Base>:
   2d838:	stp	x29, x30, [sp, #-64]!
   2d83c:	stp	x24, x23, [sp, #16]
   2d840:	stp	x20, x19, [sp, #48]
   2d844:	mov	x19, x2
   2d848:	mov	x20, x1
   2d84c:	subs	x2, x2, #0x1
   2d850:	mov	x23, x0
   2d854:	stp	x22, x21, [sp, #32]
   2d858:	mov	x29, sp
   2d85c:	b.ne	2d884 <__gmpn_sqr_basecase@@Base+0x4c>  // b.any
   2d860:	ldr	x8, [x20]
   2d864:	umulh	x9, x8, x8
   2d868:	mul	x8, x8, x8
   2d86c:	stp	x8, x9, [x23]
   2d870:	ldp	x20, x19, [sp, #48]
   2d874:	ldp	x22, x21, [sp, #32]
   2d878:	ldp	x24, x23, [sp, #16]
   2d87c:	ldp	x29, x30, [sp], #64
   2d880:	ret
   2d884:	mov	x1, x20
   2d888:	ldr	x3, [x1], #8
   2d88c:	add	x22, x23, #0x8
   2d890:	mov	x0, x22
   2d894:	bl	d490 <__gmpn_mul_1@plt>
   2d898:	subs	x21, x19, #0x2
   2d89c:	str	x0, [x23, x19, lsl #3]
   2d8a0:	b.eq	2d8e8 <__gmpn_sqr_basecase@@Base+0xb0>  // b.none
   2d8a4:	add	x8, x23, x19, lsl #3
   2d8a8:	mov	x24, xzr
   2d8ac:	add	x22, x23, #0x18
   2d8b0:	add	x23, x8, #0x8
   2d8b4:	add	x8, x20, x24
   2d8b8:	ldr	x3, [x8, #8]
   2d8bc:	add	x1, x8, #0x10
   2d8c0:	mov	x0, x22
   2d8c4:	mov	x2, x21
   2d8c8:	bl	d400 <__gmpn_addmul_1@plt>
   2d8cc:	str	x0, [x23, x24]
   2d8d0:	subs	x21, x21, #0x1
   2d8d4:	add	x22, x22, #0x10
   2d8d8:	add	x24, x24, #0x8
   2d8dc:	b.ne	2d8b4 <__gmpn_sqr_basecase@@Base+0x7c>  // b.any
   2d8e0:	sub	x22, x22, #0x10
   2d8e4:	add	x20, x20, x24
   2d8e8:	sub	x8, x22, x19, lsl #4
   2d8ec:	sub	x9, x20, x19, lsl #3
   2d8f0:	mov	x3, x19
   2d8f4:	ldp	x20, x19, [sp, #48]
   2d8f8:	ldp	x22, x21, [sp, #32]
   2d8fc:	ldp	x24, x23, [sp, #16]
   2d900:	add	x0, x8, #0x18
   2d904:	add	x1, x8, #0x20
   2d908:	add	x2, x9, #0x10
   2d90c:	ldp	x29, x30, [sp], #64
   2d910:	b	d330 <__gmpn_sqr_diag_addlsh1@plt>

000000000002d914 <__gmpn_nussbaumer_mul@@Base>:
   2d914:	stp	x29, x30, [sp, #-64]!
   2d918:	stp	x24, x23, [sp, #16]
   2d91c:	stp	x22, x21, [sp, #32]
   2d920:	stp	x20, x19, [sp, #48]
   2d924:	mov	x29, sp
   2d928:	sub	sp, sp, #0x10
   2d92c:	mov	x22, x4
   2d930:	mov	x23, x3
   2d934:	mov	x19, x2
   2d938:	mov	x20, x1
   2d93c:	mov	x21, x0
   2d940:	cmp	x1, x3
   2d944:	stur	xzr, [x29, #-8]
   2d948:	b.ne	2d9ac <__gmpn_nussbaumer_mul@@Base+0x98>  // b.any
   2d94c:	cmp	x19, x22
   2d950:	b.ne	2d9ac <__gmpn_nussbaumer_mul@@Base+0x98>  // b.any
   2d954:	lsl	x0, x19, #1
   2d958:	bl	c5d0 <__gmpn_sqrmod_bnm1_next_size@plt>
   2d95c:	cmp	x19, x0, asr #1
   2d960:	csel	x8, x19, xzr, gt
   2d964:	add	x8, x0, x8
   2d968:	lsl	x8, x8, #3
   2d96c:	add	x1, x8, #0x18
   2d970:	mov	w8, #0x7f00                	// #32512
   2d974:	mov	x22, x0
   2d978:	cmp	x1, x8
   2d97c:	b.hi	2da3c <__gmpn_nussbaumer_mul@@Base+0x128>  // b.pmore
   2d980:	add	x9, x1, #0xf
   2d984:	mov	x8, sp
   2d988:	and	x9, x9, #0xfffffffffffffff0
   2d98c:	sub	x4, x8, x9
   2d990:	mov	sp, x4
   2d994:	mov	x0, x21
   2d998:	mov	x1, x22
   2d99c:	mov	x2, x20
   2d9a0:	mov	x3, x19
   2d9a4:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   2d9a8:	b	2da14 <__gmpn_nussbaumer_mul@@Base+0x100>
   2d9ac:	add	x0, x22, x19
   2d9b0:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   2d9b4:	asr	x8, x0, #1
   2d9b8:	cmp	x8, x22
   2d9bc:	csel	x9, x0, x8, lt  // lt = tstop
   2d9c0:	cmp	x8, x19
   2d9c4:	csel	x8, x9, xzr, lt  // lt = tstop
   2d9c8:	add	x8, x0, x8
   2d9cc:	lsl	x8, x8, #3
   2d9d0:	add	x1, x8, #0x20
   2d9d4:	mov	w8, #0x7f00                	// #32512
   2d9d8:	mov	x24, x0
   2d9dc:	cmp	x1, x8
   2d9e0:	b.hi	2da4c <__gmpn_nussbaumer_mul@@Base+0x138>  // b.pmore
   2d9e4:	add	x9, x1, #0xf
   2d9e8:	mov	x8, sp
   2d9ec:	and	x9, x9, #0xfffffffffffffff0
   2d9f0:	sub	x6, x8, x9
   2d9f4:	mov	sp, x6
   2d9f8:	mov	x0, x21
   2d9fc:	mov	x1, x24
   2da00:	mov	x2, x20
   2da04:	mov	x3, x19
   2da08:	mov	x4, x23
   2da0c:	mov	x5, x22
   2da10:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   2da14:	ldur	x0, [x29, #-8]
   2da18:	cbnz	x0, 2da34 <__gmpn_nussbaumer_mul@@Base+0x120>
   2da1c:	mov	sp, x29
   2da20:	ldp	x20, x19, [sp, #48]
   2da24:	ldp	x22, x21, [sp, #32]
   2da28:	ldp	x24, x23, [sp, #16]
   2da2c:	ldp	x29, x30, [sp], #64
   2da30:	ret
   2da34:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2da38:	b	2da1c <__gmpn_nussbaumer_mul@@Base+0x108>
   2da3c:	sub	x0, x29, #0x8
   2da40:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2da44:	mov	x4, x0
   2da48:	b	2d994 <__gmpn_nussbaumer_mul@@Base+0x80>
   2da4c:	sub	x0, x29, #0x8
   2da50:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2da54:	mov	x6, x0
   2da58:	b	2d9f8 <__gmpn_nussbaumer_mul@@Base+0xe4>

000000000002da5c <__gmpn_mulmid_basecase@@Base>:
   2da5c:	stp	x29, x30, [sp, #-80]!
   2da60:	stp	x26, x25, [sp, #16]
   2da64:	stp	x24, x23, [sp, #32]
   2da68:	stp	x22, x21, [sp, #48]
   2da6c:	stp	x20, x19, [sp, #64]
   2da70:	mov	x23, x3
   2da74:	ldr	x3, [x3]
   2da78:	sub	x26, x4, #0x1
   2da7c:	sub	x20, x2, x26
   2da80:	mov	x24, x1
   2da84:	add	x1, x1, x26, lsl #3
   2da88:	mov	x2, x20
   2da8c:	mov	x29, sp
   2da90:	mov	x22, x4
   2da94:	mov	x19, x0
   2da98:	bl	d490 <__gmpn_mul_1@plt>
   2da9c:	mov	x21, x0
   2daa0:	mov	x25, xzr
   2daa4:	cbz	x26, 2dae8 <__gmpn_mulmid_basecase@@Base+0x8c>
   2daa8:	add	x8, x24, x22, lsl #3
   2daac:	sub	x22, x8, #0x10
   2dab0:	add	x23, x23, #0x8
   2dab4:	ldr	x3, [x23], #8
   2dab8:	mov	x0, x19
   2dabc:	mov	x1, x22
   2dac0:	mov	x2, x20
   2dac4:	bl	d400 <__gmpn_addmul_1@plt>
   2dac8:	mov	x9, xzr
   2dacc:	adds	x8, x21, x0
   2dad0:	adc	x25, x25, x9
   2dad4:	sub	x26, x26, #0x1
   2dad8:	sub	x22, x22, #0x8
   2dadc:	mov	x21, x8
   2dae0:	cbnz	x26, 2dab4 <__gmpn_mulmid_basecase@@Base+0x58>
   2dae4:	mov	x21, x8
   2dae8:	add	x8, x19, x20, lsl #3
   2daec:	stp	x21, x25, [x8]
   2daf0:	ldp	x20, x19, [sp, #64]
   2daf4:	ldp	x22, x21, [sp, #48]
   2daf8:	ldp	x24, x23, [sp, #32]
   2dafc:	ldp	x26, x25, [sp, #16]
   2db00:	ldp	x29, x30, [sp], #80
   2db04:	ret

000000000002db08 <__gmpn_toom42_mulmid@@Base>:
   2db08:	sub	sp, sp, #0x110
   2db0c:	cmp	x3, #0x0
   2db10:	stp	x22, x21, [sp, #240]
   2db14:	cinc	x22, x3, lt  // lt = tstop
   2db18:	stp	x24, x23, [sp, #224]
   2db1c:	and	x8, x3, #0x1
   2db20:	asr	x24, x22, #1
   2db24:	stp	x20, x19, [sp, #256]
   2db28:	add	x23, x1, x8, lsl #3
   2db2c:	lsl	x20, x24, #3
   2db30:	stp	x28, x27, [sp, #192]
   2db34:	stp	x26, x25, [sp, #208]
   2db38:	add	x28, x4, #0x10
   2db3c:	add	x26, x23, x20
   2db40:	add	x25, x2, x20
   2db44:	mov	x19, x2
   2db48:	mov	x21, x0
   2db4c:	str	x4, [sp, #48]
   2db50:	str	x3, [sp, #64]
   2db54:	sub	x5, x24, #0x1
   2db58:	add	x3, sp, #0x48
   2db5c:	mov	x0, x28
   2db60:	mov	x1, x23
   2db64:	mov	x2, x26
   2db68:	mov	x4, x25
   2db6c:	mov	x6, xzr
   2db70:	stp	x29, x30, [sp, #176]
   2db74:	add	x29, sp, #0xb0
   2db78:	str	x8, [sp, #8]
   2db7c:	bl	ccf0 <__gmpn_add_err1_n@plt>
   2db80:	add	x8, x28, x20
   2db84:	lsl	x27, x24, #4
   2db88:	mov	x7, x0
   2db8c:	str	x8, [sp, #16]
   2db90:	sub	x0, x8, #0x8
   2db94:	add	x8, x23, x27
   2db98:	stp	x26, x20, [sp, #24]
   2db9c:	sub	x20, x26, #0x8
   2dba0:	sub	x26, x8, #0x8
   2dba4:	add	x8, sp, #0x48
   2dba8:	add	x3, x8, #0x10
   2dbac:	mov	x1, x20
   2dbb0:	mov	x2, x26
   2dbb4:	mov	x4, x25
   2dbb8:	mov	x5, x19
   2dbbc:	mov	x6, x24
   2dbc0:	and	x22, x22, #0xfffffffffffffffe
   2dbc4:	bl	c630 <__gmpn_add_err2_n@plt>
   2dbc8:	add	x8, x28, x27
   2dbcc:	str	x22, [sp, #40]
   2dbd0:	add	x22, x22, x24
   2dbd4:	mov	x6, x0
   2dbd8:	sub	x0, x8, #0x8
   2dbdc:	add	x8, x23, x22, lsl #3
   2dbe0:	sub	x2, x8, #0x8
   2dbe4:	add	x8, sp, #0x48
   2dbe8:	add	x3, x8, #0x30
   2dbec:	mov	x1, x26
   2dbf0:	mov	x4, x19
   2dbf4:	mov	x5, x24
   2dbf8:	str	x28, [sp, #56]
   2dbfc:	str	x23, [sp]
   2dc00:	bl	ccf0 <__gmpn_add_err1_n@plt>
   2dc04:	sub	x8, x19, #0x8
   2dc08:	mov	x9, x24
   2dc0c:	subs	x10, x9, #0x1
   2dc10:	b.lt	2dc30 <__gmpn_toom42_mulmid@@Base+0x128>  // b.tstop
   2dc14:	ldr	x11, [x8, x27]
   2dc18:	ldr	x9, [x8, x9, lsl #3]
   2dc1c:	sub	x27, x27, #0x8
   2dc20:	cmp	x11, x9
   2dc24:	mov	x9, x10
   2dc28:	b.eq	2dc0c <__gmpn_toom42_mulmid@@Base+0x104>  // b.none
   2dc2c:	b.ls	2dcf0 <__gmpn_toom42_mulmid@@Base+0x1e8>  // b.plast
   2dc30:	add	x9, x21, x24, lsl #3
   2dc34:	add	x8, sp, #0x48
   2dc38:	add	x27, x9, #0x10
   2dc3c:	add	x3, x8, #0x40
   2dc40:	mov	x0, x27
   2dc44:	mov	x1, x25
   2dc48:	mov	x2, x19
   2dc4c:	mov	x4, x20
   2dc50:	mov	x5, x26
   2dc54:	mov	x6, x24
   2dc58:	mov	x7, xzr
   2dc5c:	mov	x23, x9
   2dc60:	bl	d370 <__gmpn_sub_err2_n@plt>
   2dc64:	mov	w26, wzr
   2dc68:	ldr	x8, [sp, #64]
   2dc6c:	cmp	x8, #0x27
   2dc70:	ldr	x8, [sp, #40]
   2dc74:	b.gt	2dd38 <__gmpn_toom42_mulmid@@Base+0x230>
   2dc78:	ldr	x1, [sp, #56]
   2dc7c:	sub	x20, x8, #0x1
   2dc80:	mov	x0, x21
   2dc84:	mov	x2, x20
   2dc88:	mov	x3, x25
   2dc8c:	mov	x4, x24
   2dc90:	mov	x28, x8
   2dc94:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2dc98:	add	x8, x21, x24, lsl #3
   2dc9c:	ldp	x9, x8, [x8]
   2dca0:	ldp	x10, x11, [sp, #88]
   2dca4:	ldr	x22, [sp, #48]
   2dca8:	ldr	x1, [sp, #24]
   2dcac:	mov	x2, x20
   2dcb0:	adds	x9, x9, x10
   2dcb4:	cinc	x8, x8, cs  // cs = hs, nlast
   2dcb8:	add	x8, x8, x11
   2dcbc:	mov	x0, x22
   2dcc0:	mov	x3, x27
   2dcc4:	mov	x4, x24
   2dcc8:	stp	x9, x8, [sp, #88]
   2dccc:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2dcd0:	ldr	x1, [sp, #16]
   2dcd4:	mov	x0, x23
   2dcd8:	mov	x2, x20
   2dcdc:	mov	x3, x19
   2dce0:	mov	x4, x24
   2dce4:	mov	x25, x23
   2dce8:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2dcec:	b	2ddb0 <__gmpn_toom42_mulmid@@Base+0x2a8>
   2dcf0:	add	x9, x21, x24, lsl #3
   2dcf4:	add	x8, sp, #0x48
   2dcf8:	add	x27, x9, #0x10
   2dcfc:	add	x3, x8, #0x40
   2dd00:	mov	x0, x27
   2dd04:	mov	x1, x19
   2dd08:	mov	x2, x25
   2dd0c:	mov	x4, x20
   2dd10:	mov	x5, x26
   2dd14:	mov	x6, x24
   2dd18:	mov	x7, xzr
   2dd1c:	mov	x23, x9
   2dd20:	bl	d370 <__gmpn_sub_err2_n@plt>
   2dd24:	mov	w26, #0x1                   	// #1
   2dd28:	ldr	x8, [sp, #64]
   2dd2c:	cmp	x8, #0x27
   2dd30:	ldr	x8, [sp, #40]
   2dd34:	b.le	2dc78 <__gmpn_toom42_mulmid@@Base+0x170>
   2dd38:	ldp	x9, x1, [sp, #48]
   2dd3c:	mov	x28, x8
   2dd40:	mov	x0, x21
   2dd44:	mov	x2, x25
   2dd48:	add	x8, x9, x22, lsl #3
   2dd4c:	add	x20, x8, #0x8
   2dd50:	mov	x3, x24
   2dd54:	mov	x4, x20
   2dd58:	mov	x22, x9
   2dd5c:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2dd60:	add	x8, x21, x24, lsl #3
   2dd64:	ldp	x9, x8, [x8]
   2dd68:	ldp	x10, x11, [sp, #88]
   2dd6c:	ldr	x1, [sp, #24]
   2dd70:	mov	x0, x22
   2dd74:	mov	x2, x27
   2dd78:	adds	x9, x9, x10
   2dd7c:	cinc	x8, x8, cs  // cs = hs, nlast
   2dd80:	add	x8, x8, x11
   2dd84:	mov	x3, x24
   2dd88:	mov	x4, x20
   2dd8c:	stp	x9, x8, [sp, #88]
   2dd90:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2dd94:	ldr	x1, [sp, #16]
   2dd98:	mov	x0, x23
   2dd9c:	mov	x2, x19
   2dda0:	mov	x3, x24
   2dda4:	mov	x4, x20
   2dda8:	mov	x25, x23
   2ddac:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2ddb0:	ldr	x8, [sp, #72]
   2ddb4:	ldp	x9, x10, [x21]
   2ddb8:	ldr	x14, [sp, #32]
   2ddbc:	subs	x8, x9, x8
   2ddc0:	str	x8, [x21]
   2ddc4:	ldr	x8, [sp, #80]
   2ddc8:	cinc	x8, x8, cc  // cc = lo, ul, last
   2ddcc:	subs	x8, x10, x8
   2ddd0:	str	x8, [x21, #8]
   2ddd4:	b.cc	2e05c <__gmpn_toom42_mulmid@@Base+0x554>  // b.lo, b.ul, b.last
   2ddd8:	ldp	x8, x9, [sp, #88]
   2dddc:	ldp	x10, x12, [sp, #104]
   2dde0:	ldr	x11, [x21, x14]
   2dde4:	subs	x8, x8, x10
   2dde8:	cset	w10, cc  // cc = lo, ul, last
   2ddec:	adds	x11, x11, x8
   2ddf0:	add	x8, x24, #0x1
   2ddf4:	lsl	x8, x8, #3
   2ddf8:	str	x11, [x21, x14]
   2ddfc:	ldr	x11, [x21, x8]
   2de00:	sub	x9, x9, x12
   2de04:	sub	x9, x9, x10
   2de08:	cinc	x9, x9, cs  // cs = hs, nlast
   2de0c:	adds	x10, x9, x11
   2de10:	asr	x9, x9, #63
   2de14:	cinc	x9, x9, cs  // cs = hs, nlast
   2de18:	str	x10, [x21, x8]
   2de1c:	cbnz	x9, 2e0b0 <__gmpn_toom42_mulmid@@Base+0x5a8>
   2de20:	lsl	x9, x28, #3
   2de24:	ldr	x10, [sp, #120]
   2de28:	ldr	x11, [x21, x9]
   2de2c:	adds	x10, x10, x11
   2de30:	orr	x11, x9, #0x8
   2de34:	str	x10, [x21, x9]
   2de38:	ldr	x9, [x21, x11]
   2de3c:	ldr	x10, [sp, #128]
   2de40:	add	x9, x10, x9
   2de44:	cinc	x9, x9, cs  // cs = hs, nlast
   2de48:	str	x9, [x21, x11]
   2de4c:	ldr	x9, [sp, #136]
   2de50:	ldp	x10, x11, [x22]
   2de54:	adds	x9, x9, x10
   2de58:	str	x9, [x22]
   2de5c:	ldr	x9, [sp, #144]
   2de60:	cinc	x10, x11, cs  // cs = hs, nlast
   2de64:	add	x9, x10, x9
   2de68:	cmp	x9, x11
   2de6c:	str	x9, [x22, #8]
   2de70:	b.cc	2e0f0 <__gmpn_toom42_mulmid@@Base+0x5e8>  // b.lo, b.ul, b.last
   2de74:	ldr	x9, [x22, x14]
   2de78:	ldr	x10, [sp, #152]
   2de7c:	subs	x9, x9, x10
   2de80:	str	x9, [x22, x14]
   2de84:	ldr	x9, [x22, x8]
   2de88:	ldr	x10, [sp, #160]
   2de8c:	cset	w11, cc  // cc = lo, ul, last
   2de90:	sub	x9, x9, x10
   2de94:	sub	x9, x9, x11
   2de98:	str	x9, [x22, x8]
   2de9c:	ldr	x8, [x27]
   2dea0:	lsr	x9, x9, #63
   2dea4:	cbz	w26, 2dfc4 <__gmpn_toom42_mulmid@@Base+0x4bc>
   2dea8:	subs	x8, x8, x9
   2deac:	str	x8, [x27]
   2deb0:	b.cs	2ded8 <__gmpn_toom42_mulmid@@Base+0x3d0>  // b.hs, b.nlast
   2deb4:	mov	w8, #0x1                   	// #1
   2deb8:	cmp	x8, x24
   2debc:	b.ge	2ded8 <__gmpn_toom42_mulmid@@Base+0x3d0>  // b.tcont
   2dec0:	lsl	x9, x8, #3
   2dec4:	ldr	x10, [x27, x9]
   2dec8:	add	x8, x8, #0x1
   2decc:	sub	x11, x10, #0x1
   2ded0:	str	x11, [x27, x9]
   2ded4:	cbz	x10, 2deb8 <__gmpn_toom42_mulmid@@Base+0x3b0>
   2ded8:	adds	x20, x24, #0x2
   2dedc:	b.eq	2df20 <__gmpn_toom42_mulmid@@Base+0x418>  // b.none
   2dee0:	mov	x0, x21
   2dee4:	mov	x1, x21
   2dee8:	mov	x2, x22
   2deec:	mov	x3, x20
   2def0:	bl	ca70 <__gmpn_add_n@plt>
   2def4:	cbz	x0, 2df20 <__gmpn_toom42_mulmid@@Base+0x418>
   2def8:	add	x8, x28, #0x2
   2defc:	mov	x9, x20
   2df00:	cmp	x9, x8
   2df04:	b.ge	2df20 <__gmpn_toom42_mulmid@@Base+0x418>  // b.tcont
   2df08:	lsl	x10, x9, #3
   2df0c:	ldr	x11, [x21, x10]
   2df10:	add	x9, x9, #0x1
   2df14:	adds	x11, x11, #0x1
   2df18:	str	x11, [x21, x10]
   2df1c:	b.cs	2df00 <__gmpn_toom42_mulmid@@Base+0x3f8>  // b.hs, b.nlast
   2df20:	mov	x0, x25
   2df24:	mov	x1, x25
   2df28:	mov	x2, x22
   2df2c:	mov	x3, x20
   2df30:	bl	c2d0 <__gmpn_sub_n@plt>
   2df34:	ldr	x8, [sp, #8]
   2df38:	cbz	x8, 2dfa4 <__gmpn_toom42_mulmid@@Base+0x49c>
   2df3c:	ldr	x23, [sp, #64]
   2df40:	ldr	x22, [sp]
   2df44:	mov	x0, x21
   2df48:	sub	x20, x23, #0x1
   2df4c:	ldr	x3, [x19, x20, lsl #3]
   2df50:	sub	x1, x22, #0x8
   2df54:	mov	x2, x23
   2df58:	bl	d400 <__gmpn_addmul_1@plt>
   2df5c:	lsl	x8, x23, #3
   2df60:	mov	x3, x19
   2df64:	add	x19, x21, x8
   2df68:	ldr	x9, [x19]
   2df6c:	add	x8, x22, x8
   2df70:	sub	x1, x8, #0x8
   2df74:	mov	x2, x20
   2df78:	adds	x9, x9, x0
   2df7c:	cset	w10, cs  // cs = hs, nlast
   2df80:	add	x0, sp, #0x48
   2df84:	mov	x4, x20
   2df88:	stp	x9, x10, [x19]
   2df8c:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2df90:	sub	x0, x19, #0x8
   2df94:	add	x2, sp, #0x48
   2df98:	mov	w3, #0x3                   	// #3
   2df9c:	mov	x1, x0
   2dfa0:	bl	ca70 <__gmpn_add_n@plt>
   2dfa4:	ldp	x20, x19, [sp, #256]
   2dfa8:	ldp	x22, x21, [sp, #240]
   2dfac:	ldp	x24, x23, [sp, #224]
   2dfb0:	ldp	x26, x25, [sp, #208]
   2dfb4:	ldp	x28, x27, [sp, #192]
   2dfb8:	ldp	x29, x30, [sp, #176]
   2dfbc:	add	sp, sp, #0x110
   2dfc0:	ret
   2dfc4:	adds	x8, x9, x8
   2dfc8:	str	x8, [x27]
   2dfcc:	b.cc	2dff4 <__gmpn_toom42_mulmid@@Base+0x4ec>  // b.lo, b.ul, b.last
   2dfd0:	mov	w8, #0x1                   	// #1
   2dfd4:	cmp	x8, x24
   2dfd8:	b.ge	2dff4 <__gmpn_toom42_mulmid@@Base+0x4ec>  // b.tcont
   2dfdc:	lsl	x9, x8, #3
   2dfe0:	ldr	x10, [x27, x9]
   2dfe4:	add	x8, x8, #0x1
   2dfe8:	adds	x10, x10, #0x1
   2dfec:	str	x10, [x27, x9]
   2dff0:	b.cs	2dfd4 <__gmpn_toom42_mulmid@@Base+0x4cc>  // b.hs, b.nlast
   2dff4:	adds	x20, x24, #0x2
   2dff8:	b.eq	2e03c <__gmpn_toom42_mulmid@@Base+0x534>  // b.none
   2dffc:	mov	x0, x21
   2e000:	mov	x1, x21
   2e004:	mov	x2, x22
   2e008:	mov	x3, x20
   2e00c:	bl	c2d0 <__gmpn_sub_n@plt>
   2e010:	cbz	x0, 2e03c <__gmpn_toom42_mulmid@@Base+0x534>
   2e014:	add	x8, x28, #0x2
   2e018:	mov	x9, x20
   2e01c:	cmp	x9, x8
   2e020:	b.ge	2e03c <__gmpn_toom42_mulmid@@Base+0x534>  // b.tcont
   2e024:	lsl	x10, x9, #3
   2e028:	ldr	x11, [x21, x10]
   2e02c:	add	x9, x9, #0x1
   2e030:	sub	x12, x11, #0x1
   2e034:	str	x12, [x21, x10]
   2e038:	cbz	x11, 2e01c <__gmpn_toom42_mulmid@@Base+0x514>
   2e03c:	mov	x0, x25
   2e040:	mov	x1, x25
   2e044:	mov	x2, x22
   2e048:	mov	x3, x20
   2e04c:	bl	ca70 <__gmpn_add_n@plt>
   2e050:	ldr	x8, [sp, #8]
   2e054:	cbnz	x8, 2df3c <__gmpn_toom42_mulmid@@Base+0x434>
   2e058:	b	2dfa4 <__gmpn_toom42_mulmid@@Base+0x49c>
   2e05c:	ldr	x8, [sp, #64]
   2e060:	cmp	x8, #0x6
   2e064:	b.lt	2e130 <__gmpn_toom42_mulmid@@Base+0x628>  // b.tstop
   2e068:	ldr	x8, [x21, #16]
   2e06c:	sub	x9, x8, #0x1
   2e070:	str	x9, [x21, #16]
   2e074:	cbnz	x8, 2e0a8 <__gmpn_toom42_mulmid@@Base+0x5a0>
   2e078:	sub	x9, x24, #0x2
   2e07c:	mov	w10, #0x3                   	// #3
   2e080:	mov	w8, #0x1                   	// #1
   2e084:	sub	x11, x10, #0x2
   2e088:	cmp	x11, x9
   2e08c:	b.ge	2e134 <__gmpn_toom42_mulmid@@Base+0x62c>  // b.tcont
   2e090:	lsl	x11, x10, #3
   2e094:	ldr	x12, [x21, x11]
   2e098:	add	x10, x10, #0x1
   2e09c:	sub	x13, x12, #0x1
   2e0a0:	str	x13, [x21, x11]
   2e0a4:	cbz	x12, 2e084 <__gmpn_toom42_mulmid@@Base+0x57c>
   2e0a8:	mov	x8, xzr
   2e0ac:	b	2e134 <__gmpn_toom42_mulmid@@Base+0x62c>
   2e0b0:	cmp	x9, #0x1
   2e0b4:	b.ne	2e14c <__gmpn_toom42_mulmid@@Base+0x644>  // b.any
   2e0b8:	ldr	x9, [x27]
   2e0bc:	adds	x9, x9, #0x1
   2e0c0:	str	x9, [x27]
   2e0c4:	b.cc	2de20 <__gmpn_toom42_mulmid@@Base+0x318>  // b.lo, b.ul, b.last
   2e0c8:	mov	w9, #0x1                   	// #1
   2e0cc:	cmp	x9, x24
   2e0d0:	b.ge	2de20 <__gmpn_toom42_mulmid@@Base+0x318>  // b.tcont
   2e0d4:	lsl	x10, x9, #3
   2e0d8:	ldr	x11, [x27, x10]
   2e0dc:	add	x9, x9, #0x1
   2e0e0:	adds	x11, x11, #0x1
   2e0e4:	str	x11, [x27, x10]
   2e0e8:	b.cs	2e0cc <__gmpn_toom42_mulmid@@Base+0x5c4>  // b.hs, b.nlast
   2e0ec:	b	2de20 <__gmpn_toom42_mulmid@@Base+0x318>
   2e0f0:	ldr	x10, [sp, #56]
   2e0f4:	ldr	x9, [x10]
   2e0f8:	adds	x9, x9, #0x1
   2e0fc:	str	x9, [x10]
   2e100:	b.cc	2de74 <__gmpn_toom42_mulmid@@Base+0x36c>  // b.lo, b.ul, b.last
   2e104:	mov	w9, #0x3                   	// #3
   2e108:	sub	x10, x9, #0x2
   2e10c:	cmp	x10, x24
   2e110:	b.ge	2de74 <__gmpn_toom42_mulmid@@Base+0x36c>  // b.tcont
   2e114:	lsl	x10, x9, #3
   2e118:	ldr	x11, [x22, x10]
   2e11c:	add	x9, x9, #0x1
   2e120:	adds	x11, x11, #0x1
   2e124:	str	x11, [x22, x10]
   2e128:	b.cs	2e108 <__gmpn_toom42_mulmid@@Base+0x600>  // b.hs, b.nlast
   2e12c:	b	2de74 <__gmpn_toom42_mulmid@@Base+0x36c>
   2e130:	mov	w8, #0x1                   	// #1
   2e134:	ldp	x9, x10, [sp, #88]
   2e138:	subs	x8, x9, x8
   2e13c:	cset	w9, cc  // cc = lo, ul, last
   2e140:	sub	x9, x10, x9
   2e144:	stp	x8, x9, [sp, #88]
   2e148:	b	2dddc <__gmpn_toom42_mulmid@@Base+0x2d4>
   2e14c:	ldr	x9, [x27]
   2e150:	sub	x10, x9, #0x1
   2e154:	str	x10, [x27]
   2e158:	cbnz	x9, 2de20 <__gmpn_toom42_mulmid@@Base+0x318>
   2e15c:	mov	w9, #0x1                   	// #1
   2e160:	cmp	x9, x24
   2e164:	b.ge	2de20 <__gmpn_toom42_mulmid@@Base+0x318>  // b.tcont
   2e168:	lsl	x10, x9, #3
   2e16c:	ldr	x11, [x27, x10]
   2e170:	add	x9, x9, #0x1
   2e174:	sub	x12, x11, #0x1
   2e178:	str	x12, [x27, x10]
   2e17c:	cbz	x11, 2e160 <__gmpn_toom42_mulmid@@Base+0x658>
   2e180:	b	2de20 <__gmpn_toom42_mulmid@@Base+0x318>

000000000002e184 <__gmpn_mulmid_n@@Base>:
   2e184:	stp	x29, x30, [sp, #-48]!
   2e188:	stp	x22, x21, [sp, #16]
   2e18c:	stp	x20, x19, [sp, #32]
   2e190:	mov	x29, sp
   2e194:	sub	sp, sp, #0x10
   2e198:	mov	x19, x3
   2e19c:	mov	x20, x2
   2e1a0:	mov	x21, x1
   2e1a4:	cmp	x3, #0x13
   2e1a8:	mov	x22, x0
   2e1ac:	b.gt	2e1dc <__gmpn_mulmid_n@@Base+0x58>
   2e1b0:	lsl	x8, x19, #1
   2e1b4:	sub	x2, x8, #0x1
   2e1b8:	mov	x0, x22
   2e1bc:	mov	x1, x21
   2e1c0:	mov	x3, x20
   2e1c4:	mov	x4, x19
   2e1c8:	mov	sp, x29
   2e1cc:	ldp	x20, x19, [sp, #32]
   2e1d0:	ldp	x22, x21, [sp, #16]
   2e1d4:	ldp	x29, x30, [sp], #48
   2e1d8:	b	c460 <__gmpn_mulmid_basecase@plt>
   2e1dc:	mov	w8, #0x18                  	// #24
   2e1e0:	orr	x9, xzr, #0x200
   2e1e4:	madd	x1, x19, x8, x9
   2e1e8:	mov	w8, #0x7f00                	// #32512
   2e1ec:	cmp	x1, x8
   2e1f0:	stur	xzr, [x29, #-8]
   2e1f4:	b.hi	2e23c <__gmpn_mulmid_n@@Base+0xb8>  // b.pmore
   2e1f8:	add	x9, x1, #0xf
   2e1fc:	mov	x8, sp
   2e200:	and	x9, x9, #0xfffffffffffffff0
   2e204:	sub	x4, x8, x9
   2e208:	mov	sp, x4
   2e20c:	mov	x0, x22
   2e210:	mov	x1, x21
   2e214:	mov	x2, x20
   2e218:	mov	x3, x19
   2e21c:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2e220:	ldur	x0, [x29, #-8]
   2e224:	cbnz	x0, 2e24c <__gmpn_mulmid_n@@Base+0xc8>
   2e228:	mov	sp, x29
   2e22c:	ldp	x20, x19, [sp, #32]
   2e230:	ldp	x22, x21, [sp, #16]
   2e234:	ldp	x29, x30, [sp], #48
   2e238:	ret
   2e23c:	sub	x0, x29, #0x8
   2e240:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2e244:	mov	x4, x0
   2e248:	b	2e20c <__gmpn_mulmid_n@@Base+0x88>
   2e24c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2e250:	b	2e228 <__gmpn_mulmid_n@@Base+0xa4>

000000000002e254 <__gmpn_mulmid@@Base>:
   2e254:	stp	x29, x30, [sp, #-96]!
   2e258:	stp	x28, x27, [sp, #16]
   2e25c:	stp	x26, x25, [sp, #32]
   2e260:	stp	x24, x23, [sp, #48]
   2e264:	stp	x22, x21, [sp, #64]
   2e268:	stp	x20, x19, [sp, #80]
   2e26c:	mov	x29, sp
   2e270:	sub	sp, sp, #0x30
   2e274:	mov	x19, x4
   2e278:	mov	x22, x3
   2e27c:	mov	x24, x2
   2e280:	mov	x20, x1
   2e284:	cmp	x4, #0x13
   2e288:	mov	x21, x0
   2e28c:	b.gt	2e2cc <__gmpn_mulmid@@Base+0x78>
   2e290:	cmp	x24, #0xdb
   2e294:	b.gt	2e2fc <__gmpn_mulmid@@Base+0xa8>
   2e298:	mov	x0, x21
   2e29c:	mov	x1, x20
   2e2a0:	mov	x2, x24
   2e2a4:	mov	x3, x22
   2e2a8:	mov	x4, x19
   2e2ac:	mov	sp, x29
   2e2b0:	ldp	x20, x19, [sp, #80]
   2e2b4:	ldp	x22, x21, [sp, #64]
   2e2b8:	ldp	x24, x23, [sp, #48]
   2e2bc:	ldp	x26, x25, [sp, #32]
   2e2c0:	ldp	x28, x27, [sp, #16]
   2e2c4:	ldp	x29, x30, [sp], #96
   2e2c8:	b	c460 <__gmpn_mulmid_basecase@plt>
   2e2cc:	sub	x28, x24, x19
   2e2d0:	cmp	x28, #0x13
   2e2d4:	b.ge	2e3b0 <__gmpn_mulmid@@Base+0x15c>  // b.tcont
   2e2d8:	cmp	x19, #0xdb
   2e2dc:	b.gt	2e4f0 <__gmpn_mulmid@@Base+0x29c>
   2e2e0:	mov	x0, x21
   2e2e4:	mov	x1, x20
   2e2e8:	mov	x2, x24
   2e2ec:	mov	x3, x22
   2e2f0:	mov	x4, x19
   2e2f4:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e2f8:	b	2e6f0 <__gmpn_mulmid@@Base+0x49c>
   2e2fc:	mov	w8, #0xdd                  	// #221
   2e300:	mov	w2, #0xdc                  	// #220
   2e304:	mov	x0, x21
   2e308:	mov	x1, x20
   2e30c:	mov	x3, x22
   2e310:	mov	x4, x19
   2e314:	sub	x25, x8, x19
   2e318:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e31c:	sub	x23, x24, x25
   2e320:	cmp	x23, #0xdc
   2e324:	b.lt	2e5cc <__gmpn_mulmid@@Base+0x378>  // b.tstop
   2e328:	lsl	x8, x19, #3
   2e32c:	mov	w9, #0x6e8                 	// #1768
   2e330:	mov	w10, #0x6f8                 	// #1784
   2e334:	sub	x26, x9, x8
   2e338:	sub	x8, x10, x8
   2e33c:	stur	x8, [x29, #-16]
   2e340:	b	2e354 <__gmpn_mulmid@@Base+0x100>
   2e344:	sub	x23, x23, x25
   2e348:	cmp	x23, #0xdb
   2e34c:	mov	x21, x24
   2e350:	b.le	2e5d0 <__gmpn_mulmid@@Base+0x37c>
   2e354:	add	x24, x21, x26
   2e358:	ldp	x28, x27, [x24]
   2e35c:	add	x20, x20, x25, lsl #3
   2e360:	mov	w2, #0xdc                  	// #220
   2e364:	mov	x0, x24
   2e368:	mov	x1, x20
   2e36c:	mov	x3, x22
   2e370:	mov	x4, x19
   2e374:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e378:	ldp	x8, x9, [x24]
   2e37c:	adds	x8, x8, x28
   2e380:	str	x8, [x24]
   2e384:	cinc	x8, x27, cs  // cs = hs, nlast
   2e388:	adds	x8, x9, x8
   2e38c:	str	x8, [x24, #8]
   2e390:	b.cc	2e344 <__gmpn_mulmid@@Base+0xf0>  // b.lo, b.ul, b.last
   2e394:	ldur	x8, [x29, #-16]
   2e398:	ldr	x9, [x21, x8]
   2e39c:	adds	x9, x9, #0x1
   2e3a0:	str	x9, [x21, x8]
   2e3a4:	add	x8, x8, #0x8
   2e3a8:	b.cs	2e398 <__gmpn_mulmid@@Base+0x144>  // b.hs, b.nlast
   2e3ac:	b	2e344 <__gmpn_mulmid@@Base+0xf0>
   2e3b0:	add	x23, x28, #0x1
   2e3b4:	subs	x26, x23, x19
   2e3b8:	b.ge	2e638 <__gmpn_mulmid@@Base+0x3e4>  // b.tcont
   2e3bc:	add	x8, x23, x23, lsl #1
   2e3c0:	add	x8, x28, x8
   2e3c4:	lsl	x8, x8, #3
   2e3c8:	add	x1, x8, #0x218
   2e3cc:	mov	w8, #0x7f00                	// #32512
   2e3d0:	cmp	x1, x8
   2e3d4:	add	x8, x28, #0x3
   2e3d8:	stp	x8, xzr, [x29, #-16]
   2e3dc:	b.hi	2e794 <__gmpn_mulmid@@Base+0x540>  // b.pmore
   2e3e0:	add	x9, x1, #0xf
   2e3e4:	mov	x8, sp
   2e3e8:	and	x9, x9, #0xfffffffffffffff0
   2e3ec:	sub	x26, x8, x9
   2e3f0:	mov	sp, x26
   2e3f4:	add	x8, x26, x23, lsl #3
   2e3f8:	sub	x27, x19, x23
   2e3fc:	add	x4, x8, #0x10
   2e400:	add	x2, x22, x27, lsl #3
   2e404:	mov	x0, x21
   2e408:	mov	x1, x20
   2e40c:	mov	x3, x23
   2e410:	mov	x25, x2
   2e414:	stur	x4, [x29, #-40]
   2e418:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2e41c:	cmp	x27, x28
   2e420:	b.le	2e4bc <__gmpn_mulmid@@Base+0x268>
   2e424:	lsl	x11, x24, #3
   2e428:	lsl	x8, x19, #3
   2e42c:	sub	x10, x11, x8
   2e430:	mov	w9, #0x18                  	// #24
   2e434:	add	x10, x10, #0x8
   2e438:	ldur	x25, [x29, #-40]
   2e43c:	sub	x8, x8, x11
   2e440:	mul	x9, x19, x9
   2e444:	stp	x10, x28, [x29, #-32]
   2e448:	mov	x10, x24
   2e44c:	sub	x24, x8, #0x8
   2e450:	sub	x8, x9, x10, lsl #4
   2e454:	sub	x28, x8, #0x10
   2e458:	stur	x11, [x29, #-48]
   2e45c:	ldur	x8, [x29, #-32]
   2e460:	add	x2, x22, x28
   2e464:	mov	x0, x26
   2e468:	mov	x3, x23
   2e46c:	add	x20, x20, x8
   2e470:	mov	x1, x20
   2e474:	mov	x4, x25
   2e478:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2e47c:	ldur	x3, [x29, #-16]
   2e480:	mov	x0, x21
   2e484:	mov	x1, x21
   2e488:	mov	x2, x26
   2e48c:	bl	ca70 <__gmpn_add_n@plt>
   2e490:	ldur	x8, [x29, #-24]
   2e494:	sub	x27, x27, x23
   2e498:	add	x22, x22, x24
   2e49c:	cmp	x27, x8
   2e4a0:	b.gt	2e45c <__gmpn_mulmid@@Base+0x208>
   2e4a4:	ldur	x9, [x29, #-48]
   2e4a8:	lsl	x8, x19, #4
   2e4ac:	ldur	x28, [x29, #-24]
   2e4b0:	sub	x8, x8, x9
   2e4b4:	add	x8, x8, x22
   2e4b8:	sub	x25, x8, #0x8
   2e4bc:	ldur	x19, [x29, #-16]
   2e4c0:	cbz	x27, 2e5bc <__gmpn_mulmid@@Base+0x368>
   2e4c4:	add	x1, x20, x23, lsl #3
   2e4c8:	sub	x3, x25, x27, lsl #3
   2e4cc:	add	x2, x27, x28
   2e4d0:	mov	x0, x26
   2e4d4:	mov	x4, x27
   2e4d8:	bl	c700 <__gmpn_mulmid@plt>
   2e4dc:	mov	x0, x21
   2e4e0:	mov	x1, x21
   2e4e4:	mov	x2, x26
   2e4e8:	mov	x3, x19
   2e4ec:	b	2e5b8 <__gmpn_mulmid@@Base+0x364>
   2e4f0:	add	x23, x28, #0x3
   2e4f4:	lsl	x1, x23, #3
   2e4f8:	mov	w8, #0x7f00                	// #32512
   2e4fc:	cmp	x1, x8
   2e500:	stur	xzr, [x29, #-8]
   2e504:	b.hi	2e7a4 <__gmpn_mulmid@@Base+0x550>  // b.pmore
   2e508:	add	x9, x1, #0xf
   2e50c:	mov	x8, sp
   2e510:	and	x9, x9, #0xfffffffffffffff0
   2e514:	sub	x25, x8, x9
   2e518:	mov	sp, x25
   2e51c:	sub	x26, x19, #0xdc
   2e520:	add	x22, x22, x26, lsl #3
   2e524:	sub	x24, x24, x26
   2e528:	mov	w4, #0xdc                  	// #220
   2e52c:	mov	x0, x21
   2e530:	mov	x1, x20
   2e534:	mov	x2, x24
   2e538:	mov	x3, x22
   2e53c:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e540:	cmp	x19, #0x1b8
   2e544:	b.lt	2e58c <__gmpn_mulmid@@Base+0x338>  // b.tstop
   2e548:	add	x20, x20, #0x6e0
   2e54c:	sub	x22, x22, #0x6e0
   2e550:	mov	w4, #0xdc                  	// #220
   2e554:	mov	x0, x25
   2e558:	mov	x1, x20
   2e55c:	mov	x2, x24
   2e560:	mov	x3, x22
   2e564:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e568:	mov	x0, x21
   2e56c:	mov	x1, x21
   2e570:	mov	x2, x25
   2e574:	mov	x3, x23
   2e578:	bl	ca70 <__gmpn_add_n@plt>
   2e57c:	sub	x19, x19, #0xdc
   2e580:	cmp	x19, #0x1b7
   2e584:	b.gt	2e548 <__gmpn_mulmid@@Base+0x2f4>
   2e588:	sub	x26, x19, #0xdc
   2e58c:	cbz	x26, 2e5bc <__gmpn_mulmid@@Base+0x368>
   2e590:	add	x1, x20, #0x6e0
   2e594:	sub	x3, x22, x26, lsl #3
   2e598:	add	x2, x26, x28
   2e59c:	mov	x0, x25
   2e5a0:	mov	x4, x26
   2e5a4:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e5a8:	mov	x0, x21
   2e5ac:	mov	x1, x21
   2e5b0:	mov	x2, x25
   2e5b4:	mov	x3, x23
   2e5b8:	bl	ca70 <__gmpn_add_n@plt>
   2e5bc:	ldur	x0, [x29, #-8]
   2e5c0:	cbz	x0, 2e6f0 <__gmpn_mulmid@@Base+0x49c>
   2e5c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2e5c8:	b	2e6f0 <__gmpn_mulmid@@Base+0x49c>
   2e5cc:	mov	x24, x21
   2e5d0:	cmp	x23, x19
   2e5d4:	b.lt	2e6f0 <__gmpn_mulmid@@Base+0x49c>  // b.tstop
   2e5d8:	lsl	x8, x25, #3
   2e5dc:	add	x21, x24, x8
   2e5e0:	ldp	x25, x26, [x21]
   2e5e4:	add	x1, x20, x8
   2e5e8:	mov	x0, x21
   2e5ec:	mov	x2, x23
   2e5f0:	mov	x3, x22
   2e5f4:	mov	x4, x19
   2e5f8:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2e5fc:	ldp	x8, x9, [x21]
   2e600:	adds	x8, x8, x25
   2e604:	str	x8, [x21]
   2e608:	cinc	x8, x26, cs  // cs = hs, nlast
   2e60c:	adds	x8, x9, x8
   2e610:	str	x8, [x21, #8]
   2e614:	b.cc	2e6f0 <__gmpn_mulmid@@Base+0x49c>  // b.lo, b.ul, b.last
   2e618:	mov	w8, #0xdf                  	// #223
   2e61c:	sub	x8, x8, x19
   2e620:	add	x8, x24, x8, lsl #3
   2e624:	ldr	x9, [x8]
   2e628:	adds	x9, x9, #0x1
   2e62c:	str	x9, [x8], #8
   2e630:	b.cs	2e624 <__gmpn_mulmid@@Base+0x3d0>  // b.hs, b.nlast
   2e634:	b	2e6f0 <__gmpn_mulmid@@Base+0x49c>
   2e638:	mov	w8, #0x18                  	// #24
   2e63c:	orr	x9, xzr, #0x200
   2e640:	madd	x1, x19, x8, x9
   2e644:	mov	w8, #0x7f00                	// #32512
   2e648:	cmp	x1, x8
   2e64c:	stur	xzr, [x29, #-8]
   2e650:	b.hi	2e7b4 <__gmpn_mulmid@@Base+0x560>  // b.pmore
   2e654:	add	x9, x1, #0xf
   2e658:	mov	x8, sp
   2e65c:	and	x9, x9, #0xfffffffffffffff0
   2e660:	sub	x24, x8, x9
   2e664:	mov	sp, x24
   2e668:	mov	x0, x21
   2e66c:	mov	x1, x20
   2e670:	mov	x2, x22
   2e674:	mov	x3, x19
   2e678:	mov	x4, x24
   2e67c:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2e680:	cmp	x26, x19
   2e684:	lsl	x27, x19, #3
   2e688:	b.ge	2e710 <__gmpn_mulmid@@Base+0x4bc>  // b.tcont
   2e68c:	mov	x24, x21
   2e690:	ldur	x0, [x29, #-8]
   2e694:	cbnz	x0, 2e7c4 <__gmpn_mulmid@@Base+0x570>
   2e698:	cbz	x26, 2e6f0 <__gmpn_mulmid@@Base+0x49c>
   2e69c:	add	x21, x24, x27
   2e6a0:	ldp	x25, x26, [x21]
   2e6a4:	add	x1, x20, x27
   2e6a8:	sub	x2, x23, #0x1
   2e6ac:	mov	x0, x21
   2e6b0:	mov	x3, x22
   2e6b4:	mov	x4, x19
   2e6b8:	bl	c700 <__gmpn_mulmid@plt>
   2e6bc:	ldp	x8, x9, [x21]
   2e6c0:	adds	x8, x8, x25
   2e6c4:	str	x8, [x21]
   2e6c8:	cinc	x8, x26, cs  // cs = hs, nlast
   2e6cc:	adds	x8, x9, x8
   2e6d0:	str	x8, [x21, #8]
   2e6d4:	b.cc	2e6f0 <__gmpn_mulmid@@Base+0x49c>  // b.lo, b.ul, b.last
   2e6d8:	add	x8, x24, x19, lsl #3
   2e6dc:	add	x8, x8, #0x10
   2e6e0:	ldr	x9, [x8]
   2e6e4:	adds	x9, x9, #0x1
   2e6e8:	str	x9, [x8], #8
   2e6ec:	b.cs	2e6e0 <__gmpn_mulmid@@Base+0x48c>  // b.hs, b.nlast
   2e6f0:	mov	sp, x29
   2e6f4:	ldp	x20, x19, [sp, #80]
   2e6f8:	ldp	x22, x21, [sp, #64]
   2e6fc:	ldp	x24, x23, [sp, #48]
   2e700:	ldp	x26, x25, [sp, #32]
   2e704:	ldp	x28, x27, [sp, #16]
   2e708:	ldp	x29, x30, [sp], #96
   2e70c:	ret
   2e710:	add	x8, x27, #0x10
   2e714:	stp	x8, x24, [x29, #-24]
   2e718:	b	2e72c <__gmpn_mulmid@@Base+0x4d8>
   2e71c:	sub	x26, x23, x19
   2e720:	cmp	x26, x19
   2e724:	mov	x21, x24
   2e728:	b.lt	2e690 <__gmpn_mulmid@@Base+0x43c>  // b.tstop
   2e72c:	add	x24, x21, x27
   2e730:	ldur	x4, [x29, #-16]
   2e734:	ldr	x28, [x21, x27]
   2e738:	ldr	x25, [x24, #8]
   2e73c:	add	x20, x20, x27
   2e740:	mov	x0, x24
   2e744:	mov	x1, x20
   2e748:	mov	x2, x22
   2e74c:	mov	x3, x19
   2e750:	mov	x23, x26
   2e754:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2e758:	ldr	x8, [x21, x27]
   2e75c:	adds	x8, x8, x28
   2e760:	str	x8, [x21, x27]
   2e764:	ldr	x8, [x24, #8]
   2e768:	cinc	x9, x25, cs  // cs = hs, nlast
   2e76c:	adds	x8, x8, x9
   2e770:	str	x8, [x24, #8]
   2e774:	b.cc	2e71c <__gmpn_mulmid@@Base+0x4c8>  // b.lo, b.ul, b.last
   2e778:	ldur	x8, [x29, #-24]
   2e77c:	ldr	x9, [x21, x8]
   2e780:	adds	x9, x9, #0x1
   2e784:	str	x9, [x21, x8]
   2e788:	add	x8, x8, #0x8
   2e78c:	b.cs	2e77c <__gmpn_mulmid@@Base+0x528>  // b.hs, b.nlast
   2e790:	b	2e71c <__gmpn_mulmid@@Base+0x4c8>
   2e794:	sub	x0, x29, #0x8
   2e798:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2e79c:	mov	x26, x0
   2e7a0:	b	2e3f4 <__gmpn_mulmid@@Base+0x1a0>
   2e7a4:	sub	x0, x29, #0x8
   2e7a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2e7ac:	mov	x25, x0
   2e7b0:	b	2e51c <__gmpn_mulmid@@Base+0x2c8>
   2e7b4:	sub	x0, x29, #0x8
   2e7b8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2e7bc:	mov	x24, x0
   2e7c0:	b	2e668 <__gmpn_mulmid@@Base+0x414>
   2e7c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2e7c8:	cbnz	x26, 2e69c <__gmpn_mulmid@@Base+0x448>
   2e7cc:	b	2e6f0 <__gmpn_mulmid@@Base+0x49c>

000000000002e7d0 <__gmpn_random@@Base>:
   2e7d0:	stp	x29, x30, [sp, #-48]!
   2e7d4:	str	x21, [sp, #16]
   2e7d8:	stp	x20, x19, [sp, #32]
   2e7dc:	mov	x29, sp
   2e7e0:	cbz	x1, 2e85c <__gmpn_random@@Base+0x8c>
   2e7e4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e7e8:	ldr	x8, [x8, #4040]
   2e7ec:	mov	x20, x1
   2e7f0:	mov	x21, x0
   2e7f4:	ldrb	w9, [x8]
   2e7f8:	cbnz	w9, 2e810 <__gmpn_random@@Base+0x40>
   2e7fc:	mov	w9, #0x1                   	// #1
   2e800:	strb	w9, [x8]
   2e804:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e808:	ldr	x0, [x0, #3976]
   2e80c:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   2e810:	adrp	x19, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e814:	ldr	x19, [x19, #3976]
   2e818:	lsl	x2, x20, #6
   2e81c:	mov	x1, x21
   2e820:	ldr	x8, [x19, #24]
   2e824:	mov	x0, x19
   2e828:	ldr	x8, [x8, #8]
   2e82c:	blr	x8
   2e830:	add	x20, x21, x20, lsl #3
   2e834:	ldr	x8, [x20, #-8]!
   2e838:	cbnz	x8, 2e85c <__gmpn_random@@Base+0x8c>
   2e83c:	ldr	x8, [x19, #24]
   2e840:	mov	w2, #0x40                  	// #64
   2e844:	mov	x0, x19
   2e848:	mov	x1, x20
   2e84c:	ldr	x8, [x8, #8]
   2e850:	blr	x8
   2e854:	ldr	x8, [x20]
   2e858:	cbz	x8, 2e83c <__gmpn_random@@Base+0x6c>
   2e85c:	ldp	x20, x19, [sp, #32]
   2e860:	ldr	x21, [sp, #16]
   2e864:	ldp	x29, x30, [sp], #48
   2e868:	ret

000000000002e86c <__gmpn_random2@@Base>:
   2e86c:	sub	sp, sp, #0x60
   2e870:	stp	x29, x30, [sp, #16]
   2e874:	str	x25, [sp, #32]
   2e878:	stp	x24, x23, [sp, #48]
   2e87c:	stp	x22, x21, [sp, #64]
   2e880:	stp	x20, x19, [sp, #80]
   2e884:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e888:	ldr	x8, [x8, #4040]
   2e88c:	mov	x21, x1
   2e890:	mov	x19, x0
   2e894:	add	x29, sp, #0x10
   2e898:	ldrb	w9, [x8]
   2e89c:	cbnz	w9, 2e8b4 <__gmpn_random2@@Base+0x48>
   2e8a0:	mov	w9, #0x1                   	// #1
   2e8a4:	strb	w9, [x8]
   2e8a8:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e8ac:	ldr	x0, [x0, #3976]
   2e8b0:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   2e8b4:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   2e8b8:	ldr	x20, [x20, #3976]
   2e8bc:	add	x1, sp, #0x8
   2e8c0:	mov	w2, #0x20                  	// #32
   2e8c4:	ldr	x8, [x20, #24]
   2e8c8:	mov	x0, x20
   2e8cc:	ldr	x8, [x8, #8]
   2e8d0:	blr	x8
   2e8d4:	ldr	x8, [sp, #8]
   2e8d8:	lsl	x9, x21, #6
   2e8dc:	mov	x10, #0xffffffffffffffff    	// #-1
   2e8e0:	and	x8, x8, #0x3f
   2e8e4:	sub	x21, x9, x8
   2e8e8:	add	x11, x21, #0x3f
   2e8ec:	neg	w8, w21
   2e8f0:	lsr	x9, x11, #6
   2e8f4:	lsr	x10, x10, x8
   2e8f8:	sub	x8, x9, #0x1
   2e8fc:	cmp	x11, #0x80
   2e900:	str	x10, [x19, x8, lsl #3]
   2e904:	b.cc	2e92c <__gmpn_random2@@Base+0xc0>  // b.lo, b.ul, b.last
   2e908:	cmp	x9, #0x2
   2e90c:	mov	w10, #0x2                   	// #2
   2e910:	csel	x9, x9, x10, cc  // cc = lo, ul, last
   2e914:	sub	x9, x9, #0x2
   2e918:	sub	x8, x8, x9
   2e91c:	add	x0, x19, x9, lsl #3
   2e920:	lsl	x2, x8, #3
   2e924:	mov	w1, #0xff                  	// #255
   2e928:	bl	c5f0 <memset@plt>
   2e92c:	ldr	x8, [x20, #24]
   2e930:	add	x1, x29, #0x18
   2e934:	mov	w2, #0x20                  	// #32
   2e938:	mov	x0, x20
   2e93c:	ldr	x8, [x8, #8]
   2e940:	blr	x8
   2e944:	ldr	x8, [x29, #24]
   2e948:	add	x22, x19, #0x8
   2e94c:	mov	w24, #0x1                   	// #1
   2e950:	and	x8, x8, #0x3
   2e954:	add	x8, x8, #0x1
   2e958:	udiv	x8, x21, x8
   2e95c:	cmp	w8, #0x0
   2e960:	cinc	w23, w8, eq  // eq = none
   2e964:	b	2e970 <__gmpn_random2@@Base+0x104>
   2e968:	cmp	x25, x8
   2e96c:	b.ls	2ea20 <__gmpn_random2@@Base+0x1b4>  // b.plast
   2e970:	ldr	x8, [x20, #24]
   2e974:	add	x1, x29, #0x18
   2e978:	mov	w2, #0x20                  	// #32
   2e97c:	mov	x0, x20
   2e980:	ldr	x8, [x8, #8]
   2e984:	blr	x8
   2e988:	ldr	x8, [x29, #24]
   2e98c:	udiv	x9, x8, x23
   2e990:	msub	x8, x9, x23, x8
   2e994:	add	x8, x8, #0x1
   2e998:	subs	x8, x21, x8
   2e99c:	csel	x25, xzr, x8, cc  // cc = lo, ul, last
   2e9a0:	b.ls	2ea20 <__gmpn_random2@@Base+0x1b4>  // b.plast
   2e9a4:	lsr	x8, x25, #3
   2e9a8:	and	x8, x8, #0x1ffffffffffffff8
   2e9ac:	ldr	x9, [x19, x8]
   2e9b0:	lsl	x10, x24, x25
   2e9b4:	add	x1, x29, #0x18
   2e9b8:	mov	w2, #0x20                  	// #32
   2e9bc:	eor	x9, x9, x10
   2e9c0:	str	x9, [x19, x8]
   2e9c4:	ldr	x8, [x20, #24]
   2e9c8:	mov	x0, x20
   2e9cc:	ldr	x8, [x8, #8]
   2e9d0:	blr	x8
   2e9d4:	ldr	x8, [x29, #24]
   2e9d8:	udiv	x9, x8, x23
   2e9dc:	msub	x8, x9, x23, x8
   2e9e0:	add	x8, x8, #0x1
   2e9e4:	subs	x9, x25, x8
   2e9e8:	csel	x21, xzr, x9, cc  // cc = lo, ul, last
   2e9ec:	lsr	x9, x21, #6
   2e9f0:	lsl	x10, x9, #3
   2e9f4:	ldr	x11, [x19, x10]
   2e9f8:	lsl	x12, x24, x21
   2e9fc:	adds	x11, x11, x12
   2ea00:	str	x11, [x19, x10]
   2ea04:	b.cc	2e968 <__gmpn_random2@@Base+0xfc>  // b.lo, b.ul, b.last
   2ea08:	add	x9, x22, x9, lsl #3
   2ea0c:	ldr	x10, [x9]
   2ea10:	adds	x10, x10, #0x1
   2ea14:	str	x10, [x9], #8
   2ea18:	b.cs	2ea0c <__gmpn_random2@@Base+0x1a0>  // b.hs, b.nlast
   2ea1c:	b	2e968 <__gmpn_random2@@Base+0xfc>
   2ea20:	ldp	x20, x19, [sp, #80]
   2ea24:	ldp	x22, x21, [sp, #64]
   2ea28:	ldp	x24, x23, [sp, #48]
   2ea2c:	ldr	x25, [sp, #32]
   2ea30:	ldp	x29, x30, [sp, #16]
   2ea34:	add	sp, sp, #0x60
   2ea38:	ret

000000000002ea3c <__gmpn_pow_1@@Base>:
   2ea3c:	stp	x29, x30, [sp, #-80]!
   2ea40:	stp	x20, x19, [sp, #64]
   2ea44:	mov	x19, x2
   2ea48:	mov	x20, x1
   2ea4c:	cmp	x3, #0x2
   2ea50:	stp	x26, x25, [sp, #16]
   2ea54:	stp	x24, x23, [sp, #32]
   2ea58:	stp	x22, x21, [sp, #48]
   2ea5c:	mov	x29, sp
   2ea60:	b.cs	2ea78 <__gmpn_pow_1@@Base+0x3c>  // b.hs, b.nlast
   2ea64:	cbz	x3, 2ec08 <__gmpn_pow_1@@Base+0x1cc>
   2ea68:	mov	x1, x20
   2ea6c:	mov	x2, x19
   2ea70:	bl	ca50 <__gmpn_copyi@plt>
   2ea74:	b	2ec10 <__gmpn_pow_1@@Base+0x1d4>
   2ea78:	mov	x9, xzr
   2ea7c:	mov	w8, #0x40                  	// #64
   2ea80:	mov	x10, x3
   2ea84:	sxtw	x9, w9
   2ea88:	eor	x9, x9, x10
   2ea8c:	lsr	x10, x10, #1
   2ea90:	sub	w8, w8, #0x1
   2ea94:	cbnz	x10, 2ea84 <__gmpn_pow_1@@Base+0x48>
   2ea98:	lsl	x25, x3, x8
   2ea9c:	cmp	x19, #0x1
   2eaa0:	sub	w26, w8, #0x3e
   2eaa4:	b.ne	2eb44 <__gmpn_pow_1@@Base+0x108>  // b.any
   2eaa8:	ldr	x20, [x20]
   2eaac:	tst	w8, #0x1
   2eab0:	csel	x21, x0, x4, eq  // eq = none
   2eab4:	umulh	x9, x20, x20
   2eab8:	mul	x10, x20, x20
   2eabc:	csel	x8, x4, x0, eq  // eq = none
   2eac0:	stp	x10, x9, [x21]
   2eac4:	cmp	x9, #0x0
   2eac8:	mov	w9, #0x1                   	// #1
   2eacc:	cinc	x19, x9, ne  // ne = any
   2ead0:	lsl	x25, x25, #1
   2ead4:	mov	x22, x8
   2ead8:	tbz	x25, #63, 2eafc <__gmpn_pow_1@@Base+0xc0>
   2eadc:	mov	x0, x21
   2eae0:	mov	x1, x21
   2eae4:	mov	x2, x19
   2eae8:	mov	x3, x20
   2eaec:	bl	d490 <__gmpn_mul_1@plt>
   2eaf0:	cmp	x0, #0x0
   2eaf4:	str	x0, [x21, x19, lsl #3]
   2eaf8:	cinc	x19, x19, ne  // ne = any
   2eafc:	cbz	w26, 2ec10 <__gmpn_pow_1@@Base+0x1d4>
   2eb00:	mov	x0, x22
   2eb04:	mov	x1, x21
   2eb08:	mov	x2, x19
   2eb0c:	bl	c8e0 <__gmpn_sqr@plt>
   2eb10:	add	x8, x22, x19, lsl #4
   2eb14:	ldur	x8, [x8, #-8]
   2eb18:	lsl	x9, x19, #1
   2eb1c:	add	w26, w26, #0x1
   2eb20:	cmp	x8, #0x0
   2eb24:	cset	w8, eq  // eq = none
   2eb28:	sub	x19, x9, x8
   2eb2c:	mov	x8, x21
   2eb30:	mov	x21, x22
   2eb34:	lsl	x25, x25, #1
   2eb38:	mov	x22, x8
   2eb3c:	tbz	x25, #63, 2eafc <__gmpn_pow_1@@Base+0xc0>
   2eb40:	b	2eadc <__gmpn_pow_1@@Base+0xa0>
   2eb44:	eor	w8, w8, w9
   2eb48:	tst	w8, #0x1
   2eb4c:	csel	x23, x4, x0, eq  // eq = none
   2eb50:	csel	x21, x0, x4, eq  // eq = none
   2eb54:	mov	x0, x23
   2eb58:	mov	x1, x20
   2eb5c:	mov	x2, x19
   2eb60:	bl	c8e0 <__gmpn_sqr@plt>
   2eb64:	add	x8, x23, x19, lsl #4
   2eb68:	ldur	x8, [x8, #-8]
   2eb6c:	lsl	x9, x19, #1
   2eb70:	cmp	x8, #0x0
   2eb74:	cset	w8, eq  // eq = none
   2eb78:	sub	x22, x9, x8
   2eb7c:	lsl	x25, x25, #1
   2eb80:	tbnz	x25, #63, 2eb94 <__gmpn_pow_1@@Base+0x158>
   2eb84:	mov	x24, x21
   2eb88:	mov	x21, x23
   2eb8c:	cbnz	w26, 2ebc4 <__gmpn_pow_1@@Base+0x188>
   2eb90:	b	2ec00 <__gmpn_pow_1@@Base+0x1c4>
   2eb94:	mov	x0, x21
   2eb98:	mov	x1, x23
   2eb9c:	mov	x2, x22
   2eba0:	mov	x3, x20
   2eba4:	mov	x4, x19
   2eba8:	add	x24, x22, x19
   2ebac:	bl	ccd0 <__gmpn_mul@plt>
   2ebb0:	cmp	x0, #0x0
   2ebb4:	cset	w8, eq  // eq = none
   2ebb8:	sub	x22, x24, x8
   2ebbc:	mov	x24, x23
   2ebc0:	cbz	w26, 2ec00 <__gmpn_pow_1@@Base+0x1c4>
   2ebc4:	mov	x0, x24
   2ebc8:	mov	x1, x21
   2ebcc:	mov	x2, x22
   2ebd0:	bl	c8e0 <__gmpn_sqr@plt>
   2ebd4:	add	x8, x24, x22, lsl #4
   2ebd8:	ldur	x8, [x8, #-8]
   2ebdc:	lsl	x9, x22, #1
   2ebe0:	add	w26, w26, #0x1
   2ebe4:	mov	x23, x24
   2ebe8:	cmp	x8, #0x0
   2ebec:	cset	w8, eq  // eq = none
   2ebf0:	sub	x22, x9, x8
   2ebf4:	lsl	x25, x25, #1
   2ebf8:	tbz	x25, #63, 2eb84 <__gmpn_pow_1@@Base+0x148>
   2ebfc:	b	2eb94 <__gmpn_pow_1@@Base+0x158>
   2ec00:	mov	x19, x22
   2ec04:	b	2ec10 <__gmpn_pow_1@@Base+0x1d4>
   2ec08:	mov	w19, #0x1                   	// #1
   2ec0c:	str	x19, [x0]
   2ec10:	mov	x0, x19
   2ec14:	ldp	x20, x19, [sp, #64]
   2ec18:	ldp	x22, x21, [sp, #48]
   2ec1c:	ldp	x24, x23, [sp, #32]
   2ec20:	ldp	x26, x25, [sp, #16]
   2ec24:	ldp	x29, x30, [sp], #80
   2ec28:	ret

000000000002ec2c <__gmpn_rootrem@@Base>:
   2ec2c:	stp	x29, x30, [sp, #-64]!
   2ec30:	stp	x24, x23, [sp, #16]
   2ec34:	stp	x22, x21, [sp, #32]
   2ec38:	stp	x20, x19, [sp, #48]
   2ec3c:	mov	x29, sp
   2ec40:	sub	sp, sp, #0x10
   2ec44:	cmp	x4, #0x2
   2ec48:	mov	x20, x0
   2ec4c:	b.eq	2ed5c <__gmpn_rootrem@@Base+0x130>  // b.none
   2ec50:	mov	x19, x4
   2ec54:	cbz	x1, 2ec60 <__gmpn_rootrem@@Base+0x34>
   2ec58:	mov	x0, x20
   2ec5c:	b	2ed3c <__gmpn_rootrem@@Base+0x110>
   2ec60:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   2ec64:	add	x8, x3, #0x2
   2ec68:	movk	x9, #0x5556
   2ec6c:	smulh	x8, x8, x9
   2ec70:	add	x8, x8, x8, lsr #63
   2ec74:	cmp	x8, x19
   2ec78:	b.ls	2ed34 <__gmpn_rootrem@@Base+0x108>  // b.plast
   2ec7c:	sub	x8, x3, #0x1
   2ec80:	add	x21, x19, x3
   2ec84:	udiv	x24, x8, x19
   2ec88:	add	x8, x21, x24
   2ec8c:	lsl	x8, x8, #3
   2ec90:	add	x1, x8, #0x10
   2ec94:	mov	w8, #0x7f00                	// #32512
   2ec98:	cmp	x1, x8
   2ec9c:	stur	xzr, [x29, #-8]
   2eca0:	b.hi	2ed78 <__gmpn_rootrem@@Base+0x14c>  // b.pmore
   2eca4:	add	x9, x1, #0xf
   2eca8:	mov	x8, sp
   2ecac:	and	x9, x9, #0xfffffffffffffff0
   2ecb0:	sub	x22, x8, x9
   2ecb4:	mov	sp, x22
   2ecb8:	lsl	x23, x19, #3
   2ecbc:	add	x0, x22, x23
   2ecc0:	mov	x1, x2
   2ecc4:	mov	x2, x3
   2ecc8:	bl	ca50 <__gmpn_copyi@plt>
   2eccc:	mov	x0, x22
   2ecd0:	mov	w1, wzr
   2ecd4:	mov	x2, x23
   2ecd8:	bl	c5f0 <memset@plt>
   2ecdc:	add	x23, x22, x21, lsl #3
   2ece0:	mov	w5, #0x1                   	// #1
   2ece4:	mov	x0, x23
   2ece8:	mov	x1, xzr
   2ecec:	mov	x2, x22
   2ecf0:	mov	x3, x21
   2ecf4:	mov	x4, x19
   2ecf8:	bl	2eda0 <__gmpn_rootrem@@Base+0x174>
   2ecfc:	mov	x19, x0
   2ed00:	add	x1, x23, #0x8
   2ed04:	add	x2, x24, #0x1
   2ed08:	mov	x0, x20
   2ed0c:	bl	ca50 <__gmpn_copyi@plt>
   2ed10:	ldur	x0, [x29, #-8]
   2ed14:	cbnz	x0, 2ed98 <__gmpn_rootrem@@Base+0x16c>
   2ed18:	mov	x0, x19
   2ed1c:	mov	sp, x29
   2ed20:	ldp	x20, x19, [sp, #48]
   2ed24:	ldp	x22, x21, [sp, #32]
   2ed28:	ldp	x24, x23, [sp, #16]
   2ed2c:	ldp	x29, x30, [sp], #64
   2ed30:	ret
   2ed34:	mov	x0, x20
   2ed38:	mov	x1, xzr
   2ed3c:	mov	x4, x19
   2ed40:	mov	w5, wzr
   2ed44:	mov	sp, x29
   2ed48:	ldp	x20, x19, [sp, #48]
   2ed4c:	ldp	x22, x21, [sp, #32]
   2ed50:	ldp	x24, x23, [sp, #16]
   2ed54:	ldp	x29, x30, [sp], #64
   2ed58:	b	2eda0 <__gmpn_rootrem@@Base+0x174>
   2ed5c:	mov	x0, x20
   2ed60:	mov	sp, x29
   2ed64:	ldp	x20, x19, [sp, #48]
   2ed68:	ldp	x22, x21, [sp, #32]
   2ed6c:	ldp	x24, x23, [sp, #16]
   2ed70:	ldp	x29, x30, [sp], #64
   2ed74:	b	d3b0 <__gmpn_sqrtrem@plt>
   2ed78:	sub	x0, x29, #0x8
   2ed7c:	mov	x22, x3
   2ed80:	mov	x23, x2
   2ed84:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ed88:	mov	x2, x23
   2ed8c:	mov	x3, x22
   2ed90:	mov	x22, x0
   2ed94:	b	2ecb8 <__gmpn_rootrem@@Base+0x8c>
   2ed98:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2ed9c:	b	2ed18 <__gmpn_rootrem@@Base+0xec>
   2eda0:	stp	x29, x30, [sp, #-96]!
   2eda4:	stp	x28, x27, [sp, #16]
   2eda8:	stp	x26, x25, [sp, #32]
   2edac:	stp	x24, x23, [sp, #48]
   2edb0:	stp	x22, x21, [sp, #64]
   2edb4:	stp	x20, x19, [sp, #80]
   2edb8:	mov	x29, sp
   2edbc:	sub	sp, sp, #0x2d0
   2edc0:	sub	x8, x3, #0x1
   2edc4:	ldr	x11, [x2, x8, lsl #3]
   2edc8:	lsl	x9, x3, #6
   2edcc:	mov	x24, x3
   2edd0:	mov	x27, x2
   2edd4:	clz	x10, x11
   2edd8:	add	w12, w10, #0x1
   2eddc:	sub	x9, x9, x12
   2ede0:	mov	x16, x1
   2ede4:	cmp	x9, x4
   2ede8:	mov	x19, sp
   2edec:	b.cs	2ee70 <__gmpn_rootrem@@Base+0x244>  // b.hs, b.nlast
   2edf0:	mov	w9, #0x1                   	// #1
   2edf4:	str	x9, [x0]
   2edf8:	ldr	x9, [x27]
   2edfc:	cbz	x16, 2f558 <__gmpn_rootrem@@Base+0x92c>
   2ee00:	sub	x10, x9, #0x1
   2ee04:	str	x10, [x16]
   2ee08:	cbz	x9, 2f620 <__gmpn_rootrem@@Base+0x9f4>
   2ee0c:	cmp	x27, x16
   2ee10:	b.eq	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.none
   2ee14:	cmp	x24, #0x2
   2ee18:	b.lt	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.tstop
   2ee1c:	cmp	x8, #0x4
   2ee20:	b.cc	2ee48 <__gmpn_rootrem@@Base+0x21c>  // b.lo, b.ul, b.last
   2ee24:	lsl	x9, x24, #3
   2ee28:	add	x10, x16, #0x8
   2ee2c:	add	x11, x27, x9
   2ee30:	cmp	x10, x11
   2ee34:	b.cs	2f704 <__gmpn_rootrem@@Base+0xad8>  // b.hs, b.nlast
   2ee38:	add	x9, x16, x9
   2ee3c:	add	x10, x27, #0x8
   2ee40:	cmp	x10, x9
   2ee44:	b.cs	2f704 <__gmpn_rootrem@@Base+0xad8>  // b.hs, b.nlast
   2ee48:	mov	w9, #0x1                   	// #1
   2ee4c:	lsl	x11, x9, #3
   2ee50:	sub	x10, x24, x9
   2ee54:	add	x9, x16, x11
   2ee58:	add	x11, x27, x11
   2ee5c:	ldr	x12, [x11], #8
   2ee60:	subs	x10, x10, #0x1
   2ee64:	str	x12, [x9], #8
   2ee68:	b.ne	2ee5c <__gmpn_rootrem@@Base+0x230>  // b.any
   2ee6c:	b	2f738 <__gmpn_rootrem@@Base+0xb0c>
   2ee70:	mov	x28, x4
   2ee74:	cmp	w12, #0x40
   2ee78:	b.ne	2ee88 <__gmpn_rootrem@@Base+0x25c>  // b.any
   2ee7c:	add	x8, x27, x24, lsl #3
   2ee80:	ldur	x8, [x8, #-16]
   2ee84:	b	2eeac <__gmpn_rootrem@@Base+0x280>
   2ee88:	cmp	x24, #0x1
   2ee8c:	cset	w13, ne  // ne = any
   2ee90:	sub	x8, x8, x13
   2ee94:	ldr	x8, [x27, x8, lsl #3]
   2ee98:	lsl	x11, x11, x12
   2ee9c:	mov	w12, #0x3f                  	// #63
   2eea0:	sub	w10, w12, w10
   2eea4:	lsr	x8, x8, x10
   2eea8:	orr	x8, x8, x11
   2eeac:	lsr	x8, x8, #56
   2eeb0:	lsr	x10, x9, #56
   2eeb4:	cbnz	x10, 2f884 <__gmpn_rootrem@@Base+0xc58>
   2eeb8:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2eebc:	add	x10, x10, #0x1b4
   2eec0:	ldrb	w8, [x10, x8]
   2eec4:	bfi	x8, x9, #8, #56
   2eec8:	udiv	x9, x8, x28
   2eecc:	lsr	x8, x9, #8
   2eed0:	and	x9, x9, #0xff
   2eed4:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2eed8:	add	x10, x10, #0x2b4
   2eedc:	ldrb	w9, [x10, x9]
   2eee0:	and	x20, x8, #0xffffffff
   2eee4:	sub	x10, x28, #0x1
   2eee8:	cmp	x20, #0x8
   2eeec:	orr	x8, x9, #0x100
   2eef0:	str	x8, [x0]
   2eef4:	str	x20, [x19, #192]
   2eef8:	stp	x10, x0, [x19, #152]
   2eefc:	str	w5, [x19, #20]
   2ef00:	b.ls	2ef78 <__gmpn_rootrem@@Base+0x34c>  // b.plast
   2ef04:	lsr	x9, x10, #2
   2ef08:	mov	w10, #0x43                  	// #67
   2ef0c:	add	x11, x19, #0xc0
   2ef10:	clz	x9, x9
   2ef14:	mov	x22, xzr
   2ef18:	sub	x9, x10, x9
   2ef1c:	add	x10, x11, #0x8
   2ef20:	mov	x25, x20
   2ef24:	add	x11, x25, x9
   2ef28:	sub	x12, x25, #0x1
   2ef2c:	cmp	x25, x9
   2ef30:	lsr	x11, x11, #1
   2ef34:	csel	x25, x11, x12, hi  // hi = pmore
   2ef38:	str	x25, [x10, x22, lsl #3]
   2ef3c:	cmp	x25, #0x8
   2ef40:	add	x22, x22, #0x1
   2ef44:	b.hi	2ef24 <__gmpn_rootrem@@Base+0x2f8>  // b.pmore
   2ef48:	mov	w9, #0x8                   	// #8
   2ef4c:	sub	x9, x9, x25
   2ef50:	lsr	x8, x8, x9
   2ef54:	cmp	w22, #0x40
   2ef58:	str	x8, [x0]
   2ef5c:	b.ls	2ef90 <__gmpn_rootrem@@Base+0x364>  // b.plast
   2ef60:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2ef64:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2ef68:	add	x0, x0, #0x198
   2ef6c:	add	x2, x2, #0x1a2
   2ef70:	mov	w1, #0x11b                 	// #283
   2ef74:	bl	c6c0 <__gmp_assert_fail@plt>
   2ef78:	mov	w9, #0x8                   	// #8
   2ef7c:	sub	x9, x9, x20
   2ef80:	lsr	x8, x8, x9
   2ef84:	mov	w22, wzr
   2ef88:	str	x8, [x0]
   2ef8c:	mov	x25, x20
   2ef90:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2ef94:	ldr	d0, [x8, #400]
   2ef98:	ucvtf	d1, x28
   2ef9c:	mov	x8, #0x3f90000000000000    	// #4580160821035794432
   2efa0:	add	x21, x24, #0x1
   2efa4:	fmul	d0, d1, d0
   2efa8:	fmov	d1, x8
   2efac:	fmul	d0, d0, d1
   2efb0:	fcvtzs	x8, d0
   2efb4:	add	x8, x24, x8
   2efb8:	add	x23, x8, #0x2
   2efbc:	add	x8, x21, x23, lsl #1
   2efc0:	lsl	x1, x8, #3
   2efc4:	mov	w8, #0x7f00                	// #32512
   2efc8:	cmp	x1, x8
   2efcc:	str	xzr, [x19, #184]
   2efd0:	b.hi	2f8a4 <__gmpn_rootrem@@Base+0xc78>  // b.pmore
   2efd4:	add	x9, x1, #0xf
   2efd8:	mov	x8, sp
   2efdc:	and	x9, x9, #0xfffffffffffffff0
   2efe0:	sub	x0, x8, x9
   2efe4:	mov	sp, x0
   2efe8:	add	x8, x0, x21, lsl #3
   2efec:	cmp	x16, #0x0
   2eff0:	str	x8, [x19, #168]
   2eff4:	add	x8, x8, x23, lsl #3
   2eff8:	csel	x21, x0, x16, eq  // eq = none
   2effc:	str	x27, [x19, #120]
   2f000:	str	x8, [x19, #176]
   2f004:	str	x16, [x19, #8]
   2f008:	str	x24, [x19, #72]
   2f00c:	str	x28, [x19, #48]
   2f010:	str	x0, [x19, #32]
   2f014:	cbz	w22, 2f560 <__gmpn_rootrem@@Base+0x934>
   2f018:	ldr	x23, [x19, #160]
   2f01c:	mul	x8, x20, x28
   2f020:	add	x9, x0, x24, lsl #4
   2f024:	mov	w10, w22
   2f028:	sub	x28, x21, #0x8
   2f02c:	str	x9, [x19, #40]
   2f030:	add	x9, x21, #0x8
   2f034:	sub	x26, x8, x25
   2f038:	mov	w24, #0x1                   	// #1
   2f03c:	str	x9, [x19, #24]
   2f040:	str	x21, [x19, #88]
   2f044:	b	2f074 <__gmpn_rootrem@@Base+0x448>
   2f048:	ldr	x8, [x19, #80]
   2f04c:	mov	x9, #0xffffffffffffffff    	// #-1
   2f050:	mvn	w8, w8
   2f054:	lsr	x8, x9, x8
   2f058:	str	x8, [x20, x27, lsl #3]
   2f05c:	ldr	x8, [x22]
   2f060:	ldp	x9, x10, [x19, #96]
   2f064:	ldr	x23, [x19, #160]
   2f068:	orr	x8, x8, x9
   2f06c:	str	x8, [x22]
   2f070:	cbz	w10, 2f568 <__gmpn_rootrem@@Base+0x93c>
   2f074:	ldr	x22, [x19, #152]
   2f078:	ldr	x8, [x19, #72]
   2f07c:	mov	x0, x21
   2f080:	str	x10, [x19, #128]
   2f084:	msub	x26, x25, x22, x26
   2f088:	lsr	x25, x26, #6
   2f08c:	sub	x20, x8, x25
   2f090:	ldr	x8, [x19, #120]
   2f094:	ands	x3, x26, #0x3f
   2f098:	mov	x2, x20
   2f09c:	add	x1, x8, x25, lsl #3
   2f0a0:	b.eq	2f0ac <__gmpn_rootrem@@Base+0x480>  // b.none
   2f0a4:	bl	c1a0 <__gmpn_rshift@plt>
   2f0a8:	b	2f0b0 <__gmpn_rootrem@@Base+0x484>
   2f0ac:	bl	ca50 <__gmpn_copyi@plt>
   2f0b0:	add	x8, x21, x20, lsl #3
   2f0b4:	ldur	x8, [x8, #-8]
   2f0b8:	cmp	x8, #0x0
   2f0bc:	cset	w8, eq  // eq = none
   2f0c0:	csetm	x9, eq  // eq = none
   2f0c4:	sub	x20, x20, x8
   2f0c8:	str	x9, [x19, #112]
   2f0cc:	lsl	x8, x9, #3
   2f0d0:	ldr	x9, [x19, #40]
   2f0d4:	sub	x8, x8, x25, lsl #3
   2f0d8:	add	x8, x9, x8
   2f0dc:	stp	x25, x8, [x19, #136]
   2f0e0:	ldp	x27, x21, [x19, #168]
   2f0e4:	mov	x1, x23
   2f0e8:	mov	x2, x24
   2f0ec:	mov	x3, x22
   2f0f0:	mov	x0, x21
   2f0f4:	mov	x4, x27
   2f0f8:	mov	x25, x24
   2f0fc:	bl	d210 <__gmpn_pow_1@plt>
   2f100:	mov	x22, x0
   2f104:	mov	x0, x27
   2f108:	mov	x1, x21
   2f10c:	mov	x2, x22
   2f110:	mov	x3, x23
   2f114:	mov	x4, x24
   2f118:	bl	ccd0 <__gmpn_mul@plt>
   2f11c:	add	x8, x22, x24
   2f120:	add	x9, x27, x8, lsl #3
   2f124:	ldur	x9, [x9, #-8]
   2f128:	mov	x10, x23
   2f12c:	cmp	x9, #0x0
   2f130:	cset	w9, eq  // eq = none
   2f134:	sub	x23, x8, x9
   2f138:	cmp	x23, x20
   2f13c:	b.gt	2f174 <__gmpn_rootrem@@Base+0x548>
   2f140:	b.ne	2f520 <__gmpn_rootrem@@Base+0x8f4>  // b.any
   2f144:	ldr	x8, [x19, #144]
   2f148:	ldr	x21, [x19, #88]
   2f14c:	mov	x9, x20
   2f150:	subs	x10, x9, #0x1
   2f154:	b.lt	2f194 <__gmpn_rootrem@@Base+0x568>  // b.tstop
   2f158:	ldr	x11, [x8], #-8
   2f15c:	ldr	x9, [x28, x9, lsl #3]
   2f160:	cmp	x11, x9
   2f164:	mov	x9, x10
   2f168:	b.eq	2f150 <__gmpn_rootrem@@Base+0x524>  // b.none
   2f16c:	ldr	x10, [x19, #160]
   2f170:	b.ls	2f528 <__gmpn_rootrem@@Base+0x8fc>  // b.plast
   2f174:	ldr	x22, [x19, #152]
   2f178:	mov	x8, x10
   2f17c:	mov	x23, x10
   2f180:	ldr	x9, [x8]
   2f184:	sub	x10, x9, #0x1
   2f188:	str	x10, [x8], #8
   2f18c:	cbz	x9, 2f180 <__gmpn_rootrem@@Base+0x554>
   2f190:	b	2f0e0 <__gmpn_rootrem@@Base+0x4b4>
   2f194:	mov	w8, #0x1                   	// #1
   2f198:	mov	x23, x20
   2f19c:	ldr	x9, [x19, #128]
   2f1a0:	add	x10, x19, #0xc0
   2f1a4:	sub	x11, x26, #0x1
   2f1a8:	sub	x12, x9, #0x1
   2f1ac:	ldr	x9, [x10, x9, lsl #3]
   2f1b0:	ldr	x10, [x10, x12, lsl #3]
   2f1b4:	str	x12, [x19, #104]
   2f1b8:	sub	x25, x10, x9
   2f1bc:	sub	x26, x26, x25
   2f1c0:	lsr	x9, x11, #6
   2f1c4:	lsr	x11, x26, #6
   2f1c8:	sub	x9, x9, x11
   2f1cc:	lsr	x27, x25, #6
   2f1d0:	add	x10, x9, #0x1
   2f1d4:	str	x26, [x19, #128]
   2f1d8:	tbz	w8, #0, 2f1ec <__gmpn_rootrem@@Base+0x5c0>
   2f1dc:	str	xzr, [x19, #144]
   2f1e0:	str	xzr, [x19, #64]
   2f1e4:	str	x27, [x19, #112]
   2f1e8:	b	2f31c <__gmpn_rootrem@@Base+0x6f0>
   2f1ec:	str	x11, [x19, #80]
   2f1f0:	str	x10, [x19, #96]
   2f1f4:	cbz	x23, 2f230 <__gmpn_rootrem@@Base+0x604>
   2f1f8:	ldr	x2, [x19, #168]
   2f1fc:	mov	x0, x21
   2f200:	mov	x1, x21
   2f204:	mov	x3, x23
   2f208:	bl	c2d0 <__gmpn_sub_n@plt>
   2f20c:	cbz	x0, 2f230 <__gmpn_rootrem@@Base+0x604>
   2f210:	cmp	x23, x20
   2f214:	b.ge	2f230 <__gmpn_rootrem@@Base+0x604>  // b.tcont
   2f218:	lsl	x8, x23, #3
   2f21c:	ldr	x9, [x21, x8]
   2f220:	add	x23, x23, #0x1
   2f224:	sub	x10, x9, #0x1
   2f228:	str	x10, [x21, x8]
   2f22c:	cbz	x9, 2f210 <__gmpn_rootrem@@Base+0x5e4>
   2f230:	ldr	x26, [x19, #72]
   2f234:	ldr	x23, [x19, #112]
   2f238:	ldr	x9, [x19, #136]
   2f23c:	mov	x20, xzr
   2f240:	str	x24, [x19, #144]
   2f244:	add	x8, x26, x23
   2f248:	sub	x8, x8, x9
   2f24c:	add	x8, x28, x8, lsl #3
   2f250:	ldr	x9, [x8, x20, lsl #3]
   2f254:	sub	x20, x20, #0x1
   2f258:	cbz	x9, 2f250 <__gmpn_rootrem@@Base+0x624>
   2f25c:	ldr	x24, [x19, #136]
   2f260:	add	x8, x26, x23
   2f264:	and	x3, x25, #0x3f
   2f268:	add	x0, x21, x27, lsl #3
   2f26c:	sub	x8, x8, x24
   2f270:	add	x8, x8, x20
   2f274:	add	x2, x8, #0x1
   2f278:	mov	x1, x21
   2f27c:	str	x0, [x19, #56]
   2f280:	cbz	x3, 2f2b0 <__gmpn_rootrem@@Base+0x684>
   2f284:	bl	c180 <__gmpn_lshift@plt>
   2f288:	add	x8, x26, x27
   2f28c:	add	x8, x8, x23
   2f290:	sub	x9, x8, x24
   2f294:	add	x8, x9, x20
   2f298:	cbz	x0, 2f2e0 <__gmpn_rootrem@@Base+0x6b4>
   2f29c:	ldr	x10, [x19, #24]
   2f2a0:	add	x9, x10, x9, lsl #3
   2f2a4:	str	x0, [x9, x20, lsl #3]
   2f2a8:	add	x9, x8, #0x2
   2f2ac:	b	2f2e4 <__gmpn_rootrem@@Base+0x6b8>
   2f2b0:	bl	c000 <__gmpn_copyd@plt>
   2f2b4:	add	x8, x26, x27
   2f2b8:	add	x8, x8, x23
   2f2bc:	ldr	x12, [x19, #56]
   2f2c0:	sub	x8, x8, x24
   2f2c4:	ldr	x26, [x19, #128]
   2f2c8:	ldr	x24, [x19, #144]
   2f2cc:	ldr	x10, [x19, #96]
   2f2d0:	ldr	x11, [x19, #80]
   2f2d4:	add	x8, x8, x20
   2f2d8:	add	x9, x8, #0x1
   2f2dc:	b	2f2f8 <__gmpn_rootrem@@Base+0x6cc>
   2f2e0:	add	x9, x8, #0x1
   2f2e4:	ldr	x26, [x19, #128]
   2f2e8:	ldr	x24, [x19, #144]
   2f2ec:	ldr	x10, [x19, #96]
   2f2f0:	ldr	x11, [x19, #80]
   2f2f4:	ldr	x12, [x19, #56]
   2f2f8:	ldr	x12, [x12]
   2f2fc:	sub	x8, x10, #0x1
   2f300:	cmp	x8, x27
   2f304:	str	x9, [x19, #112]
   2f308:	str	x12, [x19, #144]
   2f30c:	b.le	2f31c <__gmpn_rootrem@@Base+0x6f0>
   2f310:	add	x8, x21, x27, lsl #3
   2f314:	ldr	x8, [x8, #8]
   2f318:	str	x8, [x19, #64]
   2f31c:	ldr	x8, [x19, #120]
   2f320:	ldr	x23, [x19, #160]
   2f324:	ands	x3, x26, #0x3f
   2f328:	mov	x0, x21
   2f32c:	add	x1, x8, x11, lsl #3
   2f330:	mov	x20, x10
   2f334:	mov	x2, x10
   2f338:	b.eq	2f344 <__gmpn_rootrem@@Base+0x718>  // b.none
   2f33c:	bl	c1a0 <__gmpn_rshift@plt>
   2f340:	b	2f348 <__gmpn_rootrem@@Base+0x71c>
   2f344:	bl	ca50 <__gmpn_copyi@plt>
   2f348:	and	x8, x25, #0x3f
   2f34c:	str	x8, [x19, #136]
   2f350:	lsl	x8, x27, #3
   2f354:	ldr	x9, [x21, x8]
   2f358:	mov	w10, #0x1                   	// #1
   2f35c:	lsl	x11, x10, x25
   2f360:	str	x11, [x19, #56]
   2f364:	sub	x11, x11, #0x1
   2f368:	and	x9, x9, x11
   2f36c:	ldr	x11, [x19, #144]
   2f370:	sub	x10, x20, #0x1
   2f374:	cmp	x10, x27
   2f378:	orr	x9, x9, x11
   2f37c:	str	x9, [x21, x8]
   2f380:	b.le	2f390 <__gmpn_rootrem@@Base+0x764>
   2f384:	ldr	x9, [x19, #64]
   2f388:	add	x8, x21, x27, lsl #3
   2f38c:	str	x9, [x8, #8]
   2f390:	ldr	x20, [x19, #176]
   2f394:	ldr	x3, [x19, #48]
   2f398:	mov	x2, x22
   2f39c:	mov	x0, x20
   2f3a0:	mov	x1, x20
   2f3a4:	bl	d490 <__gmpn_mul_1@plt>
   2f3a8:	ldr	x3, [x19, #136]
   2f3ac:	cmp	x0, #0x0
   2f3b0:	str	x0, [x20, x22, lsl #3]
   2f3b4:	cinc	x20, x22, ne  // ne = any
   2f3b8:	add	x22, x23, x27, lsl #3
   2f3bc:	cbz	x3, 2f3e4 <__gmpn_rootrem@@Base+0x7b8>
   2f3c0:	mov	x0, x22
   2f3c4:	mov	x1, x23
   2f3c8:	mov	x2, x24
   2f3cc:	bl	c180 <__gmpn_lshift@plt>
   2f3d0:	add	x24, x27, x24
   2f3d4:	cbz	x0, 2f400 <__gmpn_rootrem@@Base+0x7d4>
   2f3d8:	str	x0, [x23, x24, lsl #3]
   2f3dc:	add	x24, x24, #0x1
   2f3e0:	b	2f400 <__gmpn_rootrem@@Base+0x7d4>
   2f3e4:	mov	x0, x22
   2f3e8:	mov	x1, x23
   2f3ec:	mov	x2, x24
   2f3f0:	str	x25, [x19, #96]
   2f3f4:	bl	c000 <__gmpn_copyd@plt>
   2f3f8:	ldr	x25, [x19, #96]
   2f3fc:	add	x24, x27, x24
   2f400:	str	x24, [x19, #144]
   2f404:	ldr	x8, [x22]
   2f408:	ldr	x2, [x19, #112]
   2f40c:	str	x8, [x19, #96]
   2f410:	sub	x8, x25, #0x1
   2f414:	subs	x23, x2, x20
   2f418:	lsr	x27, x8, #6
   2f41c:	b.lt	2f538 <__gmpn_rootrem@@Base+0x90c>  // b.tstop
   2f420:	add	x26, x27, #0x1
   2f424:	cmp	x23, x26
   2f428:	str	x8, [x19, #80]
   2f42c:	b.gt	2f464 <__gmpn_rootrem@@Base+0x838>
   2f430:	mov	x24, x25
   2f434:	mov	x25, x22
   2f438:	ldp	x22, x3, [x19, #168]
   2f43c:	ldr	x5, [x19, #32]
   2f440:	mov	x1, x21
   2f444:	mov	x4, x20
   2f448:	mov	x0, x22
   2f44c:	bl	c320 <__gmpn_div_q@plt>
   2f450:	ldr	x8, [x22, x23, lsl #3]
   2f454:	mov	x22, x25
   2f458:	mov	x25, x24
   2f45c:	cmp	x8, #0x0
   2f460:	cinc	x23, x23, ne  // ne = any
   2f464:	ldr	x8, [x19, #136]
   2f468:	cmp	x26, x23
   2f46c:	b.ge	2f4a0 <__gmpn_rootrem@@Base+0x874>  // b.tcont
   2f470:	ldr	x20, [x19, #160]
   2f474:	ldr	x26, [x19, #128]
   2f478:	ldr	x24, [x19, #144]
   2f47c:	cbz	x27, 2f048 <__gmpn_rootrem@@Base+0x41c>
   2f480:	lsl	x2, x27, #3
   2f484:	mov	w1, #0xff                  	// #255
   2f488:	mov	x0, x20
   2f48c:	bl	c5f0 <memset@plt>
   2f490:	cmp	x27, #0x1
   2f494:	b.ne	2f4f0 <__gmpn_rootrem@@Base+0x8c4>  // b.any
   2f498:	mov	w8, #0x1                   	// #1
   2f49c:	b	2f50c <__gmpn_rootrem@@Base+0x8e0>
   2f4a0:	b.ne	2f4bc <__gmpn_rootrem@@Base+0x890>  // b.any
   2f4a4:	cbz	x8, 2f4bc <__gmpn_rootrem@@Base+0x890>
   2f4a8:	ldr	x8, [x19, #168]
   2f4ac:	ldr	x9, [x19, #56]
   2f4b0:	ldr	x8, [x8, x27, lsl #3]
   2f4b4:	cmp	x8, x9
   2f4b8:	b.cs	2f470 <__gmpn_rootrem@@Base+0x844>  // b.hs, b.nlast
   2f4bc:	ldp	x0, x1, [x19, #160]
   2f4c0:	mov	x2, x23
   2f4c4:	bl	ca50 <__gmpn_copyi@plt>
   2f4c8:	subs	x8, x26, x23
   2f4cc:	ldr	x26, [x19, #128]
   2f4d0:	ldr	x24, [x19, #144]
   2f4d4:	b.eq	2f05c <__gmpn_rootrem@@Base+0x430>  // b.none
   2f4d8:	ldr	x9, [x19, #160]
   2f4dc:	lsl	x2, x8, #3
   2f4e0:	mov	w1, wzr
   2f4e4:	add	x0, x9, x23, lsl #3
   2f4e8:	bl	c5f0 <memset@plt>
   2f4ec:	b	2f05c <__gmpn_rootrem@@Base+0x430>
   2f4f0:	and	x9, x27, #0x3fffffffffffffe
   2f4f4:	orr	x8, x27, #0x1
   2f4f8:	mov	x10, x9
   2f4fc:	subs	x10, x10, #0x2
   2f500:	b.ne	2f4fc <__gmpn_rootrem@@Base+0x8d0>  // b.any
   2f504:	cmp	x27, x9
   2f508:	b.eq	2f048 <__gmpn_rootrem@@Base+0x41c>  // b.none
   2f50c:	sub	x8, x27, x8
   2f510:	add	x8, x8, #0x1
   2f514:	subs	x8, x8, #0x1
   2f518:	b.ne	2f514 <__gmpn_rootrem@@Base+0x8e8>  // b.any
   2f51c:	b	2f048 <__gmpn_rootrem@@Base+0x41c>
   2f520:	mov	w8, wzr
   2f524:	b	2f530 <__gmpn_rootrem@@Base+0x904>
   2f528:	mov	w8, wzr
   2f52c:	mov	x23, x20
   2f530:	ldr	x21, [x19, #88]
   2f534:	b	2f19c <__gmpn_rootrem@@Base+0x570>
   2f538:	ldr	x0, [x19, #160]
   2f53c:	lsl	x8, x27, #3
   2f540:	add	x2, x8, #0x8
   2f544:	mov	w1, wzr
   2f548:	bl	c5f0 <memset@plt>
   2f54c:	ldr	x26, [x19, #128]
   2f550:	ldr	x24, [x19, #144]
   2f554:	b	2f05c <__gmpn_rootrem@@Base+0x430>
   2f558:	cmp	x9, #0x1
   2f55c:	b	2f740 <__gmpn_rootrem@@Base+0xb14>
   2f560:	ldr	x23, [x19, #160]
   2f564:	mov	w24, #0x1                   	// #1
   2f568:	ldr	w8, [x19, #20]
   2f56c:	ldr	x9, [x19, #112]
   2f570:	cbz	w8, 2f580 <__gmpn_rootrem@@Base+0x954>
   2f574:	ldr	x8, [x23]
   2f578:	cmp	x8, #0x1
   2f57c:	b.hi	2f608 <__gmpn_rootrem@@Base+0x9dc>  // b.pmore
   2f580:	ldr	x23, [x19, #72]
   2f584:	ldr	x25, [x19, #120]
   2f588:	ldr	x26, [x19, #48]
   2f58c:	add	x8, x25, x23, lsl #3
   2f590:	sub	x21, x8, #0x8
   2f594:	ldr	x8, [x19, #32]
   2f598:	add	x22, x8, x23, lsl #4
   2f59c:	ldp	x27, x0, [x19, #160]
   2f5a0:	ldr	x4, [x19, #176]
   2f5a4:	mov	x2, x24
   2f5a8:	mov	x3, x26
   2f5ac:	mov	x1, x27
   2f5b0:	bl	d210 <__gmpn_pow_1@plt>
   2f5b4:	cmp	x0, x23
   2f5b8:	b.gt	2f5f0 <__gmpn_rootrem@@Base+0x9c4>
   2f5bc:	b.ne	2f76c <__gmpn_rootrem@@Base+0xb40>  // b.any
   2f5c0:	mov	x8, xzr
   2f5c4:	add	x9, x23, x8
   2f5c8:	cmp	x9, #0x1
   2f5cc:	b.lt	2f604 <__gmpn_rootrem@@Base+0x9d8>  // b.tstop
   2f5d0:	lsl	x9, x8, #3
   2f5d4:	ldr	x10, [x22, x9]
   2f5d8:	ldr	x9, [x21, x9]
   2f5dc:	sub	x8, x8, #0x1
   2f5e0:	cmp	x10, x9
   2f5e4:	b.eq	2f5c4 <__gmpn_rootrem@@Base+0x998>  // b.none
   2f5e8:	ldr	x27, [x19, #160]
   2f5ec:	b.ls	2f774 <__gmpn_rootrem@@Base+0xb48>  // b.plast
   2f5f0:	ldr	x9, [x27]
   2f5f4:	sub	x10, x9, #0x1
   2f5f8:	str	x10, [x27], #8
   2f5fc:	cbz	x9, 2f5f0 <__gmpn_rootrem@@Base+0x9c4>
   2f600:	b	2f59c <__gmpn_rootrem@@Base+0x970>
   2f604:	mov	x9, xzr
   2f608:	ldr	x0, [x19, #184]
   2f60c:	cbz	x0, 2f748 <__gmpn_rootrem@@Base+0xb1c>
   2f610:	mov	x20, x9
   2f614:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2f618:	mov	x9, x20
   2f61c:	b	2f748 <__gmpn_rootrem@@Base+0xb1c>
   2f620:	mov	x10, xzr
   2f624:	mov	w9, #0x1                   	// #1
   2f628:	mov	x11, x8
   2f62c:	cmp	x9, x24
   2f630:	b.ge	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.tcont
   2f634:	add	x12, x27, x10
   2f638:	ldr	x12, [x12, #8]
   2f63c:	add	x13, x16, x10
   2f640:	add	x9, x9, #0x1
   2f644:	add	x10, x10, #0x8
   2f648:	sub	x14, x12, #0x1
   2f64c:	sub	x11, x11, #0x1
   2f650:	str	x14, [x13, #8]
   2f654:	cbz	x12, 2f62c <__gmpn_rootrem@@Base+0xa00>
   2f658:	cmp	x27, x16
   2f65c:	b.eq	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.none
   2f660:	cmp	x9, x24
   2f664:	b.ge	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.tcont
   2f668:	sub	x12, x24, x9
   2f66c:	cmp	x12, #0x4
   2f670:	b.cc	2f6e0 <__gmpn_rootrem@@Base+0xab4>  // b.lo, b.ul, b.last
   2f674:	add	x14, x16, x10
   2f678:	lsl	x13, x24, #3
   2f67c:	add	x14, x14, #0x8
   2f680:	add	x15, x27, x13
   2f684:	cmp	x14, x15
   2f688:	b.cs	2f6a0 <__gmpn_rootrem@@Base+0xa74>  // b.hs, b.nlast
   2f68c:	add	x14, x27, x10
   2f690:	add	x13, x16, x13
   2f694:	add	x14, x14, #0x8
   2f698:	cmp	x14, x13
   2f69c:	b.cc	2f6e0 <__gmpn_rootrem@@Base+0xab4>  // b.lo, b.ul, b.last
   2f6a0:	add	x13, x16, x10
   2f6a4:	add	x14, x27, x10
   2f6a8:	and	x10, x12, #0xfffffffffffffffc
   2f6ac:	and	x15, x11, #0xfffffffffffffffc
   2f6b0:	add	x11, x13, #0x18
   2f6b4:	add	x13, x14, #0x18
   2f6b8:	add	x9, x15, x9
   2f6bc:	mov	x14, x10
   2f6c0:	ldp	q0, q1, [x13, #-16]
   2f6c4:	add	x13, x13, #0x20
   2f6c8:	subs	x14, x14, #0x4
   2f6cc:	stp	q0, q1, [x11, #-16]
   2f6d0:	add	x11, x11, #0x20
   2f6d4:	b.ne	2f6c0 <__gmpn_rootrem@@Base+0xa94>  // b.any
   2f6d8:	cmp	x12, x10
   2f6dc:	b.eq	2f738 <__gmpn_rootrem@@Base+0xb0c>  // b.none
   2f6e0:	lsl	x11, x9, #3
   2f6e4:	sub	x10, x24, x9
   2f6e8:	add	x9, x16, x11
   2f6ec:	add	x11, x27, x11
   2f6f0:	ldr	x12, [x11], #8
   2f6f4:	subs	x10, x10, #0x1
   2f6f8:	str	x12, [x9], #8
   2f6fc:	b.ne	2f6f0 <__gmpn_rootrem@@Base+0xac4>  // b.any
   2f700:	b	2f738 <__gmpn_rootrem@@Base+0xb0c>
   2f704:	and	x10, x8, #0xfffffffffffffffc
   2f708:	add	x11, x27, #0x18
   2f70c:	orr	x9, x10, #0x1
   2f710:	add	x12, x16, #0x18
   2f714:	mov	x13, x10
   2f718:	ldp	q0, q1, [x11, #-16]
   2f71c:	add	x11, x11, #0x20
   2f720:	subs	x13, x13, #0x4
   2f724:	stp	q0, q1, [x12, #-16]
   2f728:	add	x12, x12, #0x20
   2f72c:	b.ne	2f718 <__gmpn_rootrem@@Base+0xaec>  // b.any
   2f730:	cmp	x8, x10
   2f734:	b.ne	2ee4c <__gmpn_rootrem@@Base+0x220>  // b.any
   2f738:	ldr	x8, [x16, x8, lsl #3]
   2f73c:	cmp	x8, #0x0
   2f740:	cset	w8, eq  // eq = none
   2f744:	sub	x9, x24, x8
   2f748:	mov	x0, x9
   2f74c:	mov	sp, x29
   2f750:	ldp	x20, x19, [sp, #80]
   2f754:	ldp	x22, x21, [sp, #64]
   2f758:	ldp	x24, x23, [sp, #48]
   2f75c:	ldp	x26, x25, [sp, #32]
   2f760:	ldp	x28, x27, [sp, #16]
   2f764:	ldp	x29, x30, [sp], #96
   2f768:	ret
   2f76c:	mov	x20, x0
   2f770:	b	2f778 <__gmpn_rootrem@@Base+0xb4c>
   2f774:	mov	x20, x23
   2f778:	ldr	x0, [x19, #8]
   2f77c:	cbz	x0, 2f7c0 <__gmpn_rootrem@@Base+0xb94>
   2f780:	cbz	x20, 2f7cc <__gmpn_rootrem@@Base+0xba0>
   2f784:	ldr	x2, [x19, #168]
   2f788:	mov	x1, x25
   2f78c:	mov	x3, x20
   2f790:	bl	c2d0 <__gmpn_sub_n@plt>
   2f794:	cbz	x0, 2f7c8 <__gmpn_rootrem@@Base+0xb9c>
   2f798:	ldr	x0, [x19, #8]
   2f79c:	cmp	x20, x23
   2f7a0:	b.ge	2f868 <__gmpn_rootrem@@Base+0xc3c>  // b.tcont
   2f7a4:	lsl	x8, x20, #3
   2f7a8:	ldr	x9, [x25, x8]
   2f7ac:	add	x20, x20, #0x1
   2f7b0:	sub	x10, x9, #0x1
   2f7b4:	str	x10, [x0, x8]
   2f7b8:	cbz	x9, 2f79c <__gmpn_rootrem@@Base+0xb70>
   2f7bc:	b	2f7cc <__gmpn_rootrem@@Base+0xba0>
   2f7c0:	mov	w9, #0x1                   	// #1
   2f7c4:	b	2f608 <__gmpn_rootrem@@Base+0x9dc>
   2f7c8:	ldr	x0, [x19, #8]
   2f7cc:	cmp	x0, x25
   2f7d0:	b.eq	2f868 <__gmpn_rootrem@@Base+0xc3c>  // b.none
   2f7d4:	cmp	x20, x23
   2f7d8:	b.ge	2f868 <__gmpn_rootrem@@Base+0xc3c>  // b.tcont
   2f7dc:	sub	x8, x23, x20
   2f7e0:	cmp	x8, #0x4
   2f7e4:	b.cc	2f848 <__gmpn_rootrem@@Base+0xc1c>  // b.lo, b.ul, b.last
   2f7e8:	lsl	x10, x20, #3
   2f7ec:	lsl	x9, x23, #3
   2f7f0:	add	x11, x0, x10
   2f7f4:	add	x12, x25, x9
   2f7f8:	cmp	x11, x12
   2f7fc:	b.cs	2f810 <__gmpn_rootrem@@Base+0xbe4>  // b.hs, b.nlast
   2f800:	add	x9, x0, x9
   2f804:	add	x11, x25, x10
   2f808:	cmp	x11, x9
   2f80c:	b.cc	2f848 <__gmpn_rootrem@@Base+0xc1c>  // b.lo, b.ul, b.last
   2f810:	and	x9, x8, #0xfffffffffffffffc
   2f814:	add	x11, x10, #0x10
   2f818:	add	x20, x20, x9
   2f81c:	add	x10, x25, x11
   2f820:	add	x11, x0, x11
   2f824:	mov	x12, x9
   2f828:	ldp	q0, q1, [x10, #-16]
   2f82c:	add	x10, x10, #0x20
   2f830:	subs	x12, x12, #0x4
   2f834:	stp	q0, q1, [x11, #-16]
   2f838:	add	x11, x11, #0x20
   2f83c:	b.ne	2f828 <__gmpn_rootrem@@Base+0xbfc>  // b.any
   2f840:	cmp	x8, x9
   2f844:	b.eq	2f868 <__gmpn_rootrem@@Base+0xc3c>  // b.none
   2f848:	lsl	x10, x20, #3
   2f84c:	sub	x8, x23, x20
   2f850:	add	x9, x0, x10
   2f854:	add	x10, x25, x10
   2f858:	ldr	x11, [x10], #8
   2f85c:	subs	x8, x8, #0x1
   2f860:	str	x11, [x9], #8
   2f864:	b.ne	2f858 <__gmpn_rootrem@@Base+0xc2c>  // b.any
   2f868:	sub	x8, x0, #0x8
   2f86c:	ldr	x10, [x8, x23, lsl #3]
   2f870:	sub	x9, x23, #0x1
   2f874:	mov	x23, x9
   2f878:	cbz	x10, 2f86c <__gmpn_rootrem@@Base+0xc40>
   2f87c:	add	x9, x9, #0x1
   2f880:	b	2f608 <__gmpn_rootrem@@Base+0x9dc>
   2f884:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2f888:	add	x10, x10, #0x1b4
   2f88c:	ldrb	w10, [x10, x8]
   2f890:	udiv	x8, x9, x28
   2f894:	msub	x9, x8, x28, x9
   2f898:	bfi	x10, x9, #8, #56
   2f89c:	udiv	x9, x10, x28
   2f8a0:	b	2eed4 <__gmpn_rootrem@@Base+0x2a8>
   2f8a4:	add	x0, x19, #0xb8
   2f8a8:	mov	x26, x16
   2f8ac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2f8b0:	mov	x16, x26
   2f8b4:	b	2efe8 <__gmpn_rootrem@@Base+0x3bc>

000000000002f8b8 <__gmpn_sqrtrem@@Base>:
   2f8b8:	stp	x29, x30, [sp, #-96]!
   2f8bc:	str	x27, [sp, #16]
   2f8c0:	stp	x26, x25, [sp, #32]
   2f8c4:	stp	x24, x23, [sp, #48]
   2f8c8:	stp	x22, x21, [sp, #64]
   2f8cc:	stp	x20, x19, [sp, #80]
   2f8d0:	mov	x29, sp
   2f8d4:	sub	sp, sp, #0x10
   2f8d8:	add	x8, x2, x3, lsl #3
   2f8dc:	ldur	x9, [x8, #-8]
   2f8e0:	mov	x23, x2
   2f8e4:	mov	x20, x1
   2f8e8:	mov	x19, x0
   2f8ec:	clz	x8, x9
   2f8f0:	lsr	x10, x9, #62
   2f8f4:	ubfx	x8, x8, #1, #31
   2f8f8:	cmp	x10, #0x0
   2f8fc:	csel	w22, wzr, w8, ne  // ne = any
   2f900:	cmp	x3, #0x2
   2f904:	b.eq	2f9c0 <__gmpn_sqrtrem@@Base+0x108>  // b.none
   2f908:	mov	x24, x3
   2f90c:	cmp	x3, #0x1
   2f910:	b.ne	2fad4 <__gmpn_sqrtrem@@Base+0x21c>  // b.any
   2f914:	mov	x8, #0x1ffff00000000       	// #562945658454016
   2f918:	movk	x8, #0xfffd, lsl #16
   2f91c:	cbz	w22, 2fb84 <__gmpn_sqrtrem@@Base+0x2cc>
   2f920:	lsl	w10, w22, #1
   2f924:	lsl	x10, x9, x10
   2f928:	adrp	x11, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2f92c:	lsr	x12, x10, #55
   2f930:	add	x11, x11, #0x3b4
   2f934:	sub	w12, w12, #0x80
   2f938:	ldrb	w11, [x11, w12, uxtw]
   2f93c:	lsr	x13, x10, #31
   2f940:	lsr	x14, x10, #24
   2f944:	mov	x12, #0xffffff0000000000    	// #-1099511627776
   2f948:	orr	x11, x11, #0x100
   2f94c:	mul	x13, x11, x13
   2f950:	msub	x8, x13, x11, x8
   2f954:	asr	x8, x8, #16
   2f958:	mul	x8, x8, x11
   2f95c:	asr	x8, x8, #18
   2f960:	add	x8, x8, x11, lsl #16
   2f964:	mul	x11, x8, x14
   2f968:	lsl	x13, x10, #14
   2f96c:	lsr	x14, x11, #25
   2f970:	msub	x13, x14, x14, x13
   2f974:	add	x12, x13, x12
   2f978:	asr	x12, x12, #24
   2f97c:	mul	x8, x12, x8
   2f980:	add	x8, x11, x8, asr #15
   2f984:	lsr	x11, x8, #32
   2f988:	mul	x8, x11, x11
   2f98c:	lsl	x12, x11, #1
   2f990:	sub	x13, x10, #0x1
   2f994:	add	x12, x8, x12
   2f998:	mov	w14, #0x1                   	// #1
   2f99c:	cmp	x12, x13
   2f9a0:	bfi	x14, x11, #1, #32
   2f9a4:	cinc	x12, x11, ls  // ls = plast
   2f9a8:	csneg	x11, xzr, x14, hi  // hi = pmore
   2f9ac:	lsr	x12, x12, x22
   2f9b0:	str	x12, [x19]
   2f9b4:	cbz	x20, 2fd1c <__gmpn_sqrtrem@@Base+0x464>
   2f9b8:	msub	x8, x12, x12, x9
   2f9bc:	b	2fc18 <__gmpn_sqrtrem@@Base+0x360>
   2f9c0:	ldr	x10, [x23]
   2f9c4:	mov	x11, #0x1ffff00000000       	// #562945658454016
   2f9c8:	cmp	x20, #0x0
   2f9cc:	sub	x8, x29, #0x10
   2f9d0:	movk	x11, #0xfffd, lsl #16
   2f9d4:	csel	x8, x8, x20, eq  // eq = none
   2f9d8:	cbz	w22, 2fc20 <__gmpn_sqrtrem@@Base+0x368>
   2f9dc:	lsl	w12, w22, #1
   2f9e0:	neg	w14, w12
   2f9e4:	lsl	x9, x9, x12
   2f9e8:	lsr	x14, x10, x14
   2f9ec:	orr	x9, x14, x9
   2f9f0:	adrp	x13, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2f9f4:	lsr	x14, x9, #55
   2f9f8:	add	x13, x13, #0x3b4
   2f9fc:	sub	w14, w14, #0x80
   2fa00:	ldrb	w13, [x13, w14, uxtw]
   2fa04:	lsr	x15, x9, #31
   2fa08:	lsr	x16, x9, #24
   2fa0c:	mov	x14, #0xffffff0000000000    	// #-1099511627776
   2fa10:	orr	x13, x13, #0x100
   2fa14:	mul	x15, x13, x15
   2fa18:	msub	x11, x15, x13, x11
   2fa1c:	asr	x11, x11, #16
   2fa20:	mul	x11, x11, x13
   2fa24:	asr	x11, x11, #18
   2fa28:	add	x11, x11, x13, lsl #16
   2fa2c:	mul	x13, x11, x16
   2fa30:	lsl	x15, x9, #14
   2fa34:	lsr	x16, x13, #25
   2fa38:	msub	x15, x16, x16, x15
   2fa3c:	add	x14, x15, x14
   2fa40:	asr	x14, x14, #24
   2fa44:	mul	x11, x14, x11
   2fa48:	add	x11, x13, x11, asr #15
   2fa4c:	lsr	x11, x11, #32
   2fa50:	mul	x13, x11, x11
   2fa54:	lsl	x14, x11, #1
   2fa58:	mov	w16, #0x1                   	// #1
   2fa5c:	sub	x15, x9, #0x1
   2fa60:	add	x14, x13, x14
   2fa64:	bfi	x16, x11, #1, #32
   2fa68:	cmp	x14, x15
   2fa6c:	str	x9, [x8, #8]
   2fa70:	sub	x9, x9, x13
   2fa74:	csneg	x13, xzr, x16, hi  // hi = pmore
   2fa78:	lsl	x12, x10, x12
   2fa7c:	add	x9, x13, x9
   2fa80:	cinc	x11, x11, ls  // ls = plast
   2fa84:	extr	x9, x9, x12, #33
   2fa88:	udiv	x13, x9, x11
   2fa8c:	sub	x13, x13, x13, lsr #32
   2fa90:	msub	x9, x11, x13, x9
   2fa94:	orr	x11, x13, x11, lsl #32
   2fa98:	mul	x13, x13, x13
   2fa9c:	bfi	x12, x9, #33, #31
   2faa0:	cmp	x12, x13
   2faa4:	lsr	x14, x9, #31
   2faa8:	cset	w9, cc  // cc = lo, ul, last
   2faac:	sub	w9, w14, w9
   2fab0:	asr	w9, w9, #31
   2fab4:	add	x9, x11, w9, sxtw
   2fab8:	lsr	x9, x9, x22
   2fabc:	str	x9, [x19]
   2fac0:	msub	x9, x9, x9, x10
   2fac4:	cmp	x9, #0x0
   2fac8:	str	x9, [x8]
   2facc:	cset	w20, ne  // ne = any
   2fad0:	b	2ff18 <__gmpn_sqrtrem@@Base+0x660>
   2fad4:	add	x25, x24, #0x1
   2fad8:	add	x8, x24, #0x2
   2fadc:	cmp	x25, #0x0
   2fae0:	csinc	x8, x8, x24, lt  // lt = tstop
   2fae4:	asr	x21, x8, #1
   2fae8:	cbnz	x20, 2fb14 <__gmpn_sqrtrem@@Base+0x25c>
   2faec:	cmp	x24, #0x9
   2faf0:	b.lt	2fb14 <__gmpn_sqrtrem@@Base+0x25c>  // b.tstop
   2faf4:	and	w4, w24, #0x1
   2faf8:	mov	x0, x19
   2fafc:	mov	x1, x23
   2fb00:	mov	x2, x21
   2fb04:	mov	w3, w22
   2fb08:	bl	2ff54 <__gmpn_sqrtrem@@Base+0x69c>
   2fb0c:	sxtw	x20, w0
   2fb10:	b	2ff18 <__gmpn_sqrtrem@@Base+0x660>
   2fb14:	and	x27, x24, #0x1
   2fb18:	orr	x8, x27, x22
   2fb1c:	stur	xzr, [x29, #-16]
   2fb20:	cbz	x8, 2fd30 <__gmpn_sqrtrem@@Base+0x478>
   2fb24:	add	x8, x25, #0x3
   2fb28:	cmp	x25, #0x0
   2fb2c:	lsl	x26, x21, #1
   2fb30:	csel	x8, x8, x25, lt  // lt = tstop
   2fb34:	add	x8, x26, x8, lsr #2
   2fb38:	lsl	x8, x8, #3
   2fb3c:	add	x1, x8, #0x8
   2fb40:	mov	w8, #0x7f00                	// #32512
   2fb44:	cmp	x1, x8
   2fb48:	b.hi	2fdc0 <__gmpn_sqrtrem@@Base+0x508>  // b.pmore
   2fb4c:	add	x9, x1, #0xf
   2fb50:	mov	x8, sp
   2fb54:	and	x9, x9, #0xfffffffffffffff0
   2fb58:	sub	x25, x8, x9
   2fb5c:	mov	sp, x25
   2fb60:	add	x26, x25, x26, lsl #3
   2fb64:	add	x0, x25, x27, lsl #3
   2fb68:	str	xzr, [x25]
   2fb6c:	cbz	w22, 2fddc <__gmpn_sqrtrem@@Base+0x524>
   2fb70:	lsl	w3, w22, #1
   2fb74:	mov	x1, x23
   2fb78:	mov	x2, x24
   2fb7c:	bl	c180 <__gmpn_lshift@plt>
   2fb80:	b	2fde8 <__gmpn_sqrtrem@@Base+0x530>
   2fb84:	lsr	x10, x9, #55
   2fb88:	adrp	x11, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2fb8c:	add	x11, x11, #0x3b4
   2fb90:	sub	w10, w10, #0x80
   2fb94:	ldrb	w10, [x11, w10, uxtw]
   2fb98:	lsr	x11, x9, #31
   2fb9c:	lsr	x12, x9, #24
   2fba0:	lsl	x13, x9, #14
   2fba4:	orr	x10, x10, #0x100
   2fba8:	mul	x11, x10, x11
   2fbac:	msub	x8, x11, x10, x8
   2fbb0:	asr	x8, x8, #16
   2fbb4:	mul	x8, x8, x10
   2fbb8:	asr	x8, x8, #18
   2fbbc:	add	x8, x8, x10, lsl #16
   2fbc0:	mul	x10, x8, x12
   2fbc4:	lsr	x12, x10, #25
   2fbc8:	mov	x11, #0xffffff0000000000    	// #-1099511627776
   2fbcc:	msub	x12, x12, x12, x13
   2fbd0:	add	x11, x12, x11
   2fbd4:	asr	x11, x11, #24
   2fbd8:	mul	x8, x11, x8
   2fbdc:	add	x8, x10, x8, asr #15
   2fbe0:	lsr	x8, x8, #32
   2fbe4:	mul	x10, x8, x8
   2fbe8:	lsl	x11, x8, #1
   2fbec:	sub	x13, x9, #0x1
   2fbf0:	mov	w12, #0x1                   	// #1
   2fbf4:	add	x11, x10, x11
   2fbf8:	bfi	x12, x8, #1, #32
   2fbfc:	cmp	x11, x13
   2fc00:	sub	x9, x9, x10
   2fc04:	cinc	x10, x8, ls  // ls = plast
   2fc08:	csneg	x8, xzr, x12, hi  // hi = pmore
   2fc0c:	add	x8, x8, x9
   2fc10:	str	x10, [x19]
   2fc14:	cbz	x20, 2fd24 <__gmpn_sqrtrem@@Base+0x46c>
   2fc18:	str	x8, [x20]
   2fc1c:	b	2fd24 <__gmpn_sqrtrem@@Base+0x46c>
   2fc20:	ldr	x9, [x23, #8]
   2fc24:	adrp	x12, 5d000 <__gmpn_bases@@Base+0x2ab8>
   2fc28:	add	x12, x12, #0x3b4
   2fc2c:	lsr	x13, x9, #55
   2fc30:	sub	w13, w13, #0x80
   2fc34:	ldrb	w12, [x12, w13, uxtw]
   2fc38:	lsr	x14, x9, #31
   2fc3c:	lsr	x15, x9, #24
   2fc40:	mov	x13, #0xffffff0000000000    	// #-1099511627776
   2fc44:	orr	x12, x12, #0x100
   2fc48:	mul	x14, x12, x14
   2fc4c:	msub	x11, x14, x12, x11
   2fc50:	asr	x11, x11, #16
   2fc54:	mul	x11, x11, x12
   2fc58:	asr	x11, x11, #18
   2fc5c:	add	x11, x11, x12, lsl #16
   2fc60:	mul	x12, x11, x15
   2fc64:	lsl	x14, x9, #14
   2fc68:	lsr	x15, x12, #25
   2fc6c:	msub	x14, x15, x15, x14
   2fc70:	add	x13, x14, x13
   2fc74:	asr	x13, x13, #24
   2fc78:	mul	x11, x13, x11
   2fc7c:	add	x11, x12, x11, asr #15
   2fc80:	lsr	x11, x11, #32
   2fc84:	mul	x12, x11, x11
   2fc88:	lsl	x13, x11, #1
   2fc8c:	mov	w15, #0x1                   	// #1
   2fc90:	sub	x14, x9, #0x1
   2fc94:	add	x13, x12, x13
   2fc98:	bfi	x15, x11, #1, #32
   2fc9c:	cmp	x13, x14
   2fca0:	sub	x9, x9, x12
   2fca4:	cinc	x12, x11, ls  // ls = plast
   2fca8:	csneg	x11, xzr, x15, hi  // hi = pmore
   2fcac:	add	x9, x11, x9
   2fcb0:	str	x9, [x8]
   2fcb4:	extr	x9, x9, x10, #33
   2fcb8:	udiv	x11, x9, x12
   2fcbc:	sub	x13, x11, x11, lsr #32
   2fcc0:	msub	x9, x12, x13, x9
   2fcc4:	lsr	x14, x9, #31
   2fcc8:	bfi	x10, x9, #33, #31
   2fccc:	mul	x9, x13, x13
   2fcd0:	subs	x11, x10, x9
   2fcd4:	cset	w9, cc  // cc = lo, ul, last
   2fcd8:	subs	w9, w14, w9
   2fcdc:	orr	x10, x13, x12, lsl #32
   2fce0:	b.pl	2fcf8 <__gmpn_sqrtrem@@Base+0x440>  // b.nfrst
   2fce4:	adds	x11, x11, x10
   2fce8:	sub	x10, x10, #0x1
   2fcec:	cinc	w9, w9, cs  // cs = hs, nlast
   2fcf0:	adds	x11, x11, x10
   2fcf4:	cinc	w9, w9, cs  // cs = hs, nlast
   2fcf8:	str	x11, [x8]
   2fcfc:	str	x10, [x19]
   2fd00:	ldr	x10, [x8]
   2fd04:	sxtw	x9, w9
   2fd08:	str	x9, [x8, #8]
   2fd0c:	orr	x8, x10, x9
   2fd10:	cmp	x8, #0x0
   2fd14:	cinc	x20, x9, ne  // ne = any
   2fd18:	b	2ff18 <__gmpn_sqrtrem@@Base+0x660>
   2fd1c:	sub	x8, x10, x8
   2fd20:	add	x8, x8, x11
   2fd24:	cmp	x8, #0x0
   2fd28:	cset	w20, ne  // ne = any
   2fd2c:	b	2ff18 <__gmpn_sqrtrem@@Base+0x660>
   2fd30:	cmp	x20, x23
   2fd34:	b.eq	2fd68 <__gmpn_sqrtrem@@Base+0x4b0>  // b.none
   2fd38:	cbnz	x20, 2fd54 <__gmpn_sqrtrem@@Base+0x49c>
   2fd3c:	lsl	x8, x24, #3
   2fd40:	add	x8, x8, #0xf
   2fd44:	mov	x9, sp
   2fd48:	and	x8, x8, #0xfffffffffffffff0
   2fd4c:	sub	x20, x9, x8
   2fd50:	mov	sp, x20
   2fd54:	mov	x0, x20
   2fd58:	mov	x1, x23
   2fd5c:	mov	x2, x24
   2fd60:	bl	ca50 <__gmpn_copyi@plt>
   2fd64:	mov	x23, x20
   2fd68:	add	x8, x25, #0x3
   2fd6c:	cmp	x25, #0x0
   2fd70:	csel	x8, x8, x25, lt  // lt = tstop
   2fd74:	lsl	x8, x8, #1
   2fd78:	and	x8, x8, #0xfffffffffffffff8
   2fd7c:	add	x1, x8, #0x8
   2fd80:	mov	w8, #0x7f00                	// #32512
   2fd84:	cmp	x1, x8
   2fd88:	b.hi	2ff44 <__gmpn_sqrtrem@@Base+0x68c>  // b.pmore
   2fd8c:	add	x9, x1, #0xf
   2fd90:	mov	x8, sp
   2fd94:	and	x9, x9, #0xfffffffffffffff0
   2fd98:	sub	x4, x8, x9
   2fd9c:	mov	sp, x4
   2fda0:	mov	x0, x19
   2fda4:	mov	x1, x23
   2fda8:	mov	x2, x21
   2fdac:	mov	x3, xzr
   2fdb0:	bl	30408 <__gmpn_sqrtrem@@Base+0xb50>
   2fdb4:	add	x19, x0, x21
   2fdb8:	str	x0, [x23, x21, lsl #3]
   2fdbc:	b	2fef8 <__gmpn_sqrtrem@@Base+0x640>
   2fdc0:	sub	x0, x29, #0x10
   2fdc4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2fdc8:	mov	x25, x0
   2fdcc:	add	x26, x25, x26, lsl #3
   2fdd0:	add	x0, x25, x27, lsl #3
   2fdd4:	str	xzr, [x25]
   2fdd8:	cbnz	w22, 2fb70 <__gmpn_sqrtrem@@Base+0x2b8>
   2fddc:	mov	x1, x23
   2fde0:	mov	x2, x24
   2fde4:	bl	ca50 <__gmpn_copyi@plt>
   2fde8:	add	w22, w22, w27, lsl #5
   2fdec:	mov	x8, #0xffffffffffffffff    	// #-1
   2fdf0:	mov	x9, #0xfffffffffffffffe    	// #-2
   2fdf4:	lsl	x27, x8, x22
   2fdf8:	sub	x8, x9, x27
   2fdfc:	cmp	x20, #0x0
   2fe00:	csel	x3, x8, xzr, eq  // eq = none
   2fe04:	mov	x0, x19
   2fe08:	mov	x1, x25
   2fe0c:	mov	x2, x21
   2fe10:	mov	x4, x26
   2fe14:	bl	30408 <__gmpn_sqrtrem@@Base+0xb50>
   2fe18:	ldr	x8, [x19]
   2fe1c:	mov	x23, x0
   2fe20:	mov	x0, x25
   2fe24:	mov	x1, x19
   2fe28:	bic	x26, x8, x27
   2fe2c:	lsl	x3, x26, #1
   2fe30:	mov	x2, x21
   2fe34:	str	x26, [x29, #24]
   2fe38:	bl	d400 <__gmpn_addmul_1@plt>
   2fe3c:	add	x23, x0, x23
   2fe40:	add	x1, x29, #0x18
   2fe44:	mov	w2, #0x1                   	// #1
   2fe48:	mov	x0, x25
   2fe4c:	mov	x3, x26
   2fe50:	bl	c9e0 <__gmpn_submul_1@plt>
   2fe54:	cmp	x24, #0x3
   2fe58:	b.lt	2fea0 <__gmpn_sqrtrem@@Base+0x5e8>  // b.tstop
   2fe5c:	ldr	x8, [x25, #8]
   2fe60:	subs	x8, x8, x0
   2fe64:	str	x8, [x25, #8]
   2fe68:	b.cs	2fe9c <__gmpn_sqrtrem@@Base+0x5e4>  // b.hs, b.nlast
   2fe6c:	sub	x8, x21, #0x1
   2fe70:	mov	w9, #0x2                   	// #2
   2fe74:	mov	w0, #0x1                   	// #1
   2fe78:	sub	x10, x9, #0x1
   2fe7c:	cmp	x10, x8
   2fe80:	b.ge	2fea0 <__gmpn_sqrtrem@@Base+0x5e8>  // b.tcont
   2fe84:	lsl	x10, x9, #3
   2fe88:	ldr	x11, [x25, x10]
   2fe8c:	add	x9, x9, #0x1
   2fe90:	sub	x12, x11, #0x1
   2fe94:	str	x12, [x25, x10]
   2fe98:	cbz	x11, 2fe78 <__gmpn_sqrtrem@@Base+0x5c0>
   2fe9c:	mov	x0, xzr
   2fea0:	sub	x23, x23, x0
   2fea4:	mov	x0, x19
   2fea8:	mov	x1, x19
   2feac:	mov	x2, x21
   2feb0:	mov	w3, w22
   2feb4:	bl	c1a0 <__gmpn_rshift@plt>
   2feb8:	cmp	x20, #0x0
   2febc:	str	x23, [x25, x21, lsl #3]
   2fec0:	lsl	w8, w22, #1
   2fec4:	csel	x23, x25, x20, eq  // eq = none
   2fec8:	cmp	w22, #0x20
   2fecc:	add	x9, x25, #0x8
   2fed0:	sub	w10, w8, #0x40
   2fed4:	cinc	x19, x21, cc  // cc = lo, ul, last
   2fed8:	csel	w3, w8, w10, cc  // cc = lo, ul, last
   2fedc:	csel	x1, x25, x9, cc  // cc = lo, ul, last
   2fee0:	mov	x0, x23
   2fee4:	mov	x2, x19
   2fee8:	cbz	w3, 2fef4 <__gmpn_sqrtrem@@Base+0x63c>
   2feec:	bl	c1a0 <__gmpn_rshift@plt>
   2fef0:	b	2fef8 <__gmpn_sqrtrem@@Base+0x640>
   2fef4:	bl	ca50 <__gmpn_copyi@plt>
   2fef8:	sub	x8, x23, #0x8
   2fefc:	mov	x20, x19
   2ff00:	subs	x19, x19, #0x1
   2ff04:	b.lt	2ff10 <__gmpn_sqrtrem@@Base+0x658>  // b.tstop
   2ff08:	ldr	x9, [x8, x20, lsl #3]
   2ff0c:	cbz	x9, 2fefc <__gmpn_sqrtrem@@Base+0x644>
   2ff10:	ldur	x0, [x29, #-16]
   2ff14:	cbnz	x0, 2ff3c <__gmpn_sqrtrem@@Base+0x684>
   2ff18:	mov	x0, x20
   2ff1c:	mov	sp, x29
   2ff20:	ldp	x20, x19, [sp, #80]
   2ff24:	ldp	x22, x21, [sp, #64]
   2ff28:	ldp	x24, x23, [sp, #48]
   2ff2c:	ldp	x26, x25, [sp, #32]
   2ff30:	ldr	x27, [sp, #16]
   2ff34:	ldp	x29, x30, [sp], #96
   2ff38:	ret
   2ff3c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2ff40:	b	2ff18 <__gmpn_sqrtrem@@Base+0x660>
   2ff44:	sub	x0, x29, #0x10
   2ff48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ff4c:	mov	x4, x0
   2ff50:	b	2fda0 <__gmpn_sqrtrem@@Base+0x4e8>
   2ff54:	stp	x29, x30, [sp, #-96]!
   2ff58:	stp	x28, x27, [sp, #16]
   2ff5c:	stp	x26, x25, [sp, #32]
   2ff60:	stp	x24, x23, [sp, #48]
   2ff64:	stp	x22, x21, [sp, #64]
   2ff68:	stp	x20, x19, [sp, #80]
   2ff6c:	mov	x29, sp
   2ff70:	sub	sp, sp, #0x40
   2ff74:	sub	x8, x2, #0x1
   2ff78:	cmp	x8, #0x0
   2ff7c:	csel	x8, x2, x8, lt  // lt = tstop
   2ff80:	asr	x23, x8, #1
   2ff84:	add	x8, x23, x2, lsl #1
   2ff88:	lsl	x8, x8, #3
   2ff8c:	mov	x24, x1
   2ff90:	add	x1, x8, #0x20
   2ff94:	mov	w8, #0x7f00                	// #32512
   2ff98:	mov	w27, w3
   2ff9c:	mov	x19, x2
   2ffa0:	mov	x20, x0
   2ffa4:	cmp	x1, x8
   2ffa8:	sub	x8, x2, x23
   2ffac:	stp	x8, xzr, [x29, #-16]
   2ffb0:	b.hi	303f0 <__gmpn_sqrtrem@@Base+0xb38>  // b.pmore
   2ffb4:	add	x9, x1, #0xf
   2ffb8:	mov	x8, sp
   2ffbc:	and	x9, x9, #0xfffffffffffffff0
   2ffc0:	sub	x25, x8, x9
   2ffc4:	mov	sp, x25
   2ffc8:	add	x8, x25, x19, lsl #3
   2ffcc:	add	x22, x8, #0x8
   2ffd0:	stur	w4, [x29, #-28]
   2ffd4:	stur	x24, [x29, #-56]
   2ffd8:	cbz	w27, 3001c <__gmpn_sqrtrem@@Base+0x764>
   2ffdc:	ldur	x28, [x29, #-16]
   2ffe0:	add	w8, w4, #0x1
   2ffe4:	mov	x9, #0xfffffffffffffff8    	// #-8
   2ffe8:	cmp	x23, x8
   2ffec:	add	x10, x24, x23, lsl #3
   2fff0:	csel	x8, x9, xzr, gt
   2fff4:	add	x11, x19, x28
   2fff8:	add	x0, x22, x8
   2fffc:	add	x8, x10, x8
   30000:	cinc	x9, x11, gt
   30004:	sub	x8, x8, w4, uxtw #3
   30008:	add	x2, x9, #0x1
   3000c:	sub	x1, x8, #0x8
   30010:	lsl	w3, w27, #1
   30014:	bl	c180 <__gmpn_lshift@plt>
   30018:	b	3003c <__gmpn_sqrtrem@@Base+0x784>
   3001c:	ldur	x28, [x29, #-16]
   30020:	add	x8, x24, x23, lsl #3
   30024:	sub	x8, x8, w4, uxtw #3
   30028:	sub	x1, x8, #0x8
   3002c:	add	x9, x19, x28
   30030:	add	x2, x9, #0x1
   30034:	mov	x0, x22
   30038:	bl	ca50 <__gmpn_copyi@plt>
   3003c:	lsl	x8, x23, #3
   30040:	add	x24, x20, x8
   30044:	stur	x8, [x29, #-48]
   30048:	add	x8, x22, x8
   3004c:	mov	x26, x20
   30050:	add	x20, x8, #0x8
   30054:	mov	x0, x24
   30058:	mov	x1, x20
   3005c:	mov	x2, x28
   30060:	mov	x3, xzr
   30064:	mov	x4, x25
   30068:	stur	x22, [x29, #-24]
   3006c:	bl	30408 <__gmpn_sqrtrem@@Base+0xb50>
   30070:	mov	x22, x0
   30074:	cbz	x0, 3008c <__gmpn_sqrtrem@@Base+0x7d4>
   30078:	mov	x0, x20
   3007c:	mov	x1, x20
   30080:	mov	x2, x24
   30084:	mov	x3, x28
   30088:	bl	c2d0 <__gmpn_sub_n@plt>
   3008c:	ldur	x1, [x29, #-24]
   30090:	add	x2, x19, #0x1
   30094:	mov	x3, x24
   30098:	mov	x4, x28
   3009c:	add	x8, x1, x19, lsl #3
   300a0:	add	x20, x8, #0x8
   300a4:	mov	x0, x20
   300a8:	mov	x5, x25
   300ac:	stur	x25, [x29, #-40]
   300b0:	bl	30794 <__gmpn_sqrtrem@@Base+0xedc>
   300b4:	add	x21, x23, #0x1
   300b8:	ldr	x8, [x20, x21, lsl #3]
   300bc:	add	x25, x8, x22
   300c0:	cmp	x25, #0x2
   300c4:	b.cc	30104 <__gmpn_sqrtrem@@Base+0x84c>  // b.lo, b.ul, b.last
   300c8:	ldur	x2, [x29, #-48]
   300cc:	mov	w1, #0xff                  	// #255
   300d0:	mov	x0, x26
   300d4:	bl	c5f0 <memset@plt>
   300d8:	ldur	w13, [x29, #-28]
   300dc:	mov	w22, #0x1                   	// #1
   300e0:	mov	w25, w27
   300e4:	ldur	x0, [x29, #-8]
   300e8:	cbz	x0, 30168 <__gmpn_sqrtrem@@Base+0x8b0>
   300ec:	mov	w20, w13
   300f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   300f4:	mov	w13, w20
   300f8:	orr	w8, w13, w25
   300fc:	cbnz	w8, 30170 <__gmpn_sqrtrem@@Base+0x8b8>
   30100:	b	3018c <__gmpn_sqrtrem@@Base+0x8d4>
   30104:	add	x28, x20, #0x8
   30108:	mov	w3, #0x1                   	// #1
   3010c:	mov	x0, x26
   30110:	mov	x1, x28
   30114:	mov	x2, x23
   30118:	mov	w22, #0x1                   	// #1
   3011c:	bl	c1a0 <__gmpn_rshift@plt>
   30120:	add	x8, x26, x23, lsl #3
   30124:	ldur	x9, [x8, #-8]
   30128:	ldur	w13, [x29, #-28]
   3012c:	mov	w10, #0x40                  	// #64
   30130:	mvn	w11, w27
   30134:	orr	x9, x9, x25, lsl #63
   30138:	stur	x9, [x8, #-8]
   3013c:	ldp	x8, x9, [x20]
   30140:	lsr	w10, w10, w13
   30144:	add	w10, w10, w11
   30148:	mov	x11, #0xffffffffffffffff    	// #-1
   3014c:	lsr	x10, x11, x10
   30150:	and	x9, x9, x10
   30154:	mov	w25, w27
   30158:	orr	x8, x9, x8, lsr #3
   3015c:	cbz	x8, 301b0 <__gmpn_sqrtrem@@Base+0x8f8>
   30160:	ldur	x0, [x29, #-8]
   30164:	cbnz	x0, 300ec <__gmpn_sqrtrem@@Base+0x834>
   30168:	orr	w8, w13, w25
   3016c:	cbz	w8, 3018c <__gmpn_sqrtrem@@Base+0x8d4>
   30170:	cmp	w13, #0x0
   30174:	cset	w8, ne  // ne = any
   30178:	add	w3, w25, w8, lsl #5
   3017c:	mov	x0, x26
   30180:	mov	x1, x26
   30184:	mov	x2, x19
   30188:	bl	c1a0 <__gmpn_rshift@plt>
   3018c:	mov	w0, w22
   30190:	mov	sp, x29
   30194:	ldp	x20, x19, [sp, #80]
   30198:	ldp	x22, x21, [sp, #64]
   3019c:	ldp	x24, x23, [sp, #48]
   301a0:	ldp	x26, x25, [sp, #32]
   301a4:	ldp	x28, x27, [sp, #16]
   301a8:	ldp	x29, x30, [sp], #96
   301ac:	ret
   301b0:	ldur	x20, [x29, #-40]
   301b4:	ldur	x27, [x29, #-16]
   301b8:	mov	x1, x24
   301bc:	mov	x3, x28
   301c0:	mov	x0, x20
   301c4:	mov	x2, x27
   301c8:	mov	x4, x21
   301cc:	bl	ccd0 <__gmpn_mul@plt>
   301d0:	ldur	x8, [x29, #-24]
   301d4:	mov	x2, x20
   301d8:	mov	x3, x27
   301dc:	add	x22, x8, #0x8
   301e0:	mov	x0, x22
   301e4:	mov	x1, x22
   301e8:	bl	c2d0 <__gmpn_sub_n@plt>
   301ec:	lsl	x21, x27, #3
   301f0:	ldr	x8, [x22, x21]
   301f4:	lsl	x20, x19, #4
   301f8:	subs	x8, x8, x0
   301fc:	str	x8, [x22, x21]
   30200:	b.cs	30224 <__gmpn_sqrtrem@@Base+0x96c>  // b.hs, b.nlast
   30204:	ldur	x9, [x29, #-40]
   30208:	sub	x8, x20, x23, lsl #3
   3020c:	add	x8, x8, x9
   30210:	add	x8, x8, #0x18
   30214:	ldr	x9, [x8]
   30218:	sub	x10, x9, #0x1
   3021c:	str	x10, [x8], #8
   30220:	cbz	x9, 30214 <__gmpn_sqrtrem@@Base+0x95c>
   30224:	ldur	x10, [x29, #-40]
   30228:	ldur	w13, [x29, #-28]
   3022c:	ldur	x28, [x29, #-56]
   30230:	mov	x8, xzr
   30234:	add	x9, x10, x19, lsl #3
   30238:	add	x10, x10, x19, lsl #4
   3023c:	sub	x9, x9, #0x8
   30240:	add	x10, x10, #0x8
   30244:	add	x11, x23, x8
   30248:	cmp	x11, #0x1
   3024c:	b.lt	302dc <__gmpn_sqrtrem@@Base+0xa24>  // b.tstop
   30250:	lsl	x11, x8, #3
   30254:	ldr	x12, [x10, x11]
   30258:	ldr	x11, [x9, x11]
   3025c:	sub	x8, x8, #0x1
   30260:	cmp	x12, x11
   30264:	b.eq	30244 <__gmpn_sqrtrem@@Base+0x98c>  // b.none
   30268:	b.hi	302dc <__gmpn_sqrtrem@@Base+0xa24>  // b.pmore
   3026c:	ldur	x3, [x29, #-16]
   30270:	mov	x0, x22
   30274:	mov	x1, x22
   30278:	mov	x2, x24
   3027c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   30280:	ldr	x8, [x22, x21]
   30284:	adds	x8, x8, x0
   30288:	str	x8, [x22, x21]
   3028c:	b.cc	302c4 <__gmpn_sqrtrem@@Base+0xa0c>  // b.lo, b.ul, b.last
   30290:	ldur	x9, [x29, #-40]
   30294:	sub	x8, x20, x23, lsl #3
   30298:	add	x8, x9, x8
   3029c:	mov	w9, #0x3                   	// #3
   302a0:	sub	x10, x9, #0x2
   302a4:	cmp	x10, x23
   302a8:	b.ge	302c4 <__gmpn_sqrtrem@@Base+0xa0c>  // b.tcont
   302ac:	lsl	x10, x9, #3
   302b0:	ldr	x11, [x8, x10]
   302b4:	add	x9, x9, #0x1
   302b8:	adds	x11, x11, #0x1
   302bc:	str	x11, [x8, x10]
   302c0:	b.cs	302a0 <__gmpn_sqrtrem@@Base+0x9e8>  // b.hs, b.nlast
   302c4:	ldur	w13, [x29, #-28]
   302c8:	mov	x8, x26
   302cc:	ldr	x9, [x8]
   302d0:	sub	x10, x9, #0x1
   302d4:	str	x10, [x8], #8
   302d8:	cbz	x9, 302cc <__gmpn_sqrtrem@@Base+0xa14>
   302dc:	ldur	x10, [x29, #-40]
   302e0:	ldp	x21, x8, [x29, #-24]
   302e4:	sub	x9, x20, x23, lsl #3
   302e8:	add	x9, x9, x10
   302ec:	sub	x8, x8, x23
   302f0:	add	x9, x9, #0x8
   302f4:	ldr	x10, [x9]
   302f8:	cbnz	x10, 303d0 <__gmpn_sqrtrem@@Base+0xb18>
   302fc:	sub	x8, x8, #0x1
   30300:	sub	x9, x9, #0x8
   30304:	cbnz	x8, 302f4 <__gmpn_sqrtrem@@Base+0xa3c>
   30308:	ldur	x20, [x29, #-40]
   3030c:	mov	x1, x26
   30310:	mov	x2, x23
   30314:	mov	x0, x20
   30318:	bl	c8e0 <__gmpn_sqr@plt>
   3031c:	ldur	x10, [x29, #-48]
   30320:	ldur	w13, [x29, #-28]
   30324:	add	x9, x20, x23, lsl #4
   30328:	mov	x8, xzr
   3032c:	add	x10, x10, x19, lsl #3
   30330:	add	x10, x10, x20
   30334:	sub	x9, x9, #0x8
   30338:	add	x10, x10, #0x8
   3033c:	add	x11, x23, x8
   30340:	cmp	x11, #0x1
   30344:	b.lt	30364 <__gmpn_sqrtrem@@Base+0xaac>  // b.tstop
   30348:	lsl	x11, x8, #3
   3034c:	ldr	x12, [x10, x11]
   30350:	ldr	x11, [x9, x11]
   30354:	sub	x8, x8, #0x1
   30358:	cmp	x12, x11
   3035c:	b.eq	3033c <__gmpn_sqrtrem@@Base+0xa84>  // b.none
   30360:	b	303b8 <__gmpn_sqrtrem@@Base+0xb00>
   30364:	cbz	w25, 30384 <__gmpn_sqrtrem@@Base+0xacc>
   30368:	lsl	w3, w25, #1
   3036c:	mov	x0, x21
   30370:	mov	x1, x28
   30374:	mov	x2, x23
   30378:	bl	c180 <__gmpn_lshift@plt>
   3037c:	ldur	w13, [x29, #-28]
   30380:	b	30388 <__gmpn_sqrtrem@@Base+0xad0>
   30384:	mov	x21, x28
   30388:	ldur	x8, [x29, #-40]
   3038c:	sub	x10, x23, w13, uxtw
   30390:	sub	x9, x21, #0x8
   30394:	add	x8, x8, x23, lsl #3
   30398:	sub	x8, x8, #0x8
   3039c:	subs	x11, x10, #0x1
   303a0:	b.lt	303e0 <__gmpn_sqrtrem@@Base+0xb28>  // b.tstop
   303a4:	ldr	x10, [x9, x10, lsl #3]
   303a8:	ldr	x12, [x8], #-8
   303ac:	cmp	x10, x12
   303b0:	mov	x10, x11
   303b4:	b.eq	3039c <__gmpn_sqrtrem@@Base+0xae4>  // b.none
   303b8:	b.hi	303d0 <__gmpn_sqrtrem@@Base+0xb18>  // b.pmore
   303bc:	mov	x8, x26
   303c0:	ldr	x9, [x8]
   303c4:	sub	x10, x9, #0x1
   303c8:	str	x10, [x8], #8
   303cc:	cbz	x9, 303c0 <__gmpn_sqrtrem@@Base+0xb08>
   303d0:	mov	w22, #0x1                   	// #1
   303d4:	ldur	x0, [x29, #-8]
   303d8:	cbz	x0, 30168 <__gmpn_sqrtrem@@Base+0x8b0>
   303dc:	b	300ec <__gmpn_sqrtrem@@Base+0x834>
   303e0:	mov	w22, wzr
   303e4:	ldur	x0, [x29, #-8]
   303e8:	cbz	x0, 30168 <__gmpn_sqrtrem@@Base+0x8b0>
   303ec:	b	300ec <__gmpn_sqrtrem@@Base+0x834>
   303f0:	sub	x0, x29, #0x8
   303f4:	mov	w22, w4
   303f8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   303fc:	mov	w4, w22
   30400:	mov	x25, x0
   30404:	b	2ffc8 <__gmpn_sqrtrem@@Base+0x710>
   30408:	sub	sp, sp, #0x90
   3040c:	cmp	x2, #0x0
   30410:	cinc	x15, x2, lt  // lt = tstop
   30414:	stp	x24, x23, [sp, #96]
   30418:	asr	x24, x15, #1
   3041c:	stp	x28, x27, [sp, #64]
   30420:	sub	x27, x2, x24
   30424:	stp	x29, x30, [sp, #48]
   30428:	stp	x26, x25, [sp, #80]
   3042c:	stp	x22, x21, [sp, #112]
   30430:	stp	x20, x19, [sp, #128]
   30434:	add	x29, sp, #0x30
   30438:	mov	x19, x2
   3043c:	mov	x21, x1
   30440:	mov	x20, x0
   30444:	add	x23, x0, x24, lsl #3
   30448:	cmp	x27, #0x1
   3044c:	add	x25, x1, x24, lsl #4
   30450:	stur	x3, [x29, #-8]
   30454:	str	x15, [sp, #24]
   30458:	b.ne	30554 <__gmpn_sqrtrem@@Base+0xc9c>  // b.any
   3045c:	ldp	x9, x8, [x25]
   30460:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   30464:	add	x10, x10, #0x3b4
   30468:	mov	x11, #0x1ffff00000000       	// #562945658454016
   3046c:	lsr	x12, x8, #55
   30470:	sub	w12, w12, #0x80
   30474:	ldrb	w10, [x10, w12, uxtw]
   30478:	lsr	x13, x8, #31
   3047c:	movk	x11, #0xfffd, lsl #16
   30480:	lsr	x14, x8, #24
   30484:	orr	x10, x10, #0x100
   30488:	mul	x13, x10, x13
   3048c:	msub	x11, x13, x10, x11
   30490:	asr	x11, x11, #16
   30494:	mul	x11, x11, x10
   30498:	asr	x11, x11, #18
   3049c:	add	x10, x11, x10, lsl #16
   304a0:	mul	x11, x10, x14
   304a4:	lsl	x13, x8, #14
   304a8:	lsr	x14, x11, #25
   304ac:	mov	x12, #0xffffff0000000000    	// #-1099511627776
   304b0:	msub	x13, x14, x14, x13
   304b4:	add	x12, x13, x12
   304b8:	asr	x12, x12, #24
   304bc:	mul	x10, x12, x10
   304c0:	add	x10, x11, x10, asr #15
   304c4:	lsr	x10, x10, #32
   304c8:	mul	x11, x10, x10
   304cc:	lsl	x12, x10, #1
   304d0:	mov	w14, #0x1                   	// #1
   304d4:	sub	x13, x8, #0x1
   304d8:	add	x12, x11, x12
   304dc:	bfi	x14, x10, #1, #32
   304e0:	cmp	x12, x13
   304e4:	sub	x8, x8, x11
   304e8:	cinc	x11, x10, ls  // ls = plast
   304ec:	csneg	x10, xzr, x14, hi  // hi = pmore
   304f0:	add	x8, x10, x8
   304f4:	str	x8, [x25]
   304f8:	extr	x8, x8, x9, #33
   304fc:	udiv	x10, x8, x11
   30500:	sub	x12, x10, x10, lsr #32
   30504:	msub	x8, x11, x12, x8
   30508:	lsr	x13, x8, #31
   3050c:	bfi	x9, x8, #33, #31
   30510:	mul	x8, x12, x12
   30514:	subs	x10, x9, x8
   30518:	cset	w8, cc  // cc = lo, ul, last
   3051c:	subs	w9, w13, w8
   30520:	orr	x8, x12, x11, lsl #32
   30524:	mov	x22, x4
   30528:	b.pl	30540 <__gmpn_sqrtrem@@Base+0xc88>  // b.nfrst
   3052c:	adds	x10, x10, x8
   30530:	sub	x8, x8, #0x1
   30534:	cinc	w9, w9, cs  // cs = hs, nlast
   30538:	adds	x10, x10, x8
   3053c:	cinc	w9, w9, cs  // cs = hs, nlast
   30540:	str	x10, [x25]
   30544:	str	x8, [x23]
   30548:	sxtw	x26, w9
   3054c:	cbnz	x26, 30574 <__gmpn_sqrtrem@@Base+0xcbc>
   30550:	b	30588 <__gmpn_sqrtrem@@Base+0xcd0>
   30554:	mov	x0, x23
   30558:	mov	x1, x25
   3055c:	mov	x2, x27
   30560:	mov	x3, xzr
   30564:	mov	x22, x4
   30568:	bl	30408 <__gmpn_sqrtrem@@Base+0xb50>
   3056c:	mov	x26, x0
   30570:	cbz	x26, 30588 <__gmpn_sqrtrem@@Base+0xcd0>
   30574:	mov	x0, x25
   30578:	mov	x1, x25
   3057c:	mov	x2, x23
   30580:	mov	x3, x27
   30584:	bl	c2d0 <__gmpn_sub_n@plt>
   30588:	mov	x6, x27
   3058c:	lsl	x27, x24, #3
   30590:	add	x28, x21, x27
   30594:	mov	x0, x22
   30598:	mov	x1, x28
   3059c:	mov	x2, xzr
   305a0:	mov	x3, x28
   305a4:	mov	x4, x19
   305a8:	mov	x5, x23
   305ac:	str	x25, [sp, #8]
   305b0:	stur	x19, [x29, #-16]
   305b4:	str	x6, [sp, #16]
   305b8:	bl	bf00 <__gmpn_tdiv_qr@plt>
   305bc:	ldr	x8, [x22, x27]
   305c0:	ldr	x25, [x22]
   305c4:	mov	w3, #0x1                   	// #1
   305c8:	mov	x0, x20
   305cc:	mov	x1, x22
   305d0:	mov	x2, x24
   305d4:	add	x19, x8, x26
   305d8:	mov	w26, #0x1                   	// #1
   305dc:	bl	c1a0 <__gmpn_rshift@plt>
   305e0:	add	x8, x27, x20
   305e4:	ldur	x9, [x8, #-8]
   305e8:	orr	x9, x9, x19, lsl #63
   305ec:	stur	x9, [x8, #-8]
   305f0:	ldr	x8, [x20]
   305f4:	ldur	x9, [x29, #-8]
   305f8:	tst	x8, x9
   305fc:	b.ne	30770 <__gmpn_sqrtrem@@Base+0xeb8>  // b.any
   30600:	ldr	x8, [sp, #24]
   30604:	and	x26, x8, #0xfffffffffffffffe
   30608:	lsr	x8, x19, #1
   3060c:	stur	x8, [x29, #-8]
   30610:	tbnz	w25, #0, 30624 <__gmpn_sqrtrem@@Base+0xd6c>
   30614:	ldur	x19, [x29, #-16]
   30618:	ldr	x22, [sp, #16]
   3061c:	mov	x28, xzr
   30620:	b	30644 <__gmpn_sqrtrem@@Base+0xd8c>
   30624:	ldr	x22, [sp, #16]
   30628:	mov	x0, x28
   3062c:	mov	x1, x28
   30630:	mov	x2, x23
   30634:	mov	x3, x22
   30638:	bl	ca70 <__gmpn_add_n@plt>
   3063c:	ldur	x19, [x29, #-16]
   30640:	sxtw	x28, w0
   30644:	add	x27, x21, x19, lsl #3
   30648:	mov	x0, x27
   3064c:	mov	x1, x20
   30650:	mov	x2, x24
   30654:	bl	c8e0 <__gmpn_sqr@plt>
   30658:	mov	x0, x21
   3065c:	mov	x1, x21
   30660:	mov	x2, x27
   30664:	mov	x3, x26
   30668:	bl	c2d0 <__gmpn_sub_n@plt>
   3066c:	ldur	x11, [x29, #-8]
   30670:	cmp	x24, x22
   30674:	add	w8, w0, w11
   30678:	sxtw	x8, w8
   3067c:	b.eq	30694 <__gmpn_sqrtrem@@Base+0xddc>  // b.none
   30680:	ldr	x10, [sp, #8]
   30684:	ldr	x9, [x10]
   30688:	subs	x8, x9, x8
   3068c:	str	x8, [x10]
   30690:	cset	w8, cc  // cc = lo, ul, last
   30694:	sub	x24, x28, x8
   30698:	tbz	w24, #31, 3076c <__gmpn_sqrtrem@@Base+0xeb4>
   3069c:	ldr	x8, [x23]
   306a0:	adds	x8, x8, x11
   306a4:	str	x8, [x23]
   306a8:	b.cc	306d4 <__gmpn_sqrtrem@@Base+0xe1c>  // b.lo, b.ul, b.last
   306ac:	mov	w8, #0x1                   	// #1
   306b0:	mov	w25, #0x2                   	// #2
   306b4:	cmp	x8, x22
   306b8:	b.ge	306d8 <__gmpn_sqrtrem@@Base+0xe20>  // b.tcont
   306bc:	lsl	x9, x8, #3
   306c0:	ldr	x10, [x23, x9]
   306c4:	add	x8, x8, #0x1
   306c8:	adds	x10, x10, #0x1
   306cc:	str	x10, [x23, x9]
   306d0:	b.cs	306b4 <__gmpn_sqrtrem@@Base+0xdfc>  // b.hs, b.nlast
   306d4:	mov	x25, xzr
   306d8:	mov	x0, x21
   306dc:	mov	x1, x21
   306e0:	mov	x2, x20
   306e4:	mov	x3, x19
   306e8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   306ec:	ldr	x9, [x21]
   306f0:	add	x8, x25, x24
   306f4:	add	x8, x8, x0
   306f8:	sub	x10, x9, #0x1
   306fc:	str	x10, [x21]
   30700:	cbnz	x9, 3072c <__gmpn_sqrtrem@@Base+0xe74>
   30704:	mov	w9, #0x1                   	// #1
   30708:	mov	w10, #0x1                   	// #1
   3070c:	cmp	x10, x19
   30710:	b.ge	30730 <__gmpn_sqrtrem@@Base+0xe78>  // b.tcont
   30714:	lsl	x11, x10, #3
   30718:	ldr	x12, [x21, x11]
   3071c:	add	x10, x10, #0x1
   30720:	sub	x13, x12, #0x1
   30724:	str	x13, [x21, x11]
   30728:	cbz	x12, 3070c <__gmpn_sqrtrem@@Base+0xe54>
   3072c:	mov	x9, xzr
   30730:	ldr	x10, [x20]
   30734:	sxtw	x8, w8
   30738:	sub	x24, x8, x9
   3073c:	sub	x8, x10, #0x1
   30740:	str	x8, [x20]
   30744:	cbnz	x10, 3076c <__gmpn_sqrtrem@@Base+0xeb4>
   30748:	mov	w8, #0x1                   	// #1
   3074c:	cmp	x8, x19
   30750:	b.ge	3076c <__gmpn_sqrtrem@@Base+0xeb4>  // b.tcont
   30754:	lsl	x9, x8, #3
   30758:	ldr	x10, [x20, x9]
   3075c:	add	x8, x8, #0x1
   30760:	sub	x11, x10, #0x1
   30764:	str	x11, [x20, x9]
   30768:	cbz	x10, 3074c <__gmpn_sqrtrem@@Base+0xe94>
   3076c:	sxtw	x26, w24
   30770:	mov	x0, x26
   30774:	ldp	x20, x19, [sp, #128]
   30778:	ldp	x22, x21, [sp, #112]
   3077c:	ldp	x24, x23, [sp, #96]
   30780:	ldp	x26, x25, [sp, #80]
   30784:	ldp	x28, x27, [sp, #64]
   30788:	ldp	x29, x30, [sp, #48]
   3078c:	add	sp, sp, #0x90
   30790:	ret
   30794:	stp	x29, x30, [sp, #-80]!
   30798:	stp	x26, x25, [sp, #16]
   3079c:	stp	x24, x23, [sp, #32]
   307a0:	stp	x22, x21, [sp, #48]
   307a4:	stp	x20, x19, [sp, #64]
   307a8:	mov	x29, sp
   307ac:	sub	sp, sp, #0x10
   307b0:	mov	x21, x0
   307b4:	mov	x0, x5
   307b8:	mov	x24, x5
   307bc:	mov	x19, x4
   307c0:	mov	x22, x3
   307c4:	mov	x20, x2
   307c8:	mov	x23, x1
   307cc:	bl	ca50 <__gmpn_copyi@plt>
   307d0:	add	x26, x22, x19, lsl #3
   307d4:	ldur	x25, [x26, #-8]
   307d8:	mov	x0, x25
   307dc:	bl	d3f0 <__gmpn_invert_limb@plt>
   307e0:	ldur	x8, [x26, #-16]
   307e4:	mul	x9, x0, x25
   307e8:	adds	x9, x9, x8
   307ec:	b.cc	30808 <__gmpn_sqrtrem@@Base+0xf50>  // b.lo, b.ul, b.last
   307f0:	subs	x9, x9, x25
   307f4:	cset	w10, cs  // cs = hs, nlast
   307f8:	csel	x11, x25, xzr, cs  // cs = hs, nlast
   307fc:	mvn	x10, x10
   30800:	add	x0, x10, x0
   30804:	sub	x9, x9, x11
   30808:	umulh	x10, x8, x0
   3080c:	adds	x9, x10, x9
   30810:	b.cc	30838 <__gmpn_sqrtrem@@Base+0xf80>  // b.lo, b.ul, b.last
   30814:	cmp	x9, x25
   30818:	sub	x5, x0, #0x1
   3081c:	b.cc	3083c <__gmpn_sqrtrem@@Base+0xf84>  // b.lo, b.ul, b.last
   30820:	mul	x10, x0, x8
   30824:	cmp	x9, x25
   30828:	sub	x11, x0, #0x2
   3082c:	ccmp	x10, x8, #0x2, ls  // ls = plast
   30830:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   30834:	b	3083c <__gmpn_sqrtrem@@Base+0xf84>
   30838:	mov	x5, x0
   3083c:	cmp	x19, #0x97
   30840:	stur	x5, [x29, #-8]
   30844:	b.le	308b8 <__gmpn_sqrtrem@@Base+0x1000>
   30848:	cmp	x19, #0x3e5
   3084c:	b.le	308d4 <__gmpn_sqrtrem@@Base+0x101c>
   30850:	mov	x0, x20
   30854:	mov	x1, x19
   30858:	mov	w2, wzr
   3085c:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   30860:	lsl	x1, x0, #3
   30864:	mov	w8, #0x7f00                	// #32512
   30868:	cmp	x1, x8
   3086c:	stur	xzr, [x29, #-16]
   30870:	b.hi	30918 <__gmpn_sqrtrem@@Base+0x1060>  // b.pmore
   30874:	add	x9, x1, #0xf
   30878:	mov	x8, sp
   3087c:	and	x9, x9, #0xfffffffffffffff0
   30880:	sub	x5, x8, x9
   30884:	mov	sp, x5
   30888:	mov	x0, x21
   3088c:	mov	x1, x23
   30890:	mov	x2, x20
   30894:	mov	x3, x22
   30898:	mov	x4, x19
   3089c:	bl	c710 <__gmpn_mu_divappr_q@plt>
   308a0:	ldur	x8, [x29, #-16]
   308a4:	mov	x22, x0
   308a8:	cbz	x8, 308f4 <__gmpn_sqrtrem@@Base+0x103c>
   308ac:	mov	x0, x8
   308b0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   308b4:	b	308f4 <__gmpn_sqrtrem@@Base+0x103c>
   308b8:	mov	x0, x21
   308bc:	mov	x1, x24
   308c0:	mov	x2, x20
   308c4:	mov	x3, x22
   308c8:	mov	x4, x19
   308cc:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   308d0:	b	308f0 <__gmpn_sqrtrem@@Base+0x1038>
   308d4:	sub	x5, x29, #0x8
   308d8:	mov	x0, x21
   308dc:	mov	x1, x24
   308e0:	mov	x2, x20
   308e4:	mov	x3, x22
   308e8:	mov	x4, x19
   308ec:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   308f0:	mov	x22, x0
   308f4:	sub	x8, x20, x19
   308f8:	str	x22, [x21, x8, lsl #3]
   308fc:	mov	sp, x29
   30900:	ldp	x20, x19, [sp, #64]
   30904:	ldp	x22, x21, [sp, #48]
   30908:	ldp	x24, x23, [sp, #32]
   3090c:	ldp	x26, x25, [sp, #16]
   30910:	ldp	x29, x30, [sp], #80
   30914:	ret
   30918:	sub	x0, x29, #0x10
   3091c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   30920:	mov	x5, x0
   30924:	b	30888 <__gmpn_sqrtrem@@Base+0xfd0>

0000000000030928 <__gmpn_sizeinbase@@Base>:
   30928:	cbz	x1, 30974 <__gmpn_sizeinbase@@Base+0x4c>
   3092c:	add	x8, x0, x1, lsl #3
   30930:	ldur	x8, [x8, #-8]
   30934:	adrp	x11, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   30938:	ldr	x11, [x11, #3936]
   3093c:	lsl	x9, x1, #6
   30940:	sub	w10, w2, #0x1
   30944:	clz	x8, x8
   30948:	tst	w2, w10
   3094c:	sub	x8, x9, x8
   30950:	sxtw	x9, w2
   30954:	mov	w10, #0x28                  	// #40
   30958:	madd	x9, x9, x10, x11
   3095c:	b.ne	3097c <__gmpn_sizeinbase@@Base+0x54>  // b.any
   30960:	ldrsw	x9, [x9, #24]
   30964:	add	x8, x8, x9
   30968:	sub	x8, x8, #0x1
   3096c:	udiv	x0, x8, x9
   30970:	ret
   30974:	mov	w0, #0x1                   	// #1
   30978:	ret
   3097c:	ldr	x9, [x9, #8]
   30980:	add	x9, x9, #0x1
   30984:	umulh	x8, x9, x8
   30988:	add	x0, x8, #0x1
   3098c:	ret

0000000000030990 <__gmpn_get_str@@Base>:
   30990:	stp	x29, x30, [sp, #-80]!
   30994:	stp	x28, x25, [sp, #16]
   30998:	stp	x24, x23, [sp, #32]
   3099c:	stp	x22, x21, [sp, #48]
   309a0:	stp	x20, x19, [sp, #64]
   309a4:	mov	x29, sp
   309a8:	sub	sp, sp, #0xa10
   309ac:	mov	x19, x0
   309b0:	cbz	x3, 30a74 <__gmpn_get_str@@Base+0xe4>
   309b4:	sub	w8, w1, #0x1
   309b8:	mov	x21, x3
   309bc:	mov	x20, x2
   309c0:	mov	w22, w1
   309c4:	tst	w1, w8
   309c8:	b.ne	30a80 <__gmpn_get_str@@Base+0xf0>  // b.any
   309cc:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   309d0:	ldr	x9, [x9, #3936]
   309d4:	mov	w11, #0x28                  	// #40
   309d8:	sub	x8, x21, #0x1
   309dc:	ldr	x10, [x20, x8, lsl #3]
   309e0:	smaddl	x9, w22, w11, x9
   309e4:	ldr	x9, [x9, #24]
   309e8:	lsl	x12, x21, #6
   309ec:	clz	x13, x10
   309f0:	sub	x12, x12, x13
   309f4:	sxtw	x13, w9
   309f8:	udiv	x13, x12, x13
   309fc:	msub	w13, w13, w9, w12
   30a00:	mov	w11, #0xffffffff            	// #-1
   30a04:	cmp	w13, #0x0
   30a08:	sub	w13, w9, w13
   30a0c:	lsl	w11, w11, w9
   30a10:	sub	w12, w12, w8, lsl #6
   30a14:	csel	w13, wzr, w13, eq  // eq = none
   30a18:	add	w14, w12, w13
   30a1c:	eor	w12, w11, #0xff
   30a20:	mov	x11, x19
   30a24:	subs	w13, w14, w9
   30a28:	b.mi	30a40 <__gmpn_get_str@@Base+0xb0>  // b.first
   30a2c:	lsr	x14, x10, x13
   30a30:	and	w14, w14, w12
   30a34:	subs	w13, w13, w9
   30a38:	strb	w14, [x11], #1
   30a3c:	b.pl	30a2c <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   30a40:	subs	x8, x8, #0x1
   30a44:	b.lt	30b1c <__gmpn_get_str@@Base+0x18c>  // b.tstop
   30a48:	neg	w14, w13
   30a4c:	lsl	x14, x10, x14
   30a50:	ldr	x10, [x20, x8, lsl #3]
   30a54:	and	w15, w14, w12
   30a58:	add	w14, w13, #0x40
   30a5c:	lsr	x13, x10, x13
   30a60:	orr	w13, w13, w15
   30a64:	strb	w13, [x11], #1
   30a68:	subs	w13, w14, w9
   30a6c:	b.pl	30a2c <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   30a70:	b	30a40 <__gmpn_get_str@@Base+0xb0>
   30a74:	strb	wzr, [x19]
   30a78:	mov	w19, #0x1                   	// #1
   30a7c:	b	30b40 <__gmpn_get_str@@Base+0x1b0>
   30a80:	cmp	x21, #0x1c
   30a84:	b.le	30b24 <__gmpn_get_str@@Base+0x194>
   30a88:	lsl	x23, x21, #3
   30a8c:	add	x1, x23, #0x400
   30a90:	add	x0, sp, #0x8
   30a94:	str	xzr, [sp, #8]
   30a98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   30a9c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   30aa0:	ldr	x8, [x8, #3936]
   30aa4:	mov	w24, #0x28                  	// #40
   30aa8:	lsl	x10, x21, #6
   30aac:	mov	x1, x0
   30ab0:	smaddl	x8, w22, w24, x8
   30ab4:	ldr	x9, [x8, #8]
   30ab8:	ldrsw	x8, [x8]
   30abc:	umulh	x9, x9, x10
   30ac0:	add	x0, sp, #0x10
   30ac4:	mov	w3, w22
   30ac8:	udiv	x8, x9, x8
   30acc:	add	x2, x8, #0x1
   30ad0:	add	x25, sp, #0x10
   30ad4:	bl	cb10 <__gmpn_compute_powtab@plt>
   30ad8:	mov	x22, x0
   30adc:	add	x1, x23, #0x200
   30ae0:	add	x0, sp, #0x8
   30ae4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   30ae8:	mov	x5, x0
   30aec:	smaddl	x4, w22, w24, x25
   30af0:	mov	x0, x19
   30af4:	mov	x1, xzr
   30af8:	mov	x2, x20
   30afc:	mov	x3, x21
   30b00:	bl	30f94 <__gmpn_get_str@@Base+0x604>
   30b04:	ldr	x8, [sp, #8]
   30b08:	sub	x19, x0, x19
   30b0c:	cbz	x8, 30b40 <__gmpn_get_str@@Base+0x1b0>
   30b10:	mov	x0, x8
   30b14:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   30b18:	b	30b40 <__gmpn_get_str@@Base+0x1b0>
   30b1c:	sub	x19, x11, x19
   30b20:	b	30b40 <__gmpn_get_str@@Base+0x1b0>
   30b24:	mov	x0, x19
   30b28:	mov	x1, xzr
   30b2c:	mov	x2, x20
   30b30:	mov	x3, x21
   30b34:	mov	w4, w22
   30b38:	bl	30b60 <__gmpn_get_str@@Base+0x1d0>
   30b3c:	sub	x19, x0, x19
   30b40:	mov	x0, x19
   30b44:	add	sp, sp, #0xa10
   30b48:	ldp	x20, x19, [sp, #64]
   30b4c:	ldp	x22, x21, [sp, #48]
   30b50:	ldp	x24, x23, [sp, #32]
   30b54:	ldp	x28, x25, [sp, #16]
   30b58:	ldp	x29, x30, [sp], #80
   30b5c:	ret
   30b60:	stp	x29, x30, [sp, #-96]!
   30b64:	stp	x28, x27, [sp, #16]
   30b68:	stp	x26, x25, [sp, #32]
   30b6c:	stp	x24, x23, [sp, #48]
   30b70:	stp	x22, x21, [sp, #64]
   30b74:	stp	x20, x19, [sp, #80]
   30b78:	mov	x29, sp
   30b7c:	sub	sp, sp, #0x5a0
   30b80:	mov	x21, x3
   30b84:	mov	x20, x1
   30b88:	cmp	w4, #0xa
   30b8c:	mov	x19, x0
   30b90:	b.ne	30db0 <__gmpn_get_str@@Base+0x420>  // b.any
   30b94:	add	x23, sp, #0x10
   30b98:	add	x22, x23, #0x8
   30b9c:	mov	x0, x22
   30ba0:	mov	x1, x2
   30ba4:	mov	x2, x21
   30ba8:	bl	ca50 <__gmpn_copyi@plt>
   30bac:	add	x8, sp, #0xf8
   30bb0:	cmp	x21, #0x2
   30bb4:	add	x26, x8, #0x49d
   30bb8:	b.lt	30d7c <__gmpn_get_str@@Base+0x3ec>  // b.tstop
   30bbc:	mov	w24, #0xa                   	// #10
   30bc0:	mov	w25, #0x64                  	// #100
   30bc4:	mov	w27, #0x3e8                 	// #1000
   30bc8:	mov	w28, #0x2710                	// #10000
   30bcc:	mov	x5, #0xc34a                	// #49994
   30bd0:	mov	x4, #0x89e80000            	// #2313682944
   30bd4:	movk	x5, #0x6d2a, lsl #16
   30bd8:	movk	x4, #0x2304, lsl #32
   30bdc:	movk	x5, #0x94fb, lsl #32
   30be0:	add	x0, sp, #0x10
   30be4:	mov	w1, #0x1                   	// #1
   30be8:	movk	x4, #0x8ac7, lsl #48
   30bec:	movk	x5, #0xd83c, lsl #48
   30bf0:	mov	x2, x22
   30bf4:	mov	x3, x21
   30bf8:	mov	w6, wzr
   30bfc:	bl	cce0 <__gmpn_preinv_divrem_1@plt>
   30c00:	ldr	x8, [x23, x21, lsl #3]
   30c04:	ldr	x9, [sp, #16]
   30c08:	cmp	x8, #0x0
   30c0c:	add	x8, x9, #0x1
   30c10:	umulh	x11, x8, x24
   30c14:	cset	w9, eq  // eq = none
   30c18:	strb	w11, [x26, #-19]!
   30c1c:	mul	x11, x8, x25
   30c20:	add	x10, x8, x8, lsl #2
   30c24:	sub	x21, x21, x9
   30c28:	mul	x9, x8, x27
   30c2c:	mul	x8, x8, x28
   30c30:	umulh	x11, x11, x24
   30c34:	strb	w11, [x26, #2]
   30c38:	lsr	x11, x8, #2
   30c3c:	umulh	x9, x9, x24
   30c40:	add	x8, x11, x8, lsr #4
   30c44:	strb	w9, [x26, #3]
   30c48:	lsl	x9, x8, #1
   30c4c:	ubfx	x8, x8, #59, #4
   30c50:	strb	w8, [x26, #4]
   30c54:	and	x8, x9, #0xffffffffffffffe
   30c58:	add	x8, x8, x8, lsl #2
   30c5c:	lsl	x9, x8, #1
   30c60:	ubfx	x8, x8, #59, #4
   30c64:	strb	w8, [x26, #5]
   30c68:	and	x8, x9, #0xffffffffffffffc
   30c6c:	add	x8, x8, x8, lsl #2
   30c70:	lsl	x9, x8, #1
   30c74:	ubfx	x8, x8, #59, #4
   30c78:	strb	w8, [x26, #6]
   30c7c:	and	x8, x9, #0xffffffffffffff8
   30c80:	add	x8, x8, x8, lsl #2
   30c84:	lsl	x9, x8, #1
   30c88:	ubfx	x8, x8, #59, #4
   30c8c:	strb	w8, [x26, #7]
   30c90:	and	x8, x9, #0xffffffffffffff0
   30c94:	add	x8, x8, x8, lsl #2
   30c98:	lsl	x9, x8, #1
   30c9c:	ubfx	x8, x8, #59, #4
   30ca0:	strb	w8, [x26, #8]
   30ca4:	and	x8, x9, #0xfffffffffffffe0
   30ca8:	add	x8, x8, x8, lsl #2
   30cac:	lsl	x9, x8, #1
   30cb0:	ubfx	x8, x8, #59, #4
   30cb4:	strb	w8, [x26, #9]
   30cb8:	and	x8, x9, #0xfffffffffffffc0
   30cbc:	add	x8, x8, x8, lsl #2
   30cc0:	lsl	x9, x8, #1
   30cc4:	ubfx	x8, x8, #59, #4
   30cc8:	strb	w8, [x26, #10]
   30ccc:	and	x8, x9, #0xfffffffffffff80
   30cd0:	add	x8, x8, x8, lsl #2
   30cd4:	lsl	x9, x8, #1
   30cd8:	ubfx	x8, x8, #59, #4
   30cdc:	strb	w8, [x26, #11]
   30ce0:	and	x8, x9, #0xfffffffffffff00
   30ce4:	add	x8, x8, x8, lsl #2
   30ce8:	lsl	x9, x8, #1
   30cec:	ubfx	x8, x8, #59, #4
   30cf0:	strb	w8, [x26, #12]
   30cf4:	and	x8, x9, #0xffffffffffffe00
   30cf8:	add	x8, x8, x8, lsl #2
   30cfc:	lsl	x9, x8, #1
   30d00:	ubfx	x8, x8, #59, #4
   30d04:	strb	w8, [x26, #13]
   30d08:	and	x8, x9, #0xffffffffffffc00
   30d0c:	add	x8, x8, x8, lsl #2
   30d10:	lsl	x9, x8, #1
   30d14:	ubfx	x8, x8, #59, #4
   30d18:	strb	w8, [x26, #14]
   30d1c:	and	x8, x9, #0xffffffffffff800
   30d20:	add	x8, x8, x8, lsl #2
   30d24:	lsl	x9, x8, #1
   30d28:	ubfx	x8, x8, #59, #4
   30d2c:	strb	w8, [x26, #15]
   30d30:	and	x8, x9, #0xffffffffffff000
   30d34:	add	x8, x8, x8, lsl #2
   30d38:	lsl	x9, x8, #1
   30d3c:	ubfx	x8, x8, #59, #4
   30d40:	strb	w8, [x26, #16]
   30d44:	and	x8, x9, #0xfffffffffffe000
   30d48:	add	x8, x8, x8, lsl #2
   30d4c:	lsl	x9, x8, #1
   30d50:	ubfx	x8, x8, #59, #4
   30d54:	strb	w8, [x26, #17]
   30d58:	and	x8, x9, #0xfffffffffffc000
   30d5c:	add	x8, x8, x8, lsl #2
   30d60:	lsl	x10, x10, #1
   30d64:	ubfx	x8, x8, #59, #4
   30d68:	cmp	x21, #0x1
   30d6c:	umulh	x10, x10, x24
   30d70:	strb	w10, [x26, #1]
   30d74:	strb	w8, [x26, #18]
   30d78:	b.gt	30bcc <__gmpn_get_str@@Base+0x23c>
   30d7c:	ldr	x8, [sp, #24]
   30d80:	cbz	x8, 30e90 <__gmpn_get_str@@Base+0x500>
   30d84:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   30d88:	movk	x9, #0xcccd
   30d8c:	mov	w10, #0xfffffff6            	// #-10
   30d90:	umulh	x11, x8, x9
   30d94:	lsr	x11, x11, #3
   30d98:	madd	w12, w11, w10, w8
   30d9c:	cmp	x8, #0xa
   30da0:	strb	w12, [x26, #-1]!
   30da4:	mov	x8, x11
   30da8:	b.cs	30d90 <__gmpn_get_str@@Base+0x400>  // b.hs, b.nlast
   30dac:	b	30e90 <__gmpn_get_str@@Base+0x500>
   30db0:	add	x8, sp, #0x10
   30db4:	add	x22, x8, #0x8
   30db8:	mov	w23, w4
   30dbc:	mov	x0, x22
   30dc0:	mov	x1, x2
   30dc4:	mov	x2, x21
   30dc8:	sxtw	x27, w23
   30dcc:	bl	ca50 <__gmpn_copyi@plt>
   30dd0:	add	x8, sp, #0xf8
   30dd4:	cmp	x21, #0x2
   30dd8:	add	x26, x8, #0x49d
   30ddc:	b.lt	30e70 <__gmpn_get_str@@Base+0x4e0>  // b.tstop
   30de0:	str	x22, [sp, #8]
   30de4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   30de8:	ldr	x8, [x8, #3936]
   30dec:	mov	w9, #0x28                  	// #40
   30df0:	smaddl	x8, w23, w9, x8
   30df4:	ldr	w28, [x8]
   30df8:	ldp	x23, x8, [x8, #24]
   30dfc:	neg	x22, x28
   30e00:	clz	x25, x23
   30e04:	neg	x24, x28, lsl #1
   30e08:	str	x8, [sp]
   30e0c:	ldp	x5, x2, [sp]
   30e10:	add	x0, sp, #0x10
   30e14:	mov	w1, #0x1                   	// #1
   30e18:	mov	x3, x21
   30e1c:	mov	x4, x23
   30e20:	mov	w6, w25
   30e24:	bl	cce0 <__gmpn_preinv_divrem_1@plt>
   30e28:	add	x8, sp, #0x10
   30e2c:	ldr	x9, [x8, x21, lsl #3]
   30e30:	ldr	x10, [sp, #16]
   30e34:	add	x8, x26, x22
   30e38:	add	x26, x26, x24
   30e3c:	cmp	x9, #0x0
   30e40:	add	x10, x10, #0x1
   30e44:	csetm	x9, eq  // eq = none
   30e48:	mov	w11, w28
   30e4c:	umulh	x12, x10, x27
   30e50:	mul	x10, x10, x27
   30e54:	subs	w11, w11, #0x1
   30e58:	strb	w12, [x8], #1
   30e5c:	add	x26, x26, #0x1
   30e60:	b.ne	30e4c <__gmpn_get_str@@Base+0x4bc>  // b.any
   30e64:	add	x21, x21, x9
   30e68:	cmp	x21, #0x1
   30e6c:	b.gt	30e0c <__gmpn_get_str@@Base+0x47c>
   30e70:	ldr	x8, [sp, #24]
   30e74:	cbz	x8, 30e90 <__gmpn_get_str@@Base+0x500>
   30e78:	udiv	x9, x8, x27
   30e7c:	msub	w10, w9, w27, w8
   30e80:	cmp	x8, x27
   30e84:	strb	w10, [x26, #-1]!
   30e88:	mov	x8, x9
   30e8c:	b.cs	30e78 <__gmpn_get_str@@Base+0x4e8>  // b.hs, b.nlast
   30e90:	add	x8, sp, #0xf8
   30e94:	add	x23, x8, #0x49d
   30e98:	sub	x22, x23, x26
   30e9c:	cmp	x22, x20
   30ea0:	b.cs	30ef4 <__gmpn_get_str@@Base+0x564>  // b.hs, b.nlast
   30ea4:	add	x8, x26, x20
   30ea8:	sub	x21, x8, x23
   30eac:	mov	x0, x19
   30eb0:	mov	w1, wzr
   30eb4:	mov	x2, x21
   30eb8:	bl	c5f0 <memset@plt>
   30ebc:	cmp	x21, #0x1
   30ec0:	b.ls	30ee4 <__gmpn_get_str@@Base+0x554>  // b.plast
   30ec4:	and	x8, x21, #0xfffffffffffffffe
   30ec8:	add	x19, x19, x8
   30ecc:	sub	x20, x20, x8
   30ed0:	mov	x9, x8
   30ed4:	subs	x9, x9, #0x2
   30ed8:	b.ne	30ed4 <__gmpn_get_str@@Base+0x544>  // b.any
   30edc:	cmp	x21, x8
   30ee0:	b.eq	30ef4 <__gmpn_get_str@@Base+0x564>  // b.none
   30ee4:	sub	x20, x20, #0x1
   30ee8:	cmp	x22, x20
   30eec:	add	x19, x19, #0x1
   30ef0:	b.cc	30ee4 <__gmpn_get_str@@Base+0x554>  // b.lo, b.ul, b.last
   30ef4:	cbz	x22, 30f30 <__gmpn_get_str@@Base+0x5a0>
   30ef8:	cmp	x22, #0x1f
   30efc:	b.ls	30f14 <__gmpn_get_str@@Base+0x584>  // b.plast
   30f00:	cmp	x19, x23
   30f04:	b.cs	30f38 <__gmpn_get_str@@Base+0x5a8>  // b.hs, b.nlast
   30f08:	add	x8, x19, x22
   30f0c:	cmp	x26, x8
   30f10:	b.cs	30f38 <__gmpn_get_str@@Base+0x5a8>  // b.hs, b.nlast
   30f14:	mov	x0, x19
   30f18:	mov	x8, x22
   30f1c:	ldrb	w9, [x26], #1
   30f20:	subs	x8, x8, #0x1
   30f24:	strb	w9, [x0], #1
   30f28:	b.ne	30f1c <__gmpn_get_str@@Base+0x58c>  // b.any
   30f2c:	b	30f74 <__gmpn_get_str@@Base+0x5e4>
   30f30:	mov	x0, x19
   30f34:	b	30f74 <__gmpn_get_str@@Base+0x5e4>
   30f38:	and	x9, x22, #0xffffffffffffffe0
   30f3c:	and	x8, x22, #0x1f
   30f40:	add	x10, x26, #0x10
   30f44:	add	x0, x19, x9
   30f48:	add	x26, x26, x9
   30f4c:	add	x11, x19, #0x10
   30f50:	mov	x12, x9
   30f54:	ldp	q0, q1, [x10, #-16]
   30f58:	add	x10, x10, #0x20
   30f5c:	subs	x12, x12, #0x20
   30f60:	stp	q0, q1, [x11, #-16]
   30f64:	add	x11, x11, #0x20
   30f68:	b.ne	30f54 <__gmpn_get_str@@Base+0x5c4>  // b.any
   30f6c:	cmp	x22, x9
   30f70:	b.ne	30f1c <__gmpn_get_str@@Base+0x58c>  // b.any
   30f74:	add	sp, sp, #0x5a0
   30f78:	ldp	x20, x19, [sp, #80]
   30f7c:	ldp	x22, x21, [sp, #64]
   30f80:	ldp	x24, x23, [sp, #48]
   30f84:	ldp	x26, x25, [sp, #32]
   30f88:	ldp	x28, x27, [sp, #16]
   30f8c:	ldp	x29, x30, [sp], #96
   30f90:	ret
   30f94:	stp	x29, x30, [sp, #-96]!
   30f98:	stp	x24, x23, [sp, #48]
   30f9c:	stp	x22, x21, [sp, #64]
   30fa0:	stp	x20, x19, [sp, #80]
   30fa4:	mov	x24, x4
   30fa8:	mov	x23, x3
   30fac:	mov	x21, x2
   30fb0:	mov	x20, x1
   30fb4:	cmp	x3, #0xf
   30fb8:	mov	x19, x0
   30fbc:	str	x27, [sp, #16]
   30fc0:	stp	x26, x25, [sp, #32]
   30fc4:	mov	x29, sp
   30fc8:	b.lt	310a0 <__gmpn_get_str@@Base+0x710>  // b.tstop
   30fcc:	mov	x22, x5
   30fd0:	sub	x26, x21, #0x8
   30fd4:	b	31008 <__gmpn_get_str@@Base+0x678>
   30fd8:	ldr	x8, [x27, #24]
   30fdc:	sub	x1, x20, x8
   30fe0:	sub	x24, x27, #0x28
   30fe4:	add	x5, x22, x3, lsl #3
   30fe8:	mov	x0, x19
   30fec:	mov	x2, x22
   30ff0:	mov	x4, x24
   30ff4:	bl	30f94 <__gmpn_get_str@@Base+0x604>
   30ff8:	ldr	x20, [x27, #24]
   30ffc:	cmp	x23, #0xe
   31000:	mov	x19, x0
   31004:	b.le	310a0 <__gmpn_get_str@@Base+0x710>
   31008:	mov	x8, x23
   3100c:	add	x10, x26, x23, lsl #3
   31010:	mov	x27, x24
   31014:	b	3101c <__gmpn_get_str@@Base+0x68c>
   31018:	sub	x27, x27, #0x28
   3101c:	ldp	x24, x9, [x27, #8]
   31020:	add	x23, x9, x24
   31024:	cmp	x23, x8
   31028:	b.gt	31018 <__gmpn_get_str@@Base+0x688>
   3102c:	ldr	x5, [x27]
   31030:	b.ne	31064 <__gmpn_get_str@@Base+0x6d4>  // b.any
   31034:	sub	x13, x8, x9
   31038:	sub	x11, x5, #0x8
   3103c:	mov	x12, x10
   31040:	subs	x14, x13, #0x1
   31044:	b.lt	31060 <__gmpn_get_str@@Base+0x6d0>  // b.tstop
   31048:	ldr	x15, [x12], #-8
   3104c:	ldr	x13, [x11, x13, lsl #3]
   31050:	cmp	x15, x13
   31054:	mov	x13, x14
   31058:	b.eq	31040 <__gmpn_get_str@@Base+0x6b0>  // b.none
   3105c:	b.ls	31018 <__gmpn_get_str@@Base+0x688>  // b.plast
   31060:	mov	x23, x8
   31064:	add	x1, x21, x9, lsl #3
   31068:	sub	x25, x8, x9
   3106c:	mov	x0, x22
   31070:	mov	x2, xzr
   31074:	mov	x3, x1
   31078:	mov	x4, x25
   3107c:	mov	x6, x24
   31080:	bl	bf00 <__gmpn_tdiv_qr@plt>
   31084:	sub	x8, x25, x24
   31088:	ldr	x9, [x22, x8, lsl #3]
   3108c:	cmp	x9, #0x0
   31090:	cinc	x3, x8, ne  // ne = any
   31094:	cbnz	x20, 30fd8 <__gmpn_get_str@@Base+0x648>
   31098:	mov	x1, xzr
   3109c:	b	30fe0 <__gmpn_get_str@@Base+0x650>
   310a0:	cbz	x23, 310d4 <__gmpn_get_str@@Base+0x744>
   310a4:	ldr	w4, [x24, #32]
   310a8:	mov	x0, x19
   310ac:	mov	x1, x20
   310b0:	mov	x2, x21
   310b4:	mov	x3, x23
   310b8:	ldp	x20, x19, [sp, #80]
   310bc:	ldp	x22, x21, [sp, #64]
   310c0:	ldp	x24, x23, [sp, #48]
   310c4:	ldp	x26, x25, [sp, #32]
   310c8:	ldr	x27, [sp, #16]
   310cc:	ldp	x29, x30, [sp], #96
   310d0:	b	30b60 <__gmpn_get_str@@Base+0x1d0>
   310d4:	cbz	x20, 31124 <__gmpn_get_str@@Base+0x794>
   310d8:	mov	x0, x19
   310dc:	mov	w1, wzr
   310e0:	mov	x2, x20
   310e4:	bl	c5f0 <memset@plt>
   310e8:	cmp	x20, #0x1
   310ec:	b.ne	310f8 <__gmpn_get_str@@Base+0x768>  // b.any
   310f0:	mov	x8, x20
   310f4:	b	31118 <__gmpn_get_str@@Base+0x788>
   310f8:	and	x9, x20, #0xfffffffffffffffe
   310fc:	add	x19, x19, x9
   31100:	and	x8, x20, #0x1
   31104:	mov	x10, x9
   31108:	subs	x10, x10, #0x2
   3110c:	b.ne	31108 <__gmpn_get_str@@Base+0x778>  // b.any
   31110:	cmp	x20, x9
   31114:	b.eq	31124 <__gmpn_get_str@@Base+0x794>  // b.none
   31118:	subs	x8, x8, #0x1
   3111c:	add	x19, x19, #0x1
   31120:	b.ne	31118 <__gmpn_get_str@@Base+0x788>  // b.any
   31124:	mov	x0, x19
   31128:	ldp	x20, x19, [sp, #80]
   3112c:	ldp	x22, x21, [sp, #64]
   31130:	ldp	x24, x23, [sp, #48]
   31134:	ldp	x26, x25, [sp, #32]
   31138:	ldr	x27, [sp, #16]
   3113c:	ldp	x29, x30, [sp], #96
   31140:	ret

0000000000031144 <__gmpn_set_str@@Base>:
   31144:	stp	x29, x30, [sp, #-96]!
   31148:	str	x28, [sp, #16]
   3114c:	stp	x26, x25, [sp, #32]
   31150:	stp	x24, x23, [sp, #48]
   31154:	stp	x22, x21, [sp, #64]
   31158:	stp	x20, x19, [sp, #80]
   3115c:	mov	x29, sp
   31160:	sub	sp, sp, #0xa00
   31164:	sub	w8, w3, #0x1
   31168:	mov	w22, w3
   3116c:	mov	x21, x2
   31170:	mov	x19, x1
   31174:	tst	w3, w8
   31178:	mov	x20, x0
   3117c:	b.ne	31198 <__gmpn_set_str@@Base+0x54>  // b.any
   31180:	add	x8, x19, x21
   31184:	sub	x8, x8, #0x1
   31188:	cmp	x8, x19
   3118c:	b.cs	3122c <__gmpn_set_str@@Base+0xe8>  // b.hs, b.nlast
   31190:	mov	x0, xzr
   31194:	b	312cc <__gmpn_set_str@@Base+0x188>
   31198:	cmp	x21, #0x717
   3119c:	b.ls	3128c <__gmpn_set_str@@Base+0x148>  // b.plast
   311a0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   311a4:	ldr	x8, [x8, #3936]
   311a8:	mov	w24, #0x28                  	// #40
   311ac:	smull	x9, w22, w24
   311b0:	add	x0, x29, #0x18
   311b4:	ldrsw	x8, [x8, x9]
   311b8:	str	xzr, [x29, #24]
   311bc:	udiv	x8, x21, x8
   311c0:	lsl	x25, x8, #3
   311c4:	add	x1, x25, #0x408
   311c8:	add	x23, x8, #0x1
   311cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   311d0:	mov	x1, x0
   311d4:	mov	x0, sp
   311d8:	mov	x2, x23
   311dc:	mov	w3, w22
   311e0:	mov	x26, sp
   311e4:	bl	cb10 <__gmpn_compute_powtab@plt>
   311e8:	madd	x22, x0, x24, x26
   311ec:	add	x1, x25, #0x208
   311f0:	add	x0, x29, #0x18
   311f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   311f8:	mov	x4, x0
   311fc:	mov	x0, x20
   31200:	mov	x1, x19
   31204:	mov	x2, x21
   31208:	mov	x3, x22
   3120c:	bl	c360 <__gmpn_dc_set_str@plt>
   31210:	ldr	x8, [x29, #24]
   31214:	cbz	x8, 312cc <__gmpn_set_str@@Base+0x188>
   31218:	mov	x19, x0
   3121c:	mov	x0, x8
   31220:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   31224:	mov	x0, x19
   31228:	b	312cc <__gmpn_set_str@@Base+0x188>
   3122c:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   31230:	ldr	x10, [x10, #3936]
   31234:	mov	w12, #0x28                  	// #40
   31238:	mov	w11, wzr
   3123c:	mov	x9, xzr
   31240:	smaddl	x10, w22, w12, x10
   31244:	ldr	w10, [x10, #24]
   31248:	mov	x0, xzr
   3124c:	b	3125c <__gmpn_set_str@@Base+0x118>
   31250:	sub	x8, x8, #0x1
   31254:	cmp	x8, x19
   31258:	b.cc	312bc <__gmpn_set_str@@Base+0x178>  // b.lo, b.ul, b.last
   3125c:	ldrb	w12, [x8]
   31260:	lsl	x14, x12, x11
   31264:	add	w11, w11, w10
   31268:	subs	w13, w11, #0x40
   3126c:	orr	x9, x14, x9
   31270:	b.lt	31250 <__gmpn_set_str@@Base+0x10c>  // b.tstop
   31274:	str	x9, [x20, x0, lsl #3]
   31278:	sub	w9, w10, w13
   3127c:	add	x0, x0, #0x1
   31280:	lsr	w9, w12, w9
   31284:	mov	w11, w13
   31288:	b	31250 <__gmpn_set_str@@Base+0x10c>
   3128c:	mov	x0, x20
   31290:	mov	x1, x19
   31294:	mov	x2, x21
   31298:	mov	w3, w22
   3129c:	add	sp, sp, #0xa00
   312a0:	ldp	x20, x19, [sp, #80]
   312a4:	ldp	x22, x21, [sp, #64]
   312a8:	ldp	x24, x23, [sp, #48]
   312ac:	ldp	x26, x25, [sp, #32]
   312b0:	ldr	x28, [sp, #16]
   312b4:	ldp	x29, x30, [sp], #96
   312b8:	b	c110 <__gmpn_bc_set_str@plt>
   312bc:	cbz	x9, 312cc <__gmpn_set_str@@Base+0x188>
   312c0:	add	x8, x0, #0x1
   312c4:	str	x9, [x20, x0, lsl #3]
   312c8:	mov	x0, x8
   312cc:	add	sp, sp, #0xa00
   312d0:	ldp	x20, x19, [sp, #80]
   312d4:	ldp	x22, x21, [sp, #64]
   312d8:	ldp	x24, x23, [sp, #48]
   312dc:	ldp	x26, x25, [sp, #32]
   312e0:	ldr	x28, [sp, #16]
   312e4:	ldp	x29, x30, [sp], #96
   312e8:	ret

00000000000312ec <__gmpn_bc_set_str@@Base>:
   312ec:	sub	sp, sp, #0x70
   312f0:	stp	x29, x30, [sp, #16]
   312f4:	stp	x28, x27, [sp, #32]
   312f8:	stp	x26, x25, [sp, #48]
   312fc:	stp	x24, x23, [sp, #64]
   31300:	stp	x22, x21, [sp, #80]
   31304:	stp	x20, x19, [sp, #96]
   31308:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   3130c:	ldr	x8, [x8, #3936]
   31310:	mov	w9, #0x28                  	// #40
   31314:	mov	x22, x1
   31318:	ldrb	w4, [x22], #1
   3131c:	smaddl	x8, w3, w9, x8
   31320:	ldrsw	x26, [x8]
   31324:	mov	w23, w3
   31328:	mov	x21, x2
   3132c:	mov	x19, x0
   31330:	cmp	x26, x2
   31334:	sxtw	x25, w23
   31338:	add	x29, sp, #0x10
   3133c:	b.cs	31444 <__gmpn_bc_set_str@@Base+0x158>  // b.hs, b.nlast
   31340:	ldr	x24, [x8, #24]
   31344:	cmp	w23, #0xa
   31348:	b.ne	31450 <__gmpn_bc_set_str@@Base+0x164>  // b.any
   3134c:	mov	x20, xzr
   31350:	mov	w28, #0xa                   	// #10
   31354:	mov	x27, x26
   31358:	b	31380 <__gmpn_bc_set_str@@Base+0x94>
   3135c:	cbz	x4, 3143c <__gmpn_bc_set_str@@Base+0x150>
   31360:	mov	w20, #0x1                   	// #1
   31364:	str	x4, [x19]
   31368:	ldrb	w4, [x22, #18]
   3136c:	add	x27, x27, x26
   31370:	add	x8, x22, #0x13
   31374:	cmp	x27, x21
   31378:	mov	x22, x8
   3137c:	b.cs	314dc <__gmpn_bc_set_str@@Base+0x1f0>  // b.hs, b.nlast
   31380:	ldrb	w8, [x22]
   31384:	ldrb	w9, [x22, #1]
   31388:	ldrb	w10, [x22, #2]
   3138c:	ldrb	w11, [x22, #3]
   31390:	madd	x8, x4, x28, x8
   31394:	ldrb	w12, [x22, #4]
   31398:	madd	x8, x8, x28, x9
   3139c:	ldrb	w9, [x22, #5]
   313a0:	madd	x8, x8, x28, x10
   313a4:	ldrb	w10, [x22, #6]
   313a8:	madd	x8, x8, x28, x11
   313ac:	ldrb	w11, [x22, #7]
   313b0:	madd	x8, x8, x28, x12
   313b4:	ldrb	w12, [x22, #8]
   313b8:	madd	x8, x8, x28, x9
   313bc:	ldrb	w9, [x22, #9]
   313c0:	madd	x8, x8, x28, x10
   313c4:	ldrb	w10, [x22, #10]
   313c8:	madd	x8, x8, x28, x11
   313cc:	ldrb	w11, [x22, #11]
   313d0:	madd	x8, x8, x28, x12
   313d4:	ldrb	w12, [x22, #12]
   313d8:	madd	x8, x8, x28, x9
   313dc:	ldrb	w9, [x22, #13]
   313e0:	madd	x8, x8, x28, x10
   313e4:	ldrb	w10, [x22, #14]
   313e8:	madd	x8, x8, x28, x11
   313ec:	ldrb	w11, [x22, #15]
   313f0:	madd	x8, x8, x28, x12
   313f4:	ldrb	w12, [x22, #16]
   313f8:	madd	x8, x8, x28, x9
   313fc:	ldrb	w9, [x22, #17]
   31400:	madd	x8, x8, x28, x10
   31404:	madd	x8, x8, x28, x11
   31408:	madd	x8, x8, x28, x12
   3140c:	madd	x4, x8, x28, x9
   31410:	cbz	x20, 3135c <__gmpn_bc_set_str@@Base+0x70>
   31414:	mov	x0, x19
   31418:	mov	x1, x19
   3141c:	mov	x2, x20
   31420:	mov	x3, x24
   31424:	bl	d240 <__gmpn_mul_1c@plt>
   31428:	cbz	x0, 31368 <__gmpn_bc_set_str@@Base+0x7c>
   3142c:	add	x8, x20, #0x1
   31430:	str	x0, [x19, x20, lsl #3]
   31434:	mov	x20, x8
   31438:	b	31368 <__gmpn_bc_set_str@@Base+0x7c>
   3143c:	mov	x20, xzr
   31440:	b	31368 <__gmpn_bc_set_str@@Base+0x7c>
   31444:	mov	x20, xzr
   31448:	mov	x27, x26
   3144c:	b	314e8 <__gmpn_bc_set_str@@Base+0x1fc>
   31450:	sub	x28, x26, #0x1
   31454:	cbz	w28, 315d4 <__gmpn_bc_set_str@@Base+0x2e8>
   31458:	stp	x23, x24, [sp]
   3145c:	mov	x20, xzr
   31460:	sub	x23, x28, #0x1
   31464:	mov	x27, x26
   31468:	b	3148c <__gmpn_bc_set_str@@Base+0x1a0>
   3146c:	cbz	x4, 314d4 <__gmpn_bc_set_str@@Base+0x1e8>
   31470:	str	x4, [x19]
   31474:	mov	w20, #0x1                   	// #1
   31478:	ldrb	w4, [x22, x28]
   3147c:	add	x27, x27, x26
   31480:	cmp	x27, x21
   31484:	add	x22, x24, #0x2
   31488:	b.cs	314e4 <__gmpn_bc_set_str@@Base+0x1f8>  // b.hs, b.nlast
   3148c:	mov	x8, xzr
   31490:	add	x24, x22, x23
   31494:	ldrb	w9, [x22, x8]
   31498:	add	x8, x8, #0x1
   3149c:	cmp	x28, x8
   314a0:	madd	x4, x4, x25, x9
   314a4:	b.ne	31494 <__gmpn_bc_set_str@@Base+0x1a8>  // b.any
   314a8:	cbz	x20, 3146c <__gmpn_bc_set_str@@Base+0x180>
   314ac:	ldr	x3, [sp, #8]
   314b0:	mov	x0, x19
   314b4:	mov	x1, x19
   314b8:	mov	x2, x20
   314bc:	bl	d240 <__gmpn_mul_1c@plt>
   314c0:	cbz	x0, 31478 <__gmpn_bc_set_str@@Base+0x18c>
   314c4:	add	x8, x20, #0x1
   314c8:	str	x0, [x19, x20, lsl #3]
   314cc:	mov	x20, x8
   314d0:	b	31478 <__gmpn_bc_set_str@@Base+0x18c>
   314d4:	mov	x20, xzr
   314d8:	b	31478 <__gmpn_bc_set_str@@Base+0x18c>
   314dc:	mov	x22, x8
   314e0:	b	314e8 <__gmpn_bc_set_str@@Base+0x1fc>
   314e4:	ldr	x23, [sp]
   314e8:	cmp	w23, #0xa
   314ec:	b.ne	3152c <__gmpn_bc_set_str@@Base+0x240>  // b.any
   314f0:	sub	x8, x21, x27
   314f4:	add	x9, x8, #0x12
   314f8:	cmp	x9, #0x1
   314fc:	b.lt	31568 <__gmpn_bc_set_str@@Base+0x27c>  // b.tstop
   31500:	add	x8, x8, #0x13
   31504:	mov	w9, #0xa                   	// #10
   31508:	mov	w3, #0xa                   	// #10
   3150c:	ldrb	w10, [x22], #1
   31510:	add	x11, x3, x3, lsl #2
   31514:	sub	x8, x8, #0x1
   31518:	cmp	x8, #0x1
   3151c:	madd	x4, x4, x9, x10
   31520:	lsl	x3, x11, #1
   31524:	b.gt	3150c <__gmpn_bc_set_str@@Base+0x220>
   31528:	b	3156c <__gmpn_bc_set_str@@Base+0x280>
   3152c:	add	x8, x26, x21
   31530:	mvn	x9, x27
   31534:	add	x8, x8, x9
   31538:	cmp	x8, #0x1
   3153c:	b.lt	31594 <__gmpn_bc_set_str@@Base+0x2a8>  // b.tstop
   31540:	add	x8, x21, x26
   31544:	sub	x8, x8, x27
   31548:	mov	x3, x25
   3154c:	ldrb	w9, [x22], #1
   31550:	sub	x8, x8, #0x1
   31554:	cmp	x8, #0x1
   31558:	mul	x3, x3, x25
   3155c:	madd	x4, x4, x25, x9
   31560:	b.gt	3154c <__gmpn_bc_set_str@@Base+0x260>
   31564:	b	3156c <__gmpn_bc_set_str@@Base+0x280>
   31568:	mov	w3, #0xa                   	// #10
   3156c:	cbz	x20, 3159c <__gmpn_bc_set_str@@Base+0x2b0>
   31570:	mov	x0, x19
   31574:	mov	x1, x19
   31578:	mov	x2, x20
   3157c:	bl	d240 <__gmpn_mul_1c@plt>
   31580:	cbz	x0, 315b0 <__gmpn_bc_set_str@@Base+0x2c4>
   31584:	add	x8, x20, #0x1
   31588:	str	x0, [x19, x20, lsl #3]
   3158c:	mov	x20, x8
   31590:	b	315b0 <__gmpn_bc_set_str@@Base+0x2c4>
   31594:	mov	x3, x25
   31598:	cbnz	x20, 31570 <__gmpn_bc_set_str@@Base+0x284>
   3159c:	cbz	x4, 315ac <__gmpn_bc_set_str@@Base+0x2c0>
   315a0:	str	x4, [x19]
   315a4:	mov	w20, #0x1                   	// #1
   315a8:	b	315b0 <__gmpn_bc_set_str@@Base+0x2c4>
   315ac:	mov	x20, xzr
   315b0:	mov	x0, x20
   315b4:	ldp	x20, x19, [sp, #96]
   315b8:	ldp	x22, x21, [sp, #80]
   315bc:	ldp	x24, x23, [sp, #64]
   315c0:	ldp	x26, x25, [sp, #48]
   315c4:	ldp	x28, x27, [sp, #32]
   315c8:	ldp	x29, x30, [sp, #16]
   315cc:	add	sp, sp, #0x70
   315d0:	ret
   315d4:	mov	x20, xzr
   315d8:	mov	w27, #0x1                   	// #1
   315dc:	b	315fc <__gmpn_bc_set_str@@Base+0x310>
   315e0:	cbz	x4, 31628 <__gmpn_bc_set_str@@Base+0x33c>
   315e4:	mov	w20, #0x1                   	// #1
   315e8:	str	x4, [x19]
   315ec:	ldrb	w4, [x22], #1
   315f0:	add	x27, x27, x26
   315f4:	cmp	x27, x21
   315f8:	b.cs	314e8 <__gmpn_bc_set_str@@Base+0x1fc>  // b.hs, b.nlast
   315fc:	cbz	x20, 315e0 <__gmpn_bc_set_str@@Base+0x2f4>
   31600:	mov	x0, x19
   31604:	mov	x1, x19
   31608:	mov	x2, x20
   3160c:	mov	x3, x24
   31610:	bl	d240 <__gmpn_mul_1c@plt>
   31614:	cbz	x0, 315ec <__gmpn_bc_set_str@@Base+0x300>
   31618:	add	x8, x20, #0x1
   3161c:	str	x0, [x19, x20, lsl #3]
   31620:	mov	x20, x8
   31624:	b	315ec <__gmpn_bc_set_str@@Base+0x300>
   31628:	mov	x20, xzr
   3162c:	b	315ec <__gmpn_bc_set_str@@Base+0x300>

0000000000031630 <__gmpn_dc_set_str@@Base>:
   31630:	stp	x29, x30, [sp, #-80]!
   31634:	stp	x26, x25, [sp, #16]
   31638:	stp	x24, x23, [sp, #32]
   3163c:	stp	x22, x21, [sp, #48]
   31640:	stp	x20, x19, [sp, #64]
   31644:	ldr	x23, [x3, #24]
   31648:	mov	x21, x4
   3164c:	mov	x20, x3
   31650:	mov	x24, x2
   31654:	mov	x25, x1
   31658:	cmp	x23, x2
   3165c:	mov	x19, x0
   31660:	mov	x29, sp
   31664:	b.cc	31680 <__gmpn_dc_set_str@@Base+0x50>  // b.lo, b.ul, b.last
   31668:	cmp	x24, #0x314
   3166c:	b.cc	31708 <__gmpn_dc_set_str@@Base+0xd8>  // b.lo, b.ul, b.last
   31670:	ldur	x23, [x20, #-16]
   31674:	sub	x20, x20, #0x28
   31678:	cmp	x23, x24
   3167c:	b.cs	31670 <__gmpn_dc_set_str@@Base+0x40>  // b.hs, b.nlast
   31680:	sub	x2, x24, x23
   31684:	cmp	x2, #0x313
   31688:	b.ls	316d8 <__gmpn_dc_set_str@@Base+0xa8>  // b.plast
   3168c:	sub	x3, x20, #0x28
   31690:	mov	x0, x21
   31694:	mov	x1, x25
   31698:	mov	x4, x19
   3169c:	bl	c360 <__gmpn_dc_set_str@plt>
   316a0:	ldp	x4, x26, [x20, #8]
   316a4:	mov	x22, x0
   316a8:	cbz	x0, 316f4 <__gmpn_dc_set_str@@Base+0xc4>
   316ac:	ldr	x3, [x20]
   316b0:	cmp	x4, x22
   316b4:	add	x0, x19, x26, lsl #3
   316b8:	b.le	31730 <__gmpn_dc_set_str@@Base+0x100>
   316bc:	mov	x1, x3
   316c0:	mov	x2, x4
   316c4:	mov	x3, x21
   316c8:	mov	x4, x22
   316cc:	bl	ccd0 <__gmpn_mul@plt>
   316d0:	cbnz	x26, 31740 <__gmpn_dc_set_str@@Base+0x110>
   316d4:	b	31750 <__gmpn_dc_set_str@@Base+0x120>
   316d8:	ldr	w3, [x20, #32]
   316dc:	mov	x0, x21
   316e0:	mov	x1, x25
   316e4:	bl	c110 <__gmpn_bc_set_str@plt>
   316e8:	ldp	x4, x26, [x20, #8]
   316ec:	mov	x22, x0
   316f0:	cbnz	x0, 316ac <__gmpn_dc_set_str@@Base+0x7c>
   316f4:	add	x8, x26, x4
   316f8:	adds	x8, x8, #0x1
   316fc:	b.eq	31750 <__gmpn_dc_set_str@@Base+0x120>  // b.none
   31700:	lsl	x2, x8, #3
   31704:	b	31744 <__gmpn_dc_set_str@@Base+0x114>
   31708:	ldr	w3, [x20, #32]
   3170c:	mov	x0, x19
   31710:	mov	x1, x25
   31714:	mov	x2, x24
   31718:	ldp	x20, x19, [sp, #64]
   3171c:	ldp	x22, x21, [sp, #48]
   31720:	ldp	x24, x23, [sp, #32]
   31724:	ldp	x26, x25, [sp, #16]
   31728:	ldp	x29, x30, [sp], #80
   3172c:	b	c110 <__gmpn_bc_set_str@plt>
   31730:	mov	x1, x21
   31734:	mov	x2, x22
   31738:	bl	ccd0 <__gmpn_mul@plt>
   3173c:	cbz	x26, 31750 <__gmpn_dc_set_str@@Base+0x120>
   31740:	lsl	x2, x26, #3
   31744:	mov	x0, x19
   31748:	mov	w1, wzr
   3174c:	bl	c5f0 <memset@plt>
   31750:	add	x8, x25, x24
   31754:	cmp	x23, #0x313
   31758:	sub	x1, x8, x23
   3175c:	b.ls	3178c <__gmpn_dc_set_str@@Base+0x15c>  // b.plast
   31760:	ldr	x8, [x20, #8]
   31764:	sub	x3, x20, #0x28
   31768:	mov	x0, x21
   3176c:	mov	x2, x23
   31770:	add	x8, x21, x8, lsl #3
   31774:	add	x8, x8, x26, lsl #3
   31778:	add	x4, x8, #0x8
   3177c:	bl	c360 <__gmpn_dc_set_str@plt>
   31780:	mov	x23, x0
   31784:	cbnz	x0, 317a4 <__gmpn_dc_set_str@@Base+0x174>
   31788:	b	317e4 <__gmpn_dc_set_str@@Base+0x1b4>
   3178c:	ldr	w3, [x20, #32]
   31790:	mov	x0, x21
   31794:	mov	x2, x23
   31798:	bl	c110 <__gmpn_bc_set_str@plt>
   3179c:	mov	x23, x0
   317a0:	cbz	x0, 317e4 <__gmpn_dc_set_str@@Base+0x1b4>
   317a4:	mov	x0, x19
   317a8:	mov	x1, x19
   317ac:	mov	x2, x21
   317b0:	mov	x3, x23
   317b4:	bl	ca70 <__gmpn_add_n@plt>
   317b8:	lsl	x8, x23, #3
   317bc:	ldr	x9, [x19, x8]
   317c0:	adds	x9, x9, x0
   317c4:	str	x9, [x19, x8]
   317c8:	b.cc	317e4 <__gmpn_dc_set_str@@Base+0x1b4>  // b.lo, b.ul, b.last
   317cc:	add	x8, x19, x23, lsl #3
   317d0:	add	x8, x8, #0x8
   317d4:	ldr	x9, [x8]
   317d8:	adds	x9, x9, #0x1
   317dc:	str	x9, [x8], #8
   317e0:	b.cs	317d4 <__gmpn_dc_set_str@@Base+0x1a4>  // b.hs, b.nlast
   317e4:	ldr	x8, [x20, #8]
   317e8:	add	x9, x26, x22
   317ec:	ldp	x22, x21, [sp, #48]
   317f0:	ldp	x24, x23, [sp, #32]
   317f4:	add	x8, x9, x8
   317f8:	add	x9, x19, x8, lsl #3
   317fc:	ldur	x9, [x9, #-8]
   31800:	ldp	x20, x19, [sp, #64]
   31804:	ldp	x26, x25, [sp, #16]
   31808:	cmp	x9, #0x0
   3180c:	cset	w9, eq  // eq = none
   31810:	sub	x0, x8, x9
   31814:	ldp	x29, x30, [sp], #80
   31818:	ret

000000000003181c <__gmpn_compute_powtab@@Base>:
   3181c:	str	d14, [sp, #-160]!
   31820:	stp	d13, d12, [sp, #16]
   31824:	stp	d11, d10, [sp, #32]
   31828:	stp	d9, d8, [sp, #48]
   3182c:	stp	x29, x30, [sp, #64]
   31830:	stp	x28, x27, [sp, #80]
   31834:	stp	x26, x25, [sp, #96]
   31838:	stp	x24, x23, [sp, #112]
   3183c:	stp	x22, x21, [sp, #128]
   31840:	stp	x20, x19, [sp, #144]
   31844:	mov	x29, sp
   31848:	sub	sp, sp, #0x230
   3184c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   31850:	ldr	x8, [x8, #3936]
   31854:	mov	w9, #0x28                  	// #40
   31858:	smull	x9, w3, w9
   3185c:	mov	w27, w3
   31860:	ldrsw	x20, [x8, x9]
   31864:	add	x9, x2, #0x1
   31868:	lsr	x10, x9, #1
   3186c:	mov	x21, x1
   31870:	cmp	x10, #0x1
   31874:	mov	x1, x0
   31878:	mov	x9, xzr
   3187c:	b.ne	3188c <__gmpn_compute_powtab@@Base+0x70>  // b.any
   31880:	mov	w13, #0x1                   	// #1
   31884:	str	x20, [sp, #40]
   31888:	b	31b00 <__gmpn_compute_powtab@@Base+0x2e4>
   3188c:	mov	x12, #0xffffffffffffffff    	// #-1
   31890:	add	x11, sp, #0x28
   31894:	mov	x13, x10
   31898:	mul	x14, x13, x20
   3189c:	add	x13, x13, #0x1
   318a0:	lsr	x13, x13, #1
   318a4:	str	x14, [x11, x9, lsl #3]
   318a8:	add	x9, x9, #0x1
   318ac:	cmp	x13, #0x1
   318b0:	add	x12, x12, #0x1
   318b4:	b.ne	31898 <__gmpn_compute_powtab@@Base+0x7c>  // b.any
   318b8:	add	x11, sp, #0x28
   318bc:	subs	x13, x9, #0x1
   318c0:	str	x20, [x11, x9, lsl #3]
   318c4:	b.ne	318d4 <__gmpn_compute_powtab@@Base+0xb8>  // b.any
   318c8:	mov	w13, #0x1                   	// #1
   318cc:	mov	w10, #0x1                   	// #1
   318d0:	b	31b00 <__gmpn_compute_powtab@@Base+0x2e4>
   318d4:	cmp	x13, #0x7
   318d8:	sub	x11, x2, #0x1
   318dc:	b.hi	318f0 <__gmpn_compute_powtab@@Base+0xd4>  // b.pmore
   318e0:	mov	w14, #0x1                   	// #1
   318e4:	mov	x12, x13
   318e8:	mov	w13, #0x1                   	// #1
   318ec:	b	31aa4 <__gmpn_compute_powtab@@Base+0x288>
   318f0:	adrp	x16, 5d000 <__gmpn_bases@@Base+0x2ab8>
   318f4:	adrp	x15, 5d000 <__gmpn_bases@@Base+0x2ab8>
   318f8:	ldr	q20, [x16, #1360]
   318fc:	adrp	x16, 5d000 <__gmpn_bases@@Base+0x2ab8>
   31900:	ldr	q18, [x15, #1344]
   31904:	ldr	q3, [x16, #1376]
   31908:	mov	x16, #0xfffffffffffffffc    	// #-4
   3190c:	dup	v5.2d, x16
   31910:	mov	x16, #0xfffffffffffffffb    	// #-5
   31914:	mvn	x12, x12
   31918:	dup	v6.2d, x16
   3191c:	mov	w16, #0x1                   	// #1
   31920:	dup	v19.2d, x13
   31924:	dup	v16.2d, x16
   31928:	mov	x16, #0xfffffffffffffff8    	// #-8
   3192c:	orr	x12, x12, #0x7
   31930:	and	x14, x13, #0x7ffffffffffffff8
   31934:	dup	v0.2d, x11
   31938:	dup	v1.2d, x10
   3193c:	and	x15, x13, #0xfffffffffffffff8
   31940:	movi	v2.2d, #0x0
   31944:	movi	v4.2d, #0xffffffffffffffff
   31948:	movi	v7.4s, #0x1
   3194c:	dup	v17.2d, x16
   31950:	add	x12, x12, x9
   31954:	add	v18.2d, v19.2d, v18.2d
   31958:	add	v19.2d, v19.2d, v20.2d
   3195c:	mov	v20.16b, v3.16b
   31960:	movi	v21.2d, #0x0
   31964:	add	v26.2d, v19.2d, v5.2d
   31968:	neg	v29.2d, v19.2d
   3196c:	add	v27.2d, v18.2d, v5.2d
   31970:	neg	v28.2d, v18.2d
   31974:	ushl	v29.2d, v0.2d, v29.2d
   31978:	neg	v26.2d, v26.2d
   3197c:	ushl	v28.2d, v0.2d, v28.2d
   31980:	neg	v27.2d, v27.2d
   31984:	ushl	v26.2d, v0.2d, v26.2d
   31988:	add	v30.2d, v29.2d, v16.2d
   3198c:	add	v25.2d, v19.2d, v4.2d
   31990:	add	v23.2d, v19.2d, v6.2d
   31994:	ushl	v27.2d, v0.2d, v27.2d
   31998:	add	v31.2d, v28.2d, v16.2d
   3199c:	add	v8.2d, v26.2d, v16.2d
   319a0:	and	v11.16b, v30.16b, v16.16b
   319a4:	add	v24.2d, v18.2d, v4.2d
   319a8:	add	v22.2d, v18.2d, v6.2d
   319ac:	add	v9.2d, v27.2d, v16.2d
   319b0:	and	v10.16b, v31.16b, v16.16b
   319b4:	ushl	v25.2d, v30.2d, v25.2d
   319b8:	and	v13.16b, v8.16b, v16.16b
   319bc:	cmeq	v11.2d, v11.2d, #0
   319c0:	ushl	v23.2d, v8.2d, v23.2d
   319c4:	cmhi	v29.2d, v29.2d, v16.2d
   319c8:	xtn	v12.2s, v30.2d
   319cc:	ushl	v24.2d, v31.2d, v24.2d
   319d0:	cmhi	v26.2d, v26.2d, v16.2d
   319d4:	and	v30.16b, v9.16b, v16.16b
   319d8:	cmeq	v10.2d, v10.2d, #0
   319dc:	xtn	v14.2s, v8.2d
   319e0:	ushl	v22.2d, v9.2d, v22.2d
   319e4:	cmeq	v25.2d, v1.2d, v25.2d
   319e8:	xtn	v8.2s, v11.2d
   319ec:	cmeq	v11.2d, v13.2d, #0
   319f0:	cmeq	v23.2d, v1.2d, v23.2d
   319f4:	cmhi	v28.2d, v28.2d, v16.2d
   319f8:	xtn	v29.2s, v29.2d
   319fc:	cmhi	v27.2d, v27.2d, v16.2d
   31a00:	cmeq	v24.2d, v1.2d, v24.2d
   31a04:	xtn	v26.2s, v26.2d
   31a08:	cmeq	v30.2d, v30.2d, #0
   31a0c:	xtn	v25.2s, v25.2d
   31a10:	cmeq	v22.2d, v1.2d, v22.2d
   31a14:	xtn2	v8.4s, v10.2d
   31a18:	xtn	v10.2s, v11.2d
   31a1c:	xtn	v23.2s, v23.2d
   31a20:	xtn2	v10.4s, v30.2d
   31a24:	xtn2	v12.4s, v31.2d
   31a28:	xtn2	v14.4s, v9.2d
   31a2c:	xtn2	v25.4s, v24.2d
   31a30:	xtn2	v23.4s, v22.2d
   31a34:	xtn2	v29.4s, v28.2d
   31a38:	xtn2	v26.4s, v27.2d
   31a3c:	and	v27.16b, v29.16b, v8.16b
   31a40:	and	v26.16b, v26.16b, v10.16b
   31a44:	and	v27.16b, v27.16b, v7.16b
   31a48:	and	v26.16b, v26.16b, v7.16b
   31a4c:	bic	v22.16b, v12.16b, v8.16b
   31a50:	bic	v24.16b, v14.16b, v10.16b
   31a54:	ushl	v27.4s, v12.4s, v27.4s
   31a58:	ushl	v26.4s, v14.4s, v26.4s
   31a5c:	bsl	v25.16b, v22.16b, v27.16b
   31a60:	bsl	v23.16b, v24.16b, v26.16b
   31a64:	add	v18.2d, v18.2d, v17.2d
   31a68:	subs	x15, x15, #0x8
   31a6c:	add	v3.4s, v22.4s, v3.4s
   31a70:	add	v2.4s, v24.4s, v2.4s
   31a74:	add	v20.4s, v25.4s, v20.4s
   31a78:	add	v21.4s, v23.4s, v21.4s
   31a7c:	add	v19.2d, v19.2d, v17.2d
   31a80:	b.ne	31964 <__gmpn_compute_powtab@@Base+0x148>  // b.any
   31a84:	add	v0.4s, v21.4s, v20.4s
   31a88:	add	v1.4s, v2.4s, v3.4s
   31a8c:	addv	s0, v0.4s
   31a90:	addv	s1, v1.4s
   31a94:	cmp	x13, x14
   31a98:	fmov	w13, s0
   31a9c:	fmov	w14, s1
   31aa0:	b.eq	31ae8 <__gmpn_compute_powtab@@Base+0x2cc>  // b.none
   31aa4:	lsr	x15, x11, x12
   31aa8:	add	x16, x15, #0x1
   31aac:	tst	x16, #0x1
   31ab0:	cset	w17, eq  // eq = none
   31ab4:	csel	w18, wzr, w16, eq  // eq = none
   31ab8:	subs	x0, x12, #0x1
   31abc:	cmp	x15, #0x1
   31ac0:	cset	w15, hi  // hi = pmore
   31ac4:	lsl	x0, x16, x0
   31ac8:	and	w15, w15, w17
   31acc:	cmp	x10, x0
   31ad0:	lsl	w15, w16, w15
   31ad4:	csel	w15, w18, w15, eq  // eq = none
   31ad8:	subs	x12, x12, #0x1
   31adc:	add	w14, w18, w14
   31ae0:	add	w13, w15, w13
   31ae4:	b.gt	31aa4 <__gmpn_compute_powtab@@Base+0x288>
   31ae8:	mov	w10, #0x9f                  	// #159
   31aec:	mov	w11, #0x851f                	// #34079
   31af0:	mul	w10, w14, w10
   31af4:	movk	w11, #0x51eb, lsl #16
   31af8:	umull	x10, w10, w11
   31afc:	lsr	x10, x10, #37
   31b00:	cmp	w13, w10
   31b04:	cneg	x22, x9, hi  // hi = pmore
   31b08:	sxtw	x9, w27
   31b0c:	str	x22, [sp]
   31b10:	tbnz	x22, #63, 31bc8 <__gmpn_compute_powtab@@Base+0x3ac>
   31b14:	mov	w10, #0x28                  	// #40
   31b18:	madd	x8, x9, x10, x8
   31b1c:	ldr	x3, [x8, #24]
   31b20:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   31b24:	ldr	q0, [x8, #1376]
   31b28:	add	x25, x21, #0x8
   31b2c:	str	x3, [x21]
   31b30:	str	x21, [x1]
   31b34:	str	x20, [x1, #24]
   31b38:	str	w27, [x1, #32]
   31b3c:	stur	q0, [x1, #8]
   31b40:	mov	w2, #0x1                   	// #1
   31b44:	mov	x0, x25
   31b48:	mov	x26, x1
   31b4c:	mov	x1, x21
   31b50:	add	x24, x21, #0x18
   31b54:	mov	w19, #0x1                   	// #1
   31b58:	str	x3, [sp, #24]
   31b5c:	bl	d490 <__gmpn_mul_1@plt>
   31b60:	ldr	x8, [x21, #8]
   31b64:	lsl	x9, x20, #1
   31b68:	str	x0, [x21, #16]
   31b6c:	str	w27, [x26, #72]
   31b70:	cmp	x8, #0x0
   31b74:	cset	w8, eq  // eq = none
   31b78:	cinc	x23, x19, ne  // ne = any
   31b7c:	add	x25, x25, w8, uxtw #3
   31b80:	stp	x25, x23, [x26, #40]
   31b84:	stp	x8, x9, [x26, #56]
   31b88:	mov	x11, x9
   31b8c:	ldr	x9, [sp, #40]
   31b90:	mov	x28, x8
   31b94:	lsl	x8, x20, x22
   31b98:	mov	x10, x26
   31b9c:	cmp	x9, x8
   31ba0:	str	x27, [sp, #16]
   31ba4:	str	x9, [x29, #8]
   31ba8:	b.ne	31d1c <__gmpn_compute_powtab@@Base+0x500>  // b.any
   31bac:	add	x8, x10, #0x50
   31bb0:	mov	x9, #0xfffffffffffffffe    	// #-2
   31bb4:	mov	x26, x11
   31bb8:	adds	x27, x22, x9
   31bbc:	b.pl	31dbc <__gmpn_compute_powtab@@Base+0x5a0>  // b.nfrst
   31bc0:	mov	x0, x22
   31bc4:	b	3204c <__gmpn_compute_powtab@@Base+0x830>
   31bc8:	neg	x10, x22
   31bcc:	str	x10, [sp, #24]
   31bd0:	mov	w10, #0x28                  	// #40
   31bd4:	madd	x8, x9, x10, x8
   31bd8:	ldr	x24, [x8, #24]
   31bdc:	adrp	x11, 5d000 <__gmpn_bases@@Base+0x2ab8>
   31be0:	ldr	q0, [x11, #1376]
   31be4:	mov	x23, x21
   31be8:	neg	x9, x24
   31bec:	and	x9, x24, x9
   31bf0:	add	x8, x1, #0x28
   31bf4:	mvn	x26, x22
   31bf8:	str	x24, [x23], #8
   31bfc:	cmp	w27, #0xa
   31c00:	sub	x22, x9, #0x1
   31c04:	str	x21, [x1]
   31c08:	str	x20, [x1, #24]
   31c0c:	str	w27, [x1, #32]
   31c10:	stur	q0, [x1, #8]
   31c14:	str	x20, [sp, #32]
   31c18:	b.ne	31f0c <__gmpn_compute_powtab@@Base+0x6f0>  // b.any
   31c1c:	mov	x27, xzr
   31c20:	lsr	x9, x24, #19
   31c24:	mov	w25, #0x1                   	// #1
   31c28:	mov	x19, x20
   31c2c:	str	x9, [x29, #8]
   31c30:	b	31c60 <__gmpn_compute_powtab@@Base+0x444>
   31c34:	mov	x21, x23
   31c38:	mov	x8, x1
   31c3c:	subs	x26, x26, #0x1
   31c40:	str	x21, [x8], #40
   31c44:	mov	w10, #0xa                   	// #10
   31c48:	mov	x23, x9
   31c4c:	mov	x25, x24
   31c50:	stp	x27, x19, [x20, #56]
   31c54:	str	w10, [x20, #72]
   31c58:	str	x24, [x20, #48]
   31c5c:	b.mi	31ff8 <__gmpn_compute_powtab@@Base+0x7dc>  // b.first
   31c60:	mov	x20, x1
   31c64:	mov	x0, x23
   31c68:	mov	x1, x21
   31c6c:	mov	x2, x25
   31c70:	mov	x28, x8
   31c74:	lsl	x24, x25, #1
   31c78:	bl	c8e0 <__gmpn_sqr@plt>
   31c7c:	sub	x8, x24, #0x1
   31c80:	ldr	x9, [x23, x8, lsl #3]
   31c84:	add	x10, sp, #0x28
   31c88:	ldr	x10, [x10, x26, lsl #3]
   31c8c:	lsl	x19, x19, #1
   31c90:	cmp	x9, #0x0
   31c94:	csel	x24, x8, x24, eq  // eq = none
   31c98:	cmp	x19, x10
   31c9c:	b.eq	31ce4 <__gmpn_compute_powtab@@Base+0x4c8>  // b.none
   31ca0:	mov	x4, #0xce15                	// #52757
   31ca4:	ldr	x3, [x29, #8]
   31ca8:	movk	x4, #0x6559, lsl #16
   31cac:	movk	x4, #0x7250, lsl #32
   31cb0:	movk	x4, #0x26b1, lsl #48
   31cb4:	mov	w5, #0x13                  	// #19
   31cb8:	mov	x0, x23
   31cbc:	mov	x1, x23
   31cc0:	mov	x2, x24
   31cc4:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   31cc8:	add	x8, x23, x24, lsl #3
   31ccc:	ldur	x8, [x8, #-8]
   31cd0:	cmp	x8, #0x0
   31cd4:	cset	w8, eq  // eq = none
   31cd8:	sub	x24, x24, x8
   31cdc:	ldr	x8, [sp, #32]
   31ce0:	sub	x19, x19, x8
   31ce4:	ldr	x8, [x23]
   31ce8:	add	x9, x23, x25, lsl #4
   31cec:	lsl	x27, x27, #1
   31cf0:	mov	x1, x28
   31cf4:	cbnz	x8, 31c34 <__gmpn_compute_powtab@@Base+0x418>
   31cf8:	mov	x21, x23
   31cfc:	ldr	x8, [x21, #8]!
   31d00:	tst	x8, x22
   31d04:	b.ne	31c34 <__gmpn_compute_powtab@@Base+0x418>  // b.any
   31d08:	sub	x24, x24, #0x1
   31d0c:	add	x27, x27, #0x1
   31d10:	mov	x23, x21
   31d14:	cbz	x8, 31cfc <__gmpn_compute_powtab@@Base+0x4e0>
   31d18:	b	31c38 <__gmpn_compute_powtab@@Base+0x41c>
   31d1c:	mov	x8, x22
   31d20:	add	x22, x20, x20, lsl #1
   31d24:	sub	x8, x8, #0x2
   31d28:	lsl	x8, x22, x8
   31d2c:	cmp	x8, x9
   31d30:	b.ls	31d50 <__gmpn_compute_powtab@@Base+0x534>  // b.plast
   31d34:	ldr	x8, [x25]
   31d38:	add	x19, x21, #0x30
   31d3c:	mov	x26, x11
   31d40:	str	x8, [x21, #24]
   31d44:	ldr	x8, [x25, #8]
   31d48:	str	x8, [x21, #32]
   31d4c:	b	31d94 <__gmpn_compute_powtab@@Base+0x578>
   31d50:	ldr	x3, [sp, #24]
   31d54:	mov	x0, x24
   31d58:	mov	x1, x25
   31d5c:	mov	x2, x23
   31d60:	add	x19, x21, #0x38
   31d64:	bl	d490 <__gmpn_mul_1@plt>
   31d68:	str	x0, [x24, x23, lsl #3]
   31d6c:	ldr	x8, [x21, #24]
   31d70:	cmp	x0, #0x0
   31d74:	cinc	x9, x23, ne  // ne = any
   31d78:	mov	x10, x26
   31d7c:	cmp	x8, #0x0
   31d80:	cset	w8, eq  // eq = none
   31d84:	cinc	x28, x28, eq  // eq = none
   31d88:	add	x24, x24, w8, uxtw #3
   31d8c:	sub	x23, x9, x8
   31d90:	mov	x26, x22
   31d94:	stp	x24, x23, [x10, #80]
   31d98:	stp	x28, x26, [x10, #96]
   31d9c:	ldr	x22, [sp]
   31da0:	mov	x25, x24
   31da4:	add	x8, x10, #0x78
   31da8:	mov	x9, #0xfffffffffffffffd    	// #-3
   31dac:	mov	x24, x19
   31db0:	str	w27, [x10, #112]
   31db4:	adds	x27, x22, x9
   31db8:	b.mi	31bc0 <__gmpn_compute_powtab@@Base+0x3a4>  // b.first
   31dbc:	ldr	x19, [x29, #8]
   31dc0:	add	x9, sp, #0x28
   31dc4:	add	x9, x9, #0x8
   31dc8:	sub	x22, x8, #0x28
   31dcc:	str	x20, [sp, #32]
   31dd0:	str	x9, [sp, #8]
   31dd4:	mov	x0, x24
   31dd8:	mov	x1, x25
   31ddc:	mov	x2, x23
   31de0:	lsl	x20, x23, #1
   31de4:	add	x21, x24, x23, lsl #4
   31de8:	bl	c8e0 <__gmpn_sqr@plt>
   31dec:	ldur	x8, [x21, #-8]
   31df0:	ldr	x9, [x24]
   31df4:	ldr	x10, [sp, #32]
   31df8:	lsl	x26, x26, #1
   31dfc:	cmp	x8, #0x0
   31e00:	str	x21, [x29, #8]
   31e04:	add	x21, x26, x10
   31e08:	cset	w8, eq  // eq = none
   31e0c:	cmp	x9, #0x0
   31e10:	mov	x11, x26
   31e14:	lsl	x10, x21, x27
   31e18:	sub	x20, x20, x8
   31e1c:	cset	w26, eq  // eq = none
   31e20:	cmp	x10, x19
   31e24:	add	x25, x24, w26, uxtw #3
   31e28:	sub	x23, x20, x26
   31e2c:	bfi	x26, x28, #1, #63
   31e30:	b.ls	31e40 <__gmpn_compute_powtab@@Base+0x624>  // b.plast
   31e34:	mov	x28, x26
   31e38:	mov	x26, x11
   31e3c:	b	31e7c <__gmpn_compute_powtab@@Base+0x660>
   31e40:	ldr	x3, [sp, #24]
   31e44:	mov	x0, x25
   31e48:	mov	x1, x25
   31e4c:	mov	x2, x23
   31e50:	bl	d490 <__gmpn_mul_1@plt>
   31e54:	str	x0, [x24, x20, lsl #3]
   31e58:	ldr	x8, [x25]
   31e5c:	cmp	x0, #0x0
   31e60:	cinc	x9, x23, ne  // ne = any
   31e64:	cmp	x8, #0x0
   31e68:	cset	w8, eq  // eq = none
   31e6c:	cinc	x28, x26, eq  // eq = none
   31e70:	add	x25, x25, w8, uxtw #3
   31e74:	sub	x23, x9, x8
   31e78:	mov	x26, x21
   31e7c:	stp	x25, x23, [x22, #40]
   31e80:	ldp	x9, x8, [sp, #8]
   31e84:	stp	x28, x26, [x22, #56]
   31e88:	str	w8, [x22, #72]
   31e8c:	ldr	x8, [x22, #24]
   31e90:	ldr	x24, [x9, x27, lsl #3]
   31e94:	cmp	x8, x24
   31e98:	b.cs	31ee8 <__gmpn_compute_powtab@@Base+0x6cc>  // b.hs, b.nlast
   31e9c:	ldp	x21, x20, [x22]
   31ea0:	ldr	x3, [sp, #24]
   31ea4:	mov	x0, x21
   31ea8:	mov	x1, x21
   31eac:	mov	x2, x20
   31eb0:	bl	d490 <__gmpn_mul_1@plt>
   31eb4:	str	x0, [x21, x20, lsl #3]
   31eb8:	str	x24, [x22, #24]
   31ebc:	ldr	x8, [x21]
   31ec0:	ldr	x9, [x22, #16]
   31ec4:	cmp	x0, #0x0
   31ec8:	cinc	x10, x20, ne  // ne = any
   31ecc:	cmp	x8, #0x0
   31ed0:	cset	w8, eq  // eq = none
   31ed4:	cinc	x9, x9, eq  // eq = none
   31ed8:	add	x11, x21, w8, uxtw #3
   31edc:	sub	x8, x10, x8
   31ee0:	stp	x11, x8, [x22]
   31ee4:	str	x9, [x22, #16]
   31ee8:	subs	x27, x27, #0x1
   31eec:	b.lt	31f04 <__gmpn_compute_powtab@@Base+0x6e8>  // b.tstop
   31ef0:	ldr	x8, [x29, #8]
   31ef4:	ldr	x19, [sp, #40]
   31ef8:	add	x22, x22, #0x28
   31efc:	add	x24, x8, #0x10
   31f00:	b	31dd4 <__gmpn_compute_powtab@@Base+0x5b8>
   31f04:	ldr	x0, [sp]
   31f08:	b	3204c <__gmpn_compute_powtab@@Base+0x830>
   31f0c:	mov	x19, xzr
   31f10:	mov	w9, #0x1                   	// #1
   31f14:	mov	x28, x20
   31f18:	str	x27, [sp, #16]
   31f1c:	b	31f4c <__gmpn_compute_powtab@@Base+0x730>
   31f20:	mov	x21, x23
   31f24:	mov	x8, x1
   31f28:	str	x21, [x8], #40
   31f2c:	stp	x19, x28, [x20, #56]
   31f30:	ldr	x10, [sp, #16]
   31f34:	subs	x26, x26, #0x1
   31f38:	mov	x23, x9
   31f3c:	mov	x9, x25
   31f40:	str	w10, [x20, #72]
   31f44:	str	x25, [x20, #48]
   31f48:	b.mi	31ff8 <__gmpn_compute_powtab@@Base+0x7dc>  // b.first
   31f4c:	mov	x20, x1
   31f50:	mov	x0, x23
   31f54:	mov	x1, x21
   31f58:	mov	x2, x9
   31f5c:	mov	x27, x8
   31f60:	lsl	x25, x9, #1
   31f64:	mov	x21, x9
   31f68:	bl	c8e0 <__gmpn_sqr@plt>
   31f6c:	sub	x8, x25, #0x1
   31f70:	ldr	x9, [x23, x8, lsl #3]
   31f74:	add	x10, sp, #0x28
   31f78:	ldr	x10, [x10, x26, lsl #3]
   31f7c:	lsl	x28, x28, #1
   31f80:	cmp	x9, #0x0
   31f84:	csel	x25, x8, x25, eq  // eq = none
   31f88:	cmp	x28, x10
   31f8c:	b.eq	31fc0 <__gmpn_compute_powtab@@Base+0x7a4>  // b.none
   31f90:	mov	x0, x23
   31f94:	mov	x1, x23
   31f98:	mov	x2, x25
   31f9c:	mov	x3, x24
   31fa0:	bl	c770 <__gmpn_divexact_1@plt>
   31fa4:	add	x8, x23, x25, lsl #3
   31fa8:	ldur	x8, [x8, #-8]
   31fac:	cmp	x8, #0x0
   31fb0:	cset	w8, eq  // eq = none
   31fb4:	sub	x25, x25, x8
   31fb8:	ldr	x8, [sp, #32]
   31fbc:	sub	x28, x28, x8
   31fc0:	ldr	x8, [x23]
   31fc4:	add	x9, x23, x21, lsl #4
   31fc8:	lsl	x19, x19, #1
   31fcc:	mov	x1, x27
   31fd0:	cbnz	x8, 31f20 <__gmpn_compute_powtab@@Base+0x704>
   31fd4:	mov	x21, x23
   31fd8:	ldr	x8, [x21, #8]!
   31fdc:	tst	x8, x22
   31fe0:	b.ne	31f20 <__gmpn_compute_powtab@@Base+0x704>  // b.any
   31fe4:	sub	x25, x25, #0x1
   31fe8:	add	x19, x19, #0x1
   31fec:	mov	x23, x21
   31ff0:	cbz	x8, 31fd8 <__gmpn_compute_powtab@@Base+0x7bc>
   31ff4:	b	31f24 <__gmpn_compute_powtab@@Base+0x708>
   31ff8:	ldr	x9, [sp]
   31ffc:	cmp	x9, #0x0
   32000:	b.gt	32048 <__gmpn_compute_powtab@@Base+0x82c>
   32004:	mov	w8, #0x1                   	// #1
   32008:	sub	x8, x8, x9
   3200c:	add	x9, x1, #0x8
   32010:	ldp	x10, x13, [x9, #-8]
   32014:	ldr	x12, [x9, #8]
   32018:	sub	x8, x8, #0x1
   3201c:	ldr	x11, [x10]
   32020:	cmp	x11, #0x0
   32024:	cset	w11, eq  // eq = none
   32028:	cinc	x12, x12, eq  // eq = none
   3202c:	add	x10, x10, w11, uxtw #3
   32030:	sub	x11, x13, x11
   32034:	cmp	x8, #0x0
   32038:	stp	x10, x11, [x9, #-8]
   3203c:	str	x12, [x9, #8]
   32040:	sub	x9, x9, #0x28
   32044:	b.gt	32010 <__gmpn_compute_powtab@@Base+0x7f4>
   32048:	ldr	x0, [sp, #24]
   3204c:	add	sp, sp, #0x230
   32050:	ldp	x20, x19, [sp, #144]
   32054:	ldp	x22, x21, [sp, #128]
   32058:	ldp	x24, x23, [sp, #112]
   3205c:	ldp	x26, x25, [sp, #96]
   32060:	ldp	x28, x27, [sp, #80]
   32064:	ldp	x29, x30, [sp, #64]
   32068:	ldp	d9, d8, [sp, #48]
   3206c:	ldp	d11, d10, [sp, #32]
   32070:	ldp	d13, d12, [sp, #16]
   32074:	ldr	d14, [sp], #160
   32078:	ret

000000000003207c <__gmpn_scan0@@Base>:
   3207c:	lsr	x8, x1, #3
   32080:	and	x8, x8, #0x1ffffffffffffff8
   32084:	add	x8, x0, x8
   32088:	ldr	x9, [x8], #8
   3208c:	mov	x10, #0xffffffffffffffff    	// #-1
   32090:	lsl	x10, x10, x1
   32094:	bics	x9, x10, x9
   32098:	b.ne	320ac <__gmpn_scan0@@Base+0x30>  // b.any
   3209c:	ldr	x9, [x8], #8
   320a0:	cmn	x9, #0x1
   320a4:	b.eq	3209c <__gmpn_scan0@@Base+0x20>  // b.none
   320a8:	mvn	x9, x9
   320ac:	rbit	x9, x9
   320b0:	clz	x9, x9
   320b4:	sub	x8, x8, x0
   320b8:	orr	x9, x9, #0xffffffffffffffc0
   320bc:	add	x0, x9, x8, lsl #3
   320c0:	ret

00000000000320c4 <__gmpn_scan1@@Base>:
   320c4:	lsr	x8, x1, #3
   320c8:	and	x8, x8, #0x1ffffffffffffff8
   320cc:	add	x8, x0, x8
   320d0:	ldr	x9, [x8], #8
   320d4:	mov	x10, #0xffffffffffffffff    	// #-1
   320d8:	lsl	x10, x10, x1
   320dc:	ands	x9, x9, x10
   320e0:	b.ne	320ec <__gmpn_scan1@@Base+0x28>  // b.any
   320e4:	ldr	x9, [x8], #8
   320e8:	cbz	x9, 320e4 <__gmpn_scan1@@Base+0x20>
   320ec:	rbit	x9, x9
   320f0:	clz	x9, x9
   320f4:	sub	x8, x8, x0
   320f8:	orr	x9, x9, #0xffffffffffffffc0
   320fc:	add	x0, x9, x8, lsl #3
   32100:	ret
   32104:	nop

0000000000032108 <__gmpn_popcount@@Base>:
   32108:	mov	x11, #0x1fff                	// #8191
   3210c:	cmp	x1, x11
   32110:	b.hi	321ec <__gmpn_popcount@@Base+0xe4>  // b.pmore
   32114:	movi	v4.16b, #0x0
   32118:	movi	v5.16b, #0x0
   3211c:	tbz	w1, #0, 32130 <__gmpn_popcount@@Base+0x28>
   32120:	sub	x1, x1, #0x1
   32124:	ld1	{v0.1d}, [x0], #8
   32128:	cnt	v6.16b, v0.16b
   3212c:	uadalp	v4.8h, v6.16b
   32130:	tbz	w1, #1, 32144 <__gmpn_popcount@@Base+0x3c>
   32134:	sub	x1, x1, #0x2
   32138:	ld1	{v0.2d}, [x0], #16
   3213c:	cnt	v6.16b, v0.16b
   32140:	uadalp	v4.8h, v6.16b
   32144:	tbz	w1, #2, 32168 <__gmpn_popcount@@Base+0x60>
   32148:	subs	x1, x1, #0x4
   3214c:	ld1	{v0.2d, v1.2d}, [x0], #32
   32150:	b.ls	321c0 <__gmpn_popcount@@Base+0xb8>  // b.plast
   32154:	ld1	{v2.2d, v3.2d}, [x0], #32
   32158:	sub	x1, x1, #0x4
   3215c:	cnt	v6.16b, v0.16b
   32160:	cnt	v7.16b, v1.16b
   32164:	b	3219c <__gmpn_popcount@@Base+0x94>
   32168:	subs	x1, x1, #0x8
   3216c:	b.cc	321d4 <__gmpn_popcount@@Base+0xcc>  // b.lo, b.ul, b.last
   32170:	ld1	{v2.2d, v3.2d}, [x0], #32
   32174:	ld1	{v0.2d, v1.2d}, [x0], #32
   32178:	cnt	v6.16b, v2.16b
   3217c:	cnt	v7.16b, v3.16b
   32180:	subs	x1, x1, #0x8
   32184:	b.cc	321b8 <__gmpn_popcount@@Base+0xb0>  // b.lo, b.ul, b.last
   32188:	ld1	{v2.2d, v3.2d}, [x0], #32
   3218c:	uadalp	v4.8h, v6.16b
   32190:	cnt	v6.16b, v0.16b
   32194:	uadalp	v5.8h, v7.16b
   32198:	cnt	v7.16b, v1.16b
   3219c:	ld1	{v0.2d, v1.2d}, [x0], #32
   321a0:	subs	x1, x1, #0x8
   321a4:	uadalp	v4.8h, v6.16b
   321a8:	cnt	v6.16b, v2.16b
   321ac:	uadalp	v5.8h, v7.16b
   321b0:	cnt	v7.16b, v3.16b
   321b4:	b.cs	32188 <__gmpn_popcount@@Base+0x80>  // b.hs, b.nlast
   321b8:	uadalp	v4.8h, v6.16b
   321bc:	uadalp	v5.8h, v7.16b
   321c0:	cnt	v6.16b, v0.16b
   321c4:	cnt	v7.16b, v1.16b
   321c8:	uadalp	v4.8h, v6.16b
   321cc:	uadalp	v5.8h, v7.16b
   321d0:	add	v4.8h, v4.8h, v5.8h
   321d4:	uaddlp	v4.4s, v4.8h
   321d8:	uaddlp	v4.2d, v4.4s
   321dc:	mov	x0, v4.d[0]
   321e0:	mov	x1, v4.d[1]
   321e4:	add	x0, x0, x1
   321e8:	ret
   321ec:	mov	x8, x30
   321f0:	mov	x7, x1
   321f4:	mov	x4, #0x0                   	// #0
   321f8:	mov	x9, #0xff80                	// #65408
   321fc:	mov	x10, #0x1ff0                	// #8176
   32200:	add	x5, x0, x9
   32204:	mov	x1, #0x1fe8                	// #8168
   32208:	movi	v4.16b, #0x0
   3220c:	movi	v5.16b, #0x0
   32210:	bl	32170 <__gmpn_popcount@@Base+0x68>
   32214:	add	x4, x4, x0
   32218:	mov	x0, x5
   3221c:	sub	x7, x7, x10
   32220:	cmp	x7, x11
   32224:	b.hi	32200 <__gmpn_popcount@@Base+0xf8>  // b.pmore
   32228:	mov	x1, x7
   3222c:	bl	32114 <__gmpn_popcount@@Base+0xc>
   32230:	add	x0, x4, x0
   32234:	mov	x30, x8
   32238:	ret
   3223c:	nop

0000000000032240 <__gmpn_hamdist@@Base>:
   32240:	mov	x11, #0x1fff                	// #8191
   32244:	cmp	x2, x11
   32248:	b.hi	32374 <__gmpn_hamdist@@Base+0x134>  // b.pmore
   3224c:	movi	v4.16b, #0x0
   32250:	movi	v5.16b, #0x0
   32254:	tbz	w2, #0, 32270 <__gmpn_hamdist@@Base+0x30>
   32258:	sub	x2, x2, #0x1
   3225c:	ld1	{v0.1d}, [x0], #8
   32260:	ld1	{v16.1d}, [x1], #8
   32264:	eor	v0.16b, v0.16b, v16.16b
   32268:	cnt	v6.16b, v0.16b
   3226c:	uadalp	v4.8h, v6.16b
   32270:	tbz	w2, #1, 3228c <__gmpn_hamdist@@Base+0x4c>
   32274:	sub	x2, x2, #0x2
   32278:	ld1	{v0.2d}, [x0], #16
   3227c:	ld1	{v16.2d}, [x1], #16
   32280:	eor	v0.16b, v0.16b, v16.16b
   32284:	cnt	v6.16b, v0.16b
   32288:	uadalp	v4.8h, v6.16b
   3228c:	tbz	w2, #2, 322c0 <__gmpn_hamdist@@Base+0x80>
   32290:	subs	x2, x2, #0x4
   32294:	ld1	{v0.2d, v1.2d}, [x0], #32
   32298:	ld1	{v16.2d, v17.2d}, [x1], #32
   3229c:	b.ls	32340 <__gmpn_hamdist@@Base+0x100>  // b.plast
   322a0:	ld1	{v2.2d, v3.2d}, [x0], #32
   322a4:	ld1	{v18.2d, v19.2d}, [x1], #32
   322a8:	eor	v0.16b, v0.16b, v16.16b
   322ac:	eor	v1.16b, v1.16b, v17.16b
   322b0:	sub	x2, x2, #0x4
   322b4:	cnt	v6.16b, v0.16b
   322b8:	cnt	v7.16b, v1.16b
   322bc:	b	32310 <__gmpn_hamdist@@Base+0xd0>
   322c0:	subs	x2, x2, #0x8
   322c4:	b.cc	3235c <__gmpn_hamdist@@Base+0x11c>  // b.lo, b.ul, b.last
   322c8:	ld1	{v2.2d, v3.2d}, [x0], #32
   322cc:	ld1	{v0.2d, v1.2d}, [x0], #32
   322d0:	ld1	{v18.2d, v19.2d}, [x1], #32
   322d4:	ld1	{v16.2d, v17.2d}, [x1], #32
   322d8:	eor	v2.16b, v2.16b, v18.16b
   322dc:	eor	v3.16b, v3.16b, v19.16b
   322e0:	cnt	v6.16b, v2.16b
   322e4:	cnt	v7.16b, v3.16b
   322e8:	subs	x2, x2, #0x8
   322ec:	b.cc	32338 <__gmpn_hamdist@@Base+0xf8>  // b.lo, b.ul, b.last
   322f0:	ld1	{v2.2d, v3.2d}, [x0], #32
   322f4:	ld1	{v18.2d, v19.2d}, [x1], #32
   322f8:	eor	v0.16b, v0.16b, v16.16b
   322fc:	eor	v1.16b, v1.16b, v17.16b
   32300:	uadalp	v4.8h, v6.16b
   32304:	cnt	v6.16b, v0.16b
   32308:	uadalp	v5.8h, v7.16b
   3230c:	cnt	v7.16b, v1.16b
   32310:	ld1	{v0.2d, v1.2d}, [x0], #32
   32314:	ld1	{v16.2d, v17.2d}, [x1], #32
   32318:	eor	v2.16b, v2.16b, v18.16b
   3231c:	eor	v3.16b, v3.16b, v19.16b
   32320:	subs	x2, x2, #0x8
   32324:	uadalp	v4.8h, v6.16b
   32328:	cnt	v6.16b, v2.16b
   3232c:	uadalp	v5.8h, v7.16b
   32330:	cnt	v7.16b, v3.16b
   32334:	b.cs	322f0 <__gmpn_hamdist@@Base+0xb0>  // b.hs, b.nlast
   32338:	uadalp	v4.8h, v6.16b
   3233c:	uadalp	v5.8h, v7.16b
   32340:	eor	v0.16b, v0.16b, v16.16b
   32344:	eor	v1.16b, v1.16b, v17.16b
   32348:	cnt	v6.16b, v0.16b
   3234c:	cnt	v7.16b, v1.16b
   32350:	uadalp	v4.8h, v6.16b
   32354:	uadalp	v5.8h, v7.16b
   32358:	add	v4.8h, v4.8h, v5.8h
   3235c:	uaddlp	v4.4s, v4.8h
   32360:	uaddlp	v4.2d, v4.4s
   32364:	mov	x0, v4.d[0]
   32368:	mov	x1, v4.d[1]
   3236c:	add	x0, x0, x1
   32370:	ret
   32374:	mov	x8, x30
   32378:	mov	x7, x2
   3237c:	mov	x4, #0x0                   	// #0
   32380:	mov	x9, #0xff80                	// #65408
   32384:	mov	x10, #0x1ff0                	// #8176
   32388:	add	x5, x0, x9
   3238c:	add	x6, x1, x9
   32390:	mov	x2, #0x1fe8                	// #8168
   32394:	movi	v4.16b, #0x0
   32398:	movi	v5.16b, #0x0
   3239c:	bl	322c8 <__gmpn_hamdist@@Base+0x88>
   323a0:	add	x4, x4, x0
   323a4:	mov	x0, x5
   323a8:	mov	x1, x6
   323ac:	sub	x7, x7, x10
   323b0:	cmp	x7, x11
   323b4:	b.hi	32388 <__gmpn_hamdist@@Base+0x148>  // b.pmore
   323b8:	mov	x2, x7
   323bc:	bl	3224c <__gmpn_hamdist@@Base+0xc>
   323c0:	add	x0, x4, x0
   323c4:	mov	x30, x8
   323c8:	ret

00000000000323cc <__gmpn_cmp@@Base>:
   323cc:	sub	x8, x0, #0x8
   323d0:	sub	x9, x1, #0x8
   323d4:	subs	x10, x2, #0x1
   323d8:	b.lt	32400 <__gmpn_cmp@@Base+0x34>  // b.tstop
   323dc:	lsl	x11, x2, #3
   323e0:	ldr	x12, [x8, x11]
   323e4:	ldr	x11, [x9, x11]
   323e8:	mov	x2, x10
   323ec:	cmp	x12, x11
   323f0:	b.eq	323d4 <__gmpn_cmp@@Base+0x8>  // b.none
   323f4:	mov	w8, #0x1                   	// #1
   323f8:	cneg	w0, w8, ls  // ls = plast
   323fc:	ret
   32400:	mov	w0, wzr
   32404:	ret

0000000000032408 <__gmpn_zero_p@@Base>:
   32408:	sub	x8, x0, #0x8
   3240c:	ldr	x9, [x8, x1, lsl #3]
   32410:	cbnz	x9, 32424 <__gmpn_zero_p@@Base+0x1c>
   32414:	sub	x1, x1, #0x1
   32418:	cbnz	x1, 3240c <__gmpn_zero_p@@Base+0x4>
   3241c:	mov	w0, #0x1                   	// #1
   32420:	ret
   32424:	mov	w0, wzr
   32428:	ret

000000000003242c <__gmpn_perfect_square_p@@Base>:
   3242c:	stp	x29, x30, [sp, #-32]!
   32430:	stp	x20, x19, [sp, #16]
   32434:	mov	x29, sp
   32438:	sub	sp, sp, #0x10
   3243c:	ldr	x8, [x0]
   32440:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   32444:	add	x10, x10, #0x570
   32448:	ubfx	x9, x8, #6, #2
   3244c:	ldr	x9, [x10, x9, lsl #3]
   32450:	lsr	x8, x9, x8
   32454:	tbz	w8, #0, 325d8 <__gmpn_perfect_square_p@@Base+0x1ac>
   32458:	mov	x19, x0
   3245c:	mov	x20, x1
   32460:	bl	cf60 <__gmpn_mod_34lsub1@plt>
   32464:	mov	x9, #0x2fd3                	// #12243
   32468:	and	x8, x0, #0xffffffffffff
   3246c:	movk	x9, #0xd2fd, lsl #16
   32470:	movk	x9, #0xfd2f, lsl #32
   32474:	add	x8, x8, x0, lsr #48
   32478:	mul	x9, x8, x9
   3247c:	mov	w10, #0x5b                  	// #91
   32480:	and	x9, x9, #0x1ffffffffffff
   32484:	mul	x9, x9, x10
   32488:	mov	x10, #0x20e1                	// #8417
   3248c:	movk	x10, #0x9538, lsl #16
   32490:	mov	w11, #0x1240                	// #4672
   32494:	lsr	x9, x9, #49
   32498:	movk	x10, #0xa206, lsl #32
   3249c:	movk	w11, #0x219, lsl #16
   324a0:	cmp	w9, #0x40
   324a4:	movk	x10, #0x8850, lsl #48
   324a8:	csel	x10, x10, x11, cc  // cc = lo, ul, last
   324ac:	lsr	x9, x10, x9
   324b0:	tbz	w9, #0, 325d8 <__gmpn_perfect_square_p@@Base+0x1ac>
   324b4:	mov	x9, #0xfcfd                	// #64765
   324b8:	movk	x9, #0xfcfc, lsl #16
   324bc:	movk	x9, #0xfcfc, lsl #32
   324c0:	mul	x9, x8, x9
   324c4:	mov	w10, #0x55                  	// #85
   324c8:	and	x9, x9, #0x1ffffffffffff
   324cc:	mul	x9, x9, x10
   324d0:	mov	x10, #0xa105                	// #41221
   324d4:	movk	x10, #0x4206, lsl #16
   324d8:	mov	w11, #0x2158                	// #8536
   324dc:	lsr	x9, x9, #49
   324e0:	movk	x10, #0x8c4b, lsl #32
   324e4:	movk	w11, #0x8, lsl #16
   324e8:	cmp	w9, #0x40
   324ec:	movk	x10, #0x10b4, lsl #48
   324f0:	csel	x10, x10, x11, cc  // cc = lo, ul, last
   324f4:	lsr	x9, x10, x9
   324f8:	tbz	w9, #0, 325d8 <__gmpn_perfect_square_p@@Base+0x1ac>
   324fc:	mov	x9, #0x8e39                	// #36409
   32500:	movk	x9, #0x38e3, lsl #16
   32504:	movk	x9, #0xe38e, lsl #32
   32508:	mul	x9, x8, x9
   3250c:	and	x9, x9, #0x1ffffffffffff
   32510:	add	x9, x9, x9, lsl #3
   32514:	lsr	x9, x9, #49
   32518:	mov	w10, #0x93                  	// #147
   3251c:	lsr	x9, x10, x9
   32520:	tbz	w9, #0, 325d8 <__gmpn_perfect_square_p@@Base+0x1ac>
   32524:	mov	x9, #0xa3a1                	// #41889
   32528:	movk	x9, #0x5f02, lsl #16
   3252c:	movk	x9, #0xfd5c, lsl #32
   32530:	mul	x8, x8, x9
   32534:	mov	w10, #0x61                  	// #97
   32538:	and	x8, x8, #0x1ffffffffffff
   3253c:	mov	x9, #0x1b5f                	// #7007
   32540:	mov	x11, #0x8b47                	// #35655
   32544:	mul	x8, x8, x10
   32548:	movk	x9, #0x8b45, lsl #16
   3254c:	movk	x11, #0xeb62, lsl #16
   32550:	lsr	x8, x8, #49
   32554:	movk	x9, #0x981b, lsl #32
   32558:	movk	x11, #0x1, lsl #32
   3255c:	cmp	w8, #0x40
   32560:	movk	x9, #0x6067, lsl #48
   32564:	csel	x9, x9, x11, cc  // cc = lo, ul, last
   32568:	lsr	x8, x9, x8
   3256c:	tbz	w8, #0, 325d8 <__gmpn_perfect_square_p@@Base+0x1ac>
   32570:	add	x8, x20, #0x1
   32574:	add	x9, x20, #0x2
   32578:	cmp	x8, #0x0
   3257c:	csinc	x8, x9, x20, lt  // lt = tstop
   32580:	lsl	x8, x8, #2
   32584:	and	x1, x8, #0xfffffffffffffff8
   32588:	mov	w8, #0x7f00                	// #32512
   3258c:	cmp	x1, x8
   32590:	stur	xzr, [x29, #-8]
   32594:	b.hi	325f0 <__gmpn_perfect_square_p@@Base+0x1c4>  // b.pmore
   32598:	add	x9, x1, #0xf
   3259c:	mov	x8, sp
   325a0:	and	x9, x9, #0xfffffffffffffff0
   325a4:	sub	x0, x8, x9
   325a8:	mov	sp, x0
   325ac:	mov	x1, xzr
   325b0:	mov	x2, x19
   325b4:	mov	x3, x20
   325b8:	bl	d3b0 <__gmpn_sqrtrem@plt>
   325bc:	ldur	x8, [x29, #-8]
   325c0:	cmp	x0, #0x0
   325c4:	cset	w19, eq  // eq = none
   325c8:	cbz	x8, 325dc <__gmpn_perfect_square_p@@Base+0x1b0>
   325cc:	mov	x0, x8
   325d0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   325d4:	b	325dc <__gmpn_perfect_square_p@@Base+0x1b0>
   325d8:	mov	w19, wzr
   325dc:	mov	w0, w19
   325e0:	mov	sp, x29
   325e4:	ldp	x20, x19, [sp, #16]
   325e8:	ldp	x29, x30, [sp], #32
   325ec:	ret
   325f0:	sub	x0, x29, #0x8
   325f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   325f8:	b	325ac <__gmpn_perfect_square_p@@Base+0x180>

00000000000325fc <__gmpn_perfect_power_p@@Base>:
   325fc:	stp	x29, x30, [sp, #-80]!
   32600:	stp	x26, x25, [sp, #16]
   32604:	stp	x24, x23, [sp, #32]
   32608:	stp	x22, x21, [sp, #48]
   3260c:	stp	x20, x19, [sp, #64]
   32610:	mov	x29, sp
   32614:	sub	sp, sp, #0x30
   32618:	mov	x19, x1
   3261c:	mov	x20, x0
   32620:	mov	x22, x1
   32624:	stur	x1, [x29, #-8]
   32628:	tbnz	x1, #63, 32634 <__gmpn_perfect_power_p@@Base+0x38>
   3262c:	cbnz	x22, 32640 <__gmpn_perfect_power_p@@Base+0x44>
   32630:	b	32654 <__gmpn_perfect_power_p@@Base+0x58>
   32634:	neg	x22, x19
   32638:	stur	x22, [x29, #-8]
   3263c:	cbz	x22, 32654 <__gmpn_perfect_power_p@@Base+0x58>
   32640:	cmp	x22, #0x1
   32644:	b.ne	3265c <__gmpn_perfect_power_p@@Base+0x60>  // b.any
   32648:	ldr	x8, [x20]
   3264c:	cmp	x8, #0x1
   32650:	b.ne	3265c <__gmpn_perfect_power_p@@Base+0x60>  // b.any
   32654:	mov	w19, #0x1                   	// #1
   32658:	b	328e0 <__gmpn_perfect_power_p@@Base+0x2e4>
   3265c:	mov	x0, x20
   32660:	mov	x1, xzr
   32664:	stur	xzr, [x29, #-40]
   32668:	bl	d150 <__gmpn_scan1@plt>
   3266c:	mov	x23, x0
   32670:	cbz	x0, 32684 <__gmpn_perfect_power_p@@Base+0x88>
   32674:	subs	x8, x23, #0x1
   32678:	b.ne	3268c <__gmpn_perfect_power_p@@Base+0x90>  // b.any
   3267c:	mov	w19, wzr
   32680:	b	328e0 <__gmpn_perfect_power_p@@Base+0x2e4>
   32684:	mov	x26, x23
   32688:	b	3273c <__gmpn_perfect_power_p@@Base+0x140>
   3268c:	lsr	x9, x23, #6
   32690:	add	x10, x9, #0x1
   32694:	cmp	x10, x22
   32698:	b.ne	326c4 <__gmpn_perfect_power_p@@Base+0xc8>  // b.any
   3269c:	ldr	x10, [x20, x9, lsl #3]
   326a0:	sub	x11, x10, #0x1
   326a4:	tst	x10, x11
   326a8:	b.ne	326c4 <__gmpn_perfect_power_p@@Base+0xc8>  // b.any
   326ac:	cmp	x19, #0x0
   326b0:	cset	w9, ge  // ge = tcont
   326b4:	tst	x23, x8
   326b8:	cset	w8, ne  // ne = any
   326bc:	orr	w19, w8, w9
   326c0:	b	328e0 <__gmpn_perfect_power_p@@Base+0x2e4>
   326c4:	and	x24, x23, #0x3f
   326c8:	sub	x22, x22, x9
   326cc:	add	x20, x20, x9, lsl #3
   326d0:	stur	x22, [x29, #-8]
   326d4:	cbz	x24, 32738 <__gmpn_perfect_power_p@@Base+0x13c>
   326d8:	lsl	x1, x22, #3
   326dc:	mov	w8, #0x7f00                	// #32512
   326e0:	cmp	x1, x8
   326e4:	b.hi	32908 <__gmpn_perfect_power_p@@Base+0x30c>  // b.pmore
   326e8:	add	x9, x1, #0xf
   326ec:	mov	x8, sp
   326f0:	and	x9, x9, #0xfffffffffffffff0
   326f4:	sub	x21, x8, x9
   326f8:	mov	sp, x21
   326fc:	mov	x0, x21
   32700:	mov	x1, x20
   32704:	mov	x2, x22
   32708:	mov	w3, w24
   3270c:	bl	c1a0 <__gmpn_rshift@plt>
   32710:	ldur	x8, [x29, #-8]
   32714:	mov	w26, #0x1                   	// #1
   32718:	mov	x20, x21
   3271c:	add	x9, x21, x8, lsl #3
   32720:	ldur	x9, [x9, #-8]
   32724:	cmp	x9, #0x0
   32728:	cset	w9, eq  // eq = none
   3272c:	sub	x22, x8, x9
   32730:	stur	x22, [x29, #-8]
   32734:	b	3273c <__gmpn_perfect_power_p@@Base+0x140>
   32738:	mov	x26, xzr
   3273c:	cmp	x22, #0x64
   32740:	mov	w8, #0x2                   	// #2
   32744:	cset	w9, gt
   32748:	csinc	x8, x8, xzr, gt
   3274c:	cmp	x22, #0x14
   32750:	csel	x25, x9, x8, le
   32754:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   32758:	add	x8, x8, #0x598
   3275c:	ldrh	w24, [x8, x25, lsl #1]
   32760:	sub	x3, x29, #0x1c
   32764:	mov	x0, x20
   32768:	mov	x1, x22
   3276c:	mov	x2, x24
   32770:	stur	x23, [x29, #-16]
   32774:	stur	wzr, [x29, #-28]
   32778:	bl	c610 <__gmpn_trialdiv@plt>
   3277c:	cbz	x0, 3286c <__gmpn_perfect_power_p@@Base+0x270>
   32780:	cbnz	x26, 327a8 <__gmpn_perfect_power_p@@Base+0x1ac>
   32784:	lsl	x1, x22, #3
   32788:	mov	w8, #0x7f00                	// #32512
   3278c:	cmp	x1, x8
   32790:	b.hi	3291c <__gmpn_perfect_power_p@@Base+0x320>  // b.pmore
   32794:	add	x9, x1, #0xf
   32798:	mov	x8, sp
   3279c:	and	x9, x9, #0xfffffffffffffff0
   327a0:	sub	x21, x8, x9
   327a4:	mov	sp, x21
   327a8:	adrp	x22, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   327ac:	ldr	x22, [x22, #3952]
   327b0:	mov	w23, #0x2                   	// #2
   327b4:	b	327d0 <__gmpn_perfect_power_p@@Base+0x1d4>
   327b8:	sub	x3, x29, #0x1c
   327bc:	mov	x0, x21
   327c0:	mov	x2, x24
   327c4:	bl	c610 <__gmpn_trialdiv@plt>
   327c8:	mov	x20, x21
   327cc:	cbz	x0, 3287c <__gmpn_perfect_power_p@@Base+0x280>
   327d0:	ubfx	x8, x0, #1, #7
   327d4:	ldrb	w8, [x22, x8]
   327d8:	ldur	x3, [x29, #-8]
   327dc:	sub	x1, x29, #0x8
   327e0:	sub	x4, x29, #0x18
   327e4:	msub	x9, x0, x8, x23
   327e8:	mul	x8, x9, x8
   327ec:	msub	x9, x8, x0, x23
   327f0:	mul	x8, x8, x9
   327f4:	msub	x9, x8, x0, x23
   327f8:	mul	x8, x8, x9
   327fc:	mov	w5, #0x1                   	// #1
   32800:	mov	x6, #0xffffffffffffffff    	// #-1
   32804:	mov	x0, x21
   32808:	mov	x2, x20
   3280c:	stur	x8, [x29, #-24]
   32810:	bl	cd40 <__gmpn_remove@plt>
   32814:	ldur	x8, [x29, #-16]
   32818:	mov	x2, x0
   3281c:	cbz	x8, 32830 <__gmpn_perfect_power_p@@Base+0x234>
   32820:	sub	x0, x29, #0x10
   32824:	mov	w1, #0x1                   	// #1
   32828:	bl	bf90 <__gmpn_gcd_1@plt>
   3282c:	mov	x2, x0
   32830:	subs	x8, x2, #0x1
   32834:	stur	x2, [x29, #-16]
   32838:	b.eq	32874 <__gmpn_perfect_power_p@@Base+0x278>  // b.none
   3283c:	ldur	x1, [x29, #-8]
   32840:	cmp	x1, #0x1
   32844:	b.ne	327b8 <__gmpn_perfect_power_p@@Base+0x1bc>  // b.any
   32848:	ldr	x9, [x21]
   3284c:	cmp	x9, #0x1
   32850:	b.ne	327b8 <__gmpn_perfect_power_p@@Base+0x1bc>  // b.any
   32854:	cmp	x19, #0x0
   32858:	cset	w9, ge  // ge = tcont
   3285c:	tst	x2, x8
   32860:	cset	w8, ne  // ne = any
   32864:	orr	w19, w8, w9
   32868:	b	328d8 <__gmpn_perfect_power_p@@Base+0x2dc>
   3286c:	mov	x21, x20
   32870:	b	32880 <__gmpn_perfect_power_p@@Base+0x284>
   32874:	mov	w19, wzr
   32878:	b	328d8 <__gmpn_perfect_power_p@@Base+0x2dc>
   3287c:	ldp	x23, x22, [x29, #-16]
   32880:	add	x8, x21, x22, lsl #3
   32884:	ldur	x8, [x8, #-8]
   32888:	adrp	x9, 5d000 <__gmpn_bases@@Base+0x2ab8>
   3288c:	add	x9, x9, #0x5a0
   32890:	ldr	d0, [x9, x25, lsl #3]
   32894:	adrp	x10, 5d000 <__gmpn_bases@@Base+0x2ab8>
   32898:	lsl	x9, x22, #6
   3289c:	ldr	d1, [x10, #1424]
   328a0:	clz	x8, x8
   328a4:	sub	x4, x9, x8
   328a8:	ucvtf	d2, x4
   328ac:	fmul	d0, d0, d2
   328b0:	fadd	d0, d0, d1
   328b4:	fcvtzu	x8, d0
   328b8:	lsr	x5, x19, #63
   328bc:	add	x2, x8, #0x1
   328c0:	mov	x0, x21
   328c4:	mov	x1, x22
   328c8:	mov	x3, x23
   328cc:	stur	x2, [x29, #-24]
   328d0:	bl	32934 <__gmpn_perfect_power_p@@Base+0x338>
   328d4:	mov	w19, w0
   328d8:	ldur	x0, [x29, #-40]
   328dc:	cbnz	x0, 32900 <__gmpn_perfect_power_p@@Base+0x304>
   328e0:	mov	w0, w19
   328e4:	mov	sp, x29
   328e8:	ldp	x20, x19, [sp, #64]
   328ec:	ldp	x22, x21, [sp, #48]
   328f0:	ldp	x24, x23, [sp, #32]
   328f4:	ldp	x26, x25, [sp, #16]
   328f8:	ldp	x29, x30, [sp], #80
   328fc:	ret
   32900:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32904:	b	328e0 <__gmpn_perfect_power_p@@Base+0x2e4>
   32908:	sub	x0, x29, #0x28
   3290c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32910:	ldur	x22, [x29, #-8]
   32914:	mov	x21, x0
   32918:	b	326fc <__gmpn_perfect_power_p@@Base+0x100>
   3291c:	mov	x22, x0
   32920:	sub	x0, x29, #0x28
   32924:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32928:	mov	x21, x0
   3292c:	mov	x0, x22
   32930:	b	327a8 <__gmpn_perfect_power_p@@Base+0x1ac>
   32934:	stp	x29, x30, [sp, #-96]!
   32938:	stp	x28, x27, [sp, #16]
   3293c:	stp	x26, x25, [sp, #32]
   32940:	stp	x24, x23, [sp, #48]
   32944:	stp	x22, x21, [sp, #64]
   32948:	stp	x20, x19, [sp, #80]
   3294c:	mov	x29, sp
   32950:	sub	sp, sp, #0x240
   32954:	mov	x19, sp
   32958:	mov	x22, x0
   3295c:	add	x0, x19, #0x18
   32960:	str	w5, [x19, #12]
   32964:	mov	x20, x4
   32968:	mov	x23, x3
   3296c:	mov	x25, x2
   32970:	mov	x21, x1
   32974:	str	xzr, [x19, #16]
   32978:	bl	d1a0 <__gmp_init_primesieve@plt>
   3297c:	mov	w8, #0x38                  	// #56
   32980:	mul	x1, x21, x8
   32984:	mov	w8, #0x7f00                	// #32512
   32988:	cmp	x1, x8
   3298c:	add	x26, x20, #0x3
   32990:	b.hi	32afc <__gmpn_perfect_power_p@@Base+0x500>  // b.pmore
   32994:	add	x9, x1, #0xf
   32998:	mov	x8, sp
   3299c:	and	x9, x9, #0xfffffffffffffff0
   329a0:	sub	x24, x8, x9
   329a4:	mov	sp, x24
   329a8:	lsl	x2, x21, #3
   329ac:	str	x26, [x19]
   329b0:	lsr	x28, x26, #1
   329b4:	add	x26, x24, x2
   329b8:	add	x27, x26, x2
   329bc:	cbz	x21, 329cc <__gmpn_perfect_power_p@@Base+0x3d0>
   329c0:	mov	x0, x26
   329c4:	mov	w1, wzr
   329c8:	bl	c5f0 <memset@plt>
   329cc:	sub	x8, x28, #0x1
   329d0:	lsr	x28, x8, #6
   329d4:	add	x2, x28, #0x1
   329d8:	mov	x0, x24
   329dc:	mov	x1, x22
   329e0:	mov	x3, x27
   329e4:	bl	cd20 <__gmpn_binvert@plt>
   329e8:	ldr	x8, [x19]
   329ec:	ubfx	x8, x8, #1, #6
   329f0:	cbz	x8, 32a0c <__gmpn_perfect_power_p@@Base+0x410>
   329f4:	lsl	x9, x28, #3
   329f8:	ldr	x10, [x24, x9]
   329fc:	mov	x11, #0xffffffffffffffff    	// #-1
   32a00:	lsl	x8, x11, x8
   32a04:	bic	x8, x10, x8
   32a08:	str	x8, [x24, x9]
   32a0c:	ldr	w8, [x19, #12]
   32a10:	cbz	w8, 32a1c <__gmpn_perfect_power_p@@Base+0x420>
   32a14:	add	x0, x19, #0x18
   32a18:	bl	cc80 <__gmp_nextprime@plt>
   32a1c:	cbz	x23, 32abc <__gmpn_perfect_power_p@@Base+0x4c0>
   32a20:	add	x8, x23, #0x1
   32a24:	cmp	x8, x25
   32a28:	add	x0, x19, #0x18
   32a2c:	csinc	x25, x25, x23, hi  // hi = pmore
   32a30:	bl	cc80 <__gmp_nextprime@plt>
   32a34:	cmp	x0, x25
   32a38:	b.cs	32acc <__gmpn_perfect_power_p@@Base+0x4d0>  // b.hs, b.nlast
   32a3c:	mov	x2, x0
   32a40:	b	32a58 <__gmpn_perfect_power_p@@Base+0x45c>
   32a44:	add	x0, x19, #0x18
   32a48:	bl	cc80 <__gmp_nextprime@plt>
   32a4c:	mov	x2, x0
   32a50:	cmp	x0, x25
   32a54:	b.cs	32acc <__gmpn_perfect_power_p@@Base+0x4d0>  // b.hs, b.nlast
   32a58:	udiv	x8, x23, x2
   32a5c:	msub	x8, x8, x2, x23
   32a60:	cbnz	x8, 32a44 <__gmpn_perfect_power_p@@Base+0x448>
   32a64:	mov	x0, x26
   32a68:	mov	x1, x22
   32a6c:	mov	x3, x24
   32a70:	mov	x4, x21
   32a74:	mov	x5, x20
   32a78:	mov	x6, x27
   32a7c:	bl	32b0c <__gmpn_perfect_power_p@@Base+0x510>
   32a80:	cbz	w0, 32a44 <__gmpn_perfect_power_p@@Base+0x448>
   32a84:	mov	w20, #0x1                   	// #1
   32a88:	ldr	x0, [x19, #16]
   32a8c:	cbz	x0, 32ad8 <__gmpn_perfect_power_p@@Base+0x4dc>
   32a90:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32a94:	b	32ad8 <__gmpn_perfect_power_p@@Base+0x4dc>
   32a98:	mov	x2, x0
   32a9c:	mov	x0, x26
   32aa0:	mov	x1, x22
   32aa4:	mov	x3, x24
   32aa8:	mov	x4, x21
   32aac:	mov	x5, x20
   32ab0:	mov	x6, x27
   32ab4:	bl	32b0c <__gmpn_perfect_power_p@@Base+0x510>
   32ab8:	cbnz	w0, 32a84 <__gmpn_perfect_power_p@@Base+0x488>
   32abc:	add	x0, x19, #0x18
   32ac0:	bl	cc80 <__gmp_nextprime@plt>
   32ac4:	cmp	x0, x25
   32ac8:	b.cc	32a98 <__gmpn_perfect_power_p@@Base+0x49c>  // b.lo, b.ul, b.last
   32acc:	mov	w20, wzr
   32ad0:	ldr	x0, [x19, #16]
   32ad4:	cbnz	x0, 32a90 <__gmpn_perfect_power_p@@Base+0x494>
   32ad8:	mov	w0, w20
   32adc:	mov	sp, x29
   32ae0:	ldp	x20, x19, [sp, #80]
   32ae4:	ldp	x22, x21, [sp, #64]
   32ae8:	ldp	x24, x23, [sp, #48]
   32aec:	ldp	x26, x25, [sp, #32]
   32af0:	ldp	x28, x27, [sp, #16]
   32af4:	ldp	x29, x30, [sp], #96
   32af8:	ret
   32afc:	add	x0, x19, #0x10
   32b00:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32b04:	mov	x24, x0
   32b08:	b	329a8 <__gmpn_perfect_power_p@@Base+0x3ac>
   32b0c:	stp	x29, x30, [sp, #-96]!
   32b10:	stp	x24, x23, [sp, #48]
   32b14:	stp	x22, x21, [sp, #64]
   32b18:	stp	x20, x19, [sp, #80]
   32b1c:	mov	x20, x6
   32b20:	mov	x21, x5
   32b24:	mov	x22, x4
   32b28:	mov	x23, x1
   32b2c:	cmp	x2, #0x2
   32b30:	mov	x19, x0
   32b34:	str	x27, [sp, #16]
   32b38:	stp	x26, x25, [sp, #32]
   32b3c:	mov	x29, sp
   32b40:	b.ne	32bac <__gmpn_perfect_power_p@@Base+0x5b0>  // b.any
   32b44:	add	x8, x21, #0x1
   32b48:	lsr	x25, x8, #1
   32b4c:	lsr	x26, x8, #7
   32b50:	mov	x0, x19
   32b54:	mov	x1, x3
   32b58:	mov	x2, x25
   32b5c:	mov	x3, x20
   32b60:	add	x24, x26, #0x1
   32b64:	bl	c750 <__gmpn_bsqrtinv@plt>
   32b68:	cbz	w0, 32c40 <__gmpn_perfect_power_p@@Base+0x644>
   32b6c:	lsl	x27, x26, #3
   32b70:	ldr	x8, [x19, x27]
   32b74:	mov	x9, #0xffffffffffffffff    	// #-1
   32b78:	lsl	x9, x9, x25
   32b7c:	mvn	x25, x9
   32b80:	bic	x8, x8, x9
   32b84:	str	x8, [x19, x27]
   32b88:	mov	x8, x26
   32b8c:	add	x9, x8, #0x1
   32b90:	cmp	x9, #0x1
   32b94:	b.lt	32c58 <__gmpn_perfect_power_p@@Base+0x65c>  // b.tstop
   32b98:	ldr	x9, [x19, x8, lsl #3]
   32b9c:	sub	x8, x8, #0x1
   32ba0:	cbz	x9, 32b8c <__gmpn_perfect_power_p@@Base+0x590>
   32ba4:	add	x3, x8, #0x2
   32ba8:	b	32c5c <__gmpn_perfect_power_p@@Base+0x660>
   32bac:	sub	x8, x21, #0x1
   32bb0:	udiv	x8, x8, x2
   32bb4:	lsr	x24, x8, #6
   32bb8:	mov	x25, x2
   32bbc:	add	x26, x24, #0x1
   32bc0:	mov	x0, x19
   32bc4:	mov	x1, x3
   32bc8:	mov	x2, x26
   32bcc:	mov	x3, x25
   32bd0:	mov	x4, x20
   32bd4:	add	w27, w8, #0x1
   32bd8:	bl	c6b0 <__gmpn_brootinv@plt>
   32bdc:	ands	x8, x27, #0x3f
   32be0:	b.eq	32bfc <__gmpn_perfect_power_p@@Base+0x600>  // b.none
   32be4:	lsl	x9, x24, #3
   32be8:	ldr	x10, [x19, x9]
   32bec:	mov	x11, #0xffffffffffffffff    	// #-1
   32bf0:	lsl	x8, x11, x8
   32bf4:	bic	x8, x10, x8
   32bf8:	str	x8, [x19, x9]
   32bfc:	mov	x24, x26
   32c00:	subs	x26, x26, #0x1
   32c04:	b.lt	32c14 <__gmpn_perfect_power_p@@Base+0x618>  // b.tstop
   32c08:	ldr	x8, [x19, x26, lsl #3]
   32c0c:	cbz	x8, 32bfc <__gmpn_perfect_power_p@@Base+0x600>
   32c10:	b	32c18 <__gmpn_perfect_power_p@@Base+0x61c>
   32c14:	mov	x24, xzr
   32c18:	mov	x0, x23
   32c1c:	mov	x1, x22
   32c20:	mov	x2, x19
   32c24:	mov	x3, x24
   32c28:	mov	x4, x25
   32c2c:	mov	x5, x21
   32c30:	mov	x6, x20
   32c34:	bl	32d34 <__gmpn_perfect_power_p@@Base+0x738>
   32c38:	cbnz	w0, 32d14 <__gmpn_perfect_power_p@@Base+0x718>
   32c3c:	cbz	x24, 32c50 <__gmpn_perfect_power_p@@Base+0x654>
   32c40:	lsl	x2, x24, #3
   32c44:	mov	x0, x19
   32c48:	mov	w1, wzr
   32c4c:	bl	c5f0 <memset@plt>
   32c50:	mov	w0, wzr
   32c54:	b	32d18 <__gmpn_perfect_power_p@@Base+0x71c>
   32c58:	mov	x3, xzr
   32c5c:	mov	w4, #0x2                   	// #2
   32c60:	mov	x0, x23
   32c64:	mov	x1, x22
   32c68:	mov	x2, x19
   32c6c:	mov	x5, x21
   32c70:	mov	x6, x20
   32c74:	bl	32d34 <__gmpn_perfect_power_p@@Base+0x738>
   32c78:	cbnz	w0, 32d14 <__gmpn_perfect_power_p@@Base+0x718>
   32c7c:	ldr	x9, [x19]
   32c80:	cbz	x9, 32c8c <__gmpn_perfect_power_p@@Base+0x690>
   32c84:	mov	x8, x19
   32c88:	b	32ca4 <__gmpn_perfect_power_p@@Base+0x6a8>
   32c8c:	mov	x8, x19
   32c90:	subs	x24, x24, #0x1
   32c94:	str	xzr, [x8]
   32c98:	b.eq	32cc0 <__gmpn_perfect_power_p@@Base+0x6c4>  // b.none
   32c9c:	ldr	x9, [x8, #8]!
   32ca0:	cbz	x9, 32c90 <__gmpn_perfect_power_p@@Base+0x694>
   32ca4:	neg	x9, x9
   32ca8:	subs	x2, x24, #0x1
   32cac:	str	x9, [x8]
   32cb0:	b.eq	32cc0 <__gmpn_perfect_power_p@@Base+0x6c4>  // b.none
   32cb4:	add	x0, x8, #0x8
   32cb8:	mov	x1, x0
   32cbc:	bl	c290 <__gmpn_com@plt>
   32cc0:	ldr	x8, [x19, x27]
   32cc4:	and	x8, x8, x25
   32cc8:	str	x8, [x19, x27]
   32ccc:	add	x8, x26, #0x1
   32cd0:	cmp	x8, #0x1
   32cd4:	b.lt	32cec <__gmpn_perfect_power_p@@Base+0x6f0>  // b.tstop
   32cd8:	ldr	x8, [x19, x26, lsl #3]
   32cdc:	sub	x26, x26, #0x1
   32ce0:	cbz	x8, 32ccc <__gmpn_perfect_power_p@@Base+0x6d0>
   32ce4:	add	x24, x26, #0x2
   32ce8:	b	32cf0 <__gmpn_perfect_power_p@@Base+0x6f4>
   32cec:	mov	x24, xzr
   32cf0:	mov	w4, #0x2                   	// #2
   32cf4:	mov	x0, x23
   32cf8:	mov	x1, x22
   32cfc:	mov	x2, x19
   32d00:	mov	x3, x24
   32d04:	mov	x5, x21
   32d08:	mov	x6, x20
   32d0c:	bl	32d34 <__gmpn_perfect_power_p@@Base+0x738>
   32d10:	cbz	w0, 32c3c <__gmpn_perfect_power_p@@Base+0x640>
   32d14:	mov	w0, #0x1                   	// #1
   32d18:	ldp	x20, x19, [sp, #80]
   32d1c:	ldp	x22, x21, [sp, #64]
   32d20:	ldp	x24, x23, [sp, #48]
   32d24:	ldp	x26, x25, [sp, #32]
   32d28:	ldr	x27, [sp, #16]
   32d2c:	ldp	x29, x30, [sp], #96
   32d30:	ret
   32d34:	stp	x29, x30, [sp, #-96]!
   32d38:	stp	x28, x27, [sp, #16]
   32d3c:	stp	x26, x25, [sp, #32]
   32d40:	stp	x24, x23, [sp, #48]
   32d44:	stp	x22, x21, [sp, #64]
   32d48:	stp	x20, x19, [sp, #80]
   32d4c:	mov	x29, sp
   32d50:	sub	sp, sp, #0x10
   32d54:	mov	x20, x6
   32d58:	mov	x24, x5
   32d5c:	mov	x22, x3
   32d60:	mov	x23, x2
   32d64:	mov	x19, x1
   32d68:	mov	x21, x0
   32d6c:	cmp	x3, #0x1
   32d70:	stur	x4, [x29, #-8]
   32d74:	b.ne	32dac <__gmpn_perfect_power_p@@Base+0x7b0>  // b.any
   32d78:	ldr	x8, [x23]
   32d7c:	cmp	x8, #0x1
   32d80:	b.ne	32dac <__gmpn_perfect_power_p@@Base+0x7b0>  // b.any
   32d84:	mov	w24, wzr
   32d88:	mov	w0, w24
   32d8c:	mov	sp, x29
   32d90:	ldp	x20, x19, [sp, #80]
   32d94:	ldp	x22, x21, [sp, #64]
   32d98:	ldp	x24, x23, [sp, #48]
   32d9c:	ldp	x26, x25, [sp, #32]
   32da0:	ldp	x28, x27, [sp, #16]
   32da4:	ldp	x29, x30, [sp], #96
   32da8:	ret
   32dac:	asr	x8, x19, #1
   32db0:	add	x26, x8, #0x1
   32db4:	cmp	x26, #0x2
   32db8:	b.cc	32e1c <__gmpn_perfect_power_p@@Base+0x820>  // b.lo, b.ul, b.last
   32dbc:	sub	x27, x21, #0x8
   32dc0:	sub	x28, x20, #0x8
   32dc4:	mov	w25, #0x1                   	// #1
   32dc8:	add	x5, x20, x25, lsl #3
   32dcc:	sub	x2, x29, #0x8
   32dd0:	mov	w3, #0x1                   	// #1
   32dd4:	mov	x0, x20
   32dd8:	mov	x1, x23
   32ddc:	mov	x4, x25
   32de0:	bl	c3c0 <__gmpn_powlo@plt>
   32de4:	mov	x8, x25
   32de8:	subs	x9, x8, #0x1
   32dec:	b.lt	32e0c <__gmpn_perfect_power_p@@Base+0x810>  // b.tstop
   32df0:	lsl	x8, x8, #3
   32df4:	ldr	x10, [x28, x8]
   32df8:	ldr	x8, [x27, x8]
   32dfc:	cmp	x10, x8
   32e00:	mov	x8, x9
   32e04:	b.eq	32de8 <__gmpn_perfect_power_p@@Base+0x7ec>  // b.none
   32e08:	b	32d84 <__gmpn_perfect_power_p@@Base+0x788>
   32e0c:	lsl	x25, x25, #1
   32e10:	cmp	x25, x26
   32e14:	b.cc	32dc8 <__gmpn_perfect_power_p@@Base+0x7cc>  // b.lo, b.ul, b.last
   32e18:	ldur	x4, [x29, #-8]
   32e1c:	add	x8, x23, x22, lsl #3
   32e20:	ldur	x8, [x8, #-8]
   32e24:	sub	x11, x24, #0x1
   32e28:	mov	w24, wzr
   32e2c:	clz	x8, x8
   32e30:	mvn	x8, x8
   32e34:	add	x10, x8, x22, lsl #6
   32e38:	mul	x8, x10, x4
   32e3c:	cmp	x8, #0x0
   32e40:	sub	x8, x8, #0x1
   32e44:	cset	w9, eq  // eq = none
   32e48:	cmp	x8, x11
   32e4c:	b.hi	32d88 <__gmpn_perfect_power_p@@Base+0x78c>  // b.pmore
   32e50:	umulh	x10, x4, x10
   32e54:	cmp	x10, x9
   32e58:	b.ne	32d88 <__gmpn_perfect_power_p@@Base+0x78c>  // b.any
   32e5c:	adds	x8, x8, x4
   32e60:	b.cs	32f10 <__gmpn_perfect_power_p@@Base+0x914>  // b.hs, b.nlast
   32e64:	lsr	x8, x8, #6
   32e68:	lsl	x9, x8, #3
   32e6c:	cmp	x8, #0xfde
   32e70:	add	x1, x9, #0x10
   32e74:	stur	xzr, [x29, #-16]
   32e78:	b.hi	32efc <__gmpn_perfect_power_p@@Base+0x900>  // b.pmore
   32e7c:	add	x9, x1, #0xf
   32e80:	mov	x8, sp
   32e84:	and	x9, x9, #0x7ffffffffffffff0
   32e88:	sub	x8, x8, x9
   32e8c:	mov	sp, x8
   32e90:	mov	x0, x20
   32e94:	mov	x1, x23
   32e98:	mov	x2, x22
   32e9c:	mov	x3, x4
   32ea0:	mov	x4, x8
   32ea4:	bl	d210 <__gmpn_pow_1@plt>
   32ea8:	cmp	x0, x19
   32eac:	b.ne	32ed8 <__gmpn_perfect_power_p@@Base+0x8dc>  // b.any
   32eb0:	sub	x8, x21, #0x8
   32eb4:	sub	x9, x20, #0x8
   32eb8:	subs	x10, x19, #0x1
   32ebc:	b.lt	32ee8 <__gmpn_perfect_power_p@@Base+0x8ec>  // b.tstop
   32ec0:	lsl	x11, x19, #3
   32ec4:	ldr	x12, [x9, x11]
   32ec8:	ldr	x11, [x8, x11]
   32ecc:	mov	x19, x10
   32ed0:	cmp	x12, x11
   32ed4:	b.eq	32eb8 <__gmpn_perfect_power_p@@Base+0x8bc>  // b.none
   32ed8:	mov	w24, wzr
   32edc:	ldur	x0, [x29, #-16]
   32ee0:	cbz	x0, 32d88 <__gmpn_perfect_power_p@@Base+0x78c>
   32ee4:	b	32ef4 <__gmpn_perfect_power_p@@Base+0x8f8>
   32ee8:	mov	w24, #0x1                   	// #1
   32eec:	ldur	x0, [x29, #-16]
   32ef0:	cbz	x0, 32d88 <__gmpn_perfect_power_p@@Base+0x78c>
   32ef4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32ef8:	b	32d88 <__gmpn_perfect_power_p@@Base+0x78c>
   32efc:	sub	x0, x29, #0x10
   32f00:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32f04:	ldur	x4, [x29, #-8]
   32f08:	mov	x8, x0
   32f0c:	b	32e90 <__gmpn_perfect_power_p@@Base+0x894>
   32f10:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   32f14:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   32f18:	add	x0, x0, #0x5b8
   32f1c:	add	x2, x2, #0x5c2
   32f20:	mov	w1, #0x60                  	// #96
   32f24:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000032f28 <__gmpn_strongfibo@@Base>:
   32f28:	stp	x29, x30, [sp, #-96]!
   32f2c:	stp	x28, x27, [sp, #16]
   32f30:	stp	x26, x25, [sp, #32]
   32f34:	stp	x24, x23, [sp, #48]
   32f38:	stp	x22, x21, [sp, #64]
   32f3c:	stp	x20, x19, [sp, #80]
   32f40:	mov	x29, sp
   32f44:	sub	sp, sp, #0x10
   32f48:	mov	x20, x1
   32f4c:	mov	x1, xzr
   32f50:	mov	x25, x2
   32f54:	mov	x19, x0
   32f58:	bl	c690 <__gmpn_scan0@plt>
   32f5c:	lsr	x8, x0, #6
   32f60:	mov	x23, x0
   32f64:	sub	x21, x20, x8
   32f68:	and	w3, w23, #0x3f
   32f6c:	add	x1, x19, x8, lsl #3
   32f70:	mov	x0, x25
   32f74:	mov	x2, x21
   32f78:	cbz	w3, 332c8 <__gmpn_strongfibo@@Base+0x3a0>
   32f7c:	bl	c1a0 <__gmpn_rshift@plt>
   32f80:	ldr	x8, [x25]
   32f84:	add	x9, x25, x21, lsl #3
   32f88:	mov	w10, #0x7f00                	// #32512
   32f8c:	orr	x8, x8, #0x1
   32f90:	str	x8, [x25]
   32f94:	ldur	x8, [x9, #-8]
   32f98:	lsl	x9, x20, #5
   32f9c:	add	x1, x9, #0x30
   32fa0:	stur	xzr, [x29, #-8]
   32fa4:	cmp	x8, #0x0
   32fa8:	cset	w8, eq  // eq = none
   32fac:	cmp	x1, x10
   32fb0:	sub	x26, x21, x8
   32fb4:	b.hi	332d0 <__gmpn_strongfibo@@Base+0x3a8>  // b.pmore
   32fb8:	add	x9, x1, #0xf
   32fbc:	mov	x8, sp
   32fc0:	and	x9, x9, #0xfffffffffffffff0
   32fc4:	sub	x21, x8, x9
   32fc8:	mov	sp, x21
   32fcc:	add	x8, x21, x20, lsl #4
   32fd0:	add	x24, x8, #0x18
   32fd4:	mov	x0, x24
   32fd8:	mov	x1, x21
   32fdc:	mov	x2, x25
   32fe0:	mov	x3, x26
   32fe4:	mov	x4, x19
   32fe8:	mov	x5, x20
   32fec:	lsl	x22, x20, #1
   32ff0:	bl	d050 <__gmpn_fib2m@plt>
   32ff4:	mov	w9, #0x18                  	// #24
   32ff8:	madd	x9, x20, x9, x21
   32ffc:	mov	x8, xzr
   33000:	add	x9, x9, #0x10
   33004:	ldr	x10, [x9, x8, lsl #3]
   33008:	cbnz	x10, 3301c <__gmpn_strongfibo@@Base+0xf4>
   3300c:	sub	x8, x8, #0x1
   33010:	cmn	x20, x8
   33014:	b.ne	33004 <__gmpn_strongfibo@@Base+0xdc>  // b.any
   33018:	b	33298 <__gmpn_strongfibo@@Base+0x370>
   3301c:	cbz	w0, 3305c <__gmpn_strongfibo@@Base+0x134>
   33020:	mov	x0, x24
   33024:	mov	x1, x24
   33028:	mov	x2, x21
   3302c:	mov	x3, x20
   33030:	bl	d090 <__gmpn_rsblsh1_n@plt>
   33034:	mov	x25, x0
   33038:	cmp	x0, #0x2
   3303c:	b.cc	33074 <__gmpn_strongfibo@@Base+0x14c>  // b.lo, b.ul, b.last
   33040:	mov	x0, x24
   33044:	mov	x1, x24
   33048:	mov	x2, x19
   3304c:	mov	x3, x20
   33050:	bl	ca70 <__gmpn_add_n@plt>
   33054:	add	x25, x0, x25
   33058:	b	33074 <__gmpn_strongfibo@@Base+0x14c>
   3305c:	mov	x0, x24
   33060:	mov	x1, x24
   33064:	mov	x2, x21
   33068:	mov	x3, x20
   3306c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   33070:	mov	x25, x0
   33074:	mov	w8, #0x18                  	// #24
   33078:	madd	x8, x20, x8, x21
   3307c:	add	x26, x8, #0x10
   33080:	b	3309c <__gmpn_strongfibo@@Base+0x174>
   33084:	mov	x0, x24
   33088:	mov	x1, x24
   3308c:	mov	x2, x19
   33090:	mov	x3, x20
   33094:	bl	c2d0 <__gmpn_sub_n@plt>
   33098:	sub	x25, x25, x0
   3309c:	cbnz	x25, 33084 <__gmpn_strongfibo@@Base+0x15c>
   330a0:	mov	x8, x26
   330a4:	mov	x9, x20
   330a8:	subs	x10, x9, #0x1
   330ac:	b.lt	33084 <__gmpn_strongfibo@@Base+0x15c>  // b.tstop
   330b0:	add	x9, x19, x9, lsl #3
   330b4:	ldr	x11, [x8], #-8
   330b8:	ldur	x9, [x9, #-8]
   330bc:	cmp	x11, x9
   330c0:	mov	x9, x10
   330c4:	b.eq	330a8 <__gmpn_strongfibo@@Base+0x180>  // b.none
   330c8:	b.hi	33084 <__gmpn_strongfibo@@Base+0x15c>  // b.pmore
   330cc:	mov	w8, #0x18                  	// #24
   330d0:	madd	x8, x20, x8, x21
   330d4:	mov	x10, xzr
   330d8:	neg	x11, x20, lsl #3
   330dc:	add	x9, x8, #0x10
   330e0:	mov	x12, x22
   330e4:	mov	x8, x10
   330e8:	add	x10, x20, x10
   330ec:	cmp	x10, #0x1
   330f0:	mov	x25, x12
   330f4:	mov	x26, x11
   330f8:	b.lt	33114 <__gmpn_strongfibo@@Base+0x1ec>  // b.tstop
   330fc:	ldr	x13, [x9, x8, lsl #3]
   33100:	sub	x12, x25, #0x2
   33104:	sub	x10, x8, #0x1
   33108:	add	x11, x26, #0x10
   3310c:	cbz	x13, 330e4 <__gmpn_strongfibo@@Base+0x1bc>
   33110:	b	3311c <__gmpn_strongfibo@@Base+0x1f4>
   33114:	cmn	x20, x8
   33118:	b.eq	33298 <__gmpn_strongfibo@@Base+0x370>  // b.none
   3311c:	cmp	x23, #0x1
   33120:	b.eq	331f0 <__gmpn_strongfibo@@Base+0x2c8>  // b.none
   33124:	add	x2, x20, x8
   33128:	mov	x0, x21
   3312c:	mov	x1, x24
   33130:	bl	c8e0 <__gmpn_sqr@plt>
   33134:	ldr	x8, [x21]
   33138:	cmp	x25, x20
   3313c:	orr	x8, x8, #0x2
   33140:	str	x8, [x21]
   33144:	b.lt	332e8 <__gmpn_strongfibo@@Base+0x3c0>  // b.tstop
   33148:	mov	x0, x24
   3314c:	mov	x1, x21
   33150:	mov	x2, xzr
   33154:	mov	x3, x21
   33158:	mov	x4, x25
   3315c:	mov	x5, x19
   33160:	mov	x6, x20
   33164:	bl	bf00 <__gmpn_tdiv_qr@plt>
   33168:	sub	x8, x21, #0x8
   3316c:	mov	x9, x20
   33170:	ldr	x10, [x8, x9, lsl #3]
   33174:	cbnz	x10, 33188 <__gmpn_strongfibo@@Base+0x260>
   33178:	sub	x9, x9, #0x1
   3317c:	cbnz	x9, 33170 <__gmpn_strongfibo@@Base+0x248>
   33180:	mov	w23, #0x1                   	// #1
   33184:	b	33298 <__gmpn_strongfibo@@Base+0x370>
   33188:	subs	x26, x23, #0x2
   3318c:	b.eq	331f0 <__gmpn_strongfibo@@Base+0x2c8>  // b.none
   33190:	add	x8, x21, x20, lsl #3
   33194:	add	x24, x8, #0x8
   33198:	subs	x27, x20, #0x1
   3319c:	add	x25, x24, x22, lsl #3
   331a0:	b.ne	331f8 <__gmpn_strongfibo@@Base+0x2d0>  // b.any
   331a4:	mov	w2, #0x1                   	// #1
   331a8:	mov	x0, x24
   331ac:	mov	x1, x21
   331b0:	bl	c8e0 <__gmpn_sqr@plt>
   331b4:	mov	w6, #0x1                   	// #1
   331b8:	mov	x0, x25
   331bc:	mov	x1, x21
   331c0:	mov	x2, xzr
   331c4:	mov	x3, x24
   331c8:	mov	x4, x22
   331cc:	mov	x5, x19
   331d0:	bl	bf00 <__gmpn_tdiv_qr@plt>
   331d4:	ldr	x8, [x21]
   331d8:	cmp	x8, #0x5
   331dc:	b.cc	33290 <__gmpn_strongfibo@@Base+0x368>  // b.lo, b.ul, b.last
   331e0:	sub	x8, x8, #0x2
   331e4:	subs	x26, x26, #0x1
   331e8:	str	x8, [x21]
   331ec:	b.ne	331a4 <__gmpn_strongfibo@@Base+0x27c>  // b.any
   331f0:	mov	x23, xzr
   331f4:	b	33298 <__gmpn_strongfibo@@Base+0x370>
   331f8:	add	x28, x21, #0x8
   331fc:	mov	x0, x24
   33200:	mov	x1, x21
   33204:	mov	x2, x20
   33208:	bl	c8e0 <__gmpn_sqr@plt>
   3320c:	mov	x0, x25
   33210:	mov	x1, x21
   33214:	mov	x2, xzr
   33218:	mov	x3, x24
   3321c:	mov	x4, x22
   33220:	mov	x5, x19
   33224:	mov	x6, x20
   33228:	bl	bf00 <__gmpn_tdiv_qr@plt>
   3322c:	ldr	x8, [x21]
   33230:	cmp	x8, #0x4
   33234:	b.hi	33250 <__gmpn_strongfibo@@Base+0x328>  // b.pmore
   33238:	mov	x9, x27
   3323c:	ldr	x10, [x21, x9, lsl #3]
   33240:	cbnz	x10, 3325c <__gmpn_strongfibo@@Base+0x334>
   33244:	sub	x9, x9, #0x1
   33248:	cbnz	x9, 3323c <__gmpn_strongfibo@@Base+0x314>
   3324c:	b	33290 <__gmpn_strongfibo@@Base+0x368>
   33250:	sub	x8, x8, #0x2
   33254:	str	x8, [x21]
   33258:	b	33280 <__gmpn_strongfibo@@Base+0x358>
   3325c:	sub	x9, x8, #0x2
   33260:	cmp	x8, #0x1
   33264:	str	x9, [x21]
   33268:	b.hi	33280 <__gmpn_strongfibo@@Base+0x358>  // b.pmore
   3326c:	mov	x8, x28
   33270:	ldr	x9, [x8]
   33274:	sub	x10, x9, #0x1
   33278:	str	x10, [x8], #8
   3327c:	cbz	x9, 33270 <__gmpn_strongfibo@@Base+0x348>
   33280:	subs	x26, x26, #0x1
   33284:	mov	x23, xzr
   33288:	b.ne	331fc <__gmpn_strongfibo@@Base+0x2d4>  // b.any
   3328c:	b	33298 <__gmpn_strongfibo@@Base+0x370>
   33290:	cmp	x8, #0x2
   33294:	csel	x23, x26, xzr, eq  // eq = none
   33298:	ldur	x0, [x29, #-8]
   3329c:	cbnz	x0, 332e0 <__gmpn_strongfibo@@Base+0x3b8>
   332a0:	cmp	x23, #0x0
   332a4:	cset	w0, ne  // ne = any
   332a8:	mov	sp, x29
   332ac:	ldp	x20, x19, [sp, #80]
   332b0:	ldp	x22, x21, [sp, #64]
   332b4:	ldp	x24, x23, [sp, #48]
   332b8:	ldp	x26, x25, [sp, #32]
   332bc:	ldp	x28, x27, [sp, #16]
   332c0:	ldp	x29, x30, [sp], #96
   332c4:	ret
   332c8:	bl	ca50 <__gmpn_copyi@plt>
   332cc:	b	32f80 <__gmpn_strongfibo@@Base+0x58>
   332d0:	sub	x0, x29, #0x8
   332d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   332d8:	mov	x21, x0
   332dc:	b	32fcc <__gmpn_strongfibo@@Base+0xa4>
   332e0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   332e4:	b	332a0 <__gmpn_strongfibo@@Base+0x378>
   332e8:	add	x0, x21, x25, lsl #3
   332ec:	mov	w1, wzr
   332f0:	mov	x2, x26
   332f4:	bl	c5f0 <memset@plt>
   332f8:	b	33168 <__gmpn_strongfibo@@Base+0x240>
   332fc:	nop

0000000000033300 <__gmpn_gcd_11@@Base>:
   33300:	subs	x3, x0, x1
   33304:	b.eq	3332c <__gmpn_gcd_11@@Base+0x2c>  // b.none
   33308:	nop
   3330c:	nop
   33310:	rbit	x12, x3
   33314:	clz	x12, x12
   33318:	cneg	x3, x3, cc  // cc = lo, ul, last
   3331c:	csel	x0, x1, x0, cs  // cs = hs, nlast
   33320:	lsr	x1, x3, x12
   33324:	subs	x3, x0, x1
   33328:	b.ne	33310 <__gmpn_gcd_11@@Base+0x10>  // b.any
   3332c:	ret

0000000000033330 <__gmpn_gcd_22@@Base>:
   33330:	subs	x5, x1, x3
   33334:	cbz	x5, 333a8 <__gmpn_gcd_22@@Base+0x78>
   33338:	sbcs	x6, x0, x2
   3333c:	rbit	x7, x5
   33340:	cneg	x5, x5, cc  // cc = lo, ul, last
   33344:	cinv	x6, x6, cc  // cc = lo, ul, last
   33348:	csel	x3, x3, x1, cs  // cs = hs, nlast
   3334c:	csel	x2, x2, x0, cs  // cs = hs, nlast
   33350:	clz	x7, x7
   33354:	neg	x8, x7
   33358:	lsr	x1, x5, x7
   3335c:	lsl	x14, x6, x8
   33360:	lsr	x0, x6, x7
   33364:	orr	x1, x1, x14
   33368:	orr	x11, x0, x2
   3336c:	cbnz	x11, 33330 <__gmpn_gcd_22@@Base>
   33370:	subs	x4, x1, x3
   33374:	b.eq	3339c <__gmpn_gcd_22@@Base+0x6c>  // b.none
   33378:	nop
   3337c:	nop
   33380:	rbit	x12, x4
   33384:	clz	x12, x12
   33388:	cneg	x4, x4, cc  // cc = lo, ul, last
   3338c:	csel	x1, x3, x1, cs  // cs = hs, nlast
   33390:	lsr	x3, x4, x12
   33394:	subs	x4, x1, x3
   33398:	b.ne	33380 <__gmpn_gcd_22@@Base+0x50>  // b.any
   3339c:	mov	x0, x1
   333a0:	mov	x1, #0x0                   	// #0
   333a4:	ret
   333a8:	subs	x5, x0, x2
   333ac:	b.eq	333c0 <__gmpn_gcd_22@@Base+0x90>  // b.none
   333b0:	mov	x6, #0x0                   	// #0
   333b4:	rbit	x7, x5
   333b8:	cneg	x5, x5, cc  // cc = lo, ul, last
   333bc:	b	33348 <__gmpn_gcd_22@@Base+0x18>
   333c0:	mov	x0, x3
   333c4:	mov	x1, x2
   333c8:	ret

00000000000333cc <__gmpn_gcd_1@@Base>:
   333cc:	stp	x29, x30, [sp, #-32]!
   333d0:	stp	x20, x19, [sp, #16]
   333d4:	ldr	x9, [x0]
   333d8:	rbit	x8, x2
   333dc:	clz	x8, x8
   333e0:	cmp	x1, #0x2
   333e4:	rbit	x10, x9
   333e8:	lsr	x19, x2, x8
   333ec:	clz	x10, x10
   333f0:	mov	x29, sp
   333f4:	b.lt	33428 <__gmpn_gcd_1@@Base+0x5c>  // b.tstop
   333f8:	cmp	x8, x10
   333fc:	ccmp	x9, #0x0, #0x4, cs  // cs = hs, nlast
   33400:	csel	x20, x8, x10, eq  // eq = none
   33404:	mov	x2, x19
   33408:	cmp	x1, #0x27
   3340c:	b.le	33464 <__gmpn_gcd_1@@Base+0x98>
   33410:	bl	c3e0 <__gmpn_mod_1@plt>
   33414:	cbnz	x0, 33470 <__gmpn_gcd_1@@Base+0xa4>
   33418:	lsl	x0, x19, x20
   3341c:	ldp	x20, x19, [sp, #16]
   33420:	ldp	x29, x30, [sp], #32
   33424:	ret
   33428:	lsr	x9, x9, x10
   3342c:	cmp	x8, x10
   33430:	csel	x20, x8, x10, cc  // cc = lo, ul, last
   33434:	cmp	x19, x9
   33438:	csel	x0, x19, x9, hi  // hi = pmore
   3343c:	csel	x19, x9, x19, hi  // hi = pmore
   33440:	cmp	x19, x0, lsr #16
   33444:	b.cs	3347c <__gmpn_gcd_1@@Base+0xb0>  // b.hs, b.nlast
   33448:	udiv	x8, x0, x19
   3344c:	msub	x8, x8, x19, x0
   33450:	cbz	x8, 33418 <__gmpn_gcd_1@@Base+0x4c>
   33454:	rbit	x9, x8
   33458:	clz	x9, x9
   3345c:	lsr	x0, x8, x9
   33460:	b	3347c <__gmpn_gcd_1@@Base+0xb0>
   33464:	mov	x3, xzr
   33468:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   3346c:	cbz	x0, 33418 <__gmpn_gcd_1@@Base+0x4c>
   33470:	rbit	x8, x0
   33474:	clz	x8, x8
   33478:	lsr	x0, x0, x8
   3347c:	mov	x1, x19
   33480:	bl	d340 <__gmpn_gcd_11@plt>
   33484:	mov	x19, x0
   33488:	lsl	x0, x19, x20
   3348c:	ldp	x20, x19, [sp, #16]
   33490:	ldp	x29, x30, [sp], #32
   33494:	ret

0000000000033498 <__gmpn_gcd@@Base>:
   33498:	stp	x29, x30, [sp, #-96]!
   3349c:	stp	x28, x27, [sp, #16]
   334a0:	stp	x26, x25, [sp, #32]
   334a4:	stp	x24, x23, [sp, #48]
   334a8:	stp	x22, x21, [sp, #64]
   334ac:	stp	x20, x19, [sp, #80]
   334b0:	mov	x29, sp
   334b4:	sub	sp, sp, #0x50
   334b8:	sub	x8, x2, x4
   334bc:	cmp	x8, x4
   334c0:	mov	x22, x4
   334c4:	mov	x20, x3
   334c8:	mov	x24, x2
   334cc:	mov	x21, x1
   334d0:	csinc	x23, x4, x8, lt  // lt = tstop
   334d4:	cmp	x4, #0x14a
   334d8:	mov	x19, x0
   334dc:	b.lt	33534 <__gmpn_gcd@@Base+0x9c>  // b.tstop
   334e0:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   334e4:	lsl	x8, x22, #1
   334e8:	movk	x9, #0x5556
   334ec:	smulh	x8, x8, x9
   334f0:	add	x25, x8, x8, lsr #63
   334f4:	sub	x0, x22, x25
   334f8:	add	x8, x0, #0x1
   334fc:	add	x9, x0, #0x2
   33500:	cmp	x8, #0x0
   33504:	csinc	x8, x9, x0, lt  // lt = tstop
   33508:	lsl	x8, x8, #1
   3350c:	and	x26, x8, #0xfffffffffffffffc
   33510:	bl	c590 <__gmpn_hgcd_itch@plt>
   33514:	add	x8, x25, x22
   33518:	sub	x9, x8, #0x1
   3351c:	cmp	x0, x8
   33520:	csel	x8, x9, x0, lt  // lt = tstop
   33524:	add	x8, x26, x8
   33528:	add	x8, x8, #0x4
   3352c:	cmp	x8, x23
   33530:	csel	x23, x8, x23, gt
   33534:	lsl	x1, x23, #3
   33538:	mov	w8, #0x7f00                	// #32512
   3353c:	cmp	x1, x8
   33540:	stur	xzr, [x29, #-24]
   33544:	b.hi	335b4 <__gmpn_gcd@@Base+0x11c>  // b.pmore
   33548:	add	x9, x1, #0xf
   3354c:	mov	x8, sp
   33550:	and	x9, x9, #0xfffffffffffffff0
   33554:	sub	x23, x8, x9
   33558:	mov	sp, x23
   3355c:	cmp	x24, x22
   33560:	b.le	335c8 <__gmpn_gcd@@Base+0x130>
   33564:	mov	x0, x23
   33568:	mov	x1, x21
   3356c:	mov	x2, xzr
   33570:	mov	x3, x21
   33574:	mov	x4, x24
   33578:	mov	x5, x20
   3357c:	mov	x6, x22
   33580:	bl	bf00 <__gmpn_tdiv_qr@plt>
   33584:	sub	x8, x21, #0x8
   33588:	mov	x9, x22
   3358c:	ldr	x10, [x8, x9, lsl #3]
   33590:	cbnz	x10, 335c8 <__gmpn_gcd@@Base+0x130>
   33594:	sub	x9, x9, #0x1
   33598:	cbnz	x9, 3358c <__gmpn_gcd@@Base+0xf4>
   3359c:	mov	x0, x19
   335a0:	mov	x1, x20
   335a4:	mov	x2, x22
   335a8:	bl	ca50 <__gmpn_copyi@plt>
   335ac:	stur	x22, [x29, #-8]
   335b0:	b	33830 <__gmpn_gcd@@Base+0x398>
   335b4:	sub	x0, x29, #0x18
   335b8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   335bc:	mov	x23, x0
   335c0:	cmp	x24, x22
   335c4:	b.gt	33564 <__gmpn_gcd@@Base+0xcc>
   335c8:	cmp	x22, #0x14a
   335cc:	stur	x19, [x29, #-16]
   335d0:	b.lt	336a0 <__gmpn_gcd@@Base+0x208>  // b.tstop
   335d4:	mov	x28, #0x5555555555555555    	// #6148914691236517205
   335d8:	adrp	x24, 33000 <__gmpn_strongfibo@@Base+0xd8>
   335dc:	movk	x28, #0x5556
   335e0:	add	x24, x24, #0x864
   335e4:	b	33610 <__gmpn_gcd@@Base+0x178>
   335e8:	add	x1, x0, x25
   335ec:	sub	x0, x29, #0x48
   335f0:	mov	x2, x21
   335f4:	mov	x3, x20
   335f8:	mov	x4, x25
   335fc:	mov	x5, x26
   33600:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   33604:	mov	x22, x0
   33608:	cmp	x22, #0x149
   3360c:	b.le	336a0 <__gmpn_gcd@@Base+0x208>
   33610:	lsl	x8, x22, #1
   33614:	smulh	x8, x8, x28
   33618:	add	x25, x8, x8, lsr #63
   3361c:	sub	x27, x22, x25
   33620:	add	x8, x27, #0x1
   33624:	add	x9, x27, #0x2
   33628:	cmp	x8, #0x0
   3362c:	csinc	x8, x9, x27, lt  // lt = tstop
   33630:	lsl	x8, x8, #4
   33634:	sub	x0, x29, #0x48
   33638:	mov	x1, x27
   3363c:	mov	x2, x23
   33640:	and	x26, x8, #0xffffffffffffffe0
   33644:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   33648:	add	x9, x26, x23
   3364c:	lsl	x8, x25, #3
   33650:	add	x26, x9, #0x20
   33654:	add	x0, x21, x8
   33658:	add	x1, x20, x8
   3365c:	sub	x3, x29, #0x48
   33660:	mov	x2, x27
   33664:	mov	x4, x26
   33668:	bl	cde0 <__gmpn_hgcd@plt>
   3366c:	cmp	x0, #0x1
   33670:	b.ge	335e8 <__gmpn_gcd@@Base+0x150>  // b.tcont
   33674:	sub	x5, x29, #0x10
   33678:	mov	x0, x21
   3367c:	mov	x1, x20
   33680:	mov	x2, x22
   33684:	mov	x3, xzr
   33688:	mov	x4, x24
   3368c:	mov	x6, x23
   33690:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   33694:	mov	x22, x0
   33698:	cbnz	x0, 33608 <__gmpn_gcd@@Base+0x170>
   3369c:	b	33830 <__gmpn_gcd@@Base+0x398>
   336a0:	cmp	x22, #0x3
   336a4:	b.lt	337a0 <__gmpn_gcd@@Base+0x308>  // b.tstop
   336a8:	adrp	x24, 33000 <__gmpn_strongfibo@@Base+0xd8>
   336ac:	add	x24, x24, #0x864
   336b0:	b	336e4 <__gmpn_gcd@@Base+0x24c>
   336b4:	sub	x0, x29, #0x48
   336b8:	mov	x1, x23
   336bc:	mov	x2, x21
   336c0:	mov	x3, x20
   336c4:	mov	x4, x22
   336c8:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   336cc:	mov	x22, x0
   336d0:	mov	x0, x21
   336d4:	mov	x21, x23
   336d8:	mov	x23, x0
   336dc:	cmp	x22, #0x2
   336e0:	b.le	337a0 <__gmpn_gcd@@Base+0x308>
   336e4:	lsl	x8, x22, #3
   336e8:	sub	x9, x8, #0x8
   336ec:	ldr	x0, [x21, x9]
   336f0:	ldr	x2, [x20, x9]
   336f4:	orr	x9, x2, x0
   336f8:	tbnz	x9, #63, 3375c <__gmpn_gcd@@Base+0x2c4>
   336fc:	sub	x10, x8, #0x10
   33700:	sub	x8, x8, #0x18
   33704:	ldr	x12, [x21, x10]
   33708:	ldr	x14, [x21, x8]
   3370c:	ldr	x10, [x20, x10]
   33710:	ldr	x8, [x20, x8]
   33714:	clz	x9, x9
   33718:	neg	x13, x9
   3371c:	lsl	x11, x0, x9
   33720:	lsl	x15, x2, x9
   33724:	lsr	x16, x12, x13
   33728:	lsl	x12, x12, x9
   3372c:	lsr	x14, x14, x13
   33730:	lsl	x9, x10, x9
   33734:	lsr	x10, x10, x13
   33738:	lsr	x8, x8, x13
   3373c:	orr	x0, x16, x11
   33740:	orr	x1, x14, x12
   33744:	orr	x2, x10, x15
   33748:	orr	x3, x8, x9
   3374c:	sub	x4, x29, #0x48
   33750:	bl	c5a0 <__gmpn_hgcd2@plt>
   33754:	cbnz	w0, 336b4 <__gmpn_gcd@@Base+0x21c>
   33758:	b	33774 <__gmpn_gcd@@Base+0x2dc>
   3375c:	sub	x8, x8, #0x10
   33760:	ldr	x1, [x21, x8]
   33764:	ldr	x3, [x20, x8]
   33768:	sub	x4, x29, #0x48
   3376c:	bl	c5a0 <__gmpn_hgcd2@plt>
   33770:	cbnz	w0, 336b4 <__gmpn_gcd@@Base+0x21c>
   33774:	sub	x5, x29, #0x10
   33778:	mov	x0, x21
   3377c:	mov	x1, x20
   33780:	mov	x2, x22
   33784:	mov	x3, xzr
   33788:	mov	x4, x24
   3378c:	mov	x6, x23
   33790:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   33794:	mov	x22, x0
   33798:	cbnz	x0, 336dc <__gmpn_gcd@@Base+0x244>
   3379c:	b	33830 <__gmpn_gcd@@Base+0x398>
   337a0:	ldr	x8, [x21]
   337a4:	tst	x8, #0x1
   337a8:	csel	x11, x21, x20, eq  // eq = none
   337ac:	csel	x9, x20, x21, eq  // eq = none
   337b0:	ldr	x8, [x9]
   337b4:	ldr	x10, [x11]
   337b8:	cmp	x22, #0x1
   337bc:	b.ne	337e0 <__gmpn_gcd@@Base+0x348>  // b.any
   337c0:	rbit	x9, x10
   337c4:	clz	x9, x9
   337c8:	lsr	x1, x10, x9
   337cc:	mov	x0, x8
   337d0:	bl	d340 <__gmpn_gcd_11@plt>
   337d4:	str	x0, [x19]
   337d8:	mov	w8, #0x1                   	// #1
   337dc:	b	3382c <__gmpn_gcd@@Base+0x394>
   337e0:	ldr	x11, [x11, #8]
   337e4:	cmp	x10, #0x0
   337e8:	csel	x3, x11, x10, eq  // eq = none
   337ec:	csel	x2, xzr, x11, eq  // eq = none
   337f0:	tbnz	w3, #0, 33810 <__gmpn_gcd@@Base+0x378>
   337f4:	rbit	x10, x3
   337f8:	clz	x10, x10
   337fc:	neg	x11, x10
   33800:	lsr	x12, x3, x10
   33804:	lsl	x11, x2, x11
   33808:	orr	x3, x11, x12
   3380c:	lsr	x2, x2, x10
   33810:	ldr	x0, [x9, #8]
   33814:	mov	x1, x8
   33818:	bl	c030 <__gmpn_gcd_22@plt>
   3381c:	cmp	x1, #0x0
   33820:	mov	w8, #0x1                   	// #1
   33824:	cinc	x8, x8, ne  // ne = any
   33828:	stp	x0, x1, [x19]
   3382c:	stur	x8, [x29, #-8]
   33830:	ldur	x0, [x29, #-24]
   33834:	cbnz	x0, 3385c <__gmpn_gcd@@Base+0x3c4>
   33838:	ldur	x0, [x29, #-8]
   3383c:	mov	sp, x29
   33840:	ldp	x20, x19, [sp, #80]
   33844:	ldp	x22, x21, [sp, #64]
   33848:	ldp	x24, x23, [sp, #48]
   3384c:	ldp	x26, x25, [sp, #32]
   33850:	ldp	x28, x27, [sp, #16]
   33854:	ldp	x29, x30, [sp], #96
   33858:	ret
   3385c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   33860:	b	33838 <__gmpn_gcd@@Base+0x3a0>
   33864:	stp	x29, x30, [sp, #-32]!
   33868:	stp	x20, x19, [sp, #16]
   3386c:	mov	x20, x0
   33870:	ldr	x0, [x0]
   33874:	mov	x29, sp
   33878:	mov	x19, x2
   3387c:	bl	ca50 <__gmpn_copyi@plt>
   33880:	str	x19, [x20, #8]
   33884:	ldp	x20, x19, [sp, #16]
   33888:	ldp	x29, x30, [sp], #32
   3388c:	ret

0000000000033890 <__gmpn_gcdext_1@@Base>:
   33890:	mov	x8, xzr
   33894:	mov	x10, xzr
   33898:	mov	w9, #0x1                   	// #1
   3389c:	cmp	x2, x3
   338a0:	mov	w11, #0x1                   	// #1
   338a4:	b.cc	338bc <__gmpn_gcdext_1@@Base+0x2c>  // b.lo, b.ul, b.last
   338a8:	udiv	x12, x2, x3
   338ac:	msub	x2, x12, x3, x2
   338b0:	cbz	x2, 338dc <__gmpn_gcdext_1@@Base+0x4c>
   338b4:	msub	x9, x12, x10, x9
   338b8:	msub	x8, x12, x11, x8
   338bc:	udiv	x12, x3, x2
   338c0:	msub	x3, x12, x2, x3
   338c4:	cbz	x3, 338e8 <__gmpn_gcdext_1@@Base+0x58>
   338c8:	msub	x10, x12, x9, x10
   338cc:	msub	x11, x12, x8, x11
   338d0:	udiv	x12, x2, x3
   338d4:	msub	x2, x12, x3, x2
   338d8:	cbnz	x2, 338b4 <__gmpn_gcdext_1@@Base+0x24>
   338dc:	mov	x9, x10
   338e0:	mov	x8, x11
   338e4:	mov	x2, x3
   338e8:	str	x9, [x0]
   338ec:	mov	x0, x2
   338f0:	str	x8, [x1]
   338f4:	ret

00000000000338f8 <__gmpn_gcdext@@Base>:
   338f8:	stp	x29, x30, [sp, #-96]!
   338fc:	stp	x28, x27, [sp, #16]
   33900:	stp	x26, x25, [sp, #32]
   33904:	stp	x24, x23, [sp, #48]
   33908:	stp	x22, x21, [sp, #64]
   3390c:	stp	x20, x19, [sp, #80]
   33910:	mov	x29, sp
   33914:	sub	sp, sp, #0xf0
   33918:	mov	w8, #0x3                   	// #3
   3391c:	sub	x9, x4, x6
   33920:	bfi	x8, x6, #2, #62
   33924:	cmp	x9, x8
   33928:	mov	x23, x6
   3392c:	mov	x24, x5
   33930:	mov	x19, x4
   33934:	mov	x27, x3
   33938:	mov	x25, x2
   3393c:	mov	x26, x1
   33940:	mov	x21, x0
   33944:	add	x28, x6, #0x1
   33948:	csinc	x22, x8, x9, lt  // lt = tstop
   3394c:	cmp	x6, #0xf2
   33950:	stur	xzr, [x29, #-80]
   33954:	b.lt	339f8 <__gmpn_gcdext@@Base+0x100>  // b.tstop
   33958:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3395c:	movk	x9, #0xaaab
   33960:	umulh	x9, x23, x9
   33964:	lsr	x8, x23, #1
   33968:	lsr	x9, x9, #1
   3396c:	cmp	x8, x9
   33970:	csel	x10, x8, x9, lt  // lt = tstop
   33974:	stur	x28, [x29, #-136]
   33978:	mov	x28, x24
   3397c:	mov	x24, x25
   33980:	mov	x25, x23
   33984:	sub	x0, x25, x10
   33988:	csel	x23, x8, x9, gt
   3398c:	add	x8, x0, #0x1
   33990:	add	x9, x0, #0x2
   33994:	cmp	x8, #0x0
   33998:	csinc	x8, x9, x0, lt  // lt = tstop
   3399c:	lsl	x8, x8, #1
   339a0:	and	x8, x8, #0xfffffffffffffffc
   339a4:	mov	x20, x26
   339a8:	add	x26, x8, #0x4
   339ac:	bl	c590 <__gmpn_hgcd_itch@plt>
   339b0:	add	x8, x23, x25
   339b4:	sub	x9, x8, #0x1
   339b8:	cmp	x0, x8
   339bc:	csel	x8, x9, x0, lt  // lt = tstop
   339c0:	add	x8, x8, x26
   339c4:	mov	x23, x25
   339c8:	mov	x25, x24
   339cc:	mov	x24, x28
   339d0:	ldur	x28, [x29, #-136]
   339d4:	cmp	x8, x22
   339d8:	csel	x8, x8, x22, gt
   339dc:	cmp	x8, #0x6a1
   339e0:	mov	w9, #0x6a1                 	// #1697
   339e4:	csel	x8, x8, x9, gt
   339e8:	stur	x26, [x29, #-160]
   339ec:	mov	x26, x20
   339f0:	add	x22, x8, x28, lsl #1
   339f4:	b	339f8 <__gmpn_gcdext@@Base+0x100>
   339f8:	lsl	x1, x22, #3
   339fc:	mov	w8, #0x7f00                	// #32512
   33a00:	cmp	x1, x8
   33a04:	b.hi	33a80 <__gmpn_gcdext@@Base+0x188>  // b.pmore
   33a08:	add	x9, x1, #0xf
   33a0c:	mov	x8, sp
   33a10:	and	x9, x9, #0xfffffffffffffff0
   33a14:	sub	x22, x8, x9
   33a18:	mov	sp, x22
   33a1c:	cmp	x19, x23
   33a20:	b.le	33a94 <__gmpn_gcdext@@Base+0x19c>
   33a24:	mov	x0, x22
   33a28:	mov	x1, x27
   33a2c:	mov	x2, xzr
   33a30:	mov	x3, x27
   33a34:	mov	x4, x19
   33a38:	mov	x5, x24
   33a3c:	mov	x6, x23
   33a40:	bl	bf00 <__gmpn_tdiv_qr@plt>
   33a44:	sub	x8, x27, #0x8
   33a48:	mov	x9, x23
   33a4c:	ldr	x10, [x8, x9, lsl #3]
   33a50:	cbnz	x10, 33a94 <__gmpn_gcdext@@Base+0x19c>
   33a54:	sub	x9, x9, #0x1
   33a58:	cbnz	x9, 33a4c <__gmpn_gcdext@@Base+0x154>
   33a5c:	mov	x0, x21
   33a60:	mov	x1, x24
   33a64:	mov	x2, x23
   33a68:	bl	ca50 <__gmpn_copyi@plt>
   33a6c:	str	xzr, [x25]
   33a70:	ldur	x0, [x29, #-80]
   33a74:	cbnz	x0, 34494 <__gmpn_gcdext@@Base+0xb9c>
   33a78:	mov	x20, x23
   33a7c:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   33a80:	sub	x0, x29, #0x50
   33a84:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   33a88:	mov	x22, x0
   33a8c:	cmp	x19, x23
   33a90:	b.gt	33a24 <__gmpn_gcdext@@Base+0x12c>
   33a94:	cmp	x23, #0xf1
   33a98:	b.le	33bb0 <__gmpn_gcdext@@Base+0x2b8>
   33a9c:	cbz	x28, 33ab4 <__gmpn_gcdext@@Base+0x1bc>
   33aa0:	lsl	x8, x23, #4
   33aa4:	add	x2, x8, #0x10
   33aa8:	mov	x0, x22
   33aac:	mov	w1, wzr
   33ab0:	bl	c5f0 <memset@plt>
   33ab4:	cmp	x23, #0x0
   33ab8:	lsl	x8, x28, #3
   33abc:	cinc	x9, x23, lt  // lt = tstop
   33ac0:	stur	x21, [x29, #-176]
   33ac4:	stur	x21, [x29, #-72]
   33ac8:	add	x21, x22, x8
   33acc:	asr	x19, x9, #1
   33ad0:	stp	x26, x25, [x29, #-56]
   33ad4:	mov	x20, x26
   33ad8:	add	x28, x21, x8
   33adc:	mov	x26, x25
   33ae0:	sub	x25, x23, x19
   33ae4:	sub	x0, x29, #0x80
   33ae8:	mov	x1, x25
   33aec:	mov	x2, x28
   33af0:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   33af4:	lsl	x8, x19, #3
   33af8:	add	x0, x27, x8
   33afc:	add	x1, x24, x8
   33b00:	ldur	x8, [x29, #-160]
   33b04:	sub	x3, x29, #0x80
   33b08:	mov	x2, x25
   33b0c:	add	x8, x28, x8, lsl #3
   33b10:	mov	x4, x8
   33b14:	mov	x25, x8
   33b18:	bl	cde0 <__gmpn_hgcd@plt>
   33b1c:	cmp	x0, #0x1
   33b20:	stur	x21, [x29, #-136]
   33b24:	stur	x28, [x29, #-160]
   33b28:	b.lt	33be8 <__gmpn_gcdext@@Base+0x2f0>  // b.tstop
   33b2c:	add	x1, x0, x19
   33b30:	sub	x0, x29, #0x80
   33b34:	mov	x2, x27
   33b38:	mov	x3, x24
   33b3c:	mov	x4, x19
   33b40:	mov	x5, x25
   33b44:	stp	x20, x26, [x29, #-200]
   33b48:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   33b4c:	ldur	x1, [x29, #-96]
   33b50:	ldur	x2, [x29, #-120]
   33b54:	mov	x19, x0
   33b58:	mov	x0, x22
   33b5c:	bl	ca50 <__gmpn_copyi@plt>
   33b60:	ldur	x1, [x29, #-88]
   33b64:	ldur	x2, [x29, #-120]
   33b68:	mov	x0, x21
   33b6c:	bl	ca50 <__gmpn_copyi@plt>
   33b70:	ldur	x8, [x29, #-120]
   33b74:	sub	x9, x22, #0x8
   33b78:	add	x10, x22, x23, lsl #3
   33b7c:	lsl	x11, x8, #3
   33b80:	ldr	x12, [x9, x11]
   33b84:	ldr	x13, [x10, x11]
   33b88:	sub	x11, x8, #0x1
   33b8c:	mov	x8, x11
   33b90:	orr	x12, x13, x12
   33b94:	cbz	x12, 33b7c <__gmpn_gcdext@@Base+0x284>
   33b98:	add	x28, x11, #0x1
   33b9c:	cmp	x19, #0xf2
   33ba0:	stur	x23, [x29, #-184]
   33ba4:	stur	x24, [x29, #-152]
   33ba8:	b.ge	33c40 <__gmpn_gcdext@@Base+0x348>  // b.tcont
   33bac:	b	33e98 <__gmpn_gcdext@@Base+0x5a0>
   33bb0:	mov	x0, x21
   33bb4:	mov	x1, x26
   33bb8:	mov	x2, x25
   33bbc:	mov	x3, x27
   33bc0:	mov	x4, x24
   33bc4:	mov	x5, x23
   33bc8:	mov	x6, x22
   33bcc:	bl	d1b0 <__gmpn_gcdext_lehmer_n@plt>
   33bd0:	ldur	x8, [x29, #-80]
   33bd4:	mov	x20, x0
   33bd8:	cbz	x8, 34470 <__gmpn_gcdext@@Base+0xb78>
   33bdc:	mov	x0, x8
   33be0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   33be4:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   33be8:	mov	w8, #0x1                   	// #1
   33bec:	add	x9, x28, x23, lsl #3
   33bf0:	str	x8, [x21]
   33bf4:	stp	x21, x9, [x29, #-24]
   33bf8:	stp	x8, x22, [x29, #-40]
   33bfc:	adrp	x4, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   33c00:	ldr	x4, [x4, #3968]
   33c04:	sub	x5, x29, #0x48
   33c08:	mov	x0, x27
   33c0c:	mov	x1, x24
   33c10:	mov	x2, x23
   33c14:	mov	x3, xzr
   33c18:	mov	x6, x28
   33c1c:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   33c20:	cbz	x0, 33fcc <__gmpn_gcdext@@Base+0x6d4>
   33c24:	ldur	x28, [x29, #-40]
   33c28:	mov	x19, x0
   33c2c:	stp	x20, x26, [x29, #-200]
   33c30:	cmp	x19, #0xf2
   33c34:	stur	x23, [x29, #-184]
   33c38:	stur	x24, [x29, #-152]
   33c3c:	b.lt	33e98 <__gmpn_gcdext@@Base+0x5a0>  // b.tstop
   33c40:	add	x21, x22, x23, lsl #3
   33c44:	sub	x23, x22, #0x8
   33c48:	mov	x20, x25
   33c4c:	stur	x25, [x29, #-168]
   33c50:	stur	x22, [x29, #-144]
   33c54:	b	33c74 <__gmpn_gcdext@@Base+0x37c>
   33c58:	lsl	x9, x8, #3
   33c5c:	str	x25, [x24, x9]
   33c60:	str	x0, [x22, x9]
   33c64:	mov	x22, x24
   33c68:	add	x28, x8, #0x1
   33c6c:	cmp	x19, #0xf1
   33c70:	b.le	33e98 <__gmpn_gcdext@@Base+0x5a0>
   33c74:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   33c78:	movk	x8, #0xaaab
   33c7c:	ldur	x22, [x29, #-160]
   33c80:	umulh	x8, x19, x8
   33c84:	lsr	x25, x8, #1
   33c88:	sub	x26, x19, x25
   33c8c:	sub	x0, x29, #0x80
   33c90:	mov	x1, x26
   33c94:	mov	x2, x22
   33c98:	mov	x24, x28
   33c9c:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   33ca0:	mov	x28, x19
   33ca4:	ldur	x19, [x29, #-152]
   33ca8:	lsl	x8, x25, #3
   33cac:	add	x0, x27, x8
   33cb0:	sub	x3, x29, #0x80
   33cb4:	add	x1, x19, x8
   33cb8:	mov	x2, x26
   33cbc:	mov	x4, x20
   33cc0:	bl	cde0 <__gmpn_hgcd@plt>
   33cc4:	cmp	x0, #0x1
   33cc8:	b.lt	33d3c <__gmpn_gcdext@@Base+0x444>  // b.tstop
   33ccc:	add	x1, x0, x25
   33cd0:	sub	x0, x29, #0x80
   33cd4:	mov	x2, x27
   33cd8:	mov	x3, x19
   33cdc:	mov	x4, x25
   33ce0:	mov	x5, x20
   33ce4:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   33ce8:	ldur	x22, [x29, #-144]
   33cec:	mov	x19, x0
   33cf0:	mov	x0, x20
   33cf4:	mov	x2, x24
   33cf8:	mov	x1, x22
   33cfc:	mov	x28, x24
   33d00:	bl	ca50 <__gmpn_copyi@plt>
   33d04:	ldp	x4, x3, [x29, #-120]
   33d08:	add	x26, x20, x24, lsl #3
   33d0c:	mov	x0, x26
   33d10:	cmp	x4, x24
   33d14:	b.ge	33d90 <__gmpn_gcdext@@Base+0x498>  // b.tcont
   33d18:	mov	x1, x20
   33d1c:	mov	x2, x28
   33d20:	bl	ccd0 <__gmpn_mul@plt>
   33d24:	ldur	x3, [x29, #-96]
   33d28:	ldur	x4, [x29, #-120]
   33d2c:	ldur	x1, [x29, #-136]
   33d30:	mov	x0, x22
   33d34:	mov	x2, x28
   33d38:	b	33db8 <__gmpn_gcdext@@Base+0x4c0>
   33d3c:	ldur	x9, [x29, #-136]
   33d40:	add	x8, x22, x28, lsl #3
   33d44:	adrp	x4, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   33d48:	sub	x5, x29, #0x48
   33d4c:	stp	x9, x8, [x29, #-24]
   33d50:	ldur	x8, [x29, #-144]
   33d54:	mov	x0, x27
   33d58:	mov	x1, x19
   33d5c:	mov	x2, x28
   33d60:	stp	x24, x8, [x29, #-40]
   33d64:	ldr	x4, [x4, #3968]
   33d68:	mov	x3, xzr
   33d6c:	mov	x6, x22
   33d70:	mov	x25, x27
   33d74:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   33d78:	cbz	x0, 33fcc <__gmpn_gcdext@@Base+0x6d4>
   33d7c:	ldur	x28, [x29, #-40]
   33d80:	ldur	x22, [x29, #-144]
   33d84:	mov	x19, x0
   33d88:	mov	x27, x25
   33d8c:	b	33c6c <__gmpn_gcdext@@Base+0x374>
   33d90:	mov	x1, x3
   33d94:	mov	x2, x4
   33d98:	mov	x3, x20
   33d9c:	mov	x4, x28
   33da0:	bl	ccd0 <__gmpn_mul@plt>
   33da4:	ldur	x1, [x29, #-96]
   33da8:	ldur	x2, [x29, #-120]
   33dac:	ldur	x3, [x29, #-136]
   33db0:	mov	x0, x22
   33db4:	mov	x4, x28
   33db8:	bl	ccd0 <__gmpn_mul@plt>
   33dbc:	ldur	x8, [x29, #-120]
   33dc0:	mov	x0, x22
   33dc4:	mov	x1, x22
   33dc8:	mov	x2, x26
   33dcc:	add	x3, x8, x28
   33dd0:	mov	x24, x22
   33dd4:	bl	ca70 <__gmpn_add_n@plt>
   33dd8:	ldur	x4, [x29, #-120]
   33ddc:	ldur	x3, [x29, #-88]
   33de0:	mov	x25, x0
   33de4:	mov	x0, x26
   33de8:	cmp	x4, x28
   33dec:	b.ge	33e18 <__gmpn_gcdext@@Base+0x520>  // b.tcont
   33df0:	ldur	x22, [x29, #-136]
   33df4:	mov	x2, x28
   33df8:	mov	x1, x22
   33dfc:	bl	ccd0 <__gmpn_mul@plt>
   33e00:	ldur	x3, [x29, #-104]
   33e04:	ldur	x4, [x29, #-120]
   33e08:	mov	x0, x22
   33e0c:	mov	x1, x20
   33e10:	mov	x2, x28
   33e14:	b	33e44 <__gmpn_gcdext@@Base+0x54c>
   33e18:	ldur	x22, [x29, #-136]
   33e1c:	mov	x1, x3
   33e20:	mov	x2, x4
   33e24:	mov	x4, x28
   33e28:	mov	x3, x22
   33e2c:	bl	ccd0 <__gmpn_mul@plt>
   33e30:	ldur	x1, [x29, #-104]
   33e34:	ldur	x2, [x29, #-120]
   33e38:	mov	x0, x22
   33e3c:	mov	x3, x20
   33e40:	mov	x4, x28
   33e44:	bl	ccd0 <__gmpn_mul@plt>
   33e48:	ldur	x8, [x29, #-120]
   33e4c:	mov	x0, x22
   33e50:	mov	x1, x22
   33e54:	mov	x2, x26
   33e58:	add	x3, x8, x28
   33e5c:	bl	ca70 <__gmpn_add_n@plt>
   33e60:	ldur	x8, [x29, #-120]
   33e64:	orr	x9, x0, x25
   33e68:	add	x8, x8, x28
   33e6c:	cbnz	x9, 33c58 <__gmpn_gcdext@@Base+0x360>
   33e70:	lsl	x9, x8, #3
   33e74:	ldr	x10, [x23, x9]
   33e78:	ldr	x11, [x21, x9]
   33e7c:	sub	x9, x8, #0x1
   33e80:	mov	x8, x9
   33e84:	orr	x10, x11, x10
   33e88:	cbz	x10, 33e70 <__gmpn_gcdext@@Base+0x578>
   33e8c:	add	x28, x9, #0x1
   33e90:	mov	x22, x24
   33e94:	b	33c6c <__gmpn_gcdext@@Base+0x374>
   33e98:	ldur	x23, [x29, #-184]
   33e9c:	ldur	x24, [x29, #-152]
   33ea0:	sub	x11, x19, #0x1
   33ea4:	mov	x8, x11
   33ea8:	add	x9, x8, #0x1
   33eac:	cmp	x9, #0x1
   33eb0:	b.lt	33f08 <__gmpn_gcdext@@Base+0x610>  // b.tstop
   33eb4:	lsl	x9, x8, #3
   33eb8:	ldr	x10, [x27, x9]
   33ebc:	ldr	x9, [x24, x9]
   33ec0:	sub	x8, x8, #0x1
   33ec4:	cmp	x10, x9
   33ec8:	b.eq	33ea8 <__gmpn_gcdext@@Base+0x5b0>  // b.none
   33ecc:	cmp	x28, #0x1
   33ed0:	b.ne	33fdc <__gmpn_gcdext@@Base+0x6e4>  // b.any
   33ed4:	ldr	x8, [x22]
   33ed8:	cbnz	x8, 33fdc <__gmpn_gcdext@@Base+0x6e4>
   33edc:	ldur	x0, [x29, #-176]
   33ee0:	ldp	x1, x2, [x29, #-200]
   33ee4:	ldur	x6, [x29, #-160]
   33ee8:	mov	x3, x27
   33eec:	mov	x4, x24
   33ef0:	mov	x5, x19
   33ef4:	bl	d1b0 <__gmpn_gcdext_lehmer_n@plt>
   33ef8:	ldur	x8, [x29, #-80]
   33efc:	mov	x20, x0
   33f00:	cbz	x8, 34470 <__gmpn_gcdext@@Base+0xb78>
   33f04:	b	33bdc <__gmpn_gcdext@@Base+0x2e4>
   33f08:	ldur	x0, [x29, #-176]
   33f0c:	mov	x1, x27
   33f10:	mov	x2, x19
   33f14:	bl	ca50 <__gmpn_copyi@plt>
   33f18:	sub	x8, x22, #0x8
   33f1c:	add	x9, x22, x23, lsl #3
   33f20:	mov	x10, x28
   33f24:	subs	x11, x10, #0x1
   33f28:	b.lt	33f48 <__gmpn_gcdext@@Base+0x650>  // b.tstop
   33f2c:	lsl	x10, x10, #3
   33f30:	ldr	x12, [x8, x10]
   33f34:	ldr	x10, [x9, x10]
   33f38:	cmp	x12, x10
   33f3c:	mov	x10, x11
   33f40:	b.eq	33f24 <__gmpn_gcdext@@Base+0x62c>  // b.none
   33f44:	b.ls	33f88 <__gmpn_gcdext@@Base+0x690>  // b.plast
   33f48:	add	x8, x22, x23, lsl #3
   33f4c:	ldr	x10, [x8, x28, lsl #3]
   33f50:	sub	x9, x28, #0x1
   33f54:	mov	x28, x9
   33f58:	cbz	x10, 33f4c <__gmpn_gcdext@@Base+0x654>
   33f5c:	ldur	x0, [x29, #-200]
   33f60:	ldur	x1, [x29, #-136]
   33f64:	add	x20, x9, #0x1
   33f68:	mov	x2, x20
   33f6c:	bl	ca50 <__gmpn_copyi@plt>
   33f70:	ldur	x8, [x29, #-192]
   33f74:	str	x20, [x8]
   33f78:	ldur	x0, [x29, #-80]
   33f7c:	cbnz	x0, 33fc0 <__gmpn_gcdext@@Base+0x6c8>
   33f80:	mov	x20, x19
   33f84:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   33f88:	mov	x20, x28
   33f8c:	subs	x28, x28, #0x1
   33f90:	b.lt	33f9c <__gmpn_gcdext@@Base+0x6a4>  // b.tstop
   33f94:	ldr	x9, [x8, x20, lsl #3]
   33f98:	cbz	x9, 33f88 <__gmpn_gcdext@@Base+0x690>
   33f9c:	ldur	x0, [x29, #-200]
   33fa0:	mov	x1, x22
   33fa4:	mov	x2, x20
   33fa8:	bl	ca50 <__gmpn_copyi@plt>
   33fac:	neg	x20, x20
   33fb0:	ldur	x8, [x29, #-192]
   33fb4:	str	x20, [x8]
   33fb8:	ldur	x0, [x29, #-80]
   33fbc:	cbz	x0, 33f80 <__gmpn_gcdext@@Base+0x688>
   33fc0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   33fc4:	mov	x20, x19
   33fc8:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   33fcc:	ldur	x0, [x29, #-80]
   33fd0:	cbnz	x0, 344a0 <__gmpn_gcdext@@Base+0xba8>
   33fd4:	ldur	x20, [x29, #-64]
   33fd8:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   33fdc:	ldur	x26, [x29, #-160]
   33fe0:	lsl	x21, x19, #3
   33fe4:	mov	x1, x27
   33fe8:	mov	x2, x19
   33fec:	add	x20, x26, x21
   33ff0:	mov	x0, x20
   33ff4:	stur	x11, [x29, #-216]
   33ff8:	bl	ca50 <__gmpn_copyi@plt>
   33ffc:	add	x25, x20, x21
   34000:	mov	x0, x25
   34004:	mov	x1, x24
   34008:	mov	x2, x19
   3400c:	bl	ca50 <__gmpn_copyi@plt>
   34010:	ldur	x0, [x29, #-176]
   34014:	add	x6, x20, x19, lsl #4
   34018:	sub	x2, x29, #0x80
   3401c:	mov	x1, x26
   34020:	mov	x3, x20
   34024:	mov	x4, x25
   34028:	mov	x5, x19
   3402c:	stur	x20, [x29, #-168]
   34030:	bl	d1b0 <__gmpn_gcdext_lehmer_n@plt>
   34034:	mov	x20, x0
   34038:	sub	x8, x22, #0x8
   3403c:	mov	x9, x28
   34040:	mov	x26, x9
   34044:	subs	x9, x9, #0x1
   34048:	b.lt	34054 <__gmpn_gcdext@@Base+0x75c>  // b.tstop
   3404c:	ldr	x10, [x8, x26, lsl #3]
   34050:	cbz	x10, 34040 <__gmpn_gcdext@@Base+0x748>
   34054:	ldur	x11, [x29, #-128]
   34058:	cbz	x11, 340d4 <__gmpn_gcdext@@Base+0x7dc>
   3405c:	cmp	x11, #0x0
   34060:	mov	w9, #0x18                  	// #24
   34064:	lsl	x10, x23, #4
   34068:	add	x0, x25, #0x8
   3406c:	cneg	x25, x11, mi  // mi = first
   34070:	madd	x9, x19, x9, x10
   34074:	add	x9, x9, x25, lsl #3
   34078:	add	x9, x9, x22
   3407c:	stp	x11, x10, [x29, #-232]
   34080:	add	x10, x9, #0x10
   34084:	add	x9, x27, x19, lsl #3
   34088:	mov	x8, xzr
   3408c:	sub	x9, x9, #0x8
   34090:	stur	x10, [x29, #-240]
   34094:	add	x4, x19, x8
   34098:	mov	x21, x8
   3409c:	cmp	x4, #0x1
   340a0:	mov	x23, x10
   340a4:	b.lt	340b8 <__gmpn_gcdext@@Base+0x7c0>  // b.tstop
   340a8:	ldr	x11, [x9, x21, lsl #3]
   340ac:	sub	x10, x23, #0x8
   340b0:	sub	x8, x21, #0x1
   340b4:	cbz	x11, 34094 <__gmpn_gcdext@@Base+0x79c>
   340b8:	cmp	x4, x25
   340bc:	stur	x0, [x29, #-208]
   340c0:	b.ge	340fc <__gmpn_gcdext@@Base+0x804>  // b.tcont
   340c4:	ldur	x1, [x29, #-160]
   340c8:	mov	x2, x25
   340cc:	mov	x3, x27
   340d0:	b	3410c <__gmpn_gcdext@@Base+0x814>
   340d4:	ldur	x0, [x29, #-200]
   340d8:	mov	x1, x22
   340dc:	mov	x2, x26
   340e0:	bl	ca50 <__gmpn_copyi@plt>
   340e4:	ldur	x9, [x29, #-192]
   340e8:	neg	x8, x26
   340ec:	str	x8, [x9]
   340f0:	ldur	x0, [x29, #-80]
   340f4:	cbz	x0, 34470 <__gmpn_gcdext@@Base+0xb78>
   340f8:	b	33be0 <__gmpn_gcdext@@Base+0x2e8>
   340fc:	ldur	x3, [x29, #-160]
   34100:	mov	x1, x27
   34104:	mov	x2, x4
   34108:	mov	x4, x25
   3410c:	bl	ccd0 <__gmpn_mul@plt>
   34110:	ldp	x9, x27, [x29, #-232]
   34114:	add	x8, x25, x19
   34118:	cmp	x9, #0x1
   3411c:	mov	x9, x25
   34120:	add	x25, x8, x21
   34124:	b.lt	34198 <__gmpn_gcdext@@Base+0x8a0>  // b.tstop
   34128:	cbz	x20, 34180 <__gmpn_gcdext@@Base+0x888>
   3412c:	ldur	x0, [x29, #-208]
   34130:	ldur	x2, [x29, #-176]
   34134:	mov	x3, x20
   34138:	mov	x1, x0
   3413c:	bl	c2d0 <__gmpn_sub_n@plt>
   34140:	cbz	x0, 34180 <__gmpn_gcdext@@Base+0x888>
   34144:	add	x8, x27, x19, lsl #4
   34148:	add	x8, x22, x8
   3414c:	add	x9, x20, #0x3
   34150:	sub	x10, x9, #0x3
   34154:	cmp	x10, x25
   34158:	b.ge	34180 <__gmpn_gcdext@@Base+0x888>  // b.tcont
   3415c:	lsl	x10, x9, #3
   34160:	ldr	x11, [x8, x10]
   34164:	add	x9, x9, #0x1
   34168:	sub	x12, x11, #0x1
   3416c:	str	x12, [x8, x10]
   34170:	cbz	x11, 34150 <__gmpn_gcdext@@Base+0x858>
   34174:	b	34180 <__gmpn_gcdext@@Base+0x888>
   34178:	ldr	x8, [x23], #-8
   3417c:	cbnz	x8, 34204 <__gmpn_gcdext@@Base+0x90c>
   34180:	mov	x19, x25
   34184:	subs	x25, x25, #0x1
   34188:	b.ge	34178 <__gmpn_gcdext@@Base+0x880>  // b.tcont
   3418c:	cbnz	x19, 34204 <__gmpn_gcdext@@Base+0x90c>
   34190:	ldur	x25, [x29, #-168]
   34194:	b	34264 <__gmpn_gcdext@@Base+0x96c>
   34198:	mov	x23, x9
   3419c:	cbz	x20, 341e8 <__gmpn_gcdext@@Base+0x8f0>
   341a0:	ldur	x0, [x29, #-208]
   341a4:	ldur	x2, [x29, #-176]
   341a8:	mov	x3, x20
   341ac:	mov	x1, x0
   341b0:	bl	ca70 <__gmpn_add_n@plt>
   341b4:	cbz	x0, 341e8 <__gmpn_gcdext@@Base+0x8f0>
   341b8:	add	x8, x27, x19, lsl #4
   341bc:	add	x8, x22, x8
   341c0:	add	x9, x20, #0x3
   341c4:	sub	x10, x9, #0x3
   341c8:	cmp	x10, x25
   341cc:	b.ge	341e8 <__gmpn_gcdext@@Base+0x8f0>  // b.tcont
   341d0:	lsl	x10, x9, #3
   341d4:	ldr	x11, [x8, x10]
   341d8:	add	x9, x9, #0x1
   341dc:	adds	x11, x11, #0x1
   341e0:	str	x11, [x8, x10]
   341e4:	b.cs	341c4 <__gmpn_gcdext@@Base+0x8cc>  // b.hs, b.nlast
   341e8:	ldur	x8, [x29, #-240]
   341ec:	add	x9, x23, x19
   341f0:	ldr	x8, [x8, x21, lsl #3]
   341f4:	cmp	x8, #0x0
   341f8:	cset	w8, eq  // eq = none
   341fc:	sub	x8, x9, x8
   34200:	add	x19, x8, x21
   34204:	ldur	x10, [x29, #-216]
   34208:	add	x8, x27, x19, lsl #3
   3420c:	add	x8, x8, x22
   34210:	add	x8, x8, #0x10
   34214:	add	x4, x10, #0x1
   34218:	mov	x21, x10
   3421c:	cmp	x4, #0x1
   34220:	mov	x23, x8
   34224:	b.lt	34238 <__gmpn_gcdext@@Base+0x940>  // b.tstop
   34228:	ldr	x9, [x24, x21, lsl #3]
   3422c:	sub	x10, x21, #0x1
   34230:	add	x8, x23, #0x8
   34234:	cbz	x9, 34214 <__gmpn_gcdext@@Base+0x91c>
   34238:	ldur	x25, [x29, #-168]
   3423c:	ldur	x1, [x29, #-208]
   34240:	mov	x2, x19
   34244:	mov	x3, x24
   34248:	mov	x0, x25
   3424c:	bl	c430 <__gmpn_divexact@plt>
   34250:	ldr	x8, [x23]
   34254:	cmp	x8, #0x0
   34258:	cset	w8, eq  // eq = none
   3425c:	sub	x8, x19, x8
   34260:	sub	x19, x8, x21
   34264:	ldur	x4, [x29, #-128]
   34268:	ldur	x8, [x29, #-184]
   3426c:	ldur	x3, [x29, #-160]
   34270:	stur	x20, [x29, #-152]
   34274:	cmp	x4, #0x0
   34278:	b.le	34284 <__gmpn_gcdext@@Base+0x98c>
   3427c:	mov	w20, wzr
   34280:	b	34290 <__gmpn_gcdext@@Base+0x998>
   34284:	neg	x4, x4
   34288:	mov	w20, #0x1                   	// #1
   3428c:	stur	x4, [x29, #-128]
   34290:	add	x8, x22, x8, lsl #3
   34294:	mov	x24, x28
   34298:	subs	x28, x28, #0x1
   3429c:	b.lt	342a8 <__gmpn_gcdext@@Base+0x9b0>  // b.tstop
   342a0:	ldr	x9, [x8, x24, lsl #3]
   342a4:	cbz	x9, 34294 <__gmpn_gcdext@@Base+0x99c>
   342a8:	cmp	x4, x24
   342ac:	b.le	342d0 <__gmpn_gcdext@@Base+0x9d8>
   342b0:	ldur	x27, [x29, #-200]
   342b4:	ldur	x28, [x29, #-136]
   342b8:	mov	x1, x3
   342bc:	mov	x2, x4
   342c0:	mov	x0, x27
   342c4:	mov	x3, x28
   342c8:	mov	x4, x24
   342cc:	b	342e4 <__gmpn_gcdext@@Base+0x9ec>
   342d0:	ldur	x27, [x29, #-200]
   342d4:	ldur	x28, [x29, #-136]
   342d8:	mov	x2, x24
   342dc:	mov	x0, x27
   342e0:	mov	x1, x28
   342e4:	bl	ccd0 <__gmpn_mul@plt>
   342e8:	ldur	x8, [x29, #-128]
   342ec:	ldur	x23, [x29, #-192]
   342f0:	add	x9, x8, x24
   342f4:	add	x9, x27, x9, lsl #3
   342f8:	ldur	x9, [x9, #-8]
   342fc:	cmp	x9, #0x0
   34300:	cset	w9, eq  // eq = none
   34304:	sub	x8, x8, x9
   34308:	cmp	x19, #0x1
   3430c:	add	x24, x8, x24
   34310:	b.lt	34458 <__gmpn_gcdext@@Base+0xb60>  // b.tstop
   34314:	mov	x0, x28
   34318:	cmp	x19, x26
   3431c:	b.le	34334 <__gmpn_gcdext@@Base+0xa3c>
   34320:	mov	x1, x25
   34324:	mov	x2, x19
   34328:	mov	x3, x22
   3432c:	mov	x4, x26
   34330:	b	34344 <__gmpn_gcdext@@Base+0xa4c>
   34334:	mov	x1, x22
   34338:	mov	x2, x26
   3433c:	mov	x3, x25
   34340:	mov	x4, x19
   34344:	bl	ccd0 <__gmpn_mul@plt>
   34348:	add	x8, x19, x26
   3434c:	add	x8, x28, x8, lsl #3
   34350:	ldur	x8, [x8, #-8]
   34354:	cmp	x8, #0x0
   34358:	cset	w8, eq  // eq = none
   3435c:	sub	x8, x19, x8
   34360:	add	x25, x8, x26
   34364:	csetm	x21, eq  // eq = none
   34368:	cmp	x25, x24
   3436c:	b.le	34404 <__gmpn_gcdext@@Base+0xb0c>
   34370:	cbz	x24, 343bc <__gmpn_gcdext@@Base+0xac4>
   34374:	mov	x0, x27
   34378:	mov	x1, x28
   3437c:	mov	x2, x27
   34380:	mov	x3, x24
   34384:	bl	ca70 <__gmpn_add_n@plt>
   34388:	cbz	x0, 343bc <__gmpn_gcdext@@Base+0xac4>
   3438c:	ldur	x8, [x29, #-184]
   34390:	add	x8, x22, x8, lsl #3
   34394:	add	x9, x8, #0x8
   34398:	mov	w8, #0x1                   	// #1
   3439c:	cmp	x24, x25
   343a0:	b.ge	343fc <__gmpn_gcdext@@Base+0xb04>  // b.tcont
   343a4:	lsl	x10, x24, #3
   343a8:	ldr	x11, [x9, x10]
   343ac:	add	x24, x24, #0x1
   343b0:	adds	x11, x11, #0x1
   343b4:	str	x11, [x27, x10]
   343b8:	b.cs	3439c <__gmpn_gcdext@@Base+0xaa4>  // b.hs, b.nlast
   343bc:	cmp	x28, x27
   343c0:	mov	x8, xzr
   343c4:	b.eq	343fc <__gmpn_gcdext@@Base+0xb04>  // b.none
   343c8:	cmp	x25, x24
   343cc:	b.le	343fc <__gmpn_gcdext@@Base+0xb04>
   343d0:	ldur	x8, [x29, #-184]
   343d4:	add	x9, x19, x21
   343d8:	sub	x9, x9, x24
   343dc:	add	x0, x27, x24, lsl #3
   343e0:	add	x8, x24, x8
   343e4:	add	x8, x22, x8, lsl #3
   343e8:	add	x1, x8, #0x8
   343ec:	add	x8, x9, x26
   343f0:	lsl	x2, x8, #3
   343f4:	bl	bed0 <memcpy@plt>
   343f8:	mov	x8, xzr
   343fc:	mov	x24, x25
   34400:	b	34450 <__gmpn_gcdext@@Base+0xb58>
   34404:	neg	x8, x8
   34408:	cmp	x8, x26
   3440c:	b.eq	3444c <__gmpn_gcdext@@Base+0xb54>  // b.none
   34410:	mov	x0, x27
   34414:	mov	x1, x27
   34418:	mov	x2, x28
   3441c:	mov	x3, x25
   34420:	bl	ca70 <__gmpn_add_n@plt>
   34424:	cbz	x0, 3444c <__gmpn_gcdext@@Base+0xb54>
   34428:	mov	w8, #0x1                   	// #1
   3442c:	cmp	x25, x24
   34430:	b.ge	34450 <__gmpn_gcdext@@Base+0xb58>  // b.tcont
   34434:	lsl	x9, x25, #3
   34438:	ldr	x10, [x27, x9]
   3443c:	add	x25, x25, #0x1
   34440:	adds	x10, x10, #0x1
   34444:	str	x10, [x27, x9]
   34448:	b.cs	3442c <__gmpn_gcdext@@Base+0xb34>  // b.hs, b.nlast
   3444c:	mov	x8, xzr
   34450:	str	x8, [x27, x24, lsl #3]
   34454:	add	x24, x8, x24
   34458:	cmp	w20, #0x0
   3445c:	cneg	x8, x24, ne  // ne = any
   34460:	str	x8, [x23]
   34464:	ldur	x0, [x29, #-80]
   34468:	ldur	x20, [x29, #-152]
   3446c:	cbnz	x0, 33be0 <__gmpn_gcdext@@Base+0x2e8>
   34470:	mov	x0, x20
   34474:	mov	sp, x29
   34478:	ldp	x20, x19, [sp, #80]
   3447c:	ldp	x22, x21, [sp, #64]
   34480:	ldp	x24, x23, [sp, #48]
   34484:	ldp	x26, x25, [sp, #32]
   34488:	ldp	x28, x27, [sp, #16]
   3448c:	ldp	x29, x30, [sp], #96
   34490:	ret
   34494:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   34498:	mov	x20, x23
   3449c:	b	34470 <__gmpn_gcdext@@Base+0xb78>
   344a0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   344a4:	b	33fd4 <__gmpn_gcdext@@Base+0x6dc>

00000000000344a8 <__gmpn_gcd_subdiv_step@@Base>:
   344a8:	sub	sp, sp, #0x70
   344ac:	add	x9, x0, x2, lsl #3
   344b0:	stp	x24, x23, [sp, #64]
   344b4:	stp	x22, x21, [sp, #80]
   344b8:	stp	x20, x19, [sp, #96]
   344bc:	mov	x21, x6
   344c0:	mov	x20, x5
   344c4:	mov	x19, x4
   344c8:	mov	x24, x3
   344cc:	mov	x8, x0
   344d0:	mov	x11, xzr
   344d4:	sub	x10, x9, #0x8
   344d8:	stp	x29, x30, [sp, #16]
   344dc:	stp	x28, x27, [sp, #32]
   344e0:	stp	x26, x25, [sp, #48]
   344e4:	add	x29, sp, #0x10
   344e8:	add	x23, x2, x11
   344ec:	mov	x9, x11
   344f0:	cmp	x23, #0x1
   344f4:	b.lt	34518 <__gmpn_gcd_subdiv_step@@Base+0x70>  // b.tstop
   344f8:	ldr	x12, [x10, x9, lsl #3]
   344fc:	sub	x11, x9, #0x1
   34500:	cbz	x12, 344e8 <__gmpn_gcd_subdiv_step@@Base+0x40>
   34504:	b	34518 <__gmpn_gcd_subdiv_step@@Base+0x70>
   34508:	add	x9, x1, x10, lsl #3
   3450c:	ldur	x12, [x9, #-8]
   34510:	add	x9, x11, #0x1
   34514:	cbnz	x12, 34528 <__gmpn_gcd_subdiv_step@@Base+0x80>
   34518:	mov	x10, x2
   3451c:	subs	x2, x2, #0x1
   34520:	mov	x11, x9
   34524:	b.ge	34508 <__gmpn_gcd_subdiv_step@@Base+0x60>  // b.tcont
   34528:	cbz	x11, 34540 <__gmpn_gcd_subdiv_step@@Base+0x98>
   3452c:	cmp	x23, x10
   34530:	csel	x22, x10, x23, gt
   34534:	csel	x23, x23, x10, gt
   34538:	cset	w27, gt
   3453c:	b	34574 <__gmpn_gcd_subdiv_step@@Base+0xcc>
   34540:	sub	x9, x1, #0x8
   34544:	mov	x10, x23
   34548:	subs	x11, x10, #0x1
   3454c:	b.lt	34748 <__gmpn_gcd_subdiv_step@@Base+0x2a0>  // b.tstop
   34550:	lsl	x10, x10, #3
   34554:	add	x12, x8, x10
   34558:	ldur	x12, [x12, #-8]
   3455c:	ldr	x10, [x9, x10]
   34560:	cmp	x12, x10
   34564:	mov	x10, x11
   34568:	b.eq	34548 <__gmpn_gcd_subdiv_step@@Base+0xa0>  // b.none
   3456c:	cset	w27, hi  // hi = pmore
   34570:	mov	x22, x23
   34574:	cmp	w27, #0x0
   34578:	csel	x25, x8, x1, ne  // ne = any
   3457c:	csel	x26, x1, x8, ne  // ne = any
   34580:	cmp	x22, x24
   34584:	b.le	34658 <__gmpn_gcd_subdiv_step@@Base+0x1b0>
   34588:	cbz	x22, 345d8 <__gmpn_gcd_subdiv_step@@Base+0x130>
   3458c:	mov	x0, x25
   34590:	mov	x1, x25
   34594:	mov	x2, x26
   34598:	mov	x3, x22
   3459c:	bl	c2d0 <__gmpn_sub_n@plt>
   345a0:	cbz	x0, 345d8 <__gmpn_gcd_subdiv_step@@Base+0x130>
   345a4:	mov	x8, x22
   345a8:	cmp	x8, x23
   345ac:	b.ge	345d8 <__gmpn_gcd_subdiv_step@@Base+0x130>  // b.tcont
   345b0:	lsl	x9, x8, #3
   345b4:	ldr	x10, [x25, x9]
   345b8:	add	x8, x8, #0x1
   345bc:	sub	x11, x10, #0x1
   345c0:	str	x11, [x25, x9]
   345c4:	cbz	x10, 345a8 <__gmpn_gcd_subdiv_step@@Base+0x100>
   345c8:	b	345d8 <__gmpn_gcd_subdiv_step@@Base+0x130>
   345cc:	add	x8, x25, x28, lsl #3
   345d0:	ldur	x8, [x8, #-8]
   345d4:	cbnz	x8, 345e4 <__gmpn_gcd_subdiv_step@@Base+0x13c>
   345d8:	mov	x28, x23
   345dc:	subs	x23, x23, #0x1
   345e0:	b.ge	345cc <__gmpn_gcd_subdiv_step@@Base+0x124>  // b.tcont
   345e4:	cmp	x28, x24
   345e8:	b.le	3466c <__gmpn_gcd_subdiv_step@@Base+0x1c4>
   345ec:	cmp	x22, x28
   345f0:	str	x19, [sp, #8]
   345f4:	b.ne	34790 <__gmpn_gcd_subdiv_step@@Base+0x2e8>  // b.any
   345f8:	sub	x8, x26, #0x8
   345fc:	mov	x9, x22
   34600:	subs	x10, x9, #0x1
   34604:	b.lt	34854 <__gmpn_gcd_subdiv_step@@Base+0x3ac>  // b.tstop
   34608:	lsl	x9, x9, #3
   3460c:	ldr	x11, [x8, x9]
   34610:	add	x9, x25, x9
   34614:	ldur	x9, [x9, #-8]
   34618:	cmp	x11, x9
   3461c:	mov	x9, x10
   34620:	b.eq	34600 <__gmpn_gcd_subdiv_step@@Base+0x158>  // b.none
   34624:	ldr	x8, [sp, #8]
   34628:	adrp	x3, 5d000 <__gmpn_bases@@Base+0x2ab8>
   3462c:	add	x3, x3, #0x5d0
   34630:	mov	w4, #0x1                   	// #1
   34634:	mov	x0, x20
   34638:	mov	x1, xzr
   3463c:	mov	x2, xzr
   34640:	mov	w5, w27
   34644:	mov	x19, x21
   34648:	cset	w21, hi  // hi = pmore
   3464c:	blr	x8
   34650:	mov	x23, x22
   34654:	b	347c8 <__gmpn_gcd_subdiv_step@@Base+0x320>
   34658:	cbnz	x24, 34768 <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   3465c:	eor	w5, w27, #0x1
   34660:	mov	x0, x20
   34664:	mov	x1, x25
   34668:	b	34758 <__gmpn_gcd_subdiv_step@@Base+0x2b0>
   3466c:	cbz	x28, 346a8 <__gmpn_gcd_subdiv_step@@Base+0x200>
   34670:	mov	x0, x25
   34674:	mov	x1, x26
   34678:	mov	x2, x25
   3467c:	mov	x3, x28
   34680:	bl	ca70 <__gmpn_add_n@plt>
   34684:	cbz	x0, 346a8 <__gmpn_gcd_subdiv_step@@Base+0x200>
   34688:	cmp	x28, x22
   3468c:	b.ge	34878 <__gmpn_gcd_subdiv_step@@Base+0x3d0>  // b.tcont
   34690:	lsl	x8, x28, #3
   34694:	ldr	x9, [x26, x8]
   34698:	add	x28, x28, #0x1
   3469c:	adds	x9, x9, #0x1
   346a0:	str	x9, [x25, x8]
   346a4:	b.cs	34688 <__gmpn_gcd_subdiv_step@@Base+0x1e0>  // b.hs, b.nlast
   346a8:	cmp	x25, x26
   346ac:	mov	x23, xzr
   346b0:	b.eq	3476c <__gmpn_gcd_subdiv_step@@Base+0x2c4>  // b.none
   346b4:	subs	x8, x22, x28
   346b8:	b.le	3476c <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   346bc:	cmp	x8, #0x4
   346c0:	b.cc	34724 <__gmpn_gcd_subdiv_step@@Base+0x27c>  // b.lo, b.ul, b.last
   346c4:	lsl	x10, x28, #3
   346c8:	lsl	x9, x22, #3
   346cc:	add	x11, x25, x10
   346d0:	add	x12, x26, x9
   346d4:	cmp	x11, x12
   346d8:	b.cs	346ec <__gmpn_gcd_subdiv_step@@Base+0x244>  // b.hs, b.nlast
   346dc:	add	x9, x25, x9
   346e0:	add	x11, x26, x10
   346e4:	cmp	x11, x9
   346e8:	b.cc	34724 <__gmpn_gcd_subdiv_step@@Base+0x27c>  // b.lo, b.ul, b.last
   346ec:	and	x9, x8, #0xfffffffffffffffc
   346f0:	add	x11, x10, #0x10
   346f4:	add	x28, x28, x9
   346f8:	add	x10, x26, x11
   346fc:	add	x11, x25, x11
   34700:	mov	x12, x9
   34704:	ldp	q0, q1, [x10, #-16]
   34708:	add	x10, x10, #0x20
   3470c:	subs	x12, x12, #0x4
   34710:	stp	q0, q1, [x11, #-16]
   34714:	add	x11, x11, #0x20
   34718:	b.ne	34704 <__gmpn_gcd_subdiv_step@@Base+0x25c>  // b.any
   3471c:	cmp	x8, x9
   34720:	b.eq	34768 <__gmpn_gcd_subdiv_step@@Base+0x2c0>  // b.none
   34724:	lsl	x10, x28, #3
   34728:	sub	x8, x22, x28
   3472c:	add	x9, x25, x10
   34730:	add	x10, x26, x10
   34734:	ldr	x11, [x10], #8
   34738:	subs	x8, x8, #0x1
   3473c:	str	x11, [x9], #8
   34740:	b.ne	34734 <__gmpn_gcd_subdiv_step@@Base+0x28c>  // b.any
   34744:	b	34768 <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   34748:	cbnz	x24, 34768 <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   3474c:	mov	w5, #0xffffffff            	// #-1
   34750:	mov	x0, x20
   34754:	mov	x1, x8
   34758:	mov	x2, x23
   3475c:	mov	x3, xzr
   34760:	mov	x4, xzr
   34764:	blr	x19
   34768:	mov	x23, xzr
   3476c:	mov	x0, x23
   34770:	ldp	x20, x19, [sp, #96]
   34774:	ldp	x22, x21, [sp, #80]
   34778:	ldp	x24, x23, [sp, #64]
   3477c:	ldp	x26, x25, [sp, #48]
   34780:	ldp	x28, x27, [sp, #32]
   34784:	ldp	x29, x30, [sp, #16]
   34788:	add	sp, sp, #0x70
   3478c:	ret
   34790:	ldr	x8, [sp, #8]
   34794:	adrp	x3, 5d000 <__gmpn_bases@@Base+0x2ab8>
   34798:	add	x3, x3, #0x5d0
   3479c:	mov	w4, #0x1                   	// #1
   347a0:	mov	x0, x20
   347a4:	mov	x1, xzr
   347a8:	mov	x2, xzr
   347ac:	mov	w5, w27
   347b0:	mov	x19, x21
   347b4:	blr	x8
   347b8:	cmp	x22, x28
   347bc:	csel	x23, x28, x22, gt
   347c0:	csel	x22, x22, x28, gt
   347c4:	cset	w21, gt
   347c8:	cmp	w21, #0x0
   347cc:	csel	x28, x26, x25, ne  // ne = any
   347d0:	csel	x26, x25, x26, ne  // ne = any
   347d4:	mov	x0, x19
   347d8:	mov	x1, x28
   347dc:	mov	x2, xzr
   347e0:	mov	x3, x28
   347e4:	mov	x4, x22
   347e8:	mov	x5, x26
   347ec:	mov	x6, x23
   347f0:	eor	w25, w27, w21
   347f4:	bl	bf00 <__gmpn_tdiv_qr@plt>
   347f8:	ldr	x10, [sp, #8]
   347fc:	sub	x8, x22, x23
   34800:	add	x22, x8, #0x1
   34804:	mov	x8, x23
   34808:	mov	x27, x8
   3480c:	subs	x8, x8, #0x1
   34810:	b.lt	34828 <__gmpn_gcd_subdiv_step@@Base+0x380>  // b.tstop
   34814:	add	x9, x28, x27, lsl #3
   34818:	ldur	x9, [x9, #-8]
   3481c:	cbz	x9, 34808 <__gmpn_gcd_subdiv_step@@Base+0x360>
   34820:	mov	w8, #0x1                   	// #1
   34824:	b	3482c <__gmpn_gcd_subdiv_step@@Base+0x384>
   34828:	mov	w8, wzr
   3482c:	cmp	x27, x24
   34830:	b.le	348ac <__gmpn_gcd_subdiv_step@@Base+0x404>
   34834:	mov	x0, x20
   34838:	mov	x1, xzr
   3483c:	mov	x2, xzr
   34840:	mov	x3, x19
   34844:	mov	x4, x22
   34848:	mov	w5, w25
   3484c:	blr	x10
   34850:	b	3476c <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   34854:	cmp	x24, #0x1
   34858:	b.lt	34888 <__gmpn_gcd_subdiv_step@@Base+0x3e0>  // b.tstop
   3485c:	adrp	x3, 5d000 <__gmpn_bases@@Base+0x2ab8>
   34860:	add	x3, x3, #0x5d0
   34864:	mov	w4, #0x1                   	// #1
   34868:	mov	x0, x20
   3486c:	mov	x1, xzr
   34870:	mov	x2, xzr
   34874:	b	3489c <__gmpn_gcd_subdiv_step@@Base+0x3f4>
   34878:	mov	w8, #0x1                   	// #1
   3487c:	mov	x23, xzr
   34880:	str	x8, [x25, x22, lsl #3]
   34884:	b	3476c <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   34888:	mov	x0, x20
   3488c:	mov	x1, x25
   34890:	mov	x2, x22
   34894:	mov	x3, xzr
   34898:	mov	x4, xzr
   3489c:	mov	w5, w27
   348a0:	ldr	x8, [sp, #8]
   348a4:	blr	x8
   348a8:	b	34768 <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   348ac:	cbz	x24, 34980 <__gmpn_gcd_subdiv_step@@Base+0x4d8>
   348b0:	cbz	w8, 3499c <__gmpn_gcd_subdiv_step@@Base+0x4f4>
   348b4:	mov	x0, x28
   348b8:	mov	x1, x26
   348bc:	mov	x2, x28
   348c0:	mov	x3, x27
   348c4:	bl	ca70 <__gmpn_add_n@plt>
   348c8:	cbz	x0, 348ec <__gmpn_gcd_subdiv_step@@Base+0x444>
   348cc:	cmp	x27, x23
   348d0:	b.ge	349b0 <__gmpn_gcd_subdiv_step@@Base+0x508>  // b.tcont
   348d4:	lsl	x8, x27, #3
   348d8:	ldr	x9, [x26, x8]
   348dc:	add	x27, x27, #0x1
   348e0:	adds	x9, x9, #0x1
   348e4:	str	x9, [x28, x8]
   348e8:	b.cs	348cc <__gmpn_gcd_subdiv_step@@Base+0x424>  // b.hs, b.nlast
   348ec:	cmp	x28, x26
   348f0:	b.eq	349c0 <__gmpn_gcd_subdiv_step@@Base+0x518>  // b.none
   348f4:	subs	x8, x23, x27
   348f8:	b.le	349c0 <__gmpn_gcd_subdiv_step@@Base+0x518>
   348fc:	cmp	x8, #0x4
   34900:	b.cc	34964 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.lo, b.ul, b.last
   34904:	lsl	x10, x27, #3
   34908:	lsl	x9, x23, #3
   3490c:	add	x11, x28, x10
   34910:	add	x12, x26, x9
   34914:	cmp	x11, x12
   34918:	b.cs	3492c <__gmpn_gcd_subdiv_step@@Base+0x484>  // b.hs, b.nlast
   3491c:	add	x9, x28, x9
   34920:	add	x11, x26, x10
   34924:	cmp	x11, x9
   34928:	b.cc	34964 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.lo, b.ul, b.last
   3492c:	and	x9, x8, #0xfffffffffffffffc
   34930:	add	x11, x10, #0x10
   34934:	add	x27, x27, x9
   34938:	add	x10, x26, x11
   3493c:	add	x11, x28, x11
   34940:	mov	x12, x9
   34944:	ldp	q0, q1, [x10, #-16]
   34948:	add	x10, x10, #0x20
   3494c:	subs	x12, x12, #0x4
   34950:	stp	q0, q1, [x11, #-16]
   34954:	add	x11, x11, #0x20
   34958:	b.ne	34944 <__gmpn_gcd_subdiv_step@@Base+0x49c>  // b.any
   3495c:	cmp	x8, x9
   34960:	b.eq	349c0 <__gmpn_gcd_subdiv_step@@Base+0x518>  // b.none
   34964:	lsl	x8, x27, #3
   34968:	ldr	x9, [x26, x8]
   3496c:	add	x27, x27, #0x1
   34970:	cmp	x23, x27
   34974:	str	x9, [x28, x8]
   34978:	b.ne	34964 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.any
   3497c:	b	349c0 <__gmpn_gcd_subdiv_step@@Base+0x518>
   34980:	mov	x0, x20
   34984:	mov	x1, x26
   34988:	mov	x2, x23
   3498c:	mov	x3, x19
   34990:	mov	x4, x22
   34994:	mov	w5, w25
   34998:	b	348a0 <__gmpn_gcd_subdiv_step@@Base+0x3f8>
   3499c:	mov	x0, x28
   349a0:	mov	x1, x26
   349a4:	mov	x2, x23
   349a8:	bl	ca50 <__gmpn_copyi@plt>
   349ac:	b	349c0 <__gmpn_gcd_subdiv_step@@Base+0x518>
   349b0:	add	x8, x23, #0x1
   349b4:	mov	w9, #0x1                   	// #1
   349b8:	str	x9, [x28, x23, lsl #3]
   349bc:	mov	x23, x8
   349c0:	mov	x8, x19
   349c4:	ldr	x9, [x8]
   349c8:	sub	x10, x9, #0x1
   349cc:	str	x10, [x8], #8
   349d0:	cbz	x9, 349c4 <__gmpn_gcd_subdiv_step@@Base+0x51c>
   349d4:	ldr	x10, [sp, #8]
   349d8:	b	34834 <__gmpn_gcd_subdiv_step@@Base+0x38c>

00000000000349dc <__gmpn_gcdext_hook@@Base>:
   349dc:	stp	x29, x30, [sp, #-96]!
   349e0:	stp	x26, x25, [sp, #32]
   349e4:	stp	x24, x23, [sp, #48]
   349e8:	stp	x22, x21, [sp, #64]
   349ec:	stp	x20, x19, [sp, #80]
   349f0:	ldr	x20, [x0, #32]
   349f4:	mov	w21, w5
   349f8:	mov	x19, x0
   349fc:	str	x27, [sp, #16]
   34a00:	mov	x29, sp
   34a04:	cbz	x1, 34a50 <__gmpn_gcdext_hook@@Base+0x74>
   34a08:	ldr	x0, [x19]
   34a0c:	mov	x22, x2
   34a10:	bl	ca50 <__gmpn_copyi@plt>
   34a14:	str	x22, [x19, #8]
   34a18:	tbz	w21, #31, 34acc <__gmpn_gcdext_hook@@Base+0xf0>
   34a1c:	sub	x8, x20, #0x1
   34a20:	add	x9, x8, #0x1
   34a24:	cmp	x9, #0x1
   34a28:	b.lt	34ac8 <__gmpn_gcdext_hook@@Base+0xec>  // b.tstop
   34a2c:	ldp	x9, x10, [x19, #40]
   34a30:	lsl	x11, x8, #3
   34a34:	sub	x8, x8, #0x1
   34a38:	ldr	x9, [x9, x11]
   34a3c:	ldr	x10, [x10, x11]
   34a40:	cmp	x9, x10
   34a44:	b.eq	34a20 <__gmpn_gcdext_hook@@Base+0x44>  // b.none
   34a48:	cset	w21, ls  // ls = plast
   34a4c:	b	34acc <__gmpn_gcdext_hook@@Base+0xf0>
   34a50:	add	x10, x3, x4, lsl #3
   34a54:	ldp	x8, x9, [x19, #40]
   34a58:	ldur	x10, [x10, #-8]
   34a5c:	cmp	w21, #0x0
   34a60:	mov	x25, x4
   34a64:	csel	x21, x8, x9, eq  // eq = none
   34a68:	csel	x8, x9, x8, eq  // eq = none
   34a6c:	cmp	x10, #0x0
   34a70:	cset	w9, eq  // eq = none
   34a74:	sub	x4, x4, x9
   34a78:	csetm	x26, eq  // eq = none
   34a7c:	cmp	x4, #0x1
   34a80:	b.ne	34aa8 <__gmpn_gcdext_hook@@Base+0xcc>  // b.any
   34a84:	ldr	x3, [x3]
   34a88:	mov	x0, x21
   34a8c:	cmp	x3, #0x1
   34a90:	b.ne	34b40 <__gmpn_gcdext_hook@@Base+0x164>  // b.any
   34a94:	mov	x1, x21
   34a98:	mov	x2, x8
   34a9c:	mov	x3, x20
   34aa0:	bl	ca70 <__gmpn_add_n@plt>
   34aa4:	b	34cf4 <__gmpn_gcdext_hook@@Base+0x318>
   34aa8:	sub	x9, x8, #0x8
   34aac:	mov	x10, x20
   34ab0:	mov	x23, x10
   34ab4:	subs	x10, x10, #0x1
   34ab8:	b.lt	34b18 <__gmpn_gcdext_hook@@Base+0x13c>  // b.tstop
   34abc:	ldr	x11, [x9, x23, lsl #3]
   34ac0:	cbz	x11, 34ab0 <__gmpn_gcdext_hook@@Base+0xd4>
   34ac4:	b	34b1c <__gmpn_gcdext_hook@@Base+0x140>
   34ac8:	mov	w21, wzr
   34acc:	cmp	w21, #0x0
   34ad0:	mov	w8, #0x30                  	// #48
   34ad4:	mov	w9, #0x28                  	// #40
   34ad8:	csel	x8, x9, x8, ne  // ne = any
   34adc:	ldr	x1, [x19, x8]
   34ae0:	mov	x22, x20
   34ae4:	subs	x20, x20, #0x1
   34ae8:	b.lt	34af8 <__gmpn_gcdext_hook@@Base+0x11c>  // b.tstop
   34aec:	add	x8, x1, x22, lsl #3
   34af0:	ldur	x8, [x8, #-8]
   34af4:	cbz	x8, 34ae0 <__gmpn_gcdext_hook@@Base+0x104>
   34af8:	ldr	x0, [x19, #16]
   34afc:	mov	x2, x22
   34b00:	bl	ca50 <__gmpn_copyi@plt>
   34b04:	ldr	x8, [x19, #24]
   34b08:	cmp	w21, #0x0
   34b0c:	cneg	x9, x22, ne  // ne = any
   34b10:	str	x9, [x8]
   34b14:	b	34d04 <__gmpn_gcdext_hook@@Base+0x328>
   34b18:	cbz	x23, 34d04 <__gmpn_gcdext_hook@@Base+0x328>
   34b1c:	ldr	x24, [x19, #56]
   34b20:	cmp	x4, x23
   34b24:	mov	x0, x24
   34b28:	b.le	34b50 <__gmpn_gcdext_hook@@Base+0x174>
   34b2c:	mov	x1, x3
   34b30:	mov	x2, x4
   34b34:	mov	x3, x8
   34b38:	mov	x4, x23
   34b3c:	b	34b58 <__gmpn_gcdext_hook@@Base+0x17c>
   34b40:	mov	x1, x8
   34b44:	mov	x2, x20
   34b48:	bl	d400 <__gmpn_addmul_1@plt>
   34b4c:	b	34cf4 <__gmpn_gcdext_hook@@Base+0x318>
   34b50:	mov	x1, x8
   34b54:	mov	x2, x23
   34b58:	bl	ccd0 <__gmpn_mul@plt>
   34b5c:	lsl	x8, x25, #3
   34b60:	add	x8, x8, x26, lsl #3
   34b64:	add	x8, x8, x24
   34b68:	add	x8, x8, x23, lsl #3
   34b6c:	ldur	x8, [x8, #-8]
   34b70:	add	x9, x25, x26
   34b74:	cmp	x8, #0x0
   34b78:	cset	w8, eq  // eq = none
   34b7c:	sub	x9, x9, x8
   34b80:	add	x22, x9, x23
   34b84:	csetm	x27, eq  // eq = none
   34b88:	cmp	x22, x20
   34b8c:	b.ge	34be4 <__gmpn_gcdext_hook@@Base+0x208>  // b.tcont
   34b90:	sub	x8, x26, x8
   34b94:	add	x8, x8, x25
   34b98:	add	x8, x8, x23
   34b9c:	cbz	x8, 34bdc <__gmpn_gcdext_hook@@Base+0x200>
   34ba0:	mov	x0, x21
   34ba4:	mov	x1, x21
   34ba8:	mov	x2, x24
   34bac:	mov	x3, x22
   34bb0:	bl	ca70 <__gmpn_add_n@plt>
   34bb4:	cbz	x0, 34cf4 <__gmpn_gcdext_hook@@Base+0x318>
   34bb8:	mov	w0, #0x1                   	// #1
   34bbc:	cmp	x22, x20
   34bc0:	b.ge	34cf4 <__gmpn_gcdext_hook@@Base+0x318>  // b.tcont
   34bc4:	lsl	x8, x22, #3
   34bc8:	ldr	x9, [x21, x8]
   34bcc:	add	x22, x22, #0x1
   34bd0:	adds	x9, x9, #0x1
   34bd4:	str	x9, [x21, x8]
   34bd8:	b.cs	34bbc <__gmpn_gcdext_hook@@Base+0x1e0>  // b.hs, b.nlast
   34bdc:	mov	x0, xzr
   34be0:	b	34cf4 <__gmpn_gcdext_hook@@Base+0x318>
   34be4:	cbz	x20, 34c24 <__gmpn_gcdext_hook@@Base+0x248>
   34be8:	mov	x0, x21
   34bec:	mov	x1, x24
   34bf0:	mov	x2, x21
   34bf4:	mov	x3, x20
   34bf8:	bl	ca70 <__gmpn_add_n@plt>
   34bfc:	cbz	x0, 34c24 <__gmpn_gcdext_hook@@Base+0x248>
   34c00:	mov	w0, #0x1                   	// #1
   34c04:	cmp	x20, x22
   34c08:	b.ge	34cf0 <__gmpn_gcdext_hook@@Base+0x314>  // b.tcont
   34c0c:	lsl	x8, x20, #3
   34c10:	ldr	x9, [x24, x8]
   34c14:	add	x20, x20, #0x1
   34c18:	adds	x9, x9, #0x1
   34c1c:	str	x9, [x21, x8]
   34c20:	b.cs	34c04 <__gmpn_gcdext_hook@@Base+0x228>  // b.hs, b.nlast
   34c24:	cmp	x21, x24
   34c28:	mov	x0, xzr
   34c2c:	b.eq	34cf0 <__gmpn_gcdext_hook@@Base+0x314>  // b.none
   34c30:	cmp	x20, x22
   34c34:	b.ge	34cf0 <__gmpn_gcdext_hook@@Base+0x314>  // b.tcont
   34c38:	sub	x11, x27, x20
   34c3c:	add	x8, x25, x26
   34c40:	add	x9, x11, x8
   34c44:	add	x9, x9, x23
   34c48:	cmp	x9, #0x4
   34c4c:	b.cc	34cc4 <__gmpn_gcdext_hook@@Base+0x2e8>  // b.lo, b.ul, b.last
   34c50:	add	x10, x27, x8
   34c54:	add	x10, x10, x23
   34c58:	lsl	x12, x20, #3
   34c5c:	lsl	x10, x10, #3
   34c60:	add	x13, x21, x12
   34c64:	add	x14, x24, x10
   34c68:	cmp	x13, x14
   34c6c:	b.cs	34c80 <__gmpn_gcdext_hook@@Base+0x2a4>  // b.hs, b.nlast
   34c70:	add	x10, x21, x10
   34c74:	add	x13, x24, x12
   34c78:	cmp	x13, x10
   34c7c:	b.cc	34cc4 <__gmpn_gcdext_hook@@Base+0x2e8>  // b.lo, b.ul, b.last
   34c80:	add	x14, x25, x26
   34c84:	add	x11, x11, x14
   34c88:	and	x10, x9, #0xfffffffffffffffc
   34c8c:	add	x13, x12, #0x10
   34c90:	add	x11, x11, x23
   34c94:	add	x20, x20, x10
   34c98:	add	x12, x24, x13
   34c9c:	add	x13, x21, x13
   34ca0:	and	x11, x11, #0xfffffffffffffffc
   34ca4:	ldp	q0, q1, [x12, #-16]
   34ca8:	add	x12, x12, #0x20
   34cac:	subs	x11, x11, #0x4
   34cb0:	stp	q0, q1, [x13, #-16]
   34cb4:	add	x13, x13, #0x20
   34cb8:	b.ne	34ca4 <__gmpn_gcdext_hook@@Base+0x2c8>  // b.any
   34cbc:	cmp	x9, x10
   34cc0:	b.eq	34cec <__gmpn_gcdext_hook@@Base+0x310>  // b.none
   34cc4:	sub	x9, x27, x20
   34cc8:	lsl	x10, x20, #3
   34ccc:	add	x9, x9, x8
   34cd0:	add	x8, x21, x10
   34cd4:	add	x9, x9, x23
   34cd8:	add	x10, x24, x10
   34cdc:	ldr	x11, [x10], #8
   34ce0:	subs	x9, x9, #0x1
   34ce4:	str	x11, [x8], #8
   34ce8:	b.ne	34cdc <__gmpn_gcdext_hook@@Base+0x300>  // b.any
   34cec:	mov	x0, xzr
   34cf0:	mov	x20, x22
   34cf4:	cmp	x0, #0x0
   34cf8:	cinc	x8, x20, ne  // ne = any
   34cfc:	str	x0, [x21, x20, lsl #3]
   34d00:	str	x8, [x19, #32]
   34d04:	ldp	x20, x19, [sp, #80]
   34d08:	ldp	x22, x21, [sp, #64]
   34d0c:	ldp	x24, x23, [sp, #48]
   34d10:	ldp	x26, x25, [sp, #32]
   34d14:	ldr	x27, [sp, #16]
   34d18:	ldp	x29, x30, [sp], #96
   34d1c:	ret

0000000000034d20 <__gmpn_gcdext_lehmer_n@@Base>:
   34d20:	sub	sp, sp, #0xe0
   34d24:	stp	x28, x27, [sp, #144]
   34d28:	stp	x26, x25, [sp, #160]
   34d2c:	stp	x24, x23, [sp, #176]
   34d30:	stp	x20, x19, [sp, #208]
   34d34:	mov	x19, x6
   34d38:	mov	x26, x5
   34d3c:	mov	x27, x4
   34d40:	mov	x28, x3
   34d44:	mov	x23, x2
   34d48:	mov	x24, x1
   34d4c:	adds	x20, x5, #0x1
   34d50:	mov	x25, x0
   34d54:	stp	x29, x30, [sp, #128]
   34d58:	stp	x22, x21, [sp, #192]
   34d5c:	add	x29, sp, #0x80
   34d60:	b.cs	34d7c <__gmpn_gcdext_lehmer_n@@Base+0x5c>  // b.hs, b.nlast
   34d64:	mov	w8, #0x18                  	// #24
   34d68:	orr	x9, xzr, #0x18
   34d6c:	madd	x2, x26, x8, x9
   34d70:	mov	x0, x19
   34d74:	mov	w1, wzr
   34d78:	bl	c5f0 <memset@plt>
   34d7c:	lsl	x8, x20, #3
   34d80:	mov	w21, #0x1                   	// #1
   34d84:	cmp	x26, #0x2
   34d88:	add	x22, x19, x8
   34d8c:	str	x21, [x22]
   34d90:	stp	x25, x24, [sp, #8]
   34d94:	str	x25, [sp, #64]
   34d98:	stp	x24, x23, [sp, #80]
   34d9c:	str	x23, [sp]
   34da0:	b.lt	34f2c <__gmpn_gcdext_lehmer_n@@Base+0x20c>  // b.tstop
   34da4:	add	x23, x22, x8
   34da8:	mov	w21, #0x1                   	// #1
   34dac:	add	x20, x23, x20, lsl #3
   34db0:	mov	x25, x26
   34db4:	mov	x24, x19
   34db8:	b	34e14 <__gmpn_gcdext_lehmer_n@@Base+0xf4>
   34dbc:	add	x0, sp, #0x20
   34dc0:	mov	x1, x20
   34dc4:	mov	x2, x28
   34dc8:	mov	x3, x27
   34dcc:	mov	x4, x25
   34dd0:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   34dd4:	mov	x25, x0
   34dd8:	add	x0, sp, #0x20
   34ddc:	mov	x1, x23
   34de0:	mov	x2, x24
   34de4:	mov	x3, x22
   34de8:	mov	x4, x21
   34dec:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   34df0:	mov	x21, x0
   34df4:	mov	x0, x24
   34df8:	mov	x1, x28
   34dfc:	mov	x28, x20
   34e00:	mov	x24, x23
   34e04:	mov	x20, x1
   34e08:	mov	x23, x0
   34e0c:	cmp	x25, #0x1
   34e10:	b.le	34f30 <__gmpn_gcdext_lehmer_n@@Base+0x210>
   34e14:	lsl	x9, x25, #3
   34e18:	sub	x8, x9, #0x8
   34e1c:	ldr	x0, [x28, x8]
   34e20:	ldr	x2, [x27, x8]
   34e24:	orr	x8, x2, x0
   34e28:	tbnz	x8, #63, 34e74 <__gmpn_gcdext_lehmer_n@@Base+0x154>
   34e2c:	cmp	x25, #0x2
   34e30:	clz	x8, x8
   34e34:	b.ne	34e90 <__gmpn_gcdext_lehmer_n@@Base+0x170>  // b.any
   34e38:	ldp	x10, x9, [x28]
   34e3c:	ldp	x13, x12, [x27]
   34e40:	neg	x11, x8
   34e44:	lsl	x9, x9, x8
   34e48:	lsr	x14, x10, x11
   34e4c:	lsl	x1, x10, x8
   34e50:	lsl	x10, x12, x8
   34e54:	lsr	x11, x13, x11
   34e58:	orr	x0, x14, x9
   34e5c:	orr	x2, x11, x10
   34e60:	lsl	x3, x13, x8
   34e64:	add	x4, sp, #0x20
   34e68:	bl	c5a0 <__gmpn_hgcd2@plt>
   34e6c:	cbnz	w0, 34dbc <__gmpn_gcdext_lehmer_n@@Base+0x9c>
   34e70:	b	34ee8 <__gmpn_gcdext_lehmer_n@@Base+0x1c8>
   34e74:	sub	x8, x9, #0x10
   34e78:	ldr	x1, [x28, x8]
   34e7c:	ldr	x3, [x27, x8]
   34e80:	add	x4, sp, #0x20
   34e84:	bl	c5a0 <__gmpn_hgcd2@plt>
   34e88:	cbnz	w0, 34dbc <__gmpn_gcdext_lehmer_n@@Base+0x9c>
   34e8c:	b	34ee8 <__gmpn_gcdext_lehmer_n@@Base+0x1c8>
   34e90:	sub	x11, x9, #0x10
   34e94:	sub	x9, x9, #0x18
   34e98:	ldr	x14, [x28, x11]
   34e9c:	ldr	x15, [x28, x9]
   34ea0:	ldr	x11, [x27, x11]
   34ea4:	ldr	x9, [x27, x9]
   34ea8:	neg	x12, x8
   34eac:	lsl	x10, x0, x8
   34eb0:	lsl	x13, x2, x8
   34eb4:	lsr	x16, x14, x12
   34eb8:	lsl	x14, x14, x8
   34ebc:	lsr	x15, x15, x12
   34ec0:	lsl	x8, x11, x8
   34ec4:	lsr	x11, x11, x12
   34ec8:	lsr	x9, x9, x12
   34ecc:	orr	x0, x16, x10
   34ed0:	orr	x1, x15, x14
   34ed4:	orr	x2, x11, x13
   34ed8:	orr	x3, x9, x8
   34edc:	add	x4, sp, #0x20
   34ee0:	bl	c5a0 <__gmpn_hgcd2@plt>
   34ee4:	cbnz	w0, 34dbc <__gmpn_gcdext_lehmer_n@@Base+0x9c>
   34ee8:	stp	x22, x23, [sp, #112]
   34eec:	stp	x21, x24, [sp, #96]
   34ef0:	adrp	x4, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   34ef4:	ldr	x4, [x4, #3968]
   34ef8:	add	x5, sp, #0x40
   34efc:	mov	x0, x28
   34f00:	mov	x1, x27
   34f04:	mov	x2, x25
   34f08:	mov	x3, xzr
   34f0c:	mov	x6, x20
   34f10:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   34f14:	cbz	x0, 35050 <__gmpn_gcdext_lehmer_n@@Base+0x330>
   34f18:	ldr	x21, [sp, #96]
   34f1c:	mov	x25, x0
   34f20:	cmp	x25, #0x1
   34f24:	b.gt	34e14 <__gmpn_gcdext_lehmer_n@@Base+0xf4>
   34f28:	b	34f30 <__gmpn_gcdext_lehmer_n@@Base+0x210>
   34f2c:	mov	x24, x19
   34f30:	ldr	x2, [x28]
   34f34:	cbz	x2, 3513c <__gmpn_gcdext_lehmer_n@@Base+0x41c>
   34f38:	ldr	x3, [x27]
   34f3c:	ldp	x20, x23, [sp, #8]
   34f40:	cbz	x3, 35154 <__gmpn_gcdext_lehmer_n@@Base+0x434>
   34f44:	cmp	x2, x3
   34f48:	b.ne	34fe4 <__gmpn_gcdext_lehmer_n@@Base+0x2c4>  // b.any
   34f4c:	add	x8, x19, x26, lsl #3
   34f50:	mov	x9, x21
   34f54:	str	x2, [x20]
   34f58:	subs	x10, x9, #0x1
   34f5c:	b.lt	34f80 <__gmpn_gcdext_lehmer_n@@Base+0x260>  // b.tstop
   34f60:	lsl	x9, x9, #3
   34f64:	add	x11, x24, x9
   34f68:	ldur	x11, [x11, #-8]
   34f6c:	ldr	x9, [x8, x9]
   34f70:	cmp	x11, x9
   34f74:	mov	x9, x10
   34f78:	b.eq	34f58 <__gmpn_gcdext_lehmer_n@@Base+0x238>  // b.none
   34f7c:	b.ls	34fac <__gmpn_gcdext_lehmer_n@@Base+0x28c>  // b.plast
   34f80:	add	x8, x19, x26, lsl #3
   34f84:	ldr	x10, [x8, x21, lsl #3]
   34f88:	sub	x9, x21, #0x1
   34f8c:	mov	x21, x9
   34f90:	cbz	x10, 34f84 <__gmpn_gcdext_lehmer_n@@Base+0x264>
   34f94:	add	x19, x9, #0x1
   34f98:	mov	x0, x23
   34f9c:	mov	x1, x22
   34fa0:	mov	x2, x19
   34fa4:	bl	ca50 <__gmpn_copyi@plt>
   34fa8:	b	34fd8 <__gmpn_gcdext_lehmer_n@@Base+0x2b8>
   34fac:	mov	x19, x21
   34fb0:	subs	x21, x21, #0x1
   34fb4:	b.lt	34fc4 <__gmpn_gcdext_lehmer_n@@Base+0x2a4>  // b.tstop
   34fb8:	add	x8, x24, x19, lsl #3
   34fbc:	ldur	x8, [x8, #-8]
   34fc0:	cbz	x8, 34fac <__gmpn_gcdext_lehmer_n@@Base+0x28c>
   34fc4:	mov	x0, x23
   34fc8:	mov	x1, x24
   34fcc:	mov	x2, x19
   34fd0:	bl	ca50 <__gmpn_copyi@plt>
   34fd4:	neg	x19, x19
   34fd8:	ldr	x8, [sp]
   34fdc:	str	x19, [x8]
   34fe0:	b	35118 <__gmpn_gcdext_lehmer_n@@Base+0x3f8>
   34fe4:	add	x0, sp, #0x20
   34fe8:	add	x1, sp, #0x18
   34fec:	bl	d140 <__gmpn_gcdext_1@plt>
   34ff0:	str	x0, [x20]
   34ff4:	ldr	x3, [sp, #32]
   34ff8:	cbz	x3, 35028 <__gmpn_gcdext_lehmer_n@@Base+0x308>
   34ffc:	ldr	x8, [sp, #24]
   35000:	cbz	x8, 35058 <__gmpn_gcdext_lehmer_n@@Base+0x338>
   35004:	cmp	x3, #0x1
   35008:	b.lt	35088 <__gmpn_gcdext_lehmer_n@@Base+0x368>  // b.tstop
   3500c:	neg	x8, x8
   35010:	mov	w20, wzr
   35014:	str	x8, [sp, #24]
   35018:	b	35094 <__gmpn_gcdext_lehmer_n@@Base+0x374>
   3501c:	add	x8, x24, x19, lsl #3
   35020:	ldur	x8, [x8, #-8]
   35024:	cbnz	x8, 35034 <__gmpn_gcdext_lehmer_n@@Base+0x314>
   35028:	mov	x19, x21
   3502c:	subs	x21, x21, #0x1
   35030:	b.ge	3501c <__gmpn_gcdext_lehmer_n@@Base+0x2fc>  // b.tcont
   35034:	mov	x0, x23
   35038:	mov	x1, x24
   3503c:	mov	x2, x19
   35040:	bl	ca50 <__gmpn_copyi@plt>
   35044:	ldr	x11, [sp]
   35048:	neg	x19, x19
   3504c:	b	35114 <__gmpn_gcdext_lehmer_n@@Base+0x3f4>
   35050:	ldr	x0, [sp, #72]
   35054:	b	3511c <__gmpn_gcdext_lehmer_n@@Base+0x3fc>
   35058:	add	x8, x19, x26, lsl #3
   3505c:	mov	x19, x21
   35060:	subs	x21, x21, #0x1
   35064:	b.lt	35070 <__gmpn_gcdext_lehmer_n@@Base+0x350>  // b.tstop
   35068:	ldr	x9, [x8, x19, lsl #3]
   3506c:	cbz	x9, 3505c <__gmpn_gcdext_lehmer_n@@Base+0x33c>
   35070:	mov	x0, x23
   35074:	mov	x1, x22
   35078:	mov	x2, x19
   3507c:	bl	ca50 <__gmpn_copyi@plt>
   35080:	ldr	x11, [sp]
   35084:	b	35114 <__gmpn_gcdext_lehmer_n@@Base+0x3f4>
   35088:	neg	x3, x3
   3508c:	mov	w20, #0x1                   	// #1
   35090:	str	x3, [sp, #32]
   35094:	mov	x0, x23
   35098:	mov	x1, x22
   3509c:	mov	x2, x21
   350a0:	bl	d490 <__gmpn_mul_1@plt>
   350a4:	ldr	x3, [sp, #24]
   350a8:	mov	x19, x0
   350ac:	mov	x0, x23
   350b0:	mov	x1, x24
   350b4:	mov	x2, x21
   350b8:	bl	d400 <__gmpn_addmul_1@plt>
   350bc:	orr	x8, x0, x19
   350c0:	cbz	x8, 350e8 <__gmpn_gcdext_lehmer_n@@Base+0x3c8>
   350c4:	ldr	x11, [sp]
   350c8:	adds	x9, x0, x19
   350cc:	add	x8, x21, #0x1
   350d0:	str	x9, [x23, x21, lsl #3]
   350d4:	b.cc	350f0 <__gmpn_gcdext_lehmer_n@@Base+0x3d0>  // b.lo, b.ul, b.last
   350d8:	add	x21, x21, #0x2
   350dc:	mov	w9, #0x1                   	// #1
   350e0:	str	x9, [x23, x8, lsl #3]
   350e4:	b	350f4 <__gmpn_gcdext_lehmer_n@@Base+0x3d4>
   350e8:	ldr	x11, [sp]
   350ec:	b	350f4 <__gmpn_gcdext_lehmer_n@@Base+0x3d4>
   350f0:	mov	x21, x8
   350f4:	sub	x8, x23, #0x8
   350f8:	ldr	x10, [x8, x21, lsl #3]
   350fc:	sub	x9, x21, #0x1
   35100:	mov	x21, x9
   35104:	cbz	x10, 350f8 <__gmpn_gcdext_lehmer_n@@Base+0x3d8>
   35108:	mvn	x8, x9
   3510c:	cmp	w20, #0x0
   35110:	csinc	x19, x8, x9, ne  // ne = any
   35114:	str	x19, [x11]
   35118:	mov	w0, #0x1                   	// #1
   3511c:	ldp	x20, x19, [sp, #208]
   35120:	ldp	x22, x21, [sp, #192]
   35124:	ldp	x24, x23, [sp, #176]
   35128:	ldp	x26, x25, [sp, #160]
   3512c:	ldp	x28, x27, [sp, #144]
   35130:	ldp	x29, x30, [sp, #128]
   35134:	add	sp, sp, #0xe0
   35138:	ret
   3513c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   35140:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   35144:	add	x0, x0, #0x5d8
   35148:	add	x2, x2, #0x5e8
   3514c:	mov	w1, #0xf9                  	// #249
   35150:	bl	c6c0 <__gmp_assert_fail@plt>
   35154:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   35158:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   3515c:	add	x0, x0, #0x5d8
   35160:	add	x2, x2, #0x5f2
   35164:	mov	w1, #0xfa                  	// #250
   35168:	bl	c6c0 <__gmp_assert_fail@plt>

000000000003516c <__gmpn_div_q@@Base>:
   3516c:	stp	x29, x30, [sp, #-96]!
   35170:	stp	x28, x27, [sp, #16]
   35174:	stp	x26, x25, [sp, #32]
   35178:	stp	x24, x23, [sp, #48]
   3517c:	stp	x22, x21, [sp, #64]
   35180:	stp	x20, x19, [sp, #80]
   35184:	mov	x29, sp
   35188:	sub	sp, sp, #0x60
   3518c:	stur	xzr, [x29, #-16]
   35190:	subs	x25, x4, #0x1
   35194:	ldr	x28, [x3, x25, lsl #3]
   35198:	mov	x21, x2
   3519c:	mov	x2, x1
   351a0:	mov	x19, x0
   351a4:	b.ne	351c0 <__gmpn_div_q@@Base+0x54>  // b.any
   351a8:	mov	x0, x19
   351ac:	mov	x1, xzr
   351b0:	mov	x3, x21
   351b4:	mov	x4, x28
   351b8:	bl	cd00 <__gmpn_divrem_1@plt>
   351bc:	b	35be8 <__gmpn_div_q@@Base+0xa7c>
   351c0:	sub	x22, x21, x4
   351c4:	add	x8, x22, #0x6
   351c8:	mov	x23, x5
   351cc:	mov	x20, x4
   351d0:	mov	x24, x3
   351d4:	cmp	x8, x4
   351d8:	b.ge	352c0 <__gmpn_div_q@@Base+0x154>  // b.tcont
   351dc:	add	x8, x22, #0x2
   351e0:	stp	x22, x8, [x29, #-48]
   351e4:	lsl	x26, x8, #3
   351e8:	mov	w8, #0x7f00                	// #32512
   351ec:	cmp	x26, x8
   351f0:	add	x25, x22, #0x1
   351f4:	mov	x22, x24
   351f8:	stur	x23, [x29, #-24]
   351fc:	b.hi	35780 <__gmpn_div_q@@Base+0x614>  // b.pmore
   35200:	add	x9, x26, #0xf
   35204:	mov	x8, sp
   35208:	and	x9, x9, #0xfffffffffffffff0
   3520c:	sub	x8, x8, x9
   35210:	stur	x8, [x29, #-56]
   35214:	mov	sp, x8
   35218:	ldur	x8, [x29, #-24]
   3521c:	mov	w27, #0x1                   	// #1
   35220:	bfi	x27, x25, #1, #63
   35224:	lsl	x24, x27, #3
   35228:	cmp	x8, x2
   3522c:	stur	x25, [x29, #-32]
   35230:	b.ne	3525c <__gmpn_div_q@@Base+0xf0>  // b.any
   35234:	add	x1, x24, #0x8
   35238:	mov	w8, #0x7f00                	// #32512
   3523c:	cmp	x1, x8
   35240:	b.hi	358a8 <__gmpn_div_q@@Base+0x73c>  // b.pmore
   35244:	add	x9, x1, #0xf
   35248:	mov	x8, sp
   3524c:	and	x9, x9, #0xfffffffffffffff0
   35250:	sub	x8, x8, x9
   35254:	stur	x8, [x29, #-24]
   35258:	mov	sp, x8
   3525c:	stp	x2, x22, [x29, #-80]
   35260:	tbnz	x28, #63, 358c4 <__gmpn_div_q@@Base+0x758>
   35264:	ldur	x25, [x29, #-24]
   35268:	clz	x28, x28
   3526c:	add	x8, x2, x21, lsl #3
   35270:	sub	x1, x8, x24
   35274:	mov	x0, x25
   35278:	mov	x2, x27
   3527c:	mov	w3, w28
   35280:	bl	c180 <__gmpn_lshift@plt>
   35284:	cmp	x0, #0x0
   35288:	mov	w8, #0x7f00                	// #32512
   3528c:	cset	w9, ne  // ne = any
   35290:	cinc	x23, x27, ne  // ne = any
   35294:	cmp	x26, x8
   35298:	str	x0, [x25, x24]
   3529c:	stur	x9, [x29, #-88]
   352a0:	stur	x0, [x29, #-64]
   352a4:	b.hi	35350 <__gmpn_div_q@@Base+0x1e4>  // b.pmore
   352a8:	add	x9, x26, #0xf
   352ac:	mov	x8, sp
   352b0:	and	x9, x9, #0xfffffffffffffff0
   352b4:	sub	x27, x8, x9
   352b8:	mov	sp, x27
   352bc:	b	35360 <__gmpn_div_q@@Base+0x1f4>
   352c0:	tbnz	x28, #63, 3579c <__gmpn_div_q@@Base+0x630>
   352c4:	clz	x27, x28
   352c8:	mov	x0, x23
   352cc:	mov	x1, x2
   352d0:	mov	x2, x21
   352d4:	mov	w3, w27
   352d8:	bl	c180 <__gmpn_lshift@plt>
   352dc:	cmp	x0, #0x0
   352e0:	lsl	x1, x20, #3
   352e4:	mov	w8, #0x7f00                	// #32512
   352e8:	cinc	x28, x21, ne  // ne = any
   352ec:	cmp	x1, x8
   352f0:	stur	x0, [x29, #-24]
   352f4:	str	x0, [x23, x21, lsl #3]
   352f8:	b.hi	357e4 <__gmpn_div_q@@Base+0x678>  // b.pmore
   352fc:	add	x9, x1, #0xf
   35300:	mov	x8, sp
   35304:	and	x9, x9, #0xfffffffffffffff0
   35308:	sub	x26, x8, x9
   3530c:	mov	sp, x26
   35310:	mov	x0, x26
   35314:	mov	x1, x24
   35318:	mov	x2, x20
   3531c:	mov	w3, w27
   35320:	bl	c180 <__gmpn_lshift@plt>
   35324:	cmp	x20, #0x2
   35328:	b.ne	3541c <__gmpn_div_q@@Base+0x2b0>  // b.any
   3532c:	mov	x0, x19
   35330:	mov	x1, xzr
   35334:	mov	x2, x23
   35338:	mov	x3, x28
   3533c:	mov	x4, x26
   35340:	bl	c200 <__gmpn_divrem_2@plt>
   35344:	ldur	x25, [x29, #-24]
   35348:	cbnz	x25, 35be0 <__gmpn_div_q@@Base+0xa74>
   3534c:	b	35778 <__gmpn_div_q@@Base+0x60c>
   35350:	sub	x0, x29, #0x10
   35354:	mov	x1, x26
   35358:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3535c:	mov	x27, x0
   35360:	mov	x24, x22
   35364:	add	x8, x22, x20, lsl #3
   35368:	ldp	x22, x26, [x29, #-48]
   3536c:	mov	x9, #0xfffffffffffffffe    	// #-2
   35370:	mov	x0, x27
   35374:	mov	w3, w28
   35378:	sub	x9, x9, x22
   3537c:	add	x1, x8, x9, lsl #3
   35380:	mov	x2, x26
   35384:	bl	c180 <__gmpn_lshift@plt>
   35388:	sub	x8, x20, x22
   3538c:	add	x8, x24, x8, lsl #3
   35390:	ldur	x8, [x8, #-24]
   35394:	ldr	x9, [x27]
   35398:	ldp	x25, x24, [x29, #-32]
   3539c:	neg	x10, x28
   353a0:	lsr	x8, x8, x10
   353a4:	orr	x8, x9, x8
   353a8:	str	x8, [x27]
   353ac:	cbz	x22, 354dc <__gmpn_div_q@@Base+0x370>
   353b0:	cmp	x22, #0x95
   353b4:	b.le	35504 <__gmpn_div_q@@Base+0x398>
   353b8:	cmp	x22, #0x3e3
   353bc:	b.le	355d8 <__gmpn_div_q@@Base+0x46c>
   353c0:	mov	x0, x23
   353c4:	mov	x1, x26
   353c8:	mov	w2, wzr
   353cc:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   353d0:	lsl	x1, x0, #3
   353d4:	mov	w8, #0x7f00                	// #32512
   353d8:	cmp	x1, x8
   353dc:	b.hi	35ac4 <__gmpn_div_q@@Base+0x958>  // b.pmore
   353e0:	add	x9, x1, #0xf
   353e4:	mov	x8, sp
   353e8:	and	x9, x9, #0xfffffffffffffff0
   353ec:	sub	x5, x8, x9
   353f0:	mov	sp, x5
   353f4:	ldur	x22, [x29, #-56]
   353f8:	mov	x1, x24
   353fc:	mov	x2, x23
   35400:	mov	x3, x27
   35404:	mov	x0, x22
   35408:	mov	x4, x26
   3540c:	bl	c710 <__gmpn_mu_divappr_q@plt>
   35410:	ldur	x8, [x29, #-64]
   35414:	cbnz	x8, 356f4 <__gmpn_div_q@@Base+0x588>
   35418:	b	35b54 <__gmpn_div_q@@Base+0x9e8>
   3541c:	cmp	x20, #0x98
   35420:	b.lt	3556c <__gmpn_div_q@@Base+0x400>  // b.tstop
   35424:	sub	x8, x28, x20
   35428:	cmp	x8, #0x97
   3542c:	b.le	3556c <__gmpn_div_q@@Base+0x400>
   35430:	cmp	x21, #0x7cc
   35434:	b.lt	35470 <__gmpn_div_q@@Base+0x304>  // b.tstop
   35438:	mov	x8, #0x200000000000        	// #35184372088832
   3543c:	mov	x9, #0x800000000000        	// #140737488355328
   35440:	movk	x8, #0x409c, lsl #48
   35444:	movk	x9, #0x4058, lsl #48
   35448:	scvtf	d0, x20
   3544c:	scvtf	d1, x21
   35450:	fmov	d2, x8
   35454:	fmov	d3, x9
   35458:	fmul	d2, d0, d2
   3545c:	fmul	d3, d1, d3
   35460:	fadd	d2, d3, d2
   35464:	fmul	d0, d1, d0
   35468:	fcmp	d2, d0
   3546c:	b.le	35724 <__gmpn_div_q@@Base+0x5b8>
   35470:	ldr	x21, [x26, x25, lsl #3]
   35474:	mov	x0, x21
   35478:	bl	d3f0 <__gmpn_invert_limb@plt>
   3547c:	add	x8, x26, x20, lsl #3
   35480:	ldur	x8, [x8, #-16]
   35484:	mul	x9, x0, x21
   35488:	adds	x9, x9, x8
   3548c:	b.cc	354a8 <__gmpn_div_q@@Base+0x33c>  // b.lo, b.ul, b.last
   35490:	subs	x9, x9, x21
   35494:	cset	w10, cs  // cs = hs, nlast
   35498:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   3549c:	mvn	x10, x10
   354a0:	add	x0, x10, x0
   354a4:	sub	x9, x9, x11
   354a8:	ldur	x25, [x29, #-24]
   354ac:	umulh	x10, x8, x0
   354b0:	adds	x10, x10, x9
   354b4:	b.cc	35698 <__gmpn_div_q@@Base+0x52c>  // b.lo, b.ul, b.last
   354b8:	cmp	x10, x21
   354bc:	sub	x9, x0, #0x1
   354c0:	b.cc	3569c <__gmpn_div_q@@Base+0x530>  // b.lo, b.ul, b.last
   354c4:	mul	x11, x0, x8
   354c8:	cmp	x10, x21
   354cc:	sub	x12, x0, #0x2
   354d0:	ccmp	x11, x8, #0x2, ls  // ls = plast
   354d4:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   354d8:	b	3569c <__gmpn_div_q@@Base+0x530>
   354dc:	ldur	x22, [x29, #-56]
   354e0:	mov	x1, xzr
   354e4:	mov	x2, x24
   354e8:	mov	x3, x23
   354ec:	mov	x0, x22
   354f0:	mov	x4, x27
   354f4:	bl	c200 <__gmpn_divrem_2@plt>
   354f8:	ldur	x8, [x29, #-64]
   354fc:	cbnz	x8, 356f4 <__gmpn_div_q@@Base+0x588>
   35500:	b	35b54 <__gmpn_div_q@@Base+0x9e8>
   35504:	ldr	x26, [x27, x25, lsl #3]
   35508:	mov	x0, x26
   3550c:	bl	d3f0 <__gmpn_invert_limb@plt>
   35510:	ldr	x8, [x27, x22, lsl #3]
   35514:	mul	x9, x0, x26
   35518:	adds	x9, x9, x8
   3551c:	b.cc	35538 <__gmpn_div_q@@Base+0x3cc>  // b.lo, b.ul, b.last
   35520:	subs	x9, x9, x26
   35524:	cset	w10, cs  // cs = hs, nlast
   35528:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   3552c:	mvn	x10, x10
   35530:	add	x0, x10, x0
   35534:	sub	x9, x9, x11
   35538:	ldur	x22, [x29, #-56]
   3553c:	umulh	x10, x8, x0
   35540:	adds	x9, x10, x9
   35544:	b.cc	35640 <__gmpn_div_q@@Base+0x4d4>  // b.lo, b.ul, b.last
   35548:	cmp	x9, x26
   3554c:	sub	x5, x0, #0x1
   35550:	b.cc	35644 <__gmpn_div_q@@Base+0x4d8>  // b.lo, b.ul, b.last
   35554:	mul	x10, x0, x8
   35558:	cmp	x9, x26
   3555c:	sub	x11, x0, #0x2
   35560:	ccmp	x10, x8, #0x2, ls  // ls = plast
   35564:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   35568:	b	35644 <__gmpn_div_q@@Base+0x4d8>
   3556c:	ldr	x21, [x26, x25, lsl #3]
   35570:	mov	x0, x21
   35574:	bl	d3f0 <__gmpn_invert_limb@plt>
   35578:	add	x8, x26, x20, lsl #3
   3557c:	ldur	x8, [x8, #-16]
   35580:	mul	x9, x0, x21
   35584:	adds	x9, x9, x8
   35588:	b.cc	355a4 <__gmpn_div_q@@Base+0x438>  // b.lo, b.ul, b.last
   3558c:	subs	x9, x9, x21
   35590:	cset	w10, cs  // cs = hs, nlast
   35594:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   35598:	mvn	x10, x10
   3559c:	add	x0, x10, x0
   355a0:	sub	x9, x9, x11
   355a4:	ldur	x25, [x29, #-24]
   355a8:	umulh	x10, x8, x0
   355ac:	adds	x9, x10, x9
   355b0:	b.cc	35670 <__gmpn_div_q@@Base+0x504>  // b.lo, b.ul, b.last
   355b4:	cmp	x9, x21
   355b8:	sub	x5, x0, #0x1
   355bc:	b.cc	35674 <__gmpn_div_q@@Base+0x508>  // b.lo, b.ul, b.last
   355c0:	mul	x10, x0, x8
   355c4:	cmp	x9, x21
   355c8:	sub	x11, x0, #0x2
   355cc:	ccmp	x10, x8, #0x2, ls  // ls = plast
   355d0:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   355d4:	b	35674 <__gmpn_div_q@@Base+0x508>
   355d8:	ldr	x26, [x27, x25, lsl #3]
   355dc:	mov	x0, x26
   355e0:	bl	d3f0 <__gmpn_invert_limb@plt>
   355e4:	ldr	x8, [x27, x22, lsl #3]
   355e8:	mul	x9, x0, x26
   355ec:	adds	x9, x9, x8
   355f0:	b.cc	3560c <__gmpn_div_q@@Base+0x4a0>  // b.lo, b.ul, b.last
   355f4:	subs	x9, x9, x26
   355f8:	cset	w10, cs  // cs = hs, nlast
   355fc:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   35600:	mvn	x10, x10
   35604:	add	x0, x10, x0
   35608:	sub	x9, x9, x11
   3560c:	ldur	x22, [x29, #-56]
   35610:	umulh	x10, x8, x0
   35614:	adds	x10, x10, x9
   35618:	b.cc	356c4 <__gmpn_div_q@@Base+0x558>  // b.lo, b.ul, b.last
   3561c:	cmp	x10, x26
   35620:	sub	x9, x0, #0x1
   35624:	b.cc	356c8 <__gmpn_div_q@@Base+0x55c>  // b.lo, b.ul, b.last
   35628:	mul	x11, x0, x8
   3562c:	cmp	x10, x26
   35630:	sub	x12, x0, #0x2
   35634:	ccmp	x11, x8, #0x2, ls  // ls = plast
   35638:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   3563c:	b	356c8 <__gmpn_div_q@@Base+0x55c>
   35640:	mov	x5, x0
   35644:	ldur	x26, [x29, #-40]
   35648:	mov	x0, x22
   3564c:	mov	x1, x24
   35650:	mov	x2, x23
   35654:	mov	x3, x27
   35658:	mov	x4, x26
   3565c:	stur	x5, [x29, #-8]
   35660:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   35664:	ldur	x8, [x29, #-64]
   35668:	cbnz	x8, 356f4 <__gmpn_div_q@@Base+0x588>
   3566c:	b	35b54 <__gmpn_div_q@@Base+0x9e8>
   35670:	mov	x5, x0
   35674:	mov	x0, x19
   35678:	mov	x1, x23
   3567c:	mov	x2, x28
   35680:	mov	x3, x26
   35684:	mov	x4, x20
   35688:	stur	x5, [x29, #-8]
   3568c:	bl	cee0 <__gmpn_sbpi1_div_q@plt>
   35690:	cbnz	x25, 35be0 <__gmpn_div_q@@Base+0xa74>
   35694:	b	35778 <__gmpn_div_q@@Base+0x60c>
   35698:	mov	x9, x0
   3569c:	sub	x5, x29, #0x8
   356a0:	mov	x0, x19
   356a4:	mov	x1, x23
   356a8:	mov	x2, x28
   356ac:	mov	x3, x26
   356b0:	mov	x4, x20
   356b4:	stur	x9, [x29, #-8]
   356b8:	bl	caa0 <__gmpn_dcpi1_div_q@plt>
   356bc:	cbnz	x25, 35be0 <__gmpn_div_q@@Base+0xa74>
   356c0:	b	35778 <__gmpn_div_q@@Base+0x60c>
   356c4:	mov	x9, x0
   356c8:	ldur	x26, [x29, #-40]
   356cc:	sub	x5, x29, #0x8
   356d0:	mov	x0, x22
   356d4:	mov	x1, x24
   356d8:	mov	x2, x23
   356dc:	mov	x3, x27
   356e0:	mov	x4, x26
   356e4:	stur	x9, [x29, #-8]
   356e8:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   356ec:	ldur	x8, [x29, #-64]
   356f0:	cbz	x8, 35b54 <__gmpn_div_q@@Base+0x9e8>
   356f4:	cbz	x0, 35b58 <__gmpn_div_q@@Base+0x9ec>
   356f8:	cmp	x23, x26
   356fc:	b.le	35b58 <__gmpn_div_q@@Base+0x9ec>
   35700:	ldur	x8, [x29, #-88]
   35704:	mov	w1, #0xff                  	// #255
   35708:	mov	x0, x22
   3570c:	add	x8, x8, x21
   35710:	sub	x8, x8, x20
   35714:	lsl	x8, x8, #3
   35718:	add	x2, x8, #0x8
   3571c:	bl	c5f0 <memset@plt>
   35720:	b	35b58 <__gmpn_div_q@@Base+0x9ec>
   35724:	mov	x0, x28
   35728:	mov	x1, x20
   3572c:	mov	w2, wzr
   35730:	bl	cfd0 <__gmpn_mu_div_q_itch@plt>
   35734:	lsl	x1, x0, #3
   35738:	mov	w8, #0x7f00                	// #32512
   3573c:	cmp	x1, x8
   35740:	b.hi	35cac <__gmpn_div_q@@Base+0xb40>  // b.pmore
   35744:	add	x9, x1, #0xf
   35748:	mov	x8, sp
   3574c:	and	x9, x9, #0xfffffffffffffff0
   35750:	sub	x5, x8, x9
   35754:	mov	sp, x5
   35758:	ldur	x25, [x29, #-24]
   3575c:	mov	x0, x19
   35760:	mov	x1, x23
   35764:	mov	x2, x28
   35768:	mov	x3, x26
   3576c:	mov	x4, x20
   35770:	bl	c2e0 <__gmpn_mu_div_q@plt>
   35774:	cbnz	x25, 35be0 <__gmpn_div_q@@Base+0xa74>
   35778:	str	x0, [x19, x22, lsl #3]
   3577c:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   35780:	sub	x0, x29, #0x10
   35784:	mov	x1, x26
   35788:	mov	x24, x2
   3578c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35790:	mov	x2, x24
   35794:	stur	x0, [x29, #-56]
   35798:	b	35218 <__gmpn_div_q@@Base+0xac>
   3579c:	cmp	x23, x2
   357a0:	b.eq	357bc <__gmpn_div_q@@Base+0x650>  // b.none
   357a4:	mov	x0, x23
   357a8:	mov	x1, x2
   357ac:	mov	x25, x2
   357b0:	mov	x2, x21
   357b4:	bl	ca50 <__gmpn_copyi@plt>
   357b8:	mov	x2, x25
   357bc:	cmp	x20, #0x2
   357c0:	b.ne	357f4 <__gmpn_div_q@@Base+0x688>  // b.any
   357c4:	mov	x0, x19
   357c8:	mov	x1, xzr
   357cc:	mov	x2, x23
   357d0:	mov	x3, x21
   357d4:	mov	x4, x24
   357d8:	bl	c200 <__gmpn_divrem_2@plt>
   357dc:	str	x0, [x19, x22, lsl #3]
   357e0:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   357e4:	sub	x0, x29, #0x10
   357e8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   357ec:	mov	x26, x0
   357f0:	b	35310 <__gmpn_div_q@@Base+0x1a4>
   357f4:	cmp	x20, #0x98
   357f8:	b.lt	359d4 <__gmpn_div_q@@Base+0x868>  // b.tstop
   357fc:	cmp	x22, #0x97
   35800:	b.le	359d4 <__gmpn_div_q@@Base+0x868>
   35804:	cmp	x21, #0x7cc
   35808:	b.lt	35844 <__gmpn_div_q@@Base+0x6d8>  // b.tstop
   3580c:	mov	x8, #0x200000000000        	// #35184372088832
   35810:	mov	x9, #0x800000000000        	// #140737488355328
   35814:	movk	x8, #0x409c, lsl #48
   35818:	movk	x9, #0x4058, lsl #48
   3581c:	scvtf	d0, x20
   35820:	scvtf	d1, x21
   35824:	fmov	d2, x8
   35828:	fmov	d3, x9
   3582c:	fmul	d2, d0, d2
   35830:	fmul	d3, d1, d3
   35834:	fadd	d2, d3, d2
   35838:	fmul	d0, d1, d0
   3583c:	fcmp	d2, d0
   35840:	b.le	35c54 <__gmpn_div_q@@Base+0xae8>
   35844:	mov	x0, x28
   35848:	bl	d3f0 <__gmpn_invert_limb@plt>
   3584c:	add	x8, x24, x20, lsl #3
   35850:	ldur	x8, [x8, #-16]
   35854:	mul	x9, x0, x28
   35858:	adds	x9, x9, x8
   3585c:	b.cc	35878 <__gmpn_div_q@@Base+0x70c>  // b.lo, b.ul, b.last
   35860:	subs	x9, x9, x28
   35864:	cset	w10, cs  // cs = hs, nlast
   35868:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   3586c:	mvn	x10, x10
   35870:	add	x0, x10, x0
   35874:	sub	x9, x9, x11
   35878:	umulh	x10, x8, x0
   3587c:	adds	x10, x10, x9
   35880:	b.cc	35afc <__gmpn_div_q@@Base+0x990>  // b.lo, b.ul, b.last
   35884:	cmp	x10, x28
   35888:	sub	x9, x0, #0x1
   3588c:	b.cc	35b00 <__gmpn_div_q@@Base+0x994>  // b.lo, b.ul, b.last
   35890:	mul	x11, x0, x8
   35894:	cmp	x10, x28
   35898:	sub	x12, x0, #0x2
   3589c:	ccmp	x11, x8, #0x2, ls  // ls = plast
   358a0:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   358a4:	b	35b00 <__gmpn_div_q@@Base+0x994>
   358a8:	sub	x0, x29, #0x10
   358ac:	mov	x25, x2
   358b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358b4:	mov	x2, x25
   358b8:	stur	x0, [x29, #-24]
   358bc:	stp	x2, x22, [x29, #-80]
   358c0:	tbz	x28, #63, 35264 <__gmpn_div_q@@Base+0xf8>
   358c4:	ldur	x0, [x29, #-24]
   358c8:	add	x8, x2, x21, lsl #3
   358cc:	sub	x1, x8, x27, lsl #3
   358d0:	mov	x2, x27
   358d4:	bl	ca50 <__gmpn_copyi@plt>
   358d8:	add	x8, x22, x20, lsl #3
   358dc:	ldur	x22, [x29, #-48]
   358e0:	mov	x9, #0xfffffffffffffffe    	// #-2
   358e4:	sub	x9, x9, x22
   358e8:	add	x25, x8, x9, lsl #3
   358ec:	cbz	x22, 35954 <__gmpn_div_q@@Base+0x7e8>
   358f0:	cmp	x22, #0x95
   358f4:	b.le	35974 <__gmpn_div_q@@Base+0x808>
   358f8:	cmp	x22, #0x3e3
   358fc:	b.le	35a38 <__gmpn_div_q@@Base+0x8cc>
   35900:	ldur	x1, [x29, #-40]
   35904:	mov	x0, x27
   35908:	mov	w2, wzr
   3590c:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   35910:	lsl	x1, x0, #3
   35914:	mov	w8, #0x7f00                	// #32512
   35918:	cmp	x1, x8
   3591c:	b.hi	35cbc <__gmpn_div_q@@Base+0xb50>  // b.pmore
   35920:	add	x9, x1, #0xf
   35924:	mov	x8, sp
   35928:	and	x9, x9, #0xfffffffffffffff0
   3592c:	sub	x5, x8, x9
   35930:	mov	sp, x5
   35934:	ldur	x22, [x29, #-56]
   35938:	ldur	x1, [x29, #-24]
   3593c:	ldur	x4, [x29, #-40]
   35940:	mov	x2, x27
   35944:	mov	x0, x22
   35948:	mov	x3, x25
   3594c:	bl	c710 <__gmpn_mu_divappr_q@plt>
   35950:	b	35b50 <__gmpn_div_q@@Base+0x9e4>
   35954:	ldur	x22, [x29, #-56]
   35958:	ldur	x2, [x29, #-24]
   3595c:	mov	x1, xzr
   35960:	mov	x3, x27
   35964:	mov	x0, x22
   35968:	mov	x4, x25
   3596c:	bl	c200 <__gmpn_divrem_2@plt>
   35970:	b	35b50 <__gmpn_div_q@@Base+0x9e4>
   35974:	mov	x0, x28
   35978:	bl	d3f0 <__gmpn_invert_limb@plt>
   3597c:	ldr	x8, [x25, x22, lsl #3]
   35980:	mul	x9, x0, x28
   35984:	adds	x9, x9, x8
   35988:	b.cc	359a4 <__gmpn_div_q@@Base+0x838>  // b.lo, b.ul, b.last
   3598c:	subs	x9, x9, x28
   35990:	cset	w10, cs  // cs = hs, nlast
   35994:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   35998:	mvn	x10, x10
   3599c:	add	x0, x10, x0
   359a0:	sub	x9, x9, x11
   359a4:	umulh	x10, x8, x0
   359a8:	adds	x9, x10, x9
   359ac:	b.cc	35a9c <__gmpn_div_q@@Base+0x930>  // b.lo, b.ul, b.last
   359b0:	cmp	x9, x28
   359b4:	sub	x5, x0, #0x1
   359b8:	b.cc	35aa0 <__gmpn_div_q@@Base+0x934>  // b.lo, b.ul, b.last
   359bc:	mul	x10, x0, x8
   359c0:	cmp	x9, x28
   359c4:	sub	x11, x0, #0x2
   359c8:	ccmp	x10, x8, #0x2, ls  // ls = plast
   359cc:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   359d0:	b	35aa0 <__gmpn_div_q@@Base+0x934>
   359d4:	mov	x0, x28
   359d8:	bl	d3f0 <__gmpn_invert_limb@plt>
   359dc:	add	x8, x24, x20, lsl #3
   359e0:	ldur	x8, [x8, #-16]
   359e4:	mul	x9, x0, x28
   359e8:	adds	x9, x9, x8
   359ec:	b.cc	35a08 <__gmpn_div_q@@Base+0x89c>  // b.lo, b.ul, b.last
   359f0:	subs	x9, x9, x28
   359f4:	cset	w10, cs  // cs = hs, nlast
   359f8:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   359fc:	mvn	x10, x10
   35a00:	add	x0, x10, x0
   35a04:	sub	x9, x9, x11
   35a08:	umulh	x10, x8, x0
   35a0c:	adds	x9, x10, x9
   35a10:	b.cc	35ad4 <__gmpn_div_q@@Base+0x968>  // b.lo, b.ul, b.last
   35a14:	cmp	x9, x28
   35a18:	sub	x5, x0, #0x1
   35a1c:	b.cc	35ad8 <__gmpn_div_q@@Base+0x96c>  // b.lo, b.ul, b.last
   35a20:	mul	x10, x0, x8
   35a24:	cmp	x9, x28
   35a28:	sub	x11, x0, #0x2
   35a2c:	ccmp	x10, x8, #0x2, ls  // ls = plast
   35a30:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   35a34:	b	35ad8 <__gmpn_div_q@@Base+0x96c>
   35a38:	mov	x0, x28
   35a3c:	bl	d3f0 <__gmpn_invert_limb@plt>
   35a40:	ldur	x8, [x29, #-48]
   35a44:	mul	x9, x0, x28
   35a48:	ldr	x8, [x25, x8, lsl #3]
   35a4c:	adds	x9, x9, x8
   35a50:	b.cc	35a6c <__gmpn_div_q@@Base+0x900>  // b.lo, b.ul, b.last
   35a54:	subs	x9, x9, x28
   35a58:	cset	w10, cs  // cs = hs, nlast
   35a5c:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   35a60:	mvn	x10, x10
   35a64:	add	x0, x10, x0
   35a68:	sub	x9, x9, x11
   35a6c:	umulh	x10, x8, x0
   35a70:	adds	x10, x10, x9
   35a74:	b.cc	35b28 <__gmpn_div_q@@Base+0x9bc>  // b.lo, b.ul, b.last
   35a78:	cmp	x10, x28
   35a7c:	sub	x9, x0, #0x1
   35a80:	b.cc	35b2c <__gmpn_div_q@@Base+0x9c0>  // b.lo, b.ul, b.last
   35a84:	mul	x11, x0, x8
   35a88:	cmp	x10, x28
   35a8c:	sub	x12, x0, #0x2
   35a90:	ccmp	x11, x8, #0x2, ls  // ls = plast
   35a94:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   35a98:	b	35b2c <__gmpn_div_q@@Base+0x9c0>
   35a9c:	mov	x5, x0
   35aa0:	ldur	x22, [x29, #-56]
   35aa4:	ldur	x1, [x29, #-24]
   35aa8:	ldur	x4, [x29, #-40]
   35aac:	mov	x2, x27
   35ab0:	mov	x0, x22
   35ab4:	mov	x3, x25
   35ab8:	stur	x5, [x29, #-8]
   35abc:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   35ac0:	b	35b50 <__gmpn_div_q@@Base+0x9e4>
   35ac4:	sub	x0, x29, #0x10
   35ac8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35acc:	mov	x5, x0
   35ad0:	b	353f4 <__gmpn_div_q@@Base+0x288>
   35ad4:	mov	x5, x0
   35ad8:	mov	x0, x19
   35adc:	mov	x1, x23
   35ae0:	mov	x2, x21
   35ae4:	mov	x3, x24
   35ae8:	mov	x4, x20
   35aec:	stur	x5, [x29, #-8]
   35af0:	bl	cee0 <__gmpn_sbpi1_div_q@plt>
   35af4:	str	x0, [x19, x22, lsl #3]
   35af8:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   35afc:	mov	x9, x0
   35b00:	sub	x5, x29, #0x8
   35b04:	mov	x0, x19
   35b08:	mov	x1, x23
   35b0c:	mov	x2, x21
   35b10:	mov	x3, x24
   35b14:	mov	x4, x20
   35b18:	stur	x9, [x29, #-8]
   35b1c:	bl	caa0 <__gmpn_dcpi1_div_q@plt>
   35b20:	str	x0, [x19, x22, lsl #3]
   35b24:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   35b28:	mov	x9, x0
   35b2c:	ldur	x22, [x29, #-56]
   35b30:	ldur	x1, [x29, #-24]
   35b34:	ldur	x4, [x29, #-40]
   35b38:	sub	x5, x29, #0x8
   35b3c:	mov	x0, x22
   35b40:	mov	x2, x27
   35b44:	mov	x3, x25
   35b48:	stur	x9, [x29, #-8]
   35b4c:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   35b50:	ldur	x25, [x29, #-32]
   35b54:	str	x0, [x22, x25, lsl #3]
   35b58:	add	x23, x22, #0x8
   35b5c:	mov	x0, x19
   35b60:	mov	x1, x23
   35b64:	mov	x2, x25
   35b68:	bl	ca50 <__gmpn_copyi@plt>
   35b6c:	ldr	x8, [x22]
   35b70:	cmp	x8, #0x4
   35b74:	b.hi	35be0 <__gmpn_div_q@@Base+0xa74>  // b.pmore
   35b78:	add	x22, x21, #0x1
   35b7c:	lsl	x1, x22, #3
   35b80:	mov	w8, #0x7f00                	// #32512
   35b84:	cmp	x1, x8
   35b88:	b.hi	35c44 <__gmpn_div_q@@Base+0xad8>  // b.pmore
   35b8c:	add	x9, x1, #0xf
   35b90:	mov	x8, sp
   35b94:	and	x9, x9, #0xfffffffffffffff0
   35b98:	sub	x25, x8, x9
   35b9c:	mov	sp, x25
   35ba0:	ldur	x1, [x29, #-72]
   35ba4:	ldur	x4, [x29, #-32]
   35ba8:	mov	x0, x25
   35bac:	mov	x2, x20
   35bb0:	mov	x3, x23
   35bb4:	bl	ccd0 <__gmpn_mul@plt>
   35bb8:	ldr	x8, [x25, x21, lsl #3]
   35bbc:	cmp	x8, #0x0
   35bc0:	cset	w8, eq  // eq = none
   35bc4:	sub	x8, x22, x8
   35bc8:	cmp	x8, x21
   35bcc:	b.le	35c08 <__gmpn_div_q@@Base+0xa9c>
   35bd0:	ldr	x8, [x19]
   35bd4:	sub	x9, x8, #0x1
   35bd8:	str	x9, [x19], #8
   35bdc:	cbz	x8, 35bd0 <__gmpn_div_q@@Base+0xa64>
   35be0:	ldur	x0, [x29, #-16]
   35be4:	cbnz	x0, 35c3c <__gmpn_div_q@@Base+0xad0>
   35be8:	mov	sp, x29
   35bec:	ldp	x20, x19, [sp, #80]
   35bf0:	ldp	x22, x21, [sp, #64]
   35bf4:	ldp	x24, x23, [sp, #48]
   35bf8:	ldp	x26, x25, [sp, #32]
   35bfc:	ldp	x28, x27, [sp, #16]
   35c00:	ldp	x29, x30, [sp], #96
   35c04:	ret
   35c08:	ldur	x9, [x29, #-80]
   35c0c:	sub	x8, x25, #0x8
   35c10:	sub	x9, x9, #0x8
   35c14:	subs	x10, x21, #0x1
   35c18:	b.lt	35be0 <__gmpn_div_q@@Base+0xa74>  // b.tstop
   35c1c:	lsl	x11, x21, #3
   35c20:	ldr	x12, [x9, x11]
   35c24:	ldr	x11, [x8, x11]
   35c28:	mov	x21, x10
   35c2c:	cmp	x12, x11
   35c30:	b.eq	35c14 <__gmpn_div_q@@Base+0xaa8>  // b.none
   35c34:	b.ls	35bd0 <__gmpn_div_q@@Base+0xa64>  // b.plast
   35c38:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   35c3c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   35c40:	b	35be8 <__gmpn_div_q@@Base+0xa7c>
   35c44:	sub	x0, x29, #0x10
   35c48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35c4c:	mov	x25, x0
   35c50:	b	35ba0 <__gmpn_div_q@@Base+0xa34>
   35c54:	mov	x25, x2
   35c58:	mov	x0, x21
   35c5c:	mov	x1, x20
   35c60:	mov	w2, wzr
   35c64:	bl	cfd0 <__gmpn_mu_div_q_itch@plt>
   35c68:	lsl	x1, x0, #3
   35c6c:	mov	w8, #0x7f00                	// #32512
   35c70:	cmp	x1, x8
   35c74:	b.hi	35ccc <__gmpn_div_q@@Base+0xb60>  // b.pmore
   35c78:	add	x9, x1, #0xf
   35c7c:	mov	x8, sp
   35c80:	and	x9, x9, #0xfffffffffffffff0
   35c84:	sub	x5, x8, x9
   35c88:	mov	sp, x5
   35c8c:	mov	x0, x19
   35c90:	mov	x1, x25
   35c94:	mov	x2, x21
   35c98:	mov	x3, x24
   35c9c:	mov	x4, x20
   35ca0:	bl	c2e0 <__gmpn_mu_div_q@plt>
   35ca4:	str	x0, [x19, x22, lsl #3]
   35ca8:	b	35be0 <__gmpn_div_q@@Base+0xa74>
   35cac:	sub	x0, x29, #0x10
   35cb0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35cb4:	mov	x5, x0
   35cb8:	b	35758 <__gmpn_div_q@@Base+0x5ec>
   35cbc:	sub	x0, x29, #0x10
   35cc0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35cc4:	mov	x5, x0
   35cc8:	b	35934 <__gmpn_div_q@@Base+0x7c8>
   35ccc:	sub	x0, x29, #0x10
   35cd0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35cd4:	mov	x5, x0
   35cd8:	b	35c8c <__gmpn_div_q@@Base+0xb20>

0000000000035cdc <__gmpn_tdiv_qr@@Base>:
   35cdc:	stp	x29, x30, [sp, #-96]!
   35ce0:	stp	x28, x27, [sp, #16]
   35ce4:	stp	x26, x25, [sp, #32]
   35ce8:	stp	x24, x23, [sp, #48]
   35cec:	stp	x22, x21, [sp, #64]
   35cf0:	stp	x20, x19, [sp, #80]
   35cf4:	mov	x29, sp
   35cf8:	sub	sp, sp, #0x50
   35cfc:	cbnz	x2, 36860 <__gmpn_tdiv_qr@@Base+0xb84>
   35d00:	mov	x21, x6
   35d04:	mov	x22, x5
   35d08:	mov	x25, x4
   35d0c:	mov	x26, x3
   35d10:	mov	x28, x1
   35d14:	mov	x20, x0
   35d18:	subs	x19, x6, #0x1
   35d1c:	b.eq	35f20 <__gmpn_tdiv_qr@@Base+0x244>  // b.none
   35d20:	cmp	x21, #0x2
   35d24:	b.eq	35e64 <__gmpn_tdiv_qr@@Base+0x188>  // b.none
   35d28:	cbz	x21, 36878 <__gmpn_tdiv_qr@@Base+0xb9c>
   35d2c:	stur	xzr, [x29, #-8]
   35d30:	add	x8, x26, x25, lsl #3
   35d34:	ldur	x27, [x8, #-8]
   35d38:	ldr	x23, [x22, x19, lsl #3]
   35d3c:	sub	x9, x25, x21
   35d40:	str	xzr, [x20, x9, lsl #3]
   35d44:	cmp	x27, x23
   35d48:	cinc	x24, x25, cs  // cs = hs, nlast
   35d4c:	cset	w8, cs  // cs = hs, nlast
   35d50:	cmp	x24, x21, lsl #1
   35d54:	b.ge	35f40 <__gmpn_tdiv_qr@@Base+0x264>  // b.tcont
   35d58:	adds	x24, x9, x8
   35d5c:	b.eq	36034 <__gmpn_tdiv_qr@@Base+0x358>  // b.none
   35d60:	ldr	x8, [x22, x19, lsl #3]
   35d64:	sub	x19, x21, x24
   35d68:	lsl	x10, x24, #3
   35d6c:	stp	x22, x28, [x29, #-40]
   35d70:	stp	x10, x26, [x29, #-56]
   35d74:	tbnz	x8, #63, 361b4 <__gmpn_tdiv_qr@@Base+0x4d8>
   35d78:	mov	w9, #0x7f00                	// #32512
   35d7c:	clz	x8, x8
   35d80:	cmp	x10, x9
   35d84:	stp	x19, x8, [x29, #-80]
   35d88:	mov	x19, x26
   35d8c:	b.hi	367fc <__gmpn_tdiv_qr@@Base+0xb20>  // b.pmore
   35d90:	add	x9, x10, #0xf
   35d94:	mov	x8, sp
   35d98:	and	x9, x9, #0xfffffffffffffff0
   35d9c:	sub	x28, x8, x9
   35da0:	mov	sp, x28
   35da4:	ldur	x8, [x29, #-80]
   35da8:	mov	x0, x28
   35dac:	mov	x2, x24
   35db0:	add	x26, x22, x8, lsl #3
   35db4:	ldur	x22, [x29, #-72]
   35db8:	mov	x1, x26
   35dbc:	mov	w3, w22
   35dc0:	bl	c180 <__gmpn_lshift@plt>
   35dc4:	ldur	x8, [x26, #-8]
   35dc8:	ldr	x10, [x28]
   35dcc:	neg	x9, x22
   35dd0:	mov	w1, #0x8                   	// #8
   35dd4:	lsr	x8, x8, x9
   35dd8:	mov	w11, #0x7f00                	// #32512
   35ddc:	bfi	x1, x24, #4, #60
   35de0:	orr	x8, x10, x8
   35de4:	cmp	x1, x11
   35de8:	stur	x28, [x29, #-64]
   35dec:	str	x8, [x28]
   35df0:	lsl	x28, x24, #1
   35df4:	b.hi	36810 <__gmpn_tdiv_qr@@Base+0xb34>  // b.pmore
   35df8:	add	x9, x1, #0xf
   35dfc:	mov	x8, sp
   35e00:	and	x9, x9, #0xfffffffffffffff0
   35e04:	sub	x26, x8, x9
   35e08:	mov	sp, x26
   35e0c:	ldur	x22, [x29, #-72]
   35e10:	add	x8, x19, x25, lsl #3
   35e14:	sub	x1, x8, x28, lsl #3
   35e18:	mov	x0, x26
   35e1c:	mov	x2, x28
   35e20:	mov	w3, w22
   35e24:	bl	c180 <__gmpn_lshift@plt>
   35e28:	cmp	x27, x23
   35e2c:	b.cc	36224 <__gmpn_tdiv_qr@@Base+0x548>  // b.lo, b.ul, b.last
   35e30:	ldur	x19, [x29, #-80]
   35e34:	str	x0, [x26, x28, lsl #3]
   35e38:	add	x26, x26, #0x8
   35e3c:	ldur	x27, [x29, #-64]
   35e40:	cmp	x24, #0x2
   35e44:	b.ne	36258 <__gmpn_tdiv_qr@@Base+0x57c>  // b.any
   35e48:	mov	w3, #0x4                   	// #4
   35e4c:	mov	x0, x20
   35e50:	mov	x1, xzr
   35e54:	mov	x2, x26
   35e58:	mov	x4, x27
   35e5c:	bl	c200 <__gmpn_divrem_2@plt>
   35e60:	b	364dc <__gmpn_tdiv_qr@@Base+0x800>
   35e64:	stur	xzr, [x29, #-8]
   35e68:	ldr	x8, [x22, #8]
   35e6c:	tbnz	x8, #63, 35fd4 <__gmpn_tdiv_qr@@Base+0x2f8>
   35e70:	ldr	x9, [x22]
   35e74:	clz	x21, x8
   35e78:	lsl	x10, x25, #3
   35e7c:	neg	x12, x21
   35e80:	mov	w11, #0x7f00                	// #32512
   35e84:	lsl	x8, x8, x21
   35e88:	add	x1, x10, #0x8
   35e8c:	lsr	x10, x9, x12
   35e90:	mov	w19, #0x40                  	// #64
   35e94:	cmp	x1, x11
   35e98:	lsl	x9, x9, x21
   35e9c:	orr	x8, x10, x8
   35ea0:	stp	x9, x8, [x29, #-24]
   35ea4:	b.hi	36670 <__gmpn_tdiv_qr@@Base+0x994>  // b.pmore
   35ea8:	add	x9, x1, #0xf
   35eac:	mov	x8, sp
   35eb0:	and	x9, x9, #0xfffffffffffffff0
   35eb4:	sub	x22, x8, x9
   35eb8:	mov	sp, x22
   35ebc:	mov	x0, x22
   35ec0:	mov	x1, x26
   35ec4:	mov	x2, x25
   35ec8:	mov	w3, w21
   35ecc:	sub	x19, x19, x21
   35ed0:	bl	c180 <__gmpn_lshift@plt>
   35ed4:	cmp	x0, #0x0
   35ed8:	mov	x23, x0
   35edc:	str	x0, [x22, x25, lsl #3]
   35ee0:	cinc	x3, x25, ne  // ne = any
   35ee4:	sub	x4, x29, #0x18
   35ee8:	mov	x0, x20
   35eec:	mov	x1, xzr
   35ef0:	mov	x2, x22
   35ef4:	bl	c200 <__gmpn_divrem_2@plt>
   35ef8:	cbnz	x23, 35f04 <__gmpn_tdiv_qr@@Base+0x228>
   35efc:	add	x8, x20, x25, lsl #3
   35f00:	stur	x0, [x8, #-16]
   35f04:	ldp	x8, x9, [x22]
   35f08:	lsr	x8, x8, x21
   35f0c:	lsl	x10, x9, x19
   35f10:	lsr	x9, x9, x21
   35f14:	orr	x8, x10, x8
   35f18:	stp	x8, x9, [x28]
   35f1c:	b	367cc <__gmpn_tdiv_qr@@Base+0xaf0>
   35f20:	ldr	x4, [x22]
   35f24:	mov	x0, x20
   35f28:	mov	x1, xzr
   35f2c:	mov	x2, x26
   35f30:	mov	x3, x25
   35f34:	bl	cd00 <__gmpn_divrem_1@plt>
   35f38:	str	x0, [x28]
   35f3c:	b	367d4 <__gmpn_tdiv_qr@@Base+0xaf8>
   35f40:	ldr	x8, [x22, x19, lsl #3]
   35f44:	tbnz	x8, #63, 36048 <__gmpn_tdiv_qr@@Base+0x36c>
   35f48:	lsl	x1, x21, #3
   35f4c:	mov	w9, #0x7f00                	// #32512
   35f50:	mov	x23, x26
   35f54:	cmp	x1, x9
   35f58:	clz	x26, x8
   35f5c:	stur	x28, [x29, #-32]
   35f60:	b.hi	36690 <__gmpn_tdiv_qr@@Base+0x9b4>  // b.pmore
   35f64:	add	x9, x1, #0xf
   35f68:	mov	x8, sp
   35f6c:	and	x9, x9, #0xfffffffffffffff0
   35f70:	sub	x28, x8, x9
   35f74:	mov	sp, x28
   35f78:	mov	x0, x28
   35f7c:	mov	x1, x22
   35f80:	mov	x2, x21
   35f84:	mov	w3, w26
   35f88:	bl	c180 <__gmpn_lshift@plt>
   35f8c:	lsl	x8, x25, #3
   35f90:	add	x1, x8, #0x8
   35f94:	mov	w8, #0x7f00                	// #32512
   35f98:	cmp	x1, x8
   35f9c:	b.hi	366a0 <__gmpn_tdiv_qr@@Base+0x9c4>  // b.pmore
   35fa0:	add	x9, x1, #0xf
   35fa4:	mov	x8, sp
   35fa8:	and	x9, x9, #0xfffffffffffffff0
   35fac:	sub	x27, x8, x9
   35fb0:	mov	sp, x27
   35fb4:	mov	x0, x27
   35fb8:	mov	x1, x23
   35fbc:	mov	x2, x25
   35fc0:	mov	w3, w26
   35fc4:	bl	c180 <__gmpn_lshift@plt>
   35fc8:	mov	x22, x28
   35fcc:	ldur	x28, [x29, #-32]
   35fd0:	b	36088 <__gmpn_tdiv_qr@@Base+0x3ac>
   35fd4:	lsl	x1, x25, #3
   35fd8:	mov	w8, #0x7f00                	// #32512
   35fdc:	cmp	x1, x8
   35fe0:	b.hi	36680 <__gmpn_tdiv_qr@@Base+0x9a4>  // b.pmore
   35fe4:	add	x9, x1, #0xf
   35fe8:	mov	x8, sp
   35fec:	and	x9, x9, #0xfffffffffffffff0
   35ff0:	sub	x21, x8, x9
   35ff4:	mov	sp, x21
   35ff8:	mov	x0, x21
   35ffc:	mov	x1, x26
   36000:	mov	x2, x25
   36004:	bl	ca50 <__gmpn_copyi@plt>
   36008:	mov	x0, x20
   3600c:	mov	x1, xzr
   36010:	mov	x2, x21
   36014:	mov	x3, x25
   36018:	mov	x4, x22
   3601c:	bl	c200 <__gmpn_divrem_2@plt>
   36020:	add	x8, x20, x25, lsl #3
   36024:	stur	x0, [x8, #-16]
   36028:	ldr	q0, [x21]
   3602c:	str	q0, [x28]
   36030:	b	367cc <__gmpn_tdiv_qr@@Base+0xaf0>
   36034:	mov	x0, x28
   36038:	mov	x1, x26
   3603c:	mov	x2, x21
   36040:	bl	ca50 <__gmpn_copyi@plt>
   36044:	b	367d4 <__gmpn_tdiv_qr@@Base+0xaf8>
   36048:	lsl	x8, x25, #3
   3604c:	add	x1, x8, #0x8
   36050:	mov	w8, #0x7f00                	// #32512
   36054:	cmp	x1, x8
   36058:	b.hi	36820 <__gmpn_tdiv_qr@@Base+0xb44>  // b.pmore
   3605c:	add	x9, x1, #0xf
   36060:	mov	x8, sp
   36064:	and	x9, x9, #0xfffffffffffffff0
   36068:	sub	x27, x8, x9
   3606c:	mov	sp, x27
   36070:	mov	x0, x27
   36074:	mov	x1, x26
   36078:	mov	x2, x25
   3607c:	bl	ca50 <__gmpn_copyi@plt>
   36080:	mov	x0, xzr
   36084:	mov	w26, wzr
   36088:	str	x0, [x27, x25, lsl #3]
   3608c:	ldr	x23, [x22, x19, lsl #3]
   36090:	mov	x0, x23
   36094:	bl	d3f0 <__gmpn_invert_limb@plt>
   36098:	add	x8, x22, x21, lsl #3
   3609c:	ldur	x8, [x8, #-16]
   360a0:	mul	x9, x0, x23
   360a4:	adds	x9, x9, x8
   360a8:	b.cc	360c4 <__gmpn_tdiv_qr@@Base+0x3e8>  // b.lo, b.ul, b.last
   360ac:	subs	x9, x9, x23
   360b0:	cset	w10, cs  // cs = hs, nlast
   360b4:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   360b8:	mvn	x10, x10
   360bc:	add	x0, x10, x0
   360c0:	sub	x9, x9, x11
   360c4:	umulh	x10, x8, x0
   360c8:	adds	x9, x10, x9
   360cc:	b.cc	360f4 <__gmpn_tdiv_qr@@Base+0x418>  // b.lo, b.ul, b.last
   360d0:	cmp	x9, x23
   360d4:	sub	x5, x0, #0x1
   360d8:	b.cc	360f8 <__gmpn_tdiv_qr@@Base+0x41c>  // b.lo, b.ul, b.last
   360dc:	mul	x10, x0, x8
   360e0:	cmp	x9, x23
   360e4:	sub	x11, x0, #0x2
   360e8:	ccmp	x10, x8, #0x2, ls  // ls = plast
   360ec:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   360f0:	b	360f8 <__gmpn_tdiv_qr@@Base+0x41c>
   360f4:	mov	x5, x0
   360f8:	cmp	x21, #0x29
   360fc:	stur	x5, [x29, #-24]
   36100:	b.le	36180 <__gmpn_tdiv_qr@@Base+0x4a4>
   36104:	cmp	x21, #0x62
   36108:	b.lt	3614c <__gmpn_tdiv_qr@@Base+0x470>  // b.tstop
   3610c:	cmp	x24, #0x7cc
   36110:	b.lt	3614c <__gmpn_tdiv_qr@@Base+0x470>  // b.tstop
   36114:	mov	x8, #0x200000000000        	// #35184372088832
   36118:	mov	x9, #0x800000000000        	// #140737488355328
   3611c:	movk	x8, #0x409c, lsl #48
   36120:	movk	x9, #0x4058, lsl #48
   36124:	scvtf	d0, x21
   36128:	scvtf	d1, x24
   3612c:	fmov	d2, x8
   36130:	fmov	d3, x9
   36134:	fmul	d2, d0, d2
   36138:	fmul	d3, d1, d3
   3613c:	fadd	d2, d2, d3
   36140:	fmul	d0, d0, d1
   36144:	fcmp	d2, d0
   36148:	b.le	36314 <__gmpn_tdiv_qr@@Base+0x638>
   3614c:	sub	x5, x29, #0x18
   36150:	mov	x0, x20
   36154:	mov	x1, x27
   36158:	mov	x2, x24
   3615c:	mov	x3, x22
   36160:	mov	x4, x21
   36164:	bl	c3b0 <__gmpn_dcpi1_div_qr@plt>
   36168:	mov	x0, x28
   3616c:	mov	x1, x27
   36170:	mov	x2, x21
   36174:	cbnz	w26, 361a8 <__gmpn_tdiv_qr@@Base+0x4cc>
   36178:	bl	ca50 <__gmpn_copyi@plt>
   3617c:	b	367cc <__gmpn_tdiv_qr@@Base+0xaf0>
   36180:	mov	x0, x20
   36184:	mov	x1, x27
   36188:	mov	x2, x24
   3618c:	mov	x3, x22
   36190:	mov	x4, x21
   36194:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   36198:	mov	x0, x28
   3619c:	mov	x1, x27
   361a0:	mov	x2, x21
   361a4:	cbz	w26, 36178 <__gmpn_tdiv_qr@@Base+0x49c>
   361a8:	mov	w3, w26
   361ac:	bl	c1a0 <__gmpn_rshift@plt>
   361b0:	b	367cc <__gmpn_tdiv_qr@@Base+0xaf0>
   361b4:	add	x8, x22, x19, lsl #3
   361b8:	mov	w1, #0x8                   	// #8
   361bc:	stur	x8, [x29, #-64]
   361c0:	bfi	x1, x24, #4, #60
   361c4:	mov	w8, #0x7f00                	// #32512
   361c8:	cmp	x1, x8
   361cc:	lsl	x28, x24, #1
   361d0:	b.hi	36830 <__gmpn_tdiv_qr@@Base+0xb54>  // b.pmore
   361d4:	add	x9, x1, #0xf
   361d8:	mov	x8, sp
   361dc:	and	x9, x9, #0xfffffffffffffff0
   361e0:	sub	x26, x8, x9
   361e4:	mov	sp, x26
   361e8:	ldur	x8, [x29, #-48]
   361ec:	mov	x0, x26
   361f0:	mov	x2, x28
   361f4:	add	x8, x8, x25, lsl #3
   361f8:	sub	x1, x8, x28, lsl #3
   361fc:	bl	ca50 <__gmpn_copyi@plt>
   36200:	cmp	x27, x23
   36204:	b.cc	3637c <__gmpn_tdiv_qr@@Base+0x6a0>  // b.lo, b.ul, b.last
   36208:	mov	w22, wzr
   3620c:	str	xzr, [x26, x28, lsl #3]
   36210:	add	x26, x26, #0x8
   36214:	ldur	x27, [x29, #-64]
   36218:	cmp	x24, #0x2
   3621c:	b.ne	36258 <__gmpn_tdiv_qr@@Base+0x57c>  // b.any
   36220:	b	35e48 <__gmpn_tdiv_qr@@Base+0x16c>
   36224:	mvn	x8, x28
   36228:	add	x8, x8, x25
   3622c:	ldr	x8, [x19, x8, lsl #3]
   36230:	ldr	x9, [x26]
   36234:	mov	w10, #0x40                  	// #64
   36238:	sub	x10, x10, x22
   3623c:	ldur	x19, [x29, #-80]
   36240:	lsr	x8, x8, x10
   36244:	orr	x8, x9, x8
   36248:	str	x8, [x26]
   3624c:	ldur	x27, [x29, #-64]
   36250:	cmp	x24, #0x2
   36254:	b.eq	35e48 <__gmpn_tdiv_qr@@Base+0x16c>  // b.none
   36258:	cmp	x24, #0x1
   3625c:	b.ne	362ac <__gmpn_tdiv_qr@@Base+0x5d0>  // b.any
   36260:	ldr	x9, [x27]
   36264:	ldp	x8, x10, [x26]
   36268:	lsr	x12, x9, #32
   3626c:	udiv	x15, x10, x12
   36270:	and	x11, x9, #0xffffffff
   36274:	msub	w10, w15, w12, w10
   36278:	mul	x13, x15, x11
   3627c:	extr	x14, x10, x8, #32
   36280:	cmp	x14, x13
   36284:	b.cs	36390 <__gmpn_tdiv_qr@@Base+0x6b4>  // b.hs, b.nlast
   36288:	add	x14, x14, x9
   3628c:	cmp	x14, x9
   36290:	sub	x10, x15, #0x1
   36294:	b.cc	36394 <__gmpn_tdiv_qr@@Base+0x6b8>  // b.lo, b.ul, b.last
   36298:	cmp	x14, x13
   3629c:	b.cs	36394 <__gmpn_tdiv_qr@@Base+0x6b8>  // b.hs, b.nlast
   362a0:	sub	x10, x15, #0x2
   362a4:	add	x14, x14, x9
   362a8:	b	36394 <__gmpn_tdiv_qr@@Base+0x6b8>
   362ac:	add	x23, x27, x24, lsl #3
   362b0:	ldur	x28, [x23, #-8]
   362b4:	mov	x0, x28
   362b8:	bl	d3f0 <__gmpn_invert_limb@plt>
   362bc:	ldur	x8, [x23, #-16]
   362c0:	mul	x9, x0, x28
   362c4:	adds	x9, x9, x8
   362c8:	b.cc	362e4 <__gmpn_tdiv_qr@@Base+0x608>  // b.lo, b.ul, b.last
   362cc:	subs	x9, x9, x28
   362d0:	cset	w10, cs  // cs = hs, nlast
   362d4:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   362d8:	mvn	x10, x10
   362dc:	add	x0, x10, x0
   362e0:	sub	x9, x9, x11
   362e4:	umulh	x10, x8, x0
   362e8:	adds	x9, x10, x9
   362ec:	b.cc	3640c <__gmpn_tdiv_qr@@Base+0x730>  // b.lo, b.ul, b.last
   362f0:	cmp	x9, x28
   362f4:	sub	x5, x0, #0x1
   362f8:	b.cc	36410 <__gmpn_tdiv_qr@@Base+0x734>  // b.lo, b.ul, b.last
   362fc:	mul	x10, x0, x8
   36300:	cmp	x9, x28
   36304:	sub	x11, x0, #0x2
   36308:	ccmp	x10, x8, #0x2, ls  // ls = plast
   3630c:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   36310:	b	36410 <__gmpn_tdiv_qr@@Base+0x734>
   36314:	mov	x0, x24
   36318:	mov	x1, x21
   3631c:	mov	w2, wzr
   36320:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   36324:	lsl	x1, x0, #3
   36328:	mov	w8, #0x7f00                	// #32512
   3632c:	cmp	x1, x8
   36330:	b.hi	36840 <__gmpn_tdiv_qr@@Base+0xb64>  // b.pmore
   36334:	add	x9, x1, #0xf
   36338:	mov	x8, sp
   3633c:	and	x9, x9, #0xfffffffffffffff0
   36340:	sub	x6, x8, x9
   36344:	mov	sp, x6
   36348:	mov	x0, x20
   3634c:	mov	x1, x28
   36350:	mov	x2, x27
   36354:	mov	x3, x24
   36358:	mov	x4, x22
   3635c:	mov	x5, x21
   36360:	bl	c960 <__gmpn_mu_div_qr@plt>
   36364:	mov	x27, x28
   36368:	mov	x0, x28
   3636c:	mov	x1, x27
   36370:	mov	x2, x21
   36374:	cbnz	w26, 361a8 <__gmpn_tdiv_qr@@Base+0x4cc>
   36378:	b	36178 <__gmpn_tdiv_qr@@Base+0x49c>
   3637c:	mov	w22, wzr
   36380:	ldur	x27, [x29, #-64]
   36384:	cmp	x24, #0x2
   36388:	b.ne	36258 <__gmpn_tdiv_qr@@Base+0x57c>  // b.any
   3638c:	b	35e48 <__gmpn_tdiv_qr@@Base+0x16c>
   36390:	mov	x10, x15
   36394:	sub	x14, x14, x13
   36398:	udiv	x13, x14, x12
   3639c:	msub	w12, w13, w12, w14
   363a0:	mul	x11, x13, x11
   363a4:	bfi	x8, x12, #32, #32
   363a8:	cmp	x8, x11
   363ac:	b.cs	363d8 <__gmpn_tdiv_qr@@Base+0x6fc>  // b.hs, b.nlast
   363b0:	ldur	x14, [x29, #-56]
   363b4:	add	x8, x8, x9
   363b8:	cmp	x8, x9
   363bc:	sub	x12, x13, #0x1
   363c0:	b.cc	363e0 <__gmpn_tdiv_qr@@Base+0x704>  // b.lo, b.ul, b.last
   363c4:	cmp	x8, x11
   363c8:	b.cs	363e0 <__gmpn_tdiv_qr@@Base+0x704>  // b.hs, b.nlast
   363cc:	sub	x12, x13, #0x2
   363d0:	add	x8, x8, x9
   363d4:	b	363e0 <__gmpn_tdiv_qr@@Base+0x704>
   363d8:	ldur	x14, [x29, #-56]
   363dc:	mov	x12, x13
   363e0:	sub	x8, x8, x11
   363e4:	orr	x9, x12, x10, lsl #32
   363e8:	str	x8, [x26]
   363ec:	str	x9, [x20]
   363f0:	cmp	x19, #0x2
   363f4:	b.lt	364e8 <__gmpn_tdiv_qr@@Base+0x80c>  // b.tstop
   363f8:	ldur	x10, [x29, #-40]
   363fc:	add	x8, x10, x19, lsl #3
   36400:	ldur	x8, [x8, #-16]
   36404:	lsr	x8, x8, #1
   36408:	b	364f0 <__gmpn_tdiv_qr@@Base+0x814>
   3640c:	mov	x5, x0
   36410:	cmp	x24, #0x29
   36414:	stur	x5, [x29, #-24]
   36418:	b.le	364a4 <__gmpn_tdiv_qr@@Base+0x7c8>
   3641c:	cmp	x24, #0x3e5
   36420:	lsl	x28, x24, #1
   36424:	b.le	364c0 <__gmpn_tdiv_qr@@Base+0x7e4>
   36428:	mov	x0, x28
   3642c:	mov	x1, x24
   36430:	mov	w2, wzr
   36434:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   36438:	lsl	x1, x0, #3
   3643c:	mov	w8, #0x7f00                	// #32512
   36440:	cmp	x1, x8
   36444:	b.hi	36850 <__gmpn_tdiv_qr@@Base+0xb74>  // b.pmore
   36448:	add	x9, x1, #0xf
   3644c:	mov	x8, sp
   36450:	and	x9, x9, #0xfffffffffffffff0
   36454:	sub	x6, x8, x9
   36458:	mov	sp, x6
   3645c:	ldur	x9, [x29, #-32]
   36460:	ldur	x10, [x29, #-48]
   36464:	sub	x8, x25, x24
   36468:	mov	x0, x20
   3646c:	add	x8, x9, x8, lsl #3
   36470:	cmp	x10, x9
   36474:	csel	x25, x8, x9, eq  // eq = none
   36478:	mov	x1, x25
   3647c:	mov	x2, x26
   36480:	mov	x3, x28
   36484:	mov	x4, x27
   36488:	mov	x5, x24
   3648c:	bl	c960 <__gmpn_mu_div_qr@plt>
   36490:	mov	x0, x26
   36494:	mov	x1, x25
   36498:	mov	x2, x24
   3649c:	bl	ca50 <__gmpn_copyi@plt>
   364a0:	b	364dc <__gmpn_tdiv_qr@@Base+0x800>
   364a4:	lsl	x2, x24, #1
   364a8:	mov	x0, x20
   364ac:	mov	x1, x26
   364b0:	mov	x3, x27
   364b4:	mov	x4, x24
   364b8:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   364bc:	b	364dc <__gmpn_tdiv_qr@@Base+0x800>
   364c0:	sub	x5, x29, #0x18
   364c4:	mov	x0, x20
   364c8:	mov	x1, x26
   364cc:	mov	x2, x28
   364d0:	mov	x3, x27
   364d4:	mov	x4, x24
   364d8:	bl	c3b0 <__gmpn_dcpi1_div_qr@plt>
   364dc:	ldur	x14, [x29, #-56]
   364e0:	cmp	x19, #0x2
   364e4:	b.ge	363f8 <__gmpn_tdiv_qr@@Base+0x71c>  // b.tcont
   364e8:	ldur	x10, [x29, #-40]
   364ec:	mov	x8, xzr
   364f0:	sub	x25, x19, #0x1
   364f4:	ldr	x10, [x10, x25, lsl #3]
   364f8:	sub	x11, x14, #0x8
   364fc:	ldr	x12, [x20, x11]
   36500:	ldr	x11, [x26, x11]
   36504:	mvn	w9, w22
   36508:	lsl	x10, x10, x22
   3650c:	lsr	x8, x8, x9
   36510:	orr	x8, x10, x8
   36514:	umulh	x8, x8, x12
   36518:	cmp	x11, x8
   3651c:	mov	x28, x24
   36520:	b.cs	3655c <__gmpn_tdiv_qr@@Base+0x880>  // b.hs, b.nlast
   36524:	mov	x8, x20
   36528:	ldr	x9, [x8]
   3652c:	sub	x10, x9, #0x1
   36530:	str	x10, [x8], #8
   36534:	cbz	x9, 36528 <__gmpn_tdiv_qr@@Base+0x84c>
   36538:	mov	x0, x26
   3653c:	mov	x1, x26
   36540:	mov	x2, x27
   36544:	mov	x3, x24
   36548:	bl	ca70 <__gmpn_add_n@plt>
   3654c:	mov	x28, x24
   36550:	cbz	x0, 3655c <__gmpn_tdiv_qr@@Base+0x880>
   36554:	add	x28, x24, #0x1
   36558:	str	x0, [x26, x24, lsl #3]
   3655c:	cbz	w22, 365d4 <__gmpn_tdiv_qr@@Base+0x8f8>
   36560:	mov	w8, #0x40                  	// #64
   36564:	sub	w3, w8, w22
   36568:	mov	x0, x26
   3656c:	mov	x1, x26
   36570:	mov	x2, x28
   36574:	bl	c180 <__gmpn_lshift@plt>
   36578:	ldur	x9, [x29, #-48]
   3657c:	lsl	x8, x25, #3
   36580:	ldr	x10, [x26]
   36584:	mov	x11, #0xffffffffffffffff    	// #-1
   36588:	ldr	x9, [x9, x8]
   3658c:	lsr	x11, x11, x22
   36590:	ldur	x22, [x29, #-40]
   36594:	mov	x27, x0
   36598:	and	x9, x9, x11
   3659c:	orr	x9, x10, x9
   365a0:	str	x9, [x26]
   365a4:	ldr	x8, [x22, x8]
   365a8:	mov	x0, x26
   365ac:	mov	x1, x20
   365b0:	mov	x2, x24
   365b4:	and	x3, x8, x11
   365b8:	bl	c9e0 <__gmpn_submul_1@plt>
   365bc:	cmp	x24, x28
   365c0:	b.ne	365e4 <__gmpn_tdiv_qr@@Base+0x908>  // b.any
   365c4:	subs	x8, x27, x0
   365c8:	cset	w23, cc  // cc = lo, ul, last
   365cc:	add	x28, x24, #0x1
   365d0:	b	365f4 <__gmpn_tdiv_qr@@Base+0x918>
   365d4:	ldur	x22, [x29, #-40]
   365d8:	mov	x23, xzr
   365dc:	mov	x25, x19
   365e0:	b	365f8 <__gmpn_tdiv_qr@@Base+0x91c>
   365e4:	ldr	x8, [x26, x24, lsl #3]
   365e8:	subs	x8, x8, x0
   365ec:	b.cc	3687c <__gmpn_tdiv_qr@@Base+0xba0>  // b.lo, b.ul, b.last
   365f0:	mov	x23, xzr
   365f4:	str	x8, [x26, x24, lsl #3]
   365f8:	lsl	x1, x21, #3
   365fc:	mov	w8, #0x7f00                	// #32512
   36600:	cmp	x1, x8
   36604:	b.hi	366b0 <__gmpn_tdiv_qr@@Base+0x9d4>  // b.pmore
   36608:	add	x9, x1, #0xf
   3660c:	mov	x8, sp
   36610:	and	x9, x9, #0xfffffffffffffff0
   36614:	sub	x27, x8, x9
   36618:	mov	sp, x27
   3661c:	cmp	x25, x24
   36620:	b.ge	366c4 <__gmpn_tdiv_qr@@Base+0x9e8>  // b.tcont
   36624:	cbz	x25, 36640 <__gmpn_tdiv_qr@@Base+0x964>
   36628:	mov	x0, x27
   3662c:	mov	x1, x20
   36630:	mov	x2, x24
   36634:	mov	x3, x22
   36638:	mov	x4, x25
   3663c:	b	366d8 <__gmpn_tdiv_qr@@Base+0x9fc>
   36640:	ldur	x0, [x29, #-32]
   36644:	mov	x1, x26
   36648:	mov	x2, x28
   3664c:	bl	ca50 <__gmpn_copyi@plt>
   36650:	cmp	x28, x21
   36654:	b.eq	367a4 <__gmpn_tdiv_qr@@Base+0xac8>  // b.none
   36658:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   3665c:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   36660:	add	x0, x0, #0x5fc
   36664:	add	x2, x2, #0x61e
   36668:	mov	w1, #0x169                 	// #361
   3666c:	bl	c6c0 <__gmp_assert_fail@plt>
   36670:	sub	x0, x29, #0x8
   36674:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36678:	mov	x22, x0
   3667c:	b	35ebc <__gmpn_tdiv_qr@@Base+0x1e0>
   36680:	sub	x0, x29, #0x8
   36684:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36688:	mov	x21, x0
   3668c:	b	35ff8 <__gmpn_tdiv_qr@@Base+0x31c>
   36690:	sub	x0, x29, #0x8
   36694:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36698:	mov	x28, x0
   3669c:	b	35f78 <__gmpn_tdiv_qr@@Base+0x29c>
   366a0:	sub	x0, x29, #0x8
   366a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   366a8:	mov	x27, x0
   366ac:	b	35fb4 <__gmpn_tdiv_qr@@Base+0x2d8>
   366b0:	sub	x0, x29, #0x8
   366b4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   366b8:	mov	x27, x0
   366bc:	cmp	x25, x24
   366c0:	b.lt	36624 <__gmpn_tdiv_qr@@Base+0x948>  // b.tstop
   366c4:	mov	x0, x27
   366c8:	mov	x1, x22
   366cc:	mov	x2, x25
   366d0:	mov	x3, x20
   366d4:	mov	x4, x24
   366d8:	bl	ccd0 <__gmpn_mul@plt>
   366dc:	add	x2, x27, x25, lsl #3
   366e0:	mov	x0, x26
   366e4:	mov	x1, x26
   366e8:	mov	x3, x24
   366ec:	bl	c2d0 <__gmpn_sub_n@plt>
   366f0:	cbz	x0, 36718 <__gmpn_tdiv_qr@@Base+0xa3c>
   366f4:	mov	w19, #0x1                   	// #1
   366f8:	cmp	x24, x28
   366fc:	b.ge	36724 <__gmpn_tdiv_qr@@Base+0xa48>  // b.tcont
   36700:	lsl	x8, x24, #3
   36704:	ldr	x9, [x26, x8]
   36708:	add	x24, x24, #0x1
   3670c:	sub	x10, x9, #0x1
   36710:	str	x10, [x26, x8]
   36714:	cbz	x9, 366f8 <__gmpn_tdiv_qr@@Base+0xa1c>
   36718:	mov	x22, x23
   3671c:	mov	x19, xzr
   36720:	b	36728 <__gmpn_tdiv_qr@@Base+0xa4c>
   36724:	mov	x22, x23
   36728:	ldur	x23, [x29, #-32]
   3672c:	sub	x2, x21, x25
   36730:	mov	x1, x26
   36734:	add	x24, x23, x25, lsl #3
   36738:	mov	x0, x24
   3673c:	bl	ca50 <__gmpn_copyi@plt>
   36740:	ldur	x1, [x29, #-48]
   36744:	mov	x0, x23
   36748:	mov	x2, x27
   3674c:	mov	x3, x25
   36750:	orr	x19, x19, x22
   36754:	bl	c2d0 <__gmpn_sub_n@plt>
   36758:	ldr	x8, [x24]
   3675c:	subs	x8, x8, x0
   36760:	str	x8, [x24]
   36764:	b.cs	36798 <__gmpn_tdiv_qr@@Base+0xabc>  // b.hs, b.nlast
   36768:	ldur	x22, [x29, #-40]
   3676c:	mov	w8, #0x1                   	// #1
   36770:	mov	w9, #0x1                   	// #1
   36774:	cmp	x9, x28
   36778:	b.ge	367a0 <__gmpn_tdiv_qr@@Base+0xac4>  // b.tcont
   3677c:	lsl	x10, x9, #3
   36780:	ldr	x11, [x24, x10]
   36784:	add	x9, x9, #0x1
   36788:	sub	x12, x11, #0x1
   3678c:	str	x12, [x24, x10]
   36790:	cbz	x11, 36774 <__gmpn_tdiv_qr@@Base+0xa98>
   36794:	b	3679c <__gmpn_tdiv_qr@@Base+0xac0>
   36798:	ldur	x22, [x29, #-40]
   3679c:	mov	x8, xzr
   367a0:	orr	x23, x19, x8
   367a4:	cbz	x23, 367cc <__gmpn_tdiv_qr@@Base+0xaf0>
   367a8:	ldr	x8, [x20]
   367ac:	sub	x9, x8, #0x1
   367b0:	str	x9, [x20], #8
   367b4:	cbz	x8, 367a8 <__gmpn_tdiv_qr@@Base+0xacc>
   367b8:	ldur	x0, [x29, #-32]
   367bc:	mov	x2, x22
   367c0:	mov	x3, x21
   367c4:	mov	x1, x0
   367c8:	bl	ca70 <__gmpn_add_n@plt>
   367cc:	ldur	x0, [x29, #-8]
   367d0:	cbnz	x0, 367f4 <__gmpn_tdiv_qr@@Base+0xb18>
   367d4:	mov	sp, x29
   367d8:	ldp	x20, x19, [sp, #80]
   367dc:	ldp	x22, x21, [sp, #64]
   367e0:	ldp	x24, x23, [sp, #48]
   367e4:	ldp	x26, x25, [sp, #32]
   367e8:	ldp	x28, x27, [sp, #16]
   367ec:	ldp	x29, x30, [sp], #96
   367f0:	ret
   367f4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   367f8:	b	367d4 <__gmpn_tdiv_qr@@Base+0xaf8>
   367fc:	sub	x0, x29, #0x8
   36800:	mov	x1, x10
   36804:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36808:	mov	x28, x0
   3680c:	b	35da4 <__gmpn_tdiv_qr@@Base+0xc8>
   36810:	sub	x0, x29, #0x8
   36814:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36818:	mov	x26, x0
   3681c:	b	35e0c <__gmpn_tdiv_qr@@Base+0x130>
   36820:	sub	x0, x29, #0x8
   36824:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36828:	mov	x27, x0
   3682c:	b	36070 <__gmpn_tdiv_qr@@Base+0x394>
   36830:	sub	x0, x29, #0x8
   36834:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36838:	mov	x26, x0
   3683c:	b	361e8 <__gmpn_tdiv_qr@@Base+0x50c>
   36840:	sub	x0, x29, #0x8
   36844:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36848:	mov	x6, x0
   3684c:	b	36348 <__gmpn_tdiv_qr@@Base+0x66c>
   36850:	sub	x0, x29, #0x8
   36854:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36858:	mov	x6, x0
   3685c:	b	3645c <__gmpn_tdiv_qr@@Base+0x780>
   36860:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   36864:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   36868:	add	x0, x0, #0x5fc
   3686c:	add	x2, x2, #0x606
   36870:	mov	w1, #0x32                  	// #50
   36874:	bl	c6c0 <__gmp_assert_fail@plt>
   36878:	bl	bfd0 <__gmp_divide_by_zero@plt>
   3687c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   36880:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   36884:	add	x0, x0, #0x5fc
   36888:	add	x2, x2, #0x60f
   3688c:	mov	w1, #0x154                 	// #340
   36890:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000036894 <__gmpn_jacobi_base@@Base>:
   36894:	cbz	x0, 36910 <__gmpn_jacobi_base@@Base+0x7c>
   36898:	lsr	x8, x1, #1
   3689c:	rbit	x9, x0
   368a0:	clz	x9, x9
   368a4:	eor	w10, w8, w1, lsr #2
   368a8:	and	w10, w10, w9
   368ac:	lsr	x11, x0, x9
   368b0:	eor	w9, w10, w2, asr #1
   368b4:	lsr	x10, x11, #1
   368b8:	subs	x11, x10, x8
   368bc:	b.eq	36914 <__gmpn_jacobi_base@@Base+0x80>  // b.none
   368c0:	asr	x12, x11, #63
   368c4:	and	w10, w10, w8
   368c8:	and	w10, w10, w12
   368cc:	eor	w9, w9, w10
   368d0:	and	x10, x12, x11
   368d4:	add	x8, x10, x8
   368d8:	rbit	x10, x11
   368dc:	eor	x11, x12, x11
   368e0:	clz	x10, x10
   368e4:	sub	x11, x11, x12
   368e8:	lsr	x12, x8, #1
   368ec:	add	w10, w10, #0x1
   368f0:	eor	w12, w12, w8
   368f4:	and	w12, w10, w12
   368f8:	eor	w9, w9, w12
   368fc:	lsr	x10, x11, x10
   36900:	cbnz	x8, 368b8 <__gmpn_jacobi_base@@Base+0x24>
   36904:	ubfiz	w8, w9, #1, #1
   36908:	mov	w9, #0x1                   	// #1
   3690c:	sub	w0, w9, w8
   36910:	ret
   36914:	mov	w0, wzr
   36918:	ret

000000000003691c <__gmpn_jacobi_2@@Base>:
   3691c:	mov	x8, x0
   36920:	ldr	x10, [x8, #8]
   36924:	ldp	x9, x8, [x1]
   36928:	ldr	x0, [x0]
   3692c:	lsl	w2, w2, #1
   36930:	cmp	x9, #0x1
   36934:	b.ne	3693c <__gmpn_jacobi_2@@Base+0x20>  // b.any
   36938:	cbz	x8, 36ae4 <__gmpn_jacobi_2@@Base+0x1c8>
   3693c:	cbz	x0, 36a24 <__gmpn_jacobi_2@@Base+0x108>
   36940:	tbnz	w0, #0, 3696c <__gmpn_jacobi_2@@Base+0x50>
   36944:	rbit	x11, x0
   36948:	clz	x11, x11
   3694c:	eor	w12, w9, w9, lsr #1
   36950:	neg	x13, x11
   36954:	lsr	x14, x0, x11
   36958:	and	w12, w12, w11, lsl #1
   3695c:	lsl	x13, x10, x13
   36960:	lsr	x10, x10, x11
   36964:	orr	x0, x13, x14
   36968:	eor	w2, w2, w12
   3696c:	cbz	x10, 36a64 <__gmpn_jacobi_2@@Base+0x148>
   36970:	cbz	x8, 36b70 <__gmpn_jacobi_2@@Base+0x254>
   36974:	cmp	x10, x8
   36978:	b.ls	369b8 <__gmpn_jacobi_2@@Base+0x9c>  // b.plast
   3697c:	eor	x11, x9, x9, lsr #1
   36980:	subs	x13, x0, x9
   36984:	sbc	x12, x10, x8
   36988:	cbz	x13, 36a80 <__gmpn_jacobi_2@@Base+0x164>
   3698c:	rbit	x10, x13
   36990:	clz	x10, x10
   36994:	neg	x15, x10
   36998:	and	w14, w11, w10, lsl #1
   3699c:	lsr	x13, x13, x10
   369a0:	lsr	x10, x12, x10
   369a4:	lsl	x12, x12, x15
   369a8:	eor	w2, w2, w14
   369ac:	cmp	x10, x8
   369b0:	orr	x0, x12, x13
   369b4:	b.hi	36980 <__gmpn_jacobi_2@@Base+0x64>  // b.pmore
   369b8:	cmp	x10, x8
   369bc:	b.eq	36b88 <__gmpn_jacobi_2@@Base+0x26c>  // b.none
   369c0:	and	w11, w0, w9
   369c4:	eor	w2, w2, w11
   369c8:	cbz	x10, 36a70 <__gmpn_jacobi_2@@Base+0x154>
   369cc:	cmp	x8, x10
   369d0:	b.ls	36a10 <__gmpn_jacobi_2@@Base+0xf4>  // b.plast
   369d4:	eor	x11, x0, x0, lsr #1
   369d8:	subs	x12, x9, x0
   369dc:	sbc	x9, x8, x10
   369e0:	cbz	x12, 36ab0 <__gmpn_jacobi_2@@Base+0x194>
   369e4:	rbit	x8, x12
   369e8:	clz	x8, x8
   369ec:	neg	x14, x8
   369f0:	and	w13, w11, w8, lsl #1
   369f4:	lsr	x12, x12, x8
   369f8:	lsr	x8, x9, x8
   369fc:	lsl	x9, x9, x14
   36a00:	eor	w2, w2, w13
   36a04:	cmp	x8, x10
   36a08:	orr	x9, x9, x12
   36a0c:	b.hi	369d8 <__gmpn_jacobi_2@@Base+0xbc>  // b.pmore
   36a10:	and	w11, w9, w0
   36a14:	cmp	x10, x8
   36a18:	eor	w2, w2, w11
   36a1c:	b.ne	36970 <__gmpn_jacobi_2@@Base+0x54>  // b.any
   36a20:	b	36b8c <__gmpn_jacobi_2@@Base+0x270>
   36a24:	cbz	x10, 36be4 <__gmpn_jacobi_2@@Base+0x2c8>
   36a28:	rbit	x11, x10
   36a2c:	clz	x11, x11
   36a30:	lsr	x12, x9, #1
   36a34:	lsl	w13, w11, #1
   36a38:	eor	w12, w12, w9
   36a3c:	lsr	x1, x10, x11
   36a40:	orr	w10, w13, #0x80
   36a44:	and	w10, w10, w12
   36a48:	cmp	x1, #0x1
   36a4c:	eor	w10, w2, w10
   36a50:	b.ne	36af4 <__gmpn_jacobi_2@@Base+0x1d8>  // b.any
   36a54:	and	w8, w10, #0x2
   36a58:	mov	w9, #0x1                   	// #1
   36a5c:	sub	w0, w9, w8
   36a60:	ret
   36a64:	cbz	x8, 36b68 <__gmpn_jacobi_2@@Base+0x24c>
   36a68:	and	w10, w0, w9
   36a6c:	eor	w2, w2, w10
   36a70:	mov	x1, x0
   36a74:	cmp	x1, #0x1
   36a78:	b.eq	36ae4 <__gmpn_jacobi_2@@Base+0x1c8>  // b.none
   36a7c:	b	36afc <__gmpn_jacobi_2@@Base+0x1e0>
   36a80:	rbit	x10, x12
   36a84:	clz	x10, x10
   36a88:	lsl	w13, w10, #1
   36a8c:	lsr	x1, x12, x10
   36a90:	add	w10, w13, #0x80
   36a94:	and	w12, w1, w9
   36a98:	and	w10, w10, w11
   36a9c:	eor	w11, w2, w12
   36aa0:	eor	w2, w11, w10
   36aa4:	cmp	x1, #0x1
   36aa8:	b.eq	36ae4 <__gmpn_jacobi_2@@Base+0x1c8>  // b.none
   36aac:	b	36afc <__gmpn_jacobi_2@@Base+0x1e0>
   36ab0:	rbit	x8, x9
   36ab4:	clz	x8, x8
   36ab8:	lsl	w12, w8, #1
   36abc:	lsr	x1, x9, x8
   36ac0:	add	w8, w12, #0x80
   36ac4:	and	w9, w1, w0
   36ac8:	and	w8, w8, w11
   36acc:	eor	w9, w2, w9
   36ad0:	eor	w2, w9, w8
   36ad4:	mov	x9, x0
   36ad8:	mov	x8, x10
   36adc:	cmp	x1, #0x1
   36ae0:	b.ne	36afc <__gmpn_jacobi_2@@Base+0x1e0>  // b.any
   36ae4:	and	w8, w2, #0x2
   36ae8:	mov	w9, #0x1                   	// #1
   36aec:	sub	w0, w9, w8
   36af0:	ret
   36af4:	and	w11, w1, w9
   36af8:	eor	w2, w10, w11
   36afc:	cbz	x8, 36b3c <__gmpn_jacobi_2@@Base+0x220>
   36b00:	eor	x10, x1, x1, lsr #1
   36b04:	subs	x11, x9, x1
   36b08:	cset	w9, cc  // cc = lo, ul, last
   36b0c:	sub	x9, x8, x9
   36b10:	cbz	x11, 36b44 <__gmpn_jacobi_2@@Base+0x228>
   36b14:	rbit	x8, x11
   36b18:	clz	x12, x8
   36b1c:	neg	x13, x12
   36b20:	lsr	x11, x11, x12
   36b24:	lsr	x8, x9, x12
   36b28:	and	w12, w10, w12, lsl #1
   36b2c:	lsl	x9, x9, x13
   36b30:	orr	x9, x9, x11
   36b34:	eor	w2, w2, w12
   36b38:	cbnz	x8, 36b04 <__gmpn_jacobi_2@@Base+0x1e8>
   36b3c:	mov	x0, x9
   36b40:	b	c730 <__gmpn_jacobi_base@plt>
   36b44:	cbz	x9, 36be4 <__gmpn_jacobi_2@@Base+0x2c8>
   36b48:	rbit	x8, x9
   36b4c:	clz	x8, x8
   36b50:	lsl	w11, w8, #1
   36b54:	orr	w11, w11, #0x80
   36b58:	and	w10, w11, w10
   36b5c:	eor	w2, w2, w10
   36b60:	lsr	x0, x9, x8
   36b64:	b	c730 <__gmpn_jacobi_base@plt>
   36b68:	mov	x1, x9
   36b6c:	b	c730 <__gmpn_jacobi_base@plt>
   36b70:	mov	x1, x9
   36b74:	mov	x9, x0
   36b78:	mov	x8, x10
   36b7c:	cmp	x1, #0x1
   36b80:	b.eq	36ae4 <__gmpn_jacobi_2@@Base+0x1c8>  // b.none
   36b84:	b	36afc <__gmpn_jacobi_2@@Base+0x1e0>
   36b88:	mov	x10, x8
   36b8c:	cmp	x0, x9
   36b90:	csel	x11, x0, x9, cc  // cc = lo, ul, last
   36b94:	csel	x8, x9, x0, cc  // cc = lo, ul, last
   36b98:	subs	x8, x8, x11
   36b9c:	b.eq	36be4 <__gmpn_jacobi_2@@Base+0x2c8>  // b.none
   36ba0:	and	w12, w9, w0
   36ba4:	cmp	x0, x9
   36ba8:	rbit	x9, x8
   36bac:	lsr	x13, x11, #1
   36bb0:	csel	w12, w12, wzr, cc  // cc = lo, ul, last
   36bb4:	clz	x9, x9
   36bb8:	eor	w13, w13, w11
   36bbc:	eor	w12, w12, w2
   36bc0:	and	w13, w13, w9, lsl #1
   36bc4:	lsr	x1, x8, x9
   36bc8:	cmp	x1, #0x1
   36bcc:	eor	w8, w12, w13
   36bd0:	b.ne	36bec <__gmpn_jacobi_2@@Base+0x2d0>  // b.any
   36bd4:	and	w8, w8, #0x2
   36bd8:	mov	w9, #0x1                   	// #1
   36bdc:	sub	w0, w9, w8
   36be0:	ret
   36be4:	mov	w0, wzr
   36be8:	ret
   36bec:	and	w9, w1, w11
   36bf0:	eor	w2, w8, w9
   36bf4:	mov	x8, x10
   36bf8:	mov	x9, x11
   36bfc:	b	36afc <__gmpn_jacobi_2@@Base+0x1e0>

0000000000036c00 <__gmpn_jacobi_n@@Base>:
   36c00:	stp	x29, x30, [sp, #-96]!
   36c04:	str	x27, [sp, #16]
   36c08:	stp	x26, x25, [sp, #32]
   36c0c:	stp	x24, x23, [sp, #48]
   36c10:	stp	x22, x21, [sp, #64]
   36c14:	stp	x20, x19, [sp, #80]
   36c18:	mov	x29, sp
   36c1c:	sub	sp, sp, #0x40
   36c20:	mov	x21, x2
   36c24:	mov	x19, x1
   36c28:	mov	x20, x0
   36c2c:	cmp	x2, #0x14a
   36c30:	mov	x8, x2
   36c34:	str	w3, [x29, #28]
   36c38:	b.lt	36c90 <__gmpn_jacobi_n@@Base+0x90>  // b.tstop
   36c3c:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   36c40:	lsl	x8, x21, #1
   36c44:	movk	x9, #0x5556
   36c48:	smulh	x8, x8, x9
   36c4c:	add	x22, x8, x8, lsr #63
   36c50:	sub	x0, x21, x22
   36c54:	add	x8, x0, #0x1
   36c58:	add	x9, x0, #0x2
   36c5c:	cmp	x8, #0x0
   36c60:	csinc	x8, x9, x0, lt  // lt = tstop
   36c64:	lsl	x8, x8, #1
   36c68:	and	x23, x8, #0xfffffffffffffffc
   36c6c:	bl	c590 <__gmpn_hgcd_itch@plt>
   36c70:	add	x8, x22, x21
   36c74:	sub	x9, x8, #0x1
   36c78:	cmp	x0, x8
   36c7c:	csel	x8, x9, x0, lt  // lt = tstop
   36c80:	add	x8, x23, x8
   36c84:	add	x8, x8, #0x4
   36c88:	cmp	x8, x21
   36c8c:	csel	x8, x8, x21, gt
   36c90:	lsl	x1, x8, #3
   36c94:	mov	w8, #0x7f00                	// #32512
   36c98:	cmp	x1, x8
   36c9c:	stur	xzr, [x29, #-8]
   36ca0:	b.hi	36d90 <__gmpn_jacobi_n@@Base+0x190>  // b.pmore
   36ca4:	add	x9, x1, #0xf
   36ca8:	mov	x8, sp
   36cac:	and	x9, x9, #0xfffffffffffffff0
   36cb0:	sub	x22, x8, x9
   36cb4:	mov	sp, x22
   36cb8:	cmp	x21, #0x14a
   36cbc:	b.lt	36da4 <__gmpn_jacobi_n@@Base+0x1a4>  // b.tstop
   36cc0:	mov	x27, #0x5555555555555555    	// #6148914691236517205
   36cc4:	adrp	x23, 36000 <__gmpn_tdiv_qr@@Base+0x324>
   36cc8:	movk	x27, #0x5556
   36ccc:	add	x23, x23, #0xf80
   36cd0:	b	36cfc <__gmpn_jacobi_n@@Base+0xfc>
   36cd4:	add	x1, x0, x24
   36cd8:	sub	x0, x29, #0x38
   36cdc:	mov	x2, x20
   36ce0:	mov	x3, x19
   36ce4:	mov	x4, x24
   36ce8:	mov	x5, x25
   36cec:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   36cf0:	mov	x21, x0
   36cf4:	cmp	x21, #0x149
   36cf8:	b.le	36da4 <__gmpn_jacobi_n@@Base+0x1a4>
   36cfc:	lsl	x8, x21, #1
   36d00:	smulh	x8, x8, x27
   36d04:	add	x24, x8, x8, lsr #63
   36d08:	sub	x26, x21, x24
   36d0c:	add	x8, x26, #0x1
   36d10:	add	x9, x26, #0x2
   36d14:	cmp	x8, #0x0
   36d18:	csinc	x8, x9, x26, lt  // lt = tstop
   36d1c:	lsl	x8, x8, #4
   36d20:	sub	x0, x29, #0x38
   36d24:	mov	x1, x26
   36d28:	mov	x2, x22
   36d2c:	and	x25, x8, #0xffffffffffffffe0
   36d30:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   36d34:	add	x9, x25, x22
   36d38:	lsl	x8, x24, #3
   36d3c:	add	x25, x9, #0x20
   36d40:	add	x0, x20, x8
   36d44:	add	x1, x19, x8
   36d48:	sub	x3, x29, #0x38
   36d4c:	add	x4, x29, #0x1c
   36d50:	mov	x2, x26
   36d54:	mov	x5, x25
   36d58:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   36d5c:	cmp	x0, #0x1
   36d60:	b.ge	36cd4 <__gmpn_jacobi_n@@Base+0xd4>  // b.tcont
   36d64:	add	x5, x29, #0x1c
   36d68:	mov	x0, x20
   36d6c:	mov	x1, x19
   36d70:	mov	x2, x21
   36d74:	mov	x3, xzr
   36d78:	mov	x4, x23
   36d7c:	mov	x6, x22
   36d80:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   36d84:	mov	x21, x0
   36d88:	cbnz	x0, 36cf4 <__gmpn_jacobi_n@@Base+0xf4>
   36d8c:	b	36ea8 <__gmpn_jacobi_n@@Base+0x2a8>
   36d90:	sub	x0, x29, #0x8
   36d94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   36d98:	mov	x22, x0
   36d9c:	cmp	x21, #0x14a
   36da0:	b.ge	36cc0 <__gmpn_jacobi_n@@Base+0xc0>  // b.tcont
   36da4:	cmp	x21, #0x3
   36da8:	b.lt	36ecc <__gmpn_jacobi_n@@Base+0x2cc>  // b.tstop
   36dac:	adrp	x23, 36000 <__gmpn_tdiv_qr@@Base+0x324>
   36db0:	add	x23, x23, #0xf80
   36db4:	b	36de8 <__gmpn_jacobi_n@@Base+0x1e8>
   36db8:	sub	x0, x29, #0x38
   36dbc:	mov	x1, x22
   36dc0:	mov	x2, x20
   36dc4:	mov	x3, x19
   36dc8:	mov	x4, x21
   36dcc:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   36dd0:	mov	x21, x0
   36dd4:	mov	x0, x20
   36dd8:	mov	x20, x22
   36ddc:	mov	x22, x0
   36de0:	cmp	x21, #0x2
   36de4:	b.le	36ecc <__gmpn_jacobi_n@@Base+0x2cc>
   36de8:	lsl	x8, x21, #3
   36dec:	sub	x9, x8, #0x8
   36df0:	ldr	x0, [x20, x9]
   36df4:	ldr	x2, [x19, x9]
   36df8:	orr	x9, x2, x0
   36dfc:	tbnz	x9, #63, 36e64 <__gmpn_jacobi_n@@Base+0x264>
   36e00:	sub	x10, x8, #0x10
   36e04:	sub	x8, x8, #0x18
   36e08:	ldr	x12, [x20, x10]
   36e0c:	ldr	x14, [x20, x8]
   36e10:	ldr	x10, [x19, x10]
   36e14:	ldr	x8, [x19, x8]
   36e18:	clz	x9, x9
   36e1c:	neg	x13, x9
   36e20:	lsl	x11, x0, x9
   36e24:	lsl	x15, x2, x9
   36e28:	lsr	x16, x12, x13
   36e2c:	lsl	x12, x12, x9
   36e30:	lsr	x14, x14, x13
   36e34:	lsl	x9, x10, x9
   36e38:	lsr	x10, x10, x13
   36e3c:	lsr	x8, x8, x13
   36e40:	orr	x0, x16, x11
   36e44:	orr	x1, x14, x12
   36e48:	orr	x2, x10, x15
   36e4c:	orr	x3, x8, x9
   36e50:	sub	x4, x29, #0x38
   36e54:	add	x5, x29, #0x1c
   36e58:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   36e5c:	cbnz	w0, 36db8 <__gmpn_jacobi_n@@Base+0x1b8>
   36e60:	b	36e80 <__gmpn_jacobi_n@@Base+0x280>
   36e64:	sub	x8, x8, #0x10
   36e68:	ldr	x1, [x20, x8]
   36e6c:	ldr	x3, [x19, x8]
   36e70:	sub	x4, x29, #0x38
   36e74:	add	x5, x29, #0x1c
   36e78:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   36e7c:	cbnz	w0, 36db8 <__gmpn_jacobi_n@@Base+0x1b8>
   36e80:	add	x5, x29, #0x1c
   36e84:	mov	x0, x20
   36e88:	mov	x1, x19
   36e8c:	mov	x2, x21
   36e90:	mov	x3, xzr
   36e94:	mov	x4, x23
   36e98:	mov	x6, x22
   36e9c:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   36ea0:	mov	x21, x0
   36ea4:	cbnz	x0, 36de0 <__gmpn_jacobi_n@@Base+0x1e0>
   36ea8:	ldur	x0, [x29, #-8]
   36eac:	cbnz	x0, 36f78 <__gmpn_jacobi_n@@Base+0x378>
   36eb0:	ldr	w8, [x29, #28]
   36eb4:	mov	w9, #0x1                   	// #1
   36eb8:	ubfiz	w10, w8, #1, #1
   36ebc:	sub	w9, w9, w10
   36ec0:	cmp	w8, #0x1f
   36ec4:	csel	w19, wzr, w9, eq  // eq = none
   36ec8:	b	36f54 <__gmpn_jacobi_n@@Base+0x354>
   36ecc:	ldr	w8, [x29, #28]
   36ed0:	cmp	w8, #0xf
   36ed4:	csel	x1, x20, x19, hi  // hi = pmore
   36ed8:	csel	x0, x19, x20, hi  // hi = pmore
   36edc:	cmp	x21, #0x1
   36ee0:	b.ne	36f10 <__gmpn_jacobi_n@@Base+0x310>  // b.any
   36ee4:	ldr	x19, [x0]
   36ee8:	ldur	x0, [x29, #-8]
   36eec:	ldr	x20, [x1]
   36ef0:	cbnz	x0, 36f30 <__gmpn_jacobi_n@@Base+0x330>
   36ef4:	cmp	x20, #0x1
   36ef8:	lsl	w2, w8, #1
   36efc:	b.ne	36f44 <__gmpn_jacobi_n@@Base+0x344>  // b.any
   36f00:	and	w8, w2, #0x2
   36f04:	mov	w9, #0x1                   	// #1
   36f08:	sub	w19, w9, w8
   36f0c:	b	36f54 <__gmpn_jacobi_n@@Base+0x354>
   36f10:	and	w2, w8, #0x1
   36f14:	bl	cab0 <__gmpn_jacobi_2@plt>
   36f18:	ldur	x8, [x29, #-8]
   36f1c:	mov	w19, w0
   36f20:	cbz	x8, 36f54 <__gmpn_jacobi_n@@Base+0x354>
   36f24:	mov	x0, x8
   36f28:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   36f2c:	b	36f54 <__gmpn_jacobi_n@@Base+0x354>
   36f30:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   36f34:	ldr	w8, [x29, #28]
   36f38:	cmp	x20, #0x1
   36f3c:	lsl	w2, w8, #1
   36f40:	b.eq	36f00 <__gmpn_jacobi_n@@Base+0x300>  // b.none
   36f44:	mov	x0, x19
   36f48:	mov	x1, x20
   36f4c:	bl	c730 <__gmpn_jacobi_base@plt>
   36f50:	mov	w19, w0
   36f54:	mov	w0, w19
   36f58:	mov	sp, x29
   36f5c:	ldp	x20, x19, [sp, #80]
   36f60:	ldp	x22, x21, [sp, #64]
   36f64:	ldp	x24, x23, [sp, #48]
   36f68:	ldp	x26, x25, [sp, #32]
   36f6c:	ldr	x27, [sp, #16]
   36f70:	ldp	x29, x30, [sp], #96
   36f74:	ret
   36f78:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   36f7c:	b	36eb0 <__gmpn_jacobi_n@@Base+0x2b0>
   36f80:	cbz	x1, 36f98 <__gmpn_jacobi_n@@Base+0x398>
   36f84:	cmp	x2, #0x1
   36f88:	b.ne	36fc0 <__gmpn_jacobi_n@@Base+0x3c0>  // b.any
   36f8c:	ldr	x8, [x1]
   36f90:	cmp	x8, #0x1
   36f94:	b.ne	36fc0 <__gmpn_jacobi_n@@Base+0x3c0>  // b.any
   36f98:	cbz	x3, 36fc8 <__gmpn_jacobi_n@@Base+0x3c8>
   36f9c:	ldr	w8, [x0]
   36fa0:	ldr	w9, [x3]
   36fa4:	lsl	w8, w8, #3
   36fa8:	add	w8, w8, w5, lsl #2
   36fac:	bfxil	w8, w9, #0, #2
   36fb0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   36fb4:	ldr	x9, [x9, #3872]
   36fb8:	ldrb	w8, [x9, w8, uxtw]
   36fbc:	b	36fc4 <__gmpn_jacobi_n@@Base+0x3c4>
   36fc0:	mov	w8, #0x1f                  	// #31
   36fc4:	str	w8, [x0]
   36fc8:	ret

0000000000036fcc <__gmpn_get_d@@Base>:
   36fcc:	fmov	d0, xzr
   36fd0:	cbz	x1, 3705c <__gmpn_get_d@@Base+0x90>
   36fd4:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   36fd8:	lsl	x8, x1, #6
   36fdc:	sub	x9, x9, x3
   36fe0:	cmp	x8, x9
   36fe4:	b.hi	37060 <__gmpn_get_d@@Base+0x94>  // b.pmore
   36fe8:	add	x9, x0, x1, lsl #3
   36fec:	ldur	x10, [x9, #-8]
   36ff0:	add	x8, x8, x3
   36ff4:	cmp	x1, #0x2
   36ff8:	clz	x11, x10
   36ffc:	mvn	x12, x11
   37000:	add	x8, x8, x12
   37004:	lsl	x10, x10, x11
   37008:	b.lt	37024 <__gmpn_get_d@@Base+0x58>  // b.tstop
   3700c:	cmp	w11, #0xc
   37010:	b.cc	37024 <__gmpn_get_d@@Base+0x58>  // b.lo, b.ul, b.last
   37014:	ldur	x9, [x9, #-16]
   37018:	neg	x11, x11
   3701c:	lsr	x9, x9, x11
   37020:	orr	x10, x9, x10
   37024:	cmp	x8, #0x3ff
   37028:	b.gt	37060 <__gmpn_get_d@@Base+0x94>
   3702c:	cmn	x8, #0x3ff
   37030:	lsr	x9, x10, #11
   37034:	b.le	37070 <__gmpn_get_d@@Base+0xa4>
   37038:	lsr	x10, x10, #43
   3703c:	mov	x11, #0x3ff0000000000000    	// #4607182418800017408
   37040:	and	x12, x2, #0x8000000000000000
   37044:	add	x8, x11, x8, lsl #52
   37048:	bfxil	x12, x9, #0, #32
   3704c:	and	x8, x8, #0x7ff0000000000000
   37050:	bfi	x12, x10, #32, #20
   37054:	orr	x8, x12, x8
   37058:	fmov	d0, x8
   3705c:	ret
   37060:	mov	x10, xzr
   37064:	mov	x9, xzr
   37068:	mov	w8, #0x400                 	// #1024
   3706c:	b	3703c <__gmpn_get_d@@Base+0x70>
   37070:	cmn	x8, #0x432
   37074:	b.lt	3705c <__gmpn_get_d@@Base+0x90>  // b.tstop
   37078:	mov	w10, #0xfffffc02            	// #-1022
   3707c:	sub	w8, w10, w8
   37080:	lsr	x9, x9, x8
   37084:	lsr	x10, x9, #32
   37088:	mov	x8, #0xfffffffffffffc01    	// #-1023
   3708c:	b	3703c <__gmpn_get_d@@Base+0x70>

0000000000037090 <__gmpn_matrix22_mul_itch@@Base>:
   37090:	cmp	x0, #0xa
   37094:	b.lt	370b0 <__gmpn_matrix22_mul_itch@@Base+0x20>  // b.tstop
   37098:	cmp	x1, #0x9
   3709c:	b.le	370b0 <__gmpn_matrix22_mul_itch@@Base+0x20>
   370a0:	add	x8, x1, x0
   370a4:	add	x8, x8, x8, lsl #1
   370a8:	add	x0, x8, #0x5
   370ac:	ret
   370b0:	add	x8, x0, x0, lsl #1
   370b4:	add	x0, x8, x1, lsl #1
   370b8:	ret

00000000000370bc <__gmpn_matrix22_mul@@Base>:
   370bc:	sub	sp, sp, #0xd0
   370c0:	stp	x29, x30, [sp, #112]
   370c4:	stp	x28, x27, [sp, #128]
   370c8:	stp	x26, x25, [sp, #144]
   370cc:	stp	x24, x23, [sp, #160]
   370d0:	stp	x22, x21, [sp, #176]
   370d4:	stp	x20, x19, [sp, #192]
   370d8:	add	x29, sp, #0x70
   370dc:	ldp	x23, x20, [x29, #104]
   370e0:	ldr	x24, [x29, #96]
   370e4:	mov	x28, x7
   370e8:	mov	x21, x4
   370ec:	mov	x19, x3
   370f0:	mov	x25, x1
   370f4:	cmp	x4, #0xa
   370f8:	mov	x22, x0
   370fc:	stur	x6, [x29, #-16]
   37100:	stur	x2, [x29, #-40]
   37104:	str	x24, [sp, #40]
   37108:	stur	x5, [x29, #-48]
   3710c:	b.lt	37168 <__gmpn_matrix22_mul@@Base+0xac>  // b.tstop
   37110:	cmp	x23, #0x9
   37114:	b.le	37168 <__gmpn_matrix22_mul@@Base+0xac>
   37118:	add	x8, x21, #0x1
   3711c:	str	x8, [sp, #56]
   37120:	add	x9, x23, #0x1
   37124:	add	x10, x23, x21
   37128:	add	x8, x20, x8, lsl #3
   3712c:	str	x10, [sp, #48]
   37130:	add	x10, x10, #0x1
   37134:	add	x0, x8, x9, lsl #3
   37138:	cmp	x21, x23
   3713c:	add	x27, x0, x10, lsl #3
   37140:	str	x9, [sp, #32]
   37144:	stp	x20, x8, [x29, #-32]
   37148:	stur	x0, [x29, #-8]
   3714c:	str	x10, [sp, #16]
   37150:	b.lt	372cc <__gmpn_matrix22_mul@@Base+0x210>  // b.tstop
   37154:	mov	x1, x25
   37158:	mov	x2, x21
   3715c:	mov	x3, x28
   37160:	mov	x4, x23
   37164:	b	372dc <__gmpn_matrix22_mul@@Base+0x220>
   37168:	lsl	x8, x21, #3
   3716c:	add	x9, x20, x8
   37170:	add	x8, x9, x8
   37174:	mov	x27, x24
   37178:	add	x24, x8, x23, lsl #3
   3717c:	add	x8, x23, x21
   37180:	mov	x0, x20
   37184:	mov	x1, x22
   37188:	mov	x2, x21
   3718c:	stur	x25, [x29, #-24]
   37190:	mov	x26, x5
   37194:	mov	x25, x9
   37198:	stur	x8, [x29, #-8]
   3719c:	bl	ca50 <__gmpn_copyi@plt>
   371a0:	cmp	x21, x23
   371a4:	mov	x0, x25
   371a8:	stur	x25, [x29, #-32]
   371ac:	b.ge	37390 <__gmpn_matrix22_mul@@Base+0x2d4>  // b.tcont
   371b0:	mov	x1, x26
   371b4:	mov	x2, x23
   371b8:	mov	x3, x22
   371bc:	mov	x4, x21
   371c0:	bl	ccd0 <__gmpn_mul@plt>
   371c4:	ldur	x25, [x29, #-24]
   371c8:	mov	x0, x24
   371cc:	mov	x1, x27
   371d0:	mov	x2, x23
   371d4:	mov	x3, x25
   371d8:	mov	x4, x21
   371dc:	mov	x26, x24
   371e0:	bl	ccd0 <__gmpn_mul@plt>
   371e4:	mov	x0, x22
   371e8:	mov	x1, x28
   371ec:	mov	x2, x23
   371f0:	mov	x3, x25
   371f4:	mov	x4, x21
   371f8:	bl	ccd0 <__gmpn_mul@plt>
   371fc:	ldur	x1, [x29, #-16]
   37200:	mov	x0, x25
   37204:	mov	x2, x23
   37208:	mov	x3, x20
   3720c:	mov	x4, x21
   37210:	bl	ccd0 <__gmpn_mul@plt>
   37214:	ldur	x27, [x29, #-32]
   37218:	ldur	x24, [x29, #-8]
   3721c:	mov	x0, x22
   37220:	mov	x1, x22
   37224:	mov	x2, x27
   37228:	mov	x3, x24
   3722c:	bl	ca70 <__gmpn_add_n@plt>
   37230:	lsl	x8, x24, #3
   37234:	str	x0, [x22, x8]
   37238:	mov	x0, x25
   3723c:	mov	x1, x25
   37240:	mov	x2, x26
   37244:	mov	x3, x24
   37248:	mov	x22, x8
   3724c:	str	x8, [sp, #56]
   37250:	bl	ca70 <__gmpn_add_n@plt>
   37254:	str	x0, [x25, x22]
   37258:	ldur	x22, [x29, #-40]
   3725c:	mov	x0, x20
   37260:	mov	x2, x21
   37264:	mov	x1, x22
   37268:	bl	ca50 <__gmpn_copyi@plt>
   3726c:	ldur	x1, [x29, #-48]
   37270:	mov	x0, x27
   37274:	mov	x2, x23
   37278:	mov	x3, x22
   3727c:	mov	x4, x21
   37280:	bl	ccd0 <__gmpn_mul@plt>
   37284:	ldr	x1, [sp, #40]
   37288:	mov	x0, x26
   3728c:	mov	x2, x23
   37290:	mov	x3, x19
   37294:	mov	x4, x21
   37298:	bl	ccd0 <__gmpn_mul@plt>
   3729c:	mov	x0, x22
   372a0:	mov	x1, x28
   372a4:	mov	x2, x23
   372a8:	mov	x3, x19
   372ac:	mov	x4, x21
   372b0:	bl	ccd0 <__gmpn_mul@plt>
   372b4:	ldur	x1, [x29, #-16]
   372b8:	mov	x0, x19
   372bc:	mov	x2, x23
   372c0:	mov	x3, x20
   372c4:	mov	x4, x21
   372c8:	b	374a8 <__gmpn_matrix22_mul@@Base+0x3ec>
   372cc:	mov	x1, x28
   372d0:	mov	x2, x23
   372d4:	mov	x3, x25
   372d8:	mov	x4, x21
   372dc:	bl	ccd0 <__gmpn_mul@plt>
   372e0:	ldur	x2, [x29, #-40]
   372e4:	sub	x26, x21, #0x1
   372e8:	mov	x8, x26
   372ec:	add	x9, x8, #0x1
   372f0:	cmp	x9, #0x1
   372f4:	b.lt	37314 <__gmpn_matrix22_mul@@Base+0x258>  // b.tstop
   372f8:	lsl	x9, x8, #3
   372fc:	ldr	x10, [x19, x9]
   37300:	ldr	x9, [x2, x9]
   37304:	sub	x8, x8, #0x1
   37308:	cmp	x10, x9
   3730c:	b.eq	372ec <__gmpn_matrix22_mul@@Base+0x230>  // b.none
   37310:	b.ls	37510 <__gmpn_matrix22_mul@@Base+0x454>  // b.plast
   37314:	mov	x0, x19
   37318:	mov	x1, x19
   3731c:	mov	x3, x21
   37320:	mov	x20, x27
   37324:	mov	x27, x24
   37328:	bl	c2d0 <__gmpn_sub_n@plt>
   3732c:	mov	x0, x25
   37330:	mov	x1, x25
   37334:	mov	x2, x19
   37338:	mov	x3, x21
   3733c:	bl	ca70 <__gmpn_add_n@plt>
   37340:	str	x0, [x25, x21, lsl #3]
   37344:	cbz	x0, 37500 <__gmpn_matrix22_mul@@Base+0x444>
   37348:	ldur	x26, [x29, #-32]
   3734c:	mov	x24, x0
   37350:	mov	x1, x25
   37354:	mov	x2, x22
   37358:	mov	x0, x26
   3735c:	mov	x3, x21
   37360:	bl	c2d0 <__gmpn_sub_n@plt>
   37364:	mov	w8, #0x1                   	// #1
   37368:	str	wzr, [sp, #28]
   3736c:	sub	x0, x24, x0
   37370:	stp	wzr, w8, [sp, #8]
   37374:	mov	x24, x27
   37378:	mov	x27, x20
   3737c:	cmp	x21, x23
   37380:	str	x0, [x26, x21, lsl #3]
   37384:	mov	x0, x27
   37388:	b.ge	37608 <__gmpn_matrix22_mul@@Base+0x54c>  // b.tcont
   3738c:	b	3766c <__gmpn_matrix22_mul@@Base+0x5b0>
   37390:	mov	x1, x22
   37394:	mov	x2, x21
   37398:	mov	x3, x26
   3739c:	mov	x4, x23
   373a0:	bl	ccd0 <__gmpn_mul@plt>
   373a4:	ldur	x25, [x29, #-24]
   373a8:	mov	x0, x24
   373ac:	mov	x2, x21
   373b0:	mov	x3, x27
   373b4:	mov	x1, x25
   373b8:	mov	x4, x23
   373bc:	mov	x26, x24
   373c0:	bl	ccd0 <__gmpn_mul@plt>
   373c4:	mov	x0, x22
   373c8:	mov	x1, x25
   373cc:	mov	x2, x21
   373d0:	mov	x3, x28
   373d4:	mov	x4, x23
   373d8:	bl	ccd0 <__gmpn_mul@plt>
   373dc:	ldur	x3, [x29, #-16]
   373e0:	mov	x0, x25
   373e4:	mov	x1, x20
   373e8:	mov	x2, x21
   373ec:	mov	x4, x23
   373f0:	bl	ccd0 <__gmpn_mul@plt>
   373f4:	ldur	x27, [x29, #-32]
   373f8:	ldur	x24, [x29, #-8]
   373fc:	mov	x0, x22
   37400:	mov	x1, x22
   37404:	mov	x2, x27
   37408:	mov	x3, x24
   3740c:	bl	ca70 <__gmpn_add_n@plt>
   37410:	lsl	x8, x24, #3
   37414:	str	x0, [x22, x8]
   37418:	mov	x0, x25
   3741c:	mov	x1, x25
   37420:	mov	x2, x26
   37424:	mov	x3, x24
   37428:	mov	x22, x8
   3742c:	str	x8, [sp, #56]
   37430:	bl	ca70 <__gmpn_add_n@plt>
   37434:	str	x0, [x25, x22]
   37438:	ldur	x22, [x29, #-40]
   3743c:	mov	x0, x20
   37440:	mov	x2, x21
   37444:	mov	x1, x22
   37448:	bl	ca50 <__gmpn_copyi@plt>
   3744c:	ldur	x3, [x29, #-48]
   37450:	mov	x0, x27
   37454:	mov	x1, x22
   37458:	mov	x2, x21
   3745c:	mov	x4, x23
   37460:	bl	ccd0 <__gmpn_mul@plt>
   37464:	ldr	x3, [sp, #40]
   37468:	mov	x0, x26
   3746c:	mov	x1, x19
   37470:	mov	x2, x21
   37474:	mov	x4, x23
   37478:	bl	ccd0 <__gmpn_mul@plt>
   3747c:	mov	x0, x22
   37480:	mov	x1, x19
   37484:	mov	x2, x21
   37488:	mov	x3, x28
   3748c:	mov	x4, x23
   37490:	bl	ccd0 <__gmpn_mul@plt>
   37494:	ldur	x3, [x29, #-16]
   37498:	mov	x0, x19
   3749c:	mov	x1, x20
   374a0:	mov	x2, x21
   374a4:	mov	x4, x23
   374a8:	bl	ccd0 <__gmpn_mul@plt>
   374ac:	mov	x0, x22
   374b0:	mov	x1, x22
   374b4:	mov	x2, x27
   374b8:	mov	x3, x24
   374bc:	bl	ca70 <__gmpn_add_n@plt>
   374c0:	ldr	x20, [sp, #56]
   374c4:	mov	x1, x19
   374c8:	mov	x2, x26
   374cc:	mov	x3, x24
   374d0:	str	x0, [x22, x20]
   374d4:	mov	x0, x19
   374d8:	bl	ca70 <__gmpn_add_n@plt>
   374dc:	str	x0, [x19, x20]
   374e0:	ldp	x20, x19, [sp, #192]
   374e4:	ldp	x22, x21, [sp, #176]
   374e8:	ldp	x24, x23, [sp, #160]
   374ec:	ldp	x26, x25, [sp, #144]
   374f0:	ldp	x28, x27, [sp, #128]
   374f4:	ldp	x29, x30, [sp, #112]
   374f8:	add	sp, sp, #0xd0
   374fc:	ret
   37500:	mov	x24, x27
   37504:	mov	x27, x20
   37508:	str	wzr, [sp, #8]
   3750c:	b	37570 <__gmpn_matrix22_mul@@Base+0x4b4>
   37510:	mov	x0, x19
   37514:	mov	x1, x2
   37518:	mov	x2, x19
   3751c:	mov	x3, x21
   37520:	bl	c2d0 <__gmpn_sub_n@plt>
   37524:	mov	x8, x26
   37528:	add	x9, x8, #0x1
   3752c:	cmp	x9, #0x1
   37530:	b.lt	37550 <__gmpn_matrix22_mul@@Base+0x494>  // b.tstop
   37534:	lsl	x9, x8, #3
   37538:	ldr	x10, [x25, x9]
   3753c:	ldr	x9, [x19, x9]
   37540:	sub	x8, x8, #0x1
   37544:	cmp	x10, x9
   37548:	b.eq	37528 <__gmpn_matrix22_mul@@Base+0x46c>  // b.none
   3754c:	b.ls	3761c <__gmpn_matrix22_mul@@Base+0x560>  // b.plast
   37550:	mov	x0, x25
   37554:	mov	x1, x25
   37558:	mov	x2, x19
   3755c:	mov	x3, x21
   37560:	bl	c2d0 <__gmpn_sub_n@plt>
   37564:	mov	w8, #0x1                   	// #1
   37568:	str	xzr, [x25, x21, lsl #3]
   3756c:	str	w8, [sp, #8]
   37570:	add	x8, x26, #0x1
   37574:	cmp	x8, #0x1
   37578:	b.lt	37598 <__gmpn_matrix22_mul@@Base+0x4dc>  // b.tstop
   3757c:	lsl	x8, x26, #3
   37580:	ldr	x9, [x22, x8]
   37584:	ldr	x8, [x25, x8]
   37588:	sub	x26, x26, #0x1
   3758c:	cmp	x9, x8
   37590:	b.eq	37570 <__gmpn_matrix22_mul@@Base+0x4b4>  // b.none
   37594:	b.ls	375d0 <__gmpn_matrix22_mul@@Base+0x514>  // b.plast
   37598:	ldur	x26, [x29, #-32]
   3759c:	mov	x1, x22
   375a0:	mov	x2, x25
   375a4:	mov	x3, x21
   375a8:	mov	x0, x26
   375ac:	bl	c2d0 <__gmpn_sub_n@plt>
   375b0:	mov	x0, xzr
   375b4:	str	wzr, [sp, #28]
   375b8:	str	wzr, [sp, #12]
   375bc:	cmp	x21, x23
   375c0:	str	x0, [x26, x21, lsl #3]
   375c4:	mov	x0, x27
   375c8:	b.ge	37608 <__gmpn_matrix22_mul@@Base+0x54c>  // b.tcont
   375cc:	b	3766c <__gmpn_matrix22_mul@@Base+0x5b0>
   375d0:	ldur	x26, [x29, #-32]
   375d4:	mov	x1, x25
   375d8:	mov	x2, x22
   375dc:	mov	x3, x21
   375e0:	mov	x0, x26
   375e4:	bl	c2d0 <__gmpn_sub_n@plt>
   375e8:	mov	x0, xzr
   375ec:	mov	w8, #0x1                   	// #1
   375f0:	str	wzr, [sp, #28]
   375f4:	str	w8, [sp, #12]
   375f8:	cmp	x21, x23
   375fc:	str	x0, [x26, x21, lsl #3]
   37600:	mov	x0, x27
   37604:	b.lt	3766c <__gmpn_matrix22_mul@@Base+0x5b0>  // b.tstop
   37608:	ldur	x3, [x29, #-48]
   3760c:	mov	x1, x22
   37610:	mov	x2, x21
   37614:	mov	x4, x23
   37618:	b	3767c <__gmpn_matrix22_mul@@Base+0x5c0>
   3761c:	mov	x0, x25
   37620:	mov	x1, x19
   37624:	mov	x2, x25
   37628:	mov	x3, x21
   3762c:	bl	c2d0 <__gmpn_sub_n@plt>
   37630:	ldur	x26, [x29, #-32]
   37634:	mov	x1, x25
   37638:	mov	x2, x22
   3763c:	mov	x3, x21
   37640:	mov	x0, x26
   37644:	str	xzr, [x25, x21, lsl #3]
   37648:	bl	ca70 <__gmpn_add_n@plt>
   3764c:	mov	w8, #0x1                   	// #1
   37650:	str	w8, [sp, #28]
   37654:	mov	w8, #0x1                   	// #1
   37658:	stp	w8, wzr, [sp, #8]
   3765c:	cmp	x21, x23
   37660:	str	x0, [x26, x21, lsl #3]
   37664:	mov	x0, x27
   37668:	b.ge	37608 <__gmpn_matrix22_mul@@Base+0x54c>  // b.tcont
   3766c:	ldur	x1, [x29, #-48]
   37670:	mov	x2, x23
   37674:	mov	x3, x22
   37678:	mov	x4, x21
   3767c:	bl	ccd0 <__gmpn_mul@plt>
   37680:	ldr	x20, [sp, #48]
   37684:	ldur	x1, [x29, #-8]
   37688:	mov	x0, x22
   3768c:	mov	x2, x27
   37690:	mov	x3, x20
   37694:	bl	ca70 <__gmpn_add_n@plt>
   37698:	sub	x8, x23, #0x1
   3769c:	str	x0, [x22, x20, lsl #3]
   376a0:	add	x9, x8, #0x1
   376a4:	cmp	x9, #0x1
   376a8:	b.lt	376c8 <__gmpn_matrix22_mul@@Base+0x60c>  // b.tstop
   376ac:	lsl	x9, x8, #3
   376b0:	ldr	x10, [x24, x9]
   376b4:	ldr	x9, [x28, x9]
   376b8:	sub	x8, x8, #0x1
   376bc:	cmp	x10, x9
   376c0:	b.eq	376a0 <__gmpn_matrix22_mul@@Base+0x5e4>  // b.none
   376c4:	b.ls	37764 <__gmpn_matrix22_mul@@Base+0x6a8>  // b.plast
   376c8:	ldur	x0, [x29, #-24]
   376cc:	mov	x1, x24
   376d0:	mov	x2, x28
   376d4:	mov	x3, x23
   376d8:	bl	c2d0 <__gmpn_sub_n@plt>
   376dc:	mov	w22, wzr
   376e0:	ldur	x24, [x29, #-16]
   376e4:	mov	x0, x27
   376e8:	cmp	x21, x23
   376ec:	b.lt	3778c <__gmpn_matrix22_mul@@Base+0x6d0>  // b.tstop
   376f0:	ldur	x28, [x29, #-24]
   376f4:	mov	x1, x19
   376f8:	mov	x2, x21
   376fc:	mov	x4, x23
   37700:	mov	x3, x28
   37704:	bl	ccd0 <__gmpn_mul@plt>
   37708:	ldr	x8, [sp, #48]
   3770c:	str	xzr, [x27, x8, lsl #3]
   37710:	cbz	w22, 377b0 <__gmpn_matrix22_mul@@Base+0x6f4>
   37714:	ldur	x20, [x29, #-40]
   37718:	add	x8, x26, x21, lsl #3
   3771c:	mov	x9, x23
   37720:	subs	x10, x9, #0x1
   37724:	b.lt	37748 <__gmpn_matrix22_mul@@Base+0x68c>  // b.tstop
   37728:	lsl	x9, x9, #3
   3772c:	add	x11, x24, x9
   37730:	ldur	x11, [x11, #-8]
   37734:	ldr	x9, [x8, x9]
   37738:	cmp	x11, x9
   3773c:	mov	x9, x10
   37740:	b.eq	37720 <__gmpn_matrix22_mul@@Base+0x664>  // b.none
   37744:	b.ls	37818 <__gmpn_matrix22_mul@@Base+0x75c>  // b.plast
   37748:	mov	x0, x28
   3774c:	mov	x1, x24
   37750:	mov	x2, x28
   37754:	mov	x3, x23
   37758:	bl	c2d0 <__gmpn_sub_n@plt>
   3775c:	mov	w26, wzr
   37760:	b	37830 <__gmpn_matrix22_mul@@Base+0x774>
   37764:	ldur	x0, [x29, #-24]
   37768:	mov	x1, x28
   3776c:	mov	x2, x24
   37770:	mov	x3, x23
   37774:	bl	c2d0 <__gmpn_sub_n@plt>
   37778:	mov	w22, #0x1                   	// #1
   3777c:	ldur	x24, [x29, #-16]
   37780:	mov	x0, x27
   37784:	cmp	x21, x23
   37788:	b.ge	376f0 <__gmpn_matrix22_mul@@Base+0x634>  // b.tcont
   3778c:	ldur	x28, [x29, #-24]
   37790:	mov	x2, x23
   37794:	mov	x3, x19
   37798:	mov	x4, x21
   3779c:	mov	x1, x28
   377a0:	bl	ccd0 <__gmpn_mul@plt>
   377a4:	ldr	x8, [sp, #48]
   377a8:	str	xzr, [x27, x8, lsl #3]
   377ac:	cbnz	w22, 37714 <__gmpn_matrix22_mul@@Base+0x658>
   377b0:	mov	x0, x28
   377b4:	mov	x1, x28
   377b8:	mov	x2, x24
   377bc:	mov	x3, x23
   377c0:	bl	ca70 <__gmpn_add_n@plt>
   377c4:	add	x24, x28, x23, lsl #3
   377c8:	str	x0, [x24]
   377cc:	cbz	x0, 3785c <__gmpn_matrix22_mul@@Base+0x7a0>
   377d0:	ldur	x20, [x29, #-40]
   377d4:	cmp	x23, x21
   377d8:	mov	x0, x19
   377dc:	b.ge	37a00 <__gmpn_matrix22_mul@@Base+0x944>  // b.tcont
   377e0:	ldr	x4, [sp, #32]
   377e4:	mov	x1, x25
   377e8:	mov	x2, x21
   377ec:	mov	x3, x28
   377f0:	bl	ccd0 <__gmpn_mul@plt>
   377f4:	ldr	x8, [x25, x21, lsl #3]
   377f8:	cbz	x8, 37a1c <__gmpn_matrix22_mul@@Base+0x960>
   377fc:	ldr	x3, [sp, #32]
   37800:	add	x0, x19, x21, lsl #3
   37804:	mov	x1, x0
   37808:	mov	x2, x28
   3780c:	bl	ca70 <__gmpn_add_n@plt>
   37810:	mov	w26, wzr
   37814:	b	37888 <__gmpn_matrix22_mul@@Base+0x7cc>
   37818:	mov	x0, x28
   3781c:	mov	x1, x28
   37820:	mov	x2, x24
   37824:	mov	x3, x23
   37828:	bl	c2d0 <__gmpn_sub_n@plt>
   3782c:	mov	w26, #0x1                   	// #1
   37830:	ldr	x8, [sp, #56]
   37834:	add	x24, x28, x23, lsl #3
   37838:	str	xzr, [x24]
   3783c:	mov	x0, x19
   37840:	cmp	x8, x23
   37844:	b.ge	37874 <__gmpn_matrix22_mul@@Base+0x7b8>  // b.tcont
   37848:	mov	x1, x28
   3784c:	mov	x2, x23
   37850:	mov	x3, x25
   37854:	mov	x4, x8
   37858:	b	37884 <__gmpn_matrix22_mul@@Base+0x7c8>
   3785c:	ldur	x20, [x29, #-40]
   37860:	ldr	x8, [sp, #56]
   37864:	mov	w26, wzr
   37868:	mov	x0, x19
   3786c:	cmp	x8, x23
   37870:	b.lt	37848 <__gmpn_matrix22_mul@@Base+0x78c>  // b.tstop
   37874:	mov	x1, x25
   37878:	mov	x2, x8
   3787c:	mov	x3, x28
   37880:	mov	x4, x23
   37884:	bl	ccd0 <__gmpn_mul@plt>
   37888:	ldr	w8, [sp, #28]
   3788c:	ldur	x12, [x29, #-8]
   37890:	ldr	x3, [sp, #16]
   37894:	cmp	w8, w26
   37898:	ldr	x8, [sp, #48]
   3789c:	str	xzr, [x12, x8, lsl #3]
   378a0:	b.ne	378c0 <__gmpn_matrix22_mul@@Base+0x804>  // b.any
   378a4:	mov	x0, x19
   378a8:	mov	x1, x19
   378ac:	mov	x2, x12
   378b0:	bl	ca70 <__gmpn_add_n@plt>
   378b4:	mov	w28, wzr
   378b8:	cbnz	w26, 3790c <__gmpn_matrix22_mul@@Base+0x850>
   378bc:	b	37950 <__gmpn_matrix22_mul@@Base+0x894>
   378c0:	ldur	x9, [x29, #-32]
   378c4:	add	x8, x23, x21
   378c8:	add	x9, x9, x8, lsl #4
   378cc:	add	x9, x9, #0x10
   378d0:	add	x10, x8, #0x1
   378d4:	cmp	x10, #0x1
   378d8:	b.lt	378f4 <__gmpn_matrix22_mul@@Base+0x838>  // b.tstop
   378dc:	ldr	x10, [x9], #-8
   378e0:	ldr	x11, [x19, x8, lsl #3]
   378e4:	sub	x8, x8, #0x1
   378e8:	cmp	x10, x11
   378ec:	b.eq	378d0 <__gmpn_matrix22_mul@@Base+0x814>  // b.none
   378f0:	b.ls	37938 <__gmpn_matrix22_mul@@Base+0x87c>  // b.plast
   378f4:	mov	x0, x19
   378f8:	mov	x1, x12
   378fc:	mov	x2, x19
   37900:	bl	c2d0 <__gmpn_sub_n@plt>
   37904:	mov	w28, wzr
   37908:	cbz	w26, 37950 <__gmpn_matrix22_mul@@Base+0x894>
   3790c:	ldur	x24, [x29, #-24]
   37910:	ldur	x2, [x29, #-48]
   37914:	mov	x3, x23
   37918:	mov	x0, x24
   3791c:	mov	x1, x24
   37920:	bl	ca70 <__gmpn_add_n@plt>
   37924:	str	x0, [x24, x23, lsl #3]
   37928:	ldur	x0, [x29, #-8]
   3792c:	cmp	x23, x21
   37930:	b.lt	379ec <__gmpn_matrix22_mul@@Base+0x930>  // b.tstop
   37934:	b	37a48 <__gmpn_matrix22_mul@@Base+0x98c>
   37938:	mov	x0, x19
   3793c:	mov	x1, x19
   37940:	mov	x2, x12
   37944:	bl	c2d0 <__gmpn_sub_n@plt>
   37948:	mov	w28, #0x1                   	// #1
   3794c:	cbnz	w26, 3790c <__gmpn_matrix22_mul@@Base+0x850>
   37950:	ldr	x8, [x24]
   37954:	cbz	x8, 37994 <__gmpn_matrix22_mul@@Base+0x8d8>
   37958:	ldur	x24, [x29, #-24]
   3795c:	ldur	x2, [x29, #-48]
   37960:	mov	x3, x23
   37964:	mov	x0, x24
   37968:	mov	x1, x24
   3796c:	bl	c2d0 <__gmpn_sub_n@plt>
   37970:	lsl	x8, x23, #3
   37974:	ldr	x9, [x24, x8]
   37978:	mov	w26, wzr
   3797c:	sub	x9, x9, x0
   37980:	str	x9, [x24, x8]
   37984:	ldur	x0, [x29, #-8]
   37988:	cmp	x23, x21
   3798c:	b.lt	379ec <__gmpn_matrix22_mul@@Base+0x930>  // b.tstop
   37990:	b	37a48 <__gmpn_matrix22_mul@@Base+0x98c>
   37994:	ldur	x8, [x29, #-32]
   37998:	ldur	x2, [x29, #-48]
   3799c:	mov	x9, x23
   379a0:	add	x8, x8, x21, lsl #3
   379a4:	subs	x10, x9, #0x1
   379a8:	b.lt	379cc <__gmpn_matrix22_mul@@Base+0x910>  // b.tstop
   379ac:	lsl	x9, x9, #3
   379b0:	ldr	x11, [x8, x9]
   379b4:	add	x9, x2, x9
   379b8:	ldur	x9, [x9, #-8]
   379bc:	cmp	x11, x9
   379c0:	mov	x9, x10
   379c4:	b.eq	379a4 <__gmpn_matrix22_mul@@Base+0x8e8>  // b.none
   379c8:	b.ls	37a24 <__gmpn_matrix22_mul@@Base+0x968>  // b.plast
   379cc:	ldur	x0, [x29, #-24]
   379d0:	mov	x3, x23
   379d4:	mov	x1, x0
   379d8:	bl	c2d0 <__gmpn_sub_n@plt>
   379dc:	mov	w26, wzr
   379e0:	ldur	x0, [x29, #-8]
   379e4:	cmp	x23, x21
   379e8:	b.ge	37a48 <__gmpn_matrix22_mul@@Base+0x98c>  // b.tcont
   379ec:	ldur	x3, [x29, #-24]
   379f0:	ldr	x4, [sp, #32]
   379f4:	mov	x1, x20
   379f8:	mov	x2, x21
   379fc:	b	37a58 <__gmpn_matrix22_mul@@Base+0x99c>
   37a00:	ldr	x2, [sp, #32]
   37a04:	mov	x1, x28
   37a08:	mov	x3, x25
   37a0c:	mov	x4, x21
   37a10:	bl	ccd0 <__gmpn_mul@plt>
   37a14:	ldr	x8, [x25, x21, lsl #3]
   37a18:	cbnz	x8, 377fc <__gmpn_matrix22_mul@@Base+0x740>
   37a1c:	mov	w26, wzr
   37a20:	b	37888 <__gmpn_matrix22_mul@@Base+0x7cc>
   37a24:	ldur	x0, [x29, #-24]
   37a28:	mov	x1, x2
   37a2c:	mov	x3, x23
   37a30:	mov	x2, x0
   37a34:	bl	c2d0 <__gmpn_sub_n@plt>
   37a38:	mov	w26, #0x1                   	// #1
   37a3c:	ldur	x0, [x29, #-8]
   37a40:	cmp	x23, x21
   37a44:	b.lt	379ec <__gmpn_matrix22_mul@@Base+0x930>  // b.tstop
   37a48:	ldur	x1, [x29, #-24]
   37a4c:	ldr	x2, [sp, #32]
   37a50:	mov	x3, x20
   37a54:	mov	x4, x21
   37a58:	bl	ccd0 <__gmpn_mul@plt>
   37a5c:	ldr	w8, [sp, #8]
   37a60:	eor	w22, w8, w22
   37a64:	ldr	w8, [sp, #28]
   37a68:	cbz	w8, 37a84 <__gmpn_matrix22_mul@@Base+0x9c8>
   37a6c:	mov	x0, x25
   37a70:	mov	x1, x20
   37a74:	mov	x2, x25
   37a78:	mov	x3, x21
   37a7c:	bl	c2d0 <__gmpn_sub_n@plt>
   37a80:	b	37aa8 <__gmpn_matrix22_mul@@Base+0x9ec>
   37a84:	mov	x0, x25
   37a88:	mov	x1, x25
   37a8c:	mov	x2, x20
   37a90:	mov	x3, x21
   37a94:	bl	ca70 <__gmpn_add_n@plt>
   37a98:	lsl	x8, x21, #3
   37a9c:	ldr	x9, [x25, x8]
   37aa0:	add	x9, x9, x0
   37aa4:	str	x9, [x25, x8]
   37aa8:	ldr	x8, [sp, #56]
   37aac:	ldur	x12, [x29, #-8]
   37ab0:	eor	w24, w22, #0x1
   37ab4:	cmp	w28, w26
   37ab8:	add	x22, x8, x23
   37abc:	b.ne	37ae4 <__gmpn_matrix22_mul@@Base+0xa28>  // b.any
   37ac0:	mov	x0, x20
   37ac4:	mov	x1, x19
   37ac8:	mov	x2, x12
   37acc:	mov	x3, x22
   37ad0:	bl	ca70 <__gmpn_add_n@plt>
   37ad4:	mov	w26, w28
   37ad8:	cmp	w28, w24
   37adc:	b.eq	37b3c <__gmpn_matrix22_mul@@Base+0xa80>  // b.none
   37ae0:	b	37b88 <__gmpn_matrix22_mul@@Base+0xacc>
   37ae4:	ldur	x9, [x29, #-32]
   37ae8:	add	x8, x23, x21
   37aec:	add	x9, x9, x8, lsl #4
   37af0:	add	x9, x9, #0x10
   37af4:	add	x10, x8, #0x1
   37af8:	cmp	x10, #0x1
   37afc:	b.lt	37b18 <__gmpn_matrix22_mul@@Base+0xa5c>  // b.tstop
   37b00:	ldr	x10, [x19, x8, lsl #3]
   37b04:	ldr	x11, [x9], #-8
   37b08:	sub	x8, x8, #0x1
   37b0c:	cmp	x10, x11
   37b10:	b.eq	37af4 <__gmpn_matrix22_mul@@Base+0xa38>  // b.none
   37b14:	b.ls	37b64 <__gmpn_matrix22_mul@@Base+0xaa8>  // b.plast
   37b18:	mov	x0, x20
   37b1c:	mov	x1, x19
   37b20:	mov	x2, x12
   37b24:	mov	x3, x22
   37b28:	bl	c2d0 <__gmpn_sub_n@plt>
   37b2c:	mov	w8, wzr
   37b30:	eor	w26, wzr, w28
   37b34:	cmp	w28, w24
   37b38:	b.ne	37b88 <__gmpn_matrix22_mul@@Base+0xacc>  // b.any
   37b3c:	mov	x0, x19
   37b40:	mov	x1, x19
   37b44:	mov	x2, x27
   37b48:	mov	x3, x22
   37b4c:	bl	ca70 <__gmpn_add_n@plt>
   37b50:	ldr	x8, [sp, #56]
   37b54:	ldp	x28, x0, [x29, #-16]
   37b58:	cmp	x8, x23
   37b5c:	b.lt	37bec <__gmpn_matrix22_mul@@Base+0xb30>  // b.tstop
   37b60:	b	37c2c <__gmpn_matrix22_mul@@Base+0xb70>
   37b64:	mov	x0, x20
   37b68:	mov	x1, x12
   37b6c:	mov	x2, x19
   37b70:	mov	x3, x22
   37b74:	bl	c2d0 <__gmpn_sub_n@plt>
   37b78:	mov	w8, #0x1                   	// #1
   37b7c:	eor	w26, w8, w28
   37b80:	cmp	w28, w24
   37b84:	b.eq	37b3c <__gmpn_matrix22_mul@@Base+0xa80>  // b.none
   37b88:	ldur	x10, [x29, #-32]
   37b8c:	add	x8, x23, x21
   37b90:	mov	w9, #0x18                  	// #24
   37b94:	madd	x9, x8, x9, x10
   37b98:	add	x9, x9, #0x18
   37b9c:	add	x10, x8, #0x1
   37ba0:	cmp	x10, #0x1
   37ba4:	b.lt	37bc0 <__gmpn_matrix22_mul@@Base+0xb04>  // b.tstop
   37ba8:	ldr	x10, [x19, x8, lsl #3]
   37bac:	ldr	x11, [x9], #-8
   37bb0:	sub	x8, x8, #0x1
   37bb4:	cmp	x10, x11
   37bb8:	b.eq	37b9c <__gmpn_matrix22_mul@@Base+0xae0>  // b.none
   37bbc:	b.ls	37c00 <__gmpn_matrix22_mul@@Base+0xb44>  // b.plast
   37bc0:	mov	x0, x19
   37bc4:	mov	x1, x19
   37bc8:	mov	x2, x27
   37bcc:	mov	x3, x22
   37bd0:	bl	c2d0 <__gmpn_sub_n@plt>
   37bd4:	mov	w8, wzr
   37bd8:	eor	w24, wzr, w28
   37bdc:	ldr	x8, [sp, #56]
   37be0:	ldp	x28, x0, [x29, #-16]
   37be4:	cmp	x8, x23
   37be8:	b.ge	37c2c <__gmpn_matrix22_mul@@Base+0xb70>  // b.tcont
   37bec:	ldur	x3, [x29, #-32]
   37bf0:	mov	x1, x28
   37bf4:	mov	x2, x23
   37bf8:	mov	x4, x8
   37bfc:	b	37c3c <__gmpn_matrix22_mul@@Base+0xb80>
   37c00:	mov	x0, x19
   37c04:	mov	x1, x27
   37c08:	mov	x2, x19
   37c0c:	mov	x3, x22
   37c10:	bl	c2d0 <__gmpn_sub_n@plt>
   37c14:	mov	w8, #0x1                   	// #1
   37c18:	eor	w24, w8, w28
   37c1c:	ldr	x8, [sp, #56]
   37c20:	ldp	x28, x0, [x29, #-16]
   37c24:	cmp	x8, x23
   37c28:	b.lt	37bec <__gmpn_matrix22_mul@@Base+0xb30>  // b.tstop
   37c2c:	ldur	x1, [x29, #-32]
   37c30:	mov	x2, x8
   37c34:	mov	x3, x28
   37c38:	mov	x4, x23
   37c3c:	bl	ccd0 <__gmpn_mul@plt>
   37c40:	ldur	x0, [x29, #-24]
   37c44:	ldr	x1, [sp, #40]
   37c48:	mov	x2, x28
   37c4c:	mov	x3, x23
   37c50:	mov	x28, x0
   37c54:	bl	ca70 <__gmpn_add_n@plt>
   37c58:	cmp	x21, x23
   37c5c:	str	x0, [x28, x23, lsl #3]
   37c60:	mov	x0, x27
   37c64:	b.ge	37c7c <__gmpn_matrix22_mul@@Base+0xbc0>  // b.tcont
   37c68:	ldr	x2, [sp, #32]
   37c6c:	ldr	x4, [sp, #56]
   37c70:	mov	x1, x28
   37c74:	mov	x3, x25
   37c78:	b	37c8c <__gmpn_matrix22_mul@@Base+0xbd0>
   37c7c:	ldr	x2, [sp, #56]
   37c80:	ldr	x4, [sp, #32]
   37c84:	mov	x1, x25
   37c88:	mov	x3, x28
   37c8c:	bl	ccd0 <__gmpn_mul@plt>
   37c90:	ldr	w8, [sp, #12]
   37c94:	ldur	x2, [x29, #-8]
   37c98:	cmp	w24, w8
   37c9c:	b.ne	37cb4 <__gmpn_matrix22_mul@@Base+0xbf8>  // b.any
   37ca0:	mov	x0, x25
   37ca4:	mov	x1, x19
   37ca8:	mov	x3, x22
   37cac:	bl	ca70 <__gmpn_add_n@plt>
   37cb0:	b	37d08 <__gmpn_matrix22_mul@@Base+0xc4c>
   37cb4:	ldur	x9, [x29, #-32]
   37cb8:	add	x8, x23, x21
   37cbc:	add	x9, x9, x8, lsl #4
   37cc0:	add	x9, x9, #0x10
   37cc4:	add	x10, x8, #0x1
   37cc8:	cmp	x10, #0x1
   37ccc:	b.lt	37ce8 <__gmpn_matrix22_mul@@Base+0xc2c>  // b.tstop
   37cd0:	ldr	x10, [x19, x8, lsl #3]
   37cd4:	ldr	x11, [x9], #-8
   37cd8:	sub	x8, x8, #0x1
   37cdc:	cmp	x10, x11
   37ce0:	b.eq	37cc4 <__gmpn_matrix22_mul@@Base+0xc08>  // b.none
   37ce4:	b.ls	37cf4 <__gmpn_matrix22_mul@@Base+0xc38>  // b.plast
   37ce8:	mov	x0, x25
   37cec:	mov	x1, x19
   37cf0:	b	37d00 <__gmpn_matrix22_mul@@Base+0xc44>
   37cf4:	mov	x0, x25
   37cf8:	mov	x1, x2
   37cfc:	mov	x2, x19
   37d00:	mov	x3, x22
   37d04:	bl	c2d0 <__gmpn_sub_n@plt>
   37d08:	mov	x0, x19
   37d0c:	mov	x1, x27
   37d10:	mov	x2, x19
   37d14:	mov	x3, x22
   37d18:	cbz	w24, 37d24 <__gmpn_matrix22_mul@@Base+0xc68>
   37d1c:	bl	ca70 <__gmpn_add_n@plt>
   37d20:	b	37d28 <__gmpn_matrix22_mul@@Base+0xc6c>
   37d24:	bl	c2d0 <__gmpn_sub_n@plt>
   37d28:	mov	x0, x20
   37d2c:	mov	x1, x27
   37d30:	mov	x2, x20
   37d34:	mov	x3, x22
   37d38:	cbz	w26, 37d5c <__gmpn_matrix22_mul@@Base+0xca0>
   37d3c:	ldp	x20, x19, [sp, #192]
   37d40:	ldp	x22, x21, [sp, #176]
   37d44:	ldp	x24, x23, [sp, #160]
   37d48:	ldp	x26, x25, [sp, #144]
   37d4c:	ldp	x28, x27, [sp, #128]
   37d50:	ldp	x29, x30, [sp, #112]
   37d54:	add	sp, sp, #0xd0
   37d58:	b	ca70 <__gmpn_add_n@plt>
   37d5c:	ldp	x20, x19, [sp, #192]
   37d60:	ldp	x22, x21, [sp, #176]
   37d64:	ldp	x24, x23, [sp, #160]
   37d68:	ldp	x26, x25, [sp, #144]
   37d6c:	ldp	x28, x27, [sp, #128]
   37d70:	ldp	x29, x30, [sp, #112]
   37d74:	add	sp, sp, #0xd0
   37d78:	b	c2d0 <__gmpn_sub_n@plt>

0000000000037d7c <__gmpn_matrix22_mul1_inverse_vector@@Base>:
   37d7c:	stp	x29, x30, [sp, #-64]!
   37d80:	stp	x22, x21, [sp, #32]
   37d84:	stp	x20, x19, [sp, #48]
   37d88:	mov	x20, x3
   37d8c:	ldr	x3, [x0, #24]
   37d90:	str	x23, [sp, #16]
   37d94:	mov	x21, x2
   37d98:	mov	x22, x0
   37d9c:	mov	x23, x1
   37da0:	mov	x0, x1
   37da4:	mov	x1, x2
   37da8:	mov	x2, x4
   37dac:	mov	x29, sp
   37db0:	mov	x19, x4
   37db4:	bl	d490 <__gmpn_mul_1@plt>
   37db8:	ldr	x3, [x22, #8]
   37dbc:	mov	x0, x23
   37dc0:	mov	x1, x20
   37dc4:	mov	x2, x19
   37dc8:	bl	c9e0 <__gmpn_submul_1@plt>
   37dcc:	ldr	x3, [x22]
   37dd0:	mov	x0, x20
   37dd4:	mov	x1, x20
   37dd8:	mov	x2, x19
   37ddc:	bl	d490 <__gmpn_mul_1@plt>
   37de0:	ldr	x3, [x22, #16]
   37de4:	mov	x0, x20
   37de8:	mov	x1, x21
   37dec:	mov	x2, x19
   37df0:	bl	c9e0 <__gmpn_submul_1@plt>
   37df4:	lsl	x8, x19, #3
   37df8:	sub	x8, x8, #0x8
   37dfc:	ldr	x9, [x23, x8]
   37e00:	ldr	x8, [x20, x8]
   37e04:	ldp	x22, x21, [sp, #32]
   37e08:	ldr	x23, [sp, #16]
   37e0c:	orr	x8, x8, x9
   37e10:	cmp	x8, #0x0
   37e14:	cset	w8, eq  // eq = none
   37e18:	sub	x0, x19, x8
   37e1c:	ldp	x20, x19, [sp, #48]
   37e20:	ldp	x29, x30, [sp], #64
   37e24:	ret

0000000000037e28 <__gmpn_hgcd_matrix_init@@Base>:
   37e28:	stp	x29, x30, [sp, #-48]!
   37e2c:	add	x8, x1, #0x1
   37e30:	add	x9, x1, #0x2
   37e34:	cmp	x8, #0x0
   37e38:	csinc	x8, x9, x1, lt  // lt = tstop
   37e3c:	asr	x8, x8, #1
   37e40:	str	x21, [sp, #16]
   37e44:	stp	x20, x19, [sp, #32]
   37e48:	mov	x19, x2
   37e4c:	mov	x20, x0
   37e50:	mov	w10, #0x1                   	// #1
   37e54:	adds	x21, x8, #0x1
   37e58:	mov	x29, sp
   37e5c:	stp	x21, x10, [x0]
   37e60:	b.cs	37e78 <__gmpn_hgcd_matrix_init@@Base+0x50>  // b.hs, b.nlast
   37e64:	lsl	x8, x8, #5
   37e68:	add	x2, x8, #0x20
   37e6c:	mov	x0, x19
   37e70:	mov	w1, wzr
   37e74:	bl	c5f0 <memset@plt>
   37e78:	add	x8, x19, x21, lsl #3
   37e7c:	mov	w10, #0x18                  	// #24
   37e80:	stp	x19, x8, [x20, #16]
   37e84:	mul	x8, x21, x10
   37e88:	add	x9, x19, x21, lsl #4
   37e8c:	mov	w11, #0x1                   	// #1
   37e90:	add	x10, x19, x8
   37e94:	stp	x9, x10, [x20, #32]
   37e98:	str	x11, [x19, x8]
   37e9c:	str	x11, [x19]
   37ea0:	ldp	x20, x19, [sp, #32]
   37ea4:	ldr	x21, [sp, #16]
   37ea8:	ldp	x29, x30, [sp], #48
   37eac:	ret

0000000000037eb0 <__gmpn_hgcd_matrix_update_q@@Base>:
   37eb0:	sub	sp, sp, #0x70
   37eb4:	stp	x24, x23, [sp, #64]
   37eb8:	stp	x20, x19, [sp, #96]
   37ebc:	mov	x23, x1
   37ec0:	cmp	x2, #0x1
   37ec4:	mov	x19, x0
   37ec8:	stp	x29, x30, [sp, #16]
   37ecc:	stp	x28, x27, [sp, #32]
   37ed0:	stp	x26, x25, [sp, #48]
   37ed4:	stp	x22, x21, [sp, #80]
   37ed8:	add	x29, sp, #0x10
   37edc:	b.ne	37f60 <__gmpn_hgcd_matrix_update_q@@Base+0xb0>  // b.any
   37ee0:	ldr	x20, [x23]
   37ee4:	ldr	x2, [x19, #8]!
   37ee8:	mov	w8, w3
   37eec:	mov	w9, #0x1                   	// #1
   37ef0:	lsl	x22, x8, #3
   37ef4:	sub	w8, w9, w3
   37ef8:	lsl	x21, x8, #3
   37efc:	add	x23, x19, #0x8
   37f00:	ldr	x0, [x23, x22]
   37f04:	ldr	x1, [x23, x21]
   37f08:	mov	x3, x20
   37f0c:	bl	d400 <__gmpn_addmul_1@plt>
   37f10:	add	x24, x19, #0x18
   37f14:	ldr	x8, [x24, x22]
   37f18:	ldr	x1, [x24, x21]
   37f1c:	ldr	x2, [x19]
   37f20:	mov	x21, x0
   37f24:	mov	x0, x8
   37f28:	mov	x3, x20
   37f2c:	bl	d400 <__gmpn_addmul_1@plt>
   37f30:	ldr	x8, [x23, x22]
   37f34:	ldr	x9, [x19]
   37f38:	mov	x12, x19
   37f3c:	str	x21, [x8, x9, lsl #3]
   37f40:	ldr	x8, [x24, x22]
   37f44:	ldr	x9, [x19]
   37f48:	str	x0, [x8, x9, lsl #3]
   37f4c:	ldr	x8, [x19]
   37f50:	orr	x9, x0, x21
   37f54:	cmp	x9, #0x0
   37f58:	cinc	x8, x8, ne  // ne = any
   37f5c:	b	3849c <__gmpn_hgcd_matrix_update_q@@Base+0x5ec>
   37f60:	mov	x13, x19
   37f64:	ldr	x9, [x13, #8]!
   37f68:	mov	w8, #0x1                   	// #1
   37f6c:	sub	w8, w8, w3
   37f70:	add	x10, x19, w8, uxtw #3
   37f74:	mov	x22, x4
   37f78:	mov	x20, x2
   37f7c:	add	x8, x10, #0x10
   37f80:	add	x26, x10, #0x20
   37f84:	lsl	x10, x9, #3
   37f88:	mov	x11, x9
   37f8c:	add	x28, x20, x11
   37f90:	mov	x21, x11
   37f94:	cmp	x28, x9
   37f98:	mov	x27, x10
   37f9c:	b.le	37fc8 <__gmpn_hgcd_matrix_update_q@@Base+0x118>
   37fa0:	ldr	x10, [x8]
   37fa4:	add	x10, x10, x21, lsl #3
   37fa8:	ldur	x10, [x10, #-8]
   37fac:	cbnz	x10, 37fc8 <__gmpn_hgcd_matrix_update_q@@Base+0x118>
   37fb0:	ldr	x10, [x26]
   37fb4:	sub	x11, x21, #0x1
   37fb8:	add	x10, x10, x21, lsl #3
   37fbc:	ldur	x12, [x10, #-8]
   37fc0:	sub	x10, x27, #0x8
   37fc4:	cbz	x12, 37f8c <__gmpn_hgcd_matrix_update_q@@Base+0xdc>
   37fc8:	ldr	x8, [x8]
   37fcc:	cmp	x21, x20
   37fd0:	mov	w24, w3
   37fd4:	mov	x0, x22
   37fd8:	stp	x13, x24, [sp]
   37fdc:	b.ge	38210 <__gmpn_hgcd_matrix_update_q@@Base+0x360>  // b.tcont
   37fe0:	mov	x1, x23
   37fe4:	mov	x2, x20
   37fe8:	mov	x3, x8
   37fec:	mov	x4, x21
   37ff0:	bl	ccd0 <__gmpn_mul@plt>
   37ff4:	add	x8, x19, x24, lsl #3
   37ff8:	ldr	x25, [x19, #8]
   37ffc:	ldr	x24, [x8, #16]
   38000:	cbz	x25, 38040 <__gmpn_hgcd_matrix_update_q@@Base+0x190>
   38004:	mov	x0, x24
   38008:	mov	x1, x22
   3800c:	mov	x2, x24
   38010:	mov	x3, x25
   38014:	bl	ca70 <__gmpn_add_n@plt>
   38018:	cbz	x0, 38040 <__gmpn_hgcd_matrix_update_q@@Base+0x190>
   3801c:	mov	w10, #0x1                   	// #1
   38020:	cmp	x25, x28
   38024:	b.ge	380ec <__gmpn_hgcd_matrix_update_q@@Base+0x23c>  // b.tcont
   38028:	lsl	x8, x25, #3
   3802c:	ldr	x9, [x22, x8]
   38030:	add	x25, x25, #0x1
   38034:	adds	x9, x9, #0x1
   38038:	str	x9, [x24, x8]
   3803c:	b.cs	38020 <__gmpn_hgcd_matrix_update_q@@Base+0x170>  // b.hs, b.nlast
   38040:	cmp	x24, x22
   38044:	mov	x10, xzr
   38048:	b.eq	380ec <__gmpn_hgcd_matrix_update_q@@Base+0x23c>  // b.none
   3804c:	cmp	x28, x25
   38050:	b.le	380ec <__gmpn_hgcd_matrix_update_q@@Base+0x23c>
   38054:	sub	x10, x20, x25
   38058:	add	x8, x10, x21
   3805c:	cmp	x8, #0x4
   38060:	b.cc	380d0 <__gmpn_hgcd_matrix_update_q@@Base+0x220>  // b.lo, b.ul, b.last
   38064:	lsl	x9, x20, #3
   38068:	lsl	x11, x25, #3
   3806c:	add	x13, x22, x9
   38070:	add	x12, x24, x11
   38074:	add	x13, x13, x27
   38078:	cmp	x12, x13
   3807c:	b.cs	38094 <__gmpn_hgcd_matrix_update_q@@Base+0x1e4>  // b.hs, b.nlast
   38080:	add	x9, x24, x9
   38084:	add	x9, x9, x27
   38088:	add	x12, x22, x11
   3808c:	cmp	x12, x9
   38090:	b.cc	380d0 <__gmpn_hgcd_matrix_update_q@@Base+0x220>  // b.lo, b.ul, b.last
   38094:	and	x9, x8, #0xfffffffffffffffc
   38098:	add	x11, x11, #0x10
   3809c:	add	x12, x10, x21
   380a0:	add	x25, x25, x9
   380a4:	add	x10, x22, x11
   380a8:	add	x11, x24, x11
   380ac:	and	x12, x12, #0xfffffffffffffffc
   380b0:	ldp	q0, q1, [x10, #-16]
   380b4:	add	x10, x10, #0x20
   380b8:	subs	x12, x12, #0x4
   380bc:	stp	q0, q1, [x11, #-16]
   380c0:	add	x11, x11, #0x20
   380c4:	b.ne	380b0 <__gmpn_hgcd_matrix_update_q@@Base+0x200>  // b.any
   380c8:	cmp	x8, x9
   380cc:	b.eq	380e8 <__gmpn_hgcd_matrix_update_q@@Base+0x238>  // b.none
   380d0:	lsl	x8, x25, #3
   380d4:	ldr	x9, [x22, x8]
   380d8:	add	x25, x25, #0x1
   380dc:	cmp	x28, x25
   380e0:	str	x9, [x24, x8]
   380e4:	b.ne	380d0 <__gmpn_hgcd_matrix_update_q@@Base+0x220>  // b.any
   380e8:	mov	x10, xzr
   380ec:	ldr	x3, [x26]
   380f0:	mov	x0, x22
   380f4:	mov	x1, x23
   380f8:	mov	x2, x20
   380fc:	mov	x4, x21
   38100:	mov	x25, x10
   38104:	bl	ccd0 <__gmpn_mul@plt>
   38108:	ldr	x26, [sp, #8]
   3810c:	ldr	x24, [x19, #8]
   38110:	add	x8, x19, x26, lsl #3
   38114:	ldr	x23, [x8, #32]
   38118:	cbz	x24, 38158 <__gmpn_hgcd_matrix_update_q@@Base+0x2a8>
   3811c:	mov	x0, x23
   38120:	mov	x1, x22
   38124:	mov	x2, x23
   38128:	mov	x3, x24
   3812c:	bl	ca70 <__gmpn_add_n@plt>
   38130:	cbz	x0, 38158 <__gmpn_hgcd_matrix_update_q@@Base+0x2a8>
   38134:	mov	w8, #0x1                   	// #1
   38138:	cmp	x24, x28
   3813c:	b.ge	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>  // b.tcont
   38140:	lsl	x9, x24, #3
   38144:	ldr	x10, [x22, x9]
   38148:	add	x24, x24, #0x1
   3814c:	adds	x10, x10, #0x1
   38150:	str	x10, [x23, x9]
   38154:	b.cs	38138 <__gmpn_hgcd_matrix_update_q@@Base+0x288>  // b.hs, b.nlast
   38158:	cmp	x23, x22
   3815c:	mov	x8, xzr
   38160:	b.eq	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>  // b.none
   38164:	cmp	x28, x24
   38168:	b.le	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>
   3816c:	sub	x10, x20, x24
   38170:	add	x8, x10, x21
   38174:	cmp	x8, #0x4
   38178:	b.cc	381e8 <__gmpn_hgcd_matrix_update_q@@Base+0x338>  // b.lo, b.ul, b.last
   3817c:	lsl	x9, x20, #3
   38180:	lsl	x11, x24, #3
   38184:	add	x13, x22, x9
   38188:	add	x12, x23, x11
   3818c:	add	x13, x13, x27
   38190:	cmp	x12, x13
   38194:	b.cs	381ac <__gmpn_hgcd_matrix_update_q@@Base+0x2fc>  // b.hs, b.nlast
   38198:	add	x9, x23, x9
   3819c:	add	x9, x9, x27
   381a0:	add	x12, x22, x11
   381a4:	cmp	x12, x9
   381a8:	b.cc	381e8 <__gmpn_hgcd_matrix_update_q@@Base+0x338>  // b.lo, b.ul, b.last
   381ac:	and	x9, x8, #0xfffffffffffffffc
   381b0:	add	x11, x11, #0x10
   381b4:	add	x12, x10, x21
   381b8:	add	x24, x24, x9
   381bc:	add	x10, x22, x11
   381c0:	add	x11, x23, x11
   381c4:	and	x12, x12, #0xfffffffffffffffc
   381c8:	ldp	q0, q1, [x10, #-16]
   381cc:	add	x10, x10, #0x20
   381d0:	subs	x12, x12, #0x4
   381d4:	stp	q0, q1, [x11, #-16]
   381d8:	add	x11, x11, #0x20
   381dc:	b.ne	381c8 <__gmpn_hgcd_matrix_update_q@@Base+0x318>  // b.any
   381e0:	cmp	x8, x9
   381e4:	b.eq	38430 <__gmpn_hgcd_matrix_update_q@@Base+0x580>  // b.none
   381e8:	lsl	x10, x24, #3
   381ec:	sub	x8, x24, x20
   381f0:	add	x9, x23, x10
   381f4:	add	x10, x22, x10
   381f8:	ldr	x11, [x10], #8
   381fc:	add	x8, x8, #0x1
   38200:	cmp	x21, x8
   38204:	str	x11, [x9], #8
   38208:	b.ne	381f8 <__gmpn_hgcd_matrix_update_q@@Base+0x348>  // b.any
   3820c:	b	38430 <__gmpn_hgcd_matrix_update_q@@Base+0x580>
   38210:	mov	x1, x8
   38214:	mov	x2, x21
   38218:	mov	x3, x23
   3821c:	mov	x4, x20
   38220:	bl	ccd0 <__gmpn_mul@plt>
   38224:	add	x8, x19, x24, lsl #3
   38228:	ldr	x25, [x19, #8]
   3822c:	ldr	x24, [x8, #16]
   38230:	cbz	x25, 38270 <__gmpn_hgcd_matrix_update_q@@Base+0x3c0>
   38234:	mov	x0, x24
   38238:	mov	x1, x22
   3823c:	mov	x2, x24
   38240:	mov	x3, x25
   38244:	bl	ca70 <__gmpn_add_n@plt>
   38248:	cbz	x0, 38270 <__gmpn_hgcd_matrix_update_q@@Base+0x3c0>
   3824c:	mov	w10, #0x1                   	// #1
   38250:	cmp	x25, x28
   38254:	b.ge	3831c <__gmpn_hgcd_matrix_update_q@@Base+0x46c>  // b.tcont
   38258:	lsl	x8, x25, #3
   3825c:	ldr	x9, [x22, x8]
   38260:	add	x25, x25, #0x1
   38264:	adds	x9, x9, #0x1
   38268:	str	x9, [x24, x8]
   3826c:	b.cs	38250 <__gmpn_hgcd_matrix_update_q@@Base+0x3a0>  // b.hs, b.nlast
   38270:	cmp	x24, x22
   38274:	mov	x10, xzr
   38278:	b.eq	3831c <__gmpn_hgcd_matrix_update_q@@Base+0x46c>  // b.none
   3827c:	cmp	x28, x25
   38280:	b.le	3831c <__gmpn_hgcd_matrix_update_q@@Base+0x46c>
   38284:	sub	x10, x20, x25
   38288:	add	x8, x10, x21
   3828c:	cmp	x8, #0x4
   38290:	b.cc	38300 <__gmpn_hgcd_matrix_update_q@@Base+0x450>  // b.lo, b.ul, b.last
   38294:	lsl	x9, x20, #3
   38298:	lsl	x11, x25, #3
   3829c:	add	x13, x22, x9
   382a0:	add	x12, x24, x11
   382a4:	add	x13, x13, x27
   382a8:	cmp	x12, x13
   382ac:	b.cs	382c4 <__gmpn_hgcd_matrix_update_q@@Base+0x414>  // b.hs, b.nlast
   382b0:	add	x9, x24, x9
   382b4:	add	x9, x9, x27
   382b8:	add	x12, x22, x11
   382bc:	cmp	x12, x9
   382c0:	b.cc	38300 <__gmpn_hgcd_matrix_update_q@@Base+0x450>  // b.lo, b.ul, b.last
   382c4:	and	x9, x8, #0xfffffffffffffffc
   382c8:	add	x11, x11, #0x10
   382cc:	add	x12, x10, x21
   382d0:	add	x25, x25, x9
   382d4:	add	x10, x22, x11
   382d8:	add	x11, x24, x11
   382dc:	and	x12, x12, #0xfffffffffffffffc
   382e0:	ldp	q0, q1, [x10, #-16]
   382e4:	add	x10, x10, #0x20
   382e8:	subs	x12, x12, #0x4
   382ec:	stp	q0, q1, [x11, #-16]
   382f0:	add	x11, x11, #0x20
   382f4:	b.ne	382e0 <__gmpn_hgcd_matrix_update_q@@Base+0x430>  // b.any
   382f8:	cmp	x8, x9
   382fc:	b.eq	38318 <__gmpn_hgcd_matrix_update_q@@Base+0x468>  // b.none
   38300:	lsl	x8, x25, #3
   38304:	ldr	x9, [x22, x8]
   38308:	add	x25, x25, #0x1
   3830c:	cmp	x28, x25
   38310:	str	x9, [x24, x8]
   38314:	b.ne	38300 <__gmpn_hgcd_matrix_update_q@@Base+0x450>  // b.any
   38318:	mov	x10, xzr
   3831c:	ldr	x1, [x26]
   38320:	mov	x0, x22
   38324:	mov	x2, x21
   38328:	mov	x3, x23
   3832c:	mov	x4, x20
   38330:	mov	x25, x10
   38334:	bl	ccd0 <__gmpn_mul@plt>
   38338:	ldr	x26, [sp, #8]
   3833c:	ldr	x24, [x19, #8]
   38340:	add	x8, x19, x26, lsl #3
   38344:	ldr	x23, [x8, #32]
   38348:	cbz	x24, 38388 <__gmpn_hgcd_matrix_update_q@@Base+0x4d8>
   3834c:	mov	x0, x23
   38350:	mov	x1, x22
   38354:	mov	x2, x23
   38358:	mov	x3, x24
   3835c:	bl	ca70 <__gmpn_add_n@plt>
   38360:	cbz	x0, 38388 <__gmpn_hgcd_matrix_update_q@@Base+0x4d8>
   38364:	mov	w8, #0x1                   	// #1
   38368:	cmp	x24, x28
   3836c:	b.ge	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>  // b.tcont
   38370:	lsl	x9, x24, #3
   38374:	ldr	x10, [x22, x9]
   38378:	add	x24, x24, #0x1
   3837c:	adds	x10, x10, #0x1
   38380:	str	x10, [x23, x9]
   38384:	b.cs	38368 <__gmpn_hgcd_matrix_update_q@@Base+0x4b8>  // b.hs, b.nlast
   38388:	cmp	x23, x22
   3838c:	mov	x8, xzr
   38390:	b.eq	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>  // b.none
   38394:	cmp	x28, x24
   38398:	b.le	38434 <__gmpn_hgcd_matrix_update_q@@Base+0x584>
   3839c:	sub	x10, x20, x24
   383a0:	add	x8, x10, x21
   383a4:	cmp	x8, #0x4
   383a8:	b.cc	38418 <__gmpn_hgcd_matrix_update_q@@Base+0x568>  // b.lo, b.ul, b.last
   383ac:	lsl	x9, x20, #3
   383b0:	lsl	x11, x24, #3
   383b4:	add	x13, x22, x9
   383b8:	add	x12, x23, x11
   383bc:	add	x13, x13, x27
   383c0:	cmp	x12, x13
   383c4:	b.cs	383dc <__gmpn_hgcd_matrix_update_q@@Base+0x52c>  // b.hs, b.nlast
   383c8:	add	x9, x23, x9
   383cc:	add	x9, x9, x27
   383d0:	add	x12, x22, x11
   383d4:	cmp	x12, x9
   383d8:	b.cc	38418 <__gmpn_hgcd_matrix_update_q@@Base+0x568>  // b.lo, b.ul, b.last
   383dc:	and	x9, x8, #0xfffffffffffffffc
   383e0:	add	x11, x11, #0x10
   383e4:	add	x12, x10, x21
   383e8:	add	x24, x24, x9
   383ec:	add	x10, x22, x11
   383f0:	add	x11, x23, x11
   383f4:	and	x12, x12, #0xfffffffffffffffc
   383f8:	ldp	q0, q1, [x10, #-16]
   383fc:	add	x10, x10, #0x20
   38400:	subs	x12, x12, #0x4
   38404:	stp	q0, q1, [x11, #-16]
   38408:	add	x11, x11, #0x20
   3840c:	b.ne	383f8 <__gmpn_hgcd_matrix_update_q@@Base+0x548>  // b.any
   38410:	cmp	x8, x9
   38414:	b.eq	38430 <__gmpn_hgcd_matrix_update_q@@Base+0x580>  // b.none
   38418:	lsl	x8, x24, #3
   3841c:	ldr	x9, [x22, x8]
   38420:	add	x24, x24, #0x1
   38424:	cmp	x28, x24
   38428:	str	x9, [x23, x8]
   3842c:	b.ne	38418 <__gmpn_hgcd_matrix_update_q@@Base+0x568>  // b.any
   38430:	mov	x8, xzr
   38434:	add	x9, x19, x26, lsl #3
   38438:	ldr	x12, [sp]
   3843c:	ldr	x10, [x9, #16]
   38440:	orr	x11, x8, x25
   38444:	cbz	x11, 38468 <__gmpn_hgcd_matrix_update_q@@Base+0x5b8>
   38448:	lsl	x11, x20, #3
   3844c:	add	x10, x10, x11
   38450:	str	x25, [x10, x27]
   38454:	ldr	x9, [x9, #32]
   38458:	add	x9, x9, x11
   3845c:	str	x8, [x9, x21, lsl #3]
   38460:	mov	w8, #0x1                   	// #1
   38464:	b	38494 <__gmpn_hgcd_matrix_update_q@@Base+0x5e4>
   38468:	ldr	x9, [x9, #32]
   3846c:	lsl	x8, x20, #3
   38470:	add	x10, x10, x8
   38474:	add	x10, x10, x27
   38478:	add	x8, x9, x8
   3847c:	add	x8, x8, x21, lsl #3
   38480:	ldur	x10, [x10, #-8]
   38484:	ldur	x8, [x8, #-8]
   38488:	orr	x8, x8, x10
   3848c:	cmp	x8, #0x0
   38490:	csetm	x8, eq  // eq = none
   38494:	add	x8, x8, x20
   38498:	add	x8, x8, x21
   3849c:	str	x8, [x12]
   384a0:	ldp	x20, x19, [sp, #96]
   384a4:	ldp	x22, x21, [sp, #80]
   384a8:	ldp	x24, x23, [sp, #64]
   384ac:	ldp	x26, x25, [sp, #48]
   384b0:	ldp	x28, x27, [sp, #32]
   384b4:	ldp	x29, x30, [sp, #16]
   384b8:	add	sp, sp, #0x70
   384bc:	ret

00000000000384c0 <__gmpn_hgcd_matrix_mul_1@@Base>:
   384c0:	stp	x29, x30, [sp, #-48]!
   384c4:	stp	x22, x21, [sp, #16]
   384c8:	stp	x20, x19, [sp, #32]
   384cc:	mov	x19, x2
   384d0:	ldp	x2, x8, [x0, #8]
   384d4:	mov	x20, x0
   384d8:	mov	x21, x1
   384dc:	mov	x0, x19
   384e0:	mov	x1, x8
   384e4:	mov	x29, sp
   384e8:	bl	ca50 <__gmpn_copyi@plt>
   384ec:	ldp	x1, x3, [x20, #16]
   384f0:	ldr	x4, [x20, #8]
   384f4:	mov	x0, x21
   384f8:	mov	x2, x19
   384fc:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   38500:	ldr	x1, [x20, #32]
   38504:	ldr	x2, [x20, #8]
   38508:	mov	x22, x0
   3850c:	mov	x0, x19
   38510:	bl	ca50 <__gmpn_copyi@plt>
   38514:	ldp	x1, x3, [x20, #32]
   38518:	ldr	x4, [x20, #8]
   3851c:	mov	x0, x21
   38520:	mov	x2, x19
   38524:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   38528:	cmp	x22, x0
   3852c:	csel	x8, x22, x0, gt
   38530:	str	x8, [x20, #8]
   38534:	ldp	x20, x19, [sp, #32]
   38538:	ldp	x22, x21, [sp, #16]
   3853c:	ldp	x29, x30, [sp], #48
   38540:	ret

0000000000038544 <__gmpn_hgcd_matrix_mul@@Base>:
   38544:	sub	sp, sp, #0x40
   38548:	stp	x29, x30, [sp, #32]
   3854c:	stp	x20, x19, [sp, #48]
   38550:	mov	x20, x1
   38554:	mov	x19, x0
   38558:	ldp	x1, x8, [x0, #24]
   3855c:	ldp	x10, x5, [x20, #8]
   38560:	ldr	x3, [x0, #40]
   38564:	ldr	x0, [x0, #16]
   38568:	ldr	x4, [x19, #8]
   3856c:	ldp	x6, x7, [x20, #24]
   38570:	ldr	x9, [x20, #40]
   38574:	stp	x10, x2, [sp, #8]
   38578:	mov	x2, x8
   3857c:	add	x29, sp, #0x20
   38580:	str	x9, [sp]
   38584:	bl	c010 <__gmpn_matrix22_mul@plt>
   38588:	ldr	x8, [x20, #8]
   3858c:	ldp	x9, x10, [x19, #8]
   38590:	ldp	x11, x12, [x19, #24]
   38594:	ldr	x13, [x19, #40]
   38598:	add	x8, x8, x9
   3859c:	lsl	x9, x8, #3
   385a0:	ldr	x14, [x10, x9]
   385a4:	ldr	x15, [x11, x9]
   385a8:	ldr	x16, [x12, x9]
   385ac:	ldr	x9, [x13, x9]
   385b0:	orr	x14, x15, x14
   385b4:	orr	x14, x14, x16
   385b8:	orr	x9, x14, x9
   385bc:	cmp	x9, #0x0
   385c0:	cset	w9, eq  // eq = none
   385c4:	sub	x8, x8, x9
   385c8:	lsl	x9, x8, #3
   385cc:	ldr	x14, [x10, x9]
   385d0:	ldr	x15, [x11, x9]
   385d4:	ldr	x16, [x12, x9]
   385d8:	ldr	x9, [x13, x9]
   385dc:	orr	x14, x15, x14
   385e0:	orr	x14, x14, x16
   385e4:	orr	x9, x14, x9
   385e8:	cmp	x9, #0x0
   385ec:	cset	w9, eq  // eq = none
   385f0:	sub	x8, x8, x9
   385f4:	lsl	x9, x8, #3
   385f8:	ldr	x10, [x10, x9]
   385fc:	ldr	x11, [x11, x9]
   38600:	ldr	x12, [x12, x9]
   38604:	ldr	x9, [x13, x9]
   38608:	orr	x10, x11, x10
   3860c:	orr	x10, x10, x12
   38610:	orr	x9, x10, x9
   38614:	cmp	x9, #0x0
   38618:	cset	w9, eq  // eq = none
   3861c:	sub	x8, x8, x9
   38620:	add	x8, x8, #0x1
   38624:	str	x8, [x19, #8]
   38628:	ldp	x20, x19, [sp, #48]
   3862c:	ldp	x29, x30, [sp, #32]
   38630:	add	sp, sp, #0x40
   38634:	ret

0000000000038638 <__gmpn_hgcd_matrix_adjust@@Base>:
   38638:	sub	sp, sp, #0x70
   3863c:	stp	x29, x30, [sp, #16]
   38640:	stp	x28, x27, [sp, #32]
   38644:	stp	x26, x25, [sp, #48]
   38648:	stp	x24, x23, [sp, #64]
   3864c:	stp	x22, x21, [sp, #80]
   38650:	stp	x20, x19, [sp, #96]
   38654:	mov	x22, x4
   38658:	ldr	x4, [x0, #8]
   3865c:	mov	x20, x3
   38660:	ldr	x3, [x0, #40]
   38664:	add	x25, x5, x22, lsl #3
   38668:	mov	x26, x5
   3866c:	mov	x21, x2
   38670:	mov	x19, x1
   38674:	mov	x23, x0
   38678:	cmp	x4, x22
   3867c:	add	x24, x25, x4, lsl #3
   38680:	mov	x0, x5
   38684:	add	x29, sp, #0x10
   38688:	str	x24, [sp]
   3868c:	b.ge	386b4 <__gmpn_hgcd_matrix_adjust@@Base+0x7c>  // b.tcont
   38690:	mov	x1, x21
   38694:	mov	x2, x22
   38698:	bl	ccd0 <__gmpn_mul@plt>
   3869c:	ldr	x3, [x23, #32]
   386a0:	ldr	x4, [x23, #8]
   386a4:	mov	x0, x24
   386a8:	mov	x1, x21
   386ac:	mov	x2, x22
   386b0:	b	386dc <__gmpn_hgcd_matrix_adjust@@Base+0xa4>
   386b4:	mov	x1, x3
   386b8:	mov	x2, x4
   386bc:	mov	x3, x21
   386c0:	mov	x4, x22
   386c4:	bl	ccd0 <__gmpn_mul@plt>
   386c8:	ldr	x1, [x23, #32]
   386cc:	ldr	x2, [x23, #8]
   386d0:	mov	x0, x24
   386d4:	mov	x3, x21
   386d8:	mov	x4, x22
   386dc:	bl	ccd0 <__gmpn_mul@plt>
   386e0:	mov	x0, x21
   386e4:	mov	x1, x26
   386e8:	mov	x2, x22
   386ec:	bl	ca50 <__gmpn_copyi@plt>
   386f0:	ldr	x27, [x23, #8]
   386f4:	sub	x24, x19, x22
   386f8:	cbz	x27, 38738 <__gmpn_hgcd_matrix_adjust@@Base+0x100>
   386fc:	add	x28, x21, x22, lsl #3
   38700:	mov	x0, x28
   38704:	mov	x1, x28
   38708:	mov	x2, x25
   3870c:	mov	x3, x27
   38710:	bl	ca70 <__gmpn_add_n@plt>
   38714:	cbz	x0, 38738 <__gmpn_hgcd_matrix_adjust@@Base+0x100>
   38718:	cmp	x27, x24
   3871c:	b.ge	38918 <__gmpn_hgcd_matrix_adjust@@Base+0x2e0>  // b.tcont
   38720:	lsl	x8, x27, #3
   38724:	ldr	x9, [x28, x8]
   38728:	add	x27, x27, #0x1
   3872c:	adds	x9, x9, #0x1
   38730:	str	x9, [x28, x8]
   38734:	b.cs	38718 <__gmpn_hgcd_matrix_adjust@@Base+0xe0>  // b.hs, b.nlast
   38738:	str	xzr, [sp, #8]
   3873c:	ldr	x4, [x23, #8]
   38740:	ldr	x3, [x23, #24]
   38744:	mov	x0, x26
   38748:	cmp	x4, x22
   3874c:	b.ge	3876c <__gmpn_hgcd_matrix_adjust@@Base+0x134>  // b.tcont
   38750:	mov	x1, x20
   38754:	mov	x2, x22
   38758:	bl	ccd0 <__gmpn_mul@plt>
   3875c:	ldr	x8, [x23, #8]
   38760:	adds	x27, x8, x22
   38764:	b.ne	3878c <__gmpn_hgcd_matrix_adjust@@Base+0x154>  // b.any
   38768:	b	387c8 <__gmpn_hgcd_matrix_adjust@@Base+0x190>
   3876c:	mov	x1, x3
   38770:	mov	x2, x4
   38774:	mov	x3, x20
   38778:	mov	x4, x22
   3877c:	bl	ccd0 <__gmpn_mul@plt>
   38780:	ldr	x8, [x23, #8]
   38784:	adds	x27, x8, x22
   38788:	b.eq	387c8 <__gmpn_hgcd_matrix_adjust@@Base+0x190>  // b.none
   3878c:	mov	x0, x21
   38790:	mov	x1, x21
   38794:	mov	x2, x26
   38798:	mov	x3, x27
   3879c:	bl	c2d0 <__gmpn_sub_n@plt>
   387a0:	cbz	x0, 387c8 <__gmpn_hgcd_matrix_adjust@@Base+0x190>
   387a4:	mov	w28, #0x1                   	// #1
   387a8:	cmp	x27, x19
   387ac:	b.ge	387cc <__gmpn_hgcd_matrix_adjust@@Base+0x194>  // b.tcont
   387b0:	lsl	x8, x27, #3
   387b4:	ldr	x9, [x21, x8]
   387b8:	add	x27, x27, #0x1
   387bc:	sub	x10, x9, #0x1
   387c0:	str	x10, [x21, x8]
   387c4:	cbz	x9, 387a8 <__gmpn_hgcd_matrix_adjust@@Base+0x170>
   387c8:	mov	x28, xzr
   387cc:	ldp	x4, x3, [x23, #8]
   387d0:	mov	x0, x26
   387d4:	cmp	x4, x22
   387d8:	b.ge	387e8 <__gmpn_hgcd_matrix_adjust@@Base+0x1b0>  // b.tcont
   387dc:	mov	x1, x20
   387e0:	mov	x2, x22
   387e4:	b	387f8 <__gmpn_hgcd_matrix_adjust@@Base+0x1c0>
   387e8:	mov	x1, x3
   387ec:	mov	x2, x4
   387f0:	mov	x3, x20
   387f4:	mov	x4, x22
   387f8:	bl	ccd0 <__gmpn_mul@plt>
   387fc:	mov	x0, x20
   38800:	mov	x1, x26
   38804:	mov	x2, x22
   38808:	bl	ca50 <__gmpn_copyi@plt>
   3880c:	ldr	x26, [x23, #8]
   38810:	cbz	x26, 38874 <__gmpn_hgcd_matrix_adjust@@Base+0x23c>
   38814:	add	x27, x20, x22, lsl #3
   38818:	mov	x0, x27
   3881c:	mov	x1, x27
   38820:	mov	x2, x25
   38824:	mov	x3, x26
   38828:	bl	ca70 <__gmpn_add_n@plt>
   3882c:	cbz	x0, 38874 <__gmpn_hgcd_matrix_adjust@@Base+0x23c>
   38830:	ldr	x10, [sp, #8]
   38834:	mov	w25, #0x1                   	// #1
   38838:	cmp	x26, x24
   3883c:	b.ge	3885c <__gmpn_hgcd_matrix_adjust@@Base+0x224>  // b.tcont
   38840:	lsl	x8, x26, #3
   38844:	ldr	x9, [x27, x8]
   38848:	add	x26, x26, #0x1
   3884c:	adds	x9, x9, #0x1
   38850:	str	x9, [x27, x8]
   38854:	b.cs	38838 <__gmpn_hgcd_matrix_adjust@@Base+0x200>  // b.hs, b.nlast
   38858:	mov	x25, xzr
   3885c:	ldr	x2, [sp]
   38860:	ldr	x8, [x23, #8]
   38864:	sub	x23, x10, x28
   38868:	adds	x22, x8, x22
   3886c:	b.ne	3888c <__gmpn_hgcd_matrix_adjust@@Base+0x254>  // b.any
   38870:	b	388c4 <__gmpn_hgcd_matrix_adjust@@Base+0x28c>
   38874:	ldp	x2, x10, [sp]
   38878:	mov	x25, xzr
   3887c:	ldr	x8, [x23, #8]
   38880:	sub	x23, x10, x28
   38884:	adds	x22, x8, x22
   38888:	b.eq	388c4 <__gmpn_hgcd_matrix_adjust@@Base+0x28c>  // b.none
   3888c:	mov	x0, x20
   38890:	mov	x1, x20
   38894:	mov	x3, x22
   38898:	bl	c2d0 <__gmpn_sub_n@plt>
   3889c:	cbz	x0, 388c4 <__gmpn_hgcd_matrix_adjust@@Base+0x28c>
   388a0:	mov	w8, #0x1                   	// #1
   388a4:	cmp	x22, x19
   388a8:	b.ge	388c8 <__gmpn_hgcd_matrix_adjust@@Base+0x290>  // b.tcont
   388ac:	lsl	x9, x22, #3
   388b0:	ldr	x10, [x20, x9]
   388b4:	add	x22, x22, #0x1
   388b8:	sub	x11, x10, #0x1
   388bc:	str	x11, [x20, x9]
   388c0:	cbz	x10, 388a4 <__gmpn_hgcd_matrix_adjust@@Base+0x26c>
   388c4:	mov	x8, xzr
   388c8:	sub	x8, x25, x8
   388cc:	orr	x9, x8, x23
   388d0:	cbz	x9, 388e8 <__gmpn_hgcd_matrix_adjust@@Base+0x2b0>
   388d4:	lsl	x9, x19, #3
   388d8:	add	x19, x19, #0x1
   388dc:	str	x23, [x21, x9]
   388e0:	str	x8, [x20, x9]
   388e4:	b	388f4 <__gmpn_hgcd_matrix_adjust@@Base+0x2bc>
   388e8:	sub	x8, x19, #0x1
   388ec:	ldr	x9, [x21, x8, lsl #3]
   388f0:	cbz	x9, 38924 <__gmpn_hgcd_matrix_adjust@@Base+0x2ec>
   388f4:	mov	x0, x19
   388f8:	ldp	x20, x19, [sp, #96]
   388fc:	ldp	x22, x21, [sp, #80]
   38900:	ldp	x24, x23, [sp, #64]
   38904:	ldp	x26, x25, [sp, #48]
   38908:	ldp	x28, x27, [sp, #32]
   3890c:	ldp	x29, x30, [sp, #16]
   38910:	add	sp, sp, #0x70
   38914:	ret
   38918:	mov	w8, #0x1                   	// #1
   3891c:	str	x8, [sp, #8]
   38920:	b	3873c <__gmpn_hgcd_matrix_adjust@@Base+0x104>
   38924:	ldr	x9, [x20, x8, lsl #3]
   38928:	cmp	x9, #0x0
   3892c:	csel	x0, x8, x19, eq  // eq = none
   38930:	b	388f8 <__gmpn_hgcd_matrix_adjust@@Base+0x2c0>

0000000000038934 <__gmpn_hgcd2@@Base>:
   38934:	cmp	x0, #0x2
   38938:	mov	w8, wzr
   3893c:	b.cc	38bfc <__gmpn_hgcd2@@Base+0x2c8>  // b.lo, b.ul, b.last
   38940:	cmp	x2, #0x2
   38944:	b.cc	38bfc <__gmpn_hgcd2@@Base+0x2c8>  // b.lo, b.ul, b.last
   38948:	cmp	x0, x2
   3894c:	b.hi	3895c <__gmpn_hgcd2@@Base+0x28>  // b.pmore
   38950:	b.ne	38994 <__gmpn_hgcd2@@Base+0x60>  // b.any
   38954:	cmp	x1, x3
   38958:	b.ls	38994 <__gmpn_hgcd2@@Base+0x60>  // b.plast
   3895c:	subs	x10, x1, x3
   38960:	sbc	x0, x0, x2
   38964:	cmp	x0, #0x2
   38968:	b.cs	38974 <__gmpn_hgcd2@@Base+0x40>  // b.hs, b.nlast
   3896c:	mov	w0, wzr
   38970:	ret
   38974:	mov	x8, xzr
   38978:	mov	w9, #0x1                   	// #1
   3897c:	mov	x1, x10
   38980:	mov	w10, #0x1                   	// #1
   38984:	cmp	x0, x2
   38988:	mov	w11, #0x1                   	// #1
   3898c:	b.cs	389c8 <__gmpn_hgcd2@@Base+0x94>  // b.hs, b.nlast
   38990:	b	38a94 <__gmpn_hgcd2@@Base+0x160>
   38994:	subs	x10, x3, x1
   38998:	sbc	x2, x2, x0
   3899c:	cmp	x2, #0x2
   389a0:	b.cs	389ac <__gmpn_hgcd2@@Base+0x78>  // b.hs, b.nlast
   389a4:	mov	w0, wzr
   389a8:	ret
   389ac:	mov	x9, xzr
   389b0:	mov	w8, #0x1                   	// #1
   389b4:	mov	x3, x10
   389b8:	mov	w10, #0x1                   	// #1
   389bc:	cmp	x0, x2
   389c0:	mov	w11, #0x1                   	// #1
   389c4:	b.cc	38a94 <__gmpn_hgcd2@@Base+0x160>  // b.lo, b.ul, b.last
   389c8:	cmp	x0, x2
   389cc:	b.eq	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.none
   389d0:	lsr	x12, x0, #32
   389d4:	cbz	x12, 38b64 <__gmpn_hgcd2@@Base+0x230>
   389d8:	mov	x12, x1
   389dc:	subs	x1, x12, x3
   389e0:	sbc	x0, x0, x2
   389e4:	cmp	x0, #0x2
   389e8:	b.cc	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.lo, b.ul, b.last
   389ec:	cmp	x0, x2
   389f0:	b.ls	38a8c <__gmpn_hgcd2@@Base+0x158>  // b.plast
   389f4:	clz	x14, x0
   389f8:	clz	x12, x2
   389fc:	mov	w16, #0x3f                  	// #63
   38a00:	sub	w17, w12, w14
   38a04:	lsr	x13, x3, #1
   38a08:	sub	w16, w16, w17
   38a0c:	mvn	w18, w12
   38a10:	lsl	x5, x2, x17
   38a14:	lsr	x13, x13, x16
   38a18:	mov	x15, xzr
   38a1c:	lsl	x12, x3, x17
   38a20:	add	x13, x13, x5
   38a24:	add	w14, w18, w14
   38a28:	cmp	x1, x12
   38a2c:	cset	w16, cs  // cs = hs, nlast
   38a30:	cmp	x0, x13
   38a34:	cset	w17, hi  // hi = pmore
   38a38:	csel	w17, w16, w17, eq  // eq = none
   38a3c:	sbfx	x16, x17, #0, #1
   38a40:	bfi	x17, x15, #1, #63
   38a44:	and	x15, x12, x16
   38a48:	and	x18, x13, x16
   38a4c:	subs	x16, x1, x15
   38a50:	sbc	x0, x0, x18
   38a54:	extr	x12, x13, x12, #1
   38a58:	adds	w14, w14, #0x1
   38a5c:	lsr	x13, x13, #1
   38a60:	mov	x1, x16
   38a64:	mov	x15, x17
   38a68:	b.cc	38a28 <__gmpn_hgcd2@@Base+0xf4>  // b.lo, b.ul, b.last
   38a6c:	cmp	x0, #0x1
   38a70:	cinc	x12, x17, hi  // hi = pmore
   38a74:	cmp	x0, #0x2
   38a78:	madd	x11, x12, x8, x11
   38a7c:	madd	x9, x12, x10, x9
   38a80:	b.cc	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.lo, b.ul, b.last
   38a84:	mov	x1, x16
   38a88:	b	38a94 <__gmpn_hgcd2@@Base+0x160>
   38a8c:	add	x9, x9, x10
   38a90:	add	x11, x11, x8
   38a94:	cmp	x0, x2
   38a98:	b.eq	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.none
   38a9c:	lsr	x12, x2, #32
   38aa0:	cbz	x12, 38b70 <__gmpn_hgcd2@@Base+0x23c>
   38aa4:	mov	x12, x3
   38aa8:	subs	x3, x12, x1
   38aac:	sbc	x2, x2, x0
   38ab0:	cmp	x2, #0x2
   38ab4:	b.cc	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.lo, b.ul, b.last
   38ab8:	cmp	x2, x0
   38abc:	b.ls	38b58 <__gmpn_hgcd2@@Base+0x224>  // b.plast
   38ac0:	clz	x14, x2
   38ac4:	clz	x12, x0
   38ac8:	mov	w16, #0x3f                  	// #63
   38acc:	sub	w17, w12, w14
   38ad0:	lsr	x13, x1, #1
   38ad4:	sub	w16, w16, w17
   38ad8:	mvn	w18, w12
   38adc:	lsl	x5, x0, x17
   38ae0:	lsr	x13, x13, x16
   38ae4:	mov	x15, xzr
   38ae8:	lsl	x12, x1, x17
   38aec:	add	x13, x13, x5
   38af0:	add	w14, w18, w14
   38af4:	cmp	x3, x12
   38af8:	cset	w16, cs  // cs = hs, nlast
   38afc:	cmp	x2, x13
   38b00:	cset	w17, hi  // hi = pmore
   38b04:	csel	w17, w16, w17, eq  // eq = none
   38b08:	sbfx	x16, x17, #0, #1
   38b0c:	bfi	x17, x15, #1, #63
   38b10:	and	x15, x12, x16
   38b14:	and	x18, x13, x16
   38b18:	subs	x16, x3, x15
   38b1c:	sbc	x2, x2, x18
   38b20:	extr	x12, x13, x12, #1
   38b24:	adds	w14, w14, #0x1
   38b28:	lsr	x13, x13, #1
   38b2c:	mov	x3, x16
   38b30:	mov	x15, x17
   38b34:	b.cc	38af4 <__gmpn_hgcd2@@Base+0x1c0>  // b.lo, b.ul, b.last
   38b38:	cmp	x2, #0x1
   38b3c:	cinc	x12, x17, hi  // hi = pmore
   38b40:	cmp	x2, #0x2
   38b44:	madd	x8, x12, x11, x8
   38b48:	madd	x10, x12, x9, x10
   38b4c:	b.cc	38bf0 <__gmpn_hgcd2@@Base+0x2bc>  // b.lo, b.ul, b.last
   38b50:	mov	x3, x16
   38b54:	b	389c8 <__gmpn_hgcd2@@Base+0x94>
   38b58:	add	x10, x9, x10
   38b5c:	add	x8, x11, x8
   38b60:	b	389c8 <__gmpn_hgcd2@@Base+0x94>
   38b64:	extr	x12, x0, x1, #32
   38b68:	extr	x13, x2, x3, #32
   38b6c:	b	38b7c <__gmpn_hgcd2@@Base+0x248>
   38b70:	extr	x12, x0, x1, #32
   38b74:	extr	x13, x2, x3, #32
   38b78:	b	38bbc <__gmpn_hgcd2@@Base+0x288>
   38b7c:	sub	x12, x12, x13
   38b80:	lsr	x14, x12, #33
   38b84:	cbz	x14, 38bf0 <__gmpn_hgcd2@@Base+0x2bc>
   38b88:	cmp	x12, x13
   38b8c:	b.ls	38bb4 <__gmpn_hgcd2@@Base+0x280>  // b.plast
   38b90:	udiv	x14, x12, x13
   38b94:	msub	x12, x14, x13, x12
   38b98:	lsr	x15, x12, #33
   38b9c:	cmp	x15, #0x0
   38ba0:	cinc	x14, x14, ne  // ne = any
   38ba4:	madd	x11, x14, x8, x11
   38ba8:	madd	x9, x14, x10, x9
   38bac:	cbnz	x15, 38bbc <__gmpn_hgcd2@@Base+0x288>
   38bb0:	b	38bf0 <__gmpn_hgcd2@@Base+0x2bc>
   38bb4:	add	x9, x9, x10
   38bb8:	add	x11, x11, x8
   38bbc:	sub	x13, x13, x12
   38bc0:	lsr	x14, x13, #33
   38bc4:	cbz	x14, 38bf0 <__gmpn_hgcd2@@Base+0x2bc>
   38bc8:	cmp	x13, x12
   38bcc:	b.ls	38c04 <__gmpn_hgcd2@@Base+0x2d0>  // b.plast
   38bd0:	udiv	x14, x13, x12
   38bd4:	msub	x13, x14, x12, x13
   38bd8:	lsr	x15, x13, #33
   38bdc:	cmp	x15, #0x0
   38be0:	cinc	x14, x14, ne  // ne = any
   38be4:	madd	x8, x14, x11, x8
   38be8:	madd	x10, x14, x9, x10
   38bec:	cbnz	x15, 38b7c <__gmpn_hgcd2@@Base+0x248>
   38bf0:	stp	x8, x11, [x4, #16]
   38bf4:	mov	w8, #0x1                   	// #1
   38bf8:	stp	x10, x9, [x4]
   38bfc:	mov	w0, w8
   38c00:	ret
   38c04:	add	x10, x9, x10
   38c08:	add	x8, x11, x8
   38c0c:	b	38b7c <__gmpn_hgcd2@@Base+0x248>

0000000000038c10 <__gmpn_hgcd_mul_matrix1_vector@@Base>:
   38c10:	stp	x29, x30, [sp, #-64]!
   38c14:	stp	x24, x23, [sp, #16]
   38c18:	stp	x22, x21, [sp, #32]
   38c1c:	stp	x20, x19, [sp, #48]
   38c20:	mov	x20, x3
   38c24:	ldr	x3, [x0]
   38c28:	mov	x21, x2
   38c2c:	mov	x22, x0
   38c30:	mov	x23, x1
   38c34:	mov	x0, x1
   38c38:	mov	x1, x2
   38c3c:	mov	x2, x4
   38c40:	mov	x29, sp
   38c44:	mov	x19, x4
   38c48:	bl	d490 <__gmpn_mul_1@plt>
   38c4c:	ldr	x3, [x22, #16]
   38c50:	mov	x24, x0
   38c54:	mov	x0, x23
   38c58:	mov	x1, x20
   38c5c:	mov	x2, x19
   38c60:	bl	d400 <__gmpn_addmul_1@plt>
   38c64:	ldr	x3, [x22, #24]
   38c68:	add	x24, x0, x24
   38c6c:	mov	x0, x20
   38c70:	mov	x1, x20
   38c74:	mov	x2, x19
   38c78:	bl	d490 <__gmpn_mul_1@plt>
   38c7c:	ldr	x3, [x22, #8]
   38c80:	mov	x22, x0
   38c84:	mov	x0, x20
   38c88:	mov	x1, x21
   38c8c:	mov	x2, x19
   38c90:	bl	d400 <__gmpn_addmul_1@plt>
   38c94:	add	x8, x0, x22
   38c98:	lsl	x9, x19, #3
   38c9c:	orr	x10, x8, x24
   38ca0:	str	x24, [x23, x9]
   38ca4:	cmp	x10, #0x0
   38ca8:	str	x8, [x20, x9]
   38cac:	cinc	x0, x19, ne  // ne = any
   38cb0:	ldp	x20, x19, [sp, #48]
   38cb4:	ldp	x22, x21, [sp, #32]
   38cb8:	ldp	x24, x23, [sp, #16]
   38cbc:	ldp	x29, x30, [sp], #64
   38cc0:	ret

0000000000038cc4 <__gmpn_hgcd_step@@Base>:
   38cc4:	sub	sp, sp, #0x60
   38cc8:	lsl	x8, x0, #3
   38ccc:	stp	x29, x30, [sp, #32]
   38cd0:	stp	x24, x23, [sp, #48]
   38cd4:	stp	x22, x21, [sp, #64]
   38cd8:	stp	x20, x19, [sp, #80]
   38cdc:	sub	x9, x8, #0x8
   38ce0:	mov	x21, x2
   38ce4:	mov	x20, x0
   38ce8:	ldr	x0, [x1, x9]
   38cec:	ldr	x2, [x2, x9]
   38cf0:	add	x9, x3, #0x1
   38cf4:	mov	x19, x5
   38cf8:	mov	x23, x4
   38cfc:	mov	x22, x1
   38d00:	mov	x24, x3
   38d04:	cmp	x9, x20
   38d08:	orr	x9, x2, x0
   38d0c:	add	x29, sp, #0x20
   38d10:	b.ne	38d20 <__gmpn_hgcd_step@@Base+0x5c>  // b.any
   38d14:	cmp	x9, #0x4
   38d18:	b.cs	38dbc <__gmpn_hgcd_step@@Base+0xf8>  // b.hs, b.nlast
   38d1c:	b	38dd4 <__gmpn_hgcd_step@@Base+0x110>
   38d20:	tbnz	x9, #63, 38dbc <__gmpn_hgcd_step@@Base+0xf8>
   38d24:	sub	x10, x8, #0x10
   38d28:	sub	x8, x8, #0x18
   38d2c:	ldr	x12, [x22, x10]
   38d30:	ldr	x14, [x22, x8]
   38d34:	ldr	x10, [x21, x10]
   38d38:	ldr	x8, [x21, x8]
   38d3c:	clz	x9, x9
   38d40:	neg	x13, x9
   38d44:	lsl	x11, x0, x9
   38d48:	lsl	x15, x2, x9
   38d4c:	lsr	x16, x12, x13
   38d50:	lsl	x12, x12, x9
   38d54:	lsr	x14, x14, x13
   38d58:	lsl	x9, x10, x9
   38d5c:	lsr	x10, x10, x13
   38d60:	lsr	x8, x8, x13
   38d64:	orr	x0, x16, x11
   38d68:	orr	x1, x14, x12
   38d6c:	orr	x2, x10, x15
   38d70:	orr	x3, x8, x9
   38d74:	mov	x4, sp
   38d78:	bl	c5a0 <__gmpn_hgcd2@plt>
   38d7c:	cbz	w0, 38dd4 <__gmpn_hgcd_step@@Base+0x110>
   38d80:	mov	x1, sp
   38d84:	mov	x0, x23
   38d88:	mov	x2, x19
   38d8c:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   38d90:	mov	x0, x19
   38d94:	mov	x1, x22
   38d98:	mov	x2, x20
   38d9c:	bl	ca50 <__gmpn_copyi@plt>
   38da0:	mov	x0, sp
   38da4:	mov	x1, x22
   38da8:	mov	x2, x19
   38dac:	mov	x3, x21
   38db0:	mov	x4, x20
   38db4:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   38db8:	b	38df8 <__gmpn_hgcd_step@@Base+0x134>
   38dbc:	sub	x8, x8, #0x10
   38dc0:	ldr	x1, [x22, x8]
   38dc4:	ldr	x3, [x21, x8]
   38dc8:	mov	x4, sp
   38dcc:	bl	c5a0 <__gmpn_hgcd2@plt>
   38dd0:	cbnz	w0, 38d80 <__gmpn_hgcd_step@@Base+0xbc>
   38dd4:	adrp	x4, 38000 <__gmpn_hgcd_matrix_update_q@@Base+0x150>
   38dd8:	add	x4, x4, #0xe10
   38ddc:	mov	x0, x22
   38de0:	mov	x1, x21
   38de4:	mov	x2, x20
   38de8:	mov	x3, x24
   38dec:	mov	x5, x23
   38df0:	mov	x6, x19
   38df4:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   38df8:	ldp	x20, x19, [sp, #80]
   38dfc:	ldp	x22, x21, [sp, #64]
   38e00:	ldp	x24, x23, [sp, #48]
   38e04:	ldp	x29, x30, [sp, #32]
   38e08:	add	sp, sp, #0x60
   38e0c:	ret
   38e10:	add	x9, x3, x4, lsl #3
   38e14:	mov	x8, x4
   38e18:	add	x4, x9, #0x8
   38e1c:	subs	x8, x8, #0x1
   38e20:	b.lt	38e40 <__gmpn_hgcd_step@@Base+0x17c>  // b.tstop
   38e24:	ldur	x9, [x4, #-16]
   38e28:	sub	x4, x4, #0x8
   38e2c:	cbz	x9, 38e1c <__gmpn_hgcd_step@@Base+0x158>
   38e30:	add	x2, x8, #0x1
   38e34:	mov	x1, x3
   38e38:	mov	w3, w5
   38e3c:	b	d020 <__gmpn_hgcd_matrix_update_q@plt>
   38e40:	ret

0000000000038e44 <__gmpn_hgcd_reduce_itch@@Base>:
   38e44:	stp	x29, x30, [sp, #-48]!
   38e48:	str	x21, [sp, #16]
   38e4c:	cmp	x0, #0x68e
   38e50:	sub	x21, x0, x1
   38e54:	stp	x20, x19, [sp, #32]
   38e58:	mov	x29, sp
   38e5c:	b.le	38e7c <__gmpn_hgcd_reduce_itch@@Base+0x38>
   38e60:	mov	x0, x21
   38e64:	bl	c590 <__gmpn_hgcd_itch@plt>
   38e68:	add	x0, x0, x21, lsl #1
   38e6c:	ldp	x20, x19, [sp, #32]
   38e70:	ldr	x21, [sp, #16]
   38e74:	ldp	x29, x30, [sp], #48
   38e78:	ret
   38e7c:	mov	x20, x0
   38e80:	mov	x0, x21
   38e84:	mov	x19, x1
   38e88:	bl	c590 <__gmpn_hgcd_itch@plt>
   38e8c:	add	x8, x20, x19
   38e90:	sub	x8, x8, #0x1
   38e94:	cmp	x0, x8
   38e98:	csel	x0, x8, x0, lt  // lt = tstop
   38e9c:	ldp	x20, x19, [sp, #32]
   38ea0:	ldr	x21, [sp, #16]
   38ea4:	ldp	x29, x30, [sp], #48
   38ea8:	ret

0000000000038eac <__gmpn_hgcd_reduce@@Base>:
   38eac:	stp	x29, x30, [sp, #-80]!
   38eb0:	stp	x24, x23, [sp, #32]
   38eb4:	stp	x22, x21, [sp, #48]
   38eb8:	stp	x20, x19, [sp, #64]
   38ebc:	mov	x22, x5
   38ec0:	mov	x24, x4
   38ec4:	mov	x23, x3
   38ec8:	mov	x19, x2
   38ecc:	mov	x20, x1
   38ed0:	mov	x21, x0
   38ed4:	cmp	x3, #0x68e
   38ed8:	add	x8, x1, x4, lsl #3
   38edc:	str	x25, [sp, #16]
   38ee0:	mov	x29, sp
   38ee4:	b.le	38f5c <__gmpn_hgcd_reduce@@Base+0xb0>
   38ee8:	sub	x25, x23, x24
   38eec:	mov	x0, x22
   38ef0:	mov	x1, x8
   38ef4:	mov	x2, x25
   38ef8:	bl	ca50 <__gmpn_copyi@plt>
   38efc:	add	x8, x22, x23, lsl #3
   38f00:	lsl	x9, x24, #3
   38f04:	sub	x24, x8, x9
   38f08:	add	x1, x19, x9
   38f0c:	mov	x0, x24
   38f10:	mov	x2, x25
   38f14:	bl	ca50 <__gmpn_copyi@plt>
   38f18:	add	x4, x22, x25, lsl #4
   38f1c:	mov	x0, x22
   38f20:	mov	x1, x24
   38f24:	mov	x2, x25
   38f28:	mov	x3, x21
   38f2c:	bl	cd50 <__gmpn_hgcd_appr@plt>
   38f30:	cbz	w0, 38fac <__gmpn_hgcd_reduce@@Base+0x100>
   38f34:	mov	x0, x21
   38f38:	mov	x1, x20
   38f3c:	mov	x2, x19
   38f40:	mov	x3, x23
   38f44:	ldp	x20, x19, [sp, #64]
   38f48:	ldp	x22, x21, [sp, #48]
   38f4c:	ldp	x24, x23, [sp, #32]
   38f50:	ldr	x25, [sp, #16]
   38f54:	ldp	x29, x30, [sp], #80
   38f58:	b	38fc8 <__gmpn_hgcd_reduce@@Base+0x11c>
   38f5c:	add	x1, x19, x24, lsl #3
   38f60:	sub	x2, x23, x24
   38f64:	mov	x0, x8
   38f68:	mov	x3, x21
   38f6c:	mov	x4, x22
   38f70:	bl	cde0 <__gmpn_hgcd@plt>
   38f74:	cmp	x0, #0x1
   38f78:	b.lt	38fac <__gmpn_hgcd_reduce@@Base+0x100>  // b.tstop
   38f7c:	add	x1, x0, x24
   38f80:	mov	x0, x21
   38f84:	mov	x2, x20
   38f88:	mov	x3, x19
   38f8c:	mov	x4, x24
   38f90:	mov	x5, x22
   38f94:	ldp	x20, x19, [sp, #64]
   38f98:	ldp	x22, x21, [sp, #48]
   38f9c:	ldp	x24, x23, [sp, #32]
   38fa0:	ldr	x25, [sp, #16]
   38fa4:	ldp	x29, x30, [sp], #80
   38fa8:	b	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   38fac:	ldp	x20, x19, [sp, #64]
   38fb0:	ldp	x22, x21, [sp, #48]
   38fb4:	ldp	x24, x23, [sp, #32]
   38fb8:	ldr	x25, [sp, #16]
   38fbc:	mov	x0, xzr
   38fc0:	ldp	x29, x30, [sp], #80
   38fc4:	ret
   38fc8:	stp	x29, x30, [sp, #-96]!
   38fcc:	stp	x28, x27, [sp, #16]
   38fd0:	stp	x26, x25, [sp, #32]
   38fd4:	stp	x24, x23, [sp, #48]
   38fd8:	stp	x22, x21, [sp, #64]
   38fdc:	stp	x20, x19, [sp, #80]
   38fe0:	mov	x29, sp
   38fe4:	sub	sp, sp, #0x60
   38fe8:	mov	x23, x3
   38fec:	mov	x19, x2
   38ff0:	mov	x20, x1
   38ff4:	mov	x22, x0
   38ff8:	mov	x8, x3
   38ffc:	mov	x3, x8
   39000:	subs	x8, x8, #0x1
   39004:	b.lt	39010 <__gmpn_hgcd_reduce@@Base+0x164>  // b.tstop
   39008:	ldr	x9, [x20, x8, lsl #3]
   3900c:	cbz	x9, 38ffc <__gmpn_hgcd_reduce@@Base+0x150>
   39010:	mov	x9, x23
   39014:	mov	x8, x9
   39018:	subs	x9, x9, #0x1
   3901c:	b.lt	39028 <__gmpn_hgcd_reduce@@Base+0x17c>  // b.tstop
   39020:	ldr	x10, [x19, x9, lsl #3]
   39024:	cbz	x10, 39014 <__gmpn_hgcd_reduce@@Base+0x168>
   39028:	ldr	x25, [x22, #8]
   3902c:	mov	x9, x25
   39030:	mov	x21, x9
   39034:	subs	x9, x9, #0x1
   39038:	b.lt	3904c <__gmpn_hgcd_reduce@@Base+0x1a0>  // b.tstop
   3903c:	ldr	x10, [x22, #16]
   39040:	add	x10, x10, x21, lsl #3
   39044:	ldur	x10, [x10, #-8]
   39048:	cbz	x10, 39030 <__gmpn_hgcd_reduce@@Base+0x184>
   3904c:	sub	x12, x3, x21
   39050:	mov	x9, x25
   39054:	stur	x21, [x29, #-32]
   39058:	mov	x26, x9
   3905c:	subs	x9, x9, #0x1
   39060:	b.lt	39074 <__gmpn_hgcd_reduce@@Base+0x1c8>  // b.tstop
   39064:	ldr	x10, [x22, #24]
   39068:	add	x10, x10, x26, lsl #3
   3906c:	ldur	x10, [x10, #-8]
   39070:	cbz	x10, 39058 <__gmpn_hgcd_reduce@@Base+0x1ac>
   39074:	sub	x13, x3, x26
   39078:	mov	x9, x25
   3907c:	stur	x26, [x29, #-24]
   39080:	mov	x5, x9
   39084:	subs	x9, x9, #0x1
   39088:	b.lt	3909c <__gmpn_hgcd_reduce@@Base+0x1f0>  // b.tstop
   3908c:	ldr	x10, [x22, #32]
   39090:	add	x10, x10, x5, lsl #3
   39094:	ldur	x10, [x10, #-8]
   39098:	cbz	x10, 39080 <__gmpn_hgcd_reduce@@Base+0x1d4>
   3909c:	sub	x14, x8, x5
   390a0:	mov	x10, x25
   390a4:	stur	x5, [x29, #-16]
   390a8:	mov	x9, x10
   390ac:	subs	x10, x10, #0x1
   390b0:	b.lt	390c4 <__gmpn_hgcd_reduce@@Base+0x218>  // b.tstop
   390b4:	ldr	x11, [x22, #40]
   390b8:	add	x11, x11, x9, lsl #3
   390bc:	ldur	x11, [x11, #-8]
   390c0:	cbz	x11, 390a8 <__gmpn_hgcd_reduce@@Base+0x1fc>
   390c4:	stur	x9, [x29, #-8]
   390c8:	stur	xzr, [x29, #-40]
   390cc:	cbz	x26, 391b0 <__gmpn_hgcd_reduce@@Base+0x304>
   390d0:	cbz	x5, 391c4 <__gmpn_hgcd_reduce@@Base+0x318>
   390d4:	sub	x9, x8, x9
   390d8:	cmp	x12, x14
   390dc:	csel	x8, x12, x14, lt  // lt = tstop
   390e0:	cmp	x13, x9
   390e4:	stp	x9, x14, [x29, #-88]
   390e8:	csel	x9, x13, x9, lt  // lt = tstop
   390ec:	cmp	x8, x9
   390f0:	csel	x8, x8, x9, gt
   390f4:	add	x0, x8, #0x2
   390f8:	stp	x13, x12, [x29, #-72]
   390fc:	stur	x8, [x29, #-56]
   39100:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   39104:	asr	x8, x0, #1
   39108:	cmp	x8, x25
   3910c:	csel	x10, x0, x8, lt  // lt = tstop
   39110:	cmp	x8, x0
   39114:	add	x9, x0, x0, lsl #1
   39118:	csel	x8, x10, xzr, lt  // lt = tstop
   3911c:	add	x8, x9, x8
   39120:	lsl	x8, x8, #3
   39124:	add	x1, x8, #0x20
   39128:	mov	w8, #0x7f00                	// #32512
   3912c:	mov	x24, x0
   39130:	cmp	x1, x8
   39134:	b.hi	394a0 <__gmpn_hgcd_reduce@@Base+0x5f4>  // b.pmore
   39138:	add	x9, x1, #0xf
   3913c:	mov	x8, sp
   39140:	and	x9, x9, #0xfffffffffffffff0
   39144:	sub	x25, x8, x9
   39148:	mov	sp, x25
   3914c:	lsl	x8, x24, #3
   39150:	add	x9, x25, x8
   39154:	cmp	x24, x23
   39158:	add	x28, x9, x8
   3915c:	stur	x9, [x29, #-48]
   39160:	b.ge	39254 <__gmpn_hgcd_reduce@@Base+0x3a8>  // b.tcont
   39164:	subs	x27, x23, x24
   39168:	mov	x23, x24
   3916c:	b.eq	39254 <__gmpn_hgcd_reduce@@Base+0x3a8>  // b.none
   39170:	add	x2, x20, x24, lsl #3
   39174:	mov	x0, x20
   39178:	mov	x1, x20
   3917c:	mov	x3, x27
   39180:	bl	ca70 <__gmpn_add_n@plt>
   39184:	cbz	x0, 391fc <__gmpn_hgcd_reduce@@Base+0x350>
   39188:	mov	x8, x27
   3918c:	cmp	x8, x24
   39190:	b.ge	391e8 <__gmpn_hgcd_reduce@@Base+0x33c>  // b.tcont
   39194:	lsl	x9, x8, #3
   39198:	ldr	x10, [x20, x9]
   3919c:	add	x8, x8, #0x1
   391a0:	adds	x10, x10, #0x1
   391a4:	str	x10, [x20, x9]
   391a8:	b.cs	3918c <__gmpn_hgcd_reduce@@Base+0x2e0>  // b.hs, b.nlast
   391ac:	b	391fc <__gmpn_hgcd_reduce@@Base+0x350>
   391b0:	ldr	x4, [x22, #32]
   391b4:	mov	x0, x19
   391b8:	mov	x1, x8
   391bc:	mov	x2, x20
   391c0:	b	391dc <__gmpn_hgcd_reduce@@Base+0x330>
   391c4:	ldr	x4, [x22, #24]
   391c8:	mov	x0, x20
   391cc:	mov	x1, x3
   391d0:	mov	x2, x19
   391d4:	mov	x3, x8
   391d8:	mov	x5, x26
   391dc:	bl	394b8 <__gmpn_hgcd_reduce@@Base+0x60c>
   391e0:	mov	x19, x0
   391e4:	b	3947c <__gmpn_hgcd_reduce@@Base+0x5d0>
   391e8:	mov	x8, x20
   391ec:	ldr	x9, [x8]
   391f0:	adds	x9, x9, #0x1
   391f4:	str	x9, [x8], #8
   391f8:	b.cs	391ec <__gmpn_hgcd_reduce@@Base+0x340>  // b.hs, b.nlast
   391fc:	add	x2, x19, x24, lsl #3
   39200:	mov	x0, x19
   39204:	mov	x1, x19
   39208:	mov	x3, x27
   3920c:	bl	ca70 <__gmpn_add_n@plt>
   39210:	mov	x23, x24
   39214:	cbz	x0, 39254 <__gmpn_hgcd_reduce@@Base+0x3a8>
   39218:	cmp	x27, x24
   3921c:	b.ge	3923c <__gmpn_hgcd_reduce@@Base+0x390>  // b.tcont
   39220:	lsl	x8, x27, #3
   39224:	ldr	x9, [x19, x8]
   39228:	add	x27, x27, #0x1
   3922c:	adds	x9, x9, #0x1
   39230:	str	x9, [x19, x8]
   39234:	b.cs	39218 <__gmpn_hgcd_reduce@@Base+0x36c>  // b.hs, b.nlast
   39238:	b	39250 <__gmpn_hgcd_reduce@@Base+0x3a4>
   3923c:	mov	x8, x19
   39240:	ldr	x9, [x8]
   39244:	adds	x9, x9, #0x1
   39248:	str	x9, [x8], #8
   3924c:	b.cs	39240 <__gmpn_hgcd_reduce@@Base+0x394>  // b.hs, b.nlast
   39250:	mov	x23, x24
   39254:	ldur	x27, [x29, #-8]
   39258:	ldr	x4, [x22, #40]
   3925c:	mov	x0, x25
   39260:	mov	x1, x24
   39264:	mov	x2, x20
   39268:	mov	x3, x23
   3926c:	mov	x5, x27
   39270:	mov	x6, x28
   39274:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   39278:	ldr	x4, [x22, #24]
   3927c:	ldur	x0, [x29, #-48]
   39280:	mov	x1, x24
   39284:	mov	x2, x19
   39288:	mov	x3, x23
   3928c:	mov	x5, x26
   39290:	mov	x6, x28
   39294:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   39298:	add	x8, x27, x23
   3929c:	cmp	x8, x24
   392a0:	b.ge	392c4 <__gmpn_hgcd_reduce@@Base+0x418>  // b.tcont
   392a4:	sub	x8, x24, x23
   392a8:	subs	x8, x8, x27
   392ac:	b.eq	392c4 <__gmpn_hgcd_reduce@@Base+0x418>  // b.none
   392b0:	add	x9, x23, x27
   392b4:	add	x0, x25, x9, lsl #3
   392b8:	lsl	x2, x8, #3
   392bc:	mov	w1, wzr
   392c0:	bl	c5f0 <memset@plt>
   392c4:	add	x8, x23, x26
   392c8:	cmp	x8, x24
   392cc:	ldur	x8, [x29, #-56]
   392d0:	add	x8, x8, #0x1
   392d4:	stur	x8, [x29, #-56]
   392d8:	b.ge	39300 <__gmpn_hgcd_reduce@@Base+0x454>  // b.tcont
   392dc:	sub	x8, x24, x23
   392e0:	subs	x8, x8, x26
   392e4:	b.eq	39300 <__gmpn_hgcd_reduce@@Base+0x454>  // b.none
   392e8:	add	x9, x24, x23
   392ec:	add	x9, x9, x26
   392f0:	add	x0, x25, x9, lsl #3
   392f4:	lsl	x2, x8, #3
   392f8:	mov	w1, wzr
   392fc:	bl	c5f0 <memset@plt>
   39300:	ldur	x27, [x29, #-48]
   39304:	mov	x0, x25
   39308:	mov	x1, x25
   3930c:	mov	x3, x24
   39310:	mov	x2, x27
   39314:	bl	c2d0 <__gmpn_sub_n@plt>
   39318:	ldr	x8, [x25]
   3931c:	subs	x8, x8, x0
   39320:	str	x8, [x25]
   39324:	b.cs	3933c <__gmpn_hgcd_reduce@@Base+0x490>  // b.hs, b.nlast
   39328:	add	x8, x25, #0x8
   3932c:	ldr	x9, [x8]
   39330:	sub	x10, x9, #0x1
   39334:	str	x10, [x8], #8
   39338:	cbz	x9, 3932c <__gmpn_hgcd_reduce@@Base+0x480>
   3933c:	ldur	x26, [x29, #-16]
   39340:	ldr	x4, [x22, #32]
   39344:	mov	x0, x27
   39348:	mov	x1, x24
   3934c:	mov	x2, x20
   39350:	mov	x3, x23
   39354:	mov	x5, x26
   39358:	mov	x6, x28
   3935c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   39360:	ldur	x2, [x29, #-56]
   39364:	mov	x0, x20
   39368:	mov	x1, x25
   3936c:	bl	ca50 <__gmpn_copyi@plt>
   39370:	ldr	x4, [x22, #16]
   39374:	mov	x0, x25
   39378:	mov	x1, x24
   3937c:	mov	x2, x19
   39380:	mov	x3, x23
   39384:	mov	x5, x21
   39388:	mov	x6, x28
   3938c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   39390:	add	x8, x26, x23
   39394:	cmp	x8, x24
   39398:	b.ge	393c0 <__gmpn_hgcd_reduce@@Base+0x514>  // b.tcont
   3939c:	sub	x8, x24, x23
   393a0:	subs	x8, x8, x26
   393a4:	b.eq	393c0 <__gmpn_hgcd_reduce@@Base+0x514>  // b.none
   393a8:	add	x9, x24, x23
   393ac:	add	x9, x9, x26
   393b0:	add	x0, x25, x9, lsl #3
   393b4:	lsl	x2, x8, #3
   393b8:	mov	w1, wzr
   393bc:	bl	c5f0 <memset@plt>
   393c0:	add	x8, x23, x21
   393c4:	cmp	x8, x24
   393c8:	b.ge	393ec <__gmpn_hgcd_reduce@@Base+0x540>  // b.tcont
   393cc:	sub	x8, x24, x23
   393d0:	subs	x8, x8, x21
   393d4:	b.eq	393ec <__gmpn_hgcd_reduce@@Base+0x540>  // b.none
   393d8:	add	x9, x23, x21
   393dc:	add	x0, x25, x9, lsl #3
   393e0:	lsl	x2, x8, #3
   393e4:	mov	w1, wzr
   393e8:	bl	c5f0 <memset@plt>
   393ec:	mov	x0, x25
   393f0:	mov	x1, x25
   393f4:	mov	x2, x27
   393f8:	mov	x3, x24
   393fc:	bl	c2d0 <__gmpn_sub_n@plt>
   39400:	ldr	x8, [x25]
   39404:	subs	x8, x8, x0
   39408:	str	x8, [x25]
   3940c:	b.cs	39424 <__gmpn_hgcd_reduce@@Base+0x578>  // b.hs, b.nlast
   39410:	add	x8, x25, #0x8
   39414:	ldr	x9, [x8]
   39418:	sub	x10, x9, #0x1
   3941c:	str	x10, [x8], #8
   39420:	cbz	x9, 39414 <__gmpn_hgcd_reduce@@Base+0x568>
   39424:	ldur	x2, [x29, #-56]
   39428:	mov	x0, x19
   3942c:	mov	x1, x25
   39430:	bl	ca50 <__gmpn_copyi@plt>
   39434:	ldur	x8, [x29, #-64]
   39438:	ldp	x10, x9, [x29, #-88]
   3943c:	cmp	x8, x9
   39440:	csel	x8, x8, x9, lt  // lt = tstop
   39444:	ldur	x9, [x29, #-72]
   39448:	cmp	x9, x10
   3944c:	csel	x9, x9, x10, lt  // lt = tstop
   39450:	cmp	x8, x9
   39454:	csel	x8, x8, x9, gt
   39458:	lsl	x9, x8, #3
   3945c:	ldr	x10, [x20, x9]
   39460:	ldr	x9, [x19, x9]
   39464:	sub	x8, x8, #0x1
   39468:	orr	x9, x9, x10
   3946c:	cbz	x9, 39458 <__gmpn_hgcd_reduce@@Base+0x5ac>
   39470:	ldur	x0, [x29, #-40]
   39474:	add	x19, x8, #0x2
   39478:	cbnz	x0, 394b0 <__gmpn_hgcd_reduce@@Base+0x604>
   3947c:	mov	x0, x19
   39480:	mov	sp, x29
   39484:	ldp	x20, x19, [sp, #80]
   39488:	ldp	x22, x21, [sp, #64]
   3948c:	ldp	x24, x23, [sp, #48]
   39490:	ldp	x26, x25, [sp, #32]
   39494:	ldp	x28, x27, [sp, #16]
   39498:	ldp	x29, x30, [sp], #96
   3949c:	ret
   394a0:	sub	x0, x29, #0x28
   394a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   394a8:	mov	x25, x0
   394ac:	b	3914c <__gmpn_hgcd_reduce@@Base+0x2a0>
   394b0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   394b4:	b	3947c <__gmpn_hgcd_reduce@@Base+0x5d0>
   394b8:	stp	x29, x30, [sp, #-80]!
   394bc:	stp	x26, x25, [sp, #16]
   394c0:	stp	x24, x23, [sp, #32]
   394c4:	stp	x22, x21, [sp, #48]
   394c8:	stp	x20, x19, [sp, #64]
   394cc:	mov	x29, sp
   394d0:	sub	sp, sp, #0x10
   394d4:	add	x26, x5, x3
   394d8:	mov	x20, x1
   394dc:	lsl	x1, x26, #3
   394e0:	mov	w8, #0x7f00                	// #32512
   394e4:	mov	x22, x5
   394e8:	mov	x23, x4
   394ec:	mov	x19, x3
   394f0:	mov	x24, x2
   394f4:	mov	x21, x0
   394f8:	cmp	x1, x8
   394fc:	stur	xzr, [x29, #-8]
   39500:	b.hi	395b8 <__gmpn_hgcd_reduce@@Base+0x70c>  // b.pmore
   39504:	add	x9, x1, #0xf
   39508:	mov	x8, sp
   3950c:	and	x9, x9, #0xfffffffffffffff0
   39510:	sub	x25, x8, x9
   39514:	mov	sp, x25
   39518:	mov	x0, x25
   3951c:	mov	x1, x24
   39520:	mov	x2, x19
   39524:	mov	x3, x23
   39528:	mov	x4, x22
   3952c:	bl	ccd0 <__gmpn_mul@plt>
   39530:	cmp	x26, x20
   39534:	cset	w8, gt
   39538:	subs	x22, x26, x8
   3953c:	b.eq	39578 <__gmpn_hgcd_reduce@@Base+0x6cc>  // b.none
   39540:	mov	x0, x21
   39544:	mov	x1, x21
   39548:	mov	x2, x25
   3954c:	mov	x3, x22
   39550:	bl	c2d0 <__gmpn_sub_n@plt>
   39554:	cbz	x0, 39578 <__gmpn_hgcd_reduce@@Base+0x6cc>
   39558:	cmp	x22, x20
   3955c:	b.ge	39578 <__gmpn_hgcd_reduce@@Base+0x6cc>  // b.tcont
   39560:	lsl	x8, x22, #3
   39564:	ldr	x9, [x21, x8]
   39568:	add	x22, x22, #0x1
   3956c:	sub	x10, x9, #0x1
   39570:	str	x10, [x21, x8]
   39574:	cbz	x9, 39558 <__gmpn_hgcd_reduce@@Base+0x6ac>
   39578:	ldur	x0, [x29, #-8]
   3957c:	cbnz	x0, 395c8 <__gmpn_hgcd_reduce@@Base+0x71c>
   39580:	sub	x8, x21, #0x8
   39584:	mov	x0, x20
   39588:	cmp	x20, x19
   3958c:	b.le	3959c <__gmpn_hgcd_reduce@@Base+0x6f0>
   39590:	ldr	x9, [x8, x0, lsl #3]
   39594:	sub	x20, x0, #0x1
   39598:	cbz	x9, 39584 <__gmpn_hgcd_reduce@@Base+0x6d8>
   3959c:	mov	sp, x29
   395a0:	ldp	x20, x19, [sp, #64]
   395a4:	ldp	x22, x21, [sp, #48]
   395a8:	ldp	x24, x23, [sp, #32]
   395ac:	ldp	x26, x25, [sp, #16]
   395b0:	ldp	x29, x30, [sp], #80
   395b4:	ret
   395b8:	sub	x0, x29, #0x8
   395bc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   395c0:	mov	x25, x0
   395c4:	b	39518 <__gmpn_hgcd_reduce@@Base+0x66c>
   395c8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   395cc:	b	39580 <__gmpn_hgcd_reduce@@Base+0x6d4>

00000000000395d0 <__gmpn_hgcd_itch@@Base>:
   395d0:	cmp	x0, #0x65
   395d4:	b.lt	39630 <__gmpn_hgcd_itch@@Base+0x60>  // b.tstop
   395d8:	mov	x9, #0xd70b                	// #55051
   395dc:	movk	x9, #0x70a3, lsl #16
   395e0:	movk	x9, #0xa3d, lsl #32
   395e4:	sub	x8, x0, #0x1
   395e8:	movk	x9, #0xa3d7, lsl #48
   395ec:	smulh	x9, x8, x9
   395f0:	add	x8, x9, x8
   395f4:	add	x9, x0, #0x3
   395f8:	add	x11, x0, #0x6
   395fc:	cmp	x9, #0x0
   39600:	csel	x9, x11, x9, lt  // lt = tstop
   39604:	asr	x11, x8, #6
   39608:	add	x8, x11, x8, lsr #63
   3960c:	mov	w10, #0x40                  	// #64
   39610:	clz	x8, x8
   39614:	sub	w8, w10, w8
   39618:	mov	w10, #0x16                  	// #22
   3961c:	mov	w11, #0x14                  	// #20
   39620:	asr	x9, x9, #2
   39624:	mul	w8, w8, w10
   39628:	madd	x8, x9, x11, x8
   3962c:	add	x0, x8, #0x65
   39630:	ret

0000000000039634 <__gmpn_hgcd@@Base>:
   39634:	sub	sp, sp, #0x90
   39638:	cmp	x2, #0x0
   3963c:	cinc	x8, x2, lt  // lt = tstop
   39640:	stp	x26, x25, [sp, #80]
   39644:	asr	x25, x8, #1
   39648:	stp	x22, x21, [sp, #112]
   3964c:	add	x22, x25, #0x1
   39650:	cmp	x22, x2
   39654:	stp	x29, x30, [sp, #48]
   39658:	stp	x28, x27, [sp, #64]
   3965c:	stp	x24, x23, [sp, #96]
   39660:	stp	x20, x19, [sp, #128]
   39664:	add	x29, sp, #0x30
   39668:	b.ge	39700 <__gmpn_hgcd@@Base+0xcc>  // b.tcont
   3966c:	mov	x19, x4
   39670:	mov	x20, x3
   39674:	mov	x24, x2
   39678:	mov	x21, x1
   3967c:	mov	x23, x0
   39680:	cmp	x2, #0x64
   39684:	b.le	39708 <__gmpn_hgcd@@Base+0xd4>
   39688:	add	x8, x24, x24, lsl #1
   3968c:	add	x9, x8, #0x3
   39690:	cmp	x8, #0x0
   39694:	csel	x8, x9, x8, lt  // lt = tstop
   39698:	asr	x8, x8, #2
   3969c:	mov	x0, x20
   396a0:	mov	x1, x23
   396a4:	mov	x2, x21
   396a8:	mov	x3, x24
   396ac:	mov	x4, x25
   396b0:	mov	x5, x19
   396b4:	add	x26, x8, #0x1
   396b8:	bl	d2f0 <__gmpn_hgcd_reduce@plt>
   396bc:	cmp	x0, #0x0
   396c0:	cset	w8, ne  // ne = any
   396c4:	csel	x0, x24, x0, eq  // eq = none
   396c8:	mov	x24, x0
   396cc:	cmp	x0, x26
   396d0:	mov	w28, w8
   396d4:	b.le	39778 <__gmpn_hgcd@@Base+0x144>
   396d8:	mov	x0, x24
   396dc:	mov	x1, x23
   396e0:	mov	x2, x21
   396e4:	mov	x3, x22
   396e8:	mov	x4, x20
   396ec:	mov	x5, x19
   396f0:	bl	c2b0 <__gmpn_hgcd_step@plt>
   396f4:	mov	w8, #0x1                   	// #1
   396f8:	cbnz	x0, 396c8 <__gmpn_hgcd@@Base+0x94>
   396fc:	b	39750 <__gmpn_hgcd@@Base+0x11c>
   39700:	mov	x0, xzr
   39704:	b	39758 <__gmpn_hgcd@@Base+0x124>
   39708:	mov	w28, wzr
   3970c:	mov	x0, x24
   39710:	mov	x1, x23
   39714:	mov	x2, x21
   39718:	mov	x3, x22
   3971c:	mov	x4, x20
   39720:	mov	x5, x19
   39724:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39728:	cbz	x0, 39750 <__gmpn_hgcd@@Base+0x11c>
   3972c:	mov	x1, x23
   39730:	mov	x2, x21
   39734:	mov	x3, x22
   39738:	mov	x4, x20
   3973c:	mov	x5, x19
   39740:	mov	x24, x0
   39744:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39748:	cbnz	x0, 3972c <__gmpn_hgcd@@Base+0xf8>
   3974c:	mov	w28, #0x1                   	// #1
   39750:	cmp	w28, #0x0
   39754:	csel	x0, xzr, x24, eq  // eq = none
   39758:	ldp	x20, x19, [sp, #128]
   3975c:	ldp	x22, x21, [sp, #112]
   39760:	ldp	x24, x23, [sp, #96]
   39764:	ldp	x26, x25, [sp, #80]
   39768:	ldp	x28, x27, [sp, #64]
   3976c:	ldp	x29, x30, [sp, #48]
   39770:	add	sp, sp, #0x90
   39774:	ret
   39778:	add	x8, x25, #0x3
   3977c:	cmp	x24, x8
   39780:	b.le	3970c <__gmpn_hgcd@@Base+0xd8>
   39784:	lsl	x8, x22, #1
   39788:	sub	x8, x8, x24
   3978c:	add	x25, x8, #0x1
   39790:	sub	x27, x24, x25
   39794:	add	x8, x27, #0x1
   39798:	add	x9, x27, #0x2
   3979c:	cmp	x8, #0x0
   397a0:	csinc	x8, x9, x27, lt  // lt = tstop
   397a4:	lsl	x8, x8, #4
   397a8:	mov	x0, sp
   397ac:	mov	x1, x27
   397b0:	mov	x2, x19
   397b4:	and	x26, x8, #0xffffffffffffffe0
   397b8:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   397bc:	add	x9, x26, x19
   397c0:	lsl	x8, x25, #3
   397c4:	add	x26, x9, #0x20
   397c8:	add	x0, x23, x8
   397cc:	add	x1, x21, x8
   397d0:	mov	x3, sp
   397d4:	mov	x2, x27
   397d8:	mov	x4, x26
   397dc:	bl	cde0 <__gmpn_hgcd@plt>
   397e0:	cmp	x0, #0x1
   397e4:	b.lt	3970c <__gmpn_hgcd@@Base+0xd8>  // b.tstop
   397e8:	add	x1, x0, x25
   397ec:	mov	x0, sp
   397f0:	mov	x2, x23
   397f4:	mov	x3, x21
   397f8:	mov	x4, x25
   397fc:	mov	x5, x26
   39800:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   39804:	mov	x24, x0
   39808:	mov	x1, sp
   3980c:	mov	x0, x20
   39810:	mov	x2, x26
   39814:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   39818:	mov	w28, #0x1                   	// #1
   3981c:	b	3970c <__gmpn_hgcd@@Base+0xd8>

0000000000039820 <__gmpn_hgcd_appr_itch@@Base>:
   39820:	cmp	x0, #0x68
   39824:	b.lt	39880 <__gmpn_hgcd_appr_itch@@Base+0x60>  // b.tstop
   39828:	mov	x9, #0x13e3                	// #5091
   3982c:	movk	x9, #0x2548, lsl #16
   39830:	movk	x9, #0x65e7, lsl #32
   39834:	sub	x8, x0, #0x1
   39838:	movk	x9, #0x9f11, lsl #48
   3983c:	smulh	x9, x8, x9
   39840:	add	x8, x9, x8
   39844:	add	x9, x0, #0x3
   39848:	add	x11, x0, #0x6
   3984c:	cmp	x9, #0x0
   39850:	csel	x9, x11, x9, lt  // lt = tstop
   39854:	asr	x11, x8, #6
   39858:	add	x8, x11, x8, lsr #63
   3985c:	mov	w10, #0x40                  	// #64
   39860:	clz	x8, x8
   39864:	sub	w8, w10, w8
   39868:	mov	w10, #0x16                  	// #22
   3986c:	mov	w11, #0x14                  	// #20
   39870:	asr	x9, x9, #2
   39874:	mul	w8, w8, w10
   39878:	madd	x8, x9, x11, x8
   3987c:	add	x0, x8, #0x65
   39880:	ret

0000000000039884 <__gmpn_hgcd_appr@@Base>:
   39884:	sub	sp, sp, #0x90
   39888:	cmp	x2, #0x3
   3988c:	stp	x29, x30, [sp, #48]
   39890:	stp	x28, x27, [sp, #64]
   39894:	stp	x26, x25, [sp, #80]
   39898:	stp	x24, x23, [sp, #96]
   3989c:	stp	x22, x21, [sp, #112]
   398a0:	stp	x20, x19, [sp, #128]
   398a4:	add	x29, sp, #0x30
   398a8:	b.ge	398b4 <__gmpn_hgcd_appr@@Base+0x30>  // b.tcont
   398ac:	mov	w24, wzr
   398b0:	b	39be4 <__gmpn_hgcd_appr@@Base+0x360>
   398b4:	lsr	x26, x2, #1
   398b8:	mov	x20, x4
   398bc:	mov	x19, x3
   398c0:	mov	x25, x2
   398c4:	mov	x21, x1
   398c8:	mov	x22, x0
   398cc:	cmp	x2, #0x67
   398d0:	add	x23, x26, #0x1
   398d4:	b.le	39950 <__gmpn_hgcd_appr@@Base+0xcc>
   398d8:	add	x8, x25, x25, lsl #1
   398dc:	add	x9, x8, #0x3
   398e0:	cmp	x8, #0x0
   398e4:	csel	x8, x9, x8, lt  // lt = tstop
   398e8:	asr	x8, x8, #2
   398ec:	mov	x0, x19
   398f0:	mov	x1, x22
   398f4:	mov	x2, x21
   398f8:	mov	x3, x25
   398fc:	mov	x4, x26
   39900:	mov	x5, x20
   39904:	add	x27, x8, #0x1
   39908:	bl	d2f0 <__gmpn_hgcd_reduce@plt>
   3990c:	cmp	x0, #0x0
   39910:	cset	w8, ne  // ne = any
   39914:	csel	x25, x25, x0, eq  // eq = none
   39918:	cmp	x25, x27
   3991c:	mov	w24, w8
   39920:	b.le	39a6c <__gmpn_hgcd_appr@@Base+0x1e8>
   39924:	mov	x0, x25
   39928:	mov	x1, x22
   3992c:	mov	x2, x21
   39930:	mov	x3, x23
   39934:	mov	x4, x19
   39938:	mov	x5, x20
   3993c:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39940:	mov	x25, x0
   39944:	mov	w8, #0x1                   	// #1
   39948:	cbnz	x0, 39918 <__gmpn_hgcd_appr@@Base+0x94>
   3994c:	b	39be4 <__gmpn_hgcd_appr@@Base+0x360>
   39950:	mov	x0, x25
   39954:	mov	x1, x22
   39958:	mov	x2, x21
   3995c:	mov	x3, x23
   39960:	mov	x4, x19
   39964:	mov	x5, x20
   39968:	bl	c2b0 <__gmpn_hgcd_step@plt>
   3996c:	mov	w26, wzr
   39970:	cbz	x0, 39aec <__gmpn_hgcd_appr@@Base+0x268>
   39974:	lsl	w8, w26, #1
   39978:	add	x9, x8, x0, lsl #6
   3997c:	add	x9, x9, #0x40
   39980:	cmp	x9, x23, lsl #7
   39984:	b.le	39998 <__gmpn_hgcd_appr@@Base+0x114>
   39988:	mov	x25, x0
   3998c:	cmp	x25, #0x2
   39990:	b.gt	39a34 <__gmpn_hgcd_appr@@Base+0x1b0>
   39994:	b	39a60 <__gmpn_hgcd_appr@@Base+0x1dc>
   39998:	lsl	x9, x23, #1
   3999c:	sub	x9, x9, x0
   399a0:	lsl	x9, x9, #6
   399a4:	sub	x8, x9, x8
   399a8:	add	x9, x8, #0x3f
   399ac:	cmp	x8, #0x0
   399b0:	csel	x8, x9, x8, lt  // lt = tstop
   399b4:	cbz	w26, 399e4 <__gmpn_hgcd_appr@@Base+0x160>
   399b8:	sub	w26, w26, #0x1
   399bc:	mov	x9, x23
   399c0:	asr	x8, x8, #6
   399c4:	lsl	x10, x8, #3
   399c8:	sub	x25, x0, x8
   399cc:	add	x22, x22, x10
   399d0:	add	x21, x21, x10
   399d4:	sub	x23, x9, x8
   399d8:	cmp	x25, #0x2
   399dc:	b.gt	39a34 <__gmpn_hgcd_appr@@Base+0x1b0>
   399e0:	b	39a60 <__gmpn_hgcd_appr@@Base+0x1dc>
   399e4:	add	x9, x23, #0x1
   399e8:	cmp	x9, x0
   399ec:	b.eq	39a24 <__gmpn_hgcd_appr@@Base+0x1a0>  // b.none
   399f0:	sub	x10, x0, #0x1
   399f4:	mov	x11, x10
   399f8:	ldr	x12, [x22, x11, lsl #3]
   399fc:	cbnz	x12, 39a10 <__gmpn_hgcd_appr@@Base+0x18c>
   39a00:	sub	x11, x11, #0x1
   39a04:	cmp	x23, x11
   39a08:	b.ne	399f8 <__gmpn_hgcd_appr@@Base+0x174>  // b.any
   39a0c:	b	39a24 <__gmpn_hgcd_appr@@Base+0x1a0>
   39a10:	ldr	x11, [x21, x10, lsl #3]
   39a14:	cbnz	x11, 39a58 <__gmpn_hgcd_appr@@Base+0x1d4>
   39a18:	sub	x10, x10, #0x1
   39a1c:	cmp	x23, x10
   39a20:	b.ne	39a10 <__gmpn_hgcd_appr@@Base+0x18c>  // b.any
   39a24:	mov	w26, wzr
   39a28:	mov	x25, x0
   39a2c:	cmp	x25, #0x2
   39a30:	b.le	39a60 <__gmpn_hgcd_appr@@Base+0x1dc>
   39a34:	mov	x0, x25
   39a38:	mov	x1, x22
   39a3c:	mov	x2, x21
   39a40:	mov	x3, x23
   39a44:	mov	x4, x19
   39a48:	mov	x5, x20
   39a4c:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39a50:	cbnz	x0, 39974 <__gmpn_hgcd_appr@@Base+0xf0>
   39a54:	b	39a60 <__gmpn_hgcd_appr@@Base+0x1dc>
   39a58:	mov	w26, #0x3f                  	// #63
   39a5c:	b	399c0 <__gmpn_hgcd_appr@@Base+0x13c>
   39a60:	mov	w24, #0x1                   	// #1
   39a64:	cbnz	w26, 39af4 <__gmpn_hgcd_appr@@Base+0x270>
   39a68:	b	39b74 <__gmpn_hgcd_appr@@Base+0x2f0>
   39a6c:	add	x8, x26, #0x3
   39a70:	cmp	x25, x8
   39a74:	b.le	39ba4 <__gmpn_hgcd_appr@@Base+0x320>
   39a78:	lsl	x8, x23, #1
   39a7c:	sub	x8, x8, x25
   39a80:	add	x26, x8, #0x1
   39a84:	sub	x27, x25, x26
   39a88:	add	x8, x27, #0x1
   39a8c:	add	x9, x27, #0x2
   39a90:	cmp	x8, #0x0
   39a94:	csinc	x8, x9, x27, lt  // lt = tstop
   39a98:	lsl	x8, x8, #4
   39a9c:	mov	x0, sp
   39aa0:	mov	x1, x27
   39aa4:	mov	x2, x20
   39aa8:	and	x28, x8, #0xffffffffffffffe0
   39aac:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   39ab0:	add	x9, x28, x20
   39ab4:	lsl	x8, x26, #3
   39ab8:	add	x26, x9, #0x20
   39abc:	add	x0, x22, x8
   39ac0:	add	x1, x21, x8
   39ac4:	mov	x3, sp
   39ac8:	mov	x2, x27
   39acc:	mov	x4, x26
   39ad0:	bl	cd50 <__gmpn_hgcd_appr@plt>
   39ad4:	cbz	w0, 39ba4 <__gmpn_hgcd_appr@@Base+0x320>
   39ad8:	mov	x1, sp
   39adc:	mov	x0, x19
   39ae0:	mov	x2, x26
   39ae4:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   39ae8:	b	39be0 <__gmpn_hgcd_appr@@Base+0x35c>
   39aec:	mov	w24, wzr
   39af0:	cbz	w26, 39b74 <__gmpn_hgcd_appr@@Base+0x2f0>
   39af4:	mov	w8, #0x40                  	// #64
   39af8:	sub	w26, w8, w26
   39afc:	mov	x0, x22
   39b00:	mov	x1, x22
   39b04:	mov	x2, x25
   39b08:	mov	w3, w26
   39b0c:	bl	c1a0 <__gmpn_rshift@plt>
   39b10:	str	x0, [x22, #-8]!
   39b14:	mov	x0, x21
   39b18:	mov	x1, x21
   39b1c:	mov	x2, x25
   39b20:	mov	w3, w26
   39b24:	bl	c1a0 <__gmpn_rshift@plt>
   39b28:	str	x0, [x21, #-8]!
   39b2c:	lsl	x8, x25, #3
   39b30:	ldr	x9, [x22, x8]
   39b34:	ldr	x8, [x21, x8]
   39b38:	orr	x8, x8, x9
   39b3c:	cmp	x8, #0x0
   39b40:	cinc	x25, x25, ne  // ne = any
   39b44:	cmp	x25, #0x3
   39b48:	b.lt	39b74 <__gmpn_hgcd_appr@@Base+0x2f0>  // b.tstop
   39b4c:	mov	x0, x25
   39b50:	mov	x1, x22
   39b54:	mov	x2, x21
   39b58:	mov	x3, x23
   39b5c:	mov	x4, x19
   39b60:	mov	x5, x20
   39b64:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39b68:	mov	x25, x0
   39b6c:	cbnz	x0, 39b44 <__gmpn_hgcd_appr@@Base+0x2c0>
   39b70:	b	39be0 <__gmpn_hgcd_appr@@Base+0x35c>
   39b74:	cmp	x25, #0x2
   39b78:	b.ne	39be4 <__gmpn_hgcd_appr@@Base+0x360>  // b.any
   39b7c:	ldp	x1, x0, [x22]
   39b80:	ldp	x3, x2, [x21]
   39b84:	mov	x4, sp
   39b88:	bl	c5a0 <__gmpn_hgcd2@plt>
   39b8c:	cbz	w0, 39be4 <__gmpn_hgcd_appr@@Base+0x360>
   39b90:	mov	x1, sp
   39b94:	mov	x0, x19
   39b98:	mov	x2, x20
   39b9c:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   39ba0:	b	39be0 <__gmpn_hgcd_appr@@Base+0x35c>
   39ba4:	mov	x0, x25
   39ba8:	mov	x1, x22
   39bac:	mov	x2, x21
   39bb0:	mov	x3, x23
   39bb4:	mov	x4, x19
   39bb8:	mov	x5, x20
   39bbc:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39bc0:	cbz	x0, 39be4 <__gmpn_hgcd_appr@@Base+0x360>
   39bc4:	mov	x1, x22
   39bc8:	mov	x2, x21
   39bcc:	mov	x3, x23
   39bd0:	mov	x4, x19
   39bd4:	mov	x5, x20
   39bd8:	bl	c2b0 <__gmpn_hgcd_step@plt>
   39bdc:	cbnz	x0, 39bc4 <__gmpn_hgcd_appr@@Base+0x340>
   39be0:	mov	w24, #0x1                   	// #1
   39be4:	mov	w0, w24
   39be8:	ldp	x20, x19, [sp, #128]
   39bec:	ldp	x22, x21, [sp, #112]
   39bf0:	ldp	x24, x23, [sp, #96]
   39bf4:	ldp	x26, x25, [sp, #80]
   39bf8:	ldp	x28, x27, [sp, #64]
   39bfc:	ldp	x29, x30, [sp, #48]
   39c00:	add	sp, sp, #0x90
   39c04:	ret

0000000000039c08 <__gmpn_hgcd2_jacobi@@Base>:
   39c08:	sub	sp, sp, #0x80
   39c0c:	stp	x22, x21, [sp, #96]
   39c10:	mov	x22, x0
   39c14:	cmp	x0, #0x2
   39c18:	mov	w0, wzr
   39c1c:	stp	x29, x30, [sp, #32]
   39c20:	stp	x28, x27, [sp, #48]
   39c24:	stp	x26, x25, [sp, #64]
   39c28:	stp	x24, x23, [sp, #80]
   39c2c:	stp	x20, x19, [sp, #112]
   39c30:	add	x29, sp, #0x20
   39c34:	b.cc	3a03c <__gmpn_hgcd2_jacobi@@Base+0x434>  // b.lo, b.ul, b.last
   39c38:	mov	x21, x2
   39c3c:	cmp	x2, #0x2
   39c40:	b.cc	3a03c <__gmpn_hgcd2_jacobi@@Base+0x434>  // b.lo, b.ul, b.last
   39c44:	ldr	w8, [x5]
   39c48:	mov	x23, x3
   39c4c:	mov	x24, x1
   39c50:	cmp	x22, x21
   39c54:	b.hi	39c64 <__gmpn_hgcd2_jacobi@@Base+0x5c>  // b.pmore
   39c58:	b.ne	39c8c <__gmpn_hgcd2_jacobi@@Base+0x84>  // b.any
   39c5c:	cmp	x24, x23
   39c60:	b.ls	39c8c <__gmpn_hgcd2_jacobi@@Base+0x84>  // b.plast
   39c64:	subs	x10, x24, x23
   39c68:	sbc	x22, x22, x21
   39c6c:	cmp	x22, #0x2
   39c70:	b.cc	39c9c <__gmpn_hgcd2_jacobi@@Base+0x94>  // b.lo, b.ul, b.last
   39c74:	stp	x4, x5, [sp]
   39c78:	mov	x26, xzr
   39c7c:	mov	w9, #0x5                   	// #5
   39c80:	mov	w25, #0x1                   	// #1
   39c84:	mov	x24, x10
   39c88:	b	39cb8 <__gmpn_hgcd2_jacobi@@Base+0xb0>
   39c8c:	subs	x10, x23, x24
   39c90:	sbc	x21, x21, x22
   39c94:	cmp	x21, #0x2
   39c98:	b.cs	39ca4 <__gmpn_hgcd2_jacobi@@Base+0x9c>  // b.hs, b.nlast
   39c9c:	mov	w0, wzr
   39ca0:	b	3a03c <__gmpn_hgcd2_jacobi@@Base+0x434>
   39ca4:	mov	x25, xzr
   39ca8:	mov	w9, #0x1                   	// #1
   39cac:	mov	x23, x10
   39cb0:	mov	w26, #0x1                   	// #1
   39cb4:	stp	x4, x5, [sp]
   39cb8:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   39cbc:	ldr	x20, [x20, #3872]
   39cc0:	bfi	w9, w8, #3, #29
   39cc4:	mov	w27, #0x1                   	// #1
   39cc8:	cmp	x22, x21
   39ccc:	ldrb	w19, [x20, w9, uxtw]
   39cd0:	mov	w28, #0x1                   	// #1
   39cd4:	b.cc	39d68 <__gmpn_hgcd2_jacobi@@Base+0x160>  // b.lo, b.ul, b.last
   39cd8:	cmp	x22, x21
   39cdc:	b.eq	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   39ce0:	lsr	x8, x22, #32
   39ce4:	cbz	x8, 39df0 <__gmpn_hgcd2_jacobi@@Base+0x1e8>
   39ce8:	subs	x2, x24, x23
   39cec:	sbc	x22, x22, x21
   39cf0:	cmp	x22, #0x2
   39cf4:	b.cc	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.lo, b.ul, b.last
   39cf8:	cmp	x22, x21
   39cfc:	b.ls	39d44 <__gmpn_hgcd2_jacobi@@Base+0x13c>  // b.plast
   39d00:	add	x0, sp, #0x10
   39d04:	mov	x1, x22
   39d08:	mov	x3, x21
   39d0c:	mov	x4, x23
   39d10:	bl	3a05c <__gmpn_hgcd2_jacobi@@Base+0x454>
   39d14:	ldr	x22, [sp, #24]
   39d18:	cmp	x22, #0x2
   39d1c:	b.cc	39e08 <__gmpn_hgcd2_jacobi@@Base+0x200>  // b.lo, b.ul, b.last
   39d20:	add	x8, x0, #0x1
   39d24:	and	w9, w8, #0x3
   39d28:	bfi	w9, w19, #3, #8
   39d2c:	orr	x9, x9, #0x4
   39d30:	ldr	x2, [sp, #16]
   39d34:	ldrb	w19, [x20, x9]
   39d38:	mul	x9, x8, x26
   39d3c:	mul	x8, x8, x27
   39d40:	b	39d5c <__gmpn_hgcd2_jacobi@@Base+0x154>
   39d44:	lsl	w8, w19, #3
   39d48:	mov	w9, #0x5                   	// #5
   39d4c:	orr	x8, x8, x9
   39d50:	ldrb	w19, [x20, x8]
   39d54:	mov	x8, x27
   39d58:	mov	x9, x26
   39d5c:	add	x28, x9, x28
   39d60:	add	x25, x8, x25
   39d64:	mov	x24, x2
   39d68:	cmp	x22, x21
   39d6c:	b.eq	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   39d70:	lsr	x8, x21, #32
   39d74:	cbz	x8, 39dfc <__gmpn_hgcd2_jacobi@@Base+0x1f4>
   39d78:	mov	x8, x23
   39d7c:	subs	x23, x8, x24
   39d80:	sbc	x21, x21, x22
   39d84:	cmp	x21, #0x2
   39d88:	b.cc	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.lo, b.ul, b.last
   39d8c:	cmp	x21, x22
   39d90:	b.ls	39dd8 <__gmpn_hgcd2_jacobi@@Base+0x1d0>  // b.plast
   39d94:	add	x0, sp, #0x10
   39d98:	mov	x1, x21
   39d9c:	mov	x2, x23
   39da0:	mov	x3, x22
   39da4:	mov	x4, x24
   39da8:	bl	3a05c <__gmpn_hgcd2_jacobi@@Base+0x454>
   39dac:	ldr	x21, [sp, #24]
   39db0:	cmp	x21, #0x2
   39db4:	b.cc	39e24 <__gmpn_hgcd2_jacobi@@Base+0x21c>  // b.lo, b.ul, b.last
   39db8:	add	x8, x0, #0x1
   39dbc:	and	w9, w8, #0x3
   39dc0:	bfi	w9, w19, #3, #29
   39dc4:	ldr	x23, [sp, #16]
   39dc8:	ldrb	w19, [x20, w9, uxtw]
   39dcc:	madd	x26, x8, x28, x26
   39dd0:	madd	x27, x8, x25, x27
   39dd4:	b	39cd8 <__gmpn_hgcd2_jacobi@@Base+0xd0>
   39dd8:	lsl	w8, w19, #3
   39ddc:	orr	x8, x8, #0x1
   39de0:	ldrb	w19, [x20, x8]
   39de4:	add	x27, x25, x27
   39de8:	add	x26, x28, x26
   39dec:	b	39cd8 <__gmpn_hgcd2_jacobi@@Base+0xd0>
   39df0:	extr	x8, x22, x24, #32
   39df4:	extr	x9, x21, x23, #32
   39df8:	b	39e3c <__gmpn_hgcd2_jacobi@@Base+0x234>
   39dfc:	extr	x8, x22, x24, #32
   39e00:	extr	x9, x21, x23, #32
   39e04:	b	39f1c <__gmpn_hgcd2_jacobi@@Base+0x314>
   39e08:	and	w8, w0, #0x3
   39e0c:	bfi	w8, w19, #3, #8
   39e10:	orr	x8, x8, #0x4
   39e14:	ldrb	w19, [x20, x8]
   39e18:	madd	x28, x0, x26, x28
   39e1c:	madd	x25, x0, x27, x25
   39e20:	b	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>
   39e24:	and	w8, w0, #0x3
   39e28:	bfi	w8, w19, #3, #29
   39e2c:	ldrb	w19, [x20, w8, uxtw]
   39e30:	madd	x26, x0, x28, x26
   39e34:	madd	x27, x0, x25, x27
   39e38:	b	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>
   39e3c:	subs	x8, x8, x9
   39e40:	b.eq	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   39e44:	lsr	x10, x8, #33
   39e48:	cbz	x10, 3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>
   39e4c:	cmp	x8, x9
   39e50:	b.ls	39e9c <__gmpn_hgcd2_jacobi@@Base+0x294>  // b.plast
   39e54:	tbnz	x8, #63, 39eb0 <__gmpn_hgcd2_jacobi@@Base+0x2a8>
   39e58:	mov	w11, wzr
   39e5c:	mov	x12, x9
   39e60:	lsl	x12, x12, #1
   39e64:	cmp	x12, x8
   39e68:	sub	w11, w11, #0x1
   39e6c:	b.ls	39e60 <__gmpn_hgcd2_jacobi@@Base+0x258>  // b.plast
   39e70:	mov	x13, xzr
   39e74:	lsr	x12, x12, #1
   39e78:	cmp	x8, x12
   39e7c:	cset	w10, cs  // cs = hs, nlast
   39e80:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   39e84:	bfi	x10, x13, #1, #63
   39e88:	adds	w11, w11, #0x1
   39e8c:	sub	x8, x8, x14
   39e90:	mov	x13, x10
   39e94:	b.cc	39e74 <__gmpn_hgcd2_jacobi@@Base+0x26c>  // b.lo, b.ul, b.last
   39e98:	b	39ef0 <__gmpn_hgcd2_jacobi@@Base+0x2e8>
   39e9c:	mov	w10, #0x5                   	// #5
   39ea0:	bfi	w10, w19, #3, #8
   39ea4:	mov	x11, x27
   39ea8:	mov	x12, x26
   39eac:	b	39f10 <__gmpn_hgcd2_jacobi@@Base+0x308>
   39eb0:	mov	w11, #0x1                   	// #1
   39eb4:	mov	x12, x9
   39eb8:	tbnz	x9, #63, 39ec8 <__gmpn_hgcd2_jacobi@@Base+0x2c0>
   39ebc:	lsl	x12, x12, #1
   39ec0:	add	w11, w11, #0x1
   39ec4:	tbz	x12, #63, 39ebc <__gmpn_hgcd2_jacobi@@Base+0x2b4>
   39ec8:	mov	x13, xzr
   39ecc:	cmp	x8, x12
   39ed0:	cset	w10, cs  // cs = hs, nlast
   39ed4:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   39ed8:	bfi	x10, x13, #1, #63
   39edc:	sub	x8, x8, x14
   39ee0:	subs	w11, w11, #0x1
   39ee4:	lsr	x12, x12, #1
   39ee8:	mov	x13, x10
   39eec:	b.ne	39ecc <__gmpn_hgcd2_jacobi@@Base+0x2c4>  // b.any
   39ef0:	lsr	x11, x8, #33
   39ef4:	cbz	x11, 39ff8 <__gmpn_hgcd2_jacobi@@Base+0x3f0>
   39ef8:	add	x11, x10, #0x1
   39efc:	and	w10, w11, #0x3
   39f00:	bfi	w10, w19, #3, #8
   39f04:	mul	x12, x11, x26
   39f08:	orr	w10, w10, #0x4
   39f0c:	mul	x11, x11, x27
   39f10:	ldrb	w19, [x20, w10, uxtw]
   39f14:	add	x28, x12, x28
   39f18:	add	x25, x11, x25
   39f1c:	subs	x9, x9, x8
   39f20:	b.eq	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   39f24:	lsr	x10, x9, #33
   39f28:	cbz	x10, 3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>
   39f2c:	cmp	x9, x8
   39f30:	b.ls	39f7c <__gmpn_hgcd2_jacobi@@Base+0x374>  // b.plast
   39f34:	tbnz	x9, #63, 39f94 <__gmpn_hgcd2_jacobi@@Base+0x38c>
   39f38:	mov	w10, wzr
   39f3c:	mov	x12, x8
   39f40:	lsl	x12, x12, #1
   39f44:	cmp	x12, x9
   39f48:	sub	w10, w10, #0x1
   39f4c:	b.ls	39f40 <__gmpn_hgcd2_jacobi@@Base+0x338>  // b.plast
   39f50:	mov	x13, xzr
   39f54:	lsr	x12, x12, #1
   39f58:	cmp	x9, x12
   39f5c:	cset	w11, cs  // cs = hs, nlast
   39f60:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   39f64:	bfi	x11, x13, #1, #63
   39f68:	adds	w10, w10, #0x1
   39f6c:	sub	x9, x9, x14
   39f70:	mov	x13, x11
   39f74:	b.cc	39f54 <__gmpn_hgcd2_jacobi@@Base+0x34c>  // b.lo, b.ul, b.last
   39f78:	b	39fd4 <__gmpn_hgcd2_jacobi@@Base+0x3cc>
   39f7c:	lsl	w10, w19, #3
   39f80:	orr	x10, x10, #0x1
   39f84:	ldrb	w19, [x20, x10]
   39f88:	add	x27, x25, x27
   39f8c:	add	x26, x28, x26
   39f90:	b	39e3c <__gmpn_hgcd2_jacobi@@Base+0x234>
   39f94:	mov	w10, #0x1                   	// #1
   39f98:	mov	x12, x8
   39f9c:	tbnz	x8, #63, 39fac <__gmpn_hgcd2_jacobi@@Base+0x3a4>
   39fa0:	lsl	x12, x12, #1
   39fa4:	add	w10, w10, #0x1
   39fa8:	tbz	x12, #63, 39fa0 <__gmpn_hgcd2_jacobi@@Base+0x398>
   39fac:	mov	x13, xzr
   39fb0:	cmp	x9, x12
   39fb4:	cset	w11, cs  // cs = hs, nlast
   39fb8:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   39fbc:	bfi	x11, x13, #1, #63
   39fc0:	sub	x9, x9, x14
   39fc4:	subs	w10, w10, #0x1
   39fc8:	lsr	x12, x12, #1
   39fcc:	mov	x13, x11
   39fd0:	b.ne	39fb0 <__gmpn_hgcd2_jacobi@@Base+0x3a8>  // b.any
   39fd4:	lsr	x10, x9, #33
   39fd8:	cbz	x10, 3a014 <__gmpn_hgcd2_jacobi@@Base+0x40c>
   39fdc:	add	x10, x11, #0x1
   39fe0:	and	w11, w10, #0x3
   39fe4:	bfi	w11, w19, #3, #29
   39fe8:	ldrb	w19, [x20, w11, uxtw]
   39fec:	madd	x26, x10, x28, x26
   39ff0:	madd	x27, x10, x25, x27
   39ff4:	b	39e3c <__gmpn_hgcd2_jacobi@@Base+0x234>
   39ff8:	and	w8, w10, #0x3
   39ffc:	bfi	w8, w19, #3, #8
   3a000:	orr	x8, x8, #0x4
   3a004:	ldrb	w19, [x20, x8]
   3a008:	madd	x28, x10, x26, x28
   3a00c:	madd	x25, x10, x27, x25
   3a010:	b	3a028 <__gmpn_hgcd2_jacobi@@Base+0x420>
   3a014:	and	w8, w11, #0x3
   3a018:	bfi	w8, w19, #3, #29
   3a01c:	ldrb	w19, [x20, w8, uxtw]
   3a020:	madd	x26, x11, x28, x26
   3a024:	madd	x27, x11, x25, x27
   3a028:	ldp	x9, x8, [sp]
   3a02c:	mov	w0, #0x1                   	// #1
   3a030:	stp	x27, x25, [x9]
   3a034:	stp	x26, x28, [x9, #16]
   3a038:	str	w19, [x8]
   3a03c:	ldp	x20, x19, [sp, #112]
   3a040:	ldp	x22, x21, [sp, #96]
   3a044:	ldp	x24, x23, [sp, #80]
   3a048:	ldp	x26, x25, [sp, #64]
   3a04c:	ldp	x28, x27, [sp, #48]
   3a050:	ldp	x29, x30, [sp, #32]
   3a054:	add	sp, sp, #0x80
   3a058:	ret
   3a05c:	tbnz	x1, #63, 3a0d8 <__gmpn_hgcd2_jacobi@@Base+0x4d0>
   3a060:	mov	w9, wzr
   3a064:	b	3a074 <__gmpn_hgcd2_jacobi@@Base+0x46c>
   3a068:	extr	x3, x3, x4, #63
   3a06c:	lsl	x4, x4, #1
   3a070:	sub	w9, w9, #0x1
   3a074:	cmp	x3, x1
   3a078:	b.cc	3a068 <__gmpn_hgcd2_jacobi@@Base+0x460>  // b.lo, b.ul, b.last
   3a07c:	b.ne	3a088 <__gmpn_hgcd2_jacobi@@Base+0x480>  // b.any
   3a080:	cmp	x4, x2
   3a084:	b.ls	3a068 <__gmpn_hgcd2_jacobi@@Base+0x460>  // b.plast
   3a088:	mov	x8, xzr
   3a08c:	cbnz	w9, 3a0b4 <__gmpn_hgcd2_jacobi@@Base+0x4ac>
   3a090:	stp	x2, x1, [x0]
   3a094:	mov	x0, x8
   3a098:	ret
   3a09c:	subs	x10, x2, x4
   3a0a0:	sbc	x1, x1, x3
   3a0a4:	orr	x8, x8, #0x1
   3a0a8:	mov	x2, x10
   3a0ac:	adds	w9, w9, #0x1
   3a0b0:	b.cs	3a090 <__gmpn_hgcd2_jacobi@@Base+0x488>  // b.hs, b.nlast
   3a0b4:	extr	x4, x3, x4, #1
   3a0b8:	lsr	x3, x3, #1
   3a0bc:	cmp	x1, x3
   3a0c0:	lsl	x8, x8, #1
   3a0c4:	b.hi	3a09c <__gmpn_hgcd2_jacobi@@Base+0x494>  // b.pmore
   3a0c8:	b.ne	3a0ac <__gmpn_hgcd2_jacobi@@Base+0x4a4>  // b.any
   3a0cc:	cmp	x2, x4
   3a0d0:	b.cs	3a09c <__gmpn_hgcd2_jacobi@@Base+0x494>  // b.hs, b.nlast
   3a0d4:	b	3a0ac <__gmpn_hgcd2_jacobi@@Base+0x4a4>
   3a0d8:	mov	w9, #0x1                   	// #1
   3a0dc:	tbnz	x3, #63, 3a0f0 <__gmpn_hgcd2_jacobi@@Base+0x4e8>
   3a0e0:	extr	x3, x3, x4, #63
   3a0e4:	lsl	x4, x4, #1
   3a0e8:	add	w9, w9, #0x1
   3a0ec:	tbz	x3, #63, 3a0e0 <__gmpn_hgcd2_jacobi@@Base+0x4d8>
   3a0f0:	mov	x8, xzr
   3a0f4:	b	3a118 <__gmpn_hgcd2_jacobi@@Base+0x510>
   3a0f8:	subs	x10, x2, x4
   3a0fc:	sbc	x1, x1, x3
   3a100:	orr	x8, x8, #0x1
   3a104:	mov	x2, x10
   3a108:	extr	x4, x3, x4, #1
   3a10c:	subs	w9, w9, #0x1
   3a110:	lsr	x3, x3, #1
   3a114:	b.eq	3a090 <__gmpn_hgcd2_jacobi@@Base+0x488>  // b.none
   3a118:	cmp	x1, x3
   3a11c:	lsl	x8, x8, #1
   3a120:	b.hi	3a0f8 <__gmpn_hgcd2_jacobi@@Base+0x4f0>  // b.pmore
   3a124:	b.ne	3a108 <__gmpn_hgcd2_jacobi@@Base+0x500>  // b.any
   3a128:	cmp	x2, x4
   3a12c:	b.cs	3a0f8 <__gmpn_hgcd2_jacobi@@Base+0x4f0>  // b.hs, b.nlast
   3a130:	b	3a108 <__gmpn_hgcd2_jacobi@@Base+0x500>

000000000003a134 <__gmpn_hgcd_jacobi@@Base>:
   3a134:	sub	sp, sp, #0xa0
   3a138:	cmp	x2, #0x0
   3a13c:	cinc	x8, x2, lt  // lt = tstop
   3a140:	stp	x26, x25, [sp, #96]
   3a144:	asr	x26, x8, #1
   3a148:	stp	x24, x23, [sp, #112]
   3a14c:	add	x23, x26, #0x1
   3a150:	cmp	x23, x2
   3a154:	stp	x29, x30, [sp, #64]
   3a158:	stp	x28, x27, [sp, #80]
   3a15c:	stp	x22, x21, [sp, #128]
   3a160:	stp	x20, x19, [sp, #144]
   3a164:	add	x29, sp, #0x40
   3a168:	b.ge	3a1f0 <__gmpn_hgcd_jacobi@@Base+0xbc>  // b.tcont
   3a16c:	mov	x19, x5
   3a170:	mov	x20, x4
   3a174:	mov	x21, x3
   3a178:	mov	x25, x2
   3a17c:	mov	x22, x1
   3a180:	mov	x24, x0
   3a184:	cmp	x2, #0x64
   3a188:	b.le	3a1f8 <__gmpn_hgcd_jacobi@@Base+0xc4>
   3a18c:	add	x8, x25, x25, lsl #1
   3a190:	lsl	x9, x26, #3
   3a194:	add	x10, x8, #0x3
   3a198:	cmp	x8, #0x0
   3a19c:	add	x0, x24, x9
   3a1a0:	add	x1, x22, x9
   3a1a4:	csel	x8, x10, x8, lt  // lt = tstop
   3a1a8:	sub	x2, x25, x26
   3a1ac:	mov	x3, x21
   3a1b0:	mov	x4, x20
   3a1b4:	mov	x5, x19
   3a1b8:	asr	x27, x8, #2
   3a1bc:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   3a1c0:	cmp	x0, #0x1
   3a1c4:	b.lt	3a24c <__gmpn_hgcd_jacobi@@Base+0x118>  // b.tstop
   3a1c8:	add	x1, x0, x26
   3a1cc:	mov	x0, x21
   3a1d0:	mov	x2, x24
   3a1d4:	mov	x3, x22
   3a1d8:	mov	x4, x26
   3a1dc:	mov	x5, x19
   3a1e0:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   3a1e4:	mov	x25, x0
   3a1e8:	mov	w8, #0x1                   	// #1
   3a1ec:	b	3a250 <__gmpn_hgcd_jacobi@@Base+0x11c>
   3a1f0:	mov	x0, xzr
   3a1f4:	b	3a298 <__gmpn_hgcd_jacobi@@Base+0x164>
   3a1f8:	mov	w27, wzr
   3a1fc:	mov	x0, x25
   3a200:	mov	x1, x24
   3a204:	mov	x2, x22
   3a208:	mov	x3, x23
   3a20c:	mov	x4, x21
   3a210:	mov	x5, x20
   3a214:	mov	x6, x19
   3a218:	bl	3a370 <__gmpn_hgcd_jacobi@@Base+0x23c>
   3a21c:	cbz	x0, 3a290 <__gmpn_hgcd_jacobi@@Base+0x15c>
   3a220:	mov	x1, x24
   3a224:	mov	x2, x22
   3a228:	mov	x3, x23
   3a22c:	mov	x4, x21
   3a230:	mov	x5, x20
   3a234:	mov	x6, x19
   3a238:	mov	x25, x0
   3a23c:	bl	3a370 <__gmpn_hgcd_jacobi@@Base+0x23c>
   3a240:	cbnz	x0, 3a220 <__gmpn_hgcd_jacobi@@Base+0xec>
   3a244:	mov	w27, #0x1                   	// #1
   3a248:	b	3a290 <__gmpn_hgcd_jacobi@@Base+0x15c>
   3a24c:	mov	w8, wzr
   3a250:	add	x28, x27, #0x1
   3a254:	mov	x0, x25
   3a258:	mov	x25, x0
   3a25c:	cmp	x0, x28
   3a260:	mov	w27, w8
   3a264:	b.le	3a2b8 <__gmpn_hgcd_jacobi@@Base+0x184>
   3a268:	mov	x0, x25
   3a26c:	mov	x1, x24
   3a270:	mov	x2, x22
   3a274:	mov	x3, x23
   3a278:	mov	x4, x21
   3a27c:	mov	x5, x20
   3a280:	mov	x6, x19
   3a284:	bl	3a370 <__gmpn_hgcd_jacobi@@Base+0x23c>
   3a288:	mov	w8, #0x1                   	// #1
   3a28c:	cbnz	x0, 3a258 <__gmpn_hgcd_jacobi@@Base+0x124>
   3a290:	cmp	w27, #0x0
   3a294:	csel	x0, xzr, x25, eq  // eq = none
   3a298:	ldp	x20, x19, [sp, #144]
   3a29c:	ldp	x22, x21, [sp, #128]
   3a2a0:	ldp	x24, x23, [sp, #112]
   3a2a4:	ldp	x26, x25, [sp, #96]
   3a2a8:	ldp	x28, x27, [sp, #80]
   3a2ac:	ldp	x29, x30, [sp, #64]
   3a2b0:	add	sp, sp, #0xa0
   3a2b4:	ret
   3a2b8:	add	x8, x26, #0x3
   3a2bc:	cmp	x25, x8
   3a2c0:	b.le	3a1fc <__gmpn_hgcd_jacobi@@Base+0xc8>
   3a2c4:	lsl	x8, x23, #1
   3a2c8:	sub	x8, x8, x25
   3a2cc:	add	x26, x8, #0x1
   3a2d0:	sub	x28, x25, x26
   3a2d4:	add	x8, x28, #0x1
   3a2d8:	add	x9, x28, #0x2
   3a2dc:	cmp	x8, #0x0
   3a2e0:	csinc	x8, x9, x28, lt  // lt = tstop
   3a2e4:	lsl	x8, x8, #4
   3a2e8:	and	x8, x8, #0xffffffffffffffe0
   3a2ec:	add	x0, sp, #0x10
   3a2f0:	mov	x1, x28
   3a2f4:	mov	x2, x19
   3a2f8:	str	x8, [sp, #8]
   3a2fc:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   3a300:	ldr	x9, [sp, #8]
   3a304:	lsl	x8, x26, #3
   3a308:	add	x0, x24, x8
   3a30c:	add	x1, x22, x8
   3a310:	add	x9, x9, x19
   3a314:	add	x8, x9, #0x20
   3a318:	add	x3, sp, #0x10
   3a31c:	mov	x2, x28
   3a320:	mov	x4, x20
   3a324:	mov	x5, x8
   3a328:	mov	x28, x8
   3a32c:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   3a330:	cmp	x0, #0x1
   3a334:	b.lt	3a1fc <__gmpn_hgcd_jacobi@@Base+0xc8>  // b.tstop
   3a338:	add	x1, x0, x26
   3a33c:	add	x0, sp, #0x10
   3a340:	mov	x2, x24
   3a344:	mov	x3, x22
   3a348:	mov	x4, x26
   3a34c:	mov	x5, x28
   3a350:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   3a354:	mov	x25, x0
   3a358:	add	x1, sp, #0x10
   3a35c:	mov	x0, x21
   3a360:	mov	x2, x28
   3a364:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   3a368:	mov	w27, #0x1                   	// #1
   3a36c:	b	3a1fc <__gmpn_hgcd_jacobi@@Base+0xc8>
   3a370:	sub	sp, sp, #0x80
   3a374:	lsl	x8, x0, #3
   3a378:	stp	x29, x30, [sp, #48]
   3a37c:	stp	x24, x23, [sp, #80]
   3a380:	stp	x22, x21, [sp, #96]
   3a384:	stp	x20, x19, [sp, #112]
   3a388:	sub	x9, x8, #0x8
   3a38c:	mov	x20, x2
   3a390:	mov	x21, x0
   3a394:	ldr	x0, [x1, x9]
   3a398:	ldr	x2, [x2, x9]
   3a39c:	add	x9, x3, #0x1
   3a3a0:	str	x25, [sp, #64]
   3a3a4:	mov	x19, x6
   3a3a8:	mov	x25, x5
   3a3ac:	mov	x23, x4
   3a3b0:	mov	x22, x1
   3a3b4:	mov	x24, x3
   3a3b8:	cmp	x9, x21
   3a3bc:	orr	x9, x2, x0
   3a3c0:	add	x29, sp, #0x30
   3a3c4:	b.ne	3a3d4 <__gmpn_hgcd_jacobi@@Base+0x2a0>  // b.any
   3a3c8:	cmp	x9, #0x4
   3a3cc:	b.cs	3a474 <__gmpn_hgcd_jacobi@@Base+0x340>  // b.hs, b.nlast
   3a3d0:	b	3a490 <__gmpn_hgcd_jacobi@@Base+0x35c>
   3a3d4:	tbnz	x9, #63, 3a474 <__gmpn_hgcd_jacobi@@Base+0x340>
   3a3d8:	sub	x10, x8, #0x10
   3a3dc:	sub	x8, x8, #0x18
   3a3e0:	ldr	x12, [x22, x10]
   3a3e4:	ldr	x14, [x22, x8]
   3a3e8:	ldr	x10, [x20, x10]
   3a3ec:	ldr	x8, [x20, x8]
   3a3f0:	clz	x9, x9
   3a3f4:	neg	x13, x9
   3a3f8:	lsl	x11, x0, x9
   3a3fc:	lsl	x15, x2, x9
   3a400:	lsr	x16, x12, x13
   3a404:	lsl	x12, x12, x9
   3a408:	lsr	x14, x14, x13
   3a40c:	lsl	x9, x10, x9
   3a410:	lsr	x10, x10, x13
   3a414:	lsr	x8, x8, x13
   3a418:	orr	x0, x16, x11
   3a41c:	orr	x1, x14, x12
   3a420:	orr	x2, x10, x15
   3a424:	orr	x3, x8, x9
   3a428:	add	x4, sp, #0x10
   3a42c:	mov	x5, x25
   3a430:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   3a434:	cbz	w0, 3a490 <__gmpn_hgcd_jacobi@@Base+0x35c>
   3a438:	add	x1, sp, #0x10
   3a43c:	mov	x0, x23
   3a440:	mov	x2, x19
   3a444:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   3a448:	mov	x0, x19
   3a44c:	mov	x1, x22
   3a450:	mov	x2, x21
   3a454:	bl	ca50 <__gmpn_copyi@plt>
   3a458:	add	x0, sp, #0x10
   3a45c:	mov	x1, x22
   3a460:	mov	x2, x19
   3a464:	mov	x3, x20
   3a468:	mov	x4, x21
   3a46c:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   3a470:	b	3a4b8 <__gmpn_hgcd_jacobi@@Base+0x384>
   3a474:	sub	x8, x8, #0x10
   3a478:	ldr	x1, [x22, x8]
   3a47c:	ldr	x3, [x20, x8]
   3a480:	add	x4, sp, #0x10
   3a484:	mov	x5, x25
   3a488:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   3a48c:	cbnz	w0, 3a438 <__gmpn_hgcd_jacobi@@Base+0x304>
   3a490:	adrp	x4, 3a000 <__gmpn_hgcd2_jacobi@@Base+0x3f8>
   3a494:	add	x4, x4, #0x4d4
   3a498:	mov	x5, sp
   3a49c:	mov	x0, x22
   3a4a0:	mov	x1, x20
   3a4a4:	mov	x2, x21
   3a4a8:	mov	x3, x24
   3a4ac:	mov	x6, x19
   3a4b0:	stp	x23, x25, [sp]
   3a4b4:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   3a4b8:	ldp	x20, x19, [sp, #112]
   3a4bc:	ldp	x22, x21, [sp, #96]
   3a4c0:	ldp	x24, x23, [sp, #80]
   3a4c4:	ldr	x25, [sp, #64]
   3a4c8:	ldp	x29, x30, [sp, #48]
   3a4cc:	add	sp, sp, #0x80
   3a4d0:	ret
   3a4d4:	stp	x29, x30, [sp, #-48]!
   3a4d8:	add	x9, x3, x4, lsl #3
   3a4dc:	str	x21, [sp, #16]
   3a4e0:	stp	x20, x19, [sp, #32]
   3a4e4:	mov	w19, w5
   3a4e8:	mov	x8, x4
   3a4ec:	mov	x20, x3
   3a4f0:	mov	x21, x0
   3a4f4:	add	x4, x9, #0x8
   3a4f8:	mov	x29, sp
   3a4fc:	subs	x8, x8, #0x1
   3a500:	b.lt	3a54c <__gmpn_hgcd_jacobi@@Base+0x418>  // b.tstop
   3a504:	ldur	x9, [x4, #-16]
   3a508:	sub	x4, x4, #0x8
   3a50c:	cbz	x9, 3a4fc <__gmpn_hgcd_jacobi@@Base+0x3c8>
   3a510:	ldr	x0, [x21]
   3a514:	add	x2, x8, #0x1
   3a518:	mov	x1, x20
   3a51c:	mov	w3, w19
   3a520:	bl	d020 <__gmpn_hgcd_matrix_update_q@plt>
   3a524:	ldr	x8, [x21, #8]
   3a528:	ldr	w10, [x20]
   3a52c:	ldr	w9, [x8]
   3a530:	lsl	w9, w9, #3
   3a534:	add	w9, w9, w19, lsl #2
   3a538:	bfxil	w9, w10, #0, #2
   3a53c:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   3a540:	ldr	x10, [x10, #3872]
   3a544:	ldrb	w9, [x10, w9, uxtw]
   3a548:	str	w9, [x8]
   3a54c:	ldp	x20, x19, [sp, #32]
   3a550:	ldr	x21, [sp, #16]
   3a554:	ldp	x29, x30, [sp], #48
   3a558:	ret

000000000003a55c <__gmpn_mullo_n@@Base>:
   3a55c:	stp	x29, x30, [sp, #-64]!
   3a560:	stp	x22, x21, [sp, #32]
   3a564:	stp	x20, x19, [sp, #48]
   3a568:	mov	x19, x3
   3a56c:	mov	x20, x2
   3a570:	mov	x22, x1
   3a574:	cmp	x3, #0x25
   3a578:	mov	x21, x0
   3a57c:	str	x23, [sp, #16]
   3a580:	mov	x29, sp
   3a584:	b.le	3a5e8 <__gmpn_mullo_n@@Base+0x8c>
   3a588:	lsl	x1, x19, #4
   3a58c:	mov	w8, #0x7f00                	// #32512
   3a590:	cmp	x1, x8
   3a594:	str	xzr, [x29, #24]
   3a598:	b.hi	3a610 <__gmpn_mullo_n@@Base+0xb4>  // b.pmore
   3a59c:	add	x9, x1, #0xf
   3a5a0:	mov	x8, sp
   3a5a4:	and	x9, x9, #0xfffffffffffffff0
   3a5a8:	sub	x23, x8, x9
   3a5ac:	mov	sp, x23
   3a5b0:	mov	w8, #0x186c                	// #6252
   3a5b4:	cmp	x19, x8
   3a5b8:	b.le	3a628 <__gmpn_mullo_n@@Base+0xcc>
   3a5bc:	mov	x0, x23
   3a5c0:	mov	x1, x22
   3a5c4:	mov	x2, x19
   3a5c8:	mov	x3, x20
   3a5cc:	mov	x4, x19
   3a5d0:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   3a5d4:	mov	x0, x21
   3a5d8:	mov	x1, x23
   3a5dc:	mov	x2, x19
   3a5e0:	bl	ca50 <__gmpn_copyi@plt>
   3a5e4:	b	3a640 <__gmpn_mullo_n@@Base+0xe4>
   3a5e8:	mov	x0, x21
   3a5ec:	mov	x1, x22
   3a5f0:	mov	x2, x20
   3a5f4:	mov	x3, x19
   3a5f8:	mov	sp, x29
   3a5fc:	ldp	x20, x19, [sp, #48]
   3a600:	ldp	x22, x21, [sp, #32]
   3a604:	ldr	x23, [sp, #16]
   3a608:	ldp	x29, x30, [sp], #64
   3a60c:	b	d290 <__gmpn_mullo_basecase@plt>
   3a610:	add	x0, x29, #0x18
   3a614:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3a618:	mov	x23, x0
   3a61c:	mov	w8, #0x186c                	// #6252
   3a620:	cmp	x19, x8
   3a624:	b.gt	3a5bc <__gmpn_mullo_n@@Base+0x60>
   3a628:	mov	x0, x21
   3a62c:	mov	x1, x22
   3a630:	mov	x2, x20
   3a634:	mov	x3, x19
   3a638:	mov	x4, x23
   3a63c:	bl	3a668 <__gmpn_mullo_n@@Base+0x10c>
   3a640:	ldr	x0, [x29, #24]
   3a644:	cbnz	x0, 3a660 <__gmpn_mullo_n@@Base+0x104>
   3a648:	mov	sp, x29
   3a64c:	ldp	x20, x19, [sp, #48]
   3a650:	ldp	x22, x21, [sp, #32]
   3a654:	ldr	x23, [sp, #16]
   3a658:	ldp	x29, x30, [sp], #64
   3a65c:	ret
   3a660:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3a664:	b	3a648 <__gmpn_mullo_n@@Base+0xec>
   3a668:	stp	x29, x30, [sp, #-80]!
   3a66c:	stp	x24, x23, [sp, #32]
   3a670:	stp	x22, x21, [sp, #48]
   3a674:	stp	x20, x19, [sp, #64]
   3a678:	mov	x21, x4
   3a67c:	mov	x24, x3
   3a680:	mov	x20, x2
   3a684:	mov	x19, x1
   3a688:	cmp	x3, #0x45
   3a68c:	mov	x23, x0
   3a690:	str	x25, [sp, #16]
   3a694:	mov	x29, sp
   3a698:	b.le	3a6c0 <__gmpn_mullo_n@@Base+0x164>
   3a69c:	cmp	x24, #0x68
   3a6a0:	b.le	3a6e4 <__gmpn_mullo_n@@Base+0x188>
   3a6a4:	cmp	x24, #0x105
   3a6a8:	b.le	3a700 <__gmpn_mullo_n@@Base+0x1a4>
   3a6ac:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   3a6b0:	movk	x8, #0xcccd
   3a6b4:	umulh	x8, x24, x8
   3a6b8:	lsr	x22, x8, #3
   3a6bc:	b	3a728 <__gmpn_mullo_n@@Base+0x1cc>
   3a6c0:	mov	x9, #0xe38f                	// #58255
   3a6c4:	movk	x9, #0x8e38, lsl #16
   3a6c8:	mov	w8, #0xb                   	// #11
   3a6cc:	movk	x9, #0x38e3, lsl #32
   3a6d0:	mul	x8, x24, x8
   3a6d4:	movk	x9, #0xe38e, lsl #48
   3a6d8:	umulh	x8, x8, x9
   3a6dc:	lsr	x22, x8, #5
   3a6e0:	b	3a728 <__gmpn_mullo_n@@Base+0x1cc>
   3a6e4:	add	w8, w24, w24, lsl #3
   3a6e8:	mov	w9, #0xcccd                	// #52429
   3a6ec:	and	w8, w8, #0xffff
   3a6f0:	movk	w9, #0xcccc, lsl #16
   3a6f4:	umull	x8, w8, w9
   3a6f8:	lsr	x22, x8, #37
   3a6fc:	b	3a728 <__gmpn_mullo_n@@Base+0x1cc>
   3a700:	lsl	w8, w24, #3
   3a704:	sub	w8, w8, w24
   3a708:	mov	w9, #0x41a5                	// #16805
   3a70c:	and	w8, w8, #0xffff
   3a710:	movk	w9, #0xa41a, lsl #16
   3a714:	umull	x9, w8, w9
   3a718:	lsr	x9, x9, #32
   3a71c:	sub	w8, w8, w9
   3a720:	add	w8, w9, w8, lsr #1
   3a724:	lsr	w22, w8, #5
   3a728:	sub	x25, x24, x22
   3a72c:	mov	x0, x21
   3a730:	mov	x1, x19
   3a734:	mov	x2, x20
   3a738:	mov	x3, x25
   3a73c:	bl	c990 <__gmpn_mul_n@plt>
   3a740:	mov	x0, x23
   3a744:	mov	x1, x21
   3a748:	mov	x2, x25
   3a74c:	bl	ca50 <__gmpn_copyi@plt>
   3a750:	add	x24, x21, x24, lsl #3
   3a754:	cmp	x22, #0x25
   3a758:	add	x1, x19, x25, lsl #3
   3a75c:	mov	x0, x24
   3a760:	mov	x2, x20
   3a764:	mov	x3, x22
   3a768:	b.ls	3a778 <__gmpn_mullo_n@@Base+0x21c>  // b.plast
   3a76c:	mov	x4, x24
   3a770:	bl	3a668 <__gmpn_mullo_n@@Base+0x10c>
   3a774:	b	3a77c <__gmpn_mullo_n@@Base+0x220>
   3a778:	bl	d290 <__gmpn_mullo_basecase@plt>
   3a77c:	lsl	x25, x25, #3
   3a780:	add	x23, x23, x25
   3a784:	add	x1, x21, x25
   3a788:	mov	x0, x23
   3a78c:	mov	x2, x24
   3a790:	mov	x3, x22
   3a794:	bl	ca70 <__gmpn_add_n@plt>
   3a798:	cmp	x22, #0x25
   3a79c:	add	x2, x20, x25
   3a7a0:	mov	x0, x24
   3a7a4:	mov	x1, x19
   3a7a8:	mov	x3, x22
   3a7ac:	b.ls	3a7bc <__gmpn_mullo_n@@Base+0x260>  // b.plast
   3a7b0:	mov	x4, x24
   3a7b4:	bl	3a668 <__gmpn_mullo_n@@Base+0x10c>
   3a7b8:	b	3a7c0 <__gmpn_mullo_n@@Base+0x264>
   3a7bc:	bl	d290 <__gmpn_mullo_basecase@plt>
   3a7c0:	mov	x0, x23
   3a7c4:	mov	x1, x23
   3a7c8:	mov	x2, x24
   3a7cc:	mov	x3, x22
   3a7d0:	ldp	x20, x19, [sp, #64]
   3a7d4:	ldp	x22, x21, [sp, #48]
   3a7d8:	ldp	x24, x23, [sp, #32]
   3a7dc:	ldr	x25, [sp, #16]
   3a7e0:	ldp	x29, x30, [sp], #80
   3a7e4:	b	ca70 <__gmpn_add_n@plt>

000000000003a7e8 <__gmpn_mullo_basecase@@Base>:
   3a7e8:	stp	x29, x30, [sp, #-80]!
   3a7ec:	stp	x24, x23, [sp, #32]
   3a7f0:	stp	x22, x21, [sp, #48]
   3a7f4:	stp	x20, x19, [sp, #64]
   3a7f8:	mov	x22, x2
   3a7fc:	subs	x2, x3, #0x1
   3a800:	ldr	x8, [x1]
   3a804:	ldr	x9, [x22, x2, lsl #3]
   3a808:	mov	x19, x0
   3a80c:	str	x25, [sp, #16]
   3a810:	mov	x29, sp
   3a814:	mul	x24, x9, x8
   3a818:	b.eq	3a884 <__gmpn_mullo_basecase@@Base+0x9c>  // b.none
   3a81c:	ldr	x23, [x22]
   3a820:	ldr	x25, [x1, x2, lsl #3]
   3a824:	mov	x21, x3
   3a828:	mov	x0, x19
   3a82c:	mov	x3, x23
   3a830:	mov	x20, x1
   3a834:	bl	d490 <__gmpn_mul_1@plt>
   3a838:	add	x8, x0, x24
   3a83c:	cmp	x21, #0x3
   3a840:	madd	x24, x25, x23, x8
   3a844:	add	x19, x19, #0x8
   3a848:	b.lt	3a884 <__gmpn_mullo_basecase@@Base+0x9c>  // b.tstop
   3a84c:	sub	x21, x21, #0x2
   3a850:	add	x23, x22, #0x8
   3a854:	ldr	x22, [x23], #8
   3a858:	ldr	x25, [x20, x21, lsl #3]
   3a85c:	mov	x0, x19
   3a860:	mov	x1, x20
   3a864:	mov	x2, x21
   3a868:	mov	x3, x22
   3a86c:	bl	d400 <__gmpn_addmul_1@plt>
   3a870:	add	x8, x0, x24
   3a874:	subs	x21, x21, #0x1
   3a878:	madd	x24, x25, x22, x8
   3a87c:	add	x19, x19, #0x8
   3a880:	b.gt	3a854 <__gmpn_mullo_basecase@@Base+0x6c>
   3a884:	str	x24, [x19]
   3a888:	ldp	x20, x19, [sp, #64]
   3a88c:	ldp	x22, x21, [sp, #48]
   3a890:	ldp	x24, x23, [sp, #32]
   3a894:	ldr	x25, [sp, #16]
   3a898:	ldp	x29, x30, [sp], #80
   3a89c:	ret

000000000003a8a0 <__gmpn_sqrlo@@Base>:
   3a8a0:	stp	x29, x30, [sp, #-80]!
   3a8a4:	stp	x22, x21, [sp, #48]
   3a8a8:	stp	x20, x19, [sp, #64]
   3a8ac:	mov	x21, x2
   3a8b0:	mov	x20, x1
   3a8b4:	cmp	x2, #0x3
   3a8b8:	mov	x19, x0
   3a8bc:	str	x25, [sp, #16]
   3a8c0:	stp	x24, x23, [sp, #32]
   3a8c4:	mov	x29, sp
   3a8c8:	b.le	3a934 <__gmpn_sqrlo@@Base+0x94>
   3a8cc:	cmp	x21, #0x42
   3a8d0:	b.le	3a960 <__gmpn_sqrlo@@Base+0xc0>
   3a8d4:	lsl	x1, x21, #4
   3a8d8:	mov	w8, #0x7f00                	// #32512
   3a8dc:	cmp	x1, x8
   3a8e0:	str	xzr, [x29, #24]
   3a8e4:	b.hi	3a988 <__gmpn_sqrlo@@Base+0xe8>  // b.pmore
   3a8e8:	add	x9, x1, #0xf
   3a8ec:	mov	x8, sp
   3a8f0:	and	x9, x9, #0xfffffffffffffff0
   3a8f4:	sub	x22, x8, x9
   3a8f8:	mov	sp, x22
   3a8fc:	mov	w8, #0x1477                	// #5239
   3a900:	cmp	x21, x8
   3a904:	b.le	3a9a0 <__gmpn_sqrlo@@Base+0x100>
   3a908:	mov	x0, x22
   3a90c:	mov	x1, x20
   3a910:	mov	x2, x21
   3a914:	mov	x3, x20
   3a918:	mov	x4, x21
   3a91c:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   3a920:	mov	x0, x19
   3a924:	mov	x1, x22
   3a928:	mov	x2, x21
   3a92c:	bl	ca50 <__gmpn_copyi@plt>
   3a930:	b	3aa98 <__gmpn_sqrlo@@Base+0x1f8>
   3a934:	mov	x0, x19
   3a938:	mov	x1, x20
   3a93c:	mov	x2, x20
   3a940:	mov	x3, x21
   3a944:	mov	sp, x29
   3a948:	ldp	x20, x19, [sp, #64]
   3a94c:	ldp	x22, x21, [sp, #48]
   3a950:	ldp	x24, x23, [sp, #32]
   3a954:	ldr	x25, [sp, #16]
   3a958:	ldp	x29, x30, [sp], #80
   3a95c:	b	d290 <__gmpn_mullo_basecase@plt>
   3a960:	mov	x0, x19
   3a964:	mov	x1, x20
   3a968:	mov	x2, x21
   3a96c:	mov	sp, x29
   3a970:	ldp	x20, x19, [sp, #64]
   3a974:	ldp	x22, x21, [sp, #48]
   3a978:	ldp	x24, x23, [sp, #32]
   3a97c:	ldr	x25, [sp, #16]
   3a980:	ldp	x29, x30, [sp], #80
   3a984:	b	c0b0 <__gmpn_sqrlo_basecase@plt>
   3a988:	add	x0, x29, #0x18
   3a98c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3a990:	mov	x22, x0
   3a994:	mov	w8, #0x1477                	// #5239
   3a998:	cmp	x21, x8
   3a99c:	b.gt	3a908 <__gmpn_sqrlo@@Base+0x68>
   3a9a0:	cmp	x21, #0x5f
   3a9a4:	b.le	3a9c8 <__gmpn_sqrlo@@Base+0x128>
   3a9a8:	cmp	x21, #0xd5
   3a9ac:	b.le	3a9e8 <__gmpn_sqrlo@@Base+0x148>
   3a9b0:	cmp	x21, #0x171
   3a9b4:	b.le	3aa04 <__gmpn_sqrlo@@Base+0x164>
   3a9b8:	mov	w9, #0xcccd                	// #52429
   3a9bc:	and	w8, w21, #0xffff
   3a9c0:	movk	w9, #0xcccc, lsl #16
   3a9c4:	b	3a9dc <__gmpn_sqrlo@@Base+0x13c>
   3a9c8:	mov	w8, #0xb                   	// #11
   3a9cc:	mul	w8, w21, w8
   3a9d0:	mov	w9, #0x8e39                	// #36409
   3a9d4:	and	w8, w8, #0xffff
   3a9d8:	movk	w9, #0x38e3, lsl #16
   3a9dc:	umull	x8, w8, w9
   3a9e0:	lsr	x8, x8, #35
   3a9e4:	b	3aa2c <__gmpn_sqrlo@@Base+0x18c>
   3a9e8:	add	w8, w21, w21, lsl #3
   3a9ec:	mov	w9, #0xcccd                	// #52429
   3a9f0:	and	w8, w8, #0xffff
   3a9f4:	movk	w9, #0xcccc, lsl #16
   3a9f8:	umull	x8, w8, w9
   3a9fc:	lsr	x8, x8, #37
   3aa00:	b	3aa2c <__gmpn_sqrlo@@Base+0x18c>
   3aa04:	lsl	w8, w21, #3
   3aa08:	sub	w8, w8, w21
   3aa0c:	mov	w9, #0x41a5                	// #16805
   3aa10:	and	w8, w8, #0xffff
   3aa14:	movk	w9, #0xa41a, lsl #16
   3aa18:	umull	x9, w8, w9
   3aa1c:	lsr	x9, x9, #32
   3aa20:	sub	w8, w8, w9
   3aa24:	add	w8, w9, w8, lsr #1
   3aa28:	lsr	w8, w8, #5
   3aa2c:	and	x23, x8, #0xffff
   3aa30:	sub	x24, x21, x23
   3aa34:	mov	x0, x22
   3aa38:	mov	x1, x20
   3aa3c:	mov	x2, x24
   3aa40:	and	w25, w8, #0xffff
   3aa44:	bl	c8e0 <__gmpn_sqr@plt>
   3aa48:	mov	x0, x19
   3aa4c:	mov	x1, x22
   3aa50:	mov	x2, x24
   3aa54:	bl	ca50 <__gmpn_copyi@plt>
   3aa58:	add	x21, x22, x21, lsl #3
   3aa5c:	cmp	w25, #0x25
   3aa60:	add	x1, x20, x24, lsl #3
   3aa64:	mov	x0, x21
   3aa68:	mov	x2, x20
   3aa6c:	mov	x3, x23
   3aa70:	b.ls	3aa7c <__gmpn_sqrlo@@Base+0x1dc>  // b.plast
   3aa74:	bl	cec0 <__gmpn_mullo_n@plt>
   3aa78:	b	3aa80 <__gmpn_sqrlo@@Base+0x1e0>
   3aa7c:	bl	d290 <__gmpn_mullo_basecase@plt>
   3aa80:	lsl	x8, x24, #3
   3aa84:	add	x0, x19, x8
   3aa88:	add	x1, x22, x8
   3aa8c:	mov	x2, x21
   3aa90:	mov	x3, x23
   3aa94:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3aa98:	ldr	x0, [x29, #24]
   3aa9c:	cbnz	x0, 3aabc <__gmpn_sqrlo@@Base+0x21c>
   3aaa0:	mov	sp, x29
   3aaa4:	ldp	x20, x19, [sp, #64]
   3aaa8:	ldp	x22, x21, [sp, #48]
   3aaac:	ldp	x24, x23, [sp, #32]
   3aab0:	ldr	x25, [sp, #16]
   3aab4:	ldp	x29, x30, [sp], #80
   3aab8:	ret
   3aabc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3aac0:	b	3aaa0 <__gmpn_sqrlo@@Base+0x200>

000000000003aac4 <__gmpn_sqrlo_basecase@@Base>:
   3aac4:	stp	x29, x30, [sp, #-96]!
   3aac8:	stp	x28, x27, [sp, #16]
   3aacc:	stp	x26, x25, [sp, #32]
   3aad0:	stp	x24, x23, [sp, #48]
   3aad4:	stp	x22, x21, [sp, #64]
   3aad8:	stp	x20, x19, [sp, #80]
   3aadc:	mov	x29, sp
   3aae0:	sub	sp, sp, #0x240
   3aae4:	ldr	x24, [x1]
   3aae8:	sub	x19, x2, #0x1
   3aaec:	ldr	x20, [x1, x19, lsl #3]
   3aaf0:	sub	x25, x2, #0x2
   3aaf4:	mov	x27, x2
   3aaf8:	mov	x23, x1
   3aafc:	mov	x21, x0
   3ab00:	add	x1, x1, #0x8
   3ab04:	add	x0, sp, #0x28
   3ab08:	mov	x2, x25
   3ab0c:	mov	x3, x24
   3ab10:	add	x22, sp, #0x28
   3ab14:	bl	d490 <__gmpn_mul_1@plt>
   3ab18:	cmp	x27, #0x5
   3ab1c:	madd	x28, x20, x24, x0
   3ab20:	b.lt	3abb4 <__gmpn_sqrlo_basecase@@Base+0xf0>  // b.tstop
   3ab24:	add	x8, x23, x27, lsl #3
   3ab28:	stp	x25, x27, [sp, #8]
   3ab2c:	stp	x23, x21, [sp, #24]
   3ab30:	mov	x20, xzr
   3ab34:	add	x24, x22, #0x10
   3ab38:	sub	x25, x27, #0x4
   3ab3c:	add	x26, x23, #0x10
   3ab40:	sub	x23, x8, #0x10
   3ab44:	mov	w22, #0x3                   	// #3
   3ab48:	ldur	x27, [x26, #-8]
   3ab4c:	mov	x21, x19
   3ab50:	ldr	x19, [x23, x20, lsl #3]
   3ab54:	mov	x0, x24
   3ab58:	mov	x1, x26
   3ab5c:	mov	x2, x25
   3ab60:	mov	x3, x27
   3ab64:	bl	d400 <__gmpn_addmul_1@plt>
   3ab68:	add	x8, x0, x28
   3ab6c:	add	x22, x22, #0x2
   3ab70:	add	x24, x24, #0x10
   3ab74:	sub	x25, x25, #0x2
   3ab78:	add	x26, x26, #0x8
   3ab7c:	madd	x28, x19, x27, x8
   3ab80:	mov	x19, x21
   3ab84:	cmp	x22, x21
   3ab88:	sub	x20, x20, #0x1
   3ab8c:	b.lt	3ab48 <__gmpn_sqrlo_basecase@@Base+0x84>  // b.tstop
   3ab90:	ldp	x23, x21, [sp, #24]
   3ab94:	ldp	x25, x27, [sp, #8]
   3ab98:	mov	w8, #0x1                   	// #1
   3ab9c:	sub	x8, x8, x20
   3aba0:	tbz	w19, #0, 3abbc <__gmpn_sqrlo_basecase@@Base+0xf8>
   3aba4:	add	x8, x23, x8, lsl #3
   3aba8:	ldp	x9, x8, [x8]
   3abac:	mul	x8, x8, x9
   3abb0:	b	3abc0 <__gmpn_sqrlo_basecase@@Base+0xfc>
   3abb4:	mov	w8, #0x1                   	// #1
   3abb8:	tbnz	w19, #0, 3aba4 <__gmpn_sqrlo_basecase@@Base+0xe0>
   3abbc:	mov	x8, xzr
   3abc0:	add	x8, x8, x28
   3abc4:	add	x9, sp, #0x28
   3abc8:	cmp	x27, #0x2
   3abcc:	str	x8, [x9, x25, lsl #3]
   3abd0:	asr	x8, x27, #1
   3abd4:	b.lt	3ac00 <__gmpn_sqrlo_basecase@@Base+0x13c>  // b.tstop
   3abd8:	mov	x9, xzr
   3abdc:	add	x10, x21, #0x8
   3abe0:	ldr	x11, [x23, x9, lsl #3]
   3abe4:	add	x9, x9, #0x1
   3abe8:	umulh	x12, x11, x11
   3abec:	cmp	x9, x8
   3abf0:	mul	x11, x11, x11
   3abf4:	stp	x11, x12, [x10, #-8]
   3abf8:	add	x10, x10, #0x10
   3abfc:	b.lt	3abe0 <__gmpn_sqrlo_basecase@@Base+0x11c>  // b.tstop
   3ac00:	tbz	w27, #0, 3ac10 <__gmpn_sqrlo_basecase@@Base+0x14c>
   3ac04:	ldr	x8, [x23, x8, lsl #3]
   3ac08:	mul	x8, x8, x8
   3ac0c:	str	x8, [x21, x19, lsl #3]
   3ac10:	add	x0, x21, #0x8
   3ac14:	add	x2, sp, #0x28
   3ac18:	mov	x1, x0
   3ac1c:	mov	x3, x19
   3ac20:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ac24:	add	sp, sp, #0x240
   3ac28:	ldp	x20, x19, [sp, #80]
   3ac2c:	ldp	x22, x21, [sp, #64]
   3ac30:	ldp	x24, x23, [sp, #48]
   3ac34:	ldp	x26, x25, [sp, #32]
   3ac38:	ldp	x28, x27, [sp, #16]
   3ac3c:	ldp	x29, x30, [sp], #96
   3ac40:	ret

000000000003ac44 <__gmpn_toom22_mul@@Base>:
   3ac44:	sub	sp, sp, #0x80
   3ac48:	stp	x22, x21, [sp, #96]
   3ac4c:	asr	x21, x2, #1
   3ac50:	sub	x22, x2, x21
   3ac54:	stp	x29, x30, [sp, #32]
   3ac58:	stp	x28, x27, [sp, #48]
   3ac5c:	stp	x26, x25, [sp, #64]
   3ac60:	stp	x24, x23, [sp, #80]
   3ac64:	stp	x20, x19, [sp, #112]
   3ac68:	add	x29, sp, #0x20
   3ac6c:	mov	x28, x4
   3ac70:	mov	x26, x3
   3ac74:	mov	x20, x2
   3ac78:	mov	x27, x1
   3ac7c:	mov	x19, x0
   3ac80:	sub	x25, x4, x22
   3ac84:	add	x24, x0, x22, lsl #3
   3ac88:	subs	x12, x21, x22
   3ac8c:	add	x2, x1, x21, lsl #3
   3ac90:	stur	x5, [x29, #-8]
   3ac94:	str	x12, [sp, #8]
   3ac98:	b.ne	3acf0 <__gmpn_toom22_mul@@Base+0xac>  // b.any
   3ac9c:	lsl	x8, x21, #4
   3aca0:	sub	x8, x8, #0x8
   3aca4:	mov	x10, x21
   3aca8:	subs	x9, x10, #0x1
   3acac:	b.lt	3acd0 <__gmpn_toom22_mul@@Base+0x8c>  // b.tstop
   3acb0:	add	x10, x27, x10, lsl #3
   3acb4:	ldur	x10, [x10, #-8]
   3acb8:	ldr	x11, [x27, x8]
   3acbc:	sub	x8, x8, #0x8
   3acc0:	cmp	x10, x11
   3acc4:	mov	x10, x9
   3acc8:	b.eq	3aca8 <__gmpn_toom22_mul@@Base+0x64>  // b.none
   3accc:	b.ls	3add8 <__gmpn_toom22_mul@@Base+0x194>  // b.plast
   3acd0:	mov	x0, x19
   3acd4:	mov	x1, x27
   3acd8:	mov	x3, x21
   3acdc:	bl	c2d0 <__gmpn_sub_n@plt>
   3ace0:	stur	wzr, [x29, #-12]
   3ace4:	subs	x23, x22, x25
   3ace8:	b.eq	3ad20 <__gmpn_toom22_mul@@Base+0xdc>  // b.none
   3acec:	b	3adfc <__gmpn_toom22_mul@@Base+0x1b8>
   3acf0:	ldr	x23, [x2]
   3acf4:	cbz	x23, 3ad78 <__gmpn_toom22_mul@@Base+0x134>
   3acf8:	add	x2, x27, x22, lsl #3
   3acfc:	mov	x0, x19
   3ad00:	mov	x1, x27
   3ad04:	mov	x3, x21
   3ad08:	bl	c2d0 <__gmpn_sub_n@plt>
   3ad0c:	sub	x8, x23, x0
   3ad10:	stur	wzr, [x29, #-12]
   3ad14:	str	x8, [x19, x21, lsl #3]
   3ad18:	subs	x23, x22, x25
   3ad1c:	b.ne	3adfc <__gmpn_toom22_mul@@Base+0x1b8>  // b.any
   3ad20:	ldur	x23, [x29, #-8]
   3ad24:	lsl	x9, x20, #4
   3ad28:	add	x2, x26, x22, lsl #3
   3ad2c:	sub	x8, x26, #0x8
   3ad30:	sub	x9, x9, x21, lsl #4
   3ad34:	mov	x10, x22
   3ad38:	subs	x11, x10, #0x1
   3ad3c:	b.lt	3ad5c <__gmpn_toom22_mul@@Base+0x118>  // b.tstop
   3ad40:	ldr	x10, [x8, x10, lsl #3]
   3ad44:	ldr	x12, [x8, x9]
   3ad48:	sub	x9, x9, #0x8
   3ad4c:	cmp	x10, x12
   3ad50:	mov	x10, x11
   3ad54:	b.eq	3ad38 <__gmpn_toom22_mul@@Base+0xf4>  // b.none
   3ad58:	b.ls	3af64 <__gmpn_toom22_mul@@Base+0x320>  // b.plast
   3ad5c:	mov	x0, x24
   3ad60:	mov	x1, x26
   3ad64:	mov	x3, x22
   3ad68:	bl	c2d0 <__gmpn_sub_n@plt>
   3ad6c:	cmp	x22, #0xd
   3ad70:	b.gt	3af8c <__gmpn_toom22_mul@@Base+0x348>
   3ad74:	b	3b040 <__gmpn_toom22_mul@@Base+0x3fc>
   3ad78:	lsl	x8, x20, #3
   3ad7c:	add	x1, x27, x22, lsl #3
   3ad80:	sub	x8, x8, #0x8
   3ad84:	mov	x10, x21
   3ad88:	subs	x9, x10, #0x1
   3ad8c:	b.lt	3acf8 <__gmpn_toom22_mul@@Base+0xb4>  // b.tstop
   3ad90:	add	x10, x27, x10, lsl #3
   3ad94:	ldur	x10, [x10, #-8]
   3ad98:	ldr	x11, [x27, x8]
   3ad9c:	sub	x8, x8, #0x8
   3ada0:	cmp	x10, x11
   3ada4:	mov	x10, x9
   3ada8:	b.eq	3ad88 <__gmpn_toom22_mul@@Base+0x144>  // b.none
   3adac:	b.hi	3acf8 <__gmpn_toom22_mul@@Base+0xb4>  // b.pmore
   3adb0:	mov	x0, x19
   3adb4:	mov	x2, x27
   3adb8:	mov	x3, x21
   3adbc:	bl	c2d0 <__gmpn_sub_n@plt>
   3adc0:	str	xzr, [x19, x21, lsl #3]
   3adc4:	mov	w8, #0x1                   	// #1
   3adc8:	stur	w8, [x29, #-12]
   3adcc:	subs	x23, x22, x25
   3add0:	b.eq	3ad20 <__gmpn_toom22_mul@@Base+0xdc>  // b.none
   3add4:	b	3adfc <__gmpn_toom22_mul@@Base+0x1b8>
   3add8:	mov	x0, x19
   3addc:	mov	x1, x2
   3ade0:	mov	x2, x27
   3ade4:	mov	x3, x21
   3ade8:	bl	c2d0 <__gmpn_sub_n@plt>
   3adec:	mov	w8, #0x1                   	// #1
   3adf0:	stur	w8, [x29, #-12]
   3adf4:	subs	x23, x22, x25
   3adf8:	b.eq	3ad20 <__gmpn_toom22_mul@@Base+0xdc>  // b.none
   3adfc:	lsl	x9, x20, #3
   3ae00:	add	x8, x28, x21, lsl #1
   3ae04:	sub	x9, x9, x21, lsl #3
   3ae08:	sub	x8, x8, x20, lsl #1
   3ae0c:	sub	x9, x9, #0x8
   3ae10:	ldr	x10, [x26, x9]
   3ae14:	cbnz	x10, 3ae5c <__gmpn_toom22_mul@@Base+0x218>
   3ae18:	adds	x8, x8, #0x1
   3ae1c:	sub	x9, x9, #0x8
   3ae20:	b.cc	3ae10 <__gmpn_toom22_mul@@Base+0x1cc>  // b.lo, b.ul, b.last
   3ae24:	lsl	x8, x28, #3
   3ae28:	add	x1, x26, x22, lsl #3
   3ae2c:	sub	x8, x8, #0x8
   3ae30:	mov	x10, x25
   3ae34:	subs	x9, x10, #0x1
   3ae38:	b.lt	3ae5c <__gmpn_toom22_mul@@Base+0x218>  // b.tstop
   3ae3c:	add	x10, x26, x10, lsl #3
   3ae40:	ldur	x10, [x10, #-8]
   3ae44:	ldr	x11, [x26, x8]
   3ae48:	sub	x8, x8, #0x8
   3ae4c:	cmp	x10, x11
   3ae50:	mov	x10, x9
   3ae54:	b.eq	3ae34 <__gmpn_toom22_mul@@Base+0x1f0>  // b.none
   3ae58:	b.ls	3aff4 <__gmpn_toom22_mul@@Base+0x3b0>  // b.plast
   3ae5c:	cbz	x25, 3aea8 <__gmpn_toom22_mul@@Base+0x264>
   3ae60:	add	x2, x26, x22, lsl #3
   3ae64:	mov	x0, x24
   3ae68:	mov	x1, x26
   3ae6c:	mov	x3, x25
   3ae70:	bl	c2d0 <__gmpn_sub_n@plt>
   3ae74:	ldur	x23, [x29, #-8]
   3ae78:	mov	x8, x25
   3ae7c:	cbz	x0, 3aeb0 <__gmpn_toom22_mul@@Base+0x26c>
   3ae80:	add	x9, x19, x28, lsl #3
   3ae84:	mov	x8, x25
   3ae88:	cmp	x8, x22
   3ae8c:	b.ge	3af84 <__gmpn_toom22_mul@@Base+0x340>  // b.tcont
   3ae90:	ldr	x10, [x26, x8, lsl #3]
   3ae94:	add	x8, x8, #0x1
   3ae98:	sub	x11, x10, #0x1
   3ae9c:	str	x11, [x9], #8
   3aea0:	cbz	x10, 3ae88 <__gmpn_toom22_mul@@Base+0x244>
   3aea4:	b	3aeb0 <__gmpn_toom22_mul@@Base+0x26c>
   3aea8:	ldur	x23, [x29, #-8]
   3aeac:	mov	x8, xzr
   3aeb0:	cmp	x24, x26
   3aeb4:	b.eq	3af84 <__gmpn_toom22_mul@@Base+0x340>  // b.none
   3aeb8:	cmp	x8, x22
   3aebc:	b.ge	3af84 <__gmpn_toom22_mul@@Base+0x340>  // b.tcont
   3aec0:	sub	x9, x20, x8
   3aec4:	sub	x9, x9, x21
   3aec8:	cmp	x9, #0x4
   3aecc:	b.cc	3af38 <__gmpn_toom22_mul@@Base+0x2f4>  // b.lo, b.ul, b.last
   3aed0:	add	x10, x8, x20
   3aed4:	sub	x10, x10, x21
   3aed8:	add	x12, x19, x10, lsl #3
   3aedc:	add	x10, x26, x22, lsl #3
   3aee0:	cmp	x12, x10
   3aee4:	add	x11, x26, x8, lsl #3
   3aee8:	b.cs	3af04 <__gmpn_toom22_mul@@Base+0x2c0>  // b.hs, b.nlast
   3aeec:	lsl	x10, x20, #1
   3aef0:	and	x13, x20, #0x1ffffffffffffffe
   3aef4:	sub	x10, x10, x13
   3aef8:	add	x10, x19, x10, lsl #3
   3aefc:	cmp	x11, x10
   3af00:	b.cc	3af38 <__gmpn_toom22_mul@@Base+0x2f4>  // b.lo, b.ul, b.last
   3af04:	and	x10, x9, #0xfffffffffffffffc
   3af08:	add	x11, x11, #0x10
   3af0c:	add	x8, x8, x10
   3af10:	add	x12, x12, #0x10
   3af14:	mov	x13, x10
   3af18:	ldp	q0, q1, [x11, #-16]
   3af1c:	subs	x13, x13, #0x4
   3af20:	add	x11, x11, #0x20
   3af24:	stp	q0, q1, [x12, #-16]
   3af28:	add	x12, x12, #0x20
   3af2c:	b.ne	3af18 <__gmpn_toom22_mul@@Base+0x2d4>  // b.any
   3af30:	cmp	x9, x10
   3af34:	b.eq	3af84 <__gmpn_toom22_mul@@Base+0x340>  // b.none
   3af38:	add	x10, x8, x20
   3af3c:	add	x9, x8, x21
   3af40:	sub	x10, x10, x21
   3af44:	sub	x9, x9, x20
   3af48:	add	x10, x19, x10, lsl #3
   3af4c:	add	x8, x26, x8, lsl #3
   3af50:	ldr	x11, [x8], #8
   3af54:	adds	x9, x9, #0x1
   3af58:	str	x11, [x10], #8
   3af5c:	b.cc	3af50 <__gmpn_toom22_mul@@Base+0x30c>  // b.lo, b.ul, b.last
   3af60:	b	3af84 <__gmpn_toom22_mul@@Base+0x340>
   3af64:	mov	x0, x24
   3af68:	mov	x1, x2
   3af6c:	mov	x2, x26
   3af70:	mov	x3, x22
   3af74:	bl	c2d0 <__gmpn_sub_n@plt>
   3af78:	ldur	w8, [x29, #-12]
   3af7c:	eor	w8, w8, #0x1
   3af80:	stur	w8, [x29, #-12]
   3af84:	cmp	x22, #0xd
   3af88:	b.le	3b040 <__gmpn_toom22_mul@@Base+0x3fc>
   3af8c:	add	x5, x23, x22, lsl #4
   3af90:	mov	x0, x23
   3af94:	mov	x1, x19
   3af98:	mov	x2, x22
   3af9c:	mov	x3, x24
   3afa0:	mov	x4, x22
   3afa4:	bl	d450 <__gmpn_toom22_mul@plt>
   3afa8:	cmp	x21, x25
   3afac:	lsl	x28, x22, #1
   3afb0:	b.gt	3b064 <__gmpn_toom22_mul@@Base+0x420>
   3afb4:	lsl	x8, x22, #3
   3afb8:	add	x0, x19, x22, lsl #4
   3afbc:	cmp	x20, #0x1b
   3afc0:	add	x1, x27, x8
   3afc4:	add	x3, x26, x8
   3afc8:	b.le	3afdc <__gmpn_toom22_mul@@Base+0x398>
   3afcc:	add	x5, x23, x28, lsl #3
   3afd0:	mov	x2, x21
   3afd4:	mov	x4, x21
   3afd8:	b	3b098 <__gmpn_toom22_mul@@Base+0x454>
   3afdc:	mov	x2, x21
   3afe0:	mov	x4, x21
   3afe4:	bl	c550 <__gmpn_mul_basecase@plt>
   3afe8:	cmp	x22, #0xd
   3afec:	b.gt	3b0cc <__gmpn_toom22_mul@@Base+0x488>
   3aff0:	b	3b0f8 <__gmpn_toom22_mul@@Base+0x4b4>
   3aff4:	mov	x0, x24
   3aff8:	mov	x2, x26
   3affc:	mov	x3, x25
   3b000:	bl	c2d0 <__gmpn_sub_n@plt>
   3b004:	cbz	x23, 3b028 <__gmpn_toom22_mul@@Base+0x3e4>
   3b008:	lsl	x8, x20, #1
   3b00c:	sub	x8, x8, x28
   3b010:	and	x9, x20, #0x1ffffffffffffffe
   3b014:	sub	x8, x8, x9
   3b018:	add	x0, x19, x28, lsl #3
   3b01c:	lsl	x2, x8, #3
   3b020:	mov	w1, wzr
   3b024:	bl	c5f0 <memset@plt>
   3b028:	ldur	w8, [x29, #-12]
   3b02c:	ldur	x23, [x29, #-8]
   3b030:	eor	w8, w8, #0x1
   3b034:	stur	w8, [x29, #-12]
   3b038:	cmp	x22, #0xd
   3b03c:	b.gt	3af8c <__gmpn_toom22_mul@@Base+0x348>
   3b040:	mov	x0, x23
   3b044:	mov	x1, x19
   3b048:	mov	x2, x22
   3b04c:	mov	x3, x24
   3b050:	mov	x4, x22
   3b054:	bl	c550 <__gmpn_mul_basecase@plt>
   3b058:	cmp	x21, x25
   3b05c:	lsl	x28, x22, #1
   3b060:	b.le	3afb4 <__gmpn_toom22_mul@@Base+0x370>
   3b064:	cmp	x25, #0xd
   3b068:	b.le	3b0a8 <__gmpn_toom22_mul@@Base+0x464>
   3b06c:	add	x8, x25, x25, lsl #2
   3b070:	lsl	x9, x22, #4
   3b074:	lsl	x10, x22, #3
   3b078:	add	x0, x19, x9
   3b07c:	add	x1, x27, x10
   3b080:	add	x3, x26, x10
   3b084:	cmp	x8, x21, lsl #2
   3b088:	add	x5, x23, x9
   3b08c:	mov	x2, x21
   3b090:	mov	x4, x25
   3b094:	b.le	3b0ec <__gmpn_toom22_mul@@Base+0x4a8>
   3b098:	bl	d450 <__gmpn_toom22_mul@plt>
   3b09c:	cmp	x22, #0xd
   3b0a0:	b.gt	3b0cc <__gmpn_toom22_mul@@Base+0x488>
   3b0a4:	b	3b0f8 <__gmpn_toom22_mul@@Base+0x4b4>
   3b0a8:	lsl	x8, x22, #3
   3b0ac:	add	x0, x19, x22, lsl #4
   3b0b0:	add	x1, x27, x8
   3b0b4:	add	x3, x26, x8
   3b0b8:	mov	x2, x21
   3b0bc:	mov	x4, x25
   3b0c0:	bl	c550 <__gmpn_mul_basecase@plt>
   3b0c4:	cmp	x22, #0xd
   3b0c8:	b.le	3b0f8 <__gmpn_toom22_mul@@Base+0x4b4>
   3b0cc:	add	x5, x23, x22, lsl #4
   3b0d0:	mov	x0, x19
   3b0d4:	mov	x1, x27
   3b0d8:	mov	x2, x22
   3b0dc:	mov	x3, x26
   3b0e0:	mov	x4, x22
   3b0e4:	bl	d450 <__gmpn_toom22_mul@plt>
   3b0e8:	b	3b110 <__gmpn_toom22_mul@@Base+0x4cc>
   3b0ec:	bl	c850 <__gmpn_toom32_mul@plt>
   3b0f0:	cmp	x22, #0xd
   3b0f4:	b.gt	3b0cc <__gmpn_toom22_mul@@Base+0x488>
   3b0f8:	mov	x0, x19
   3b0fc:	mov	x1, x27
   3b100:	mov	x2, x22
   3b104:	mov	x3, x26
   3b108:	mov	x4, x22
   3b10c:	bl	c550 <__gmpn_mul_basecase@plt>
   3b110:	add	x26, x19, x28, lsl #3
   3b114:	mov	x0, x26
   3b118:	mov	x1, x24
   3b11c:	mov	x2, x26
   3b120:	mov	x3, x22
   3b124:	bl	ca70 <__gmpn_add_n@plt>
   3b128:	mov	x27, x0
   3b12c:	mov	x0, x24
   3b130:	mov	x1, x26
   3b134:	mov	x2, x19
   3b138:	mov	x3, x22
   3b13c:	bl	ca70 <__gmpn_add_n@plt>
   3b140:	ldr	x8, [sp, #8]
   3b144:	adds	x23, x8, x25
   3b148:	mov	x25, x0
   3b14c:	b.eq	3b18c <__gmpn_toom22_mul@@Base+0x548>  // b.none
   3b150:	add	x2, x26, x22, lsl #3
   3b154:	mov	x0, x26
   3b158:	mov	x1, x26
   3b15c:	mov	x3, x23
   3b160:	bl	ca70 <__gmpn_add_n@plt>
   3b164:	cbz	x0, 3b18c <__gmpn_toom22_mul@@Base+0x548>
   3b168:	mov	w8, #0x1                   	// #1
   3b16c:	cmp	x23, x22
   3b170:	b.ge	3b190 <__gmpn_toom22_mul@@Base+0x54c>  // b.tcont
   3b174:	lsl	x9, x23, #3
   3b178:	ldr	x10, [x26, x9]
   3b17c:	add	x23, x23, #0x1
   3b180:	adds	x10, x10, #0x1
   3b184:	str	x10, [x26, x9]
   3b188:	b.cs	3b16c <__gmpn_toom22_mul@@Base+0x528>  // b.hs, b.nlast
   3b18c:	mov	x8, xzr
   3b190:	add	x23, x8, x27
   3b194:	ldur	w8, [x29, #-12]
   3b198:	cbz	w8, 3b1b8 <__gmpn_toom22_mul@@Base+0x574>
   3b19c:	ldur	x2, [x29, #-8]
   3b1a0:	mov	x0, x24
   3b1a4:	mov	x1, x24
   3b1a8:	mov	x3, x28
   3b1ac:	bl	ca70 <__gmpn_add_n@plt>
   3b1b0:	add	x8, x0, x23
   3b1b4:	b	3b1d8 <__gmpn_toom22_mul@@Base+0x594>
   3b1b8:	ldur	x2, [x29, #-8]
   3b1bc:	mov	x0, x24
   3b1c0:	mov	x1, x24
   3b1c4:	mov	x3, x28
   3b1c8:	bl	c2d0 <__gmpn_sub_n@plt>
   3b1cc:	sub	x8, x23, x0
   3b1d0:	cmn	x8, #0x1
   3b1d4:	b.eq	3b260 <__gmpn_toom22_mul@@Base+0x61c>  // b.none
   3b1d8:	ldr	x9, [x26]
   3b1dc:	add	x10, x25, x27
   3b1e0:	adds	x9, x9, x10
   3b1e4:	str	x9, [x26]
   3b1e8:	b.cc	3b204 <__gmpn_toom22_mul@@Base+0x5c0>  // b.lo, b.ul, b.last
   3b1ec:	add	x9, x19, x28, lsl #3
   3b1f0:	add	x9, x9, #0x8
   3b1f4:	ldr	x10, [x9]
   3b1f8:	adds	x10, x10, #0x1
   3b1fc:	str	x10, [x9], #8
   3b200:	b.cs	3b1f4 <__gmpn_toom22_mul@@Base+0x5b0>  // b.hs, b.nlast
   3b204:	mov	w9, #0x18                  	// #24
   3b208:	mul	x9, x22, x9
   3b20c:	ldr	x10, [x19, x9]
   3b210:	adds	x8, x10, x8
   3b214:	str	x8, [x19, x9]
   3b218:	b.cc	3b240 <__gmpn_toom22_mul@@Base+0x5fc>  // b.lo, b.ul, b.last
   3b21c:	add	x8, x20, x20, lsl #1
   3b220:	add	x9, x21, x21, lsl #1
   3b224:	sub	x8, x8, x9
   3b228:	add	x8, x19, x8, lsl #3
   3b22c:	add	x8, x8, #0x8
   3b230:	ldr	x9, [x8]
   3b234:	adds	x9, x9, #0x1
   3b238:	str	x9, [x8], #8
   3b23c:	b.cs	3b230 <__gmpn_toom22_mul@@Base+0x5ec>  // b.hs, b.nlast
   3b240:	ldp	x20, x19, [sp, #112]
   3b244:	ldp	x22, x21, [sp, #96]
   3b248:	ldp	x24, x23, [sp, #80]
   3b24c:	ldp	x26, x25, [sp, #64]
   3b250:	ldp	x28, x27, [sp, #48]
   3b254:	ldp	x29, x30, [sp, #32]
   3b258:	add	sp, sp, #0x80
   3b25c:	ret
   3b260:	lsl	x2, x22, #3
   3b264:	mov	x0, x26
   3b268:	mov	w1, wzr
   3b26c:	bl	c5f0 <memset@plt>
   3b270:	b	3b240 <__gmpn_toom22_mul@@Base+0x5fc>

000000000003b274 <__gmpn_toom32_mul@@Base>:
   3b274:	sub	sp, sp, #0xd0
   3b278:	add	x8, x4, x4, lsl #1
   3b27c:	stp	x29, x30, [sp, #112]
   3b280:	stp	x28, x27, [sp, #128]
   3b284:	stp	x26, x25, [sp, #144]
   3b288:	stp	x24, x23, [sp, #160]
   3b28c:	stp	x22, x21, [sp, #176]
   3b290:	stp	x20, x19, [sp, #192]
   3b294:	add	x29, sp, #0x70
   3b298:	mov	x24, x4
   3b29c:	mov	x26, x3
   3b2a0:	mov	x21, x2
   3b2a4:	mov	x27, x1
   3b2a8:	cmp	x8, x2, lsl #1
   3b2ac:	mov	x19, x0
   3b2b0:	stur	x5, [x29, #-16]
   3b2b4:	b.le	3b2c4 <__gmpn_toom32_mul@@Base+0x50>
   3b2b8:	sub	x8, x24, #0x1
   3b2bc:	asr	x28, x8, #1
   3b2c0:	b	3b2d8 <__gmpn_toom32_mul@@Base+0x64>
   3b2c4:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3b2c8:	sub	x8, x21, #0x1
   3b2cc:	movk	x9, #0xaaab
   3b2d0:	umulh	x8, x8, x9
   3b2d4:	lsr	x28, x8, #1
   3b2d8:	add	x22, x28, #0x1
   3b2dc:	lsl	x25, x22, #1
   3b2e0:	sub	x8, x24, x22
   3b2e4:	stur	x8, [x29, #-8]
   3b2e8:	sub	x23, x21, x25
   3b2ec:	add	x2, x27, x22, lsl #4
   3b2f0:	lsl	x8, x22, #3
   3b2f4:	stp	x8, x23, [x29, #-48]
   3b2f8:	str	x21, [sp, #8]
   3b2fc:	str	x2, [sp, #24]
   3b300:	cbz	x23, 3b348 <__gmpn_toom32_mul@@Base+0xd4>
   3b304:	mov	x0, x19
   3b308:	mov	x1, x27
   3b30c:	mov	x3, x23
   3b310:	bl	ca70 <__gmpn_add_n@plt>
   3b314:	mov	x8, x23
   3b318:	cbz	x0, 3b34c <__gmpn_toom32_mul@@Base+0xd8>
   3b31c:	mov	w20, #0x1                   	// #1
   3b320:	mov	x8, x23
   3b324:	cmp	x8, x28
   3b328:	b.gt	3b42c <__gmpn_toom32_mul@@Base+0x1b8>
   3b32c:	lsl	x9, x8, #3
   3b330:	ldr	x10, [x27, x9]
   3b334:	add	x8, x8, #0x1
   3b338:	adds	x10, x10, #0x1
   3b33c:	str	x10, [x19, x9]
   3b340:	b.cs	3b324 <__gmpn_toom32_mul@@Base+0xb0>  // b.hs, b.nlast
   3b344:	b	3b34c <__gmpn_toom32_mul@@Base+0xd8>
   3b348:	mov	x8, xzr
   3b34c:	ldur	x23, [x29, #-48]
   3b350:	cmp	x19, x27
   3b354:	b.eq	3b3f0 <__gmpn_toom32_mul@@Base+0x17c>  // b.none
   3b358:	cmp	x8, x28
   3b35c:	b.gt	3b3f0 <__gmpn_toom32_mul@@Base+0x17c>
   3b360:	sub	x9, x28, x8
   3b364:	add	x9, x9, #0x1
   3b368:	cmp	x9, #0x4
   3b36c:	b.cc	3b3cc <__gmpn_toom32_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3b370:	lsl	x11, x8, #3
   3b374:	add	x10, x19, x11
   3b378:	add	x12, x27, x23
   3b37c:	cmp	x10, x12
   3b380:	b.cs	3b394 <__gmpn_toom32_mul@@Base+0x120>  // b.hs, b.nlast
   3b384:	add	x10, x19, x23
   3b388:	add	x12, x27, x11
   3b38c:	cmp	x12, x10
   3b390:	b.cc	3b3cc <__gmpn_toom32_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3b394:	and	x10, x9, #0xfffffffffffffffc
   3b398:	add	x12, x11, #0x10
   3b39c:	add	x8, x8, x10
   3b3a0:	add	x11, x27, x12
   3b3a4:	add	x12, x19, x12
   3b3a8:	mov	x13, x10
   3b3ac:	ldp	q0, q1, [x11, #-16]
   3b3b0:	add	x11, x11, #0x20
   3b3b4:	subs	x13, x13, #0x4
   3b3b8:	stp	q0, q1, [x12, #-16]
   3b3bc:	add	x12, x12, #0x20
   3b3c0:	b.ne	3b3ac <__gmpn_toom32_mul@@Base+0x138>  // b.any
   3b3c4:	cmp	x9, x10
   3b3c8:	b.eq	3b3f0 <__gmpn_toom32_mul@@Base+0x17c>  // b.none
   3b3cc:	sub	x9, x28, x8
   3b3d0:	lsl	x10, x8, #3
   3b3d4:	add	x8, x9, #0x1
   3b3d8:	add	x9, x19, x10
   3b3dc:	add	x10, x27, x10
   3b3e0:	ldr	x11, [x10], #8
   3b3e4:	subs	x8, x8, #0x1
   3b3e8:	str	x11, [x9], #8
   3b3ec:	b.ne	3b3e0 <__gmpn_toom32_mul@@Base+0x16c>  // b.any
   3b3f0:	add	x8, x27, x28, lsl #4
   3b3f4:	add	x1, x27, x22, lsl #3
   3b3f8:	add	x8, x8, #0x8
   3b3fc:	mov	x9, x28
   3b400:	add	x10, x9, #0x1
   3b404:	cmp	x10, #0x1
   3b408:	b.lt	3b424 <__gmpn_toom32_mul@@Base+0x1b0>  // b.tstop
   3b40c:	ldr	x10, [x19, x9, lsl #3]
   3b410:	ldr	x11, [x8], #-8
   3b414:	sub	x9, x9, #0x1
   3b418:	cmp	x10, x11
   3b41c:	b.eq	3b400 <__gmpn_toom32_mul@@Base+0x18c>  // b.none
   3b420:	b.ls	3b6ac <__gmpn_toom32_mul@@Base+0x438>  // b.plast
   3b424:	mov	x20, xzr
   3b428:	b	3b430 <__gmpn_toom32_mul@@Base+0x1bc>
   3b42c:	ldur	x23, [x29, #-48]
   3b430:	add	x0, x19, x25, lsl #3
   3b434:	add	x2, x27, x22, lsl #3
   3b438:	mov	x1, x19
   3b43c:	mov	x3, x22
   3b440:	bl	c2d0 <__gmpn_sub_n@plt>
   3b444:	sub	x8, x20, x0
   3b448:	stur	wzr, [x29, #-28]
   3b44c:	str	x8, [sp, #56]
   3b450:	add	x2, x27, x23
   3b454:	mov	x0, x19
   3b458:	mov	x1, x19
   3b45c:	mov	x3, x22
   3b460:	str	x27, [sp, #40]
   3b464:	bl	ca70 <__gmpn_add_n@plt>
   3b468:	ldur	x8, [x29, #-8]
   3b46c:	mov	x9, x23
   3b470:	add	x27, x0, x20
   3b474:	add	x21, x19, x23
   3b478:	subs	x23, x22, x8
   3b47c:	add	x2, x26, x9
   3b480:	stur	x21, [x29, #-24]
   3b484:	str	x2, [sp, #48]
   3b488:	b.ne	3b4f4 <__gmpn_toom32_mul@@Base+0x280>  // b.any
   3b48c:	mov	x0, x21
   3b490:	mov	x1, x26
   3b494:	mov	x3, x22
   3b498:	bl	ca70 <__gmpn_add_n@plt>
   3b49c:	ldur	x23, [x29, #-16]
   3b4a0:	mov	w8, #0x8                   	// #8
   3b4a4:	mov	x20, x0
   3b4a8:	bfi	x8, x28, #4, #60
   3b4ac:	mov	x9, x28
   3b4b0:	add	x10, x9, #0x1
   3b4b4:	cmp	x10, #0x1
   3b4b8:	b.lt	3b4d8 <__gmpn_toom32_mul@@Base+0x264>  // b.tstop
   3b4bc:	ldr	x10, [x26, x9, lsl #3]
   3b4c0:	ldr	x11, [x26, x8]
   3b4c4:	sub	x9, x9, #0x1
   3b4c8:	sub	x8, x8, #0x8
   3b4cc:	cmp	x10, x11
   3b4d0:	b.eq	3b4b0 <__gmpn_toom32_mul@@Base+0x23c>  // b.none
   3b4d4:	b.ls	3b794 <__gmpn_toom32_mul@@Base+0x520>  // b.plast
   3b4d8:	ldr	x2, [sp, #48]
   3b4dc:	mov	w8, #0x18                  	// #24
   3b4e0:	madd	x0, x22, x8, x19
   3b4e4:	mov	x1, x26
   3b4e8:	mov	x3, x22
   3b4ec:	bl	c2d0 <__gmpn_sub_n@plt>
   3b4f0:	b	3b804 <__gmpn_toom32_mul@@Base+0x590>
   3b4f4:	ldur	x8, [x29, #-8]
   3b4f8:	cbz	x8, 3b540 <__gmpn_toom32_mul@@Base+0x2cc>
   3b4fc:	ldur	x20, [x29, #-8]
   3b500:	mov	x0, x21
   3b504:	mov	x1, x26
   3b508:	mov	x3, x20
   3b50c:	bl	ca70 <__gmpn_add_n@plt>
   3b510:	mov	x8, x20
   3b514:	cbz	x0, 3b540 <__gmpn_toom32_mul@@Base+0x2cc>
   3b518:	ldur	x8, [x29, #-8]
   3b51c:	add	x9, x19, x24, lsl #3
   3b520:	mov	w20, #0x1                   	// #1
   3b524:	cmp	x8, x28
   3b528:	b.gt	3b5f0 <__gmpn_toom32_mul@@Base+0x37c>
   3b52c:	ldr	x10, [x26, x8, lsl #3]
   3b530:	add	x8, x8, #0x1
   3b534:	adds	x10, x10, #0x1
   3b538:	str	x10, [x9], #8
   3b53c:	b.cs	3b524 <__gmpn_toom32_mul@@Base+0x2b0>  // b.hs, b.nlast
   3b540:	cmp	x21, x26
   3b544:	mov	x20, xzr
   3b548:	b.eq	3b5f0 <__gmpn_toom32_mul@@Base+0x37c>  // b.none
   3b54c:	cmp	x8, x28
   3b550:	b.gt	3b5f0 <__gmpn_toom32_mul@@Base+0x37c>
   3b554:	sub	x9, x28, x8
   3b558:	add	x9, x9, #0x1
   3b55c:	cmp	x9, #0x4
   3b560:	b.cc	3b5c4 <__gmpn_toom32_mul@@Base+0x350>  // b.lo, b.ul, b.last
   3b564:	add	x10, x8, x28
   3b568:	add	x12, x19, x10, lsl #3
   3b56c:	add	x10, x12, #0x8
   3b570:	add	x11, x26, x22, lsl #3
   3b574:	cmp	x10, x11
   3b578:	add	x11, x26, x8, lsl #3
   3b57c:	b.cs	3b590 <__gmpn_toom32_mul@@Base+0x31c>  // b.hs, b.nlast
   3b580:	add	x10, x19, x28, lsl #4
   3b584:	add	x10, x10, #0x10
   3b588:	cmp	x11, x10
   3b58c:	b.cc	3b5c4 <__gmpn_toom32_mul@@Base+0x350>  // b.lo, b.ul, b.last
   3b590:	and	x10, x9, #0xfffffffffffffffc
   3b594:	add	x11, x11, #0x10
   3b598:	add	x8, x8, x10
   3b59c:	add	x12, x12, #0x18
   3b5a0:	mov	x13, x10
   3b5a4:	ldp	q0, q1, [x11, #-16]
   3b5a8:	subs	x13, x13, #0x4
   3b5ac:	add	x11, x11, #0x20
   3b5b0:	stp	q0, q1, [x12, #-16]
   3b5b4:	add	x12, x12, #0x20
   3b5b8:	b.ne	3b5a4 <__gmpn_toom32_mul@@Base+0x330>  // b.any
   3b5bc:	cmp	x9, x10
   3b5c0:	b.eq	3b5ec <__gmpn_toom32_mul@@Base+0x378>  // b.none
   3b5c4:	add	x10, x8, x28
   3b5c8:	sub	x9, x28, x8
   3b5cc:	add	x10, x19, x10, lsl #3
   3b5d0:	add	x9, x9, #0x1
   3b5d4:	add	x10, x10, #0x8
   3b5d8:	add	x8, x26, x8, lsl #3
   3b5dc:	ldr	x11, [x8], #8
   3b5e0:	subs	x9, x9, #0x1
   3b5e4:	str	x11, [x10], #8
   3b5e8:	b.ne	3b5dc <__gmpn_toom32_mul@@Base+0x368>  // b.any
   3b5ec:	mov	x20, xzr
   3b5f0:	sub	x8, x24, x28, lsl #1
   3b5f4:	sub	x8, x8, #0x2
   3b5f8:	lsl	x9, x28, #3
   3b5fc:	ldr	x10, [x26, x9]
   3b600:	cbnz	x10, 3b648 <__gmpn_toom32_mul@@Base+0x3d4>
   3b604:	adds	x8, x8, #0x1
   3b608:	sub	x9, x9, #0x8
   3b60c:	b.cc	3b5fc <__gmpn_toom32_mul@@Base+0x388>  // b.lo, b.ul, b.last
   3b610:	sub	x8, x24, x28
   3b614:	lsl	x9, x24, #3
   3b618:	sub	x8, x8, #0x2
   3b61c:	sub	x9, x9, #0x8
   3b620:	add	x10, x8, #0x1
   3b624:	cmp	x10, #0x1
   3b628:	b.lt	3b648 <__gmpn_toom32_mul@@Base+0x3d4>  // b.tstop
   3b62c:	ldr	x10, [x26, x8, lsl #3]
   3b630:	ldr	x11, [x26, x9]
   3b634:	sub	x8, x8, #0x1
   3b638:	sub	x9, x9, #0x8
   3b63c:	cmp	x10, x11
   3b640:	b.eq	3b620 <__gmpn_toom32_mul@@Base+0x3ac>  // b.none
   3b644:	b.ls	3b7b4 <__gmpn_toom32_mul@@Base+0x540>  // b.plast
   3b648:	mov	w8, #0x18                  	// #24
   3b64c:	madd	x21, x22, x8, x19
   3b650:	ldur	x8, [x29, #-8]
   3b654:	cbz	x8, 3b6d0 <__gmpn_toom32_mul@@Base+0x45c>
   3b658:	ldur	x23, [x29, #-8]
   3b65c:	ldr	x2, [sp, #48]
   3b660:	mov	x0, x21
   3b664:	mov	x1, x26
   3b668:	mov	x3, x23
   3b66c:	bl	c2d0 <__gmpn_sub_n@plt>
   3b670:	mov	x8, x23
   3b674:	ldur	x23, [x29, #-16]
   3b678:	cbz	x0, 3b6d4 <__gmpn_toom32_mul@@Base+0x460>
   3b67c:	add	x8, x24, x28, lsl #1
   3b680:	add	x8, x19, x8, lsl #3
   3b684:	add	x9, x8, #0x10
   3b688:	ldur	x8, [x29, #-8]
   3b68c:	cmp	x8, x28
   3b690:	b.gt	3b78c <__gmpn_toom32_mul@@Base+0x518>
   3b694:	ldr	x10, [x26, x8, lsl #3]
   3b698:	add	x8, x8, #0x1
   3b69c:	sub	x11, x10, #0x1
   3b6a0:	str	x11, [x9], #8
   3b6a4:	cbz	x10, 3b68c <__gmpn_toom32_mul@@Base+0x418>
   3b6a8:	b	3b6d4 <__gmpn_toom32_mul@@Base+0x460>
   3b6ac:	add	x0, x19, x25, lsl #3
   3b6b0:	mov	x2, x19
   3b6b4:	mov	x3, x22
   3b6b8:	bl	c2d0 <__gmpn_sub_n@plt>
   3b6bc:	mov	w8, #0x1                   	// #1
   3b6c0:	mov	x20, xzr
   3b6c4:	str	xzr, [sp, #56]
   3b6c8:	stur	w8, [x29, #-28]
   3b6cc:	b	3b450 <__gmpn_toom32_mul@@Base+0x1dc>
   3b6d0:	ldur	x23, [x29, #-16]
   3b6d4:	cmp	x21, x26
   3b6d8:	b.eq	3b78c <__gmpn_toom32_mul@@Base+0x518>  // b.none
   3b6dc:	cmp	x8, x28
   3b6e0:	b.gt	3b78c <__gmpn_toom32_mul@@Base+0x518>
   3b6e4:	ldur	x21, [x29, #-24]
   3b6e8:	sub	x9, x28, x8
   3b6ec:	add	x9, x9, #0x1
   3b6f0:	cmp	x9, #0x4
   3b6f4:	b.cc	3b75c <__gmpn_toom32_mul@@Base+0x4e8>  // b.lo, b.ul, b.last
   3b6f8:	add	x10, x28, x28, lsl #1
   3b6fc:	add	x10, x8, x10
   3b700:	add	x12, x19, x10, lsl #3
   3b704:	add	x10, x12, #0x18
   3b708:	add	x11, x26, x22, lsl #3
   3b70c:	cmp	x10, x11
   3b710:	add	x11, x26, x8, lsl #3
   3b714:	b.cs	3b728 <__gmpn_toom32_mul@@Base+0x4b4>  // b.hs, b.nlast
   3b718:	add	x10, x19, x28, lsl #5
   3b71c:	add	x10, x10, #0x20
   3b720:	cmp	x11, x10
   3b724:	b.cc	3b75c <__gmpn_toom32_mul@@Base+0x4e8>  // b.lo, b.ul, b.last
   3b728:	and	x10, x9, #0xfffffffffffffffc
   3b72c:	add	x11, x11, #0x10
   3b730:	add	x8, x8, x10
   3b734:	add	x12, x12, #0x28
   3b738:	mov	x13, x10
   3b73c:	ldp	q0, q1, [x11, #-16]
   3b740:	subs	x13, x13, #0x4
   3b744:	add	x11, x11, #0x20
   3b748:	stp	q0, q1, [x12, #-16]
   3b74c:	add	x12, x12, #0x20
   3b750:	b.ne	3b73c <__gmpn_toom32_mul@@Base+0x4c8>  // b.any
   3b754:	cmp	x9, x10
   3b758:	b.eq	3b804 <__gmpn_toom32_mul@@Base+0x590>  // b.none
   3b75c:	add	x10, x28, x28, lsl #1
   3b760:	add	x10, x8, x10
   3b764:	sub	x9, x28, x8
   3b768:	add	x10, x19, x10, lsl #3
   3b76c:	add	x9, x9, #0x1
   3b770:	add	x10, x10, #0x18
   3b774:	add	x8, x26, x8, lsl #3
   3b778:	ldr	x11, [x8], #8
   3b77c:	subs	x9, x9, #0x1
   3b780:	str	x11, [x10], #8
   3b784:	b.ne	3b778 <__gmpn_toom32_mul@@Base+0x504>  // b.any
   3b788:	b	3b804 <__gmpn_toom32_mul@@Base+0x590>
   3b78c:	ldur	x21, [x29, #-24]
   3b790:	b	3b804 <__gmpn_toom32_mul@@Base+0x590>
   3b794:	ldr	x1, [sp, #48]
   3b798:	mov	w8, #0x18                  	// #24
   3b79c:	madd	x0, x22, x8, x19
   3b7a0:	mov	x2, x26
   3b7a4:	mov	x3, x22
   3b7a8:	bl	c2d0 <__gmpn_sub_n@plt>
   3b7ac:	ldur	w8, [x29, #-28]
   3b7b0:	b	3b7fc <__gmpn_toom32_mul@@Base+0x588>
   3b7b4:	ldr	x1, [sp, #48]
   3b7b8:	ldur	x3, [x29, #-8]
   3b7bc:	mov	w8, #0x18                  	// #24
   3b7c0:	madd	x21, x22, x8, x19
   3b7c4:	mov	x0, x21
   3b7c8:	mov	x2, x26
   3b7cc:	bl	c2d0 <__gmpn_sub_n@plt>
   3b7d0:	cbz	x23, 3b7f4 <__gmpn_toom32_mul@@Base+0x580>
   3b7d4:	ldur	x8, [x29, #-8]
   3b7d8:	mov	w1, wzr
   3b7dc:	add	x0, x21, x8, lsl #3
   3b7e0:	lsl	x8, x28, #1
   3b7e4:	sub	x8, x8, x24
   3b7e8:	lsl	x8, x8, #3
   3b7ec:	add	x2, x8, #0x10
   3b7f0:	bl	c5f0 <memset@plt>
   3b7f4:	ldur	w8, [x29, #-28]
   3b7f8:	ldp	x21, x23, [x29, #-24]
   3b7fc:	eor	w8, w8, #0x1
   3b800:	stur	w8, [x29, #-28]
   3b804:	mov	x0, x23
   3b808:	mov	x1, x19
   3b80c:	mov	x2, x21
   3b810:	mov	x3, x22
   3b814:	bl	c990 <__gmpn_mul_n@plt>
   3b818:	cmp	x27, #0x2
   3b81c:	b.eq	3bbf4 <__gmpn_toom32_mul@@Base+0x980>  // b.none
   3b820:	cmp	x27, #0x1
   3b824:	b.ne	3bc14 <__gmpn_toom32_mul@@Base+0x9a0>  // b.any
   3b828:	add	x0, x23, x22, lsl #3
   3b82c:	mov	x1, x0
   3b830:	mov	x2, x21
   3b834:	mov	x3, x22
   3b838:	bl	ca70 <__gmpn_add_n@plt>
   3b83c:	add	x21, x0, x20
   3b840:	cbz	x20, 3b85c <__gmpn_toom32_mul@@Base+0x5e8>
   3b844:	add	x0, x23, x22, lsl #3
   3b848:	mov	x1, x0
   3b84c:	mov	x2, x19
   3b850:	mov	x3, x22
   3b854:	bl	ca70 <__gmpn_add_n@plt>
   3b858:	add	x21, x0, x21
   3b85c:	mov	x8, x23
   3b860:	mov	x27, x25
   3b864:	lsl	x25, x25, #3
   3b868:	add	x23, x22, x22, lsl #1
   3b86c:	str	x21, [x8, x25]
   3b870:	add	x20, x19, x25
   3b874:	add	x21, x19, x23, lsl #3
   3b878:	mov	x0, x19
   3b87c:	mov	x1, x20
   3b880:	mov	x2, x21
   3b884:	mov	x3, x22
   3b888:	bl	c990 <__gmpn_mul_n@plt>
   3b88c:	ldr	x8, [sp, #56]
   3b890:	cbz	x8, 3b8ac <__gmpn_toom32_mul@@Base+0x638>
   3b894:	ldur	x0, [x29, #-24]
   3b898:	mov	x2, x21
   3b89c:	mov	x3, x22
   3b8a0:	mov	x1, x0
   3b8a4:	bl	ca70 <__gmpn_add_n@plt>
   3b8a8:	b	3b8b0 <__gmpn_toom32_mul@@Base+0x63c>
   3b8ac:	mov	x0, xzr
   3b8b0:	ldur	w8, [x29, #-28]
   3b8b4:	orr	x3, x27, #0x1
   3b8b8:	str	x28, [sp, #56]
   3b8bc:	str	x0, [x20]
   3b8c0:	str	x24, [sp, #16]
   3b8c4:	cbz	w8, 3b8e0 <__gmpn_toom32_mul@@Base+0x66c>
   3b8c8:	ldur	x28, [x29, #-16]
   3b8cc:	mov	x2, x19
   3b8d0:	mov	x0, x28
   3b8d4:	mov	x1, x28
   3b8d8:	bl	c840 <__gmpn_rsh1sub_n@plt>
   3b8dc:	b	3b8f4 <__gmpn_toom32_mul@@Base+0x680>
   3b8e0:	ldur	x28, [x29, #-16]
   3b8e4:	mov	x2, x19
   3b8e8:	mov	x0, x28
   3b8ec:	mov	x1, x28
   3b8f0:	bl	c950 <__gmpn_rsh1add_n@plt>
   3b8f4:	ldr	x8, [x20]
   3b8f8:	add	x24, x28, x22, lsl #3
   3b8fc:	mov	x0, x20
   3b900:	mov	x1, x28
   3b904:	mov	x2, x24
   3b908:	mov	x3, x22
   3b90c:	str	x8, [sp, #32]
   3b910:	bl	ca70 <__gmpn_add_n@plt>
   3b914:	ldr	x8, [x28, x25]
   3b918:	ldr	x9, [x24]
   3b91c:	add	x8, x8, x0
   3b920:	add	x8, x9, x8
   3b924:	str	x8, [x24]
   3b928:	ldr	x9, [x28, x25]
   3b92c:	ldr	x28, [sp, #56]
   3b930:	add	x9, x9, x0
   3b934:	cmp	x8, x9
   3b938:	b.cs	3b958 <__gmpn_toom32_mul@@Base+0x6e4>  // b.hs, b.nlast
   3b93c:	ldur	x8, [x29, #-16]
   3b940:	add	x8, x8, x28, lsl #3
   3b944:	add	x8, x8, #0x10
   3b948:	ldr	x9, [x8]
   3b94c:	adds	x9, x9, #0x1
   3b950:	str	x9, [x8], #8
   3b954:	b.cs	3b948 <__gmpn_toom32_mul@@Base+0x6d4>  // b.hs, b.nlast
   3b958:	ldur	w8, [x29, #-28]
   3b95c:	mov	x25, x27
   3b960:	cbz	w8, 3b9d4 <__gmpn_toom32_mul@@Base+0x760>
   3b964:	ldur	x27, [x29, #-16]
   3b968:	mov	x2, x19
   3b96c:	mov	x3, x22
   3b970:	mov	x0, x27
   3b974:	mov	x1, x27
   3b978:	bl	ca70 <__gmpn_add_n@plt>
   3b97c:	ldur	x2, [x29, #-24]
   3b980:	mov	x4, x0
   3b984:	mov	x0, x20
   3b988:	mov	x1, x20
   3b98c:	mov	x3, x22
   3b990:	bl	ce90 <__gmpn_add_nc@plt>
   3b994:	ldur	x10, [x29, #-48]
   3b998:	ldr	x9, [sp, #32]
   3b99c:	ldr	x8, [x27, x10]
   3b9a0:	add	x9, x0, x9
   3b9a4:	adds	x8, x8, x9
   3b9a8:	str	x8, [x27, x10]
   3b9ac:	ldur	x27, [x29, #-40]
   3b9b0:	b.cc	3ba40 <__gmpn_toom32_mul@@Base+0x7cc>  // b.lo, b.ul, b.last
   3b9b4:	ldur	x8, [x29, #-16]
   3b9b8:	add	x8, x8, x28, lsl #3
   3b9bc:	add	x8, x8, #0x10
   3b9c0:	ldr	x9, [x8]
   3b9c4:	adds	x9, x9, #0x1
   3b9c8:	str	x9, [x8], #8
   3b9cc:	b.cs	3b9c0 <__gmpn_toom32_mul@@Base+0x74c>  // b.hs, b.nlast
   3b9d0:	b	3ba40 <__gmpn_toom32_mul@@Base+0x7cc>
   3b9d4:	ldur	x27, [x29, #-16]
   3b9d8:	mov	x2, x19
   3b9dc:	mov	x3, x22
   3b9e0:	mov	x0, x27
   3b9e4:	mov	x1, x27
   3b9e8:	bl	c2d0 <__gmpn_sub_n@plt>
   3b9ec:	ldur	x2, [x29, #-24]
   3b9f0:	mov	x4, x0
   3b9f4:	mov	x0, x20
   3b9f8:	mov	x1, x20
   3b9fc:	mov	x3, x22
   3ba00:	bl	c760 <__gmpn_sub_nc@plt>
   3ba04:	ldur	x10, [x29, #-48]
   3ba08:	ldr	x9, [sp, #32]
   3ba0c:	ldr	x8, [x27, x10]
   3ba10:	add	x9, x0, x9
   3ba14:	subs	x8, x8, x9
   3ba18:	str	x8, [x27, x10]
   3ba1c:	ldur	x27, [x29, #-40]
   3ba20:	b.cs	3ba40 <__gmpn_toom32_mul@@Base+0x7cc>  // b.hs, b.nlast
   3ba24:	ldur	x8, [x29, #-16]
   3ba28:	add	x8, x8, x28, lsl #3
   3ba2c:	add	x8, x8, #0x10
   3ba30:	ldr	x9, [x8]
   3ba34:	sub	x10, x9, #0x1
   3ba38:	str	x10, [x8], #8
   3ba3c:	cbz	x9, 3ba30 <__gmpn_toom32_mul@@Base+0x7bc>
   3ba40:	ldr	x1, [sp, #40]
   3ba44:	mov	x0, x19
   3ba48:	mov	x2, x26
   3ba4c:	mov	x3, x22
   3ba50:	bl	c990 <__gmpn_mul_n@plt>
   3ba54:	ldur	x8, [x29, #-8]
   3ba58:	mov	x0, x21
   3ba5c:	str	x23, [sp]
   3ba60:	cmp	x27, x8
   3ba64:	b.le	3ba7c <__gmpn_toom32_mul@@Base+0x808>
   3ba68:	ldr	x1, [sp, #24]
   3ba6c:	ldr	x3, [sp, #48]
   3ba70:	ldur	x4, [x29, #-8]
   3ba74:	mov	x2, x27
   3ba78:	b	3ba8c <__gmpn_toom32_mul@@Base+0x818>
   3ba7c:	ldr	x1, [sp, #48]
   3ba80:	ldur	x2, [x29, #-8]
   3ba84:	ldr	x3, [sp, #24]
   3ba88:	mov	x4, x27
   3ba8c:	bl	ccd0 <__gmpn_mul@plt>
   3ba90:	ldur	x27, [x29, #-24]
   3ba94:	mov	x2, x21
   3ba98:	mov	x3, x22
   3ba9c:	mov	x0, x27
   3baa0:	mov	x1, x27
   3baa4:	bl	c2d0 <__gmpn_sub_n@plt>
   3baa8:	ldur	x28, [x29, #-16]
   3baac:	mov	x26, x0
   3bab0:	mov	x0, x20
   3bab4:	mov	x1, x20
   3bab8:	ldr	x23, [x28, x25, lsl #3]
   3babc:	mov	x2, x19
   3bac0:	mov	x3, x22
   3bac4:	mov	x4, x26
   3bac8:	bl	c760 <__gmpn_sub_nc@plt>
   3bacc:	mov	x4, x0
   3bad0:	mov	x0, x21
   3bad4:	mov	x1, x24
   3bad8:	mov	x2, x27
   3badc:	mov	x3, x22
   3bae0:	bl	c760 <__gmpn_sub_nc@plt>
   3bae4:	mov	x24, x0
   3bae8:	cbz	x22, 3bb40 <__gmpn_toom32_mul@@Base+0x8cc>
   3baec:	mov	x0, x27
   3baf0:	mov	x1, x27
   3baf4:	mov	x2, x28
   3baf8:	mov	x3, x22
   3bafc:	bl	ca70 <__gmpn_add_n@plt>
   3bb00:	cbz	x0, 3bb40 <__gmpn_toom32_mul@@Base+0x8cc>
   3bb04:	ldr	x28, [sp, #56]
   3bb08:	ldr	x13, [sp]
   3bb0c:	mov	w8, #0x1                   	// #1
   3bb10:	mov	x9, x20
   3bb14:	mov	x12, x22
   3bb18:	cmp	x22, x13
   3bb1c:	b.ge	3bb38 <__gmpn_toom32_mul@@Base+0x8c4>  // b.tcont
   3bb20:	ldr	x11, [x9]
   3bb24:	add	x22, x22, #0x1
   3bb28:	adds	x11, x11, #0x1
   3bb2c:	str	x11, [x9], #8
   3bb30:	b.cs	3bb18 <__gmpn_toom32_mul@@Base+0x8a4>  // b.hs, b.nlast
   3bb34:	mov	x8, xzr
   3bb38:	mov	x22, x12
   3bb3c:	b	3bb48 <__gmpn_toom32_mul@@Base+0x8d4>
   3bb40:	ldr	x28, [sp, #56]
   3bb44:	mov	x8, xzr
   3bb48:	ldur	x9, [x29, #-8]
   3bb4c:	ldur	x10, [x29, #-40]
   3bb50:	add	x9, x10, x9
   3bb54:	cmp	x9, x22
   3bb58:	b.le	3bc5c <__gmpn_toom32_mul@@Base+0x9e8>
   3bb5c:	add	x10, x23, x26
   3bb60:	subs	x23, x9, x22
   3bb64:	sub	x9, x10, x24
   3bb68:	add	x24, x9, x8
   3bb6c:	add	x21, x19, x22, lsl #5
   3bb70:	b.eq	3bbbc <__gmpn_toom32_mul@@Base+0x948>  // b.none
   3bb74:	mov	x0, x20
   3bb78:	mov	x1, x20
   3bb7c:	mov	x2, x21
   3bb80:	mov	x3, x23
   3bb84:	bl	c2d0 <__gmpn_sub_n@plt>
   3bb88:	cbz	x0, 3bbbc <__gmpn_toom32_mul@@Base+0x948>
   3bb8c:	ldp	x9, x8, [sp, #8]
   3bb90:	add	x8, x8, x9
   3bb94:	sub	x8, x8, x28, lsl #1
   3bb98:	add	x8, x19, x8, lsl #3
   3bb9c:	sub	x8, x8, #0x10
   3bba0:	cmp	x23, x25
   3bba4:	b.ge	3bc20 <__gmpn_toom32_mul@@Base+0x9ac>  // b.tcont
   3bba8:	ldr	x9, [x8]
   3bbac:	add	x23, x23, #0x1
   3bbb0:	sub	x10, x9, #0x1
   3bbb4:	str	x10, [x8], #8
   3bbb8:	cbz	x9, 3bba0 <__gmpn_toom32_mul@@Base+0x92c>
   3bbbc:	mov	x8, xzr
   3bbc0:	adds	x8, x24, x8
   3bbc4:	b.mi	3bc2c <__gmpn_toom32_mul@@Base+0x9b8>  // b.first
   3bbc8:	ldr	x9, [x21]
   3bbcc:	adds	x8, x9, x8
   3bbd0:	str	x8, [x21]
   3bbd4:	b.cc	3bc5c <__gmpn_toom32_mul@@Base+0x9e8>  // b.lo, b.ul, b.last
   3bbd8:	add	x8, x19, x28, lsl #5
   3bbdc:	add	x8, x8, #0x28
   3bbe0:	ldr	x9, [x8]
   3bbe4:	adds	x9, x9, #0x1
   3bbe8:	str	x9, [x8], #8
   3bbec:	b.cs	3bbe0 <__gmpn_toom32_mul@@Base+0x96c>  // b.hs, b.nlast
   3bbf0:	b	3bc5c <__gmpn_toom32_mul@@Base+0x9e8>
   3bbf4:	add	x0, x23, x22, lsl #3
   3bbf8:	mov	x1, x0
   3bbfc:	mov	x2, x21
   3bc00:	mov	x3, x22
   3bc04:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3bc08:	add	x21, x0, x20, lsl #1
   3bc0c:	cbnz	x20, 3b844 <__gmpn_toom32_mul@@Base+0x5d0>
   3bc10:	b	3b85c <__gmpn_toom32_mul@@Base+0x5e8>
   3bc14:	mov	x21, xzr
   3bc18:	cbnz	x20, 3b844 <__gmpn_toom32_mul@@Base+0x5d0>
   3bc1c:	b	3b85c <__gmpn_toom32_mul@@Base+0x5e8>
   3bc20:	mov	x8, #0xffffffffffffffff    	// #-1
   3bc24:	adds	x8, x24, x8
   3bc28:	b.pl	3bbc8 <__gmpn_toom32_mul@@Base+0x954>  // b.nfrst
   3bc2c:	ldr	x9, [x21]
   3bc30:	neg	x10, x8
   3bc34:	add	x8, x9, x8
   3bc38:	cmp	x9, x10
   3bc3c:	str	x8, [x21]
   3bc40:	b.cs	3bc5c <__gmpn_toom32_mul@@Base+0x9e8>  // b.hs, b.nlast
   3bc44:	add	x8, x19, x28, lsl #5
   3bc48:	add	x8, x8, #0x28
   3bc4c:	ldr	x9, [x8]
   3bc50:	sub	x10, x9, #0x1
   3bc54:	str	x10, [x8], #8
   3bc58:	cbz	x9, 3bc4c <__gmpn_toom32_mul@@Base+0x9d8>
   3bc5c:	ldp	x20, x19, [sp, #192]
   3bc60:	ldp	x22, x21, [sp, #176]
   3bc64:	ldp	x24, x23, [sp, #160]
   3bc68:	ldp	x26, x25, [sp, #144]
   3bc6c:	ldp	x28, x27, [sp, #128]
   3bc70:	ldp	x29, x30, [sp, #112]
   3bc74:	add	sp, sp, #0xd0
   3bc78:	ret

000000000003bc7c <__gmpn_toom42_mul@@Base>:
   3bc7c:	stp	x29, x30, [sp, #-96]!
   3bc80:	stp	x28, x27, [sp, #16]
   3bc84:	stp	x26, x25, [sp, #32]
   3bc88:	stp	x24, x23, [sp, #48]
   3bc8c:	stp	x22, x21, [sp, #64]
   3bc90:	stp	x20, x19, [sp, #80]
   3bc94:	mov	x29, sp
   3bc98:	sub	sp, sp, #0x70
   3bc9c:	add	x8, x2, #0x3
   3bca0:	add	x9, x4, #0x1
   3bca4:	cmp	x2, x4, lsl #1
   3bca8:	asr	x8, x8, #2
   3bcac:	asr	x9, x9, #1
   3bcb0:	mov	w10, #0x30                  	// #48
   3bcb4:	csel	x23, x9, x8, lt  // lt = tstop
   3bcb8:	mul	x8, x23, x10
   3bcbc:	stp	x0, x1, [x29, #-24]
   3bcc0:	add	x22, x23, x23, lsl #1
   3bcc4:	add	x1, x8, #0x28
   3bcc8:	mov	w8, #0x7f00                	// #32512
   3bccc:	mov	x19, x4
   3bcd0:	mov	x21, x3
   3bcd4:	mov	x20, x2
   3bcd8:	sub	x4, x2, x22
   3bcdc:	cmp	x1, x8
   3bce0:	stur	x5, [x29, #-48]
   3bce4:	stur	xzr, [x29, #-8]
   3bce8:	stur	x4, [x29, #-32]
   3bcec:	b.hi	3c454 <__gmpn_toom42_mul@@Base+0x7d8>  // b.pmore
   3bcf0:	add	x9, x1, #0xf
   3bcf4:	mov	x8, sp
   3bcf8:	and	x9, x9, #0xfffffffffffffff0
   3bcfc:	sub	x26, x8, x9
   3bd00:	mov	sp, x26
   3bd04:	add	x8, x23, #0x1
   3bd08:	stur	x8, [x29, #-88]
   3bd0c:	lsl	x8, x8, #3
   3bd10:	ldp	x5, x2, [x29, #-24]
   3bd14:	add	x1, x26, x8
   3bd18:	add	x27, x1, x8
   3bd1c:	add	x9, x27, x8
   3bd20:	mov	x0, x26
   3bd24:	mov	x3, x23
   3bd28:	stur	x19, [x29, #-56]
   3bd2c:	sub	x25, x19, x23
   3bd30:	stp	x1, x9, [x29, #-112]
   3bd34:	add	x28, x9, x8
   3bd38:	mov	x19, x4
   3bd3c:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3bd40:	and	w8, w0, #0x1
   3bd44:	stur	w8, [x29, #-76]
   3bd48:	ldur	x8, [x29, #-16]
   3bd4c:	mov	x0, x27
   3bd50:	mov	x3, x19
   3bd54:	add	x24, x8, x23, lsl #4
   3bd58:	ldur	x8, [x29, #-16]
   3bd5c:	mov	x1, x24
   3bd60:	add	x2, x8, x22, lsl #3
   3bd64:	stur	x2, [x29, #-96]
   3bd68:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3bd6c:	ldur	x11, [x29, #-16]
   3bd70:	subs	x8, x23, x19
   3bd74:	mov	x22, x0
   3bd78:	b.eq	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3bd7c:	ldur	x9, [x29, #-32]
   3bd80:	lsl	x9, x9, #3
   3bd84:	ldr	x10, [x24, x9]
   3bd88:	adds	x10, x10, x22
   3bd8c:	str	x10, [x27, x9]
   3bd90:	b.cc	3bdfc <__gmpn_toom42_mul@@Base+0x180>  // b.lo, b.ul, b.last
   3bd94:	lsl	x9, x20, #3
   3bd98:	sub	x10, x20, x23
   3bd9c:	sub	x9, x9, x23, lsl #3
   3bda0:	add	x10, x11, x10, lsl #3
   3bda4:	add	x9, x9, x26
   3bda8:	mov	w22, #0x1                   	// #1
   3bdac:	add	x1, x10, #0x8
   3bdb0:	add	x0, x9, #0x18
   3bdb4:	mov	w9, #0x1                   	// #1
   3bdb8:	cmp	x9, x8
   3bdbc:	b.ge	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.tcont
   3bdc0:	ldr	x10, [x1], #8
   3bdc4:	add	x9, x9, #0x1
   3bdc8:	adds	x10, x10, #0x1
   3bdcc:	str	x10, [x0], #8
   3bdd0:	b.cs	3bdb8 <__gmpn_toom42_mul@@Base+0x13c>  // b.hs, b.nlast
   3bdd4:	cmp	x24, x27
   3bdd8:	mov	x22, xzr
   3bddc:	b.eq	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3bde0:	cmp	x9, x8
   3bde4:	b.ge	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.tcont
   3bde8:	lsl	x8, x23, #2
   3bdec:	sub	x8, x8, x20
   3bdf0:	sub	x8, x8, x9
   3bdf4:	lsl	x2, x8, #3
   3bdf8:	b	3be38 <__gmpn_toom42_mul@@Base+0x1bc>
   3bdfc:	cmp	x8, #0x2
   3be00:	mov	x22, xzr
   3be04:	b.lt	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.tstop
   3be08:	cmp	x24, x27
   3be0c:	b.eq	3be44 <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3be10:	lsl	x8, x20, #3
   3be14:	sub	x9, x20, x23
   3be18:	mvn	x10, x20
   3be1c:	sub	x8, x8, x23, lsl #3
   3be20:	add	x9, x11, x9, lsl #3
   3be24:	add	x10, x10, x23, lsl #2
   3be28:	add	x8, x8, x26
   3be2c:	add	x1, x9, #0x8
   3be30:	add	x0, x8, #0x18
   3be34:	lsl	x2, x10, #3
   3be38:	bl	bed0 <memcpy@plt>
   3be3c:	ldur	x11, [x29, #-16]
   3be40:	mov	x22, xzr
   3be44:	mov	x19, x28
   3be48:	add	x8, x28, x23, lsl #3
   3be4c:	lsl	x28, x23, #3
   3be50:	add	x1, x11, x28
   3be54:	mov	x0, x27
   3be58:	mov	x2, x27
   3be5c:	mov	x3, x23
   3be60:	stur	x8, [x29, #-40]
   3be64:	mov	x24, x11
   3be68:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be6c:	add	x20, x0, x22, lsl #1
   3be70:	mov	x0, x27
   3be74:	mov	x1, x24
   3be78:	mov	x2, x27
   3be7c:	mov	x3, x23
   3be80:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be84:	add	x8, x0, x20, lsl #1
   3be88:	subs	x22, x23, x25
   3be8c:	add	x24, x21, x28
   3be90:	str	x8, [x27, x28]
   3be94:	stp	x24, x28, [x29, #-72]
   3be98:	b.ne	3bf0c <__gmpn_toom42_mul@@Base+0x290>  // b.any
   3be9c:	ldur	x28, [x29, #-104]
   3bea0:	mov	x1, x21
   3bea4:	mov	x2, x24
   3bea8:	mov	x3, x23
   3beac:	mov	x0, x28
   3beb0:	bl	ca70 <__gmpn_add_n@plt>
   3beb4:	ldp	x20, x22, [x29, #-56]
   3beb8:	lsl	x8, x23, #4
   3bebc:	sub	x8, x8, #0x8
   3bec0:	mov	x10, x23
   3bec4:	str	x0, [x28, x23, lsl #3]
   3bec8:	subs	x9, x10, #0x1
   3becc:	b.lt	3bef0 <__gmpn_toom42_mul@@Base+0x274>  // b.tstop
   3bed0:	add	x10, x21, x10, lsl #3
   3bed4:	ldur	x10, [x10, #-8]
   3bed8:	ldr	x11, [x21, x8]
   3bedc:	sub	x8, x8, #0x8
   3bee0:	cmp	x10, x11
   3bee4:	mov	x10, x9
   3bee8:	b.eq	3bec8 <__gmpn_toom42_mul@@Base+0x24c>  // b.none
   3beec:	b.ls	3c0ac <__gmpn_toom42_mul@@Base+0x430>  // b.plast
   3bef0:	mov	x0, x19
   3bef4:	mov	x1, x21
   3bef8:	mov	x2, x24
   3befc:	mov	x3, x23
   3bf00:	bl	c2d0 <__gmpn_sub_n@plt>
   3bf04:	cbnz	x25, 3c0d0 <__gmpn_toom42_mul@@Base+0x454>
   3bf08:	b	3c17c <__gmpn_toom42_mul@@Base+0x500>
   3bf0c:	ldur	x28, [x29, #-104]
   3bf10:	ldur	x20, [x29, #-56]
   3bf14:	cbz	x25, 3bf6c <__gmpn_toom42_mul@@Base+0x2f0>
   3bf18:	mov	x0, x28
   3bf1c:	mov	x1, x21
   3bf20:	mov	x2, x24
   3bf24:	mov	x3, x25
   3bf28:	bl	ca70 <__gmpn_add_n@plt>
   3bf2c:	mov	x8, x25
   3bf30:	cbz	x0, 3bf70 <__gmpn_toom42_mul@@Base+0x2f4>
   3bf34:	lsl	x8, x23, #4
   3bf38:	add	x8, x8, x20, lsl #3
   3bf3c:	add	x8, x8, x26
   3bf40:	add	x10, x8, #0x18
   3bf44:	mov	w9, #0x1                   	// #1
   3bf48:	mov	x8, x25
   3bf4c:	cmp	x8, x23
   3bf50:	b.ge	3bfac <__gmpn_toom42_mul@@Base+0x330>  // b.tcont
   3bf54:	ldr	x11, [x21, x8, lsl #3]
   3bf58:	add	x8, x8, #0x1
   3bf5c:	adds	x11, x11, #0x1
   3bf60:	str	x11, [x10], #8
   3bf64:	b.cs	3bf4c <__gmpn_toom42_mul@@Base+0x2d0>  // b.hs, b.nlast
   3bf68:	b	3bf70 <__gmpn_toom42_mul@@Base+0x2f4>
   3bf6c:	mov	x8, xzr
   3bf70:	cmp	x28, x21
   3bf74:	mov	x9, xzr
   3bf78:	b.eq	3bfac <__gmpn_toom42_mul@@Base+0x330>  // b.none
   3bf7c:	cmp	x8, x23
   3bf80:	b.ge	3bfac <__gmpn_toom42_mul@@Base+0x330>  // b.tcont
   3bf84:	mov	w9, #0x18                  	// #24
   3bf88:	lsl	x8, x8, #3
   3bf8c:	madd	x9, x23, x9, x8
   3bf90:	add	x9, x9, x26
   3bf94:	add	x0, x9, #0x18
   3bf98:	ldur	x9, [x29, #-64]
   3bf9c:	add	x1, x21, x8
   3bfa0:	sub	x2, x9, x8
   3bfa4:	bl	bed0 <memcpy@plt>
   3bfa8:	mov	x9, xzr
   3bfac:	ldur	x10, [x29, #-64]
   3bfb0:	sub	x8, x20, x23, lsl #1
   3bfb4:	str	x9, [x28, x10]
   3bfb8:	sub	x9, x10, #0x8
   3bfbc:	ldr	x10, [x21, x9]
   3bfc0:	cbnz	x10, 3c008 <__gmpn_toom42_mul@@Base+0x38c>
   3bfc4:	adds	x8, x8, #0x1
   3bfc8:	sub	x9, x9, #0x8
   3bfcc:	b.cc	3bfbc <__gmpn_toom42_mul@@Base+0x340>  // b.lo, b.ul, b.last
   3bfd0:	ldur	x8, [x29, #-56]
   3bfd4:	mov	x10, x25
   3bfd8:	lsl	x24, x8, #3
   3bfdc:	sub	x8, x24, #0x8
   3bfe0:	subs	x9, x10, #0x1
   3bfe4:	b.lt	3c008 <__gmpn_toom42_mul@@Base+0x38c>  // b.tstop
   3bfe8:	add	x10, x21, x10, lsl #3
   3bfec:	ldur	x10, [x10, #-8]
   3bff0:	ldr	x11, [x21, x8]
   3bff4:	sub	x8, x8, #0x8
   3bff8:	cmp	x10, x11
   3bffc:	mov	x10, x9
   3c000:	b.eq	3bfe0 <__gmpn_toom42_mul@@Base+0x364>  // b.none
   3c004:	b.ls	3c12c <__gmpn_toom42_mul@@Base+0x4b0>  // b.plast
   3c008:	cbz	x25, 3c068 <__gmpn_toom42_mul@@Base+0x3ec>
   3c00c:	ldur	x24, [x29, #-72]
   3c010:	mov	x0, x19
   3c014:	mov	x1, x21
   3c018:	mov	x3, x25
   3c01c:	mov	x2, x24
   3c020:	bl	c2d0 <__gmpn_sub_n@plt>
   3c024:	ldp	x20, x22, [x29, #-56]
   3c028:	mov	x8, x25
   3c02c:	cbz	x0, 3c074 <__gmpn_toom42_mul@@Base+0x3f8>
   3c030:	mov	w8, #0x18                  	// #24
   3c034:	mul	x8, x23, x8
   3c038:	add	x8, x8, x20, lsl #3
   3c03c:	add	x8, x8, x26
   3c040:	add	x9, x8, #0x20
   3c044:	mov	x8, x25
   3c048:	cmp	x8, x23
   3c04c:	b.ge	3c0cc <__gmpn_toom42_mul@@Base+0x450>  // b.tcont
   3c050:	ldr	x10, [x21, x8, lsl #3]
   3c054:	add	x8, x8, #0x1
   3c058:	sub	x11, x10, #0x1
   3c05c:	str	x11, [x9], #8
   3c060:	cbz	x10, 3c048 <__gmpn_toom42_mul@@Base+0x3cc>
   3c064:	b	3c074 <__gmpn_toom42_mul@@Base+0x3f8>
   3c068:	ldp	x20, x22, [x29, #-56]
   3c06c:	ldur	x24, [x29, #-72]
   3c070:	mov	x8, xzr
   3c074:	cmp	x19, x21
   3c078:	b.eq	3c0cc <__gmpn_toom42_mul@@Base+0x450>  // b.none
   3c07c:	cmp	x8, x23
   3c080:	b.ge	3c0cc <__gmpn_toom42_mul@@Base+0x450>  // b.tcont
   3c084:	lsl	x8, x8, #3
   3c088:	add	x9, x8, x23, lsl #5
   3c08c:	add	x9, x9, x26
   3c090:	add	x0, x9, #0x20
   3c094:	ldur	x9, [x29, #-64]
   3c098:	add	x1, x21, x8
   3c09c:	sub	x2, x9, x8
   3c0a0:	bl	bed0 <memcpy@plt>
   3c0a4:	cbnz	x25, 3c0d0 <__gmpn_toom42_mul@@Base+0x454>
   3c0a8:	b	3c17c <__gmpn_toom42_mul@@Base+0x500>
   3c0ac:	mov	x0, x19
   3c0b0:	mov	x1, x24
   3c0b4:	mov	x2, x21
   3c0b8:	mov	x3, x23
   3c0bc:	bl	c2d0 <__gmpn_sub_n@plt>
   3c0c0:	ldur	w8, [x29, #-76]
   3c0c4:	eor	w8, w8, #0x1
   3c0c8:	stur	w8, [x29, #-76]
   3c0cc:	cbz	x25, 3c17c <__gmpn_toom42_mul@@Base+0x500>
   3c0d0:	ldur	x0, [x29, #-40]
   3c0d4:	mov	x1, x28
   3c0d8:	mov	x2, x24
   3c0dc:	mov	x3, x25
   3c0e0:	bl	ca70 <__gmpn_add_n@plt>
   3c0e4:	mov	x8, x25
   3c0e8:	cbz	x0, 3c180 <__gmpn_toom42_mul@@Base+0x504>
   3c0ec:	lsl	x8, x20, #3
   3c0f0:	add	x9, x8, x23, lsl #5
   3c0f4:	add	x8, x8, x23, lsl #4
   3c0f8:	add	x9, x9, x26
   3c0fc:	add	x8, x8, x26
   3c100:	add	x9, x9, #0x20
   3c104:	add	x10, x8, #0x18
   3c108:	mov	x8, x25
   3c10c:	cmp	x8, x23
   3c110:	b.gt	3c274 <__gmpn_toom42_mul@@Base+0x5f8>
   3c114:	ldr	x11, [x10], #8
   3c118:	add	x8, x8, #0x1
   3c11c:	adds	x11, x11, #0x1
   3c120:	str	x11, [x9], #8
   3c124:	b.cs	3c10c <__gmpn_toom42_mul@@Base+0x490>  // b.hs, b.nlast
   3c128:	b	3c180 <__gmpn_toom42_mul@@Base+0x504>
   3c12c:	ldur	x1, [x29, #-72]
   3c130:	mov	x0, x19
   3c134:	mov	x2, x21
   3c138:	mov	x3, x25
   3c13c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c140:	cbz	x22, 3c164 <__gmpn_toom42_mul@@Base+0x4e8>
   3c144:	mov	w8, #0x18                  	// #24
   3c148:	madd	x8, x23, x8, x24
   3c14c:	lsl	x9, x23, #4
   3c150:	add	x8, x8, x26
   3c154:	add	x0, x8, #0x20
   3c158:	sub	x2, x9, x24
   3c15c:	mov	w1, wzr
   3c160:	bl	c5f0 <memset@plt>
   3c164:	ldur	w8, [x29, #-76]
   3c168:	ldp	x20, x22, [x29, #-56]
   3c16c:	ldur	x24, [x29, #-72]
   3c170:	eor	w8, w8, #0x1
   3c174:	stur	w8, [x29, #-76]
   3c178:	cbnz	x25, 3c0d0 <__gmpn_toom42_mul@@Base+0x454>
   3c17c:	mov	x8, xzr
   3c180:	ldur	x9, [x29, #-40]
   3c184:	cmp	x9, x28
   3c188:	b.eq	3c274 <__gmpn_toom42_mul@@Base+0x5f8>  // b.none
   3c18c:	cmp	x8, x23
   3c190:	b.gt	3c274 <__gmpn_toom42_mul@@Base+0x5f8>
   3c194:	sub	x9, x23, x8
   3c198:	add	x9, x9, #0x1
   3c19c:	cmp	x9, #0x4
   3c1a0:	b.cc	3c238 <__gmpn_toom42_mul@@Base+0x5bc>  // b.lo, b.ul, b.last
   3c1a4:	mov	w10, #0x28                  	// #40
   3c1a8:	lsl	x11, x8, #3
   3c1ac:	madd	x10, x23, x10, x11
   3c1b0:	add	x12, x26, x23, lsl #5
   3c1b4:	add	x10, x10, x26
   3c1b8:	add	x10, x10, #0x20
   3c1bc:	add	x12, x12, #0x20
   3c1c0:	cmp	x10, x12
   3c1c4:	b.cs	3c1ec <__gmpn_toom42_mul@@Base+0x570>  // b.hs, b.nlast
   3c1c8:	mov	w12, #0x18                  	// #24
   3c1cc:	mov	w10, #0x30                  	// #48
   3c1d0:	madd	x12, x23, x12, x11
   3c1d4:	madd	x10, x23, x10, x26
   3c1d8:	add	x12, x12, x26
   3c1dc:	add	x10, x10, #0x28
   3c1e0:	add	x12, x12, #0x18
   3c1e4:	cmp	x12, x10
   3c1e8:	b.cc	3c238 <__gmpn_toom42_mul@@Base+0x5bc>  // b.lo, b.ul, b.last
   3c1ec:	mov	w12, #0x18                  	// #24
   3c1f0:	mov	w13, #0x28                  	// #40
   3c1f4:	madd	x12, x23, x12, x11
   3c1f8:	madd	x11, x23, x13, x11
   3c1fc:	and	x10, x9, #0xfffffffffffffffc
   3c200:	add	x12, x12, x26
   3c204:	add	x13, x11, x26
   3c208:	add	x8, x8, x10
   3c20c:	add	x11, x12, #0x28
   3c210:	add	x12, x13, #0x30
   3c214:	mov	x13, x10
   3c218:	ldp	q0, q1, [x11, #-16]
   3c21c:	subs	x13, x13, #0x4
   3c220:	add	x11, x11, #0x20
   3c224:	stp	q0, q1, [x12, #-16]
   3c228:	add	x12, x12, #0x20
   3c22c:	b.ne	3c218 <__gmpn_toom42_mul@@Base+0x59c>  // b.any
   3c230:	cmp	x9, x10
   3c234:	b.eq	3c274 <__gmpn_toom42_mul@@Base+0x5f8>  // b.none
   3c238:	sub	x9, x23, x8
   3c23c:	mov	w10, #0x28                  	// #40
   3c240:	lsl	x11, x8, #3
   3c244:	mov	w12, #0x18                  	// #24
   3c248:	add	x8, x9, #0x1
   3c24c:	madd	x9, x23, x10, x11
   3c250:	madd	x10, x23, x12, x11
   3c254:	add	x9, x9, x26
   3c258:	add	x10, x10, x26
   3c25c:	add	x9, x9, #0x20
   3c260:	add	x10, x10, #0x18
   3c264:	ldr	x11, [x10], #8
   3c268:	subs	x8, x8, #0x1
   3c26c:	str	x11, [x9], #8
   3c270:	b.ne	3c264 <__gmpn_toom42_mul@@Base+0x5e8>  // b.any
   3c274:	ldur	x20, [x29, #-112]
   3c278:	mov	x0, x22
   3c27c:	mov	x2, x19
   3c280:	mov	x3, x23
   3c284:	mov	x1, x20
   3c288:	lsl	x24, x23, #1
   3c28c:	bl	c990 <__gmpn_mul_n@plt>
   3c290:	ldr	x8, [x20, x23, lsl #3]
   3c294:	cbz	x8, 3c2b0 <__gmpn_toom42_mul@@Base+0x634>
   3c298:	add	x0, x22, x23, lsl #3
   3c29c:	mov	x1, x0
   3c2a0:	mov	x2, x19
   3c2a4:	mov	x3, x23
   3c2a8:	bl	ca70 <__gmpn_add_n@plt>
   3c2ac:	b	3c2b4 <__gmpn_toom42_mul@@Base+0x638>
   3c2b0:	mov	x0, xzr
   3c2b4:	ldur	x2, [x29, #-40]
   3c2b8:	ldur	x3, [x29, #-88]
   3c2bc:	add	x19, x22, x24, lsl #3
   3c2c0:	str	x0, [x19], #8
   3c2c4:	mov	x0, x19
   3c2c8:	mov	x1, x27
   3c2cc:	bl	c990 <__gmpn_mul_n@plt>
   3c2d0:	ldp	x4, x8, [x29, #-32]
   3c2d4:	add	x22, x8, x23, lsl #5
   3c2d8:	cmp	x4, x25
   3c2dc:	mov	x0, x22
   3c2e0:	b.le	3c2f8 <__gmpn_toom42_mul@@Base+0x67c>
   3c2e4:	ldur	x1, [x29, #-96]
   3c2e8:	ldur	x3, [x29, #-72]
   3c2ec:	mov	x2, x4
   3c2f0:	mov	x4, x25
   3c2f4:	b	3c304 <__gmpn_toom42_mul@@Base+0x688>
   3c2f8:	ldur	x1, [x29, #-72]
   3c2fc:	ldur	x3, [x29, #-96]
   3c300:	mov	x2, x25
   3c304:	bl	ccd0 <__gmpn_mul@plt>
   3c308:	ldur	x8, [x29, #-24]
   3c30c:	ldr	x20, [x22]
   3c310:	mov	x1, x26
   3c314:	mov	x2, x28
   3c318:	add	x27, x8, x24, lsl #3
   3c31c:	mov	x0, x27
   3c320:	mov	x3, x23
   3c324:	bl	c990 <__gmpn_mul_n@plt>
   3c328:	ldr	x8, [x26, x23, lsl #3]
   3c32c:	cmp	x8, #0x3
   3c330:	b.eq	3c370 <__gmpn_toom42_mul@@Base+0x6f4>  // b.none
   3c334:	cmp	x8, #0x2
   3c338:	b.eq	3c3a0 <__gmpn_toom42_mul@@Base+0x724>  // b.none
   3c33c:	cmp	x8, #0x1
   3c340:	b.ne	3c3cc <__gmpn_toom42_mul@@Base+0x750>  // b.any
   3c344:	ldur	x8, [x29, #-64]
   3c348:	mov	x2, x28
   3c34c:	mov	x3, x23
   3c350:	ldr	x22, [x28, x8]
   3c354:	add	x0, x27, x8
   3c358:	mov	x1, x0
   3c35c:	bl	ca70 <__gmpn_add_n@plt>
   3c360:	add	x22, x0, x22
   3c364:	ldr	x8, [x28, x23, lsl #3]
   3c368:	cbnz	x8, 3c3d8 <__gmpn_toom42_mul@@Base+0x75c>
   3c36c:	b	3c3f0 <__gmpn_toom42_mul@@Base+0x774>
   3c370:	ldur	x9, [x29, #-64]
   3c374:	mov	w3, #0x3                   	// #3
   3c378:	mov	x1, x28
   3c37c:	mov	x2, x23
   3c380:	ldr	x8, [x28, x9]
   3c384:	add	x0, x27, x9
   3c388:	add	x22, x8, x8, lsl #1
   3c38c:	bl	d400 <__gmpn_addmul_1@plt>
   3c390:	add	x22, x22, x0
   3c394:	ldr	x8, [x28, x23, lsl #3]
   3c398:	cbnz	x8, 3c3d8 <__gmpn_toom42_mul@@Base+0x75c>
   3c39c:	b	3c3f0 <__gmpn_toom42_mul@@Base+0x774>
   3c3a0:	ldur	x8, [x29, #-64]
   3c3a4:	mov	x2, x28
   3c3a8:	mov	x3, x23
   3c3ac:	ldr	x22, [x28, x8]
   3c3b0:	add	x0, x27, x8
   3c3b4:	mov	x1, x0
   3c3b8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3c3bc:	add	x22, x0, x22, lsl #1
   3c3c0:	ldr	x8, [x28, x23, lsl #3]
   3c3c4:	cbnz	x8, 3c3d8 <__gmpn_toom42_mul@@Base+0x75c>
   3c3c8:	b	3c3f0 <__gmpn_toom42_mul@@Base+0x774>
   3c3cc:	mov	x22, xzr
   3c3d0:	ldr	x8, [x28, x23, lsl #3]
   3c3d4:	cbz	x8, 3c3f0 <__gmpn_toom42_mul@@Base+0x774>
   3c3d8:	add	x0, x27, x23, lsl #3
   3c3dc:	mov	x1, x0
   3c3e0:	mov	x2, x26
   3c3e4:	mov	x3, x23
   3c3e8:	bl	ca70 <__gmpn_add_n@plt>
   3c3ec:	add	x22, x0, x22
   3c3f0:	str	x22, [x27, x24, lsl #3]
   3c3f4:	ldp	x22, x1, [x29, #-24]
   3c3f8:	mov	x2, x21
   3c3fc:	mov	x3, x23
   3c400:	mov	x0, x22
   3c404:	bl	c990 <__gmpn_mul_n@plt>
   3c408:	ldur	x8, [x29, #-32]
   3c40c:	ldur	x2, [x29, #-48]
   3c410:	ldur	w5, [x29, #-76]
   3c414:	mov	x0, x22
   3c418:	add	x4, x8, x25
   3c41c:	mov	x1, x19
   3c420:	mov	x3, x23
   3c424:	mov	x6, x20
   3c428:	bl	ca20 <__gmpn_toom_interpolate_5pts@plt>
   3c42c:	ldur	x0, [x29, #-8]
   3c430:	cbnz	x0, 3c468 <__gmpn_toom42_mul@@Base+0x7ec>
   3c434:	mov	sp, x29
   3c438:	ldp	x20, x19, [sp, #80]
   3c43c:	ldp	x22, x21, [sp, #64]
   3c440:	ldp	x24, x23, [sp, #48]
   3c444:	ldp	x26, x25, [sp, #32]
   3c448:	ldp	x28, x27, [sp, #16]
   3c44c:	ldp	x29, x30, [sp], #96
   3c450:	ret
   3c454:	sub	x0, x29, #0x8
   3c458:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3c45c:	ldur	x4, [x29, #-32]
   3c460:	mov	x26, x0
   3c464:	b	3bd04 <__gmpn_toom42_mul@@Base+0x88>
   3c468:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3c46c:	b	3c434 <__gmpn_toom42_mul@@Base+0x7b8>

000000000003c470 <__gmpn_toom52_mul@@Base>:
   3c470:	sub	sp, sp, #0xf0
   3c474:	add	x8, x4, x4, lsl #2
   3c478:	stp	x28, x27, [sp, #160]
   3c47c:	stp	x22, x21, [sp, #208]
   3c480:	stp	x20, x19, [sp, #224]
   3c484:	mov	x28, x5
   3c488:	mov	x19, x4
   3c48c:	mov	x20, x3
   3c490:	mov	x22, x1
   3c494:	cmp	x8, x2, lsl #1
   3c498:	mov	x21, x0
   3c49c:	stp	x29, x30, [sp, #144]
   3c4a0:	stp	x26, x25, [sp, #176]
   3c4a4:	stp	x24, x23, [sp, #192]
   3c4a8:	add	x29, sp, #0x90
   3c4ac:	b.le	3c4bc <__gmpn_toom52_mul@@Base+0x4c>
   3c4b0:	sub	x8, x19, #0x1
   3c4b4:	asr	x26, x8, #1
   3c4b8:	b	3c4d0 <__gmpn_toom52_mul@@Base+0x60>
   3c4bc:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   3c4c0:	sub	x8, x2, #0x1
   3c4c4:	movk	x9, #0xcccd
   3c4c8:	umulh	x8, x8, x9
   3c4cc:	lsr	x26, x8, #2
   3c4d0:	add	x23, x26, #0x1
   3c4d4:	add	x8, x23, x23, lsl #1
   3c4d8:	lsl	x10, x23, #2
   3c4dc:	add	x9, x28, x23, lsl #5
   3c4e0:	lsl	x8, x8, #3
   3c4e4:	str	x10, [sp, #8]
   3c4e8:	stur	x9, [x29, #-64]
   3c4ec:	add	x1, x9, #0x20
   3c4f0:	add	x9, x21, x8
   3c4f4:	add	x8, x28, x8
   3c4f8:	add	x27, x9, #0x18
   3c4fc:	sub	x5, x2, x10
   3c500:	add	x6, x8, #0x18
   3c504:	mov	w2, #0x4                   	// #4
   3c508:	mov	x0, x27
   3c50c:	mov	x3, x22
   3c510:	mov	x4, x23
   3c514:	sub	x24, x19, x23
   3c518:	str	x1, [sp, #72]
   3c51c:	stur	x5, [x29, #-40]
   3c520:	str	x6, [sp, #64]
   3c524:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3c528:	mov	x3, x24
   3c52c:	mov	x24, x23
   3c530:	and	w25, w0, #0x2
   3c534:	subs	x8, x23, x3
   3c538:	add	x23, x20, x23, lsl #3
   3c53c:	stur	x3, [x29, #-8]
   3c540:	stur	x24, [x29, #-24]
   3c544:	stur	x27, [x29, #-56]
   3c548:	str	x22, [sp, #56]
   3c54c:	stur	x19, [x29, #-32]
   3c550:	b.ne	3c5c8 <__gmpn_toom52_mul@@Base+0x158>  // b.any
   3c554:	mov	x0, x21
   3c558:	mov	x1, x20
   3c55c:	mov	x2, x23
   3c560:	mov	x3, x24
   3c564:	bl	ca70 <__gmpn_add_n@plt>
   3c568:	mov	w8, #0x8                   	// #8
   3c56c:	bfi	x8, x26, #4, #60
   3c570:	mov	x9, x26
   3c574:	str	x0, [x21, x24, lsl #3]
   3c578:	add	x10, x9, #0x1
   3c57c:	cmp	x10, #0x1
   3c580:	b.lt	3c5a0 <__gmpn_toom52_mul@@Base+0x130>  // b.tstop
   3c584:	ldr	x10, [x20, x9, lsl #3]
   3c588:	ldr	x11, [x20, x8]
   3c58c:	sub	x9, x9, #0x1
   3c590:	sub	x8, x8, #0x8
   3c594:	cmp	x10, x11
   3c598:	b.eq	3c578 <__gmpn_toom52_mul@@Base+0x108>  // b.none
   3c59c:	b.ls	3c80c <__gmpn_toom52_mul@@Base+0x39c>  // b.plast
   3c5a0:	add	x8, x28, x24, lsl #4
   3c5a4:	add	x0, x8, #0x10
   3c5a8:	mov	x1, x20
   3c5ac:	mov	x2, x23
   3c5b0:	mov	x3, x24
   3c5b4:	lsl	x27, x24, #1
   3c5b8:	bl	c2d0 <__gmpn_sub_n@plt>
   3c5bc:	ldur	x3, [x29, #-8]
   3c5c0:	mov	w16, w25
   3c5c4:	b	3c884 <__gmpn_toom52_mul@@Base+0x414>
   3c5c8:	stur	x8, [x29, #-16]
   3c5cc:	cbz	x3, 3c618 <__gmpn_toom52_mul@@Base+0x1a8>
   3c5d0:	mov	x0, x21
   3c5d4:	mov	x1, x20
   3c5d8:	mov	x2, x23
   3c5dc:	bl	ca70 <__gmpn_add_n@plt>
   3c5e0:	ldur	x3, [x29, #-8]
   3c5e4:	mov	x8, x3
   3c5e8:	cbz	x0, 3c61c <__gmpn_toom52_mul@@Base+0x1ac>
   3c5ec:	mov	w9, #0x1                   	// #1
   3c5f0:	mov	x8, x3
   3c5f4:	cmp	x8, x26
   3c5f8:	b.gt	3c6c8 <__gmpn_toom52_mul@@Base+0x258>
   3c5fc:	lsl	x10, x8, #3
   3c600:	ldr	x11, [x20, x10]
   3c604:	add	x8, x8, #0x1
   3c608:	adds	x11, x11, #0x1
   3c60c:	str	x11, [x21, x10]
   3c610:	b.cs	3c5f4 <__gmpn_toom52_mul@@Base+0x184>  // b.hs, b.nlast
   3c614:	b	3c61c <__gmpn_toom52_mul@@Base+0x1ac>
   3c618:	mov	x8, xzr
   3c61c:	cmp	x21, x20
   3c620:	mov	x9, xzr
   3c624:	b.eq	3c6c8 <__gmpn_toom52_mul@@Base+0x258>  // b.none
   3c628:	cmp	x8, x26
   3c62c:	b.gt	3c6c8 <__gmpn_toom52_mul@@Base+0x258>
   3c630:	sub	x9, x26, x8
   3c634:	add	x9, x9, #0x1
   3c638:	cmp	x9, #0x4
   3c63c:	b.cc	3c6a0 <__gmpn_toom52_mul@@Base+0x230>  // b.lo, b.ul, b.last
   3c640:	lsl	x11, x8, #3
   3c644:	lsl	x10, x24, #3
   3c648:	add	x12, x21, x11
   3c64c:	add	x13, x20, x10
   3c650:	cmp	x12, x13
   3c654:	b.cs	3c668 <__gmpn_toom52_mul@@Base+0x1f8>  // b.hs, b.nlast
   3c658:	add	x10, x21, x10
   3c65c:	add	x12, x20, x11
   3c660:	cmp	x12, x10
   3c664:	b.cc	3c6a0 <__gmpn_toom52_mul@@Base+0x230>  // b.lo, b.ul, b.last
   3c668:	and	x10, x9, #0xfffffffffffffffc
   3c66c:	add	x12, x11, #0x10
   3c670:	add	x8, x8, x10
   3c674:	add	x11, x20, x12
   3c678:	add	x12, x21, x12
   3c67c:	mov	x13, x10
   3c680:	ldp	q0, q1, [x11, #-16]
   3c684:	add	x11, x11, #0x20
   3c688:	subs	x13, x13, #0x4
   3c68c:	stp	q0, q1, [x12, #-16]
   3c690:	add	x12, x12, #0x20
   3c694:	b.ne	3c680 <__gmpn_toom52_mul@@Base+0x210>  // b.any
   3c698:	cmp	x9, x10
   3c69c:	b.eq	3c6c4 <__gmpn_toom52_mul@@Base+0x254>  // b.none
   3c6a0:	sub	x9, x26, x8
   3c6a4:	lsl	x10, x8, #3
   3c6a8:	add	x8, x9, #0x1
   3c6ac:	add	x9, x21, x10
   3c6b0:	add	x10, x20, x10
   3c6b4:	ldr	x11, [x10], #8
   3c6b8:	subs	x8, x8, #0x1
   3c6bc:	str	x11, [x9], #8
   3c6c0:	b.ne	3c6b4 <__gmpn_toom52_mul@@Base+0x244>  // b.any
   3c6c4:	mov	x9, xzr
   3c6c8:	sub	x8, x19, x26, lsl #1
   3c6cc:	str	x9, [x21, x24, lsl #3]
   3c6d0:	sub	x8, x8, #0x2
   3c6d4:	lsl	x9, x26, #3
   3c6d8:	ldr	x10, [x20, x9]
   3c6dc:	cbnz	x10, 3c724 <__gmpn_toom52_mul@@Base+0x2b4>
   3c6e0:	adds	x8, x8, #0x1
   3c6e4:	sub	x9, x9, #0x8
   3c6e8:	b.cc	3c6d8 <__gmpn_toom52_mul@@Base+0x268>  // b.lo, b.ul, b.last
   3c6ec:	sub	x8, x19, x26
   3c6f0:	lsl	x9, x19, #3
   3c6f4:	sub	x8, x8, #0x2
   3c6f8:	sub	x9, x9, #0x8
   3c6fc:	add	x10, x8, #0x1
   3c700:	cmp	x10, #0x1
   3c704:	b.lt	3c724 <__gmpn_toom52_mul@@Base+0x2b4>  // b.tstop
   3c708:	ldr	x10, [x20, x8, lsl #3]
   3c70c:	ldr	x11, [x20, x9]
   3c710:	sub	x8, x8, #0x1
   3c714:	sub	x9, x9, #0x8
   3c718:	cmp	x10, x11
   3c71c:	b.eq	3c6fc <__gmpn_toom52_mul@@Base+0x28c>  // b.none
   3c720:	b.ls	3c830 <__gmpn_toom52_mul@@Base+0x3c0>  // b.plast
   3c724:	add	x8, x28, x24, lsl #4
   3c728:	lsl	x27, x24, #1
   3c72c:	add	x22, x8, #0x10
   3c730:	cbz	x3, 3c780 <__gmpn_toom52_mul@@Base+0x310>
   3c734:	mov	x0, x22
   3c738:	mov	x1, x20
   3c73c:	mov	x2, x23
   3c740:	bl	c2d0 <__gmpn_sub_n@plt>
   3c744:	ldur	x3, [x29, #-8]
   3c748:	mov	x8, x3
   3c74c:	cbz	x0, 3c784 <__gmpn_toom52_mul@@Base+0x314>
   3c750:	add	x8, x26, x19
   3c754:	add	x8, x28, x8, lsl #3
   3c758:	add	x9, x8, #0x18
   3c75c:	mov	x8, x3
   3c760:	cmp	x8, x26
   3c764:	b.gt	3c804 <__gmpn_toom52_mul@@Base+0x394>
   3c768:	ldr	x10, [x20, x8, lsl #3]
   3c76c:	add	x8, x8, #0x1
   3c770:	sub	x11, x10, #0x1
   3c774:	str	x11, [x9], #8
   3c778:	cbz	x10, 3c760 <__gmpn_toom52_mul@@Base+0x2f0>
   3c77c:	b	3c784 <__gmpn_toom52_mul@@Base+0x314>
   3c780:	mov	x8, xzr
   3c784:	cmp	x22, x20
   3c788:	b.eq	3c804 <__gmpn_toom52_mul@@Base+0x394>  // b.none
   3c78c:	cmp	x8, x26
   3c790:	b.gt	3c804 <__gmpn_toom52_mul@@Base+0x394>
   3c794:	sub	x9, x26, x8
   3c798:	add	x9, x9, #0x1
   3c79c:	cmp	x9, #0x4
   3c7a0:	b.cc	3c7d4 <__gmpn_toom52_mul@@Base+0x364>  // b.lo, b.ul, b.last
   3c7a4:	add	x10, x8, x26, lsl #1
   3c7a8:	add	x12, x28, x10, lsl #3
   3c7ac:	add	x10, x12, #0x20
   3c7b0:	add	x11, x20, x24, lsl #3
   3c7b4:	cmp	x10, x11
   3c7b8:	add	x11, x20, x8, lsl #3
   3c7bc:	b.cs	3cd54 <__gmpn_toom52_mul@@Base+0x8e4>  // b.hs, b.nlast
   3c7c0:	mov	w10, #0x18                  	// #24
   3c7c4:	madd	x10, x26, x10, x28
   3c7c8:	add	x10, x10, #0x28
   3c7cc:	cmp	x11, x10
   3c7d0:	b.cs	3cd54 <__gmpn_toom52_mul@@Base+0x8e4>  // b.hs, b.nlast
   3c7d4:	mov	w16, w25
   3c7d8:	add	x10, x8, x26, lsl #1
   3c7dc:	sub	x9, x26, x8
   3c7e0:	add	x10, x28, x10, lsl #3
   3c7e4:	add	x9, x9, #0x1
   3c7e8:	add	x10, x10, #0x20
   3c7ec:	add	x8, x20, x8, lsl #3
   3c7f0:	ldr	x11, [x8], #8
   3c7f4:	subs	x9, x9, #0x1
   3c7f8:	str	x11, [x10], #8
   3c7fc:	b.ne	3c7f0 <__gmpn_toom52_mul@@Base+0x380>  // b.any
   3c800:	b	3c884 <__gmpn_toom52_mul@@Base+0x414>
   3c804:	mov	w16, w25
   3c808:	b	3c884 <__gmpn_toom52_mul@@Base+0x414>
   3c80c:	add	x8, x28, x24, lsl #4
   3c810:	add	x0, x8, #0x10
   3c814:	mov	x1, x23
   3c818:	mov	x2, x20
   3c81c:	mov	x3, x24
   3c820:	lsl	x27, x24, #1
   3c824:	bl	c2d0 <__gmpn_sub_n@plt>
   3c828:	ldur	x3, [x29, #-8]
   3c82c:	b	3c87c <__gmpn_toom52_mul@@Base+0x40c>
   3c830:	add	x8, x28, x24, lsl #4
   3c834:	add	x22, x8, #0x10
   3c838:	mov	x0, x22
   3c83c:	mov	x1, x23
   3c840:	mov	x2, x20
   3c844:	lsl	x27, x24, #1
   3c848:	mov	x24, x3
   3c84c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c850:	ldur	x8, [x29, #-16]
   3c854:	cbz	x8, 3c874 <__gmpn_toom52_mul@@Base+0x404>
   3c858:	lsl	x8, x26, #1
   3c85c:	sub	x8, x8, x19
   3c860:	lsl	x8, x8, #3
   3c864:	add	x0, x22, x24, lsl #3
   3c868:	add	x2, x8, #0x10
   3c86c:	mov	w1, wzr
   3c870:	bl	c5f0 <memset@plt>
   3c874:	mov	x3, x24
   3c878:	ldur	x24, [x29, #-24]
   3c87c:	mov	w16, w25
   3c880:	orr	w16, w25, #0x1
   3c884:	add	x8, x21, x27, lsl #3
   3c888:	mov	x22, x23
   3c88c:	add	x15, x8, #0x10
   3c890:	add	x25, x26, #0x2
   3c894:	mov	w19, w16
   3c898:	str	x8, [sp, #40]
   3c89c:	cbz	x3, 3c904 <__gmpn_toom52_mul@@Base+0x494>
   3c8a0:	mov	x0, x15
   3c8a4:	mov	x1, x21
   3c8a8:	mov	x2, x22
   3c8ac:	mov	x23, x15
   3c8b0:	bl	ca70 <__gmpn_add_n@plt>
   3c8b4:	ldur	x3, [x29, #-8]
   3c8b8:	mov	w16, w19
   3c8bc:	mov	x15, x23
   3c8c0:	mov	x8, x3
   3c8c4:	cbz	x0, 3c908 <__gmpn_toom52_mul@@Base+0x498>
   3c8c8:	ldur	x8, [x29, #-32]
   3c8cc:	add	x8, x27, x8
   3c8d0:	lsl	x8, x8, #3
   3c8d4:	sub	x8, x8, x26, lsl #3
   3c8d8:	add	x9, x8, #0x8
   3c8dc:	mov	x8, x3
   3c8e0:	cmp	x8, x25
   3c8e4:	b.ge	3c9ac <__gmpn_toom52_mul@@Base+0x53c>  // b.tcont
   3c8e8:	ldr	x10, [x21, x8, lsl #3]
   3c8ec:	add	x8, x8, #0x1
   3c8f0:	adds	x10, x10, #0x1
   3c8f4:	str	x10, [x21, x9]
   3c8f8:	add	x9, x9, #0x8
   3c8fc:	b.cs	3c8e0 <__gmpn_toom52_mul@@Base+0x470>  // b.hs, b.nlast
   3c900:	b	3c908 <__gmpn_toom52_mul@@Base+0x498>
   3c904:	mov	x8, xzr
   3c908:	cmp	x15, x21
   3c90c:	b.eq	3c9ac <__gmpn_toom52_mul@@Base+0x53c>  // b.none
   3c910:	cmp	x8, x25
   3c914:	b.ge	3c9ac <__gmpn_toom52_mul@@Base+0x53c>  // b.tcont
   3c918:	sub	x9, x26, x8
   3c91c:	add	x10, x9, #0x2
   3c920:	cmp	x10, #0x4
   3c924:	lsl	x9, x27, #3
   3c928:	b.cc	3c988 <__gmpn_toom52_mul@@Base+0x518>  // b.lo, b.ul, b.last
   3c92c:	add	x11, x27, x8
   3c930:	add	x11, x21, x11, lsl #3
   3c934:	add	x11, x11, #0x10
   3c938:	add	x12, x21, x25, lsl #3
   3c93c:	cmp	x11, x12
   3c940:	add	x11, x21, x8, lsl #3
   3c944:	b.cs	3c95c <__gmpn_toom52_mul@@Base+0x4ec>  // b.hs, b.nlast
   3c948:	add	x12, x27, x26
   3c94c:	add	x12, x21, x12, lsl #3
   3c950:	add	x12, x12, #0x20
   3c954:	cmp	x11, x12
   3c958:	b.cc	3c988 <__gmpn_toom52_mul@@Base+0x518>  // b.lo, b.ul, b.last
   3c95c:	and	x12, x10, #0xfffffffffffffffc
   3c960:	add	x8, x8, x12
   3c964:	mov	x13, x12
   3c968:	ldp	q0, q1, [x11]
   3c96c:	add	x14, x11, x9
   3c970:	subs	x13, x13, #0x4
   3c974:	add	x11, x11, #0x20
   3c978:	stp	q0, q1, [x14, #16]
   3c97c:	b.ne	3c968 <__gmpn_toom52_mul@@Base+0x4f8>  // b.any
   3c980:	cmp	x10, x12
   3c984:	b.eq	3c9ac <__gmpn_toom52_mul@@Base+0x53c>  // b.none
   3c988:	sub	x10, x26, x8
   3c98c:	add	x9, x9, #0x10
   3c990:	add	x10, x10, #0x2
   3c994:	add	x8, x21, x8, lsl #3
   3c998:	ldr	x11, [x8]
   3c99c:	subs	x10, x10, #0x1
   3c9a0:	str	x11, [x8, x9]
   3c9a4:	add	x8, x8, #0x8
   3c9a8:	b.ne	3c998 <__gmpn_toom52_mul@@Base+0x528>  // b.any
   3c9ac:	add	x8, x21, x24, lsl #3
   3c9b0:	add	x17, x8, #0x8
   3c9b4:	str	x25, [sp, #32]
   3c9b8:	stur	x28, [x29, #-16]
   3c9bc:	stur	x22, [x29, #-48]
   3c9c0:	str	x20, [sp, #48]
   3c9c4:	stp	x17, x15, [sp, #16]
   3c9c8:	tbnz	w16, #0, 3ca38 <__gmpn_toom52_mul@@Base+0x5c8>
   3c9cc:	add	x8, x28, x27, lsl #3
   3c9d0:	subs	x13, x24, x3
   3c9d4:	add	x25, x8, #0x10
   3c9d8:	mov	x23, x22
   3c9dc:	str	xzr, [x17, x24, lsl #3]
   3c9e0:	b.ne	3cab0 <__gmpn_toom52_mul@@Base+0x640>  // b.any
   3c9e4:	add	x8, x20, x26, lsl #4
   3c9e8:	add	x8, x8, #0x8
   3c9ec:	add	x9, x26, #0x1
   3c9f0:	cmp	x9, #0x1
   3c9f4:	b.lt	3ca10 <__gmpn_toom52_mul@@Base+0x5a0>  // b.tstop
   3c9f8:	ldr	x9, [x25, x26, lsl #3]
   3c9fc:	ldr	x10, [x8], #-8
   3ca00:	sub	x26, x26, #0x1
   3ca04:	cmp	x9, x10
   3ca08:	b.eq	3c9ec <__gmpn_toom52_mul@@Base+0x57c>  // b.none
   3ca0c:	b.ls	3cd28 <__gmpn_toom52_mul@@Base+0x8b8>  // b.plast
   3ca10:	mov	x0, x17
   3ca14:	mov	x1, x25
   3ca18:	mov	x2, x23
   3ca1c:	mov	x3, x24
   3ca20:	mov	x28, x27
   3ca24:	mov	x27, x21
   3ca28:	mov	w20, w16
   3ca2c:	bl	c2d0 <__gmpn_sub_n@plt>
   3ca30:	ldr	x14, [sp, #8]
   3ca34:	b	3cde0 <__gmpn_toom52_mul@@Base+0x970>
   3ca38:	add	x8, x28, x27, lsl #3
   3ca3c:	add	x25, x8, #0x10
   3ca40:	cbz	x3, 3cb80 <__gmpn_toom52_mul@@Base+0x710>
   3ca44:	mov	x0, x17
   3ca48:	mov	x1, x25
   3ca4c:	mov	x2, x22
   3ca50:	mov	x23, x17
   3ca54:	bl	ca70 <__gmpn_add_n@plt>
   3ca58:	ldur	x12, [x29, #-8]
   3ca5c:	mov	x17, x23
   3ca60:	mov	w16, w19
   3ca64:	mov	x8, x12
   3ca68:	cbz	x0, 3cb84 <__gmpn_toom52_mul@@Base+0x714>
   3ca6c:	ldur	x9, [x29, #-32]
   3ca70:	add	x8, x21, x9, lsl #3
   3ca74:	add	x9, x27, x9
   3ca78:	add	x10, x8, #0x8
   3ca7c:	sub	x8, x9, x26
   3ca80:	add	x8, x28, x8, lsl #3
   3ca84:	add	x11, x8, #0x8
   3ca88:	mov	w9, #0x1                   	// #1
   3ca8c:	mov	x8, x12
   3ca90:	cmp	x8, x26
   3ca94:	b.gt	3cc54 <__gmpn_toom52_mul@@Base+0x7e4>
   3ca98:	ldr	x12, [x11], #8
   3ca9c:	add	x8, x8, #0x1
   3caa0:	adds	x12, x12, #0x1
   3caa4:	str	x12, [x10], #8
   3caa8:	b.cs	3ca90 <__gmpn_toom52_mul@@Base+0x620>  // b.hs, b.nlast
   3caac:	b	3cb84 <__gmpn_toom52_mul@@Base+0x714>
   3cab0:	ldur	x8, [x29, #-32]
   3cab4:	add	x9, x27, x26
   3cab8:	add	x9, x28, x9, lsl #3
   3cabc:	add	x9, x9, #0x10
   3cac0:	sub	x8, x8, x26, lsl #1
   3cac4:	sub	x8, x8, #0x2
   3cac8:	ldr	x10, [x9]
   3cacc:	cbnz	x10, 3cb14 <__gmpn_toom52_mul@@Base+0x6a4>
   3cad0:	adds	x8, x8, #0x1
   3cad4:	sub	x9, x9, #0x8
   3cad8:	b.cc	3cac8 <__gmpn_toom52_mul@@Base+0x658>  // b.lo, b.ul, b.last
   3cadc:	ldur	x9, [x29, #-32]
   3cae0:	mov	x10, x3
   3cae4:	add	x8, x20, x9, lsl #3
   3cae8:	add	x9, x27, x9
   3caec:	sub	x9, x9, x26
   3caf0:	sub	x8, x8, #0x8
   3caf4:	add	x9, x28, x9, lsl #3
   3caf8:	subs	x10, x10, #0x1
   3cafc:	b.lt	3cb14 <__gmpn_toom52_mul@@Base+0x6a4>  // b.tstop
   3cb00:	ldr	x11, [x9], #-8
   3cb04:	ldr	x12, [x8], #-8
   3cb08:	cmp	x11, x12
   3cb0c:	b.eq	3caf8 <__gmpn_toom52_mul@@Base+0x688>  // b.none
   3cb10:	b.ls	3cd90 <__gmpn_toom52_mul@@Base+0x920>  // b.plast
   3cb14:	cbz	x3, 3cc6c <__gmpn_toom52_mul@@Base+0x7fc>
   3cb18:	mov	x0, x17
   3cb1c:	mov	x1, x25
   3cb20:	mov	x2, x23
   3cb24:	mov	x22, x17
   3cb28:	bl	c2d0 <__gmpn_sub_n@plt>
   3cb2c:	ldur	x11, [x29, #-8]
   3cb30:	mov	x17, x22
   3cb34:	mov	w16, w19
   3cb38:	mov	x8, x11
   3cb3c:	cbz	x0, 3cc70 <__gmpn_toom52_mul@@Base+0x800>
   3cb40:	ldur	x9, [x29, #-32]
   3cb44:	add	x8, x21, x9, lsl #3
   3cb48:	add	x10, x27, x9
   3cb4c:	add	x9, x8, #0x8
   3cb50:	sub	x8, x10, x26
   3cb54:	add	x8, x28, x8, lsl #3
   3cb58:	add	x10, x8, #0x8
   3cb5c:	mov	x8, x11
   3cb60:	cmp	x8, x26
   3cb64:	b.gt	3cd14 <__gmpn_toom52_mul@@Base+0x8a4>
   3cb68:	ldr	x11, [x10], #8
   3cb6c:	add	x8, x8, #0x1
   3cb70:	sub	x12, x11, #0x1
   3cb74:	str	x12, [x9], #8
   3cb78:	cbz	x11, 3cb60 <__gmpn_toom52_mul@@Base+0x6f0>
   3cb7c:	b	3cc70 <__gmpn_toom52_mul@@Base+0x800>
   3cb80:	mov	x8, xzr
   3cb84:	cmp	x17, x25
   3cb88:	mov	x9, xzr
   3cb8c:	b.eq	3cc54 <__gmpn_toom52_mul@@Base+0x7e4>  // b.none
   3cb90:	ldr	x14, [sp, #8]
   3cb94:	cmp	x8, x26
   3cb98:	b.gt	3cc58 <__gmpn_toom52_mul@@Base+0x7e8>
   3cb9c:	sub	x9, x26, x8
   3cba0:	add	x9, x9, #0x1
   3cba4:	cmp	x9, #0x4
   3cba8:	b.cc	3cc1c <__gmpn_toom52_mul@@Base+0x7ac>  // b.lo, b.ul, b.last
   3cbac:	add	x10, x8, x26
   3cbb0:	add	x13, x27, x26
   3cbb4:	add	x12, x21, x10, lsl #3
   3cbb8:	add	x10, x28, x13, lsl #3
   3cbbc:	add	x11, x27, x8
   3cbc0:	add	x13, x12, #0x10
   3cbc4:	add	x10, x10, #0x18
   3cbc8:	cmp	x13, x10
   3cbcc:	add	x11, x28, x11, lsl #3
   3cbd0:	b.cs	3cbe8 <__gmpn_toom52_mul@@Base+0x778>  // b.hs, b.nlast
   3cbd4:	add	x10, x21, x26, lsl #4
   3cbd8:	add	x10, x10, #0x18
   3cbdc:	add	x13, x11, #0x10
   3cbe0:	cmp	x13, x10
   3cbe4:	b.cc	3cc1c <__gmpn_toom52_mul@@Base+0x7ac>  // b.lo, b.ul, b.last
   3cbe8:	and	x10, x9, #0xfffffffffffffffc
   3cbec:	add	x11, x11, #0x20
   3cbf0:	add	x8, x8, x10
   3cbf4:	add	x12, x12, #0x20
   3cbf8:	mov	x13, x10
   3cbfc:	ldp	q0, q1, [x11, #-16]
   3cc00:	subs	x13, x13, #0x4
   3cc04:	add	x11, x11, #0x20
   3cc08:	stp	q0, q1, [x12, #-16]
   3cc0c:	add	x12, x12, #0x20
   3cc10:	b.ne	3cbfc <__gmpn_toom52_mul@@Base+0x78c>  // b.any
   3cc14:	cmp	x9, x10
   3cc18:	b.eq	3cc4c <__gmpn_toom52_mul@@Base+0x7dc>  // b.none
   3cc1c:	sub	x9, x26, x8
   3cc20:	add	x10, x8, x26
   3cc24:	add	x11, x27, x8
   3cc28:	add	x8, x9, #0x1
   3cc2c:	add	x9, x21, x10, lsl #3
   3cc30:	add	x10, x28, x11, lsl #3
   3cc34:	add	x9, x9, #0x10
   3cc38:	add	x10, x10, #0x10
   3cc3c:	ldr	x11, [x10], #8
   3cc40:	subs	x8, x8, #0x1
   3cc44:	str	x11, [x9], #8
   3cc48:	b.ne	3cc3c <__gmpn_toom52_mul@@Base+0x7cc>  // b.any
   3cc4c:	mov	x9, xzr
   3cc50:	b	3cc58 <__gmpn_toom52_mul@@Base+0x7e8>
   3cc54:	ldr	x14, [sp, #8]
   3cc58:	mov	x28, x27
   3cc5c:	mov	x27, x21
   3cc60:	str	x9, [x17, x24, lsl #3]
   3cc64:	eor	w20, w16, #0x2
   3cc68:	b	3cde0 <__gmpn_toom52_mul@@Base+0x970>
   3cc6c:	mov	x8, xzr
   3cc70:	cmp	x17, x25
   3cc74:	b.eq	3cd14 <__gmpn_toom52_mul@@Base+0x8a4>  // b.none
   3cc78:	cmp	x8, x26
   3cc7c:	b.gt	3cd14 <__gmpn_toom52_mul@@Base+0x8a4>
   3cc80:	sub	x9, x26, x8
   3cc84:	add	x9, x9, #0x1
   3cc88:	cmp	x9, #0x4
   3cc8c:	b.cc	3cccc <__gmpn_toom52_mul@@Base+0x85c>  // b.lo, b.ul, b.last
   3cc90:	add	x10, x8, x26
   3cc94:	add	x13, x27, x26
   3cc98:	add	x12, x21, x10, lsl #3
   3cc9c:	add	x10, x28, x13, lsl #3
   3cca0:	add	x11, x27, x8
   3cca4:	add	x13, x12, #0x10
   3cca8:	add	x10, x10, #0x18
   3ccac:	cmp	x13, x10
   3ccb0:	add	x11, x28, x11, lsl #3
   3ccb4:	b.cs	3cf28 <__gmpn_toom52_mul@@Base+0xab8>  // b.hs, b.nlast
   3ccb8:	add	x10, x21, x26, lsl #4
   3ccbc:	add	x10, x10, #0x18
   3ccc0:	add	x13, x11, #0x10
   3ccc4:	cmp	x13, x10
   3ccc8:	b.cs	3cf28 <__gmpn_toom52_mul@@Base+0xab8>  // b.hs, b.nlast
   3cccc:	ldr	x14, [sp, #8]
   3ccd0:	sub	x9, x26, x8
   3ccd4:	add	x10, x8, x26
   3ccd8:	mov	x12, x28
   3ccdc:	add	x11, x27, x8
   3cce0:	add	x8, x9, #0x1
   3cce4:	add	x9, x21, x10, lsl #3
   3cce8:	add	x10, x12, x11, lsl #3
   3ccec:	mov	w20, w16
   3ccf0:	mov	x28, x27
   3ccf4:	mov	x27, x21
   3ccf8:	add	x9, x9, #0x10
   3ccfc:	add	x10, x10, #0x10
   3cd00:	ldr	x11, [x10], #8
   3cd04:	subs	x8, x8, #0x1
   3cd08:	str	x11, [x9], #8
   3cd0c:	b.ne	3cd00 <__gmpn_toom52_mul@@Base+0x890>  // b.any
   3cd10:	b	3cde0 <__gmpn_toom52_mul@@Base+0x970>
   3cd14:	ldr	x14, [sp, #8]
   3cd18:	mov	x28, x27
   3cd1c:	mov	x27, x21
   3cd20:	mov	w20, w16
   3cd24:	b	3cde0 <__gmpn_toom52_mul@@Base+0x970>
   3cd28:	mov	x0, x17
   3cd2c:	mov	x1, x23
   3cd30:	mov	x2, x25
   3cd34:	mov	x3, x24
   3cd38:	mov	x28, x27
   3cd3c:	mov	x27, x21
   3cd40:	mov	w20, w16
   3cd44:	bl	c2d0 <__gmpn_sub_n@plt>
   3cd48:	ldr	x14, [sp, #8]
   3cd4c:	eor	w20, w20, #0x2
   3cd50:	b	3cde0 <__gmpn_toom52_mul@@Base+0x970>
   3cd54:	and	x10, x9, #0xfffffffffffffffc
   3cd58:	add	x11, x11, #0x10
   3cd5c:	add	x8, x8, x10
   3cd60:	add	x12, x12, #0x30
   3cd64:	mov	x13, x10
   3cd68:	mov	w16, w25
   3cd6c:	ldp	q0, q1, [x11, #-16]
   3cd70:	subs	x13, x13, #0x4
   3cd74:	add	x11, x11, #0x20
   3cd78:	stp	q0, q1, [x12, #-16]
   3cd7c:	add	x12, x12, #0x20
   3cd80:	b.ne	3cd6c <__gmpn_toom52_mul@@Base+0x8fc>  // b.any
   3cd84:	cmp	x9, x10
   3cd88:	b.eq	3c884 <__gmpn_toom52_mul@@Base+0x414>  // b.none
   3cd8c:	b	3c7d8 <__gmpn_toom52_mul@@Base+0x368>
   3cd90:	mov	x0, x17
   3cd94:	mov	x1, x23
   3cd98:	mov	x2, x25
   3cd9c:	mov	x28, x27
   3cda0:	mov	x27, x21
   3cda4:	mov	x21, x17
   3cda8:	mov	x20, x3
   3cdac:	mov	x22, x13
   3cdb0:	bl	c2d0 <__gmpn_sub_n@plt>
   3cdb4:	cbz	x22, 3cdd8 <__gmpn_toom52_mul@@Base+0x968>
   3cdb8:	ldur	x9, [x29, #-32]
   3cdbc:	lsl	x8, x26, #1
   3cdc0:	add	x0, x21, x20, lsl #3
   3cdc4:	mov	w1, wzr
   3cdc8:	sub	x8, x8, x9
   3cdcc:	lsl	x8, x8, #3
   3cdd0:	add	x2, x8, #0x10
   3cdd4:	bl	c5f0 <memset@plt>
   3cdd8:	ldr	x14, [sp, #8]
   3cddc:	eor	w20, w19, #0x2
   3cde0:	lsl	x19, x14, #3
   3cde4:	mov	x21, x27
   3cde8:	add	x8, x27, x19
   3cdec:	ldp	x22, x25, [sp, #56]
   3cdf0:	mov	x4, x24
   3cdf4:	mov	x23, x24
   3cdf8:	ldur	x24, [x29, #-40]
   3cdfc:	ldur	x27, [x29, #-16]
   3ce00:	add	x26, x8, #0x20
   3ce04:	mov	w2, #0x4                   	// #4
   3ce08:	mov	x0, x26
   3ce0c:	mov	x1, x25
   3ce10:	mov	x3, x22
   3ce14:	mov	x5, x24
   3ce18:	mov	x6, x27
   3ce1c:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3ce20:	and	w8, w0, #0x1
   3ce24:	eor	w8, w8, w20
   3ce28:	stur	w8, [x29, #-32]
   3ce2c:	ldr	x20, [sp, #32]
   3ce30:	add	x28, x27, x28, lsl #3
   3ce34:	add	x3, x28, #0x10
   3ce38:	mov	x0, x27
   3ce3c:	mov	x1, x25
   3ce40:	mov	x2, x20
   3ce44:	mov	x4, x23
   3ce48:	bl	ccd0 <__gmpn_mul@plt>
   3ce4c:	ldr	x1, [sp, #72]
   3ce50:	ldr	x2, [sp, #16]
   3ce54:	add	x28, x28, #0x8
   3ce58:	mov	x0, x28
   3ce5c:	mov	x3, x20
   3ce60:	bl	c990 <__gmpn_mul_n@plt>
   3ce64:	ldp	x8, x1, [x29, #-64]
   3ce68:	ldr	x2, [sp, #24]
   3ce6c:	mov	x3, x20
   3ce70:	add	x27, x8, #0x10
   3ce74:	mov	x0, x27
   3ce78:	bl	c990 <__gmpn_mul_n@plt>
   3ce7c:	ldr	x0, [sp, #40]
   3ce80:	mov	x1, x26
   3ce84:	mov	x2, x21
   3ce88:	mov	x3, x20
   3ce8c:	bl	c990 <__gmpn_mul_n@plt>
   3ce90:	ldur	x9, [x29, #-8]
   3ce94:	mov	w8, #0x28                  	// #40
   3ce98:	madd	x0, x23, x8, x21
   3ce9c:	add	x3, x22, x19
   3cea0:	cmp	x24, x9
   3cea4:	b.le	3cec0 <__gmpn_toom52_mul@@Base+0xa50>
   3cea8:	mov	x1, x3
   3ceac:	ldur	x3, [x29, #-48]
   3ceb0:	mov	x2, x24
   3ceb4:	mov	x19, x9
   3ceb8:	mov	x4, x9
   3cebc:	b	3ced0 <__gmpn_toom52_mul@@Base+0xa60>
   3cec0:	ldur	x1, [x29, #-48]
   3cec4:	mov	x2, x9
   3cec8:	mov	x19, x9
   3cecc:	mov	x4, x24
   3ced0:	bl	ccd0 <__gmpn_mul@plt>
   3ced4:	ldur	x20, [x29, #-24]
   3ced8:	ldr	x2, [sp, #48]
   3cedc:	mov	x0, x21
   3cee0:	mov	x1, x22
   3cee4:	mov	x3, x20
   3cee8:	bl	c990 <__gmpn_mul_n@plt>
   3ceec:	add	x6, x24, x19
   3cef0:	mov	x0, x21
   3cef4:	mov	x1, x20
   3cef8:	ldur	w2, [x29, #-32]
   3cefc:	ldur	x3, [x29, #-16]
   3cf00:	mov	x4, x28
   3cf04:	mov	x5, x27
   3cf08:	ldp	x20, x19, [sp, #224]
   3cf0c:	ldp	x22, x21, [sp, #208]
   3cf10:	ldp	x24, x23, [sp, #192]
   3cf14:	ldp	x26, x25, [sp, #176]
   3cf18:	ldp	x28, x27, [sp, #160]
   3cf1c:	ldp	x29, x30, [sp, #144]
   3cf20:	add	sp, sp, #0xf0
   3cf24:	b	c910 <__gmpn_toom_interpolate_6pts@plt>
   3cf28:	and	x10, x9, #0xfffffffffffffffc
   3cf2c:	add	x11, x11, #0x20
   3cf30:	add	x8, x8, x10
   3cf34:	add	x12, x12, #0x20
   3cf38:	mov	x13, x10
   3cf3c:	ldp	q0, q1, [x11, #-16]
   3cf40:	subs	x13, x13, #0x4
   3cf44:	add	x11, x11, #0x20
   3cf48:	stp	q0, q1, [x12, #-16]
   3cf4c:	add	x12, x12, #0x20
   3cf50:	b.ne	3cf3c <__gmpn_toom52_mul@@Base+0xacc>  // b.any
   3cf54:	ldr	x14, [sp, #8]
   3cf58:	cmp	x9, x10
   3cf5c:	b.ne	3ccd0 <__gmpn_toom52_mul@@Base+0x860>  // b.any
   3cf60:	b	3cd18 <__gmpn_toom52_mul@@Base+0x8a8>

000000000003cf64 <__gmpn_toom62_mul@@Base>:
   3cf64:	stp	x29, x30, [sp, #-96]!
   3cf68:	stp	x28, x27, [sp, #16]
   3cf6c:	stp	x26, x25, [sp, #32]
   3cf70:	stp	x24, x23, [sp, #48]
   3cf74:	stp	x22, x21, [sp, #64]
   3cf78:	stp	x20, x19, [sp, #80]
   3cf7c:	mov	x29, sp
   3cf80:	sub	sp, sp, #0xd0
   3cf84:	add	x8, x4, x4, lsl #1
   3cf88:	mov	x25, x3
   3cf8c:	mov	x23, x1
   3cf90:	cmp	x8, x2
   3cf94:	mov	x20, x0
   3cf98:	stur	x5, [x29, #-40]
   3cf9c:	b.le	3cfac <__gmpn_toom62_mul@@Base+0x48>
   3cfa0:	sub	x8, x4, #0x1
   3cfa4:	asr	x24, x8, #1
   3cfa8:	b	3cfc0 <__gmpn_toom62_mul@@Base+0x5c>
   3cfac:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3cfb0:	sub	x8, x2, #0x1
   3cfb4:	movk	x9, #0xaaab
   3cfb8:	umulh	x8, x8, x9
   3cfbc:	lsr	x24, x8, #2
   3cfc0:	add	x21, x24, #0x1
   3cfc4:	add	x9, x24, #0x2
   3cfc8:	lsl	x10, x21, #2
   3cfcc:	sub	x8, x4, x21
   3cfd0:	stp	x9, x8, [x29, #-56]
   3cfd4:	add	x8, x10, x21
   3cfd8:	stur	x8, [x29, #-64]
   3cfdc:	sub	x19, x2, x8
   3cfe0:	lsl	x8, x9, #3
   3cfe4:	add	x8, x8, #0xf
   3cfe8:	mov	x9, sp
   3cfec:	and	x8, x8, #0xfffffffffffffff0
   3cff0:	sub	x0, x9, x8
   3cff4:	stur	x4, [x29, #-104]
   3cff8:	stur	x10, [x29, #-200]
   3cffc:	stur	x2, [x29, #-24]
   3d000:	mov	sp, x0
   3d004:	mov	x9, sp
   3d008:	sub	x1, x9, x8
   3d00c:	mov	sp, x1
   3d010:	mov	x9, sp
   3d014:	sub	x22, x9, x8
   3d018:	mov	sp, x22
   3d01c:	mov	x9, sp
   3d020:	sub	x27, x9, x8
   3d024:	mov	sp, x27
   3d028:	mov	x9, sp
   3d02c:	sub	x28, x9, x8
   3d030:	mov	sp, x28
   3d034:	mov	x9, sp
   3d038:	sub	x9, x9, x8
   3d03c:	stur	x9, [x29, #-152]
   3d040:	mov	sp, x9
   3d044:	lsl	x26, x21, #3
   3d048:	add	x10, x26, #0xf
   3d04c:	mov	x9, sp
   3d050:	and	x10, x10, #0xfffffffffffffff0
   3d054:	sub	x9, x9, x10
   3d058:	stp	x9, x1, [x29, #-136]
   3d05c:	mov	sp, x9
   3d060:	mov	x9, sp
   3d064:	sub	x9, x9, x8
   3d068:	stur	x9, [x29, #-32]
   3d06c:	mov	sp, x9
   3d070:	mov	x9, sp
   3d074:	sub	x9, x9, x8
   3d078:	stur	x9, [x29, #-184]
   3d07c:	mov	sp, x9
   3d080:	mov	x9, sp
   3d084:	sub	x8, x9, x8
   3d088:	stur	x8, [x29, #-176]
   3d08c:	mov	sp, x8
   3d090:	mov	w2, #0x5                   	// #5
   3d094:	mov	x3, x23
   3d098:	mov	x4, x21
   3d09c:	mov	x5, x19
   3d0a0:	mov	x6, x20
   3d0a4:	stur	x0, [x29, #-88]
   3d0a8:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3d0ac:	stur	w0, [x29, #-120]
   3d0b0:	mov	w2, #0x5                   	// #5
   3d0b4:	mov	x0, x22
   3d0b8:	mov	x1, x27
   3d0bc:	mov	x3, x23
   3d0c0:	mov	x4, x21
   3d0c4:	mov	x5, x19
   3d0c8:	mov	x6, x20
   3d0cc:	stur	x22, [x29, #-160]
   3d0d0:	stur	x27, [x29, #-144]
   3d0d4:	stur	x20, [x29, #-72]
   3d0d8:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3d0dc:	stur	w0, [x29, #-116]
   3d0e0:	add	x1, x23, x26
   3d0e4:	mov	x0, x28
   3d0e8:	mov	x2, x23
   3d0ec:	mov	x3, x21
   3d0f0:	stur	x26, [x29, #-80]
   3d0f4:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d0f8:	stur	x23, [x29, #-16]
   3d0fc:	mov	x8, x23
   3d100:	mov	x23, x0
   3d104:	mov	x0, x28
   3d108:	mov	x2, x28
   3d10c:	add	x1, x8, x21, lsl #4
   3d110:	mov	x3, x21
   3d114:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d118:	add	x20, x0, x23, lsl #1
   3d11c:	ldur	x23, [x29, #-16]
   3d120:	mov	w8, #0x18                  	// #24
   3d124:	mov	x0, x28
   3d128:	mov	x2, x28
   3d12c:	madd	x1, x21, x8, x23
   3d130:	mov	x3, x21
   3d134:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d138:	add	x20, x0, x20, lsl #1
   3d13c:	add	x1, x23, x21, lsl #5
   3d140:	mov	x0, x28
   3d144:	mov	x2, x28
   3d148:	mov	x3, x21
   3d14c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d150:	cmp	x24, x19
   3d154:	add	x26, x0, x20, lsl #1
   3d158:	stur	x19, [x29, #-112]
   3d15c:	b.ge	3d184 <__gmpn_toom62_mul@@Base+0x220>  // b.tcont
   3d160:	ldur	x8, [x29, #-64]
   3d164:	mov	x0, x28
   3d168:	mov	x2, x28
   3d16c:	mov	x3, x21
   3d170:	add	x1, x23, x8, lsl #3
   3d174:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d178:	add	x8, x0, x26, lsl #1
   3d17c:	str	x8, [x28, x21, lsl #3]
   3d180:	b	3d1f8 <__gmpn_toom62_mul@@Base+0x294>
   3d184:	ldur	x8, [x29, #-64]
   3d188:	mov	x0, x28
   3d18c:	mov	x2, x28
   3d190:	mov	x3, x19
   3d194:	add	x1, x23, x8, lsl #3
   3d198:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d19c:	add	x20, x28, x19, lsl #3
   3d1a0:	mov	x23, x0
   3d1a4:	sub	x2, x21, x19
   3d1a8:	mov	w3, #0x1                   	// #1
   3d1ac:	mov	x0, x20
   3d1b0:	mov	x1, x20
   3d1b4:	bl	c180 <__gmpn_lshift@plt>
   3d1b8:	add	x8, x0, x26, lsl #1
   3d1bc:	str	x8, [x28, x21, lsl #3]
   3d1c0:	ldr	x8, [x20]
   3d1c4:	adds	x8, x8, x23
   3d1c8:	str	x8, [x20]
   3d1cc:	b.cc	3d1f8 <__gmpn_toom62_mul@@Base+0x294>  // b.lo, b.ul, b.last
   3d1d0:	ldur	x8, [x29, #-24]
   3d1d4:	mov	w9, #0x28                  	// #40
   3d1d8:	lsl	x8, x8, #3
   3d1dc:	msub	x8, x24, x9, x8
   3d1e0:	add	x8, x8, x28
   3d1e4:	sub	x8, x8, #0x20
   3d1e8:	ldr	x9, [x8]
   3d1ec:	adds	x9, x9, #0x1
   3d1f0:	str	x9, [x8], #8
   3d1f4:	b.cs	3d1e8 <__gmpn_toom62_mul@@Base+0x284>  // b.hs, b.nlast
   3d1f8:	ldp	x19, x23, [x29, #-56]
   3d1fc:	stur	x28, [x29, #-168]
   3d200:	ldur	x28, [x29, #-136]
   3d204:	ldur	x27, [x29, #-152]
   3d208:	ldur	x26, [x29, #-184]
   3d20c:	subs	x20, x21, x23
   3d210:	add	x22, x25, x21, lsl #3
   3d214:	stur	x22, [x29, #-96]
   3d218:	b.ne	3d290 <__gmpn_toom62_mul@@Base+0x32c>  // b.any
   3d21c:	mov	x0, x27
   3d220:	mov	x1, x25
   3d224:	mov	x2, x22
   3d228:	mov	x3, x21
   3d22c:	bl	ca70 <__gmpn_add_n@plt>
   3d230:	mov	w8, #0x8                   	// #8
   3d234:	bfi	x8, x24, #4, #60
   3d238:	mov	x9, x24
   3d23c:	str	x0, [x27, x21, lsl #3]
   3d240:	add	x10, x9, #0x1
   3d244:	cmp	x10, #0x1
   3d248:	b.lt	3d268 <__gmpn_toom62_mul@@Base+0x304>  // b.tstop
   3d24c:	ldr	x10, [x25, x9, lsl #3]
   3d250:	ldr	x11, [x25, x8]
   3d254:	sub	x9, x9, #0x1
   3d258:	sub	x8, x8, #0x8
   3d25c:	cmp	x10, x11
   3d260:	b.eq	3d240 <__gmpn_toom62_mul@@Base+0x2dc>  // b.none
   3d264:	b.ls	3d40c <__gmpn_toom62_mul@@Base+0x4a8>  // b.plast
   3d268:	mov	x0, x28
   3d26c:	mov	x1, x25
   3d270:	mov	x2, x22
   3d274:	mov	x3, x21
   3d278:	bl	c2d0 <__gmpn_sub_n@plt>
   3d27c:	ldur	x12, [x29, #-32]
   3d280:	mov	w20, wzr
   3d284:	cbnz	x23, 3d42c <__gmpn_toom62_mul@@Base+0x4c8>
   3d288:	mov	x8, xzr
   3d28c:	b	3d470 <__gmpn_toom62_mul@@Base+0x50c>
   3d290:	cbz	x23, 3d2dc <__gmpn_toom62_mul@@Base+0x378>
   3d294:	mov	x0, x27
   3d298:	mov	x1, x25
   3d29c:	mov	x2, x22
   3d2a0:	mov	x3, x23
   3d2a4:	bl	ca70 <__gmpn_add_n@plt>
   3d2a8:	mov	x8, x23
   3d2ac:	cbz	x0, 3d2e0 <__gmpn_toom62_mul@@Base+0x37c>
   3d2b0:	mov	w9, #0x1                   	// #1
   3d2b4:	mov	x8, x23
   3d2b8:	cmp	x8, x24
   3d2bc:	b.gt	3d310 <__gmpn_toom62_mul@@Base+0x3ac>
   3d2c0:	lsl	x10, x8, #3
   3d2c4:	ldr	x11, [x25, x10]
   3d2c8:	add	x8, x8, #0x1
   3d2cc:	adds	x11, x11, #0x1
   3d2d0:	str	x11, [x27, x10]
   3d2d4:	b.cs	3d2b8 <__gmpn_toom62_mul@@Base+0x354>  // b.hs, b.nlast
   3d2d8:	b	3d2e0 <__gmpn_toom62_mul@@Base+0x37c>
   3d2dc:	mov	x8, xzr
   3d2e0:	cmp	x27, x25
   3d2e4:	mov	x9, xzr
   3d2e8:	b.eq	3d310 <__gmpn_toom62_mul@@Base+0x3ac>  // b.none
   3d2ec:	cmp	x8, x24
   3d2f0:	b.gt	3d310 <__gmpn_toom62_mul@@Base+0x3ac>
   3d2f4:	lsl	x9, x8, #3
   3d2f8:	sub	x8, x21, x8
   3d2fc:	add	x0, x27, x9
   3d300:	add	x1, x25, x9
   3d304:	lsl	x2, x8, #3
   3d308:	bl	bed0 <memcpy@plt>
   3d30c:	mov	x9, xzr
   3d310:	ldur	x8, [x29, #-104]
   3d314:	str	x9, [x27, x21, lsl #3]
   3d318:	lsl	x9, x24, #3
   3d31c:	sub	x8, x8, x24, lsl #1
   3d320:	sub	x8, x8, #0x2
   3d324:	ldr	x10, [x25, x9]
   3d328:	cbnz	x10, 3d374 <__gmpn_toom62_mul@@Base+0x410>
   3d32c:	adds	x8, x8, #0x1
   3d330:	sub	x9, x9, #0x8
   3d334:	b.cc	3d324 <__gmpn_toom62_mul@@Base+0x3c0>  // b.lo, b.ul, b.last
   3d338:	ldur	x9, [x29, #-104]
   3d33c:	sub	x8, x9, x24
   3d340:	lsl	x22, x9, #3
   3d344:	sub	x8, x8, #0x2
   3d348:	sub	x9, x22, #0x8
   3d34c:	add	x10, x8, #0x1
   3d350:	cmp	x10, #0x1
   3d354:	b.lt	3d374 <__gmpn_toom62_mul@@Base+0x410>  // b.tstop
   3d358:	ldr	x10, [x25, x8, lsl #3]
   3d35c:	ldr	x11, [x25, x9]
   3d360:	sub	x8, x8, #0x1
   3d364:	sub	x9, x9, #0x8
   3d368:	cmp	x10, x11
   3d36c:	b.eq	3d34c <__gmpn_toom62_mul@@Base+0x3e8>  // b.none
   3d370:	b.ls	3d650 <__gmpn_toom62_mul@@Base+0x6ec>  // b.plast
   3d374:	cbz	x23, 3d3c4 <__gmpn_toom62_mul@@Base+0x460>
   3d378:	ldur	x22, [x29, #-96]
   3d37c:	mov	x0, x28
   3d380:	mov	x1, x25
   3d384:	mov	x3, x23
   3d388:	mov	x2, x22
   3d38c:	bl	c2d0 <__gmpn_sub_n@plt>
   3d390:	ldur	x12, [x29, #-32]
   3d394:	mov	x8, x23
   3d398:	cbz	x0, 3d3d0 <__gmpn_toom62_mul@@Base+0x46c>
   3d39c:	mov	x8, x23
   3d3a0:	cmp	x8, x24
   3d3a4:	b.gt	3d400 <__gmpn_toom62_mul@@Base+0x49c>
   3d3a8:	lsl	x9, x8, #3
   3d3ac:	ldr	x10, [x25, x9]
   3d3b0:	add	x8, x8, #0x1
   3d3b4:	sub	x11, x10, #0x1
   3d3b8:	str	x11, [x28, x9]
   3d3bc:	cbz	x10, 3d3a0 <__gmpn_toom62_mul@@Base+0x43c>
   3d3c0:	b	3d3d0 <__gmpn_toom62_mul@@Base+0x46c>
   3d3c4:	ldur	x22, [x29, #-96]
   3d3c8:	ldur	x12, [x29, #-32]
   3d3cc:	mov	x8, xzr
   3d3d0:	cmp	x28, x25
   3d3d4:	mov	w20, wzr
   3d3d8:	b.eq	3d428 <__gmpn_toom62_mul@@Base+0x4c4>  // b.none
   3d3dc:	cmp	x8, x24
   3d3e0:	b.gt	3d428 <__gmpn_toom62_mul@@Base+0x4c4>
   3d3e4:	lsl	x9, x8, #3
   3d3e8:	sub	x8, x21, x8
   3d3ec:	add	x0, x28, x9
   3d3f0:	add	x1, x25, x9
   3d3f4:	lsl	x2, x8, #3
   3d3f8:	bl	bed0 <memcpy@plt>
   3d3fc:	ldur	x12, [x29, #-32]
   3d400:	mov	w20, wzr
   3d404:	cbnz	x23, 3d42c <__gmpn_toom62_mul@@Base+0x4c8>
   3d408:	b	3d288 <__gmpn_toom62_mul@@Base+0x324>
   3d40c:	mov	x0, x28
   3d410:	mov	x1, x22
   3d414:	mov	x2, x25
   3d418:	mov	x3, x21
   3d41c:	bl	c2d0 <__gmpn_sub_n@plt>
   3d420:	ldur	x12, [x29, #-32]
   3d424:	mov	w20, #0x2                   	// #2
   3d428:	cbz	x23, 3d288 <__gmpn_toom62_mul@@Base+0x324>
   3d42c:	mov	x0, x12
   3d430:	mov	x1, x27
   3d434:	mov	x2, x22
   3d438:	mov	x3, x23
   3d43c:	bl	ca70 <__gmpn_add_n@plt>
   3d440:	ldur	x12, [x29, #-32]
   3d444:	mov	x8, x23
   3d448:	cbz	x0, 3d470 <__gmpn_toom62_mul@@Base+0x50c>
   3d44c:	mov	x8, x23
   3d450:	cmp	x8, x19
   3d454:	b.ge	3d48c <__gmpn_toom62_mul@@Base+0x528>  // b.tcont
   3d458:	lsl	x9, x8, #3
   3d45c:	ldr	x10, [x27, x9]
   3d460:	add	x8, x8, #0x1
   3d464:	adds	x10, x10, #0x1
   3d468:	str	x10, [x12, x9]
   3d46c:	b.cs	3d450 <__gmpn_toom62_mul@@Base+0x4ec>  // b.hs, b.nlast
   3d470:	subs	x9, x19, x8
   3d474:	b.le	3d48c <__gmpn_toom62_mul@@Base+0x528>
   3d478:	lsl	x8, x8, #3
   3d47c:	add	x0, x12, x8
   3d480:	add	x1, x27, x8
   3d484:	lsl	x2, x9, #3
   3d488:	bl	bed0 <memcpy@plt>
   3d48c:	lsl	x8, x21, #1
   3d490:	stur	x8, [x29, #-24]
   3d494:	cbz	w20, 3d4e4 <__gmpn_toom62_mul@@Base+0x580>
   3d498:	cbz	x23, 3d538 <__gmpn_toom62_mul@@Base+0x5d4>
   3d49c:	mov	x0, x26
   3d4a0:	mov	x1, x28
   3d4a4:	mov	x2, x22
   3d4a8:	mov	x3, x23
   3d4ac:	bl	ca70 <__gmpn_add_n@plt>
   3d4b0:	mov	x8, x23
   3d4b4:	cbz	x0, 3d53c <__gmpn_toom62_mul@@Base+0x5d8>
   3d4b8:	mov	w9, #0x1                   	// #1
   3d4bc:	mov	x8, x23
   3d4c0:	cmp	x8, x24
   3d4c4:	b.gt	3d560 <__gmpn_toom62_mul@@Base+0x5fc>
   3d4c8:	lsl	x10, x8, #3
   3d4cc:	ldr	x11, [x28, x10]
   3d4d0:	add	x8, x8, #0x1
   3d4d4:	adds	x11, x11, #0x1
   3d4d8:	str	x11, [x26, x10]
   3d4dc:	b.cs	3d4c0 <__gmpn_toom62_mul@@Base+0x55c>  // b.hs, b.nlast
   3d4e0:	b	3d53c <__gmpn_toom62_mul@@Base+0x5d8>
   3d4e4:	cmp	x24, x23
   3d4e8:	b.ge	3d578 <__gmpn_toom62_mul@@Base+0x614>  // b.tcont
   3d4ec:	ldur	x19, [x29, #-200]
   3d4f0:	add	x8, x25, x24, lsl #4
   3d4f4:	add	x8, x8, #0x8
   3d4f8:	add	x9, x24, #0x1
   3d4fc:	cmp	x9, #0x1
   3d500:	b.lt	3d51c <__gmpn_toom62_mul@@Base+0x5b8>  // b.tstop
   3d504:	ldr	x9, [x28, x24, lsl #3]
   3d508:	ldr	x10, [x8], #-8
   3d50c:	sub	x24, x24, #0x1
   3d510:	cmp	x9, x10
   3d514:	b.eq	3d4f8 <__gmpn_toom62_mul@@Base+0x594>  // b.none
   3d518:	b.ls	3d6a4 <__gmpn_toom62_mul@@Base+0x740>  // b.plast
   3d51c:	mov	x0, x26
   3d520:	mov	x1, x28
   3d524:	mov	x2, x22
   3d528:	mov	x3, x21
   3d52c:	bl	c2d0 <__gmpn_sub_n@plt>
   3d530:	stur	wzr, [x29, #-188]
   3d534:	b	3d6c0 <__gmpn_toom62_mul@@Base+0x75c>
   3d538:	mov	x8, xzr
   3d53c:	cmp	x8, x24
   3d540:	b.gt	3d55c <__gmpn_toom62_mul@@Base+0x5f8>
   3d544:	lsl	x9, x8, #3
   3d548:	sub	x8, x21, x8
   3d54c:	add	x0, x26, x9
   3d550:	add	x1, x28, x9
   3d554:	lsl	x2, x8, #3
   3d558:	bl	bed0 <memcpy@plt>
   3d55c:	mov	x9, xzr
   3d560:	ldur	x24, [x29, #-80]
   3d564:	ldur	x19, [x29, #-200]
   3d568:	orr	w8, w20, #0x1
   3d56c:	str	x9, [x26, x21, lsl #3]
   3d570:	stur	w8, [x29, #-188]
   3d574:	b	3d6c8 <__gmpn_toom62_mul@@Base+0x764>
   3d578:	ldur	x8, [x29, #-104]
   3d57c:	add	x9, x28, x24, lsl #3
   3d580:	sub	x8, x8, x24, lsl #1
   3d584:	sub	x8, x8, #0x2
   3d588:	ldr	x10, [x9]
   3d58c:	cbnz	x10, 3d5d4 <__gmpn_toom62_mul@@Base+0x670>
   3d590:	adds	x8, x8, #0x1
   3d594:	sub	x9, x9, #0x8
   3d598:	b.cc	3d588 <__gmpn_toom62_mul@@Base+0x624>  // b.lo, b.ul, b.last
   3d59c:	ldur	x8, [x29, #-104]
   3d5a0:	sub	x9, x28, #0x10
   3d5a4:	sub	x10, x8, x24
   3d5a8:	add	x8, x25, x8, lsl #3
   3d5ac:	sub	x8, x8, #0x8
   3d5b0:	sub	x11, x10, #0x1
   3d5b4:	cmp	x11, #0x1
   3d5b8:	b.lt	3d5d4 <__gmpn_toom62_mul@@Base+0x670>  // b.tstop
   3d5bc:	ldr	x10, [x9, x10, lsl #3]
   3d5c0:	ldr	x12, [x8], #-8
   3d5c4:	cmp	x10, x12
   3d5c8:	mov	x10, x11
   3d5cc:	b.eq	3d5b0 <__gmpn_toom62_mul@@Base+0x64c>  // b.none
   3d5d0:	b.ls	3d954 <__gmpn_toom62_mul@@Base+0x9f0>  // b.plast
   3d5d4:	cbz	x23, 3d61c <__gmpn_toom62_mul@@Base+0x6b8>
   3d5d8:	mov	x0, x26
   3d5dc:	mov	x1, x28
   3d5e0:	mov	x2, x22
   3d5e4:	mov	x3, x23
   3d5e8:	bl	c2d0 <__gmpn_sub_n@plt>
   3d5ec:	mov	x8, x23
   3d5f0:	cbz	x0, 3d620 <__gmpn_toom62_mul@@Base+0x6bc>
   3d5f4:	mov	x8, x23
   3d5f8:	cmp	x8, x24
   3d5fc:	b.gt	3d640 <__gmpn_toom62_mul@@Base+0x6dc>
   3d600:	lsl	x9, x8, #3
   3d604:	ldr	x10, [x28, x9]
   3d608:	add	x8, x8, #0x1
   3d60c:	sub	x11, x10, #0x1
   3d610:	str	x11, [x26, x9]
   3d614:	cbz	x10, 3d5f8 <__gmpn_toom62_mul@@Base+0x694>
   3d618:	b	3d620 <__gmpn_toom62_mul@@Base+0x6bc>
   3d61c:	mov	x8, xzr
   3d620:	cmp	x8, x24
   3d624:	b.gt	3d640 <__gmpn_toom62_mul@@Base+0x6dc>
   3d628:	lsl	x9, x8, #3
   3d62c:	sub	x8, x21, x8
   3d630:	add	x0, x26, x9
   3d634:	add	x1, x28, x9
   3d638:	lsl	x2, x8, #3
   3d63c:	bl	bed0 <memcpy@plt>
   3d640:	ldur	x24, [x29, #-80]
   3d644:	ldur	x19, [x29, #-200]
   3d648:	stur	wzr, [x29, #-188]
   3d64c:	b	3d6c4 <__gmpn_toom62_mul@@Base+0x760>
   3d650:	ldur	x1, [x29, #-96]
   3d654:	mov	x0, x28
   3d658:	mov	x2, x25
   3d65c:	mov	x3, x23
   3d660:	bl	c2d0 <__gmpn_sub_n@plt>
   3d664:	cbz	x20, 3d690 <__gmpn_toom62_mul@@Base+0x72c>
   3d668:	ldur	x10, [x29, #-104]
   3d66c:	sub	x8, x22, x24, lsl #3
   3d670:	lsl	x9, x24, #1
   3d674:	add	x8, x8, x28
   3d678:	sub	x9, x9, x10
   3d67c:	sub	x0, x8, #0x8
   3d680:	lsl	x8, x9, #3
   3d684:	add	x2, x8, #0x10
   3d688:	mov	w1, wzr
   3d68c:	bl	c5f0 <memset@plt>
   3d690:	ldur	x22, [x29, #-96]
   3d694:	ldur	x12, [x29, #-32]
   3d698:	mov	w20, #0x2                   	// #2
   3d69c:	cbnz	x23, 3d42c <__gmpn_toom62_mul@@Base+0x4c8>
   3d6a0:	b	3d288 <__gmpn_toom62_mul@@Base+0x324>
   3d6a4:	mov	x0, x26
   3d6a8:	mov	x1, x22
   3d6ac:	mov	x2, x28
   3d6b0:	mov	x3, x21
   3d6b4:	bl	c2d0 <__gmpn_sub_n@plt>
   3d6b8:	mov	w8, #0x1                   	// #1
   3d6bc:	stur	w8, [x29, #-188]
   3d6c0:	ldur	x24, [x29, #-80]
   3d6c4:	str	xzr, [x26, x21, lsl #3]
   3d6c8:	mov	x23, x26
   3d6cc:	ldur	x26, [x29, #-176]
   3d6d0:	ldr	x20, [x27, x24]
   3d6d4:	mov	x1, x27
   3d6d8:	mov	x2, x25
   3d6dc:	mov	x0, x26
   3d6e0:	mov	x3, x21
   3d6e4:	bl	ca70 <__gmpn_add_n@plt>
   3d6e8:	add	x8, x0, x20
   3d6ec:	ldp	x22, x2, [x29, #-40]
   3d6f0:	ldur	x20, [x29, #-56]
   3d6f4:	ldur	x1, [x29, #-160]
   3d6f8:	str	x8, [x26, x24]
   3d6fc:	mov	x0, x22
   3d700:	mov	x3, x20
   3d704:	bl	c990 <__gmpn_mul_n@plt>
   3d708:	ldur	x8, [x29, #-24]
   3d70c:	ldur	x1, [x29, #-144]
   3d710:	mov	x2, x23
   3d714:	mov	x3, x20
   3d718:	add	x8, x22, x8, lsl #3
   3d71c:	add	x0, x8, #0x8
   3d720:	stur	x0, [x29, #-32]
   3d724:	bl	c990 <__gmpn_mul_n@plt>
   3d728:	ldur	x1, [x29, #-168]
   3d72c:	add	x8, x22, x19, lsl #3
   3d730:	add	x0, x8, #0x10
   3d734:	mov	x2, x26
   3d738:	mov	x3, x20
   3d73c:	mov	x23, x0
   3d740:	bl	c990 <__gmpn_mul_n@plt>
   3d744:	add	x20, x21, x21, lsl #1
   3d748:	add	x8, x22, x20, lsl #4
   3d74c:	ldur	x22, [x29, #-128]
   3d750:	add	x19, x8, #0x18
   3d754:	mov	x0, x19
   3d758:	mov	x2, x28
   3d75c:	mov	x1, x22
   3d760:	mov	x3, x21
   3d764:	bl	c990 <__gmpn_mul_n@plt>
   3d768:	ldr	x8, [x22, x24]
   3d76c:	stur	x23, [x29, #-104]
   3d770:	cmp	x8, #0x2
   3d774:	b.eq	3d798 <__gmpn_toom62_mul@@Base+0x834>  // b.none
   3d778:	cmp	x8, #0x1
   3d77c:	b.ne	3d7b0 <__gmpn_toom62_mul@@Base+0x84c>  // b.any
   3d780:	add	x0, x19, x21, lsl #3
   3d784:	mov	x1, x0
   3d788:	mov	x2, x28
   3d78c:	mov	x3, x21
   3d790:	bl	ca70 <__gmpn_add_n@plt>
   3d794:	b	3d7b4 <__gmpn_toom62_mul@@Base+0x850>
   3d798:	add	x0, x19, x21, lsl #3
   3d79c:	mov	x1, x0
   3d7a0:	mov	x2, x28
   3d7a4:	mov	x3, x21
   3d7a8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d7ac:	b	3d7b4 <__gmpn_toom62_mul@@Base+0x850>
   3d7b0:	mov	x0, xzr
   3d7b4:	ldur	x8, [x29, #-24]
   3d7b8:	ldp	x9, x28, [x29, #-72]
   3d7bc:	ldur	x26, [x29, #-88]
   3d7c0:	mov	x2, x27
   3d7c4:	lsl	x8, x8, #3
   3d7c8:	add	x22, x9, x8
   3d7cc:	str	x0, [x19, x8]
   3d7d0:	mov	x0, x22
   3d7d4:	mov	x1, x26
   3d7d8:	mov	x3, x21
   3d7dc:	bl	c990 <__gmpn_mul_n@plt>
   3d7e0:	ldr	x26, [x26, x21, lsl #3]
   3d7e4:	cbz	x26, 3d868 <__gmpn_toom62_mul@@Base+0x904>
   3d7e8:	cmp	x26, #0x2
   3d7ec:	b.eq	3d818 <__gmpn_toom62_mul@@Base+0x8b4>  // b.none
   3d7f0:	cmp	x26, #0x1
   3d7f4:	b.ne	3d838 <__gmpn_toom62_mul@@Base+0x8d4>  // b.any
   3d7f8:	ldr	x26, [x27, x24]
   3d7fc:	add	x0, x22, x24
   3d800:	mov	x1, x0
   3d804:	mov	x2, x27
   3d808:	mov	x3, x21
   3d80c:	bl	ca70 <__gmpn_add_n@plt>
   3d810:	add	x26, x0, x26
   3d814:	b	3d868 <__gmpn_toom62_mul@@Base+0x904>
   3d818:	ldr	x26, [x27, x24]
   3d81c:	add	x0, x22, x24
   3d820:	mov	x1, x0
   3d824:	mov	x2, x27
   3d828:	mov	x3, x21
   3d82c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d830:	add	x26, x0, x26, lsl #1
   3d834:	b	3d868 <__gmpn_toom62_mul@@Base+0x904>
   3d838:	ldur	x8, [x29, #-80]
   3d83c:	mov	x24, x28
   3d840:	mov	x1, x27
   3d844:	mov	x2, x21
   3d848:	ldr	x28, [x27, x8]
   3d84c:	ldur	x8, [x29, #-80]
   3d850:	mov	x3, x26
   3d854:	add	x0, x22, x8
   3d858:	bl	d400 <__gmpn_addmul_1@plt>
   3d85c:	madd	x26, x28, x26, x0
   3d860:	mov	x28, x24
   3d864:	ldur	x24, [x29, #-80]
   3d868:	ldur	w9, [x29, #-120]
   3d86c:	ldr	x8, [x27, x21, lsl #3]
   3d870:	lsl	x23, x20, #1
   3d874:	and	w27, w9, #0x2
   3d878:	cbz	x8, 3d894 <__gmpn_toom62_mul@@Base+0x930>
   3d87c:	ldur	x2, [x29, #-88]
   3d880:	add	x0, x22, x21, lsl #3
   3d884:	mov	x1, x0
   3d888:	mov	x3, x21
   3d88c:	bl	ca70 <__gmpn_add_n@plt>
   3d890:	add	x26, x0, x26
   3d894:	ldur	w8, [x29, #-116]
   3d898:	mov	x2, x25
   3d89c:	mov	x3, x21
   3d8a0:	bfxil	w27, w8, #0, #1
   3d8a4:	ldur	x8, [x29, #-24]
   3d8a8:	str	x26, [x22, x8, lsl #3]
   3d8ac:	ldur	x22, [x29, #-72]
   3d8b0:	ldur	x26, [x29, #-16]
   3d8b4:	mov	x0, x22
   3d8b8:	mov	x1, x26
   3d8bc:	bl	c990 <__gmpn_mul_n@plt>
   3d8c0:	add	x0, x22, x23, lsl #3
   3d8c4:	ldur	x23, [x29, #-48]
   3d8c8:	ldur	x25, [x29, #-112]
   3d8cc:	add	x3, x26, x28, lsl #3
   3d8d0:	cmp	x25, x23
   3d8d4:	b.le	3d8ec <__gmpn_toom62_mul@@Base+0x988>
   3d8d8:	mov	x1, x3
   3d8dc:	ldur	x3, [x29, #-96]
   3d8e0:	mov	x2, x25
   3d8e4:	mov	x4, x23
   3d8e8:	b	3d8f8 <__gmpn_toom62_mul@@Base+0x994>
   3d8ec:	ldur	x1, [x29, #-96]
   3d8f0:	mov	x2, x23
   3d8f4:	mov	x4, x25
   3d8f8:	bl	ccd0 <__gmpn_mul@plt>
   3d8fc:	ldur	w8, [x29, #-188]
   3d900:	ldur	x5, [x29, #-40]
   3d904:	add	x7, x25, x23
   3d908:	eor	w2, w8, w27
   3d90c:	add	x8, x5, x24, lsl #3
   3d910:	add	x8, x8, #0x20
   3d914:	str	x8, [sp, #-16]!
   3d918:	ldur	x3, [x29, #-32]
   3d91c:	ldur	x6, [x29, #-104]
   3d920:	mov	x0, x22
   3d924:	mov	x1, x21
   3d928:	mov	x4, x19
   3d92c:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   3d930:	add	sp, sp, #0x10
   3d934:	mov	sp, x29
   3d938:	ldp	x20, x19, [sp, #80]
   3d93c:	ldp	x22, x21, [sp, #64]
   3d940:	ldp	x24, x23, [sp, #48]
   3d944:	ldp	x26, x25, [sp, #32]
   3d948:	ldp	x28, x27, [sp, #16]
   3d94c:	ldp	x29, x30, [sp], #96
   3d950:	ret
   3d954:	mov	x0, x26
   3d958:	mov	x1, x22
   3d95c:	mov	x2, x28
   3d960:	mov	x3, x23
   3d964:	bl	c2d0 <__gmpn_sub_n@plt>
   3d968:	cmp	x19, x23
   3d96c:	ldur	x19, [x29, #-200]
   3d970:	b.eq	3d9a0 <__gmpn_toom62_mul@@Base+0xa3c>  // b.none
   3d974:	ldur	x10, [x29, #-104]
   3d978:	lsl	x9, x24, #1
   3d97c:	mov	w1, wzr
   3d980:	lsl	x8, x10, #3
   3d984:	sub	x8, x8, x24, lsl #3
   3d988:	sub	x9, x9, x10
   3d98c:	add	x8, x8, x26
   3d990:	lsl	x9, x9, #3
   3d994:	sub	x0, x8, #0x8
   3d998:	add	x2, x9, #0x18
   3d99c:	bl	c5f0 <memset@plt>
   3d9a0:	ldur	x24, [x29, #-80]
   3d9a4:	mov	w8, #0x1                   	// #1
   3d9a8:	stur	w8, [x29, #-188]
   3d9ac:	b	3d6c8 <__gmpn_toom62_mul@@Base+0x764>

000000000003d9b0 <__gmpn_toom33_mul@@Base>:
   3d9b0:	sub	sp, sp, #0xe0
   3d9b4:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3d9b8:	add	x8, x2, #0x2
   3d9bc:	movk	x9, #0xaaab
   3d9c0:	umulh	x8, x8, x9
   3d9c4:	lsr	x14, x8, #1
   3d9c8:	add	x8, x5, x14, lsl #5
   3d9cc:	stp	x29, x30, [sp, #128]
   3d9d0:	stp	x26, x25, [sp, #160]
   3d9d4:	add	x29, sp, #0x80
   3d9d8:	lsl	x26, x14, #4
   3d9dc:	add	x8, x8, #0x20
   3d9e0:	stp	x28, x27, [sp, #144]
   3d9e4:	stp	x24, x23, [sp, #176]
   3d9e8:	stp	x22, x21, [sp, #192]
   3d9ec:	stp	x20, x19, [sp, #208]
   3d9f0:	stur	x3, [x29, #-40]
   3d9f4:	lsl	x21, x14, #1
   3d9f8:	add	x9, x0, x14, lsl #3
   3d9fc:	str	x2, [sp, #24]
   3da00:	stur	x8, [x29, #-24]
   3da04:	add	x8, x5, x26
   3da08:	mov	x19, x5
   3da0c:	mov	x28, x4
   3da10:	mov	x23, x1
   3da14:	mov	x20, x0
   3da18:	sub	x24, x2, x21
   3da1c:	str	x8, [sp, #56]
   3da20:	add	x8, x9, #0x8
   3da24:	add	x2, x1, x26
   3da28:	lsl	x22, x14, #3
   3da2c:	str	x9, [sp, #8]
   3da30:	str	x8, [sp, #64]
   3da34:	stur	x14, [x29, #-8]
   3da38:	stur	x2, [x29, #-56]
   3da3c:	cbz	x24, 3da88 <__gmpn_toom33_mul@@Base+0xd8>
   3da40:	mov	x0, x19
   3da44:	mov	x1, x23
   3da48:	mov	x3, x24
   3da4c:	bl	ca70 <__gmpn_add_n@plt>
   3da50:	ldur	x14, [x29, #-8]
   3da54:	mov	x8, x24
   3da58:	cbz	x0, 3da8c <__gmpn_toom33_mul@@Base+0xdc>
   3da5c:	mov	w27, #0x1                   	// #1
   3da60:	mov	x8, x24
   3da64:	cmp	x8, x14
   3da68:	b.ge	3db2c <__gmpn_toom33_mul@@Base+0x17c>  // b.tcont
   3da6c:	lsl	x9, x8, #3
   3da70:	ldr	x10, [x23, x9]
   3da74:	add	x8, x8, #0x1
   3da78:	adds	x10, x10, #0x1
   3da7c:	str	x10, [x19, x9]
   3da80:	b.cs	3da64 <__gmpn_toom33_mul@@Base+0xb4>  // b.hs, b.nlast
   3da84:	b	3da8c <__gmpn_toom33_mul@@Base+0xdc>
   3da88:	mov	x8, xzr
   3da8c:	cmp	x19, x23
   3da90:	mov	x27, xzr
   3da94:	b.eq	3db2c <__gmpn_toom33_mul@@Base+0x17c>  // b.none
   3da98:	subs	x9, x14, x8
   3da9c:	b.le	3db2c <__gmpn_toom33_mul@@Base+0x17c>
   3daa0:	cmp	x9, #0x4
   3daa4:	b.cc	3db08 <__gmpn_toom33_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3daa8:	lsl	x11, x8, #3
   3daac:	add	x10, x19, x11
   3dab0:	add	x12, x23, x22
   3dab4:	cmp	x10, x12
   3dab8:	b.cs	3dad0 <__gmpn_toom33_mul@@Base+0x120>  // b.hs, b.nlast
   3dabc:	add	x10, x19, x22
   3dac0:	add	x12, x23, x11
   3dac4:	cmp	x12, x10
   3dac8:	mov	x13, x22
   3dacc:	b.cc	3db08 <__gmpn_toom33_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3dad0:	and	x10, x9, #0xfffffffffffffffc
   3dad4:	add	x12, x11, #0x10
   3dad8:	add	x8, x8, x10
   3dadc:	add	x11, x23, x12
   3dae0:	add	x12, x19, x12
   3dae4:	mov	x13, x10
   3dae8:	ldp	q0, q1, [x11, #-16]
   3daec:	add	x11, x11, #0x20
   3daf0:	subs	x13, x13, #0x4
   3daf4:	stp	q0, q1, [x12, #-16]
   3daf8:	add	x12, x12, #0x20
   3dafc:	b.ne	3dae8 <__gmpn_toom33_mul@@Base+0x138>  // b.any
   3db00:	cmp	x9, x10
   3db04:	b.eq	3db28 <__gmpn_toom33_mul@@Base+0x178>  // b.none
   3db08:	lsl	x10, x8, #3
   3db0c:	sub	x9, x14, x8
   3db10:	add	x8, x19, x10
   3db14:	add	x10, x23, x10
   3db18:	ldr	x11, [x10], #8
   3db1c:	subs	x9, x9, #0x1
   3db20:	str	x11, [x8], #8
   3db24:	b.ne	3db18 <__gmpn_toom33_mul@@Base+0x168>  // b.any
   3db28:	mov	x27, xzr
   3db2c:	sub	x8, x28, x21
   3db30:	stur	x24, [x29, #-48]
   3db34:	stp	x28, x21, [sp, #32]
   3db38:	stur	x8, [x29, #-16]
   3db3c:	ldr	x8, [sp, #56]
   3db40:	add	x24, x23, x22
   3db44:	mov	x1, x19
   3db48:	mov	x2, x24
   3db4c:	add	x8, x8, #0x10
   3db50:	str	x8, [sp, #48]
   3db54:	ldur	x28, [x29, #-24]
   3db58:	ldur	x3, [x29, #-8]
   3db5c:	mov	x25, x22
   3db60:	mov	x0, x28
   3db64:	bl	ca70 <__gmpn_add_n@plt>
   3db68:	add	x8, x0, x27
   3db6c:	stur	x19, [x29, #-32]
   3db70:	ldur	x21, [x29, #-8]
   3db74:	str	x8, [x28, x22]
   3db78:	ldr	x28, [sp, #64]
   3db7c:	cbz	x27, 3dc70 <__gmpn_toom33_mul@@Base+0x2c0>
   3db80:	ldr	x25, [sp, #48]
   3db84:	mov	x1, x19
   3db88:	mov	x2, x24
   3db8c:	mov	x3, x21
   3db90:	mov	x0, x25
   3db94:	bl	c2d0 <__gmpn_sub_n@plt>
   3db98:	sub	x8, x27, x0
   3db9c:	str	wzr, [sp, #20]
   3dba0:	str	x8, [x25, x21, lsl #3]
   3dba4:	ldp	x1, x19, [x29, #-56]
   3dba8:	ldur	x24, [x29, #-24]
   3dbac:	mov	x0, x28
   3dbb0:	add	x26, x20, x26
   3dbb4:	mov	x3, x19
   3dbb8:	mov	x2, x24
   3dbbc:	bl	ca70 <__gmpn_add_n@plt>
   3dbc0:	mov	x27, x22
   3dbc4:	ldur	x22, [x29, #-40]
   3dbc8:	subs	x14, x21, x19
   3dbcc:	b.eq	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3dbd0:	lsl	x8, x19, #3
   3dbd4:	ldr	x9, [x24, x8]
   3dbd8:	adds	x9, x9, x0
   3dbdc:	str	x9, [x28, x8]
   3dbe0:	b.cc	3dcc8 <__gmpn_toom33_mul@@Base+0x318>  // b.lo, b.ul, b.last
   3dbe4:	ldur	x8, [x29, #-32]
   3dbe8:	ldr	x11, [sp, #24]
   3dbec:	add	x9, x21, x21, lsl #1
   3dbf0:	sub	x12, x20, x21, lsl #3
   3dbf4:	add	x10, x8, x21, lsl #4
   3dbf8:	mvn	x8, x11
   3dbfc:	lsl	x13, x11, #3
   3dc00:	mov	w0, #0x1                   	// #1
   3dc04:	add	x11, x8, x9
   3dc08:	mov	w8, #0x1                   	// #1
   3dc0c:	cmp	x8, x14
   3dc10:	b.ge	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.tcont
   3dc14:	add	x15, x10, x13
   3dc18:	ldr	x15, [x15, #40]
   3dc1c:	add	x16, x12, x13
   3dc20:	add	x8, x8, #0x1
   3dc24:	add	x12, x12, #0x8
   3dc28:	add	x10, x10, #0x8
   3dc2c:	adds	x15, x15, #0x1
   3dc30:	sub	x11, x11, #0x1
   3dc34:	str	x15, [x16, #16]
   3dc38:	b.cs	3dc0c <__gmpn_toom33_mul@@Base+0x25c>  // b.hs, b.nlast
   3dc3c:	cmp	x24, x28
   3dc40:	mov	x0, xzr
   3dc44:	b.eq	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3dc48:	cmp	x8, x14
   3dc4c:	b.ge	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.tcont
   3dc50:	ldr	x0, [sp, #24]
   3dc54:	add	x14, x21, x21, lsl #1
   3dc58:	sub	x15, x14, x0
   3dc5c:	sub	x15, x15, x8
   3dc60:	cmp	x15, #0x4
   3dc64:	b.cs	3ddb8 <__gmpn_toom33_mul@@Base+0x408>  // b.hs, b.nlast
   3dc68:	ldur	x18, [x29, #-32]
   3dc6c:	b	3de3c <__gmpn_toom33_mul@@Base+0x48c>
   3dc70:	add	x8, x23, x21, lsl #4
   3dc74:	sub	x8, x8, #0x8
   3dc78:	mov	x10, x21
   3dc7c:	subs	x9, x10, #0x1
   3dc80:	b.lt	3db80 <__gmpn_toom33_mul@@Base+0x1d0>  // b.tstop
   3dc84:	add	x10, x19, x10, lsl #3
   3dc88:	ldur	x10, [x10, #-8]
   3dc8c:	ldr	x11, [x8], #-8
   3dc90:	cmp	x10, x11
   3dc94:	mov	x10, x9
   3dc98:	b.eq	3dc7c <__gmpn_toom33_mul@@Base+0x2cc>  // b.none
   3dc9c:	b.hi	3db80 <__gmpn_toom33_mul@@Base+0x1d0>  // b.pmore
   3dca0:	ldr	x25, [sp, #48]
   3dca4:	mov	x1, x24
   3dca8:	mov	x2, x19
   3dcac:	mov	x3, x21
   3dcb0:	mov	x0, x25
   3dcb4:	bl	c2d0 <__gmpn_sub_n@plt>
   3dcb8:	mov	w9, #0x1                   	// #1
   3dcbc:	mov	x8, xzr
   3dcc0:	str	w9, [sp, #20]
   3dcc4:	b	3dba0 <__gmpn_toom33_mul@@Base+0x1f0>
   3dcc8:	cmp	x24, x28
   3dccc:	mov	x0, xzr
   3dcd0:	b.eq	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3dcd4:	cmp	x14, #0x2
   3dcd8:	b.lt	3de74 <__gmpn_toom33_mul@@Base+0x4c4>  // b.tstop
   3dcdc:	ldr	x16, [sp, #24]
   3dce0:	add	x8, x21, x21, lsl #1
   3dce4:	mvn	x9, x16
   3dce8:	add	x9, x8, x9
   3dcec:	cmp	x9, #0x4
   3dcf0:	b.cs	3dd00 <__gmpn_toom33_mul@@Base+0x350>  // b.hs, b.nlast
   3dcf4:	ldur	x15, [x29, #-32]
   3dcf8:	mov	w10, #0x1                   	// #1
   3dcfc:	b	3dd80 <__gmpn_toom33_mul@@Base+0x3d0>
   3dd00:	ldur	x15, [x29, #-32]
   3dd04:	sub	x10, x16, x21
   3dd08:	mov	w12, #0x28                  	// #40
   3dd0c:	add	x13, x20, x10, lsl #3
   3dd10:	madd	x10, x21, x12, x15
   3dd14:	add	x11, x16, x21, lsl #1
   3dd18:	add	x12, x13, #0x10
   3dd1c:	add	x10, x10, #0x20
   3dd20:	cmp	x12, x10
   3dd24:	add	x10, x15, x11, lsl #3
   3dd28:	b.cs	3dd4c <__gmpn_toom33_mul@@Base+0x39c>  // b.hs, b.nlast
   3dd2c:	mov	w11, #0x8                   	// #8
   3dd30:	bfi	x11, x21, #4, #60
   3dd34:	add	x11, x20, x11
   3dd38:	add	x12, x10, #0x28
   3dd3c:	cmp	x12, x11
   3dd40:	b.cs	3dd4c <__gmpn_toom33_mul@@Base+0x39c>  // b.hs, b.nlast
   3dd44:	mov	w10, #0x1                   	// #1
   3dd48:	b	3dd80 <__gmpn_toom33_mul@@Base+0x3d0>
   3dd4c:	and	x11, x9, #0xfffffffffffffffc
   3dd50:	add	x12, x10, #0x38
   3dd54:	orr	x10, x11, #0x1
   3dd58:	add	x13, x13, #0x20
   3dd5c:	mov	x14, x11
   3dd60:	ldp	q0, q1, [x12, #-16]
   3dd64:	subs	x14, x14, #0x4
   3dd68:	add	x12, x12, #0x20
   3dd6c:	stp	q0, q1, [x13, #-16]
   3dd70:	add	x13, x13, #0x20
   3dd74:	b.ne	3dd60 <__gmpn_toom33_mul@@Base+0x3b0>  // b.any
   3dd78:	cmp	x9, x11
   3dd7c:	b.eq	3de70 <__gmpn_toom33_mul@@Base+0x4c0>  // b.none
   3dd80:	add	x9, x10, x16
   3dd84:	sub	x8, x8, x10
   3dd88:	sub	x10, x9, x21
   3dd8c:	add	x9, x9, x21, lsl #1
   3dd90:	add	x10, x20, x10, lsl #3
   3dd94:	add	x11, x15, x9, lsl #3
   3dd98:	sub	x8, x8, x16
   3dd9c:	add	x9, x10, #0x8
   3dda0:	add	x10, x11, #0x20
   3dda4:	ldr	x11, [x10], #8
   3dda8:	subs	x8, x8, #0x1
   3ddac:	str	x11, [x9], #8
   3ddb0:	b.ne	3dda4 <__gmpn_toom33_mul@@Base+0x3f4>  // b.any
   3ddb4:	b	3de70 <__gmpn_toom33_mul@@Base+0x4c0>
   3ddb8:	ldur	x18, [x29, #-32]
   3ddbc:	mov	w17, #0x28                  	// #40
   3ddc0:	add	x16, x12, x13
   3ddc4:	add	x16, x16, #0x10
   3ddc8:	madd	x17, x21, x17, x18
   3ddcc:	add	x17, x17, #0x20
   3ddd0:	cmp	x16, x17
   3ddd4:	b.cs	3ddf4 <__gmpn_toom33_mul@@Base+0x444>  // b.hs, b.nlast
   3ddd8:	mov	w16, #0x8                   	// #8
   3dddc:	add	x17, x10, x13
   3dde0:	bfi	x16, x21, #4, #60
   3dde4:	add	x16, x20, x16
   3dde8:	add	x17, x17, #0x28
   3ddec:	cmp	x17, x16
   3ddf0:	b.cc	3de3c <__gmpn_toom33_mul@@Base+0x48c>  // b.lo, b.ul, b.last
   3ddf4:	add	x12, x12, x13
   3ddf8:	sub	x16, x9, x0
   3ddfc:	add	x13, x10, x13
   3de00:	and	x17, x11, #0xfffffffffffffffc
   3de04:	add	x10, x12, #0x20
   3de08:	sub	x12, x16, x8
   3de0c:	and	x9, x15, #0xfffffffffffffffc
   3de10:	add	x11, x13, #0x38
   3de14:	add	x8, x17, x8
   3de18:	and	x12, x12, #0xfffffffffffffffc
   3de1c:	ldp	q0, q1, [x11, #-16]
   3de20:	subs	x12, x12, #0x4
   3de24:	add	x11, x11, #0x20
   3de28:	stp	q0, q1, [x10, #-16]
   3de2c:	add	x10, x10, #0x20
   3de30:	b.ne	3de1c <__gmpn_toom33_mul@@Base+0x46c>  // b.any
   3de34:	cmp	x15, x9
   3de38:	b.eq	3de70 <__gmpn_toom33_mul@@Base+0x4c0>  // b.none
   3de3c:	sub	x9, x14, x8
   3de40:	add	x10, x8, x0
   3de44:	sub	x8, x9, x0
   3de48:	sub	x9, x10, x21
   3de4c:	add	x10, x10, x21, lsl #1
   3de50:	add	x9, x20, x9, lsl #3
   3de54:	add	x10, x18, x10, lsl #3
   3de58:	add	x9, x9, #0x8
   3de5c:	add	x10, x10, #0x20
   3de60:	ldr	x11, [x10], #8
   3de64:	subs	x8, x8, #0x1
   3de68:	str	x11, [x9], #8
   3de6c:	b.ne	3de60 <__gmpn_toom33_mul@@Base+0x4b0>  // b.any
   3de70:	mov	x0, xzr
   3de74:	ldr	x8, [x24, x27]
   3de78:	ldur	x19, [x29, #-32]
   3de7c:	mov	w9, #0x18                  	// #24
   3de80:	str	x26, [sp]
   3de84:	add	x25, x26, #0x10
   3de88:	add	x26, x8, x0
   3de8c:	mov	x0, x28
   3de90:	mov	x1, x23
   3de94:	mov	x2, x28
   3de98:	mov	x3, x21
   3de9c:	madd	x24, x21, x9, x19
   3dea0:	bl	d090 <__gmpn_rsblsh1_n@plt>
   3dea4:	add	x8, x0, x26, lsl #1
   3dea8:	str	x8, [x28, x27]
   3deac:	ldr	x8, [sp, #40]
   3deb0:	ldur	x21, [x29, #-16]
   3deb4:	add	x2, x22, x8, lsl #3
   3deb8:	str	x2, [sp, #40]
   3debc:	cbz	x21, 3df08 <__gmpn_toom33_mul@@Base+0x558>
   3dec0:	mov	x0, x19
   3dec4:	mov	x1, x22
   3dec8:	mov	x3, x21
   3decc:	bl	ca70 <__gmpn_add_n@plt>
   3ded0:	ldur	x14, [x29, #-8]
   3ded4:	mov	x8, x21
   3ded8:	cbz	x0, 3df10 <__gmpn_toom33_mul@@Base+0x560>
   3dedc:	mov	w26, #0x1                   	// #1
   3dee0:	mov	x8, x21
   3dee4:	cmp	x8, x14
   3dee8:	b.ge	3dfac <__gmpn_toom33_mul@@Base+0x5fc>  // b.tcont
   3deec:	lsl	x9, x8, #3
   3def0:	ldr	x10, [x22, x9]
   3def4:	add	x8, x8, #0x1
   3def8:	adds	x10, x10, #0x1
   3defc:	str	x10, [x19, x9]
   3df00:	b.cs	3dee4 <__gmpn_toom33_mul@@Base+0x534>  // b.hs, b.nlast
   3df04:	b	3df10 <__gmpn_toom33_mul@@Base+0x560>
   3df08:	ldur	x14, [x29, #-8]
   3df0c:	mov	x8, xzr
   3df10:	cmp	x19, x22
   3df14:	mov	x26, xzr
   3df18:	b.eq	3dfac <__gmpn_toom33_mul@@Base+0x5fc>  // b.none
   3df1c:	subs	x9, x14, x8
   3df20:	b.le	3dfac <__gmpn_toom33_mul@@Base+0x5fc>
   3df24:	cmp	x9, #0x4
   3df28:	b.cc	3df88 <__gmpn_toom33_mul@@Base+0x5d8>  // b.lo, b.ul, b.last
   3df2c:	lsl	x11, x8, #3
   3df30:	add	x10, x19, x11
   3df34:	add	x12, x22, x27
   3df38:	cmp	x10, x12
   3df3c:	b.cs	3df50 <__gmpn_toom33_mul@@Base+0x5a0>  // b.hs, b.nlast
   3df40:	add	x10, x19, x27
   3df44:	add	x12, x22, x11
   3df48:	cmp	x12, x10
   3df4c:	b.cc	3df88 <__gmpn_toom33_mul@@Base+0x5d8>  // b.lo, b.ul, b.last
   3df50:	and	x10, x9, #0xfffffffffffffffc
   3df54:	add	x12, x11, #0x10
   3df58:	add	x8, x8, x10
   3df5c:	add	x11, x22, x12
   3df60:	add	x12, x19, x12
   3df64:	mov	x13, x10
   3df68:	ldp	q0, q1, [x11, #-16]
   3df6c:	add	x11, x11, #0x20
   3df70:	subs	x13, x13, #0x4
   3df74:	stp	q0, q1, [x12, #-16]
   3df78:	add	x12, x12, #0x20
   3df7c:	b.ne	3df68 <__gmpn_toom33_mul@@Base+0x5b8>  // b.any
   3df80:	cmp	x9, x10
   3df84:	b.eq	3dfa8 <__gmpn_toom33_mul@@Base+0x5f8>  // b.none
   3df88:	lsl	x10, x8, #3
   3df8c:	sub	x9, x14, x8
   3df90:	add	x8, x19, x10
   3df94:	add	x10, x22, x10
   3df98:	ldr	x11, [x10], #8
   3df9c:	subs	x9, x9, #0x1
   3dfa0:	str	x11, [x8], #8
   3dfa4:	b.ne	3df98 <__gmpn_toom33_mul@@Base+0x5e8>  // b.any
   3dfa8:	mov	x26, xzr
   3dfac:	ldur	x3, [x29, #-8]
   3dfb0:	add	x28, x24, #0x18
   3dfb4:	add	x24, x22, x27
   3dfb8:	mov	x0, x20
   3dfbc:	mov	x1, x19
   3dfc0:	mov	x2, x24
   3dfc4:	bl	ca70 <__gmpn_add_n@plt>
   3dfc8:	ldur	x10, [x29, #-8]
   3dfcc:	add	x8, x0, x26
   3dfd0:	str	x8, [x20, x27]
   3dfd4:	cbz	x26, 3e14c <__gmpn_toom33_mul@@Base+0x79c>
   3dfd8:	ldur	x3, [x29, #-8]
   3dfdc:	mov	x0, x28
   3dfe0:	mov	x1, x19
   3dfe4:	mov	x2, x24
   3dfe8:	bl	c2d0 <__gmpn_sub_n@plt>
   3dfec:	ldur	x9, [x29, #-8]
   3dff0:	sub	x8, x26, x0
   3dff4:	str	x8, [x28, x9, lsl #3]
   3dff8:	ldur	x21, [x29, #-16]
   3dffc:	ldr	x1, [sp, #40]
   3e000:	mov	x0, x25
   3e004:	mov	x2, x20
   3e008:	mov	x3, x21
   3e00c:	lsl	x26, x9, #2
   3e010:	bl	ca70 <__gmpn_add_n@plt>
   3e014:	ldur	x16, [x29, #-8]
   3e018:	subs	x11, x16, x21
   3e01c:	b.eq	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3e020:	ldur	x8, [x29, #-16]
   3e024:	lsl	x8, x8, #3
   3e028:	ldr	x9, [x20, x8]
   3e02c:	adds	x9, x9, x0
   3e030:	str	x9, [x25, x8]
   3e034:	b.cc	3e1a4 <__gmpn_toom33_mul@@Base+0x7f4>  // b.lo, b.ul, b.last
   3e038:	ldr	x10, [sp, #32]
   3e03c:	add	x9, x16, x16, lsl #1
   3e040:	mov	w0, #0x1                   	// #1
   3e044:	lsl	x8, x10, #3
   3e048:	mvn	x10, x10
   3e04c:	add	x12, x8, #0x18
   3e050:	sub	x8, x8, x16, lsl #4
   3e054:	add	x10, x10, x9
   3e058:	add	x13, x8, #0x8
   3e05c:	mov	w8, #0x1                   	// #1
   3e060:	cmp	x8, x11
   3e064:	b.ge	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.tcont
   3e068:	ldr	x14, [x20, x13]
   3e06c:	add	x8, x8, #0x1
   3e070:	add	x13, x13, #0x8
   3e074:	sub	x10, x10, #0x1
   3e078:	adds	x14, x14, #0x1
   3e07c:	str	x14, [x20, x12]
   3e080:	add	x12, x12, #0x8
   3e084:	b.cs	3e060 <__gmpn_toom33_mul@@Base+0x6b0>  // b.hs, b.nlast
   3e088:	cmp	x25, x20
   3e08c:	mov	x0, xzr
   3e090:	b.eq	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3e094:	cmp	x8, x11
   3e098:	b.ge	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.tcont
   3e09c:	ldr	x14, [sp, #32]
   3e0a0:	add	x11, x16, x16, lsl #1
   3e0a4:	sub	x14, x11, x14
   3e0a8:	sub	x14, x14, x8
   3e0ac:	cmp	x14, #0x4
   3e0b0:	b.cc	3e11c <__gmpn_toom33_mul@@Base+0x76c>  // b.lo, b.ul, b.last
   3e0b4:	ldr	x17, [sp, #8]
   3e0b8:	add	x15, x20, x12
   3e0bc:	cmp	x15, x17
   3e0c0:	b.cs	3e0dc <__gmpn_toom33_mul@@Base+0x72c>  // b.hs, b.nlast
   3e0c4:	mov	w15, #0x18                  	// #24
   3e0c8:	madd	x15, x16, x15, x20
   3e0cc:	add	x15, x15, #0x10
   3e0d0:	add	x13, x20, x13
   3e0d4:	cmp	x13, x15
   3e0d8:	b.cc	3e11c <__gmpn_toom33_mul@@Base+0x76c>  // b.lo, b.ul, b.last
   3e0dc:	ldr	x13, [sp, #32]
   3e0e0:	and	x10, x10, #0xfffffffffffffffc
   3e0e4:	add	x12, x20, x12
   3e0e8:	sub	x13, x9, x13
   3e0ec:	sub	x13, x13, x8
   3e0f0:	and	x9, x14, #0xfffffffffffffffc
   3e0f4:	add	x8, x10, x8
   3e0f8:	and	x10, x13, #0xfffffffffffffffc
   3e0fc:	neg	x13, x16, lsl #4
   3e100:	add	x15, x12, x13
   3e104:	ldp	q0, q1, [x15, #-16]
   3e108:	subs	x10, x10, #0x4
   3e10c:	stp	q0, q1, [x12], #32
   3e110:	b.ne	3e100 <__gmpn_toom33_mul@@Base+0x750>  // b.any
   3e114:	cmp	x14, x9
   3e118:	b.eq	3e234 <__gmpn_toom33_mul@@Base+0x884>  // b.none
   3e11c:	sub	x9, x11, x8
   3e120:	ldr	x11, [sp, #32]
   3e124:	add	x10, x8, x11
   3e128:	sub	x8, x9, x11
   3e12c:	add	x9, x20, x10, lsl #3
   3e130:	neg	x10, x16, lsl #4
   3e134:	ldr	x11, [x9, x10]
   3e138:	subs	x8, x8, #0x1
   3e13c:	str	x11, [x9, #16]
   3e140:	add	x9, x9, #0x8
   3e144:	b.ne	3e134 <__gmpn_toom33_mul@@Base+0x784>  // b.any
   3e148:	b	3e234 <__gmpn_toom33_mul@@Base+0x884>
   3e14c:	add	x8, x22, x10, lsl #4
   3e150:	sub	x8, x8, #0x8
   3e154:	subs	x9, x10, #0x1
   3e158:	b.lt	3dfd8 <__gmpn_toom33_mul@@Base+0x628>  // b.tstop
   3e15c:	add	x10, x19, x10, lsl #3
   3e160:	ldur	x10, [x10, #-8]
   3e164:	ldr	x11, [x8], #-8
   3e168:	cmp	x10, x11
   3e16c:	mov	x10, x9
   3e170:	b.eq	3e154 <__gmpn_toom33_mul@@Base+0x7a4>  // b.none
   3e174:	b.hi	3dfd8 <__gmpn_toom33_mul@@Base+0x628>  // b.pmore
   3e178:	ldur	x3, [x29, #-8]
   3e17c:	mov	x0, x28
   3e180:	mov	x1, x24
   3e184:	mov	x2, x19
   3e188:	bl	c2d0 <__gmpn_sub_n@plt>
   3e18c:	ldr	w8, [sp, #20]
   3e190:	ldur	x9, [x29, #-8]
   3e194:	eor	w8, w8, #0x1
   3e198:	str	xzr, [x28, x9, lsl #3]
   3e19c:	str	w8, [sp, #20]
   3e1a0:	b	3dff8 <__gmpn_toom33_mul@@Base+0x648>
   3e1a4:	cmp	x25, x20
   3e1a8:	mov	x0, xzr
   3e1ac:	b.eq	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3e1b0:	cmp	x11, #0x2
   3e1b4:	b.lt	3e238 <__gmpn_toom33_mul@@Base+0x888>  // b.tstop
   3e1b8:	ldr	x12, [sp, #32]
   3e1bc:	add	x8, x16, x16, lsl #1
   3e1c0:	mvn	x9, x12
   3e1c4:	add	x9, x8, x9
   3e1c8:	cmp	x9, #0x4
   3e1cc:	b.cc	3e204 <__gmpn_toom33_mul@@Base+0x854>  // b.lo, b.ul, b.last
   3e1d0:	ldr	x11, [sp, #8]
   3e1d4:	add	x10, x20, x12, lsl #3
   3e1d8:	add	x10, x10, #0x18
   3e1dc:	cmp	x10, x11
   3e1e0:	b.cs	3e380 <__gmpn_toom33_mul@@Base+0x9d0>  // b.hs, b.nlast
   3e1e4:	mov	w11, #0x18                  	// #24
   3e1e8:	sub	x12, x12, x16, lsl #1
   3e1ec:	madd	x11, x16, x11, x20
   3e1f0:	add	x12, x20, x12, lsl #3
   3e1f4:	add	x11, x11, #0x10
   3e1f8:	add	x12, x12, #0x8
   3e1fc:	cmp	x12, x11
   3e200:	b.cs	3e380 <__gmpn_toom33_mul@@Base+0x9d0>  // b.hs, b.nlast
   3e204:	mov	w11, #0x1                   	// #1
   3e208:	ldr	x10, [sp, #32]
   3e20c:	sub	x8, x8, x11
   3e210:	add	x9, x11, x10
   3e214:	sub	x8, x8, x10
   3e218:	add	x9, x20, x9, lsl #3
   3e21c:	neg	x10, x16, lsl #4
   3e220:	ldr	x11, [x9, x10]
   3e224:	subs	x8, x8, #0x1
   3e228:	str	x11, [x9, #16]
   3e22c:	add	x9, x9, #0x8
   3e230:	b.ne	3e220 <__gmpn_toom33_mul@@Base+0x870>  // b.any
   3e234:	mov	x0, xzr
   3e238:	ldr	x8, [x20, x27]
   3e23c:	mov	x1, x22
   3e240:	mov	x2, x25
   3e244:	mov	x3, x16
   3e248:	add	x24, x8, x0
   3e24c:	mov	x0, x25
   3e250:	str	x23, [sp, #8]
   3e254:	mov	x21, x16
   3e258:	bl	d090 <__gmpn_rsblsh1_n@plt>
   3e25c:	add	x8, x0, x24, lsl #1
   3e260:	mov	w9, #0x28                  	// #40
   3e264:	ldr	x1, [sp, #48]
   3e268:	str	x8, [x25, x27]
   3e26c:	madd	x8, x21, x9, x19
   3e270:	add	x24, x21, #0x1
   3e274:	add	x27, x8, #0x28
   3e278:	mov	x0, x19
   3e27c:	mov	x2, x24
   3e280:	mov	x3, x28
   3e284:	mov	x4, x24
   3e288:	mov	x5, x27
   3e28c:	bl	d450 <__gmpn_toom22_mul@plt>
   3e290:	ldp	x8, x1, [sp, #56]
   3e294:	mov	x2, x24
   3e298:	mov	x3, x25
   3e29c:	mov	x4, x24
   3e2a0:	add	x28, x8, #0x8
   3e2a4:	mov	x0, x28
   3e2a8:	mov	x5, x27
   3e2ac:	bl	d450 <__gmpn_toom22_mul@plt>
   3e2b0:	ldp	x9, x8, [sp, #24]
   3e2b4:	add	x25, x20, x26, lsl #3
   3e2b8:	mov	x23, x20
   3e2bc:	mov	x0, x25
   3e2c0:	cmp	x9, x8
   3e2c4:	b.le	3e2e4 <__gmpn_toom33_mul@@Base+0x934>
   3e2c8:	ldp	x1, x19, [x29, #-56]
   3e2cc:	ldur	x21, [x29, #-16]
   3e2d0:	ldr	x3, [sp, #40]
   3e2d4:	mov	x2, x19
   3e2d8:	mov	x4, x21
   3e2dc:	bl	ccd0 <__gmpn_mul@plt>
   3e2e0:	b	3e300 <__gmpn_toom33_mul@@Base+0x950>
   3e2e4:	ldp	x1, x19, [x29, #-56]
   3e2e8:	ldr	x3, [sp, #40]
   3e2ec:	mov	x5, x27
   3e2f0:	mov	x2, x19
   3e2f4:	mov	x4, x19
   3e2f8:	bl	d450 <__gmpn_toom22_mul@plt>
   3e2fc:	ldur	x21, [x29, #-16]
   3e300:	ldr	x0, [sp]
   3e304:	ldur	x1, [x29, #-24]
   3e308:	ldp	x26, x22, [x25]
   3e30c:	mov	x2, x24
   3e310:	mov	x3, x23
   3e314:	mov	x4, x24
   3e318:	mov	x5, x27
   3e31c:	bl	d450 <__gmpn_toom22_mul@plt>
   3e320:	str	x22, [x25, #8]
   3e324:	ldur	x22, [x29, #-8]
   3e328:	ldr	x1, [sp, #8]
   3e32c:	ldur	x3, [x29, #-40]
   3e330:	mov	x0, x23
   3e334:	mov	x2, x22
   3e338:	mov	x4, x22
   3e33c:	mov	x5, x27
   3e340:	bl	d450 <__gmpn_toom22_mul@plt>
   3e344:	add	x4, x19, x21
   3e348:	mov	x0, x23
   3e34c:	mov	x1, x28
   3e350:	ldur	x2, [x29, #-32]
   3e354:	mov	x3, x22
   3e358:	ldr	w5, [sp, #20]
   3e35c:	mov	x6, x26
   3e360:	ldp	x20, x19, [sp, #208]
   3e364:	ldp	x22, x21, [sp, #192]
   3e368:	ldp	x24, x23, [sp, #176]
   3e36c:	ldp	x26, x25, [sp, #160]
   3e370:	ldp	x28, x27, [sp, #144]
   3e374:	ldp	x29, x30, [sp, #128]
   3e378:	add	sp, sp, #0xe0
   3e37c:	b	ca20 <__gmpn_toom_interpolate_5pts@plt>
   3e380:	and	x12, x9, #0xfffffffffffffffc
   3e384:	orr	x11, x12, #0x1
   3e388:	neg	x13, x16, lsl #4
   3e38c:	mov	x14, x12
   3e390:	add	x15, x10, x13
   3e394:	ldp	q0, q1, [x15, #-16]
   3e398:	subs	x14, x14, #0x4
   3e39c:	stp	q0, q1, [x10], #32
   3e3a0:	b.ne	3e390 <__gmpn_toom33_mul@@Base+0x9e0>  // b.any
   3e3a4:	cmp	x9, x12
   3e3a8:	b.eq	3e234 <__gmpn_toom33_mul@@Base+0x884>  // b.none
   3e3ac:	b	3e208 <__gmpn_toom33_mul@@Base+0x858>

000000000003e3b0 <__gmpn_toom43_mul@@Base>:
   3e3b0:	sub	sp, sp, #0xf0
   3e3b4:	add	x8, x2, x2, lsl #1
   3e3b8:	stp	x28, x27, [sp, #160]
   3e3bc:	stp	x22, x21, [sp, #208]
   3e3c0:	stp	x20, x19, [sp, #224]
   3e3c4:	mov	x19, x5
   3e3c8:	mov	x27, x4
   3e3cc:	mov	x20, x3
   3e3d0:	mov	x10, x1
   3e3d4:	cmp	x8, x4, lsl #2
   3e3d8:	mov	x21, x0
   3e3dc:	stp	x29, x30, [sp, #144]
   3e3e0:	stp	x26, x25, [sp, #176]
   3e3e4:	stp	x24, x23, [sp, #192]
   3e3e8:	add	x29, sp, #0x90
   3e3ec:	b.ge	3e408 <__gmpn_toom43_mul@@Base+0x58>  // b.tcont
   3e3f0:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3e3f4:	sub	x8, x27, #0x1
   3e3f8:	movk	x9, #0xaaab
   3e3fc:	umulh	x8, x8, x9
   3e400:	lsr	x25, x8, #1
   3e404:	b	3e410 <__gmpn_toom43_mul@@Base+0x60>
   3e408:	sub	x8, x2, #0x1
   3e40c:	asr	x25, x8, #2
   3e410:	add	x23, x25, #0x1
   3e414:	lsl	x11, x23, #1
   3e418:	add	x9, x19, x23, lsl #5
   3e41c:	add	x8, x11, x23
   3e420:	str	x11, [sp, #40]
   3e424:	str	x9, [sp, #64]
   3e428:	sub	x4, x2, x8
   3e42c:	stur	x8, [x29, #-64]
   3e430:	lsl	x8, x8, #3
   3e434:	add	x1, x9, #0x20
   3e438:	add	x9, x21, x8
   3e43c:	add	x8, x19, x8
   3e440:	add	x0, x9, #0x18
   3e444:	add	x5, x8, #0x18
   3e448:	mov	x2, x10
   3e44c:	mov	x3, x23
   3e450:	sub	x24, x27, x11
   3e454:	str	x0, [sp, #72]
   3e458:	str	x1, [sp, #56]
   3e45c:	stp	x4, x10, [x29, #-16]
   3e460:	stur	x5, [x29, #-24]
   3e464:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3e468:	and	w8, w0, #0x2
   3e46c:	lsl	x22, x23, #4
   3e470:	stur	w8, [x29, #-48]
   3e474:	add	x8, x19, x22
   3e478:	lsl	x26, x23, #3
   3e47c:	add	x28, x8, #0x10
   3e480:	add	x1, x20, x26
   3e484:	mov	w3, #0x1                   	// #1
   3e488:	mov	x0, x28
   3e48c:	mov	x2, x23
   3e490:	str	x8, [sp, #48]
   3e494:	stur	x1, [x29, #-32]
   3e498:	bl	c180 <__gmpn_lshift@plt>
   3e49c:	str	x0, [x28, x26]
   3e4a0:	add	x1, x20, x22
   3e4a4:	mov	w3, #0x2                   	// #2
   3e4a8:	mov	x0, x19
   3e4ac:	mov	x2, x24
   3e4b0:	stur	x26, [x29, #-56]
   3e4b4:	stur	x1, [x29, #-40]
   3e4b8:	bl	c180 <__gmpn_lshift@plt>
   3e4bc:	mov	x22, x0
   3e4c0:	mov	x0, x19
   3e4c4:	mov	x1, x19
   3e4c8:	mov	x2, x20
   3e4cc:	mov	x3, x24
   3e4d0:	bl	ca70 <__gmpn_add_n@plt>
   3e4d4:	subs	x15, x23, x24
   3e4d8:	add	x9, x0, x22
   3e4dc:	b.eq	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3e4e0:	lsl	x8, x24, #3
   3e4e4:	ldr	x10, [x20, x8]
   3e4e8:	adds	x9, x10, x9
   3e4ec:	str	x9, [x19, x8]
   3e4f0:	b.cc	3e628 <__gmpn_toom43_mul@@Base+0x278>  // b.lo, b.ul, b.last
   3e4f4:	add	x9, x25, x25, lsl #1
   3e4f8:	lsl	x10, x25, #4
   3e4fc:	sub	x11, x9, x27
   3e500:	mov	x8, xzr
   3e504:	lsl	x12, x27, #3
   3e508:	sub	x14, x19, x10
   3e50c:	sub	x13, x20, x10
   3e510:	add	x10, x11, #0x2
   3e514:	mov	w9, #0x1                   	// #1
   3e518:	add	x8, x8, #0x1
   3e51c:	cmp	x8, x15
   3e520:	b.ge	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.tcont
   3e524:	add	x16, x13, x12
   3e528:	ldur	x16, [x16, #-8]
   3e52c:	add	x17, x14, x12
   3e530:	add	x14, x14, #0x8
   3e534:	add	x13, x13, #0x8
   3e538:	adds	x16, x16, #0x1
   3e53c:	sub	x10, x10, #0x1
   3e540:	stur	x16, [x17, #-8]
   3e544:	b.cs	3e518 <__gmpn_toom43_mul@@Base+0x168>  // b.hs, b.nlast
   3e548:	cmp	x20, x19
   3e54c:	mov	x9, xzr
   3e550:	b.eq	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3e554:	add	x16, x8, #0x1
   3e558:	cmp	x16, x15
   3e55c:	b.ge	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.tcont
   3e560:	add	x9, x25, x25, lsl #1
   3e564:	sub	x15, x9, x27
   3e568:	sub	x15, x15, x8
   3e56c:	add	x15, x15, #0x2
   3e570:	cmp	x15, #0x4
   3e574:	b.cc	3e5f4 <__gmpn_toom43_mul@@Base+0x244>  // b.lo, b.ul, b.last
   3e578:	ldur	x18, [x29, #-56]
   3e57c:	add	x17, x14, x12
   3e580:	sub	x17, x17, #0x8
   3e584:	add	x18, x20, x18
   3e588:	cmp	x17, x18
   3e58c:	b.cs	3e5a8 <__gmpn_toom43_mul@@Base+0x1f8>  // b.hs, b.nlast
   3e590:	ldur	x17, [x29, #-56]
   3e594:	add	x18, x13, x12
   3e598:	sub	x18, x18, #0x8
   3e59c:	add	x17, x19, x17
   3e5a0:	cmp	x18, x17
   3e5a4:	b.cc	3e5f4 <__gmpn_toom43_mul@@Base+0x244>  // b.lo, b.ul, b.last
   3e5a8:	add	x14, x14, x12
   3e5ac:	add	x12, x13, x12
   3e5b0:	sub	x13, x11, x8
   3e5b4:	and	x16, x10, #0xfffffffffffffffc
   3e5b8:	add	x13, x13, #0x2
   3e5bc:	add	x8, x16, x8
   3e5c0:	and	x11, x15, #0xfffffffffffffffc
   3e5c4:	add	x10, x14, #0x8
   3e5c8:	add	x12, x12, #0x8
   3e5cc:	add	x16, x8, #0x1
   3e5d0:	and	x8, x13, #0xfffffffffffffffc
   3e5d4:	ldp	q0, q1, [x12, #-16]
   3e5d8:	subs	x8, x8, #0x4
   3e5dc:	add	x12, x12, #0x20
   3e5e0:	stp	q0, q1, [x10, #-16]
   3e5e4:	add	x10, x10, #0x20
   3e5e8:	b.ne	3e5d4 <__gmpn_toom43_mul@@Base+0x224>  // b.any
   3e5ec:	cmp	x15, x11
   3e5f0:	b.eq	3e6b8 <__gmpn_toom43_mul@@Base+0x308>  // b.none
   3e5f4:	add	x8, x16, x27
   3e5f8:	sub	x9, x8, x9
   3e5fc:	sub	x10, x8, x25, lsl #1
   3e600:	sub	x8, x9, #0x3
   3e604:	lsl	x9, x10, #3
   3e608:	sub	x10, x9, #0x10
   3e60c:	add	x9, x19, x10
   3e610:	add	x10, x20, x10
   3e614:	ldr	x11, [x10], #8
   3e618:	adds	x8, x8, #0x1
   3e61c:	str	x11, [x9], #8
   3e620:	b.cc	3e614 <__gmpn_toom43_mul@@Base+0x264>  // b.lo, b.ul, b.last
   3e624:	b	3e6b8 <__gmpn_toom43_mul@@Base+0x308>
   3e628:	cmp	x20, x19
   3e62c:	mov	x9, xzr
   3e630:	b.eq	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3e634:	cmp	x15, #0x2
   3e638:	b.lt	3e6bc <__gmpn_toom43_mul@@Base+0x30c>  // b.tstop
   3e63c:	add	x8, x25, x25, lsl #1
   3e640:	sub	x9, x8, x27
   3e644:	add	x9, x9, #0x2
   3e648:	cmp	x9, #0x4
   3e64c:	b.cc	3e684 <__gmpn_toom43_mul@@Base+0x2d4>  // b.lo, b.ul, b.last
   3e650:	mov	x10, #0xffffffffffffffff    	// #-1
   3e654:	ldur	x13, [x29, #-56]
   3e658:	eor	x10, x10, x25, lsl #1
   3e65c:	add	x10, x10, x27
   3e660:	lsl	x10, x10, #3
   3e664:	add	x11, x19, x10
   3e668:	add	x12, x20, x13
   3e66c:	cmp	x11, x12
   3e670:	b.cs	3ea44 <__gmpn_toom43_mul@@Base+0x694>  // b.hs, b.nlast
   3e674:	add	x11, x19, x13
   3e678:	add	x10, x20, x10
   3e67c:	cmp	x10, x11
   3e680:	b.cs	3ea44 <__gmpn_toom43_mul@@Base+0x694>  // b.hs, b.nlast
   3e684:	mov	w10, #0x1                   	// #1
   3e688:	add	x9, x10, x27
   3e68c:	sub	x8, x9, x8
   3e690:	sub	x9, x9, x25, lsl #1
   3e694:	lsl	x9, x9, #3
   3e698:	sub	x10, x9, #0x10
   3e69c:	sub	x8, x8, #0x3
   3e6a0:	add	x9, x19, x10
   3e6a4:	add	x10, x20, x10
   3e6a8:	ldr	x11, [x10], #8
   3e6ac:	adds	x8, x8, #0x1
   3e6b0:	str	x11, [x9], #8
   3e6b4:	b.cc	3e6a8 <__gmpn_toom43_mul@@Base+0x2f8>  // b.lo, b.ul, b.last
   3e6b8:	mov	x9, xzr
   3e6bc:	ldr	x8, [sp, #40]
   3e6c0:	add	x22, x25, #0x2
   3e6c4:	mov	x1, x19
   3e6c8:	mov	x2, x28
   3e6cc:	add	x8, x21, x8, lsl #3
   3e6d0:	add	x0, x8, #0x10
   3e6d4:	mov	x3, x22
   3e6d8:	str	x27, [sp, #8]
   3e6dc:	lsl	x27, x23, #2
   3e6e0:	str	x9, [x19, x23, lsl #3]
   3e6e4:	stp	x0, x8, [sp, #32]
   3e6e8:	bl	ca70 <__gmpn_add_n@plt>
   3e6ec:	mov	w8, #0x18                  	// #24
   3e6f0:	mul	x8, x25, x8
   3e6f4:	add	x8, x8, #0x28
   3e6f8:	mov	x9, x23
   3e6fc:	add	x10, x9, #0x1
   3e700:	cmp	x10, #0x1
   3e704:	b.lt	3e724 <__gmpn_toom43_mul@@Base+0x374>  // b.tstop
   3e708:	ldr	x10, [x19, x9, lsl #3]
   3e70c:	ldr	x11, [x19, x8]
   3e710:	sub	x9, x9, #0x1
   3e714:	sub	x8, x8, #0x8
   3e718:	cmp	x10, x11
   3e71c:	b.eq	3e6fc <__gmpn_toom43_mul@@Base+0x34c>  // b.none
   3e720:	b.ls	3e744 <__gmpn_toom43_mul@@Base+0x394>  // b.plast
   3e724:	add	x8, x21, x23, lsl #3
   3e728:	add	x0, x8, #0x8
   3e72c:	mov	x1, x19
   3e730:	mov	x2, x28
   3e734:	mov	x3, x22
   3e738:	str	x0, [sp, #24]
   3e73c:	bl	c2d0 <__gmpn_sub_n@plt>
   3e740:	b	3e76c <__gmpn_toom43_mul@@Base+0x3bc>
   3e744:	add	x8, x21, x23, lsl #3
   3e748:	add	x0, x8, #0x8
   3e74c:	mov	x1, x28
   3e750:	mov	x2, x19
   3e754:	mov	x3, x22
   3e758:	str	x0, [sp, #24]
   3e75c:	bl	c2d0 <__gmpn_sub_n@plt>
   3e760:	ldur	w8, [x29, #-48]
   3e764:	eor	w8, w8, #0x2
   3e768:	stur	w8, [x29, #-48]
   3e76c:	add	x8, x21, x27, lsl #3
   3e770:	add	x0, x8, #0x20
   3e774:	str	x0, [sp, #16]
   3e778:	ldp	x1, x4, [x29, #-24]
   3e77c:	ldur	x2, [x29, #-8]
   3e780:	mov	x3, x23
   3e784:	mov	x5, x19
   3e788:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3e78c:	and	w26, w0, #0x1
   3e790:	cbz	x24, 3e7e4 <__gmpn_toom43_mul@@Base+0x434>
   3e794:	ldur	x2, [x29, #-40]
   3e798:	mov	x0, x28
   3e79c:	mov	x1, x20
   3e7a0:	mov	x3, x24
   3e7a4:	bl	ca70 <__gmpn_add_n@plt>
   3e7a8:	mov	x8, x24
   3e7ac:	cbz	x0, 3e7e8 <__gmpn_toom43_mul@@Base+0x438>
   3e7b0:	ldr	x8, [sp, #8]
   3e7b4:	mov	w27, #0x1                   	// #1
   3e7b8:	add	x8, x19, x8, lsl #3
   3e7bc:	add	x9, x8, #0x10
   3e7c0:	mov	x8, x24
   3e7c4:	cmp	x8, x25
   3e7c8:	b.gt	3e8a4 <__gmpn_toom43_mul@@Base+0x4f4>
   3e7cc:	ldr	x10, [x20, x8, lsl #3]
   3e7d0:	add	x8, x8, #0x1
   3e7d4:	adds	x10, x10, #0x1
   3e7d8:	str	x10, [x9], #8
   3e7dc:	b.cs	3e7c4 <__gmpn_toom43_mul@@Base+0x414>  // b.hs, b.nlast
   3e7e0:	b	3e7e8 <__gmpn_toom43_mul@@Base+0x438>
   3e7e4:	mov	x8, xzr
   3e7e8:	cmp	x28, x20
   3e7ec:	mov	x27, xzr
   3e7f0:	b.eq	3e8a4 <__gmpn_toom43_mul@@Base+0x4f4>  // b.none
   3e7f4:	ldur	w14, [x29, #-48]
   3e7f8:	cmp	x8, x25
   3e7fc:	b.gt	3e8a8 <__gmpn_toom43_mul@@Base+0x4f8>
   3e800:	sub	x9, x25, x8
   3e804:	add	x9, x9, #0x1
   3e808:	cmp	x9, #0x4
   3e80c:	b.cc	3e874 <__gmpn_toom43_mul@@Base+0x4c4>  // b.lo, b.ul, b.last
   3e810:	add	x10, x8, x25, lsl #1
   3e814:	add	x12, x19, x10, lsl #3
   3e818:	add	x10, x12, #0x20
   3e81c:	add	x11, x20, x23, lsl #3
   3e820:	cmp	x10, x11
   3e824:	add	x11, x20, x8, lsl #3
   3e828:	b.cs	3e840 <__gmpn_toom43_mul@@Base+0x490>  // b.hs, b.nlast
   3e82c:	mov	w10, #0x18                  	// #24
   3e830:	madd	x10, x25, x10, x19
   3e834:	add	x10, x10, #0x28
   3e838:	cmp	x11, x10
   3e83c:	b.cc	3e874 <__gmpn_toom43_mul@@Base+0x4c4>  // b.lo, b.ul, b.last
   3e840:	and	x10, x9, #0xfffffffffffffffc
   3e844:	add	x11, x11, #0x10
   3e848:	add	x8, x8, x10
   3e84c:	add	x12, x12, #0x30
   3e850:	mov	x13, x10
   3e854:	ldp	q0, q1, [x11, #-16]
   3e858:	subs	x13, x13, #0x4
   3e85c:	add	x11, x11, #0x20
   3e860:	stp	q0, q1, [x12, #-16]
   3e864:	add	x12, x12, #0x20
   3e868:	b.ne	3e854 <__gmpn_toom43_mul@@Base+0x4a4>  // b.any
   3e86c:	cmp	x9, x10
   3e870:	b.eq	3e89c <__gmpn_toom43_mul@@Base+0x4ec>  // b.none
   3e874:	add	x10, x8, x25, lsl #1
   3e878:	sub	x9, x25, x8
   3e87c:	add	x10, x19, x10, lsl #3
   3e880:	add	x9, x9, #0x1
   3e884:	add	x10, x10, #0x20
   3e888:	add	x8, x20, x8, lsl #3
   3e88c:	ldr	x11, [x8], #8
   3e890:	subs	x9, x9, #0x1
   3e894:	str	x11, [x10], #8
   3e898:	b.ne	3e88c <__gmpn_toom43_mul@@Base+0x4dc>  // b.any
   3e89c:	mov	x27, xzr
   3e8a0:	b	3e8a8 <__gmpn_toom43_mul@@Base+0x4f8>
   3e8a4:	ldur	w14, [x29, #-48]
   3e8a8:	stur	x24, [x29, #-48]
   3e8ac:	eor	w24, w26, w14
   3e8b0:	ldur	x26, [x29, #-56]
   3e8b4:	ldur	x2, [x29, #-32]
   3e8b8:	mov	x0, x21
   3e8bc:	mov	x1, x28
   3e8c0:	mov	x3, x23
   3e8c4:	str	x27, [x28, x26]
   3e8c8:	bl	ca70 <__gmpn_add_n@plt>
   3e8cc:	add	x8, x0, x27
   3e8d0:	str	x8, [x21, x26]
   3e8d4:	ldr	x8, [x28, x26]
   3e8d8:	str	x21, [sp, #8]
   3e8dc:	cbz	x8, 3e9f0 <__gmpn_toom43_mul@@Base+0x640>
   3e8e0:	ldur	x2, [x29, #-32]
   3e8e4:	mov	x0, x28
   3e8e8:	mov	x1, x28
   3e8ec:	mov	x3, x23
   3e8f0:	bl	c2d0 <__gmpn_sub_n@plt>
   3e8f4:	ldr	x8, [x28, x26]
   3e8f8:	mov	w27, w24
   3e8fc:	sub	x8, x8, x0
   3e900:	str	x8, [x28, x26]
   3e904:	ldur	x1, [x29, #-24]
   3e908:	mov	x0, x19
   3e90c:	mov	x2, x28
   3e910:	mov	x3, x22
   3e914:	bl	c990 <__gmpn_mul_n@plt>
   3e918:	ldp	x8, x1, [sp, #48]
   3e91c:	ldr	x2, [sp, #24]
   3e920:	mov	x3, x22
   3e924:	add	x28, x8, #0x8
   3e928:	mov	x0, x28
   3e92c:	bl	c990 <__gmpn_mul_n@plt>
   3e930:	ldp	x8, x1, [sp, #64]
   3e934:	ldr	x2, [sp, #32]
   3e938:	mov	x3, x22
   3e93c:	add	x26, x8, #0x10
   3e940:	mov	x0, x26
   3e944:	bl	c990 <__gmpn_mul_n@plt>
   3e948:	ldp	x21, x1, [sp, #8]
   3e94c:	ldr	x0, [sp, #40]
   3e950:	mov	x3, x22
   3e954:	mov	x2, x21
   3e958:	bl	c990 <__gmpn_mul_n@plt>
   3e95c:	mov	w8, #0x28                  	// #40
   3e960:	ldp	x25, x22, [x29, #-16]
   3e964:	ldur	x24, [x29, #-48]
   3e968:	madd	x0, x23, x8, x21
   3e96c:	ldur	x8, [x29, #-64]
   3e970:	cmp	x25, x24
   3e974:	add	x3, x22, x8, lsl #3
   3e978:	b.le	3e990 <__gmpn_toom43_mul@@Base+0x5e0>
   3e97c:	mov	x1, x3
   3e980:	ldur	x3, [x29, #-40]
   3e984:	mov	x2, x25
   3e988:	mov	x4, x24
   3e98c:	b	3e99c <__gmpn_toom43_mul@@Base+0x5ec>
   3e990:	ldur	x1, [x29, #-40]
   3e994:	mov	x2, x24
   3e998:	mov	x4, x25
   3e99c:	bl	ccd0 <__gmpn_mul@plt>
   3e9a0:	mov	x0, x21
   3e9a4:	mov	x1, x22
   3e9a8:	mov	x2, x20
   3e9ac:	mov	x3, x23
   3e9b0:	bl	c990 <__gmpn_mul_n@plt>
   3e9b4:	add	x6, x24, x25
   3e9b8:	mov	x0, x21
   3e9bc:	mov	x1, x23
   3e9c0:	mov	w2, w27
   3e9c4:	mov	x3, x19
   3e9c8:	mov	x4, x28
   3e9cc:	mov	x5, x26
   3e9d0:	ldp	x20, x19, [sp, #224]
   3e9d4:	ldp	x22, x21, [sp, #208]
   3e9d8:	ldp	x24, x23, [sp, #192]
   3e9dc:	ldp	x26, x25, [sp, #176]
   3e9e0:	ldp	x28, x27, [sp, #160]
   3e9e4:	ldp	x29, x30, [sp, #144]
   3e9e8:	add	sp, sp, #0xf0
   3e9ec:	b	c910 <__gmpn_toom_interpolate_6pts@plt>
   3e9f0:	mov	w9, #0x18                  	// #24
   3e9f4:	add	x8, x20, x25, lsl #4
   3e9f8:	madd	x9, x25, x9, x19
   3e9fc:	add	x8, x8, #0x8
   3ea00:	add	x9, x9, #0x20
   3ea04:	mov	x10, x23
   3ea08:	subs	x10, x10, #0x1
   3ea0c:	b.lt	3e8e0 <__gmpn_toom43_mul@@Base+0x530>  // b.tstop
   3ea10:	ldr	x11, [x9], #-8
   3ea14:	ldr	x12, [x8], #-8
   3ea18:	cmp	x11, x12
   3ea1c:	b.eq	3ea08 <__gmpn_toom43_mul@@Base+0x658>  // b.none
   3ea20:	b.hi	3e8e0 <__gmpn_toom43_mul@@Base+0x530>  // b.pmore
   3ea24:	ldur	x1, [x29, #-32]
   3ea28:	mov	x0, x28
   3ea2c:	mov	x2, x28
   3ea30:	mov	x3, x23
   3ea34:	bl	c2d0 <__gmpn_sub_n@plt>
   3ea38:	mov	w27, w24
   3ea3c:	eor	w27, w24, #0x1
   3ea40:	b	3e904 <__gmpn_toom43_mul@@Base+0x554>
   3ea44:	sub	x12, x27, x25, lsl #1
   3ea48:	lsl	x12, x12, #3
   3ea4c:	and	x11, x9, #0xfffffffffffffffc
   3ea50:	add	x13, x12, #0x8
   3ea54:	orr	x10, x11, #0x1
   3ea58:	add	x12, x20, x13
   3ea5c:	add	x13, x19, x13
   3ea60:	mov	x14, x11
   3ea64:	ldp	q0, q1, [x12, #-16]
   3ea68:	subs	x14, x14, #0x4
   3ea6c:	add	x12, x12, #0x20
   3ea70:	stp	q0, q1, [x13, #-16]
   3ea74:	add	x13, x13, #0x20
   3ea78:	b.ne	3ea64 <__gmpn_toom43_mul@@Base+0x6b4>  // b.any
   3ea7c:	cmp	x9, x11
   3ea80:	b.eq	3e6b8 <__gmpn_toom43_mul@@Base+0x308>  // b.none
   3ea84:	b	3e688 <__gmpn_toom43_mul@@Base+0x2d8>

000000000003ea88 <__gmpn_toom53_mul@@Base>:
   3ea88:	stp	x29, x30, [sp, #-96]!
   3ea8c:	stp	x28, x27, [sp, #16]
   3ea90:	stp	x26, x25, [sp, #32]
   3ea94:	stp	x24, x23, [sp, #48]
   3ea98:	stp	x22, x21, [sp, #64]
   3ea9c:	stp	x20, x19, [sp, #80]
   3eaa0:	mov	x29, sp
   3eaa4:	sub	sp, sp, #0xb0
   3eaa8:	add	x8, x2, x2, lsl #1
   3eaac:	add	x9, x4, x4, lsl #2
   3eab0:	cmp	x8, x9
   3eab4:	mov	w10, #0x5                   	// #5
   3eab8:	mov	w11, #0x3                   	// #3
   3eabc:	csel	x8, x4, x2, lt  // lt = tstop
   3eac0:	csel	x9, x11, x10, lt  // lt = tstop
   3eac4:	sub	x8, x8, #0x1
   3eac8:	udiv	x11, x8, x9
   3eacc:	add	x19, x11, #0x2
   3ead0:	add	x21, x11, #0x1
   3ead4:	add	x8, x19, x19, lsl #2
   3ead8:	mov	x25, x1
   3eadc:	lsl	x9, x21, #2
   3eae0:	lsl	x10, x21, #1
   3eae4:	lsl	x1, x8, #4
   3eae8:	mov	w8, #0x7f00                	// #32512
   3eaec:	mov	x20, x0
   3eaf0:	stur	x9, [x29, #-88]
   3eaf4:	sub	x23, x2, x9
   3eaf8:	sub	x9, x4, x10
   3eafc:	cmp	x1, x8
   3eb00:	stur	x5, [x29, #-72]
   3eb04:	stp	x3, xzr, [x29, #-24]
   3eb08:	stp	x2, x10, [x29, #-56]
   3eb0c:	stp	x4, x11, [x29, #-176]
   3eb10:	stur	x9, [x29, #-40]
   3eb14:	b.hi	3f1c4 <__gmpn_toom53_mul@@Base+0x73c>  // b.pmore
   3eb18:	add	x9, x1, #0xf
   3eb1c:	mov	x8, sp
   3eb20:	and	x9, x9, #0xfffffffffffffff0
   3eb24:	sub	x0, x8, x9
   3eb28:	mov	sp, x0
   3eb2c:	stur	x19, [x29, #-80]
   3eb30:	lsl	x19, x19, #3
   3eb34:	add	x1, x0, x19
   3eb38:	add	x24, x1, x19
   3eb3c:	add	x22, x24, x19
   3eb40:	add	x28, x22, x19
   3eb44:	add	x26, x28, x19
   3eb48:	add	x8, x26, x19
   3eb4c:	mov	w2, #0x4                   	// #4
   3eb50:	mov	x3, x25
   3eb54:	mov	x4, x21
   3eb58:	mov	x5, x23
   3eb5c:	mov	x6, x20
   3eb60:	stp	x0, x8, [x29, #-112]
   3eb64:	add	x27, x8, x19
   3eb68:	stur	x1, [x29, #-136]
   3eb6c:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3eb70:	and	w8, w0, #0x2
   3eb74:	mov	w2, #0x4                   	// #4
   3eb78:	mov	x0, x24
   3eb7c:	mov	x1, x22
   3eb80:	mov	x3, x25
   3eb84:	mov	x4, x21
   3eb88:	mov	x5, x23
   3eb8c:	mov	x6, x20
   3eb90:	stur	w8, [x29, #-28]
   3eb94:	stp	x24, x22, [x29, #-160]
   3eb98:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3eb9c:	stur	w0, [x29, #-64]
   3eba0:	add	x1, x25, x21, lsl #3
   3eba4:	mov	x0, x28
   3eba8:	mov	x2, x25
   3ebac:	mov	x3, x21
   3ebb0:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ebb4:	ldur	x8, [x29, #-48]
   3ebb8:	mov	x22, x0
   3ebbc:	mov	x0, x28
   3ebc0:	mov	x2, x28
   3ebc4:	add	x1, x25, x8, lsl #3
   3ebc8:	mov	x3, x21
   3ebcc:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ebd0:	mov	w8, #0x18                  	// #24
   3ebd4:	add	x22, x0, x22, lsl #1
   3ebd8:	madd	x1, x21, x8, x25
   3ebdc:	mov	x0, x28
   3ebe0:	mov	x2, x28
   3ebe4:	mov	x3, x21
   3ebe8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ebec:	ldur	x24, [x29, #-168]
   3ebf0:	add	x22, x0, x22, lsl #1
   3ebf4:	stur	x23, [x29, #-128]
   3ebf8:	stur	x28, [x29, #-144]
   3ebfc:	cmp	x24, x23
   3ec00:	b.ge	3ec28 <__gmpn_toom53_mul@@Base+0x1a0>  // b.tcont
   3ec04:	ldur	x8, [x29, #-88]
   3ec08:	mov	x0, x28
   3ec0c:	mov	x2, x28
   3ec10:	mov	x3, x21
   3ec14:	add	x1, x25, x8, lsl #3
   3ec18:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ec1c:	add	x8, x0, x22, lsl #1
   3ec20:	str	x8, [x28, x21, lsl #3]
   3ec24:	b	3ec9c <__gmpn_toom53_mul@@Base+0x214>
   3ec28:	ldur	x8, [x29, #-88]
   3ec2c:	mov	x0, x28
   3ec30:	mov	x2, x28
   3ec34:	mov	x3, x23
   3ec38:	add	x1, x25, x8, lsl #3
   3ec3c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ec40:	mov	x8, x23
   3ec44:	add	x23, x28, x23, lsl #3
   3ec48:	stur	x0, [x29, #-96]
   3ec4c:	sub	x2, x21, x8
   3ec50:	mov	w3, #0x1                   	// #1
   3ec54:	mov	x0, x23
   3ec58:	mov	x1, x23
   3ec5c:	bl	c180 <__gmpn_lshift@plt>
   3ec60:	add	x8, x0, x22, lsl #1
   3ec64:	str	x8, [x28, x21, lsl #3]
   3ec68:	ldr	x8, [x23]
   3ec6c:	ldur	x9, [x29, #-96]
   3ec70:	adds	x8, x8, x9
   3ec74:	str	x8, [x23]
   3ec78:	b.cc	3ec9c <__gmpn_toom53_mul@@Base+0x214>  // b.lo, b.ul, b.last
   3ec7c:	ldur	x8, [x29, #-112]
   3ec80:	ldur	x9, [x29, #-56]
   3ec84:	add	x8, x8, x9, lsl #3
   3ec88:	add	x8, x8, #0x28
   3ec8c:	ldr	x9, [x8]
   3ec90:	adds	x9, x9, #0x1
   3ec94:	str	x9, [x8], #8
   3ec98:	b.cs	3ec8c <__gmpn_toom53_mul@@Base+0x204>  // b.hs, b.nlast
   3ec9c:	add	x8, x27, x19
   3eca0:	stur	x8, [x29, #-56]
   3eca4:	ldur	w8, [x29, #-28]
   3eca8:	ldur	w9, [x29, #-64]
   3ecac:	ldur	x22, [x29, #-24]
   3ecb0:	lsl	x23, x21, #3
   3ecb4:	stur	x25, [x29, #-120]
   3ecb8:	bfxil	w8, w9, #0, #1
   3ecbc:	stur	w8, [x29, #-28]
   3ecc0:	ldp	x8, x28, [x29, #-48]
   3ecc4:	add	x2, x22, x8, lsl #3
   3ecc8:	stur	x2, [x29, #-64]
   3eccc:	cbz	x28, 3ed08 <__gmpn_toom53_mul@@Base+0x280>
   3ecd0:	mov	x0, x26
   3ecd4:	mov	x1, x22
   3ecd8:	mov	x3, x28
   3ecdc:	bl	ca70 <__gmpn_add_n@plt>
   3ece0:	cbz	x0, 3ed08 <__gmpn_toom53_mul@@Base+0x280>
   3ece4:	ldur	x28, [x29, #-40]
   3ece8:	cmp	x28, x24
   3ecec:	b.gt	3ed78 <__gmpn_toom53_mul@@Base+0x2f0>
   3ecf0:	lsl	x9, x28, #3
   3ecf4:	ldr	x10, [x22, x9]
   3ecf8:	add	x28, x28, #0x1
   3ecfc:	adds	x10, x10, #0x1
   3ed00:	str	x10, [x26, x9]
   3ed04:	b.cs	3ece8 <__gmpn_toom53_mul@@Base+0x260>  // b.hs, b.nlast
   3ed08:	cmp	x26, x22
   3ed0c:	b.eq	3ed40 <__gmpn_toom53_mul@@Base+0x2b8>  // b.none
   3ed10:	cmp	x28, x24
   3ed14:	b.gt	3ed40 <__gmpn_toom53_mul@@Base+0x2b8>
   3ed18:	mov	w9, #0x28                  	// #40
   3ed1c:	lsl	x10, x28, #3
   3ed20:	madd	x9, x24, x9, x10
   3ed24:	add	x1, x22, x10
   3ed28:	ldur	x10, [x29, #-112]
   3ed2c:	sub	x8, x21, x28
   3ed30:	lsl	x2, x8, #3
   3ed34:	add	x9, x9, x10
   3ed38:	add	x0, x9, #0x50
   3ed3c:	bl	bed0 <memcpy@plt>
   3ed40:	ldur	x28, [x29, #-64]
   3ed44:	add	x1, x22, x23
   3ed48:	mov	x8, x21
   3ed4c:	str	xzr, [x26, x23]
   3ed50:	subs	x8, x8, #0x1
   3ed54:	b.lt	3ed70 <__gmpn_toom53_mul@@Base+0x2e8>  // b.tstop
   3ed58:	lsl	x9, x8, #3
   3ed5c:	ldr	x10, [x26, x9]
   3ed60:	ldr	x9, [x1, x9]
   3ed64:	cmp	x10, x9
   3ed68:	b.eq	3ed50 <__gmpn_toom53_mul@@Base+0x2c8>  // b.none
   3ed6c:	b.ls	3f19c <__gmpn_toom53_mul@@Base+0x714>  // b.plast
   3ed70:	mov	x22, xzr
   3ed74:	b	3ed80 <__gmpn_toom53_mul@@Base+0x2f8>
   3ed78:	mov	w22, #0x1                   	// #1
   3ed7c:	str	x22, [x26, x21, lsl #3]
   3ed80:	ldur	x25, [x29, #-24]
   3ed84:	ldur	x28, [x29, #-104]
   3ed88:	mov	x1, x26
   3ed8c:	mov	x3, x21
   3ed90:	add	x2, x25, x23
   3ed94:	mov	x0, x28
   3ed98:	bl	c2d0 <__gmpn_sub_n@plt>
   3ed9c:	sub	x8, x22, x0
   3eda0:	str	x8, [x28, x23]
   3eda4:	ldur	x28, [x29, #-64]
   3eda8:	mov	x22, x25
   3edac:	ldur	x8, [x29, #-56]
   3edb0:	mov	x0, x26
   3edb4:	mov	x1, x26
   3edb8:	mov	x3, x21
   3edbc:	add	x8, x8, x19
   3edc0:	mov	x19, x22
   3edc4:	add	x22, x22, x23
   3edc8:	mov	x2, x22
   3edcc:	stur	x8, [x29, #-96]
   3edd0:	bl	ca70 <__gmpn_add_n@plt>
   3edd4:	ldr	x8, [x26, x23]
   3edd8:	ldur	x25, [x29, #-40]
   3eddc:	mov	x1, x19
   3ede0:	mov	x2, x28
   3ede4:	add	x8, x8, x0
   3ede8:	mov	x0, x27
   3edec:	mov	x3, x25
   3edf0:	str	x8, [x26, x23]
   3edf4:	bl	cba0 <__gmpn_addlsh2_n@plt>
   3edf8:	ldur	x19, [x29, #-80]
   3edfc:	cmp	x24, x25
   3ee00:	b.lt	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.tstop
   3ee04:	ldur	x9, [x29, #-24]
   3ee08:	lsl	x8, x25, #3
   3ee0c:	add	x11, x27, x8
   3ee10:	add	x10, x9, x8
   3ee14:	ldr	x12, [x10]
   3ee18:	sub	x9, x21, x25
   3ee1c:	adds	x8, x12, x0
   3ee20:	str	x8, [x11]
   3ee24:	b.cc	3eeb8 <__gmpn_toom53_mul@@Base+0x430>  // b.lo, b.ul, b.last
   3ee28:	mov	w0, #0x1                   	// #1
   3ee2c:	mov	w8, #0x1                   	// #1
   3ee30:	cmp	x8, x9
   3ee34:	b.ge	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.tcont
   3ee38:	lsl	x12, x8, #3
   3ee3c:	ldr	x13, [x10, x12]
   3ee40:	add	x8, x8, #0x1
   3ee44:	adds	x13, x13, #0x1
   3ee48:	str	x13, [x11, x12]
   3ee4c:	b.cs	3ee30 <__gmpn_toom53_mul@@Base+0x3a8>  // b.hs, b.nlast
   3ee50:	ldur	x10, [x29, #-24]
   3ee54:	mov	x0, xzr
   3ee58:	cmp	x27, x10
   3ee5c:	b.eq	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.none
   3ee60:	cmp	x8, x9
   3ee64:	b.ge	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.tcont
   3ee68:	ldur	x13, [x29, #-176]
   3ee6c:	ldur	x12, [x29, #-24]
   3ee70:	lsl	x11, x24, #1
   3ee74:	mov	w9, #0x28                  	// #40
   3ee78:	add	x10, x8, x13
   3ee7c:	sub	x10, x10, x11
   3ee80:	add	x10, x12, x10, lsl #3
   3ee84:	sub	x1, x10, #0x10
   3ee88:	ldur	x10, [x29, #-112]
   3ee8c:	mul	x9, x24, x9
   3ee90:	add	x11, x11, x24
   3ee94:	add	x9, x9, x8, lsl #3
   3ee98:	sub	x8, x11, x8
   3ee9c:	add	x9, x9, x13, lsl #3
   3eea0:	sub	x8, x8, x13
   3eea4:	add	x9, x9, x10
   3eea8:	lsl	x8, x8, #3
   3eeac:	add	x0, x9, #0x60
   3eeb0:	add	x2, x8, #0x18
   3eeb4:	b	3ef10 <__gmpn_toom53_mul@@Base+0x488>
   3eeb8:	cmp	x9, #0x2
   3eebc:	mov	x0, xzr
   3eec0:	b.lt	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.tstop
   3eec4:	ldur	x8, [x29, #-24]
   3eec8:	cmp	x27, x8
   3eecc:	b.eq	3ef18 <__gmpn_toom53_mul@@Base+0x490>  // b.none
   3eed0:	ldur	x11, [x29, #-176]
   3eed4:	mov	w8, #0x28                  	// #40
   3eed8:	lsl	x9, x24, #1
   3eedc:	mul	x8, x24, x8
   3eee0:	mvn	x10, x9
   3eee4:	add	x9, x9, x24
   3eee8:	add	x8, x8, x11, lsl #3
   3eeec:	add	x10, x10, x11
   3eef0:	sub	x9, x9, x11
   3eef4:	ldur	x11, [x29, #-112]
   3eef8:	lsl	x9, x9, #3
   3eefc:	add	x2, x9, #0x10
   3ef00:	add	x8, x8, x11
   3ef04:	ldur	x11, [x29, #-24]
   3ef08:	add	x0, x8, #0x68
   3ef0c:	add	x1, x11, x10, lsl #3
   3ef10:	bl	bed0 <memcpy@plt>
   3ef14:	mov	x0, xzr
   3ef18:	str	x0, [x27, x23]
   3ef1c:	mov	w3, #0x1                   	// #1
   3ef20:	mov	x0, x20
   3ef24:	mov	x1, x22
   3ef28:	mov	x2, x21
   3ef2c:	bl	c180 <__gmpn_lshift@plt>
   3ef30:	mov	x8, x19
   3ef34:	str	x0, [x20, x23]
   3ef38:	subs	x8, x8, #0x1
   3ef3c:	b.lt	3ef58 <__gmpn_toom53_mul@@Base+0x4d0>  // b.tstop
   3ef40:	lsl	x9, x8, #3
   3ef44:	ldr	x10, [x27, x9]
   3ef48:	ldr	x9, [x20, x9]
   3ef4c:	cmp	x10, x9
   3ef50:	b.eq	3ef38 <__gmpn_toom53_mul@@Base+0x4b0>  // b.none
   3ef54:	b.ls	3ef70 <__gmpn_toom53_mul@@Base+0x4e8>  // b.plast
   3ef58:	ldur	x0, [x29, #-56]
   3ef5c:	mov	x1, x27
   3ef60:	mov	x2, x20
   3ef64:	mov	x3, x19
   3ef68:	bl	c2d0 <__gmpn_sub_n@plt>
   3ef6c:	b	3ef90 <__gmpn_toom53_mul@@Base+0x508>
   3ef70:	ldur	x0, [x29, #-56]
   3ef74:	mov	x1, x20
   3ef78:	mov	x2, x27
   3ef7c:	mov	x3, x19
   3ef80:	bl	c2d0 <__gmpn_sub_n@plt>
   3ef84:	ldur	w8, [x29, #-28]
   3ef88:	eor	w8, w8, #0x1
   3ef8c:	stur	w8, [x29, #-28]
   3ef90:	mov	x0, x27
   3ef94:	mov	x1, x27
   3ef98:	mov	x2, x20
   3ef9c:	mov	x3, x19
   3efa0:	bl	ca70 <__gmpn_add_n@plt>
   3efa4:	ldur	x19, [x29, #-96]
   3efa8:	ldur	x2, [x29, #-24]
   3efac:	mov	x1, x22
   3efb0:	mov	x3, x21
   3efb4:	mov	x0, x19
   3efb8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3efbc:	cmp	x24, x25
   3efc0:	mov	x22, x0
   3efc4:	mov	x0, x19
   3efc8:	mov	x1, x28
   3efcc:	mov	x2, x19
   3efd0:	b.lt	3f024 <__gmpn_toom53_mul@@Base+0x59c>  // b.tstop
   3efd4:	mov	x3, x25
   3efd8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3efdc:	add	x28, x19, x25, lsl #3
   3efe0:	mov	x24, x0
   3efe4:	sub	x2, x21, x25
   3efe8:	mov	w3, #0x1                   	// #1
   3efec:	mov	x0, x28
   3eff0:	mov	x1, x28
   3eff4:	bl	c180 <__gmpn_lshift@plt>
   3eff8:	add	x8, x0, x22, lsl #1
   3effc:	str	x8, [x19, x21, lsl #3]
   3f000:	ldr	x8, [x28]
   3f004:	adds	x8, x8, x24
   3f008:	str	x8, [x28]
   3f00c:	b.cc	3f034 <__gmpn_toom53_mul@@Base+0x5ac>  // b.lo, b.ul, b.last
   3f010:	ldr	x8, [x28, #8]!
   3f014:	adds	x8, x8, #0x1
   3f018:	str	x8, [x28]
   3f01c:	b.cs	3f010 <__gmpn_toom53_mul@@Base+0x588>  // b.hs, b.nlast
   3f020:	b	3f034 <__gmpn_toom53_mul@@Base+0x5ac>
   3f024:	mov	x3, x21
   3f028:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3f02c:	add	x8, x0, x22, lsl #1
   3f030:	str	x8, [x19, x21, lsl #3]
   3f034:	ldp	x25, x19, [x29, #-80]
   3f038:	ldur	x1, [x29, #-160]
   3f03c:	mov	x2, x27
   3f040:	mov	x0, x19
   3f044:	mov	x3, x25
   3f048:	bl	c990 <__gmpn_mul_n@plt>
   3f04c:	ldp	x2, x8, [x29, #-56]
   3f050:	ldur	x1, [x29, #-152]
   3f054:	mov	x3, x25
   3f058:	lsl	x24, x8, #3
   3f05c:	add	x8, x19, x24
   3f060:	add	x22, x8, #0x8
   3f064:	mov	x0, x22
   3f068:	bl	c990 <__gmpn_mul_n@plt>
   3f06c:	ldp	x2, x8, [x29, #-96]
   3f070:	ldur	x1, [x29, #-144]
   3f074:	mov	x3, x25
   3f078:	lsl	x28, x8, #3
   3f07c:	add	x8, x19, x28
   3f080:	add	x27, x8, #0x10
   3f084:	mov	x0, x27
   3f088:	bl	c990 <__gmpn_mul_n@plt>
   3f08c:	add	x8, x21, x21, lsl #1
   3f090:	lsl	x25, x8, #4
   3f094:	ldur	x1, [x29, #-136]
   3f098:	ldur	x2, [x29, #-104]
   3f09c:	add	x8, x19, x25
   3f0a0:	add	x19, x8, #0x18
   3f0a4:	str	xzr, [x19, x24]
   3f0a8:	ldr	x8, [x1, x23]
   3f0ac:	ldr	x9, [x2, x23]
   3f0b0:	mov	x0, x19
   3f0b4:	orr	x8, x9, x8
   3f0b8:	cmp	x8, #0x0
   3f0bc:	cinc	x3, x21, ne  // ne = any
   3f0c0:	bl	c990 <__gmpn_mul_n@plt>
   3f0c4:	ldur	x1, [x29, #-112]
   3f0c8:	add	x0, x20, x24
   3f0cc:	str	xzr, [x0, x24]
   3f0d0:	ldr	x9, [x26, x23]
   3f0d4:	ldr	x8, [x1, x23]
   3f0d8:	mov	x2, x26
   3f0dc:	orr	x8, x9, x8
   3f0e0:	cmp	x8, #0x0
   3f0e4:	cinc	x3, x21, ne  // ne = any
   3f0e8:	bl	c990 <__gmpn_mul_n@plt>
   3f0ec:	ldur	x23, [x29, #-120]
   3f0f0:	ldur	x2, [x29, #-24]
   3f0f4:	mov	x0, x20
   3f0f8:	mov	x3, x21
   3f0fc:	mov	x1, x23
   3f100:	bl	c990 <__gmpn_mul_n@plt>
   3f104:	add	x0, x20, x25
   3f108:	ldur	x24, [x29, #-128]
   3f10c:	ldur	x25, [x29, #-40]
   3f110:	add	x3, x23, x28
   3f114:	cmp	x24, x25
   3f118:	b.le	3f130 <__gmpn_toom53_mul@@Base+0x6a8>
   3f11c:	mov	x1, x3
   3f120:	ldur	x3, [x29, #-64]
   3f124:	mov	x2, x24
   3f128:	mov	x4, x25
   3f12c:	b	3f13c <__gmpn_toom53_mul@@Base+0x6b4>
   3f130:	ldur	x1, [x29, #-64]
   3f134:	mov	x2, x25
   3f138:	mov	x4, x24
   3f13c:	bl	ccd0 <__gmpn_mul@plt>
   3f140:	ldur	x5, [x29, #-72]
   3f144:	add	x7, x24, x25
   3f148:	add	x8, x5, x21, lsl #6
   3f14c:	add	x8, x8, #0x20
   3f150:	str	x8, [sp, #-16]!
   3f154:	ldur	w2, [x29, #-28]
   3f158:	mov	x0, x20
   3f15c:	mov	x1, x21
   3f160:	mov	x3, x22
   3f164:	mov	x4, x19
   3f168:	mov	x6, x27
   3f16c:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   3f170:	add	sp, sp, #0x10
   3f174:	ldur	x0, [x29, #-16]
   3f178:	cbnz	x0, 3f1d0 <__gmpn_toom53_mul@@Base+0x748>
   3f17c:	mov	sp, x29
   3f180:	ldp	x20, x19, [sp, #80]
   3f184:	ldp	x22, x21, [sp, #64]
   3f188:	ldp	x24, x23, [sp, #48]
   3f18c:	ldp	x26, x25, [sp, #32]
   3f190:	ldp	x28, x27, [sp, #16]
   3f194:	ldp	x29, x30, [sp], #96
   3f198:	ret
   3f19c:	ldur	x25, [x29, #-104]
   3f1a0:	mov	x2, x26
   3f1a4:	mov	x3, x21
   3f1a8:	mov	x0, x25
   3f1ac:	bl	c2d0 <__gmpn_sub_n@plt>
   3f1b0:	ldur	w8, [x29, #-28]
   3f1b4:	str	xzr, [x25, x21, lsl #3]
   3f1b8:	eor	w8, w8, #0x2
   3f1bc:	stur	w8, [x29, #-28]
   3f1c0:	b	3edac <__gmpn_toom53_mul@@Base+0x324>
   3f1c4:	sub	x0, x29, #0x10
   3f1c8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3f1cc:	b	3eb2c <__gmpn_toom53_mul@@Base+0xa4>
   3f1d0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3f1d4:	b	3f17c <__gmpn_toom53_mul@@Base+0x6f4>

000000000003f1d8 <__gmpn_toom54_mul@@Base>:
   3f1d8:	sub	sp, sp, #0xc0
   3f1dc:	add	x8, x4, x4, lsl #2
   3f1e0:	stp	x29, x30, [sp, #96]
   3f1e4:	stp	x24, x23, [sp, #144]
   3f1e8:	stp	x20, x19, [sp, #176]
   3f1ec:	add	x29, sp, #0x60
   3f1f0:	mov	x24, x3
   3f1f4:	mov	x19, x1
   3f1f8:	cmp	x8, x2, lsl #2
   3f1fc:	mov	x20, x0
   3f200:	stp	x28, x27, [sp, #112]
   3f204:	stp	x26, x25, [sp, #128]
   3f208:	stp	x22, x21, [sp, #160]
   3f20c:	stur	x5, [x29, #-32]
   3f210:	b.le	3f21c <__gmpn_toom54_mul@@Base+0x44>
   3f214:	sub	x8, x4, #0x1
   3f218:	b	3f22c <__gmpn_toom54_mul@@Base+0x54>
   3f21c:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   3f220:	sub	x8, x2, #0x1
   3f224:	movk	x9, #0xcccd
   3f228:	umulh	x8, x8, x9
   3f22c:	lsr	x23, x8, #2
   3f230:	add	x21, x23, #0x1
   3f234:	add	x26, x21, x21, lsl #1
   3f238:	mov	w8, #0x28                  	// #40
   3f23c:	lsl	x25, x26, #3
   3f240:	lsl	x9, x21, #2
   3f244:	madd	x8, x21, x8, x20
   3f248:	add	x27, x20, x25
   3f24c:	sub	x5, x2, x9
   3f250:	sub	x28, x4, x26
   3f254:	add	x0, x8, #0x10
   3f258:	mov	w2, #0x4                   	// #4
   3f25c:	mov	w6, #0x2                   	// #2
   3f260:	mov	x1, x27
   3f264:	mov	x3, x19
   3f268:	mov	x4, x21
   3f26c:	mov	x7, x20
   3f270:	str	x9, [sp, #40]
   3f274:	stur	x5, [x29, #-40]
   3f278:	str	x28, [sp, #16]
   3f27c:	stur	x0, [x29, #-24]
   3f280:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3f284:	lsl	x8, x26, #1
   3f288:	stur	x19, [x29, #-16]
   3f28c:	str	x8, [sp, #48]
   3f290:	add	x8, x20, x26, lsl #4
   3f294:	add	x9, x20, x21, lsl #5
   3f298:	str	x26, [sp, #32]
   3f29c:	add	x26, x8, #0x18
   3f2a0:	add	x19, x9, #0x8
   3f2a4:	mov	w22, w0
   3f2a8:	mov	w2, #0x3                   	// #3
   3f2ac:	mov	w6, #0x2                   	// #2
   3f2b0:	mov	x0, x26
   3f2b4:	mov	x1, x19
   3f2b8:	mov	x3, x24
   3f2bc:	mov	x4, x21
   3f2c0:	mov	x5, x28
   3f2c4:	mov	x7, x20
   3f2c8:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3f2cc:	eor	w8, w0, w22
   3f2d0:	add	x22, x23, #0x2
   3f2d4:	mov	x0, x20
   3f2d8:	mov	x1, x27
   3f2dc:	mov	x2, x19
   3f2e0:	mov	x3, x22
   3f2e4:	stur	x24, [x29, #-8]
   3f2e8:	str	w8, [sp, #12]
   3f2ec:	bl	c990 <__gmpn_mul_n@plt>
   3f2f0:	ldp	x28, x24, [x29, #-32]
   3f2f4:	mov	x2, x26
   3f2f8:	mov	x3, x22
   3f2fc:	add	x8, x28, x25
   3f300:	add	x25, x8, #0x8
   3f304:	mov	x0, x25
   3f308:	mov	x1, x24
   3f30c:	bl	c990 <__gmpn_mul_n@plt>
   3f310:	ldr	w3, [sp, #12]
   3f314:	mov	w23, #0x1                   	// #1
   3f318:	bfi	x23, x21, #1, #63
   3f31c:	mov	w5, #0x2                   	// #2
   3f320:	mov	w6, #0x4                   	// #4
   3f324:	mov	x0, x25
   3f328:	mov	x1, x23
   3f32c:	mov	x2, x20
   3f330:	mov	x4, x21
   3f334:	str	x25, [sp, #24]
   3f338:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f33c:	ldur	x3, [x29, #-16]
   3f340:	ldur	x5, [x29, #-40]
   3f344:	mov	w2, #0x4                   	// #4
   3f348:	mov	x0, x24
   3f34c:	mov	x1, x27
   3f350:	mov	x4, x21
   3f354:	mov	x6, x20
   3f358:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3f35c:	ldr	x25, [sp, #16]
   3f360:	ldur	x2, [x29, #-8]
   3f364:	mov	w24, w0
   3f368:	mov	x0, x26
   3f36c:	mov	x1, x19
   3f370:	mov	x3, x21
   3f374:	mov	x4, x25
   3f378:	mov	x5, x20
   3f37c:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3f380:	eor	w8, w0, w24
   3f384:	mov	x0, x20
   3f388:	mov	x1, x27
   3f38c:	mov	x2, x19
   3f390:	mov	x3, x22
   3f394:	str	w8, [sp, #12]
   3f398:	bl	c990 <__gmpn_mul_n@plt>
   3f39c:	ldur	x24, [x29, #-24]
   3f3a0:	mov	x0, x28
   3f3a4:	mov	x2, x26
   3f3a8:	mov	x3, x22
   3f3ac:	mov	x1, x24
   3f3b0:	bl	c990 <__gmpn_mul_n@plt>
   3f3b4:	ldr	w3, [sp, #12]
   3f3b8:	mov	x0, x28
   3f3bc:	mov	x1, x23
   3f3c0:	mov	x2, x20
   3f3c4:	mov	x4, x21
   3f3c8:	mov	w5, wzr
   3f3cc:	mov	w6, wzr
   3f3d0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f3d4:	ldur	x28, [x29, #-40]
   3f3d8:	ldur	x3, [x29, #-16]
   3f3dc:	mov	w2, #0x4                   	// #4
   3f3e0:	mov	x0, x24
   3f3e4:	mov	x1, x27
   3f3e8:	mov	x4, x21
   3f3ec:	mov	x5, x28
   3f3f0:	mov	x6, x20
   3f3f4:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3f3f8:	ldur	x2, [x29, #-8]
   3f3fc:	mov	w24, w0
   3f400:	mov	x0, x26
   3f404:	mov	x1, x19
   3f408:	mov	x3, x21
   3f40c:	mov	x4, x25
   3f410:	mov	x5, x20
   3f414:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3f418:	eor	w24, w0, w24
   3f41c:	mov	x0, x20
   3f420:	mov	x1, x27
   3f424:	mov	x2, x19
   3f428:	mov	x3, x22
   3f42c:	bl	c990 <__gmpn_mul_n@plt>
   3f430:	mov	x3, x22
   3f434:	ldp	x1, x22, [x29, #-24]
   3f438:	mov	x0, x27
   3f43c:	mov	x2, x26
   3f440:	bl	c990 <__gmpn_mul_n@plt>
   3f444:	ldur	x19, [x29, #-8]
   3f448:	mov	w5, #0x1                   	// #1
   3f44c:	mov	w6, #0x2                   	// #2
   3f450:	mov	x0, x27
   3f454:	mov	x1, x23
   3f458:	mov	x2, x20
   3f45c:	mov	w3, w24
   3f460:	mov	x4, x21
   3f464:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f468:	mov	x0, x20
   3f46c:	mov	x1, x22
   3f470:	mov	x2, x19
   3f474:	mov	x3, x21
   3f478:	bl	c990 <__gmpn_mul_n@plt>
   3f47c:	mov	w8, #0x38                  	// #56
   3f480:	cmp	x28, x25
   3f484:	madd	x0, x21, x8, x20
   3f488:	b.le	3f4a8 <__gmpn_toom54_mul@@Base+0x2d0>
   3f48c:	ldr	x8, [sp, #40]
   3f490:	mov	x2, x28
   3f494:	mov	x4, x25
   3f498:	add	x1, x22, x8, lsl #3
   3f49c:	ldr	x8, [sp, #32]
   3f4a0:	add	x3, x19, x8, lsl #3
   3f4a4:	b	3f4c0 <__gmpn_toom54_mul@@Base+0x2e8>
   3f4a8:	ldr	x8, [sp, #32]
   3f4ac:	mov	x2, x25
   3f4b0:	mov	x4, x28
   3f4b4:	add	x1, x19, x8, lsl #3
   3f4b8:	ldr	x8, [sp, #40]
   3f4bc:	add	x3, x22, x8, lsl #3
   3f4c0:	bl	ccd0 <__gmpn_mul@plt>
   3f4c4:	ldr	x8, [sp, #48]
   3f4c8:	ldur	x3, [x29, #-32]
   3f4cc:	add	x4, x28, x25
   3f4d0:	mov	x0, x20
   3f4d4:	mov	x1, x21
   3f4d8:	ldr	x2, [sp, #24]
   3f4dc:	ldp	x20, x19, [sp, #176]
   3f4e0:	ldp	x22, x21, [sp, #160]
   3f4e4:	ldp	x24, x23, [sp, #144]
   3f4e8:	ldp	x26, x25, [sp, #128]
   3f4ec:	ldp	x28, x27, [sp, #112]
   3f4f0:	ldp	x29, x30, [sp, #96]
   3f4f4:	add	x8, x3, x8, lsl #3
   3f4f8:	add	x5, x8, #0x10
   3f4fc:	add	sp, sp, #0xc0
   3f500:	b	c7b0 <__gmpn_toom_interpolate_8pts@plt>

000000000003f504 <__gmpn_toom63_mul@@Base>:
   3f504:	sub	sp, sp, #0xe0
   3f508:	lsl	x8, x4, #1
   3f50c:	cmp	x8, x2
   3f510:	mov	w9, #0x6                   	// #6
   3f514:	mov	w10, #0x3                   	// #3
   3f518:	csel	x8, x4, x2, gt
   3f51c:	csel	x9, x10, x9, gt
   3f520:	sub	x8, x8, #0x1
   3f524:	stp	x26, x25, [sp, #160]
   3f528:	udiv	x26, x8, x9
   3f52c:	stp	x22, x21, [sp, #192]
   3f530:	add	x21, x26, #0x1
   3f534:	stp	x29, x30, [sp, #128]
   3f538:	add	x29, sp, #0x80
   3f53c:	add	x8, x21, x21, lsl #2
   3f540:	lsl	x9, x21, #1
   3f544:	stp	x28, x27, [sp, #144]
   3f548:	stp	x24, x23, [sp, #176]
   3f54c:	stp	x20, x19, [sp, #208]
   3f550:	stur	x5, [x29, #-16]
   3f554:	mov	x20, x0
   3f558:	sub	x5, x2, x8
   3f55c:	stp	x8, x9, [sp, #32]
   3f560:	add	x8, x0, x8, lsl #3
   3f564:	add	x22, x9, x21
   3f568:	mov	x25, x3
   3f56c:	mov	x3, x1
   3f570:	sub	x19, x4, x9
   3f574:	add	x0, x8, #0x10
   3f578:	add	x1, x20, x22, lsl #3
   3f57c:	mov	w2, #0x5                   	// #5
   3f580:	mov	w6, #0x2                   	// #2
   3f584:	mov	x4, x21
   3f588:	mov	x7, x20
   3f58c:	stp	x0, x1, [x29, #-48]
   3f590:	stp	x3, x5, [x29, #-32]
   3f594:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3f598:	lsl	x23, x21, #3
   3f59c:	add	x24, x25, x23
   3f5a0:	mov	w28, w0
   3f5a4:	mov	w3, #0x2                   	// #2
   3f5a8:	mov	x0, x20
   3f5ac:	mov	x1, x24
   3f5b0:	mov	x2, x21
   3f5b4:	bl	c180 <__gmpn_lshift@plt>
   3f5b8:	lsl	x8, x22, #1
   3f5bc:	str	x8, [sp, #16]
   3f5c0:	add	x8, x20, x22, lsl #4
   3f5c4:	add	x27, x8, #0x18
   3f5c8:	str	x0, [x20, x23]
   3f5cc:	add	x1, x25, x21, lsl #4
   3f5d0:	mov	w3, #0x4                   	// #4
   3f5d4:	mov	x0, x27
   3f5d8:	mov	x2, x19
   3f5dc:	str	x22, [sp, #64]
   3f5e0:	stur	x1, [x29, #-56]
   3f5e4:	bl	c180 <__gmpn_lshift@plt>
   3f5e8:	cmp	x21, x19
   3f5ec:	str	x0, [x27, x19, lsl #3]
   3f5f0:	stur	x19, [x29, #-8]
   3f5f4:	stp	x24, x21, [sp, #48]
   3f5f8:	b.ne	3f62c <__gmpn_toom63_mul@@Base+0x128>  // b.any
   3f5fc:	mov	x0, x27
   3f600:	mov	x1, x27
   3f604:	mov	x2, x25
   3f608:	mov	x3, x21
   3f60c:	bl	ca70 <__gmpn_add_n@plt>
   3f610:	ldr	x8, [x27, x23]
   3f614:	add	x9, x20, x21, lsl #5
   3f618:	add	x19, x9, #0x8
   3f61c:	add	x22, x26, #0x2
   3f620:	add	x8, x8, x0
   3f624:	str	x8, [x27, x23]
   3f628:	b	3f74c <__gmpn_toom63_mul@@Base+0x248>
   3f62c:	adds	x19, x19, #0x1
   3f630:	b.cc	3f63c <__gmpn_toom63_mul@@Base+0x138>  // b.lo, b.ul, b.last
   3f634:	mov	x19, xzr
   3f638:	b	3f678 <__gmpn_toom63_mul@@Base+0x174>
   3f63c:	mov	x0, x27
   3f640:	mov	x1, x25
   3f644:	mov	x2, x27
   3f648:	mov	x3, x19
   3f64c:	bl	ca70 <__gmpn_add_n@plt>
   3f650:	cbz	x0, 3f678 <__gmpn_toom63_mul@@Base+0x174>
   3f654:	mov	w8, #0x1                   	// #1
   3f658:	cmp	x19, x26
   3f65c:	b.gt	3f734 <__gmpn_toom63_mul@@Base+0x230>
   3f660:	lsl	x9, x19, #3
   3f664:	ldr	x10, [x25, x9]
   3f668:	add	x19, x19, #0x1
   3f66c:	adds	x10, x10, #0x1
   3f670:	str	x10, [x27, x9]
   3f674:	b.cs	3f658 <__gmpn_toom63_mul@@Base+0x154>  // b.hs, b.nlast
   3f678:	cmp	x27, x25
   3f67c:	mov	x8, xzr
   3f680:	b.eq	3f734 <__gmpn_toom63_mul@@Base+0x230>  // b.none
   3f684:	cmp	x19, x26
   3f688:	b.gt	3f734 <__gmpn_toom63_mul@@Base+0x230>
   3f68c:	sub	x8, x26, x19
   3f690:	add	x8, x8, #0x1
   3f694:	cmp	x8, #0x4
   3f698:	b.cs	3f6a4 <__gmpn_toom63_mul@@Base+0x1a0>  // b.hs, b.nlast
   3f69c:	mov	x9, x19
   3f6a0:	b	3f718 <__gmpn_toom63_mul@@Base+0x214>
   3f6a4:	mov	w9, #0x6                   	// #6
   3f6a8:	madd	x9, x26, x9, x19
   3f6ac:	add	x9, x20, x9, lsl #3
   3f6b0:	add	x9, x9, #0x48
   3f6b4:	add	x10, x25, x21, lsl #3
   3f6b8:	cmp	x9, x10
   3f6bc:	add	x12, x25, x19, lsl #3
   3f6c0:	b.cs	3f6e0 <__gmpn_toom63_mul@@Base+0x1dc>  // b.hs, b.nlast
   3f6c4:	mov	w9, #0x38                  	// #56
   3f6c8:	madd	x9, x26, x9, x20
   3f6cc:	add	x9, x9, #0x50
   3f6d0:	cmp	x12, x9
   3f6d4:	b.cs	3f6e0 <__gmpn_toom63_mul@@Base+0x1dc>  // b.hs, b.nlast
   3f6d8:	mov	x9, x19
   3f6dc:	b	3f718 <__gmpn_toom63_mul@@Base+0x214>
   3f6e0:	and	x10, x8, #0xfffffffffffffffc
   3f6e4:	mov	x11, xzr
   3f6e8:	add	x9, x19, x10
   3f6ec:	add	x12, x12, #0x10
   3f6f0:	ldp	q0, q1, [x12, #-16]
   3f6f4:	add	x13, x19, x11
   3f6f8:	add	x11, x11, #0x4
   3f6fc:	add	x13, x27, x13, lsl #3
   3f700:	cmp	x11, x10
   3f704:	add	x12, x12, #0x20
   3f708:	stp	q0, q1, [x13]
   3f70c:	b.ne	3f6f0 <__gmpn_toom63_mul@@Base+0x1ec>  // b.any
   3f710:	cmp	x8, x10
   3f714:	b.eq	3f730 <__gmpn_toom63_mul@@Base+0x22c>  // b.none
   3f718:	lsl	x8, x9, #3
   3f71c:	ldr	x10, [x25, x8]
   3f720:	cmp	x9, x26
   3f724:	add	x9, x9, #0x1
   3f728:	str	x10, [x27, x8]
   3f72c:	b.ne	3f718 <__gmpn_toom63_mul@@Base+0x214>  // b.any
   3f730:	mov	x8, xzr
   3f734:	str	x8, [x27, x21, lsl #3]
   3f738:	add	x8, x20, x21, lsl #5
   3f73c:	add	x22, x26, #0x2
   3f740:	cmp	x22, #0x1
   3f744:	add	x19, x8, #0x8
   3f748:	b.lt	3f944 <__gmpn_toom63_mul@@Base+0x440>  // b.tstop
   3f74c:	sub	x8, x22, #0x1
   3f750:	mov	x9, x22
   3f754:	sub	x9, x9, #0x1
   3f758:	ldr	x10, [x27, x9, lsl #3]
   3f75c:	ldr	x11, [x20, x8, lsl #3]
   3f760:	cmp	x10, x11
   3f764:	b.ne	3f784 <__gmpn_toom63_mul@@Base+0x280>  // b.any
   3f768:	add	x10, x8, #0x1
   3f76c:	sub	x11, x8, #0x1
   3f770:	cmp	x10, #0x1
   3f774:	str	xzr, [x19, x8, lsl #3]
   3f778:	mov	x8, x11
   3f77c:	b.gt	3f754 <__gmpn_toom63_mul@@Base+0x250>
   3f780:	b	3f79c <__gmpn_toom63_mul@@Base+0x298>
   3f784:	add	x3, x8, #0x1
   3f788:	mov	x0, x19
   3f78c:	b.ls	3f7a4 <__gmpn_toom63_mul@@Base+0x2a0>  // b.plast
   3f790:	mov	x1, x27
   3f794:	mov	x2, x20
   3f798:	bl	c2d0 <__gmpn_sub_n@plt>
   3f79c:	mov	w23, wzr
   3f7a0:	b	3f7b4 <__gmpn_toom63_mul@@Base+0x2b0>
   3f7a4:	mov	x1, x20
   3f7a8:	mov	x2, x27
   3f7ac:	bl	c2d0 <__gmpn_sub_n@plt>
   3f7b0:	mov	w23, #0xffffffff            	// #-1
   3f7b4:	mov	w8, #0x1                   	// #1
   3f7b8:	str	w8, [sp, #8]
   3f7bc:	mov	x0, x27
   3f7c0:	mov	x1, x27
   3f7c4:	mov	x2, x20
   3f7c8:	mov	x3, x22
   3f7cc:	bl	ca70 <__gmpn_add_n@plt>
   3f7d0:	eor	w8, w23, w28
   3f7d4:	str	w8, [sp, #12]
   3f7d8:	ldur	x24, [x29, #-40]
   3f7dc:	mov	x0, x20
   3f7e0:	mov	x2, x19
   3f7e4:	mov	x3, x22
   3f7e8:	mov	x1, x24
   3f7ec:	bl	c990 <__gmpn_mul_n@plt>
   3f7f0:	ldur	x23, [x29, #-16]
   3f7f4:	ldr	x8, [sp, #64]
   3f7f8:	ldur	x21, [x29, #-48]
   3f7fc:	mov	x2, x27
   3f800:	mov	x3, x22
   3f804:	add	x8, x23, x8, lsl #3
   3f808:	add	x28, x8, #0x8
   3f80c:	mov	x0, x28
   3f810:	mov	x1, x21
   3f814:	str	x22, [sp, #64]
   3f818:	bl	c990 <__gmpn_mul_n@plt>
   3f81c:	ldr	x8, [sp, #40]
   3f820:	ldr	x22, [sp, #56]
   3f824:	ldr	w3, [sp, #12]
   3f828:	mov	w5, #0x2                   	// #2
   3f82c:	orr	x1, x8, #0x1
   3f830:	mov	w6, #0x4                   	// #4
   3f834:	mov	x0, x28
   3f838:	mov	x2, x20
   3f83c:	mov	x4, x22
   3f840:	str	x28, [sp, #24]
   3f844:	str	x1, [sp, #40]
   3f848:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f84c:	ldp	x3, x5, [x29, #-32]
   3f850:	mov	w2, #0x5                   	// #5
   3f854:	mov	x0, x21
   3f858:	mov	x1, x24
   3f85c:	mov	x4, x22
   3f860:	mov	x6, x20
   3f864:	mov	x21, x22
   3f868:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3f86c:	ldr	x8, [sp, #16]
   3f870:	mov	w24, w0
   3f874:	add	x8, x23, x8, lsl #3
   3f878:	ldur	x23, [x29, #-8]
   3f87c:	add	x28, x8, #0x10
   3f880:	cbz	x23, 3f8d0 <__gmpn_toom63_mul@@Base+0x3cc>
   3f884:	ldur	x2, [x29, #-56]
   3f888:	mov	x0, x28
   3f88c:	mov	x1, x25
   3f890:	mov	x3, x23
   3f894:	bl	ca70 <__gmpn_add_n@plt>
   3f898:	ldr	x22, [sp, #48]
   3f89c:	mov	x8, x23
   3f8a0:	cbz	x0, 3f8d8 <__gmpn_toom63_mul@@Base+0x3d4>
   3f8a4:	ldur	x8, [x29, #-8]
   3f8a8:	mov	w23, #0x1                   	// #1
   3f8ac:	cmp	x8, x26
   3f8b0:	b.gt	3f9a4 <__gmpn_toom63_mul@@Base+0x4a0>
   3f8b4:	lsl	x9, x8, #3
   3f8b8:	ldr	x10, [x25, x9]
   3f8bc:	add	x8, x8, #0x1
   3f8c0:	adds	x10, x10, #0x1
   3f8c4:	str	x10, [x28, x9]
   3f8c8:	b.cs	3f8ac <__gmpn_toom63_mul@@Base+0x3a8>  // b.hs, b.nlast
   3f8cc:	b	3f8d8 <__gmpn_toom63_mul@@Base+0x3d4>
   3f8d0:	ldr	x22, [sp, #48]
   3f8d4:	mov	x8, xzr
   3f8d8:	cmp	x28, x25
   3f8dc:	mov	x23, xzr
   3f8e0:	b.eq	3f9a4 <__gmpn_toom63_mul@@Base+0x4a0>  // b.none
   3f8e4:	cmp	x8, x26
   3f8e8:	b.gt	3f9a4 <__gmpn_toom63_mul@@Base+0x4a0>
   3f8ec:	sub	x9, x26, x8
   3f8f0:	add	x9, x9, #0x1
   3f8f4:	cmp	x9, #0x4
   3f8f8:	b.cs	3f904 <__gmpn_toom63_mul@@Base+0x400>  // b.hs, b.nlast
   3f8fc:	mov	x10, x8
   3f900:	b	3f988 <__gmpn_toom63_mul@@Base+0x484>
   3f904:	ldur	x12, [x29, #-16]
   3f908:	mov	w10, #0x6                   	// #6
   3f90c:	madd	x10, x26, x10, x8
   3f910:	add	x11, x25, x21, lsl #3
   3f914:	add	x10, x12, x10, lsl #3
   3f918:	add	x10, x10, #0x40
   3f91c:	cmp	x10, x11
   3f920:	add	x13, x25, x8, lsl #3
   3f924:	b.cs	3f950 <__gmpn_toom63_mul@@Base+0x44c>  // b.hs, b.nlast
   3f928:	mov	w10, #0x38                  	// #56
   3f92c:	madd	x10, x26, x10, x12
   3f930:	add	x10, x10, #0x48
   3f934:	cmp	x13, x10
   3f938:	b.cs	3f950 <__gmpn_toom63_mul@@Base+0x44c>  // b.hs, b.nlast
   3f93c:	mov	x10, x8
   3f940:	b	3f988 <__gmpn_toom63_mul@@Base+0x484>
   3f944:	str	wzr, [sp, #8]
   3f948:	mov	w23, wzr
   3f94c:	b	3f7bc <__gmpn_toom63_mul@@Base+0x2b8>
   3f950:	and	x11, x9, #0xfffffffffffffffc
   3f954:	mov	x12, xzr
   3f958:	add	x10, x8, x11
   3f95c:	add	x13, x13, #0x10
   3f960:	ldp	q0, q1, [x13, #-16]
   3f964:	add	x14, x8, x12
   3f968:	add	x12, x12, #0x4
   3f96c:	add	x14, x28, x14, lsl #3
   3f970:	cmp	x12, x11
   3f974:	add	x13, x13, #0x20
   3f978:	stp	q0, q1, [x14]
   3f97c:	b.ne	3f960 <__gmpn_toom63_mul@@Base+0x45c>  // b.any
   3f980:	cmp	x9, x11
   3f984:	b.eq	3f9a0 <__gmpn_toom63_mul@@Base+0x49c>  // b.none
   3f988:	lsl	x8, x10, #3
   3f98c:	ldr	x9, [x25, x8]
   3f990:	cmp	x10, x26
   3f994:	add	x10, x10, #0x1
   3f998:	str	x9, [x28, x8]
   3f99c:	b.ne	3f988 <__gmpn_toom63_mul@@Base+0x484>  // b.any
   3f9a0:	mov	x23, xzr
   3f9a4:	mov	x0, x27
   3f9a8:	mov	x1, x28
   3f9ac:	mov	x2, x22
   3f9b0:	mov	x3, x21
   3f9b4:	bl	ca70 <__gmpn_add_n@plt>
   3f9b8:	add	x8, x0, x23
   3f9bc:	str	x8, [x27, x21, lsl #3]
   3f9c0:	cbz	x23, 3fae8 <__gmpn_toom63_mul@@Base+0x5e4>
   3f9c4:	mov	x0, x19
   3f9c8:	mov	x1, x28
   3f9cc:	mov	x2, x22
   3f9d0:	mov	x3, x21
   3f9d4:	str	w24, [sp, #12]
   3f9d8:	bl	c2d0 <__gmpn_sub_n@plt>
   3f9dc:	sub	x8, x23, x0
   3f9e0:	str	x8, [x19, x21, lsl #3]
   3f9e4:	mov	x23, x21
   3f9e8:	ldur	x24, [x29, #-40]
   3f9ec:	ldr	x21, [sp, #64]
   3f9f0:	mov	x0, x20
   3f9f4:	mov	x2, x19
   3f9f8:	mov	x1, x24
   3f9fc:	mov	x3, x21
   3fa00:	str	x19, [sp, #16]
   3fa04:	bl	c990 <__gmpn_mul_n@plt>
   3fa08:	ldur	x19, [x29, #-16]
   3fa0c:	ldur	x22, [x29, #-48]
   3fa10:	mov	x2, x27
   3fa14:	mov	x3, x21
   3fa18:	mov	x0, x19
   3fa1c:	mov	x1, x22
   3fa20:	bl	c990 <__gmpn_mul_n@plt>
   3fa24:	ldr	x1, [sp, #40]
   3fa28:	ldr	w3, [sp, #12]
   3fa2c:	mov	x0, x19
   3fa30:	mov	x2, x20
   3fa34:	mov	x4, x23
   3fa38:	mov	w5, wzr
   3fa3c:	mov	w6, wzr
   3fa40:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fa44:	ldp	x3, x5, [x29, #-32]
   3fa48:	mov	w2, #0x5                   	// #5
   3fa4c:	mov	x0, x22
   3fa50:	mov	x1, x24
   3fa54:	mov	x4, x23
   3fa58:	mov	x6, x20
   3fa5c:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3fa60:	ldr	x1, [sp, #48]
   3fa64:	str	w0, [sp, #12]
   3fa68:	mov	w3, #0x1                   	// #1
   3fa6c:	mov	x0, x20
   3fa70:	mov	x2, x23
   3fa74:	bl	c180 <__gmpn_lshift@plt>
   3fa78:	ldur	x22, [x29, #-8]
   3fa7c:	ldur	x1, [x29, #-56]
   3fa80:	str	x0, [x20, x23, lsl #3]
   3fa84:	mov	w3, #0x2                   	// #2
   3fa88:	mov	x0, x27
   3fa8c:	mov	x2, x22
   3fa90:	bl	c180 <__gmpn_lshift@plt>
   3fa94:	cmp	x23, x22
   3fa98:	str	x0, [x27, x22, lsl #3]
   3fa9c:	b.ne	3facc <__gmpn_toom63_mul@@Base+0x5c8>  // b.any
   3faa0:	mov	x0, x27
   3faa4:	mov	x1, x27
   3faa8:	mov	x2, x25
   3faac:	mov	x3, x23
   3fab0:	bl	ca70 <__gmpn_add_n@plt>
   3fab4:	ldr	x8, [x27, x23, lsl #3]
   3fab8:	ldr	x24, [sp, #64]
   3fabc:	ldr	w14, [sp, #8]
   3fac0:	ldr	x19, [sp, #16]
   3fac4:	add	x8, x8, x0
   3fac8:	b	3fc34 <__gmpn_toom63_mul@@Base+0x730>
   3facc:	ldr	x24, [sp, #64]
   3fad0:	ldr	x19, [sp, #16]
   3fad4:	adds	x22, x22, #0x1
   3fad8:	b.cc	3fb30 <__gmpn_toom63_mul@@Base+0x62c>  // b.lo, b.ul, b.last
   3fadc:	ldr	w14, [sp, #8]
   3fae0:	mov	x22, xzr
   3fae4:	b	3fb78 <__gmpn_toom63_mul@@Base+0x674>
   3fae8:	mov	x8, x21
   3faec:	subs	x8, x8, #0x1
   3faf0:	b.lt	3f9c4 <__gmpn_toom63_mul@@Base+0x4c0>  // b.tstop
   3faf4:	lsl	x9, x8, #3
   3faf8:	ldr	x10, [x28, x9]
   3fafc:	ldr	x9, [x22, x9]
   3fb00:	cmp	x10, x9
   3fb04:	b.eq	3faec <__gmpn_toom63_mul@@Base+0x5e8>  // b.none
   3fb08:	b.hi	3f9c4 <__gmpn_toom63_mul@@Base+0x4c0>  // b.pmore
   3fb0c:	mov	x0, x19
   3fb10:	mov	x1, x22
   3fb14:	mov	x2, x28
   3fb18:	mov	x3, x21
   3fb1c:	bl	c2d0 <__gmpn_sub_n@plt>
   3fb20:	mvn	w24, w24
   3fb24:	str	xzr, [x19, x21, lsl #3]
   3fb28:	str	w24, [sp, #12]
   3fb2c:	b	3f9e4 <__gmpn_toom63_mul@@Base+0x4e0>
   3fb30:	mov	x0, x27
   3fb34:	mov	x1, x25
   3fb38:	mov	x2, x27
   3fb3c:	mov	x3, x22
   3fb40:	bl	ca70 <__gmpn_add_n@plt>
   3fb44:	cbz	x0, 3fb74 <__gmpn_toom63_mul@@Base+0x670>
   3fb48:	ldr	w14, [sp, #8]
   3fb4c:	mov	w8, #0x1                   	// #1
   3fb50:	cmp	x22, x26
   3fb54:	b.gt	3fc34 <__gmpn_toom63_mul@@Base+0x730>
   3fb58:	lsl	x9, x22, #3
   3fb5c:	ldr	x10, [x25, x9]
   3fb60:	add	x22, x22, #0x1
   3fb64:	adds	x10, x10, #0x1
   3fb68:	str	x10, [x27, x9]
   3fb6c:	b.cs	3fb50 <__gmpn_toom63_mul@@Base+0x64c>  // b.hs, b.nlast
   3fb70:	b	3fb78 <__gmpn_toom63_mul@@Base+0x674>
   3fb74:	ldr	w14, [sp, #8]
   3fb78:	cmp	x27, x25
   3fb7c:	mov	x8, xzr
   3fb80:	b.eq	3fc34 <__gmpn_toom63_mul@@Base+0x730>  // b.none
   3fb84:	cmp	x22, x26
   3fb88:	b.gt	3fc34 <__gmpn_toom63_mul@@Base+0x730>
   3fb8c:	sub	x8, x26, x22
   3fb90:	add	x8, x8, #0x1
   3fb94:	cmp	x8, #0x4
   3fb98:	b.cs	3fba4 <__gmpn_toom63_mul@@Base+0x6a0>  // b.hs, b.nlast
   3fb9c:	mov	x9, x22
   3fba0:	b	3fc18 <__gmpn_toom63_mul@@Base+0x714>
   3fba4:	mov	w9, #0x6                   	// #6
   3fba8:	madd	x9, x26, x9, x22
   3fbac:	add	x9, x20, x9, lsl #3
   3fbb0:	add	x9, x9, #0x48
   3fbb4:	add	x10, x25, x23, lsl #3
   3fbb8:	cmp	x9, x10
   3fbbc:	add	x12, x25, x22, lsl #3
   3fbc0:	b.cs	3fbe0 <__gmpn_toom63_mul@@Base+0x6dc>  // b.hs, b.nlast
   3fbc4:	mov	w9, #0x38                  	// #56
   3fbc8:	madd	x9, x26, x9, x20
   3fbcc:	add	x9, x9, #0x50
   3fbd0:	cmp	x12, x9
   3fbd4:	b.cs	3fbe0 <__gmpn_toom63_mul@@Base+0x6dc>  // b.hs, b.nlast
   3fbd8:	mov	x9, x22
   3fbdc:	b	3fc18 <__gmpn_toom63_mul@@Base+0x714>
   3fbe0:	and	x10, x8, #0xfffffffffffffffc
   3fbe4:	mov	x11, xzr
   3fbe8:	add	x9, x22, x10
   3fbec:	add	x12, x12, #0x10
   3fbf0:	ldp	q0, q1, [x12, #-16]
   3fbf4:	add	x13, x22, x11
   3fbf8:	add	x11, x11, #0x4
   3fbfc:	add	x13, x27, x13, lsl #3
   3fc00:	cmp	x11, x10
   3fc04:	add	x12, x12, #0x20
   3fc08:	stp	q0, q1, [x13]
   3fc0c:	b.ne	3fbf0 <__gmpn_toom63_mul@@Base+0x6ec>  // b.any
   3fc10:	cmp	x8, x10
   3fc14:	b.eq	3fc30 <__gmpn_toom63_mul@@Base+0x72c>  // b.none
   3fc18:	lsl	x8, x9, #3
   3fc1c:	ldr	x10, [x25, x8]
   3fc20:	cmp	x9, x26
   3fc24:	add	x9, x9, #0x1
   3fc28:	str	x10, [x27, x8]
   3fc2c:	b.ne	3fc18 <__gmpn_toom63_mul@@Base+0x714>  // b.any
   3fc30:	mov	x8, xzr
   3fc34:	str	x8, [x27, x23, lsl #3]
   3fc38:	cbz	w14, 3fc8c <__gmpn_toom63_mul@@Base+0x788>
   3fc3c:	sub	x8, x24, #0x1
   3fc40:	mov	x9, x24
   3fc44:	sub	x9, x9, #0x1
   3fc48:	ldr	x10, [x27, x9, lsl #3]
   3fc4c:	ldr	x11, [x20, x8, lsl #3]
   3fc50:	cmp	x10, x11
   3fc54:	b.ne	3fc74 <__gmpn_toom63_mul@@Base+0x770>  // b.any
   3fc58:	add	x10, x8, #0x1
   3fc5c:	sub	x11, x8, #0x1
   3fc60:	cmp	x10, #0x1
   3fc64:	str	xzr, [x19, x8, lsl #3]
   3fc68:	mov	x8, x11
   3fc6c:	b.gt	3fc44 <__gmpn_toom63_mul@@Base+0x740>
   3fc70:	b	3fc8c <__gmpn_toom63_mul@@Base+0x788>
   3fc74:	add	x3, x8, #0x1
   3fc78:	mov	x0, x19
   3fc7c:	b.ls	3fd94 <__gmpn_toom63_mul@@Base+0x890>  // b.plast
   3fc80:	mov	x1, x27
   3fc84:	mov	x2, x20
   3fc88:	bl	c2d0 <__gmpn_sub_n@plt>
   3fc8c:	mov	w22, wzr
   3fc90:	mov	x0, x27
   3fc94:	mov	x1, x27
   3fc98:	mov	x2, x20
   3fc9c:	mov	x3, x24
   3fca0:	bl	ca70 <__gmpn_add_n@plt>
   3fca4:	mov	x21, x24
   3fca8:	ldur	x24, [x29, #-40]
   3fcac:	ldr	w8, [sp, #12]
   3fcb0:	mov	x0, x20
   3fcb4:	mov	x2, x19
   3fcb8:	mov	x1, x24
   3fcbc:	mov	x3, x21
   3fcc0:	eor	w22, w22, w8
   3fcc4:	bl	c990 <__gmpn_mul_n@plt>
   3fcc8:	ldur	x1, [x29, #-48]
   3fccc:	mov	x0, x24
   3fcd0:	mov	x2, x27
   3fcd4:	mov	x3, x21
   3fcd8:	bl	c990 <__gmpn_mul_n@plt>
   3fcdc:	ldr	x21, [sp, #56]
   3fce0:	ldr	x1, [sp, #40]
   3fce4:	mov	w5, #0x1                   	// #1
   3fce8:	mov	w6, #0x2                   	// #2
   3fcec:	mov	x0, x24
   3fcf0:	mov	x2, x20
   3fcf4:	mov	w3, w22
   3fcf8:	mov	x4, x21
   3fcfc:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fd00:	ldur	x24, [x29, #-32]
   3fd04:	mov	x0, x20
   3fd08:	mov	x2, x25
   3fd0c:	mov	x3, x21
   3fd10:	mov	x1, x24
   3fd14:	bl	c990 <__gmpn_mul_n@plt>
   3fd18:	mov	w8, #0x38                  	// #56
   3fd1c:	ldur	x19, [x29, #-24]
   3fd20:	ldur	x22, [x29, #-8]
   3fd24:	madd	x0, x21, x8, x20
   3fd28:	ldr	x8, [sp, #32]
   3fd2c:	cmp	x19, x22
   3fd30:	add	x3, x24, x8, lsl #3
   3fd34:	b.le	3fd4c <__gmpn_toom63_mul@@Base+0x848>
   3fd38:	mov	x1, x3
   3fd3c:	ldur	x3, [x29, #-56]
   3fd40:	mov	x2, x19
   3fd44:	mov	x4, x22
   3fd48:	b	3fd58 <__gmpn_toom63_mul@@Base+0x854>
   3fd4c:	ldur	x1, [x29, #-56]
   3fd50:	mov	x2, x22
   3fd54:	mov	x4, x19
   3fd58:	bl	ccd0 <__gmpn_mul@plt>
   3fd5c:	add	x4, x19, x22
   3fd60:	mov	x0, x20
   3fd64:	mov	x1, x21
   3fd68:	ldr	x2, [sp, #24]
   3fd6c:	ldur	x3, [x29, #-16]
   3fd70:	mov	x5, x28
   3fd74:	ldp	x20, x19, [sp, #208]
   3fd78:	ldp	x22, x21, [sp, #192]
   3fd7c:	ldp	x24, x23, [sp, #176]
   3fd80:	ldp	x26, x25, [sp, #160]
   3fd84:	ldp	x28, x27, [sp, #144]
   3fd88:	ldp	x29, x30, [sp, #128]
   3fd8c:	add	sp, sp, #0xe0
   3fd90:	b	c7b0 <__gmpn_toom_interpolate_8pts@plt>
   3fd94:	mov	x1, x20
   3fd98:	mov	x2, x27
   3fd9c:	bl	c2d0 <__gmpn_sub_n@plt>
   3fda0:	mov	w22, #0xffffffff            	// #-1
   3fda4:	b	3fc90 <__gmpn_toom63_mul@@Base+0x78c>

000000000003fda8 <__gmpn_toom44_mul@@Base>:
   3fda8:	sub	sp, sp, #0xf0
   3fdac:	stp	x26, x25, [sp, #176]
   3fdb0:	mov	x25, x2
   3fdb4:	stp	x20, x19, [sp, #224]
   3fdb8:	add	x19, x25, #0x3
   3fdbc:	stp	x22, x21, [sp, #208]
   3fdc0:	asr	x21, x19, #2
   3fdc4:	lsl	x10, x21, #1
   3fdc8:	add	x8, x0, x21, lsl #3
   3fdcc:	add	x9, x5, x21, lsl #6
   3fdd0:	stp	x4, x10, [sp, #64]
   3fdd4:	add	x10, x10, x21
   3fdd8:	stp	x28, x27, [sp, #160]
   3fddc:	add	x22, x8, #0x8
   3fde0:	sub	x8, x25, x10
   3fde4:	add	x28, x9, #0x28
   3fde8:	stp	x29, x30, [sp, #144]
   3fdec:	stp	x24, x23, [sp, #192]
   3fdf0:	add	x29, sp, #0x90
   3fdf4:	mov	x23, x5
   3fdf8:	mov	x26, x3
   3fdfc:	mov	x2, x1
   3fe00:	sub	x24, x4, x10
   3fe04:	mov	x1, x22
   3fe08:	mov	x3, x21
   3fe0c:	mov	x4, x8
   3fe10:	mov	x5, x28
   3fe14:	mov	x20, x0
   3fe18:	stur	x10, [x29, #-8]
   3fe1c:	stur	x2, [x29, #-48]
   3fe20:	stur	x8, [x29, #-24]
   3fe24:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3fe28:	and	x8, x19, #0xfffffffffffffffc
   3fe2c:	add	x9, x20, x21, lsl #4
   3fe30:	str	x8, [sp, #24]
   3fe34:	add	x8, x20, x8, lsl #3
   3fe38:	add	x27, x8, #0x10
   3fe3c:	add	x19, x9, #0x10
   3fe40:	str	w0, [sp, #20]
   3fe44:	mov	x0, x27
   3fe48:	mov	x1, x19
   3fe4c:	mov	x2, x26
   3fe50:	mov	x3, x21
   3fe54:	mov	x4, x24
   3fe58:	mov	x5, x28
   3fe5c:	str	x9, [sp, #40]
   3fe60:	stp	x24, x26, [x29, #-40]
   3fe64:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3fe68:	add	x24, x21, #0x1
   3fe6c:	str	w0, [sp, #16]
   3fe70:	cmp	x25, #0xbc
   3fe74:	mov	x0, x23
   3fe78:	mov	x1, x20
   3fe7c:	mov	x2, x24
   3fe80:	mov	x3, x27
   3fe84:	mov	x4, x24
   3fe88:	mov	x5, x28
   3fe8c:	stp	x24, x25, [x29, #-64]
   3fe90:	stur	x23, [x29, #-16]
   3fe94:	str	x22, [sp, #48]
   3fe98:	str	x19, [sp, #32]
   3fe9c:	b.le	3fed0 <__gmpn_toom44_mul@@Base+0x128>
   3fea0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3fea4:	ldr	x8, [sp, #72]
   3fea8:	mov	x1, x22
   3feac:	mov	x2, x24
   3feb0:	mov	x3, x19
   3feb4:	add	x8, x23, x8, lsl #3
   3feb8:	add	x0, x8, #0x8
   3febc:	mov	x4, x24
   3fec0:	mov	x5, x28
   3fec4:	str	x0, [sp, #56]
   3fec8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3fecc:	b	3fefc <__gmpn_toom44_mul@@Base+0x154>
   3fed0:	bl	d450 <__gmpn_toom22_mul@plt>
   3fed4:	ldr	x8, [sp, #72]
   3fed8:	mov	x1, x22
   3fedc:	mov	x2, x24
   3fee0:	mov	x3, x19
   3fee4:	add	x8, x23, x8, lsl #3
   3fee8:	add	x0, x8, #0x8
   3feec:	mov	x4, x24
   3fef0:	mov	x5, x28
   3fef4:	str	x0, [sp, #56]
   3fef8:	bl	d450 <__gmpn_toom22_mul@plt>
   3fefc:	ldur	x25, [x29, #-48]
   3ff00:	mov	x0, x20
   3ff04:	mov	x3, x21
   3ff08:	mov	x22, x28
   3ff0c:	add	x1, x25, x21, lsl #3
   3ff10:	mov	x2, x25
   3ff14:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ff18:	ldr	x19, [sp, #72]
   3ff1c:	mov	x23, x0
   3ff20:	mov	x0, x20
   3ff24:	mov	x2, x20
   3ff28:	add	x1, x25, x19, lsl #3
   3ff2c:	mov	x3, x21
   3ff30:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ff34:	ldur	x28, [x29, #-24]
   3ff38:	add	x26, x0, x23, lsl #1
   3ff3c:	subs	x24, x21, x28
   3ff40:	b.le	3ffac <__gmpn_toom44_mul@@Base+0x204>
   3ff44:	ldur	x8, [x29, #-8]
   3ff48:	mov	x0, x20
   3ff4c:	mov	x2, x20
   3ff50:	mov	x3, x28
   3ff54:	add	x1, x25, x8, lsl #3
   3ff58:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ff5c:	mov	x8, x28
   3ff60:	add	x23, x20, x8, lsl #3
   3ff64:	mov	x28, x0
   3ff68:	mov	w3, #0x1                   	// #1
   3ff6c:	mov	x0, x23
   3ff70:	mov	x1, x23
   3ff74:	mov	x2, x24
   3ff78:	bl	c180 <__gmpn_lshift@plt>
   3ff7c:	add	x8, x0, x26, lsl #1
   3ff80:	str	x8, [x20, x21, lsl #3]
   3ff84:	ldr	x8, [x23]
   3ff88:	adds	x8, x8, x28
   3ff8c:	str	x8, [x23]
   3ff90:	b.cc	3ffcc <__gmpn_toom44_mul@@Base+0x224>  // b.lo, b.ul, b.last
   3ff94:	add	x8, x23, #0x8
   3ff98:	ldr	x9, [x8]
   3ff9c:	adds	x9, x9, #0x1
   3ffa0:	str	x9, [x8], #8
   3ffa4:	b.cs	3ff98 <__gmpn_toom44_mul@@Base+0x1f0>  // b.hs, b.nlast
   3ffa8:	b	3ffcc <__gmpn_toom44_mul@@Base+0x224>
   3ffac:	ldur	x8, [x29, #-8]
   3ffb0:	mov	x0, x20
   3ffb4:	mov	x2, x20
   3ffb8:	mov	x3, x21
   3ffbc:	add	x1, x25, x8, lsl #3
   3ffc0:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ffc4:	add	x8, x0, x26, lsl #1
   3ffc8:	str	x8, [x20, x21, lsl #3]
   3ffcc:	ldp	w9, w8, [sp, #16]
   3ffd0:	ldur	x26, [x29, #-32]
   3ffd4:	mov	x0, x27
   3ffd8:	mov	x3, x21
   3ffdc:	eor	w8, w9, w8
   3ffe0:	add	x1, x26, x21, lsl #3
   3ffe4:	mov	x2, x26
   3ffe8:	str	w8, [sp, #20]
   3ffec:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3fff0:	mov	x23, x0
   3fff4:	add	x1, x26, x19, lsl #3
   3fff8:	mov	x0, x27
   3fffc:	mov	x2, x27
   40000:	mov	x3, x21
   40004:	bl	cc40 <__gmpn_addlsh1_n@plt>
   40008:	ldur	x28, [x29, #-40]
   4000c:	add	x25, x0, x23, lsl #1
   40010:	subs	x24, x21, x28
   40014:	b.le	400a4 <__gmpn_toom44_mul@@Base+0x2fc>
   40018:	ldur	x8, [x29, #-8]
   4001c:	mov	x0, x27
   40020:	mov	x2, x27
   40024:	mov	x3, x28
   40028:	add	x1, x26, x8, lsl #3
   4002c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   40030:	ldur	x8, [x29, #-40]
   40034:	mov	x23, x0
   40038:	mov	w3, #0x1                   	// #1
   4003c:	mov	x2, x24
   40040:	add	x28, x27, x8, lsl #3
   40044:	mov	x0, x28
   40048:	mov	x1, x28
   4004c:	bl	c180 <__gmpn_lshift@plt>
   40050:	add	x8, x0, x25, lsl #1
   40054:	str	x8, [x27, x21, lsl #3]
   40058:	ldr	x8, [x28]
   4005c:	ldur	x19, [x29, #-24]
   40060:	ldr	x25, [sp, #48]
   40064:	ldr	x10, [sp, #24]
   40068:	adds	x8, x8, x23
   4006c:	str	x8, [x28]
   40070:	ldur	x28, [x29, #-40]
   40074:	b.cc	400d0 <__gmpn_toom44_mul@@Base+0x328>  // b.lo, b.ul, b.last
   40078:	ldr	x8, [sp, #64]
   4007c:	ldur	x9, [x29, #-8]
   40080:	add	x8, x8, x10
   40084:	sub	x8, x8, x9
   40088:	add	x8, x20, x8, lsl #3
   4008c:	add	x8, x8, #0x18
   40090:	ldr	x9, [x8]
   40094:	adds	x9, x9, #0x1
   40098:	str	x9, [x8], #8
   4009c:	b.cs	40090 <__gmpn_toom44_mul@@Base+0x2e8>  // b.hs, b.nlast
   400a0:	b	400d0 <__gmpn_toom44_mul@@Base+0x328>
   400a4:	ldur	x8, [x29, #-8]
   400a8:	mov	x0, x27
   400ac:	mov	x2, x27
   400b0:	mov	x3, x21
   400b4:	add	x1, x26, x8, lsl #3
   400b8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   400bc:	add	x8, x0, x25, lsl #1
   400c0:	ldur	x19, [x29, #-24]
   400c4:	ldr	x25, [sp, #48]
   400c8:	ldr	x10, [sp, #24]
   400cc:	str	x8, [x27, x21, lsl #3]
   400d0:	ldr	w8, [sp, #20]
   400d4:	ldp	x9, x26, [x29, #-56]
   400d8:	mov	x1, x20
   400dc:	and	w23, w8, #0x1
   400e0:	ldur	x8, [x29, #-16]
   400e4:	cmp	x9, #0xbc
   400e8:	add	x8, x8, x10, lsl #3
   400ec:	add	x0, x8, #0x10
   400f0:	stur	x0, [x29, #-40]
   400f4:	b.le	40110 <__gmpn_toom44_mul@@Base+0x368>
   400f8:	ldur	x2, [x29, #-64]
   400fc:	mov	x3, x27
   40100:	mov	x5, x22
   40104:	mov	x4, x2
   40108:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4010c:	b	40124 <__gmpn_toom44_mul@@Base+0x37c>
   40110:	ldur	x2, [x29, #-64]
   40114:	mov	x3, x27
   40118:	mov	x5, x22
   4011c:	mov	x4, x2
   40120:	bl	d450 <__gmpn_toom22_mul@plt>
   40124:	mov	x0, x20
   40128:	mov	x1, x25
   4012c:	mov	x2, x26
   40130:	mov	x3, x21
   40134:	mov	x4, x19
   40138:	mov	x5, x22
   4013c:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   40140:	ldr	x19, [sp, #32]
   40144:	ldur	x2, [x29, #-32]
   40148:	and	w8, w0, #0x2
   4014c:	mov	x0, x27
   40150:	mov	x1, x19
   40154:	mov	x3, x21
   40158:	mov	x4, x28
   4015c:	mov	x5, x22
   40160:	orr	w24, w8, w23
   40164:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   40168:	and	w8, w0, #0x2
   4016c:	eor	w8, w24, w8
   40170:	str	w8, [sp, #72]
   40174:	ldur	x8, [x29, #-16]
   40178:	ldur	x24, [x29, #-56]
   4017c:	add	x23, x21, x21, lsl #1
   40180:	mov	x1, x25
   40184:	add	x8, x8, x23, lsl #4
   40188:	cmp	x24, #0xbc
   4018c:	add	x0, x8, #0x18
   40190:	str	x0, [sp, #24]
   40194:	b.le	401f8 <__gmpn_toom44_mul@@Base+0x450>
   40198:	ldur	x25, [x29, #-64]
   4019c:	mov	x3, x19
   401a0:	mov	x5, x22
   401a4:	mov	x2, x25
   401a8:	mov	x4, x25
   401ac:	bl	c0a0 <__gmpn_toom33_mul@plt>
   401b0:	ldr	x0, [sp, #40]
   401b4:	mov	x1, x20
   401b8:	mov	x2, x25
   401bc:	mov	x3, x27
   401c0:	mov	x4, x25
   401c4:	mov	x5, x22
   401c8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   401cc:	cmp	x24, #0xc0
   401d0:	b.le	4022c <__gmpn_toom44_mul@@Base+0x484>
   401d4:	ldur	x19, [x29, #-32]
   401d8:	mov	x0, x20
   401dc:	mov	x1, x26
   401e0:	mov	x2, x21
   401e4:	mov	x3, x19
   401e8:	mov	x4, x21
   401ec:	mov	x5, x22
   401f0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   401f4:	b	4024c <__gmpn_toom44_mul@@Base+0x4a4>
   401f8:	ldur	x25, [x29, #-64]
   401fc:	mov	x3, x19
   40200:	mov	x5, x22
   40204:	mov	x2, x25
   40208:	mov	x4, x25
   4020c:	bl	d450 <__gmpn_toom22_mul@plt>
   40210:	ldr	x0, [sp, #40]
   40214:	mov	x1, x20
   40218:	mov	x2, x25
   4021c:	mov	x3, x27
   40220:	mov	x4, x25
   40224:	mov	x5, x22
   40228:	bl	d450 <__gmpn_toom22_mul@plt>
   4022c:	ldur	x19, [x29, #-32]
   40230:	mov	x0, x20
   40234:	mov	x1, x26
   40238:	mov	x2, x21
   4023c:	mov	x3, x19
   40240:	mov	x4, x21
   40244:	mov	x5, x22
   40248:	bl	d450 <__gmpn_toom22_mul@plt>
   4024c:	ldr	x8, [sp, #64]
   40250:	ldur	x25, [x29, #-24]
   40254:	cmp	x24, x8
   40258:	lsl	x8, x23, #1
   4025c:	add	x0, x20, x8, lsl #3
   40260:	b.le	40284 <__gmpn_toom44_mul@@Base+0x4dc>
   40264:	ldur	x8, [x29, #-8]
   40268:	mov	x2, x25
   4026c:	mov	x4, x28
   40270:	lsl	x8, x8, #3
   40274:	add	x1, x26, x8
   40278:	add	x3, x19, x8
   4027c:	bl	ccd0 <__gmpn_mul@plt>
   40280:	b	402b4 <__gmpn_toom44_mul@@Base+0x50c>
   40284:	ldur	x8, [x29, #-8]
   40288:	cmp	x25, #0x30
   4028c:	mov	x2, x25
   40290:	mov	x4, x25
   40294:	lsl	x8, x8, #3
   40298:	add	x1, x26, x8
   4029c:	add	x3, x19, x8
   402a0:	mov	x5, x22
   402a4:	b.le	402b0 <__gmpn_toom44_mul@@Base+0x508>
   402a8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   402ac:	b	402b4 <__gmpn_toom44_mul@@Base+0x50c>
   402b0:	bl	d450 <__gmpn_toom22_mul@plt>
   402b4:	ldr	w2, [sp, #72]
   402b8:	ldr	x3, [sp, #56]
   402bc:	ldr	x4, [sp, #24]
   402c0:	ldur	x5, [x29, #-16]
   402c4:	ldur	x6, [x29, #-40]
   402c8:	add	x7, x25, x28
   402cc:	mov	x0, x20
   402d0:	mov	x1, x21
   402d4:	str	x22, [sp]
   402d8:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   402dc:	ldp	x20, x19, [sp, #224]
   402e0:	ldp	x22, x21, [sp, #208]
   402e4:	ldp	x24, x23, [sp, #192]
   402e8:	ldp	x26, x25, [sp, #176]
   402ec:	ldp	x28, x27, [sp, #160]
   402f0:	ldp	x29, x30, [sp, #144]
   402f4:	add	sp, sp, #0xf0
   402f8:	ret

00000000000402fc <__gmpn_toom6h_mul@@Base>:
   402fc:	sub	sp, sp, #0xd0
   40300:	add	x8, x2, x2, lsl #4
   40304:	add	x9, x4, x4, lsl #3
   40308:	stp	x29, x30, [sp, #112]
   4030c:	stp	x20, x19, [sp, #192]
   40310:	add	x29, sp, #0x70
   40314:	cmp	x8, x9, lsl #1
   40318:	mov	x20, x0
   4031c:	stp	x28, x27, [sp, #128]
   40320:	stp	x26, x25, [sp, #144]
   40324:	stp	x24, x23, [sp, #160]
   40328:	stp	x22, x21, [sp, #176]
   4032c:	str	x5, [sp, #56]
   40330:	stp	x3, x1, [x29, #-16]
   40334:	b.ge	40d28 <__gmpn_toom6h_mul@@Base+0xa2c>  // b.tcont
   40338:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   4033c:	sub	x8, x2, #0x1
   40340:	movk	x9, #0xaaab
   40344:	umulh	x8, x8, x9
   40348:	lsr	x8, x8, #2
   4034c:	add	x23, x8, #0x1
   40350:	add	x8, x23, x23, lsl #2
   40354:	sub	x5, x2, x8
   40358:	sub	x8, x4, x8
   4035c:	str	wzr, [sp, #44]
   40360:	mov	w15, #0x5                   	// #5
   40364:	stur	x8, [x29, #-24]
   40368:	mov	w26, #0x5                   	// #5
   4036c:	add	x8, x23, x23, lsl #3
   40370:	lsl	x19, x8, #3
   40374:	ldur	x3, [x29, #-8]
   40378:	mov	w9, #0x38                  	// #56
   4037c:	add	x8, x20, x19
   40380:	add	x28, x8, #0x10
   40384:	madd	x27, x23, x9, x20
   40388:	mov	w6, #0x1                   	// #1
   4038c:	mov	x0, x28
   40390:	mov	x1, x27
   40394:	mov	w2, w15
   40398:	mov	x4, x23
   4039c:	mov	x7, x20
   403a0:	stur	x5, [x29, #-32]
   403a4:	stur	x15, [x29, #-48]
   403a8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   403ac:	ldr	x21, [sp, #56]
   403b0:	ldp	x5, x3, [x29, #-24]
   403b4:	add	x9, x20, x23, lsl #6
   403b8:	add	x25, x9, #0x8
   403bc:	add	x8, x21, x19
   403c0:	add	x24, x8, #0x18
   403c4:	mov	w22, w0
   403c8:	mov	w6, #0x1                   	// #1
   403cc:	mov	x0, x24
   403d0:	mov	x1, x25
   403d4:	mov	w2, w26
   403d8:	mov	x4, x23
   403dc:	mov	x7, x20
   403e0:	stur	x26, [x29, #-40]
   403e4:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   403e8:	eor	w8, w0, w22
   403ec:	str	w8, [sp, #32]
   403f0:	mov	w8, #0x50                  	// #80
   403f4:	cmp	x23, #0x2f
   403f8:	add	x19, x23, #0x1
   403fc:	madd	x8, x23, x8, x21
   40400:	b.le	40458 <__gmpn_toom6h_mul@@Base+0x15c>
   40404:	cmp	x23, #0x50
   40408:	b.le	4049c <__gmpn_toom6h_mul@@Base+0x1a0>
   4040c:	add	x26, x8, #0x20
   40410:	cmp	x23, #0xab
   40414:	mov	x22, x28
   40418:	mov	x0, x20
   4041c:	mov	x1, x27
   40420:	mov	x2, x19
   40424:	mov	x3, x25
   40428:	mov	x4, x19
   4042c:	mov	x5, x26
   40430:	b.le	404e0 <__gmpn_toom6h_mul@@Base+0x1e4>
   40434:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40438:	mov	x0, x21
   4043c:	mov	x1, x22
   40440:	mov	x2, x19
   40444:	mov	x3, x24
   40448:	mov	x4, x19
   4044c:	mov	x5, x26
   40450:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40454:	b	40500 <__gmpn_toom6h_mul@@Base+0x204>
   40458:	add	x26, x8, #0x20
   4045c:	mov	x0, x20
   40460:	mov	x1, x27
   40464:	mov	x2, x19
   40468:	mov	x3, x25
   4046c:	mov	x4, x19
   40470:	mov	x5, x26
   40474:	bl	d450 <__gmpn_toom22_mul@plt>
   40478:	mov	x0, x21
   4047c:	mov	x1, x28
   40480:	mov	x2, x19
   40484:	mov	x3, x24
   40488:	mov	x4, x19
   4048c:	mov	x5, x26
   40490:	bl	d450 <__gmpn_toom22_mul@plt>
   40494:	mov	x22, x28
   40498:	b	40500 <__gmpn_toom6h_mul@@Base+0x204>
   4049c:	add	x26, x8, #0x20
   404a0:	mov	x0, x20
   404a4:	mov	x1, x27
   404a8:	mov	x2, x19
   404ac:	mov	x3, x25
   404b0:	mov	x4, x19
   404b4:	mov	x5, x26
   404b8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   404bc:	mov	x0, x21
   404c0:	mov	x1, x28
   404c4:	mov	x2, x19
   404c8:	mov	x3, x24
   404cc:	mov	x4, x19
   404d0:	mov	x5, x26
   404d4:	mov	x22, x28
   404d8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   404dc:	b	40500 <__gmpn_toom6h_mul@@Base+0x204>
   404e0:	bl	c720 <__gmpn_toom44_mul@plt>
   404e4:	mov	x0, x21
   404e8:	mov	x1, x22
   404ec:	mov	x2, x19
   404f0:	mov	x3, x24
   404f4:	mov	x4, x19
   404f8:	mov	x5, x26
   404fc:	bl	c720 <__gmpn_toom44_mul@plt>
   40500:	ldr	w6, [sp, #44]
   40504:	ldr	w3, [sp, #32]
   40508:	mov	w1, #0x1                   	// #1
   4050c:	bfi	x1, x23, #1, #63
   40510:	add	w5, w6, #0x1
   40514:	mov	x0, x21
   40518:	mov	x2, x20
   4051c:	mov	x4, x23
   40520:	str	x1, [sp, #48]
   40524:	str	w5, [sp, #28]
   40528:	bl	c970 <__gmpn_toom_couple_handling@plt>
   4052c:	ldur	x3, [x29, #-8]
   40530:	ldur	x5, [x29, #-32]
   40534:	mov	x0, x22
   40538:	mov	x1, x27
   4053c:	ldur	x2, [x29, #-48]
   40540:	mov	x4, x23
   40544:	mov	x6, x20
   40548:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   4054c:	ldur	x2, [x29, #-40]
   40550:	mov	w26, w0
   40554:	mov	x0, x24
   40558:	mov	x1, x25
   4055c:	cmp	w2, #0x3
   40560:	b.eq	40d4c <__gmpn_toom6h_mul@@Base+0xa50>  // b.none
   40564:	ldp	x5, x3, [x29, #-24]
   40568:	mov	x4, x23
   4056c:	mov	x6, x20
   40570:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   40574:	eor	w8, w0, w26
   40578:	str	w8, [sp, #16]
   4057c:	mov	w8, #0x50                  	// #80
   40580:	cmp	x23, #0x2f
   40584:	madd	x8, x23, x8, x21
   40588:	b.le	405e4 <__gmpn_toom6h_mul@@Base+0x2e8>
   4058c:	cmp	x23, #0x50
   40590:	b.le	4062c <__gmpn_toom6h_mul@@Base+0x330>
   40594:	add	x26, x8, #0x20
   40598:	cmp	x23, #0xab
   4059c:	mov	x0, x20
   405a0:	mov	x1, x27
   405a4:	mov	x2, x19
   405a8:	mov	x3, x25
   405ac:	mov	x4, x19
   405b0:	mov	x5, x26
   405b4:	b.le	40674 <__gmpn_toom6h_mul@@Base+0x378>
   405b8:	bl	cc20 <__gmpn_toom6h_mul@plt>
   405bc:	add	x28, x23, x23, lsl #1
   405c0:	add	x8, x21, x28, lsl #3
   405c4:	add	x0, x8, #0x8
   405c8:	mov	x1, x22
   405cc:	mov	x2, x19
   405d0:	mov	x3, x24
   405d4:	mov	x4, x19
   405d8:	mov	x5, x26
   405dc:	bl	cc20 <__gmpn_toom6h_mul@plt>
   405e0:	b	4069c <__gmpn_toom6h_mul@@Base+0x3a0>
   405e4:	add	x26, x8, #0x20
   405e8:	mov	x0, x20
   405ec:	mov	x1, x27
   405f0:	mov	x2, x19
   405f4:	mov	x3, x25
   405f8:	mov	x4, x19
   405fc:	mov	x5, x26
   40600:	bl	d450 <__gmpn_toom22_mul@plt>
   40604:	add	x28, x23, x23, lsl #1
   40608:	add	x8, x21, x28, lsl #3
   4060c:	add	x0, x8, #0x8
   40610:	mov	x1, x22
   40614:	mov	x2, x19
   40618:	mov	x3, x24
   4061c:	mov	x4, x19
   40620:	mov	x5, x26
   40624:	bl	d450 <__gmpn_toom22_mul@plt>
   40628:	b	4069c <__gmpn_toom6h_mul@@Base+0x3a0>
   4062c:	add	x26, x8, #0x20
   40630:	mov	x0, x20
   40634:	mov	x1, x27
   40638:	mov	x2, x19
   4063c:	mov	x3, x25
   40640:	mov	x4, x19
   40644:	mov	x5, x26
   40648:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4064c:	add	x28, x23, x23, lsl #1
   40650:	add	x8, x21, x28, lsl #3
   40654:	add	x0, x8, #0x8
   40658:	mov	x1, x22
   4065c:	mov	x2, x19
   40660:	mov	x3, x24
   40664:	mov	x4, x19
   40668:	mov	x5, x26
   4066c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40670:	b	4069c <__gmpn_toom6h_mul@@Base+0x3a0>
   40674:	bl	c720 <__gmpn_toom44_mul@plt>
   40678:	add	x28, x23, x23, lsl #1
   4067c:	add	x8, x21, x28, lsl #3
   40680:	add	x0, x8, #0x8
   40684:	mov	x1, x22
   40688:	mov	x2, x19
   4068c:	mov	x3, x24
   40690:	mov	x4, x19
   40694:	mov	x5, x26
   40698:	bl	c720 <__gmpn_toom44_mul@plt>
   4069c:	ldr	x1, [sp, #48]
   406a0:	ldr	w3, [sp, #16]
   406a4:	ldur	x26, [x29, #-32]
   406a8:	add	x8, x21, x28, lsl #3
   406ac:	add	x0, x8, #0x8
   406b0:	mov	x2, x20
   406b4:	mov	x4, x23
   406b8:	mov	w5, wzr
   406bc:	mov	w6, wzr
   406c0:	str	x28, [sp, #8]
   406c4:	str	x0, [sp, #32]
   406c8:	bl	c970 <__gmpn_toom_couple_handling@plt>
   406cc:	ldur	x3, [x29, #-8]
   406d0:	mov	w6, #0x2                   	// #2
   406d4:	mov	x0, x22
   406d8:	mov	x1, x27
   406dc:	ldur	x2, [x29, #-48]
   406e0:	mov	x4, x23
   406e4:	mov	x5, x26
   406e8:	mov	x7, x20
   406ec:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   406f0:	ldp	x5, x3, [x29, #-24]
   406f4:	mov	w26, w0
   406f8:	mov	w6, #0x2                   	// #2
   406fc:	mov	x0, x24
   40700:	mov	x1, x25
   40704:	ldur	x2, [x29, #-40]
   40708:	mov	x4, x23
   4070c:	mov	x7, x20
   40710:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   40714:	eor	w8, w0, w26
   40718:	cmp	x23, #0x2f
   4071c:	str	w8, [sp, #4]
   40720:	mov	w8, #0x50                  	// #80
   40724:	b.le	4078c <__gmpn_toom6h_mul@@Base+0x490>
   40728:	cmp	x23, #0x50
   4072c:	b.le	407dc <__gmpn_toom6h_mul@@Base+0x4e0>
   40730:	ldr	x21, [sp, #56]
   40734:	cmp	x23, #0xab
   40738:	mov	x0, x20
   4073c:	mov	x1, x27
   40740:	madd	x8, x23, x8, x21
   40744:	add	x26, x8, #0x20
   40748:	mov	x2, x19
   4074c:	mov	x3, x25
   40750:	mov	x4, x19
   40754:	mov	x5, x26
   40758:	b.le	40830 <__gmpn_toom6h_mul@@Base+0x534>
   4075c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40760:	add	x8, x23, x23, lsl #1
   40764:	lsl	x28, x8, #1
   40768:	add	x8, x21, x8, lsl #4
   4076c:	add	x0, x8, #0x10
   40770:	mov	x1, x22
   40774:	mov	x2, x19
   40778:	mov	x3, x24
   4077c:	mov	x4, x19
   40780:	mov	x5, x26
   40784:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40788:	b	4085c <__gmpn_toom6h_mul@@Base+0x560>
   4078c:	madd	x8, x23, x8, x21
   40790:	add	x26, x8, #0x20
   40794:	mov	x0, x20
   40798:	mov	x1, x27
   4079c:	mov	x2, x19
   407a0:	mov	x3, x25
   407a4:	mov	x4, x19
   407a8:	mov	x5, x26
   407ac:	bl	d450 <__gmpn_toom22_mul@plt>
   407b0:	add	x8, x23, x23, lsl #1
   407b4:	lsl	x28, x8, #1
   407b8:	add	x8, x21, x8, lsl #4
   407bc:	add	x0, x8, #0x10
   407c0:	mov	x1, x22
   407c4:	mov	x2, x19
   407c8:	mov	x3, x24
   407cc:	mov	x4, x19
   407d0:	mov	x5, x26
   407d4:	bl	d450 <__gmpn_toom22_mul@plt>
   407d8:	b	4085c <__gmpn_toom6h_mul@@Base+0x560>
   407dc:	ldr	x21, [sp, #56]
   407e0:	mov	x0, x20
   407e4:	mov	x1, x27
   407e8:	mov	x2, x19
   407ec:	madd	x8, x23, x8, x21
   407f0:	add	x26, x8, #0x20
   407f4:	mov	x3, x25
   407f8:	mov	x4, x19
   407fc:	mov	x5, x26
   40800:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40804:	add	x8, x23, x23, lsl #1
   40808:	lsl	x28, x8, #1
   4080c:	add	x8, x21, x8, lsl #4
   40810:	add	x0, x8, #0x10
   40814:	mov	x1, x22
   40818:	mov	x2, x19
   4081c:	mov	x3, x24
   40820:	mov	x4, x19
   40824:	mov	x5, x26
   40828:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4082c:	b	4085c <__gmpn_toom6h_mul@@Base+0x560>
   40830:	bl	c720 <__gmpn_toom44_mul@plt>
   40834:	add	x8, x23, x23, lsl #1
   40838:	lsl	x28, x8, #1
   4083c:	add	x8, x21, x8, lsl #4
   40840:	add	x0, x8, #0x10
   40844:	mov	x1, x22
   40848:	mov	x2, x19
   4084c:	mov	x3, x24
   40850:	mov	x4, x19
   40854:	mov	x5, x26
   40858:	bl	c720 <__gmpn_toom44_mul@plt>
   4085c:	ldr	x1, [sp, #48]
   40860:	ldr	w3, [sp, #4]
   40864:	ldur	x26, [x29, #-32]
   40868:	add	x8, x21, x28, lsl #3
   4086c:	add	x0, x8, #0x10
   40870:	mov	w5, #0x2                   	// #2
   40874:	mov	w6, #0x4                   	// #4
   40878:	mov	x2, x20
   4087c:	mov	x4, x23
   40880:	str	x0, [sp, #16]
   40884:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40888:	ldur	x3, [x29, #-8]
   4088c:	mov	w6, #0x2                   	// #2
   40890:	mov	x0, x22
   40894:	mov	x1, x27
   40898:	ldur	x2, [x29, #-48]
   4089c:	mov	x4, x23
   408a0:	mov	x5, x26
   408a4:	mov	x7, x20
   408a8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   408ac:	ldp	x5, x3, [x29, #-24]
   408b0:	mov	w26, w0
   408b4:	mov	w6, #0x2                   	// #2
   408b8:	mov	x0, x24
   408bc:	mov	x1, x25
   408c0:	ldur	x2, [x29, #-40]
   408c4:	mov	x4, x23
   408c8:	mov	x7, x20
   408cc:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   408d0:	cmp	x23, #0x2f
   408d4:	mov	x28, x22
   408d8:	mov	x22, x27
   408dc:	mov	x27, x25
   408e0:	eor	w25, w0, w26
   408e4:	str	w25, [sp, #4]
   408e8:	b.le	40950 <__gmpn_toom6h_mul@@Base+0x654>
   408ec:	cmp	x23, #0x50
   408f0:	b.le	409a4 <__gmpn_toom6h_mul@@Base+0x6a8>
   408f4:	ldp	x25, x9, [sp, #48]
   408f8:	mov	w8, #0x50                  	// #80
   408fc:	mov	x21, x24
   40900:	cmp	x23, #0xab
   40904:	madd	x8, x23, x8, x9
   40908:	add	x26, x8, #0x20
   4090c:	mov	x0, x20
   40910:	mov	x1, x22
   40914:	mov	x2, x19
   40918:	mov	x3, x27
   4091c:	mov	x4, x19
   40920:	mov	x5, x26
   40924:	b.le	409fc <__gmpn_toom6h_mul@@Base+0x700>
   40928:	bl	cc20 <__gmpn_toom6h_mul@plt>
   4092c:	ldr	x24, [sp, #8]
   40930:	mov	x1, x28
   40934:	mov	x2, x19
   40938:	mov	x3, x21
   4093c:	add	x0, x20, x24, lsl #3
   40940:	mov	x4, x19
   40944:	mov	x5, x26
   40948:	bl	cc20 <__gmpn_toom6h_mul@plt>
   4094c:	b	40a20 <__gmpn_toom6h_mul@@Base+0x724>
   40950:	mov	w8, #0x50                  	// #80
   40954:	madd	x8, x23, x8, x21
   40958:	add	x26, x8, #0x20
   4095c:	mov	x0, x20
   40960:	mov	x1, x22
   40964:	mov	x2, x19
   40968:	mov	x3, x27
   4096c:	mov	x4, x19
   40970:	mov	x5, x26
   40974:	bl	d450 <__gmpn_toom22_mul@plt>
   40978:	mov	x21, x24
   4097c:	ldr	x24, [sp, #8]
   40980:	mov	x1, x28
   40984:	mov	x2, x19
   40988:	mov	x3, x21
   4098c:	add	x0, x20, x24, lsl #3
   40990:	mov	x4, x19
   40994:	mov	x5, x26
   40998:	bl	d450 <__gmpn_toom22_mul@plt>
   4099c:	ldr	x25, [sp, #48]
   409a0:	b	40a20 <__gmpn_toom6h_mul@@Base+0x724>
   409a4:	ldr	x9, [sp, #56]
   409a8:	mov	w8, #0x50                  	// #80
   409ac:	mov	x0, x20
   409b0:	mov	x1, x22
   409b4:	madd	x8, x23, x8, x9
   409b8:	add	x26, x8, #0x20
   409bc:	mov	x2, x19
   409c0:	mov	x3, x27
   409c4:	mov	x4, x19
   409c8:	mov	x5, x26
   409cc:	bl	c0a0 <__gmpn_toom33_mul@plt>
   409d0:	mov	x21, x24
   409d4:	ldr	x24, [sp, #8]
   409d8:	mov	x1, x28
   409dc:	mov	x2, x19
   409e0:	mov	x3, x21
   409e4:	add	x0, x20, x24, lsl #3
   409e8:	mov	x4, x19
   409ec:	mov	x5, x26
   409f0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   409f4:	ldr	x25, [sp, #48]
   409f8:	b	40a20 <__gmpn_toom6h_mul@@Base+0x724>
   409fc:	bl	c720 <__gmpn_toom44_mul@plt>
   40a00:	ldr	x24, [sp, #8]
   40a04:	mov	x1, x28
   40a08:	mov	x2, x19
   40a0c:	mov	x3, x21
   40a10:	add	x0, x20, x24, lsl #3
   40a14:	mov	x4, x19
   40a18:	mov	x5, x26
   40a1c:	bl	c720 <__gmpn_toom44_mul@plt>
   40a20:	ldr	w8, [sp, #28]
   40a24:	ldr	w3, [sp, #4]
   40a28:	add	x0, x20, x24, lsl #3
   40a2c:	mov	x1, x25
   40a30:	lsl	w5, w8, #1
   40a34:	ldr	w8, [sp, #44]
   40a38:	mov	x2, x20
   40a3c:	mov	x4, x23
   40a40:	lsl	w6, w8, #1
   40a44:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40a48:	ldur	x3, [x29, #-8]
   40a4c:	ldur	x5, [x29, #-32]
   40a50:	mov	x0, x28
   40a54:	mov	x1, x22
   40a58:	ldur	x2, [x29, #-48]
   40a5c:	mov	x4, x23
   40a60:	mov	x6, x20
   40a64:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   40a68:	ldp	x5, x3, [x29, #-24]
   40a6c:	mov	w26, w0
   40a70:	mov	x0, x21
   40a74:	mov	x1, x27
   40a78:	ldur	x2, [x29, #-40]
   40a7c:	mov	x4, x23
   40a80:	mov	x6, x20
   40a84:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   40a88:	cmp	x23, #0x2f
   40a8c:	eor	w10, w0, w26
   40a90:	mov	x24, x21
   40a94:	b.le	40b48 <__gmpn_toom6h_mul@@Base+0x84c>
   40a98:	ldr	x9, [sp, #56]
   40a9c:	mov	x11, x20
   40aa0:	cmp	x23, #0x51
   40aa4:	b.lt	40bb8 <__gmpn_toom6h_mul@@Base+0x8bc>  // b.tstop
   40aa8:	mov	w8, #0x50                  	// #80
   40aac:	madd	x8, x23, x8, x9
   40ab0:	add	x26, x8, #0x20
   40ab4:	mov	x20, x22
   40ab8:	mov	w21, w10
   40abc:	cmp	x23, #0xab
   40ac0:	mov	x22, x11
   40ac4:	mov	x0, x11
   40ac8:	mov	x1, x20
   40acc:	mov	x2, x19
   40ad0:	mov	x3, x27
   40ad4:	mov	x4, x19
   40ad8:	mov	x5, x26
   40adc:	b.le	40c50 <__gmpn_toom6h_mul@@Base+0x954>
   40ae0:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40ae4:	mov	x0, x20
   40ae8:	mov	x1, x28
   40aec:	mov	x2, x19
   40af0:	mov	x3, x24
   40af4:	mov	x4, x19
   40af8:	mov	x5, x26
   40afc:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40b00:	ldr	x1, [sp, #48]
   40b04:	mov	w5, #0x1                   	// #1
   40b08:	mov	w6, #0x2                   	// #2
   40b0c:	mov	x0, x20
   40b10:	mov	x2, x22
   40b14:	mov	w3, w21
   40b18:	mov	x4, x23
   40b1c:	mov	x20, x22
   40b20:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40b24:	cmp	x23, #0xac
   40b28:	b.eq	40c9c <__gmpn_toom6h_mul@@Base+0x9a0>  // b.none
   40b2c:	ldp	x3, x1, [x29, #-16]
   40b30:	mov	x0, x20
   40b34:	mov	x2, x23
   40b38:	mov	x4, x23
   40b3c:	mov	x5, x24
   40b40:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40b44:	b	40cd0 <__gmpn_toom6h_mul@@Base+0x9d4>
   40b48:	ldr	x9, [sp, #56]
   40b4c:	mov	w8, #0x50                  	// #80
   40b50:	mov	x0, x20
   40b54:	mov	x1, x22
   40b58:	madd	x8, x23, x8, x9
   40b5c:	add	x26, x8, #0x20
   40b60:	mov	x2, x19
   40b64:	mov	x3, x27
   40b68:	mov	x4, x19
   40b6c:	mov	x5, x26
   40b70:	mov	w21, w10
   40b74:	bl	d450 <__gmpn_toom22_mul@plt>
   40b78:	mov	x0, x22
   40b7c:	mov	x1, x28
   40b80:	mov	x2, x19
   40b84:	mov	x3, x24
   40b88:	mov	x4, x19
   40b8c:	mov	x5, x26
   40b90:	bl	d450 <__gmpn_toom22_mul@plt>
   40b94:	mov	w5, #0x1                   	// #1
   40b98:	mov	w6, #0x2                   	// #2
   40b9c:	mov	x0, x22
   40ba0:	mov	x1, x25
   40ba4:	mov	x2, x20
   40ba8:	mov	w3, w21
   40bac:	mov	x4, x23
   40bb0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40bb4:	b	40c34 <__gmpn_toom6h_mul@@Base+0x938>
   40bb8:	mov	w8, #0x50                  	// #80
   40bbc:	madd	x8, x23, x8, x9
   40bc0:	add	x26, x8, #0x20
   40bc4:	mov	x0, x11
   40bc8:	mov	x1, x22
   40bcc:	mov	x2, x19
   40bd0:	mov	x3, x27
   40bd4:	mov	x4, x19
   40bd8:	mov	x5, x26
   40bdc:	mov	x21, x11
   40be0:	mov	x20, x22
   40be4:	mov	w22, w10
   40be8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40bec:	mov	x0, x20
   40bf0:	mov	x1, x28
   40bf4:	mov	x2, x19
   40bf8:	mov	x3, x24
   40bfc:	mov	x4, x19
   40c00:	mov	x5, x26
   40c04:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40c08:	ldr	x1, [sp, #48]
   40c0c:	mov	w5, #0x1                   	// #1
   40c10:	mov	w6, #0x2                   	// #2
   40c14:	mov	x0, x20
   40c18:	mov	x2, x21
   40c1c:	mov	w3, w22
   40c20:	mov	x4, x23
   40c24:	mov	x20, x21
   40c28:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40c2c:	cmp	x23, #0x30
   40c30:	b.gt	40cb8 <__gmpn_toom6h_mul@@Base+0x9bc>
   40c34:	ldp	x3, x1, [x29, #-16]
   40c38:	mov	x0, x20
   40c3c:	mov	x2, x23
   40c40:	mov	x4, x23
   40c44:	mov	x5, x24
   40c48:	bl	d450 <__gmpn_toom22_mul@plt>
   40c4c:	b	40cd0 <__gmpn_toom6h_mul@@Base+0x9d4>
   40c50:	bl	c720 <__gmpn_toom44_mul@plt>
   40c54:	mov	x0, x20
   40c58:	mov	x1, x28
   40c5c:	mov	x2, x19
   40c60:	mov	x3, x24
   40c64:	mov	x4, x19
   40c68:	mov	x5, x26
   40c6c:	bl	c720 <__gmpn_toom44_mul@plt>
   40c70:	ldr	x1, [sp, #48]
   40c74:	mov	w5, #0x1                   	// #1
   40c78:	mov	w6, #0x2                   	// #2
   40c7c:	mov	x0, x20
   40c80:	mov	x2, x22
   40c84:	mov	w3, w21
   40c88:	mov	x4, x23
   40c8c:	mov	x20, x22
   40c90:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40c94:	cmp	x23, #0x51
   40c98:	b.le	40cb8 <__gmpn_toom6h_mul@@Base+0x9bc>
   40c9c:	ldp	x3, x1, [x29, #-16]
   40ca0:	mov	x0, x20
   40ca4:	mov	x2, x23
   40ca8:	mov	x4, x23
   40cac:	mov	x5, x24
   40cb0:	bl	c720 <__gmpn_toom44_mul@plt>
   40cb4:	b	40cd0 <__gmpn_toom6h_mul@@Base+0x9d4>
   40cb8:	ldp	x3, x1, [x29, #-16]
   40cbc:	mov	x0, x20
   40cc0:	mov	x2, x23
   40cc4:	mov	x4, x23
   40cc8:	mov	x5, x24
   40ccc:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40cd0:	ldr	w21, [sp, #44]
   40cd4:	ldp	x22, x25, [x29, #-32]
   40cd8:	ldur	x9, [x29, #-40]
   40cdc:	ldr	x19, [sp, #32]
   40ce0:	ldr	x26, [sp, #16]
   40ce4:	cbnz	w21, 40d60 <__gmpn_toom6h_mul@@Base+0xa64>
   40ce8:	add	x5, x25, x22
   40cec:	mov	x0, x20
   40cf0:	mov	x1, x26
   40cf4:	mov	x2, x19
   40cf8:	ldr	x3, [sp, #56]
   40cfc:	mov	x4, x23
   40d00:	mov	w6, w21
   40d04:	mov	x7, x24
   40d08:	ldp	x20, x19, [sp, #192]
   40d0c:	ldp	x22, x21, [sp, #176]
   40d10:	ldp	x24, x23, [sp, #160]
   40d14:	ldp	x26, x25, [sp, #144]
   40d18:	ldp	x28, x27, [sp, #128]
   40d1c:	ldp	x29, x30, [sp, #112]
   40d20:	add	sp, sp, #0xd0
   40d24:	b	bfb0 <__gmpn_toom_interpolate_12pts@plt>
   40d28:	mov	w9, #0x5a                  	// #90
   40d2c:	mov	w10, #0x77                  	// #119
   40d30:	mul	x9, x2, x9
   40d34:	mul	x10, x4, x10
   40d38:	cmp	x9, x10
   40d3c:	b.ge	40d9c <__gmpn_toom6h_mul@@Base+0xaa0>  // b.tcont
   40d40:	mov	w8, #0x6                   	// #6
   40d44:	mov	w9, #0x7                   	// #7
   40d48:	b	40e20 <__gmpn_toom6h_mul@@Base+0xb24>
   40d4c:	ldp	x4, x2, [x29, #-24]
   40d50:	mov	x3, x23
   40d54:	mov	x5, x20
   40d58:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   40d5c:	b	40574 <__gmpn_toom6h_mul@@Base+0x278>
   40d60:	mov	w8, #0x58                  	// #88
   40d64:	cmp	x22, x25
   40d68:	madd	x0, x23, x8, x20
   40d6c:	b.le	40dc0 <__gmpn_toom6h_mul@@Base+0xac4>
   40d70:	ldur	x8, [x29, #-48]
   40d74:	ldur	x10, [x29, #-8]
   40d78:	mul	x9, x23, x9
   40d7c:	mov	x2, x22
   40d80:	mov	w8, w8
   40d84:	mul	x8, x23, x8
   40d88:	add	x1, x10, x8, lsl #3
   40d8c:	ldur	x8, [x29, #-16]
   40d90:	mov	x4, x25
   40d94:	add	x3, x8, x9, lsl #3
   40d98:	b	40de8 <__gmpn_toom6h_mul@@Base+0xaec>
   40d9c:	mov	w9, #0x55                  	// #85
   40da0:	mov	w10, #0x7e                  	// #126
   40da4:	mul	x9, x2, x9
   40da8:	mul	x10, x4, x10
   40dac:	cmp	x9, x10
   40db0:	b.ge	40df0 <__gmpn_toom6h_mul@@Base+0xaf4>  // b.tcont
   40db4:	mov	w8, #0x5                   	// #5
   40db8:	mov	w9, #0x7                   	// #7
   40dbc:	b	40e20 <__gmpn_toom6h_mul@@Base+0xb24>
   40dc0:	mul	x8, x23, x9
   40dc4:	ldur	x9, [x29, #-48]
   40dc8:	ldur	x10, [x29, #-16]
   40dcc:	mov	x2, x25
   40dd0:	mov	x4, x22
   40dd4:	mov	w9, w9
   40dd8:	add	x1, x10, x8, lsl #3
   40ddc:	mul	x8, x23, x9
   40de0:	ldur	x9, [x29, #-8]
   40de4:	add	x3, x9, x8, lsl #3
   40de8:	bl	ccd0 <__gmpn_mul@plt>
   40dec:	b	40ce8 <__gmpn_toom6h_mul@@Base+0x9ec>
   40df0:	add	x9, x2, x2, lsl #3
   40df4:	lsl	x9, x9, #1
   40df8:	add	x10, x4, x4, lsl #4
   40dfc:	cmp	x9, x10, lsl #1
   40e00:	mov	w9, #0x8                   	// #8
   40e04:	b.ge	40e10 <__gmpn_toom6h_mul@@Base+0xb14>  // b.tcont
   40e08:	mov	w8, #0x5                   	// #5
   40e0c:	b	40e20 <__gmpn_toom6h_mul@@Base+0xb24>
   40e10:	add	x10, x4, x4, lsl #3
   40e14:	cmp	x8, x10, lsl #2
   40e18:	cinc	w9, w9, ge  // ge = tcont
   40e1c:	mov	w8, #0x4                   	// #4
   40e20:	mov	w12, w9
   40e24:	mul	x11, x8, x2
   40e28:	mul	x13, x12, x4
   40e2c:	cmp	x11, x13
   40e30:	csel	x11, x4, x2, lt  // lt = tstop
   40e34:	csel	x12, x8, x12, lt  // lt = tstop
   40e38:	sub	x11, x11, #0x1
   40e3c:	sub	w15, w9, #0x1
   40e40:	udiv	x11, x11, x12
   40e44:	sub	w26, w8, #0x1
   40e48:	sxtw	x14, w15
   40e4c:	add	x23, x11, #0x1
   40e50:	eor	w10, w8, w9
   40e54:	msub	x5, x23, x14, x2
   40e58:	msub	x21, x23, x26, x4
   40e5c:	stur	x21, [x29, #-24]
   40e60:	tbnz	w10, #0, 40e6c <__gmpn_toom6h_mul@@Base+0xb70>
   40e64:	str	wzr, [sp, #44]
   40e68:	b	4036c <__gmpn_toom6h_mul@@Base+0x70>
   40e6c:	cmp	x5, #0x0
   40e70:	b.le	40e88 <__gmpn_toom6h_mul@@Base+0xb8c>
   40e74:	cmp	x21, #0x0
   40e78:	b.le	40e98 <__gmpn_toom6h_mul@@Base+0xb9c>
   40e7c:	mov	w8, #0x1                   	// #1
   40e80:	str	w8, [sp, #44]
   40e84:	b	4036c <__gmpn_toom6h_mul@@Base+0x70>
   40e88:	str	wzr, [sp, #44]
   40e8c:	sub	w15, w9, #0x2
   40e90:	add	x5, x5, x23
   40e94:	b	4036c <__gmpn_toom6h_mul@@Base+0x70>
   40e98:	mov	x9, x21
   40e9c:	sub	w26, w8, #0x2
   40ea0:	add	x9, x21, x23
   40ea4:	str	wzr, [sp, #44]
   40ea8:	stur	x9, [x29, #-24]
   40eac:	b	4036c <__gmpn_toom6h_mul@@Base+0x70>

0000000000040eb0 <__gmpn_toom6_sqr@@Base>:
   40eb0:	sub	sp, sp, #0x90
   40eb4:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   40eb8:	sub	x8, x2, #0x1
   40ebc:	movk	x9, #0xaaab
   40ec0:	umulh	x8, x8, x9
   40ec4:	stp	x20, x19, [sp, #128]
   40ec8:	lsr	x19, x8, #2
   40ecc:	stp	x22, x21, [sp, #112]
   40ed0:	add	x22, x19, #0x1
   40ed4:	add	x9, x22, x22, lsl #3
   40ed8:	add	x8, x22, x22, lsl #2
   40edc:	lsl	x21, x9, #3
   40ee0:	mov	w10, #0x38                  	// #56
   40ee4:	sub	x5, x2, x8
   40ee8:	add	x8, x0, x21
   40eec:	stp	x28, x27, [sp, #64]
   40ef0:	stp	x26, x25, [sp, #80]
   40ef4:	mov	x20, x0
   40ef8:	add	x27, x8, #0x10
   40efc:	madd	x25, x22, x10, x0
   40f00:	stp	x24, x23, [sp, #96]
   40f04:	mov	x24, x3
   40f08:	mov	x3, x1
   40f0c:	mov	w2, #0x5                   	// #5
   40f10:	mov	w6, #0x1                   	// #1
   40f14:	mov	x0, x27
   40f18:	mov	x1, x25
   40f1c:	mov	x4, x22
   40f20:	mov	x7, x20
   40f24:	stp	x29, x30, [sp, #48]
   40f28:	add	x29, sp, #0x30
   40f2c:	mov	x23, x3
   40f30:	mov	x26, x5
   40f34:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   40f38:	add	x8, x24, x21
   40f3c:	add	x28, x19, #0x2
   40f40:	mov	x19, x24
   40f44:	add	x24, x8, #0x18
   40f48:	mov	x0, x20
   40f4c:	mov	x1, x25
   40f50:	mov	x2, x28
   40f54:	mov	x3, x24
   40f58:	bl	c050 <__gmpn_toom2_sqr@plt>
   40f5c:	mov	x0, x19
   40f60:	mov	x1, x27
   40f64:	mov	x2, x28
   40f68:	mov	x3, x24
   40f6c:	bl	c050 <__gmpn_toom2_sqr@plt>
   40f70:	mov	w1, #0x1                   	// #1
   40f74:	bfi	x1, x22, #1, #63
   40f78:	mov	w5, #0x1                   	// #1
   40f7c:	mov	x0, x19
   40f80:	mov	x2, x20
   40f84:	mov	w3, wzr
   40f88:	mov	x4, x22
   40f8c:	mov	w6, wzr
   40f90:	str	x1, [sp, #8]
   40f94:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40f98:	mov	w2, #0x5                   	// #5
   40f9c:	mov	x0, x27
   40fa0:	mov	x1, x25
   40fa4:	mov	x3, x23
   40fa8:	mov	x4, x22
   40fac:	mov	x5, x26
   40fb0:	mov	x6, x20
   40fb4:	mov	x21, x26
   40fb8:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   40fbc:	mov	x0, x20
   40fc0:	mov	x1, x25
   40fc4:	mov	x2, x28
   40fc8:	mov	x3, x24
   40fcc:	bl	c050 <__gmpn_toom2_sqr@plt>
   40fd0:	add	x8, x22, x22, lsl #1
   40fd4:	lsl	x8, x8, #3
   40fd8:	str	x8, [sp, #24]
   40fdc:	add	x8, x19, x8
   40fe0:	add	x26, x8, #0x8
   40fe4:	mov	x0, x26
   40fe8:	mov	x1, x27
   40fec:	mov	x2, x28
   40ff0:	mov	x3, x24
   40ff4:	stp	x19, x26, [x29, #-16]
   40ff8:	bl	c050 <__gmpn_toom2_sqr@plt>
   40ffc:	mov	x0, x26
   41000:	ldr	x26, [sp, #8]
   41004:	mov	x2, x20
   41008:	mov	w3, wzr
   4100c:	mov	x4, x22
   41010:	mov	x1, x26
   41014:	mov	w5, wzr
   41018:	mov	w6, wzr
   4101c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41020:	mov	w2, #0x5                   	// #5
   41024:	mov	w6, #0x2                   	// #2
   41028:	mov	x0, x27
   4102c:	mov	x1, x25
   41030:	mov	x3, x23
   41034:	mov	x4, x22
   41038:	mov	x5, x21
   4103c:	mov	x7, x20
   41040:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   41044:	mov	x0, x20
   41048:	mov	x1, x25
   4104c:	mov	x2, x28
   41050:	mov	x3, x24
   41054:	bl	c050 <__gmpn_toom2_sqr@plt>
   41058:	mov	w8, #0x30                  	// #48
   4105c:	madd	x8, x22, x8, x19
   41060:	add	x19, x8, #0x10
   41064:	mov	x0, x19
   41068:	mov	x1, x27
   4106c:	mov	x2, x28
   41070:	mov	x3, x24
   41074:	str	x19, [sp, #16]
   41078:	bl	c050 <__gmpn_toom2_sqr@plt>
   4107c:	mov	w5, #0x2                   	// #2
   41080:	mov	w6, #0x4                   	// #4
   41084:	mov	x0, x19
   41088:	mov	x1, x26
   4108c:	mov	x2, x20
   41090:	mov	w3, wzr
   41094:	mov	x4, x22
   41098:	bl	c970 <__gmpn_toom_couple_handling@plt>
   4109c:	mov	w2, #0x5                   	// #5
   410a0:	mov	w6, #0x2                   	// #2
   410a4:	mov	x0, x27
   410a8:	mov	x1, x25
   410ac:	mov	x3, x23
   410b0:	mov	x4, x22
   410b4:	mov	x5, x21
   410b8:	mov	x7, x20
   410bc:	mov	x19, x23
   410c0:	mov	x23, x21
   410c4:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   410c8:	mov	x0, x20
   410cc:	mov	x1, x25
   410d0:	mov	x2, x28
   410d4:	mov	x3, x24
   410d8:	bl	c050 <__gmpn_toom2_sqr@plt>
   410dc:	ldr	x8, [sp, #24]
   410e0:	mov	x1, x27
   410e4:	mov	x2, x28
   410e8:	mov	x3, x24
   410ec:	add	x21, x20, x8
   410f0:	mov	x0, x21
   410f4:	bl	c050 <__gmpn_toom2_sqr@plt>
   410f8:	mov	w5, #0x2                   	// #2
   410fc:	mov	x0, x21
   41100:	mov	x1, x26
   41104:	mov	x2, x20
   41108:	mov	w3, wzr
   4110c:	mov	x4, x22
   41110:	mov	w6, wzr
   41114:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41118:	mov	w2, #0x5                   	// #5
   4111c:	mov	x0, x27
   41120:	mov	x1, x25
   41124:	mov	x3, x19
   41128:	mov	x4, x22
   4112c:	mov	x5, x23
   41130:	mov	x6, x20
   41134:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   41138:	mov	x0, x20
   4113c:	mov	x1, x25
   41140:	mov	x2, x28
   41144:	mov	x3, x24
   41148:	bl	c050 <__gmpn_toom2_sqr@plt>
   4114c:	mov	x0, x25
   41150:	mov	x1, x27
   41154:	mov	x2, x28
   41158:	mov	x3, x24
   4115c:	bl	c050 <__gmpn_toom2_sqr@plt>
   41160:	mov	w5, #0x1                   	// #1
   41164:	mov	w6, #0x2                   	// #2
   41168:	mov	x0, x25
   4116c:	mov	x1, x26
   41170:	mov	x2, x20
   41174:	mov	w3, wzr
   41178:	mov	x4, x22
   4117c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41180:	mov	x0, x20
   41184:	mov	x1, x19
   41188:	mov	x2, x22
   4118c:	mov	x3, x24
   41190:	bl	c050 <__gmpn_toom2_sqr@plt>
   41194:	lsl	x5, x23, #1
   41198:	mov	x0, x20
   4119c:	ldr	x1, [sp, #16]
   411a0:	ldp	x3, x2, [x29, #-16]
   411a4:	mov	x4, x22
   411a8:	mov	x7, x24
   411ac:	ldp	x20, x19, [sp, #128]
   411b0:	ldp	x22, x21, [sp, #112]
   411b4:	ldp	x24, x23, [sp, #96]
   411b8:	ldp	x26, x25, [sp, #80]
   411bc:	ldp	x28, x27, [sp, #64]
   411c0:	ldp	x29, x30, [sp, #48]
   411c4:	mov	w6, wzr
   411c8:	add	sp, sp, #0x90
   411cc:	b	bfb0 <__gmpn_toom_interpolate_12pts@plt>

00000000000411d0 <__gmpn_toom8h_mul@@Base>:
   411d0:	sub	sp, sp, #0x100
   411d4:	stp	x29, x30, [sp, #160]
   411d8:	stp	x28, x27, [sp, #176]
   411dc:	stp	x20, x19, [sp, #240]
   411e0:	add	x29, sp, #0xa0
   411e4:	mov	x27, x5
   411e8:	mov	x16, x1
   411ec:	cmp	x2, x4
   411f0:	mov	x20, x0
   411f4:	stp	x26, x25, [sp, #192]
   411f8:	stp	x24, x23, [sp, #208]
   411fc:	stp	x22, x21, [sp, #224]
   41200:	stp	x1, x3, [x29, #-16]
   41204:	str	x0, [sp, #32]
   41208:	b.ne	41f60 <__gmpn_toom8h_mul@@Base+0xd90>  // b.any
   4120c:	sub	x8, x2, #0x1
   41210:	asr	x8, x8, #3
   41214:	add	x23, x8, #0x1
   41218:	lsl	x8, x23, #3
   4121c:	sub	x8, x8, x23
   41220:	mov	w10, wzr
   41224:	mov	w15, #0x7                   	// #7
   41228:	sub	x5, x2, x8
   4122c:	sub	x26, x4, x8
   41230:	mov	w28, #0x7                   	// #7
   41234:	mov	w8, #0xd                   	// #13
   41238:	mul	x19, x23, x8
   4123c:	add	x8, x20, x19, lsl #3
   41240:	mov	w9, #0x58                  	// #88
   41244:	add	x25, x8, #0x10
   41248:	madd	x1, x23, x9, x20
   4124c:	mov	w6, #0x3                   	// #3
   41250:	mov	x0, x25
   41254:	mov	w2, w15
   41258:	mov	x3, x16
   4125c:	mov	x4, x23
   41260:	mov	x7, x20
   41264:	stp	x5, x26, [x29, #-32]
   41268:	stur	w10, [x29, #-76]
   4126c:	str	x1, [sp, #72]
   41270:	stur	x15, [x29, #-48]
   41274:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   41278:	add	x8, x23, x23, lsl #1
   4127c:	lsl	x8, x8, #5
   41280:	ldur	x3, [x29, #-8]
   41284:	add	x9, x27, x8
   41288:	add	x8, x20, x8
   4128c:	add	x21, x9, #0x20
   41290:	add	x24, x8, #0x8
   41294:	mov	w22, w0
   41298:	mov	w6, #0x3                   	// #3
   4129c:	mov	x0, x21
   412a0:	mov	x1, x24
   412a4:	mov	w2, w28
   412a8:	mov	x4, x23
   412ac:	mov	x5, x26
   412b0:	mov	x7, x20
   412b4:	stur	x28, [x29, #-56]
   412b8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   412bc:	eor	w28, w0, w22
   412c0:	cmp	x23, #0x2f
   412c4:	add	x22, x23, #0x1
   412c8:	stur	x21, [x29, #-64]
   412cc:	stp	x24, x25, [sp, #48]
   412d0:	stur	x19, [x29, #-40]
   412d4:	str	x27, [sp, #64]
   412d8:	b.le	41350 <__gmpn_toom8h_mul@@Base+0x180>
   412dc:	ldr	x9, [sp, #72]
   412e0:	cmp	x23, #0x50
   412e4:	b.le	41394 <__gmpn_toom8h_mul@@Base+0x1c4>
   412e8:	ldp	x25, x27, [sp, #56]
   412ec:	ldur	x8, [x29, #-40]
   412f0:	cmp	x23, #0xab
   412f4:	add	x8, x27, x8, lsl #3
   412f8:	b.le	413f0 <__gmpn_toom8h_mul@@Base+0x220>
   412fc:	cmp	x23, #0xea
   41300:	add	x26, x8, #0x28
   41304:	b.le	4143c <__gmpn_toom8h_mul@@Base+0x26c>
   41308:	ldr	x20, [sp, #32]
   4130c:	ldr	x24, [sp, #48]
   41310:	mov	x1, x9
   41314:	mov	x2, x22
   41318:	mov	x0, x20
   4131c:	mov	x3, x24
   41320:	mov	x4, x22
   41324:	mov	x5, x26
   41328:	bl	cb20 <__gmpn_toom8h_mul@plt>
   4132c:	ldur	x21, [x29, #-64]
   41330:	mov	x0, x27
   41334:	mov	x1, x25
   41338:	mov	x2, x22
   4133c:	mov	x3, x21
   41340:	mov	x4, x22
   41344:	mov	x5, x26
   41348:	bl	cb20 <__gmpn_toom8h_mul@plt>
   4134c:	b	41480 <__gmpn_toom8h_mul@@Base+0x2b0>
   41350:	ldr	x1, [sp, #72]
   41354:	add	x8, x27, x19, lsl #3
   41358:	add	x26, x8, #0x28
   4135c:	mov	x0, x20
   41360:	mov	x2, x22
   41364:	mov	x3, x24
   41368:	mov	x4, x22
   4136c:	mov	x5, x26
   41370:	bl	d450 <__gmpn_toom22_mul@plt>
   41374:	mov	x0, x27
   41378:	mov	x1, x25
   4137c:	mov	x2, x22
   41380:	mov	x3, x21
   41384:	mov	x4, x22
   41388:	mov	x5, x26
   4138c:	bl	d450 <__gmpn_toom22_mul@plt>
   41390:	b	41480 <__gmpn_toom8h_mul@@Base+0x2b0>
   41394:	ldur	x8, [x29, #-40]
   41398:	ldr	x27, [sp, #64]
   4139c:	ldr	x20, [sp, #32]
   413a0:	ldr	x24, [sp, #48]
   413a4:	mov	x1, x9
   413a8:	add	x8, x27, x8, lsl #3
   413ac:	add	x26, x8, #0x28
   413b0:	mov	x0, x20
   413b4:	mov	x2, x22
   413b8:	mov	x3, x24
   413bc:	mov	x4, x22
   413c0:	mov	x5, x26
   413c4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   413c8:	ldr	x25, [sp, #56]
   413cc:	ldur	x21, [x29, #-64]
   413d0:	mov	x0, x27
   413d4:	mov	x2, x22
   413d8:	mov	x1, x25
   413dc:	mov	x3, x21
   413e0:	mov	x4, x22
   413e4:	mov	x5, x26
   413e8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   413ec:	b	41480 <__gmpn_toom8h_mul@@Base+0x2b0>
   413f0:	ldr	x20, [sp, #32]
   413f4:	ldr	x24, [sp, #48]
   413f8:	add	x26, x8, #0x28
   413fc:	mov	x1, x9
   41400:	mov	x0, x20
   41404:	mov	x2, x22
   41408:	mov	x3, x24
   4140c:	mov	x4, x22
   41410:	mov	x5, x26
   41414:	bl	c720 <__gmpn_toom44_mul@plt>
   41418:	ldur	x21, [x29, #-64]
   4141c:	mov	x0, x27
   41420:	mov	x1, x25
   41424:	mov	x2, x22
   41428:	mov	x3, x21
   4142c:	mov	x4, x22
   41430:	mov	x5, x26
   41434:	bl	c720 <__gmpn_toom44_mul@plt>
   41438:	b	41480 <__gmpn_toom8h_mul@@Base+0x2b0>
   4143c:	ldr	x20, [sp, #32]
   41440:	ldr	x24, [sp, #48]
   41444:	mov	x1, x9
   41448:	mov	x2, x22
   4144c:	mov	x0, x20
   41450:	mov	x3, x24
   41454:	mov	x4, x22
   41458:	mov	x5, x26
   4145c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41460:	ldur	x21, [x29, #-64]
   41464:	mov	x0, x27
   41468:	mov	x1, x25
   4146c:	mov	x2, x22
   41470:	mov	x3, x21
   41474:	mov	x4, x22
   41478:	mov	x5, x26
   4147c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41480:	ldur	w6, [x29, #-76]
   41484:	mov	w19, #0x1                   	// #1
   41488:	bfi	x19, x23, #1, #63
   4148c:	mov	x0, x27
   41490:	add	w8, w6, #0x1
   41494:	bfi	w6, w6, #1, #1
   41498:	add	w5, w6, #0x3
   4149c:	mov	x1, x19
   414a0:	mov	x2, x20
   414a4:	mov	w3, w28
   414a8:	mov	x4, x23
   414ac:	str	w8, [sp, #44]
   414b0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   414b4:	ldr	x1, [sp, #72]
   414b8:	ldur	x3, [x29, #-16]
   414bc:	ldur	x5, [x29, #-32]
   414c0:	mov	w6, #0x2                   	// #2
   414c4:	mov	x0, x25
   414c8:	ldur	x2, [x29, #-48]
   414cc:	mov	x4, x23
   414d0:	mov	x7, x20
   414d4:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   414d8:	ldur	x3, [x29, #-8]
   414dc:	ldur	x5, [x29, #-24]
   414e0:	mov	w28, w0
   414e4:	mov	w6, #0x2                   	// #2
   414e8:	mov	x0, x21
   414ec:	mov	x1, x24
   414f0:	ldur	x2, [x29, #-56]
   414f4:	mov	x4, x23
   414f8:	mov	x7, x20
   414fc:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   41500:	cmp	x23, #0x2f
   41504:	eor	w9, w0, w28
   41508:	stur	w9, [x29, #-72]
   4150c:	b.le	41588 <__gmpn_toom8h_mul@@Base+0x3b8>
   41510:	ldr	x1, [sp, #72]
   41514:	cmp	x23, #0x50
   41518:	b.le	415dc <__gmpn_toom8h_mul@@Base+0x40c>
   4151c:	ldur	x8, [x29, #-40]
   41520:	cmp	x23, #0xab
   41524:	b.le	41638 <__gmpn_toom8h_mul@@Base+0x468>
   41528:	ldr	x27, [sp, #64]
   4152c:	cmp	x23, #0xea
   41530:	add	x8, x27, x8, lsl #3
   41534:	add	x26, x8, #0x28
   41538:	b.le	41690 <__gmpn_toom8h_mul@@Base+0x4c0>
   4153c:	ldr	x20, [sp, #32]
   41540:	ldr	x24, [sp, #48]
   41544:	mov	x2, x22
   41548:	mov	x4, x22
   4154c:	mov	x0, x20
   41550:	mov	x3, x24
   41554:	mov	x5, x26
   41558:	bl	cb20 <__gmpn_toom8h_mul@plt>
   4155c:	ldr	x25, [sp, #56]
   41560:	add	x28, x23, x23, lsl #1
   41564:	add	x8, x27, x28, lsl #3
   41568:	add	x0, x8, #0x8
   4156c:	mov	x1, x25
   41570:	mov	x2, x22
   41574:	mov	x3, x21
   41578:	mov	x4, x22
   4157c:	mov	x5, x26
   41580:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41584:	b	416d8 <__gmpn_toom8h_mul@@Base+0x508>
   41588:	ldur	x8, [x29, #-40]
   4158c:	ldr	x1, [sp, #72]
   41590:	mov	x0, x20
   41594:	mov	x2, x22
   41598:	add	x8, x27, x8, lsl #3
   4159c:	add	x26, x8, #0x28
   415a0:	mov	x3, x24
   415a4:	mov	x4, x22
   415a8:	mov	x5, x26
   415ac:	bl	d450 <__gmpn_toom22_mul@plt>
   415b0:	add	x28, x23, x23, lsl #1
   415b4:	mov	x5, x26
   415b8:	ldur	w26, [x29, #-76]
   415bc:	add	x8, x27, x28, lsl #3
   415c0:	add	x0, x8, #0x8
   415c4:	mov	x1, x25
   415c8:	mov	x2, x22
   415cc:	mov	x3, x21
   415d0:	mov	x4, x22
   415d4:	bl	d450 <__gmpn_toom22_mul@plt>
   415d8:	b	416dc <__gmpn_toom8h_mul@@Base+0x50c>
   415dc:	ldur	x8, [x29, #-40]
   415e0:	ldr	x27, [sp, #64]
   415e4:	ldr	x20, [sp, #32]
   415e8:	ldr	x24, [sp, #48]
   415ec:	mov	x2, x22
   415f0:	add	x8, x27, x8, lsl #3
   415f4:	add	x26, x8, #0x28
   415f8:	mov	x0, x20
   415fc:	mov	x3, x24
   41600:	mov	x4, x22
   41604:	mov	x5, x26
   41608:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4160c:	ldr	x25, [sp, #56]
   41610:	add	x28, x23, x23, lsl #1
   41614:	add	x8, x27, x28, lsl #3
   41618:	add	x0, x8, #0x8
   4161c:	mov	x1, x25
   41620:	mov	x2, x22
   41624:	mov	x3, x21
   41628:	mov	x4, x22
   4162c:	mov	x5, x26
   41630:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41634:	b	416d8 <__gmpn_toom8h_mul@@Base+0x508>
   41638:	ldr	x27, [sp, #64]
   4163c:	ldr	x20, [sp, #32]
   41640:	ldr	x24, [sp, #48]
   41644:	mov	x2, x22
   41648:	add	x8, x27, x8, lsl #3
   4164c:	add	x26, x8, #0x28
   41650:	mov	x0, x20
   41654:	mov	x3, x24
   41658:	mov	x4, x22
   4165c:	mov	x5, x26
   41660:	bl	c720 <__gmpn_toom44_mul@plt>
   41664:	ldr	x25, [sp, #56]
   41668:	add	x28, x23, x23, lsl #1
   4166c:	add	x8, x27, x28, lsl #3
   41670:	add	x0, x8, #0x8
   41674:	mov	x1, x25
   41678:	mov	x2, x22
   4167c:	mov	x3, x21
   41680:	mov	x4, x22
   41684:	mov	x5, x26
   41688:	bl	c720 <__gmpn_toom44_mul@plt>
   4168c:	b	416d8 <__gmpn_toom8h_mul@@Base+0x508>
   41690:	ldr	x20, [sp, #32]
   41694:	ldr	x24, [sp, #48]
   41698:	mov	x2, x22
   4169c:	mov	x4, x22
   416a0:	mov	x0, x20
   416a4:	mov	x3, x24
   416a8:	mov	x5, x26
   416ac:	bl	cc20 <__gmpn_toom6h_mul@plt>
   416b0:	ldr	x25, [sp, #56]
   416b4:	add	x28, x23, x23, lsl #1
   416b8:	add	x8, x27, x28, lsl #3
   416bc:	add	x0, x8, #0x8
   416c0:	mov	x1, x25
   416c4:	mov	x2, x22
   416c8:	mov	x3, x21
   416cc:	mov	x4, x22
   416d0:	mov	x5, x26
   416d4:	bl	cc20 <__gmpn_toom6h_mul@plt>
   416d8:	ldur	w26, [x29, #-76]
   416dc:	ldr	w9, [sp, #44]
   416e0:	ldur	w3, [x29, #-72]
   416e4:	add	x8, x27, x28, lsl #3
   416e8:	add	x0, x8, #0x8
   416ec:	lsl	w5, w9, #1
   416f0:	lsl	w6, w26, #1
   416f4:	mov	x1, x19
   416f8:	mov	x2, x20
   416fc:	mov	x4, x23
   41700:	stp	x28, x0, [sp, #16]
   41704:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41708:	ldr	x28, [sp, #72]
   4170c:	ldur	x3, [x29, #-16]
   41710:	ldur	x5, [x29, #-32]
   41714:	mov	x0, x25
   41718:	mov	x1, x28
   4171c:	ldur	x2, [x29, #-48]
   41720:	mov	x4, x23
   41724:	mov	x6, x20
   41728:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   4172c:	ldur	x3, [x29, #-8]
   41730:	ldur	x5, [x29, #-24]
   41734:	mov	w26, w0
   41738:	mov	x0, x21
   4173c:	mov	x1, x24
   41740:	ldur	x2, [x29, #-56]
   41744:	mov	x4, x23
   41748:	mov	x6, x20
   4174c:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   41750:	cmp	x23, #0x2f
   41754:	eor	w8, w0, w26
   41758:	stur	x19, [x29, #-72]
   4175c:	str	w8, [sp, #12]
   41760:	b.le	417e0 <__gmpn_toom8h_mul@@Base+0x610>
   41764:	cmp	x23, #0x50
   41768:	b.le	4183c <__gmpn_toom8h_mul@@Base+0x66c>
   4176c:	ldp	x25, x19, [sp, #56]
   41770:	cmp	x23, #0xab
   41774:	b.le	418a4 <__gmpn_toom8h_mul@@Base+0x6d4>
   41778:	ldur	x8, [x29, #-40]
   4177c:	cmp	x23, #0xea
   41780:	add	x8, x19, x8, lsl #3
   41784:	add	x26, x8, #0x28
   41788:	b.le	41904 <__gmpn_toom8h_mul@@Base+0x734>
   4178c:	ldr	x20, [sp, #32]
   41790:	mov	x1, x28
   41794:	mov	x2, x22
   41798:	mov	x3, x24
   4179c:	mov	x0, x20
   417a0:	mov	x4, x22
   417a4:	mov	x5, x26
   417a8:	bl	cb20 <__gmpn_toom8h_mul@plt>
   417ac:	ldur	x21, [x29, #-64]
   417b0:	add	x8, x23, x23, lsl #1
   417b4:	mov	x27, x24
   417b8:	lsl	x24, x8, #1
   417bc:	add	x8, x19, x8, lsl #4
   417c0:	add	x0, x8, #0x10
   417c4:	mov	x1, x25
   417c8:	mov	x2, x22
   417cc:	mov	x3, x21
   417d0:	mov	x4, x22
   417d4:	mov	x5, x26
   417d8:	bl	cb20 <__gmpn_toom8h_mul@plt>
   417dc:	b	41954 <__gmpn_toom8h_mul@@Base+0x784>
   417e0:	ldur	x8, [x29, #-40]
   417e4:	mov	x0, x20
   417e8:	mov	x1, x28
   417ec:	mov	x2, x22
   417f0:	add	x8, x27, x8, lsl #3
   417f4:	add	x26, x8, #0x28
   417f8:	mov	x3, x24
   417fc:	mov	x4, x22
   41800:	mov	x5, x26
   41804:	bl	d450 <__gmpn_toom22_mul@plt>
   41808:	add	x8, x23, x23, lsl #1
   4180c:	mov	x19, x27
   41810:	mov	x27, x24
   41814:	lsl	x24, x8, #1
   41818:	add	x8, x19, x8, lsl #4
   4181c:	add	x0, x8, #0x10
   41820:	mov	x1, x25
   41824:	mov	x2, x22
   41828:	mov	x3, x21
   4182c:	mov	x4, x22
   41830:	mov	x5, x26
   41834:	bl	d450 <__gmpn_toom22_mul@plt>
   41838:	b	41954 <__gmpn_toom8h_mul@@Base+0x784>
   4183c:	ldur	x8, [x29, #-40]
   41840:	ldr	x19, [sp, #64]
   41844:	ldr	x20, [sp, #32]
   41848:	mov	x1, x28
   4184c:	mov	x2, x22
   41850:	add	x8, x19, x8, lsl #3
   41854:	add	x26, x8, #0x28
   41858:	mov	x0, x20
   4185c:	mov	x3, x24
   41860:	mov	x4, x22
   41864:	mov	x5, x26
   41868:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4186c:	ldr	x25, [sp, #56]
   41870:	ldur	x21, [x29, #-64]
   41874:	add	x8, x23, x23, lsl #1
   41878:	mov	x27, x24
   4187c:	lsl	x24, x8, #1
   41880:	add	x8, x19, x8, lsl #4
   41884:	add	x0, x8, #0x10
   41888:	mov	x1, x25
   4188c:	mov	x2, x22
   41890:	mov	x3, x21
   41894:	mov	x4, x22
   41898:	mov	x5, x26
   4189c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   418a0:	b	41954 <__gmpn_toom8h_mul@@Base+0x784>
   418a4:	ldur	x8, [x29, #-40]
   418a8:	ldr	x20, [sp, #32]
   418ac:	mov	x1, x28
   418b0:	mov	x2, x22
   418b4:	add	x8, x19, x8, lsl #3
   418b8:	add	x26, x8, #0x28
   418bc:	mov	x0, x20
   418c0:	mov	x3, x24
   418c4:	mov	x4, x22
   418c8:	mov	x5, x26
   418cc:	bl	c720 <__gmpn_toom44_mul@plt>
   418d0:	ldur	x21, [x29, #-64]
   418d4:	add	x8, x23, x23, lsl #1
   418d8:	mov	x27, x24
   418dc:	lsl	x24, x8, #1
   418e0:	add	x8, x19, x8, lsl #4
   418e4:	add	x0, x8, #0x10
   418e8:	mov	x1, x25
   418ec:	mov	x2, x22
   418f0:	mov	x3, x21
   418f4:	mov	x4, x22
   418f8:	mov	x5, x26
   418fc:	bl	c720 <__gmpn_toom44_mul@plt>
   41900:	b	41954 <__gmpn_toom8h_mul@@Base+0x784>
   41904:	ldr	x20, [sp, #32]
   41908:	mov	x1, x28
   4190c:	mov	x2, x22
   41910:	mov	x3, x24
   41914:	mov	x0, x20
   41918:	mov	x4, x22
   4191c:	mov	x5, x26
   41920:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41924:	ldur	x21, [x29, #-64]
   41928:	add	x8, x23, x23, lsl #1
   4192c:	mov	x27, x24
   41930:	lsl	x24, x8, #1
   41934:	add	x8, x19, x8, lsl #4
   41938:	add	x0, x8, #0x10
   4193c:	mov	x1, x25
   41940:	mov	x2, x22
   41944:	mov	x3, x21
   41948:	mov	x4, x22
   4194c:	mov	x5, x26
   41950:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41954:	ldur	x1, [x29, #-72]
   41958:	ldr	w3, [sp, #12]
   4195c:	ldur	x26, [x29, #-32]
   41960:	add	x8, x19, x24, lsl #3
   41964:	add	x0, x8, #0x10
   41968:	mov	w5, #0x1                   	// #1
   4196c:	mov	w6, #0x2                   	// #2
   41970:	mov	x2, x20
   41974:	mov	x4, x23
   41978:	str	x0, [sp, #72]
   4197c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41980:	ldur	x3, [x29, #-16]
   41984:	mov	w6, #0x3                   	// #3
   41988:	mov	x0, x25
   4198c:	mov	x1, x28
   41990:	ldur	x2, [x29, #-48]
   41994:	mov	x4, x23
   41998:	mov	x5, x26
   4199c:	mov	x7, x20
   419a0:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   419a4:	ldur	x3, [x29, #-8]
   419a8:	ldur	x5, [x29, #-24]
   419ac:	mov	w26, w0
   419b0:	mov	w6, #0x3                   	// #3
   419b4:	mov	x0, x21
   419b8:	mov	x1, x27
   419bc:	ldur	x2, [x29, #-56]
   419c0:	mov	x4, x23
   419c4:	mov	x7, x20
   419c8:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   419cc:	cmp	x23, #0x2f
   419d0:	eor	w9, w0, w26
   419d4:	str	w9, [sp, #12]
   419d8:	b.le	41a54 <__gmpn_toom8h_mul@@Base+0x884>
   419dc:	ldur	x8, [x29, #-40]
   419e0:	cmp	x23, #0x50
   419e4:	b.le	41aa8 <__gmpn_toom8h_mul@@Base+0x8d8>
   419e8:	ldr	x25, [sp, #56]
   419ec:	cmp	x23, #0xab
   419f0:	add	x8, x19, x8, lsl #3
   419f4:	b.le	41b00 <__gmpn_toom8h_mul@@Base+0x930>
   419f8:	cmp	x23, #0xea
   419fc:	add	x26, x8, #0x28
   41a00:	mov	x24, x19
   41a04:	mov	x0, x20
   41a08:	mov	x1, x28
   41a0c:	mov	x2, x22
   41a10:	b.le	41b50 <__gmpn_toom8h_mul@@Base+0x980>
   41a14:	ldr	x27, [sp, #48]
   41a18:	mov	x4, x22
   41a1c:	mov	x5, x26
   41a20:	mov	x3, x27
   41a24:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41a28:	ldur	x21, [x29, #-64]
   41a2c:	add	x19, x23, x23, lsl #3
   41a30:	add	x8, x24, x19, lsl #3
   41a34:	add	x0, x8, #0x18
   41a38:	mov	x1, x25
   41a3c:	mov	x2, x22
   41a40:	mov	x3, x21
   41a44:	mov	x4, x22
   41a48:	mov	x5, x26
   41a4c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41a50:	b	41b8c <__gmpn_toom8h_mul@@Base+0x9bc>
   41a54:	ldur	x8, [x29, #-40]
   41a58:	mov	x0, x20
   41a5c:	mov	x1, x28
   41a60:	mov	x2, x22
   41a64:	add	x8, x19, x8, lsl #3
   41a68:	add	x26, x8, #0x28
   41a6c:	mov	x3, x27
   41a70:	mov	x4, x22
   41a74:	mov	x5, x26
   41a78:	bl	d450 <__gmpn_toom22_mul@plt>
   41a7c:	mov	x24, x19
   41a80:	add	x19, x23, x23, lsl #3
   41a84:	add	x8, x24, x19, lsl #3
   41a88:	add	x0, x8, #0x18
   41a8c:	mov	x1, x25
   41a90:	mov	x2, x22
   41a94:	mov	x3, x21
   41a98:	mov	x4, x22
   41a9c:	mov	x5, x26
   41aa0:	bl	d450 <__gmpn_toom22_mul@plt>
   41aa4:	b	41b8c <__gmpn_toom8h_mul@@Base+0x9bc>
   41aa8:	add	x8, x19, x8, lsl #3
   41aac:	add	x26, x8, #0x28
   41ab0:	mov	x0, x20
   41ab4:	mov	x1, x28
   41ab8:	mov	x2, x22
   41abc:	mov	x3, x27
   41ac0:	mov	x4, x22
   41ac4:	mov	x5, x26
   41ac8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41acc:	ldr	x25, [sp, #56]
   41ad0:	ldur	x21, [x29, #-64]
   41ad4:	mov	x24, x19
   41ad8:	add	x19, x23, x23, lsl #3
   41adc:	add	x8, x24, x19, lsl #3
   41ae0:	add	x0, x8, #0x18
   41ae4:	mov	x1, x25
   41ae8:	mov	x2, x22
   41aec:	mov	x3, x21
   41af0:	mov	x4, x22
   41af4:	mov	x5, x26
   41af8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41afc:	b	41b8c <__gmpn_toom8h_mul@@Base+0x9bc>
   41b00:	add	x26, x8, #0x28
   41b04:	mov	x0, x20
   41b08:	mov	x1, x28
   41b0c:	mov	x2, x22
   41b10:	mov	x3, x27
   41b14:	mov	x4, x22
   41b18:	mov	x5, x26
   41b1c:	bl	c720 <__gmpn_toom44_mul@plt>
   41b20:	ldur	x21, [x29, #-64]
   41b24:	mov	x24, x19
   41b28:	add	x19, x23, x23, lsl #3
   41b2c:	add	x8, x24, x19, lsl #3
   41b30:	add	x0, x8, #0x18
   41b34:	mov	x1, x25
   41b38:	mov	x2, x22
   41b3c:	mov	x3, x21
   41b40:	mov	x4, x22
   41b44:	mov	x5, x26
   41b48:	bl	c720 <__gmpn_toom44_mul@plt>
   41b4c:	b	41b8c <__gmpn_toom8h_mul@@Base+0x9bc>
   41b50:	ldr	x27, [sp, #48]
   41b54:	mov	x4, x22
   41b58:	mov	x5, x26
   41b5c:	mov	x3, x27
   41b60:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41b64:	ldur	x21, [x29, #-64]
   41b68:	add	x19, x23, x23, lsl #3
   41b6c:	add	x8, x24, x19, lsl #3
   41b70:	add	x0, x8, #0x18
   41b74:	mov	x1, x25
   41b78:	mov	x2, x22
   41b7c:	mov	x3, x21
   41b80:	mov	x4, x22
   41b84:	mov	x5, x26
   41b88:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41b8c:	ldur	x1, [x29, #-72]
   41b90:	ldr	w3, [sp, #12]
   41b94:	ldur	x26, [x29, #-32]
   41b98:	add	x8, x24, x19, lsl #3
   41b9c:	add	x0, x8, #0x18
   41ba0:	mov	w5, #0x3                   	// #3
   41ba4:	mov	w6, #0x6                   	// #6
   41ba8:	mov	x2, x20
   41bac:	mov	x4, x23
   41bb0:	str	x0, [sp, #32]
   41bb4:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41bb8:	ldur	x3, [x29, #-16]
   41bbc:	mov	w6, #0x1                   	// #1
   41bc0:	mov	x0, x25
   41bc4:	mov	x1, x28
   41bc8:	ldur	x2, [x29, #-48]
   41bcc:	mov	x4, x23
   41bd0:	mov	x5, x26
   41bd4:	mov	x7, x20
   41bd8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   41bdc:	ldur	x3, [x29, #-8]
   41be0:	ldur	x5, [x29, #-24]
   41be4:	mov	w26, w0
   41be8:	mov	w6, #0x1                   	// #1
   41bec:	mov	x0, x21
   41bf0:	mov	x1, x27
   41bf4:	ldur	x2, [x29, #-56]
   41bf8:	mov	x4, x23
   41bfc:	mov	x7, x20
   41c00:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   41c04:	cmp	x23, #0x2f
   41c08:	eor	w9, w0, w26
   41c0c:	mov	x19, x24
   41c10:	str	w9, [sp, #12]
   41c14:	b.le	41c88 <__gmpn_toom8h_mul@@Base+0xab8>
   41c18:	ldur	x24, [x29, #-16]
   41c1c:	ldur	x8, [x29, #-40]
   41c20:	mov	x27, x28
   41c24:	cmp	x23, #0x50
   41c28:	b.le	41ce0 <__gmpn_toom8h_mul@@Base+0xb10>
   41c2c:	ldp	x21, x25, [sp, #48]
   41c30:	cmp	x23, #0xab
   41c34:	add	x8, x19, x8, lsl #3
   41c38:	b.le	41d30 <__gmpn_toom8h_mul@@Base+0xb60>
   41c3c:	ldr	x28, [sp, #16]
   41c40:	add	x26, x8, #0x28
   41c44:	cmp	x23, #0xea
   41c48:	mov	x0, x20
   41c4c:	mov	x1, x27
   41c50:	mov	x2, x22
   41c54:	mov	x3, x21
   41c58:	mov	x4, x22
   41c5c:	mov	x5, x26
   41c60:	b.le	41d74 <__gmpn_toom8h_mul@@Base+0xba4>
   41c64:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41c68:	ldur	x3, [x29, #-64]
   41c6c:	add	x0, x20, x28, lsl #3
   41c70:	mov	x1, x25
   41c74:	mov	x2, x22
   41c78:	mov	x4, x22
   41c7c:	mov	x5, x26
   41c80:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41c84:	b	41d94 <__gmpn_toom8h_mul@@Base+0xbc4>
   41c88:	ldur	x8, [x29, #-40]
   41c8c:	mov	x0, x20
   41c90:	mov	x1, x28
   41c94:	mov	x2, x22
   41c98:	add	x8, x19, x8, lsl #3
   41c9c:	add	x26, x8, #0x28
   41ca0:	mov	x3, x27
   41ca4:	mov	x4, x22
   41ca8:	mov	x5, x26
   41cac:	bl	d450 <__gmpn_toom22_mul@plt>
   41cb0:	mov	x3, x21
   41cb4:	mov	x21, x27
   41cb8:	mov	x27, x28
   41cbc:	ldr	x28, [sp, #16]
   41cc0:	mov	x1, x25
   41cc4:	mov	x2, x22
   41cc8:	mov	x4, x22
   41ccc:	add	x0, x20, x28, lsl #3
   41cd0:	mov	x5, x26
   41cd4:	bl	d450 <__gmpn_toom22_mul@plt>
   41cd8:	ldur	x24, [x29, #-16]
   41cdc:	b	41d94 <__gmpn_toom8h_mul@@Base+0xbc4>
   41ce0:	ldr	x21, [sp, #48]
   41ce4:	add	x8, x19, x8, lsl #3
   41ce8:	add	x26, x8, #0x28
   41cec:	mov	x0, x20
   41cf0:	mov	x1, x27
   41cf4:	mov	x2, x22
   41cf8:	mov	x3, x21
   41cfc:	mov	x4, x22
   41d00:	mov	x5, x26
   41d04:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41d08:	ldr	x28, [sp, #16]
   41d0c:	ldr	x25, [sp, #56]
   41d10:	ldur	x3, [x29, #-64]
   41d14:	mov	x2, x22
   41d18:	add	x0, x20, x28, lsl #3
   41d1c:	mov	x1, x25
   41d20:	mov	x4, x22
   41d24:	mov	x5, x26
   41d28:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41d2c:	b	41d94 <__gmpn_toom8h_mul@@Base+0xbc4>
   41d30:	add	x26, x8, #0x28
   41d34:	mov	x0, x20
   41d38:	mov	x1, x27
   41d3c:	mov	x2, x22
   41d40:	mov	x3, x21
   41d44:	mov	x4, x22
   41d48:	mov	x5, x26
   41d4c:	bl	c720 <__gmpn_toom44_mul@plt>
   41d50:	ldr	x28, [sp, #16]
   41d54:	ldur	x3, [x29, #-64]
   41d58:	mov	x1, x25
   41d5c:	mov	x2, x22
   41d60:	add	x0, x20, x28, lsl #3
   41d64:	mov	x4, x22
   41d68:	mov	x5, x26
   41d6c:	bl	c720 <__gmpn_toom44_mul@plt>
   41d70:	b	41d94 <__gmpn_toom8h_mul@@Base+0xbc4>
   41d74:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41d78:	ldur	x3, [x29, #-64]
   41d7c:	add	x0, x20, x28, lsl #3
   41d80:	mov	x1, x25
   41d84:	mov	x2, x22
   41d88:	mov	x4, x22
   41d8c:	mov	x5, x26
   41d90:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41d94:	ldur	w6, [x29, #-76]
   41d98:	ldur	x1, [x29, #-72]
   41d9c:	ldr	w3, [sp, #12]
   41da0:	ldr	w5, [sp, #44]
   41da4:	ldur	x26, [x29, #-32]
   41da8:	add	x0, x20, x28, lsl #3
   41dac:	mov	x2, x20
   41db0:	mov	x4, x23
   41db4:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41db8:	mov	x0, x25
   41dbc:	mov	x1, x27
   41dc0:	ldur	x2, [x29, #-48]
   41dc4:	mov	x3, x24
   41dc8:	mov	x4, x23
   41dcc:	mov	x5, x26
   41dd0:	mov	x6, x20
   41dd4:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   41dd8:	ldur	x2, [x29, #-56]
   41ddc:	mov	w28, w0
   41de0:	cmp	w2, #0x3
   41de4:	b.eq	41f98 <__gmpn_toom8h_mul@@Base+0xdc8>  // b.none
   41de8:	ldur	x0, [x29, #-64]
   41dec:	ldur	x3, [x29, #-8]
   41df0:	ldur	x5, [x29, #-24]
   41df4:	mov	x1, x21
   41df8:	mov	x4, x23
   41dfc:	mov	x6, x20
   41e00:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   41e04:	cmp	x23, #0x2f
   41e08:	eor	w28, w0, w28
   41e0c:	b.le	41fc0 <__gmpn_toom8h_mul@@Base+0xdf0>
   41e10:	cmp	x23, #0x50
   41e14:	b.le	41e80 <__gmpn_toom8h_mul@@Base+0xcb0>
   41e18:	cmp	x23, #0xab
   41e1c:	b.le	41ed8 <__gmpn_toom8h_mul@@Base+0xd08>
   41e20:	ldur	x8, [x29, #-40]
   41e24:	cmp	x23, #0xea
   41e28:	mov	x0, x20
   41e2c:	mov	x1, x27
   41e30:	add	x8, x19, x8, lsl #3
   41e34:	add	x8, x8, #0x28
   41e38:	mov	x2, x22
   41e3c:	mov	x3, x21
   41e40:	mov	x4, x22
   41e44:	mov	x5, x8
   41e48:	str	x8, [sp, #56]
   41e4c:	b.le	41f30 <__gmpn_toom8h_mul@@Base+0xd60>
   41e50:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41e54:	ldur	x24, [x29, #-64]
   41e58:	ldr	x5, [sp, #56]
   41e5c:	lsl	x8, x23, #3
   41e60:	sub	x19, x8, x23
   41e64:	add	x0, x20, x19, lsl #3
   41e68:	mov	x1, x25
   41e6c:	mov	x2, x22
   41e70:	mov	x3, x24
   41e74:	mov	x4, x22
   41e78:	bl	cb20 <__gmpn_toom8h_mul@plt>
   41e7c:	b	42014 <__gmpn_toom8h_mul@@Base+0xe44>
   41e80:	ldur	x8, [x29, #-40]
   41e84:	mov	x0, x20
   41e88:	mov	x1, x27
   41e8c:	mov	x2, x22
   41e90:	add	x8, x19, x8, lsl #3
   41e94:	add	x26, x8, #0x28
   41e98:	mov	x3, x21
   41e9c:	mov	x4, x22
   41ea0:	mov	x5, x26
   41ea4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41ea8:	ldur	x24, [x29, #-64]
   41eac:	lsl	x8, x23, #3
   41eb0:	mov	x5, x26
   41eb4:	ldur	x26, [x29, #-32]
   41eb8:	sub	x19, x8, x23
   41ebc:	add	x0, x20, x19, lsl #3
   41ec0:	mov	x1, x25
   41ec4:	mov	x2, x22
   41ec8:	mov	x3, x24
   41ecc:	mov	x4, x22
   41ed0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   41ed4:	b	42014 <__gmpn_toom8h_mul@@Base+0xe44>
   41ed8:	ldur	x8, [x29, #-40]
   41edc:	mov	x0, x20
   41ee0:	mov	x1, x27
   41ee4:	mov	x2, x22
   41ee8:	add	x8, x19, x8, lsl #3
   41eec:	add	x26, x8, #0x28
   41ef0:	mov	x3, x21
   41ef4:	mov	x4, x22
   41ef8:	mov	x5, x26
   41efc:	bl	c720 <__gmpn_toom44_mul@plt>
   41f00:	ldur	x24, [x29, #-64]
   41f04:	lsl	x8, x23, #3
   41f08:	mov	x5, x26
   41f0c:	ldur	x26, [x29, #-32]
   41f10:	sub	x19, x8, x23
   41f14:	add	x0, x20, x19, lsl #3
   41f18:	mov	x1, x25
   41f1c:	mov	x2, x22
   41f20:	mov	x3, x24
   41f24:	mov	x4, x22
   41f28:	bl	c720 <__gmpn_toom44_mul@plt>
   41f2c:	b	42014 <__gmpn_toom8h_mul@@Base+0xe44>
   41f30:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41f34:	ldur	x24, [x29, #-64]
   41f38:	ldr	x5, [sp, #56]
   41f3c:	lsl	x8, x23, #3
   41f40:	sub	x19, x8, x23
   41f44:	add	x0, x20, x19, lsl #3
   41f48:	mov	x1, x25
   41f4c:	mov	x2, x22
   41f50:	mov	x3, x24
   41f54:	mov	x4, x22
   41f58:	bl	cc20 <__gmpn_toom6h_mul@plt>
   41f5c:	b	42014 <__gmpn_toom8h_mul@@Base+0xe44>
   41f60:	add	x9, x2, x2, lsl #2
   41f64:	asr	x8, x4, #1
   41f68:	mov	w10, #0x15                  	// #21
   41f6c:	lsl	x9, x9, #1
   41f70:	mul	x10, x8, x10
   41f74:	cmp	x9, x10
   41f78:	b.lt	4120c <__gmpn_toom8h_mul@@Base+0x3c>  // b.tstop
   41f7c:	mov	w10, #0xd                   	// #13
   41f80:	mul	x10, x2, x10
   41f84:	cmp	x10, x4, lsl #4
   41f88:	b.ge	42428 <__gmpn_toom8h_mul@@Base+0x1258>  // b.tcont
   41f8c:	mov	w8, #0x8                   	// #8
   41f90:	mov	w9, #0x9                   	// #9
   41f94:	b	424f8 <__gmpn_toom8h_mul@@Base+0x1328>
   41f98:	ldur	x0, [x29, #-64]
   41f9c:	ldur	x2, [x29, #-8]
   41fa0:	ldur	x4, [x29, #-24]
   41fa4:	mov	x1, x21
   41fa8:	mov	x3, x23
   41fac:	mov	x5, x20
   41fb0:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   41fb4:	cmp	x23, #0x2f
   41fb8:	eor	w28, w0, w28
   41fbc:	b.gt	41e10 <__gmpn_toom8h_mul@@Base+0xc40>
   41fc0:	ldur	x8, [x29, #-40]
   41fc4:	mov	x0, x20
   41fc8:	mov	x1, x27
   41fcc:	mov	x2, x22
   41fd0:	add	x8, x19, x8, lsl #3
   41fd4:	add	x26, x8, #0x28
   41fd8:	mov	x3, x21
   41fdc:	mov	x4, x22
   41fe0:	mov	x5, x26
   41fe4:	bl	d450 <__gmpn_toom22_mul@plt>
   41fe8:	ldur	x24, [x29, #-64]
   41fec:	lsl	x8, x23, #3
   41ff0:	mov	x5, x26
   41ff4:	ldur	x26, [x29, #-32]
   41ff8:	sub	x19, x8, x23
   41ffc:	add	x0, x20, x19, lsl #3
   42000:	mov	x1, x25
   42004:	mov	x2, x22
   42008:	mov	x3, x24
   4200c:	mov	x4, x22
   42010:	bl	d450 <__gmpn_toom22_mul@plt>
   42014:	add	x0, x20, x19, lsl #3
   42018:	ldur	x19, [x29, #-72]
   4201c:	mov	x2, x20
   42020:	mov	w3, w28
   42024:	mov	x4, x23
   42028:	mov	x1, x19
   4202c:	mov	w5, wzr
   42030:	mov	w6, wzr
   42034:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42038:	ldur	x3, [x29, #-16]
   4203c:	mov	w6, #0x2                   	// #2
   42040:	mov	x0, x25
   42044:	mov	x1, x27
   42048:	ldur	x2, [x29, #-48]
   4204c:	mov	x4, x23
   42050:	mov	x5, x26
   42054:	mov	x7, x20
   42058:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   4205c:	ldur	x3, [x29, #-8]
   42060:	ldur	x5, [x29, #-24]
   42064:	mov	w26, w0
   42068:	mov	w6, #0x2                   	// #2
   4206c:	mov	x0, x24
   42070:	mov	x1, x21
   42074:	ldur	x2, [x29, #-56]
   42078:	mov	x4, x23
   4207c:	mov	x7, x20
   42080:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   42084:	cmp	x23, #0x2f
   42088:	eor	w28, w0, w26
   4208c:	b.le	42144 <__gmpn_toom8h_mul@@Base+0xf74>
   42090:	ldr	x9, [sp, #64]
   42094:	cmp	x23, #0x51
   42098:	b.lt	421bc <__gmpn_toom8h_mul@@Base+0xfec>  // b.tstop
   4209c:	ldur	x8, [x29, #-40]
   420a0:	ldr	x3, [sp, #48]
   420a4:	cmp	x23, #0xac
   420a8:	add	x8, x9, x8, lsl #3
   420ac:	b.lt	42254 <__gmpn_toom8h_mul@@Base+0x1084>  // b.tstop
   420b0:	add	x19, x8, #0x28
   420b4:	cmp	x23, #0xea
   420b8:	mov	x0, x20
   420bc:	mov	x1, x27
   420c0:	mov	x2, x22
   420c4:	mov	x4, x22
   420c8:	mov	x5, x19
   420cc:	b.le	422e0 <__gmpn_toom8h_mul@@Base+0x1110>
   420d0:	bl	cb20 <__gmpn_toom8h_mul@plt>
   420d4:	ldur	x26, [x29, #-64]
   420d8:	mov	x0, x27
   420dc:	mov	x1, x25
   420e0:	mov	x2, x22
   420e4:	mov	x3, x26
   420e8:	mov	x4, x22
   420ec:	mov	x5, x19
   420f0:	bl	cb20 <__gmpn_toom8h_mul@plt>
   420f4:	ldur	x1, [x29, #-72]
   420f8:	mov	w5, #0x2                   	// #2
   420fc:	mov	w6, #0x4                   	// #4
   42100:	mov	x0, x27
   42104:	mov	x2, x20
   42108:	mov	w3, w28
   4210c:	mov	x4, x23
   42110:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42114:	ldur	w21, [x29, #-76]
   42118:	ldp	x22, x25, [x29, #-32]
   4211c:	cmp	x23, #0xeb
   42120:	b.eq	42334 <__gmpn_toom8h_mul@@Base+0x1164>  // b.none
   42124:	ldp	x28, x3, [x29, #-16]
   42128:	mov	x0, x20
   4212c:	mov	x2, x23
   42130:	mov	x4, x23
   42134:	mov	x1, x28
   42138:	mov	x5, x26
   4213c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   42140:	b	42370 <__gmpn_toom8h_mul@@Base+0x11a0>
   42144:	ldur	x8, [x29, #-40]
   42148:	ldr	x9, [sp, #64]
   4214c:	mov	x0, x20
   42150:	mov	x1, x27
   42154:	mov	x2, x22
   42158:	add	x8, x9, x8, lsl #3
   4215c:	add	x26, x8, #0x28
   42160:	mov	x3, x21
   42164:	mov	x4, x22
   42168:	mov	x5, x26
   4216c:	bl	d450 <__gmpn_toom22_mul@plt>
   42170:	mov	x0, x27
   42174:	mov	x1, x25
   42178:	mov	x2, x22
   4217c:	mov	x3, x24
   42180:	mov	x4, x22
   42184:	mov	x5, x26
   42188:	bl	d450 <__gmpn_toom22_mul@plt>
   4218c:	mov	w5, #0x2                   	// #2
   42190:	mov	w6, #0x4                   	// #4
   42194:	mov	x0, x27
   42198:	mov	x1, x19
   4219c:	mov	x2, x20
   421a0:	mov	w3, w28
   421a4:	mov	x4, x23
   421a8:	bl	c970 <__gmpn_toom_couple_handling@plt>
   421ac:	ldur	w21, [x29, #-76]
   421b0:	ldp	x22, x25, [x29, #-32]
   421b4:	mov	x26, x24
   421b8:	b	42234 <__gmpn_toom8h_mul@@Base+0x1064>
   421bc:	ldur	x8, [x29, #-40]
   421c0:	ldr	x3, [sp, #48]
   421c4:	mov	x0, x20
   421c8:	mov	x1, x27
   421cc:	add	x8, x9, x8, lsl #3
   421d0:	add	x26, x8, #0x28
   421d4:	mov	x2, x22
   421d8:	mov	x4, x22
   421dc:	mov	x5, x26
   421e0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   421e4:	ldur	x3, [x29, #-64]
   421e8:	mov	x0, x27
   421ec:	mov	x1, x25
   421f0:	mov	x2, x22
   421f4:	mov	x4, x22
   421f8:	mov	x5, x26
   421fc:	mov	x26, x3
   42200:	bl	c0a0 <__gmpn_toom33_mul@plt>
   42204:	ldur	x1, [x29, #-72]
   42208:	mov	w5, #0x2                   	// #2
   4220c:	mov	w6, #0x4                   	// #4
   42210:	mov	x0, x27
   42214:	mov	x2, x20
   42218:	mov	w3, w28
   4221c:	mov	x4, x23
   42220:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42224:	ldur	w21, [x29, #-76]
   42228:	ldp	x22, x25, [x29, #-32]
   4222c:	cmp	x23, #0x30
   42230:	b.gt	422c0 <__gmpn_toom8h_mul@@Base+0x10f0>
   42234:	ldp	x28, x3, [x29, #-16]
   42238:	mov	x0, x20
   4223c:	mov	x2, x23
   42240:	mov	x4, x23
   42244:	mov	x1, x28
   42248:	mov	x5, x26
   4224c:	bl	d450 <__gmpn_toom22_mul@plt>
   42250:	b	42370 <__gmpn_toom8h_mul@@Base+0x11a0>
   42254:	add	x26, x8, #0x28
   42258:	mov	x0, x20
   4225c:	mov	x1, x27
   42260:	mov	x2, x22
   42264:	mov	x4, x22
   42268:	mov	x5, x26
   4226c:	bl	c720 <__gmpn_toom44_mul@plt>
   42270:	ldur	x3, [x29, #-64]
   42274:	mov	x0, x27
   42278:	mov	x1, x25
   4227c:	mov	x2, x22
   42280:	mov	x4, x22
   42284:	mov	x5, x26
   42288:	mov	x26, x3
   4228c:	bl	c720 <__gmpn_toom44_mul@plt>
   42290:	ldur	x1, [x29, #-72]
   42294:	mov	w5, #0x2                   	// #2
   42298:	mov	w6, #0x4                   	// #4
   4229c:	mov	x0, x27
   422a0:	mov	x2, x20
   422a4:	mov	w3, w28
   422a8:	mov	x4, x23
   422ac:	bl	c970 <__gmpn_toom_couple_handling@plt>
   422b0:	ldur	w21, [x29, #-76]
   422b4:	ldp	x22, x25, [x29, #-32]
   422b8:	cmp	x23, #0x51
   422bc:	b.gt	42354 <__gmpn_toom8h_mul@@Base+0x1184>
   422c0:	ldp	x28, x3, [x29, #-16]
   422c4:	mov	x0, x20
   422c8:	mov	x2, x23
   422cc:	mov	x4, x23
   422d0:	mov	x1, x28
   422d4:	mov	x5, x26
   422d8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   422dc:	b	42370 <__gmpn_toom8h_mul@@Base+0x11a0>
   422e0:	bl	cc20 <__gmpn_toom6h_mul@plt>
   422e4:	ldur	x26, [x29, #-64]
   422e8:	mov	x0, x27
   422ec:	mov	x1, x25
   422f0:	mov	x2, x22
   422f4:	mov	x3, x26
   422f8:	mov	x4, x22
   422fc:	mov	x5, x19
   42300:	bl	cc20 <__gmpn_toom6h_mul@plt>
   42304:	ldur	x1, [x29, #-72]
   42308:	mov	w5, #0x2                   	// #2
   4230c:	mov	w6, #0x4                   	// #4
   42310:	mov	x0, x27
   42314:	mov	x2, x20
   42318:	mov	w3, w28
   4231c:	mov	x4, x23
   42320:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42324:	ldur	w21, [x29, #-76]
   42328:	ldp	x22, x25, [x29, #-32]
   4232c:	cmp	x23, #0xac
   42330:	b.le	42354 <__gmpn_toom8h_mul@@Base+0x1184>
   42334:	ldp	x28, x3, [x29, #-16]
   42338:	mov	x0, x20
   4233c:	mov	x2, x23
   42340:	mov	x4, x23
   42344:	mov	x1, x28
   42348:	mov	x5, x26
   4234c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   42350:	b	42370 <__gmpn_toom8h_mul@@Base+0x11a0>
   42354:	ldp	x28, x3, [x29, #-16]
   42358:	mov	x0, x20
   4235c:	mov	x2, x23
   42360:	mov	x4, x23
   42364:	mov	x1, x28
   42368:	mov	x5, x26
   4236c:	bl	c720 <__gmpn_toom44_mul@plt>
   42370:	ldp	x19, x27, [sp, #24]
   42374:	ldr	x24, [sp, #72]
   42378:	cbnz	w21, 423c4 <__gmpn_toom8h_mul@@Base+0x11f4>
   4237c:	ldr	x4, [sp, #64]
   42380:	add	x6, x25, x22
   42384:	mov	x0, x20
   42388:	mov	x1, x27
   4238c:	mov	x2, x24
   42390:	mov	x3, x19
   42394:	mov	x5, x23
   42398:	mov	w7, w21
   4239c:	str	x26, [sp]
   423a0:	bl	d350 <__gmpn_toom_interpolate_16pts@plt>
   423a4:	ldp	x20, x19, [sp, #240]
   423a8:	ldp	x22, x21, [sp, #224]
   423ac:	ldp	x24, x23, [sp, #208]
   423b0:	ldp	x26, x25, [sp, #192]
   423b4:	ldp	x28, x27, [sp, #176]
   423b8:	ldp	x29, x30, [sp, #160]
   423bc:	add	sp, sp, #0x100
   423c0:	ret
   423c4:	mov	w8, #0x78                  	// #120
   423c8:	cmp	x22, x25
   423cc:	madd	x0, x23, x8, x20
   423d0:	b.le	423fc <__gmpn_toom8h_mul@@Base+0x122c>
   423d4:	ldp	x9, x8, [x29, #-56]
   423d8:	mov	x2, x22
   423dc:	mov	x4, x25
   423e0:	mov	w8, w8
   423e4:	mul	x8, x23, x8
   423e8:	add	x1, x28, x8, lsl #3
   423ec:	ldur	x8, [x29, #-8]
   423f0:	mul	x9, x23, x9
   423f4:	add	x3, x8, x9, lsl #3
   423f8:	b	42420 <__gmpn_toom8h_mul@@Base+0x1250>
   423fc:	ldp	x8, x9, [x29, #-56]
   42400:	ldur	x10, [x29, #-8]
   42404:	mov	x2, x25
   42408:	mov	x4, x22
   4240c:	mul	x8, x23, x8
   42410:	mov	w9, w9
   42414:	add	x1, x10, x8, lsl #3
   42418:	mul	x8, x23, x9
   4241c:	add	x3, x28, x8, lsl #3
   42420:	bl	ccd0 <__gmpn_mul@plt>
   42424:	b	4237c <__gmpn_toom8h_mul@@Base+0x11ac>
   42428:	mov	w10, #0x1b                  	// #27
   4242c:	mul	x10, x8, x10
   42430:	cmp	x9, x10
   42434:	b.ge	42444 <__gmpn_toom8h_mul@@Base+0x1274>  // b.tcont
   42438:	mov	w8, #0x7                   	// #7
   4243c:	mov	w9, #0x9                   	// #9
   42440:	b	424f8 <__gmpn_toom8h_mul@@Base+0x1328>
   42444:	add	x8, x8, x8, lsl #5
   42448:	cmp	x9, x8
   4244c:	b.ge	4245c <__gmpn_toom8h_mul@@Base+0x128c>  // b.tcont
   42450:	mov	w8, #0x7                   	// #7
   42454:	mov	w9, #0xa                   	// #10
   42458:	b	424f0 <__gmpn_toom8h_mul@@Base+0x1320>
   4245c:	lsl	x9, x4, #3
   42460:	lsl	x8, x2, #2
   42464:	sub	x9, x9, x4
   42468:	cmp	x8, x9
   4246c:	b.ge	4247c <__gmpn_toom8h_mul@@Base+0x12ac>  // b.tcont
   42470:	mov	w8, #0x6                   	// #6
   42474:	mov	w9, #0xa                   	// #10
   42478:	b	424f0 <__gmpn_toom8h_mul@@Base+0x1320>
   4247c:	add	x9, x2, x2, lsl #1
   42480:	mov	w10, #0xd                   	// #13
   42484:	lsl	x9, x9, #1
   42488:	mul	x10, x4, x10
   4248c:	cmp	x9, x10
   42490:	b.ge	424a0 <__gmpn_toom8h_mul@@Base+0x12d0>  // b.tcont
   42494:	mov	w8, #0x6                   	// #6
   42498:	mov	w9, #0xb                   	// #11
   4249c:	b	424f0 <__gmpn_toom8h_mul@@Base+0x1320>
   424a0:	add	x9, x4, x4, lsl #3
   424a4:	cmp	x8, x9
   424a8:	b.ge	424b8 <__gmpn_toom8h_mul@@Base+0x12e8>  // b.tcont
   424ac:	mov	w8, #0x5                   	// #5
   424b0:	mov	w9, #0xb                   	// #11
   424b4:	b	424f0 <__gmpn_toom8h_mul@@Base+0x1320>
   424b8:	lsl	x8, x2, #3
   424bc:	sub	x8, x8, x2
   424c0:	add	x9, x4, x4, lsl #2
   424c4:	cmp	x8, x9, lsl #2
   424c8:	mov	w9, #0xc                   	// #12
   424cc:	b.ge	424d8 <__gmpn_toom8h_mul@@Base+0x1308>  // b.tcont
   424d0:	mov	w8, #0x5                   	// #5
   424d4:	b	424f0 <__gmpn_toom8h_mul@@Base+0x1320>
   424d8:	mov	w10, #0x1c                  	// #28
   424dc:	add	x8, x2, x2, lsl #3
   424e0:	mul	x10, x4, x10
   424e4:	cmp	x8, x10
   424e8:	cinc	w9, w9, ge  // ge = tcont
   424ec:	mov	w8, #0x4                   	// #4
   424f0:	ldur	x16, [x29, #-16]
   424f4:	ldr	x20, [sp, #32]
   424f8:	mov	w12, w9
   424fc:	mul	x11, x8, x2
   42500:	mul	x13, x12, x4
   42504:	cmp	x11, x13
   42508:	csel	x11, x4, x2, lt  // lt = tstop
   4250c:	csel	x12, x8, x12, lt  // lt = tstop
   42510:	sub	x11, x11, #0x1
   42514:	sub	w14, w8, #0x1
   42518:	udiv	x11, x11, x12
   4251c:	sub	w15, w9, #0x1
   42520:	mov	x28, x14
   42524:	sxtw	x14, w14
   42528:	add	x23, x11, #0x1
   4252c:	add	w10, w8, w9
   42530:	msub	x5, x23, x15, x2
   42534:	msub	x26, x23, x14, x4
   42538:	tbnz	w10, #0, 42544 <__gmpn_toom8h_mul@@Base+0x1374>
   4253c:	mov	w10, wzr
   42540:	b	41234 <__gmpn_toom8h_mul@@Base+0x64>
   42544:	cmp	x5, #0x0
   42548:	b.le	4255c <__gmpn_toom8h_mul@@Base+0x138c>
   4254c:	cmp	x26, #0x0
   42550:	b.le	4256c <__gmpn_toom8h_mul@@Base+0x139c>
   42554:	mov	w10, #0x1                   	// #1
   42558:	b	41234 <__gmpn_toom8h_mul@@Base+0x64>
   4255c:	mov	w10, wzr
   42560:	sub	w15, w9, #0x2
   42564:	add	x5, x5, x23
   42568:	b	41234 <__gmpn_toom8h_mul@@Base+0x64>
   4256c:	mov	w10, wzr
   42570:	sub	w28, w8, #0x2
   42574:	add	x26, x26, x23
   42578:	b	41234 <__gmpn_toom8h_mul@@Base+0x64>

000000000004257c <__gmpn_toom8_sqr@@Base>:
   4257c:	sub	sp, sp, #0xb0
   42580:	sub	x8, x2, #0x1
   42584:	stp	x22, x21, [sp, #144]
   42588:	asr	x22, x8, #3
   4258c:	add	x21, x22, #0x1
   42590:	mov	w9, #0x68                  	// #104
   42594:	lsl	x8, x21, #3
   42598:	mov	w10, #0x58                  	// #88
   4259c:	madd	x9, x21, x9, x0
   425a0:	sub	x8, x8, x21
   425a4:	stp	x26, x25, [sp, #112]
   425a8:	stp	x24, x23, [sp, #128]
   425ac:	stp	x20, x19, [sp, #160]
   425b0:	mov	x20, x0
   425b4:	add	x26, x9, #0x10
   425b8:	sub	x23, x2, x8
   425bc:	madd	x25, x21, x10, x0
   425c0:	stp	x29, x30, [sp, #80]
   425c4:	add	x29, sp, #0x50
   425c8:	mov	x19, x3
   425cc:	mov	x24, x2
   425d0:	mov	x3, x1
   425d4:	mov	w2, #0x7                   	// #7
   425d8:	mov	w6, #0x3                   	// #3
   425dc:	mov	x0, x26
   425e0:	mov	x1, x25
   425e4:	mov	x4, x21
   425e8:	mov	x5, x23
   425ec:	mov	x7, x20
   425f0:	stp	x28, x27, [sp, #96]
   425f4:	str	x8, [sp, #40]
   425f8:	stur	x3, [x29, #-8]
   425fc:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   42600:	mov	w8, #0x60                  	// #96
   42604:	cmp	x24, #0x208
   42608:	add	x27, x22, #0x2
   4260c:	madd	x8, x21, x8, x19
   42610:	b.le	4265c <__gmpn_toom8_sqr@@Base+0xe0>
   42614:	cmp	x24, #0x520
   42618:	b.le	4268c <__gmpn_toom8_sqr@@Base+0x110>
   4261c:	cmp	x24, #0x6e0
   42620:	b.le	426bc <__gmpn_toom8_sqr@@Base+0x140>
   42624:	add	x28, x8, #0x20
   42628:	cmp	x24, #0xa58
   4262c:	mov	x0, x20
   42630:	mov	x1, x25
   42634:	mov	x2, x27
   42638:	mov	x3, x28
   4263c:	b.le	426ec <__gmpn_toom8_sqr@@Base+0x170>
   42640:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42644:	mov	x0, x19
   42648:	mov	x1, x26
   4264c:	mov	x2, x27
   42650:	mov	x3, x28
   42654:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42658:	b	42704 <__gmpn_toom8_sqr@@Base+0x188>
   4265c:	add	x28, x8, #0x20
   42660:	mov	x0, x20
   42664:	mov	x1, x25
   42668:	mov	x2, x27
   4266c:	mov	x3, x28
   42670:	bl	c050 <__gmpn_toom2_sqr@plt>
   42674:	mov	x0, x19
   42678:	mov	x1, x26
   4267c:	mov	x2, x27
   42680:	mov	x3, x28
   42684:	bl	c050 <__gmpn_toom2_sqr@plt>
   42688:	b	42704 <__gmpn_toom8_sqr@@Base+0x188>
   4268c:	add	x28, x8, #0x20
   42690:	mov	x0, x20
   42694:	mov	x1, x25
   42698:	mov	x2, x27
   4269c:	mov	x3, x28
   426a0:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   426a4:	mov	x0, x19
   426a8:	mov	x1, x26
   426ac:	mov	x2, x27
   426b0:	mov	x3, x28
   426b4:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   426b8:	b	42704 <__gmpn_toom8_sqr@@Base+0x188>
   426bc:	add	x28, x8, #0x20
   426c0:	mov	x0, x20
   426c4:	mov	x1, x25
   426c8:	mov	x2, x27
   426cc:	mov	x3, x28
   426d0:	bl	c220 <__gmpn_toom4_sqr@plt>
   426d4:	mov	x0, x19
   426d8:	mov	x1, x26
   426dc:	mov	x2, x27
   426e0:	mov	x3, x28
   426e4:	bl	c220 <__gmpn_toom4_sqr@plt>
   426e8:	b	42704 <__gmpn_toom8_sqr@@Base+0x188>
   426ec:	bl	d470 <__gmpn_toom6_sqr@plt>
   426f0:	mov	x0, x19
   426f4:	mov	x1, x26
   426f8:	mov	x2, x27
   426fc:	mov	x3, x28
   42700:	bl	d470 <__gmpn_toom6_sqr@plt>
   42704:	mov	w28, #0x1                   	// #1
   42708:	bfi	x28, x21, #1, #63
   4270c:	mov	w5, #0x3                   	// #3
   42710:	mov	x0, x19
   42714:	mov	x1, x28
   42718:	mov	x2, x20
   4271c:	mov	w3, wzr
   42720:	mov	x4, x21
   42724:	mov	w6, wzr
   42728:	bl	c970 <__gmpn_toom_couple_handling@plt>
   4272c:	ldur	x3, [x29, #-8]
   42730:	mov	w2, #0x7                   	// #7
   42734:	mov	w6, #0x2                   	// #2
   42738:	mov	x0, x26
   4273c:	mov	x1, x25
   42740:	mov	x4, x21
   42744:	mov	x5, x23
   42748:	mov	x7, x20
   4274c:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   42750:	mov	w8, #0x60                  	// #96
   42754:	madd	x8, x21, x8, x19
   42758:	cmp	x24, #0x208
   4275c:	b.le	427b4 <__gmpn_toom8_sqr@@Base+0x238>
   42760:	cmp	x24, #0x520
   42764:	b.le	427f0 <__gmpn_toom8_sqr@@Base+0x274>
   42768:	cmp	x24, #0x6e0
   4276c:	b.le	4282c <__gmpn_toom8_sqr@@Base+0x2b0>
   42770:	add	x8, x8, #0x20
   42774:	cmp	x24, #0xa58
   42778:	mov	x0, x20
   4277c:	mov	x1, x25
   42780:	mov	x2, x27
   42784:	mov	x3, x8
   42788:	stur	x8, [x29, #-16]
   4278c:	b.le	42868 <__gmpn_toom8_sqr@@Base+0x2ec>
   42790:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42794:	ldur	x3, [x29, #-16]
   42798:	add	x22, x21, x21, lsl #1
   4279c:	add	x8, x19, x22, lsl #3
   427a0:	add	x0, x8, #0x8
   427a4:	mov	x1, x26
   427a8:	mov	x2, x27
   427ac:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   427b0:	b	42888 <__gmpn_toom8_sqr@@Base+0x30c>
   427b4:	add	x22, x8, #0x20
   427b8:	mov	x0, x20
   427bc:	mov	x1, x25
   427c0:	mov	x2, x27
   427c4:	mov	x3, x22
   427c8:	bl	c050 <__gmpn_toom2_sqr@plt>
   427cc:	add	x9, x21, x21, lsl #1
   427d0:	add	x8, x19, x9, lsl #3
   427d4:	add	x0, x8, #0x8
   427d8:	mov	x1, x26
   427dc:	mov	x2, x27
   427e0:	mov	x3, x22
   427e4:	mov	x22, x9
   427e8:	bl	c050 <__gmpn_toom2_sqr@plt>
   427ec:	b	42888 <__gmpn_toom8_sqr@@Base+0x30c>
   427f0:	add	x22, x8, #0x20
   427f4:	mov	x0, x20
   427f8:	mov	x1, x25
   427fc:	mov	x2, x27
   42800:	mov	x3, x22
   42804:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42808:	add	x9, x21, x21, lsl #1
   4280c:	add	x8, x19, x9, lsl #3
   42810:	add	x0, x8, #0x8
   42814:	mov	x1, x26
   42818:	mov	x2, x27
   4281c:	mov	x3, x22
   42820:	mov	x22, x9
   42824:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42828:	b	42888 <__gmpn_toom8_sqr@@Base+0x30c>
   4282c:	add	x22, x8, #0x20
   42830:	mov	x0, x20
   42834:	mov	x1, x25
   42838:	mov	x2, x27
   4283c:	mov	x3, x22
   42840:	bl	c220 <__gmpn_toom4_sqr@plt>
   42844:	add	x9, x21, x21, lsl #1
   42848:	add	x8, x19, x9, lsl #3
   4284c:	add	x0, x8, #0x8
   42850:	mov	x1, x26
   42854:	mov	x2, x27
   42858:	mov	x3, x22
   4285c:	mov	x22, x9
   42860:	bl	c220 <__gmpn_toom4_sqr@plt>
   42864:	b	42888 <__gmpn_toom8_sqr@@Base+0x30c>
   42868:	bl	d470 <__gmpn_toom6_sqr@plt>
   4286c:	ldur	x3, [x29, #-16]
   42870:	add	x22, x21, x21, lsl #1
   42874:	add	x8, x19, x22, lsl #3
   42878:	add	x0, x8, #0x8
   4287c:	mov	x1, x26
   42880:	mov	x2, x27
   42884:	bl	d470 <__gmpn_toom6_sqr@plt>
   42888:	add	x8, x19, x22, lsl #3
   4288c:	str	x22, [sp, #24]
   42890:	add	x22, x8, #0x8
   42894:	mov	w5, #0x2                   	// #2
   42898:	mov	x0, x22
   4289c:	mov	x1, x28
   428a0:	mov	x2, x20
   428a4:	mov	w3, wzr
   428a8:	mov	x4, x21
   428ac:	mov	w6, wzr
   428b0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   428b4:	ldur	x3, [x29, #-8]
   428b8:	mov	w2, #0x7                   	// #7
   428bc:	mov	x0, x26
   428c0:	mov	x1, x25
   428c4:	mov	x4, x21
   428c8:	mov	x5, x23
   428cc:	mov	x6, x20
   428d0:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   428d4:	mov	w8, #0x60                  	// #96
   428d8:	cmp	x24, #0x208
   428dc:	madd	x8, x21, x8, x19
   428e0:	stur	x23, [x29, #-16]
   428e4:	str	x22, [sp, #32]
   428e8:	b.le	42944 <__gmpn_toom8_sqr@@Base+0x3c8>
   428ec:	cmp	x24, #0x520
   428f0:	b.le	42980 <__gmpn_toom8_sqr@@Base+0x404>
   428f4:	cmp	x24, #0x6e0
   428f8:	b.le	429bc <__gmpn_toom8_sqr@@Base+0x440>
   428fc:	add	x8, x8, #0x20
   42900:	cmp	x24, #0xa58
   42904:	mov	x0, x20
   42908:	mov	x1, x25
   4290c:	mov	x2, x27
   42910:	mov	x22, x8
   42914:	mov	x3, x8
   42918:	b.le	429f8 <__gmpn_toom8_sqr@@Base+0x47c>
   4291c:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42920:	add	x8, x21, x21, lsl #1
   42924:	lsl	x23, x8, #1
   42928:	add	x8, x19, x8, lsl #4
   4292c:	add	x0, x8, #0x10
   42930:	mov	x1, x26
   42934:	mov	x2, x27
   42938:	mov	x3, x22
   4293c:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42940:	b	42a1c <__gmpn_toom8_sqr@@Base+0x4a0>
   42944:	add	x22, x8, #0x20
   42948:	mov	x0, x20
   4294c:	mov	x1, x25
   42950:	mov	x2, x27
   42954:	mov	x3, x22
   42958:	bl	c050 <__gmpn_toom2_sqr@plt>
   4295c:	add	x8, x21, x21, lsl #1
   42960:	lsl	x23, x8, #1
   42964:	add	x8, x19, x8, lsl #4
   42968:	add	x0, x8, #0x10
   4296c:	mov	x1, x26
   42970:	mov	x2, x27
   42974:	mov	x3, x22
   42978:	bl	c050 <__gmpn_toom2_sqr@plt>
   4297c:	b	42a1c <__gmpn_toom8_sqr@@Base+0x4a0>
   42980:	add	x22, x8, #0x20
   42984:	mov	x0, x20
   42988:	mov	x1, x25
   4298c:	mov	x2, x27
   42990:	mov	x3, x22
   42994:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42998:	add	x8, x21, x21, lsl #1
   4299c:	lsl	x23, x8, #1
   429a0:	add	x8, x19, x8, lsl #4
   429a4:	add	x0, x8, #0x10
   429a8:	mov	x1, x26
   429ac:	mov	x2, x27
   429b0:	mov	x3, x22
   429b4:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   429b8:	b	42a1c <__gmpn_toom8_sqr@@Base+0x4a0>
   429bc:	add	x22, x8, #0x20
   429c0:	mov	x0, x20
   429c4:	mov	x1, x25
   429c8:	mov	x2, x27
   429cc:	mov	x3, x22
   429d0:	bl	c220 <__gmpn_toom4_sqr@plt>
   429d4:	add	x8, x21, x21, lsl #1
   429d8:	lsl	x23, x8, #1
   429dc:	add	x8, x19, x8, lsl #4
   429e0:	add	x0, x8, #0x10
   429e4:	mov	x1, x26
   429e8:	mov	x2, x27
   429ec:	mov	x3, x22
   429f0:	bl	c220 <__gmpn_toom4_sqr@plt>
   429f4:	b	42a1c <__gmpn_toom8_sqr@@Base+0x4a0>
   429f8:	bl	d470 <__gmpn_toom6_sqr@plt>
   429fc:	add	x8, x21, x21, lsl #1
   42a00:	lsl	x23, x8, #1
   42a04:	add	x8, x19, x8, lsl #4
   42a08:	add	x0, x8, #0x10
   42a0c:	mov	x1, x26
   42a10:	mov	x2, x27
   42a14:	mov	x3, x22
   42a18:	bl	d470 <__gmpn_toom6_sqr@plt>
   42a1c:	add	x8, x19, x23, lsl #3
   42a20:	add	x22, x8, #0x10
   42a24:	mov	w5, #0x1                   	// #1
   42a28:	mov	w6, #0x2                   	// #2
   42a2c:	mov	x0, x22
   42a30:	mov	x1, x28
   42a34:	mov	x2, x20
   42a38:	mov	w3, wzr
   42a3c:	mov	x4, x21
   42a40:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42a44:	ldp	x5, x3, [x29, #-16]
   42a48:	mov	w2, #0x7                   	// #7
   42a4c:	mov	w6, #0x3                   	// #3
   42a50:	mov	x0, x26
   42a54:	mov	x1, x25
   42a58:	mov	x4, x21
   42a5c:	mov	x7, x20
   42a60:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   42a64:	mov	w8, #0x60                  	// #96
   42a68:	cmp	x24, #0x208
   42a6c:	madd	x8, x21, x8, x19
   42a70:	stur	x22, [x29, #-24]
   42a74:	b.le	42acc <__gmpn_toom8_sqr@@Base+0x550>
   42a78:	cmp	x24, #0x520
   42a7c:	b.le	42b04 <__gmpn_toom8_sqr@@Base+0x588>
   42a80:	cmp	x24, #0x6e0
   42a84:	b.le	42b3c <__gmpn_toom8_sqr@@Base+0x5c0>
   42a88:	add	x8, x8, #0x20
   42a8c:	cmp	x24, #0xa58
   42a90:	mov	x0, x20
   42a94:	mov	x1, x25
   42a98:	mov	x2, x27
   42a9c:	mov	x22, x8
   42aa0:	mov	x3, x8
   42aa4:	b.le	42b74 <__gmpn_toom8_sqr@@Base+0x5f8>
   42aa8:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42aac:	add	x23, x21, x21, lsl #3
   42ab0:	add	x8, x19, x23, lsl #3
   42ab4:	add	x0, x8, #0x18
   42ab8:	mov	x1, x26
   42abc:	mov	x2, x27
   42ac0:	mov	x3, x22
   42ac4:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42ac8:	b	42b94 <__gmpn_toom8_sqr@@Base+0x618>
   42acc:	add	x22, x8, #0x20
   42ad0:	mov	x0, x20
   42ad4:	mov	x1, x25
   42ad8:	mov	x2, x27
   42adc:	mov	x3, x22
   42ae0:	bl	c050 <__gmpn_toom2_sqr@plt>
   42ae4:	add	x23, x21, x21, lsl #3
   42ae8:	add	x8, x19, x23, lsl #3
   42aec:	add	x0, x8, #0x18
   42af0:	mov	x1, x26
   42af4:	mov	x2, x27
   42af8:	mov	x3, x22
   42afc:	bl	c050 <__gmpn_toom2_sqr@plt>
   42b00:	b	42b94 <__gmpn_toom8_sqr@@Base+0x618>
   42b04:	add	x22, x8, #0x20
   42b08:	mov	x0, x20
   42b0c:	mov	x1, x25
   42b10:	mov	x2, x27
   42b14:	mov	x3, x22
   42b18:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b1c:	add	x23, x21, x21, lsl #3
   42b20:	add	x8, x19, x23, lsl #3
   42b24:	add	x0, x8, #0x18
   42b28:	mov	x1, x26
   42b2c:	mov	x2, x27
   42b30:	mov	x3, x22
   42b34:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b38:	b	42b94 <__gmpn_toom8_sqr@@Base+0x618>
   42b3c:	add	x22, x8, #0x20
   42b40:	mov	x0, x20
   42b44:	mov	x1, x25
   42b48:	mov	x2, x27
   42b4c:	mov	x3, x22
   42b50:	bl	c220 <__gmpn_toom4_sqr@plt>
   42b54:	add	x23, x21, x21, lsl #3
   42b58:	add	x8, x19, x23, lsl #3
   42b5c:	add	x0, x8, #0x18
   42b60:	mov	x1, x26
   42b64:	mov	x2, x27
   42b68:	mov	x3, x22
   42b6c:	bl	c220 <__gmpn_toom4_sqr@plt>
   42b70:	b	42b94 <__gmpn_toom8_sqr@@Base+0x618>
   42b74:	bl	d470 <__gmpn_toom6_sqr@plt>
   42b78:	add	x23, x21, x21, lsl #3
   42b7c:	add	x8, x19, x23, lsl #3
   42b80:	add	x0, x8, #0x18
   42b84:	mov	x1, x26
   42b88:	mov	x2, x27
   42b8c:	mov	x3, x22
   42b90:	bl	d470 <__gmpn_toom6_sqr@plt>
   42b94:	add	x8, x19, x23, lsl #3
   42b98:	add	x22, x8, #0x18
   42b9c:	mov	w5, #0x3                   	// #3
   42ba0:	mov	w6, #0x6                   	// #6
   42ba4:	mov	x0, x22
   42ba8:	mov	x1, x28
   42bac:	mov	x2, x20
   42bb0:	mov	w3, wzr
   42bb4:	mov	x4, x21
   42bb8:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42bbc:	ldp	x23, x3, [x29, #-16]
   42bc0:	mov	w2, #0x7                   	// #7
   42bc4:	mov	w6, #0x1                   	// #1
   42bc8:	mov	x0, x26
   42bcc:	mov	x1, x25
   42bd0:	mov	x4, x21
   42bd4:	mov	x5, x23
   42bd8:	mov	x7, x20
   42bdc:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   42be0:	mov	w8, #0x60                  	// #96
   42be4:	cmp	x24, #0x208
   42be8:	madd	x8, x21, x8, x19
   42bec:	stur	x22, [x29, #-32]
   42bf0:	b.le	42c40 <__gmpn_toom8_sqr@@Base+0x6c4>
   42bf4:	cmp	x24, #0x520
   42bf8:	b.le	42c78 <__gmpn_toom8_sqr@@Base+0x6fc>
   42bfc:	cmp	x24, #0x6e0
   42c00:	b.le	42cb0 <__gmpn_toom8_sqr@@Base+0x734>
   42c04:	add	x8, x8, #0x20
   42c08:	cmp	x24, #0xa58
   42c0c:	mov	x0, x20
   42c10:	mov	x1, x25
   42c14:	mov	x2, x27
   42c18:	mov	x3, x8
   42c1c:	str	x8, [sp, #16]
   42c20:	b.le	42ce8 <__gmpn_toom8_sqr@@Base+0x76c>
   42c24:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42c28:	ldp	x3, x22, [sp, #16]
   42c2c:	mov	x1, x26
   42c30:	mov	x2, x27
   42c34:	add	x0, x20, x22, lsl #3
   42c38:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42c3c:	b	42d00 <__gmpn_toom8_sqr@@Base+0x784>
   42c40:	add	x22, x8, #0x20
   42c44:	mov	x0, x20
   42c48:	mov	x1, x25
   42c4c:	mov	x2, x27
   42c50:	mov	x3, x22
   42c54:	bl	c050 <__gmpn_toom2_sqr@plt>
   42c58:	ldr	x8, [sp, #24]
   42c5c:	mov	x1, x26
   42c60:	mov	x2, x27
   42c64:	mov	x3, x22
   42c68:	add	x0, x20, x8, lsl #3
   42c6c:	mov	x22, x8
   42c70:	bl	c050 <__gmpn_toom2_sqr@plt>
   42c74:	b	42d00 <__gmpn_toom8_sqr@@Base+0x784>
   42c78:	add	x22, x8, #0x20
   42c7c:	mov	x0, x20
   42c80:	mov	x1, x25
   42c84:	mov	x2, x27
   42c88:	mov	x3, x22
   42c8c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42c90:	ldr	x8, [sp, #24]
   42c94:	mov	x1, x26
   42c98:	mov	x2, x27
   42c9c:	mov	x3, x22
   42ca0:	add	x0, x20, x8, lsl #3
   42ca4:	mov	x22, x8
   42ca8:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42cac:	b	42d00 <__gmpn_toom8_sqr@@Base+0x784>
   42cb0:	add	x22, x8, #0x20
   42cb4:	mov	x0, x20
   42cb8:	mov	x1, x25
   42cbc:	mov	x2, x27
   42cc0:	mov	x3, x22
   42cc4:	bl	c220 <__gmpn_toom4_sqr@plt>
   42cc8:	ldr	x8, [sp, #24]
   42ccc:	mov	x1, x26
   42cd0:	mov	x2, x27
   42cd4:	mov	x3, x22
   42cd8:	add	x0, x20, x8, lsl #3
   42cdc:	mov	x22, x8
   42ce0:	bl	c220 <__gmpn_toom4_sqr@plt>
   42ce4:	b	42d00 <__gmpn_toom8_sqr@@Base+0x784>
   42ce8:	bl	d470 <__gmpn_toom6_sqr@plt>
   42cec:	ldp	x3, x22, [sp, #16]
   42cf0:	mov	x1, x26
   42cf4:	mov	x2, x27
   42cf8:	add	x0, x20, x22, lsl #3
   42cfc:	bl	d470 <__gmpn_toom6_sqr@plt>
   42d00:	add	x0, x20, x22, lsl #3
   42d04:	mov	w5, #0x1                   	// #1
   42d08:	mov	x1, x28
   42d0c:	mov	x2, x20
   42d10:	mov	w3, wzr
   42d14:	mov	x4, x21
   42d18:	mov	w6, wzr
   42d1c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42d20:	ldur	x3, [x29, #-8]
   42d24:	mov	w2, #0x7                   	// #7
   42d28:	mov	x0, x26
   42d2c:	mov	x1, x25
   42d30:	mov	x4, x21
   42d34:	mov	x5, x23
   42d38:	mov	x6, x20
   42d3c:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   42d40:	mov	w8, #0x60                  	// #96
   42d44:	madd	x8, x21, x8, x19
   42d48:	cmp	x24, #0x208
   42d4c:	b.le	42da0 <__gmpn_toom8_sqr@@Base+0x824>
   42d50:	cmp	x24, #0x520
   42d54:	b.le	42dd8 <__gmpn_toom8_sqr@@Base+0x85c>
   42d58:	cmp	x24, #0x6e0
   42d5c:	b.le	42e10 <__gmpn_toom8_sqr@@Base+0x894>
   42d60:	add	x8, x8, #0x20
   42d64:	cmp	x24, #0xa58
   42d68:	mov	x0, x20
   42d6c:	mov	x1, x25
   42d70:	mov	x2, x27
   42d74:	mov	x3, x8
   42d78:	str	x8, [sp, #24]
   42d7c:	b.le	42e48 <__gmpn_toom8_sqr@@Base+0x8cc>
   42d80:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42d84:	ldr	x22, [sp, #40]
   42d88:	ldr	x3, [sp, #24]
   42d8c:	mov	x1, x26
   42d90:	mov	x2, x27
   42d94:	add	x0, x20, x22, lsl #3
   42d98:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42d9c:	b	42e64 <__gmpn_toom8_sqr@@Base+0x8e8>
   42da0:	add	x22, x8, #0x20
   42da4:	mov	x0, x20
   42da8:	mov	x1, x25
   42dac:	mov	x2, x27
   42db0:	mov	x3, x22
   42db4:	bl	c050 <__gmpn_toom2_sqr@plt>
   42db8:	ldr	x8, [sp, #40]
   42dbc:	mov	x1, x26
   42dc0:	mov	x2, x27
   42dc4:	mov	x3, x22
   42dc8:	add	x0, x20, x8, lsl #3
   42dcc:	mov	x22, x8
   42dd0:	bl	c050 <__gmpn_toom2_sqr@plt>
   42dd4:	b	42e64 <__gmpn_toom8_sqr@@Base+0x8e8>
   42dd8:	add	x22, x8, #0x20
   42ddc:	mov	x0, x20
   42de0:	mov	x1, x25
   42de4:	mov	x2, x27
   42de8:	mov	x3, x22
   42dec:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42df0:	ldr	x8, [sp, #40]
   42df4:	mov	x1, x26
   42df8:	mov	x2, x27
   42dfc:	mov	x3, x22
   42e00:	add	x0, x20, x8, lsl #3
   42e04:	mov	x22, x8
   42e08:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42e0c:	b	42e64 <__gmpn_toom8_sqr@@Base+0x8e8>
   42e10:	add	x22, x8, #0x20
   42e14:	mov	x0, x20
   42e18:	mov	x1, x25
   42e1c:	mov	x2, x27
   42e20:	mov	x3, x22
   42e24:	bl	c220 <__gmpn_toom4_sqr@plt>
   42e28:	ldr	x8, [sp, #40]
   42e2c:	mov	x1, x26
   42e30:	mov	x2, x27
   42e34:	mov	x3, x22
   42e38:	add	x0, x20, x8, lsl #3
   42e3c:	mov	x22, x8
   42e40:	bl	c220 <__gmpn_toom4_sqr@plt>
   42e44:	b	42e64 <__gmpn_toom8_sqr@@Base+0x8e8>
   42e48:	bl	d470 <__gmpn_toom6_sqr@plt>
   42e4c:	ldr	x22, [sp, #40]
   42e50:	ldr	x3, [sp, #24]
   42e54:	mov	x1, x26
   42e58:	mov	x2, x27
   42e5c:	add	x0, x20, x22, lsl #3
   42e60:	bl	d470 <__gmpn_toom6_sqr@plt>
   42e64:	add	x0, x20, x22, lsl #3
   42e68:	mov	x1, x28
   42e6c:	mov	x2, x20
   42e70:	mov	w3, wzr
   42e74:	mov	x4, x21
   42e78:	mov	w5, wzr
   42e7c:	mov	w6, wzr
   42e80:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42e84:	ldur	x3, [x29, #-8]
   42e88:	mov	w2, #0x7                   	// #7
   42e8c:	mov	w6, #0x2                   	// #2
   42e90:	mov	x0, x26
   42e94:	mov	x1, x25
   42e98:	mov	x4, x21
   42e9c:	mov	x5, x23
   42ea0:	mov	x7, x20
   42ea4:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   42ea8:	add	x8, x21, x21, lsl #1
   42eac:	lsl	x23, x8, #2
   42eb0:	add	x8, x19, x8, lsl #5
   42eb4:	cmp	x24, #0x208
   42eb8:	b.le	42f48 <__gmpn_toom8_sqr@@Base+0x9cc>
   42ebc:	cmp	x24, #0x521
   42ec0:	b.lt	42fb4 <__gmpn_toom8_sqr@@Base+0xa38>  // b.tstop
   42ec4:	cmp	x24, #0x6e1
   42ec8:	b.lt	43010 <__gmpn_toom8_sqr@@Base+0xa94>  // b.tstop
   42ecc:	add	x22, x8, #0x20
   42ed0:	cmp	x24, #0xa58
   42ed4:	mov	x0, x20
   42ed8:	mov	x1, x25
   42edc:	mov	x2, x27
   42ee0:	mov	x3, x22
   42ee4:	b.le	43084 <__gmpn_toom8_sqr@@Base+0xb08>
   42ee8:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42eec:	mov	x0, x25
   42ef0:	mov	x1, x26
   42ef4:	mov	x2, x27
   42ef8:	mov	x3, x22
   42efc:	mov	x26, x22
   42f00:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42f04:	mov	w5, #0x2                   	// #2
   42f08:	mov	w6, #0x4                   	// #4
   42f0c:	mov	x0, x25
   42f10:	mov	x1, x28
   42f14:	mov	x2, x20
   42f18:	mov	w3, wzr
   42f1c:	mov	x4, x21
   42f20:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42f24:	ldr	x22, [sp, #32]
   42f28:	cmp	x24, #0xa60
   42f2c:	b.le	430cc <__gmpn_toom8_sqr@@Base+0xb50>
   42f30:	ldur	x1, [x29, #-8]
   42f34:	mov	x0, x20
   42f38:	mov	x2, x21
   42f3c:	mov	x3, x26
   42f40:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   42f44:	b	430fc <__gmpn_toom8_sqr@@Base+0xb80>
   42f48:	add	x22, x8, #0x20
   42f4c:	mov	x0, x20
   42f50:	mov	x1, x25
   42f54:	mov	x2, x27
   42f58:	mov	x3, x22
   42f5c:	bl	c050 <__gmpn_toom2_sqr@plt>
   42f60:	mov	x0, x25
   42f64:	mov	x1, x26
   42f68:	mov	x2, x27
   42f6c:	mov	x3, x22
   42f70:	bl	c050 <__gmpn_toom2_sqr@plt>
   42f74:	mov	w5, #0x2                   	// #2
   42f78:	mov	w6, #0x4                   	// #4
   42f7c:	mov	x0, x25
   42f80:	mov	x1, x28
   42f84:	mov	x2, x20
   42f88:	mov	w3, wzr
   42f8c:	mov	x4, x21
   42f90:	bl	c970 <__gmpn_toom_couple_handling@plt>
   42f94:	ldr	x22, [sp, #32]
   42f98:	ldur	x1, [x29, #-8]
   42f9c:	add	x8, x19, x23, lsl #3
   42fa0:	add	x3, x8, #0x20
   42fa4:	mov	x0, x20
   42fa8:	mov	x2, x21
   42fac:	bl	c050 <__gmpn_toom2_sqr@plt>
   42fb0:	b	430fc <__gmpn_toom8_sqr@@Base+0xb80>
   42fb4:	add	x22, x8, #0x20
   42fb8:	mov	x0, x20
   42fbc:	mov	x1, x25
   42fc0:	mov	x2, x27
   42fc4:	mov	x3, x22
   42fc8:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42fcc:	mov	x0, x25
   42fd0:	mov	x1, x26
   42fd4:	mov	x2, x27
   42fd8:	mov	x3, x22
   42fdc:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42fe0:	mov	w5, #0x2                   	// #2
   42fe4:	mov	w6, #0x4                   	// #4
   42fe8:	mov	x0, x25
   42fec:	mov	x1, x28
   42ff0:	mov	x2, x20
   42ff4:	mov	w3, wzr
   42ff8:	mov	x4, x21
   42ffc:	bl	c970 <__gmpn_toom_couple_handling@plt>
   43000:	ldr	x22, [sp, #32]
   43004:	cmp	x24, #0x210
   43008:	b.le	42f98 <__gmpn_toom8_sqr@@Base+0xa1c>
   4300c:	b	43068 <__gmpn_toom8_sqr@@Base+0xaec>
   43010:	add	x22, x8, #0x20
   43014:	mov	x0, x20
   43018:	mov	x1, x25
   4301c:	mov	x2, x27
   43020:	mov	x3, x22
   43024:	bl	c220 <__gmpn_toom4_sqr@plt>
   43028:	mov	x0, x25
   4302c:	mov	x1, x26
   43030:	mov	x2, x27
   43034:	mov	x3, x22
   43038:	bl	c220 <__gmpn_toom4_sqr@plt>
   4303c:	mov	w5, #0x2                   	// #2
   43040:	mov	w6, #0x4                   	// #4
   43044:	mov	x0, x25
   43048:	mov	x1, x28
   4304c:	mov	x2, x20
   43050:	mov	w3, wzr
   43054:	mov	x4, x21
   43058:	bl	c970 <__gmpn_toom_couple_handling@plt>
   4305c:	ldr	x22, [sp, #32]
   43060:	cmp	x24, #0x528
   43064:	b.gt	430e4 <__gmpn_toom8_sqr@@Base+0xb68>
   43068:	ldur	x1, [x29, #-8]
   4306c:	add	x8, x19, x23, lsl #3
   43070:	add	x3, x8, #0x20
   43074:	mov	x0, x20
   43078:	mov	x2, x21
   4307c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43080:	b	430fc <__gmpn_toom8_sqr@@Base+0xb80>
   43084:	bl	d470 <__gmpn_toom6_sqr@plt>
   43088:	mov	x0, x25
   4308c:	mov	x1, x26
   43090:	mov	x2, x27
   43094:	mov	x3, x22
   43098:	mov	x26, x22
   4309c:	bl	d470 <__gmpn_toom6_sqr@plt>
   430a0:	mov	w5, #0x2                   	// #2
   430a4:	mov	w6, #0x4                   	// #4
   430a8:	mov	x0, x25
   430ac:	mov	x1, x28
   430b0:	mov	x2, x20
   430b4:	mov	w3, wzr
   430b8:	mov	x4, x21
   430bc:	bl	c970 <__gmpn_toom_couple_handling@plt>
   430c0:	ldr	x22, [sp, #32]
   430c4:	cmp	x24, #0x6e8
   430c8:	b.le	430e4 <__gmpn_toom8_sqr@@Base+0xb68>
   430cc:	ldur	x1, [x29, #-8]
   430d0:	mov	x0, x20
   430d4:	mov	x2, x21
   430d8:	mov	x3, x26
   430dc:	bl	d470 <__gmpn_toom6_sqr@plt>
   430e0:	b	430fc <__gmpn_toom8_sqr@@Base+0xb80>
   430e4:	ldur	x1, [x29, #-8]
   430e8:	add	x8, x19, x23, lsl #3
   430ec:	add	x3, x8, #0x20
   430f0:	mov	x0, x20
   430f4:	mov	x2, x21
   430f8:	bl	c220 <__gmpn_toom4_sqr@plt>
   430fc:	ldur	x8, [x29, #-16]
   43100:	ldp	x1, x2, [x29, #-32]
   43104:	mov	x0, x20
   43108:	mov	x3, x22
   4310c:	lsl	x6, x8, #1
   43110:	add	x8, x19, x23, lsl #3
   43114:	add	x8, x8, #0x20
   43118:	mov	x4, x19
   4311c:	mov	x5, x21
   43120:	mov	w7, wzr
   43124:	str	x8, [sp]
   43128:	bl	d350 <__gmpn_toom_interpolate_16pts@plt>
   4312c:	ldp	x20, x19, [sp, #160]
   43130:	ldp	x22, x21, [sp, #144]
   43134:	ldp	x24, x23, [sp, #128]
   43138:	ldp	x26, x25, [sp, #112]
   4313c:	ldp	x28, x27, [sp, #96]
   43140:	ldp	x29, x30, [sp, #80]
   43144:	add	sp, sp, #0xb0
   43148:	ret

000000000004314c <__gmpn_toom_couple_handling@@Base>:
   4314c:	stp	x29, x30, [sp, #-64]!
   43150:	stp	x20, x19, [sp, #48]
   43154:	mov	x19, x0
   43158:	stp	x24, x23, [sp, #16]
   4315c:	stp	x22, x21, [sp, #32]
   43160:	mov	w23, w6
   43164:	mov	w24, w5
   43168:	mov	x21, x4
   4316c:	mov	x22, x2
   43170:	mov	x20, x1
   43174:	mov	x0, x2
   43178:	mov	x1, x19
   4317c:	mov	x29, sp
   43180:	cbz	w3, 43190 <__gmpn_toom_couple_handling@@Base+0x44>
   43184:	mov	x3, x20
   43188:	bl	c840 <__gmpn_rsh1sub_n@plt>
   4318c:	b	43198 <__gmpn_toom_couple_handling@@Base+0x4c>
   43190:	mov	x3, x20
   43194:	bl	c950 <__gmpn_rsh1add_n@plt>
   43198:	mov	x0, x19
   4319c:	mov	x1, x19
   431a0:	mov	x2, x22
   431a4:	mov	x3, x20
   431a8:	cmp	w24, #0x1
   431ac:	b.ne	431b8 <__gmpn_toom_couple_handling@@Base+0x6c>  // b.any
   431b0:	bl	c840 <__gmpn_rsh1sub_n@plt>
   431b4:	b	431d8 <__gmpn_toom_couple_handling@@Base+0x8c>
   431b8:	bl	c2d0 <__gmpn_sub_n@plt>
   431bc:	cmp	w24, #0x1
   431c0:	b.lt	431d8 <__gmpn_toom_couple_handling@@Base+0x8c>  // b.tstop
   431c4:	mov	x0, x19
   431c8:	mov	x1, x19
   431cc:	mov	x2, x20
   431d0:	mov	w3, w24
   431d4:	bl	c1a0 <__gmpn_rshift@plt>
   431d8:	cmp	w23, #0x1
   431dc:	b.lt	431f4 <__gmpn_toom_couple_handling@@Base+0xa8>  // b.tstop
   431e0:	mov	x0, x22
   431e4:	mov	x1, x22
   431e8:	mov	x2, x20
   431ec:	mov	w3, w23
   431f0:	bl	c1a0 <__gmpn_rshift@plt>
   431f4:	lsl	x23, x21, #3
   431f8:	add	x0, x19, x23
   431fc:	sub	x3, x20, x21
   43200:	mov	x1, x0
   43204:	mov	x2, x22
   43208:	bl	ca70 <__gmpn_add_n@plt>
   4320c:	lsl	x10, x20, #3
   43210:	add	x14, x19, x10
   43214:	add	x8, x22, x10
   43218:	str	x0, [x14]
   4321c:	sub	x15, x8, x23
   43220:	ldr	x9, [x15]
   43224:	adds	x9, x9, x0
   43228:	str	x9, [x14]
   4322c:	b.cc	43318 <__gmpn_toom_couple_handling@@Base+0x1cc>  // b.lo, b.ul, b.last
   43230:	neg	x9, x21
   43234:	sub	x11, x21, #0x1
   43238:	add	x12, x22, x9, lsl #3
   4323c:	mov	w9, #0x1                   	// #1
   43240:	mov	x13, x19
   43244:	cmp	x9, x21
   43248:	b.ge	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.tcont
   4324c:	add	x16, x12, x10
   43250:	ldr	x16, [x16, #8]
   43254:	add	x17, x13, x10
   43258:	add	x9, x9, #0x1
   4325c:	add	x13, x13, #0x8
   43260:	add	x12, x12, #0x8
   43264:	adds	x16, x16, #0x1
   43268:	sub	x11, x11, #0x1
   4326c:	str	x16, [x17, #8]
   43270:	b.cs	43244 <__gmpn_toom_couple_handling@@Base+0xf8>  // b.hs, b.nlast
   43274:	cmp	x15, x14
   43278:	b.eq	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   4327c:	cmp	x9, x21
   43280:	b.ge	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.tcont
   43284:	sub	x14, x21, x9
   43288:	cmp	x14, #0x4
   4328c:	b.cc	432f8 <__gmpn_toom_couple_handling@@Base+0x1ac>  // b.lo, b.ul, b.last
   43290:	add	x15, x13, x10
   43294:	add	x15, x15, #0x8
   43298:	cmp	x15, x8
   4329c:	b.cs	432b8 <__gmpn_toom_couple_handling@@Base+0x16c>  // b.hs, b.nlast
   432a0:	add	x15, x21, x20
   432a4:	add	x16, x12, x10
   432a8:	add	x15, x19, x15, lsl #3
   432ac:	add	x16, x16, #0x8
   432b0:	cmp	x16, x15
   432b4:	b.cc	432f8 <__gmpn_toom_couple_handling@@Base+0x1ac>  // b.lo, b.ul, b.last
   432b8:	add	x13, x13, x10
   432bc:	add	x12, x12, x10
   432c0:	and	x10, x14, #0xfffffffffffffffc
   432c4:	and	x15, x11, #0xfffffffffffffffc
   432c8:	add	x11, x13, #0x18
   432cc:	add	x12, x12, #0x18
   432d0:	add	x9, x15, x9
   432d4:	mov	x13, x10
   432d8:	ldp	q0, q1, [x12, #-16]
   432dc:	subs	x13, x13, #0x4
   432e0:	add	x12, x12, #0x20
   432e4:	stp	q0, q1, [x11, #-16]
   432e8:	add	x11, x11, #0x20
   432ec:	b.ne	432d8 <__gmpn_toom_couple_handling@@Base+0x18c>  // b.any
   432f0:	cmp	x14, x10
   432f4:	b.eq	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   432f8:	sub	x10, x9, x21
   432fc:	add	x9, x9, x20
   43300:	add	x9, x19, x9, lsl #3
   43304:	ldr	x11, [x8, x10, lsl #3]
   43308:	adds	x10, x10, #0x1
   4330c:	str	x11, [x9], #8
   43310:	b.cc	43304 <__gmpn_toom_couple_handling@@Base+0x1b8>  // b.lo, b.ul, b.last
   43314:	b	4337c <__gmpn_toom_couple_handling@@Base+0x230>
   43318:	cmp	x21, #0x2
   4331c:	b.lt	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.tstop
   43320:	cmp	x15, x14
   43324:	b.eq	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   43328:	sub	x9, x21, #0x1
   4332c:	cmp	x9, #0x4
   43330:	b.cc	4335c <__gmpn_toom_couple_handling@@Base+0x210>  // b.lo, b.ul, b.last
   43334:	add	x10, x20, #0x1
   43338:	add	x11, x19, x10, lsl #3
   4333c:	cmp	x11, x8
   43340:	b.cs	43390 <__gmpn_toom_couple_handling@@Base+0x244>  // b.hs, b.nlast
   43344:	add	x11, x21, x20
   43348:	sub	x10, x10, x21
   4334c:	add	x11, x19, x11, lsl #3
   43350:	add	x10, x22, x10, lsl #3
   43354:	cmp	x10, x11
   43358:	b.cs	43390 <__gmpn_toom_couple_handling@@Base+0x244>  // b.hs, b.nlast
   4335c:	mov	w10, #0x1                   	// #1
   43360:	sub	x9, x10, x21
   43364:	add	x10, x10, x20
   43368:	add	x10, x19, x10, lsl #3
   4336c:	ldr	x11, [x8, x9, lsl #3]
   43370:	adds	x9, x9, #0x1
   43374:	str	x11, [x10], #8
   43378:	b.cc	4336c <__gmpn_toom_couple_handling@@Base+0x220>  // b.lo, b.ul, b.last
   4337c:	ldp	x20, x19, [sp, #48]
   43380:	ldp	x22, x21, [sp, #32]
   43384:	ldp	x24, x23, [sp, #16]
   43388:	ldp	x29, x30, [sp], #64
   4338c:	ret
   43390:	add	x13, x20, #0x3
   43394:	and	x11, x9, #0xfffffffffffffffc
   43398:	sub	x12, x13, x21
   4339c:	orr	x10, x11, #0x1
   433a0:	add	x12, x22, x12, lsl #3
   433a4:	add	x13, x19, x13, lsl #3
   433a8:	mov	x14, x11
   433ac:	ldp	q0, q1, [x12, #-16]
   433b0:	subs	x14, x14, #0x4
   433b4:	add	x12, x12, #0x20
   433b8:	stp	q0, q1, [x13, #-16]
   433bc:	add	x13, x13, #0x20
   433c0:	b.ne	433ac <__gmpn_toom_couple_handling@@Base+0x260>  // b.any
   433c4:	cmp	x9, x11
   433c8:	b.eq	4337c <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   433cc:	b	43360 <__gmpn_toom_couple_handling@@Base+0x214>

00000000000433d0 <__gmpn_toom2_sqr@@Base>:
   433d0:	sub	sp, sp, #0x70
   433d4:	stp	x22, x21, [sp, #80]
   433d8:	asr	x21, x2, #1
   433dc:	sub	x22, x2, x21
   433e0:	stp	x28, x27, [sp, #32]
   433e4:	stp	x24, x23, [sp, #64]
   433e8:	stp	x20, x19, [sp, #96]
   433ec:	mov	x27, x3
   433f0:	mov	x28, x2
   433f4:	mov	x24, x1
   433f8:	mov	x19, x0
   433fc:	cmp	x21, x22
   43400:	add	x2, x1, x21, lsl #3
   43404:	stp	x29, x30, [sp, #16]
   43408:	stp	x26, x25, [sp, #48]
   4340c:	add	x29, sp, #0x10
   43410:	b.ne	43454 <__gmpn_toom2_sqr@@Base+0x84>  // b.any
   43414:	lsl	x8, x21, #4
   43418:	sub	x8, x8, #0x8
   4341c:	mov	x10, x21
   43420:	subs	x9, x10, #0x1
   43424:	b.lt	43448 <__gmpn_toom2_sqr@@Base+0x78>  // b.tstop
   43428:	add	x10, x24, x10, lsl #3
   4342c:	ldur	x10, [x10, #-8]
   43430:	ldr	x11, [x24, x8]
   43434:	sub	x8, x8, #0x8
   43438:	cmp	x10, x11
   4343c:	mov	x10, x9
   43440:	b.eq	43420 <__gmpn_toom2_sqr@@Base+0x50>  // b.none
   43444:	b.ls	43528 <__gmpn_toom2_sqr@@Base+0x158>  // b.plast
   43448:	mov	x0, x19
   4344c:	mov	x1, x24
   43450:	b	43534 <__gmpn_toom2_sqr@@Base+0x164>
   43454:	ldr	x23, [x2]
   43458:	cbz	x23, 43484 <__gmpn_toom2_sqr@@Base+0xb4>
   4345c:	add	x2, x24, x22, lsl #3
   43460:	mov	x0, x19
   43464:	mov	x1, x24
   43468:	mov	x3, x21
   4346c:	bl	c2d0 <__gmpn_sub_n@plt>
   43470:	sub	x8, x23, x0
   43474:	str	x8, [x19, x21, lsl #3]
   43478:	cmp	x22, #0x11
   4347c:	b.le	434d8 <__gmpn_toom2_sqr@@Base+0x108>
   43480:	b	43544 <__gmpn_toom2_sqr@@Base+0x174>
   43484:	lsl	x8, x28, #3
   43488:	add	x1, x24, x22, lsl #3
   4348c:	sub	x8, x8, #0x8
   43490:	mov	x10, x21
   43494:	subs	x9, x10, #0x1
   43498:	b.lt	4345c <__gmpn_toom2_sqr@@Base+0x8c>  // b.tstop
   4349c:	add	x10, x24, x10, lsl #3
   434a0:	ldur	x10, [x10, #-8]
   434a4:	ldr	x11, [x24, x8]
   434a8:	sub	x8, x8, #0x8
   434ac:	cmp	x10, x11
   434b0:	mov	x10, x9
   434b4:	b.eq	43494 <__gmpn_toom2_sqr@@Base+0xc4>  // b.none
   434b8:	b.hi	4345c <__gmpn_toom2_sqr@@Base+0x8c>  // b.pmore
   434bc:	mov	x0, x19
   434c0:	mov	x2, x24
   434c4:	mov	x3, x21
   434c8:	bl	c2d0 <__gmpn_sub_n@plt>
   434cc:	str	xzr, [x19, x21, lsl #3]
   434d0:	cmp	x22, #0x11
   434d4:	b.gt	43544 <__gmpn_toom2_sqr@@Base+0x174>
   434d8:	mov	x0, x27
   434dc:	mov	x1, x19
   434e0:	mov	x2, x22
   434e4:	bl	c190 <__gmpn_sqr_basecase@plt>
   434e8:	lsl	x25, x22, #1
   434ec:	add	x26, x19, x25, lsl #3
   434f0:	cmp	x28, #0x23
   434f4:	add	x1, x24, x22, lsl #3
   434f8:	b.gt	4356c <__gmpn_toom2_sqr@@Base+0x19c>
   434fc:	mov	x0, x26
   43500:	mov	x2, x21
   43504:	bl	c190 <__gmpn_sqr_basecase@plt>
   43508:	cmp	x22, #0x11
   4350c:	b.gt	43584 <__gmpn_toom2_sqr@@Base+0x1b4>
   43510:	mov	x0, x19
   43514:	mov	x1, x24
   43518:	mov	x2, x22
   4351c:	mov	x20, x27
   43520:	bl	c190 <__gmpn_sqr_basecase@plt>
   43524:	b	4359c <__gmpn_toom2_sqr@@Base+0x1cc>
   43528:	mov	x0, x19
   4352c:	mov	x1, x2
   43530:	mov	x2, x24
   43534:	mov	x3, x21
   43538:	bl	c2d0 <__gmpn_sub_n@plt>
   4353c:	cmp	x22, #0x11
   43540:	b.le	434d8 <__gmpn_toom2_sqr@@Base+0x108>
   43544:	add	x3, x27, x22, lsl #4
   43548:	mov	x0, x27
   4354c:	mov	x1, x19
   43550:	mov	x2, x22
   43554:	lsl	x25, x22, #1
   43558:	bl	c050 <__gmpn_toom2_sqr@plt>
   4355c:	add	x26, x19, x25, lsl #3
   43560:	cmp	x28, #0x23
   43564:	add	x1, x24, x22, lsl #3
   43568:	b.le	434fc <__gmpn_toom2_sqr@@Base+0x12c>
   4356c:	add	x3, x27, x25, lsl #3
   43570:	mov	x0, x26
   43574:	mov	x2, x21
   43578:	bl	c050 <__gmpn_toom2_sqr@plt>
   4357c:	cmp	x22, #0x11
   43580:	b.le	43510 <__gmpn_toom2_sqr@@Base+0x140>
   43584:	add	x3, x27, x25, lsl #3
   43588:	mov	x0, x19
   4358c:	mov	x1, x24
   43590:	mov	x2, x22
   43594:	mov	x20, x27
   43598:	bl	c050 <__gmpn_toom2_sqr@plt>
   4359c:	add	x27, x19, x22, lsl #3
   435a0:	mov	x0, x26
   435a4:	mov	x1, x27
   435a8:	mov	x2, x26
   435ac:	mov	x3, x22
   435b0:	bl	ca70 <__gmpn_add_n@plt>
   435b4:	mov	x24, x0
   435b8:	mov	x0, x27
   435bc:	mov	x1, x26
   435c0:	mov	x2, x19
   435c4:	mov	x3, x22
   435c8:	bl	ca70 <__gmpn_add_n@plt>
   435cc:	and	x8, x28, #0xfffffffffffffffe
   435d0:	str	x28, [sp, #8]
   435d4:	subs	x23, x8, x22
   435d8:	mov	x28, x0
   435dc:	b.eq	4361c <__gmpn_toom2_sqr@@Base+0x24c>  // b.none
   435e0:	add	x2, x26, x22, lsl #3
   435e4:	mov	x0, x26
   435e8:	mov	x1, x26
   435ec:	mov	x3, x23
   435f0:	bl	ca70 <__gmpn_add_n@plt>
   435f4:	cbz	x0, 4361c <__gmpn_toom2_sqr@@Base+0x24c>
   435f8:	mov	w8, #0x1                   	// #1
   435fc:	cmp	x23, x22
   43600:	b.ge	43620 <__gmpn_toom2_sqr@@Base+0x250>  // b.tcont
   43604:	lsl	x9, x23, #3
   43608:	ldr	x10, [x26, x9]
   4360c:	add	x23, x23, #0x1
   43610:	adds	x10, x10, #0x1
   43614:	str	x10, [x26, x9]
   43618:	b.cs	435fc <__gmpn_toom2_sqr@@Base+0x22c>  // b.hs, b.nlast
   4361c:	mov	x8, xzr
   43620:	mov	x0, x27
   43624:	mov	x1, x27
   43628:	mov	x2, x20
   4362c:	mov	x3, x25
   43630:	add	x23, x8, x24
   43634:	bl	c2d0 <__gmpn_sub_n@plt>
   43638:	sub	x8, x23, x0
   4363c:	cmp	x8, #0x3
   43640:	b.cs	436d0 <__gmpn_toom2_sqr@@Base+0x300>  // b.hs, b.nlast
   43644:	ldr	x9, [x26]
   43648:	add	x10, x28, x24
   4364c:	adds	x9, x9, x10
   43650:	str	x9, [x26]
   43654:	b.cc	43670 <__gmpn_toom2_sqr@@Base+0x2a0>  // b.lo, b.ul, b.last
   43658:	add	x9, x19, x25, lsl #3
   4365c:	add	x9, x9, #0x8
   43660:	ldr	x10, [x9]
   43664:	adds	x10, x10, #0x1
   43668:	str	x10, [x9], #8
   4366c:	b.cs	43660 <__gmpn_toom2_sqr@@Base+0x290>  // b.hs, b.nlast
   43670:	mov	w9, #0x18                  	// #24
   43674:	mul	x9, x22, x9
   43678:	ldr	x10, [x19, x9]
   4367c:	adds	x8, x10, x8
   43680:	str	x8, [x19, x9]
   43684:	ldr	x8, [sp, #8]
   43688:	b.cc	436b0 <__gmpn_toom2_sqr@@Base+0x2e0>  // b.lo, b.ul, b.last
   4368c:	add	x8, x8, x8, lsl #1
   43690:	add	x9, x21, x21, lsl #1
   43694:	sub	x8, x8, x9
   43698:	add	x8, x19, x8, lsl #3
   4369c:	add	x8, x8, #0x8
   436a0:	ldr	x9, [x8]
   436a4:	adds	x9, x9, #0x1
   436a8:	str	x9, [x8], #8
   436ac:	b.cs	436a0 <__gmpn_toom2_sqr@@Base+0x2d0>  // b.hs, b.nlast
   436b0:	ldp	x20, x19, [sp, #96]
   436b4:	ldp	x22, x21, [sp, #80]
   436b8:	ldp	x24, x23, [sp, #64]
   436bc:	ldp	x26, x25, [sp, #48]
   436c0:	ldp	x28, x27, [sp, #32]
   436c4:	ldp	x29, x30, [sp, #16]
   436c8:	add	sp, sp, #0x70
   436cc:	ret
   436d0:	lsl	x2, x22, #3
   436d4:	mov	x0, x26
   436d8:	mov	w1, wzr
   436dc:	bl	c5f0 <memset@plt>
   436e0:	b	436b0 <__gmpn_toom2_sqr@@Base+0x2e0>

00000000000436e4 <__gmpn_toom3_sqr@@Base>:
   436e4:	sub	sp, sp, #0xa0
   436e8:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   436ec:	add	x8, x2, #0x2
   436f0:	movk	x9, #0xaaab
   436f4:	umulh	x8, x8, x9
   436f8:	stp	x22, x21, [sp, #128]
   436fc:	lsr	x21, x8, #1
   43700:	lsl	x11, x21, #1
   43704:	add	x8, x3, x21, lsl #5
   43708:	lsl	x9, x21, #4
   4370c:	add	x10, x0, x21, lsl #3
   43710:	stp	x29, x30, [sp, #64]
   43714:	stp	x26, x25, [sp, #96]
   43718:	stp	x24, x23, [sp, #112]
   4371c:	stp	x20, x19, [sp, #144]
   43720:	add	x29, sp, #0x40
   43724:	mov	x19, x3
   43728:	mov	x22, x1
   4372c:	stp	x11, x0, [sp, #24]
   43730:	str	x2, [sp]
   43734:	subs	x11, x2, x11
   43738:	add	x24, x8, #0x20
   4373c:	add	x20, x3, x9
   43740:	add	x26, x10, #0x8
   43744:	add	x2, x1, x9
   43748:	lsl	x25, x21, #3
   4374c:	stp	x28, x27, [sp, #80]
   43750:	stp	x2, x11, [x29, #-16]
   43754:	b.eq	4379c <__gmpn_toom3_sqr@@Base+0xb8>  // b.none
   43758:	ldur	x23, [x29, #-8]
   4375c:	mov	x0, x19
   43760:	mov	x1, x22
   43764:	mov	x3, x23
   43768:	bl	ca70 <__gmpn_add_n@plt>
   4376c:	cbz	x0, 437a0 <__gmpn_toom3_sqr@@Base+0xbc>
   43770:	ldur	x23, [x29, #-8]
   43774:	mov	w27, #0x1                   	// #1
   43778:	cmp	x23, x21
   4377c:	b.ge	4383c <__gmpn_toom3_sqr@@Base+0x158>  // b.tcont
   43780:	lsl	x9, x23, #3
   43784:	ldr	x10, [x22, x9]
   43788:	add	x23, x23, #0x1
   4378c:	adds	x10, x10, #0x1
   43790:	str	x10, [x19, x9]
   43794:	b.cs	43778 <__gmpn_toom3_sqr@@Base+0x94>  // b.hs, b.nlast
   43798:	b	437a0 <__gmpn_toom3_sqr@@Base+0xbc>
   4379c:	mov	x23, xzr
   437a0:	cmp	x19, x22
   437a4:	mov	x27, xzr
   437a8:	b.eq	4383c <__gmpn_toom3_sqr@@Base+0x158>  // b.none
   437ac:	subs	x9, x21, x23
   437b0:	b.le	4383c <__gmpn_toom3_sqr@@Base+0x158>
   437b4:	cmp	x9, #0x4
   437b8:	b.cc	43818 <__gmpn_toom3_sqr@@Base+0x134>  // b.lo, b.ul, b.last
   437bc:	lsl	x11, x23, #3
   437c0:	add	x10, x19, x11
   437c4:	add	x12, x22, x25
   437c8:	cmp	x10, x12
   437cc:	b.cs	437e0 <__gmpn_toom3_sqr@@Base+0xfc>  // b.hs, b.nlast
   437d0:	add	x10, x19, x25
   437d4:	add	x12, x22, x11
   437d8:	cmp	x12, x10
   437dc:	b.cc	43818 <__gmpn_toom3_sqr@@Base+0x134>  // b.lo, b.ul, b.last
   437e0:	and	x10, x9, #0xfffffffffffffffc
   437e4:	add	x12, x11, #0x10
   437e8:	add	x23, x23, x10
   437ec:	add	x11, x22, x12
   437f0:	add	x12, x19, x12
   437f4:	mov	x13, x10
   437f8:	ldp	q0, q1, [x11, #-16]
   437fc:	add	x11, x11, #0x20
   43800:	subs	x13, x13, #0x4
   43804:	stp	q0, q1, [x12, #-16]
   43808:	add	x12, x12, #0x20
   4380c:	b.ne	437f8 <__gmpn_toom3_sqr@@Base+0x114>  // b.any
   43810:	cmp	x9, x10
   43814:	b.eq	43838 <__gmpn_toom3_sqr@@Base+0x154>  // b.none
   43818:	lsl	x10, x23, #3
   4381c:	sub	x9, x21, x23
   43820:	add	x8, x19, x10
   43824:	add	x10, x22, x10
   43828:	ldr	x11, [x10], #8
   4382c:	subs	x9, x9, #0x1
   43830:	str	x11, [x8], #8
   43834:	b.ne	43828 <__gmpn_toom3_sqr@@Base+0x144>  // b.any
   43838:	mov	x27, xzr
   4383c:	mov	x23, x26
   43840:	add	x26, x22, x25
   43844:	add	x8, x20, #0x10
   43848:	mov	x0, x24
   4384c:	mov	x1, x19
   43850:	mov	x2, x26
   43854:	mov	x3, x21
   43858:	mov	x28, x20
   4385c:	stur	x8, [x29, #-24]
   43860:	bl	ca70 <__gmpn_add_n@plt>
   43864:	add	x8, x0, x27
   43868:	mov	x20, x24
   4386c:	str	x25, [sp, #16]
   43870:	str	x8, [x24, x25]
   43874:	cbz	x27, 43a1c <__gmpn_toom3_sqr@@Base+0x338>
   43878:	ldur	x24, [x29, #-24]
   4387c:	mov	x1, x19
   43880:	mov	x2, x26
   43884:	mov	x3, x21
   43888:	mov	x0, x24
   4388c:	bl	c2d0 <__gmpn_sub_n@plt>
   43890:	sub	x8, x27, x0
   43894:	lsl	x9, x21, #2
   43898:	str	x9, [sp, #8]
   4389c:	str	x8, [x24, x21, lsl #3]
   438a0:	mov	x25, x20
   438a4:	mov	x2, x20
   438a8:	ldp	x1, x20, [x29, #-16]
   438ac:	mov	x0, x23
   438b0:	mov	x3, x20
   438b4:	bl	ca70 <__gmpn_add_n@plt>
   438b8:	subs	x14, x21, x20
   438bc:	b.eq	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   438c0:	ldur	x8, [x29, #-8]
   438c4:	lsl	x8, x8, #3
   438c8:	ldr	x9, [x25, x8]
   438cc:	adds	x9, x9, x0
   438d0:	str	x9, [x23, x8]
   438d4:	b.cc	43a6c <__gmpn_toom3_sqr@@Base+0x388>  // b.lo, b.ul, b.last
   438d8:	ldr	x8, [sp, #32]
   438dc:	ldr	x11, [sp]
   438e0:	add	x9, x21, x21, lsl #1
   438e4:	add	x10, x19, x21, lsl #4
   438e8:	sub	x12, x8, x21, lsl #3
   438ec:	mvn	x8, x11
   438f0:	lsl	x13, x11, #3
   438f4:	mov	w0, #0x1                   	// #1
   438f8:	add	x11, x8, x9
   438fc:	mov	w8, #0x1                   	// #1
   43900:	cmp	x8, x14
   43904:	b.ge	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tcont
   43908:	add	x15, x10, x13
   4390c:	ldr	x15, [x15, #40]
   43910:	add	x16, x12, x13
   43914:	add	x8, x8, #0x1
   43918:	add	x12, x12, #0x8
   4391c:	add	x10, x10, #0x8
   43920:	adds	x15, x15, #0x1
   43924:	sub	x11, x11, #0x1
   43928:	str	x15, [x16, #16]
   4392c:	b.cs	43900 <__gmpn_toom3_sqr@@Base+0x21c>  // b.hs, b.nlast
   43930:	cmp	x25, x23
   43934:	mov	x0, xzr
   43938:	b.eq	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   4393c:	cmp	x8, x14
   43940:	b.ge	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tcont
   43944:	ldr	x0, [sp]
   43948:	add	x14, x21, x21, lsl #1
   4394c:	sub	x15, x14, x0
   43950:	sub	x15, x15, x8
   43954:	cmp	x15, #0x4
   43958:	b.cc	439e0 <__gmpn_toom3_sqr@@Base+0x2fc>  // b.lo, b.ul, b.last
   4395c:	mov	w17, #0x28                  	// #40
   43960:	add	x16, x12, x13
   43964:	madd	x17, x21, x17, x19
   43968:	add	x16, x16, #0x10
   4396c:	add	x17, x17, #0x20
   43970:	cmp	x16, x17
   43974:	b.cs	43998 <__gmpn_toom3_sqr@@Base+0x2b4>  // b.hs, b.nlast
   43978:	ldr	x18, [sp, #32]
   4397c:	mov	w16, #0x8                   	// #8
   43980:	add	x17, x10, x13
   43984:	bfi	x16, x21, #4, #60
   43988:	add	x16, x18, x16
   4398c:	add	x17, x17, #0x28
   43990:	cmp	x17, x16
   43994:	b.cc	439e0 <__gmpn_toom3_sqr@@Base+0x2fc>  // b.lo, b.ul, b.last
   43998:	add	x12, x12, x13
   4399c:	sub	x16, x9, x0
   439a0:	add	x13, x10, x13
   439a4:	and	x17, x11, #0xfffffffffffffffc
   439a8:	add	x10, x12, #0x20
   439ac:	sub	x12, x16, x8
   439b0:	and	x9, x15, #0xfffffffffffffffc
   439b4:	add	x11, x13, #0x38
   439b8:	add	x8, x17, x8
   439bc:	and	x12, x12, #0xfffffffffffffffc
   439c0:	ldp	q0, q1, [x11, #-16]
   439c4:	subs	x12, x12, #0x4
   439c8:	add	x11, x11, #0x20
   439cc:	stp	q0, q1, [x10, #-16]
   439d0:	add	x10, x10, #0x20
   439d4:	b.ne	439c0 <__gmpn_toom3_sqr@@Base+0x2dc>  // b.any
   439d8:	cmp	x15, x9
   439dc:	b.eq	43b1c <__gmpn_toom3_sqr@@Base+0x438>  // b.none
   439e0:	ldr	x11, [sp, #32]
   439e4:	sub	x9, x14, x8
   439e8:	add	x10, x8, x0
   439ec:	sub	x8, x9, x0
   439f0:	sub	x9, x10, x21
   439f4:	add	x10, x10, x21, lsl #1
   439f8:	add	x9, x11, x9, lsl #3
   439fc:	add	x10, x19, x10, lsl #3
   43a00:	add	x9, x9, #0x8
   43a04:	add	x10, x10, #0x20
   43a08:	ldr	x11, [x10], #8
   43a0c:	subs	x8, x8, #0x1
   43a10:	str	x11, [x9], #8
   43a14:	b.ne	43a08 <__gmpn_toom3_sqr@@Base+0x324>  // b.any
   43a18:	b	43b1c <__gmpn_toom3_sqr@@Base+0x438>
   43a1c:	add	x8, x22, x21, lsl #4
   43a20:	sub	x8, x8, #0x8
   43a24:	mov	x10, x21
   43a28:	subs	x9, x10, #0x1
   43a2c:	b.lt	43878 <__gmpn_toom3_sqr@@Base+0x194>  // b.tstop
   43a30:	add	x10, x19, x10, lsl #3
   43a34:	ldur	x10, [x10, #-8]
   43a38:	ldr	x11, [x8], #-8
   43a3c:	cmp	x10, x11
   43a40:	mov	x10, x9
   43a44:	b.eq	43a28 <__gmpn_toom3_sqr@@Base+0x344>  // b.none
   43a48:	b.hi	43878 <__gmpn_toom3_sqr@@Base+0x194>  // b.pmore
   43a4c:	ldur	x24, [x29, #-24]
   43a50:	mov	x1, x26
   43a54:	mov	x2, x19
   43a58:	mov	x3, x21
   43a5c:	mov	x0, x24
   43a60:	bl	c2d0 <__gmpn_sub_n@plt>
   43a64:	mov	x8, xzr
   43a68:	b	43894 <__gmpn_toom3_sqr@@Base+0x1b0>
   43a6c:	cmp	x25, x23
   43a70:	mov	x0, xzr
   43a74:	b.eq	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   43a78:	cmp	x14, #0x2
   43a7c:	b.lt	43b20 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tstop
   43a80:	ldr	x15, [sp]
   43a84:	add	x8, x21, x21, lsl #1
   43a88:	mvn	x9, x15
   43a8c:	add	x9, x8, x9
   43a90:	cmp	x9, #0x4
   43a94:	b.cc	43ae0 <__gmpn_toom3_sqr@@Base+0x3fc>  // b.lo, b.ul, b.last
   43a98:	ldr	x13, [sp, #32]
   43a9c:	sub	x10, x15, x21
   43aa0:	mov	w12, #0x28                  	// #40
   43aa4:	add	x11, x15, x21, lsl #1
   43aa8:	add	x13, x13, x10, lsl #3
   43aac:	madd	x10, x21, x12, x19
   43ab0:	add	x12, x13, #0x10
   43ab4:	add	x10, x10, #0x20
   43ab8:	cmp	x12, x10
   43abc:	add	x10, x19, x11, lsl #3
   43ac0:	b.cs	43c24 <__gmpn_toom3_sqr@@Base+0x540>  // b.hs, b.nlast
   43ac4:	ldr	x12, [sp, #32]
   43ac8:	mov	w11, #0x8                   	// #8
   43acc:	bfi	x11, x21, #4, #60
   43ad0:	add	x11, x12, x11
   43ad4:	add	x12, x10, #0x28
   43ad8:	cmp	x12, x11
   43adc:	b.cs	43c24 <__gmpn_toom3_sqr@@Base+0x540>  // b.hs, b.nlast
   43ae0:	mov	w10, #0x1                   	// #1
   43ae4:	ldr	x11, [sp, #32]
   43ae8:	add	x9, x10, x15
   43aec:	sub	x8, x8, x10
   43af0:	sub	x10, x9, x21
   43af4:	add	x9, x9, x21, lsl #1
   43af8:	add	x10, x11, x10, lsl #3
   43afc:	add	x11, x19, x9, lsl #3
   43b00:	sub	x8, x8, x15
   43b04:	add	x9, x10, #0x8
   43b08:	add	x10, x11, #0x20
   43b0c:	ldr	x11, [x10], #8
   43b10:	subs	x8, x8, #0x1
   43b14:	str	x11, [x9], #8
   43b18:	b.ne	43b0c <__gmpn_toom3_sqr@@Base+0x428>  // b.any
   43b1c:	mov	x0, xzr
   43b20:	ldr	x24, [sp, #16]
   43b24:	str	x25, [sp]
   43b28:	mov	x1, x22
   43b2c:	mov	x2, x23
   43b30:	ldr	x8, [x25, x24]
   43b34:	mov	x3, x21
   43b38:	add	x25, x8, x0
   43b3c:	mov	x0, x23
   43b40:	bl	d090 <__gmpn_rsblsh1_n@plt>
   43b44:	add	x8, x0, x25, lsl #1
   43b48:	mov	w9, #0x28                  	// #40
   43b4c:	ldur	x1, [x29, #-24]
   43b50:	str	x8, [x23, x24]
   43b54:	madd	x8, x21, x9, x19
   43b58:	add	x25, x21, #0x1
   43b5c:	add	x27, x8, #0x28
   43b60:	mov	x0, x19
   43b64:	mov	x2, x25
   43b68:	mov	x3, x27
   43b6c:	bl	c050 <__gmpn_toom2_sqr@plt>
   43b70:	add	x28, x28, #0x8
   43b74:	mov	x0, x28
   43b78:	mov	x1, x23
   43b7c:	mov	x2, x25
   43b80:	mov	x3, x27
   43b84:	bl	c050 <__gmpn_toom2_sqr@plt>
   43b88:	ldr	x24, [sp, #32]
   43b8c:	ldr	x8, [sp, #8]
   43b90:	ldp	x1, x23, [x29, #-16]
   43b94:	mov	x3, x27
   43b98:	add	x26, x24, x8, lsl #3
   43b9c:	mov	x0, x26
   43ba0:	mov	x2, x23
   43ba4:	bl	c050 <__gmpn_toom2_sqr@plt>
   43ba8:	mov	x20, x19
   43bac:	mov	x19, x23
   43bb0:	ldp	x8, x23, [x26]
   43bb4:	stur	x8, [x29, #-16]
   43bb8:	ldr	x8, [sp, #24]
   43bbc:	ldr	x1, [sp]
   43bc0:	mov	x2, x25
   43bc4:	mov	x3, x27
   43bc8:	add	x0, x24, x8, lsl #3
   43bcc:	bl	c050 <__gmpn_toom2_sqr@plt>
   43bd0:	mov	x0, x24
   43bd4:	mov	x1, x22
   43bd8:	mov	x2, x21
   43bdc:	mov	x3, x27
   43be0:	str	x23, [x26, #8]
   43be4:	bl	c050 <__gmpn_toom2_sqr@plt>
   43be8:	lsl	x4, x19, #1
   43bec:	mov	x0, x24
   43bf0:	mov	x1, x28
   43bf4:	mov	x2, x20
   43bf8:	mov	x3, x21
   43bfc:	ldur	x6, [x29, #-16]
   43c00:	ldp	x20, x19, [sp, #144]
   43c04:	ldp	x22, x21, [sp, #128]
   43c08:	ldp	x24, x23, [sp, #112]
   43c0c:	ldp	x26, x25, [sp, #96]
   43c10:	ldp	x28, x27, [sp, #80]
   43c14:	ldp	x29, x30, [sp, #64]
   43c18:	mov	w5, wzr
   43c1c:	add	sp, sp, #0xa0
   43c20:	b	ca20 <__gmpn_toom_interpolate_5pts@plt>
   43c24:	and	x11, x9, #0xfffffffffffffffc
   43c28:	add	x12, x10, #0x38
   43c2c:	orr	x10, x11, #0x1
   43c30:	add	x13, x13, #0x20
   43c34:	mov	x14, x11
   43c38:	ldp	q0, q1, [x12, #-16]
   43c3c:	subs	x14, x14, #0x4
   43c40:	add	x12, x12, #0x20
   43c44:	stp	q0, q1, [x13, #-16]
   43c48:	add	x13, x13, #0x20
   43c4c:	b.ne	43c38 <__gmpn_toom3_sqr@@Base+0x554>  // b.any
   43c50:	cmp	x9, x11
   43c54:	b.eq	43b1c <__gmpn_toom3_sqr@@Base+0x438>  // b.none
   43c58:	b	43ae4 <__gmpn_toom3_sqr@@Base+0x400>

0000000000043c5c <__gmpn_toom4_sqr@@Base>:
   43c5c:	sub	sp, sp, #0xb0
   43c60:	add	x8, x2, #0x3
   43c64:	stp	x24, x23, [sp, #128]
   43c68:	stp	x22, x21, [sp, #144]
   43c6c:	asr	x21, x8, #2
   43c70:	and	x23, x8, #0xfffffffffffffffc
   43c74:	add	x8, x0, x23, lsl #3
   43c78:	add	x9, x3, x21, lsl #6
   43c7c:	stp	x26, x25, [sp, #112]
   43c80:	mov	x24, x1
   43c84:	add	x10, x21, x21, lsl #1
   43c88:	add	x25, x8, #0x10
   43c8c:	add	x22, x9, #0x28
   43c90:	stp	x29, x30, [sp, #80]
   43c94:	stp	x20, x19, [sp, #160]
   43c98:	add	x29, sp, #0x50
   43c9c:	mov	x19, x3
   43ca0:	mov	x26, x2
   43ca4:	sub	x4, x2, x10
   43ca8:	mov	x1, x25
   43cac:	mov	x2, x24
   43cb0:	mov	x3, x21
   43cb4:	mov	x5, x22
   43cb8:	stp	x28, x27, [sp, #96]
   43cbc:	mov	x20, x0
   43cc0:	stp	x4, x10, [x29, #-24]
   43cc4:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   43cc8:	add	x27, x21, #0x1
   43ccc:	cmp	x26, #0x104
   43cd0:	mov	x0, x19
   43cd4:	mov	x1, x20
   43cd8:	mov	x2, x27
   43cdc:	mov	x3, x22
   43ce0:	str	x26, [sp, #40]
   43ce4:	stur	x19, [x29, #-8]
   43ce8:	stur	x25, [x29, #-32]
   43cec:	str	x23, [sp, #32]
   43cf0:	b.le	43d1c <__gmpn_toom4_sqr@@Base+0xc0>
   43cf4:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43cf8:	add	x8, x19, x21, lsl #4
   43cfc:	add	x0, x8, #0x8
   43d00:	mov	x1, x25
   43d04:	mov	x2, x27
   43d08:	mov	x3, x22
   43d0c:	lsl	x28, x21, #1
   43d10:	str	x0, [sp, #24]
   43d14:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43d18:	b	43d40 <__gmpn_toom4_sqr@@Base+0xe4>
   43d1c:	bl	c050 <__gmpn_toom2_sqr@plt>
   43d20:	add	x8, x19, x21, lsl #4
   43d24:	add	x0, x8, #0x8
   43d28:	mov	x1, x25
   43d2c:	mov	x2, x27
   43d30:	mov	x3, x22
   43d34:	lsl	x28, x21, #1
   43d38:	str	x0, [sp, #24]
   43d3c:	bl	c050 <__gmpn_toom2_sqr@plt>
   43d40:	add	x1, x24, x21, lsl #3
   43d44:	mov	x0, x20
   43d48:	mov	x2, x24
   43d4c:	mov	x3, x21
   43d50:	bl	cc40 <__gmpn_addlsh1_n@plt>
   43d54:	mov	x26, x0
   43d58:	add	x1, x24, x28, lsl #3
   43d5c:	mov	x0, x20
   43d60:	mov	x2, x20
   43d64:	mov	x3, x21
   43d68:	mov	x23, x28
   43d6c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   43d70:	ldur	x19, [x29, #-24]
   43d74:	add	x28, x0, x26, lsl #1
   43d78:	subs	x25, x21, x19
   43d7c:	b.le	43df4 <__gmpn_toom4_sqr@@Base+0x198>
   43d80:	ldur	x8, [x29, #-16]
   43d84:	mov	x0, x20
   43d88:	mov	x2, x20
   43d8c:	mov	x3, x19
   43d90:	add	x1, x24, x8, lsl #3
   43d94:	bl	cc40 <__gmpn_addlsh1_n@plt>
   43d98:	add	x26, x20, x19, lsl #3
   43d9c:	str	x0, [sp, #16]
   43da0:	mov	w3, #0x1                   	// #1
   43da4:	mov	x0, x26
   43da8:	mov	x1, x26
   43dac:	mov	x2, x25
   43db0:	bl	c180 <__gmpn_lshift@plt>
   43db4:	add	x8, x0, x28, lsl #1
   43db8:	str	x8, [x20, x21, lsl #3]
   43dbc:	ldr	x8, [x26]
   43dc0:	ldr	x9, [sp, #16]
   43dc4:	ldur	x25, [x29, #-8]
   43dc8:	mov	x28, x23
   43dcc:	adds	x8, x8, x9
   43dd0:	str	x8, [x26]
   43dd4:	ldp	x10, x19, [sp, #32]
   43dd8:	b.cc	43e20 <__gmpn_toom4_sqr@@Base+0x1c4>  // b.lo, b.ul, b.last
   43ddc:	add	x8, x26, #0x8
   43de0:	ldr	x9, [x8]
   43de4:	adds	x9, x9, #0x1
   43de8:	str	x9, [x8], #8
   43dec:	b.cs	43de0 <__gmpn_toom4_sqr@@Base+0x184>  // b.hs, b.nlast
   43df0:	b	43e20 <__gmpn_toom4_sqr@@Base+0x1c4>
   43df4:	ldur	x8, [x29, #-16]
   43df8:	mov	x0, x20
   43dfc:	mov	x2, x20
   43e00:	mov	x3, x21
   43e04:	add	x1, x24, x8, lsl #3
   43e08:	bl	cc40 <__gmpn_addlsh1_n@plt>
   43e0c:	add	x8, x0, x28, lsl #1
   43e10:	str	x8, [x20, x21, lsl #3]
   43e14:	ldur	x25, [x29, #-8]
   43e18:	ldp	x10, x19, [sp, #32]
   43e1c:	mov	x28, x23
   43e20:	add	x8, x25, x10, lsl #3
   43e24:	cmp	x19, #0x104
   43e28:	add	x0, x8, #0x10
   43e2c:	mov	x1, x20
   43e30:	mov	x2, x27
   43e34:	mov	x3, x22
   43e38:	str	x0, [sp, #40]
   43e3c:	b.le	43ebc <__gmpn_toom4_sqr@@Base+0x260>
   43e40:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43e44:	ldp	x26, x23, [x29, #-32]
   43e48:	mov	x0, x20
   43e4c:	mov	x2, x24
   43e50:	mov	x3, x21
   43e54:	mov	x1, x26
   43e58:	mov	x4, x23
   43e5c:	mov	x5, x22
   43e60:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   43e64:	add	x0, x20, x28, lsl #3
   43e68:	mov	x1, x20
   43e6c:	mov	x2, x27
   43e70:	mov	x3, x22
   43e74:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43e78:	add	x8, x21, x21, lsl #1
   43e7c:	lsl	x28, x8, #1
   43e80:	add	x8, x25, x8, lsl #4
   43e84:	add	x25, x8, #0x18
   43e88:	mov	x0, x25
   43e8c:	mov	x1, x26
   43e90:	mov	x2, x27
   43e94:	mov	x3, x22
   43e98:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43e9c:	cmp	x19, #0x108
   43ea0:	b.le	43f18 <__gmpn_toom4_sqr@@Base+0x2bc>
   43ea4:	mov	x0, x20
   43ea8:	mov	x1, x24
   43eac:	mov	x2, x21
   43eb0:	mov	x3, x22
   43eb4:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43eb8:	b	43f2c <__gmpn_toom4_sqr@@Base+0x2d0>
   43ebc:	bl	c050 <__gmpn_toom2_sqr@plt>
   43ec0:	ldp	x26, x23, [x29, #-32]
   43ec4:	mov	x0, x20
   43ec8:	mov	x2, x24
   43ecc:	mov	x3, x21
   43ed0:	mov	x1, x26
   43ed4:	mov	x4, x23
   43ed8:	mov	x5, x22
   43edc:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   43ee0:	add	x0, x20, x28, lsl #3
   43ee4:	mov	x1, x20
   43ee8:	mov	x2, x27
   43eec:	mov	x3, x22
   43ef0:	bl	c050 <__gmpn_toom2_sqr@plt>
   43ef4:	add	x8, x21, x21, lsl #1
   43ef8:	lsl	x28, x8, #1
   43efc:	add	x8, x25, x8, lsl #4
   43f00:	add	x25, x8, #0x18
   43f04:	mov	x0, x25
   43f08:	mov	x1, x26
   43f0c:	mov	x2, x27
   43f10:	mov	x3, x22
   43f14:	bl	c050 <__gmpn_toom2_sqr@plt>
   43f18:	mov	x0, x20
   43f1c:	mov	x1, x24
   43f20:	mov	x2, x21
   43f24:	mov	x3, x22
   43f28:	bl	c050 <__gmpn_toom2_sqr@plt>
   43f2c:	ldur	x8, [x29, #-16]
   43f30:	add	x0, x20, x28, lsl #3
   43f34:	cmp	x23, #0x42
   43f38:	mov	x2, x23
   43f3c:	add	x1, x24, x8, lsl #3
   43f40:	mov	x3, x22
   43f44:	b.le	43f50 <__gmpn_toom4_sqr@@Base+0x2f4>
   43f48:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   43f4c:	b	43f54 <__gmpn_toom4_sqr@@Base+0x2f8>
   43f50:	bl	c050 <__gmpn_toom2_sqr@plt>
   43f54:	ldr	x3, [sp, #24]
   43f58:	ldr	x6, [sp, #40]
   43f5c:	ldur	x5, [x29, #-8]
   43f60:	lsl	x7, x23, #1
   43f64:	mov	x0, x20
   43f68:	mov	x1, x21
   43f6c:	mov	w2, wzr
   43f70:	mov	x4, x25
   43f74:	str	x22, [sp]
   43f78:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   43f7c:	ldp	x20, x19, [sp, #160]
   43f80:	ldp	x22, x21, [sp, #144]
   43f84:	ldp	x24, x23, [sp, #128]
   43f88:	ldp	x26, x25, [sp, #112]
   43f8c:	ldp	x28, x27, [sp, #96]
   43f90:	ldp	x29, x30, [sp, #80]
   43f94:	add	sp, sp, #0xb0
   43f98:	ret

0000000000043f9c <__gmpn_toom_eval_dgr3_pm1@@Base>:
   43f9c:	stp	x29, x30, [sp, #-80]!
   43fa0:	stp	x24, x23, [sp, #32]
   43fa4:	mov	x23, x2
   43fa8:	stp	x22, x21, [sp, #48]
   43fac:	mov	x21, x1
   43fb0:	add	x2, x2, x3, lsl #4
   43fb4:	mov	x1, x23
   43fb8:	str	x25, [sp, #16]
   43fbc:	stp	x20, x19, [sp, #64]
   43fc0:	mov	x29, sp
   43fc4:	mov	x19, x5
   43fc8:	mov	x24, x4
   43fcc:	mov	x22, x3
   43fd0:	mov	x20, x0
   43fd4:	bl	ca70 <__gmpn_add_n@plt>
   43fd8:	lsl	x8, x22, #3
   43fdc:	add	x25, x23, x8
   43fe0:	str	x0, [x20, x8]
   43fe4:	cbz	x24, 44028 <__gmpn_toom_eval_dgr3_pm1@@Base+0x8c>
   43fe8:	mov	w8, #0x18                  	// #24
   43fec:	madd	x2, x22, x8, x23
   43ff0:	mov	x0, x19
   43ff4:	mov	x1, x25
   43ff8:	mov	x3, x24
   43ffc:	bl	ca70 <__gmpn_add_n@plt>
   44000:	cbz	x0, 44028 <__gmpn_toom_eval_dgr3_pm1@@Base+0x8c>
   44004:	mov	w8, #0x1                   	// #1
   44008:	cmp	x24, x22
   4400c:	b.ge	440c8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.tcont
   44010:	lsl	x9, x24, #3
   44014:	ldr	x10, [x25, x9]
   44018:	add	x24, x24, #0x1
   4401c:	adds	x10, x10, #0x1
   44020:	str	x10, [x19, x9]
   44024:	b.cs	44008 <__gmpn_toom_eval_dgr3_pm1@@Base+0x6c>  // b.hs, b.nlast
   44028:	cmp	x25, x19
   4402c:	mov	x8, xzr
   44030:	b.eq	440c8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.none
   44034:	cmp	x24, x22
   44038:	b.ge	440c8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.tcont
   4403c:	sub	x8, x22, x24
   44040:	cmp	x8, #0x4
   44044:	b.cc	440a8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.lo, b.ul, b.last
   44048:	add	x11, x19, x24, lsl #3
   4404c:	add	x9, x24, x22
   44050:	add	x10, x23, x22, lsl #4
   44054:	cmp	x11, x10
   44058:	add	x10, x23, x9, lsl #3
   4405c:	b.cs	4406c <__gmpn_toom_eval_dgr3_pm1@@Base+0xd0>  // b.hs, b.nlast
   44060:	add	x9, x19, x22, lsl #3
   44064:	cmp	x10, x9
   44068:	b.cc	440a8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.lo, b.ul, b.last
   4406c:	and	x9, x8, #0xfffffffffffffffc
   44070:	add	x10, x10, #0x10
   44074:	add	x24, x24, x9
   44078:	add	x11, x11, #0x10
   4407c:	mov	x12, x9
   44080:	ldp	q0, q1, [x10, #-16]
   44084:	add	x10, x10, #0x20
   44088:	subs	x12, x12, #0x4
   4408c:	stp	q0, q1, [x11, #-16]
   44090:	add	x11, x11, #0x20
   44094:	b.ne	44080 <__gmpn_toom_eval_dgr3_pm1@@Base+0xe4>  // b.any
   44098:	cmp	x8, x9
   4409c:	b.ne	440a8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.any
   440a0:	mov	x8, xzr
   440a4:	b	440c8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>
   440a8:	add	x10, x24, x22
   440ac:	sub	x8, x22, x24
   440b0:	add	x9, x19, x24, lsl #3
   440b4:	add	x10, x23, x10, lsl #3
   440b8:	ldr	x11, [x10], #8
   440bc:	subs	x8, x8, #0x1
   440c0:	str	x11, [x9], #8
   440c4:	b.ne	440b8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x11c>  // b.any
   440c8:	add	x23, x22, #0x1
   440cc:	str	x8, [x19, x22, lsl #3]
   440d0:	add	x8, x22, #0x1
   440d4:	cmp	x8, #0x1
   440d8:	b.lt	440f8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x15c>  // b.tstop
   440dc:	lsl	x8, x22, #3
   440e0:	ldr	x9, [x20, x8]
   440e4:	ldr	x8, [x19, x8]
   440e8:	sub	x22, x22, #0x1
   440ec:	cmp	x9, x8
   440f0:	b.eq	440d0 <__gmpn_toom_eval_dgr3_pm1@@Base+0x134>  // b.none
   440f4:	b.ls	44114 <__gmpn_toom_eval_dgr3_pm1@@Base+0x178>  // b.plast
   440f8:	mov	x0, x21
   440fc:	mov	x1, x20
   44100:	mov	x2, x19
   44104:	mov	x3, x23
   44108:	bl	c2d0 <__gmpn_sub_n@plt>
   4410c:	mov	w21, wzr
   44110:	b	4412c <__gmpn_toom_eval_dgr3_pm1@@Base+0x190>
   44114:	mov	x0, x21
   44118:	mov	x1, x19
   4411c:	mov	x2, x20
   44120:	mov	x3, x23
   44124:	bl	c2d0 <__gmpn_sub_n@plt>
   44128:	mov	w21, #0xffffffff            	// #-1
   4412c:	mov	x0, x20
   44130:	mov	x1, x20
   44134:	mov	x2, x19
   44138:	mov	x3, x23
   4413c:	bl	ca70 <__gmpn_add_n@plt>
   44140:	mov	w0, w21
   44144:	ldp	x20, x19, [sp, #64]
   44148:	ldp	x22, x21, [sp, #48]
   4414c:	ldp	x24, x23, [sp, #32]
   44150:	ldr	x25, [sp, #16]
   44154:	ldp	x29, x30, [sp], #80
   44158:	ret

000000000004415c <__gmpn_toom_eval_dgr3_pm2@@Base>:
   4415c:	stp	x29, x30, [sp, #-80]!
   44160:	stp	x24, x23, [sp, #32]
   44164:	mov	x23, x2
   44168:	stp	x22, x21, [sp, #48]
   4416c:	mov	x21, x1
   44170:	add	x2, x2, x3, lsl #4
   44174:	mov	x1, x23
   44178:	str	x25, [sp, #16]
   4417c:	stp	x20, x19, [sp, #64]
   44180:	mov	x29, sp
   44184:	mov	x19, x5
   44188:	mov	x24, x4
   4418c:	mov	x22, x3
   44190:	mov	x20, x0
   44194:	bl	cba0 <__gmpn_addlsh2_n@plt>
   44198:	lsl	x8, x22, #3
   4419c:	mov	w9, #0x18                  	// #24
   441a0:	add	x25, x23, x8
   441a4:	str	x0, [x20, x8]
   441a8:	madd	x2, x22, x9, x23
   441ac:	mov	x0, x19
   441b0:	mov	x1, x25
   441b4:	mov	x3, x24
   441b8:	bl	cba0 <__gmpn_addlsh2_n@plt>
   441bc:	subs	x13, x22, x24
   441c0:	b.le	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>
   441c4:	lsl	x8, x24, #3
   441c8:	add	x9, x25, x8
   441cc:	ldr	x11, [x9]
   441d0:	add	x10, x19, x8
   441d4:	adds	x8, x11, x0
   441d8:	str	x8, [x10]
   441dc:	b.cc	442d0 <__gmpn_toom_eval_dgr3_pm2@@Base+0x174>  // b.lo, b.ul, b.last
   441e0:	mvn	x8, x24
   441e4:	mov	x11, xzr
   441e8:	mov	w0, #0x1                   	// #1
   441ec:	add	x12, x8, x22
   441f0:	mov	w8, #0x1                   	// #1
   441f4:	cmp	x8, x13
   441f8:	b.ge	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tcont
   441fc:	add	x14, x9, x11
   44200:	ldr	x14, [x14, #8]
   44204:	add	x15, x10, x11
   44208:	add	x8, x8, #0x1
   4420c:	add	x11, x11, #0x8
   44210:	adds	x14, x14, #0x1
   44214:	sub	x12, x12, #0x1
   44218:	str	x14, [x15, #8]
   4421c:	b.cs	441f4 <__gmpn_toom_eval_dgr3_pm2@@Base+0x98>  // b.hs, b.nlast
   44220:	cmp	x25, x19
   44224:	mov	x0, xzr
   44228:	b.eq	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.none
   4422c:	cmp	x8, x13
   44230:	b.ge	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tcont
   44234:	sub	x13, x13, x8
   44238:	cmp	x13, #0x4
   4423c:	b.cc	442a8 <__gmpn_toom_eval_dgr3_pm2@@Base+0x14c>  // b.lo, b.ul, b.last
   44240:	add	x14, x10, x11
   44244:	add	x14, x14, #0x8
   44248:	add	x15, x23, x22, lsl #4
   4424c:	cmp	x14, x15
   44250:	b.cs	44268 <__gmpn_toom_eval_dgr3_pm2@@Base+0x10c>  // b.hs, b.nlast
   44254:	add	x15, x9, x11
   44258:	add	x14, x19, x22, lsl #3
   4425c:	add	x15, x15, #0x8
   44260:	cmp	x15, x14
   44264:	b.cc	442a8 <__gmpn_toom_eval_dgr3_pm2@@Base+0x14c>  // b.lo, b.ul, b.last
   44268:	add	x10, x10, x11
   4426c:	add	x11, x9, x11
   44270:	and	x9, x13, #0xfffffffffffffffc
   44274:	and	x12, x12, #0xfffffffffffffffc
   44278:	add	x10, x10, #0x18
   4427c:	add	x11, x11, #0x18
   44280:	add	x8, x12, x8
   44284:	mov	x12, x9
   44288:	ldp	q0, q1, [x11, #-16]
   4428c:	subs	x12, x12, #0x4
   44290:	add	x11, x11, #0x20
   44294:	stp	q0, q1, [x10, #-16]
   44298:	add	x10, x10, #0x20
   4429c:	b.ne	44288 <__gmpn_toom_eval_dgr3_pm2@@Base+0x12c>  // b.any
   442a0:	cmp	x13, x9
   442a4:	b.eq	44348 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>  // b.none
   442a8:	add	x10, x8, x24
   442ac:	sub	x8, x10, x22
   442b0:	add	x9, x19, x10, lsl #3
   442b4:	add	x10, x10, x22
   442b8:	add	x10, x23, x10, lsl #3
   442bc:	ldr	x11, [x10], #8
   442c0:	adds	x8, x8, #0x1
   442c4:	str	x11, [x9], #8
   442c8:	b.cc	442bc <__gmpn_toom_eval_dgr3_pm2@@Base+0x160>  // b.lo, b.ul, b.last
   442cc:	b	44348 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>
   442d0:	cmp	x25, x19
   442d4:	mov	x0, xzr
   442d8:	b.eq	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.none
   442dc:	cmp	x13, #0x2
   442e0:	b.lt	4434c <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tstop
   442e4:	mvn	x8, x24
   442e8:	add	x8, x8, x22
   442ec:	cmp	x8, #0x4
   442f0:	b.cc	44320 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1c4>  // b.lo, b.ul, b.last
   442f4:	add	x12, x19, x24, lsl #3
   442f8:	add	x9, x24, x22
   442fc:	add	x10, x12, #0x8
   44300:	add	x11, x23, x22, lsl #4
   44304:	cmp	x10, x11
   44308:	add	x9, x23, x9, lsl #3
   4430c:	b.cs	443f4 <__gmpn_toom_eval_dgr3_pm2@@Base+0x298>  // b.hs, b.nlast
   44310:	add	x10, x19, x22, lsl #3
   44314:	add	x11, x9, #0x8
   44318:	cmp	x11, x10
   4431c:	b.cs	443f4 <__gmpn_toom_eval_dgr3_pm2@@Base+0x298>  // b.hs, b.nlast
   44320:	mov	w9, #0x1                   	// #1
   44324:	add	x10, x9, x24
   44328:	sub	x8, x10, x22
   4432c:	add	x9, x19, x10, lsl #3
   44330:	add	x10, x10, x22
   44334:	add	x10, x23, x10, lsl #3
   44338:	ldr	x11, [x10], #8
   4433c:	adds	x8, x8, #0x1
   44340:	str	x11, [x9], #8
   44344:	b.cc	44338 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1dc>  // b.lo, b.ul, b.last
   44348:	mov	x0, xzr
   4434c:	add	x23, x22, #0x1
   44350:	str	x0, [x19, x22, lsl #3]
   44354:	mov	w3, #0x1                   	// #1
   44358:	mov	x0, x19
   4435c:	mov	x1, x19
   44360:	mov	x2, x23
   44364:	bl	c180 <__gmpn_lshift@plt>
   44368:	add	x8, x22, #0x1
   4436c:	cmp	x8, #0x1
   44370:	b.lt	44390 <__gmpn_toom_eval_dgr3_pm2@@Base+0x234>  // b.tstop
   44374:	lsl	x8, x22, #3
   44378:	ldr	x9, [x20, x8]
   4437c:	ldr	x8, [x19, x8]
   44380:	sub	x22, x22, #0x1
   44384:	cmp	x9, x8
   44388:	b.eq	44368 <__gmpn_toom_eval_dgr3_pm2@@Base+0x20c>  // b.none
   4438c:	b.ls	443ac <__gmpn_toom_eval_dgr3_pm2@@Base+0x250>  // b.plast
   44390:	mov	x0, x21
   44394:	mov	x1, x20
   44398:	mov	x2, x19
   4439c:	mov	x3, x23
   443a0:	bl	c2d0 <__gmpn_sub_n@plt>
   443a4:	mov	w21, wzr
   443a8:	b	443c4 <__gmpn_toom_eval_dgr3_pm2@@Base+0x268>
   443ac:	mov	x0, x21
   443b0:	mov	x1, x19
   443b4:	mov	x2, x20
   443b8:	mov	x3, x23
   443bc:	bl	c2d0 <__gmpn_sub_n@plt>
   443c0:	mov	w21, #0xffffffff            	// #-1
   443c4:	mov	x0, x20
   443c8:	mov	x1, x20
   443cc:	mov	x2, x19
   443d0:	mov	x3, x23
   443d4:	bl	ca70 <__gmpn_add_n@plt>
   443d8:	mov	w0, w21
   443dc:	ldp	x20, x19, [sp, #64]
   443e0:	ldp	x22, x21, [sp, #48]
   443e4:	ldp	x24, x23, [sp, #32]
   443e8:	ldr	x25, [sp, #16]
   443ec:	ldp	x29, x30, [sp], #80
   443f0:	ret
   443f4:	and	x10, x8, #0xfffffffffffffffc
   443f8:	add	x11, x9, #0x18
   443fc:	orr	x9, x10, #0x1
   44400:	add	x12, x12, #0x18
   44404:	mov	x13, x10
   44408:	ldp	q0, q1, [x11, #-16]
   4440c:	subs	x13, x13, #0x4
   44410:	add	x11, x11, #0x20
   44414:	stp	q0, q1, [x12, #-16]
   44418:	add	x12, x12, #0x20
   4441c:	b.ne	44408 <__gmpn_toom_eval_dgr3_pm2@@Base+0x2ac>  // b.any
   44420:	cmp	x8, x10
   44424:	b.eq	44348 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>  // b.none
   44428:	b	44324 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1c8>

000000000004442c <__gmpn_toom_eval_pm1@@Base>:
   4442c:	stp	x29, x30, [sp, #-80]!
   44430:	stp	x26, x25, [sp, #16]
   44434:	stp	x24, x23, [sp, #32]
   44438:	stp	x22, x21, [sp, #48]
   4443c:	mov	x25, x3
   44440:	mov	w24, w2
   44444:	mov	x21, x1
   44448:	add	x2, x3, x4, lsl #4
   4444c:	mov	x1, x3
   44450:	mov	x3, x4
   44454:	stp	x20, x19, [sp, #64]
   44458:	mov	x29, sp
   4445c:	mov	x19, x6
   44460:	mov	x23, x5
   44464:	mov	x22, x4
   44468:	mov	x20, x0
   4446c:	bl	ca70 <__gmpn_add_n@plt>
   44470:	cmp	w24, #0x5
   44474:	str	x0, [x20, x22, lsl #3]
   44478:	b.cc	444ec <__gmpn_toom_eval_pm1@@Base+0xc0>  // b.lo, b.ul, b.last
   4447c:	cbz	x22, 444dc <__gmpn_toom_eval_pm1@@Base+0xb0>
   44480:	mov	w26, #0x4                   	// #4
   44484:	b	44494 <__gmpn_toom_eval_pm1@@Base+0x68>
   44488:	add	w26, w26, #0x2
   4448c:	cmp	w26, w24
   44490:	b.cs	444ec <__gmpn_toom_eval_pm1@@Base+0xc0>  // b.hs, b.nlast
   44494:	mov	w8, w26
   44498:	mul	x8, x8, x22
   4449c:	add	x2, x25, x8, lsl #3
   444a0:	mov	x0, x20
   444a4:	mov	x1, x20
   444a8:	mov	x3, x22
   444ac:	bl	ca70 <__gmpn_add_n@plt>
   444b0:	cbz	x0, 44488 <__gmpn_toom_eval_pm1@@Base+0x5c>
   444b4:	mov	x8, x22
   444b8:	cmp	x8, x22
   444bc:	b.gt	44488 <__gmpn_toom_eval_pm1@@Base+0x5c>
   444c0:	lsl	x9, x8, #3
   444c4:	ldr	x10, [x20, x9]
   444c8:	add	x8, x8, #0x1
   444cc:	adds	x10, x10, #0x1
   444d0:	str	x10, [x20, x9]
   444d4:	b.cs	444b8 <__gmpn_toom_eval_pm1@@Base+0x8c>  // b.hs, b.nlast
   444d8:	b	44488 <__gmpn_toom_eval_pm1@@Base+0x5c>
   444dc:	mov	w8, #0x4                   	// #4
   444e0:	add	w8, w8, #0x2
   444e4:	cmp	w8, w24
   444e8:	b.cc	444e0 <__gmpn_toom_eval_pm1@@Base+0xb4>  // b.lo, b.ul, b.last
   444ec:	lsl	x26, x22, #3
   444f0:	mov	w8, #0x18                  	// #24
   444f4:	add	x1, x25, x26
   444f8:	madd	x2, x22, x8, x25
   444fc:	mov	x0, x19
   44500:	mov	x3, x22
   44504:	bl	ca70 <__gmpn_add_n@plt>
   44508:	cmp	w24, #0x6
   4450c:	str	x0, [x19, x26]
   44510:	b.cc	44584 <__gmpn_toom_eval_pm1@@Base+0x158>  // b.lo, b.ul, b.last
   44514:	cbz	x22, 44574 <__gmpn_toom_eval_pm1@@Base+0x148>
   44518:	mov	w26, #0x5                   	// #5
   4451c:	b	4452c <__gmpn_toom_eval_pm1@@Base+0x100>
   44520:	add	w26, w26, #0x2
   44524:	cmp	w26, w24
   44528:	b.cs	44584 <__gmpn_toom_eval_pm1@@Base+0x158>  // b.hs, b.nlast
   4452c:	mov	w8, w26
   44530:	mul	x8, x8, x22
   44534:	add	x2, x25, x8, lsl #3
   44538:	mov	x0, x19
   4453c:	mov	x1, x19
   44540:	mov	x3, x22
   44544:	bl	ca70 <__gmpn_add_n@plt>
   44548:	cbz	x0, 44520 <__gmpn_toom_eval_pm1@@Base+0xf4>
   4454c:	mov	x8, x22
   44550:	cmp	x8, x22
   44554:	b.gt	44520 <__gmpn_toom_eval_pm1@@Base+0xf4>
   44558:	lsl	x9, x8, #3
   4455c:	ldr	x10, [x19, x9]
   44560:	add	x8, x8, #0x1
   44564:	adds	x10, x10, #0x1
   44568:	str	x10, [x19, x9]
   4456c:	b.cs	44550 <__gmpn_toom_eval_pm1@@Base+0x124>  // b.hs, b.nlast
   44570:	b	44520 <__gmpn_toom_eval_pm1@@Base+0xf4>
   44574:	mov	w8, #0x5                   	// #5
   44578:	add	w8, w8, #0x2
   4457c:	cmp	w8, w24
   44580:	b.cc	44578 <__gmpn_toom_eval_pm1@@Base+0x14c>  // b.lo, b.ul, b.last
   44584:	mov	w8, w24
   44588:	mul	x8, x8, x22
   4458c:	add	x26, x22, #0x1
   44590:	add	x2, x25, x8, lsl #3
   44594:	tbnz	w24, #0, 445d4 <__gmpn_toom_eval_pm1@@Base+0x1a8>
   44598:	cbz	x23, 44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   4459c:	mov	x0, x20
   445a0:	mov	x1, x20
   445a4:	mov	x3, x23
   445a8:	bl	ca70 <__gmpn_add_n@plt>
   445ac:	cbz	x0, 44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445b0:	cmp	x23, x22
   445b4:	b.gt	44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445b8:	lsl	x8, x23, #3
   445bc:	ldr	x9, [x20, x8]
   445c0:	add	x23, x23, #0x1
   445c4:	adds	x9, x9, #0x1
   445c8:	str	x9, [x20, x8]
   445cc:	b.cs	445b0 <__gmpn_toom_eval_pm1@@Base+0x184>  // b.hs, b.nlast
   445d0:	b	44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445d4:	cbz	x23, 44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445d8:	mov	x0, x19
   445dc:	mov	x1, x19
   445e0:	mov	x3, x23
   445e4:	bl	ca70 <__gmpn_add_n@plt>
   445e8:	cbz	x0, 44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445ec:	cmp	x23, x22
   445f0:	b.gt	44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   445f4:	lsl	x8, x23, #3
   445f8:	ldr	x9, [x19, x8]
   445fc:	add	x23, x23, #0x1
   44600:	adds	x9, x9, #0x1
   44604:	str	x9, [x19, x8]
   44608:	b.cs	445ec <__gmpn_toom_eval_pm1@@Base+0x1c0>  // b.hs, b.nlast
   4460c:	b	44628 <__gmpn_toom_eval_pm1@@Base+0x1fc>
   44610:	lsl	x8, x22, #3
   44614:	ldr	x9, [x20, x8]
   44618:	ldr	x8, [x19, x8]
   4461c:	sub	x22, x22, #0x1
   44620:	cmp	x9, x8
   44624:	b.ne	44638 <__gmpn_toom_eval_pm1@@Base+0x20c>  // b.any
   44628:	add	x8, x22, #0x1
   4462c:	cmp	x8, #0x1
   44630:	b.ge	44610 <__gmpn_toom_eval_pm1@@Base+0x1e4>  // b.tcont
   44634:	b	4463c <__gmpn_toom_eval_pm1@@Base+0x210>
   44638:	b.ls	44658 <__gmpn_toom_eval_pm1@@Base+0x22c>  // b.plast
   4463c:	mov	x0, x21
   44640:	mov	x1, x20
   44644:	mov	x2, x19
   44648:	mov	x3, x26
   4464c:	bl	c2d0 <__gmpn_sub_n@plt>
   44650:	mov	w21, wzr
   44654:	b	44670 <__gmpn_toom_eval_pm1@@Base+0x244>
   44658:	mov	x0, x21
   4465c:	mov	x1, x19
   44660:	mov	x2, x20
   44664:	mov	x3, x26
   44668:	bl	c2d0 <__gmpn_sub_n@plt>
   4466c:	mov	w21, #0xffffffff            	// #-1
   44670:	mov	x0, x20
   44674:	mov	x1, x20
   44678:	mov	x2, x19
   4467c:	mov	x3, x26
   44680:	bl	ca70 <__gmpn_add_n@plt>
   44684:	mov	w0, w21
   44688:	ldp	x20, x19, [sp, #64]
   4468c:	ldp	x22, x21, [sp, #48]
   44690:	ldp	x24, x23, [sp, #32]
   44694:	ldp	x26, x25, [sp, #16]
   44698:	ldp	x29, x30, [sp], #80
   4469c:	ret

00000000000446a0 <__gmpn_toom_eval_pm2@@Base>:
   446a0:	stp	x29, x30, [sp, #-96]!
   446a4:	stp	x28, x27, [sp, #16]
   446a8:	sub	w28, w2, #0x2
   446ac:	mov	w8, w2
   446b0:	mul	x9, x28, x4
   446b4:	mul	x8, x8, x4
   446b8:	add	x27, x3, x9, lsl #3
   446bc:	stp	x24, x23, [sp, #48]
   446c0:	stp	x22, x21, [sp, #64]
   446c4:	mov	x23, x3
   446c8:	mov	w24, w2
   446cc:	mov	x21, x1
   446d0:	add	x2, x3, x8, lsl #3
   446d4:	mov	x1, x27
   446d8:	mov	x3, x5
   446dc:	stp	x26, x25, [sp, #32]
   446e0:	stp	x20, x19, [sp, #80]
   446e4:	mov	x29, sp
   446e8:	mov	x19, x6
   446ec:	mov	x25, x5
   446f0:	mov	x22, x4
   446f4:	mov	x20, x0
   446f8:	bl	cba0 <__gmpn_addlsh2_n@plt>
   446fc:	subs	x13, x22, x25
   44700:	mov	x26, x0
   44704:	b.eq	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   44708:	lsl	x8, x25, #3
   4470c:	add	x9, x27, x8
   44710:	ldr	x11, [x9]
   44714:	add	x10, x20, x8
   44718:	adds	x8, x11, x26
   4471c:	str	x8, [x10]
   44720:	b.cc	4481c <__gmpn_toom_eval_pm2@@Base+0x17c>  // b.lo, b.ul, b.last
   44724:	mvn	x8, x25
   44728:	mov	x11, xzr
   4472c:	mov	w26, #0x1                   	// #1
   44730:	add	x12, x8, x22
   44734:	mov	w8, #0x1                   	// #1
   44738:	cmp	x8, x13
   4473c:	b.ge	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tcont
   44740:	add	x14, x9, x11
   44744:	ldr	x14, [x14, #8]
   44748:	add	x15, x10, x11
   4474c:	add	x8, x8, #0x1
   44750:	add	x11, x11, #0x8
   44754:	adds	x14, x14, #0x1
   44758:	sub	x12, x12, #0x1
   4475c:	str	x14, [x15, #8]
   44760:	b.cs	44738 <__gmpn_toom_eval_pm2@@Base+0x98>  // b.hs, b.nlast
   44764:	cmp	x27, x20
   44768:	mov	x26, xzr
   4476c:	b.eq	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   44770:	cmp	x8, x13
   44774:	b.ge	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tcont
   44778:	sub	x13, x13, x8
   4477c:	cmp	x13, #0x4
   44780:	b.cc	447f4 <__gmpn_toom_eval_pm2@@Base+0x154>  // b.lo, b.ul, b.last
   44784:	add	x15, x28, #0x1
   44788:	add	x14, x10, x11
   4478c:	mul	x15, x15, x22
   44790:	add	x14, x14, #0x8
   44794:	add	x15, x23, x15, lsl #3
   44798:	cmp	x14, x15
   4479c:	b.cs	447b4 <__gmpn_toom_eval_pm2@@Base+0x114>  // b.hs, b.nlast
   447a0:	add	x15, x9, x11
   447a4:	add	x14, x20, x22, lsl #3
   447a8:	add	x15, x15, #0x8
   447ac:	cmp	x15, x14
   447b0:	b.cc	447f4 <__gmpn_toom_eval_pm2@@Base+0x154>  // b.lo, b.ul, b.last
   447b4:	add	x10, x10, x11
   447b8:	add	x11, x9, x11
   447bc:	and	x9, x13, #0xfffffffffffffffc
   447c0:	and	x12, x12, #0xfffffffffffffffc
   447c4:	add	x10, x10, #0x18
   447c8:	add	x11, x11, #0x18
   447cc:	add	x8, x12, x8
   447d0:	mov	x12, x9
   447d4:	ldp	q0, q1, [x11, #-16]
   447d8:	subs	x12, x12, #0x4
   447dc:	add	x11, x11, #0x20
   447e0:	stp	q0, q1, [x10, #-16]
   447e4:	add	x10, x10, #0x20
   447e8:	b.ne	447d4 <__gmpn_toom_eval_pm2@@Base+0x134>  // b.any
   447ec:	cmp	x13, x9
   447f0:	b.eq	4489c <__gmpn_toom_eval_pm2@@Base+0x1fc>  // b.none
   447f4:	add	x10, x8, x25
   447f8:	sub	x8, x10, x22
   447fc:	add	x9, x20, x10, lsl #3
   44800:	madd	x10, x22, x28, x10
   44804:	add	x10, x23, x10, lsl #3
   44808:	ldr	x11, [x10], #8
   4480c:	adds	x8, x8, #0x1
   44810:	str	x11, [x9], #8
   44814:	b.cc	44808 <__gmpn_toom_eval_pm2@@Base+0x168>  // b.lo, b.ul, b.last
   44818:	b	4489c <__gmpn_toom_eval_pm2@@Base+0x1fc>
   4481c:	cmp	x27, x20
   44820:	mov	x26, xzr
   44824:	b.eq	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   44828:	cmp	x13, #0x2
   4482c:	b.lt	448a0 <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tstop
   44830:	mvn	x8, x25
   44834:	add	x8, x8, x22
   44838:	cmp	x8, #0x4
   4483c:	b.cc	44874 <__gmpn_toom_eval_pm2@@Base+0x1d4>  // b.lo, b.ul, b.last
   44840:	add	x9, x28, #0x1
   44844:	add	x12, x20, x25, lsl #3
   44848:	mul	x9, x9, x22
   4484c:	add	x10, x12, #0x8
   44850:	add	x9, x23, x9, lsl #3
   44854:	cmp	x10, x9
   44858:	b.cs	449f4 <__gmpn_toom_eval_pm2@@Base+0x354>  // b.hs, b.nlast
   4485c:	madd	x10, x28, x22, x25
   44860:	add	x10, x23, x10, lsl #3
   44864:	add	x9, x20, x22, lsl #3
   44868:	add	x10, x10, #0x8
   4486c:	cmp	x10, x9
   44870:	b.cs	449f4 <__gmpn_toom_eval_pm2@@Base+0x354>  // b.hs, b.nlast
   44874:	mov	w9, #0x1                   	// #1
   44878:	add	x10, x9, x25
   4487c:	sub	x8, x10, x22
   44880:	add	x9, x20, x10, lsl #3
   44884:	madd	x10, x22, x28, x10
   44888:	add	x10, x23, x10, lsl #3
   4488c:	ldr	x11, [x10], #8
   44890:	adds	x8, x8, #0x1
   44894:	str	x11, [x9], #8
   44898:	b.cc	4488c <__gmpn_toom_eval_pm2@@Base+0x1ec>  // b.lo, b.ul, b.last
   4489c:	mov	x26, xzr
   448a0:	cmp	w24, #0x4
   448a4:	b.mi	448d0 <__gmpn_toom_eval_pm2@@Base+0x230>  // b.first
   448a8:	sub	w28, w28, #0x2
   448ac:	mul	x8, x28, x22
   448b0:	add	x1, x23, x8, lsl #3
   448b4:	mov	x0, x20
   448b8:	mov	x2, x20
   448bc:	mov	x3, x22
   448c0:	bl	cba0 <__gmpn_addlsh2_n@plt>
   448c4:	cmp	w28, #0x1
   448c8:	add	x26, x0, x26, lsl #2
   448cc:	b.gt	448a8 <__gmpn_toom_eval_pm2@@Base+0x208>
   448d0:	sub	w25, w24, #0x1
   448d4:	sub	w8, w24, #0x3
   448d8:	mul	x8, x8, x22
   448dc:	mul	x9, x25, x22
   448e0:	add	x1, x23, x8, lsl #3
   448e4:	add	x2, x23, x9, lsl #3
   448e8:	mov	x0, x19
   448ec:	mov	x3, x22
   448f0:	str	x26, [x20, x22, lsl #3]
   448f4:	bl	cba0 <__gmpn_addlsh2_n@plt>
   448f8:	subs	w26, w24, #0x5
   448fc:	mov	x24, x0
   44900:	b.mi	44930 <__gmpn_toom_eval_pm2@@Base+0x290>  // b.first
   44904:	mov	w8, w26
   44908:	mul	x8, x8, x22
   4490c:	add	x1, x23, x8, lsl #3
   44910:	mov	x0, x19
   44914:	mov	x2, x19
   44918:	mov	x3, x22
   4491c:	bl	cba0 <__gmpn_addlsh2_n@plt>
   44920:	cmp	w26, #0x1
   44924:	sub	w26, w26, #0x2
   44928:	add	x24, x0, x24, lsl #2
   4492c:	b.gt	44904 <__gmpn_toom_eval_pm2@@Base+0x264>
   44930:	add	x23, x22, #0x1
   44934:	mov	w3, #0x1                   	// #1
   44938:	str	x24, [x19, x22, lsl #3]
   4493c:	tbnz	w25, #0, 4494c <__gmpn_toom_eval_pm2@@Base+0x2ac>
   44940:	mov	x0, x20
   44944:	mov	x1, x20
   44948:	b	44954 <__gmpn_toom_eval_pm2@@Base+0x2b4>
   4494c:	mov	x0, x19
   44950:	mov	x1, x19
   44954:	mov	x2, x23
   44958:	bl	c180 <__gmpn_lshift@plt>
   4495c:	and	w24, w25, #0x1
   44960:	add	x8, x22, #0x1
   44964:	cmp	x8, #0x1
   44968:	b.lt	44988 <__gmpn_toom_eval_pm2@@Base+0x2e8>  // b.tstop
   4496c:	lsl	x8, x22, #3
   44970:	ldr	x9, [x20, x8]
   44974:	ldr	x8, [x19, x8]
   44978:	sub	x22, x22, #0x1
   4497c:	cmp	x9, x8
   44980:	b.eq	44960 <__gmpn_toom_eval_pm2@@Base+0x2c0>  // b.none
   44984:	b.ls	449a4 <__gmpn_toom_eval_pm2@@Base+0x304>  // b.plast
   44988:	mov	x0, x21
   4498c:	mov	x1, x20
   44990:	mov	x2, x19
   44994:	mov	x3, x23
   44998:	bl	c2d0 <__gmpn_sub_n@plt>
   4499c:	mov	w21, wzr
   449a0:	b	449bc <__gmpn_toom_eval_pm2@@Base+0x31c>
   449a4:	mov	x0, x21
   449a8:	mov	x1, x19
   449ac:	mov	x2, x20
   449b0:	mov	x3, x23
   449b4:	bl	c2d0 <__gmpn_sub_n@plt>
   449b8:	mov	w21, #0xffffffff            	// #-1
   449bc:	mov	x0, x20
   449c0:	mov	x1, x20
   449c4:	mov	x2, x19
   449c8:	mov	x3, x23
   449cc:	bl	ca70 <__gmpn_add_n@plt>
   449d0:	sub	w8, w24, #0x1
   449d4:	eor	w0, w21, w8
   449d8:	ldp	x20, x19, [sp, #80]
   449dc:	ldp	x22, x21, [sp, #64]
   449e0:	ldp	x24, x23, [sp, #48]
   449e4:	ldp	x26, x25, [sp, #32]
   449e8:	ldp	x28, x27, [sp, #16]
   449ec:	ldp	x29, x30, [sp], #96
   449f0:	ret
   449f4:	madd	x11, x22, x28, x25
   449f8:	and	x10, x8, #0xfffffffffffffffc
   449fc:	add	x11, x23, x11, lsl #3
   44a00:	orr	x9, x10, #0x1
   44a04:	add	x11, x11, #0x18
   44a08:	add	x12, x12, #0x18
   44a0c:	mov	x13, x10
   44a10:	ldp	q0, q1, [x11, #-16]
   44a14:	subs	x13, x13, #0x4
   44a18:	add	x11, x11, #0x20
   44a1c:	stp	q0, q1, [x12, #-16]
   44a20:	add	x12, x12, #0x20
   44a24:	b.ne	44a10 <__gmpn_toom_eval_pm2@@Base+0x370>  // b.any
   44a28:	cmp	x8, x10
   44a2c:	b.eq	4489c <__gmpn_toom_eval_pm2@@Base+0x1fc>  // b.none
   44a30:	b	44878 <__gmpn_toom_eval_pm2@@Base+0x1d8>

0000000000044a34 <__gmpn_toom_eval_pm2exp@@Base>:
   44a34:	sub	sp, sp, #0x70
   44a38:	stp	x28, x27, [sp, #32]
   44a3c:	lsl	w27, w6, #1
   44a40:	stp	x29, x30, [sp, #16]
   44a44:	stp	x26, x25, [sp, #48]
   44a48:	stp	x24, x23, [sp, #64]
   44a4c:	stp	x22, x21, [sp, #80]
   44a50:	stp	x20, x19, [sp, #96]
   44a54:	add	x29, sp, #0x10
   44a58:	mov	x26, x3
   44a5c:	mov	w24, w2
   44a60:	mov	x21, x1
   44a64:	mov	x20, x0
   44a68:	add	x1, x3, x4, lsl #4
   44a6c:	mov	x0, x7
   44a70:	mov	x2, x4
   44a74:	mov	w3, w27
   44a78:	mov	x19, x7
   44a7c:	str	x5, [sp]
   44a80:	mov	x22, x4
   44a84:	stur	w6, [x29, #-4]
   44a88:	bl	c180 <__gmpn_lshift@plt>
   44a8c:	lsl	x23, x22, #3
   44a90:	str	x0, [x20, x23]
   44a94:	mov	x0, x20
   44a98:	mov	x1, x26
   44a9c:	mov	x2, x19
   44aa0:	mov	x3, x22
   44aa4:	bl	ca70 <__gmpn_add_n@plt>
   44aa8:	ldr	x8, [x20, x23]
   44aac:	cmp	w24, #0x5
   44ab0:	add	x8, x8, x0
   44ab4:	str	x8, [x20, x23]
   44ab8:	b.cc	44b20 <__gmpn_toom_eval_pm2exp@@Base+0xec>  // b.lo, b.ul, b.last
   44abc:	ldur	w8, [x29, #-4]
   44ac0:	mov	w25, #0x4                   	// #4
   44ac4:	lsl	w28, w8, #2
   44ac8:	mov	w8, w25
   44acc:	mul	x8, x8, x22
   44ad0:	add	x1, x26, x8, lsl #3
   44ad4:	mov	x0, x19
   44ad8:	mov	x2, x22
   44adc:	mov	w3, w28
   44ae0:	bl	c180 <__gmpn_lshift@plt>
   44ae4:	ldr	x8, [x20, x23]
   44ae8:	mov	x1, x20
   44aec:	mov	x2, x19
   44af0:	mov	x3, x22
   44af4:	add	x8, x8, x0
   44af8:	mov	x0, x20
   44afc:	str	x8, [x20, x23]
   44b00:	bl	ca70 <__gmpn_add_n@plt>
   44b04:	ldr	x8, [x20, x23]
   44b08:	add	w25, w25, #0x2
   44b0c:	cmp	w25, w24
   44b10:	add	w28, w28, w27
   44b14:	add	x8, x8, x0
   44b18:	str	x8, [x20, x23]
   44b1c:	b.cc	44ac8 <__gmpn_toom_eval_pm2exp@@Base+0x94>  // b.lo, b.ul, b.last
   44b20:	ldur	w3, [x29, #-4]
   44b24:	add	x1, x26, x23
   44b28:	mov	x0, x19
   44b2c:	mov	x2, x22
   44b30:	bl	c180 <__gmpn_lshift@plt>
   44b34:	cmp	w24, #0x4
   44b38:	str	x0, [x19, x23]
   44b3c:	b.cc	44ba4 <__gmpn_toom_eval_pm2exp@@Base+0x170>  // b.lo, b.ul, b.last
   44b40:	ldur	w8, [x29, #-4]
   44b44:	mov	w25, #0x3                   	// #3
   44b48:	add	w28, w27, w8
   44b4c:	mov	w8, w25
   44b50:	mul	x8, x8, x22
   44b54:	add	x1, x26, x8, lsl #3
   44b58:	mov	x0, x21
   44b5c:	mov	x2, x22
   44b60:	mov	w3, w28
   44b64:	bl	c180 <__gmpn_lshift@plt>
   44b68:	ldr	x8, [x19, x23]
   44b6c:	mov	x1, x19
   44b70:	mov	x2, x21
   44b74:	mov	x3, x22
   44b78:	add	x8, x8, x0
   44b7c:	mov	x0, x19
   44b80:	str	x8, [x19, x23]
   44b84:	bl	ca70 <__gmpn_add_n@plt>
   44b88:	ldr	x8, [x19, x23]
   44b8c:	add	w25, w25, #0x2
   44b90:	cmp	w25, w24
   44b94:	add	w28, w28, w27
   44b98:	add	x8, x8, x0
   44b9c:	str	x8, [x19, x23]
   44ba0:	b.cc	44b4c <__gmpn_toom_eval_pm2exp@@Base+0x118>  // b.lo, b.ul, b.last
   44ba4:	mov	w8, w24
   44ba8:	mul	x8, x8, x22
   44bac:	add	x1, x26, x8, lsl #3
   44bb0:	ldur	w8, [x29, #-4]
   44bb4:	ldr	x23, [sp]
   44bb8:	mov	x0, x21
   44bbc:	mul	w3, w8, w24
   44bc0:	mov	x2, x23
   44bc4:	bl	c180 <__gmpn_lshift@plt>
   44bc8:	str	x0, [x21, x23, lsl #3]
   44bcc:	add	x25, x22, #0x1
   44bd0:	add	x23, x23, #0x1
   44bd4:	tbnz	w24, #0, 44c18 <__gmpn_toom_eval_pm2exp@@Base+0x1e4>
   44bd8:	cbz	x23, 44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44bdc:	mov	x0, x20
   44be0:	mov	x1, x20
   44be4:	mov	x2, x21
   44be8:	mov	x3, x23
   44bec:	bl	ca70 <__gmpn_add_n@plt>
   44bf0:	cbz	x0, 44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44bf4:	cmp	x23, x22
   44bf8:	b.gt	44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44bfc:	lsl	x8, x23, #3
   44c00:	ldr	x9, [x20, x8]
   44c04:	add	x23, x23, #0x1
   44c08:	adds	x9, x9, #0x1
   44c0c:	str	x9, [x20, x8]
   44c10:	b.cs	44bf4 <__gmpn_toom_eval_pm2exp@@Base+0x1c0>  // b.hs, b.nlast
   44c14:	b	44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44c18:	cbz	x23, 44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44c1c:	mov	x0, x19
   44c20:	mov	x1, x19
   44c24:	mov	x2, x21
   44c28:	mov	x3, x23
   44c2c:	bl	ca70 <__gmpn_add_n@plt>
   44c30:	cbz	x0, 44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44c34:	cmp	x23, x22
   44c38:	b.gt	44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44c3c:	lsl	x8, x23, #3
   44c40:	ldr	x9, [x19, x8]
   44c44:	add	x23, x23, #0x1
   44c48:	adds	x9, x9, #0x1
   44c4c:	str	x9, [x19, x8]
   44c50:	b.cs	44c34 <__gmpn_toom_eval_pm2exp@@Base+0x200>  // b.hs, b.nlast
   44c54:	b	44c70 <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   44c58:	lsl	x8, x22, #3
   44c5c:	ldr	x9, [x20, x8]
   44c60:	ldr	x8, [x19, x8]
   44c64:	sub	x22, x22, #0x1
   44c68:	cmp	x9, x8
   44c6c:	b.ne	44c80 <__gmpn_toom_eval_pm2exp@@Base+0x24c>  // b.any
   44c70:	add	x8, x22, #0x1
   44c74:	cmp	x8, #0x1
   44c78:	b.ge	44c58 <__gmpn_toom_eval_pm2exp@@Base+0x224>  // b.tcont
   44c7c:	b	44c84 <__gmpn_toom_eval_pm2exp@@Base+0x250>
   44c80:	b.ls	44ca0 <__gmpn_toom_eval_pm2exp@@Base+0x26c>  // b.plast
   44c84:	mov	x0, x21
   44c88:	mov	x1, x20
   44c8c:	mov	x2, x19
   44c90:	mov	x3, x25
   44c94:	bl	c2d0 <__gmpn_sub_n@plt>
   44c98:	mov	w21, wzr
   44c9c:	b	44cb8 <__gmpn_toom_eval_pm2exp@@Base+0x284>
   44ca0:	mov	x0, x21
   44ca4:	mov	x1, x19
   44ca8:	mov	x2, x20
   44cac:	mov	x3, x25
   44cb0:	bl	c2d0 <__gmpn_sub_n@plt>
   44cb4:	mov	w21, #0xffffffff            	// #-1
   44cb8:	mov	x0, x20
   44cbc:	mov	x1, x20
   44cc0:	mov	x2, x19
   44cc4:	mov	x3, x25
   44cc8:	bl	ca70 <__gmpn_add_n@plt>
   44ccc:	mov	w0, w21
   44cd0:	ldp	x20, x19, [sp, #96]
   44cd4:	ldp	x22, x21, [sp, #80]
   44cd8:	ldp	x24, x23, [sp, #64]
   44cdc:	ldp	x26, x25, [sp, #48]
   44ce0:	ldp	x28, x27, [sp, #32]
   44ce4:	ldp	x29, x30, [sp, #16]
   44ce8:	add	sp, sp, #0x70
   44cec:	ret

0000000000044cf0 <__gmpn_toom_eval_pm2rexp@@Base>:
   44cf0:	sub	sp, sp, #0x80
   44cf4:	stp	x24, x23, [sp, #80]
   44cf8:	mov	x23, x3
   44cfc:	stp	x22, x21, [sp, #96]
   44d00:	mov	w24, w2
   44d04:	mov	x21, x1
   44d08:	mul	w3, w6, w2
   44d0c:	mov	x1, x23
   44d10:	mov	x2, x4
   44d14:	stp	x29, x30, [sp, #32]
   44d18:	stp	x28, x27, [sp, #48]
   44d1c:	stp	x26, x25, [sp, #64]
   44d20:	stp	x20, x19, [sp, #112]
   44d24:	add	x29, sp, #0x20
   44d28:	mov	x19, x7
   44d2c:	mov	w28, w6
   44d30:	mov	x26, x5
   44d34:	mov	x22, x4
   44d38:	mov	x20, x0
   44d3c:	bl	c180 <__gmpn_lshift@plt>
   44d40:	lsl	x27, x22, #3
   44d44:	sub	w25, w24, #0x1
   44d48:	str	x0, [x20, x27]
   44d4c:	add	x1, x23, x27
   44d50:	mul	w3, w25, w28
   44d54:	mov	x0, x19
   44d58:	mov	x2, x22
   44d5c:	stur	w28, [x29, #-12]
   44d60:	bl	c180 <__gmpn_lshift@plt>
   44d64:	mov	w8, w24
   44d68:	mul	x8, x8, x22
   44d6c:	add	x2, x23, x8, lsl #3
   44d70:	str	x0, [x19, x27]
   44d74:	tbnz	w24, #0, 44db8 <__gmpn_toom_eval_pm2rexp@@Base+0xc8>
   44d78:	mov	x28, x23
   44d7c:	cbz	x26, 44e38 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   44d80:	mov	x0, x20
   44d84:	mov	x1, x20
   44d88:	mov	x3, x26
   44d8c:	bl	ca70 <__gmpn_add_n@plt>
   44d90:	cbz	x0, 44e38 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   44d94:	cmp	x26, x22
   44d98:	b.gt	44e38 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   44d9c:	lsl	x8, x26, #3
   44da0:	ldr	x9, [x20, x8]
   44da4:	add	x26, x26, #0x1
   44da8:	adds	x9, x9, #0x1
   44dac:	str	x9, [x20, x8]
   44db0:	b.cs	44d94 <__gmpn_toom_eval_pm2rexp@@Base+0xa4>  // b.hs, b.nlast
   44db4:	b	44e38 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   44db8:	cbz	x26, 44df0 <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   44dbc:	mov	x0, x19
   44dc0:	mov	x1, x19
   44dc4:	mov	x3, x26
   44dc8:	bl	ca70 <__gmpn_add_n@plt>
   44dcc:	cbz	x0, 44df0 <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   44dd0:	cmp	x26, x22
   44dd4:	b.gt	44df0 <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   44dd8:	lsl	x8, x26, #3
   44ddc:	ldr	x9, [x19, x8]
   44de0:	add	x26, x26, #0x1
   44de4:	adds	x9, x9, #0x1
   44de8:	str	x9, [x19, x8]
   44dec:	b.cs	44dd0 <__gmpn_toom_eval_pm2rexp@@Base+0xe0>  // b.hs, b.nlast
   44df0:	ldur	w3, [x29, #-12]
   44df4:	mov	w8, w25
   44df8:	mul	x8, x8, x22
   44dfc:	add	x1, x23, x8, lsl #3
   44e00:	mov	x0, x21
   44e04:	mov	x2, x22
   44e08:	mov	x28, x23
   44e0c:	bl	c180 <__gmpn_lshift@plt>
   44e10:	mov	x26, x0
   44e14:	mov	x0, x20
   44e18:	mov	x1, x20
   44e1c:	mov	x2, x21
   44e20:	mov	x3, x22
   44e24:	bl	ca70 <__gmpn_add_n@plt>
   44e28:	ldr	x8, [x20, x27]
   44e2c:	add	x9, x0, x26
   44e30:	add	x8, x9, x8
   44e34:	str	x8, [x20, x27]
   44e38:	cmp	w25, #0x3
   44e3c:	add	x8, x22, #0x1
   44e40:	str	x8, [sp, #8]
   44e44:	b.cc	44f1c <__gmpn_toom_eval_pm2rexp@@Base+0x22c>  // b.lo, b.ul, b.last
   44e48:	ldur	w10, [x29, #-12]
   44e4c:	sub	w8, w24, #0x2
   44e50:	mov	w23, w25
   44e54:	mov	w26, wzr
   44e58:	lsl	w9, w10, #1
   44e5c:	stur	w9, [x29, #-4]
   44e60:	sub	w9, w24, #0x3
   44e64:	mul	w8, w10, w8
   44e68:	stur	w8, [x29, #-8]
   44e6c:	mul	w8, w10, w9
   44e70:	mov	w25, #0x2                   	// #2
   44e74:	stur	w8, [x29, #-12]
   44e78:	mov	w8, w25
   44e7c:	mul	x8, x8, x22
   44e80:	add	x1, x28, x8, lsl #3
   44e84:	ldur	w8, [x29, #-8]
   44e88:	mov	x0, x21
   44e8c:	mov	x2, x22
   44e90:	add	w3, w8, w26
   44e94:	bl	c180 <__gmpn_lshift@plt>
   44e98:	mov	x24, x0
   44e9c:	mov	x0, x20
   44ea0:	mov	x1, x20
   44ea4:	mov	x2, x21
   44ea8:	mov	x3, x22
   44eac:	bl	ca70 <__gmpn_add_n@plt>
   44eb0:	ldr	x9, [x20, x27]
   44eb4:	add	x8, x0, x24
   44eb8:	add	w10, w25, #0x1
   44ebc:	mul	x10, x10, x22
   44ec0:	add	x8, x8, x9
   44ec4:	str	x8, [x20, x27]
   44ec8:	ldur	w8, [x29, #-12]
   44ecc:	add	x1, x28, x10, lsl #3
   44ed0:	mov	x0, x21
   44ed4:	mov	x2, x22
   44ed8:	add	w3, w8, w26
   44edc:	bl	c180 <__gmpn_lshift@plt>
   44ee0:	mov	x24, x0
   44ee4:	mov	x0, x19
   44ee8:	mov	x1, x19
   44eec:	mov	x2, x21
   44ef0:	mov	x3, x22
   44ef4:	bl	ca70 <__gmpn_add_n@plt>
   44ef8:	ldr	x8, [x19, x27]
   44efc:	add	x9, x0, x24
   44f00:	add	w25, w25, #0x2
   44f04:	cmp	w25, w23
   44f08:	add	x8, x9, x8
   44f0c:	str	x8, [x19, x27]
   44f10:	ldur	w8, [x29, #-4]
   44f14:	sub	w26, w26, w8
   44f18:	b.cc	44e78 <__gmpn_toom_eval_pm2rexp@@Base+0x188>  // b.lo, b.ul, b.last
   44f1c:	add	x8, x22, #0x1
   44f20:	cmp	x8, #0x1
   44f24:	b.lt	44f44 <__gmpn_toom_eval_pm2rexp@@Base+0x254>  // b.tstop
   44f28:	lsl	x8, x22, #3
   44f2c:	ldr	x9, [x20, x8]
   44f30:	ldr	x8, [x19, x8]
   44f34:	sub	x22, x22, #0x1
   44f38:	cmp	x9, x8
   44f3c:	b.eq	44f1c <__gmpn_toom_eval_pm2rexp@@Base+0x22c>  // b.none
   44f40:	b.ls	44f64 <__gmpn_toom_eval_pm2rexp@@Base+0x274>  // b.plast
   44f44:	ldr	x22, [sp, #8]
   44f48:	mov	x0, x21
   44f4c:	mov	x1, x20
   44f50:	mov	x2, x19
   44f54:	mov	x3, x22
   44f58:	bl	c2d0 <__gmpn_sub_n@plt>
   44f5c:	mov	w21, wzr
   44f60:	b	44f80 <__gmpn_toom_eval_pm2rexp@@Base+0x290>
   44f64:	ldr	x22, [sp, #8]
   44f68:	mov	x0, x21
   44f6c:	mov	x1, x19
   44f70:	mov	x2, x20
   44f74:	mov	x3, x22
   44f78:	bl	c2d0 <__gmpn_sub_n@plt>
   44f7c:	mov	w21, #0xffffffff            	// #-1
   44f80:	mov	x0, x20
   44f84:	mov	x1, x20
   44f88:	mov	x2, x19
   44f8c:	mov	x3, x22
   44f90:	bl	ca70 <__gmpn_add_n@plt>
   44f94:	mov	w0, w21
   44f98:	ldp	x20, x19, [sp, #112]
   44f9c:	ldp	x22, x21, [sp, #96]
   44fa0:	ldp	x24, x23, [sp, #80]
   44fa4:	ldp	x26, x25, [sp, #64]
   44fa8:	ldp	x28, x27, [sp, #48]
   44fac:	ldp	x29, x30, [sp, #32]
   44fb0:	add	sp, sp, #0x80
   44fb4:	ret

0000000000044fb8 <__gmpn_toom_interpolate_5pts@@Base>:
   44fb8:	sub	sp, sp, #0x80
   44fbc:	stp	x20, x19, [sp, #112]
   44fc0:	lsl	x20, x3, #3
   44fc4:	stp	x24, x23, [sp, #80]
   44fc8:	add	x23, x0, x20
   44fcc:	stp	x28, x27, [sp, #48]
   44fd0:	stp	x26, x25, [sp, #64]
   44fd4:	mov	w27, #0x1                   	// #1
   44fd8:	add	x25, x23, x20
   44fdc:	stp	x29, x30, [sp, #32]
   44fe0:	stp	x22, x21, [sp, #96]
   44fe4:	add	x29, sp, #0x20
   44fe8:	bfi	x27, x3, #1, #63
   44fec:	add	x22, x25, x20
   44ff0:	mov	x24, x4
   44ff4:	mov	x19, x3
   44ff8:	mov	x28, x2
   44ffc:	mov	x21, x1
   45000:	lsl	x8, x3, #1
   45004:	stur	x0, [x29, #-8]
   45008:	add	x26, x22, x20
   4500c:	mov	x0, x1
   45010:	mov	x3, x27
   45014:	stp	x8, x6, [sp, #8]
   45018:	cbz	w5, 45050 <__gmpn_toom_interpolate_5pts@@Base+0x98>
   4501c:	bl	ca70 <__gmpn_add_n@plt>
   45020:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   45024:	mov	x0, x21
   45028:	mov	x1, x21
   4502c:	mov	x2, x27
   45030:	mov	x4, xzr
   45034:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   45038:	mov	x0, x28
   4503c:	mov	x1, x25
   45040:	mov	x2, x28
   45044:	mov	x3, x27
   45048:	bl	c950 <__gmpn_rsh1add_n@plt>
   4504c:	b	45080 <__gmpn_toom_interpolate_5pts@@Base+0xc8>
   45050:	bl	c2d0 <__gmpn_sub_n@plt>
   45054:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   45058:	mov	x0, x21
   4505c:	mov	x1, x21
   45060:	mov	x2, x27
   45064:	mov	x4, xzr
   45068:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   4506c:	mov	x0, x28
   45070:	mov	x1, x25
   45074:	mov	x2, x28
   45078:	mov	x3, x27
   4507c:	bl	c840 <__gmpn_rsh1sub_n@plt>
   45080:	ldur	x2, [x29, #-8]
   45084:	ldr	x3, [sp, #8]
   45088:	mov	x0, x25
   4508c:	mov	x1, x25
   45090:	bl	c2d0 <__gmpn_sub_n@plt>
   45094:	ldr	x8, [x26]
   45098:	mov	x1, x21
   4509c:	mov	x2, x25
   450a0:	mov	x3, x27
   450a4:	sub	x8, x8, x0
   450a8:	mov	x0, x21
   450ac:	str	x8, [x26]
   450b0:	bl	c840 <__gmpn_rsh1sub_n@plt>
   450b4:	mov	x0, x25
   450b8:	mov	x1, x25
   450bc:	mov	x2, x28
   450c0:	mov	x3, x27
   450c4:	bl	c2d0 <__gmpn_sub_n@plt>
   450c8:	mov	x0, x23
   450cc:	mov	x1, x23
   450d0:	mov	x2, x28
   450d4:	mov	x3, x27
   450d8:	bl	ca70 <__gmpn_add_n@plt>
   450dc:	ldr	x8, [x22, #8]
   450e0:	adds	x8, x8, x0
   450e4:	str	x8, [x22, #8]
   450e8:	b.cc	4510c <__gmpn_toom_interpolate_5pts@@Base+0x154>  // b.lo, b.ul, b.last
   450ec:	ldur	x9, [x29, #-8]
   450f0:	mov	w8, #0x18                  	// #24
   450f4:	madd	x8, x19, x8, x9
   450f8:	add	x8, x8, #0x10
   450fc:	ldr	x9, [x8]
   45100:	adds	x9, x9, #0x1
   45104:	str	x9, [x8], #8
   45108:	b.cs	450fc <__gmpn_toom_interpolate_5pts@@Base+0x144>  // b.hs, b.nlast
   4510c:	ldr	x8, [x26]
   45110:	mov	x0, x21
   45114:	mov	x1, x21
   45118:	mov	x2, x26
   4511c:	str	x8, [sp, #8]
   45120:	ldr	x8, [sp, #16]
   45124:	mov	x3, x24
   45128:	str	x8, [x26]
   4512c:	bl	c440 <__gmpn_sublsh1_n@plt>
   45130:	lsl	x28, x24, #3
   45134:	ldr	x8, [x21, x28]
   45138:	subs	x8, x8, x0
   4513c:	str	x8, [x21, x28]
   45140:	b.cs	4515c <__gmpn_toom_interpolate_5pts@@Base+0x1a4>  // b.hs, b.nlast
   45144:	add	x8, x21, x24, lsl #3
   45148:	add	x8, x8, #0x8
   4514c:	ldr	x9, [x8]
   45150:	sub	x10, x9, #0x1
   45154:	str	x10, [x8], #8
   45158:	cbz	x9, 4514c <__gmpn_toom_interpolate_5pts@@Base+0x194>
   4515c:	add	x3, x19, #0x1
   45160:	cmp	x3, x24
   45164:	add	x2, x21, x19, lsl #3
   45168:	mov	x0, x26
   4516c:	mov	x1, x26
   45170:	b.ge	452a0 <__gmpn_toom_interpolate_5pts@@Base+0x2e8>  // b.tcont
   45174:	bl	ca70 <__gmpn_add_n@plt>
   45178:	lsl	x8, x27, #3
   4517c:	ldr	x9, [x22, x8]
   45180:	adds	x9, x9, x0
   45184:	str	x9, [x22, x8]
   45188:	b.cc	451ac <__gmpn_toom_interpolate_5pts@@Base+0x1f4>  // b.lo, b.ul, b.last
   4518c:	ldur	x9, [x29, #-8]
   45190:	mov	w8, #0x28                  	// #40
   45194:	madd	x8, x19, x8, x9
   45198:	add	x8, x8, #0x10
   4519c:	ldr	x9, [x8]
   451a0:	adds	x9, x9, #0x1
   451a4:	str	x9, [x8], #8
   451a8:	b.cs	4519c <__gmpn_toom_interpolate_5pts@@Base+0x1e4>  // b.hs, b.nlast
   451ac:	mov	x0, x25
   451b0:	mov	x1, x25
   451b4:	mov	x2, x26
   451b8:	mov	x3, x24
   451bc:	bl	c2d0 <__gmpn_sub_n@plt>
   451c0:	ldr	x8, [sp, #8]
   451c4:	ldr	x27, [x26]
   451c8:	str	x8, [x26]
   451cc:	ldr	x8, [x25, x28]
   451d0:	subs	x8, x8, x0
   451d4:	str	x8, [x25, x28]
   451d8:	b.cs	451fc <__gmpn_toom_interpolate_5pts@@Base+0x244>  // b.hs, b.nlast
   451dc:	ldur	x9, [x29, #-8]
   451e0:	add	x8, x24, x19, lsl #1
   451e4:	add	x8, x9, x8, lsl #3
   451e8:	add	x8, x8, #0x8
   451ec:	ldr	x9, [x8]
   451f0:	sub	x10, x9, #0x1
   451f4:	str	x10, [x8], #8
   451f8:	cbz	x9, 451ec <__gmpn_toom_interpolate_5pts@@Base+0x234>
   451fc:	mov	x0, x23
   45200:	mov	x1, x23
   45204:	mov	x2, x21
   45208:	mov	x3, x19
   4520c:	bl	c2d0 <__gmpn_sub_n@plt>
   45210:	ldr	x8, [x23, x20]
   45214:	subs	x8, x8, x0
   45218:	str	x8, [x23, x20]
   4521c:	b.cs	4523c <__gmpn_toom_interpolate_5pts@@Base+0x284>  // b.hs, b.nlast
   45220:	ldur	x8, [x29, #-8]
   45224:	add	x8, x8, x19, lsl #4
   45228:	add	x8, x8, #0x8
   4522c:	ldr	x9, [x8]
   45230:	sub	x10, x9, #0x1
   45234:	str	x10, [x8], #8
   45238:	cbz	x9, 4522c <__gmpn_toom_interpolate_5pts@@Base+0x274>
   4523c:	mov	x0, x22
   45240:	mov	x1, x22
   45244:	mov	x2, x21
   45248:	mov	x3, x19
   4524c:	bl	ca70 <__gmpn_add_n@plt>
   45250:	ldr	x8, [x22, x20]
   45254:	add	x8, x8, x0
   45258:	adds	x8, x8, x27
   4525c:	str	x8, [x22, x20]
   45260:	b.cc	45280 <__gmpn_toom_interpolate_5pts@@Base+0x2c8>  // b.lo, b.ul, b.last
   45264:	ldur	x8, [x29, #-8]
   45268:	add	x8, x8, x19, lsl #5
   4526c:	add	x8, x8, #0x8
   45270:	ldr	x9, [x8]
   45274:	adds	x9, x9, #0x1
   45278:	str	x9, [x8], #8
   4527c:	b.cs	45270 <__gmpn_toom_interpolate_5pts@@Base+0x2b8>  // b.hs, b.nlast
   45280:	ldp	x20, x19, [sp, #112]
   45284:	ldp	x22, x21, [sp, #96]
   45288:	ldp	x24, x23, [sp, #80]
   4528c:	ldp	x26, x25, [sp, #64]
   45290:	ldp	x28, x27, [sp, #48]
   45294:	ldp	x29, x30, [sp, #32]
   45298:	add	sp, sp, #0x80
   4529c:	ret
   452a0:	mov	x3, x24
   452a4:	bl	ca70 <__gmpn_add_n@plt>
   452a8:	b	451ac <__gmpn_toom_interpolate_5pts@@Base+0x1f4>

00000000000452ac <__gmpn_toom_interpolate_6pts@@Base>:
   452ac:	sub	sp, sp, #0x90
   452b0:	stp	x26, x25, [sp, #80]
   452b4:	mov	w25, #0x1                   	// #1
   452b8:	stp	x29, x30, [sp, #48]
   452bc:	stp	x28, x27, [sp, #64]
   452c0:	stp	x24, x23, [sp, #96]
   452c4:	stp	x22, x21, [sp, #112]
   452c8:	stp	x20, x19, [sp, #128]
   452cc:	add	x29, sp, #0x30
   452d0:	mov	x23, x5
   452d4:	mov	x27, x4
   452d8:	mov	x22, x3
   452dc:	mov	w24, w2
   452e0:	mov	x20, x1
   452e4:	mov	x19, x0
   452e8:	lsl	x28, x1, #1
   452ec:	bfi	x25, x1, #1, #63
   452f0:	mov	x0, x4
   452f4:	mov	x1, x5
   452f8:	stur	x6, [x29, #-8]
   452fc:	tbnz	w2, #1, 45310 <__gmpn_toom_interpolate_6pts@@Base+0x64>
   45300:	mov	x2, x27
   45304:	mov	x3, x25
   45308:	bl	c2d0 <__gmpn_sub_n@plt>
   4530c:	b	4531c <__gmpn_toom_interpolate_6pts@@Base+0x70>
   45310:	mov	x2, x27
   45314:	mov	x3, x25
   45318:	bl	ca70 <__gmpn_add_n@plt>
   4531c:	mov	w3, #0x2                   	// #2
   45320:	mov	x0, x27
   45324:	mov	x1, x27
   45328:	mov	x2, x25
   4532c:	bl	c1a0 <__gmpn_rshift@plt>
   45330:	mov	x0, x23
   45334:	mov	x1, x23
   45338:	mov	x2, x19
   4533c:	mov	x3, x28
   45340:	bl	c2d0 <__gmpn_sub_n@plt>
   45344:	lsl	x21, x28, #3
   45348:	ldr	x8, [x23, x21]
   4534c:	mov	w3, #0x1                   	// #1
   45350:	mov	x1, x23
   45354:	mov	x2, x25
   45358:	sub	x8, x8, x0
   4535c:	mov	x0, x23
   45360:	str	x8, [x23, x21]
   45364:	bl	c1a0 <__gmpn_rshift@plt>
   45368:	mov	x0, x23
   4536c:	mov	x1, x23
   45370:	mov	x2, x27
   45374:	mov	x3, x25
   45378:	bl	c840 <__gmpn_rsh1sub_n@plt>
   4537c:	add	x26, x19, x21
   45380:	mov	x0, x22
   45384:	mov	x1, x26
   45388:	mov	x2, x22
   4538c:	mov	x3, x25
   45390:	tbnz	w24, #0, 4539c <__gmpn_toom_interpolate_6pts@@Base+0xf0>
   45394:	bl	c840 <__gmpn_rsh1sub_n@plt>
   45398:	b	453a0 <__gmpn_toom_interpolate_6pts@@Base+0xf4>
   4539c:	bl	c950 <__gmpn_rsh1add_n@plt>
   453a0:	mov	x0, x27
   453a4:	mov	x1, x27
   453a8:	mov	x2, x22
   453ac:	mov	x3, x25
   453b0:	bl	c2d0 <__gmpn_sub_n@plt>
   453b4:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   453b8:	mov	x0, x27
   453bc:	mov	x1, x27
   453c0:	mov	x2, x25
   453c4:	mov	x4, xzr
   453c8:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   453cc:	mov	x0, x26
   453d0:	mov	x1, x26
   453d4:	mov	x2, x22
   453d8:	mov	x3, x25
   453dc:	bl	c2d0 <__gmpn_sub_n@plt>
   453e0:	mov	x0, x26
   453e4:	mov	x1, x26
   453e8:	mov	x2, x19
   453ec:	mov	x3, x28
   453f0:	str	x28, [sp, #8]
   453f4:	bl	c2d0 <__gmpn_sub_n@plt>
   453f8:	ldr	x8, [x26, x21]
   453fc:	mov	x1, x23
   45400:	mov	x2, x26
   45404:	mov	x3, x25
   45408:	sub	x8, x8, x0
   4540c:	mov	x0, x23
   45410:	str	x8, [x26, x21]
   45414:	bl	c2d0 <__gmpn_sub_n@plt>
   45418:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   4541c:	mov	x0, x23
   45420:	mov	x1, x23
   45424:	mov	x2, x25
   45428:	mov	x4, xzr
   4542c:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   45430:	add	x28, x19, x20, lsl #3
   45434:	mov	x0, x28
   45438:	mov	x1, x28
   4543c:	mov	x2, x22
   45440:	mov	x3, x25
   45444:	bl	ca70 <__gmpn_add_n@plt>
   45448:	mov	w8, #0x18                  	// #24
   4544c:	madd	x25, x20, x8, x19
   45450:	ldr	x8, [x25, #8]
   45454:	adds	x8, x8, x0
   45458:	str	x8, [x25, #8]
   4545c:	b.cc	4547c <__gmpn_toom_interpolate_6pts@@Base+0x1d0>  // b.lo, b.ul, b.last
   45460:	mov	w8, #0x18                  	// #24
   45464:	madd	x8, x20, x8, x19
   45468:	add	x8, x8, #0x10
   4546c:	ldr	x9, [x8]
   45470:	adds	x9, x9, #0x1
   45474:	str	x9, [x8], #8
   45478:	b.cs	4546c <__gmpn_toom_interpolate_6pts@@Base+0x1c0>  // b.hs, b.nlast
   4547c:	ldur	x24, [x29, #-8]
   45480:	mov	w8, #0x28                  	// #40
   45484:	madd	x22, x20, x8, x19
   45488:	mov	x0, x27
   4548c:	mov	x1, x27
   45490:	mov	x2, x22
   45494:	mov	x3, x24
   45498:	bl	c160 <__gmpn_sublsh2_n@plt>
   4549c:	lsl	x9, x24, #3
   454a0:	ldr	x8, [x27, x9]
   454a4:	stur	x9, [x29, #-16]
   454a8:	subs	x8, x8, x0
   454ac:	str	x8, [x27, x9]
   454b0:	b.cs	454d0 <__gmpn_toom_interpolate_6pts@@Base+0x224>  // b.hs, b.nlast
   454b4:	ldur	x8, [x29, #-8]
   454b8:	add	x8, x27, x8, lsl #3
   454bc:	add	x8, x8, #0x8
   454c0:	ldr	x9, [x8]
   454c4:	sub	x10, x9, #0x1
   454c8:	str	x10, [x8], #8
   454cc:	cbz	x9, 454c0 <__gmpn_toom_interpolate_6pts@@Base+0x214>
   454d0:	mov	x0, x28
   454d4:	mov	x1, x28
   454d8:	mov	x2, x27
   454dc:	mov	x3, x20
   454e0:	bl	c2d0 <__gmpn_sub_n@plt>
   454e4:	ldr	x8, [x26]
   454e8:	subs	x8, x8, x0
   454ec:	str	x8, [x26]
   454f0:	b.cs	4550c <__gmpn_toom_interpolate_6pts@@Base+0x260>  // b.hs, b.nlast
   454f4:	add	x8, x19, x20, lsl #4
   454f8:	add	x8, x8, #0x8
   454fc:	ldr	x9, [x8]
   45500:	sub	x10, x9, #0x1
   45504:	str	x10, [x8], #8
   45508:	cbz	x9, 454fc <__gmpn_toom_interpolate_6pts@@Base+0x250>
   4550c:	ldr	x8, [x26, x21]
   45510:	mov	x0, x25
   45514:	mov	x1, x25
   45518:	mov	x2, x27
   4551c:	mov	x3, x20
   45520:	str	x8, [sp, #24]
   45524:	bl	ca70 <__gmpn_add_n@plt>
   45528:	ldr	x24, [x27, x21]
   4552c:	add	x28, x19, x20, lsl #5
   45530:	lsl	x21, x20, #3
   45534:	str	x0, [sp, #16]
   45538:	add	x2, x27, x21
   4553c:	mov	x0, x28
   45540:	mov	x1, x23
   45544:	mov	x3, x20
   45548:	bl	ca70 <__gmpn_add_n@plt>
   4554c:	add	x2, x23, x21
   45550:	ldr	x8, [x2]
   45554:	add	x9, x0, x24
   45558:	adds	x8, x8, x9
   4555c:	str	x8, [x2]
   45560:	b.cc	4557c <__gmpn_toom_interpolate_6pts@@Base+0x2d0>  // b.lo, b.ul, b.last
   45564:	add	x8, x23, x20, lsl #3
   45568:	add	x8, x8, #0x8
   4556c:	ldr	x9, [x8]
   45570:	adds	x9, x9, #0x1
   45574:	str	x9, [x8], #8
   45578:	b.cs	4556c <__gmpn_toom_interpolate_6pts@@Base+0x2c0>  // b.hs, b.nlast
   4557c:	ldur	x27, [x29, #-8]
   45580:	cmp	x27, x20
   45584:	b.le	45730 <__gmpn_toom_interpolate_6pts@@Base+0x484>
   45588:	ldr	x8, [sp, #8]
   4558c:	mov	x0, x22
   45590:	mov	x1, x22
   45594:	mov	x3, x20
   45598:	ldr	x23, [x23, x8, lsl #3]
   4559c:	bl	ca70 <__gmpn_add_n@plt>
   455a0:	add	x23, x0, x23
   455a4:	ldp	x9, x8, [sp, #16]
   455a8:	add	x3, x27, x20
   455ac:	mov	x0, x26
   455b0:	mov	x1, x26
   455b4:	mov	x2, x28
   455b8:	add	x24, x9, x8
   455bc:	bl	c2d0 <__gmpn_sub_n@plt>
   455c0:	sub	x8, x27, #0x1
   455c4:	lsl	x8, x8, #3
   455c8:	ldr	x9, [x22, x8]
   455cc:	mov	w10, #0x1                   	// #1
   455d0:	cmp	x27, x20
   455d4:	str	x10, [x22, x8]
   455d8:	sub	x9, x9, #0x1
   455dc:	b.le	45614 <__gmpn_toom_interpolate_6pts@@Base+0x368>
   455e0:	subs	x10, x24, x23
   455e4:	b.ls	45678 <__gmpn_toom_interpolate_6pts@@Base+0x3cc>  // b.plast
   455e8:	ldr	x11, [x28]
   455ec:	adds	x10, x11, x10
   455f0:	str	x10, [x28]
   455f4:	b.cc	456a4 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.lo, b.ul, b.last
   455f8:	add	x10, x19, x20, lsl #5
   455fc:	add	x10, x10, #0x8
   45600:	ldr	x11, [x10]
   45604:	adds	x11, x11, #0x1
   45608:	str	x11, [x10], #8
   4560c:	b.cs	45600 <__gmpn_toom_interpolate_6pts@@Base+0x354>  // b.hs, b.nlast
   45610:	b	456a4 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>
   45614:	ldr	x10, [x28]
   45618:	adds	x10, x10, x24
   4561c:	str	x10, [x28]
   45620:	b.cc	4563c <__gmpn_toom_interpolate_6pts@@Base+0x390>  // b.lo, b.ul, b.last
   45624:	add	x10, x19, x20, lsl #5
   45628:	add	x10, x10, #0x8
   4562c:	ldr	x11, [x10]
   45630:	adds	x11, x11, #0x1
   45634:	str	x11, [x10], #8
   45638:	b.cs	4562c <__gmpn_toom_interpolate_6pts@@Base+0x380>  // b.hs, b.nlast
   4563c:	ldur	x12, [x29, #-16]
   45640:	add	x11, x0, x23
   45644:	ldr	x10, [x25, x12]
   45648:	subs	x10, x10, x11
   4564c:	str	x10, [x25, x12]
   45650:	b.cs	45704 <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.hs, b.nlast
   45654:	add	x10, x20, x20, lsl #1
   45658:	add	x10, x27, x10
   4565c:	add	x10, x19, x10, lsl #3
   45660:	add	x10, x10, #0x8
   45664:	ldr	x11, [x10]
   45668:	sub	x12, x11, #0x1
   4566c:	str	x12, [x10], #8
   45670:	cbz	x11, 45664 <__gmpn_toom_interpolate_6pts@@Base+0x3b8>
   45674:	b	45704 <__gmpn_toom_interpolate_6pts@@Base+0x458>
   45678:	ldr	x10, [x28]
   4567c:	sub	x11, x23, x24
   45680:	subs	x10, x10, x11
   45684:	str	x10, [x28]
   45688:	b.cs	456a4 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.hs, b.nlast
   4568c:	add	x10, x19, x20, lsl #5
   45690:	add	x10, x10, #0x8
   45694:	ldr	x11, [x10]
   45698:	sub	x12, x11, #0x1
   4569c:	str	x12, [x10], #8
   456a0:	cbz	x11, 45694 <__gmpn_toom_interpolate_6pts@@Base+0x3e8>
   456a4:	ldur	x11, [x29, #-16]
   456a8:	ldr	x10, [x25, x11]
   456ac:	subs	x10, x10, x0
   456b0:	str	x10, [x25, x11]
   456b4:	b.cs	456d8 <__gmpn_toom_interpolate_6pts@@Base+0x42c>  // b.hs, b.nlast
   456b8:	add	x10, x20, x20, lsl #1
   456bc:	add	x10, x27, x10
   456c0:	add	x10, x19, x10, lsl #3
   456c4:	add	x10, x10, #0x8
   456c8:	ldr	x11, [x10]
   456cc:	sub	x12, x11, #0x1
   456d0:	str	x12, [x10], #8
   456d4:	cbz	x11, 456c8 <__gmpn_toom_interpolate_6pts@@Base+0x41c>
   456d8:	ldr	x10, [x22, x21]
   456dc:	adds	x10, x10, x23
   456e0:	str	x10, [x22, x21]
   456e4:	b.cc	45704 <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.lo, b.ul, b.last
   456e8:	mov	w10, #0x30                  	// #48
   456ec:	madd	x10, x20, x10, x19
   456f0:	add	x10, x10, #0x8
   456f4:	ldr	x11, [x10]
   456f8:	adds	x11, x11, #0x1
   456fc:	str	x11, [x10], #8
   45700:	b.cs	456f4 <__gmpn_toom_interpolate_6pts@@Base+0x448>  // b.hs, b.nlast
   45704:	ldr	x10, [x22, x8]
   45708:	add	x9, x9, x10
   4570c:	str	x9, [x22, x8]
   45710:	ldp	x20, x19, [sp, #128]
   45714:	ldp	x22, x21, [sp, #112]
   45718:	ldp	x24, x23, [sp, #96]
   4571c:	ldp	x26, x25, [sp, #80]
   45720:	ldp	x28, x27, [sp, #64]
   45724:	ldp	x29, x30, [sp, #48]
   45728:	add	sp, sp, #0x90
   4572c:	ret
   45730:	mov	x0, x22
   45734:	mov	x1, x22
   45738:	mov	x3, x27
   4573c:	bl	ca70 <__gmpn_add_n@plt>
   45740:	mov	x23, x0
   45744:	b	455a4 <__gmpn_toom_interpolate_6pts@@Base+0x2f8>

0000000000045748 <__gmpn_toom_interpolate_7pts@@Base>:
   45748:	sub	sp, sp, #0x90
   4574c:	stp	x29, x30, [sp, #48]
   45750:	stp	x28, x27, [sp, #64]
   45754:	stp	x26, x25, [sp, #80]
   45758:	stp	x24, x23, [sp, #96]
   4575c:	stp	x22, x21, [sp, #112]
   45760:	stp	x20, x19, [sp, #128]
   45764:	add	x29, sp, #0x30
   45768:	ldr	x8, [x29, #96]
   4576c:	mov	w27, #0x1                   	// #1
   45770:	bfi	x27, x1, #1, #63
   45774:	mov	x26, x3
   45778:	mov	w21, w2
   4577c:	mov	x19, x1
   45780:	mov	x20, x0
   45784:	lsl	x25, x1, #1
   45788:	mov	x0, x6
   4578c:	mov	x1, x6
   45790:	mov	x2, x5
   45794:	mov	x3, x27
   45798:	mov	x23, x6
   4579c:	mov	x22, x5
   457a0:	mov	x24, x4
   457a4:	stp	x8, x7, [x29, #-16]
   457a8:	bl	ca70 <__gmpn_add_n@plt>
   457ac:	mov	x0, x26
   457b0:	str	w21, [sp, #12]
   457b4:	tbnz	w21, #0, 457d0 <__gmpn_toom_interpolate_7pts@@Base+0x88>
   457b8:	mov	x1, x22
   457bc:	mov	x2, x26
   457c0:	mov	x3, x27
   457c4:	bl	c840 <__gmpn_rsh1sub_n@plt>
   457c8:	cbnz	x19, 457e4 <__gmpn_toom_interpolate_7pts@@Base+0x9c>
   457cc:	b	45824 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   457d0:	mov	x1, x26
   457d4:	mov	x2, x22
   457d8:	mov	x3, x27
   457dc:	bl	c950 <__gmpn_rsh1add_n@plt>
   457e0:	cbz	x19, 45824 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   457e4:	mov	x0, x22
   457e8:	mov	x1, x22
   457ec:	mov	x2, x20
   457f0:	mov	x3, x25
   457f4:	bl	c2d0 <__gmpn_sub_n@plt>
   457f8:	cbz	x0, 45824 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   457fc:	add	x9, x22, x27, lsl #3
   45800:	mov	x8, xzr
   45804:	sub	x9, x9, #0x8
   45808:	cmp	x8, #0x8
   4580c:	b.eq	45824 <__gmpn_toom_interpolate_7pts@@Base+0xdc>  // b.none
   45810:	ldr	x10, [x9, x8]
   45814:	sub	x11, x10, #0x1
   45818:	str	x11, [x9, x8]
   4581c:	add	x8, x8, #0x8
   45820:	cbz	x10, 45808 <__gmpn_toom_interpolate_7pts@@Base+0xc0>
   45824:	mov	x0, x22
   45828:	mov	x1, x22
   4582c:	mov	x2, x26
   45830:	mov	x3, x27
   45834:	mov	x28, x20
   45838:	str	x25, [sp, #16]
   4583c:	bl	c2d0 <__gmpn_sub_n@plt>
   45840:	mov	w3, #0x2                   	// #2
   45844:	mov	x0, x22
   45848:	mov	x1, x22
   4584c:	mov	x2, x27
   45850:	bl	c1a0 <__gmpn_rshift@plt>
   45854:	mov	w8, #0x30                  	// #48
   45858:	madd	x1, x19, x8, x20
   4585c:	ldur	x21, [x29, #-16]
   45860:	str	x1, [sp, #24]
   45864:	ldur	x25, [x29, #-8]
   45868:	mov	w3, #0x4                   	// #4
   4586c:	mov	x0, x21
   45870:	mov	x2, x25
   45874:	bl	c180 <__gmpn_lshift@plt>
   45878:	adds	x28, x25, #0x1
   4587c:	str	x0, [x21, x25, lsl #3]
   45880:	b.cs	458bc <__gmpn_toom_interpolate_7pts@@Base+0x174>  // b.hs, b.nlast
   45884:	ldur	x2, [x29, #-16]
   45888:	mov	x0, x22
   4588c:	mov	x1, x22
   45890:	mov	x3, x28
   45894:	bl	c2d0 <__gmpn_sub_n@plt>
   45898:	cbz	x0, 458bc <__gmpn_toom_interpolate_7pts@@Base+0x174>
   4589c:	cmp	x28, x27
   458a0:	b.ge	458bc <__gmpn_toom_interpolate_7pts@@Base+0x174>  // b.tcont
   458a4:	lsl	x8, x28, #3
   458a8:	ldr	x9, [x22, x8]
   458ac:	add	x28, x28, #0x1
   458b0:	sub	x10, x9, #0x1
   458b4:	str	x10, [x22, x8]
   458b8:	cbz	x9, 4589c <__gmpn_toom_interpolate_7pts@@Base+0x154>
   458bc:	ldr	x25, [sp, #16]
   458c0:	ldr	w8, [sp, #12]
   458c4:	add	x28, x20, x25, lsl #3
   458c8:	tbnz	w8, #1, 458e4 <__gmpn_toom_interpolate_7pts@@Base+0x19c>
   458cc:	mov	x0, x24
   458d0:	mov	x1, x28
   458d4:	mov	x2, x24
   458d8:	mov	x3, x27
   458dc:	bl	c840 <__gmpn_rsh1sub_n@plt>
   458e0:	b	458f8 <__gmpn_toom_interpolate_7pts@@Base+0x1b0>
   458e4:	mov	x0, x24
   458e8:	mov	x1, x24
   458ec:	mov	x2, x28
   458f0:	mov	x3, x27
   458f4:	bl	c950 <__gmpn_rsh1add_n@plt>
   458f8:	ldur	x21, [x29, #-8]
   458fc:	mov	x0, x28
   45900:	mov	x1, x28
   45904:	mov	x2, x24
   45908:	mov	x3, x27
   4590c:	bl	c2d0 <__gmpn_sub_n@plt>
   45910:	mov	w3, #0x41                  	// #65
   45914:	mov	x0, x23
   45918:	mov	x1, x28
   4591c:	mov	x2, x27
   45920:	bl	c9e0 <__gmpn_submul_1@plt>
   45924:	cbz	x21, 45960 <__gmpn_toom_interpolate_7pts@@Base+0x218>
   45928:	ldr	x2, [sp, #24]
   4592c:	mov	x0, x28
   45930:	mov	x1, x28
   45934:	mov	x3, x21
   45938:	bl	c2d0 <__gmpn_sub_n@plt>
   4593c:	cbz	x0, 45960 <__gmpn_toom_interpolate_7pts@@Base+0x218>
   45940:	cmp	x21, x27
   45944:	b.ge	45960 <__gmpn_toom_interpolate_7pts@@Base+0x218>  // b.tcont
   45948:	lsl	x9, x21, #3
   4594c:	ldr	x10, [x28, x9]
   45950:	add	x21, x21, #0x1
   45954:	sub	x11, x10, #0x1
   45958:	str	x11, [x28, x9]
   4595c:	cbz	x10, 45940 <__gmpn_toom_interpolate_7pts@@Base+0x1f8>
   45960:	cbz	x19, 459a0 <__gmpn_toom_interpolate_7pts@@Base+0x258>
   45964:	mov	x0, x28
   45968:	mov	x1, x28
   4596c:	mov	x2, x20
   45970:	mov	x3, x25
   45974:	bl	c2d0 <__gmpn_sub_n@plt>
   45978:	cbz	x0, 459a0 <__gmpn_toom_interpolate_7pts@@Base+0x258>
   4597c:	mov	x8, xzr
   45980:	add	x9, x20, x19, lsl #5
   45984:	cmp	x8, #0x8
   45988:	b.eq	459a0 <__gmpn_toom_interpolate_7pts@@Base+0x258>  // b.none
   4598c:	ldr	x10, [x9, x8]
   45990:	sub	x11, x10, #0x1
   45994:	str	x11, [x9, x8]
   45998:	add	x8, x8, #0x8
   4599c:	cbz	x10, 45984 <__gmpn_toom_interpolate_7pts@@Base+0x23c>
   459a0:	mov	w3, #0x2d                  	// #45
   459a4:	mov	x0, x23
   459a8:	mov	x1, x28
   459ac:	mov	x2, x27
   459b0:	bl	d400 <__gmpn_addmul_1@plt>
   459b4:	mov	w3, #0x1                   	// #1
   459b8:	mov	x0, x23
   459bc:	mov	x1, x23
   459c0:	mov	x2, x27
   459c4:	bl	c1a0 <__gmpn_rshift@plt>
   459c8:	mov	x0, x22
   459cc:	mov	x1, x22
   459d0:	mov	x2, x28
   459d4:	mov	x3, x27
   459d8:	bl	c2d0 <__gmpn_sub_n@plt>
   459dc:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   459e0:	mov	x0, x22
   459e4:	mov	x1, x22
   459e8:	mov	x2, x27
   459ec:	mov	x4, xzr
   459f0:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   459f4:	mov	x0, x28
   459f8:	mov	x1, x28
   459fc:	mov	x2, x22
   45a00:	mov	x3, x27
   45a04:	bl	c2d0 <__gmpn_sub_n@plt>
   45a08:	mov	x0, x26
   45a0c:	mov	x1, x23
   45a10:	mov	x2, x26
   45a14:	mov	x3, x27
   45a18:	bl	c2d0 <__gmpn_sub_n@plt>
   45a1c:	ldur	x21, [x29, #-16]
   45a20:	mov	w3, #0x3                   	// #3
   45a24:	mov	x1, x24
   45a28:	mov	x2, x27
   45a2c:	mov	x0, x21
   45a30:	bl	c180 <__gmpn_lshift@plt>
   45a34:	mov	x0, x23
   45a38:	mov	x1, x23
   45a3c:	mov	x2, x21
   45a40:	mov	x3, x27
   45a44:	bl	c2d0 <__gmpn_sub_n@plt>
   45a48:	mov	x4, #0x8e39                	// #36409
   45a4c:	movk	x4, #0x38e3, lsl #16
   45a50:	movk	x4, #0xe38e, lsl #32
   45a54:	mov	w3, #0x9                   	// #9
   45a58:	movk	x4, #0x8e38, lsl #48
   45a5c:	mov	x0, x23
   45a60:	mov	x1, x23
   45a64:	mov	x2, x27
   45a68:	mov	w5, wzr
   45a6c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   45a70:	mov	x0, x24
   45a74:	mov	x1, x24
   45a78:	mov	x2, x23
   45a7c:	mov	x3, x27
   45a80:	bl	c2d0 <__gmpn_sub_n@plt>
   45a84:	mov	x3, #0x1111111111111111    	// #1229782938247303441
   45a88:	mov	x0, x26
   45a8c:	mov	x1, x26
   45a90:	mov	x2, x27
   45a94:	mov	x4, xzr
   45a98:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   45a9c:	mov	x0, x26
   45aa0:	mov	x1, x26
   45aa4:	mov	x2, x23
   45aa8:	mov	x3, x27
   45aac:	bl	c950 <__gmpn_rsh1add_n@plt>
   45ab0:	lsl	x21, x25, #3
   45ab4:	ldr	x8, [x26, x21]
   45ab8:	mov	x0, x23
   45abc:	mov	x1, x23
   45ac0:	mov	x2, x26
   45ac4:	and	x8, x8, #0x7fffffffffffffff
   45ac8:	mov	x3, x27
   45acc:	str	x8, [x26, x21]
   45ad0:	bl	c2d0 <__gmpn_sub_n@plt>
   45ad4:	lsl	x25, x19, #3
   45ad8:	add	x0, x20, x25
   45adc:	mov	x1, x0
   45ae0:	mov	x2, x26
   45ae4:	mov	x3, x27
   45ae8:	bl	ca70 <__gmpn_add_n@plt>
   45aec:	add	x8, x28, x25
   45af0:	ldr	x9, [x8, #8]
   45af4:	adds	x9, x9, x0
   45af8:	str	x9, [x8, #8]
   45afc:	b.cc	45b1c <__gmpn_toom_interpolate_7pts@@Base+0x3d4>  // b.lo, b.ul, b.last
   45b00:	mov	w8, #0x18                  	// #24
   45b04:	madd	x8, x19, x8, x20
   45b08:	add	x8, x8, #0x10
   45b0c:	ldr	x9, [x8]
   45b10:	adds	x9, x9, #0x1
   45b14:	str	x9, [x8], #8
   45b18:	b.cs	45b0c <__gmpn_toom_interpolate_7pts@@Base+0x3c4>  // b.hs, b.nlast
   45b1c:	mov	w8, #0x18                  	// #24
   45b20:	madd	x0, x19, x8, x20
   45b24:	mov	x1, x0
   45b28:	mov	x2, x24
   45b2c:	mov	x3, x19
   45b30:	bl	ca70 <__gmpn_add_n@plt>
   45b34:	add	x1, x24, x19, lsl #3
   45b38:	ldr	x8, [x28, x21]
   45b3c:	ldr	x9, [x1]
   45b40:	add	x8, x8, x0
   45b44:	add	x8, x9, x8
   45b48:	str	x8, [x1]
   45b4c:	ldr	x9, [x28, x21]
   45b50:	add	x9, x9, x0
   45b54:	cmp	x8, x9
   45b58:	b.cs	45b70 <__gmpn_toom_interpolate_7pts@@Base+0x428>  // b.hs, b.nlast
   45b5c:	add	x8, x1, #0x8
   45b60:	ldr	x9, [x8]
   45b64:	adds	x9, x9, #0x1
   45b68:	str	x9, [x8], #8
   45b6c:	b.cs	45b60 <__gmpn_toom_interpolate_7pts@@Base+0x418>  // b.hs, b.nlast
   45b70:	add	x0, x20, x19, lsl #5
   45b74:	mov	x2, x22
   45b78:	mov	x3, x19
   45b7c:	bl	ca70 <__gmpn_add_n@plt>
   45b80:	add	x1, x22, x19, lsl #3
   45b84:	ldr	x8, [x24, x21]
   45b88:	ldr	x9, [x1]
   45b8c:	add	x8, x8, x0
   45b90:	add	x8, x9, x8
   45b94:	str	x8, [x1]
   45b98:	ldr	x9, [x24, x21]
   45b9c:	add	x9, x9, x0
   45ba0:	cmp	x8, x9
   45ba4:	b.cs	45bbc <__gmpn_toom_interpolate_7pts@@Base+0x474>  // b.hs, b.nlast
   45ba8:	add	x8, x1, #0x8
   45bac:	ldr	x9, [x8]
   45bb0:	adds	x9, x9, #0x1
   45bb4:	str	x9, [x8], #8
   45bb8:	b.cs	45bac <__gmpn_toom_interpolate_7pts@@Base+0x464>  // b.hs, b.nlast
   45bbc:	mov	w8, #0x28                  	// #40
   45bc0:	madd	x0, x19, x8, x20
   45bc4:	mov	x2, x23
   45bc8:	mov	x3, x19
   45bcc:	bl	ca70 <__gmpn_add_n@plt>
   45bd0:	add	x2, x23, x19, lsl #3
   45bd4:	ldr	x8, [x22, x21]
   45bd8:	ldr	x9, [x2]
   45bdc:	add	x8, x8, x0
   45be0:	add	x8, x9, x8
   45be4:	str	x8, [x2]
   45be8:	ldr	x9, [x22, x21]
   45bec:	add	x9, x9, x0
   45bf0:	cmp	x8, x9
   45bf4:	b.cs	45c0c <__gmpn_toom_interpolate_7pts@@Base+0x4c4>  // b.hs, b.nlast
   45bf8:	add	x8, x2, #0x8
   45bfc:	ldr	x9, [x8]
   45c00:	adds	x9, x9, #0x1
   45c04:	str	x9, [x8], #8
   45c08:	b.cs	45bfc <__gmpn_toom_interpolate_7pts@@Base+0x4b4>  // b.hs, b.nlast
   45c0c:	ldur	x8, [x29, #-8]
   45c10:	add	x3, x19, #0x1
   45c14:	cmp	x3, x8
   45c18:	b.ge	45c7c <__gmpn_toom_interpolate_7pts@@Base+0x534>  // b.tcont
   45c1c:	ldr	x0, [sp, #24]
   45c20:	mov	x1, x0
   45c24:	bl	ca70 <__gmpn_add_n@plt>
   45c28:	mov	w8, #0x38                  	// #56
   45c2c:	madd	x8, x19, x8, x20
   45c30:	ldr	x9, [x8, #8]
   45c34:	adds	x9, x9, x0
   45c38:	str	x9, [x8, #8]
   45c3c:	b.cc	45c5c <__gmpn_toom_interpolate_7pts@@Base+0x514>  // b.lo, b.ul, b.last
   45c40:	mov	w8, #0x38                  	// #56
   45c44:	madd	x8, x19, x8, x20
   45c48:	add	x8, x8, #0x10
   45c4c:	ldr	x9, [x8]
   45c50:	adds	x9, x9, #0x1
   45c54:	str	x9, [x8], #8
   45c58:	b.cs	45c4c <__gmpn_toom_interpolate_7pts@@Base+0x504>  // b.hs, b.nlast
   45c5c:	ldp	x20, x19, [sp, #128]
   45c60:	ldp	x22, x21, [sp, #112]
   45c64:	ldp	x24, x23, [sp, #96]
   45c68:	ldp	x26, x25, [sp, #80]
   45c6c:	ldp	x28, x27, [sp, #64]
   45c70:	ldp	x29, x30, [sp, #48]
   45c74:	add	sp, sp, #0x90
   45c78:	ret
   45c7c:	ldr	x0, [sp, #24]
   45c80:	ldp	x20, x19, [sp, #128]
   45c84:	ldp	x22, x21, [sp, #112]
   45c88:	ldp	x24, x23, [sp, #96]
   45c8c:	ldp	x26, x25, [sp, #80]
   45c90:	ldp	x28, x27, [sp, #64]
   45c94:	ldp	x29, x30, [sp, #48]
   45c98:	mov	x1, x0
   45c9c:	mov	x3, x8
   45ca0:	add	sp, sp, #0x90
   45ca4:	b	ca70 <__gmpn_add_n@plt>

0000000000045ca8 <__gmpn_toom_interpolate_8pts@@Base>:
   45ca8:	sub	sp, sp, #0x90
   45cac:	stp	x29, x30, [sp, #48]
   45cb0:	stp	x28, x27, [sp, #64]
   45cb4:	stp	x26, x25, [sp, #80]
   45cb8:	stp	x24, x23, [sp, #96]
   45cbc:	stp	x22, x21, [sp, #112]
   45cc0:	stp	x20, x19, [sp, #128]
   45cc4:	add	x25, x2, x1, lsl #3
   45cc8:	ldr	x8, [x0]
   45ccc:	ldr	x9, [x25]
   45cd0:	mov	w10, #0x38                  	// #56
   45cd4:	mov	x23, x5
   45cd8:	mov	x26, x3
   45cdc:	sub	x8, x9, x8, lsr #4
   45ce0:	str	x8, [x25]
   45ce4:	ldr	x8, [x0]
   45ce8:	mov	x21, x2
   45cec:	mov	x19, x1
   45cf0:	add	x29, sp, #0x30
   45cf4:	cmp	x9, x8, lsr #4
   45cf8:	madd	x8, x1, x10, x0
   45cfc:	stp	x8, x4, [sp, #16]
   45d00:	b.cs	45d18 <__gmpn_toom_interpolate_8pts@@Base+0x70>  // b.hs, b.nlast
   45d04:	add	x8, x25, #0x8
   45d08:	ldr	x9, [x8]
   45d0c:	sub	x10, x9, #0x1
   45d10:	str	x10, [x8], #8
   45d14:	cbz	x9, 45d08 <__gmpn_toom_interpolate_8pts@@Base+0x60>
   45d18:	add	x8, x19, x19, lsl #1
   45d1c:	str	x8, [sp, #8]
   45d20:	lsl	x8, x19, #1
   45d24:	add	x22, x0, #0x8
   45d28:	sub	x24, x8, #0x1
   45d2c:	stp	x0, x8, [x29, #-16]
   45d30:	mov	w3, #0x3c                  	// #60
   45d34:	mov	x0, x23
   45d38:	mov	x1, x22
   45d3c:	mov	x2, x24
   45d40:	bl	c180 <__gmpn_lshift@plt>
   45d44:	mov	x27, x0
   45d48:	mov	x0, x25
   45d4c:	mov	x1, x25
   45d50:	mov	x2, x23
   45d54:	mov	x3, x24
   45d58:	bl	c2d0 <__gmpn_sub_n@plt>
   45d5c:	add	x8, x25, x19, lsl #4
   45d60:	ldur	x9, [x8, #-8]
   45d64:	add	x10, x0, x27
   45d68:	subs	x9, x9, x10
   45d6c:	stur	x9, [x8, #-8]
   45d70:	b.cs	45d84 <__gmpn_toom_interpolate_8pts@@Base+0xdc>  // b.hs, b.nlast
   45d74:	ldr	x9, [x8]
   45d78:	sub	x10, x9, #0x1
   45d7c:	str	x10, [x8], #8
   45d80:	cbz	x9, 45d74 <__gmpn_toom_interpolate_8pts@@Base+0xcc>
   45d84:	ldr	x20, [sp, #24]
   45d88:	ldur	x8, [x29, #-16]
   45d8c:	ldp	x9, x1, [sp, #8]
   45d90:	mov	w3, #0xc                   	// #12
   45d94:	mov	x0, x23
   45d98:	mov	x2, x20
   45d9c:	add	x28, x8, x9, lsl #3
   45da0:	bl	c180 <__gmpn_lshift@plt>
   45da4:	mov	x27, x0
   45da8:	mov	x0, x21
   45dac:	mov	x1, x21
   45db0:	mov	x2, x23
   45db4:	mov	x3, x20
   45db8:	bl	c2d0 <__gmpn_sub_n@plt>
   45dbc:	lsl	x20, x20, #3
   45dc0:	ldr	x8, [x21, x20]
   45dc4:	add	x9, x0, x27
   45dc8:	subs	x8, x8, x9
   45dcc:	str	x8, [x21, x20]
   45dd0:	b.cs	45df0 <__gmpn_toom_interpolate_8pts@@Base+0x148>  // b.hs, b.nlast
   45dd4:	ldr	x8, [sp, #24]
   45dd8:	add	x8, x21, x8, lsl #3
   45ddc:	add	x8, x8, #0x8
   45de0:	ldr	x9, [x8]
   45de4:	sub	x10, x9, #0x1
   45de8:	str	x10, [x8], #8
   45dec:	cbz	x9, 45de0 <__gmpn_toom_interpolate_8pts@@Base+0x138>
   45df0:	ldur	x10, [x29, #-16]
   45df4:	add	x27, x28, x19, lsl #3
   45df8:	ldr	x9, [x27]
   45dfc:	ldr	x8, [x10]
   45e00:	sub	x8, x9, x8, lsr #2
   45e04:	str	x8, [x27]
   45e08:	ldr	x8, [x10]
   45e0c:	cmp	x9, x8, lsr #2
   45e10:	b.cs	45e30 <__gmpn_toom_interpolate_8pts@@Base+0x188>  // b.hs, b.nlast
   45e14:	ldur	x8, [x29, #-16]
   45e18:	add	x8, x8, x19, lsl #5
   45e1c:	add	x8, x8, #0x8
   45e20:	ldr	x9, [x8]
   45e24:	sub	x10, x9, #0x1
   45e28:	str	x10, [x8], #8
   45e2c:	cbz	x9, 45e20 <__gmpn_toom_interpolate_8pts@@Base+0x178>
   45e30:	mov	w3, #0x3e                  	// #62
   45e34:	mov	x0, x23
   45e38:	mov	x1, x22
   45e3c:	mov	x2, x24
   45e40:	bl	c180 <__gmpn_lshift@plt>
   45e44:	mov	x22, x0
   45e48:	mov	x0, x27
   45e4c:	mov	x1, x27
   45e50:	mov	x2, x23
   45e54:	mov	x3, x24
   45e58:	bl	c2d0 <__gmpn_sub_n@plt>
   45e5c:	ldur	x8, [x29, #-8]
   45e60:	add	x10, x0, x22
   45e64:	add	x8, x27, x8, lsl #3
   45e68:	ldur	x9, [x8, #-8]
   45e6c:	subs	x9, x9, x10
   45e70:	stur	x9, [x8, #-8]
   45e74:	b.cs	45e88 <__gmpn_toom_interpolate_8pts@@Base+0x1e0>  // b.hs, b.nlast
   45e78:	ldr	x9, [x8]
   45e7c:	sub	x10, x9, #0x1
   45e80:	str	x10, [x8], #8
   45e84:	cbz	x9, 45e78 <__gmpn_toom_interpolate_8pts@@Base+0x1d0>
   45e88:	ldp	x1, x24, [sp, #16]
   45e8c:	mov	w3, #0x6                   	// #6
   45e90:	mov	x0, x23
   45e94:	mov	x2, x24
   45e98:	bl	c180 <__gmpn_lshift@plt>
   45e9c:	mov	x22, x0
   45ea0:	mov	x0, x28
   45ea4:	mov	x1, x28
   45ea8:	mov	x2, x23
   45eac:	mov	x3, x24
   45eb0:	bl	c2d0 <__gmpn_sub_n@plt>
   45eb4:	ldr	x8, [x28, x20]
   45eb8:	ldur	x24, [x29, #-16]
   45ebc:	add	x9, x0, x22
   45ec0:	subs	x8, x8, x9
   45ec4:	str	x8, [x28, x20]
   45ec8:	b.cs	45ef0 <__gmpn_toom_interpolate_8pts@@Base+0x248>  // b.hs, b.nlast
   45ecc:	ldr	x9, [sp, #24]
   45ed0:	add	x8, x19, x19, lsl #1
   45ed4:	add	x8, x9, x8
   45ed8:	add	x8, x24, x8, lsl #3
   45edc:	add	x8, x8, #0x8
   45ee0:	ldr	x9, [x8]
   45ee4:	sub	x10, x9, #0x1
   45ee8:	str	x10, [x8], #8
   45eec:	cbz	x9, 45ee0 <__gmpn_toom_interpolate_8pts@@Base+0x238>
   45ef0:	ldur	x3, [x29, #-8]
   45ef4:	add	x23, x26, x19, lsl #3
   45ef8:	mov	x0, x23
   45efc:	mov	x1, x23
   45f00:	mov	x2, x24
   45f04:	bl	c2d0 <__gmpn_sub_n@plt>
   45f08:	ldp	x8, x2, [sp, #8]
   45f0c:	ldr	x3, [sp, #24]
   45f10:	mov	x1, x26
   45f14:	lsl	x9, x8, #3
   45f18:	ldr	x8, [x26, x9]
   45f1c:	str	x9, [sp]
   45f20:	sub	x8, x8, x0
   45f24:	mov	x0, x26
   45f28:	str	x8, [x26, x9]
   45f2c:	bl	c2d0 <__gmpn_sub_n@plt>
   45f30:	ldr	x8, [x26, x20]
   45f34:	subs	x8, x8, x0
   45f38:	str	x8, [x26, x20]
   45f3c:	b.cs	45f5c <__gmpn_toom_interpolate_8pts@@Base+0x2b4>  // b.hs, b.nlast
   45f40:	ldr	x8, [sp, #24]
   45f44:	add	x8, x26, x8, lsl #3
   45f48:	add	x8, x8, #0x8
   45f4c:	ldr	x9, [x8]
   45f50:	sub	x10, x9, #0x1
   45f54:	str	x10, [x8], #8
   45f58:	cbz	x9, 45f4c <__gmpn_toom_interpolate_8pts@@Base+0x2a4>
   45f5c:	ldr	x8, [sp, #8]
   45f60:	mov	x0, x21
   45f64:	mov	x1, x21
   45f68:	mov	x2, x28
   45f6c:	add	x22, x8, #0x1
   45f70:	mov	x3, x22
   45f74:	bl	c2d0 <__gmpn_sub_n@plt>
   45f78:	mov	w3, #0x2                   	// #2
   45f7c:	mov	x0, x21
   45f80:	mov	x1, x21
   45f84:	mov	x2, x22
   45f88:	bl	c1a0 <__gmpn_rshift@plt>
   45f8c:	mov	x0, x28
   45f90:	mov	x1, x28
   45f94:	mov	x2, x26
   45f98:	mov	x3, x22
   45f9c:	bl	c2d0 <__gmpn_sub_n@plt>
   45fa0:	mov	x0, x21
   45fa4:	mov	x1, x21
   45fa8:	mov	x2, x28
   45fac:	mov	x3, x22
   45fb0:	bl	c2d0 <__gmpn_sub_n@plt>
   45fb4:	mov	x4, #0x4fa5                	// #20389
   45fb8:	movk	x4, #0xa4fa, lsl #16
   45fbc:	movk	x4, #0xfa4f, lsl #32
   45fc0:	mov	w3, #0x2d                  	// #45
   45fc4:	movk	x4, #0x4fa4, lsl #48
   45fc8:	mov	x0, x21
   45fcc:	mov	x1, x21
   45fd0:	mov	x2, x22
   45fd4:	mov	w5, wzr
   45fd8:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   45fdc:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   45fe0:	mov	x0, x28
   45fe4:	mov	x1, x28
   45fe8:	mov	x2, x22
   45fec:	mov	x4, xzr
   45ff0:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   45ff4:	mov	x0, x28
   45ff8:	mov	x1, x28
   45ffc:	mov	x2, x21
   46000:	mov	x3, x22
   46004:	bl	c160 <__gmpn_sublsh2_n@plt>
   46008:	add	x22, x24, x19, lsl #3
   4600c:	mov	x0, x22
   46010:	mov	x1, x22
   46014:	mov	x2, x26
   46018:	mov	x3, x19
   4601c:	bl	ca70 <__gmpn_add_n@plt>
   46020:	mov	x24, x0
   46024:	mov	x0, x22
   46028:	mov	x1, x22
   4602c:	mov	x2, x28
   46030:	mov	x3, x19
   46034:	bl	c2d0 <__gmpn_sub_n@plt>
   46038:	sub	x8, x24, x0
   4603c:	cmp	x8, #0x1
   46040:	b.lt	4605c <__gmpn_toom_interpolate_8pts@@Base+0x3b4>  // b.tstop
   46044:	mov	x8, x23
   46048:	ldr	x9, [x8]
   4604c:	adds	x9, x9, #0x1
   46050:	str	x9, [x8], #8
   46054:	b.cs	46048 <__gmpn_toom_interpolate_8pts@@Base+0x3a0>  // b.hs, b.nlast
   46058:	mov	x8, xzr
   4605c:	ldp	x9, x10, [x29, #-16]
   46060:	neg	x4, x8
   46064:	mov	x1, x23
   46068:	mov	x2, x27
   4606c:	lsl	x20, x10, #3
   46070:	add	x0, x9, x20
   46074:	mov	x3, x19
   46078:	bl	c760 <__gmpn_sub_nc@plt>
   4607c:	add	x2, x26, x20
   46080:	ldr	x8, [x2]
   46084:	subs	x8, x8, x0
   46088:	str	x8, [x2]
   4608c:	b.cs	460a8 <__gmpn_toom_interpolate_8pts@@Base+0x400>  // b.hs, b.nlast
   46090:	add	x8, x26, x19, lsl #4
   46094:	add	x8, x8, #0x8
   46098:	ldr	x9, [x8]
   4609c:	sub	x10, x9, #0x1
   460a0:	str	x10, [x8], #8
   460a4:	cbz	x9, 46098 <__gmpn_toom_interpolate_8pts@@Base+0x3f0>
   460a8:	add	x22, x19, #0x1
   460ac:	mov	x0, x28
   460b0:	mov	x1, x28
   460b4:	mov	x3, x22
   460b8:	bl	ca70 <__gmpn_add_n@plt>
   460bc:	ldur	x8, [x29, #-8]
   460c0:	mov	x23, x0
   460c4:	mov	x2, x21
   460c8:	mov	x3, x19
   460cc:	add	x24, x28, x8, lsl #3
   460d0:	mov	x0, x24
   460d4:	mov	x1, x24
   460d8:	bl	ca70 <__gmpn_add_n@plt>
   460dc:	ldr	x26, [sp]
   460e0:	mov	x1, x28
   460e4:	mov	x2, x24
   460e8:	mov	x3, x22
   460ec:	ldr	x8, [x28, x26]
   460f0:	add	x8, x8, x0
   460f4:	mov	x0, x28
   460f8:	str	x8, [x28, x26]
   460fc:	bl	c2d0 <__gmpn_sub_n@plt>
   46100:	subs	x8, x23, x0
   46104:	b.mi	463b4 <__gmpn_toom_interpolate_8pts@@Base+0x70c>  // b.first
   46108:	ldr	x9, [x27, #8]
   4610c:	ldur	x20, [x29, #-16]
   46110:	adds	x8, x9, x8
   46114:	str	x8, [x27, #8]
   46118:	b.cc	46134 <__gmpn_toom_interpolate_8pts@@Base+0x48c>  // b.lo, b.ul, b.last
   4611c:	add	x8, x20, x19, lsl #5
   46120:	add	x8, x8, #0x10
   46124:	ldr	x9, [x8]
   46128:	adds	x9, x9, #0x1
   4612c:	str	x9, [x8], #8
   46130:	b.cs	46124 <__gmpn_toom_interpolate_8pts@@Base+0x47c>  // b.hs, b.nlast
   46134:	ldur	x8, [x29, #-8]
   46138:	add	x0, x20, x19, lsl #5
   4613c:	mov	x1, x27
   46140:	mov	x2, x25
   46144:	orr	x3, x8, #0x1
   46148:	bl	c2d0 <__gmpn_sub_n@plt>
   4614c:	mov	w8, #0x30                  	// #48
   46150:	madd	x9, x19, x8, x20
   46154:	ldr	x8, [x9]
   46158:	ldr	x10, [x25]
   4615c:	adds	x8, x10, x8
   46160:	str	x8, [x9]
   46164:	b.cc	46254 <__gmpn_toom_interpolate_8pts@@Base+0x5ac>  // b.lo, b.ul, b.last
   46168:	mov	x11, xzr
   4616c:	sub	x10, x19, #0x1
   46170:	mov	w8, #0x1                   	// #1
   46174:	cmp	x8, x19
   46178:	b.ge	462dc <__gmpn_toom_interpolate_8pts@@Base+0x634>  // b.tcont
   4617c:	add	x12, x25, x11
   46180:	ldr	x12, [x12, #8]
   46184:	add	x13, x9, x11
   46188:	add	x8, x8, #0x1
   4618c:	add	x11, x11, #0x8
   46190:	adds	x12, x12, #0x1
   46194:	sub	x10, x10, #0x1
   46198:	str	x12, [x13, #8]
   4619c:	b.cs	46174 <__gmpn_toom_interpolate_8pts@@Base+0x4cc>  // b.hs, b.nlast
   461a0:	cmp	x25, x9
   461a4:	b.eq	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   461a8:	cmp	x8, x19
   461ac:	b.ge	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.tcont
   461b0:	sub	x12, x19, x8
   461b4:	cmp	x12, #0x4
   461b8:	b.cc	46228 <__gmpn_toom_interpolate_8pts@@Base+0x580>  // b.lo, b.ul, b.last
   461bc:	add	x13, x9, x11
   461c0:	add	x13, x13, #0x8
   461c4:	add	x14, x21, x19, lsl #4
   461c8:	cmp	x13, x14
   461cc:	b.cs	461e8 <__gmpn_toom_interpolate_8pts@@Base+0x540>  // b.hs, b.nlast
   461d0:	mov	w13, #0x38                  	// #56
   461d4:	add	x14, x25, x11
   461d8:	madd	x13, x19, x13, x20
   461dc:	add	x14, x14, #0x8
   461e0:	cmp	x14, x13
   461e4:	b.cc	46228 <__gmpn_toom_interpolate_8pts@@Base+0x580>  // b.lo, b.ul, b.last
   461e8:	add	x13, x9, x11
   461ec:	add	x11, x25, x11
   461f0:	and	x9, x12, #0xfffffffffffffffc
   461f4:	and	x14, x10, #0xfffffffffffffffc
   461f8:	add	x10, x13, #0x18
   461fc:	add	x11, x11, #0x18
   46200:	add	x8, x14, x8
   46204:	mov	x13, x9
   46208:	ldp	q0, q1, [x11, #-16]
   4620c:	add	x11, x11, #0x20
   46210:	subs	x13, x13, #0x4
   46214:	stp	q0, q1, [x10, #-16]
   46218:	add	x10, x10, #0x20
   4621c:	b.ne	46208 <__gmpn_toom_interpolate_8pts@@Base+0x560>  // b.any
   46220:	cmp	x12, x9
   46224:	b.eq	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   46228:	mov	w10, #0x6                   	// #6
   4622c:	sub	x9, x19, x8
   46230:	add	x11, x8, x19
   46234:	madd	x8, x19, x10, x8
   46238:	add	x8, x20, x8, lsl #3
   4623c:	add	x10, x21, x11, lsl #3
   46240:	ldr	x11, [x10], #8
   46244:	subs	x9, x9, #0x1
   46248:	str	x11, [x8], #8
   4624c:	b.ne	46240 <__gmpn_toom_interpolate_8pts@@Base+0x598>  // b.any
   46250:	b	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>
   46254:	cmp	x19, #0x2
   46258:	b.lt	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.tstop
   4625c:	cmp	x25, x9
   46260:	b.eq	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   46264:	sub	x8, x19, #0x1
   46268:	cmp	x8, #0x4
   4626c:	b.cc	462a4 <__gmpn_toom_interpolate_8pts@@Base+0x5fc>  // b.lo, b.ul, b.last
   46270:	add	x9, x19, x19, lsl #1
   46274:	mov	w10, #0x8                   	// #8
   46278:	bfi	x10, x9, #4, #60
   4627c:	add	x9, x20, x10
   46280:	add	x10, x21, x19, lsl #4
   46284:	cmp	x9, x10
   46288:	add	x9, x21, x19, lsl #3
   4628c:	b.cs	46374 <__gmpn_toom_interpolate_8pts@@Base+0x6cc>  // b.hs, b.nlast
   46290:	mov	w10, #0x38                  	// #56
   46294:	madd	x10, x19, x10, x20
   46298:	add	x11, x9, #0x8
   4629c:	cmp	x11, x10
   462a0:	b.cs	46374 <__gmpn_toom_interpolate_8pts@@Base+0x6cc>  // b.hs, b.nlast
   462a4:	mov	w9, #0x1                   	// #1
   462a8:	mov	w10, #0x6                   	// #6
   462ac:	sub	x8, x19, x9
   462b0:	add	x11, x9, x19
   462b4:	madd	x9, x19, x10, x9
   462b8:	add	x9, x20, x9, lsl #3
   462bc:	add	x10, x21, x11, lsl #3
   462c0:	ldr	x11, [x10], #8
   462c4:	subs	x8, x8, #0x1
   462c8:	str	x11, [x9], #8
   462cc:	b.ne	462c0 <__gmpn_toom_interpolate_8pts@@Base+0x618>  // b.any
   462d0:	ldur	x8, [x29, #-8]
   462d4:	add	x2, x21, x8, lsl #3
   462d8:	b	462f8 <__gmpn_toom_interpolate_8pts@@Base+0x650>
   462dc:	ldur	x8, [x29, #-8]
   462e0:	add	x2, x21, x8, lsl #3
   462e4:	mov	x8, x2
   462e8:	ldr	x9, [x8]
   462ec:	adds	x9, x9, #0x1
   462f0:	str	x9, [x8], #8
   462f4:	b.cs	462e8 <__gmpn_toom_interpolate_8pts@@Base+0x640>  // b.hs, b.nlast
   462f8:	ldr	x0, [sp, #16]
   462fc:	mov	x3, x19
   46300:	mov	x1, x0
   46304:	bl	ca70 <__gmpn_add_n@plt>
   46308:	ldr	x8, [sp, #24]
   4630c:	cmp	x8, x19
   46310:	b.eq	46354 <__gmpn_toom_interpolate_8pts@@Base+0x6ac>  // b.none
   46314:	lsl	x8, x19, #6
   46318:	ldr	x9, [x21, x26]
   4631c:	ldr	x10, [x20, x8]
   46320:	add	x9, x9, x0
   46324:	add	x9, x10, x9
   46328:	str	x9, [x20, x8]
   4632c:	ldr	x8, [x21, x26]
   46330:	add	x8, x8, x0
   46334:	cmp	x9, x8
   46338:	b.cs	46354 <__gmpn_toom_interpolate_8pts@@Base+0x6ac>  // b.hs, b.nlast
   4633c:	add	x8, x20, x19, lsl #6
   46340:	add	x8, x8, #0x8
   46344:	ldr	x9, [x8]
   46348:	adds	x9, x9, #0x1
   4634c:	str	x9, [x8], #8
   46350:	b.cs	46344 <__gmpn_toom_interpolate_8pts@@Base+0x69c>  // b.hs, b.nlast
   46354:	ldp	x20, x19, [sp, #128]
   46358:	ldp	x22, x21, [sp, #112]
   4635c:	ldp	x24, x23, [sp, #96]
   46360:	ldp	x26, x25, [sp, #80]
   46364:	ldp	x28, x27, [sp, #64]
   46368:	ldp	x29, x30, [sp, #48]
   4636c:	add	sp, sp, #0x90
   46370:	ret
   46374:	mov	w12, #0x30                  	// #48
   46378:	and	x10, x8, #0xfffffffffffffffc
   4637c:	madd	x12, x19, x12, x20
   46380:	add	x11, x9, #0x18
   46384:	orr	x9, x10, #0x1
   46388:	add	x12, x12, #0x18
   4638c:	mov	x13, x10
   46390:	ldp	q0, q1, [x11, #-16]
   46394:	add	x11, x11, #0x20
   46398:	subs	x13, x13, #0x4
   4639c:	stp	q0, q1, [x12, #-16]
   463a0:	add	x12, x12, #0x20
   463a4:	b.ne	46390 <__gmpn_toom_interpolate_8pts@@Base+0x6e8>  // b.any
   463a8:	cmp	x8, x10
   463ac:	b.eq	462d0 <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   463b0:	b	462a8 <__gmpn_toom_interpolate_8pts@@Base+0x600>
   463b4:	ldur	x20, [x29, #-16]
   463b8:	add	x8, x20, x19, lsl #5
   463bc:	add	x8, x8, #0x8
   463c0:	ldr	x9, [x8]
   463c4:	sub	x10, x9, #0x1
   463c8:	str	x10, [x8], #8
   463cc:	cbz	x9, 463c0 <__gmpn_toom_interpolate_8pts@@Base+0x718>
   463d0:	b	46134 <__gmpn_toom_interpolate_8pts@@Base+0x48c>

00000000000463d4 <__gmpn_toom_interpolate_12pts@@Base>:
   463d4:	sub	sp, sp, #0xd0
   463d8:	stp	x29, x30, [sp, #112]
   463dc:	stp	x28, x27, [sp, #128]
   463e0:	stp	x26, x25, [sp, #144]
   463e4:	stp	x20, x19, [sp, #192]
   463e8:	add	x29, sp, #0x70
   463ec:	mov	x27, x7
   463f0:	mov	x28, x5
   463f4:	mov	x20, x4
   463f8:	mov	x25, x0
   463fc:	add	x11, x4, x4, lsl #1
   46400:	lsl	x26, x4, #3
   46404:	stp	x24, x23, [sp, #160]
   46408:	stp	x22, x21, [sp, #176]
   4640c:	stp	x3, x2, [x29, #-32]
   46410:	stur	x1, [x29, #-8]
   46414:	stur	x0, [x29, #-40]
   46418:	str	w6, [sp, #36]
   4641c:	str	x11, [sp, #56]
   46420:	cbz	w6, 46674 <__gmpn_toom_interpolate_12pts@@Base+0x2a0>
   46424:	ldur	x19, [x29, #-24]
   46428:	mov	w8, #0x58                  	// #88
   4642c:	madd	x21, x20, x8, x25
   46430:	mov	x2, x21
   46434:	mov	x0, x19
   46438:	mov	x1, x19
   4643c:	mov	x3, x28
   46440:	bl	c2d0 <__gmpn_sub_n@plt>
   46444:	lsl	x22, x28, #3
   46448:	ldr	x8, [x19, x22]
   4644c:	subs	x8, x8, x0
   46450:	str	x8, [x19, x22]
   46454:	b.cs	46474 <__gmpn_toom_interpolate_12pts@@Base+0xa0>  // b.hs, b.nlast
   46458:	ldur	x8, [x29, #-24]
   4645c:	add	x8, x8, x28, lsl #3
   46460:	add	x8, x8, #0x8
   46464:	ldr	x9, [x8]
   46468:	sub	x10, x9, #0x1
   4646c:	str	x10, [x8], #8
   46470:	cbz	x9, 46464 <__gmpn_toom_interpolate_12pts@@Base+0x90>
   46474:	mov	w8, #0x38                  	// #56
   46478:	mov	w3, #0xa                   	// #10
   4647c:	mov	x0, x27
   46480:	mov	x1, x21
   46484:	mov	x2, x28
   46488:	madd	x19, x20, x8, x25
   4648c:	bl	c180 <__gmpn_lshift@plt>
   46490:	mov	x23, x0
   46494:	mov	x0, x19
   46498:	mov	x1, x19
   4649c:	mov	x2, x27
   464a0:	mov	x3, x28
   464a4:	bl	c2d0 <__gmpn_sub_n@plt>
   464a8:	ldr	x8, [x19, x22]
   464ac:	add	x9, x0, x23
   464b0:	subs	x8, x8, x9
   464b4:	str	x8, [x19, x22]
   464b8:	b.cs	464dc <__gmpn_toom_interpolate_12pts@@Base+0x108>  // b.hs, b.nlast
   464bc:	sub	x8, x26, x20
   464c0:	add	x8, x28, x8
   464c4:	add	x8, x25, x8, lsl #3
   464c8:	add	x8, x8, #0x8
   464cc:	ldr	x9, [x8]
   464d0:	sub	x10, x9, #0x1
   464d4:	str	x10, [x8], #8
   464d8:	cbz	x9, 464cc <__gmpn_toom_interpolate_12pts@@Base+0xf8>
   464dc:	ldur	x10, [x29, #-32]
   464e0:	ldr	x8, [x21]
   464e4:	ldr	x9, [x10]
   464e8:	sub	x8, x9, x8, lsr #2
   464ec:	str	x8, [x10]
   464f0:	ldr	x8, [x21]
   464f4:	cmp	x9, x8, lsr #2
   464f8:	b.cs	46514 <__gmpn_toom_interpolate_12pts@@Base+0x140>  // b.hs, b.nlast
   464fc:	ldur	x8, [x29, #-32]
   46500:	add	x8, x8, #0x8
   46504:	ldr	x9, [x8]
   46508:	sub	x10, x9, #0x1
   4650c:	str	x10, [x8], #8
   46510:	cbz	x9, 46504 <__gmpn_toom_interpolate_12pts@@Base+0x130>
   46514:	sub	x23, x28, #0x1
   46518:	add	x1, x21, #0x8
   4651c:	mov	w3, #0x3e                  	// #62
   46520:	mov	x0, x27
   46524:	mov	x2, x23
   46528:	stur	x1, [x29, #-16]
   4652c:	bl	c180 <__gmpn_lshift@plt>
   46530:	ldur	x24, [x29, #-32]
   46534:	mov	x19, x0
   46538:	mov	x2, x27
   4653c:	mov	x3, x23
   46540:	mov	x0, x24
   46544:	mov	x1, x24
   46548:	bl	c2d0 <__gmpn_sub_n@plt>
   4654c:	add	x8, x24, x28, lsl #3
   46550:	ldur	x9, [x8, #-8]
   46554:	add	x10, x0, x19
   46558:	subs	x9, x9, x10
   4655c:	stur	x9, [x8, #-8]
   46560:	b.cs	46574 <__gmpn_toom_interpolate_12pts@@Base+0x1a0>  // b.hs, b.nlast
   46564:	ldr	x9, [x8]
   46568:	sub	x10, x9, #0x1
   4656c:	str	x10, [x8], #8
   46570:	cbz	x9, 46564 <__gmpn_toom_interpolate_12pts@@Base+0x190>
   46574:	mov	w3, #0x14                  	// #20
   46578:	mov	x0, x27
   4657c:	mov	x1, x21
   46580:	mov	x2, x28
   46584:	bl	c180 <__gmpn_lshift@plt>
   46588:	ldur	x24, [x29, #-8]
   4658c:	mov	x19, x0
   46590:	mov	x2, x27
   46594:	mov	x3, x28
   46598:	mov	x0, x24
   4659c:	mov	x1, x24
   465a0:	bl	c2d0 <__gmpn_sub_n@plt>
   465a4:	ldr	x8, [x24, x22]
   465a8:	add	x9, x0, x19
   465ac:	subs	x8, x8, x9
   465b0:	str	x8, [x24, x22]
   465b4:	b.cs	465d4 <__gmpn_toom_interpolate_12pts@@Base+0x200>  // b.hs, b.nlast
   465b8:	ldur	x8, [x29, #-8]
   465bc:	add	x8, x8, x28, lsl #3
   465c0:	add	x8, x8, #0x8
   465c4:	ldr	x9, [x8]
   465c8:	sub	x10, x9, #0x1
   465cc:	str	x10, [x8], #8
   465d0:	cbz	x9, 465c4 <__gmpn_toom_interpolate_12pts@@Base+0x1f0>
   465d4:	ldr	x8, [sp, #56]
   465d8:	add	x25, x25, x8, lsl #3
   465dc:	ldr	x8, [x21]
   465e0:	ldr	x9, [x25]
   465e4:	sub	x8, x9, x8, lsr #4
   465e8:	str	x8, [x25]
   465ec:	ldr	x8, [x21]
   465f0:	cmp	x9, x8, lsr #4
   465f4:	b.cs	46618 <__gmpn_toom_interpolate_12pts@@Base+0x244>  // b.hs, b.nlast
   465f8:	ldur	x9, [x29, #-40]
   465fc:	mov	w8, #0x18                  	// #24
   46600:	madd	x8, x20, x8, x9
   46604:	add	x8, x8, #0x8
   46608:	ldr	x9, [x8]
   4660c:	sub	x10, x9, #0x1
   46610:	str	x10, [x8], #8
   46614:	cbz	x9, 46608 <__gmpn_toom_interpolate_12pts@@Base+0x234>
   46618:	ldur	x1, [x29, #-16]
   4661c:	mov	w3, #0x3c                  	// #60
   46620:	mov	x0, x27
   46624:	mov	x2, x23
   46628:	bl	c180 <__gmpn_lshift@plt>
   4662c:	mov	x19, x0
   46630:	mov	x0, x25
   46634:	mov	x1, x25
   46638:	mov	x2, x27
   4663c:	mov	x3, x23
   46640:	bl	c2d0 <__gmpn_sub_n@plt>
   46644:	add	x8, x25, x28, lsl #3
   46648:	ldur	x9, [x8, #-8]
   4664c:	ldur	x25, [x29, #-40]
   46650:	ldr	x11, [sp, #56]
   46654:	add	x10, x0, x19
   46658:	subs	x9, x9, x10
   4665c:	stur	x9, [x8, #-8]
   46660:	b.cs	46674 <__gmpn_toom_interpolate_12pts@@Base+0x2a0>  // b.hs, b.nlast
   46664:	ldr	x9, [x8]
   46668:	sub	x10, x9, #0x1
   4666c:	str	x10, [x8], #8
   46670:	cbz	x9, 46664 <__gmpn_toom_interpolate_12pts@@Base+0x290>
   46674:	lsl	x22, x11, #3
   46678:	lsl	x24, x20, #1
   4667c:	str	x28, [sp, #40]
   46680:	add	x28, x25, x22
   46684:	mov	w3, #0x14                  	// #20
   46688:	mov	x0, x27
   4668c:	mov	x1, x25
   46690:	mov	x2, x24
   46694:	add	x21, x11, #0x1
   46698:	add	x19, x28, x26
   4669c:	bl	c180 <__gmpn_lshift@plt>
   466a0:	mov	x23, x0
   466a4:	mov	x0, x19
   466a8:	mov	x1, x19
   466ac:	mov	x2, x27
   466b0:	mov	x3, x24
   466b4:	stur	x24, [x29, #-16]
   466b8:	bl	c2d0 <__gmpn_sub_n@plt>
   466bc:	ldr	x8, [x28, x22]
   466c0:	ldur	x10, [x29, #-8]
   466c4:	add	x9, x0, x23
   466c8:	stur	x28, [x29, #-48]
   466cc:	sub	x8, x8, x9
   466d0:	add	x23, x10, x26
   466d4:	str	x8, [x28, x22]
   466d8:	ldr	x8, [x25]
   466dc:	ldr	x9, [x23]
   466e0:	sub	x8, x9, x8, lsr #4
   466e4:	str	x8, [x23]
   466e8:	ldr	x8, [x25]
   466ec:	cmp	x9, x8, lsr #4
   466f0:	b.cs	46710 <__gmpn_toom_interpolate_12pts@@Base+0x33c>  // b.hs, b.nlast
   466f4:	ldur	x8, [x29, #-8]
   466f8:	add	x8, x8, x20, lsl #3
   466fc:	add	x8, x8, #0x8
   46700:	ldr	x9, [x8]
   46704:	sub	x10, x9, #0x1
   46708:	str	x10, [x8], #8
   4670c:	cbz	x9, 46700 <__gmpn_toom_interpolate_12pts@@Base+0x32c>
   46710:	str	x20, [sp, #24]
   46714:	add	x1, x25, #0x8
   46718:	ldur	x25, [x29, #-16]
   4671c:	mov	w3, #0x3c                  	// #60
   46720:	mov	x0, x27
   46724:	str	x1, [sp, #8]
   46728:	sub	x24, x25, #0x1
   4672c:	mov	x2, x24
   46730:	bl	c180 <__gmpn_lshift@plt>
   46734:	mov	x19, x0
   46738:	mov	x0, x23
   4673c:	mov	x1, x23
   46740:	mov	x2, x27
   46744:	mov	x3, x24
   46748:	bl	c2d0 <__gmpn_sub_n@plt>
   4674c:	add	x8, x23, x25, lsl #3
   46750:	ldur	x9, [x8, #-8]
   46754:	add	x10, x0, x19
   46758:	str	x23, [sp]
   4675c:	subs	x9, x9, x10
   46760:	stur	x9, [x8, #-8]
   46764:	b.cs	46778 <__gmpn_toom_interpolate_12pts@@Base+0x3a4>  // b.hs, b.nlast
   46768:	ldr	x9, [x8]
   4676c:	sub	x10, x9, #0x1
   46770:	str	x10, [x8], #8
   46774:	cbz	x9, 46768 <__gmpn_toom_interpolate_12pts@@Base+0x394>
   46778:	str	x27, [sp, #48]
   4677c:	ldur	x28, [x29, #-8]
   46780:	ldur	x19, [x29, #-48]
   46784:	mov	x0, x27
   46788:	mov	x3, x21
   4678c:	mov	x1, x28
   46790:	mov	x2, x19
   46794:	bl	ca70 <__gmpn_add_n@plt>
   46798:	mov	x0, x19
   4679c:	mov	x1, x19
   467a0:	mov	x2, x28
   467a4:	mov	x3, x21
   467a8:	bl	c2d0 <__gmpn_sub_n@plt>
   467ac:	ldp	x25, x23, [x29, #-40]
   467b0:	mov	x20, x26
   467b4:	mov	w3, #0xa                   	// #10
   467b8:	mov	x0, x28
   467bc:	add	x19, x23, x26
   467c0:	ldur	x26, [x29, #-16]
   467c4:	mov	x1, x25
   467c8:	mov	x2, x26
   467cc:	bl	c180 <__gmpn_lshift@plt>
   467d0:	mov	x27, x0
   467d4:	mov	x0, x19
   467d8:	mov	x1, x19
   467dc:	mov	x2, x28
   467e0:	mov	x3, x26
   467e4:	mov	x19, x20
   467e8:	bl	c2d0 <__gmpn_sub_n@plt>
   467ec:	ldr	x8, [x23, x22]
   467f0:	ldr	x20, [sp, #24]
   467f4:	add	x9, x0, x27
   467f8:	mov	w10, #0x38                  	// #56
   467fc:	sub	x8, x8, x9
   46800:	madd	x27, x20, x10, x25
   46804:	str	x19, [sp, #16]
   46808:	add	x19, x27, x19
   4680c:	str	x8, [x23, x22]
   46810:	ldr	x8, [x25]
   46814:	ldr	x9, [x19]
   46818:	sub	x8, x9, x8, lsr #2
   4681c:	str	x8, [x19]
   46820:	ldr	x8, [x25]
   46824:	cmp	x9, x8, lsr #2
   46828:	b.cs	46848 <__gmpn_toom_interpolate_12pts@@Base+0x474>  // b.hs, b.nlast
   4682c:	ldur	x8, [x29, #-40]
   46830:	add	x8, x8, x20, lsl #6
   46834:	add	x8, x8, #0x8
   46838:	ldr	x9, [x8]
   4683c:	sub	x10, x9, #0x1
   46840:	str	x10, [x8], #8
   46844:	cbz	x9, 46838 <__gmpn_toom_interpolate_12pts@@Base+0x464>
   46848:	ldur	x26, [x29, #-8]
   4684c:	ldr	x1, [sp, #8]
   46850:	mov	w3, #0x3e                  	// #62
   46854:	mov	x2, x24
   46858:	mov	x0, x26
   4685c:	bl	c180 <__gmpn_lshift@plt>
   46860:	mov	x25, x0
   46864:	mov	x0, x19
   46868:	mov	x1, x19
   4686c:	mov	x2, x26
   46870:	mov	x3, x24
   46874:	bl	c2d0 <__gmpn_sub_n@plt>
   46878:	ldur	x8, [x29, #-16]
   4687c:	add	x10, x0, x25
   46880:	add	x8, x19, x8, lsl #3
   46884:	ldur	x9, [x8, #-8]
   46888:	subs	x9, x9, x10
   4688c:	stur	x9, [x8, #-8]
   46890:	b.cs	468a4 <__gmpn_toom_interpolate_12pts@@Base+0x4d0>  // b.hs, b.nlast
   46894:	ldr	x9, [x8]
   46898:	sub	x10, x9, #0x1
   4689c:	str	x10, [x8], #8
   468a0:	cbz	x9, 46894 <__gmpn_toom_interpolate_12pts@@Base+0x4c0>
   468a4:	ldur	x25, [x29, #-8]
   468a8:	ldur	x19, [x29, #-32]
   468ac:	mov	x2, x27
   468b0:	mov	x3, x21
   468b4:	mov	x0, x25
   468b8:	mov	x1, x19
   468bc:	bl	c2d0 <__gmpn_sub_n@plt>
   468c0:	mov	x0, x27
   468c4:	mov	x1, x27
   468c8:	mov	x2, x19
   468cc:	mov	x3, x21
   468d0:	bl	ca70 <__gmpn_add_n@plt>
   468d4:	ldp	x19, x3, [x29, #-24]
   468d8:	ldur	x2, [x29, #-40]
   468dc:	add	x0, x19, x20, lsl #3
   468e0:	mov	x1, x0
   468e4:	mov	x23, x0
   468e8:	bl	c2d0 <__gmpn_sub_n@plt>
   468ec:	ldr	x8, [x19, x22]
   468f0:	ldur	x28, [x29, #-48]
   468f4:	mov	w3, #0x101                 	// #257
   468f8:	mov	x1, x25
   468fc:	sub	x8, x8, x0
   46900:	mov	x0, x28
   46904:	mov	x2, x21
   46908:	str	x8, [x19, x22]
   4690c:	bl	c9e0 <__gmpn_submul_1@plt>
   46910:	mov	x4, #0x771b                	// #30491
   46914:	movk	x4, #0x53e3, lsl #16
   46918:	movk	x4, #0xc705, lsl #32
   4691c:	mov	w3, #0xb13                 	// #2835
   46920:	movk	x4, #0x938c, lsl #48
   46924:	mov	w5, #0x2                   	// #2
   46928:	mov	x0, x28
   4692c:	mov	x1, x28
   46930:	mov	x2, x21
   46934:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46938:	ldr	x8, [x28, x22]
   4693c:	lsr	x9, x8, #61
   46940:	cbz	x9, 46950 <__gmpn_toom_interpolate_12pts@@Base+0x57c>
   46944:	ldr	x9, [sp, #56]
   46948:	orr	x8, x8, #0xc000000000000000
   4694c:	str	x8, [x28, x9, lsl #3]
   46950:	ldur	x25, [x29, #-8]
   46954:	mov	w3, #0x3c                  	// #60
   46958:	mov	x1, x28
   4695c:	mov	x2, x21
   46960:	mov	x0, x25
   46964:	bl	d400 <__gmpn_addmul_1@plt>
   46968:	mov	x3, #0x101010101010101     	// #72340172838076673
   4696c:	mov	x0, x25
   46970:	mov	x1, x25
   46974:	mov	x2, x21
   46978:	mov	x4, xzr
   4697c:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   46980:	ldp	x24, x26, [x29, #-32]
   46984:	mov	w3, #0x5                   	// #5
   46988:	mov	x2, x21
   4698c:	mov	x0, x24
   46990:	mov	x1, x26
   46994:	bl	c180 <__gmpn_lshift@plt>
   46998:	mov	x0, x27
   4699c:	mov	x1, x27
   469a0:	mov	x2, x24
   469a4:	mov	x3, x21
   469a8:	bl	c2d0 <__gmpn_sub_n@plt>
   469ac:	ldr	x19, [sp, #48]
   469b0:	mov	w3, #0x64                  	// #100
   469b4:	mov	x1, x27
   469b8:	mov	x2, x21
   469bc:	mov	x0, x19
   469c0:	bl	c9e0 <__gmpn_submul_1@plt>
   469c4:	mov	w3, #0x9                   	// #9
   469c8:	mov	x0, x24
   469cc:	mov	x1, x26
   469d0:	mov	x2, x21
   469d4:	bl	c180 <__gmpn_lshift@plt>
   469d8:	mov	x0, x19
   469dc:	mov	x1, x19
   469e0:	mov	x2, x24
   469e4:	mov	x3, x21
   469e8:	bl	c2d0 <__gmpn_sub_n@plt>
   469ec:	mov	x4, #0x4c35                	// #19509
   469f0:	movk	x4, #0x9f31, lsl #16
   469f4:	movk	x4, #0xd44, lsl #32
   469f8:	mov	w3, #0xa61d                	// #42525
   469fc:	movk	x4, #0xe7b4, lsl #48
   46a00:	mov	x0, x19
   46a04:	mov	x1, x19
   46a08:	mov	x2, x21
   46a0c:	mov	w5, wzr
   46a10:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46a14:	mov	w3, #0xe1                  	// #225
   46a18:	mov	x0, x27
   46a1c:	mov	x1, x19
   46a20:	mov	x2, x21
   46a24:	bl	c9e0 <__gmpn_submul_1@plt>
   46a28:	mov	x4, #0x8e39                	// #36409
   46a2c:	movk	x4, #0x38e3, lsl #16
   46a30:	movk	x4, #0xe38e, lsl #32
   46a34:	mov	w3, #0x9                   	// #9
   46a38:	movk	x4, #0x8e38, lsl #48
   46a3c:	mov	w5, #0x2                   	// #2
   46a40:	mov	x0, x27
   46a44:	mov	x1, x27
   46a48:	mov	x2, x21
   46a4c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46a50:	mov	x0, x26
   46a54:	mov	x1, x26
   46a58:	mov	x2, x27
   46a5c:	mov	x3, x21
   46a60:	bl	c2d0 <__gmpn_sub_n@plt>
   46a64:	mov	x0, x28
   46a68:	mov	x1, x27
   46a6c:	mov	x2, x28
   46a70:	mov	x3, x21
   46a74:	bl	c840 <__gmpn_rsh1sub_n@plt>
   46a78:	ldr	x8, [x28, x22]
   46a7c:	mov	x0, x27
   46a80:	mov	x1, x27
   46a84:	mov	x2, x28
   46a88:	and	x8, x8, #0x7fffffffffffffff
   46a8c:	mov	x3, x21
   46a90:	str	x8, [x28, x22]
   46a94:	bl	c2d0 <__gmpn_sub_n@plt>
   46a98:	mov	x0, x25
   46a9c:	mov	x1, x25
   46aa0:	mov	x2, x19
   46aa4:	mov	x3, x21
   46aa8:	bl	c950 <__gmpn_rsh1add_n@plt>
   46aac:	ldr	x8, [x25, x22]
   46ab0:	mov	x0, x26
   46ab4:	mov	x1, x26
   46ab8:	mov	x2, x19
   46abc:	and	x8, x8, #0x7fffffffffffffff
   46ac0:	mov	x3, x21
   46ac4:	str	x8, [x25, x22]
   46ac8:	bl	c2d0 <__gmpn_sub_n@plt>
   46acc:	mov	x0, x19
   46ad0:	mov	x1, x19
   46ad4:	mov	x2, x25
   46ad8:	mov	x3, x21
   46adc:	bl	c2d0 <__gmpn_sub_n@plt>
   46ae0:	ldr	x21, [sp, #16]
   46ae4:	ldur	x26, [x29, #-40]
   46ae8:	mov	x2, x25
   46aec:	mov	x3, x20
   46af0:	add	x0, x26, x21
   46af4:	mov	x1, x0
   46af8:	bl	ca70 <__gmpn_add_n@plt>
   46afc:	ldr	x8, [x25, x21]
   46b00:	ldur	x9, [x29, #-16]
   46b04:	adds	x8, x8, x0
   46b08:	add	x9, x26, x9, lsl #3
   46b0c:	str	x8, [x9]
   46b10:	b.cc	46c18 <__gmpn_toom_interpolate_12pts@@Base+0x844>  // b.lo, b.ul, b.last
   46b14:	ldur	x22, [x29, #-48]
   46b18:	ldr	x15, [sp]
   46b1c:	mov	x11, xzr
   46b20:	sub	x10, x20, #0x1
   46b24:	mov	w4, #0x1                   	// #1
   46b28:	mov	w8, #0x1                   	// #1
   46b2c:	mov	x24, x23
   46b30:	cmp	x8, x20
   46b34:	b.ge	46cb8 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.tcont
   46b38:	add	x12, x15, x11
   46b3c:	ldr	x12, [x12, #8]
   46b40:	add	x13, x9, x11
   46b44:	add	x8, x8, #0x1
   46b48:	add	x11, x11, #0x8
   46b4c:	adds	x12, x12, #0x1
   46b50:	sub	x10, x10, #0x1
   46b54:	str	x12, [x13, #8]
   46b58:	b.cs	46b30 <__gmpn_toom_interpolate_12pts@@Base+0x75c>  // b.hs, b.nlast
   46b5c:	cmp	x15, x9
   46b60:	mov	x4, xzr
   46b64:	b.eq	46cb8 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.none
   46b68:	cmp	x8, x20
   46b6c:	b.ge	46cb8 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.tcont
   46b70:	sub	x12, x20, x8
   46b74:	cmp	x12, #0x4
   46b78:	b.cc	46bec <__gmpn_toom_interpolate_12pts@@Base+0x818>  // b.lo, b.ul, b.last
   46b7c:	ldur	x14, [x29, #-8]
   46b80:	add	x13, x9, x11
   46b84:	add	x13, x13, #0x8
   46b88:	add	x14, x14, x20, lsl #4
   46b8c:	cmp	x13, x14
   46b90:	b.cs	46bac <__gmpn_toom_interpolate_12pts@@Base+0x7d8>  // b.hs, b.nlast
   46b94:	mov	w13, #0x18                  	// #24
   46b98:	add	x14, x15, x11
   46b9c:	madd	x13, x20, x13, x26
   46ba0:	add	x14, x14, #0x8
   46ba4:	cmp	x14, x13
   46ba8:	b.cc	46bec <__gmpn_toom_interpolate_12pts@@Base+0x818>  // b.lo, b.ul, b.last
   46bac:	add	x13, x9, x11
   46bb0:	add	x11, x15, x11
   46bb4:	and	x9, x12, #0xfffffffffffffffc
   46bb8:	and	x14, x10, #0xfffffffffffffffc
   46bbc:	add	x10, x13, #0x18
   46bc0:	add	x11, x11, #0x18
   46bc4:	add	x8, x14, x8
   46bc8:	mov	x13, x9
   46bcc:	ldp	q0, q1, [x11, #-16]
   46bd0:	add	x11, x11, #0x20
   46bd4:	subs	x13, x13, #0x4
   46bd8:	stp	q0, q1, [x10, #-16]
   46bdc:	add	x10, x10, #0x20
   46be0:	b.ne	46bcc <__gmpn_toom_interpolate_12pts@@Base+0x7f8>  // b.any
   46be4:	cmp	x12, x9
   46be8:	b.eq	46ca8 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>  // b.none
   46bec:	add	x10, x8, x20, lsl #1
   46bf0:	sub	x9, x20, x8
   46bf4:	add	x11, x8, x20
   46bf8:	add	x8, x26, x10, lsl #3
   46bfc:	ldur	x10, [x29, #-8]
   46c00:	add	x10, x10, x11, lsl #3
   46c04:	ldr	x11, [x10], #8
   46c08:	subs	x9, x9, #0x1
   46c0c:	str	x11, [x8], #8
   46c10:	b.ne	46c04 <__gmpn_toom_interpolate_12pts@@Base+0x830>  // b.any
   46c14:	b	46ca8 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>
   46c18:	cmp	x20, #0x2
   46c1c:	mov	x4, xzr
   46c20:	b.lt	46cb0 <__gmpn_toom_interpolate_12pts@@Base+0x8dc>  // b.tstop
   46c24:	ldr	x8, [sp]
   46c28:	ldur	x22, [x29, #-48]
   46c2c:	mov	x24, x23
   46c30:	cmp	x8, x9
   46c34:	b.eq	46cb8 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.none
   46c38:	sub	x8, x20, #0x1
   46c3c:	cmp	x8, #0x4
   46c40:	b.cc	46c7c <__gmpn_toom_interpolate_12pts@@Base+0x8a8>  // b.lo, b.ul, b.last
   46c44:	ldur	x11, [x29, #-8]
   46c48:	mov	w9, #0x8                   	// #8
   46c4c:	lsl	x10, x20, #4
   46c50:	bfi	x9, x20, #4, #60
   46c54:	add	x9, x26, x9
   46c58:	add	x10, x11, x10
   46c5c:	cmp	x9, x10
   46c60:	add	x9, x11, x20, lsl #3
   46c64:	b.cs	472cc <__gmpn_toom_interpolate_12pts@@Base+0xef8>  // b.hs, b.nlast
   46c68:	mov	w10, #0x18                  	// #24
   46c6c:	madd	x10, x20, x10, x26
   46c70:	add	x11, x9, #0x8
   46c74:	cmp	x11, x10
   46c78:	b.cs	472cc <__gmpn_toom_interpolate_12pts@@Base+0xef8>  // b.hs, b.nlast
   46c7c:	mov	w9, #0x1                   	// #1
   46c80:	add	x10, x9, x20, lsl #1
   46c84:	sub	x8, x20, x9
   46c88:	add	x11, x9, x20
   46c8c:	add	x9, x26, x10, lsl #3
   46c90:	ldur	x10, [x29, #-8]
   46c94:	add	x10, x10, x11, lsl #3
   46c98:	ldr	x11, [x10], #8
   46c9c:	subs	x8, x8, #0x1
   46ca0:	str	x11, [x9], #8
   46ca4:	b.ne	46c98 <__gmpn_toom_interpolate_12pts@@Base+0x8c4>  // b.any
   46ca8:	mov	x4, xzr
   46cac:	b	46cb8 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>
   46cb0:	ldur	x22, [x29, #-48]
   46cb4:	mov	x24, x23
   46cb8:	ldr	x23, [sp, #56]
   46cbc:	ldp	x8, x9, [x29, #-16]
   46cc0:	mov	x0, x22
   46cc4:	mov	x1, x22
   46cc8:	mov	x3, x20
   46ccc:	ldr	x19, [x9, x23, lsl #3]
   46cd0:	add	x2, x9, x8, lsl #3
   46cd4:	bl	ce90 <__gmpn_add_nc@plt>
   46cd8:	ldr	x8, [x22, x21]
   46cdc:	add	x9, x0, x19
   46ce0:	adds	x8, x8, x9
   46ce4:	str	x8, [x22, x21]
   46ce8:	b.cc	46d04 <__gmpn_toom_interpolate_12pts@@Base+0x930>  // b.lo, b.ul, b.last
   46cec:	add	x8, x26, x20, lsl #5
   46cf0:	add	x8, x8, #0x8
   46cf4:	ldr	x9, [x8]
   46cf8:	adds	x9, x9, #0x1
   46cfc:	str	x9, [x8], #8
   46d00:	b.cs	46cf4 <__gmpn_toom_interpolate_12pts@@Base+0x920>  // b.hs, b.nlast
   46d04:	ldur	x19, [x29, #-24]
   46d08:	mov	w8, #0x28                  	// #40
   46d0c:	madd	x0, x20, x8, x26
   46d10:	mov	x1, x0
   46d14:	mov	x2, x19
   46d18:	mov	x3, x20
   46d1c:	bl	ca70 <__gmpn_add_n@plt>
   46d20:	add	x8, x20, x20, lsl #1
   46d24:	add	x10, x26, x8, lsl #4
   46d28:	ldr	x9, [x10]
   46d2c:	ldr	x22, [sp, #40]
   46d30:	lsl	x8, x8, #1
   46d34:	add	x9, x9, x0
   46d38:	str	x9, [x10]
   46d3c:	ldr	x11, [x19, x20, lsl #3]
   46d40:	adds	x9, x11, x9
   46d44:	str	x9, [x10]
   46d48:	b.cc	46e3c <__gmpn_toom_interpolate_12pts@@Base+0xa68>  // b.lo, b.ul, b.last
   46d4c:	mov	x12, xzr
   46d50:	sub	x11, x20, #0x1
   46d54:	mov	w4, #0x1                   	// #1
   46d58:	mov	w9, #0x1                   	// #1
   46d5c:	cmp	x9, x20
   46d60:	b.ge	46eb8 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tcont
   46d64:	add	x13, x24, x12
   46d68:	ldr	x13, [x13, #8]
   46d6c:	add	x14, x10, x12
   46d70:	add	x9, x9, #0x1
   46d74:	add	x12, x12, #0x8
   46d78:	adds	x13, x13, #0x1
   46d7c:	sub	x11, x11, #0x1
   46d80:	str	x13, [x14, #8]
   46d84:	b.cs	46d5c <__gmpn_toom_interpolate_12pts@@Base+0x988>  // b.hs, b.nlast
   46d88:	cmp	x24, x10
   46d8c:	mov	x4, xzr
   46d90:	b.eq	46eb8 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.none
   46d94:	cmp	x9, x20
   46d98:	b.ge	46eb8 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tcont
   46d9c:	sub	x13, x20, x9
   46da0:	cmp	x13, #0x4
   46da4:	b.cc	46e10 <__gmpn_toom_interpolate_12pts@@Base+0xa3c>  // b.lo, b.ul, b.last
   46da8:	ldur	x15, [x29, #-24]
   46dac:	add	x14, x10, x12
   46db0:	add	x14, x14, #0x8
   46db4:	add	x15, x15, x20, lsl #4
   46db8:	cmp	x14, x15
   46dbc:	b.cs	46dd0 <__gmpn_toom_interpolate_12pts@@Base+0x9fc>  // b.hs, b.nlast
   46dc0:	add	x14, x24, x12
   46dc4:	add	x14, x14, #0x8
   46dc8:	cmp	x14, x27
   46dcc:	b.cc	46e10 <__gmpn_toom_interpolate_12pts@@Base+0xa3c>  // b.lo, b.ul, b.last
   46dd0:	add	x14, x10, x12
   46dd4:	add	x12, x24, x12
   46dd8:	and	x10, x13, #0xfffffffffffffffc
   46ddc:	and	x15, x11, #0xfffffffffffffffc
   46de0:	add	x11, x14, #0x18
   46de4:	add	x12, x12, #0x18
   46de8:	add	x9, x15, x9
   46dec:	mov	x14, x10
   46df0:	ldp	q0, q1, [x12, #-16]
   46df4:	add	x12, x12, #0x20
   46df8:	subs	x14, x14, #0x4
   46dfc:	stp	q0, q1, [x11, #-16]
   46e00:	add	x11, x11, #0x20
   46e04:	b.ne	46df0 <__gmpn_toom_interpolate_12pts@@Base+0xa1c>  // b.any
   46e08:	cmp	x13, x10
   46e0c:	b.eq	46eb4 <__gmpn_toom_interpolate_12pts@@Base+0xae0>  // b.none
   46e10:	ldur	x11, [x29, #-24]
   46e14:	sub	x10, x20, x9
   46e18:	add	x8, x9, x8
   46e1c:	add	x9, x9, x20
   46e20:	add	x8, x26, x8, lsl #3
   46e24:	add	x9, x11, x9, lsl #3
   46e28:	ldr	x11, [x9], #8
   46e2c:	subs	x10, x10, #0x1
   46e30:	str	x11, [x8], #8
   46e34:	b.ne	46e28 <__gmpn_toom_interpolate_12pts@@Base+0xa54>  // b.any
   46e38:	b	46eb4 <__gmpn_toom_interpolate_12pts@@Base+0xae0>
   46e3c:	cmp	x20, #0x2
   46e40:	mov	x4, xzr
   46e44:	b.lt	46eb8 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tstop
   46e48:	cmp	x24, x10
   46e4c:	b.eq	46eb8 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.none
   46e50:	sub	x9, x20, #0x1
   46e54:	cmp	x9, #0x4
   46e58:	b.cc	46e88 <__gmpn_toom_interpolate_12pts@@Base+0xab4>  // b.lo, b.ul, b.last
   46e5c:	ldur	x12, [x29, #-24]
   46e60:	lsl	x10, x8, #3
   46e64:	orr	x10, x10, #0x8
   46e68:	add	x10, x26, x10
   46e6c:	add	x11, x12, x20, lsl #4
   46e70:	cmp	x10, x11
   46e74:	add	x10, x12, x20, lsl #3
   46e78:	b.cs	47308 <__gmpn_toom_interpolate_12pts@@Base+0xf34>  // b.hs, b.nlast
   46e7c:	add	x11, x10, #0x8
   46e80:	cmp	x11, x27
   46e84:	b.cs	47308 <__gmpn_toom_interpolate_12pts@@Base+0xf34>  // b.hs, b.nlast
   46e88:	mov	w10, #0x1                   	// #1
   46e8c:	ldur	x11, [x29, #-24]
   46e90:	sub	x9, x20, x10
   46e94:	add	x8, x10, x8
   46e98:	add	x10, x10, x20
   46e9c:	add	x8, x26, x8, lsl #3
   46ea0:	add	x10, x11, x10, lsl #3
   46ea4:	ldr	x11, [x10], #8
   46ea8:	subs	x9, x9, #0x1
   46eac:	str	x11, [x8], #8
   46eb0:	b.ne	46ea4 <__gmpn_toom_interpolate_12pts@@Base+0xad0>  // b.any
   46eb4:	mov	x4, xzr
   46eb8:	ldp	x9, x8, [x29, #-24]
   46ebc:	mov	x0, x27
   46ec0:	mov	x1, x27
   46ec4:	mov	x3, x20
   46ec8:	ldr	x19, [x9, x23, lsl #3]
   46ecc:	add	x2, x9, x8, lsl #3
   46ed0:	bl	ce90 <__gmpn_add_nc@plt>
   46ed4:	lsl	x8, x20, #6
   46ed8:	ldr	x9, [x26, x8]
   46edc:	add	x10, x0, x19
   46ee0:	adds	x9, x9, x10
   46ee4:	str	x9, [x26, x8]
   46ee8:	b.cc	46f04 <__gmpn_toom_interpolate_12pts@@Base+0xb30>  // b.lo, b.ul, b.last
   46eec:	add	x8, x26, x21, lsl #3
   46ef0:	add	x8, x8, #0x8
   46ef4:	ldr	x9, [x8]
   46ef8:	adds	x9, x9, #0x1
   46efc:	str	x9, [x8], #8
   46f00:	b.cs	46ef4 <__gmpn_toom_interpolate_12pts@@Base+0xb20>  // b.hs, b.nlast
   46f04:	ldr	x21, [sp, #48]
   46f08:	mov	w8, #0x48                  	// #72
   46f0c:	madd	x0, x20, x8, x26
   46f10:	mov	x1, x0
   46f14:	mov	x2, x21
   46f18:	mov	x3, x20
   46f1c:	bl	ca70 <__gmpn_add_n@plt>
   46f20:	mov	w8, #0x50                  	// #80
   46f24:	madd	x9, x20, x8, x26
   46f28:	ldr	x8, [x9]
   46f2c:	ldr	w12, [sp, #36]
   46f30:	add	x10, x8, x0
   46f34:	str	x10, [x9]
   46f38:	add	x8, x21, x20, lsl #3
   46f3c:	ldr	x11, [x8]
   46f40:	add	x11, x11, x10
   46f44:	str	x11, [x9]
   46f48:	cbz	w12, 47048 <__gmpn_toom_interpolate_12pts@@Base+0xc74>
   46f4c:	cmp	x11, x10
   46f50:	b.cs	47144 <__gmpn_toom_interpolate_12pts@@Base+0xd70>  // b.hs, b.nlast
   46f54:	mov	x12, xzr
   46f58:	sub	x11, x20, #0x1
   46f5c:	mov	w4, #0x1                   	// #1
   46f60:	mov	w10, #0x1                   	// #1
   46f64:	cmp	x10, x20
   46f68:	b.ge	471c8 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tcont
   46f6c:	add	x13, x8, x12
   46f70:	ldr	x13, [x13, #8]
   46f74:	add	x14, x9, x12
   46f78:	add	x10, x10, #0x1
   46f7c:	add	x12, x12, #0x8
   46f80:	adds	x13, x13, #0x1
   46f84:	sub	x11, x11, #0x1
   46f88:	str	x13, [x14, #8]
   46f8c:	b.cs	46f64 <__gmpn_toom_interpolate_12pts@@Base+0xb90>  // b.hs, b.nlast
   46f90:	cmp	x8, x9
   46f94:	mov	x4, xzr
   46f98:	b.eq	471c8 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.none
   46f9c:	cmp	x10, x20
   46fa0:	b.ge	471c8 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tcont
   46fa4:	sub	x13, x20, x10
   46fa8:	cmp	x13, #0x4
   46fac:	b.cc	4701c <__gmpn_toom_interpolate_12pts@@Base+0xc48>  // b.lo, b.ul, b.last
   46fb0:	add	x14, x9, x12
   46fb4:	add	x14, x14, #0x8
   46fb8:	add	x15, x21, x20, lsl #4
   46fbc:	cmp	x14, x15
   46fc0:	b.cs	46fdc <__gmpn_toom_interpolate_12pts@@Base+0xc08>  // b.hs, b.nlast
   46fc4:	mov	w14, #0x58                  	// #88
   46fc8:	add	x15, x8, x12
   46fcc:	madd	x14, x20, x14, x26
   46fd0:	add	x15, x15, #0x8
   46fd4:	cmp	x15, x14
   46fd8:	b.cc	4701c <__gmpn_toom_interpolate_12pts@@Base+0xc48>  // b.lo, b.ul, b.last
   46fdc:	add	x9, x9, x12
   46fe0:	add	x12, x8, x12
   46fe4:	and	x8, x13, #0xfffffffffffffffc
   46fe8:	and	x14, x11, #0xfffffffffffffffc
   46fec:	add	x9, x9, #0x18
   46ff0:	add	x11, x12, #0x18
   46ff4:	add	x10, x14, x10
   46ff8:	mov	x12, x8
   46ffc:	ldp	q0, q1, [x11, #-16]
   47000:	add	x11, x11, #0x20
   47004:	subs	x12, x12, #0x4
   47008:	stp	q0, q1, [x9, #-16]
   4700c:	add	x9, x9, #0x20
   47010:	b.ne	46ffc <__gmpn_toom_interpolate_12pts@@Base+0xc28>  // b.any
   47014:	cmp	x13, x8
   47018:	b.eq	471c4 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>  // b.none
   4701c:	mov	w9, #0xa                   	// #10
   47020:	add	x11, x10, x20
   47024:	madd	x9, x20, x9, x10
   47028:	sub	x8, x20, x10
   4702c:	add	x9, x26, x9, lsl #3
   47030:	add	x10, x21, x11, lsl #3
   47034:	ldr	x11, [x10], #8
   47038:	subs	x8, x8, #0x1
   4703c:	str	x11, [x9], #8
   47040:	b.ne	47034 <__gmpn_toom_interpolate_12pts@@Base+0xc60>  // b.any
   47044:	b	471c4 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>
   47048:	cmp	x11, x10
   4704c:	b.cs	4722c <__gmpn_toom_interpolate_12pts@@Base+0xe58>  // b.hs, b.nlast
   47050:	mov	x12, xzr
   47054:	sub	x11, x22, #0x1
   47058:	mov	w10, #0x1                   	// #1
   4705c:	cmp	x10, x22
   47060:	b.ge	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tcont
   47064:	add	x13, x8, x12
   47068:	ldr	x13, [x13, #8]
   4706c:	add	x14, x9, x12
   47070:	add	x10, x10, #0x1
   47074:	add	x12, x12, #0x8
   47078:	adds	x13, x13, #0x1
   4707c:	sub	x11, x11, #0x1
   47080:	str	x13, [x14, #8]
   47084:	b.cs	4705c <__gmpn_toom_interpolate_12pts@@Base+0xc88>  // b.hs, b.nlast
   47088:	cmp	x8, x9
   4708c:	b.eq	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   47090:	cmp	x10, x22
   47094:	b.ge	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tcont
   47098:	sub	x13, x22, x10
   4709c:	cmp	x13, #0x4
   470a0:	b.cc	47118 <__gmpn_toom_interpolate_12pts@@Base+0xd44>  // b.lo, b.ul, b.last
   470a4:	add	x14, x9, x12
   470a8:	add	x15, x22, x20
   470ac:	add	x14, x14, #0x8
   470b0:	add	x15, x21, x15, lsl #3
   470b4:	cmp	x14, x15
   470b8:	b.cs	470d8 <__gmpn_toom_interpolate_12pts@@Base+0xd04>  // b.hs, b.nlast
   470bc:	mov	w14, #0xa                   	// #10
   470c0:	add	x15, x8, x12
   470c4:	madd	x14, x20, x14, x22
   470c8:	add	x14, x26, x14, lsl #3
   470cc:	add	x15, x15, #0x8
   470d0:	cmp	x15, x14
   470d4:	b.cc	47118 <__gmpn_toom_interpolate_12pts@@Base+0xd44>  // b.lo, b.ul, b.last
   470d8:	add	x9, x9, x12
   470dc:	add	x12, x8, x12
   470e0:	and	x8, x13, #0xfffffffffffffffc
   470e4:	and	x14, x11, #0xfffffffffffffffc
   470e8:	add	x9, x9, #0x18
   470ec:	add	x11, x12, #0x18
   470f0:	add	x10, x14, x10
   470f4:	mov	x12, x8
   470f8:	ldp	q0, q1, [x11, #-16]
   470fc:	add	x11, x11, #0x20
   47100:	subs	x12, x12, #0x4
   47104:	stp	q0, q1, [x9, #-16]
   47108:	add	x9, x9, #0x20
   4710c:	b.ne	470f8 <__gmpn_toom_interpolate_12pts@@Base+0xd24>  // b.any
   47110:	cmp	x13, x8
   47114:	b.eq	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   47118:	mov	w9, #0xa                   	// #10
   4711c:	add	x11, x10, x20
   47120:	madd	x9, x20, x9, x10
   47124:	sub	x8, x22, x10
   47128:	add	x9, x26, x9, lsl #3
   4712c:	add	x10, x21, x11, lsl #3
   47130:	ldr	x11, [x10], #8
   47134:	subs	x8, x8, #0x1
   47138:	str	x11, [x9], #8
   4713c:	b.ne	47130 <__gmpn_toom_interpolate_12pts@@Base+0xd5c>  // b.any
   47140:	b	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>
   47144:	cmp	x20, #0x2
   47148:	mov	x4, xzr
   4714c:	b.lt	471c8 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tstop
   47150:	cmp	x8, x9
   47154:	b.eq	471c8 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.none
   47158:	sub	x8, x20, #0x1
   4715c:	cmp	x8, #0x4
   47160:	b.cc	47198 <__gmpn_toom_interpolate_12pts@@Base+0xdc4>  // b.lo, b.ul, b.last
   47164:	add	x9, x20, x20, lsl #2
   47168:	mov	w10, #0x8                   	// #8
   4716c:	bfi	x10, x9, #4, #60
   47170:	add	x9, x26, x10
   47174:	add	x10, x21, x20, lsl #4
   47178:	cmp	x9, x10
   4717c:	add	x9, x21, x20, lsl #3
   47180:	b.cs	47344 <__gmpn_toom_interpolate_12pts@@Base+0xf70>  // b.hs, b.nlast
   47184:	mov	w10, #0x58                  	// #88
   47188:	madd	x10, x20, x10, x26
   4718c:	add	x11, x9, #0x8
   47190:	cmp	x11, x10
   47194:	b.cs	47344 <__gmpn_toom_interpolate_12pts@@Base+0xf70>  // b.hs, b.nlast
   47198:	mov	w9, #0x1                   	// #1
   4719c:	mov	w10, #0xa                   	// #10
   471a0:	sub	x8, x20, x9
   471a4:	add	x11, x9, x20
   471a8:	madd	x9, x20, x10, x9
   471ac:	add	x9, x26, x9, lsl #3
   471b0:	add	x10, x21, x11, lsl #3
   471b4:	ldr	x11, [x10], #8
   471b8:	subs	x8, x8, #0x1
   471bc:	str	x11, [x9], #8
   471c0:	b.ne	471b4 <__gmpn_toom_interpolate_12pts@@Base+0xde0>  // b.any
   471c4:	mov	x4, xzr
   471c8:	cmp	x22, x20
   471cc:	b.le	473c4 <__gmpn_toom_interpolate_12pts@@Base+0xff0>
   471d0:	mov	w8, #0x58                  	// #88
   471d4:	madd	x0, x20, x8, x26
   471d8:	ldur	x8, [x29, #-16]
   471dc:	ldr	x19, [x21, x23, lsl #3]
   471e0:	mov	x1, x0
   471e4:	mov	x3, x20
   471e8:	add	x2, x21, x8, lsl #3
   471ec:	bl	ce90 <__gmpn_add_nc@plt>
   471f0:	add	x8, x20, x20, lsl #1
   471f4:	lsl	x9, x8, #5
   471f8:	ldr	x10, [x26, x9]
   471fc:	add	x11, x0, x19
   47200:	adds	x10, x10, x11
   47204:	str	x10, [x26, x9]
   47208:	b.cc	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.lo, b.ul, b.last
   4720c:	lsl	x8, x8, #2
   47210:	add	x8, x26, x8, lsl #3
   47214:	add	x8, x8, #0x8
   47218:	ldr	x9, [x8]
   4721c:	adds	x9, x9, #0x1
   47220:	str	x9, [x8], #8
   47224:	b.cs	47218 <__gmpn_toom_interpolate_12pts@@Base+0xe44>  // b.hs, b.nlast
   47228:	b	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>
   4722c:	cmp	x22, #0x2
   47230:	b.lt	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tstop
   47234:	cmp	x8, x9
   47238:	b.eq	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   4723c:	sub	x8, x22, #0x1
   47240:	cmp	x8, #0x4
   47244:	b.cc	47280 <__gmpn_toom_interpolate_12pts@@Base+0xeac>  // b.lo, b.ul, b.last
   47248:	add	x10, x20, x20, lsl #2
   4724c:	mov	w9, #0x8                   	// #8
   47250:	add	x11, x22, x20
   47254:	bfi	x9, x10, #4, #60
   47258:	add	x9, x26, x9
   4725c:	add	x11, x21, x11, lsl #3
   47260:	cmp	x9, x11
   47264:	add	x9, x21, x20, lsl #3
   47268:	b.cs	47384 <__gmpn_toom_interpolate_12pts@@Base+0xfb0>  // b.hs, b.nlast
   4726c:	add	x10, x22, x10, lsl #1
   47270:	add	x10, x26, x10, lsl #3
   47274:	add	x11, x9, #0x8
   47278:	cmp	x11, x10
   4727c:	b.cs	47384 <__gmpn_toom_interpolate_12pts@@Base+0xfb0>  // b.hs, b.nlast
   47280:	mov	w9, #0x1                   	// #1
   47284:	mov	w10, #0xa                   	// #10
   47288:	sub	x8, x22, x9
   4728c:	add	x11, x9, x20
   47290:	madd	x9, x20, x10, x9
   47294:	add	x9, x26, x9, lsl #3
   47298:	add	x10, x21, x11, lsl #3
   4729c:	ldr	x11, [x10], #8
   472a0:	subs	x8, x8, #0x1
   472a4:	str	x11, [x9], #8
   472a8:	b.ne	4729c <__gmpn_toom_interpolate_12pts@@Base+0xec8>  // b.any
   472ac:	ldp	x20, x19, [sp, #192]
   472b0:	ldp	x22, x21, [sp, #176]
   472b4:	ldp	x24, x23, [sp, #160]
   472b8:	ldp	x26, x25, [sp, #144]
   472bc:	ldp	x28, x27, [sp, #128]
   472c0:	ldp	x29, x30, [sp, #112]
   472c4:	add	sp, sp, #0xd0
   472c8:	ret
   472cc:	and	x10, x8, #0xfffffffffffffffc
   472d0:	add	x12, x26, x20, lsl #4
   472d4:	add	x11, x9, #0x18
   472d8:	orr	x9, x10, #0x1
   472dc:	add	x12, x12, #0x18
   472e0:	mov	x13, x10
   472e4:	ldp	q0, q1, [x11, #-16]
   472e8:	add	x11, x11, #0x20
   472ec:	subs	x13, x13, #0x4
   472f0:	stp	q0, q1, [x12, #-16]
   472f4:	add	x12, x12, #0x20
   472f8:	b.ne	472e4 <__gmpn_toom_interpolate_12pts@@Base+0xf10>  // b.any
   472fc:	cmp	x8, x10
   47300:	b.eq	46ca8 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>  // b.none
   47304:	b	46c80 <__gmpn_toom_interpolate_12pts@@Base+0x8ac>
   47308:	and	x11, x9, #0xfffffffffffffffc
   4730c:	add	x13, x26, x8, lsl #3
   47310:	add	x12, x10, #0x18
   47314:	orr	x10, x11, #0x1
   47318:	add	x13, x13, #0x18
   4731c:	mov	x14, x11
   47320:	ldp	q0, q1, [x12, #-16]
   47324:	add	x12, x12, #0x20
   47328:	subs	x14, x14, #0x4
   4732c:	stp	q0, q1, [x13, #-16]
   47330:	add	x13, x13, #0x20
   47334:	b.ne	47320 <__gmpn_toom_interpolate_12pts@@Base+0xf4c>  // b.any
   47338:	cmp	x9, x11
   4733c:	b.eq	46eb4 <__gmpn_toom_interpolate_12pts@@Base+0xae0>  // b.none
   47340:	b	46e8c <__gmpn_toom_interpolate_12pts@@Base+0xab8>
   47344:	mov	w12, #0x50                  	// #80
   47348:	and	x10, x8, #0xfffffffffffffffc
   4734c:	madd	x12, x20, x12, x26
   47350:	add	x11, x9, #0x18
   47354:	orr	x9, x10, #0x1
   47358:	add	x12, x12, #0x18
   4735c:	mov	x13, x10
   47360:	ldp	q0, q1, [x11, #-16]
   47364:	add	x11, x11, #0x20
   47368:	subs	x13, x13, #0x4
   4736c:	stp	q0, q1, [x12, #-16]
   47370:	add	x12, x12, #0x20
   47374:	b.ne	47360 <__gmpn_toom_interpolate_12pts@@Base+0xf8c>  // b.any
   47378:	cmp	x8, x10
   4737c:	b.eq	471c4 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>  // b.none
   47380:	b	4719c <__gmpn_toom_interpolate_12pts@@Base+0xdc8>
   47384:	mov	w12, #0x50                  	// #80
   47388:	and	x10, x8, #0xfffffffffffffffc
   4738c:	madd	x12, x20, x12, x26
   47390:	add	x11, x9, #0x18
   47394:	orr	x9, x10, #0x1
   47398:	add	x12, x12, #0x18
   4739c:	mov	x13, x10
   473a0:	ldp	q0, q1, [x11, #-16]
   473a4:	add	x11, x11, #0x20
   473a8:	subs	x13, x13, #0x4
   473ac:	stp	q0, q1, [x12, #-16]
   473b0:	add	x12, x12, #0x20
   473b4:	b.ne	473a0 <__gmpn_toom_interpolate_12pts@@Base+0xfcc>  // b.any
   473b8:	cmp	x8, x10
   473bc:	b.eq	472ac <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   473c0:	b	47284 <__gmpn_toom_interpolate_12pts@@Base+0xeb0>
   473c4:	mov	w8, #0x58                  	// #88
   473c8:	madd	x0, x20, x8, x26
   473cc:	ldur	x8, [x29, #-16]
   473d0:	mov	x3, x22
   473d4:	ldp	x20, x19, [sp, #192]
   473d8:	ldp	x24, x23, [sp, #160]
   473dc:	add	x2, x21, x8, lsl #3
   473e0:	ldp	x22, x21, [sp, #176]
   473e4:	ldp	x26, x25, [sp, #144]
   473e8:	ldp	x28, x27, [sp, #128]
   473ec:	ldp	x29, x30, [sp, #112]
   473f0:	mov	x1, x0
   473f4:	add	sp, sp, #0xd0
   473f8:	b	ce90 <__gmpn_add_nc@plt>

00000000000473fc <__gmpn_toom_interpolate_16pts@@Base>:
   473fc:	sub	sp, sp, #0xe0
   47400:	stp	x29, x30, [sp, #128]
   47404:	add	x29, sp, #0x80
   47408:	stp	x28, x27, [sp, #144]
   4740c:	stp	x26, x25, [sp, #160]
   47410:	stp	x24, x23, [sp, #176]
   47414:	stp	x22, x21, [sp, #192]
   47418:	stp	x20, x19, [sp, #208]
   4741c:	stur	x4, [x29, #-40]
   47420:	stp	x2, x3, [x29, #-24]
   47424:	str	x1, [sp, #24]
   47428:	ldr	x22, [x29, #96]
   4742c:	mov	x19, x5
   47430:	mov	x24, x0
   47434:	add	x28, x5, x5, lsl #1
   47438:	lsl	x27, x5, #3
   4743c:	stur	x0, [x29, #-56]
   47440:	str	x28, [sp, #64]
   47444:	str	w7, [sp, #12]
   47448:	stur	x22, [x29, #-32]
   4744c:	str	x6, [sp, #16]
   47450:	cbz	w7, 477c0 <__gmpn_toom_interpolate_16pts@@Base+0x3c4>
   47454:	mov	w8, #0x38                  	// #56
   47458:	mov	w9, #0x78                  	// #120
   4745c:	madd	x20, x19, x8, x24
   47460:	madd	x21, x19, x9, x24
   47464:	mov	x0, x20
   47468:	mov	x1, x20
   4746c:	mov	x2, x21
   47470:	mov	x3, x6
   47474:	mov	x25, x6
   47478:	bl	c2d0 <__gmpn_sub_n@plt>
   4747c:	lsl	x24, x25, #3
   47480:	ldr	x8, [x20, x24]
   47484:	subs	x8, x8, x0
   47488:	str	x8, [x20, x24]
   4748c:	b.cs	474b4 <__gmpn_toom_interpolate_16pts@@Base+0xb8>  // b.hs, b.nlast
   47490:	ldur	x9, [x29, #-56]
   47494:	sub	x8, x27, x19
   47498:	add	x8, x25, x8
   4749c:	add	x8, x9, x8, lsl #3
   474a0:	add	x8, x8, #0x8
   474a4:	ldr	x9, [x8]
   474a8:	sub	x10, x9, #0x1
   474ac:	str	x10, [x8], #8
   474b0:	cbz	x9, 474a4 <__gmpn_toom_interpolate_16pts@@Base+0xa8>
   474b4:	mov	w3, #0xe                   	// #14
   474b8:	mov	x0, x22
   474bc:	mov	x1, x21
   474c0:	mov	x2, x25
   474c4:	bl	c180 <__gmpn_lshift@plt>
   474c8:	ldur	x23, [x29, #-24]
   474cc:	mov	x20, x0
   474d0:	mov	x2, x22
   474d4:	mov	x3, x25
   474d8:	mov	x0, x23
   474dc:	mov	x1, x23
   474e0:	bl	c2d0 <__gmpn_sub_n@plt>
   474e4:	ldr	x8, [x23, x24]
   474e8:	add	x9, x0, x20
   474ec:	subs	x8, x8, x9
   474f0:	str	x8, [x23, x24]
   474f4:	b.cs	47514 <__gmpn_toom_interpolate_16pts@@Base+0x118>  // b.hs, b.nlast
   474f8:	ldur	x8, [x29, #-24]
   474fc:	add	x8, x8, x25, lsl #3
   47500:	add	x8, x8, #0x8
   47504:	ldr	x9, [x8]
   47508:	sub	x10, x9, #0x1
   4750c:	str	x10, [x8], #8
   47510:	cbz	x9, 47504 <__gmpn_toom_interpolate_16pts@@Base+0x108>
   47514:	ldur	x8, [x29, #-56]
   47518:	add	x20, x8, x28, lsl #3
   4751c:	ldr	x8, [x21]
   47520:	ldr	x9, [x20]
   47524:	sub	x8, x9, x8, lsr #2
   47528:	str	x8, [x20]
   4752c:	ldr	x8, [x21]
   47530:	cmp	x9, x8, lsr #2
   47534:	b.cs	47558 <__gmpn_toom_interpolate_16pts@@Base+0x15c>  // b.hs, b.nlast
   47538:	ldur	x9, [x29, #-56]
   4753c:	mov	w8, #0x18                  	// #24
   47540:	madd	x8, x19, x8, x9
   47544:	add	x8, x8, #0x8
   47548:	ldr	x9, [x8]
   4754c:	sub	x10, x9, #0x1
   47550:	str	x10, [x8], #8
   47554:	cbz	x9, 47548 <__gmpn_toom_interpolate_16pts@@Base+0x14c>
   47558:	add	x26, x21, #0x8
   4755c:	sub	x23, x25, #0x1
   47560:	mov	w3, #0x3e                  	// #62
   47564:	mov	x0, x22
   47568:	mov	x1, x26
   4756c:	mov	x2, x23
   47570:	bl	c180 <__gmpn_lshift@plt>
   47574:	mov	x28, x0
   47578:	mov	x0, x20
   4757c:	mov	x1, x20
   47580:	mov	x2, x22
   47584:	mov	x3, x23
   47588:	bl	c2d0 <__gmpn_sub_n@plt>
   4758c:	add	x8, x20, x25, lsl #3
   47590:	ldur	x9, [x8, #-8]
   47594:	add	x10, x0, x28
   47598:	subs	x9, x9, x10
   4759c:	stur	x9, [x8, #-8]
   475a0:	b.cs	475b4 <__gmpn_toom_interpolate_16pts@@Base+0x1b8>  // b.hs, b.nlast
   475a4:	ldr	x9, [x8]
   475a8:	sub	x10, x9, #0x1
   475ac:	str	x10, [x8], #8
   475b0:	cbz	x9, 475a4 <__gmpn_toom_interpolate_16pts@@Base+0x1a8>
   475b4:	mov	w8, #0xb                   	// #11
   475b8:	mul	x9, x19, x8
   475bc:	ldur	x8, [x29, #-56]
   475c0:	mov	w3, #0x1c                  	// #28
   475c4:	mov	x0, x22
   475c8:	mov	x1, x21
   475cc:	mov	x2, x25
   475d0:	str	x9, [sp, #48]
   475d4:	add	x20, x8, x9, lsl #3
   475d8:	bl	c180 <__gmpn_lshift@plt>
   475dc:	mov	x28, x0
   475e0:	mov	x0, x20
   475e4:	mov	x1, x20
   475e8:	mov	x2, x22
   475ec:	mov	x3, x25
   475f0:	bl	c2d0 <__gmpn_sub_n@plt>
   475f4:	ldr	x8, [x20, x24]
   475f8:	add	x9, x0, x28
   475fc:	subs	x8, x8, x9
   47600:	str	x8, [x20, x24]
   47604:	b.cs	4762c <__gmpn_toom_interpolate_16pts@@Base+0x230>  // b.hs, b.nlast
   47608:	ldur	x9, [x29, #-56]
   4760c:	mov	w8, #0xb                   	// #11
   47610:	madd	x8, x19, x8, x25
   47614:	add	x8, x9, x8, lsl #3
   47618:	add	x8, x8, #0x8
   4761c:	ldr	x9, [x8]
   47620:	sub	x10, x9, #0x1
   47624:	str	x10, [x8], #8
   47628:	cbz	x9, 4761c <__gmpn_toom_interpolate_16pts@@Base+0x220>
   4762c:	ldur	x10, [x29, #-16]
   47630:	ldr	x8, [x21]
   47634:	ldr	x28, [sp, #64]
   47638:	ldr	x9, [x10]
   4763c:	sub	x8, x9, x8, lsr #4
   47640:	str	x8, [x10]
   47644:	ldr	x8, [x21]
   47648:	cmp	x9, x8, lsr #4
   4764c:	b.cs	47668 <__gmpn_toom_interpolate_16pts@@Base+0x26c>  // b.hs, b.nlast
   47650:	ldur	x8, [x29, #-16]
   47654:	add	x8, x8, #0x8
   47658:	ldr	x9, [x8]
   4765c:	sub	x10, x9, #0x1
   47660:	str	x10, [x8], #8
   47664:	cbz	x9, 47658 <__gmpn_toom_interpolate_16pts@@Base+0x25c>
   47668:	mov	w3, #0x3c                  	// #60
   4766c:	mov	x0, x22
   47670:	mov	x1, x26
   47674:	mov	x2, x23
   47678:	stur	x26, [x29, #-8]
   4767c:	bl	c180 <__gmpn_lshift@plt>
   47680:	ldur	x26, [x29, #-16]
   47684:	mov	x20, x0
   47688:	mov	x2, x22
   4768c:	mov	x3, x23
   47690:	mov	x0, x26
   47694:	mov	x1, x26
   47698:	bl	c2d0 <__gmpn_sub_n@plt>
   4769c:	add	x8, x26, x25, lsl #3
   476a0:	ldur	x9, [x8, #-8]
   476a4:	add	x10, x0, x20
   476a8:	subs	x9, x9, x10
   476ac:	stur	x9, [x8, #-8]
   476b0:	b.cs	476c4 <__gmpn_toom_interpolate_16pts@@Base+0x2c8>  // b.hs, b.nlast
   476b4:	ldr	x9, [x8]
   476b8:	sub	x10, x9, #0x1
   476bc:	str	x10, [x8], #8
   476c0:	cbz	x9, 476b4 <__gmpn_toom_interpolate_16pts@@Base+0x2b8>
   476c4:	ldur	x26, [x29, #-32]
   476c8:	mov	w3, #0x2a                  	// #42
   476cc:	mov	x1, x21
   476d0:	mov	x2, x25
   476d4:	mov	x0, x26
   476d8:	bl	c180 <__gmpn_lshift@plt>
   476dc:	ldr	x22, [sp, #24]
   476e0:	mov	x20, x0
   476e4:	mov	x2, x26
   476e8:	mov	x3, x25
   476ec:	mov	x0, x22
   476f0:	mov	x1, x22
   476f4:	bl	c2d0 <__gmpn_sub_n@plt>
   476f8:	ldr	x8, [x22, x24]
   476fc:	add	x9, x0, x20
   47700:	subs	x8, x8, x9
   47704:	str	x8, [x22, x24]
   47708:	b.cs	47728 <__gmpn_toom_interpolate_16pts@@Base+0x32c>  // b.hs, b.nlast
   4770c:	ldr	x8, [sp, #24]
   47710:	add	x8, x8, x25, lsl #3
   47714:	add	x8, x8, #0x8
   47718:	ldr	x9, [x8]
   4771c:	sub	x10, x9, #0x1
   47720:	str	x10, [x8], #8
   47724:	cbz	x9, 47718 <__gmpn_toom_interpolate_16pts@@Base+0x31c>
   47728:	ldur	x10, [x29, #-40]
   4772c:	ldr	x8, [x21]
   47730:	ldur	x24, [x29, #-56]
   47734:	ldr	x9, [x10]
   47738:	sub	x8, x9, x8, lsr #6
   4773c:	str	x8, [x10]
   47740:	ldr	x8, [x21]
   47744:	cmp	x9, x8, lsr #6
   47748:	b.cs	47764 <__gmpn_toom_interpolate_16pts@@Base+0x368>  // b.hs, b.nlast
   4774c:	ldur	x8, [x29, #-40]
   47750:	add	x8, x8, #0x8
   47754:	ldr	x9, [x8]
   47758:	sub	x10, x9, #0x1
   4775c:	str	x10, [x8], #8
   47760:	cbz	x9, 47754 <__gmpn_toom_interpolate_16pts@@Base+0x358>
   47764:	ldur	x1, [x29, #-8]
   47768:	mov	w3, #0x3a                  	// #58
   4776c:	mov	x0, x26
   47770:	mov	x2, x23
   47774:	bl	c180 <__gmpn_lshift@plt>
   47778:	ldur	x21, [x29, #-40]
   4777c:	mov	x20, x0
   47780:	mov	x2, x26
   47784:	mov	x3, x23
   47788:	mov	x0, x21
   4778c:	mov	x1, x21
   47790:	bl	c2d0 <__gmpn_sub_n@plt>
   47794:	add	x8, x21, x25, lsl #3
   47798:	ldur	x9, [x8, #-8]
   4779c:	add	x10, x0, x20
   477a0:	subs	x9, x9, x10
   477a4:	stur	x9, [x8, #-8]
   477a8:	b.cs	477cc <__gmpn_toom_interpolate_16pts@@Base+0x3d0>  // b.hs, b.nlast
   477ac:	ldr	x9, [x8]
   477b0:	sub	x10, x9, #0x1
   477b4:	str	x10, [x8], #8
   477b8:	cbz	x9, 477ac <__gmpn_toom_interpolate_16pts@@Base+0x3b0>
   477bc:	b	477cc <__gmpn_toom_interpolate_16pts@@Base+0x3d0>
   477c0:	mov	w8, #0xb                   	// #11
   477c4:	mul	x8, x19, x8
   477c8:	str	x8, [sp, #48]
   477cc:	ldur	x26, [x29, #-32]
   477d0:	ldur	x23, [x29, #-16]
   477d4:	lsl	x25, x19, #1
   477d8:	mov	w3, #0x1c                  	// #28
   477dc:	mov	x0, x26
   477e0:	mov	x1, x24
   477e4:	mov	x2, x25
   477e8:	add	x21, x28, #0x1
   477ec:	mov	x22, x28
   477f0:	mov	x28, x27
   477f4:	add	x27, x23, x27
   477f8:	bl	c180 <__gmpn_lshift@plt>
   477fc:	mov	x20, x0
   47800:	mov	x0, x27
   47804:	mov	x1, x27
   47808:	mov	x2, x26
   4780c:	mov	x3, x25
   47810:	str	x27, [sp]
   47814:	stur	x25, [x29, #-8]
   47818:	bl	c2d0 <__gmpn_sub_n@plt>
   4781c:	lsl	x25, x22, #3
   47820:	ldr	x8, [x23, x25]
   47824:	ldr	x22, [sp, #48]
   47828:	add	x9, x0, x20
   4782c:	sub	x8, x8, x9
   47830:	add	x10, x24, x22, lsl #3
   47834:	add	x27, x10, x28
   47838:	str	x8, [x23, x25]
   4783c:	ldr	x8, [x24]
   47840:	ldr	x9, [x27]
   47844:	str	x10, [sp, #56]
   47848:	stur	x28, [x29, #-48]
   4784c:	sub	x8, x9, x8, lsr #4
   47850:	str	x8, [x27]
   47854:	ldr	x8, [x24]
   47858:	cmp	x9, x8, lsr #4
   4785c:	b.cs	4787c <__gmpn_toom_interpolate_16pts@@Base+0x480>  // b.hs, b.nlast
   47860:	add	x8, x22, x19
   47864:	add	x8, x24, x8, lsl #3
   47868:	add	x8, x8, #0x8
   4786c:	ldr	x9, [x8]
   47870:	sub	x10, x9, #0x1
   47874:	str	x10, [x8], #8
   47878:	cbz	x9, 4786c <__gmpn_toom_interpolate_16pts@@Base+0x470>
   4787c:	ldur	x20, [x29, #-8]
   47880:	ldur	x22, [x29, #-32]
   47884:	add	x1, x24, #0x8
   47888:	mov	w3, #0x3c                  	// #60
   4788c:	sub	x23, x20, #0x1
   47890:	mov	x0, x22
   47894:	mov	x2, x23
   47898:	str	x1, [sp, #32]
   4789c:	bl	c180 <__gmpn_lshift@plt>
   478a0:	mov	x28, x0
   478a4:	mov	x0, x27
   478a8:	mov	x1, x27
   478ac:	mov	x2, x22
   478b0:	mov	x3, x23
   478b4:	bl	c2d0 <__gmpn_sub_n@plt>
   478b8:	add	x8, x27, x20, lsl #3
   478bc:	ldur	x9, [x8, #-8]
   478c0:	add	x10, x0, x28
   478c4:	subs	x9, x9, x10
   478c8:	stur	x9, [x8, #-8]
   478cc:	b.cs	478e0 <__gmpn_toom_interpolate_16pts@@Base+0x4e4>  // b.hs, b.nlast
   478d0:	ldr	x9, [x8]
   478d4:	sub	x10, x9, #0x1
   478d8:	str	x10, [x8], #8
   478dc:	cbz	x9, 478d0 <__gmpn_toom_interpolate_16pts@@Base+0x4d4>
   478e0:	ldur	x26, [x29, #-16]
   478e4:	ldr	x20, [sp, #56]
   478e8:	mov	x0, x22
   478ec:	mov	x3, x21
   478f0:	mov	x1, x26
   478f4:	mov	x2, x20
   478f8:	bl	c2d0 <__gmpn_sub_n@plt>
   478fc:	mov	x0, x20
   47900:	mov	x1, x20
   47904:	mov	x2, x26
   47908:	mov	x3, x21
   4790c:	bl	ca70 <__gmpn_add_n@plt>
   47910:	ldur	x22, [x29, #-8]
   47914:	ldur	x8, [x29, #-48]
   47918:	add	x27, x24, x25
   4791c:	mov	w3, #0xe                   	// #14
   47920:	mov	x0, x26
   47924:	mov	x1, x24
   47928:	mov	x2, x22
   4792c:	add	x28, x27, x8
   47930:	bl	c180 <__gmpn_lshift@plt>
   47934:	mov	x3, x22
   47938:	ldur	x22, [x29, #-48]
   4793c:	mov	x20, x24
   47940:	mov	x24, x0
   47944:	mov	x0, x28
   47948:	mov	x1, x28
   4794c:	mov	x2, x26
   47950:	mov	x26, x27
   47954:	bl	c2d0 <__gmpn_sub_n@plt>
   47958:	ldr	x8, [x27, x25]
   4795c:	ldur	x10, [x29, #-24]
   47960:	add	x9, x0, x24
   47964:	sub	x8, x8, x9
   47968:	add	x10, x10, x22
   4796c:	str	x8, [x27, x25]
   47970:	ldr	x8, [x20]
   47974:	ldr	x9, [x10]
   47978:	str	x10, [sp, #40]
   4797c:	sub	x8, x9, x8, lsr #2
   47980:	str	x8, [x10]
   47984:	ldr	x8, [x20]
   47988:	cmp	x9, x8, lsr #2
   4798c:	b.cs	479ac <__gmpn_toom_interpolate_16pts@@Base+0x5b0>  // b.hs, b.nlast
   47990:	ldur	x8, [x29, #-24]
   47994:	add	x8, x8, x19, lsl #3
   47998:	add	x8, x8, #0x8
   4799c:	ldr	x9, [x8]
   479a0:	sub	x10, x9, #0x1
   479a4:	str	x10, [x8], #8
   479a8:	cbz	x9, 4799c <__gmpn_toom_interpolate_16pts@@Base+0x5a0>
   479ac:	ldur	x20, [x29, #-16]
   479b0:	ldr	x1, [sp, #32]
   479b4:	mov	w3, #0x3e                  	// #62
   479b8:	mov	x2, x23
   479bc:	mov	x0, x20
   479c0:	bl	c180 <__gmpn_lshift@plt>
   479c4:	ldr	x27, [sp, #40]
   479c8:	mov	x24, x0
   479cc:	mov	x2, x20
   479d0:	mov	x3, x23
   479d4:	mov	x0, x27
   479d8:	mov	x1, x27
   479dc:	bl	c2d0 <__gmpn_sub_n@plt>
   479e0:	ldur	x8, [x29, #-8]
   479e4:	add	x10, x0, x24
   479e8:	add	x8, x27, x8, lsl #3
   479ec:	ldur	x9, [x8, #-8]
   479f0:	subs	x9, x9, x10
   479f4:	stur	x9, [x8, #-8]
   479f8:	b.cs	47a0c <__gmpn_toom_interpolate_16pts@@Base+0x610>  // b.hs, b.nlast
   479fc:	ldr	x9, [x8]
   47a00:	sub	x10, x9, #0x1
   47a04:	str	x10, [x8], #8
   47a08:	cbz	x9, 479fc <__gmpn_toom_interpolate_16pts@@Base+0x600>
   47a0c:	ldp	x28, x0, [x29, #-24]
   47a10:	mov	x2, x26
   47a14:	mov	x3, x21
   47a18:	mov	x1, x28
   47a1c:	bl	ca70 <__gmpn_add_n@plt>
   47a20:	mov	x0, x26
   47a24:	mov	x1, x26
   47a28:	mov	x2, x28
   47a2c:	mov	x3, x21
   47a30:	str	x26, [sp, #48]
   47a34:	bl	c2d0 <__gmpn_sub_n@plt>
   47a38:	ldur	x27, [x29, #-40]
   47a3c:	ldur	x24, [x29, #-56]
   47a40:	mov	w3, #0x2a                  	// #42
   47a44:	mov	x0, x28
   47a48:	add	x26, x27, x22
   47a4c:	ldur	x22, [x29, #-8]
   47a50:	mov	x1, x24
   47a54:	mov	x2, x22
   47a58:	bl	c180 <__gmpn_lshift@plt>
   47a5c:	mov	x20, x0
   47a60:	mov	x0, x26
   47a64:	mov	x1, x26
   47a68:	mov	x2, x28
   47a6c:	mov	x3, x22
   47a70:	bl	c2d0 <__gmpn_sub_n@plt>
   47a74:	ldr	x8, [x27, x25]
   47a78:	ldr	x28, [sp, #24]
   47a7c:	ldur	x10, [x29, #-48]
   47a80:	add	x9, x0, x20
   47a84:	sub	x8, x8, x9
   47a88:	str	x8, [x27, x25]
   47a8c:	add	x22, x28, x10
   47a90:	ldr	x8, [x24]
   47a94:	ldr	x9, [x22]
   47a98:	sub	x8, x9, x8, lsr #6
   47a9c:	str	x8, [x22]
   47aa0:	ldr	x8, [x24]
   47aa4:	cmp	x9, x8, lsr #6
   47aa8:	b.cs	47ac4 <__gmpn_toom_interpolate_16pts@@Base+0x6c8>  // b.hs, b.nlast
   47aac:	add	x8, x28, x19, lsl #3
   47ab0:	add	x8, x8, #0x8
   47ab4:	ldr	x9, [x8]
   47ab8:	sub	x10, x9, #0x1
   47abc:	str	x10, [x8], #8
   47ac0:	cbz	x9, 47ab4 <__gmpn_toom_interpolate_16pts@@Base+0x6b8>
   47ac4:	ldur	x26, [x29, #-24]
   47ac8:	ldr	x1, [sp, #32]
   47acc:	mov	w3, #0x3a                  	// #58
   47ad0:	mov	x2, x23
   47ad4:	mov	x0, x26
   47ad8:	bl	c180 <__gmpn_lshift@plt>
   47adc:	mov	x20, x0
   47ae0:	mov	x0, x22
   47ae4:	mov	x1, x22
   47ae8:	mov	x2, x26
   47aec:	mov	x3, x23
   47af0:	bl	c2d0 <__gmpn_sub_n@plt>
   47af4:	ldur	x8, [x29, #-8]
   47af8:	add	x10, x0, x20
   47afc:	add	x8, x22, x8, lsl #3
   47b00:	ldur	x9, [x8, #-8]
   47b04:	subs	x9, x9, x10
   47b08:	stur	x9, [x8, #-8]
   47b0c:	b.cs	47b20 <__gmpn_toom_interpolate_16pts@@Base+0x724>  // b.hs, b.nlast
   47b10:	ldr	x9, [x8]
   47b14:	sub	x10, x9, #0x1
   47b18:	str	x10, [x8], #8
   47b1c:	cbz	x9, 47b10 <__gmpn_toom_interpolate_16pts@@Base+0x714>
   47b20:	ldur	x26, [x29, #-24]
   47b24:	ldur	x20, [x29, #-40]
   47b28:	mov	x2, x28
   47b2c:	mov	x3, x21
   47b30:	mov	x0, x26
   47b34:	mov	x1, x20
   47b38:	bl	c2d0 <__gmpn_sub_n@plt>
   47b3c:	mov	x0, x28
   47b40:	mov	x1, x28
   47b44:	mov	x2, x20
   47b48:	mov	x3, x21
   47b4c:	bl	ca70 <__gmpn_add_n@plt>
   47b50:	mov	w8, #0x38                  	// #56
   47b54:	ldur	x3, [x29, #-8]
   47b58:	madd	x23, x19, x8, x24
   47b5c:	add	x0, x23, x19, lsl #3
   47b60:	mov	x1, x0
   47b64:	mov	x2, x24
   47b68:	bl	c2d0 <__gmpn_sub_n@plt>
   47b6c:	ldr	x8, [x23, x25]
   47b70:	ldur	x20, [x29, #-32]
   47b74:	ldr	x27, [sp, #48]
   47b78:	mov	w3, #0x404                 	// #1028
   47b7c:	sub	x8, x8, x0
   47b80:	mov	x0, x20
   47b84:	mov	x1, x27
   47b88:	mov	x2, x21
   47b8c:	str	x8, [x23, x25]
   47b90:	bl	c9e0 <__gmpn_submul_1@plt>
   47b94:	mov	w3, #0x514                 	// #1300
   47b98:	mov	x0, x26
   47b9c:	mov	x1, x20
   47ba0:	mov	x2, x21
   47ba4:	bl	c9e0 <__gmpn_submul_1@plt>
   47ba8:	mov	w3, #0x1010                	// #4112
   47bac:	movk	w3, #0x10, lsl #16
   47bb0:	mov	x0, x26
   47bb4:	mov	x1, x27
   47bb8:	mov	x2, x21
   47bbc:	bl	c9e0 <__gmpn_submul_1@plt>
   47bc0:	mov	x4, #0x275b                	// #10075
   47bc4:	mov	x3, #0xb0d3                	// #45267
   47bc8:	movk	x4, #0x6864, lsl #16
   47bcc:	movk	x3, #0x313f, lsl #16
   47bd0:	movk	x4, #0x993a, lsl #32
   47bd4:	movk	x3, #0xb, lsl #32
   47bd8:	movk	x4, #0x6db, lsl #48
   47bdc:	mov	x0, x26
   47be0:	mov	x1, x26
   47be4:	mov	x2, x21
   47be8:	mov	w5, wzr
   47bec:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47bf0:	mov	w3, #0xc403                	// #50179
   47bf4:	movk	w3, #0xbf, lsl #16
   47bf8:	mov	x0, x20
   47bfc:	mov	x1, x26
   47c00:	mov	x2, x21
   47c04:	bl	c9e0 <__gmpn_submul_1@plt>
   47c08:	mov	x4, #0x771b                	// #30491
   47c0c:	movk	x4, #0x53e3, lsl #16
   47c10:	movk	x4, #0xc705, lsl #32
   47c14:	mov	w3, #0xb13                 	// #2835
   47c18:	movk	x4, #0x938c, lsl #48
   47c1c:	mov	w5, #0x6                   	// #6
   47c20:	mov	x0, x20
   47c24:	mov	x1, x20
   47c28:	mov	x2, x21
   47c2c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47c30:	ldr	x8, [x20, x25]
   47c34:	lsr	x9, x8, #57
   47c38:	cbz	x9, 47c48 <__gmpn_toom_interpolate_16pts@@Base+0x84c>
   47c3c:	ldr	x9, [sp, #64]
   47c40:	orr	x8, x8, #0xfc00000000000000
   47c44:	str	x8, [x20, x9, lsl #3]
   47c48:	str	x22, [sp, #32]
   47c4c:	ldr	x22, [sp, #48]
   47c50:	ldur	x1, [x29, #-24]
   47c54:	mov	w3, #0xfff                 	// #4095
   47c58:	mov	x2, x21
   47c5c:	mov	x0, x22
   47c60:	bl	c9e0 <__gmpn_submul_1@plt>
   47c64:	mov	w3, #0xf0                  	// #240
   47c68:	mov	x0, x22
   47c6c:	mov	x1, x20
   47c70:	mov	x2, x21
   47c74:	bl	d400 <__gmpn_addmul_1@plt>
   47c78:	mov	x4, #0xfefefefefefefefe    	// #-72340172838076674
   47c7c:	mov	w3, #0xff                  	// #255
   47c80:	movk	x4, #0xfeff
   47c84:	mov	w5, #0x2                   	// #2
   47c88:	mov	x0, x22
   47c8c:	mov	x1, x22
   47c90:	mov	x2, x21
   47c94:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47c98:	ldr	x8, [sp, #64]
   47c9c:	ldur	x27, [x29, #-48]
   47ca0:	ldr	x8, [x22, x8, lsl #3]
   47ca4:	lsr	x9, x8, #61
   47ca8:	cbz	x9, 47cb8 <__gmpn_toom_interpolate_16pts@@Base+0x8bc>
   47cac:	ldr	x9, [sp, #64]
   47cb0:	orr	x8, x8, #0xc000000000000000
   47cb4:	str	x8, [x22, x9, lsl #3]
   47cb8:	ldur	x26, [x29, #-40]
   47cbc:	mov	w3, #0x7                   	// #7
   47cc0:	mov	x1, x23
   47cc4:	mov	x2, x21
   47cc8:	mov	x0, x26
   47ccc:	bl	c180 <__gmpn_lshift@plt>
   47cd0:	ldur	x20, [x29, #-16]
   47cd4:	mov	x2, x26
   47cd8:	mov	x3, x21
   47cdc:	mov	x0, x20
   47ce0:	mov	x1, x20
   47ce4:	bl	c2d0 <__gmpn_sub_n@plt>
   47ce8:	mov	w3, #0xd                   	// #13
   47cec:	mov	x0, x26
   47cf0:	mov	x1, x23
   47cf4:	mov	x2, x21
   47cf8:	bl	c180 <__gmpn_lshift@plt>
   47cfc:	ldr	x22, [sp, #56]
   47d00:	mov	x2, x26
   47d04:	mov	x3, x21
   47d08:	mov	x0, x22
   47d0c:	mov	x1, x22
   47d10:	bl	c2d0 <__gmpn_sub_n@plt>
   47d14:	mov	w3, #0x190                 	// #400
   47d18:	mov	x0, x22
   47d1c:	mov	x1, x20
   47d20:	mov	x2, x21
   47d24:	bl	c9e0 <__gmpn_submul_1@plt>
   47d28:	mov	w3, #0x13                  	// #19
   47d2c:	mov	x0, x26
   47d30:	mov	x1, x23
   47d34:	mov	x2, x21
   47d38:	bl	c180 <__gmpn_lshift@plt>
   47d3c:	mov	x2, x26
   47d40:	ldr	x26, [sp, #48]
   47d44:	mov	x0, x28
   47d48:	mov	x1, x28
   47d4c:	mov	x3, x21
   47d50:	bl	c2d0 <__gmpn_sub_n@plt>
   47d54:	mov	w3, #0x594                 	// #1428
   47d58:	mov	x0, x28
   47d5c:	mov	x1, x22
   47d60:	mov	x2, x21
   47d64:	bl	c9e0 <__gmpn_submul_1@plt>
   47d68:	mov	w3, #0xb900                	// #47360
   47d6c:	movk	w3, #0x1, lsl #16
   47d70:	mov	x0, x28
   47d74:	mov	x1, x20
   47d78:	mov	x2, x21
   47d7c:	bl	c9e0 <__gmpn_submul_1@plt>
   47d80:	mov	x4, #0xcb25                	// #52005
   47d84:	mov	x3, #0x58ad                	// #22701
   47d88:	movk	x4, #0x6fc4, lsl #16
   47d8c:	movk	x3, #0xd916, lsl #16
   47d90:	movk	x4, #0x9a07, lsl #32
   47d94:	movk	x3, #0xa, lsl #32
   47d98:	movk	x4, #0x1b64, lsl #48
   47d9c:	mov	x0, x28
   47da0:	mov	x1, x28
   47da4:	mov	x2, x21
   47da8:	mov	w5, wzr
   47dac:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47db0:	mov	w3, #0xa671                	// #42609
   47db4:	movk	w3, #0xe7, lsl #16
   47db8:	mov	x0, x22
   47dbc:	mov	x1, x28
   47dc0:	mov	x2, x21
   47dc4:	bl	c9e0 <__gmpn_submul_1@plt>
   47dc8:	mov	x4, #0x4c35                	// #19509
   47dcc:	movk	x4, #0x9f31, lsl #16
   47dd0:	movk	x4, #0xd44, lsl #32
   47dd4:	mov	w3, #0xa61d                	// #42525
   47dd8:	movk	x4, #0xe7b4, lsl #48
   47ddc:	mov	w5, #0x4                   	// #4
   47de0:	mov	x0, x22
   47de4:	mov	x1, x22
   47de8:	mov	x2, x21
   47dec:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47df0:	mov	w3, #0xf81                 	// #3969
   47df4:	mov	x0, x20
   47df8:	mov	x1, x28
   47dfc:	mov	x2, x21
   47e00:	bl	c9e0 <__gmpn_submul_1@plt>
   47e04:	mov	w3, #0x384                 	// #900
   47e08:	mov	x0, x20
   47e0c:	mov	x1, x22
   47e10:	mov	x2, x21
   47e14:	bl	c9e0 <__gmpn_submul_1@plt>
   47e18:	mov	x4, #0x8e39                	// #36409
   47e1c:	movk	x4, #0x38e3, lsl #16
   47e20:	movk	x4, #0xe38e, lsl #32
   47e24:	mov	w3, #0x9                   	// #9
   47e28:	movk	x4, #0x8e38, lsl #48
   47e2c:	mov	w5, #0x4                   	// #4
   47e30:	mov	x0, x20
   47e34:	mov	x1, x20
   47e38:	mov	x2, x21
   47e3c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   47e40:	mov	x0, x23
   47e44:	mov	x1, x23
   47e48:	mov	x2, x28
   47e4c:	mov	x3, x21
   47e50:	bl	c2d0 <__gmpn_sub_n@plt>
   47e54:	mov	x0, x23
   47e58:	mov	x1, x23
   47e5c:	mov	x2, x20
   47e60:	mov	x3, x21
   47e64:	bl	c2d0 <__gmpn_sub_n@plt>
   47e68:	mov	x0, x23
   47e6c:	mov	x1, x23
   47e70:	mov	x2, x22
   47e74:	mov	x3, x21
   47e78:	bl	c2d0 <__gmpn_sub_n@plt>
   47e7c:	mov	x0, x26
   47e80:	mov	x1, x22
   47e84:	mov	x2, x26
   47e88:	mov	x3, x21
   47e8c:	bl	c950 <__gmpn_rsh1add_n@plt>
   47e90:	ldr	x8, [x26, x25]
   47e94:	mov	x0, x22
   47e98:	mov	x1, x22
   47e9c:	mov	x2, x26
   47ea0:	and	x8, x8, #0x7fffffffffffffff
   47ea4:	mov	x3, x21
   47ea8:	str	x8, [x26, x25]
   47eac:	bl	c2d0 <__gmpn_sub_n@plt>
   47eb0:	ldur	x22, [x29, #-32]
   47eb4:	mov	x1, x20
   47eb8:	mov	x3, x21
   47ebc:	mov	x0, x22
   47ec0:	mov	x2, x22
   47ec4:	bl	c840 <__gmpn_rsh1sub_n@plt>
   47ec8:	ldr	x8, [x22, x25]
   47ecc:	mov	x0, x20
   47ed0:	mov	x1, x20
   47ed4:	mov	x2, x22
   47ed8:	and	x8, x8, #0x7fffffffffffffff
   47edc:	mov	x3, x21
   47ee0:	str	x8, [x22, x25]
   47ee4:	bl	c2d0 <__gmpn_sub_n@plt>
   47ee8:	ldur	x20, [x29, #-24]
   47eec:	mov	x1, x28
   47ef0:	mov	x3, x21
   47ef4:	mov	x0, x20
   47ef8:	mov	x2, x20
   47efc:	bl	c950 <__gmpn_rsh1add_n@plt>
   47f00:	ldr	x8, [x20, x25]
   47f04:	mov	x0, x28
   47f08:	mov	x1, x28
   47f0c:	mov	x2, x20
   47f10:	and	x8, x8, #0x7fffffffffffffff
   47f14:	mov	x3, x21
   47f18:	str	x8, [x20, x25]
   47f1c:	bl	c2d0 <__gmpn_sub_n@plt>
   47f20:	add	x0, x24, x27
   47f24:	mov	x1, x0
   47f28:	mov	x2, x20
   47f2c:	mov	x3, x19
   47f30:	bl	ca70 <__gmpn_add_n@plt>
   47f34:	ldr	x8, [x20, x27]
   47f38:	ldur	x9, [x29, #-8]
   47f3c:	adds	x8, x8, x0
   47f40:	add	x9, x24, x9, lsl #3
   47f44:	str	x8, [x9]
   47f48:	b.cc	4804c <__gmpn_toom_interpolate_16pts@@Base+0xc50>  // b.lo, b.ul, b.last
   47f4c:	ldr	x21, [sp, #16]
   47f50:	ldp	x22, x15, [sp, #32]
   47f54:	mov	x11, xzr
   47f58:	sub	x10, x19, #0x1
   47f5c:	mov	w4, #0x1                   	// #1
   47f60:	mov	w8, #0x1                   	// #1
   47f64:	cmp	x8, x19
   47f68:	b.ge	480e8 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.tcont
   47f6c:	add	x12, x15, x11
   47f70:	ldr	x12, [x12, #8]
   47f74:	add	x13, x9, x11
   47f78:	add	x8, x8, #0x1
   47f7c:	add	x11, x11, #0x8
   47f80:	adds	x12, x12, #0x1
   47f84:	sub	x10, x10, #0x1
   47f88:	str	x12, [x13, #8]
   47f8c:	b.cs	47f64 <__gmpn_toom_interpolate_16pts@@Base+0xb68>  // b.hs, b.nlast
   47f90:	cmp	x15, x9
   47f94:	mov	x4, xzr
   47f98:	b.eq	480e8 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.none
   47f9c:	cmp	x8, x19
   47fa0:	b.ge	480e8 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.tcont
   47fa4:	sub	x12, x19, x8
   47fa8:	cmp	x12, #0x4
   47fac:	b.cc	48020 <__gmpn_toom_interpolate_16pts@@Base+0xc24>  // b.lo, b.ul, b.last
   47fb0:	ldur	x14, [x29, #-24]
   47fb4:	add	x13, x9, x11
   47fb8:	add	x13, x13, #0x8
   47fbc:	add	x14, x14, x19, lsl #4
   47fc0:	cmp	x13, x14
   47fc4:	b.cs	47fe0 <__gmpn_toom_interpolate_16pts@@Base+0xbe4>  // b.hs, b.nlast
   47fc8:	mov	w13, #0x18                  	// #24
   47fcc:	add	x14, x15, x11
   47fd0:	madd	x13, x19, x13, x24
   47fd4:	add	x14, x14, #0x8
   47fd8:	cmp	x14, x13
   47fdc:	b.cc	48020 <__gmpn_toom_interpolate_16pts@@Base+0xc24>  // b.lo, b.ul, b.last
   47fe0:	add	x13, x9, x11
   47fe4:	add	x11, x15, x11
   47fe8:	and	x9, x12, #0xfffffffffffffffc
   47fec:	and	x14, x10, #0xfffffffffffffffc
   47ff0:	add	x10, x13, #0x18
   47ff4:	add	x11, x11, #0x18
   47ff8:	add	x8, x14, x8
   47ffc:	mov	x13, x9
   48000:	ldp	q0, q1, [x11, #-16]
   48004:	add	x11, x11, #0x20
   48008:	subs	x13, x13, #0x4
   4800c:	stp	q0, q1, [x10, #-16]
   48010:	add	x10, x10, #0x20
   48014:	b.ne	48000 <__gmpn_toom_interpolate_16pts@@Base+0xc04>  // b.any
   48018:	cmp	x12, x9
   4801c:	b.eq	480dc <__gmpn_toom_interpolate_16pts@@Base+0xce0>  // b.none
   48020:	add	x10, x8, x19, lsl #1
   48024:	sub	x9, x19, x8
   48028:	add	x11, x8, x19
   4802c:	add	x8, x24, x10, lsl #3
   48030:	ldur	x10, [x29, #-24]
   48034:	add	x10, x10, x11, lsl #3
   48038:	ldr	x11, [x10], #8
   4803c:	subs	x9, x9, #0x1
   48040:	str	x11, [x8], #8
   48044:	b.ne	48038 <__gmpn_toom_interpolate_16pts@@Base+0xc3c>  // b.any
   48048:	b	480dc <__gmpn_toom_interpolate_16pts@@Base+0xce0>
   4804c:	ldr	x21, [sp, #16]
   48050:	ldr	x8, [sp, #40]
   48054:	cmp	x19, #0x2
   48058:	mov	x4, xzr
   4805c:	b.lt	480e4 <__gmpn_toom_interpolate_16pts@@Base+0xce8>  // b.tstop
   48060:	ldr	x22, [sp, #32]
   48064:	cmp	x8, x9
   48068:	b.eq	480e8 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.none
   4806c:	sub	x8, x19, #0x1
   48070:	cmp	x8, #0x4
   48074:	b.cc	480b0 <__gmpn_toom_interpolate_16pts@@Base+0xcb4>  // b.lo, b.ul, b.last
   48078:	ldur	x11, [x29, #-24]
   4807c:	mov	w9, #0x8                   	// #8
   48080:	lsl	x10, x19, #4
   48084:	bfi	x9, x19, #4, #60
   48088:	add	x9, x24, x9
   4808c:	add	x10, x11, x10
   48090:	cmp	x9, x10
   48094:	add	x9, x11, x19, lsl #3
   48098:	b.cs	48934 <__gmpn_toom_interpolate_16pts@@Base+0x1538>  // b.hs, b.nlast
   4809c:	mov	w10, #0x18                  	// #24
   480a0:	madd	x10, x19, x10, x24
   480a4:	add	x11, x9, #0x8
   480a8:	cmp	x11, x10
   480ac:	b.cs	48934 <__gmpn_toom_interpolate_16pts@@Base+0x1538>  // b.hs, b.nlast
   480b0:	mov	w9, #0x1                   	// #1
   480b4:	add	x10, x9, x19, lsl #1
   480b8:	sub	x8, x19, x9
   480bc:	add	x11, x9, x19
   480c0:	add	x9, x24, x10, lsl #3
   480c4:	ldur	x10, [x29, #-24]
   480c8:	add	x10, x10, x11, lsl #3
   480cc:	ldr	x11, [x10], #8
   480d0:	subs	x8, x8, #0x1
   480d4:	str	x11, [x9], #8
   480d8:	b.ne	480cc <__gmpn_toom_interpolate_16pts@@Base+0xcd0>  // b.any
   480dc:	mov	x4, xzr
   480e0:	b	480e8 <__gmpn_toom_interpolate_16pts@@Base+0xcec>
   480e4:	ldr	x22, [sp, #32]
   480e8:	ldr	x25, [sp, #64]
   480ec:	ldur	x9, [x29, #-24]
   480f0:	ldur	x8, [x29, #-8]
   480f4:	mov	x0, x26
   480f8:	mov	x1, x26
   480fc:	ldr	x20, [x9, x25, lsl #3]
   48100:	add	x2, x9, x8, lsl #3
   48104:	mov	x3, x19
   48108:	bl	ce90 <__gmpn_add_nc@plt>
   4810c:	lsl	x8, x19, #5
   48110:	ldr	x9, [x24, x8]
   48114:	add	x10, x0, x20
   48118:	adds	x9, x9, x10
   4811c:	str	x9, [x24, x8]
   48120:	b.cc	4813c <__gmpn_toom_interpolate_16pts@@Base+0xd40>  // b.lo, b.ul, b.last
   48124:	add	x8, x24, x19, lsl #5
   48128:	add	x8, x8, #0x8
   4812c:	ldr	x9, [x8]
   48130:	adds	x9, x9, #0x1
   48134:	str	x9, [x8], #8
   48138:	b.cs	4812c <__gmpn_toom_interpolate_16pts@@Base+0xd30>  // b.hs, b.nlast
   4813c:	ldur	x20, [x29, #-32]
   48140:	mov	w8, #0x28                  	// #40
   48144:	madd	x0, x19, x8, x24
   48148:	mov	x1, x0
   4814c:	mov	x2, x20
   48150:	mov	x3, x19
   48154:	bl	ca70 <__gmpn_add_n@plt>
   48158:	add	x8, x19, x19, lsl #1
   4815c:	add	x10, x24, x8, lsl #4
   48160:	ldr	x9, [x10]
   48164:	add	x11, x20, x19, lsl #3
   48168:	lsl	x8, x8, #1
   4816c:	add	x9, x9, x0
   48170:	str	x9, [x10]
   48174:	ldr	x12, [x11]
   48178:	adds	x9, x12, x9
   4817c:	str	x9, [x10]
   48180:	b.cc	4827c <__gmpn_toom_interpolate_16pts@@Base+0xe80>  // b.lo, b.ul, b.last
   48184:	mov	x13, xzr
   48188:	sub	x12, x19, #0x1
   4818c:	mov	w4, #0x1                   	// #1
   48190:	mov	w9, #0x1                   	// #1
   48194:	cmp	x9, x19
   48198:	b.ge	48300 <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tcont
   4819c:	add	x14, x11, x13
   481a0:	ldr	x14, [x14, #8]
   481a4:	add	x15, x10, x13
   481a8:	add	x9, x9, #0x1
   481ac:	add	x13, x13, #0x8
   481b0:	adds	x14, x14, #0x1
   481b4:	sub	x12, x12, #0x1
   481b8:	str	x14, [x15, #8]
   481bc:	b.cs	48194 <__gmpn_toom_interpolate_16pts@@Base+0xd98>  // b.hs, b.nlast
   481c0:	cmp	x11, x10
   481c4:	mov	x4, xzr
   481c8:	b.eq	48300 <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.none
   481cc:	cmp	x9, x19
   481d0:	b.ge	48300 <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tcont
   481d4:	sub	x14, x19, x9
   481d8:	cmp	x14, #0x4
   481dc:	b.cc	48250 <__gmpn_toom_interpolate_16pts@@Base+0xe54>  // b.lo, b.ul, b.last
   481e0:	ldur	x16, [x29, #-32]
   481e4:	add	x15, x10, x13
   481e8:	add	x15, x15, #0x8
   481ec:	add	x16, x16, x19, lsl #4
   481f0:	cmp	x15, x16
   481f4:	b.cs	48210 <__gmpn_toom_interpolate_16pts@@Base+0xe14>  // b.hs, b.nlast
   481f8:	mov	w15, #0x38                  	// #56
   481fc:	add	x16, x11, x13
   48200:	madd	x15, x19, x15, x24
   48204:	add	x16, x16, #0x8
   48208:	cmp	x16, x15
   4820c:	b.cc	48250 <__gmpn_toom_interpolate_16pts@@Base+0xe54>  // b.lo, b.ul, b.last
   48210:	add	x15, x10, x13
   48214:	add	x13, x11, x13
   48218:	and	x10, x14, #0xfffffffffffffffc
   4821c:	and	x16, x12, #0xfffffffffffffffc
   48220:	add	x11, x15, #0x18
   48224:	add	x12, x13, #0x18
   48228:	add	x9, x16, x9
   4822c:	mov	x13, x10
   48230:	ldp	q0, q1, [x12, #-16]
   48234:	add	x12, x12, #0x20
   48238:	subs	x13, x13, #0x4
   4823c:	stp	q0, q1, [x11, #-16]
   48240:	add	x11, x11, #0x20
   48244:	b.ne	48230 <__gmpn_toom_interpolate_16pts@@Base+0xe34>  // b.any
   48248:	cmp	x14, x10
   4824c:	b.eq	482fc <__gmpn_toom_interpolate_16pts@@Base+0xf00>  // b.none
   48250:	ldur	x11, [x29, #-32]
   48254:	sub	x10, x19, x9
   48258:	add	x8, x9, x8
   4825c:	add	x9, x9, x19
   48260:	add	x8, x24, x8, lsl #3
   48264:	add	x9, x11, x9, lsl #3
   48268:	ldr	x11, [x9], #8
   4826c:	subs	x10, x10, #0x1
   48270:	str	x11, [x8], #8
   48274:	b.ne	48268 <__gmpn_toom_interpolate_16pts@@Base+0xe6c>  // b.any
   48278:	b	482fc <__gmpn_toom_interpolate_16pts@@Base+0xf00>
   4827c:	cmp	x19, #0x2
   48280:	mov	x4, xzr
   48284:	b.lt	48300 <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tstop
   48288:	cmp	x11, x10
   4828c:	b.eq	48300 <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.none
   48290:	sub	x9, x19, #0x1
   48294:	cmp	x9, #0x4
   48298:	b.cc	482d0 <__gmpn_toom_interpolate_16pts@@Base+0xed4>  // b.lo, b.ul, b.last
   4829c:	ldur	x12, [x29, #-32]
   482a0:	lsl	x10, x8, #3
   482a4:	orr	x10, x10, #0x8
   482a8:	add	x10, x24, x10
   482ac:	add	x11, x12, x19, lsl #4
   482b0:	cmp	x10, x11
   482b4:	add	x10, x12, x19, lsl #3
   482b8:	b.cs	48970 <__gmpn_toom_interpolate_16pts@@Base+0x1574>  // b.hs, b.nlast
   482bc:	mov	w11, #0x38                  	// #56
   482c0:	madd	x11, x19, x11, x24
   482c4:	add	x12, x10, #0x8
   482c8:	cmp	x12, x11
   482cc:	b.cs	48970 <__gmpn_toom_interpolate_16pts@@Base+0x1574>  // b.hs, b.nlast
   482d0:	mov	w10, #0x1                   	// #1
   482d4:	ldur	x11, [x29, #-32]
   482d8:	sub	x9, x19, x10
   482dc:	add	x8, x10, x8
   482e0:	add	x10, x10, x19
   482e4:	add	x8, x24, x8, lsl #3
   482e8:	add	x10, x11, x10, lsl #3
   482ec:	ldr	x11, [x10], #8
   482f0:	subs	x9, x9, #0x1
   482f4:	str	x11, [x8], #8
   482f8:	b.ne	482ec <__gmpn_toom_interpolate_16pts@@Base+0xef0>  // b.any
   482fc:	mov	x4, xzr
   48300:	ldur	x9, [x29, #-32]
   48304:	ldur	x8, [x29, #-8]
   48308:	mov	x0, x23
   4830c:	mov	x1, x23
   48310:	ldr	x20, [x9, x25, lsl #3]
   48314:	add	x2, x9, x8, lsl #3
   48318:	mov	x3, x19
   4831c:	bl	ce90 <__gmpn_add_nc@plt>
   48320:	lsl	x8, x19, #6
   48324:	ldr	x9, [x24, x8]
   48328:	add	x10, x0, x20
   4832c:	adds	x9, x9, x10
   48330:	str	x9, [x24, x8]
   48334:	b.cc	48350 <__gmpn_toom_interpolate_16pts@@Base+0xf54>  // b.lo, b.ul, b.last
   48338:	add	x8, x24, x27, lsl #3
   4833c:	add	x8, x8, #0x8
   48340:	ldr	x9, [x8]
   48344:	adds	x9, x9, #0x1
   48348:	str	x9, [x8], #8
   4834c:	b.cs	48340 <__gmpn_toom_interpolate_16pts@@Base+0xf44>  // b.hs, b.nlast
   48350:	ldur	x20, [x29, #-16]
   48354:	mov	w8, #0x48                  	// #72
   48358:	madd	x0, x19, x8, x24
   4835c:	mov	x1, x0
   48360:	mov	x2, x20
   48364:	mov	x3, x19
   48368:	bl	ca70 <__gmpn_add_n@plt>
   4836c:	mov	w8, #0x50                  	// #80
   48370:	madd	x9, x19, x8, x24
   48374:	ldr	x8, [x9]
   48378:	add	x8, x8, x0
   4837c:	str	x8, [x9]
   48380:	ldr	x10, [x20, x19, lsl #3]
   48384:	adds	x8, x10, x8
   48388:	str	x8, [x9]
   4838c:	b.cc	48490 <__gmpn_toom_interpolate_16pts@@Base+0x1094>  // b.lo, b.ul, b.last
   48390:	ldr	x15, [sp]
   48394:	mov	x11, xzr
   48398:	sub	x10, x19, #0x1
   4839c:	mov	w4, #0x1                   	// #1
   483a0:	mov	w8, #0x1                   	// #1
   483a4:	cmp	x8, x19
   483a8:	b.ge	48520 <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tcont
   483ac:	add	x12, x15, x11
   483b0:	ldr	x12, [x12, #8]
   483b4:	add	x13, x9, x11
   483b8:	add	x8, x8, #0x1
   483bc:	add	x11, x11, #0x8
   483c0:	adds	x12, x12, #0x1
   483c4:	sub	x10, x10, #0x1
   483c8:	str	x12, [x13, #8]
   483cc:	b.cs	483a4 <__gmpn_toom_interpolate_16pts@@Base+0xfa8>  // b.hs, b.nlast
   483d0:	cmp	x15, x9
   483d4:	mov	x4, xzr
   483d8:	b.eq	48520 <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.none
   483dc:	cmp	x8, x19
   483e0:	b.ge	48520 <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tcont
   483e4:	sub	x12, x19, x8
   483e8:	cmp	x12, #0x4
   483ec:	b.cc	48460 <__gmpn_toom_interpolate_16pts@@Base+0x1064>  // b.lo, b.ul, b.last
   483f0:	ldur	x14, [x29, #-16]
   483f4:	add	x13, x9, x11
   483f8:	add	x13, x13, #0x8
   483fc:	add	x14, x14, x19, lsl #4
   48400:	cmp	x13, x14
   48404:	b.cs	48420 <__gmpn_toom_interpolate_16pts@@Base+0x1024>  // b.hs, b.nlast
   48408:	mov	w13, #0x58                  	// #88
   4840c:	add	x14, x15, x11
   48410:	madd	x13, x19, x13, x24
   48414:	add	x14, x14, #0x8
   48418:	cmp	x14, x13
   4841c:	b.cc	48460 <__gmpn_toom_interpolate_16pts@@Base+0x1064>  // b.lo, b.ul, b.last
   48420:	add	x13, x9, x11
   48424:	add	x11, x15, x11
   48428:	and	x9, x12, #0xfffffffffffffffc
   4842c:	and	x14, x10, #0xfffffffffffffffc
   48430:	add	x10, x13, #0x18
   48434:	add	x11, x11, #0x18
   48438:	add	x8, x14, x8
   4843c:	mov	x13, x9
   48440:	ldp	q0, q1, [x11, #-16]
   48444:	add	x11, x11, #0x20
   48448:	subs	x13, x13, #0x4
   4844c:	stp	q0, q1, [x10, #-16]
   48450:	add	x10, x10, #0x20
   48454:	b.ne	48440 <__gmpn_toom_interpolate_16pts@@Base+0x1044>  // b.any
   48458:	cmp	x12, x9
   4845c:	b.eq	4851c <__gmpn_toom_interpolate_16pts@@Base+0x1120>  // b.none
   48460:	mov	w10, #0xa                   	// #10
   48464:	sub	x9, x19, x8
   48468:	add	x11, x8, x19
   4846c:	madd	x8, x19, x10, x8
   48470:	ldur	x10, [x29, #-16]
   48474:	add	x8, x24, x8, lsl #3
   48478:	add	x10, x10, x11, lsl #3
   4847c:	ldr	x11, [x10], #8
   48480:	subs	x9, x9, #0x1
   48484:	str	x11, [x8], #8
   48488:	b.ne	4847c <__gmpn_toom_interpolate_16pts@@Base+0x1080>  // b.any
   4848c:	b	4851c <__gmpn_toom_interpolate_16pts@@Base+0x1120>
   48490:	ldr	x8, [sp]
   48494:	cmp	x19, #0x2
   48498:	mov	x4, xzr
   4849c:	b.lt	48520 <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tstop
   484a0:	cmp	x8, x9
   484a4:	b.eq	48520 <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.none
   484a8:	sub	x8, x19, #0x1
   484ac:	cmp	x8, #0x4
   484b0:	b.cc	484ec <__gmpn_toom_interpolate_16pts@@Base+0x10f0>  // b.lo, b.ul, b.last
   484b4:	ldur	x11, [x29, #-16]
   484b8:	add	x9, x19, x19, lsl #2
   484bc:	mov	w10, #0x8                   	// #8
   484c0:	bfi	x10, x9, #4, #60
   484c4:	add	x9, x24, x10
   484c8:	add	x10, x11, x19, lsl #4
   484cc:	cmp	x9, x10
   484d0:	add	x9, x11, x19, lsl #3
   484d4:	b.cs	489ac <__gmpn_toom_interpolate_16pts@@Base+0x15b0>  // b.hs, b.nlast
   484d8:	mov	w10, #0x58                  	// #88
   484dc:	madd	x10, x19, x10, x24
   484e0:	add	x11, x9, #0x8
   484e4:	cmp	x11, x10
   484e8:	b.cs	489ac <__gmpn_toom_interpolate_16pts@@Base+0x15b0>  // b.hs, b.nlast
   484ec:	mov	w9, #0x1                   	// #1
   484f0:	mov	w10, #0xa                   	// #10
   484f4:	sub	x8, x19, x9
   484f8:	add	x11, x9, x19
   484fc:	madd	x9, x19, x10, x9
   48500:	ldur	x10, [x29, #-16]
   48504:	add	x9, x24, x9, lsl #3
   48508:	add	x10, x10, x11, lsl #3
   4850c:	ldr	x11, [x10], #8
   48510:	subs	x8, x8, #0x1
   48514:	str	x11, [x9], #8
   48518:	b.ne	4850c <__gmpn_toom_interpolate_16pts@@Base+0x1110>  // b.any
   4851c:	mov	x4, xzr
   48520:	ldp	x8, x9, [x29, #-16]
   48524:	ldr	x0, [sp, #56]
   48528:	mov	x3, x19
   4852c:	ldr	x20, [x8, x25, lsl #3]
   48530:	add	x2, x8, x9, lsl #3
   48534:	mov	x1, x0
   48538:	bl	ce90 <__gmpn_add_nc@plt>
   4853c:	mov	w8, #0x60                  	// #96
   48540:	mul	x8, x19, x8
   48544:	ldr	x9, [x24, x8]
   48548:	add	x10, x0, x20
   4854c:	adds	x9, x9, x10
   48550:	str	x9, [x24, x8]
   48554:	b.cc	48574 <__gmpn_toom_interpolate_16pts@@Base+0x1178>  // b.lo, b.ul, b.last
   48558:	mov	w8, #0x60                  	// #96
   4855c:	madd	x8, x19, x8, x24
   48560:	add	x8, x8, #0x8
   48564:	ldr	x9, [x8]
   48568:	adds	x9, x9, #0x1
   4856c:	str	x9, [x8], #8
   48570:	b.cs	48564 <__gmpn_toom_interpolate_16pts@@Base+0x1168>  // b.hs, b.nlast
   48574:	mov	w8, #0x68                  	// #104
   48578:	madd	x0, x19, x8, x24
   4857c:	mov	x1, x0
   48580:	mov	x2, x28
   48584:	mov	x3, x19
   48588:	bl	ca70 <__gmpn_add_n@plt>
   4858c:	mov	w8, #0x70                  	// #112
   48590:	madd	x8, x19, x8, x24
   48594:	ldr	x9, [x8]
   48598:	ldr	w11, [sp, #12]
   4859c:	add	x9, x9, x0
   485a0:	str	x9, [x8]
   485a4:	ldr	x10, [x28, x19, lsl #3]
   485a8:	add	x10, x10, x9
   485ac:	str	x10, [x8]
   485b0:	cbz	w11, 486b0 <__gmpn_toom_interpolate_16pts@@Base+0x12b4>
   485b4:	cmp	x10, x9
   485b8:	b.cs	487ac <__gmpn_toom_interpolate_16pts@@Base+0x13b0>  // b.hs, b.nlast
   485bc:	mov	x11, xzr
   485c0:	sub	x10, x19, #0x1
   485c4:	mov	w4, #0x1                   	// #1
   485c8:	mov	w9, #0x1                   	// #1
   485cc:	cmp	x9, x19
   485d0:	b.ge	48830 <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tcont
   485d4:	add	x12, x22, x11
   485d8:	ldr	x12, [x12, #8]
   485dc:	add	x13, x8, x11
   485e0:	add	x9, x9, #0x1
   485e4:	add	x11, x11, #0x8
   485e8:	adds	x12, x12, #0x1
   485ec:	sub	x10, x10, #0x1
   485f0:	str	x12, [x13, #8]
   485f4:	b.cs	485cc <__gmpn_toom_interpolate_16pts@@Base+0x11d0>  // b.hs, b.nlast
   485f8:	cmp	x22, x8
   485fc:	mov	x4, xzr
   48600:	b.eq	48830 <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.none
   48604:	cmp	x9, x19
   48608:	b.ge	48830 <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tcont
   4860c:	sub	x12, x19, x9
   48610:	cmp	x12, #0x4
   48614:	b.cc	48684 <__gmpn_toom_interpolate_16pts@@Base+0x1288>  // b.lo, b.ul, b.last
   48618:	add	x13, x8, x11
   4861c:	add	x13, x13, #0x8
   48620:	add	x14, x28, x19, lsl #4
   48624:	cmp	x13, x14
   48628:	b.cs	48644 <__gmpn_toom_interpolate_16pts@@Base+0x1248>  // b.hs, b.nlast
   4862c:	mov	w13, #0x78                  	// #120
   48630:	add	x14, x22, x11
   48634:	madd	x13, x19, x13, x24
   48638:	add	x14, x14, #0x8
   4863c:	cmp	x14, x13
   48640:	b.cc	48684 <__gmpn_toom_interpolate_16pts@@Base+0x1288>  // b.lo, b.ul, b.last
   48644:	add	x13, x8, x11
   48648:	add	x11, x22, x11
   4864c:	and	x8, x12, #0xfffffffffffffffc
   48650:	and	x14, x10, #0xfffffffffffffffc
   48654:	add	x10, x13, #0x18
   48658:	add	x11, x11, #0x18
   4865c:	add	x9, x14, x9
   48660:	mov	x13, x8
   48664:	ldp	q0, q1, [x11, #-16]
   48668:	add	x11, x11, #0x20
   4866c:	subs	x13, x13, #0x4
   48670:	stp	q0, q1, [x10, #-16]
   48674:	add	x10, x10, #0x20
   48678:	b.ne	48664 <__gmpn_toom_interpolate_16pts@@Base+0x1268>  // b.any
   4867c:	cmp	x12, x8
   48680:	b.eq	4882c <__gmpn_toom_interpolate_16pts@@Base+0x1430>  // b.none
   48684:	mov	w10, #0xe                   	// #14
   48688:	sub	x8, x19, x9
   4868c:	add	x11, x9, x19
   48690:	madd	x9, x19, x10, x9
   48694:	add	x9, x24, x9, lsl #3
   48698:	add	x10, x28, x11, lsl #3
   4869c:	ldr	x11, [x10], #8
   486a0:	subs	x8, x8, #0x1
   486a4:	str	x11, [x9], #8
   486a8:	b.ne	4869c <__gmpn_toom_interpolate_16pts@@Base+0x12a0>  // b.any
   486ac:	b	4882c <__gmpn_toom_interpolate_16pts@@Base+0x1430>
   486b0:	cmp	x10, x9
   486b4:	b.cs	4888c <__gmpn_toom_interpolate_16pts@@Base+0x1490>  // b.hs, b.nlast
   486b8:	mov	x11, xzr
   486bc:	sub	x10, x21, #0x1
   486c0:	mov	w9, #0x1                   	// #1
   486c4:	cmp	x9, x21
   486c8:	b.ge	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tcont
   486cc:	add	x12, x22, x11
   486d0:	ldr	x12, [x12, #8]
   486d4:	add	x13, x8, x11
   486d8:	add	x9, x9, #0x1
   486dc:	add	x11, x11, #0x8
   486e0:	adds	x12, x12, #0x1
   486e4:	sub	x10, x10, #0x1
   486e8:	str	x12, [x13, #8]
   486ec:	b.cs	486c4 <__gmpn_toom_interpolate_16pts@@Base+0x12c8>  // b.hs, b.nlast
   486f0:	cmp	x22, x8
   486f4:	b.eq	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   486f8:	cmp	x9, x21
   486fc:	b.ge	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tcont
   48700:	sub	x12, x21, x9
   48704:	cmp	x12, #0x4
   48708:	b.cc	48780 <__gmpn_toom_interpolate_16pts@@Base+0x1384>  // b.lo, b.ul, b.last
   4870c:	add	x13, x8, x11
   48710:	add	x14, x21, x19
   48714:	add	x13, x13, #0x8
   48718:	add	x14, x28, x14, lsl #3
   4871c:	cmp	x13, x14
   48720:	b.cs	48740 <__gmpn_toom_interpolate_16pts@@Base+0x1344>  // b.hs, b.nlast
   48724:	mov	w13, #0xe                   	// #14
   48728:	add	x14, x22, x11
   4872c:	madd	x13, x19, x13, x21
   48730:	add	x13, x24, x13, lsl #3
   48734:	add	x14, x14, #0x8
   48738:	cmp	x14, x13
   4873c:	b.cc	48780 <__gmpn_toom_interpolate_16pts@@Base+0x1384>  // b.lo, b.ul, b.last
   48740:	add	x13, x8, x11
   48744:	add	x11, x22, x11
   48748:	and	x8, x12, #0xfffffffffffffffc
   4874c:	and	x14, x10, #0xfffffffffffffffc
   48750:	add	x10, x13, #0x18
   48754:	add	x11, x11, #0x18
   48758:	add	x9, x14, x9
   4875c:	mov	x13, x8
   48760:	ldp	q0, q1, [x11, #-16]
   48764:	add	x11, x11, #0x20
   48768:	subs	x13, x13, #0x4
   4876c:	stp	q0, q1, [x10, #-16]
   48770:	add	x10, x10, #0x20
   48774:	b.ne	48760 <__gmpn_toom_interpolate_16pts@@Base+0x1364>  // b.any
   48778:	cmp	x12, x8
   4877c:	b.eq	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   48780:	mov	w10, #0xe                   	// #14
   48784:	sub	x8, x21, x9
   48788:	add	x11, x9, x19
   4878c:	madd	x9, x19, x10, x9
   48790:	add	x9, x24, x9, lsl #3
   48794:	add	x10, x28, x11, lsl #3
   48798:	ldr	x11, [x10], #8
   4879c:	subs	x8, x8, #0x1
   487a0:	str	x11, [x9], #8
   487a4:	b.ne	48798 <__gmpn_toom_interpolate_16pts@@Base+0x139c>  // b.any
   487a8:	b	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>
   487ac:	cmp	x19, #0x2
   487b0:	mov	x4, xzr
   487b4:	b.lt	48830 <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tstop
   487b8:	cmp	x22, x8
   487bc:	b.eq	48830 <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.none
   487c0:	sub	x8, x19, #0x1
   487c4:	cmp	x8, #0x4
   487c8:	b.cc	48800 <__gmpn_toom_interpolate_16pts@@Base+0x1404>  // b.lo, b.ul, b.last
   487cc:	mov	w9, #0x70                  	// #112
   487d0:	mul	x9, x19, x9
   487d4:	orr	x9, x9, #0x8
   487d8:	add	x9, x24, x9
   487dc:	add	x10, x28, x19, lsl #4
   487e0:	cmp	x9, x10
   487e4:	add	x9, x28, x19, lsl #3
   487e8:	b.cs	489ec <__gmpn_toom_interpolate_16pts@@Base+0x15f0>  // b.hs, b.nlast
   487ec:	mov	w10, #0x78                  	// #120
   487f0:	madd	x10, x19, x10, x24
   487f4:	add	x11, x9, #0x8
   487f8:	cmp	x11, x10
   487fc:	b.cs	489ec <__gmpn_toom_interpolate_16pts@@Base+0x15f0>  // b.hs, b.nlast
   48800:	mov	w9, #0x1                   	// #1
   48804:	mov	w10, #0xe                   	// #14
   48808:	sub	x8, x19, x9
   4880c:	add	x11, x9, x19
   48810:	madd	x9, x19, x10, x9
   48814:	add	x9, x24, x9, lsl #3
   48818:	add	x10, x28, x11, lsl #3
   4881c:	ldr	x11, [x10], #8
   48820:	subs	x8, x8, #0x1
   48824:	str	x11, [x9], #8
   48828:	b.ne	4881c <__gmpn_toom_interpolate_16pts@@Base+0x1420>  // b.any
   4882c:	mov	x4, xzr
   48830:	cmp	x21, x19
   48834:	b.le	48a6c <__gmpn_toom_interpolate_16pts@@Base+0x1670>
   48838:	mov	w8, #0x78                  	// #120
   4883c:	madd	x0, x19, x8, x24
   48840:	ldur	x8, [x29, #-8]
   48844:	ldr	x20, [x28, x25, lsl #3]
   48848:	mov	x1, x0
   4884c:	mov	x3, x19
   48850:	add	x2, x28, x8, lsl #3
   48854:	bl	ce90 <__gmpn_add_nc@plt>
   48858:	lsl	x8, x19, #7
   4885c:	ldr	x9, [x24, x8]
   48860:	add	x10, x0, x20
   48864:	adds	x9, x9, x10
   48868:	str	x9, [x24, x8]
   4886c:	b.cc	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.lo, b.ul, b.last
   48870:	add	x8, x24, x19, lsl #7
   48874:	add	x8, x8, #0x8
   48878:	ldr	x9, [x8]
   4887c:	adds	x9, x9, #0x1
   48880:	str	x9, [x8], #8
   48884:	b.cs	48878 <__gmpn_toom_interpolate_16pts@@Base+0x147c>  // b.hs, b.nlast
   48888:	b	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>
   4888c:	cmp	x21, #0x2
   48890:	b.lt	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tstop
   48894:	cmp	x22, x8
   48898:	b.eq	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   4889c:	sub	x8, x21, #0x1
   488a0:	cmp	x8, #0x4
   488a4:	b.cc	488e8 <__gmpn_toom_interpolate_16pts@@Base+0x14ec>  // b.lo, b.ul, b.last
   488a8:	mov	w9, #0xe                   	// #14
   488ac:	mul	x10, x19, x9
   488b0:	mov	w11, #0x8                   	// #8
   488b4:	lsr	x9, x10, #1
   488b8:	add	x12, x21, x19
   488bc:	bfi	x11, x9, #4, #60
   488c0:	add	x9, x24, x11
   488c4:	add	x11, x28, x12, lsl #3
   488c8:	cmp	x9, x11
   488cc:	add	x9, x28, x19, lsl #3
   488d0:	b.cs	48a2c <__gmpn_toom_interpolate_16pts@@Base+0x1630>  // b.hs, b.nlast
   488d4:	add	x10, x10, x21
   488d8:	add	x10, x24, x10, lsl #3
   488dc:	add	x11, x9, #0x8
   488e0:	cmp	x11, x10
   488e4:	b.cs	48a2c <__gmpn_toom_interpolate_16pts@@Base+0x1630>  // b.hs, b.nlast
   488e8:	mov	w9, #0x1                   	// #1
   488ec:	mov	w10, #0xe                   	// #14
   488f0:	sub	x8, x21, x9
   488f4:	add	x11, x9, x19
   488f8:	madd	x9, x19, x10, x9
   488fc:	add	x9, x24, x9, lsl #3
   48900:	add	x10, x28, x11, lsl #3
   48904:	ldr	x11, [x10], #8
   48908:	subs	x8, x8, #0x1
   4890c:	str	x11, [x9], #8
   48910:	b.ne	48904 <__gmpn_toom_interpolate_16pts@@Base+0x1508>  // b.any
   48914:	ldp	x20, x19, [sp, #208]
   48918:	ldp	x22, x21, [sp, #192]
   4891c:	ldp	x24, x23, [sp, #176]
   48920:	ldp	x26, x25, [sp, #160]
   48924:	ldp	x28, x27, [sp, #144]
   48928:	ldp	x29, x30, [sp, #128]
   4892c:	add	sp, sp, #0xe0
   48930:	ret
   48934:	and	x10, x8, #0xfffffffffffffffc
   48938:	add	x12, x24, x19, lsl #4
   4893c:	add	x11, x9, #0x18
   48940:	orr	x9, x10, #0x1
   48944:	add	x12, x12, #0x18
   48948:	mov	x13, x10
   4894c:	ldp	q0, q1, [x11, #-16]
   48950:	add	x11, x11, #0x20
   48954:	subs	x13, x13, #0x4
   48958:	stp	q0, q1, [x12, #-16]
   4895c:	add	x12, x12, #0x20
   48960:	b.ne	4894c <__gmpn_toom_interpolate_16pts@@Base+0x1550>  // b.any
   48964:	cmp	x8, x10
   48968:	b.eq	480dc <__gmpn_toom_interpolate_16pts@@Base+0xce0>  // b.none
   4896c:	b	480b4 <__gmpn_toom_interpolate_16pts@@Base+0xcb8>
   48970:	and	x11, x9, #0xfffffffffffffffc
   48974:	add	x13, x24, x8, lsl #3
   48978:	add	x12, x10, #0x18
   4897c:	orr	x10, x11, #0x1
   48980:	add	x13, x13, #0x18
   48984:	mov	x14, x11
   48988:	ldp	q0, q1, [x12, #-16]
   4898c:	add	x12, x12, #0x20
   48990:	subs	x14, x14, #0x4
   48994:	stp	q0, q1, [x13, #-16]
   48998:	add	x13, x13, #0x20
   4899c:	b.ne	48988 <__gmpn_toom_interpolate_16pts@@Base+0x158c>  // b.any
   489a0:	cmp	x9, x11
   489a4:	b.eq	482fc <__gmpn_toom_interpolate_16pts@@Base+0xf00>  // b.none
   489a8:	b	482d4 <__gmpn_toom_interpolate_16pts@@Base+0xed8>
   489ac:	mov	w12, #0x50                  	// #80
   489b0:	and	x10, x8, #0xfffffffffffffffc
   489b4:	madd	x12, x19, x12, x24
   489b8:	add	x11, x9, #0x18
   489bc:	orr	x9, x10, #0x1
   489c0:	add	x12, x12, #0x18
   489c4:	mov	x13, x10
   489c8:	ldp	q0, q1, [x11, #-16]
   489cc:	add	x11, x11, #0x20
   489d0:	subs	x13, x13, #0x4
   489d4:	stp	q0, q1, [x12, #-16]
   489d8:	add	x12, x12, #0x20
   489dc:	b.ne	489c8 <__gmpn_toom_interpolate_16pts@@Base+0x15cc>  // b.any
   489e0:	cmp	x8, x10
   489e4:	b.eq	4851c <__gmpn_toom_interpolate_16pts@@Base+0x1120>  // b.none
   489e8:	b	484f0 <__gmpn_toom_interpolate_16pts@@Base+0x10f4>
   489ec:	mov	w12, #0x70                  	// #112
   489f0:	and	x10, x8, #0xfffffffffffffffc
   489f4:	madd	x12, x19, x12, x24
   489f8:	add	x11, x9, #0x18
   489fc:	orr	x9, x10, #0x1
   48a00:	add	x12, x12, #0x18
   48a04:	mov	x13, x10
   48a08:	ldp	q0, q1, [x11, #-16]
   48a0c:	add	x11, x11, #0x20
   48a10:	subs	x13, x13, #0x4
   48a14:	stp	q0, q1, [x12, #-16]
   48a18:	add	x12, x12, #0x20
   48a1c:	b.ne	48a08 <__gmpn_toom_interpolate_16pts@@Base+0x160c>  // b.any
   48a20:	cmp	x8, x10
   48a24:	b.eq	4882c <__gmpn_toom_interpolate_16pts@@Base+0x1430>  // b.none
   48a28:	b	48804 <__gmpn_toom_interpolate_16pts@@Base+0x1408>
   48a2c:	mov	w12, #0x70                  	// #112
   48a30:	and	x10, x8, #0xfffffffffffffffc
   48a34:	madd	x12, x19, x12, x24
   48a38:	add	x11, x9, #0x18
   48a3c:	orr	x9, x10, #0x1
   48a40:	add	x12, x12, #0x18
   48a44:	mov	x13, x10
   48a48:	ldp	q0, q1, [x11, #-16]
   48a4c:	add	x11, x11, #0x20
   48a50:	subs	x13, x13, #0x4
   48a54:	stp	q0, q1, [x12, #-16]
   48a58:	add	x12, x12, #0x20
   48a5c:	b.ne	48a48 <__gmpn_toom_interpolate_16pts@@Base+0x164c>  // b.any
   48a60:	cmp	x8, x10
   48a64:	b.eq	48914 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   48a68:	b	488ec <__gmpn_toom_interpolate_16pts@@Base+0x14f0>
   48a6c:	mov	w8, #0x78                  	// #120
   48a70:	madd	x0, x19, x8, x24
   48a74:	ldur	x8, [x29, #-8]
   48a78:	mov	x3, x21
   48a7c:	ldp	x20, x19, [sp, #208]
   48a80:	ldp	x22, x21, [sp, #192]
   48a84:	add	x2, x28, x8, lsl #3
   48a88:	ldp	x24, x23, [sp, #176]
   48a8c:	ldp	x26, x25, [sp, #160]
   48a90:	ldp	x28, x27, [sp, #144]
   48a94:	ldp	x29, x30, [sp, #128]
   48a98:	mov	x1, x0
   48a9c:	add	sp, sp, #0xe0
   48aa0:	b	ce90 <__gmpn_add_nc@plt>

0000000000048aa4 <__gmpn_ni_invertappr@@Base>:
   48aa4:	stp	x29, x30, [sp, #-96]!
   48aa8:	stp	x28, x27, [sp, #16]
   48aac:	stp	x26, x25, [sp, #32]
   48ab0:	stp	x24, x23, [sp, #48]
   48ab4:	stp	x22, x21, [sp, #64]
   48ab8:	stp	x20, x19, [sp, #80]
   48abc:	mov	x29, sp
   48ac0:	sub	sp, sp, #0x1b0
   48ac4:	mov	x19, sp
   48ac8:	mov	x20, x3
   48acc:	mov	x25, x2
   48ad0:	mov	x24, x1
   48ad4:	mov	x23, x0
   48ad8:	mov	x22, xzr
   48adc:	add	x8, x19, #0x68
   48ae0:	mov	x21, x2
   48ae4:	asr	x9, x21, #1
   48ae8:	str	x21, [x8, x22, lsl #3]
   48aec:	cmp	x21, #0x143
   48af0:	add	x21, x9, #0x1
   48af4:	add	x22, x22, #0x1
   48af8:	b.gt	48ae4 <__gmpn_ni_invertappr@@Base+0x40>
   48afc:	lsl	x26, x25, #3
   48b00:	mvn	x8, x9
   48b04:	add	x9, x24, x26
   48b08:	add	x28, x23, x26
   48b0c:	lsl	x8, x8, #3
   48b10:	add	x0, x28, x8
   48b14:	add	x1, x9, x8
   48b18:	mov	x2, x21
   48b1c:	mov	x3, x20
   48b20:	str	x9, [x19, #48]
   48b24:	bl	48fdc <__gmpn_ni_invertappr@@Base+0x538>
   48b28:	add	x0, x25, #0x1
   48b2c:	str	xzr, [x19, #96]
   48b30:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   48b34:	asr	x8, x0, #1
   48b38:	cmp	x8, x25, asr #1
   48b3c:	csel	x9, x8, x0, gt
   48b40:	cmp	x8, x25
   48b44:	csel	x8, x9, xzr, lt  // lt = tstop
   48b48:	add	x8, x0, x8
   48b4c:	lsl	x8, x8, #3
   48b50:	add	x1, x8, #0x20
   48b54:	mov	w8, #0x7f00                	// #32512
   48b58:	cmp	x1, x8
   48b5c:	b.hi	48fc4 <__gmpn_ni_invertappr@@Base+0x520>  // b.pmore
   48b60:	add	x9, x1, #0xf
   48b64:	mov	x8, sp
   48b68:	and	x9, x9, #0xfffffffffffffff0
   48b6c:	sub	x8, x8, x9
   48b70:	str	x8, [x19, #16]
   48b74:	mov	sp, x8
   48b78:	add	x8, x20, #0x8
   48b7c:	stp	x8, x28, [x19, #32]
   48b80:	add	x8, x26, x24
   48b84:	ldr	x24, [x19, #48]
   48b88:	sub	x9, x20, #0x8
   48b8c:	str	x9, [x19, #8]
   48b90:	add	x9, x26, x23
   48b94:	sub	x8, x8, #0x8
   48b98:	str	x8, [x19, #64]
   48b9c:	add	x8, x9, #0x8
   48ba0:	str	x8, [x19, #24]
   48ba4:	b	48bb4 <__gmpn_ni_invertappr@@Base+0x110>
   48ba8:	ldr	x24, [x19, #48]
   48bac:	mov	x21, x23
   48bb0:	cbz	x22, 48f84 <__gmpn_ni_invertappr@@Base+0x4e0>
   48bb4:	sub	x22, x22, #0x1
   48bb8:	add	x8, x19, #0x68
   48bbc:	ldr	x23, [x8, x22, lsl #3]
   48bc0:	neg	x8, x21
   48bc4:	str	x8, [x19, #88]
   48bc8:	add	x0, x23, #0x1
   48bcc:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   48bd0:	add	x8, x23, x21
   48bd4:	sub	x26, x24, x23, lsl #3
   48bd8:	cmp	x0, x8
   48bdc:	sub	x4, x28, x21, lsl #3
   48be0:	str	x4, [x19, #80]
   48be4:	b.le	48c20 <__gmpn_ni_invertappr@@Base+0x17c>
   48be8:	mov	x0, x20
   48bec:	mov	x1, x26
   48bf0:	mov	x2, x23
   48bf4:	mov	x3, x4
   48bf8:	mov	x4, x21
   48bfc:	bl	ccd0 <__gmpn_mul@plt>
   48c00:	add	x0, x20, x21, lsl #3
   48c04:	sub	x8, x23, x21
   48c08:	add	x3, x8, #0x1
   48c0c:	mov	x1, x0
   48c10:	mov	x2, x26
   48c14:	bl	ca70 <__gmpn_add_n@plt>
   48c18:	mov	w8, #0x1                   	// #1
   48c1c:	b	48cfc <__gmpn_ni_invertappr@@Base+0x258>
   48c20:	ldr	x6, [x19, #16]
   48c24:	mov	x25, x0
   48c28:	mov	x0, x20
   48c2c:	mov	x1, x25
   48c30:	mov	x2, x26
   48c34:	mov	x3, x23
   48c38:	mov	x5, x21
   48c3c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   48c40:	add	x27, x20, x21, lsl #3
   48c44:	sub	x28, x25, x21
   48c48:	mov	x0, x27
   48c4c:	mov	x1, x27
   48c50:	mov	x2, x26
   48c54:	mov	x3, x28
   48c58:	bl	ca70 <__gmpn_add_n@plt>
   48c5c:	sub	x3, x23, x28
   48c60:	mov	x4, x0
   48c64:	sub	x2, x24, x3, lsl #3
   48c68:	mov	x0, x20
   48c6c:	mov	x1, x20
   48c70:	bl	ce90 <__gmpn_add_nc@plt>
   48c74:	lsl	x8, x25, #3
   48c78:	add	x9, x27, x23, lsl #3
   48c7c:	mov	w11, #0x1                   	// #1
   48c80:	str	x11, [x20, x8]
   48c84:	sub	x9, x9, x8
   48c88:	ldr	x10, [x9]
   48c8c:	sub	x11, x11, x0
   48c90:	subs	x10, x10, x11
   48c94:	str	x10, [x9]
   48c98:	b.cs	48cbc <__gmpn_ni_invertappr@@Base+0x218>  // b.hs, b.nlast
   48c9c:	ldr	x10, [x19, #32]
   48ca0:	add	x9, x21, x23
   48ca4:	sub	x9, x9, x25
   48ca8:	add	x9, x10, x9, lsl #3
   48cac:	ldr	x10, [x9]
   48cb0:	sub	x11, x10, #0x1
   48cb4:	str	x11, [x9], #8
   48cb8:	cbz	x10, 48cac <__gmpn_ni_invertappr@@Base+0x208>
   48cbc:	ldr	x9, [x20, x8]
   48cc0:	ldr	x10, [x20]
   48cc4:	add	x9, x9, x10
   48cc8:	sub	x9, x9, #0x1
   48ccc:	str	x9, [x20]
   48cd0:	ldr	x8, [x20, x8]
   48cd4:	mov	w9, #0x1                   	// #1
   48cd8:	sub	x8, x9, x8
   48cdc:	cmp	x10, x8
   48ce0:	b.cs	48cf8 <__gmpn_ni_invertappr@@Base+0x254>  // b.hs, b.nlast
   48ce4:	ldr	x8, [x19, #32]
   48ce8:	ldr	x9, [x8]
   48cec:	sub	x10, x9, #0x1
   48cf0:	str	x10, [x8], #8
   48cf4:	cbz	x9, 48ce8 <__gmpn_ni_invertappr@@Base+0x244>
   48cf8:	mov	x8, xzr
   48cfc:	add	x25, x20, x23, lsl #3
   48d00:	neg	x9, x23
   48d04:	ldr	x28, [x25]
   48d08:	str	x9, [x19, #72]
   48d0c:	ldr	x9, [x19, #88]
   48d10:	cmp	x28, #0x1
   48d14:	lsl	x27, x9, #3
   48d18:	b.hi	48d70 <__gmpn_ni_invertappr@@Base+0x2cc>  // b.pmore
   48d1c:	cbz	x28, 48dfc <__gmpn_ni_invertappr@@Base+0x358>
   48d20:	add	x8, x28, #0x1
   48d24:	str	x8, [x19, #56]
   48d28:	ldr	x8, [x19, #64]
   48d2c:	mov	x10, x23
   48d30:	subs	x9, x10, #0x1
   48d34:	b.lt	48de4 <__gmpn_ni_invertappr@@Base+0x340>  // b.tstop
   48d38:	add	x10, x20, x10, lsl #3
   48d3c:	ldur	x10, [x10, #-8]
   48d40:	ldr	x11, [x8], #-8
   48d44:	cmp	x10, x11
   48d48:	mov	x10, x9
   48d4c:	b.eq	48d30 <__gmpn_ni_invertappr@@Base+0x28c>  // b.none
   48d50:	b.ls	48de4 <__gmpn_ni_invertappr@@Base+0x340>  // b.plast
   48d54:	mov	x0, x20
   48d58:	mov	x1, x20
   48d5c:	mov	x2, x26
   48d60:	mov	x3, x23
   48d64:	bl	c440 <__gmpn_sublsh1_n@plt>
   48d68:	add	x8, x28, #0x2
   48d6c:	b	48e00 <__gmpn_ni_invertappr@@Base+0x35c>
   48d70:	ldr	x9, [x20]
   48d74:	subs	x8, x9, x8
   48d78:	str	x8, [x20]
   48d7c:	b.cs	48d94 <__gmpn_ni_invertappr@@Base+0x2f0>  // b.hs, b.nlast
   48d80:	ldr	x8, [x19, #32]
   48d84:	ldr	x9, [x8]
   48d88:	sub	x10, x9, #0x1
   48d8c:	str	x10, [x8], #8
   48d90:	cbz	x9, 48d84 <__gmpn_ni_invertappr@@Base+0x2e0>
   48d94:	ldr	x8, [x25]
   48d98:	ldr	x28, [x19, #80]
   48d9c:	cmn	x8, #0x1
   48da0:	b.eq	48dcc <__gmpn_ni_invertappr@@Base+0x328>  // b.none
   48da4:	mov	x8, x28
   48da8:	ldr	x9, [x8]
   48dac:	adds	x9, x9, #0x1
   48db0:	str	x9, [x8], #8
   48db4:	b.cs	48da8 <__gmpn_ni_invertappr@@Base+0x304>  // b.hs, b.nlast
   48db8:	mov	x0, x20
   48dbc:	mov	x1, x20
   48dc0:	mov	x2, x26
   48dc4:	mov	x3, x23
   48dc8:	bl	ca70 <__gmpn_add_n@plt>
   48dcc:	add	x8, x20, x23, lsl #4
   48dd0:	add	x0, x8, x27
   48dd4:	add	x1, x25, x27
   48dd8:	mov	x2, x21
   48ddc:	bl	c290 <__gmpn_com@plt>
   48de0:	b	48ef0 <__gmpn_ni_invertappr@@Base+0x44c>
   48de4:	mov	x0, x20
   48de8:	mov	x1, x20
   48dec:	mov	x2, x26
   48df0:	mov	x3, x23
   48df4:	bl	c2d0 <__gmpn_sub_n@plt>
   48df8:	b	48e04 <__gmpn_ni_invertappr@@Base+0x360>
   48dfc:	mov	w8, #0x1                   	// #1
   48e00:	str	x8, [x19, #56]
   48e04:	ldr	x8, [x19, #64]
   48e08:	ldr	x28, [x19, #80]
   48e0c:	mov	x10, x23
   48e10:	subs	x9, x10, #0x1
   48e14:	b.lt	48e64 <__gmpn_ni_invertappr@@Base+0x3c0>  // b.tstop
   48e18:	add	x10, x20, x10, lsl #3
   48e1c:	ldur	x10, [x10, #-8]
   48e20:	ldr	x11, [x8], #-8
   48e24:	cmp	x10, x11
   48e28:	mov	x10, x9
   48e2c:	b.eq	48e10 <__gmpn_ni_invertappr@@Base+0x36c>  // b.none
   48e30:	b.ls	48e64 <__gmpn_ni_invertappr@@Base+0x3c0>  // b.plast
   48e34:	mov	x0, x25
   48e38:	mov	x1, x20
   48e3c:	mov	x2, x26
   48e40:	mov	x3, x23
   48e44:	bl	d090 <__gmpn_rsblsh1_n@plt>
   48e48:	ldr	x9, [x19, #56]
   48e4c:	add	x9, x9, #0x1
   48e50:	ldr	x8, [x28]
   48e54:	subs	x8, x8, x9
   48e58:	str	x8, [x28]
   48e5c:	b.cc	48ed4 <__gmpn_ni_invertappr@@Base+0x430>  // b.lo, b.ul, b.last
   48e60:	b	48ef0 <__gmpn_ni_invertappr@@Base+0x44c>
   48e64:	add	x8, x20, x23, lsl #4
   48e68:	add	x0, x8, x27
   48e6c:	ldr	x8, [x19, #64]
   48e70:	ldr	x12, [x19, #8]
   48e74:	add	x1, x24, x27
   48e78:	add	x2, x25, x27
   48e7c:	sub	x9, x23, x21
   48e80:	add	x8, x8, x27
   48e84:	subs	x10, x9, #0x1
   48e88:	b.lt	48eac <__gmpn_ni_invertappr@@Base+0x408>  // b.tstop
   48e8c:	ldr	x9, [x12, x9, lsl #3]
   48e90:	ldr	x11, [x8], #-8
   48e94:	cmp	x9, x11
   48e98:	mov	x9, x10
   48e9c:	b.eq	48e84 <__gmpn_ni_invertappr@@Base+0x3e0>  // b.none
   48ea0:	mov	w8, #0x1                   	// #1
   48ea4:	cneg	w8, w8, ls  // ls = plast
   48ea8:	b	48eb0 <__gmpn_ni_invertappr@@Base+0x40c>
   48eac:	mov	w8, wzr
   48eb0:	cmp	w8, #0x0
   48eb4:	cset	w4, gt
   48eb8:	mov	x3, x21
   48ebc:	bl	c760 <__gmpn_sub_nc@plt>
   48ec0:	ldr	x9, [x19, #56]
   48ec4:	ldr	x8, [x28]
   48ec8:	subs	x8, x8, x9
   48ecc:	str	x8, [x28]
   48ed0:	b.cs	48ef0 <__gmpn_ni_invertappr@@Base+0x44c>  // b.hs, b.nlast
   48ed4:	ldr	x8, [x19, #24]
   48ed8:	ldr	x9, [x19, #88]
   48edc:	add	x8, x8, x9, lsl #3
   48ee0:	ldr	x9, [x8]
   48ee4:	sub	x10, x9, #0x1
   48ee8:	str	x10, [x8], #8
   48eec:	cbz	x9, 48ee0 <__gmpn_ni_invertappr@@Base+0x43c>
   48ef0:	add	x8, x20, x23, lsl #4
   48ef4:	add	x26, x8, x27
   48ef8:	mov	x0, x20
   48efc:	mov	x1, x26
   48f00:	mov	x2, x28
   48f04:	mov	x3, x21
   48f08:	bl	c990 <__gmpn_mul_n@plt>
   48f0c:	lsl	x28, x21, #3
   48f10:	lsl	x24, x21, #1
   48f14:	add	x0, x20, x28
   48f18:	sub	x3, x24, x23
   48f1c:	mov	x1, x0
   48f20:	mov	x2, x26
   48f24:	bl	ca70 <__gmpn_add_n@plt>
   48f28:	ldr	x8, [x19, #72]
   48f2c:	add	x2, x25, x28
   48f30:	ldr	x28, [x19, #40]
   48f34:	add	x26, x24, x21
   48f38:	lsl	x8, x8, #3
   48f3c:	add	x9, x20, x26, lsl #3
   48f40:	mov	x4, x0
   48f44:	add	x0, x28, x8
   48f48:	add	x1, x9, x8
   48f4c:	sub	x3, x23, x21
   48f50:	bl	ce90 <__gmpn_add_nc@plt>
   48f54:	ldr	x8, [x28, x27]
   48f58:	adds	x8, x8, x0
   48f5c:	str	x8, [x28, x27]
   48f60:	b.cc	48ba8 <__gmpn_ni_invertappr@@Base+0x104>  // b.lo, b.ul, b.last
   48f64:	ldr	x8, [x19, #24]
   48f68:	ldr	x9, [x19, #88]
   48f6c:	add	x8, x8, x9, lsl #3
   48f70:	ldr	x9, [x8]
   48f74:	adds	x9, x9, #0x1
   48f78:	str	x9, [x8], #8
   48f7c:	b.cs	48f70 <__gmpn_ni_invertappr@@Base+0x4cc>  // b.hs, b.nlast
   48f80:	b	48ba8 <__gmpn_ni_invertappr@@Base+0x104>
   48f84:	mvn	x8, x23
   48f88:	add	x8, x26, x8
   48f8c:	ldr	x8, [x20, x8, lsl #3]
   48f90:	ldr	x0, [x19, #96]
   48f94:	cmn	x8, #0x8
   48f98:	cset	w20, hi  // hi = pmore
   48f9c:	cbnz	x0, 48fd4 <__gmpn_ni_invertappr@@Base+0x530>
   48fa0:	mov	x0, x20
   48fa4:	mov	sp, x29
   48fa8:	ldp	x20, x19, [sp, #80]
   48fac:	ldp	x22, x21, [sp, #64]
   48fb0:	ldp	x24, x23, [sp, #48]
   48fb4:	ldp	x26, x25, [sp, #32]
   48fb8:	ldp	x28, x27, [sp, #16]
   48fbc:	ldp	x29, x30, [sp], #96
   48fc0:	ret
   48fc4:	add	x0, x19, #0x60
   48fc8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   48fcc:	str	x0, [x19, #16]
   48fd0:	b	48b78 <__gmpn_ni_invertappr@@Base+0xd4>
   48fd4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   48fd8:	b	48fa0 <__gmpn_ni_invertappr@@Base+0x4fc>
   48fdc:	stp	x29, x30, [sp, #-64]!
   48fe0:	stp	x20, x19, [sp, #48]
   48fe4:	mov	x20, x1
   48fe8:	cmp	x2, #0x1
   48fec:	mov	x19, x0
   48ff0:	stp	x24, x23, [sp, #16]
   48ff4:	stp	x22, x21, [sp, #32]
   48ff8:	mov	x29, sp
   48ffc:	b.ne	49018 <__gmpn_ni_invertappr@@Base+0x574>  // b.any
   49000:	ldr	x0, [x20]
   49004:	bl	d3f0 <__gmpn_invert_limb@plt>
   49008:	mov	x8, x0
   4900c:	mov	x0, xzr
   49010:	str	x8, [x19]
   49014:	b	49104 <__gmpn_ni_invertappr@@Base+0x660>
   49018:	lsl	x23, x2, #3
   4901c:	mov	x22, x2
   49020:	mov	w1, #0xff                  	// #255
   49024:	mov	x0, x3
   49028:	mov	x2, x23
   4902c:	mov	x21, x3
   49030:	bl	c5f0 <memset@plt>
   49034:	add	x0, x21, x23
   49038:	mov	x1, x20
   4903c:	mov	x2, x22
   49040:	bl	c290 <__gmpn_com@plt>
   49044:	cmp	x22, #0x2
   49048:	b.ne	4906c <__gmpn_ni_invertappr@@Base+0x5c8>  // b.any
   4904c:	mov	w3, #0x4                   	// #4
   49050:	mov	x0, x19
   49054:	mov	x1, xzr
   49058:	mov	x2, x21
   4905c:	mov	x4, x20
   49060:	bl	c200 <__gmpn_divrem_2@plt>
   49064:	mov	x0, xzr
   49068:	b	49104 <__gmpn_ni_invertappr@@Base+0x660>
   4906c:	add	x24, x20, x22, lsl #3
   49070:	ldur	x23, [x24, #-8]
   49074:	mov	x0, x23
   49078:	bl	d3f0 <__gmpn_invert_limb@plt>
   4907c:	ldur	x8, [x24, #-16]
   49080:	mul	x9, x0, x23
   49084:	adds	x9, x9, x8
   49088:	b.cc	490a4 <__gmpn_ni_invertappr@@Base+0x600>  // b.lo, b.ul, b.last
   4908c:	subs	x9, x9, x23
   49090:	cset	w10, cs  // cs = hs, nlast
   49094:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   49098:	mvn	x10, x10
   4909c:	add	x0, x10, x0
   490a0:	sub	x9, x9, x11
   490a4:	umulh	x10, x8, x0
   490a8:	adds	x9, x10, x9
   490ac:	b.cc	490d4 <__gmpn_ni_invertappr@@Base+0x630>  // b.lo, b.ul, b.last
   490b0:	cmp	x9, x23
   490b4:	sub	x5, x0, #0x1
   490b8:	b.cc	490d8 <__gmpn_ni_invertappr@@Base+0x634>  // b.lo, b.ul, b.last
   490bc:	mul	x10, x0, x8
   490c0:	cmp	x9, x23
   490c4:	sub	x11, x0, #0x2
   490c8:	ccmp	x10, x8, #0x2, ls  // ls = plast
   490cc:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   490d0:	b	490d8 <__gmpn_ni_invertappr@@Base+0x634>
   490d4:	mov	x5, x0
   490d8:	lsl	x2, x22, #1
   490dc:	mov	x0, x19
   490e0:	mov	x1, x21
   490e4:	mov	x3, x20
   490e8:	mov	x4, x22
   490ec:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   490f0:	mov	w0, #0x1                   	// #1
   490f4:	ldr	x8, [x19]
   490f8:	sub	x9, x8, #0x1
   490fc:	str	x9, [x19], #8
   49100:	cbz	x8, 490f4 <__gmpn_ni_invertappr@@Base+0x650>
   49104:	ldp	x20, x19, [sp, #48]
   49108:	ldp	x22, x21, [sp, #32]
   4910c:	ldp	x24, x23, [sp, #16]
   49110:	ldp	x29, x30, [sp], #64
   49114:	ret

0000000000049118 <__gmpn_invertappr@@Base>:
   49118:	cmp	x2, #0xa2
   4911c:	b.le	49124 <__gmpn_invertappr@@Base+0xc>
   49120:	b	d180 <__gmpn_ni_invertappr@plt>
   49124:	b	48fdc <__gmpn_ni_invertappr@@Base+0x538>

0000000000049128 <__gmpn_invert@@Base>:
   49128:	stp	x29, x30, [sp, #-64]!
   4912c:	stp	x20, x19, [sp, #48]
   49130:	mov	x20, x1
   49134:	cmp	x2, #0x1
   49138:	mov	x19, x0
   4913c:	stp	x24, x23, [sp, #16]
   49140:	stp	x22, x21, [sp, #32]
   49144:	mov	x29, sp
   49148:	b.ne	4915c <__gmpn_invert@@Base+0x34>  // b.any
   4914c:	ldr	x0, [x20]
   49150:	bl	d3f0 <__gmpn_invert_limb@plt>
   49154:	str	x0, [x19]
   49158:	b	49184 <__gmpn_invert@@Base+0x5c>
   4915c:	mov	x21, x3
   49160:	mov	x22, x2
   49164:	cmp	x2, #0xa1
   49168:	b.le	49198 <__gmpn_invert@@Base+0x70>
   4916c:	mov	x0, x19
   49170:	mov	x1, x20
   49174:	mov	x2, x22
   49178:	mov	x3, x21
   4917c:	bl	d180 <__gmpn_ni_invertappr@plt>
   49180:	cbnz	x0, 49280 <__gmpn_invert@@Base+0x158>
   49184:	ldp	x20, x19, [sp, #48]
   49188:	ldp	x22, x21, [sp, #32]
   4918c:	ldp	x24, x23, [sp, #16]
   49190:	ldp	x29, x30, [sp], #64
   49194:	ret
   49198:	lsl	x23, x22, #3
   4919c:	mov	w1, #0xff                  	// #255
   491a0:	mov	x0, x21
   491a4:	mov	x2, x23
   491a8:	bl	c5f0 <memset@plt>
   491ac:	add	x0, x21, x23
   491b0:	mov	x1, x20
   491b4:	mov	x2, x22
   491b8:	bl	c290 <__gmpn_com@plt>
   491bc:	cmp	x22, #0x2
   491c0:	b.ne	491ec <__gmpn_invert@@Base+0xc4>  // b.any
   491c4:	mov	x0, x19
   491c8:	mov	x2, x21
   491cc:	mov	x4, x20
   491d0:	ldp	x20, x19, [sp, #48]
   491d4:	ldp	x22, x21, [sp, #32]
   491d8:	ldp	x24, x23, [sp, #16]
   491dc:	mov	w3, #0x4                   	// #4
   491e0:	mov	x1, xzr
   491e4:	ldp	x29, x30, [sp], #64
   491e8:	b	c200 <__gmpn_divrem_2@plt>
   491ec:	add	x24, x20, x22, lsl #3
   491f0:	ldur	x23, [x24, #-8]
   491f4:	mov	x0, x23
   491f8:	bl	d3f0 <__gmpn_invert_limb@plt>
   491fc:	ldur	x8, [x24, #-16]
   49200:	mul	x9, x0, x23
   49204:	adds	x9, x9, x8
   49208:	b.cc	49224 <__gmpn_invert@@Base+0xfc>  // b.lo, b.ul, b.last
   4920c:	subs	x9, x9, x23
   49210:	cset	w10, cs  // cs = hs, nlast
   49214:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   49218:	mvn	x10, x10
   4921c:	add	x0, x10, x0
   49220:	sub	x9, x9, x11
   49224:	umulh	x10, x8, x0
   49228:	adds	x9, x10, x9
   4922c:	b.cc	49254 <__gmpn_invert@@Base+0x12c>  // b.lo, b.ul, b.last
   49230:	cmp	x9, x23
   49234:	sub	x5, x0, #0x1
   49238:	b.cc	49258 <__gmpn_invert@@Base+0x130>  // b.lo, b.ul, b.last
   4923c:	mul	x10, x0, x8
   49240:	cmp	x9, x23
   49244:	sub	x11, x0, #0x2
   49248:	ccmp	x10, x8, #0x2, ls  // ls = plast
   4924c:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   49250:	b	49258 <__gmpn_invert@@Base+0x130>
   49254:	mov	x5, x0
   49258:	lsl	x2, x22, #1
   4925c:	mov	x0, x19
   49260:	mov	x1, x21
   49264:	mov	x3, x20
   49268:	mov	x4, x22
   4926c:	ldp	x20, x19, [sp, #48]
   49270:	ldp	x22, x21, [sp, #32]
   49274:	ldp	x24, x23, [sp, #16]
   49278:	ldp	x29, x30, [sp], #64
   4927c:	b	cee0 <__gmpn_sbpi1_div_q@plt>
   49280:	mov	x0, x21
   49284:	mov	x1, x19
   49288:	mov	x2, x20
   4928c:	mov	x3, x22
   49290:	bl	c990 <__gmpn_mul_n@plt>
   49294:	mov	x0, x21
   49298:	mov	x1, x21
   4929c:	mov	x2, x20
   492a0:	mov	x3, x22
   492a4:	bl	ca70 <__gmpn_add_n@plt>
   492a8:	cbz	x0, 492f0 <__gmpn_invert@@Base+0x1c8>
   492ac:	mov	x4, x0
   492b0:	add	x0, x21, x22, lsl #3
   492b4:	mov	x1, x0
   492b8:	mov	x2, x20
   492bc:	mov	x3, x22
   492c0:	bl	ce90 <__gmpn_add_nc@plt>
   492c4:	eor	x8, x0, #0x1
   492c8:	ldr	x9, [x19]
   492cc:	adds	x8, x9, x8
   492d0:	str	x8, [x19]
   492d4:	b.cc	49184 <__gmpn_invert@@Base+0x5c>  // b.lo, b.ul, b.last
   492d8:	add	x8, x19, #0x8
   492dc:	ldr	x9, [x8]
   492e0:	adds	x9, x9, #0x1
   492e4:	str	x9, [x8], #8
   492e8:	b.cs	492dc <__gmpn_invert@@Base+0x1b4>  // b.hs, b.nlast
   492ec:	b	49184 <__gmpn_invert@@Base+0x5c>
   492f0:	mov	w8, #0x1                   	// #1
   492f4:	ldr	x9, [x19]
   492f8:	adds	x8, x9, x8
   492fc:	str	x8, [x19]
   49300:	b.cc	49184 <__gmpn_invert@@Base+0x5c>  // b.lo, b.ul, b.last
   49304:	b	492d8 <__gmpn_invert@@Base+0x1b0>

0000000000049308 <__gmpn_binvert_itch@@Base>:
   49308:	stp	x29, x30, [sp, #-32]!
   4930c:	str	x19, [sp, #16]
   49310:	mov	x29, sp
   49314:	mov	x19, x0
   49318:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4931c:	add	x8, x19, #0x1
   49320:	asr	x9, x0, #1
   49324:	cmp	x9, x8, asr #1
   49328:	csel	x8, x0, x9, lt  // lt = tstop
   4932c:	cmp	x9, x19
   49330:	ldr	x19, [sp, #16]
   49334:	csel	x8, x8, xzr, lt  // lt = tstop
   49338:	add	x8, x8, x0, lsl #1
   4933c:	add	x0, x8, #0x4
   49340:	ldp	x29, x30, [sp], #32
   49344:	ret

0000000000049348 <__gmpn_binvert@@Base>:
   49348:	sub	sp, sp, #0x1b0
   4934c:	stp	x28, x27, [sp, #352]
   49350:	stp	x22, x21, [sp, #400]
   49354:	stp	x20, x19, [sp, #416]
   49358:	mov	x19, x3
   4935c:	mov	x20, x2
   49360:	mov	x21, x1
   49364:	cmp	x2, #0xc2
   49368:	mov	x22, x0
   4936c:	add	x27, sp, #0x8
   49370:	stp	x29, x30, [sp, #336]
   49374:	stp	x26, x25, [sp, #368]
   49378:	stp	x24, x23, [sp, #384]
   4937c:	add	x29, sp, #0x150
   49380:	b.lt	49424 <__gmpn_binvert@@Base+0xdc>  // b.tstop
   49384:	mov	x8, x20
   49388:	add	x9, x8, #0x1
   4938c:	asr	x23, x9, #1
   49390:	cmp	x8, #0x182
   49394:	str	x8, [x27], #8
   49398:	mov	x8, x23
   4939c:	b.gt	49388 <__gmpn_binvert@@Base+0x40>
   493a0:	cbz	x23, 493b4 <__gmpn_binvert@@Base+0x6c>
   493a4:	lsl	x2, x23, #3
   493a8:	mov	x0, x19
   493ac:	mov	w1, wzr
   493b0:	bl	c5f0 <memset@plt>
   493b4:	mov	w8, #0x1                   	// #1
   493b8:	str	x8, [x19]
   493bc:	ldr	x8, [x21]
   493c0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   493c4:	ldr	x9, [x9, #3952]
   493c8:	orr	x11, xzr, #0xfffffffffffffffe
   493cc:	ubfx	x10, x8, #1, #7
   493d0:	cmp	x23, #0x5c
   493d4:	ldrb	w9, [x9, x10]
   493d8:	mov	w10, #0x2                   	// #2
   493dc:	mov	x0, x22
   493e0:	mov	x1, x19
   493e4:	msub	x12, x8, x9, x10
   493e8:	mul	x9, x12, x9
   493ec:	msub	x10, x9, x8, x10
   493f0:	mul	x9, x9, x10
   493f4:	madd	x8, x9, x8, x11
   493f8:	mul	x5, x8, x9
   493fc:	mov	x2, x23
   49400:	mov	x3, x21
   49404:	mov	x4, x23
   49408:	b.le	49430 <__gmpn_binvert@@Base+0xe8>
   4940c:	bl	ce10 <__gmpn_dcpi1_bdiv_q@plt>
   49410:	ldr	x10, [x22]
   49414:	mov	x8, x22
   49418:	mov	x9, x23
   4941c:	cbnz	x10, 49460 <__gmpn_binvert@@Base+0x118>
   49420:	b	49444 <__gmpn_binvert@@Base+0xfc>
   49424:	mov	x23, x20
   49428:	cbnz	x23, 493a4 <__gmpn_binvert@@Base+0x5c>
   4942c:	b	493b4 <__gmpn_binvert@@Base+0x6c>
   49430:	bl	c510 <__gmpn_sbpi1_bdiv_q@plt>
   49434:	ldr	x10, [x22]
   49438:	mov	x8, x22
   4943c:	mov	x9, x23
   49440:	cbnz	x10, 49460 <__gmpn_binvert@@Base+0x118>
   49444:	mov	x9, x23
   49448:	mov	x8, x22
   4944c:	subs	x9, x9, #0x1
   49450:	str	xzr, [x8]
   49454:	b.eq	4947c <__gmpn_binvert@@Base+0x134>  // b.none
   49458:	ldr	x10, [x8, #8]!
   4945c:	cbz	x10, 4944c <__gmpn_binvert@@Base+0x104>
   49460:	neg	x10, x10
   49464:	subs	x2, x9, #0x1
   49468:	str	x10, [x8]
   4946c:	b.eq	4947c <__gmpn_binvert@@Base+0x134>  // b.none
   49470:	add	x0, x8, #0x8
   49474:	mov	x1, x0
   49478:	bl	c290 <__gmpn_com@plt>
   4947c:	cmp	x23, x20
   49480:	b.ge	496c0 <__gmpn_binvert@@Base+0x378>  // b.tcont
   49484:	add	x28, x19, #0x8
   49488:	b	49498 <__gmpn_binvert@@Base+0x150>
   4948c:	cmp	x24, x20
   49490:	mov	x23, x24
   49494:	b.ge	496c0 <__gmpn_binvert@@Base+0x378>  // b.tcont
   49498:	ldr	x24, [x27, #-8]!
   4949c:	mov	x0, x24
   494a0:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   494a4:	mov	x25, x0
   494a8:	add	x26, x19, x0, lsl #3
   494ac:	mov	x0, x19
   494b0:	mov	x1, x25
   494b4:	mov	x2, x21
   494b8:	mov	x3, x24
   494bc:	mov	x4, x22
   494c0:	mov	x5, x23
   494c4:	mov	x6, x26
   494c8:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   494cc:	ldr	x8, [x19]
   494d0:	sub	x9, x24, x25
   494d4:	add	x10, x9, x23
   494d8:	sub	x9, x8, #0x1
   494dc:	str	x9, [x26]
   494e0:	cbz	x8, 49548 <__gmpn_binvert@@Base+0x200>
   494e4:	cbz	x25, 49664 <__gmpn_binvert@@Base+0x31c>
   494e8:	cmp	x10, #0x2
   494ec:	b.lt	49664 <__gmpn_binvert@@Base+0x31c>  // b.tstop
   494f0:	add	x8, x23, x24
   494f4:	mvn	x9, x25
   494f8:	add	x9, x9, x8
   494fc:	cmp	x9, #0x4
   49500:	b.cc	49520 <__gmpn_binvert@@Base+0x1d8>  // b.lo, b.ul, b.last
   49504:	add	x11, x28, x25, lsl #3
   49508:	add	x10, x19, x10, lsl #3
   4950c:	cmp	x11, x10
   49510:	b.cs	49628 <__gmpn_binvert@@Base+0x2e0>  // b.hs, b.nlast
   49514:	add	x10, x19, x8, lsl #3
   49518:	cmp	x28, x10
   4951c:	b.cs	49628 <__gmpn_binvert@@Base+0x2e0>  // b.hs, b.nlast
   49520:	mov	w10, #0x1                   	// #1
   49524:	sub	x8, x8, x25
   49528:	sub	x8, x8, x10
   4952c:	add	x9, x19, x10, lsl #3
   49530:	ldr	x10, [x9]
   49534:	subs	x8, x8, #0x1
   49538:	str	x10, [x9, x25, lsl #3]
   4953c:	add	x9, x9, #0x8
   49540:	b.ne	49530 <__gmpn_binvert@@Base+0x1e8>  // b.any
   49544:	b	49664 <__gmpn_binvert@@Base+0x31c>
   49548:	add	x8, x23, x24
   4954c:	sub	x8, x8, x25
   49550:	mov	x11, x28
   49554:	sub	x13, x8, #0x2
   49558:	mov	w8, #0x1                   	// #1
   4955c:	cmp	x8, x10
   49560:	b.ge	49664 <__gmpn_binvert@@Base+0x31c>  // b.tcont
   49564:	mov	x12, x13
   49568:	lsl	x13, x8, #3
   4956c:	ldr	x14, [x19, x13]
   49570:	mov	x9, x11
   49574:	add	x8, x8, #0x1
   49578:	add	x11, x11, #0x8
   4957c:	sub	x15, x14, #0x1
   49580:	str	x15, [x26, x13]
   49584:	sub	x13, x12, #0x1
   49588:	cbz	x14, 4955c <__gmpn_binvert@@Base+0x214>
   4958c:	cbz	x25, 49664 <__gmpn_binvert@@Base+0x31c>
   49590:	cmp	x8, x10
   49594:	b.ge	49664 <__gmpn_binvert@@Base+0x31c>  // b.tcont
   49598:	add	x14, x23, x24
   4959c:	sub	x11, x14, x25
   495a0:	sub	x13, x11, x8
   495a4:	cmp	x13, #0x4
   495a8:	b.cc	49608 <__gmpn_binvert@@Base+0x2c0>  // b.lo, b.ul, b.last
   495ac:	add	x15, x25, x8
   495b0:	add	x15, x19, x15, lsl #3
   495b4:	add	x10, x19, x10, lsl #3
   495b8:	cmp	x15, x10
   495bc:	b.cs	495d0 <__gmpn_binvert@@Base+0x288>  // b.hs, b.nlast
   495c0:	add	x10, x19, x14, lsl #3
   495c4:	add	x14, x19, x8, lsl #3
   495c8:	cmp	x14, x10
   495cc:	b.cc	49608 <__gmpn_binvert@@Base+0x2c0>  // b.lo, b.ul, b.last
   495d0:	and	x10, x13, #0xfffffffffffffffc
   495d4:	lsl	x14, x25, #3
   495d8:	add	x8, x8, x10
   495dc:	and	x12, x12, #0xfffffffffffffffc
   495e0:	ldur	q0, [x9, #8]
   495e4:	ldur	q1, [x9, #24]
   495e8:	add	x15, x9, x14
   495ec:	subs	x12, x12, #0x4
   495f0:	add	x9, x9, #0x20
   495f4:	stur	q0, [x15, #8]
   495f8:	stur	q1, [x15, #24]
   495fc:	b.ne	495e0 <__gmpn_binvert@@Base+0x298>  // b.any
   49600:	cmp	x13, x10
   49604:	b.eq	49664 <__gmpn_binvert@@Base+0x31c>  // b.none
   49608:	sub	x9, x11, x8
   4960c:	add	x8, x19, x8, lsl #3
   49610:	ldr	x10, [x8]
   49614:	subs	x9, x9, #0x1
   49618:	str	x10, [x8, x25, lsl #3]
   4961c:	add	x8, x8, #0x8
   49620:	b.ne	49610 <__gmpn_binvert@@Base+0x2c8>  // b.any
   49624:	b	49664 <__gmpn_binvert@@Base+0x31c>
   49628:	and	x11, x9, #0xfffffffffffffffc
   4962c:	orr	x10, x11, #0x1
   49630:	lsl	x12, x25, #3
   49634:	mov	x13, x11
   49638:	mov	x14, x19
   4963c:	ldur	q0, [x14, #8]
   49640:	ldur	q1, [x14, #24]
   49644:	add	x15, x14, x12
   49648:	subs	x13, x13, #0x4
   4964c:	add	x14, x14, #0x20
   49650:	stur	q0, [x15, #8]
   49654:	stur	q1, [x15, #24]
   49658:	b.ne	4963c <__gmpn_binvert@@Base+0x2f4>  // b.any
   4965c:	cmp	x9, x11
   49660:	b.ne	49524 <__gmpn_binvert@@Base+0x1dc>  // b.any
   49664:	lsl	x8, x23, #3
   49668:	add	x25, x22, x8
   4966c:	sub	x23, x24, x23
   49670:	add	x2, x19, x8
   49674:	mov	x0, x25
   49678:	mov	x1, x22
   4967c:	mov	x3, x23
   49680:	bl	cec0 <__gmpn_mullo_n@plt>
   49684:	ldr	x8, [x25]
   49688:	cbnz	x8, 496a0 <__gmpn_binvert@@Base+0x358>
   4968c:	subs	x23, x23, #0x1
   49690:	str	xzr, [x25]
   49694:	b.eq	4948c <__gmpn_binvert@@Base+0x144>  // b.none
   49698:	ldr	x8, [x25, #8]!
   4969c:	cbz	x8, 4968c <__gmpn_binvert@@Base+0x344>
   496a0:	neg	x8, x8
   496a4:	subs	x2, x23, #0x1
   496a8:	str	x8, [x25]
   496ac:	b.eq	4948c <__gmpn_binvert@@Base+0x144>  // b.none
   496b0:	add	x0, x25, #0x8
   496b4:	mov	x1, x0
   496b8:	bl	c290 <__gmpn_com@plt>
   496bc:	b	4948c <__gmpn_binvert@@Base+0x144>
   496c0:	ldp	x20, x19, [sp, #416]
   496c4:	ldp	x22, x21, [sp, #400]
   496c8:	ldp	x24, x23, [sp, #384]
   496cc:	ldp	x26, x25, [sp, #368]
   496d0:	ldp	x28, x27, [sp, #352]
   496d4:	ldp	x29, x30, [sp, #336]
   496d8:	add	sp, sp, #0x1b0
   496dc:	ret

00000000000496e0 <__gmpn_bc_mulmod_bnm1@@Base>:
   496e0:	stp	x29, x30, [sp, #-48]!
   496e4:	stp	x20, x19, [sp, #32]
   496e8:	mov	x19, x0
   496ec:	mov	x0, x4
   496f0:	str	x21, [sp, #16]
   496f4:	mov	x29, sp
   496f8:	mov	x20, x4
   496fc:	mov	x21, x3
   49700:	bl	c990 <__gmpn_mul_n@plt>
   49704:	add	x2, x20, x21, lsl #3
   49708:	mov	x0, x19
   4970c:	mov	x1, x20
   49710:	mov	x3, x21
   49714:	bl	ca70 <__gmpn_add_n@plt>
   49718:	ldr	x8, [x19]
   4971c:	adds	x8, x8, x0
   49720:	str	x8, [x19]
   49724:	b.cc	4973c <__gmpn_bc_mulmod_bnm1@@Base+0x5c>  // b.lo, b.ul, b.last
   49728:	add	x8, x19, #0x8
   4972c:	ldr	x9, [x8]
   49730:	adds	x9, x9, #0x1
   49734:	str	x9, [x8], #8
   49738:	b.cs	4972c <__gmpn_bc_mulmod_bnm1@@Base+0x4c>  // b.hs, b.nlast
   4973c:	ldp	x20, x19, [sp, #32]
   49740:	ldr	x21, [sp, #16]
   49744:	ldp	x29, x30, [sp], #48
   49748:	ret

000000000004974c <__gmpn_mulmod_bnm1@@Base>:
   4974c:	sub	sp, sp, #0x80
   49750:	stp	x28, x27, [sp, #48]
   49754:	stp	x26, x25, [sp, #64]
   49758:	stp	x22, x21, [sp, #96]
   4975c:	stp	x20, x19, [sp, #112]
   49760:	mov	x20, x6
   49764:	mov	x22, x5
   49768:	mov	x25, x4
   4976c:	mov	x28, x3
   49770:	mov	x26, x2
   49774:	mov	x21, x1
   49778:	cmp	x1, #0xa
   4977c:	mov	x19, x0
   49780:	stp	x29, x30, [sp, #32]
   49784:	stp	x24, x23, [sp, #80]
   49788:	add	x29, sp, #0x20
   4978c:	b.lt	497fc <__gmpn_mulmod_bnm1@@Base+0xb0>  // b.tstop
   49790:	tbnz	w21, #0, 497fc <__gmpn_mulmod_bnm1@@Base+0xb0>
   49794:	lsr	x24, x21, #1
   49798:	cmp	x24, x28
   4979c:	lsl	x23, x24, #3
   497a0:	stur	x28, [x29, #-8]
   497a4:	str	x23, [sp, #16]
   497a8:	b.ge	49d18 <__gmpn_mulmod_bnm1@@Base+0x5cc>  // b.tcont
   497ac:	subs	x28, x28, x24
   497b0:	add	x2, x26, x24, lsl #3
   497b4:	str	x2, [sp, #8]
   497b8:	b.eq	49854 <__gmpn_mulmod_bnm1@@Base+0x108>  // b.none
   497bc:	mov	x0, x20
   497c0:	mov	x1, x26
   497c4:	mov	x3, x28
   497c8:	bl	ca70 <__gmpn_add_n@plt>
   497cc:	mov	x8, x28
   497d0:	cbz	x0, 49858 <__gmpn_mulmod_bnm1@@Base+0x10c>
   497d4:	mov	x8, x28
   497d8:	cmp	x8, x24
   497dc:	b.ge	498f0 <__gmpn_mulmod_bnm1@@Base+0x1a4>  // b.tcont
   497e0:	lsl	x9, x8, #3
   497e4:	ldr	x10, [x26, x9]
   497e8:	add	x8, x8, #0x1
   497ec:	adds	x10, x10, #0x1
   497f0:	str	x10, [x20, x9]
   497f4:	b.cs	497d8 <__gmpn_mulmod_bnm1@@Base+0x8c>  // b.hs, b.nlast
   497f8:	b	49858 <__gmpn_mulmod_bnm1@@Base+0x10c>
   497fc:	cmp	x22, x21
   49800:	b.lt	49e8c <__gmpn_mulmod_bnm1@@Base+0x740>  // b.tstop
   49804:	mov	x0, x20
   49808:	mov	x1, x26
   4980c:	mov	x2, x25
   49810:	mov	x3, x21
   49814:	bl	c990 <__gmpn_mul_n@plt>
   49818:	add	x2, x20, x21, lsl #3
   4981c:	mov	x0, x19
   49820:	mov	x1, x20
   49824:	mov	x3, x21
   49828:	bl	ca70 <__gmpn_add_n@plt>
   4982c:	ldr	x8, [x19]
   49830:	adds	x8, x8, x0
   49834:	str	x8, [x19]
   49838:	b.cc	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.lo, b.ul, b.last
   4983c:	add	x8, x19, #0x8
   49840:	ldr	x9, [x8]
   49844:	adds	x9, x9, #0x1
   49848:	str	x9, [x8], #8
   4984c:	b.cs	49840 <__gmpn_mulmod_bnm1@@Base+0xf4>  // b.hs, b.nlast
   49850:	b	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>
   49854:	mov	x8, xzr
   49858:	cmp	x20, x26
   4985c:	b.eq	49904 <__gmpn_mulmod_bnm1@@Base+0x1b8>  // b.none
   49860:	subs	x9, x24, x8
   49864:	b.le	49904 <__gmpn_mulmod_bnm1@@Base+0x1b8>
   49868:	cmp	x9, #0x4
   4986c:	b.cc	498cc <__gmpn_mulmod_bnm1@@Base+0x180>  // b.lo, b.ul, b.last
   49870:	lsl	x11, x8, #3
   49874:	add	x10, x20, x11
   49878:	add	x12, x26, x23
   4987c:	cmp	x10, x12
   49880:	b.cs	49894 <__gmpn_mulmod_bnm1@@Base+0x148>  // b.hs, b.nlast
   49884:	add	x10, x20, x23
   49888:	add	x12, x26, x11
   4988c:	cmp	x12, x10
   49890:	b.cc	498cc <__gmpn_mulmod_bnm1@@Base+0x180>  // b.lo, b.ul, b.last
   49894:	and	x10, x9, #0xfffffffffffffffc
   49898:	add	x12, x11, #0x10
   4989c:	add	x8, x8, x10
   498a0:	add	x11, x26, x12
   498a4:	add	x12, x20, x12
   498a8:	mov	x13, x10
   498ac:	ldp	q0, q1, [x11, #-16]
   498b0:	add	x11, x11, #0x20
   498b4:	subs	x13, x13, #0x4
   498b8:	stp	q0, q1, [x12, #-16]
   498bc:	add	x12, x12, #0x20
   498c0:	b.ne	498ac <__gmpn_mulmod_bnm1@@Base+0x160>  // b.any
   498c4:	cmp	x9, x10
   498c8:	b.eq	49904 <__gmpn_mulmod_bnm1@@Base+0x1b8>  // b.none
   498cc:	lsl	x10, x8, #3
   498d0:	sub	x9, x24, x8
   498d4:	add	x8, x20, x10
   498d8:	add	x10, x26, x10
   498dc:	ldr	x11, [x10], #8
   498e0:	subs	x9, x9, #0x1
   498e4:	str	x11, [x8], #8
   498e8:	b.ne	498dc <__gmpn_mulmod_bnm1@@Base+0x190>  // b.any
   498ec:	b	49904 <__gmpn_mulmod_bnm1@@Base+0x1b8>
   498f0:	mov	x8, x20
   498f4:	ldr	x9, [x8]
   498f8:	adds	x9, x9, #0x1
   498fc:	str	x9, [x8], #8
   49900:	b.cs	498f4 <__gmpn_mulmod_bnm1@@Base+0x1a8>  // b.hs, b.nlast
   49904:	cmp	x24, x22
   49908:	add	x27, x20, x24, lsl #3
   4990c:	b.ge	4a030 <__gmpn_mulmod_bnm1@@Base+0x8e4>  // b.tcont
   49910:	subs	x23, x22, x24
   49914:	b.eq	49950 <__gmpn_mulmod_bnm1@@Base+0x204>  // b.none
   49918:	add	x2, x25, x24, lsl #3
   4991c:	mov	x0, x27
   49920:	mov	x1, x25
   49924:	mov	x3, x23
   49928:	bl	ca70 <__gmpn_add_n@plt>
   4992c:	cbz	x0, 49950 <__gmpn_mulmod_bnm1@@Base+0x204>
   49930:	add	x8, x20, x22, lsl #3
   49934:	cmp	x23, x24
   49938:	b.ge	499e8 <__gmpn_mulmod_bnm1@@Base+0x29c>  // b.tcont
   4993c:	ldr	x9, [x25, x23, lsl #3]
   49940:	add	x23, x23, #0x1
   49944:	adds	x9, x9, #0x1
   49948:	str	x9, [x8], #8
   4994c:	b.cs	49934 <__gmpn_mulmod_bnm1@@Base+0x1e8>  // b.hs, b.nlast
   49950:	cmp	x27, x25
   49954:	b.eq	499fc <__gmpn_mulmod_bnm1@@Base+0x2b0>  // b.none
   49958:	subs	x8, x24, x23
   4995c:	b.le	499fc <__gmpn_mulmod_bnm1@@Base+0x2b0>
   49960:	cmp	x8, #0x4
   49964:	b.cc	499c4 <__gmpn_mulmod_bnm1@@Base+0x278>  // b.lo, b.ul, b.last
   49968:	add	x9, x23, x24
   4996c:	add	x11, x20, x9, lsl #3
   49970:	add	x9, x25, x24, lsl #3
   49974:	cmp	x11, x9
   49978:	add	x10, x25, x23, lsl #3
   4997c:	b.cs	49990 <__gmpn_mulmod_bnm1@@Base+0x244>  // b.hs, b.nlast
   49980:	and	x9, x21, #0x1ffffffffffffffe
   49984:	add	x9, x20, x9, lsl #3
   49988:	cmp	x10, x9
   4998c:	b.cc	499c4 <__gmpn_mulmod_bnm1@@Base+0x278>  // b.lo, b.ul, b.last
   49990:	and	x9, x8, #0xfffffffffffffffc
   49994:	add	x10, x10, #0x10
   49998:	add	x23, x23, x9
   4999c:	add	x11, x11, #0x10
   499a0:	mov	x12, x9
   499a4:	ldp	q0, q1, [x10, #-16]
   499a8:	add	x10, x10, #0x20
   499ac:	subs	x12, x12, #0x4
   499b0:	stp	q0, q1, [x11, #-16]
   499b4:	add	x11, x11, #0x20
   499b8:	b.ne	499a4 <__gmpn_mulmod_bnm1@@Base+0x258>  // b.any
   499bc:	cmp	x8, x9
   499c0:	b.eq	499fc <__gmpn_mulmod_bnm1@@Base+0x2b0>  // b.none
   499c4:	add	x9, x23, x24
   499c8:	sub	x8, x24, x23
   499cc:	add	x9, x20, x9, lsl #3
   499d0:	add	x10, x25, x23, lsl #3
   499d4:	ldr	x11, [x10], #8
   499d8:	subs	x8, x8, #0x1
   499dc:	str	x11, [x9], #8
   499e0:	b.ne	499d4 <__gmpn_mulmod_bnm1@@Base+0x288>  // b.any
   499e4:	b	499fc <__gmpn_mulmod_bnm1@@Base+0x2b0>
   499e8:	mov	x8, x27
   499ec:	ldr	x9, [x8]
   499f0:	adds	x9, x9, #0x1
   499f4:	str	x9, [x8], #8
   499f8:	b.cs	499ec <__gmpn_mulmod_bnm1@@Base+0x2a0>  // b.hs, b.nlast
   499fc:	add	x6, x27, x24, lsl #3
   49a00:	mov	x5, x24
   49a04:	mov	x0, x19
   49a08:	mov	x1, x24
   49a0c:	mov	x2, x20
   49a10:	mov	x3, x24
   49a14:	mov	x4, x27
   49a18:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   49a1c:	and	x23, x21, #0xfffffffffffffffe
   49a20:	add	x8, x20, x23, lsl #3
   49a24:	add	x27, x8, #0x10
   49a28:	cbz	x28, 49a70 <__gmpn_mulmod_bnm1@@Base+0x324>
   49a2c:	ldr	x2, [sp, #8]
   49a30:	mov	x0, x27
   49a34:	mov	x1, x26
   49a38:	mov	x3, x28
   49a3c:	bl	c2d0 <__gmpn_sub_n@plt>
   49a40:	cbz	x0, 49a70 <__gmpn_mulmod_bnm1@@Base+0x324>
   49a44:	ldur	x8, [x29, #-8]
   49a48:	add	x8, x8, x24
   49a4c:	add	x8, x20, x8, lsl #3
   49a50:	add	x8, x8, #0x10
   49a54:	cmp	x28, x24
   49a58:	b.ge	49ce4 <__gmpn_mulmod_bnm1@@Base+0x598>  // b.tcont
   49a5c:	ldr	x9, [x26, x28, lsl #3]
   49a60:	add	x28, x28, #0x1
   49a64:	sub	x10, x9, #0x1
   49a68:	str	x10, [x8], #8
   49a6c:	cbz	x9, 49a54 <__gmpn_mulmod_bnm1@@Base+0x308>
   49a70:	cmp	x27, x26
   49a74:	b.eq	49b10 <__gmpn_mulmod_bnm1@@Base+0x3c4>  // b.none
   49a78:	subs	x8, x24, x28
   49a7c:	b.le	49b10 <__gmpn_mulmod_bnm1@@Base+0x3c4>
   49a80:	cmp	x8, #0x4
   49a84:	b.cc	49aec <__gmpn_mulmod_bnm1@@Base+0x3a0>  // b.lo, b.ul, b.last
   49a88:	add	x9, x28, x23
   49a8c:	add	x11, x20, x9, lsl #3
   49a90:	add	x9, x11, #0x10
   49a94:	add	x10, x26, x24, lsl #3
   49a98:	cmp	x9, x10
   49a9c:	add	x10, x26, x28, lsl #3
   49aa0:	b.cs	49ab8 <__gmpn_mulmod_bnm1@@Base+0x36c>  // b.hs, b.nlast
   49aa4:	mov	w9, #0x18                  	// #24
   49aa8:	madd	x9, x24, x9, x20
   49aac:	add	x9, x9, #0x10
   49ab0:	cmp	x10, x9
   49ab4:	b.cc	49aec <__gmpn_mulmod_bnm1@@Base+0x3a0>  // b.lo, b.ul, b.last
   49ab8:	and	x9, x8, #0xfffffffffffffffc
   49abc:	add	x10, x10, #0x10
   49ac0:	add	x28, x28, x9
   49ac4:	add	x11, x11, #0x20
   49ac8:	mov	x12, x9
   49acc:	ldp	q0, q1, [x10, #-16]
   49ad0:	subs	x12, x12, #0x4
   49ad4:	add	x10, x10, #0x20
   49ad8:	stp	q0, q1, [x11, #-16]
   49adc:	add	x11, x11, #0x20
   49ae0:	b.ne	49acc <__gmpn_mulmod_bnm1@@Base+0x380>  // b.any
   49ae4:	cmp	x8, x9
   49ae8:	b.eq	49b10 <__gmpn_mulmod_bnm1@@Base+0x3c4>  // b.none
   49aec:	add	x9, x28, x23
   49af0:	add	x9, x20, x9, lsl #3
   49af4:	sub	x8, x24, x28
   49af8:	add	x9, x9, #0x10
   49afc:	add	x10, x26, x28, lsl #3
   49b00:	ldr	x11, [x10], #8
   49b04:	subs	x8, x8, #0x1
   49b08:	str	x11, [x9], #8
   49b0c:	b.ne	49b00 <__gmpn_mulmod_bnm1@@Base+0x3b4>  // b.any
   49b10:	mov	x8, xzr
   49b14:	str	xzr, [x27, x24, lsl #3]
   49b18:	cmp	x24, x22
   49b1c:	add	x8, x8, x24
   49b20:	str	x8, [sp, #8]
   49b24:	b.ge	49d10 <__gmpn_mulmod_bnm1@@Base+0x5c4>  // b.tcont
   49b28:	add	x8, x27, x24, lsl #3
   49b2c:	subs	x28, x22, x24
   49b30:	add	x26, x8, #0x8
   49b34:	b.eq	49b7c <__gmpn_mulmod_bnm1@@Base+0x430>  // b.none
   49b38:	add	x2, x25, x24, lsl #3
   49b3c:	mov	x0, x26
   49b40:	mov	x1, x25
   49b44:	mov	x3, x28
   49b48:	bl	c2d0 <__gmpn_sub_n@plt>
   49b4c:	cbz	x0, 49b7c <__gmpn_mulmod_bnm1@@Base+0x430>
   49b50:	add	x8, x22, x23
   49b54:	add	x8, x20, x8, lsl #3
   49b58:	add	x8, x8, #0x18
   49b5c:	mov	w9, #0x1                   	// #1
   49b60:	cmp	x28, x24
   49b64:	b.ge	49c2c <__gmpn_mulmod_bnm1@@Base+0x4e0>  // b.tcont
   49b68:	ldr	x10, [x25, x28, lsl #3]
   49b6c:	add	x28, x28, #0x1
   49b70:	sub	x11, x10, #0x1
   49b74:	str	x11, [x8], #8
   49b78:	cbz	x10, 49b60 <__gmpn_mulmod_bnm1@@Base+0x414>
   49b7c:	cmp	x26, x25
   49b80:	mov	x9, xzr
   49b84:	b.eq	49c2c <__gmpn_mulmod_bnm1@@Base+0x4e0>  // b.none
   49b88:	subs	x8, x24, x28
   49b8c:	b.le	49c2c <__gmpn_mulmod_bnm1@@Base+0x4e0>
   49b90:	cmp	x8, #0x4
   49b94:	b.cc	49c00 <__gmpn_mulmod_bnm1@@Base+0x4b4>  // b.lo, b.ul, b.last
   49b98:	add	x9, x24, x24, lsl #1
   49b9c:	add	x9, x28, x9
   49ba0:	add	x11, x20, x9, lsl #3
   49ba4:	add	x9, x11, #0x18
   49ba8:	add	x10, x25, x24, lsl #3
   49bac:	cmp	x9, x10
   49bb0:	add	x10, x25, x28, lsl #3
   49bb4:	b.cs	49bcc <__gmpn_mulmod_bnm1@@Base+0x480>  // b.hs, b.nlast
   49bb8:	mov	w9, #0x18                  	// #24
   49bbc:	bfi	x9, x24, #5, #59
   49bc0:	add	x9, x20, x9
   49bc4:	cmp	x10, x9
   49bc8:	b.cc	49c00 <__gmpn_mulmod_bnm1@@Base+0x4b4>  // b.lo, b.ul, b.last
   49bcc:	and	x9, x8, #0xfffffffffffffffc
   49bd0:	add	x10, x10, #0x10
   49bd4:	add	x28, x28, x9
   49bd8:	add	x11, x11, #0x28
   49bdc:	mov	x12, x9
   49be0:	ldp	q0, q1, [x10, #-16]
   49be4:	subs	x12, x12, #0x4
   49be8:	add	x10, x10, #0x20
   49bec:	stp	q0, q1, [x11, #-16]
   49bf0:	add	x11, x11, #0x20
   49bf4:	b.ne	49be0 <__gmpn_mulmod_bnm1@@Base+0x494>  // b.any
   49bf8:	cmp	x8, x9
   49bfc:	b.eq	49c28 <__gmpn_mulmod_bnm1@@Base+0x4dc>  // b.none
   49c00:	add	x9, x24, x24, lsl #1
   49c04:	add	x9, x28, x9
   49c08:	add	x9, x20, x9, lsl #3
   49c0c:	sub	x8, x24, x28
   49c10:	add	x9, x9, #0x18
   49c14:	add	x10, x25, x28, lsl #3
   49c18:	ldr	x11, [x10], #8
   49c1c:	subs	x8, x8, #0x1
   49c20:	str	x11, [x9], #8
   49c24:	b.ne	49c18 <__gmpn_mulmod_bnm1@@Base+0x4cc>  // b.any
   49c28:	mov	x9, xzr
   49c2c:	lsl	x8, x21, #3
   49c30:	orr	x8, x8, #0x8
   49c34:	str	xzr, [x27, x8]
   49c38:	ldr	x8, [x26]
   49c3c:	adds	x8, x8, x9
   49c40:	str	x8, [x26]
   49c44:	b.cc	49c64 <__gmpn_mulmod_bnm1@@Base+0x518>  // b.lo, b.ul, b.last
   49c48:	mov	w8, #0x18                  	// #24
   49c4c:	madd	x8, x24, x8, x20
   49c50:	add	x8, x8, #0x20
   49c54:	ldr	x9, [x8]
   49c58:	adds	x9, x9, #0x1
   49c5c:	str	x9, [x8], #8
   49c60:	b.cs	49c54 <__gmpn_mulmod_bnm1@@Base+0x508>  // b.hs, b.nlast
   49c64:	ldr	x8, [x26, x24, lsl #3]
   49c68:	add	x28, x8, x24
   49c6c:	cmp	x21, #0x278
   49c70:	b.lt	49d50 <__gmpn_mulmod_bnm1@@Base+0x604>  // b.tstop
   49c74:	mov	x0, x24
   49c78:	mov	w1, wzr
   49c7c:	bl	cad0 <__gmpn_fft_best_k@plt>
   49c80:	mov	w8, #0xffffffff            	// #-1
   49c84:	lsl	w8, w8, w0
   49c88:	mvn	w8, w8
   49c8c:	sxtw	x9, w8
   49c90:	mov	w6, w0
   49c94:	tst	x24, x9
   49c98:	b.eq	49cb0 <__gmpn_mulmod_bnm1@@Base+0x564>  // b.none
   49c9c:	sbfx	x9, x8, #1, #31
   49ca0:	asr	w8, w8, #1
   49ca4:	tst	x24, x9
   49ca8:	sub	w6, w6, #0x1
   49cac:	b.ne	49c9c <__gmpn_mulmod_bnm1@@Base+0x550>  // b.any
   49cb0:	cmp	w6, #0x4
   49cb4:	b.lt	49d50 <__gmpn_mulmod_bnm1@@Base+0x604>  // b.tstop
   49cb8:	ldr	x3, [sp, #8]
   49cbc:	mov	x0, x20
   49cc0:	mov	x1, x24
   49cc4:	mov	x2, x27
   49cc8:	mov	x4, x26
   49ccc:	mov	x5, x28
   49cd0:	bl	c300 <__gmpn_mul_fft@plt>
   49cd4:	ldur	x26, [x29, #-8]
   49cd8:	ldr	x27, [sp, #16]
   49cdc:	str	x0, [x20, x24, lsl #3]
   49ce0:	b	49dc0 <__gmpn_mulmod_bnm1@@Base+0x674>
   49ce4:	mov	x8, x27
   49ce8:	str	xzr, [x27, x24, lsl #3]
   49cec:	ldr	x9, [x8]
   49cf0:	adds	x9, x9, #0x1
   49cf4:	str	x9, [x8], #8
   49cf8:	b.cs	49cec <__gmpn_mulmod_bnm1@@Base+0x5a0>  // b.hs, b.nlast
   49cfc:	ldr	x8, [x27, x24, lsl #3]
   49d00:	cmp	x24, x22
   49d04:	add	x8, x8, x24
   49d08:	str	x8, [sp, #8]
   49d0c:	b.lt	49b28 <__gmpn_mulmod_bnm1@@Base+0x3dc>  // b.tstop
   49d10:	mov	x26, x25
   49d14:	b	49d44 <__gmpn_mulmod_bnm1@@Base+0x5f8>
   49d18:	mov	x0, x19
   49d1c:	mov	x1, x24
   49d20:	mov	x2, x26
   49d24:	mov	x3, x28
   49d28:	mov	x4, x25
   49d2c:	mov	x5, x22
   49d30:	mov	x6, x20
   49d34:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   49d38:	mov	x27, x26
   49d3c:	mov	x26, x25
   49d40:	str	x28, [sp, #8]
   49d44:	mov	x28, x22
   49d48:	cmp	x21, #0x278
   49d4c:	b.ge	49c74 <__gmpn_mulmod_bnm1@@Base+0x528>  // b.tcont
   49d50:	cmp	x26, x25
   49d54:	b.eq	4a040 <__gmpn_mulmod_bnm1@@Base+0x8f4>  // b.none
   49d58:	add	x3, x24, #0x1
   49d5c:	mov	x0, x20
   49d60:	mov	x1, x27
   49d64:	mov	x2, x26
   49d68:	bl	c990 <__gmpn_mul_n@plt>
   49d6c:	and	x8, x21, #0x1ffffffffffffffe
   49d70:	ldr	x25, [x20, x8, lsl #3]
   49d74:	add	x23, x20, x24, lsl #3
   49d78:	mov	x0, x20
   49d7c:	mov	x1, x20
   49d80:	mov	x2, x23
   49d84:	mov	x3, x24
   49d88:	bl	c2d0 <__gmpn_sub_n@plt>
   49d8c:	str	xzr, [x23]
   49d90:	ldr	x8, [x20]
   49d94:	ldur	x26, [x29, #-8]
   49d98:	ldr	x27, [sp, #16]
   49d9c:	add	x9, x0, x25
   49da0:	adds	x8, x8, x9
   49da4:	str	x8, [x20]
   49da8:	b.cc	49dc0 <__gmpn_mulmod_bnm1@@Base+0x674>  // b.lo, b.ul, b.last
   49dac:	add	x8, x20, #0x8
   49db0:	ldr	x9, [x8]
   49db4:	adds	x9, x9, #0x1
   49db8:	str	x9, [x8], #8
   49dbc:	b.cs	49db0 <__gmpn_mulmod_bnm1@@Base+0x664>  // b.hs, b.nlast
   49dc0:	ldr	x23, [x20, x27]
   49dc4:	mov	x0, x19
   49dc8:	mov	x1, x19
   49dcc:	mov	x2, x20
   49dd0:	mov	x3, x24
   49dd4:	bl	c950 <__gmpn_rsh1add_n@plt>
   49dd8:	mov	x8, xzr
   49ddc:	add	x9, x27, x19
   49de0:	add	x11, x0, x23
   49de4:	ldur	x10, [x9, #-8]
   49de8:	lsr	x12, x11, #1
   49dec:	lsl	x11, x11, #63
   49df0:	adds	x13, x10, x11
   49df4:	adc	x8, x12, x8
   49df8:	stur	x13, [x9, #-8]
   49dfc:	ldr	x9, [x19]
   49e00:	adds	x8, x9, x8
   49e04:	str	x8, [x19]
   49e08:	b.cc	49e20 <__gmpn_mulmod_bnm1@@Base+0x6d4>  // b.lo, b.ul, b.last
   49e0c:	add	x8, x19, #0x8
   49e10:	ldr	x9, [x8]
   49e14:	adds	x9, x9, #0x1
   49e18:	str	x9, [x8], #8
   49e1c:	b.cs	49e10 <__gmpn_mulmod_bnm1@@Base+0x6c4>  // b.hs, b.nlast
   49e20:	add	x23, x22, x26
   49e24:	cmp	x23, x21
   49e28:	b.lt	49fa4 <__gmpn_mulmod_bnm1@@Base+0x858>  // b.tstop
   49e2c:	ldr	x21, [x20, x27]
   49e30:	add	x0, x19, x27
   49e34:	mov	x1, x19
   49e38:	mov	x2, x20
   49e3c:	mov	x3, x24
   49e40:	bl	c2d0 <__gmpn_sub_n@plt>
   49e44:	ldr	x8, [x19]
   49e48:	add	x9, x0, x21
   49e4c:	subs	x8, x8, x9
   49e50:	str	x8, [x19]
   49e54:	b.cs	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.hs, b.nlast
   49e58:	add	x8, x19, #0x8
   49e5c:	ldr	x9, [x8]
   49e60:	sub	x10, x9, #0x1
   49e64:	str	x10, [x8], #8
   49e68:	cbz	x9, 49e5c <__gmpn_mulmod_bnm1@@Base+0x710>
   49e6c:	ldp	x20, x19, [sp, #112]
   49e70:	ldp	x22, x21, [sp, #96]
   49e74:	ldp	x24, x23, [sp, #80]
   49e78:	ldp	x26, x25, [sp, #64]
   49e7c:	ldp	x28, x27, [sp, #48]
   49e80:	ldp	x29, x30, [sp, #32]
   49e84:	add	sp, sp, #0x80
   49e88:	ret
   49e8c:	add	x23, x22, x28
   49e90:	cmp	x23, x21
   49e94:	b.le	4a0dc <__gmpn_mulmod_bnm1@@Base+0x990>
   49e98:	mov	x0, x20
   49e9c:	mov	x1, x26
   49ea0:	mov	x2, x28
   49ea4:	mov	x3, x25
   49ea8:	mov	x4, x22
   49eac:	bl	ccd0 <__gmpn_mul@plt>
   49eb0:	subs	x22, x23, x21
   49eb4:	b.eq	49ef0 <__gmpn_mulmod_bnm1@@Base+0x7a4>  // b.none
   49eb8:	add	x2, x20, x21, lsl #3
   49ebc:	mov	x0, x19
   49ec0:	mov	x1, x20
   49ec4:	mov	x3, x22
   49ec8:	bl	ca70 <__gmpn_add_n@plt>
   49ecc:	cbz	x0, 49ef0 <__gmpn_mulmod_bnm1@@Base+0x7a4>
   49ed0:	cmp	x22, x21
   49ed4:	b.ge	49f90 <__gmpn_mulmod_bnm1@@Base+0x844>  // b.tcont
   49ed8:	lsl	x8, x22, #3
   49edc:	ldr	x9, [x20, x8]
   49ee0:	add	x22, x22, #0x1
   49ee4:	adds	x9, x9, #0x1
   49ee8:	str	x9, [x19, x8]
   49eec:	b.cs	49ed0 <__gmpn_mulmod_bnm1@@Base+0x784>  // b.hs, b.nlast
   49ef0:	cmp	x19, x20
   49ef4:	b.eq	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.none
   49ef8:	cmp	x22, x21
   49efc:	b.ge	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.tcont
   49f00:	sub	x8, x21, x22
   49f04:	cmp	x8, #0x4
   49f08:	b.cc	49f6c <__gmpn_mulmod_bnm1@@Base+0x820>  // b.lo, b.ul, b.last
   49f0c:	lsl	x10, x22, #3
   49f10:	lsl	x9, x21, #3
   49f14:	add	x11, x19, x10
   49f18:	add	x12, x20, x9
   49f1c:	cmp	x11, x12
   49f20:	b.cs	49f34 <__gmpn_mulmod_bnm1@@Base+0x7e8>  // b.hs, b.nlast
   49f24:	add	x9, x19, x9
   49f28:	add	x11, x20, x10
   49f2c:	cmp	x11, x9
   49f30:	b.cc	49f6c <__gmpn_mulmod_bnm1@@Base+0x820>  // b.lo, b.ul, b.last
   49f34:	and	x9, x8, #0xfffffffffffffffc
   49f38:	add	x11, x10, #0x10
   49f3c:	add	x22, x22, x9
   49f40:	add	x10, x20, x11
   49f44:	add	x11, x19, x11
   49f48:	mov	x12, x9
   49f4c:	ldp	q0, q1, [x10, #-16]
   49f50:	add	x10, x10, #0x20
   49f54:	subs	x12, x12, #0x4
   49f58:	stp	q0, q1, [x11, #-16]
   49f5c:	add	x11, x11, #0x20
   49f60:	b.ne	49f4c <__gmpn_mulmod_bnm1@@Base+0x800>  // b.any
   49f64:	cmp	x8, x9
   49f68:	b.eq	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.none
   49f6c:	lsl	x10, x22, #3
   49f70:	sub	x8, x21, x22
   49f74:	add	x9, x19, x10
   49f78:	add	x10, x20, x10
   49f7c:	ldr	x11, [x10], #8
   49f80:	subs	x8, x8, #0x1
   49f84:	str	x11, [x9], #8
   49f88:	b.ne	49f7c <__gmpn_mulmod_bnm1@@Base+0x830>  // b.any
   49f8c:	b	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>
   49f90:	ldr	x8, [x19]
   49f94:	adds	x8, x8, #0x1
   49f98:	str	x8, [x19], #8
   49f9c:	b.cs	49f90 <__gmpn_mulmod_bnm1@@Base+0x844>  // b.hs, b.nlast
   49fa0:	b	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>
   49fa4:	add	x0, x19, x27
   49fa8:	sub	x3, x23, x24
   49fac:	mov	x1, x19
   49fb0:	mov	x2, x20
   49fb4:	neg	x25, x24
   49fb8:	bl	c2d0 <__gmpn_sub_n@plt>
   49fbc:	lsl	x8, x26, #3
   49fc0:	lsl	x9, x22, #3
   49fc4:	add	x11, x20, x8
   49fc8:	ldr	x24, [x20, x27]
   49fcc:	lsl	x10, x25, #3
   49fd0:	add	x8, x19, x8
   49fd4:	add	x11, x11, x9
   49fd8:	mov	x4, x0
   49fdc:	add	x8, x8, x9
   49fe0:	add	x0, x11, x10
   49fe4:	add	x1, x8, x10
   49fe8:	sub	x3, x21, x23
   49fec:	mov	x2, x0
   49ff0:	bl	c760 <__gmpn_sub_nc@plt>
   49ff4:	ldr	x8, [x19]
   49ff8:	add	x9, x0, x24
   49ffc:	subs	x8, x8, x9
   4a000:	str	x8, [x19]
   4a004:	b.cs	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.hs, b.nlast
   4a008:	mov	w8, #0x1                   	// #1
   4a00c:	cmp	x8, x23
   4a010:	b.ge	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>  // b.tcont
   4a014:	lsl	x9, x8, #3
   4a018:	ldr	x10, [x19, x9]
   4a01c:	add	x8, x8, #0x1
   4a020:	sub	x11, x10, #0x1
   4a024:	str	x11, [x19, x9]
   4a028:	cbz	x10, 4a00c <__gmpn_mulmod_bnm1@@Base+0x8c0>
   4a02c:	b	49e6c <__gmpn_mulmod_bnm1@@Base+0x720>
   4a030:	mov	x6, x27
   4a034:	mov	x5, x22
   4a038:	mov	x27, x25
   4a03c:	b	49a04 <__gmpn_mulmod_bnm1@@Base+0x2b8>
   4a040:	ldr	x23, [sp, #8]
   4a044:	mov	x0, x20
   4a048:	mov	x1, x27
   4a04c:	mov	x3, x25
   4a050:	mov	x2, x23
   4a054:	mov	x4, x28
   4a058:	bl	ccd0 <__gmpn_mul@plt>
   4a05c:	sub	x8, x23, x24
   4a060:	add	x8, x8, x28
   4a064:	ldur	x26, [x29, #-8]
   4a068:	ldr	x27, [sp, #16]
   4a06c:	cmp	x8, x24
   4a070:	cset	w9, gt
   4a074:	subs	x25, x8, x9
   4a078:	add	x23, x20, x24, lsl #3
   4a07c:	b.eq	4a0b8 <__gmpn_mulmod_bnm1@@Base+0x96c>  // b.none
   4a080:	mov	x0, x20
   4a084:	mov	x1, x20
   4a088:	mov	x2, x23
   4a08c:	mov	x3, x25
   4a090:	bl	c2d0 <__gmpn_sub_n@plt>
   4a094:	cbz	x0, 4a0b8 <__gmpn_mulmod_bnm1@@Base+0x96c>
   4a098:	cmp	x25, x24
   4a09c:	b.ge	4a0c0 <__gmpn_mulmod_bnm1@@Base+0x974>  // b.tcont
   4a0a0:	lsl	x8, x25, #3
   4a0a4:	ldr	x9, [x20, x8]
   4a0a8:	add	x25, x25, #0x1
   4a0ac:	sub	x10, x9, #0x1
   4a0b0:	str	x10, [x20, x8]
   4a0b4:	cbz	x9, 4a098 <__gmpn_mulmod_bnm1@@Base+0x94c>
   4a0b8:	str	xzr, [x23]
   4a0bc:	b	49dc0 <__gmpn_mulmod_bnm1@@Base+0x674>
   4a0c0:	mov	x8, x20
   4a0c4:	str	xzr, [x23]
   4a0c8:	ldr	x9, [x8]
   4a0cc:	adds	x9, x9, #0x1
   4a0d0:	str	x9, [x8], #8
   4a0d4:	b.cs	4a0c8 <__gmpn_mulmod_bnm1@@Base+0x97c>  // b.hs, b.nlast
   4a0d8:	b	49dc0 <__gmpn_mulmod_bnm1@@Base+0x674>
   4a0dc:	mov	x0, x19
   4a0e0:	mov	x1, x26
   4a0e4:	mov	x2, x28
   4a0e8:	mov	x3, x25
   4a0ec:	mov	x4, x22
   4a0f0:	ldp	x20, x19, [sp, #112]
   4a0f4:	ldp	x22, x21, [sp, #96]
   4a0f8:	ldp	x24, x23, [sp, #80]
   4a0fc:	ldp	x26, x25, [sp, #64]
   4a100:	ldp	x28, x27, [sp, #48]
   4a104:	ldp	x29, x30, [sp, #32]
   4a108:	add	sp, sp, #0x80
   4a10c:	b	ccd0 <__gmpn_mul@plt>

000000000004a110 <__gmpn_mulmod_bnm1_next_size@@Base>:
   4a110:	cmp	x0, #0xa
   4a114:	b.lt	4a168 <__gmpn_mulmod_bnm1_next_size@@Base+0x58>  // b.tstop
   4a118:	cmp	x0, #0x24
   4a11c:	b.le	4a16c <__gmpn_mulmod_bnm1_next_size@@Base+0x5c>
   4a120:	cmp	x0, #0x48
   4a124:	b.le	4a178 <__gmpn_mulmod_bnm1_next_size@@Base+0x68>
   4a128:	cmp	x0, #0x276
   4a12c:	b.le	4a184 <__gmpn_mulmod_bnm1_next_size@@Base+0x74>
   4a130:	stp	x29, x30, [sp, #-32]!
   4a134:	add	x8, x0, #0x1
   4a138:	str	x19, [sp, #16]
   4a13c:	asr	x19, x8, #1
   4a140:	mov	x0, x19
   4a144:	mov	w1, wzr
   4a148:	mov	x29, sp
   4a14c:	bl	cad0 <__gmpn_fft_best_k@plt>
   4a150:	mov	w1, w0
   4a154:	mov	x0, x19
   4a158:	bl	d1d0 <__gmpn_fft_next_size@plt>
   4a15c:	ldr	x19, [sp, #16]
   4a160:	lsl	x0, x0, #1
   4a164:	ldp	x29, x30, [sp], #32
   4a168:	ret
   4a16c:	add	x8, x0, #0x1
   4a170:	and	x0, x8, #0xfffffffffffffffe
   4a174:	ret
   4a178:	add	x8, x0, #0x3
   4a17c:	and	x0, x8, #0xfffffffffffffffc
   4a180:	ret
   4a184:	add	x8, x0, #0x7
   4a188:	and	x0, x8, #0xfffffffffffffff8
   4a18c:	ret

000000000004a190 <__gmpn_sqrmod_bnm1@@Base>:
   4a190:	sub	sp, sp, #0x70
   4a194:	stp	x28, x27, [sp, #32]
   4a198:	stp	x24, x23, [sp, #64]
   4a19c:	stp	x22, x21, [sp, #80]
   4a1a0:	stp	x20, x19, [sp, #96]
   4a1a4:	mov	x20, x4
   4a1a8:	mov	x27, x3
   4a1ac:	mov	x24, x2
   4a1b0:	mov	x21, x1
   4a1b4:	cmp	x1, #0xb
   4a1b8:	mov	x19, x0
   4a1bc:	stp	x29, x30, [sp, #16]
   4a1c0:	stp	x26, x25, [sp, #48]
   4a1c4:	add	x29, sp, #0x10
   4a1c8:	b.lt	4a238 <__gmpn_sqrmod_bnm1@@Base+0xa8>  // b.tstop
   4a1cc:	tbnz	w21, #0, 4a238 <__gmpn_sqrmod_bnm1@@Base+0xa8>
   4a1d0:	lsr	x23, x21, #1
   4a1d4:	cmp	x23, x27
   4a1d8:	lsl	x28, x23, #3
   4a1dc:	b.ge	4a630 <__gmpn_sqrmod_bnm1@@Base+0x4a0>  // b.tcont
   4a1e0:	add	x26, x20, x28
   4a1e4:	str	x27, [sp, #8]
   4a1e8:	subs	x25, x27, x23
   4a1ec:	add	x27, x24, x28
   4a1f0:	b.eq	4a28c <__gmpn_sqrmod_bnm1@@Base+0xfc>  // b.none
   4a1f4:	mov	x0, x20
   4a1f8:	mov	x1, x24
   4a1fc:	mov	x2, x27
   4a200:	mov	x3, x25
   4a204:	bl	ca70 <__gmpn_add_n@plt>
   4a208:	mov	x8, x25
   4a20c:	cbz	x0, 4a290 <__gmpn_sqrmod_bnm1@@Base+0x100>
   4a210:	mov	x8, x25
   4a214:	cmp	x8, x23
   4a218:	b.ge	4a328 <__gmpn_sqrmod_bnm1@@Base+0x198>  // b.tcont
   4a21c:	lsl	x9, x8, #3
   4a220:	ldr	x10, [x24, x9]
   4a224:	add	x8, x8, #0x1
   4a228:	adds	x10, x10, #0x1
   4a22c:	str	x10, [x20, x9]
   4a230:	b.cs	4a214 <__gmpn_sqrmod_bnm1@@Base+0x84>  // b.hs, b.nlast
   4a234:	b	4a290 <__gmpn_sqrmod_bnm1@@Base+0x100>
   4a238:	cmp	x27, x21
   4a23c:	b.lt	4a654 <__gmpn_sqrmod_bnm1@@Base+0x4c4>  // b.tstop
   4a240:	mov	x0, x20
   4a244:	mov	x1, x24
   4a248:	mov	x2, x21
   4a24c:	bl	c8e0 <__gmpn_sqr@plt>
   4a250:	add	x2, x20, x21, lsl #3
   4a254:	mov	x0, x19
   4a258:	mov	x1, x20
   4a25c:	mov	x3, x21
   4a260:	bl	ca70 <__gmpn_add_n@plt>
   4a264:	ldr	x8, [x19]
   4a268:	adds	x8, x8, x0
   4a26c:	str	x8, [x19]
   4a270:	b.cc	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.lo, b.ul, b.last
   4a274:	add	x8, x19, #0x8
   4a278:	ldr	x9, [x8]
   4a27c:	adds	x9, x9, #0x1
   4a280:	str	x9, [x8], #8
   4a284:	b.cs	4a278 <__gmpn_sqrmod_bnm1@@Base+0xe8>  // b.hs, b.nlast
   4a288:	b	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>
   4a28c:	mov	x8, xzr
   4a290:	cmp	x20, x24
   4a294:	b.eq	4a33c <__gmpn_sqrmod_bnm1@@Base+0x1ac>  // b.none
   4a298:	subs	x9, x23, x8
   4a29c:	b.le	4a33c <__gmpn_sqrmod_bnm1@@Base+0x1ac>
   4a2a0:	cmp	x9, #0x4
   4a2a4:	b.cc	4a304 <__gmpn_sqrmod_bnm1@@Base+0x174>  // b.lo, b.ul, b.last
   4a2a8:	lsl	x11, x8, #3
   4a2ac:	add	x10, x20, x11
   4a2b0:	add	x12, x24, x28
   4a2b4:	cmp	x10, x12
   4a2b8:	b.cs	4a2cc <__gmpn_sqrmod_bnm1@@Base+0x13c>  // b.hs, b.nlast
   4a2bc:	add	x10, x20, x28
   4a2c0:	add	x12, x24, x11
   4a2c4:	cmp	x12, x10
   4a2c8:	b.cc	4a304 <__gmpn_sqrmod_bnm1@@Base+0x174>  // b.lo, b.ul, b.last
   4a2cc:	and	x10, x9, #0xfffffffffffffffc
   4a2d0:	add	x12, x11, #0x10
   4a2d4:	add	x8, x8, x10
   4a2d8:	add	x11, x24, x12
   4a2dc:	add	x12, x20, x12
   4a2e0:	mov	x13, x10
   4a2e4:	ldp	q0, q1, [x11, #-16]
   4a2e8:	add	x11, x11, #0x20
   4a2ec:	subs	x13, x13, #0x4
   4a2f0:	stp	q0, q1, [x12, #-16]
   4a2f4:	add	x12, x12, #0x20
   4a2f8:	b.ne	4a2e4 <__gmpn_sqrmod_bnm1@@Base+0x154>  // b.any
   4a2fc:	cmp	x9, x10
   4a300:	b.eq	4a33c <__gmpn_sqrmod_bnm1@@Base+0x1ac>  // b.none
   4a304:	lsl	x10, x8, #3
   4a308:	sub	x9, x23, x8
   4a30c:	add	x8, x20, x10
   4a310:	add	x10, x24, x10
   4a314:	ldr	x11, [x10], #8
   4a318:	subs	x9, x9, #0x1
   4a31c:	str	x11, [x8], #8
   4a320:	b.ne	4a314 <__gmpn_sqrmod_bnm1@@Base+0x184>  // b.any
   4a324:	b	4a33c <__gmpn_sqrmod_bnm1@@Base+0x1ac>
   4a328:	mov	x8, x20
   4a32c:	ldr	x9, [x8]
   4a330:	adds	x9, x9, #0x1
   4a334:	str	x9, [x8], #8
   4a338:	b.cs	4a32c <__gmpn_sqrmod_bnm1@@Base+0x19c>  // b.hs, b.nlast
   4a33c:	mov	x0, x19
   4a340:	mov	x1, x23
   4a344:	mov	x2, x20
   4a348:	mov	x3, x23
   4a34c:	mov	x4, x26
   4a350:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   4a354:	and	x22, x21, #0xfffffffffffffffe
   4a358:	add	x8, x20, x22, lsl #3
   4a35c:	add	x26, x8, #0x10
   4a360:	cbz	x25, 4a3ac <__gmpn_sqrmod_bnm1@@Base+0x21c>
   4a364:	mov	x0, x26
   4a368:	mov	x1, x24
   4a36c:	mov	x2, x27
   4a370:	mov	x3, x25
   4a374:	bl	c2d0 <__gmpn_sub_n@plt>
   4a378:	cbz	x0, 4a3bc <__gmpn_sqrmod_bnm1@@Base+0x22c>
   4a37c:	ldr	x27, [sp, #8]
   4a380:	add	x8, x27, x23
   4a384:	add	x8, x20, x8, lsl #3
   4a388:	add	x8, x8, #0x10
   4a38c:	cmp	x25, x23
   4a390:	b.ge	4a60c <__gmpn_sqrmod_bnm1@@Base+0x47c>  // b.tcont
   4a394:	ldr	x9, [x24, x25, lsl #3]
   4a398:	add	x25, x25, #0x1
   4a39c:	sub	x10, x9, #0x1
   4a3a0:	str	x10, [x8], #8
   4a3a4:	cbz	x9, 4a38c <__gmpn_sqrmod_bnm1@@Base+0x1fc>
   4a3a8:	b	4a3b0 <__gmpn_sqrmod_bnm1@@Base+0x220>
   4a3ac:	ldr	x27, [sp, #8]
   4a3b0:	cmp	x26, x24
   4a3b4:	b.ne	4a3c8 <__gmpn_sqrmod_bnm1@@Base+0x238>  // b.any
   4a3b8:	b	4a460 <__gmpn_sqrmod_bnm1@@Base+0x2d0>
   4a3bc:	ldr	x27, [sp, #8]
   4a3c0:	cmp	x26, x24
   4a3c4:	b.eq	4a460 <__gmpn_sqrmod_bnm1@@Base+0x2d0>  // b.none
   4a3c8:	subs	x8, x23, x25
   4a3cc:	b.le	4a460 <__gmpn_sqrmod_bnm1@@Base+0x2d0>
   4a3d0:	cmp	x8, #0x4
   4a3d4:	b.cc	4a43c <__gmpn_sqrmod_bnm1@@Base+0x2ac>  // b.lo, b.ul, b.last
   4a3d8:	add	x9, x25, x22
   4a3dc:	add	x11, x20, x9, lsl #3
   4a3e0:	add	x9, x11, #0x10
   4a3e4:	add	x10, x24, x23, lsl #3
   4a3e8:	cmp	x9, x10
   4a3ec:	add	x10, x24, x25, lsl #3
   4a3f0:	b.cs	4a408 <__gmpn_sqrmod_bnm1@@Base+0x278>  // b.hs, b.nlast
   4a3f4:	mov	w9, #0x18                  	// #24
   4a3f8:	madd	x9, x23, x9, x20
   4a3fc:	add	x9, x9, #0x10
   4a400:	cmp	x10, x9
   4a404:	b.cc	4a43c <__gmpn_sqrmod_bnm1@@Base+0x2ac>  // b.lo, b.ul, b.last
   4a408:	and	x9, x8, #0xfffffffffffffffc
   4a40c:	add	x10, x10, #0x10
   4a410:	add	x25, x25, x9
   4a414:	add	x11, x11, #0x20
   4a418:	mov	x12, x9
   4a41c:	ldp	q0, q1, [x10, #-16]
   4a420:	subs	x12, x12, #0x4
   4a424:	add	x10, x10, #0x20
   4a428:	stp	q0, q1, [x11, #-16]
   4a42c:	add	x11, x11, #0x20
   4a430:	b.ne	4a41c <__gmpn_sqrmod_bnm1@@Base+0x28c>  // b.any
   4a434:	cmp	x8, x9
   4a438:	b.eq	4a460 <__gmpn_sqrmod_bnm1@@Base+0x2d0>  // b.none
   4a43c:	add	x9, x25, x22
   4a440:	add	x9, x20, x9, lsl #3
   4a444:	sub	x8, x23, x25
   4a448:	add	x9, x9, #0x10
   4a44c:	add	x10, x24, x25, lsl #3
   4a450:	ldr	x11, [x10], #8
   4a454:	subs	x8, x8, #0x1
   4a458:	str	x11, [x9], #8
   4a45c:	b.ne	4a450 <__gmpn_sqrmod_bnm1@@Base+0x2c0>  // b.any
   4a460:	mov	x8, xzr
   4a464:	str	xzr, [x26, x23, lsl #3]
   4a468:	add	x25, xzr, x23
   4a46c:	cmp	x21, #0x278
   4a470:	b.lt	4a4dc <__gmpn_sqrmod_bnm1@@Base+0x34c>  // b.tstop
   4a474:	mov	w1, #0x1                   	// #1
   4a478:	mov	x0, x23
   4a47c:	bl	cad0 <__gmpn_fft_best_k@plt>
   4a480:	mov	w8, #0xffffffff            	// #-1
   4a484:	lsl	w8, w8, w0
   4a488:	mvn	w8, w8
   4a48c:	sxtw	x9, w8
   4a490:	mov	w6, w0
   4a494:	tst	x23, x9
   4a498:	b.eq	4a4b0 <__gmpn_sqrmod_bnm1@@Base+0x320>  // b.none
   4a49c:	sbfx	x9, x8, #1, #31
   4a4a0:	asr	w8, w8, #1
   4a4a4:	tst	x23, x9
   4a4a8:	sub	w6, w6, #0x1
   4a4ac:	b.ne	4a49c <__gmpn_sqrmod_bnm1@@Base+0x30c>  // b.any
   4a4b0:	cmp	w6, #0x4
   4a4b4:	b.lt	4a4dc <__gmpn_sqrmod_bnm1@@Base+0x34c>  // b.tstop
   4a4b8:	mov	x0, x20
   4a4bc:	mov	x1, x23
   4a4c0:	mov	x2, x26
   4a4c4:	mov	x3, x25
   4a4c8:	mov	x4, x26
   4a4cc:	mov	x5, x25
   4a4d0:	bl	c300 <__gmpn_mul_fft@plt>
   4a4d4:	str	x0, [x20, x23, lsl #3]
   4a4d8:	b	4a540 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   4a4dc:	cmp	x26, x24
   4a4e0:	b.eq	4a7dc <__gmpn_sqrmod_bnm1@@Base+0x64c>  // b.none
   4a4e4:	add	x2, x23, #0x1
   4a4e8:	mov	x0, x20
   4a4ec:	mov	x1, x26
   4a4f0:	bl	c8e0 <__gmpn_sqr@plt>
   4a4f4:	and	x8, x21, #0x1ffffffffffffffe
   4a4f8:	ldr	x22, [x20, x8, lsl #3]
   4a4fc:	add	x24, x20, x23, lsl #3
   4a500:	mov	x0, x20
   4a504:	mov	x1, x20
   4a508:	mov	x2, x24
   4a50c:	mov	x3, x23
   4a510:	bl	c2d0 <__gmpn_sub_n@plt>
   4a514:	str	xzr, [x24]
   4a518:	ldr	x8, [x20]
   4a51c:	add	x9, x0, x22
   4a520:	adds	x8, x8, x9
   4a524:	str	x8, [x20]
   4a528:	b.cc	4a540 <__gmpn_sqrmod_bnm1@@Base+0x3b0>  // b.lo, b.ul, b.last
   4a52c:	add	x8, x20, #0x8
   4a530:	ldr	x9, [x8]
   4a534:	adds	x9, x9, #0x1
   4a538:	str	x9, [x8], #8
   4a53c:	b.cs	4a530 <__gmpn_sqrmod_bnm1@@Base+0x3a0>  // b.hs, b.nlast
   4a540:	ldr	x22, [x20, x28]
   4a544:	mov	x0, x19
   4a548:	mov	x1, x19
   4a54c:	mov	x2, x20
   4a550:	mov	x3, x23
   4a554:	bl	c950 <__gmpn_rsh1add_n@plt>
   4a558:	mov	x8, xzr
   4a55c:	add	x9, x28, x19
   4a560:	add	x11, x0, x22
   4a564:	ldur	x10, [x9, #-8]
   4a568:	lsr	x12, x11, #1
   4a56c:	lsl	x11, x11, #63
   4a570:	adds	x13, x10, x11
   4a574:	adc	x8, x12, x8
   4a578:	stur	x13, [x9, #-8]
   4a57c:	ldr	x9, [x19]
   4a580:	adds	x8, x9, x8
   4a584:	str	x8, [x19]
   4a588:	b.cc	4a5a0 <__gmpn_sqrmod_bnm1@@Base+0x410>  // b.lo, b.ul, b.last
   4a58c:	add	x8, x19, #0x8
   4a590:	ldr	x9, [x8]
   4a594:	adds	x9, x9, #0x1
   4a598:	str	x9, [x8], #8
   4a59c:	b.cs	4a590 <__gmpn_sqrmod_bnm1@@Base+0x400>  // b.hs, b.nlast
   4a5a0:	lsl	x22, x27, #1
   4a5a4:	cmp	x22, x21
   4a5a8:	b.lt	4a764 <__gmpn_sqrmod_bnm1@@Base+0x5d4>  // b.tstop
   4a5ac:	ldr	x21, [x20, x28]
   4a5b0:	add	x0, x19, x28
   4a5b4:	mov	x1, x19
   4a5b8:	mov	x2, x20
   4a5bc:	mov	x3, x23
   4a5c0:	bl	c2d0 <__gmpn_sub_n@plt>
   4a5c4:	ldr	x8, [x19]
   4a5c8:	add	x9, x0, x21
   4a5cc:	subs	x8, x8, x9
   4a5d0:	str	x8, [x19]
   4a5d4:	b.cs	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.hs, b.nlast
   4a5d8:	add	x8, x19, #0x8
   4a5dc:	ldr	x9, [x8]
   4a5e0:	sub	x10, x9, #0x1
   4a5e4:	str	x10, [x8], #8
   4a5e8:	cbz	x9, 4a5dc <__gmpn_sqrmod_bnm1@@Base+0x44c>
   4a5ec:	ldp	x20, x19, [sp, #96]
   4a5f0:	ldp	x22, x21, [sp, #80]
   4a5f4:	ldp	x24, x23, [sp, #64]
   4a5f8:	ldp	x26, x25, [sp, #48]
   4a5fc:	ldp	x28, x27, [sp, #32]
   4a600:	ldp	x29, x30, [sp, #16]
   4a604:	add	sp, sp, #0x70
   4a608:	ret
   4a60c:	mov	x8, x26
   4a610:	str	xzr, [x26, x23, lsl #3]
   4a614:	ldr	x9, [x8]
   4a618:	adds	x9, x9, #0x1
   4a61c:	str	x9, [x8], #8
   4a620:	b.cs	4a614 <__gmpn_sqrmod_bnm1@@Base+0x484>  // b.hs, b.nlast
   4a624:	ldr	x8, [x26, x23, lsl #3]
   4a628:	add	x25, x8, x23
   4a62c:	b	4a46c <__gmpn_sqrmod_bnm1@@Base+0x2dc>
   4a630:	mov	x0, x19
   4a634:	mov	x1, x23
   4a638:	mov	x2, x24
   4a63c:	mov	x3, x27
   4a640:	mov	x4, x20
   4a644:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   4a648:	mov	x26, x24
   4a64c:	mov	x25, x27
   4a650:	b	4a46c <__gmpn_sqrmod_bnm1@@Base+0x2dc>
   4a654:	lsl	x22, x27, #1
   4a658:	cmp	x22, x21
   4a65c:	b.le	4a858 <__gmpn_sqrmod_bnm1@@Base+0x6c8>
   4a660:	mov	x0, x20
   4a664:	mov	x1, x24
   4a668:	mov	x2, x27
   4a66c:	bl	c8e0 <__gmpn_sqr@plt>
   4a670:	subs	x22, x22, x21
   4a674:	b.eq	4a6b0 <__gmpn_sqrmod_bnm1@@Base+0x520>  // b.none
   4a678:	add	x2, x20, x21, lsl #3
   4a67c:	mov	x0, x19
   4a680:	mov	x1, x20
   4a684:	mov	x3, x22
   4a688:	bl	ca70 <__gmpn_add_n@plt>
   4a68c:	cbz	x0, 4a6b0 <__gmpn_sqrmod_bnm1@@Base+0x520>
   4a690:	cmp	x22, x21
   4a694:	b.ge	4a750 <__gmpn_sqrmod_bnm1@@Base+0x5c0>  // b.tcont
   4a698:	lsl	x8, x22, #3
   4a69c:	ldr	x9, [x20, x8]
   4a6a0:	add	x22, x22, #0x1
   4a6a4:	adds	x9, x9, #0x1
   4a6a8:	str	x9, [x19, x8]
   4a6ac:	b.cs	4a690 <__gmpn_sqrmod_bnm1@@Base+0x500>  // b.hs, b.nlast
   4a6b0:	cmp	x19, x20
   4a6b4:	b.eq	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.none
   4a6b8:	cmp	x22, x21
   4a6bc:	b.ge	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.tcont
   4a6c0:	sub	x8, x21, x22
   4a6c4:	cmp	x8, #0x4
   4a6c8:	b.cc	4a72c <__gmpn_sqrmod_bnm1@@Base+0x59c>  // b.lo, b.ul, b.last
   4a6cc:	lsl	x10, x22, #3
   4a6d0:	lsl	x9, x21, #3
   4a6d4:	add	x11, x19, x10
   4a6d8:	add	x12, x20, x9
   4a6dc:	cmp	x11, x12
   4a6e0:	b.cs	4a6f4 <__gmpn_sqrmod_bnm1@@Base+0x564>  // b.hs, b.nlast
   4a6e4:	add	x9, x19, x9
   4a6e8:	add	x11, x20, x10
   4a6ec:	cmp	x11, x9
   4a6f0:	b.cc	4a72c <__gmpn_sqrmod_bnm1@@Base+0x59c>  // b.lo, b.ul, b.last
   4a6f4:	and	x9, x8, #0xfffffffffffffffc
   4a6f8:	add	x11, x10, #0x10
   4a6fc:	add	x22, x22, x9
   4a700:	add	x10, x20, x11
   4a704:	add	x11, x19, x11
   4a708:	mov	x12, x9
   4a70c:	ldp	q0, q1, [x10, #-16]
   4a710:	add	x10, x10, #0x20
   4a714:	subs	x12, x12, #0x4
   4a718:	stp	q0, q1, [x11, #-16]
   4a71c:	add	x11, x11, #0x20
   4a720:	b.ne	4a70c <__gmpn_sqrmod_bnm1@@Base+0x57c>  // b.any
   4a724:	cmp	x8, x9
   4a728:	b.eq	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.none
   4a72c:	lsl	x10, x22, #3
   4a730:	sub	x8, x21, x22
   4a734:	add	x9, x19, x10
   4a738:	add	x10, x20, x10
   4a73c:	ldr	x11, [x10], #8
   4a740:	subs	x8, x8, #0x1
   4a744:	str	x11, [x9], #8
   4a748:	b.ne	4a73c <__gmpn_sqrmod_bnm1@@Base+0x5ac>  // b.any
   4a74c:	b	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>
   4a750:	ldr	x8, [x19]
   4a754:	adds	x8, x8, #0x1
   4a758:	str	x8, [x19], #8
   4a75c:	b.cs	4a750 <__gmpn_sqrmod_bnm1@@Base+0x5c0>  // b.hs, b.nlast
   4a760:	b	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>
   4a764:	add	x0, x19, x28
   4a768:	sub	x3, x22, x23
   4a76c:	mov	x1, x19
   4a770:	mov	x2, x20
   4a774:	bl	c2d0 <__gmpn_sub_n@plt>
   4a778:	lsl	x8, x22, #3
   4a77c:	ldr	x23, [x20, x28]
   4a780:	add	x9, x20, x8
   4a784:	mov	x4, x0
   4a788:	add	x8, x19, x8
   4a78c:	sub	x0, x9, x28
   4a790:	sub	x1, x8, x28
   4a794:	sub	x3, x21, x22
   4a798:	mov	x2, x0
   4a79c:	bl	c760 <__gmpn_sub_nc@plt>
   4a7a0:	ldr	x8, [x19]
   4a7a4:	add	x9, x0, x23
   4a7a8:	subs	x8, x8, x9
   4a7ac:	str	x8, [x19]
   4a7b0:	b.cs	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.hs, b.nlast
   4a7b4:	mov	w8, #0x1                   	// #1
   4a7b8:	cmp	x8, x22
   4a7bc:	b.ge	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.tcont
   4a7c0:	lsl	x9, x8, #3
   4a7c4:	ldr	x10, [x19, x9]
   4a7c8:	add	x8, x8, #0x1
   4a7cc:	sub	x11, x10, #0x1
   4a7d0:	str	x11, [x19, x9]
   4a7d4:	cbz	x10, 4a7b8 <__gmpn_sqrmod_bnm1@@Base+0x628>
   4a7d8:	b	4a5ec <__gmpn_sqrmod_bnm1@@Base+0x45c>
   4a7dc:	mov	x0, x20
   4a7e0:	mov	x1, x24
   4a7e4:	mov	x2, x27
   4a7e8:	bl	c8e0 <__gmpn_sqr@plt>
   4a7ec:	lsl	x8, x27, #1
   4a7f0:	subs	x25, x8, x23
   4a7f4:	add	x24, x20, x23, lsl #3
   4a7f8:	b.eq	4a834 <__gmpn_sqrmod_bnm1@@Base+0x6a4>  // b.none
   4a7fc:	mov	x0, x20
   4a800:	mov	x1, x20
   4a804:	mov	x2, x24
   4a808:	mov	x3, x25
   4a80c:	bl	c2d0 <__gmpn_sub_n@plt>
   4a810:	cbz	x0, 4a834 <__gmpn_sqrmod_bnm1@@Base+0x6a4>
   4a814:	cmp	x25, x23
   4a818:	b.ge	4a83c <__gmpn_sqrmod_bnm1@@Base+0x6ac>  // b.tcont
   4a81c:	lsl	x8, x25, #3
   4a820:	ldr	x9, [x20, x8]
   4a824:	add	x25, x25, #0x1
   4a828:	sub	x10, x9, #0x1
   4a82c:	str	x10, [x20, x8]
   4a830:	cbz	x9, 4a814 <__gmpn_sqrmod_bnm1@@Base+0x684>
   4a834:	str	xzr, [x24]
   4a838:	b	4a540 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   4a83c:	mov	x8, x20
   4a840:	str	xzr, [x24]
   4a844:	ldr	x9, [x8]
   4a848:	adds	x9, x9, #0x1
   4a84c:	str	x9, [x8], #8
   4a850:	b.cs	4a844 <__gmpn_sqrmod_bnm1@@Base+0x6b4>  // b.hs, b.nlast
   4a854:	b	4a540 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   4a858:	mov	x0, x19
   4a85c:	mov	x1, x24
   4a860:	mov	x2, x27
   4a864:	ldp	x20, x19, [sp, #96]
   4a868:	ldp	x22, x21, [sp, #80]
   4a86c:	ldp	x24, x23, [sp, #64]
   4a870:	ldp	x26, x25, [sp, #48]
   4a874:	ldp	x28, x27, [sp, #32]
   4a878:	ldp	x29, x30, [sp, #16]
   4a87c:	add	sp, sp, #0x70
   4a880:	b	c8e0 <__gmpn_sqr@plt>

000000000004a884 <__gmpn_sqrmod_bnm1_next_size@@Base>:
   4a884:	cmp	x0, #0xb
   4a888:	b.lt	4a8dc <__gmpn_sqrmod_bnm1_next_size@@Base+0x58>  // b.tstop
   4a88c:	cmp	x0, #0x28
   4a890:	b.le	4a8e0 <__gmpn_sqrmod_bnm1_next_size@@Base+0x5c>
   4a894:	cmp	x0, #0x50
   4a898:	b.le	4a8ec <__gmpn_sqrmod_bnm1_next_size@@Base+0x68>
   4a89c:	cmp	x0, #0x21e
   4a8a0:	b.le	4a8f8 <__gmpn_sqrmod_bnm1_next_size@@Base+0x74>
   4a8a4:	stp	x29, x30, [sp, #-32]!
   4a8a8:	add	x8, x0, #0x1
   4a8ac:	str	x19, [sp, #16]
   4a8b0:	asr	x19, x8, #1
   4a8b4:	mov	w1, #0x1                   	// #1
   4a8b8:	mov	x0, x19
   4a8bc:	mov	x29, sp
   4a8c0:	bl	cad0 <__gmpn_fft_best_k@plt>
   4a8c4:	mov	w1, w0
   4a8c8:	mov	x0, x19
   4a8cc:	bl	d1d0 <__gmpn_fft_next_size@plt>
   4a8d0:	ldr	x19, [sp, #16]
   4a8d4:	lsl	x0, x0, #1
   4a8d8:	ldp	x29, x30, [sp], #32
   4a8dc:	ret
   4a8e0:	add	x8, x0, #0x1
   4a8e4:	and	x0, x8, #0xfffffffffffffffe
   4a8e8:	ret
   4a8ec:	add	x8, x0, #0x3
   4a8f0:	and	x0, x8, #0xfffffffffffffffc
   4a8f4:	ret
   4a8f8:	add	x8, x0, #0x7
   4a8fc:	and	x0, x8, #0xfffffffffffffff8
   4a900:	ret

000000000004a904 <__gmpn_div_qr_1@@Base>:
   4a904:	stp	x29, x30, [sp, #-80]!
   4a908:	stp	x24, x23, [sp, #32]
   4a90c:	stp	x22, x21, [sp, #48]
   4a910:	stp	x20, x19, [sp, #64]
   4a914:	mov	x19, x4
   4a918:	mov	x22, x2
   4a91c:	mov	x24, x1
   4a920:	mov	x20, x0
   4a924:	stp	x26, x25, [sp, #16]
   4a928:	mov	x29, sp
   4a92c:	tbnz	x4, #63, 4a9a8 <__gmpn_div_qr_1@@Base+0xa4>
   4a930:	sub	x23, x3, #0x1
   4a934:	ldr	x25, [x22, x23, lsl #3]
   4a938:	clz	x21, x19
   4a93c:	mov	x0, x20
   4a940:	mov	x1, x22
   4a944:	mov	x2, x23
   4a948:	mov	w3, w21
   4a94c:	lsl	x19, x19, x21
   4a950:	lsl	x26, x25, x21
   4a954:	bl	c180 <__gmpn_lshift@plt>
   4a958:	neg	x9, x21
   4a95c:	lsr	x10, x19, #32
   4a960:	lsr	x9, x25, x9
   4a964:	udiv	x14, x9, x10
   4a968:	orr	x8, x26, x0
   4a96c:	and	x11, x19, #0xffffffff
   4a970:	msub	w9, w14, w10, w9
   4a974:	mul	x12, x14, x11
   4a978:	extr	x13, x9, x8, #32
   4a97c:	cmp	x13, x12
   4a980:	b.cs	4aa04 <__gmpn_div_qr_1@@Base+0x100>  // b.hs, b.nlast
   4a984:	add	x13, x13, x19
   4a988:	cmp	x13, x19
   4a98c:	sub	x9, x14, #0x1
   4a990:	b.cc	4aa08 <__gmpn_div_qr_1@@Base+0x104>  // b.lo, b.ul, b.last
   4a994:	cmp	x13, x12
   4a998:	b.cs	4aa08 <__gmpn_div_qr_1@@Base+0x104>  // b.hs, b.nlast
   4a99c:	sub	x9, x14, #0x2
   4a9a0:	add	x13, x13, x19
   4a9a4:	b	4aa08 <__gmpn_div_qr_1@@Base+0x104>
   4a9a8:	sub	x23, x3, #0x1
   4a9ac:	ldr	x8, [x22, x23, lsl #3]
   4a9b0:	cmp	x8, x19
   4a9b4:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   4a9b8:	cset	w9, cs  // cs = hs, nlast
   4a9bc:	cmp	x3, #0xd
   4a9c0:	sub	x25, x8, x10
   4a9c4:	str	x9, [x24]
   4a9c8:	b.le	4ab14 <__gmpn_div_qr_1@@Base+0x210>
   4a9cc:	mov	x0, x19
   4a9d0:	bl	d3f0 <__gmpn_invert_limb@plt>
   4a9d4:	mov	x5, x0
   4a9d8:	mov	x0, x20
   4a9dc:	mov	x1, x22
   4a9e0:	mov	x2, x23
   4a9e4:	mov	x3, x25
   4a9e8:	mov	x4, x19
   4a9ec:	ldp	x20, x19, [sp, #64]
   4a9f0:	ldp	x22, x21, [sp, #48]
   4a9f4:	ldp	x24, x23, [sp, #32]
   4a9f8:	ldp	x26, x25, [sp, #16]
   4a9fc:	ldp	x29, x30, [sp], #80
   4aa00:	b	c470 <__gmpn_div_qr_1n_pi1@plt>
   4aa04:	mov	x9, x14
   4aa08:	sub	x13, x13, x12
   4aa0c:	udiv	x12, x13, x10
   4aa10:	msub	w13, w12, w10, w13
   4aa14:	mul	x10, x12, x11
   4aa18:	bfi	x8, x13, #32, #32
   4aa1c:	cmp	x8, x10
   4aa20:	b.cs	4aa48 <__gmpn_div_qr_1@@Base+0x144>  // b.hs, b.nlast
   4aa24:	add	x8, x8, x19
   4aa28:	cmp	x8, x19
   4aa2c:	sub	x11, x12, #0x1
   4aa30:	b.cc	4aa4c <__gmpn_div_qr_1@@Base+0x148>  // b.lo, b.ul, b.last
   4aa34:	cmp	x8, x10
   4aa38:	b.cs	4aa4c <__gmpn_div_qr_1@@Base+0x148>  // b.hs, b.nlast
   4aa3c:	sub	x11, x12, #0x2
   4aa40:	add	x8, x8, x19
   4aa44:	b	4aa4c <__gmpn_div_qr_1@@Base+0x148>
   4aa48:	mov	x11, x12
   4aa4c:	sub	x25, x8, x10
   4aa50:	orr	x8, x11, x9, lsl #32
   4aa54:	str	x8, [x24]
   4aa58:	mov	x22, x20
   4aa5c:	subs	x8, x23, #0x1
   4aa60:	b.lt	4ab20 <__gmpn_div_qr_1@@Base+0x21c>  // b.tstop
   4aa64:	lsr	x9, x19, #32
   4aa68:	and	x10, x19, #0xffffffff
   4aa6c:	b	4aa90 <__gmpn_div_qr_1@@Base+0x18c>
   4aa70:	mov	x15, x14
   4aa74:	orr	x12, x15, x12, lsl #32
   4aa78:	add	x14, x8, #0x1
   4aa7c:	str	x12, [x20, x8, lsl #3]
   4aa80:	sub	x8, x8, #0x1
   4aa84:	cmp	x14, #0x1
   4aa88:	sub	x25, x11, x13
   4aa8c:	b.le	4ab20 <__gmpn_div_qr_1@@Base+0x21c>
   4aa90:	ldr	x11, [x22, x8, lsl #3]
   4aa94:	udiv	x15, x25, x9
   4aa98:	msub	w12, w15, w9, w25
   4aa9c:	mul	x13, x15, x10
   4aaa0:	extr	x14, x12, x11, #32
   4aaa4:	cmp	x14, x13
   4aaa8:	b.cs	4aad0 <__gmpn_div_qr_1@@Base+0x1cc>  // b.hs, b.nlast
   4aaac:	add	x14, x14, x19
   4aab0:	cmp	x14, x19
   4aab4:	sub	x12, x15, #0x1
   4aab8:	b.cc	4aad4 <__gmpn_div_qr_1@@Base+0x1d0>  // b.lo, b.ul, b.last
   4aabc:	cmp	x14, x13
   4aac0:	b.cs	4aad4 <__gmpn_div_qr_1@@Base+0x1d0>  // b.hs, b.nlast
   4aac4:	sub	x12, x15, #0x2
   4aac8:	add	x14, x14, x19
   4aacc:	b	4aad4 <__gmpn_div_qr_1@@Base+0x1d0>
   4aad0:	mov	x12, x15
   4aad4:	sub	x13, x14, x13
   4aad8:	udiv	x14, x13, x9
   4aadc:	msub	w15, w14, w9, w13
   4aae0:	mul	x13, x14, x10
   4aae4:	bfi	x11, x15, #32, #32
   4aae8:	cmp	x11, x13
   4aaec:	b.cs	4aa70 <__gmpn_div_qr_1@@Base+0x16c>  // b.hs, b.nlast
   4aaf0:	add	x11, x11, x19
   4aaf4:	cmp	x11, x19
   4aaf8:	sub	x15, x14, #0x1
   4aafc:	b.cc	4aa74 <__gmpn_div_qr_1@@Base+0x170>  // b.lo, b.ul, b.last
   4ab00:	cmp	x11, x13
   4ab04:	b.cs	4aa74 <__gmpn_div_qr_1@@Base+0x170>  // b.hs, b.nlast
   4ab08:	sub	x15, x14, #0x2
   4ab0c:	add	x11, x11, x19
   4ab10:	b	4aa74 <__gmpn_div_qr_1@@Base+0x170>
   4ab14:	mov	x21, xzr
   4ab18:	subs	x8, x23, #0x1
   4ab1c:	b.ge	4aa64 <__gmpn_div_qr_1@@Base+0x160>  // b.tcont
   4ab20:	lsr	x0, x25, x21
   4ab24:	ldp	x20, x19, [sp, #64]
   4ab28:	ldp	x22, x21, [sp, #48]
   4ab2c:	ldp	x24, x23, [sp, #32]
   4ab30:	ldp	x26, x25, [sp, #16]
   4ab34:	ldp	x29, x30, [sp], #80
   4ab38:	ret

000000000004ab3c <__gmpn_div_qr_1n_pi1@@Base>:
   4ab3c:	str	x19, [sp, #-16]!
   4ab40:	cmp	x2, #0x1
   4ab44:	b.ne	4ab90 <__gmpn_div_qr_1n_pi1@@Base+0x54>  // b.any
   4ab48:	ldr	x8, [x1]
   4ab4c:	mul	x10, x5, x3
   4ab50:	umulh	x9, x3, x5
   4ab54:	add	x11, x3, #0x1
   4ab58:	adds	x12, x10, x8
   4ab5c:	adc	x10, x9, x11
   4ab60:	msub	x8, x10, x4, x8
   4ab64:	cmp	x8, x12
   4ab68:	csel	x9, x4, xzr, hi  // hi = pmore
   4ab6c:	cset	w11, hi  // hi = pmore
   4ab70:	add	x8, x9, x8
   4ab74:	subs	x9, x8, x4
   4ab78:	sub	x10, x10, x11
   4ab7c:	b.cs	4ad50 <__gmpn_div_qr_1n_pi1@@Base+0x214>  // b.hs, b.nlast
   4ab80:	str	x10, [x0]
   4ab84:	mov	x0, x8
   4ab88:	ldr	x19, [sp], #16
   4ab8c:	ret
   4ab90:	umulh	x10, x5, x3
   4ab94:	lsl	x11, x2, #3
   4ab98:	add	x10, x10, x3
   4ab9c:	sub	x11, x11, #0x8
   4aba0:	sub	x15, x2, #0x2
   4aba4:	ldr	x12, [x1, x11]
   4aba8:	str	x10, [x0, x11]
   4abac:	ldr	x10, [x1, x15, lsl #3]
   4abb0:	mul	x9, x4, x5
   4abb4:	mneg	x9, x9, x3
   4abb8:	mneg	x8, x4, x5
   4abbc:	adds	x10, x10, x9
   4abc0:	mov	w11, #0x2                   	// #2
   4abc4:	umulh	x13, x8, x3
   4abc8:	cset	w9, cs  // cs = hs, nlast
   4abcc:	adds	x12, x12, x13
   4abd0:	cset	w13, cs  // cs = hs, nlast
   4abd4:	csinc	x14, x11, xzr, cs  // cs = hs, nlast
   4abd8:	adds	x11, x12, x9
   4abdc:	csel	x16, x13, x14, cc  // cc = lo, ul, last
   4abe0:	subs	x17, x2, #0x3
   4abe4:	mul	x18, x5, x3
   4abe8:	b.lt	4acb4 <__gmpn_div_qr_1n_pi1@@Base+0x178>  // b.tstop
   4abec:	add	x12, x0, #0x10
   4abf0:	add	x13, x0, x2, lsl #3
   4abf4:	mov	w14, #0x2                   	// #2
   4abf8:	b	4ac30 <__gmpn_div_qr_1n_pi1@@Base+0xf4>
   4abfc:	ldr	x18, [x1, x15, lsl #3]
   4ac00:	sub	x17, x15, #0x1
   4ac04:	sub	x13, x13, #0x8
   4ac08:	adds	x10, x18, x10
   4ac0c:	cset	w18, cs  // cs = hs, nlast
   4ac10:	adds	x11, x16, x11
   4ac14:	cset	w16, cs  // cs = hs, nlast
   4ac18:	csinc	x2, x14, xzr, cs  // cs = hs, nlast
   4ac1c:	adds	x11, x11, x18
   4ac20:	csel	x16, x16, x2, cc  // cc = lo, ul, last
   4ac24:	cmp	x15, #0x0
   4ac28:	mov	x18, x9
   4ac2c:	b.le	4acb8 <__gmpn_div_qr_1n_pi1@@Base+0x17c>
   4ac30:	neg	x3, x16
   4ac34:	mov	x2, xzr
   4ac38:	and	x7, x3, x5
   4ac3c:	adds	x19, x7, x11
   4ac40:	adc	x16, x16, x2
   4ac44:	umulh	x6, x11, x5
   4ac48:	adds	x7, x19, x6
   4ac4c:	adc	x16, x16, x2
   4ac50:	and	x3, x3, x8
   4ac54:	adds	x6, x7, x18
   4ac58:	adc	x16, x16, x2
   4ac5c:	adds	x18, x10, x3
   4ac60:	cset	w10, cs  // cs = hs, nlast
   4ac64:	csel	x3, x4, xzr, cs  // cs = hs, nlast
   4ac68:	adds	x7, x6, x10
   4ac6c:	adc	x2, x16, x2
   4ac70:	str	x7, [x0, x15, lsl #3]
   4ac74:	mov	x15, x17
   4ac78:	lsl	x17, x17, #3
   4ac7c:	ldr	x6, [x12, x17]
   4ac80:	mul	x9, x11, x5
   4ac84:	umulh	x16, x11, x8
   4ac88:	mul	x10, x11, x8
   4ac8c:	sub	x11, x18, x3
   4ac90:	adds	x18, x6, x2
   4ac94:	str	x18, [x12, x17]
   4ac98:	b.cc	4abfc <__gmpn_div_qr_1n_pi1@@Base+0xc0>  // b.lo, b.ul, b.last
   4ac9c:	mov	x17, x13
   4aca0:	ldr	x18, [x17]
   4aca4:	adds	x18, x18, #0x1
   4aca8:	str	x18, [x17], #8
   4acac:	b.cs	4aca0 <__gmpn_div_qr_1n_pi1@@Base+0x164>  // b.hs, b.nlast
   4acb0:	b	4abfc <__gmpn_div_qr_1n_pi1@@Base+0xc0>
   4acb4:	mov	x9, x18
   4acb8:	mov	x12, xzr
   4acbc:	cmp	x16, #0x0
   4acc0:	csel	x14, x4, x12, ne  // ne = any
   4acc4:	mov	w8, #0x2                   	// #2
   4acc8:	sub	x11, x11, x14
   4accc:	cset	w14, ne  // ne = any
   4acd0:	csinc	x8, x8, xzr, ne  // ne = any
   4acd4:	cmp	x11, x4
   4acd8:	csel	x14, x14, x8, cc  // cc = lo, ul, last
   4acdc:	csel	x8, x4, x12, cs  // cs = hs, nlast
   4ace0:	sub	x8, x11, x8
   4ace4:	umulh	x11, x8, x5
   4ace8:	mul	x15, x8, x5
   4acec:	add	x8, x8, #0x1
   4acf0:	adds	x16, x15, x10
   4acf4:	adc	x8, x11, x8
   4acf8:	msub	x10, x8, x4, x10
   4acfc:	cmp	x10, x16
   4ad00:	csel	x15, x4, x12, hi  // hi = pmore
   4ad04:	ldr	x13, [x0, #8]
   4ad08:	cset	w11, hi  // hi = pmore
   4ad0c:	add	x10, x15, x10
   4ad10:	sub	x8, x8, x11
   4ad14:	cmp	x10, x4
   4ad18:	cinc	x11, x8, cs  // cs = hs, nlast
   4ad1c:	csel	x8, x12, x4, cc  // cc = lo, ul, last
   4ad20:	sub	x8, x10, x8
   4ad24:	adds	x10, x9, x11
   4ad28:	adc	x9, x14, x12
   4ad2c:	adds	x9, x13, x9
   4ad30:	str	x9, [x0, #8]
   4ad34:	b.cc	4ab80 <__gmpn_div_qr_1n_pi1@@Base+0x44>  // b.lo, b.ul, b.last
   4ad38:	add	x9, x0, #0x10
   4ad3c:	ldr	x11, [x9]
   4ad40:	adds	x11, x11, #0x1
   4ad44:	str	x11, [x9], #8
   4ad48:	b.cs	4ad3c <__gmpn_div_qr_1n_pi1@@Base+0x200>  // b.hs, b.nlast
   4ad4c:	b	4ab80 <__gmpn_div_qr_1n_pi1@@Base+0x44>
   4ad50:	add	x10, x10, #0x1
   4ad54:	mov	x8, x9
   4ad58:	str	x10, [x0]
   4ad5c:	mov	x0, x8
   4ad60:	ldr	x19, [sp], #16
   4ad64:	ret

000000000004ad68 <__gmpn_div_qr_2@@Base>:
   4ad68:	stp	x29, x30, [sp, #-80]!
   4ad6c:	str	x25, [sp, #16]
   4ad70:	stp	x24, x23, [sp, #32]
   4ad74:	stp	x22, x21, [sp, #48]
   4ad78:	stp	x20, x19, [sp, #64]
   4ad7c:	ldp	x23, x25, [x4]
   4ad80:	mov	x19, x3
   4ad84:	mov	x20, x2
   4ad88:	mov	x21, x1
   4ad8c:	mov	x22, x0
   4ad90:	mov	x29, sp
   4ad94:	tbnz	x25, #63, 4ae44 <__gmpn_div_qr_2@@Base+0xdc>
   4ad98:	clz	x24, x25
   4ad9c:	neg	x9, x24
   4ada0:	lsl	x8, x25, x24
   4ada4:	lsr	x9, x23, x9
   4ada8:	orr	x25, x9, x8
   4adac:	mov	x0, x25
   4adb0:	lsl	x23, x23, x24
   4adb4:	bl	d3f0 <__gmpn_invert_limb@plt>
   4adb8:	mul	x8, x0, x25
   4adbc:	adds	x8, x8, x23
   4adc0:	b.cc	4addc <__gmpn_div_qr_2@@Base+0x74>  // b.lo, b.ul, b.last
   4adc4:	subs	x8, x8, x25
   4adc8:	cset	w9, cs  // cs = hs, nlast
   4adcc:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   4add0:	mvn	x9, x9
   4add4:	add	x0, x9, x0
   4add8:	sub	x8, x8, x10
   4addc:	umulh	x9, x23, x0
   4ade0:	adds	x8, x9, x8
   4ade4:	b.cc	4ae0c <__gmpn_div_qr_2@@Base+0xa4>  // b.lo, b.ul, b.last
   4ade8:	cmp	x8, x25
   4adec:	sub	x7, x0, #0x1
   4adf0:	b.cc	4ae10 <__gmpn_div_qr_2@@Base+0xa8>  // b.lo, b.ul, b.last
   4adf4:	mul	x9, x0, x23
   4adf8:	cmp	x8, x25
   4adfc:	sub	x10, x0, #0x2
   4ae00:	ccmp	x9, x23, #0x2, ls  // ls = plast
   4ae04:	csel	x7, x7, x10, cc  // cc = lo, ul, last
   4ae08:	b	4ae10 <__gmpn_div_qr_2@@Base+0xa8>
   4ae0c:	mov	x7, x0
   4ae10:	mov	x0, x22
   4ae14:	mov	x1, x21
   4ae18:	mov	x2, x20
   4ae1c:	mov	x3, x19
   4ae20:	mov	x4, x25
   4ae24:	mov	x5, x23
   4ae28:	mov	w6, w24
   4ae2c:	ldp	x20, x19, [sp, #64]
   4ae30:	ldp	x22, x21, [sp, #48]
   4ae34:	ldp	x24, x23, [sp, #32]
   4ae38:	ldr	x25, [sp, #16]
   4ae3c:	ldp	x29, x30, [sp], #80
   4ae40:	b	cf20 <__gmpn_div_qr_2u_pi1@plt>
   4ae44:	mov	x0, x25
   4ae48:	bl	d3f0 <__gmpn_invert_limb@plt>
   4ae4c:	mul	x8, x0, x25
   4ae50:	adds	x8, x8, x23
   4ae54:	b.cc	4ae70 <__gmpn_div_qr_2@@Base+0x108>  // b.lo, b.ul, b.last
   4ae58:	subs	x8, x8, x25
   4ae5c:	cset	w9, cs  // cs = hs, nlast
   4ae60:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   4ae64:	mvn	x9, x9
   4ae68:	add	x0, x9, x0
   4ae6c:	sub	x8, x8, x10
   4ae70:	umulh	x9, x23, x0
   4ae74:	adds	x8, x9, x8
   4ae78:	b.cc	4aea0 <__gmpn_div_qr_2@@Base+0x138>  // b.lo, b.ul, b.last
   4ae7c:	cmp	x8, x25
   4ae80:	sub	x6, x0, #0x1
   4ae84:	b.cc	4aea4 <__gmpn_div_qr_2@@Base+0x13c>  // b.lo, b.ul, b.last
   4ae88:	mul	x9, x0, x23
   4ae8c:	cmp	x8, x25
   4ae90:	sub	x10, x0, #0x2
   4ae94:	ccmp	x9, x23, #0x2, ls  // ls = plast
   4ae98:	csel	x6, x6, x10, cc  // cc = lo, ul, last
   4ae9c:	b	4aea4 <__gmpn_div_qr_2@@Base+0x13c>
   4aea0:	mov	x6, x0
   4aea4:	mov	x0, x22
   4aea8:	mov	x1, x21
   4aeac:	mov	x2, x20
   4aeb0:	mov	x3, x19
   4aeb4:	mov	x4, x25
   4aeb8:	mov	x5, x23
   4aebc:	ldp	x20, x19, [sp, #64]
   4aec0:	ldp	x22, x21, [sp, #48]
   4aec4:	ldp	x24, x23, [sp, #32]
   4aec8:	ldr	x25, [sp, #16]
   4aecc:	ldp	x29, x30, [sp], #80
   4aed0:	b	c920 <__gmpn_div_qr_2n_pi1@plt>

000000000004aed4 <__gmpn_div_qr_2n_pi1@@Base>:
   4aed4:	add	x8, x2, x3, lsl #3
   4aed8:	ldp	x10, x9, [x8, #-16]
   4aedc:	cmp	x9, x4
   4aee0:	b.cc	4aef0 <__gmpn_div_qr_2n_pi1@@Base+0x1c>  // b.lo, b.ul, b.last
   4aee4:	b.hi	4af00 <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.pmore
   4aee8:	cmp	x10, x5
   4aeec:	b.cs	4af00 <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.hs, b.nlast
   4aef0:	mov	x8, xzr
   4aef4:	subs	x11, x3, #0x3
   4aef8:	b.ge	4af40 <__gmpn_div_qr_2n_pi1@@Base+0x6c>  // b.tcont
   4aefc:	b	4afb0 <__gmpn_div_qr_2n_pi1@@Base+0xdc>
   4af00:	subs	x11, x10, x5
   4af04:	sbc	x9, x9, x4
   4af08:	mov	w8, #0x1                   	// #1
   4af0c:	mov	x10, x11
   4af10:	subs	x11, x3, #0x3
   4af14:	b.lt	4afb0 <__gmpn_div_qr_2n_pi1@@Base+0xdc>  // b.tstop
   4af18:	b	4af40 <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   4af1c:	cmp	x10, x5
   4af20:	b.cs	4af2c <__gmpn_div_qr_2n_pi1@@Base+0x58>  // b.hs, b.nlast
   4af24:	cmp	x9, x4
   4af28:	b.ls	4af9c <__gmpn_div_qr_2n_pi1@@Base+0xc8>  // b.plast
   4af2c:	subs	x13, x10, x5
   4af30:	sbc	x9, x9, x4
   4af34:	add	x12, x12, #0x1
   4af38:	mov	x10, x13
   4af3c:	b	4af9c <__gmpn_div_qr_2n_pi1@@Base+0xc8>
   4af40:	mul	x13, x9, x6
   4af44:	ldr	x12, [x2, x11, lsl #3]
   4af48:	umulh	x14, x9, x6
   4af4c:	adds	x15, x13, x10
   4af50:	adc	x9, x14, x9
   4af54:	msub	x10, x9, x4, x10
   4af58:	mul	x13, x9, x5
   4af5c:	umulh	x14, x5, x9
   4af60:	subs	x16, x12, x5
   4af64:	sbc	x10, x10, x4
   4af68:	subs	x12, x16, x13
   4af6c:	sbc	x13, x10, x14
   4af70:	cmp	x13, x15
   4af74:	cset	w10, cs  // cs = hs, nlast
   4af78:	csetm	x14, cs  // cs = hs, nlast
   4af7c:	sub	x15, x9, x10
   4af80:	and	x9, x14, x5
   4af84:	and	x14, x14, x4
   4af88:	adds	x10, x12, x9
   4af8c:	adc	x9, x13, x14
   4af90:	cmp	x9, x4
   4af94:	add	x12, x15, #0x1
   4af98:	b.cs	4af1c <__gmpn_div_qr_2n_pi1@@Base+0x48>  // b.hs, b.nlast
   4af9c:	sub	x13, x11, #0x1
   4afa0:	cmp	x11, #0x0
   4afa4:	str	x12, [x0, x11, lsl #3]
   4afa8:	mov	x11, x13
   4afac:	b.gt	4af40 <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   4afb0:	mov	x0, x8
   4afb4:	stp	x10, x9, [x1]
   4afb8:	ret

000000000004afbc <__gmpn_div_qr_2u_pi1@@Base>:
   4afbc:	add	x8, x2, x3, lsl #3
   4afc0:	ldp	x8, x12, [x8, #-16]
   4afc4:	neg	w11, w6
   4afc8:	mov	w10, #0x40                  	// #64
   4afcc:	mov	w9, w6
   4afd0:	lsr	x13, x12, x11
   4afd4:	lsl	x12, x12, x6
   4afd8:	lsr	x11, x8, x11
   4afdc:	lsl	x8, x8, x6
   4afe0:	orr	x11, x11, x12
   4afe4:	mul	x12, x13, x7
   4afe8:	umulh	x14, x13, x7
   4afec:	adds	x15, x12, x11
   4aff0:	adc	x12, x14, x13
   4aff4:	msub	x11, x12, x4, x11
   4aff8:	subs	x16, x8, x5
   4affc:	sbc	x8, x11, x4
   4b000:	mul	x13, x12, x5
   4b004:	umulh	x14, x5, x12
   4b008:	subs	x11, x16, x13
   4b00c:	sbc	x8, x8, x14
   4b010:	cmp	x8, x15
   4b014:	cset	w13, cs  // cs = hs, nlast
   4b018:	csetm	x14, cs  // cs = hs, nlast
   4b01c:	sub	x13, x12, x13
   4b020:	and	x15, x14, x5
   4b024:	and	x14, x14, x4
   4b028:	adds	x12, x11, x15
   4b02c:	adc	x11, x8, x14
   4b030:	sub	w10, w10, w6
   4b034:	cmp	x11, x4
   4b038:	add	x8, x13, #0x1
   4b03c:	b.cs	4b108 <__gmpn_div_qr_2u_pi1@@Base+0x14c>  // b.hs, b.nlast
   4b040:	subs	x13, x3, #0x3
   4b044:	b.lt	4b0ec <__gmpn_div_qr_2u_pi1@@Base+0x130>  // b.tstop
   4b048:	ldr	x14, [x2, x13, lsl #3]
   4b04c:	mul	x15, x11, x7
   4b050:	umulh	x16, x11, x7
   4b054:	lsr	x17, x14, x10
   4b058:	orr	x12, x17, x12
   4b05c:	lsl	x14, x14, x9
   4b060:	adds	x17, x15, x12
   4b064:	adc	x11, x16, x11
   4b068:	msub	x12, x11, x4, x12
   4b06c:	mul	x15, x11, x5
   4b070:	umulh	x16, x5, x11
   4b074:	subs	x18, x14, x5
   4b078:	sbc	x12, x12, x4
   4b07c:	subs	x14, x18, x15
   4b080:	sbc	x15, x12, x16
   4b084:	cmp	x15, x17
   4b088:	cset	w12, cs  // cs = hs, nlast
   4b08c:	csetm	x16, cs  // cs = hs, nlast
   4b090:	sub	x17, x11, x12
   4b094:	and	x11, x16, x5
   4b098:	and	x16, x16, x4
   4b09c:	adds	x12, x14, x11
   4b0a0:	adc	x11, x15, x16
   4b0a4:	cmp	x11, x4
   4b0a8:	add	x14, x17, #0x1
   4b0ac:	b.cs	4b0c8 <__gmpn_div_qr_2u_pi1@@Base+0x10c>  // b.hs, b.nlast
   4b0b0:	sub	x15, x13, #0x1
   4b0b4:	cmp	x13, #0x0
   4b0b8:	str	x14, [x0, x13, lsl #3]
   4b0bc:	mov	x13, x15
   4b0c0:	b.gt	4b048 <__gmpn_div_qr_2u_pi1@@Base+0x8c>
   4b0c4:	b	4b0ec <__gmpn_div_qr_2u_pi1@@Base+0x130>
   4b0c8:	cmp	x12, x5
   4b0cc:	b.cs	4b0d8 <__gmpn_div_qr_2u_pi1@@Base+0x11c>  // b.hs, b.nlast
   4b0d0:	cmp	x11, x4
   4b0d4:	b.ls	4b0b0 <__gmpn_div_qr_2u_pi1@@Base+0xf4>  // b.plast
   4b0d8:	subs	x15, x12, x5
   4b0dc:	sbc	x11, x11, x4
   4b0e0:	add	x14, x14, #0x1
   4b0e4:	mov	x12, x15
   4b0e8:	b	4b0b0 <__gmpn_div_qr_2u_pi1@@Base+0xf4>
   4b0ec:	lsr	x12, x12, x9
   4b0f0:	lsl	x10, x11, x10
   4b0f4:	lsr	x9, x11, x9
   4b0f8:	orr	x10, x10, x12
   4b0fc:	mov	x0, x8
   4b100:	stp	x10, x9, [x1]
   4b104:	ret
   4b108:	cmp	x12, x5
   4b10c:	b.cs	4b118 <__gmpn_div_qr_2u_pi1@@Base+0x15c>  // b.hs, b.nlast
   4b110:	cmp	x11, x4
   4b114:	b.ls	4b040 <__gmpn_div_qr_2u_pi1@@Base+0x84>  // b.plast
   4b118:	subs	x13, x12, x5
   4b11c:	sbc	x11, x11, x4
   4b120:	add	x8, x8, #0x1
   4b124:	mov	x12, x13
   4b128:	b	4b040 <__gmpn_div_qr_2u_pi1@@Base+0x84>

000000000004b12c <__gmpn_sbpi1_div_q@@Base>:
   4b12c:	sub	sp, sp, #0xe0
   4b130:	stp	x28, x27, [sp, #144]
   4b134:	sub	x27, x2, x4
   4b138:	add	x8, x27, #0x1
   4b13c:	str	x8, [sp, #16]
   4b140:	subs	x8, x4, x8
   4b144:	stp	x26, x25, [sp, #160]
   4b148:	stp	x20, x19, [sp, #208]
   4b14c:	add	x19, x1, x2, lsl #3
   4b150:	csinc	x25, x4, x27, le
   4b154:	str	x8, [sp]
   4b158:	add	x8, x3, x8, lsl #3
   4b15c:	stp	x29, x30, [sp, #128]
   4b160:	stp	x24, x23, [sp, #176]
   4b164:	stp	x22, x21, [sp, #192]
   4b168:	add	x29, sp, #0x80
   4b16c:	mov	x21, x4
   4b170:	mov	x20, x2
   4b174:	mov	x22, x0
   4b178:	mov	x9, x25
   4b17c:	csel	x23, x8, x3, gt
   4b180:	sub	x0, x19, x25, lsl #3
   4b184:	sub	x8, x19, #0x8
   4b188:	stur	x5, [x29, #-24]
   4b18c:	stp	x3, x1, [sp, #24]
   4b190:	str	x25, [sp, #64]
   4b194:	subs	x10, x9, #0x1
   4b198:	b.lt	4b1bc <__gmpn_sbpi1_div_q@@Base+0x90>  // b.tstop
   4b19c:	add	x9, x23, x9, lsl #3
   4b1a0:	ldr	x11, [x8], #-8
   4b1a4:	ldur	x12, [x9, #-8]
   4b1a8:	mov	x9, x10
   4b1ac:	cmp	x11, x12
   4b1b0:	b.eq	4b194 <__gmpn_sbpi1_div_q@@Base+0x68>  // b.none
   4b1b4:	cmp	x11, x12
   4b1b8:	b.ls	4b1dc <__gmpn_sbpi1_div_q@@Base+0xb0>  // b.plast
   4b1bc:	ldr	x3, [sp, #64]
   4b1c0:	mov	x1, x0
   4b1c4:	mov	x2, x23
   4b1c8:	bl	c2d0 <__gmpn_sub_n@plt>
   4b1cc:	mov	w26, #0x1                   	// #1
   4b1d0:	mov	w8, #0x1                   	// #1
   4b1d4:	str	w8, [sp, #12]
   4b1d8:	b	4b1e4 <__gmpn_sbpi1_div_q@@Base+0xb8>
   4b1dc:	mov	x26, xzr
   4b1e0:	str	wzr, [sp, #12]
   4b1e4:	sub	x8, x25, #0x2
   4b1e8:	sub	x9, x25, #0x1
   4b1ec:	ldr	x15, [x23, x9, lsl #3]
   4b1f0:	stp	x8, x23, [x29, #-40]
   4b1f4:	ldr	x24, [x23, x8, lsl #3]
   4b1f8:	ldur	x23, [x19, #-8]
   4b1fc:	subs	x8, x27, x25
   4b200:	str	x21, [sp, #56]
   4b204:	stp	x24, x15, [x29, #-16]
   4b208:	stur	x25, [x29, #-56]
   4b20c:	b.mi	4b43c <__gmpn_sbpi1_div_q@@Base+0x310>  // b.first
   4b210:	sub	x24, x22, x21, lsl #3
   4b214:	ldr	x21, [sp, #32]
   4b218:	stur	x9, [x29, #-48]
   4b21c:	mov	x9, x25
   4b220:	stp	x27, x26, [sp, #40]
   4b224:	add	x27, x8, #0x1
   4b228:	mvn	x8, x9
   4b22c:	lsl	x25, x20, #3
   4b230:	add	x19, x21, x8, lsl #3
   4b234:	cmp	x23, x15
   4b238:	add	x28, x21, x25
   4b23c:	b.eq	4b2f0 <__gmpn_sbpi1_div_q@@Base+0x1c4>  // b.none
   4b240:	ldp	x10, x16, [x29, #-24]
   4b244:	ldp	x11, x8, [x28, #-24]
   4b248:	mul	x9, x23, x10
   4b24c:	umulh	x10, x23, x10
   4b250:	adds	x12, x9, x8
   4b254:	adc	x9, x10, x23
   4b258:	msub	x8, x9, x15, x8
   4b25c:	subs	x14, x11, x16
   4b260:	sbc	x8, x8, x15
   4b264:	mul	x10, x9, x16
   4b268:	umulh	x13, x16, x9
   4b26c:	subs	x11, x14, x10
   4b270:	sbc	x8, x8, x13
   4b274:	cmp	x8, x12
   4b278:	cset	w10, cs  // cs = hs, nlast
   4b27c:	csetm	x12, cs  // cs = hs, nlast
   4b280:	sub	x9, x9, x10
   4b284:	and	x10, x16, x12
   4b288:	and	x12, x15, x12
   4b28c:	adds	x26, x11, x10
   4b290:	adc	x23, x8, x12
   4b294:	cmp	x23, x15
   4b298:	add	x20, x9, #0x1
   4b29c:	b.cs	4b324 <__gmpn_sbpi1_div_q@@Base+0x1f8>  // b.hs, b.nlast
   4b2a0:	ldp	x2, x1, [x29, #-40]
   4b2a4:	add	x22, x19, x25
   4b2a8:	mov	x0, x22
   4b2ac:	mov	x3, x20
   4b2b0:	bl	c9e0 <__gmpn_submul_1@plt>
   4b2b4:	subs	x8, x26, x0
   4b2b8:	cset	w9, cc  // cc = lo, ul, last
   4b2bc:	subs	x23, x23, x9
   4b2c0:	stur	x8, [x28, #-24]
   4b2c4:	b.cc	4b350 <__gmpn_sbpi1_div_q@@Base+0x224>  // b.lo, b.ul, b.last
   4b2c8:	ldur	x15, [x29, #-8]
   4b2cc:	sub	x27, x27, #0x1
   4b2d0:	add	x8, x24, x25
   4b2d4:	sub	x24, x24, #0x8
   4b2d8:	sub	x21, x21, #0x8
   4b2dc:	cmp	x27, #0x0
   4b2e0:	sub	x19, x19, #0x8
   4b2e4:	stur	x20, [x8, #-8]
   4b2e8:	b.gt	4b234 <__gmpn_sbpi1_div_q@@Base+0x108>
   4b2ec:	b	4b378 <__gmpn_sbpi1_div_q@@Base+0x24c>
   4b2f0:	ldur	x8, [x28, #-16]
   4b2f4:	ldur	x9, [x29, #-16]
   4b2f8:	cmp	x8, x9
   4b2fc:	b.ne	4b240 <__gmpn_sbpi1_div_q@@Base+0x114>  // b.any
   4b300:	ldur	x1, [x29, #-32]
   4b304:	ldr	x2, [sp, #64]
   4b308:	add	x0, x19, x25
   4b30c:	mov	x3, #0xffffffffffffffff    	// #-1
   4b310:	mov	x20, #0xffffffffffffffff    	// #-1
   4b314:	bl	c9e0 <__gmpn_submul_1@plt>
   4b318:	ldur	x15, [x29, #-8]
   4b31c:	ldur	x23, [x28, #-16]
   4b320:	b	4b2cc <__gmpn_sbpi1_div_q@@Base+0x1a0>
   4b324:	ldur	x8, [x29, #-16]
   4b328:	cmp	x26, x8
   4b32c:	b.cs	4b338 <__gmpn_sbpi1_div_q@@Base+0x20c>  // b.hs, b.nlast
   4b330:	cmp	x23, x15
   4b334:	b.ls	4b2a0 <__gmpn_sbpi1_div_q@@Base+0x174>  // b.plast
   4b338:	ldur	x9, [x29, #-16]
   4b33c:	subs	x8, x26, x9
   4b340:	sbc	x23, x23, x15
   4b344:	add	x20, x20, #0x1
   4b348:	mov	x26, x8
   4b34c:	b	4b2a0 <__gmpn_sbpi1_div_q@@Base+0x174>
   4b350:	ldur	x2, [x29, #-32]
   4b354:	ldur	x3, [x29, #-48]
   4b358:	mov	x0, x22
   4b35c:	mov	x1, x22
   4b360:	bl	ca70 <__gmpn_add_n@plt>
   4b364:	ldur	x15, [x29, #-8]
   4b368:	sub	x20, x20, #0x1
   4b36c:	add	x8, x23, x15
   4b370:	add	x23, x8, x0
   4b374:	b	4b2cc <__gmpn_sbpi1_div_q@@Base+0x1a0>
   4b378:	add	x8, x24, x25
   4b37c:	stur	x8, [x29, #-40]
   4b380:	add	x8, x21, x25
   4b384:	ldp	x27, x26, [sp, #40]
   4b388:	ldur	x25, [x29, #-56]
   4b38c:	ldur	x24, [x29, #-16]
   4b390:	sub	x22, x8, #0x10
   4b394:	cmp	x25, #0x2
   4b398:	b.lt	4b450 <__gmpn_sbpi1_div_q@@Base+0x324>  // b.tstop
   4b39c:	cmp	x23, x15
   4b3a0:	cset	w8, cs  // cs = hs, nlast
   4b3a4:	cmp	x25, #0x2
   4b3a8:	b.ne	4b460 <__gmpn_sbpi1_div_q@@Base+0x334>  // b.any
   4b3ac:	ldur	x25, [x29, #-32]
   4b3b0:	ldr	x21, [sp, #56]
   4b3b4:	mov	x10, #0xffffffffffffffff    	// #-1
   4b3b8:	mov	x19, x22
   4b3bc:	sub	x22, x22, #0x8
   4b3c0:	stur	x10, [x29, #-48]
   4b3c4:	cbnz	w8, 4b634 <__gmpn_sbpi1_div_q@@Base+0x508>
   4b3c8:	ldur	x10, [x29, #-24]
   4b3cc:	ldr	x8, [x19]
   4b3d0:	ldr	x11, [x22]
   4b3d4:	ldur	x25, [x29, #-56]
   4b3d8:	mul	x9, x23, x10
   4b3dc:	umulh	x10, x23, x10
   4b3e0:	adds	x12, x9, x8
   4b3e4:	adc	x9, x10, x23
   4b3e8:	msub	x8, x9, x15, x8
   4b3ec:	mul	x10, x9, x24
   4b3f0:	umulh	x13, x24, x9
   4b3f4:	subs	x14, x11, x24
   4b3f8:	sbc	x8, x8, x15
   4b3fc:	subs	x11, x14, x10
   4b400:	sbc	x10, x8, x13
   4b404:	cmp	x10, x12
   4b408:	cset	w8, cs  // cs = hs, nlast
   4b40c:	csetm	x12, cs  // cs = hs, nlast
   4b410:	sub	x9, x9, x8
   4b414:	and	x13, x24, x12
   4b418:	and	x12, x15, x12
   4b41c:	adds	x8, x11, x13
   4b420:	adc	x23, x10, x12
   4b424:	cmp	x23, x15
   4b428:	add	x20, x9, #0x1
   4b42c:	b.cs	4b710 <__gmpn_sbpi1_div_q@@Base+0x5e4>  // b.hs, b.nlast
   4b430:	str	x8, [x22]
   4b434:	str	x23, [x19]
   4b438:	b	4b65c <__gmpn_sbpi1_div_q@@Base+0x530>
   4b43c:	add	x8, x22, x27, lsl #3
   4b440:	stur	x8, [x29, #-40]
   4b444:	sub	x22, x19, #0x10
   4b448:	cmp	x25, #0x2
   4b44c:	b.ge	4b39c <__gmpn_sbpi1_div_q@@Base+0x270>  // b.tcont
   4b450:	ldr	x21, [sp, #56]
   4b454:	mov	x8, #0xffffffffffffffff    	// #-1
   4b458:	stur	x8, [x29, #-48]
   4b45c:	b	4b668 <__gmpn_sbpi1_div_q@@Base+0x53c>
   4b460:	mov	w9, #0x1                   	// #1
   4b464:	sub	x9, x9, x25
   4b468:	ldur	x25, [x29, #-32]
   4b46c:	ldr	x2, [sp, #64]
   4b470:	stp	x27, x26, [sp, #40]
   4b474:	mov	x21, xzr
   4b478:	mov	x26, x22
   4b47c:	add	x28, x22, x9, lsl #3
   4b480:	mov	x9, #0xffffffffffffffff    	// #-1
   4b484:	stur	x9, [x29, #-48]
   4b488:	b	4b4e4 <__gmpn_sbpi1_div_q@@Base+0x3b8>
   4b48c:	mov	x3, #0xffffffffffffffff    	// #-1
   4b490:	mov	x0, x28
   4b494:	mov	x1, x25
   4b498:	mov	x20, #0xffffffffffffffff    	// #-1
   4b49c:	mov	x27, x2
   4b4a0:	bl	c9e0 <__gmpn_submul_1@plt>
   4b4a4:	cmp	x23, x0
   4b4a8:	b.ne	4b588 <__gmpn_sbpi1_div_q@@Base+0x45c>  // b.any
   4b4ac:	ldur	x15, [x29, #-8]
   4b4b0:	ldr	x23, [x19]
   4b4b4:	ldp	x9, x8, [x29, #-48]
   4b4b8:	mov	x2, x27
   4b4bc:	add	x25, x25, #0x8
   4b4c0:	sub	x2, x27, #0x1
   4b4c4:	and	x9, x9, x15
   4b4c8:	add	x8, x8, x21
   4b4cc:	cmp	x23, x9
   4b4d0:	sub	x21, x21, #0x8
   4b4d4:	stur	x20, [x8, #-8]
   4b4d8:	cset	w8, cs  // cs = hs, nlast
   4b4dc:	cmp	x22, #0x1
   4b4e0:	b.le	4b614 <__gmpn_sbpi1_div_q@@Base+0x4e8>
   4b4e4:	sub	x22, x2, #0x2
   4b4e8:	add	x19, x26, x21
   4b4ec:	tbnz	w8, #0, 4b48c <__gmpn_sbpi1_div_q@@Base+0x360>
   4b4f0:	ldur	x10, [x29, #-24]
   4b4f4:	ldp	x11, x8, [x19, #-8]
   4b4f8:	mov	x27, x2
   4b4fc:	mul	x9, x23, x10
   4b500:	umulh	x10, x23, x10
   4b504:	adds	x12, x9, x8
   4b508:	adc	x9, x10, x23
   4b50c:	msub	x8, x9, x15, x8
   4b510:	subs	x14, x11, x24
   4b514:	sbc	x8, x8, x15
   4b518:	mul	x10, x9, x24
   4b51c:	umulh	x13, x24, x9
   4b520:	subs	x11, x14, x10
   4b524:	sbc	x8, x8, x13
   4b528:	cmp	x8, x12
   4b52c:	cset	w10, cs  // cs = hs, nlast
   4b530:	csetm	x12, cs  // cs = hs, nlast
   4b534:	sub	x9, x9, x10
   4b538:	and	x10, x24, x12
   4b53c:	and	x12, x15, x12
   4b540:	adds	x24, x11, x10
   4b544:	adc	x23, x8, x12
   4b548:	cmp	x23, x15
   4b54c:	add	x20, x9, #0x1
   4b550:	b.cs	4b5b4 <__gmpn_sbpi1_div_q@@Base+0x488>  // b.hs, b.nlast
   4b554:	mov	x0, x28
   4b558:	mov	x1, x25
   4b55c:	mov	x2, x22
   4b560:	mov	x3, x20
   4b564:	bl	c9e0 <__gmpn_submul_1@plt>
   4b568:	subs	x8, x24, x0
   4b56c:	cset	w9, cc  // cc = lo, ul, last
   4b570:	subs	x23, x23, x9
   4b574:	stur	x8, [x19, #-8]
   4b578:	b.cc	4b5e0 <__gmpn_sbpi1_div_q@@Base+0x4b4>  // b.lo, b.ul, b.last
   4b57c:	ldur	x15, [x29, #-8]
   4b580:	ldur	x24, [x29, #-16]
   4b584:	b	4b4b4 <__gmpn_sbpi1_div_q@@Base+0x388>
   4b588:	ldur	x8, [x29, #-48]
   4b58c:	and	x8, x0, x8
   4b590:	cmp	x23, x8
   4b594:	b.cs	4b608 <__gmpn_sbpi1_div_q@@Base+0x4dc>  // b.hs, b.nlast
   4b598:	mov	x0, x28
   4b59c:	mov	x1, x28
   4b5a0:	mov	x2, x25
   4b5a4:	mov	x3, x27
   4b5a8:	bl	ca70 <__gmpn_add_n@plt>
   4b5ac:	mov	x20, #0xfffffffffffffffe    	// #-2
   4b5b0:	b	4b4ac <__gmpn_sbpi1_div_q@@Base+0x380>
   4b5b4:	ldur	x8, [x29, #-16]
   4b5b8:	cmp	x24, x8
   4b5bc:	b.cs	4b5c8 <__gmpn_sbpi1_div_q@@Base+0x49c>  // b.hs, b.nlast
   4b5c0:	cmp	x23, x15
   4b5c4:	b.ls	4b554 <__gmpn_sbpi1_div_q@@Base+0x428>  // b.plast
   4b5c8:	ldur	x9, [x29, #-16]
   4b5cc:	subs	x8, x24, x9
   4b5d0:	sbc	x23, x23, x15
   4b5d4:	add	x20, x20, #0x1
   4b5d8:	mov	x24, x8
   4b5dc:	b	4b554 <__gmpn_sbpi1_div_q@@Base+0x428>
   4b5e0:	sub	x3, x27, #0x1
   4b5e4:	mov	x0, x28
   4b5e8:	mov	x1, x28
   4b5ec:	mov	x2, x25
   4b5f0:	bl	ca70 <__gmpn_add_n@plt>
   4b5f4:	ldur	x15, [x29, #-8]
   4b5f8:	sub	x20, x20, #0x1
   4b5fc:	add	x8, x23, x15
   4b600:	add	x23, x8, x0
   4b604:	b	4b580 <__gmpn_sbpi1_div_q@@Base+0x454>
   4b608:	stur	xzr, [x29, #-48]
   4b60c:	mov	x20, #0xffffffffffffffff    	// #-1
   4b610:	b	4b4ac <__gmpn_sbpi1_div_q@@Base+0x380>
   4b614:	ldur	x9, [x29, #-40]
   4b618:	add	x19, x26, x21
   4b61c:	ldr	x27, [sp, #40]
   4b620:	sub	x22, x19, #0x8
   4b624:	add	x9, x9, x21
   4b628:	ldp	x26, x21, [sp, #48]
   4b62c:	stur	x9, [x29, #-40]
   4b630:	cbz	w8, 4b3c8 <__gmpn_sbpi1_div_q@@Base+0x29c>
   4b634:	mov	w2, #0x2                   	// #2
   4b638:	mov	x3, #0xffffffffffffffff    	// #-1
   4b63c:	mov	x0, x22
   4b640:	mov	x1, x25
   4b644:	mov	x20, #0xffffffffffffffff    	// #-1
   4b648:	bl	c9e0 <__gmpn_submul_1@plt>
   4b64c:	cmp	x23, x0
   4b650:	b.ne	4b734 <__gmpn_sbpi1_div_q@@Base+0x608>  // b.any
   4b654:	ldr	x23, [x19]
   4b658:	ldur	x25, [x29, #-56]
   4b65c:	ldur	x8, [x29, #-40]
   4b660:	str	x20, [x8, #-8]!
   4b664:	stur	x8, [x29, #-40]
   4b668:	ldr	x8, [x22, #8]
   4b66c:	cmp	x8, x23
   4b670:	b.ne	4b930 <__gmpn_sbpi1_div_q@@Base+0x804>  // b.any
   4b674:	ldur	x8, [x29, #-48]
   4b678:	and	x8, x8, x21
   4b67c:	cmp	x23, x8
   4b680:	b.cc	4b6a8 <__gmpn_sbpi1_div_q@@Base+0x57c>  // b.lo, b.ul, b.last
   4b684:	mov	x0, x26
   4b688:	ldp	x20, x19, [sp, #208]
   4b68c:	ldp	x22, x21, [sp, #192]
   4b690:	ldp	x24, x23, [sp, #176]
   4b694:	ldp	x26, x25, [sp, #160]
   4b698:	ldp	x28, x27, [sp, #144]
   4b69c:	ldp	x29, x30, [sp, #128]
   4b6a0:	add	sp, sp, #0xe0
   4b6a4:	ret
   4b6a8:	cmp	x21, #0x3
   4b6ac:	b.lt	4b774 <__gmpn_sbpi1_div_q@@Base+0x648>  // b.tstop
   4b6b0:	mov	x24, x21
   4b6b4:	ldr	x21, [x22]
   4b6b8:	mov	x8, x22
   4b6bc:	subs	x22, x25, #0x3
   4b6c0:	b.lt	4b768 <__gmpn_sbpi1_div_q@@Base+0x63c>  // b.tstop
   4b6c4:	mov	x25, x8
   4b6c8:	sub	x19, x8, #0x8
   4b6cc:	mov	w20, #0x1                   	// #1
   4b6d0:	b	4b6e8 <__gmpn_sbpi1_div_q@@Base+0x5bc>
   4b6d4:	cmp	x22, #0x0
   4b6d8:	sub	x22, x22, #0x1
   4b6dc:	add	x20, x20, #0x1
   4b6e0:	sub	x19, x19, #0x8
   4b6e4:	b.le	4b76c <__gmpn_sbpi1_div_q@@Base+0x640>
   4b6e8:	ldp	x8, x1, [x29, #-40]
   4b6ec:	mov	x0, x19
   4b6f0:	mov	x2, x20
   4b6f4:	ldr	x3, [x8, x22, lsl #3]
   4b6f8:	bl	c9e0 <__gmpn_submul_1@plt>
   4b6fc:	subs	x21, x21, x0
   4b700:	b.cs	4b6d4 <__gmpn_sbpi1_div_q@@Base+0x5a8>  // b.hs, b.nlast
   4b704:	cbz	x23, 4b858 <__gmpn_sbpi1_div_q@@Base+0x72c>
   4b708:	sub	x23, x23, #0x1
   4b70c:	b	4b6d4 <__gmpn_sbpi1_div_q@@Base+0x5a8>
   4b710:	cmp	x8, x24
   4b714:	b.cs	4b720 <__gmpn_sbpi1_div_q@@Base+0x5f4>  // b.hs, b.nlast
   4b718:	cmp	x23, x15
   4b71c:	b.ls	4b430 <__gmpn_sbpi1_div_q@@Base+0x304>  // b.plast
   4b720:	subs	x9, x8, x24
   4b724:	sbc	x23, x23, x15
   4b728:	add	x20, x20, #0x1
   4b72c:	mov	x8, x9
   4b730:	b	4b430 <__gmpn_sbpi1_div_q@@Base+0x304>
   4b734:	ldur	x8, [x29, #-48]
   4b738:	and	x8, x0, x8
   4b73c:	cmp	x23, x8
   4b740:	b.cs	4b84c <__gmpn_sbpi1_div_q@@Base+0x720>  // b.hs, b.nlast
   4b744:	ldr	x8, [x19]
   4b748:	mov	x20, #0xfffffffffffffffe    	// #-2
   4b74c:	ldp	x9, x10, [x25]
   4b750:	ldr	x11, [x22]
   4b754:	adds	x12, x11, x9
   4b758:	adc	x8, x8, x10
   4b75c:	str	x8, [x19]
   4b760:	str	x12, [x22]
   4b764:	b	4b654 <__gmpn_sbpi1_div_q@@Base+0x528>
   4b768:	mov	x25, x8
   4b76c:	str	x21, [x25]
   4b770:	mov	x21, x24
   4b774:	ldr	x8, [sp, #16]
   4b778:	cmp	x8, x21
   4b77c:	b.ge	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tcont
   4b780:	ldr	w8, [sp, #12]
   4b784:	cbz	w8, 4b7a8 <__gmpn_sbpi1_div_q@@Base+0x67c>
   4b788:	ldp	x2, x8, [sp, #24]
   4b78c:	ldr	x3, [sp]
   4b790:	add	x0, x8, x27, lsl #3
   4b794:	mov	x1, x0
   4b798:	bl	c2d0 <__gmpn_sub_n@plt>
   4b79c:	cbz	x0, 4b7a8 <__gmpn_sbpi1_div_q@@Base+0x67c>
   4b7a0:	cbz	x23, 4b89c <__gmpn_sbpi1_div_q@@Base+0x770>
   4b7a4:	sub	x23, x23, #0x1
   4b7a8:	cbz	x27, 4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b7ac:	sub	x19, x21, x27
   4b7b0:	subs	x20, x19, #0x2
   4b7b4:	b.lt	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tstop
   4b7b8:	ldr	x8, [sp, #32]
   4b7bc:	mov	x9, x21
   4b7c0:	add	x21, x8, x27, lsl #3
   4b7c4:	add	x22, x8, x9, lsl #3
   4b7c8:	b	4b7dc <__gmpn_sbpi1_div_q@@Base+0x6b0>
   4b7cc:	cmp	x20, #0x0
   4b7d0:	sub	x20, x20, #0x1
   4b7d4:	sub	x22, x22, #0x8
   4b7d8:	b.le	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b7dc:	ldr	x8, [sp, #24]
   4b7e0:	lsl	x24, x20, #3
   4b7e4:	ldur	x1, [x29, #-40]
   4b7e8:	mov	x2, x27
   4b7ec:	ldr	x3, [x8, x24]
   4b7f0:	ldr	x8, [sp, #32]
   4b7f4:	add	x0, x8, x24
   4b7f8:	bl	c9e0 <__gmpn_submul_1@plt>
   4b7fc:	ldr	x8, [x21, x24]
   4b800:	subs	x8, x8, x0
   4b804:	str	x8, [x21, x24]
   4b808:	b.cs	4b7cc <__gmpn_sbpi1_div_q@@Base+0x6a0>  // b.hs, b.nlast
   4b80c:	mvn	x8, x20
   4b810:	add	x8, x19, x8
   4b814:	mov	x9, #0xffffffffffffffff    	// #-1
   4b818:	add	x10, x9, #0x2
   4b81c:	cmp	x10, x8
   4b820:	b.ge	4b840 <__gmpn_sbpi1_div_q@@Base+0x714>  // b.tcont
   4b824:	lsl	x10, x9, #3
   4b828:	ldr	x11, [x22, x10]
   4b82c:	add	x9, x9, #0x1
   4b830:	sub	x12, x11, #0x1
   4b834:	str	x12, [x22, x10]
   4b838:	cbz	x11, 4b818 <__gmpn_sbpi1_div_q@@Base+0x6ec>
   4b83c:	b	4b7cc <__gmpn_sbpi1_div_q@@Base+0x6a0>
   4b840:	cbz	x23, 4b8f0 <__gmpn_sbpi1_div_q@@Base+0x7c4>
   4b844:	sub	x23, x23, #0x1
   4b848:	b	4b7cc <__gmpn_sbpi1_div_q@@Base+0x6a0>
   4b84c:	stur	xzr, [x29, #-48]
   4b850:	mov	x20, #0xffffffffffffffff    	// #-1
   4b854:	b	4b654 <__gmpn_sbpi1_div_q@@Base+0x528>
   4b858:	ldur	x10, [x29, #-40]
   4b85c:	ldr	x8, [x10]
   4b860:	sub	x9, x8, #0x1
   4b864:	str	x9, [x10]
   4b868:	cbnz	x8, 4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b86c:	ldur	x13, [x29, #-40]
   4b870:	mov	w8, #0x1                   	// #1
   4b874:	mov	x12, x27
   4b878:	cmp	x8, x12
   4b87c:	b.ge	4b948 <__gmpn_sbpi1_div_q@@Base+0x81c>  // b.tcont
   4b880:	lsl	x9, x8, #3
   4b884:	ldr	x10, [x13, x9]
   4b888:	add	x8, x8, #0x1
   4b88c:	sub	x11, x10, #0x1
   4b890:	str	x11, [x13, x9]
   4b894:	cbz	x10, 4b878 <__gmpn_sbpi1_div_q@@Base+0x74c>
   4b898:	b	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b89c:	cbz	x27, 4b8e8 <__gmpn_sbpi1_div_q@@Base+0x7bc>
   4b8a0:	ldur	x10, [x29, #-40]
   4b8a4:	ldr	x8, [x10]
   4b8a8:	sub	x9, x8, #0x1
   4b8ac:	str	x9, [x10]
   4b8b0:	cbnz	x8, 4b8e4 <__gmpn_sbpi1_div_q@@Base+0x7b8>
   4b8b4:	ldur	x13, [x29, #-40]
   4b8b8:	mov	x12, x27
   4b8bc:	mov	w0, #0x1                   	// #1
   4b8c0:	mov	w8, #0x1                   	// #1
   4b8c4:	cmp	x8, x12
   4b8c8:	b.ge	4b8e8 <__gmpn_sbpi1_div_q@@Base+0x7bc>  // b.tcont
   4b8cc:	lsl	x9, x8, #3
   4b8d0:	ldr	x10, [x13, x9]
   4b8d4:	add	x8, x8, #0x1
   4b8d8:	sub	x11, x10, #0x1
   4b8dc:	str	x11, [x13, x9]
   4b8e0:	cbz	x10, 4b8c4 <__gmpn_sbpi1_div_q@@Base+0x798>
   4b8e4:	mov	x0, xzr
   4b8e8:	sub	x26, x26, x0
   4b8ec:	b	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b8f0:	ldur	x10, [x29, #-40]
   4b8f4:	ldr	x8, [x10]
   4b8f8:	sub	x9, x8, #0x1
   4b8fc:	str	x9, [x10]
   4b900:	cbnz	x8, 4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b904:	ldur	x12, [x29, #-40]
   4b908:	mov	w8, #0x1                   	// #1
   4b90c:	cmp	x8, x27
   4b910:	b.ge	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tcont
   4b914:	lsl	x9, x8, #3
   4b918:	ldr	x10, [x12, x9]
   4b91c:	add	x8, x8, #0x1
   4b920:	sub	x11, x10, #0x1
   4b924:	str	x11, [x12, x9]
   4b928:	cbz	x10, 4b90c <__gmpn_sbpi1_div_q@@Base+0x7e0>
   4b92c:	b	4b684 <__gmpn_sbpi1_div_q@@Base+0x558>
   4b930:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4b934:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4b938:	add	x0, x0, #0x6f7
   4b93c:	add	x2, x2, #0x705
   4b940:	mov	w1, #0xc5                  	// #197
   4b944:	bl	c6c0 <__gmp_assert_fail@plt>
   4b948:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4b94c:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4b950:	add	x0, x0, #0x6f7
   4b954:	add	x2, x2, #0x711
   4b958:	mov	w1, #0xf8                  	// #248
   4b95c:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004b960 <__gmpn_sbpi1_div_qr@@Base>:
   4b960:	sub	sp, sp, #0xb0
   4b964:	stp	x20, x19, [sp, #160]
   4b968:	add	x20, x1, x2, lsl #3
   4b96c:	stp	x29, x30, [sp, #80]
   4b970:	stp	x28, x27, [sp, #96]
   4b974:	stp	x24, x23, [sp, #128]
   4b978:	stp	x22, x21, [sp, #144]
   4b97c:	add	x29, sp, #0x50
   4b980:	mov	x21, x4
   4b984:	mov	x22, x2
   4b988:	mov	x23, x1
   4b98c:	mov	x27, x0
   4b990:	sub	x0, x20, x4, lsl #3
   4b994:	sub	x8, x20, #0x8
   4b998:	mov	x9, x4
   4b99c:	mov	x12, x3
   4b9a0:	stp	x26, x25, [sp, #112]
   4b9a4:	stp	x5, x3, [x29, #-16]
   4b9a8:	subs	x10, x9, #0x1
   4b9ac:	b.lt	4b9cc <__gmpn_sbpi1_div_qr@@Base+0x6c>  // b.tstop
   4b9b0:	add	x9, x12, x9, lsl #3
   4b9b4:	ldr	x11, [x8], #-8
   4b9b8:	ldur	x9, [x9, #-8]
   4b9bc:	cmp	x11, x9
   4b9c0:	mov	x9, x10
   4b9c4:	b.eq	4b9a8 <__gmpn_sbpi1_div_qr@@Base+0x48>  // b.none
   4b9c8:	b.ls	4bb8c <__gmpn_sbpi1_div_qr@@Base+0x22c>  // b.plast
   4b9cc:	ldur	x2, [x29, #-8]
   4b9d0:	mov	x1, x0
   4b9d4:	mov	x3, x21
   4b9d8:	bl	c2d0 <__gmpn_sub_n@plt>
   4b9dc:	mov	w0, #0x1                   	// #1
   4b9e0:	ldur	x19, [x20, #-8]
   4b9e4:	sub	x8, x22, x21
   4b9e8:	cmp	x8, #0x1
   4b9ec:	b.lt	4bba0 <__gmpn_sbpi1_div_qr@@Base+0x240>  // b.tstop
   4b9f0:	ldur	x10, [x29, #-8]
   4b9f4:	stp	x0, x22, [sp, #24]
   4b9f8:	sub	x11, x21, #0x2
   4b9fc:	sub	x12, x21, #0x1
   4ba00:	ldr	x16, [x10, x12, lsl #3]
   4ba04:	ldr	x10, [x10, x11, lsl #3]
   4ba08:	mvn	x9, x21
   4ba0c:	lsl	x9, x9, #3
   4ba10:	lsl	x28, x22, #3
   4ba14:	add	x20, x27, x9
   4ba18:	add	x25, x23, x9
   4ba1c:	add	x22, x8, #0x1
   4ba20:	stp	x21, x12, [sp, #8]
   4ba24:	str	x11, [sp, #40]
   4ba28:	stp	x16, x10, [x29, #-32]
   4ba2c:	cmp	x19, x16
   4ba30:	add	x26, x23, x28
   4ba34:	b.eq	4baf0 <__gmpn_sbpi1_div_qr@@Base+0x190>  // b.none
   4ba38:	ldp	x15, x10, [x29, #-24]
   4ba3c:	ldp	x11, x8, [x26, #-24]
   4ba40:	mul	x9, x19, x10
   4ba44:	umulh	x10, x19, x10
   4ba48:	adds	x12, x9, x8
   4ba4c:	adc	x9, x10, x19
   4ba50:	msub	x8, x9, x16, x8
   4ba54:	subs	x14, x11, x15
   4ba58:	sbc	x8, x8, x16
   4ba5c:	mul	x10, x9, x15
   4ba60:	umulh	x13, x15, x9
   4ba64:	subs	x11, x14, x10
   4ba68:	sbc	x8, x8, x13
   4ba6c:	cmp	x8, x12
   4ba70:	cset	w10, cs  // cs = hs, nlast
   4ba74:	csetm	x12, cs  // cs = hs, nlast
   4ba78:	sub	x9, x9, x10
   4ba7c:	and	x10, x15, x12
   4ba80:	and	x12, x16, x12
   4ba84:	adds	x21, x11, x10
   4ba88:	adc	x19, x8, x12
   4ba8c:	cmp	x19, x16
   4ba90:	add	x27, x9, #0x1
   4ba94:	b.cs	4bb24 <__gmpn_sbpi1_div_qr@@Base+0x1c4>  // b.hs, b.nlast
   4ba98:	ldur	x1, [x29, #-8]
   4ba9c:	ldr	x2, [sp, #40]
   4baa0:	mov	x24, x28
   4baa4:	add	x28, x25, x28
   4baa8:	mov	x0, x28
   4baac:	mov	x3, x27
   4bab0:	bl	c9e0 <__gmpn_submul_1@plt>
   4bab4:	subs	x8, x21, x0
   4bab8:	cset	w9, cc  // cc = lo, ul, last
   4babc:	subs	x19, x19, x9
   4bac0:	stur	x8, [x26, #-24]
   4bac4:	b.cc	4bb50 <__gmpn_sbpi1_div_qr@@Base+0x1f0>  // b.lo, b.ul, b.last
   4bac8:	ldur	x16, [x29, #-32]
   4bacc:	mov	x28, x24
   4bad0:	sub	x22, x22, #0x1
   4bad4:	str	x27, [x20, x28]
   4bad8:	sub	x20, x20, #0x8
   4badc:	sub	x23, x23, #0x8
   4bae0:	cmp	x22, #0x1
   4bae4:	sub	x25, x25, #0x8
   4bae8:	b.gt	4ba2c <__gmpn_sbpi1_div_qr@@Base+0xcc>
   4baec:	b	4bb7c <__gmpn_sbpi1_div_qr@@Base+0x21c>
   4baf0:	ldur	x8, [x26, #-16]
   4baf4:	ldur	x9, [x29, #-24]
   4baf8:	cmp	x8, x9
   4bafc:	b.ne	4ba38 <__gmpn_sbpi1_div_qr@@Base+0xd8>  // b.any
   4bb00:	ldur	x1, [x29, #-8]
   4bb04:	ldr	x2, [sp, #8]
   4bb08:	add	x0, x25, x28
   4bb0c:	mov	x3, #0xffffffffffffffff    	// #-1
   4bb10:	mov	x27, #0xffffffffffffffff    	// #-1
   4bb14:	bl	c9e0 <__gmpn_submul_1@plt>
   4bb18:	ldur	x16, [x29, #-32]
   4bb1c:	ldur	x19, [x26, #-16]
   4bb20:	b	4bad0 <__gmpn_sbpi1_div_qr@@Base+0x170>
   4bb24:	ldur	x8, [x29, #-24]
   4bb28:	cmp	x21, x8
   4bb2c:	b.cs	4bb38 <__gmpn_sbpi1_div_qr@@Base+0x1d8>  // b.hs, b.nlast
   4bb30:	cmp	x19, x16
   4bb34:	b.ls	4ba98 <__gmpn_sbpi1_div_qr@@Base+0x138>  // b.plast
   4bb38:	ldur	x9, [x29, #-24]
   4bb3c:	subs	x8, x21, x9
   4bb40:	sbc	x19, x19, x16
   4bb44:	add	x27, x27, #0x1
   4bb48:	mov	x21, x8
   4bb4c:	b	4ba98 <__gmpn_sbpi1_div_qr@@Base+0x138>
   4bb50:	ldur	x2, [x29, #-8]
   4bb54:	ldr	x3, [sp, #16]
   4bb58:	mov	x0, x28
   4bb5c:	mov	x1, x28
   4bb60:	bl	ca70 <__gmpn_add_n@plt>
   4bb64:	ldur	x16, [x29, #-32]
   4bb68:	sub	x27, x27, #0x1
   4bb6c:	mov	x28, x24
   4bb70:	add	x8, x19, x16
   4bb74:	add	x19, x8, x0
   4bb78:	b	4bad0 <__gmpn_sbpi1_div_qr@@Base+0x170>
   4bb7c:	ldp	x0, x8, [sp, #24]
   4bb80:	add	x8, x23, x8, lsl #3
   4bb84:	sub	x8, x8, #0x10
   4bb88:	b	4bba4 <__gmpn_sbpi1_div_qr@@Base+0x244>
   4bb8c:	mov	x0, xzr
   4bb90:	ldur	x19, [x20, #-8]
   4bb94:	sub	x8, x22, x21
   4bb98:	cmp	x8, #0x1
   4bb9c:	b.ge	4b9f0 <__gmpn_sbpi1_div_qr@@Base+0x90>  // b.tcont
   4bba0:	sub	x8, x20, #0x10
   4bba4:	str	x19, [x8, #8]
   4bba8:	ldp	x20, x19, [sp, #160]
   4bbac:	ldp	x22, x21, [sp, #144]
   4bbb0:	ldp	x24, x23, [sp, #128]
   4bbb4:	ldp	x26, x25, [sp, #112]
   4bbb8:	ldp	x28, x27, [sp, #96]
   4bbbc:	ldp	x29, x30, [sp, #80]
   4bbc0:	add	sp, sp, #0xb0
   4bbc4:	ret

000000000004bbc8 <__gmpn_sbpi1_divappr_q@@Base>:
   4bbc8:	sub	sp, sp, #0xa0
   4bbcc:	stp	x22, x21, [sp, #128]
   4bbd0:	sub	x22, x2, x4
   4bbd4:	add	x8, x22, #0x1
   4bbd8:	subs	x8, x4, x8
   4bbdc:	stp	x20, x19, [sp, #144]
   4bbe0:	add	x20, x1, x2, lsl #3
   4bbe4:	add	x8, x3, x8, lsl #3
   4bbe8:	csinc	x12, x4, x22, le
   4bbec:	stp	x29, x30, [sp, #64]
   4bbf0:	stp	x28, x27, [sp, #80]
   4bbf4:	stp	x26, x25, [sp, #96]
   4bbf8:	stp	x24, x23, [sp, #112]
   4bbfc:	add	x29, sp, #0x40
   4bc00:	mov	x27, x4
   4bc04:	mov	x26, x2
   4bc08:	mov	x23, x1
   4bc0c:	mov	x25, x0
   4bc10:	csel	x15, x8, x3, gt
   4bc14:	sub	x0, x20, x12, lsl #3
   4bc18:	sub	x8, x20, #0x8
   4bc1c:	mov	x9, x12
   4bc20:	stur	x5, [x29, #-24]
   4bc24:	str	x15, [sp, #32]
   4bc28:	subs	x10, x9, #0x1
   4bc2c:	b.lt	4bc4c <__gmpn_sbpi1_divappr_q@@Base+0x84>  // b.tstop
   4bc30:	add	x9, x15, x9, lsl #3
   4bc34:	ldr	x11, [x8], #-8
   4bc38:	ldur	x9, [x9, #-8]
   4bc3c:	cmp	x11, x9
   4bc40:	mov	x9, x10
   4bc44:	b.eq	4bc28 <__gmpn_sbpi1_divappr_q@@Base+0x60>  // b.none
   4bc48:	b.ls	4bc70 <__gmpn_sbpi1_divappr_q@@Base+0xa8>  // b.plast
   4bc4c:	mov	x1, x0
   4bc50:	mov	x2, x15
   4bc54:	mov	x3, x12
   4bc58:	mov	x19, x12
   4bc5c:	bl	c2d0 <__gmpn_sub_n@plt>
   4bc60:	ldr	x15, [sp, #32]
   4bc64:	mov	x12, x19
   4bc68:	mov	w19, #0x1                   	// #1
   4bc6c:	b	4bc74 <__gmpn_sbpi1_divappr_q@@Base+0xac>
   4bc70:	mov	x19, xzr
   4bc74:	sub	x8, x12, #0x2
   4bc78:	sub	x9, x12, #0x1
   4bc7c:	ldr	x16, [x15, x9, lsl #3]
   4bc80:	str	x8, [sp, #24]
   4bc84:	ldr	x8, [x15, x8, lsl #3]
   4bc88:	stp	x8, x16, [x29, #-16]
   4bc8c:	ldur	x21, [x20, #-8]
   4bc90:	subs	x8, x22, x12
   4bc94:	b.mi	4bea4 <__gmpn_sbpi1_divappr_q@@Base+0x2dc>  // b.first
   4bc98:	add	x22, x8, #0x1
   4bc9c:	mvn	x8, x12
   4bca0:	sub	x24, x25, x27, lsl #3
   4bca4:	lsl	x25, x26, #3
   4bca8:	add	x20, x23, x8, lsl #3
   4bcac:	stp	x9, x12, [sp]
   4bcb0:	str	x19, [sp, #16]
   4bcb4:	ldur	x17, [x29, #-16]
   4bcb8:	cmp	x21, x16
   4bcbc:	add	x28, x23, x25
   4bcc0:	b.eq	4bd7c <__gmpn_sbpi1_divappr_q@@Base+0x1b4>  // b.none
   4bcc4:	ldur	x10, [x29, #-24]
   4bcc8:	ldp	x11, x8, [x28, #-24]
   4bccc:	mul	x9, x21, x10
   4bcd0:	umulh	x10, x21, x10
   4bcd4:	adds	x12, x9, x8
   4bcd8:	adc	x9, x10, x21
   4bcdc:	msub	x8, x9, x16, x8
   4bce0:	subs	x14, x11, x17
   4bce4:	sbc	x8, x8, x16
   4bce8:	mul	x10, x9, x17
   4bcec:	umulh	x13, x17, x9
   4bcf0:	subs	x11, x14, x10
   4bcf4:	sbc	x8, x8, x13
   4bcf8:	cmp	x8, x12
   4bcfc:	cset	w10, cs  // cs = hs, nlast
   4bd00:	csetm	x12, cs  // cs = hs, nlast
   4bd04:	sub	x9, x9, x10
   4bd08:	and	x10, x17, x12
   4bd0c:	and	x12, x16, x12
   4bd10:	adds	x19, x11, x10
   4bd14:	adc	x21, x8, x12
   4bd18:	cmp	x21, x16
   4bd1c:	add	x26, x9, #0x1
   4bd20:	b.cs	4bdac <__gmpn_sbpi1_divappr_q@@Base+0x1e4>  // b.hs, b.nlast
   4bd24:	ldr	x2, [sp, #24]
   4bd28:	add	x27, x20, x25
   4bd2c:	mov	x0, x27
   4bd30:	mov	x1, x15
   4bd34:	mov	x3, x26
   4bd38:	bl	c9e0 <__gmpn_submul_1@plt>
   4bd3c:	subs	x8, x19, x0
   4bd40:	cset	w9, cc  // cc = lo, ul, last
   4bd44:	subs	x21, x21, x9
   4bd48:	stur	x8, [x28, #-24]
   4bd4c:	b.cc	4bdd8 <__gmpn_sbpi1_divappr_q@@Base+0x210>  // b.lo, b.ul, b.last
   4bd50:	ldur	x16, [x29, #-8]
   4bd54:	ldr	x15, [sp, #32]
   4bd58:	sub	x22, x22, #0x1
   4bd5c:	add	x8, x24, x25
   4bd60:	sub	x24, x24, #0x8
   4bd64:	sub	x23, x23, #0x8
   4bd68:	cmp	x22, #0x0
   4bd6c:	sub	x20, x20, #0x8
   4bd70:	stur	x26, [x8, #-8]
   4bd74:	b.gt	4bcb4 <__gmpn_sbpi1_divappr_q@@Base+0xec>
   4bd78:	b	4be00 <__gmpn_sbpi1_divappr_q@@Base+0x238>
   4bd7c:	ldur	x8, [x28, #-16]
   4bd80:	cmp	x8, x17
   4bd84:	b.ne	4bcc4 <__gmpn_sbpi1_divappr_q@@Base+0xfc>  // b.any
   4bd88:	ldr	x2, [sp, #8]
   4bd8c:	add	x0, x20, x25
   4bd90:	mov	x3, #0xffffffffffffffff    	// #-1
   4bd94:	mov	x1, x15
   4bd98:	mov	x26, #0xffffffffffffffff    	// #-1
   4bd9c:	bl	c9e0 <__gmpn_submul_1@plt>
   4bda0:	ldur	x16, [x29, #-8]
   4bda4:	ldur	x21, [x28, #-16]
   4bda8:	b	4bd54 <__gmpn_sbpi1_divappr_q@@Base+0x18c>
   4bdac:	ldur	x8, [x29, #-16]
   4bdb0:	cmp	x19, x8
   4bdb4:	b.cs	4bdc0 <__gmpn_sbpi1_divappr_q@@Base+0x1f8>  // b.hs, b.nlast
   4bdb8:	cmp	x21, x16
   4bdbc:	b.ls	4bd24 <__gmpn_sbpi1_divappr_q@@Base+0x15c>  // b.plast
   4bdc0:	ldur	x9, [x29, #-16]
   4bdc4:	subs	x8, x19, x9
   4bdc8:	sbc	x21, x21, x16
   4bdcc:	add	x26, x26, #0x1
   4bdd0:	mov	x19, x8
   4bdd4:	b	4bd24 <__gmpn_sbpi1_divappr_q@@Base+0x15c>
   4bdd8:	ldr	x2, [sp, #32]
   4bddc:	ldr	x3, [sp]
   4bde0:	mov	x0, x27
   4bde4:	mov	x1, x27
   4bde8:	bl	ca70 <__gmpn_add_n@plt>
   4bdec:	ldur	x16, [x29, #-8]
   4bdf0:	sub	x26, x26, #0x1
   4bdf4:	add	x8, x21, x16
   4bdf8:	add	x21, x8, x0
   4bdfc:	b	4bd54 <__gmpn_sbpi1_divappr_q@@Base+0x18c>
   4be00:	ldp	x12, x19, [sp, #8]
   4be04:	add	x8, x23, x25
   4be08:	add	x28, x24, x25
   4be0c:	sub	x26, x8, #0x10
   4be10:	cmp	x12, #0x2
   4be14:	b.lt	4beb4 <__gmpn_sbpi1_divappr_q@@Base+0x2ec>  // b.tstop
   4be18:	cmp	x21, x16
   4be1c:	cset	w8, cs  // cs = hs, nlast
   4be20:	cmp	x12, #0x2
   4be24:	b.ne	4bebc <__gmpn_sbpi1_divappr_q@@Base+0x2f4>  // b.any
   4be28:	sub	x22, x26, #0x8
   4be2c:	mov	x23, #0xffffffffffffffff    	// #-1
   4be30:	cbnz	w8, 4c0a8 <__gmpn_sbpi1_divappr_q@@Base+0x4e0>
   4be34:	ldp	x10, x15, [x29, #-24]
   4be38:	ldr	x8, [x26]
   4be3c:	ldr	x11, [x22]
   4be40:	mul	x9, x21, x10
   4be44:	umulh	x10, x21, x10
   4be48:	adds	x12, x9, x8
   4be4c:	adc	x9, x10, x21
   4be50:	msub	x8, x9, x16, x8
   4be54:	mul	x10, x9, x15
   4be58:	umulh	x13, x15, x9
   4be5c:	subs	x14, x11, x15
   4be60:	sbc	x8, x8, x16
   4be64:	subs	x11, x14, x10
   4be68:	sbc	x10, x8, x13
   4be6c:	cmp	x10, x12
   4be70:	cset	w8, cs  // cs = hs, nlast
   4be74:	csetm	x12, cs  // cs = hs, nlast
   4be78:	sub	x9, x9, x8
   4be7c:	and	x13, x15, x12
   4be80:	and	x12, x16, x12
   4be84:	adds	x8, x11, x13
   4be88:	adc	x21, x10, x12
   4be8c:	cmp	x21, x16
   4be90:	add	x20, x9, #0x1
   4be94:	b.cs	4c114 <__gmpn_sbpi1_divappr_q@@Base+0x54c>  // b.hs, b.nlast
   4be98:	str	x21, [x26]
   4be9c:	str	x8, [x22]
   4bea0:	b	4c0e0 <__gmpn_sbpi1_divappr_q@@Base+0x518>
   4bea4:	add	x28, x25, x22, lsl #3
   4bea8:	sub	x26, x20, #0x10
   4beac:	cmp	x12, #0x2
   4beb0:	b.ge	4be18 <__gmpn_sbpi1_divappr_q@@Base+0x250>  // b.tcont
   4beb4:	mov	x22, x26
   4beb8:	b	4c0e4 <__gmpn_sbpi1_divappr_q@@Base+0x51c>
   4bebc:	mov	w9, #0x1                   	// #1
   4bec0:	sub	x9, x9, x12
   4bec4:	mov	x27, xzr
   4bec8:	add	x9, x26, x9, lsl #3
   4becc:	mov	x23, #0xffffffffffffffff    	// #-1
   4bed0:	str	x19, [sp, #16]
   4bed4:	str	x9, [sp, #32]
   4bed8:	b	4bf3c <__gmpn_sbpi1_divappr_q@@Base+0x374>
   4bedc:	ldr	x0, [sp, #32]
   4bee0:	mov	x3, #0xffffffffffffffff    	// #-1
   4bee4:	mov	x1, x15
   4bee8:	mov	x2, x12
   4beec:	mov	x25, #0xffffffffffffffff    	// #-1
   4bef0:	mov	x22, x15
   4bef4:	mov	x19, x12
   4bef8:	bl	c9e0 <__gmpn_submul_1@plt>
   4befc:	cmp	x21, x0
   4bf00:	b.ne	4bffc <__gmpn_sbpi1_divappr_q@@Base+0x434>  // b.any
   4bf04:	ldur	x16, [x29, #-8]
   4bf08:	ldr	x21, [x20]
   4bf0c:	mov	x12, x19
   4bf10:	and	x9, x23, x16
   4bf14:	add	x8, x28, x27
   4bf18:	mov	x15, x22
   4bf1c:	cmp	x21, x9
   4bf20:	add	x15, x22, #0x8
   4bf24:	sub	x27, x27, #0x8
   4bf28:	stur	x25, [x8, #-8]
   4bf2c:	cset	w8, cs  // cs = hs, nlast
   4bf30:	cmp	x24, #0x1
   4bf34:	sub	x12, x12, #0x1
   4bf38:	b.le	4c094 <__gmpn_sbpi1_divappr_q@@Base+0x4cc>
   4bf3c:	sub	x24, x12, #0x2
   4bf40:	add	x20, x26, x27
   4bf44:	tbnz	w8, #0, 4bedc <__gmpn_sbpi1_divappr_q@@Base+0x314>
   4bf48:	str	x23, [sp, #24]
   4bf4c:	ldp	x10, x17, [x29, #-24]
   4bf50:	ldp	x11, x8, [x20, #-8]
   4bf54:	mov	x23, x26
   4bf58:	mov	x26, x28
   4bf5c:	mul	x9, x21, x10
   4bf60:	mov	x28, x12
   4bf64:	umulh	x10, x21, x10
   4bf68:	adds	x12, x9, x8
   4bf6c:	adc	x9, x10, x21
   4bf70:	msub	x8, x9, x16, x8
   4bf74:	subs	x14, x11, x17
   4bf78:	sbc	x8, x8, x16
   4bf7c:	mul	x10, x9, x17
   4bf80:	umulh	x13, x17, x9
   4bf84:	subs	x11, x14, x10
   4bf88:	sbc	x8, x8, x13
   4bf8c:	cmp	x8, x12
   4bf90:	cset	w10, cs  // cs = hs, nlast
   4bf94:	csetm	x12, cs  // cs = hs, nlast
   4bf98:	sub	x9, x9, x10
   4bf9c:	and	x10, x17, x12
   4bfa0:	and	x12, x16, x12
   4bfa4:	adds	x19, x11, x10
   4bfa8:	adc	x21, x8, x12
   4bfac:	cmp	x21, x16
   4bfb0:	add	x25, x9, #0x1
   4bfb4:	b.cs	4c05c <__gmpn_sbpi1_divappr_q@@Base+0x494>  // b.hs, b.nlast
   4bfb8:	ldr	x0, [sp, #32]
   4bfbc:	mov	x1, x15
   4bfc0:	mov	x2, x24
   4bfc4:	mov	x3, x25
   4bfc8:	mov	x22, x15
   4bfcc:	bl	c9e0 <__gmpn_submul_1@plt>
   4bfd0:	subs	x8, x19, x0
   4bfd4:	cset	w9, cc  // cc = lo, ul, last
   4bfd8:	subs	x21, x21, x9
   4bfdc:	stur	x8, [x20, #-8]
   4bfe0:	b.cc	4c030 <__gmpn_sbpi1_divappr_q@@Base+0x468>  // b.lo, b.ul, b.last
   4bfe4:	ldur	x16, [x29, #-8]
   4bfe8:	mov	x12, x28
   4bfec:	mov	x28, x26
   4bff0:	mov	x26, x23
   4bff4:	ldr	x23, [sp, #24]
   4bff8:	b	4bf10 <__gmpn_sbpi1_divappr_q@@Base+0x348>
   4bffc:	and	x8, x0, x23
   4c000:	cmp	x21, x8
   4c004:	b.cs	4c088 <__gmpn_sbpi1_divappr_q@@Base+0x4c0>  // b.hs, b.nlast
   4c008:	ldr	x0, [sp, #32]
   4c00c:	mov	x2, x22
   4c010:	mov	x3, x19
   4c014:	mov	x1, x0
   4c018:	bl	ca70 <__gmpn_add_n@plt>
   4c01c:	ldur	x16, [x29, #-8]
   4c020:	ldr	x21, [x20]
   4c024:	mov	x12, x19
   4c028:	mov	x25, #0xfffffffffffffffe    	// #-2
   4c02c:	b	4bf10 <__gmpn_sbpi1_divappr_q@@Base+0x348>
   4c030:	ldr	x0, [sp, #32]
   4c034:	sub	x3, x28, #0x1
   4c038:	mov	x2, x22
   4c03c:	mov	x1, x0
   4c040:	bl	ca70 <__gmpn_add_n@plt>
   4c044:	ldur	x16, [x29, #-8]
   4c048:	mov	x12, x28
   4c04c:	sub	x25, x25, #0x1
   4c050:	add	x8, x21, x16
   4c054:	add	x21, x8, x0
   4c058:	b	4bfec <__gmpn_sbpi1_divappr_q@@Base+0x424>
   4c05c:	ldur	x8, [x29, #-16]
   4c060:	cmp	x19, x8
   4c064:	b.cs	4c070 <__gmpn_sbpi1_divappr_q@@Base+0x4a8>  // b.hs, b.nlast
   4c068:	cmp	x21, x16
   4c06c:	b.ls	4bfb8 <__gmpn_sbpi1_divappr_q@@Base+0x3f0>  // b.plast
   4c070:	ldur	x9, [x29, #-16]
   4c074:	subs	x8, x19, x9
   4c078:	sbc	x21, x21, x16
   4c07c:	add	x25, x25, #0x1
   4c080:	mov	x19, x8
   4c084:	b	4bfb8 <__gmpn_sbpi1_divappr_q@@Base+0x3f0>
   4c088:	mov	x23, xzr
   4c08c:	mov	x25, #0xffffffffffffffff    	// #-1
   4c090:	b	4bf04 <__gmpn_sbpi1_divappr_q@@Base+0x33c>
   4c094:	ldr	x19, [sp, #16]
   4c098:	add	x26, x26, x27
   4c09c:	sub	x22, x26, #0x8
   4c0a0:	add	x28, x28, x27
   4c0a4:	cbz	w8, 4be34 <__gmpn_sbpi1_divappr_q@@Base+0x26c>
   4c0a8:	mov	w2, #0x2                   	// #2
   4c0ac:	mov	x3, #0xffffffffffffffff    	// #-1
   4c0b0:	mov	x0, x22
   4c0b4:	mov	x1, x15
   4c0b8:	mov	x24, x23
   4c0bc:	mov	x20, #0xffffffffffffffff    	// #-1
   4c0c0:	mov	x23, x15
   4c0c4:	bl	c9e0 <__gmpn_submul_1@plt>
   4c0c8:	cmp	x21, x0
   4c0cc:	b.eq	4c0dc <__gmpn_sbpi1_divappr_q@@Base+0x514>  // b.none
   4c0d0:	and	x8, x0, x24
   4c0d4:	cmp	x21, x8
   4c0d8:	b.cc	4c138 <__gmpn_sbpi1_divappr_q@@Base+0x570>  // b.lo, b.ul, b.last
   4c0dc:	ldr	x21, [x26]
   4c0e0:	stur	x20, [x28, #-8]
   4c0e4:	ldr	x8, [x22, #8]
   4c0e8:	cmp	x8, x21
   4c0ec:	b.ne	4c15c <__gmpn_sbpi1_divappr_q@@Base+0x594>  // b.any
   4c0f0:	mov	x0, x19
   4c0f4:	ldp	x20, x19, [sp, #144]
   4c0f8:	ldp	x22, x21, [sp, #128]
   4c0fc:	ldp	x24, x23, [sp, #112]
   4c100:	ldp	x26, x25, [sp, #96]
   4c104:	ldp	x28, x27, [sp, #80]
   4c108:	ldp	x29, x30, [sp, #64]
   4c10c:	add	sp, sp, #0xa0
   4c110:	ret
   4c114:	cmp	x8, x15
   4c118:	b.cs	4c124 <__gmpn_sbpi1_divappr_q@@Base+0x55c>  // b.hs, b.nlast
   4c11c:	cmp	x21, x16
   4c120:	b.ls	4be98 <__gmpn_sbpi1_divappr_q@@Base+0x2d0>  // b.plast
   4c124:	subs	x9, x8, x15
   4c128:	sbc	x21, x21, x16
   4c12c:	add	x20, x20, #0x1
   4c130:	mov	x8, x9
   4c134:	b	4be98 <__gmpn_sbpi1_divappr_q@@Base+0x2d0>
   4c138:	ldr	x8, [x26]
   4c13c:	mov	x20, #0xfffffffffffffffe    	// #-2
   4c140:	ldp	x9, x10, [x23]
   4c144:	ldr	x11, [x22]
   4c148:	adds	x12, x11, x9
   4c14c:	adc	x8, x8, x10
   4c150:	str	x8, [x26]
   4c154:	str	x12, [x22]
   4c158:	b	4c0dc <__gmpn_sbpi1_divappr_q@@Base+0x514>
   4c15c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4c160:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4c164:	add	x0, x0, #0x719
   4c168:	add	x2, x2, #0x705
   4c16c:	mov	w1, #0xc3                  	// #195
   4c170:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004c174 <__gmpn_dcpi1_div_q@@Base>:
   4c174:	stp	x29, x30, [sp, #-96]!
   4c178:	stp	x28, x27, [sp, #16]
   4c17c:	stp	x26, x25, [sp, #32]
   4c180:	stp	x24, x23, [sp, #48]
   4c184:	stp	x22, x21, [sp, #64]
   4c188:	stp	x20, x19, [sp, #80]
   4c18c:	mov	x29, sp
   4c190:	sub	sp, sp, #0x10
   4c194:	add	x28, x2, #0x1
   4c198:	mov	x25, x1
   4c19c:	lsl	x1, x28, #3
   4c1a0:	mov	w8, #0x7f00                	// #32512
   4c1a4:	mov	x20, x5
   4c1a8:	mov	x21, x4
   4c1ac:	mov	x26, x3
   4c1b0:	mov	x22, x2
   4c1b4:	mov	x19, x0
   4c1b8:	cmp	x1, x8
   4c1bc:	stur	xzr, [x29, #-8]
   4c1c0:	b.hi	4c3dc <__gmpn_dcpi1_div_q@@Base+0x268>  // b.pmore
   4c1c4:	add	x9, x1, #0xf
   4c1c8:	mov	x8, sp
   4c1cc:	and	x9, x9, #0xfffffffffffffff0
   4c1d0:	sub	x27, x8, x9
   4c1d4:	mov	sp, x27
   4c1d8:	add	x0, x27, #0x8
   4c1dc:	mov	x1, x25
   4c1e0:	mov	x2, x22
   4c1e4:	bl	ca50 <__gmpn_copyi@plt>
   4c1e8:	sub	x23, x22, x21
   4c1ec:	lsl	x8, x23, #3
   4c1f0:	add	x1, x8, #0x8
   4c1f4:	mov	w8, #0x7f00                	// #32512
   4c1f8:	cmp	x1, x8
   4c1fc:	str	xzr, [x27]
   4c200:	b.hi	4c3ec <__gmpn_dcpi1_div_q@@Base+0x278>  // b.pmore
   4c204:	add	x9, x1, #0xf
   4c208:	mov	x8, sp
   4c20c:	and	x9, x9, #0xfffffffffffffff0
   4c210:	sub	x24, x8, x9
   4c214:	mov	sp, x24
   4c218:	mov	x0, x24
   4c21c:	mov	x1, x27
   4c220:	mov	x2, x28
   4c224:	mov	x3, x26
   4c228:	mov	x4, x21
   4c22c:	mov	x5, x20
   4c230:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   4c234:	ldr	x8, [x24]
   4c238:	mov	x20, x0
   4c23c:	cbz	x8, 4c25c <__gmpn_dcpi1_div_q@@Base+0xe8>
   4c240:	add	x1, x24, #0x8
   4c244:	mov	x0, x19
   4c248:	mov	x2, x23
   4c24c:	bl	ca50 <__gmpn_copyi@plt>
   4c250:	ldur	x0, [x29, #-8]
   4c254:	cbz	x0, 4c3b8 <__gmpn_dcpi1_div_q@@Base+0x244>
   4c258:	b	4c33c <__gmpn_dcpi1_div_q@@Base+0x1c8>
   4c25c:	cmp	x23, x21
   4c260:	add	x28, x24, #0x8
   4c264:	mov	x0, x27
   4c268:	b.le	4c288 <__gmpn_dcpi1_div_q@@Base+0x114>
   4c26c:	mov	x1, x28
   4c270:	mov	x2, x23
   4c274:	mov	x3, x26
   4c278:	mov	x4, x21
   4c27c:	bl	ccd0 <__gmpn_mul@plt>
   4c280:	cbnz	x20, 4c2a0 <__gmpn_dcpi1_div_q@@Base+0x12c>
   4c284:	b	4c2b8 <__gmpn_dcpi1_div_q@@Base+0x144>
   4c288:	mov	x1, x26
   4c28c:	mov	x2, x21
   4c290:	mov	x3, x28
   4c294:	mov	x4, x23
   4c298:	bl	ccd0 <__gmpn_mul@plt>
   4c29c:	cbz	x20, 4c2b8 <__gmpn_dcpi1_div_q@@Base+0x144>
   4c2a0:	add	x0, x27, x23, lsl #3
   4c2a4:	mov	x1, x0
   4c2a8:	mov	x2, x26
   4c2ac:	mov	x3, x21
   4c2b0:	bl	ca70 <__gmpn_add_n@plt>
   4c2b4:	cbnz	x0, 4c2e8 <__gmpn_dcpi1_div_q@@Base+0x174>
   4c2b8:	sub	x8, x25, #0x8
   4c2bc:	sub	x9, x27, #0x8
   4c2c0:	mov	x10, x22
   4c2c4:	subs	x11, x10, #0x1
   4c2c8:	b.lt	4c324 <__gmpn_dcpi1_div_q@@Base+0x1b0>  // b.tstop
   4c2cc:	lsl	x10, x10, #3
   4c2d0:	ldr	x12, [x9, x10]
   4c2d4:	ldr	x10, [x8, x10]
   4c2d8:	cmp	x12, x10
   4c2dc:	mov	x10, x11
   4c2e0:	b.eq	4c2c4 <__gmpn_dcpi1_div_q@@Base+0x150>  // b.none
   4c2e4:	b.ls	4c324 <__gmpn_dcpi1_div_q@@Base+0x1b0>  // b.plast
   4c2e8:	ldr	x8, [x28]
   4c2ec:	sub	x9, x8, #0x1
   4c2f0:	str	x9, [x19]
   4c2f4:	cbz	x8, 4c344 <__gmpn_dcpi1_div_q@@Base+0x1d0>
   4c2f8:	cmp	x23, #0x2
   4c2fc:	mov	x8, xzr
   4c300:	b.lt	4c3ac <__gmpn_dcpi1_div_q@@Base+0x238>  // b.tstop
   4c304:	cmp	x28, x19
   4c308:	b.eq	4c3ac <__gmpn_dcpi1_div_q@@Base+0x238>  // b.none
   4c30c:	mvn	x8, x21
   4c310:	add	x8, x8, x22
   4c314:	add	x0, x19, #0x8
   4c318:	add	x1, x24, #0x10
   4c31c:	lsl	x2, x8, #3
   4c320:	b	4c3a4 <__gmpn_dcpi1_div_q@@Base+0x230>
   4c324:	mov	x0, x19
   4c328:	mov	x1, x28
   4c32c:	mov	x2, x23
   4c330:	bl	ca50 <__gmpn_copyi@plt>
   4c334:	ldur	x0, [x29, #-8]
   4c338:	cbz	x0, 4c3b8 <__gmpn_dcpi1_div_q@@Base+0x244>
   4c33c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4c340:	b	4c3b8 <__gmpn_dcpi1_div_q@@Base+0x244>
   4c344:	mov	x9, xzr
   4c348:	add	x11, x24, #0x10
   4c34c:	mov	w8, #0x1                   	// #1
   4c350:	mov	w10, #0x1                   	// #1
   4c354:	cmp	x10, x23
   4c358:	b.ge	4c3ac <__gmpn_dcpi1_div_q@@Base+0x238>  // b.tcont
   4c35c:	ldr	x12, [x11, x9]
   4c360:	add	x13, x19, x9
   4c364:	add	x10, x10, #0x1
   4c368:	add	x9, x9, #0x8
   4c36c:	sub	x14, x12, #0x1
   4c370:	str	x14, [x13, #8]
   4c374:	cbz	x12, 4c354 <__gmpn_dcpi1_div_q@@Base+0x1e0>
   4c378:	cmp	x28, x19
   4c37c:	mov	x8, xzr
   4c380:	b.eq	4c3ac <__gmpn_dcpi1_div_q@@Base+0x238>  // b.none
   4c384:	cmp	x10, x23
   4c388:	b.ge	4c3ac <__gmpn_dcpi1_div_q@@Base+0x238>  // b.tcont
   4c38c:	add	x8, x19, x9
   4c390:	add	x9, x24, x9
   4c394:	sub	x10, x23, x10
   4c398:	add	x0, x8, #0x8
   4c39c:	add	x1, x9, #0x10
   4c3a0:	lsl	x2, x10, #3
   4c3a4:	bl	bed0 <memcpy@plt>
   4c3a8:	mov	x8, xzr
   4c3ac:	sub	x20, x20, x8
   4c3b0:	ldur	x0, [x29, #-8]
   4c3b4:	cbnz	x0, 4c33c <__gmpn_dcpi1_div_q@@Base+0x1c8>
   4c3b8:	mov	x0, x20
   4c3bc:	mov	sp, x29
   4c3c0:	ldp	x20, x19, [sp, #80]
   4c3c4:	ldp	x22, x21, [sp, #64]
   4c3c8:	ldp	x24, x23, [sp, #48]
   4c3cc:	ldp	x26, x25, [sp, #32]
   4c3d0:	ldp	x28, x27, [sp, #16]
   4c3d4:	ldp	x29, x30, [sp], #96
   4c3d8:	ret
   4c3dc:	sub	x0, x29, #0x8
   4c3e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4c3e4:	mov	x27, x0
   4c3e8:	b	4c1d8 <__gmpn_dcpi1_div_q@@Base+0x64>
   4c3ec:	sub	x0, x29, #0x8
   4c3f0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4c3f4:	mov	x24, x0
   4c3f8:	b	4c218 <__gmpn_dcpi1_div_q@@Base+0xa4>

000000000004c3fc <__gmpn_dcpi1_div_qr_n@@Base>:
   4c3fc:	sub	sp, sp, #0x80
   4c400:	stp	x24, x23, [sp, #80]
   4c404:	asr	x23, x3, #1
   4c408:	stp	x26, x25, [sp, #64]
   4c40c:	sub	x25, x3, x23
   4c410:	and	x8, x3, #0xfffffffffffffffe
   4c414:	stp	x28, x27, [sp, #48]
   4c418:	stp	x22, x21, [sp, #96]
   4c41c:	stp	x20, x19, [sp, #112]
   4c420:	mov	x26, x5
   4c424:	mov	x19, x3
   4c428:	mov	x20, x2
   4c42c:	mov	x21, x1
   4c430:	mov	x22, x0
   4c434:	add	x27, x0, x23, lsl #3
   4c438:	cmp	x25, #0x29
   4c43c:	add	x1, x1, x8, lsl #3
   4c440:	stp	x29, x30, [sp, #32]
   4c444:	add	x29, sp, #0x20
   4c448:	stp	x8, x4, [sp, #8]
   4c44c:	b.le	4c468 <__gmpn_dcpi1_div_qr_n@@Base+0x6c>
   4c450:	add	x2, x20, x23, lsl #3
   4c454:	mov	x0, x27
   4c458:	mov	x3, x25
   4c45c:	mov	x5, x26
   4c460:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4c464:	b	4c480 <__gmpn_dcpi1_div_qr_n@@Base+0x84>
   4c468:	ldr	x5, [x4]
   4c46c:	lsl	x2, x25, #1
   4c470:	add	x3, x20, x23, lsl #3
   4c474:	mov	x0, x27
   4c478:	mov	x4, x25
   4c47c:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4c480:	mov	x24, x0
   4c484:	mov	x0, x26
   4c488:	mov	x1, x27
   4c48c:	mov	x2, x25
   4c490:	mov	x3, x20
   4c494:	mov	x4, x23
   4c498:	bl	ccd0 <__gmpn_mul@plt>
   4c49c:	add	x28, x21, x23, lsl #3
   4c4a0:	mov	x0, x28
   4c4a4:	mov	x1, x28
   4c4a8:	mov	x2, x26
   4c4ac:	mov	x3, x19
   4c4b0:	stur	x26, [x29, #-8]
   4c4b4:	bl	c2d0 <__gmpn_sub_n@plt>
   4c4b8:	mov	x26, x0
   4c4bc:	cbz	x24, 4c4d8 <__gmpn_dcpi1_div_qr_n@@Base+0xdc>
   4c4c0:	add	x0, x21, x19, lsl #3
   4c4c4:	mov	x1, x0
   4c4c8:	mov	x2, x20
   4c4cc:	mov	x3, x23
   4c4d0:	bl	c2d0 <__gmpn_sub_n@plt>
   4c4d4:	add	x26, x0, x26
   4c4d8:	cbnz	x26, 4c534 <__gmpn_dcpi1_div_qr_n@@Base+0x138>
   4c4dc:	lsl	x8, x25, #3
   4c4e0:	cmp	x19, #0x53
   4c4e4:	add	x1, x21, x8
   4c4e8:	add	x3, x20, x8
   4c4ec:	b.le	4c574 <__gmpn_dcpi1_div_qr_n@@Base+0x178>
   4c4f0:	ldur	x26, [x29, #-8]
   4c4f4:	ldr	x4, [sp, #16]
   4c4f8:	mov	x0, x22
   4c4fc:	mov	x2, x3
   4c500:	mov	x3, x23
   4c504:	mov	x5, x26
   4c508:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4c50c:	b	4c58c <__gmpn_dcpi1_div_qr_n@@Base+0x190>
   4c510:	mov	x8, xzr
   4c514:	mov	x0, x28
   4c518:	mov	x1, x28
   4c51c:	mov	x2, x20
   4c520:	mov	x3, x19
   4c524:	sub	x24, x24, x8
   4c528:	bl	ca70 <__gmpn_add_n@plt>
   4c52c:	subs	x26, x26, x0
   4c530:	b.eq	4c4dc <__gmpn_dcpi1_div_qr_n@@Base+0xe0>  // b.none
   4c534:	ldr	x8, [x27]
   4c538:	sub	x9, x8, #0x1
   4c53c:	str	x9, [x27]
   4c540:	cbnz	x8, 4c510 <__gmpn_dcpi1_div_qr_n@@Base+0x114>
   4c544:	mov	w8, #0x1                   	// #1
   4c548:	cmp	x8, x25
   4c54c:	b.ge	4c56c <__gmpn_dcpi1_div_qr_n@@Base+0x170>  // b.tcont
   4c550:	lsl	x9, x8, #3
   4c554:	ldr	x10, [x27, x9]
   4c558:	add	x8, x8, #0x1
   4c55c:	sub	x11, x10, #0x1
   4c560:	str	x11, [x27, x9]
   4c564:	cbz	x10, 4c548 <__gmpn_dcpi1_div_qr_n@@Base+0x14c>
   4c568:	b	4c510 <__gmpn_dcpi1_div_qr_n@@Base+0x114>
   4c56c:	mov	w8, #0x1                   	// #1
   4c570:	b	4c514 <__gmpn_dcpi1_div_qr_n@@Base+0x118>
   4c574:	ldp	x2, x8, [sp, #8]
   4c578:	mov	x0, x22
   4c57c:	mov	x4, x23
   4c580:	ldr	x5, [x8]
   4c584:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4c588:	ldur	x26, [x29, #-8]
   4c58c:	mov	x27, x0
   4c590:	mov	x0, x26
   4c594:	mov	x1, x20
   4c598:	mov	x2, x25
   4c59c:	mov	x3, x22
   4c5a0:	mov	x4, x23
   4c5a4:	bl	ccd0 <__gmpn_mul@plt>
   4c5a8:	mov	x0, x21
   4c5ac:	mov	x1, x21
   4c5b0:	mov	x2, x26
   4c5b4:	mov	x3, x19
   4c5b8:	bl	c2d0 <__gmpn_sub_n@plt>
   4c5bc:	mov	x26, x0
   4c5c0:	cbz	x27, 4c5dc <__gmpn_dcpi1_div_qr_n@@Base+0x1e0>
   4c5c4:	mov	x0, x28
   4c5c8:	mov	x1, x28
   4c5cc:	mov	x2, x20
   4c5d0:	mov	x3, x25
   4c5d4:	bl	c2d0 <__gmpn_sub_n@plt>
   4c5d8:	add	x26, x0, x26
   4c5dc:	cbnz	x26, 4c620 <__gmpn_dcpi1_div_qr_n@@Base+0x224>
   4c5e0:	mov	x0, x24
   4c5e4:	ldp	x20, x19, [sp, #112]
   4c5e8:	ldp	x22, x21, [sp, #96]
   4c5ec:	ldp	x24, x23, [sp, #80]
   4c5f0:	ldp	x26, x25, [sp, #64]
   4c5f4:	ldp	x28, x27, [sp, #48]
   4c5f8:	ldp	x29, x30, [sp, #32]
   4c5fc:	add	sp, sp, #0x80
   4c600:	ret
   4c604:	mov	x0, x21
   4c608:	mov	x1, x21
   4c60c:	mov	x2, x20
   4c610:	mov	x3, x19
   4c614:	bl	ca70 <__gmpn_add_n@plt>
   4c618:	subs	x26, x26, x0
   4c61c:	b.eq	4c5e0 <__gmpn_dcpi1_div_qr_n@@Base+0x1e4>  // b.none
   4c620:	ldr	x8, [x22]
   4c624:	sub	x9, x8, #0x1
   4c628:	str	x9, [x22]
   4c62c:	cbnz	x8, 4c604 <__gmpn_dcpi1_div_qr_n@@Base+0x208>
   4c630:	mov	w8, #0x1                   	// #1
   4c634:	cmp	x8, x23
   4c638:	b.ge	4c604 <__gmpn_dcpi1_div_qr_n@@Base+0x208>  // b.tcont
   4c63c:	lsl	x9, x8, #3
   4c640:	ldr	x10, [x22, x9]
   4c644:	add	x8, x8, #0x1
   4c648:	sub	x11, x10, #0x1
   4c64c:	str	x11, [x22, x9]
   4c650:	cbz	x10, 4c634 <__gmpn_dcpi1_div_qr_n@@Base+0x238>
   4c654:	b	4c604 <__gmpn_dcpi1_div_qr_n@@Base+0x208>

000000000004c658 <__gmpn_dcpi1_div_qr@@Base>:
   4c658:	stp	x29, x30, [sp, #-96]!
   4c65c:	stp	x28, x27, [sp, #16]
   4c660:	stp	x26, x25, [sp, #32]
   4c664:	stp	x24, x23, [sp, #48]
   4c668:	stp	x22, x21, [sp, #64]
   4c66c:	stp	x20, x19, [sp, #80]
   4c670:	mov	x29, sp
   4c674:	sub	sp, sp, #0x50
   4c678:	lsl	x25, x4, #3
   4c67c:	mov	w8, #0x7f00                	// #32512
   4c680:	mov	x19, x4
   4c684:	mov	x20, x3
   4c688:	mov	x26, x2
   4c68c:	mov	x23, x1
   4c690:	mov	x21, x0
   4c694:	cmp	x25, x8
   4c698:	stur	xzr, [x29, #-8]
   4c69c:	stur	x5, [x29, #-24]
   4c6a0:	b.hi	4cb9c <__gmpn_dcpi1_div_qr@@Base+0x544>  // b.pmore
   4c6a4:	add	x9, x25, #0xf
   4c6a8:	mov	x8, sp
   4c6ac:	and	x9, x9, #0xfffffffffffffff0
   4c6b0:	sub	x27, x8, x9
   4c6b4:	mov	sp, x27
   4c6b8:	sub	x22, x26, x19
   4c6bc:	add	x9, x23, x26, lsl #3
   4c6c0:	cmp	x22, x19
   4c6c4:	add	x28, x20, x19, lsl #3
   4c6c8:	b.le	4c788 <__gmpn_dcpi1_div_qr@@Base+0x130>
   4c6cc:	add	x8, x25, x21
   4c6d0:	stp	x23, x27, [x29, #-40]
   4c6d4:	add	x10, x21, x22, lsl #3
   4c6d8:	add	x11, x8, #0x8
   4c6dc:	mov	x8, x25
   4c6e0:	mov	x27, x22
   4c6e4:	sub	x27, x27, x19
   4c6e8:	mov	x24, x8
   4c6ec:	mov	x23, x11
   4c6f0:	add	x8, x8, x25
   4c6f4:	cmp	x27, x19
   4c6f8:	add	x11, x11, x25
   4c6fc:	b.gt	4c6e4 <__gmpn_dcpi1_div_qr@@Base+0x8c>
   4c700:	lsl	x11, x27, #3
   4c704:	cmp	x27, #0x2
   4c708:	sub	x10, x10, x11
   4c70c:	sub	x26, x9, x11
   4c710:	stur	x10, [x29, #-16]
   4c714:	b.eq	4c814 <__gmpn_dcpi1_div_qr@@Base+0x1bc>  // b.none
   4c718:	cmp	x27, #0x1
   4c71c:	b.ne	4c868 <__gmpn_dcpi1_div_qr@@Base+0x210>  // b.any
   4c720:	ldur	x9, [x29, #-40]
   4c724:	sub	x14, x26, x19, lsl #3
   4c728:	neg	x13, x19
   4c72c:	add	x0, x14, #0x8
   4c730:	mov	x10, x19
   4c734:	stp	x14, x13, [x29, #-56]
   4c738:	subs	x11, x10, #0x1
   4c73c:	b.lt	4c760 <__gmpn_dcpi1_div_qr@@Base+0x108>  // b.tstop
   4c740:	add	x10, x20, x10, lsl #3
   4c744:	ldr	x12, [x9, x8]
   4c748:	ldur	x10, [x10, #-8]
   4c74c:	sub	x9, x9, #0x8
   4c750:	cmp	x12, x10
   4c754:	mov	x10, x11
   4c758:	b.eq	4c738 <__gmpn_dcpi1_div_qr@@Base+0xe0>  // b.none
   4c75c:	b.ls	4ca54 <__gmpn_dcpi1_div_qr@@Base+0x3fc>  // b.plast
   4c760:	mov	x1, x0
   4c764:	mov	x2, x20
   4c768:	mov	x3, x19
   4c76c:	bl	c2d0 <__gmpn_sub_n@plt>
   4c770:	mov	w25, #0x1                   	// #1
   4c774:	ldp	x10, x9, [x26, #-8]
   4c778:	ldp	x8, x23, [x28, #-16]
   4c77c:	cmp	x9, x23
   4c780:	b.eq	4ca68 <__gmpn_dcpi1_div_qr@@Base+0x410>  // b.none
   4c784:	b	4ca70 <__gmpn_dcpi1_div_qr@@Base+0x418>
   4c788:	lsl	x10, x22, #3
   4c78c:	sub	x23, x9, x10
   4c790:	neg	x8, x22
   4c794:	cmp	x22, #0x29
   4c798:	sub	x1, x23, x10
   4c79c:	b.le	4c7c8 <__gmpn_dcpi1_div_qr@@Base+0x170>
   4c7a0:	ldur	x4, [x29, #-24]
   4c7a4:	add	x2, x28, x8, lsl #3
   4c7a8:	mov	x0, x21
   4c7ac:	mov	x3, x22
   4c7b0:	mov	x5, x27
   4c7b4:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4c7b8:	mov	x25, x0
   4c7bc:	cmp	x22, x19
   4c7c0:	b.ne	4c7f0 <__gmpn_dcpi1_div_qr@@Base+0x198>  // b.any
   4c7c4:	b	4cb70 <__gmpn_dcpi1_div_qr@@Base+0x518>
   4c7c8:	ldur	x9, [x29, #-24]
   4c7cc:	lsl	x2, x22, #1
   4c7d0:	add	x3, x28, x8, lsl #3
   4c7d4:	mov	x0, x21
   4c7d8:	ldr	x5, [x9]
   4c7dc:	mov	x4, x22
   4c7e0:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4c7e4:	mov	x25, x0
   4c7e8:	cmp	x22, x19
   4c7ec:	b.eq	4cb70 <__gmpn_dcpi1_div_qr@@Base+0x518>  // b.none
   4c7f0:	sub	x26, x19, x22
   4c7f4:	mov	x0, x27
   4c7f8:	cmp	x22, x26
   4c7fc:	b.le	4c8a0 <__gmpn_dcpi1_div_qr@@Base+0x248>
   4c800:	mov	x1, x21
   4c804:	mov	x2, x22
   4c808:	mov	x3, x20
   4c80c:	mov	x4, x26
   4c810:	b	4c8b0 <__gmpn_dcpi1_div_qr@@Base+0x258>
   4c814:	ldur	x0, [x29, #-16]
   4c818:	sub	x2, x26, #0x10
   4c81c:	sub	x4, x28, #0x10
   4c820:	mov	w3, #0x4                   	// #4
   4c824:	mov	x1, xzr
   4c828:	bl	c200 <__gmpn_divrem_2@plt>
   4c82c:	ldur	x28, [x29, #-24]
   4c830:	mov	x25, x0
   4c834:	cmp	x27, x19
   4c838:	b.eq	4ca4c <__gmpn_dcpi1_div_qr@@Base+0x3f4>  // b.none
   4c83c:	sub	x8, x19, x27
   4c840:	cmp	x27, x8
   4c844:	stur	x8, [x29, #-56]
   4c848:	b.le	4c958 <__gmpn_dcpi1_div_qr@@Base+0x300>
   4c84c:	ldur	x28, [x29, #-32]
   4c850:	ldur	x1, [x29, #-16]
   4c854:	mov	x2, x27
   4c858:	mov	x3, x20
   4c85c:	mov	x0, x28
   4c860:	mov	x4, x8
   4c864:	b	4c970 <__gmpn_dcpi1_div_qr@@Base+0x318>
   4c868:	neg	x8, x27
   4c86c:	cmp	x27, #0x29
   4c870:	add	x1, x26, x8, lsl #3
   4c874:	b.le	4ca24 <__gmpn_dcpi1_div_qr@@Base+0x3cc>
   4c878:	add	x2, x28, x8, lsl #3
   4c87c:	ldp	x28, x0, [x29, #-24]
   4c880:	ldur	x5, [x29, #-32]
   4c884:	mov	x3, x27
   4c888:	mov	x4, x28
   4c88c:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4c890:	mov	x25, x0
   4c894:	cmp	x27, x19
   4c898:	b.ne	4c83c <__gmpn_dcpi1_div_qr@@Base+0x1e4>  // b.any
   4c89c:	b	4ca4c <__gmpn_dcpi1_div_qr@@Base+0x3f4>
   4c8a0:	mov	x1, x20
   4c8a4:	mov	x2, x26
   4c8a8:	mov	x3, x21
   4c8ac:	mov	x4, x22
   4c8b0:	bl	ccd0 <__gmpn_mul@plt>
   4c8b4:	sub	x24, x23, x19, lsl #3
   4c8b8:	mov	x0, x24
   4c8bc:	mov	x1, x24
   4c8c0:	mov	x2, x27
   4c8c4:	mov	x3, x19
   4c8c8:	bl	c2d0 <__gmpn_sub_n@plt>
   4c8cc:	mov	x23, x0
   4c8d0:	cbz	x25, 4c8ec <__gmpn_dcpi1_div_qr@@Base+0x294>
   4c8d4:	add	x0, x24, x22, lsl #3
   4c8d8:	mov	x1, x0
   4c8dc:	mov	x2, x20
   4c8e0:	mov	x3, x26
   4c8e4:	bl	c2d0 <__gmpn_sub_n@plt>
   4c8e8:	add	x23, x0, x23
   4c8ec:	cbnz	x23, 4c918 <__gmpn_dcpi1_div_qr@@Base+0x2c0>
   4c8f0:	b	4cb70 <__gmpn_dcpi1_div_qr@@Base+0x518>
   4c8f4:	mov	x8, xzr
   4c8f8:	mov	x0, x24
   4c8fc:	mov	x1, x24
   4c900:	mov	x2, x20
   4c904:	mov	x3, x19
   4c908:	sub	x25, x25, x8
   4c90c:	bl	ca70 <__gmpn_add_n@plt>
   4c910:	subs	x23, x23, x0
   4c914:	b.eq	4cb70 <__gmpn_dcpi1_div_qr@@Base+0x518>  // b.none
   4c918:	ldr	x8, [x21]
   4c91c:	sub	x9, x8, #0x1
   4c920:	str	x9, [x21]
   4c924:	cbnz	x8, 4c8f4 <__gmpn_dcpi1_div_qr@@Base+0x29c>
   4c928:	mov	w8, #0x1                   	// #1
   4c92c:	cmp	x8, x22
   4c930:	b.ge	4c950 <__gmpn_dcpi1_div_qr@@Base+0x2f8>  // b.tcont
   4c934:	lsl	x9, x8, #3
   4c938:	ldr	x10, [x21, x9]
   4c93c:	add	x8, x8, #0x1
   4c940:	sub	x11, x10, #0x1
   4c944:	str	x11, [x21, x9]
   4c948:	cbz	x10, 4c92c <__gmpn_dcpi1_div_qr@@Base+0x2d4>
   4c94c:	b	4c8f4 <__gmpn_dcpi1_div_qr@@Base+0x29c>
   4c950:	mov	w8, #0x1                   	// #1
   4c954:	b	4c8f8 <__gmpn_dcpi1_div_qr@@Base+0x2a0>
   4c958:	ldur	x28, [x29, #-32]
   4c95c:	ldur	x3, [x29, #-16]
   4c960:	mov	x1, x20
   4c964:	mov	x2, x8
   4c968:	mov	x0, x28
   4c96c:	mov	x4, x27
   4c970:	bl	ccd0 <__gmpn_mul@plt>
   4c974:	sub	x26, x26, x19, lsl #3
   4c978:	neg	x8, x19
   4c97c:	mov	x0, x26
   4c980:	mov	x1, x26
   4c984:	mov	x2, x28
   4c988:	mov	x3, x19
   4c98c:	stur	x8, [x29, #-48]
   4c990:	bl	c2d0 <__gmpn_sub_n@plt>
   4c994:	mov	x28, x0
   4c998:	cbz	x25, 4c9b4 <__gmpn_dcpi1_div_qr@@Base+0x35c>
   4c99c:	ldur	x3, [x29, #-56]
   4c9a0:	add	x0, x26, x27, lsl #3
   4c9a4:	mov	x1, x0
   4c9a8:	mov	x2, x20
   4c9ac:	bl	c2d0 <__gmpn_sub_n@plt>
   4c9b0:	add	x28, x0, x28
   4c9b4:	cbnz	x28, 4c9e0 <__gmpn_dcpi1_div_qr@@Base+0x388>
   4c9b8:	b	4cb24 <__gmpn_dcpi1_div_qr@@Base+0x4cc>
   4c9bc:	mov	x8, xzr
   4c9c0:	mov	x0, x26
   4c9c4:	mov	x1, x26
   4c9c8:	mov	x2, x20
   4c9cc:	mov	x3, x19
   4c9d0:	sub	x25, x25, x8
   4c9d4:	bl	ca70 <__gmpn_add_n@plt>
   4c9d8:	subs	x28, x28, x0
   4c9dc:	b.eq	4cb24 <__gmpn_dcpi1_div_qr@@Base+0x4cc>  // b.none
   4c9e0:	ldur	x10, [x29, #-16]
   4c9e4:	ldr	x8, [x10]
   4c9e8:	sub	x9, x8, #0x1
   4c9ec:	str	x9, [x10]
   4c9f0:	cbnz	x8, 4c9bc <__gmpn_dcpi1_div_qr@@Base+0x364>
   4c9f4:	mov	x8, x23
   4c9f8:	mov	w9, #0x1                   	// #1
   4c9fc:	cmp	x9, x27
   4ca00:	b.ge	4ca1c <__gmpn_dcpi1_div_qr@@Base+0x3c4>  // b.tcont
   4ca04:	ldr	x10, [x8]
   4ca08:	add	x9, x9, #0x1
   4ca0c:	sub	x11, x10, #0x1
   4ca10:	str	x11, [x8], #8
   4ca14:	cbz	x10, 4c9fc <__gmpn_dcpi1_div_qr@@Base+0x3a4>
   4ca18:	b	4c9bc <__gmpn_dcpi1_div_qr@@Base+0x364>
   4ca1c:	mov	w8, #0x1                   	// #1
   4ca20:	b	4c9c0 <__gmpn_dcpi1_div_qr@@Base+0x368>
   4ca24:	ldp	x9, x0, [x29, #-24]
   4ca28:	lsl	x2, x27, #1
   4ca2c:	add	x3, x28, x8, lsl #3
   4ca30:	mov	x4, x27
   4ca34:	ldr	x5, [x9]
   4ca38:	mov	x28, x9
   4ca3c:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4ca40:	mov	x25, x0
   4ca44:	cmp	x27, x19
   4ca48:	b.ne	4c83c <__gmpn_dcpi1_div_qr@@Base+0x1e4>  // b.any
   4ca4c:	neg	x9, x19
   4ca50:	b	4cb2c <__gmpn_dcpi1_div_qr@@Base+0x4d4>
   4ca54:	mov	x25, xzr
   4ca58:	ldp	x10, x9, [x26, #-8]
   4ca5c:	ldp	x8, x23, [x28, #-16]
   4ca60:	cmp	x9, x23
   4ca64:	b.ne	4ca70 <__gmpn_dcpi1_div_qr@@Base+0x418>  // b.any
   4ca68:	cmp	x10, x8
   4ca6c:	b.eq	4cbdc <__gmpn_dcpi1_div_qr@@Base+0x584>  // b.none
   4ca70:	ldur	x11, [x29, #-24]
   4ca74:	ldur	x12, [x26, #-16]
   4ca78:	ldr	x11, [x11]
   4ca7c:	mul	x13, x11, x9
   4ca80:	umulh	x11, x9, x11
   4ca84:	adds	x14, x13, x10
   4ca88:	adc	x9, x11, x9
   4ca8c:	msub	x10, x9, x23, x10
   4ca90:	subs	x15, x12, x8
   4ca94:	sbc	x10, x10, x23
   4ca98:	mul	x11, x9, x8
   4ca9c:	umulh	x13, x8, x9
   4caa0:	subs	x12, x15, x11
   4caa4:	sbc	x10, x10, x13
   4caa8:	cmp	x10, x14
   4caac:	cset	w11, cs  // cs = hs, nlast
   4cab0:	csetm	x13, cs  // cs = hs, nlast
   4cab4:	sub	x9, x9, x11
   4cab8:	and	x11, x8, x13
   4cabc:	and	x13, x23, x13
   4cac0:	adds	x14, x12, x11
   4cac4:	adc	x28, x10, x13
   4cac8:	cmp	x28, x23
   4cacc:	add	x3, x9, #0x1
   4cad0:	b.cs	4cbb8 <__gmpn_dcpi1_div_qr@@Base+0x560>  // b.hs, b.nlast
   4cad4:	cmp	x19, #0x3
   4cad8:	b.lt	4cb14 <__gmpn_dcpi1_div_qr@@Base+0x4bc>  // b.tstop
   4cadc:	ldur	x0, [x29, #-56]
   4cae0:	sub	x2, x19, #0x2
   4cae4:	mov	x1, x20
   4cae8:	stp	x28, x3, [x29, #-72]
   4caec:	mov	x28, x14
   4caf0:	bl	c9e0 <__gmpn_submul_1@plt>
   4caf4:	subs	x8, x28, x0
   4caf8:	ldur	x28, [x29, #-72]
   4cafc:	cset	w9, cc  // cc = lo, ul, last
   4cb00:	stur	x8, [x26, #-16]
   4cb04:	subs	x28, x28, x9
   4cb08:	b.cc	4cc00 <__gmpn_dcpi1_div_qr@@Base+0x5a8>  // b.lo, b.ul, b.last
   4cb0c:	ldur	x3, [x29, #-64]
   4cb10:	b	4cb18 <__gmpn_dcpi1_div_qr@@Base+0x4c0>
   4cb14:	stur	x14, [x26, #-16]
   4cb18:	ldur	x8, [x29, #-16]
   4cb1c:	stur	x28, [x26, #-8]
   4cb20:	str	x3, [x8]
   4cb24:	ldur	x28, [x29, #-24]
   4cb28:	ldur	x9, [x29, #-48]
   4cb2c:	sub	x22, x22, x27
   4cb30:	add	x8, x19, x9, lsl #1
   4cb34:	lsl	x23, x9, #3
   4cb38:	ldp	x9, x27, [x29, #-40]
   4cb3c:	add	x21, x21, x23
   4cb40:	add	x26, x9, x8, lsl #3
   4cb44:	add	x0, x21, x24
   4cb48:	add	x1, x26, x24
   4cb4c:	mov	x2, x20
   4cb50:	mov	x3, x19
   4cb54:	mov	x4, x28
   4cb58:	mov	x5, x27
   4cb5c:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4cb60:	sub	x22, x22, x19
   4cb64:	cmp	x22, #0x0
   4cb68:	add	x24, x24, x23
   4cb6c:	b.gt	4cb44 <__gmpn_dcpi1_div_qr@@Base+0x4ec>
   4cb70:	ldur	x0, [x29, #-8]
   4cb74:	cbnz	x0, 4cbb0 <__gmpn_dcpi1_div_qr@@Base+0x558>
   4cb78:	mov	x0, x25
   4cb7c:	mov	sp, x29
   4cb80:	ldp	x20, x19, [sp, #80]
   4cb84:	ldp	x22, x21, [sp, #64]
   4cb88:	ldp	x24, x23, [sp, #48]
   4cb8c:	ldp	x26, x25, [sp, #32]
   4cb90:	ldp	x28, x27, [sp, #16]
   4cb94:	ldp	x29, x30, [sp], #96
   4cb98:	ret
   4cb9c:	sub	x0, x29, #0x8
   4cba0:	mov	x1, x25
   4cba4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4cba8:	mov	x27, x0
   4cbac:	b	4c6b8 <__gmpn_dcpi1_div_qr@@Base+0x60>
   4cbb0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4cbb4:	b	4cb78 <__gmpn_dcpi1_div_qr@@Base+0x520>
   4cbb8:	cmp	x14, x8
   4cbbc:	b.cs	4cbc8 <__gmpn_dcpi1_div_qr@@Base+0x570>  // b.hs, b.nlast
   4cbc0:	cmp	x28, x23
   4cbc4:	b.ls	4cad4 <__gmpn_dcpi1_div_qr@@Base+0x47c>  // b.plast
   4cbc8:	subs	x9, x14, x8
   4cbcc:	sbc	x28, x28, x23
   4cbd0:	add	x3, x3, #0x1
   4cbd4:	mov	x14, x9
   4cbd8:	b	4cad4 <__gmpn_dcpi1_div_qr@@Base+0x47c>
   4cbdc:	ldur	x0, [x29, #-56]
   4cbe0:	mov	x3, #0xffffffffffffffff    	// #-1
   4cbe4:	mov	x1, x20
   4cbe8:	mov	x2, x19
   4cbec:	mov	x23, #0xffffffffffffffff    	// #-1
   4cbf0:	bl	c9e0 <__gmpn_submul_1@plt>
   4cbf4:	ldur	x8, [x29, #-16]
   4cbf8:	str	x23, [x8]
   4cbfc:	b	4cb24 <__gmpn_dcpi1_div_qr@@Base+0x4cc>
   4cc00:	ldur	x0, [x29, #-56]
   4cc04:	sub	x3, x19, #0x1
   4cc08:	mov	x2, x20
   4cc0c:	mov	x1, x0
   4cc10:	bl	ca70 <__gmpn_add_n@plt>
   4cc14:	ldur	x3, [x29, #-64]
   4cc18:	add	x8, x28, x23
   4cc1c:	add	x28, x8, x0
   4cc20:	cmp	x3, #0x0
   4cc24:	cset	w8, eq  // eq = none
   4cc28:	sub	x25, x25, x8
   4cc2c:	sub	x3, x3, #0x1
   4cc30:	b	4cb18 <__gmpn_dcpi1_div_qr@@Base+0x4c0>

000000000004cc34 <__gmpn_dcpi1_divappr_q@@Base>:
   4cc34:	stp	x29, x30, [sp, #-96]!
   4cc38:	stp	x28, x27, [sp, #16]
   4cc3c:	stp	x26, x25, [sp, #32]
   4cc40:	stp	x24, x23, [sp, #48]
   4cc44:	stp	x22, x21, [sp, #64]
   4cc48:	stp	x20, x19, [sp, #80]
   4cc4c:	mov	x29, sp
   4cc50:	sub	sp, sp, #0x50
   4cc54:	sub	x23, x2, x4
   4cc58:	mov	x12, x5
   4cc5c:	mov	x28, x2
   4cc60:	mov	x22, x0
   4cc64:	cmp	x23, x4
   4cc68:	add	x16, x3, x4, lsl #3
   4cc6c:	stur	x3, [x29, #-8]
   4cc70:	b.ge	4ccd4 <__gmpn_dcpi1_divappr_q@@Base+0xa0>  // b.tcont
   4cc74:	add	x4, x23, #0x1
   4cc78:	lsl	x10, x4, #3
   4cc7c:	add	x10, x10, #0xf
   4cc80:	add	x9, x1, x28, lsl #3
   4cc84:	mov	x11, sp
   4cc88:	and	x10, x10, #0xfffffffffffffff0
   4cc8c:	neg	x8, x23
   4cc90:	sub	x9, x9, x23, lsl #3
   4cc94:	sub	x20, x11, x10
   4cc98:	mov	sp, x20
   4cc9c:	cmp	x23, #0x97
   4cca0:	b.le	4cdbc <__gmpn_dcpi1_divappr_q@@Base+0x188>
   4cca4:	mov	x11, sp
   4cca8:	sub	x5, x11, x10
   4ccac:	mov	sp, x5
   4ccb0:	add	x8, x9, x8, lsl #3
   4ccb4:	mvn	x9, x23
   4ccb8:	sub	x1, x8, #0x10
   4ccbc:	add	x2, x16, x9, lsl #3
   4ccc0:	mov	x0, x20
   4ccc4:	mov	x3, x4
   4ccc8:	mov	x4, x12
   4cccc:	bl	4d1c8 <__gmpn_dcpi1_divappr_q@@Base+0x594>
   4ccd0:	b	4cddc <__gmpn_dcpi1_divappr_q@@Base+0x1a8>
   4ccd4:	lsl	x19, x4, #3
   4ccd8:	lsl	x8, x28, #3
   4ccdc:	lsl	x11, x4, #4
   4cce0:	add	x27, x1, x19
   4cce4:	sub	x8, x8, x11
   4cce8:	mov	x20, x4
   4ccec:	mov	x21, xzr
   4ccf0:	mov	x24, xzr
   4ccf4:	add	x10, x23, #0x1
   4ccf8:	sub	x14, x4, #0x1
   4ccfc:	add	x9, x8, #0x10
   4cd00:	mov	x8, x27
   4cd04:	stur	x12, [x29, #-24]
   4cd08:	sub	x21, x21, x20
   4cd0c:	add	x26, x10, x21
   4cd10:	add	x24, x24, x19
   4cd14:	add	x8, x8, x19
   4cd18:	cmp	x26, x20
   4cd1c:	sub	x9, x9, x11
   4cd20:	b.gt	4cd08 <__gmpn_dcpi1_divappr_q@@Base+0xd4>
   4cd24:	add	x11, x19, #0xf
   4cd28:	add	x23, x22, x24
   4cd2c:	mov	x10, sp
   4cd30:	and	x11, x11, #0xfffffffffffffff0
   4cd34:	sub	x22, x23, #0x8
   4cd38:	add	x13, x27, x24
   4cd3c:	sub	x10, x10, x11
   4cd40:	stur	x10, [x29, #-16]
   4cd44:	mov	sp, x10
   4cd48:	cmp	x26, #0x2
   4cd4c:	stp	x13, x1, [x29, #-40]
   4cd50:	b.eq	4cdf4 <__gmpn_dcpi1_divappr_q@@Base+0x1c0>  // b.none
   4cd54:	cmp	x26, #0x1
   4cd58:	b.ne	4ce10 <__gmpn_dcpi1_divappr_q@@Base+0x1dc>  // b.any
   4cd5c:	ldur	x2, [x29, #-8]
   4cd60:	add	x0, x1, x24
   4cd64:	mov	x9, xzr
   4cd68:	sub	x15, x0, #0x8
   4cd6c:	add	x10, x2, x14, lsl #3
   4cd70:	stp	x14, x15, [x29, #-56]
   4cd74:	add	x11, x20, x9
   4cd78:	cmp	x11, #0x1
   4cd7c:	b.lt	4cda0 <__gmpn_dcpi1_divappr_q@@Base+0x16c>  // b.tstop
   4cd80:	lsl	x11, x9, #3
   4cd84:	add	x12, x8, x11
   4cd88:	ldur	x12, [x12, #-8]
   4cd8c:	ldr	x11, [x10, x11]
   4cd90:	sub	x9, x9, #0x1
   4cd94:	cmp	x12, x11
   4cd98:	b.eq	4cd74 <__gmpn_dcpi1_divappr_q@@Base+0x140>  // b.none
   4cd9c:	b.ls	4cfe0 <__gmpn_dcpi1_divappr_q@@Base+0x3ac>  // b.plast
   4cda0:	mov	x1, x0
   4cda4:	mov	x3, x20
   4cda8:	mov	x25, x16
   4cdac:	bl	c2d0 <__gmpn_sub_n@plt>
   4cdb0:	mov	x16, x25
   4cdb4:	mov	w25, #0x1                   	// #1
   4cdb8:	b	4cfe4 <__gmpn_dcpi1_divappr_q@@Base+0x3b0>
   4cdbc:	ldr	x5, [x12]
   4cdc0:	add	x8, x9, x8, lsl #3
   4cdc4:	sub	x1, x8, #0x10
   4cdc8:	mvn	x8, x23
   4cdcc:	lsl	x2, x4, #1
   4cdd0:	add	x3, x16, x8, lsl #3
   4cdd4:	mov	x0, x20
   4cdd8:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   4cddc:	mov	x25, x0
   4cde0:	add	x1, x20, #0x8
   4cde4:	mov	x0, x22
   4cde8:	mov	x2, x23
   4cdec:	bl	ca50 <__gmpn_copyi@plt>
   4cdf0:	b	4d120 <__gmpn_dcpi1_divappr_q@@Base+0x4ec>
   4cdf4:	sub	x2, x13, #0x18
   4cdf8:	sub	x4, x16, #0x10
   4cdfc:	mov	w3, #0x4                   	// #4
   4ce00:	mov	x0, x22
   4ce04:	mov	x1, xzr
   4ce08:	bl	c200 <__gmpn_divrem_2@plt>
   4ce0c:	b	4ce78 <__gmpn_dcpi1_divappr_q@@Base+0x244>
   4ce10:	cmp	x26, #0x29
   4ce14:	sub	x1, x1, x9
   4ce18:	b.le	4ce48 <__gmpn_dcpi1_divappr_q@@Base+0x214>
   4ce1c:	ldp	x5, x9, [x29, #-16]
   4ce20:	lsl	x8, x20, #1
   4ce24:	sub	x8, x8, x28
   4ce28:	ldur	x4, [x29, #-24]
   4ce2c:	add	x8, x9, x8, lsl #3
   4ce30:	add	x8, x8, x24
   4ce34:	sub	x2, x8, #0x8
   4ce38:	mov	x0, x22
   4ce3c:	mov	x3, x26
   4ce40:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4ce44:	b	4ce78 <__gmpn_dcpi1_divappr_q@@Base+0x244>
   4ce48:	ldur	x9, [x29, #-24]
   4ce4c:	lsl	x8, x20, #1
   4ce50:	sub	x8, x8, x28
   4ce54:	lsl	x2, x26, #1
   4ce58:	ldr	x5, [x9]
   4ce5c:	ldur	x9, [x29, #-8]
   4ce60:	mov	x0, x22
   4ce64:	mov	x4, x26
   4ce68:	add	x8, x9, x8, lsl #3
   4ce6c:	add	x8, x8, x24
   4ce70:	sub	x3, x8, #0x8
   4ce74:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4ce78:	mvn	x8, x28
   4ce7c:	add	x8, x8, x20, lsl #1
   4ce80:	mov	x25, x0
   4ce84:	cmp	x8, x21
   4ce88:	b.eq	4cf18 <__gmpn_dcpi1_divappr_q@@Base+0x2e4>  // b.none
   4ce8c:	lsl	x8, x20, #1
   4ce90:	sub	x8, x8, x28
   4ce94:	mvn	x9, x21
   4ce98:	add	x8, x9, x8
   4ce9c:	cmp	x26, x8
   4cea0:	stur	x8, [x29, #-48]
   4cea4:	b.le	4cebc <__gmpn_dcpi1_divappr_q@@Base+0x288>
   4cea8:	ldp	x28, x3, [x29, #-16]
   4ceac:	mov	x1, x22
   4ceb0:	mov	x2, x26
   4ceb4:	mov	x4, x8
   4ceb8:	b	4cecc <__gmpn_dcpi1_divappr_q@@Base+0x298>
   4cebc:	ldp	x28, x1, [x29, #-16]
   4cec0:	mov	x2, x8
   4cec4:	mov	x3, x22
   4cec8:	mov	x4, x26
   4cecc:	mov	x0, x28
   4ced0:	bl	ccd0 <__gmpn_mul@plt>
   4ced4:	ldur	x8, [x29, #-32]
   4ced8:	mov	x2, x28
   4cedc:	mov	x3, x20
   4cee0:	add	x8, x8, x24
   4cee4:	sub	x27, x8, #0x8
   4cee8:	mov	x0, x27
   4ceec:	mov	x1, x27
   4cef0:	bl	c2d0 <__gmpn_sub_n@plt>
   4cef4:	mov	x28, x0
   4cef8:	cbz	x25, 4cf14 <__gmpn_dcpi1_divappr_q@@Base+0x2e0>
   4cefc:	ldur	x2, [x29, #-8]
   4cf00:	ldur	x3, [x29, #-48]
   4cf04:	add	x0, x27, x26, lsl #3
   4cf08:	mov	x1, x0
   4cf0c:	bl	c2d0 <__gmpn_sub_n@plt>
   4cf10:	add	x28, x0, x28
   4cf14:	cbnz	x28, 4cfa0 <__gmpn_dcpi1_divappr_q@@Base+0x36c>
   4cf18:	neg	x21, x21
   4cf1c:	cmp	x21, x20
   4cf20:	neg	x10, x20
   4cf24:	b.le	4d0c4 <__gmpn_dcpi1_divappr_q@@Base+0x490>
   4cf28:	ldp	x8, x27, [x29, #-32]
   4cf2c:	ldp	x23, x28, [x29, #-16]
   4cf30:	neg	x26, x20, lsl #3
   4cf34:	stur	x10, [x29, #-40]
   4cf38:	add	x8, x8, x24
   4cf3c:	sub	x24, x8, #0x8
   4cf40:	add	x22, x22, x26
   4cf44:	add	x24, x24, x26
   4cf48:	mov	x0, x22
   4cf4c:	mov	x1, x24
   4cf50:	mov	x2, x28
   4cf54:	mov	x3, x20
   4cf58:	mov	x4, x27
   4cf5c:	mov	x5, x23
   4cf60:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4cf64:	sub	x21, x21, x20
   4cf68:	cmp	x21, x20
   4cf6c:	b.gt	4cf40 <__gmpn_dcpi1_divappr_q@@Base+0x30c>
   4cf70:	ldur	x10, [x29, #-40]
   4cf74:	add	x8, x24, x19
   4cf78:	b	4d0d4 <__gmpn_dcpi1_divappr_q@@Base+0x4a0>
   4cf7c:	mov	x8, xzr
   4cf80:	ldur	x2, [x29, #-8]
   4cf84:	mov	x0, x27
   4cf88:	mov	x1, x27
   4cf8c:	mov	x3, x20
   4cf90:	sub	x25, x25, x8
   4cf94:	bl	ca70 <__gmpn_add_n@plt>
   4cf98:	subs	x28, x28, x0
   4cf9c:	b.eq	4cf18 <__gmpn_dcpi1_divappr_q@@Base+0x2e4>  // b.none
   4cfa0:	ldur	x8, [x23, #-8]
   4cfa4:	sub	x9, x8, #0x1
   4cfa8:	stur	x9, [x23, #-8]
   4cfac:	cbnz	x8, 4cf7c <__gmpn_dcpi1_divappr_q@@Base+0x348>
   4cfb0:	mov	x8, x23
   4cfb4:	mov	w9, #0x1                   	// #1
   4cfb8:	cmp	x9, x26
   4cfbc:	b.ge	4cfd8 <__gmpn_dcpi1_divappr_q@@Base+0x3a4>  // b.tcont
   4cfc0:	ldr	x10, [x8]
   4cfc4:	add	x9, x9, #0x1
   4cfc8:	sub	x11, x10, #0x1
   4cfcc:	str	x11, [x8], #8
   4cfd0:	cbz	x10, 4cfb8 <__gmpn_dcpi1_divappr_q@@Base+0x384>
   4cfd4:	b	4cf7c <__gmpn_dcpi1_divappr_q@@Base+0x348>
   4cfd8:	mov	w8, #0x1                   	// #1
   4cfdc:	b	4cf80 <__gmpn_dcpi1_divappr_q@@Base+0x34c>
   4cfe0:	mov	x25, xzr
   4cfe4:	ldur	x8, [x29, #-40]
   4cfe8:	add	x17, x27, x24
   4cfec:	ldur	x10, [x17, #-16]
   4cff0:	ldur	x9, [x8, #-8]
   4cff4:	ldp	x8, x28, [x16, #-16]
   4cff8:	cmp	x9, x28
   4cffc:	b.ne	4d008 <__gmpn_dcpi1_divappr_q@@Base+0x3d4>  // b.any
   4d000:	cmp	x10, x8
   4d004:	b.eq	4d168 <__gmpn_dcpi1_divappr_q@@Base+0x534>  // b.none
   4d008:	ldur	x11, [x29, #-24]
   4d00c:	ldur	x12, [x17, #-24]
   4d010:	ldr	x11, [x11]
   4d014:	mul	x13, x11, x9
   4d018:	umulh	x11, x9, x11
   4d01c:	adds	x14, x13, x10
   4d020:	adc	x9, x11, x9
   4d024:	msub	x10, x9, x28, x10
   4d028:	subs	x15, x12, x8
   4d02c:	sbc	x10, x10, x28
   4d030:	mul	x11, x9, x8
   4d034:	umulh	x13, x8, x9
   4d038:	subs	x12, x15, x11
   4d03c:	sbc	x10, x10, x13
   4d040:	cmp	x10, x14
   4d044:	cset	w11, cs  // cs = hs, nlast
   4d048:	csetm	x13, cs  // cs = hs, nlast
   4d04c:	sub	x9, x9, x11
   4d050:	and	x11, x8, x13
   4d054:	and	x13, x28, x13
   4d058:	adds	x26, x12, x11
   4d05c:	adc	x27, x10, x13
   4d060:	cmp	x27, x28
   4d064:	add	x3, x9, #0x1
   4d068:	b.cs	4d144 <__gmpn_dcpi1_divappr_q@@Base+0x510>  // b.hs, b.nlast
   4d06c:	cmp	x20, #0x3
   4d070:	b.lt	4d0a8 <__gmpn_dcpi1_divappr_q@@Base+0x474>  // b.tstop
   4d074:	ldur	x0, [x29, #-48]
   4d078:	ldur	x1, [x29, #-8]
   4d07c:	sub	x2, x20, #0x2
   4d080:	stp	x3, x17, [x29, #-72]
   4d084:	bl	c9e0 <__gmpn_submul_1@plt>
   4d088:	ldur	x17, [x29, #-64]
   4d08c:	subs	x8, x26, x0
   4d090:	cset	w9, cc  // cc = lo, ul, last
   4d094:	subs	x27, x27, x9
   4d098:	stur	x8, [x17, #-24]
   4d09c:	b.cc	4d198 <__gmpn_dcpi1_divappr_q@@Base+0x564>  // b.lo, b.ul, b.last
   4d0a0:	ldur	x3, [x29, #-72]
   4d0a4:	b	4d0ac <__gmpn_dcpi1_divappr_q@@Base+0x478>
   4d0a8:	stur	x26, [x17, #-24]
   4d0ac:	stur	x27, [x17, #-16]
   4d0b0:	stur	x3, [x23, #-8]
   4d0b4:	neg	x21, x21
   4d0b8:	cmp	x21, x20
   4d0bc:	neg	x10, x20
   4d0c0:	b.gt	4cf28 <__gmpn_dcpi1_divappr_q@@Base+0x2f4>
   4d0c4:	ldur	x8, [x29, #-40]
   4d0c8:	ldur	x27, [x29, #-24]
   4d0cc:	ldur	x28, [x29, #-8]
   4d0d0:	sub	x8, x8, #0x8
   4d0d4:	mov	w9, #0x1                   	// #1
   4d0d8:	ldur	x5, [x29, #-16]
   4d0dc:	lsl	x10, x10, #3
   4d0e0:	ldr	x19, [x22]
   4d0e4:	sub	x9, x9, x21
   4d0e8:	add	x26, x22, x9, lsl #3
   4d0ec:	add	x8, x8, x10
   4d0f0:	add	x1, x8, x10
   4d0f4:	mov	x0, x26
   4d0f8:	mov	x2, x28
   4d0fc:	mov	x3, x20
   4d100:	mov	x4, x27
   4d104:	sub	x24, x21, #0x1
   4d108:	bl	4d1c8 <__gmpn_dcpi1_divappr_q@@Base+0x594>
   4d10c:	add	x1, x26, #0x8
   4d110:	mov	x0, x26
   4d114:	mov	x2, x24
   4d118:	bl	ca50 <__gmpn_copyi@plt>
   4d11c:	str	x19, [x22]
   4d120:	mov	x0, x25
   4d124:	mov	sp, x29
   4d128:	ldp	x20, x19, [sp, #80]
   4d12c:	ldp	x22, x21, [sp, #64]
   4d130:	ldp	x24, x23, [sp, #48]
   4d134:	ldp	x26, x25, [sp, #32]
   4d138:	ldp	x28, x27, [sp, #16]
   4d13c:	ldp	x29, x30, [sp], #96
   4d140:	ret
   4d144:	cmp	x26, x8
   4d148:	b.cs	4d154 <__gmpn_dcpi1_divappr_q@@Base+0x520>  // b.hs, b.nlast
   4d14c:	cmp	x27, x28
   4d150:	b.ls	4d06c <__gmpn_dcpi1_divappr_q@@Base+0x438>  // b.plast
   4d154:	subs	x9, x26, x8
   4d158:	sbc	x27, x27, x28
   4d15c:	add	x3, x3, #0x1
   4d160:	mov	x26, x9
   4d164:	b	4d06c <__gmpn_dcpi1_divappr_q@@Base+0x438>
   4d168:	ldur	x0, [x29, #-48]
   4d16c:	ldur	x1, [x29, #-8]
   4d170:	mov	x3, #0xffffffffffffffff    	// #-1
   4d174:	mov	x2, x20
   4d178:	mov	x26, #0xffffffffffffffff    	// #-1
   4d17c:	bl	c9e0 <__gmpn_submul_1@plt>
   4d180:	stur	x26, [x23, #-8]
   4d184:	neg	x21, x21
   4d188:	cmp	x21, x20
   4d18c:	neg	x10, x20
   4d190:	b.gt	4cf28 <__gmpn_dcpi1_divappr_q@@Base+0x2f4>
   4d194:	b	4d0c4 <__gmpn_dcpi1_divappr_q@@Base+0x490>
   4d198:	ldp	x3, x0, [x29, #-56]
   4d19c:	ldur	x2, [x29, #-8]
   4d1a0:	mov	x1, x0
   4d1a4:	bl	ca70 <__gmpn_add_n@plt>
   4d1a8:	ldp	x3, x17, [x29, #-72]
   4d1ac:	add	x8, x27, x28
   4d1b0:	add	x27, x8, x0
   4d1b4:	cmp	x3, #0x0
   4d1b8:	cset	w8, eq  // eq = none
   4d1bc:	sub	x25, x25, x8
   4d1c0:	sub	x3, x3, #0x1
   4d1c4:	b	4d0ac <__gmpn_dcpi1_divappr_q@@Base+0x478>
   4d1c8:	sub	sp, sp, #0x80
   4d1cc:	stp	x20, x19, [sp, #112]
   4d1d0:	asr	x20, x3, #1
   4d1d4:	stp	x28, x27, [sp, #48]
   4d1d8:	sub	x28, x3, x20
   4d1dc:	and	x8, x3, #0xfffffffffffffffe
   4d1e0:	stp	x29, x30, [sp, #32]
   4d1e4:	stp	x26, x25, [sp, #64]
   4d1e8:	stp	x24, x23, [sp, #80]
   4d1ec:	stp	x22, x21, [sp, #96]
   4d1f0:	add	x29, sp, #0x20
   4d1f4:	mov	x23, x5
   4d1f8:	mov	x21, x3
   4d1fc:	mov	x25, x2
   4d200:	mov	x27, x1
   4d204:	add	x19, x0, x20, lsl #3
   4d208:	cmp	x28, #0x29
   4d20c:	add	x1, x1, x8, lsl #3
   4d210:	stur	x0, [x29, #-8]
   4d214:	stp	x8, x4, [sp, #8]
   4d218:	b.le	4d234 <__gmpn_dcpi1_divappr_q@@Base+0x600>
   4d21c:	add	x2, x25, x20, lsl #3
   4d220:	mov	x0, x19
   4d224:	mov	x3, x28
   4d228:	mov	x5, x23
   4d22c:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4d230:	b	4d24c <__gmpn_dcpi1_divappr_q@@Base+0x618>
   4d234:	ldr	x5, [x4]
   4d238:	lsl	x2, x28, #1
   4d23c:	add	x3, x25, x20, lsl #3
   4d240:	mov	x0, x19
   4d244:	mov	x4, x28
   4d248:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4d24c:	mov	x24, x0
   4d250:	mov	x0, x23
   4d254:	mov	x1, x19
   4d258:	mov	x2, x28
   4d25c:	mov	x3, x25
   4d260:	mov	x4, x20
   4d264:	bl	ccd0 <__gmpn_mul@plt>
   4d268:	add	x26, x27, x20, lsl #3
   4d26c:	mov	x0, x26
   4d270:	mov	x1, x26
   4d274:	mov	x2, x23
   4d278:	mov	x3, x21
   4d27c:	bl	c2d0 <__gmpn_sub_n@plt>
   4d280:	mov	x22, x0
   4d284:	cbz	x24, 4d2a0 <__gmpn_dcpi1_divappr_q@@Base+0x66c>
   4d288:	add	x0, x27, x21, lsl #3
   4d28c:	mov	x1, x0
   4d290:	mov	x2, x25
   4d294:	mov	x3, x20
   4d298:	bl	c2d0 <__gmpn_sub_n@plt>
   4d29c:	add	x22, x0, x22
   4d2a0:	cbnz	x22, 4d300 <__gmpn_dcpi1_divappr_q@@Base+0x6cc>
   4d2a4:	lsl	x8, x28, #3
   4d2a8:	cmp	x21, #0x12f
   4d2ac:	add	x1, x27, x8
   4d2b0:	add	x3, x25, x8
   4d2b4:	b.le	4d340 <__gmpn_dcpi1_divappr_q@@Base+0x70c>
   4d2b8:	ldur	x19, [x29, #-8]
   4d2bc:	ldr	x4, [sp, #16]
   4d2c0:	mov	x2, x3
   4d2c4:	mov	x3, x20
   4d2c8:	mov	x0, x19
   4d2cc:	mov	x5, x23
   4d2d0:	bl	4d1c8 <__gmpn_dcpi1_divappr_q@@Base+0x594>
   4d2d4:	cbnz	x0, 4d35c <__gmpn_dcpi1_divappr_q@@Base+0x728>
   4d2d8:	b	4d364 <__gmpn_dcpi1_divappr_q@@Base+0x730>
   4d2dc:	mov	x8, xzr
   4d2e0:	mov	x0, x26
   4d2e4:	mov	x1, x26
   4d2e8:	mov	x2, x25
   4d2ec:	mov	x3, x21
   4d2f0:	sub	x24, x24, x8
   4d2f4:	bl	ca70 <__gmpn_add_n@plt>
   4d2f8:	subs	x22, x22, x0
   4d2fc:	b.eq	4d2a4 <__gmpn_dcpi1_divappr_q@@Base+0x670>  // b.none
   4d300:	ldr	x8, [x19]
   4d304:	sub	x9, x8, #0x1
   4d308:	str	x9, [x19]
   4d30c:	cbnz	x8, 4d2dc <__gmpn_dcpi1_divappr_q@@Base+0x6a8>
   4d310:	mov	w8, #0x1                   	// #1
   4d314:	cmp	x8, x28
   4d318:	b.ge	4d338 <__gmpn_dcpi1_divappr_q@@Base+0x704>  // b.tcont
   4d31c:	lsl	x9, x8, #3
   4d320:	ldr	x10, [x19, x9]
   4d324:	add	x8, x8, #0x1
   4d328:	sub	x11, x10, #0x1
   4d32c:	str	x11, [x19, x9]
   4d330:	cbz	x10, 4d314 <__gmpn_dcpi1_divappr_q@@Base+0x6e0>
   4d334:	b	4d2dc <__gmpn_dcpi1_divappr_q@@Base+0x6a8>
   4d338:	mov	w8, #0x1                   	// #1
   4d33c:	b	4d2e0 <__gmpn_dcpi1_divappr_q@@Base+0x6ac>
   4d340:	ldp	x2, x8, [sp, #8]
   4d344:	ldur	x19, [x29, #-8]
   4d348:	mov	x4, x20
   4d34c:	ldr	x5, [x8]
   4d350:	mov	x0, x19
   4d354:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   4d358:	cbz	x0, 4d364 <__gmpn_dcpi1_divappr_q@@Base+0x730>
   4d35c:	cmp	x21, #0x2
   4d360:	b.ge	4d388 <__gmpn_dcpi1_divappr_q@@Base+0x754>  // b.tcont
   4d364:	mov	x0, x24
   4d368:	ldp	x20, x19, [sp, #112]
   4d36c:	ldp	x22, x21, [sp, #96]
   4d370:	ldp	x24, x23, [sp, #80]
   4d374:	ldp	x26, x25, [sp, #64]
   4d378:	ldp	x28, x27, [sp, #48]
   4d37c:	ldp	x29, x30, [sp, #32]
   4d380:	add	sp, sp, #0x80
   4d384:	ret
   4d388:	cmp	x20, #0x1
   4d38c:	csinc	x8, x20, xzr, gt
   4d390:	lsl	x2, x8, #3
   4d394:	mov	w1, #0xff                  	// #255
   4d398:	mov	x0, x19
   4d39c:	bl	c5f0 <memset@plt>
   4d3a0:	b	4d364 <__gmpn_dcpi1_divappr_q@@Base+0x730>

000000000004d3a4 <__gmpn_mu_div_qr@@Base>:
   4d3a4:	sub	sp, sp, #0x80
   4d3a8:	stp	x22, x21, [sp, #96]
   4d3ac:	sub	x22, x3, x5
   4d3b0:	add	x8, x22, #0x64
   4d3b4:	stp	x28, x27, [sp, #48]
   4d3b8:	stp	x26, x25, [sp, #64]
   4d3bc:	stp	x24, x23, [sp, #80]
   4d3c0:	stp	x20, x19, [sp, #112]
   4d3c4:	mov	x25, x6
   4d3c8:	mov	x19, x5
   4d3cc:	mov	x26, x3
   4d3d0:	mov	x27, x2
   4d3d4:	cmp	x8, x5
   4d3d8:	mov	x23, x0
   4d3dc:	stp	x29, x30, [sp, #32]
   4d3e0:	add	x29, sp, #0x20
   4d3e4:	b.ge	4d468 <__gmpn_mu_div_qr@@Base+0xc4>  // b.tcont
   4d3e8:	mov	w20, #0x1                   	// #1
   4d3ec:	lsl	x8, x26, #3
   4d3f0:	bfi	x20, x22, #1, #63
   4d3f4:	add	x9, x4, x19, lsl #3
   4d3f8:	add	x21, x22, #0x1
   4d3fc:	mvn	x10, x22
   4d400:	add	x11, x1, x8
   4d404:	add	x8, x27, x8
   4d408:	lsl	x12, x20, #3
   4d40c:	mov	x28, x4
   4d410:	str	x1, [sp, #16]
   4d414:	sub	x1, x11, x12
   4d418:	sub	x2, x8, x12
   4d41c:	add	x4, x9, x10, lsl #3
   4d420:	mov	x0, x23
   4d424:	mov	x3, x20
   4d428:	mov	x5, x21
   4d42c:	mov	x6, x25
   4d430:	str	x1, [sp, #8]
   4d434:	bl	4d598 <__gmpn_mu_div_qr@@Base+0x1f4>
   4d438:	str	x21, [sp]
   4d43c:	sub	x21, x19, x21
   4d440:	cmp	x21, x22
   4d444:	mov	x24, x0
   4d448:	mov	x0, x25
   4d44c:	stur	x28, [x29, #-8]
   4d450:	b.le	4d49c <__gmpn_mu_div_qr@@Base+0xf8>
   4d454:	mov	x1, x28
   4d458:	mov	x2, x21
   4d45c:	mov	x3, x23
   4d460:	mov	x4, x22
   4d464:	b	4d4ac <__gmpn_mu_div_qr@@Base+0x108>
   4d468:	mov	x0, x23
   4d46c:	mov	x2, x27
   4d470:	mov	x3, x26
   4d474:	mov	x5, x19
   4d478:	mov	x6, x25
   4d47c:	ldp	x20, x19, [sp, #112]
   4d480:	ldp	x22, x21, [sp, #96]
   4d484:	ldp	x24, x23, [sp, #80]
   4d488:	ldp	x26, x25, [sp, #64]
   4d48c:	ldp	x28, x27, [sp, #48]
   4d490:	ldp	x29, x30, [sp, #32]
   4d494:	add	sp, sp, #0x80
   4d498:	b	4d598 <__gmpn_mu_div_qr@@Base+0x1f4>
   4d49c:	mov	x1, x23
   4d4a0:	mov	x2, x22
   4d4a4:	mov	x3, x28
   4d4a8:	mov	x4, x21
   4d4ac:	bl	ccd0 <__gmpn_mul@plt>
   4d4b0:	mov	x28, x24
   4d4b4:	ldr	x24, [sp]
   4d4b8:	neg	x8, x20
   4d4bc:	str	x8, [sp]
   4d4c0:	cbz	x28, 4d4dc <__gmpn_mu_div_qr@@Base+0x138>
   4d4c4:	ldur	x2, [x29, #-8]
   4d4c8:	add	x0, x25, x22, lsl #3
   4d4cc:	mov	x1, x0
   4d4d0:	mov	x3, x21
   4d4d4:	bl	ca70 <__gmpn_add_n@plt>
   4d4d8:	b	4d4e0 <__gmpn_mu_div_qr@@Base+0x13c>
   4d4dc:	mov	x0, xzr
   4d4e0:	sub	x3, x26, x20
   4d4e4:	ldr	x20, [sp, #16]
   4d4e8:	add	x8, x25, x19, lsl #3
   4d4ec:	stur	x0, [x8, #-8]
   4d4f0:	mov	x1, x27
   4d4f4:	mov	x0, x20
   4d4f8:	mov	x2, x25
   4d4fc:	bl	c2d0 <__gmpn_sub_n@plt>
   4d500:	mov	x4, x0
   4d504:	ldp	x9, x0, [sp]
   4d508:	add	x8, x25, x26, lsl #3
   4d50c:	mov	x3, x24
   4d510:	add	x2, x8, x9, lsl #3
   4d514:	mov	x1, x0
   4d518:	bl	c760 <__gmpn_sub_nc@plt>
   4d51c:	cbz	x0, 4d574 <__gmpn_mu_div_qr@@Base+0x1d0>
   4d520:	ldr	x8, [x23]
   4d524:	sub	x9, x8, #0x1
   4d528:	str	x9, [x23]
   4d52c:	cbnz	x8, 4d558 <__gmpn_mu_div_qr@@Base+0x1b4>
   4d530:	mov	w8, #0x1                   	// #1
   4d534:	mov	w9, #0x1                   	// #1
   4d538:	cmp	x9, x22
   4d53c:	b.ge	4d55c <__gmpn_mu_div_qr@@Base+0x1b8>  // b.tcont
   4d540:	lsl	x10, x9, #3
   4d544:	ldr	x11, [x23, x10]
   4d548:	add	x9, x9, #0x1
   4d54c:	sub	x12, x11, #0x1
   4d550:	str	x12, [x23, x10]
   4d554:	cbz	x11, 4d538 <__gmpn_mu_div_qr@@Base+0x194>
   4d558:	mov	x8, xzr
   4d55c:	ldur	x2, [x29, #-8]
   4d560:	mov	x0, x20
   4d564:	mov	x1, x20
   4d568:	mov	x3, x19
   4d56c:	sub	x28, x28, x8
   4d570:	bl	ca70 <__gmpn_add_n@plt>
   4d574:	mov	x0, x28
   4d578:	ldp	x20, x19, [sp, #112]
   4d57c:	ldp	x22, x21, [sp, #96]
   4d580:	ldp	x24, x23, [sp, #80]
   4d584:	ldp	x26, x25, [sp, #64]
   4d588:	ldp	x28, x27, [sp, #48]
   4d58c:	ldp	x29, x30, [sp, #32]
   4d590:	add	sp, sp, #0x80
   4d594:	ret
   4d598:	sub	sp, sp, #0x70
   4d59c:	stp	x26, x25, [sp, #48]
   4d5a0:	sub	x26, x3, x5
   4d5a4:	stp	x24, x23, [sp, #64]
   4d5a8:	stp	x22, x21, [sp, #80]
   4d5ac:	stp	x20, x19, [sp, #96]
   4d5b0:	mov	x19, x6
   4d5b4:	mov	x20, x5
   4d5b8:	mov	x21, x4
   4d5bc:	mov	x22, x3
   4d5c0:	mov	x23, x2
   4d5c4:	mov	x24, x1
   4d5c8:	cmp	x26, x5
   4d5cc:	mov	x25, x0
   4d5d0:	stp	x29, x30, [sp, #16]
   4d5d4:	stp	x28, x27, [sp, #32]
   4d5d8:	add	x29, sp, #0x10
   4d5dc:	b.le	4d5f0 <__gmpn_mu_div_qr@@Base+0x24c>
   4d5e0:	sub	x8, x26, #0x1
   4d5e4:	sdiv	x9, x8, x20
   4d5e8:	add	x9, x9, #0x1
   4d5ec:	b	4d604 <__gmpn_mu_div_qr@@Base+0x260>
   4d5f0:	add	x8, x26, x26, lsl #1
   4d5f4:	cmp	x8, x20
   4d5f8:	b.le	4d60c <__gmpn_mu_div_qr@@Base+0x268>
   4d5fc:	sub	x8, x26, #0x1
   4d600:	mov	w9, #0x2                   	// #2
   4d604:	sdiv	x8, x8, x9
   4d608:	add	x26, x8, #0x1
   4d60c:	add	x28, x19, x26, lsl #3
   4d610:	cmp	x26, x20
   4d614:	add	x27, x28, #0x8
   4d618:	b.ne	4d65c <__gmpn_mu_div_qr@@Base+0x2b8>  // b.any
   4d61c:	add	x0, x27, #0x8
   4d620:	mov	x1, x21
   4d624:	mov	x2, x20
   4d628:	bl	ca50 <__gmpn_copyi@plt>
   4d62c:	add	x9, x27, x20, lsl #3
   4d630:	mov	w8, #0x1                   	// #1
   4d634:	add	x2, x20, #0x1
   4d638:	add	x3, x9, #0x8
   4d63c:	mov	x0, x19
   4d640:	mov	x1, x27
   4d644:	str	x8, [x27]
   4d648:	bl	d060 <__gmpn_invertappr@plt>
   4d64c:	add	x1, x19, #0x8
   4d650:	mov	x0, x19
   4d654:	mov	x2, x20
   4d658:	b	4d800 <__gmpn_mu_div_qr@@Base+0x45c>
   4d65c:	add	x8, x21, x20, lsl #3
   4d660:	mvn	x9, x26
   4d664:	add	x13, x8, x9, lsl #3
   4d668:	ldr	x9, [x13]
   4d66c:	adds	x9, x9, #0x1
   4d670:	str	x9, [x27]
   4d674:	b.cc	4d774 <__gmpn_mu_div_qr@@Base+0x3d0>  // b.lo, b.ul, b.last
   4d678:	sub	x10, x20, x26
   4d67c:	mov	x15, xzr
   4d680:	mov	x9, xzr
   4d684:	add	x14, x21, x10, lsl #3
   4d688:	mov	x11, x26
   4d68c:	add	x12, x15, #0x1
   4d690:	cmp	x12, x26
   4d694:	b.gt	4d84c <__gmpn_mu_div_qr@@Base+0x4a8>
   4d698:	lsl	x15, x15, #3
   4d69c:	ldr	x16, [x14, x15]
   4d6a0:	add	x15, x28, x15
   4d6a4:	add	x9, x9, #0x8
   4d6a8:	sub	x11, x11, #0x1
   4d6ac:	adds	x16, x16, #0x1
   4d6b0:	str	x16, [x15, #16]
   4d6b4:	mov	x15, x12
   4d6b8:	b.cs	4d68c <__gmpn_mu_div_qr@@Base+0x2e8>  // b.hs, b.nlast
   4d6bc:	cmp	x13, x27
   4d6c0:	b.eq	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4d6c4:	cmp	x12, x26
   4d6c8:	b.ge	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.tcont
   4d6cc:	sub	x13, x26, x12
   4d6d0:	cmp	x13, #0x4
   4d6d4:	add	x14, x12, #0x1
   4d6d8:	b.cc	4d74c <__gmpn_mu_div_qr@@Base+0x3a8>  // b.lo, b.ul, b.last
   4d6dc:	add	x15, x28, x9
   4d6e0:	add	x15, x15, #0x10
   4d6e4:	cmp	x15, x8
   4d6e8:	b.cs	4d704 <__gmpn_mu_div_qr@@Base+0x360>  // b.hs, b.nlast
   4d6ec:	add	x15, x19, x26, lsl #4
   4d6f0:	add	x16, x21, x10, lsl #3
   4d6f4:	add	x15, x15, #0x10
   4d6f8:	add	x16, x16, x9
   4d6fc:	cmp	x16, x15
   4d700:	b.cc	4d74c <__gmpn_mu_div_qr@@Base+0x3a8>  // b.lo, b.ul, b.last
   4d704:	add	x15, x21, x10, lsl #3
   4d708:	and	x16, x11, #0xfffffffffffffffc
   4d70c:	add	x14, x28, x9
   4d710:	and	x10, x13, #0xfffffffffffffffc
   4d714:	add	x9, x15, x9
   4d718:	add	x12, x16, x12
   4d71c:	add	x11, x14, #0x20
   4d720:	add	x9, x9, #0x10
   4d724:	add	x14, x12, #0x1
   4d728:	mov	x12, x10
   4d72c:	ldp	q0, q1, [x9, #-16]
   4d730:	subs	x12, x12, #0x4
   4d734:	add	x9, x9, #0x20
   4d738:	stp	q0, q1, [x11, #-16]
   4d73c:	add	x11, x11, #0x20
   4d740:	b.ne	4d72c <__gmpn_mu_div_qr@@Base+0x388>  // b.any
   4d744:	cmp	x13, x10
   4d748:	b.eq	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4d74c:	add	x10, x14, x26
   4d750:	mvn	x9, x26
   4d754:	add	x10, x19, x10, lsl #3
   4d758:	add	x9, x9, x14
   4d75c:	add	x10, x10, #0x8
   4d760:	ldr	x11, [x8, x9, lsl #3]
   4d764:	adds	x9, x9, #0x1
   4d768:	str	x11, [x10], #8
   4d76c:	b.cc	4d760 <__gmpn_mu_div_qr@@Base+0x3bc>  // b.lo, b.ul, b.last
   4d770:	b	4d7dc <__gmpn_mu_div_qr@@Base+0x438>
   4d774:	cmp	x26, #0x1
   4d778:	b.lt	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.tstop
   4d77c:	cmp	x13, x27
   4d780:	b.eq	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4d784:	cmp	x26, #0x4
   4d788:	b.cc	4d7b4 <__gmpn_mu_div_qr@@Base+0x410>  // b.lo, b.ul, b.last
   4d78c:	add	x12, x19, x26, lsl #3
   4d790:	add	x9, x12, #0x10
   4d794:	sub	x10, x20, x26
   4d798:	cmp	x9, x8
   4d79c:	add	x9, x21, x10, lsl #3
   4d7a0:	b.cs	4d864 <__gmpn_mu_div_qr@@Base+0x4c0>  // b.hs, b.nlast
   4d7a4:	add	x10, x19, x26, lsl #4
   4d7a8:	add	x10, x10, #0x10
   4d7ac:	cmp	x9, x10
   4d7b0:	b.cs	4d864 <__gmpn_mu_div_qr@@Base+0x4c0>  // b.hs, b.nlast
   4d7b4:	mov	w9, #0x1                   	// #1
   4d7b8:	mvn	x10, x26
   4d7bc:	add	x11, x9, x26
   4d7c0:	add	x9, x10, x9
   4d7c4:	add	x10, x19, x11, lsl #3
   4d7c8:	add	x10, x10, #0x8
   4d7cc:	ldr	x11, [x8, x9, lsl #3]
   4d7d0:	adds	x9, x9, #0x1
   4d7d4:	str	x11, [x10], #8
   4d7d8:	b.cc	4d7cc <__gmpn_mu_div_qr@@Base+0x428>  // b.lo, b.ul, b.last
   4d7dc:	add	x8, x27, x26, lsl #3
   4d7e0:	add	x2, x26, #0x1
   4d7e4:	add	x3, x8, #0x8
   4d7e8:	mov	x0, x19
   4d7ec:	mov	x1, x27
   4d7f0:	bl	d060 <__gmpn_invertappr@plt>
   4d7f4:	add	x1, x19, #0x8
   4d7f8:	mov	x0, x19
   4d7fc:	mov	x2, x26
   4d800:	bl	ca50 <__gmpn_copyi@plt>
   4d804:	mov	x0, x25
   4d808:	mov	x1, x24
   4d80c:	mov	x2, x23
   4d810:	mov	x3, x22
   4d814:	mov	x4, x21
   4d818:	mov	x5, x20
   4d81c:	mov	x6, x19
   4d820:	mov	x7, x26
   4d824:	str	x28, [sp]
   4d828:	bl	d080 <__gmpn_preinv_mu_div_qr@plt>
   4d82c:	ldp	x20, x19, [sp, #96]
   4d830:	ldp	x22, x21, [sp, #80]
   4d834:	ldp	x24, x23, [sp, #64]
   4d838:	ldp	x26, x25, [sp, #48]
   4d83c:	ldp	x28, x27, [sp, #32]
   4d840:	ldp	x29, x30, [sp, #16]
   4d844:	add	sp, sp, #0x70
   4d848:	ret
   4d84c:	cbz	x26, 4d804 <__gmpn_mu_div_qr@@Base+0x460>
   4d850:	lsl	x2, x26, #3
   4d854:	mov	x0, x19
   4d858:	mov	w1, wzr
   4d85c:	bl	c5f0 <memset@plt>
   4d860:	b	4d804 <__gmpn_mu_div_qr@@Base+0x460>
   4d864:	and	x10, x26, #0xfffffffffffffffc
   4d868:	add	x11, x9, #0x10
   4d86c:	orr	x9, x10, #0x1
   4d870:	add	x12, x12, #0x20
   4d874:	mov	x13, x10
   4d878:	ldp	q0, q1, [x11, #-16]
   4d87c:	add	x11, x11, #0x20
   4d880:	subs	x13, x13, #0x4
   4d884:	stp	q0, q1, [x12, #-16]
   4d888:	add	x12, x12, #0x20
   4d88c:	b.ne	4d878 <__gmpn_mu_div_qr@@Base+0x4d4>  // b.any
   4d890:	cmp	x26, x10
   4d894:	b.eq	4d7dc <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4d898:	b	4d7b8 <__gmpn_mu_div_qr@@Base+0x414>

000000000004d89c <__gmpn_preinv_mu_div_qr@@Base>:
   4d89c:	sub	sp, sp, #0xc0
   4d8a0:	stp	x29, x30, [sp, #96]
   4d8a4:	add	x29, sp, #0x60
   4d8a8:	stp	x28, x27, [sp, #112]
   4d8ac:	stp	x24, x23, [sp, #144]
   4d8b0:	ldr	x24, [x29, #96]
   4d8b4:	sub	x28, x3, x5
   4d8b8:	lsl	x8, x28, #3
   4d8bc:	stp	x26, x25, [sp, #128]
   4d8c0:	mov	x23, x1
   4d8c4:	add	x1, x2, x8
   4d8c8:	add	x26, x0, x8
   4d8cc:	add	x8, x2, x3, lsl #3
   4d8d0:	stp	x22, x21, [sp, #160]
   4d8d4:	stp	x20, x19, [sp, #176]
   4d8d8:	mov	x19, x7
   4d8dc:	mov	x27, x6
   4d8e0:	mov	x21, x5
   4d8e4:	mov	x22, x4
   4d8e8:	sub	x8, x8, #0x8
   4d8ec:	mov	x9, x5
   4d8f0:	stur	x1, [x29, #-8]
   4d8f4:	subs	x10, x9, #0x1
   4d8f8:	b.lt	4d918 <__gmpn_preinv_mu_div_qr@@Base+0x7c>  // b.tstop
   4d8fc:	add	x9, x22, x9, lsl #3
   4d900:	ldr	x11, [x8], #-8
   4d904:	ldur	x9, [x9, #-8]
   4d908:	cmp	x11, x9
   4d90c:	mov	x9, x10
   4d910:	b.eq	4d8f4 <__gmpn_preinv_mu_div_qr@@Base+0x58>  // b.none
   4d914:	b.ls	4d93c <__gmpn_preinv_mu_div_qr@@Base+0xa0>  // b.plast
   4d918:	mov	x0, x23
   4d91c:	mov	x2, x22
   4d920:	mov	x3, x21
   4d924:	bl	c2d0 <__gmpn_sub_n@plt>
   4d928:	mov	w8, #0x1                   	// #1
   4d92c:	str	x8, [sp]
   4d930:	cmp	x28, #0x1
   4d934:	b.ge	4d954 <__gmpn_preinv_mu_div_qr@@Base+0xb8>  // b.tcont
   4d938:	b	4dc4c <__gmpn_preinv_mu_div_qr@@Base+0x3b0>
   4d93c:	mov	x0, x23
   4d940:	mov	x2, x21
   4d944:	bl	ca50 <__gmpn_copyi@plt>
   4d948:	str	xzr, [sp]
   4d94c:	cmp	x28, #0x1
   4d950:	b.lt	4dc4c <__gmpn_preinv_mu_div_qr@@Base+0x3b0>  // b.tstop
   4d954:	add	x8, x23, x21, lsl #3
   4d958:	stur	x8, [x29, #-32]
   4d95c:	add	x8, x21, #0x1
   4d960:	str	x8, [sp, #32]
   4d964:	sub	x8, x23, #0x8
   4d968:	str	x8, [sp, #16]
   4d96c:	add	x8, x24, #0x8
   4d970:	neg	x25, x21
   4d974:	str	x8, [sp, #8]
   4d978:	sub	x8, x21, #0x1
   4d97c:	stur	x8, [x29, #-40]
   4d980:	str	x25, [sp, #24]
   4d984:	b	4d990 <__gmpn_preinv_mu_div_qr@@Base+0xf4>
   4d988:	cmp	x28, #0x0
   4d98c:	b.le	4dc4c <__gmpn_preinv_mu_div_qr@@Base+0x3b0>
   4d990:	subs	x8, x19, x28
   4d994:	add	x8, x27, x8, lsl #3
   4d998:	csel	x27, x8, x27, gt
   4d99c:	ldur	x8, [x29, #-32]
   4d9a0:	csel	x19, x28, x19, gt
   4d9a4:	lsl	x20, x19, #3
   4d9a8:	mov	x0, x24
   4d9ac:	sub	x25, x8, x20
   4d9b0:	mov	x1, x25
   4d9b4:	mov	x2, x27
   4d9b8:	mov	x3, x19
   4d9bc:	sub	x26, x26, x20
   4d9c0:	bl	c990 <__gmpn_mul_n@plt>
   4d9c4:	add	x1, x24, x20
   4d9c8:	mov	x0, x26
   4d9cc:	mov	x2, x25
   4d9d0:	mov	x3, x19
   4d9d4:	stur	x1, [x29, #-24]
   4d9d8:	bl	ca70 <__gmpn_add_n@plt>
   4d9dc:	cbnz	x0, 4dc70 <__gmpn_preinv_mu_div_qr@@Base+0x3d4>
   4d9e0:	cmp	x19, #0x11
   4d9e4:	stur	x28, [x29, #-16]
   4d9e8:	b.le	4da98 <__gmpn_preinv_mu_div_qr@@Base+0x1fc>
   4d9ec:	ldr	x0, [sp, #32]
   4d9f0:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4d9f4:	mov	x28, x0
   4d9f8:	add	x6, x24, x0, lsl #3
   4d9fc:	mov	x0, x24
   4da00:	mov	x1, x28
   4da04:	mov	x2, x22
   4da08:	mov	x3, x21
   4da0c:	mov	x4, x26
   4da10:	mov	x5, x19
   4da14:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4da18:	add	x8, x19, x21
   4da1c:	sub	x25, x8, x28
   4da20:	cmp	x25, #0x1
   4da24:	b.lt	4db34 <__gmpn_preinv_mu_div_qr@@Base+0x298>  // b.tstop
   4da28:	neg	x8, x19
   4da2c:	stp	x8, x27, [sp, #40]
   4da30:	ldur	x8, [x29, #-32]
   4da34:	lsl	x27, x25, #3
   4da38:	mov	x0, x24
   4da3c:	mov	x1, x24
   4da40:	sub	x2, x8, x27
   4da44:	mov	x3, x25
   4da48:	bl	c2d0 <__gmpn_sub_n@plt>
   4da4c:	add	x8, x24, x27
   4da50:	ldr	x9, [x8]
   4da54:	subs	x9, x9, x0
   4da58:	str	x9, [x8]
   4da5c:	b.cs	4dab4 <__gmpn_preinv_mu_div_qr@@Base+0x218>  // b.hs, b.nlast
   4da60:	ldr	x27, [sp, #48]
   4da64:	ldr	x14, [sp, #24]
   4da68:	sub	x9, x28, x25
   4da6c:	mov	w10, #0x1                   	// #1
   4da70:	cmp	x10, x9
   4da74:	b.ge	4dac4 <__gmpn_preinv_mu_div_qr@@Base+0x228>  // b.tcont
   4da78:	lsl	x11, x10, #3
   4da7c:	ldr	x12, [x8, x11]
   4da80:	add	x10, x10, #0x1
   4da84:	sub	x13, x12, #0x1
   4da88:	str	x13, [x8, x11]
   4da8c:	cbz	x12, 4da70 <__gmpn_preinv_mu_div_qr@@Base+0x1d4>
   4da90:	mov	x8, xzr
   4da94:	b	4dac8 <__gmpn_preinv_mu_div_qr@@Base+0x22c>
   4da98:	mov	x0, x24
   4da9c:	mov	x1, x22
   4daa0:	mov	x2, x21
   4daa4:	mov	x3, x26
   4daa8:	mov	x4, x19
   4daac:	bl	ccd0 <__gmpn_mul@plt>
   4dab0:	b	4db34 <__gmpn_preinv_mu_div_qr@@Base+0x298>
   4dab4:	ldr	x27, [sp, #48]
   4dab8:	ldr	x14, [sp, #24]
   4dabc:	mov	x8, xzr
   4dac0:	b	4dac8 <__gmpn_preinv_mu_div_qr@@Base+0x22c>
   4dac4:	mov	w8, #0x1                   	// #1
   4dac8:	ldr	x9, [sp, #16]
   4dacc:	ldr	x10, [sp, #40]
   4dad0:	add	x9, x9, x10, lsl #3
   4dad4:	add	x10, x14, x28
   4dad8:	cmp	x10, #0x1
   4dadc:	b.lt	4db04 <__gmpn_preinv_mu_div_qr@@Base+0x268>  // b.tstop
   4dae0:	lsl	x10, x28, #3
   4dae4:	ldr	x11, [x9, x10]
   4dae8:	add	x10, x24, x10
   4daec:	ldur	x10, [x10, #-8]
   4daf0:	sub	x28, x28, #0x1
   4daf4:	cmp	x11, x10
   4daf8:	b.eq	4dad4 <__gmpn_preinv_mu_div_qr@@Base+0x238>  // b.none
   4dafc:	cset	w9, ls  // ls = plast
   4db00:	b	4db08 <__gmpn_preinv_mu_div_qr@@Base+0x26c>
   4db04:	mov	x9, xzr
   4db08:	subs	x8, x9, x8
   4db0c:	b.cc	4dc88 <__gmpn_preinv_mu_div_qr@@Base+0x3ec>  // b.lo, b.ul, b.last
   4db10:	ldr	x9, [x24]
   4db14:	adds	x8, x9, x8
   4db18:	str	x8, [x24]
   4db1c:	b.cc	4db34 <__gmpn_preinv_mu_div_qr@@Base+0x298>  // b.lo, b.ul, b.last
   4db20:	ldr	x8, [sp, #8]
   4db24:	ldr	x9, [x8]
   4db28:	adds	x9, x9, #0x1
   4db2c:	str	x9, [x8], #8
   4db30:	b.cs	4db24 <__gmpn_preinv_mu_div_qr@@Base+0x288>  // b.hs, b.nlast
   4db34:	subs	x8, x21, x19
   4db38:	ldr	x8, [x23, x8, lsl #3]
   4db3c:	ldr	x9, [x24, x21, lsl #3]
   4db40:	ldp	x28, x1, [x29, #-16]
   4db44:	subs	x25, x21, x19
   4db48:	sub	x1, x1, x20
   4db4c:	sub	x20, x8, x9
   4db50:	stur	x1, [x29, #-8]
   4db54:	b.ne	4db7c <__gmpn_preinv_mu_div_qr@@Base+0x2e0>  // b.any
   4db58:	mov	x0, x23
   4db5c:	mov	x2, x24
   4db60:	mov	x3, x21
   4db64:	bl	c2d0 <__gmpn_sub_n@plt>
   4db68:	mov	x25, x0
   4db6c:	subs	x20, x20, x25
   4db70:	sub	x28, x28, x19
   4db74:	b.ne	4dbc4 <__gmpn_preinv_mu_div_qr@@Base+0x328>  // b.any
   4db78:	b	4dbf4 <__gmpn_preinv_mu_div_qr@@Base+0x358>
   4db7c:	mov	x0, x24
   4db80:	mov	x2, x24
   4db84:	mov	x3, x19
   4db88:	bl	c2d0 <__gmpn_sub_n@plt>
   4db8c:	mov	x4, x0
   4db90:	ldur	x0, [x29, #-24]
   4db94:	mov	x1, x23
   4db98:	mov	x3, x25
   4db9c:	mov	x2, x0
   4dba0:	bl	c760 <__gmpn_sub_nc@plt>
   4dba4:	mov	x25, x0
   4dba8:	mov	x0, x23
   4dbac:	mov	x1, x24
   4dbb0:	mov	x2, x21
   4dbb4:	bl	ca50 <__gmpn_copyi@plt>
   4dbb8:	subs	x20, x20, x25
   4dbbc:	sub	x28, x28, x19
   4dbc0:	b.eq	4dbf4 <__gmpn_preinv_mu_div_qr@@Base+0x358>  // b.none
   4dbc4:	mov	x8, x26
   4dbc8:	ldr	x9, [x8]
   4dbcc:	adds	x9, x9, #0x1
   4dbd0:	str	x9, [x8], #8
   4dbd4:	b.cs	4dbc8 <__gmpn_preinv_mu_div_qr@@Base+0x32c>  // b.hs, b.nlast
   4dbd8:	mov	x0, x23
   4dbdc:	mov	x1, x23
   4dbe0:	mov	x2, x22
   4dbe4:	mov	x3, x21
   4dbe8:	bl	c2d0 <__gmpn_sub_n@plt>
   4dbec:	subs	x20, x20, x0
   4dbf0:	b.ne	4dbc4 <__gmpn_preinv_mu_div_qr@@Base+0x328>  // b.any
   4dbf4:	ldur	x8, [x29, #-40]
   4dbf8:	add	x9, x8, #0x1
   4dbfc:	cmp	x9, #0x1
   4dc00:	b.lt	4dc20 <__gmpn_preinv_mu_div_qr@@Base+0x384>  // b.tstop
   4dc04:	lsl	x9, x8, #3
   4dc08:	ldr	x10, [x23, x9]
   4dc0c:	ldr	x9, [x22, x9]
   4dc10:	sub	x8, x8, #0x1
   4dc14:	cmp	x10, x9
   4dc18:	b.eq	4dbf8 <__gmpn_preinv_mu_div_qr@@Base+0x35c>  // b.none
   4dc1c:	b.ls	4d988 <__gmpn_preinv_mu_div_qr@@Base+0xec>  // b.plast
   4dc20:	mov	x8, x26
   4dc24:	ldr	x9, [x8]
   4dc28:	adds	x9, x9, #0x1
   4dc2c:	str	x9, [x8], #8
   4dc30:	b.cs	4dc24 <__gmpn_preinv_mu_div_qr@@Base+0x388>  // b.hs, b.nlast
   4dc34:	mov	x0, x23
   4dc38:	mov	x1, x23
   4dc3c:	mov	x2, x22
   4dc40:	mov	x3, x21
   4dc44:	bl	c2d0 <__gmpn_sub_n@plt>
   4dc48:	b	4d988 <__gmpn_preinv_mu_div_qr@@Base+0xec>
   4dc4c:	ldr	x0, [sp]
   4dc50:	ldp	x20, x19, [sp, #176]
   4dc54:	ldp	x22, x21, [sp, #160]
   4dc58:	ldp	x24, x23, [sp, #144]
   4dc5c:	ldp	x26, x25, [sp, #128]
   4dc60:	ldp	x28, x27, [sp, #112]
   4dc64:	ldp	x29, x30, [sp, #96]
   4dc68:	add	sp, sp, #0xc0
   4dc6c:	ret
   4dc70:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4dc74:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4dc78:	add	x0, x0, #0x72b
   4dc7c:	add	x2, x2, #0x711
   4dc80:	mov	w1, #0x118                 	// #280
   4dc84:	bl	c6c0 <__gmp_assert_fail@plt>
   4dc88:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4dc8c:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4dc90:	add	x0, x0, #0x72b
   4dc94:	add	x2, x2, #0x737
   4dc98:	mov	w1, #0x12c                 	// #300
   4dc9c:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004dca0 <__gmpn_mu_div_qr_itch@@Base>:
   4dca0:	stp	x29, x30, [sp, #-32]!
   4dca4:	stp	x20, x19, [sp, #16]
   4dca8:	sub	x20, x0, x1
   4dcac:	mov	x19, x1
   4dcb0:	cmp	x20, x1
   4dcb4:	mov	x29, sp
   4dcb8:	cbz	w2, 4dccc <__gmpn_mu_div_qr_itch@@Base+0x2c>
   4dcbc:	csel	x8, x19, x20, gt
   4dcc0:	sub	x8, x8, #0x1
   4dcc4:	sxtw	x9, w2
   4dcc8:	b	4dcdc <__gmpn_mu_div_qr_itch@@Base+0x3c>
   4dccc:	b.le	4dd28 <__gmpn_mu_div_qr_itch@@Base+0x88>
   4dcd0:	sub	x8, x20, #0x1
   4dcd4:	sdiv	x9, x8, x19
   4dcd8:	add	x9, x9, #0x1
   4dcdc:	sdiv	x8, x8, x9
   4dce0:	add	x20, x8, #0x1
   4dce4:	add	x0, x19, #0x1
   4dce8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4dcec:	asr	x8, x0, #1
   4dcf0:	cmp	x8, x20
   4dcf4:	csel	x10, x0, x8, lt  // lt = tstop
   4dcf8:	cmp	x8, x19
   4dcfc:	csel	x8, x10, xzr, lt  // lt = tstop
   4dd00:	add	x9, x20, x20, lsl #1
   4dd04:	add	x8, x8, x0, lsl #1
   4dd08:	add	x8, x8, #0x4
   4dd0c:	add	x9, x9, #0x4
   4dd10:	cmp	x9, x8
   4dd14:	csel	x8, x9, x8, gt
   4dd18:	add	x0, x8, x20
   4dd1c:	ldp	x20, x19, [sp, #16]
   4dd20:	ldp	x29, x30, [sp], #32
   4dd24:	ret
   4dd28:	add	x8, x20, x20, lsl #1
   4dd2c:	cmp	x8, x19
   4dd30:	b.le	4dce4 <__gmpn_mu_div_qr_itch@@Base+0x44>
   4dd34:	sub	x8, x20, #0x1
   4dd38:	cmp	x8, #0x0
   4dd3c:	csel	x8, x20, x8, lt  // lt = tstop
   4dd40:	asr	x8, x8, #1
   4dd44:	b	4dce0 <__gmpn_mu_div_qr_itch@@Base+0x40>

000000000004dd48 <__gmpn_preinv_mu_div_qr_itch@@Base>:
   4dd48:	stp	x29, x30, [sp, #-32]!
   4dd4c:	add	x0, x1, #0x1
   4dd50:	stp	x20, x19, [sp, #16]
   4dd54:	mov	x29, sp
   4dd58:	mov	x19, x2
   4dd5c:	mov	x20, x1
   4dd60:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4dd64:	asr	x8, x0, #1
   4dd68:	cmp	x8, x19
   4dd6c:	csel	x9, x0, x8, lt  // lt = tstop
   4dd70:	cmp	x8, x20
   4dd74:	ldp	x20, x19, [sp, #16]
   4dd78:	csel	x8, x9, xzr, lt  // lt = tstop
   4dd7c:	add	x8, x8, x0, lsl #1
   4dd80:	add	x0, x8, #0x4
   4dd84:	ldp	x29, x30, [sp], #32
   4dd88:	ret

000000000004dd8c <__gmpn_mu_divappr_q@@Base>:
   4dd8c:	sub	sp, sp, #0xf0
   4dd90:	stp	x28, x27, [sp, #160]
   4dd94:	sub	x28, x2, x4
   4dd98:	add	x8, x28, #0x1
   4dd9c:	stp	x26, x25, [sp, #176]
   4dda0:	stp	x24, x23, [sp, #192]
   4dda4:	stp	x22, x21, [sp, #208]
   4dda8:	stp	x20, x19, [sp, #224]
   4ddac:	mov	x19, x5
   4ddb0:	mov	x21, x4
   4ddb4:	mov	x20, x3
   4ddb8:	mov	x24, x2
   4ddbc:	mov	x25, x1
   4ddc0:	cmp	x8, x4
   4ddc4:	mov	x27, x0
   4ddc8:	stp	x29, x30, [sp, #144]
   4ddcc:	add	x29, sp, #0x90
   4ddd0:	b.ge	4ddf0 <__gmpn_mu_divappr_q@@Base+0x64>  // b.tcont
   4ddd4:	sub	x9, x21, x8
   4ddd8:	lsl	x10, x9, #3
   4dddc:	sub	x24, x24, x9
   4dde0:	add	x25, x25, x10
   4dde4:	add	x20, x20, x10
   4dde8:	mov	x21, x8
   4ddec:	b	4de08 <__gmpn_mu_divappr_q@@Base+0x7c>
   4ddf0:	cmp	x28, x21
   4ddf4:	b.le	4de08 <__gmpn_mu_divappr_q@@Base+0x7c>
   4ddf8:	sub	x8, x28, #0x1
   4ddfc:	sdiv	x9, x8, x21
   4de00:	add	x9, x9, #0x1
   4de04:	b	4de1c <__gmpn_mu_divappr_q@@Base+0x90>
   4de08:	add	x8, x28, x28, lsl #1
   4de0c:	cmp	x8, x21
   4de10:	b.le	4de24 <__gmpn_mu_divappr_q@@Base+0x98>
   4de14:	sub	x8, x28, #0x1
   4de18:	mov	w9, #0x2                   	// #2
   4de1c:	sdiv	x8, x8, x9
   4de20:	add	x28, x8, #0x1
   4de24:	add	x23, x19, x28, lsl #3
   4de28:	subs	x9, x21, x28
   4de2c:	add	x22, x23, #0x8
   4de30:	b.ne	4de78 <__gmpn_mu_divappr_q@@Base+0xec>  // b.any
   4de34:	add	x0, x22, #0x8
   4de38:	mov	x1, x20
   4de3c:	mov	x2, x21
   4de40:	mov	x26, x19
   4de44:	bl	ca50 <__gmpn_copyi@plt>
   4de48:	add	x9, x22, x21, lsl #3
   4de4c:	mov	w8, #0x1                   	// #1
   4de50:	add	x2, x21, #0x1
   4de54:	add	x3, x9, #0x8
   4de58:	mov	x0, x19
   4de5c:	mov	x1, x22
   4de60:	str	x8, [x22]
   4de64:	bl	d060 <__gmpn_invertappr@plt>
   4de68:	add	x1, x19, #0x8
   4de6c:	mov	x0, x19
   4de70:	mov	x2, x21
   4de74:	b	4e01c <__gmpn_mu_divappr_q@@Base+0x290>
   4de78:	add	x8, x20, x21, lsl #3
   4de7c:	mvn	x10, x28
   4de80:	add	x13, x8, x10, lsl #3
   4de84:	ldr	x10, [x13]
   4de88:	adds	x10, x10, #0x1
   4de8c:	str	x10, [x22]
   4de90:	b.cc	4df8c <__gmpn_mu_divappr_q@@Base+0x200>  // b.lo, b.ul, b.last
   4de94:	mov	x15, xzr
   4de98:	mov	x10, xzr
   4de9c:	add	x14, x20, x9, lsl #3
   4dea0:	mov	x11, x28
   4dea4:	add	x12, x15, #0x1
   4dea8:	cmp	x12, x28
   4deac:	b.gt	4e4c4 <__gmpn_mu_divappr_q@@Base+0x738>
   4deb0:	lsl	x15, x15, #3
   4deb4:	ldr	x16, [x14, x15]
   4deb8:	add	x15, x23, x15
   4debc:	add	x10, x10, #0x8
   4dec0:	sub	x11, x11, #0x1
   4dec4:	adds	x16, x16, #0x1
   4dec8:	str	x16, [x15, #16]
   4decc:	mov	x15, x12
   4ded0:	b.cs	4dea4 <__gmpn_mu_divappr_q@@Base+0x118>  // b.hs, b.nlast
   4ded4:	cmp	x13, x22
   4ded8:	b.eq	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4dedc:	cmp	x12, x28
   4dee0:	b.ge	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.tcont
   4dee4:	sub	x13, x28, x12
   4dee8:	cmp	x13, #0x4
   4deec:	add	x14, x12, #0x1
   4def0:	b.cc	4df64 <__gmpn_mu_divappr_q@@Base+0x1d8>  // b.lo, b.ul, b.last
   4def4:	add	x15, x23, x10
   4def8:	add	x15, x15, #0x10
   4defc:	cmp	x15, x8
   4df00:	b.cs	4df1c <__gmpn_mu_divappr_q@@Base+0x190>  // b.hs, b.nlast
   4df04:	add	x15, x19, x28, lsl #4
   4df08:	add	x16, x20, x9, lsl #3
   4df0c:	add	x15, x15, #0x10
   4df10:	add	x16, x16, x10
   4df14:	cmp	x16, x15
   4df18:	b.cc	4df64 <__gmpn_mu_divappr_q@@Base+0x1d8>  // b.lo, b.ul, b.last
   4df1c:	add	x15, x20, x9, lsl #3
   4df20:	and	x16, x11, #0xfffffffffffffffc
   4df24:	add	x14, x23, x10
   4df28:	and	x9, x13, #0xfffffffffffffffc
   4df2c:	add	x10, x15, x10
   4df30:	add	x12, x16, x12
   4df34:	add	x11, x14, #0x20
   4df38:	add	x10, x10, #0x10
   4df3c:	add	x14, x12, #0x1
   4df40:	mov	x12, x9
   4df44:	ldp	q0, q1, [x10, #-16]
   4df48:	subs	x12, x12, #0x4
   4df4c:	add	x10, x10, #0x20
   4df50:	stp	q0, q1, [x11, #-16]
   4df54:	add	x11, x11, #0x20
   4df58:	b.ne	4df44 <__gmpn_mu_divappr_q@@Base+0x1b8>  // b.any
   4df5c:	cmp	x13, x9
   4df60:	b.eq	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4df64:	add	x10, x14, x28
   4df68:	mvn	x9, x28
   4df6c:	add	x10, x19, x10, lsl #3
   4df70:	add	x9, x9, x14
   4df74:	add	x10, x10, #0x8
   4df78:	ldr	x11, [x8, x9, lsl #3]
   4df7c:	adds	x9, x9, #0x1
   4df80:	str	x11, [x10], #8
   4df84:	b.cc	4df78 <__gmpn_mu_divappr_q@@Base+0x1ec>  // b.lo, b.ul, b.last
   4df88:	b	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>
   4df8c:	cmp	x28, #0x1
   4df90:	b.lt	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.tstop
   4df94:	cmp	x13, x22
   4df98:	b.eq	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4df9c:	cmp	x28, #0x4
   4dfa0:	b.cc	4dfcc <__gmpn_mu_divappr_q@@Base+0x240>  // b.lo, b.ul, b.last
   4dfa4:	add	x12, x19, x28, lsl #3
   4dfa8:	add	x10, x12, #0x10
   4dfac:	cmp	x10, x8
   4dfb0:	add	x9, x20, x9, lsl #3
   4dfb4:	b.cs	4e4dc <__gmpn_mu_divappr_q@@Base+0x750>  // b.hs, b.nlast
   4dfb8:	add	x10, x19, x28, lsl #4
   4dfbc:	add	x10, x10, #0x10
   4dfc0:	cmp	x9, x10
   4dfc4:	mov	x11, x19
   4dfc8:	b.cs	4e4dc <__gmpn_mu_divappr_q@@Base+0x750>  // b.hs, b.nlast
   4dfcc:	mov	w9, #0x1                   	// #1
   4dfd0:	mvn	x10, x28
   4dfd4:	add	x11, x9, x28
   4dfd8:	add	x9, x10, x9
   4dfdc:	add	x10, x19, x11, lsl #3
   4dfe0:	add	x10, x10, #0x8
   4dfe4:	ldr	x11, [x8, x9, lsl #3]
   4dfe8:	adds	x9, x9, #0x1
   4dfec:	str	x11, [x10], #8
   4dff0:	b.cc	4dfe4 <__gmpn_mu_divappr_q@@Base+0x258>  // b.lo, b.ul, b.last
   4dff4:	add	x8, x22, x28, lsl #3
   4dff8:	add	x2, x28, #0x1
   4dffc:	add	x3, x8, #0x8
   4e000:	mov	x0, x19
   4e004:	mov	x1, x22
   4e008:	mov	x26, x19
   4e00c:	bl	d060 <__gmpn_invertappr@plt>
   4e010:	add	x1, x19, #0x8
   4e014:	mov	x0, x19
   4e018:	mov	x2, x28
   4e01c:	bl	ca50 <__gmpn_copyi@plt>
   4e020:	sub	x9, x24, x21
   4e024:	add	x8, x25, x24, lsl #3
   4e028:	lsl	x22, x9, #3
   4e02c:	str	x9, [sp, #64]
   4e030:	add	x24, x25, x22
   4e034:	add	x25, x27, x22
   4e038:	sub	x8, x8, #0x8
   4e03c:	mov	x9, x21
   4e040:	subs	x10, x9, #0x1
   4e044:	b.lt	4e064 <__gmpn_mu_divappr_q@@Base+0x2d8>  // b.tstop
   4e048:	add	x9, x20, x9, lsl #3
   4e04c:	ldr	x11, [x8], #-8
   4e050:	ldur	x9, [x9, #-8]
   4e054:	cmp	x11, x9
   4e058:	mov	x9, x10
   4e05c:	b.eq	4e040 <__gmpn_mu_divappr_q@@Base+0x2b4>  // b.none
   4e060:	b.ls	4e08c <__gmpn_mu_divappr_q@@Base+0x300>  // b.plast
   4e064:	mov	x0, x23
   4e068:	mov	x1, x24
   4e06c:	mov	x2, x20
   4e070:	mov	x3, x21
   4e074:	bl	c2d0 <__gmpn_sub_n@plt>
   4e078:	mov	w8, wzr
   4e07c:	mov	w0, #0x1                   	// #1
   4e080:	ldr	x10, [sp, #64]
   4e084:	cbnz	x10, 4e0ac <__gmpn_mu_divappr_q@@Base+0x320>
   4e088:	b	4e4a4 <__gmpn_mu_divappr_q@@Base+0x718>
   4e08c:	mov	x0, x23
   4e090:	mov	x1, x24
   4e094:	mov	x2, x21
   4e098:	bl	ca50 <__gmpn_copyi@plt>
   4e09c:	mov	x0, xzr
   4e0a0:	mov	w8, #0x1                   	// #1
   4e0a4:	ldr	x10, [sp, #64]
   4e0a8:	cbz	x10, 4e4a4 <__gmpn_mu_divappr_q@@Base+0x718>
   4e0ac:	cmp	x10, #0x1
   4e0b0:	str	w8, [sp, #20]
   4e0b4:	str	x0, [sp, #24]
   4e0b8:	str	x22, [sp, #8]
   4e0bc:	b.lt	4e410 <__gmpn_mu_divappr_q@@Base+0x684>  // b.tstop
   4e0c0:	add	x8, x23, x21, lsl #3
   4e0c4:	stur	x8, [x29, #-8]
   4e0c8:	add	x8, x21, #0x1
   4e0cc:	stur	x8, [x29, #-64]
   4e0d0:	add	x8, x19, x21, lsl #4
   4e0d4:	sub	x9, x19, #0x8
   4e0d8:	add	x11, x21, x28
   4e0dc:	add	x8, x8, #0x8
   4e0e0:	str	x9, [sp, #56]
   4e0e4:	add	x9, x19, x28, lsl #3
   4e0e8:	str	x8, [sp, #40]
   4e0ec:	add	x8, x19, x11, lsl #3
   4e0f0:	stp	x24, x19, [x29, #-24]
   4e0f4:	add	x8, x8, #0x8
   4e0f8:	sub	x19, x9, #0x8
   4e0fc:	mov	x22, x28
   4e100:	str	x11, [sp, #48]
   4e104:	str	x8, [sp, #32]
   4e108:	str	x28, [sp, #72]
   4e10c:	stur	x19, [x29, #-56]
   4e110:	b	4e120 <__gmpn_mu_divappr_q@@Base+0x394>
   4e114:	ldur	x10, [x29, #-40]
   4e118:	cmp	x10, #0x0
   4e11c:	b.le	4e410 <__gmpn_mu_divappr_q@@Base+0x684>
   4e120:	ldp	x2, x28, [x29, #-16]
   4e124:	subs	x8, x22, x10
   4e128:	csel	x22, x10, x22, gt
   4e12c:	mov	x19, x21
   4e130:	lsl	x21, x22, #3
   4e134:	add	x8, x2, x8, lsl #3
   4e138:	sub	x26, x28, x21
   4e13c:	csel	x2, x8, x2, gt
   4e140:	mov	x0, x28
   4e144:	mov	x1, x26
   4e148:	mov	x3, x22
   4e14c:	sub	x25, x25, x21
   4e150:	stur	x2, [x29, #-16]
   4e154:	mov	x24, x10
   4e158:	bl	c990 <__gmpn_mul_n@plt>
   4e15c:	add	x28, x28, x21
   4e160:	mov	x0, x25
   4e164:	mov	x1, x28
   4e168:	mov	x2, x26
   4e16c:	mov	x3, x22
   4e170:	stur	x21, [x29, #-32]
   4e174:	bl	ca70 <__gmpn_add_n@plt>
   4e178:	cbnz	x0, 4e514 <__gmpn_mu_divappr_q@@Base+0x788>
   4e17c:	subs	x24, x24, x22
   4e180:	b.eq	4e410 <__gmpn_mu_divappr_q@@Base+0x684>  // b.none
   4e184:	cmp	x22, #0x11
   4e188:	stp	x28, x24, [x29, #-48]
   4e18c:	b.le	4e24c <__gmpn_mu_divappr_q@@Base+0x4c0>
   4e190:	ldur	x0, [x29, #-64]
   4e194:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e198:	ldur	x26, [x29, #-8]
   4e19c:	mov	x28, x0
   4e1a0:	mov	x1, x28
   4e1a4:	mov	x2, x20
   4e1a8:	add	x6, x26, x0, lsl #3
   4e1ac:	mov	x0, x26
   4e1b0:	mov	x3, x19
   4e1b4:	mov	x4, x25
   4e1b8:	mov	x5, x22
   4e1bc:	mov	x21, x19
   4e1c0:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4e1c4:	add	x8, x22, x19
   4e1c8:	mov	x10, x26
   4e1cc:	sub	x26, x8, x28
   4e1d0:	cmp	x26, #0x1
   4e1d4:	b.lt	4e2ec <__gmpn_mu_divappr_q@@Base+0x560>  // b.tstop
   4e1d8:	lsl	x27, x26, #3
   4e1dc:	sub	x2, x10, x27
   4e1e0:	mov	x0, x10
   4e1e4:	mov	x1, x10
   4e1e8:	mov	x3, x26
   4e1ec:	mov	x19, x10
   4e1f0:	bl	c2d0 <__gmpn_sub_n@plt>
   4e1f4:	ldr	x8, [x19, x27]
   4e1f8:	subs	x8, x8, x0
   4e1fc:	str	x8, [x19, x27]
   4e200:	b.cs	4e244 <__gmpn_mu_divappr_q@@Base+0x4b8>  // b.hs, b.nlast
   4e204:	ldr	x9, [sp, #72]
   4e208:	ldr	x11, [sp, #40]
   4e20c:	mov	x10, xzr
   4e210:	sub	x8, x28, x26
   4e214:	add	x9, x9, x22
   4e218:	sub	x9, x9, x28
   4e21c:	add	x9, x11, x9, lsl #3
   4e220:	add	x11, x10, #0x1
   4e224:	cmp	x11, x8
   4e228:	b.ge	4e274 <__gmpn_mu_divappr_q@@Base+0x4e8>  // b.tcont
   4e22c:	lsl	x10, x10, #3
   4e230:	ldr	x12, [x9, x10]
   4e234:	sub	x13, x12, #0x1
   4e238:	str	x13, [x9, x10]
   4e23c:	mov	x10, x11
   4e240:	cbz	x12, 4e220 <__gmpn_mu_divappr_q@@Base+0x494>
   4e244:	mov	x8, xzr
   4e248:	b	4e278 <__gmpn_mu_divappr_q@@Base+0x4ec>
   4e24c:	ldur	x26, [x29, #-8]
   4e250:	mov	x1, x20
   4e254:	mov	x2, x19
   4e258:	mov	x3, x25
   4e25c:	mov	x0, x26
   4e260:	mov	x4, x22
   4e264:	mov	x21, x19
   4e268:	bl	ccd0 <__gmpn_mul@plt>
   4e26c:	mov	x10, x26
   4e270:	b	4e2ec <__gmpn_mu_divappr_q@@Base+0x560>
   4e274:	mov	w8, #0x1                   	// #1
   4e278:	ldr	x11, [sp, #72]
   4e27c:	ldp	x10, x12, [sp, #48]
   4e280:	sub	x9, x28, x21
   4e284:	add	x11, x11, x28
   4e288:	add	x10, x10, x28
   4e28c:	sub	x11, x11, x22
   4e290:	add	x10, x12, x10, lsl #3
   4e294:	add	x11, x12, x11, lsl #3
   4e298:	subs	x9, x9, #0x1
   4e29c:	b.lt	4e2b8 <__gmpn_mu_divappr_q@@Base+0x52c>  // b.tstop
   4e2a0:	ldr	x12, [x11], #-8
   4e2a4:	ldr	x13, [x10], #-8
   4e2a8:	cmp	x12, x13
   4e2ac:	b.eq	4e298 <__gmpn_mu_divappr_q@@Base+0x50c>  // b.none
   4e2b0:	cset	w9, ls  // ls = plast
   4e2b4:	b	4e2bc <__gmpn_mu_divappr_q@@Base+0x530>
   4e2b8:	mov	x9, xzr
   4e2bc:	ldur	x10, [x29, #-8]
   4e2c0:	subs	x8, x9, x8
   4e2c4:	b.cc	4e52c <__gmpn_mu_divappr_q@@Base+0x7a0>  // b.lo, b.ul, b.last
   4e2c8:	ldr	x9, [x10]
   4e2cc:	adds	x8, x9, x8
   4e2d0:	str	x8, [x10]
   4e2d4:	b.cc	4e2ec <__gmpn_mu_divappr_q@@Base+0x560>  // b.lo, b.ul, b.last
   4e2d8:	ldr	x8, [sp, #32]
   4e2dc:	ldr	x9, [x8]
   4e2e0:	adds	x9, x9, #0x1
   4e2e4:	str	x9, [x8], #8
   4e2e8:	b.cs	4e2dc <__gmpn_mu_divappr_q@@Base+0x550>  // b.hs, b.nlast
   4e2ec:	subs	x8, x21, x22
   4e2f0:	ldr	x8, [x23, x8, lsl #3]
   4e2f4:	ldr	x9, [x10, x21, lsl #3]
   4e2f8:	ldp	x11, x1, [x29, #-32]
   4e2fc:	subs	x26, x21, x22
   4e300:	sub	x28, x8, x9
   4e304:	sub	x1, x1, x11
   4e308:	stur	x1, [x29, #-24]
   4e30c:	b.ne	4e334 <__gmpn_mu_divappr_q@@Base+0x5a8>  // b.any
   4e310:	mov	x0, x23
   4e314:	mov	x2, x10
   4e318:	mov	x3, x21
   4e31c:	bl	c2d0 <__gmpn_sub_n@plt>
   4e320:	mov	x24, x0
   4e324:	ldur	x19, [x29, #-56]
   4e328:	subs	x26, x28, x24
   4e32c:	b.ne	4e380 <__gmpn_mu_divappr_q@@Base+0x5f4>  // b.any
   4e330:	b	4e3b4 <__gmpn_mu_divappr_q@@Base+0x628>
   4e334:	mov	x0, x10
   4e338:	mov	x2, x10
   4e33c:	mov	x3, x22
   4e340:	mov	x27, x10
   4e344:	bl	c2d0 <__gmpn_sub_n@plt>
   4e348:	mov	x4, x0
   4e34c:	ldur	x0, [x29, #-48]
   4e350:	mov	x1, x23
   4e354:	mov	x3, x26
   4e358:	mov	x2, x0
   4e35c:	bl	c760 <__gmpn_sub_nc@plt>
   4e360:	mov	x24, x0
   4e364:	mov	x0, x23
   4e368:	mov	x1, x27
   4e36c:	mov	x2, x21
   4e370:	bl	ca50 <__gmpn_copyi@plt>
   4e374:	ldur	x19, [x29, #-56]
   4e378:	subs	x26, x28, x24
   4e37c:	b.eq	4e3b4 <__gmpn_mu_divappr_q@@Base+0x628>  // b.none
   4e380:	mov	x8, x25
   4e384:	ldr	x9, [x8]
   4e388:	adds	x9, x9, #0x1
   4e38c:	str	x9, [x8], #8
   4e390:	b.cs	4e384 <__gmpn_mu_divappr_q@@Base+0x5f8>  // b.hs, b.nlast
   4e394:	mov	x0, x23
   4e398:	mov	x1, x23
   4e39c:	mov	x2, x20
   4e3a0:	mov	x3, x21
   4e3a4:	bl	c2d0 <__gmpn_sub_n@plt>
   4e3a8:	subs	x26, x26, x0
   4e3ac:	b.ne	4e380 <__gmpn_mu_divappr_q@@Base+0x5f4>  // b.any
   4e3b0:	mov	x24, x0
   4e3b4:	mov	x8, x21
   4e3b8:	subs	x9, x8, #0x1
   4e3bc:	b.lt	4e3e0 <__gmpn_mu_divappr_q@@Base+0x654>  // b.tstop
   4e3c0:	lsl	x8, x8, #3
   4e3c4:	ldr	x10, [x19, x8]
   4e3c8:	add	x8, x20, x8
   4e3cc:	ldur	x8, [x8, #-8]
   4e3d0:	cmp	x10, x8
   4e3d4:	mov	x8, x9
   4e3d8:	b.eq	4e3b8 <__gmpn_mu_divappr_q@@Base+0x62c>  // b.none
   4e3dc:	b.ls	4e114 <__gmpn_mu_divappr_q@@Base+0x388>  // b.plast
   4e3e0:	mov	x8, x25
   4e3e4:	ldr	x9, [x8]
   4e3e8:	adds	x9, x9, #0x1
   4e3ec:	str	x9, [x8], #8
   4e3f0:	b.cs	4e3e4 <__gmpn_mu_divappr_q@@Base+0x658>  // b.hs, b.nlast
   4e3f4:	mov	x0, x23
   4e3f8:	mov	x1, x23
   4e3fc:	mov	x2, x20
   4e400:	mov	x3, x21
   4e404:	bl	c2d0 <__gmpn_sub_n@plt>
   4e408:	mov	x24, x0
   4e40c:	b	4e114 <__gmpn_mu_divappr_q@@Base+0x388>
   4e410:	ldr	x8, [x25]
   4e414:	ldr	x19, [sp, #24]
   4e418:	add	x9, x8, #0x3
   4e41c:	cmn	x8, #0x3
   4e420:	str	x9, [x25]
   4e424:	b.cc	4e458 <__gmpn_mu_divappr_q@@Base+0x6cc>  // b.lo, b.ul, b.last
   4e428:	ldr	x11, [sp, #64]
   4e42c:	mov	w8, #0x1                   	// #1
   4e430:	cmp	x8, x11
   4e434:	b.ge	4e464 <__gmpn_mu_divappr_q@@Base+0x6d8>  // b.tcont
   4e438:	lsl	x9, x8, #3
   4e43c:	ldr	x10, [x25, x9]
   4e440:	add	x8, x8, #0x1
   4e444:	adds	x10, x10, #0x1
   4e448:	str	x10, [x25, x9]
   4e44c:	b.cs	4e430 <__gmpn_mu_divappr_q@@Base+0x6a4>  // b.hs, b.nlast
   4e450:	mov	x8, xzr
   4e454:	b	4e468 <__gmpn_mu_divappr_q@@Base+0x6dc>
   4e458:	ldr	x11, [sp, #64]
   4e45c:	mov	x8, xzr
   4e460:	b	4e468 <__gmpn_mu_divappr_q@@Base+0x6dc>
   4e464:	mov	x8, #0xffffffffffffffff    	// #-1
   4e468:	ldr	w9, [sp, #20]
   4e46c:	cmp	x24, x8
   4e470:	cset	w8, eq  // eq = none
   4e474:	orr	w8, w9, w8
   4e478:	csinc	x9, x19, xzr, eq  // eq = none
   4e47c:	cmp	w8, #0x0
   4e480:	csel	x0, x9, x19, ne  // ne = any
   4e484:	tbnz	w8, #0, 4e4a4 <__gmpn_mu_divappr_q@@Base+0x718>
   4e488:	cmp	x11, #0x1
   4e48c:	b.lt	4e4a4 <__gmpn_mu_divappr_q@@Base+0x718>  // b.tstop
   4e490:	ldr	x2, [sp, #8]
   4e494:	mov	w1, #0xff                  	// #255
   4e498:	mov	x0, x25
   4e49c:	bl	c5f0 <memset@plt>
   4e4a0:	mov	x0, x19
   4e4a4:	ldp	x20, x19, [sp, #224]
   4e4a8:	ldp	x22, x21, [sp, #208]
   4e4ac:	ldp	x24, x23, [sp, #192]
   4e4b0:	ldp	x26, x25, [sp, #176]
   4e4b4:	ldp	x28, x27, [sp, #160]
   4e4b8:	ldp	x29, x30, [sp, #144]
   4e4bc:	add	sp, sp, #0xf0
   4e4c0:	ret
   4e4c4:	cbz	x28, 4e020 <__gmpn_mu_divappr_q@@Base+0x294>
   4e4c8:	lsl	x2, x28, #3
   4e4cc:	mov	x0, x19
   4e4d0:	mov	w1, wzr
   4e4d4:	bl	c5f0 <memset@plt>
   4e4d8:	b	4e020 <__gmpn_mu_divappr_q@@Base+0x294>
   4e4dc:	and	x10, x28, #0xfffffffffffffffc
   4e4e0:	add	x11, x9, #0x10
   4e4e4:	orr	x9, x10, #0x1
   4e4e8:	add	x12, x12, #0x20
   4e4ec:	mov	x13, x10
   4e4f0:	ldp	q0, q1, [x11, #-16]
   4e4f4:	add	x11, x11, #0x20
   4e4f8:	subs	x13, x13, #0x4
   4e4fc:	stp	q0, q1, [x12, #-16]
   4e500:	add	x12, x12, #0x20
   4e504:	b.ne	4e4f0 <__gmpn_mu_divappr_q@@Base+0x764>  // b.any
   4e508:	cmp	x28, x10
   4e50c:	b.eq	4dff4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4e510:	b	4dfd0 <__gmpn_mu_divappr_q@@Base+0x244>
   4e514:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4e518:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4e51c:	add	x0, x0, #0x740
   4e520:	add	x2, x2, #0x711
   4e524:	mov	w1, #0xd0                  	// #208
   4e528:	bl	c6c0 <__gmp_assert_fail@plt>
   4e52c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4e530:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   4e534:	add	x0, x0, #0x740
   4e538:	add	x2, x2, #0x737
   4e53c:	mov	w1, #0xe6                  	// #230
   4e540:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004e544 <__gmpn_mu_divappr_q_itch@@Base>:
   4e544:	stp	x29, x30, [sp, #-32]!
   4e548:	stp	x20, x19, [sp, #16]
   4e54c:	sub	x20, x0, x1
   4e550:	add	x8, x20, #0x1
   4e554:	cmp	x8, x1
   4e558:	csinc	x19, x1, x20, ge  // ge = tcont
   4e55c:	mov	x29, sp
   4e560:	cbz	w2, 4e578 <__gmpn_mu_divappr_q_itch@@Base+0x34>
   4e564:	cmp	x19, x20
   4e568:	csel	x8, x19, x20, lt  // lt = tstop
   4e56c:	sub	x8, x8, #0x1
   4e570:	sxtw	x9, w2
   4e574:	b	4e58c <__gmpn_mu_divappr_q_itch@@Base+0x48>
   4e578:	cmp	x20, x19
   4e57c:	b.le	4e5dc <__gmpn_mu_divappr_q_itch@@Base+0x98>
   4e580:	sub	x8, x20, #0x1
   4e584:	sdiv	x9, x8, x19
   4e588:	add	x9, x9, #0x1
   4e58c:	sdiv	x8, x8, x9
   4e590:	add	x20, x8, #0x1
   4e594:	add	x0, x19, #0x1
   4e598:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e59c:	asr	x8, x0, #1
   4e5a0:	cmp	x8, x20
   4e5a4:	csel	x10, x0, x8, lt  // lt = tstop
   4e5a8:	cmp	x8, x19
   4e5ac:	csel	x8, x10, xzr, lt  // lt = tstop
   4e5b0:	add	x10, x19, x0, lsl #1
   4e5b4:	add	x9, x20, x20, lsl #1
   4e5b8:	add	x8, x10, x8
   4e5bc:	add	x9, x9, #0x4
   4e5c0:	add	x8, x8, #0x4
   4e5c4:	cmp	x8, x9
   4e5c8:	csel	x8, x8, x9, gt
   4e5cc:	add	x0, x8, x20
   4e5d0:	ldp	x20, x19, [sp, #16]
   4e5d4:	ldp	x29, x30, [sp], #32
   4e5d8:	ret
   4e5dc:	add	x8, x20, x20, lsl #1
   4e5e0:	cmp	x8, x19
   4e5e4:	b.le	4e594 <__gmpn_mu_divappr_q_itch@@Base+0x50>
   4e5e8:	sub	x8, x20, #0x1
   4e5ec:	cmp	x8, #0x0
   4e5f0:	csel	x8, x20, x8, lt  // lt = tstop
   4e5f4:	asr	x8, x8, #1
   4e5f8:	b	4e590 <__gmpn_mu_divappr_q_itch@@Base+0x4c>

000000000004e5fc <__gmpn_mu_div_q@@Base>:
   4e5fc:	sub	sp, sp, #0x80
   4e600:	stp	x22, x21, [sp, #96]
   4e604:	sub	x21, x2, x4
   4e608:	stp	x29, x30, [sp, #32]
   4e60c:	add	x29, sp, #0x20
   4e610:	add	x22, x21, #0x1
   4e614:	stp	x28, x27, [sp, #48]
   4e618:	stp	x26, x25, [sp, #64]
   4e61c:	mov	x26, x1
   4e620:	mov	x28, x0
   4e624:	lsl	x1, x22, #3
   4e628:	sub	x0, x29, #0x8
   4e62c:	stp	x24, x23, [sp, #80]
   4e630:	stp	x20, x19, [sp, #112]
   4e634:	mov	x19, x5
   4e638:	mov	x24, x4
   4e63c:	mov	x27, x3
   4e640:	mov	x23, x2
   4e644:	stur	xzr, [x29, #-8]
   4e648:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4e64c:	cmp	x21, x24
   4e650:	mov	x25, x0
   4e654:	str	x0, [sp, #16]
   4e658:	b.ge	4e6c0 <__gmpn_mu_div_q@@Base+0xc4>  // b.tcont
   4e65c:	lsl	x9, x21, #1
   4e660:	mov	x10, #0xfffffffffffffffe    	// #-2
   4e664:	add	x8, x26, x23, lsl #3
   4e668:	add	x11, x27, x24, lsl #3
   4e66c:	mvn	x12, x21
   4e670:	add	x2, x9, #0x2
   4e674:	sub	x9, x10, x9
   4e678:	add	x1, x8, x9, lsl #3
   4e67c:	add	x3, x11, x12, lsl #3
   4e680:	mov	x0, x25
   4e684:	mov	x4, x22
   4e688:	mov	x5, x19
   4e68c:	bl	c710 <__gmpn_mu_divappr_q@plt>
   4e690:	ldr	x8, [x25]
   4e694:	mov	x20, x0
   4e698:	cmp	x8, #0x7
   4e69c:	b.cc	4e74c <__gmpn_mu_div_q@@Base+0x150>  // b.lo, b.ul, b.last
   4e6a0:	ldr	x8, [sp, #16]
   4e6a4:	mov	x0, x28
   4e6a8:	add	x1, x8, #0x8
   4e6ac:	mov	x2, x21
   4e6b0:	bl	ca50 <__gmpn_copyi@plt>
   4e6b4:	ldur	x0, [x29, #-8]
   4e6b8:	cbz	x0, 4ea20 <__gmpn_mu_div_q@@Base+0x424>
   4e6bc:	b	4e870 <__gmpn_mu_div_q@@Base+0x274>
   4e6c0:	add	x22, x23, #0x1
   4e6c4:	lsl	x1, x22, #3
   4e6c8:	sub	x0, x29, #0x8
   4e6cc:	str	x28, [sp, #8]
   4e6d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4e6d4:	add	x20, x0, #0x8
   4e6d8:	mov	x28, x0
   4e6dc:	mov	x0, x20
   4e6e0:	mov	x1, x26
   4e6e4:	mov	x2, x23
   4e6e8:	bl	ca50 <__gmpn_copyi@plt>
   4e6ec:	lsl	x8, x23, #3
   4e6f0:	add	x9, x20, x8
   4e6f4:	sub	x0, x9, x24, lsl #3
   4e6f8:	add	x8, x28, x8
   4e6fc:	mov	x9, x24
   4e700:	str	xzr, [x28]
   4e704:	subs	x10, x9, #0x1
   4e708:	b.lt	4e72c <__gmpn_mu_div_q@@Base+0x130>  // b.tstop
   4e70c:	add	x9, x27, x9, lsl #3
   4e710:	ldr	x11, [x8], #-8
   4e714:	ldur	x12, [x9, #-8]
   4e718:	mov	x9, x10
   4e71c:	cmp	x11, x12
   4e720:	b.eq	4e704 <__gmpn_mu_div_q@@Base+0x108>  // b.none
   4e724:	cmp	x11, x12
   4e728:	b.ls	4e800 <__gmpn_mu_div_q@@Base+0x204>  // b.plast
   4e72c:	mov	x1, x0
   4e730:	mov	x2, x27
   4e734:	mov	x3, x24
   4e738:	bl	c2d0 <__gmpn_sub_n@plt>
   4e73c:	mov	w8, #0x1                   	// #1
   4e740:	mov	w20, #0x1                   	// #1
   4e744:	str	x8, [sp]
   4e748:	b	4e808 <__gmpn_mu_div_q@@Base+0x20c>
   4e74c:	lsl	x1, x23, #3
   4e750:	sub	x0, x29, #0x8
   4e754:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4e758:	ldr	x25, [sp, #16]
   4e75c:	mov	x1, x27
   4e760:	mov	x2, x24
   4e764:	mov	x4, x21
   4e768:	add	x19, x25, #0x8
   4e76c:	mov	x3, x19
   4e770:	mov	x22, x0
   4e774:	bl	ccd0 <__gmpn_mul@plt>
   4e778:	cbz	x20, 4e794 <__gmpn_mu_div_q@@Base+0x198>
   4e77c:	add	x0, x22, x21, lsl #3
   4e780:	mov	x1, x0
   4e784:	mov	x2, x27
   4e788:	mov	x3, x24
   4e78c:	bl	ca70 <__gmpn_add_n@plt>
   4e790:	cbnz	x0, 4e7c4 <__gmpn_mu_div_q@@Base+0x1c8>
   4e794:	sub	x8, x26, #0x8
   4e798:	sub	x9, x22, #0x8
   4e79c:	mov	x10, x23
   4e7a0:	subs	x11, x10, #0x1
   4e7a4:	b.lt	4e858 <__gmpn_mu_div_q@@Base+0x25c>  // b.tstop
   4e7a8:	lsl	x10, x10, #3
   4e7ac:	ldr	x12, [x9, x10]
   4e7b0:	ldr	x10, [x8, x10]
   4e7b4:	cmp	x12, x10
   4e7b8:	mov	x10, x11
   4e7bc:	b.eq	4e7a0 <__gmpn_mu_div_q@@Base+0x1a4>  // b.none
   4e7c0:	b.ls	4e858 <__gmpn_mu_div_q@@Base+0x25c>  // b.plast
   4e7c4:	ldr	x8, [x19]
   4e7c8:	sub	x9, x8, #0x1
   4e7cc:	str	x9, [x28]
   4e7d0:	cbz	x8, 4e878 <__gmpn_mu_div_q@@Base+0x27c>
   4e7d4:	cmp	x21, #0x2
   4e7d8:	mov	x8, xzr
   4e7dc:	b.lt	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.tstop
   4e7e0:	cmp	x19, x28
   4e7e4:	b.eq	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.none
   4e7e8:	mvn	x8, x24
   4e7ec:	add	x8, x8, x23
   4e7f0:	add	x0, x28, #0x8
   4e7f4:	add	x1, x25, #0x10
   4e7f8:	lsl	x2, x8, #3
   4e7fc:	b	4ea0c <__gmpn_mu_div_q@@Base+0x410>
   4e800:	str	xzr, [sp]
   4e804:	mov	w20, wzr
   4e808:	mov	x0, x25
   4e80c:	mov	x1, x28
   4e810:	mov	x2, x22
   4e814:	mov	x3, x27
   4e818:	mov	x4, x24
   4e81c:	mov	x5, x19
   4e820:	bl	c710 <__gmpn_mu_divappr_q@plt>
   4e824:	cbz	x0, 4e82c <__gmpn_mu_div_q@@Base+0x230>
   4e828:	tbz	x21, #63, 4e8d4 <__gmpn_mu_div_q@@Base+0x2d8>
   4e82c:	ldr	x8, [x25], #8
   4e830:	cmp	x8, #0x5
   4e834:	b.cc	4e8f4 <__gmpn_mu_div_q@@Base+0x2f8>  // b.lo, b.ul, b.last
   4e838:	ldr	x0, [sp, #8]
   4e83c:	mov	x1, x25
   4e840:	mov	x2, x21
   4e844:	bl	ca50 <__gmpn_copyi@plt>
   4e848:	ldr	x20, [sp]
   4e84c:	ldur	x0, [x29, #-8]
   4e850:	cbz	x0, 4ea20 <__gmpn_mu_div_q@@Base+0x424>
   4e854:	b	4e870 <__gmpn_mu_div_q@@Base+0x274>
   4e858:	mov	x0, x28
   4e85c:	mov	x1, x19
   4e860:	mov	x2, x21
   4e864:	bl	ca50 <__gmpn_copyi@plt>
   4e868:	ldur	x0, [x29, #-8]
   4e86c:	cbz	x0, 4ea20 <__gmpn_mu_div_q@@Base+0x424>
   4e870:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4e874:	b	4ea20 <__gmpn_mu_div_q@@Base+0x424>
   4e878:	mov	x9, xzr
   4e87c:	add	x11, x25, #0x10
   4e880:	mov	w8, #0x1                   	// #1
   4e884:	mov	w10, #0x1                   	// #1
   4e888:	cmp	x10, x21
   4e88c:	b.ge	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.tcont
   4e890:	ldr	x12, [x11, x9]
   4e894:	add	x13, x28, x9
   4e898:	add	x10, x10, #0x1
   4e89c:	add	x9, x9, #0x8
   4e8a0:	sub	x14, x12, #0x1
   4e8a4:	str	x14, [x13, #8]
   4e8a8:	cbz	x12, 4e888 <__gmpn_mu_div_q@@Base+0x28c>
   4e8ac:	cmp	x19, x28
   4e8b0:	mov	x8, xzr
   4e8b4:	b.eq	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.none
   4e8b8:	cmp	x10, x21
   4e8bc:	b.ge	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.tcont
   4e8c0:	add	x8, x28, x9
   4e8c4:	add	x9, x25, x9
   4e8c8:	sub	x10, x21, x10
   4e8cc:	add	x0, x8, #0x8
   4e8d0:	b	4ea04 <__gmpn_mu_div_q@@Base+0x408>
   4e8d4:	sub	x8, x22, x24
   4e8d8:	lsl	x2, x8, #3
   4e8dc:	mov	w1, #0xff                  	// #255
   4e8e0:	mov	x0, x25
   4e8e4:	bl	c5f0 <memset@plt>
   4e8e8:	ldr	x8, [x25], #8
   4e8ec:	cmp	x8, #0x5
   4e8f0:	b.cs	4e838 <__gmpn_mu_div_q@@Base+0x23c>  // b.hs, b.nlast
   4e8f4:	mov	x0, x28
   4e8f8:	mov	x1, x25
   4e8fc:	mov	x2, x21
   4e900:	mov	x3, x27
   4e904:	mov	x4, x24
   4e908:	bl	ccd0 <__gmpn_mul@plt>
   4e90c:	cbz	w20, 4e928 <__gmpn_mu_div_q@@Base+0x32c>
   4e910:	add	x0, x28, x21, lsl #3
   4e914:	mov	x1, x0
   4e918:	mov	x2, x27
   4e91c:	mov	x3, x24
   4e920:	bl	ca70 <__gmpn_add_n@plt>
   4e924:	cbnz	x0, 4e958 <__gmpn_mu_div_q@@Base+0x35c>
   4e928:	sub	x8, x26, #0x8
   4e92c:	sub	x9, x28, #0x8
   4e930:	mov	x10, x23
   4e934:	subs	x11, x10, #0x1
   4e938:	b.lt	4e838 <__gmpn_mu_div_q@@Base+0x23c>  // b.tstop
   4e93c:	lsl	x10, x10, #3
   4e940:	ldr	x12, [x9, x10]
   4e944:	ldr	x10, [x8, x10]
   4e948:	cmp	x12, x10
   4e94c:	mov	x10, x11
   4e950:	b.eq	4e934 <__gmpn_mu_div_q@@Base+0x338>  // b.none
   4e954:	b.ls	4e838 <__gmpn_mu_div_q@@Base+0x23c>  // b.plast
   4e958:	ldr	x8, [x25]
   4e95c:	ldr	x15, [sp, #8]
   4e960:	sub	x9, x8, #0x1
   4e964:	str	x9, [x15]
   4e968:	cbz	x8, 4e9a0 <__gmpn_mu_div_q@@Base+0x3a4>
   4e96c:	cmp	x21, #0x2
   4e970:	mov	x8, xzr
   4e974:	b.lt	4ea44 <__gmpn_mu_div_q@@Base+0x448>  // b.tstop
   4e978:	ldr	x20, [sp]
   4e97c:	cmp	x25, x15
   4e980:	b.eq	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.none
   4e984:	ldr	x8, [sp, #16]
   4e988:	add	x0, x15, #0x8
   4e98c:	add	x1, x8, #0x10
   4e990:	mvn	x8, x24
   4e994:	add	x8, x8, x23
   4e998:	lsl	x2, x8, #3
   4e99c:	b	4ea0c <__gmpn_mu_div_q@@Base+0x410>
   4e9a0:	ldr	x8, [sp, #16]
   4e9a4:	mov	x9, xzr
   4e9a8:	mov	w10, #0x1                   	// #1
   4e9ac:	add	x11, x8, #0x10
   4e9b0:	mov	w8, #0x1                   	// #1
   4e9b4:	cmp	x10, x21
   4e9b8:	b.ge	4ea44 <__gmpn_mu_div_q@@Base+0x448>  // b.tcont
   4e9bc:	ldr	x12, [x11, x9]
   4e9c0:	add	x13, x15, x9
   4e9c4:	add	x10, x10, #0x1
   4e9c8:	add	x9, x9, #0x8
   4e9cc:	sub	x14, x12, #0x1
   4e9d0:	str	x14, [x13, #8]
   4e9d4:	cbz	x12, 4e9b4 <__gmpn_mu_div_q@@Base+0x3b8>
   4e9d8:	cmp	x25, x15
   4e9dc:	mov	x8, xzr
   4e9e0:	b.eq	4ea44 <__gmpn_mu_div_q@@Base+0x448>  // b.none
   4e9e4:	ldr	x20, [sp]
   4e9e8:	cmp	x10, x21
   4e9ec:	b.ge	4ea14 <__gmpn_mu_div_q@@Base+0x418>  // b.tcont
   4e9f0:	ldr	x11, [sp, #16]
   4e9f4:	add	x8, x15, x9
   4e9f8:	sub	x10, x21, x10
   4e9fc:	add	x0, x8, #0x8
   4ea00:	add	x9, x11, x9
   4ea04:	add	x1, x9, #0x10
   4ea08:	lsl	x2, x10, #3
   4ea0c:	bl	bed0 <memcpy@plt>
   4ea10:	mov	x8, xzr
   4ea14:	sub	x20, x20, x8
   4ea18:	ldur	x0, [x29, #-8]
   4ea1c:	cbnz	x0, 4e870 <__gmpn_mu_div_q@@Base+0x274>
   4ea20:	mov	x0, x20
   4ea24:	ldp	x20, x19, [sp, #112]
   4ea28:	ldp	x22, x21, [sp, #96]
   4ea2c:	ldp	x24, x23, [sp, #80]
   4ea30:	ldp	x26, x25, [sp, #64]
   4ea34:	ldp	x28, x27, [sp, #48]
   4ea38:	ldp	x29, x30, [sp, #32]
   4ea3c:	add	sp, sp, #0x80
   4ea40:	ret
   4ea44:	ldr	x20, [sp]
   4ea48:	sub	x20, x20, x8
   4ea4c:	ldur	x0, [x29, #-8]
   4ea50:	cbz	x0, 4ea20 <__gmpn_mu_div_q@@Base+0x424>
   4ea54:	b	4e870 <__gmpn_mu_div_q@@Base+0x274>

000000000004ea58 <__gmpn_mu_div_q_itch@@Base>:
   4ea58:	sub	x8, x0, x1
   4ea5c:	lsl	x9, x8, #1
   4ea60:	cmp	x8, x1
   4ea64:	add	x9, x9, #0x2
   4ea68:	csinc	x1, x1, x8, ge  // ge = tcont
   4ea6c:	csinc	x0, x9, x0, lt  // lt = tstop
   4ea70:	b	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   4ea74:	nop

000000000004ea78 <__gmpn_bdiv_q_1@@Base>:
   4ea78:	rbit	x6, x3
   4ea7c:	clz	x5, x6
   4ea80:	lsr	x3, x3, x5
   4ea84:	adrp	x7, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   4ea88:	ubfx	x6, x3, #1, #7
   4ea8c:	ldr	x7, [x7, #3952]
   4ea90:	ldrb	w6, [x7, x6]
   4ea94:	ubfiz	x7, x6, #1, #8
   4ea98:	umull	x6, w6, w6
   4ea9c:	msub	x6, x6, x3, x7
   4eaa0:	lsl	x7, x6, #1
   4eaa4:	mul	x6, x6, x6
   4eaa8:	msub	x6, x6, x3, x7
   4eaac:	lsl	x7, x6, #1
   4eab0:	mul	x6, x6, x6
   4eab4:	msub	x4, x6, x3, x7
   4eab8:	b	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4eabc:	nop

000000000004eac0 <__gmpn_pi1_bdiv_q_1@@Base>:
   4eac0:	sub	x2, x2, #0x1
   4eac4:	subs	x6, x6, x6
   4eac8:	ldr	x9, [x1], #8
   4eacc:	cbz	x5, 4eb14 <__gmpn_pi1_bdiv_q_1@@Base+0x54>
   4ead0:	lsr	x12, x9, x5
   4ead4:	cbz	x2, 4eb04 <__gmpn_pi1_bdiv_q_1@@Base+0x44>
   4ead8:	neg	x8, x5
   4eadc:	ldr	x9, [x1], #8
   4eae0:	lsl	x7, x9, x8
   4eae4:	orr	x7, x7, x12
   4eae8:	sbcs	x6, x7, x6
   4eaec:	mul	x7, x6, x4
   4eaf0:	str	x7, [x0], #8
   4eaf4:	lsr	x12, x9, x5
   4eaf8:	umulh	x6, x7, x3
   4eafc:	sub	x2, x2, #0x1
   4eb00:	cbnz	x2, 4eadc <__gmpn_pi1_bdiv_q_1@@Base+0x1c>
   4eb04:	sbcs	x6, x12, x6
   4eb08:	mul	x6, x6, x4
   4eb0c:	str	x6, [x0]
   4eb10:	ret
   4eb14:	mul	x5, x9, x4
   4eb18:	str	x5, [x0], #8
   4eb1c:	cbz	x2, 4eb3c <__gmpn_pi1_bdiv_q_1@@Base+0x7c>
   4eb20:	ldr	x9, [x1], #8
   4eb24:	umulh	x5, x5, x3
   4eb28:	sbcs	x5, x9, x5
   4eb2c:	mul	x5, x5, x4
   4eb30:	str	x5, [x0], #8
   4eb34:	sub	x2, x2, #0x1
   4eb38:	cbnz	x2, 4eb20 <__gmpn_pi1_bdiv_q_1@@Base+0x60>
   4eb3c:	ret

000000000004eb40 <__gmpn_sbpi1_bdiv_q@@Base>:
   4eb40:	stp	x29, x30, [sp, #-96]!
   4eb44:	stp	x26, x25, [sp, #32]
   4eb48:	stp	x24, x23, [sp, #48]
   4eb4c:	stp	x22, x21, [sp, #64]
   4eb50:	stp	x20, x19, [sp, #80]
   4eb54:	mov	x20, x5
   4eb58:	mov	x23, x4
   4eb5c:	mov	x21, x3
   4eb60:	mov	x22, x1
   4eb64:	subs	x26, x2, x4
   4eb68:	mov	x19, x0
   4eb6c:	stp	x28, x27, [sp, #16]
   4eb70:	mov	x29, sp
   4eb74:	b.le	4ec10 <__gmpn_sbpi1_bdiv_q@@Base+0xd0>
   4eb78:	ldr	x8, [x22]
   4eb7c:	mvn	x9, x23
   4eb80:	add	x25, x9, x2
   4eb84:	mov	x0, x22
   4eb88:	mul	x24, x8, x20
   4eb8c:	mov	x1, x21
   4eb90:	mov	x2, x23
   4eb94:	mov	x3, x24
   4eb98:	bl	d400 <__gmpn_addmul_1@plt>
   4eb9c:	cmp	x25, #0x1
   4eba0:	lsl	x25, x23, #3
   4eba4:	mov	x27, xzr
   4eba8:	b.lt	4ebf8 <__gmpn_sbpi1_bdiv_q@@Base+0xb8>  // b.tstop
   4ebac:	mov	w28, #0x2                   	// #2
   4ebb0:	str	x24, [x19], #8
   4ebb4:	ldr	x8, [x22, x25]
   4ebb8:	adds	x9, x0, x27
   4ebbc:	cset	w10, cs  // cs = hs, nlast
   4ebc0:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   4ebc4:	adds	x8, x8, x9
   4ebc8:	str	x8, [x22, x25]
   4ebcc:	ldr	x8, [x22, #8]!
   4ebd0:	mov	x1, x21
   4ebd4:	mov	x2, x23
   4ebd8:	csel	x27, x10, x11, cc  // cc = lo, ul, last
   4ebdc:	mul	x24, x8, x20
   4ebe0:	mov	x0, x22
   4ebe4:	mov	x3, x24
   4ebe8:	bl	d400 <__gmpn_addmul_1@plt>
   4ebec:	sub	x26, x26, #0x1
   4ebf0:	cmp	x26, #0x1
   4ebf4:	b.gt	4ebb0 <__gmpn_sbpi1_bdiv_q@@Base+0x70>
   4ebf8:	str	x24, [x19], #8
   4ebfc:	ldr	x8, [x22, x25]
   4ec00:	add	x9, x0, x27
   4ec04:	add	x8, x9, x8
   4ec08:	str	x8, [x22, x25]
   4ec0c:	add	x22, x22, #0x8
   4ec10:	ldr	x8, [x22]
   4ec14:	cmp	x23, #0x2
   4ec18:	mul	x24, x8, x20
   4ec1c:	b.lt	4ec4c <__gmpn_sbpi1_bdiv_q@@Base+0x10c>  // b.tstop
   4ec20:	mov	x0, x22
   4ec24:	mov	x1, x21
   4ec28:	mov	x2, x23
   4ec2c:	mov	x3, x24
   4ec30:	bl	d400 <__gmpn_addmul_1@plt>
   4ec34:	str	x24, [x19], #8
   4ec38:	ldr	x8, [x22, #8]!
   4ec3c:	cmp	x23, #0x2
   4ec40:	sub	x23, x23, #0x1
   4ec44:	mul	x24, x8, x20
   4ec48:	b.gt	4ec20 <__gmpn_sbpi1_bdiv_q@@Base+0xe0>
   4ec4c:	str	x24, [x19]
   4ec50:	ldp	x20, x19, [sp, #80]
   4ec54:	ldp	x22, x21, [sp, #64]
   4ec58:	ldp	x24, x23, [sp, #48]
   4ec5c:	ldp	x26, x25, [sp, #32]
   4ec60:	ldp	x28, x27, [sp, #16]
   4ec64:	ldp	x29, x30, [sp], #96
   4ec68:	ret

000000000004ec6c <__gmpn_sbpi1_bdiv_qr@@Base>:
   4ec6c:	stp	x29, x30, [sp, #-96]!
   4ec70:	cmp	x2, x4
   4ec74:	stp	x28, x27, [sp, #16]
   4ec78:	stp	x26, x25, [sp, #32]
   4ec7c:	stp	x24, x23, [sp, #48]
   4ec80:	stp	x22, x21, [sp, #64]
   4ec84:	stp	x20, x19, [sp, #80]
   4ec88:	mov	x29, sp
   4ec8c:	b.eq	4ed04 <__gmpn_sbpi1_bdiv_qr@@Base+0x98>  // b.none
   4ec90:	mov	x19, x5
   4ec94:	mov	x20, x4
   4ec98:	mov	x21, x3
   4ec9c:	mov	x22, x2
   4eca0:	mov	x23, x1
   4eca4:	mov	x24, x0
   4eca8:	mov	x25, xzr
   4ecac:	lsl	x27, x4, #3
   4ecb0:	mov	w28, #0x2                   	// #2
   4ecb4:	ldr	x8, [x23]
   4ecb8:	mov	x0, x23
   4ecbc:	mov	x1, x21
   4ecc0:	mov	x2, x20
   4ecc4:	mul	x26, x8, x19
   4ecc8:	mov	x3, x26
   4eccc:	bl	d400 <__gmpn_addmul_1@plt>
   4ecd0:	str	x26, [x24], #8
   4ecd4:	ldr	x9, [x23, x27]
   4ecd8:	adds	x8, x0, x25
   4ecdc:	sub	x22, x22, #0x1
   4ece0:	cset	w10, cs  // cs = hs, nlast
   4ece4:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   4ece8:	adds	x8, x9, x8
   4ecec:	csel	x25, x10, x11, cc  // cc = lo, ul, last
   4ecf0:	str	x8, [x23, x27]
   4ecf4:	cmp	x20, x22
   4ecf8:	add	x23, x23, #0x8
   4ecfc:	b.ne	4ecb4 <__gmpn_sbpi1_bdiv_qr@@Base+0x48>  // b.any
   4ed00:	b	4ed08 <__gmpn_sbpi1_bdiv_qr@@Base+0x9c>
   4ed04:	mov	x25, xzr
   4ed08:	mov	x0, x25
   4ed0c:	ldp	x20, x19, [sp, #80]
   4ed10:	ldp	x22, x21, [sp, #64]
   4ed14:	ldp	x24, x23, [sp, #48]
   4ed18:	ldp	x26, x25, [sp, #32]
   4ed1c:	ldp	x28, x27, [sp, #16]
   4ed20:	ldp	x29, x30, [sp], #96
   4ed24:	ret

000000000004ed28 <__gmpn_sbpi1_bdiv_r@@Base>:
   4ed28:	stp	x29, x30, [sp, #-80]!
   4ed2c:	cmp	x1, x3
   4ed30:	stp	x26, x25, [sp, #16]
   4ed34:	stp	x24, x23, [sp, #32]
   4ed38:	stp	x22, x21, [sp, #48]
   4ed3c:	stp	x20, x19, [sp, #64]
   4ed40:	mov	x29, sp
   4ed44:	b.eq	4edb0 <__gmpn_sbpi1_bdiv_r@@Base+0x88>  // b.none
   4ed48:	mov	x19, x4
   4ed4c:	mov	x20, x3
   4ed50:	mov	x21, x2
   4ed54:	mov	x22, x1
   4ed58:	mov	x23, x0
   4ed5c:	mov	x24, xzr
   4ed60:	lsl	x25, x3, #3
   4ed64:	mov	w26, #0x2                   	// #2
   4ed68:	ldr	x8, [x23]
   4ed6c:	mov	x0, x23
   4ed70:	mov	x1, x21
   4ed74:	mov	x2, x20
   4ed78:	mul	x3, x8, x19
   4ed7c:	bl	d400 <__gmpn_addmul_1@plt>
   4ed80:	ldr	x9, [x23, x25]
   4ed84:	adds	x8, x0, x24
   4ed88:	sub	x22, x22, #0x1
   4ed8c:	cset	w10, cs  // cs = hs, nlast
   4ed90:	csinc	x11, x26, xzr, cs  // cs = hs, nlast
   4ed94:	adds	x8, x8, x9
   4ed98:	csel	x24, x10, x11, cc  // cc = lo, ul, last
   4ed9c:	str	x8, [x23, x25]
   4eda0:	cmp	x20, x22
   4eda4:	add	x23, x23, #0x8
   4eda8:	b.ne	4ed68 <__gmpn_sbpi1_bdiv_r@@Base+0x40>  // b.any
   4edac:	b	4edb4 <__gmpn_sbpi1_bdiv_r@@Base+0x8c>
   4edb0:	mov	x24, xzr
   4edb4:	mov	x0, x24
   4edb8:	ldp	x20, x19, [sp, #64]
   4edbc:	ldp	x22, x21, [sp, #48]
   4edc0:	ldp	x24, x23, [sp, #32]
   4edc4:	ldp	x26, x25, [sp, #16]
   4edc8:	ldp	x29, x30, [sp], #80
   4edcc:	ret

000000000004edd0 <__gmpn_dcpi1_bdiv_q@@Base>:
   4edd0:	stp	x29, x30, [sp, #-96]!
   4edd4:	stp	x28, x27, [sp, #16]
   4edd8:	stp	x26, x25, [sp, #32]
   4eddc:	stp	x24, x23, [sp, #48]
   4ede0:	stp	x22, x21, [sp, #64]
   4ede4:	stp	x20, x19, [sp, #80]
   4ede8:	mov	x29, sp
   4edec:	sub	sp, sp, #0x20
   4edf0:	lsl	x28, x4, #3
   4edf4:	add	x9, x28, #0xf
   4edf8:	mov	x8, sp
   4edfc:	and	x9, x9, #0xfffffffffffffff0
   4ee00:	mov	x25, x2
   4ee04:	mov	x24, x1
   4ee08:	sub	x21, x8, x9
   4ee0c:	stur	x3, [x29, #-8]
   4ee10:	mov	sp, x21
   4ee14:	cmp	x2, x4
   4ee18:	b.le	4eec0 <__gmpn_dcpi1_bdiv_q@@Base+0xf0>
   4ee1c:	lsl	x10, x25, #3
   4ee20:	sub	x11, x10, x28
   4ee24:	add	x9, x24, x10
   4ee28:	add	x11, x11, x21
   4ee2c:	mov	x22, x4
   4ee30:	neg	x8, x28
   4ee34:	add	x10, x9, #0x8
   4ee38:	add	x11, x11, #0x8
   4ee3c:	mov	x26, x25
   4ee40:	stur	x5, [x29, #-16]
   4ee44:	sub	x26, x26, x22
   4ee48:	mov	x19, x10
   4ee4c:	mov	x23, x9
   4ee50:	mov	x20, x11
   4ee54:	add	x10, x10, x8
   4ee58:	add	x9, x9, x8
   4ee5c:	cmp	x26, x22
   4ee60:	add	x11, x11, x8
   4ee64:	b.gt	4ee44 <__gmpn_dcpi1_bdiv_q@@Base+0x74>
   4ee68:	cmp	x26, #0x26
   4ee6c:	stur	x0, [x29, #-24]
   4ee70:	b.le	4eedc <__gmpn_dcpi1_bdiv_q@@Base+0x10c>
   4ee74:	ldp	x4, x2, [x29, #-16]
   4ee78:	mov	x1, x24
   4ee7c:	mov	x3, x26
   4ee80:	mov	x5, x21
   4ee84:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4ee88:	mov	x27, x0
   4ee8c:	cmp	x26, x22
   4ee90:	lsl	x11, x26, #3
   4ee94:	b.eq	4ef00 <__gmpn_dcpi1_bdiv_q@@Base+0x130>  // b.none
   4ee98:	ldur	x8, [x29, #-8]
   4ee9c:	sub	x4, x22, x26
   4eea0:	cmp	x26, x4
   4eea4:	mov	x0, x21
   4eea8:	add	x3, x8, x26, lsl #3
   4eeac:	stur	x11, [x29, #-32]
   4eeb0:	b.le	4ef34 <__gmpn_dcpi1_bdiv_q@@Base+0x164>
   4eeb4:	ldur	x1, [x29, #-24]
   4eeb8:	mov	x2, x26
   4eebc:	b	4ef44 <__gmpn_dcpi1_bdiv_q@@Base+0x174>
   4eec0:	mov	x1, x24
   4eec4:	cmp	x25, #0x5c
   4eec8:	b.le	4ef08 <__gmpn_dcpi1_bdiv_q@@Base+0x138>
   4eecc:	ldur	x2, [x29, #-8]
   4eed0:	mov	x3, x25
   4eed4:	mov	x4, x5
   4eed8:	b	4f010 <__gmpn_dcpi1_bdiv_q@@Base+0x240>
   4eedc:	ldp	x5, x3, [x29, #-16]
   4eee0:	lsl	x2, x26, #1
   4eee4:	mov	x1, x24
   4eee8:	mov	x4, x26
   4eeec:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4eef0:	mov	x27, x0
   4eef4:	cmp	x26, x22
   4eef8:	lsl	x11, x26, #3
   4eefc:	b.ne	4ee98 <__gmpn_dcpi1_bdiv_q@@Base+0xc8>  // b.any
   4ef00:	sub	x20, x25, x26
   4ef04:	b	4efc4 <__gmpn_dcpi1_bdiv_q@@Base+0x1f4>
   4ef08:	ldur	x3, [x29, #-8]
   4ef0c:	mov	x2, x25
   4ef10:	mov	x4, x25
   4ef14:	mov	sp, x29
   4ef18:	ldp	x20, x19, [sp, #80]
   4ef1c:	ldp	x22, x21, [sp, #64]
   4ef20:	ldp	x24, x23, [sp, #48]
   4ef24:	ldp	x26, x25, [sp, #32]
   4ef28:	ldp	x28, x27, [sp, #16]
   4ef2c:	ldp	x29, x30, [sp], #96
   4ef30:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>
   4ef34:	mov	x1, x3
   4ef38:	ldur	x3, [x29, #-24]
   4ef3c:	mov	x2, x4
   4ef40:	mov	x4, x26
   4ef44:	bl	ccd0 <__gmpn_mul@plt>
   4ef48:	ldur	x11, [x29, #-32]
   4ef4c:	ldr	x8, [x21, x11]
   4ef50:	adds	x8, x8, x27
   4ef54:	str	x8, [x21, x11]
   4ef58:	b.cc	4ef6c <__gmpn_dcpi1_bdiv_q@@Base+0x19c>  // b.lo, b.ul, b.last
   4ef5c:	ldr	x8, [x20]
   4ef60:	adds	x8, x8, #0x1
   4ef64:	str	x8, [x20], #8
   4ef68:	b.cs	4ef5c <__gmpn_dcpi1_bdiv_q@@Base+0x18c>  // b.hs, b.nlast
   4ef6c:	sub	x20, x25, x26
   4ef70:	cbz	x22, 4efc0 <__gmpn_dcpi1_bdiv_q@@Base+0x1f0>
   4ef74:	add	x25, x24, x26, lsl #3
   4ef78:	mov	x0, x25
   4ef7c:	mov	x1, x25
   4ef80:	mov	x2, x21
   4ef84:	mov	x3, x22
   4ef88:	bl	ca70 <__gmpn_add_n@plt>
   4ef8c:	cbz	x0, 4efe0 <__gmpn_dcpi1_bdiv_q@@Base+0x210>
   4ef90:	ldp	x10, x26, [x29, #-24]
   4ef94:	ldur	x11, [x29, #-32]
   4ef98:	mov	x8, x22
   4ef9c:	cmp	x8, x20
   4efa0:	b.ge	4efb8 <__gmpn_dcpi1_bdiv_q@@Base+0x1e8>  // b.tcont
   4efa4:	ldr	x9, [x25, x8, lsl #3]
   4efa8:	add	x8, x8, #0x1
   4efac:	adds	x9, x9, #0x1
   4efb0:	str	x9, [x23], #8
   4efb4:	b.cs	4ef9c <__gmpn_dcpi1_bdiv_q@@Base+0x1cc>  // b.hs, b.nlast
   4efb8:	ldur	x25, [x29, #-8]
   4efbc:	b	4efe8 <__gmpn_dcpi1_bdiv_q@@Base+0x218>
   4efc0:	mov	x27, xzr
   4efc4:	ldp	x26, x25, [x29, #-16]
   4efc8:	ldur	x10, [x29, #-24]
   4efcc:	add	x24, x24, x11
   4efd0:	cmp	x20, x22
   4efd4:	add	x23, x10, x11
   4efd8:	b.gt	4f064 <__gmpn_dcpi1_bdiv_q@@Base+0x294>
   4efdc:	b	4effc <__gmpn_dcpi1_bdiv_q@@Base+0x22c>
   4efe0:	ldp	x26, x25, [x29, #-16]
   4efe4:	ldp	x11, x10, [x29, #-32]
   4efe8:	mov	x27, xzr
   4efec:	add	x24, x24, x11
   4eff0:	cmp	x20, x22
   4eff4:	add	x23, x10, x11
   4eff8:	b.gt	4f064 <__gmpn_dcpi1_bdiv_q@@Base+0x294>
   4effc:	mov	x0, x23
   4f000:	mov	x1, x24
   4f004:	mov	x2, x25
   4f008:	mov	x3, x22
   4f00c:	mov	x4, x26
   4f010:	mov	x5, x21
   4f014:	bl	4f0a8 <__gmpn_dcpi1_bdiv_q@@Base+0x2d8>
   4f018:	mov	sp, x29
   4f01c:	ldp	x20, x19, [sp, #80]
   4f020:	ldp	x22, x21, [sp, #64]
   4f024:	ldp	x24, x23, [sp, #48]
   4f028:	ldp	x26, x25, [sp, #32]
   4f02c:	ldp	x28, x27, [sp, #16]
   4f030:	ldp	x29, x30, [sp], #96
   4f034:	ret
   4f038:	mov	x0, x23
   4f03c:	mov	x2, x25
   4f040:	mov	x3, x22
   4f044:	mov	x4, x26
   4f048:	mov	x5, x21
   4f04c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f050:	mov	x27, x0
   4f054:	add	x23, x23, x22, lsl #3
   4f058:	cmp	x20, x22
   4f05c:	add	x19, x19, x28
   4f060:	b.le	4effc <__gmpn_dcpi1_bdiv_q@@Base+0x22c>
   4f064:	mov	x1, x24
   4f068:	add	x24, x24, x22, lsl #3
   4f06c:	ldr	x8, [x24]
   4f070:	sub	x20, x20, x22
   4f074:	adds	x8, x8, x27
   4f078:	str	x8, [x24]
   4f07c:	b.cc	4f038 <__gmpn_dcpi1_bdiv_q@@Base+0x268>  // b.lo, b.ul, b.last
   4f080:	mov	x8, xzr
   4f084:	add	x9, x8, #0x1
   4f088:	cmp	x9, x20
   4f08c:	b.ge	4f038 <__gmpn_dcpi1_bdiv_q@@Base+0x268>  // b.tcont
   4f090:	ldr	x10, [x24, x9, lsl #3]
   4f094:	adds	x10, x10, #0x1
   4f098:	str	x10, [x19, x8, lsl #3]
   4f09c:	mov	x8, x9
   4f0a0:	b.cs	4f084 <__gmpn_dcpi1_bdiv_q@@Base+0x2b4>  // b.hs, b.nlast
   4f0a4:	b	4f038 <__gmpn_dcpi1_bdiv_q@@Base+0x268>
   4f0a8:	stp	x29, x30, [sp, #-96]!
   4f0ac:	stp	x26, x25, [sp, #32]
   4f0b0:	stp	x22, x21, [sp, #64]
   4f0b4:	stp	x20, x19, [sp, #80]
   4f0b8:	mov	x19, x4
   4f0bc:	mov	x25, x3
   4f0c0:	mov	x20, x2
   4f0c4:	mov	x21, x1
   4f0c8:	cmp	x3, #0x5d
   4f0cc:	mov	x22, x0
   4f0d0:	stp	x28, x27, [sp, #16]
   4f0d4:	stp	x24, x23, [sp, #48]
   4f0d8:	mov	x29, sp
   4f0dc:	b.lt	4f188 <__gmpn_dcpi1_bdiv_q@@Base+0x3b8>  // b.tstop
   4f0e0:	mov	x23, x5
   4f0e4:	b	4f0fc <__gmpn_dcpi1_bdiv_q@@Base+0x32c>
   4f0e8:	add	x22, x22, x28
   4f0ec:	cmp	x24, #0x5c
   4f0f0:	add	x21, x21, x28
   4f0f4:	mov	x25, x24
   4f0f8:	b.le	4f18c <__gmpn_dcpi1_bdiv_q@@Base+0x3bc>
   4f0fc:	lsr	x26, x25, #1
   4f100:	mov	x0, x22
   4f104:	mov	x1, x21
   4f108:	mov	x2, x20
   4f10c:	mov	x3, x26
   4f110:	mov	x4, x19
   4f114:	mov	x5, x23
   4f118:	sub	x24, x25, x26
   4f11c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f120:	lsl	x28, x24, #3
   4f124:	mov	x27, x0
   4f128:	add	x2, x20, x28
   4f12c:	mov	x0, x23
   4f130:	mov	x1, x22
   4f134:	mov	x3, x26
   4f138:	bl	cec0 <__gmpn_mullo_n@plt>
   4f13c:	add	x0, x21, x28
   4f140:	mov	x1, x0
   4f144:	mov	x2, x23
   4f148:	mov	x3, x26
   4f14c:	bl	ca70 <__gmpn_add_n@plt>
   4f150:	cmp	x26, x24
   4f154:	lsl	x28, x26, #3
   4f158:	b.ge	4f0e8 <__gmpn_dcpi1_bdiv_q@@Base+0x318>  // b.tcont
   4f15c:	ldr	x3, [x20, x28]
   4f160:	add	x0, x21, x28
   4f164:	mov	x1, x22
   4f168:	mov	x2, x26
   4f16c:	bl	d400 <__gmpn_addmul_1@plt>
   4f170:	add	x8, x21, x25, lsl #3
   4f174:	ldur	x9, [x8, #-8]
   4f178:	add	x10, x0, x27
   4f17c:	add	x9, x10, x9
   4f180:	stur	x9, [x8, #-8]
   4f184:	b	4f0e8 <__gmpn_dcpi1_bdiv_q@@Base+0x318>
   4f188:	mov	x24, x25
   4f18c:	mov	x0, x22
   4f190:	mov	x1, x21
   4f194:	mov	x2, x24
   4f198:	mov	x3, x20
   4f19c:	mov	x4, x24
   4f1a0:	mov	x5, x19
   4f1a4:	ldp	x20, x19, [sp, #80]
   4f1a8:	ldp	x22, x21, [sp, #64]
   4f1ac:	ldp	x24, x23, [sp, #48]
   4f1b0:	ldp	x26, x25, [sp, #32]
   4f1b4:	ldp	x28, x27, [sp, #16]
   4f1b8:	ldp	x29, x30, [sp], #96
   4f1bc:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>

000000000004f1c0 <__gmpn_dcpi1_bdiv_qr_n_itch@@Base>:
   4f1c0:	ret

000000000004f1c4 <__gmpn_dcpi1_bdiv_qr_n@@Base>:
   4f1c4:	stp	x29, x30, [sp, #-96]!
   4f1c8:	stp	x24, x23, [sp, #48]
   4f1cc:	asr	x23, x3, #1
   4f1d0:	stp	x26, x25, [sp, #32]
   4f1d4:	stp	x22, x21, [sp, #64]
   4f1d8:	stp	x20, x19, [sp, #80]
   4f1dc:	mov	x19, x5
   4f1e0:	mov	x25, x4
   4f1e4:	mov	x20, x3
   4f1e8:	mov	x24, x2
   4f1ec:	mov	x21, x1
   4f1f0:	mov	x26, x0
   4f1f4:	cmp	x3, #0x4d
   4f1f8:	sub	x22, x3, x23
   4f1fc:	stp	x28, x27, [sp, #16]
   4f200:	mov	x29, sp
   4f204:	b.le	4f228 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x64>
   4f208:	mov	x0, x26
   4f20c:	mov	x1, x21
   4f210:	mov	x2, x24
   4f214:	mov	x3, x23
   4f218:	mov	x4, x25
   4f21c:	mov	x5, x19
   4f220:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f224:	b	4f244 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x80>
   4f228:	and	x2, x20, #0xfffffffffffffffe
   4f22c:	mov	x0, x26
   4f230:	mov	x1, x21
   4f234:	mov	x3, x24
   4f238:	mov	x4, x23
   4f23c:	mov	x5, x25
   4f240:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4f244:	lsl	x28, x23, #3
   4f248:	mov	x27, x0
   4f24c:	add	x1, x24, x28
   4f250:	mov	x0, x19
   4f254:	mov	x2, x22
   4f258:	mov	x3, x26
   4f25c:	mov	x4, x23
   4f260:	bl	ccd0 <__gmpn_mul@plt>
   4f264:	ldr	x8, [x19, x28]
   4f268:	adds	x8, x8, x27
   4f26c:	str	x8, [x19, x28]
   4f270:	b.cc	4f28c <__gmpn_dcpi1_bdiv_qr_n@@Base+0xc8>  // b.lo, b.ul, b.last
   4f274:	add	x8, x19, x23, lsl #3
   4f278:	add	x8, x8, #0x8
   4f27c:	ldr	x9, [x8]
   4f280:	adds	x9, x9, #0x1
   4f284:	str	x9, [x8], #8
   4f288:	b.cs	4f27c <__gmpn_dcpi1_bdiv_qr_n@@Base+0xb8>  // b.hs, b.nlast
   4f28c:	add	x27, x21, x23, lsl #3
   4f290:	cbz	x20, 4f2d8 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x114>
   4f294:	mov	x0, x27
   4f298:	mov	x1, x27
   4f29c:	mov	x2, x19
   4f2a0:	mov	x3, x20
   4f2a4:	bl	ca70 <__gmpn_add_n@plt>
   4f2a8:	cbz	x0, 4f2d8 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x114>
   4f2ac:	add	x8, x22, x20
   4f2b0:	mov	w28, #0x1                   	// #1
   4f2b4:	mov	x9, x20
   4f2b8:	cmp	x9, x8
   4f2bc:	b.ge	4f2dc <__gmpn_dcpi1_bdiv_qr_n@@Base+0x118>  // b.tcont
   4f2c0:	lsl	x10, x9, #3
   4f2c4:	ldr	x11, [x27, x10]
   4f2c8:	add	x9, x9, #0x1
   4f2cc:	adds	x11, x11, #0x1
   4f2d0:	str	x11, [x27, x10]
   4f2d4:	b.cs	4f2b8 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xf4>  // b.hs, b.nlast
   4f2d8:	mov	x28, xzr
   4f2dc:	cmp	x22, #0x26
   4f2e0:	add	x26, x26, x23, lsl #3
   4f2e4:	b.le	4f308 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x144>
   4f2e8:	mov	x0, x26
   4f2ec:	mov	x1, x27
   4f2f0:	mov	x2, x24
   4f2f4:	mov	x3, x22
   4f2f8:	mov	x4, x25
   4f2fc:	mov	x5, x19
   4f300:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f304:	b	4f324 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x160>
   4f308:	lsl	x2, x22, #1
   4f30c:	mov	x0, x26
   4f310:	mov	x1, x27
   4f314:	mov	x3, x24
   4f318:	mov	x4, x22
   4f31c:	mov	x5, x25
   4f320:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4f324:	lsl	x27, x22, #3
   4f328:	mov	x25, x0
   4f32c:	add	x3, x24, x27
   4f330:	mov	x0, x19
   4f334:	mov	x1, x26
   4f338:	mov	x2, x22
   4f33c:	mov	x4, x23
   4f340:	bl	ccd0 <__gmpn_mul@plt>
   4f344:	ldr	x8, [x19, x27]
   4f348:	adds	x8, x8, x25
   4f34c:	str	x8, [x19, x27]
   4f350:	b.cc	4f36c <__gmpn_dcpi1_bdiv_qr_n@@Base+0x1a8>  // b.lo, b.ul, b.last
   4f354:	add	x8, x19, x22, lsl #3
   4f358:	add	x8, x8, #0x8
   4f35c:	ldr	x9, [x8]
   4f360:	adds	x9, x9, #0x1
   4f364:	str	x9, [x8], #8
   4f368:	b.cs	4f35c <__gmpn_dcpi1_bdiv_qr_n@@Base+0x198>  // b.hs, b.nlast
   4f36c:	add	x0, x21, x20, lsl #3
   4f370:	mov	x1, x0
   4f374:	mov	x2, x19
   4f378:	mov	x3, x20
   4f37c:	bl	ca70 <__gmpn_add_n@plt>
   4f380:	add	x0, x0, x28
   4f384:	ldp	x20, x19, [sp, #80]
   4f388:	ldp	x22, x21, [sp, #64]
   4f38c:	ldp	x24, x23, [sp, #48]
   4f390:	ldp	x26, x25, [sp, #32]
   4f394:	ldp	x28, x27, [sp, #16]
   4f398:	ldp	x29, x30, [sp], #96
   4f39c:	ret

000000000004f3a0 <__gmpn_dcpi1_bdiv_qr@@Base>:
   4f3a0:	stp	x29, x30, [sp, #-96]!
   4f3a4:	stp	x28, x27, [sp, #16]
   4f3a8:	stp	x26, x25, [sp, #32]
   4f3ac:	stp	x24, x23, [sp, #48]
   4f3b0:	stp	x22, x21, [sp, #64]
   4f3b4:	stp	x20, x19, [sp, #80]
   4f3b8:	mov	x29, sp
   4f3bc:	sub	sp, sp, #0x30
   4f3c0:	lsl	x28, x4, #3
   4f3c4:	add	x9, x28, #0xf
   4f3c8:	mov	x8, sp
   4f3cc:	and	x9, x9, #0xfffffffffffffff0
   4f3d0:	mov	x19, x4
   4f3d4:	mov	x21, x3
   4f3d8:	mov	x24, x2
   4f3dc:	sub	x20, x8, x9
   4f3e0:	stur	x5, [x29, #-8]
   4f3e4:	mov	sp, x20
   4f3e8:	sub	x25, x2, x4
   4f3ec:	cmp	x25, x4
   4f3f0:	b.le	4f47c <__gmpn_dcpi1_bdiv_qr@@Base+0xdc>
   4f3f4:	mov	x12, x21
   4f3f8:	lsl	x11, x24, #1
   4f3fc:	lsl	x10, x19, #1
   4f400:	mov	x26, xzr
   4f404:	neg	x8, x28
   4f408:	lsl	x13, x24, #3
   4f40c:	stp	x20, x12, [x29, #-24]
   4f410:	sub	x21, x20, x28
   4f414:	neg	x9, x1
   4f418:	sub	x22, x28, x12
   4f41c:	sub	x20, x28, x1
   4f420:	sub	x2, x11, x10
   4f424:	sub	x23, x28, x0
   4f428:	sub	x26, x26, x19
   4f42c:	add	x27, x25, x26
   4f430:	add	x21, x21, x8
   4f434:	add	x9, x9, x28
   4f438:	sub	x2, x2, x10
   4f43c:	add	x22, x22, x28
   4f440:	add	x20, x20, x28
   4f444:	cmp	x27, x19
   4f448:	add	x23, x23, x28
   4f44c:	b.gt	4f428 <__gmpn_dcpi1_bdiv_qr@@Base+0x88>
   4f450:	sub	x25, x13, x9
   4f454:	cmp	x27, #0x26
   4f458:	sub	x8, x10, x24
   4f45c:	stp	x0, x13, [x29, #-40]
   4f460:	b.le	4f4cc <__gmpn_dcpi1_bdiv_qr@@Base+0x12c>
   4f464:	ldp	x2, x4, [x29, #-16]
   4f468:	ldur	x5, [x29, #-24]
   4f46c:	mov	x3, x27
   4f470:	mov	x24, x8
   4f474:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f478:	b	4f4dc <__gmpn_dcpi1_bdiv_qr@@Base+0x13c>
   4f47c:	cmp	x25, #0x26
   4f480:	b.le	4f4fc <__gmpn_dcpi1_bdiv_qr@@Base+0x15c>
   4f484:	ldur	x4, [x29, #-8]
   4f488:	mov	x2, x21
   4f48c:	mov	x3, x25
   4f490:	mov	x5, x20
   4f494:	mov	x23, x0
   4f498:	mov	x26, x1
   4f49c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f4a0:	mov	x22, x0
   4f4a4:	cmp	x25, x19
   4f4a8:	b.eq	4f524 <__gmpn_dcpi1_bdiv_qr@@Base+0x184>  // b.none
   4f4ac:	sub	x4, x19, x25
   4f4b0:	cmp	x25, x4
   4f4b4:	add	x3, x21, x25, lsl #3
   4f4b8:	mov	x0, x20
   4f4bc:	b.le	4f548 <__gmpn_dcpi1_bdiv_qr@@Base+0x1a8>
   4f4c0:	mov	x1, x23
   4f4c4:	mov	x2, x25
   4f4c8:	b	4f558 <__gmpn_dcpi1_bdiv_qr@@Base+0x1b8>
   4f4cc:	ldp	x3, x5, [x29, #-16]
   4f4d0:	mov	x4, x27
   4f4d4:	mov	x24, x8
   4f4d8:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4f4dc:	ldur	x12, [x29, #-32]
   4f4e0:	mov	x8, x24
   4f4e4:	mov	x24, x0
   4f4e8:	subs	x4, x8, x26
   4f4ec:	b.ne	4f52c <__gmpn_dcpi1_bdiv_qr@@Base+0x18c>  // b.any
   4f4f0:	ldur	x27, [x29, #-24]
   4f4f4:	mov	x21, xzr
   4f4f8:	b	4f66c <__gmpn_dcpi1_bdiv_qr@@Base+0x2cc>
   4f4fc:	ldur	x5, [x29, #-8]
   4f500:	lsl	x2, x25, #1
   4f504:	mov	x3, x21
   4f508:	mov	x4, x25
   4f50c:	mov	x23, x0
   4f510:	mov	x26, x1
   4f514:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4f518:	mov	x22, x0
   4f51c:	cmp	x25, x19
   4f520:	b.ne	4f4ac <__gmpn_dcpi1_bdiv_qr@@Base+0x10c>  // b.any
   4f524:	mov	x8, xzr
   4f528:	b	4f658 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b8>
   4f52c:	cmp	x27, x4
   4f530:	sub	x3, x12, x22
   4f534:	b.le	4f5b8 <__gmpn_dcpi1_bdiv_qr@@Base+0x218>
   4f538:	ldur	x0, [x29, #-24]
   4f53c:	ldur	x1, [x29, #-40]
   4f540:	mov	x2, x27
   4f544:	b	4f5cc <__gmpn_dcpi1_bdiv_qr@@Base+0x22c>
   4f548:	mov	x1, x3
   4f54c:	mov	x2, x4
   4f550:	mov	x3, x23
   4f554:	mov	x4, x25
   4f558:	bl	ccd0 <__gmpn_mul@plt>
   4f55c:	lsl	x8, x25, #3
   4f560:	ldr	x9, [x20, x8]
   4f564:	adds	x9, x9, x22
   4f568:	str	x9, [x20, x8]
   4f56c:	b.cc	4f590 <__gmpn_dcpi1_bdiv_qr@@Base+0x1f0>  // b.lo, b.ul, b.last
   4f570:	lsl	x8, x24, #3
   4f574:	sub	x8, x8, x19, lsl #3
   4f578:	add	x8, x8, x20
   4f57c:	add	x8, x8, #0x8
   4f580:	ldr	x9, [x8]
   4f584:	adds	x9, x9, #0x1
   4f588:	str	x9, [x8], #8
   4f58c:	b.cs	4f580 <__gmpn_dcpi1_bdiv_qr@@Base+0x1e0>  // b.hs, b.nlast
   4f590:	cbz	x19, 4f650 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b0>
   4f594:	add	x0, x26, x25, lsl #3
   4f598:	mov	x1, x0
   4f59c:	mov	x2, x20
   4f5a0:	mov	x3, x19
   4f5a4:	bl	ca70 <__gmpn_add_n@plt>
   4f5a8:	cmp	x0, #0x0
   4f5ac:	mov	x22, xzr
   4f5b0:	cset	w8, ne  // ne = any
   4f5b4:	b	4f658 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b8>
   4f5b8:	ldur	x0, [x29, #-24]
   4f5bc:	mov	x1, x3
   4f5c0:	ldur	x3, [x29, #-40]
   4f5c4:	mov	x2, x4
   4f5c8:	mov	x4, x27
   4f5cc:	bl	ccd0 <__gmpn_mul@plt>
   4f5d0:	ldur	x12, [x29, #-32]
   4f5d4:	ldr	x8, [x21, x12]
   4f5d8:	adds	x8, x8, x24
   4f5dc:	str	x8, [x21, x12]
   4f5e0:	b.cc	4f5fc <__gmpn_dcpi1_bdiv_qr@@Base+0x25c>  // b.lo, b.ul, b.last
   4f5e4:	add	x8, x21, x12
   4f5e8:	add	x8, x8, #0x8
   4f5ec:	ldr	x9, [x8]
   4f5f0:	adds	x9, x9, #0x1
   4f5f4:	str	x9, [x8], #8
   4f5f8:	b.cs	4f5ec <__gmpn_dcpi1_bdiv_qr@@Base+0x24c>  // b.hs, b.nlast
   4f5fc:	ldur	x27, [x29, #-24]
   4f600:	cbz	x19, 4f664 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c4>
   4f604:	sub	x0, x12, x20
   4f608:	mov	x1, x0
   4f60c:	mov	x2, x27
   4f610:	mov	x3, x19
   4f614:	bl	ca70 <__gmpn_add_n@plt>
   4f618:	cbz	x0, 4f660 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c0>
   4f61c:	ldur	x12, [x29, #-32]
   4f620:	sub	x8, x19, x26
   4f624:	mov	w21, #0x1                   	// #1
   4f628:	mov	x9, x25
   4f62c:	mov	x10, x19
   4f630:	cmp	x10, x8
   4f634:	b.ge	4f668 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c8>  // b.tcont
   4f638:	ldr	x11, [x9]
   4f63c:	add	x10, x10, #0x1
   4f640:	adds	x11, x11, #0x1
   4f644:	str	x11, [x9], #8
   4f648:	b.cs	4f630 <__gmpn_dcpi1_bdiv_qr@@Base+0x290>  // b.hs, b.nlast
   4f64c:	b	4f664 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c4>
   4f650:	mov	x8, xzr
   4f654:	mov	x22, xzr
   4f658:	add	x0, x22, x8
   4f65c:	b	4f704 <__gmpn_dcpi1_bdiv_qr@@Base+0x364>
   4f660:	ldur	x12, [x29, #-32]
   4f664:	mov	x21, xzr
   4f668:	mov	x24, xzr
   4f66c:	sub	x1, x12, x20
   4f670:	neg	x20, x26
   4f674:	ldur	x26, [x29, #-16]
   4f678:	sub	x23, x12, x23
   4f67c:	b	4f6bc <__gmpn_dcpi1_bdiv_qr@@Base+0x31c>
   4f680:	mov	x8, xzr
   4f684:	ldur	x4, [x29, #-8]
   4f688:	mov	x0, x23
   4f68c:	mov	x2, x26
   4f690:	mov	x3, x19
   4f694:	mov	x5, x27
   4f698:	add	x21, x8, x21
   4f69c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4f6a0:	sub	x20, x20, x19
   4f6a4:	mov	x24, x0
   4f6a8:	add	x23, x23, x19, lsl #3
   4f6ac:	cmp	x20, #0x0
   4f6b0:	add	x25, x25, x28
   4f6b4:	mov	x1, x22
   4f6b8:	b.le	4f700 <__gmpn_dcpi1_bdiv_qr@@Base+0x360>
   4f6bc:	add	x22, x1, x19, lsl #3
   4f6c0:	ldr	x8, [x22]
   4f6c4:	adds	x8, x8, x24
   4f6c8:	str	x8, [x22]
   4f6cc:	b.cc	4f680 <__gmpn_dcpi1_bdiv_qr@@Base+0x2e0>  // b.lo, b.ul, b.last
   4f6d0:	mov	w8, #0x1                   	// #1
   4f6d4:	cmp	x8, x20
   4f6d8:	b.ge	4f6f8 <__gmpn_dcpi1_bdiv_qr@@Base+0x358>  // b.tcont
   4f6dc:	lsl	x9, x8, #3
   4f6e0:	ldr	x10, [x25, x9]
   4f6e4:	add	x8, x8, #0x1
   4f6e8:	adds	x10, x10, #0x1
   4f6ec:	str	x10, [x25, x9]
   4f6f0:	b.cs	4f6d4 <__gmpn_dcpi1_bdiv_qr@@Base+0x334>  // b.hs, b.nlast
   4f6f4:	b	4f680 <__gmpn_dcpi1_bdiv_qr@@Base+0x2e0>
   4f6f8:	mov	w8, #0x1                   	// #1
   4f6fc:	b	4f684 <__gmpn_dcpi1_bdiv_qr@@Base+0x2e4>
   4f700:	add	x0, x21, x24
   4f704:	mov	sp, x29
   4f708:	ldp	x20, x19, [sp, #80]
   4f70c:	ldp	x22, x21, [sp, #64]
   4f710:	ldp	x24, x23, [sp, #48]
   4f714:	ldp	x26, x25, [sp, #32]
   4f718:	ldp	x28, x27, [sp, #16]
   4f71c:	ldp	x29, x30, [sp], #96
   4f720:	ret

000000000004f724 <__gmpn_mu_bdiv_q@@Base>:
   4f724:	sub	sp, sp, #0xf0
   4f728:	stp	x28, x27, [sp, #160]
   4f72c:	stp	x26, x25, [sp, #176]
   4f730:	stp	x22, x21, [sp, #208]
   4f734:	stp	x20, x19, [sp, #224]
   4f738:	mov	x27, x5
   4f73c:	mov	x26, x3
   4f740:	mov	x20, x2
   4f744:	mov	x22, x1
   4f748:	cmp	x2, x4
   4f74c:	mov	x19, x0
   4f750:	stp	x29, x30, [sp, #144]
   4f754:	stp	x24, x23, [sp, #192]
   4f758:	add	x29, sp, #0x90
   4f75c:	b.le	4f970 <__gmpn_mu_bdiv_q@@Base+0x24c>
   4f760:	sub	x8, x20, #0x1
   4f764:	sdiv	x9, x8, x4
   4f768:	add	x9, x9, #0x1
   4f76c:	sdiv	x28, x8, x9
   4f770:	add	x25, x28, #0x1
   4f774:	mov	x1, x26
   4f778:	add	x26, x27, x25, lsl #3
   4f77c:	mov	x0, x27
   4f780:	mov	x2, x25
   4f784:	mov	x3, x26
   4f788:	mov	x24, x4
   4f78c:	stur	x1, [x29, #-16]
   4f790:	bl	cd20 <__gmpn_binvert@plt>
   4f794:	mov	x0, x26
   4f798:	mov	x1, x22
   4f79c:	mov	x2, x24
   4f7a0:	bl	ca50 <__gmpn_copyi@plt>
   4f7a4:	mov	x2, x27
   4f7a8:	mov	x0, x19
   4f7ac:	mov	x1, x26
   4f7b0:	mov	x3, x25
   4f7b4:	add	x27, x22, x24, lsl #3
   4f7b8:	stur	x2, [x29, #-8]
   4f7bc:	bl	cec0 <__gmpn_mullo_n@plt>
   4f7c0:	sub	x23, x20, x25
   4f7c4:	cmp	x23, x25
   4f7c8:	lsl	x10, x24, #3
   4f7cc:	str	x20, [sp, #8]
   4f7d0:	stur	x26, [x29, #-40]
   4f7d4:	str	x28, [sp, #32]
   4f7d8:	str	x10, [sp, #16]
   4f7dc:	b.le	4fa34 <__gmpn_mu_bdiv_q@@Base+0x310>
   4f7e0:	add	x11, x26, x10
   4f7e4:	lsl	x8, x25, #3
   4f7e8:	sub	x9, x24, x25
   4f7ec:	stur	x9, [x29, #-56]
   4f7f0:	mvn	x9, x28
   4f7f4:	add	x12, x26, x8
   4f7f8:	add	x8, x11, x8
   4f7fc:	cmp	x28, #0x10
   4f800:	stur	x12, [x29, #-64]
   4f804:	mov	x26, x11
   4f808:	str	x8, [sp, #72]
   4f80c:	add	x8, x11, x10
   4f810:	add	x9, x11, x9, lsl #3
   4f814:	stp	x9, x8, [x29, #-32]
   4f818:	b.le	4fbbc <__gmpn_mu_bdiv_q@@Base+0x498>
   4f81c:	mov	x0, x24
   4f820:	add	x21, x25, x24
   4f824:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4f828:	sub	x11, x21, x0
   4f82c:	mov	x10, x0
   4f830:	cmp	x11, #0x0
   4f834:	add	x8, x26, x0, lsl #3
   4f838:	stp	x8, x0, [sp, #56]
   4f83c:	b.le	4fdc0 <__gmpn_mu_bdiv_q@@Base+0x69c>
   4f840:	ldur	x9, [x29, #-8]
   4f844:	add	x8, x28, x24
   4f848:	lsl	x8, x8, #1
   4f84c:	sub	x8, x8, x10
   4f850:	add	x8, x9, x8, lsl #3
   4f854:	add	x8, x8, #0x18
   4f858:	stur	x26, [x29, #-48]
   4f85c:	str	x8, [sp, #24]
   4f860:	lsl	x8, x11, #3
   4f864:	stp	x8, x11, [sp, #40]
   4f868:	ldp	x22, x26, [x29, #-48]
   4f86c:	mov	w21, wzr
   4f870:	mov	x28, x19
   4f874:	b	4f8bc <__gmpn_mu_bdiv_q@@Base+0x198>
   4f878:	ldp	x0, x2, [x29, #-32]
   4f87c:	sxtw	x4, w21
   4f880:	mov	x1, x27
   4f884:	mov	x3, x25
   4f888:	bl	c760 <__gmpn_sub_nc@plt>
   4f88c:	ldur	x26, [x29, #-40]
   4f890:	ldur	x2, [x29, #-8]
   4f894:	mov	x21, x0
   4f898:	mov	x0, x28
   4f89c:	mov	x1, x26
   4f8a0:	mov	x3, x25
   4f8a4:	add	x27, x27, x25, lsl #3
   4f8a8:	bl	cec0 <__gmpn_mullo_n@plt>
   4f8ac:	ldr	x10, [sp, #64]
   4f8b0:	sub	x23, x23, x25
   4f8b4:	cmp	x23, x25
   4f8b8:	b.le	4fa3c <__gmpn_mu_bdiv_q@@Base+0x318>
   4f8bc:	mov	x20, x24
   4f8c0:	mov	x3, x24
   4f8c4:	ldr	x24, [sp, #56]
   4f8c8:	ldur	x2, [x29, #-16]
   4f8cc:	mov	x0, x22
   4f8d0:	mov	x1, x10
   4f8d4:	mov	x4, x28
   4f8d8:	mov	x5, x25
   4f8dc:	mov	x6, x24
   4f8e0:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4f8e4:	ldr	x3, [sp, #48]
   4f8e8:	mov	x0, x24
   4f8ec:	mov	x1, x22
   4f8f0:	mov	x2, x26
   4f8f4:	bl	c2d0 <__gmpn_sub_n@plt>
   4f8f8:	ldr	x10, [sp, #40]
   4f8fc:	sxtw	x9, w0
   4f900:	ldr	x8, [x22, x10]
   4f904:	subs	x8, x8, x9
   4f908:	str	x8, [x22, x10]
   4f90c:	b.cs	4f924 <__gmpn_mu_bdiv_q@@Base+0x200>  // b.hs, b.nlast
   4f910:	ldr	x8, [sp, #24]
   4f914:	ldr	x9, [x8]
   4f918:	sub	x10, x9, #0x1
   4f91c:	str	x10, [x8], #8
   4f920:	cbz	x9, 4f914 <__gmpn_mu_bdiv_q@@Base+0x1f0>
   4f924:	mov	x24, x20
   4f928:	cmp	x25, x20
   4f92c:	add	x28, x28, x25, lsl #3
   4f930:	b.eq	4f878 <__gmpn_mu_bdiv_q@@Base+0x154>  // b.none
   4f934:	ldur	x0, [x29, #-40]
   4f938:	ldp	x1, x3, [x29, #-64]
   4f93c:	ldr	x2, [sp, #72]
   4f940:	bl	c2d0 <__gmpn_sub_n@plt>
   4f944:	add	w21, w21, w0
   4f948:	ldp	x0, x2, [x29, #-32]
   4f94c:	cmp	w21, #0x2
   4f950:	b.ne	4f87c <__gmpn_mu_bdiv_q@@Base+0x158>  // b.any
   4f954:	mov	x8, x2
   4f958:	ldr	x9, [x8]
   4f95c:	adds	x9, x9, #0x1
   4f960:	str	x9, [x8], #8
   4f964:	b.cs	4f958 <__gmpn_mu_bdiv_q@@Base+0x234>  // b.hs, b.nlast
   4f968:	mov	w21, #0x1                   	// #1
   4f96c:	b	4f87c <__gmpn_mu_bdiv_q@@Base+0x158>
   4f970:	asr	x21, x20, #1
   4f974:	sub	x24, x20, x21
   4f978:	add	x23, x27, x24, lsl #3
   4f97c:	mov	x0, x27
   4f980:	mov	x1, x26
   4f984:	mov	x2, x24
   4f988:	mov	x3, x23
   4f98c:	bl	cd20 <__gmpn_binvert@plt>
   4f990:	mov	x0, x19
   4f994:	mov	x1, x22
   4f998:	mov	x2, x27
   4f99c:	mov	x3, x24
   4f9a0:	bl	cec0 <__gmpn_mullo_n@plt>
   4f9a4:	cmp	x24, #0x11
   4f9a8:	b.le	4faec <__gmpn_mu_bdiv_q@@Base+0x3c8>
   4f9ac:	mov	x0, x20
   4f9b0:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4f9b4:	mov	x25, x0
   4f9b8:	add	x6, x23, x0, lsl #3
   4f9bc:	mov	x0, x23
   4f9c0:	mov	x1, x25
   4f9c4:	mov	x2, x26
   4f9c8:	mov	x3, x20
   4f9cc:	mov	x4, x19
   4f9d0:	mov	x5, x24
   4f9d4:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4f9d8:	add	x8, x24, x20
   4f9dc:	sub	x8, x8, x25
   4f9e0:	cmp	x8, #0x1
   4f9e4:	b.lt	4fc80 <__gmpn_mu_bdiv_q@@Base+0x55c>  // b.tstop
   4f9e8:	lsl	x9, x20, #1
   4f9ec:	add	x10, x25, x21
   4f9f0:	sub	x10, x9, x10
   4f9f4:	add	x9, x9, x20
   4f9f8:	add	x10, x22, x10, lsl #3
   4f9fc:	sub	x11, x9, x25
   4fa00:	sub	x9, x10, #0x8
   4fa04:	sub	x10, x11, x21, lsl #1
   4fa08:	add	x10, x27, x10, lsl #3
   4fa0c:	sub	x10, x10, #0x8
   4fa10:	mov	x11, x8
   4fa14:	subs	x11, x11, #0x1
   4fa18:	b.lt	4fc44 <__gmpn_mu_bdiv_q@@Base+0x520>  // b.tstop
   4fa1c:	ldr	x12, [x10], #-8
   4fa20:	ldr	x13, [x9], #-8
   4fa24:	cmp	x12, x13
   4fa28:	b.eq	4fa14 <__gmpn_mu_bdiv_q@@Base+0x2f0>  // b.none
   4fa2c:	cset	w9, ls  // ls = plast
   4fa30:	b	4fc48 <__gmpn_mu_bdiv_q@@Base+0x524>
   4fa34:	mov	w21, wzr
   4fa38:	mov	x28, x19
   4fa3c:	mov	x22, x26
   4fa40:	ldr	x8, [sp, #32]
   4fa44:	cmp	x8, #0x10
   4fa48:	b.le	4fb08 <__gmpn_mu_bdiv_q@@Base+0x3e4>
   4fa4c:	mov	x0, x24
   4fa50:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4fa54:	ldur	x2, [x29, #-16]
   4fa58:	mov	x20, x0
   4fa5c:	add	x26, x22, x24, lsl #3
   4fa60:	add	x6, x26, x0, lsl #3
   4fa64:	mov	x0, x26
   4fa68:	mov	x1, x20
   4fa6c:	mov	x3, x24
   4fa70:	mov	x4, x28
   4fa74:	mov	x5, x25
   4fa78:	stur	x6, [x29, #-16]
   4fa7c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4fa80:	add	x8, x25, x24
   4fa84:	sub	x22, x8, x20
   4fa88:	cmp	x22, #0x1
   4fa8c:	b.lt	4fb20 <__gmpn_mu_bdiv_q@@Base+0x3fc>  // b.tstop
   4fa90:	ldur	x0, [x29, #-16]
   4fa94:	ldur	x2, [x29, #-40]
   4fa98:	mov	x1, x26
   4fa9c:	mov	x3, x22
   4faa0:	bl	c2d0 <__gmpn_sub_n@plt>
   4faa4:	lsl	x8, x22, #3
   4faa8:	ldr	x9, [x26, x8]
   4faac:	sxtw	x10, w0
   4fab0:	subs	x9, x9, x10
   4fab4:	str	x9, [x26, x8]
   4fab8:	b.cs	4fb20 <__gmpn_mu_bdiv_q@@Base+0x3fc>  // b.hs, b.nlast
   4fabc:	ldr	x8, [sp, #32]
   4fac0:	ldur	x9, [x29, #-8]
   4fac4:	add	x8, x8, x24
   4fac8:	lsl	x8, x8, #1
   4facc:	sub	x8, x8, x20
   4fad0:	add	x8, x9, x8, lsl #3
   4fad4:	add	x8, x8, #0x18
   4fad8:	ldr	x9, [x8]
   4fadc:	sub	x10, x9, #0x1
   4fae0:	str	x10, [x8], #8
   4fae4:	cbz	x9, 4fad8 <__gmpn_mu_bdiv_q@@Base+0x3b4>
   4fae8:	b	4fb20 <__gmpn_mu_bdiv_q@@Base+0x3fc>
   4faec:	mov	x0, x23
   4faf0:	mov	x1, x26
   4faf4:	mov	x2, x20
   4faf8:	mov	x3, x19
   4fafc:	mov	x4, x24
   4fb00:	bl	ccd0 <__gmpn_mul@plt>
   4fb04:	b	4fc80 <__gmpn_mu_bdiv_q@@Base+0x55c>
   4fb08:	ldur	x1, [x29, #-16]
   4fb0c:	add	x0, x22, x24, lsl #3
   4fb10:	mov	x2, x24
   4fb14:	mov	x3, x28
   4fb18:	mov	x4, x25
   4fb1c:	bl	ccd0 <__gmpn_mul@plt>
   4fb20:	subs	x26, x25, x24
   4fb24:	add	x22, x28, x25, lsl #3
   4fb28:	b.eq	4fb6c <__gmpn_mu_bdiv_q@@Base+0x448>  // b.none
   4fb2c:	ldur	x0, [x29, #-40]
   4fb30:	lsl	x8, x25, #3
   4fb34:	sub	x3, x24, x25
   4fb38:	add	x28, x0, x24, lsl #3
   4fb3c:	add	x1, x0, x8
   4fb40:	add	x2, x28, x8
   4fb44:	bl	c2d0 <__gmpn_sub_n@plt>
   4fb48:	add	w21, w21, w0
   4fb4c:	cmp	w21, #0x2
   4fb50:	b.ne	4fb6c <__gmpn_mu_bdiv_q@@Base+0x448>  // b.any
   4fb54:	add	x8, x28, x24, lsl #3
   4fb58:	ldr	x9, [x8]
   4fb5c:	adds	x9, x9, #0x1
   4fb60:	str	x9, [x8], #8
   4fb64:	b.cs	4fb58 <__gmpn_mu_bdiv_q@@Base+0x434>  // b.hs, b.nlast
   4fb68:	mov	w21, #0x1                   	// #1
   4fb6c:	ldur	x24, [x29, #-40]
   4fb70:	ldr	x10, [sp, #16]
   4fb74:	ldr	x9, [sp, #32]
   4fb78:	add	x3, x26, x23
   4fb7c:	sxtw	x4, w21
   4fb80:	add	x8, x24, x10
   4fb84:	mvn	x9, x9
   4fb88:	add	x0, x8, x9, lsl #3
   4fb8c:	add	x2, x8, x10
   4fb90:	mov	x1, x27
   4fb94:	bl	c760 <__gmpn_sub_nc@plt>
   4fb98:	ldur	x2, [x29, #-8]
   4fb9c:	mov	x0, x22
   4fba0:	mov	x1, x24
   4fba4:	mov	x3, x23
   4fba8:	bl	cec0 <__gmpn_mullo_n@plt>
   4fbac:	ldr	x20, [sp, #8]
   4fbb0:	ldr	x8, [x19]
   4fbb4:	cbnz	x8, 4fcb4 <__gmpn_mu_bdiv_q@@Base+0x590>
   4fbb8:	b	4fcf4 <__gmpn_mu_bdiv_q@@Base+0x5d0>
   4fbbc:	cmp	x25, x24
   4fbc0:	b.ne	4fd20 <__gmpn_mu_bdiv_q@@Base+0x5fc>  // b.any
   4fbc4:	stur	x26, [x29, #-48]
   4fbc8:	ldur	x20, [x29, #-40]
   4fbcc:	ldr	x26, [sp, #16]
   4fbd0:	mov	x21, xzr
   4fbd4:	mov	x28, x19
   4fbd8:	mov	x27, x9
   4fbdc:	ldur	x0, [x29, #-48]
   4fbe0:	ldur	x1, [x29, #-16]
   4fbe4:	mov	x2, x24
   4fbe8:	mov	x3, x28
   4fbec:	mov	x4, x24
   4fbf0:	add	x22, x22, x26
   4fbf4:	bl	ccd0 <__gmpn_mul@plt>
   4fbf8:	ldur	x2, [x29, #-24]
   4fbfc:	sxtw	x4, w21
   4fc00:	mov	x0, x27
   4fc04:	mov	x1, x22
   4fc08:	mov	x3, x24
   4fc0c:	add	x28, x28, x26
   4fc10:	bl	c760 <__gmpn_sub_nc@plt>
   4fc14:	ldur	x2, [x29, #-8]
   4fc18:	mov	x21, x0
   4fc1c:	mov	x0, x28
   4fc20:	mov	x1, x20
   4fc24:	mov	x3, x24
   4fc28:	bl	cec0 <__gmpn_mullo_n@plt>
   4fc2c:	sub	x23, x23, x24
   4fc30:	cmp	x23, x24
   4fc34:	b.gt	4fbdc <__gmpn_mu_bdiv_q@@Base+0x4b8>
   4fc38:	add	x27, x22, x26
   4fc3c:	mov	x22, x20
   4fc40:	b	4fa40 <__gmpn_mu_bdiv_q@@Base+0x31c>
   4fc44:	mov	x9, xzr
   4fc48:	lsl	x8, x8, #3
   4fc4c:	ldr	x10, [x23, x8]
   4fc50:	subs	x9, x10, x9
   4fc54:	str	x9, [x23, x8]
   4fc58:	b.cs	4fc80 <__gmpn_mu_bdiv_q@@Base+0x55c>  // b.hs, b.nlast
   4fc5c:	add	x8, x20, x20, lsl #1
   4fc60:	sub	x8, x8, x25
   4fc64:	sub	x8, x8, x21, lsl #1
   4fc68:	add	x8, x27, x8, lsl #3
   4fc6c:	add	x8, x8, #0x8
   4fc70:	ldr	x9, [x8]
   4fc74:	sub	x10, x9, #0x1
   4fc78:	str	x10, [x8], #8
   4fc7c:	cbz	x9, 4fc70 <__gmpn_mu_bdiv_q@@Base+0x54c>
   4fc80:	lsl	x24, x24, #3
   4fc84:	add	x1, x22, x24
   4fc88:	add	x2, x23, x24
   4fc8c:	mov	x0, x23
   4fc90:	mov	x3, x21
   4fc94:	bl	c2d0 <__gmpn_sub_n@plt>
   4fc98:	add	x0, x19, x24
   4fc9c:	mov	x1, x23
   4fca0:	mov	x2, x27
   4fca4:	mov	x3, x21
   4fca8:	bl	cec0 <__gmpn_mullo_n@plt>
   4fcac:	ldr	x8, [x19]
   4fcb0:	cbz	x8, 4fcf4 <__gmpn_mu_bdiv_q@@Base+0x5d0>
   4fcb4:	neg	x8, x8
   4fcb8:	subs	x2, x20, #0x1
   4fcbc:	str	x8, [x19]
   4fcc0:	b.eq	4fd00 <__gmpn_mu_bdiv_q@@Base+0x5dc>  // b.none
   4fcc4:	add	x0, x19, #0x8
   4fcc8:	ldp	x20, x19, [sp, #224]
   4fccc:	ldp	x22, x21, [sp, #208]
   4fcd0:	ldp	x24, x23, [sp, #192]
   4fcd4:	ldp	x26, x25, [sp, #176]
   4fcd8:	ldp	x28, x27, [sp, #160]
   4fcdc:	ldp	x29, x30, [sp, #144]
   4fce0:	mov	x1, x0
   4fce4:	add	sp, sp, #0xf0
   4fce8:	b	c290 <__gmpn_com@plt>
   4fcec:	ldr	x8, [x19, #8]!
   4fcf0:	cbnz	x8, 4fcb4 <__gmpn_mu_bdiv_q@@Base+0x590>
   4fcf4:	subs	x20, x20, #0x1
   4fcf8:	str	xzr, [x19]
   4fcfc:	b.ne	4fcec <__gmpn_mu_bdiv_q@@Base+0x5c8>  // b.any
   4fd00:	ldp	x20, x19, [sp, #224]
   4fd04:	ldp	x22, x21, [sp, #208]
   4fd08:	ldp	x24, x23, [sp, #192]
   4fd0c:	ldp	x26, x25, [sp, #176]
   4fd10:	ldp	x28, x27, [sp, #160]
   4fd14:	ldp	x29, x30, [sp, #144]
   4fd18:	add	sp, sp, #0xf0
   4fd1c:	ret
   4fd20:	ldur	x22, [x29, #-40]
   4fd24:	mov	w21, wzr
   4fd28:	mov	x28, x19
   4fd2c:	b	4fd68 <__gmpn_mu_bdiv_q@@Base+0x644>
   4fd30:	sxtw	x4, w8
   4fd34:	mov	x1, x27
   4fd38:	mov	x3, x25
   4fd3c:	bl	c760 <__gmpn_sub_nc@plt>
   4fd40:	ldur	x2, [x29, #-8]
   4fd44:	mov	x21, x0
   4fd48:	mov	x0, x28
   4fd4c:	mov	x1, x22
   4fd50:	mov	x3, x25
   4fd54:	add	x27, x27, x25, lsl #3
   4fd58:	bl	cec0 <__gmpn_mullo_n@plt>
   4fd5c:	sub	x23, x23, x25
   4fd60:	cmp	x23, x25
   4fd64:	b.le	4fa40 <__gmpn_mu_bdiv_q@@Base+0x31c>
   4fd68:	ldur	x1, [x29, #-16]
   4fd6c:	mov	x0, x26
   4fd70:	mov	x2, x24
   4fd74:	mov	x3, x28
   4fd78:	mov	x4, x25
   4fd7c:	bl	ccd0 <__gmpn_mul@plt>
   4fd80:	ldp	x1, x3, [x29, #-64]
   4fd84:	ldr	x2, [sp, #72]
   4fd88:	mov	x0, x22
   4fd8c:	add	x28, x28, x25, lsl #3
   4fd90:	bl	c2d0 <__gmpn_sub_n@plt>
   4fd94:	add	w8, w21, w0
   4fd98:	ldp	x0, x2, [x29, #-32]
   4fd9c:	cmp	w8, #0x2
   4fda0:	b.ne	4fd30 <__gmpn_mu_bdiv_q@@Base+0x60c>  // b.any
   4fda4:	mov	x8, x2
   4fda8:	ldr	x9, [x8]
   4fdac:	adds	x9, x9, #0x1
   4fdb0:	str	x9, [x8], #8
   4fdb4:	b.cs	4fda8 <__gmpn_mu_bdiv_q@@Base+0x684>  // b.hs, b.nlast
   4fdb8:	mov	w8, #0x1                   	// #1
   4fdbc:	b	4fd30 <__gmpn_mu_bdiv_q@@Base+0x60c>
   4fdc0:	ldur	x22, [x29, #-40]
   4fdc4:	mov	w21, wzr
   4fdc8:	mov	x28, x19
   4fdcc:	b	4fe10 <__gmpn_mu_bdiv_q@@Base+0x6ec>
   4fdd0:	ldp	x0, x2, [x29, #-32]
   4fdd4:	sxtw	x4, w21
   4fdd8:	mov	x1, x27
   4fddc:	mov	x3, x25
   4fde0:	bl	c760 <__gmpn_sub_nc@plt>
   4fde4:	ldur	x2, [x29, #-8]
   4fde8:	mov	x21, x0
   4fdec:	mov	x0, x28
   4fdf0:	mov	x1, x22
   4fdf4:	mov	x3, x25
   4fdf8:	add	x27, x27, x25, lsl #3
   4fdfc:	bl	cec0 <__gmpn_mullo_n@plt>
   4fe00:	ldr	x10, [sp, #64]
   4fe04:	sub	x23, x23, x25
   4fe08:	cmp	x23, x25
   4fe0c:	b.le	4fa40 <__gmpn_mu_bdiv_q@@Base+0x31c>
   4fe10:	ldur	x2, [x29, #-16]
   4fe14:	ldr	x6, [sp, #56]
   4fe18:	mov	x0, x26
   4fe1c:	mov	x1, x10
   4fe20:	mov	x3, x24
   4fe24:	mov	x4, x28
   4fe28:	mov	x5, x25
   4fe2c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4fe30:	cmp	x25, x24
   4fe34:	add	x28, x28, x25, lsl #3
   4fe38:	b.eq	4fdd0 <__gmpn_mu_bdiv_q@@Base+0x6ac>  // b.none
   4fe3c:	ldp	x1, x3, [x29, #-64]
   4fe40:	ldr	x2, [sp, #72]
   4fe44:	mov	x0, x22
   4fe48:	bl	c2d0 <__gmpn_sub_n@plt>
   4fe4c:	add	w21, w21, w0
   4fe50:	ldp	x0, x2, [x29, #-32]
   4fe54:	cmp	w21, #0x2
   4fe58:	b.ne	4fdd4 <__gmpn_mu_bdiv_q@@Base+0x6b0>  // b.any
   4fe5c:	mov	x8, x2
   4fe60:	ldr	x9, [x8]
   4fe64:	adds	x9, x9, #0x1
   4fe68:	str	x9, [x8], #8
   4fe6c:	b.cs	4fe60 <__gmpn_mu_bdiv_q@@Base+0x73c>  // b.hs, b.nlast
   4fe70:	mov	w21, #0x1                   	// #1
   4fe74:	b	4fdd4 <__gmpn_mu_bdiv_q@@Base+0x6b0>

000000000004fe78 <__gmpn_mu_bdiv_q_itch@@Base>:
   4fe78:	stp	x29, x30, [sp, #-48]!
   4fe7c:	str	x21, [sp, #16]
   4fe80:	mov	x21, x0
   4fe84:	cmp	x0, x1
   4fe88:	stp	x20, x19, [sp, #32]
   4fe8c:	mov	x29, sp
   4fe90:	b.le	4fedc <__gmpn_mu_bdiv_q_itch@@Base+0x64>
   4fe94:	sub	x8, x21, #0x1
   4fe98:	sdiv	x9, x8, x1
   4fe9c:	add	x9, x9, #0x1
   4fea0:	sdiv	x21, x8, x9
   4fea4:	mov	x20, x1
   4fea8:	cmp	x21, #0x10
   4feac:	add	x19, x21, #0x1
   4feb0:	b.le	4ff10 <__gmpn_mu_bdiv_q_itch@@Base+0x98>
   4feb4:	mov	x0, x20
   4feb8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4febc:	asr	x8, x0, #1
   4fec0:	cmp	x8, x21
   4fec4:	csel	x9, x8, x0, gt
   4fec8:	cmp	x8, x20
   4fecc:	csel	x8, x9, xzr, lt  // lt = tstop
   4fed0:	add	x8, x0, x8
   4fed4:	add	x8, x8, #0x4
   4fed8:	b	4ff18 <__gmpn_mu_bdiv_q_itch@@Base+0xa0>
   4fedc:	sub	x19, x21, x21, asr #1
   4fee0:	cmp	x19, #0x11
   4fee4:	b.le	4ff24 <__gmpn_mu_bdiv_q_itch@@Base+0xac>
   4fee8:	mov	x0, x21
   4feec:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4fef0:	asr	x8, x0, #1
   4fef4:	cmp	x8, x19
   4fef8:	csel	x9, x0, x8, lt  // lt = tstop
   4fefc:	cmp	x8, x21
   4ff00:	csel	x8, x9, xzr, lt  // lt = tstop
   4ff04:	add	x8, x0, x8
   4ff08:	add	x8, x8, #0x4
   4ff0c:	b	4ff2c <__gmpn_mu_bdiv_q_itch@@Base+0xb4>
   4ff10:	mov	x8, xzr
   4ff14:	add	x0, x19, x20
   4ff18:	add	x9, x0, x20
   4ff1c:	add	x20, x9, x8
   4ff20:	b	4ff30 <__gmpn_mu_bdiv_q_itch@@Base+0xb8>
   4ff24:	mov	x8, xzr
   4ff28:	add	x0, x19, x21
   4ff2c:	add	x20, x8, x0
   4ff30:	mov	x0, x19
   4ff34:	bl	d1e0 <__gmpn_binvert_itch@plt>
   4ff38:	cmp	x20, x0
   4ff3c:	csel	x8, x20, x0, gt
   4ff40:	add	x0, x8, x19
   4ff44:	ldp	x20, x19, [sp, #32]
   4ff48:	ldr	x21, [sp, #16]
   4ff4c:	ldp	x29, x30, [sp], #48
   4ff50:	ret

000000000004ff54 <__gmpn_mu_bdiv_qr@@Base>:
   4ff54:	sub	sp, sp, #0xd0
   4ff58:	stp	x24, x23, [sp, #160]
   4ff5c:	sub	x24, x3, x5
   4ff60:	stp	x29, x30, [sp, #112]
   4ff64:	stp	x28, x27, [sp, #128]
   4ff68:	stp	x22, x21, [sp, #176]
   4ff6c:	stp	x20, x19, [sp, #192]
   4ff70:	add	x29, sp, #0x70
   4ff74:	mov	x27, x6
   4ff78:	mov	x19, x5
   4ff7c:	mov	x28, x4
   4ff80:	mov	x22, x2
   4ff84:	cmp	x24, x5
   4ff88:	mov	x23, x0
   4ff8c:	stp	x26, x25, [sp, #144]
   4ff90:	stur	x4, [x29, #-40]
   4ff94:	stur	x1, [x29, #-16]
   4ff98:	str	x24, [sp, #24]
   4ff9c:	stur	x6, [x29, #-24]
   4ffa0:	b.le	5013c <__gmpn_mu_bdiv_qr@@Base+0x1e8>
   4ffa4:	sub	x8, x24, #0x1
   4ffa8:	sdiv	x9, x8, x19
   4ffac:	add	x9, x9, #0x1
   4ffb0:	mov	x25, x22
   4ffb4:	sdiv	x22, x8, x9
   4ffb8:	add	x20, x22, #0x1
   4ffbc:	add	x26, x27, x20, lsl #3
   4ffc0:	mov	x21, x1
   4ffc4:	mov	x0, x27
   4ffc8:	mov	x1, x28
   4ffcc:	mov	x2, x20
   4ffd0:	mov	x3, x26
   4ffd4:	bl	cd20 <__gmpn_binvert@plt>
   4ffd8:	mov	x0, x21
   4ffdc:	mov	x1, x25
   4ffe0:	mov	x2, x19
   4ffe4:	bl	ca50 <__gmpn_copyi@plt>
   4ffe8:	cmp	x24, x20
   4ffec:	add	x25, x25, x19, lsl #3
   4fff0:	stur	x26, [x29, #-8]
   4fff4:	str	x22, [sp, #8]
   4fff8:	b.le	50214 <__gmpn_mu_bdiv_qr@@Base+0x2c0>
   4fffc:	lsl	x8, x20, #3
   50000:	lsl	x24, x19, #3
   50004:	add	x10, x21, x8
   50008:	add	x8, x26, x8
   5000c:	sub	x9, x19, x20
   50010:	stp	x8, x10, [sp, #40]
   50014:	add	x8, x26, x24
   50018:	str	x9, [sp, #56]
   5001c:	mvn	x9, x22
   50020:	stur	x8, [x29, #-32]
   50024:	add	x8, x21, x24
   50028:	cmp	x22, #0x10
   5002c:	add	x8, x8, x9, lsl #3
   50030:	stur	x8, [x29, #-48]
   50034:	b.le	503a0 <__gmpn_mu_bdiv_qr@@Base+0x44c>
   50038:	ldur	x9, [x29, #-24]
   5003c:	add	x8, x20, x19
   50040:	str	x8, [sp, #32]
   50044:	add	x8, x19, x22, lsl #1
   50048:	add	x8, x9, x8, lsl #3
   5004c:	cmp	x20, x19
   50050:	mov	x27, xzr
   50054:	add	x8, x8, #0x18
   50058:	b.ne	505ec <__gmpn_mu_bdiv_qr@@Base+0x698>  // b.any
   5005c:	ldr	x26, [sp, #24]
   50060:	mov	x22, x23
   50064:	str	x8, [sp, #56]
   50068:	b	500a4 <__gmpn_mu_bdiv_qr@@Base+0x150>
   5006c:	ldur	x0, [x29, #-48]
   50070:	ldur	x2, [x29, #-32]
   50074:	mov	x1, x25
   50078:	mov	x3, x19
   5007c:	mov	x4, x27
   50080:	add	x22, x22, x24
   50084:	sub	x26, x26, x19
   50088:	bl	c760 <__gmpn_sub_nc@plt>
   5008c:	ldur	x28, [x29, #-40]
   50090:	ldur	x21, [x29, #-16]
   50094:	mov	x27, x0
   50098:	cmp	x26, x19
   5009c:	add	x25, x25, x24
   500a0:	b.le	50424 <__gmpn_mu_bdiv_qr@@Base+0x4d0>
   500a4:	ldur	x2, [x29, #-24]
   500a8:	mov	x0, x22
   500ac:	mov	x1, x21
   500b0:	mov	x3, x19
   500b4:	bl	cec0 <__gmpn_mullo_n@plt>
   500b8:	mov	x0, x19
   500bc:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   500c0:	mov	x20, x0
   500c4:	ldur	x0, [x29, #-8]
   500c8:	mov	x1, x20
   500cc:	mov	x2, x28
   500d0:	mov	x3, x19
   500d4:	add	x21, x0, x20, lsl #3
   500d8:	mov	x4, x22
   500dc:	mov	x5, x19
   500e0:	mov	x6, x21
   500e4:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   500e8:	ldr	x8, [sp, #32]
   500ec:	sub	x28, x8, x20
   500f0:	cmp	x28, #0x1
   500f4:	b.lt	5006c <__gmpn_mu_bdiv_qr@@Base+0x118>  // b.tstop
   500f8:	mov	x0, x21
   500fc:	ldp	x2, x21, [x29, #-16]
   50100:	mov	x3, x28
   50104:	mov	x1, x21
   50108:	bl	c2d0 <__gmpn_sub_n@plt>
   5010c:	lsl	x8, x28, #3
   50110:	ldr	x9, [x21, x8]
   50114:	subs	x9, x9, x0
   50118:	str	x9, [x21, x8]
   5011c:	b.cs	5006c <__gmpn_mu_bdiv_qr@@Base+0x118>  // b.hs, b.nlast
   50120:	ldr	x8, [sp, #56]
   50124:	sub	x8, x8, x20, lsl #3
   50128:	ldr	x9, [x8]
   5012c:	sub	x10, x9, #0x1
   50130:	str	x10, [x8], #8
   50134:	cbz	x9, 50128 <__gmpn_mu_bdiv_qr@@Base+0x1d4>
   50138:	b	5006c <__gmpn_mu_bdiv_qr@@Base+0x118>
   5013c:	asr	x20, x24, #1
   50140:	sub	x24, x24, x20
   50144:	add	x25, x27, x24, lsl #3
   50148:	mov	x26, x3
   5014c:	mov	x0, x27
   50150:	mov	x1, x28
   50154:	mov	x2, x24
   50158:	mov	x3, x25
   5015c:	bl	cd20 <__gmpn_binvert@plt>
   50160:	mov	x0, x23
   50164:	mov	x1, x22
   50168:	mov	x2, x27
   5016c:	mov	x3, x24
   50170:	bl	cec0 <__gmpn_mullo_n@plt>
   50174:	cmp	x24, #0x11
   50178:	b.le	50224 <__gmpn_mu_bdiv_qr@@Base+0x2d0>
   5017c:	mov	x0, x19
   50180:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   50184:	mov	x27, x0
   50188:	add	x21, x25, x0, lsl #3
   5018c:	mov	x0, x25
   50190:	mov	x1, x27
   50194:	mov	x2, x28
   50198:	mov	x3, x19
   5019c:	mov	x4, x23
   501a0:	mov	x5, x24
   501a4:	mov	x6, x21
   501a8:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   501ac:	add	x8, x24, x19
   501b0:	sub	x28, x8, x27
   501b4:	cmp	x28, #0x1
   501b8:	b.lt	5023c <__gmpn_mu_bdiv_qr@@Base+0x2e8>  // b.tstop
   501bc:	mov	x0, x21
   501c0:	mov	x1, x25
   501c4:	mov	x2, x22
   501c8:	mov	x3, x28
   501cc:	bl	c2d0 <__gmpn_sub_n@plt>
   501d0:	lsl	x8, x28, #3
   501d4:	ldr	x9, [x25, x8]
   501d8:	subs	x9, x9, x0
   501dc:	str	x9, [x25, x8]
   501e0:	b.cs	5023c <__gmpn_mu_bdiv_qr@@Base+0x2e8>  // b.hs, b.nlast
   501e4:	lsl	x8, x26, #1
   501e8:	add	x9, x27, x19
   501ec:	sub	x8, x8, x9
   501f0:	ldur	x9, [x29, #-24]
   501f4:	sub	x8, x8, x20, lsl #1
   501f8:	add	x8, x9, x8, lsl #3
   501fc:	add	x8, x8, #0x8
   50200:	ldr	x9, [x8]
   50204:	sub	x10, x9, #0x1
   50208:	str	x10, [x8], #8
   5020c:	cbz	x9, 50200 <__gmpn_mu_bdiv_qr@@Base+0x2ac>
   50210:	b	5023c <__gmpn_mu_bdiv_qr@@Base+0x2e8>
   50214:	mov	x26, x24
   50218:	mov	x27, xzr
   5021c:	mov	x22, x23
   50220:	b	50424 <__gmpn_mu_bdiv_qr@@Base+0x4d0>
   50224:	mov	x0, x25
   50228:	mov	x1, x28
   5022c:	mov	x2, x19
   50230:	mov	x3, x23
   50234:	mov	x4, x24
   50238:	bl	ccd0 <__gmpn_mul@plt>
   5023c:	ldur	x21, [x29, #-16]
   50240:	lsl	x8, x24, #3
   50244:	add	x1, x22, x8
   50248:	add	x2, x25, x8
   5024c:	mov	x0, x21
   50250:	mov	x3, x19
   50254:	add	x28, x23, x8
   50258:	bl	c2d0 <__gmpn_sub_n@plt>
   5025c:	ldur	x2, [x29, #-24]
   50260:	mov	x27, x0
   50264:	mov	x0, x28
   50268:	mov	x1, x21
   5026c:	mov	x3, x20
   50270:	bl	cec0 <__gmpn_mullo_n@plt>
   50274:	ldr	x8, [sp, #24]
   50278:	stur	x27, [x29, #-8]
   5027c:	cmp	x8, #0x23
   50280:	b.le	50310 <__gmpn_mu_bdiv_qr@@Base+0x3bc>
   50284:	mov	x0, x19
   50288:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   5028c:	ldur	x2, [x29, #-40]
   50290:	mov	x27, x0
   50294:	add	x21, x25, x0, lsl #3
   50298:	mov	x0, x25
   5029c:	mov	x1, x27
   502a0:	mov	x3, x19
   502a4:	mov	x4, x28
   502a8:	mov	x5, x20
   502ac:	mov	x6, x21
   502b0:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   502b4:	add	x8, x20, x19
   502b8:	sub	x28, x8, x27
   502bc:	cmp	x28, #0x1
   502c0:	b.lt	50328 <__gmpn_mu_bdiv_qr@@Base+0x3d4>  // b.tstop
   502c4:	ldur	x2, [x29, #-16]
   502c8:	mov	x0, x21
   502cc:	mov	x1, x25
   502d0:	mov	x3, x28
   502d4:	bl	c2d0 <__gmpn_sub_n@plt>
   502d8:	lsl	x8, x28, #3
   502dc:	ldr	x9, [x25, x8]
   502e0:	subs	x9, x9, x0
   502e4:	str	x9, [x25, x8]
   502e8:	b.cs	50328 <__gmpn_mu_bdiv_qr@@Base+0x3d4>  // b.hs, b.nlast
   502ec:	ldur	x9, [x29, #-24]
   502f0:	sub	x8, x26, x27
   502f4:	add	x8, x9, x8, lsl #3
   502f8:	add	x8, x8, #0x8
   502fc:	ldr	x9, [x8]
   50300:	sub	x10, x9, #0x1
   50304:	str	x10, [x8], #8
   50308:	cbz	x9, 502fc <__gmpn_mu_bdiv_qr@@Base+0x3a8>
   5030c:	b	50328 <__gmpn_mu_bdiv_qr@@Base+0x3d4>
   50310:	ldur	x1, [x29, #-40]
   50314:	mov	x0, x25
   50318:	mov	x2, x19
   5031c:	mov	x3, x28
   50320:	mov	x4, x20
   50324:	bl	ccd0 <__gmpn_mul@plt>
   50328:	ldp	x8, x21, [x29, #-24]
   5032c:	ldr	x26, [sp, #24]
   50330:	sub	x3, x19, x20
   50334:	add	x1, x21, x20, lsl #3
   50338:	add	x2, x8, x26, lsl #3
   5033c:	mov	x0, x21
   50340:	bl	c2d0 <__gmpn_sub_n@plt>
   50344:	ldur	x8, [x29, #-8]
   50348:	ldur	x28, [x29, #-40]
   5034c:	add	x4, x0, x8
   50350:	cmp	x4, #0x2
   50354:	b.ne	50370 <__gmpn_mu_bdiv_qr@@Base+0x41c>  // b.any
   50358:	add	x8, x25, x19, lsl #3
   5035c:	mov	w4, #0x1                   	// #1
   50360:	ldr	x9, [x8]
   50364:	adds	x9, x9, #0x1
   50368:	str	x9, [x8], #8
   5036c:	b.cs	50360 <__gmpn_mu_bdiv_qr@@Base+0x40c>  // b.hs, b.nlast
   50370:	lsl	x8, x19, #3
   50374:	add	x9, x21, x8
   50378:	add	x10, x22, x8
   5037c:	sub	x0, x9, x20, lsl #3
   50380:	add	x1, x10, x24, lsl #3
   50384:	add	x2, x25, x8
   50388:	mov	x3, x20
   5038c:	bl	c760 <__gmpn_sub_nc@plt>
   50390:	mov	x20, x0
   50394:	ldr	x8, [x23]
   50398:	cbnz	x8, 5057c <__gmpn_mu_bdiv_qr@@Base+0x628>
   5039c:	b	505d8 <__gmpn_mu_bdiv_qr@@Base+0x684>
   503a0:	cmp	x20, x19
   503a4:	b.ne	506f8 <__gmpn_mu_bdiv_qr@@Base+0x7a4>  // b.any
   503a8:	ldr	x26, [sp, #24]
   503ac:	ldur	x22, [x29, #-48]
   503b0:	mov	x28, xzr
   503b4:	mov	x27, xzr
   503b8:	ldp	x2, x1, [x29, #-24]
   503bc:	add	x20, x23, x28
   503c0:	mov	x0, x20
   503c4:	mov	x3, x19
   503c8:	add	x21, x25, x28
   503cc:	bl	cec0 <__gmpn_mullo_n@plt>
   503d0:	ldur	x0, [x29, #-8]
   503d4:	ldur	x1, [x29, #-40]
   503d8:	mov	x2, x19
   503dc:	mov	x3, x20
   503e0:	mov	x4, x19
   503e4:	bl	ccd0 <__gmpn_mul@plt>
   503e8:	ldur	x2, [x29, #-32]
   503ec:	mov	x0, x22
   503f0:	mov	x1, x21
   503f4:	mov	x3, x19
   503f8:	mov	x4, x27
   503fc:	sub	x26, x26, x19
   50400:	bl	c760 <__gmpn_sub_nc@plt>
   50404:	mov	x27, x0
   50408:	cmp	x26, x19
   5040c:	add	x28, x28, x24
   50410:	b.gt	503b8 <__gmpn_mu_bdiv_qr@@Base+0x464>
   50414:	add	x22, x23, x28
   50418:	add	x25, x25, x28
   5041c:	ldur	x28, [x29, #-40]
   50420:	ldur	x21, [x29, #-16]
   50424:	ldur	x2, [x29, #-24]
   50428:	mov	x0, x22
   5042c:	mov	x1, x21
   50430:	mov	x3, x26
   50434:	bl	cec0 <__gmpn_mullo_n@plt>
   50438:	cmp	x26, #0x11
   5043c:	b.le	504e8 <__gmpn_mu_bdiv_qr@@Base+0x594>
   50440:	mov	x0, x19
   50444:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   50448:	ldur	x24, [x29, #-8]
   5044c:	mov	x20, x0
   50450:	mov	x1, x20
   50454:	mov	x2, x28
   50458:	add	x21, x24, x0, lsl #3
   5045c:	mov	x0, x24
   50460:	mov	x3, x19
   50464:	mov	x4, x22
   50468:	mov	x5, x26
   5046c:	mov	x6, x21
   50470:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   50474:	add	x8, x26, x19
   50478:	sub	x22, x8, x20
   5047c:	cmp	x22, #0x1
   50480:	b.lt	504d8 <__gmpn_mu_bdiv_qr@@Base+0x584>  // b.tstop
   50484:	ldur	x2, [x29, #-16]
   50488:	mov	x0, x21
   5048c:	mov	x1, x24
   50490:	mov	x3, x22
   50494:	bl	c2d0 <__gmpn_sub_n@plt>
   50498:	lsl	x8, x22, #3
   5049c:	ldr	x9, [x24, x8]
   504a0:	subs	x9, x9, x0
   504a4:	str	x9, [x24, x8]
   504a8:	b.cs	504d8 <__gmpn_mu_bdiv_qr@@Base+0x584>  // b.hs, b.nlast
   504ac:	ldr	x8, [sp, #8]
   504b0:	ldur	x9, [x29, #-24]
   504b4:	add	x8, x26, x8
   504b8:	add	x8, x8, x19
   504bc:	sub	x8, x8, x20
   504c0:	add	x8, x9, x8, lsl #3
   504c4:	add	x8, x8, #0x10
   504c8:	ldr	x9, [x8]
   504cc:	sub	x10, x9, #0x1
   504d0:	str	x10, [x8], #8
   504d4:	cbz	x9, 504c8 <__gmpn_mu_bdiv_qr@@Base+0x574>
   504d8:	cmp	x26, x19
   504dc:	b.ne	5050c <__gmpn_mu_bdiv_qr@@Base+0x5b8>  // b.any
   504e0:	ldur	x21, [x29, #-16]
   504e4:	b	5054c <__gmpn_mu_bdiv_qr@@Base+0x5f8>
   504e8:	ldur	x24, [x29, #-8]
   504ec:	mov	x1, x28
   504f0:	mov	x2, x19
   504f4:	mov	x3, x22
   504f8:	mov	x0, x24
   504fc:	mov	x4, x26
   50500:	bl	ccd0 <__gmpn_mul@plt>
   50504:	cmp	x26, x19
   50508:	b.eq	504e0 <__gmpn_mu_bdiv_qr@@Base+0x58c>  // b.none
   5050c:	ldur	x21, [x29, #-16]
   50510:	lsl	x8, x26, #3
   50514:	add	x2, x24, x8
   50518:	sub	x3, x19, x26
   5051c:	add	x1, x21, x8
   50520:	mov	x0, x21
   50524:	bl	c2d0 <__gmpn_sub_n@plt>
   50528:	add	x27, x0, x27
   5052c:	cmp	x27, #0x2
   50530:	b.ne	5054c <__gmpn_mu_bdiv_qr@@Base+0x5f8>  // b.any
   50534:	add	x8, x24, x19, lsl #3
   50538:	mov	w27, #0x1                   	// #1
   5053c:	ldr	x9, [x8]
   50540:	adds	x9, x9, #0x1
   50544:	str	x9, [x8], #8
   50548:	b.cs	5053c <__gmpn_mu_bdiv_qr@@Base+0x5e8>  // b.hs, b.nlast
   5054c:	lsl	x8, x19, #3
   50550:	add	x9, x21, x8
   50554:	sub	x0, x9, x26, lsl #3
   50558:	add	x2, x24, x8
   5055c:	mov	x1, x25
   50560:	mov	x3, x26
   50564:	mov	x4, x27
   50568:	bl	c760 <__gmpn_sub_nc@plt>
   5056c:	ldr	x26, [sp, #24]
   50570:	mov	x20, x0
   50574:	ldr	x8, [x23]
   50578:	cbz	x8, 505d8 <__gmpn_mu_bdiv_qr@@Base+0x684>
   5057c:	neg	x8, x8
   50580:	subs	x2, x26, #0x1
   50584:	str	x8, [x23]
   50588:	b.eq	50598 <__gmpn_mu_bdiv_qr@@Base+0x644>  // b.none
   5058c:	add	x0, x23, #0x8
   50590:	mov	x1, x0
   50594:	bl	c290 <__gmpn_com@plt>
   50598:	mov	x0, x21
   5059c:	mov	x1, x21
   505a0:	mov	x2, x28
   505a4:	mov	x3, x19
   505a8:	bl	ca70 <__gmpn_add_n@plt>
   505ac:	sub	x0, x0, x20
   505b0:	ldp	x20, x19, [sp, #192]
   505b4:	ldp	x22, x21, [sp, #176]
   505b8:	ldp	x24, x23, [sp, #160]
   505bc:	ldp	x26, x25, [sp, #144]
   505c0:	ldp	x28, x27, [sp, #128]
   505c4:	ldp	x29, x30, [sp, #112]
   505c8:	add	sp, sp, #0xd0
   505cc:	ret
   505d0:	ldr	x8, [x23, #8]!
   505d4:	cbnz	x8, 5057c <__gmpn_mu_bdiv_qr@@Base+0x628>
   505d8:	subs	x26, x26, #0x1
   505dc:	str	xzr, [x23]
   505e0:	b.ne	505d0 <__gmpn_mu_bdiv_qr@@Base+0x67c>  // b.any
   505e4:	mov	x0, xzr
   505e8:	b	505b0 <__gmpn_mu_bdiv_qr@@Base+0x65c>
   505ec:	ldr	x26, [sp, #24]
   505f0:	mov	x22, x23
   505f4:	str	x8, [sp, #16]
   505f8:	b	50618 <__gmpn_mu_bdiv_qr@@Base+0x6c4>
   505fc:	mov	x1, x25
   50600:	mov	x3, x20
   50604:	bl	c760 <__gmpn_sub_nc@plt>
   50608:	mov	x27, x0
   5060c:	cmp	x26, x20
   50610:	add	x25, x25, x20, lsl #3
   50614:	b.le	50424 <__gmpn_mu_bdiv_qr@@Base+0x4d0>
   50618:	ldur	x2, [x29, #-24]
   5061c:	mov	x0, x22
   50620:	mov	x1, x21
   50624:	mov	x3, x20
   50628:	bl	cec0 <__gmpn_mullo_n@plt>
   5062c:	mov	x0, x19
   50630:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   50634:	mov	x21, x0
   50638:	ldur	x0, [x29, #-8]
   5063c:	mov	x2, x28
   50640:	mov	x1, x21
   50644:	mov	x3, x19
   50648:	add	x28, x0, x21, lsl #3
   5064c:	mov	x4, x22
   50650:	mov	x5, x20
   50654:	mov	x6, x28
   50658:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   5065c:	ldr	x8, [sp, #32]
   50660:	sub	x24, x8, x21
   50664:	cmp	x24, #0x1
   50668:	b.lt	506ac <__gmpn_mu_bdiv_qr@@Base+0x758>  // b.tstop
   5066c:	mov	x0, x28
   50670:	ldp	x2, x28, [x29, #-16]
   50674:	mov	x3, x24
   50678:	mov	x1, x28
   5067c:	bl	c2d0 <__gmpn_sub_n@plt>
   50680:	lsl	x8, x24, #3
   50684:	ldr	x9, [x28, x8]
   50688:	subs	x9, x9, x0
   5068c:	str	x9, [x28, x8]
   50690:	b.cs	506ac <__gmpn_mu_bdiv_qr@@Base+0x758>  // b.hs, b.nlast
   50694:	ldr	x8, [sp, #16]
   50698:	sub	x8, x8, x21, lsl #3
   5069c:	ldr	x9, [x8]
   506a0:	sub	x10, x9, #0x1
   506a4:	str	x10, [x8], #8
   506a8:	cbz	x9, 5069c <__gmpn_mu_bdiv_qr@@Base+0x748>
   506ac:	ldur	x21, [x29, #-16]
   506b0:	ldp	x2, x1, [sp, #40]
   506b4:	ldr	x3, [sp, #56]
   506b8:	add	x22, x22, x20, lsl #3
   506bc:	mov	x0, x21
   506c0:	sub	x26, x26, x20
   506c4:	bl	c2d0 <__gmpn_sub_n@plt>
   506c8:	add	x4, x0, x27
   506cc:	ldp	x28, x2, [x29, #-40]
   506d0:	ldur	x0, [x29, #-48]
   506d4:	cmp	x4, #0x2
   506d8:	b.ne	505fc <__gmpn_mu_bdiv_qr@@Base+0x6a8>  // b.any
   506dc:	mov	x8, x2
   506e0:	ldr	x9, [x8]
   506e4:	adds	x9, x9, #0x1
   506e8:	str	x9, [x8], #8
   506ec:	b.cs	506e0 <__gmpn_mu_bdiv_qr@@Base+0x78c>  // b.hs, b.nlast
   506f0:	mov	w4, #0x1                   	// #1
   506f4:	b	505fc <__gmpn_mu_bdiv_qr@@Base+0x6a8>
   506f8:	ldr	x26, [sp, #24]
   506fc:	mov	x27, xzr
   50700:	mov	x22, x23
   50704:	b	50724 <__gmpn_mu_bdiv_qr@@Base+0x7d0>
   50708:	mov	x1, x25
   5070c:	mov	x3, x20
   50710:	bl	c760 <__gmpn_sub_nc@plt>
   50714:	mov	x27, x0
   50718:	cmp	x26, x20
   5071c:	add	x25, x25, x20, lsl #3
   50720:	b.le	50424 <__gmpn_mu_bdiv_qr@@Base+0x4d0>
   50724:	ldur	x2, [x29, #-24]
   50728:	mov	x0, x22
   5072c:	mov	x1, x21
   50730:	mov	x3, x20
   50734:	bl	cec0 <__gmpn_mullo_n@plt>
   50738:	ldur	x0, [x29, #-8]
   5073c:	mov	x1, x28
   50740:	mov	x2, x19
   50744:	mov	x3, x22
   50748:	mov	x4, x20
   5074c:	bl	ccd0 <__gmpn_mul@plt>
   50750:	ldp	x2, x1, [sp, #40]
   50754:	ldr	x3, [sp, #56]
   50758:	mov	x0, x21
   5075c:	add	x22, x22, x20, lsl #3
   50760:	sub	x26, x26, x20
   50764:	bl	c2d0 <__gmpn_sub_n@plt>
   50768:	add	x4, x0, x27
   5076c:	ldur	x2, [x29, #-32]
   50770:	ldur	x0, [x29, #-48]
   50774:	cmp	x4, #0x2
   50778:	b.ne	50708 <__gmpn_mu_bdiv_qr@@Base+0x7b4>  // b.any
   5077c:	mov	x8, x2
   50780:	ldr	x9, [x8]
   50784:	adds	x9, x9, #0x1
   50788:	str	x9, [x8], #8
   5078c:	b.cs	50780 <__gmpn_mu_bdiv_qr@@Base+0x82c>  // b.hs, b.nlast
   50790:	mov	w4, #0x1                   	// #1
   50794:	b	50708 <__gmpn_mu_bdiv_qr@@Base+0x7b4>

0000000000050798 <__gmpn_mu_bdiv_qr_itch@@Base>:
   50798:	stp	x29, x30, [sp, #-48]!
   5079c:	sub	x8, x0, x1
   507a0:	stp	x20, x19, [sp, #32]
   507a4:	mov	x20, x1
   507a8:	cmp	x8, x1
   507ac:	stp	x22, x21, [sp, #16]
   507b0:	mov	x29, sp
   507b4:	b.le	507e0 <__gmpn_mu_bdiv_qr_itch@@Base+0x48>
   507b8:	sub	x8, x8, #0x1
   507bc:	sdiv	x9, x8, x20
   507c0:	add	x9, x9, #0x1
   507c4:	sdiv	x8, x8, x9
   507c8:	add	x19, x8, #0x1
   507cc:	cmp	x19, #0x11
   507d0:	b.gt	507ec <__gmpn_mu_bdiv_qr_itch@@Base+0x54>
   507d4:	mov	x22, xzr
   507d8:	add	x21, x19, x20
   507dc:	b	50814 <__gmpn_mu_bdiv_qr_itch@@Base+0x7c>
   507e0:	sub	x19, x8, x8, asr #1
   507e4:	cmp	x19, #0x11
   507e8:	b.le	507d4 <__gmpn_mu_bdiv_qr_itch@@Base+0x3c>
   507ec:	mov	x0, x20
   507f0:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   507f4:	asr	x8, x0, #1
   507f8:	cmp	x8, x19
   507fc:	csel	x9, x0, x8, lt  // lt = tstop
   50800:	cmp	x8, x20
   50804:	csel	x8, x9, xzr, lt  // lt = tstop
   50808:	add	x8, x0, x8
   5080c:	mov	x21, x0
   50810:	add	x22, x8, #0x4
   50814:	mov	x0, x19
   50818:	bl	d1e0 <__gmpn_binvert_itch@plt>
   5081c:	add	x8, x21, x22
   50820:	cmp	x8, x0
   50824:	csel	x8, x8, x0, gt
   50828:	add	x0, x8, x19
   5082c:	ldp	x20, x19, [sp, #32]
   50830:	ldp	x22, x21, [sp, #16]
   50834:	ldp	x29, x30, [sp], #48
   50838:	ret

000000000005083c <__gmpn_bdiv_q@@Base>:
   5083c:	stp	x29, x30, [sp, #-64]!
   50840:	str	x23, [sp, #16]
   50844:	stp	x22, x21, [sp, #32]
   50848:	stp	x20, x19, [sp, #48]
   5084c:	mov	x21, x5
   50850:	mov	x19, x4
   50854:	mov	x20, x3
   50858:	mov	x22, x2
   5085c:	cmp	x4, #0x5c
   50860:	mov	x23, x0
   50864:	mov	x29, sp
   50868:	b.le	5089c <__gmpn_bdiv_q@@Base+0x60>
   5086c:	cmp	x19, #0x39b
   50870:	b.le	50904 <__gmpn_bdiv_q@@Base+0xc8>
   50874:	mov	x0, x23
   50878:	mov	x2, x22
   5087c:	mov	x3, x20
   50880:	mov	x4, x19
   50884:	mov	x5, x21
   50888:	ldp	x20, x19, [sp, #48]
   5088c:	ldp	x22, x21, [sp, #32]
   50890:	ldr	x23, [sp, #16]
   50894:	ldp	x29, x30, [sp], #64
   50898:	b	d3c0 <__gmpn_mu_bdiv_q@plt>
   5089c:	mov	x0, x21
   508a0:	mov	x2, x22
   508a4:	bl	ca50 <__gmpn_copyi@plt>
   508a8:	ldr	x8, [x20]
   508ac:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   508b0:	ldr	x9, [x9, #3952]
   508b4:	mov	x0, x23
   508b8:	ubfx	x10, x8, #1, #7
   508bc:	mov	x1, x21
   508c0:	ldrb	w9, [x9, x10]
   508c4:	mov	w10, #0x2                   	// #2
   508c8:	mov	x2, x22
   508cc:	mov	x3, x20
   508d0:	msub	x11, x8, x9, x10
   508d4:	mul	x9, x11, x9
   508d8:	msub	x10, x9, x8, x10
   508dc:	mov	x4, x19
   508e0:	ldp	x20, x19, [sp, #48]
   508e4:	ldp	x22, x21, [sp, #32]
   508e8:	ldr	x23, [sp, #16]
   508ec:	mul	x9, x9, x10
   508f0:	orr	x10, xzr, #0xfffffffffffffffe
   508f4:	madd	x8, x9, x8, x10
   508f8:	mul	x5, x8, x9
   508fc:	ldp	x29, x30, [sp], #64
   50900:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>
   50904:	mov	x0, x21
   50908:	mov	x2, x22
   5090c:	bl	ca50 <__gmpn_copyi@plt>
   50910:	ldr	x8, [x20]
   50914:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   50918:	ldr	x9, [x9, #3952]
   5091c:	mov	x0, x23
   50920:	ubfx	x10, x8, #1, #7
   50924:	mov	x1, x21
   50928:	ldrb	w9, [x9, x10]
   5092c:	mov	w10, #0x2                   	// #2
   50930:	mov	x2, x22
   50934:	mov	x3, x20
   50938:	msub	x11, x8, x9, x10
   5093c:	mul	x9, x11, x9
   50940:	msub	x10, x9, x8, x10
   50944:	mov	x4, x19
   50948:	ldp	x20, x19, [sp, #48]
   5094c:	ldp	x22, x21, [sp, #32]
   50950:	ldr	x23, [sp, #16]
   50954:	mul	x9, x9, x10
   50958:	orr	x10, xzr, #0xfffffffffffffffe
   5095c:	madd	x8, x9, x8, x10
   50960:	mul	x5, x8, x9
   50964:	ldp	x29, x30, [sp], #64
   50968:	b	ce10 <__gmpn_dcpi1_bdiv_q@plt>

000000000005096c <__gmpn_bdiv_q_itch@@Base>:
   5096c:	cmp	x1, #0x39c
   50970:	b.lt	50978 <__gmpn_bdiv_q_itch@@Base+0xc>  // b.tstop
   50974:	b	c880 <__gmpn_mu_bdiv_q_itch@plt>
   50978:	ret

000000000005097c <__gmpn_bdiv_qr@@Base>:
   5097c:	stp	x29, x30, [sp, #-64]!
   50980:	stp	x24, x23, [sp, #16]
   50984:	stp	x22, x21, [sp, #32]
   50988:	stp	x20, x19, [sp, #48]
   5098c:	mov	x20, x6
   50990:	mov	x19, x5
   50994:	mov	x23, x4
   50998:	mov	x22, x3
   5099c:	mov	x21, x1
   509a0:	cmp	x5, #0x27
   509a4:	mov	x24, x0
   509a8:	mov	x29, sp
   509ac:	b.lt	509f0 <__gmpn_bdiv_qr@@Base+0x74>  // b.tstop
   509b0:	sub	x8, x22, x19
   509b4:	cmp	x8, #0x26
   509b8:	b.le	509f0 <__gmpn_bdiv_qr@@Base+0x74>
   509bc:	cmp	x19, #0x326
   509c0:	b.le	50a50 <__gmpn_bdiv_qr@@Base+0xd4>
   509c4:	mov	x0, x24
   509c8:	mov	x1, x21
   509cc:	mov	x3, x22
   509d0:	mov	x4, x23
   509d4:	mov	x5, x19
   509d8:	mov	x6, x20
   509dc:	ldp	x20, x19, [sp, #48]
   509e0:	ldp	x22, x21, [sp, #32]
   509e4:	ldp	x24, x23, [sp, #16]
   509e8:	ldp	x29, x30, [sp], #64
   509ec:	b	ccb0 <__gmpn_mu_bdiv_qr@plt>
   509f0:	mov	x0, x20
   509f4:	mov	x1, x2
   509f8:	mov	x2, x22
   509fc:	bl	ca50 <__gmpn_copyi@plt>
   50a00:	ldr	x8, [x23]
   50a04:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   50a08:	ldr	x9, [x9, #3952]
   50a0c:	mov	x0, x24
   50a10:	ubfx	x10, x8, #1, #7
   50a14:	mov	x1, x20
   50a18:	ldrb	w9, [x9, x10]
   50a1c:	mov	w10, #0x2                   	// #2
   50a20:	mov	x2, x22
   50a24:	mov	x3, x23
   50a28:	msub	x11, x8, x9, x10
   50a2c:	mul	x9, x11, x9
   50a30:	msub	x10, x9, x8, x10
   50a34:	mul	x9, x9, x10
   50a38:	orr	x10, xzr, #0xfffffffffffffffe
   50a3c:	madd	x8, x9, x8, x10
   50a40:	mul	x5, x8, x9
   50a44:	mov	x4, x19
   50a48:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   50a4c:	b	50aac <__gmpn_bdiv_qr@@Base+0x130>
   50a50:	mov	x0, x20
   50a54:	mov	x1, x2
   50a58:	mov	x2, x22
   50a5c:	bl	ca50 <__gmpn_copyi@plt>
   50a60:	ldr	x8, [x23]
   50a64:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   50a68:	ldr	x9, [x9, #3952]
   50a6c:	mov	x0, x24
   50a70:	ubfx	x10, x8, #1, #7
   50a74:	mov	x1, x20
   50a78:	ldrb	w9, [x9, x10]
   50a7c:	mov	w10, #0x2                   	// #2
   50a80:	mov	x2, x22
   50a84:	mov	x3, x23
   50a88:	msub	x11, x8, x9, x10
   50a8c:	mul	x9, x11, x9
   50a90:	msub	x10, x9, x8, x10
   50a94:	mul	x9, x9, x10
   50a98:	orr	x10, xzr, #0xfffffffffffffffe
   50a9c:	madd	x8, x9, x8, x10
   50aa0:	mul	x5, x8, x9
   50aa4:	mov	x4, x19
   50aa8:	bl	c5e0 <__gmpn_dcpi1_bdiv_qr@plt>
   50aac:	add	x8, x20, x22, lsl #3
   50ab0:	mov	x23, x0
   50ab4:	sub	x1, x8, x19, lsl #3
   50ab8:	mov	x0, x21
   50abc:	mov	x2, x19
   50ac0:	bl	ca50 <__gmpn_copyi@plt>
   50ac4:	mov	x0, x23
   50ac8:	ldp	x20, x19, [sp, #48]
   50acc:	ldp	x22, x21, [sp, #32]
   50ad0:	ldp	x24, x23, [sp, #16]
   50ad4:	ldp	x29, x30, [sp], #64
   50ad8:	ret

0000000000050adc <__gmpn_bdiv_qr_itch@@Base>:
   50adc:	cmp	x1, #0x327
   50ae0:	b.lt	50ae8 <__gmpn_bdiv_qr_itch@@Base+0xc>  // b.tstop
   50ae4:	b	d170 <__gmpn_mu_bdiv_qr_itch@plt>
   50ae8:	ret

0000000000050aec <__gmpn_broot_invm1@@Base>:
   50aec:	stp	x29, x30, [sp, #-96]!
   50af0:	stp	x28, x27, [sp, #16]
   50af4:	stp	x26, x25, [sp, #32]
   50af8:	stp	x24, x23, [sp, #48]
   50afc:	stp	x22, x21, [sp, #64]
   50b00:	stp	x20, x19, [sp, #80]
   50b04:	mov	x29, sp
   50b08:	sub	sp, sp, #0x440
   50b0c:	mov	x23, x1
   50b10:	lsl	x1, x2, #5
   50b14:	mov	w8, #0x7f00                	// #32512
   50b18:	mov	x19, sp
   50b1c:	mov	x20, x3
   50b20:	mov	x25, x2
   50b24:	cmp	x1, x8
   50b28:	stp	x0, xzr, [x19, #24]
   50b2c:	b.hi	50ddc <__gmpn_broot_invm1@@Base+0x2f0>  // b.pmore
   50b30:	add	x9, x1, #0xf
   50b34:	mov	x8, sp
   50b38:	and	x9, x9, #0xfffffffffffffff0
   50b3c:	sub	x22, x8, x9
   50b40:	mov	sp, x22
   50b44:	add	x5, x22, x25, lsl #3
   50b48:	sub	x8, x20, #0x1
   50b4c:	add	x2, x19, #0x30
   50b50:	mov	w3, #0x1                   	// #1
   50b54:	mov	x0, x22
   50b58:	mov	x1, x23
   50b5c:	mov	x4, x25
   50b60:	str	x8, [x19, #48]
   50b64:	mov	w21, #0x1                   	// #1
   50b68:	str	x5, [x19, #16]
   50b6c:	bl	c3c0 <__gmpn_powlo@plt>
   50b70:	adrp	x11, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   50b74:	ldr	w9, [x23]
   50b78:	ldr	x11, [x11, #3952]
   50b7c:	ubfx	x10, x20, #1, #7
   50b80:	mov	w13, #0x2                   	// #2
   50b84:	lsl	w12, w9, #1
   50b88:	ldrb	w11, [x11, x10]
   50b8c:	eor	w9, w12, w9, lsl #2
   50b90:	and	w9, w9, w20, lsl #2
   50b94:	ldr	x8, [x22]
   50b98:	msub	x12, x11, x20, x13
   50b9c:	mul	x11, x12, x11
   50ba0:	msub	x12, x11, x20, x13
   50ba4:	and	x9, x9, #0x8
   50ba8:	mul	x11, x11, x12
   50bac:	orr	x12, x9, #0x1
   50bb0:	msub	x9, x11, x20, x13
   50bb4:	mul	x24, x11, x9
   50bb8:	ands	x10, x20, #0x7f
   50bbc:	mul	x11, x24, x12
   50bc0:	add	x9, x20, #0x1
   50bc4:	b.eq	50be4 <__gmpn_broot_invm1@@Base+0xf8>  // b.none
   50bc8:	mov	w21, #0x1                   	// #1
   50bcc:	tst	x10, #0x1
   50bd0:	csinc	x13, x12, xzr, ne  // ne = any
   50bd4:	lsr	x10, x10, #1
   50bd8:	mul	x21, x13, x21
   50bdc:	mul	x12, x12, x12
   50be0:	cbnz	x10, 50bcc <__gmpn_broot_invm1@@Base+0xe0>
   50be4:	msub	x10, x21, x8, x9
   50be8:	mul	x11, x11, x10
   50bec:	ands	x13, x20, #0x7fff
   50bf0:	mul	x10, x11, x24
   50bf4:	mov	w12, #0x1                   	// #1
   50bf8:	b.eq	50c14 <__gmpn_broot_invm1@@Base+0x128>  // b.none
   50bfc:	tst	x13, #0x1
   50c00:	csinc	x14, x11, xzr, ne  // ne = any
   50c04:	lsr	x13, x13, #1
   50c08:	mul	x12, x14, x12
   50c0c:	mul	x11, x11, x11
   50c10:	cbnz	x13, 50bfc <__gmpn_broot_invm1@@Base+0x110>
   50c14:	msub	x11, x12, x8, x9
   50c18:	mul	x11, x10, x11
   50c1c:	mul	x10, x11, x24
   50c20:	cbz	x20, 50c74 <__gmpn_broot_invm1@@Base+0x188>
   50c24:	mov	w12, #0x1                   	// #1
   50c28:	mov	x13, x20
   50c2c:	tst	x13, #0x1
   50c30:	csinc	x14, x11, xzr, ne  // ne = any
   50c34:	lsr	x13, x13, #1
   50c38:	mul	x12, x14, x12
   50c3c:	mul	x11, x11, x11
   50c40:	cbnz	x13, 50c2c <__gmpn_broot_invm1@@Base+0x140>
   50c44:	msub	x11, x12, x8, x9
   50c48:	mul	x10, x11, x10
   50c4c:	mov	w11, #0x1                   	// #1
   50c50:	mov	x12, x10
   50c54:	mov	x13, x20
   50c58:	tst	x13, #0x1
   50c5c:	csinc	x14, x12, xzr, ne  // ne = any
   50c60:	lsr	x13, x13, #1
   50c64:	mul	x11, x14, x11
   50c68:	mul	x12, x12, x12
   50c6c:	cbnz	x13, 50c58 <__gmpn_broot_invm1@@Base+0x16c>
   50c70:	b	50c80 <__gmpn_broot_invm1@@Base+0x194>
   50c74:	sub	x11, x9, x8
   50c78:	mul	x10, x10, x11
   50c7c:	mov	w11, #0x1                   	// #1
   50c80:	msub	x8, x11, x8, x9
   50c84:	ldr	x9, [x19, #24]
   50c88:	mul	x10, x10, x24
   50c8c:	mul	x8, x10, x8
   50c90:	cmp	x25, #0x1
   50c94:	str	x8, [x9]
   50c98:	b.eq	50e00 <__gmpn_broot_invm1@@Base+0x314>  // b.none
   50c9c:	mov	w1, #0x8                   	// #8
   50ca0:	lsr	x8, x20, #1
   50ca4:	bfi	x1, x25, #4, #60
   50ca8:	mov	w9, #0x7f00                	// #32512
   50cac:	add	x8, x8, #0x1
   50cb0:	cmp	x1, x9
   50cb4:	str	x8, [x19, #40]
   50cb8:	str	x22, [x19, #8]
   50cbc:	b.hi	50dec <__gmpn_broot_invm1@@Base+0x300>  // b.pmore
   50cc0:	add	x9, x1, #0xf
   50cc4:	mov	x8, sp
   50cc8:	and	x9, x9, #0xfffffffffffffff0
   50ccc:	sub	x26, x8, x9
   50cd0:	mov	sp, x26
   50cd4:	cmp	x25, #0x2
   50cd8:	b.lt	50e00 <__gmpn_broot_invm1@@Base+0x314>  // b.tstop
   50cdc:	mov	w9, wzr
   50ce0:	add	x27, x26, x25, lsl #3
   50ce4:	add	x8, x19, #0x38
   50ce8:	add	x10, x25, #0x1
   50cec:	add	x11, x25, #0x2
   50cf0:	cmp	x10, #0x0
   50cf4:	csinc	x10, x11, x25, lt  // lt = tstop
   50cf8:	str	x25, [x8, w9, uxtw #3]
   50cfc:	cmp	x25, #0x2
   50d00:	asr	x25, x10, #1
   50d04:	add	w9, w9, #0x1
   50d08:	b.gt	50ce8 <__gmpn_broot_invm1@@Base+0x1fc>
   50d0c:	cbz	w9, 50e00 <__gmpn_broot_invm1@@Base+0x314>
   50d10:	mov	w22, w9
   50d14:	mov	w25, #0x1                   	// #1
   50d18:	b	50d24 <__gmpn_broot_invm1@@Base+0x238>
   50d1c:	sub	x22, x22, #0x1
   50d20:	cbz	w21, 50e00 <__gmpn_broot_invm1@@Base+0x314>
   50d24:	ldr	x28, [x19, #24]
   50d28:	mov	x0, x27
   50d2c:	mov	x2, x25
   50d30:	mov	x23, x25
   50d34:	mov	x1, x28
   50d38:	sub	w21, w22, #0x1
   50d3c:	bl	c8e0 <__gmpn_sqr@plt>
   50d40:	add	x8, x19, #0x38
   50d44:	ldr	x25, [x8, w21, uxtw #3]
   50d48:	ldr	x5, [x19, #16]
   50d4c:	add	x2, x19, #0x28
   50d50:	mov	w3, #0x1                   	// #1
   50d54:	mov	x0, x26
   50d58:	mov	x1, x27
   50d5c:	mov	x4, x25
   50d60:	bl	c3c0 <__gmpn_powlo@plt>
   50d64:	ldr	x2, [x19, #8]
   50d68:	mov	x0, x27
   50d6c:	mov	x1, x26
   50d70:	mov	x3, x25
   50d74:	bl	cec0 <__gmpn_mullo_n@plt>
   50d78:	lsl	x8, x23, #3
   50d7c:	add	x28, x28, x8
   50d80:	sub	x23, x25, x23
   50d84:	add	x1, x27, x8
   50d88:	mov	x0, x28
   50d8c:	mov	x2, x23
   50d90:	mov	x3, x20
   50d94:	mov	x4, x24
   50d98:	mov	w5, wzr
   50d9c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   50da0:	ldr	x8, [x28]
   50da4:	cbnz	x8, 50dbc <__gmpn_broot_invm1@@Base+0x2d0>
   50da8:	subs	x23, x23, #0x1
   50dac:	str	xzr, [x28]
   50db0:	b.eq	50d1c <__gmpn_broot_invm1@@Base+0x230>  // b.none
   50db4:	ldr	x8, [x28, #8]!
   50db8:	cbz	x8, 50da8 <__gmpn_broot_invm1@@Base+0x2bc>
   50dbc:	neg	x8, x8
   50dc0:	subs	x2, x23, #0x1
   50dc4:	str	x8, [x28]
   50dc8:	b.eq	50d1c <__gmpn_broot_invm1@@Base+0x230>  // b.none
   50dcc:	add	x0, x28, #0x8
   50dd0:	mov	x1, x0
   50dd4:	bl	c290 <__gmpn_com@plt>
   50dd8:	b	50d1c <__gmpn_broot_invm1@@Base+0x230>
   50ddc:	add	x0, x19, #0x20
   50de0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50de4:	mov	x22, x0
   50de8:	b	50b44 <__gmpn_broot_invm1@@Base+0x58>
   50dec:	add	x0, x19, #0x20
   50df0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50df4:	mov	x26, x0
   50df8:	cmp	x25, #0x2
   50dfc:	b.ge	50cdc <__gmpn_broot_invm1@@Base+0x1f0>  // b.tcont
   50e00:	ldr	x0, [x19, #32]
   50e04:	cbnz	x0, 50e28 <__gmpn_broot_invm1@@Base+0x33c>
   50e08:	mov	sp, x29
   50e0c:	ldp	x20, x19, [sp, #80]
   50e10:	ldp	x22, x21, [sp, #64]
   50e14:	ldp	x24, x23, [sp, #48]
   50e18:	ldp	x26, x25, [sp, #32]
   50e1c:	ldp	x28, x27, [sp, #16]
   50e20:	ldp	x29, x30, [sp], #96
   50e24:	ret
   50e28:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50e2c:	b	50e08 <__gmpn_broot_invm1@@Base+0x31c>

0000000000050e30 <__gmpn_broot@@Base>:
   50e30:	stp	x29, x30, [sp, #-64]!
   50e34:	stp	x22, x21, [sp, #32]
   50e38:	stp	x20, x19, [sp, #48]
   50e3c:	mov	x19, x2
   50e40:	mov	x20, x1
   50e44:	cmp	x3, #0x1
   50e48:	mov	x21, x0
   50e4c:	str	x23, [sp, #16]
   50e50:	mov	x29, sp
   50e54:	b.ne	50e6c <__gmpn_broot@@Base+0x3c>  // b.any
   50e58:	mov	x0, x21
   50e5c:	mov	x1, x20
   50e60:	mov	x2, x19
   50e64:	bl	ca50 <__gmpn_copyi@plt>
   50e68:	b	50ec8 <__gmpn_broot@@Base+0x98>
   50e6c:	lsl	x1, x19, #3
   50e70:	mov	w8, #0x7f00                	// #32512
   50e74:	mov	x22, x3
   50e78:	cmp	x1, x8
   50e7c:	str	xzr, [x29, #24]
   50e80:	b.hi	50ee0 <__gmpn_broot@@Base+0xb0>  // b.pmore
   50e84:	add	x9, x1, #0xf
   50e88:	mov	x8, sp
   50e8c:	and	x9, x9, #0xfffffffffffffff0
   50e90:	sub	x23, x8, x9
   50e94:	mov	sp, x23
   50e98:	mov	x0, x23
   50e9c:	mov	x1, x20
   50ea0:	mov	x2, x19
   50ea4:	mov	x3, x22
   50ea8:	bl	c930 <__gmpn_broot_invm1@plt>
   50eac:	mov	x0, x21
   50eb0:	mov	x1, x23
   50eb4:	mov	x2, x20
   50eb8:	mov	x3, x19
   50ebc:	bl	cec0 <__gmpn_mullo_n@plt>
   50ec0:	ldr	x0, [x29, #24]
   50ec4:	cbnz	x0, 50ef0 <__gmpn_broot@@Base+0xc0>
   50ec8:	mov	sp, x29
   50ecc:	ldp	x20, x19, [sp, #48]
   50ed0:	ldp	x22, x21, [sp, #32]
   50ed4:	ldr	x23, [sp, #16]
   50ed8:	ldp	x29, x30, [sp], #64
   50edc:	ret
   50ee0:	add	x0, x29, #0x18
   50ee4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50ee8:	mov	x23, x0
   50eec:	b	50e98 <__gmpn_broot@@Base+0x68>
   50ef0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50ef4:	b	50ec8 <__gmpn_broot@@Base+0x98>

0000000000050ef8 <__gmpn_brootinv@@Base>:
   50ef8:	stp	x29, x30, [sp, #-96]!
   50efc:	stp	x28, x27, [sp, #16]
   50f00:	stp	x26, x25, [sp, #32]
   50f04:	stp	x24, x23, [sp, #48]
   50f08:	stp	x22, x21, [sp, #64]
   50f0c:	stp	x20, x19, [sp, #80]
   50f10:	mov	x29, sp
   50f14:	sub	sp, sp, #0x220
   50f18:	adrp	x12, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   50f1c:	ldr	x12, [x12, #3952]
   50f20:	add	x8, x2, #0x3
   50f24:	lsr	x9, x3, #1
   50f28:	ubfx	x10, x3, #1, #7
   50f2c:	asr	x11, x8, #1
   50f30:	add	x8, x9, #0x1
   50f34:	ldrb	w12, [x12, x10]
   50f38:	stur	x8, [x29, #-16]
   50f3c:	ldr	x10, [x1]
   50f40:	mov	w14, #0x2                   	// #2
   50f44:	msub	x13, x12, x3, x14
   50f48:	mul	x12, x13, x12
   50f4c:	msub	x13, x12, x3, x14
   50f50:	lsl	w15, w10, #1
   50f54:	mul	x13, x12, x13
   50f58:	eor	w12, w15, w10, lsl #2
   50f5c:	and	w12, w12, w8, lsl #3
   50f60:	and	x12, x12, #0x8
   50f64:	eor	x12, x12, x10
   50f68:	mov	x19, x4
   50f6c:	mov	x20, x3
   50f70:	mov	x22, x0
   50f74:	lsl	x9, x8, #1
   50f78:	mov	x28, x1
   50f7c:	msub	x14, x13, x3, x14
   50f80:	and	x16, x8, #0x3f
   50f84:	mov	w15, #0x1                   	// #1
   50f88:	mov	x17, x12
   50f8c:	mul	x17, x17, x17
   50f90:	tst	x16, #0x1
   50f94:	csinc	x18, x17, xzr, ne  // ne = any
   50f98:	lsr	x16, x16, #1
   50f9c:	mul	x15, x18, x15
   50fa0:	cbnz	x16, 50f8c <__gmpn_brootinv@@Base+0x94>
   50fa4:	add	x23, x19, x2, lsl #3
   50fa8:	mul	x24, x13, x14
   50fac:	mul	x13, x15, x10
   50fb0:	add	x25, x23, x11, lsl #3
   50fb4:	neg	x11, x13
   50fb8:	madd	x11, x9, x12, x11
   50fbc:	mul	x11, x11, x24
   50fc0:	and	x13, x8, #0x3fff
   50fc4:	mov	w12, #0x1                   	// #1
   50fc8:	mov	x14, x11
   50fcc:	mul	x14, x14, x14
   50fd0:	tst	x13, #0x1
   50fd4:	csinc	x15, x14, xzr, ne  // ne = any
   50fd8:	lsr	x13, x13, #1
   50fdc:	mul	x12, x15, x12
   50fe0:	cbnz	x13, 50fcc <__gmpn_brootinv@@Base+0xd4>
   50fe4:	mul	x12, x12, x10
   50fe8:	neg	x12, x12
   50fec:	madd	x11, x9, x11, x12
   50ff0:	mul	x11, x11, x24
   50ff4:	mov	w12, #0x1                   	// #1
   50ff8:	mov	x13, x11
   50ffc:	mov	x14, x8
   51000:	mul	x13, x13, x13
   51004:	tst	x14, #0x1
   51008:	csinc	x15, x13, xzr, ne  // ne = any
   5100c:	lsr	x14, x14, #1
   51010:	mul	x12, x15, x12
   51014:	cbnz	x14, 51000 <__gmpn_brootinv@@Base+0x108>
   51018:	mul	x12, x12, x10
   5101c:	neg	x12, x12
   51020:	madd	x11, x9, x11, x12
   51024:	mul	x11, x11, x24
   51028:	mov	w12, #0x1                   	// #1
   5102c:	mov	x13, x11
   51030:	mul	x13, x13, x13
   51034:	tst	x8, #0x1
   51038:	csinc	x14, x13, xzr, ne  // ne = any
   5103c:	lsr	x8, x8, #1
   51040:	mul	x12, x14, x12
   51044:	cbnz	x8, 51030 <__gmpn_brootinv@@Base+0x138>
   51048:	mul	x8, x12, x10
   5104c:	neg	x8, x8
   51050:	madd	x8, x9, x11, x8
   51054:	mul	x8, x8, x24
   51058:	cmp	x2, #0x1
   5105c:	str	x8, [x22]
   51060:	b.eq	511a4 <__gmpn_brootinv@@Base+0x2ac>  // b.none
   51064:	cmp	x2, #0x2
   51068:	b.ne	51074 <__gmpn_brootinv@@Base+0x17c>  // b.any
   5106c:	mov	w8, wzr
   51070:	b	51094 <__gmpn_brootinv@@Base+0x19c>
   51074:	mov	x8, xzr
   51078:	add	x9, sp, #0x8
   5107c:	add	x10, x2, #0x1
   51080:	str	x2, [x9, x8, lsl #3]
   51084:	asr	x2, x10, #1
   51088:	cmp	x2, #0x2
   5108c:	add	x8, x8, #0x1
   51090:	b.ne	5107c <__gmpn_brootinv@@Base+0x184>  // b.any
   51094:	mov	w9, #0x2                   	// #2
   51098:	add	x10, sp, #0x8
   5109c:	sxtw	x21, w8
   510a0:	mov	w26, #0x1                   	// #1
   510a4:	str	x9, [x10, w8, uxtw #3]
   510a8:	b	510dc <__gmpn_brootinv@@Base+0x1e4>
   510ac:	mov	x1, x0
   510b0:	bl	c290 <__gmpn_com@plt>
   510b4:	mov	x0, x22
   510b8:	mov	x1, x19
   510bc:	mov	x2, x26
   510c0:	mov	x3, x20
   510c4:	mov	x4, x24
   510c8:	mov	w5, wzr
   510cc:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   510d0:	cmp	x21, #0x0
   510d4:	sub	x21, x21, #0x1
   510d8:	b.le	511a4 <__gmpn_brootinv@@Base+0x2ac>
   510dc:	mov	x0, x19
   510e0:	mov	x1, x22
   510e4:	mov	x2, x26
   510e8:	bl	c8e0 <__gmpn_sqr@plt>
   510ec:	ldur	x8, [x29, #-16]
   510f0:	mov	x0, x23
   510f4:	mov	x1, x22
   510f8:	mov	x2, x26
   510fc:	lsl	x3, x8, #1
   51100:	bl	d490 <__gmpn_mul_1@plt>
   51104:	str	x0, [x23, x26, lsl #3]
   51108:	add	x8, sp, #0x8
   5110c:	ldr	x26, [x8, x21, lsl #3]
   51110:	sub	x2, x29, #0x10
   51114:	mov	w3, #0x1                   	// #1
   51118:	mov	x0, x22
   5111c:	mov	x1, x19
   51120:	mov	x4, x26
   51124:	mov	x5, x25
   51128:	bl	c3c0 <__gmpn_powlo@plt>
   5112c:	mov	x0, x19
   51130:	mov	x1, x28
   51134:	mov	x2, x22
   51138:	mov	x3, x26
   5113c:	bl	cec0 <__gmpn_mullo_n@plt>
   51140:	add	x8, x26, #0x3
   51144:	asr	x27, x8, #1
   51148:	mov	x0, x19
   5114c:	mov	x1, x23
   51150:	mov	x2, x19
   51154:	mov	x3, x27
   51158:	bl	c2d0 <__gmpn_sub_n@plt>
   5115c:	subs	x2, x26, x27
   51160:	b.le	510b4 <__gmpn_brootinv@@Base+0x1bc>
   51164:	mov	x8, x0
   51168:	add	x0, x19, x27, lsl #3
   5116c:	cbnz	x8, 510ac <__gmpn_brootinv@@Base+0x1b4>
   51170:	ldr	x8, [x0]
   51174:	cbnz	x8, 5118c <__gmpn_brootinv@@Base+0x294>
   51178:	subs	x2, x2, #0x1
   5117c:	str	xzr, [x0]
   51180:	b.eq	510b4 <__gmpn_brootinv@@Base+0x1bc>  // b.none
   51184:	ldr	x8, [x0, #8]!
   51188:	cbz	x8, 51178 <__gmpn_brootinv@@Base+0x280>
   5118c:	neg	x8, x8
   51190:	subs	x2, x2, #0x1
   51194:	str	x8, [x0]
   51198:	b.eq	510b4 <__gmpn_brootinv@@Base+0x1bc>  // b.none
   5119c:	add	x0, x0, #0x8
   511a0:	b	510ac <__gmpn_brootinv@@Base+0x1b4>
   511a4:	add	sp, sp, #0x220
   511a8:	ldp	x20, x19, [sp, #80]
   511ac:	ldp	x22, x21, [sp, #64]
   511b0:	ldp	x24, x23, [sp, #48]
   511b4:	ldp	x26, x25, [sp, #32]
   511b8:	ldp	x28, x27, [sp, #16]
   511bc:	ldp	x29, x30, [sp], #96
   511c0:	ret

00000000000511c4 <__gmpn_bsqrt@@Base>:
   511c4:	stp	x29, x30, [sp, #-48]!
   511c8:	stp	x22, x21, [sp, #16]
   511cc:	stp	x20, x19, [sp, #32]
   511d0:	mov	x19, x3
   511d4:	lsr	x22, x2, #6
   511d8:	mov	x21, x0
   511dc:	add	x3, x3, x22, lsl #3
   511e0:	mov	x0, x19
   511e4:	mov	x29, sp
   511e8:	mov	x20, x1
   511ec:	bl	c750 <__gmpn_bsqrtinv@plt>
   511f0:	mov	x0, x21
   511f4:	mov	x1, x19
   511f8:	mov	x2, x20
   511fc:	mov	x3, x22
   51200:	ldp	x20, x19, [sp, #32]
   51204:	ldp	x22, x21, [sp, #16]
   51208:	ldp	x29, x30, [sp], #48
   5120c:	b	cec0 <__gmpn_mullo_n@plt>

0000000000051210 <__gmpn_bsqrtinv@@Base>:
   51210:	stp	x29, x30, [sp, #-80]!
   51214:	stp	x28, x25, [sp, #16]
   51218:	stp	x24, x23, [sp, #32]
   5121c:	stp	x22, x21, [sp, #48]
   51220:	stp	x20, x19, [sp, #64]
   51224:	mov	x29, sp
   51228:	sub	sp, sp, #0x210
   5122c:	mov	w8, #0x1                   	// #1
   51230:	str	x8, [x0]
   51234:	ldr	x8, [x1]
   51238:	cmp	x2, #0x1
   5123c:	b.ne	51250 <__gmpn_bsqrtinv@@Base+0x40>  // b.any
   51240:	and	x8, x8, #0x3
   51244:	cmp	x8, #0x1
   51248:	b.eq	51328 <__gmpn_bsqrtinv@@Base+0x118>  // b.none
   5124c:	b	51330 <__gmpn_bsqrtinv@@Base+0x120>
   51250:	and	x8, x8, #0x7
   51254:	cmp	x8, #0x1
   51258:	b.ne	51330 <__gmpn_bsqrtinv@@Base+0x120>  // b.any
   5125c:	cmp	x2, #0x2
   51260:	b.eq	51328 <__gmpn_bsqrtinv@@Base+0x118>  // b.none
   51264:	lsr	x9, x2, #3
   51268:	and	x9, x9, #0x1ffffffffffffff8
   5126c:	add	x9, x9, x3
   51270:	mov	x19, x1
   51274:	mov	x20, x0
   51278:	mov	x21, x3
   5127c:	mov	x8, xzr
   51280:	add	x22, x9, #0x8
   51284:	add	x9, sp, #0x8
   51288:	add	x10, x2, #0x2
   5128c:	str	x2, [x9, x8, lsl #3]
   51290:	lsr	x2, x10, #1
   51294:	cmp	x2, #0x2
   51298:	add	x8, x8, #0x1
   5129c:	b.ne	51288 <__gmpn_bsqrtinv@@Base+0x78>  // b.any
   512a0:	cmp	w8, #0x1
   512a4:	b.lt	51328 <__gmpn_bsqrtinv@@Base+0x118>  // b.tstop
   512a8:	and	x24, x8, #0xffffffff
   512ac:	add	x8, sp, #0x8
   512b0:	sub	x25, x8, #0x8
   512b4:	ldr	x8, [x25, x24, lsl #3]
   512b8:	mov	x0, x21
   512bc:	mov	x1, x20
   512c0:	lsr	x8, x8, #6
   512c4:	add	x23, x8, #0x1
   512c8:	mov	x2, x23
   512cc:	bl	c9f0 <__gmpn_sqrlo@plt>
   512d0:	mov	x0, x22
   512d4:	mov	x1, x20
   512d8:	mov	x2, x21
   512dc:	mov	x3, x23
   512e0:	bl	cec0 <__gmpn_mullo_n@plt>
   512e4:	mov	w3, #0x3                   	// #3
   512e8:	mov	x0, x21
   512ec:	mov	x1, x20
   512f0:	mov	x2, x23
   512f4:	bl	d490 <__gmpn_mul_1@plt>
   512f8:	mov	x0, x20
   512fc:	mov	x1, x19
   51300:	mov	x2, x22
   51304:	mov	x3, x23
   51308:	bl	cec0 <__gmpn_mullo_n@plt>
   5130c:	mov	x0, x20
   51310:	mov	x1, x21
   51314:	mov	x2, x20
   51318:	mov	x3, x23
   5131c:	bl	c840 <__gmpn_rsh1sub_n@plt>
   51320:	subs	x24, x24, #0x1
   51324:	b.gt	512b4 <__gmpn_bsqrtinv@@Base+0xa4>
   51328:	mov	w0, #0x1                   	// #1
   5132c:	b	51334 <__gmpn_bsqrtinv@@Base+0x124>
   51330:	mov	w0, wzr
   51334:	add	sp, sp, #0x210
   51338:	ldp	x20, x19, [sp, #64]
   5133c:	ldp	x22, x21, [sp, #48]
   51340:	ldp	x24, x23, [sp, #32]
   51344:	ldp	x28, x25, [sp, #16]
   51348:	ldp	x29, x30, [sp], #80
   5134c:	ret

0000000000051350 <__gmpn_divexact@@Base>:
   51350:	stp	x29, x30, [sp, #-96]!
   51354:	stp	x26, x25, [sp, #32]
   51358:	stp	x24, x23, [sp, #48]
   5135c:	stp	x22, x21, [sp, #64]
   51360:	stp	x20, x19, [sp, #80]
   51364:	mov	x23, x3
   51368:	ldr	x3, [x3]
   5136c:	mov	x22, x4
   51370:	mov	x20, x1
   51374:	mov	x19, x0
   51378:	str	x27, [sp, #16]
   5137c:	mov	x29, sp
   51380:	cbnz	x3, 51398 <__gmpn_divexact@@Base+0x48>
   51384:	ldr	x3, [x23, #8]!
   51388:	add	x20, x20, #0x8
   5138c:	sub	x22, x22, #0x1
   51390:	sub	x2, x2, #0x1
   51394:	cbz	x3, 51384 <__gmpn_divexact@@Base+0x34>
   51398:	cmp	x22, #0x1
   5139c:	b.ne	513b0 <__gmpn_divexact@@Base+0x60>  // b.any
   513a0:	mov	x0, x19
   513a4:	mov	x1, x20
   513a8:	bl	c770 <__gmpn_divexact_1@plt>
   513ac:	b	51500 <__gmpn_divexact@@Base+0x1b0>
   513b0:	sub	x8, x2, x22
   513b4:	rbit	x9, x3
   513b8:	clz	x24, x9
   513bc:	add	x21, x8, #0x1
   513c0:	str	xzr, [x29, #24]
   513c4:	cbz	w24, 5144c <__gmpn_divexact@@Base+0xfc>
   513c8:	cmp	x22, x21
   513cc:	csinc	x27, x22, x21, le
   513d0:	lsl	x1, x27, #3
   513d4:	mov	w8, #0x7f00                	// #32512
   513d8:	cmp	x1, x8
   513dc:	add	x25, x21, #0x1
   513e0:	b.hi	51520 <__gmpn_divexact@@Base+0x1d0>  // b.pmore
   513e4:	add	x9, x1, #0xf
   513e8:	mov	x8, sp
   513ec:	and	x9, x9, #0xfffffffffffffff0
   513f0:	sub	x26, x8, x9
   513f4:	mov	sp, x26
   513f8:	mov	x0, x26
   513fc:	mov	x1, x23
   51400:	mov	x2, x27
   51404:	mov	w3, w24
   51408:	bl	c1a0 <__gmpn_rshift@plt>
   5140c:	lsl	x1, x25, #3
   51410:	mov	w8, #0x7f00                	// #32512
   51414:	cmp	x1, x8
   51418:	b.hi	51530 <__gmpn_divexact@@Base+0x1e0>  // b.pmore
   5141c:	add	x9, x1, #0xf
   51420:	mov	x8, sp
   51424:	and	x9, x9, #0xfffffffffffffff0
   51428:	sub	x27, x8, x9
   5142c:	mov	sp, x27
   51430:	mov	x0, x27
   51434:	mov	x1, x20
   51438:	mov	x2, x25
   5143c:	mov	w3, w24
   51440:	bl	c1a0 <__gmpn_rshift@plt>
   51444:	mov	x23, x26
   51448:	mov	x20, x27
   5144c:	cmp	x22, x21
   51450:	csel	x22, x21, x22, gt
   51454:	mov	x0, x21
   51458:	mov	x1, x22
   5145c:	bl	d380 <__gmpn_bdiv_q_itch@plt>
   51460:	lsl	x1, x0, #3
   51464:	mov	w8, #0x7f00                	// #32512
   51468:	cmp	x1, x8
   5146c:	b.hi	514cc <__gmpn_divexact@@Base+0x17c>  // b.pmore
   51470:	add	x9, x1, #0xf
   51474:	mov	x8, sp
   51478:	and	x9, x9, #0xfffffffffffffff0
   5147c:	sub	x5, x8, x9
   51480:	mov	sp, x5
   51484:	mov	x0, x19
   51488:	mov	x1, x20
   5148c:	mov	x2, x21
   51490:	mov	x3, x23
   51494:	mov	x4, x22
   51498:	bl	ca30 <__gmpn_bdiv_q@plt>
   5149c:	ldr	x0, [x29, #24]
   514a0:	cbnz	x0, 514dc <__gmpn_divexact@@Base+0x18c>
   514a4:	ldr	x8, [x19]
   514a8:	cbz	x8, 514f4 <__gmpn_divexact@@Base+0x1a4>
   514ac:	neg	x8, x8
   514b0:	subs	x2, x21, #0x1
   514b4:	str	x8, [x19]
   514b8:	b.eq	51500 <__gmpn_divexact@@Base+0x1b0>  // b.none
   514bc:	add	x0, x19, #0x8
   514c0:	mov	x1, x0
   514c4:	bl	c290 <__gmpn_com@plt>
   514c8:	b	51500 <__gmpn_divexact@@Base+0x1b0>
   514cc:	add	x0, x29, #0x18
   514d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   514d4:	mov	x5, x0
   514d8:	b	51484 <__gmpn_divexact@@Base+0x134>
   514dc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   514e0:	ldr	x8, [x19]
   514e4:	cbnz	x8, 514ac <__gmpn_divexact@@Base+0x15c>
   514e8:	b	514f4 <__gmpn_divexact@@Base+0x1a4>
   514ec:	ldr	x8, [x19, #8]!
   514f0:	cbnz	x8, 514ac <__gmpn_divexact@@Base+0x15c>
   514f4:	subs	x21, x21, #0x1
   514f8:	str	xzr, [x19]
   514fc:	b.ne	514ec <__gmpn_divexact@@Base+0x19c>  // b.any
   51500:	mov	sp, x29
   51504:	ldp	x20, x19, [sp, #80]
   51508:	ldp	x22, x21, [sp, #64]
   5150c:	ldp	x24, x23, [sp, #48]
   51510:	ldp	x26, x25, [sp, #32]
   51514:	ldr	x27, [sp, #16]
   51518:	ldp	x29, x30, [sp], #96
   5151c:	ret
   51520:	add	x0, x29, #0x18
   51524:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   51528:	mov	x26, x0
   5152c:	b	513f8 <__gmpn_divexact@@Base+0xa8>
   51530:	add	x0, x29, #0x18
   51534:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   51538:	mov	x27, x0
   5153c:	b	51430 <__gmpn_divexact@@Base+0xe0>

0000000000051540 <__gmpn_bdiv_dbm1c@@Base>:
   51540:	ldr	x5, [x1], #8
   51544:	ands	x6, x2, #0x3
   51548:	b.eq	51568 <__gmpn_bdiv_dbm1c@@Base+0x28>  // b.none
   5154c:	cmp	x6, #0x2
   51550:	b.cc	51578 <__gmpn_bdiv_dbm1c@@Base+0x38>  // b.lo, b.ul, b.last
   51554:	b.eq	51590 <__gmpn_bdiv_dbm1c@@Base+0x50>  // b.none
   51558:	mul	x12, x5, x3
   5155c:	umulh	x13, x5, x3
   51560:	ldr	x5, [x1], #8
   51564:	b	515e0 <__gmpn_bdiv_dbm1c@@Base+0xa0>
   51568:	mul	x10, x5, x3
   5156c:	umulh	x11, x5, x3
   51570:	ldr	x5, [x1], #8
   51574:	b	515c8 <__gmpn_bdiv_dbm1c@@Base+0x88>
   51578:	subs	x2, x2, #0x1
   5157c:	mul	x12, x5, x3
   51580:	umulh	x13, x5, x3
   51584:	b.ls	51614 <__gmpn_bdiv_dbm1c@@Base+0xd4>  // b.plast
   51588:	ldr	x5, [x1], #8
   5158c:	b	515b0 <__gmpn_bdiv_dbm1c@@Base+0x70>
   51590:	mul	x10, x5, x3
   51594:	umulh	x11, x5, x3
   51598:	ldr	x5, [x1], #8
   5159c:	b	515f8 <__gmpn_bdiv_dbm1c@@Base+0xb8>
   515a0:	ldr	x5, [x1], #8
   515a4:	subs	x4, x4, x10
   515a8:	str	x4, [x0], #8
   515ac:	sbc	x4, x4, x11
   515b0:	mul	x10, x5, x3
   515b4:	umulh	x11, x5, x3
   515b8:	ldr	x5, [x1], #8
   515bc:	subs	x4, x4, x12
   515c0:	str	x4, [x0], #8
   515c4:	sbc	x4, x4, x13
   515c8:	mul	x12, x5, x3
   515cc:	umulh	x13, x5, x3
   515d0:	ldr	x5, [x1], #8
   515d4:	subs	x4, x4, x10
   515d8:	str	x4, [x0], #8
   515dc:	sbc	x4, x4, x11
   515e0:	mul	x10, x5, x3
   515e4:	umulh	x11, x5, x3
   515e8:	ldr	x5, [x1], #8
   515ec:	subs	x4, x4, x12
   515f0:	str	x4, [x0], #8
   515f4:	sbc	x4, x4, x13
   515f8:	subs	x2, x2, #0x4
   515fc:	mul	x12, x5, x3
   51600:	umulh	x13, x5, x3
   51604:	b.hi	515a0 <__gmpn_bdiv_dbm1c@@Base+0x60>  // b.pmore
   51608:	subs	x4, x4, x10
   5160c:	str	x4, [x0], #8
   51610:	sbc	x4, x4, x11
   51614:	subs	x4, x4, x12
   51618:	str	x4, [x0]
   5161c:	sbc	x0, x4, x13
   51620:	ret

0000000000051624 <__gmpn_redc_1@@Base>:
   51624:	stp	x29, x30, [sp, #-64]!
   51628:	stp	x22, x21, [sp, #32]
   5162c:	stp	x20, x19, [sp, #48]
   51630:	mov	x19, x3
   51634:	mov	x20, x1
   51638:	cmp	x3, #0x1
   5163c:	mov	x21, x0
   51640:	stp	x24, x23, [sp, #16]
   51644:	mov	x29, sp
   51648:	b.lt	51680 <__gmpn_redc_1@@Base+0x5c>  // b.tstop
   5164c:	mov	x22, x4
   51650:	mov	x23, x2
   51654:	add	x24, x19, #0x1
   51658:	ldr	x8, [x20]
   5165c:	mov	x0, x20
   51660:	mov	x1, x23
   51664:	mov	x2, x19
   51668:	mul	x3, x8, x22
   5166c:	bl	d400 <__gmpn_addmul_1@plt>
   51670:	sub	x24, x24, #0x1
   51674:	cmp	x24, #0x1
   51678:	str	x0, [x20], #8
   5167c:	b.gt	51658 <__gmpn_redc_1@@Base+0x34>
   51680:	sub	x2, x20, x19, lsl #3
   51684:	mov	x0, x21
   51688:	mov	x1, x20
   5168c:	mov	x3, x19
   51690:	ldp	x20, x19, [sp, #48]
   51694:	ldp	x22, x21, [sp, #32]
   51698:	ldp	x24, x23, [sp, #16]
   5169c:	ldp	x29, x30, [sp], #64
   516a0:	b	ca70 <__gmpn_add_n@plt>

00000000000516a4 <__gmpn_redc_2@@Base>:
   516a4:	stp	x29, x30, [sp, #-96]!
   516a8:	stp	x24, x23, [sp, #48]
   516ac:	stp	x22, x21, [sp, #64]
   516b0:	stp	x20, x19, [sp, #80]
   516b4:	mov	x22, x4
   516b8:	mov	x19, x3
   516bc:	mov	x23, x2
   516c0:	mov	x20, x1
   516c4:	mov	x21, x0
   516c8:	stp	x28, x27, [sp, #16]
   516cc:	stp	x26, x25, [sp, #32]
   516d0:	mov	x29, sp
   516d4:	tbz	w19, #0, 516f8 <__gmpn_redc_2@@Base+0x54>
   516d8:	ldr	x8, [x20]
   516dc:	ldr	x9, [x22]
   516e0:	mov	x0, x20
   516e4:	mov	x1, x23
   516e8:	mov	x2, x19
   516ec:	mul	x3, x9, x8
   516f0:	bl	d400 <__gmpn_addmul_1@plt>
   516f4:	str	x0, [x20], #8
   516f8:	cmp	x19, #0x2
   516fc:	b.lt	51774 <__gmpn_redc_2@@Base+0xd0>  // b.tstop
   51700:	add	x26, x19, #0x2
   51704:	lsl	x27, x19, #3
   51708:	mov	x24, x20
   5170c:	ldp	x8, x11, [x22]
   51710:	ldr	x9, [x20]
   51714:	ldr	x10, [x24, #8]!
   51718:	ldr	x28, [x20, x27]
   5171c:	umulh	x12, x8, x9
   51720:	mul	x3, x9, x8
   51724:	madd	x8, x10, x8, x12
   51728:	mov	x0, x20
   5172c:	mov	x1, x23
   51730:	mov	x2, x19
   51734:	madd	x25, x11, x9, x8
   51738:	bl	d400 <__gmpn_addmul_1@plt>
   5173c:	str	x0, [x20, x27]
   51740:	mov	x0, x24
   51744:	mov	x1, x23
   51748:	mov	x2, x19
   5174c:	mov	x3, x25
   51750:	bl	d400 <__gmpn_addmul_1@plt>
   51754:	str	x0, [x24]
   51758:	ldr	x8, [x20, x27]
   5175c:	sub	x26, x26, #0x2
   51760:	cmp	x26, #0x3
   51764:	str	x8, [x20]
   51768:	str	x28, [x20, x27]
   5176c:	add	x20, x20, #0x10
   51770:	b.gt	51708 <__gmpn_redc_2@@Base+0x64>
   51774:	sub	x2, x20, x19, lsl #3
   51778:	mov	x0, x21
   5177c:	mov	x1, x20
   51780:	mov	x3, x19
   51784:	ldp	x20, x19, [sp, #80]
   51788:	ldp	x22, x21, [sp, #64]
   5178c:	ldp	x24, x23, [sp, #48]
   51790:	ldp	x26, x25, [sp, #32]
   51794:	ldp	x28, x27, [sp, #16]
   51798:	ldp	x29, x30, [sp], #96
   5179c:	b	ca70 <__gmpn_add_n@plt>

00000000000517a0 <__gmpn_redc_n@@Base>:
   517a0:	stp	x29, x30, [sp, #-96]!
   517a4:	stp	x22, x21, [sp, #64]
   517a8:	mov	x29, sp
   517ac:	mov	x21, x0
   517b0:	mov	x0, x3
   517b4:	str	x27, [sp, #16]
   517b8:	stp	x26, x25, [sp, #32]
   517bc:	stp	x24, x23, [sp, #48]
   517c0:	stp	x20, x19, [sp, #80]
   517c4:	mov	x25, x4
   517c8:	mov	x19, x3
   517cc:	mov	x20, x2
   517d0:	mov	x22, x1
   517d4:	str	xzr, [x29, #24]
   517d8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   517dc:	cmp	x19, x0, asr #1
   517e0:	add	x8, x19, x0, lsl #1
   517e4:	csel	x9, x0, xzr, gt
   517e8:	add	x8, x8, x9
   517ec:	lsl	x8, x8, #3
   517f0:	add	x1, x8, #0x20
   517f4:	mov	w8, #0x7f00                	// #32512
   517f8:	mov	x23, x0
   517fc:	cmp	x1, x8
   51800:	b.hi	51904 <__gmpn_redc_n@@Base+0x164>  // b.pmore
   51804:	add	x9, x1, #0xf
   51808:	mov	x8, sp
   5180c:	and	x9, x9, #0xfffffffffffffff0
   51810:	sub	x24, x8, x9
   51814:	mov	sp, x24
   51818:	mov	x0, x24
   5181c:	mov	x1, x22
   51820:	mov	x2, x25
   51824:	mov	x3, x19
   51828:	bl	cec0 <__gmpn_mullo_n@plt>
   5182c:	add	x25, x24, x19, lsl #3
   51830:	add	x26, x25, x23, lsl #3
   51834:	mov	x0, x25
   51838:	mov	x1, x23
   5183c:	mov	x2, x24
   51840:	mov	x3, x19
   51844:	mov	x4, x20
   51848:	mov	x5, x19
   5184c:	mov	x6, x26
   51850:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   51854:	lsl	x27, x19, #1
   51858:	subs	x3, x27, x23
   5185c:	b.le	5191c <__gmpn_redc_n@@Base+0x17c>
   51860:	mov	x0, x26
   51864:	mov	x1, x25
   51868:	mov	x2, x22
   5186c:	bl	c2d0 <__gmpn_sub_n@plt>
   51870:	add	x8, x25, x27, lsl #3
   51874:	sub	x8, x8, x23, lsl #3
   51878:	ldr	x9, [x8]
   5187c:	subs	x9, x9, x0
   51880:	str	x9, [x8]
   51884:	b.cs	518ac <__gmpn_redc_n@@Base+0x10c>  // b.hs, b.nlast
   51888:	mov	w8, #0x18                  	// #24
   5188c:	mul	x8, x19, x8
   51890:	sub	x8, x8, x23, lsl #3
   51894:	add	x8, x8, x24
   51898:	add	x8, x8, #0x8
   5189c:	ldr	x9, [x8]
   518a0:	sub	x10, x9, #0x1
   518a4:	str	x10, [x8], #8
   518a8:	cbz	x9, 5189c <__gmpn_redc_n@@Base+0xfc>
   518ac:	lsl	x8, x19, #3
   518b0:	add	x1, x22, x8
   518b4:	add	x2, x25, x8
   518b8:	mov	x0, x21
   518bc:	mov	x3, x19
   518c0:	bl	c2d0 <__gmpn_sub_n@plt>
   518c4:	cbz	x0, 518dc <__gmpn_redc_n@@Base+0x13c>
   518c8:	mov	x0, x21
   518cc:	mov	x1, x21
   518d0:	mov	x2, x20
   518d4:	mov	x3, x19
   518d8:	bl	ca70 <__gmpn_add_n@plt>
   518dc:	ldr	x0, [x29, #24]
   518e0:	cbnz	x0, 51914 <__gmpn_redc_n@@Base+0x174>
   518e4:	mov	sp, x29
   518e8:	ldp	x20, x19, [sp, #80]
   518ec:	ldp	x22, x21, [sp, #64]
   518f0:	ldp	x24, x23, [sp, #48]
   518f4:	ldp	x26, x25, [sp, #32]
   518f8:	ldr	x27, [sp, #16]
   518fc:	ldp	x29, x30, [sp], #96
   51900:	ret
   51904:	add	x0, x29, #0x18
   51908:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   5190c:	mov	x24, x0
   51910:	b	51818 <__gmpn_redc_n@@Base+0x78>
   51914:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   51918:	b	518e4 <__gmpn_redc_n@@Base+0x144>
   5191c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   51920:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   51924:	add	x0, x0, #0x74f
   51928:	add	x2, x2, #0x758
   5192c:	mov	w1, #0x46                  	// #70
   51930:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000051934 <__gmpn_powm@@Base>:
   51934:	stp	x29, x30, [sp, #-96]!
   51938:	stp	x28, x27, [sp, #16]
   5193c:	stp	x26, x25, [sp, #32]
   51940:	stp	x24, x23, [sp, #48]
   51944:	stp	x22, x21, [sp, #64]
   51948:	stp	x20, x19, [sp, #80]
   5194c:	mov	x29, sp
   51950:	sub	sp, sp, #0x50
   51954:	stur	xzr, [x29, #-24]
   51958:	add	x8, x3, x4, lsl #3
   5195c:	ldur	x8, [x8, #-8]
   51960:	lsl	x9, x4, #6
   51964:	mov	x22, x7
   51968:	mov	x19, x6
   5196c:	clz	x8, x8
   51970:	sub	x11, x9, x8
   51974:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   51978:	mov	x20, x5
   5197c:	mov	x28, x1
   51980:	mov	x21, x0
   51984:	mov	x23, xzr
   51988:	mov	x24, xzr
   5198c:	add	x8, x8, #0x768
   51990:	mov	x9, #0x100000000           	// #4294967296
   51994:	stur	x3, [x29, #-56]
   51998:	stur	x11, [x29, #-40]
   5199c:	add	x10, x8, x24, lsl #3
   519a0:	ldr	x10, [x10, #8]
   519a4:	add	x24, x24, #0x1
   519a8:	add	x23, x23, x9
   519ac:	cmp	x10, x11
   519b0:	b.cc	5199c <__gmpn_powm@@Base+0x68>  // b.lo, b.ul, b.last
   519b4:	cmp	x19, #0x2a
   519b8:	lsl	x1, x19, #3
   519bc:	stur	x1, [x29, #-48]
   519c0:	stur	x11, [x29, #-64]
   519c4:	b.le	51a08 <__gmpn_powm@@Base+0xd4>
   519c8:	mov	w8, #0x7f00                	// #32512
   519cc:	mov	x26, x2
   519d0:	cmp	x1, x8
   519d4:	b.hi	523cc <__gmpn_powm@@Base+0xa98>  // b.pmore
   519d8:	add	x9, x1, #0xf
   519dc:	mov	x8, sp
   519e0:	and	x9, x9, #0xfffffffffffffff0
   519e4:	sub	x25, x8, x9
   519e8:	mov	sp, x25
   519ec:	mov	x0, x25
   519f0:	mov	x1, x20
   519f4:	mov	x2, x19
   519f8:	mov	x3, x22
   519fc:	bl	cd20 <__gmpn_binvert@plt>
   51a00:	mov	x2, x26
   51a04:	b	51a44 <__gmpn_powm@@Base+0x110>
   51a08:	ldr	x8, [x20]
   51a0c:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   51a10:	ldr	x9, [x9, #3952]
   51a14:	sub	x25, x29, #0x10
   51a18:	ubfx	x10, x8, #1, #7
   51a1c:	ldrb	w9, [x9, x10]
   51a20:	mov	w10, #0x2                   	// #2
   51a24:	msub	x11, x8, x9, x10
   51a28:	mul	x9, x11, x9
   51a2c:	msub	x10, x9, x8, x10
   51a30:	mul	x9, x9, x10
   51a34:	orr	x10, xzr, #0xfffffffffffffffe
   51a38:	madd	x8, x9, x8, x10
   51a3c:	mul	x8, x8, x9
   51a40:	stur	x8, [x29, #-16]
   51a44:	sub	x27, x24, #0x1
   51a48:	lsl	x8, x19, x27
   51a4c:	lsl	x1, x8, #3
   51a50:	mov	w8, #0x7f00                	// #32512
   51a54:	cmp	x1, x8
   51a58:	b.hi	523ac <__gmpn_powm@@Base+0xa78>  // b.pmore
   51a5c:	add	x9, x1, #0xf
   51a60:	mov	x8, sp
   51a64:	and	x9, x9, #0xfffffffffffffff0
   51a68:	sub	x26, x8, x9
   51a6c:	mov	sp, x26
   51a70:	mov	x0, x26
   51a74:	mov	x1, x28
   51a78:	mov	x3, x20
   51a7c:	mov	x4, x19
   51a80:	bl	523dc <__gmpn_powm@@Base+0xaa8>
   51a84:	mov	x0, x22
   51a88:	mov	x1, x26
   51a8c:	mov	x2, x19
   51a90:	bl	c8e0 <__gmpn_sqr@plt>
   51a94:	cmp	x19, #0x2a
   51a98:	stur	x26, [x29, #-32]
   51a9c:	b.le	51ac0 <__gmpn_powm@@Base+0x18c>
   51aa0:	mov	x0, x21
   51aa4:	mov	x1, x22
   51aa8:	mov	x2, x20
   51aac:	mov	x3, x19
   51ab0:	mov	x4, x25
   51ab4:	bl	c7d0 <__gmpn_redc_n@plt>
   51ab8:	ldur	x28, [x29, #-48]
   51abc:	b	51af4 <__gmpn_powm@@Base+0x1c0>
   51ac0:	ldr	x4, [x25]
   51ac4:	mov	x0, x21
   51ac8:	mov	x1, x22
   51acc:	mov	x2, x20
   51ad0:	mov	x3, x19
   51ad4:	bl	d0f0 <__gmpn_redc_1@plt>
   51ad8:	ldur	x28, [x29, #-48]
   51adc:	cbz	x0, 51af4 <__gmpn_powm@@Base+0x1c0>
   51ae0:	mov	x0, x21
   51ae4:	mov	x1, x21
   51ae8:	mov	x2, x20
   51aec:	mov	x3, x19
   51af0:	bl	c2d0 <__gmpn_sub_n@plt>
   51af4:	mov	w8, #0xffffffff            	// #-1
   51af8:	lsl	w8, w8, w27
   51afc:	cmn	w8, #0x2
   51b00:	b.gt	51b64 <__gmpn_powm@@Base+0x230>
   51b04:	mvn	w8, w8
   51b08:	cmp	x19, #0x1
   51b0c:	sxtw	x9, w8
   51b10:	b.ne	51ed8 <__gmpn_powm@@Base+0x5a4>  // b.any
   51b14:	ldur	x8, [x29, #-32]
   51b18:	add	x9, x9, #0x1
   51b1c:	ldr	x10, [x8], #8
   51b20:	ldr	x11, [x21]
   51b24:	umulh	x12, x10, x11
   51b28:	sub	x9, x9, #0x1
   51b2c:	mul	x10, x11, x10
   51b30:	stp	x10, x12, [x22]
   51b34:	ldr	x11, [x25]
   51b38:	ldr	x13, [x20]
   51b3c:	cmp	x10, #0x0
   51b40:	mul	x10, x11, x10
   51b44:	umulh	x10, x13, x10
   51b48:	cinc	x10, x10, ne  // ne = any
   51b4c:	adds	x10, x10, x12
   51b50:	csel	x11, x13, xzr, cs  // cs = hs, nlast
   51b54:	sub	x10, x10, x11
   51b58:	cmp	x9, #0x1
   51b5c:	str	x10, [x8], #8
   51b60:	b.gt	51b20 <__gmpn_powm@@Base+0x1ec>
   51b64:	ldur	x14, [x29, #-64]
   51b68:	asr	x13, x23, #32
   51b6c:	subs	x8, x14, x13
   51b70:	b.cs	51b84 <__gmpn_powm@@Base+0x250>  // b.hs, b.nlast
   51b74:	ldur	x28, [x29, #-56]
   51b78:	ldur	x11, [x29, #-40]
   51b7c:	ldr	x8, [x28]
   51b80:	b	51bbc <__gmpn_powm@@Base+0x288>
   51b84:	ldur	x28, [x29, #-56]
   51b88:	lsr	x9, x8, #6
   51b8c:	and	x10, x8, #0x3f
   51b90:	mov	w12, #0x40                  	// #64
   51b94:	ldr	x11, [x28, x9, lsl #3]
   51b98:	sub	w10, w12, w10
   51b9c:	cmp	w10, w24
   51ba0:	lsr	x8, x11, x8
   51ba4:	b.ge	51bb8 <__gmpn_powm@@Base+0x284>  // b.tcont
   51ba8:	add	x9, x28, x9, lsl #3
   51bac:	ldr	x9, [x9, #8]
   51bb0:	lsl	x9, x9, x10
   51bb4:	add	x8, x9, x8
   51bb8:	and	x11, x24, #0xffffffff
   51bbc:	mov	x9, #0xffffffffffffffff    	// #-1
   51bc0:	lsl	x9, x9, x11
   51bc4:	bic	x8, x8, x9
   51bc8:	subs	x10, x14, x13
   51bcc:	rbit	x9, x8
   51bd0:	csel	x10, xzr, x10, cc  // cc = lo, ul, last
   51bd4:	clz	x9, x9
   51bd8:	add	x26, x9, x10
   51bdc:	lsr	x8, x8, x9
   51be0:	ldur	x9, [x29, #-32]
   51be4:	lsr	x8, x8, #1
   51be8:	mul	x8, x8, x19
   51bec:	mov	x0, x21
   51bf0:	add	x1, x9, x8, lsl #3
   51bf4:	mov	x2, x19
   51bf8:	stur	x13, [x29, #-40]
   51bfc:	bl	ca50 <__gmpn_copyi@plt>
   51c00:	subs	x27, x19, #0x1
   51c04:	b.ne	51d6c <__gmpn_powm@@Base+0x438>  // b.any
   51c08:	cbz	x26, 522f0 <__gmpn_powm@@Base+0x9bc>
   51c0c:	ldur	x0, [x29, #-40]
   51c10:	mov	x8, #0xffffffffffffffff    	// #-1
   51c14:	lsl	x9, x8, x24
   51c18:	mvn	x9, x9
   51c1c:	mov	w10, #0x40                  	// #64
   51c20:	b	51c64 <__gmpn_powm@@Base+0x330>
   51c24:	ldr	x12, [x21]
   51c28:	umulh	x13, x12, x12
   51c2c:	mov	x26, x11
   51c30:	mul	x12, x12, x12
   51c34:	stp	x12, x13, [x22]
   51c38:	ldr	x14, [x25]
   51c3c:	ldr	x15, [x20]
   51c40:	cmp	x12, #0x0
   51c44:	mul	x14, x14, x12
   51c48:	umulh	x14, x15, x14
   51c4c:	cinc	x12, x14, ne  // ne = any
   51c50:	adds	x12, x12, x13
   51c54:	csel	x13, x15, xzr, cs  // cs = hs, nlast
   51c58:	sub	x12, x12, x13
   51c5c:	str	x12, [x21]
   51c60:	cbz	x11, 522b8 <__gmpn_powm@@Base+0x984>
   51c64:	sub	x11, x26, #0x1
   51c68:	lsr	x12, x11, #3
   51c6c:	and	x12, x12, #0x1ffffffffffffff8
   51c70:	ldr	x12, [x28, x12]
   51c74:	lsr	x12, x12, x11
   51c78:	tbz	w12, #0, 51c24 <__gmpn_powm@@Base+0x2f0>
   51c7c:	subs	x11, x26, x0
   51c80:	b.cs	51c94 <__gmpn_powm@@Base+0x360>  // b.hs, b.nlast
   51c84:	ldr	x11, [x28]
   51c88:	lsl	x12, x8, x26
   51c8c:	bic	x11, x11, x12
   51c90:	b	51cc4 <__gmpn_powm@@Base+0x390>
   51c94:	lsr	x12, x11, #6
   51c98:	ldr	x14, [x28, x12, lsl #3]
   51c9c:	and	x13, x11, #0x3f
   51ca0:	sub	w13, w10, w13
   51ca4:	cmp	w13, w24
   51ca8:	lsr	x11, x14, x11
   51cac:	b.ge	51cc0 <__gmpn_powm@@Base+0x38c>  // b.tcont
   51cb0:	add	x12, x28, x12, lsl #3
   51cb4:	ldr	x12, [x12, #8]
   51cb8:	lsl	x12, x12, x13
   51cbc:	add	x11, x12, x11
   51cc0:	and	x11, x11, x9
   51cc4:	ldr	x12, [x25]
   51cc8:	ldr	x15, [x21]
   51ccc:	subs	x13, x26, x0
   51cd0:	rbit	x14, x11
   51cd4:	csel	w16, w26, w24, cc  // cc = lo, ul, last
   51cd8:	csel	x17, xzr, x13, cc  // cc = lo, ul, last
   51cdc:	clz	x13, x14
   51ce0:	add	x26, x13, x17
   51ce4:	sub	w14, w13, w16
   51ce8:	umulh	x16, x15, x15
   51cec:	mul	x15, x15, x15
   51cf0:	stp	x15, x16, [x22]
   51cf4:	ldr	x17, [x20]
   51cf8:	mul	x18, x12, x15
   51cfc:	cmp	x15, #0x0
   51d00:	umulh	x15, x17, x18
   51d04:	cinc	x15, x15, ne  // ne = any
   51d08:	adds	x15, x15, x16
   51d0c:	csel	x16, x17, xzr, cs  // cs = hs, nlast
   51d10:	sub	x15, x15, x16
   51d14:	adds	w14, w14, #0x1
   51d18:	str	x15, [x21]
   51d1c:	b.cc	51ce8 <__gmpn_powm@@Base+0x3b4>  // b.lo, b.ul, b.last
   51d20:	lsr	x11, x11, x13
   51d24:	ldur	x13, [x29, #-32]
   51d28:	lsl	x11, x11, #2
   51d2c:	and	x11, x11, #0xfffffffffffffff8
   51d30:	ldr	x11, [x13, x11]
   51d34:	umulh	x13, x15, x11
   51d38:	mul	x11, x11, x15
   51d3c:	stp	x11, x13, [x22]
   51d40:	ldr	x14, [x20]
   51d44:	mul	x12, x11, x12
   51d48:	cmp	x11, #0x0
   51d4c:	umulh	x12, x14, x12
   51d50:	cinc	x11, x12, ne  // ne = any
   51d54:	adds	x11, x11, x13
   51d58:	csel	x12, x14, xzr, cs  // cs = hs, nlast
   51d5c:	sub	x11, x11, x12
   51d60:	str	x11, [x21]
   51d64:	cbnz	x26, 51c64 <__gmpn_powm@@Base+0x330>
   51d68:	b	522b8 <__gmpn_powm@@Base+0x984>
   51d6c:	cmp	x19, #0xd
   51d70:	b.le	51f64 <__gmpn_powm@@Base+0x630>
   51d74:	cmp	x19, #0x2a
   51d78:	b.le	52110 <__gmpn_powm@@Base+0x7dc>
   51d7c:	cbz	x26, 522f0 <__gmpn_powm@@Base+0x9bc>
   51d80:	ldur	x12, [x29, #-40]
   51d84:	mov	x8, #0xffffffffffffffff    	// #-1
   51d88:	lsl	x8, x8, x24
   51d8c:	mvn	x8, x8
   51d90:	stur	x8, [x29, #-72]
   51d94:	b	51dcc <__gmpn_powm@@Base+0x498>
   51d98:	mov	x0, x22
   51d9c:	mov	x1, x21
   51da0:	mov	x2, x19
   51da4:	bl	c8e0 <__gmpn_sqr@plt>
   51da8:	mov	x0, x21
   51dac:	mov	x1, x22
   51db0:	mov	x2, x20
   51db4:	mov	x3, x19
   51db8:	mov	x4, x25
   51dbc:	bl	c7d0 <__gmpn_redc_n@plt>
   51dc0:	ldur	x12, [x29, #-40]
   51dc4:	mov	x26, x23
   51dc8:	cbz	x23, 522b8 <__gmpn_powm@@Base+0x984>
   51dcc:	sub	x23, x26, #0x1
   51dd0:	lsr	x8, x23, #3
   51dd4:	and	x8, x8, #0x1ffffffffffffff8
   51dd8:	ldr	x8, [x28, x8]
   51ddc:	lsr	x8, x8, x23
   51de0:	tbz	w8, #0, 51d98 <__gmpn_powm@@Base+0x464>
   51de4:	subs	x8, x26, x12
   51de8:	b.cs	51e00 <__gmpn_powm@@Base+0x4cc>  // b.hs, b.nlast
   51dec:	ldr	x8, [x28]
   51df0:	mov	x9, #0xffffffffffffffff    	// #-1
   51df4:	lsl	x9, x9, x26
   51df8:	bic	x9, x8, x9
   51dfc:	b	51e38 <__gmpn_powm@@Base+0x504>
   51e00:	lsr	x9, x8, #6
   51e04:	ldr	x11, [x28, x9, lsl #3]
   51e08:	and	x10, x8, #0x3f
   51e0c:	mov	w13, #0x40                  	// #64
   51e10:	sub	w10, w13, w10
   51e14:	cmp	w10, w24
   51e18:	lsr	x8, x11, x8
   51e1c:	b.ge	51e30 <__gmpn_powm@@Base+0x4fc>  // b.tcont
   51e20:	add	x9, x28, x9, lsl #3
   51e24:	ldr	x9, [x9, #8]
   51e28:	lsl	x9, x9, x10
   51e2c:	add	x8, x9, x8
   51e30:	ldur	x9, [x29, #-72]
   51e34:	and	x9, x8, x9
   51e38:	subs	x8, x26, x12
   51e3c:	stur	x9, [x29, #-64]
   51e40:	rbit	x9, x9
   51e44:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   51e48:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   51e4c:	clz	x28, x9
   51e50:	add	x26, x28, x8
   51e54:	sub	w23, w28, w10
   51e58:	mov	x0, x22
   51e5c:	mov	x1, x21
   51e60:	mov	x2, x19
   51e64:	bl	c8e0 <__gmpn_sqr@plt>
   51e68:	mov	x0, x21
   51e6c:	mov	x1, x22
   51e70:	mov	x2, x20
   51e74:	mov	x3, x19
   51e78:	mov	x4, x25
   51e7c:	bl	c7d0 <__gmpn_redc_n@plt>
   51e80:	adds	w23, w23, #0x1
   51e84:	b.cc	51e58 <__gmpn_powm@@Base+0x524>  // b.lo, b.ul, b.last
   51e88:	ldur	x8, [x29, #-64]
   51e8c:	ldur	x9, [x29, #-32]
   51e90:	mov	x0, x22
   51e94:	mov	x1, x21
   51e98:	lsr	x8, x8, x28
   51e9c:	lsr	x8, x8, #1
   51ea0:	mul	x8, x8, x19
   51ea4:	add	x2, x9, x8, lsl #3
   51ea8:	mov	x3, x19
   51eac:	bl	c990 <__gmpn_mul_n@plt>
   51eb0:	mov	x0, x21
   51eb4:	mov	x1, x22
   51eb8:	mov	x2, x20
   51ebc:	mov	x3, x19
   51ec0:	mov	x4, x25
   51ec4:	bl	c7d0 <__gmpn_redc_n@plt>
   51ec8:	ldur	x28, [x29, #-56]
   51ecc:	ldur	x12, [x29, #-40]
   51ed0:	cbnz	x26, 51dcc <__gmpn_powm@@Base+0x498>
   51ed4:	b	522b8 <__gmpn_powm@@Base+0x984>
   51ed8:	ldur	x27, [x29, #-32]
   51edc:	add	x26, x9, #0x1
   51ee0:	b	51f10 <__gmpn_powm@@Base+0x5dc>
   51ee4:	mov	x0, x28
   51ee8:	mov	x1, x22
   51eec:	mov	x2, x20
   51ef0:	mov	x3, x19
   51ef4:	mov	x4, x25
   51ef8:	bl	c7d0 <__gmpn_redc_n@plt>
   51efc:	ldur	x28, [x29, #-48]
   51f00:	sub	x26, x26, #0x1
   51f04:	cmp	x26, #0x1
   51f08:	add	x27, x27, x28
   51f0c:	b.le	51b64 <__gmpn_powm@@Base+0x230>
   51f10:	mov	x0, x22
   51f14:	mov	x1, x27
   51f18:	mov	x2, x21
   51f1c:	mov	x3, x19
   51f20:	bl	c990 <__gmpn_mul_n@plt>
   51f24:	cmp	x19, #0x2a
   51f28:	add	x28, x27, x28
   51f2c:	b.gt	51ee4 <__gmpn_powm@@Base+0x5b0>
   51f30:	ldr	x4, [x25]
   51f34:	mov	x0, x28
   51f38:	mov	x1, x22
   51f3c:	mov	x2, x20
   51f40:	mov	x3, x19
   51f44:	bl	d0f0 <__gmpn_redc_1@plt>
   51f48:	cbz	x0, 51efc <__gmpn_powm@@Base+0x5c8>
   51f4c:	mov	x0, x28
   51f50:	mov	x1, x28
   51f54:	mov	x2, x20
   51f58:	mov	x3, x19
   51f5c:	bl	c2d0 <__gmpn_sub_n@plt>
   51f60:	b	51efc <__gmpn_powm@@Base+0x5c8>
   51f64:	ldur	x12, [x29, #-40]
   51f68:	cbz	x26, 522b8 <__gmpn_powm@@Base+0x984>
   51f6c:	mov	x8, #0xffffffffffffffff    	// #-1
   51f70:	lsl	x8, x8, x24
   51f74:	mvn	x8, x8
   51f78:	stur	x8, [x29, #-72]
   51f7c:	b	51f8c <__gmpn_powm@@Base+0x658>
   51f80:	ldur	x28, [x29, #-56]
   51f84:	ldur	x12, [x29, #-40]
   51f88:	cbz	x26, 522b8 <__gmpn_powm@@Base+0x984>
   51f8c:	sub	x8, x26, #0x1
   51f90:	lsr	x9, x8, #3
   51f94:	and	x9, x9, #0x1ffffffffffffff8
   51f98:	ldr	x9, [x28, x9]
   51f9c:	lsr	x9, x9, x8
   51fa0:	tbz	w9, #0, 51fc0 <__gmpn_powm@@Base+0x68c>
   51fa4:	subs	x8, x26, x12
   51fa8:	b.cs	52008 <__gmpn_powm@@Base+0x6d4>  // b.hs, b.nlast
   51fac:	ldr	x8, [x28]
   51fb0:	mov	x9, #0xffffffffffffffff    	// #-1
   51fb4:	lsl	x9, x9, x26
   51fb8:	bic	x9, x8, x9
   51fbc:	b	52040 <__gmpn_powm@@Base+0x70c>
   51fc0:	mov	x0, x22
   51fc4:	mov	x1, x21
   51fc8:	mov	x2, x19
   51fcc:	mov	x26, x8
   51fd0:	bl	c190 <__gmpn_sqr_basecase@plt>
   51fd4:	ldr	x4, [x25]
   51fd8:	mov	x0, x21
   51fdc:	mov	x1, x22
   51fe0:	mov	x2, x20
   51fe4:	mov	x3, x19
   51fe8:	bl	d0f0 <__gmpn_redc_1@plt>
   51fec:	cbz	x0, 51f84 <__gmpn_powm@@Base+0x650>
   51ff0:	mov	x0, x21
   51ff4:	mov	x1, x21
   51ff8:	mov	x2, x20
   51ffc:	mov	x3, x19
   52000:	bl	c2d0 <__gmpn_sub_n@plt>
   52004:	b	51f84 <__gmpn_powm@@Base+0x650>
   52008:	lsr	x9, x8, #6
   5200c:	ldr	x11, [x28, x9, lsl #3]
   52010:	and	x10, x8, #0x3f
   52014:	mov	w13, #0x40                  	// #64
   52018:	sub	w10, w13, w10
   5201c:	cmp	w10, w24
   52020:	lsr	x8, x11, x8
   52024:	b.ge	52038 <__gmpn_powm@@Base+0x704>  // b.tcont
   52028:	add	x9, x28, x9, lsl #3
   5202c:	ldr	x9, [x9, #8]
   52030:	lsl	x9, x9, x10
   52034:	add	x8, x9, x8
   52038:	ldur	x9, [x29, #-72]
   5203c:	and	x9, x8, x9
   52040:	subs	x8, x26, x12
   52044:	stur	x9, [x29, #-64]
   52048:	rbit	x9, x9
   5204c:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   52050:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   52054:	clz	x28, x9
   52058:	add	x26, x28, x8
   5205c:	sub	w23, w28, w10
   52060:	b	5206c <__gmpn_powm@@Base+0x738>
   52064:	adds	w23, w23, #0x1
   52068:	b.cs	520b0 <__gmpn_powm@@Base+0x77c>  // b.hs, b.nlast
   5206c:	mov	x0, x22
   52070:	mov	x1, x21
   52074:	mov	x2, x19
   52078:	bl	c190 <__gmpn_sqr_basecase@plt>
   5207c:	ldr	x4, [x25]
   52080:	mov	x0, x21
   52084:	mov	x1, x22
   52088:	mov	x2, x20
   5208c:	mov	x3, x19
   52090:	bl	d0f0 <__gmpn_redc_1@plt>
   52094:	cbz	x0, 52064 <__gmpn_powm@@Base+0x730>
   52098:	mov	x0, x21
   5209c:	mov	x1, x21
   520a0:	mov	x2, x20
   520a4:	mov	x3, x19
   520a8:	bl	c2d0 <__gmpn_sub_n@plt>
   520ac:	b	52064 <__gmpn_powm@@Base+0x730>
   520b0:	ldur	x8, [x29, #-64]
   520b4:	ldur	x9, [x29, #-32]
   520b8:	mov	x0, x22
   520bc:	mov	x1, x21
   520c0:	lsr	x8, x8, x28
   520c4:	lsr	x8, x8, #1
   520c8:	mul	x8, x8, x19
   520cc:	add	x3, x9, x8, lsl #3
   520d0:	mov	x2, x19
   520d4:	mov	x4, x19
   520d8:	bl	c550 <__gmpn_mul_basecase@plt>
   520dc:	ldr	x4, [x25]
   520e0:	mov	x0, x21
   520e4:	mov	x1, x22
   520e8:	mov	x2, x20
   520ec:	mov	x3, x19
   520f0:	bl	d0f0 <__gmpn_redc_1@plt>
   520f4:	cbz	x0, 51f80 <__gmpn_powm@@Base+0x64c>
   520f8:	mov	x0, x21
   520fc:	mov	x1, x21
   52100:	mov	x2, x20
   52104:	mov	x3, x19
   52108:	bl	c2d0 <__gmpn_sub_n@plt>
   5210c:	b	51f80 <__gmpn_powm@@Base+0x64c>
   52110:	cbz	x26, 522f0 <__gmpn_powm@@Base+0x9bc>
   52114:	ldur	x12, [x29, #-40]
   52118:	mov	x8, #0xffffffffffffffff    	// #-1
   5211c:	lsl	x8, x8, x24
   52120:	mvn	x8, x8
   52124:	stur	x8, [x29, #-72]
   52128:	b	52138 <__gmpn_powm@@Base+0x804>
   5212c:	ldur	x28, [x29, #-56]
   52130:	ldur	x12, [x29, #-40]
   52134:	cbz	x26, 522b8 <__gmpn_powm@@Base+0x984>
   52138:	sub	x8, x26, #0x1
   5213c:	lsr	x9, x8, #3
   52140:	and	x9, x9, #0x1ffffffffffffff8
   52144:	ldr	x9, [x28, x9]
   52148:	lsr	x9, x9, x8
   5214c:	tbz	w9, #0, 5216c <__gmpn_powm@@Base+0x838>
   52150:	subs	x8, x26, x12
   52154:	b.cs	521b4 <__gmpn_powm@@Base+0x880>  // b.hs, b.nlast
   52158:	ldr	x8, [x28]
   5215c:	mov	x9, #0xffffffffffffffff    	// #-1
   52160:	lsl	x9, x9, x26
   52164:	bic	x9, x8, x9
   52168:	b	521ec <__gmpn_powm@@Base+0x8b8>
   5216c:	mov	x0, x22
   52170:	mov	x1, x21
   52174:	mov	x2, x19
   52178:	mov	x26, x8
   5217c:	bl	c8e0 <__gmpn_sqr@plt>
   52180:	ldr	x4, [x25]
   52184:	mov	x0, x21
   52188:	mov	x1, x22
   5218c:	mov	x2, x20
   52190:	mov	x3, x19
   52194:	bl	d0f0 <__gmpn_redc_1@plt>
   52198:	cbz	x0, 52130 <__gmpn_powm@@Base+0x7fc>
   5219c:	mov	x0, x21
   521a0:	mov	x1, x21
   521a4:	mov	x2, x20
   521a8:	mov	x3, x19
   521ac:	bl	c2d0 <__gmpn_sub_n@plt>
   521b0:	b	52130 <__gmpn_powm@@Base+0x7fc>
   521b4:	lsr	x9, x8, #6
   521b8:	ldr	x11, [x28, x9, lsl #3]
   521bc:	and	x10, x8, #0x3f
   521c0:	mov	w13, #0x40                  	// #64
   521c4:	sub	w10, w13, w10
   521c8:	cmp	w10, w24
   521cc:	lsr	x8, x11, x8
   521d0:	b.ge	521e4 <__gmpn_powm@@Base+0x8b0>  // b.tcont
   521d4:	add	x9, x28, x9, lsl #3
   521d8:	ldr	x9, [x9, #8]
   521dc:	lsl	x9, x9, x10
   521e0:	add	x8, x9, x8
   521e4:	ldur	x9, [x29, #-72]
   521e8:	and	x9, x8, x9
   521ec:	subs	x8, x26, x12
   521f0:	stur	x9, [x29, #-64]
   521f4:	rbit	x9, x9
   521f8:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   521fc:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   52200:	clz	x28, x9
   52204:	add	x26, x28, x8
   52208:	sub	w23, w28, w10
   5220c:	b	52218 <__gmpn_powm@@Base+0x8e4>
   52210:	adds	w23, w23, #0x1
   52214:	b.cs	5225c <__gmpn_powm@@Base+0x928>  // b.hs, b.nlast
   52218:	mov	x0, x22
   5221c:	mov	x1, x21
   52220:	mov	x2, x19
   52224:	bl	c8e0 <__gmpn_sqr@plt>
   52228:	ldr	x4, [x25]
   5222c:	mov	x0, x21
   52230:	mov	x1, x22
   52234:	mov	x2, x20
   52238:	mov	x3, x19
   5223c:	bl	d0f0 <__gmpn_redc_1@plt>
   52240:	cbz	x0, 52210 <__gmpn_powm@@Base+0x8dc>
   52244:	mov	x0, x21
   52248:	mov	x1, x21
   5224c:	mov	x2, x20
   52250:	mov	x3, x19
   52254:	bl	c2d0 <__gmpn_sub_n@plt>
   52258:	b	52210 <__gmpn_powm@@Base+0x8dc>
   5225c:	ldur	x8, [x29, #-64]
   52260:	ldur	x9, [x29, #-32]
   52264:	mov	x0, x22
   52268:	mov	x1, x21
   5226c:	lsr	x8, x8, x28
   52270:	lsr	x8, x8, #1
   52274:	mul	x8, x8, x19
   52278:	add	x2, x9, x8, lsl #3
   5227c:	mov	x3, x19
   52280:	bl	c990 <__gmpn_mul_n@plt>
   52284:	ldr	x4, [x25]
   52288:	mov	x0, x21
   5228c:	mov	x1, x22
   52290:	mov	x2, x20
   52294:	mov	x3, x19
   52298:	bl	d0f0 <__gmpn_redc_1@plt>
   5229c:	cbz	x0, 5212c <__gmpn_powm@@Base+0x7f8>
   522a0:	mov	x0, x21
   522a4:	mov	x1, x21
   522a8:	mov	x2, x20
   522ac:	mov	x3, x19
   522b0:	bl	c2d0 <__gmpn_sub_n@plt>
   522b4:	b	5212c <__gmpn_powm@@Base+0x7f8>
   522b8:	mov	x0, x22
   522bc:	mov	x1, x21
   522c0:	mov	x2, x19
   522c4:	bl	ca50 <__gmpn_copyi@plt>
   522c8:	cbnz	x19, 52300 <__gmpn_powm@@Base+0x9cc>
   522cc:	cmp	x19, #0x2a
   522d0:	b.le	52318 <__gmpn_powm@@Base+0x9e4>
   522d4:	mov	x0, x21
   522d8:	mov	x1, x22
   522dc:	mov	x2, x20
   522e0:	mov	x3, x19
   522e4:	mov	x4, x25
   522e8:	bl	c7d0 <__gmpn_redc_n@plt>
   522ec:	b	52348 <__gmpn_powm@@Base+0xa14>
   522f0:	mov	x0, x22
   522f4:	mov	x1, x21
   522f8:	mov	x2, x19
   522fc:	bl	ca50 <__gmpn_copyi@plt>
   52300:	ldur	x2, [x29, #-48]
   52304:	mov	w1, wzr
   52308:	add	x0, x22, x2
   5230c:	bl	c5f0 <memset@plt>
   52310:	cmp	x19, #0x2a
   52314:	b.gt	522d4 <__gmpn_powm@@Base+0x9a0>
   52318:	ldr	x4, [x25]
   5231c:	mov	x0, x21
   52320:	mov	x1, x22
   52324:	mov	x2, x20
   52328:	mov	x3, x19
   5232c:	bl	d0f0 <__gmpn_redc_1@plt>
   52330:	cbz	x0, 52348 <__gmpn_powm@@Base+0xa14>
   52334:	mov	x0, x21
   52338:	mov	x1, x21
   5233c:	mov	x2, x20
   52340:	mov	x3, x19
   52344:	bl	c2d0 <__gmpn_sub_n@plt>
   52348:	add	x8, x27, #0x1
   5234c:	cmp	x8, #0x1
   52350:	b.lt	52370 <__gmpn_powm@@Base+0xa3c>  // b.tstop
   52354:	lsl	x8, x27, #3
   52358:	ldr	x9, [x21, x8]
   5235c:	ldr	x8, [x20, x8]
   52360:	sub	x27, x27, #0x1
   52364:	cmp	x9, x8
   52368:	b.eq	52348 <__gmpn_powm@@Base+0xa14>  // b.none
   5236c:	b.ls	52384 <__gmpn_powm@@Base+0xa50>  // b.plast
   52370:	mov	x0, x21
   52374:	mov	x1, x21
   52378:	mov	x2, x20
   5237c:	mov	x3, x19
   52380:	bl	c2d0 <__gmpn_sub_n@plt>
   52384:	ldur	x0, [x29, #-24]
   52388:	cbnz	x0, 523c4 <__gmpn_powm@@Base+0xa90>
   5238c:	mov	sp, x29
   52390:	ldp	x20, x19, [sp, #80]
   52394:	ldp	x22, x21, [sp, #64]
   52398:	ldp	x24, x23, [sp, #48]
   5239c:	ldp	x26, x25, [sp, #32]
   523a0:	ldp	x28, x27, [sp, #16]
   523a4:	ldp	x29, x30, [sp], #96
   523a8:	ret
   523ac:	sub	x0, x29, #0x18
   523b0:	mov	x26, x2
   523b4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   523b8:	mov	x2, x26
   523bc:	mov	x26, x0
   523c0:	b	51a70 <__gmpn_powm@@Base+0x13c>
   523c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   523c8:	b	5238c <__gmpn_powm@@Base+0xa58>
   523cc:	sub	x0, x29, #0x18
   523d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   523d4:	mov	x25, x0
   523d8:	b	519ec <__gmpn_powm@@Base+0xb8>
   523dc:	stp	x29, x30, [sp, #-80]!
   523e0:	stp	x26, x25, [sp, #16]
   523e4:	stp	x24, x23, [sp, #32]
   523e8:	stp	x22, x21, [sp, #48]
   523ec:	stp	x20, x19, [sp, #64]
   523f0:	mov	x29, sp
   523f4:	sub	sp, sp, #0x10
   523f8:	add	x21, x4, x2
   523fc:	add	x8, x2, x21
   52400:	lsl	x8, x8, #3
   52404:	mov	x24, x1
   52408:	add	x1, x8, #0x8
   5240c:	mov	w8, #0x7f00                	// #32512
   52410:	mov	x19, x4
   52414:	mov	x20, x3
   52418:	mov	x23, x2
   5241c:	mov	x22, x0
   52420:	cmp	x1, x8
   52424:	stur	xzr, [x29, #-8]
   52428:	b.hi	524ac <__gmpn_powm@@Base+0xb78>  // b.pmore
   5242c:	add	x9, x1, #0xf
   52430:	mov	x8, sp
   52434:	and	x9, x9, #0xfffffffffffffff0
   52438:	sub	x25, x8, x9
   5243c:	mov	sp, x25
   52440:	add	x26, x25, x21, lsl #3
   52444:	cbz	x19, 52458 <__gmpn_powm@@Base+0xb24>
   52448:	lsl	x2, x19, #3
   5244c:	mov	x0, x25
   52450:	mov	w1, wzr
   52454:	bl	c5f0 <memset@plt>
   52458:	add	x0, x25, x19, lsl #3
   5245c:	mov	x1, x24
   52460:	mov	x2, x23
   52464:	bl	ca50 <__gmpn_copyi@plt>
   52468:	mov	x0, x26
   5246c:	mov	x1, x22
   52470:	mov	x2, xzr
   52474:	mov	x3, x25
   52478:	mov	x4, x21
   5247c:	mov	x5, x20
   52480:	mov	x6, x19
   52484:	bl	bf00 <__gmpn_tdiv_qr@plt>
   52488:	ldur	x0, [x29, #-8]
   5248c:	cbnz	x0, 524c4 <__gmpn_powm@@Base+0xb90>
   52490:	mov	sp, x29
   52494:	ldp	x20, x19, [sp, #64]
   52498:	ldp	x22, x21, [sp, #48]
   5249c:	ldp	x24, x23, [sp, #32]
   524a0:	ldp	x26, x25, [sp, #16]
   524a4:	ldp	x29, x30, [sp], #80
   524a8:	ret
   524ac:	sub	x0, x29, #0x8
   524b0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   524b4:	mov	x25, x0
   524b8:	add	x26, x25, x21, lsl #3
   524bc:	cbnz	x19, 52448 <__gmpn_powm@@Base+0xb14>
   524c0:	b	52458 <__gmpn_powm@@Base+0xb24>
   524c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   524c8:	b	52490 <__gmpn_powm@@Base+0xb5c>

00000000000524cc <__gmpn_powlo@@Base>:
   524cc:	stp	x29, x30, [sp, #-96]!
   524d0:	stp	x28, x27, [sp, #16]
   524d4:	stp	x26, x25, [sp, #32]
   524d8:	stp	x24, x23, [sp, #48]
   524dc:	stp	x22, x21, [sp, #64]
   524e0:	stp	x20, x19, [sp, #80]
   524e4:	mov	x29, sp
   524e8:	sub	sp, sp, #0x30
   524ec:	stur	xzr, [x29, #-8]
   524f0:	add	x8, x2, x3, lsl #3
   524f4:	ldur	x8, [x8, #-8]
   524f8:	lsl	x9, x3, #6
   524fc:	mov	x23, x5
   52500:	mov	x19, x4
   52504:	clz	x8, x8
   52508:	sub	x25, x9, x8
   5250c:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   52510:	mov	x21, x2
   52514:	mov	x24, x1
   52518:	mov	x20, x0
   5251c:	mov	w26, #0xffffffff            	// #-1
   52520:	add	x8, x8, #0x7c0
   52524:	add	w26, w26, #0x1
   52528:	ldr	x9, [x8, w26, uxtw #3]
   5252c:	cmp	x9, x25
   52530:	b.cc	52524 <__gmpn_powlo@@Base+0x58>  // b.lo, b.ul, b.last
   52534:	add	w27, w26, #0x1
   52538:	cmp	w27, #0x2
   5253c:	stur	w27, [x29, #-28]
   52540:	b.cc	525e0 <__gmpn_powlo@@Base+0x114>  // b.lo, b.ul, b.last
   52544:	lsl	x8, x19, x26
   52548:	lsl	x1, x8, #3
   5254c:	mov	w8, #0x7f00                	// #32512
   52550:	cmp	x1, x8
   52554:	b.hi	52828 <__gmpn_powlo@@Base+0x35c>  // b.pmore
   52558:	add	x9, x1, #0xf
   5255c:	mov	x8, sp
   52560:	and	x9, x9, #0xfffffffffffffff0
   52564:	sub	x28, x8, x9
   52568:	mov	sp, x28
   5256c:	mov	x0, x28
   52570:	mov	x1, x24
   52574:	mov	x2, x19
   52578:	bl	ca50 <__gmpn_copyi@plt>
   5257c:	mov	x0, x23
   52580:	mov	x1, x24
   52584:	mov	x2, x19
   52588:	bl	c9f0 <__gmpn_sqrlo@plt>
   5258c:	mov	w8, #0xffffffff            	// #-1
   52590:	lsl	w8, w8, w26
   52594:	mvn	w8, w8
   52598:	sxtw	x22, w8
   5259c:	lsl	x26, x19, #3
   525a0:	mov	x1, x28
   525a4:	add	x24, x1, x26
   525a8:	mov	x0, x24
   525ac:	mov	x2, x23
   525b0:	mov	x3, x19
   525b4:	bl	cec0 <__gmpn_mullo_n@plt>
   525b8:	subs	x22, x22, #0x1
   525bc:	mov	x1, x24
   525c0:	b.ne	525a4 <__gmpn_powlo@@Base+0xd8>  // b.any
   525c4:	mov	w26, w27
   525c8:	subs	x8, x25, x26
   525cc:	b.cs	52610 <__gmpn_powlo@@Base+0x144>  // b.hs, b.nlast
   525d0:	ldr	x9, [x21]
   525d4:	mov	x10, #0xffffffffffffffff    	// #-1
   525d8:	lsl	x10, x10, x25
   525dc:	b	52648 <__gmpn_powlo@@Base+0x17c>
   525e0:	add	x28, x23, x19, lsl #3
   525e4:	mov	x0, x28
   525e8:	mov	x1, x24
   525ec:	mov	x2, x19
   525f0:	bl	ca50 <__gmpn_copyi@plt>
   525f4:	mov	x0, x20
   525f8:	mov	x1, x24
   525fc:	mov	x2, x19
   52600:	bl	ca50 <__gmpn_copyi@plt>
   52604:	sub	x25, x25, #0x1
   52608:	mov	w26, w27
   5260c:	b	52674 <__gmpn_powlo@@Base+0x1a8>
   52610:	lsr	x10, x8, #6
   52614:	ldr	x12, [x21, x10, lsl #3]
   52618:	and	x9, x8, #0x3f
   5261c:	mov	w11, #0x40                  	// #64
   52620:	sub	w11, w11, w9
   52624:	cmp	w11, w27
   52628:	lsr	x9, x12, x8
   5262c:	b.cs	52640 <__gmpn_powlo@@Base+0x174>  // b.hs, b.nlast
   52630:	add	x10, x21, x10, lsl #3
   52634:	ldr	x10, [x10, #8]
   52638:	lsl	x10, x10, x11
   5263c:	add	x9, x10, x9
   52640:	mov	x10, #0xffffffffffffffff    	// #-1
   52644:	lsl	x10, x10, x26
   52648:	bic	x9, x9, x10
   5264c:	rbit	x10, x9
   52650:	clz	x10, x10
   52654:	add	x25, x8, x10
   52658:	lsr	x8, x9, x10
   5265c:	lsr	x8, x8, #1
   52660:	mul	x8, x8, x19
   52664:	add	x1, x28, x8, lsl #3
   52668:	mov	x0, x20
   5266c:	mov	x2, x19
   52670:	bl	ca50 <__gmpn_copyi@plt>
   52674:	mov	x8, #0xffffffffffffffff    	// #-1
   52678:	lsl	x8, x8, x26
   5267c:	mov	w27, wzr
   52680:	mvn	x8, x8
   52684:	stp	x28, x21, [x29, #-24]
   52688:	stur	x8, [x29, #-40]
   5268c:	b	526b4 <__gmpn_powlo@@Base+0x1e8>
   52690:	mov	x0, x20
   52694:	mov	x1, x24
   52698:	mov	x2, x19
   5269c:	bl	c9f0 <__gmpn_sqrlo@plt>
   526a0:	cmp	w27, #0x0
   526a4:	cset	w27, eq  // eq = none
   526a8:	mov	x25, x22
   526ac:	mov	x23, x24
   526b0:	cbz	x22, 527e4 <__gmpn_powlo@@Base+0x318>
   526b4:	sub	x22, x25, #0x1
   526b8:	lsr	x8, x22, #3
   526bc:	and	x8, x8, #0x1ffffffffffffff8
   526c0:	ldr	x8, [x21, x8]
   526c4:	mov	x24, x20
   526c8:	mov	x20, x23
   526cc:	lsr	x8, x8, x22
   526d0:	tbz	w8, #0, 52690 <__gmpn_powlo@@Base+0x1c4>
   526d4:	subs	x8, x25, x26
   526d8:	b.cs	526f0 <__gmpn_powlo@@Base+0x224>  // b.hs, b.nlast
   526dc:	ldr	x8, [x21]
   526e0:	mov	x9, #0xffffffffffffffff    	// #-1
   526e4:	lsl	x9, x9, x25
   526e8:	bic	x23, x8, x9
   526ec:	b	5272c <__gmpn_powlo@@Base+0x260>
   526f0:	lsr	x9, x8, #6
   526f4:	and	x10, x8, #0x3f
   526f8:	mov	w12, #0x40                  	// #64
   526fc:	ldr	x11, [x21, x9, lsl #3]
   52700:	sub	w10, w12, w10
   52704:	ldur	w12, [x29, #-28]
   52708:	lsr	x8, x11, x8
   5270c:	cmp	w10, w12
   52710:	b.cs	52724 <__gmpn_powlo@@Base+0x258>  // b.hs, b.nlast
   52714:	add	x9, x21, x9, lsl #3
   52718:	ldr	x9, [x9, #8]
   5271c:	lsl	x9, x9, x10
   52720:	add	x8, x9, x8
   52724:	ldur	x9, [x29, #-40]
   52728:	and	x23, x8, x9
   5272c:	cmp	x25, x26
   52730:	rbit	x8, x23
   52734:	csel	x9, x26, x25, hi  // hi = pmore
   52738:	clz	x28, x8
   5273c:	sub	w21, w9, w28
   52740:	cmp	w21, #0x2
   52744:	sub	x25, x25, x9
   52748:	b.cc	52780 <__gmpn_powlo@@Base+0x2b4>  // b.lo, b.ul, b.last
   5274c:	mov	w22, w21
   52750:	mov	x0, x20
   52754:	mov	x1, x24
   52758:	mov	x2, x19
   5275c:	bl	c9f0 <__gmpn_sqrlo@plt>
   52760:	mov	x0, x24
   52764:	mov	x1, x20
   52768:	mov	x2, x19
   5276c:	bl	c9f0 <__gmpn_sqrlo@plt>
   52770:	sub	w22, w22, #0x2
   52774:	cmp	w22, #0x1
   52778:	b.hi	52750 <__gmpn_powlo@@Base+0x284>  // b.pmore
   5277c:	and	w21, w21, #0x1
   52780:	add	x25, x28, x25
   52784:	lsr	x28, x23, x28
   52788:	cbz	w21, 527a8 <__gmpn_powlo@@Base+0x2dc>
   5278c:	mov	x0, x20
   52790:	mov	x1, x24
   52794:	mov	x2, x19
   52798:	bl	c9f0 <__gmpn_sqrlo@plt>
   5279c:	mov	x23, x20
   527a0:	mov	x20, x24
   527a4:	b	527b8 <__gmpn_powlo@@Base+0x2ec>
   527a8:	cmp	w27, #0x0
   527ac:	cset	w27, eq  // eq = none
   527b0:	mov	x23, x24
   527b4:	mov	x24, x20
   527b8:	ldur	x9, [x29, #-24]
   527bc:	lsr	x8, x28, #1
   527c0:	mul	x8, x8, x19
   527c4:	mov	x0, x24
   527c8:	add	x2, x9, x8, lsl #3
   527cc:	mov	x1, x23
   527d0:	mov	x3, x19
   527d4:	bl	cec0 <__gmpn_mullo_n@plt>
   527d8:	ldur	x21, [x29, #-16]
   527dc:	mov	x24, x23
   527e0:	cbnz	x25, 526b4 <__gmpn_powlo@@Base+0x1e8>
   527e4:	cbz	w27, 527f8 <__gmpn_powlo@@Base+0x32c>
   527e8:	mov	x0, x24
   527ec:	mov	x1, x20
   527f0:	mov	x2, x19
   527f4:	bl	ca50 <__gmpn_copyi@plt>
   527f8:	ldur	x0, [x29, #-8]
   527fc:	cbnz	x0, 52820 <__gmpn_powlo@@Base+0x354>
   52800:	mov	sp, x29
   52804:	ldp	x20, x19, [sp, #80]
   52808:	ldp	x22, x21, [sp, #64]
   5280c:	ldp	x24, x23, [sp, #48]
   52810:	ldp	x26, x25, [sp, #32]
   52814:	ldp	x28, x27, [sp, #16]
   52818:	ldp	x29, x30, [sp], #96
   5281c:	ret
   52820:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   52824:	b	52800 <__gmpn_powlo@@Base+0x334>
   52828:	sub	x0, x29, #0x8
   5282c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   52830:	mov	x28, x0
   52834:	b	5256c <__gmpn_powlo@@Base+0xa0>

0000000000052838 <__gmpn_sec_powm@@Base>:
   52838:	sub	sp, sp, #0xb0
   5283c:	adrp	x8, 5d000 <__gmpn_bases@@Base+0x2ab8>
   52840:	stp	x29, x30, [sp, #80]
   52844:	stp	x28, x27, [sp, #96]
   52848:	stp	x22, x21, [sp, #144]
   5284c:	stp	x20, x19, [sp, #160]
   52850:	add	x29, sp, #0x50
   52854:	mov	x19, x6
   52858:	mov	x20, x5
   5285c:	mov	x21, x0
   52860:	mov	x12, xzr
   52864:	mov	x28, xzr
   52868:	add	x8, x8, #0x830
   5286c:	mov	x9, #0x100000000           	// #4294967296
   52870:	stp	x26, x25, [sp, #112]
   52874:	stp	x24, x23, [sp, #128]
   52878:	stur	x3, [x29, #-8]
   5287c:	stp	x1, x2, [sp, #32]
   52880:	add	x10, x8, x12, lsl #3
   52884:	ldr	x10, [x10, #8]
   52888:	add	x28, x28, x9
   5288c:	add	x12, x12, #0x1
   52890:	cmp	x10, x4
   52894:	b.cc	52880 <__gmpn_sec_powm@@Base+0x48>  // b.lo, b.ul, b.last
   52898:	str	x4, [sp, #8]
   5289c:	ldr	x8, [x20]
   528a0:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   528a4:	ldr	x9, [x9, #3952]
   528a8:	add	x1, x7, x19, lsl #3
   528ac:	ubfx	x11, x8, #1, #7
   528b0:	mov	w10, #0x2                   	// #2
   528b4:	ldrb	w9, [x9, x11]
   528b8:	mov	w11, #0x1                   	// #1
   528bc:	mov	x25, x1
   528c0:	stp	x12, x7, [x29, #-24]
   528c4:	str	x11, [x25], #8
   528c8:	msub	x11, x8, x9, x10
   528cc:	mul	x9, x11, x9
   528d0:	msub	x10, x9, x8, x10
   528d4:	orr	x11, xzr, #0xfffffffffffffffe
   528d8:	mul	x23, x9, x10
   528dc:	lsl	x22, x19, x12
   528e0:	madd	x24, x23, x8, x11
   528e4:	lsl	x27, x19, #3
   528e8:	stur	x1, [x29, #-32]
   528ec:	cbz	x19, 52904 <__gmpn_sec_powm@@Base+0xcc>
   528f0:	mov	x0, x25
   528f4:	mov	w1, wzr
   528f8:	mov	x2, x27
   528fc:	bl	c5f0 <memset@plt>
   52900:	ldur	x1, [x29, #-32]
   52904:	mul	x26, x24, x23
   52908:	ldur	x23, [x29, #-16]
   5290c:	mov	w2, #0x1                   	// #1
   52910:	add	x24, x23, x22, lsl #3
   52914:	add	x22, x25, x27
   52918:	mov	x0, x22
   5291c:	bl	ca50 <__gmpn_copyi@plt>
   52920:	add	x1, x19, #0x1
   52924:	add	x4, x22, #0x8
   52928:	mov	x0, x25
   5292c:	mov	x2, x20
   52930:	mov	x3, x19
   52934:	bl	c140 <__gmpn_sec_div_r@plt>
   52938:	mov	x0, x23
   5293c:	mov	x1, x25
   52940:	mov	x2, x19
   52944:	bl	ca50 <__gmpn_copyi@plt>
   52948:	ldur	x8, [x29, #-32]
   5294c:	add	x25, x8, x27
   52950:	cbz	x19, 52964 <__gmpn_sec_powm@@Base+0x12c>
   52954:	mov	x0, x25
   52958:	mov	w1, wzr
   5295c:	mov	x2, x27
   52960:	bl	c5f0 <memset@plt>
   52964:	ldp	x1, x23, [sp, #32]
   52968:	add	x0, x25, x27
   5296c:	mov	x2, x23
   52970:	bl	ca50 <__gmpn_copyi@plt>
   52974:	add	x8, x25, x23, lsl #3
   52978:	add	x1, x19, x23
   5297c:	add	x4, x8, x27
   52980:	mov	x0, x25
   52984:	mov	x2, x20
   52988:	mov	x3, x19
   5298c:	bl	c140 <__gmpn_sec_div_r@plt>
   52990:	ldur	x0, [x29, #-32]
   52994:	mov	x1, x25
   52998:	mov	x2, x19
   5299c:	bl	ca50 <__gmpn_copyi@plt>
   529a0:	ldur	x8, [x29, #-24]
   529a4:	mov	w9, #0x1                   	// #1
   529a8:	str	x27, [sp, #32]
   529ac:	lsl	w8, w9, w8
   529b0:	cmp	w8, #0x3
   529b4:	str	x8, [sp, #16]
   529b8:	b.lt	52b64 <__gmpn_sec_powm@@Base+0x32c>  // b.tstop
   529bc:	ldr	x8, [sp, #16]
   529c0:	cmp	x19, #0x11
   529c4:	add	x9, x19, x19, lsl #1
   529c8:	lsl	x10, x19, #4
   529cc:	sub	w8, w8, #0x2
   529d0:	sxtw	x8, w8
   529d4:	b.le	52aa4 <__gmpn_sec_powm@@Base+0x26c>
   529d8:	lsl	x9, x9, #3
   529dc:	str	x10, [sp, #40]
   529e0:	str	x9, [sp, #24]
   529e4:	ldur	x25, [x29, #-32]
   529e8:	ldur	x23, [x29, #-16]
   529ec:	add	x27, x8, #0x2
   529f0:	mov	x0, x24
   529f4:	mov	x1, x25
   529f8:	mov	x2, x19
   529fc:	mov	x3, x25
   52a00:	mov	x4, x19
   52a04:	bl	c550 <__gmpn_mul_basecase@plt>
   52a08:	ldr	x8, [sp, #40]
   52a0c:	mov	x1, x24
   52a10:	mov	x2, x20
   52a14:	mov	x3, x19
   52a18:	add	x22, x23, x8
   52a1c:	mov	x0, x22
   52a20:	mov	x4, x26
   52a24:	bl	d0f0 <__gmpn_redc_1@plt>
   52a28:	mov	x1, x22
   52a2c:	mov	x2, x22
   52a30:	mov	x3, x20
   52a34:	mov	x4, x19
   52a38:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52a3c:	ldur	x3, [x29, #-32]
   52a40:	mov	x0, x24
   52a44:	mov	x1, x22
   52a48:	mov	x2, x19
   52a4c:	mov	x4, x19
   52a50:	bl	c550 <__gmpn_mul_basecase@plt>
   52a54:	ldr	x8, [sp, #24]
   52a58:	mov	x1, x24
   52a5c:	mov	x2, x20
   52a60:	mov	x3, x19
   52a64:	add	x23, x23, x8
   52a68:	mov	x0, x23
   52a6c:	mov	x4, x26
   52a70:	bl	d0f0 <__gmpn_redc_1@plt>
   52a74:	mov	x1, x23
   52a78:	mov	x2, x23
   52a7c:	mov	x3, x20
   52a80:	mov	x4, x19
   52a84:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52a88:	ldr	x8, [sp, #32]
   52a8c:	sub	x27, x27, #0x2
   52a90:	cmp	x27, #0x2
   52a94:	mov	x23, x22
   52a98:	add	x25, x25, x8
   52a9c:	b.gt	529f0 <__gmpn_sec_powm@@Base+0x1b8>
   52aa0:	b	52b64 <__gmpn_sec_powm@@Base+0x32c>
   52aa4:	lsl	x9, x9, #3
   52aa8:	str	x10, [sp, #40]
   52aac:	str	x9, [sp, #24]
   52ab0:	ldur	x25, [x29, #-32]
   52ab4:	ldur	x23, [x29, #-16]
   52ab8:	add	x27, x8, #0x2
   52abc:	mov	x0, x24
   52ac0:	mov	x1, x25
   52ac4:	mov	x2, x19
   52ac8:	bl	c190 <__gmpn_sqr_basecase@plt>
   52acc:	ldr	x8, [sp, #40]
   52ad0:	mov	x1, x24
   52ad4:	mov	x2, x20
   52ad8:	mov	x3, x19
   52adc:	add	x22, x23, x8
   52ae0:	mov	x0, x22
   52ae4:	mov	x4, x26
   52ae8:	bl	d0f0 <__gmpn_redc_1@plt>
   52aec:	mov	x1, x22
   52af0:	mov	x2, x22
   52af4:	mov	x3, x20
   52af8:	mov	x4, x19
   52afc:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52b00:	ldur	x3, [x29, #-32]
   52b04:	mov	x0, x24
   52b08:	mov	x1, x22
   52b0c:	mov	x2, x19
   52b10:	mov	x4, x19
   52b14:	bl	c550 <__gmpn_mul_basecase@plt>
   52b18:	ldr	x8, [sp, #24]
   52b1c:	mov	x1, x24
   52b20:	mov	x2, x20
   52b24:	mov	x3, x19
   52b28:	add	x23, x23, x8
   52b2c:	mov	x0, x23
   52b30:	mov	x4, x26
   52b34:	bl	d0f0 <__gmpn_redc_1@plt>
   52b38:	mov	x1, x23
   52b3c:	mov	x2, x23
   52b40:	mov	x3, x20
   52b44:	mov	x4, x19
   52b48:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52b4c:	ldr	x8, [sp, #32]
   52b50:	sub	x27, x27, #0x2
   52b54:	cmp	x27, #0x2
   52b58:	mov	x23, x22
   52b5c:	add	x25, x25, x8
   52b60:	b.gt	52abc <__gmpn_sec_powm@@Base+0x284>
   52b64:	ldr	x9, [sp, #8]
   52b68:	asr	x10, x28, #32
   52b6c:	cmp	x10, x9
   52b70:	b.hi	52e1c <__gmpn_sec_powm@@Base+0x5e4>  // b.pmore
   52b74:	ldur	x11, [x29, #-8]
   52b78:	sub	x22, x9, x10
   52b7c:	stur	x10, [x29, #-32]
   52b80:	lsr	x10, x22, #6
   52b84:	ldur	x13, [x29, #-24]
   52b88:	ldr	x12, [x11, x10, lsl #3]
   52b8c:	and	x9, x22, #0x3f
   52b90:	mov	w11, #0x40                  	// #64
   52b94:	sub	w11, w11, w9
   52b98:	and	x8, x13, #0xffffffff
   52b9c:	cmp	w11, w13
   52ba0:	lsr	x9, x12, x22
   52ba4:	b.ge	52bbc <__gmpn_sec_powm@@Base+0x384>  // b.tcont
   52ba8:	ldur	x12, [x29, #-8]
   52bac:	add	x10, x12, x10, lsl #3
   52bb0:	ldr	x10, [x10, #8]
   52bb4:	lsl	x10, x10, x11
   52bb8:	add	x9, x10, x9
   52bbc:	mov	x10, #0xffffffffffffffff    	// #-1
   52bc0:	lsl	x23, x10, x8
   52bc4:	ldr	x8, [sp, #16]
   52bc8:	ldur	x1, [x29, #-16]
   52bcc:	bic	x4, x9, x23
   52bd0:	mov	x0, x21
   52bd4:	sxtw	x3, w8
   52bd8:	mov	x2, x19
   52bdc:	str	x3, [sp, #40]
   52be0:	bl	c4c0 <__gmpn_sec_tabselect@plt>
   52be4:	ldur	x13, [x29, #-32]
   52be8:	cbz	x22, 52d80 <__gmpn_sec_powm@@Base+0x548>
   52bec:	mvn	x8, x23
   52bf0:	add	x27, x24, x19, lsl #4
   52bf4:	str	x8, [sp, #24]
   52bf8:	b	52c6c <__gmpn_sec_powm@@Base+0x434>
   52bfc:	ldur	x1, [x29, #-16]
   52c00:	ldr	x3, [sp, #40]
   52c04:	mov	x0, x27
   52c08:	mov	x2, x19
   52c0c:	mov	x4, x25
   52c10:	mov	x23, x27
   52c14:	bl	c4c0 <__gmpn_sec_tabselect@plt>
   52c18:	mov	x0, x24
   52c1c:	mov	x1, x21
   52c20:	mov	x2, x19
   52c24:	mov	x3, x27
   52c28:	mov	x4, x19
   52c2c:	bl	c550 <__gmpn_mul_basecase@plt>
   52c30:	mov	x0, x21
   52c34:	mov	x1, x24
   52c38:	mov	x2, x20
   52c3c:	mov	x3, x19
   52c40:	mov	x4, x26
   52c44:	bl	d0f0 <__gmpn_redc_1@plt>
   52c48:	mov	x1, x21
   52c4c:	mov	x2, x21
   52c50:	mov	x3, x20
   52c54:	mov	x4, x19
   52c58:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52c5c:	ldur	x13, [x29, #-32]
   52c60:	cmp	x22, x13
   52c64:	mov	x22, x28
   52c68:	b.ls	52d80 <__gmpn_sec_powm@@Base+0x548>  // b.plast
   52c6c:	subs	x8, x22, x13
   52c70:	b.cs	52c90 <__gmpn_sec_powm@@Base+0x458>  // b.hs, b.nlast
   52c74:	ldur	x8, [x29, #-8]
   52c78:	ldur	x12, [x29, #-24]
   52c7c:	mov	x9, #0xffffffffffffffff    	// #-1
   52c80:	lsl	x9, x9, x22
   52c84:	ldr	x8, [x8]
   52c88:	bic	x25, x8, x9
   52c8c:	b	52cd4 <__gmpn_sec_powm@@Base+0x49c>
   52c90:	ldur	x10, [x29, #-8]
   52c94:	lsr	x9, x8, #6
   52c98:	mov	w12, #0x40                  	// #64
   52c9c:	ldr	x11, [x10, x9, lsl #3]
   52ca0:	and	x10, x8, #0x3f
   52ca4:	sub	w10, w12, w10
   52ca8:	ldur	x12, [x29, #-24]
   52cac:	lsr	x8, x11, x8
   52cb0:	cmp	w10, w12
   52cb4:	b.ge	52ccc <__gmpn_sec_powm@@Base+0x494>  // b.tcont
   52cb8:	ldur	x11, [x29, #-8]
   52cbc:	add	x9, x11, x9, lsl #3
   52cc0:	ldr	x9, [x9, #8]
   52cc4:	lsl	x9, x9, x10
   52cc8:	add	x8, x9, x8
   52ccc:	ldr	x9, [sp, #24]
   52cd0:	and	x25, x8, x9
   52cd4:	subs	x8, x22, x13
   52cd8:	csel	w23, w22, w12, cc  // cc = lo, ul, last
   52cdc:	csel	x28, xzr, x8, cc  // cc = lo, ul, last
   52ce0:	cmp	x19, #0x11
   52ce4:	b.le	52d38 <__gmpn_sec_powm@@Base+0x500>
   52ce8:	mov	x0, x24
   52cec:	mov	x1, x21
   52cf0:	mov	x2, x19
   52cf4:	mov	x3, x21
   52cf8:	mov	x4, x19
   52cfc:	bl	c550 <__gmpn_mul_basecase@plt>
   52d00:	mov	x0, x21
   52d04:	mov	x1, x24
   52d08:	mov	x2, x20
   52d0c:	mov	x3, x19
   52d10:	mov	x4, x26
   52d14:	bl	d0f0 <__gmpn_redc_1@plt>
   52d18:	mov	x1, x21
   52d1c:	mov	x2, x21
   52d20:	mov	x3, x20
   52d24:	mov	x4, x19
   52d28:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52d2c:	subs	w23, w23, #0x1
   52d30:	b.ne	52ce8 <__gmpn_sec_powm@@Base+0x4b0>  // b.any
   52d34:	b	52bfc <__gmpn_sec_powm@@Base+0x3c4>
   52d38:	mov	x0, x24
   52d3c:	mov	x1, x21
   52d40:	mov	x2, x19
   52d44:	bl	c190 <__gmpn_sqr_basecase@plt>
   52d48:	mov	x0, x21
   52d4c:	mov	x1, x24
   52d50:	mov	x2, x20
   52d54:	mov	x3, x19
   52d58:	mov	x4, x26
   52d5c:	bl	d0f0 <__gmpn_redc_1@plt>
   52d60:	mov	x1, x21
   52d64:	mov	x2, x21
   52d68:	mov	x3, x20
   52d6c:	mov	x4, x19
   52d70:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52d74:	subs	w23, w23, #0x1
   52d78:	b.ne	52d38 <__gmpn_sec_powm@@Base+0x500>  // b.any
   52d7c:	b	52bfc <__gmpn_sec_powm@@Base+0x3c4>
   52d80:	mov	x0, x24
   52d84:	mov	x1, x21
   52d88:	mov	x2, x19
   52d8c:	bl	ca50 <__gmpn_copyi@plt>
   52d90:	cbz	x19, 52da4 <__gmpn_sec_powm@@Base+0x56c>
   52d94:	ldr	x2, [sp, #32]
   52d98:	mov	w1, wzr
   52d9c:	add	x0, x24, x2
   52da0:	bl	c5f0 <memset@plt>
   52da4:	mov	x0, x21
   52da8:	mov	x1, x24
   52dac:	mov	x2, x20
   52db0:	mov	x3, x19
   52db4:	mov	x4, x26
   52db8:	bl	d0f0 <__gmpn_redc_1@plt>
   52dbc:	mov	x1, x21
   52dc0:	mov	x2, x21
   52dc4:	mov	x3, x20
   52dc8:	mov	x4, x19
   52dcc:	bl	c020 <__gmpn_cnd_sub_n@plt>
   52dd0:	mov	x0, x24
   52dd4:	mov	x1, x21
   52dd8:	mov	x2, x20
   52ddc:	mov	x3, x19
   52de0:	bl	c2d0 <__gmpn_sub_n@plt>
   52de4:	mov	x1, x21
   52de8:	mov	x2, x21
   52dec:	mov	x3, x20
   52df0:	mov	x4, x19
   52df4:	ldp	x20, x19, [sp, #160]
   52df8:	ldp	x22, x21, [sp, #144]
   52dfc:	ldp	x24, x23, [sp, #128]
   52e00:	ldp	x26, x25, [sp, #112]
   52e04:	ldp	x28, x27, [sp, #96]
   52e08:	ldp	x29, x30, [sp, #80]
   52e0c:	cmp	w0, #0x0
   52e10:	cset	w0, eq  // eq = none
   52e14:	add	sp, sp, #0xb0
   52e18:	b	c020 <__gmpn_cnd_sub_n@plt>
   52e1c:	adrp	x0, 5d000 <__gmpn_bases@@Base+0x2ab8>
   52e20:	adrp	x2, 5d000 <__gmpn_bases@@Base+0x2ab8>
   52e24:	add	x0, x0, #0x810
   52e28:	add	x2, x2, #0x81b
   52e2c:	mov	w1, #0x12a                 	// #298
   52e30:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000052e34 <__gmpn_sec_powm_itch@@Base>:
   52e34:	adrp	x9, 5d000 <__gmpn_bases@@Base+0x2ab8>
   52e38:	mov	x8, xzr
   52e3c:	add	x9, x9, #0x830
   52e40:	add	x10, x9, x8, lsl #3
   52e44:	ldr	x10, [x10, #8]
   52e48:	add	x8, x8, #0x1
   52e4c:	cmp	x10, x1
   52e50:	b.cc	52e40 <__gmpn_sec_powm_itch@@Base+0xc>  // b.lo, b.ul, b.last
   52e54:	add	x9, x2, x0
   52e58:	add	x9, x9, x2, lsl #1
   52e5c:	lsl	x8, x2, x8
   52e60:	lsl	x9, x9, #1
   52e64:	add	x9, x9, #0x2
   52e68:	add	x8, x8, x2, lsl #2
   52e6c:	cmp	x8, x9
   52e70:	csel	x0, x8, x9, gt
   52e74:	ret

0000000000052e78 <__gmpn_sec_mul@@Base>:
   52e78:	b	c550 <__gmpn_mul_basecase@plt>

0000000000052e7c <__gmpn_sec_mul_itch@@Base>:
   52e7c:	mov	x0, xzr
   52e80:	ret

0000000000052e84 <__gmpn_sec_sqr@@Base>:
   52e84:	mov	x3, x1
   52e88:	mov	x4, x2
   52e8c:	b	c550 <__gmpn_mul_basecase@plt>

0000000000052e90 <__gmpn_sec_sqr_itch@@Base>:
   52e90:	mov	x0, xzr
   52e94:	ret

0000000000052e98 <__gmpn_sec_div_qr_itch@@Base>:
   52e98:	add	x8, x0, x0, lsl #1
   52e9c:	add	x0, x8, #0x4
   52ea0:	ret

0000000000052ea4 <__gmpn_sec_div_qr@@Base>:
   52ea4:	sub	sp, sp, #0x70
   52ea8:	stp	x29, x30, [sp, #16]
   52eac:	stp	x28, x27, [sp, #32]
   52eb0:	stp	x26, x25, [sp, #48]
   52eb4:	stp	x24, x23, [sp, #64]
   52eb8:	stp	x22, x21, [sp, #80]
   52ebc:	stp	x20, x19, [sp, #96]
   52ec0:	sub	x26, x4, #0x1
   52ec4:	ldr	x8, [x3, x26, lsl #3]
   52ec8:	mov	x21, x5
   52ecc:	mov	x19, x4
   52ed0:	mov	x25, x3
   52ed4:	mov	x22, x2
   52ed8:	mov	x20, x1
   52edc:	clz	x23, x8
   52ee0:	mov	x24, x0
   52ee4:	add	x29, sp, #0x10
   52ee8:	cbz	w23, 52fb0 <__gmpn_sec_div_qr@@Base+0x10c>
   52eec:	mov	x0, x21
   52ef0:	mov	x1, x25
   52ef4:	mov	x2, x19
   52ef8:	mov	w3, w23
   52efc:	bl	c180 <__gmpn_lshift@plt>
   52f00:	lsl	x28, x19, #3
   52f04:	add	x25, x21, x28
   52f08:	mov	x0, x25
   52f0c:	mov	x1, x20
   52f10:	mov	x2, x22
   52f14:	mov	w3, w23
   52f18:	bl	c180 <__gmpn_lshift@plt>
   52f1c:	str	x24, [sp, #8]
   52f20:	lsl	x24, x22, #3
   52f24:	str	x0, [x25, x24]
   52f28:	ldr	x8, [x21, x26, lsl #3]
   52f2c:	add	x26, x22, #0x1
   52f30:	cmn	x8, #0x1
   52f34:	cinc	x0, x8, ne  // ne = any
   52f38:	bl	d3f0 <__gmpn_invert_limb@plt>
   52f3c:	add	x27, x25, x28
   52f40:	add	x8, x21, x26, lsl #3
   52f44:	mov	x5, x0
   52f48:	add	x6, x8, x28
   52f4c:	mov	x0, x27
   52f50:	mov	x1, x25
   52f54:	mov	x2, x26
   52f58:	mov	x3, x21
   52f5c:	mov	x4, x19
   52f60:	bl	cef0 <__gmpn_sec_pi1_div_qr@plt>
   52f64:	ldr	x0, [sp, #8]
   52f68:	sub	x2, x22, x19
   52f6c:	mov	x1, x27
   52f70:	bl	ca50 <__gmpn_copyi@plt>
   52f74:	ldr	x21, [x25, x24]
   52f78:	mov	x0, x20
   52f7c:	mov	x1, x25
   52f80:	mov	x2, x19
   52f84:	mov	w3, w23
   52f88:	bl	c1a0 <__gmpn_rshift@plt>
   52f8c:	mov	x0, x21
   52f90:	ldp	x20, x19, [sp, #96]
   52f94:	ldp	x22, x21, [sp, #80]
   52f98:	ldp	x24, x23, [sp, #64]
   52f9c:	ldp	x26, x25, [sp, #48]
   52fa0:	ldp	x28, x27, [sp, #32]
   52fa4:	ldp	x29, x30, [sp, #16]
   52fa8:	add	sp, sp, #0x70
   52fac:	ret
   52fb0:	cmn	x8, #0x1
   52fb4:	cinc	x0, x8, ne  // ne = any
   52fb8:	bl	d3f0 <__gmpn_invert_limb@plt>
   52fbc:	mov	x5, x0
   52fc0:	mov	x0, x24
   52fc4:	mov	x1, x20
   52fc8:	mov	x2, x22
   52fcc:	mov	x3, x25
   52fd0:	mov	x4, x19
   52fd4:	mov	x6, x21
   52fd8:	ldp	x20, x19, [sp, #96]
   52fdc:	ldp	x22, x21, [sp, #80]
   52fe0:	ldp	x24, x23, [sp, #64]
   52fe4:	ldp	x26, x25, [sp, #48]
   52fe8:	ldp	x28, x27, [sp, #32]
   52fec:	ldp	x29, x30, [sp, #16]
   52ff0:	add	sp, sp, #0x70
   52ff4:	b	cef0 <__gmpn_sec_pi1_div_qr@plt>

0000000000052ff8 <__gmpn_sec_div_r_itch@@Base>:
   52ff8:	add	x8, x0, x1, lsl #1
   52ffc:	add	x0, x8, #0x2
   53000:	ret

0000000000053004 <__gmpn_sec_div_r@@Base>:
   53004:	stp	x29, x30, [sp, #-80]!
   53008:	stp	x26, x25, [sp, #16]
   5300c:	stp	x24, x23, [sp, #32]
   53010:	stp	x22, x21, [sp, #48]
   53014:	stp	x20, x19, [sp, #64]
   53018:	sub	x25, x3, #0x1
   5301c:	ldr	x8, [x2, x25, lsl #3]
   53020:	mov	x20, x4
   53024:	mov	x19, x3
   53028:	mov	x24, x2
   5302c:	mov	x23, x1
   53030:	clz	x22, x8
   53034:	mov	x21, x0
   53038:	mov	x29, sp
   5303c:	cbz	w22, 530d0 <__gmpn_sec_div_r@@Base+0xcc>
   53040:	mov	x0, x20
   53044:	mov	x1, x24
   53048:	mov	x2, x19
   5304c:	mov	w3, w22
   53050:	bl	c180 <__gmpn_lshift@plt>
   53054:	lsl	x26, x19, #3
   53058:	add	x24, x20, x26
   5305c:	mov	x0, x24
   53060:	mov	x1, x21
   53064:	mov	x2, x23
   53068:	mov	w3, w22
   5306c:	bl	c180 <__gmpn_lshift@plt>
   53070:	str	x0, [x24, x23, lsl #3]
   53074:	ldr	x8, [x20, x25, lsl #3]
   53078:	add	x23, x23, #0x1
   5307c:	cmn	x8, #0x1
   53080:	cinc	x0, x8, ne  // ne = any
   53084:	bl	d3f0 <__gmpn_invert_limb@plt>
   53088:	add	x8, x20, x23, lsl #3
   5308c:	mov	x4, x0
   53090:	add	x5, x8, x26
   53094:	mov	x0, x24
   53098:	mov	x1, x23
   5309c:	mov	x2, x20
   530a0:	mov	x3, x19
   530a4:	bl	c800 <__gmpn_sec_pi1_div_r@plt>
   530a8:	mov	x0, x21
   530ac:	mov	x1, x24
   530b0:	mov	x2, x19
   530b4:	mov	w3, w22
   530b8:	ldp	x20, x19, [sp, #64]
   530bc:	ldp	x22, x21, [sp, #48]
   530c0:	ldp	x24, x23, [sp, #32]
   530c4:	ldp	x26, x25, [sp, #16]
   530c8:	ldp	x29, x30, [sp], #80
   530cc:	b	c1a0 <__gmpn_rshift@plt>
   530d0:	cmn	x8, #0x1
   530d4:	cinc	x0, x8, ne  // ne = any
   530d8:	bl	d3f0 <__gmpn_invert_limb@plt>
   530dc:	mov	x4, x0
   530e0:	mov	x0, x21
   530e4:	mov	x1, x23
   530e8:	mov	x2, x24
   530ec:	mov	x3, x19
   530f0:	mov	x5, x20
   530f4:	ldp	x20, x19, [sp, #64]
   530f8:	ldp	x22, x21, [sp, #48]
   530fc:	ldp	x24, x23, [sp, #32]
   53100:	ldp	x26, x25, [sp, #16]
   53104:	ldp	x29, x30, [sp], #80
   53108:	b	c800 <__gmpn_sec_pi1_div_r@plt>

000000000005310c <__gmpn_sec_pi1_div_qr@@Base>:
   5310c:	sub	sp, sp, #0xa0
   53110:	stp	x29, x30, [sp, #64]
   53114:	stp	x28, x27, [sp, #80]
   53118:	stp	x22, x21, [sp, #128]
   5311c:	stp	x20, x19, [sp, #144]
   53120:	add	x29, sp, #0x40
   53124:	mov	x28, x3
   53128:	mov	x21, x2
   5312c:	mov	x19, x1
   53130:	subs	x22, x2, x4
   53134:	stp	x26, x25, [sp, #96]
   53138:	stp	x24, x23, [sp, #112]
   5313c:	stur	x6, [x29, #-8]
   53140:	b.ne	5317c <__gmpn_sec_pi1_div_qr@@Base+0x70>  // b.any
   53144:	mov	x0, x19
   53148:	mov	x1, x19
   5314c:	mov	x2, x28
   53150:	mov	x3, x21
   53154:	bl	c2d0 <__gmpn_sub_n@plt>
   53158:	mov	x1, x19
   5315c:	mov	x2, x19
   53160:	mov	x3, x28
   53164:	mov	x4, x21
   53168:	mov	x20, x0
   5316c:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   53170:	mov	w8, #0x1                   	// #1
   53174:	sub	x0, x8, x20
   53178:	b	5337c <__gmpn_sec_pi1_div_qr@@Base+0x270>
   5317c:	mov	x20, x19
   53180:	ldur	x19, [x29, #-8]
   53184:	mov	x23, x0
   53188:	mov	w3, #0x20                  	// #32
   5318c:	mov	x1, x28
   53190:	mov	x0, x19
   53194:	mov	x2, x4
   53198:	mov	x27, x5
   5319c:	mov	x26, x4
   531a0:	bl	c180 <__gmpn_lshift@plt>
   531a4:	add	x12, x26, #0x1
   531a8:	add	x8, x19, x21, lsl #3
   531ac:	cmp	x22, #0x1
   531b0:	add	x24, x19, x12, lsl #3
   531b4:	add	x25, x8, #0x8
   531b8:	add	x10, x20, x22, lsl #3
   531bc:	str	x0, [x19, x26, lsl #3]
   531c0:	b.lt	532a0 <__gmpn_sec_pi1_div_qr@@Base+0x194>  // b.tstop
   531c4:	ldur	x11, [x29, #-8]
   531c8:	lsl	x9, x21, #1
   531cc:	mov	x14, x20
   531d0:	lsl	x8, x21, #3
   531d4:	sub	x9, x9, x26
   531d8:	stp	x25, x24, [sp]
   531dc:	stp	x23, x22, [sp, #16]
   531e0:	mov	x23, xzr
   531e4:	mov	x20, xzr
   531e8:	add	x24, x22, #0x1
   531ec:	add	x13, x11, x8
   531f0:	add	x9, x11, x9, lsl #3
   531f4:	add	x8, x14, x8
   531f8:	stp	x9, x13, [x29, #-24]
   531fc:	str	x8, [sp, #32]
   53200:	ldr	x8, [sp, #32]
   53204:	mov	x19, x26
   53208:	mov	x26, x28
   5320c:	add	x9, x10, x23
   53210:	add	x28, x8, x23
   53214:	ldur	x8, [x28, #-8]
   53218:	sub	x21, x9, #0x8
   5321c:	ldur	x1, [x29, #-8]
   53220:	mov	x0, x21
   53224:	extr	x8, x20, x8, #32
   53228:	umulh	x9, x8, x27
   5322c:	add	x3, x8, x9
   53230:	ldur	x8, [x29, #-24]
   53234:	mov	x2, x12
   53238:	mov	x22, x10
   5323c:	mov	x25, x12
   53240:	str	x3, [x8, x23]
   53244:	bl	c9e0 <__gmpn_submul_1@plt>
   53248:	ldur	x20, [x28, #-8]
   5324c:	umulh	x8, x20, x27
   53250:	mov	x28, x26
   53254:	mov	x0, x21
   53258:	add	x3, x8, x20
   5325c:	ldur	x8, [x29, #-16]
   53260:	mov	x1, x28
   53264:	mov	x2, x19
   53268:	mov	x26, x19
   5326c:	str	x3, [x8, x23]
   53270:	bl	c9e0 <__gmpn_submul_1@plt>
   53274:	sub	x24, x24, #0x1
   53278:	mov	x12, x25
   5327c:	mov	x10, x22
   53280:	sub	x20, x20, x0
   53284:	cmp	x24, #0x1
   53288:	sub	x23, x23, #0x8
   5328c:	b.gt	53200 <__gmpn_sec_pi1_div_qr@@Base+0xf4>
   53290:	add	x10, x10, x23
   53294:	ldp	x23, x22, [sp, #16]
   53298:	ldp	x25, x24, [sp]
   5329c:	b	532a4 <__gmpn_sec_pi1_div_qr@@Base+0x198>
   532a0:	mov	x20, xzr
   532a4:	ldr	x8, [x24]
   532a8:	cmp	x20, #0x0
   532ac:	cset	w0, ne  // ne = any
   532b0:	mov	x1, x10
   532b4:	cinc	x8, x8, ne  // ne = any
   532b8:	mov	x2, x10
   532bc:	mov	x3, x28
   532c0:	mov	x4, x26
   532c4:	str	x8, [x24]
   532c8:	mov	x19, x10
   532cc:	bl	c020 <__gmpn_cnd_sub_n@plt>
   532d0:	mov	x21, x0
   532d4:	mov	x0, x19
   532d8:	mov	x1, x19
   532dc:	mov	x2, x28
   532e0:	mov	x3, x26
   532e4:	bl	c2d0 <__gmpn_sub_n@plt>
   532e8:	ldr	x8, [x24]
   532ec:	sub	x9, x21, x20
   532f0:	add	x0, x0, x9
   532f4:	mov	x1, x19
   532f8:	sub	x8, x8, x0
   532fc:	add	x8, x8, #0x1
   53300:	mov	x2, x19
   53304:	mov	x3, x28
   53308:	mov	x4, x26
   5330c:	str	x8, [x24]
   53310:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   53314:	mov	x0, x19
   53318:	mov	x1, x19
   5331c:	mov	x2, x28
   53320:	mov	x3, x26
   53324:	bl	c2d0 <__gmpn_sub_n@plt>
   53328:	ldr	x8, [x24]
   5332c:	mov	x1, x19
   53330:	mov	x2, x19
   53334:	mov	x3, x28
   53338:	sub	x8, x8, x0
   5333c:	add	x8, x8, #0x1
   53340:	mov	x4, x26
   53344:	str	x8, [x24]
   53348:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   5334c:	mov	w3, #0x20                  	// #32
   53350:	mov	x0, x25
   53354:	mov	x1, x25
   53358:	mov	x2, x22
   5335c:	bl	c180 <__gmpn_lshift@plt>
   53360:	mov	x19, x0
   53364:	mov	x0, x23
   53368:	mov	x1, x25
   5336c:	mov	x2, x24
   53370:	mov	x3, x22
   53374:	bl	ca70 <__gmpn_add_n@plt>
   53378:	add	x0, x0, x19
   5337c:	ldp	x20, x19, [sp, #144]
   53380:	ldp	x22, x21, [sp, #128]
   53384:	ldp	x24, x23, [sp, #112]
   53388:	ldp	x26, x25, [sp, #96]
   5338c:	ldp	x28, x27, [sp, #80]
   53390:	ldp	x29, x30, [sp, #64]
   53394:	add	sp, sp, #0xa0
   53398:	ret

000000000005339c <__gmpn_sec_pi1_div_r@@Base>:
   5339c:	stp	x29, x30, [sp, #-96]!
   533a0:	stp	x28, x27, [sp, #16]
   533a4:	stp	x26, x25, [sp, #32]
   533a8:	stp	x24, x23, [sp, #48]
   533ac:	stp	x20, x19, [sp, #80]
   533b0:	mov	x19, x2
   533b4:	mov	x24, x1
   533b8:	subs	x28, x1, x3
   533bc:	mov	x25, x0
   533c0:	stp	x22, x21, [sp, #64]
   533c4:	mov	x29, sp
   533c8:	b.ne	533f4 <__gmpn_sec_pi1_div_r@@Base+0x58>  // b.any
   533cc:	mov	x0, x25
   533d0:	mov	x1, x25
   533d4:	mov	x2, x19
   533d8:	mov	x3, x24
   533dc:	bl	c2d0 <__gmpn_sub_n@plt>
   533e0:	mov	x1, x25
   533e4:	mov	x2, x25
   533e8:	mov	x3, x19
   533ec:	mov	x4, x24
   533f0:	b	53508 <__gmpn_sec_pi1_div_r@@Base+0x16c>
   533f4:	mov	x20, x3
   533f8:	mov	w3, #0x20                  	// #32
   533fc:	mov	x0, x5
   53400:	mov	x1, x19
   53404:	mov	x2, x20
   53408:	mov	x21, x5
   5340c:	mov	x22, x4
   53410:	bl	c180 <__gmpn_lshift@plt>
   53414:	cmp	x28, #0x1
   53418:	mov	x26, xzr
   5341c:	str	x0, [x21, x20, lsl #3]
   53420:	b.lt	53490 <__gmpn_sec_pi1_div_r@@Base+0xf4>  // b.tstop
   53424:	add	x23, x20, #0x1
   53428:	add	x25, x25, x24, lsl #3
   5342c:	neg	x27, x20, lsl #3
   53430:	add	x28, x28, #0x1
   53434:	add	x8, x25, x27
   53438:	ldr	x9, [x25, #-8]!
   5343c:	sub	x24, x8, #0x8
   53440:	mov	x0, x24
   53444:	mov	x1, x21
   53448:	extr	x8, x26, x9, #32
   5344c:	umulh	x9, x8, x22
   53450:	add	x3, x8, x9
   53454:	mov	x2, x23
   53458:	bl	c9e0 <__gmpn_submul_1@plt>
   5345c:	ldr	x26, [x25]
   53460:	umulh	x8, x26, x22
   53464:	mov	x0, x24
   53468:	mov	x1, x19
   5346c:	add	x3, x8, x26
   53470:	mov	x2, x20
   53474:	bl	c9e0 <__gmpn_submul_1@plt>
   53478:	sub	x28, x28, #0x1
   5347c:	cmp	x28, #0x1
   53480:	sub	x26, x26, x0
   53484:	b.gt	53434 <__gmpn_sec_pi1_div_r@@Base+0x98>
   53488:	sub	x21, x25, x20, lsl #3
   5348c:	b	53494 <__gmpn_sec_pi1_div_r@@Base+0xf8>
   53490:	add	x21, x25, x28, lsl #3
   53494:	cmp	x26, #0x0
   53498:	cset	w0, ne  // ne = any
   5349c:	mov	x1, x21
   534a0:	mov	x2, x21
   534a4:	mov	x3, x19
   534a8:	mov	x4, x20
   534ac:	bl	c020 <__gmpn_cnd_sub_n@plt>
   534b0:	mov	x22, x0
   534b4:	mov	x0, x21
   534b8:	mov	x1, x21
   534bc:	mov	x2, x19
   534c0:	mov	x3, x20
   534c4:	bl	c2d0 <__gmpn_sub_n@plt>
   534c8:	sub	x8, x22, x26
   534cc:	add	x0, x8, x0
   534d0:	mov	x1, x21
   534d4:	mov	x2, x21
   534d8:	mov	x3, x19
   534dc:	mov	x4, x20
   534e0:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   534e4:	mov	x0, x21
   534e8:	mov	x1, x21
   534ec:	mov	x2, x19
   534f0:	mov	x3, x20
   534f4:	bl	c2d0 <__gmpn_sub_n@plt>
   534f8:	mov	x1, x21
   534fc:	mov	x2, x21
   53500:	mov	x3, x19
   53504:	mov	x4, x20
   53508:	ldp	x20, x19, [sp, #80]
   5350c:	ldp	x22, x21, [sp, #64]
   53510:	ldp	x24, x23, [sp, #48]
   53514:	ldp	x26, x25, [sp, #32]
   53518:	ldp	x28, x27, [sp, #16]
   5351c:	ldp	x29, x30, [sp], #96
   53520:	b	d4d0 <__gmpn_cnd_add_n@plt>

0000000000053524 <__gmpn_sec_add_1_itch@@Base>:
   53524:	ret

0000000000053528 <__gmpn_sec_add_1@@Base>:
   53528:	stp	x29, x30, [sp, #-48]!
   5352c:	stp	x22, x21, [sp, #16]
   53530:	stp	x20, x19, [sp, #32]
   53534:	mov	x19, x4
   53538:	mov	x20, x2
   5353c:	mov	x21, x1
   53540:	mov	x22, x0
   53544:	cmp	x2, #0x1
   53548:	mov	x29, sp
   5354c:	str	x3, [x4]
   53550:	b.eq	53568 <__gmpn_sec_add_1@@Base+0x40>  // b.none
   53554:	lsl	x8, x20, #3
   53558:	add	x0, x19, #0x8
   5355c:	sub	x2, x8, #0x8
   53560:	mov	w1, wzr
   53564:	bl	c5f0 <memset@plt>
   53568:	mov	x0, x22
   5356c:	mov	x1, x21
   53570:	mov	x2, x19
   53574:	mov	x3, x20
   53578:	ldp	x20, x19, [sp, #32]
   5357c:	ldp	x22, x21, [sp, #16]
   53580:	ldp	x29, x30, [sp], #48
   53584:	b	ca70 <__gmpn_add_n@plt>

0000000000053588 <__gmpn_sec_sub_1_itch@@Base>:
   53588:	ret

000000000005358c <__gmpn_sec_sub_1@@Base>:
   5358c:	stp	x29, x30, [sp, #-48]!
   53590:	stp	x22, x21, [sp, #16]
   53594:	stp	x20, x19, [sp, #32]
   53598:	mov	x19, x4
   5359c:	mov	x20, x2
   535a0:	mov	x21, x1
   535a4:	mov	x22, x0
   535a8:	cmp	x2, #0x1
   535ac:	mov	x29, sp
   535b0:	str	x3, [x4]
   535b4:	b.eq	535cc <__gmpn_sec_sub_1@@Base+0x40>  // b.none
   535b8:	lsl	x8, x20, #3
   535bc:	add	x0, x19, #0x8
   535c0:	sub	x2, x8, #0x8
   535c4:	mov	w1, wzr
   535c8:	bl	c5f0 <memset@plt>
   535cc:	mov	x0, x22
   535d0:	mov	x1, x21
   535d4:	mov	x2, x19
   535d8:	mov	x3, x20
   535dc:	ldp	x20, x19, [sp, #32]
   535e0:	ldp	x22, x21, [sp, #16]
   535e4:	ldp	x29, x30, [sp], #48
   535e8:	b	c2d0 <__gmpn_sub_n@plt>

00000000000535ec <__gmpn_sec_invert_itch@@Base>:
   535ec:	lsl	x0, x0, #2
   535f0:	ret

00000000000535f4 <__gmpn_sec_invert@@Base>:
   535f4:	sub	sp, sp, #0x70
   535f8:	stp	x26, x25, [sp, #48]
   535fc:	add	x26, x5, x3, lsl #4
   53600:	stp	x20, x19, [sp, #96]
   53604:	mov	x19, x0
   53608:	mov	w8, #0x1                   	// #1
   5360c:	mov	x0, x26
   53610:	stp	x29, x30, [sp, #16]
   53614:	stp	x28, x27, [sp, #32]
   53618:	stp	x24, x23, [sp, #64]
   5361c:	stp	x22, x21, [sp, #80]
   53620:	mov	x24, x1
   53624:	str	x8, [x0], #8
   53628:	sub	x1, x3, #0x1
   5362c:	add	x29, sp, #0x10
   53630:	mov	x25, x5
   53634:	mov	x22, x4
   53638:	mov	x21, x3
   5363c:	mov	x28, x2
   53640:	str	x1, [sp]
   53644:	bl	cf30 <__gmpn_zero@plt>
   53648:	add	x27, x25, x21, lsl #3
   5364c:	mov	x0, x27
   53650:	mov	x1, x28
   53654:	mov	x2, x21
   53658:	bl	ca50 <__gmpn_copyi@plt>
   5365c:	mov	x0, x19
   53660:	mov	x1, x21
   53664:	str	x19, [sp, #8]
   53668:	bl	cf30 <__gmpn_zero@plt>
   5366c:	mov	w8, #0x18                  	// #24
   53670:	madd	x23, x21, x8, x25
   53674:	mov	w3, #0x1                   	// #1
   53678:	mov	x0, x23
   5367c:	mov	x1, x28
   53680:	mov	x2, x21
   53684:	bl	c1a0 <__gmpn_rshift@plt>
   53688:	mov	w3, #0x1                   	// #1
   5368c:	mov	x0, x23
   53690:	mov	x1, x23
   53694:	mov	x2, x21
   53698:	mov	x4, x25
   5369c:	bl	c380 <__gmpn_sec_add_1@plt>
   536a0:	cbz	x22, 53798 <__gmpn_sec_invert@@Base+0x1a4>
   536a4:	ldr	x8, [x24]
   536a8:	mov	x1, x24
   536ac:	mov	x2, x24
   536b0:	mov	x3, x27
   536b4:	and	x20, x8, #0x1
   536b8:	mov	x0, x20
   536bc:	mov	x4, x21
   536c0:	sub	x22, x22, #0x1
   536c4:	bl	c020 <__gmpn_cnd_sub_n@plt>
   536c8:	mov	x1, x27
   536cc:	mov	x2, x27
   536d0:	mov	x3, x24
   536d4:	mov	x4, x21
   536d8:	mov	x19, x23
   536dc:	mov	x23, x0
   536e0:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   536e4:	mov	w3, #0x1                   	// #1
   536e8:	mov	x0, x25
   536ec:	mov	x1, x24
   536f0:	mov	x2, x21
   536f4:	bl	c180 <__gmpn_lshift@plt>
   536f8:	sxtw	x0, w23
   536fc:	mov	x1, x24
   53700:	mov	x2, x24
   53704:	mov	x3, x25
   53708:	mov	x4, x21
   5370c:	bl	c020 <__gmpn_cnd_sub_n@plt>
   53710:	mov	x0, x23
   53714:	mov	x23, x19
   53718:	ldr	x19, [sp, #8]
   5371c:	mov	x1, x26
   53720:	mov	x3, x21
   53724:	mov	x2, x19
   53728:	bl	c660 <__gmpn_cnd_swap@plt>
   5372c:	mov	x0, x20
   53730:	mov	x1, x26
   53734:	mov	x2, x26
   53738:	mov	x3, x19
   5373c:	mov	x4, x21
   53740:	bl	c020 <__gmpn_cnd_sub_n@plt>
   53744:	mov	x1, x26
   53748:	mov	x2, x26
   5374c:	mov	x3, x28
   53750:	mov	x4, x21
   53754:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   53758:	mov	w3, #0x1                   	// #1
   5375c:	mov	x0, x24
   53760:	mov	x1, x24
   53764:	mov	x2, x21
   53768:	bl	c1a0 <__gmpn_rshift@plt>
   5376c:	mov	w3, #0x1                   	// #1
   53770:	mov	x0, x26
   53774:	mov	x1, x26
   53778:	mov	x2, x21
   5377c:	bl	c1a0 <__gmpn_rshift@plt>
   53780:	mov	x1, x26
   53784:	mov	x2, x26
   53788:	mov	x3, x23
   5378c:	mov	x4, x21
   53790:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   53794:	cbnz	x22, 536a4 <__gmpn_sec_invert@@Base+0xb0>
   53798:	ldr	x8, [x27]
   5379c:	cmp	x21, #0x2
   537a0:	eor	x8, x8, #0x1
   537a4:	b.lt	53808 <__gmpn_sec_invert@@Base+0x214>  // b.tstop
   537a8:	ldr	x15, [sp]
   537ac:	cmp	x15, #0x2
   537b0:	b.cc	537f0 <__gmpn_sec_invert@@Base+0x1fc>  // b.lo, b.ul, b.last
   537b4:	and	x10, x15, #0xfffffffffffffffe
   537b8:	add	x11, x25, x21, lsl #4
   537bc:	mov	x9, xzr
   537c0:	sub	x21, x21, x10
   537c4:	sub	x11, x11, #0x8
   537c8:	mov	x12, x10
   537cc:	ldp	x14, x13, [x11, #-8]
   537d0:	subs	x12, x12, #0x2
   537d4:	sub	x11, x11, #0x10
   537d8:	orr	x8, x13, x8
   537dc:	orr	x9, x14, x9
   537e0:	b.ne	537cc <__gmpn_sec_invert@@Base+0x1d8>  // b.any
   537e4:	cmp	x15, x10
   537e8:	orr	x8, x9, x8
   537ec:	b.eq	53808 <__gmpn_sec_invert@@Base+0x214>  // b.none
   537f0:	add	x9, x25, x15, lsl #3
   537f4:	ldr	x10, [x9, x21, lsl #3]
   537f8:	cmp	x21, #0x2
   537fc:	sub	x21, x21, #0x1
   53800:	orr	x8, x10, x8
   53804:	b.gt	537f4 <__gmpn_sec_invert@@Base+0x200>
   53808:	ldp	x20, x19, [sp, #96]
   5380c:	ldp	x22, x21, [sp, #80]
   53810:	ldp	x24, x23, [sp, #64]
   53814:	ldp	x26, x25, [sp, #48]
   53818:	ldp	x28, x27, [sp, #32]
   5381c:	ldp	x29, x30, [sp, #16]
   53820:	cmp	x8, #0x0
   53824:	cset	w0, eq  // eq = none
   53828:	add	sp, sp, #0x70
   5382c:	ret

0000000000053830 <__gmpn_trialdiv@@Base>:
   53830:	stp	x29, x30, [sp, #-96]!
   53834:	stp	x26, x25, [sp, #32]
   53838:	stp	x24, x23, [sp, #48]
   5383c:	stp	x22, x21, [sp, #64]
   53840:	stp	x20, x19, [sp, #80]
   53844:	ldr	w23, [x3]
   53848:	str	x27, [sp, #16]
   5384c:	mov	x29, sp
   53850:	cmp	w23, #0xc6
   53854:	b.hi	538f0 <__gmpn_trialdiv@@Base+0xc0>  // b.pmore
   53858:	adrp	x25, 61000 <__gmp_jacobi_table@@Base+0x39d9>
   5385c:	adrp	x26, 5d000 <__gmpn_bases@@Base+0x2ab8>
   53860:	mov	x19, x3
   53864:	mov	x20, x2
   53868:	mov	x21, x1
   5386c:	mov	x22, x0
   53870:	mov	w24, #0x48                  	// #72
   53874:	add	x25, x25, #0x718
   53878:	add	x26, x26, #0x868
   5387c:	madd	x27, x23, x24, x25
   53880:	ldr	x8, [x27]
   53884:	ldr	x9, [x27, #16]
   53888:	add	x3, x27, #0x8
   5388c:	mov	x0, x22
   53890:	mov	x1, x21
   53894:	lsl	x2, x8, x9
   53898:	bl	d410 <__gmpn_mod_1s_4p@plt>
   5389c:	ldr	w8, [x27, #64]
   538a0:	lsr	x9, x8, #24
   538a4:	cbz	w9, 538d8 <__gmpn_trialdiv@@Base+0xa8>
   538a8:	and	x8, x8, #0xffffff
   538ac:	add	x8, x26, x8, lsl #4
   538b0:	mvn	x10, x9
   538b4:	add	x11, x8, #0x8
   538b8:	ldp	x8, x12, [x11, #-8]
   538bc:	mul	x13, x8, x0
   538c0:	cmp	x13, x12
   538c4:	b.ls	538f8 <__gmpn_trialdiv@@Base+0xc8>  // b.plast
   538c8:	add	x10, x10, #0x1
   538cc:	cmn	x10, #0x2
   538d0:	add	x11, x11, #0x10
   538d4:	b.le	538b8 <__gmpn_trialdiv@@Base+0x88>
   538d8:	sub	x20, x20, x9
   538dc:	cmp	x20, #0x1
   538e0:	b.lt	538f0 <__gmpn_trialdiv@@Base+0xc0>  // b.tstop
   538e4:	cmp	x23, #0xc6
   538e8:	add	x23, x23, #0x1
   538ec:	b.cc	5387c <__gmpn_trialdiv@@Base+0x4c>  // b.lo, b.ul, b.last
   538f0:	mov	x8, xzr
   538f4:	b	538fc <__gmpn_trialdiv@@Base+0xcc>
   538f8:	str	w23, [x19]
   538fc:	ldp	x20, x19, [sp, #80]
   53900:	ldp	x22, x21, [sp, #64]
   53904:	ldp	x24, x23, [sp, #48]
   53908:	ldp	x26, x25, [sp, #32]
   5390c:	ldr	x27, [sp, #16]
   53910:	mov	x0, x8
   53914:	ldp	x29, x30, [sp], #96
   53918:	ret

000000000005391c <__gmpn_remove@@Base>:
   5391c:	stp	x29, x30, [sp, #-96]!
   53920:	stp	x28, x27, [sp, #16]
   53924:	stp	x26, x25, [sp, #32]
   53928:	stp	x24, x23, [sp, #48]
   5392c:	stp	x22, x21, [sp, #64]
   53930:	stp	x20, x19, [sp, #80]
   53934:	mov	x29, sp
   53938:	sub	sp, sp, #0x370
   5393c:	add	x22, x3, #0x1
   53940:	add	x8, x22, x5
   53944:	cmp	x8, #0x0
   53948:	cinc	x8, x8, lt  // lt = tstop
   5394c:	lsr	x8, x8, #1
   53950:	add	x8, x8, x22, lsl #1
   53954:	mov	x9, x1
   53958:	lsl	x1, x8, #3
   5395c:	mov	w8, #0x7f00                	// #32512
   53960:	mov	x19, sp
   53964:	mov	x20, x5
   53968:	mov	x21, x4
   5396c:	mov	x27, x3
   53970:	mov	x23, x2
   53974:	cmp	x1, x8
   53978:	str	x6, [x19, #48]
   5397c:	str	xzr, [x19, #64]
   53980:	stp	x0, x9, [x19, #8]
   53984:	b.hi	53d10 <__gmpn_remove@@Base+0x3f4>  // b.pmore
   53988:	add	x9, x1, #0xf
   5398c:	mov	x8, sp
   53990:	and	x9, x9, #0xfffffffffffffff0
   53994:	sub	x25, x8, x9
   53998:	mov	sp, x25
   5399c:	mov	x0, x25
   539a0:	mov	x1, x23
   539a4:	mov	x2, x27
   539a8:	bl	ca50 <__gmpn_copyi@plt>
   539ac:	cmp	x27, x20
   539b0:	b.ge	539bc <__gmpn_remove@@Base+0xa0>  // b.tcont
   539b4:	mov	x21, xzr
   539b8:	b	53ccc <__gmpn_remove@@Base+0x3b0>
   539bc:	lsl	x8, x22, #3
   539c0:	add	x10, x25, x8
   539c4:	add	x9, x25, x27, lsl #4
   539c8:	add	x8, x10, x8
   539cc:	mov	x24, xzr
   539d0:	add	x26, x9, #0x8
   539d4:	str	x8, [x19, #24]
   539d8:	mov	x8, x25
   539dc:	stp	x27, x10, [x19, #32]
   539e0:	str	x25, [x19]
   539e4:	ldr	x25, [x19, #40]
   539e8:	ldr	x1, [x19, #24]
   539ec:	mov	x2, x8
   539f0:	add	x3, x27, #0x1
   539f4:	mov	x0, x25
   539f8:	mov	x4, x21
   539fc:	mov	x5, x20
   53a00:	mov	x28, x24
   53a04:	str	xzr, [x8, x27, lsl #3]
   53a08:	str	x8, [x19, #40]
   53a0c:	bl	53d28 <__gmpn_remove@@Base+0x40c>
   53a10:	mov	x8, x20
   53a14:	ldr	x9, [x26, x8, lsl #3]
   53a18:	cbnz	x9, 53ac0 <__gmpn_remove@@Base+0x1a4>
   53a1c:	sub	x8, x8, #0x1
   53a20:	cbnz	x8, 53a14 <__gmpn_remove@@Base+0xf8>
   53a24:	ldr	x10, [x25]
   53a28:	sub	x22, x27, x20
   53a2c:	add	x8, x22, #0x1
   53a30:	mov	x9, x25
   53a34:	cbnz	x10, 53a4c <__gmpn_remove@@Base+0x130>
   53a38:	subs	x8, x8, #0x1
   53a3c:	str	xzr, [x9]
   53a40:	b.eq	53a68 <__gmpn_remove@@Base+0x14c>  // b.none
   53a44:	ldr	x10, [x9, #8]!
   53a48:	cbz	x10, 53a38 <__gmpn_remove@@Base+0x11c>
   53a4c:	neg	x10, x10
   53a50:	subs	x2, x8, #0x1
   53a54:	str	x10, [x9]
   53a58:	b.eq	53a68 <__gmpn_remove@@Base+0x14c>  // b.none
   53a5c:	add	x0, x9, #0x8
   53a60:	mov	x1, x0
   53a64:	bl	c290 <__gmpn_com@plt>
   53a68:	ldr	x8, [x25, x22, lsl #3]
   53a6c:	lsl	x9, x28, #3
   53a70:	mov	w10, #0x4                   	// #4
   53a74:	add	x11, x19, #0x1d8
   53a78:	cmp	x8, #0x0
   53a7c:	ldr	x8, [x19, #48]
   53a80:	lsl	x10, x10, x28
   53a84:	str	x21, [x11, x9]
   53a88:	add	x11, x19, #0x48
   53a8c:	str	x20, [x11, x9]
   53a90:	sub	x9, x10, #0x1
   53a94:	cinc	x27, x22, ne  // ne = any
   53a98:	cmp	x9, x8
   53a9c:	add	x24, x28, #0x1
   53aa0:	b.hi	53b78 <__gmpn_remove@@Base+0x25c>  // b.pmore
   53aa4:	lsl	x8, x20, #1
   53aa8:	sub	x22, x8, #0x1
   53aac:	cmp	x22, x27
   53ab0:	b.gt	53b78 <__gmpn_remove@@Base+0x25c>
   53ab4:	cbz	x28, 53aec <__gmpn_remove@@Base+0x1d0>
   53ab8:	add	x23, x23, x20, lsl #3
   53abc:	b	53b14 <__gmpn_remove@@Base+0x1f8>
   53ac0:	sub	x8, x21, #0x8
   53ac4:	mov	x9, x20
   53ac8:	subs	x10, x9, #0x1
   53acc:	b.lt	53a24 <__gmpn_remove@@Base+0x108>  // b.tstop
   53ad0:	lsl	x9, x9, #3
   53ad4:	ldr	x11, [x26, x9]
   53ad8:	ldr	x9, [x8, x9]
   53adc:	cmp	x11, x9
   53ae0:	mov	x9, x10
   53ae4:	b.eq	53ac8 <__gmpn_remove@@Base+0x1ac>  // b.none
   53ae8:	b	53b54 <__gmpn_remove@@Base+0x238>
   53aec:	lsl	x8, x27, #3
   53af0:	add	x1, x8, #0x190
   53af4:	mov	w8, #0x7f00                	// #32512
   53af8:	cmp	x1, x8
   53afc:	b.hi	53b44 <__gmpn_remove@@Base+0x228>  // b.pmore
   53b00:	add	x9, x1, #0xf
   53b04:	mov	x8, sp
   53b08:	and	x9, x9, #0xfffffffffffffff0
   53b0c:	sub	x23, x8, x9
   53b10:	mov	sp, x23
   53b14:	mov	x0, x23
   53b18:	mov	x1, x21
   53b1c:	mov	x2, x20
   53b20:	bl	c8e0 <__gmpn_sqr@plt>
   53b24:	ldr	x8, [x23, x22, lsl #3]
   53b28:	mov	x21, x23
   53b2c:	cmp	x8, #0x0
   53b30:	cinc	x20, x22, ne  // ne = any
   53b34:	cmp	x27, x20
   53b38:	mov	x8, x25
   53b3c:	b.ge	539e4 <__gmpn_remove@@Base+0xc8>  // b.tcont
   53b40:	b	53b78 <__gmpn_remove@@Base+0x25c>
   53b44:	add	x0, x19, #0x40
   53b48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   53b4c:	mov	x23, x0
   53b50:	b	53b14 <__gmpn_remove@@Base+0x1f8>
   53b54:	str	x25, [x19, #56]
   53b58:	ldr	x25, [x19, #40]
   53b5c:	mov	x8, #0xffffffffffffffff    	// #-1
   53b60:	lsl	x8, x8, x28
   53b64:	mvn	x21, x8
   53b68:	mov	x24, x28
   53b6c:	mov	x8, x25
   53b70:	cbnz	x28, 53b90 <__gmpn_remove@@Base+0x274>
   53b74:	b	53ccc <__gmpn_remove@@Base+0x3b0>
   53b78:	ldr	x9, [x19, #40]
   53b7c:	mov	x8, #0xfffffffffffffffe    	// #-2
   53b80:	lsl	x8, x8, x28
   53b84:	mvn	x21, x8
   53b88:	mov	x8, x25
   53b8c:	str	x9, [x19, #56]
   53b90:	ldr	x9, [x19, #32]
   53b94:	ldr	x10, [x19]
   53b98:	add	x12, x19, #0x48
   53b9c:	add	x9, x10, x9, lsl #4
   53ba0:	add	x28, x9, #0x8
   53ba4:	b	53bc4 <__gmpn_remove@@Base+0x2a8>
   53ba8:	ldr	x8, [x25, x21, lsl #3]
   53bac:	cmp	x8, #0x0
   53bb0:	cinc	x27, x21, ne  // ne = any
   53bb4:	ldr	x21, [x19, #40]
   53bb8:	cmp	x22, #0x1
   53bbc:	mov	x8, x25
   53bc0:	b.le	53ccc <__gmpn_remove@@Base+0x3b0>
   53bc4:	ldr	x25, [x19, #56]
   53bc8:	mov	x20, x21
   53bcc:	str	x8, [x19, #56]
   53bd0:	add	x8, x27, #0x1
   53bd4:	str	x8, [x19, #32]
   53bd8:	b	53be4 <__gmpn_remove@@Base+0x2c8>
   53bdc:	cmp	x22, #0x1
   53be0:	b.le	53cc4 <__gmpn_remove@@Base+0x3a8>
   53be4:	mov	x22, x24
   53be8:	sub	x24, x24, #0x1
   53bec:	ldr	x23, [x12, x24, lsl #3]
   53bf0:	subs	x21, x27, x23
   53bf4:	b.lt	53bdc <__gmpn_remove@@Base+0x2c0>  // b.tstop
   53bf8:	mov	w8, #0x1                   	// #1
   53bfc:	lsl	x8, x8, x24
   53c00:	add	x9, x8, x20
   53c04:	ldr	x8, [x19, #48]
   53c08:	str	x9, [x19, #40]
   53c0c:	cmp	x9, x8
   53c10:	b.hi	53bdc <__gmpn_remove@@Base+0x2c0>  // b.pmore
   53c14:	add	x8, x19, #0x1d8
   53c18:	ldr	x26, [x8, x24, lsl #3]
   53c1c:	ldr	x2, [x19, #56]
   53c20:	ldp	x1, x3, [x19, #24]
   53c24:	mov	x0, x25
   53c28:	mov	x4, x26
   53c2c:	mov	x5, x23
   53c30:	str	xzr, [x2, x27, lsl #3]
   53c34:	bl	53d28 <__gmpn_remove@@Base+0x40c>
   53c38:	add	x12, x19, #0x48
   53c3c:	mov	x8, x23
   53c40:	ldr	x9, [x28, x8, lsl #3]
   53c44:	cbnz	x9, 53c54 <__gmpn_remove@@Base+0x338>
   53c48:	sub	x8, x8, #0x1
   53c4c:	cbnz	x8, 53c40 <__gmpn_remove@@Base+0x324>
   53c50:	b	53c7c <__gmpn_remove@@Base+0x360>
   53c54:	sub	x8, x26, #0x8
   53c58:	subs	x9, x23, #0x1
   53c5c:	b.lt	53c7c <__gmpn_remove@@Base+0x360>  // b.tstop
   53c60:	lsl	x10, x23, #3
   53c64:	ldr	x11, [x28, x10]
   53c68:	ldr	x10, [x8, x10]
   53c6c:	mov	x23, x9
   53c70:	cmp	x11, x10
   53c74:	b.eq	53c58 <__gmpn_remove@@Base+0x33c>  // b.none
   53c78:	b	53bdc <__gmpn_remove@@Base+0x2c0>
   53c7c:	ldr	x10, [x25]
   53c80:	add	x8, x21, #0x1
   53c84:	mov	x9, x25
   53c88:	cbnz	x10, 53ca0 <__gmpn_remove@@Base+0x384>
   53c8c:	subs	x8, x8, #0x1
   53c90:	str	xzr, [x9]
   53c94:	b.eq	53ba8 <__gmpn_remove@@Base+0x28c>  // b.none
   53c98:	ldr	x10, [x9, #8]!
   53c9c:	cbz	x10, 53c8c <__gmpn_remove@@Base+0x370>
   53ca0:	neg	x10, x10
   53ca4:	subs	x2, x8, #0x1
   53ca8:	str	x10, [x9]
   53cac:	b.eq	53ba8 <__gmpn_remove@@Base+0x28c>  // b.none
   53cb0:	add	x0, x9, #0x8
   53cb4:	mov	x1, x0
   53cb8:	bl	c290 <__gmpn_com@plt>
   53cbc:	add	x12, x19, #0x48
   53cc0:	b	53ba8 <__gmpn_remove@@Base+0x28c>
   53cc4:	ldr	x25, [x19, #56]
   53cc8:	mov	x21, x20
   53ccc:	ldr	x0, [x19, #8]
   53cd0:	mov	x1, x25
   53cd4:	mov	x2, x27
   53cd8:	bl	ca50 <__gmpn_copyi@plt>
   53cdc:	ldr	x8, [x19, #16]
   53ce0:	str	x27, [x8]
   53ce4:	ldr	x0, [x19, #64]
   53ce8:	cbnz	x0, 53d20 <__gmpn_remove@@Base+0x404>
   53cec:	mov	x0, x21
   53cf0:	mov	sp, x29
   53cf4:	ldp	x20, x19, [sp, #80]
   53cf8:	ldp	x22, x21, [sp, #64]
   53cfc:	ldp	x24, x23, [sp, #48]
   53d00:	ldp	x26, x25, [sp, #32]
   53d04:	ldp	x28, x27, [sp, #16]
   53d08:	ldp	x29, x30, [sp], #96
   53d0c:	ret
   53d10:	add	x0, x19, #0x40
   53d14:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   53d18:	mov	x25, x0
   53d1c:	b	5399c <__gmpn_remove@@Base+0x80>
   53d20:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   53d24:	b	53cec <__gmpn_remove@@Base+0x3d0>
   53d28:	stp	x29, x30, [sp, #-64]!
   53d2c:	stp	x24, x23, [sp, #16]
   53d30:	stp	x22, x21, [sp, #32]
   53d34:	stp	x20, x19, [sp, #48]
   53d38:	mov	x29, sp
   53d3c:	sub	sp, sp, #0x10
   53d40:	mov	x23, x1
   53d44:	mov	x24, x0
   53d48:	mov	x0, x3
   53d4c:	mov	x1, x5
   53d50:	mov	x19, x5
   53d54:	mov	x20, x4
   53d58:	mov	x21, x3
   53d5c:	mov	x22, x2
   53d60:	stur	xzr, [x29, #-8]
   53d64:	bl	c8c0 <__gmpn_bdiv_qr_itch@plt>
   53d68:	lsl	x1, x0, #3
   53d6c:	mov	w8, #0x7f00                	// #32512
   53d70:	cmp	x1, x8
   53d74:	b.hi	53dc8 <__gmpn_remove@@Base+0x4ac>  // b.pmore
   53d78:	add	x9, x1, #0xf
   53d7c:	mov	x8, sp
   53d80:	and	x9, x9, #0xfffffffffffffff0
   53d84:	sub	x6, x8, x9
   53d88:	mov	sp, x6
   53d8c:	mov	x0, x24
   53d90:	mov	x1, x23
   53d94:	mov	x2, x22
   53d98:	mov	x3, x21
   53d9c:	mov	x4, x20
   53da0:	mov	x5, x19
   53da4:	bl	cf50 <__gmpn_bdiv_qr@plt>
   53da8:	ldur	x0, [x29, #-8]
   53dac:	cbnz	x0, 53dd8 <__gmpn_remove@@Base+0x4bc>
   53db0:	mov	sp, x29
   53db4:	ldp	x20, x19, [sp, #48]
   53db8:	ldp	x22, x21, [sp, #32]
   53dbc:	ldp	x24, x23, [sp, #16]
   53dc0:	ldp	x29, x30, [sp], #64
   53dc4:	ret
   53dc8:	sub	x0, x29, #0x8
   53dcc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   53dd0:	mov	x6, x0
   53dd4:	b	53d8c <__gmpn_remove@@Base+0x470>
   53dd8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   53ddc:	b	53db0 <__gmpn_remove@@Base+0x494>

0000000000053de0 <__gmpn_and_n@@Base>:
   53de0:	lsr	x18, x3, #2
   53de4:	tbz	w3, #0, 53e2c <__gmpn_and_n@@Base+0x4c>
   53de8:	ldr	x7, [x1]
   53dec:	ldr	x11, [x2]
   53df0:	and	x15, x7, x11
   53df4:	str	x15, [x0], #8
   53df8:	tbnz	w3, #1, 53e14 <__gmpn_and_n@@Base+0x34>
   53dfc:	cbz	x18, 53e8c <__gmpn_and_n@@Base+0xac>
   53e00:	ldp	x4, x5, [x1, #8]
   53e04:	ldp	x8, x9, [x2, #8]
   53e08:	sub	x1, x1, #0x8
   53e0c:	sub	x2, x2, #0x8
   53e10:	b	53e64 <__gmpn_and_n@@Base+0x84>
   53e14:	ldp	x6, x7, [x1, #8]
   53e18:	ldp	x10, x11, [x2, #8]
   53e1c:	add	x1, x1, #0x8
   53e20:	add	x2, x2, #0x8
   53e24:	cbz	x18, 53e80 <__gmpn_and_n@@Base+0xa0>
   53e28:	b	53e50 <__gmpn_and_n@@Base+0x70>
   53e2c:	tbnz	w3, #1, 53e3c <__gmpn_and_n@@Base+0x5c>
   53e30:	ldp	x4, x5, [x1], #-16
   53e34:	ldp	x8, x9, [x2], #-16
   53e38:	b	53e64 <__gmpn_and_n@@Base+0x84>
   53e3c:	ldp	x6, x7, [x1]
   53e40:	ldp	x10, x11, [x2]
   53e44:	cbz	x18, 53e80 <__gmpn_and_n@@Base+0xa0>
   53e48:	nop
   53e4c:	nop
   53e50:	ldp	x4, x5, [x1, #16]
   53e54:	ldp	x8, x9, [x2, #16]
   53e58:	and	x12, x6, x10
   53e5c:	and	x13, x7, x11
   53e60:	stp	x12, x13, [x0], #16
   53e64:	ldp	x6, x7, [x1, #32]!
   53e68:	ldp	x10, x11, [x2, #32]!
   53e6c:	and	x12, x4, x8
   53e70:	and	x13, x5, x9
   53e74:	stp	x12, x13, [x0], #16
   53e78:	sub	x18, x18, #0x1
   53e7c:	cbnz	x18, 53e50 <__gmpn_and_n@@Base+0x70>
   53e80:	and	x12, x6, x10
   53e84:	and	x13, x7, x11
   53e88:	stp	x12, x13, [x0]
   53e8c:	ret

0000000000053e90 <__gmpn_andn_n@@Base>:
   53e90:	lsr	x18, x3, #2
   53e94:	tbz	w3, #0, 53edc <__gmpn_andn_n@@Base+0x4c>
   53e98:	ldr	x7, [x1]
   53e9c:	ldr	x11, [x2]
   53ea0:	bic	x15, x7, x11
   53ea4:	str	x15, [x0], #8
   53ea8:	tbnz	w3, #1, 53ec4 <__gmpn_andn_n@@Base+0x34>
   53eac:	cbz	x18, 53f3c <__gmpn_andn_n@@Base+0xac>
   53eb0:	ldp	x4, x5, [x1, #8]
   53eb4:	ldp	x8, x9, [x2, #8]
   53eb8:	sub	x1, x1, #0x8
   53ebc:	sub	x2, x2, #0x8
   53ec0:	b	53f14 <__gmpn_andn_n@@Base+0x84>
   53ec4:	ldp	x6, x7, [x1, #8]
   53ec8:	ldp	x10, x11, [x2, #8]
   53ecc:	add	x1, x1, #0x8
   53ed0:	add	x2, x2, #0x8
   53ed4:	cbz	x18, 53f30 <__gmpn_andn_n@@Base+0xa0>
   53ed8:	b	53f00 <__gmpn_andn_n@@Base+0x70>
   53edc:	tbnz	w3, #1, 53eec <__gmpn_andn_n@@Base+0x5c>
   53ee0:	ldp	x4, x5, [x1], #-16
   53ee4:	ldp	x8, x9, [x2], #-16
   53ee8:	b	53f14 <__gmpn_andn_n@@Base+0x84>
   53eec:	ldp	x6, x7, [x1]
   53ef0:	ldp	x10, x11, [x2]
   53ef4:	cbz	x18, 53f30 <__gmpn_andn_n@@Base+0xa0>
   53ef8:	nop
   53efc:	nop
   53f00:	ldp	x4, x5, [x1, #16]
   53f04:	ldp	x8, x9, [x2, #16]
   53f08:	bic	x12, x6, x10
   53f0c:	bic	x13, x7, x11
   53f10:	stp	x12, x13, [x0], #16
   53f14:	ldp	x6, x7, [x1, #32]!
   53f18:	ldp	x10, x11, [x2, #32]!
   53f1c:	bic	x12, x4, x8
   53f20:	bic	x13, x5, x9
   53f24:	stp	x12, x13, [x0], #16
   53f28:	sub	x18, x18, #0x1
   53f2c:	cbnz	x18, 53f00 <__gmpn_andn_n@@Base+0x70>
   53f30:	bic	x12, x6, x10
   53f34:	bic	x13, x7, x11
   53f38:	stp	x12, x13, [x0]
   53f3c:	ret

0000000000053f40 <__gmpn_nand_n@@Base>:
   53f40:	lsr	x18, x3, #2
   53f44:	tbz	w3, #0, 53f90 <__gmpn_nand_n@@Base+0x50>
   53f48:	ldr	x7, [x1]
   53f4c:	ldr	x11, [x2]
   53f50:	and	x15, x7, x11
   53f54:	mvn	x15, x15
   53f58:	str	x15, [x0], #8
   53f5c:	tbnz	w3, #1, 53f78 <__gmpn_nand_n@@Base+0x38>
   53f60:	cbz	x18, 54004 <__gmpn_nand_n@@Base+0xc4>
   53f64:	ldp	x4, x5, [x1, #8]
   53f68:	ldp	x8, x9, [x2, #8]
   53f6c:	sub	x1, x1, #0x8
   53f70:	sub	x2, x2, #0x8
   53f74:	b	53fcc <__gmpn_nand_n@@Base+0x8c>
   53f78:	ldp	x6, x7, [x1, #8]
   53f7c:	ldp	x10, x11, [x2, #8]
   53f80:	add	x1, x1, #0x8
   53f84:	add	x2, x2, #0x8
   53f88:	cbz	x18, 53ff0 <__gmpn_nand_n@@Base+0xb0>
   53f8c:	b	53fb0 <__gmpn_nand_n@@Base+0x70>
   53f90:	tbnz	w3, #1, 53fa0 <__gmpn_nand_n@@Base+0x60>
   53f94:	ldp	x4, x5, [x1], #-16
   53f98:	ldp	x8, x9, [x2], #-16
   53f9c:	b	53fcc <__gmpn_nand_n@@Base+0x8c>
   53fa0:	ldp	x6, x7, [x1]
   53fa4:	ldp	x10, x11, [x2]
   53fa8:	cbz	x18, 53ff0 <__gmpn_nand_n@@Base+0xb0>
   53fac:	nop
   53fb0:	ldp	x4, x5, [x1, #16]
   53fb4:	ldp	x8, x9, [x2, #16]
   53fb8:	and	x12, x6, x10
   53fbc:	and	x13, x7, x11
   53fc0:	mvn	x12, x12
   53fc4:	mvn	x13, x13
   53fc8:	stp	x12, x13, [x0], #16
   53fcc:	ldp	x6, x7, [x1, #32]!
   53fd0:	ldp	x10, x11, [x2, #32]!
   53fd4:	and	x12, x4, x8
   53fd8:	and	x13, x5, x9
   53fdc:	mvn	x12, x12
   53fe0:	mvn	x13, x13
   53fe4:	stp	x12, x13, [x0], #16
   53fe8:	sub	x18, x18, #0x1
   53fec:	cbnz	x18, 53fb0 <__gmpn_nand_n@@Base+0x70>
   53ff0:	and	x12, x6, x10
   53ff4:	and	x13, x7, x11
   53ff8:	mvn	x12, x12
   53ffc:	mvn	x13, x13
   54000:	stp	x12, x13, [x0]
   54004:	ret
   54008:	nop
   5400c:	nop

0000000000054010 <__gmpn_ior_n@@Base>:
   54010:	lsr	x18, x3, #2
   54014:	tbz	w3, #0, 5405c <__gmpn_ior_n@@Base+0x4c>
   54018:	ldr	x7, [x1]
   5401c:	ldr	x11, [x2]
   54020:	orr	x15, x7, x11
   54024:	str	x15, [x0], #8
   54028:	tbnz	w3, #1, 54044 <__gmpn_ior_n@@Base+0x34>
   5402c:	cbz	x18, 540bc <__gmpn_ior_n@@Base+0xac>
   54030:	ldp	x4, x5, [x1, #8]
   54034:	ldp	x8, x9, [x2, #8]
   54038:	sub	x1, x1, #0x8
   5403c:	sub	x2, x2, #0x8
   54040:	b	54094 <__gmpn_ior_n@@Base+0x84>
   54044:	ldp	x6, x7, [x1, #8]
   54048:	ldp	x10, x11, [x2, #8]
   5404c:	add	x1, x1, #0x8
   54050:	add	x2, x2, #0x8
   54054:	cbz	x18, 540b0 <__gmpn_ior_n@@Base+0xa0>
   54058:	b	54080 <__gmpn_ior_n@@Base+0x70>
   5405c:	tbnz	w3, #1, 5406c <__gmpn_ior_n@@Base+0x5c>
   54060:	ldp	x4, x5, [x1], #-16
   54064:	ldp	x8, x9, [x2], #-16
   54068:	b	54094 <__gmpn_ior_n@@Base+0x84>
   5406c:	ldp	x6, x7, [x1]
   54070:	ldp	x10, x11, [x2]
   54074:	cbz	x18, 540b0 <__gmpn_ior_n@@Base+0xa0>
   54078:	nop
   5407c:	nop
   54080:	ldp	x4, x5, [x1, #16]
   54084:	ldp	x8, x9, [x2, #16]
   54088:	orr	x12, x6, x10
   5408c:	orr	x13, x7, x11
   54090:	stp	x12, x13, [x0], #16
   54094:	ldp	x6, x7, [x1, #32]!
   54098:	ldp	x10, x11, [x2, #32]!
   5409c:	orr	x12, x4, x8
   540a0:	orr	x13, x5, x9
   540a4:	stp	x12, x13, [x0], #16
   540a8:	sub	x18, x18, #0x1
   540ac:	cbnz	x18, 54080 <__gmpn_ior_n@@Base+0x70>
   540b0:	orr	x12, x6, x10
   540b4:	orr	x13, x7, x11
   540b8:	stp	x12, x13, [x0]
   540bc:	ret

00000000000540c0 <__gmpn_iorn_n@@Base>:
   540c0:	lsr	x18, x3, #2
   540c4:	tbz	w3, #0, 5410c <__gmpn_iorn_n@@Base+0x4c>
   540c8:	ldr	x7, [x1]
   540cc:	ldr	x11, [x2]
   540d0:	orn	x15, x7, x11
   540d4:	str	x15, [x0], #8
   540d8:	tbnz	w3, #1, 540f4 <__gmpn_iorn_n@@Base+0x34>
   540dc:	cbz	x18, 5416c <__gmpn_iorn_n@@Base+0xac>
   540e0:	ldp	x4, x5, [x1, #8]
   540e4:	ldp	x8, x9, [x2, #8]
   540e8:	sub	x1, x1, #0x8
   540ec:	sub	x2, x2, #0x8
   540f0:	b	54144 <__gmpn_iorn_n@@Base+0x84>
   540f4:	ldp	x6, x7, [x1, #8]
   540f8:	ldp	x10, x11, [x2, #8]
   540fc:	add	x1, x1, #0x8
   54100:	add	x2, x2, #0x8
   54104:	cbz	x18, 54160 <__gmpn_iorn_n@@Base+0xa0>
   54108:	b	54130 <__gmpn_iorn_n@@Base+0x70>
   5410c:	tbnz	w3, #1, 5411c <__gmpn_iorn_n@@Base+0x5c>
   54110:	ldp	x4, x5, [x1], #-16
   54114:	ldp	x8, x9, [x2], #-16
   54118:	b	54144 <__gmpn_iorn_n@@Base+0x84>
   5411c:	ldp	x6, x7, [x1]
   54120:	ldp	x10, x11, [x2]
   54124:	cbz	x18, 54160 <__gmpn_iorn_n@@Base+0xa0>
   54128:	nop
   5412c:	nop
   54130:	ldp	x4, x5, [x1, #16]
   54134:	ldp	x8, x9, [x2, #16]
   54138:	orn	x12, x6, x10
   5413c:	orn	x13, x7, x11
   54140:	stp	x12, x13, [x0], #16
   54144:	ldp	x6, x7, [x1, #32]!
   54148:	ldp	x10, x11, [x2, #32]!
   5414c:	orn	x12, x4, x8
   54150:	orn	x13, x5, x9
   54154:	stp	x12, x13, [x0], #16
   54158:	sub	x18, x18, #0x1
   5415c:	cbnz	x18, 54130 <__gmpn_iorn_n@@Base+0x70>
   54160:	orn	x12, x6, x10
   54164:	orn	x13, x7, x11
   54168:	stp	x12, x13, [x0]
   5416c:	ret

0000000000054170 <__gmpn_nior_n@@Base>:
   54170:	lsr	x18, x3, #2
   54174:	tbz	w3, #0, 541c0 <__gmpn_nior_n@@Base+0x50>
   54178:	ldr	x7, [x1]
   5417c:	ldr	x11, [x2]
   54180:	orr	x15, x7, x11
   54184:	mvn	x15, x15
   54188:	str	x15, [x0], #8
   5418c:	tbnz	w3, #1, 541a8 <__gmpn_nior_n@@Base+0x38>
   54190:	cbz	x18, 54234 <__gmpn_nior_n@@Base+0xc4>
   54194:	ldp	x4, x5, [x1, #8]
   54198:	ldp	x8, x9, [x2, #8]
   5419c:	sub	x1, x1, #0x8
   541a0:	sub	x2, x2, #0x8
   541a4:	b	541fc <__gmpn_nior_n@@Base+0x8c>
   541a8:	ldp	x6, x7, [x1, #8]
   541ac:	ldp	x10, x11, [x2, #8]
   541b0:	add	x1, x1, #0x8
   541b4:	add	x2, x2, #0x8
   541b8:	cbz	x18, 54220 <__gmpn_nior_n@@Base+0xb0>
   541bc:	b	541e0 <__gmpn_nior_n@@Base+0x70>
   541c0:	tbnz	w3, #1, 541d0 <__gmpn_nior_n@@Base+0x60>
   541c4:	ldp	x4, x5, [x1], #-16
   541c8:	ldp	x8, x9, [x2], #-16
   541cc:	b	541fc <__gmpn_nior_n@@Base+0x8c>
   541d0:	ldp	x6, x7, [x1]
   541d4:	ldp	x10, x11, [x2]
   541d8:	cbz	x18, 54220 <__gmpn_nior_n@@Base+0xb0>
   541dc:	nop
   541e0:	ldp	x4, x5, [x1, #16]
   541e4:	ldp	x8, x9, [x2, #16]
   541e8:	orr	x12, x6, x10
   541ec:	orr	x13, x7, x11
   541f0:	mvn	x12, x12
   541f4:	mvn	x13, x13
   541f8:	stp	x12, x13, [x0], #16
   541fc:	ldp	x6, x7, [x1, #32]!
   54200:	ldp	x10, x11, [x2, #32]!
   54204:	orr	x12, x4, x8
   54208:	orr	x13, x5, x9
   5420c:	mvn	x12, x12
   54210:	mvn	x13, x13
   54214:	stp	x12, x13, [x0], #16
   54218:	sub	x18, x18, #0x1
   5421c:	cbnz	x18, 541e0 <__gmpn_nior_n@@Base+0x70>
   54220:	orr	x12, x6, x10
   54224:	orr	x13, x7, x11
   54228:	mvn	x12, x12
   5422c:	mvn	x13, x13
   54230:	stp	x12, x13, [x0]
   54234:	ret
   54238:	nop
   5423c:	nop

0000000000054240 <__gmpn_xor_n@@Base>:
   54240:	lsr	x18, x3, #2
   54244:	tbz	w3, #0, 5428c <__gmpn_xor_n@@Base+0x4c>
   54248:	ldr	x7, [x1]
   5424c:	ldr	x11, [x2]
   54250:	eor	x15, x7, x11
   54254:	str	x15, [x0], #8
   54258:	tbnz	w3, #1, 54274 <__gmpn_xor_n@@Base+0x34>
   5425c:	cbz	x18, 542ec <__gmpn_xor_n@@Base+0xac>
   54260:	ldp	x4, x5, [x1, #8]
   54264:	ldp	x8, x9, [x2, #8]
   54268:	sub	x1, x1, #0x8
   5426c:	sub	x2, x2, #0x8
   54270:	b	542c4 <__gmpn_xor_n@@Base+0x84>
   54274:	ldp	x6, x7, [x1, #8]
   54278:	ldp	x10, x11, [x2, #8]
   5427c:	add	x1, x1, #0x8
   54280:	add	x2, x2, #0x8
   54284:	cbz	x18, 542e0 <__gmpn_xor_n@@Base+0xa0>
   54288:	b	542b0 <__gmpn_xor_n@@Base+0x70>
   5428c:	tbnz	w3, #1, 5429c <__gmpn_xor_n@@Base+0x5c>
   54290:	ldp	x4, x5, [x1], #-16
   54294:	ldp	x8, x9, [x2], #-16
   54298:	b	542c4 <__gmpn_xor_n@@Base+0x84>
   5429c:	ldp	x6, x7, [x1]
   542a0:	ldp	x10, x11, [x2]
   542a4:	cbz	x18, 542e0 <__gmpn_xor_n@@Base+0xa0>
   542a8:	nop
   542ac:	nop
   542b0:	ldp	x4, x5, [x1, #16]
   542b4:	ldp	x8, x9, [x2, #16]
   542b8:	eor	x12, x6, x10
   542bc:	eor	x13, x7, x11
   542c0:	stp	x12, x13, [x0], #16
   542c4:	ldp	x6, x7, [x1, #32]!
   542c8:	ldp	x10, x11, [x2, #32]!
   542cc:	eor	x12, x4, x8
   542d0:	eor	x13, x5, x9
   542d4:	stp	x12, x13, [x0], #16
   542d8:	sub	x18, x18, #0x1
   542dc:	cbnz	x18, 542b0 <__gmpn_xor_n@@Base+0x70>
   542e0:	eor	x12, x6, x10
   542e4:	eor	x13, x7, x11
   542e8:	stp	x12, x13, [x0]
   542ec:	ret

00000000000542f0 <__gmpn_xnor_n@@Base>:
   542f0:	lsr	x18, x3, #2
   542f4:	tbz	w3, #0, 5433c <__gmpn_xnor_n@@Base+0x4c>
   542f8:	ldr	x7, [x1]
   542fc:	ldr	x11, [x2]
   54300:	eon	x15, x7, x11
   54304:	str	x15, [x0], #8
   54308:	tbnz	w3, #1, 54324 <__gmpn_xnor_n@@Base+0x34>
   5430c:	cbz	x18, 5439c <__gmpn_xnor_n@@Base+0xac>
   54310:	ldp	x4, x5, [x1, #8]
   54314:	ldp	x8, x9, [x2, #8]
   54318:	sub	x1, x1, #0x8
   5431c:	sub	x2, x2, #0x8
   54320:	b	54374 <__gmpn_xnor_n@@Base+0x84>
   54324:	ldp	x6, x7, [x1, #8]
   54328:	ldp	x10, x11, [x2, #8]
   5432c:	add	x1, x1, #0x8
   54330:	add	x2, x2, #0x8
   54334:	cbz	x18, 54390 <__gmpn_xnor_n@@Base+0xa0>
   54338:	b	54360 <__gmpn_xnor_n@@Base+0x70>
   5433c:	tbnz	w3, #1, 5434c <__gmpn_xnor_n@@Base+0x5c>
   54340:	ldp	x4, x5, [x1], #-16
   54344:	ldp	x8, x9, [x2], #-16
   54348:	b	54374 <__gmpn_xnor_n@@Base+0x84>
   5434c:	ldp	x6, x7, [x1]
   54350:	ldp	x10, x11, [x2]
   54354:	cbz	x18, 54390 <__gmpn_xnor_n@@Base+0xa0>
   54358:	nop
   5435c:	nop
   54360:	ldp	x4, x5, [x1, #16]
   54364:	ldp	x8, x9, [x2, #16]
   54368:	eon	x12, x6, x10
   5436c:	eon	x13, x7, x11
   54370:	stp	x12, x13, [x0], #16
   54374:	ldp	x6, x7, [x1, #32]!
   54378:	ldp	x10, x11, [x2, #32]!
   5437c:	eon	x12, x4, x8
   54380:	eon	x13, x5, x9
   54384:	stp	x12, x13, [x0], #16
   54388:	sub	x18, x18, #0x1
   5438c:	cbnz	x18, 54360 <__gmpn_xnor_n@@Base+0x70>
   54390:	eon	x12, x6, x10
   54394:	eon	x13, x7, x11
   54398:	stp	x12, x13, [x0]
   5439c:	ret

00000000000543a0 <__gmpn_copyi@@Base>:
   543a0:	cmp	x2, #0x3
   543a4:	b.le	543ec <__gmpn_copyi@@Base+0x4c>
   543a8:	tbz	w0, #3, 543b8 <__gmpn_copyi@@Base+0x18>
   543ac:	ld1	{v22.1d}, [x1], #8
   543b0:	sub	x2, x2, #0x1
   543b4:	st1	{v22.1d}, [x0], #8
   543b8:	ld1	{v26.2d}, [x1], #16
   543bc:	sub	x2, x2, #0x6
   543c0:	tbnz	x2, #63, 543e8 <__gmpn_copyi@@Base+0x48>
   543c4:	nop
   543c8:	nop
   543cc:	nop
   543d0:	ld1	{v22.2d}, [x1], #16
   543d4:	st1	{v26.2d}, [x0], #16
   543d8:	ld1	{v26.2d}, [x1], #16
   543dc:	st1	{v22.2d}, [x0], #16
   543e0:	sub	x2, x2, #0x4
   543e4:	tbz	x2, #63, 543d0 <__gmpn_copyi@@Base+0x30>
   543e8:	st1	{v26.2d}, [x0], #16
   543ec:	tbz	w2, #1, 543f8 <__gmpn_copyi@@Base+0x58>
   543f0:	ld1	{v22.2d}, [x1], #16
   543f4:	st1	{v22.2d}, [x0], #16
   543f8:	tbz	w2, #0, 54404 <__gmpn_copyi@@Base+0x64>
   543fc:	ld1	{v22.1d}, [x1]
   54400:	st1	{v22.1d}, [x0]
   54404:	ret
   54408:	nop
   5440c:	nop

0000000000054410 <__gmpn_copyd@@Base>:
   54410:	add	x0, x0, x2, lsl #3
   54414:	add	x1, x1, x2, lsl #3
   54418:	cmp	x2, #0x3
   5441c:	b.le	54480 <__gmpn_copyd@@Base+0x70>
   54420:	tbz	w0, #3, 54438 <__gmpn_copyd@@Base+0x28>
   54424:	sub	x1, x1, #0x8
   54428:	ld1	{v22.1d}, [x1]
   5442c:	sub	x2, x2, #0x1
   54430:	sub	x0, x0, #0x8
   54434:	st1	{v22.1d}, [x0]
   54438:	sub	x1, x1, #0x10
   5443c:	ld1	{v26.2d}, [x1]
   54440:	sub	x2, x2, #0x6
   54444:	sub	x0, x0, #0x10
   54448:	tbnz	x2, #63, 5447c <__gmpn_copyd@@Base+0x6c>
   5444c:	sub	x1, x1, #0x10
   54450:	mov	x12, #0xfffffffffffffff0    	// #-16
   54454:	nop
   54458:	nop
   5445c:	nop
   54460:	ld1	{v22.2d}, [x1], x12
   54464:	st1	{v26.2d}, [x0], x12
   54468:	ld1	{v26.2d}, [x1], x12
   5446c:	st1	{v22.2d}, [x0], x12
   54470:	sub	x2, x2, #0x4
   54474:	tbz	x2, #63, 54460 <__gmpn_copyd@@Base+0x50>
   54478:	add	x1, x1, #0x10
   5447c:	st1	{v26.2d}, [x0]
   54480:	tbz	w2, #1, 54494 <__gmpn_copyd@@Base+0x84>
   54484:	sub	x1, x1, #0x10
   54488:	ld1	{v22.2d}, [x1]
   5448c:	sub	x0, x0, #0x10
   54490:	st1	{v22.2d}, [x0]
   54494:	tbz	w2, #0, 544a8 <__gmpn_copyd@@Base+0x98>
   54498:	sub	x1, x1, #0x8
   5449c:	ld1	{v22.1d}, [x1]
   544a0:	sub	x0, x0, #0x8
   544a4:	st1	{v22.1d}, [x0]
   544a8:	ret

00000000000544ac <__gmpn_zero@@Base>:
   544ac:	cbz	x1, 544c8 <__gmpn_zero@@Base+0x1c>
   544b0:	stp	x29, x30, [sp, #-16]!
   544b4:	lsl	x2, x1, #3
   544b8:	mov	w1, wzr
   544bc:	mov	x29, sp
   544c0:	bl	c5f0 <memset@plt>
   544c4:	ldp	x29, x30, [sp], #16
   544c8:	ret
   544cc:	nop

00000000000544d0 <__gmpn_sec_tabselect@@Base>:
   544d0:	dup	v7.2d, x4
   544d4:	mov	x10, #0x1                   	// #1
   544d8:	dup	v6.2d, x10
   544dc:	subs	x6, x2, #0x4
   544e0:	b.mi	54530 <__gmpn_sec_tabselect@@Base+0x60>  // b.first
   544e4:	mov	x5, x3
   544e8:	mov	x12, x1
   544ec:	movi	v5.16b, #0x0
   544f0:	movi	v2.16b, #0x0
   544f4:	movi	v3.16b, #0x0
   544f8:	nop
   544fc:	nop
   54500:	cmeq	v4.2d, v5.2d, v7.2d
   54504:	ld1	{v0.2d, v1.2d}, [x1]
   54508:	add	v5.2d, v5.2d, v6.2d
   5450c:	bit	v2.16b, v0.16b, v4.16b
   54510:	bit	v3.16b, v1.16b, v4.16b
   54514:	add	x1, x1, x2, lsl #3
   54518:	sub	x5, x5, #0x1
   5451c:	cbnz	x5, 54500 <__gmpn_sec_tabselect@@Base+0x30>
   54520:	st1	{v2.2d, v3.2d}, [x0], #32
   54524:	add	x1, x12, #0x20
   54528:	subs	x6, x6, #0x4
   5452c:	b.pl	544e4 <__gmpn_sec_tabselect@@Base+0x14>  // b.nfrst
   54530:	tbz	w2, #1, 54574 <__gmpn_sec_tabselect@@Base+0xa4>
   54534:	mov	x5, x3
   54538:	mov	x12, x1
   5453c:	movi	v5.16b, #0x0
   54540:	movi	v2.16b, #0x0
   54544:	nop
   54548:	nop
   5454c:	nop
   54550:	cmeq	v4.2d, v5.2d, v7.2d
   54554:	ld1	{v0.2d}, [x1]
   54558:	add	v5.2d, v5.2d, v6.2d
   5455c:	bit	v2.16b, v0.16b, v4.16b
   54560:	add	x1, x1, x2, lsl #3
   54564:	sub	x5, x5, #0x1
   54568:	cbnz	x5, 54550 <__gmpn_sec_tabselect@@Base+0x80>
   5456c:	st1	{v2.2d}, [x0], #16
   54570:	add	x1, x12, #0x10
   54574:	tbz	w2, #0, 545b4 <__gmpn_sec_tabselect@@Base+0xe4>
   54578:	mov	x5, x3
   5457c:	mov	x12, x1
   54580:	movi	v5.16b, #0x0
   54584:	movi	v2.16b, #0x0
   54588:	nop
   5458c:	nop
   54590:	cmeq	v4.2d, v5.2d, v7.2d
   54594:	ld1	{v0.1d}, [x1]
   54598:	add	v5.2d, v5.2d, v6.2d
   5459c:	bit	v2.8b, v0.8b, v4.8b
   545a0:	add	x1, x1, x2, lsl #3
   545a4:	sub	x5, x5, #0x1
   545a8:	cbnz	x5, 54590 <__gmpn_sec_tabselect@@Base+0xc0>
   545ac:	st1	{v2.1d}, [x0], #8
   545b0:	add	x1, x12, #0x8
   545b4:	ret

00000000000545b8 <__gmpn_invert_limb@@Base>:
   545b8:	lsr	x2, x0, #54
   545bc:	adrp	x1, 65000 <__gmp_oddfac_table@@Base+0xf0>
   545c0:	and	x2, x2, #0x1fe
   545c4:	add	x1, x1, #0x220
   545c8:	ldrh	w3, [x1, x2]
   545cc:	lsr	x4, x0, #24
   545d0:	add	x4, x4, #0x1
   545d4:	ubfiz	x2, x3, #11, #16
   545d8:	umull	x3, w3, w3
   545dc:	mul	x3, x3, x4
   545e0:	sub	x2, x2, #0x1
   545e4:	sub	x2, x2, x3, lsr #40
   545e8:	lsl	x3, x2, #60
   545ec:	mul	x1, x2, x2
   545f0:	msub	x1, x1, x4, x3
   545f4:	lsl	x2, x2, #13
   545f8:	add	x1, x2, x1, lsr #47
   545fc:	and	x2, x0, #0x1
   54600:	neg	x3, x2
   54604:	and	x3, x3, x1, lsr #1
   54608:	add	x2, x2, x0, lsr #1
   5460c:	msub	x2, x1, x2, x3
   54610:	umulh	x2, x2, x1
   54614:	lsl	x1, x1, #31
   54618:	add	x1, x1, x2, lsr #1
   5461c:	mul	x3, x1, x0
   54620:	umulh	x2, x1, x0
   54624:	adds	x4, x3, x0
   54628:	adc	x0, x2, x0
   5462c:	sub	x0, x1, x0
   54630:	ret
   54634:	nop
   54638:	nop
   5463c:	nop

0000000000054640 <__gmpn_sqr_diag_addlsh1@@Base>:
   54640:	ldr	x15, [x2], #8
   54644:	lsr	x18, x3, #1
   54648:	tbz	w3, #0, 54664 <__gmpn_sqr_diag_addlsh1@@Base+0x24>
   5464c:	adds	x7, xzr, xzr
   54650:	mul	x12, x15, x15
   54654:	ldr	x16, [x2], #8
   54658:	ldp	x4, x5, [x1], #16
   5465c:	umulh	x11, x15, x15
   54660:	b	546a4 <__gmpn_sqr_diag_addlsh1@@Base+0x64>
   54664:	adds	x5, xzr, xzr
   54668:	mul	x12, x15, x15
   5466c:	ldr	x17, [x2], #16
   54670:	ldp	x6, x7, [x1], #32
   54674:	umulh	x11, x15, x15
   54678:	sub	x18, x18, #0x1
   5467c:	cbz	x18, 546d0 <__gmpn_sqr_diag_addlsh1@@Base+0x90>
   54680:	extr	x9, x6, x5, #63
   54684:	mul	x10, x17, x17
   54688:	ldur	x16, [x2, #-8]
   5468c:	adcs	x13, x9, x11
   54690:	ldp	x4, x5, [x1, #-16]
   54694:	umulh	x11, x17, x17
   54698:	extr	x8, x7, x6, #63
   5469c:	stp	x12, x13, [x0], #16
   546a0:	adcs	x12, x8, x10
   546a4:	extr	x9, x4, x7, #63
   546a8:	mul	x10, x16, x16
   546ac:	ldr	x17, [x2], #16
   546b0:	adcs	x13, x9, x11
   546b4:	ldp	x6, x7, [x1], #32
   546b8:	umulh	x11, x16, x16
   546bc:	extr	x8, x5, x4, #63
   546c0:	stp	x12, x13, [x0], #16
   546c4:	adcs	x12, x8, x10
   546c8:	sub	x18, x18, #0x1
   546cc:	cbnz	x18, 54680 <__gmpn_sqr_diag_addlsh1@@Base+0x40>
   546d0:	extr	x9, x6, x5, #63
   546d4:	mul	x10, x17, x17
   546d8:	adcs	x13, x9, x11
   546dc:	umulh	x11, x17, x17
   546e0:	extr	x8, x7, x6, #63
   546e4:	stp	x12, x13, [x0]
   546e8:	adcs	x12, x8, x10
   546ec:	extr	x9, xzr, x7, #63
   546f0:	adcs	x13, x9, x11
   546f4:	stp	x12, x13, [x0, #16]
   546f8:	ret
   546fc:	nop

0000000000054700 <__gmpn_addlsh1_n@@Base>:
   54700:	lsr	x18, x3, #2
   54704:	tbz	w3, #0, 5476c <__gmpn_addlsh1_n@@Base+0x6c>
   54708:	ldr	x5, [x1]
   5470c:	tbnz	w3, #1, 5474c <__gmpn_addlsh1_n@@Base+0x4c>
   54710:	ldr	x11, [x2]
   54714:	cbz	x18, 54734 <__gmpn_addlsh1_n@@Base+0x34>
   54718:	ldp	x8, x9, [x2, #8]
   5471c:	lsl	x13, x11, #1
   54720:	adds	x15, x13, x5
   54724:	str	x15, [x0], #8
   54728:	sub	x1, x1, #0x18
   5472c:	sub	x2, x2, #0x8
   54730:	b	547ac <__gmpn_addlsh1_n@@Base+0xac>
   54734:	lsl	x13, x11, #1
   54738:	adds	x15, x13, x5
   5473c:	str	x15, [x0]
   54740:	lsr	x0, x11, #63
   54744:	adc	x0, x0, xzr
   54748:	ret
   5474c:	ldr	x9, [x2]
   54750:	ldp	x10, x11, [x2, #8]!
   54754:	lsl	x13, x9, #1
   54758:	adds	x17, x13, x5
   5475c:	str	x17, [x0], #8
   54760:	sub	x1, x1, #0x8
   54764:	cbz	x18, 547d0 <__gmpn_addlsh1_n@@Base+0xd0>
   54768:	b	54790 <__gmpn_addlsh1_n@@Base+0x90>
   5476c:	tbnz	w3, #1, 54780 <__gmpn_addlsh1_n@@Base+0x80>
   54770:	adds	x11, xzr, xzr
   54774:	ldp	x8, x9, [x2], #-16
   54778:	sub	x1, x1, #0x20
   5477c:	b	547ac <__gmpn_addlsh1_n@@Base+0xac>
   54780:	adds	x9, xzr, xzr
   54784:	ldp	x10, x11, [x2]
   54788:	sub	x1, x1, #0x10
   5478c:	cbz	x18, 547d0 <__gmpn_addlsh1_n@@Base+0xd0>
   54790:	ldp	x4, x5, [x1, #16]
   54794:	extr	x12, x10, x9, #63
   54798:	ldp	x8, x9, [x2, #16]
   5479c:	extr	x13, x11, x10, #63
   547a0:	adcs	x14, x12, x4
   547a4:	adcs	x15, x13, x5
   547a8:	stp	x14, x15, [x0], #16
   547ac:	ldp	x4, x5, [x1, #32]!
   547b0:	extr	x12, x8, x11, #63
   547b4:	ldp	x10, x11, [x2, #32]!
   547b8:	extr	x13, x9, x8, #63
   547bc:	adcs	x16, x12, x4
   547c0:	adcs	x17, x13, x5
   547c4:	stp	x16, x17, [x0], #16
   547c8:	sub	x18, x18, #0x1
   547cc:	cbnz	x18, 54790 <__gmpn_addlsh1_n@@Base+0x90>
   547d0:	ldp	x4, x5, [x1, #16]
   547d4:	extr	x12, x10, x9, #63
   547d8:	extr	x13, x11, x10, #63
   547dc:	adcs	x14, x12, x4
   547e0:	adcs	x15, x13, x5
   547e4:	stp	x14, x15, [x0]
   547e8:	lsr	x0, x11, #63
   547ec:	adc	x0, x0, xzr
   547f0:	ret
   547f4:	nop
   547f8:	nop
   547fc:	nop

0000000000054800 <__gmpn_sublsh1_n@@Base>:
   54800:	lsr	x18, x3, #2
   54804:	tbz	w3, #0, 5486c <__gmpn_sublsh1_n@@Base+0x6c>
   54808:	ldr	x5, [x1]
   5480c:	tbnz	w3, #1, 5484c <__gmpn_sublsh1_n@@Base+0x4c>
   54810:	ldr	x11, [x2]
   54814:	cbz	x18, 54834 <__gmpn_sublsh1_n@@Base+0x34>
   54818:	ldp	x8, x9, [x2, #8]
   5481c:	lsl	x13, x11, #1
   54820:	subs	x15, x5, x13
   54824:	str	x15, [x0], #8
   54828:	sub	x1, x1, #0x18
   5482c:	sub	x2, x2, #0x8
   54830:	b	548ac <__gmpn_sublsh1_n@@Base+0xac>
   54834:	lsl	x13, x11, #1
   54838:	subs	x15, x5, x13
   5483c:	str	x15, [x0]
   54840:	lsr	x0, x11, #63
   54844:	cinc	x0, x0, cc  // cc = lo, ul, last
   54848:	ret
   5484c:	ldr	x9, [x2]
   54850:	ldp	x10, x11, [x2, #8]!
   54854:	lsl	x13, x9, #1
   54858:	subs	x17, x5, x13
   5485c:	str	x17, [x0], #8
   54860:	sub	x1, x1, #0x8
   54864:	cbz	x18, 548d0 <__gmpn_sublsh1_n@@Base+0xd0>
   54868:	b	54890 <__gmpn_sublsh1_n@@Base+0x90>
   5486c:	tbnz	w3, #1, 54880 <__gmpn_sublsh1_n@@Base+0x80>
   54870:	negs	x11, xzr
   54874:	ldp	x8, x9, [x2], #-16
   54878:	sub	x1, x1, #0x20
   5487c:	b	548ac <__gmpn_sublsh1_n@@Base+0xac>
   54880:	negs	x9, xzr
   54884:	ldp	x10, x11, [x2]
   54888:	sub	x1, x1, #0x10
   5488c:	cbz	x18, 548d0 <__gmpn_sublsh1_n@@Base+0xd0>
   54890:	ldp	x4, x5, [x1, #16]
   54894:	extr	x12, x10, x9, #63
   54898:	ldp	x8, x9, [x2, #16]
   5489c:	extr	x13, x11, x10, #63
   548a0:	sbcs	x14, x4, x12
   548a4:	sbcs	x15, x5, x13
   548a8:	stp	x14, x15, [x0], #16
   548ac:	ldp	x4, x5, [x1, #32]!
   548b0:	extr	x12, x8, x11, #63
   548b4:	ldp	x10, x11, [x2, #32]!
   548b8:	extr	x13, x9, x8, #63
   548bc:	sbcs	x16, x4, x12
   548c0:	sbcs	x17, x5, x13
   548c4:	stp	x16, x17, [x0], #16
   548c8:	sub	x18, x18, #0x1
   548cc:	cbnz	x18, 54890 <__gmpn_sublsh1_n@@Base+0x90>
   548d0:	ldp	x4, x5, [x1, #16]
   548d4:	extr	x12, x10, x9, #63
   548d8:	extr	x13, x11, x10, #63
   548dc:	sbcs	x14, x4, x12
   548e0:	sbcs	x15, x5, x13
   548e4:	stp	x14, x15, [x0]
   548e8:	lsr	x0, x11, #63
   548ec:	cinc	x0, x0, cc  // cc = lo, ul, last
   548f0:	ret
   548f4:	nop
   548f8:	nop
   548fc:	nop

0000000000054900 <__gmpn_rsblsh1_n@@Base>:
   54900:	lsr	x18, x3, #2
   54904:	tbz	w3, #0, 5496c <__gmpn_rsblsh1_n@@Base+0x6c>
   54908:	ldr	x5, [x1]
   5490c:	tbnz	w3, #1, 5494c <__gmpn_rsblsh1_n@@Base+0x4c>
   54910:	ldr	x11, [x2]
   54914:	cbz	x18, 54934 <__gmpn_rsblsh1_n@@Base+0x34>
   54918:	ldp	x8, x9, [x2, #8]
   5491c:	lsl	x13, x11, #1
   54920:	subs	x15, x13, x5
   54924:	str	x15, [x0], #8
   54928:	sub	x1, x1, #0x18
   5492c:	sub	x2, x2, #0x8
   54930:	b	549ac <__gmpn_rsblsh1_n@@Base+0xac>
   54934:	lsl	x13, x11, #1
   54938:	subs	x15, x13, x5
   5493c:	str	x15, [x0]
   54940:	lsr	x0, x11, #63
   54944:	sbc	x0, x0, xzr
   54948:	ret
   5494c:	ldr	x9, [x2]
   54950:	ldp	x10, x11, [x2, #8]!
   54954:	lsl	x13, x9, #1
   54958:	subs	x17, x13, x5
   5495c:	str	x17, [x0], #8
   54960:	sub	x1, x1, #0x8
   54964:	cbz	x18, 549d0 <__gmpn_rsblsh1_n@@Base+0xd0>
   54968:	b	54990 <__gmpn_rsblsh1_n@@Base+0x90>
   5496c:	tbnz	w3, #1, 54980 <__gmpn_rsblsh1_n@@Base+0x80>
   54970:	negs	x11, xzr
   54974:	ldp	x8, x9, [x2], #-16
   54978:	sub	x1, x1, #0x20
   5497c:	b	549ac <__gmpn_rsblsh1_n@@Base+0xac>
   54980:	negs	x9, xzr
   54984:	ldp	x10, x11, [x2]
   54988:	sub	x1, x1, #0x10
   5498c:	cbz	x18, 549d0 <__gmpn_rsblsh1_n@@Base+0xd0>
   54990:	ldp	x4, x5, [x1, #16]
   54994:	extr	x12, x10, x9, #63
   54998:	ldp	x8, x9, [x2, #16]
   5499c:	extr	x13, x11, x10, #63
   549a0:	sbcs	x14, x12, x4
   549a4:	sbcs	x15, x13, x5
   549a8:	stp	x14, x15, [x0], #16
   549ac:	ldp	x4, x5, [x1, #32]!
   549b0:	extr	x12, x8, x11, #63
   549b4:	ldp	x10, x11, [x2, #32]!
   549b8:	extr	x13, x9, x8, #63
   549bc:	sbcs	x16, x12, x4
   549c0:	sbcs	x17, x13, x5
   549c4:	stp	x16, x17, [x0], #16
   549c8:	sub	x18, x18, #0x1
   549cc:	cbnz	x18, 54990 <__gmpn_rsblsh1_n@@Base+0x90>
   549d0:	ldp	x4, x5, [x1, #16]
   549d4:	extr	x12, x10, x9, #63
   549d8:	extr	x13, x11, x10, #63
   549dc:	sbcs	x14, x12, x4
   549e0:	sbcs	x15, x13, x5
   549e4:	stp	x14, x15, [x0]
   549e8:	lsr	x0, x11, #63
   549ec:	sbc	x0, x0, xzr
   549f0:	ret
   549f4:	nop
   549f8:	nop
   549fc:	nop

0000000000054a00 <__gmpn_rsh1add_n@@Base>:
   54a00:	lsr	x18, x3, #2
   54a04:	tbz	w3, #0, 54aac <__gmpn_rsh1add_n@@Base+0xac>
   54a08:	ldr	x5, [x1], #8
   54a0c:	ldr	x9, [x2], #8
   54a10:	tbnz	w3, #1, 54a68 <__gmpn_rsh1add_n@@Base+0x68>
   54a14:	adds	x13, x5, x9
   54a18:	and	x10, x13, #0x1
   54a1c:	cbz	x18, 54a54 <__gmpn_rsh1add_n@@Base+0x54>
   54a20:	ldp	x4, x5, [x1], #48
   54a24:	ldp	x8, x9, [x2], #48
   54a28:	adcs	x14, x4, x8
   54a2c:	adcs	x15, x5, x9
   54a30:	ldp	x4, x5, [x1, #-32]
   54a34:	ldp	x8, x9, [x2, #-32]
   54a38:	extr	x17, x14, x13, #1
   54a3c:	adcs	x12, x4, x8
   54a40:	adcs	x13, x5, x9
   54a44:	str	x17, [x0], #24
   54a48:	sub	x18, x18, #0x1
   54a4c:	cbz	x18, 54b50 <__gmpn_rsh1add_n@@Base+0x150>
   54a50:	b	54b10 <__gmpn_rsh1add_n@@Base+0x110>
   54a54:	cset	x14, cs  // cs = hs, nlast
   54a58:	extr	x17, x14, x13, #1
   54a5c:	str	x17, [x0]
   54a60:	mov	x0, x10
   54a64:	ret
   54a68:	adds	x15, x5, x9
   54a6c:	and	x10, x15, #0x1
   54a70:	ldp	x4, x5, [x1], #32
   54a74:	ldp	x8, x9, [x2], #32
   54a78:	adcs	x12, x4, x8
   54a7c:	adcs	x13, x5, x9
   54a80:	cbz	x18, 54aa0 <__gmpn_rsh1add_n@@Base+0xa0>
   54a84:	ldp	x4, x5, [x1, #-16]
   54a88:	ldp	x8, x9, [x2, #-16]
   54a8c:	extr	x17, x12, x15, #1
   54a90:	adcs	x14, x4, x8
   54a94:	adcs	x15, x5, x9
   54a98:	str	x17, [x0], #8
   54a9c:	b	54b2c <__gmpn_rsh1add_n@@Base+0x12c>
   54aa0:	extr	x17, x12, x15, #1
   54aa4:	str	x17, [x0], #8
   54aa8:	b	54b5c <__gmpn_rsh1add_n@@Base+0x15c>
   54aac:	tbz	w3, #1, 54adc <__gmpn_rsh1add_n@@Base+0xdc>
   54ab0:	ldp	x4, x5, [x1], #32
   54ab4:	ldp	x8, x9, [x2], #32
   54ab8:	adds	x12, x4, x8
   54abc:	adcs	x13, x5, x9
   54ac0:	and	x10, x12, #0x1
   54ac4:	cbz	x18, 54b5c <__gmpn_rsh1add_n@@Base+0x15c>
   54ac8:	ldp	x4, x5, [x1, #-16]
   54acc:	ldp	x8, x9, [x2, #-16]
   54ad0:	adcs	x14, x4, x8
   54ad4:	adcs	x15, x5, x9
   54ad8:	b	54b2c <__gmpn_rsh1add_n@@Base+0x12c>
   54adc:	ldp	x4, x5, [x1], #48
   54ae0:	ldp	x8, x9, [x2], #48
   54ae4:	adds	x14, x4, x8
   54ae8:	adcs	x15, x5, x9
   54aec:	and	x10, x14, #0x1
   54af0:	ldp	x4, x5, [x1, #-32]
   54af4:	ldp	x8, x9, [x2, #-32]
   54af8:	adcs	x12, x4, x8
   54afc:	adcs	x13, x5, x9
   54b00:	add	x0, x0, #0x10
   54b04:	sub	x18, x18, #0x1
   54b08:	cbz	x18, 54b50 <__gmpn_rsh1add_n@@Base+0x150>
   54b0c:	nop
   54b10:	ldp	x4, x5, [x1, #-16]
   54b14:	ldp	x8, x9, [x2, #-16]
   54b18:	extr	x16, x15, x14, #1
   54b1c:	extr	x17, x12, x15, #1
   54b20:	adcs	x14, x4, x8
   54b24:	adcs	x15, x5, x9
   54b28:	stp	x16, x17, [x0, #-16]
   54b2c:	ldp	x4, x5, [x1], #32
   54b30:	ldp	x8, x9, [x2], #32
   54b34:	extr	x16, x13, x12, #1
   54b38:	extr	x17, x14, x13, #1
   54b3c:	adcs	x12, x4, x8
   54b40:	adcs	x13, x5, x9
   54b44:	stp	x16, x17, [x0], #32
   54b48:	sub	x18, x18, #0x1
   54b4c:	cbnz	x18, 54b10 <__gmpn_rsh1add_n@@Base+0x110>
   54b50:	extr	x16, x15, x14, #1
   54b54:	extr	x17, x12, x15, #1
   54b58:	stp	x16, x17, [x0, #-16]
   54b5c:	cset	x14, cs  // cs = hs, nlast
   54b60:	extr	x16, x13, x12, #1
   54b64:	extr	x17, x14, x13, #1
   54b68:	stp	x16, x17, [x0]
   54b6c:	mov	x0, x10
   54b70:	ret
   54b74:	nop
   54b78:	nop
   54b7c:	nop

0000000000054b80 <__gmpn_rsh1sub_n@@Base>:
   54b80:	lsr	x18, x3, #2
   54b84:	tbz	w3, #0, 54c2c <__gmpn_rsh1sub_n@@Base+0xac>
   54b88:	ldr	x5, [x1], #8
   54b8c:	ldr	x9, [x2], #8
   54b90:	tbnz	w3, #1, 54be8 <__gmpn_rsh1sub_n@@Base+0x68>
   54b94:	subs	x13, x5, x9
   54b98:	and	x10, x13, #0x1
   54b9c:	cbz	x18, 54bd4 <__gmpn_rsh1sub_n@@Base+0x54>
   54ba0:	ldp	x4, x5, [x1], #48
   54ba4:	ldp	x8, x9, [x2], #48
   54ba8:	sbcs	x14, x4, x8
   54bac:	sbcs	x15, x5, x9
   54bb0:	ldp	x4, x5, [x1, #-32]
   54bb4:	ldp	x8, x9, [x2, #-32]
   54bb8:	extr	x17, x14, x13, #1
   54bbc:	sbcs	x12, x4, x8
   54bc0:	sbcs	x13, x5, x9
   54bc4:	str	x17, [x0], #24
   54bc8:	sub	x18, x18, #0x1
   54bcc:	cbz	x18, 54cd0 <__gmpn_rsh1sub_n@@Base+0x150>
   54bd0:	b	54c90 <__gmpn_rsh1sub_n@@Base+0x110>
   54bd4:	cset	x14, cc  // cc = lo, ul, last
   54bd8:	extr	x17, x14, x13, #1
   54bdc:	str	x17, [x0]
   54be0:	mov	x0, x10
   54be4:	ret
   54be8:	subs	x15, x5, x9
   54bec:	and	x10, x15, #0x1
   54bf0:	ldp	x4, x5, [x1], #32
   54bf4:	ldp	x8, x9, [x2], #32
   54bf8:	sbcs	x12, x4, x8
   54bfc:	sbcs	x13, x5, x9
   54c00:	cbz	x18, 54c20 <__gmpn_rsh1sub_n@@Base+0xa0>
   54c04:	ldp	x4, x5, [x1, #-16]
   54c08:	ldp	x8, x9, [x2, #-16]
   54c0c:	extr	x17, x12, x15, #1
   54c10:	sbcs	x14, x4, x8
   54c14:	sbcs	x15, x5, x9
   54c18:	str	x17, [x0], #8
   54c1c:	b	54cac <__gmpn_rsh1sub_n@@Base+0x12c>
   54c20:	extr	x17, x12, x15, #1
   54c24:	str	x17, [x0], #8
   54c28:	b	54cdc <__gmpn_rsh1sub_n@@Base+0x15c>
   54c2c:	tbz	w3, #1, 54c5c <__gmpn_rsh1sub_n@@Base+0xdc>
   54c30:	ldp	x4, x5, [x1], #32
   54c34:	ldp	x8, x9, [x2], #32
   54c38:	subs	x12, x4, x8
   54c3c:	sbcs	x13, x5, x9
   54c40:	and	x10, x12, #0x1
   54c44:	cbz	x18, 54cdc <__gmpn_rsh1sub_n@@Base+0x15c>
   54c48:	ldp	x4, x5, [x1, #-16]
   54c4c:	ldp	x8, x9, [x2, #-16]
   54c50:	sbcs	x14, x4, x8
   54c54:	sbcs	x15, x5, x9
   54c58:	b	54cac <__gmpn_rsh1sub_n@@Base+0x12c>
   54c5c:	ldp	x4, x5, [x1], #48
   54c60:	ldp	x8, x9, [x2], #48
   54c64:	subs	x14, x4, x8
   54c68:	sbcs	x15, x5, x9
   54c6c:	and	x10, x14, #0x1
   54c70:	ldp	x4, x5, [x1, #-32]
   54c74:	ldp	x8, x9, [x2, #-32]
   54c78:	sbcs	x12, x4, x8
   54c7c:	sbcs	x13, x5, x9
   54c80:	add	x0, x0, #0x10
   54c84:	sub	x18, x18, #0x1
   54c88:	cbz	x18, 54cd0 <__gmpn_rsh1sub_n@@Base+0x150>
   54c8c:	nop
   54c90:	ldp	x4, x5, [x1, #-16]
   54c94:	ldp	x8, x9, [x2, #-16]
   54c98:	extr	x16, x15, x14, #1
   54c9c:	extr	x17, x12, x15, #1
   54ca0:	sbcs	x14, x4, x8
   54ca4:	sbcs	x15, x5, x9
   54ca8:	stp	x16, x17, [x0, #-16]
   54cac:	ldp	x4, x5, [x1], #32
   54cb0:	ldp	x8, x9, [x2], #32
   54cb4:	extr	x16, x13, x12, #1
   54cb8:	extr	x17, x14, x13, #1
   54cbc:	sbcs	x12, x4, x8
   54cc0:	sbcs	x13, x5, x9
   54cc4:	stp	x16, x17, [x0], #32
   54cc8:	sub	x18, x18, #0x1
   54ccc:	cbnz	x18, 54c90 <__gmpn_rsh1sub_n@@Base+0x110>
   54cd0:	extr	x16, x15, x14, #1
   54cd4:	extr	x17, x12, x15, #1
   54cd8:	stp	x16, x17, [x0, #-16]
   54cdc:	cset	x14, cc  // cc = lo, ul, last
   54ce0:	extr	x16, x13, x12, #1
   54ce4:	extr	x17, x14, x13, #1
   54ce8:	stp	x16, x17, [x0]
   54cec:	mov	x0, x10
   54cf0:	ret
   54cf4:	nop
   54cf8:	nop
   54cfc:	nop

0000000000054d00 <__gmpn_addlsh2_n@@Base>:
   54d00:	lsr	x18, x3, #2
   54d04:	tbz	w3, #0, 54d6c <__gmpn_addlsh2_n@@Base+0x6c>
   54d08:	ldr	x5, [x1]
   54d0c:	tbnz	w3, #1, 54d4c <__gmpn_addlsh2_n@@Base+0x4c>
   54d10:	ldr	x11, [x2]
   54d14:	cbz	x18, 54d34 <__gmpn_addlsh2_n@@Base+0x34>
   54d18:	ldp	x8, x9, [x2, #8]
   54d1c:	lsl	x13, x11, #2
   54d20:	adds	x15, x13, x5
   54d24:	str	x15, [x0], #8
   54d28:	sub	x1, x1, #0x18
   54d2c:	sub	x2, x2, #0x8
   54d30:	b	54dac <__gmpn_addlsh2_n@@Base+0xac>
   54d34:	lsl	x13, x11, #2
   54d38:	adds	x15, x13, x5
   54d3c:	str	x15, [x0]
   54d40:	lsr	x0, x11, #62
   54d44:	adc	x0, x0, xzr
   54d48:	ret
   54d4c:	ldr	x9, [x2]
   54d50:	ldp	x10, x11, [x2, #8]!
   54d54:	lsl	x13, x9, #2
   54d58:	adds	x17, x13, x5
   54d5c:	str	x17, [x0], #8
   54d60:	sub	x1, x1, #0x8
   54d64:	cbz	x18, 54dd0 <__gmpn_addlsh2_n@@Base+0xd0>
   54d68:	b	54d90 <__gmpn_addlsh2_n@@Base+0x90>
   54d6c:	tbnz	w3, #1, 54d80 <__gmpn_addlsh2_n@@Base+0x80>
   54d70:	adds	x11, xzr, xzr
   54d74:	ldp	x8, x9, [x2], #-16
   54d78:	sub	x1, x1, #0x20
   54d7c:	b	54dac <__gmpn_addlsh2_n@@Base+0xac>
   54d80:	adds	x9, xzr, xzr
   54d84:	ldp	x10, x11, [x2]
   54d88:	sub	x1, x1, #0x10
   54d8c:	cbz	x18, 54dd0 <__gmpn_addlsh2_n@@Base+0xd0>
   54d90:	ldp	x4, x5, [x1, #16]
   54d94:	extr	x12, x10, x9, #62
   54d98:	ldp	x8, x9, [x2, #16]
   54d9c:	extr	x13, x11, x10, #62
   54da0:	adcs	x14, x12, x4
   54da4:	adcs	x15, x13, x5
   54da8:	stp	x14, x15, [x0], #16
   54dac:	ldp	x4, x5, [x1, #32]!
   54db0:	extr	x12, x8, x11, #62
   54db4:	ldp	x10, x11, [x2, #32]!
   54db8:	extr	x13, x9, x8, #62
   54dbc:	adcs	x16, x12, x4
   54dc0:	adcs	x17, x13, x5
   54dc4:	stp	x16, x17, [x0], #16
   54dc8:	sub	x18, x18, #0x1
   54dcc:	cbnz	x18, 54d90 <__gmpn_addlsh2_n@@Base+0x90>
   54dd0:	ldp	x4, x5, [x1, #16]
   54dd4:	extr	x12, x10, x9, #62
   54dd8:	extr	x13, x11, x10, #62
   54ddc:	adcs	x14, x12, x4
   54de0:	adcs	x15, x13, x5
   54de4:	stp	x14, x15, [x0]
   54de8:	lsr	x0, x11, #62
   54dec:	adc	x0, x0, xzr
   54df0:	ret
   54df4:	nop
   54df8:	nop
   54dfc:	nop

0000000000054e00 <__gmpn_sublsh2_n@@Base>:
   54e00:	lsr	x18, x3, #2
   54e04:	tbz	w3, #0, 54e6c <__gmpn_sublsh2_n@@Base+0x6c>
   54e08:	ldr	x5, [x1]
   54e0c:	tbnz	w3, #1, 54e4c <__gmpn_sublsh2_n@@Base+0x4c>
   54e10:	ldr	x11, [x2]
   54e14:	cbz	x18, 54e34 <__gmpn_sublsh2_n@@Base+0x34>
   54e18:	ldp	x8, x9, [x2, #8]
   54e1c:	lsl	x13, x11, #2
   54e20:	subs	x15, x5, x13
   54e24:	str	x15, [x0], #8
   54e28:	sub	x1, x1, #0x18
   54e2c:	sub	x2, x2, #0x8
   54e30:	b	54eac <__gmpn_sublsh2_n@@Base+0xac>
   54e34:	lsl	x13, x11, #2
   54e38:	subs	x15, x5, x13
   54e3c:	str	x15, [x0]
   54e40:	lsr	x0, x11, #62
   54e44:	cinc	x0, x0, cc  // cc = lo, ul, last
   54e48:	ret
   54e4c:	ldr	x9, [x2]
   54e50:	ldp	x10, x11, [x2, #8]!
   54e54:	lsl	x13, x9, #2
   54e58:	subs	x17, x5, x13
   54e5c:	str	x17, [x0], #8
   54e60:	sub	x1, x1, #0x8
   54e64:	cbz	x18, 54ed0 <__gmpn_sublsh2_n@@Base+0xd0>
   54e68:	b	54e90 <__gmpn_sublsh2_n@@Base+0x90>
   54e6c:	tbnz	w3, #1, 54e80 <__gmpn_sublsh2_n@@Base+0x80>
   54e70:	negs	x11, xzr
   54e74:	ldp	x8, x9, [x2], #-16
   54e78:	sub	x1, x1, #0x20
   54e7c:	b	54eac <__gmpn_sublsh2_n@@Base+0xac>
   54e80:	negs	x9, xzr
   54e84:	ldp	x10, x11, [x2]
   54e88:	sub	x1, x1, #0x10
   54e8c:	cbz	x18, 54ed0 <__gmpn_sublsh2_n@@Base+0xd0>
   54e90:	ldp	x4, x5, [x1, #16]
   54e94:	extr	x12, x10, x9, #62
   54e98:	ldp	x8, x9, [x2, #16]
   54e9c:	extr	x13, x11, x10, #62
   54ea0:	sbcs	x14, x4, x12
   54ea4:	sbcs	x15, x5, x13
   54ea8:	stp	x14, x15, [x0], #16
   54eac:	ldp	x4, x5, [x1, #32]!
   54eb0:	extr	x12, x8, x11, #62
   54eb4:	ldp	x10, x11, [x2, #32]!
   54eb8:	extr	x13, x9, x8, #62
   54ebc:	sbcs	x16, x4, x12
   54ec0:	sbcs	x17, x5, x13
   54ec4:	stp	x16, x17, [x0], #16
   54ec8:	sub	x18, x18, #0x1
   54ecc:	cbnz	x18, 54e90 <__gmpn_sublsh2_n@@Base+0x90>
   54ed0:	ldp	x4, x5, [x1, #16]
   54ed4:	extr	x12, x10, x9, #62
   54ed8:	extr	x13, x11, x10, #62
   54edc:	sbcs	x14, x4, x12
   54ee0:	sbcs	x15, x5, x13
   54ee4:	stp	x14, x15, [x0]
   54ee8:	lsr	x0, x11, #62
   54eec:	cinc	x0, x0, cc  // cc = lo, ul, last
   54ef0:	ret
   54ef4:	nop
   54ef8:	nop
   54efc:	nop

0000000000054f00 <__gmpn_rsblsh2_n@@Base>:
   54f00:	lsr	x18, x3, #2
   54f04:	tbz	w3, #0, 54f6c <__gmpn_rsblsh2_n@@Base+0x6c>
   54f08:	ldr	x5, [x1]
   54f0c:	tbnz	w3, #1, 54f4c <__gmpn_rsblsh2_n@@Base+0x4c>
   54f10:	ldr	x11, [x2]
   54f14:	cbz	x18, 54f34 <__gmpn_rsblsh2_n@@Base+0x34>
   54f18:	ldp	x8, x9, [x2, #8]
   54f1c:	lsl	x13, x11, #2
   54f20:	subs	x15, x13, x5
   54f24:	str	x15, [x0], #8
   54f28:	sub	x1, x1, #0x18
   54f2c:	sub	x2, x2, #0x8
   54f30:	b	54fac <__gmpn_rsblsh2_n@@Base+0xac>
   54f34:	lsl	x13, x11, #2
   54f38:	subs	x15, x13, x5
   54f3c:	str	x15, [x0]
   54f40:	lsr	x0, x11, #62
   54f44:	sbc	x0, x0, xzr
   54f48:	ret
   54f4c:	ldr	x9, [x2]
   54f50:	ldp	x10, x11, [x2, #8]!
   54f54:	lsl	x13, x9, #2
   54f58:	subs	x17, x13, x5
   54f5c:	str	x17, [x0], #8
   54f60:	sub	x1, x1, #0x8
   54f64:	cbz	x18, 54fd0 <__gmpn_rsblsh2_n@@Base+0xd0>
   54f68:	b	54f90 <__gmpn_rsblsh2_n@@Base+0x90>
   54f6c:	tbnz	w3, #1, 54f80 <__gmpn_rsblsh2_n@@Base+0x80>
   54f70:	negs	x11, xzr
   54f74:	ldp	x8, x9, [x2], #-16
   54f78:	sub	x1, x1, #0x20
   54f7c:	b	54fac <__gmpn_rsblsh2_n@@Base+0xac>
   54f80:	negs	x9, xzr
   54f84:	ldp	x10, x11, [x2]
   54f88:	sub	x1, x1, #0x10
   54f8c:	cbz	x18, 54fd0 <__gmpn_rsblsh2_n@@Base+0xd0>
   54f90:	ldp	x4, x5, [x1, #16]
   54f94:	extr	x12, x10, x9, #62
   54f98:	ldp	x8, x9, [x2, #16]
   54f9c:	extr	x13, x11, x10, #62
   54fa0:	sbcs	x14, x12, x4
   54fa4:	sbcs	x15, x13, x5
   54fa8:	stp	x14, x15, [x0], #16
   54fac:	ldp	x4, x5, [x1, #32]!
   54fb0:	extr	x12, x8, x11, #62
   54fb4:	ldp	x10, x11, [x2, #32]!
   54fb8:	extr	x13, x9, x8, #62
   54fbc:	sbcs	x16, x12, x4
   54fc0:	sbcs	x17, x13, x5
   54fc4:	stp	x16, x17, [x0], #16
   54fc8:	sub	x18, x18, #0x1
   54fcc:	cbnz	x18, 54f90 <__gmpn_rsblsh2_n@@Base+0x90>
   54fd0:	ldp	x4, x5, [x1, #16]
   54fd4:	extr	x12, x10, x9, #62
   54fd8:	extr	x13, x11, x10, #62
   54fdc:	sbcs	x14, x12, x4
   54fe0:	sbcs	x15, x13, x5
   54fe4:	stp	x14, x15, [x0]
   54fe8:	lsr	x0, x11, #62
   54fec:	sbc	x0, x0, xzr
   54ff0:	ret

0000000000054ff4 <__gmpn_add_n_sub_n@@Base>:
   54ff4:	stp	x29, x30, [sp, #-96]!
   54ff8:	stp	x28, x27, [sp, #16]
   54ffc:	stp	x26, x25, [sp, #32]
   55000:	stp	x24, x23, [sp, #48]
   55004:	stp	x22, x21, [sp, #64]
   55008:	stp	x20, x19, [sp, #80]
   5500c:	mov	x29, sp
   55010:	sub	sp, sp, #0x560
   55014:	mov	x19, x4
   55018:	mov	x20, x3
   5501c:	mov	x21, x2
   55020:	mov	x22, x0
   55024:	cmp	x0, x2
   55028:	mov	x23, x1
   5502c:	b.eq	550b8 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   55030:	cmp	x22, x20
   55034:	b.eq	550b8 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   55038:	cmp	x19, #0x1
   5503c:	b.lt	551d8 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   55040:	mov	x27, xzr
   55044:	mov	x25, xzr
   55048:	mov	x24, xzr
   5504c:	mov	x8, x19
   55050:	subs	x28, x8, #0xaa
   55054:	mov	w9, #0xaa                  	// #170
   55058:	csel	x26, x8, x9, lt  // lt = tstop
   5505c:	mov	x0, x22
   55060:	mov	x1, x21
   55064:	mov	x2, x20
   55068:	mov	x3, x26
   5506c:	mov	x4, x24
   55070:	bl	ce90 <__gmpn_add_nc@plt>
   55074:	mov	x24, x0
   55078:	mov	x0, x23
   5507c:	mov	x1, x21
   55080:	mov	x2, x20
   55084:	mov	x3, x26
   55088:	mov	x4, x25
   5508c:	bl	c760 <__gmpn_sub_nc@plt>
   55090:	add	x27, x27, #0xaa
   55094:	mov	x25, x0
   55098:	add	x23, x23, #0x550
   5509c:	add	x20, x20, #0x550
   550a0:	add	x21, x21, #0x550
   550a4:	cmp	x27, x19
   550a8:	add	x22, x22, #0x550
   550ac:	mov	x8, x28
   550b0:	b.lt	55050 <__gmpn_add_n_sub_n@@Base+0x5c>  // b.tstop
   550b4:	b	551e0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   550b8:	cmp	x23, x21
   550bc:	b.eq	55148 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   550c0:	cmp	x23, x20
   550c4:	b.eq	55148 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   550c8:	cmp	x19, #0x1
   550cc:	b.lt	551d8 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   550d0:	mov	x27, xzr
   550d4:	mov	x25, xzr
   550d8:	mov	x24, xzr
   550dc:	mov	x8, x19
   550e0:	subs	x28, x8, #0xaa
   550e4:	mov	w9, #0xaa                  	// #170
   550e8:	csel	x26, x8, x9, lt  // lt = tstop
   550ec:	mov	x0, x23
   550f0:	mov	x1, x21
   550f4:	mov	x2, x20
   550f8:	mov	x3, x26
   550fc:	mov	x4, x25
   55100:	bl	c760 <__gmpn_sub_nc@plt>
   55104:	mov	x25, x0
   55108:	mov	x0, x22
   5510c:	mov	x1, x21
   55110:	mov	x2, x20
   55114:	mov	x3, x26
   55118:	mov	x4, x24
   5511c:	bl	ce90 <__gmpn_add_nc@plt>
   55120:	add	x27, x27, #0xaa
   55124:	mov	x24, x0
   55128:	add	x22, x22, #0x550
   5512c:	add	x20, x20, #0x550
   55130:	add	x21, x21, #0x550
   55134:	cmp	x27, x19
   55138:	add	x23, x23, #0x550
   5513c:	mov	x8, x28
   55140:	b.lt	550e0 <__gmpn_add_n_sub_n@@Base+0xec>  // b.tstop
   55144:	b	551e0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   55148:	cmp	x19, #0x1
   5514c:	b.lt	551d8 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   55150:	mov	x27, xzr
   55154:	mov	x25, xzr
   55158:	mov	x24, xzr
   5515c:	mov	x8, x19
   55160:	subs	x28, x8, #0xaa
   55164:	mov	w9, #0xaa                  	// #170
   55168:	csel	x26, x8, x9, lt  // lt = tstop
   5516c:	add	x0, sp, #0x8
   55170:	mov	x1, x21
   55174:	mov	x2, x20
   55178:	mov	x3, x26
   5517c:	mov	x4, x24
   55180:	bl	ce90 <__gmpn_add_nc@plt>
   55184:	mov	x24, x0
   55188:	mov	x0, x23
   5518c:	mov	x1, x21
   55190:	mov	x2, x20
   55194:	mov	x3, x26
   55198:	mov	x4, x25
   5519c:	bl	c760 <__gmpn_sub_nc@plt>
   551a0:	mov	x25, x0
   551a4:	add	x1, sp, #0x8
   551a8:	mov	x0, x22
   551ac:	mov	x2, x26
   551b0:	bl	ca50 <__gmpn_copyi@plt>
   551b4:	add	x27, x27, #0xaa
   551b8:	add	x22, x22, #0x550
   551bc:	add	x23, x23, #0x550
   551c0:	add	x20, x20, #0x550
   551c4:	cmp	x27, x19
   551c8:	add	x21, x21, #0x550
   551cc:	mov	x8, x28
   551d0:	b.lt	55160 <__gmpn_add_n_sub_n@@Base+0x16c>  // b.tstop
   551d4:	b	551e0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   551d8:	mov	x24, xzr
   551dc:	mov	x25, xzr
   551e0:	add	x0, x25, x24, lsl #1
   551e4:	add	sp, sp, #0x560
   551e8:	ldp	x20, x19, [sp, #80]
   551ec:	ldp	x22, x21, [sp, #64]
   551f0:	ldp	x24, x23, [sp, #48]
   551f4:	ldp	x26, x25, [sp, #32]
   551f8:	ldp	x28, x27, [sp, #16]
   551fc:	ldp	x29, x30, [sp], #96
   55200:	ret

0000000000055204 <__gmp_asprintf@@Base>:
   55204:	sub	sp, sp, #0x100
   55208:	stp	x29, x30, [sp, #240]
   5520c:	add	x29, sp, #0xf0
   55210:	mov	x8, #0xffffffffffffffd0    	// #-48
   55214:	mov	x9, sp
   55218:	sub	x10, x29, #0x70
   5521c:	movk	x8, #0xff80, lsl #32
   55220:	add	x11, x29, #0x10
   55224:	add	x9, x9, #0x80
   55228:	add	x10, x10, #0x30
   5522c:	stp	x9, x8, [x29, #-16]
   55230:	stp	x11, x10, [x29, #-32]
   55234:	stp	x2, x3, [x29, #-112]
   55238:	stp	x4, x5, [x29, #-96]
   5523c:	stp	x6, x7, [x29, #-80]
   55240:	stp	q1, q2, [sp, #16]
   55244:	str	q0, [sp]
   55248:	ldp	q0, q1, [x29, #-32]
   5524c:	sub	x2, x29, #0x40
   55250:	stp	q3, q4, [sp, #48]
   55254:	stp	q5, q6, [sp, #80]
   55258:	str	q7, [sp, #112]
   5525c:	stp	q0, q1, [x29, #-64]
   55260:	bl	c500 <__gmp_vasprintf@plt>
   55264:	ldp	x29, x30, [sp, #240]
   55268:	add	sp, sp, #0x100
   5526c:	ret

0000000000055270 <__gmp_asprintf_memory@@Base>:
   55270:	stp	x29, x30, [sp, #-48]!
   55274:	str	x21, [sp, #16]
   55278:	stp	x20, x19, [sp, #32]
   5527c:	ldp	x9, x8, [x0, #16]
   55280:	mov	x19, x0
   55284:	mov	x20, x2
   55288:	mov	x21, x1
   5528c:	add	x10, x9, x2
   55290:	cmp	x8, x10
   55294:	mov	x29, sp
   55298:	b.ls	552a4 <__gmp_asprintf_memory@@Base+0x34>  // b.plast
   5529c:	ldr	x0, [x19, #8]
   552a0:	b	552cc <__gmp_asprintf_memory@@Base+0x5c>
   552a4:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   552a8:	ldr	x9, [x9, #3792]
   552ac:	lsl	x2, x10, #1
   552b0:	str	x2, [x19, #24]
   552b4:	ldr	x0, [x19, #8]
   552b8:	ldr	x9, [x9]
   552bc:	mov	x1, x8
   552c0:	blr	x9
   552c4:	ldr	x9, [x19, #16]
   552c8:	str	x0, [x19, #8]
   552cc:	add	x0, x0, x9
   552d0:	mov	x1, x21
   552d4:	mov	x2, x20
   552d8:	bl	bed0 <memcpy@plt>
   552dc:	ldr	x8, [x19, #16]
   552e0:	mov	w0, w20
   552e4:	ldr	x21, [sp, #16]
   552e8:	add	x8, x8, x20
   552ec:	str	x8, [x19, #16]
   552f0:	ldp	x20, x19, [sp, #32]
   552f4:	ldp	x29, x30, [sp], #48
   552f8:	ret

00000000000552fc <__gmp_asprintf_reps@@Base>:
   552fc:	stp	x29, x30, [sp, #-48]!
   55300:	stp	x22, x21, [sp, #16]
   55304:	stp	x20, x19, [sp, #32]
   55308:	ldp	x9, x8, [x0, #16]
   5530c:	mov	w20, w2
   55310:	sxtw	x21, w20
   55314:	mov	x19, x0
   55318:	add	x10, x9, x21
   5531c:	cmp	x8, x10
   55320:	mov	w22, w1
   55324:	mov	x29, sp
   55328:	b.ls	55334 <__gmp_asprintf_reps@@Base+0x38>  // b.plast
   5532c:	ldr	x0, [x19, #8]
   55330:	b	5535c <__gmp_asprintf_reps@@Base+0x60>
   55334:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55338:	ldr	x9, [x9, #3792]
   5533c:	lsl	x2, x10, #1
   55340:	str	x2, [x19, #24]
   55344:	ldr	x0, [x19, #8]
   55348:	ldr	x9, [x9]
   5534c:	mov	x1, x8
   55350:	blr	x9
   55354:	ldr	x9, [x19, #16]
   55358:	str	x0, [x19, #8]
   5535c:	add	x0, x0, x9
   55360:	mov	w1, w22
   55364:	mov	x2, x21
   55368:	bl	c5f0 <memset@plt>
   5536c:	ldr	x8, [x19, #16]
   55370:	mov	w0, w20
   55374:	add	x8, x8, x21
   55378:	str	x8, [x19, #16]
   5537c:	ldp	x20, x19, [sp, #32]
   55380:	ldp	x22, x21, [sp, #16]
   55384:	ldp	x29, x30, [sp], #48
   55388:	ret

000000000005538c <__gmp_asprintf_final@@Base>:
   5538c:	stp	x29, x30, [sp, #-32]!
   55390:	str	x19, [sp, #16]
   55394:	mov	x19, x0
   55398:	ldr	x0, [x0, #8]
   5539c:	ldr	x8, [x19, #16]
   553a0:	mov	x29, sp
   553a4:	strb	wzr, [x0, x8]
   553a8:	ldp	x8, x1, [x19, #16]
   553ac:	add	x2, x8, #0x1
   553b0:	cmp	x1, x2
   553b4:	b.eq	553c8 <__gmp_asprintf_final@@Base+0x3c>  // b.none
   553b8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   553bc:	ldr	x8, [x8, #3792]
   553c0:	ldr	x8, [x8]
   553c4:	blr	x8
   553c8:	ldr	x8, [x19]
   553cc:	ldr	x19, [sp, #16]
   553d0:	str	x0, [x8]
   553d4:	mov	w0, wzr
   553d8:	ldp	x29, x30, [sp], #32
   553dc:	ret

00000000000553e0 <__gmp_doprnt@@Base>:
   553e0:	sub	sp, sp, #0x190
   553e4:	str	d8, [sp, #288]
   553e8:	stp	x29, x30, [sp, #304]
   553ec:	stp	x28, x27, [sp, #320]
   553f0:	stp	x26, x25, [sp, #336]
   553f4:	stp	x24, x23, [sp, #352]
   553f8:	stp	x22, x21, [sp, #368]
   553fc:	stp	x20, x19, [sp, #384]
   55400:	stp	x1, x0, [sp, #16]
   55404:	ldp	q1, q0, [x3]
   55408:	add	x29, sp, #0x120
   5540c:	mov	x0, x2
   55410:	mov	x23, x2
   55414:	stp	q1, q0, [x29, #-48]
   55418:	bl	bf60 <strlen@plt>
   5541c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55420:	ldr	x8, [x8, #3840]
   55424:	add	x19, x0, #0x1
   55428:	mov	x0, x19
   5542c:	ldr	x8, [x8]
   55430:	blr	x8
   55434:	mov	x1, x23
   55438:	mov	x2, x19
   5543c:	mov	x20, x0
   55440:	str	x19, [sp, #8]
   55444:	bl	bed0 <memcpy@plt>
   55448:	ldp	q0, q1, [x29, #-48]
   5544c:	mov	w1, #0x25                  	// #37
   55450:	mov	x0, x20
   55454:	stp	q0, q1, [x29, #-112]
   55458:	bl	cda0 <strchr@plt>
   5545c:	str	x20, [sp]
   55460:	cbz	x0, 55c48 <__gmp_doprnt@@Base+0x868>
   55464:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55468:	adrp	x10, 65000 <__gmp_oddfac_table@@Base+0xf0>
   5546c:	ldr	d8, [x9, #1056]
   55470:	ldr	q0, [x10, #1072]
   55474:	add	x8, sp, #0x78
   55478:	adrp	x22, 65000 <__gmp_oddfac_table@@Base+0xf0>
   5547c:	mov	x26, x0
   55480:	add	x21, x8, #0x30
   55484:	add	x8, x8, #0x1c
   55488:	add	x22, x22, #0x440
   5548c:	str	xzr, [sp, #32]
   55490:	str	x8, [x29, #8]
   55494:	str	q0, [sp, #48]
   55498:	str	x20, [sp, #40]
   5549c:	b	554c8 <__gmp_doprnt@@Base+0xe8>
   554a0:	ldur	w8, [x29, #-20]
   554a4:	tbnz	w8, #31, 558d0 <__gmp_doprnt@@Base+0x4f0>
   554a8:	ldur	x8, [x29, #-48]
   554ac:	add	x8, x8, #0x8
   554b0:	stur	x8, [x29, #-48]
   554b4:	mov	w1, #0x25                  	// #37
   554b8:	mov	x0, x25
   554bc:	bl	cda0 <strchr@plt>
   554c0:	mov	x26, x0
   554c4:	cbz	x0, 55c4c <__gmp_doprnt@@Base+0x86c>
   554c8:	ldp	q0, q1, [x29, #-48]
   554cc:	adrp	x8, 65000 <__gmp_oddfac_table@@Base+0xf0>
   554d0:	add	x8, x8, #0x5a0
   554d4:	str	x8, [sp, #128]
   554d8:	stp	q0, q1, [x29, #-80]
   554dc:	ldr	q0, [sp, #48]
   554e0:	mov	w8, #0x20                  	// #32
   554e4:	mov	w20, wzr
   554e8:	mov	w23, wzr
   554ec:	add	x25, x26, #0x1
   554f0:	strb	w8, [sp, #140]
   554f4:	mov	w8, #0x1                   	// #1
   554f8:	str	d8, [sp, #120]
   554fc:	str	wzr, [sp, #136]
   55500:	stur	q0, [sp, #144]
   55504:	str	w8, [sp, #160]
   55508:	strb	wzr, [sp, #164]
   5550c:	str	wzr, [sp, #168]
   55510:	mov	x27, x21
   55514:	b	5551c <__gmp_doprnt@@Base+0x13c>
   55518:	mov	w23, w28
   5551c:	mov	x8, x25
   55520:	ldrb	w28, [x25], #1
   55524:	sub	w9, w28, #0x20
   55528:	cmp	w9, #0x5a
   5552c:	b.hi	554b4 <__gmp_doprnt@@Base+0xd4>  // b.pmore
   55530:	adr	x10, 554b4 <__gmp_doprnt@@Base+0xd4>
   55534:	ldrh	w11, [x22, x9, lsl #1]
   55538:	add	x10, x10, x11, lsl #2
   5553c:	br	x10
   55540:	mov	w19, wzr
   55544:	mov	x24, x25
   55548:	mov	w8, #0xa                   	// #10
   5554c:	madd	w8, w19, w8, w28
   55550:	ldrb	w28, [x24]
   55554:	mov	x25, x24
   55558:	sub	w19, w8, #0x30
   5555c:	tbnz	w28, #7, 55574 <__gmp_doprnt@@Base+0x194>
   55560:	add	x24, x25, #0x1
   55564:	bl	cae0 <__ctype_b_loc@plt>
   55568:	ldr	x8, [x0]
   5556c:	ldrh	w8, [x8, x28, lsl #1]
   55570:	tbnz	w8, #11, 55548 <__gmp_doprnt@@Base+0x168>
   55574:	str	w19, [x27]
   55578:	b	5551c <__gmp_doprnt@@Base+0x13c>
   5557c:	strb	w28, [sp, #164]
   55580:	b	5551c <__gmp_doprnt@@Base+0x13c>
   55584:	mov	w8, #0x3                   	// #3
   55588:	str	w8, [sp, #152]
   5558c:	b	5551c <__gmp_doprnt@@Base+0x13c>
   55590:	ldursw	x8, [x29, #-24]
   55594:	tbz	w8, #31, 5561c <__gmp_doprnt@@Base+0x23c>
   55598:	add	w9, w8, #0x8
   5559c:	cmn	w8, #0x8
   555a0:	stur	w9, [x29, #-24]
   555a4:	b.gt	5561c <__gmp_doprnt@@Base+0x23c>
   555a8:	ldur	x9, [x29, #-40]
   555ac:	add	x8, x9, x8
   555b0:	ldr	w8, [x8]
   555b4:	cmp	x27, x21
   555b8:	b.ne	55634 <__gmp_doprnt@@Base+0x254>  // b.any
   555bc:	b	55664 <__gmp_doprnt@@Base+0x284>
   555c0:	mov	w8, #0x1                   	// #1
   555c4:	str	w8, [sp, #144]
   555c8:	b	5551c <__gmp_doprnt@@Base+0x13c>
   555cc:	ldr	x27, [x29, #8]
   555d0:	mov	w8, #0xffffffff            	// #-1
   555d4:	str	w8, [sp, #148]
   555d8:	mov	w20, #0x1                   	// #1
   555dc:	b	5551c <__gmp_doprnt@@Base+0x13c>
   555e0:	cmp	x27, x21
   555e4:	b.eq	55640 <__gmp_doprnt@@Base+0x260>  // b.none
   555e8:	str	wzr, [x27]
   555ec:	b	5551c <__gmp_doprnt@@Base+0x13c>
   555f0:	mov	w23, #0x6c                  	// #108
   555f4:	strb	w23, [x8]
   555f8:	b	5551c <__gmp_doprnt@@Base+0x13c>
   555fc:	cmp	w23, #0x68
   55600:	mov	w23, #0x48                  	// #72
   55604:	b.eq	5551c <__gmp_doprnt@@Base+0x13c>  // b.none
   55608:	b	55518 <__gmp_doprnt@@Base+0x138>
   5560c:	cmp	w23, #0x6c
   55610:	mov	w23, #0x4c                  	// #76
   55614:	b.eq	5551c <__gmp_doprnt@@Base+0x13c>  // b.none
   55618:	b	55518 <__gmp_doprnt@@Base+0x138>
   5561c:	ldur	x8, [x29, #-48]
   55620:	add	x9, x8, #0x8
   55624:	stur	x9, [x29, #-48]
   55628:	ldr	w8, [x8]
   5562c:	cmp	x27, x21
   55630:	b.eq	55664 <__gmp_doprnt@@Base+0x284>  // b.none
   55634:	bic	w8, w8, w8, asr #31
   55638:	str	w8, [sp, #148]
   5563c:	b	5551c <__gmp_doprnt@@Base+0x13c>
   55640:	ldr	w8, [sp, #144]
   55644:	mov	w9, #0x30                  	// #48
   55648:	mov	x27, x21
   5564c:	strb	w9, [sp, #140]
   55650:	cmp	w8, #0x2
   55654:	b.ne	5551c <__gmp_doprnt@@Base+0x13c>  // b.any
   55658:	mov	w8, #0x3                   	// #3
   5565c:	str	w8, [sp, #144]
   55660:	b	55510 <__gmp_doprnt@@Base+0x130>
   55664:	tbz	w8, #31, 55674 <__gmp_doprnt@@Base+0x294>
   55668:	mov	w9, #0x1                   	// #1
   5566c:	neg	w8, w8
   55670:	str	w9, [sp, #144]
   55674:	str	w8, [sp, #168]
   55678:	b	55510 <__gmp_doprnt@@Base+0x130>
   5567c:	adrp	x8, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55680:	add	x8, x8, #0x5b0
   55684:	mov	w9, #0xfffffff0            	// #-16
   55688:	b	557a8 <__gmp_doprnt@@Base+0x3c8>
   5568c:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55690:	mov	w8, #0xfffffff6            	// #-10
   55694:	add	x9, x9, #0x5b7
   55698:	str	w8, [sp, #120]
   5569c:	str	x9, [sp, #128]
   556a0:	mov	w8, #0x2                   	// #2
   556a4:	str	w8, [sp, #124]
   556a8:	ldr	w8, [sp, #152]
   556ac:	cmp	w8, #0x3
   556b0:	b.eq	55788 <__gmp_doprnt@@Base+0x3a8>  // b.none
   556b4:	and	w8, w23, #0xff
   556b8:	cmp	w8, #0x4c
   556bc:	b.eq	557e0 <__gmp_doprnt@@Base+0x400>  // b.none
   556c0:	cmp	w8, #0x46
   556c4:	b.ne	554a0 <__gmp_doprnt@@Base+0xc0>  // b.any
   556c8:	ldp	x19, x1, [sp, #32]
   556cc:	cmp	x26, x1
   556d0:	b.eq	556fc <__gmp_doprnt@@Base+0x31c>  // b.none
   556d4:	strb	wzr, [x26]
   556d8:	ldp	x0, x8, [sp, #16]
   556dc:	ldp	q0, q1, [x29, #-112]
   556e0:	add	x2, sp, #0x40
   556e4:	ldr	x8, [x8]
   556e8:	stp	q0, q1, [sp, #64]
   556ec:	blr	x8
   556f0:	cmn	w0, #0x1
   556f4:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   556f8:	add	w19, w0, w19
   556fc:	mov	w0, #0x10000               	// #65536
   55700:	bl	c400 <nl_langinfo@plt>
   55704:	ldursw	x8, [x29, #-24]
   55708:	mov	x3, x0
   5570c:	tbz	w8, #31, 5572c <__gmp_doprnt@@Base+0x34c>
   55710:	add	w9, w8, #0x8
   55714:	cmn	w8, #0x8
   55718:	stur	w9, [x29, #-24]
   5571c:	b.gt	5572c <__gmp_doprnt@@Base+0x34c>
   55720:	ldur	x9, [x29, #-40]
   55724:	add	x8, x9, x8
   55728:	b	55738 <__gmp_doprnt@@Base+0x358>
   5572c:	ldur	x8, [x29, #-48]
   55730:	add	x9, x8, #0x8
   55734:	stur	x9, [x29, #-48]
   55738:	ldr	x4, [x8]
   5573c:	ldp	x1, x0, [sp, #16]
   55740:	add	x2, sp, #0x78
   55744:	bl	d230 <__gmp_doprnt_mpf2@plt>
   55748:	cmn	w0, #0x1
   5574c:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   55750:	ldp	q0, q1, [x29, #-48]
   55754:	add	w19, w0, w19
   55758:	b	55b94 <__gmp_doprnt@@Base+0x7b4>
   5575c:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55760:	mov	w8, #0xfffffff6            	// #-10
   55764:	add	x9, x9, #0x5b7
   55768:	str	w8, [sp, #120]
   5576c:	str	x9, [sp, #128]
   55770:	mov	w8, #0x3                   	// #3
   55774:	str	w8, [sp, #124]
   55778:	str	wzr, [sp, #160]
   5577c:	ldr	w8, [sp, #152]
   55780:	cmp	w8, #0x3
   55784:	b.ne	556b4 <__gmp_doprnt@@Base+0x2d4>  // b.any
   55788:	mov	w8, #0x1                   	// #1
   5578c:	str	w8, [sp, #156]
   55790:	b	557cc <__gmp_doprnt@@Base+0x3ec>
   55794:	mov	w8, #0xfffffff0            	// #-16
   55798:	b	5587c <__gmp_doprnt@@Base+0x49c>
   5579c:	adrp	x8, 65000 <__gmp_oddfac_table@@Base+0xf0>
   557a0:	add	x8, x8, #0x5a9
   557a4:	mov	w9, #0x10                  	// #16
   557a8:	str	x8, [sp, #128]
   557ac:	mov	w10, #0x2                   	// #2
   557b0:	mov	w8, #0x1                   	// #1
   557b4:	stp	w9, w10, [sp, #120]
   557b8:	str	w8, [sp, #136]
   557bc:	cbnz	w20, 557c8 <__gmp_doprnt@@Base+0x3e8>
   557c0:	mov	w9, #0xffffffff            	// #-1
   557c4:	str	w9, [sp, #148]
   557c8:	str	w8, [sp, #152]
   557cc:	mov	w8, #0x1                   	// #1
   557d0:	str	w8, [sp, #160]
   557d4:	and	w8, w23, #0xff
   557d8:	cmp	w8, #0x4c
   557dc:	b.ne	556c0 <__gmp_doprnt@@Base+0x2e0>  // b.any
   557e0:	ldur	w8, [x29, #-20]
   557e4:	tbz	w8, #31, 557f8 <__gmp_doprnt@@Base+0x418>
   557e8:	add	w9, w8, #0x10
   557ec:	cmn	w8, #0xf
   557f0:	stur	w9, [x29, #-20]
   557f4:	b.lt	554b4 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   557f8:	ldur	x8, [x29, #-48]
   557fc:	add	x8, x8, #0xf
   55800:	and	x8, x8, #0xfffffffffffffff0
   55804:	add	x8, x8, #0x10
   55808:	b	554b0 <__gmp_doprnt@@Base+0xd0>
   5580c:	mov	w8, #0x1                   	// #1
   55810:	str	w8, [sp, #124]
   55814:	ldr	w8, [sp, #152]
   55818:	cmp	w8, #0x3
   5581c:	b.ne	556b4 <__gmp_doprnt@@Base+0x2d4>  // b.any
   55820:	b	55788 <__gmp_doprnt@@Base+0x3a8>
   55824:	ldr	x1, [sp, #40]
   55828:	cmp	x26, x1
   5582c:	b.eq	55b20 <__gmp_doprnt@@Base+0x740>  // b.none
   55830:	strb	wzr, [x26]
   55834:	ldp	x0, x8, [sp, #16]
   55838:	ldp	q0, q1, [x29, #-112]
   5583c:	add	x2, sp, #0x40
   55840:	ldr	x8, [x8]
   55844:	stp	q0, q1, [sp, #64]
   55848:	blr	x8
   5584c:	ldr	x19, [sp, #32]
   55850:	cmn	w0, #0x1
   55854:	csel	w8, wzr, w0, eq  // eq = none
   55858:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   5585c:	add	w19, w8, w19
   55860:	ldur	w9, [x29, #-24]
   55864:	mov	w8, w9
   55868:	tbz	w9, #31, 55b50 <__gmp_doprnt@@Base+0x770>
   5586c:	b	55b30 <__gmp_doprnt@@Base+0x750>
   55870:	mov	w8, #0x8                   	// #8
   55874:	b	5587c <__gmp_doprnt@@Base+0x49c>
   55878:	mov	w8, #0x10                  	// #16
   5587c:	str	w8, [sp, #120]
   55880:	cbnz	w20, 5588c <__gmp_doprnt@@Base+0x4ac>
   55884:	mov	w8, #0xffffffff            	// #-1
   55888:	str	w8, [sp, #148]
   5588c:	and	w8, w23, #0xff
   55890:	sub	w8, w8, #0x4c
   55894:	cmp	w8, #0x2e
   55898:	b.hi	558b4 <__gmp_doprnt@@Base+0x4d4>  // b.pmore
   5589c:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   558a0:	add	x9, x9, #0x571
   558a4:	adr	x10, 558b4 <__gmp_doprnt@@Base+0x4d4>
   558a8:	ldrb	w11, [x9, x8]
   558ac:	add	x10, x10, x11, lsl #2
   558b0:	br	x10
   558b4:	ldur	w8, [x29, #-24]
   558b8:	tbz	w8, #31, 554a8 <__gmp_doprnt@@Base+0xc8>
   558bc:	add	w9, w8, #0x8
   558c0:	cmn	w8, #0x7
   558c4:	stur	w9, [x29, #-24]
   558c8:	b.lt	554b4 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   558cc:	b	554a8 <__gmp_doprnt@@Base+0xc8>
   558d0:	add	w9, w8, #0x10
   558d4:	cmn	w8, #0xf
   558d8:	stur	w9, [x29, #-20]
   558dc:	b.lt	554b4 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   558e0:	b	554a8 <__gmp_doprnt@@Base+0xc8>
   558e4:	ldp	x23, x1, [sp, #32]
   558e8:	cmp	x26, x1
   558ec:	b.eq	5591c <__gmp_doprnt@@Base+0x53c>  // b.none
   558f0:	strb	wzr, [x26]
   558f4:	ldp	x0, x8, [sp, #16]
   558f8:	ldp	q0, q1, [x29, #-112]
   558fc:	add	x2, sp, #0x40
   55900:	ldr	x8, [x8]
   55904:	stp	q0, q1, [sp, #64]
   55908:	blr	x8
   5590c:	cmn	w0, #0x1
   55910:	csel	w8, wzr, w0, eq  // eq = none
   55914:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   55918:	add	w23, w8, w23
   5591c:	ldur	w9, [x29, #-24]
   55920:	mov	w8, w9
   55924:	tbz	w9, #31, 55a0c <__gmp_doprnt@@Base+0x62c>
   55928:	add	w8, w9, #0x8
   5592c:	cmn	w9, #0x8
   55930:	stur	w8, [x29, #-24]
   55934:	b.gt	55a0c <__gmp_doprnt@@Base+0x62c>
   55938:	ldur	x10, [x29, #-40]
   5593c:	sxtw	x9, w9
   55940:	add	x9, x10, x9
   55944:	ldr	x9, [x9]
   55948:	str	x9, [sp, #112]
   5594c:	tbz	w8, #31, 55a70 <__gmp_doprnt@@Base+0x690>
   55950:	b	55a24 <__gmp_doprnt@@Base+0x644>
   55954:	ldp	x23, x1, [sp, #32]
   55958:	cmp	x26, x1
   5595c:	b.eq	55988 <__gmp_doprnt@@Base+0x5a8>  // b.none
   55960:	strb	wzr, [x26]
   55964:	ldp	x0, x8, [sp, #16]
   55968:	ldp	q0, q1, [x29, #-112]
   5596c:	add	x2, sp, #0x40
   55970:	ldr	x8, [x8]
   55974:	stp	q0, q1, [sp, #64]
   55978:	blr	x8
   5597c:	cmn	w0, #0x1
   55980:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   55984:	add	w23, w0, w23
   55988:	ldursw	x8, [x29, #-24]
   5598c:	ldr	w1, [sp, #120]
   55990:	tbz	w8, #31, 55a40 <__gmp_doprnt@@Base+0x660>
   55994:	add	w9, w8, #0x8
   55998:	cmn	w8, #0x8
   5599c:	stur	w9, [x29, #-24]
   559a0:	b.gt	55a40 <__gmp_doprnt@@Base+0x660>
   559a4:	ldur	x9, [x29, #-40]
   559a8:	add	x8, x9, x8
   559ac:	b	55a4c <__gmp_doprnt@@Base+0x66c>
   559b0:	ldp	x23, x1, [sp, #32]
   559b4:	cmp	x26, x1
   559b8:	b.eq	559e4 <__gmp_doprnt@@Base+0x604>  // b.none
   559bc:	strb	wzr, [x26]
   559c0:	ldp	x0, x8, [sp, #16]
   559c4:	ldp	q0, q1, [x29, #-112]
   559c8:	add	x2, sp, #0x40
   559cc:	ldr	x8, [x8]
   559d0:	stp	q0, q1, [sp, #64]
   559d4:	blr	x8
   559d8:	cmn	w0, #0x1
   559dc:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   559e0:	add	w23, w0, w23
   559e4:	ldursw	x8, [x29, #-24]
   559e8:	ldr	w1, [sp, #120]
   559ec:	tbz	w8, #31, 55a5c <__gmp_doprnt@@Base+0x67c>
   559f0:	add	w9, w8, #0x8
   559f4:	cmn	w8, #0x8
   559f8:	stur	w9, [x29, #-24]
   559fc:	b.gt	55a5c <__gmp_doprnt@@Base+0x67c>
   55a00:	ldur	x9, [x29, #-40]
   55a04:	add	x8, x9, x8
   55a08:	b	55a68 <__gmp_doprnt@@Base+0x688>
   55a0c:	ldur	x9, [x29, #-48]
   55a10:	add	x10, x9, #0x8
   55a14:	stur	x10, [x29, #-48]
   55a18:	ldr	x9, [x9]
   55a1c:	str	x9, [sp, #112]
   55a20:	tbz	w8, #31, 55a70 <__gmp_doprnt@@Base+0x690>
   55a24:	add	w10, w8, #0x8
   55a28:	cmn	w8, #0x8
   55a2c:	stur	w10, [x29, #-24]
   55a30:	b.gt	55a70 <__gmp_doprnt@@Base+0x690>
   55a34:	ldur	x10, [x29, #-40]
   55a38:	add	x8, x10, w8, sxtw
   55a3c:	b	55a7c <__gmp_doprnt@@Base+0x69c>
   55a40:	ldur	x8, [x29, #-48]
   55a44:	add	x9, x8, #0x8
   55a48:	stur	x9, [x29, #-48]
   55a4c:	ldr	x2, [x8]
   55a50:	mov	x0, xzr
   55a54:	bl	c130 <__gmpq_get_str@plt>
   55a58:	b	55ad0 <__gmp_doprnt@@Base+0x6f0>
   55a5c:	ldur	x8, [x29, #-48]
   55a60:	add	x9, x8, #0x8
   55a64:	stur	x9, [x29, #-48]
   55a68:	ldr	x2, [x8]
   55a6c:	b	55ac8 <__gmp_doprnt@@Base+0x6e8>
   55a70:	ldur	x8, [x29, #-48]
   55a74:	add	x10, x8, #0x8
   55a78:	stur	x10, [x29, #-48]
   55a7c:	ldr	x10, [x8]
   55a80:	mov	x11, #0xffffffff00000000    	// #-4294967296
   55a84:	lsl	x8, x10, #32
   55a88:	sxtw	x10, w10
   55a8c:	cmp	x8, x11
   55a90:	cneg	x10, x10, le
   55a94:	sub	x11, x9, #0x8
   55a98:	mov	x9, x10
   55a9c:	subs	x10, x10, #0x1
   55aa0:	b.lt	55aac <__gmp_doprnt@@Base+0x6cc>  // b.tstop
   55aa4:	ldr	x12, [x11, x9, lsl #3]
   55aa8:	cbz	x12, 55a98 <__gmp_doprnt@@Base+0x6b8>
   55aac:	mov	x11, #0xffffffff00000000    	// #-4294967296
   55ab0:	ldr	w1, [sp, #120]
   55ab4:	neg	w10, w9
   55ab8:	cmp	x8, x11
   55abc:	csel	x8, x9, x10, gt
   55ac0:	str	w8, [sp, #108]
   55ac4:	add	x2, sp, #0x68
   55ac8:	mov	x0, xzr
   55acc:	bl	c3a0 <__gmpz_get_str@plt>
   55ad0:	mov	x19, x0
   55ad4:	ldp	x1, x0, [sp, #16]
   55ad8:	add	x2, sp, #0x78
   55adc:	mov	x3, x19
   55ae0:	bl	cd10 <__gmp_doprnt_integer@plt>
   55ae4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55ae8:	ldr	x8, [x8, #4016]
   55aec:	mov	w26, w0
   55af0:	mov	x0, x19
   55af4:	ldr	x20, [x8]
   55af8:	bl	bf60 <strlen@plt>
   55afc:	add	x1, x0, #0x1
   55b00:	mov	x0, x19
   55b04:	blr	x20
   55b08:	cmn	w26, #0x1
   55b0c:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   55b10:	ldp	q0, q1, [x29, #-48]
   55b14:	add	w23, w26, w23
   55b18:	str	x23, [sp, #32]
   55b1c:	b	55b98 <__gmp_doprnt@@Base+0x7b8>
   55b20:	ldr	x19, [sp, #32]
   55b24:	ldur	w9, [x29, #-24]
   55b28:	mov	w8, w9
   55b2c:	tbz	w9, #31, 55b50 <__gmp_doprnt@@Base+0x770>
   55b30:	add	w8, w9, #0x8
   55b34:	cmn	w9, #0x8
   55b38:	stur	w8, [x29, #-24]
   55b3c:	b.gt	55b50 <__gmp_doprnt@@Base+0x770>
   55b40:	ldur	x10, [x29, #-40]
   55b44:	sxtw	x9, w9
   55b48:	add	x9, x10, x9
   55b4c:	b	55b5c <__gmp_doprnt@@Base+0x77c>
   55b50:	ldur	x9, [x29, #-48]
   55b54:	add	x10, x9, #0x8
   55b58:	stur	x10, [x29, #-48]
   55b5c:	and	w10, w23, #0xff
   55b60:	cmp	w10, #0x7a
   55b64:	b.hi	55b90 <__gmp_doprnt@@Base+0x7b0>  // b.pmore
   55b68:	ldr	x0, [x9]
   55b6c:	adrp	x10, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55b70:	and	x9, x23, #0xff
   55b74:	add	x10, x10, #0x4f6
   55b78:	adr	x11, 55b88 <__gmp_doprnt@@Base+0x7a8>
   55b7c:	ldrb	w12, [x10, x9]
   55b80:	add	x11, x11, x12, lsl #2
   55b84:	br	x11
   55b88:	sxtw	x8, w19
   55b8c:	str	x8, [x0]
   55b90:	ldp	q0, q1, [x29, #-48]
   55b94:	str	x19, [sp, #32]
   55b98:	stp	q0, q1, [x29, #-112]
   55b9c:	str	x25, [sp, #40]
   55ba0:	b	554b4 <__gmp_doprnt@@Base+0xd4>
   55ba4:	str	w19, [x0]
   55ba8:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55bac:	sxtw	x1, w19
   55bb0:	bl	c620 <__gmpf_set_si@plt>
   55bb4:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55bb8:	strb	w19, [x0]
   55bbc:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55bc0:	tbz	w8, #31, 55c04 <__gmp_doprnt@@Base+0x824>
   55bc4:	add	w9, w8, #0x8
   55bc8:	cmn	w8, #0x8
   55bcc:	stur	w9, [x29, #-24]
   55bd0:	b.gt	55c04 <__gmp_doprnt@@Base+0x824>
   55bd4:	ldur	x9, [x29, #-40]
   55bd8:	add	x8, x9, w8, sxtw
   55bdc:	b	55c10 <__gmp_doprnt@@Base+0x830>
   55be0:	sxtw	x1, w19
   55be4:	mov	w2, #0x1                   	// #1
   55be8:	bl	cb70 <__gmpq_set_si@plt>
   55bec:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55bf0:	sxtw	x1, w19
   55bf4:	bl	d270 <__gmpz_set_si@plt>
   55bf8:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55bfc:	strh	w19, [x0]
   55c00:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55c04:	ldur	x8, [x29, #-48]
   55c08:	add	x9, x8, #0x8
   55c0c:	stur	x9, [x29, #-48]
   55c10:	ldr	x8, [x8]
   55c14:	cbz	x8, 55b90 <__gmp_doprnt@@Base+0x7b0>
   55c18:	cmp	x8, #0x0
   55c1c:	cneg	x8, x8, mi  // mi = first
   55c20:	sxtw	x9, w19
   55c24:	cmp	x8, #0x1
   55c28:	str	x9, [x0]
   55c2c:	b.eq	55b90 <__gmp_doprnt@@Base+0x7b0>  // b.none
   55c30:	lsl	x8, x8, #3
   55c34:	add	x0, x0, #0x8
   55c38:	sub	x2, x8, #0x8
   55c3c:	mov	w1, wzr
   55c40:	bl	c5f0 <memset@plt>
   55c44:	b	55b90 <__gmp_doprnt@@Base+0x7b0>
   55c48:	stp	xzr, x20, [sp, #32]
   55c4c:	ldr	x1, [sp, #40]
   55c50:	ldrb	w8, [x1]
   55c54:	cbz	w8, 55c90 <__gmp_doprnt@@Base+0x8b0>
   55c58:	ldp	x0, x8, [sp, #16]
   55c5c:	ldp	q0, q1, [x29, #-112]
   55c60:	add	x2, sp, #0x40
   55c64:	ldr	x8, [x8]
   55c68:	stp	q0, q1, [sp, #64]
   55c6c:	blr	x8
   55c70:	ldr	x19, [sp, #32]
   55c74:	cmn	w0, #0x1
   55c78:	b.eq	55cac <__gmp_doprnt@@Base+0x8cc>  // b.none
   55c7c:	add	w19, w0, w19
   55c80:	ldr	x8, [sp, #24]
   55c84:	ldr	x8, [x8, #24]
   55c88:	cbnz	x8, 55c9c <__gmp_doprnt@@Base+0x8bc>
   55c8c:	b	55cb0 <__gmp_doprnt@@Base+0x8d0>
   55c90:	ldp	x8, x19, [sp, #24]
   55c94:	ldr	x8, [x8, #24]
   55c98:	cbz	x8, 55cb0 <__gmp_doprnt@@Base+0x8d0>
   55c9c:	ldr	x0, [sp, #16]
   55ca0:	blr	x8
   55ca4:	cmn	w0, #0x1
   55ca8:	b.ne	55cb0 <__gmp_doprnt@@Base+0x8d0>  // b.any
   55cac:	mov	w19, #0xffffffff            	// #-1
   55cb0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55cb4:	ldr	x8, [x8, #4016]
   55cb8:	ldp	x0, x1, [sp]
   55cbc:	ldr	x8, [x8]
   55cc0:	blr	x8
   55cc4:	mov	w0, w19
   55cc8:	ldp	x20, x19, [sp, #384]
   55ccc:	ldp	x22, x21, [sp, #368]
   55cd0:	ldp	x24, x23, [sp, #352]
   55cd4:	ldp	x26, x25, [sp, #336]
   55cd8:	ldp	x28, x27, [sp, #320]
   55cdc:	ldp	x29, x30, [sp, #304]
   55ce0:	ldr	d8, [sp, #288]
   55ce4:	add	sp, sp, #0x190
   55ce8:	ret

0000000000055cec <__gmp_doprnt_mpf2@@Base>:
   55cec:	sub	sp, sp, #0x110
   55cf0:	stp	x29, x30, [sp, #176]
   55cf4:	stp	x28, x27, [sp, #192]
   55cf8:	stp	x26, x25, [sp, #208]
   55cfc:	stp	x24, x23, [sp, #224]
   55d00:	stp	x22, x21, [sp, #240]
   55d04:	stp	x20, x19, [sp, #256]
   55d08:	stp	x0, x1, [sp, #64]
   55d0c:	ldr	w19, [x2, #28]
   55d10:	ldr	w8, [x2, #4]
   55d14:	mov	x24, x3
   55d18:	mov	x20, x2
   55d1c:	add	x29, sp, #0xb0
   55d20:	tbnz	w19, #31, 55d78 <__gmp_doprnt_mpf2@@Base+0x8c>
   55d24:	cmp	w8, #0x2
   55d28:	b.eq	55dbc <__gmp_doprnt_mpf2@@Base+0xd0>  // b.none
   55d2c:	cmp	w8, #0x1
   55d30:	b.ne	55dc4 <__gmp_doprnt_mpf2@@Base+0xd8>  // b.any
   55d34:	ldr	w8, [x20]
   55d38:	mov	w10, #0x28                  	// #40
   55d3c:	ldr	x9, [x4, #8]
   55d40:	cmp	w8, #0x0
   55d44:	cneg	w8, w8, mi  // mi = first
   55d48:	umull	x8, w8, w10
   55d4c:	adrp	x10, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55d50:	ldr	x10, [x10, #3936]
   55d54:	ldr	w8, [x10, x8]
   55d58:	lsr	x10, x9, #63
   55d5c:	eor	w10, w10, #0x1
   55d60:	add	w8, w10, w8
   55d64:	madd	w8, w8, w9, w19
   55d68:	add	w8, w8, #0x3
   55d6c:	cmp	w8, #0x1
   55d70:	csinc	w8, w8, wzr, gt
   55d74:	b	55dd4 <__gmp_doprnt_mpf2@@Base+0xe8>
   55d78:	cmp	w8, #0x3
   55d7c:	b.ne	55dd0 <__gmp_doprnt_mpf2@@Base+0xe4>  // b.any
   55d80:	ldr	w11, [x20]
   55d84:	adrp	x12, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   55d88:	ldrsw	x9, [x4]
   55d8c:	ldr	x12, [x12, #3936]
   55d90:	mov	w10, #0x28                  	// #40
   55d94:	cmp	w11, #0x0
   55d98:	mov	w8, wzr
   55d9c:	madd	x9, x9, x10, x12
   55da0:	cneg	w10, w11, mi  // mi = first
   55da4:	ldr	x9, [x9, #8]
   55da8:	sub	w10, w10, #0x1
   55dac:	sbfiz	x10, x10, #6, #32
   55db0:	umulh	x9, x9, x10
   55db4:	add	w19, w9, #0x2
   55db8:	b	55dd4 <__gmp_doprnt_mpf2@@Base+0xe8>
   55dbc:	add	w8, w19, #0x1
   55dc0:	b	55dd4 <__gmp_doprnt_mpf2@@Base+0xe8>
   55dc4:	cmp	w19, #0x1
   55dc8:	csinc	w8, w19, wzr, gt
   55dcc:	b	55dd4 <__gmp_doprnt_mpf2@@Base+0xe8>
   55dd0:	mov	w8, wzr
   55dd4:	ldr	w2, [x20]
   55dd8:	mov	w3, w8
   55ddc:	sub	x1, x29, #0x10
   55de0:	mov	x0, xzr
   55de4:	bl	ce20 <__gmpf_get_str@plt>
   55de8:	mov	x21, x0
   55dec:	bl	bf60 <strlen@plt>
   55df0:	mov	x10, x21
   55df4:	ldrb	w9, [x20, #44]
   55df8:	ldrb	w11, [x10], #1
   55dfc:	ldr	w8, [x20, #4]
   55e00:	stp	x0, x21, [sp, #48]
   55e04:	cmp	w11, #0x2d
   55e08:	cset	w23, eq  // eq = none
   55e0c:	csel	x10, x10, x21, eq  // eq = none
   55e10:	csel	w21, w11, w9, eq  // eq = none
   55e14:	cmp	w8, #0x2
   55e18:	sub	w28, w0, w23
   55e1c:	str	x10, [sp, #40]
   55e20:	b.eq	55e40 <__gmp_doprnt_mpf2@@Base+0x154>  // b.none
   55e24:	cmp	w8, #0x1
   55e28:	b.ne	55e54 <__gmp_doprnt_mpf2@@Base+0x168>  // b.any
   55e2c:	tbnz	w19, #31, 55ed4 <__gmp_doprnt_mpf2@@Base+0x1e8>
   55e30:	ldur	x22, [x29, #-16]
   55e34:	adds	w26, w19, w22
   55e38:	b.pl	55ef0 <__gmp_doprnt_mpf2@@Base+0x204>  // b.nfrst
   55e3c:	b	5607c <__gmp_doprnt_mpf2@@Base+0x390>
   55e40:	tbz	w19, #31, 55e70 <__gmp_doprnt_mpf2@@Base+0x184>
   55e44:	sub	w8, w28, #0x1
   55e48:	cmp	w28, #0x0
   55e4c:	csel	w19, w8, wzr, gt
   55e50:	b	55e70 <__gmp_doprnt_mpf2@@Base+0x184>
   55e54:	ldur	x22, [x29, #-16]
   55e58:	cmn	x22, #0x3
   55e5c:	b.lt	55e70 <__gmp_doprnt_mpf2@@Base+0x184>  // b.tstop
   55e60:	cmp	w19, #0x1
   55e64:	csinc	w8, w19, wzr, gt
   55e68:	cmp	x22, x8
   55e6c:	b.le	56018 <__gmp_doprnt_mpf2@@Base+0x32c>
   55e70:	ldur	x8, [x29, #-16]
   55e74:	ldr	w9, [x20, #16]
   55e78:	cmp	w28, #0x1
   55e7c:	csinc	w27, w28, wzr, lt  // lt = tstop
   55e80:	cmp	w27, #0x0
   55e84:	sub	x8, x8, w27, sxtw
   55e88:	ldr	x2, [x20, #8]
   55e8c:	cset	w25, eq  // eq = none
   55e90:	cmp	w9, #0x0
   55e94:	lsl	x9, x8, #2
   55e98:	csel	x8, x8, x9, eq  // eq = none
   55e9c:	mov	w10, #0x2d                  	// #45
   55ea0:	mov	w9, #0x2b                  	// #43
   55ea4:	cmp	x8, #0x0
   55ea8:	cneg	x4, x8, mi  // mi = first
   55eac:	csel	w3, w9, w10, ge  // ge = tcont
   55eb0:	add	x0, sp, #0x54
   55eb4:	mov	w1, #0x4a                  	// #74
   55eb8:	sub	w28, w28, w27
   55ebc:	bl	c2f0 <snprintf@plt>
   55ec0:	mov	w23, w0
   55ec4:	mov	w22, wzr
   55ec8:	ldr	w8, [x20, #40]
   55ecc:	cbnz	w8, 560a0 <__gmp_doprnt_mpf2@@Base+0x3b4>
   55ed0:	b	56044 <__gmp_doprnt_mpf2@@Base+0x358>
   55ed4:	ldur	x22, [x29, #-16]
   55ed8:	sxtw	x8, w28
   55edc:	sub	x8, x8, x22
   55ee0:	cmp	x8, #0x0
   55ee4:	csel	w19, w8, wzr, gt
   55ee8:	adds	w26, w19, w22
   55eec:	b.mi	5607c <__gmp_doprnt_mpf2@@Base+0x390>  // b.first
   55ef0:	cmp	w28, w26
   55ef4:	b.le	56018 <__gmp_doprnt_mpf2@@Base+0x32c>
   55ef8:	ldr	w8, [x20]
   55efc:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   55f00:	adrp	x10, 59000 <__gmp_randget_mt@@Base+0x44c>
   55f04:	add	x9, x9, #0x5c0
   55f08:	add	x10, x10, #0xc7d
   55f0c:	cmp	w8, #0x0
   55f10:	mov	x27, x24
   55f14:	csel	x24, x10, x9, ge  // ge = tcont
   55f18:	cneg	w25, w8, mi  // mi = first
   55f1c:	bl	cae0 <__ctype_b_loc@plt>
   55f20:	ldr	x8, [sp, #40]
   55f24:	ldr	x9, [x0]
   55f28:	ldrb	w8, [x8, w26, uxtw]
   55f2c:	ldrh	w10, [x9, x8, lsl #1]
   55f30:	tbnz	w10, #11, 55fb0 <__gmp_doprnt_mpf2@@Base+0x2c4>
   55f34:	tst	w10, #0x200
   55f38:	mov	w10, #0xffffffa9            	// #-87
   55f3c:	mov	w11, #0xffffffc9            	// #-55
   55f40:	csel	w10, w11, w10, eq  // eq = none
   55f44:	add	w8, w10, w8
   55f48:	add	w10, w25, #0x1
   55f4c:	cmp	w8, w10, lsr #1
   55f50:	mov	w8, w26
   55f54:	b.lt	55fc4 <__gmp_doprnt_mpf2@@Base+0x2d8>  // b.tstop
   55f58:	ldr	x11, [sp, #40]
   55f5c:	mov	x10, xzr
   55f60:	mov	w13, #0xffffffc9            	// #-55
   55f64:	add	x12, x8, x11
   55f68:	mov	w11, #0xffffffa9            	// #-87
   55f6c:	sub	x12, x12, #0x1
   55f70:	b	55f94 <__gmp_doprnt_mpf2@@Base+0x2a8>
   55f74:	tst	w15, #0x200
   55f78:	csel	w15, w13, w11, eq  // eq = none
   55f7c:	add	w14, w15, w14
   55f80:	sxtw	x14, w14
   55f84:	add	x14, x14, #0x1
   55f88:	cmp	w14, w25
   55f8c:	sub	x10, x10, #0x1
   55f90:	b.ne	56058 <__gmp_doprnt_mpf2@@Base+0x36c>  // b.any
   55f94:	cmn	x8, x10
   55f98:	b.eq	55ff8 <__gmp_doprnt_mpf2@@Base+0x30c>  // b.none
   55f9c:	ldrb	w14, [x12, x10]
   55fa0:	ldrh	w15, [x9, x14, lsl #1]
   55fa4:	tbz	w15, #11, 55f74 <__gmp_doprnt_mpf2@@Base+0x288>
   55fa8:	sub	w14, w14, #0x30
   55fac:	b	55f80 <__gmp_doprnt_mpf2@@Base+0x294>
   55fb0:	sub	w8, w8, #0x30
   55fb4:	add	w10, w25, #0x1
   55fb8:	cmp	w8, w10, lsr #1
   55fbc:	mov	w8, w26
   55fc0:	b.ge	55f58 <__gmp_doprnt_mpf2@@Base+0x26c>  // b.tcont
   55fc4:	ldr	x9, [sp, #40]
   55fc8:	mov	x24, x27
   55fcc:	sub	x9, x9, #0x1
   55fd0:	subs	x10, x8, #0x1
   55fd4:	b.lt	5607c <__gmp_doprnt_mpf2@@Base+0x390>  // b.tstop
   55fd8:	ldrb	w8, [x9, x8]
   55fdc:	cmp	w8, #0x30
   55fe0:	mov	x8, x10
   55fe4:	b.eq	55fd0 <__gmp_doprnt_mpf2@@Base+0x2e4>  // b.none
   55fe8:	add	w28, w10, #0x1
   55fec:	cmp	x22, #0x0
   55ff0:	b.gt	56020 <__gmp_doprnt_mpf2@@Base+0x334>
   55ff4:	b	56088 <__gmp_doprnt_mpf2@@Base+0x39c>
   55ff8:	ldr	x9, [sp, #56]
   55ffc:	mov	w8, #0x31                  	// #49
   56000:	mov	w28, #0x1                   	// #1
   56004:	mov	x24, x27
   56008:	strb	w8, [x9, x23]
   5600c:	ldur	x8, [x29, #-16]
   56010:	add	x22, x8, #0x1
   56014:	stur	x22, [x29, #-16]
   56018:	cmp	x22, #0x0
   5601c:	b.le	56088 <__gmp_doprnt_mpf2@@Base+0x39c>
   56020:	cmp	x22, w28, sxtw
   56024:	mov	w8, wzr
   56028:	csel	w27, w28, w22, gt
   5602c:	mov	w23, wzr
   56030:	sub	w25, w22, w27
   56034:	mov	w22, w8
   56038:	sub	w28, w28, w27
   5603c:	ldr	w8, [x20, #40]
   56040:	cbnz	w8, 560a0 <__gmp_doprnt_mpf2@@Base+0x3b4>
   56044:	mov	w10, wzr
   56048:	add	w8, w22, w28
   5604c:	cmn	w8, w10
   56050:	b.eq	560c8 <__gmp_doprnt_mpf2@@Base+0x3dc>  // b.none
   56054:	b	560d0 <__gmp_doprnt_mpf2@@Base+0x3e4>
   56058:	ldr	x9, [sp, #40]
   5605c:	ldrb	w11, [x24, x14]
   56060:	add	w12, w19, w22
   56064:	mvn	w12, w12
   56068:	add	x9, x8, x9
   5606c:	cmp	w12, w10
   56070:	mov	x24, x27
   56074:	strb	w11, [x9, x10]
   56078:	b.ne	563c4 <__gmp_doprnt_mpf2@@Base+0x6d8>  // b.any
   5607c:	mov	w28, wzr
   56080:	mov	x22, xzr
   56084:	stur	xzr, [x29, #-16]
   56088:	mov	w27, wzr
   5608c:	mov	w23, wzr
   56090:	neg	w22, w22
   56094:	mov	w25, #0x1                   	// #1
   56098:	ldr	w8, [x20, #40]
   5609c:	cbz	w8, 56044 <__gmp_doprnt_mpf2@@Base+0x358>
   560a0:	ldr	w9, [x20, #4]
   560a4:	add	w10, w27, w25
   560a8:	add	w8, w22, w28
   560ac:	cmp	w9, #0x3
   560b0:	csneg	w9, wzr, w10, ne  // ne = any
   560b4:	sub	w10, w19, w8
   560b8:	add	w9, w10, w9
   560bc:	bic	w10, w9, w9, asr #31
   560c0:	cmn	w8, w10
   560c4:	b.ne	560d0 <__gmp_doprnt_mpf2@@Base+0x3e4>  // b.any
   560c8:	ldr	w8, [x20, #36]
   560cc:	cbz	w8, 5639c <__gmp_doprnt_mpf2@@Base+0x6b0>
   560d0:	mov	x0, x24
   560d4:	mov	w19, w10
   560d8:	bl	bf60 <strlen@plt>
   560dc:	mov	w10, w19
   560e0:	ldr	w8, [x20, #32]
   560e4:	and	w19, w21, #0xff
   560e8:	str	x24, [sp, #16]
   560ec:	cmp	w8, #0x1
   560f0:	b.eq	56104 <__gmp_doprnt_mpf2@@Base+0x418>  // b.none
   560f4:	cmp	w8, #0x3
   560f8:	b.ne	56130 <__gmp_doprnt_mpf2@@Base+0x444>  // b.any
   560fc:	orr	w8, w27, w28
   56100:	cbz	w8, 56130 <__gmp_doprnt_mpf2@@Base+0x444>
   56104:	ldr	w8, [x20]
   56108:	cmn	w8, #0x10
   5610c:	b.eq	563a4 <__gmp_doprnt_mpf2@@Base+0x6b8>  // b.none
   56110:	cmp	w8, #0x8
   56114:	b.eq	563b4 <__gmp_doprnt_mpf2@@Base+0x6c8>  // b.none
   56118:	cmp	w8, #0x10
   5611c:	b.ne	56130 <__gmp_doprnt_mpf2@@Base+0x444>  // b.any
   56120:	adrp	x26, 65000 <__gmp_oddfac_table@@Base+0xf0>
   56124:	mov	w21, #0x2                   	// #2
   56128:	add	x26, x26, #0x5e5
   5612c:	b	56138 <__gmp_doprnt_mpf2@@Base+0x44c>
   56130:	mov	x26, xzr
   56134:	mov	w21, wzr
   56138:	cmp	w19, #0x0
   5613c:	csetm	w9, ne  // ne = any
   56140:	sub	w9, w9, w28
   56144:	sub	w9, w9, w22
   56148:	sub	w9, w9, w25
   5614c:	sub	w9, w9, w27
   56150:	ldr	w8, [x20, #48]
   56154:	sub	w9, w9, w23
   56158:	sub	w9, w9, w10
   5615c:	str	w10, [sp, #12]
   56160:	sub	w9, w9, w0
   56164:	ldr	w10, [x20, #24]
   56168:	sub	w9, w9, w21
   5616c:	add	w24, w9, w8
   56170:	cmp	w24, #0x0
   56174:	str	x23, [sp]
   56178:	csel	w23, w10, wzr, gt
   5617c:	str	w22, [sp, #28]
   56180:	cmp	w23, #0x2
   56184:	mov	w22, wzr
   56188:	str	x0, [sp, #32]
   5618c:	b.ne	561b0 <__gmp_doprnt_mpf2@@Base+0x4c4>  // b.any
   56190:	ldp	x8, x0, [sp, #64]
   56194:	ldrb	w1, [x20, #20]
   56198:	mov	w2, w24
   5619c:	ldr	x8, [x8, #16]
   561a0:	blr	x8
   561a4:	mov	w22, w0
   561a8:	cmn	w0, #0x1
   561ac:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   561b0:	cbz	w19, 561d4 <__gmp_doprnt_mpf2@@Base+0x4e8>
   561b4:	ldp	x8, x0, [sp, #64]
   561b8:	mov	w2, #0x1                   	// #1
   561bc:	mov	w1, w19
   561c0:	ldr	x8, [x8, #16]
   561c4:	blr	x8
   561c8:	cmn	w0, #0x1
   561cc:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   561d0:	add	w22, w0, w22
   561d4:	cbz	w21, 561f8 <__gmp_doprnt_mpf2@@Base+0x50c>
   561d8:	ldp	x8, x0, [sp, #64]
   561dc:	mov	x1, x26
   561e0:	mov	x2, x21
   561e4:	ldr	x8, [x8, #8]
   561e8:	blr	x8
   561ec:	cmn	w0, #0x1
   561f0:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   561f4:	add	w22, w0, w22
   561f8:	cmp	w23, #0x3
   561fc:	b.ne	56220 <__gmp_doprnt_mpf2@@Base+0x534>  // b.any
   56200:	ldp	x8, x0, [sp, #64]
   56204:	ldrb	w1, [x20, #20]
   56208:	mov	w2, w24
   5620c:	ldr	x8, [x8, #16]
   56210:	blr	x8
   56214:	cmn	w0, #0x1
   56218:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   5621c:	add	w22, w0, w22
   56220:	ldp	x8, x0, [sp, #64]
   56224:	ldr	x1, [sp, #40]
   56228:	sxtw	x21, w27
   5622c:	mov	x2, x21
   56230:	ldr	x8, [x8, #8]
   56234:	blr	x8
   56238:	add	w19, w0, w22
   5623c:	cmn	w0, #0x1
   56240:	csel	w22, w22, w19, eq  // eq = none
   56244:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56248:	cbz	w25, 5626c <__gmp_doprnt_mpf2@@Base+0x580>
   5624c:	ldp	x8, x0, [sp, #64]
   56250:	mov	w1, #0x30                  	// #48
   56254:	mov	w2, w25
   56258:	ldr	x8, [x8, #16]
   5625c:	blr	x8
   56260:	cmn	w0, #0x1
   56264:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56268:	add	w19, w0, w22
   5626c:	ldr	x9, [sp, #32]
   56270:	cbz	w9, 56294 <__gmp_doprnt_mpf2@@Base+0x5a8>
   56274:	ldp	x8, x0, [sp, #64]
   56278:	ldr	x1, [sp, #16]
   5627c:	sxtw	x2, w9
   56280:	ldr	x8, [x8, #8]
   56284:	blr	x8
   56288:	cmn	w0, #0x1
   5628c:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56290:	add	w19, w0, w19
   56294:	ldr	w2, [sp, #28]
   56298:	cbz	w2, 562b8 <__gmp_doprnt_mpf2@@Base+0x5cc>
   5629c:	ldp	x8, x0, [sp, #64]
   562a0:	mov	w1, #0x30                  	// #48
   562a4:	ldr	x8, [x8, #16]
   562a8:	blr	x8
   562ac:	cmn	w0, #0x1
   562b0:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   562b4:	add	w19, w0, w19
   562b8:	cbz	w28, 562e0 <__gmp_doprnt_mpf2@@Base+0x5f4>
   562bc:	ldp	x8, x0, [sp, #64]
   562c0:	ldr	x9, [sp, #40]
   562c4:	sxtw	x2, w28
   562c8:	ldr	x8, [x8, #8]
   562cc:	add	x1, x9, x21
   562d0:	blr	x8
   562d4:	cmn	w0, #0x1
   562d8:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   562dc:	add	w19, w0, w19
   562e0:	ldr	w2, [sp, #12]
   562e4:	cbz	w2, 56304 <__gmp_doprnt_mpf2@@Base+0x618>
   562e8:	ldp	x8, x0, [sp, #64]
   562ec:	mov	w1, #0x30                  	// #48
   562f0:	ldr	x8, [x8, #16]
   562f4:	blr	x8
   562f8:	cmn	w0, #0x1
   562fc:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56300:	add	w19, w0, w19
   56304:	ldr	x9, [sp]
   56308:	cbz	w9, 5632c <__gmp_doprnt_mpf2@@Base+0x640>
   5630c:	ldp	x8, x0, [sp, #64]
   56310:	sxtw	x2, w9
   56314:	add	x1, sp, #0x54
   56318:	ldr	x8, [x8, #8]
   5631c:	blr	x8
   56320:	cmn	w0, #0x1
   56324:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56328:	add	w19, w0, w19
   5632c:	cmp	w23, #0x1
   56330:	b.ne	5635c <__gmp_doprnt_mpf2@@Base+0x670>  // b.any
   56334:	ldp	x8, x0, [sp, #64]
   56338:	ldrb	w1, [x20, #20]
   5633c:	mov	w2, w24
   56340:	ldr	x8, [x8, #16]
   56344:	blr	x8
   56348:	cmn	w0, #0x1
   5634c:	b.eq	56358 <__gmp_doprnt_mpf2@@Base+0x66c>  // b.none
   56350:	add	w19, w0, w19
   56354:	b	5635c <__gmp_doprnt_mpf2@@Base+0x670>
   56358:	mov	w19, #0xffffffff            	// #-1
   5635c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56360:	ldp	x9, x0, [sp, #48]
   56364:	ldr	x8, [x8, #4016]
   56368:	add	w9, w9, #0x1
   5636c:	ldr	x8, [x8]
   56370:	sxtw	x1, w9
   56374:	blr	x8
   56378:	mov	w0, w19
   5637c:	ldp	x20, x19, [sp, #256]
   56380:	ldp	x22, x21, [sp, #240]
   56384:	ldp	x24, x23, [sp, #224]
   56388:	ldp	x26, x25, [sp, #208]
   5638c:	ldp	x28, x27, [sp, #192]
   56390:	ldp	x29, x30, [sp, #176]
   56394:	add	sp, sp, #0x110
   56398:	ret
   5639c:	mov	x0, xzr
   563a0:	b	560e0 <__gmp_doprnt_mpf2@@Base+0x3f4>
   563a4:	adrp	x26, 65000 <__gmp_oddfac_table@@Base+0xf0>
   563a8:	mov	w21, #0x2                   	// #2
   563ac:	add	x26, x26, #0x5e8
   563b0:	b	56138 <__gmp_doprnt_mpf2@@Base+0x44c>
   563b4:	adrp	x26, 65000 <__gmp_oddfac_table@@Base+0xf0>
   563b8:	mov	w21, #0x1                   	// #1
   563bc:	add	x26, x26, #0x8a3
   563c0:	b	56138 <__gmp_doprnt_mpf2@@Base+0x44c>
   563c4:	ldur	x22, [x29, #-16]
   563c8:	add	x8, x8, x10
   563cc:	add	x28, x8, #0x1
   563d0:	cmp	x22, #0x0
   563d4:	b.gt	56020 <__gmp_doprnt_mpf2@@Base+0x334>
   563d8:	b	56088 <__gmp_doprnt_mpf2@@Base+0x39c>

00000000000563dc <__gmp_doprnt_integer@@Base>:
   563dc:	sub	sp, sp, #0x90
   563e0:	stp	x29, x30, [sp, #48]
   563e4:	add	x29, sp, #0x30
   563e8:	stp	x28, x27, [sp, #64]
   563ec:	stp	x26, x25, [sp, #80]
   563f0:	stp	x24, x23, [sp, #96]
   563f4:	stp	x22, x21, [sp, #112]
   563f8:	stp	x20, x19, [sp, #128]
   563fc:	stur	x1, [x29, #-8]
   56400:	mov	x9, x3
   56404:	ldrb	w8, [x2, #44]
   56408:	ldrb	w10, [x9], #1
   5640c:	mov	x20, x2
   56410:	stur	x0, [x29, #-16]
   56414:	cmp	w10, #0x2d
   56418:	csel	w19, w10, w8, eq  // eq = none
   5641c:	csel	x8, x3, x9, ne  // ne = any
   56420:	ldrb	w8, [x8]
   56424:	csel	x22, x9, x3, eq  // eq = none
   56428:	tst	w19, #0xff
   5642c:	cset	w23, ne  // ne = any
   56430:	cmp	w8, #0x30
   56434:	b.ne	56444 <__gmp_doprnt_integer@@Base+0x68>  // b.any
   56438:	ldr	w8, [x20, #28]
   5643c:	cmp	w8, #0x0
   56440:	cinc	x22, x22, eq  // eq = none
   56444:	mov	x0, x22
   56448:	bl	bf60 <strlen@plt>
   5644c:	mov	x21, x0
   56450:	mov	w1, #0x2f                  	// #47
   56454:	mov	x0, x22
   56458:	bl	cda0 <strchr@plt>
   5645c:	ldr	w8, [x20, #32]
   56460:	stur	w23, [x29, #-20]
   56464:	cmp	w8, #0x2
   56468:	b.ne	56494 <__gmp_doprnt_integer@@Base+0xb8>  // b.any
   5646c:	stp	x0, xzr, [sp, #8]
   56470:	mov	w27, wzr
   56474:	and	w19, w19, #0xff
   56478:	cmp	w8, #0x3
   5647c:	cbz	x0, 564f4 <__gmp_doprnt_integer@@Base+0x118>
   56480:	b.ne	564c0 <__gmp_doprnt_integer@@Base+0xe4>  // b.any
   56484:	ldrb	w8, [x0, #1]
   56488:	cmp	w8, #0x30
   5648c:	csel	w28, wzr, w27, eq  // eq = none
   56490:	b	564fc <__gmp_doprnt_integer@@Base+0x120>
   56494:	ldr	w9, [x20]
   56498:	cmn	w9, #0x10
   5649c:	b.eq	564c8 <__gmp_doprnt_integer@@Base+0xec>  // b.none
   564a0:	cmp	w9, #0x8
   564a4:	b.eq	564d8 <__gmp_doprnt_integer@@Base+0xfc>  // b.none
   564a8:	cmp	w9, #0x10
   564ac:	b.ne	5646c <__gmp_doprnt_integer@@Base+0x90>  // b.any
   564b0:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   564b4:	mov	w27, #0x2                   	// #2
   564b8:	add	x9, x9, #0x5e5
   564bc:	b	564e4 <__gmp_doprnt_integer@@Base+0x108>
   564c0:	mov	w28, w27
   564c4:	b	56508 <__gmp_doprnt_integer@@Base+0x12c>
   564c8:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   564cc:	mov	w27, #0x2                   	// #2
   564d0:	add	x9, x9, #0x5e8
   564d4:	b	564e4 <__gmp_doprnt_integer@@Base+0x108>
   564d8:	adrp	x9, 65000 <__gmp_oddfac_table@@Base+0xf0>
   564dc:	mov	w27, #0x1                   	// #1
   564e0:	add	x9, x9, #0x8a3
   564e4:	stp	x0, x9, [sp, #8]
   564e8:	and	w19, w19, #0xff
   564ec:	cmp	w8, #0x3
   564f0:	cbnz	x0, 56480 <__gmp_doprnt_integer@@Base+0xa4>
   564f4:	mov	w28, wzr
   564f8:	b.ne	56508 <__gmp_doprnt_integer@@Base+0x12c>  // b.any
   564fc:	ldrb	w8, [x22]
   56500:	cmp	w8, #0x30
   56504:	csel	w27, wzr, w27, eq  // eq = none
   56508:	ldp	w10, w9, [x20, #24]
   5650c:	cmp	w19, #0x0
   56510:	ldr	w8, [x20, #48]
   56514:	cinc	w11, w21, ne  // ne = any
   56518:	add	w11, w28, w11
   5651c:	add	w11, w11, w27
   56520:	sub	w23, w9, w21
   56524:	bic	w24, w23, w23, asr #31
   56528:	sub	w8, w8, w11
   5652c:	sub	w26, w8, w24
   56530:	cmp	w26, #0x0
   56534:	str	x21, [sp]
   56538:	csel	w21, w10, wzr, gt
   5653c:	cmp	w21, #0x2
   56540:	mov	w25, wzr
   56544:	b.ne	56568 <__gmp_doprnt_integer@@Base+0x18c>  // b.any
   56548:	ldp	x8, x0, [x29, #-16]
   5654c:	ldrb	w1, [x20, #20]
   56550:	mov	w2, w26
   56554:	ldr	x8, [x8, #16]
   56558:	blr	x8
   5655c:	mov	w25, w0
   56560:	cmn	w0, #0x1
   56564:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   56568:	cbz	w19, 5658c <__gmp_doprnt_integer@@Base+0x1b0>
   5656c:	ldp	x8, x0, [x29, #-16]
   56570:	ldur	w2, [x29, #-20]
   56574:	mov	w1, w19
   56578:	ldr	x8, [x8, #16]
   5657c:	blr	x8
   56580:	cmn	w0, #0x1
   56584:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   56588:	add	w25, w0, w25
   5658c:	cbz	w27, 565b0 <__gmp_doprnt_integer@@Base+0x1d4>
   56590:	ldp	x8, x0, [x29, #-16]
   56594:	ldr	x1, [sp, #16]
   56598:	mov	x2, x27
   5659c:	ldr	x8, [x8, #8]
   565a0:	blr	x8
   565a4:	cmn	w0, #0x1
   565a8:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   565ac:	add	w25, w0, w25
   565b0:	cmp	w23, #0x1
   565b4:	b.lt	565d8 <__gmp_doprnt_integer@@Base+0x1fc>  // b.tstop
   565b8:	ldp	x8, x0, [x29, #-16]
   565bc:	mov	w1, #0x30                  	// #48
   565c0:	mov	w2, w24
   565c4:	ldr	x8, [x8, #16]
   565c8:	blr	x8
   565cc:	cmn	w0, #0x1
   565d0:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   565d4:	add	w25, w0, w25
   565d8:	cmp	w21, #0x3
   565dc:	b.ne	56600 <__gmp_doprnt_integer@@Base+0x224>  // b.any
   565e0:	ldp	x8, x0, [x29, #-16]
   565e4:	ldrb	w1, [x20, #20]
   565e8:	mov	w2, w26
   565ec:	ldr	x8, [x8, #16]
   565f0:	blr	x8
   565f4:	cmn	w0, #0x1
   565f8:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   565fc:	add	w25, w0, w25
   56600:	cbz	w28, 56668 <__gmp_doprnt_integer@@Base+0x28c>
   56604:	ldp	x8, x0, [x29, #-16]
   56608:	ldr	x9, [sp, #8]
   5660c:	mov	x1, x22
   56610:	ldr	x8, [x8, #8]
   56614:	sub	x9, x9, x22
   56618:	add	x23, x9, #0x1
   5661c:	sxtw	x19, w23
   56620:	mov	x2, x19
   56624:	blr	x8
   56628:	cmn	w0, #0x1
   5662c:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   56630:	mov	w24, w0
   56634:	ldp	x8, x0, [x29, #-16]
   56638:	ldr	x1, [sp, #16]
   5663c:	mov	x2, x28
   56640:	ldr	x8, [x8, #8]
   56644:	blr	x8
   56648:	cmn	w0, #0x1
   5664c:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   56650:	ldr	x9, [sp]
   56654:	add	w8, w24, w25
   56658:	add	x22, x22, x19
   5665c:	add	w25, w8, w0
   56660:	sub	x9, x9, x23
   56664:	b	5666c <__gmp_doprnt_integer@@Base+0x290>
   56668:	ldr	x9, [sp]
   5666c:	ldp	x8, x0, [x29, #-16]
   56670:	sxtw	x2, w9
   56674:	mov	x1, x22
   56678:	ldr	x8, [x8, #8]
   5667c:	blr	x8
   56680:	mov	w8, w0
   56684:	add	w0, w0, w25
   56688:	cmn	w8, #0x1
   5668c:	csel	w19, w25, w0, eq  // eq = none
   56690:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   56694:	cmp	w21, #0x1
   56698:	b.ne	566c4 <__gmp_doprnt_integer@@Base+0x2e8>  // b.any
   5669c:	ldp	x8, x0, [x29, #-16]
   566a0:	ldrb	w1, [x20, #20]
   566a4:	mov	w2, w26
   566a8:	ldr	x8, [x8, #16]
   566ac:	blr	x8
   566b0:	cmn	w0, #0x1
   566b4:	b.eq	566c0 <__gmp_doprnt_integer@@Base+0x2e4>  // b.none
   566b8:	add	w0, w0, w19
   566bc:	b	566c4 <__gmp_doprnt_integer@@Base+0x2e8>
   566c0:	mov	w0, #0xffffffff            	// #-1
   566c4:	ldp	x20, x19, [sp, #128]
   566c8:	ldp	x22, x21, [sp, #112]
   566cc:	ldp	x24, x23, [sp, #96]
   566d0:	ldp	x26, x25, [sp, #80]
   566d4:	ldp	x28, x27, [sp, #64]
   566d8:	ldp	x29, x30, [sp, #48]
   566dc:	add	sp, sp, #0x90
   566e0:	ret

00000000000566e4 <__gmp_fprintf@@Base>:
   566e4:	sub	sp, sp, #0x100
   566e8:	stp	x29, x30, [sp, #240]
   566ec:	add	x29, sp, #0xf0
   566f0:	mov	x9, #0xffffffffffffffd0    	// #-48
   566f4:	mov	x10, sp
   566f8:	sub	x11, x29, #0x70
   566fc:	movk	x9, #0xff80, lsl #32
   56700:	add	x12, x29, #0x10
   56704:	add	x10, x10, #0x80
   56708:	add	x11, x11, #0x30
   5670c:	stp	x10, x9, [x29, #-16]
   56710:	stp	x12, x11, [x29, #-32]
   56714:	stp	x2, x3, [x29, #-112]
   56718:	stp	x4, x5, [x29, #-96]
   5671c:	stp	x6, x7, [x29, #-80]
   56720:	stp	q1, q2, [sp, #16]
   56724:	str	q0, [sp]
   56728:	ldp	q0, q1, [x29, #-32]
   5672c:	mov	x8, x1
   56730:	mov	x1, x0
   56734:	stp	q3, q4, [sp, #48]
   56738:	stp	q5, q6, [sp, #80]
   5673c:	str	q7, [sp, #112]
   56740:	stp	q0, q1, [x29, #-64]
   56744:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56748:	ldr	x0, [x0, #3816]
   5674c:	sub	x3, x29, #0x40
   56750:	mov	x2, x8
   56754:	bl	d030 <__gmp_doprnt@plt>
   56758:	ldp	x29, x30, [sp, #240]
   5675c:	add	sp, sp, #0x100
   56760:	ret

0000000000056764 <__gmp_obstack_printf@@Base>:
   56764:	sub	sp, sp, #0x100
   56768:	stp	x29, x30, [sp, #240]
   5676c:	add	x29, sp, #0xf0
   56770:	mov	x9, #0xffffffffffffffd0    	// #-48
   56774:	mov	x10, sp
   56778:	sub	x11, x29, #0x70
   5677c:	movk	x9, #0xff80, lsl #32
   56780:	add	x12, x29, #0x10
   56784:	add	x10, x10, #0x80
   56788:	add	x11, x11, #0x30
   5678c:	stp	x10, x9, [x29, #-16]
   56790:	stp	x12, x11, [x29, #-32]
   56794:	stp	x2, x3, [x29, #-112]
   56798:	stp	x4, x5, [x29, #-96]
   5679c:	stp	x6, x7, [x29, #-80]
   567a0:	stp	q1, q2, [sp, #16]
   567a4:	str	q0, [sp]
   567a8:	ldp	q0, q1, [x29, #-32]
   567ac:	mov	x8, x1
   567b0:	mov	x1, x0
   567b4:	stp	q3, q4, [sp, #48]
   567b8:	stp	q5, q6, [sp, #80]
   567bc:	str	q7, [sp, #112]
   567c0:	stp	q0, q1, [x29, #-64]
   567c4:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   567c8:	ldr	x0, [x0, #4048]
   567cc:	sub	x3, x29, #0x40
   567d0:	mov	x2, x8
   567d4:	bl	d030 <__gmp_doprnt@plt>
   567d8:	ldp	x29, x30, [sp, #240]
   567dc:	add	sp, sp, #0x100
   567e0:	ret

00000000000567e4 <__gmp_obstack_vprintf@@Base>:
   567e4:	sub	sp, sp, #0x30
   567e8:	stp	x29, x30, [sp, #32]
   567ec:	ldp	q1, q0, [x2]
   567f0:	mov	x8, x1
   567f4:	mov	x1, x0
   567f8:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   567fc:	stp	q1, q0, [sp]
   56800:	ldr	x0, [x0, #4048]
   56804:	mov	x3, sp
   56808:	mov	x2, x8
   5680c:	add	x29, sp, #0x20
   56810:	bl	d030 <__gmp_doprnt@plt>
   56814:	ldp	x29, x30, [sp, #32]
   56818:	add	sp, sp, #0x30
   5681c:	ret
   56820:	stp	x29, x30, [sp, #-48]!
   56824:	stp	x22, x21, [sp, #16]
   56828:	stp	x20, x19, [sp, #32]
   5682c:	mov	x19, x0
   56830:	ldr	x0, [x0, #24]
   56834:	ldr	x8, [x19, #32]
   56838:	sxtw	x21, w2
   5683c:	mov	x20, x2
   56840:	add	x9, x0, x21
   56844:	cmp	x9, x8
   56848:	mov	x22, x1
   5684c:	mov	x29, sp
   56850:	b.ls	56864 <__gmp_obstack_vprintf@@Base+0x80>  // b.plast
   56854:	mov	x0, x19
   56858:	mov	w1, w20
   5685c:	bl	d040 <_obstack_newchunk@plt>
   56860:	ldr	x0, [x19, #24]
   56864:	mov	x1, x22
   56868:	mov	x2, x21
   5686c:	bl	bed0 <memcpy@plt>
   56870:	ldr	x8, [x19, #24]
   56874:	mov	w0, w20
   56878:	add	x8, x8, x21
   5687c:	str	x8, [x19, #24]
   56880:	ldp	x20, x19, [sp, #32]
   56884:	ldp	x22, x21, [sp, #16]
   56888:	ldp	x29, x30, [sp], #48
   5688c:	ret
   56890:	stp	x29, x30, [sp, #-48]!
   56894:	stp	x22, x21, [sp, #16]
   56898:	stp	x20, x19, [sp, #32]
   5689c:	mov	x20, x0
   568a0:	ldp	x0, x8, [x0, #24]
   568a4:	mov	w19, w2
   568a8:	sxtw	x21, w19
   568ac:	mov	w22, w1
   568b0:	sub	x8, x8, x0
   568b4:	cmp	x8, x21
   568b8:	mov	x29, sp
   568bc:	b.ge	568d0 <__gmp_obstack_vprintf@@Base+0xec>  // b.tcont
   568c0:	mov	x0, x20
   568c4:	mov	w1, w19
   568c8:	bl	d040 <_obstack_newchunk@plt>
   568cc:	ldr	x0, [x20, #24]
   568d0:	add	x8, x0, x21
   568d4:	mov	w1, w22
   568d8:	mov	x2, x21
   568dc:	str	x8, [x20, #24]
   568e0:	bl	c5f0 <memset@plt>
   568e4:	mov	w0, w19
   568e8:	ldp	x20, x19, [sp, #32]
   568ec:	ldp	x22, x21, [sp, #16]
   568f0:	ldp	x29, x30, [sp], #48
   568f4:	ret

00000000000568f8 <__gmp_printf@@Base>:
   568f8:	sub	sp, sp, #0x120
   568fc:	stp	x29, x30, [sp, #256]
   56900:	add	x29, sp, #0x100
   56904:	mov	x9, #0xffffffffffffffc8    	// #-56
   56908:	mov	x10, sp
   5690c:	sub	x11, x29, #0x78
   56910:	str	x28, [sp, #272]
   56914:	stp	x1, x2, [x29, #-120]
   56918:	stp	x3, x4, [x29, #-104]
   5691c:	stp	x5, x6, [x29, #-88]
   56920:	stur	x7, [x29, #-72]
   56924:	stp	q0, q1, [sp]
   56928:	stp	q2, q3, [sp, #32]
   5692c:	stp	q4, q5, [sp, #64]
   56930:	movk	x9, #0xff80, lsl #32
   56934:	add	x12, x29, #0x20
   56938:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   5693c:	add	x10, x10, #0x80
   56940:	add	x11, x11, #0x38
   56944:	ldr	x13, [x13, #3856]
   56948:	stp	x10, x9, [x29, #-16]
   5694c:	stp	x12, x11, [x29, #-32]
   56950:	ldp	q0, q1, [x29, #-32]
   56954:	mov	x8, x0
   56958:	stp	q6, q7, [sp, #96]
   5695c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56960:	stp	q0, q1, [x29, #-64]
   56964:	ldr	x1, [x13]
   56968:	ldr	x0, [x0, #3816]
   5696c:	sub	x3, x29, #0x40
   56970:	mov	x2, x8
   56974:	bl	d030 <__gmp_doprnt@plt>
   56978:	ldr	x28, [sp, #272]
   5697c:	ldp	x29, x30, [sp, #256]
   56980:	add	sp, sp, #0x120
   56984:	ret
   56988:	stp	x29, x30, [sp, #-16]!
   5698c:	mov	x8, x1
   56990:	mov	x3, x0
   56994:	mov	w1, #0x1                   	// #1
   56998:	mov	x0, x8
   5699c:	mov	x29, sp
   569a0:	bl	ce30 <fwrite@plt>
   569a4:	ldp	x29, x30, [sp], #16
   569a8:	ret
   569ac:	sub	sp, sp, #0x140
   569b0:	stp	x20, x19, [sp, #304]
   569b4:	mov	w19, w2
   569b8:	sxtw	x8, w19
   569bc:	stp	x22, x21, [sp, #288]
   569c0:	cmp	x8, #0x100
   569c4:	mov	w21, #0x100                 	// #256
   569c8:	mov	x20, x0
   569cc:	csel	x2, x8, x21, cc  // cc = lo, ul, last
   569d0:	mov	x0, sp
   569d4:	stp	x29, x30, [sp, #256]
   569d8:	stp	x28, x23, [sp, #272]
   569dc:	add	x29, sp, #0x100
   569e0:	bl	c5f0 <memset@plt>
   569e4:	cmp	w19, #0x1
   569e8:	b.lt	56a24 <__gmp_printf@@Base+0x12c>  // b.tstop
   569ec:	mov	w22, w19
   569f0:	subs	w23, w22, #0x100
   569f4:	csel	w2, w22, w21, cc  // cc = lo, ul, last
   569f8:	mov	x0, sp
   569fc:	mov	w1, #0x1                   	// #1
   56a00:	mov	x3, x20
   56a04:	bl	ce30 <fwrite@plt>
   56a08:	cmn	w0, #0x1
   56a0c:	b.eq	56a20 <__gmp_printf@@Base+0x128>  // b.none
   56a10:	cmp	w22, #0x101
   56a14:	mov	w22, w23
   56a18:	b.ge	569f0 <__gmp_printf@@Base+0xf8>  // b.tcont
   56a1c:	b	56a24 <__gmp_printf@@Base+0x12c>
   56a20:	mov	w19, #0xffffffff            	// #-1
   56a24:	mov	w0, w19
   56a28:	ldp	x20, x19, [sp, #304]
   56a2c:	ldp	x22, x21, [sp, #288]
   56a30:	ldp	x28, x23, [sp, #272]
   56a34:	ldp	x29, x30, [sp, #256]
   56a38:	add	sp, sp, #0x140
   56a3c:	ret

0000000000056a40 <__gmp_snprintf@@Base>:
   56a40:	sub	sp, sp, #0x120
   56a44:	stp	x29, x30, [sp, #256]
   56a48:	add	x29, sp, #0x100
   56a4c:	mov	x8, #0xffffffffffffffd8    	// #-40
   56a50:	mov	x9, sp
   56a54:	sub	x10, x29, #0x78
   56a58:	movk	x8, #0xff80, lsl #32
   56a5c:	add	x11, x29, #0x20
   56a60:	add	x9, x9, #0x80
   56a64:	add	x10, x10, #0x28
   56a68:	stp	x9, x8, [x29, #-32]
   56a6c:	stp	x11, x10, [x29, #-48]
   56a70:	stp	x3, x4, [x29, #-120]
   56a74:	stp	x5, x6, [x29, #-104]
   56a78:	stur	x7, [x29, #-88]
   56a7c:	stp	q1, q2, [sp, #16]
   56a80:	str	q0, [sp]
   56a84:	ldp	q0, q1, [x29, #-48]
   56a88:	str	x28, [sp, #272]
   56a8c:	stp	q3, q4, [sp, #48]
   56a90:	stp	q5, q6, [sp, #80]
   56a94:	str	q7, [sp, #112]
   56a98:	stp	x0, x1, [x29, #-16]
   56a9c:	stp	q0, q1, [x29, #-80]
   56aa0:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56aa4:	ldr	x0, [x0, #4024]
   56aa8:	sub	x1, x29, #0x10
   56aac:	sub	x3, x29, #0x50
   56ab0:	bl	d030 <__gmp_doprnt@plt>
   56ab4:	ldr	x28, [sp, #272]
   56ab8:	ldp	x29, x30, [sp, #256]
   56abc:	add	sp, sp, #0x120
   56ac0:	ret
   56ac4:	sub	sp, sp, #0x90
   56ac8:	stp	x29, x30, [sp, #64]
   56acc:	stp	x24, x23, [sp, #96]
   56ad0:	stp	x22, x21, [sp, #112]
   56ad4:	stp	x20, x19, [sp, #128]
   56ad8:	ldr	x23, [x0, #8]
   56adc:	mov	x19, x2
   56ae0:	mov	x20, x1
   56ae4:	str	x25, [sp, #80]
   56ae8:	cmp	x23, #0x2
   56aec:	add	x29, sp, #0x40
   56af0:	b.cc	56b5c <__gmp_snprintf@@Base+0x11c>  // b.lo, b.ul, b.last
   56af4:	ldp	q1, q0, [x19]
   56af8:	mov	x21, x0
   56afc:	mov	x3, sp
   56b00:	mov	x1, x23
   56b04:	stp	q1, q0, [sp, #32]
   56b08:	ldr	x0, [x0]
   56b0c:	mov	x2, x20
   56b10:	stp	q1, q0, [sp]
   56b14:	bl	d110 <vsnprintf@plt>
   56b18:	cmn	w0, #0x1
   56b1c:	b.eq	56bc8 <__gmp_snprintf@@Base+0x188>  // b.none
   56b20:	mov	w22, w0
   56b24:	ldp	x11, x10, [x21]
   56b28:	sxtw	x8, w22
   56b2c:	sub	x9, x23, #0x1
   56b30:	cmp	x9, x8
   56b34:	csel	x12, x8, x9, hi  // hi = pmore
   56b38:	cmp	x9, x8
   56b3c:	sub	x8, x10, x12
   56b40:	add	x9, x11, x12
   56b44:	stp	x9, x8, [x21]
   56b48:	b.ne	56bcc <__gmp_snprintf@@Base+0x18c>  // b.any
   56b4c:	cmp	w22, #0x80
   56b50:	mov	w8, #0x80                  	// #128
   56b54:	csel	w21, w22, w8, gt
   56b58:	b	56b60 <__gmp_snprintf@@Base+0x120>
   56b5c:	mov	w21, #0x80                  	// #128
   56b60:	adrp	x24, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56b64:	adrp	x25, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56b68:	ldr	x24, [x24, #3840]
   56b6c:	ldr	x25, [x25, #4016]
   56b70:	ldr	x8, [x24]
   56b74:	lsl	x21, x21, #1
   56b78:	mov	x0, x21
   56b7c:	blr	x8
   56b80:	ldp	q0, q1, [x19]
   56b84:	mov	x3, sp
   56b88:	mov	x1, x21
   56b8c:	mov	x2, x20
   56b90:	stp	q0, q1, [sp, #32]
   56b94:	ldp	q0, q1, [sp, #32]
   56b98:	mov	x23, x0
   56b9c:	stp	q0, q1, [sp]
   56ba0:	bl	d110 <vsnprintf@plt>
   56ba4:	ldr	x8, [x25]
   56ba8:	mov	w22, w0
   56bac:	mov	x0, x23
   56bb0:	mov	x1, x21
   56bb4:	blr	x8
   56bb8:	sub	x8, x21, #0x1
   56bbc:	cmp	x8, w22, sxtw
   56bc0:	b.eq	56b70 <__gmp_snprintf@@Base+0x130>  // b.none
   56bc4:	b	56bcc <__gmp_snprintf@@Base+0x18c>
   56bc8:	mov	w22, #0xffffffff            	// #-1
   56bcc:	mov	w0, w22
   56bd0:	ldp	x20, x19, [sp, #128]
   56bd4:	ldp	x22, x21, [sp, #112]
   56bd8:	ldp	x24, x23, [sp, #96]
   56bdc:	ldr	x25, [sp, #80]
   56be0:	ldp	x29, x30, [sp, #64]
   56be4:	add	sp, sp, #0x90
   56be8:	ret
   56bec:	stp	x29, x30, [sp, #-48]!
   56bf0:	stp	x20, x19, [sp, #32]
   56bf4:	ldr	x8, [x0, #8]
   56bf8:	mov	x19, x2
   56bfc:	str	x21, [sp, #16]
   56c00:	mov	x29, sp
   56c04:	cmp	x8, #0x2
   56c08:	b.cc	56c38 <__gmp_snprintf@@Base+0x1f8>  // b.lo, b.ul, b.last
   56c0c:	mov	x20, x0
   56c10:	ldr	x0, [x0]
   56c14:	sub	x8, x8, #0x1
   56c18:	cmp	x8, x19
   56c1c:	csel	x21, x8, x19, cc  // cc = lo, ul, last
   56c20:	mov	x2, x21
   56c24:	bl	bed0 <memcpy@plt>
   56c28:	ldp	x8, x9, [x20]
   56c2c:	add	x8, x8, x21
   56c30:	sub	x9, x9, x21
   56c34:	stp	x8, x9, [x20]
   56c38:	mov	w0, w19
   56c3c:	ldp	x20, x19, [sp, #32]
   56c40:	ldr	x21, [sp, #16]
   56c44:	ldp	x29, x30, [sp], #48
   56c48:	ret
   56c4c:	stp	x29, x30, [sp, #-48]!
   56c50:	stp	x20, x19, [sp, #32]
   56c54:	ldr	x8, [x0, #8]
   56c58:	mov	w19, w2
   56c5c:	str	x21, [sp, #16]
   56c60:	mov	x29, sp
   56c64:	cmp	x8, #0x2
   56c68:	b.cc	56c9c <__gmp_snprintf@@Base+0x25c>  // b.lo, b.ul, b.last
   56c6c:	mov	x20, x0
   56c70:	sub	x8, x8, #0x1
   56c74:	ldr	x0, [x0]
   56c78:	sxtw	x9, w19
   56c7c:	cmp	x8, x9
   56c80:	csel	x21, x8, x9, cc  // cc = lo, ul, last
   56c84:	mov	x2, x21
   56c88:	bl	c5f0 <memset@plt>
   56c8c:	ldp	x8, x9, [x20]
   56c90:	add	x8, x8, x21
   56c94:	sub	x9, x9, x21
   56c98:	stp	x8, x9, [x20]
   56c9c:	mov	w0, w19
   56ca0:	ldp	x20, x19, [sp, #32]
   56ca4:	ldr	x21, [sp, #16]
   56ca8:	ldp	x29, x30, [sp], #48
   56cac:	ret
   56cb0:	ldr	x8, [x0, #8]
   56cb4:	cbz	x8, 56cc0 <__gmp_snprintf@@Base+0x280>
   56cb8:	ldr	x8, [x0]
   56cbc:	strb	wzr, [x8]
   56cc0:	mov	w0, wzr
   56cc4:	ret

0000000000056cc8 <__gmp_sprintf@@Base>:
   56cc8:	sub	sp, sp, #0x120
   56ccc:	stp	x29, x30, [sp, #256]
   56cd0:	add	x29, sp, #0x100
   56cd4:	mov	x10, #0xffffffffffffffd0    	// #-48
   56cd8:	mov	x11, sp
   56cdc:	add	x12, sp, #0x80
   56ce0:	movk	x10, #0xff80, lsl #32
   56ce4:	add	x13, x29, #0x20
   56ce8:	add	x11, x11, #0x80
   56cec:	add	x12, x12, #0x30
   56cf0:	sub	x9, x29, #0x28
   56cf4:	stp	x11, x10, [x29, #-24]
   56cf8:	stp	x13, x12, [x29, #-40]
   56cfc:	stp	q1, q2, [sp, #16]
   56d00:	str	q0, [sp]
   56d04:	ldp	q0, q1, [x9]
   56d08:	str	x28, [sp, #272]
   56d0c:	stp	x2, x3, [sp, #128]
   56d10:	stp	x4, x5, [sp, #144]
   56d14:	stp	x6, x7, [sp, #160]
   56d18:	stp	q3, q4, [sp, #48]
   56d1c:	stp	q5, q6, [sp, #80]
   56d20:	str	q7, [sp, #112]
   56d24:	stur	x0, [x29, #-8]
   56d28:	stp	q0, q1, [x29, #-80]
   56d2c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56d30:	ldr	x0, [x0, #3944]
   56d34:	mov	x8, x1
   56d38:	sub	x1, x29, #0x8
   56d3c:	sub	x3, x29, #0x50
   56d40:	mov	x2, x8
   56d44:	bl	d030 <__gmp_doprnt@plt>
   56d48:	ldr	x28, [sp, #272]
   56d4c:	ldp	x29, x30, [sp, #256]
   56d50:	add	sp, sp, #0x120
   56d54:	ret
   56d58:	sub	sp, sp, #0x40
   56d5c:	stp	x29, x30, [sp, #32]
   56d60:	stp	x20, x19, [sp, #48]
   56d64:	ldr	x20, [x0]
   56d68:	ldp	q1, q0, [x2]
   56d6c:	mov	x19, x0
   56d70:	mov	x2, sp
   56d74:	mov	x0, x20
   56d78:	add	x29, sp, #0x20
   56d7c:	stp	q1, q0, [sp]
   56d80:	bl	cf10 <vsprintf@plt>
   56d84:	mov	x0, x20
   56d88:	bl	bf60 <strlen@plt>
   56d8c:	add	x8, x20, w0, sxtw
   56d90:	str	x8, [x19]
   56d94:	ldp	x20, x19, [sp, #48]
   56d98:	ldp	x29, x30, [sp, #32]
   56d9c:	add	sp, sp, #0x40
   56da0:	ret
   56da4:	stp	x29, x30, [sp, #-32]!
   56da8:	ldr	x8, [x0]
   56dac:	str	x19, [sp, #16]
   56db0:	mov	x29, sp
   56db4:	mov	x19, x2
   56db8:	add	x9, x8, x2
   56dbc:	str	x9, [x0]
   56dc0:	mov	x0, x8
   56dc4:	bl	bed0 <memcpy@plt>
   56dc8:	mov	w0, w19
   56dcc:	ldr	x19, [sp, #16]
   56dd0:	ldp	x29, x30, [sp], #32
   56dd4:	ret
   56dd8:	stp	x29, x30, [sp, #-32]!
   56ddc:	ldr	x8, [x0]
   56de0:	str	x19, [sp, #16]
   56de4:	mov	w19, w2
   56de8:	sxtw	x2, w19
   56dec:	add	x9, x8, x2
   56df0:	str	x9, [x0]
   56df4:	mov	x0, x8
   56df8:	mov	x29, sp
   56dfc:	bl	c5f0 <memset@plt>
   56e00:	mov	w0, w19
   56e04:	ldr	x19, [sp, #16]
   56e08:	ldp	x29, x30, [sp], #32
   56e0c:	ret
   56e10:	ldr	x8, [x0]
   56e14:	mov	w0, wzr
   56e18:	strb	wzr, [x8]
   56e1c:	ret
   56e20:	sub	sp, sp, #0x80
   56e24:	stp	x29, x30, [sp, #64]
   56e28:	str	x23, [sp, #80]
   56e2c:	stp	x22, x21, [sp, #96]
   56e30:	stp	x20, x19, [sp, #112]
   56e34:	adrp	x23, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56e38:	ldr	x23, [x23, #3792]
   56e3c:	mov	x20, x2
   56e40:	mov	x21, x1
   56e44:	mov	x19, x0
   56e48:	mov	w9, #0x100                 	// #256
   56e4c:	add	x29, sp, #0x40
   56e50:	ldp	x8, x1, [x19, #16]
   56e54:	add	x9, x8, x9
   56e58:	cmp	x1, x9
   56e5c:	b.hi	56e7c <__gmp_sprintf@@Base+0x1b4>  // b.pmore
   56e60:	lsl	x2, x9, #1
   56e64:	str	x2, [x19, #24]
   56e68:	ldr	x8, [x23]
   56e6c:	ldr	x0, [x19, #8]
   56e70:	blr	x8
   56e74:	ldp	x8, x1, [x19, #16]
   56e78:	str	x0, [x19, #8]
   56e7c:	ldp	q1, q0, [x20]
   56e80:	sub	x22, x1, x8
   56e84:	mov	x3, sp
   56e88:	mov	x1, x22
   56e8c:	stp	q1, q0, [sp, #32]
   56e90:	ldp	x9, x10, [x19, #8]
   56e94:	mov	x2, x21
   56e98:	stp	q1, q0, [sp]
   56e9c:	add	x0, x9, x10
   56ea0:	bl	d110 <vsnprintf@plt>
   56ea4:	sub	w8, w22, #0x1
   56ea8:	cmn	w0, #0x1
   56eac:	csel	w8, w8, w0, eq  // eq = none
   56eb0:	sxtw	x0, w8
   56eb4:	sub	x8, x22, #0x1
   56eb8:	cmp	x8, x0
   56ebc:	b.hi	56ee8 <__gmp_sprintf@@Base+0x220>  // b.pmore
   56ec0:	add	w10, w0, #0x2
   56ec4:	lsl	x9, x22, #1
   56ec8:	sxtw	x10, w10
   56ecc:	cmp	x8, x0
   56ed0:	csel	x9, x9, x10, eq  // eq = none
   56ed4:	ldp	x8, x1, [x19, #16]
   56ed8:	add	x9, x8, x9
   56edc:	cmp	x1, x9
   56ee0:	b.ls	56e60 <__gmp_sprintf@@Base+0x198>  // b.plast
   56ee4:	b	56e7c <__gmp_sprintf@@Base+0x1b4>
   56ee8:	ldr	x8, [x19, #16]
   56eec:	ldr	x23, [sp, #80]
   56ef0:	add	x8, x8, x0
   56ef4:	str	x8, [x19, #16]
   56ef8:	ldp	x20, x19, [sp, #112]
   56efc:	ldp	x22, x21, [sp, #96]
   56f00:	ldp	x29, x30, [sp, #64]
   56f04:	add	sp, sp, #0x80
   56f08:	ret

0000000000056f0c <__gmp_vasprintf@@Base>:
   56f0c:	sub	sp, sp, #0x60
   56f10:	stp	x29, x30, [sp, #64]
   56f14:	stp	x20, x19, [sp, #80]
   56f18:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56f1c:	ldr	x8, [x8, #3840]
   56f20:	mov	w9, #0x100                 	// #256
   56f24:	str	x0, [sp, #32]
   56f28:	mov	w0, #0x100                 	// #256
   56f2c:	ldr	x8, [x8]
   56f30:	add	x29, sp, #0x40
   56f34:	mov	x19, x2
   56f38:	mov	x20, x1
   56f3c:	str	x9, [sp, #56]
   56f40:	blr	x8
   56f44:	stp	x0, xzr, [sp, #40]
   56f48:	ldp	q1, q0, [x19]
   56f4c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56f50:	add	x1, sp, #0x20
   56f54:	mov	x3, sp
   56f58:	stp	q1, q0, [sp]
   56f5c:	ldr	x0, [x0, #3864]
   56f60:	mov	x2, x20
   56f64:	bl	d030 <__gmp_doprnt@plt>
   56f68:	ldp	x20, x19, [sp, #80]
   56f6c:	ldp	x29, x30, [sp, #64]
   56f70:	add	sp, sp, #0x60
   56f74:	ret

0000000000056f78 <__gmp_vfprintf@@Base>:
   56f78:	sub	sp, sp, #0x30
   56f7c:	stp	x29, x30, [sp, #32]
   56f80:	ldp	q1, q0, [x2]
   56f84:	mov	x8, x1
   56f88:	mov	x1, x0
   56f8c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56f90:	stp	q1, q0, [sp]
   56f94:	ldr	x0, [x0, #3816]
   56f98:	mov	x3, sp
   56f9c:	mov	x2, x8
   56fa0:	add	x29, sp, #0x20
   56fa4:	bl	d030 <__gmp_doprnt@plt>
   56fa8:	ldp	x29, x30, [sp, #32]
   56fac:	add	sp, sp, #0x30
   56fb0:	ret

0000000000056fb4 <__gmp_vprintf@@Base>:
   56fb4:	sub	sp, sp, #0x30
   56fb8:	stp	x29, x30, [sp, #32]
   56fbc:	ldp	q0, q1, [x1]
   56fc0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56fc4:	ldr	x8, [x8, #3856]
   56fc8:	mov	x2, x0
   56fcc:	stp	q0, q1, [sp]
   56fd0:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   56fd4:	ldr	x1, [x8]
   56fd8:	ldr	x0, [x0, #3816]
   56fdc:	mov	x3, sp
   56fe0:	add	x29, sp, #0x20
   56fe4:	bl	d030 <__gmp_doprnt@plt>
   56fe8:	ldp	x29, x30, [sp, #32]
   56fec:	add	sp, sp, #0x30
   56ff0:	ret

0000000000056ff4 <__gmp_vsnprintf@@Base>:
   56ff4:	sub	sp, sp, #0x40
   56ff8:	stp	x29, x30, [sp, #48]
   56ffc:	add	x29, sp, #0x30
   57000:	stp	x0, x1, [x29, #-16]
   57004:	ldp	q1, q0, [x3]
   57008:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   5700c:	sub	x1, x29, #0x10
   57010:	mov	x3, sp
   57014:	stp	q1, q0, [sp]
   57018:	ldr	x0, [x0, #4024]
   5701c:	bl	d030 <__gmp_doprnt@plt>
   57020:	ldp	x29, x30, [sp, #48]
   57024:	add	sp, sp, #0x40
   57028:	ret

000000000005702c <__gmp_vsprintf@@Base>:
   5702c:	sub	sp, sp, #0x40
   57030:	stp	x29, x30, [sp, #48]
   57034:	add	x29, sp, #0x30
   57038:	stur	x0, [x29, #-8]
   5703c:	ldp	q1, q0, [x2]
   57040:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57044:	mov	x8, x1
   57048:	sub	x1, x29, #0x8
   5704c:	stp	q1, q0, [sp]
   57050:	ldr	x0, [x0, #3944]
   57054:	mov	x3, sp
   57058:	mov	x2, x8
   5705c:	bl	d030 <__gmp_doprnt@plt>
   57060:	ldp	x29, x30, [sp, #48]
   57064:	add	sp, sp, #0x40
   57068:	ret

000000000005706c <__gmp_doscan@@Base>:
   5706c:	sub	sp, sp, #0x110
   57070:	stp	x29, x30, [sp, #176]
   57074:	add	x29, sp, #0xb0
   57078:	stp	x28, x27, [sp, #192]
   5707c:	stp	x26, x25, [sp, #208]
   57080:	stp	x24, x23, [sp, #224]
   57084:	stp	x22, x21, [sp, #240]
   57088:	stp	x20, x19, [sp, #256]
   5708c:	stur	x1, [x29, #-64]
   57090:	ldp	q1, q0, [x3]
   57094:	mov	x24, x0
   57098:	mov	x0, x2
   5709c:	mov	x25, x2
   570a0:	stp	q1, q0, [x29, #-48]
   570a4:	bl	bf60 <strlen@plt>
   570a8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   570ac:	ldr	x8, [x8, #3840]
   570b0:	add	x0, x0, #0x4
   570b4:	str	x0, [sp, #40]
   570b8:	ldr	x8, [x8]
   570bc:	blr	x8
   570c0:	adrp	x28, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   570c4:	ldrb	w23, [x25]
   570c8:	ldr	x28, [x28, #4016]
   570cc:	mov	x27, x0
   570d0:	cbz	w23, 57db4 <__gmp_doscan@@Base+0xd48>
   570d4:	bl	cae0 <__ctype_b_loc@plt>
   570d8:	adrp	x12, 65000 <__gmp_oddfac_table@@Base+0xf0>
   570dc:	mov	w20, wzr
   570e0:	mov	w26, wzr
   570e4:	add	x12, x12, #0x5ec
   570e8:	str	x0, [sp, #88]
   570ec:	str	x27, [sp, #48]
   570f0:	stur	x24, [x29, #-80]
   570f4:	b	57114 <__gmp_doscan@@Base+0xa8>
   570f8:	ldp	w26, w8, [sp, #76]
   570fc:	ldrb	w23, [x25]
   57100:	adrp	x12, 65000 <__gmp_oddfac_table@@Base+0xf0>
   57104:	add	x12, x12, #0x5ec
   57108:	eor	w8, w8, #0x1
   5710c:	add	w26, w26, w8
   57110:	cbz	w23, 57df0 <__gmp_doscan@@Base+0xd84>
   57114:	mov	x24, x25
   57118:	str	w26, [sp, #76]
   5711c:	sxtw	x19, w20
   57120:	mov	x25, x24
   57124:	ldr	x8, [sp, #88]
   57128:	and	x9, x23, #0xff
   5712c:	add	x24, x25, #0x1
   57130:	ldr	x8, [x8]
   57134:	ldrh	w9, [x8, x9, lsl #1]
   57138:	tbnz	w9, #13, 572cc <__gmp_doscan@@Base+0x260>
   5713c:	and	w9, w23, #0xff
   57140:	cmp	w9, #0x25
   57144:	b.ne	57308 <__gmp_doscan@@Base+0x29c>  // b.any
   57148:	mov	x1, x25
   5714c:	mov	w22, wzr
   57150:	str	xzr, [sp, #80]
   57154:	mov	x25, x24
   57158:	add	x9, x25, #0x1
   5715c:	mov	x24, x25
   57160:	mov	x25, x9
   57164:	ldurb	w23, [x25, #-1]
   57168:	cmp	w23, #0x7a
   5716c:	b.hi	572c0 <__gmp_doscan@@Base+0x254>  // b.pmore
   57170:	adr	x9, 57180 <__gmp_doscan@@Base+0x114>
   57174:	ldrh	w10, [x12, x23, lsl #1]
   57178:	add	x9, x9, x10, lsl #2
   5717c:	br	x9
   57180:	add	x25, x25, #0x1
   57184:	add	x24, x24, #0x1
   57188:	ldurb	w23, [x25, #-1]
   5718c:	cmp	w23, #0x7a
   57190:	b.ls	57170 <__gmp_doscan@@Base+0x104>  // b.plast
   57194:	b	572c0 <__gmp_doscan@@Base+0x254>
   57198:	mov	w9, #0x1                   	// #1
   5719c:	str	w9, [sp, #80]
   571a0:	b	57158 <__gmp_doscan@@Base+0xec>
   571a4:	ldr	w9, [sp, #84]
   571a8:	and	w9, w9, #0xff
   571ac:	cmp	w9, #0x68
   571b0:	mov	w9, #0x48                  	// #72
   571b4:	str	w9, [sp, #84]
   571b8:	b.eq	57158 <__gmp_doscan@@Base+0xec>  // b.none
   571bc:	b	571d8 <__gmp_doscan@@Base+0x16c>
   571c0:	ldr	w9, [sp, #84]
   571c4:	and	w9, w9, #0xff
   571c8:	cmp	w9, #0x6c
   571cc:	mov	w9, #0x4c                  	// #76
   571d0:	str	w9, [sp, #84]
   571d4:	b.eq	57158 <__gmp_doscan@@Base+0xec>  // b.none
   571d8:	mov	w9, w23
   571dc:	str	w23, [sp, #84]
   571e0:	b	57158 <__gmp_doscan@@Base+0xec>
   571e4:	mov	w22, wzr
   571e8:	mov	w11, #0xa                   	// #10
   571ec:	mul	w9, w22, w11
   571f0:	add	w9, w9, w23, uxtb
   571f4:	ldrb	w23, [x24, #1]!
   571f8:	sub	w22, w9, #0x30
   571fc:	ldrh	w10, [x8, x23, lsl #1]
   57200:	tbnz	w10, #11, 571ec <__gmp_doscan@@Base+0x180>
   57204:	b	57154 <__gmp_doscan@@Base+0xe8>
   57208:	ldr	w8, [sp, #80]
   5720c:	cbnz	w8, 572c0 <__gmp_doscan@@Base+0x254>
   57210:	ldursw	x8, [x29, #-24]
   57214:	tbz	w8, #31, 57234 <__gmp_doscan@@Base+0x1c8>
   57218:	add	w9, w8, #0x8
   5721c:	cmn	w8, #0x8
   57220:	stur	w9, [x29, #-24]
   57224:	b.gt	57234 <__gmp_doscan@@Base+0x1c8>
   57228:	ldur	x9, [x29, #-40]
   5722c:	add	x8, x9, x8
   57230:	b	57240 <__gmp_doscan@@Base+0x1d4>
   57234:	ldur	x8, [x29, #-48]
   57238:	add	x9, x8, #0x8
   5723c:	stur	x9, [x29, #-48]
   57240:	ldr	x0, [x8]
   57244:	ldr	w8, [sp, #84]
   57248:	and	w8, w8, #0xff
   5724c:	sub	w9, w8, #0x46
   57250:	cmp	w9, #0x34
   57254:	b.hi	57278 <__gmp_doscan@@Base+0x20c>  // b.pmore
   57258:	adrp	x11, 65000 <__gmp_oddfac_table@@Base+0xf0>
   5725c:	add	x11, x11, #0x6e2
   57260:	adr	x8, 57270 <__gmp_doscan@@Base+0x204>
   57264:	ldrb	w10, [x11, x9]
   57268:	add	x8, x8, x10, lsl #2
   5726c:	br	x8
   57270:	str	x19, [x0]
   57274:	b	572c0 <__gmp_doscan@@Base+0x254>
   57278:	cbnz	w8, 572c0 <__gmp_doscan@@Base+0x254>
   5727c:	str	w20, [x0]
   57280:	b	572c0 <__gmp_doscan@@Base+0x254>
   57284:	mov	x1, x19
   57288:	bl	c620 <__gmpf_set_si@plt>
   5728c:	b	572b0 <__gmp_doscan@@Base+0x244>
   57290:	strb	w20, [x0]
   57294:	b	572c0 <__gmp_doscan@@Base+0x254>
   57298:	mov	w2, #0x1                   	// #1
   5729c:	mov	x1, x19
   572a0:	bl	cb70 <__gmpq_set_si@plt>
   572a4:	b	572b0 <__gmp_doscan@@Base+0x244>
   572a8:	mov	x1, x19
   572ac:	bl	d270 <__gmpz_set_si@plt>
   572b0:	adrp	x12, 65000 <__gmp_oddfac_table@@Base+0xf0>
   572b4:	add	x12, x12, #0x5ec
   572b8:	b	572c0 <__gmp_doscan@@Base+0x254>
   572bc:	strh	w20, [x0]
   572c0:	ldrb	w23, [x25]
   572c4:	cbnz	w23, 57124 <__gmp_doscan@@Base+0xb8>
   572c8:	b	57df0 <__gmp_doscan@@Base+0xd84>
   572cc:	ldur	x19, [x29, #-64]
   572d0:	ldur	x22, [x29, #-80]
   572d4:	ldr	x23, [sp, #88]
   572d8:	sub	w20, w20, #0x1
   572dc:	ldr	x8, [x22, #16]
   572e0:	mov	x0, x19
   572e4:	blr	x8
   572e8:	ldr	x8, [x23]
   572ec:	add	w20, w20, #0x1
   572f0:	ldrh	w8, [x8, w0, sxtw #1]
   572f4:	tbnz	w8, #13, 572dc <__gmp_doscan@@Base+0x270>
   572f8:	ldr	x8, [x22, #24]
   572fc:	mov	x1, x19
   57300:	blr	x8
   57304:	b	5732c <__gmp_doscan@@Base+0x2c0>
   57308:	mov	x25, x24
   5730c:	ldur	x22, [x29, #-80]
   57310:	ldur	x0, [x29, #-64]
   57314:	ldr	x8, [x22, #16]
   57318:	blr	x8
   5731c:	cmp	w0, w23, uxtb
   57320:	b.ne	57e24 <__gmp_doscan@@Base+0xdb8>  // b.any
   57324:	add	w20, w20, #0x1
   57328:	mov	x24, x25
   5732c:	ldrb	w23, [x24]
   57330:	adrp	x12, 65000 <__gmp_oddfac_table@@Base+0xf0>
   57334:	add	x12, x12, #0x5ec
   57338:	cbnz	w23, 5711c <__gmp_doscan@@Base+0xb0>
   5733c:	b	57df0 <__gmp_doscan@@Base+0xd84>
   57340:	str	wzr, [sp, #72]
   57344:	b	57394 <__gmp_doscan@@Base+0x328>
   57348:	ldur	x23, [x29, #-64]
   5734c:	b	57408 <__gmp_doscan@@Base+0x39c>
   57350:	mov	w8, #0x10                  	// #16
   57354:	b	57390 <__gmp_doscan@@Base+0x324>
   57358:	mov	w8, #0xa                   	// #10
   5735c:	b	57390 <__gmp_doscan@@Base+0x324>
   57360:	mov	x9, x25
   57364:	ldrb	w8, [x9], #1
   57368:	cmp	w8, #0x5e
   5736c:	b.ne	57b48 <__gmp_doscan@@Base+0xadc>  // b.any
   57370:	ldrb	w8, [x25, #1]
   57374:	add	x25, x25, #0x2
   57378:	ldur	x23, [x29, #-64]
   5737c:	ldur	x24, [x29, #-80]
   57380:	cmp	w8, #0x5d
   57384:	b.eq	57b68 <__gmp_doscan@@Base+0xafc>  // b.none
   57388:	b	57b5c <__gmp_doscan@@Base+0xaf0>
   5738c:	mov	w8, #0x8                   	// #8
   57390:	str	w8, [sp, #72]
   57394:	ldr	w8, [sp, #84]
   57398:	ldur	x23, [x29, #-64]
   5739c:	and	w8, w8, #0xff
   573a0:	sub	w8, w8, #0x46
   573a4:	cmp	w8, #0x14
   573a8:	b.hi	57408 <__gmp_doscan@@Base+0x39c>  // b.pmore
   573ac:	mov	w9, #0x1                   	// #1
   573b0:	ldur	x24, [x29, #-80]
   573b4:	lsl	w8, w9, w8
   573b8:	mov	w9, #0x801                 	// #2049
   573bc:	movk	w9, #0x10, lsl #16
   573c0:	tst	w8, w9
   573c4:	b.eq	57b74 <__gmp_doscan@@Base+0xb08>  // b.none
   573c8:	sub	w19, w20, #0x2
   573cc:	ldr	x20, [sp, #88]
   573d0:	ldr	x8, [x24, #16]
   573d4:	mov	x0, x23
   573d8:	blr	x8
   573dc:	ldr	x8, [x20]
   573e0:	add	w19, w19, #0x1
   573e4:	ldrh	w8, [x8, w0, sxtw #1]
   573e8:	tbnz	w8, #13, 573d0 <__gmp_doscan@@Base+0x364>
   573ec:	ldr	x8, [x24, #24]
   573f0:	mov	x1, x23
   573f4:	blr	x8
   573f8:	ldr	w8, [sp, #80]
   573fc:	cbz	w8, 57410 <__gmp_doscan@@Base+0x3a4>
   57400:	mov	x21, xzr
   57404:	b	57444 <__gmp_doscan@@Base+0x3d8>
   57408:	ldur	x24, [x29, #-80]
   5740c:	b	57b74 <__gmp_doscan@@Base+0xb08>
   57410:	ldursw	x8, [x29, #-24]
   57414:	tbz	w8, #31, 57434 <__gmp_doscan@@Base+0x3c8>
   57418:	add	w9, w8, #0x8
   5741c:	cmn	w8, #0x8
   57420:	stur	w9, [x29, #-24]
   57424:	b.gt	57434 <__gmp_doscan@@Base+0x3c8>
   57428:	ldur	x9, [x29, #-40]
   5742c:	add	x8, x9, x8
   57430:	b	57440 <__gmp_doscan@@Base+0x3d4>
   57434:	ldur	x8, [x29, #-48]
   57438:	add	x9, x8, #0x8
   5743c:	stur	x9, [x29, #-48]
   57440:	ldr	x21, [x8]
   57444:	ldr	x8, [x24, #16]
   57448:	mov	x0, x23
   5744c:	blr	x8
   57450:	cmn	w0, #0x1
   57454:	b.eq	57dbc <__gmp_doscan@@Base+0xd50>  // b.none
   57458:	str	x21, [sp, #24]
   5745c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57460:	ldr	x8, [x8, #3840]
   57464:	cmp	w22, #0x0
   57468:	mov	w9, #0x7ffffffe            	// #2147483646
   5746c:	mov	w28, w0
   57470:	ldr	x8, [x8]
   57474:	csel	w9, w9, w22, eq  // eq = none
   57478:	mov	w0, #0x200                 	// #512
   5747c:	stur	w9, [x29, #-68]
   57480:	blr	x8
   57484:	ldr	w8, [sp, #84]
   57488:	ldr	w22, [sp, #72]
   5748c:	mov	w9, #0x1                   	// #1
   57490:	stp	w9, wzr, [sp, #64]
   57494:	and	w8, w8, #0xff
   57498:	cmp	w8, #0x46
   5749c:	mov	w8, #0xa                   	// #10
   574a0:	mov	w9, #0x8                   	// #8
   574a4:	mov	x23, xzr
   574a8:	mov	w27, #0x200                 	// #512
   574ac:	csel	w8, w8, w9, eq  // eq = none
   574b0:	mov	w26, #0x1                   	// #1
   574b4:	str	xzr, [sp, #8]
   574b8:	str	wzr, [sp, #36]
   574bc:	str	w8, [sp, #20]
   574c0:	cmp	w28, #0x2b
   574c4:	b.eq	5750c <__gmp_doscan@@Base+0x4a0>  // b.none
   574c8:	cmp	w28, #0x2d
   574cc:	b.ne	57534 <__gmp_doscan@@Base+0x4c8>  // b.any
   574d0:	cmp	x23, x27
   574d4:	b.cc	574fc <__gmp_doscan@@Base+0x490>  // b.lo, b.ul, b.last
   574d8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   574dc:	ldr	x8, [x8, #3792]
   574e0:	add	x20, x27, #0x200
   574e4:	mov	x1, x27
   574e8:	mov	x2, x20
   574ec:	ldr	x8, [x8]
   574f0:	blr	x8
   574f4:	mov	x27, x20
   574f8:	ldr	x20, [sp, #88]
   574fc:	add	x8, x23, #0x1
   57500:	mov	w9, #0x2d                  	// #45
   57504:	strb	w9, [x0, x23]
   57508:	mov	x23, x8
   5750c:	ldur	w8, [x29, #-68]
   57510:	mov	x21, x0
   57514:	cmp	w26, w8
   57518:	add	w26, w26, #0x1
   5751c:	b.ge	57c44 <__gmp_doscan@@Base+0xbd8>  // b.tcont
   57520:	ldr	x8, [x24, #16]
   57524:	ldur	x0, [x29, #-64]
   57528:	blr	x8
   5752c:	mov	w28, w0
   57530:	mov	x0, x21
   57534:	cbz	w22, 5754c <__gmp_doscan@@Base+0x4e0>
   57538:	mov	w9, wzr
   5753c:	str	w22, [sp, #56]
   57540:	cmp	w22, #0x10
   57544:	b.ne	57698 <__gmp_doscan@@Base+0x62c>  // b.any
   57548:	b	577f0 <__gmp_doscan@@Base+0x784>
   5754c:	cmp	w28, #0x30
   57550:	b.ne	575d8 <__gmp_doscan@@Base+0x56c>  // b.any
   57554:	cmp	x23, x27
   57558:	b.cc	5757c <__gmp_doscan@@Base+0x510>  // b.lo, b.ul, b.last
   5755c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57560:	ldr	x8, [x8, #3792]
   57564:	add	x20, x27, #0x200
   57568:	mov	x1, x27
   5756c:	mov	x2, x20
   57570:	ldr	x8, [x8]
   57574:	blr	x8
   57578:	mov	x27, x20
   5757c:	ldur	w8, [x29, #-68]
   57580:	add	x22, x23, #0x1
   57584:	mov	w28, #0x30                  	// #48
   57588:	add	w20, w26, #0x1
   5758c:	cmp	w26, w8
   57590:	strb	w28, [x0, x23]
   57594:	b.ge	57c4c <__gmp_doscan@@Base+0xbe0>  // b.tcont
   57598:	mov	x21, x0
   5759c:	ldr	x8, [x24, #16]
   575a0:	ldur	x0, [x29, #-64]
   575a4:	blr	x8
   575a8:	orr	w8, w0, #0x20
   575ac:	mov	w28, w0
   575b0:	cmp	w8, #0x78
   575b4:	b.ne	575f0 <__gmp_doscan@@Base+0x584>  // b.any
   575b8:	ldr	w8, [sp, #84]
   575bc:	and	w8, w8, #0xff
   575c0:	cmp	w8, #0x46
   575c4:	b.ne	57618 <__gmp_doscan@@Base+0x5ac>  // b.any
   575c8:	ldr	x20, [sp, #88]
   575cc:	mov	w8, #0x1                   	// #1
   575d0:	str	w8, [sp, #36]
   575d4:	b	57658 <__gmp_doscan@@Base+0x5ec>
   575d8:	mov	w22, #0xa                   	// #10
   575dc:	mov	w9, wzr
   575e0:	str	w22, [sp, #56]
   575e4:	cmp	w22, #0x10
   575e8:	b.ne	57698 <__gmp_doscan@@Base+0x62c>  // b.any
   575ec:	b	577f0 <__gmp_doscan@@Base+0x784>
   575f0:	mov	x23, x22
   575f4:	ldr	w22, [sp, #20]
   575f8:	mov	w26, w20
   575fc:	ldr	x20, [sp, #88]
   57600:	mov	w9, #0x1                   	// #1
   57604:	mov	x0, x21
   57608:	str	w22, [sp, #56]
   5760c:	cmp	w22, #0x10
   57610:	b.ne	57698 <__gmp_doscan@@Base+0x62c>  // b.any
   57614:	b	577f0 <__gmp_doscan@@Base+0x784>
   57618:	cmp	x22, x27
   5761c:	b.cc	57648 <__gmp_doscan@@Base+0x5dc>  // b.lo, b.ul, b.last
   57620:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57624:	ldr	x8, [x8, #3792]
   57628:	add	x20, x27, #0x200
   5762c:	mov	x0, x21
   57630:	mov	x1, x27
   57634:	ldr	x8, [x8]
   57638:	mov	x2, x20
   5763c:	blr	x8
   57640:	mov	x21, x0
   57644:	mov	x27, x20
   57648:	ldr	x20, [sp, #88]
   5764c:	add	x8, x23, #0x2
   57650:	strb	w28, [x21, x22]
   57654:	mov	x22, x8
   57658:	ldur	w8, [x29, #-68]
   5765c:	ldur	x23, [x29, #-64]
   57660:	add	w26, w26, #0x2
   57664:	cmp	w26, w8
   57668:	b.gt	57b30 <__gmp_doscan@@Base+0xac4>
   5766c:	ldr	x8, [x24, #16]
   57670:	mov	x0, x23
   57674:	blr	x8
   57678:	mov	w28, w0
   5767c:	mov	w9, wzr
   57680:	mov	x23, x22
   57684:	mov	w22, #0x10                  	// #16
   57688:	mov	x0, x21
   5768c:	str	w22, [sp, #56]
   57690:	cmp	w22, #0x10
   57694:	b.eq	577f0 <__gmp_doscan@@Base+0x784>  // b.none
   57698:	cmp	w22, #0x8
   5769c:	b.ne	57748 <__gmp_doscan@@Base+0x6dc>  // b.any
   576a0:	orr	w8, w28, #0x1
   576a4:	cmp	w8, #0x39
   576a8:	b.eq	577d4 <__gmp_doscan@@Base+0x768>  // b.none
   576ac:	ldr	x8, [x20]
   576b0:	mov	x22, x23
   576b4:	mov	w24, w26
   576b8:	ldrh	w8, [x8, w28, sxtw #1]
   576bc:	tbz	w8, #11, 57804 <__gmp_doscan@@Base+0x798>
   576c0:	cmp	x23, x27
   576c4:	b.cc	576ec <__gmp_doscan@@Base+0x680>  // b.lo, b.ul, b.last
   576c8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   576cc:	ldr	x8, [x8, #3792]
   576d0:	add	x20, x27, #0x200
   576d4:	mov	x1, x27
   576d8:	mov	x2, x20
   576dc:	ldr	x8, [x8]
   576e0:	blr	x8
   576e4:	mov	x27, x20
   576e8:	ldr	x20, [sp, #88]
   576ec:	ldur	w8, [x29, #-68]
   576f0:	add	x22, x23, #0x1
   576f4:	add	w24, w26, #0x1
   576f8:	strb	w28, [x0, x23]
   576fc:	cmp	w26, w8
   57700:	b.ge	57a88 <__gmp_doscan@@Base+0xa1c>  // b.tcont
   57704:	ldur	x8, [x29, #-80]
   57708:	mov	x21, x0
   5770c:	ldur	x0, [x29, #-64]
   57710:	ldr	x8, [x8, #16]
   57714:	blr	x8
   57718:	orr	w8, w0, #0x1
   5771c:	mov	w28, w0
   57720:	cmp	w8, #0x39
   57724:	mov	w9, #0x1                   	// #1
   57728:	b.eq	577e0 <__gmp_doscan@@Base+0x774>  // b.none
   5772c:	ldr	x8, [x20]
   57730:	mov	w26, w24
   57734:	mov	x23, x22
   57738:	mov	x0, x21
   5773c:	ldrh	w8, [x8, w28, sxtw #1]
   57740:	tbnz	w8, #11, 576c0 <__gmp_doscan@@Base+0x654>
   57744:	b	57804 <__gmp_doscan@@Base+0x798>
   57748:	ldr	x8, [x20]
   5774c:	mov	x22, x23
   57750:	mov	w24, w26
   57754:	ldrh	w8, [x8, w28, sxtw #1]
   57758:	tbz	w8, #11, 57804 <__gmp_doscan@@Base+0x798>
   5775c:	cmp	x23, x27
   57760:	b.cc	57788 <__gmp_doscan@@Base+0x71c>  // b.lo, b.ul, b.last
   57764:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57768:	ldr	x8, [x8, #3792]
   5776c:	add	x20, x27, #0x200
   57770:	mov	x1, x27
   57774:	mov	x2, x20
   57778:	ldr	x8, [x8]
   5777c:	blr	x8
   57780:	mov	x27, x20
   57784:	ldr	x20, [sp, #88]
   57788:	ldur	w8, [x29, #-68]
   5778c:	add	x22, x23, #0x1
   57790:	add	w24, w26, #0x1
   57794:	strb	w28, [x0, x23]
   57798:	cmp	w26, w8
   5779c:	b.ge	57a88 <__gmp_doscan@@Base+0xa1c>  // b.tcont
   577a0:	ldur	x8, [x29, #-80]
   577a4:	mov	x21, x0
   577a8:	ldur	x0, [x29, #-64]
   577ac:	ldr	x8, [x8, #16]
   577b0:	blr	x8
   577b4:	ldr	x8, [x20]
   577b8:	mov	w28, w0
   577bc:	mov	x0, x21
   577c0:	mov	w26, w24
   577c4:	ldrh	w8, [x8, w28, sxtw #1]
   577c8:	mov	x23, x22
   577cc:	tbnz	w8, #11, 5775c <__gmp_doscan@@Base+0x6f0>
   577d0:	b	5793c <__gmp_doscan@@Base+0x8d0>
   577d4:	mov	x22, x23
   577d8:	mov	w24, w26
   577dc:	b	57804 <__gmp_doscan@@Base+0x798>
   577e0:	mov	x0, x21
   577e4:	ldr	w8, [sp, #64]
   577e8:	cbnz	w8, 5780c <__gmp_doscan@@Base+0x7a0>
   577ec:	b	57c34 <__gmp_doscan@@Base+0xbc8>
   577f0:	ldr	x8, [x20]
   577f4:	mov	x22, x23
   577f8:	mov	w24, w26
   577fc:	ldrh	w8, [x8, w28, sxtw #1]
   57800:	tbnz	w8, #12, 578c8 <__gmp_doscan@@Base+0x85c>
   57804:	ldr	w8, [sp, #64]
   57808:	cbz	w8, 57c34 <__gmp_doscan@@Base+0xbc8>
   5780c:	ldr	w8, [sp, #84]
   57810:	mov	x21, x0
   57814:	str	w9, [sp, #60]
   57818:	and	w8, w8, #0xff
   5781c:	cmp	w8, #0x46
   57820:	b.ne	57974 <__gmp_doscan@@Base+0x908>  // b.any
   57824:	ldr	w9, [sp, #68]
   57828:	cbnz	w9, 57974 <__gmp_doscan@@Base+0x908>
   5782c:	mov	w0, #0x10000               	// #65536
   57830:	bl	c400 <nl_langinfo@plt>
   57834:	ldrb	w8, [x0]
   57838:	cmp	w28, w8
   5783c:	b.ne	5797c <__gmp_doscan@@Base+0x910>  // b.any
   57840:	add	x23, x0, #0x1
   57844:	cmp	x22, x27
   57848:	b.cs	57854 <__gmp_doscan@@Base+0x7e8>  // b.hs, b.nlast
   5784c:	mov	x0, x21
   57850:	b	57878 <__gmp_doscan@@Base+0x80c>
   57854:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57858:	ldr	x8, [x8, #3792]
   5785c:	add	x20, x27, #0x200
   57860:	mov	x0, x21
   57864:	mov	x1, x27
   57868:	ldr	x8, [x8]
   5786c:	mov	x2, x20
   57870:	blr	x8
   57874:	mov	x27, x20
   57878:	ldur	w8, [x29, #-68]
   5787c:	add	x20, x22, #0x1
   57880:	add	w26, w24, #0x1
   57884:	strb	w28, [x0, x22]
   57888:	cmp	w24, w8
   5788c:	b.ge	57aa0 <__gmp_doscan@@Base+0xa34>  // b.tcont
   57890:	ldur	x8, [x29, #-80]
   57894:	mov	x21, x0
   57898:	ldur	x0, [x29, #-64]
   5789c:	ldr	x8, [x8, #16]
   578a0:	blr	x8
   578a4:	ldrb	w8, [x23]
   578a8:	mov	w28, w0
   578ac:	cbz	w8, 5794c <__gmp_doscan@@Base+0x8e0>
   578b0:	cmp	w28, w8
   578b4:	add	x23, x23, #0x1
   578b8:	mov	x22, x20
   578bc:	mov	w24, w26
   578c0:	b.eq	57844 <__gmp_doscan@@Base+0x7d8>  // b.none
   578c4:	b	57b20 <__gmp_doscan@@Base+0xab4>
   578c8:	cmp	x23, x27
   578cc:	b.cc	578f4 <__gmp_doscan@@Base+0x888>  // b.lo, b.ul, b.last
   578d0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   578d4:	ldr	x8, [x8, #3792]
   578d8:	add	x20, x27, #0x200
   578dc:	mov	x1, x27
   578e0:	mov	x2, x20
   578e4:	ldr	x8, [x8]
   578e8:	blr	x8
   578ec:	mov	x27, x20
   578f0:	ldr	x20, [sp, #88]
   578f4:	ldur	w8, [x29, #-68]
   578f8:	add	x22, x23, #0x1
   578fc:	add	w24, w26, #0x1
   57900:	strb	w28, [x0, x23]
   57904:	cmp	w26, w8
   57908:	b.ge	57a88 <__gmp_doscan@@Base+0xa1c>  // b.tcont
   5790c:	ldur	x8, [x29, #-80]
   57910:	mov	x21, x0
   57914:	ldur	x0, [x29, #-64]
   57918:	ldr	x8, [x8, #16]
   5791c:	blr	x8
   57920:	ldr	x8, [x20]
   57924:	mov	w28, w0
   57928:	mov	x0, x21
   5792c:	mov	w26, w24
   57930:	ldrh	w8, [x8, w28, sxtw #1]
   57934:	mov	x23, x22
   57938:	tbnz	w8, #12, 578c8 <__gmp_doscan@@Base+0x85c>
   5793c:	mov	w9, #0x1                   	// #1
   57940:	ldr	w8, [sp, #64]
   57944:	cbnz	w8, 5780c <__gmp_doscan@@Base+0x7a0>
   57948:	b	57c34 <__gmp_doscan@@Base+0xbc8>
   5794c:	add	x23, x22, #0x1
   57950:	ldr	x20, [sp, #88]
   57954:	ldp	w22, w9, [sp, #56]
   57958:	add	w26, w24, #0x1
   5795c:	mov	w8, #0x1                   	// #1
   57960:	mov	x0, x21
   57964:	str	w8, [sp, #68]
   57968:	cmp	w22, #0x10
   5796c:	b.ne	57698 <__gmp_doscan@@Base+0x62c>  // b.any
   57970:	b	577f0 <__gmp_doscan@@Base+0x784>
   57974:	cmp	w8, #0x46
   57978:	b.ne	57a48 <__gmp_doscan@@Base+0x9dc>  // b.any
   5797c:	ldr	w9, [sp, #36]
   57980:	orr	w8, w28, #0x20
   57984:	cbz	w9, 579ac <__gmp_doscan@@Base+0x940>
   57988:	cmp	w8, #0x70
   5798c:	b.ne	579ac <__gmp_doscan@@Base+0x940>  // b.any
   57990:	ldur	x23, [x29, #-64]
   57994:	ldr	w9, [sp, #60]
   57998:	mov	w8, #0xa                   	// #10
   5799c:	str	w8, [sp, #56]
   579a0:	str	x22, [sp, #8]
   579a4:	mov	x0, x21
   579a8:	b	579c8 <__gmp_doscan@@Base+0x95c>
   579ac:	ldur	x23, [x29, #-64]
   579b0:	cmp	w8, #0x65
   579b4:	mov	x0, x21
   579b8:	b.ne	57d98 <__gmp_doscan@@Base+0xd2c>  // b.any
   579bc:	ldr	w8, [sp, #36]
   579c0:	ldr	w9, [sp, #60]
   579c4:	cbnz	w8, 57d7c <__gmp_doscan@@Base+0xd10>
   579c8:	cbz	w9, 57d8c <__gmp_doscan@@Base+0xd20>
   579cc:	cmp	x22, x27
   579d0:	b.cc	57a00 <__gmp_doscan@@Base+0x994>  // b.lo, b.ul, b.last
   579d4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   579d8:	ldr	x8, [x8, #3792]
   579dc:	add	x20, x27, #0x200
   579e0:	mov	x1, x27
   579e4:	mov	x2, x20
   579e8:	ldr	x8, [x8]
   579ec:	mov	w21, w9
   579f0:	blr	x8
   579f4:	mov	x27, x20
   579f8:	ldr	x20, [sp, #88]
   579fc:	mov	w9, w21
   57a00:	ldur	w8, [x29, #-68]
   57a04:	add	x23, x22, #0x1
   57a08:	add	w26, w24, #0x1
   57a0c:	strb	w28, [x0, x22]
   57a10:	cmp	w24, w8
   57a14:	b.ge	57d74 <__gmp_doscan@@Base+0xd08>  // b.tcont
   57a18:	ldur	x24, [x29, #-80]
   57a1c:	mov	x21, x0
   57a20:	ldur	x0, [x29, #-64]
   57a24:	ldr	x8, [x24, #16]
   57a28:	blr	x8
   57a2c:	ldr	w22, [sp, #56]
   57a30:	mov	w28, w0
   57a34:	mov	x0, x21
   57a38:	str	wzr, [sp, #64]
   57a3c:	cmp	w28, #0x2b
   57a40:	b.ne	574c8 <__gmp_doscan@@Base+0x45c>  // b.any
   57a44:	b	5750c <__gmp_doscan@@Base+0x4a0>
   57a48:	ldr	w8, [sp, #84]
   57a4c:	ldr	w9, [sp, #60]
   57a50:	and	w8, w8, #0xff
   57a54:	cmp	w8, #0x51
   57a58:	b.ne	57da0 <__gmp_doscan@@Base+0xd34>  // b.any
   57a5c:	ldur	x23, [x29, #-64]
   57a60:	cmp	w28, #0x2f
   57a64:	mov	x0, x21
   57a68:	b.ne	57d7c <__gmp_doscan@@Base+0xd10>  // b.any
   57a6c:	cbz	w9, 57d88 <__gmp_doscan@@Base+0xd1c>
   57a70:	ldr	w8, [sp, #72]
   57a74:	mov	w9, wzr
   57a78:	str	w8, [sp, #56]
   57a7c:	cmp	x22, x27
   57a80:	b.cs	579d4 <__gmp_doscan@@Base+0x968>  // b.hs, b.nlast
   57a84:	b	57a00 <__gmp_doscan@@Base+0x994>
   57a88:	mov	w26, w24
   57a8c:	ldur	x23, [x29, #-64]
   57a90:	ldur	x24, [x29, #-80]
   57a94:	ldr	w8, [sp, #80]
   57a98:	cbnz	w8, 57abc <__gmp_doscan@@Base+0xa50>
   57a9c:	b	57c5c <__gmp_doscan@@Base+0xbf0>
   57aa0:	ldur	x23, [x29, #-64]
   57aa4:	mov	x22, x20
   57aa8:	ldur	x24, [x29, #-80]
   57aac:	ldr	w9, [sp, #60]
   57ab0:	cbz	w9, 57b2c <__gmp_doscan@@Base+0xac0>
   57ab4:	ldr	w8, [sp, #80]
   57ab8:	cbz	w8, 57c5c <__gmp_doscan@@Base+0xbf0>
   57abc:	mov	x21, x0
   57ac0:	mov	w20, wzr
   57ac4:	ldur	w8, [x29, #-68]
   57ac8:	add	w8, w8, #0x1
   57acc:	cmp	w26, w8
   57ad0:	b.eq	57ae4 <__gmp_doscan@@Base+0xa78>  // b.none
   57ad4:	ldr	x8, [x24, #24]
   57ad8:	mov	w0, w28
   57adc:	mov	x1, x23
   57ae0:	blr	x8
   57ae4:	adrp	x28, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57ae8:	ldr	x28, [x28, #4016]
   57aec:	mov	x0, x21
   57af0:	mov	x1, x27
   57af4:	ldr	x8, [x28]
   57af8:	blr	x8
   57afc:	cbnz	w20, 57de0 <__gmp_doscan@@Base+0xd74>
   57b00:	ldr	x27, [sp, #48]
   57b04:	sub	w8, w26, #0x1
   57b08:	cmn	w26, #0x1
   57b0c:	stur	w8, [x29, #-52]
   57b10:	b.eq	57dd0 <__gmp_doscan@@Base+0xd64>  // b.none
   57b14:	cbz	w26, 57dec <__gmp_doscan@@Base+0xd80>
   57b18:	add	w20, w26, w19
   57b1c:	b	570f8 <__gmp_doscan@@Base+0x8c>
   57b20:	ldur	x23, [x29, #-64]
   57b24:	ldur	x24, [x29, #-80]
   57b28:	b	57b30 <__gmp_doscan@@Base+0xac4>
   57b2c:	mov	x21, x0
   57b30:	mov	w20, #0x1                   	// #1
   57b34:	ldur	w8, [x29, #-68]
   57b38:	add	w8, w8, #0x1
   57b3c:	cmp	w26, w8
   57b40:	b.eq	57ae4 <__gmp_doscan@@Base+0xa78>  // b.none
   57b44:	b	57ad4 <__gmp_doscan@@Base+0xa68>
   57b48:	mov	x25, x9
   57b4c:	ldur	x23, [x29, #-64]
   57b50:	ldur	x24, [x29, #-80]
   57b54:	cmp	w8, #0x5d
   57b58:	b.eq	57b68 <__gmp_doscan@@Base+0xafc>  // b.none
   57b5c:	cmp	w8, #0x5d
   57b60:	b.eq	57b74 <__gmp_doscan@@Base+0xb08>  // b.none
   57b64:	cbz	w8, 57df0 <__gmp_doscan@@Base+0xd84>
   57b68:	ldrb	w8, [x25], #1
   57b6c:	cmp	w8, #0x5d
   57b70:	b.ne	57b64 <__gmp_doscan@@Base+0xaf8>  // b.any
   57b74:	sub	x19, x25, x1
   57b78:	mov	x0, x27
   57b7c:	mov	x2, x19
   57b80:	bl	bed0 <memcpy@plt>
   57b84:	add	x8, x27, x19
   57b88:	mov	w9, #0x6e25                	// #28197
   57b8c:	strh	w9, [x8]
   57b90:	strb	wzr, [x8, #2]
   57b94:	mov	w8, #0xffffffff            	// #-1
   57b98:	stur	w8, [x29, #-52]
   57b9c:	ldr	w8, [sp, #80]
   57ba0:	cbz	w8, 57bc0 <__gmp_doscan@@Base+0xb54>
   57ba4:	ldr	x8, [x24]
   57ba8:	sub	x2, x29, #0x34
   57bac:	mov	x0, x23
   57bb0:	mov	x1, x27
   57bb4:	mov	x3, xzr
   57bb8:	blr	x8
   57bbc:	b	57c0c <__gmp_doscan@@Base+0xba0>
   57bc0:	ldursw	x8, [x29, #-24]
   57bc4:	tbz	w8, #31, 57be4 <__gmp_doscan@@Base+0xb78>
   57bc8:	add	w9, w8, #0x8
   57bcc:	cmn	w8, #0x8
   57bd0:	stur	w9, [x29, #-24]
   57bd4:	b.gt	57be4 <__gmp_doscan@@Base+0xb78>
   57bd8:	ldur	x9, [x29, #-40]
   57bdc:	add	x8, x9, x8
   57be0:	b	57bf0 <__gmp_doscan@@Base+0xb84>
   57be4:	ldur	x8, [x29, #-48]
   57be8:	add	x9, x8, #0x8
   57bec:	stur	x9, [x29, #-48]
   57bf0:	ldr	x2, [x8]
   57bf4:	ldr	x8, [x24]
   57bf8:	sub	x3, x29, #0x34
   57bfc:	mov	x0, x23
   57c00:	mov	x1, x27
   57c04:	blr	x8
   57c08:	cbz	w0, 57dec <__gmp_doscan@@Base+0xd80>
   57c0c:	cmn	w0, #0x1
   57c10:	b.eq	57dd0 <__gmp_doscan@@Base+0xd64>  // b.none
   57c14:	ldur	w1, [x29, #-52]
   57c18:	cmn	w1, #0x1
   57c1c:	b.eq	57dec <__gmp_doscan@@Base+0xd80>  // b.none
   57c20:	ldr	x8, [x24, #8]
   57c24:	mov	x0, x23
   57c28:	add	w20, w1, w20
   57c2c:	blr	x8
   57c30:	b	570f8 <__gmp_doscan@@Base+0x8c>
   57c34:	mov	w26, w24
   57c38:	ldur	x23, [x29, #-64]
   57c3c:	ldur	x24, [x29, #-80]
   57c40:	b	57ab0 <__gmp_doscan@@Base+0xa44>
   57c44:	ldur	x23, [x29, #-64]
   57c48:	b	57b30 <__gmp_doscan@@Base+0xac4>
   57c4c:	ldur	x23, [x29, #-64]
   57c50:	mov	w26, w20
   57c54:	ldr	w8, [sp, #80]
   57c58:	cbnz	w8, 57abc <__gmp_doscan@@Base+0xa50>
   57c5c:	cmp	x22, x27
   57c60:	b.cc	57c84 <__gmp_doscan@@Base+0xc18>  // b.lo, b.ul, b.last
   57c64:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57c68:	ldr	x8, [x8, #3792]
   57c6c:	add	x20, x27, #0x200
   57c70:	mov	x1, x27
   57c74:	mov	x2, x20
   57c78:	ldr	x8, [x8]
   57c7c:	blr	x8
   57c80:	mov	x27, x20
   57c84:	ldr	w8, [sp, #84]
   57c88:	strb	wzr, [x0, x22]
   57c8c:	and	w8, w8, #0xff
   57c90:	cmp	w8, #0x5a
   57c94:	b.eq	57d08 <__gmp_doscan@@Base+0xc9c>  // b.none
   57c98:	cmp	w8, #0x51
   57c9c:	b.eq	57d20 <__gmp_doscan@@Base+0xcb4>  // b.none
   57ca0:	cmp	w8, #0x46
   57ca4:	b.ne	57abc <__gmp_doscan@@Base+0xa50>  // b.any
   57ca8:	ldr	x8, [sp, #8]
   57cac:	cbz	x8, 57d38 <__gmp_doscan@@Base+0xccc>
   57cb0:	add	x20, x0, x8
   57cb4:	ldr	w8, [sp, #36]
   57cb8:	ldr	x22, [sp, #24]
   57cbc:	mov	w9, #0x10                  	// #16
   57cc0:	mov	x1, x0
   57cc4:	cmp	w8, #0x0
   57cc8:	mov	w8, #0xa                   	// #10
   57ccc:	csel	w2, w8, w9, eq  // eq = none
   57cd0:	mov	x0, x22
   57cd4:	strb	wzr, [x20], #1
   57cd8:	mov	x21, x1
   57cdc:	bl	c1c0 <__gmpf_set_str@plt>
   57ce0:	sub	x1, x29, #0x10
   57ce4:	mov	w2, #0xa                   	// #10
   57ce8:	mov	x0, x20
   57cec:	bl	cb60 <strtol@plt>
   57cf0:	mov	x2, x0
   57cf4:	tbnz	x0, #63, 57d60 <__gmp_doscan@@Base+0xcf4>
   57cf8:	mov	x0, x22
   57cfc:	mov	x1, x22
   57d00:	bl	cd90 <__gmpf_mul_2exp@plt>
   57d04:	b	57ac0 <__gmp_doscan@@Base+0xa54>
   57d08:	mov	x1, x0
   57d0c:	ldr	x0, [sp, #24]
   57d10:	ldr	w2, [sp, #72]
   57d14:	mov	x21, x1
   57d18:	bl	c0d0 <__gmpz_set_str@plt>
   57d1c:	b	57ac0 <__gmp_doscan@@Base+0xa54>
   57d20:	mov	x1, x0
   57d24:	ldr	x0, [sp, #24]
   57d28:	ldr	w2, [sp, #72]
   57d2c:	mov	x21, x1
   57d30:	bl	bfe0 <__gmpq_set_str@plt>
   57d34:	b	57ac0 <__gmp_doscan@@Base+0xa54>
   57d38:	ldr	w8, [sp, #36]
   57d3c:	mov	x1, x0
   57d40:	ldr	x0, [sp, #24]
   57d44:	mov	w9, #0x10                  	// #16
   57d48:	cmp	w8, #0x0
   57d4c:	mov	w8, #0xa                   	// #10
   57d50:	csel	w2, w8, w9, eq  // eq = none
   57d54:	mov	x21, x1
   57d58:	bl	c1c0 <__gmpf_set_str@plt>
   57d5c:	b	57ac0 <__gmp_doscan@@Base+0xa54>
   57d60:	neg	x2, x2
   57d64:	mov	x0, x22
   57d68:	mov	x1, x22
   57d6c:	bl	d4c0 <__gmpf_div_2exp@plt>
   57d70:	b	57ac0 <__gmp_doscan@@Base+0xa54>
   57d74:	mov	x22, x23
   57d78:	b	57c38 <__gmp_doscan@@Base+0xbcc>
   57d7c:	mov	w26, w24
   57d80:	ldur	x24, [x29, #-80]
   57d84:	b	57ab0 <__gmp_doscan@@Base+0xa44>
   57d88:	mov	w28, #0x2f                  	// #47
   57d8c:	mov	w26, w24
   57d90:	ldur	x24, [x29, #-80]
   57d94:	b	57b30 <__gmp_doscan@@Base+0xac4>
   57d98:	mov	w26, w24
   57d9c:	b	57aa8 <__gmp_doscan@@Base+0xa3c>
   57da0:	mov	w26, w24
   57da4:	ldur	x23, [x29, #-64]
   57da8:	ldur	x24, [x29, #-80]
   57dac:	mov	x0, x21
   57db0:	b	57ab0 <__gmp_doscan@@Base+0xa44>
   57db4:	mov	w26, wzr
   57db8:	b	57df0 <__gmp_doscan@@Base+0xd84>
   57dbc:	mov	w8, #0xfffffffe            	// #-2
   57dc0:	stur	w8, [x29, #-52]
   57dc4:	adrp	x28, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57dc8:	ldr	x27, [sp, #48]
   57dcc:	ldr	x28, [x28, #4016]
   57dd0:	ldr	w26, [sp, #76]
   57dd4:	cbnz	w26, 57df0 <__gmp_doscan@@Base+0xd84>
   57dd8:	mov	w26, #0xffffffff            	// #-1
   57ddc:	b	57df0 <__gmp_doscan@@Base+0xd84>
   57de0:	ldr	x27, [sp, #48]
   57de4:	mov	w8, #0xffffffff            	// #-1
   57de8:	stur	w8, [x29, #-52]
   57dec:	ldr	w26, [sp, #76]
   57df0:	ldr	x8, [x28]
   57df4:	ldr	x1, [sp, #40]
   57df8:	mov	x0, x27
   57dfc:	blr	x8
   57e00:	mov	w0, w26
   57e04:	ldp	x20, x19, [sp, #256]
   57e08:	ldp	x22, x21, [sp, #240]
   57e0c:	ldp	x24, x23, [sp, #224]
   57e10:	ldp	x26, x25, [sp, #208]
   57e14:	ldp	x28, x27, [sp, #192]
   57e18:	ldp	x29, x30, [sp, #176]
   57e1c:	add	sp, sp, #0x110
   57e20:	ret
   57e24:	ldr	x8, [x22, #24]
   57e28:	ldur	x1, [x29, #-64]
   57e2c:	mov	w19, w0
   57e30:	blr	x8
   57e34:	cbnz	w26, 57df0 <__gmp_doscan@@Base+0xd84>
   57e38:	cmn	w19, #0x1
   57e3c:	b.eq	57dd8 <__gmp_doscan@@Base+0xd6c>  // b.none
   57e40:	b	57df0 <__gmp_doscan@@Base+0xd84>

0000000000057e44 <__gmp_fscanf@@Base>:
   57e44:	sub	sp, sp, #0x100
   57e48:	stp	x29, x30, [sp, #240]
   57e4c:	add	x29, sp, #0xf0
   57e50:	mov	x9, #0xffffffffffffffd0    	// #-48
   57e54:	mov	x10, sp
   57e58:	sub	x11, x29, #0x70
   57e5c:	movk	x9, #0xff80, lsl #32
   57e60:	add	x12, x29, #0x10
   57e64:	add	x10, x10, #0x80
   57e68:	add	x11, x11, #0x30
   57e6c:	stp	x10, x9, [x29, #-16]
   57e70:	stp	x12, x11, [x29, #-32]
   57e74:	stp	x2, x3, [x29, #-112]
   57e78:	stp	x4, x5, [x29, #-96]
   57e7c:	stp	x6, x7, [x29, #-80]
   57e80:	stp	q1, q2, [sp, #16]
   57e84:	str	q0, [sp]
   57e88:	ldp	q0, q1, [x29, #-32]
   57e8c:	mov	x8, x1
   57e90:	mov	x1, x0
   57e94:	stp	q3, q4, [sp, #48]
   57e98:	stp	q5, q6, [sp, #80]
   57e9c:	str	q7, [sp, #112]
   57ea0:	stp	q0, q1, [x29, #-64]
   57ea4:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57ea8:	ldr	x0, [x0, #4064]
   57eac:	sub	x3, x29, #0x40
   57eb0:	mov	x2, x8
   57eb4:	bl	c0f0 <__gmp_doscan@plt>
   57eb8:	ldp	x29, x30, [sp, #240]
   57ebc:	add	sp, sp, #0x100
   57ec0:	ret
   57ec4:	ret

0000000000057ec8 <__gmp_scanf@@Base>:
   57ec8:	sub	sp, sp, #0x120
   57ecc:	stp	x29, x30, [sp, #256]
   57ed0:	add	x29, sp, #0x100
   57ed4:	mov	x9, #0xffffffffffffffc8    	// #-56
   57ed8:	mov	x10, sp
   57edc:	sub	x11, x29, #0x78
   57ee0:	str	x28, [sp, #272]
   57ee4:	stp	x1, x2, [x29, #-120]
   57ee8:	stp	x3, x4, [x29, #-104]
   57eec:	stp	x5, x6, [x29, #-88]
   57ef0:	stur	x7, [x29, #-72]
   57ef4:	stp	q0, q1, [sp]
   57ef8:	stp	q2, q3, [sp, #32]
   57efc:	stp	q4, q5, [sp, #64]
   57f00:	movk	x9, #0xff80, lsl #32
   57f04:	add	x12, x29, #0x20
   57f08:	adrp	x13, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57f0c:	add	x10, x10, #0x80
   57f10:	add	x11, x11, #0x38
   57f14:	ldr	x13, [x13, #3888]
   57f18:	stp	x10, x9, [x29, #-16]
   57f1c:	stp	x12, x11, [x29, #-32]
   57f20:	ldp	q0, q1, [x29, #-32]
   57f24:	mov	x8, x0
   57f28:	stp	q6, q7, [sp, #96]
   57f2c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57f30:	stp	q0, q1, [x29, #-64]
   57f34:	ldr	x1, [x13]
   57f38:	ldr	x0, [x0, #4064]
   57f3c:	sub	x3, x29, #0x40
   57f40:	mov	x2, x8
   57f44:	bl	c0f0 <__gmp_doscan@plt>
   57f48:	ldr	x28, [sp, #272]
   57f4c:	ldp	x29, x30, [sp, #256]
   57f50:	add	sp, sp, #0x120
   57f54:	ret

0000000000057f58 <__gmp_sscanf@@Base>:
   57f58:	sub	sp, sp, #0x120
   57f5c:	stp	x29, x30, [sp, #256]
   57f60:	add	x29, sp, #0x100
   57f64:	mov	x10, #0xffffffffffffffd0    	// #-48
   57f68:	mov	x11, sp
   57f6c:	add	x12, sp, #0x80
   57f70:	movk	x10, #0xff80, lsl #32
   57f74:	add	x13, x29, #0x20
   57f78:	add	x11, x11, #0x80
   57f7c:	add	x12, x12, #0x30
   57f80:	sub	x9, x29, #0x28
   57f84:	stp	x11, x10, [x29, #-24]
   57f88:	stp	x13, x12, [x29, #-40]
   57f8c:	stp	q1, q2, [sp, #16]
   57f90:	str	q0, [sp]
   57f94:	ldp	q0, q1, [x9]
   57f98:	str	x28, [sp, #272]
   57f9c:	stp	x2, x3, [sp, #128]
   57fa0:	stp	x4, x5, [sp, #144]
   57fa4:	stp	x6, x7, [sp, #160]
   57fa8:	stp	q3, q4, [sp, #48]
   57fac:	stp	q5, q6, [sp, #80]
   57fb0:	str	q7, [sp, #112]
   57fb4:	stur	x0, [x29, #-8]
   57fb8:	stp	q0, q1, [x29, #-80]
   57fbc:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   57fc0:	ldr	x0, [x0, #3984]
   57fc4:	mov	x8, x1
   57fc8:	sub	x1, x29, #0x8
   57fcc:	sub	x3, x29, #0x50
   57fd0:	mov	x2, x8
   57fd4:	bl	c0f0 <__gmp_doscan@plt>
   57fd8:	ldr	x28, [sp, #272]
   57fdc:	ldp	x29, x30, [sp, #256]
   57fe0:	add	sp, sp, #0x120
   57fe4:	ret
   57fe8:	sub	sp, sp, #0xe0
   57fec:	stp	x29, x30, [sp, #208]
   57ff0:	add	x29, sp, #0xd0
   57ff4:	mov	x8, #0xffffffffffffffd0    	// #-48
   57ff8:	mov	x10, sp
   57ffc:	movk	x8, #0xff80, lsl #32
   58000:	sub	x11, x29, #0x50
   58004:	add	x10, x10, #0x80
   58008:	add	x12, x29, #0x10
   5800c:	mov	w9, #0xffffffd0            	// #-48
   58010:	add	x11, x11, #0x30
   58014:	stp	x10, x8, [x29, #-16]
   58018:	mov	w8, #0xffffffd0            	// #-48
   5801c:	stp	x2, x3, [x29, #-80]
   58020:	stp	x4, x5, [x29, #-64]
   58024:	stp	x6, x7, [x29, #-48]
   58028:	stp	q1, q2, [sp, #16]
   5802c:	stp	q3, q4, [sp, #48]
   58030:	str	q0, [sp]
   58034:	stp	q5, q6, [sp, #80]
   58038:	str	q7, [sp, #112]
   5803c:	stp	x12, x11, [x29, #-32]
   58040:	tbz	w9, #31, 5806c <__gmp_sscanf@@Base+0x114>
   58044:	add	w8, w9, #0x8
   58048:	cmn	w9, #0x8
   5804c:	stur	w8, [x29, #-8]
   58050:	b.gt	5806c <__gmp_sscanf@@Base+0x114>
   58054:	ldur	x9, [x29, #-24]
   58058:	mov	x10, #0xffffffffffffffd0    	// #-48
   5805c:	add	x9, x9, x10
   58060:	ldr	x2, [x9]
   58064:	tbz	w8, #31, 5809c <__gmp_sscanf@@Base+0x144>
   58068:	b	58080 <__gmp_sscanf@@Base+0x128>
   5806c:	ldur	x9, [x29, #-32]
   58070:	add	x10, x9, #0x8
   58074:	stur	x10, [x29, #-32]
   58078:	ldr	x2, [x9]
   5807c:	tbz	w8, #31, 5809c <__gmp_sscanf@@Base+0x144>
   58080:	add	w9, w8, #0x8
   58084:	cmn	w8, #0x8
   58088:	stur	w9, [x29, #-8]
   5808c:	b.gt	5809c <__gmp_sscanf@@Base+0x144>
   58090:	ldur	x9, [x29, #-24]
   58094:	add	x8, x9, w8, sxtw
   58098:	b	580a8 <__gmp_sscanf@@Base+0x150>
   5809c:	ldur	x8, [x29, #-32]
   580a0:	add	x9, x8, #0x8
   580a4:	stur	x9, [x29, #-32]
   580a8:	ldr	x3, [x8]
   580ac:	ldr	x0, [x0]
   580b0:	bl	d100 <__isoc99_sscanf@plt>
   580b4:	ldp	x29, x30, [sp, #208]
   580b8:	add	sp, sp, #0xe0
   580bc:	ret
   580c0:	ldr	x8, [x0]
   580c4:	add	x8, x8, w1, sxtw
   580c8:	str	x8, [x0]
   580cc:	ret
   580d0:	ldr	x9, [x0]
   580d4:	mov	x8, x0
   580d8:	ldrb	w0, [x9]
   580dc:	cbz	w0, 580ec <__gmp_sscanf@@Base+0x194>
   580e0:	add	x9, x9, #0x1
   580e4:	str	x9, [x8]
   580e8:	ret
   580ec:	mov	w0, #0xffffffff            	// #-1
   580f0:	ret
   580f4:	cmn	w0, #0x1
   580f8:	b.eq	58108 <__gmp_sscanf@@Base+0x1b0>  // b.none
   580fc:	ldr	x8, [x1]
   58100:	sub	x8, x8, #0x1
   58104:	str	x8, [x1]
   58108:	ret

000000000005810c <__gmp_vfscanf@@Base>:
   5810c:	sub	sp, sp, #0x30
   58110:	stp	x29, x30, [sp, #32]
   58114:	ldp	q1, q0, [x2]
   58118:	mov	x8, x1
   5811c:	mov	x1, x0
   58120:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58124:	stp	q1, q0, [sp]
   58128:	ldr	x0, [x0, #4064]
   5812c:	mov	x3, sp
   58130:	mov	x2, x8
   58134:	add	x29, sp, #0x20
   58138:	bl	c0f0 <__gmp_doscan@plt>
   5813c:	ldp	x29, x30, [sp, #32]
   58140:	add	sp, sp, #0x30
   58144:	ret

0000000000058148 <__gmp_vscanf@@Base>:
   58148:	sub	sp, sp, #0x30
   5814c:	stp	x29, x30, [sp, #32]
   58150:	ldp	q0, q1, [x1]
   58154:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58158:	ldr	x8, [x8, #3888]
   5815c:	mov	x2, x0
   58160:	stp	q0, q1, [sp]
   58164:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58168:	ldr	x1, [x8]
   5816c:	ldr	x0, [x0, #4064]
   58170:	mov	x3, sp
   58174:	add	x29, sp, #0x20
   58178:	bl	c0f0 <__gmp_doscan@plt>
   5817c:	ldp	x29, x30, [sp, #32]
   58180:	add	sp, sp, #0x30
   58184:	ret

0000000000058188 <__gmp_vsscanf@@Base>:
   58188:	sub	sp, sp, #0x40
   5818c:	stp	x29, x30, [sp, #48]
   58190:	add	x29, sp, #0x30
   58194:	stur	x0, [x29, #-8]
   58198:	ldp	q1, q0, [x2]
   5819c:	adrp	x0, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   581a0:	mov	x8, x1
   581a4:	sub	x1, x29, #0x8
   581a8:	stp	q1, q0, [sp]
   581ac:	ldr	x0, [x0, #3984]
   581b0:	mov	x3, sp
   581b4:	mov	x2, x8
   581b8:	bl	c0f0 <__gmp_doscan@plt>
   581bc:	ldp	x29, x30, [sp, #48]
   581c0:	add	sp, sp, #0x40
   581c4:	ret

00000000000581c8 <__gmp_randinit@@Base>:
   581c8:	sub	sp, sp, #0xe0
   581cc:	stp	x29, x30, [sp, #208]
   581d0:	add	x29, sp, #0xd0
   581d4:	mov	x8, #0xffffffffffffffd0    	// #-48
   581d8:	mov	x9, sp
   581dc:	sub	x10, x29, #0x50
   581e0:	movk	x8, #0xff80, lsl #32
   581e4:	add	x11, x29, #0x10
   581e8:	add	x9, x9, #0x80
   581ec:	add	x10, x10, #0x30
   581f0:	stp	x2, x3, [x29, #-80]
   581f4:	stp	x4, x5, [x29, #-64]
   581f8:	stp	x6, x7, [x29, #-48]
   581fc:	stp	q1, q2, [sp, #16]
   58200:	stp	q3, q4, [sp, #48]
   58204:	str	q0, [sp]
   58208:	stp	q5, q6, [sp, #80]
   5820c:	str	q7, [sp, #112]
   58210:	stp	x9, x8, [x29, #-16]
   58214:	stp	x11, x10, [x29, #-32]
   58218:	cbz	w1, 58238 <__gmp_randinit@@Base+0x70>
   5821c:	mov	w8, #0x1                   	// #1
   58220:	adrp	x9, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58224:	ldr	x9, [x9, #3896]
   58228:	ldr	w10, [x9]
   5822c:	orr	w8, w10, w8
   58230:	str	w8, [x9]
   58234:	b	58284 <__gmp_randinit@@Base+0xbc>
   58238:	ldursw	x8, [x29, #-8]
   5823c:	tbz	w8, #31, 5826c <__gmp_randinit@@Base+0xa4>
   58240:	add	w9, w8, #0x8
   58244:	cmn	w8, #0x8
   58248:	stur	w9, [x29, #-8]
   5824c:	b.gt	5826c <__gmp_randinit@@Base+0xa4>
   58250:	ldur	x9, [x29, #-24]
   58254:	add	x8, x9, x8
   58258:	ldr	x1, [x8]
   5825c:	bl	d190 <__gmp_randinit_lc_2exp_size@plt>
   58260:	cbnz	w0, 58284 <__gmp_randinit@@Base+0xbc>
   58264:	mov	w8, #0x8                   	// #8
   58268:	b	58220 <__gmp_randinit@@Base+0x58>
   5826c:	ldur	x8, [x29, #-32]
   58270:	add	x9, x8, #0x8
   58274:	stur	x9, [x29, #-32]
   58278:	ldr	x1, [x8]
   5827c:	bl	d190 <__gmp_randinit_lc_2exp_size@plt>
   58280:	cbz	w0, 58264 <__gmp_randinit@@Base+0x9c>
   58284:	ldp	x29, x30, [sp, #208]
   58288:	add	sp, sp, #0xe0
   5828c:	ret

0000000000058290 <__gmp_randclear@@Base>:
   58290:	ldr	x8, [x0, #24]
   58294:	ldr	x1, [x8, #16]
   58298:	br	x1

000000000005829c <__gmp_randinit_default@@Base>:
   5829c:	b	cb00 <__gmp_randinit_mt@plt>

00000000000582a0 <__gmp_randinit_set@@Base>:
   582a0:	ldr	x8, [x1, #24]
   582a4:	ldr	x2, [x8, #24]
   582a8:	br	x2

00000000000582ac <__gmp_randinit_lc_2exp_size@@Base>:
   582ac:	sub	sp, sp, #0x30
   582b0:	stp	x20, x19, [sp, #32]
   582b4:	cmp	x1, #0x10
   582b8:	mov	x19, x0
   582bc:	stp	x29, x30, [sp, #16]
   582c0:	add	x29, sp, #0x10
   582c4:	b.ls	582dc <__gmp_randinit_lc_2exp_size@@Base+0x30>  // b.plast
   582c8:	cmp	x1, #0x11
   582cc:	b.ne	58324 <__gmp_randinit_lc_2exp_size@@Base+0x78>  // b.any
   582d0:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   582d4:	add	x20, x20, #0xb00
   582d8:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   582dc:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   582e0:	add	x20, x20, #0xad0
   582e4:	ldr	x1, [x20, #8]
   582e8:	mov	x0, sp
   582ec:	mov	w2, #0x10                  	// #16
   582f0:	bl	d0a0 <__gmpz_init_set_str@plt>
   582f4:	ldr	x2, [x20, #16]
   582f8:	ldr	x3, [x20]
   582fc:	mov	x1, sp
   58300:	mov	x0, x19
   58304:	bl	cf40 <__gmp_randinit_lc_2exp@plt>
   58308:	mov	x0, sp
   5830c:	bl	cb50 <__gmpz_clear@plt>
   58310:	mov	w0, #0x1                   	// #1
   58314:	ldp	x20, x19, [sp, #32]
   58318:	ldp	x29, x30, [sp, #16]
   5831c:	add	sp, sp, #0x30
   58320:	ret
   58324:	cmp	x1, #0x13
   58328:	b.cc	58340 <__gmp_randinit_lc_2exp_size@@Base+0x94>  // b.lo, b.ul, b.last
   5832c:	cmp	x1, #0x13
   58330:	b.ne	5834c <__gmp_randinit_lc_2exp_size@@Base+0xa0>  // b.any
   58334:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58338:	add	x20, x20, #0xb60
   5833c:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   58340:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58344:	add	x20, x20, #0xb30
   58348:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   5834c:	cmp	x1, #0x15
   58350:	b.cc	583a8 <__gmp_randinit_lc_2exp_size@@Base+0xfc>  // b.lo, b.ul, b.last
   58354:	cmp	x1, #0x1d
   58358:	b.cc	583b4 <__gmp_randinit_lc_2exp_size@@Base+0x108>  // b.lo, b.ul, b.last
   5835c:	cmp	x1, #0x21
   58360:	b.cc	583c0 <__gmp_randinit_lc_2exp_size@@Base+0x114>  // b.lo, b.ul, b.last
   58364:	cmp	x1, #0x33
   58368:	b.cc	583cc <__gmp_randinit_lc_2exp_size@@Base+0x120>  // b.lo, b.ul, b.last
   5836c:	cmp	x1, #0x41
   58370:	b.cc	583d8 <__gmp_randinit_lc_2exp_size@@Base+0x12c>  // b.lo, b.ul, b.last
   58374:	cmp	x1, #0x4f
   58378:	b.cc	583e4 <__gmp_randinit_lc_2exp_size@@Base+0x138>  // b.lo, b.ul, b.last
   5837c:	cmp	x1, #0x63
   58380:	b.cc	583f0 <__gmp_randinit_lc_2exp_size@@Base+0x144>  // b.lo, b.ul, b.last
   58384:	cmp	x1, #0x65
   58388:	b.cc	583fc <__gmp_randinit_lc_2exp_size@@Base+0x150>  // b.lo, b.ul, b.last
   5838c:	cmp	x1, #0x80
   58390:	b.ls	58408 <__gmp_randinit_lc_2exp_size@@Base+0x15c>  // b.plast
   58394:	mov	w0, wzr
   58398:	ldp	x20, x19, [sp, #32]
   5839c:	ldp	x29, x30, [sp, #16]
   583a0:	add	sp, sp, #0x30
   583a4:	ret
   583a8:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583ac:	add	x20, x20, #0xb90
   583b0:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583b4:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583b8:	add	x20, x20, #0xba8
   583bc:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583c0:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583c4:	add	x20, x20, #0xbc0
   583c8:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583cc:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583d0:	add	x20, x20, #0xbd8
   583d4:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583d8:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583dc:	add	x20, x20, #0xbf0
   583e0:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583e4:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583e8:	add	x20, x20, #0xc08
   583ec:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583f0:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   583f4:	add	x20, x20, #0xc20
   583f8:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   583fc:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58400:	add	x20, x20, #0xc38
   58404:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   58408:	adrp	x20, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   5840c:	add	x20, x20, #0xc50
   58410:	b	582e4 <__gmp_randinit_lc_2exp_size@@Base+0x38>

0000000000058414 <__gmp_randinit_lc_2exp@@Base>:
   58414:	stp	x29, x30, [sp, #-64]!
   58418:	stp	x24, x23, [sp, #16]
   5841c:	stp	x22, x21, [sp, #32]
   58420:	stp	x20, x19, [sp, #48]
   58424:	mov	x29, sp
   58428:	cbz	x3, 58508 <__gmp_randinit_lc_2exp@@Base+0xf4>
   5842c:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58430:	ldr	x8, [x8, #3840]
   58434:	mov	x23, x0
   58438:	add	x9, x3, #0x3f
   5843c:	mov	w0, #0x38                  	// #56
   58440:	ldr	x8, [x8]
   58444:	mov	x19, x3
   58448:	mov	x20, x2
   5844c:	mov	x22, x1
   58450:	lsr	x24, x9, #6
   58454:	blr	x8
   58458:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   5845c:	add	x8, x8, #0xc80
   58460:	mov	x1, x19
   58464:	mov	x21, x0
   58468:	str	x0, [x23, #8]
   5846c:	str	x8, [x23, #24]
   58470:	bl	d130 <__gmpz_init2@plt>
   58474:	cbz	x24, 58488 <__gmp_randinit_lc_2exp@@Base+0x74>
   58478:	ldr	x0, [x21, #8]
   5847c:	lsl	x2, x24, #3
   58480:	mov	w1, wzr
   58484:	bl	c5f0 <memset@plt>
   58488:	ldr	x8, [x21, #8]
   5848c:	add	x23, x21, #0x10
   58490:	str	w24, [x21, #4]
   58494:	mov	w24, #0x1                   	// #1
   58498:	mov	x0, x23
   5849c:	str	x24, [x8]
   584a0:	bl	d250 <__gmpz_init@plt>
   584a4:	mov	x0, x23
   584a8:	mov	x1, x22
   584ac:	mov	x2, x19
   584b0:	bl	d0c0 <__gmpz_fdiv_r_2exp@plt>
   584b4:	ldr	w8, [x21, #20]
   584b8:	cbnz	w8, 584d4 <__gmp_randinit_lc_2exp@@Base+0xc0>
   584bc:	ldr	w8, [x21, #16]
   584c0:	str	w24, [x21, #20]
   584c4:	cmp	w8, #0x0
   584c8:	b.le	584f8 <__gmp_randinit_lc_2exp@@Base+0xe4>
   584cc:	ldr	x0, [x21, #24]
   584d0:	str	xzr, [x0]
   584d4:	cmp	x20, #0x0
   584d8:	cset	w8, ne  // ne = any
   584dc:	stp	x8, x20, [x21, #32]
   584e0:	str	x19, [x21, #48]
   584e4:	ldp	x20, x19, [sp, #48]
   584e8:	ldp	x22, x21, [sp, #32]
   584ec:	ldp	x24, x23, [sp, #16]
   584f0:	ldp	x29, x30, [sp], #64
   584f4:	ret
   584f8:	mov	w1, #0x1                   	// #1
   584fc:	mov	x0, x23
   58500:	bl	c080 <__gmpz_realloc@plt>
   58504:	b	584d0 <__gmp_randinit_lc_2exp@@Base+0xbc>
   58508:	adrp	x0, 65000 <__gmp_oddfac_table@@Base+0xf0>
   5850c:	adrp	x2, 65000 <__gmp_oddfac_table@@Base+0xf0>
   58510:	add	x0, x0, #0x88f
   58514:	add	x2, x2, #0x89a
   58518:	mov	w1, #0x12d                 	// #301
   5851c:	bl	c6c0 <__gmp_assert_fail@plt>
   58520:	stp	x29, x30, [sp, #-32]!
   58524:	stp	x20, x19, [sp, #16]
   58528:	ldr	x19, [x0, #8]
   5852c:	mov	x29, sp
   58530:	ldr	x2, [x19, #48]
   58534:	mov	x0, x19
   58538:	add	x8, x2, #0x3f
   5853c:	lsr	x20, x8, #6
   58540:	bl	d0c0 <__gmpz_fdiv_r_2exp@plt>
   58544:	ldrsw	x8, [x19, #4]
   58548:	subs	x9, x20, x8
   5854c:	b.eq	58564 <__gmp_randinit_lc_2exp@@Base+0x150>  // b.none
   58550:	ldr	x10, [x19, #8]
   58554:	lsl	x2, x9, #3
   58558:	mov	w1, wzr
   5855c:	add	x0, x10, x8, lsl #3
   58560:	bl	c5f0 <memset@plt>
   58564:	str	w20, [x19, #4]
   58568:	ldp	x20, x19, [sp, #16]
   5856c:	ldp	x29, x30, [sp], #32
   58570:	ret
   58574:	stp	x29, x30, [sp, #-96]!
   58578:	stp	x28, x27, [sp, #16]
   5857c:	stp	x26, x25, [sp, #32]
   58580:	stp	x24, x23, [sp, #48]
   58584:	stp	x22, x21, [sp, #64]
   58588:	stp	x20, x19, [sp, #80]
   5858c:	mov	x29, sp
   58590:	sub	sp, sp, #0x10
   58594:	ldr	x8, [x0, #8]
   58598:	stp	x1, xzr, [x29, #-16]
   5859c:	mov	x19, x2
   585a0:	mov	x21, x0
   585a4:	ldr	x24, [x8, #48]
   585a8:	lsr	x25, x24, #1
   585ac:	add	w8, w25, #0x3f
   585b0:	add	w9, w25, #0x7e
   585b4:	cmp	w8, #0x0
   585b8:	csel	w23, w9, w8, lt  // lt = tstop
   585bc:	asr	w8, w23, #6
   585c0:	sbfiz	x8, x8, #3, #32
   585c4:	mov	w9, #0x7f00                	// #32512
   585c8:	cmp	x8, x9
   585cc:	b.hi	586d8 <__gmp_randinit_lc_2exp@@Base+0x2c4>  // b.pmore
   585d0:	add	x8, x8, #0xf
   585d4:	mov	x9, sp
   585d8:	and	x8, x8, #0xfffffffffffffff0
   585dc:	sub	x22, x9, x8
   585e0:	mov	sp, x22
   585e4:	cmp	x19, w25, sxtw
   585e8:	b.cs	586f0 <__gmp_randinit_lc_2exp@@Base+0x2dc>  // b.hs, b.nlast
   585ec:	mov	x26, xzr
   585f0:	cmp	x26, x19
   585f4:	b.eq	58694 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   585f8:	ldur	x11, [x29, #-16]
   585fc:	sub	w9, w19, w26
   58600:	lsr	x8, x26, #3
   58604:	add	w10, w9, #0x3f
   58608:	and	x8, x8, #0x1ffffffffffffff8
   5860c:	add	w9, w9, #0x7e
   58610:	cmp	w10, #0x0
   58614:	add	x23, x11, x8
   58618:	csel	w8, w9, w10, lt  // lt = tstop
   5861c:	mov	x0, x22
   58620:	mov	x1, x21
   58624:	sbfx	x24, x8, #6, #26
   58628:	bl	58854 <__gmp_randinit_lc_2exp@@Base+0x440>
   5862c:	ands	x21, x26, #0x3f
   58630:	b.eq	586bc <__gmp_randinit_lc_2exp@@Base+0x2a8>  // b.none
   58634:	ldr	x20, [x23]
   58638:	mov	x0, x23
   5863c:	mov	x1, x22
   58640:	mov	x2, x24
   58644:	mov	w3, w21
   58648:	bl	c180 <__gmpn_lshift@plt>
   5864c:	ldr	x8, [x23]
   58650:	sub	x9, x26, x21
   58654:	add	x9, x9, x24, lsl #6
   58658:	cmp	x9, x19
   5865c:	orr	x8, x8, x20
   58660:	str	x8, [x23]
   58664:	b.cs	5866c <__gmp_randinit_lc_2exp@@Base+0x258>  // b.hs, b.nlast
   58668:	str	x0, [x23, x24, lsl #3]
   5866c:	ands	x8, x19, #0x3f
   58670:	b.eq	58694 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   58674:	ldur	x12, [x29, #-16]
   58678:	lsr	x9, x19, #3
   5867c:	and	x9, x9, #0x1ffffffffffffff8
   58680:	mov	x11, #0xffffffffffffffff    	// #-1
   58684:	ldr	x10, [x12, x9]
   58688:	lsl	x8, x11, x8
   5868c:	bic	x8, x10, x8
   58690:	str	x8, [x12, x9]
   58694:	ldur	x0, [x29, #-8]
   58698:	cbnz	x0, 58790 <__gmp_randinit_lc_2exp@@Base+0x37c>
   5869c:	mov	sp, x29
   586a0:	ldp	x20, x19, [sp, #80]
   586a4:	ldp	x22, x21, [sp, #64]
   586a8:	ldp	x24, x23, [sp, #48]
   586ac:	ldp	x26, x25, [sp, #32]
   586b0:	ldp	x28, x27, [sp, #16]
   586b4:	ldp	x29, x30, [sp], #96
   586b8:	ret
   586bc:	mov	x0, x23
   586c0:	mov	x1, x22
   586c4:	mov	x2, x24
   586c8:	bl	ca50 <__gmpn_copyi@plt>
   586cc:	ands	x8, x19, #0x3f
   586d0:	b.ne	58674 <__gmp_randinit_lc_2exp@@Base+0x260>  // b.any
   586d4:	b	58694 <__gmp_randinit_lc_2exp@@Base+0x280>
   586d8:	sub	x0, x29, #0x8
   586dc:	mov	x1, x8
   586e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   586e4:	mov	x22, x0
   586e8:	cmp	x19, w25, sxtw
   586ec:	b.cc	585ec <__gmp_randinit_lc_2exp@@Base+0x1d8>  // b.lo, b.ul, b.last
   586f0:	add	w8, w25, #0x3f
   586f4:	cmp	w25, #0x0
   586f8:	csel	w8, w8, w25, lt  // lt = tstop
   586fc:	and	w8, w8, #0xffffffc0
   58700:	sub	w8, w25, w8
   58704:	mov	x26, xzr
   58708:	sbfx	x23, x23, #6, #26
   5870c:	sxtw	x27, w8
   58710:	sbfx	x28, x24, #1, #32
   58714:	b	58734 <__gmp_randinit_lc_2exp@@Base+0x320>
   58718:	mov	x0, x24
   5871c:	mov	x1, x21
   58720:	bl	58854 <__gmp_randinit_lc_2exp@@Base+0x440>
   58724:	add	x26, x26, x28
   58728:	add	x8, x28, x26
   5872c:	cmp	x8, x19
   58730:	b.hi	585f0 <__gmp_randinit_lc_2exp@@Base+0x1dc>  // b.pmore
   58734:	ldur	x9, [x29, #-16]
   58738:	lsr	x8, x26, #3
   5873c:	and	x8, x8, #0x1ffffffffffffff8
   58740:	ands	x25, x26, #0x3f
   58744:	add	x24, x9, x8
   58748:	b.eq	58718 <__gmp_randinit_lc_2exp@@Base+0x304>  // b.none
   5874c:	mov	x0, x22
   58750:	mov	x1, x21
   58754:	bl	58854 <__gmp_randinit_lc_2exp@@Base+0x440>
   58758:	ldr	x20, [x24]
   5875c:	mov	x0, x24
   58760:	mov	x1, x22
   58764:	mov	x2, x23
   58768:	mov	w3, w25
   5876c:	bl	c180 <__gmpn_lshift@plt>
   58770:	ldr	x8, [x24]
   58774:	add	x9, x25, x27
   58778:	cmp	x9, #0x41
   5877c:	orr	x8, x8, x20
   58780:	str	x8, [x24]
   58784:	b.cc	58724 <__gmp_randinit_lc_2exp@@Base+0x310>  // b.lo, b.ul, b.last
   58788:	str	x0, [x24, x23, lsl #3]
   5878c:	b	58724 <__gmp_randinit_lc_2exp@@Base+0x310>
   58790:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   58794:	b	5869c <__gmp_randinit_lc_2exp@@Base+0x288>
   58798:	stp	x29, x30, [sp, #-32]!
   5879c:	str	x19, [sp, #16]
   587a0:	ldr	x19, [x0, #8]
   587a4:	mov	x29, sp
   587a8:	mov	x0, x19
   587ac:	bl	cb50 <__gmpz_clear@plt>
   587b0:	add	x0, x19, #0x10
   587b4:	bl	cb50 <__gmpz_clear@plt>
   587b8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   587bc:	ldr	x8, [x8, #4016]
   587c0:	mov	x0, x19
   587c4:	ldr	x19, [sp, #16]
   587c8:	mov	w1, #0x38                  	// #56
   587cc:	ldr	x2, [x8]
   587d0:	ldp	x29, x30, [sp], #32
   587d4:	br	x2
   587d8:	stp	x29, x30, [sp, #-48]!
   587dc:	str	x21, [sp, #16]
   587e0:	stp	x20, x19, [sp, #32]
   587e4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   587e8:	ldr	x19, [x1, #8]
   587ec:	ldr	x8, [x8, #3840]
   587f0:	mov	x20, x0
   587f4:	mov	w0, #0x38                  	// #56
   587f8:	mov	x29, sp
   587fc:	ldr	x8, [x8]
   58800:	blr	x8
   58804:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   58808:	add	x8, x8, #0xc80
   5880c:	mov	x1, x19
   58810:	mov	x21, x0
   58814:	str	x0, [x20, #8]
   58818:	str	x8, [x20, #24]
   5881c:	bl	bf80 <__gmpz_init_set@plt>
   58820:	add	x0, x21, #0x10
   58824:	add	x1, x19, #0x10
   58828:	bl	bf80 <__gmpz_init_set@plt>
   5882c:	ldr	x8, [x19, #32]
   58830:	str	x8, [x21, #32]
   58834:	ldr	x8, [x19, #40]
   58838:	str	x8, [x21, #40]
   5883c:	ldr	x8, [x19, #48]
   58840:	str	x8, [x21, #48]
   58844:	ldp	x20, x19, [sp, #32]
   58848:	ldr	x21, [sp, #16]
   5884c:	ldp	x29, x30, [sp], #48
   58850:	ret
   58854:	stp	x29, x30, [sp, #-96]!
   58858:	stp	x28, x27, [sp, #16]
   5885c:	stp	x26, x25, [sp, #32]
   58860:	stp	x24, x23, [sp, #48]
   58864:	stp	x22, x21, [sp, #64]
   58868:	stp	x20, x19, [sp, #80]
   5886c:	mov	x29, sp
   58870:	sub	sp, sp, #0x10
   58874:	ldr	x27, [x1, #8]
   58878:	ldr	x26, [x27, #48]
   5887c:	ldrsw	x22, [x27, #4]
   58880:	ldrsw	x23, [x27, #20]
   58884:	ldr	x25, [x27, #8]
   58888:	ldr	x24, [x27, #24]
   5888c:	add	x8, x26, #0x3f
   58890:	add	x28, x23, x22
   58894:	lsr	x20, x8, #6
   58898:	cmp	x28, x20
   5889c:	stp	x0, xzr, [x29, #-16]
   588a0:	b.ge	588e8 <__gmp_randinit_lc_2exp@@Base+0x4d4>  // b.tcont
   588a4:	add	x19, x20, #0x1
   588a8:	lsr	x8, x8, #11
   588ac:	cmp	x8, #0x7e
   588b0:	lsl	x1, x19, #3
   588b4:	b.hi	58a00 <__gmp_randinit_lc_2exp@@Base+0x5ec>  // b.pmore
   588b8:	add	x9, x1, #0xf
   588bc:	mov	x8, sp
   588c0:	and	x9, x9, #0x7ffffffffffffff0
   588c4:	sub	x21, x8, x9
   588c8:	mov	sp, x21
   588cc:	subs	x8, x19, x28
   588d0:	b.eq	58910 <__gmp_randinit_lc_2exp@@Base+0x4fc>  // b.none
   588d4:	add	x0, x21, x28, lsl #3
   588d8:	lsl	x2, x8, #3
   588dc:	mov	w1, wzr
   588e0:	bl	c5f0 <memset@plt>
   588e4:	b	58910 <__gmp_randinit_lc_2exp@@Base+0x4fc>
   588e8:	lsl	x8, x28, #3
   588ec:	add	x1, x8, #0x8
   588f0:	mov	w8, #0x7f00                	// #32512
   588f4:	cmp	x1, x8
   588f8:	b.hi	58a18 <__gmp_randinit_lc_2exp@@Base+0x604>  // b.pmore
   588fc:	add	x9, x1, #0xf
   58900:	mov	x8, sp
   58904:	and	x9, x9, #0xfffffffffffffff0
   58908:	sub	x21, x8, x9
   5890c:	mov	sp, x21
   58910:	mov	x0, x21
   58914:	mov	x1, x25
   58918:	mov	x2, x22
   5891c:	mov	x3, x24
   58920:	mov	x4, x23
   58924:	bl	ccd0 <__gmpn_mul@plt>
   58928:	ldr	x22, [x27, #32]
   5892c:	cbz	x22, 58968 <__gmp_randinit_lc_2exp@@Base+0x554>
   58930:	add	x2, x27, #0x28
   58934:	mov	x0, x21
   58938:	mov	x1, x21
   5893c:	mov	x3, x22
   58940:	bl	ca70 <__gmpn_add_n@plt>
   58944:	cbz	x0, 58968 <__gmp_randinit_lc_2exp@@Base+0x554>
   58948:	cmp	x22, x20
   5894c:	b.ge	58968 <__gmp_randinit_lc_2exp@@Base+0x554>  // b.tcont
   58950:	lsl	x8, x22, #3
   58954:	ldr	x9, [x21, x8]
   58958:	add	x22, x22, #0x1
   5895c:	adds	x9, x9, #0x1
   58960:	str	x9, [x21, x8]
   58964:	b.cs	58948 <__gmp_randinit_lc_2exp@@Base+0x534>  // b.hs, b.nlast
   58968:	lsr	x8, x26, #3
   5896c:	and	x8, x8, #0x1ffffffffffffff8
   58970:	ldr	x9, [x21, x8]
   58974:	mov	x10, #0xffffffffffffffff    	// #-1
   58978:	lsl	x10, x10, x26
   5897c:	mov	x1, x21
   58980:	bic	x9, x9, x10
   58984:	str	x9, [x21, x8]
   58988:	ldr	x0, [x27, #8]
   5898c:	mov	x2, x20
   58990:	bl	ca50 <__gmpn_copyi@plt>
   58994:	lsr	x19, x26, #7
   58998:	sub	x2, x20, x19
   5899c:	cmp	x2, #0x1
   589a0:	b.lt	589d0 <__gmp_randinit_lc_2exp@@Base+0x5bc>  // b.tstop
   589a4:	ubfx	w3, w26, #1, #6
   589a8:	add	x1, x21, x19, lsl #3
   589ac:	cbz	w3, 589c8 <__gmp_randinit_lc_2exp@@Base+0x5b4>
   589b0:	mov	x0, x21
   589b4:	bl	c1a0 <__gmpn_rshift@plt>
   589b8:	ldur	x0, [x29, #-16]
   589bc:	add	x2, x19, #0x1
   589c0:	mov	x1, x21
   589c4:	b	589cc <__gmp_randinit_lc_2exp@@Base+0x5b8>
   589c8:	ldur	x0, [x29, #-16]
   589cc:	bl	ca50 <__gmpn_copyi@plt>
   589d0:	ldur	x0, [x29, #-8]
   589d4:	cbnz	x0, 589f8 <__gmp_randinit_lc_2exp@@Base+0x5e4>
   589d8:	mov	sp, x29
   589dc:	ldp	x20, x19, [sp, #80]
   589e0:	ldp	x22, x21, [sp, #64]
   589e4:	ldp	x24, x23, [sp, #48]
   589e8:	ldp	x26, x25, [sp, #32]
   589ec:	ldp	x28, x27, [sp, #16]
   589f0:	ldp	x29, x30, [sp], #96
   589f4:	ret
   589f8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   589fc:	b	589d8 <__gmp_randinit_lc_2exp@@Base+0x5c4>
   58a00:	sub	x0, x29, #0x8
   58a04:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   58a08:	mov	x21, x0
   58a0c:	subs	x8, x19, x28
   58a10:	b.ne	588d4 <__gmp_randinit_lc_2exp@@Base+0x4c0>  // b.any
   58a14:	b	58910 <__gmp_randinit_lc_2exp@@Base+0x4fc>
   58a18:	sub	x0, x29, #0x8
   58a1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   58a20:	mov	x21, x0
   58a24:	b	58910 <__gmp_randinit_lc_2exp@@Base+0x4fc>

0000000000058a28 <__gmp_mt_recalc_buffer@@Base>:
   58a28:	ld1r	{v0.4s}, [x0]
   58a2c:	mov	w9, #0x7ffffffe            	// #2147483646
   58a30:	dup	v2.4s, w9
   58a34:	mov	w9, #0xb0df                	// #45279
   58a38:	movk	w9, #0x9908, lsl #16
   58a3c:	mov	x8, xzr
   58a40:	movi	v1.4s, #0x80, lsl #24
   58a44:	movi	v3.4s, #0x1
   58a48:	dup	v4.4s, w9
   58a4c:	add	x9, x0, x8
   58a50:	mov	v5.16b, v0.16b
   58a54:	ldur	q0, [x9, #4]
   58a58:	add	x10, x9, #0x634
   58a5c:	ldr	q6, [x10]
   58a60:	add	x8, x8, #0x10
   58a64:	ext	v5.16b, v5.16b, v0.16b, #12
   58a68:	and	v7.16b, v0.16b, v2.16b
   58a6c:	and	v5.16b, v5.16b, v1.16b
   58a70:	orr	v5.16b, v7.16b, v5.16b
   58a74:	ushr	v5.4s, v5.4s, #1
   58a78:	eor	v5.16b, v5.16b, v6.16b
   58a7c:	and	v6.16b, v0.16b, v3.16b
   58a80:	cmeq	v6.4s, v6.4s, #0
   58a84:	bic	v6.16b, v4.16b, v6.16b
   58a88:	eor	v5.16b, v5.16b, v6.16b
   58a8c:	cmp	x8, #0x380
   58a90:	str	q5, [x9]
   58a94:	b.ne	58a4c <__gmp_mt_recalc_buffer@@Base+0x24>  // b.any
   58a98:	ldr	w10, [x0, #900]
   58a9c:	ldr	w11, [x0, #2484]
   58aa0:	mov	w9, v0.s[3]
   58aa4:	ldr	w12, [x0, #904]
   58aa8:	mov	w16, #0x7ffffffe            	// #2147483646
   58aac:	ldr	w14, [x0, #908]
   58ab0:	and	w9, w9, #0x80000000
   58ab4:	dup	v0.4s, w16
   58ab8:	and	w16, w10, #0x7ffffffe
   58abc:	orr	w9, w16, w9
   58ac0:	ldr	w13, [x0, #2488]
   58ac4:	ldr	w15, [x0, #2492]
   58ac8:	eor	w11, w11, w9, lsr #1
   58acc:	mov	w9, #0xb0df                	// #45279
   58ad0:	and	w17, w10, #0x80000000
   58ad4:	and	w18, w12, #0x7ffffffe
   58ad8:	movk	w9, #0x9908, lsl #16
   58adc:	sbfx	w10, w10, #0, #1
   58ae0:	and	w16, w12, #0x80000000
   58ae4:	orr	w17, w18, w17
   58ae8:	and	w18, w14, #0x7ffffffe
   58aec:	and	w10, w10, w9
   58af0:	orr	w16, w18, w16
   58af4:	eor	w10, w11, w10
   58af8:	sbfx	w11, w12, #0, #1
   58afc:	sbfx	w12, w14, #0, #1
   58b00:	eor	w13, w13, w17, lsr #1
   58b04:	eor	w15, w15, w16, lsr #1
   58b08:	and	w11, w11, w9
   58b0c:	and	w12, w12, w9
   58b10:	mov	x8, xzr
   58b14:	eor	w11, w13, w11
   58b18:	dup	v4.4s, w14
   58b1c:	eor	w12, w15, w12
   58b20:	movi	v1.4s, #0x80, lsl #24
   58b24:	movi	v2.4s, #0x1
   58b28:	dup	v3.4s, w9
   58b2c:	str	w10, [x0, #896]
   58b30:	str	w11, [x0, #900]
   58b34:	str	w12, [x0, #904]
   58b38:	add	x10, x0, x8
   58b3c:	ldr	q5, [x10, #912]
   58b40:	add	x8, x8, #0x10
   58b44:	cmp	x8, #0x630
   58b48:	ext	v4.16b, v4.16b, v5.16b, #12
   58b4c:	and	v6.16b, v5.16b, v0.16b
   58b50:	and	v4.16b, v4.16b, v1.16b
   58b54:	orr	v4.16b, v6.16b, v4.16b
   58b58:	ldr	q6, [x10]
   58b5c:	ushr	v4.4s, v4.4s, #1
   58b60:	add	x10, x10, #0x38c
   58b64:	eor	v4.16b, v4.16b, v6.16b
   58b68:	and	v6.16b, v5.16b, v2.16b
   58b6c:	cmeq	v6.4s, v6.4s, #0
   58b70:	bic	v6.16b, v3.16b, v6.16b
   58b74:	eor	v4.16b, v4.16b, v6.16b
   58b78:	str	q4, [x10]
   58b7c:	mov	v4.16b, v5.16b
   58b80:	b.ne	58b38 <__gmp_mt_recalc_buffer@@Base+0x110>  // b.any
   58b84:	ldr	w8, [x0, #2492]
   58b88:	ldr	w10, [x0]
   58b8c:	ldr	w11, [x0, #1584]
   58b90:	and	w8, w8, #0x80000000
   58b94:	and	w12, w10, #0x7ffffffe
   58b98:	sbfx	w10, w10, #0, #1
   58b9c:	orr	w8, w12, w8
   58ba0:	eor	w8, w11, w8, lsr #1
   58ba4:	and	w9, w10, w9
   58ba8:	eor	w8, w8, w9
   58bac:	str	w8, [x0, #2492]
   58bb0:	ret

0000000000058bb4 <__gmp_randget_mt@@Base>:
   58bb4:	str	x19, [sp, #-16]!
   58bb8:	ldr	x12, [x0, #8]
   58bbc:	mov	w10, #0x5680                	// #22144
   58bc0:	mov	w13, #0xb0df                	// #45279
   58bc4:	mov	w9, #0xefc60000            	// #-272236544
   58bc8:	movk	w10, #0x9d2c, lsl #16
   58bcc:	movk	w13, #0x9908, lsl #16
   58bd0:	lsr	x8, x2, #6
   58bd4:	and	w11, w2, #0x3f
   58bd8:	cbz	x8, 58f20 <__gmp_randget_mt@@Base+0x36c>
   58bdc:	ldr	w16, [x12, #2496]
   58be0:	mov	w15, #0x7ffffffe            	// #2147483646
   58be4:	mov	x14, xzr
   58be8:	movi	v0.4s, #0x80, lsl #24
   58bec:	dup	v1.4s, w15
   58bf0:	movi	v2.4s, #0x1
   58bf4:	dup	v3.4s, w13
   58bf8:	b	58c34 <__gmp_randget_mt@@Base+0x80>
   58bfc:	add	w16, w17, #0x1
   58c00:	str	w16, [x12, #2496]
   58c04:	ldr	w17, [x12, w17, sxtw #2]
   58c08:	eor	w17, w17, w17, lsr #11
   58c0c:	and	w18, w10, w17, lsl #7
   58c10:	eor	w17, w18, w17
   58c14:	and	w18, w9, w17, lsl #15
   58c18:	eor	w17, w18, w17
   58c1c:	eor	w17, w17, w17, lsr #18
   58c20:	bfi	x15, x17, #32, #32
   58c24:	str	x15, [x1, x14, lsl #3]
   58c28:	add	x14, x14, #0x1
   58c2c:	cmp	x14, x8
   58c30:	b.eq	58f20 <__gmp_randget_mt@@Base+0x36c>  // b.none
   58c34:	cmp	w16, #0x26f
   58c38:	b.le	58d94 <__gmp_randget_mt@@Base+0x1e0>
   58c3c:	ld1r	{v4.4s}, [x12]
   58c40:	mov	x15, xzr
   58c44:	add	x16, x12, x15
   58c48:	mov	v5.16b, v4.16b
   58c4c:	ldur	q4, [x16, #4]
   58c50:	add	x17, x16, #0x634
   58c54:	ldr	q6, [x17]
   58c58:	add	x15, x15, #0x10
   58c5c:	ext	v5.16b, v5.16b, v4.16b, #12
   58c60:	and	v7.16b, v4.16b, v1.16b
   58c64:	and	v5.16b, v5.16b, v0.16b
   58c68:	orr	v5.16b, v7.16b, v5.16b
   58c6c:	ushr	v5.4s, v5.4s, #1
   58c70:	eor	v5.16b, v5.16b, v6.16b
   58c74:	and	v6.16b, v4.16b, v2.16b
   58c78:	cmeq	v6.4s, v6.4s, #0
   58c7c:	bic	v6.16b, v3.16b, v6.16b
   58c80:	eor	v5.16b, v5.16b, v6.16b
   58c84:	cmp	x15, #0x380
   58c88:	str	q5, [x16]
   58c8c:	b.ne	58c44 <__gmp_randget_mt@@Base+0x90>  // b.any
   58c90:	ldr	w17, [x12, #900]
   58c94:	ldr	w18, [x12, #2484]
   58c98:	ldr	w0, [x12, #904]
   58c9c:	mov	w16, v4.s[3]
   58ca0:	ldr	w2, [x12, #2488]
   58ca4:	ldr	w3, [x12, #908]
   58ca8:	and	w16, w16, #0x80000000
   58cac:	and	w5, w17, #0x7ffffffe
   58cb0:	and	w6, w17, #0x80000000
   58cb4:	orr	w16, w5, w16
   58cb8:	sbfx	w17, w17, #0, #1
   58cbc:	ldr	w4, [x12, #2492]
   58cc0:	and	w7, w0, #0x7ffffffe
   58cc4:	eor	w16, w18, w16, lsr #1
   58cc8:	and	w17, w17, w13
   58ccc:	orr	w6, w7, w6
   58cd0:	eor	w16, w16, w17
   58cd4:	sbfx	w17, w0, #0, #1
   58cd8:	and	w5, w0, #0x80000000
   58cdc:	and	w7, w3, #0x7ffffffe
   58ce0:	eor	w18, w2, w6, lsr #1
   58ce4:	and	w17, w17, w13
   58ce8:	orr	w5, w7, w5
   58cec:	eor	w17, w18, w17
   58cf0:	sbfx	w18, w3, #0, #1
   58cf4:	eor	w2, w4, w5, lsr #1
   58cf8:	and	w18, w18, w13
   58cfc:	mov	x15, xzr
   58d00:	eor	w18, w2, w18
   58d04:	dup	v4.4s, w3
   58d08:	str	w16, [x12, #896]
   58d0c:	str	w17, [x12, #900]
   58d10:	str	w18, [x12, #904]
   58d14:	add	x16, x12, x15
   58d18:	ldr	q5, [x16, #912]
   58d1c:	add	x15, x15, #0x10
   58d20:	cmp	x15, #0x630
   58d24:	ext	v4.16b, v4.16b, v5.16b, #12
   58d28:	and	v6.16b, v5.16b, v1.16b
   58d2c:	and	v4.16b, v4.16b, v0.16b
   58d30:	orr	v4.16b, v6.16b, v4.16b
   58d34:	ldr	q6, [x16]
   58d38:	ushr	v4.4s, v4.4s, #1
   58d3c:	add	x16, x16, #0x38c
   58d40:	eor	v4.16b, v4.16b, v6.16b
   58d44:	and	v6.16b, v5.16b, v2.16b
   58d48:	cmeq	v6.4s, v6.4s, #0
   58d4c:	bic	v6.16b, v3.16b, v6.16b
   58d50:	eor	v4.16b, v4.16b, v6.16b
   58d54:	str	q4, [x16]
   58d58:	mov	v4.16b, v5.16b
   58d5c:	b.ne	58d14 <__gmp_randget_mt@@Base+0x160>  // b.any
   58d60:	ldr	w15, [x12, #2492]
   58d64:	ldr	w17, [x12]
   58d68:	ldr	w18, [x12, #1584]
   58d6c:	mov	w16, wzr
   58d70:	and	w15, w15, #0x80000000
   58d74:	and	w0, w17, #0x7ffffffe
   58d78:	sbfx	w17, w17, #0, #1
   58d7c:	orr	w15, w0, w15
   58d80:	and	w17, w17, w13
   58d84:	eor	w15, w18, w15, lsr #1
   58d88:	eor	w15, w15, w17
   58d8c:	str	w15, [x12, #2492]
   58d90:	str	wzr, [x12, #2496]
   58d94:	add	w17, w16, #0x1
   58d98:	str	w17, [x12, #2496]
   58d9c:	ldr	w15, [x12, w16, sxtw #2]
   58da0:	cmp	w16, #0x26f
   58da4:	eor	w15, w15, w15, lsr #11
   58da8:	and	w18, w10, w15, lsl #7
   58dac:	eor	w15, w18, w15
   58db0:	and	w18, w9, w15, lsl #15
   58db4:	eor	w15, w18, w15
   58db8:	eor	w15, w15, w15, lsr #18
   58dbc:	str	x15, [x1, x14, lsl #3]
   58dc0:	b.lt	58bfc <__gmp_randget_mt@@Base+0x48>  // b.tstop
   58dc4:	ld1r	{v4.4s}, [x12]
   58dc8:	mov	x16, xzr
   58dcc:	add	x17, x12, x16
   58dd0:	mov	v5.16b, v4.16b
   58dd4:	ldur	q4, [x17, #4]
   58dd8:	add	x18, x17, #0x634
   58ddc:	ldr	q6, [x18]
   58de0:	add	x16, x16, #0x10
   58de4:	ext	v5.16b, v5.16b, v4.16b, #12
   58de8:	and	v7.16b, v4.16b, v1.16b
   58dec:	and	v5.16b, v5.16b, v0.16b
   58df0:	orr	v5.16b, v7.16b, v5.16b
   58df4:	ushr	v5.4s, v5.4s, #1
   58df8:	eor	v5.16b, v5.16b, v6.16b
   58dfc:	and	v6.16b, v4.16b, v2.16b
   58e00:	cmeq	v6.4s, v6.4s, #0
   58e04:	bic	v6.16b, v3.16b, v6.16b
   58e08:	eor	v5.16b, v5.16b, v6.16b
   58e0c:	cmp	x16, #0x380
   58e10:	str	q5, [x17]
   58e14:	b.ne	58dcc <__gmp_randget_mt@@Base+0x218>  // b.any
   58e18:	ldr	w18, [x12, #900]
   58e1c:	ldr	w0, [x12, #2484]
   58e20:	ldr	w2, [x12, #904]
   58e24:	mov	w17, v4.s[3]
   58e28:	ldr	w3, [x12, #2488]
   58e2c:	ldr	w4, [x12, #908]
   58e30:	and	w17, w17, #0x80000000
   58e34:	and	w6, w18, #0x7ffffffe
   58e38:	and	w7, w18, #0x80000000
   58e3c:	orr	w17, w6, w17
   58e40:	sbfx	w18, w18, #0, #1
   58e44:	ldr	w5, [x12, #2492]
   58e48:	and	w19, w2, #0x7ffffffe
   58e4c:	eor	w17, w0, w17, lsr #1
   58e50:	and	w18, w18, w13
   58e54:	orr	w7, w19, w7
   58e58:	eor	w17, w17, w18
   58e5c:	sbfx	w18, w2, #0, #1
   58e60:	and	w6, w2, #0x80000000
   58e64:	and	w19, w4, #0x7ffffffe
   58e68:	eor	w0, w3, w7, lsr #1
   58e6c:	and	w18, w18, w13
   58e70:	orr	w6, w19, w6
   58e74:	eor	w18, w0, w18
   58e78:	sbfx	w0, w4, #0, #1
   58e7c:	eor	w3, w5, w6, lsr #1
   58e80:	and	w0, w0, w13
   58e84:	mov	x16, xzr
   58e88:	eor	w0, w3, w0
   58e8c:	dup	v4.4s, w4
   58e90:	str	w17, [x12, #896]
   58e94:	str	w18, [x12, #900]
   58e98:	str	w0, [x12, #904]
   58e9c:	add	x17, x12, x16
   58ea0:	ldr	q5, [x17, #912]
   58ea4:	add	x16, x16, #0x10
   58ea8:	cmp	x16, #0x630
   58eac:	ext	v4.16b, v4.16b, v5.16b, #12
   58eb0:	and	v6.16b, v5.16b, v1.16b
   58eb4:	and	v4.16b, v4.16b, v0.16b
   58eb8:	orr	v4.16b, v6.16b, v4.16b
   58ebc:	ldr	q6, [x17]
   58ec0:	ushr	v4.4s, v4.4s, #1
   58ec4:	add	x17, x17, #0x38c
   58ec8:	eor	v4.16b, v4.16b, v6.16b
   58ecc:	and	v6.16b, v5.16b, v2.16b
   58ed0:	cmeq	v6.4s, v6.4s, #0
   58ed4:	bic	v6.16b, v3.16b, v6.16b
   58ed8:	eor	v4.16b, v4.16b, v6.16b
   58edc:	str	q4, [x17]
   58ee0:	mov	v4.16b, v5.16b
   58ee4:	b.ne	58e9c <__gmp_randget_mt@@Base+0x2e8>  // b.any
   58ee8:	ldr	w16, [x12, #2492]
   58eec:	ldr	w18, [x12]
   58ef0:	ldr	w0, [x12, #1584]
   58ef4:	mov	w17, wzr
   58ef8:	and	w16, w16, #0x80000000
   58efc:	and	w2, w18, #0x7ffffffe
   58f00:	sbfx	w18, w18, #0, #1
   58f04:	orr	w16, w2, w16
   58f08:	and	w18, w18, w13
   58f0c:	eor	w16, w0, w16, lsr #1
   58f10:	eor	w16, w16, w18
   58f14:	str	w16, [x12, #2492]
   58f18:	str	wzr, [x12, #2496]
   58f1c:	b	58bfc <__gmp_randget_mt@@Base+0x48>
   58f20:	cbz	w11, 594a0 <__gmp_randget_mt@@Base+0x8ec>
   58f24:	ldr	w15, [x12, #2496]
   58f28:	cmp	w11, #0x1f
   58f2c:	b.hi	59104 <__gmp_randget_mt@@Base+0x550>  // b.pmore
   58f30:	cmp	w15, #0x270
   58f34:	b.lt	590c8 <__gmp_randget_mt@@Base+0x514>  // b.tstop
   58f38:	ld1r	{v0.4s}, [x12]
   58f3c:	mov	w15, #0x7ffffffe            	// #2147483646
   58f40:	dup	v2.4s, w15
   58f44:	mov	w15, #0xb0df                	// #45279
   58f48:	movk	w15, #0x9908, lsl #16
   58f4c:	mov	x14, xzr
   58f50:	movi	v1.4s, #0x80, lsl #24
   58f54:	movi	v3.4s, #0x1
   58f58:	dup	v4.4s, w15
   58f5c:	add	x15, x12, x14
   58f60:	mov	v5.16b, v0.16b
   58f64:	ldur	q0, [x15, #4]
   58f68:	add	x16, x15, #0x634
   58f6c:	ldr	q6, [x16]
   58f70:	add	x14, x14, #0x10
   58f74:	ext	v5.16b, v5.16b, v0.16b, #12
   58f78:	and	v7.16b, v0.16b, v2.16b
   58f7c:	and	v5.16b, v5.16b, v1.16b
   58f80:	orr	v5.16b, v7.16b, v5.16b
   58f84:	ushr	v5.4s, v5.4s, #1
   58f88:	eor	v5.16b, v5.16b, v6.16b
   58f8c:	and	v6.16b, v0.16b, v3.16b
   58f90:	cmeq	v6.4s, v6.4s, #0
   58f94:	bic	v6.16b, v4.16b, v6.16b
   58f98:	eor	v5.16b, v5.16b, v6.16b
   58f9c:	cmp	x14, #0x380
   58fa0:	str	q5, [x15]
   58fa4:	b.ne	58f5c <__gmp_randget_mt@@Base+0x3a8>  // b.any
   58fa8:	ldr	w16, [x12, #900]
   58fac:	ldr	w17, [x12, #2484]
   58fb0:	ldr	w18, [x12, #904]
   58fb4:	mov	w15, v0.s[3]
   58fb8:	mov	w4, #0x7ffffffe            	// #2147483646
   58fbc:	ldr	w0, [x12, #2488]
   58fc0:	ldr	w2, [x12, #908]
   58fc4:	and	w15, w15, #0x80000000
   58fc8:	dup	v0.4s, w4
   58fcc:	and	w4, w16, #0x7ffffffe
   58fd0:	and	w5, w16, #0x80000000
   58fd4:	orr	w15, w4, w15
   58fd8:	sbfx	w16, w16, #0, #1
   58fdc:	ldr	w3, [x12, #2492]
   58fe0:	and	w6, w18, #0x7ffffffe
   58fe4:	eor	w15, w17, w15, lsr #1
   58fe8:	and	w16, w16, w13
   58fec:	orr	w5, w6, w5
   58ff0:	eor	w15, w15, w16
   58ff4:	sbfx	w16, w18, #0, #1
   58ff8:	and	w4, w18, #0x80000000
   58ffc:	and	w6, w2, #0x7ffffffe
   59000:	eor	w17, w0, w5, lsr #1
   59004:	and	w16, w16, w13
   59008:	orr	w4, w6, w4
   5900c:	eor	w16, w17, w16
   59010:	sbfx	w17, w2, #0, #1
   59014:	str	w15, [x12, #896]
   59018:	mov	w15, #0xb0df                	// #45279
   5901c:	eor	w0, w3, w4, lsr #1
   59020:	and	w17, w17, w13
   59024:	movk	w15, #0x9908, lsl #16
   59028:	mov	x14, xzr
   5902c:	dup	v4.4s, w2
   59030:	eor	w17, w0, w17
   59034:	movi	v1.4s, #0x80, lsl #24
   59038:	movi	v2.4s, #0x1
   5903c:	dup	v3.4s, w15
   59040:	str	w16, [x12, #900]
   59044:	str	w17, [x12, #904]
   59048:	add	x15, x12, x14
   5904c:	ldr	q5, [x15, #912]
   59050:	add	x14, x14, #0x10
   59054:	cmp	x14, #0x630
   59058:	ext	v4.16b, v4.16b, v5.16b, #12
   5905c:	and	v6.16b, v5.16b, v0.16b
   59060:	and	v4.16b, v4.16b, v1.16b
   59064:	orr	v4.16b, v6.16b, v4.16b
   59068:	ldr	q6, [x15]
   5906c:	ushr	v4.4s, v4.4s, #1
   59070:	add	x15, x15, #0x38c
   59074:	eor	v4.16b, v4.16b, v6.16b
   59078:	and	v6.16b, v5.16b, v2.16b
   5907c:	cmeq	v6.4s, v6.4s, #0
   59080:	bic	v6.16b, v3.16b, v6.16b
   59084:	eor	v4.16b, v4.16b, v6.16b
   59088:	str	q4, [x15]
   5908c:	mov	v4.16b, v5.16b
   59090:	b.ne	59048 <__gmp_randget_mt@@Base+0x494>  // b.any
   59094:	ldr	w14, [x12, #2492]
   59098:	ldr	w16, [x12]
   5909c:	ldr	w17, [x12, #1584]
   590a0:	mov	w15, wzr
   590a4:	and	w14, w14, #0x80000000
   590a8:	and	w18, w16, #0x7ffffffe
   590ac:	sbfx	w16, w16, #0, #1
   590b0:	orr	w14, w18, w14
   590b4:	and	w13, w16, w13
   590b8:	eor	w14, w17, w14, lsr #1
   590bc:	eor	w13, w14, w13
   590c0:	str	w13, [x12, #2492]
   590c4:	str	wzr, [x12, #2496]
   590c8:	add	w13, w15, #0x1
   590cc:	str	w13, [x12, #2496]
   590d0:	ldr	w12, [x12, w15, sxtw #2]
   590d4:	mov	x13, #0xffffffffffffffff    	// #-1
   590d8:	eor	w12, w12, w12, lsr #11
   590dc:	and	w10, w10, w12, lsl #7
   590e0:	eor	w10, w10, w12
   590e4:	and	w9, w9, w10, lsl #15
   590e8:	eor	w9, w9, w10
   590ec:	eor	w9, w9, w9, lsr #18
   590f0:	lsl	x10, x13, x11
   590f4:	bic	x9, x9, x10
   590f8:	str	x9, [x1, x8, lsl #3]
   590fc:	ldr	x19, [sp], #16
   59100:	ret
   59104:	cmp	w15, #0x270
   59108:	b.lt	5929c <__gmp_randget_mt@@Base+0x6e8>  // b.tstop
   5910c:	ld1r	{v0.4s}, [x12]
   59110:	mov	w15, #0x7ffffffe            	// #2147483646
   59114:	dup	v2.4s, w15
   59118:	mov	w15, #0xb0df                	// #45279
   5911c:	movk	w15, #0x9908, lsl #16
   59120:	mov	x14, xzr
   59124:	movi	v1.4s, #0x80, lsl #24
   59128:	movi	v3.4s, #0x1
   5912c:	dup	v4.4s, w15
   59130:	add	x15, x12, x14
   59134:	mov	v5.16b, v0.16b
   59138:	ldur	q0, [x15, #4]
   5913c:	add	x16, x15, #0x634
   59140:	ldr	q6, [x16]
   59144:	add	x14, x14, #0x10
   59148:	ext	v5.16b, v5.16b, v0.16b, #12
   5914c:	and	v7.16b, v0.16b, v2.16b
   59150:	and	v5.16b, v5.16b, v1.16b
   59154:	orr	v5.16b, v7.16b, v5.16b
   59158:	ushr	v5.4s, v5.4s, #1
   5915c:	eor	v5.16b, v5.16b, v6.16b
   59160:	and	v6.16b, v0.16b, v3.16b
   59164:	cmeq	v6.4s, v6.4s, #0
   59168:	bic	v6.16b, v4.16b, v6.16b
   5916c:	eor	v5.16b, v5.16b, v6.16b
   59170:	cmp	x14, #0x380
   59174:	str	q5, [x15]
   59178:	b.ne	59130 <__gmp_randget_mt@@Base+0x57c>  // b.any
   5917c:	ldr	w16, [x12, #900]
   59180:	ldr	w17, [x12, #2484]
   59184:	ldr	w18, [x12, #904]
   59188:	mov	w15, v0.s[3]
   5918c:	mov	w4, #0x7ffffffe            	// #2147483646
   59190:	ldr	w0, [x12, #2488]
   59194:	ldr	w2, [x12, #908]
   59198:	and	w15, w15, #0x80000000
   5919c:	dup	v0.4s, w4
   591a0:	and	w4, w16, #0x7ffffffe
   591a4:	and	w5, w16, #0x80000000
   591a8:	orr	w15, w4, w15
   591ac:	sbfx	w16, w16, #0, #1
   591b0:	ldr	w3, [x12, #2492]
   591b4:	and	w6, w18, #0x7ffffffe
   591b8:	eor	w15, w17, w15, lsr #1
   591bc:	and	w16, w16, w13
   591c0:	orr	w5, w6, w5
   591c4:	eor	w15, w15, w16
   591c8:	sbfx	w16, w18, #0, #1
   591cc:	and	w4, w18, #0x80000000
   591d0:	and	w6, w2, #0x7ffffffe
   591d4:	eor	w17, w0, w5, lsr #1
   591d8:	and	w16, w16, w13
   591dc:	orr	w4, w6, w4
   591e0:	eor	w16, w17, w16
   591e4:	sbfx	w17, w2, #0, #1
   591e8:	str	w15, [x12, #896]
   591ec:	mov	w15, #0xb0df                	// #45279
   591f0:	eor	w0, w3, w4, lsr #1
   591f4:	and	w17, w17, w13
   591f8:	movk	w15, #0x9908, lsl #16
   591fc:	mov	x14, xzr
   59200:	dup	v4.4s, w2
   59204:	eor	w17, w0, w17
   59208:	movi	v1.4s, #0x80, lsl #24
   5920c:	movi	v2.4s, #0x1
   59210:	dup	v3.4s, w15
   59214:	str	w16, [x12, #900]
   59218:	str	w17, [x12, #904]
   5921c:	add	x15, x12, x14
   59220:	ldr	q5, [x15, #912]
   59224:	add	x14, x14, #0x10
   59228:	cmp	x14, #0x630
   5922c:	ext	v4.16b, v4.16b, v5.16b, #12
   59230:	and	v6.16b, v5.16b, v0.16b
   59234:	and	v4.16b, v4.16b, v1.16b
   59238:	orr	v4.16b, v6.16b, v4.16b
   5923c:	ldr	q6, [x15]
   59240:	ushr	v4.4s, v4.4s, #1
   59244:	add	x15, x15, #0x38c
   59248:	eor	v4.16b, v4.16b, v6.16b
   5924c:	and	v6.16b, v5.16b, v2.16b
   59250:	cmeq	v6.4s, v6.4s, #0
   59254:	bic	v6.16b, v3.16b, v6.16b
   59258:	eor	v4.16b, v4.16b, v6.16b
   5925c:	str	q4, [x15]
   59260:	mov	v4.16b, v5.16b
   59264:	b.ne	5921c <__gmp_randget_mt@@Base+0x668>  // b.any
   59268:	ldr	w14, [x12, #2492]
   5926c:	ldr	w16, [x12]
   59270:	ldr	w17, [x12, #1584]
   59274:	mov	w15, wzr
   59278:	and	w14, w14, #0x80000000
   5927c:	and	w18, w16, #0x7ffffffe
   59280:	sbfx	w16, w16, #0, #1
   59284:	orr	w14, w18, w14
   59288:	and	w16, w16, w13
   5928c:	eor	w14, w17, w14, lsr #1
   59290:	eor	w14, w14, w16
   59294:	str	w14, [x12, #2492]
   59298:	str	wzr, [x12, #2496]
   5929c:	add	w16, w15, #0x1
   592a0:	str	w16, [x12, #2496]
   592a4:	ldr	w14, [x12, w15, sxtw #2]
   592a8:	cmp	w11, #0x21
   592ac:	eor	w14, w14, w14, lsr #11
   592b0:	and	w17, w10, w14, lsl #7
   592b4:	eor	w14, w17, w14
   592b8:	and	w17, w9, w14, lsl #15
   592bc:	eor	w14, w17, w14
   592c0:	eor	w14, w14, w14, lsr #18
   592c4:	str	x14, [x1, x8, lsl #3]
   592c8:	b.cc	594a0 <__gmp_randget_mt@@Base+0x8ec>  // b.lo, b.ul, b.last
   592cc:	cmp	w15, #0x26f
   592d0:	b.lt	59464 <__gmp_randget_mt@@Base+0x8b0>  // b.tstop
   592d4:	ld1r	{v0.4s}, [x12]
   592d8:	mov	w16, #0x7ffffffe            	// #2147483646
   592dc:	dup	v2.4s, w16
   592e0:	mov	w16, #0xb0df                	// #45279
   592e4:	movk	w16, #0x9908, lsl #16
   592e8:	mov	x15, xzr
   592ec:	movi	v1.4s, #0x80, lsl #24
   592f0:	movi	v3.4s, #0x1
   592f4:	dup	v4.4s, w16
   592f8:	add	x16, x12, x15
   592fc:	mov	v5.16b, v0.16b
   59300:	ldur	q0, [x16, #4]
   59304:	add	x17, x16, #0x634
   59308:	ldr	q6, [x17]
   5930c:	add	x15, x15, #0x10
   59310:	ext	v5.16b, v5.16b, v0.16b, #12
   59314:	and	v7.16b, v0.16b, v2.16b
   59318:	and	v5.16b, v5.16b, v1.16b
   5931c:	orr	v5.16b, v7.16b, v5.16b
   59320:	ushr	v5.4s, v5.4s, #1
   59324:	eor	v5.16b, v5.16b, v6.16b
   59328:	and	v6.16b, v0.16b, v3.16b
   5932c:	cmeq	v6.4s, v6.4s, #0
   59330:	bic	v6.16b, v4.16b, v6.16b
   59334:	eor	v5.16b, v5.16b, v6.16b
   59338:	cmp	x15, #0x380
   5933c:	str	q5, [x16]
   59340:	b.ne	592f8 <__gmp_randget_mt@@Base+0x744>  // b.any
   59344:	ldr	w17, [x12, #900]
   59348:	ldr	w18, [x12, #2484]
   5934c:	ldr	w0, [x12, #904]
   59350:	mov	w16, v0.s[3]
   59354:	mov	w5, #0x7ffffffe            	// #2147483646
   59358:	ldr	w2, [x12, #2488]
   5935c:	ldr	w3, [x12, #908]
   59360:	and	w16, w16, #0x80000000
   59364:	dup	v0.4s, w5
   59368:	and	w5, w17, #0x7ffffffe
   5936c:	and	w6, w17, #0x80000000
   59370:	orr	w16, w5, w16
   59374:	sbfx	w17, w17, #0, #1
   59378:	ldr	w4, [x12, #2492]
   5937c:	and	w7, w0, #0x7ffffffe
   59380:	eor	w16, w18, w16, lsr #1
   59384:	and	w17, w17, w13
   59388:	orr	w6, w7, w6
   5938c:	eor	w16, w16, w17
   59390:	sbfx	w17, w0, #0, #1
   59394:	and	w5, w0, #0x80000000
   59398:	and	w7, w3, #0x7ffffffe
   5939c:	eor	w18, w2, w6, lsr #1
   593a0:	and	w17, w17, w13
   593a4:	orr	w5, w7, w5
   593a8:	eor	w17, w18, w17
   593ac:	sbfx	w18, w3, #0, #1
   593b0:	str	w16, [x12, #896]
   593b4:	mov	w16, #0xb0df                	// #45279
   593b8:	eor	w2, w4, w5, lsr #1
   593bc:	and	w18, w18, w13
   593c0:	movk	w16, #0x9908, lsl #16
   593c4:	mov	x15, xzr
   593c8:	dup	v4.4s, w3
   593cc:	eor	w18, w2, w18
   593d0:	movi	v1.4s, #0x80, lsl #24
   593d4:	movi	v2.4s, #0x1
   593d8:	dup	v3.4s, w16
   593dc:	str	w17, [x12, #900]
   593e0:	str	w18, [x12, #904]
   593e4:	add	x16, x12, x15
   593e8:	ldr	q5, [x16, #912]
   593ec:	add	x15, x15, #0x10
   593f0:	cmp	x15, #0x630
   593f4:	ext	v4.16b, v4.16b, v5.16b, #12
   593f8:	and	v6.16b, v5.16b, v0.16b
   593fc:	and	v4.16b, v4.16b, v1.16b
   59400:	orr	v4.16b, v6.16b, v4.16b
   59404:	ldr	q6, [x16]
   59408:	ushr	v4.4s, v4.4s, #1
   5940c:	add	x16, x16, #0x38c
   59410:	eor	v4.16b, v4.16b, v6.16b
   59414:	and	v6.16b, v5.16b, v2.16b
   59418:	cmeq	v6.4s, v6.4s, #0
   5941c:	bic	v6.16b, v3.16b, v6.16b
   59420:	eor	v4.16b, v4.16b, v6.16b
   59424:	str	q4, [x16]
   59428:	mov	v4.16b, v5.16b
   5942c:	b.ne	593e4 <__gmp_randget_mt@@Base+0x830>  // b.any
   59430:	ldr	w15, [x12, #2492]
   59434:	ldr	w17, [x12]
   59438:	ldr	w18, [x12, #1584]
   5943c:	mov	w16, wzr
   59440:	and	w15, w15, #0x80000000
   59444:	and	w0, w17, #0x7ffffffe
   59448:	sbfx	w17, w17, #0, #1
   5944c:	orr	w15, w0, w15
   59450:	and	w13, w17, w13
   59454:	eor	w15, w18, w15, lsr #1
   59458:	eor	w13, w15, w13
   5945c:	str	w13, [x12, #2492]
   59460:	str	wzr, [x12, #2496]
   59464:	add	w13, w16, #0x1
   59468:	str	w13, [x12, #2496]
   5946c:	ldr	w12, [x12, w16, sxtw #2]
   59470:	sub	w11, w11, #0x20
   59474:	mov	x13, #0xffffffffffffffff    	// #-1
   59478:	eor	w12, w12, w12, lsr #11
   5947c:	and	w10, w10, w12, lsl #7
   59480:	eor	w10, w10, w12
   59484:	and	w9, w9, w10, lsl #15
   59488:	eor	w9, w9, w10
   5948c:	eor	w9, w9, w9, lsr #18
   59490:	lsl	x10, x13, x11
   59494:	bic	w9, w9, w10
   59498:	bfi	x14, x9, #32, #32
   5949c:	str	x14, [x1, x8, lsl #3]
   594a0:	ldr	x19, [sp], #16
   594a4:	ret

00000000000594a8 <__gmp_randclear_mt@@Base>:
   594a8:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   594ac:	ldr	x8, [x8, #4016]
   594b0:	ldrsw	x9, [x0]
   594b4:	ldr	x0, [x0, #8]
   594b8:	ldr	x2, [x8]
   594bc:	lsl	x1, x9, #3
   594c0:	br	x2

00000000000594c4 <__gmp_randiset_mt@@Base>:
   594c4:	stp	x29, x30, [sp, #-32]!
   594c8:	stp	x20, x19, [sp, #16]
   594cc:	ldr	x8, [x1, #24]
   594d0:	mov	x20, x0
   594d4:	mov	x29, sp
   594d8:	mov	x19, x1
   594dc:	str	x8, [x0, #24]
   594e0:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   594e4:	ldr	x8, [x8, #3840]
   594e8:	mov	w0, #0x9c8                 	// #2504
   594ec:	ldr	x8, [x8]
   594f0:	blr	x8
   594f4:	mov	w8, #0x139                 	// #313
   594f8:	str	x0, [x20, #8]
   594fc:	str	w8, [x20]
   59500:	ldr	x8, [x19, #8]
   59504:	add	x9, x8, #0x9c0
   59508:	cmp	x0, x9
   5950c:	b.cs	59538 <__gmp_randiset_mt@@Base+0x74>  // b.hs, b.nlast
   59510:	add	x9, x0, #0x9c0
   59514:	cmp	x9, x8
   59518:	b.ls	59538 <__gmp_randiset_mt@@Base+0x74>  // b.plast
   5951c:	mov	x9, xzr
   59520:	ldr	w10, [x8, x9]
   59524:	str	w10, [x0, x9]
   59528:	add	x9, x9, #0x4
   5952c:	cmp	x9, #0x9c0
   59530:	b.ne	59520 <__gmp_randiset_mt@@Base+0x5c>  // b.any
   59534:	b	59558 <__gmp_randiset_mt@@Base+0x94>
   59538:	mov	x9, xzr
   5953c:	add	x10, x8, x9
   59540:	ldp	q0, q1, [x10]
   59544:	add	x11, x0, x9
   59548:	add	x9, x9, #0x20
   5954c:	cmp	x9, #0x9c0
   59550:	stp	q0, q1, [x11]
   59554:	b.ne	5953c <__gmp_randiset_mt@@Base+0x78>  // b.any
   59558:	ldr	w8, [x8, #2496]
   5955c:	str	w8, [x0, #2496]
   59560:	ldp	x20, x19, [sp, #16]
   59564:	ldp	x29, x30, [sp], #32
   59568:	ret

000000000005956c <__gmp_randinit_mt_noseed@@Base>:
   5956c:	stp	x29, x30, [sp, #-32]!
   59570:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   59574:	add	x8, x8, #0xca0
   59578:	stp	x20, x19, [sp, #16]
   5957c:	str	x8, [x0, #24]
   59580:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   59584:	ldr	x8, [x8, #3840]
   59588:	mov	x19, x0
   5958c:	mov	w0, #0x9c8                 	// #2504
   59590:	mov	x29, sp
   59594:	ldr	x8, [x8]
   59598:	blr	x8
   5959c:	adrp	x1, 65000 <__gmp_oddfac_table@@Base+0xf0>
   595a0:	mov	w8, #0x139                 	// #313
   595a4:	add	x1, x1, #0x8a8
   595a8:	mov	w2, #0x9c0                 	// #2496
   595ac:	mov	x20, x0
   595b0:	str	x0, [x19, #8]
   595b4:	str	w8, [x19]
   595b8:	bl	bed0 <memcpy@plt>
   595bc:	mov	w8, #0x80                  	// #128
   595c0:	str	w8, [x20, #2496]
   595c4:	ldp	x20, x19, [sp, #16]
   595c8:	ldp	x29, x30, [sp], #32
   595cc:	ret

00000000000595d0 <__gmp_randinit_mt@@Base>:
   595d0:	stp	x29, x30, [sp, #-32]!
   595d4:	str	x19, [sp, #16]
   595d8:	mov	x29, sp
   595dc:	mov	x19, x0
   595e0:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   595e4:	adrp	x8, 76000 <__gmp_limbroots_table@@Base+0x10e20>
   595e8:	add	x8, x8, #0xcc0
   595ec:	str	x8, [x19, #24]
   595f0:	ldr	x19, [sp, #16]
   595f4:	ldp	x29, x30, [sp], #32
   595f8:	ret
   595fc:	sub	sp, sp, #0x70
   59600:	stp	x29, x30, [sp, #64]
   59604:	stp	x20, x19, [sp, #96]
   59608:	ldr	x19, [x0, #8]
   5960c:	mov	x20, x1
   59610:	add	x0, sp, #0x10
   59614:	mov	w1, #0x4de2                	// #19938
   59618:	str	x21, [sp, #80]
   5961c:	add	x29, sp, #0x40
   59620:	bl	d130 <__gmpz_init2@plt>
   59624:	mov	x0, sp
   59628:	mov	w1, #0x4de1                	// #19937
   5962c:	bl	d130 <__gmpz_init2@plt>
   59630:	add	x0, sp, #0x10
   59634:	mov	w1, #0x4de1                	// #19937
   59638:	bl	c310 <__gmpz_setbit@plt>
   5963c:	add	x0, sp, #0x10
   59640:	add	x1, sp, #0x10
   59644:	mov	w2, #0x4e3b                	// #20027
   59648:	bl	c120 <__gmpz_sub_ui@plt>
   5964c:	mov	x0, sp
   59650:	add	x2, sp, #0x10
   59654:	mov	x1, x20
   59658:	bl	cdf0 <__gmpz_mod@plt>
   5965c:	add	x0, sp, #0x10
   59660:	bl	cb50 <__gmpz_clear@plt>
   59664:	mov	x0, sp
   59668:	mov	x1, sp
   5966c:	mov	w2, #0x2                   	// #2
   59670:	bl	c8b0 <__gmpz_add_ui@plt>
   59674:	sub	x0, x29, #0x10
   59678:	mov	w1, #0x4de1                	// #19937
   5967c:	bl	d130 <__gmpz_init2@plt>
   59680:	add	x0, sp, #0x20
   59684:	mov	x1, sp
   59688:	bl	bf80 <__gmpz_init_set@plt>
   5968c:	mov	w21, #0x8124                	// #33060
   59690:	mov	w20, #0x20000000            	// #536870912
   59694:	movk	w21, #0x4011, lsl #16
   59698:	b	596a4 <__gmp_randinit_mt@@Base+0xd4>
   5969c:	lsr	x20, x20, #1
   596a0:	cbz	x20, 59704 <__gmp_randinit_mt@@Base+0x134>
   596a4:	mov	x2, sp
   596a8:	mov	x0, sp
   596ac:	mov	x1, sp
   596b0:	bl	c4b0 <__gmpz_mul@plt>
   596b4:	sub	x0, x29, #0x10
   596b8:	mov	x1, sp
   596bc:	mov	w2, #0x4de1                	// #19937
   596c0:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   596c4:	ldur	w8, [x29, #-12]
   596c8:	cbz	w8, 596f0 <__gmp_randinit_mt@@Base+0x120>
   596cc:	mov	x0, sp
   596d0:	mov	x1, sp
   596d4:	mov	w2, #0x4de1                	// #19937
   596d8:	bl	bee0 <__gmpz_tdiv_r_2exp@plt>
   596dc:	mov	x0, sp
   596e0:	sub	x1, x29, #0x10
   596e4:	mov	w2, #0x4e37                	// #20023
   596e8:	bl	d320 <__gmpz_addmul_ui@plt>
   596ec:	b	596b4 <__gmp_randinit_mt@@Base+0xe4>
   596f0:	tst	x21, x20
   596f4:	b.eq	5969c <__gmp_randinit_mt@@Base+0xcc>  // b.none
   596f8:	add	x2, sp, #0x20
   596fc:	eor	x21, x21, x20
   59700:	b	596a8 <__gmp_randinit_mt@@Base+0xd8>
   59704:	sub	x0, x29, #0x10
   59708:	bl	cb50 <__gmpz_clear@plt>
   5970c:	add	x0, sp, #0x20
   59710:	bl	cb50 <__gmpz_clear@plt>
   59714:	mov	x0, sp
   59718:	mov	w1, #0x4de0                	// #19936
   5971c:	bl	c480 <__gmpz_tstbit@plt>
   59720:	cmp	w0, #0x0
   59724:	cset	w8, ne  // ne = any
   59728:	lsl	w8, w8, #31
   5972c:	mov	x20, x19
   59730:	mov	x0, sp
   59734:	mov	w1, #0x4de0                	// #19936
   59738:	str	w8, [x20], #4
   5973c:	bl	c940 <__gmpz_clrbit@plt>
   59740:	sub	x1, x29, #0x10
   59744:	mov	x6, sp
   59748:	mov	w2, #0xffffffff            	// #-1
   5974c:	mov	w3, #0x4                   	// #4
   59750:	mov	x0, x20
   59754:	mov	w4, wzr
   59758:	mov	x5, xzr
   5975c:	bl	d220 <__gmpz_export@plt>
   59760:	mov	x0, sp
   59764:	bl	cb50 <__gmpz_clear@plt>
   59768:	ldur	x21, [x29, #-16]
   5976c:	add	x20, x21, #0x1
   59770:	cmp	x20, #0x26f
   59774:	stur	x20, [x29, #-16]
   59778:	b.hi	597d4 <__gmp_randinit_mt@@Base+0x204>  // b.pmore
   5977c:	mov	w8, #0x9bc                 	// #2492
   59780:	add	x0, x19, x20, lsl #2
   59784:	sub	x2, x8, x21, lsl #2
   59788:	mov	w1, wzr
   5978c:	bl	c5f0 <memset@plt>
   59790:	mov	w8, #0x26f                 	// #623
   59794:	sub	x8, x8, x21
   59798:	cmp	x8, #0x1
   5979c:	b.ls	597bc <__gmp_randinit_mt@@Base+0x1ec>  // b.plast
   597a0:	and	x9, x8, #0xfffffffffffffffe
   597a4:	add	x20, x20, x9
   597a8:	mov	x10, x9
   597ac:	subs	x10, x10, #0x2
   597b0:	b.ne	597ac <__gmp_randinit_mt@@Base+0x1dc>  // b.any
   597b4:	cmp	x8, x9
   597b8:	b.eq	597d0 <__gmp_randinit_mt@@Base+0x200>  // b.none
   597bc:	mov	x8, x20
   597c0:	add	x20, x8, #0x1
   597c4:	cmp	x8, #0x26f
   597c8:	mov	x8, x20
   597cc:	b.cc	597c0 <__gmp_randinit_mt@@Base+0x1f0>  // b.lo, b.ul, b.last
   597d0:	stur	x20, [x29, #-16]
   597d4:	mov	x0, x19
   597d8:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   597dc:	mov	x0, x19
   597e0:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   597e4:	mov	x0, x19
   597e8:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   597ec:	mov	w8, #0x80                  	// #128
   597f0:	str	w8, [x19, #2496]
   597f4:	ldp	x20, x19, [sp, #96]
   597f8:	ldr	x21, [sp, #80]
   597fc:	ldp	x29, x30, [sp, #64]
   59800:	add	sp, sp, #0x70
   59804:	ret

0000000000059808 <__gmp_randseed@@Base>:
   59808:	ldr	x8, [x0, #24]
   5980c:	ldr	x2, [x8]
   59810:	br	x2

0000000000059814 <__gmp_randseed_ui@@Base>:
   59814:	sub	sp, sp, #0x30
   59818:	add	x8, sp, #0x8
   5981c:	cmp	x1, #0x0
   59820:	str	x1, [sp, #8]
   59824:	str	x8, [sp, #24]
   59828:	cset	w8, ne  // ne = any
   5982c:	add	x1, sp, #0x10
   59830:	stp	x29, x30, [sp, #32]
   59834:	add	x29, sp, #0x20
   59838:	str	w8, [sp, #20]
   5983c:	bl	cfb0 <__gmp_randseed@plt>
   59840:	ldp	x29, x30, [sp, #32]
   59844:	add	sp, sp, #0x30
   59848:	ret

000000000005984c <__gmp_urandomb_ui@@Base>:
   5984c:	sub	sp, sp, #0x20
   59850:	stp	x29, x30, [sp, #16]
   59854:	str	xzr, [sp, #8]
   59858:	ldr	x8, [x0, #24]
   5985c:	cmp	x1, #0x40
   59860:	mov	w9, #0x40                  	// #64
   59864:	csel	x2, x1, x9, cc  // cc = lo, ul, last
   59868:	ldr	x8, [x8, #8]
   5986c:	add	x1, sp, #0x8
   59870:	add	x29, sp, #0x10
   59874:	blr	x8
   59878:	ldr	x0, [sp, #8]
   5987c:	ldp	x29, x30, [sp, #16]
   59880:	add	sp, sp, #0x20
   59884:	ret

0000000000059888 <__gmp_urandomm_ui@@Base>:
   59888:	sub	sp, sp, #0x40
   5988c:	stp	x29, x30, [sp, #16]
   59890:	stp	x22, x21, [sp, #32]
   59894:	stp	x20, x19, [sp, #48]
   59898:	add	x29, sp, #0x10
   5989c:	cbz	x1, 5990c <__gmp_urandomm_ui@@Base+0x84>
   598a0:	sub	x9, x1, #0x1
   598a4:	tst	x1, x9
   598a8:	clz	x8, x1
   598ac:	csetm	x9, eq  // eq = none
   598b0:	sub	x8, x9, x8
   598b4:	mov	x19, x1
   598b8:	mov	x20, x0
   598bc:	str	xzr, [sp, #8]
   598c0:	add	x21, x8, #0x40
   598c4:	mov	w22, #0x50                  	// #80
   598c8:	ldr	x8, [x20, #24]
   598cc:	add	x1, sp, #0x8
   598d0:	mov	x0, x20
   598d4:	mov	x2, x21
   598d8:	ldr	x8, [x8, #8]
   598dc:	blr	x8
   598e0:	ldr	x0, [sp, #8]
   598e4:	subs	x8, x0, x19
   598e8:	b.cc	598f8 <__gmp_urandomm_ui@@Base+0x70>  // b.lo, b.ul, b.last
   598ec:	subs	w22, w22, #0x1
   598f0:	b.ne	598c8 <__gmp_urandomm_ui@@Base+0x40>  // b.any
   598f4:	mov	x0, x8
   598f8:	ldp	x20, x19, [sp, #48]
   598fc:	ldp	x22, x21, [sp, #32]
   59900:	ldp	x29, x30, [sp, #16]
   59904:	add	sp, sp, #0x40
   59908:	ret
   5990c:	bl	bfd0 <__gmp_divide_by_zero@plt>

Disassembly of section .fini:

0000000000059910 <.fini>:
   59910:	stp	x29, x30, [sp, #-16]!
   59914:	mov	x29, sp
   59918:	ldp	x29, x30, [sp], #16
   5991c:	ret
