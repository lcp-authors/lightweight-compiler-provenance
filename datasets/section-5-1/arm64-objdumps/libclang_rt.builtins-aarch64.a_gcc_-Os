In archive /home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libclang_rt.builtins-aarch64.a_gcc_-Os:

comparetf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmptf2>:
   0:	sub	sp, sp, #0x10
   4:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
   8:	str	q0, [sp]
   c:	ldp	x1, x2, [sp]
  10:	str	q1, [sp]
  14:	ldp	x4, x3, [sp]
  18:	and	x6, x2, #0x7fffffffffffffff
  1c:	mov	x0, x1
  20:	cmp	x6, x5
  24:	mov	x1, x2
  28:	mov	x2, x3
  2c:	mov	w3, #0x1                   	// #1
  30:	b.hi	40 <__cmptf2+0x40>  // b.pmore
  34:	b.ne	3c <__cmptf2+0x3c>  // b.any
  38:	cbnz	x0, 40 <__cmptf2+0x40>
  3c:	mov	w3, #0x0                   	// #0
  40:	and	x7, x2, #0x7fffffffffffffff
  44:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
  48:	mov	w5, #0x1                   	// #1
  4c:	cmp	x7, x6
  50:	b.hi	60 <__cmptf2+0x60>  // b.pmore
  54:	b.ne	5c <__cmptf2+0x5c>  // b.any
  58:	cbnz	x4, 60 <__cmptf2+0x60>
  5c:	mov	w5, #0x0                   	// #0
  60:	orr	w3, w3, w5
  64:	tbnz	w3, #0, c8 <__cmptf2+0xc8>
  68:	orr	x5, x1, x2
  6c:	orr	x3, x0, x4
  70:	and	x5, x5, #0x7fffffffffffffff
  74:	orr	x3, x3, x5
  78:	cbz	x3, d0 <__cmptf2+0xd0>
  7c:	tst	x1, x2
  80:	b.mi	b4 <__cmptf2+0xb4>  // b.first
  84:	cmp	x2, x1
  88:	b.gt	d8 <__cmptf2+0xd8>
  8c:	b.ne	98 <__cmptf2+0x98>  // b.any
  90:	cmp	x4, x0
  94:	b.hi	d8 <__cmptf2+0xd8>  // b.pmore
  98:	eor	x0, x0, x4
  9c:	eor	x1, x1, x2
  a0:	orr	x0, x0, x1
  a4:	cmp	x0, #0x0
  a8:	cset	w0, ne  // ne = any
  ac:	add	sp, sp, #0x10
  b0:	ret
  b4:	cmp	x1, x2
  b8:	b.gt	d8 <__cmptf2+0xd8>
  bc:	b.ne	98 <__cmptf2+0x98>  // b.any
  c0:	cmp	x0, x4
  c4:	b	94 <__cmptf2+0x94>
  c8:	mov	w0, #0x1                   	// #1
  cc:	b	ac <__cmptf2+0xac>
  d0:	mov	w0, #0x0                   	// #0
  d4:	b	ac <__cmptf2+0xac>
  d8:	mov	w0, #0xffffffff            	// #-1
  dc:	b	ac <__cmptf2+0xac>

00000000000000e0 <__getf2>:
  e0:	sub	sp, sp, #0x10
  e4:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
  e8:	str	q0, [sp]
  ec:	ldp	x1, x2, [sp]
  f0:	str	q1, [sp]
  f4:	ldp	x4, x3, [sp]
  f8:	and	x6, x2, #0x7fffffffffffffff
  fc:	mov	x0, x1
 100:	cmp	x6, x5
 104:	mov	x1, x2
 108:	mov	x2, x3
 10c:	mov	w3, #0x1                   	// #1
 110:	b.hi	120 <__getf2+0x40>  // b.pmore
 114:	b.ne	11c <__getf2+0x3c>  // b.any
 118:	cbnz	x0, 120 <__getf2+0x40>
 11c:	mov	w3, #0x0                   	// #0
 120:	and	x7, x2, #0x7fffffffffffffff
 124:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
 128:	mov	w5, #0x1                   	// #1
 12c:	cmp	x7, x6
 130:	b.hi	140 <__getf2+0x60>  // b.pmore
 134:	b.ne	13c <__getf2+0x5c>  // b.any
 138:	cbnz	x4, 140 <__getf2+0x60>
 13c:	mov	w5, #0x0                   	// #0
 140:	orr	w3, w3, w5
 144:	tbnz	w3, #0, 1b0 <__getf2+0xd0>
 148:	orr	x5, x1, x2
 14c:	orr	x3, x0, x4
 150:	and	x5, x5, #0x7fffffffffffffff
 154:	orr	x3, x3, x5
 158:	cbz	x3, 1a8 <__getf2+0xc8>
 15c:	tst	x1, x2
 160:	b.mi	194 <__getf2+0xb4>  // b.first
 164:	cmp	x2, x1
 168:	b.gt	1b0 <__getf2+0xd0>
 16c:	b.ne	178 <__getf2+0x98>  // b.any
 170:	cmp	x4, x0
 174:	b.hi	1b0 <__getf2+0xd0>  // b.pmore
 178:	eor	x0, x0, x4
 17c:	eor	x1, x1, x2
 180:	orr	x0, x0, x1
 184:	cmp	x0, #0x0
 188:	cset	w0, ne  // ne = any
 18c:	add	sp, sp, #0x10
 190:	ret
 194:	cmp	x1, x2
 198:	b.gt	1b0 <__getf2+0xd0>
 19c:	b.ne	178 <__getf2+0x98>  // b.any
 1a0:	cmp	x0, x4
 1a4:	b	174 <__getf2+0x94>
 1a8:	mov	w0, #0x0                   	// #0
 1ac:	b	18c <__getf2+0xac>
 1b0:	mov	w0, #0xffffffff            	// #-1
 1b4:	b	18c <__getf2+0xac>

00000000000001b8 <__unordtf2>:
 1b8:	sub	sp, sp, #0x10
 1bc:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
 1c0:	mov	w0, #0x1                   	// #1
 1c4:	str	q0, [sp]
 1c8:	ldp	x4, x2, [sp]
 1cc:	str	q1, [sp]
 1d0:	ldp	x3, x1, [sp]
 1d4:	and	x2, x2, #0x7fffffffffffffff
 1d8:	cmp	x2, x5
 1dc:	b.hi	1ec <__unordtf2+0x34>  // b.pmore
 1e0:	b.ne	1e8 <__unordtf2+0x30>  // b.any
 1e4:	cbnz	x4, 1ec <__unordtf2+0x34>
 1e8:	mov	w0, #0x0                   	// #0
 1ec:	and	x1, x1, #0x7fffffffffffffff
 1f0:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 1f4:	mov	w2, #0x1                   	// #1
 1f8:	cmp	x1, x4
 1fc:	b.hi	20c <__unordtf2+0x54>  // b.pmore
 200:	b.ne	208 <__unordtf2+0x50>  // b.any
 204:	cbnz	x3, 20c <__unordtf2+0x54>
 208:	mov	w2, #0x0                   	// #0
 20c:	orr	w0, w0, w2
 210:	and	w0, w0, #0x1
 214:	add	sp, sp, #0x10
 218:	ret

extenddftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extenddftf2>:
   0:	fmov	x1, d0
   4:	stp	x29, x30, [sp, #-32]!
   8:	mov	x2, #0x7fdfffffffffffff    	// #9214364837600034815
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	and	x0, x1, #0x7fffffffffffffff
  18:	and	x19, x1, #0x8000000000000000
  1c:	mov	x1, #0xfff0000000000000    	// #-4503599627370496
  20:	add	x1, x0, x1
  24:	cmp	x1, x2
  28:	b.hi	50 <__extenddftf2+0x50>  // b.pmore
  2c:	mov	x1, #0x3c00000000000000    	// #4323455642275676160
  30:	lsl	x2, x0, #60
  34:	add	x1, x1, x0, lsr #4
  38:	fmov	d0, x2
  3c:	orr	x5, x19, x1
  40:	ldp	x19, x20, [sp, #16]
  44:	fmov	v0.d[1], x5
  48:	ldp	x29, x30, [sp], #32
  4c:	ret
  50:	mov	x1, #0x7fefffffffffffff    	// #9218868437227405311
  54:	cmp	x0, x1
  58:	b.ls	6c <__extenddftf2+0x6c>  // b.plast
  5c:	ubfx	x1, x0, #4, #48
  60:	lsl	x2, x0, #60
  64:	orr	x1, x1, #0x7fff000000000000
  68:	b	38 <__extenddftf2+0x38>
  6c:	cbz	x0, 98 <__extenddftf2+0x98>
  70:	clz	x20, x0
  74:	mov	x1, #0x0                   	// #0
  78:	add	x2, x20, #0x31
  7c:	bl	0 <__ashlti3>
  80:	eor	x3, x1, #0x1000000000000
  84:	mov	w1, #0x3c0c                	// #15372
  88:	sub	w1, w1, w20
  8c:	mov	x2, x0
  90:	orr	x1, x3, x1, lsl #48
  94:	b	38 <__extenddftf2+0x38>
  98:	mov	x2, #0x0                   	// #0
  9c:	mov	x1, #0x0                   	// #0
  a0:	b	38 <__extenddftf2+0x38>

extendsftf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsftf2>:
   0:	fmov	w1, s0
   4:	stp	x29, x30, [sp, #-32]!
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	and	w0, w1, #0x7fffffff
  14:	sub	w2, w0, #0x800, lsl #12
  18:	and	w19, w1, #0x80000000
  1c:	mov	w1, #0x7effffff            	// #2130706431
  20:	cmp	w2, w1
  24:	b.hi	50 <__extendsftf2+0x50>  // b.pmore
  28:	ubfiz	x0, x0, #25, #31
  2c:	mov	x1, #0x3f80000000000000    	// #4575657221408423936
  30:	add	x1, x0, x1
  34:	mov	x2, #0x0                   	// #0
  38:	fmov	d0, x2
  3c:	orr	x5, x1, x19, lsl #32
  40:	ldp	x19, x20, [sp, #16]
  44:	fmov	v0.d[1], x5
  48:	ldp	x29, x30, [sp], #32
  4c:	ret
  50:	mov	w1, #0x7f7fffff            	// #2139095039
  54:	cmp	w0, w1
  58:	b.ls	6c <__extendsftf2+0x6c>  // b.plast
  5c:	ubfiz	x0, x0, #25, #23
  60:	mov	x2, #0x0                   	// #0
  64:	orr	x1, x0, #0x7fff000000000000
  68:	b	38 <__extendsftf2+0x38>
  6c:	cbz	w0, a0 <__extendsftf2+0xa0>
  70:	clz	w20, w0
  74:	mov	x1, #0x0                   	// #0
  78:	add	w2, w20, #0x51
  7c:	and	x0, x0, #0x7fffffff
  80:	and	x2, x2, #0xff
  84:	bl	0 <__ashlti3>
  88:	eor	x3, x1, #0x1000000000000
  8c:	mov	w1, #0x3f89                	// #16265
  90:	sub	w1, w1, w20
  94:	mov	x2, x0
  98:	orr	x1, x3, x1, lsl #48
  9c:	b	38 <__extendsftf2+0x38>
  a0:	mov	x2, #0x0                   	// #0
  a4:	mov	x1, #0x0                   	// #0
  a8:	b	38 <__extendsftf2+0x38>

fixtfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfdi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	w2, #0xffffc001            	// #-16383
   8:	mov	x29, sp
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	str	x19, [sp, #16]
  18:	mov	x19, #0x1                   	// #1
  1c:	cmp	x1, #0x0
  20:	ubfx	x3, x1, #48, #15
  24:	add	w2, w3, w2
  28:	cneg	x19, x19, lt  // lt = tstop
  2c:	tbnz	w2, #31, 84 <__fixtfdi+0x84>
  30:	cmp	w2, #0x3f
  34:	b.ls	50 <__fixtfdi+0x50>  // b.plast
  38:	cmp	x19, #0x1
  3c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  40:	cinv	x0, x0, eq  // eq = none
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	and	x1, x1, #0xffffffffffff
  54:	cmp	w2, #0x6f
  58:	orr	x1, x1, #0x1000000000000
  5c:	b.gt	74 <__fixtfdi+0x74>
  60:	mov	w3, #0x70                  	// #112
  64:	sub	w2, w3, w2
  68:	bl	0 <__lshrti3>
  6c:	mul	x0, x0, x19
  70:	b	44 <__fixtfdi+0x44>
  74:	mov	w1, #0xffffbf91            	// #-16495
  78:	add	w3, w3, w1
  7c:	lsl	x0, x0, x3
  80:	b	6c <__fixtfdi+0x6c>
  84:	mov	x0, #0x0                   	// #0
  88:	b	44 <__fixtfdi+0x44>

fixtfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfsi>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	w2, #0xffffc001            	// #-16383
   8:	mov	x29, sp
   c:	str	q0, [sp, #32]
  10:	ldp	x0, x1, [sp, #32]
  14:	str	x19, [sp, #16]
  18:	mov	w19, #0x1                   	// #1
  1c:	cmp	x1, #0x0
  20:	ubfx	x3, x1, #48, #15
  24:	add	w2, w3, w2
  28:	cneg	w19, w19, lt  // lt = tstop
  2c:	tbnz	w2, #31, 84 <__fixtfsi+0x84>
  30:	cmp	w2, #0x1f
  34:	b.ls	50 <__fixtfsi+0x50>  // b.plast
  38:	cmp	w19, #0x1
  3c:	mov	w0, #0x80000000            	// #-2147483648
  40:	cinv	w0, w0, eq  // eq = none
  44:	ldr	x19, [sp, #16]
  48:	ldp	x29, x30, [sp], #48
  4c:	ret
  50:	and	x1, x1, #0xffffffffffff
  54:	cmp	w2, #0x6f
  58:	orr	x1, x1, #0x1000000000000
  5c:	b.gt	74 <__fixtfsi+0x74>
  60:	mov	w3, #0x70                  	// #112
  64:	sub	w2, w3, w2
  68:	bl	0 <__lshrti3>
  6c:	mul	w0, w0, w19
  70:	b	44 <__fixtfsi+0x44>
  74:	mov	w1, #0xffffbf91            	// #-16495
  78:	add	w3, w3, w1
  7c:	lsl	w0, w0, w3
  80:	b	6c <__fixtfsi+0x6c>
  84:	mov	w0, #0x0                   	// #0
  88:	b	44 <__fixtfsi+0x44>

fixtfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixtfti>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #32]
   c:	ldp	x0, x1, [sp, #32]
  10:	stp	x19, x20, [sp, #16]
  14:	tbnz	x1, #63, 5c <__fixtfti+0x5c>
  18:	mov	x19, #0x1                   	// #1
  1c:	mov	x20, #0x0                   	// #0
  20:	ubfx	x2, x1, #48, #15
  24:	mov	w4, #0xffffc001            	// #-16383
  28:	add	w3, w2, w4
  2c:	tbnz	w3, #31, ac <__fixtfti+0xac>
  30:	cmp	w3, #0x7f
  34:	b.ls	68 <__fixtfti+0x68>  // b.plast
  38:	cmp	x19, #0x1
  3c:	b.ne	44 <__fixtfti+0x44>  // b.any
  40:	cbz	x20, b8 <__fixtfti+0xb8>
  44:	adrp	x1, 10 <__fixtfti+0x10>
  48:	mov	x0, #0x0                   	// #0
  4c:	ldr	x1, [x1]
  50:	ldp	x19, x20, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret
  5c:	mov	x19, #0xffffffffffffffff    	// #-1
  60:	mov	x20, x19
  64:	b	20 <__fixtfti+0x20>
  68:	and	x1, x1, #0xffffffffffff
  6c:	cmp	w3, #0x6f
  70:	orr	x1, x1, #0x1000000000000
  74:	b.gt	9c <__fixtfti+0x9c>
  78:	mov	w2, #0x70                  	// #112
  7c:	sub	w2, w2, w3
  80:	bl	0 <__lshrti3>
  84:	mov	x2, x0
  88:	umulh	x3, x0, x19
  8c:	madd	x3, x1, x19, x3
  90:	mul	x0, x0, x19
  94:	madd	x1, x2, x20, x3
  98:	b	50 <__fixtfti+0x50>
  9c:	mov	w3, #0xffffbf91            	// #-16495
  a0:	add	w2, w2, w3
  a4:	bl	0 <__ashlti3>
  a8:	b	84 <__fixtfti+0x84>
  ac:	mov	x0, #0x0                   	// #0
  b0:	mov	x1, #0x0                   	// #0
  b4:	b	50 <__fixtfti+0x50>
  b8:	adrp	x1, 0 <__fixtfti>
  bc:	mov	x0, #0xffffffffffffffff    	// #-1
  c0:	ldr	x1, [x1]
  c4:	b	50 <__fixtfti+0x50>

fixunstfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfdi>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	w2, #0x1                   	// #1
   8:	mov	x29, sp
   c:	str	q0, [sp, #16]
  10:	ldp	x0, x1, [sp, #16]
  14:	cmp	x1, #0x0
  18:	ubfx	x3, x1, #48, #15
  1c:	cneg	w4, w2, lt  // lt = tstop
  20:	mov	w2, #0xffffc001            	// #-16383
  24:	add	w2, w3, w2
  28:	cmp	w2, #0x0
  2c:	ccmn	w4, #0x1, #0x4, ge  // ge = tcont
  30:	b.eq	70 <__fixunstfdi+0x70>  // b.none
  34:	cmp	w2, #0x3f
  38:	b.hi	78 <__fixunstfdi+0x78>  // b.pmore
  3c:	and	x1, x1, #0xffffffffffff
  40:	cmp	w2, #0x6f
  44:	orr	x1, x1, #0x1000000000000
  48:	b.gt	60 <__fixunstfdi+0x60>
  4c:	mov	w3, #0x70                  	// #112
  50:	sub	w2, w3, w2
  54:	bl	0 <__lshrti3>
  58:	ldp	x29, x30, [sp], #32
  5c:	ret
  60:	mov	w1, #0xffffbf91            	// #-16495
  64:	add	w3, w3, w1
  68:	lsl	x0, x0, x3
  6c:	b	58 <__fixunstfdi+0x58>
  70:	mov	x0, #0x0                   	// #0
  74:	b	58 <__fixunstfdi+0x58>
  78:	mov	x0, #0xffffffffffffffff    	// #-1
  7c:	b	58 <__fixunstfdi+0x58>

fixunstfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfsi>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	w2, #0x1                   	// #1
   8:	mov	x29, sp
   c:	str	q0, [sp, #16]
  10:	ldp	x0, x1, [sp, #16]
  14:	cmp	x1, #0x0
  18:	ubfx	x3, x1, #48, #15
  1c:	cneg	w4, w2, lt  // lt = tstop
  20:	mov	w2, #0xffffc001            	// #-16383
  24:	add	w2, w3, w2
  28:	cmp	w2, #0x0
  2c:	ccmn	w4, #0x1, #0x4, ge  // ge = tcont
  30:	b.eq	70 <__fixunstfsi+0x70>  // b.none
  34:	cmp	w2, #0x1f
  38:	b.hi	78 <__fixunstfsi+0x78>  // b.pmore
  3c:	and	x1, x1, #0xffffffffffff
  40:	cmp	w2, #0x6f
  44:	orr	x1, x1, #0x1000000000000
  48:	b.gt	60 <__fixunstfsi+0x60>
  4c:	mov	w3, #0x70                  	// #112
  50:	sub	w2, w3, w2
  54:	bl	0 <__lshrti3>
  58:	ldp	x29, x30, [sp], #32
  5c:	ret
  60:	mov	w1, #0xffffbf91            	// #-16495
  64:	add	w3, w3, w1
  68:	lsl	w0, w0, w3
  6c:	b	58 <__fixunstfsi+0x58>
  70:	mov	w0, #0x0                   	// #0
  74:	b	58 <__fixunstfsi+0x58>
  78:	mov	w0, #0xffffffff            	// #-1
  7c:	b	58 <__fixunstfsi+0x58>

fixunstfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunstfti>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	w3, #0xffffc001            	// #-16383
   8:	mov	w2, #0x1                   	// #1
   c:	mov	x29, sp
  10:	str	q0, [sp, #16]
  14:	ldp	x0, x1, [sp, #16]
  18:	ubfx	x4, x1, #48, #15
  1c:	cmp	x1, #0x0
  20:	add	w3, w4, w3
  24:	cneg	w2, w2, lt  // lt = tstop
  28:	cmp	w3, #0x0
  2c:	ccmn	w2, #0x1, #0x4, ge  // ge = tcont
  30:	b.eq	70 <__fixunstfti+0x70>  // b.none
  34:	cmp	w3, #0x7f
  38:	b.hi	7c <__fixunstfti+0x7c>  // b.pmore
  3c:	and	x1, x1, #0xffffffffffff
  40:	cmp	w3, #0x6f
  44:	orr	x1, x1, #0x1000000000000
  48:	b.gt	60 <__fixunstfti+0x60>
  4c:	mov	w2, #0x70                  	// #112
  50:	sub	w2, w2, w3
  54:	bl	0 <__lshrti3>
  58:	ldp	x29, x30, [sp], #32
  5c:	ret
  60:	mov	w2, #0xffffbf91            	// #-16495
  64:	add	w2, w4, w2
  68:	bl	0 <__ashlti3>
  6c:	b	58 <__fixunstfti+0x58>
  70:	mov	x0, #0x0                   	// #0
  74:	mov	x1, #0x0                   	// #0
  78:	b	58 <__fixunstfti+0x58>
  7c:	mov	x0, #0xffffffffffffffff    	// #-1
  80:	mov	x1, x0
  84:	b	58 <__fixunstfti+0x58>

floatditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatditf>:
   0:	cbz	x0, 74 <__floatditf+0x74>
   4:	stp	x29, x30, [sp, #-48]!
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	str	x21, [sp, #32]
  14:	tbz	x0, #63, 68 <__floatditf+0x68>
  18:	adrp	x1, 0 <__floatditf>
  1c:	neg	x0, x0
  20:	mov	x20, #0x0                   	// #0
  24:	ldr	x19, [x1]
  28:	clz	x21, x0
  2c:	add	x2, x21, #0x31
  30:	mov	x1, #0x0                   	// #0
  34:	bl	0 <__ashlti3>
  38:	eor	x2, x1, #0x1000000000000
  3c:	mov	w1, #0x403e                	// #16446
  40:	sub	w1, w1, w21
  44:	ldr	x21, [sp, #32]
  48:	add	x1, x2, x1, lsl #48
  4c:	orr	x2, x0, x20
  50:	fmov	d0, x2
  54:	orr	x3, x1, x19
  58:	fmov	v0.d[1], x3
  5c:	ldp	x19, x20, [sp, #16]
  60:	ldp	x29, x30, [sp], #48
  64:	ret
  68:	mov	x20, #0x0                   	// #0
  6c:	mov	x19, #0x0                   	// #0
  70:	b	28 <__floatditf+0x28>
  74:	movi	v0.2d, #0x0
  78:	ret

floatsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsitf>:
   0:	cbz	w0, 7c <__floatsitf+0x7c>
   4:	stp	x29, x30, [sp, #-48]!
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	str	x21, [sp, #32]
  14:	tbz	w0, #31, 70 <__floatsitf+0x70>
  18:	adrp	x1, 0 <__floatsitf>
  1c:	neg	w0, w0
  20:	mov	x20, #0x0                   	// #0
  24:	ldr	x19, [x1]
  28:	clz	w21, w0
  2c:	add	w2, w21, #0x51
  30:	mov	w0, w0
  34:	and	x2, x2, #0xff
  38:	mov	x1, #0x0                   	// #0
  3c:	bl	0 <__ashlti3>
  40:	eor	x2, x1, #0x1000000000000
  44:	mov	w1, #0x401e                	// #16414
  48:	sub	w1, w1, w21
  4c:	ldr	x21, [sp, #32]
  50:	add	x1, x2, x1, lsl #48
  54:	orr	x2, x0, x20
  58:	fmov	d0, x2
  5c:	orr	x3, x1, x19
  60:	fmov	v0.d[1], x3
  64:	ldp	x19, x20, [sp, #16]
  68:	ldp	x29, x30, [sp], #48
  6c:	ret
  70:	mov	x20, #0x0                   	// #0
  74:	mov	x19, #0x0                   	// #0
  78:	b	28 <__floatsitf+0x28>
  7c:	movi	v0.2d, #0x0
  80:	ret

floattitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattitf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 12c <__floattitf+0x12c>
   8:	stp	x29, x30, [sp, #-80]!
   c:	eor	x0, x0, x1, asr #63
  10:	mov	x29, sp
  14:	stp	x21, x22, [sp, #32]
  18:	asr	x21, x1, #63
  1c:	eor	x1, x1, x1, asr #63
  20:	stp	x23, x24, [sp, #48]
  24:	subs	x24, x0, x21
  28:	mov	x0, x24
  2c:	stp	x19, x20, [sp, #16]
  30:	sbc	x20, x1, x21
  34:	mov	x1, x20
  38:	str	x25, [sp, #64]
  3c:	bl	0 <__clzti2>
  40:	mov	w25, #0x80                  	// #128
  44:	sub	w25, w25, w0
  48:	mov	x23, x24
  4c:	mov	x19, x20
  50:	sub	w22, w25, #0x1
  54:	cmp	w25, #0x71
  58:	b.le	114 <__floattitf+0x114>
  5c:	cmp	w25, #0x72
  60:	b.eq	ac <__floattitf+0xac>  // b.none
  64:	cmp	w25, #0x73
  68:	b.eq	b4 <__floattitf+0xb4>  // b.none
  6c:	add	w2, w0, #0x73
  70:	mov	x1, #0xffffffffffffffff    	// #-1
  74:	mov	x0, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x20
  80:	and	x0, x0, x24
  84:	orr	x0, x0, x1
  88:	sub	w2, w25, #0x73
  8c:	cmp	x0, #0x0
  90:	mov	x1, x19
  94:	cset	x20, ne  // ne = any
  98:	mov	x0, x24
  9c:	bl	0 <__lshrti3>
  a0:	orr	x23, x20, x0
  a4:	mov	x19, x1
  a8:	b	b4 <__floattitf+0xb4>
  ac:	lsl	x23, x24, #1
  b0:	extr	x19, x20, x24, #63
  b4:	ubfx	x0, x23, #2, #1
  b8:	orr	x23, x0, x23
  bc:	adds	x23, x23, #0x1
  c0:	cinc	x19, x19, cs  // cs = hs, nlast
  c4:	asr	x1, x19, #2
  c8:	extr	x0, x19, x23, #2
  cc:	tbz	x19, #51, dc <__floattitf+0xdc>
  d0:	asr	x1, x19, #3
  d4:	extr	x0, x19, x23, #3
  d8:	mov	w22, w25
  dc:	and	x21, x21, #0x8000000000000000
  e0:	mov	w2, #0x3fff                	// #16383
  e4:	fmov	d0, x0
  e8:	add	w22, w22, w2
  ec:	and	x1, x1, #0xffffffffffff
  f0:	orr	x1, x1, x21
  f4:	orr	x3, x1, x22, lsl #48
  f8:	fmov	v0.d[1], x3
  fc:	ldp	x19, x20, [sp, #16]
 100:	ldp	x21, x22, [sp, #32]
 104:	ldp	x23, x24, [sp, #48]
 108:	ldr	x25, [sp, #64]
 10c:	ldp	x29, x30, [sp], #80
 110:	ret
 114:	mov	x0, x24
 118:	mov	x1, x20
 11c:	mov	w2, #0x71                  	// #113
 120:	sub	w2, w2, w25
 124:	bl	0 <__ashlti3>
 128:	b	dc <__floattitf+0xdc>
 12c:	movi	v0.2d, #0x0
 130:	ret

floatunditf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunditf>:
   0:	cbz	x0, 44 <__floatunditf+0x44>
   4:	stp	x29, x30, [sp, #-32]!
   8:	mov	x1, #0x0                   	// #0
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	clz	x19, x0
  18:	add	x2, x19, #0x31
  1c:	bl	0 <__ashlti3>
  20:	mov	w2, #0x403e                	// #16446
  24:	fmov	d0, x0
  28:	sub	w19, w2, w19
  2c:	eor	x1, x1, #0x1000000000000
  30:	add	x3, x1, x19, lsl #48
  34:	fmov	v0.d[1], x3
  38:	ldr	x19, [sp, #16]
  3c:	ldp	x29, x30, [sp], #32
  40:	ret
  44:	movi	v0.2d, #0x0
  48:	ret

floatunsitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsitf>:
   0:	cbz	w0, 4c <__floatunsitf+0x4c>
   4:	stp	x29, x30, [sp, #-32]!
   8:	mov	x1, #0x0                   	// #0
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	clz	w19, w0
  18:	add	w2, w19, #0x51
  1c:	mov	w0, w0
  20:	and	x2, x2, #0xff
  24:	bl	0 <__ashlti3>
  28:	mov	w4, #0x401e                	// #16414
  2c:	fmov	d0, x0
  30:	sub	w4, w4, w19
  34:	eor	x1, x1, #0x1000000000000
  38:	ldr	x19, [sp, #16]
  3c:	add	x3, x1, x4, lsl #48
  40:	ldp	x29, x30, [sp], #32
  44:	fmov	v0.d[1], x3
  48:	ret
  4c:	movi	v0.2d, #0x0
  50:	ret

floatuntitf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntitf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 114 <__floatuntitf+0x114>
   8:	stp	x29, x30, [sp, #-80]!
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	x20, x0
  18:	mov	x19, x1
  1c:	stp	x21, x22, [sp, #32]
  20:	mov	w21, #0x80                  	// #128
  24:	stp	x23, x24, [sp, #48]
  28:	str	x25, [sp, #64]
  2c:	bl	0 <__clzti2>
  30:	sub	w21, w21, w0
  34:	mov	w23, w0
  38:	sub	w22, w21, #0x1
  3c:	cmp	w21, #0x71
  40:	b.le	fc <__floatuntitf+0xfc>
  44:	cmp	w21, #0x72
  48:	b.eq	9c <__floatuntitf+0x9c>  // b.none
  4c:	cmp	w21, #0x73
  50:	b.eq	a4 <__floatuntitf+0xa4>  // b.none
  54:	sub	w2, w21, #0x73
  58:	mov	x0, x20
  5c:	mov	x1, x19
  60:	bl	0 <__lshrti3>
  64:	add	w2, w23, #0x73
  68:	mov	x25, x0
  6c:	mov	x24, x1
  70:	mov	x0, #0xffffffffffffffff    	// #-1
  74:	mov	x1, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x19
  80:	and	x20, x0, x20
  84:	orr	x20, x20, x1
  88:	mov	x19, x24
  8c:	cmp	x20, #0x0
  90:	cset	x20, ne  // ne = any
  94:	orr	x20, x20, x25
  98:	b	a4 <__floatuntitf+0xa4>
  9c:	extr	x19, x19, x20, #63
  a0:	lsl	x20, x20, #1
  a4:	ubfx	x0, x20, #2, #1
  a8:	orr	x20, x0, x20
  ac:	adds	x20, x20, #0x1
  b0:	cinc	x19, x19, cs  // cs = hs, nlast
  b4:	lsr	x1, x19, #2
  b8:	extr	x0, x19, x20, #2
  bc:	tbz	x19, #51, cc <__floatuntitf+0xcc>
  c0:	lsr	x1, x19, #3
  c4:	extr	x0, x19, x20, #3
  c8:	mov	w22, w21
  cc:	mov	w2, #0x3fff                	// #16383
  d0:	fmov	d0, x0
  d4:	add	w22, w22, w2
  d8:	and	x1, x1, #0xffffffffffff
  dc:	ldp	x19, x20, [sp, #16]
  e0:	orr	x3, x1, x22, lsl #48
  e4:	fmov	v0.d[1], x3
  e8:	ldp	x21, x22, [sp, #32]
  ec:	ldp	x23, x24, [sp, #48]
  f0:	ldr	x25, [sp, #64]
  f4:	ldp	x29, x30, [sp], #80
  f8:	ret
  fc:	mov	x0, x20
 100:	mov	x1, x19
 104:	mov	w2, #0x71                  	// #113
 108:	sub	w2, w2, w21
 10c:	bl	0 <__ashlti3>
 110:	b	cc <__floatuntitf+0xcc>
 114:	movi	v0.2d, #0x0
 118:	ret

multc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multc3>:
   0:	stp	x29, x30, [sp, #-240]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #96]
   c:	stp	x21, x22, [sp, #32]
  10:	stp	x25, x26, [sp, #64]
  14:	ldp	x21, x25, [sp, #96]
  18:	str	q1, [sp, #96]
  1c:	stp	x19, x20, [sp, #16]
  20:	stp	x23, x24, [sp, #48]
  24:	ldp	x19, x23, [sp, #96]
  28:	str	q2, [sp, #96]
  2c:	ldp	x20, x24, [sp, #96]
  30:	str	q3, [sp, #96]
  34:	ldp	x22, x26, [sp, #96]
  38:	stp	x20, x24, [sp, #96]
  3c:	ldr	q1, [sp, #96]
  40:	stp	x21, x25, [sp, #96]
  44:	stp	x27, x28, [sp, #80]
  48:	ldr	q0, [sp, #96]
  4c:	bl	0 <__multf3>
  50:	str	q0, [sp, #96]
  54:	ldp	x1, x0, [sp, #96]
  58:	stp	x22, x26, [sp, #96]
  5c:	ldr	q1, [sp, #96]
  60:	stp	x19, x23, [sp, #96]
  64:	ldr	q0, [sp, #96]
  68:	str	x1, [sp, #152]
  6c:	str	x0, [sp, #200]
  70:	bl	0 <__multf3>
  74:	str	q0, [sp, #96]
  78:	ldp	x1, x0, [sp, #96]
  7c:	stp	x22, x26, [sp, #96]
  80:	ldr	q1, [sp, #96]
  84:	stp	x21, x25, [sp, #96]
  88:	ldr	q0, [sp, #96]
  8c:	str	x1, [sp, #160]
  90:	str	x0, [sp, #208]
  94:	bl	0 <__multf3>
  98:	str	q0, [sp, #96]
  9c:	ldp	x1, x0, [sp, #96]
  a0:	stp	x19, x23, [sp, #96]
  a4:	ldr	q1, [sp, #96]
  a8:	stp	x20, x24, [sp, #96]
  ac:	ldr	q0, [sp, #96]
  b0:	str	x1, [sp, #168]
  b4:	str	x0, [sp, #216]
  b8:	bl	0 <__multf3>
  bc:	str	q0, [sp, #96]
  c0:	ldp	x1, x0, [sp, #96]
  c4:	str	x0, [sp, #224]
  c8:	ldr	x0, [sp, #160]
  cc:	str	x0, [sp, #96]
  d0:	ldr	x0, [sp, #208]
  d4:	str	x0, [sp, #104]
  d8:	ldr	x0, [sp, #152]
  dc:	str	x1, [sp, #192]
  e0:	ldr	q1, [sp, #96]
  e4:	str	x0, [sp, #96]
  e8:	ldr	x0, [sp, #200]
  ec:	str	x0, [sp, #104]
  f0:	ldr	q0, [sp, #96]
  f4:	bl	0 <__subtf3>
  f8:	str	q0, [sp, #112]
  fc:	ldr	x0, [sp, #192]
 100:	str	x0, [sp, #96]
 104:	ldr	x0, [sp, #224]
 108:	str	x0, [sp, #104]
 10c:	ldr	x0, [sp, #168]
 110:	ldr	q1, [sp, #96]
 114:	str	x0, [sp, #96]
 118:	ldr	x0, [sp, #216]
 11c:	str	x0, [sp, #104]
 120:	ldr	q0, [sp, #96]
 124:	bl	0 <__addtf3>
 128:	ldr	q2, [sp, #112]
 12c:	str	q0, [sp, #112]
 130:	mov	v1.16b, v2.16b
 134:	mov	v0.16b, v2.16b
 138:	str	q2, [sp, #96]
 13c:	bl	0 <__unordtf2>
 140:	cbz	w0, 51c <__multc3+0x51c>
 144:	ldr	q1, [sp, #112]
 148:	mov	v0.16b, v1.16b
 14c:	bl	0 <__unordtf2>
 150:	cbz	w0, 51c <__multc3+0x51c>
 154:	adrp	x0, 0 <__multc3>
 158:	add	x0, x0, #0x0
 15c:	and	x28, x25, #0x7fffffffffffffff
 160:	stp	x21, x28, [sp, #128]
 164:	mov	w27, #0x1                   	// #1
 168:	ldr	q1, [x0]
 16c:	ldr	q0, [sp, #128]
 170:	bl	0 <__unordtf2>
 174:	cbnz	w0, 198 <__multc3+0x198>
 178:	adrp	x0, 0 <__multc3>
 17c:	add	x0, x0, #0x0
 180:	stp	x21, x28, [sp, #128]
 184:	ldr	q1, [x0]
 188:	ldr	q0, [sp, #128]
 18c:	bl	0 <__letf2>
 190:	cmp	w0, #0x0
 194:	cset	w27, le
 198:	eor	w27, w27, #0x1
 19c:	stp	x21, x28, [sp, #128]
 1a0:	and	w0, w27, #0xff
 1a4:	str	w0, [sp, #176]
 1a8:	adrp	x0, 0 <__multc3>
 1ac:	add	x0, x0, #0x0
 1b0:	ldr	q0, [sp, #128]
 1b4:	and	x27, x23, #0x7fffffffffffffff
 1b8:	ldr	q1, [x0]
 1bc:	bl	0 <__unordtf2>
 1c0:	cbnz	w0, 1e4 <__multc3+0x1e4>
 1c4:	adrp	x0, 0 <__multc3>
 1c8:	add	x0, x0, #0x0
 1cc:	stp	x21, x28, [sp, #128]
 1d0:	ldr	q1, [x0]
 1d4:	ldr	q0, [sp, #128]
 1d8:	bl	0 <__letf2>
 1dc:	cmp	w0, #0x0
 1e0:	b.gt	220 <__multc3+0x220>
 1e4:	adrp	x0, 0 <__multc3>
 1e8:	add	x0, x0, #0x0
 1ec:	stp	x19, x27, [sp, #128]
 1f0:	ldr	q1, [x0]
 1f4:	ldr	q0, [sp, #128]
 1f8:	bl	0 <__unordtf2>
 1fc:	cbnz	w0, 2e8 <__multc3+0x2e8>
 200:	adrp	x0, 0 <__multc3>
 204:	add	x0, x0, #0x0
 208:	stp	x19, x27, [sp, #128]
 20c:	ldr	q1, [x0]
 210:	ldr	q0, [sp, #128]
 214:	bl	0 <__letf2>
 218:	cmp	w0, #0x0
 21c:	b.le	2e8 <__multc3+0x2e8>
 220:	ldr	w0, [sp, #176]
 224:	mov	w28, #0x1                   	// #1
 228:	bl	0 <__floatsitf>
 22c:	str	q0, [sp, #128]
 230:	ldp	x21, x0, [sp, #128]
 234:	stp	x19, x27, [sp, #128]
 238:	ldr	q0, [sp, #128]
 23c:	bfxil	x25, x0, #0, #63
 240:	adrp	x0, 0 <__multc3>
 244:	add	x0, x0, #0x0
 248:	ldr	q1, [x0]
 24c:	bl	0 <__unordtf2>
 250:	cbnz	w0, 274 <__multc3+0x274>
 254:	adrp	x0, 0 <__multc3>
 258:	add	x0, x0, #0x0
 25c:	stp	x19, x27, [sp, #128]
 260:	ldr	q1, [x0]
 264:	ldr	q0, [sp, #128]
 268:	bl	0 <__letf2>
 26c:	cmp	w0, #0x0
 270:	cset	w28, le
 274:	eor	w0, w28, #0x1
 278:	and	w0, w0, #0x1
 27c:	bl	0 <__floatsitf>
 280:	str	q0, [sp, #128]
 284:	ldp	x19, x0, [sp, #128]
 288:	stp	x20, x24, [sp, #128]
 28c:	ldr	q1, [sp, #128]
 290:	bfxil	x23, x0, #0, #63
 294:	mov	v0.16b, v1.16b
 298:	bl	0 <__unordtf2>
 29c:	cbz	w0, 2b4 <__multc3+0x2b4>
 2a0:	mov	x20, #0x0                   	// #0
 2a4:	mov	x0, #0x0                   	// #0
 2a8:	tbz	x24, #63, 2b0 <__multc3+0x2b0>
 2ac:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2b0:	mov	x24, x0
 2b4:	stp	x22, x26, [sp, #128]
 2b8:	ldr	q1, [sp, #128]
 2bc:	mov	v0.16b, v1.16b
 2c0:	bl	0 <__unordtf2>
 2c4:	cbz	w0, 2dc <__multc3+0x2dc>
 2c8:	mov	x22, #0x0                   	// #0
 2cc:	mov	x0, #0x0                   	// #0
 2d0:	tbz	x26, #63, 2d8 <__multc3+0x2d8>
 2d4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 2d8:	mov	x26, x0
 2dc:	mov	w0, #0x1                   	// #1
 2e0:	str	w0, [sp, #128]
 2e4:	b	2ec <__multc3+0x2ec>
 2e8:	str	wzr, [sp, #128]
 2ec:	adrp	x0, 0 <__multc3>
 2f0:	add	x0, x0, #0x0
 2f4:	and	x28, x24, #0x7fffffffffffffff
 2f8:	stp	x20, x28, [sp, #176]
 2fc:	mov	w27, #0x1                   	// #1
 300:	ldr	q1, [x0]
 304:	ldr	q0, [sp, #176]
 308:	bl	0 <__unordtf2>
 30c:	cbnz	w0, 330 <__multc3+0x330>
 310:	adrp	x0, 0 <__multc3>
 314:	add	x0, x0, #0x0
 318:	stp	x20, x28, [sp, #176]
 31c:	ldr	q1, [x0]
 320:	ldr	q0, [sp, #176]
 324:	bl	0 <__letf2>
 328:	cmp	w0, #0x0
 32c:	cset	w27, le
 330:	eor	w27, w27, #0x1
 334:	stp	x20, x28, [sp, #176]
 338:	and	w0, w27, #0xff
 33c:	str	w0, [sp, #236]
 340:	adrp	x0, 0 <__multc3>
 344:	add	x0, x0, #0x0
 348:	ldr	q0, [sp, #176]
 34c:	and	x27, x26, #0x7fffffffffffffff
 350:	ldr	q1, [x0]
 354:	bl	0 <__unordtf2>
 358:	cbnz	w0, 37c <__multc3+0x37c>
 35c:	adrp	x0, 0 <__multc3>
 360:	add	x0, x0, #0x0
 364:	stp	x20, x28, [sp, #176]
 368:	ldr	q1, [x0]
 36c:	ldr	q0, [sp, #176]
 370:	bl	0 <__letf2>
 374:	cmp	w0, #0x0
 378:	b.gt	3b8 <__multc3+0x3b8>
 37c:	adrp	x0, 0 <__multc3>
 380:	add	x0, x0, #0x0
 384:	stp	x22, x27, [sp, #176]
 388:	ldr	q1, [x0]
 38c:	ldr	q0, [sp, #176]
 390:	bl	0 <__unordtf2>
 394:	cbnz	w0, 540 <__multc3+0x540>
 398:	adrp	x0, 0 <__multc3>
 39c:	add	x0, x0, #0x0
 3a0:	stp	x22, x27, [sp, #176]
 3a4:	ldr	q1, [x0]
 3a8:	ldr	q0, [sp, #176]
 3ac:	bl	0 <__letf2>
 3b0:	cmp	w0, #0x0
 3b4:	b.le	540 <__multc3+0x540>
 3b8:	ldr	w0, [sp, #236]
 3bc:	mov	w28, #0x1                   	// #1
 3c0:	bl	0 <__floatsitf>
 3c4:	str	q0, [sp, #96]
 3c8:	ldp	x20, x0, [sp, #96]
 3cc:	stp	x22, x27, [sp, #96]
 3d0:	ldr	q0, [sp, #96]
 3d4:	bfxil	x24, x0, #0, #63
 3d8:	adrp	x0, 0 <__multc3>
 3dc:	add	x0, x0, #0x0
 3e0:	ldr	q1, [x0]
 3e4:	bl	0 <__unordtf2>
 3e8:	cbnz	w0, 40c <__multc3+0x40c>
 3ec:	adrp	x0, 0 <__multc3>
 3f0:	add	x0, x0, #0x0
 3f4:	stp	x22, x27, [sp, #96]
 3f8:	ldr	q1, [x0]
 3fc:	ldr	q0, [sp, #96]
 400:	bl	0 <__letf2>
 404:	cmp	w0, #0x0
 408:	cset	w28, le
 40c:	eor	w0, w28, #0x1
 410:	and	w0, w0, #0x1
 414:	bl	0 <__floatsitf>
 418:	str	q0, [sp, #96]
 41c:	ldp	x22, x0, [sp, #96]
 420:	stp	x21, x25, [sp, #96]
 424:	ldr	q1, [sp, #96]
 428:	bfxil	x26, x0, #0, #63
 42c:	mov	v0.16b, v1.16b
 430:	bl	0 <__unordtf2>
 434:	cbz	w0, 44c <__multc3+0x44c>
 438:	mov	x21, #0x0                   	// #0
 43c:	mov	x0, #0x0                   	// #0
 440:	tbz	x25, #63, 448 <__multc3+0x448>
 444:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 448:	mov	x25, x0
 44c:	stp	x19, x23, [sp, #96]
 450:	ldr	q1, [sp, #96]
 454:	mov	v0.16b, v1.16b
 458:	bl	0 <__unordtf2>
 45c:	cbz	w0, 474 <__multc3+0x474>
 460:	mov	x19, #0x0                   	// #0
 464:	mov	x0, #0x0                   	// #0
 468:	tbz	x23, #63, 470 <__multc3+0x470>
 46c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 470:	mov	x23, x0
 474:	stp	x20, x24, [sp, #96]
 478:	ldr	q1, [sp, #96]
 47c:	stp	x21, x25, [sp, #96]
 480:	ldr	q0, [sp, #96]
 484:	bl	0 <__multf3>
 488:	stp	x22, x26, [sp, #96]
 48c:	ldr	q1, [sp, #96]
 490:	stp	x19, x23, [sp, #96]
 494:	str	q0, [sp, #112]
 498:	ldr	q0, [sp, #96]
 49c:	bl	0 <__multf3>
 4a0:	mov	v1.16b, v0.16b
 4a4:	ldr	q2, [sp, #112]
 4a8:	mov	v0.16b, v2.16b
 4ac:	bl	0 <__subtf3>
 4b0:	adrp	x0, 0 <__multc3>
 4b4:	add	x0, x0, #0x0
 4b8:	ldr	q1, [x0]
 4bc:	bl	0 <__multf3>
 4c0:	stp	x22, x26, [sp, #96]
 4c4:	ldr	q1, [sp, #96]
 4c8:	stp	x21, x25, [sp, #96]
 4cc:	str	q0, [sp, #128]
 4d0:	ldr	q0, [sp, #96]
 4d4:	bl	0 <__multf3>
 4d8:	stp	x20, x24, [sp, #96]
 4dc:	ldr	q1, [sp, #96]
 4e0:	stp	x19, x23, [sp, #96]
 4e4:	str	q0, [sp, #112]
 4e8:	ldr	q0, [sp, #96]
 4ec:	bl	0 <__multf3>
 4f0:	mov	v1.16b, v0.16b
 4f4:	ldr	q4, [sp, #112]
 4f8:	mov	v0.16b, v4.16b
 4fc:	bl	0 <__addtf3>
 500:	adrp	x0, 0 <__multc3>
 504:	add	x0, x0, #0x0
 508:	ldr	q1, [x0]
 50c:	bl	0 <__multf3>
 510:	str	q0, [sp, #112]
 514:	ldr	q2, [sp, #128]
 518:	str	q2, [sp, #96]
 51c:	ldp	x19, x20, [sp, #16]
 520:	ldp	x21, x22, [sp, #32]
 524:	ldp	x23, x24, [sp, #48]
 528:	ldp	x25, x26, [sp, #64]
 52c:	ldp	x27, x28, [sp, #80]
 530:	ldr	q0, [sp, #96]
 534:	ldr	q1, [sp, #112]
 538:	ldp	x29, x30, [sp], #240
 53c:	ret
 540:	ldr	w0, [sp, #128]
 544:	cbnz	w0, 474 <__multc3+0x474>
 548:	ldr	x0, [sp, #200]
 54c:	and	x27, x0, #0x7fffffffffffffff
 550:	adrp	x0, 0 <__multc3>
 554:	add	x0, x0, #0x0
 558:	ldr	q1, [x0]
 55c:	ldr	x0, [sp, #152]
 560:	stp	x0, x27, [sp, #128]
 564:	ldr	q0, [sp, #128]
 568:	bl	0 <__unordtf2>
 56c:	cbnz	w0, 594 <__multc3+0x594>
 570:	adrp	x0, 0 <__multc3>
 574:	add	x0, x0, #0x0
 578:	ldr	q1, [x0]
 57c:	ldr	x0, [sp, #152]
 580:	stp	x0, x27, [sp, #128]
 584:	ldr	q0, [sp, #128]
 588:	bl	0 <__letf2>
 58c:	cmp	w0, #0x0
 590:	b.gt	678 <__multc3+0x678>
 594:	ldr	x0, [sp, #208]
 598:	and	x27, x0, #0x7fffffffffffffff
 59c:	adrp	x0, 0 <__multc3>
 5a0:	add	x0, x0, #0x0
 5a4:	ldr	q1, [x0]
 5a8:	ldr	x0, [sp, #160]
 5ac:	stp	x0, x27, [sp, #128]
 5b0:	ldr	q0, [sp, #128]
 5b4:	bl	0 <__unordtf2>
 5b8:	cbnz	w0, 5e0 <__multc3+0x5e0>
 5bc:	adrp	x0, 0 <__multc3>
 5c0:	add	x0, x0, #0x0
 5c4:	ldr	q1, [x0]
 5c8:	ldr	x0, [sp, #160]
 5cc:	stp	x0, x27, [sp, #128]
 5d0:	ldr	q0, [sp, #128]
 5d4:	bl	0 <__letf2>
 5d8:	cmp	w0, #0x0
 5dc:	b.gt	678 <__multc3+0x678>
 5e0:	ldr	x0, [sp, #216]
 5e4:	and	x27, x0, #0x7fffffffffffffff
 5e8:	adrp	x0, 0 <__multc3>
 5ec:	add	x0, x0, #0x0
 5f0:	ldr	q1, [x0]
 5f4:	ldr	x0, [sp, #168]
 5f8:	stp	x0, x27, [sp, #128]
 5fc:	ldr	q0, [sp, #128]
 600:	bl	0 <__unordtf2>
 604:	cbnz	w0, 62c <__multc3+0x62c>
 608:	adrp	x0, 0 <__multc3>
 60c:	add	x0, x0, #0x0
 610:	ldr	q1, [x0]
 614:	ldr	x0, [sp, #168]
 618:	stp	x0, x27, [sp, #128]
 61c:	ldr	q0, [sp, #128]
 620:	bl	0 <__letf2>
 624:	cmp	w0, #0x0
 628:	b.gt	678 <__multc3+0x678>
 62c:	ldr	x0, [sp, #224]
 630:	and	x27, x0, #0x7fffffffffffffff
 634:	adrp	x0, 0 <__multc3>
 638:	add	x0, x0, #0x0
 63c:	ldr	q1, [x0]
 640:	ldr	x0, [sp, #192]
 644:	stp	x0, x27, [sp, #128]
 648:	ldr	q0, [sp, #128]
 64c:	bl	0 <__unordtf2>
 650:	cbnz	w0, 51c <__multc3+0x51c>
 654:	adrp	x0, 0 <__multc3>
 658:	add	x0, x0, #0x0
 65c:	ldr	q1, [x0]
 660:	ldr	x0, [sp, #192]
 664:	stp	x0, x27, [sp, #128]
 668:	ldr	q0, [sp, #128]
 66c:	bl	0 <__letf2>
 670:	cmp	w0, #0x0
 674:	b.le	51c <__multc3+0x51c>
 678:	stp	x21, x25, [sp, #96]
 67c:	ldr	q1, [sp, #96]
 680:	mov	v0.16b, v1.16b
 684:	bl	0 <__unordtf2>
 688:	cbz	w0, 6a0 <__multc3+0x6a0>
 68c:	mov	x21, #0x0                   	// #0
 690:	mov	x0, #0x0                   	// #0
 694:	tbz	x25, #63, 69c <__multc3+0x69c>
 698:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 69c:	mov	x25, x0
 6a0:	stp	x19, x23, [sp, #96]
 6a4:	ldr	q1, [sp, #96]
 6a8:	mov	v0.16b, v1.16b
 6ac:	bl	0 <__unordtf2>
 6b0:	cbz	w0, 6c8 <__multc3+0x6c8>
 6b4:	mov	x19, #0x0                   	// #0
 6b8:	mov	x0, #0x0                   	// #0
 6bc:	tbz	x23, #63, 6c4 <__multc3+0x6c4>
 6c0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6c4:	mov	x23, x0
 6c8:	stp	x20, x24, [sp, #96]
 6cc:	ldr	q1, [sp, #96]
 6d0:	mov	v0.16b, v1.16b
 6d4:	bl	0 <__unordtf2>
 6d8:	cbz	w0, 6f0 <__multc3+0x6f0>
 6dc:	mov	x20, #0x0                   	// #0
 6e0:	mov	x0, #0x0                   	// #0
 6e4:	tbz	x24, #63, 6ec <__multc3+0x6ec>
 6e8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 6ec:	mov	x24, x0
 6f0:	stp	x22, x26, [sp, #96]
 6f4:	ldr	q1, [sp, #96]
 6f8:	mov	v0.16b, v1.16b
 6fc:	bl	0 <__unordtf2>
 700:	cbz	w0, 474 <__multc3+0x474>
 704:	mov	x22, #0x0                   	// #0
 708:	mov	x0, #0x0                   	// #0
 70c:	tbz	x26, #63, 714 <__multc3+0x714>
 710:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 714:	mov	x26, x0
 718:	b	474 <__multc3+0x474>

trunctfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfdf2>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x0, #0xc3ff000000000000    	// #-4323737117252386816
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	str	q0, [sp, #64]
  14:	ldp	x1, x20, [sp, #64]
  18:	stp	x21, x22, [sp, #32]
  1c:	str	x23, [sp, #48]
  20:	and	x2, x20, #0x7fffffffffffffff
  24:	mov	x19, x1
  28:	add	x0, x2, x0
  2c:	mov	x1, #0xbc01000000000000    	// #-4899634919602388992
  30:	add	x1, x2, x1
  34:	cmp	x1, x0
  38:	b.hi	60 <__trunctfdf2+0x60>  // b.pmore
  3c:	mov	x0, #0x7fff000000000000    	// #9223090561878065152
  40:	cmp	x2, x0
  44:	b.hi	50 <__trunctfdf2+0x50>  // b.pmore
  48:	b.ne	bc <__trunctfdf2+0xbc>  // b.any
  4c:	cbz	x19, bc <__trunctfdf2+0xbc>
  50:	extr	x1, x2, x19, #60
  54:	and	x1, x1, #0x7ffffffffffff
  58:	orr	x1, x1, #0x7ff8000000000000
  5c:	b	9c <__trunctfdf2+0x9c>
  60:	mov	x1, #0x1                   	// #1
  64:	extr	x2, x2, x19, #60
  68:	movk	x1, #0x4000, lsl #48
  6c:	and	x19, x19, #0xfffffffffffffff
  70:	add	x1, x2, x1
  74:	mov	x3, #0x800000000000000     	// #576460752303423488
  78:	and	x0, x1, #0xfffffffffffffffe
  7c:	cmp	x19, x3
  80:	mov	x1, #0x4000000000000000    	// #4611686018427387904
  84:	add	x1, x2, x1
  88:	csel	x1, x1, x0, ne  // ne = any
  8c:	mov	x0, #0x1                   	// #1
  90:	movk	x0, #0x4000, lsl #48
  94:	add	x2, x2, x0
  98:	csel	x1, x2, x1, hi  // hi = pmore
  9c:	and	x20, x20, #0x8000000000000000
  a0:	orr	x0, x1, x20
  a4:	fmov	d0, x0
  a8:	ldp	x19, x20, [sp, #16]
  ac:	ldp	x21, x22, [sp, #32]
  b0:	ldr	x23, [sp, #48]
  b4:	ldp	x29, x30, [sp], #80
  b8:	ret
  bc:	mov	x0, #0x43feffffffffffff    	// #4899634919602388991
  c0:	cmp	x2, x0
  c4:	b.hi	140 <__trunctfdf2+0x140>  // b.pmore
  c8:	lsr	x2, x2, #48
  cc:	mov	w23, #0x3c01                	// #15361
  d0:	sub	w23, w23, w2
  d4:	cmp	w23, #0x70
  d8:	b.gt	148 <__trunctfdf2+0x148>
  dc:	and	x22, x20, #0xffffffffffff
  e0:	mov	w0, #0xffffc47f            	// #-15233
  e4:	orr	x22, x22, #0x1000000000000
  e8:	add	w2, w2, w0
  ec:	mov	x1, x22
  f0:	mov	x0, x19
  f4:	bl	0 <__ashlti3>
  f8:	orr	x0, x0, x1
  fc:	cmp	x0, #0x0
 100:	mov	w2, w23
 104:	cset	x21, ne  // ne = any
 108:	mov	x0, x19
 10c:	mov	x1, x22
 110:	bl	0 <__lshrti3>
 114:	extr	x1, x1, x0, #60
 118:	orr	x21, x21, x0
 11c:	add	x2, x1, #0x1
 120:	and	x0, x21, #0xfffffffffffffff
 124:	and	x2, x2, #0xfffffffffffffffe
 128:	mov	x3, #0x800000000000000     	// #576460752303423488
 12c:	cmp	x0, x3
 130:	csel	x2, x2, x1, eq  // eq = none
 134:	add	x1, x1, #0x1
 138:	csel	x1, x1, x2, hi  // hi = pmore
 13c:	b	9c <__trunctfdf2+0x9c>
 140:	mov	x1, #0x7ff0000000000000    	// #9218868437227405312
 144:	b	9c <__trunctfdf2+0x9c>
 148:	mov	x1, #0x0                   	// #0
 14c:	b	9c <__trunctfdf2+0x9c>

trunctfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__trunctfsf2>:
   0:	stp	x29, x30, [sp, #-80]!
   4:	mov	x1, #0xbf81000000000000    	// #-4647433340469641216
   8:	mov	x0, #0xc07f000000000000    	// #-4575938696385134592
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	str	q0, [sp, #64]
  18:	ldp	x20, x19, [sp, #64]
  1c:	stp	x21, x22, [sp, #32]
  20:	str	x23, [sp, #48]
  24:	and	x2, x19, #0x7fffffffffffffff
  28:	add	x1, x2, x1
  2c:	add	x0, x2, x0
  30:	cmp	x1, x0
  34:	b.hi	58 <__trunctfsf2+0x58>  // b.pmore
  38:	mov	x0, #0x7fff000000000000    	// #9223090561878065152
  3c:	cmp	x2, x0
  40:	b.hi	4c <__trunctfsf2+0x4c>  // b.pmore
  44:	b.ne	d0 <__trunctfsf2+0xd0>  // b.any
  48:	cbz	x20, d0 <__trunctfsf2+0xd0>
  4c:	ubfx	x2, x2, #25, #22
  50:	orr	w2, w2, #0x7fc00000
  54:	b	80 <__trunctfsf2+0x80>
  58:	and	x0, x19, #0x1ffffff
  5c:	mov	x1, #0x1000000             	// #16777216
  60:	lsr	x2, x2, #25
  64:	cmp	x0, x1
  68:	b.hi	74 <__trunctfsf2+0x74>  // b.pmore
  6c:	b.ne	a4 <__trunctfsf2+0xa4>  // b.any
  70:	cbz	x20, a8 <__trunctfsf2+0xa8>
  74:	mov	w0, #0x1                   	// #1
  78:	movk	w0, #0x4000, lsl #16
  7c:	add	w2, w0, w2
  80:	lsr	x19, x19, #32
  84:	and	x19, x19, #0x80000000
  88:	orr	w0, w2, w19
  8c:	fmov	s0, w0
  90:	ldp	x19, x20, [sp, #16]
  94:	ldp	x21, x22, [sp, #32]
  98:	ldr	x23, [sp, #48]
  9c:	ldp	x29, x30, [sp], #80
  a0:	ret
  a4:	cbnz	x20, b4 <__trunctfsf2+0xb4>
  a8:	mov	x1, #0x1000000             	// #16777216
  ac:	cmp	x0, x1
  b0:	b.eq	bc <__trunctfsf2+0xbc>  // b.none
  b4:	mov	w0, #0x40000000            	// #1073741824
  b8:	b	7c <__trunctfsf2+0x7c>
  bc:	mov	w0, #0x1                   	// #1
  c0:	movk	w0, #0x4000, lsl #16
  c4:	add	w2, w0, w2
  c8:	and	w2, w2, #0xfffffffe
  cc:	b	80 <__trunctfsf2+0x80>
  d0:	mov	x0, #0x407effffffffffff    	// #4647433340469641215
  d4:	cmp	x2, x0
  d8:	b.hi	16c <__trunctfsf2+0x16c>  // b.pmore
  dc:	lsr	x2, x2, #48
  e0:	mov	w23, #0x3f81                	// #16257
  e4:	sub	w23, w23, w2
  e8:	cmp	w23, #0x70
  ec:	b.gt	174 <__trunctfsf2+0x174>
  f0:	and	x22, x19, #0xffffffffffff
  f4:	mov	w0, #0xffffc0ff            	// #-16129
  f8:	orr	x22, x22, #0x1000000000000
  fc:	add	w2, w2, w0
 100:	mov	x1, x22
 104:	mov	x0, x20
 108:	bl	0 <__ashlti3>
 10c:	orr	x0, x0, x1
 110:	cmp	x0, #0x0
 114:	mov	w2, w23
 118:	cset	x21, ne  // ne = any
 11c:	mov	x0, x20
 120:	mov	x1, x22
 124:	bl	0 <__lshrti3>
 128:	lsr	x3, x1, #25
 12c:	and	x1, x1, #0x1ffffff
 130:	mov	x4, #0x1000000             	// #16777216
 134:	orr	x0, x21, x0
 138:	mov	w2, w3
 13c:	cmp	x1, x4
 140:	b.hi	14c <__trunctfsf2+0x14c>  // b.pmore
 144:	b.ne	154 <__trunctfsf2+0x154>  // b.any
 148:	cbz	x0, 158 <__trunctfsf2+0x158>
 14c:	add	w2, w3, #0x1
 150:	b	80 <__trunctfsf2+0x80>
 154:	cbnz	x0, 80 <__trunctfsf2+0x80>
 158:	mov	x0, #0x1000000             	// #16777216
 15c:	cmp	x1, x0
 160:	b.ne	80 <__trunctfsf2+0x80>  // b.any
 164:	add	w2, w3, #0x1
 168:	b	c8 <__trunctfsf2+0xc8>
 16c:	mov	w2, #0x7f800000            	// #2139095040
 170:	b	80 <__trunctfsf2+0x80>
 174:	mov	w2, #0x0                   	// #0
 178:	b	80 <__trunctfsf2+0x80>

absvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.ne	2c <__absvdi2+0x2c>  // b.any
   c:	stp	x29, x30, [sp, #-16]!
  10:	adrp	x2, 0 <__absvdi2>
  14:	adrp	x0, 0 <__absvdi2>
  18:	mov	x29, sp
  1c:	add	x2, x2, #0x0
  20:	add	x0, x0, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	bl	0 <__compilerrt_abort_impl>
  2c:	eor	x1, x0, x0, asr #63
  30:	sub	x0, x1, x0, asr #63
  34:	ret

absvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.ne	2c <__absvsi2+0x2c>  // b.any
   c:	stp	x29, x30, [sp, #-16]!
  10:	adrp	x2, 0 <__absvsi2>
  14:	adrp	x0, 0 <__absvsi2>
  18:	mov	x29, sp
  1c:	add	x2, x2, #0x0
  20:	add	x0, x0, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	bl	0 <__compilerrt_abort_impl>
  2c:	eor	w1, w0, w0, asr #31
  30:	sub	w0, w1, w0, asr #31
  34:	ret

absvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__absvti2>:
   0:	cbnz	x0, 30 <__absvti2+0x30>
   4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
   8:	cmp	x1, x2
   c:	b.ne	30 <__absvti2+0x30>  // b.any
  10:	stp	x29, x30, [sp, #-16]!
  14:	adrp	x2, 0 <__absvti2>
  18:	adrp	x0, 0 <__absvti2>
  1c:	mov	x29, sp
  20:	add	x2, x2, #0x0
  24:	add	x0, x0, #0x0
  28:	mov	w1, #0x18                  	// #24
  2c:	bl	0 <__compilerrt_abort_impl>
  30:	asr	x2, x1, #63
  34:	eor	x0, x0, x1, asr #63
  38:	subs	x0, x0, x2
  3c:	eor	x1, x1, x1, asr #63
  40:	sbc	x1, x1, x2
  44:	ret

adddf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__adddf3>:
   0:	fmov	d2, d0
   4:	stp	x29, x30, [sp, #-48]!
   8:	fmov	x4, d1
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	x2, #0x7fefffffffffffff    	// #9218868437227405311
  18:	fmov	x20, d2
  1c:	str	x21, [sp, #32]
  20:	fmov	d0, d1
  24:	and	x0, x4, #0x7fffffffffffffff
  28:	and	x1, x20, #0x7fffffffffffffff
  2c:	sub	x3, x1, #0x1
  30:	cmp	x3, x2
  34:	b.cs	164 <__adddf3+0x164>  // b.hs, b.nlast
  38:	sub	x3, x0, #0x1
  3c:	cmp	x3, x2
  40:	b.cs	240 <__adddf3+0x240>  // b.hs, b.nlast
  44:	cmp	x1, x0
  48:	b.cs	58 <__adddf3+0x58>  // b.hs, b.nlast
  4c:	mov	x0, x20
  50:	mov	x20, x4
  54:	mov	x4, x0
  58:	ubfx	x2, x20, #52, #11
  5c:	ubfx	x5, x4, #52, #11
  60:	and	x1, x20, #0xfffffffffffff
  64:	and	x3, x4, #0xfffffffffffff
  68:	cbnz	w2, 80 <__adddf3+0x80>
  6c:	clz	x2, x1
  70:	sub	w0, w2, #0xb
  74:	mov	w2, #0x1                   	// #1
  78:	sub	w2, w2, w0
  7c:	lsl	x1, x1, x0
  80:	cbnz	w5, 98 <__adddf3+0x98>
  84:	clz	x5, x3
  88:	sub	w0, w5, #0xb
  8c:	mov	w5, #0x1                   	// #1
  90:	sub	w5, w5, w0
  94:	lsl	x3, x3, x0
  98:	lsl	x0, x1, #3
  9c:	lsl	x3, x3, #3
  a0:	and	x19, x20, #0x8000000000000000
  a4:	eor	x4, x20, x4
  a8:	orr	x0, x0, #0x80000000000000
  ac:	orr	x1, x3, #0x80000000000000
  b0:	subs	w5, w2, w5
  b4:	b.eq	d8 <__adddf3+0xd8>  // b.none
  b8:	cmp	w5, #0x3f
  bc:	b.hi	1d4 <__adddf3+0x1d4>  // b.pmore
  c0:	neg	w3, w5
  c4:	lsl	x3, x1, x3
  c8:	cmp	x3, #0x0
  cc:	cset	x3, ne  // ne = any
  d0:	lsr	x1, x1, x5
  d4:	orr	x1, x3, x1
  d8:	tbz	x4, #63, 1dc <__adddf3+0x1dc>
  dc:	subs	x0, x0, x1
  e0:	b.eq	238 <__adddf3+0x238>  // b.none
  e4:	mov	x1, #0x7fffffffffffff      	// #36028797018963967
  e8:	cmp	x0, x1
  ec:	b.hi	1f0 <__adddf3+0x1f0>  // b.pmore
  f0:	clz	x1, x0
  f4:	sub	w1, w1, #0x8
  f8:	sub	w2, w2, w1
  fc:	lsl	x0, x0, x1
 100:	cmp	w2, #0x0
 104:	b.gt	12c <__adddf3+0x12c>
 108:	mov	w1, #0x1                   	// #1
 10c:	sub	w2, w1, w2
 110:	neg	w1, w2
 114:	lsl	x1, x0, x1
 118:	cmp	x1, #0x0
 11c:	cset	x1, ne  // ne = any
 120:	lsr	x0, x0, x2
 124:	orr	x0, x1, x0
 128:	mov	w2, #0x0                   	// #0
 12c:	and	w21, w0, #0x7
 130:	ubfx	x0, x0, #3, #52
 134:	orr	x0, x0, x2, lsl #52
 138:	orr	x19, x19, x0
 13c:	bl	0 <__fe_getround>
 140:	cmp	w0, #0x1
 144:	b.eq	214 <__adddf3+0x214>  // b.none
 148:	cmp	w0, #0x2
 14c:	b.eq	22c <__adddf3+0x22c>  // b.none
 150:	cbnz	w0, 220 <__adddf3+0x220>
 154:	cmp	w21, #0x4
 158:	b.le	200 <__adddf3+0x200>
 15c:	add	x19, x19, #0x1
 160:	b	20c <__adddf3+0x20c>
 164:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
 168:	cmp	x1, x2
 16c:	b.ls	17c <__adddf3+0x17c>  // b.plast
 170:	orr	x0, x20, #0x8000000000000
 174:	fmov	d0, x0
 178:	b	1ac <__adddf3+0x1ac>
 17c:	cmp	x0, x2
 180:	b.ls	18c <__adddf3+0x18c>  // b.plast
 184:	orr	x0, x4, #0x8000000000000
 188:	b	174 <__adddf3+0x174>
 18c:	cmp	x1, x2
 190:	b.ne	1bc <__adddf3+0x1bc>  // b.any
 194:	mov	x0, #0x7ff8000000000000    	// #9221120237041090560
 198:	eor	x4, x20, x4
 19c:	fmov	d1, x0
 1a0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1a4:	cmp	x4, x0
 1a8:	fcsel	d0, d2, d1, ne  // ne = any
 1ac:	ldp	x19, x20, [sp, #16]
 1b0:	ldr	x21, [sp, #32]
 1b4:	ldp	x29, x30, [sp], #48
 1b8:	ret
 1bc:	cmp	x0, x2
 1c0:	b.eq	1ac <__adddf3+0x1ac>  // b.none
 1c4:	cbnz	x1, 250 <__adddf3+0x250>
 1c8:	cbnz	x0, 1ac <__adddf3+0x1ac>
 1cc:	and	x0, x20, x4
 1d0:	b	174 <__adddf3+0x174>
 1d4:	mov	x1, #0x1                   	// #1
 1d8:	b	d8 <__adddf3+0xd8>
 1dc:	add	x0, x0, x1
 1e0:	tbz	x0, #56, 1f0 <__adddf3+0x1f0>
 1e4:	and	x1, x0, #0x1
 1e8:	add	w2, w2, #0x1
 1ec:	orr	x0, x1, x0, lsr #1
 1f0:	cmp	w2, #0x7fe
 1f4:	b.le	100 <__adddf3+0x100>
 1f8:	orr	x0, x19, #0x7ff0000000000000
 1fc:	b	174 <__adddf3+0x174>
 200:	b.ne	220 <__adddf3+0x220>  // b.any
 204:	add	x19, x19, #0x1
 208:	and	x19, x19, #0xfffffffffffffffe
 20c:	bl	0 <__fe_raise_inexact>
 210:	b	224 <__adddf3+0x224>
 214:	cmp	x20, #0x0
 218:	ccmp	w21, #0x0, #0x4, lt  // lt = tstop
 21c:	b.ne	15c <__adddf3+0x15c>  // b.any
 220:	cbnz	w21, 20c <__adddf3+0x20c>
 224:	fmov	d0, x19
 228:	b	1ac <__adddf3+0x1ac>
 22c:	cmp	x20, #0x0
 230:	ccmp	w21, #0x0, #0x4, ge  // ge = tcont
 234:	b	21c <__adddf3+0x21c>
 238:	movi	d0, #0x0
 23c:	b	1ac <__adddf3+0x1ac>
 240:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
 244:	cmp	x0, x2
 248:	b.hi	184 <__adddf3+0x184>  // b.pmore
 24c:	b.eq	1ac <__adddf3+0x1ac>  // b.none
 250:	cbnz	x0, 44 <__adddf3+0x44>
 254:	fmov	d0, d2
 258:	b	1ac <__adddf3+0x1ac>

addsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addsf3>:
   0:	fmov	s2, s0
   4:	stp	x29, x30, [sp, #-48]!
   8:	fmov	w4, s1
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	w2, #0x7f7fffff            	// #2139095039
  18:	fmov	w20, s2
  1c:	str	x21, [sp, #32]
  20:	fmov	s0, s1
  24:	and	w0, w4, #0x7fffffff
  28:	and	w1, w20, #0x7fffffff
  2c:	sub	w3, w1, #0x1
  30:	cmp	w3, w2
  34:	b.cs	164 <__addsf3+0x164>  // b.hs, b.nlast
  38:	sub	w3, w0, #0x1
  3c:	cmp	w3, w2
  40:	b.cs	240 <__addsf3+0x240>  // b.hs, b.nlast
  44:	cmp	w1, w0
  48:	b.cs	58 <__addsf3+0x58>  // b.hs, b.nlast
  4c:	mov	w0, w20
  50:	mov	w20, w4
  54:	mov	w4, w0
  58:	ubfx	x2, x20, #23, #8
  5c:	ubfx	x5, x4, #23, #8
  60:	and	w1, w20, #0x7fffff
  64:	and	w3, w4, #0x7fffff
  68:	cbnz	w2, 80 <__addsf3+0x80>
  6c:	clz	w2, w1
  70:	sub	w0, w2, #0x8
  74:	mov	w2, #0x1                   	// #1
  78:	sub	w2, w2, w0
  7c:	lsl	w1, w1, w0
  80:	cbnz	w5, 98 <__addsf3+0x98>
  84:	clz	w5, w3
  88:	sub	w0, w5, #0x8
  8c:	mov	w5, #0x1                   	// #1
  90:	sub	w5, w5, w0
  94:	lsl	w3, w3, w0
  98:	lsl	w0, w1, #3
  9c:	lsl	w3, w3, #3
  a0:	and	w19, w20, #0x80000000
  a4:	eor	w4, w20, w4
  a8:	orr	w0, w0, #0x4000000
  ac:	orr	w1, w3, #0x4000000
  b0:	subs	w5, w2, w5
  b4:	b.eq	d8 <__addsf3+0xd8>  // b.none
  b8:	cmp	w5, #0x1f
  bc:	b.hi	1d4 <__addsf3+0x1d4>  // b.pmore
  c0:	neg	w3, w5
  c4:	lsl	w3, w1, w3
  c8:	cmp	w3, #0x0
  cc:	cset	w3, ne  // ne = any
  d0:	lsr	w1, w1, w5
  d4:	orr	w1, w3, w1
  d8:	tbz	w4, #31, 1dc <__addsf3+0x1dc>
  dc:	subs	w0, w0, w1
  e0:	b.eq	238 <__addsf3+0x238>  // b.none
  e4:	mov	w1, #0x3ffffff             	// #67108863
  e8:	cmp	w0, w1
  ec:	b.hi	1f0 <__addsf3+0x1f0>  // b.pmore
  f0:	clz	w1, w0
  f4:	sub	w1, w1, #0x5
  f8:	sub	w2, w2, w1
  fc:	lsl	w0, w0, w1
 100:	cmp	w2, #0x0
 104:	b.gt	12c <__addsf3+0x12c>
 108:	mov	w1, #0x1                   	// #1
 10c:	sub	w2, w1, w2
 110:	neg	w1, w2
 114:	lsl	w1, w0, w1
 118:	cmp	w1, #0x0
 11c:	cset	w1, ne  // ne = any
 120:	lsr	w0, w0, w2
 124:	orr	w0, w1, w0
 128:	mov	w2, #0x0                   	// #0
 12c:	and	w21, w0, #0x7
 130:	ubfx	x0, x0, #3, #23
 134:	orr	w0, w0, w2, lsl #23
 138:	orr	w19, w19, w0
 13c:	bl	0 <__fe_getround>
 140:	cmp	w0, #0x1
 144:	b.eq	214 <__addsf3+0x214>  // b.none
 148:	cmp	w0, #0x2
 14c:	b.eq	22c <__addsf3+0x22c>  // b.none
 150:	cbnz	w0, 220 <__addsf3+0x220>
 154:	cmp	w21, #0x4
 158:	b.le	200 <__addsf3+0x200>
 15c:	add	w19, w19, #0x1
 160:	b	20c <__addsf3+0x20c>
 164:	mov	w2, #0x7f800000            	// #2139095040
 168:	cmp	w1, w2
 16c:	b.ls	17c <__addsf3+0x17c>  // b.plast
 170:	orr	w0, w20, #0x400000
 174:	fmov	s0, w0
 178:	b	1ac <__addsf3+0x1ac>
 17c:	cmp	w0, w2
 180:	b.ls	18c <__addsf3+0x18c>  // b.plast
 184:	orr	w0, w4, #0x400000
 188:	b	174 <__addsf3+0x174>
 18c:	cmp	w1, w2
 190:	b.ne	1bc <__addsf3+0x1bc>  // b.any
 194:	mov	w0, #0x7fc00000            	// #2143289344
 198:	eor	w4, w20, w4
 19c:	fmov	s1, w0
 1a0:	mov	w0, #0x80000000            	// #-2147483648
 1a4:	cmp	w4, w0
 1a8:	fcsel	s0, s2, s1, ne  // ne = any
 1ac:	ldp	x19, x20, [sp, #16]
 1b0:	ldr	x21, [sp, #32]
 1b4:	ldp	x29, x30, [sp], #48
 1b8:	ret
 1bc:	cmp	w0, w2
 1c0:	b.eq	1ac <__addsf3+0x1ac>  // b.none
 1c4:	cbnz	w1, 250 <__addsf3+0x250>
 1c8:	cbnz	w0, 1ac <__addsf3+0x1ac>
 1cc:	and	w0, w20, w4
 1d0:	b	174 <__addsf3+0x174>
 1d4:	mov	w1, #0x1                   	// #1
 1d8:	b	d8 <__addsf3+0xd8>
 1dc:	add	w0, w0, w1
 1e0:	tbz	w0, #27, 1f0 <__addsf3+0x1f0>
 1e4:	and	w1, w0, #0x1
 1e8:	add	w2, w2, #0x1
 1ec:	orr	w0, w1, w0, lsr #1
 1f0:	cmp	w2, #0xfe
 1f4:	b.le	100 <__addsf3+0x100>
 1f8:	orr	w0, w19, #0x7f800000
 1fc:	b	174 <__addsf3+0x174>
 200:	b.ne	220 <__addsf3+0x220>  // b.any
 204:	add	w19, w19, #0x1
 208:	and	w19, w19, #0xfffffffe
 20c:	bl	0 <__fe_raise_inexact>
 210:	b	224 <__addsf3+0x224>
 214:	cmp	w20, #0x0
 218:	ccmp	w21, #0x0, #0x4, lt  // lt = tstop
 21c:	b.ne	15c <__addsf3+0x15c>  // b.any
 220:	cbnz	w21, 20c <__addsf3+0x20c>
 224:	fmov	s0, w19
 228:	b	1ac <__addsf3+0x1ac>
 22c:	cmp	w20, #0x0
 230:	ccmp	w21, #0x0, #0x4, ge  // ge = tcont
 234:	b	21c <__addsf3+0x21c>
 238:	movi	v0.2s, #0x0
 23c:	b	1ac <__addsf3+0x1ac>
 240:	mov	w2, #0x7f800000            	// #2139095040
 244:	cmp	w0, w2
 248:	b.hi	184 <__addsf3+0x184>  // b.pmore
 24c:	b.eq	1ac <__addsf3+0x1ac>  // b.none
 250:	cbnz	w0, 44 <__addsf3+0x44>
 254:	fmov	s0, s2
 258:	b	1ac <__addsf3+0x1ac>

addtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addtf3>:
   0:	stp	x29, x30, [sp, #-112]!
   4:	mov	x7, #0x7ffeffffffffffff    	// #9223090561878065151
   8:	mov	x29, sp
   c:	str	q0, [sp, #96]
  10:	ldp	x5, x6, [sp, #96]
  14:	str	q1, [sp, #96]
  18:	ldp	x1, x0, [sp, #96]
  1c:	stp	x19, x20, [sp, #16]
  20:	stp	x21, x22, [sp, #32]
  24:	subs	x4, x5, #0x1
  28:	and	x3, x6, #0x7fffffffffffffff
  2c:	stp	x23, x24, [sp, #48]
  30:	sbc	x8, x3, xzr
  34:	stp	x25, x26, [sp, #64]
  38:	mov	x19, x5
  3c:	mov	x24, x6
  40:	stp	x27, x28, [sp, #80]
  44:	cmp	x8, x7
  48:	mov	x23, x1
  4c:	mov	x26, x0
  50:	and	x2, x0, #0x7fffffffffffffff
  54:	b.hi	21c <__addtf3+0x21c>  // b.pmore
  58:	b.ne	64 <__addtf3+0x64>  // b.any
  5c:	cmn	x4, #0x2
  60:	b.hi	21c <__addtf3+0x21c>  // b.pmore
  64:	subs	x4, x23, #0x1
  68:	mov	x7, #0x7ffeffffffffffff    	// #9223090561878065151
  6c:	sbc	x8, x2, xzr
  70:	cmp	x8, x7
  74:	b.hi	3ec <__addtf3+0x3ec>  // b.pmore
  78:	b.ne	84 <__addtf3+0x84>  // b.any
  7c:	cmn	x4, #0x2
  80:	b.hi	3ec <__addtf3+0x3ec>  // b.pmore
  84:	cmp	x2, x3
  88:	b.hi	98 <__addtf3+0x98>  // b.pmore
  8c:	b.ne	b0 <__addtf3+0xb0>  // b.any
  90:	cmp	x23, x19
  94:	b.ls	b0 <__addtf3+0xb0>  // b.plast
  98:	mov	x1, x19
  9c:	mov	x0, x24
  a0:	mov	x19, x23
  a4:	mov	x24, x26
  a8:	mov	x23, x1
  ac:	mov	x26, x0
  b0:	ubfx	x25, x24, #48, #15
  b4:	ubfx	x27, x26, #48, #15
  b8:	and	x20, x24, #0xffffffffffff
  bc:	mov	x21, x23
  c0:	and	x22, x26, #0xffffffffffff
  c4:	cbnz	w25, 104 <__addtf3+0x104>
  c8:	cmp	x20, #0x0
  cc:	mov	x25, #0x40                  	// #64
  d0:	csel	x0, x20, x19, ne  // ne = any
  d4:	csel	x25, xzr, x25, ne  // ne = any
  d8:	clz	x0, x0
  dc:	mov	x1, x20
  e0:	add	w25, w25, w0
  e4:	mov	x0, x19
  e8:	sub	w2, w25, #0xf
  ec:	mov	x25, x2
  f0:	bl	0 <__ashlti3>
  f4:	mov	x19, x0
  f8:	mov	x20, x1
  fc:	mov	w0, #0x1                   	// #1
 100:	sub	w25, w0, w25
 104:	cbnz	w27, 144 <__addtf3+0x144>
 108:	cmp	x22, #0x0
 10c:	mov	x27, #0x40                  	// #64
 110:	csel	x0, x22, x23, ne  // ne = any
 114:	csel	x27, xzr, x27, ne  // ne = any
 118:	clz	x0, x0
 11c:	mov	x1, x22
 120:	add	w27, w27, w0
 124:	mov	x0, x23
 128:	sub	w2, w27, #0xf
 12c:	mov	x27, x2
 130:	bl	0 <__ashlti3>
 134:	mov	x21, x0
 138:	mov	x22, x1
 13c:	mov	w0, #0x1                   	// #1
 140:	sub	w27, w0, w27
 144:	extr	x20, x20, x19, #61
 148:	and	x23, x24, #0x8000000000000000
 14c:	extr	x1, x22, x21, #61
 150:	eor	x26, x24, x26
 154:	lsl	x19, x19, #3
 158:	orr	x20, x20, #0x8000000000000
 15c:	lsl	x21, x21, #3
 160:	orr	x22, x1, #0x8000000000000
 164:	subs	w27, w25, w27
 168:	b.eq	1ac <__addtf3+0x1ac>  // b.none
 16c:	cmp	w27, #0x7f
 170:	b.hi	2dc <__addtf3+0x2dc>  // b.pmore
 174:	mov	x0, x21
 178:	mov	x1, x22
 17c:	mov	w2, #0x80                  	// #128
 180:	sub	w2, w2, w27
 184:	bl	0 <__ashlti3>
 188:	orr	x0, x0, x1
 18c:	cmp	x0, #0x0
 190:	mov	x1, x22
 194:	cset	x28, ne  // ne = any
 198:	mov	x0, x21
 19c:	mov	w2, w27
 1a0:	bl	0 <__lshrti3>
 1a4:	orr	x21, x28, x0
 1a8:	mov	x22, x1
 1ac:	tbz	x26, #63, 2e8 <__addtf3+0x2e8>
 1b0:	subs	x19, x19, x21
 1b4:	sbc	x22, x20, x22
 1b8:	orr	x0, x19, x22
 1bc:	cbz	x0, 3e0 <__addtf3+0x3e0>
 1c0:	mov	x0, #0x7ffffffffffff       	// #2251799813685247
 1c4:	cmp	x22, x0
 1c8:	b.hi	204 <__addtf3+0x204>  // b.pmore
 1cc:	cmp	x22, #0x0
 1d0:	mov	x20, #0x40                  	// #64
 1d4:	csel	x0, x22, x19, ne  // ne = any
 1d8:	csel	x20, xzr, x20, ne  // ne = any
 1dc:	clz	x0, x0
 1e0:	mov	x1, x22
 1e4:	add	w20, w20, w0
 1e8:	mov	x0, x19
 1ec:	sub	w2, w20, #0xc
 1f0:	mov	x20, x2
 1f4:	bl	0 <__ashlti3>
 1f8:	sub	w25, w25, w20
 1fc:	mov	x19, x0
 200:	mov	x22, x1
 204:	mov	w0, #0x7ffe                	// #32766
 208:	cmp	w25, w0
 20c:	b.le	30c <__addtf3+0x30c>
 210:	orr	x3, x23, #0x7fff000000000000
 214:	mov	x2, #0x0                   	// #0
 218:	b	238 <__addtf3+0x238>
 21c:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 220:	cmp	x3, x4
 224:	b.hi	230 <__addtf3+0x230>  // b.pmore
 228:	b.ne	264 <__addtf3+0x264>  // b.any
 22c:	cbz	x19, 264 <__addtf3+0x264>
 230:	mov	x2, x19
 234:	orr	x3, x24, #0x800000000000
 238:	mov	x1, x2
 23c:	mov	x0, x3
 240:	stp	x1, x0, [sp, #96]
 244:	ldp	x19, x20, [sp, #16]
 248:	ldp	x21, x22, [sp, #32]
 24c:	ldp	x23, x24, [sp, #48]
 250:	ldp	x25, x26, [sp, #64]
 254:	ldp	x27, x28, [sp, #80]
 258:	ldr	q0, [sp, #96]
 25c:	ldp	x29, x30, [sp], #112
 260:	ret
 264:	cmp	x2, x4
 268:	b.hi	274 <__addtf3+0x274>  // b.pmore
 26c:	b.ne	280 <__addtf3+0x280>  // b.any
 270:	cbz	x23, 280 <__addtf3+0x280>
 274:	mov	x2, x23
 278:	orr	x3, x26, #0x800000000000
 27c:	b	238 <__addtf3+0x238>
 280:	cbnz	x19, 2b0 <__addtf3+0x2b0>
 284:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 288:	cmp	x3, x4
 28c:	b.ne	2b0 <__addtf3+0x2b0>  // b.any
 290:	eor	x24, x24, x26
 294:	cbnz	x23, 41c <__addtf3+0x41c>
 298:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 29c:	cmp	x24, x0
 2a0:	b.ne	41c <__addtf3+0x41c>  // b.any
 2a4:	mov	x1, #0x0                   	// #0
 2a8:	mov	x0, #0x7fff800000000000    	// #9223231299366420480
 2ac:	b	240 <__addtf3+0x240>
 2b0:	cbnz	x23, 2c0 <__addtf3+0x2c0>
 2b4:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 2b8:	cmp	x2, x4
 2bc:	b.eq	240 <__addtf3+0x240>  // b.none
 2c0:	orr	x4, x19, x3
 2c4:	cbnz	x4, 414 <__addtf3+0x414>
 2c8:	orr	x2, x23, x2
 2cc:	cbnz	x2, 240 <__addtf3+0x240>
 2d0:	and	x2, x19, x23
 2d4:	and	x3, x24, x26
 2d8:	b	238 <__addtf3+0x238>
 2dc:	mov	x21, #0x1                   	// #1
 2e0:	mov	x22, #0x0                   	// #0
 2e4:	b	1ac <__addtf3+0x1ac>
 2e8:	adds	x19, x19, x21
 2ec:	adc	x22, x20, x22
 2f0:	tbz	x22, #52, 204 <__addtf3+0x204>
 2f4:	add	w25, w25, #0x1
 2f8:	extr	x0, x22, x19, #1
 2fc:	and	x19, x19, #0x1
 300:	lsr	x22, x22, #1
 304:	orr	x19, x0, x19
 308:	b	204 <__addtf3+0x204>
 30c:	cmp	w25, #0x0
 310:	b.gt	358 <__addtf3+0x358>
 314:	mov	w0, #0x1                   	// #1
 318:	sub	w25, w0, w25
 31c:	mov	x1, x22
 320:	mov	x0, x19
 324:	mov	w2, #0x80                  	// #128
 328:	sub	w2, w2, w25
 32c:	bl	0 <__ashlti3>
 330:	orr	x0, x0, x1
 334:	cmp	x0, #0x0
 338:	mov	w2, w25
 33c:	cset	x20, ne  // ne = any
 340:	mov	x0, x19
 344:	mov	x1, x22
 348:	mov	w25, #0x0                   	// #0
 34c:	bl	0 <__lshrti3>
 350:	orr	x19, x20, x0
 354:	mov	x22, x1
 358:	ubfx	x0, x22, #3, #48
 35c:	and	w26, w19, #0x7
 360:	orr	x0, x0, x25, lsl #48
 364:	extr	x20, x22, x19, #3
 368:	orr	x21, x23, x0
 36c:	bl	0 <__fe_getround>
 370:	cmp	w0, #0x1
 374:	b.eq	3bc <__addtf3+0x3bc>  // b.none
 378:	cmp	w0, #0x2
 37c:	b.eq	3b8 <__addtf3+0x3b8>  // b.none
 380:	cbnz	w0, 3d0 <__addtf3+0x3d0>
 384:	cmp	w26, #0x4
 388:	b.le	39c <__addtf3+0x39c>
 38c:	adds	x2, x20, #0x1
 390:	cinc	x0, x21, cs  // cs = hs, nlast
 394:	mov	x20, x2
 398:	b	3ac <__addtf3+0x3ac>
 39c:	b.ne	3d0 <__addtf3+0x3d0>  // b.any
 3a0:	adds	x1, x20, #0x1
 3a4:	cinc	x0, x21, cs  // cs = hs, nlast
 3a8:	and	x20, x1, #0xfffffffffffffffe
 3ac:	mov	x21, x0
 3b0:	bl	0 <__fe_raise_inexact>
 3b4:	b	3d4 <__addtf3+0x3d4>
 3b8:	mvn	x24, x24
 3bc:	cmp	w26, #0x0
 3c0:	lsr	x24, x24, #63
 3c4:	cset	w0, ne  // ne = any
 3c8:	tst	w0, w24
 3cc:	b.ne	38c <__addtf3+0x38c>  // b.any
 3d0:	cbnz	w26, 3b0 <__addtf3+0x3b0>
 3d4:	mov	x1, x20
 3d8:	mov	x0, x21
 3dc:	b	240 <__addtf3+0x240>
 3e0:	mov	x1, #0x0                   	// #0
 3e4:	mov	x0, #0x0                   	// #0
 3e8:	b	240 <__addtf3+0x240>
 3ec:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 3f0:	cmp	x2, x4
 3f4:	b.hi	274 <__addtf3+0x274>  // b.pmore
 3f8:	b.ne	410 <__addtf3+0x410>  // b.any
 3fc:	cbnz	x23, 274 <__addtf3+0x274>
 400:	mov	x4, #0x7fff000000000000    	// #9223090561878065152
 404:	cmp	x2, x4
 408:	b.ne	414 <__addtf3+0x414>  // b.any
 40c:	b	240 <__addtf3+0x240>
 410:	cbz	x23, 400 <__addtf3+0x400>
 414:	orr	x0, x23, x2
 418:	cbnz	x0, 84 <__addtf3+0x84>
 41c:	mov	x1, x5
 420:	mov	x0, x6
 424:	b	240 <__addtf3+0x240>

addvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x2, x0
   8:	add	x0, x0, x1
   c:	mov	x29, sp
  10:	tbnz	x1, #63, 34 <__addvdi3+0x34>
  14:	cmp	x2, x0
  18:	b.le	4c <__addvdi3+0x4c>
  1c:	adrp	x2, 0 <__addvdi3>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x17                  	// #23
  28:	adrp	x0, 0 <__addvdi3>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>
  34:	cmp	x2, x0
  38:	b.gt	4c <__addvdi3+0x4c>
  3c:	adrp	x2, 0 <__addvdi3>
  40:	mov	w1, #0x1a                  	// #26
  44:	add	x2, x2, #0x0
  48:	b	28 <__addvdi3+0x28>
  4c:	ldp	x29, x30, [sp], #16
  50:	ret

addvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w2, w0
   8:	add	w0, w0, w1
   c:	mov	x29, sp
  10:	tbnz	w1, #31, 34 <__addvsi3+0x34>
  14:	cmp	w2, w0
  18:	b.le	4c <__addvsi3+0x4c>
  1c:	adrp	x2, 0 <__addvsi3>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x17                  	// #23
  28:	adrp	x0, 0 <__addvsi3>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>
  34:	cmp	w2, w0
  38:	b.gt	4c <__addvsi3+0x4c>
  3c:	adrp	x2, 0 <__addvsi3>
  40:	mov	w1, #0x1a                  	// #26
  44:	add	x2, x2, #0x0
  48:	b	28 <__addvsi3+0x28>
  4c:	ldp	x29, x30, [sp], #16
  50:	ret

addvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__addvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x4, x0
   8:	adds	x0, x0, x2
   c:	mov	x29, sp
  10:	mov	x5, x1
  14:	adc	x1, x1, x3
  18:	tbnz	x3, #63, 48 <__addvti3+0x48>
  1c:	cmp	x5, x1
  20:	b.gt	30 <__addvti3+0x30>
  24:	b.ne	6c <__addvti3+0x6c>  // b.any
  28:	cmp	x4, x0
  2c:	b.ls	6c <__addvti3+0x6c>  // b.plast
  30:	adrp	x2, 0 <__addvti3>
  34:	add	x2, x2, #0x0
  38:	mov	w1, #0x19                  	// #25
  3c:	adrp	x0, 0 <__addvti3>
  40:	add	x0, x0, #0x0
  44:	bl	0 <__compilerrt_abort_impl>
  48:	cmp	x5, x1
  4c:	b.gt	6c <__addvti3+0x6c>
  50:	b.ne	5c <__addvti3+0x5c>  // b.any
  54:	cmp	x4, x0
  58:	b.hi	6c <__addvti3+0x6c>  // b.pmore
  5c:	adrp	x2, 0 <__addvti3>
  60:	mov	w1, #0x1c                  	// #28
  64:	add	x2, x2, #0x0
  68:	b	3c <__addvti3+0x3c>
  6c:	ldp	x29, x30, [sp], #16
  70:	ret

apple_versioning.c.o:     file format elf64-littleaarch64


ashldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashldi3>:
   0:	tbz	w1, #5, 1c <__ashldi3+0x1c>
   4:	sub	w1, w1, #0x20
   8:	mov	x2, #0x0                   	// #0
   c:	lsl	w0, w0, w1
  10:	bfi	x2, x0, #32, #32
  14:	mov	x0, x2
  18:	ret
  1c:	cbz	w1, 18 <__ashldi3+0x18>
  20:	lsl	w3, w0, w1
  24:	mov	x2, #0x0                   	// #0
  28:	bfxil	x2, x3, #0, #32
  2c:	neg	w3, w1
  30:	lsr	w3, w0, w3
  34:	asr	x0, x0, #32
  38:	lsl	w0, w0, w1
  3c:	orr	w0, w3, w0
  40:	b	10 <__ashldi3+0x10>

ashlti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashlti3>:
   0:	tbz	w2, #6, 18 <__ashlti3+0x18>
   4:	sub	w1, w2, #0x40
   8:	mov	x3, #0x0                   	// #0
   c:	lsl	x1, x0, x1
  10:	mov	x0, x3
  14:	ret
  18:	cbz	w2, 14 <__ashlti3+0x14>
  1c:	neg	w4, w2
  20:	lsl	x3, x0, x2
  24:	lsl	x1, x1, x2
  28:	lsr	x0, x0, x4
  2c:	orr	x1, x0, x1
  30:	b	10 <__ashlti3+0x10>

ashrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrdi3>:
   0:	tbz	w1, #5, 28 <__ashrdi3+0x28>
   4:	asr	x3, x0, #32
   8:	asr	x0, x0, #63
   c:	mov	x2, #0x0                   	// #0
  10:	bfi	x2, x0, #32, #32
  14:	sub	w0, w1, #0x20
  18:	asr	w0, w3, w0
  1c:	bfxil	x2, x0, #0, #32
  20:	mov	x0, x2
  24:	ret
  28:	cbz	w1, 24 <__ashrdi3+0x24>
  2c:	asr	x3, x0, #32
  30:	mov	x2, #0x0                   	// #0
  34:	lsr	w0, w0, w1
  38:	asr	w4, w3, w1
  3c:	bfi	x2, x4, #32, #32
  40:	neg	w4, w1
  44:	lsl	w3, w3, w4
  48:	orr	w0, w3, w0
  4c:	b	1c <__ashrdi3+0x1c>

ashrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ashrti3>:
   0:	tbz	w2, #6, 18 <__ashrti3+0x18>
   4:	sub	w0, w2, #0x40
   8:	asr	x3, x1, #63
   c:	asr	x0, x1, x0
  10:	mov	x1, x3
  14:	ret
  18:	cbz	w2, 14 <__ashrti3+0x14>
  1c:	neg	w4, w2
  20:	asr	x3, x1, x2
  24:	lsr	x0, x0, x2
  28:	lsl	x1, x1, x4
  2c:	orr	x0, x1, x0
  30:	b	10 <__ashrti3+0x10>

bswapdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapdi2>:
   0:	rev	x0, x0
   4:	ret

bswapsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__bswapsi2>:
   0:	rev	w0, w0
   4:	ret

clzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzdi2>:
   0:	asr	x2, x0, #32
   4:	cmp	w2, #0x0
   8:	csetm	w1, eq  // eq = none
   c:	csel	w0, w0, w2, eq  // eq = none
  10:	and	w1, w1, #0x20
  14:	clz	w0, w0
  18:	add	w0, w1, w0
  1c:	ret

clzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzsi2>:
   0:	tst	w0, #0xffff0000
   4:	mov	w2, #0x10                  	// #16
   8:	cset	w1, eq  // eq = none
   c:	mov	w4, #0x8                   	// #8
  10:	lsl	w3, w1, #4
  14:	sub	w1, w2, w1, lsl #4
  18:	lsr	w1, w0, w1
  1c:	tst	w1, #0xff00
  20:	cset	w2, eq  // eq = none
  24:	mov	w0, #0x4                   	// #4
  28:	sub	w4, w4, w2, lsl #3
  2c:	add	w3, w3, w2, lsl #3
  30:	lsr	w1, w1, w4
  34:	tst	w1, #0xf0
  38:	cset	w4, eq  // eq = none
  3c:	lsl	w5, w4, #2
  40:	sub	w4, w0, w4, lsl #2
  44:	lsr	w1, w1, w4
  48:	tst	w1, #0xc
  4c:	cset	w2, eq  // eq = none
  50:	mov	w4, #0x2                   	// #2
  54:	sub	w0, w4, w2, lsl #1
  58:	add	w2, w5, w2, lsl #1
  5c:	add	w2, w2, w3
  60:	lsr	w1, w1, w0
  64:	eor	x0, x1, #0x2
  68:	sub	w1, w4, w1
  6c:	sbfx	w0, w0, #1, #1
  70:	and	w0, w0, w1
  74:	add	w0, w0, w2
  78:	ret

clzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clzti2>:
   0:	cmp	x1, #0x0
   4:	csel	x1, x1, x0, ne  // ne = any
   8:	csetm	w2, eq  // eq = none
   c:	clz	x1, x1
  10:	and	w0, w2, #0x40
  14:	add	w0, w0, w1
  18:	ret

cmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdi2>:
   0:	asr	x3, x0, #32
   4:	asr	x2, x1, #32
   8:	cmp	w3, w2
   c:	b.lt	28 <__cmpdi2+0x28>  // b.tstop
  10:	b.gt	30 <__cmpdi2+0x30>
  14:	cmp	w0, w1
  18:	cset	w2, hi  // hi = pmore
  1c:	add	w2, w2, #0x1
  20:	csel	w0, w2, wzr, cs  // cs = hs, nlast
  24:	ret
  28:	mov	w0, #0x0                   	// #0
  2c:	b	24 <__cmpdi2+0x24>
  30:	mov	w0, #0x2                   	// #2
  34:	b	24 <__cmpdi2+0x24>

cmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpti2>:
   0:	cmp	x1, x3
   4:	b.lt	20 <__cmpti2+0x20>  // b.tstop
   8:	b.gt	28 <__cmpti2+0x28>
   c:	cmp	x0, x2
  10:	cset	w1, hi  // hi = pmore
  14:	add	w1, w1, #0x1
  18:	csel	w0, w1, wzr, cs  // cs = hs, nlast
  1c:	ret
  20:	mov	w0, #0x0                   	// #0
  24:	b	1c <__cmpti2+0x1c>
  28:	mov	w0, #0x2                   	// #2
  2c:	b	1c <__cmpti2+0x1c>

comparedf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpdf2>:
   0:	fmov	x2, d0
   4:	fmov	x1, d1
   8:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
   c:	and	x4, x2, #0x7fffffffffffffff
  10:	and	x3, x1, #0x7fffffffffffffff
  14:	cmp	x4, x0
  18:	ccmp	x3, x0, #0x2, ls  // ls = plast
  1c:	b.hi	54 <__cmpdf2+0x54>  // b.pmore
  20:	orr	x0, x2, x1
  24:	tst	x0, #0x7fffffffffffffff
  28:	b.eq	5c <__cmpdf2+0x5c>  // b.none
  2c:	tst	x2, x1
  30:	b.mi	44 <__cmpdf2+0x44>  // b.first
  34:	cmp	x2, x1
  38:	cset	w0, ne  // ne = any
  3c:	csinv	w0, w0, wzr, ge  // ge = tcont
  40:	ret
  44:	cmp	x2, x1
  48:	cset	w0, ne  // ne = any
  4c:	csinv	w0, w0, wzr, le
  50:	b	40 <__cmpdf2+0x40>
  54:	mov	w0, #0x1                   	// #1
  58:	b	40 <__cmpdf2+0x40>
  5c:	mov	w0, #0x0                   	// #0
  60:	b	40 <__cmpdf2+0x40>

0000000000000064 <__gedf2>:
  64:	fmov	x2, d0
  68:	fmov	x1, d1
  6c:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  70:	and	x4, x2, #0x7fffffffffffffff
  74:	and	x3, x1, #0x7fffffffffffffff
  78:	cmp	x4, x0
  7c:	ccmp	x3, x0, #0x2, ls  // ls = plast
  80:	b.hi	b8 <__gedf2+0x54>  // b.pmore
  84:	orr	x0, x2, x1
  88:	tst	x0, #0x7fffffffffffffff
  8c:	b.eq	c0 <__gedf2+0x5c>  // b.none
  90:	tst	x2, x1
  94:	b.mi	a8 <__gedf2+0x44>  // b.first
  98:	cmp	x2, x1
  9c:	cset	w0, ne  // ne = any
  a0:	csinv	w0, w0, wzr, ge  // ge = tcont
  a4:	ret
  a8:	cmp	x2, x1
  ac:	cset	w0, ne  // ne = any
  b0:	csinv	w0, w0, wzr, le
  b4:	b	a4 <__gedf2+0x40>
  b8:	mov	w0, #0xffffffff            	// #-1
  bc:	b	a4 <__gedf2+0x40>
  c0:	mov	w0, #0x0                   	// #0
  c4:	b	a4 <__gedf2+0x40>

00000000000000c8 <__unorddf2>:
  c8:	fmov	x1, d0
  cc:	fmov	x0, d1
  d0:	mov	x2, #0x7ff0000000000000    	// #9218868437227405312
  d4:	and	x1, x1, #0x7fffffffffffffff
  d8:	and	x0, x0, #0x7fffffffffffffff
  dc:	cmp	x1, x2
  e0:	ccmp	x0, x2, #0x2, ls  // ls = plast
  e4:	cset	w0, hi  // hi = pmore
  e8:	ret

comparesf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__cmpsf2>:
   0:	fmov	w2, s0
   4:	fmov	w1, s1
   8:	mov	w0, #0x7f800000            	// #2139095040
   c:	and	w4, w2, #0x7fffffff
  10:	and	w3, w1, #0x7fffffff
  14:	cmp	w4, w0
  18:	ccmp	w3, w0, #0x2, ls  // ls = plast
  1c:	b.hi	54 <__cmpsf2+0x54>  // b.pmore
  20:	orr	w0, w2, w1
  24:	tst	x0, #0x7fffffff
  28:	b.eq	5c <__cmpsf2+0x5c>  // b.none
  2c:	tst	w2, w1
  30:	b.mi	44 <__cmpsf2+0x44>  // b.first
  34:	cmp	w2, w1
  38:	cset	w0, ne  // ne = any
  3c:	csinv	w0, w0, wzr, ge  // ge = tcont
  40:	ret
  44:	cmp	w2, w1
  48:	cset	w0, ne  // ne = any
  4c:	csinv	w0, w0, wzr, le
  50:	b	40 <__cmpsf2+0x40>
  54:	mov	w0, #0x1                   	// #1
  58:	b	40 <__cmpsf2+0x40>
  5c:	mov	w0, #0x0                   	// #0
  60:	b	40 <__cmpsf2+0x40>

0000000000000064 <__gesf2>:
  64:	fmov	w2, s0
  68:	fmov	w1, s1
  6c:	mov	w0, #0x7f800000            	// #2139095040
  70:	and	w4, w2, #0x7fffffff
  74:	and	w3, w1, #0x7fffffff
  78:	cmp	w4, w0
  7c:	ccmp	w3, w0, #0x2, ls  // ls = plast
  80:	b.hi	b8 <__gesf2+0x54>  // b.pmore
  84:	orr	w0, w2, w1
  88:	tst	x0, #0x7fffffff
  8c:	b.eq	c0 <__gesf2+0x5c>  // b.none
  90:	tst	w2, w1
  94:	b.mi	a8 <__gesf2+0x44>  // b.first
  98:	cmp	w2, w1
  9c:	cset	w0, ne  // ne = any
  a0:	csinv	w0, w0, wzr, ge  // ge = tcont
  a4:	ret
  a8:	cmp	w2, w1
  ac:	cset	w0, ne  // ne = any
  b0:	csinv	w0, w0, wzr, le
  b4:	b	a4 <__gesf2+0x40>
  b8:	mov	w0, #0xffffffff            	// #-1
  bc:	b	a4 <__gesf2+0x40>
  c0:	mov	w0, #0x0                   	// #0
  c4:	b	a4 <__gesf2+0x40>

00000000000000c8 <__unordsf2>:
  c8:	fmov	w1, s0
  cc:	fmov	w0, s1
  d0:	mov	w2, #0x7f800000            	// #2139095040
  d4:	and	w1, w1, #0x7fffffff
  d8:	and	w0, w0, #0x7fffffff
  dc:	cmp	w1, w2
  e0:	ccmp	w0, w2, #0x2, ls  // ls = plast
  e4:	cset	w0, hi  // hi = pmore
  e8:	ret

ctzdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzdi2>:
   0:	cmp	w0, #0x0
   4:	asr	x1, x0, #32
   8:	csel	w1, w1, w0, eq  // eq = none
   c:	csetm	w2, eq  // eq = none
  10:	rbit	w1, w1
  14:	and	w0, w2, #0x20
  18:	clz	w1, w1
  1c:	add	w0, w0, w1
  20:	ret

ctzsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzsi2>:
   0:	tst	w0, #0xffff
   4:	cset	w4, eq  // eq = none
   8:	lsl	w4, w4, #4
   c:	lsr	w1, w0, w4
  10:	tst	w1, #0xff
  14:	cset	w2, eq  // eq = none
  18:	mov	w0, #0x2                   	// #2
  1c:	lsl	w2, w2, #3
  20:	add	w4, w2, w4
  24:	lsr	w1, w1, w2
  28:	tst	x1, #0xf
  2c:	cset	w2, eq  // eq = none
  30:	lsl	w5, w2, #2
  34:	lsr	w1, w1, w5
  38:	tst	x1, #0x3
  3c:	cset	w3, eq  // eq = none
  40:	lsl	w2, w3, #1
  44:	lsr	w1, w1, w2
  48:	add	w2, w2, w5
  4c:	ubfx	x3, x1, #1, #1
  50:	mvn	w1, w1
  54:	sub	w0, w0, w3
  58:	add	w2, w2, w4
  5c:	sbfx	x1, x1, #0, #1
  60:	and	w0, w0, w1
  64:	add	w0, w0, w2
  68:	ret

ctzti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ctzti2>:
   0:	cmp	x0, #0x0
   4:	csel	x0, x0, x1, ne  // ne = any
   8:	csetm	w2, eq  // eq = none
   c:	rbit	x0, x0
  10:	and	w2, w2, #0x40
  14:	clz	x0, x0
  18:	add	w0, w2, w0
  1c:	ret

divdc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdc3>:
   0:	stp	x29, x30, [sp, #-112]!
   4:	mov	x29, sp
   8:	stp	d10, d11, [sp, #48]
   c:	fabs	d10, d2
  10:	stp	d14, d15, [sp, #80]
  14:	fmov	d14, d0
  18:	fabs	d0, d3
  1c:	stp	x19, x20, [sp, #16]
  20:	stp	d8, d9, [sp, #32]
  24:	fmov	d8, d1
  28:	fmaxnm	d10, d10, d0
  2c:	stp	d12, d13, [sp, #64]
  30:	fmov	d9, d2
  34:	fmov	d13, d3
  38:	fmov	x0, d10
  3c:	ubfx	x1, x0, #52, #11
  40:	cmp	w1, #0x7ff
  44:	b.ne	150 <__divdc3+0x150>  // b.any
  48:	cmp	x0, #0x0
  4c:	fmov	d0, d10
  50:	fccmp	d10, d10, #0x0, lt  // lt = tstop
  54:	fneg	d10, d10
  58:	fcsel	d10, d10, d0, eq  // eq = none
  5c:	adrp	x0, 0 <__divdc3>
  60:	fabs	d12, d10
  64:	ldr	d15, [x0]
  68:	fcmp	d12, d15
  6c:	b.gt	198 <__divdc3+0x198>
  70:	fcmp	d10, d10
  74:	b.vs	198 <__divdc3+0x198>
  78:	fcvtzs	w19, d10
  7c:	fmov	d0, d9
  80:	neg	w20, w19
  84:	mov	w0, w20
  88:	bl	0 <scalbn>
  8c:	fmov	d9, d0
  90:	fmov	d0, d13
  94:	mov	w0, w20
  98:	bl	0 <scalbn>
  9c:	fmov	d13, d0
  a0:	fmul	d2, d9, d9
  a4:	fmul	d0, d13, d13
  a8:	fmul	d1, d13, d8
  ac:	neg	w19, w19
  b0:	mov	w0, w19
  b4:	fadd	d11, d2, d0
  b8:	fmul	d0, d9, d14
  bc:	fadd	d0, d0, d1
  c0:	fdiv	d0, d0, d11
  c4:	bl	0 <scalbn>
  c8:	fmov	d2, d0
  cc:	fmul	d1, d13, d14
  d0:	fmul	d0, d9, d8
  d4:	mov	w0, w19
  d8:	str	d2, [sp, #104]
  dc:	fsub	d0, d0, d1
  e0:	fdiv	d0, d0, d11
  e4:	bl	0 <scalbn>
  e8:	fmov	d1, d0
  ec:	ldr	d2, [sp, #104]
  f0:	fcmp	d2, d2
  f4:	fmov	d0, d2
  f8:	b.vc	134 <__divdc3+0x134>
  fc:	fcmp	d1, d1
 100:	b.vc	134 <__divdc3+0x134>
 104:	fcmp	d11, #0.0
 108:	b.ne	1a0 <__divdc3+0x1a0>  // b.any
 10c:	fcmp	d14, d14
 110:	fccmp	d8, d8, #0x0, vs
 114:	b.vs	1a0 <__divdc3+0x1a0>
 118:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
 11c:	fmov	d3, x0
 120:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 124:	fmov	d2, x0
 128:	bsl	v2.8b, v9.8b, v3.8b
 12c:	fmul	d0, d2, d14
 130:	fmul	d1, d2, d8
 134:	ldp	x19, x20, [sp, #16]
 138:	ldp	d8, d9, [sp, #32]
 13c:	ldp	d10, d11, [sp, #48]
 140:	ldp	d12, d13, [sp, #64]
 144:	ldp	d14, d15, [sp, #80]
 148:	ldp	x29, x30, [sp], #112
 14c:	ret
 150:	fcmp	d10, #0.0
 154:	b.eq	18c <__divdc3+0x18c>  // b.none
 158:	cbz	w1, 168 <__divdc3+0x168>
 15c:	sub	w1, w1, #0x3ff
 160:	scvtf	d10, w1
 164:	b	5c <__divdc3+0x5c>
 168:	and	x0, x0, #0x7fffffffffffffff
 16c:	clz	x1, x0
 170:	sub	w1, w1, #0xb
 174:	lsl	x0, x0, x1
 178:	ubfx	x0, x0, #52, #11
 17c:	sub	w0, w0, #0x3ff
 180:	sub	w0, w0, w1
 184:	scvtf	d10, w0
 188:	b	5c <__divdc3+0x5c>
 18c:	adrp	x0, 0 <__divdc3>
 190:	ldr	d10, [x0]
 194:	b	5c <__divdc3+0x5c>
 198:	mov	w19, #0x0                   	// #0
 19c:	b	a0 <__divdc3+0xa0>
 1a0:	fabs	d2, d14
 1a4:	fcmp	d2, d15
 1a8:	b.gt	1b8 <__divdc3+0x1b8>
 1ac:	fabs	d3, d8
 1b0:	fcmp	d3, d15
 1b4:	b.le	238 <__divdc3+0x238>
 1b8:	fabs	d3, d9
 1bc:	fcmp	d3, d15
 1c0:	b.gt	238 <__divdc3+0x238>
 1c4:	fcmp	d9, d9
 1c8:	b.vs	238 <__divdc3+0x238>
 1cc:	fabs	d3, d13
 1d0:	fcmp	d3, d15
 1d4:	b.gt	238 <__divdc3+0x238>
 1d8:	fcmp	d13, d13
 1dc:	b.vs	238 <__divdc3+0x238>
 1e0:	fcmp	d2, d15
 1e4:	movi	d5, #0x0
 1e8:	fmov	d0, #1.000000000000000000e+00
 1ec:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1f0:	fmov	d4, x0
 1f4:	adrp	x0, 0 <__divdc3>
 1f8:	fcsel	d1, d5, d0, le
 1fc:	ldr	d2, [x0]
 200:	bif	v14.8b, v1.8b, v4.8b
 204:	fabs	d1, d8
 208:	fcmp	d1, d15
 20c:	fcsel	d5, d5, d0, le
 210:	bsl	v4.8b, v8.8b, v5.8b
 214:	fmul	d5, d9, d14
 218:	fmul	d14, d13, d14
 21c:	fmul	d0, d13, d4
 220:	fmul	d1, d9, d4
 224:	fadd	d5, d5, d0
 228:	fsub	d1, d1, d14
 22c:	fmul	d0, d5, d2
 230:	fmul	d1, d1, d2
 234:	b	134 <__divdc3+0x134>
 238:	fcmp	d12, d15
 23c:	b.le	134 <__divdc3+0x134>
 240:	fcmpe	d10, #0.0
 244:	b.le	134 <__divdc3+0x134>
 248:	fcmp	d2, d15
 24c:	b.gt	134 <__divdc3+0x134>
 250:	fcmp	d14, d14
 254:	b.vs	134 <__divdc3+0x134>
 258:	fabs	d2, d8
 25c:	fcmp	d2, d15
 260:	b.gt	134 <__divdc3+0x134>
 264:	fcmp	d8, d8
 268:	b.vs	134 <__divdc3+0x134>
 26c:	fabs	d0, d9
 270:	movi	d4, #0x0
 274:	fabs	d2, d13
 278:	fmov	d5, #1.000000000000000000e+00
 27c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 280:	fcmp	d0, d15
 284:	fmov	d0, x0
 288:	fcsel	d1, d4, d5, le
 28c:	fcmp	d2, d15
 290:	fcsel	d5, d4, d5, le
 294:	bit	v1.8b, v9.8b, v0.8b
 298:	bif	v13.8b, v5.8b, v0.8b
 29c:	fmul	d5, d14, d1
 2a0:	fmul	d1, d8, d1
 2a4:	fmul	d0, d8, d13
 2a8:	fmul	d14, d14, d13
 2ac:	fadd	d5, d5, d0
 2b0:	fsub	d1, d1, d14
 2b4:	fmul	d0, d5, d4
 2b8:	fmul	d1, d1, d4
 2bc:	b	134 <__divdc3+0x134>

divdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdf3>:
   0:	fmov	x3, d0
   4:	fmov	x5, d1
   8:	ubfx	x8, x3, #52, #11
   c:	eor	x0, x5, x3
  10:	and	x2, x0, #0x8000000000000000
  14:	sub	w0, w8, #0x1
  18:	ubfx	x10, x5, #52, #11
  1c:	and	x1, x3, #0xfffffffffffff
  20:	and	x6, x5, #0xfffffffffffff
  24:	cmp	w0, #0x7fd
  28:	b.hi	38 <__divdf3+0x38>  // b.pmore
  2c:	sub	w0, w10, #0x1
  30:	cmp	w0, #0x7fd
  34:	b.ls	224 <__divdf3+0x224>  // b.plast
  38:	and	x7, x3, #0x7fffffffffffffff
  3c:	mov	x9, #0x7ff0000000000000    	// #9218868437227405312
  40:	cmp	x7, x9
  44:	b.ls	54 <__divdf3+0x54>  // b.plast
  48:	orr	x0, x3, #0x8000000000000
  4c:	fmov	d0, x0
  50:	ret
  54:	and	x0, x5, #0x7fffffffffffffff
  58:	cmp	x0, x9
  5c:	b.ls	68 <__divdf3+0x68>  // b.plast
  60:	orr	x0, x5, #0x8000000000000
  64:	b	4c <__divdf3+0x4c>
  68:	cmp	x7, x9
  6c:	b.ne	90 <__divdf3+0x90>  // b.any
  70:	mov	x1, #0x7ff8000000000000    	// #9221120237041090560
  74:	cmp	x0, x7
  78:	fmov	d1, x1
  7c:	orr	x1, x2, #0x7ff0000000000000
  80:	fmov	d0, x1
  84:	fcsel	d0, d1, d0, eq  // eq = none
  88:	fmov	x0, d0
  8c:	b	4c <__divdf3+0x4c>
  90:	cmp	x0, x9
  94:	b.ne	a0 <__divdf3+0xa0>  // b.any
  98:	mov	x0, x2
  9c:	b	4c <__divdf3+0x4c>
  a0:	cbnz	x7, bc <__divdf3+0xbc>
  a4:	cmp	x0, #0x0
  a8:	mov	x1, #0x7ff8000000000000    	// #9221120237041090560
  ac:	fmov	d1, x2
  b0:	fmov	d0, x1
  b4:	fcsel	d0, d1, d0, ne  // ne = any
  b8:	b	88 <__divdf3+0x88>
  bc:	cbnz	x0, c8 <__divdf3+0xc8>
  c0:	orr	x0, x2, #0x7ff0000000000000
  c4:	b	4c <__divdf3+0x4c>
  c8:	tst	x3, #0x7ff0000000000000
  cc:	b.ne	21c <__divdf3+0x21c>  // b.any
  d0:	clz	x3, x1
  d4:	sub	w4, w3, #0xb
  d8:	mov	w3, #0x1                   	// #1
  dc:	sub	w3, w3, w4
  e0:	lsl	x1, x1, x4
  e4:	tst	x5, #0x7ff0000000000000
  e8:	b.ne	100 <__divdf3+0x100>  // b.any
  ec:	clz	x4, x6
  f0:	sub	w4, w4, #0xb
  f4:	lsl	x6, x6, x4
  f8:	add	w4, w3, w4
  fc:	sub	w3, w4, #0x1
 100:	orr	x6, x6, #0x10000000000000
 104:	sub	w4, w8, w10
 108:	mov	w5, #0xf333                	// #62259
 10c:	add	w4, w4, w3
 110:	lsr	x0, x6, #21
 114:	movk	w5, #0x7504, lsl #16
 118:	and	x8, x0, #0xffffffff
 11c:	sub	w3, w5, w0
 120:	sub	w0, w5, w0
 124:	orr	x7, x1, #0x10000000000000
 128:	lsl	w1, w1, #2
 12c:	mul	x0, x0, x8
 130:	lsr	x0, x0, #32
 134:	neg	w0, w0
 138:	umull	x0, w0, w3
 13c:	ubfx	x0, x0, #31, #32
 140:	mul	x3, x0, x8
 144:	lsr	x3, x3, #32
 148:	neg	w3, w3
 14c:	mul	x0, x3, x0
 150:	ubfx	x0, x0, #31, #32
 154:	mul	x3, x8, x0
 158:	lsr	x3, x3, #32
 15c:	neg	w3, w3
 160:	mul	x3, x3, x0
 164:	lsr	x3, x3, #31
 168:	sub	w5, w3, #0x1
 16c:	lsl	w3, w6, #11
 170:	mul	x8, x5, x8
 174:	umull	x3, w3, w5
 178:	add	x3, x8, x3, lsr #32
 17c:	ubfx	x8, x7, #30, #32
 180:	neg	x0, x3
 184:	neg	w3, w3
 188:	lsr	x0, x0, #32
 18c:	mul	x3, x3, x5
 190:	mul	x0, x0, x5
 194:	sub	x0, x0, #0x2
 198:	add	x3, x0, x3, lsr #32
 19c:	and	x9, x3, #0xffffffff
 1a0:	lsr	x3, x3, #32
 1a4:	mul	x5, x9, x8
 1a8:	mul	x0, x1, x3
 1ac:	mul	x1, x1, x9
 1b0:	and	x9, x0, #0xffffffff
 1b4:	mul	x3, x3, x8
 1b8:	add	x1, x9, x1, lsr #32
 1bc:	add	x1, x1, w5, uxtw
 1c0:	lsr	x5, x5, #32
 1c4:	add	x0, x5, x0, lsr #32
 1c8:	add	x1, x3, x1, lsr #32
 1cc:	add	x0, x1, x0
 1d0:	mov	x1, #0x1fffffffffffff      	// #9007199254740991
 1d4:	cmp	x0, x1
 1d8:	b.hi	22c <__divdf3+0x22c>  // b.pmore
 1dc:	lsl	x7, x7, #53
 1e0:	sub	w4, w4, #0x1
 1e4:	msub	x7, x6, x0, x7
 1e8:	add	w4, w4, #0x3ff
 1ec:	cmp	w4, #0x7fe
 1f0:	b.gt	c0 <__divdf3+0xc0>
 1f4:	cmp	w4, #0x0
 1f8:	b.gt	23c <__divdf3+0x23c>
 1fc:	b.ne	98 <__divdf3+0x98>  // b.any
 200:	cmp	x6, x7, lsl #1
 204:	and	x1, x0, #0xfffffffffffff
 208:	cinc	x1, x1, cc  // cc = lo, ul, last
 20c:	tst	x1, #0x10000000000000
 210:	b.eq	98 <__divdf3+0x98>  // b.none
 214:	orr	x0, x2, #0x10000000000000
 218:	b	4c <__divdf3+0x4c>
 21c:	mov	w3, #0x0                   	// #0
 220:	b	e4 <__divdf3+0xe4>
 224:	mov	w3, #0x0                   	// #0
 228:	b	100 <__divdf3+0x100>
 22c:	lsr	x0, x0, #1
 230:	lsl	x7, x7, #52
 234:	msub	x7, x6, x0, x7
 238:	b	1e8 <__divdf3+0x1e8>
 23c:	and	x0, x0, #0xfffffffffffff
 240:	cmp	x6, x7, lsl #1
 244:	orr	x0, x0, x4, lsl #52
 248:	cinc	x0, x0, cc  // cc = lo, ul, last
 24c:	orr	x0, x0, x2
 250:	b	4c <__divdf3+0x4c>

divdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divdi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	asr	x3, x0, #63
   8:	eor	x4, x1, x1, asr #63
   c:	mov	x29, sp
  10:	eor	x0, x0, x0, asr #63
  14:	str	x19, [sp, #16]
  18:	eor	x19, x3, x1, asr #63
  1c:	sub	x0, x0, x3
  20:	sub	x1, x4, x1, asr #63
  24:	mov	x2, #0x0                   	// #0
  28:	bl	0 <__udivmoddi4>
  2c:	eor	x0, x0, x19
  30:	sub	x0, x0, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #32
  3c:	ret

divmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmoddi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x1
  10:	mov	x20, x2
  14:	str	x21, [sp, #32]
  18:	mov	x21, x0
  1c:	bl	0 <__divdi3>
  20:	msub	x19, x19, x0, x21
  24:	ldr	x21, [sp, #32]
  28:	str	x19, [x20]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	x20, x2
  14:	str	x21, [sp, #32]
  18:	mov	w21, w0
  1c:	bl	0 <__divsi3>
  20:	msub	w19, w19, w0, w21
  24:	ldr	x21, [sp, #32]
  28:	str	w19, [x20]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

divsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsc3>:
   0:	stp	x29, x30, [sp, #-112]!
   4:	mov	x29, sp
   8:	stp	d10, d11, [sp, #48]
   c:	fabs	s10, s2
  10:	stp	d14, d15, [sp, #80]
  14:	fmov	s14, s0
  18:	fabs	s0, s3
  1c:	str	x19, [sp, #16]
  20:	stp	d8, d9, [sp, #32]
  24:	fmov	s8, s1
  28:	fmov	s9, s2
  2c:	fmaxnm	s10, s10, s0
  30:	stp	d12, d13, [sp, #64]
  34:	fmov	s13, s3
  38:	fmov	w0, s10
  3c:	ubfx	x1, x0, #23, #8
  40:	cmp	w1, #0xff
  44:	b.ne	154 <__divsc3+0x154>  // b.any
  48:	cmp	w0, #0x0
  4c:	fmov	s0, s10
  50:	fccmp	s10, s10, #0x0, lt  // lt = tstop
  54:	fneg	s10, s10
  58:	fcsel	s10, s10, s0, eq  // eq = none
  5c:	adrp	x0, 0 <__divsc3>
  60:	fabs	s0, s10
  64:	ldr	s15, [x0]
  68:	str	s0, [sp, #108]
  6c:	fcmp	s0, s15
  70:	b.gt	198 <__divsc3+0x198>
  74:	fcmp	s10, s10
  78:	b.vs	198 <__divsc3+0x198>
  7c:	fcvtzs	s11, s10
  80:	fmov	s0, s9
  84:	fmov	w0, s11
  88:	neg	w19, w0
  8c:	mov	w0, w19
  90:	bl	0 <scalbnf>
  94:	fmov	s9, s0
  98:	fmov	s0, s13
  9c:	mov	w0, w19
  a0:	bl	0 <scalbnf>
  a4:	fmov	s13, s0
  a8:	fmul	s0, s13, s13
  ac:	fmul	s2, s9, s9
  b0:	fmul	s1, s13, s8
  b4:	fmov	w0, s11
  b8:	fadd	s12, s2, s0
  bc:	fmul	s0, s9, s14
  c0:	neg	w19, w0
  c4:	mov	w0, w19
  c8:	fadd	s0, s0, s1
  cc:	fdiv	s0, s0, s12
  d0:	bl	0 <scalbnf>
  d4:	fmov	s11, s0
  d8:	fmul	s1, s13, s14
  dc:	fmul	s0, s9, s8
  e0:	mov	w0, w19
  e4:	fsub	s0, s0, s1
  e8:	fdiv	s0, s0, s12
  ec:	bl	0 <scalbnf>
  f0:	fcmp	s11, s11
  f4:	fmov	s1, s0
  f8:	fmov	s0, s11
  fc:	b.vc	138 <__divsc3+0x138>
 100:	fcmp	s1, s1
 104:	b.vc	138 <__divsc3+0x138>
 108:	fcmp	s12, #0.0
 10c:	b.ne	1a0 <__divsc3+0x1a0>  // b.any
 110:	fcmp	s14, s14
 114:	fccmp	s8, s8, #0x0, vs
 118:	b.vs	1a0 <__divsc3+0x1a0>
 11c:	movi	v1.2s, #0x80, lsl #24
 120:	mov	w0, #0x7f800000            	// #2139095040
 124:	fmov	s3, w0
 128:	fmov	s2, s1
 12c:	bsl	v2.8b, v9.8b, v3.8b
 130:	fmul	s0, s2, s14
 134:	fmul	s1, s2, s8
 138:	ldr	x19, [sp, #16]
 13c:	ldp	d8, d9, [sp, #32]
 140:	ldp	d10, d11, [sp, #48]
 144:	ldp	d12, d13, [sp, #64]
 148:	ldp	d14, d15, [sp, #80]
 14c:	ldp	x29, x30, [sp], #112
 150:	ret
 154:	fcmp	s10, #0.0
 158:	b.eq	190 <__divsc3+0x190>  // b.none
 15c:	cbz	w1, 16c <__divsc3+0x16c>
 160:	sub	w1, w1, #0x7f
 164:	scvtf	s10, w1
 168:	b	5c <__divsc3+0x5c>
 16c:	and	w0, w0, #0x7fffffff
 170:	clz	w1, w0
 174:	sub	w1, w1, #0x8
 178:	lsl	w0, w0, w1
 17c:	ubfx	x0, x0, #23, #8
 180:	sub	w0, w0, #0x7f
 184:	sub	w0, w0, w1
 188:	scvtf	s10, w0
 18c:	b	5c <__divsc3+0x5c>
 190:	mvni	v10.2s, #0x7f, msl #16
 194:	b	5c <__divsc3+0x5c>
 198:	fmov	s11, wzr
 19c:	b	a8 <__divsc3+0xa8>
 1a0:	fabs	s2, s14
 1a4:	fcmp	s2, s15
 1a8:	b.gt	1b8 <__divsc3+0x1b8>
 1ac:	fabs	s3, s8
 1b0:	fcmp	s3, s15
 1b4:	b.le	234 <__divsc3+0x234>
 1b8:	fabs	s3, s9
 1bc:	fcmp	s3, s15
 1c0:	b.gt	234 <__divsc3+0x234>
 1c4:	fcmp	s9, s9
 1c8:	b.vs	234 <__divsc3+0x234>
 1cc:	fabs	s3, s13
 1d0:	fcmp	s3, s15
 1d4:	b.gt	234 <__divsc3+0x234>
 1d8:	fcmp	s13, s13
 1dc:	b.vs	234 <__divsc3+0x234>
 1e0:	fcmp	s2, s15
 1e4:	fabs	s0, s8
 1e8:	movi	v4.2s, #0x80, lsl #24
 1ec:	cset	w0, gt
 1f0:	fcmp	s0, s15
 1f4:	scvtf	s1, w0
 1f8:	cset	w0, gt
 1fc:	scvtf	s5, w0
 200:	bif	v14.8b, v1.8b, v4.8b
 204:	adrp	x0, 0 <__divsc3>
 208:	ldr	s2, [x0]
 20c:	bsl	v4.8b, v8.8b, v5.8b
 210:	fmul	s5, s9, s14
 214:	fmul	s14, s13, s14
 218:	fmul	s0, s13, s4
 21c:	fmul	s1, s9, s4
 220:	fadd	s5, s5, s0
 224:	fsub	s1, s1, s14
 228:	fmul	s0, s5, s2
 22c:	fmul	s1, s1, s2
 230:	b	138 <__divsc3+0x138>
 234:	fabs	s3, s10
 238:	fcmp	s3, s15
 23c:	b.le	138 <__divsc3+0x138>
 240:	fcmpe	s10, #0.0
 244:	b.le	138 <__divsc3+0x138>
 248:	fcmp	s2, s15
 24c:	b.gt	138 <__divsc3+0x138>
 250:	fcmp	s14, s14
 254:	b.vs	138 <__divsc3+0x138>
 258:	fabs	s2, s8
 25c:	fcmp	s2, s15
 260:	b.gt	138 <__divsc3+0x138>
 264:	fcmp	s8, s8
 268:	b.vs	138 <__divsc3+0x138>
 26c:	fabs	s0, s9
 270:	fabs	s2, s13
 274:	movi	v4.2s, #0x0
 278:	fcmp	s0, s15
 27c:	movi	v0.2s, #0x80, lsl #24
 280:	cset	w0, gt
 284:	fcmp	s2, s15
 288:	scvtf	s1, w0
 28c:	cset	w0, gt
 290:	scvtf	s3, w0
 294:	bit	v1.8b, v9.8b, v0.8b
 298:	bif	v13.8b, v3.8b, v0.8b
 29c:	fmul	s5, s14, s1
 2a0:	fmul	s1, s8, s1
 2a4:	fmul	s0, s8, s13
 2a8:	fmul	s14, s14, s13
 2ac:	fadd	s5, s5, s0
 2b0:	fsub	s1, s1, s14
 2b4:	fmul	s0, s5, s4
 2b8:	fmul	s1, s1, s4
 2bc:	b	138 <__divsc3+0x138>

divsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsf3>:
   0:	fmov	w6, s0
   4:	fmov	w7, s1
   8:	ubfx	x2, x6, #23, #8
   c:	eor	w0, w7, w6
  10:	sub	w3, w2, #0x1
  14:	ubfx	x10, x7, #23, #8
  18:	and	w0, w0, #0x80000000
  1c:	and	w1, w6, #0x7fffff
  20:	and	w5, w7, #0x7fffff
  24:	cmp	w3, #0xfd
  28:	b.hi	38 <__divsf3+0x38>  // b.pmore
  2c:	sub	w3, w10, #0x1
  30:	cmp	w3, #0xfd
  34:	b.ls	1bc <__divsf3+0x1bc>  // b.plast
  38:	and	w3, w6, #0x7fffffff
  3c:	mov	w9, #0x7f800000            	// #2139095040
  40:	cmp	w3, w9
  44:	b.ls	54 <__divsf3+0x54>  // b.plast
  48:	orr	w0, w6, #0x400000
  4c:	fmov	s0, w0
  50:	ret
  54:	and	w8, w7, #0x7fffffff
  58:	cmp	w8, w9
  5c:	b.ls	68 <__divsf3+0x68>  // b.plast
  60:	orr	w0, w7, #0x400000
  64:	b	4c <__divsf3+0x4c>
  68:	cmp	w3, w9
  6c:	b.ne	90 <__divsf3+0x90>  // b.any
  70:	cmp	w8, w3
  74:	orr	w0, w0, #0x7f800000
  78:	mov	w1, #0x7fc00000            	// #2143289344
  7c:	fmov	s0, w0
  80:	fmov	s1, w1
  84:	fcsel	s0, s1, s0, eq  // eq = none
  88:	fmov	w0, s0
  8c:	b	4c <__divsf3+0x4c>
  90:	cmp	w8, w9
  94:	b.eq	4c <__divsf3+0x4c>  // b.none
  98:	cbnz	w3, b4 <__divsf3+0xb4>
  9c:	cmp	w8, #0x0
  a0:	mov	w1, #0x7fc00000            	// #2143289344
  a4:	fmov	s1, w0
  a8:	fmov	s0, w1
  ac:	fcsel	s0, s1, s0, ne  // ne = any
  b0:	b	88 <__divsf3+0x88>
  b4:	cbnz	w8, c0 <__divsf3+0xc0>
  b8:	orr	w0, w0, #0x7f800000
  bc:	b	4c <__divsf3+0x4c>
  c0:	tst	w6, #0x7f800000
  c4:	b.ne	1b4 <__divsf3+0x1b4>  // b.any
  c8:	clz	w6, w1
  cc:	sub	w4, w6, #0x8
  d0:	mov	w6, #0x1                   	// #1
  d4:	sub	w6, w6, w4
  d8:	lsl	w1, w1, w4
  dc:	tst	w7, #0x7f800000
  e0:	b.ne	f8 <__divsf3+0xf8>  // b.any
  e4:	clz	w4, w5
  e8:	sub	w4, w4, #0x8
  ec:	lsl	w5, w5, w4
  f0:	add	w4, w6, w4
  f4:	sub	w6, w4, #0x1
  f8:	sub	w2, w2, w10
  fc:	orr	w5, w5, #0x800000
 100:	add	w4, w2, w6
 104:	mov	w6, #0xf333                	// #62259
 108:	movk	w6, #0x7504, lsl #16
 10c:	lsl	w3, w5, #8
 110:	sub	w6, w6, w5, lsl #8
 114:	lsl	w2, w5, #8
 118:	orr	w7, w1, #0x800000
 11c:	umull	x3, w6, w3
 120:	lsr	x3, x3, #32
 124:	neg	w3, w3
 128:	umull	x3, w3, w6
 12c:	ubfx	x3, x3, #31, #32
 130:	mul	x6, x2, x3
 134:	lsr	x6, x6, #32
 138:	neg	w6, w6
 13c:	mul	x6, x6, x3
 140:	ubfx	x6, x6, #31, #32
 144:	mul	x1, x2, x6
 148:	lsl	w2, w7, #1
 14c:	lsr	x1, x1, #32
 150:	neg	w1, w1
 154:	mul	x1, x1, x6
 158:	lsr	x1, x1, #31
 15c:	sub	w1, w1, #0x2
 160:	umull	x1, w1, w2
 164:	mov	x2, #0xffffff              	// #16777215
 168:	lsr	x1, x1, #32
 16c:	mov	w6, w1
 170:	cmp	x1, x2
 174:	b.hi	1c4 <__divsf3+0x1c4>  // b.pmore
 178:	lsl	w3, w7, #24
 17c:	sub	w4, w4, #0x1
 180:	msub	w1, w5, w1, w3
 184:	add	w2, w4, #0x7f
 188:	cmp	w2, #0xfe
 18c:	b.gt	b8 <__divsf3+0xb8>
 190:	cmp	w2, #0x0
 194:	b.gt	1d4 <__divsf3+0x1d4>
 198:	b.ne	4c <__divsf3+0x4c>  // b.any
 19c:	cmp	w5, w1, lsl #1
 1a0:	and	w6, w6, #0x7fffff
 1a4:	cinc	w6, w6, cc  // cc = lo, ul, last
 1a8:	tbz	w6, #23, 4c <__divsf3+0x4c>
 1ac:	orr	w0, w0, #0x800000
 1b0:	b	4c <__divsf3+0x4c>
 1b4:	mov	w6, #0x0                   	// #0
 1b8:	b	dc <__divsf3+0xdc>
 1bc:	mov	w6, #0x0                   	// #0
 1c0:	b	f8 <__divsf3+0xf8>
 1c4:	lsr	w6, w1, #1
 1c8:	lsl	w1, w7, #23
 1cc:	msub	w1, w5, w6, w1
 1d0:	b	184 <__divsf3+0x184>
 1d4:	and	w6, w6, #0x7fffff
 1d8:	cmp	w5, w1, lsl #1
 1dc:	orr	w2, w6, w2, lsl #23
 1e0:	cinc	w2, w2, cc  // cc = lo, ul, last
 1e4:	orr	w0, w2, w0
 1e8:	b	4c <__divsf3+0x4c>

divsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divsi3>:
   0:	asr	w3, w0, #31
   4:	eor	w4, w1, w1, asr #31
   8:	eor	w0, w0, w0, asr #31
   c:	eor	w2, w3, w1, asr #31
  10:	sub	w0, w0, w3
  14:	sub	w1, w4, w1, asr #31
  18:	udiv	w0, w0, w1
  1c:	eor	w0, w0, w2
  20:	sub	w0, w0, w2
  24:	ret

divtc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divtc3>:
   0:	stp	x29, x30, [sp, #-176]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #96]
   c:	stp	x23, x24, [sp, #48]
  10:	stp	x25, x26, [sp, #64]
  14:	ldp	x23, x26, [sp, #96]
  18:	str	q1, [sp, #96]
  1c:	ldp	x25, x24, [sp, #96]
  20:	str	q2, [sp, #96]
  24:	stp	x19, x20, [sp, #16]
  28:	stp	x21, x22, [sp, #32]
  2c:	ldp	x22, x19, [sp, #96]
  30:	str	q3, [sp, #96]
  34:	ldp	x21, x20, [sp, #96]
  38:	stp	x27, x28, [sp, #80]
  3c:	and	x0, x20, #0x7fffffffffffffff
  40:	str	x0, [sp, #104]
  44:	and	x0, x19, #0x7fffffffffffffff
  48:	ldr	q1, [sp, #96]
  4c:	stp	x22, x0, [sp, #96]
  50:	ldr	q0, [sp, #96]
  54:	bl	0 <fmaxl>
  58:	str	q0, [sp, #96]
  5c:	ldp	x28, x27, [sp, #96]
  60:	mov	w0, #0x7fff                	// #32767
  64:	ubfx	x2, x27, #48, #15
  68:	cmp	w2, w0
  6c:	b.ne	338 <__divtc3+0x338>  // b.any
  70:	mvn	x1, x27
  74:	mov	v1.16b, v0.16b
  78:	str	x28, [sp, #128]
  7c:	lsr	x1, x1, #63
  80:	str	x1, [sp, #112]
  84:	bl	0 <__netf2>
  88:	cmp	w0, #0x0
  8c:	ldr	x1, [sp, #112]
  90:	cset	w0, ne  // ne = any
  94:	orr	w1, w0, w1
  98:	cbnz	w1, a8 <__divtc3+0xa8>
  9c:	ldr	x3, [sp, #128]
  a0:	eor	x27, x27, #0x8000000000000000
  a4:	mov	x28, x3
  a8:	adrp	x0, 0 <__divtc3>
  ac:	add	x0, x0, #0x0
  b0:	str	x28, [sp, #96]
  b4:	ldr	q1, [x0]
  b8:	and	x0, x27, #0x7fffffffffffffff
  bc:	str	x0, [sp, #104]
  c0:	ldr	q0, [sp, #96]
  c4:	bl	0 <__unordtf2>
  c8:	cbnz	w0, f4 <__divtc3+0xf4>
  cc:	adrp	x0, 0 <__divtc3>
  d0:	add	x0, x0, #0x0
  d4:	str	x28, [sp, #96]
  d8:	ldr	q1, [x0]
  dc:	and	x0, x27, #0x7fffffffffffffff
  e0:	str	x0, [sp, #104]
  e4:	ldr	q0, [sp, #96]
  e8:	bl	0 <__letf2>
  ec:	cmp	w0, #0x0
  f0:	b.gt	3c4 <__divtc3+0x3c4>
  f4:	stp	x28, x27, [sp, #96]
  f8:	ldr	q1, [sp, #96]
  fc:	mov	v0.16b, v1.16b
 100:	bl	0 <__unordtf2>
 104:	cbnz	w0, 3c4 <__divtc3+0x3c4>
 108:	stp	x28, x27, [sp, #96]
 10c:	ldr	q0, [sp, #96]
 110:	bl	0 <__fixtfsi>
 114:	stp	x22, x19, [sp, #96]
 118:	mov	w1, w0
 11c:	ldr	q0, [sp, #96]
 120:	neg	w0, w0
 124:	str	w0, [sp, #112]
 128:	str	w1, [sp, #128]
 12c:	bl	0 <scalbnl>
 130:	str	q0, [sp, #96]
 134:	ldp	x22, x19, [sp, #96]
 138:	stp	x21, x20, [sp, #96]
 13c:	ldr	w0, [sp, #112]
 140:	ldr	q0, [sp, #96]
 144:	bl	0 <scalbnl>
 148:	str	q0, [sp, #96]
 14c:	ldp	x21, x20, [sp, #96]
 150:	ldr	w1, [sp, #128]
 154:	stp	x22, x19, [sp, #96]
 158:	ldr	q1, [sp, #96]
 15c:	str	w1, [sp, #144]
 160:	mov	v0.16b, v1.16b
 164:	bl	0 <__multf3>
 168:	stp	x21, x20, [sp, #96]
 16c:	ldr	q1, [sp, #96]
 170:	str	q0, [sp, #112]
 174:	mov	v0.16b, v1.16b
 178:	bl	0 <__multf3>
 17c:	ldr	q2, [sp, #112]
 180:	mov	v1.16b, v0.16b
 184:	mov	v0.16b, v2.16b
 188:	bl	0 <__addtf3>
 18c:	stp	x23, x26, [sp, #96]
 190:	ldr	w1, [sp, #144]
 194:	ldr	q1, [sp, #96]
 198:	stp	x22, x19, [sp, #96]
 19c:	neg	w0, w1
 1a0:	str	q0, [sp, #128]
 1a4:	ldr	q0, [sp, #96]
 1a8:	str	w0, [sp, #144]
 1ac:	bl	0 <__multf3>
 1b0:	stp	x25, x24, [sp, #96]
 1b4:	ldr	q1, [sp, #96]
 1b8:	stp	x21, x20, [sp, #96]
 1bc:	str	q0, [sp, #112]
 1c0:	ldr	q0, [sp, #96]
 1c4:	bl	0 <__multf3>
 1c8:	mov	v1.16b, v0.16b
 1cc:	ldr	q2, [sp, #112]
 1d0:	mov	v0.16b, v2.16b
 1d4:	bl	0 <__addtf3>
 1d8:	ldr	q1, [sp, #128]
 1dc:	bl	0 <__divtf3>
 1e0:	ldr	w0, [sp, #144]
 1e4:	str	w0, [sp, #172]
 1e8:	bl	0 <scalbnl>
 1ec:	stp	x25, x24, [sp, #96]
 1f0:	ldr	q1, [sp, #96]
 1f4:	stp	x22, x19, [sp, #96]
 1f8:	str	q0, [sp, #144]
 1fc:	ldr	q0, [sp, #96]
 200:	bl	0 <__multf3>
 204:	stp	x23, x26, [sp, #96]
 208:	ldr	q1, [sp, #96]
 20c:	stp	x21, x20, [sp, #96]
 210:	str	q0, [sp, #112]
 214:	ldr	q0, [sp, #96]
 218:	bl	0 <__multf3>
 21c:	mov	v1.16b, v0.16b
 220:	ldr	q4, [sp, #112]
 224:	mov	v0.16b, v4.16b
 228:	bl	0 <__subtf3>
 22c:	ldr	q1, [sp, #128]
 230:	bl	0 <__divtf3>
 234:	ldr	w0, [sp, #172]
 238:	bl	0 <scalbnl>
 23c:	str	q0, [sp, #112]
 240:	ldr	q2, [sp, #144]
 244:	str	q0, [sp, #144]
 248:	mov	v1.16b, v2.16b
 24c:	mov	v0.16b, v2.16b
 250:	str	q2, [sp, #96]
 254:	bl	0 <__unordtf2>
 258:	ldr	q4, [sp, #144]
 25c:	cbz	w0, 314 <__divtc3+0x314>
 260:	mov	v1.16b, v4.16b
 264:	mov	v0.16b, v4.16b
 268:	bl	0 <__unordtf2>
 26c:	cbz	w0, 314 <__divtc3+0x314>
 270:	movi	v1.2d, #0x0
 274:	ldr	q0, [sp, #128]
 278:	bl	0 <__eqtf2>
 27c:	cbnz	w0, 3cc <__divtc3+0x3cc>
 280:	stp	x23, x26, [sp, #128]
 284:	ldr	q1, [sp, #128]
 288:	mov	v0.16b, v1.16b
 28c:	bl	0 <__unordtf2>
 290:	stp	x25, x24, [sp, #128]
 294:	cmp	w0, #0x0
 298:	cset	w1, eq  // eq = none
 29c:	ldr	q1, [sp, #128]
 2a0:	str	w1, [sp, #144]
 2a4:	mov	v0.16b, v1.16b
 2a8:	bl	0 <__unordtf2>
 2ac:	cmp	w0, #0x0
 2b0:	ldr	w1, [sp, #144]
 2b4:	cset	w0, eq  // eq = none
 2b8:	orr	w1, w1, w0
 2bc:	tbz	w1, #0, 3cc <__divtc3+0x3cc>
 2c0:	adrp	x0, 0 <__divtc3>
 2c4:	add	x0, x0, #0x0
 2c8:	ldr	q4, [x0]
 2cc:	tbz	x19, #63, 2dc <__divtc3+0x2dc>
 2d0:	adrp	x0, 0 <__divtc3>
 2d4:	add	x0, x0, #0x0
 2d8:	ldr	q4, [x0]
 2dc:	stp	x23, x26, [sp, #96]
 2e0:	mov	v0.16b, v4.16b
 2e4:	ldr	q1, [sp, #96]
 2e8:	str	q4, [sp, #128]
 2ec:	bl	0 <__multf3>
 2f0:	stp	x25, x24, [sp, #96]
 2f4:	ldr	q4, [sp, #128]
 2f8:	ldr	q1, [sp, #96]
 2fc:	str	q0, [sp, #112]
 300:	mov	v0.16b, v4.16b
 304:	bl	0 <__multf3>
 308:	ldr	q2, [sp, #112]
 30c:	str	q2, [sp, #96]
 310:	str	q0, [sp, #112]
 314:	ldp	x19, x20, [sp, #16]
 318:	ldp	x21, x22, [sp, #32]
 31c:	ldp	x23, x24, [sp, #48]
 320:	ldp	x25, x26, [sp, #64]
 324:	ldp	x27, x28, [sp, #80]
 328:	ldr	q0, [sp, #96]
 32c:	ldr	q1, [sp, #112]
 330:	ldp	x29, x30, [sp], #176
 334:	ret
 338:	movi	v1.2d, #0x0
 33c:	str	w2, [sp, #96]
 340:	str	x28, [sp, #112]
 344:	str	x27, [sp, #128]
 348:	bl	0 <__eqtf2>
 34c:	cbz	w0, 3b8 <__divtc3+0x3b8>
 350:	ldr	w2, [sp, #96]
 354:	ldr	x3, [sp, #112]
 358:	ldr	x1, [sp, #128]
 35c:	cbz	w2, 378 <__divtc3+0x378>
 360:	mov	w3, #0xffffc001            	// #-16383
 364:	add	w0, w2, w3
 368:	bl	0 <__floatsitf>
 36c:	str	q0, [sp, #96]
 370:	ldp	x28, x27, [sp, #96]
 374:	b	a8 <__divtc3+0xa8>
 378:	ands	x1, x1, #0x7fffffffffffffff
 37c:	mov	x27, #0x40                  	// #64
 380:	csel	x0, x1, x3, ne  // ne = any
 384:	cmp	x1, #0x0
 388:	csel	x27, xzr, x27, ne  // ne = any
 38c:	clz	x0, x0
 390:	add	w27, w27, w0
 394:	mov	x0, x3
 398:	sub	w2, w27, #0xf
 39c:	mov	x27, x2
 3a0:	bl	0 <__ashlti3>
 3a4:	ubfx	x0, x1, #48, #15
 3a8:	mov	w1, #0xffffc001            	// #-16383
 3ac:	add	w0, w0, w1
 3b0:	sub	w0, w0, w27
 3b4:	b	368 <__divtc3+0x368>
 3b8:	mov	x28, #0x0                   	// #0
 3bc:	mov	x27, #0xffff000000000000    	// #-281474976710656
 3c0:	b	a8 <__divtc3+0xa8>
 3c4:	mov	w1, #0x0                   	// #0
 3c8:	b	154 <__divtc3+0x154>
 3cc:	adrp	x0, 0 <__divtc3>
 3d0:	add	x0, x0, #0x0
 3d4:	ldr	q1, [x0]
 3d8:	and	x0, x26, #0x7fffffffffffffff
 3dc:	stp	x23, x0, [sp, #128]
 3e0:	ldr	q0, [sp, #128]
 3e4:	bl	0 <__unordtf2>
 3e8:	cbnz	w0, 410 <__divtc3+0x410>
 3ec:	adrp	x0, 0 <__divtc3>
 3f0:	add	x0, x0, #0x0
 3f4:	ldr	q1, [x0]
 3f8:	and	x0, x26, #0x7fffffffffffffff
 3fc:	stp	x23, x0, [sp, #128]
 400:	ldr	q0, [sp, #128]
 404:	bl	0 <__letf2>
 408:	cmp	w0, #0x0
 40c:	b.gt	458 <__divtc3+0x458>
 410:	adrp	x0, 0 <__divtc3>
 414:	add	x0, x0, #0x0
 418:	and	x1, x24, #0x7fffffffffffffff
 41c:	stp	x25, x1, [sp, #128]
 420:	ldr	q1, [x0]
 424:	ldr	q0, [sp, #128]
 428:	str	x1, [sp, #144]
 42c:	bl	0 <__unordtf2>
 430:	cbnz	w0, 660 <__divtc3+0x660>
 434:	adrp	x0, 0 <__divtc3>
 438:	add	x0, x0, #0x0
 43c:	ldr	x1, [sp, #144]
 440:	stp	x25, x1, [sp, #128]
 444:	ldr	q1, [x0]
 448:	ldr	q0, [sp, #128]
 44c:	bl	0 <__letf2>
 450:	cmp	w0, #0x0
 454:	b.le	660 <__divtc3+0x660>
 458:	adrp	x0, 0 <__divtc3>
 45c:	add	x0, x0, #0x0
 460:	and	x1, x19, #0x7fffffffffffffff
 464:	stp	x22, x1, [sp, #128]
 468:	ldr	q1, [x0]
 46c:	ldr	q0, [sp, #128]
 470:	str	x1, [sp, #144]
 474:	bl	0 <__unordtf2>
 478:	cbnz	w0, 4a0 <__divtc3+0x4a0>
 47c:	adrp	x0, 0 <__divtc3>
 480:	add	x0, x0, #0x0
 484:	ldr	x1, [sp, #144]
 488:	stp	x22, x1, [sp, #128]
 48c:	ldr	q1, [x0]
 490:	ldr	q0, [sp, #128]
 494:	bl	0 <__letf2>
 498:	cmp	w0, #0x0
 49c:	b.gt	660 <__divtc3+0x660>
 4a0:	stp	x22, x19, [sp, #128]
 4a4:	ldr	q1, [sp, #128]
 4a8:	mov	v0.16b, v1.16b
 4ac:	bl	0 <__unordtf2>
 4b0:	cbnz	w0, 660 <__divtc3+0x660>
 4b4:	adrp	x0, 0 <__divtc3>
 4b8:	add	x0, x0, #0x0
 4bc:	and	x1, x20, #0x7fffffffffffffff
 4c0:	stp	x21, x1, [sp, #128]
 4c4:	ldr	q1, [x0]
 4c8:	ldr	q0, [sp, #128]
 4cc:	str	x1, [sp, #144]
 4d0:	bl	0 <__unordtf2>
 4d4:	cbnz	w0, 4fc <__divtc3+0x4fc>
 4d8:	adrp	x0, 0 <__divtc3>
 4dc:	add	x0, x0, #0x0
 4e0:	ldr	x1, [sp, #144]
 4e4:	stp	x21, x1, [sp, #128]
 4e8:	ldr	q1, [x0]
 4ec:	ldr	q0, [sp, #128]
 4f0:	bl	0 <__letf2>
 4f4:	cmp	w0, #0x0
 4f8:	b.gt	660 <__divtc3+0x660>
 4fc:	stp	x21, x20, [sp, #128]
 500:	ldr	q1, [sp, #128]
 504:	mov	v0.16b, v1.16b
 508:	bl	0 <__unordtf2>
 50c:	cbnz	w0, 660 <__divtc3+0x660>
 510:	adrp	x0, 0 <__divtc3>
 514:	add	x0, x0, #0x0
 518:	ldr	q1, [x0]
 51c:	and	x0, x26, #0x7fffffffffffffff
 520:	stp	x23, x0, [sp, #96]
 524:	ldr	q0, [sp, #96]
 528:	bl	0 <__unordtf2>
 52c:	cbnz	w0, 650 <__divtc3+0x650>
 530:	adrp	x0, 0 <__divtc3>
 534:	add	x0, x0, #0x0
 538:	ldr	q1, [x0]
 53c:	and	x0, x26, #0x7fffffffffffffff
 540:	stp	x23, x0, [sp, #96]
 544:	ldr	q0, [sp, #96]
 548:	bl	0 <__letf2>
 54c:	cmp	w0, #0x0
 550:	b.le	650 <__divtc3+0x650>
 554:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 558:	and	x26, x26, #0x8000000000000000
 55c:	and	x23, x24, #0x7fffffffffffffff
 560:	orr	x26, x0, x26
 564:	adrp	x0, 0 <__divtc3>
 568:	add	x0, x0, #0x0
 56c:	stp	x25, x23, [sp, #96]
 570:	ldr	q0, [sp, #96]
 574:	ldr	q1, [x0]
 578:	bl	0 <__unordtf2>
 57c:	cbnz	w0, 658 <__divtc3+0x658>
 580:	adrp	x0, 0 <__divtc3>
 584:	add	x0, x0, #0x0
 588:	stp	x25, x23, [sp, #96]
 58c:	ldr	q1, [x0]
 590:	ldr	q0, [sp, #96]
 594:	bl	0 <__letf2>
 598:	cmp	w0, #0x0
 59c:	b.le	658 <__divtc3+0x658>
 5a0:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 5a4:	stp	xzr, x26, [sp, #96]
 5a8:	and	x24, x24, #0x8000000000000000
 5ac:	orr	x24, x0, x24
 5b0:	ldr	q1, [sp, #96]
 5b4:	stp	x22, x19, [sp, #96]
 5b8:	ldr	q0, [sp, #96]
 5bc:	bl	0 <__multf3>
 5c0:	stp	xzr, x24, [sp, #96]
 5c4:	ldr	q1, [sp, #96]
 5c8:	stp	x21, x20, [sp, #96]
 5cc:	str	q0, [sp, #112]
 5d0:	ldr	q0, [sp, #96]
 5d4:	bl	0 <__multf3>
 5d8:	mov	v1.16b, v0.16b
 5dc:	ldr	q2, [sp, #112]
 5e0:	mov	v0.16b, v2.16b
 5e4:	bl	0 <__addtf3>
 5e8:	adrp	x0, 0 <__divtc3>
 5ec:	add	x0, x0, #0x0
 5f0:	ldr	q1, [x0]
 5f4:	bl	0 <__multf3>
 5f8:	stp	xzr, x24, [sp, #96]
 5fc:	ldr	q1, [sp, #96]
 600:	stp	x22, x19, [sp, #96]
 604:	str	q0, [sp, #128]
 608:	ldr	q0, [sp, #96]
 60c:	bl	0 <__multf3>
 610:	stp	xzr, x26, [sp, #96]
 614:	ldr	q1, [sp, #96]
 618:	stp	x21, x20, [sp, #96]
 61c:	str	q0, [sp, #112]
 620:	ldr	q0, [sp, #96]
 624:	bl	0 <__multf3>
 628:	mov	v1.16b, v0.16b
 62c:	ldr	q4, [sp, #112]
 630:	mov	v0.16b, v4.16b
 634:	bl	0 <__subtf3>
 638:	adrp	x0, 0 <__divtc3>
 63c:	add	x0, x0, #0x0
 640:	ldr	q1, [x0]
 644:	bl	0 <__multf3>
 648:	ldr	q2, [sp, #128]
 64c:	b	30c <__divtc3+0x30c>
 650:	mov	x0, #0x0                   	// #0
 654:	b	558 <__divtc3+0x558>
 658:	mov	x0, #0x0                   	// #0
 65c:	b	5a4 <__divtc3+0x5a4>
 660:	adrp	x0, 0 <__divtc3>
 664:	add	x0, x0, #0x0
 668:	str	x28, [sp, #128]
 66c:	ldr	q1, [x0]
 670:	and	x0, x27, #0x7fffffffffffffff
 674:	str	x0, [sp, #136]
 678:	ldr	q0, [sp, #128]
 67c:	bl	0 <__unordtf2>
 680:	cbnz	w0, 314 <__divtc3+0x314>
 684:	adrp	x0, 0 <__divtc3>
 688:	add	x0, x0, #0x0
 68c:	str	x28, [sp, #128]
 690:	ldr	q1, [x0]
 694:	and	x0, x27, #0x7fffffffffffffff
 698:	str	x0, [sp, #136]
 69c:	ldr	q0, [sp, #128]
 6a0:	bl	0 <__letf2>
 6a4:	cmp	w0, #0x0
 6a8:	b.le	314 <__divtc3+0x314>
 6ac:	movi	v1.2d, #0x0
 6b0:	stp	x28, x27, [sp, #128]
 6b4:	ldr	q0, [sp, #128]
 6b8:	bl	0 <__gttf2>
 6bc:	cmp	w0, #0x0
 6c0:	b.le	314 <__divtc3+0x314>
 6c4:	adrp	x0, 0 <__divtc3>
 6c8:	add	x0, x0, #0x0
 6cc:	ldr	q1, [x0]
 6d0:	and	x0, x26, #0x7fffffffffffffff
 6d4:	stp	x23, x0, [sp, #128]
 6d8:	ldr	q0, [sp, #128]
 6dc:	bl	0 <__unordtf2>
 6e0:	cbnz	w0, 708 <__divtc3+0x708>
 6e4:	adrp	x0, 0 <__divtc3>
 6e8:	add	x0, x0, #0x0
 6ec:	ldr	q1, [x0]
 6f0:	and	x0, x26, #0x7fffffffffffffff
 6f4:	stp	x23, x0, [sp, #128]
 6f8:	ldr	q0, [sp, #128]
 6fc:	bl	0 <__letf2>
 700:	cmp	w0, #0x0
 704:	b.gt	314 <__divtc3+0x314>
 708:	stp	x23, x26, [sp, #128]
 70c:	ldr	q1, [sp, #128]
 710:	mov	v0.16b, v1.16b
 714:	bl	0 <__unordtf2>
 718:	cbnz	w0, 314 <__divtc3+0x314>
 71c:	adrp	x0, 0 <__divtc3>
 720:	add	x0, x0, #0x0
 724:	and	x27, x24, #0x7fffffffffffffff
 728:	stp	x25, x27, [sp, #128]
 72c:	ldr	q1, [x0]
 730:	ldr	q0, [sp, #128]
 734:	bl	0 <__unordtf2>
 738:	cbnz	w0, 75c <__divtc3+0x75c>
 73c:	adrp	x0, 0 <__divtc3>
 740:	add	x0, x0, #0x0
 744:	stp	x25, x27, [sp, #128]
 748:	ldr	q1, [x0]
 74c:	ldr	q0, [sp, #128]
 750:	bl	0 <__letf2>
 754:	cmp	w0, #0x0
 758:	b.gt	314 <__divtc3+0x314>
 75c:	stp	x25, x24, [sp, #128]
 760:	ldr	q1, [sp, #128]
 764:	mov	v0.16b, v1.16b
 768:	bl	0 <__unordtf2>
 76c:	cbnz	w0, 314 <__divtc3+0x314>
 770:	adrp	x0, 0 <__divtc3>
 774:	add	x0, x0, #0x0
 778:	and	x27, x19, #0x7fffffffffffffff
 77c:	stp	x22, x27, [sp, #96]
 780:	ldr	q1, [x0]
 784:	ldr	q0, [sp, #96]
 788:	bl	0 <__unordtf2>
 78c:	cbnz	w0, 894 <__divtc3+0x894>
 790:	adrp	x0, 0 <__divtc3>
 794:	add	x0, x0, #0x0
 798:	stp	x22, x27, [sp, #96]
 79c:	ldr	q1, [x0]
 7a0:	ldr	q0, [sp, #96]
 7a4:	bl	0 <__letf2>
 7a8:	cmp	w0, #0x0
 7ac:	b.le	894 <__divtc3+0x894>
 7b0:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 7b4:	and	x19, x19, #0x8000000000000000
 7b8:	and	x22, x20, #0x7fffffffffffffff
 7bc:	orr	x19, x0, x19
 7c0:	adrp	x0, 0 <__divtc3>
 7c4:	add	x0, x0, #0x0
 7c8:	stp	x21, x22, [sp, #96]
 7cc:	ldr	q0, [sp, #96]
 7d0:	ldr	q1, [x0]
 7d4:	bl	0 <__unordtf2>
 7d8:	cbnz	w0, 89c <__divtc3+0x89c>
 7dc:	adrp	x0, 0 <__divtc3>
 7e0:	add	x0, x0, #0x0
 7e4:	stp	x21, x22, [sp, #96]
 7e8:	ldr	q1, [x0]
 7ec:	ldr	q0, [sp, #96]
 7f0:	bl	0 <__letf2>
 7f4:	cmp	w0, #0x0
 7f8:	b.le	89c <__divtc3+0x89c>
 7fc:	mov	x0, #0x3fff000000000000    	// #4611404543450677248
 800:	stp	xzr, x19, [sp, #96]
 804:	and	x20, x20, #0x8000000000000000
 808:	orr	x20, x0, x20
 80c:	ldr	q1, [sp, #96]
 810:	stp	x23, x26, [sp, #96]
 814:	ldr	q0, [sp, #96]
 818:	bl	0 <__multf3>
 81c:	stp	xzr, x20, [sp, #96]
 820:	ldr	q1, [sp, #96]
 824:	stp	x25, x24, [sp, #96]
 828:	str	q0, [sp, #112]
 82c:	ldr	q0, [sp, #96]
 830:	bl	0 <__multf3>
 834:	mov	v1.16b, v0.16b
 838:	ldr	q2, [sp, #112]
 83c:	mov	v0.16b, v2.16b
 840:	bl	0 <__addtf3>
 844:	movi	v1.2d, #0x0
 848:	bl	0 <__multf3>
 84c:	stp	xzr, x19, [sp, #96]
 850:	ldr	q1, [sp, #96]
 854:	stp	x25, x24, [sp, #96]
 858:	str	q0, [sp, #128]
 85c:	ldr	q0, [sp, #96]
 860:	bl	0 <__multf3>
 864:	stp	xzr, x20, [sp, #96]
 868:	ldr	q1, [sp, #96]
 86c:	stp	x23, x26, [sp, #96]
 870:	str	q0, [sp, #112]
 874:	ldr	q0, [sp, #96]
 878:	bl	0 <__multf3>
 87c:	mov	v1.16b, v0.16b
 880:	ldr	q4, [sp, #112]
 884:	mov	v0.16b, v4.16b
 888:	bl	0 <__subtf3>
 88c:	movi	v1.2d, #0x0
 890:	b	644 <__divtc3+0x644>
 894:	mov	x0, #0x0                   	// #0
 898:	b	7b4 <__divtc3+0x7b4>
 89c:	mov	x0, #0x0                   	// #0
 8a0:	b	800 <__divtc3+0x800>

divti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__divti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	asr	x5, x1, #63
   8:	asr	x6, x3, #63
   c:	eor	x2, x2, x3, asr #63
  10:	mov	x29, sp
  14:	subs	x2, x2, x6
  18:	eor	x0, x0, x1, asr #63
  1c:	eor	x4, x3, x3, asr #63
  20:	eor	x1, x1, x1, asr #63
  24:	str	x19, [sp, #16]
  28:	eor	x19, x5, x3, asr #63
  2c:	sbc	x3, x4, x6
  30:	subs	x0, x0, x5
  34:	mov	x4, #0x0                   	// #0
  38:	sbc	x1, x1, x5
  3c:	bl	0 <__udivmodti4>
  40:	eor	x0, x0, x19
  44:	eor	x1, x1, x19
  48:	subs	x0, x0, x19
  4c:	sbc	x1, x1, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #32
  58:	ret

divtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <wideMultiply>:
   0:	lsr	x15, x0, #32
   4:	lsr	x6, x2, #32
   8:	and	x0, x0, #0xffffffff
   c:	and	x2, x2, #0xffffffff
  10:	lsr	x7, x1, #32
  14:	and	x1, x1, #0xffffffff
  18:	lsr	x11, x3, #32
  1c:	and	x3, x3, #0xffffffff
  20:	mul	x8, x2, x15
  24:	mul	x9, x6, x0
  28:	mul	x10, x6, x15
  2c:	adds	x16, x8, x9
  30:	mul	x9, x2, x1
  34:	mul	x12, x3, x0
  38:	cset	x8, cs  // cs = hs, nlast
  3c:	adds	x9, x9, x10
  40:	mul	x14, x3, x15
  44:	cset	x10, cs  // cs = hs, nlast
  48:	adds	x9, x9, x12
  4c:	cinc	x13, x10, cs  // cs = hs, nlast
  50:	mul	x12, x11, x0
  54:	mul	x10, x7, x2
  58:	extr	x8, x8, x16, #32
  5c:	mul	x0, x2, x0
  60:	adds	x10, x10, x12
  64:	mul	x12, x6, x1
  68:	cset	x17, cs  // cs = hs, nlast
  6c:	mul	x2, x11, x15
  70:	adds	x12, x12, x14
  74:	mul	x6, x7, x6
  78:	cset	x14, cs  // cs = hs, nlast
  7c:	adds	x12, x10, x12
  80:	adc	x10, x17, x14
  84:	adds	x8, x8, x9
  88:	lsl	x9, x12, #32
  8c:	cset	x14, cs  // cs = hs, nlast
  90:	adds	x8, x8, x9
  94:	lsl	x9, x16, #32
  98:	cinc	x14, x14, cs  // cs = hs, nlast
  9c:	adds	x9, x9, x0
  a0:	cinc	x8, x8, cs  // cs = hs, nlast
  a4:	stp	x9, x8, [x5]
  a8:	mul	x5, x3, x1
  ac:	mul	x0, x7, x11
  b0:	extr	x12, x10, x12, #32
  b4:	adds	x2, x5, x2
  b8:	mul	x1, x11, x1
  bc:	cset	x5, cs  // cs = hs, nlast
  c0:	adds	x6, x6, x2
  c4:	adc	x0, x0, x5
  c8:	mul	x7, x7, x3
  cc:	adds	x10, x13, x12
  d0:	cset	x2, cs  // cs = hs, nlast
  d4:	adds	x6, x6, x10
  d8:	adc	x0, x0, x2
  dc:	adds	x7, x7, x1
  e0:	cset	x1, cs  // cs = hs, nlast
  e4:	extr	x1, x1, x7, #32
  e8:	lsl	x7, x7, #32
  ec:	adds	x7, x7, x14
  f0:	cinc	x1, x1, cs  // cs = hs, nlast
  f4:	adds	x6, x6, x7
  f8:	adc	x0, x0, x1
  fc:	stp	x6, x0, [x4]
 100:	ret

0000000000000104 <__divtf3>:
 104:	stp	x29, x30, [sp, #-240]!
 108:	mov	x29, sp
 10c:	str	q0, [sp, #96]
 110:	ldp	x0, x1, [sp, #96]
 114:	stp	x25, x26, [sp, #64]
 118:	str	q1, [sp, #96]
 11c:	ldp	x25, x3, [sp, #96]
 120:	stp	x19, x20, [sp, #16]
 124:	ubfx	x26, x1, #48, #15
 128:	stp	x21, x22, [sp, #32]
 12c:	sub	w4, w26, #0x1
 130:	mov	x19, x0
 134:	stp	x23, x24, [sp, #48]
 138:	mov	x20, #0x0                   	// #0
 13c:	eor	x2, x3, x1
 140:	stp	x27, x28, [sp, #80]
 144:	and	x21, x2, #0x8000000000000000
 148:	and	x24, x1, #0xffffffffffff
 14c:	mov	w2, #0x7ffd                	// #32765
 150:	ubfx	x27, x3, #48, #15
 154:	mov	x22, x25
 158:	and	x23, x3, #0xffffffffffff
 15c:	cmp	w4, w2
 160:	b.hi	170 <__divtf3+0x6c>  // b.pmore
 164:	sub	w4, w27, #0x1
 168:	cmp	w4, w2
 16c:	b.ls	4ac <__divtf3+0x3a8>  // b.plast
 170:	and	x4, x1, #0x7fffffffffffffff
 174:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
 178:	cmp	x4, x5
 17c:	b.hi	188 <__divtf3+0x84>  // b.pmore
 180:	b.ne	1b0 <__divtf3+0xac>  // b.any
 184:	cbz	x0, 1b0 <__divtf3+0xac>
 188:	fmov	d0, x0
 18c:	orr	x3, x1, #0x800000000000
 190:	fmov	v0.d[1], x3
 194:	ldp	x19, x20, [sp, #16]
 198:	ldp	x21, x22, [sp, #32]
 19c:	ldp	x23, x24, [sp, #48]
 1a0:	ldp	x25, x26, [sp, #64]
 1a4:	ldp	x27, x28, [sp, #80]
 1a8:	ldp	x29, x30, [sp], #240
 1ac:	ret
 1b0:	and	x2, x3, #0x7fffffffffffffff
 1b4:	cmp	x2, x5
 1b8:	b.hi	1c4 <__divtf3+0xc0>  // b.pmore
 1bc:	b.ne	1d8 <__divtf3+0xd4>  // b.any
 1c0:	cbz	x25, 1d8 <__divtf3+0xd4>
 1c4:	mov	x0, x25
 1c8:	orr	x1, x3, #0x800000000000
 1cc:	fmov	d0, x0
 1d0:	fmov	v0.d[1], x1
 1d4:	b	194 <__divtf3+0x90>
 1d8:	cbnz	x0, 200 <__divtf3+0xfc>
 1dc:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
 1e0:	cmp	x4, x5
 1e4:	b.ne	200 <__divtf3+0xfc>  // b.any
 1e8:	cbnz	x25, 1f4 <__divtf3+0xf0>
 1ec:	cmp	x2, x4
 1f0:	b.eq	22c <__divtf3+0x128>  // b.none
 1f4:	mov	x0, x20
 1f8:	orr	x1, x21, #0x7fff000000000000
 1fc:	b	1cc <__divtf3+0xc8>
 200:	cbnz	x25, 21c <__divtf3+0x118>
 204:	mov	x5, #0x7fff000000000000    	// #9223090561878065152
 208:	cmp	x2, x5
 20c:	b.ne	21c <__divtf3+0x118>  // b.any
 210:	fmov	d0, x20
 214:	fmov	v0.d[1], x21
 218:	b	194 <__divtf3+0x90>
 21c:	orr	x4, x0, x4
 220:	orr	x2, x25, x2
 224:	cbnz	x4, 23c <__divtf3+0x138>
 228:	cbnz	x2, 210 <__divtf3+0x10c>
 22c:	adrp	x0, 0 <wideMultiply>
 230:	add	x0, x0, #0x0
 234:	ldr	q0, [x0]
 238:	b	194 <__divtf3+0x90>
 23c:	cbz	x2, 1f4 <__divtf3+0xf0>
 240:	tst	x1, #0x7fff000000000000
 244:	b.ne	4a4 <__divtf3+0x3a0>  // b.any
 248:	cmp	x24, #0x0
 24c:	mov	x28, #0x40                  	// #64
 250:	csel	x18, x24, x0, ne  // ne = any
 254:	csel	x28, xzr, x28, ne  // ne = any
 258:	clz	x18, x18
 25c:	mov	x1, x24
 260:	add	w28, w28, w18
 264:	str	x3, [sp, #96]
 268:	sub	w2, w28, #0xf
 26c:	mov	x28, x2
 270:	bl	0 <__ashlti3>
 274:	ldr	x3, [sp, #96]
 278:	mov	w18, #0x1                   	// #1
 27c:	mov	x19, x0
 280:	mov	x24, x1
 284:	sub	w28, w18, w28
 288:	tst	x3, #0x7fff000000000000
 28c:	b.ne	2d0 <__divtf3+0x1cc>  // b.any
 290:	cmp	x23, #0x0
 294:	mov	x18, #0x40                  	// #64
 298:	csel	x0, x23, x25, ne  // ne = any
 29c:	csel	x18, xzr, x18, ne  // ne = any
 2a0:	clz	x0, x0
 2a4:	mov	x1, x23
 2a8:	add	w18, w18, w0
 2ac:	mov	x0, x25
 2b0:	sub	w2, w18, #0xf
 2b4:	str	w2, [sp, #96]
 2b8:	bl	0 <__ashlti3>
 2bc:	mov	x23, x1
 2c0:	ldr	w18, [sp, #96]
 2c4:	mov	x22, x0
 2c8:	add	w18, w28, w18
 2cc:	sub	w28, w18, #0x1
 2d0:	orr	x23, x23, #0x1000000000000
 2d4:	mov	x1, #0x6484                	// #25732
 2d8:	movk	x1, #0xf9de, lsl #16
 2dc:	orr	x25, x24, #0x1000000000000
 2e0:	movk	x1, #0xf333, lsl #32
 2e4:	extr	x2, x23, x22, #49
 2e8:	movk	x1, #0x7504, lsl #48
 2ec:	sub	x3, x1, x2
 2f0:	sub	w18, w26, w27
 2f4:	add	x27, sp, #0xb0
 2f8:	add	w18, w18, w28
 2fc:	mov	x4, x27
 300:	umulh	x0, x3, x2
 304:	add	x5, sp, #0x70
 308:	neg	x1, x0
 30c:	mneg	x0, x0, x3
 310:	umulh	x1, x1, x3
 314:	mov	x3, #0x0                   	// #0
 318:	extr	x1, x1, x0, #63
 31c:	umulh	x24, x1, x2
 320:	neg	x0, x24
 324:	mneg	x24, x24, x1
 328:	umulh	x0, x0, x1
 32c:	extr	x1, x0, x24, #63
 330:	umulh	x24, x2, x1
 334:	neg	x0, x24
 338:	mneg	x24, x24, x1
 33c:	umulh	x0, x0, x1
 340:	extr	x0, x0, x24, #63
 344:	umulh	x1, x0, x2
 348:	neg	x24, x1
 34c:	mneg	x1, x1, x0
 350:	umulh	x24, x24, x0
 354:	extr	x0, x24, x1, #63
 358:	umulh	x1, x0, x2
 35c:	neg	x24, x1
 360:	mneg	x1, x1, x0
 364:	umulh	x24, x24, x0
 368:	extr	x24, x24, x1, #63
 36c:	sub	x24, x24, #0x1
 370:	mov	x1, #0x0                   	// #0
 374:	mov	x0, x24
 378:	bl	0 <wideMultiply>
 37c:	add	x5, sp, #0x80
 380:	lsl	x2, x22, #15
 384:	mov	x0, x24
 388:	mov	x3, #0x0                   	// #0
 38c:	mov	x1, #0x0                   	// #0
 390:	bl	0 <wideMultiply>
 394:	ldp	x1, x2, [sp, #112]
 398:	add	x5, sp, #0x90
 39c:	ldr	x0, [sp, #136]
 3a0:	mov	x3, #0x0                   	// #0
 3a4:	adds	x0, x0, x1
 3a8:	mov	x1, #0x0                   	// #0
 3ac:	cinc	x2, x2, cs  // cs = hs, nlast
 3b0:	negs	x26, x0
 3b4:	ngc	x2, x2
 3b8:	mov	x0, x24
 3bc:	bl	0 <wideMultiply>
 3c0:	add	x5, sp, #0xa0
 3c4:	mov	x2, x26
 3c8:	mov	x0, x24
 3cc:	mov	x3, #0x0                   	// #0
 3d0:	mov	x1, #0x0                   	// #0
 3d4:	bl	0 <wideMultiply>
 3d8:	ldp	x0, x3, [sp, #144]
 3dc:	add	x5, sp, #0xd0
 3e0:	ldr	x2, [sp, #168]
 3e4:	extr	x1, x25, x19, #62
 3e8:	add	x4, sp, #0xc0
 3ec:	subs	x0, x0, #0x2
 3f0:	sbc	x3, x3, xzr
 3f4:	adds	x2, x2, x0
 3f8:	cinc	x3, x3, cs  // cs = hs, nlast
 3fc:	lsl	x0, x19, #2
 400:	bl	0 <wideMultiply>
 404:	ldp	x0, x1, [sp, #192]
 408:	mov	x2, #0x1ffffffffffff       	// #562949953421311
 40c:	add	x5, sp, #0xe0
 410:	cmp	x1, x2
 414:	b.hi	4b4 <__divtf3+0x3b0>  // b.pmore
 418:	mov	x2, x22
 41c:	mov	x4, x27
 420:	mov	x3, x23
 424:	bl	0 <wideMultiply>
 428:	ldp	x2, x0, [sp, #224]
 42c:	lsl	x19, x19, #49
 430:	sub	w18, w18, #0x1
 434:	negs	x2, x2
 438:	sbc	x19, x19, x0
 43c:	mov	w0, #0x3fff                	// #16383
 440:	add	w18, w18, w0
 444:	mov	w0, #0x7ffe                	// #32766
 448:	cmp	w18, w0
 44c:	b.gt	1f4 <__divtf3+0xf0>
 450:	cmp	w18, #0x0
 454:	b.gt	4e4 <__divtf3+0x3e0>
 458:	b.ne	210 <__divtf3+0x10c>  // b.any
 45c:	ldp	x3, x0, [sp, #192]
 460:	extr	x19, x19, x2, #63
 464:	cmp	x19, x23
 468:	lsl	x2, x2, #1
 46c:	mov	x1, #0x1                   	// #1
 470:	and	x0, x0, #0xffffffffffff
 474:	b.hi	488 <__divtf3+0x384>  // b.pmore
 478:	b.ne	484 <__divtf3+0x380>  // b.any
 47c:	cmp	x2, x22
 480:	b.hi	488 <__divtf3+0x384>  // b.pmore
 484:	mov	x1, #0x0                   	// #0
 488:	adds	x1, x1, x3
 48c:	cinc	x0, x0, cs  // cs = hs, nlast
 490:	tst	x0, #0x1000000000000
 494:	b.eq	210 <__divtf3+0x10c>  // b.none
 498:	mov	x0, x20
 49c:	orr	x1, x21, #0x1000000000000
 4a0:	b	1cc <__divtf3+0xc8>
 4a4:	mov	w28, #0x0                   	// #0
 4a8:	b	288 <__divtf3+0x184>
 4ac:	mov	w28, #0x0                   	// #0
 4b0:	b	2d0 <__divtf3+0x1cc>
 4b4:	mov	x2, x22
 4b8:	extr	x0, x1, x0, #1
 4bc:	mov	x4, x27
 4c0:	lsr	x1, x1, #1
 4c4:	mov	x3, x23
 4c8:	stp	x0, x1, [sp, #192]
 4cc:	bl	0 <wideMultiply>
 4d0:	ldp	x2, x0, [sp, #224]
 4d4:	lsl	x19, x19, #48
 4d8:	negs	x2, x2
 4dc:	sbc	x19, x19, x0
 4e0:	b	43c <__divtf3+0x338>
 4e4:	ldp	x3, x1, [sp, #192]
 4e8:	extr	x19, x19, x2, #63
 4ec:	cmp	x23, x19
 4f0:	lsl	x2, x2, #1
 4f4:	mov	x0, #0x1                   	// #1
 4f8:	and	x1, x1, #0xffffffffffff
 4fc:	orr	x18, x1, x18, lsl #48
 500:	b.hi	510 <__divtf3+0x40c>  // b.pmore
 504:	b.ne	514 <__divtf3+0x410>  // b.any
 508:	cmp	x22, x2
 50c:	b.ls	514 <__divtf3+0x410>  // b.plast
 510:	mov	x0, #0x0                   	// #0
 514:	adds	x2, x0, x3
 518:	cinc	x18, x18, cs  // cs = hs, nlast
 51c:	orr	x0, x2, x20
 520:	orr	x1, x18, x21
 524:	b	1cc <__divtf3+0xc8>

extendsfdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendsfdf2>:
   0:	fmov	w0, s0
   4:	and	w1, w0, #0x7fffffff
   8:	and	w2, w0, #0x80000000
   c:	sub	w3, w1, #0x800, lsl #12
  10:	mov	w0, #0x7effffff            	// #2130706431
  14:	cmp	w3, w0
  18:	b.hi	38 <__extendsfdf2+0x38>  // b.pmore
  1c:	ubfiz	x1, x1, #29, #31
  20:	mov	x0, #0x3800000000000000    	// #4035225266123964416
  24:	add	x0, x1, x0
  28:	mov	w1, w2
  2c:	orr	x0, x0, x1, lsl #32
  30:	fmov	d0, x0
  34:	ret
  38:	mov	w0, #0x7f7fffff            	// #2139095039
  3c:	cmp	w1, w0
  40:	b.ls	50 <__extendsfdf2+0x50>  // b.plast
  44:	ubfiz	x0, x1, #29, #23
  48:	orr	x0, x0, #0x7ff0000000000000
  4c:	b	28 <__extendsfdf2+0x28>
  50:	clz	w4, w1
  54:	mov	w3, w1
  58:	add	w0, w4, #0x15
  5c:	cmp	w1, #0x0
  60:	lsl	x0, x3, x0
  64:	eor	x3, x0, #0x10000000000000
  68:	mov	w0, #0x389                 	// #905
  6c:	sub	w0, w0, w4
  70:	orr	x0, x3, x0, lsl #52
  74:	csel	x0, x0, xzr, ne  // ne = any
  78:	b	28 <__extendsfdf2+0x28>

extendhfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__extendhfsf2>:
   0:	and	w2, w0, #0x7fff
   4:	mov	w1, #0x77ff                	// #30719
   8:	sub	w3, w2, #0x400
   c:	and	w0, w0, #0x8000
  10:	cmp	w1, w3, uxth
  14:	b.cc	2c <__extendhfsf2+0x2c>  // b.lo, b.ul, b.last
  18:	mov	w1, #0x38000000            	// #939524096
  1c:	add	w1, w1, w2, lsl #13
  20:	orr	w1, w1, w0, lsl #16
  24:	fmov	s0, w1
  28:	ret
  2c:	mov	w1, #0x7bff                	// #31743
  30:	cmp	w2, w1
  34:	b.ls	44 <__extendhfsf2+0x44>  // b.plast
  38:	ubfiz	w1, w2, #13, #10
  3c:	orr	w1, w1, #0x7f800000
  40:	b	20 <__extendhfsf2+0x20>
  44:	clz	w4, w2
  48:	mov	w1, #0x86                  	// #134
  4c:	sub	w3, w4, #0x8
  50:	sub	w1, w1, w4
  54:	cmp	w2, #0x0
  58:	lsl	w3, w2, w3
  5c:	eor	w3, w3, #0x800000
  60:	orr	w1, w3, w1, lsl #23
  64:	csel	w1, w1, wzr, ne  // ne = any
  68:	b	20 <__extendhfsf2+0x20>

000000000000006c <__gnu_h2f_ieee>:
  6c:	b	0 <__extendhfsf2>

ffsdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsdi2>:
   0:	cbnz	w0, 20 <__ffsdi2+0x20>
   4:	asr	x0, x0, #32
   8:	rbit	w1, w0
   c:	cmp	w0, #0x0
  10:	clz	w1, w1
  14:	add	w1, w1, #0x21
  18:	csel	w0, w0, w1, eq  // eq = none
  1c:	ret
  20:	rbit	w0, w0
  24:	clz	w0, w0
  28:	add	w0, w0, #0x1
  2c:	b	1c <__ffsdi2+0x1c>

ffssi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffssi2>:
   0:	cbz	w0, 10 <__ffssi2+0x10>
   4:	rbit	w0, w0
   8:	clz	w0, w0
   c:	add	w0, w0, #0x1
  10:	ret

ffsti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ffsti2>:
   0:	cbnz	x0, 1c <__ffsti2+0x1c>
   4:	rbit	x0, x1
   8:	cmp	x1, #0x0
   c:	clz	x0, x0
  10:	add	w0, w0, #0x41
  14:	csel	w0, w0, wzr, ne  // ne = any
  18:	ret
  1c:	rbit	x0, x0
  20:	clz	x0, x0
  24:	add	w0, w0, #0x1
  28:	b	18 <__ffsti2+0x18>

fixdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfdi>:
   0:	fcmpe	d0, #0.0
   4:	b.pl	24 <__fixdfdi+0x24>  // b.nfrst
   8:	fneg	d0, d0
   c:	stp	x29, x30, [sp, #-16]!
  10:	mov	x29, sp
  14:	bl	0 <__fixunsdfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunsdfdi>

fixdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfsi>:
   0:	fmov	x1, d0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	x1, #0x0
   c:	cneg	w3, w0, lt  // lt = tstop
  10:	ubfx	x0, x1, #52, #11
  14:	subs	w4, w0, #0x3ff
  18:	b.mi	64 <__fixdfsi+0x64>  // b.first
  1c:	cmp	w4, #0x1f
  20:	b.ls	34 <__fixdfsi+0x34>  // b.plast
  24:	cmp	w3, #0x1
  28:	mov	w0, #0x80000000            	// #-2147483648
  2c:	cinv	w0, w0, eq  // eq = none
  30:	ret
  34:	and	x1, x1, #0xfffffffffffff
  38:	sub	w0, w0, #0x433
  3c:	orr	x1, x1, #0x10000000000000
  40:	mov	w2, #0x34                  	// #52
  44:	sub	w2, w2, w4
  48:	cmp	w4, #0x33
  4c:	lsl	w0, w1, w0
  50:	lsr	x2, x1, x2
  54:	mul	w0, w0, w3
  58:	mul	w2, w3, w2
  5c:	csel	w0, w0, w2, gt
  60:	b	30 <__fixdfsi+0x30>
  64:	mov	w0, #0x0                   	// #0
  68:	b	30 <__fixdfsi+0x30>

fixdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixdfti>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	fmov	x0, d0
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	tbnz	x0, #63, 54 <__fixdfti+0x54>
  14:	mov	x19, #0x1                   	// #1
  18:	mov	x20, #0x0                   	// #0
  1c:	ubfx	x2, x0, #52, #11
  20:	subs	w3, w2, #0x3ff
  24:	b.mi	b0 <__fixdfti+0xb0>  // b.first
  28:	cmp	w3, #0x7f
  2c:	b.ls	60 <__fixdfti+0x60>  // b.plast
  30:	cmp	x19, #0x1
  34:	b.ne	3c <__fixdfti+0x3c>  // b.any
  38:	cbz	x20, bc <__fixdfti+0xbc>
  3c:	adrp	x1, 10 <__fixdfti+0x10>
  40:	mov	x0, #0x0                   	// #0
  44:	ldr	x1, [x1]
  48:	ldp	x19, x20, [sp, #16]
  4c:	ldp	x29, x30, [sp], #32
  50:	ret
  54:	mov	x19, #0xffffffffffffffff    	// #-1
  58:	mov	x20, x19
  5c:	b	1c <__fixdfti+0x1c>
  60:	and	x0, x0, #0xfffffffffffff
  64:	cmp	w3, #0x33
  68:	orr	x0, x0, #0x10000000000000
  6c:	b.gt	8c <__fixdfti+0x8c>
  70:	mov	w1, #0x34                  	// #52
  74:	sub	w1, w1, w3
  78:	lsr	x1, x0, x1
  7c:	umulh	x2, x1, x19
  80:	mul	x0, x1, x19
  84:	madd	x1, x1, x20, x2
  88:	b	48 <__fixdfti+0x48>
  8c:	sub	w2, w2, #0x433
  90:	mov	x1, #0x0                   	// #0
  94:	bl	0 <__ashlti3>
  98:	mov	x2, x0
  9c:	umulh	x3, x0, x19
  a0:	madd	x3, x1, x19, x3
  a4:	mul	x0, x0, x19
  a8:	madd	x1, x2, x20, x3
  ac:	b	48 <__fixdfti+0x48>
  b0:	mov	x0, #0x0                   	// #0
  b4:	mov	x1, #0x0                   	// #0
  b8:	b	48 <__fixdfti+0x48>
  bc:	adrp	x1, 0 <__fixdfti>
  c0:	mov	x0, #0xffffffffffffffff    	// #-1
  c4:	ldr	x1, [x1]
  c8:	b	48 <__fixdfti+0x48>

fixsfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfdi>:
   0:	fcmpe	s0, #0.0
   4:	b.pl	24 <__fixsfdi+0x24>  // b.nfrst
   8:	fneg	s0, s0
   c:	stp	x29, x30, [sp, #-16]!
  10:	mov	x29, sp
  14:	bl	0 <__fixunssfdi>
  18:	neg	x0, x0
  1c:	ldp	x29, x30, [sp], #16
  20:	ret
  24:	b	0 <__fixunssfdi>

fixsfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfsi>:
   0:	fmov	w1, s0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	w1, #0x0
   c:	cneg	w3, w0, lt  // lt = tstop
  10:	ubfx	x0, x1, #23, #8
  14:	subs	w4, w0, #0x7f
  18:	b.mi	64 <__fixsfsi+0x64>  // b.first
  1c:	cmp	w4, #0x1f
  20:	b.ls	34 <__fixsfsi+0x34>  // b.plast
  24:	cmp	w3, #0x1
  28:	mov	w0, #0x80000000            	// #-2147483648
  2c:	cinv	w0, w0, eq  // eq = none
  30:	ret
  34:	and	w1, w1, #0x7fffff
  38:	sub	w0, w0, #0x96
  3c:	orr	w1, w1, #0x800000
  40:	mov	w2, #0x17                  	// #23
  44:	sub	w2, w2, w4
  48:	cmp	w4, #0x16
  4c:	lsl	w0, w1, w0
  50:	lsr	w2, w1, w2
  54:	mul	w0, w0, w3
  58:	mul	w2, w2, w3
  5c:	csel	w0, w0, w2, gt
  60:	b	30 <__fixsfsi+0x30>
  64:	mov	w0, #0x0                   	// #0
  68:	b	30 <__fixsfsi+0x30>

fixsfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixsfti>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	fmov	w0, s0
   8:	mov	x29, sp
   c:	stp	x19, x20, [sp, #16]
  10:	tbnz	w0, #31, 54 <__fixsfti+0x54>
  14:	mov	x19, #0x1                   	// #1
  18:	mov	x20, #0x0                   	// #0
  1c:	ubfx	x2, x0, #23, #8
  20:	subs	w3, w2, #0x7f
  24:	b.mi	b4 <__fixsfti+0xb4>  // b.first
  28:	cmp	w3, #0x80
  2c:	b.ne	60 <__fixsfti+0x60>  // b.any
  30:	cmp	x19, #0x1
  34:	b.ne	3c <__fixsfti+0x3c>  // b.any
  38:	cbz	x20, c0 <__fixsfti+0xc0>
  3c:	adrp	x1, 10 <__fixsfti+0x10>
  40:	mov	x0, #0x0                   	// #0
  44:	ldr	x1, [x1]
  48:	ldp	x19, x20, [sp, #16]
  4c:	ldp	x29, x30, [sp], #32
  50:	ret
  54:	mov	x19, #0xffffffffffffffff    	// #-1
  58:	mov	x20, x19
  5c:	b	1c <__fixsfti+0x1c>
  60:	and	w0, w0, #0x7fffff
  64:	cmp	w3, #0x16
  68:	orr	w0, w0, #0x800000
  6c:	b.gt	8c <__fixsfti+0x8c>
  70:	mov	w1, #0x17                  	// #23
  74:	sub	w1, w1, w3
  78:	lsr	w1, w0, w1
  7c:	umulh	x2, x1, x19
  80:	mul	x0, x1, x19
  84:	madd	x1, x1, x20, x2
  88:	b	48 <__fixsfti+0x48>
  8c:	sub	w2, w2, #0x96
  90:	and	x0, x0, #0xffffff
  94:	mov	x1, #0x0                   	// #0
  98:	bl	0 <__ashlti3>
  9c:	mov	x2, x0
  a0:	umulh	x3, x0, x19
  a4:	madd	x3, x1, x19, x3
  a8:	mul	x0, x0, x19
  ac:	madd	x1, x2, x20, x3
  b0:	b	48 <__fixsfti+0x48>
  b4:	mov	x0, #0x0                   	// #0
  b8:	mov	x1, #0x0                   	// #0
  bc:	b	48 <__fixsfti+0x48>
  c0:	adrp	x1, 0 <__fixsfti>
  c4:	mov	x0, #0xffffffffffffffff    	// #-1
  c8:	ldr	x1, [x1]
  cc:	b	48 <__fixsfti+0x48>

fixunsdfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfdi>:
   0:	fcmpe	d0, #0.0
   4:	b.ls	38 <__fixunsdfdi+0x38>  // b.plast
   8:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
   c:	fmov	d1, x0
  10:	mov	x1, #0x41f0000000000000    	// #4751297606875873280
  14:	fmov	d2, x1
  18:	fmul	d1, d0, d1
  1c:	fcvtzu	w0, d1
  20:	ucvtf	d1, w0
  24:	fmul	d1, d1, d2
  28:	fsub	d0, d0, d1
  2c:	fcvtzu	w1, d0
  30:	orr	x0, x1, x0, lsl #32
  34:	ret
  38:	mov	x0, #0x0                   	// #0
  3c:	b	34 <__fixunsdfdi+0x34>

fixunsdfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfsi>:
   0:	fmov	x1, d0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	x1, #0x0
   c:	ubfx	x2, x1, #52, #11
  10:	cneg	w0, w0, lt  // lt = tstop
  14:	subs	w3, w2, #0x3ff
  18:	ccmn	w0, #0x1, #0x4, pl  // pl = nfrst
  1c:	b.eq	50 <__fixunsdfsi+0x50>  // b.none
  20:	cmp	w3, #0x1f
  24:	b.hi	58 <__fixunsdfsi+0x58>  // b.pmore
  28:	and	x1, x1, #0xfffffffffffff
  2c:	sub	w2, w2, #0x433
  30:	orr	x1, x1, #0x10000000000000
  34:	mov	w0, #0x34                  	// #52
  38:	sub	w0, w0, w3
  3c:	cmp	w3, #0x33
  40:	lsr	x0, x1, x0
  44:	lsl	w1, w1, w2
  48:	csel	w0, w1, w0, gt
  4c:	ret
  50:	mov	w0, #0x0                   	// #0
  54:	b	4c <__fixunsdfsi+0x4c>
  58:	mov	w0, #0xffffffff            	// #-1
  5c:	b	4c <__fixunsdfsi+0x4c>

fixunsdfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunsdfti>:
   0:	fmov	x0, d0
   4:	mov	w1, #0x1                   	// #1
   8:	cmp	x0, #0x0
   c:	ubfx	x2, x0, #52, #11
  10:	cneg	w1, w1, lt  // lt = tstop
  14:	subs	w3, w2, #0x3ff
  18:	ccmn	w1, #0x1, #0x4, pl  // pl = nfrst
  1c:	b.eq	68 <__fixunsdfti+0x68>  // b.none
  20:	cmp	w3, #0x7f
  24:	b.hi	70 <__fixunsdfti+0x70>  // b.pmore
  28:	and	x0, x0, #0xfffffffffffff
  2c:	cmp	w3, #0x33
  30:	orr	x0, x0, #0x10000000000000
  34:	b.gt	4c <__fixunsdfti+0x4c>
  38:	mov	w1, #0x34                  	// #52
  3c:	sub	w1, w1, w3
  40:	lsr	x0, x0, x1
  44:	mov	x1, #0x0                   	// #0
  48:	ret
  4c:	stp	x29, x30, [sp, #-16]!
  50:	sub	w2, w2, #0x433
  54:	mov	x1, #0x0                   	// #0
  58:	mov	x29, sp
  5c:	bl	0 <__ashlti3>
  60:	ldp	x29, x30, [sp], #16
  64:	ret
  68:	mov	x0, #0x0                   	// #0
  6c:	b	44 <__fixunsdfti+0x44>
  70:	mov	x0, #0xffffffffffffffff    	// #-1
  74:	mov	x1, x0
  78:	ret

fixunssfdi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfdi>:
   0:	fcmpe	s0, #0.0
   4:	b.ls	3c <__fixunssfdi+0x3c>  // b.plast
   8:	fcvt	d0, s0
   c:	mov	x0, #0x3df0000000000000    	// #4463067230724161536
  10:	fmov	d1, x0
  14:	mov	x0, #0x41f0000000000000    	// #4751297606875873280
  18:	fmov	d2, x0
  1c:	fmul	d1, d0, d1
  20:	fcvtzu	w1, d1
  24:	ucvtf	d1, w1
  28:	fmul	d1, d1, d2
  2c:	fsub	d0, d0, d1
  30:	fcvtzu	w0, d0
  34:	orr	x0, x0, x1, lsl #32
  38:	ret
  3c:	mov	x0, #0x0                   	// #0
  40:	b	38 <__fixunssfdi+0x38>

fixunssfsi.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfsi>:
   0:	fmov	w1, s0
   4:	mov	w0, #0x1                   	// #1
   8:	cmp	w1, #0x0
   c:	ubfx	x2, x1, #23, #8
  10:	cneg	w0, w0, lt  // lt = tstop
  14:	subs	w3, w2, #0x7f
  18:	ccmn	w0, #0x1, #0x4, pl  // pl = nfrst
  1c:	b.eq	50 <__fixunssfsi+0x50>  // b.none
  20:	cmp	w3, #0x1f
  24:	b.hi	58 <__fixunssfsi+0x58>  // b.pmore
  28:	and	w1, w1, #0x7fffff
  2c:	sub	w2, w2, #0x96
  30:	orr	w1, w1, #0x800000
  34:	mov	w0, #0x17                  	// #23
  38:	sub	w0, w0, w3
  3c:	cmp	w3, #0x16
  40:	lsr	w0, w1, w0
  44:	lsl	w1, w1, w2
  48:	csel	w0, w1, w0, gt
  4c:	ret
  50:	mov	w0, #0x0                   	// #0
  54:	b	4c <__fixunssfsi+0x4c>
  58:	mov	w0, #0xffffffff            	// #-1
  5c:	b	4c <__fixunssfsi+0x4c>

fixunssfti.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fixunssfti>:
   0:	fmov	w0, s0
   4:	mov	w1, #0x1                   	// #1
   8:	cmp	w0, #0x0
   c:	ubfx	x2, x0, #23, #8
  10:	cneg	w1, w1, lt  // lt = tstop
  14:	subs	w3, w2, #0x7f
  18:	ccmn	w1, #0x1, #0x4, pl  // pl = nfrst
  1c:	b.eq	6c <__fixunssfti+0x6c>  // b.none
  20:	cmp	w3, #0x80
  24:	b.eq	74 <__fixunssfti+0x74>  // b.none
  28:	and	w0, w0, #0x7fffff
  2c:	cmp	w3, #0x16
  30:	orr	w0, w0, #0x800000
  34:	b.gt	4c <__fixunssfti+0x4c>
  38:	mov	w1, #0x17                  	// #23
  3c:	sub	w1, w1, w3
  40:	lsr	w0, w0, w1
  44:	mov	x1, #0x0                   	// #0
  48:	ret
  4c:	stp	x29, x30, [sp, #-16]!
  50:	sub	w2, w2, #0x96
  54:	and	x0, x0, #0xffffff
  58:	mov	x29, sp
  5c:	mov	x1, #0x0                   	// #0
  60:	bl	0 <__ashlti3>
  64:	ldp	x29, x30, [sp], #16
  68:	ret
  6c:	mov	x0, #0x0                   	// #0
  70:	b	44 <__fixunssfti+0x44>
  74:	mov	x0, #0xffffffffffffffff    	// #-1
  78:	mov	x1, x0
  7c:	ret

floatdidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdidf>:
   0:	asr	x1, x0, #32
   4:	and	x0, x0, #0xffffffff
   8:	scvtf	d0, w1
   c:	mov	x1, #0x41f0000000000000    	// #4751297606875873280
  10:	fmov	d1, x1
  14:	mov	x1, #0x4330000000000000    	// #4841369599423283200
  18:	orr	x0, x0, x1
  1c:	fmul	d0, d0, d1
  20:	fmov	d1, x1
  24:	fsub	d0, d0, d1
  28:	fmov	d1, x0
  2c:	fadd	d0, d0, d1
  30:	ret

floatdisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatdisf>:
   0:	cbz	x0, a8 <__floatdisf+0xa8>
   4:	eor	x1, x0, x0, asr #63
   8:	asr	x4, x0, #63
   c:	sub	x0, x1, x0, asr #63
  10:	mov	w1, #0x40                  	// #64
  14:	clz	x3, x0
  18:	sub	w1, w1, w3
  1c:	sub	w2, w1, #0x1
  20:	cmp	w1, #0x18
  24:	b.ls	98 <__floatdisf+0x98>  // b.plast
  28:	cmp	w1, #0x19
  2c:	b.eq	5c <__floatdisf+0x5c>  // b.none
  30:	cmp	w1, #0x1a
  34:	b.eq	60 <__floatdisf+0x60>  // b.none
  38:	add	w3, w3, #0x1a
  3c:	mov	x5, #0xffffffffffffffff    	// #-1
  40:	lsr	x3, x5, x3
  44:	sub	w5, w1, #0x1a
  48:	tst	x3, x0
  4c:	cset	x3, ne  // ne = any
  50:	lsr	x0, x0, x5
  54:	orr	x0, x3, x0
  58:	b	60 <__floatdisf+0x60>
  5c:	lsl	x0, x0, #1
  60:	ubfx	x3, x0, #2, #1
  64:	orr	x0, x3, x0
  68:	add	x3, x0, #0x1
  6c:	asr	x0, x3, #2
  70:	tbz	w3, #26, 7c <__floatdisf+0x7c>
  74:	asr	x0, x3, #3
  78:	mov	w2, w1
  7c:	and	w1, w4, #0x80000000
  80:	and	w0, w0, #0x7fffff
  84:	add	w2, w2, #0x7f
  88:	orr	w0, w1, w0
  8c:	orr	w0, w0, w2, lsl #23
  90:	fmov	s0, w0
  94:	ret
  98:	mov	w3, #0x18                  	// #24
  9c:	sub	w1, w3, w1
  a0:	lsl	x0, x0, x1
  a4:	b	7c <__floatdisf+0x7c>
  a8:	movi	v0.2s, #0x0
  ac:	b	94 <__floatdisf+0x94>

floatsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsidf>:
   0:	cmp	w0, #0x0
   4:	cbz	w0, 48 <__floatsidf+0x48>
   8:	b.ge	40 <__floatsidf+0x40>  // b.tcont
   c:	neg	w0, w0
  10:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
  14:	clz	w3, w0
  18:	sxtw	x0, w0
  1c:	add	w1, w3, #0x15
  20:	lsl	x0, x0, x1
  24:	mov	w1, #0x41e                 	// #1054
  28:	sub	w1, w1, w3
  2c:	eor	x0, x0, #0x10000000000000
  30:	add	x0, x0, x1, lsl #52
  34:	orr	x0, x0, x2
  38:	fmov	d0, x0
  3c:	ret
  40:	mov	x2, #0x0                   	// #0
  44:	b	14 <__floatsidf+0x14>
  48:	movi	d0, #0x0
  4c:	b	3c <__floatsidf+0x3c>

floatsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatsisf>:
   0:	cmp	w0, #0x0
   4:	cbz	w0, 8c <__floatsisf+0x8c>
   8:	b.ge	4c <__floatsisf+0x4c>  // b.tcont
   c:	neg	w0, w0
  10:	mov	w4, #0x80000000            	// #-2147483648
  14:	clz	w3, w0
  18:	mov	w1, #0x1f                  	// #31
  1c:	sub	w1, w1, w3
  20:	cmp	w1, #0x17
  24:	b.gt	54 <__floatsisf+0x54>
  28:	sub	w1, w3, #0x8
  2c:	lsl	w0, w0, w1
  30:	eor	w1, w0, #0x800000
  34:	mov	w0, #0x9e                  	// #158
  38:	sub	w0, w0, w3
  3c:	add	w0, w1, w0, lsl #23
  40:	orr	w0, w0, w4
  44:	fmov	s0, w0
  48:	ret
  4c:	mov	w4, #0x0                   	// #0
  50:	b	14 <__floatsisf+0x14>
  54:	mov	w2, #0x8                   	// #8
  58:	sub	w2, w2, w3
  5c:	lsr	w1, w0, w2
  60:	neg	w2, w2
  64:	eor	w1, w1, #0x800000
  68:	add	w5, w1, #0x1
  6c:	mov	w6, w1
  70:	lsl	w0, w0, w2
  74:	and	w1, w5, #0xfffffffe
  78:	mov	w2, #0x80000000            	// #-2147483648
  7c:	cmp	w0, w2
  80:	csel	w1, w1, w6, eq  // eq = none
  84:	csel	w1, w1, w5, ls  // ls = plast
  88:	b	34 <__floatsisf+0x34>
  8c:	movi	v0.2s, #0x0
  90:	b	48 <__floatsisf+0x48>

floattidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattidf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 120 <__floattidf+0x120>
   8:	stp	x29, x30, [sp, #-80]!
   c:	eor	x0, x0, x1, asr #63
  10:	mov	x29, sp
  14:	stp	x21, x22, [sp, #32]
  18:	asr	x21, x1, #63
  1c:	eor	x1, x1, x1, asr #63
  20:	stp	x23, x24, [sp, #48]
  24:	subs	x24, x0, x21
  28:	mov	x0, x24
  2c:	stp	x19, x20, [sp, #16]
  30:	sbc	x20, x1, x21
  34:	mov	x1, x20
  38:	str	x25, [sp, #64]
  3c:	bl	0 <__clzti2>
  40:	mov	w25, #0x80                  	// #128
  44:	sub	w25, w25, w0
  48:	mov	x23, x24
  4c:	mov	x19, x20
  50:	sub	w22, w25, #0x1
  54:	cmp	w25, #0x35
  58:	b.le	108 <__floattidf+0x108>
  5c:	cmp	w25, #0x36
  60:	b.eq	ac <__floattidf+0xac>  // b.none
  64:	cmp	w25, #0x37
  68:	b.eq	b4 <__floattidf+0xb4>  // b.none
  6c:	add	w2, w0, #0x37
  70:	mov	x1, #0xffffffffffffffff    	// #-1
  74:	mov	x0, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x20
  80:	and	x0, x0, x24
  84:	orr	x0, x0, x1
  88:	sub	w2, w25, #0x37
  8c:	cmp	x0, #0x0
  90:	mov	x1, x19
  94:	cset	x20, ne  // ne = any
  98:	mov	x0, x24
  9c:	bl	0 <__lshrti3>
  a0:	orr	x23, x20, x0
  a4:	mov	x19, x1
  a8:	b	b4 <__floattidf+0xb4>
  ac:	lsl	x23, x24, #1
  b0:	extr	x19, x20, x24, #63
  b4:	ubfx	x0, x23, #2, #1
  b8:	orr	x23, x0, x23
  bc:	adds	x23, x23, #0x1
  c0:	cinc	x19, x19, cs  // cs = hs, nlast
  c4:	extr	x0, x19, x23, #2
  c8:	tbz	x23, #55, d4 <__floattidf+0xd4>
  cc:	mov	w22, w25
  d0:	extr	x0, x19, x23, #3
  d4:	add	w22, w22, #0x3ff
  d8:	ubfx	x1, x0, #32, #20
  dc:	and	w21, w21, #0x80000000
  e0:	orr	w22, w1, w22, lsl #20
  e4:	orr	w21, w22, w21
  e8:	ldp	x19, x20, [sp, #16]
  ec:	bfi	x0, x21, #32, #32
  f0:	fmov	d0, x0
  f4:	ldp	x21, x22, [sp, #32]
  f8:	ldp	x23, x24, [sp, #48]
  fc:	ldr	x25, [sp, #64]
 100:	ldp	x29, x30, [sp], #80
 104:	ret
 108:	mov	x0, x24
 10c:	mov	x1, x20
 110:	mov	w2, #0x35                  	// #53
 114:	sub	w2, w2, w25
 118:	bl	0 <__ashlti3>
 11c:	b	d4 <__floattidf+0xd4>
 120:	movi	d0, #0x0
 124:	ret

floattisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floattisf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 11c <__floattisf+0x11c>
   8:	stp	x29, x30, [sp, #-80]!
   c:	eor	x0, x0, x1, asr #63
  10:	mov	x29, sp
  14:	stp	x21, x22, [sp, #32]
  18:	asr	x22, x1, #63
  1c:	eor	x1, x1, x1, asr #63
  20:	stp	x23, x24, [sp, #48]
  24:	subs	x23, x0, x22
  28:	mov	x0, x23
  2c:	stp	x19, x20, [sp, #16]
  30:	sbc	x20, x1, x22
  34:	mov	x1, x20
  38:	str	x25, [sp, #64]
  3c:	bl	0 <__clzti2>
  40:	mov	w25, #0x80                  	// #128
  44:	sub	w25, w25, w0
  48:	mov	x21, x23
  4c:	mov	x19, x20
  50:	sub	w24, w25, #0x1
  54:	cmp	w25, #0x18
  58:	b.le	104 <__floattisf+0x104>
  5c:	cmp	w25, #0x19
  60:	b.eq	ac <__floattisf+0xac>  // b.none
  64:	cmp	w25, #0x1a
  68:	b.eq	b4 <__floattisf+0xb4>  // b.none
  6c:	add	w2, w0, #0x1a
  70:	mov	x1, #0xffffffffffffffff    	// #-1
  74:	mov	x0, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x20
  80:	and	x0, x0, x23
  84:	orr	x0, x0, x1
  88:	sub	w2, w25, #0x1a
  8c:	cmp	x0, #0x0
  90:	mov	x1, x19
  94:	cset	x20, ne  // ne = any
  98:	mov	x0, x23
  9c:	bl	0 <__lshrti3>
  a0:	orr	x21, x20, x0
  a4:	mov	x19, x1
  a8:	b	b4 <__floattisf+0xb4>
  ac:	lsl	x21, x23, #1
  b0:	extr	x19, x20, x23, #63
  b4:	ubfx	x0, x21, #2, #1
  b8:	orr	x21, x0, x21
  bc:	adds	x21, x21, #0x1
  c0:	cinc	x19, x19, cs  // cs = hs, nlast
  c4:	extr	x0, x19, x21, #2
  c8:	tbz	w21, #26, d4 <__floattisf+0xd4>
  cc:	mov	w24, w25
  d0:	extr	x0, x19, x21, #3
  d4:	and	w0, w0, #0x7fffff
  d8:	add	w24, w24, #0x7f
  dc:	and	w22, w22, #0x80000000
  e0:	orr	w22, w22, w0
  e4:	orr	w0, w22, w24, lsl #23
  e8:	fmov	s0, w0
  ec:	ldp	x19, x20, [sp, #16]
  f0:	ldp	x21, x22, [sp, #32]
  f4:	ldp	x23, x24, [sp, #48]
  f8:	ldr	x25, [sp, #64]
  fc:	ldp	x29, x30, [sp], #80
 100:	ret
 104:	mov	x0, x23
 108:	mov	x1, x20
 10c:	mov	w2, #0x18                  	// #24
 110:	sub	w2, w2, w25
 114:	bl	0 <__ashlti3>
 118:	b	d4 <__floattisf+0xd4>
 11c:	movi	v0.2s, #0x0
 120:	ret

floatundidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundidf>:
   0:	mov	x1, #0x4530000000000000    	// #4985484787499139072
   4:	mov	x2, #0x100000              	// #1048576
   8:	orr	x1, x1, x0, lsr #32
   c:	movk	x2, #0x4530, lsl #48
  10:	fmov	d1, x1
  14:	fmov	d0, x2
  18:	and	x0, x0, #0xffffffff
  1c:	mov	x1, #0x4330000000000000    	// #4841369599423283200
  20:	fsub	d0, d1, d0
  24:	orr	x0, x0, x1
  28:	fmov	d1, x0
  2c:	fadd	d0, d0, d1
  30:	ret

floatundisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatundisf>:
   0:	cbz	x0, 94 <__floatundisf+0x94>
   4:	clz	x3, x0
   8:	mov	w1, #0x40                  	// #64
   c:	sub	w1, w1, w3
  10:	sub	w2, w1, #0x1
  14:	cmp	w1, #0x18
  18:	b.ls	84 <__floatundisf+0x84>  // b.plast
  1c:	cmp	w1, #0x19
  20:	b.eq	50 <__floatundisf+0x50>  // b.none
  24:	cmp	w1, #0x1a
  28:	b.eq	54 <__floatundisf+0x54>  // b.none
  2c:	add	w3, w3, #0x1a
  30:	sub	w4, w1, #0x1a
  34:	mov	x5, #0xffffffffffffffff    	// #-1
  38:	lsr	x3, x5, x3
  3c:	tst	x3, x0
  40:	lsr	x4, x0, x4
  44:	cset	x0, ne  // ne = any
  48:	orr	x0, x4, x0
  4c:	b	54 <__floatundisf+0x54>
  50:	lsl	x0, x0, #1
  54:	ubfx	x3, x0, #2, #1
  58:	orr	x0, x3, x0
  5c:	add	x3, x0, #0x1
  60:	lsr	x0, x3, #2
  64:	tbz	w3, #26, 70 <__floatundisf+0x70>
  68:	lsr	x0, x3, #3
  6c:	mov	w2, w1
  70:	add	w2, w2, #0x7f
  74:	and	w0, w0, #0x7fffff
  78:	orr	w0, w0, w2, lsl #23
  7c:	fmov	s0, w0
  80:	ret
  84:	mov	w3, #0x18                  	// #24
  88:	sub	w1, w3, w1
  8c:	lsl	x0, x0, x1
  90:	b	70 <__floatundisf+0x70>
  94:	movi	v0.2s, #0x0
  98:	b	80 <__floatundisf+0x80>

floatunsidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsidf>:
   0:	clz	w3, w0
   4:	mov	w2, w0
   8:	add	w1, w3, #0x15
   c:	movi	d0, #0x0
  10:	cmp	w0, #0x0
  14:	lsl	x2, x2, x1
  18:	mov	w1, #0x41e                 	// #1054
  1c:	sub	w1, w1, w3
  20:	eor	x2, x2, #0x10000000000000
  24:	add	x1, x2, x1, lsl #52
  28:	fmov	d1, x1
  2c:	fcsel	d0, d1, d0, ne  // ne = any
  30:	ret

floatunsisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatunsisf>:
   0:	cbz	w0, 70 <__floatunsisf+0x70>
   4:	clz	w3, w0
   8:	mov	w1, #0x1f                  	// #31
   c:	sub	w1, w1, w3
  10:	cmp	w1, #0x17
  14:	b.gt	38 <__floatunsisf+0x38>
  18:	sub	w1, w3, #0x8
  1c:	lsl	w0, w0, w1
  20:	eor	w1, w0, #0x800000
  24:	mov	w0, #0x9e                  	// #158
  28:	sub	w0, w0, w3
  2c:	add	w0, w1, w0, lsl #23
  30:	fmov	s0, w0
  34:	ret
  38:	mov	w2, #0x8                   	// #8
  3c:	sub	w2, w2, w3
  40:	lsr	w1, w0, w2
  44:	neg	w2, w2
  48:	eor	w1, w1, #0x800000
  4c:	add	w4, w1, #0x1
  50:	mov	w5, w1
  54:	lsl	w0, w0, w2
  58:	and	w1, w4, #0xfffffffe
  5c:	mov	w2, #0x80000000            	// #-2147483648
  60:	cmp	w0, w2
  64:	csel	w1, w1, w5, eq  // eq = none
  68:	csel	w1, w1, w4, ls  // ls = plast
  6c:	b	24 <__floatunsisf+0x24>
  70:	movi	v0.2s, #0x0
  74:	b	34 <__floatunsisf+0x34>

floatuntidf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntidf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 108 <__floatuntidf+0x108>
   8:	stp	x29, x30, [sp, #-80]!
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	x20, x0
  18:	mov	x19, x1
  1c:	stp	x21, x22, [sp, #32]
  20:	mov	w21, #0x80                  	// #128
  24:	stp	x23, x24, [sp, #48]
  28:	str	x25, [sp, #64]
  2c:	bl	0 <__clzti2>
  30:	sub	w21, w21, w0
  34:	mov	w23, w0
  38:	sub	w22, w21, #0x1
  3c:	cmp	w21, #0x35
  40:	b.le	f0 <__floatuntidf+0xf0>
  44:	cmp	w21, #0x36
  48:	b.eq	9c <__floatuntidf+0x9c>  // b.none
  4c:	cmp	w21, #0x37
  50:	b.eq	a4 <__floatuntidf+0xa4>  // b.none
  54:	sub	w2, w21, #0x37
  58:	mov	x0, x20
  5c:	mov	x1, x19
  60:	bl	0 <__lshrti3>
  64:	add	w2, w23, #0x37
  68:	mov	x25, x0
  6c:	mov	x24, x1
  70:	mov	x0, #0xffffffffffffffff    	// #-1
  74:	mov	x1, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x19
  80:	and	x20, x0, x20
  84:	orr	x20, x20, x1
  88:	mov	x19, x24
  8c:	cmp	x20, #0x0
  90:	cset	x20, ne  // ne = any
  94:	orr	x20, x20, x25
  98:	b	a4 <__floatuntidf+0xa4>
  9c:	extr	x19, x19, x20, #63
  a0:	lsl	x20, x20, #1
  a4:	ubfx	x0, x20, #2, #1
  a8:	orr	x20, x0, x20
  ac:	adds	x20, x20, #0x1
  b0:	cinc	x19, x19, cs  // cs = hs, nlast
  b4:	extr	x0, x19, x20, #2
  b8:	tbz	x20, #55, c4 <__floatuntidf+0xc4>
  bc:	mov	w22, w21
  c0:	extr	x0, x19, x20, #3
  c4:	add	w22, w22, #0x3ff
  c8:	ubfx	x1, x0, #32, #20
  cc:	ldp	x19, x20, [sp, #16]
  d0:	orr	w22, w1, w22, lsl #20
  d4:	ldp	x23, x24, [sp, #48]
  d8:	bfi	x0, x22, #32, #32
  dc:	fmov	d0, x0
  e0:	ldp	x21, x22, [sp, #32]
  e4:	ldr	x25, [sp, #64]
  e8:	ldp	x29, x30, [sp], #80
  ec:	ret
  f0:	mov	x0, x20
  f4:	mov	x1, x19
  f8:	mov	w2, #0x35                  	// #53
  fc:	sub	w2, w2, w21
 100:	bl	0 <__ashlti3>
 104:	b	c4 <__floatuntidf+0xc4>
 108:	movi	d0, #0x0
 10c:	ret

floatuntisf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__floatuntisf>:
   0:	orr	x2, x0, x1
   4:	cbz	x2, 104 <__floatuntisf+0x104>
   8:	stp	x29, x30, [sp, #-80]!
   c:	mov	x29, sp
  10:	stp	x19, x20, [sp, #16]
  14:	mov	x20, x0
  18:	mov	x19, x1
  1c:	stp	x21, x22, [sp, #32]
  20:	mov	w21, #0x80                  	// #128
  24:	stp	x23, x24, [sp, #48]
  28:	str	x25, [sp, #64]
  2c:	bl	0 <__clzti2>
  30:	sub	w21, w21, w0
  34:	mov	w23, w0
  38:	sub	w22, w21, #0x1
  3c:	cmp	w21, #0x18
  40:	b.le	ec <__floatuntisf+0xec>
  44:	cmp	w21, #0x19
  48:	b.eq	9c <__floatuntisf+0x9c>  // b.none
  4c:	cmp	w21, #0x1a
  50:	b.eq	a4 <__floatuntisf+0xa4>  // b.none
  54:	sub	w2, w21, #0x1a
  58:	mov	x0, x20
  5c:	mov	x1, x19
  60:	bl	0 <__lshrti3>
  64:	add	w2, w23, #0x1a
  68:	mov	x25, x0
  6c:	mov	x24, x1
  70:	mov	x0, #0xffffffffffffffff    	// #-1
  74:	mov	x1, #0xffffffffffffffff    	// #-1
  78:	bl	0 <__lshrti3>
  7c:	and	x1, x1, x19
  80:	and	x20, x0, x20
  84:	orr	x20, x20, x1
  88:	mov	x19, x24
  8c:	cmp	x20, #0x0
  90:	cset	x20, ne  // ne = any
  94:	orr	x20, x20, x25
  98:	b	a4 <__floatuntisf+0xa4>
  9c:	extr	x19, x19, x20, #63
  a0:	lsl	x20, x20, #1
  a4:	ubfx	x0, x20, #2, #1
  a8:	orr	x20, x0, x20
  ac:	adds	x20, x20, #0x1
  b0:	cinc	x19, x19, cs  // cs = hs, nlast
  b4:	extr	x0, x19, x20, #2
  b8:	tbz	w20, #26, c4 <__floatuntisf+0xc4>
  bc:	mov	w22, w21
  c0:	extr	x0, x19, x20, #3
  c4:	add	w22, w22, #0x7f
  c8:	and	w0, w0, #0x7fffff
  cc:	ldp	x19, x20, [sp, #16]
  d0:	orr	w0, w0, w22, lsl #23
  d4:	fmov	s0, w0
  d8:	ldp	x21, x22, [sp, #32]
  dc:	ldp	x23, x24, [sp, #48]
  e0:	ldr	x25, [sp, #64]
  e4:	ldp	x29, x30, [sp], #80
  e8:	ret
  ec:	mov	x0, x20
  f0:	mov	x1, x19
  f4:	mov	w2, #0x18                  	// #24
  f8:	sub	w2, w2, w21
  fc:	bl	0 <__ashlti3>
 100:	b	c4 <__floatuntisf+0xc4>
 104:	movi	v0.2s, #0x0
 108:	ret

int_util.c.o:     file format elf64-littleaarch64


Disassembly of section .text.unlikely:

0000000000000000 <__compilerrt_abort_impl>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x29, sp
   8:	bl	0 <abort>

lshrdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrdi3>:
   0:	tbz	w1, #5, 20 <__lshrdi3+0x20>
   4:	lsr	x0, x0, #32
   8:	sub	w1, w1, #0x20
   c:	mov	x3, #0x0                   	// #0
  10:	lsr	w0, w0, w1
  14:	bfxil	x3, x0, #0, #32
  18:	mov	x0, x3
  1c:	ret
  20:	cbz	w1, 1c <__lshrdi3+0x1c>
  24:	lsr	x2, x0, #32
  28:	mov	x3, #0x0                   	// #0
  2c:	lsr	w0, w0, w1
  30:	lsr	w4, w2, w1
  34:	bfi	x3, x4, #32, #32
  38:	neg	w4, w1
  3c:	lsl	w2, w2, w4
  40:	orr	w0, w2, w0
  44:	b	14 <__lshrdi3+0x14>

lshrti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__lshrti3>:
   0:	tbz	w2, #6, 18 <__lshrti3+0x18>
   4:	sub	w0, w2, #0x40
   8:	mov	x3, #0x0                   	// #0
   c:	lsr	x0, x1, x0
  10:	mov	x1, x3
  14:	ret
  18:	cbz	w2, 14 <__lshrti3+0x14>
  1c:	neg	w4, w2
  20:	lsr	x3, x1, x2
  24:	lsr	x0, x0, x2
  28:	lsl	x1, x1, x4
  2c:	orr	x0, x1, x0
  30:	b	10 <__lshrti3+0x10>

moddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__moddi3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	eor	x4, x1, x1, asr #63
   8:	eor	x3, x0, x0, asr #63
   c:	mov	x29, sp
  10:	add	x2, sp, #0x28
  14:	sub	x1, x4, x1, asr #63
  18:	str	x19, [sp, #16]
  1c:	asr	x19, x0, #63
  20:	sub	x0, x3, x0, asr #63
  24:	bl	0 <__udivmoddi4>
  28:	ldr	x0, [sp, #40]
  2c:	eor	x0, x19, x0
  30:	sub	x0, x0, x19
  34:	ldr	x19, [sp, #16]
  38:	ldp	x29, x30, [sp], #48
  3c:	ret

modsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__divsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

modti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__modti3>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	asr	x4, x3, #63
   8:	eor	x2, x2, x3, asr #63
   c:	mov	x29, sp
  10:	subs	x2, x2, x4
  14:	eor	x0, x0, x1, asr #63
  18:	eor	x3, x3, x3, asr #63
  1c:	str	x19, [sp, #16]
  20:	asr	x19, x1, #63
  24:	sbc	x3, x3, x4
  28:	eor	x1, x1, x1, asr #63
  2c:	subs	x0, x0, x19
  30:	add	x4, sp, #0x20
  34:	sbc	x1, x1, x19
  38:	bl	0 <__udivmodti4>
  3c:	ldp	x0, x1, [sp, #32]
  40:	eor	x0, x0, x19
  44:	eor	x1, x1, x19
  48:	subs	x0, x0, x19
  4c:	sbc	x1, x1, x19
  50:	ldr	x19, [sp, #16]
  54:	ldp	x29, x30, [sp], #48
  58:	ret

muldc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldc3>:
   0:	fmul	d17, d1, d3
   4:	fmul	d18, d0, d2
   8:	fmul	d16, d0, d3
   c:	fmov	d4, d0
  10:	fmul	d7, d2, d1
  14:	fmov	d5, d1
  18:	fsub	d0, d18, d17
  1c:	fadd	d1, d16, d7
  20:	fcmp	d0, d0
  24:	b.vc	12c <__muldc3+0x12c>
  28:	fcmp	d1, d1
  2c:	b.vc	12c <__muldc3+0x12c>
  30:	adrp	x0, 0 <__muldc3>
  34:	fabs	d19, d4
  38:	fabs	d21, d5
  3c:	ldr	d6, [x0]
  40:	fcmp	d19, d6
  44:	cset	w0, gt
  48:	b.gt	54 <__muldc3+0x54>
  4c:	fcmp	d21, d6
  50:	b.le	130 <__muldc3+0x130>
  54:	fcmp	d21, d6
  58:	scvtf	d20, w0
  5c:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  60:	fmov	d19, x0
  64:	cset	w0, gt
  68:	bif	v4.8b, v20.8b, v19.8b
  6c:	fcmp	d2, d2
  70:	scvtf	d20, w0
  74:	bif	v5.8b, v20.8b, v19.8b
  78:	b.vc	84 <__muldc3+0x84>
  7c:	movi	d20, #0x0
  80:	bif	v2.8b, v20.8b, v19.8b
  84:	fcmp	d3, d3
  88:	mov	w1, #0x1                   	// #1
  8c:	b.vc	a0 <__muldc3+0xa0>
  90:	movi	d19, #0x0
  94:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  98:	fmov	d20, x0
  9c:	bif	v3.8b, v19.8b, v20.8b
  a0:	fabs	d20, d2
  a4:	fabs	d19, d3
  a8:	fcmp	d20, d6
  ac:	cset	w0, gt
  b0:	b.gt	bc <__muldc3+0xbc>
  b4:	fcmp	d19, d6
  b8:	b.le	138 <__muldc3+0x138>
  bc:	fcmp	d19, d6
  c0:	scvtf	d1, w0
  c4:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  c8:	fmov	d0, x0
  cc:	cset	w0, gt
  d0:	bif	v2.8b, v1.8b, v0.8b
  d4:	fcmp	d4, d4
  d8:	scvtf	d1, w0
  dc:	bif	v3.8b, v1.8b, v0.8b
  e0:	b.vc	ec <__muldc3+0xec>
  e4:	movi	d1, #0x0
  e8:	bif	v4.8b, v1.8b, v0.8b
  ec:	fcmp	d5, d5
  f0:	b.vc	104 <__muldc3+0x104>
  f4:	movi	d0, #0x0
  f8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  fc:	fmov	d1, x0
 100:	bif	v5.8b, v0.8b, v1.8b
 104:	fmul	d1, d5, d3
 108:	fmul	d0, d4, d2
 10c:	fmul	d5, d5, d2
 110:	fmul	d4, d4, d3
 114:	adrp	x0, 0 <__muldc3>
 118:	fsub	d0, d0, d1
 11c:	ldr	d1, [x0]
 120:	fadd	d4, d4, d5
 124:	fmul	d0, d0, d1
 128:	fmul	d1, d4, d1
 12c:	ret
 130:	mov	w1, #0x0                   	// #0
 134:	b	a0 <__muldc3+0xa0>
 138:	cbnz	w1, 104 <__muldc3+0x104>
 13c:	fabs	d18, d18
 140:	fcmp	d18, d6
 144:	b.gt	16c <__muldc3+0x16c>
 148:	fabs	d17, d17
 14c:	fcmp	d17, d6
 150:	b.gt	16c <__muldc3+0x16c>
 154:	fabs	d16, d16
 158:	fcmp	d16, d6
 15c:	b.gt	16c <__muldc3+0x16c>
 160:	fabs	d7, d7
 164:	fcmp	d7, d6
 168:	b.le	12c <__muldc3+0x12c>
 16c:	fcmp	d4, d4
 170:	b.vc	184 <__muldc3+0x184>
 174:	movi	d0, #0x0
 178:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 17c:	fmov	d1, x0
 180:	bif	v4.8b, v0.8b, v1.8b
 184:	fcmp	d5, d5
 188:	b.vc	19c <__muldc3+0x19c>
 18c:	movi	d0, #0x0
 190:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 194:	fmov	d1, x0
 198:	bif	v5.8b, v0.8b, v1.8b
 19c:	fcmp	d2, d2
 1a0:	b.vc	1b4 <__muldc3+0x1b4>
 1a4:	movi	d0, #0x0
 1a8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1ac:	fmov	d1, x0
 1b0:	bif	v2.8b, v0.8b, v1.8b
 1b4:	fcmp	d3, d3
 1b8:	b.vc	104 <__muldc3+0x104>
 1bc:	movi	d0, #0x0
 1c0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 1c4:	fmov	d1, x0
 1c8:	bif	v3.8b, v0.8b, v1.8b
 1cc:	b	104 <__muldc3+0x104>

muldf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldf3>:
   0:	fmov	x4, d0
   4:	fmov	x2, d1
   8:	ubfx	x3, x4, #52, #11
   c:	eor	x0, x4, x2
  10:	sub	w1, w3, #0x1
  14:	ubfx	x10, x2, #52, #11
  18:	and	x0, x0, #0x8000000000000000
  1c:	and	x7, x4, #0xfffffffffffff
  20:	and	x5, x2, #0xfffffffffffff
  24:	cmp	w1, #0x7fd
  28:	b.hi	38 <__muldf3+0x38>  // b.pmore
  2c:	sub	w1, w10, #0x1
  30:	cmp	w1, #0x7fd
  34:	b.ls	168 <__muldf3+0x168>  // b.plast
  38:	and	x6, x4, #0x7fffffffffffffff
  3c:	mov	x8, #0x7ff0000000000000    	// #9218868437227405312
  40:	cmp	x6, x8
  44:	b.ls	54 <__muldf3+0x54>  // b.plast
  48:	orr	x0, x4, #0x8000000000000
  4c:	fmov	d0, x0
  50:	b	88 <__muldf3+0x88>
  54:	and	x1, x2, #0x7fffffffffffffff
  58:	cmp	x1, x8
  5c:	b.ls	68 <__muldf3+0x68>  // b.plast
  60:	orr	x0, x2, #0x8000000000000
  64:	b	4c <__muldf3+0x4c>
  68:	cmp	x6, x8
  6c:	b.ne	8c <__muldf3+0x8c>  // b.any
  70:	cmp	x1, #0x0
  74:	orr	x0, x0, #0x7ff0000000000000
  78:	mov	x2, #0x7ff8000000000000    	// #9221120237041090560
  7c:	fmov	d0, x0
  80:	fmov	d1, x2
  84:	fcsel	d0, d1, d0, eq  // eq = none
  88:	ret
  8c:	cmp	x1, x8
  90:	b.ne	ac <__muldf3+0xac>  // b.any
  94:	orr	x0, x0, #0x7ff0000000000000
  98:	mov	x1, #0x7ff8000000000000    	// #9221120237041090560
  9c:	cmp	x6, #0x0
  a0:	fmov	d1, x1
  a4:	fmov	d0, x0
  a8:	b	84 <__muldf3+0x84>
  ac:	cbnz	x6, b8 <__muldf3+0xb8>
  b0:	fmov	d0, x0
  b4:	b	88 <__muldf3+0x88>
  b8:	cbz	x1, b0 <__muldf3+0xb0>
  bc:	tst	x4, #0x7ff0000000000000
  c0:	b.ne	160 <__muldf3+0x160>  // b.any
  c4:	clz	x1, x7
  c8:	sub	w4, w1, #0xb
  cc:	mov	w1, #0x1                   	// #1
  d0:	sub	w1, w1, w4
  d4:	lsl	x7, x7, x4
  d8:	tst	x2, #0x7ff0000000000000
  dc:	b.ne	f4 <__muldf3+0xf4>  // b.any
  e0:	clz	x2, x5
  e4:	sub	w2, w2, #0xb
  e8:	sub	w1, w1, w2
  ec:	add	w1, w1, #0x1
  f0:	lsl	x5, x5, x2
  f4:	and	x8, x7, #0xffffffff
  f8:	lsr	x7, x7, #32
  fc:	orr	x4, x7, #0x100000
 100:	lsl	w9, w5, #11
 104:	ubfx	x5, x5, #21, #32
 108:	add	w3, w3, w10
 10c:	orr	x5, x5, #0x80000000
 110:	add	w1, w3, w1
 114:	mul	x2, x9, x8
 118:	mul	x9, x4, x9
 11c:	mul	x8, x5, x8
 120:	and	x7, x9, #0xffffffff
 124:	add	x7, x7, x2, lsr #32
 128:	lsr	x9, x9, #32
 12c:	add	x7, x7, w8, uxtw
 130:	and	x2, x2, #0xffffffff
 134:	madd	x5, x5, x4, x9
 138:	add	x6, x2, x7, lsl #32
 13c:	lsr	x7, x7, #32
 140:	add	x7, x7, x8, lsr #32
 144:	add	x4, x5, x7
 148:	tbz	x4, #52, 170 <__muldf3+0x170>
 14c:	sub	w1, w1, #0x3fe
 150:	cmp	w1, #0x7fe
 154:	b.le	180 <__muldf3+0x180>
 158:	orr	x0, x0, #0x7ff0000000000000
 15c:	b	4c <__muldf3+0x4c>
 160:	mov	w1, #0x0                   	// #0
 164:	b	d8 <__muldf3+0xd8>
 168:	mov	w1, #0x0                   	// #0
 16c:	b	f4 <__muldf3+0xf4>
 170:	sub	w1, w1, #0x3ff
 174:	extr	x4, x4, x6, #63
 178:	lsl	x6, x6, #1
 17c:	b	150 <__muldf3+0x150>
 180:	cmp	w1, #0x0
 184:	b.gt	1e0 <__muldf3+0x1e0>
 188:	mov	w3, #0x1                   	// #1
 18c:	sub	w3, w3, w1
 190:	cmp	w3, #0x3f
 194:	b.hi	b0 <__muldf3+0xb0>  // b.pmore
 198:	add	w2, w1, #0x3f
 19c:	lsr	x1, x6, x3
 1a0:	lsl	x5, x4, x2
 1a4:	orr	x5, x5, x1
 1a8:	lsl	x2, x6, x2
 1ac:	cmp	x2, #0x0
 1b0:	cset	x2, ne  // ne = any
 1b4:	lsr	x1, x4, x3
 1b8:	orr	x6, x5, x2
 1bc:	orr	x0, x0, x1
 1c0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 1c4:	add	x2, x0, #0x1
 1c8:	cmp	x6, x1
 1cc:	mov	x3, x0
 1d0:	and	x0, x2, #0xfffffffffffffffe
 1d4:	csel	x0, x0, x3, eq  // eq = none
 1d8:	csel	x0, x0, x2, ls  // ls = plast
 1dc:	b	b0 <__muldf3+0xb0>
 1e0:	and	x4, x4, #0xfffffffffffff
 1e4:	orr	x1, x4, x1, lsl #52
 1e8:	b	1bc <__muldf3+0x1bc>

muldi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muldi3>:
   0:	mov	x4, x0
   4:	and	w3, w1, #0xffff
   8:	and	w2, w0, #0xffff
   c:	lsr	w7, w1, #16
  10:	lsr	w5, w4, #16
  14:	mul	w0, w2, w3
  18:	mul	w3, w3, w5
  1c:	mul	w2, w2, w7
  20:	mul	w5, w5, w7
  24:	add	w3, w3, w0, lsr #16
  28:	add	w2, w2, w3, uxth
  2c:	add	w3, w5, w3, lsr #16
  30:	lsl	w6, w2, #16
  34:	add	w2, w3, w2, lsr #16
  38:	add	w6, w6, w0, uxth
  3c:	mov	x0, #0x0                   	// #0
  40:	bfxil	x0, x6, #0, #32
  44:	bfi	x0, x2, #32, #32
  48:	asr	x2, x1, #32
  4c:	asr	x3, x0, #32
  50:	madd	w2, w4, w2, w3
  54:	asr	x4, x4, #32
  58:	madd	w1, w1, w4, w2
  5c:	bfi	x0, x1, #32, #32
  60:	ret

mulodi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulodi4>:
   0:	mov	x3, x0
   4:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
   8:	mul	x0, x0, x1
   c:	cmp	x3, x4
  10:	b.ne	24 <__mulodi4+0x24>  // b.any
  14:	cmp	x1, #0x1
  18:	b.hi	70 <__mulodi4+0x70>  // b.pmore
  1c:	str	wzr, [x2]
  20:	ret
  24:	cmp	x1, x4
  28:	b.ne	34 <__mulodi4+0x34>  // b.any
  2c:	cmp	x3, #0x1
  30:	b	18 <__mulodi4+0x18>
  34:	eor	x5, x3, x3, asr #63
  38:	asr	x7, x3, #63
  3c:	eor	x6, x1, x1, asr #63
  40:	sub	x3, x5, x3, asr #63
  44:	cmp	x3, #0x1
  48:	asr	x5, x1, #63
  4c:	sub	x1, x6, x1, asr #63
  50:	ccmp	x1, #0x1, #0x4, gt
  54:	b.le	1c <__mulodi4+0x1c>
  58:	cmp	x7, x5
  5c:	b.ne	7c <__mulodi4+0x7c>  // b.any
  60:	mov	x4, #0x7fffffffffffffff    	// #9223372036854775807
  64:	sdiv	x1, x4, x1
  68:	cmp	x1, x3
  6c:	b.ge	1c <__mulodi4+0x1c>  // b.tcont
  70:	mov	w1, #0x1                   	// #1
  74:	str	w1, [x2]
  78:	b	20 <__mulodi4+0x20>
  7c:	sub	x5, x5, x6
  80:	sdiv	x4, x4, x5
  84:	cmp	x4, x3
  88:	b	6c <__mulodi4+0x6c>

mulosi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulosi4>:
   0:	mov	w3, w0
   4:	mov	w4, #0x80000000            	// #-2147483648
   8:	mul	w0, w0, w1
   c:	cmp	w3, w4
  10:	b.ne	24 <__mulosi4+0x24>  // b.any
  14:	cmp	w1, #0x1
  18:	b.hi	70 <__mulosi4+0x70>  // b.pmore
  1c:	str	wzr, [x2]
  20:	ret
  24:	cmp	w1, w4
  28:	b.ne	34 <__mulosi4+0x34>  // b.any
  2c:	cmp	w3, #0x1
  30:	b	18 <__mulosi4+0x18>
  34:	eor	w5, w3, w3, asr #31
  38:	asr	w7, w3, #31
  3c:	eor	w6, w1, w1, asr #31
  40:	sub	w3, w5, w3, asr #31
  44:	cmp	w3, #0x1
  48:	asr	w5, w1, #31
  4c:	sub	w1, w6, w1, asr #31
  50:	ccmp	w1, #0x1, #0x4, gt
  54:	b.le	1c <__mulosi4+0x1c>
  58:	cmp	w7, w5
  5c:	b.ne	7c <__mulosi4+0x7c>  // b.any
  60:	mov	w4, #0x7fffffff            	// #2147483647
  64:	sdiv	w1, w4, w1
  68:	cmp	w1, w3
  6c:	b.ge	1c <__mulosi4+0x1c>  // b.tcont
  70:	mov	w1, #0x1                   	// #1
  74:	str	w1, [x2]
  78:	b	20 <__mulosi4+0x20>
  7c:	sub	w5, w5, w6
  80:	sdiv	w4, w4, w5
  84:	cmp	w4, w3
  88:	b	6c <__mulosi4+0x6c>

muloti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__muloti4>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x20, x4
  10:	stp	x21, x22, [sp, #32]
  14:	mul	x21, x0, x2
  18:	str	x23, [sp, #48]
  1c:	umulh	x23, x0, x2
  20:	madd	x23, x1, x2, x23
  24:	madd	x23, x0, x3, x23
  28:	cbnz	x0, 70 <__muloti4+0x70>
  2c:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  30:	cmp	x1, x4
  34:	b.ne	70 <__muloti4+0x70>  // b.any
  38:	cbnz	x3, 64 <__muloti4+0x64>
  3c:	cmp	x2, #0x1
  40:	b.hi	64 <__muloti4+0x64>  // b.pmore
  44:	str	wzr, [x20]
  48:	mov	x0, x21
  4c:	mov	x1, x23
  50:	ldp	x19, x20, [sp, #16]
  54:	ldp	x21, x22, [sp, #32]
  58:	ldr	x23, [sp, #48]
  5c:	ldp	x29, x30, [sp], #64
  60:	ret
  64:	mov	w0, #0x1                   	// #1
  68:	str	w0, [x20]
  6c:	b	48 <__muloti4+0x48>
  70:	cbnz	x2, 8c <__muloti4+0x8c>
  74:	mov	x4, #0x8000000000000000    	// #-9223372036854775808
  78:	cmp	x3, x4
  7c:	b.ne	8c <__muloti4+0x8c>  // b.any
  80:	cbnz	x1, 64 <__muloti4+0x64>
  84:	cmp	x0, #0x1
  88:	b	40 <__muloti4+0x40>
  8c:	asr	x5, x1, #63
  90:	eor	x0, x0, x1, asr #63
  94:	subs	x22, x0, x5
  98:	eor	x1, x1, x1, asr #63
  9c:	asr	x4, x3, #63
  a0:	eor	x0, x2, x3, asr #63
  a4:	sbc	x19, x1, x5
  a8:	subs	x2, x0, x4
  ac:	eor	x1, x3, x3, asr #63
  b0:	mov	w6, #0x1                   	// #1
  b4:	sbc	x3, x1, x4
  b8:	cmp	x19, #0x0
  bc:	b.gt	cc <__muloti4+0xcc>
  c0:	b.ne	d0 <__muloti4+0xd0>  // b.any
  c4:	cmp	x22, #0x1
  c8:	b.ls	d0 <__muloti4+0xd0>  // b.plast
  cc:	mov	w6, #0x0                   	// #0
  d0:	cmp	x3, #0x0
  d4:	mov	w7, #0x1                   	// #1
  d8:	b.gt	e8 <__muloti4+0xe8>
  dc:	b.ne	ec <__muloti4+0xec>  // b.any
  e0:	cmp	x2, #0x1
  e4:	b.ls	ec <__muloti4+0xec>  // b.plast
  e8:	mov	w7, #0x0                   	// #0
  ec:	orr	w6, w6, w7
  f0:	tbnz	w6, #0, 44 <__muloti4+0x44>
  f4:	cmp	x5, x4
  f8:	b.ne	11c <__muloti4+0x11c>  // b.any
  fc:	mov	x0, #0xffffffffffffffff    	// #-1
 100:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 104:	bl	0 <__divti3>
 108:	cmp	x19, x1
 10c:	b.gt	64 <__muloti4+0x64>
 110:	b.ne	44 <__muloti4+0x44>  // b.any
 114:	cmp	x22, x0
 118:	b	40 <__muloti4+0x40>
 11c:	subs	x2, x4, x0
 120:	mov	x0, #0x0                   	// #0
 124:	sbc	x3, x4, x1
 128:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 12c:	b	104 <__muloti4+0x104>

mulsc3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsc3>:
   0:	fmul	s17, s1, s3
   4:	fmul	s18, s0, s2
   8:	fmul	s16, s0, s3
   c:	fmov	s4, s0
  10:	fmul	s7, s2, s1
  14:	fmov	s5, s1
  18:	fsub	s0, s18, s17
  1c:	fadd	s1, s16, s7
  20:	fcmp	s0, s0
  24:	b.vc	11c <__mulsc3+0x11c>
  28:	fcmp	s1, s1
  2c:	b.vc	11c <__mulsc3+0x11c>
  30:	adrp	x0, 0 <__mulsc3>
  34:	fabs	s19, s4
  38:	fabs	s21, s5
  3c:	ldr	s6, [x0]
  40:	fcmp	s19, s6
  44:	cset	w0, gt
  48:	b.gt	54 <__mulsc3+0x54>
  4c:	fcmp	s21, s6
  50:	b.le	120 <__mulsc3+0x120>
  54:	fcmp	s21, s6
  58:	scvtf	s20, w0
  5c:	movi	v19.2s, #0x80, lsl #24
  60:	cset	w0, gt
  64:	fcmp	s2, s2
  68:	bif	v4.8b, v20.8b, v19.8b
  6c:	scvtf	s20, w0
  70:	bif	v5.8b, v20.8b, v19.8b
  74:	b.vc	80 <__mulsc3+0x80>
  78:	movi	v20.2s, #0x0
  7c:	bif	v2.8b, v20.8b, v19.8b
  80:	fcmp	s3, s3
  84:	mov	w1, #0x1                   	// #1
  88:	b.vc	98 <__mulsc3+0x98>
  8c:	movi	v19.2s, #0x0
  90:	movi	v20.2s, #0x80, lsl #24
  94:	bif	v3.8b, v19.8b, v20.8b
  98:	fabs	s20, s2
  9c:	fabs	s19, s3
  a0:	fcmp	s20, s6
  a4:	cset	w0, gt
  a8:	b.gt	b4 <__mulsc3+0xb4>
  ac:	fcmp	s19, s6
  b0:	b.le	128 <__mulsc3+0x128>
  b4:	fcmp	s19, s6
  b8:	scvtf	s1, w0
  bc:	movi	v0.2s, #0x80, lsl #24
  c0:	cset	w0, gt
  c4:	fcmp	s4, s4
  c8:	bif	v2.8b, v1.8b, v0.8b
  cc:	scvtf	s1, w0
  d0:	bif	v3.8b, v1.8b, v0.8b
  d4:	b.vc	e0 <__mulsc3+0xe0>
  d8:	movi	v1.2s, #0x0
  dc:	bif	v4.8b, v1.8b, v0.8b
  e0:	fcmp	s5, s5
  e4:	b.vc	f4 <__mulsc3+0xf4>
  e8:	movi	v0.2s, #0x0
  ec:	movi	v1.2s, #0x80, lsl #24
  f0:	bif	v5.8b, v0.8b, v1.8b
  f4:	fmul	s1, s5, s3
  f8:	fmul	s0, s4, s2
  fc:	fmul	s5, s5, s2
 100:	fmul	s4, s4, s3
 104:	adrp	x0, 0 <__mulsc3>
 108:	fsub	s0, s0, s1
 10c:	ldr	s1, [x0]
 110:	fadd	s4, s4, s5
 114:	fmul	s0, s0, s1
 118:	fmul	s1, s4, s1
 11c:	ret
 120:	mov	w1, #0x0                   	// #0
 124:	b	98 <__mulsc3+0x98>
 128:	cbnz	w1, f4 <__mulsc3+0xf4>
 12c:	fabs	s18, s18
 130:	fcmp	s18, s6
 134:	b.gt	15c <__mulsc3+0x15c>
 138:	fabs	s17, s17
 13c:	fcmp	s17, s6
 140:	b.gt	15c <__mulsc3+0x15c>
 144:	fabs	s16, s16
 148:	fcmp	s16, s6
 14c:	b.gt	15c <__mulsc3+0x15c>
 150:	fabs	s7, s7
 154:	fcmp	s7, s6
 158:	b.le	11c <__mulsc3+0x11c>
 15c:	fcmp	s4, s4
 160:	b.vc	170 <__mulsc3+0x170>
 164:	movi	v0.2s, #0x0
 168:	movi	v1.2s, #0x80, lsl #24
 16c:	bif	v4.8b, v0.8b, v1.8b
 170:	fcmp	s5, s5
 174:	b.vc	184 <__mulsc3+0x184>
 178:	movi	v0.2s, #0x0
 17c:	movi	v1.2s, #0x80, lsl #24
 180:	bif	v5.8b, v0.8b, v1.8b
 184:	fcmp	s2, s2
 188:	b.vc	198 <__mulsc3+0x198>
 18c:	movi	v0.2s, #0x0
 190:	movi	v1.2s, #0x80, lsl #24
 194:	bif	v2.8b, v0.8b, v1.8b
 198:	fcmp	s3, s3
 19c:	b.vc	f4 <__mulsc3+0xf4>
 1a0:	movi	v0.2s, #0x0
 1a4:	movi	v1.2s, #0x80, lsl #24
 1a8:	bif	v3.8b, v0.8b, v1.8b
 1ac:	b	f4 <__mulsc3+0xf4>

mulsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulsf3>:
   0:	fmov	w6, s0
   4:	fmov	w5, s1
   8:	ubfx	x3, x6, #23, #8
   c:	eor	w0, w6, w5
  10:	sub	w1, w3, #0x1
  14:	ubfx	x10, x5, #23, #8
  18:	and	w0, w0, #0x80000000
  1c:	and	w7, w6, #0x7fffff
  20:	and	w2, w5, #0x7fffff
  24:	cmp	w1, #0xfd
  28:	b.hi	38 <__mulsf3+0x38>  // b.pmore
  2c:	sub	w1, w10, #0x1
  30:	cmp	w1, #0xfd
  34:	b.ls	138 <__mulsf3+0x138>  // b.plast
  38:	and	w8, w6, #0x7fffffff
  3c:	mov	w9, #0x7f800000            	// #2139095040
  40:	cmp	w8, w9
  44:	b.ls	54 <__mulsf3+0x54>  // b.plast
  48:	orr	w0, w6, #0x400000
  4c:	fmov	s0, w0
  50:	b	88 <__mulsf3+0x88>
  54:	and	w4, w5, #0x7fffffff
  58:	cmp	w4, w9
  5c:	b.ls	68 <__mulsf3+0x68>  // b.plast
  60:	orr	w0, w5, #0x400000
  64:	b	4c <__mulsf3+0x4c>
  68:	cmp	w8, w9
  6c:	b.ne	8c <__mulsf3+0x8c>  // b.any
  70:	cmp	w4, #0x0
  74:	orr	w0, w0, #0x7f800000
  78:	mov	w1, #0x7fc00000            	// #2143289344
  7c:	fmov	s0, w0
  80:	fmov	s1, w1
  84:	fcsel	s0, s1, s0, eq  // eq = none
  88:	ret
  8c:	cmp	w4, w9
  90:	b.ne	ac <__mulsf3+0xac>  // b.any
  94:	orr	w0, w0, #0x7f800000
  98:	mov	w1, #0x7fc00000            	// #2143289344
  9c:	cmp	w8, #0x0
  a0:	fmov	s1, w1
  a4:	fmov	s0, w0
  a8:	b	84 <__mulsf3+0x84>
  ac:	cbnz	w8, b8 <__mulsf3+0xb8>
  b0:	fmov	s0, w0
  b4:	b	88 <__mulsf3+0x88>
  b8:	cbz	w4, b0 <__mulsf3+0xb0>
  bc:	tst	w6, #0x7f800000
  c0:	b.ne	130 <__mulsf3+0x130>  // b.any
  c4:	clz	w1, w7
  c8:	sub	w4, w1, #0x8
  cc:	mov	w1, #0x1                   	// #1
  d0:	sub	w1, w1, w4
  d4:	lsl	w7, w7, w4
  d8:	tst	w5, #0x7f800000
  dc:	b.ne	f4 <__mulsf3+0xf4>  // b.any
  e0:	clz	w4, w2
  e4:	sub	w4, w4, #0x8
  e8:	sub	w1, w1, w4
  ec:	add	w1, w1, #0x1
  f0:	lsl	w2, w2, w4
  f4:	lsl	w4, w2, #8
  f8:	orr	w7, w7, #0x800000
  fc:	orr	w4, w4, #0x80000000
 100:	add	w3, w3, w10
 104:	add	w1, w3, w1
 108:	umull	x4, w4, w7
 10c:	mov	w5, w4
 110:	lsr	x2, x4, #32
 114:	mov	w6, w2
 118:	tbz	w2, #23, 140 <__mulsf3+0x140>
 11c:	sub	w1, w1, #0x7e
 120:	cmp	w1, #0xfe
 124:	b.le	150 <__mulsf3+0x150>
 128:	orr	w0, w0, #0x7f800000
 12c:	b	4c <__mulsf3+0x4c>
 130:	mov	w1, #0x0                   	// #0
 134:	b	d8 <__mulsf3+0xd8>
 138:	mov	w1, #0x0                   	// #0
 13c:	b	f4 <__mulsf3+0xf4>
 140:	sub	w1, w1, #0x7f
 144:	extr	w6, w2, w4, #31
 148:	lsl	w5, w4, #1
 14c:	b	120 <__mulsf3+0x120>
 150:	cmp	w1, #0x0
 154:	b.gt	1b0 <__mulsf3+0x1b0>
 158:	mov	w3, #0x1                   	// #1
 15c:	sub	w3, w3, w1
 160:	cmp	w3, #0x1f
 164:	b.hi	b0 <__mulsf3+0xb0>  // b.pmore
 168:	add	w2, w1, #0x1f
 16c:	lsr	w4, w5, w3
 170:	lsl	w1, w6, w2
 174:	orr	w1, w1, w4
 178:	lsl	w2, w5, w2
 17c:	cmp	w2, #0x0
 180:	cset	w5, ne  // ne = any
 184:	orr	w5, w1, w5
 188:	lsr	w1, w6, w3
 18c:	orr	w0, w0, w1
 190:	mov	w1, #0x80000000            	// #-2147483648
 194:	add	w2, w0, #0x1
 198:	cmp	w5, w1
 19c:	mov	w3, w0
 1a0:	and	w0, w2, #0xfffffffe
 1a4:	csel	w0, w0, w3, eq  // eq = none
 1a8:	csel	w0, w0, w2, ls  // ls = plast
 1ac:	b	b0 <__mulsf3+0xb0>
 1b0:	and	w6, w6, #0x7fffff
 1b4:	orr	w1, w6, w1, lsl #23
 1b8:	b	18c <__mulsf3+0x18c>

multi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multi3>:
   0:	and	x4, x0, #0xffffffff
   4:	and	x5, x2, #0xffffffff
   8:	lsr	x6, x0, #32
   c:	lsr	x8, x2, #32
  10:	mul	x7, x4, x5
  14:	mul	x5, x5, x6
  18:	mul	x4, x4, x8
  1c:	mul	x6, x6, x8
  20:	add	x5, x5, x7, lsr #32
  24:	add	x4, x4, w5, uxtw
  28:	add	x5, x6, x5, lsr #32
  2c:	lsl	x9, x4, #32
  30:	add	x4, x5, x4, lsr #32
  34:	madd	x4, x0, x3, x4
  38:	add	x0, x9, w7, uxtw
  3c:	madd	x1, x2, x1, x4
  40:	ret

multf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__multf3>:
   0:	stp	x29, x30, [sp, #-128]!
   4:	mov	x29, sp
   8:	str	q0, [sp, #96]
   c:	ldp	x0, x1, [sp, #96]
  10:	str	q1, [sp, #96]
  14:	ldp	x2, x5, [sp, #96]
  18:	stp	x19, x20, [sp, #16]
  1c:	stp	x21, x22, [sp, #32]
  20:	mov	x22, #0x0                   	// #0
  24:	ubfx	x20, x1, #48, #15
  28:	stp	x23, x24, [sp, #48]
  2c:	sub	w4, w20, #0x1
  30:	mov	x3, x2
  34:	stp	x25, x26, [sp, #64]
  38:	eor	x2, x1, x5
  3c:	and	x23, x2, #0x8000000000000000
  40:	stp	x27, x28, [sp, #80]
  44:	mov	w2, #0x7ffd                	// #32765
  48:	mov	x25, x0
  4c:	and	x24, x1, #0xffffffffffff
  50:	ubfx	x21, x5, #48, #15
  54:	mov	x28, x3
  58:	and	x19, x5, #0xffffffffffff
  5c:	cmp	w4, w2
  60:	b.hi	70 <__multf3+0x70>  // b.pmore
  64:	sub	w4, w21, #0x1
  68:	cmp	w4, w2
  6c:	b.ls	308 <__multf3+0x308>  // b.plast
  70:	and	x4, x1, #0x7fffffffffffffff
  74:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
  78:	cmp	x4, x6
  7c:	b.hi	88 <__multf3+0x88>  // b.pmore
  80:	b.ne	b0 <__multf3+0xb0>  // b.any
  84:	cbz	x0, b0 <__multf3+0xb0>
  88:	fmov	d0, x0
  8c:	orr	x3, x1, #0x800000000000
  90:	fmov	v0.d[1], x3
  94:	ldp	x19, x20, [sp, #16]
  98:	ldp	x21, x22, [sp, #32]
  9c:	ldp	x23, x24, [sp, #48]
  a0:	ldp	x25, x26, [sp, #64]
  a4:	ldp	x27, x28, [sp, #80]
  a8:	ldp	x29, x30, [sp], #128
  ac:	ret
  b0:	and	x2, x5, #0x7fffffffffffffff
  b4:	cmp	x2, x6
  b8:	b.hi	c4 <__multf3+0xc4>  // b.pmore
  bc:	b.ne	d8 <__multf3+0xd8>  // b.any
  c0:	cbz	x3, d8 <__multf3+0xd8>
  c4:	mov	x0, x3
  c8:	orr	x1, x5, #0x800000000000
  cc:	fmov	d0, x0
  d0:	fmov	v0.d[1], x1
  d4:	b	94 <__multf3+0x94>
  d8:	cbnz	x0, fc <__multf3+0xfc>
  dc:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
  e0:	cmp	x4, x6
  e4:	b.ne	fc <__multf3+0xfc>  // b.any
  e8:	orr	x2, x3, x2
  ec:	cbz	x2, 114 <__multf3+0x114>
  f0:	mov	x0, x22
  f4:	orr	x1, x23, x6
  f8:	b	cc <__multf3+0xcc>
  fc:	orr	x4, x0, x4
 100:	cbnz	x3, 124 <__multf3+0x124>
 104:	mov	x6, #0x7fff000000000000    	// #9223090561878065152
 108:	cmp	x2, x6
 10c:	b.ne	124 <__multf3+0x124>  // b.any
 110:	cbnz	x4, f0 <__multf3+0xf0>
 114:	adrp	x0, 0 <__multf3>
 118:	add	x0, x0, #0x0
 11c:	ldr	q0, [x0]
 120:	b	94 <__multf3+0x94>
 124:	cbnz	x4, 134 <__multf3+0x134>
 128:	fmov	d0, x22
 12c:	fmov	v0.d[1], x23
 130:	b	94 <__multf3+0x94>
 134:	orr	x2, x3, x2
 138:	cbz	x2, 128 <__multf3+0x128>
 13c:	tst	x1, #0x7fff000000000000
 140:	b.ne	300 <__multf3+0x300>  // b.any
 144:	cmp	x24, #0x0
 148:	mov	x27, #0x40                  	// #64
 14c:	csel	x26, x24, x0, ne  // ne = any
 150:	csel	x27, xzr, x27, ne  // ne = any
 154:	clz	x26, x26
 158:	mov	x1, x24
 15c:	add	w27, w27, w26
 160:	str	x3, [sp, #96]
 164:	sub	w2, w27, #0xf
 168:	str	x5, [sp, #120]
 16c:	mov	x27, x2
 170:	bl	0 <__ashlti3>
 174:	ldr	x3, [sp, #96]
 178:	mov	w2, #0x1                   	// #1
 17c:	ldr	x5, [sp, #120]
 180:	mov	x25, x0
 184:	mov	x24, x1
 188:	sub	w27, w2, w27
 18c:	tst	x5, #0x7fff000000000000
 190:	b.ne	1d0 <__multf3+0x1d0>  // b.any
 194:	cmp	x19, #0x0
 198:	mov	x26, #0x40                  	// #64
 19c:	csel	x2, x19, x3, ne  // ne = any
 1a0:	csel	x26, xzr, x26, ne  // ne = any
 1a4:	clz	x2, x2
 1a8:	mov	x1, x19
 1ac:	add	w26, w26, w2
 1b0:	mov	x0, x3
 1b4:	sub	w2, w26, #0xf
 1b8:	mov	x26, x2
 1bc:	bl	0 <__ashlti3>
 1c0:	sub	w2, w27, w26
 1c4:	mov	x28, x0
 1c8:	mov	x19, x1
 1cc:	add	w27, w2, #0x1
 1d0:	ubfx	x4, x28, #17, #32
 1d4:	lsr	x3, x25, #32
 1d8:	and	x25, x25, #0xffffffff
 1dc:	extr	x19, x19, x28, #49
 1e0:	lsl	w28, w28, #15
 1e4:	orr	x19, x19, #0x8000000000000000
 1e8:	and	x5, x19, #0xffffffff
 1ec:	orr	x24, x24, #0x1000000000000
 1f0:	mul	x13, x4, x25
 1f4:	and	x7, x24, #0xffffffff
 1f8:	mul	x2, x28, x3
 1fc:	lsr	x6, x24, #32
 200:	mul	x24, x5, x25
 204:	lsr	x8, x19, #32
 208:	adds	x13, x2, x13
 20c:	mul	x2, x4, x3
 210:	cset	x1, cs  // cs = hs, nlast
 214:	mul	x9, x4, x7
 218:	adds	x2, x2, x24
 21c:	mul	x24, x28, x7
 220:	mul	x19, x5, x3
 224:	cset	x12, cs  // cs = hs, nlast
 228:	mul	x0, x8, x25
 22c:	adds	x2, x2, x24
 230:	mul	x10, x6, x28
 234:	cinc	x12, x12, cs  // cs = hs, nlast
 238:	adds	x9, x9, x19
 23c:	extr	x1, x1, x13, #32
 240:	cset	x19, cs  // cs = hs, nlast
 244:	adds	x10, x10, x0
 248:	cset	x0, cs  // cs = hs, nlast
 24c:	adds	x10, x9, x10
 250:	adc	x9, x19, x0
 254:	mul	x25, x28, x25
 258:	adds	x1, x1, x2
 25c:	lsl	x24, x10, #32
 260:	cset	x0, cs  // cs = hs, nlast
 264:	mul	x19, x5, x7
 268:	adds	x1, x1, x24
 26c:	lsl	x2, x13, #32
 270:	mul	x4, x6, x4
 274:	cinc	x11, x0, cs  // cs = hs, nlast
 278:	mul	x3, x8, x3
 27c:	adds	x25, x2, x25
 280:	cinc	x24, x1, cs  // cs = hs, nlast
 284:	mul	x26, x6, x8
 288:	adds	x19, x4, x19
 28c:	extr	x10, x9, x10, #32
 290:	cset	x1, cs  // cs = hs, nlast
 294:	adds	x19, x3, x19
 298:	adc	x26, x26, x1
 29c:	mul	x0, x6, x5
 2a0:	adds	x3, x12, x10
 2a4:	mul	x7, x8, x7
 2a8:	cset	x1, cs  // cs = hs, nlast
 2ac:	adds	x19, x19, x3
 2b0:	adc	x26, x26, x1
 2b4:	adds	x0, x0, x7
 2b8:	cset	x1, cs  // cs = hs, nlast
 2bc:	add	w2, w20, w21
 2c0:	add	w2, w2, w27
 2c4:	extr	x1, x1, x0, #32
 2c8:	lsl	x0, x0, #32
 2cc:	adds	x0, x0, x11
 2d0:	cinc	x1, x1, cs  // cs = hs, nlast
 2d4:	adds	x19, x19, x0
 2d8:	adc	x26, x26, x1
 2dc:	tbz	x26, #48, 310 <__multf3+0x310>
 2e0:	mov	w1, #0xffffc002            	// #-16382
 2e4:	add	w2, w2, w1
 2e8:	mov	w0, #0x7ffe                	// #32766
 2ec:	cmp	w2, w0
 2f0:	b.le	32c <__multf3+0x32c>
 2f4:	mov	x0, x22
 2f8:	orr	x1, x23, #0x7fff000000000000
 2fc:	b	cc <__multf3+0xcc>
 300:	mov	w27, #0x0                   	// #0
 304:	b	18c <__multf3+0x18c>
 308:	mov	w27, #0x0                   	// #0
 30c:	b	1d0 <__multf3+0x1d0>
 310:	mov	w0, #0xffffc001            	// #-16383
 314:	extr	x26, x26, x19, #63
 318:	add	w2, w2, w0
 31c:	extr	x19, x19, x24, #63
 320:	extr	x24, x24, x25, #63
 324:	lsl	x25, x25, #1
 328:	b	2e8 <__multf3+0x2e8>
 32c:	cmp	w2, #0x0
 330:	b.gt	3e8 <__multf3+0x3e8>
 334:	mov	w21, #0x1                   	// #1
 338:	sub	w21, w21, w2
 33c:	cmp	w21, #0x7f
 340:	b.hi	128 <__multf3+0x128>  // b.pmore
 344:	add	w20, w2, #0x7f
 348:	mov	x0, x19
 34c:	mov	x2, x20
 350:	mov	x1, x26
 354:	bl	0 <__ashlti3>
 358:	mov	x28, x0
 35c:	mov	x27, x1
 360:	mov	x2, x21
 364:	mov	x0, x25
 368:	mov	x1, x24
 36c:	bl	0 <__lshrti3>
 370:	orr	x28, x28, x0
 374:	orr	x27, x27, x1
 378:	mov	x2, x20
 37c:	mov	x0, x25
 380:	mov	x1, x24
 384:	bl	0 <__ashlti3>
 388:	orr	x0, x0, x1
 38c:	cmp	x0, #0x0
 390:	mov	x2, x21
 394:	cset	x25, ne  // ne = any
 398:	mov	x0, x19
 39c:	mov	x1, x26
 3a0:	orr	x25, x28, x25
 3a4:	bl	0 <__lshrti3>
 3a8:	mov	x24, x27
 3ac:	mov	x19, x0
 3b0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 3b4:	orr	x4, x22, x19
 3b8:	orr	x5, x23, x1
 3bc:	cmp	x24, x0
 3c0:	b.hi	3cc <__multf3+0x3cc>  // b.pmore
 3c4:	b.ne	3f4 <__multf3+0x3f4>  // b.any
 3c8:	cbz	x25, 3f8 <__multf3+0x3f8>
 3cc:	adds	x2, x4, #0x1
 3d0:	cinc	x0, x5, cs  // cs = hs, nlast
 3d4:	mov	x4, x2
 3d8:	mov	x5, x0
 3dc:	fmov	d0, x4
 3e0:	fmov	v0.d[1], x5
 3e4:	b	94 <__multf3+0x94>
 3e8:	and	x1, x26, #0xffffffffffff
 3ec:	orr	x1, x1, x2, lsl #48
 3f0:	b	3b0 <__multf3+0x3b0>
 3f4:	cbnz	x25, 3dc <__multf3+0x3dc>
 3f8:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
 3fc:	cmp	x24, x0
 400:	b.ne	3dc <__multf3+0x3dc>  // b.any
 404:	adds	x1, x4, #0x1
 408:	cinc	x0, x5, cs  // cs = hs, nlast
 40c:	and	x4, x1, #0xfffffffffffffffe
 410:	mov	x5, x0
 414:	b	3dc <__multf3+0x3dc>

mulvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x3, #0x8000000000000000    	// #-9223372036854775808
   8:	cmp	x0, x3
   c:	mov	x29, sp
  10:	b.ne	40 <__mulvdi3+0x40>  // b.any
  14:	cmp	x1, #0x1
  18:	b.hi	28 <__mulvdi3+0x28>  // b.pmore
  1c:	lsl	x0, x1, #63
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	adrp	x2, 0 <__mulvdi3>
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x1a                  	// #26
  34:	adrp	x0, 0 <__mulvdi3>
  38:	add	x0, x0, #0x0
  3c:	bl	0 <__compilerrt_abort_impl>
  40:	cmp	x1, x3
  44:	b.ne	68 <__mulvdi3+0x68>  // b.any
  48:	cmp	x0, #0x1
  4c:	b.hi	58 <__mulvdi3+0x58>  // b.pmore
  50:	lsl	x0, x0, #63
  54:	b	20 <__mulvdi3+0x20>
  58:	adrp	x2, 0 <__mulvdi3>
  5c:	mov	w1, #0x1f                  	// #31
  60:	add	x2, x2, #0x0
  64:	b	34 <__mulvdi3+0x34>
  68:	eor	x2, x0, x0, asr #63
  6c:	eor	x5, x1, x1, asr #63
  70:	sub	x2, x2, x0, asr #63
  74:	sub	x6, x5, x1, asr #63
  78:	cmp	x2, #0x1
  7c:	asr	x7, x0, #63
  80:	asr	x4, x1, #63
  84:	ccmp	x6, #0x1, #0x4, gt
  88:	b.gt	94 <__mulvdi3+0x94>
  8c:	mul	x0, x0, x1
  90:	b	20 <__mulvdi3+0x20>
  94:	cmp	x7, x4
  98:	b.ne	bc <__mulvdi3+0xbc>  // b.any
  9c:	mov	x3, #0x7fffffffffffffff    	// #9223372036854775807
  a0:	sdiv	x3, x3, x6
  a4:	cmp	x3, x2
  a8:	b.ge	8c <__mulvdi3+0x8c>  // b.tcont
  ac:	adrp	x2, 0 <__mulvdi3>
  b0:	mov	w1, #0x29                  	// #41
  b4:	add	x2, x2, #0x0
  b8:	b	34 <__mulvdi3+0x34>
  bc:	sub	x4, x4, x5
  c0:	sdiv	x3, x3, x4
  c4:	cmp	x3, x2
  c8:	b.ge	8c <__mulvdi3+0x8c>  // b.tcont
  cc:	adrp	x2, 0 <__mulvdi3>
  d0:	mov	w1, #0x2c                  	// #44
  d4:	add	x2, x2, #0x0
  d8:	b	34 <__mulvdi3+0x34>

mulvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w3, #0x80000000            	// #-2147483648
   8:	cmp	w0, w3
   c:	mov	x29, sp
  10:	b.ne	40 <__mulvsi3+0x40>  // b.any
  14:	cmp	w1, #0x1
  18:	b.hi	28 <__mulvsi3+0x28>  // b.pmore
  1c:	lsl	w0, w1, #31
  20:	ldp	x29, x30, [sp], #16
  24:	ret
  28:	adrp	x2, 0 <__mulvsi3>
  2c:	add	x2, x2, #0x0
  30:	mov	w1, #0x1a                  	// #26
  34:	adrp	x0, 0 <__mulvsi3>
  38:	add	x0, x0, #0x0
  3c:	bl	0 <__compilerrt_abort_impl>
  40:	cmp	w1, w3
  44:	b.ne	68 <__mulvsi3+0x68>  // b.any
  48:	cmp	w0, #0x1
  4c:	b.hi	58 <__mulvsi3+0x58>  // b.pmore
  50:	lsl	w0, w0, #31
  54:	b	20 <__mulvsi3+0x20>
  58:	adrp	x2, 0 <__mulvsi3>
  5c:	mov	w1, #0x1f                  	// #31
  60:	add	x2, x2, #0x0
  64:	b	34 <__mulvsi3+0x34>
  68:	eor	w2, w0, w0, asr #31
  6c:	eor	w5, w1, w1, asr #31
  70:	sub	w2, w2, w0, asr #31
  74:	sub	w6, w5, w1, asr #31
  78:	cmp	w2, #0x1
  7c:	asr	w7, w0, #31
  80:	asr	w4, w1, #31
  84:	ccmp	w6, #0x1, #0x4, gt
  88:	b.gt	94 <__mulvsi3+0x94>
  8c:	mul	w0, w0, w1
  90:	b	20 <__mulvsi3+0x20>
  94:	cmp	w7, w4
  98:	b.ne	bc <__mulvsi3+0xbc>  // b.any
  9c:	mov	w3, #0x7fffffff            	// #2147483647
  a0:	sdiv	w3, w3, w6
  a4:	cmp	w3, w2
  a8:	b.ge	8c <__mulvsi3+0x8c>  // b.tcont
  ac:	adrp	x2, 0 <__mulvsi3>
  b0:	mov	w1, #0x29                  	// #41
  b4:	add	x2, x2, #0x0
  b8:	b	34 <__mulvsi3+0x34>
  bc:	sub	w4, w4, w5
  c0:	sdiv	w3, w3, w4
  c4:	cmp	w3, w2
  c8:	b.ge	8c <__mulvsi3+0x8c>  // b.tcont
  cc:	adrp	x2, 0 <__mulvsi3>
  d0:	mov	w1, #0x2c                  	// #44
  d4:	add	x2, x2, #0x0
  d8:	b	34 <__mulvsi3+0x34>

mulvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__mulvti3>:
   0:	stp	x29, x30, [sp, #-64]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	x19, x0
  10:	mov	x20, x2
  14:	stp	x21, x22, [sp, #32]
  18:	mov	x21, x1
  1c:	mov	x22, x3
  20:	stp	x23, x24, [sp, #48]
  24:	cbnz	x0, 74 <__mulvti3+0x74>
  28:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  2c:	cmp	x1, x0
  30:	b.ne	74 <__mulvti3+0x74>  // b.any
  34:	cbnz	x3, 5c <__mulvti3+0x5c>
  38:	cmp	x2, #0x1
  3c:	b.hi	5c <__mulvti3+0x5c>  // b.pmore
  40:	lsl	x1, x2, #63
  44:	mov	x0, #0x0                   	// #0
  48:	ldp	x19, x20, [sp, #16]
  4c:	ldp	x21, x22, [sp, #32]
  50:	ldp	x23, x24, [sp, #48]
  54:	ldp	x29, x30, [sp], #64
  58:	ret
  5c:	adrp	x2, 0 <__mulvti3>
  60:	add	x2, x2, #0x0
  64:	mov	w1, #0x1c                  	// #28
  68:	adrp	x0, 0 <__mulvti3>
  6c:	add	x0, x0, #0x0
  70:	bl	0 <__compilerrt_abort_impl>
  74:	cbnz	x20, a8 <__mulvti3+0xa8>
  78:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
  7c:	cmp	x22, x0
  80:	b.ne	a8 <__mulvti3+0xa8>  // b.any
  84:	cbnz	x21, 98 <__mulvti3+0x98>
  88:	cmp	x19, #0x1
  8c:	b.hi	98 <__mulvti3+0x98>  // b.pmore
  90:	lsl	x1, x19, #63
  94:	b	44 <__mulvti3+0x44>
  98:	adrp	x2, 0 <__mulvti3>
  9c:	mov	w1, #0x21                  	// #33
  a0:	add	x2, x2, #0x0
  a4:	b	68 <__mulvti3+0x68>
  a8:	asr	x4, x21, #63
  ac:	eor	x23, x19, x21, asr #63
  b0:	subs	x23, x23, x4
  b4:	asr	x0, x22, #63
  b8:	eor	x24, x21, x21, asr #63
  bc:	eor	x1, x20, x22, asr #63
  c0:	sbc	x24, x24, x4
  c4:	eor	x6, x22, x22, asr #63
  c8:	subs	x2, x1, x0
  cc:	mov	w5, #0x1                   	// #1
  d0:	sbc	x3, x6, x0
  d4:	cmp	x24, #0x0
  d8:	b.gt	e8 <__mulvti3+0xe8>
  dc:	b.ne	ec <__mulvti3+0xec>  // b.any
  e0:	cmp	x23, #0x1
  e4:	b.ls	ec <__mulvti3+0xec>  // b.plast
  e8:	mov	w5, #0x0                   	// #0
  ec:	cmp	x3, #0x0
  f0:	mov	w7, #0x1                   	// #1
  f4:	b.gt	104 <__mulvti3+0x104>
  f8:	b.ne	108 <__mulvti3+0x108>  // b.any
  fc:	cmp	x2, #0x1
 100:	b.ls	108 <__mulvti3+0x108>  // b.plast
 104:	mov	w7, #0x0                   	// #0
 108:	orr	w5, w5, w7
 10c:	tbz	w5, #0, 124 <__mulvti3+0x124>
 110:	umulh	x1, x19, x20
 114:	madd	x1, x21, x20, x1
 118:	mul	x0, x19, x20
 11c:	madd	x1, x19, x22, x1
 120:	b	48 <__mulvti3+0x48>
 124:	cmp	x4, x0
 128:	b.ne	15c <__mulvti3+0x15c>  // b.any
 12c:	mov	x0, #0xffffffffffffffff    	// #-1
 130:	mov	x1, #0x7fffffffffffffff    	// #9223372036854775807
 134:	bl	0 <__divti3>
 138:	cmp	x24, x1
 13c:	b.gt	14c <__mulvti3+0x14c>
 140:	b.ne	110 <__mulvti3+0x110>  // b.any
 144:	cmp	x23, x0
 148:	b.ls	110 <__mulvti3+0x110>  // b.plast
 14c:	adrp	x2, 0 <__mulvti3>
 150:	mov	w1, #0x2b                  	// #43
 154:	add	x2, x2, #0x0
 158:	b	68 <__mulvti3+0x68>
 15c:	subs	x2, x0, x1
 160:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
 164:	sbc	x3, x0, x6
 168:	mov	x0, #0x0                   	// #0
 16c:	bl	0 <__divti3>
 170:	cmp	x24, x1
 174:	b.gt	184 <__mulvti3+0x184>
 178:	b.ne	110 <__mulvti3+0x110>  // b.any
 17c:	cmp	x23, x0
 180:	b.ls	110 <__mulvti3+0x110>  // b.plast
 184:	adrp	x2, 0 <__mulvti3>
 188:	mov	w1, #0x2e                  	// #46
 18c:	add	x2, x2, #0x0
 190:	b	68 <__mulvti3+0x68>

negdf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdf2>:
   0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
   4:	fmov	d1, x0
   8:	add	d0, d0, d1
   c:	ret

negdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negdi2>:
   0:	neg	x0, x0
   4:	ret

negsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negsf2>:
   0:	mov	w0, #0x80000000            	// #-2147483648
   4:	fmov	s1, w0
   8:	add	v0.2s, v0.2s, v1.2s
   c:	ret

negti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negti2>:
   0:	negs	x0, x0
   4:	ngc	x1, x1
   8:	ret

negvdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvdi2>:
   0:	mov	x1, #0x8000000000000000    	// #-9223372036854775808
   4:	cmp	x0, x1
   8:	b.ne	2c <__negvdi2+0x2c>  // b.any
   c:	stp	x29, x30, [sp, #-16]!
  10:	adrp	x2, 0 <__negvdi2>
  14:	adrp	x0, 0 <__negvdi2>
  18:	mov	x29, sp
  1c:	add	x2, x2, #0x0
  20:	add	x0, x0, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	bl	0 <__compilerrt_abort_impl>
  2c:	neg	x0, x0
  30:	ret

negvsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvsi2>:
   0:	mov	w1, #0x80000000            	// #-2147483648
   4:	cmp	w0, w1
   8:	b.ne	2c <__negvsi2+0x2c>  // b.any
   c:	stp	x29, x30, [sp, #-16]!
  10:	adrp	x2, 0 <__negvsi2>
  14:	adrp	x0, 0 <__negvsi2>
  18:	mov	x29, sp
  1c:	add	x2, x2, #0x0
  20:	add	x0, x0, #0x0
  24:	mov	w1, #0x16                  	// #22
  28:	bl	0 <__compilerrt_abort_impl>
  2c:	neg	w0, w0
  30:	ret

negvti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__negvti2>:
   0:	cbnz	x0, 30 <__negvti2+0x30>
   4:	mov	x2, #0x8000000000000000    	// #-9223372036854775808
   8:	cmp	x1, x2
   c:	b.ne	30 <__negvti2+0x30>  // b.any
  10:	stp	x29, x30, [sp, #-16]!
  14:	adrp	x2, 0 <__negvti2>
  18:	adrp	x0, 0 <__negvti2>
  1c:	mov	x29, sp
  20:	add	x2, x2, #0x0
  24:	add	x0, x0, #0x0
  28:	mov	w1, #0x18                  	// #24
  2c:	bl	0 <__compilerrt_abort_impl>
  30:	negs	x0, x0
  34:	ngc	x1, x1
  38:	ret

os_version_check.c.o:     file format elf64-littleaarch64


paritydi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritydi2>:
   0:	lsr	x1, x0, #32
   4:	eor	w0, w1, w0
   8:	b	0 <__paritysi2>

paritysi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__paritysi2>:
   0:	eor	w0, w0, w0, lsr #16
   4:	mov	w1, #0x6996                	// #27030
   8:	eor	w0, w0, w0, lsr #8
   c:	eor	w0, w0, w0, lsr #4
  10:	and	w0, w0, #0xf
  14:	asr	w0, w1, w0
  18:	and	w0, w0, #0x1
  1c:	ret

parityti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__parityti2>:
   0:	eor	x0, x1, x0
   4:	b	0 <__paritydi2>

popcountdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountdi2>:
   0:	lsr	x1, x0, #1
   4:	and	x1, x1, #0x5555555555555555
   8:	sub	x1, x0, x1
   c:	lsr	x0, x1, #2
  10:	and	x1, x1, #0x3333333333333333
  14:	and	x0, x0, #0x3333333333333333
  18:	add	x0, x0, x1
  1c:	add	x0, x0, x0, lsr #4
  20:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  24:	lsr	x1, x0, #32
  28:	add	w0, w1, w0
  2c:	add	w0, w0, w0, lsr #16
  30:	add	w0, w0, w0, lsr #8
  34:	and	w0, w0, #0x7f
  38:	ret

popcountsi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountsi2>:
   0:	lsr	w1, w0, #1
   4:	and	w1, w1, #0x55555555
   8:	sub	w1, w0, w1
   c:	lsr	w0, w1, #2
  10:	and	w1, w1, #0x33333333
  14:	and	w0, w0, #0x33333333
  18:	add	w0, w0, w1
  1c:	add	w0, w0, w0, lsr #4
  20:	and	w0, w0, #0xf0f0f0f
  24:	add	w0, w0, w0, lsr #16
  28:	add	w0, w0, w0, lsr #8
  2c:	and	w0, w0, #0x3f
  30:	ret

popcountti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__popcountti2>:
   0:	mov	x3, x0
   4:	extr	x0, x1, x0, #1
   8:	lsr	x2, x1, #1
   c:	and	x0, x0, #0x5555555555555555
  10:	subs	x3, x3, x0
  14:	and	x2, x2, #0x5555555555555555
  18:	sbc	x1, x1, x2
  1c:	lsr	x0, x1, #2
  20:	extr	x2, x1, x3, #2
  24:	and	x2, x2, #0x3333333333333333
  28:	and	x3, x3, #0x3333333333333333
  2c:	adds	x2, x2, x3
  30:	and	x1, x1, #0x3333333333333333
  34:	and	x0, x0, #0x3333333333333333
  38:	adc	x0, x0, x1
  3c:	lsr	x3, x0, #4
  40:	extr	x1, x0, x2, #4
  44:	adds	x2, x1, x2
  48:	adc	x0, x0, x3
  4c:	and	x2, x2, #0xf0f0f0f0f0f0f0f
  50:	and	x0, x0, #0xf0f0f0f0f0f0f0f
  54:	add	x0, x0, x2
  58:	lsr	x1, x0, #32
  5c:	add	w0, w1, w0
  60:	add	w0, w0, w0, lsr #16
  64:	add	w0, w0, w0, lsr #8
  68:	and	w0, w0, #0xff
  6c:	ret

powidf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powidf2>:
   0:	fmov	d1, d0
   4:	fmov	d0, #1.000000000000000000e+00
   8:	mov	w1, w0
   c:	mov	w2, #0x2                   	// #2
  10:	fmov	d2, d0
  14:	tbz	w1, #0, 1c <__powidf2+0x1c>
  18:	fmul	d0, d0, d1
  1c:	sdiv	w1, w1, w2
  20:	cbz	w1, 2c <__powidf2+0x2c>
  24:	fmul	d1, d1, d1
  28:	b	14 <__powidf2+0x14>
  2c:	tbz	w0, #31, 34 <__powidf2+0x34>
  30:	fdiv	d0, d2, d0
  34:	ret

powisf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__powisf2>:
   0:	fmov	s1, s0
   4:	fmov	s0, #1.000000000000000000e+00
   8:	mov	w1, w0
   c:	mov	w2, #0x2                   	// #2
  10:	fmov	s2, s0
  14:	tbz	w1, #0, 1c <__powisf2+0x1c>
  18:	fmul	s0, s0, s1
  1c:	sdiv	w1, w1, w2
  20:	cbz	w1, 2c <__powisf2+0x2c>
  24:	fmul	s1, s1, s1
  28:	b	14 <__powisf2+0x14>
  2c:	tbz	w0, #31, 34 <__powisf2+0x34>
  30:	fdiv	s0, s2, s0
  34:	ret

powitf2.c.o:     file format elf64-littleaarch64


subdf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subdf3>:
   0:	mov	x0, #0x8000000000000000    	// #-9223372036854775808
   4:	fmov	d2, x0
   8:	add	d1, d1, d2
   c:	b	0 <__adddf3>

subsf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subsf3>:
   0:	movi	v2.2s, #0x80, lsl #24
   4:	add	v1.2s, v1.2s, v2.2s
   8:	b	0 <__addsf3>

subvdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvdi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x2, x0
   8:	sub	x0, x0, x1
   c:	mov	x29, sp
  10:	tbnz	x1, #63, 34 <__subvdi3+0x34>
  14:	cmp	x2, x0
  18:	b.ge	4c <__subvdi3+0x4c>  // b.tcont
  1c:	adrp	x2, 0 <__subvdi3>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x17                  	// #23
  28:	adrp	x0, 0 <__subvdi3>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>
  34:	cmp	x2, x0
  38:	b.lt	4c <__subvdi3+0x4c>  // b.tstop
  3c:	adrp	x2, 0 <__subvdi3>
  40:	mov	w1, #0x1a                  	// #26
  44:	add	x2, x2, #0x0
  48:	b	28 <__subvdi3+0x28>
  4c:	ldp	x29, x30, [sp], #16
  50:	ret

subvsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvsi3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	w2, w0
   8:	sub	w0, w0, w1
   c:	mov	x29, sp
  10:	tbnz	w1, #31, 34 <__subvsi3+0x34>
  14:	cmp	w2, w0
  18:	b.ge	4c <__subvsi3+0x4c>  // b.tcont
  1c:	adrp	x2, 0 <__subvsi3>
  20:	add	x2, x2, #0x0
  24:	mov	w1, #0x17                  	// #23
  28:	adrp	x0, 0 <__subvsi3>
  2c:	add	x0, x0, #0x0
  30:	bl	0 <__compilerrt_abort_impl>
  34:	cmp	w2, w0
  38:	b.lt	4c <__subvsi3+0x4c>  // b.tstop
  3c:	adrp	x2, 0 <__subvsi3>
  40:	mov	w1, #0x1a                  	// #26
  44:	add	x2, x2, #0x0
  48:	b	28 <__subvsi3+0x28>
  4c:	ldp	x29, x30, [sp], #16
  50:	ret

subvti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subvti3>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	mov	x4, x0
   8:	subs	x0, x0, x2
   c:	mov	x29, sp
  10:	mov	x5, x1
  14:	sbc	x1, x1, x3
  18:	tbnz	x3, #63, 48 <__subvti3+0x48>
  1c:	cmp	x1, x5
  20:	b.gt	30 <__subvti3+0x30>
  24:	b.ne	6c <__subvti3+0x6c>  // b.any
  28:	cmp	x0, x4
  2c:	b.ls	6c <__subvti3+0x6c>  // b.plast
  30:	adrp	x2, 0 <__subvti3>
  34:	add	x2, x2, #0x0
  38:	mov	w1, #0x19                  	// #25
  3c:	adrp	x0, 0 <__subvti3>
  40:	add	x0, x0, #0x0
  44:	bl	0 <__compilerrt_abort_impl>
  48:	cmp	x1, x5
  4c:	b.gt	6c <__subvti3+0x6c>
  50:	b.ne	5c <__subvti3+0x5c>  // b.any
  54:	cmp	x0, x4
  58:	b.hi	6c <__subvti3+0x6c>  // b.pmore
  5c:	adrp	x2, 0 <__subvti3>
  60:	mov	w1, #0x1c                  	// #28
  64:	add	x2, x2, #0x0
  68:	b	3c <__subvti3+0x3c>
  6c:	ldp	x29, x30, [sp], #16
  70:	ret

subtf3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__subtf3>:
   0:	sub	sp, sp, #0x10
   4:	str	q1, [sp]
   8:	ldp	x3, x2, [sp]
   c:	add	sp, sp, #0x10
  10:	fmov	d1, x3
  14:	eor	x1, x2, #0x8000000000000000
  18:	fmov	v1.d[1], x1
  1c:	b	0 <__addtf3>

trampoline_setup.c.o:     file format elf64-littleaarch64


truncdfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfhf2>:
   0:	fmov	x3, d0
   4:	mov	x2, #0xc0f0000000000000    	// #-4544132024016830464
   8:	mov	x0, #0xbf10000000000000    	// #-4679240012837945344
   c:	and	x1, x3, #0x7fffffffffffffff
  10:	add	x2, x1, x2
  14:	add	x0, x1, x0
  18:	cmp	x2, x0
  1c:	b.cs	68 <__truncdfhf2+0x68>  // b.hs, b.nlast
  20:	ubfx	x0, x1, #42, #16
  24:	mov	x4, #0x20000000000         	// #2199023255552
  28:	and	x5, x3, #0x3ffffffffff
  2c:	add	w2, w0, #0x4, lsl #12
  30:	cmp	x5, x4
  34:	mov	w1, #0x4001                	// #16385
  38:	mov	w4, #0x4001                	// #16385
  3c:	add	w1, w0, w1
  40:	add	w0, w0, w4
  44:	and	w2, w2, #0xffff
  48:	and	w0, w0, #0xfffe
  4c:	and	w1, w1, #0xffff
  50:	csel	w0, w0, w2, eq  // eq = none
  54:	csel	w1, w0, w1, ls  // ls = plast
  58:	lsr	x0, x3, #48
  5c:	and	x0, x0, #0x8000
  60:	orr	w0, w1, w0
  64:	ret
  68:	mov	x0, #0x7ff0000000000000    	// #9218868437227405312
  6c:	cmp	x1, x0
  70:	b.ls	80 <__truncdfhf2+0x80>  // b.plast
  74:	ubfx	x1, x1, #42, #9
  78:	orr	w1, w1, #0x7e00
  7c:	b	58 <__truncdfhf2+0x58>
  80:	mov	x0, #0x40efffffffffffff    	// #4679240012837945343
  84:	cmp	x1, x0
  88:	b.hi	f0 <__truncdfhf2+0xf0>  // b.pmore
  8c:	lsr	x2, x1, #52
  90:	mov	w1, #0x3f1                 	// #1009
  94:	sub	w0, w1, w2
  98:	cmp	w0, #0x34
  9c:	b.gt	f8 <__truncdfhf2+0xf8>
  a0:	and	x1, x3, #0xfffffffffffff
  a4:	sub	w2, w2, #0x3b1
  a8:	orr	x1, x1, #0x10000000000000
  ac:	mov	x5, #0x20000000000         	// #2199023255552
  b0:	lsl	x2, x1, x2
  b4:	cmp	x2, #0x0
  b8:	lsr	x1, x1, x0
  bc:	cset	x2, ne  // ne = any
  c0:	orr	x2, x2, x1
  c4:	ubfx	x1, x1, #42, #16
  c8:	add	w4, w1, #0x1
  cc:	mov	w0, w1
  d0:	and	x2, x2, #0x3ffffffffff
  d4:	add	w1, w1, #0x1
  d8:	cmp	x2, x5
  dc:	and	w1, w1, #0xfffe
  e0:	and	w4, w4, #0xffff
  e4:	csel	w1, w1, w0, eq  // eq = none
  e8:	csel	w1, w1, w4, ls  // ls = plast
  ec:	b	58 <__truncdfhf2+0x58>
  f0:	mov	w1, #0x7c00                	// #31744
  f4:	b	58 <__truncdfhf2+0x58>
  f8:	mov	w1, #0x0                   	// #0
  fc:	b	58 <__truncdfhf2+0x58>

truncdfsf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncdfsf2>:
   0:	fmov	x2, d0
   4:	mov	x3, #0xc7f0000000000000    	// #-4039728865751334912
   8:	mov	x1, #0xb810000000000000    	// #-5183643171103440896
   c:	and	x0, x2, #0x7fffffffffffffff
  10:	add	x3, x0, x3
  14:	add	x1, x0, x1
  18:	cmp	x3, x1
  1c:	b.cs	64 <__truncdfsf2+0x64>  // b.hs, b.nlast
  20:	lsr	x0, x0, #29
  24:	mov	w3, #0x1                   	// #1
  28:	and	x6, x2, #0x1fffffff
  2c:	movk	w3, #0x4000, lsl #16
  30:	add	w3, w3, w0
  34:	mov	x5, #0x10000000            	// #268435456
  38:	mov	w4, #0x40000000            	// #1073741824
  3c:	cmp	x6, x5
  40:	add	w4, w4, w0
  44:	and	w0, w3, #0xfffffffe
  48:	csel	w0, w0, w4, eq  // eq = none
  4c:	csel	w0, w0, w3, ls  // ls = plast
  50:	lsr	x2, x2, #32
  54:	and	x2, x2, #0x80000000
  58:	orr	w0, w0, w2
  5c:	fmov	s0, w0
  60:	ret
  64:	mov	x1, #0x7ff0000000000000    	// #9218868437227405312
  68:	cmp	x0, x1
  6c:	b.ls	7c <__truncdfsf2+0x7c>  // b.plast
  70:	ubfx	x0, x0, #29, #22
  74:	orr	w0, w0, #0x7fc00000
  78:	b	50 <__truncdfsf2+0x50>
  7c:	mov	x1, #0x47efffffffffffff    	// #5183643171103440895
  80:	cmp	x0, x1
  84:	b.hi	e4 <__truncdfsf2+0xe4>  // b.pmore
  88:	lsr	x0, x0, #52
  8c:	mov	w1, #0x381                 	// #897
  90:	sub	w4, w1, w0
  94:	cmp	w4, #0x34
  98:	b.gt	ec <__truncdfsf2+0xec>
  9c:	and	x1, x2, #0xfffffffffffff
  a0:	sub	w0, w0, #0x341
  a4:	orr	x1, x1, #0x10000000000000
  a8:	lsl	x0, x1, x0
  ac:	cmp	x0, #0x0
  b0:	lsr	x1, x1, x4
  b4:	cset	x3, ne  // ne = any
  b8:	orr	x3, x3, x1
  bc:	lsr	x1, x1, #29
  c0:	add	w5, w1, #0x1
  c4:	and	x3, x3, #0x1fffffff
  c8:	mov	w0, w1
  cc:	mov	x4, #0x10000000            	// #268435456
  d0:	and	w1, w5, #0xfffffffe
  d4:	cmp	x3, x4
  d8:	csel	w0, w1, w0, eq  // eq = none
  dc:	csel	w0, w0, w5, ls  // ls = plast
  e0:	b	50 <__truncdfsf2+0x50>
  e4:	mov	w0, #0x7f800000            	// #2139095040
  e8:	b	50 <__truncdfsf2+0x50>
  ec:	mov	w0, #0x0                   	// #0
  f0:	b	50 <__truncdfsf2+0x50>

truncsfhf2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__truncsfhf2>:
   0:	fmov	w2, s0
   4:	mov	w3, #0xc7800000            	// #-947912704
   8:	mov	w1, #0xb8800000            	// #-1199570944
   c:	and	w0, w2, #0x7fffffff
  10:	add	w3, w0, w3
  14:	add	w1, w0, w1
  18:	cmp	w3, w1
  1c:	b.cs	68 <__truncsfhf2+0x68>  // b.hs, b.nlast
  20:	and	w1, w2, #0x1fff
  24:	ubfx	x0, x0, #13, #16
  28:	cmp	w1, #0x1, lsl #12
  2c:	b.ls	40 <__truncsfhf2+0x40>  // b.plast
  30:	mov	w3, #0x4001                	// #16385
  34:	add	w1, w0, w3
  38:	and	w1, w1, #0xffff
  3c:	b	58 <__truncsfhf2+0x58>
  40:	add	w1, w0, #0x4, lsl #12
  44:	and	w3, w1, #0xffff
  48:	mov	w1, #0x4001                	// #16385
  4c:	add	w1, w0, w1
  50:	and	w1, w1, #0xfffe
  54:	csel	w1, w1, w3, eq  // eq = none
  58:	lsr	w0, w2, #16
  5c:	and	w0, w0, #0x8000
  60:	orr	w0, w1, w0
  64:	ret
  68:	mov	w1, #0x7f800000            	// #2139095040
  6c:	cmp	w0, w1
  70:	b.ls	80 <__truncsfhf2+0x80>  // b.plast
  74:	ubfx	x0, x0, #13, #9
  78:	orr	w1, w0, #0x7e00
  7c:	b	58 <__truncsfhf2+0x58>
  80:	mov	w1, #0x477fffff            	// #1199570943
  84:	cmp	w0, w1
  88:	b.hi	ec <__truncsfhf2+0xec>  // b.pmore
  8c:	mov	w1, #0x71                  	// #113
  90:	lsr	w3, w0, #23
  94:	sub	w0, w1, w0, lsr #23
  98:	cmp	w0, #0x17
  9c:	b.gt	f4 <__truncsfhf2+0xf4>
  a0:	and	w1, w2, #0x7fffff
  a4:	sub	w3, w3, #0x51
  a8:	orr	w1, w1, #0x800000
  ac:	lsl	w3, w1, w3
  b0:	cmp	w3, #0x0
  b4:	cset	w3, ne  // ne = any
  b8:	lsr	w1, w1, w0
  bc:	orr	w0, w3, w1
  c0:	ubfx	x1, x1, #13, #16
  c4:	and	w0, w0, #0x1fff
  c8:	cmp	w0, #0x1, lsl #12
  cc:	b.ls	d8 <__truncsfhf2+0xd8>  // b.plast
  d0:	add	w1, w1, #0x1
  d4:	b	38 <__truncsfhf2+0x38>
  d8:	mov	w0, w1
  dc:	add	w1, w1, #0x1
  e0:	and	w1, w1, #0xfffe
  e4:	csel	w1, w1, w0, eq  // eq = none
  e8:	b	58 <__truncsfhf2+0x58>
  ec:	mov	w1, #0x7c00                	// #31744
  f0:	b	58 <__truncsfhf2+0x58>
  f4:	mov	w1, #0x0                   	// #0
  f8:	b	58 <__truncsfhf2+0x58>

00000000000000fc <__gnu_f2h_ieee>:
  fc:	b	0 <__truncsfhf2>

ucmpdi2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpdi2>:
   0:	lsr	x3, x0, #32
   4:	lsr	x2, x1, #32
   8:	cmp	w3, w2
   c:	b.cc	28 <__ucmpdi2+0x28>  // b.lo, b.ul, b.last
  10:	b.hi	30 <__ucmpdi2+0x30>  // b.pmore
  14:	cmp	w0, w1
  18:	cset	w2, hi  // hi = pmore
  1c:	add	w2, w2, #0x1
  20:	csel	w0, w2, wzr, cs  // cs = hs, nlast
  24:	ret
  28:	mov	w0, #0x0                   	// #0
  2c:	b	24 <__ucmpdi2+0x24>
  30:	mov	w0, #0x2                   	// #2
  34:	b	24 <__ucmpdi2+0x24>

ucmpti2.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__ucmpti2>:
   0:	cmp	x1, x3
   4:	b.cc	20 <__ucmpti2+0x20>  // b.lo, b.ul, b.last
   8:	b.hi	28 <__ucmpti2+0x28>  // b.pmore
   c:	cmp	x0, x2
  10:	cset	w1, hi  // hi = pmore
  14:	add	w1, w1, #0x1
  18:	csel	w0, w1, wzr, cs  // cs = hs, nlast
  1c:	ret
  20:	mov	w0, #0x0                   	// #0
  24:	b	1c <__ucmpti2+0x1c>
  28:	mov	w0, #0x2                   	// #2
  2c:	b	1c <__ucmpti2+0x1c>

udivdi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivdi3>:
   0:	mov	x2, #0x0                   	// #0
   4:	b	0 <__udivmoddi4>

udivmoddi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmoddi4>:
   0:	lsr	x11, x0, #32
   4:	lsr	x3, x1, #32
   8:	cmp	xzr, x0, lsr #32
   c:	b.ne	40 <__udivmoddi4+0x40>  // b.any
  10:	cbnz	x3, 30 <__udivmoddi4+0x30>
  14:	mov	w4, w0
  18:	cbz	x2, 28 <__udivmoddi4+0x28>
  1c:	udiv	w3, w0, w1
  20:	msub	w0, w3, w1, w0
  24:	str	x0, [x2]
  28:	udiv	w0, w4, w1
  2c:	ret
  30:	cbz	x2, b8 <__udivmoddi4+0xb8>
  34:	and	x0, x0, #0xffffffff
  38:	str	x0, [x2]
  3c:	b	b8 <__udivmoddi4+0xb8>
  40:	cbnz	w1, 158 <__udivmoddi4+0x158>
  44:	cbnz	x3, 4c <__udivmoddi4+0x4c>
  48:	brk	#0x3e8
  4c:	mov	w4, w0
  50:	cbnz	w0, 6c <__udivmoddi4+0x6c>
  54:	udiv	w0, w11, w3
  58:	cbz	x2, 2c <__udivmoddi4+0x2c>
  5c:	msub	w3, w0, w3, w11
  60:	lsl	x3, x3, #32
  64:	str	x3, [x2]
  68:	b	2c <__udivmoddi4+0x2c>
  6c:	sub	w5, w3, #0x1
  70:	tst	w5, w3
  74:	b.ne	a0 <__udivmoddi4+0xa0>  // b.any
  78:	cbz	x2, 90 <__udivmoddi4+0x90>
  7c:	mov	x8, #0x0                   	// #0
  80:	and	w5, w5, w11
  84:	bfxil	x8, x4, #0, #32
  88:	bfi	x8, x5, #32, #32
  8c:	str	x8, [x2]
  90:	rbit	w0, w3
  94:	clz	w0, w0
  98:	lsr	w0, w11, w0
  9c:	b	2c <__udivmoddi4+0x2c>
  a0:	clz	w3, w3
  a4:	clz	w5, w11
  a8:	sub	w3, w3, w5
  ac:	cmp	w3, #0x1e
  b0:	b.ls	c0 <__udivmoddi4+0xc0>  // b.plast
  b4:	cbnz	x2, 38 <__udivmoddi4+0x38>
  b8:	mov	x0, #0x0                   	// #0
  bc:	b	2c <__udivmoddi4+0x2c>
  c0:	add	w9, w3, #0x1
  c4:	mov	w8, #0x20                  	// #32
  c8:	sub	w7, w8, w9
  cc:	mov	x10, #0x0                   	// #0
  d0:	mov	x8, #0x0                   	// #0
  d4:	lsr	w4, w4, w9
  d8:	lsl	w0, w0, w7
  dc:	bfi	x10, x0, #32, #32
  e0:	lsr	w0, w11, w9
  e4:	bfi	x8, x0, #32, #32
  e8:	lsl	w7, w11, w7
  ec:	orr	w7, w7, w4
  f0:	bfxil	x8, x7, #0, #32
  f4:	lsr	x5, x10, #32
  f8:	mov	w3, w10
  fc:	mov	w0, #0x0                   	// #0
 100:	lsr	x6, x8, #32
 104:	mov	w4, w8
 108:	subs	w9, w9, #0x1
 10c:	extr	w6, w6, w8, #31
 110:	extr	w4, w4, w5, #31
 114:	bfi	x8, x6, #32, #32
 118:	extr	w5, w5, w3, #31
 11c:	orr	w3, w0, w3, lsl #1
 120:	bfxil	x8, x4, #0, #32
 124:	mvn	x4, x8
 128:	add	x4, x4, x1
 12c:	asr	x4, x4, #63
 130:	and	w0, w4, #0x1
 134:	and	x4, x4, x1
 138:	sub	x8, x8, x4
 13c:	b.ne	100 <__udivmoddi4+0x100>  // b.any
 140:	bfi	x10, x5, #32, #32
 144:	bfxil	x10, x3, #0, #32
 148:	orr	x0, x0, x10, lsl #1
 14c:	cbz	x2, 2c <__udivmoddi4+0x2c>
 150:	str	x8, [x2]
 154:	b	2c <__udivmoddi4+0x2c>
 158:	cbnz	x3, 250 <__udivmoddi4+0x250>
 15c:	sub	w3, w1, #0x1
 160:	tst	w3, w1
 164:	b.ne	1ac <__udivmoddi4+0x1ac>  // b.any
 168:	cbz	x2, 174 <__udivmoddi4+0x174>
 16c:	and	w3, w3, w0
 170:	str	x3, [x2]
 174:	cmp	w1, #0x1
 178:	b.eq	2c <__udivmoddi4+0x2c>  // b.none
 17c:	rbit	w1, w1
 180:	mov	x10, #0x0                   	// #0
 184:	clz	w1, w1
 188:	neg	w3, w1
 18c:	lsr	w2, w11, w1
 190:	bfi	x10, x2, #32, #32
 194:	lsr	w1, w0, w1
 198:	lsl	w3, w11, w3
 19c:	orr	w3, w3, w1
 1a0:	bfxil	x10, x3, #0, #32
 1a4:	mov	x0, x10
 1a8:	b	2c <__udivmoddi4+0x2c>
 1ac:	clz	w4, w1
 1b0:	clz	w3, w11
 1b4:	sub	w4, w4, w3
 1b8:	mov	w6, w0
 1bc:	add	w9, w4, #0x21
 1c0:	cmp	w9, #0x20
 1c4:	b.ne	1dc <__udivmoddi4+0x1dc>  // b.any
 1c8:	mov	x10, #0x0                   	// #0
 1cc:	mov	x8, #0x0                   	// #0
 1d0:	bfi	x10, x0, #32, #32
 1d4:	bfxil	x8, x11, #0, #32
 1d8:	b	f4 <__udivmoddi4+0xf4>
 1dc:	cmp	w9, #0x1f
 1e0:	b.hi	218 <__udivmoddi4+0x218>  // b.pmore
 1e4:	mov	w5, #0x20                  	// #32
 1e8:	sub	w5, w5, w9
 1ec:	mov	x10, #0x0                   	// #0
 1f0:	mov	x8, #0x0                   	// #0
 1f4:	lsl	w0, w0, w5
 1f8:	bfi	x10, x0, #32, #32
 1fc:	lsr	w0, w11, w9
 200:	bfi	x8, x0, #32, #32
 204:	lsl	w5, w11, w5
 208:	lsr	w6, w6, w9
 20c:	orr	w6, w5, w6
 210:	bfxil	x8, x6, #0, #32
 214:	b	f4 <__udivmoddi4+0xf4>
 218:	mov	w5, #0x40                  	// #64
 21c:	sub	w5, w5, w9
 220:	add	w4, w4, #0x1
 224:	mov	x10, #0x0                   	// #0
 228:	lsl	w0, w0, w5
 22c:	mov	x8, #0x0                   	// #0
 230:	lsr	w6, w6, w4
 234:	bfxil	x10, x0, #0, #32
 238:	lsl	w5, w11, w5
 23c:	orr	w5, w5, w6
 240:	lsr	w4, w11, w4
 244:	bfi	x10, x5, #32, #32
 248:	bfxil	x8, x4, #0, #32
 24c:	b	f4 <__udivmoddi4+0xf4>
 250:	clz	w3, w3
 254:	clz	w4, w11
 258:	sub	w3, w3, w4
 25c:	cmp	w3, #0x1f
 260:	b.hi	b4 <__udivmoddi4+0xb4>  // b.pmore
 264:	add	w9, w3, #0x1
 268:	mov	x10, #0x0                   	// #0
 26c:	b.eq	1cc <__udivmoddi4+0x1cc>  // b.none
 270:	mov	w3, #0x20                  	// #32
 274:	sub	w3, w3, w9
 278:	mov	x8, #0x0                   	// #0
 27c:	lsl	w4, w0, w3
 280:	bfi	x10, x4, #32, #32
 284:	lsr	w4, w11, w9
 288:	bfi	x8, x4, #32, #32
 28c:	lsl	w3, w11, w3
 290:	lsr	w0, w0, w9
 294:	orr	w3, w3, w0
 298:	bfxil	x8, x3, #0, #32
 29c:	b	f4 <__udivmoddi4+0xf4>

udivmodsi4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodsi4>:
   0:	stp	x29, x30, [sp, #-48]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w20, w1
  10:	mov	x19, x2
  14:	str	x21, [sp, #32]
  18:	mov	w21, w0
  1c:	bl	0 <__udivsi3>
  20:	msub	w1, w0, w20, w21
  24:	ldr	x21, [sp, #32]
  28:	str	w1, [x19]
  2c:	ldp	x19, x20, [sp, #16]
  30:	ldp	x29, x30, [sp], #48
  34:	ret

udivmodti4.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivmodti4>:
   0:	cbnz	x1, 34 <__udivmodti4+0x34>
   4:	cbnz	x3, 24 <__udivmodti4+0x24>
   8:	udiv	x1, x0, x2
   c:	cbz	x4, 18 <__udivmodti4+0x18>
  10:	msub	x0, x1, x2, x0
  14:	stp	x0, xzr, [x4]
  18:	mov	x0, x1
  1c:	mov	x1, #0x0                   	// #0
  20:	b	160 <__udivmodti4+0x160>
  24:	cbz	x4, 2c <__udivmodti4+0x2c>
  28:	stp	x0, xzr, [x4]
  2c:	mov	x0, #0x0                   	// #0
  30:	b	160 <__udivmodti4+0x160>
  34:	cbnz	x2, 120 <__udivmodti4+0x120>
  38:	cbnz	x3, 40 <__udivmodti4+0x40>
  3c:	brk	#0x3e8
  40:	cbnz	x0, 58 <__udivmodti4+0x58>
  44:	udiv	x0, x1, x3
  48:	cbz	x4, 1c <__udivmodti4+0x1c>
  4c:	msub	x1, x0, x3, x1
  50:	stp	xzr, x1, [x4]
  54:	b	1c <__udivmodti4+0x1c>
  58:	sub	x5, x3, #0x1
  5c:	tst	x5, x3
  60:	b.ne	80 <__udivmodti4+0x80>  // b.any
  64:	cbz	x4, 70 <__udivmodti4+0x70>
  68:	and	x5, x5, x1
  6c:	stp	x0, x5, [x4]
  70:	rbit	x0, x3
  74:	clz	x0, x0
  78:	lsr	x0, x1, x0
  7c:	b	1c <__udivmodti4+0x1c>
  80:	clz	x8, x3
  84:	clz	x5, x1
  88:	sub	w8, w8, w5
  8c:	cmp	w8, #0x3e
  90:	b.ls	a4 <__udivmodti4+0xa4>  // b.plast
  94:	cbz	x4, 9c <__udivmodti4+0x9c>
  98:	stp	x0, x1, [x4]
  9c:	mov	x0, #0x0                   	// #0
  a0:	b	1c <__udivmodti4+0x1c>
  a4:	add	w8, w8, #0x1
  a8:	mov	w7, #0x40                  	// #64
  ac:	sub	w7, w7, w8
  b0:	mov	x11, #0x0                   	// #0
  b4:	lsr	x10, x1, x8
  b8:	lsl	x9, x0, x7
  bc:	lsl	x7, x1, x7
  c0:	lsr	x0, x0, x8
  c4:	orr	x5, x7, x0
  c8:	mov	x0, #0x0                   	// #0
  cc:	extr	x10, x10, x5, #63
  d0:	mvn	x1, x10
  d4:	extr	x5, x5, x9, #63
  d8:	extr	x9, x9, x11, #63
  dc:	orr	x11, x0, x11, lsl #1
  e0:	mvn	x0, x5
  e4:	adds	x0, x0, x2
  e8:	adc	x1, x3, x1
  ec:	asr	x1, x1, #63
  f0:	and	x6, x1, x2
  f4:	and	w0, w1, #0x1
  f8:	subs	x5, x5, x6
  fc:	and	x1, x1, x3
 100:	sbc	x10, x10, x1
 104:	subs	w8, w8, #0x1
 108:	b.ne	cc <__udivmodti4+0xcc>  // b.any
 10c:	orr	x0, x0, x11, lsl #1
 110:	extr	x1, x9, x11, #63
 114:	cbz	x4, 160 <__udivmodti4+0x160>
 118:	stp	x5, x10, [x4]
 11c:	b	160 <__udivmodti4+0x160>
 120:	cbnz	x3, 1e4 <__udivmodti4+0x1e4>
 124:	sub	x5, x2, #0x1
 128:	tst	x5, x2
 12c:	b.ne	164 <__udivmodti4+0x164>  // b.any
 130:	cbz	x4, 13c <__udivmodti4+0x13c>
 134:	and	x5, x5, x0
 138:	stp	x5, xzr, [x4]
 13c:	cmp	x2, #0x1
 140:	b.eq	160 <__udivmodti4+0x160>  // b.none
 144:	rbit	x2, x2
 148:	clz	x2, x2
 14c:	neg	w4, w2
 150:	lsr	x0, x0, x2
 154:	lsl	x4, x1, x4
 158:	orr	x0, x4, x0
 15c:	lsr	x1, x1, x2
 160:	ret
 164:	clz	x5, x2
 168:	clz	x6, x1
 16c:	sub	w5, w5, w6
 170:	add	w8, w5, #0x41
 174:	cmp	w8, #0x40
 178:	b.ne	190 <__udivmodti4+0x190>  // b.any
 17c:	mov	x11, #0x0                   	// #0
 180:	mov	x9, x0
 184:	mov	x5, x1
 188:	mov	x10, #0x0                   	// #0
 18c:	b	c8 <__udivmodti4+0xc8>
 190:	cmp	w8, #0x3f
 194:	b.hi	1bc <__udivmodti4+0x1bc>  // b.pmore
 198:	mov	w6, #0x40                  	// #64
 19c:	sub	w6, w6, w8
 1a0:	mov	x11, #0x0                   	// #0
 1a4:	lsr	x10, x1, x8
 1a8:	lsl	x9, x0, x6
 1ac:	lsl	x5, x1, x6
 1b0:	lsr	x0, x0, x8
 1b4:	orr	x5, x5, x0
 1b8:	b	c8 <__udivmodti4+0xc8>
 1bc:	add	w5, w5, #0x1
 1c0:	mov	w9, #0x80                  	// #128
 1c4:	sub	w9, w9, w8
 1c8:	mov	x10, #0x0                   	// #0
 1cc:	lsl	x11, x0, x9
 1d0:	lsr	x0, x0, x5
 1d4:	lsl	x9, x1, x9
 1d8:	orr	x9, x9, x0
 1dc:	lsr	x5, x1, x5
 1e0:	b	c8 <__udivmodti4+0xc8>
 1e4:	clz	x5, x3
 1e8:	clz	x6, x1
 1ec:	sub	w5, w5, w6
 1f0:	cmp	w5, #0x3f
 1f4:	b.hi	94 <__udivmodti4+0x94>  // b.pmore
 1f8:	add	w8, w5, #0x1
 1fc:	mov	x11, #0x0                   	// #0
 200:	b.eq	180 <__udivmodti4+0x180>  // b.none
 204:	mov	w9, #0x40                  	// #64
 208:	sub	w9, w9, w8
 20c:	lsr	x10, x1, x8
 210:	lsr	x5, x0, x8
 214:	lsl	x1, x1, x9
 218:	orr	x5, x1, x5
 21c:	lsl	x9, x0, x9
 220:	b	c8 <__udivmodti4+0xc8>

udivsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivsi3>:
   0:	cbz	w1, 60 <__udivsi3+0x60>
   4:	cbz	w0, 5c <__udivsi3+0x5c>
   8:	clz	w2, w1
   c:	clz	w3, w0
  10:	sub	w2, w2, w3
  14:	cmp	w2, #0x1f
  18:	b.hi	60 <__udivsi3+0x60>  // b.pmore
  1c:	b.eq	5c <__udivsi3+0x5c>  // b.none
  20:	add	w2, w2, #0x1
  24:	sub	w6, w1, #0x1
  28:	neg	w3, w2
  2c:	lsr	w4, w0, w2
  30:	lsl	w3, w0, w3
  34:	mov	w0, #0x0                   	// #0
  38:	subs	w2, w2, #0x1
  3c:	extr	w4, w4, w3, #31
  40:	sub	w5, w6, w4
  44:	orr	w3, w0, w3, lsl #1
  48:	lsr	w0, w5, #31
  4c:	and	w5, w1, w5, asr #31
  50:	sub	w4, w4, w5
  54:	b.ne	38 <__udivsi3+0x38>  // b.any
  58:	orr	w0, w0, w3, lsl #1
  5c:	ret
  60:	mov	w0, #0x0                   	// #0
  64:	b	5c <__udivsi3+0x5c>

udivti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__udivti3>:
   0:	mov	x4, #0x0                   	// #0
   4:	b	0 <__udivmodti4>

umoddi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umoddi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x2, sp, #0x18
   c:	bl	0 <__udivmoddi4>
  10:	ldr	x0, [sp, #24]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

umodsi3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodsi3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	stp	x19, x20, [sp, #16]
   c:	mov	w19, w1
  10:	mov	w20, w0
  14:	bl	0 <__udivsi3>
  18:	msub	w0, w0, w19, w20
  1c:	ldp	x19, x20, [sp, #16]
  20:	ldp	x29, x30, [sp], #32
  24:	ret

umodti3.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__umodti3>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	add	x4, sp, #0x10
   c:	bl	0 <__udivmodti4>
  10:	ldp	x0, x1, [sp, #16]
  14:	ldp	x29, x30, [sp], #32
  18:	ret

emutls.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <emutls_init>:
   0:	stp	x29, x30, [sp, #-16]!
   4:	adrp	x1, 0 <emutls_init>
   8:	adrp	x0, 0 <emutls_init>
   c:	mov	x29, sp
  10:	add	x1, x1, #0x0
  14:	add	x0, x0, #0x0
  18:	bl	0 <pthread_key_create>
  1c:	cbz	w0, 24 <emutls_init+0x24>
  20:	bl	0 <abort>
  24:	ldp	x29, x30, [sp], #16
  28:	ret

000000000000002c <emutls_key_destructor>:
  2c:	stp	x29, x30, [sp, #-48]!
  30:	mov	x29, sp
  34:	stp	x19, x20, [sp, #16]
  38:	mov	x19, x0
  3c:	ldr	x20, [x0]
  40:	str	x21, [sp, #32]
  44:	cbnz	x20, 6c <emutls_key_destructor+0x40>
  48:	add	x21, x0, #0x10
  4c:	ldr	x0, [x19, #8]
  50:	cmp	x20, x0
  54:	b.cc	90 <emutls_key_destructor+0x64>  // b.lo, b.ul, b.last
  58:	mov	x0, x19
  5c:	ldp	x19, x20, [sp, #16]
  60:	ldr	x21, [sp, #32]
  64:	ldp	x29, x30, [sp], #48
  68:	b	0 <free>
  6c:	mov	x1, x0
  70:	adrp	x0, 0 <emutls_init>
  74:	sub	x20, x20, #0x1
  78:	ldr	w0, [x0]
  7c:	ldr	x21, [sp, #32]
  80:	str	x20, [x19]
  84:	ldp	x19, x20, [sp, #16]
  88:	ldp	x29, x30, [sp], #48
  8c:	b	0 <pthread_setspecific>
  90:	ldr	x0, [x21, x20, lsl #3]
  94:	cbz	x0, a0 <emutls_key_destructor+0x74>
  98:	ldur	x0, [x0, #-8]
  9c:	bl	0 <free>
  a0:	add	x20, x20, #0x1
  a4:	b	4c <emutls_key_destructor+0x20>

00000000000000a8 <__emutls_get_address>:
  a8:	stp	x29, x30, [sp, #-64]!
  ac:	mov	x29, sp
  b0:	stp	x19, x20, [sp, #16]
  b4:	stp	x21, x22, [sp, #32]
  b8:	mov	x21, x0
  bc:	stp	x23, x24, [sp, #48]
  c0:	add	x24, x0, #0x10
  c4:	ldar	x20, [x24]
  c8:	adrp	x22, 0 <emutls_init>
  cc:	cbnz	x20, 110 <__emutls_get_address+0x68>
  d0:	adrp	x1, 0 <emutls_init>
  d4:	add	x1, x1, #0x0
  d8:	add	x23, x22, #0x0
  dc:	add	x0, x23, #0x4
  e0:	bl	0 <pthread_once>
  e4:	add	x0, x23, #0x8
  e8:	bl	0 <pthread_mutex_lock>
  ec:	ldr	x20, [x21, #16]
  f0:	cbnz	x20, 104 <__emutls_get_address+0x5c>
  f4:	ldr	x19, [x23, #56]
  f8:	add	x20, x19, #0x1
  fc:	str	x20, [x23, #56]
 100:	stlr	x20, [x24]
 104:	add	x0, x22, #0x0
 108:	add	x0, x0, #0x8
 10c:	bl	0 <pthread_mutex_unlock>
 110:	ldr	w0, [x22]
 114:	bl	0 <pthread_getspecific>
 118:	mov	x19, x0
 11c:	cbnz	x0, 188 <__emutls_get_address+0xe0>
 120:	add	x0, x20, #0x11
 124:	and	x0, x0, #0xfffffffffffffff0
 128:	sub	x23, x0, #0x2
 12c:	lsl	x0, x0, #3
 130:	bl	0 <malloc>
 134:	mov	x19, x0
 138:	cbz	x0, 184 <__emutls_get_address+0xdc>
 13c:	lsl	x2, x23, #3
 140:	add	x0, x0, #0x10
 144:	mov	w1, #0x0                   	// #0
 148:	bl	0 <memset>
 14c:	stp	xzr, x23, [x19]
 150:	ldr	w0, [x22]
 154:	mov	x1, x19
 158:	bl	0 <pthread_setspecific>
 15c:	add	x19, x19, x20, lsl #3
 160:	ldr	x0, [x19, #8]
 164:	cbnz	x0, 20c <__emutls_get_address+0x164>
 168:	ldp	x22, x20, [x21]
 16c:	mov	x0, #0x8                   	// #8
 170:	cmp	x20, #0x8
 174:	csel	x20, x20, x0, cs  // cs = hs, nlast
 178:	sub	x0, x20, #0x1
 17c:	tst	x0, x20
 180:	b.eq	1d0 <__emutls_get_address+0x128>  // b.none
 184:	bl	0 <abort>
 188:	ldr	x23, [x0, #8]
 18c:	cmp	x23, x20
 190:	b.cs	15c <__emutls_get_address+0xb4>  // b.hs, b.nlast
 194:	add	x1, x20, #0x11
 198:	and	x1, x1, #0xfffffffffffffff0
 19c:	sub	x24, x1, #0x2
 1a0:	lsl	x1, x1, #3
 1a4:	bl	0 <realloc>
 1a8:	mov	x19, x0
 1ac:	cbz	x0, 184 <__emutls_get_address+0xdc>
 1b0:	sub	x2, x24, x23
 1b4:	add	x0, x0, #0x10
 1b8:	add	x0, x0, x23, lsl #3
 1bc:	mov	w1, #0x0                   	// #0
 1c0:	lsl	x2, x2, #3
 1c4:	bl	0 <memset>
 1c8:	str	x24, [x19, #8]
 1cc:	b	150 <__emutls_get_address+0xa8>
 1d0:	add	x0, x22, #0x7
 1d4:	add	x0, x0, x20
 1d8:	bl	0 <malloc>
 1dc:	cbz	x0, 184 <__emutls_get_address+0xdc>
 1e0:	add	x1, x20, #0x7
 1e4:	neg	x20, x20
 1e8:	add	x1, x0, x1
 1ec:	mov	x2, x22
 1f0:	and	x20, x1, x20
 1f4:	stur	x0, [x20, #-8]
 1f8:	ldr	x1, [x21, #24]
 1fc:	cbz	x1, 224 <__emutls_get_address+0x17c>
 200:	mov	x0, x20
 204:	bl	0 <memcpy>
 208:	str	x20, [x19, #8]
 20c:	ldr	x0, [x19, #8]
 210:	ldp	x19, x20, [sp, #16]
 214:	ldp	x21, x22, [sp, #32]
 218:	ldp	x23, x24, [sp, #48]
 21c:	ldp	x29, x30, [sp], #64
 220:	ret
 224:	mov	x0, x20
 228:	mov	w1, #0x0                   	// #0
 22c:	bl	0 <memset>
 230:	b	208 <__emutls_get_address+0x160>

enable_execute_stack.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__enable_execute_stack>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x29, sp
   8:	str	x19, [sp, #16]
   c:	mov	x19, x0
  10:	mov	w0, #0x1e                  	// #30
  14:	bl	0 <sysconf>
  18:	neg	x2, x0
  1c:	mov	x1, x0
  20:	and	x0, x2, x19
  24:	add	x19, x19, #0x30
  28:	add	x1, x19, x1
  2c:	ldr	x19, [sp, #16]
  30:	and	x1, x1, x2
  34:	ldp	x29, x30, [sp], #32
  38:	sub	x1, x1, x0
  3c:	mov	w2, #0x7                   	// #7
  40:	b	0 <mprotect>

eprintf.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__eprintf>:
   0:	stp	x29, x30, [sp, #-32]!
   4:	mov	x4, x3
   8:	mov	x3, x2
   c:	mov	x29, sp
  10:	str	x19, [sp, #16]
  14:	adrp	x19, 0 <stderr>
  18:	mov	x2, x1
  1c:	mov	x1, x0
  20:	ldr	x19, [x19]
  24:	ldr	x0, [x19]
  28:	bl	0 <fprintf>
  2c:	ldr	x0, [x19]
  30:	bl	0 <fflush>
  34:	adrp	x2, 0 <__eprintf>
  38:	adrp	x0, 0 <__eprintf>
  3c:	add	x2, x2, #0x0
  40:	add	x0, x0, #0x0
  44:	mov	w1, #0x1a                  	// #26
  48:	bl	0 <__compilerrt_abort_impl>

gcc_personality_v0.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <readULEB128>:
   0:	mov	x2, x0
   4:	mov	x4, #0x0                   	// #0
   8:	mov	x0, #0x0                   	// #0
   c:	ldr	x3, [x2]
  10:	ldrb	w5, [x3], #1
  14:	and	w1, w5, #0x7f
  18:	lsl	w1, w1, w4
  1c:	add	x4, x4, #0x7
  20:	sxtw	x1, w1
  24:	orr	x0, x0, x1
  28:	tbnz	w5, #7, 10 <readULEB128+0x10>
  2c:	str	x3, [x2]
  30:	ret

0000000000000034 <readEncodedPointer>:
  34:	stp	x29, x30, [sp, #-32]!
  38:	and	w7, w1, #0xff
  3c:	cmp	w7, #0xff
  40:	mov	x29, sp
  44:	ldr	x2, [x0]
  48:	str	x2, [sp, #24]
  4c:	b.eq	104 <readEncodedPointer+0xd0>  // b.none
  50:	and	w1, w7, #0xf
  54:	cmp	w1, #0xc
  58:	b.hi	dc <readEncodedPointer+0xa8>  // b.pmore
  5c:	mov	x6, x0
  60:	adrp	x0, 0 <readULEB128>
  64:	add	x0, x0, #0x0
  68:	ldrb	w0, [x0, w1, uxtw]
  6c:	adr	x1, 78 <readEncodedPointer+0x44>
  70:	add	x0, x1, w0, sxtb #2
  74:	br	x0
  78:	add	x0, sp, #0x18
  7c:	bl	0 <readULEB128>
  80:	ands	w1, w7, #0x70
  84:	b.eq	98 <readEncodedPointer+0x64>  // b.none
  88:	cmp	w1, #0x10
  8c:	b.ne	f4 <readEncodedPointer+0xc0>  // b.any
  90:	ldr	x1, [x6]
  94:	add	x0, x1, x0
  98:	tbz	w7, #7, a0 <readEncodedPointer+0x6c>
  9c:	ldr	x0, [x0]
  a0:	ldr	x1, [sp, #24]
  a4:	str	x1, [x6]
  a8:	ldp	x29, x30, [sp], #32
  ac:	ret
  b0:	ldrh	w0, [x2], #2
  b4:	str	x2, [sp, #24]
  b8:	b	80 <readEncodedPointer+0x4c>
  bc:	ldr	w0, [x2], #4
  c0:	b	b4 <readEncodedPointer+0x80>
  c4:	ldrsh	x0, [x2], #2
  c8:	b	b4 <readEncodedPointer+0x80>
  cc:	ldrsw	x0, [x2], #4
  d0:	b	b4 <readEncodedPointer+0x80>
  d4:	ldr	x0, [x2], #8
  d8:	b	b4 <readEncodedPointer+0x80>
  dc:	adrp	x2, 0 <readULEB128>
  e0:	add	x2, x2, #0x0
  e4:	mov	w1, #0x68                  	// #104
  e8:	adrp	x0, 0 <readULEB128>
  ec:	add	x0, x0, #0x0
  f0:	bl	0 <__compilerrt_abort_impl>
  f4:	adrp	x2, 0 <readULEB128>
  f8:	mov	w1, #0x7a                  	// #122
  fc:	add	x2, x2, #0x0
 100:	b	e8 <readEncodedPointer+0xb4>
 104:	mov	x0, #0x0                   	// #0
 108:	b	a8 <readEncodedPointer+0x74>

000000000000010c <__gcc_personality_v0>:
 10c:	tbz	w1, #0, 118 <__gcc_personality_v0+0xc>
 110:	mov	w0, #0x8                   	// #8
 114:	ret
 118:	stp	x29, x30, [sp, #-112]!
 11c:	mov	x0, x4
 120:	mov	x29, sp
 124:	stp	x19, x20, [sp, #16]
 128:	stp	x21, x22, [sp, #32]
 12c:	mov	x21, x4
 130:	stp	x23, x24, [sp, #48]
 134:	mov	x23, x3
 138:	stp	x25, x26, [sp, #64]
 13c:	stp	x27, x28, [sp, #80]
 140:	bl	0 <_Unwind_GetLanguageSpecificData>
 144:	str	x0, [sp, #96]
 148:	cbz	x0, 1e4 <__gcc_personality_v0+0xd8>
 14c:	mov	x0, x21
 150:	bl	0 <_Unwind_GetIP>
 154:	mov	x19, x0
 158:	mov	x0, x21
 15c:	sub	x19, x19, #0x1
 160:	bl	0 <_Unwind_GetRegionStart>
 164:	mov	x22, x0
 168:	sub	x19, x19, x0
 16c:	ldr	x0, [sp, #96]
 170:	add	x20, sp, #0x60
 174:	add	x1, x0, #0x1
 178:	str	x1, [sp, #96]
 17c:	ldrb	w1, [x0]
 180:	cmp	w1, #0xff
 184:	b.eq	190 <__gcc_personality_v0+0x84>  // b.none
 188:	mov	x0, x20
 18c:	bl	34 <readEncodedPointer>
 190:	ldr	x0, [sp, #96]
 194:	add	x1, x0, #0x1
 198:	str	x1, [sp, #96]
 19c:	ldrb	w0, [x0]
 1a0:	cmp	w0, #0xff
 1a4:	b.eq	1b0 <__gcc_personality_v0+0xa4>  // b.none
 1a8:	mov	x0, x20
 1ac:	bl	0 <readULEB128>
 1b0:	ldr	x0, [sp, #96]
 1b4:	add	x24, sp, #0x68
 1b8:	add	x1, x0, #0x1
 1bc:	str	x1, [sp, #96]
 1c0:	ldrb	w26, [x0]
 1c4:	mov	x0, x20
 1c8:	bl	0 <readULEB128>
 1cc:	ldr	x1, [sp, #96]
 1d0:	str	x1, [sp, #104]
 1d4:	add	x27, x1, w0, uxtw
 1d8:	ldr	x0, [sp, #104]
 1dc:	cmp	x0, x27
 1e0:	b.cc	204 <__gcc_personality_v0+0xf8>  // b.lo, b.ul, b.last
 1e4:	mov	w0, #0x8                   	// #8
 1e8:	ldp	x19, x20, [sp, #16]
 1ec:	ldp	x21, x22, [sp, #32]
 1f0:	ldp	x23, x24, [sp, #48]
 1f4:	ldp	x25, x26, [sp, #64]
 1f8:	ldp	x27, x28, [sp, #80]
 1fc:	ldp	x29, x30, [sp], #112
 200:	ret
 204:	mov	w1, w26
 208:	mov	x0, x24
 20c:	bl	34 <readEncodedPointer>
 210:	mov	x20, x0
 214:	mov	w1, w26
 218:	mov	x0, x24
 21c:	bl	34 <readEncodedPointer>
 220:	mov	x28, x0
 224:	mov	w1, w26
 228:	mov	x0, x24
 22c:	bl	34 <readEncodedPointer>
 230:	mov	x25, x0
 234:	mov	x0, x24
 238:	bl	0 <readULEB128>
 23c:	cbz	x25, 1d8 <__gcc_personality_v0+0xcc>
 240:	cmp	x19, x20
 244:	b.cc	1d8 <__gcc_personality_v0+0xcc>  // b.lo, b.ul, b.last
 248:	add	x20, x20, x28
 24c:	cmp	x20, x19
 250:	b.ls	1d8 <__gcc_personality_v0+0xcc>  // b.plast
 254:	mov	x2, x23
 258:	mov	x0, x21
 25c:	mov	w1, #0x0                   	// #0
 260:	bl	0 <_Unwind_SetGR>
 264:	mov	x2, #0x0                   	// #0
 268:	mov	x0, x21
 26c:	mov	w1, #0x1                   	// #1
 270:	bl	0 <_Unwind_SetGR>
 274:	mov	x0, x21
 278:	add	x1, x22, x25
 27c:	bl	0 <_Unwind_SetIP>
 280:	mov	w0, #0x7                   	// #7
 284:	b	1e8 <__gcc_personality_v0+0xdc>

clear_cache.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__clear_cache>:
   0:	adrp	x2, 0 <__clear_cache>
   4:	ldr	x3, [x2]
   8:	cbnz	x3, 14 <__clear_cache+0x14>
   c:	mrs	x3, ctr_el0
  10:	str	x3, [x2]
  14:	ldr	x4, [x2]
  18:	tbnz	w4, #28, 3c <__clear_cache+0x3c>
  1c:	ubfx	w3, w4, #16, #4
  20:	mov	w4, #0x4                   	// #4
  24:	lsl	w4, w4, w3
  28:	sxtw	x4, w4
  2c:	neg	x3, x4
  30:	and	x3, x0, x3
  34:	cmp	x3, x1
  38:	b.cc	70 <__clear_cache+0x70>  // b.lo, b.ul, b.last
  3c:	dsb	ish
  40:	ldr	x2, [x2]
  44:	tbnz	w2, #29, 68 <__clear_cache+0x68>
  48:	and	w3, w2, #0xf
  4c:	mov	w2, #0x4                   	// #4
  50:	lsl	w2, w2, w3
  54:	sxtw	x2, w2
  58:	neg	x3, x2
  5c:	and	x0, x0, x3
  60:	cmp	x0, x1
  64:	b.cc	7c <__clear_cache+0x7c>  // b.lo, b.ul, b.last
  68:	isb
  6c:	ret
  70:	dc	cvau, x3
  74:	add	x3, x3, x4
  78:	b	34 <__clear_cache+0x34>
  7c:	ic	ivau, x0
  80:	add	x0, x0, x2
  84:	b	60 <__clear_cache+0x60>

fp_mode.c.o:     file format elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <__fe_getround>:
   0:	mrs	x0, fpcr
   4:	ubfx	x0, x0, #22, #2
   8:	sub	x0, x0, #0x1
   c:	cmp	x0, #0x2
  10:	b.hi	24 <__fe_getround+0x24>  // b.pmore
  14:	adrp	x1, 0 <__fe_getround>
  18:	add	x1, x1, #0x0
  1c:	ldrb	w0, [x1, x0]
  20:	ret
  24:	mov	w0, #0x0                   	// #0
  28:	b	20 <__fe_getround+0x20>

000000000000002c <__fe_raise_inexact>:
  2c:	mrs	x0, fpsr
  30:	orr	x0, x0, #0x10
  34:	msr	fpsr, x0
  38:	mov	w0, #0x0                   	// #0
  3c:	ret
