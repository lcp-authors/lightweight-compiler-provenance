
/home/anony/Documents/anonymous--anonymous/pizzolotto-binaries//libgmp.so.10.4.0_clang_-O2:     file format elf64-littleaarch64


Disassembly of section .init:

000000000000be90 <.init>:
    be90:	stp	x29, x30, [sp, #-16]!
    be94:	mov	x29, sp
    be98:	bl	d4e0 <__gmpn_cnd_add_n@plt+0x10>
    be9c:	ldp	x29, x30, [sp], #16
    bea0:	ret

Disassembly of section .plt:

000000000000beb0 <memcpy@plt-0x20>:
    beb0:	stp	x16, x30, [sp, #-16]!
    beb4:	adrp	x16, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    beb8:	ldr	x17, [x16, #4088]
    bebc:	add	x16, x16, #0xff8
    bec0:	br	x17
    bec4:	nop
    bec8:	nop
    becc:	nop

000000000000bed0 <memcpy@plt>:
    bed0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bed4:	ldr	x17, [x16]
    bed8:	add	x16, x16, #0x0
    bedc:	br	x17

000000000000bee0 <__gmpz_tdiv_r_2exp@plt>:
    bee0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bee4:	ldr	x17, [x16, #8]
    bee8:	add	x16, x16, #0x8
    beec:	br	x17

000000000000bef0 <__gmp_tmp_reentrant_free@plt>:
    bef0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bef4:	ldr	x17, [x16, #16]
    bef8:	add	x16, x16, #0x10
    befc:	br	x17

000000000000bf00 <__gmpn_tdiv_qr@plt>:
    bf00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf04:	ldr	x17, [x16, #24]
    bf08:	add	x16, x16, #0x18
    bf0c:	br	x17

000000000000bf10 <__gmpq_cmp_ui@plt>:
    bf10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf14:	ldr	x17, [x16, #32]
    bf18:	add	x16, x16, #0x20
    bf1c:	br	x17

000000000000bf20 <__gmpz_scan1@plt>:
    bf20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf24:	ldr	x17, [x16, #40]
    bf28:	add	x16, x16, #0x28
    bf2c:	br	x17

000000000000bf30 <__gmp_randinit_mt_noseed@plt>:
    bf30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf34:	ldr	x17, [x16, #48]
    bf38:	add	x16, x16, #0x30
    bf3c:	br	x17

000000000000bf40 <__gmpn_get_d@plt>:
    bf40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf44:	ldr	x17, [x16, #56]
    bf48:	add	x16, x16, #0x38
    bf4c:	br	x17

000000000000bf50 <__gmpn_sqrmod_bnm1@plt>:
    bf50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf54:	ldr	x17, [x16, #64]
    bf58:	add	x16, x16, #0x40
    bf5c:	br	x17

000000000000bf60 <strlen@plt>:
    bf60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf64:	ldr	x17, [x16, #72]
    bf68:	add	x16, x16, #0x48
    bf6c:	br	x17

000000000000bf70 <__gmpf_add@plt>:
    bf70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf74:	ldr	x17, [x16, #80]
    bf78:	add	x16, x16, #0x50
    bf7c:	br	x17

000000000000bf80 <__gmpz_init_set@plt>:
    bf80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf84:	ldr	x17, [x16, #88]
    bf88:	add	x16, x16, #0x58
    bf8c:	br	x17

000000000000bf90 <__gmpn_gcd_1@plt>:
    bf90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bf94:	ldr	x17, [x16, #96]
    bf98:	add	x16, x16, #0x60
    bf9c:	br	x17

000000000000bfa0 <__gmpz_tdiv_ui@plt>:
    bfa0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bfa4:	ldr	x17, [x16, #104]
    bfa8:	add	x16, x16, #0x68
    bfac:	br	x17

000000000000bfb0 <__gmpn_toom_interpolate_12pts@plt>:
    bfb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bfb4:	ldr	x17, [x16, #112]
    bfb8:	add	x16, x16, #0x70
    bfbc:	br	x17

000000000000bfc0 <raise@plt>:
    bfc0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bfc4:	ldr	x17, [x16, #120]
    bfc8:	add	x16, x16, #0x78
    bfcc:	br	x17

000000000000bfd0 <__gmp_divide_by_zero@plt>:
    bfd0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bfd4:	ldr	x17, [x16, #128]
    bfd8:	add	x16, x16, #0x80
    bfdc:	br	x17

000000000000bfe0 <__gmpq_set_str@plt>:
    bfe0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bfe4:	ldr	x17, [x16, #136]
    bfe8:	add	x16, x16, #0x88
    bfec:	br	x17

000000000000bff0 <__gmpz_tdiv_qr@plt>:
    bff0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    bff4:	ldr	x17, [x16, #144]
    bff8:	add	x16, x16, #0x90
    bffc:	br	x17

000000000000c000 <__gmpn_copyd@plt>:
    c000:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c004:	ldr	x17, [x16, #152]
    c008:	add	x16, x16, #0x98
    c00c:	br	x17

000000000000c010 <__gmpn_matrix22_mul@plt>:
    c010:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c014:	ldr	x17, [x16, #160]
    c018:	add	x16, x16, #0xa0
    c01c:	br	x17

000000000000c020 <__gmpn_cnd_sub_n@plt>:
    c020:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c024:	ldr	x17, [x16, #168]
    c028:	add	x16, x16, #0xa8
    c02c:	br	x17

000000000000c030 <__gmpn_gcd_22@plt>:
    c030:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c034:	ldr	x17, [x16, #176]
    c038:	add	x16, x16, #0xb0
    c03c:	br	x17

000000000000c040 <__gmpz_tdiv_q@plt>:
    c040:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c044:	ldr	x17, [x16, #184]
    c048:	add	x16, x16, #0xb8
    c04c:	br	x17

000000000000c050 <__gmpn_toom2_sqr@plt>:
    c050:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c054:	ldr	x17, [x16, #192]
    c058:	add	x16, x16, #0xc0
    c05c:	br	x17

000000000000c060 <__gmpn_andn_n@plt>:
    c060:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c064:	ldr	x17, [x16, #200]
    c068:	add	x16, x16, #0xc8
    c06c:	br	x17

000000000000c070 <__gmpz_jacobi@plt>:
    c070:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c074:	ldr	x17, [x16, #208]
    c078:	add	x16, x16, #0xd0
    c07c:	br	x17

000000000000c080 <__gmpz_realloc@plt>:
    c080:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c084:	ldr	x17, [x16, #216]
    c088:	add	x16, x16, #0xd8
    c08c:	br	x17

000000000000c090 <__gmpn_set_str@plt>:
    c090:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c094:	ldr	x17, [x16, #224]
    c098:	add	x16, x16, #0xe0
    c09c:	br	x17

000000000000c0a0 <__gmpn_toom33_mul@plt>:
    c0a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0a4:	ldr	x17, [x16, #232]
    c0a8:	add	x16, x16, #0xe8
    c0ac:	br	x17

000000000000c0b0 <__gmpn_sqrlo_basecase@plt>:
    c0b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0b4:	ldr	x17, [x16, #240]
    c0b8:	add	x16, x16, #0xf0
    c0bc:	br	x17

000000000000c0c0 <__gmpz_gcdext@plt>:
    c0c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0c4:	ldr	x17, [x16, #248]
    c0c8:	add	x16, x16, #0xf8
    c0cc:	br	x17

000000000000c0d0 <__gmpz_set_str@plt>:
    c0d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0d4:	ldr	x17, [x16, #256]
    c0d8:	add	x16, x16, #0x100
    c0dc:	br	x17

000000000000c0e0 <__gmpn_mu_divappr_q_itch@plt>:
    c0e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0e4:	ldr	x17, [x16, #264]
    c0e8:	add	x16, x16, #0x108
    c0ec:	br	x17

000000000000c0f0 <__gmp_doscan@plt>:
    c0f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c0f4:	ldr	x17, [x16, #272]
    c0f8:	add	x16, x16, #0x110
    c0fc:	br	x17

000000000000c100 <__gmpz_cmpabs_ui@plt>:
    c100:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c104:	ldr	x17, [x16, #280]
    c108:	add	x16, x16, #0x118
    c10c:	br	x17

000000000000c110 <__gmpn_bc_set_str@plt>:
    c110:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c114:	ldr	x17, [x16, #288]
    c118:	add	x16, x16, #0x120
    c11c:	br	x17

000000000000c120 <__gmpz_sub_ui@plt>:
    c120:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c124:	ldr	x17, [x16, #296]
    c128:	add	x16, x16, #0x128
    c12c:	br	x17

000000000000c130 <__gmpq_get_str@plt>:
    c130:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c134:	ldr	x17, [x16, #304]
    c138:	add	x16, x16, #0x130
    c13c:	br	x17

000000000000c140 <__gmpn_sec_div_r@plt>:
    c140:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c144:	ldr	x17, [x16, #312]
    c148:	add	x16, x16, #0x138
    c14c:	br	x17

000000000000c150 <__gmpf_set@plt>:
    c150:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c154:	ldr	x17, [x16, #320]
    c158:	add	x16, x16, #0x140
    c15c:	br	x17

000000000000c160 <__gmpn_sublsh2_n@plt>:
    c160:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c164:	ldr	x17, [x16, #328]
    c168:	add	x16, x16, #0x148
    c16c:	br	x17

000000000000c170 <__gmpz_set_ui@plt>:
    c170:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c174:	ldr	x17, [x16, #336]
    c178:	add	x16, x16, #0x150
    c17c:	br	x17

000000000000c180 <__gmpn_lshift@plt>:
    c180:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c184:	ldr	x17, [x16, #344]
    c188:	add	x16, x16, #0x158
    c18c:	br	x17

000000000000c190 <__gmpn_sqr_basecase@plt>:
    c190:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c194:	ldr	x17, [x16, #352]
    c198:	add	x16, x16, #0x160
    c19c:	br	x17

000000000000c1a0 <__gmpn_rshift@plt>:
    c1a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1a4:	ldr	x17, [x16, #360]
    c1a8:	add	x16, x16, #0x168
    c1ac:	br	x17

000000000000c1b0 <__gmp_invalid_operation@plt>:
    c1b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1b4:	ldr	x17, [x16, #368]
    c1b8:	add	x16, x16, #0x170
    c1bc:	br	x17

000000000000c1c0 <__gmpf_set_str@plt>:
    c1c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1c4:	ldr	x17, [x16, #376]
    c1c8:	add	x16, x16, #0x178
    c1cc:	br	x17

000000000000c1d0 <__cxa_finalize@plt>:
    c1d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1d4:	ldr	x17, [x16, #384]
    c1d8:	add	x16, x16, #0x180
    c1dc:	br	x17

000000000000c1e0 <putc@plt>:
    c1e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1e4:	ldr	x17, [x16, #392]
    c1e8:	add	x16, x16, #0x188
    c1ec:	br	x17

000000000000c1f0 <__gmpf_sub_ui@plt>:
    c1f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c1f4:	ldr	x17, [x16, #400]
    c1f8:	add	x16, x16, #0x190
    c1fc:	br	x17

000000000000c200 <__gmpn_divrem_2@plt>:
    c200:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c204:	ldr	x17, [x16, #408]
    c208:	add	x16, x16, #0x198
    c20c:	br	x17

000000000000c210 <fputc@plt>:
    c210:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c214:	ldr	x17, [x16, #416]
    c218:	add	x16, x16, #0x1a0
    c21c:	br	x17

000000000000c220 <__gmpn_toom4_sqr@plt>:
    c220:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c224:	ldr	x17, [x16, #424]
    c228:	add	x16, x16, #0x1a8
    c22c:	br	x17

000000000000c230 <__gmpn_sec_powm_itch@plt>:
    c230:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c234:	ldr	x17, [x16, #432]
    c238:	add	x16, x16, #0x1b0
    c23c:	br	x17

000000000000c240 <__gmpn_perfect_power_p@plt>:
    c240:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c244:	ldr	x17, [x16, #440]
    c248:	add	x16, x16, #0x1b8
    c24c:	br	x17

000000000000c250 <__gmpn_mod_1_1p@plt>:
    c250:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c254:	ldr	x17, [x16, #448]
    c258:	add	x16, x16, #0x1c0
    c25c:	br	x17

000000000000c260 <__gmpz_sub@plt>:
    c260:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c264:	ldr	x17, [x16, #456]
    c268:	add	x16, x16, #0x1c8
    c26c:	br	x17

000000000000c270 <__gmpn_and_n@plt>:
    c270:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c274:	ldr	x17, [x16, #464]
    c278:	add	x16, x16, #0x1d0
    c27c:	br	x17

000000000000c280 <__gmpn_toom_eval_dgr3_pm1@plt>:
    c280:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c284:	ldr	x17, [x16, #472]
    c288:	add	x16, x16, #0x1d8
    c28c:	br	x17

000000000000c290 <__gmpn_com@plt>:
    c290:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c294:	ldr	x17, [x16, #480]
    c298:	add	x16, x16, #0x1e0
    c29c:	br	x17

000000000000c2a0 <__gmpn_rootrem@plt>:
    c2a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2a4:	ldr	x17, [x16, #488]
    c2a8:	add	x16, x16, #0x1e8
    c2ac:	br	x17

000000000000c2b0 <__gmpn_hgcd_step@plt>:
    c2b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2b4:	ldr	x17, [x16, #496]
    c2b8:	add	x16, x16, #0x1f0
    c2bc:	br	x17

000000000000c2c0 <__gmpn_bdiv_dbm1c@plt>:
    c2c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2c4:	ldr	x17, [x16, #504]
    c2c8:	add	x16, x16, #0x1f8
    c2cc:	br	x17

000000000000c2d0 <__gmpn_sub_n@plt>:
    c2d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2d4:	ldr	x17, [x16, #512]
    c2d8:	add	x16, x16, #0x200
    c2dc:	br	x17

000000000000c2e0 <__gmpn_mu_div_q@plt>:
    c2e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2e4:	ldr	x17, [x16, #520]
    c2e8:	add	x16, x16, #0x208
    c2ec:	br	x17

000000000000c2f0 <snprintf@plt>:
    c2f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c2f4:	ldr	x17, [x16, #528]
    c2f8:	add	x16, x16, #0x210
    c2fc:	br	x17

000000000000c300 <__gmpn_mul_fft@plt>:
    c300:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c304:	ldr	x17, [x16, #536]
    c308:	add	x16, x16, #0x218
    c30c:	br	x17

000000000000c310 <__gmpz_setbit@plt>:
    c310:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c314:	ldr	x17, [x16, #544]
    c318:	add	x16, x16, #0x220
    c31c:	br	x17

000000000000c320 <__gmpn_div_q@plt>:
    c320:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c324:	ldr	x17, [x16, #552]
    c328:	add	x16, x16, #0x228
    c32c:	br	x17

000000000000c330 <__gmpf_clear@plt>:
    c330:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c334:	ldr	x17, [x16, #560]
    c338:	add	x16, x16, #0x230
    c33c:	br	x17

000000000000c340 <__gmpz_n_pow_ui@plt>:
    c340:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c344:	ldr	x17, [x16, #568]
    c348:	add	x16, x16, #0x238
    c34c:	br	x17

000000000000c350 <__gmpf_get_prec@plt>:
    c350:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c354:	ldr	x17, [x16, #576]
    c358:	add	x16, x16, #0x240
    c35c:	br	x17

000000000000c360 <__gmpn_dc_set_str@plt>:
    c360:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c364:	ldr	x17, [x16, #584]
    c368:	add	x16, x16, #0x248
    c36c:	br	x17

000000000000c370 <__gmpz_powm@plt>:
    c370:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c374:	ldr	x17, [x16, #592]
    c378:	add	x16, x16, #0x250
    c37c:	br	x17

000000000000c380 <__gmpn_sec_add_1@plt>:
    c380:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c384:	ldr	x17, [x16, #600]
    c388:	add	x16, x16, #0x258
    c38c:	br	x17

000000000000c390 <__gmpn_toom_eval_pm2exp@plt>:
    c390:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c394:	ldr	x17, [x16, #608]
    c398:	add	x16, x16, #0x260
    c39c:	br	x17

000000000000c3a0 <__gmpz_get_str@plt>:
    c3a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3a4:	ldr	x17, [x16, #616]
    c3a8:	add	x16, x16, #0x268
    c3ac:	br	x17

000000000000c3b0 <__gmpn_dcpi1_div_qr@plt>:
    c3b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3b4:	ldr	x17, [x16, #624]
    c3b8:	add	x16, x16, #0x270
    c3bc:	br	x17

000000000000c3c0 <__gmpn_powlo@plt>:
    c3c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3c4:	ldr	x17, [x16, #632]
    c3c8:	add	x16, x16, #0x278
    c3cc:	br	x17

000000000000c3d0 <__gmpz_oddfac_1@plt>:
    c3d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3d4:	ldr	x17, [x16, #640]
    c3d8:	add	x16, x16, #0x280
    c3dc:	br	x17

000000000000c3e0 <__gmpn_mod_1@plt>:
    c3e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3e4:	ldr	x17, [x16, #648]
    c3e8:	add	x16, x16, #0x288
    c3ec:	br	x17

000000000000c3f0 <__gmpz_divexact@plt>:
    c3f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c3f4:	ldr	x17, [x16, #656]
    c3f8:	add	x16, x16, #0x290
    c3fc:	br	x17

000000000000c400 <nl_langinfo@plt>:
    c400:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c404:	ldr	x17, [x16, #664]
    c408:	add	x16, x16, #0x298
    c40c:	br	x17

000000000000c410 <malloc@plt>:
    c410:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c414:	ldr	x17, [x16, #672]
    c418:	add	x16, x16, #0x2a0
    c41c:	br	x17

000000000000c420 <__gmpz_set@plt>:
    c420:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c424:	ldr	x17, [x16, #680]
    c428:	add	x16, x16, #0x2a8
    c42c:	br	x17

000000000000c430 <__gmpn_divexact@plt>:
    c430:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c434:	ldr	x17, [x16, #688]
    c438:	add	x16, x16, #0x2b0
    c43c:	br	x17

000000000000c440 <__gmpn_sublsh1_n@plt>:
    c440:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c444:	ldr	x17, [x16, #696]
    c448:	add	x16, x16, #0x2b8
    c44c:	br	x17

000000000000c450 <__gmpz_fac_ui@plt>:
    c450:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c454:	ldr	x17, [x16, #704]
    c458:	add	x16, x16, #0x2c0
    c45c:	br	x17

000000000000c460 <__gmpn_mulmid_basecase@plt>:
    c460:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c464:	ldr	x17, [x16, #712]
    c468:	add	x16, x16, #0x2c8
    c46c:	br	x17

000000000000c470 <__gmpn_div_qr_1n_pi1@plt>:
    c470:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c474:	ldr	x17, [x16, #720]
    c478:	add	x16, x16, #0x2d0
    c47c:	br	x17

000000000000c480 <__gmpz_tstbit@plt>:
    c480:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c484:	ldr	x17, [x16, #728]
    c488:	add	x16, x16, #0x2d8
    c48c:	br	x17

000000000000c490 <__gmp_randclear@plt>:
    c490:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c494:	ldr	x17, [x16, #736]
    c498:	add	x16, x16, #0x2e0
    c49c:	br	x17

000000000000c4a0 <__gmpf_set_d@plt>:
    c4a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4a4:	ldr	x17, [x16, #744]
    c4a8:	add	x16, x16, #0x2e8
    c4ac:	br	x17

000000000000c4b0 <__gmpz_mul@plt>:
    c4b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4b4:	ldr	x17, [x16, #752]
    c4b8:	add	x16, x16, #0x2f0
    c4bc:	br	x17

000000000000c4c0 <__gmpn_sec_tabselect@plt>:
    c4c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4c4:	ldr	x17, [x16, #760]
    c4c8:	add	x16, x16, #0x2f8
    c4cc:	br	x17

000000000000c4d0 <__gmpn_dcpi1_divappr_q@plt>:
    c4d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4d4:	ldr	x17, [x16, #768]
    c4d8:	add	x16, x16, #0x300
    c4dc:	br	x17

000000000000c4e0 <__gmpn_pi1_bdiv_q_1@plt>:
    c4e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4e4:	ldr	x17, [x16, #776]
    c4e8:	add	x16, x16, #0x308
    c4ec:	br	x17

000000000000c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>:
    c4f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c4f4:	ldr	x17, [x16, #784]
    c4f8:	add	x16, x16, #0x310
    c4fc:	br	x17

000000000000c500 <__gmp_vasprintf@plt>:
    c500:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c504:	ldr	x17, [x16, #792]
    c508:	add	x16, x16, #0x318
    c50c:	br	x17

000000000000c510 <__gmpn_sbpi1_bdiv_q@plt>:
    c510:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c514:	ldr	x17, [x16, #800]
    c518:	add	x16, x16, #0x320
    c51c:	br	x17

000000000000c520 <__gmpz_lucas_mod@plt>:
    c520:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c524:	ldr	x17, [x16, #808]
    c528:	add	x16, x16, #0x328
    c52c:	br	x17

000000000000c530 <__gmpn_toom_eval_pm2@plt>:
    c530:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c534:	ldr	x17, [x16, #816]
    c538:	add	x16, x16, #0x330
    c53c:	br	x17

000000000000c540 <__gmpz_out_str@plt>:
    c540:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c544:	ldr	x17, [x16, #824]
    c548:	add	x16, x16, #0x338
    c54c:	br	x17

000000000000c550 <__gmpn_mul_basecase@plt>:
    c550:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c554:	ldr	x17, [x16, #832]
    c558:	add	x16, x16, #0x340
    c55c:	br	x17

000000000000c560 <__gmpn_gcdext@plt>:
    c560:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c564:	ldr	x17, [x16, #840]
    c568:	add	x16, x16, #0x348
    c56c:	br	x17

000000000000c570 <__isoc99_fscanf@plt>:
    c570:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c574:	ldr	x17, [x16, #848]
    c578:	add	x16, x16, #0x350
    c57c:	br	x17

000000000000c580 <__gmpz_swap@plt>:
    c580:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c584:	ldr	x17, [x16, #856]
    c588:	add	x16, x16, #0x358
    c58c:	br	x17

000000000000c590 <__gmpn_hgcd_itch@plt>:
    c590:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c594:	ldr	x17, [x16, #864]
    c598:	add	x16, x16, #0x360
    c59c:	br	x17

000000000000c5a0 <__gmpn_hgcd2@plt>:
    c5a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5a4:	ldr	x17, [x16, #872]
    c5a8:	add	x16, x16, #0x368
    c5ac:	br	x17

000000000000c5b0 <fgetc@plt>:
    c5b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5b4:	ldr	x17, [x16, #880]
    c5b8:	add	x16, x16, #0x370
    c5bc:	br	x17

000000000000c5c0 <__gmpz_mul_ui@plt>:
    c5c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5c4:	ldr	x17, [x16, #888]
    c5c8:	add	x16, x16, #0x378
    c5cc:	br	x17

000000000000c5d0 <__gmpn_sqrmod_bnm1_next_size@plt>:
    c5d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5d4:	ldr	x17, [x16, #896]
    c5d8:	add	x16, x16, #0x380
    c5dc:	br	x17

000000000000c5e0 <__gmpn_dcpi1_bdiv_qr@plt>:
    c5e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5e4:	ldr	x17, [x16, #904]
    c5e8:	add	x16, x16, #0x388
    c5ec:	br	x17

000000000000c5f0 <memset@plt>:
    c5f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c5f4:	ldr	x17, [x16, #912]
    c5f8:	add	x16, x16, #0x390
    c5fc:	br	x17

000000000000c600 <__gmpz_2fac_ui@plt>:
    c600:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c604:	ldr	x17, [x16, #920]
    c608:	add	x16, x16, #0x398
    c60c:	br	x17

000000000000c610 <__gmpn_trialdiv@plt>:
    c610:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c614:	ldr	x17, [x16, #928]
    c618:	add	x16, x16, #0x3a0
    c61c:	br	x17

000000000000c620 <__gmpf_set_si@plt>:
    c620:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c624:	ldr	x17, [x16, #936]
    c628:	add	x16, x16, #0x3a8
    c62c:	br	x17

000000000000c630 <__gmpn_add_err2_n@plt>:
    c630:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c634:	ldr	x17, [x16, #944]
    c638:	add	x16, x16, #0x3b0
    c63c:	br	x17

000000000000c640 <__gmpn_sbpi1_div_qr@plt>:
    c640:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c644:	ldr	x17, [x16, #952]
    c648:	add	x16, x16, #0x3b8
    c64c:	br	x17

000000000000c650 <__gmpz_fdiv_q_2exp@plt>:
    c650:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c654:	ldr	x17, [x16, #960]
    c658:	add	x16, x16, #0x3c0
    c65c:	br	x17

000000000000c660 <__gmpn_cnd_swap@plt>:
    c660:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c664:	ldr	x17, [x16, #968]
    c668:	add	x16, x16, #0x3c8
    c66c:	br	x17

000000000000c670 <__gmpf_cmp@plt>:
    c670:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c674:	ldr	x17, [x16, #976]
    c678:	add	x16, x16, #0x3d0
    c67c:	br	x17

000000000000c680 <__gmpn_mod_1s_4p_cps@plt>:
    c680:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c684:	ldr	x17, [x16, #984]
    c688:	add	x16, x16, #0x3d8
    c68c:	br	x17

000000000000c690 <__gmpn_scan0@plt>:
    c690:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c694:	ldr	x17, [x16, #992]
    c698:	add	x16, x16, #0x3e0
    c69c:	br	x17

000000000000c6a0 <__gmpf_set_ui@plt>:
    c6a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6a4:	ldr	x17, [x16, #1000]
    c6a8:	add	x16, x16, #0x3e8
    c6ac:	br	x17

000000000000c6b0 <__gmpn_brootinv@plt>:
    c6b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6b4:	ldr	x17, [x16, #1008]
    c6b8:	add	x16, x16, #0x3f0
    c6bc:	br	x17

000000000000c6c0 <__gmp_assert_fail@plt>:
    c6c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6c4:	ldr	x17, [x16, #1016]
    c6c8:	add	x16, x16, #0x3f8
    c6cc:	br	x17

000000000000c6d0 <__gmpn_preinv_mod_1@plt>:
    c6d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6d4:	ldr	x17, [x16, #1024]
    c6d8:	add	x16, x16, #0x400
    c6dc:	br	x17

000000000000c6e0 <__gmpz_mul_2exp@plt>:
    c6e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6e4:	ldr	x17, [x16, #1032]
    c6e8:	add	x16, x16, #0x408
    c6ec:	br	x17

000000000000c6f0 <__gmpn_sbpi1_divappr_q@plt>:
    c6f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c6f4:	ldr	x17, [x16, #1040]
    c6f8:	add	x16, x16, #0x410
    c6fc:	br	x17

000000000000c700 <__gmpn_mulmid@plt>:
    c700:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c704:	ldr	x17, [x16, #1048]
    c708:	add	x16, x16, #0x418
    c70c:	br	x17

000000000000c710 <__gmpn_mu_divappr_q@plt>:
    c710:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c714:	ldr	x17, [x16, #1056]
    c718:	add	x16, x16, #0x420
    c71c:	br	x17

000000000000c720 <__gmpn_toom44_mul@plt>:
    c720:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c724:	ldr	x17, [x16, #1064]
    c728:	add	x16, x16, #0x428
    c72c:	br	x17

000000000000c730 <__gmpn_jacobi_base@plt>:
    c730:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c734:	ldr	x17, [x16, #1072]
    c738:	add	x16, x16, #0x430
    c73c:	br	x17

000000000000c740 <__gmpn_toom63_mul@plt>:
    c740:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c744:	ldr	x17, [x16, #1080]
    c748:	add	x16, x16, #0x438
    c74c:	br	x17

000000000000c750 <__gmpn_bsqrtinv@plt>:
    c750:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c754:	ldr	x17, [x16, #1088]
    c758:	add	x16, x16, #0x440
    c75c:	br	x17

000000000000c760 <__gmpn_sub_nc@plt>:
    c760:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c764:	ldr	x17, [x16, #1096]
    c768:	add	x16, x16, #0x448
    c76c:	br	x17

000000000000c770 <__gmpn_divexact_1@plt>:
    c770:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c774:	ldr	x17, [x16, #1104]
    c778:	add	x16, x16, #0x450
    c77c:	br	x17

000000000000c780 <__gmpn_hgcd_matrix_mul_1@plt>:
    c780:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c784:	ldr	x17, [x16, #1112]
    c788:	add	x16, x16, #0x458
    c78c:	br	x17

000000000000c790 <__gmpn_toom42_mulmid@plt>:
    c790:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c794:	ldr	x17, [x16, #1120]
    c798:	add	x16, x16, #0x460
    c79c:	br	x17

000000000000c7a0 <__gmp_randinit_default@plt>:
    c7a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7a4:	ldr	x17, [x16, #1128]
    c7a8:	add	x16, x16, #0x468
    c7ac:	br	x17

000000000000c7b0 <__gmpn_toom_interpolate_8pts@plt>:
    c7b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7b4:	ldr	x17, [x16, #1136]
    c7b8:	add	x16, x16, #0x470
    c7bc:	br	x17

000000000000c7c0 <realloc@plt>:
    c7c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7c4:	ldr	x17, [x16, #1144]
    c7c8:	add	x16, x16, #0x478
    c7cc:	br	x17

000000000000c7d0 <__gmpn_redc_n@plt>:
    c7d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7d4:	ldr	x17, [x16, #1152]
    c7d8:	add	x16, x16, #0x480
    c7dc:	br	x17

000000000000c7e0 <__gmpn_modexact_1c_odd@plt>:
    c7e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7e4:	ldr	x17, [x16, #1160]
    c7e8:	add	x16, x16, #0x488
    c7ec:	br	x17

000000000000c7f0 <getc@plt>:
    c7f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c7f4:	ldr	x17, [x16, #1168]
    c7f8:	add	x16, x16, #0x490
    c7fc:	br	x17

000000000000c800 <__gmpn_sec_pi1_div_r@plt>:
    c800:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c804:	ldr	x17, [x16, #1176]
    c808:	add	x16, x16, #0x498
    c80c:	br	x17

000000000000c810 <__gmpn_toom_interpolate_7pts@plt>:
    c810:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c814:	ldr	x17, [x16, #1184]
    c818:	add	x16, x16, #0x4a0
    c81c:	br	x17

000000000000c820 <__gmpn_sbpi1_bdiv_qr@plt>:
    c820:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c824:	ldr	x17, [x16, #1192]
    c828:	add	x16, x16, #0x4a8
    c82c:	br	x17

000000000000c830 <__gmpn_hgcd_matrix_init@plt>:
    c830:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c834:	ldr	x17, [x16, #1200]
    c838:	add	x16, x16, #0x4b0
    c83c:	br	x17

000000000000c840 <__gmpn_rsh1sub_n@plt>:
    c840:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c844:	ldr	x17, [x16, #1208]
    c848:	add	x16, x16, #0x4b8
    c84c:	br	x17

000000000000c850 <__gmpn_toom32_mul@plt>:
    c850:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c854:	ldr	x17, [x16, #1216]
    c858:	add	x16, x16, #0x4c0
    c85c:	br	x17

000000000000c860 <__gmpn_mulmod_bnm1_next_size@plt>:
    c860:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c864:	ldr	x17, [x16, #1224]
    c868:	add	x16, x16, #0x4c8
    c86c:	br	x17

000000000000c870 <__gmpz_submul_ui@plt>:
    c870:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c874:	ldr	x17, [x16, #1232]
    c878:	add	x16, x16, #0x4d0
    c87c:	br	x17

000000000000c880 <__gmpn_mu_bdiv_q_itch@plt>:
    c880:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c884:	ldr	x17, [x16, #1240]
    c888:	add	x16, x16, #0x4d8
    c88c:	br	x17

000000000000c890 <__gmpz_set_d@plt>:
    c890:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c894:	ldr	x17, [x16, #1248]
    c898:	add	x16, x16, #0x4e0
    c89c:	br	x17

000000000000c8a0 <__gmpn_hgcd_matrix_adjust@plt>:
    c8a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8a4:	ldr	x17, [x16, #1256]
    c8a8:	add	x16, x16, #0x4e8
    c8ac:	br	x17

000000000000c8b0 <__gmpz_add_ui@plt>:
    c8b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8b4:	ldr	x17, [x16, #1264]
    c8b8:	add	x16, x16, #0x4f0
    c8bc:	br	x17

000000000000c8c0 <__gmpn_bdiv_qr_itch@plt>:
    c8c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8c4:	ldr	x17, [x16, #1272]
    c8c8:	add	x16, x16, #0x4f8
    c8cc:	br	x17

000000000000c8d0 <__gmon_start__@plt>:
    c8d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8d4:	ldr	x17, [x16, #1280]
    c8d8:	add	x16, x16, #0x500
    c8dc:	br	x17

000000000000c8e0 <__gmpn_sqr@plt>:
    c8e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8e4:	ldr	x17, [x16, #1288]
    c8e8:	add	x16, x16, #0x508
    c8ec:	br	x17

000000000000c8f0 <__gmpz_urandomm@plt>:
    c8f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c8f4:	ldr	x17, [x16, #1296]
    c8f8:	add	x16, x16, #0x510
    c8fc:	br	x17

000000000000c900 <abort@plt>:
    c900:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c904:	ldr	x17, [x16, #1304]
    c908:	add	x16, x16, #0x518
    c90c:	br	x17

000000000000c910 <__gmpn_toom_interpolate_6pts@plt>:
    c910:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c914:	ldr	x17, [x16, #1312]
    c918:	add	x16, x16, #0x520
    c91c:	br	x17

000000000000c920 <__gmpn_div_qr_2n_pi1@plt>:
    c920:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c924:	ldr	x17, [x16, #1320]
    c928:	add	x16, x16, #0x528
    c92c:	br	x17

000000000000c930 <__gmpn_broot_invm1@plt>:
    c930:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c934:	ldr	x17, [x16, #1328]
    c938:	add	x16, x16, #0x530
    c93c:	br	x17

000000000000c940 <__gmpz_clrbit@plt>:
    c940:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c944:	ldr	x17, [x16, #1336]
    c948:	add	x16, x16, #0x538
    c94c:	br	x17

000000000000c950 <__gmpn_rsh1add_n@plt>:
    c950:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c954:	ldr	x17, [x16, #1344]
    c958:	add	x16, x16, #0x540
    c95c:	br	x17

000000000000c960 <__gmpn_mu_div_qr@plt>:
    c960:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c964:	ldr	x17, [x16, #1352]
    c968:	add	x16, x16, #0x548
    c96c:	br	x17

000000000000c970 <__gmpn_toom_couple_handling@plt>:
    c970:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c974:	ldr	x17, [x16, #1360]
    c978:	add	x16, x16, #0x550
    c97c:	br	x17

000000000000c980 <__gmpn_mulmod_bnm1@plt>:
    c980:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c984:	ldr	x17, [x16, #1368]
    c988:	add	x16, x16, #0x558
    c98c:	br	x17

000000000000c990 <__gmpn_mul_n@plt>:
    c990:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c994:	ldr	x17, [x16, #1376]
    c998:	add	x16, x16, #0x560
    c99c:	br	x17

000000000000c9a0 <__gmpz_scan0@plt>:
    c9a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9a4:	ldr	x17, [x16, #1384]
    c9a8:	add	x16, x16, #0x568
    c9ac:	br	x17

000000000000c9b0 <puts@plt>:
    c9b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9b4:	ldr	x17, [x16, #1392]
    c9b8:	add	x16, x16, #0x570
    c9bc:	br	x17

000000000000c9c0 <__gmpz_stronglucas@plt>:
    c9c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9c4:	ldr	x17, [x16, #1400]
    c9c8:	add	x16, x16, #0x578
    c9cc:	br	x17

000000000000c9d0 <__gmpz_inp_str_nowhite@plt>:
    c9d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9d4:	ldr	x17, [x16, #1408]
    c9d8:	add	x16, x16, #0x580
    c9dc:	br	x17

000000000000c9e0 <__gmpn_submul_1@plt>:
    c9e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9e4:	ldr	x17, [x16, #1416]
    c9e8:	add	x16, x16, #0x588
    c9ec:	br	x17

000000000000c9f0 <__gmpn_sqrlo@plt>:
    c9f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    c9f4:	ldr	x17, [x16, #1424]
    c9f8:	add	x16, x16, #0x590
    c9fc:	br	x17

000000000000ca00 <__gmpz_divexact_gcd@plt>:
    ca00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca04:	ldr	x17, [x16, #1432]
    ca08:	add	x16, x16, #0x598
    ca0c:	br	x17

000000000000ca10 <__gmpz_ui_pow_ui@plt>:
    ca10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca14:	ldr	x17, [x16, #1440]
    ca18:	add	x16, x16, #0x5a0
    ca1c:	br	x17

000000000000ca20 <__gmpn_toom_interpolate_5pts@plt>:
    ca20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca24:	ldr	x17, [x16, #1448]
    ca28:	add	x16, x16, #0x5a8
    ca2c:	br	x17

000000000000ca30 <__gmpn_bdiv_q@plt>:
    ca30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca34:	ldr	x17, [x16, #1456]
    ca38:	add	x16, x16, #0x5b0
    ca3c:	br	x17

000000000000ca40 <__gmpn_toom53_mul@plt>:
    ca40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca44:	ldr	x17, [x16, #1464]
    ca48:	add	x16, x16, #0x5b8
    ca4c:	br	x17

000000000000ca50 <__gmpn_copyi@plt>:
    ca50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca54:	ldr	x17, [x16, #1472]
    ca58:	add	x16, x16, #0x5c0
    ca5c:	br	x17

000000000000ca60 <__gmpq_set_ui@plt>:
    ca60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca64:	ldr	x17, [x16, #1480]
    ca68:	add	x16, x16, #0x5c8
    ca6c:	br	x17

000000000000ca70 <__gmpn_add_n@plt>:
    ca70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca74:	ldr	x17, [x16, #1488]
    ca78:	add	x16, x16, #0x5d0
    ca7c:	br	x17

000000000000ca80 <__gmpz_tdiv_r@plt>:
    ca80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca84:	ldr	x17, [x16, #1496]
    ca88:	add	x16, x16, #0x5d8
    ca8c:	br	x17

000000000000ca90 <__gmpn_get_str@plt>:
    ca90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ca94:	ldr	x17, [x16, #1504]
    ca98:	add	x16, x16, #0x5e0
    ca9c:	br	x17

000000000000caa0 <__gmpn_dcpi1_div_q@plt>:
    caa0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    caa4:	ldr	x17, [x16, #1512]
    caa8:	add	x16, x16, #0x5e8
    caac:	br	x17

000000000000cab0 <__gmpn_jacobi_2@plt>:
    cab0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cab4:	ldr	x17, [x16, #1520]
    cab8:	add	x16, x16, #0x5f0
    cabc:	br	x17

000000000000cac0 <__gmpn_mod_1_1p_cps@plt>:
    cac0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cac4:	ldr	x17, [x16, #1528]
    cac8:	add	x16, x16, #0x5f8
    cacc:	br	x17

000000000000cad0 <__gmpn_fft_best_k@plt>:
    cad0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cad4:	ldr	x17, [x16, #1536]
    cad8:	add	x16, x16, #0x600
    cadc:	br	x17

000000000000cae0 <__ctype_b_loc@plt>:
    cae0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cae4:	ldr	x17, [x16, #1544]
    cae8:	add	x16, x16, #0x608
    caec:	br	x17

000000000000caf0 <__gmpn_hgcd2_jacobi@plt>:
    caf0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    caf4:	ldr	x17, [x16, #1552]
    caf8:	add	x16, x16, #0x610
    cafc:	br	x17

000000000000cb00 <__gmp_randinit_mt@plt>:
    cb00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb04:	ldr	x17, [x16, #1560]
    cb08:	add	x16, x16, #0x618
    cb0c:	br	x17

000000000000cb10 <__gmpn_compute_powtab@plt>:
    cb10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb14:	ldr	x17, [x16, #1568]
    cb18:	add	x16, x16, #0x620
    cb1c:	br	x17

000000000000cb20 <__gmpn_toom8h_mul@plt>:
    cb20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb24:	ldr	x17, [x16, #1576]
    cb28:	add	x16, x16, #0x628
    cb2c:	br	x17

000000000000cb30 <__gmpz_kronecker_ui@plt>:
    cb30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb34:	ldr	x17, [x16, #1584]
    cb38:	add	x16, x16, #0x630
    cb3c:	br	x17

000000000000cb40 <__gmpn_xor_n@plt>:
    cb40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb44:	ldr	x17, [x16, #1592]
    cb48:	add	x16, x16, #0x638
    cb4c:	br	x17

000000000000cb50 <__gmpz_clear@plt>:
    cb50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb54:	ldr	x17, [x16, #1600]
    cb58:	add	x16, x16, #0x640
    cb5c:	br	x17

000000000000cb60 <strtol@plt>:
    cb60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb64:	ldr	x17, [x16, #1608]
    cb68:	add	x16, x16, #0x648
    cb6c:	br	x17

000000000000cb70 <__gmpq_set_si@plt>:
    cb70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb74:	ldr	x17, [x16, #1616]
    cb78:	add	x16, x16, #0x650
    cb7c:	br	x17

000000000000cb80 <__gmpz_millerrabin@plt>:
    cb80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb84:	ldr	x17, [x16, #1624]
    cb88:	add	x16, x16, #0x658
    cb8c:	br	x17

000000000000cb90 <fread@plt>:
    cb90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cb94:	ldr	x17, [x16, #1632]
    cb98:	add	x16, x16, #0x660
    cb9c:	br	x17

000000000000cba0 <__gmpn_addlsh2_n@plt>:
    cba0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cba4:	ldr	x17, [x16, #1640]
    cba8:	add	x16, x16, #0x668
    cbac:	br	x17

000000000000cbb0 <__gmpz_mul_si@plt>:
    cbb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cbb4:	ldr	x17, [x16, #1648]
    cbb8:	add	x16, x16, #0x670
    cbbc:	br	x17

000000000000cbc0 <__gmp_tmp_reentrant_alloc@plt>:
    cbc0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cbc4:	ldr	x17, [x16, #1656]
    cbc8:	add	x16, x16, #0x678
    cbcc:	br	x17

000000000000cbd0 <__gmpz_invert@plt>:
    cbd0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cbd4:	ldr	x17, [x16, #1664]
    cbd8:	add	x16, x16, #0x680
    cbdc:	br	x17

000000000000cbe0 <__gmpn_rsblsh2_n@plt>:
    cbe0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cbe4:	ldr	x17, [x16, #1672]
    cbe8:	add	x16, x16, #0x688
    cbec:	br	x17

000000000000cbf0 <__gmpf_neg@plt>:
    cbf0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cbf4:	ldr	x17, [x16, #1680]
    cbf8:	add	x16, x16, #0x690
    cbfc:	br	x17

000000000000cc00 <__gmpn_ior_n@plt>:
    cc00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc04:	ldr	x17, [x16, #1688]
    cc08:	add	x16, x16, #0x698
    cc0c:	br	x17

000000000000cc10 <__gmpn_gcd@plt>:
    cc10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc14:	ldr	x17, [x16, #1696]
    cc18:	add	x16, x16, #0x6a0
    cc1c:	br	x17

000000000000cc20 <__gmpn_toom6h_mul@plt>:
    cc20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc24:	ldr	x17, [x16, #1704]
    cc28:	add	x16, x16, #0x6a8
    cc2c:	br	x17

000000000000cc30 <free@plt>:
    cc30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc34:	ldr	x17, [x16, #1712]
    cc38:	add	x16, x16, #0x6b0
    cc3c:	br	x17

000000000000cc40 <__gmpn_addlsh1_n@plt>:
    cc40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc44:	ldr	x17, [x16, #1720]
    cc48:	add	x16, x16, #0x6b8
    cc4c:	br	x17

000000000000cc50 <ungetc@plt>:
    cc50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc54:	ldr	x17, [x16, #1728]
    cc58:	add	x16, x16, #0x6c0
    cc5c:	br	x17

000000000000cc60 <__gmpn_sec_powm@plt>:
    cc60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc64:	ldr	x17, [x16, #1736]
    cc68:	add	x16, x16, #0x6c8
    cc6c:	br	x17

000000000000cc70 <__gmpz_tdiv_q_2exp@plt>:
    cc70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc74:	ldr	x17, [x16, #1744]
    cc78:	add	x16, x16, #0x6d0
    cc7c:	br	x17

000000000000cc80 <__gmp_nextprime@plt>:
    cc80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc84:	ldr	x17, [x16, #1752]
    cc88:	add	x16, x16, #0x6d8
    cc8c:	br	x17

000000000000cc90 <__gmpz_roinit_n@plt>:
    cc90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cc94:	ldr	x17, [x16, #1760]
    cc98:	add	x16, x16, #0x6e0
    cc9c:	br	x17

000000000000cca0 <__gmpn_nussbaumer_mul@plt>:
    cca0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cca4:	ldr	x17, [x16, #1768]
    cca8:	add	x16, x16, #0x6e8
    ccac:	br	x17

000000000000ccb0 <__gmpn_mu_bdiv_qr@plt>:
    ccb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ccb4:	ldr	x17, [x16, #1776]
    ccb8:	add	x16, x16, #0x6f0
    ccbc:	br	x17

000000000000ccc0 <__gmpn_mod_1s_2p_cps@plt>:
    ccc0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ccc4:	ldr	x17, [x16, #1784]
    ccc8:	add	x16, x16, #0x6f8
    cccc:	br	x17

000000000000ccd0 <__gmpn_mul@plt>:
    ccd0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ccd4:	ldr	x17, [x16, #1792]
    ccd8:	add	x16, x16, #0x700
    ccdc:	br	x17

000000000000cce0 <__gmpn_preinv_divrem_1@plt>:
    cce0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cce4:	ldr	x17, [x16, #1800]
    cce8:	add	x16, x16, #0x708
    ccec:	br	x17

000000000000ccf0 <__gmpn_add_err1_n@plt>:
    ccf0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ccf4:	ldr	x17, [x16, #1808]
    ccf8:	add	x16, x16, #0x710
    ccfc:	br	x17

000000000000cd00 <__gmpn_divrem_1@plt>:
    cd00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd04:	ldr	x17, [x16, #1816]
    cd08:	add	x16, x16, #0x718
    cd0c:	br	x17

000000000000cd10 <__gmp_doprnt_integer@plt>:
    cd10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd14:	ldr	x17, [x16, #1824]
    cd18:	add	x16, x16, #0x720
    cd1c:	br	x17

000000000000cd20 <__gmpn_binvert@plt>:
    cd20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd24:	ldr	x17, [x16, #1832]
    cd28:	add	x16, x16, #0x728
    cd2c:	br	x17

000000000000cd30 <__gmpf_mul@plt>:
    cd30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd34:	ldr	x17, [x16, #1840]
    cd38:	add	x16, x16, #0x730
    cd3c:	br	x17

000000000000cd40 <__gmpn_remove@plt>:
    cd40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd44:	ldr	x17, [x16, #1848]
    cd48:	add	x16, x16, #0x738
    cd4c:	br	x17

000000000000cd50 <__gmpn_hgcd_appr@plt>:
    cd50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd54:	ldr	x17, [x16, #1856]
    cd58:	add	x16, x16, #0x740
    cd5c:	br	x17

000000000000cd60 <__gmpn_toom_eval_dgr3_pm2@plt>:
    cd60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd64:	ldr	x17, [x16, #1864]
    cd68:	add	x16, x16, #0x748
    cd6c:	br	x17

000000000000cd70 <__gmpz_prodlimbs@plt>:
    cd70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd74:	ldr	x17, [x16, #1872]
    cd78:	add	x16, x16, #0x750
    cd7c:	br	x17

000000000000cd80 <__gmpn_popcount@plt>:
    cd80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd84:	ldr	x17, [x16, #1880]
    cd88:	add	x16, x16, #0x758
    cd8c:	br	x17

000000000000cd90 <__gmpf_mul_2exp@plt>:
    cd90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cd94:	ldr	x17, [x16, #1888]
    cd98:	add	x16, x16, #0x760
    cd9c:	br	x17

000000000000cda0 <strchr@plt>:
    cda0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cda4:	ldr	x17, [x16, #1896]
    cda8:	add	x16, x16, #0x768
    cdac:	br	x17

000000000000cdb0 <__gmp_assert_header@plt>:
    cdb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cdb4:	ldr	x17, [x16, #1904]
    cdb8:	add	x16, x16, #0x770
    cdbc:	br	x17

000000000000cdc0 <obstack_vprintf@plt>:
    cdc0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cdc4:	ldr	x17, [x16, #1912]
    cdc8:	add	x16, x16, #0x778
    cdcc:	br	x17

000000000000cdd0 <__gmpq_init@plt>:
    cdd0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cdd4:	ldr	x17, [x16, #1920]
    cdd8:	add	x16, x16, #0x780
    cddc:	br	x17

000000000000cde0 <__gmpn_hgcd@plt>:
    cde0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cde4:	ldr	x17, [x16, #1928]
    cde8:	add	x16, x16, #0x788
    cdec:	br	x17

000000000000cdf0 <__gmpz_mod@plt>:
    cdf0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cdf4:	ldr	x17, [x16, #1936]
    cdf8:	add	x16, x16, #0x790
    cdfc:	br	x17

000000000000ce00 <__gmpf_sub@plt>:
    ce00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce04:	ldr	x17, [x16, #1944]
    ce08:	add	x16, x16, #0x798
    ce0c:	br	x17

000000000000ce10 <__gmpn_dcpi1_bdiv_q@plt>:
    ce10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce14:	ldr	x17, [x16, #1952]
    ce18:	add	x16, x16, #0x7a0
    ce1c:	br	x17

000000000000ce20 <__gmpf_get_str@plt>:
    ce20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce24:	ldr	x17, [x16, #1960]
    ce28:	add	x16, x16, #0x7a8
    ce2c:	br	x17

000000000000ce30 <fwrite@plt>:
    ce30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce34:	ldr	x17, [x16, #1968]
    ce38:	add	x16, x16, #0x7b0
    ce3c:	br	x17

000000000000ce40 <__gmpn_hamdist@plt>:
    ce40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce44:	ldr	x17, [x16, #1976]
    ce48:	add	x16, x16, #0x7b8
    ce4c:	br	x17

000000000000ce50 <__gmpz_init_set_ui@plt>:
    ce50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce54:	ldr	x17, [x16, #1984]
    ce58:	add	x16, x16, #0x7c0
    ce5c:	br	x17

000000000000ce60 <__gmpf_init@plt>:
    ce60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce64:	ldr	x17, [x16, #1992]
    ce68:	add	x16, x16, #0x7c8
    ce6c:	br	x17

000000000000ce70 <__gmpz_cmp@plt>:
    ce70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce74:	ldr	x17, [x16, #2000]
    ce78:	add	x16, x16, #0x7d0
    ce7c:	br	x17

000000000000ce80 <__gmpn_mod_1s_2p@plt>:
    ce80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce84:	ldr	x17, [x16, #2008]
    ce88:	add	x16, x16, #0x7d8
    ce8c:	br	x17

000000000000ce90 <__gmpn_add_nc@plt>:
    ce90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ce94:	ldr	x17, [x16, #2016]
    ce98:	add	x16, x16, #0x7e0
    ce9c:	br	x17

000000000000cea0 <__gmpn_jacobi_n@plt>:
    cea0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cea4:	ldr	x17, [x16, #2024]
    cea8:	add	x16, x16, #0x7e8
    ceac:	br	x17

000000000000ceb0 <__gmpf_init2@plt>:
    ceb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ceb4:	ldr	x17, [x16, #2032]
    ceb8:	add	x16, x16, #0x7f0
    cebc:	br	x17

000000000000cec0 <__gmpn_mullo_n@plt>:
    cec0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cec4:	ldr	x17, [x16, #2040]
    cec8:	add	x16, x16, #0x7f8
    cecc:	br	x17

000000000000ced0 <__gmpf_div@plt>:
    ced0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    ced4:	ldr	x17, [x16, #2048]
    ced8:	add	x16, x16, #0x800
    cedc:	br	x17

000000000000cee0 <__gmpn_sbpi1_div_q@plt>:
    cee0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cee4:	ldr	x17, [x16, #2056]
    cee8:	add	x16, x16, #0x808
    ceec:	br	x17

000000000000cef0 <__gmpn_sec_pi1_div_qr@plt>:
    cef0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cef4:	ldr	x17, [x16, #2064]
    cef8:	add	x16, x16, #0x810
    cefc:	br	x17

000000000000cf00 <__gmpn_toom43_mul@plt>:
    cf00:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf04:	ldr	x17, [x16, #2072]
    cf08:	add	x16, x16, #0x818
    cf0c:	br	x17

000000000000cf10 <vsprintf@plt>:
    cf10:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf14:	ldr	x17, [x16, #2080]
    cf18:	add	x16, x16, #0x820
    cf1c:	br	x17

000000000000cf20 <__gmpn_div_qr_2u_pi1@plt>:
    cf20:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf24:	ldr	x17, [x16, #2088]
    cf28:	add	x16, x16, #0x828
    cf2c:	br	x17

000000000000cf30 <__gmpn_zero@plt>:
    cf30:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf34:	ldr	x17, [x16, #2096]
    cf38:	add	x16, x16, #0x830
    cf3c:	br	x17

000000000000cf40 <__gmp_randinit_lc_2exp@plt>:
    cf40:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf44:	ldr	x17, [x16, #2104]
    cf48:	add	x16, x16, #0x838
    cf4c:	br	x17

000000000000cf50 <__gmpn_bdiv_qr@plt>:
    cf50:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf54:	ldr	x17, [x16, #2112]
    cf58:	add	x16, x16, #0x840
    cf5c:	br	x17

000000000000cf60 <__gmpn_mod_34lsub1@plt>:
    cf60:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf64:	ldr	x17, [x16, #2120]
    cf68:	add	x16, x16, #0x848
    cf6c:	br	x17

000000000000cf70 <__gmpz_gcd@plt>:
    cf70:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf74:	ldr	x17, [x16, #2128]
    cf78:	add	x16, x16, #0x850
    cf7c:	br	x17

000000000000cf80 <__gmpz_aorsmul_1@plt>:
    cf80:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf84:	ldr	x17, [x16, #2136]
    cf88:	add	x16, x16, #0x858
    cf8c:	br	x17

000000000000cf90 <__gmpz_add@plt>:
    cf90:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cf94:	ldr	x17, [x16, #2144]
    cf98:	add	x16, x16, #0x860
    cf9c:	br	x17

000000000000cfa0 <__gmpn_hgcd_matrix_mul@plt>:
    cfa0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cfa4:	ldr	x17, [x16, #2152]
    cfa8:	add	x16, x16, #0x868
    cfac:	br	x17

000000000000cfb0 <__gmp_randseed@plt>:
    cfb0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cfb4:	ldr	x17, [x16, #2160]
    cfb8:	add	x16, x16, #0x870
    cfbc:	br	x17

000000000000cfc0 <__gmpn_toom_eval_pm1@plt>:
    cfc0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cfc4:	ldr	x17, [x16, #2168]
    cfc8:	add	x16, x16, #0x878
    cfcc:	br	x17

000000000000cfd0 <__gmpn_mu_div_q_itch@plt>:
    cfd0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cfd4:	ldr	x17, [x16, #2176]
    cfd8:	add	x16, x16, #0x880
    cfdc:	br	x17

000000000000cfe0 <__gmpq_mul@plt>:
    cfe0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cfe4:	ldr	x17, [x16, #2184]
    cfe8:	add	x16, x16, #0x888
    cfec:	br	x17

000000000000cff0 <__gmp_sqrt_of_negative@plt>:
    cff0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    cff4:	ldr	x17, [x16, #2192]
    cff8:	add	x16, x16, #0x890
    cffc:	br	x17

000000000000d000 <__gmpn_powm@plt>:
    d000:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d004:	ldr	x17, [x16, #2200]
    d008:	add	x16, x16, #0x898
    d00c:	br	x17

000000000000d010 <__gmpn_mu_div_qr_itch@plt>:
    d010:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d014:	ldr	x17, [x16, #2208]
    d018:	add	x16, x16, #0x8a0
    d01c:	br	x17

000000000000d020 <__gmpn_hgcd_matrix_update_q@plt>:
    d020:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d024:	ldr	x17, [x16, #2216]
    d028:	add	x16, x16, #0x8a8
    d02c:	br	x17

000000000000d030 <__gmp_doprnt@plt>:
    d030:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d034:	ldr	x17, [x16, #2224]
    d038:	add	x16, x16, #0x8b0
    d03c:	br	x17

000000000000d040 <_obstack_newchunk@plt>:
    d040:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d044:	ldr	x17, [x16, #2232]
    d048:	add	x16, x16, #0x8b8
    d04c:	br	x17

000000000000d050 <__gmpn_fib2m@plt>:
    d050:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d054:	ldr	x17, [x16, #2240]
    d058:	add	x16, x16, #0x8c0
    d05c:	br	x17

000000000000d060 <__gmpn_invertappr@plt>:
    d060:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d064:	ldr	x17, [x16, #2248]
    d068:	add	x16, x16, #0x8c8
    d06c:	br	x17

000000000000d070 <__gmpn_fib2_ui@plt>:
    d070:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d074:	ldr	x17, [x16, #2256]
    d078:	add	x16, x16, #0x8d0
    d07c:	br	x17

000000000000d080 <__gmpn_preinv_mu_div_qr@plt>:
    d080:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d084:	ldr	x17, [x16, #2264]
    d088:	add	x16, x16, #0x8d8
    d08c:	br	x17

000000000000d090 <__gmpn_rsblsh1_n@plt>:
    d090:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d094:	ldr	x17, [x16, #2272]
    d098:	add	x16, x16, #0x8e0
    d09c:	br	x17

000000000000d0a0 <__gmpz_init_set_str@plt>:
    d0a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0a4:	ldr	x17, [x16, #2280]
    d0a8:	add	x16, x16, #0x8e8
    d0ac:	br	x17

000000000000d0b0 <__gmpn_perfect_square_p@plt>:
    d0b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0b4:	ldr	x17, [x16, #2288]
    d0b8:	add	x16, x16, #0x8f0
    d0bc:	br	x17

000000000000d0c0 <__gmpz_fdiv_r_2exp@plt>:
    d0c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0c4:	ldr	x17, [x16, #2296]
    d0c8:	add	x16, x16, #0x8f8
    d0cc:	br	x17

000000000000d0d0 <__gmpz_inp_str@plt>:
    d0d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0d4:	ldr	x17, [x16, #2304]
    d0d8:	add	x16, x16, #0x900
    d0dc:	br	x17

000000000000d0e0 <__gmpn_toom_eval_pm2rexp@plt>:
    d0e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0e4:	ldr	x17, [x16, #2312]
    d0e8:	add	x16, x16, #0x908
    d0ec:	br	x17

000000000000d0f0 <__gmpn_redc_1@plt>:
    d0f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d0f4:	ldr	x17, [x16, #2320]
    d0f8:	add	x16, x16, #0x910
    d0fc:	br	x17

000000000000d100 <__isoc99_sscanf@plt>:
    d100:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d104:	ldr	x17, [x16, #2328]
    d108:	add	x16, x16, #0x918
    d10c:	br	x17

000000000000d110 <vsnprintf@plt>:
    d110:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d114:	ldr	x17, [x16, #2336]
    d118:	add	x16, x16, #0x920
    d11c:	br	x17

000000000000d120 <__gmpn_strongfibo@plt>:
    d120:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d124:	ldr	x17, [x16, #2344]
    d128:	add	x16, x16, #0x928
    d12c:	br	x17

000000000000d130 <__gmpz_init2@plt>:
    d130:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d134:	ldr	x17, [x16, #2352]
    d138:	add	x16, x16, #0x930
    d13c:	br	x17

000000000000d140 <__gmpn_gcdext_1@plt>:
    d140:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d144:	ldr	x17, [x16, #2360]
    d148:	add	x16, x16, #0x938
    d14c:	br	x17

000000000000d150 <__gmpn_scan1@plt>:
    d150:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d154:	ldr	x17, [x16, #2368]
    d158:	add	x16, x16, #0x940
    d15c:	br	x17

000000000000d160 <__gmpn_lshiftc@plt>:
    d160:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d164:	ldr	x17, [x16, #2376]
    d168:	add	x16, x16, #0x948
    d16c:	br	x17

000000000000d170 <__gmpn_mu_bdiv_qr_itch@plt>:
    d170:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d174:	ldr	x17, [x16, #2384]
    d178:	add	x16, x16, #0x950
    d17c:	br	x17

000000000000d180 <__gmpn_ni_invertappr@plt>:
    d180:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d184:	ldr	x17, [x16, #2392]
    d188:	add	x16, x16, #0x958
    d18c:	br	x17

000000000000d190 <__gmp_randinit_lc_2exp_size@plt>:
    d190:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d194:	ldr	x17, [x16, #2400]
    d198:	add	x16, x16, #0x960
    d19c:	br	x17

000000000000d1a0 <__gmp_init_primesieve@plt>:
    d1a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1a4:	ldr	x17, [x16, #2408]
    d1a8:	add	x16, x16, #0x968
    d1ac:	br	x17

000000000000d1b0 <__gmpn_gcdext_lehmer_n@plt>:
    d1b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1b4:	ldr	x17, [x16, #2416]
    d1b8:	add	x16, x16, #0x970
    d1bc:	br	x17

000000000000d1c0 <__gmpn_random2@plt>:
    d1c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1c4:	ldr	x17, [x16, #2424]
    d1c8:	add	x16, x16, #0x978
    d1cc:	br	x17

000000000000d1d0 <__gmpn_fft_next_size@plt>:
    d1d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1d4:	ldr	x17, [x16, #2432]
    d1d8:	add	x16, x16, #0x980
    d1dc:	br	x17

000000000000d1e0 <__gmpn_binvert_itch@plt>:
    d1e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1e4:	ldr	x17, [x16, #2440]
    d1e8:	add	x16, x16, #0x988
    d1ec:	br	x17

000000000000d1f0 <__gmpz_cmp_ui@plt>:
    d1f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d1f4:	ldr	x17, [x16, #2448]
    d1f8:	add	x16, x16, #0x990
    d1fc:	br	x17

000000000000d200 <__gmp_primesieve@plt>:
    d200:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d204:	ldr	x17, [x16, #2456]
    d208:	add	x16, x16, #0x998
    d20c:	br	x17

000000000000d210 <__gmpn_pow_1@plt>:
    d210:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d214:	ldr	x17, [x16, #2464]
    d218:	add	x16, x16, #0x9a0
    d21c:	br	x17

000000000000d220 <__gmpz_export@plt>:
    d220:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d224:	ldr	x17, [x16, #2472]
    d228:	add	x16, x16, #0x9a8
    d22c:	br	x17

000000000000d230 <__gmp_doprnt_mpf2@plt>:
    d230:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d234:	ldr	x17, [x16, #2480]
    d238:	add	x16, x16, #0x9b0
    d23c:	br	x17

000000000000d240 <__gmpn_mul_1c@plt>:
    d240:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d244:	ldr	x17, [x16, #2488]
    d248:	add	x16, x16, #0x9b8
    d24c:	br	x17

000000000000d250 <__gmpz_init@plt>:
    d250:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d254:	ldr	x17, [x16, #2496]
    d258:	add	x16, x16, #0x9c0
    d25c:	br	x17

000000000000d260 <__gmpz_sizeinbase@plt>:
    d260:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d264:	ldr	x17, [x16, #2504]
    d268:	add	x16, x16, #0x9c8
    d26c:	br	x17

000000000000d270 <__gmpz_set_si@plt>:
    d270:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d274:	ldr	x17, [x16, #2512]
    d278:	add	x16, x16, #0x9d0
    d27c:	br	x17

000000000000d280 <__gmp_extract_double@plt>:
    d280:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d284:	ldr	x17, [x16, #2520]
    d288:	add	x16, x16, #0x9d8
    d28c:	br	x17

000000000000d290 <__gmpn_mullo_basecase@plt>:
    d290:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d294:	ldr	x17, [x16, #2528]
    d298:	add	x16, x16, #0x9e0
    d29c:	br	x17

000000000000d2a0 <__gmpn_toom3_sqr@plt>:
    d2a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2a4:	ldr	x17, [x16, #2536]
    d2a8:	add	x16, x16, #0x9e8
    d2ac:	br	x17

000000000000d2b0 <__gmpn_gcd_subdiv_step@plt>:
    d2b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2b4:	ldr	x17, [x16, #2544]
    d2b8:	add	x16, x16, #0x9f0
    d2bc:	br	x17

000000000000d2c0 <__gmpz_powm_ui@plt>:
    d2c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2c4:	ldr	x17, [x16, #2552]
    d2c8:	add	x16, x16, #0x9f8
    d2cc:	br	x17

000000000000d2d0 <vfprintf@plt>:
    d2d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2d4:	ldr	x17, [x16, #2560]
    d2d8:	add	x16, x16, #0xa00
    d2dc:	br	x17

000000000000d2e0 <printf@plt>:
    d2e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2e4:	ldr	x17, [x16, #2568]
    d2e8:	add	x16, x16, #0xa08
    d2ec:	br	x17

000000000000d2f0 <__gmpn_hgcd_reduce@plt>:
    d2f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d2f4:	ldr	x17, [x16, #2576]
    d2f8:	add	x16, x16, #0xa10
    d2fc:	br	x17

000000000000d300 <__gmpn_dcpi1_bdiv_qr_n@plt>:
    d300:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d304:	ldr	x17, [x16, #2584]
    d308:	add	x16, x16, #0xa18
    d30c:	br	x17

000000000000d310 <putchar@plt>:
    d310:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d314:	ldr	x17, [x16, #2592]
    d318:	add	x16, x16, #0xa20
    d31c:	br	x17

000000000000d320 <__gmpz_addmul_ui@plt>:
    d320:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d324:	ldr	x17, [x16, #2600]
    d328:	add	x16, x16, #0xa28
    d32c:	br	x17

000000000000d330 <__gmpn_sqr_diag_addlsh1@plt>:
    d330:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d334:	ldr	x17, [x16, #2608]
    d338:	add	x16, x16, #0xa30
    d33c:	br	x17

000000000000d340 <__gmpn_gcd_11@plt>:
    d340:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d344:	ldr	x17, [x16, #2616]
    d348:	add	x16, x16, #0xa38
    d34c:	br	x17

000000000000d350 <__gmpn_toom_interpolate_16pts@plt>:
    d350:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d354:	ldr	x17, [x16, #2624]
    d358:	add	x16, x16, #0xa40
    d35c:	br	x17

000000000000d360 <__gmpn_divisible_p@plt>:
    d360:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d364:	ldr	x17, [x16, #2632]
    d368:	add	x16, x16, #0xa48
    d36c:	br	x17

000000000000d370 <__gmpn_sub_err2_n@plt>:
    d370:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d374:	ldr	x17, [x16, #2640]
    d378:	add	x16, x16, #0xa50
    d37c:	br	x17

000000000000d380 <__gmpn_bdiv_q_itch@plt>:
    d380:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d384:	ldr	x17, [x16, #2648]
    d388:	add	x16, x16, #0xa58
    d38c:	br	x17

000000000000d390 <__gmpn_hgcd_jacobi@plt>:
    d390:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d394:	ldr	x17, [x16, #2656]
    d398:	add	x16, x16, #0xa60
    d39c:	br	x17

000000000000d3a0 <__gmpn_divrem@plt>:
    d3a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3a4:	ldr	x17, [x16, #2664]
    d3a8:	add	x16, x16, #0xa68
    d3ac:	br	x17

000000000000d3b0 <__gmpn_sqrtrem@plt>:
    d3b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3b4:	ldr	x17, [x16, #2672]
    d3b8:	add	x16, x16, #0xa70
    d3bc:	br	x17

000000000000d3c0 <__gmpn_mu_bdiv_q@plt>:
    d3c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3c4:	ldr	x17, [x16, #2680]
    d3c8:	add	x16, x16, #0xa78
    d3cc:	br	x17

000000000000d3d0 <__gmp_exception@plt>:
    d3d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3d4:	ldr	x17, [x16, #2688]
    d3d8:	add	x16, x16, #0xa80
    d3dc:	br	x17

000000000000d3e0 <__gmpn_dcpi1_div_qr_n@plt>:
    d3e0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3e4:	ldr	x17, [x16, #2696]
    d3e8:	add	x16, x16, #0xa88
    d3ec:	br	x17

000000000000d3f0 <__gmpn_invert_limb@plt>:
    d3f0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d3f4:	ldr	x17, [x16, #2704]
    d3f8:	add	x16, x16, #0xa90
    d3fc:	br	x17

000000000000d400 <__gmpn_addmul_1@plt>:
    d400:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d404:	ldr	x17, [x16, #2712]
    d408:	add	x16, x16, #0xa98
    d40c:	br	x17

000000000000d410 <__gmpn_mod_1s_4p@plt>:
    d410:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d414:	ldr	x17, [x16, #2720]
    d418:	add	x16, x16, #0xaa0
    d41c:	br	x17

000000000000d420 <fprintf@plt>:
    d420:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d424:	ldr	x17, [x16, #2728]
    d428:	add	x16, x16, #0xaa8
    d42c:	br	x17

000000000000d430 <__gmpz_urandomb@plt>:
    d430:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d434:	ldr	x17, [x16, #2736]
    d438:	add	x16, x16, #0xab0
    d43c:	br	x17

000000000000d440 <__gmpn_hgcd_mul_matrix1_vector@plt>:
    d440:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d444:	ldr	x17, [x16, #2744]
    d448:	add	x16, x16, #0xab8
    d44c:	br	x17

000000000000d450 <__gmpn_toom22_mul@plt>:
    d450:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d454:	ldr	x17, [x16, #2752]
    d458:	add	x16, x16, #0xac0
    d45c:	br	x17

000000000000d460 <__gmp_mt_recalc_buffer@plt>:
    d460:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d464:	ldr	x17, [x16, #2760]
    d468:	add	x16, x16, #0xac8
    d46c:	br	x17

000000000000d470 <__gmpn_toom6_sqr@plt>:
    d470:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d474:	ldr	x17, [x16, #2768]
    d478:	add	x16, x16, #0xad0
    d47c:	br	x17

000000000000d480 <__gmpn_toom42_mul@plt>:
    d480:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d484:	ldr	x17, [x16, #2776]
    d488:	add	x16, x16, #0xad8
    d48c:	br	x17

000000000000d490 <__gmpn_mul_1@plt>:
    d490:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d494:	ldr	x17, [x16, #2784]
    d498:	add	x16, x16, #0xae0
    d49c:	br	x17

000000000000d4a0 <ferror@plt>:
    d4a0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d4a4:	ldr	x17, [x16, #2792]
    d4a8:	add	x16, x16, #0xae8
    d4ac:	br	x17

000000000000d4b0 <__gmpn_toom8_sqr@plt>:
    d4b0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d4b4:	ldr	x17, [x16, #2800]
    d4b8:	add	x16, x16, #0xaf0
    d4bc:	br	x17

000000000000d4c0 <__gmpf_div_2exp@plt>:
    d4c0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d4c4:	ldr	x17, [x16, #2808]
    d4c8:	add	x16, x16, #0xaf8
    d4cc:	br	x17

000000000000d4d0 <__gmpn_cnd_add_n@plt>:
    d4d0:	adrp	x16, 75000 <memcpy@GLIBC_2.17>
    d4d4:	ldr	x17, [x16, #2816]
    d4d8:	add	x16, x16, #0xb00
    d4dc:	br	x17

Disassembly of section .text:

000000000000d4e0 <__gmp_assert_header@@Base-0xd4>:
    d4e0:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d4e4:	ldr	x0, [x0, #3904]
    d4e8:	cbz	x0, d4f0 <__gmpn_cnd_add_n@plt+0x20>
    d4ec:	b	c8d0 <__gmon_start__@plt>
    d4f0:	ret
    d4f4:	nop
    d4f8:	adrp	x0, 75000 <memcpy@GLIBC_2.17>
    d4fc:	add	x0, x0, #0xb30
    d500:	adrp	x1, 75000 <memcpy@GLIBC_2.17>
    d504:	add	x1, x1, #0xb30
    d508:	cmp	x1, x0
    d50c:	b.eq	d524 <__gmpn_cnd_add_n@plt+0x54>  // b.none
    d510:	adrp	x1, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d514:	ldr	x1, [x1, #3784]
    d518:	cbz	x1, d524 <__gmpn_cnd_add_n@plt+0x54>
    d51c:	mov	x16, x1
    d520:	br	x16
    d524:	ret
    d528:	adrp	x0, 75000 <memcpy@GLIBC_2.17>
    d52c:	add	x0, x0, #0xb30
    d530:	adrp	x1, 75000 <memcpy@GLIBC_2.17>
    d534:	add	x1, x1, #0xb30
    d538:	sub	x1, x1, x0
    d53c:	lsr	x2, x1, #63
    d540:	add	x1, x2, x1, asr #3
    d544:	cmp	xzr, x1, asr #1
    d548:	asr	x1, x1, #1
    d54c:	b.eq	d564 <__gmpn_cnd_add_n@plt+0x94>  // b.none
    d550:	adrp	x2, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d554:	ldr	x2, [x2, #4056]
    d558:	cbz	x2, d564 <__gmpn_cnd_add_n@plt+0x94>
    d55c:	mov	x16, x2
    d560:	br	x16
    d564:	ret
    d568:	stp	x29, x30, [sp, #-32]!
    d56c:	mov	x29, sp
    d570:	str	x19, [sp, #16]
    d574:	adrp	x19, 75000 <memcpy@GLIBC_2.17>
    d578:	ldrb	w0, [x19, #2864]
    d57c:	cbnz	w0, d5a4 <__gmpn_cnd_add_n@plt+0xd4>
    d580:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d584:	ldr	x0, [x0, #3800]
    d588:	cbz	x0, d598 <__gmpn_cnd_add_n@plt+0xc8>
    d58c:	adrp	x0, 75000 <memcpy@GLIBC_2.17>
    d590:	ldr	x0, [x0, #2824]
    d594:	bl	c1d0 <__cxa_finalize@plt>
    d598:	bl	d4f8 <__gmpn_cnd_add_n@plt+0x28>
    d59c:	mov	w0, #0x1                   	// #1
    d5a0:	strb	w0, [x19, #2864]
    d5a4:	ldr	x19, [sp, #16]
    d5a8:	ldp	x29, x30, [sp], #32
    d5ac:	ret
    d5b0:	b	d528 <__gmpn_cnd_add_n@plt+0x58>

000000000000d5b4 <__gmp_assert_header@@Base>:
    d5b4:	stp	x29, x30, [sp, #-32]!
    d5b8:	stp	x20, x19, [sp, #16]
    d5bc:	mov	x29, sp
    d5c0:	cbz	x0, d5d0 <__gmp_assert_header@@Base+0x1c>
    d5c4:	ldrb	w8, [x0]
    d5c8:	mov	x2, x0
    d5cc:	cbnz	w8, d5dc <__gmp_assert_header@@Base+0x28>
    d5d0:	ldp	x20, x19, [sp, #16]
    d5d4:	ldp	x29, x30, [sp], #32
    d5d8:	ret
    d5dc:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d5e0:	ldr	x20, [x20, #3824]
    d5e4:	mov	w19, w1
    d5e8:	adrp	x1, 57000 <__gmp_randget_mt@@Base+0xc>
    d5ec:	add	x1, x1, #0xd60
    d5f0:	ldr	x0, [x20]
    d5f4:	bl	d420 <fprintf@plt>
    d5f8:	cmn	w19, #0x1
    d5fc:	b.eq	d5d0 <__gmp_assert_header@@Base+0x1c>  // b.none
    d600:	ldr	x0, [x20]
    d604:	mov	w2, w19
    d608:	ldp	x20, x19, [sp, #16]
    d60c:	adrp	x1, 57000 <__gmp_randget_mt@@Base+0xc>
    d610:	add	x1, x1, #0xd64
    d614:	ldp	x29, x30, [sp], #32
    d618:	b	d420 <fprintf@plt>

000000000000d61c <__gmp_assert_fail@@Base>:
    d61c:	stp	x29, x30, [sp, #-32]!
    d620:	str	x19, [sp, #16]
    d624:	mov	x29, sp
    d628:	mov	x19, x2
    d62c:	bl	cdb0 <__gmp_assert_header@plt>
    d630:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d634:	ldr	x8, [x8, #3824]
    d638:	adrp	x1, 57000 <__gmp_randget_mt@@Base+0xc>
    d63c:	add	x1, x1, #0xd69
    d640:	mov	x2, x19
    d644:	ldr	x0, [x8]
    d648:	bl	d420 <fprintf@plt>
    d64c:	bl	c900 <abort@plt>

000000000000d650 <__gmpn_divexact_by3@@Base>:
    d650:	stp	x29, x30, [sp, #-16]!
    d654:	mov	x3, #0x5555555555555555    	// #6148914691236517205
    d658:	mov	x4, xzr
    d65c:	mov	x29, sp
    d660:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
    d664:	and	x0, x0, #0x3
    d668:	ldp	x29, x30, [sp], #16
    d66c:	ret

000000000000d670 <__gmpn_divmod_1@@Base>:
    d670:	mov	x4, x3
    d674:	mov	x3, x2
    d678:	mov	x2, x1
    d67c:	mov	x1, xzr
    d680:	b	cd00 <__gmpn_divrem_1@plt>

000000000000d684 <__gmpz_legendre@@Base>:
    d684:	b	c070 <__gmpz_jacobi@plt>

000000000000d688 <__gmp_exception@@Base>:
    d688:	stp	x29, x30, [sp, #-16]!
    d68c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d690:	ldr	x8, [x8, #3896]
    d694:	mov	x29, sp
    d698:	ldr	w9, [x8]
    d69c:	orr	w9, w9, w0
    d6a0:	mov	w0, #0x8                   	// #8
    d6a4:	str	w9, [x8]
    d6a8:	bl	bfc0 <raise@plt>
    d6ac:	bl	c900 <abort@plt>

000000000000d6b0 <__gmp_sqrt_of_negative@@Base>:
    d6b0:	stp	x29, x30, [sp, #-16]!
    d6b4:	mov	w0, #0x4                   	// #4
    d6b8:	mov	x29, sp
    d6bc:	bl	d3d0 <__gmp_exception@plt>

000000000000d6c0 <__gmp_divide_by_zero@@Base>:
    d6c0:	stp	x29, x30, [sp, #-16]!
    d6c4:	mov	w0, #0x2                   	// #2
    d6c8:	mov	x29, sp
    d6cc:	bl	d3d0 <__gmp_exception@plt>

000000000000d6d0 <__gmp_extract_double@@Base>:
    d6d0:	fcmp	d0, #0.0
    d6d4:	b.ne	d6e8 <__gmp_extract_double@@Base+0x18>  // b.any
    d6d8:	mov	w8, wzr
    d6dc:	stp	xzr, xzr, [x0]
    d6e0:	mov	w0, w8
    d6e4:	ret
    d6e8:	fmov	x9, d0
    d6ec:	ubfx	x8, x9, #52, #11
    d6f0:	lsl	x9, x9, #11
    d6f4:	orr	x9, x9, #0x8000000000000000
    d6f8:	cbnz	x8, d70c <__gmp_extract_double@@Base+0x3c>
    d6fc:	mov	w8, #0x1                   	// #1
    d700:	lsl	x9, x9, #1
    d704:	sub	x8, x8, #0x1
    d708:	tbz	x9, #63, d700 <__gmp_extract_double@@Base+0x30>
    d70c:	add	x10, x8, #0xc02
    d710:	add	x11, x8, #0xc41
    d714:	cmp	x10, #0x0
    d718:	and	w8, w10, #0x3f
    d71c:	csel	x10, x11, x10, lt  // lt = tstop
    d720:	asr	x11, x10, #6
    d724:	cbz	w8, d73c <__gmp_extract_double@@Base+0x6c>
    d728:	neg	w12, w8
    d72c:	lsl	x10, x9, x8
    d730:	lsr	x9, x9, x12
    d734:	sub	x8, x11, #0x3f
    d738:	b	d744 <__gmp_extract_double@@Base+0x74>
    d73c:	mov	x10, xzr
    d740:	sub	x8, x11, #0x40
    d744:	stp	x10, x9, [x0]
    d748:	mov	w0, w8
    d74c:	ret

000000000000d750 <__gmp_invalid_operation@@Base>:
    d750:	stp	x29, x30, [sp, #-16]!
    d754:	mov	w0, #0x8                   	// #8
    d758:	mov	x29, sp
    d75c:	bl	bfc0 <raise@plt>
    d760:	bl	c900 <abort@plt>

000000000000d764 <__gmp_default_allocate@@Base>:
    d764:	stp	x29, x30, [sp, #-32]!
    d768:	str	x19, [sp, #16]
    d76c:	mov	x29, sp
    d770:	mov	x19, x0
    d774:	bl	c410 <malloc@plt>
    d778:	cbz	x0, d788 <__gmp_default_allocate@@Base+0x24>
    d77c:	ldr	x19, [sp, #16]
    d780:	ldp	x29, x30, [sp], #32
    d784:	ret
    d788:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d78c:	ldr	x8, [x8, #3824]
    d790:	adrp	x1, 57000 <__gmp_randget_mt@@Base+0xc>
    d794:	add	x1, x1, #0xd86
    d798:	mov	x2, x19
    d79c:	ldr	x0, [x8]
    d7a0:	bl	d420 <fprintf@plt>
    d7a4:	bl	c900 <abort@plt>

000000000000d7a8 <__gmp_default_reallocate@@Base>:
    d7a8:	stp	x29, x30, [sp, #-32]!
    d7ac:	stp	x20, x19, [sp, #16]
    d7b0:	mov	x20, x1
    d7b4:	mov	x1, x2
    d7b8:	mov	x29, sp
    d7bc:	mov	x19, x2
    d7c0:	bl	c7c0 <realloc@plt>
    d7c4:	cbz	x0, d7d4 <__gmp_default_reallocate@@Base+0x2c>
    d7c8:	ldp	x20, x19, [sp, #16]
    d7cc:	ldp	x29, x30, [sp], #32
    d7d0:	ret
    d7d4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d7d8:	ldr	x8, [x8, #3824]
    d7dc:	adrp	x1, 57000 <__gmp_randget_mt@@Base+0xc>
    d7e0:	add	x1, x1, #0xdb1
    d7e4:	mov	x2, x20
    d7e8:	ldr	x0, [x8]
    d7ec:	mov	x3, x19
    d7f0:	bl	d420 <fprintf@plt>
    d7f4:	bl	c900 <abort@plt>

000000000000d7f8 <__gmp_default_free@@Base>:
    d7f8:	b	cc30 <free@plt>

000000000000d7fc <__gmp_get_memory_functions@@Base>:
    d7fc:	cbz	x0, d810 <__gmp_get_memory_functions@@Base+0x14>
    d800:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d804:	ldr	x8, [x8, #3840]
    d808:	ldr	x8, [x8]
    d80c:	str	x8, [x0]
    d810:	cbz	x1, d824 <__gmp_get_memory_functions@@Base+0x28>
    d814:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d818:	ldr	x8, [x8, #3792]
    d81c:	ldr	x8, [x8]
    d820:	str	x8, [x1]
    d824:	cbz	x2, d838 <__gmp_get_memory_functions@@Base+0x3c>
    d828:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d82c:	ldr	x8, [x8, #4016]
    d830:	ldr	x8, [x8]
    d834:	str	x8, [x2]
    d838:	ret

000000000000d83c <__gmp_set_memory_functions@@Base>:
    d83c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d840:	ldr	x8, [x8, #4008]
    d844:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d848:	ldr	x9, [x9, #3840]
    d84c:	cmp	x0, #0x0
    d850:	csel	x8, x8, x0, eq  // eq = none
    d854:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d858:	str	x8, [x9]
    d85c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d860:	ldr	x8, [x8, #3912]
    d864:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d868:	ldr	x9, [x9, #4032]
    d86c:	ldr	x10, [x10, #3792]
    d870:	cmp	x1, #0x0
    d874:	csel	x8, x8, x1, eq  // eq = none
    d878:	cmp	x2, #0x0
    d87c:	str	x8, [x10]
    d880:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    d884:	ldr	x8, [x8, #4016]
    d888:	csel	x9, x9, x2, eq  // eq = none
    d88c:	str	x9, [x8]
    d890:	ret

000000000000d894 <__gmp_nextprime@@Base>:
    d894:	str	x19, [sp, #-16]!
    d898:	mov	x10, x0
    d89c:	ldr	x3, [x10], #31
    d8a0:	mov	x18, #0x2493                	// #9363
    d8a4:	movk	x18, #0x9249, lsl #16
    d8a8:	mov	x16, #0xffffffffffffffe7    	// #-25
    d8ac:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    d8b0:	mov	x15, #0xcccccccccccccccc    	// #-3689348814741910324
    d8b4:	movk	x18, #0x4924, lsl #32
    d8b8:	adrp	x1, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    d8bc:	add	x8, x0, #0x18
    d8c0:	add	x9, x0, #0x218
    d8c4:	add	x11, x0, #0x1d
    d8c8:	add	x12, x0, #0x1b
    d8cc:	movi	v0.2d, #0x0
    d8d0:	movk	x13, #0xaaab
    d8d4:	mov	w14, #0x1                   	// #1
    d8d8:	movk	x15, #0xcccd
    d8dc:	sub	x16, x16, x0
    d8e0:	mov	w17, #0x5                   	// #5
    d8e4:	movk	x18, #0x2492, lsl #48
    d8e8:	add	x1, x1, #0x4e
    d8ec:	mov	w2, #0x30                  	// #48
    d8f0:	b	d8fc <__gmp_nextprime@@Base+0x68>
    d8f4:	mov	x3, xzr
    d8f8:	str	xzr, [x0]
    d8fc:	add	x4, x0, x3
    d900:	ldrb	w4, [x4, #24]
    d904:	add	x3, x3, #0x1
    d908:	cbnz	w4, d8fc <__gmp_nextprime@@Base+0x68>
    d90c:	cmp	x3, #0x201
    d910:	b.ne	dc08 <__gmp_nextprime@@Base+0x374>  // b.any
    d914:	ldr	x3, [x0, #8]
    d918:	cmp	x3, #0x2
    d91c:	b.ls	dc2c <__gmp_nextprime@@Base+0x398>  // b.plast
    d920:	stp	q0, q0, [x8, #480]
    d924:	stp	q0, q0, [x8, #448]
    d928:	stp	q0, q0, [x8, #416]
    d92c:	stp	q0, q0, [x8, #384]
    d930:	stp	q0, q0, [x8, #352]
    d934:	stp	q0, q0, [x8, #320]
    d938:	stp	q0, q0, [x8, #288]
    d93c:	stp	q0, q0, [x8, #256]
    d940:	stp	q0, q0, [x8, #224]
    d944:	stp	q0, q0, [x8, #192]
    d948:	stp	q0, q0, [x8, #160]
    d94c:	stp	q0, q0, [x8, #128]
    d950:	stp	q0, q0, [x8, #96]
    d954:	stp	q0, q0, [x8, #64]
    d958:	stp	q0, q0, [x8, #32]
    d95c:	stp	q0, q0, [x8]
    d960:	ldr	x5, [x0, #16]
    d964:	add	x4, x3, #0x400
    d968:	str	x4, [x0, #8]
    d96c:	add	x6, x5, #0x1
    d970:	mul	x7, x6, x6
    d974:	add	x6, x3, #0x7ff
    d978:	cmp	x7, x6
    d97c:	b.hi	d998 <__gmp_nextprime@@Base+0x104>  // b.pmore
    d980:	add	x7, x5, #0x2
    d984:	mul	x7, x7, x7
    d988:	cmp	x7, x6
    d98c:	add	x5, x5, #0x1
    d990:	b.ls	d980 <__gmp_nextprime@@Base+0xec>  // b.plast
    d994:	str	x5, [x0, #16]
    d998:	add	x3, x3, #0x403
    d99c:	lsr	x3, x3, #1
    d9a0:	umulh	x5, x3, x13
    d9a4:	lsr	x5, x5, #1
    d9a8:	add	x5, x5, x5, lsl #1
    d9ac:	subs	x3, x3, x5
    d9b0:	eor	x3, x3, #0x3
    d9b4:	csel	x3, xzr, x3, eq  // eq = none
    d9b8:	add	x4, x4, x3, lsl #1
    d9bc:	add	x5, x3, #0x3
    d9c0:	cmp	x4, #0x4
    d9c4:	csel	x3, x5, x3, cc  // cc = lo, ul, last
    d9c8:	add	x4, x12, x3
    d9cc:	cmp	x4, x9
    d9d0:	csel	x5, x4, x9, hi  // hi = pmore
    d9d4:	add	x5, x5, x16
    d9d8:	add	x6, x0, x3
    d9dc:	sub	x5, x5, x3
    d9e0:	cmp	x5, #0x3
    d9e4:	add	x3, x6, #0x18
    d9e8:	b.cc	da20 <__gmp_nextprime@@Base+0x18c>  // b.lo, b.ul, b.last
    d9ec:	umulh	x5, x5, x13
    d9f0:	lsr	x5, x5, #1
    d9f4:	add	x5, x5, #0x1
    d9f8:	and	x6, x5, #0x7ffffffffffffffe
    d9fc:	add	x7, x6, x6, lsl #1
    da00:	add	x3, x3, x7
    da04:	mov	x7, x6
    da08:	sturb	w14, [x4, #-3]
    da0c:	strb	w14, [x4], #6
    da10:	subs	x7, x7, #0x2
    da14:	b.ne	da08 <__gmp_nextprime@@Base+0x174>  // b.any
    da18:	cmp	x5, x6
    da1c:	b.eq	da2c <__gmp_nextprime@@Base+0x198>  // b.none
    da20:	strb	w14, [x3], #3
    da24:	cmp	x3, x9
    da28:	b.cc	da20 <__gmp_nextprime@@Base+0x18c>  // b.lo, b.ul, b.last
    da2c:	ldr	x3, [x0, #8]
    da30:	add	x4, x3, #0x5
    da34:	lsr	x4, x4, #1
    da38:	umulh	x5, x4, x15
    da3c:	lsr	x5, x5, #2
    da40:	add	x5, x5, x5, lsl #2
    da44:	subs	x4, x4, x5
    da48:	sub	x4, x17, x4
    da4c:	csel	x4, xzr, x4, eq  // eq = none
    da50:	add	x5, x3, x4, lsl #1
    da54:	add	x6, x4, #0x5
    da58:	cmp	x5, #0x6
    da5c:	csel	x5, x6, x4, cc  // cc = lo, ul, last
    da60:	cmp	x5, #0x1ff
    da64:	b.gt	dad0 <__gmp_nextprime@@Base+0x23c>
    da68:	add	x4, x11, x5
    da6c:	cmp	x4, x9
    da70:	csel	x6, x4, x9, hi  // hi = pmore
    da74:	add	x6, x6, x16
    da78:	add	x3, x0, x5
    da7c:	sub	x5, x6, x5
    da80:	cmp	x5, #0x5
    da84:	add	x3, x3, #0x18
    da88:	b.cc	dac0 <__gmp_nextprime@@Base+0x22c>  // b.lo, b.ul, b.last
    da8c:	umulh	x5, x5, x15
    da90:	lsr	x5, x5, #2
    da94:	add	x5, x5, #0x1
    da98:	and	x6, x5, #0x7ffffffffffffffe
    da9c:	add	x7, x6, x6, lsl #2
    daa0:	add	x3, x3, x7
    daa4:	mov	x7, x6
    daa8:	sturb	w14, [x4, #-5]
    daac:	strb	w14, [x4], #10
    dab0:	subs	x7, x7, #0x2
    dab4:	b.ne	daa8 <__gmp_nextprime@@Base+0x214>  // b.any
    dab8:	cmp	x5, x6
    dabc:	b.eq	dacc <__gmp_nextprime@@Base+0x238>  // b.none
    dac0:	strb	w14, [x3], #5
    dac4:	cmp	x3, x9
    dac8:	b.cc	dac0 <__gmp_nextprime@@Base+0x22c>  // b.lo, b.ul, b.last
    dacc:	ldr	x3, [x0, #8]
    dad0:	add	x4, x3, #0x7
    dad4:	lsr	x4, x4, #1
    dad8:	umulh	x5, x4, x18
    dadc:	sub	x6, x4, x5
    dae0:	add	x5, x5, x6, lsr #1
    dae4:	lsr	x5, x5, #2
    dae8:	sub	x5, x5, x5, lsl #3
    daec:	adds	x4, x4, x5
    daf0:	eor	x4, x4, #0x7
    daf4:	csel	x4, xzr, x4, eq  // eq = none
    daf8:	add	x3, x3, x4, lsl #1
    dafc:	add	x5, x4, #0x7
    db00:	cmp	x3, #0x8
    db04:	csel	x3, x5, x4, cc  // cc = lo, ul, last
    db08:	add	x4, x10, x3
    db0c:	cmp	x4, x9
    db10:	csel	x5, x4, x9, hi  // hi = pmore
    db14:	add	x5, x5, x16
    db18:	add	x6, x0, x3
    db1c:	sub	x5, x5, x3
    db20:	cmp	x5, #0x6
    db24:	add	x3, x6, #0x18
    db28:	b.ls	db6c <__gmp_nextprime@@Base+0x2d8>  // b.plast
    db2c:	umulh	x6, x5, x18
    db30:	sub	x5, x5, x6
    db34:	add	x5, x6, x5, lsr #1
    db38:	lsr	x5, x5, #2
    db3c:	add	x5, x5, #0x1
    db40:	and	x6, x5, #0x7ffffffffffffffe
    db44:	lsl	x7, x6, #3
    db48:	sub	x7, x7, x6
    db4c:	add	x3, x3, x7
    db50:	mov	x7, x6
    db54:	sturb	w14, [x4, #-7]
    db58:	strb	w14, [x4], #14
    db5c:	subs	x7, x7, #0x2
    db60:	b.ne	db54 <__gmp_nextprime@@Base+0x2c0>  // b.any
    db64:	cmp	x5, x6
    db68:	b.eq	db78 <__gmp_nextprime@@Base+0x2e4>  // b.none
    db6c:	strb	w14, [x3], #7
    db70:	cmp	x3, x9
    db74:	b.cc	db6c <__gmp_nextprime@@Base+0x2d8>  // b.lo, b.ul, b.last
    db78:	ldr	x4, [x0, #16]
    db7c:	cmp	x4, #0xb
    db80:	b.cc	d8f4 <__gmp_nextprime@@Base+0x60>  // b.lo, b.ul, b.last
    db84:	mov	x5, xzr
    db88:	mov	w3, #0xb                   	// #11
    db8c:	b	dbb0 <__gmp_nextprime@@Base+0x31c>
    db90:	ldrb	w6, [x1, x5]
    db94:	add	x5, x5, #0x1
    db98:	umulh	x7, x5, x13
    db9c:	add	x3, x3, x6
    dba0:	lsr	x6, x7, #5
    dba4:	cmp	x3, x4
    dba8:	msub	x5, x6, x2, x5
    dbac:	b.hi	d8f4 <__gmp_nextprime@@Base+0x60>  // b.pmore
    dbb0:	ldr	x6, [x0, #8]
    dbb4:	add	x7, x6, x3
    dbb8:	lsr	x7, x7, #1
    dbbc:	udiv	x19, x7, x3
    dbc0:	msub	x7, x19, x3, x7
    dbc4:	sub	x19, x3, x7
    dbc8:	cmp	x7, #0x0
    dbcc:	csel	x7, xzr, x19, eq  // eq = none
    dbd0:	add	x6, x6, x7, lsl #1
    dbd4:	cmp	x6, x3
    dbd8:	csel	x6, xzr, x3, hi  // hi = pmore
    dbdc:	add	x6, x6, x7
    dbe0:	cmp	x6, #0x1ff
    dbe4:	b.gt	db90 <__gmp_nextprime@@Base+0x2fc>
    dbe8:	add	x4, x0, x6
    dbec:	add	x4, x4, #0x18
    dbf0:	strb	w14, [x4]
    dbf4:	add	x4, x4, x3
    dbf8:	cmp	x4, x9
    dbfc:	b.cc	dbf0 <__gmp_nextprime@@Base+0x35c>  // b.lo, b.ul, b.last
    dc00:	ldr	x4, [x0, #16]
    dc04:	b	db90 <__gmp_nextprime@@Base+0x2fc>
    dc08:	ldr	x10, [x0, #8]
    dc0c:	add	x9, x0, x3
    dc10:	sub	x8, x9, x8
    dc14:	add	x9, x8, #0x18
    dc18:	add	x8, x10, x8, lsl #1
    dc1c:	str	x9, [x0]
    dc20:	add	x0, x8, #0x2e
    dc24:	ldr	x19, [sp], #16
    dc28:	ret
    dc2c:	mov	x8, #0xfffffffffffffc03    	// #-1021
    dc30:	str	x8, [x0, #8]
    dc34:	mov	w0, #0x2                   	// #2
    dc38:	ldr	x19, [sp], #16
    dc3c:	ret

000000000000dc40 <__gmp_init_primesieve@@Base>:
    dc40:	mov	w8, #0x200                 	// #512
    dc44:	stp	xzr, xzr, [x0, #8]
    dc48:	str	x8, [x0]
    dc4c:	strb	wzr, [x0, #536]
    dc50:	ret

000000000000dc54 <__gmp_primesieve@@Base>:
    dc54:	sub	sp, sp, #0x70
    dc58:	sub	x8, x1, #0x5
    dc5c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    dc60:	movk	x9, #0xaaab
    dc64:	orr	x8, x8, #0x1
    dc68:	umulh	x9, x8, x9
    dc6c:	stp	x22, x21, [sp, #80]
    dc70:	lsr	x21, x9, #7
    dc74:	stp	x20, x19, [sp, #96]
    dc78:	mov	x19, x0
    dc7c:	lsr	x10, x9, #1
    dc80:	cmp	x8, #0xc0, lsl #12
    dc84:	add	x8, x21, #0x1
    dc88:	stp	x29, x30, [sp, #16]
    dc8c:	stp	x28, x27, [sp, #32]
    dc90:	stp	x26, x25, [sp, #48]
    dc94:	stp	x24, x23, [sp, #64]
    dc98:	add	x29, sp, #0x10
    dc9c:	stp	x10, x8, [sp]
    dca0:	b.cc	e0f8 <__gmp_primesieve@@Base+0x4a4>  // b.lo, b.ul, b.last
    dca4:	mov	w28, #0x800                 	// #2048
    dca8:	mov	x23, #0x184                 	// #388
    dcac:	mov	x24, #0x2058                	// #8280
    dcb0:	mov	x25, #0x2120                	// #8480
    dcb4:	bfxil	x28, x8, #0, #11
    dcb8:	movk	x23, #0x4023, lsl #16
    dcbc:	movk	x24, #0x489, lsl #16
    dcc0:	movk	x25, #0x8840, lsl #16
    dcc4:	mov	x27, #0x1244                	// #4676
    dcc8:	add	x8, x28, x28, lsl #1
    dccc:	mov	w1, #0x1                   	// #1
    dcd0:	movk	x23, #0x180c, lsl #32
    dcd4:	movk	x24, #0x4a12, lsl #32
    dcd8:	movk	x25, #0x210, lsl #32
    dcdc:	movk	x27, #0x3068, lsl #16
    dce0:	bfi	x1, x8, #6, #14
    dce4:	mov	x0, x19
    dce8:	movk	x23, #0x9402, lsl #48
    dcec:	movk	x24, #0x8121, lsl #48
    dcf0:	movk	x25, #0x285, lsl #48
    dcf4:	movk	x27, #0xc81, lsl #32
    dcf8:	mov	w22, #0x1                   	// #1
    dcfc:	bl	e15c <__gmp_primesieve@@Base+0x508>
    dd00:	add	x8, x19, x28, lsl #3
    dd04:	mov	w10, #0x6e                  	// #110
    dd08:	mov	w12, #0xb6                  	// #182
    dd0c:	mov	w14, #0x40                  	// #64
    dd10:	b	dd24 <__gmp_primesieve@@Base+0xd0>
    dd14:	add	x28, x28, #0x800
    dd18:	cmp	x28, x21
    dd1c:	add	x8, x8, #0x4, lsl #12
    dd20:	b.hi	e100 <__gmp_primesieve@@Base+0x4ac>  // b.pmore
    dd24:	mov	x9, #0x7905                	// #30981
    dd28:	lsl	x18, x28, #6
    dd2c:	movk	x9, #0x904a, lsl #16
    dd30:	sub	x4, x18, #0x40
    dd34:	movk	x9, #0x4a7, lsl #32
    dd38:	lsr	x3, x4, #1
    dd3c:	movk	x9, #0x4a79, lsl #48
    dd40:	mov	x1, #0x2058                	// #8280
    dd44:	umulh	x15, x3, x9
    dd48:	movk	x1, #0x489, lsl #16
    dd4c:	mov	x2, #0x1244                	// #4676
    dd50:	lsr	x15, x15, #4
    dd54:	movk	x1, #0x4a12, lsl #32
    dd58:	movk	x2, #0x3068, lsl #16
    dd5c:	msub	x15, x15, x10, x4
    dd60:	movk	x1, #0x8121, lsl #48
    dd64:	movk	x2, #0xc81, lsl #32
    dd68:	cbz	x15, ddd4 <__gmp_primesieve@@Base+0x180>
    dd6c:	cmp	x15, #0x3f
    dd70:	b.hi	dda0 <__gmp_primesieve@@Base+0x14c>  // b.pmore
    dd74:	neg	x16, x15
    dd78:	lsr	x17, x24, x15
    dd7c:	lsl	x0, x27, x16
    dd80:	subs	x16, x15, #0x2e
    dd84:	orr	x1, x0, x17
    dd88:	b.hi	ddc4 <__gmp_primesieve@@Base+0x170>  // b.pmore
    dd8c:	mov	w9, #0x2e                  	// #46
    dd90:	sub	x16, x9, x15
    dd94:	lsl	x16, x24, x16
    dd98:	lsr	x15, x27, x15
    dd9c:	b	ddbc <__gmp_primesieve@@Base+0x168>
    dda0:	sub	x16, x10, x15
    dda4:	lsr	x17, x27, x15
    dda8:	sub	x15, x15, #0x2e
    ddac:	lsl	x0, x24, x16
    ddb0:	lsl	x16, x27, x16
    ddb4:	lsr	x15, x24, x15
    ddb8:	orr	x1, x0, x17
    ddbc:	orr	x2, x16, x15
    ddc0:	b	ddd4 <__gmp_primesieve@@Base+0x180>
    ddc4:	sub	x15, x10, x15
    ddc8:	lsl	x15, x24, x15
    ddcc:	orr	x1, x1, x15
    ddd0:	lsr	x2, x24, x16
    ddd4:	mov	x9, #0x2d03                	// #11523
    ddd8:	movk	x9, #0x2d0, lsl #16
    dddc:	movk	x9, #0xd02d, lsl #32
    dde0:	movk	x9, #0x2d02, lsl #48
    dde4:	umulh	x15, x3, x9
    dde8:	lsr	x15, x15, #4
    ddec:	mov	x3, #0x184                 	// #388
    ddf0:	msub	x6, x15, x12, x4
    ddf4:	mov	x4, #0x2120                	// #8480
    ddf8:	movk	x3, #0x4023, lsl #16
    ddfc:	movk	x4, #0x8840, lsl #16
    de00:	mov	x5, #0x4421                	// #17441
    de04:	movk	x3, #0x180c, lsl #32
    de08:	movk	x4, #0x210, lsl #32
    de0c:	movk	x5, #0x1008, lsl #16
    de10:	add	x0, x19, x28, lsl #3
    de14:	movk	x3, #0x9402, lsl #48
    de18:	movk	x4, #0x285, lsl #48
    de1c:	movk	x5, #0xa412, lsl #32
    de20:	cbz	x6, df4c <__gmp_primesieve@@Base+0x2f8>
    de24:	subs	x15, x6, #0x40
    de28:	b.hi	de80 <__gmp_primesieve@@Base+0x22c>  // b.pmore
    de2c:	mov	x9, #0x4421                	// #17441
    de30:	movk	x9, #0x1008, lsl #16
    de34:	neg	x15, x6
    de38:	lsr	x16, x23, x6
    de3c:	lsr	x17, x25, x6
    de40:	cmp	x6, #0x40
    de44:	movk	x9, #0xa412, lsl #32
    de48:	lsl	x3, x25, x15
    de4c:	lsl	x4, x9, x15
    de50:	csel	x16, xzr, x16, eq  // eq = none
    de54:	csel	x17, xzr, x17, eq  // eq = none
    de58:	subs	x15, x6, #0x36
    de5c:	orr	x3, x3, x16
    de60:	orr	x4, x4, x17
    de64:	b.hi	dedc <__gmp_primesieve@@Base+0x288>  // b.pmore
    de68:	mov	w11, #0x36                  	// #54
    de6c:	sub	x15, x11, x6
    de70:	lsl	x15, x23, x15
    de74:	lsr	x16, x9, x6
    de78:	orr	x5, x15, x16
    de7c:	b	df4c <__gmp_primesieve@@Base+0x2f8>
    de80:	cmp	x6, #0x7f
    de84:	b.hi	def4 <__gmp_primesieve@@Base+0x2a0>  // b.pmore
    de88:	mov	x9, #0x4421                	// #17441
    de8c:	movk	x9, #0x1008, lsl #16
    de90:	neg	x16, x6
    de94:	movk	x9, #0xa412, lsl #32
    de98:	lsr	x17, x25, x6
    de9c:	lsl	x3, x9, x16
    dea0:	subs	x16, x6, #0x76
    dea4:	orr	x3, x17, x3
    dea8:	b.hi	df30 <__gmp_primesieve@@Base+0x2dc>  // b.pmore
    deac:	lsr	x15, x9, x15
    deb0:	mov	w9, #0x76                  	// #118
    deb4:	sub	x16, x9, x6
    deb8:	lsl	x17, x23, x16
    debc:	cmp	x6, #0x76
    dec0:	orr	x4, x15, x17
    dec4:	lsl	x5, x25, x16
    dec8:	b.eq	df4c <__gmp_primesieve@@Base+0x2f8>  // b.none
    decc:	sub	x15, x6, #0x36
    ded0:	lsr	x15, x23, x15
    ded4:	orr	x5, x5, x15
    ded8:	b	df4c <__gmp_primesieve@@Base+0x2f8>
    dedc:	mov	w9, #0x76                  	// #118
    dee0:	sub	x16, x9, x6
    dee4:	lsl	x16, x23, x16
    dee8:	orr	x4, x4, x16
    deec:	lsr	x5, x23, x15
    def0:	b	df4c <__gmp_primesieve@@Base+0x2f8>
    def4:	mov	x9, #0x4421                	// #17441
    def8:	movk	x9, #0x1008, lsl #16
    defc:	sub	x15, x12, x6
    df00:	movk	x9, #0xa412, lsl #32
    df04:	sub	x17, x6, #0x76
    df08:	lsr	x16, x9, x6
    df0c:	lsl	x3, x23, x15
    df10:	lsl	x4, x25, x15
    df14:	lsr	x5, x23, x17
    df18:	lsl	x15, x9, x15
    df1c:	lsr	x17, x25, x17
    df20:	orr	x3, x3, x16
    df24:	orr	x4, x4, x5
    df28:	orr	x5, x15, x17
    df2c:	b	df4c <__gmp_primesieve@@Base+0x2f8>
    df30:	sub	x15, x12, x6
    df34:	lsr	x17, x23, x16
    df38:	lsl	x4, x23, x15
    df3c:	lsl	x15, x25, x15
    df40:	orr	x3, x3, x4
    df44:	orr	x4, x15, x17
    df48:	lsr	x5, x25, x16
    df4c:	mov	x6, xzr
    df50:	orr	x17, x2, x1, lsl #46
    df54:	add	x15, x8, x6
    df58:	orr	x16, x3, x1
    df5c:	extr	x1, x2, x1, #18
    df60:	lsr	x7, x4, #10
    df64:	add	x6, x6, #0x10
    df68:	orr	x2, x4, x17
    df6c:	lsr	x17, x17, #18
    df70:	extr	x4, x4, x3, #10
    df74:	cmp	x6, #0x4, lsl #12
    df78:	stp	x16, x2, [x15]
    df7c:	orr	x3, x5, x3, lsl #54
    df80:	mov	x2, x17
    df84:	mov	x5, x7
    df88:	b.ne	df50 <__gmp_primesieve@@Base+0x2fc>  // b.any
    df8c:	mov	w9, #0x1ffff               	// #131071
    df90:	mov	x1, xzr
    df94:	add	x2, x18, x9
    df98:	mov	w15, #0x4                   	// #4
    df9c:	mov	w3, #0x10                  	// #16
    dfa0:	b	dfb4 <__gmp_primesieve@@Base+0x360>
    dfa4:	ror	x9, x3, #63
    dfa8:	add	x1, x1, x3, lsr #63
    dfac:	mov	x15, x4
    dfb0:	mov	x3, x9
    dfb4:	ldr	x16, [x19, x1, lsl #3]
    dfb8:	add	x4, x15, #0x1
    dfbc:	tst	x16, x3
    dfc0:	b.ne	dfa4 <__gmp_primesieve@@Base+0x350>  // b.any
    dfc4:	add	x7, x4, x4, lsl #1
    dfc8:	and	x30, x4, #0x1
    dfcc:	add	x15, x15, #0x2
    dfd0:	add	x16, x7, x30
    dfd4:	neg	x17, x30
    dfd8:	add	x5, x16, #0x2
    dfdc:	and	x15, x15, x17
    dfe0:	madd	x15, x5, x4, x15
    dfe4:	sub	x15, x15, #0x1
    dfe8:	cmp	x15, x2
    dfec:	b.gt	dd14 <__gmp_primesieve@@Base+0xc0>
    dff0:	add	x16, x16, #0x1
    dff4:	lsl	x5, x16, #1
    dff8:	add	x16, x5, #0x3f
    dffc:	cmp	x5, #0x0
    e000:	csel	x16, x16, x5, lt  // lt = tstop
    e004:	and	x16, x16, #0xffffffffffffffc0
    e008:	cmp	x15, x18
    e00c:	sub	x6, x5, x16
    e010:	b.ge	e028 <__gmp_primesieve@@Base+0x3d4>  // b.tcont
    e014:	mvn	x16, x15
    e018:	add	x16, x18, x16
    e01c:	sdiv	x16, x16, x5
    e020:	add	x16, x16, #0x1
    e024:	madd	x15, x16, x5, x15
    e028:	sub	x15, x15, x18
    e02c:	cmp	x15, #0x20, lsl #12
    e030:	b.ge	e07c <__gmp_primesieve@@Base+0x428>  // b.tcont
    e034:	sub	w20, w14, w6
    e038:	lsl	x17, x22, x15
    e03c:	and	x16, x6, #0xfffffffe
    e040:	and	x20, x20, #0xfffffffe
    e044:	add	x26, x15, #0x3f
    e048:	cmp	x15, #0x0
    e04c:	csel	x26, x26, x15, lt  // lt = tstop
    e050:	asr	x26, x26, #6
    e054:	lsl	x26, x26, #3
    e058:	ldr	x11, [x0, x26]
    e05c:	lsl	x9, x17, x16
    e060:	lsr	x13, x17, x20
    e064:	add	x15, x15, x5
    e068:	cmp	x15, #0x20, lsl #12
    e06c:	orr	x11, x11, x17
    e070:	orr	x17, x9, x13
    e074:	str	x11, [x0, x26]
    e078:	b.lt	e044 <__gmp_primesieve@@Base+0x3f0>  // b.tstop
    e07c:	add	x9, x7, #0x6
    e080:	madd	x15, x9, x4, x30
    e084:	cmp	x15, x18
    e088:	b.ge	e0a0 <__gmp_primesieve@@Base+0x44c>  // b.tcont
    e08c:	mvn	x9, x15
    e090:	add	x9, x18, x9
    e094:	sdiv	x9, x9, x5
    e098:	add	x9, x9, #0x1
    e09c:	madd	x15, x9, x5, x15
    e0a0:	sub	x15, x15, x18
    e0a4:	cmp	x15, #0x20, lsl #12
    e0a8:	b.ge	dfa4 <__gmp_primesieve@@Base+0x350>  // b.tcont
    e0ac:	sub	w9, w14, w6
    e0b0:	lsl	x17, x22, x15
    e0b4:	and	x16, x6, #0xfffffffe
    e0b8:	and	x6, x9, #0xfffffffe
    e0bc:	add	x9, x15, #0x3f
    e0c0:	cmp	x15, #0x0
    e0c4:	csel	x9, x9, x15, lt  // lt = tstop
    e0c8:	asr	x9, x9, #6
    e0cc:	lsl	x9, x9, #3
    e0d0:	ldr	x13, [x0, x9]
    e0d4:	lsl	x11, x17, x16
    e0d8:	lsr	x7, x17, x6
    e0dc:	add	x15, x15, x5
    e0e0:	cmp	x15, #0x20, lsl #12
    e0e4:	orr	x13, x13, x17
    e0e8:	orr	x17, x11, x7
    e0ec:	str	x13, [x0, x9]
    e0f0:	b.lt	e0bc <__gmp_primesieve@@Base+0x468>  // b.tstop
    e0f4:	b	dfa4 <__gmp_primesieve@@Base+0x350>
    e0f8:	mov	x0, x19
    e0fc:	bl	e15c <__gmp_primesieve@@Base+0x508>
    e100:	ldr	x8, [sp]
    e104:	add	w8, w8, #0x1
    e108:	ands	x8, x8, #0x3f
    e10c:	b.eq	e128 <__gmp_primesieve@@Base+0x4d4>  // b.none
    e110:	lsl	x9, x21, #3
    e114:	ldr	x10, [x19, x9]
    e118:	mov	x11, #0xffffffffffffffff    	// #-1
    e11c:	lsl	x8, x11, x8
    e120:	orr	x8, x10, x8
    e124:	str	x8, [x19, x9]
    e128:	ldr	x1, [sp, #8]
    e12c:	mov	x0, x19
    e130:	lsl	x20, x1, #6
    e134:	bl	cd80 <__gmpn_popcount@plt>
    e138:	sub	x0, x20, x0
    e13c:	ldp	x20, x19, [sp, #96]
    e140:	ldp	x22, x21, [sp, #80]
    e144:	ldp	x24, x23, [sp, #64]
    e148:	ldp	x26, x25, [sp, #48]
    e14c:	ldp	x28, x27, [sp, #32]
    e150:	ldp	x29, x30, [sp, #16]
    e154:	add	sp, sp, #0x70
    e158:	ret
    e15c:	sub	x8, x1, #0x5
    e160:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
    e164:	movk	x9, #0xaaab
    e168:	orr	x10, x8, #0x1
    e16c:	umulh	x11, x8, x9
    e170:	umulh	x8, x10, x9
    e174:	cmp	x10, #0xc0
    e178:	lsr	x8, x8, #1
    e17c:	lsr	x9, x11, #7
    e180:	b.cc	e218 <__gmp_primesieve@@Base+0x5c4>  // b.lo, b.ul, b.last
    e184:	mov	x11, #0x2120                	// #8480
    e188:	mov	x12, #0x184                 	// #388
    e18c:	mov	x14, #0x2058                	// #8280
    e190:	mov	x13, #0x4421                	// #17441
    e194:	movk	x11, #0x8840, lsl #16
    e198:	movk	x12, #0x4023, lsl #16
    e19c:	mov	x16, #0x1244                	// #4676
    e1a0:	movk	x14, #0x489, lsl #16
    e1a4:	movk	x13, #0x1008, lsl #16
    e1a8:	movk	x11, #0x210, lsl #32
    e1ac:	movk	x12, #0x180c, lsl #32
    e1b0:	movk	x16, #0x3068, lsl #16
    e1b4:	movk	x14, #0x4a12, lsl #32
    e1b8:	add	x10, x0, #0x8
    e1bc:	movk	x13, #0xa412, lsl #32
    e1c0:	movk	x11, #0x285, lsl #48
    e1c4:	movk	x12, #0x9402, lsl #48
    e1c8:	movk	x16, #0xc81, lsl #32
    e1cc:	movk	x14, #0x8121, lsl #48
    e1d0:	mov	x15, x9
    e1d4:	orr	x17, x12, x14
    e1d8:	cmp	x15, #0x1
    e1dc:	str	x17, [x10]
    e1e0:	b.eq	e218 <__gmp_primesieve@@Base+0x5c4>  // b.none
    e1e4:	orr	x17, x16, x14, lsl #46
    e1e8:	extr	x14, x16, x14, #18
    e1ec:	lsr	x18, x11, #10
    e1f0:	orr	x16, x11, x17
    e1f4:	lsr	x17, x17, #18
    e1f8:	subs	x15, x15, #0x2
    e1fc:	extr	x11, x11, x12, #10
    e200:	orr	x12, x13, x12, lsl #54
    e204:	str	x16, [x10, #8]
    e208:	add	x10, x10, #0x10
    e20c:	mov	x16, x17
    e210:	mov	x13, x18
    e214:	b.ne	e1d4 <__gmp_primesieve@@Base+0x580>  // b.any
    e218:	mov	x11, #0x8480                	// #33920
    e21c:	movk	x11, #0x6912, lsl #16
    e220:	movk	x11, #0xc9e0, lsl #32
    e224:	add	w10, w8, #0x1
    e228:	movk	x11, #0x3294, lsl #48
    e22c:	ands	x10, x10, #0x3f
    e230:	str	x11, [x0]
    e234:	b.eq	e250 <__gmp_primesieve@@Base+0x5fc>  // b.none
    e238:	lsl	x9, x9, #3
    e23c:	ldr	x11, [x0, x9]
    e240:	mov	x12, #0xffffffffffffffff    	// #-1
    e244:	lsl	x10, x12, x10
    e248:	orr	x10, x11, x10
    e24c:	str	x10, [x0, x9]
    e250:	cmp	x1, #0xd3
    e254:	b.cc	e36c <__gmp_primesieve@@Base+0x718>  // b.lo, b.ul, b.last
    e258:	mov	x9, xzr
    e25c:	mov	w14, #0x4                   	// #4
    e260:	mov	w11, #0x10                  	// #16
    e264:	mov	w10, #0x1                   	// #1
    e268:	mov	w12, #0x40                  	// #64
    e26c:	b	e280 <__gmp_primesieve@@Base+0x62c>
    e270:	ror	x14, x11, #63
    e274:	add	x9, x9, x11, lsr #63
    e278:	mov	x11, x14
    e27c:	mov	x14, x13
    e280:	ldr	x13, [x0, x9, lsl #3]
    e284:	tst	x13, x11
    e288:	add	x13, x14, #0x1
    e28c:	b.ne	e270 <__gmp_primesieve@@Base+0x61c>  // b.any
    e290:	add	x17, x13, x13, lsl #1
    e294:	and	x18, x13, #0x1
    e298:	add	x15, x14, #0x2
    e29c:	add	x14, x17, x18
    e2a0:	neg	x16, x18
    e2a4:	add	x1, x14, #0x2
    e2a8:	and	x15, x15, x16
    e2ac:	madd	x15, x1, x13, x15
    e2b0:	sub	x1, x15, #0x1
    e2b4:	cmp	x1, x8
    e2b8:	b.gt	e36c <__gmp_primesieve@@Base+0x718>
    e2bc:	add	x14, x14, #0x1
    e2c0:	lsl	x14, x14, #1
    e2c4:	add	x15, x14, #0x3f
    e2c8:	cmp	x14, #0x0
    e2cc:	csel	x15, x15, x14, lt  // lt = tstop
    e2d0:	and	x15, x15, #0xffffffffffffffc0
    e2d4:	sub	x16, x14, x15
    e2d8:	lsl	x2, x10, x1
    e2dc:	and	x15, x16, #0xfffffffe
    e2e0:	sub	w16, w12, w16
    e2e4:	add	x3, x1, #0x3f
    e2e8:	cmp	x1, #0x0
    e2ec:	csel	x3, x3, x1, lt  // lt = tstop
    e2f0:	asr	x3, x3, #6
    e2f4:	lsl	x3, x3, #3
    e2f8:	ldr	x5, [x0, x3]
    e2fc:	lsl	x4, x2, x15
    e300:	lsr	x6, x2, x16
    e304:	add	x1, x1, x14
    e308:	orr	x2, x5, x2
    e30c:	cmp	x1, x8
    e310:	str	x2, [x0, x3]
    e314:	orr	x2, x4, x6
    e318:	b.le	e2e4 <__gmp_primesieve@@Base+0x690>
    e31c:	add	x17, x17, #0x6
    e320:	madd	x17, x17, x13, x18
    e324:	cmp	x17, x8
    e328:	b.gt	e270 <__gmp_primesieve@@Base+0x61c>
    e32c:	lsl	x18, x10, x17
    e330:	add	x1, x17, #0x3f
    e334:	cmp	x17, #0x0
    e338:	csel	x1, x1, x17, lt  // lt = tstop
    e33c:	asr	x1, x1, #6
    e340:	lsl	x1, x1, #3
    e344:	ldr	x3, [x0, x1]
    e348:	lsl	x2, x18, x15
    e34c:	lsr	x4, x18, x16
    e350:	add	x17, x17, x14
    e354:	orr	x18, x3, x18
    e358:	cmp	x17, x8
    e35c:	str	x18, [x0, x1]
    e360:	orr	x18, x2, x4
    e364:	b.le	e330 <__gmp_primesieve@@Base+0x6dc>
    e368:	b	e270 <__gmp_primesieve@@Base+0x61c>
    e36c:	ret

000000000000e370 <__gmp_tmp_reentrant_alloc@@Base>:
    e370:	stp	x29, x30, [sp, #-32]!
    e374:	stp	x20, x19, [sp, #16]
    e378:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e37c:	ldr	x8, [x8, #3840]
    e380:	add	x20, x1, #0x10
    e384:	mov	x19, x0
    e388:	mov	x0, x20
    e38c:	ldr	x8, [x8]
    e390:	mov	x29, sp
    e394:	blr	x8
    e398:	str	x20, [x0, #8]
    e39c:	ldr	x9, [x19]
    e3a0:	add	x8, x0, #0x10
    e3a4:	str	x9, [x0]
    e3a8:	str	x0, [x19]
    e3ac:	ldp	x20, x19, [sp, #16]
    e3b0:	mov	x0, x8
    e3b4:	ldp	x29, x30, [sp], #32
    e3b8:	ret

000000000000e3bc <__gmp_tmp_reentrant_free@@Base>:
    e3bc:	stp	x29, x30, [sp, #-32]!
    e3c0:	stp	x20, x19, [sp, #16]
    e3c4:	mov	x29, sp
    e3c8:	cbz	x0, e3e8 <__gmp_tmp_reentrant_free@@Base+0x2c>
    e3cc:	adrp	x19, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e3d0:	ldr	x19, [x19, #4016]
    e3d4:	ldr	x8, [x19]
    e3d8:	ldp	x20, x1, [x0]
    e3dc:	blr	x8
    e3e0:	mov	x0, x20
    e3e4:	cbnz	x20, e3d4 <__gmp_tmp_reentrant_free@@Base+0x18>
    e3e8:	ldp	x20, x19, [sp, #16]
    e3ec:	ldp	x29, x30, [sp], #32
    e3f0:	ret

000000000000e3f4 <__gmpf_init@@Base>:
    e3f4:	stp	x29, x30, [sp, #-32]!
    e3f8:	str	x19, [sp, #16]
    e3fc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e400:	ldr	x8, [x8, #3960]
    e404:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e408:	mov	x19, x0
    e40c:	mov	x29, sp
    e410:	ldr	x8, [x8]
    e414:	str	xzr, [x0, #8]
    e418:	stp	w8, wzr, [x0]
    e41c:	ldr	x9, [x9, #3840]
    e420:	lsl	x8, x8, #3
    e424:	add	x0, x8, #0x8
    e428:	ldr	x9, [x9]
    e42c:	blr	x9
    e430:	str	x0, [x19, #16]
    e434:	ldr	x19, [sp, #16]
    e438:	ldp	x29, x30, [sp], #32
    e43c:	ret

000000000000e440 <__gmpf_init2@@Base>:
    e440:	stp	x29, x30, [sp, #-32]!
    e444:	cmp	x1, #0x35
    e448:	mov	w8, #0x35                  	// #53
    e44c:	csel	x8, x1, x8, hi  // hi = pmore
    e450:	add	x8, x8, #0x7f
    e454:	lsr	x8, x8, #6
    e458:	str	x19, [sp, #16]
    e45c:	str	xzr, [x0, #8]
    e460:	stp	w8, wzr, [x0]
    e464:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e468:	ldr	x9, [x9, #3840]
    e46c:	lsl	x8, x8, #3
    e470:	mov	x19, x0
    e474:	add	x0, x8, #0x8
    e478:	ldr	x9, [x9]
    e47c:	mov	x29, sp
    e480:	blr	x9
    e484:	str	x0, [x19, #16]
    e488:	ldr	x19, [sp, #16]
    e48c:	ldp	x29, x30, [sp], #32
    e490:	ret

000000000000e494 <__gmpf_inits@@Base>:
    e494:	sub	sp, sp, #0xf0
    e498:	stp	x29, x30, [sp, #224]
    e49c:	add	x29, sp, #0xe0
    e4a0:	mov	x8, #0xffffffffffffffc8    	// #-56
    e4a4:	mov	x9, sp
    e4a8:	sub	x10, x29, #0x58
    e4ac:	movk	x8, #0xff80, lsl #32
    e4b0:	add	x11, x29, #0x10
    e4b4:	add	x9, x9, #0x80
    e4b8:	add	x10, x10, #0x38
    e4bc:	stp	x1, x2, [x29, #-88]
    e4c0:	stp	x3, x4, [x29, #-72]
    e4c4:	stp	x5, x6, [x29, #-56]
    e4c8:	stur	x7, [x29, #-40]
    e4cc:	stp	q0, q1, [sp]
    e4d0:	stp	q2, q3, [sp, #32]
    e4d4:	stp	q4, q5, [sp, #64]
    e4d8:	stp	q6, q7, [sp, #96]
    e4dc:	stp	x9, x8, [x29, #-16]
    e4e0:	stp	x11, x10, [x29, #-32]
    e4e4:	b	e4fc <__gmpf_inits@@Base+0x68>
    e4e8:	ldur	x8, [x29, #-32]
    e4ec:	add	x9, x8, #0x8
    e4f0:	stur	x9, [x29, #-32]
    e4f4:	ldr	x0, [x8]
    e4f8:	cbz	x0, e528 <__gmpf_inits@@Base+0x94>
    e4fc:	bl	ce60 <__gmpf_init@plt>
    e500:	ldursw	x8, [x29, #-8]
    e504:	tbz	w8, #31, e4e8 <__gmpf_inits@@Base+0x54>
    e508:	add	w9, w8, #0x8
    e50c:	cmn	w8, #0x8
    e510:	stur	w9, [x29, #-8]
    e514:	b.gt	e4e8 <__gmpf_inits@@Base+0x54>
    e518:	ldur	x9, [x29, #-24]
    e51c:	add	x8, x9, x8
    e520:	ldr	x0, [x8]
    e524:	cbnz	x0, e4fc <__gmpf_inits@@Base+0x68>
    e528:	ldp	x29, x30, [sp, #224]
    e52c:	add	sp, sp, #0xf0
    e530:	ret

000000000000e534 <__gmpf_set@@Base>:
    e534:	ldrsw	x10, [x1, #4]
    e538:	ldrsw	x9, [x0]
    e53c:	ldp	x12, x11, [x1, #8]
    e540:	ldr	x8, [x0, #16]
    e544:	cmp	x10, #0x0
    e548:	add	x13, x9, #0x1
    e54c:	str	x12, [x0, #8]
    e550:	cneg	x12, x10, mi  // mi = first
    e554:	subs	x13, x12, x13
    e558:	add	x13, x11, x13, lsl #3
    e55c:	csinc	x2, x12, x9, le
    e560:	csel	x1, x13, x11, gt
    e564:	neg	w9, w2
    e568:	cmp	w10, #0x0
    e56c:	csel	x9, x2, x9, ge  // ge = tcont
    e570:	str	w9, [x0, #4]
    e574:	mov	x0, x8
    e578:	b	ca50 <__gmpn_copyi@plt>

000000000000e57c <__gmpf_set_ui@@Base>:
    e57c:	ldr	x8, [x0, #16]
    e580:	cmp	x1, #0x0
    e584:	cset	w9, ne  // ne = any
    e588:	str	x1, [x8]
    e58c:	str	w9, [x0, #4]
    e590:	str	x9, [x0, #8]
    e594:	ret

000000000000e598 <__gmpf_set_si@@Base>:
    e598:	ldr	x8, [x0, #16]
    e59c:	cmp	x1, #0x0
    e5a0:	cset	w10, ne  // ne = any
    e5a4:	csetm	x11, ne  // ne = any
    e5a8:	cneg	x9, x1, mi  // mi = first
    e5ac:	csel	x11, x10, x11, ge  // ge = tcont
    e5b0:	str	x9, [x8]
    e5b4:	str	x10, [x0, #8]
    e5b8:	str	w11, [x0, #4]
    e5bc:	ret

000000000000e5c0 <__gmpf_set_str@@Base>:
    e5c0:	stp	x29, x30, [sp, #-96]!
    e5c4:	stp	x28, x27, [sp, #16]
    e5c8:	stp	x26, x25, [sp, #32]
    e5cc:	stp	x24, x23, [sp, #48]
    e5d0:	stp	x22, x21, [sp, #64]
    e5d4:	stp	x20, x19, [sp, #80]
    e5d8:	mov	x29, sp
    e5dc:	sub	sp, sp, #0x40
    e5e0:	mov	x19, x0
    e5e4:	mov	w0, #0x10000               	// #65536
    e5e8:	mov	w22, w2
    e5ec:	mov	x20, x1
    e5f0:	bl	c400 <nl_langinfo@plt>
    e5f4:	mov	x21, x0
    e5f8:	bl	bf60 <strlen@plt>
    e5fc:	mov	x23, x0
    e600:	bl	cae0 <__ctype_b_loc@plt>
    e604:	ldr	x9, [x0]
    e608:	mov	x24, x0
    e60c:	ldrb	w8, [x20], #1
    e610:	ldrh	w10, [x9, x8, lsl #1]
    e614:	tbnz	w10, #13, e60c <__gmpf_set_str@@Base+0x4c>
    e618:	cmp	w8, #0x2d
    e61c:	b.ne	e62c <__gmpf_set_str@@Base+0x6c>  // b.any
    e620:	ldrb	w8, [x20]
    e624:	mov	w28, #0x1                   	// #1
    e628:	b	e634 <__gmpf_set_str@@Base+0x74>
    e62c:	mov	w28, wzr
    e630:	sub	x20, x20, #0x1
    e634:	cmp	w22, #0x0
    e638:	mov	w9, #0xa                   	// #10
    e63c:	adrp	x26, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e640:	csel	w10, w9, w22, eq  // eq = none
    e644:	ldr	x26, [x26, #3920]
    e648:	cmp	w10, #0x0
    e64c:	cneg	w22, w10, mi  // mi = first
    e650:	csel	w13, w9, w10, lt  // lt = tstop
    e654:	cmp	w22, #0x25
    e658:	b.lt	e668 <__gmpf_set_str@@Base+0xa8>  // b.tstop
    e65c:	cmp	w22, #0x3e
    e660:	b.gt	e904 <__gmpf_set_str@@Base+0x344>
    e664:	add	x26, x26, #0xd0
    e668:	ldrb	w9, [x26, w8, uxtw]
    e66c:	cmp	w22, w9
    e670:	b.gt	e6bc <__gmpf_set_str@@Base+0xfc>
    e674:	cbz	x23, e6ac <__gmpf_set_str@@Base+0xec>
    e678:	ldrb	w9, [x21]
    e67c:	cmp	w8, w9
    e680:	b.ne	e904 <__gmpf_set_str@@Base+0x344>  // b.any
    e684:	add	x8, x21, #0x1
    e688:	sub	x9, x23, #0x1
    e68c:	add	x10, x20, #0x1
    e690:	cbz	x9, e6ac <__gmpf_set_str@@Base+0xec>
    e694:	ldrb	w11, [x10], #1
    e698:	ldrb	w12, [x8], #1
    e69c:	sub	x9, x9, #0x1
    e6a0:	cmp	w11, w12
    e6a4:	b.eq	e690 <__gmpf_set_str@@Base+0xd0>  // b.none
    e6a8:	b	e904 <__gmpf_set_str@@Base+0x344>
    e6ac:	ldrb	w8, [x20, x23]
    e6b0:	ldrb	w8, [x26, x8]
    e6b4:	cmp	w22, w8
    e6b8:	b.le	e904 <__gmpf_set_str@@Base+0x344>
    e6bc:	mov	x0, x20
    e6c0:	stur	x13, [x29, #-32]
    e6c4:	stur	x19, [x29, #-16]
    e6c8:	bl	bf60 <strlen@plt>
    e6cc:	mov	x25, x0
    e6d0:	mov	x9, x0
    e6d4:	subs	x8, x9, #0x1
    e6d8:	b.eq	e710 <__gmpf_set_str@@Base+0x150>  // b.none
    e6dc:	add	x27, x20, x9
    e6e0:	ldurb	w10, [x27, #-1]
    e6e4:	cmp	w10, #0x40
    e6e8:	b.eq	e708 <__gmpf_set_str@@Base+0x148>  // b.none
    e6ec:	cmp	w22, #0xa
    e6f0:	mov	x9, x8
    e6f4:	b.gt	e6d4 <__gmpf_set_str@@Base+0x114>
    e6f8:	orr	w9, w10, #0x20
    e6fc:	cmp	w9, #0x65
    e700:	mov	x9, x8
    e704:	b.ne	e6d4 <__gmpf_set_str@@Base+0x114>  // b.any
    e708:	mov	x25, x8
    e70c:	b	e714 <__gmpf_set_str@@Base+0x154>
    e710:	mov	x27, xzr
    e714:	add	x1, x25, #0x1
    e718:	mov	w8, #0x7f01                	// #32513
    e71c:	cmp	x1, x8
    e720:	stur	xzr, [x29, #-8]
    e724:	stur	w28, [x29, #-20]
    e728:	b.cs	ecb8 <__gmpf_set_str@@Base+0x6f8>  // b.hs, b.nlast
    e72c:	add	x9, x1, #0xf
    e730:	mov	x8, sp
    e734:	and	x9, x9, #0xfffffffffffffff0
    e738:	sub	x1, x8, x9
    e73c:	mov	sp, x1
    e740:	cbz	x25, e7fc <__gmpf_set_str@@Base+0x23c>
    e744:	mov	x8, xzr
    e748:	mov	x19, xzr
    e74c:	mov	w9, wzr
    e750:	mov	x0, xzr
    e754:	sub	x10, x23, #0x1
    e758:	add	x11, x21, #0x1
    e75c:	mov	w12, #0x1                   	// #1
    e760:	mov	x28, x1
    e764:	b	e788 <__gmpf_set_str@@Base+0x1c8>
    e768:	cbnz	x19, e8fc <__gmpf_set_str@@Base+0x33c>
    e76c:	add	x20, x20, x10
    e770:	add	x8, x8, x10
    e774:	mov	x19, x28
    e778:	add	x8, x8, #0x1
    e77c:	cmp	x8, x25
    e780:	add	x20, x20, #0x1
    e784:	b.cs	e808 <__gmpf_set_str@@Base+0x248>  // b.hs, b.nlast
    e788:	ldrb	w13, [x20]
    e78c:	ldr	x14, [x24]
    e790:	ldrh	w14, [x14, x13, lsl #1]
    e794:	tbnz	w14, #13, e778 <__gmpf_set_str@@Base+0x1b8>
    e798:	cbz	x23, e768 <__gmpf_set_str@@Base+0x1a8>
    e79c:	ldrb	w14, [x21]
    e7a0:	cmp	w13, w14
    e7a4:	b.ne	e7cc <__gmpf_set_str@@Base+0x20c>  // b.any
    e7a8:	add	x14, x20, #0x1
    e7ac:	mov	x15, x10
    e7b0:	mov	x16, x11
    e7b4:	cbz	x15, e768 <__gmpf_set_str@@Base+0x1a8>
    e7b8:	ldrb	w17, [x14], #1
    e7bc:	ldrb	w18, [x16], #1
    e7c0:	sub	x15, x15, #0x1
    e7c4:	cmp	w17, w18
    e7c8:	b.eq	e7b4 <__gmpf_set_str@@Base+0x1f4>  // b.none
    e7cc:	ldrb	w13, [x26, x13]
    e7d0:	cmp	w22, w13
    e7d4:	b.le	e8fc <__gmpf_set_str@@Base+0x33c>
    e7d8:	cmp	w13, #0x0
    e7dc:	strb	w13, [x28]
    e7e0:	cset	w13, ne  // ne = any
    e7e4:	orr	w9, w9, w13
    e7e8:	add	x28, x28, w9, sxtw
    e7ec:	cbz	x19, e778 <__gmpf_set_str@@Base+0x1b8>
    e7f0:	sub	w13, w12, w9
    e7f4:	add	x0, x0, w13, sxtw
    e7f8:	b	e778 <__gmpf_set_str@@Base+0x1b8>
    e7fc:	mov	x0, xzr
    e800:	mov	x19, xzr
    e804:	mov	x28, x1
    e808:	ldur	x25, [x29, #-16]
    e80c:	subs	x21, x28, x1
    e810:	b.eq	e90c <__gmpf_set_str@@Base+0x34c>  // b.none
    e814:	stur	x0, [x29, #-40]
    e818:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    e81c:	ldrsw	x8, [x25]
    e820:	ldr	x9, [x9, #3936]
    e824:	mov	w10, #0x28                  	// #40
    e828:	add	x20, x8, #0x1
    e82c:	umaddl	x9, w22, w10, x9
    e830:	ldr	x9, [x9, #16]
    e834:	umulh	x8, x9, x21
    e838:	and	x8, x8, #0x1ffffffffffffff8
    e83c:	mov	w9, #0x7ef0                	// #32496
    e840:	cmp	x8, x9
    e844:	add	x8, x8, #0x10
    e848:	b.hi	ecd8 <__gmpf_set_str@@Base+0x718>  // b.pmore
    e84c:	add	x8, x8, #0xf
    e850:	mov	x9, sp
    e854:	and	x8, x8, #0x7ffffffffffffff0
    e858:	sub	x23, x9, x8
    e85c:	mov	sp, x23
    e860:	mov	x0, x23
    e864:	mov	x2, x21
    e868:	mov	w3, w22
    e86c:	bl	c090 <__gmpn_set_str@plt>
    e870:	subs	x8, x0, x20
    e874:	add	x9, x23, x8, lsl #3
    e878:	mov	x24, x0
    e87c:	csel	x2, x20, x0, gt
    e880:	csel	x23, x9, x23, gt
    e884:	csel	x21, x8, xzr, gt
    e888:	cbz	x27, e918 <__gmpf_set_str@@Base+0x358>
    e88c:	ldrb	w9, [x27]
    e890:	cmp	w9, #0x2d
    e894:	cset	w10, eq  // eq = none
    e898:	csetm	x8, eq  // eq = none
    e89c:	cmp	w9, #0x2b
    e8a0:	cset	w9, eq  // eq = none
    e8a4:	orr	w12, w10, w9
    e8a8:	add	x11, x27, x12
    e8ac:	ldrb	w9, [x11]
    e8b0:	ldur	x10, [x29, #-32]
    e8b4:	ldrb	w9, [x26, x9]
    e8b8:	sxtw	x10, w10
    e8bc:	cmp	x9, x10
    e8c0:	b.ge	e8fc <__gmpf_set_str@@Base+0x33c>  // b.tcont
    e8c4:	ldrb	w11, [x11, #1]
    e8c8:	ldrb	w11, [x26, x11]
    e8cc:	cmp	x11, x10
    e8d0:	b.ge	e8f0 <__gmpf_set_str@@Base+0x330>  // b.tcont
    e8d4:	add	x12, x12, x27
    e8d8:	add	x12, x12, #0x2
    e8dc:	ldrb	w13, [x12], #1
    e8e0:	madd	x9, x9, x10, x11
    e8e4:	ldrb	w11, [x26, x13]
    e8e8:	cmp	x11, x10
    e8ec:	b.lt	e8dc <__gmpf_set_str@@Base+0x31c>  // b.tstop
    e8f0:	eor	x9, x9, x8
    e8f4:	sub	x8, x9, x8
    e8f8:	b	e91c <__gmpf_set_str@@Base+0x35c>
    e8fc:	ldur	x0, [x29, #-8]
    e900:	cbnz	x0, ecf4 <__gmpf_set_str@@Base+0x734>
    e904:	mov	w0, #0xffffffff            	// #-1
    e908:	b	ec98 <__gmpf_set_str@@Base+0x6d8>
    e90c:	str	wzr, [x25, #4]
    e910:	str	xzr, [x25, #8]
    e914:	b	ec8c <__gmpf_set_str@@Base+0x6cc>
    e918:	mov	x8, xzr
    e91c:	ldur	x9, [x29, #-40]
    e920:	cmp	x19, #0x0
    e924:	sub	x9, x9, x19
    e928:	add	x9, x9, x28
    e92c:	csel	x9, xzr, x9, eq  // eq = none
    e930:	subs	x28, x8, x9
    e934:	cneg	x27, x28, mi  // mi = first
    e938:	cbz	x27, ea10 <__gmpf_set_str@@Base+0x450>
    e93c:	add	x25, x20, #0x1
    e940:	lsl	x1, x25, #5
    e944:	mov	w8, #0x7f00                	// #32512
    e948:	cmp	x1, x8
    e94c:	mov	w24, w22
    e950:	stur	x2, [x29, #-40]
    e954:	b.hi	ed00 <__gmpf_set_str@@Base+0x740>  // b.pmore
    e958:	add	x9, x1, #0xf
    e95c:	mov	x8, sp
    e960:	and	x9, x9, #0xfffffffffffffff0
    e964:	sub	x26, x8, x9
    e968:	mov	sp, x26
    e96c:	clz	x9, x27
    e970:	cmp	x9, #0x3f
    e974:	stur	x23, [x29, #-32]
    e978:	stp	x26, x21, [x29, #-56]
    e97c:	str	x24, [x26]
    e980:	stur	x25, [x29, #-64]
    e984:	b.ne	ea3c <__gmpf_set_str@@Base+0x47c>  // b.any
    e988:	mov	x8, xzr
    e98c:	mov	x19, xzr
    e990:	mov	w25, #0x1                   	// #1
    e994:	subs	x9, x25, x20
    e998:	add	x10, x26, x9, lsl #3
    e99c:	csel	x9, x9, xzr, gt
    e9a0:	add	x27, x9, x19
    e9a4:	csel	x9, x10, x26, gt
    e9a8:	ldur	x26, [x29, #-56]
    e9ac:	csel	x24, x20, x25, gt
    e9b0:	add	x1, x9, x8, lsl #3
    e9b4:	mov	x2, x24
    e9b8:	mov	x0, x26
    e9bc:	bl	ca50 <__gmpn_copyi@plt>
    e9c0:	tbnz	x28, #63, eaec <__gmpf_set_str@@Base+0x52c>
    e9c4:	ldp	x21, x4, [x29, #-48]
    e9c8:	mov	w8, #0x7f00                	// #32512
    e9cc:	add	x19, x24, x4
    e9d0:	lsl	x1, x19, #3
    e9d4:	cmp	x1, x8
    e9d8:	b.hi	ed10 <__gmpf_set_str@@Base+0x750>  // b.pmore
    e9dc:	add	x9, x1, #0xf
    e9e0:	mov	x8, sp
    e9e4:	and	x9, x9, #0xfffffffffffffff0
    e9e8:	sub	x25, x8, x9
    e9ec:	mov	sp, x25
    e9f0:	ldur	w22, [x29, #-20]
    e9f4:	ldur	x3, [x29, #-32]
    e9f8:	mov	x0, x25
    e9fc:	cmp	x24, x4
    ea00:	b.le	eb64 <__gmpf_set_str@@Base+0x5a4>
    ea04:	mov	x1, x26
    ea08:	mov	x2, x24
    ea0c:	b	eb74 <__gmpf_set_str@@Base+0x5b4>
    ea10:	ldr	x0, [x25, #16]
    ea14:	mov	x1, x23
    ea18:	mov	x19, x2
    ea1c:	bl	ca50 <__gmpn_copyi@plt>
    ea20:	ldur	w9, [x29, #-20]
    ea24:	neg	w8, w19
    ea28:	str	x24, [x25, #8]
    ea2c:	cmp	w9, #0x0
    ea30:	csel	x8, x19, x8, eq  // eq = none
    ea34:	str	w8, [x25, #4]
    ea38:	b	ec8c <__gmpf_set_str@@Base+0x6cc>
    ea3c:	lsl	x10, x25, #1
    ea40:	mov	w11, #0x3e                  	// #62
    ea44:	mov	w12, #0x3f                  	// #63
    ea48:	mov	x19, xzr
    ea4c:	mov	x8, xzr
    ea50:	add	x22, x26, x10, lsl #3
    ea54:	sub	w23, w11, w9
    ea58:	sub	w21, w12, w9
    ea5c:	mov	w25, #0x1                   	// #1
    ea60:	mov	x9, x26
    ea64:	b	ea7c <__gmpf_set_str@@Base+0x4bc>
    ea68:	sub	w21, w21, #0x1
    ea6c:	cmp	w21, #0x0
    ea70:	sub	x23, x23, #0x1
    ea74:	mov	x9, x26
    ea78:	b.le	e994 <__gmpf_set_str@@Base+0x3d4>
    ea7c:	mov	x26, x22
    ea80:	add	x1, x9, x8, lsl #3
    ea84:	mov	x0, x26
    ea88:	mov	x2, x25
    ea8c:	mov	x22, x9
    ea90:	bl	c8e0 <__gmpn_sqr@plt>
    ea94:	add	x8, x26, x25, lsl #4
    ea98:	ldur	x8, [x8, #-8]
    ea9c:	lsl	x9, x25, #1
    eaa0:	cmp	x8, #0x0
    eaa4:	cset	w8, eq  // eq = none
    eaa8:	sub	x9, x9, x8
    eaac:	subs	x8, x9, x20
    eab0:	csel	x8, x8, xzr, gt
    eab4:	csel	x25, x20, x9, gt
    eab8:	lsr	x9, x27, x23
    eabc:	add	x19, x8, x19, lsl #1
    eac0:	tbz	w9, #0, ea68 <__gmpf_set_str@@Base+0x4a8>
    eac4:	add	x1, x26, x8, lsl #3
    eac8:	mov	x0, x26
    eacc:	mov	x2, x25
    ead0:	mov	x3, x24
    ead4:	bl	d490 <__gmpn_mul_1@plt>
    ead8:	cmp	x0, #0x0
    eadc:	mov	x8, xzr
    eae0:	str	x0, [x26, x25, lsl #3]
    eae4:	cinc	x25, x25, ne  // ne = any
    eae8:	b	ea68 <__gmpf_set_str@@Base+0x4a8>
    eaec:	ldp	x23, x10, [x29, #-48]
    eaf0:	cmp	x24, x10
    eaf4:	b.le	ebac <__gmpf_set_str@@Base+0x5ec>
    eaf8:	lsl	x19, x24, #3
    eafc:	add	x1, x19, #0x8
    eb00:	mov	w8, #0x7f00                	// #32512
    eb04:	cmp	x1, x8
    eb08:	b.hi	ed3c <__gmpf_set_str@@Base+0x77c>  // b.pmore
    eb0c:	add	x9, x1, #0xf
    eb10:	mov	x8, sp
    eb14:	and	x9, x9, #0xfffffffffffffff0
    eb18:	sub	x25, x8, x9
    eb1c:	mov	sp, x25
    eb20:	ldur	w22, [x29, #-20]
    eb24:	subs	x21, x24, x10
    eb28:	b.eq	eb40 <__gmpf_set_str@@Base+0x580>  // b.none
    eb2c:	sub	x2, x19, x10, lsl #3
    eb30:	mov	x0, x25
    eb34:	mov	w1, wzr
    eb38:	bl	c5f0 <memset@plt>
    eb3c:	ldur	x10, [x29, #-40]
    eb40:	ldur	x1, [x29, #-32]
    eb44:	add	x8, x25, x24, lsl #3
    eb48:	sub	x0, x8, x10, lsl #3
    eb4c:	mov	x2, x10
    eb50:	bl	ca50 <__gmpn_copyi@plt>
    eb54:	sub	x23, x23, x21
    eb58:	mov	x10, x24
    eb5c:	stur	x25, [x29, #-32]
    eb60:	b	ebb0 <__gmpf_set_str@@Base+0x5f0>
    eb64:	mov	x1, x3
    eb68:	mov	x2, x4
    eb6c:	mov	x3, x26
    eb70:	mov	x4, x24
    eb74:	bl	ccd0 <__gmpn_mul@plt>
    eb78:	add	x8, x25, x19, lsl #3
    eb7c:	ldur	x8, [x8, #-8]
    eb80:	add	x10, x27, x21
    eb84:	cmp	x8, #0x0
    eb88:	cset	w8, eq  // eq = none
    eb8c:	sub	x8, x19, x8
    eb90:	subs	x9, x8, x20
    eb94:	add	x19, x10, x8
    eb98:	b.le	eba4 <__gmpf_set_str@@Base+0x5e4>
    eb9c:	add	x25, x25, x9, lsl #3
    eba0:	b	ec64 <__gmpf_set_str@@Base+0x6a4>
    eba4:	mov	x20, x8
    eba8:	b	ec64 <__gmpf_set_str@@Base+0x6a4>
    ebac:	ldur	w22, [x29, #-20]
    ebb0:	add	x8, x26, x24, lsl #3
    ebb4:	ldur	x8, [x8, #-8]
    ebb8:	tbnz	x8, #63, ec04 <__gmpf_set_str@@Base+0x644>
    ebbc:	clz	x25, x8
    ebc0:	mov	x0, x26
    ebc4:	mov	x1, x26
    ebc8:	mov	x2, x24
    ebcc:	mov	w3, w25
    ebd0:	mov	x19, x10
    ebd4:	bl	c180 <__gmpn_lshift@plt>
    ebd8:	ldur	x21, [x29, #-32]
    ebdc:	mov	x2, x19
    ebe0:	mov	w3, w25
    ebe4:	mov	x0, x21
    ebe8:	mov	x1, x21
    ebec:	bl	c180 <__gmpn_lshift@plt>
    ebf0:	cbz	x0, ec00 <__gmpf_set_str@@Base+0x640>
    ebf4:	add	x10, x19, #0x1
    ebf8:	str	x0, [x21, x19, lsl #3]
    ebfc:	b	ec04 <__gmpf_set_str@@Base+0x644>
    ec00:	mov	x10, x19
    ec04:	ldur	x8, [x29, #-64]
    ec08:	lsl	x1, x8, #3
    ec0c:	mov	w8, #0x7f00                	// #32512
    ec10:	cmp	x1, x8
    ec14:	b.hi	ed24 <__gmpf_set_str@@Base+0x764>  // b.pmore
    ec18:	add	x9, x1, #0xf
    ec1c:	mov	x8, sp
    ec20:	and	x9, x9, #0xfffffffffffffff0
    ec24:	sub	x25, x8, x9
    ec28:	mov	sp, x25
    ec2c:	ldur	x2, [x29, #-32]
    ec30:	sub	x19, x10, x24
    ec34:	sub	x1, x20, x19
    ec38:	mov	x0, x25
    ec3c:	mov	x3, x10
    ec40:	mov	x4, x26
    ec44:	mov	x5, x24
    ec48:	bl	d3a0 <__gmpn_divrem@plt>
    ec4c:	sub	x8, x23, x27
    ec50:	add	x8, x19, x8
    ec54:	add	x19, x8, x0
    ec58:	cbz	x0, ec64 <__gmpf_set_str@@Base+0x6a4>
    ec5c:	str	x0, [x25, x20, lsl #3]
    ec60:	add	x25, x25, #0x8
    ec64:	ldur	x21, [x29, #-16]
    ec68:	mov	x1, x25
    ec6c:	mov	x2, x20
    ec70:	ldr	x0, [x21, #16]
    ec74:	bl	ca50 <__gmpn_copyi@plt>
    ec78:	neg	w8, w20
    ec7c:	cmp	w22, #0x0
    ec80:	csel	x8, x20, x8, eq  // eq = none
    ec84:	str	w8, [x21, #4]
    ec88:	str	x19, [x21, #8]
    ec8c:	ldur	x8, [x29, #-8]
    ec90:	mov	w0, wzr
    ec94:	cbnz	x8, ecc8 <__gmpf_set_str@@Base+0x708>
    ec98:	mov	sp, x29
    ec9c:	ldp	x20, x19, [sp, #80]
    eca0:	ldp	x22, x21, [sp, #64]
    eca4:	ldp	x24, x23, [sp, #48]
    eca8:	ldp	x26, x25, [sp, #32]
    ecac:	ldp	x28, x27, [sp, #16]
    ecb0:	ldp	x29, x30, [sp], #96
    ecb4:	ret
    ecb8:	sub	x0, x29, #0x8
    ecbc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ecc0:	mov	x1, x0
    ecc4:	b	e744 <__gmpf_set_str@@Base+0x184>
    ecc8:	mov	x0, x8
    eccc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    ecd0:	mov	w0, wzr
    ecd4:	b	ec98 <__gmpf_set_str@@Base+0x6d8>
    ecd8:	sub	x0, x29, #0x8
    ecdc:	mov	x23, x1
    ece0:	mov	x1, x8
    ece4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ece8:	mov	x1, x23
    ecec:	mov	x23, x0
    ecf0:	b	e860 <__gmpf_set_str@@Base+0x2a0>
    ecf4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    ecf8:	mov	w0, #0xffffffff            	// #-1
    ecfc:	b	ec98 <__gmpf_set_str@@Base+0x6d8>
    ed00:	sub	x0, x29, #0x8
    ed04:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed08:	mov	x26, x0
    ed0c:	b	e96c <__gmpf_set_str@@Base+0x3ac>
    ed10:	sub	x0, x29, #0x8
    ed14:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed18:	ldur	x4, [x29, #-40]
    ed1c:	mov	x25, x0
    ed20:	b	e9f0 <__gmpf_set_str@@Base+0x430>
    ed24:	sub	x0, x29, #0x8
    ed28:	mov	x19, x10
    ed2c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed30:	mov	x10, x19
    ed34:	mov	x25, x0
    ed38:	b	ec2c <__gmpf_set_str@@Base+0x66c>
    ed3c:	sub	x0, x29, #0x8
    ed40:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ed44:	ldur	x10, [x29, #-40]
    ed48:	mov	x25, x0
    ed4c:	b	eb20 <__gmpf_set_str@@Base+0x560>

000000000000ed50 <__gmpf_set_d@@Base>:
    ed50:	stp	x29, x30, [sp, #-32]!
    ed54:	fmov	x8, d0
    ed58:	mvn	x8, x8
    ed5c:	tst	x8, #0x7ff0000000000000
    ed60:	str	x19, [sp, #16]
    ed64:	mov	x29, sp
    ed68:	b.eq	edb8 <__gmpf_set_d@@Base+0x68>  // b.none
    ed6c:	mov	x19, x0
    ed70:	fcmp	d0, #0.0
    ed74:	b.eq	edac <__gmpf_set_d@@Base+0x5c>  // b.none
    ed78:	ldr	x0, [x19, #16]
    ed7c:	fneg	d1, d0
    ed80:	mov	w8, #0x2                   	// #2
    ed84:	mov	w9, #0xfffffffe            	// #-2
    ed88:	fcsel	d0, d0, d1, ge  // ge = tcont
    ed8c:	csel	w8, w9, w8, mi  // mi = first
    ed90:	str	w8, [x19, #4]
    ed94:	bl	d280 <__gmp_extract_double@plt>
    ed98:	sxtw	x8, w0
    ed9c:	str	x8, [x19, #8]
    eda0:	ldr	x19, [sp, #16]
    eda4:	ldp	x29, x30, [sp], #32
    eda8:	ret
    edac:	mov	x8, xzr
    edb0:	str	wzr, [x19, #4]
    edb4:	b	ed9c <__gmpf_set_d@@Base+0x4c>
    edb8:	bl	c1b0 <__gmp_invalid_operation@plt>

000000000000edbc <__gmpf_set_z@@Base>:
    edbc:	ldrsw	x10, [x1, #4]
    edc0:	ldrsw	x9, [x0]
    edc4:	ldr	x11, [x1, #8]
    edc8:	ldr	x8, [x0, #16]
    edcc:	cmp	x10, #0x0
    edd0:	add	x12, x9, #0x1
    edd4:	cneg	x13, x10, mi  // mi = first
    edd8:	subs	x12, x13, x12
    eddc:	add	x12, x11, x12, lsl #3
    ede0:	csinc	x2, x13, x9, le
    ede4:	csel	x1, x12, x11, gt
    ede8:	neg	w9, w2
    edec:	cmp	w10, #0x0
    edf0:	csel	x9, x2, x9, ge  // ge = tcont
    edf4:	str	x13, [x0, #8]
    edf8:	str	w9, [x0, #4]
    edfc:	mov	x0, x8
    ee00:	b	ca50 <__gmpn_copyi@plt>

000000000000ee04 <__gmpf_init_set@@Base>:
    ee04:	stp	x29, x30, [sp, #-48]!
    ee08:	stp	x22, x21, [sp, #16]
    ee0c:	stp	x20, x19, [sp, #32]
    ee10:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ee14:	ldr	x8, [x8, #3960]
    ee18:	mov	x20, x0
    ee1c:	mov	x29, sp
    ee20:	mov	x19, x1
    ee24:	ldr	x21, [x8]
    ee28:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ee2c:	ldr	x8, [x8, #3840]
    ee30:	add	x22, x21, #0x1
    ee34:	lsl	x0, x22, #3
    ee38:	ldr	x8, [x8]
    ee3c:	blr	x8
    ee40:	str	x0, [x20, #16]
    ee44:	str	w21, [x20]
    ee48:	ldrsw	x8, [x19, #4]
    ee4c:	ldp	x9, x10, [x19, #8]
    ee50:	cmp	x8, #0x0
    ee54:	str	x9, [x20, #8]
    ee58:	cneg	x9, x8, mi  // mi = first
    ee5c:	subs	x11, x9, x22
    ee60:	add	x11, x10, x11, lsl #3
    ee64:	csinc	x2, x9, x21, le
    ee68:	csel	x1, x11, x10, gt
    ee6c:	neg	w9, w2
    ee70:	cmp	w8, #0x0
    ee74:	csel	x8, x2, x9, ge  // ge = tcont
    ee78:	str	w8, [x20, #4]
    ee7c:	ldp	x20, x19, [sp, #32]
    ee80:	ldp	x22, x21, [sp, #16]
    ee84:	ldp	x29, x30, [sp], #48
    ee88:	b	ca50 <__gmpn_copyi@plt>

000000000000ee8c <__gmpf_init_set_ui@@Base>:
    ee8c:	stp	x29, x30, [sp, #-32]!
    ee90:	stp	x20, x19, [sp, #16]
    ee94:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ee98:	ldr	x8, [x8, #3960]
    ee9c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    eea0:	mov	x20, x0
    eea4:	mov	x29, sp
    eea8:	ldr	x8, [x8]
    eeac:	mov	x19, x1
    eeb0:	str	w8, [x0]
    eeb4:	ldr	x9, [x9, #3840]
    eeb8:	lsl	x8, x8, #3
    eebc:	add	x0, x8, #0x8
    eec0:	ldr	x9, [x9]
    eec4:	blr	x9
    eec8:	cmp	x19, #0x0
    eecc:	cset	w8, ne  // ne = any
    eed0:	str	x0, [x20, #16]
    eed4:	str	x19, [x0]
    eed8:	str	w8, [x20, #4]
    eedc:	str	x8, [x20, #8]
    eee0:	ldp	x20, x19, [sp, #16]
    eee4:	ldp	x29, x30, [sp], #32
    eee8:	ret

000000000000eeec <__gmpf_init_set_si@@Base>:
    eeec:	stp	x29, x30, [sp, #-32]!
    eef0:	stp	x20, x19, [sp, #16]
    eef4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    eef8:	ldr	x8, [x8, #3960]
    eefc:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ef00:	mov	x20, x0
    ef04:	mov	x29, sp
    ef08:	ldr	x8, [x8]
    ef0c:	mov	x19, x1
    ef10:	str	w8, [x0]
    ef14:	ldr	x9, [x9, #3840]
    ef18:	lsl	x8, x8, #3
    ef1c:	add	x0, x8, #0x8
    ef20:	ldr	x9, [x9]
    ef24:	blr	x9
    ef28:	cmp	x19, #0x0
    ef2c:	cneg	x8, x19, mi  // mi = first
    ef30:	cset	w9, ne  // ne = any
    ef34:	csetm	x10, ne  // ne = any
    ef38:	str	x0, [x20, #16]
    ef3c:	str	x8, [x0]
    ef40:	csel	x8, x9, x10, ge  // ge = tcont
    ef44:	str	x9, [x20, #8]
    ef48:	str	w8, [x20, #4]
    ef4c:	ldp	x20, x19, [sp, #16]
    ef50:	ldp	x29, x30, [sp], #32
    ef54:	ret

000000000000ef58 <__gmpf_init_set_str@@Base>:
    ef58:	stp	x29, x30, [sp, #-48]!
    ef5c:	str	x21, [sp, #16]
    ef60:	stp	x20, x19, [sp, #32]
    ef64:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ef68:	ldr	x8, [x8, #3960]
    ef6c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    ef70:	mov	x21, x0
    ef74:	mov	x29, sp
    ef78:	ldr	x8, [x8]
    ef7c:	str	xzr, [x0, #8]
    ef80:	mov	w19, w2
    ef84:	mov	x20, x1
    ef88:	stp	w8, wzr, [x0]
    ef8c:	ldr	x9, [x9, #3840]
    ef90:	lsl	x8, x8, #3
    ef94:	add	x0, x8, #0x8
    ef98:	ldr	x9, [x9]
    ef9c:	blr	x9
    efa0:	str	x0, [x21, #16]
    efa4:	mov	x0, x21
    efa8:	mov	x1, x20
    efac:	mov	w2, w19
    efb0:	ldp	x20, x19, [sp, #32]
    efb4:	ldr	x21, [sp, #16]
    efb8:	ldp	x29, x30, [sp], #48
    efbc:	b	c1c0 <__gmpf_set_str@plt>

000000000000efc0 <__gmpf_init_set_d@@Base>:
    efc0:	str	d8, [sp, #-32]!
    efc4:	stp	x29, x30, [sp, #8]
    efc8:	str	x19, [sp, #24]
    efcc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    efd0:	ldr	x8, [x8, #3960]
    efd4:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    efd8:	mov	x19, x0
    efdc:	mov	x29, sp
    efe0:	ldr	x8, [x8]
    efe4:	mov	v8.16b, v0.16b
    efe8:	str	w8, [x0]
    efec:	ldr	x9, [x9, #3840]
    eff0:	lsl	x8, x8, #3
    eff4:	add	x0, x8, #0x8
    eff8:	ldr	x9, [x9]
    effc:	blr	x9
    f000:	str	x0, [x19, #16]
    f004:	mov	x0, x19
    f008:	ldr	x19, [sp, #24]
    f00c:	ldp	x29, x30, [sp, #8]
    f010:	mov	v0.16b, v8.16b
    f014:	ldr	d8, [sp], #32
    f018:	b	c4a0 <__gmpf_set_d@plt>

000000000000f01c <__gmpf_clear@@Base>:
    f01c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f020:	ldr	x8, [x8, #4016]
    f024:	ldrsw	x9, [x0]
    f028:	ldr	x0, [x0, #16]
    f02c:	ldr	x2, [x8]
    f030:	lsl	x8, x9, #3
    f034:	add	x1, x8, #0x8
    f038:	br	x2

000000000000f03c <__gmpf_clears@@Base>:
    f03c:	sub	sp, sp, #0x100
    f040:	stp	x29, x30, [sp, #224]
    f044:	add	x29, sp, #0xe0
    f048:	mov	x8, #0xffffffffffffffc8    	// #-56
    f04c:	mov	x9, sp
    f050:	sub	x10, x29, #0x58
    f054:	movk	x8, #0xff80, lsl #32
    f058:	add	x11, x29, #0x20
    f05c:	add	x9, x9, #0x80
    f060:	add	x10, x10, #0x38
    f064:	str	x19, [sp, #240]
    f068:	stp	x1, x2, [x29, #-88]
    f06c:	stp	x3, x4, [x29, #-72]
    f070:	stp	x5, x6, [x29, #-56]
    f074:	stur	x7, [x29, #-40]
    f078:	stp	q0, q1, [sp]
    f07c:	stp	q2, q3, [sp, #32]
    f080:	stp	q4, q5, [sp, #64]
    f084:	stp	q6, q7, [sp, #96]
    f088:	stp	x9, x8, [x29, #-16]
    f08c:	stp	x11, x10, [x29, #-32]
    f090:	adrp	x19, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f094:	ldr	x19, [x19, #4016]
    f098:	b	f0b0 <__gmpf_clears@@Base+0x74>
    f09c:	ldur	x8, [x29, #-32]
    f0a0:	add	x9, x8, #0x8
    f0a4:	stur	x9, [x29, #-32]
    f0a8:	ldr	x0, [x8]
    f0ac:	cbz	x0, f0f0 <__gmpf_clears@@Base+0xb4>
    f0b0:	ldrsw	x8, [x0]
    f0b4:	ldr	x9, [x19]
    f0b8:	ldr	x0, [x0, #16]
    f0bc:	lsl	x8, x8, #3
    f0c0:	add	x1, x8, #0x8
    f0c4:	blr	x9
    f0c8:	ldursw	x8, [x29, #-8]
    f0cc:	tbz	w8, #31, f09c <__gmpf_clears@@Base+0x60>
    f0d0:	add	w9, w8, #0x8
    f0d4:	cmn	w8, #0x8
    f0d8:	stur	w9, [x29, #-8]
    f0dc:	b.gt	f09c <__gmpf_clears@@Base+0x60>
    f0e0:	ldur	x9, [x29, #-24]
    f0e4:	add	x8, x9, x8
    f0e8:	ldr	x0, [x8]
    f0ec:	cbnz	x0, f0b0 <__gmpf_clears@@Base+0x74>
    f0f0:	ldr	x19, [sp, #240]
    f0f4:	ldp	x29, x30, [sp, #224]
    f0f8:	add	sp, sp, #0x100
    f0fc:	ret

000000000000f100 <__gmpf_get_str@@Base>:
    f100:	stp	x29, x30, [sp, #-96]!
    f104:	stp	x28, x27, [sp, #16]
    f108:	stp	x26, x25, [sp, #32]
    f10c:	stp	x24, x23, [sp, #48]
    f110:	stp	x22, x21, [sp, #64]
    f114:	stp	x20, x19, [sp, #80]
    f118:	mov	x29, sp
    f11c:	sub	sp, sp, #0x60
    f120:	ldr	w8, [x4, #4]
    f124:	ldp	x11, x22, [x4, #8]
    f128:	mov	x20, x4
    f12c:	mov	w21, w2
    f130:	cmp	w8, #0x0
    f134:	mov	x23, x1
    f138:	cneg	w25, w8, mi  // mi = first
    f13c:	cmp	w2, #0x2
    f140:	mov	x24, x0
    f144:	b.lt	f15c <__gmpf_get_str@@Base+0x5c>  // b.tstop
    f148:	cmp	w21, #0x25
    f14c:	b.ge	f16c <__gmpf_get_str@@Base+0x6c>  // b.tcont
    f150:	adrp	x19, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    f154:	add	x19, x19, #0xbd
    f158:	b	f18c <__gmpf_get_str@@Base+0x8c>
    f15c:	cmn	w21, #0x2
    f160:	b.le	f178 <__gmpf_get_str@@Base+0x78>
    f164:	mov	w21, #0xa                   	// #10
    f168:	b	f184 <__gmpf_get_str@@Base+0x84>
    f16c:	cmp	w21, #0x3e
    f170:	b.le	f184 <__gmpf_get_str@@Base+0x84>
    f174:	b	f62c <__gmpf_get_str@@Base+0x52c>
    f178:	cmn	w21, #0x24
    f17c:	b.lt	f62c <__gmpf_get_str@@Base+0x52c>  // b.tstop
    f180:	neg	w21, w21
    f184:	adrp	x19, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    f188:	add	x19, x19, #0x7e
    f18c:	adrp	x26, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f190:	ldr	x26, [x26, #3936]
    f194:	ldrsw	x9, [x20]
    f198:	mov	w8, #0x28                  	// #40
    f19c:	umaddl	x8, w21, w8, x26
    f1a0:	ldr	x28, [x8, #8]
    f1a4:	lsl	x8, x9, #6
    f1a8:	sub	x8, x8, #0x40
    f1ac:	umulh	x8, x28, x8
    f1b0:	add	x8, x8, #0x2
    f1b4:	sub	x9, x3, #0x1
    f1b8:	cmp	x9, x8
    f1bc:	csel	x9, x3, x8, cc  // cc = lo, ul, last
    f1c0:	stur	x9, [x29, #-24]
    f1c4:	cbz	x24, f374 <__gmpf_get_str@@Base+0x274>
    f1c8:	mov	x27, xzr
    f1cc:	cbz	w25, f39c <__gmpf_get_str@@Base+0x29c>
    f1d0:	ldur	x10, [x29, #-24]
    f1d4:	mov	w8, #0x7f00                	// #32512
    f1d8:	mov	w2, w21
    f1dc:	stur	xzr, [x29, #-8]
    f1e0:	add	x1, x10, #0x83
    f1e4:	cmp	x1, x8
    f1e8:	stp	x23, x20, [x29, #-56]
    f1ec:	stur	x27, [x29, #-72]
    f1f0:	b.hi	f5dc <__gmpf_get_str@@Base+0x4dc>  // b.pmore
    f1f4:	add	x9, x1, #0xf
    f1f8:	mov	x8, sp
    f1fc:	and	x9, x9, #0xfffffffffffffff0
    f200:	sub	x23, x8, x9
    f204:	mov	sp, x23
    f208:	mov	w8, #0x28                  	// #40
    f20c:	madd	x8, x2, x8, x26
    f210:	ldr	x8, [x8, #16]
    f214:	umulh	x8, x8, x10
    f218:	ubfx	x21, x8, #3, #58
    f21c:	add	x20, x21, #0x2
    f220:	subs	x8, x25, x20
    f224:	lsl	x9, x20, #1
    f228:	add	x8, x22, x8, lsl #3
    f22c:	csel	x27, x20, x25, hi  // hi = pmore
    f230:	add	x25, x9, #0x4
    f234:	csel	x8, x8, x22, hi  // hi = pmore
    f238:	cmp	x21, #0x3f4
    f23c:	lsl	x1, x25, #4
    f240:	stur	x8, [x29, #-64]
    f244:	stur	x24, [x29, #-80]
    f248:	b.hi	f600 <__gmpf_get_str@@Base+0x500>  // b.pmore
    f24c:	add	x9, x1, #0xf
    f250:	mov	x8, sp
    f254:	and	x9, x9, #0xfffffffffffffff0
    f258:	sub	x26, x8, x9
    f25c:	mov	sp, x26
    f260:	subs	x24, x11, x20
    f264:	add	x25, x26, x25, lsl #3
    f268:	stp	x11, x2, [x29, #-40]
    f26c:	b.le	f3b0 <__gmpf_get_str@@Base+0x2b0>
    f270:	add	x4, x21, #0x3
    f274:	sub	x1, x29, #0x10
    f278:	mov	x0, x26
    f27c:	mov	x5, x25
    f280:	lsl	x8, x24, #6
    f284:	umulh	x3, x28, x8
    f288:	stur	x3, [x29, #-88]
    f28c:	bl	f654 <__gmpf_get_str@@Base+0x554>
    f290:	ldur	x22, [x29, #-16]
    f294:	sub	x8, x24, x22
    f298:	add	x21, x8, x20
    f29c:	lsl	x1, x21, #3
    f2a0:	mov	w8, #0x7f00                	// #32512
    f2a4:	cmp	x1, x8
    f2a8:	mov	x20, x0
    f2ac:	b.hi	f634 <__gmpf_get_str@@Base+0x534>  // b.pmore
    f2b0:	add	x9, x1, #0xf
    f2b4:	mov	x8, sp
    f2b8:	and	x9, x9, #0xfffffffffffffff0
    f2bc:	sub	x28, x8, x9
    f2c0:	mov	sp, x28
    f2c4:	ldur	x8, [x29, #-40]
    f2c8:	subs	x24, x21, x27
    f2cc:	b.eq	f2e8 <__gmpf_get_str@@Base+0x1e8>  // b.none
    f2d0:	sub	x8, x8, x22
    f2d4:	sub	x8, x8, x27
    f2d8:	lsl	x2, x8, #3
    f2dc:	mov	x0, x28
    f2e0:	mov	w1, wzr
    f2e4:	bl	c5f0 <memset@plt>
    f2e8:	ldur	x1, [x29, #-64]
    f2ec:	add	x0, x28, x24, lsl #3
    f2f0:	mov	x2, x27
    f2f4:	bl	ca50 <__gmpn_copyi@plt>
    f2f8:	lsl	x1, x20, #3
    f2fc:	mov	w8, #0x7f00                	// #32512
    f300:	cmp	x1, x8
    f304:	b.hi	f644 <__gmpf_get_str@@Base+0x544>  // b.pmore
    f308:	add	x9, x1, #0xf
    f30c:	mov	x8, sp
    f310:	and	x9, x9, #0xfffffffffffffff0
    f314:	sub	x1, x8, x9
    f318:	mov	sp, x1
    f31c:	ldur	x24, [x29, #-80]
    f320:	ldp	x27, x22, [x29, #-56]
    f324:	mov	x0, x25
    f328:	mov	x2, xzr
    f32c:	mov	x3, x28
    f330:	mov	x4, x21
    f334:	mov	x5, x26
    f338:	mov	x6, x20
    f33c:	bl	bf00 <__gmpn_tdiv_qr@plt>
    f340:	sub	x8, x21, x20
    f344:	ldr	x9, [x25, x8, lsl #3]
    f348:	mov	x0, x23
    f34c:	ldur	x1, [x29, #-32]
    f350:	mov	x2, x25
    f354:	cmp	x9, #0x0
    f358:	cset	w9, eq  // eq = none
    f35c:	sub	x8, x8, x9
    f360:	add	x3, x8, #0x1
    f364:	bl	ca90 <__gmpn_get_str@plt>
    f368:	ldur	x8, [x29, #-88]
    f36c:	add	x8, x0, x8
    f370:	b	f490 <__gmpf_get_str@@Base+0x390>
    f374:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f378:	ldr	x8, [x8, #3840]
    f37c:	add	x27, x9, #0x2
    f380:	mov	x0, x27
    f384:	mov	x24, x11
    f388:	ldr	x8, [x8]
    f38c:	blr	x8
    f390:	mov	x11, x24
    f394:	mov	x24, x0
    f398:	cbnz	w25, f1d0 <__gmpf_get_str@@Base+0xd0>
    f39c:	mov	x21, xzr
    f3a0:	str	xzr, [x23]
    f3a4:	strb	wzr, [x24]
    f3a8:	cbnz	x27, f590 <__gmpf_get_str@@Base+0x490>
    f3ac:	b	f5b8 <__gmpf_get_str@@Base+0x4b8>
    f3b0:	sub	x8, x20, x11
    f3b4:	lsl	x8, x8, #6
    f3b8:	umulh	x28, x28, x8
    f3bc:	add	x4, x21, #0x3
    f3c0:	sub	x1, x29, #0x10
    f3c4:	mov	x0, x26
    f3c8:	mov	x3, x28
    f3cc:	mov	x5, x25
    f3d0:	bl	f654 <__gmpf_get_str@@Base+0x554>
    f3d4:	mov	x22, x0
    f3d8:	cmp	x27, x0
    f3dc:	b.le	f3f8 <__gmpf_get_str@@Base+0x2f8>
    f3e0:	ldur	x1, [x29, #-64]
    f3e4:	mov	x0, x25
    f3e8:	mov	x2, x27
    f3ec:	mov	x3, x26
    f3f0:	mov	x4, x22
    f3f4:	b	f40c <__gmpf_get_str@@Base+0x30c>
    f3f8:	ldur	x3, [x29, #-64]
    f3fc:	mov	x0, x25
    f400:	mov	x1, x26
    f404:	mov	x2, x22
    f408:	mov	x4, x27
    f40c:	bl	ccd0 <__gmpn_mul@plt>
    f410:	add	x8, x22, x27
    f414:	add	x9, x25, x8, lsl #3
    f418:	ldur	x9, [x9, #-8]
    f41c:	ldur	x11, [x29, #-40]
    f420:	ldur	x10, [x29, #-16]
    f424:	ldur	x24, [x29, #-80]
    f428:	cmp	x9, #0x0
    f42c:	sub	x11, x27, x11
    f430:	cset	w9, eq  // eq = none
    f434:	subs	x20, x11, x10
    f438:	sub	x22, x8, x9
    f43c:	b.pl	f474 <__gmpf_get_str@@Base+0x374>  // b.nfrst
    f440:	neg	x8, x20
    f444:	sub	x0, x25, x20, lsl #3
    f448:	mov	x1, x25
    f44c:	mov	x2, x22
    f450:	lsl	x27, x8, #3
    f454:	bl	c000 <__gmpn_copyd@plt>
    f458:	add	x8, x26, x21, lsl #4
    f45c:	add	x0, x8, #0x40
    f460:	mov	w1, wzr
    f464:	mov	x2, x27
    f468:	bl	c5f0 <memset@plt>
    f46c:	sub	x22, x22, x20
    f470:	mov	x20, xzr
    f474:	add	x2, x25, x20, lsl #3
    f478:	sub	x3, x22, x20
    f47c:	mov	x0, x23
    f480:	ldur	x1, [x29, #-32]
    f484:	bl	ca90 <__gmpn_get_str@plt>
    f488:	ldp	x27, x22, [x29, #-56]
    f48c:	sub	x8, x0, x28
    f490:	ldur	x11, [x29, #-24]
    f494:	cmp	x0, x11
    f498:	b.ls	f504 <__gmpf_get_str@@Base+0x404>  // b.plast
    f49c:	ldrb	w9, [x23, x11]
    f4a0:	ldur	x12, [x29, #-32]
    f4a4:	cmp	w12, w9, lsl #1
    f4a8:	b.gt	f504 <__gmpf_get_str@@Base+0x404>
    f4ac:	add	x9, x11, x23
    f4b0:	ldurb	w10, [x9, #-1]
    f4b4:	add	w10, w10, #0x1
    f4b8:	cmp	w12, w10, uxtb
    f4bc:	sturb	w10, [x9, #-1]
    f4c0:	b.ne	f4f0 <__gmpf_get_str@@Base+0x3f0>  // b.any
    f4c4:	mov	x9, x11
    f4c8:	subs	x0, x9, #0x1
    f4cc:	b.eq	f4f8 <__gmpf_get_str@@Base+0x3f8>  // b.none
    f4d0:	add	x9, x23, x9
    f4d4:	ldurb	w10, [x9, #-2]
    f4d8:	add	w10, w10, #0x1
    f4dc:	cmp	w12, w10, uxtb
    f4e0:	sturb	w10, [x9, #-2]
    f4e4:	mov	x9, x0
    f4e8:	b.eq	f4c8 <__gmpf_get_str@@Base+0x3c8>  // b.none
    f4ec:	b	f504 <__gmpf_get_str@@Base+0x404>
    f4f0:	mov	x0, x11
    f4f4:	b	f504 <__gmpf_get_str@@Base+0x404>
    f4f8:	mov	w0, #0x1                   	// #1
    f4fc:	strb	w0, [x23]
    f500:	add	x8, x8, #0x1
    f504:	cmp	x11, x0
    f508:	csel	x9, x0, x11, hi  // hi = pmore
    f50c:	sub	x10, x23, #0x1
    f510:	cbz	x9, f554 <__gmpf_get_str@@Base+0x454>
    f514:	ldrb	w11, [x10, x9]
    f518:	sub	x12, x9, #0x1
    f51c:	mov	x9, x12
    f520:	cbz	w11, f510 <__gmpf_get_str@@Base+0x410>
    f524:	ldr	w9, [x22, #4]
    f528:	mov	x10, xzr
    f52c:	add	x21, x12, #0x1
    f530:	lsr	x9, x9, #31
    f534:	add	x11, x24, x9
    f538:	ldrb	w12, [x23, x10]
    f53c:	ldrb	w12, [x19, x12]
    f540:	strb	w12, [x11, x10]
    f544:	add	x10, x10, #0x1
    f548:	cmp	x21, x10
    f54c:	b.ne	f538 <__gmpf_get_str@@Base+0x438>  // b.any
    f550:	b	f560 <__gmpf_get_str@@Base+0x460>
    f554:	ldr	w9, [x22, #4]
    f558:	mov	x21, xzr
    f55c:	lsr	x9, x9, #31
    f560:	add	x9, x24, x9
    f564:	strb	wzr, [x9, x21]
    f568:	str	x8, [x27]
    f56c:	ldr	w8, [x22, #4]
    f570:	tbz	w8, #31, f580 <__gmpf_get_str@@Base+0x480>
    f574:	mov	w8, #0x2d                  	// #45
    f578:	add	x21, x21, #0x1
    f57c:	strb	w8, [x24]
    f580:	ldur	x27, [x29, #-72]
    f584:	ldur	x0, [x29, #-8]
    f588:	cbnz	x0, f620 <__gmpf_get_str@@Base+0x520>
    f58c:	cbz	x27, f5b8 <__gmpf_get_str@@Base+0x4b8>
    f590:	add	x2, x21, #0x1
    f594:	cmp	x27, x2
    f598:	b.eq	f5b8 <__gmpf_get_str@@Base+0x4b8>  // b.none
    f59c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f5a0:	ldr	x8, [x8, #3792]
    f5a4:	mov	x0, x24
    f5a8:	mov	x1, x27
    f5ac:	ldr	x8, [x8]
    f5b0:	blr	x8
    f5b4:	mov	x24, x0
    f5b8:	mov	x0, x24
    f5bc:	mov	sp, x29
    f5c0:	ldp	x20, x19, [sp, #80]
    f5c4:	ldp	x22, x21, [sp, #64]
    f5c8:	ldp	x24, x23, [sp, #48]
    f5cc:	ldp	x26, x25, [sp, #32]
    f5d0:	ldp	x28, x27, [sp, #16]
    f5d4:	ldp	x29, x30, [sp], #96
    f5d8:	ret
    f5dc:	sub	x0, x29, #0x8
    f5e0:	mov	x20, x2
    f5e4:	mov	x21, x11
    f5e8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f5ec:	ldur	x10, [x29, #-24]
    f5f0:	mov	x11, x21
    f5f4:	mov	x2, x20
    f5f8:	mov	x23, x0
    f5fc:	b	f208 <__gmpf_get_str@@Base+0x108>
    f600:	sub	x0, x29, #0x8
    f604:	mov	x22, x2
    f608:	mov	x24, x11
    f60c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f610:	mov	x11, x24
    f614:	mov	x2, x22
    f618:	mov	x26, x0
    f61c:	b	f260 <__gmpf_get_str@@Base+0x160>
    f620:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    f624:	cbnz	x27, f590 <__gmpf_get_str@@Base+0x490>
    f628:	b	f5b8 <__gmpf_get_str@@Base+0x4b8>
    f62c:	mov	x24, xzr
    f630:	b	f5b8 <__gmpf_get_str@@Base+0x4b8>
    f634:	sub	x0, x29, #0x8
    f638:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f63c:	mov	x28, x0
    f640:	b	f2c4 <__gmpf_get_str@@Base+0x1c4>
    f644:	sub	x0, x29, #0x8
    f648:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    f64c:	mov	x1, x0
    f650:	b	f31c <__gmpf_get_str@@Base+0x21c>
    f654:	sub	sp, sp, #0x70
    f658:	stp	x20, x19, [sp, #96]
    f65c:	mov	x20, x0
    f660:	stp	x29, x30, [sp, #16]
    f664:	stp	x28, x27, [sp, #32]
    f668:	stp	x26, x25, [sp, #48]
    f66c:	stp	x24, x23, [sp, #64]
    f670:	stp	x22, x21, [sp, #80]
    f674:	add	x29, sp, #0x10
    f678:	cbz	x3, f6e4 <__gmpf_get_str@@Base+0x5e4>
    f67c:	clz	x9, x3
    f680:	mov	x21, x4
    f684:	mov	x23, x3
    f688:	mov	x24, x2
    f68c:	cmp	x9, #0x3f
    f690:	str	x2, [x20]
    f694:	str	x1, [sp, #8]
    f698:	b.ne	f718 <__gmpf_get_str@@Base+0x618>  // b.any
    f69c:	mov	x8, xzr
    f6a0:	mov	x27, xzr
    f6a4:	mov	w25, #0x1                   	// #1
    f6a8:	mov	x26, x20
    f6ac:	subs	x9, x25, x21
    f6b0:	add	x10, x26, x9, lsl #3
    f6b4:	csel	x10, x10, x26, gt
    f6b8:	csel	x9, x9, xzr, gt
    f6bc:	add	x1, x10, x8, lsl #3
    f6c0:	csel	x21, x21, x25, gt
    f6c4:	cmp	x1, x20
    f6c8:	add	x19, x9, x27
    f6cc:	b.eq	f6dc <__gmpf_get_str@@Base+0x5dc>  // b.none
    f6d0:	mov	x0, x20
    f6d4:	mov	x2, x21
    f6d8:	bl	ca50 <__gmpn_copyi@plt>
    f6dc:	ldr	x1, [sp, #8]
    f6e0:	b	f6f0 <__gmpf_get_str@@Base+0x5f0>
    f6e4:	mov	w21, #0x1                   	// #1
    f6e8:	mov	x19, xzr
    f6ec:	str	x21, [x20]
    f6f0:	str	x19, [x1]
    f6f4:	mov	x0, x21
    f6f8:	ldp	x20, x19, [sp, #96]
    f6fc:	ldp	x22, x21, [sp, #80]
    f700:	ldp	x24, x23, [sp, #64]
    f704:	ldp	x26, x25, [sp, #48]
    f708:	ldp	x28, x27, [sp, #32]
    f70c:	ldp	x29, x30, [sp, #16]
    f710:	add	sp, sp, #0x70
    f714:	ret
    f718:	mov	w10, #0x3e                  	// #62
    f71c:	mov	w11, #0x3f                  	// #63
    f720:	mov	x22, x5
    f724:	mov	x27, xzr
    f728:	mov	x8, xzr
    f72c:	sub	w28, w10, w9
    f730:	sub	w19, w11, w9
    f734:	mov	w25, #0x1                   	// #1
    f738:	mov	x9, x20
    f73c:	b	f754 <__gmpf_get_str@@Base+0x654>
    f740:	sub	w19, w19, #0x1
    f744:	cmp	w19, #0x0
    f748:	sub	x28, x28, #0x1
    f74c:	mov	x9, x26
    f750:	b.le	f6ac <__gmpf_get_str@@Base+0x5ac>
    f754:	mov	x26, x22
    f758:	add	x1, x9, x8, lsl #3
    f75c:	mov	x0, x26
    f760:	mov	x2, x25
    f764:	mov	x22, x9
    f768:	bl	c8e0 <__gmpn_sqr@plt>
    f76c:	add	x8, x26, x25, lsl #4
    f770:	ldur	x8, [x8, #-8]
    f774:	lsl	x9, x25, #1
    f778:	lsr	x10, x23, x28
    f77c:	cmp	x8, #0x0
    f780:	cset	w8, eq  // eq = none
    f784:	sub	x9, x9, x8
    f788:	subs	x8, x9, x21
    f78c:	csel	x8, x8, xzr, gt
    f790:	add	x27, x8, x27, lsl #1
    f794:	csel	x25, x21, x9, gt
    f798:	tbz	w10, #0, f740 <__gmpf_get_str@@Base+0x640>
    f79c:	add	x1, x26, x8, lsl #3
    f7a0:	mov	x0, x26
    f7a4:	mov	x2, x25
    f7a8:	mov	x3, x24
    f7ac:	bl	d490 <__gmpn_mul_1@plt>
    f7b0:	cmp	x0, #0x0
    f7b4:	mov	x8, xzr
    f7b8:	str	x0, [x26, x25, lsl #3]
    f7bc:	cinc	x25, x25, ne  // ne = any
    f7c0:	b	f740 <__gmpf_get_str@@Base+0x640>

000000000000f7c4 <__gmpf_dump@@Base>:
    f7c4:	sub	sp, sp, #0x30
    f7c8:	mov	x4, x0
    f7cc:	add	x1, sp, #0x8
    f7d0:	mov	w2, #0xa                   	// #10
    f7d4:	mov	x0, xzr
    f7d8:	mov	x3, xzr
    f7dc:	stp	x29, x30, [sp, #16]
    f7e0:	stp	x20, x19, [sp, #32]
    f7e4:	add	x29, sp, #0x10
    f7e8:	bl	ce20 <__gmpf_get_str@plt>
    f7ec:	ldrb	w8, [x0]
    f7f0:	mov	x19, x0
    f7f4:	cmp	w8, #0x2d
    f7f8:	b.ne	f810 <__gmpf_dump@@Base+0x4c>  // b.any
    f7fc:	ldr	x2, [sp, #8]
    f800:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    f804:	add	x1, x19, #0x1
    f808:	add	x0, x0, #0xe2
    f80c:	b	f820 <__gmpf_dump@@Base+0x5c>
    f810:	ldr	x2, [sp, #8]
    f814:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    f818:	add	x0, x0, #0xe3
    f81c:	mov	x1, x19
    f820:	bl	d2e0 <printf@plt>
    f824:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    f828:	ldr	x8, [x8, #4016]
    f82c:	mov	x0, x19
    f830:	ldr	x20, [x8]
    f834:	bl	bf60 <strlen@plt>
    f838:	add	x1, x0, #0x1
    f83c:	mov	x0, x19
    f840:	blr	x20
    f844:	ldp	x20, x19, [sp, #32]
    f848:	ldp	x29, x30, [sp, #16]
    f84c:	add	sp, sp, #0x30
    f850:	ret

000000000000f854 <__gmpf_size@@Base>:
    f854:	ldr	w8, [x0, #4]
    f858:	cmp	w8, #0x0
    f85c:	cneg	w0, w8, mi  // mi = first
    f860:	ret

000000000000f864 <__gmpf_eq@@Base>:
    f864:	ldrsw	x10, [x0, #4]
    f868:	ldrsw	x9, [x1, #4]
    f86c:	eor	w11, w9, w10
    f870:	tbnz	w11, #31, f984 <__gmpf_eq@@Base+0x120>
    f874:	orr	w11, w9, w10
    f878:	cmp	w11, #0x0
    f87c:	mov	x8, x0
    f880:	cset	w0, eq  // eq = none
    f884:	cbz	w10, f9b8 <__gmpf_eq@@Base+0x154>
    f888:	cbz	w9, f9b8 <__gmpf_eq@@Base+0x154>
    f88c:	ldr	x11, [x8, #8]
    f890:	ldr	x12, [x1, #8]
    f894:	cmp	x11, x12
    f898:	b.ne	f984 <__gmpf_eq@@Base+0x120>  // b.any
    f89c:	ldr	x8, [x8, #16]
    f8a0:	cmp	x10, #0x0
    f8a4:	ldr	x12, [x1, #16]
    f8a8:	cneg	x11, x10, mi  // mi = first
    f8ac:	add	x13, x8, x11, lsl #3
    f8b0:	cmp	x9, #0x0
    f8b4:	mov	x10, x13
    f8b8:	ldr	x8, [x10, #-8]!
    f8bc:	cneg	x9, x9, mi  // mi = first
    f8c0:	add	x16, x12, x9, lsl #3
    f8c4:	ldur	x12, [x16, #-8]
    f8c8:	clz	x8, x8
    f8cc:	mov	w14, #0x3f                  	// #63
    f8d0:	sub	w14, w14, w8
    f8d4:	lsr	x12, x12, x14
    f8d8:	cmp	x12, #0x1
    f8dc:	b.ne	f984 <__gmpf_eq@@Base+0x120>  // b.any
    f8e0:	add	x8, x8, x2
    f8e4:	add	x12, x8, #0x3f
    f8e8:	lsr	x12, x12, #6
    f8ec:	cmp	x11, x12
    f8f0:	csel	x11, x11, x12, lt  // lt = tstop
    f8f4:	cmp	x9, x12
    f8f8:	csel	x12, x9, x12, lt  // lt = tstop
    f8fc:	cmp	x11, x12
    f900:	csel	x15, x11, x12, lt  // lt = tstop
    f904:	add	x9, x11, x12
    f908:	lsl	x14, x15, #3
    f90c:	sub	x9, x9, x15
    f910:	sub	x13, x13, x14
    f914:	sub	x14, x16, x14
    f918:	sub	x16, x16, #0x8
    f91c:	mov	x17, x15
    f920:	cmp	x17, #0x2
    f924:	b.lt	f940 <__gmpf_eq@@Base+0xdc>  // b.tstop
    f928:	ldr	x18, [x10], #-8
    f92c:	ldr	x0, [x16], #-8
    f930:	sub	x17, x17, #0x1
    f934:	cmp	x18, x0
    f938:	b.eq	f920 <__gmpf_eq@@Base+0xbc>  // b.none
    f93c:	b	f984 <__gmpf_eq@@Base+0x120>
    f940:	ldr	x16, [x13]
    f944:	ldr	x17, [x14]
    f948:	subs	x10, x9, x15
    f94c:	b.eq	f98c <__gmpf_eq@@Base+0x128>  // b.none
    f950:	cmp	x16, x17
    f954:	b.ne	f984 <__gmpf_eq@@Base+0x120>  // b.any
    f958:	cmp	x11, x12
    f95c:	csel	x11, x13, x14, gt
    f960:	neg	x15, x10
    f964:	sub	x12, x11, #0x8
    f968:	cmp	x10, #0x2
    f96c:	b.lt	f994 <__gmpf_eq@@Base+0x130>  // b.tstop
    f970:	ldr	x13, [x12], #-8
    f974:	mov	w0, wzr
    f978:	sub	x10, x10, #0x1
    f97c:	cbz	x13, f968 <__gmpf_eq@@Base+0x104>
    f980:	b	f9b8 <__gmpf_eq@@Base+0x154>
    f984:	mov	w0, wzr
    f988:	ret
    f98c:	eor	x10, x17, x16
    f990:	b	f998 <__gmpf_eq@@Base+0x134>
    f994:	ldr	x10, [x11, x15, lsl #3]
    f998:	sub	x8, x8, x9, lsl #6
    f99c:	add	x8, x8, #0x40
    f9a0:	mov	w9, #0x40                  	// #64
    f9a4:	subs	x8, x9, x8
    f9a8:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
    f9ac:	lsr	x8, x10, x8
    f9b0:	cmp	x8, #0x0
    f9b4:	cset	w0, eq  // eq = none
    f9b8:	ret

000000000000f9bc <__gmpf_reldiff@@Base>:
    f9bc:	stp	x29, x30, [sp, #-48]!
    f9c0:	str	x21, [sp, #16]
    f9c4:	stp	x20, x19, [sp, #32]
    f9c8:	mov	x29, sp
    f9cc:	sub	sp, sp, #0x20
    f9d0:	ldr	w8, [x1, #4]
    f9d4:	mov	x21, x2
    f9d8:	mov	x19, x0
    f9dc:	cbz	w8, fa74 <__gmpf_reldiff@@Base+0xb8>
    f9e0:	str	xzr, [x29, #24]
    f9e4:	ldr	w9, [x19]
    f9e8:	cmp	w8, #0x0
    f9ec:	cneg	w8, w8, mi  // mi = first
    f9f0:	mov	x20, x1
    f9f4:	add	w8, w9, w8
    f9f8:	sbfiz	x9, x8, #3, #32
    f9fc:	add	x1, x9, #0x8
    fa00:	mov	w9, #0x7f00                	// #32512
    fa04:	cmp	x1, x9
    fa08:	stur	w8, [x29, #-24]
    fa0c:	b.hi	fa98 <__gmpf_reldiff@@Base+0xdc>  // b.pmore
    fa10:	add	x9, x1, #0xf
    fa14:	mov	x8, sp
    fa18:	and	x9, x9, #0xfffffffffffffff0
    fa1c:	sub	x0, x8, x9
    fa20:	mov	sp, x0
    fa24:	stur	x0, [x29, #-8]
    fa28:	sub	x0, x29, #0x18
    fa2c:	mov	x1, x20
    fa30:	mov	x2, x21
    fa34:	bl	ce00 <__gmpf_sub@plt>
    fa38:	ldur	w8, [x29, #-20]
    fa3c:	sub	x1, x29, #0x18
    fa40:	mov	x0, x19
    fa44:	mov	x2, x20
    fa48:	cmp	w8, #0x0
    fa4c:	cneg	w8, w8, mi  // mi = first
    fa50:	stur	w8, [x29, #-20]
    fa54:	bl	ced0 <__gmpf_div@plt>
    fa58:	ldr	x0, [x29, #24]
    fa5c:	cbnz	x0, faa4 <__gmpf_reldiff@@Base+0xe8>
    fa60:	mov	sp, x29
    fa64:	ldp	x20, x19, [sp, #32]
    fa68:	ldr	x21, [sp, #16]
    fa6c:	ldp	x29, x30, [sp], #48
    fa70:	ret
    fa74:	ldr	w8, [x21, #4]
    fa78:	mov	x0, x19
    fa7c:	cmp	w8, #0x0
    fa80:	cset	w1, ne  // ne = any
    fa84:	mov	sp, x29
    fa88:	ldp	x20, x19, [sp, #32]
    fa8c:	ldr	x21, [sp, #16]
    fa90:	ldp	x29, x30, [sp], #48
    fa94:	b	c6a0 <__gmpf_set_ui@plt>
    fa98:	add	x0, x29, #0x18
    fa9c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    faa0:	b	fa24 <__gmpf_reldiff@@Base+0x68>
    faa4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    faa8:	b	fa60 <__gmpf_reldiff@@Base+0xa4>

000000000000faac <__gmpf_sqrt@@Base>:
    faac:	stp	x29, x30, [sp, #-80]!
    fab0:	stp	x26, x25, [sp, #16]
    fab4:	stp	x24, x23, [sp, #32]
    fab8:	stp	x22, x21, [sp, #48]
    fabc:	stp	x20, x19, [sp, #64]
    fac0:	mov	x29, sp
    fac4:	sub	sp, sp, #0x10
    fac8:	ldrsw	x20, [x1, #4]
    facc:	mov	x19, x0
    fad0:	cmp	w20, #0x0
    fad4:	b.le	fb48 <__gmpf_sqrt@@Base+0x9c>
    fad8:	stur	xzr, [x29, #-8]
    fadc:	ldp	x8, x22, [x1, #8]
    fae0:	ldrsw	x9, [x19]
    fae4:	mov	w10, #0x7f00                	// #32512
    fae8:	and	x24, x8, #0x1
    faec:	lsl	x25, x9, #1
    faf0:	add	x8, x24, x8
    faf4:	sub	x21, x25, x24
    faf8:	cmp	x8, #0x0
    fafc:	lsl	x1, x21, #3
    fb00:	cinc	x8, x8, lt  // lt = tstop
    fb04:	cmp	x1, x10
    fb08:	asr	x8, x8, #1
    fb0c:	str	w9, [x19, #4]
    fb10:	str	x8, [x19, #8]
    fb14:	b.hi	fb58 <__gmpf_sqrt@@Base+0xac>  // b.pmore
    fb18:	add	x9, x1, #0xf
    fb1c:	mov	x8, sp
    fb20:	and	x9, x9, #0xfffffffffffffff0
    fb24:	sub	x23, x8, x9
    fb28:	mov	sp, x23
    fb2c:	subs	x26, x21, x20
    fb30:	b.ge	fb6c <__gmpf_sqrt@@Base+0xc0>  // b.tcont
    fb34:	sub	x8, x20, x21
    fb38:	add	x1, x22, x8, lsl #3
    fb3c:	mov	x0, x23
    fb40:	mov	x2, x21
    fb44:	b	fb94 <__gmpf_sqrt@@Base+0xe8>
    fb48:	tbnz	w20, #31, fbd8 <__gmpf_sqrt@@Base+0x12c>
    fb4c:	str	wzr, [x19, #4]
    fb50:	str	xzr, [x19, #8]
    fb54:	b	fbb4 <__gmpf_sqrt@@Base+0x108>
    fb58:	sub	x0, x29, #0x8
    fb5c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    fb60:	mov	x23, x0
    fb64:	subs	x26, x21, x20
    fb68:	b.lt	fb34 <__gmpf_sqrt@@Base+0x88>  // b.tstop
    fb6c:	b.eq	fb88 <__gmpf_sqrt@@Base+0xdc>  // b.none
    fb70:	sub	x8, x25, x20
    fb74:	sub	x8, x8, x24
    fb78:	lsl	x2, x8, #3
    fb7c:	mov	x0, x23
    fb80:	mov	w1, wzr
    fb84:	bl	c5f0 <memset@plt>
    fb88:	add	x0, x23, x26, lsl #3
    fb8c:	mov	x1, x22
    fb90:	mov	x2, x20
    fb94:	bl	ca50 <__gmpn_copyi@plt>
    fb98:	ldr	x0, [x19, #16]
    fb9c:	mov	x1, xzr
    fba0:	mov	x2, x23
    fba4:	mov	x3, x21
    fba8:	bl	d3b0 <__gmpn_sqrtrem@plt>
    fbac:	ldur	x0, [x29, #-8]
    fbb0:	cbnz	x0, fbd0 <__gmpf_sqrt@@Base+0x124>
    fbb4:	mov	sp, x29
    fbb8:	ldp	x20, x19, [sp, #64]
    fbbc:	ldp	x22, x21, [sp, #48]
    fbc0:	ldp	x24, x23, [sp, #32]
    fbc4:	ldp	x26, x25, [sp, #16]
    fbc8:	ldp	x29, x30, [sp], #80
    fbcc:	ret
    fbd0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    fbd4:	b	fbb4 <__gmpf_sqrt@@Base+0x108>
    fbd8:	bl	cff0 <__gmp_sqrt_of_negative@plt>

000000000000fbdc <__gmpf_random2@@Base>:
    fbdc:	sub	sp, sp, #0x40
    fbe0:	cmp	x1, #0x0
    fbe4:	stp	x20, x19, [sp, #48]
    fbe8:	cneg	x8, x1, mi  // mi = first
    fbec:	mov	x19, x0
    fbf0:	stp	x29, x30, [sp, #16]
    fbf4:	stp	x22, x21, [sp, #32]
    fbf8:	add	x29, sp, #0x10
    fbfc:	cbz	x8, fc94 <__gmpf_random2@@Base+0xb8>
    fc00:	ldrsw	x9, [x19]
    fc04:	ldr	x0, [x19, #16]
    fc08:	mov	x20, x1
    fc0c:	mov	x21, x2
    fc10:	add	x10, x9, #0x1
    fc14:	cmp	x8, x10
    fc18:	csinc	x22, x8, x9, le
    fc1c:	mov	x1, x22
    fc20:	bl	d1c0 <__gmpn_random2@plt>
    fc24:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fc28:	ldr	x8, [x8, #4040]
    fc2c:	ldrb	w9, [x8]
    fc30:	cbnz	w9, fc48 <__gmpf_random2@@Base+0x6c>
    fc34:	mov	w9, #0x1                   	// #1
    fc38:	strb	w9, [x8]
    fc3c:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fc40:	ldr	x0, [x0, #3976]
    fc44:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
    fc48:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fc4c:	ldr	x0, [x0, #3976]
    fc50:	add	x1, sp, #0x8
    fc54:	mov	w2, #0x40                  	// #64
    fc58:	ldr	x8, [x0, #24]
    fc5c:	ldr	x8, [x8, #8]
    fc60:	blr	x8
    fc64:	ldr	x8, [sp, #8]
    fc68:	cmp	x21, #0x0
    fc6c:	mov	w9, #0x1                   	// #1
    fc70:	cneg	x10, x21, mi  // mi = first
    fc74:	bfi	x9, x10, #1, #63
    fc78:	udiv	x11, x8, x9
    fc7c:	msub	x8, x11, x9, x8
    fc80:	cmp	x20, #0x0
    fc84:	sub	x8, x8, x10
    fc88:	str	x8, [x19, #8]
    fc8c:	cneg	w8, w22, lt  // lt = tstop
    fc90:	b	fc98 <__gmpf_random2@@Base+0xbc>
    fc94:	str	xzr, [x19, #8]
    fc98:	str	w8, [x19, #4]
    fc9c:	ldp	x20, x19, [sp, #48]
    fca0:	ldp	x22, x21, [sp, #32]
    fca4:	ldp	x29, x30, [sp, #16]
    fca8:	add	sp, sp, #0x40
    fcac:	ret

000000000000fcb0 <__gmpf_inp_str@@Base>:
    fcb0:	sub	sp, sp, #0x70
    fcb4:	stp	x29, x30, [sp, #16]
    fcb8:	add	x29, sp, #0x10
    fcbc:	stp	x28, x27, [sp, #32]
    fcc0:	stp	x26, x25, [sp, #48]
    fcc4:	stp	x24, x23, [sp, #64]
    fcc8:	stp	x22, x21, [sp, #80]
    fccc:	stp	x20, x19, [sp, #96]
    fcd0:	stur	w2, [x29, #-4]
    fcd4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fcd8:	ldr	x8, [x8, #3888]
    fcdc:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fce0:	mov	x20, x0
    fce4:	cmp	x1, #0x0
    fce8:	ldr	x8, [x8]
    fcec:	ldr	x9, [x9, #3840]
    fcf0:	mov	w0, #0x64                  	// #100
    fcf4:	csel	x22, x8, x1, eq  // eq = none
    fcf8:	ldr	x9, [x9]
    fcfc:	blr	x9
    fd00:	mov	x21, x0
    fd04:	mov	x27, #0xffffffffffffffff    	// #-1
    fd08:	mov	x0, x22
    fd0c:	bl	c7f0 <getc@plt>
    fd10:	mov	w24, w0
    fd14:	bl	cae0 <__ctype_b_loc@plt>
    fd18:	ldr	x8, [x0]
    fd1c:	add	x27, x27, #0x1
    fd20:	ldrh	w8, [x8, w24, sxtw #1]
    fd24:	tbnz	w8, #13, fd08 <__gmpf_inp_str@@Base+0x58>
    fd28:	adrp	x19, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fd2c:	ldr	x19, [x19, #3792]
    fd30:	mov	x25, x0
    fd34:	mov	x28, xzr
    fd38:	mov	w23, #0x64                  	// #100
    fd3c:	cmp	x28, x23
    fd40:	b.cc	fd68 <__gmpf_inp_str@@Base+0xb8>  // b.lo, b.ul, b.last
    fd44:	ldr	x8, [x19]
    fd48:	add	x9, x23, x23, lsl #1
    fd4c:	lsr	x26, x9, #1
    fd50:	mov	x0, x21
    fd54:	mov	x1, x23
    fd58:	mov	x2, x26
    fd5c:	blr	x8
    fd60:	mov	x21, x0
    fd64:	mov	x23, x26
    fd68:	cmn	w24, #0x1
    fd6c:	b.eq	fda0 <__gmpf_inp_str@@Base+0xf0>  // b.none
    fd70:	ldr	x8, [x25]
    fd74:	ldrh	w8, [x8, w24, sxtw #1]
    fd78:	tbnz	w8, #13, fda0 <__gmpf_inp_str@@Base+0xf0>
    fd7c:	mov	x0, x22
    fd80:	add	x26, x28, #0x1
    fd84:	strb	w24, [x21, x28]
    fd88:	bl	c7f0 <getc@plt>
    fd8c:	mov	w24, w0
    fd90:	mov	x28, x26
    fd94:	cmp	x28, x23
    fd98:	b.cs	fd44 <__gmpf_inp_str@@Base+0x94>  // b.hs, b.nlast
    fd9c:	b	fd68 <__gmpf_inp_str@@Base+0xb8>
    fda0:	mov	w0, w24
    fda4:	mov	x1, x22
    fda8:	bl	cc50 <ungetc@plt>
    fdac:	cmp	x28, x23
    fdb0:	b.cc	fdd8 <__gmpf_inp_str@@Base+0x128>  // b.lo, b.ul, b.last
    fdb4:	ldr	x8, [x19]
    fdb8:	add	x9, x23, x23, lsl #1
    fdbc:	lsr	x22, x9, #1
    fdc0:	mov	x0, x21
    fdc4:	mov	x1, x23
    fdc8:	mov	x2, x22
    fdcc:	blr	x8
    fdd0:	mov	x21, x0
    fdd4:	mov	x23, x22
    fdd8:	ldur	w2, [x29, #-4]
    fddc:	mov	x0, x20
    fde0:	mov	x1, x21
    fde4:	strb	wzr, [x21, x28]
    fde8:	bl	c1c0 <__gmpf_set_str@plt>
    fdec:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fdf0:	ldr	x8, [x8, #4016]
    fdf4:	mov	w19, w0
    fdf8:	mov	x0, x21
    fdfc:	mov	x1, x23
    fe00:	ldr	x8, [x8]
    fe04:	blr	x8
    fe08:	add	x8, x27, x28
    fe0c:	cmn	w19, #0x1
    fe10:	ldp	x20, x19, [sp, #96]
    fe14:	ldp	x22, x21, [sp, #80]
    fe18:	ldp	x24, x23, [sp, #64]
    fe1c:	ldp	x26, x25, [sp, #48]
    fe20:	ldp	x28, x27, [sp, #32]
    fe24:	ldp	x29, x30, [sp, #16]
    fe28:	csel	x0, xzr, x8, eq  // eq = none
    fe2c:	add	sp, sp, #0x70
    fe30:	ret

000000000000fe34 <__gmpf_out_str@@Base>:
    fe34:	stp	x29, x30, [sp, #-80]!
    fe38:	str	x25, [sp, #16]
    fe3c:	stp	x24, x23, [sp, #32]
    fe40:	stp	x22, x21, [sp, #48]
    fe44:	stp	x20, x19, [sp, #64]
    fe48:	mov	x29, sp
    fe4c:	sub	sp, sp, #0x10
    fe50:	cmp	w1, #0x0
    fe54:	mov	w8, #0xa                   	// #10
    fe58:	mov	x22, x3
    fe5c:	mov	x23, x2
    fe60:	csel	w19, w8, w1, eq  // eq = none
    fe64:	stur	xzr, [x29, #-8]
    fe68:	cbnz	x2, fe94 <__gmpf_out_str@@Base+0x60>
    fe6c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fe70:	ldr	x8, [x8, #3936]
    fe74:	ldrsw	x10, [x22]
    fe78:	mov	w9, #0x28                  	// #40
    fe7c:	smaddl	x8, w19, w9, x8
    fe80:	ldr	x8, [x8, #8]
    fe84:	lsl	x9, x10, #6
    fe88:	sub	x9, x9, #0x40
    fe8c:	umulh	x8, x8, x9
    fe90:	add	x23, x8, #0x2
    fe94:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
    fe98:	ldr	x8, [x8, #3856]
    fe9c:	cmp	x0, #0x0
    fea0:	add	x1, x23, #0x2
    fea4:	mov	w9, #0x7f00                	// #32512
    fea8:	ldr	x8, [x8]
    feac:	csel	x20, x8, x0, eq  // eq = none
    feb0:	cmp	x1, x9
    feb4:	b.hi	ffd0 <__gmpf_out_str@@Base+0x19c>  // b.pmore
    feb8:	add	x9, x1, #0xf
    febc:	mov	x8, sp
    fec0:	and	x9, x9, #0xfffffffffffffff0
    fec4:	sub	x21, x8, x9
    fec8:	mov	sp, x21
    fecc:	add	x1, x29, #0x18
    fed0:	mov	x0, x21
    fed4:	mov	w2, w19
    fed8:	mov	x3, x23
    fedc:	mov	x4, x22
    fee0:	bl	ce20 <__gmpf_get_str@plt>
    fee4:	mov	x0, x21
    fee8:	bl	bf60 <strlen@plt>
    feec:	ldrb	w8, [x21]
    fef0:	mov	x22, x0
    fef4:	cmp	w8, #0x2d
    fef8:	b.ne	ff18 <__gmpf_out_str@@Base+0xe4>  // b.any
    fefc:	mov	w0, #0x2d                  	// #45
    ff00:	mov	x1, x20
    ff04:	add	x21, x21, #0x1
    ff08:	bl	c210 <fputc@plt>
    ff0c:	sub	x22, x22, #0x1
    ff10:	mov	w25, #0x2                   	// #2
    ff14:	b	ff1c <__gmpf_out_str@@Base+0xe8>
    ff18:	mov	w25, #0x1                   	// #1
    ff1c:	mov	w0, #0x10000               	// #65536
    ff20:	bl	c400 <nl_langinfo@plt>
    ff24:	mov	x23, x0
    ff28:	bl	bf60 <strlen@plt>
    ff2c:	mov	x24, x0
    ff30:	mov	w0, #0x30                  	// #48
    ff34:	mov	x1, x20
    ff38:	bl	c1e0 <putc@plt>
    ff3c:	mov	w1, #0x1                   	// #1
    ff40:	mov	x0, x23
    ff44:	mov	x2, x24
    ff48:	mov	x3, x20
    ff4c:	bl	ce30 <fwrite@plt>
    ff50:	mov	w1, #0x1                   	// #1
    ff54:	mov	x0, x21
    ff58:	mov	x2, x22
    ff5c:	mov	x3, x20
    ff60:	bl	ce30 <fwrite@plt>
    ff64:	ldr	x2, [x29, #24]
    ff68:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    ff6c:	adrp	x9, 58000 <__gmp_binvert_limb_table@@Base+0x38>
    ff70:	add	x8, x8, #0xf2
    ff74:	add	x9, x9, #0xed
    ff78:	cmp	w19, #0xb
    ff7c:	mov	x21, x0
    ff80:	csel	x1, x9, x8, lt  // lt = tstop
    ff84:	mov	x0, x20
    ff88:	bl	d420 <fprintf@plt>
    ff8c:	mov	w8, w0
    ff90:	ldur	x0, [x29, #-8]
    ff94:	add	x9, x25, x24
    ff98:	add	x9, x9, x21
    ff9c:	add	x19, x9, w8, sxtw
    ffa0:	cbnz	x0, ffe0 <__gmpf_out_str@@Base+0x1ac>
    ffa4:	mov	x0, x20
    ffa8:	bl	d4a0 <ferror@plt>
    ffac:	cmp	w0, #0x0
    ffb0:	csel	x0, x19, xzr, eq  // eq = none
    ffb4:	mov	sp, x29
    ffb8:	ldp	x20, x19, [sp, #64]
    ffbc:	ldp	x22, x21, [sp, #48]
    ffc0:	ldp	x24, x23, [sp, #32]
    ffc4:	ldr	x25, [sp, #16]
    ffc8:	ldp	x29, x30, [sp], #80
    ffcc:	ret
    ffd0:	sub	x0, x29, #0x8
    ffd4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
    ffd8:	mov	x21, x0
    ffdc:	b	fecc <__gmpf_out_str@@Base+0x98>
    ffe0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
    ffe4:	b	ffa4 <__gmpf_out_str@@Base+0x170>

000000000000ffe8 <__gmpf_add@@Base>:
    ffe8:	stp	x29, x30, [sp, #-96]!
    ffec:	stp	x28, x27, [sp, #16]
    fff0:	stp	x26, x25, [sp, #32]
    fff4:	stp	x24, x23, [sp, #48]
    fff8:	stp	x22, x21, [sp, #64]
    fffc:	stp	x20, x19, [sp, #80]
   10000:	mov	x29, sp
   10004:	sub	sp, sp, #0x30
   10008:	ldr	w20, [x1, #4]
   1000c:	mov	x19, x0
   10010:	cbz	w20, 10160 <__gmpf_add@@Base+0x178>
   10014:	ldr	w8, [x2, #4]
   10018:	cbz	w8, 1015c <__gmpf_add@@Base+0x174>
   1001c:	eor	w9, w8, w20
   10020:	tbnz	w9, #31, 10178 <__gmpf_add@@Base+0x190>
   10024:	stur	xzr, [x29, #-24]
   10028:	ldr	x9, [x1, #8]
   1002c:	ldr	x10, [x2, #8]
   10030:	ldrsw	x28, [x19]
   10034:	ldr	x0, [x19, #16]
   10038:	mov	w11, #0x7f00                	// #32512
   1003c:	cmp	x9, x10
   10040:	csel	x10, x1, x2, lt  // lt = tstop
   10044:	csel	x12, x2, x1, lt  // lt = tstop
   10048:	csel	w9, w20, w8, lt  // lt = tstop
   1004c:	csel	w8, w8, w20, lt  // lt = tstop
   10050:	ldp	x14, x13, [x12, #8]
   10054:	ldp	x23, x12, [x10, #8]
   10058:	sxtw	x8, w8
   1005c:	sxtw	x9, w9
   10060:	cmp	x8, #0x0
   10064:	cneg	x8, x8, mi  // mi = first
   10068:	cmp	x9, #0x0
   1006c:	cneg	x9, x9, mi  // mi = first
   10070:	subs	x10, x8, x28
   10074:	sub	x27, x14, x23
   10078:	add	x10, x13, x10, lsl #3
   1007c:	csel	x22, x28, x8, gt
   10080:	add	x8, x27, x9
   10084:	csel	x24, x10, x13, gt
   10088:	subs	x8, x8, x28
   1008c:	sub	x10, x28, x27
   10090:	add	x8, x12, x8, lsl #3
   10094:	lsl	x1, x28, #3
   10098:	mov	x21, x14
   1009c:	csel	x25, x10, x9, gt
   100a0:	csel	x26, x8, x12, gt
   100a4:	cmp	x1, x11
   100a8:	b.hi	10444 <__gmpf_add@@Base+0x45c>  // b.pmore
   100ac:	add	x9, x1, #0xf
   100b0:	mov	x8, sp
   100b4:	and	x9, x9, #0xfffffffffffffff0
   100b8:	sub	x9, x8, x9
   100bc:	mov	sp, x9
   100c0:	cmp	x27, x28
   100c4:	b.ge	10460 <__gmpf_add@@Base+0x478>  // b.tcont
   100c8:	subs	x8, x22, x27
   100cc:	add	x10, x25, x27
   100d0:	stp	x10, x0, [x29, #-40]
   100d4:	b.le	10198 <__gmpf_add@@Base+0x1b0>
   100d8:	subs	x28, x10, x22
   100dc:	stur	x9, [x29, #-48]
   100e0:	b.le	101ec <__gmpf_add@@Base+0x204>
   100e4:	mov	x0, x9
   100e8:	mov	x1, x26
   100ec:	mov	x2, x28
   100f0:	bl	ca50 <__gmpn_copyi@plt>
   100f4:	ldur	x1, [x29, #-48]
   100f8:	subs	x25, x22, x27
   100fc:	add	x27, x1, x28, lsl #3
   10100:	b.eq	10270 <__gmpf_add@@Base+0x288>  // b.none
   10104:	add	x2, x26, x28, lsl #3
   10108:	mov	x0, x27
   1010c:	mov	x1, x24
   10110:	mov	x3, x25
   10114:	bl	ca70 <__gmpn_add_n@plt>
   10118:	cbz	x0, 1028c <__gmpf_add@@Base+0x2a4>
   1011c:	ldp	x1, x28, [x29, #-48]
   10120:	mov	w26, #0x1                   	// #1
   10124:	add	x8, x23, x28
   10128:	lsl	x8, x8, #3
   1012c:	mov	x23, x21
   10130:	sub	x8, x8, x21, lsl #3
   10134:	ldur	x21, [x29, #-32]
   10138:	add	x8, x1, x8
   1013c:	cmp	x25, x22
   10140:	b.ge	103d4 <__gmpf_add@@Base+0x3ec>  // b.tcont
   10144:	ldr	x9, [x24, x25, lsl #3]
   10148:	add	x25, x25, #0x1
   1014c:	adds	x9, x9, #0x1
   10150:	str	x9, [x8], #8
   10154:	b.cs	1013c <__gmpf_add@@Base+0x154>  // b.hs, b.nlast
   10158:	b	10298 <__gmpf_add@@Base+0x2b0>
   1015c:	mov	x2, x1
   10160:	cmp	x2, x19
   10164:	b.eq	10494 <__gmpf_add@@Base+0x4ac>  // b.none
   10168:	mov	x0, x19
   1016c:	mov	x1, x2
   10170:	bl	c150 <__gmpf_set@plt>
   10174:	b	10494 <__gmpf_add@@Base+0x4ac>
   10178:	neg	w8, w8
   1017c:	stur	w8, [x29, #-20]
   10180:	ldur	q0, [x2, #8]
   10184:	sub	x2, x29, #0x18
   10188:	mov	x0, x19
   1018c:	stur	q0, [x29, #-16]
   10190:	bl	ce00 <__gmpf_sub@plt>
   10194:	b	10494 <__gmpf_add@@Base+0x4ac>
   10198:	mov	x0, x9
   1019c:	mov	x1, x26
   101a0:	mov	x2, x25
   101a4:	sub	x23, x10, x22
   101a8:	mov	x28, x9
   101ac:	bl	ca50 <__gmpn_copyi@plt>
   101b0:	subs	x8, x27, x22
   101b4:	b.eq	101c8 <__gmpf_add@@Base+0x1e0>  // b.none
   101b8:	add	x0, x28, x25, lsl #3
   101bc:	lsl	x2, x8, #3
   101c0:	mov	w1, wzr
   101c4:	bl	c5f0 <memset@plt>
   101c8:	add	x0, x28, x23, lsl #3
   101cc:	mov	x1, x24
   101d0:	mov	x2, x22
   101d4:	bl	ca50 <__gmpn_copyi@plt>
   101d8:	mov	x23, x21
   101dc:	mov	x1, x28
   101e0:	ldp	x28, x21, [x29, #-40]
   101e4:	mov	x26, xzr
   101e8:	b	103d4 <__gmpf_add@@Base+0x3ec>
   101ec:	sub	x27, x8, x25
   101f0:	mov	x0, x9
   101f4:	mov	x1, x24
   101f8:	mov	x2, x27
   101fc:	bl	ca50 <__gmpn_copyi@plt>
   10200:	sub	x28, x22, x27
   10204:	cbz	x25, 1027c <__gmpf_add@@Base+0x294>
   10208:	lsl	x8, x27, #3
   1020c:	ldur	x27, [x29, #-48]
   10210:	add	x1, x24, x8
   10214:	mov	x2, x26
   10218:	mov	x3, x25
   1021c:	add	x0, x27, x8
   10220:	bl	ca70 <__gmpn_add_n@plt>
   10224:	cbz	x0, 1034c <__gmpf_add@@Base+0x364>
   10228:	add	x8, x23, x22
   1022c:	lsl	x9, x8, #3
   10230:	mov	x23, x21
   10234:	sub	x8, x8, x21
   10238:	sub	x9, x9, x21, lsl #3
   1023c:	ldur	x21, [x29, #-32]
   10240:	add	x8, x24, x8, lsl #3
   10244:	add	x9, x27, x9
   10248:	mov	w26, #0x1                   	// #1
   1024c:	mov	x1, x27
   10250:	cmp	x25, x28
   10254:	b.ge	103d0 <__gmpf_add@@Base+0x3e8>  // b.tcont
   10258:	ldr	x10, [x8], #8
   1025c:	add	x25, x25, #0x1
   10260:	adds	x10, x10, #0x1
   10264:	str	x10, [x9], #8
   10268:	b.cs	10250 <__gmpf_add@@Base+0x268>  // b.hs, b.nlast
   1026c:	b	10358 <__gmpf_add@@Base+0x370>
   10270:	mov	x23, x21
   10274:	ldp	x28, x21, [x29, #-40]
   10278:	b	10298 <__gmpf_add@@Base+0x2b0>
   1027c:	mov	x23, x21
   10280:	ldur	x21, [x29, #-32]
   10284:	ldur	x1, [x29, #-48]
   10288:	b	10358 <__gmpf_add@@Base+0x370>
   1028c:	mov	x23, x21
   10290:	ldp	x28, x21, [x29, #-40]
   10294:	ldur	x1, [x29, #-48]
   10298:	cmp	x27, x24
   1029c:	mov	x26, xzr
   102a0:	b.eq	103d4 <__gmpf_add@@Base+0x3ec>  // b.none
   102a4:	subs	x8, x22, x25
   102a8:	b.le	103d4 <__gmpf_add@@Base+0x3ec>
   102ac:	cmp	x8, #0x4
   102b0:	b.cc	1032c <__gmpf_add@@Base+0x344>  // b.lo, b.ul, b.last
   102b4:	lsl	x10, x25, #3
   102b8:	lsl	x9, x28, #3
   102bc:	lsl	x11, x22, #3
   102c0:	add	x12, x10, x9
   102c4:	sub	x12, x12, x11
   102c8:	add	x12, x1, x12
   102cc:	add	x11, x24, x11
   102d0:	cmp	x12, x11
   102d4:	b.cs	102e8 <__gmpf_add@@Base+0x300>  // b.hs, b.nlast
   102d8:	add	x9, x1, x9
   102dc:	add	x11, x24, x10
   102e0:	cmp	x9, x11
   102e4:	b.hi	1032c <__gmpf_add@@Base+0x344>  // b.pmore
   102e8:	add	x11, x10, x24
   102ec:	add	x12, x10, x28, lsl #3
   102f0:	add	x10, x11, #0x10
   102f4:	sub	x11, x12, x22, lsl #3
   102f8:	and	x9, x8, #0xfffffffffffffffc
   102fc:	add	x11, x11, x1
   10300:	add	x25, x25, x9
   10304:	add	x11, x11, #0x10
   10308:	mov	x12, x9
   1030c:	ldp	q0, q1, [x10, #-16]
   10310:	subs	x12, x12, #0x4
   10314:	add	x10, x10, #0x20
   10318:	stp	q0, q1, [x11, #-16]
   1031c:	add	x11, x11, #0x20
   10320:	b.ne	1030c <__gmpf_add@@Base+0x324>  // b.any
   10324:	cmp	x8, x9
   10328:	b.eq	101e4 <__gmpf_add@@Base+0x1fc>  // b.none
   1032c:	sub	x8, x25, x22
   10330:	add	x9, x1, x28, lsl #3
   10334:	add	x10, x24, x25, lsl #3
   10338:	ldr	x11, [x10], #8
   1033c:	str	x11, [x9, x8, lsl #3]
   10340:	adds	x8, x8, #0x1
   10344:	b.cc	10338 <__gmpf_add@@Base+0x350>  // b.lo, b.ul, b.last
   10348:	b	101e4 <__gmpf_add@@Base+0x1fc>
   1034c:	mov	x23, x21
   10350:	ldur	x21, [x29, #-32]
   10354:	mov	x1, x27
   10358:	cmp	x24, x1
   1035c:	mov	x26, xzr
   10360:	b.eq	103d0 <__gmpf_add@@Base+0x3e8>  // b.none
   10364:	subs	x9, x28, x25
   10368:	b.le	103d0 <__gmpf_add@@Base+0x3e8>
   1036c:	cmp	x9, #0x4
   10370:	lsl	x8, x22, #3
   10374:	b.cc	103a8 <__gmpf_add@@Base+0x3c0>  // b.lo, b.ul, b.last
   10378:	add	x10, x8, x25, lsl #3
   1037c:	sub	x10, x10, x28, lsl #3
   10380:	add	x10, x1, x10
   10384:	add	x11, x24, x8
   10388:	cmp	x10, x11
   1038c:	add	x11, x25, x22
   10390:	b.cs	103f0 <__gmpf_add@@Base+0x408>  // b.hs, b.nlast
   10394:	sub	x12, x11, x28
   10398:	add	x10, x1, x8
   1039c:	add	x12, x24, x12, lsl #3
   103a0:	cmp	x10, x12
   103a4:	b.ls	103f0 <__gmpf_add@@Base+0x408>  // b.plast
   103a8:	ldur	x14, [x29, #-40]
   103ac:	sub	x9, x25, x14
   103b0:	add	x10, x1, x8
   103b4:	add	x8, x24, x8
   103b8:	lsl	x11, x9, #3
   103bc:	ldr	x12, [x8, x11]
   103c0:	adds	x9, x9, #0x1
   103c4:	str	x12, [x10, x11]
   103c8:	b.cc	103b8 <__gmpf_add@@Base+0x3d0>  // b.lo, b.ul, b.last
   103cc:	mov	x26, xzr
   103d0:	mov	x28, x22
   103d4:	mov	x0, x21
   103d8:	mov	x2, x28
   103dc:	bl	ca50 <__gmpn_copyi@plt>
   103e0:	add	x22, x26, x28
   103e4:	add	x23, x26, x23
   103e8:	str	x26, [x21, x28, lsl #3]
   103ec:	b	10478 <__gmpf_add@@Base+0x490>
   103f0:	ldur	x14, [x29, #-40]
   103f4:	lsl	x12, x25, #3
   103f8:	add	x12, x12, x22, lsl #3
   103fc:	and	x10, x9, #0xfffffffffffffffc
   10400:	sub	x11, x11, x14
   10404:	sub	x12, x12, x14, lsl #3
   10408:	add	x11, x24, x11, lsl #3
   1040c:	add	x12, x12, x1
   10410:	add	x25, x25, x10
   10414:	add	x11, x11, #0x10
   10418:	add	x12, x12, #0x10
   1041c:	mov	x13, x10
   10420:	ldp	q0, q1, [x11, #-16]
   10424:	subs	x13, x13, #0x4
   10428:	add	x11, x11, #0x20
   1042c:	stp	q0, q1, [x12, #-16]
   10430:	add	x12, x12, #0x20
   10434:	b.ne	10420 <__gmpf_add@@Base+0x438>  // b.any
   10438:	cmp	x9, x10
   1043c:	b.ne	103ac <__gmpf_add@@Base+0x3c4>  // b.any
   10440:	b	103cc <__gmpf_add@@Base+0x3e4>
   10444:	stur	x0, [x29, #-32]
   10448:	sub	x0, x29, #0x18
   1044c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   10450:	mov	x9, x0
   10454:	ldur	x0, [x29, #-32]
   10458:	cmp	x27, x28
   1045c:	b.lt	100c8 <__gmpf_add@@Base+0xe0>  // b.tstop
   10460:	cmp	x0, x24
   10464:	b.eq	10474 <__gmpf_add@@Base+0x48c>  // b.none
   10468:	mov	x1, x24
   1046c:	mov	x2, x22
   10470:	bl	ca50 <__gmpn_copyi@plt>
   10474:	mov	x23, x21
   10478:	neg	w8, w22
   1047c:	cmp	w20, #0x0
   10480:	csel	x8, x8, x22, lt  // lt = tstop
   10484:	str	w8, [x19, #4]
   10488:	str	x23, [x19, #8]
   1048c:	ldur	x0, [x29, #-24]
   10490:	cbnz	x0, 104b4 <__gmpf_add@@Base+0x4cc>
   10494:	mov	sp, x29
   10498:	ldp	x20, x19, [sp, #80]
   1049c:	ldp	x22, x21, [sp, #64]
   104a0:	ldp	x24, x23, [sp, #48]
   104a4:	ldp	x26, x25, [sp, #32]
   104a8:	ldp	x28, x27, [sp, #16]
   104ac:	ldp	x29, x30, [sp], #96
   104b0:	ret
   104b4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   104b8:	b	10494 <__gmpf_add@@Base+0x4ac>

00000000000104bc <__gmpf_add_ui@@Base>:
   104bc:	sub	sp, sp, #0x70
   104c0:	stp	x29, x30, [sp, #32]
   104c4:	stp	x24, x23, [sp, #64]
   104c8:	stp	x22, x21, [sp, #80]
   104cc:	stp	x20, x19, [sp, #96]
   104d0:	mov	x23, x1
   104d4:	ldrsw	x22, [x1, #4]
   104d8:	ldr	x1, [x1, #16]
   104dc:	ldr	x24, [x23, #8]
   104e0:	mov	x21, x2
   104e4:	cmp	w22, #0x0
   104e8:	mov	x19, x0
   104ec:	str	x25, [sp, #48]
   104f0:	add	x29, sp, #0x20
   104f4:	b.le	10548 <__gmpf_add_ui@@Base+0x8c>
   104f8:	ldr	x20, [x19, #16]
   104fc:	ldrsw	x8, [x19]
   10500:	cbz	x21, 10514 <__gmpf_add_ui@@Base+0x58>
   10504:	subs	x25, x24, #0x1
   10508:	b.lt	10578 <__gmpf_add_ui@@Base+0xbc>  // b.tstop
   1050c:	cmp	x24, x8
   10510:	b.le	10608 <__gmpf_add_ui@@Base+0x14c>
   10514:	cmp	x23, x19
   10518:	b.eq	10808 <__gmpf_add_ui@@Base+0x34c>  // b.none
   1051c:	cmp	w22, w8
   10520:	csinc	x21, x22, x8, le
   10524:	add	x8, x1, x22, lsl #3
   10528:	sub	x1, x8, x21, lsl #3
   1052c:	mov	x0, x20
   10530:	mov	x2, x21
   10534:	bl	ca50 <__gmpn_copyi@plt>
   10538:	str	w21, [x19, #4]
   1053c:	ldr	x8, [x23, #8]
   10540:	str	x8, [x19, #8]
   10544:	b	10808 <__gmpf_add_ui@@Base+0x34c>
   10548:	cbz	w22, 105e4 <__gmpf_add_ui@@Base+0x128>
   1054c:	neg	w8, w22
   10550:	stp	x24, x1, [sp, #16]
   10554:	add	x1, sp, #0x8
   10558:	mov	x0, x19
   1055c:	mov	x2, x21
   10560:	str	w8, [sp, #12]
   10564:	bl	c1f0 <__gmpf_sub_ui@plt>
   10568:	ldr	w8, [x19, #4]
   1056c:	neg	w8, w8
   10570:	str	w8, [x19, #4]
   10574:	b	10808 <__gmpf_add_ui@@Base+0x34c>
   10578:	neg	x9, x24
   1057c:	cmp	x9, x8
   10580:	b.ge	1064c <__gmpf_add_ui@@Base+0x190>  // b.tcont
   10584:	add	x10, x8, x24
   10588:	sub	x9, x22, x24
   1058c:	sub	x10, x22, x10
   10590:	cmp	x9, x8
   10594:	add	x8, x10, #0x1
   10598:	add	x8, x1, x8, lsl #3
   1059c:	csinc	x9, xzr, x10, lt  // lt = tstop
   105a0:	csel	x1, x1, x8, lt  // lt = tstop
   105a4:	cmp	x20, x1
   105a8:	sub	x22, x22, x9
   105ac:	b.eq	105bc <__gmpf_add_ui@@Base+0x100>  // b.none
   105b0:	mov	x0, x20
   105b4:	mov	x2, x22
   105b8:	bl	ca50 <__gmpn_copyi@plt>
   105bc:	cbz	x24, 105d0 <__gmpf_add_ui@@Base+0x114>
   105c0:	add	x0, x20, x22, lsl #3
   105c4:	neg	x2, x24, lsl #3
   105c8:	mov	w1, wzr
   105cc:	bl	c5f0 <memset@plt>
   105d0:	sub	x8, x22, x24
   105d4:	mov	w9, #0x1                   	// #1
   105d8:	str	x21, [x20, x8, lsl #3]
   105dc:	add	w8, w8, #0x1
   105e0:	b	10800 <__gmpf_add_ui@@Base+0x344>
   105e4:	mov	x0, x19
   105e8:	mov	x1, x21
   105ec:	ldp	x20, x19, [sp, #96]
   105f0:	ldp	x22, x21, [sp, #80]
   105f4:	ldp	x24, x23, [sp, #64]
   105f8:	ldr	x25, [sp, #48]
   105fc:	ldp	x29, x30, [sp, #32]
   10600:	add	sp, sp, #0x70
   10604:	b	c6a0 <__gmpf_set_ui@plt>
   10608:	cmp	x24, x22
   1060c:	b.le	10660 <__gmpf_add_ui@@Base+0x1a4>
   10610:	add	x8, x20, x24, lsl #3
   10614:	sub	x0, x8, x22, lsl #3
   10618:	mov	x2, x22
   1061c:	bl	c000 <__gmpn_copyd@plt>
   10620:	mvn	x8, x22
   10624:	adds	x8, x24, x8
   10628:	str	x21, [x20]
   1062c:	b.eq	10640 <__gmpf_add_ui@@Base+0x184>  // b.none
   10630:	add	x0, x20, #0x8
   10634:	lsl	x2, x8, #3
   10638:	mov	w1, wzr
   1063c:	bl	c5f0 <memset@plt>
   10640:	str	w24, [x19, #4]
   10644:	str	x24, [x19, #8]
   10648:	b	10808 <__gmpf_add_ui@@Base+0x34c>
   1064c:	mov	w8, #0x1                   	// #1
   10650:	str	x21, [x20]
   10654:	str	w8, [x19, #4]
   10658:	str	x8, [x19, #8]
   1065c:	b	10808 <__gmpf_add_ui@@Base+0x34c>
   10660:	sub	x9, x22, x8
   10664:	cmp	w22, w8
   10668:	add	x9, x1, x9, lsl #3
   1066c:	csel	w8, w22, w8, lt  // lt = tstop
   10670:	csel	x22, x9, x1, gt
   10674:	cmp	x20, x22
   10678:	sxtw	x23, w8
   1067c:	b.eq	10690 <__gmpf_add_ui@@Base+0x1d4>  // b.none
   10680:	sub	x2, x23, x24
   10684:	mov	x0, x20
   10688:	mov	x1, x22
   1068c:	bl	ca50 <__gmpn_copyi@plt>
   10690:	lsl	x11, x23, #3
   10694:	lsl	x10, x24, #3
   10698:	add	x9, x22, x11
   1069c:	sub	x8, x9, x10
   106a0:	ldr	x12, [x8]
   106a4:	add	x8, x20, x11
   106a8:	sub	x13, x8, x10
   106ac:	adds	x12, x12, x21
   106b0:	str	x12, [x13]
   106b4:	b.cc	10794 <__gmpf_add_ui@@Base+0x2d8>  // b.lo, b.ul, b.last
   106b8:	sub	x14, x20, x10
   106bc:	sub	x13, x22, x10
   106c0:	mov	w12, #0x1                   	// #1
   106c4:	mov	w10, #0x1                   	// #1
   106c8:	cmp	x10, x24
   106cc:	b.ge	107f4 <__gmpf_add_ui@@Base+0x338>  // b.tcont
   106d0:	add	x15, x13, x11
   106d4:	ldr	x15, [x15, #8]
   106d8:	add	x16, x14, x11
   106dc:	add	x10, x10, #0x1
   106e0:	add	x14, x14, #0x8
   106e4:	add	x13, x13, #0x8
   106e8:	adds	x15, x15, #0x1
   106ec:	sub	x25, x25, #0x1
   106f0:	str	x15, [x16, #8]
   106f4:	b.cs	106c8 <__gmpf_add_ui@@Base+0x20c>  // b.hs, b.nlast
   106f8:	cmp	x22, x20
   106fc:	mov	x12, xzr
   10700:	b.eq	107f4 <__gmpf_add_ui@@Base+0x338>  // b.none
   10704:	subs	x15, x24, x10
   10708:	b.le	107f4 <__gmpf_add_ui@@Base+0x338>
   1070c:	cmp	x15, #0x4
   10710:	b.cc	10778 <__gmpf_add_ui@@Base+0x2bc>  // b.lo, b.ul, b.last
   10714:	add	x12, x14, x11
   10718:	add	x12, x12, #0x8
   1071c:	cmp	x12, x9
   10720:	b.cs	10734 <__gmpf_add_ui@@Base+0x278>  // b.hs, b.nlast
   10724:	add	x12, x13, x11
   10728:	add	x12, x12, #0x8
   1072c:	cmp	x12, x8
   10730:	b.cc	10778 <__gmpf_add_ui@@Base+0x2bc>  // b.lo, b.ul, b.last
   10734:	add	x12, x14, x11
   10738:	add	x13, x13, x11
   1073c:	sub	x14, x24, x10
   10740:	and	x16, x25, #0xfffffffffffffffc
   10744:	and	x11, x15, #0xfffffffffffffffc
   10748:	add	x12, x12, #0x18
   1074c:	add	x13, x13, #0x18
   10750:	add	x10, x16, x10
   10754:	and	x14, x14, #0xfffffffffffffffc
   10758:	ldp	q0, q1, [x13, #-16]
   1075c:	subs	x14, x14, #0x4
   10760:	add	x13, x13, #0x20
   10764:	stp	q0, q1, [x12, #-16]
   10768:	add	x12, x12, #0x20
   1076c:	b.ne	10758 <__gmpf_add_ui@@Base+0x29c>  // b.any
   10770:	cmp	x15, x11
   10774:	b.eq	107f0 <__gmpf_add_ui@@Base+0x334>  // b.none
   10778:	sub	x10, x10, x24
   1077c:	lsl	x11, x10, #3
   10780:	ldr	x12, [x9, x11]
   10784:	adds	x10, x10, #0x1
   10788:	str	x12, [x8, x11]
   1078c:	b.cc	1077c <__gmpf_add_ui@@Base+0x2c0>  // b.lo, b.ul, b.last
   10790:	b	107f0 <__gmpf_add_ui@@Base+0x334>
   10794:	cmp	x24, #0x2
   10798:	mov	x12, xzr
   1079c:	b.lt	107f4 <__gmpf_add_ui@@Base+0x338>  // b.tstop
   107a0:	cmp	x22, x20
   107a4:	b.eq	107f4 <__gmpf_add_ui@@Base+0x338>  // b.none
   107a8:	cmp	x25, #0x4
   107ac:	b.cc	107d4 <__gmpf_add_ui@@Base+0x318>  // b.lo, b.ul, b.last
   107b0:	sub	x10, x23, x24
   107b4:	lsl	x10, x10, #3
   107b8:	add	x11, x10, #0x8
   107bc:	add	x12, x20, x11
   107c0:	cmp	x12, x9
   107c4:	b.cs	10824 <__gmpf_add_ui@@Base+0x368>  // b.hs, b.nlast
   107c8:	add	x11, x22, x11
   107cc:	cmp	x11, x8
   107d0:	b.cs	10824 <__gmpf_add_ui@@Base+0x368>  // b.hs, b.nlast
   107d4:	mov	w10, #0x1                   	// #1
   107d8:	sub	x10, x10, x24
   107dc:	lsl	x11, x10, #3
   107e0:	ldr	x12, [x9, x11]
   107e4:	adds	x10, x10, #0x1
   107e8:	str	x12, [x8, x11]
   107ec:	b.cc	107dc <__gmpf_add_ui@@Base+0x320>  // b.lo, b.ul, b.last
   107f0:	mov	x12, xzr
   107f4:	str	x12, [x8]
   107f8:	add	w8, w23, w12
   107fc:	add	x9, x12, x24
   10800:	str	w8, [x19, #4]
   10804:	str	x9, [x19, #8]
   10808:	ldp	x20, x19, [sp, #96]
   1080c:	ldp	x22, x21, [sp, #80]
   10810:	ldp	x24, x23, [sp, #64]
   10814:	ldr	x25, [sp, #48]
   10818:	ldp	x29, x30, [sp, #32]
   1081c:	add	sp, sp, #0x70
   10820:	ret
   10824:	and	x11, x25, #0xfffffffffffffffc
   10828:	add	x13, x10, #0x18
   1082c:	orr	x10, x11, #0x1
   10830:	add	x12, x22, x13
   10834:	add	x13, x20, x13
   10838:	mov	x14, x11
   1083c:	ldp	q0, q1, [x12, #-16]
   10840:	subs	x14, x14, #0x4
   10844:	add	x12, x12, #0x20
   10848:	stp	q0, q1, [x13, #-16]
   1084c:	add	x13, x13, #0x20
   10850:	b.ne	1083c <__gmpf_add_ui@@Base+0x380>  // b.any
   10854:	cmp	x25, x11
   10858:	b.eq	107f0 <__gmpf_add_ui@@Base+0x334>  // b.none
   1085c:	b	107d8 <__gmpf_add_ui@@Base+0x31c>

0000000000010860 <__gmpf_sub@@Base>:
   10860:	stp	x29, x30, [sp, #-96]!
   10864:	stp	x28, x27, [sp, #16]
   10868:	stp	x26, x25, [sp, #32]
   1086c:	stp	x24, x23, [sp, #48]
   10870:	stp	x22, x21, [sp, #64]
   10874:	stp	x20, x19, [sp, #80]
   10878:	mov	x29, sp
   1087c:	sub	sp, sp, #0x70
   10880:	ldr	w8, [x1, #4]
   10884:	mov	x25, x0
   10888:	cbz	w8, 10a28 <__gmpf_sub@@Base+0x1c8>
   1088c:	ldr	w9, [x2, #4]
   10890:	cbz	w9, 10a38 <__gmpf_sub@@Base+0x1d8>
   10894:	eor	w10, w9, w8
   10898:	tbnz	w10, #31, 10a4c <__gmpf_sub@@Base+0x1ec>
   1089c:	stur	xzr, [x29, #-24]
   108a0:	ldr	x10, [x1, #8]
   108a4:	ldr	x11, [x2, #8]
   108a8:	ldrsw	x12, [x25]
   108ac:	ldr	x18, [x25, #16]
   108b0:	cmp	x10, x11
   108b4:	cset	w10, lt  // lt = tstop
   108b8:	csel	x13, x1, x2, lt  // lt = tstop
   108bc:	csel	x14, x2, x1, lt  // lt = tstop
   108c0:	csel	w11, w8, w9, lt  // lt = tstop
   108c4:	csel	w9, w9, w8, lt  // lt = tstop
   108c8:	eor	w24, w10, w8, lsr #31
   108cc:	ldp	x4, x23, [x14, #8]
   108d0:	ldp	x5, x8, [x13, #8]
   108d4:	sxtw	x9, w9
   108d8:	sxtw	x10, w11
   108dc:	cmp	x9, #0x0
   108e0:	cneg	x9, x9, mi  // mi = first
   108e4:	cmp	x10, #0x0
   108e8:	sub	x0, x4, x5
   108ec:	cneg	x10, x10, mi  // mi = first
   108f0:	cmp	x0, #0x1
   108f4:	add	x21, x12, #0x1
   108f8:	b.gt	10920 <__gmpf_sub@@Base+0xc0>
   108fc:	cbz	x0, 10a80 <__gmpf_sub@@Base+0x220>
   10900:	add	x11, x23, x9, lsl #3
   10904:	ldur	x11, [x11, #-8]
   10908:	cmp	x11, #0x1
   1090c:	b.ne	10920 <__gmpf_sub@@Base+0xc0>  // b.any
   10910:	add	x11, x8, x10, lsl #3
   10914:	ldur	x11, [x11, #-8]
   10918:	cmn	x11, #0x1
   1091c:	b.eq	10c24 <__gmpf_sub@@Base+0x3c4>  // b.none
   10920:	mov	x20, x4
   10924:	mov	x19, x10
   10928:	subs	x10, x9, x21
   1092c:	add	x10, x23, x10, lsl #3
   10930:	add	x11, x19, x0
   10934:	csel	x27, x21, x9, gt
   10938:	csel	x14, x10, x23, gt
   1093c:	subs	x9, x21, x0
   10940:	subs	x10, x11, x21
   10944:	add	x10, x8, x10, lsl #3
   10948:	csel	x22, x9, x19, gt
   1094c:	csel	x26, x10, x8, gt
   10950:	cmp	x21, x0
   10954:	b.le	10a6c <__gmpf_sub@@Base+0x20c>
   10958:	lsl	x1, x21, #3
   1095c:	mov	w8, #0x7f00                	// #32512
   10960:	cmp	x1, x8
   10964:	stp	x20, x27, [x29, #-40]
   10968:	b.hi	1158c <__gmpf_sub@@Base+0xd2c>  // b.pmore
   1096c:	add	x9, x1, #0xf
   10970:	mov	x8, sp
   10974:	and	x9, x9, #0xfffffffffffffff0
   10978:	sub	x1, x8, x9
   1097c:	mov	sp, x1
   10980:	cbz	x22, 109f4 <__gmpf_sub@@Base+0x194>
   10984:	add	x8, x19, x4
   10988:	ldur	x13, [x29, #-32]
   1098c:	sub	x8, x8, x5
   10990:	cmp	x8, x21
   10994:	csel	x19, x8, x21, lt  // lt = tstop
   10998:	add	x11, x5, x19
   1099c:	lsl	x10, x13, #3
   109a0:	lsl	x9, x4, #3
   109a4:	lsl	x8, x19, #3
   109a8:	lsl	x11, x11, #3
   109ac:	sub	x12, x8, x10
   109b0:	sub	x8, x10, x8
   109b4:	sub	x10, x13, x19
   109b8:	sub	x11, x11, x9
   109bc:	add	x21, x1, x12
   109c0:	add	x20, x1, x8
   109c4:	add	x28, x1, x11
   109c8:	add	x27, x14, x10, lsl #3
   109cc:	ldr	x8, [x26]
   109d0:	cbnz	x8, 10ad8 <__gmpf_sub@@Base+0x278>
   109d4:	add	x26, x26, #0x8
   109d8:	sub	x22, x22, #0x1
   109dc:	sub	x21, x21, #0x8
   109e0:	sub	x28, x28, #0x8
   109e4:	sub	x19, x19, #0x1
   109e8:	add	x20, x20, #0x8
   109ec:	add	x27, x27, #0x8
   109f0:	cbnz	x22, 109cc <__gmpf_sub@@Base+0x16c>
   109f4:	ldur	x27, [x29, #-32]
   109f8:	mov	x0, x18
   109fc:	mov	x1, x14
   10a00:	mov	x2, x27
   10a04:	bl	ca50 <__gmpn_copyi@plt>
   10a08:	ldur	x20, [x29, #-40]
   10a0c:	ldur	x0, [x29, #-24]
   10a10:	cbz	x0, 10bec <__gmpf_sub@@Base+0x38c>
   10a14:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   10a18:	cbnz	x27, 10bf0 <__gmpf_sub@@Base+0x390>
   10a1c:	str	wzr, [x25, #4]
   10a20:	str	xzr, [x25, #8]
   10a24:	b	10c04 <__gmpf_sub@@Base+0x3a4>
   10a28:	mov	x0, x25
   10a2c:	mov	x1, x2
   10a30:	bl	cbf0 <__gmpf_neg@plt>
   10a34:	b	10c04 <__gmpf_sub@@Base+0x3a4>
   10a38:	cmp	x25, x1
   10a3c:	b.eq	10c04 <__gmpf_sub@@Base+0x3a4>  // b.none
   10a40:	mov	x0, x25
   10a44:	bl	c150 <__gmpf_set@plt>
   10a48:	b	10c04 <__gmpf_sub@@Base+0x3a4>
   10a4c:	neg	w8, w9
   10a50:	stur	w8, [x29, #-20]
   10a54:	ldur	q0, [x2, #8]
   10a58:	sub	x2, x29, #0x18
   10a5c:	mov	x0, x25
   10a60:	stur	q0, [x29, #-16]
   10a64:	bl	bf70 <__gmpf_add@plt>
   10a68:	b	10c04 <__gmpf_sub@@Base+0x3a4>
   10a6c:	cmp	x18, x14
   10a70:	b.eq	10be4 <__gmpf_sub@@Base+0x384>  // b.none
   10a74:	mov	x0, x18
   10a78:	mov	x1, x14
   10a7c:	b	10bdc <__gmpf_sub@@Base+0x37c>
   10a80:	add	x14, x8, x10, lsl #3
   10a84:	add	x15, x23, x9, lsl #3
   10a88:	mov	x12, xzr
   10a8c:	sub	x11, x10, x9
   10a90:	sub	x13, x9, x10
   10a94:	sub	x14, x14, #0x8
   10a98:	sub	x15, x15, #0x8
   10a9c:	lsl	x16, x12, #3
   10aa0:	ldr	x17, [x15, x16]
   10aa4:	ldr	x16, [x14, x16]
   10aa8:	cmp	x17, x16
   10aac:	add	x16, x9, x12
   10ab0:	b.ne	10b54 <__gmpf_sub@@Base+0x2f4>  // b.any
   10ab4:	sub	x16, x16, #0x1
   10ab8:	cbz	x16, 10b9c <__gmpf_sub@@Base+0x33c>
   10abc:	sub	x12, x12, #0x1
   10ac0:	cmn	x10, x12
   10ac4:	b.ne	10a9c <__gmpf_sub@@Base+0x23c>  // b.any
   10ac8:	mov	x9, x10
   10acc:	mov	x11, x13
   10ad0:	mov	x8, x23
   10ad4:	b	10ba0 <__gmpf_sub@@Base+0x340>
   10ad8:	ldur	x10, [x29, #-32]
   10adc:	stur	w24, [x29, #-52]
   10ae0:	stur	x25, [x29, #-48]
   10ae4:	cbz	x10, 10b28 <__gmpf_sub@@Base+0x2c8>
   10ae8:	ldur	x3, [x29, #-32]
   10aec:	mov	x24, x14
   10af0:	add	x10, x5, x3
   10af4:	lsl	x10, x10, #3
   10af8:	sub	x9, x10, x9
   10afc:	add	x25, x1, x9
   10b00:	sub	x23, x3, #0x1
   10b04:	ldr	x9, [x24]
   10b08:	cbnz	x9, 10c48 <__gmpf_sub@@Base+0x3e8>
   10b0c:	add	x24, x24, #0x8
   10b10:	sub	x3, x3, #0x1
   10b14:	add	x21, x21, #0x8
   10b18:	sub	x20, x20, #0x8
   10b1c:	sub	x25, x25, #0x8
   10b20:	sub	x23, x23, #0x1
   10b24:	cbnz	x3, 10b04 <__gmpf_sub@@Base+0x2a4>
   10b28:	mov	x0, x18
   10b2c:	mov	x1, x26
   10b30:	mov	x2, x22
   10b34:	bl	ca50 <__gmpn_copyi@plt>
   10b38:	ldur	w24, [x29, #-52]
   10b3c:	ldp	x25, x20, [x29, #-48]
   10b40:	mov	x27, x22
   10b44:	eor	w24, w24, #0x1
   10b48:	ldur	x0, [x29, #-24]
   10b4c:	cbz	x0, 10bec <__gmpf_sub@@Base+0x38c>
   10b50:	b	10a14 <__gmpf_sub@@Base+0x1b4>
   10b54:	add	x9, x10, x12
   10b58:	csel	x19, x16, x9, cc  // cc = lo, ul, last
   10b5c:	csel	x13, x23, x8, cc  // cc = lo, ul, last
   10b60:	csel	x23, x8, x23, cc  // cc = lo, ul, last
   10b64:	csel	x9, x9, x16, cc  // cc = lo, ul, last
   10b68:	sub	x10, x19, #0x1
   10b6c:	add	x8, x23, x9, lsl #3
   10b70:	ldr	x11, [x13, x10, lsl #3]
   10b74:	ldur	x8, [x8, #-8]
   10b78:	cset	w14, cc  // cc = lo, ul, last
   10b7c:	eor	w24, w24, w14
   10b80:	add	x11, x11, #0x1
   10b84:	cmp	x8, x11
   10b88:	add	x11, x4, x12
   10b8c:	mov	x8, x13
   10b90:	mov	x20, x11
   10b94:	b.ne	10928 <__gmpf_sub@@Base+0xc8>  // b.any
   10b98:	b	10d34 <__gmpf_sub@@Base+0x4d4>
   10b9c:	eor	w24, w24, #0x1
   10ba0:	sub	x20, x4, x9
   10ba4:	cbz	x11, 10bc8 <__gmpf_sub@@Base+0x368>
   10ba8:	sub	x9, x20, x11
   10bac:	sub	x10, x8, #0x8
   10bb0:	ldr	x12, [x10, x11, lsl #3]
   10bb4:	cbnz	x12, 10bc8 <__gmpf_sub@@Base+0x368>
   10bb8:	sub	x11, x11, #0x1
   10bbc:	sub	x20, x20, #0x1
   10bc0:	cbnz	x11, 10bb0 <__gmpf_sub@@Base+0x350>
   10bc4:	mov	x20, x9
   10bc8:	subs	x9, x11, x21
   10bcc:	add	x9, x8, x9, lsl #3
   10bd0:	csel	x27, x21, x11, gt
   10bd4:	csel	x1, x9, x8, gt
   10bd8:	mov	x0, x18
   10bdc:	mov	x2, x27
   10be0:	bl	ca50 <__gmpn_copyi@plt>
   10be4:	ldur	x0, [x29, #-24]
   10be8:	cbnz	x0, 10a14 <__gmpf_sub@@Base+0x1b4>
   10bec:	cbz	x27, 10a1c <__gmpf_sub@@Base+0x1bc>
   10bf0:	neg	w8, w27
   10bf4:	cmp	w24, #0x0
   10bf8:	csel	x8, x27, x8, eq  // eq = none
   10bfc:	str	w8, [x25, #4]
   10c00:	str	x20, [x25, #8]
   10c04:	mov	sp, x29
   10c08:	ldp	x20, x19, [sp, #80]
   10c0c:	ldp	x22, x21, [sp, #64]
   10c10:	ldp	x24, x23, [sp, #48]
   10c14:	ldp	x26, x25, [sp, #32]
   10c18:	ldp	x28, x27, [sp, #16]
   10c1c:	ldp	x29, x30, [sp], #96
   10c20:	ret
   10c24:	cmp	x9, #0x2
   10c28:	b.lt	10d30 <__gmpf_sub@@Base+0x4d0>  // b.tstop
   10c2c:	add	x11, x23, x9, lsl #3
   10c30:	ldur	x12, [x11, #-16]
   10c34:	mov	x11, x4
   10c38:	mov	x20, x4
   10c3c:	mov	x19, x10
   10c40:	cbnz	x12, 10928 <__gmpf_sub@@Base+0xc8>
   10c44:	b	10d34 <__gmpf_sub@@Base+0x4d4>
   10c48:	subs	x9, x3, x0
   10c4c:	stur	x1, [x29, #-72]
   10c50:	b.le	10e10 <__gmpf_sub@@Base+0x5b0>
   10c54:	cbz	x0, 10fac <__gmpf_sub@@Base+0x74c>
   10c58:	add	x10, x22, x0
   10c5c:	subs	x23, x10, x3
   10c60:	stur	x14, [x29, #-88]
   10c64:	stur	x18, [x29, #-64]
   10c68:	b.le	1115c <__gmpf_sub@@Base+0x8fc>
   10c6c:	neg	x8, x8
   10c70:	subs	x2, x23, #0x1
   10c74:	stur	x10, [x29, #-96]
   10c78:	str	x8, [x1]
   10c7c:	b.eq	10cb0 <__gmpf_sub@@Base+0x450>  // b.none
   10c80:	mov	x27, x0
   10c84:	add	x0, x1, #0x8
   10c88:	add	x1, x26, #0x8
   10c8c:	mov	x19, x3
   10c90:	mov	x20, x4
   10c94:	mov	x25, x5
   10c98:	bl	c290 <__gmpn_com@plt>
   10c9c:	ldur	x1, [x29, #-72]
   10ca0:	mov	x0, x27
   10ca4:	mov	x5, x25
   10ca8:	mov	x4, x20
   10cac:	mov	x3, x19
   10cb0:	ldp	x25, x20, [x29, #-48]
   10cb4:	subs	x8, x3, x0
   10cb8:	add	x19, x1, x23, lsl #3
   10cbc:	b.eq	113dc <__gmpf_sub@@Base+0xb7c>  // b.none
   10cc0:	add	x2, x26, x23, lsl #3
   10cc4:	mov	x0, x19
   10cc8:	mov	x1, x24
   10ccc:	stur	x3, [x29, #-80]
   10cd0:	mov	x3, x8
   10cd4:	mov	x26, x4
   10cd8:	mov	x27, x5
   10cdc:	mov	x23, x8
   10ce0:	bl	c2d0 <__gmpn_sub_n@plt>
   10ce4:	ldp	x3, x1, [x29, #-80]
   10ce8:	ldur	x15, [x29, #-64]
   10cec:	mov	x5, x27
   10cf0:	mov	x4, x26
   10cf4:	cbz	x0, 113e4 <__gmpf_sub@@Base+0xb84>
   10cf8:	ldur	x9, [x29, #-32]
   10cfc:	ldur	x10, [x29, #-88]
   10d00:	sub	x8, x5, x4
   10d04:	add	x9, x10, x9, lsl #3
   10d08:	add	x10, x3, x8
   10d0c:	cmp	x10, x3
   10d10:	b.ge	1144c <__gmpf_sub@@Base+0xbec>  // b.tcont
   10d14:	ldr	x10, [x9, x8, lsl #3]
   10d18:	add	x8, x8, #0x1
   10d1c:	sub	x11, x10, #0x1
   10d20:	str	x11, [x28], #8
   10d24:	cbz	x10, 10d08 <__gmpf_sub@@Base+0x4a8>
   10d28:	add	x23, x3, x8
   10d2c:	b	113e4 <__gmpf_sub@@Base+0xb84>
   10d30:	mov	x11, x4
   10d34:	lsl	x12, x10, #3
   10d38:	mov	w13, #0x8                   	// #8
   10d3c:	lsl	x1, x21, #3
   10d40:	add	x14, x12, x8
   10d44:	mov	w27, w24
   10d48:	mov	x0, xzr
   10d4c:	sub	x2, x13, x12
   10d50:	sub	x16, x14, #0x8
   10d54:	add	x17, x23, x9, lsl #3
   10d58:	mov	x3, x1
   10d5c:	stur	x18, [x29, #-64]
   10d60:	add	x18, x9, x0
   10d64:	mov	x15, x2
   10d68:	mov	x13, x0
   10d6c:	mov	x14, x3
   10d70:	add	x12, x10, x0
   10d74:	sub	x24, x18, #0x1
   10d78:	cbz	x12, 10da4 <__gmpf_sub@@Base+0x544>
   10d7c:	cbz	x24, 10da4 <__gmpf_sub@@Base+0x544>
   10d80:	add	x0, x17, x13, lsl #3
   10d84:	ldur	x0, [x0, #-16]
   10d88:	cbnz	x0, 10da4 <__gmpf_sub@@Base+0x544>
   10d8c:	ldr	x3, [x16, x13, lsl #3]
   10d90:	add	x2, x15, #0x8
   10d94:	sub	x0, x13, #0x1
   10d98:	cmn	x3, #0x1
   10d9c:	add	x3, x14, #0x8
   10da0:	b.eq	10d60 <__gmpf_sub@@Base+0x500>  // b.none
   10da4:	add	x16, x11, x13
   10da8:	sub	x28, x16, #0x1
   10dac:	cbz	x24, 10dcc <__gmpf_sub@@Base+0x56c>
   10db0:	mov	x20, x25
   10db4:	cmp	x18, x21
   10db8:	b.le	10ef8 <__gmpf_sub@@Base+0x698>
   10dbc:	add	x9, x23, x9, lsl #3
   10dc0:	sub	x24, x21, #0x1
   10dc4:	sub	x23, x9, x14
   10dc8:	b	10ef8 <__gmpf_sub@@Base+0x698>
   10dcc:	cbz	x12, 10ed0 <__gmpf_sub@@Base+0x670>
   10dd0:	mov	x9, xzr
   10dd4:	sub	x14, x8, x15
   10dd8:	sub	x28, x28, x12
   10ddc:	add	x12, x10, x13
   10de0:	ldr	x15, [x14]
   10de4:	cmn	x15, #0x1
   10de8:	b.ne	10edc <__gmpf_sub@@Base+0x67c>  // b.any
   10dec:	add	x15, x12, x9
   10df0:	sub	x9, x9, #0x1
   10df4:	cmp	x15, #0x1
   10df8:	sub	x14, x14, #0x8
   10dfc:	b.ne	10de0 <__gmpf_sub@@Base+0x580>  // b.any
   10e00:	mov	x20, x25
   10e04:	mov	x12, xzr
   10e08:	mov	x24, xzr
   10e0c:	b	10ef8 <__gmpf_sub@@Base+0x698>
   10e10:	add	x25, x22, x0
   10e14:	mov	x20, x18
   10e18:	neg	x8, x8
   10e1c:	subs	x2, x22, #0x1
   10e20:	sub	x19, x25, x3
   10e24:	str	x8, [x1]
   10e28:	b.eq	10e54 <__gmpf_sub@@Base+0x5f4>  // b.none
   10e2c:	add	x0, x1, #0x8
   10e30:	add	x1, x26, #0x8
   10e34:	mov	x26, x3
   10e38:	mov	x27, x4
   10e3c:	mov	x28, x5
   10e40:	bl	c290 <__gmpn_com@plt>
   10e44:	ldur	x1, [x29, #-72]
   10e48:	mov	x5, x28
   10e4c:	mov	x4, x27
   10e50:	mov	x3, x26
   10e54:	cmp	x22, x19
   10e58:	b.ge	10e90 <__gmpf_sub@@Base+0x630>  // b.tcont
   10e5c:	add	x8, x3, x5
   10e60:	sub	x8, x4, x8
   10e64:	add	x0, x1, x22, lsl #3
   10e68:	lsl	x2, x8, #3
   10e6c:	mov	w1, #0xff                  	// #255
   10e70:	mov	x26, x3
   10e74:	mov	x27, x4
   10e78:	mov	x28, x5
   10e7c:	bl	c5f0 <memset@plt>
   10e80:	ldur	x1, [x29, #-72]
   10e84:	mov	x5, x28
   10e88:	mov	x4, x27
   10e8c:	mov	x3, x26
   10e90:	ldr	x8, [x24]
   10e94:	add	x10, x1, x19, lsl #3
   10e98:	sub	x9, x8, #0x1
   10e9c:	str	x9, [x10]
   10ea0:	cbz	x8, 11004 <__gmpf_sub@@Base+0x7a4>
   10ea4:	cmp	x3, #0x2
   10ea8:	mov	x15, x20
   10eac:	b.lt	1138c <__gmpf_sub@@Base+0xb2c>  // b.tstop
   10eb0:	ldur	x20, [x29, #-40]
   10eb4:	cmp	x10, x24
   10eb8:	b.eq	114c4 <__gmpf_sub@@Base+0xc64>  // b.none
   10ebc:	sub	x9, x3, #0x1
   10ec0:	cmp	x9, #0x4
   10ec4:	b.cs	11398 <__gmpf_sub@@Base+0xb38>  // b.hs, b.nlast
   10ec8:	mov	w8, #0x1                   	// #1
   10ecc:	b	114ac <__gmpf_sub@@Base+0xc4c>
   10ed0:	mov	x20, x25
   10ed4:	mov	x24, xzr
   10ed8:	b	10ef8 <__gmpf_sub@@Base+0x698>
   10edc:	add	x10, x10, x13
   10ee0:	add	x11, x11, x13
   10ee4:	add	x12, x10, x9
   10ee8:	add	x9, x11, x9
   10eec:	mov	x20, x25
   10ef0:	mov	x24, xzr
   10ef4:	sub	x28, x9, #0x1
   10ef8:	sub	x9, x21, #0x1
   10efc:	cmp	x12, x21
   10f00:	sub	x11, x12, x9
   10f04:	mov	w10, #0x7f00                	// #32512
   10f08:	csel	x22, x12, x9, lt  // lt = tstop
   10f0c:	add	x9, x8, x11, lsl #3
   10f10:	csel	x25, x8, x9, lt  // lt = tstop
   10f14:	cmp	x1, x10
   10f18:	b.hi	115c8 <__gmpf_sub@@Base+0xd68>  // b.pmore
   10f1c:	add	x9, x1, #0xf
   10f20:	mov	x8, sp
   10f24:	and	x9, x9, #0xfffffffffffffff0
   10f28:	sub	x1, x8, x9
   10f2c:	mov	sp, x1
   10f30:	cbz	x22, 115d8 <__gmpf_sub@@Base+0xd78>
   10f34:	cbz	x24, 10f5c <__gmpf_sub@@Base+0x6fc>
   10f38:	subs	x26, x24, x22
   10f3c:	b.ge	10f74 <__gmpf_sub@@Base+0x714>  // b.tcont
   10f40:	ldr	x8, [x25]
   10f44:	sub	x19, x22, x24
   10f48:	cbz	x8, 1128c <__gmpf_sub@@Base+0xa2c>
   10f4c:	mov	x9, x1
   10f50:	mov	x10, x25
   10f54:	mov	x11, x19
   10f58:	b	112c8 <__gmpf_sub@@Base+0xa68>
   10f5c:	ldr	x8, [x25]
   10f60:	cbz	x8, 110e8 <__gmpf_sub@@Base+0x888>
   10f64:	mov	x9, x1
   10f68:	mov	x10, x22
   10f6c:	mov	w24, w27
   10f70:	b	11128 <__gmpf_sub@@Base+0x8c8>
   10f74:	mov	x0, x1
   10f78:	mov	x21, x1
   10f7c:	mov	x1, x23
   10f80:	mov	x2, x26
   10f84:	bl	ca50 <__gmpn_copyi@plt>
   10f88:	lsl	x8, x26, #3
   10f8c:	add	x0, x21, x8
   10f90:	add	x1, x23, x8
   10f94:	mov	x2, x25
   10f98:	mov	x3, x22
   10f9c:	mov	x19, x21
   10fa0:	bl	c2d0 <__gmpn_sub_n@plt>
   10fa4:	mov	x22, x24
   10fa8:	b	1130c <__gmpf_sub@@Base+0xaac>
   10fac:	mov	x20, x18
   10fb0:	subs	x25, x3, x22
   10fb4:	b.ge	11328 <__gmpf_sub@@Base+0xac8>  // b.tcont
   10fb8:	sub	x19, x22, x3
   10fbc:	neg	x8, x8
   10fc0:	subs	x2, x19, #0x1
   10fc4:	str	x8, [x1]
   10fc8:	b.eq	10fe4 <__gmpf_sub@@Base+0x784>  // b.none
   10fcc:	add	x0, x1, #0x8
   10fd0:	add	x1, x26, #0x8
   10fd4:	mov	x21, x3
   10fd8:	bl	c290 <__gmpn_com@plt>
   10fdc:	ldur	x1, [x29, #-72]
   10fe0:	mov	x3, x21
   10fe4:	lsl	x8, x19, #3
   10fe8:	add	x0, x1, x8
   10fec:	add	x2, x26, x8
   10ff0:	mov	w4, #0x1                   	// #1
   10ff4:	mov	x1, x24
   10ff8:	bl	c760 <__gmpn_sub_nc@plt>
   10ffc:	ldur	x1, [x29, #-72]
   11000:	b	11360 <__gmpf_sub@@Base+0xb00>
   11004:	mov	x15, x20
   11008:	ldur	x20, [x29, #-40]
   1100c:	mov	x9, xzr
   11010:	mov	w8, #0x1                   	// #1
   11014:	cmp	x8, x3
   11018:	b.ge	114c4 <__gmpf_sub@@Base+0xc64>  // b.tcont
   1101c:	add	x11, x24, x9
   11020:	ldr	x11, [x11, #8]
   11024:	add	x12, x21, x9
   11028:	add	x8, x8, #0x1
   1102c:	add	x9, x9, #0x8
   11030:	sub	x13, x11, #0x1
   11034:	sub	x23, x23, #0x1
   11038:	str	x13, [x12, #8]
   1103c:	cbz	x11, 11014 <__gmpf_sub@@Base+0x7b4>
   11040:	cmp	x10, x24
   11044:	b.eq	114c4 <__gmpf_sub@@Base+0xc64>  // b.none
   11048:	subs	x10, x3, x8
   1104c:	b.le	114c4 <__gmpf_sub@@Base+0xc64>
   11050:	cmp	x10, #0x4
   11054:	b.cc	110cc <__gmpf_sub@@Base+0x86c>  // b.lo, b.ul, b.last
   11058:	add	x11, x21, x9
   1105c:	add	x11, x11, #0x8
   11060:	add	x12, x24, x3, lsl #3
   11064:	cmp	x11, x12
   11068:	b.cs	11088 <__gmpf_sub@@Base+0x828>  // b.hs, b.nlast
   1106c:	add	x11, x22, x4
   11070:	add	x12, x24, x9
   11074:	sub	x11, x11, x5
   11078:	add	x11, x1, x11, lsl #3
   1107c:	add	x12, x12, #0x8
   11080:	cmp	x11, x12
   11084:	b.hi	110cc <__gmpf_sub@@Base+0x86c>  // b.pmore
   11088:	sub	x13, x3, x8
   1108c:	add	x11, x21, x9
   11090:	add	x12, x24, x9
   11094:	and	x14, x23, #0xfffffffffffffffc
   11098:	and	x9, x10, #0xfffffffffffffffc
   1109c:	add	x11, x11, #0x18
   110a0:	add	x12, x12, #0x18
   110a4:	add	x8, x14, x8
   110a8:	and	x13, x13, #0xfffffffffffffffc
   110ac:	ldp	q0, q1, [x12, #-16]
   110b0:	add	x12, x12, #0x20
   110b4:	subs	x13, x13, #0x4
   110b8:	stp	q0, q1, [x11, #-16]
   110bc:	add	x11, x11, #0x20
   110c0:	b.ne	110ac <__gmpf_sub@@Base+0x84c>  // b.any
   110c4:	cmp	x10, x9
   110c8:	b.eq	114c4 <__gmpf_sub@@Base+0xc64>  // b.none
   110cc:	lsl	x9, x8, #3
   110d0:	ldr	x10, [x24, x9]
   110d4:	add	x8, x8, #0x1
   110d8:	cmp	x3, x8
   110dc:	str	x10, [x21, x9]
   110e0:	b.ne	110cc <__gmpf_sub@@Base+0x86c>  // b.any
   110e4:	b	114c4 <__gmpf_sub@@Base+0xc64>
   110e8:	ldur	x15, [x29, #-64]
   110ec:	mov	x9, xzr
   110f0:	mov	x10, xzr
   110f4:	sub	x11, x22, #0x1
   110f8:	mov	w24, w27
   110fc:	cmp	x11, x10
   11100:	str	xzr, [x1, x10, lsl #3]
   11104:	b.eq	11370 <__gmpf_sub@@Base+0xb10>  // b.none
   11108:	add	x8, x25, x10, lsl #3
   1110c:	ldr	x8, [x8, #8]
   11110:	add	x10, x10, #0x1
   11114:	sub	x9, x9, #0x8
   11118:	cbz	x8, 110fc <__gmpf_sub@@Base+0x89c>
   1111c:	sub	x10, x22, x10
   11120:	sub	x25, x25, x9
   11124:	sub	x9, x1, x9
   11128:	neg	x8, x8
   1112c:	subs	x2, x10, #0x1
   11130:	str	x8, [x9]
   11134:	b.eq	1114c <__gmpf_sub@@Base+0x8ec>  // b.none
   11138:	add	x0, x9, #0x8
   1113c:	mov	x19, x1
   11140:	add	x1, x25, #0x8
   11144:	bl	c290 <__gmpn_com@plt>
   11148:	mov	x1, x19
   1114c:	ldur	x15, [x29, #-64]
   11150:	mov	x25, x20
   11154:	mov	x20, x28
   11158:	b	114ec <__gmpf_sub@@Base+0xc8c>
   1115c:	sub	x23, x9, x22
   11160:	mov	x0, x1
   11164:	mov	x1, x24
   11168:	mov	x2, x23
   1116c:	mov	x21, x3
   11170:	stur	x4, [x29, #-96]
   11174:	mov	x28, x5
   11178:	bl	ca50 <__gmpn_copyi@plt>
   1117c:	ldur	x9, [x29, #-72]
   11180:	lsl	x8, x23, #3
   11184:	add	x1, x24, x8
   11188:	mov	x2, x26
   1118c:	add	x0, x9, x8
   11190:	mov	x3, x22
   11194:	stur	x21, [x29, #-80]
   11198:	stur	x23, [x29, #-104]
   1119c:	sub	x21, x21, x23
   111a0:	bl	c2d0 <__gmpn_sub_n@plt>
   111a4:	ldur	x13, [x29, #-96]
   111a8:	ldp	x1, x15, [x29, #-72]
   111ac:	mov	x9, x22
   111b0:	cbz	x0, 111f0 <__gmpf_sub@@Base+0x990>
   111b4:	ldur	x9, [x29, #-32]
   111b8:	ldp	x10, x12, [x29, #-88]
   111bc:	mov	x8, xzr
   111c0:	add	x9, x28, x9
   111c4:	sub	x9, x9, x13
   111c8:	add	x9, x10, x9, lsl #3
   111cc:	add	x10, x22, x8
   111d0:	cmp	x10, x21
   111d4:	b.ge	11464 <__gmpf_sub@@Base+0xc04>  // b.tcont
   111d8:	ldr	x10, [x9, x8, lsl #3]
   111dc:	add	x8, x8, #0x1
   111e0:	sub	x11, x10, #0x1
   111e4:	str	x11, [x25], #8
   111e8:	cbz	x10, 111cc <__gmpf_sub@@Base+0x96c>
   111ec:	add	x9, x22, x8
   111f0:	cmp	x24, x1
   111f4:	b.eq	114d4 <__gmpf_sub@@Base+0xc74>  // b.none
   111f8:	ldur	x25, [x29, #-48]
   111fc:	ldur	x14, [x29, #-80]
   11200:	cmp	x9, x21
   11204:	b.ge	114e0 <__gmpf_sub@@Base+0xc80>  // b.tcont
   11208:	add	x8, x22, x13
   1120c:	sub	x8, x8, x9
   11210:	sub	x8, x8, x28
   11214:	cmp	x8, #0x4
   11218:	b.cc	11258 <__gmpf_sub@@Base+0x9f8>  // b.lo, b.ul, b.last
   1121c:	ldur	x10, [x29, #-104]
   11220:	add	x11, x9, x10
   11224:	lsl	x10, x14, #3
   11228:	add	x11, x1, x11, lsl #3
   1122c:	add	x12, x24, x10
   11230:	cmp	x11, x12
   11234:	b.cs	1154c <__gmpf_sub@@Base+0xcec>  // b.hs, b.nlast
   11238:	add	x11, x9, x14
   1123c:	add	x11, x11, x28
   11240:	sub	x11, x11, x22
   11244:	sub	x11, x11, x13
   11248:	add	x10, x1, x10
   1124c:	add	x11, x24, x11, lsl #3
   11250:	cmp	x10, x11
   11254:	b.ls	1154c <__gmpf_sub@@Base+0xcec>  // b.plast
   11258:	ldur	w24, [x29, #-52]
   1125c:	mov	x10, x9
   11260:	sub	x8, x19, x10
   11264:	lsl	x10, x10, #3
   11268:	add	x9, x20, x10
   1126c:	add	x10, x27, x10
   11270:	ldr	x11, [x10], #8
   11274:	subs	x8, x8, #0x1
   11278:	str	x11, [x9], #8
   1127c:	b.ne	11270 <__gmpf_sub@@Base+0xa10>  // b.any
   11280:	ldur	x20, [x29, #-40]
   11284:	mov	x22, x14
   11288:	b	114ec <__gmpf_sub@@Base+0xc8c>
   1128c:	mvn	x8, x24
   11290:	mov	x9, xzr
   11294:	mov	x10, xzr
   11298:	add	x11, x8, x22
   1129c:	cmp	x11, x10
   112a0:	str	xzr, [x1, x10, lsl #3]
   112a4:	b.eq	113d4 <__gmpf_sub@@Base+0xb74>  // b.none
   112a8:	add	x8, x25, x10, lsl #3
   112ac:	ldr	x8, [x8, #8]
   112b0:	add	x10, x10, #0x1
   112b4:	sub	x9, x9, #0x8
   112b8:	cbz	x8, 1129c <__gmpf_sub@@Base+0xa3c>
   112bc:	sub	x11, x19, x10
   112c0:	sub	x10, x25, x9
   112c4:	sub	x9, x1, x9
   112c8:	neg	x8, x8
   112cc:	subs	x2, x11, #0x1
   112d0:	str	x8, [x9]
   112d4:	b.eq	112ec <__gmpf_sub@@Base+0xa8c>  // b.none
   112d8:	add	x0, x9, #0x8
   112dc:	mov	x21, x1
   112e0:	add	x1, x10, #0x8
   112e4:	bl	c290 <__gmpn_com@plt>
   112e8:	mov	x1, x21
   112ec:	mov	w4, #0x1                   	// #1
   112f0:	lsl	x8, x19, #3
   112f4:	mov	x19, x1
   112f8:	add	x0, x1, x8
   112fc:	add	x2, x25, x8
   11300:	mov	x1, x23
   11304:	mov	x3, x24
   11308:	bl	c760 <__gmpn_sub_nc@plt>
   1130c:	ldur	x15, [x29, #-64]
   11310:	mov	x25, x20
   11314:	mov	w24, w27
   11318:	mov	x1, x19
   1131c:	cbz	x0, 11374 <__gmpf_sub@@Base+0xb14>
   11320:	mov	x20, x28
   11324:	b	114ec <__gmpf_sub@@Base+0xc8c>
   11328:	mov	x0, x1
   1132c:	mov	x1, x24
   11330:	mov	x2, x25
   11334:	mov	x19, x3
   11338:	bl	ca50 <__gmpn_copyi@plt>
   1133c:	ldur	x9, [x29, #-72]
   11340:	lsl	x8, x25, #3
   11344:	add	x1, x24, x8
   11348:	mov	x2, x26
   1134c:	add	x0, x9, x8
   11350:	mov	x3, x22
   11354:	bl	c2d0 <__gmpn_sub_n@plt>
   11358:	ldur	x1, [x29, #-72]
   1135c:	mov	x22, x19
   11360:	ldur	w24, [x29, #-52]
   11364:	mov	x15, x20
   11368:	ldp	x25, x20, [x29, #-48]
   1136c:	b	114ec <__gmpf_sub@@Base+0xc8c>
   11370:	mov	x25, x20
   11374:	mov	w8, #0x1                   	// #1
   11378:	mov	x20, x28
   1137c:	str	x8, [x1, x22, lsl #3]
   11380:	add	x22, x22, #0x1
   11384:	add	x20, x28, #0x1
   11388:	b	11610 <__gmpf_sub@@Base+0xdb0>
   1138c:	mov	x22, x25
   11390:	ldur	x25, [x29, #-48]
   11394:	b	114e4 <__gmpf_sub@@Base+0xc84>
   11398:	add	x8, x22, x4
   1139c:	sub	x10, x8, x3
   113a0:	sub	x10, x10, x5
   113a4:	add	x10, x1, x10, lsl #3
   113a8:	add	x10, x10, #0x8
   113ac:	add	x11, x24, x3, lsl #3
   113b0:	cmp	x10, x11
   113b4:	b.cs	11470 <__gmpf_sub@@Base+0xc10>  // b.hs, b.nlast
   113b8:	sub	x8, x8, x5
   113bc:	add	x8, x1, x8, lsl #3
   113c0:	add	x10, x24, #0x8
   113c4:	cmp	x8, x10
   113c8:	b.ls	11470 <__gmpf_sub@@Base+0xc10>  // b.plast
   113cc:	mov	w8, #0x1                   	// #1
   113d0:	b	114ac <__gmpf_sub@@Base+0xc4c>
   113d4:	mov	x4, xzr
   113d8:	b	112f0 <__gmpf_sub@@Base+0xa90>
   113dc:	ldur	x15, [x29, #-64]
   113e0:	mov	x23, xzr
   113e4:	cmp	x19, x24
   113e8:	b.eq	1144c <__gmpf_sub@@Base+0xbec>  // b.none
   113ec:	subs	x9, x3, x23
   113f0:	b.le	1144c <__gmpf_sub@@Base+0xbec>
   113f4:	cmp	x9, #0x4
   113f8:	b.cc	11430 <__gmpf_sub@@Base+0xbd0>  // b.lo, b.ul, b.last
   113fc:	add	x8, x22, x4
   11400:	sub	x10, x8, x3
   11404:	sub	x10, x10, x5
   11408:	add	x10, x23, x10
   1140c:	add	x10, x1, x10, lsl #3
   11410:	add	x11, x24, x3, lsl #3
   11414:	cmp	x10, x11
   11418:	b.cs	11514 <__gmpf_sub@@Base+0xcb4>  // b.hs, b.nlast
   1141c:	sub	x8, x8, x5
   11420:	add	x8, x1, x8, lsl #3
   11424:	add	x10, x24, x23, lsl #3
   11428:	cmp	x8, x10
   1142c:	b.ls	11514 <__gmpf_sub@@Base+0xcb4>  // b.plast
   11430:	mov	x8, x23
   11434:	add	x9, x21, x8, lsl #3
   11438:	ldr	x10, [x24, x8, lsl #3]
   1143c:	add	x8, x8, #0x1
   11440:	cmp	x3, x8
   11444:	str	x10, [x9], #8
   11448:	b.ne	11438 <__gmpf_sub@@Base+0xbd8>  // b.any
   1144c:	ldr	x8, [x19]
   11450:	sub	x9, x8, #0x1
   11454:	str	x9, [x19], #8
   11458:	cbz	x8, 1144c <__gmpf_sub@@Base+0xbec>
   1145c:	ldur	x22, [x29, #-96]
   11460:	b	114cc <__gmpf_sub@@Base+0xc6c>
   11464:	ldur	x25, [x29, #-48]
   11468:	mov	x22, x12
   1146c:	b	114e4 <__gmpf_sub@@Base+0xc84>
   11470:	and	x10, x9, #0xfffffffffffffffc
   11474:	mov	x11, xzr
   11478:	orr	x8, x10, #0x1
   1147c:	and	x12, x23, #0xfffffffffffffffc
   11480:	add	x13, x24, x11
   11484:	ldur	q0, [x13, #8]
   11488:	ldur	q1, [x13, #24]
   1148c:	add	x13, x21, x11
   11490:	subs	x12, x12, #0x4
   11494:	add	x11, x11, #0x20
   11498:	stur	q0, [x13, #8]
   1149c:	stur	q1, [x13, #24]
   114a0:	b.ne	11480 <__gmpf_sub@@Base+0xc20>  // b.any
   114a4:	cmp	x9, x10
   114a8:	b.eq	114c4 <__gmpf_sub@@Base+0xc64>  // b.none
   114ac:	lsl	x9, x8, #3
   114b0:	ldr	x10, [x24, x9]
   114b4:	add	x8, x8, #0x1
   114b8:	cmp	x3, x8
   114bc:	str	x10, [x21, x9]
   114c0:	b.ne	114ac <__gmpf_sub@@Base+0xc4c>  // b.any
   114c4:	mov	x22, x25
   114c8:	ldur	x25, [x29, #-48]
   114cc:	ldur	w24, [x29, #-52]
   114d0:	b	114ec <__gmpf_sub@@Base+0xc8c>
   114d4:	ldur	x22, [x29, #-80]
   114d8:	ldur	x25, [x29, #-48]
   114dc:	b	114e4 <__gmpf_sub@@Base+0xc84>
   114e0:	mov	x22, x14
   114e4:	ldur	w24, [x29, #-52]
   114e8:	ldur	x20, [x29, #-40]
   114ec:	cbz	x22, 11610 <__gmpf_sub@@Base+0xdb0>
   114f0:	sub	x8, x20, x22
   114f4:	add	x9, x1, x22, lsl #3
   114f8:	ldur	x9, [x9, #-8]
   114fc:	cbnz	x9, 11610 <__gmpf_sub@@Base+0xdb0>
   11500:	sub	x22, x22, #0x1
   11504:	sub	x20, x20, #0x1
   11508:	cbnz	x22, 114f4 <__gmpf_sub@@Base+0xc94>
   1150c:	mov	x20, x8
   11510:	b	11610 <__gmpf_sub@@Base+0xdb0>
   11514:	and	x10, x9, #0xfffffffffffffffc
   11518:	add	x8, x23, x10
   1151c:	lsl	x11, x23, #3
   11520:	mov	x12, x10
   11524:	add	x13, x24, x11
   11528:	ldp	q0, q1, [x13]
   1152c:	add	x13, x21, x11
   11530:	subs	x12, x12, #0x4
   11534:	add	x11, x11, #0x20
   11538:	stp	q0, q1, [x13]
   1153c:	b.ne	11524 <__gmpf_sub@@Base+0xcc4>  // b.any
   11540:	cmp	x9, x10
   11544:	b.eq	1144c <__gmpf_sub@@Base+0xbec>  // b.none
   11548:	b	11434 <__gmpf_sub@@Base+0xbd4>
   1154c:	and	x11, x8, #0xfffffffffffffffc
   11550:	sub	x12, x19, x9
   11554:	add	x10, x9, x11
   11558:	and	x12, x12, #0xfffffffffffffffc
   1155c:	lsl	x9, x9, #3
   11560:	add	x13, x27, x9
   11564:	ldp	q0, q1, [x13]
   11568:	add	x13, x20, x9
   1156c:	subs	x12, x12, #0x4
   11570:	add	x9, x9, #0x20
   11574:	stp	q0, q1, [x13]
   11578:	b.ne	11560 <__gmpf_sub@@Base+0xd00>  // b.any
   1157c:	ldur	w24, [x29, #-52]
   11580:	cmp	x8, x11
   11584:	b.ne	11260 <__gmpf_sub@@Base+0xa00>  // b.any
   11588:	b	11280 <__gmpf_sub@@Base+0xa20>
   1158c:	stur	x0, [x29, #-48]
   11590:	sub	x0, x29, #0x18
   11594:	mov	x20, x18
   11598:	mov	x23, x4
   1159c:	mov	x27, x5
   115a0:	mov	x28, x14
   115a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   115a8:	mov	x1, x0
   115ac:	ldur	x0, [x29, #-48]
   115b0:	mov	x14, x28
   115b4:	mov	x5, x27
   115b8:	mov	x4, x23
   115bc:	mov	x18, x20
   115c0:	cbnz	x22, 10984 <__gmpf_sub@@Base+0x124>
   115c4:	b	109f4 <__gmpf_sub@@Base+0x194>
   115c8:	sub	x0, x29, #0x18
   115cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   115d0:	mov	x1, x0
   115d4:	cbnz	x22, 10f34 <__gmpf_sub@@Base+0x6d4>
   115d8:	mov	x0, x1
   115dc:	mov	x19, x1
   115e0:	mov	x1, x23
   115e4:	mov	x2, x24
   115e8:	bl	ca50 <__gmpn_copyi@plt>
   115ec:	ldur	x15, [x29, #-64]
   115f0:	mov	w8, #0x1                   	// #1
   115f4:	add	x28, x28, #0x1
   115f8:	mov	x1, x19
   115fc:	add	x22, x24, #0x1
   11600:	str	x8, [x19, x24, lsl #3]
   11604:	mov	x25, x20
   11608:	mov	x20, x28
   1160c:	mov	w24, w27
   11610:	mov	x0, x15
   11614:	mov	x2, x22
   11618:	bl	ca50 <__gmpn_copyi@plt>
   1161c:	mov	x27, x22
   11620:	ldur	x0, [x29, #-24]
   11624:	cbz	x0, 10bec <__gmpf_sub@@Base+0x38c>
   11628:	b	10a14 <__gmpf_sub@@Base+0x1b4>

000000000001162c <__gmpf_sub_ui@@Base>:
   1162c:	sub	sp, sp, #0x30
   11630:	stp	x29, x30, [sp, #32]
   11634:	add	x29, sp, #0x20
   11638:	cbz	x2, 1165c <__gmpf_sub_ui@@Base+0x30>
   1163c:	str	x2, [sp]
   11640:	mov	w8, #0x1                   	// #1
   11644:	mov	x9, sp
   11648:	add	x2, sp, #0x8
   1164c:	str	w8, [sp, #12]
   11650:	stp	x8, x9, [sp, #16]
   11654:	bl	ce00 <__gmpf_sub@plt>
   11658:	b	11660 <__gmpf_sub_ui@@Base+0x34>
   1165c:	bl	c150 <__gmpf_set@plt>
   11660:	ldp	x29, x30, [sp, #32]
   11664:	add	sp, sp, #0x30
   11668:	ret

000000000001166c <__gmpf_ui_sub@@Base>:
   1166c:	sub	sp, sp, #0x30
   11670:	stp	x29, x30, [sp, #32]
   11674:	add	x29, sp, #0x20
   11678:	cbz	x1, 1169c <__gmpf_ui_sub@@Base+0x30>
   1167c:	str	x1, [sp]
   11680:	mov	w8, #0x1                   	// #1
   11684:	mov	x9, sp
   11688:	add	x1, sp, #0x8
   1168c:	str	w8, [sp, #12]
   11690:	stp	x8, x9, [sp, #16]
   11694:	bl	ce00 <__gmpf_sub@plt>
   11698:	b	116a4 <__gmpf_ui_sub@@Base+0x38>
   1169c:	mov	x1, x2
   116a0:	bl	cbf0 <__gmpf_neg@plt>
   116a4:	ldp	x29, x30, [sp, #32]
   116a8:	add	sp, sp, #0x30
   116ac:	ret

00000000000116b0 <__gmpf_mul@@Base>:
   116b0:	stp	x29, x30, [sp, #-96]!
   116b4:	stp	x28, x27, [sp, #16]
   116b8:	stp	x26, x25, [sp, #32]
   116bc:	stp	x24, x23, [sp, #48]
   116c0:	stp	x22, x21, [sp, #64]
   116c4:	stp	x20, x19, [sp, #80]
   116c8:	mov	x29, sp
   116cc:	sub	sp, sp, #0x10
   116d0:	ldrsw	x27, [x0]
   116d4:	ldrsw	x8, [x1, #4]
   116d8:	mov	x20, x1
   116dc:	mov	x19, x0
   116e0:	mov	x21, x2
   116e4:	cmp	x1, x2
   116e8:	b.eq	11784 <__gmpf_mul@@Base+0xd4>  // b.none
   116ec:	ldrsw	x9, [x21, #4]
   116f0:	ldr	x10, [x20, #16]
   116f4:	cmp	x8, #0x0
   116f8:	ldr	x11, [x21, #16]
   116fc:	cneg	x12, x8, mi  // mi = first
   11700:	cmp	x9, #0x0
   11704:	cneg	x13, x9, mi  // mi = first
   11708:	subs	x14, x12, x27
   1170c:	add	x14, x10, x14, lsl #3
   11710:	csel	x23, x27, x12, gt
   11714:	csel	x24, x14, x10, gt
   11718:	subs	x10, x13, x27
   1171c:	add	x10, x11, x10, lsl #3
   11720:	csel	x26, x10, x11, gt
   11724:	csel	x25, x27, x13, gt
   11728:	cbz	x23, 117f0 <__gmpf_mul@@Base+0x140>
   1172c:	cbz	x25, 117f0 <__gmpf_mul@@Base+0x140>
   11730:	eor	w8, w9, w8
   11734:	add	x28, x25, x23
   11738:	sxtw	x8, w8
   1173c:	stp	x8, xzr, [x29, #-16]
   11740:	lsl	x1, x28, #3
   11744:	mov	w8, #0x7f00                	// #32512
   11748:	cmp	x1, x8
   1174c:	b.hi	11894 <__gmpf_mul@@Base+0x1e4>  // b.pmore
   11750:	add	x9, x1, #0xf
   11754:	mov	x8, sp
   11758:	and	x9, x9, #0xfffffffffffffff0
   1175c:	sub	x22, x8, x9
   11760:	mov	sp, x22
   11764:	mov	x0, x22
   11768:	cmp	x23, x25
   1176c:	b.ge	117fc <__gmpf_mul@@Base+0x14c>  // b.tcont
   11770:	mov	x1, x26
   11774:	mov	x2, x25
   11778:	mov	x3, x24
   1177c:	mov	x4, x23
   11780:	b	1180c <__gmpf_mul@@Base+0x15c>
   11784:	ldr	x9, [x20, #16]
   11788:	cmp	x8, #0x0
   1178c:	cneg	x8, x8, mi  // mi = first
   11790:	subs	x10, x8, x27
   11794:	add	x10, x9, x10, lsl #3
   11798:	csel	x23, x27, x8, gt
   1179c:	csel	x24, x10, x9, gt
   117a0:	cbz	x23, 117f0 <__gmpf_mul@@Base+0x140>
   117a4:	lsl	x1, x23, #4
   117a8:	mov	w8, #0x7f00                	// #32512
   117ac:	cmp	x1, x8
   117b0:	lsl	x28, x23, #1
   117b4:	stur	xzr, [x29, #-8]
   117b8:	b.hi	118a4 <__gmpf_mul@@Base+0x1f4>  // b.pmore
   117bc:	add	x9, x1, #0xf
   117c0:	mov	x8, sp
   117c4:	and	x9, x9, #0xfffffffffffffff0
   117c8:	sub	x22, x8, x9
   117cc:	mov	sp, x22
   117d0:	mov	x0, x22
   117d4:	mov	x1, x24
   117d8:	mov	x2, x23
   117dc:	bl	c8e0 <__gmpn_sqr@plt>
   117e0:	add	x8, x22, x28, lsl #3
   117e4:	ldur	x0, [x8, #-8]
   117e8:	mov	x25, xzr
   117ec:	b	11814 <__gmpf_mul@@Base+0x164>
   117f0:	str	wzr, [x19, #4]
   117f4:	str	xzr, [x19, #8]
   117f8:	b	1186c <__gmpf_mul@@Base+0x1bc>
   117fc:	mov	x1, x24
   11800:	mov	x2, x23
   11804:	mov	x3, x26
   11808:	mov	x4, x25
   1180c:	bl	ccd0 <__gmpn_mul@plt>
   11810:	ldur	x25, [x29, #-16]
   11814:	cmp	x0, #0x0
   11818:	cset	w24, eq  // eq = none
   1181c:	add	x8, x27, #0x1
   11820:	ldr	x0, [x19, #16]
   11824:	sub	x9, x28, x24
   11828:	subs	x8, x9, x8
   1182c:	add	x8, x22, x8, lsl #3
   11830:	csinc	x23, x9, x27, le
   11834:	csel	x1, x8, x22, gt
   11838:	mov	x2, x23
   1183c:	bl	ca50 <__gmpn_copyi@plt>
   11840:	ldr	x8, [x20, #8]
   11844:	ldr	x9, [x21, #8]
   11848:	neg	w10, w23
   1184c:	cmp	x25, #0x0
   11850:	sub	x8, x8, x24
   11854:	csel	x10, x23, x10, ge  // ge = tcont
   11858:	add	x8, x8, x9
   1185c:	str	x8, [x19, #8]
   11860:	str	w10, [x19, #4]
   11864:	ldur	x0, [x29, #-8]
   11868:	cbnz	x0, 1188c <__gmpf_mul@@Base+0x1dc>
   1186c:	mov	sp, x29
   11870:	ldp	x20, x19, [sp, #80]
   11874:	ldp	x22, x21, [sp, #64]
   11878:	ldp	x24, x23, [sp, #48]
   1187c:	ldp	x26, x25, [sp, #32]
   11880:	ldp	x28, x27, [sp, #16]
   11884:	ldp	x29, x30, [sp], #96
   11888:	ret
   1188c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11890:	b	1186c <__gmpf_mul@@Base+0x1bc>
   11894:	sub	x0, x29, #0x8
   11898:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1189c:	mov	x22, x0
   118a0:	b	11764 <__gmpf_mul@@Base+0xb4>
   118a4:	sub	x0, x29, #0x8
   118a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   118ac:	mov	x22, x0
   118b0:	b	117d0 <__gmpf_mul@@Base+0x120>

00000000000118b4 <__gmpf_mul_ui@@Base>:
   118b4:	stp	x29, x30, [sp, #-64]!
   118b8:	stp	x20, x19, [sp, #48]
   118bc:	mov	x19, x0
   118c0:	str	x23, [sp, #16]
   118c4:	stp	x22, x21, [sp, #32]
   118c8:	mov	x29, sp
   118cc:	cbz	x2, 1199c <__gmpf_mul_ui@@Base+0xe8>
   118d0:	ldr	w8, [x1, #4]
   118d4:	mov	x20, x1
   118d8:	cbz	w8, 1199c <__gmpf_mul_ui@@Base+0xe8>
   118dc:	ldrsw	x21, [x19]
   118e0:	sxtw	x23, w8
   118e4:	cmp	w23, #0x0
   118e8:	ldr	x1, [x20, #16]
   118ec:	cneg	x9, x23, lt  // lt = tstop
   118f0:	sub	x8, x9, x21
   118f4:	mov	x3, x2
   118f8:	cmp	x8, #0x1
   118fc:	b.lt	11948 <__gmpf_mul_ui@@Base+0x94>  // b.tstop
   11900:	add	x9, x1, x8, lsl #3
   11904:	ldur	x9, [x9, #-8]
   11908:	sub	x10, x1, #0x10
   1190c:	mov	x11, x8
   11910:	umulh	x4, x9, x3
   11914:	sub	x12, x11, #0x1
   11918:	cmp	x12, #0x1
   1191c:	b.lt	11940 <__gmpf_mul_ui@@Base+0x8c>  // b.tstop
   11920:	mul	x13, x9, x3
   11924:	ldr	x9, [x10, x11, lsl #3]
   11928:	umulh	x11, x9, x3
   1192c:	adds	x11, x11, x13
   11930:	cinc	x4, x4, cs  // cs = hs, nlast
   11934:	cmn	x11, #0x1
   11938:	mov	x11, x12
   1193c:	b.eq	11914 <__gmpf_mul_ui@@Base+0x60>  // b.none
   11940:	add	x1, x1, x8, lsl #3
   11944:	b	11950 <__gmpf_mul_ui@@Base+0x9c>
   11948:	mov	x4, xzr
   1194c:	mov	x21, x9
   11950:	ldr	x22, [x19, #16]
   11954:	mov	x2, x21
   11958:	mov	x0, x22
   1195c:	bl	d240 <__gmpn_mul_1c@plt>
   11960:	str	x0, [x22, x21, lsl #3]
   11964:	ldr	x8, [x20, #8]
   11968:	cmp	x0, #0x0
   1196c:	cinc	x9, x21, ne  // ne = any
   11970:	neg	w10, w9
   11974:	cinc	x8, x8, ne  // ne = any
   11978:	cmp	w23, #0x0
   1197c:	str	x8, [x19, #8]
   11980:	csel	x8, x9, x10, ge  // ge = tcont
   11984:	str	w8, [x19, #4]
   11988:	ldp	x20, x19, [sp, #48]
   1198c:	ldp	x22, x21, [sp, #32]
   11990:	ldr	x23, [sp, #16]
   11994:	ldp	x29, x30, [sp], #64
   11998:	ret
   1199c:	str	wzr, [x19, #4]
   119a0:	str	xzr, [x19, #8]
   119a4:	b	11988 <__gmpf_mul_ui@@Base+0xd4>

00000000000119a8 <__gmpf_div@@Base>:
   119a8:	stp	x29, x30, [sp, #-96]!
   119ac:	stp	x28, x27, [sp, #16]
   119b0:	stp	x26, x25, [sp, #32]
   119b4:	stp	x24, x23, [sp, #48]
   119b8:	stp	x22, x21, [sp, #64]
   119bc:	stp	x20, x19, [sp, #80]
   119c0:	mov	x29, sp
   119c4:	sub	sp, sp, #0x30
   119c8:	ldrsw	x27, [x2, #4]
   119cc:	cbz	w27, 11bec <__gmpf_div@@Base+0x244>
   119d0:	ldrsw	x28, [x1, #4]
   119d4:	mov	x19, x0
   119d8:	cbz	w28, 11a58 <__gmpf_div@@Base+0xb0>
   119dc:	ldr	x8, [x2, #8]
   119e0:	cmp	x28, #0x0
   119e4:	ldrsw	x26, [x19]
   119e8:	cneg	x11, x28, mi  // mi = first
   119ec:	cmp	x27, #0x0
   119f0:	cneg	x21, x27, mi  // mi = first
   119f4:	ldr	x20, [x19, #16]
   119f8:	ldp	x23, x10, [x1, #8]
   119fc:	stp	x8, xzr, [x29, #-16]
   11a00:	sub	x8, x21, x11
   11a04:	ldr	x24, [x2, #16]
   11a08:	add	x8, x8, x26
   11a0c:	neg	x9, x8
   11a10:	and	x9, x9, x8, asr #63
   11a14:	cmp	x8, #0x0
   11a18:	add	x12, x10, x9, lsl #3
   11a1c:	sub	x22, x11, x9
   11a20:	b.gt	11a64 <__gmpf_div@@Base+0xbc>
   11a24:	cmp	x20, x10
   11a28:	b.eq	11a64 <__gmpf_div@@Base+0xbc>  // b.none
   11a2c:	lsl	x8, x22, #3
   11a30:	add	x1, x8, #0x8
   11a34:	mov	w8, #0x7f00                	// #32512
   11a38:	cmp	x1, x8
   11a3c:	b.hi	11ba0 <__gmpf_div@@Base+0x1f8>  // b.pmore
   11a40:	add	x9, x1, #0xf
   11a44:	mov	x8, sp
   11a48:	and	x9, x9, #0xfffffffffffffff0
   11a4c:	sub	x25, x8, x9
   11a50:	mov	sp, x25
   11a54:	b	11adc <__gmpf_div@@Base+0x134>
   11a58:	str	wzr, [x19, #4]
   11a5c:	str	xzr, [x19, #8]
   11a60:	b	11b78 <__gmpf_div@@Base+0x1d0>
   11a64:	add	x10, x21, x26
   11a68:	stp	x10, x23, [x29, #-32]
   11a6c:	lsl	x10, x10, #3
   11a70:	add	x1, x10, #0x8
   11a74:	mov	w10, #0x7f00                	// #32512
   11a78:	cmp	x1, x10
   11a7c:	add	x23, x9, x8
   11a80:	b.hi	11bc0 <__gmpf_div@@Base+0x218>  // b.pmore
   11a84:	add	x9, x1, #0xf
   11a88:	mov	x8, sp
   11a8c:	and	x9, x9, #0xfffffffffffffff0
   11a90:	sub	x25, x8, x9
   11a94:	mov	sp, x25
   11a98:	cbz	x23, 11ac4 <__gmpf_div@@Base+0x11c>
   11a9c:	lsl	x2, x23, #3
   11aa0:	mov	x0, x25
   11aa4:	mov	w1, wzr
   11aa8:	stur	x26, [x29, #-40]
   11aac:	mov	x26, x22
   11ab0:	mov	x22, x12
   11ab4:	bl	c5f0 <memset@plt>
   11ab8:	mov	x12, x22
   11abc:	mov	x22, x26
   11ac0:	ldur	x26, [x29, #-40]
   11ac4:	add	x0, x25, x23, lsl #3
   11ac8:	mov	x1, x12
   11acc:	mov	x2, x22
   11ad0:	bl	ca50 <__gmpn_copyi@plt>
   11ad4:	ldp	x22, x23, [x29, #-32]
   11ad8:	mov	x12, x25
   11adc:	eor	w27, w27, w28
   11ae0:	cmp	x20, x24
   11ae4:	add	x28, x26, #0x1
   11ae8:	b.ne	11b24 <__gmpf_div@@Base+0x17c>  // b.any
   11aec:	cmp	x21, #0xfe0
   11af0:	lsl	x1, x21, #3
   11af4:	stur	x12, [x29, #-24]
   11af8:	b.hi	11bdc <__gmpf_div@@Base+0x234>  // b.pmore
   11afc:	add	x9, x1, #0xf
   11b00:	mov	x8, sp
   11b04:	and	x9, x9, #0xfffffffffffffff0
   11b08:	sub	x24, x8, x9
   11b0c:	mov	sp, x24
   11b10:	mov	x0, x24
   11b14:	mov	x1, x20
   11b18:	mov	x2, x21
   11b1c:	bl	ca50 <__gmpn_copyi@plt>
   11b20:	ldur	x12, [x29, #-24]
   11b24:	mov	x0, x20
   11b28:	mov	x1, x12
   11b2c:	mov	x2, x22
   11b30:	mov	x3, x24
   11b34:	mov	x4, x21
   11b38:	mov	x5, x25
   11b3c:	bl	c320 <__gmpn_div_q@plt>
   11b40:	ldr	x8, [x20, x26, lsl #3]
   11b44:	ldp	x9, x0, [x29, #-16]
   11b48:	cmp	x8, #0x0
   11b4c:	cset	w8, eq  // eq = none
   11b50:	sub	x9, x23, x9
   11b54:	sub	x10, x28, x8
   11b58:	cmp	w27, #0x0
   11b5c:	sub	x8, x9, x8
   11b60:	neg	w9, w10
   11b64:	add	x8, x8, #0x1
   11b68:	csel	x9, x10, x9, ge  // ge = tcont
   11b6c:	str	w9, [x19, #4]
   11b70:	str	x8, [x19, #8]
   11b74:	cbnz	x0, 11b98 <__gmpf_div@@Base+0x1f0>
   11b78:	mov	sp, x29
   11b7c:	ldp	x20, x19, [sp, #80]
   11b80:	ldp	x22, x21, [sp, #64]
   11b84:	ldp	x24, x23, [sp, #48]
   11b88:	ldp	x26, x25, [sp, #32]
   11b8c:	ldp	x28, x27, [sp, #16]
   11b90:	ldp	x29, x30, [sp], #96
   11b94:	ret
   11b98:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11b9c:	b	11b78 <__gmpf_div@@Base+0x1d0>
   11ba0:	sub	x0, x29, #0x8
   11ba4:	mov	x25, x22
   11ba8:	mov	x22, x12
   11bac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11bb0:	mov	x12, x22
   11bb4:	mov	x22, x25
   11bb8:	mov	x25, x0
   11bbc:	b	11adc <__gmpf_div@@Base+0x134>
   11bc0:	sub	x0, x29, #0x8
   11bc4:	mov	x25, x12
   11bc8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11bcc:	mov	x12, x25
   11bd0:	mov	x25, x0
   11bd4:	cbnz	x23, 11a9c <__gmpf_div@@Base+0xf4>
   11bd8:	b	11ac4 <__gmpf_div@@Base+0x11c>
   11bdc:	sub	x0, x29, #0x8
   11be0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11be4:	mov	x24, x0
   11be8:	b	11b10 <__gmpf_div@@Base+0x168>
   11bec:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000011bf0 <__gmpf_div_ui@@Base>:
   11bf0:	stp	x29, x30, [sp, #-96]!
   11bf4:	stp	x28, x27, [sp, #16]
   11bf8:	stp	x26, x25, [sp, #32]
   11bfc:	stp	x24, x23, [sp, #48]
   11c00:	stp	x22, x21, [sp, #64]
   11c04:	stp	x20, x19, [sp, #80]
   11c08:	mov	x29, sp
   11c0c:	sub	sp, sp, #0x10
   11c10:	cbz	x2, 11d48 <__gmpf_div_ui@@Base+0x158>
   11c14:	ldrsw	x27, [x1, #4]
   11c18:	mov	x20, x1
   11c1c:	mov	x19, x0
   11c20:	cbz	w27, 11c84 <__gmpf_div_ui@@Base+0x94>
   11c24:	ldrsw	x28, [x19]
   11c28:	stur	xzr, [x29, #-8]
   11c2c:	ldr	x23, [x19, #16]
   11c30:	ldr	x24, [x20, #16]
   11c34:	lsl	x8, x28, #3
   11c38:	cmp	w27, #0x0
   11c3c:	add	x1, x8, #0x10
   11c40:	mov	w8, #0x7f00                	// #32512
   11c44:	mov	x21, x2
   11c48:	cneg	x25, x27, lt  // lt = tstop
   11c4c:	cmp	x1, x8
   11c50:	add	x22, x28, #0x1
   11c54:	b.hi	11c90 <__gmpf_div_ui@@Base+0xa0>  // b.pmore
   11c58:	add	x9, x1, #0xf
   11c5c:	mov	x8, sp
   11c60:	and	x9, x9, #0xfffffffffffffff0
   11c64:	sub	x26, x8, x9
   11c68:	mov	sp, x26
   11c6c:	subs	x8, x25, x22
   11c70:	b.le	11ca4 <__gmpf_div_ui@@Base+0xb4>
   11c74:	add	x24, x24, x8, lsl #3
   11c78:	mov	x25, x22
   11c7c:	mov	x0, x26
   11c80:	b	11cc8 <__gmpf_div_ui@@Base+0xd8>
   11c84:	str	wzr, [x19, #4]
   11c88:	str	xzr, [x19, #8]
   11c8c:	b	11d20 <__gmpf_div_ui@@Base+0x130>
   11c90:	sub	x0, x29, #0x8
   11c94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   11c98:	mov	x26, x0
   11c9c:	subs	x8, x25, x22
   11ca0:	b.gt	11c74 <__gmpf_div_ui@@Base+0x84>
   11ca4:	stur	x23, [x29, #-16]
   11ca8:	subs	x23, x22, x25
   11cac:	b.eq	11cc0 <__gmpf_div_ui@@Base+0xd0>  // b.none
   11cb0:	lsl	x2, x23, #3
   11cb4:	mov	x0, x26
   11cb8:	mov	w1, wzr
   11cbc:	bl	c5f0 <memset@plt>
   11cc0:	add	x0, x26, x23, lsl #3
   11cc4:	ldur	x23, [x29, #-16]
   11cc8:	mov	x1, x24
   11ccc:	mov	x2, x25
   11cd0:	bl	ca50 <__gmpn_copyi@plt>
   11cd4:	mov	x0, x23
   11cd8:	mov	x1, xzr
   11cdc:	mov	x2, x26
   11ce0:	mov	x3, x22
   11ce4:	mov	x4, x21
   11ce8:	bl	cd00 <__gmpn_divrem_1@plt>
   11cec:	ldr	x8, [x23, x28, lsl #3]
   11cf0:	ldr	x9, [x20, #8]
   11cf4:	cmp	x8, #0x0
   11cf8:	cset	w8, eq  // eq = none
   11cfc:	sub	x10, x22, x8
   11d00:	cmp	w27, #0x0
   11d04:	sub	x8, x9, x8
   11d08:	neg	w9, w10
   11d0c:	csel	x9, x10, x9, ge  // ge = tcont
   11d10:	str	w9, [x19, #4]
   11d14:	str	x8, [x19, #8]
   11d18:	ldur	x0, [x29, #-8]
   11d1c:	cbnz	x0, 11d40 <__gmpf_div_ui@@Base+0x150>
   11d20:	mov	sp, x29
   11d24:	ldp	x20, x19, [sp, #80]
   11d28:	ldp	x22, x21, [sp, #64]
   11d2c:	ldp	x24, x23, [sp, #48]
   11d30:	ldp	x26, x25, [sp, #32]
   11d34:	ldp	x28, x27, [sp, #16]
   11d38:	ldp	x29, x30, [sp], #96
   11d3c:	ret
   11d40:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   11d44:	b	11d20 <__gmpf_div_ui@@Base+0x130>
   11d48:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000011d4c <__gmpf_cmp_z@@Base>:
   11d4c:	sub	sp, sp, #0x30
   11d50:	stp	x29, x30, [sp, #32]
   11d54:	ldrsw	x8, [x1, #4]
   11d58:	add	x29, sp, #0x20
   11d5c:	cmp	x8, #0x0
   11d60:	str	w8, [sp, #12]
   11d64:	cneg	x8, x8, mi  // mi = first
   11d68:	str	x8, [sp, #16]
   11d6c:	ldr	x8, [x1, #8]
   11d70:	add	x1, sp, #0x8
   11d74:	str	x8, [sp, #24]
   11d78:	bl	c670 <__gmpf_cmp@plt>
   11d7c:	ldp	x29, x30, [sp, #32]
   11d80:	add	sp, sp, #0x30
   11d84:	ret

0000000000011d88 <__gmpf_cmp@@Base>:
   11d88:	ldrsw	x12, [x0, #4]
   11d8c:	ldrsw	x9, [x1, #4]
   11d90:	mov	w10, #0x1                   	// #1
   11d94:	mov	x8, x0
   11d98:	cmp	w12, #0x0
   11d9c:	eor	w11, w9, w12
   11da0:	cneg	w0, w10, lt  // lt = tstop
   11da4:	tbnz	w11, #31, 11dc8 <__gmpf_cmp@@Base+0x40>
   11da8:	cbz	w12, 11dcc <__gmpf_cmp@@Base+0x44>
   11dac:	cbz	w9, 11dd8 <__gmpf_cmp@@Base+0x50>
   11db0:	ldr	x10, [x8, #8]
   11db4:	ldr	x11, [x1, #8]
   11db8:	cmp	x10, x11
   11dbc:	b.gt	11dc8 <__gmpf_cmp@@Base+0x40>
   11dc0:	b.ge	11de0 <__gmpf_cmp@@Base+0x58>  // b.tcont
   11dc4:	neg	w0, w0
   11dc8:	ret
   11dcc:	cmp	w9, #0x0
   11dd0:	csetm	w0, ne  // ne = any
   11dd4:	ret
   11dd8:	mov	w0, #0x1                   	// #1
   11ddc:	ret
   11de0:	ldr	x10, [x8, #16]
   11de4:	ldr	x11, [x1, #16]
   11de8:	cmp	w12, #0x0
   11dec:	cneg	x8, x12, lt  // lt = tstop
   11df0:	ldr	x13, [x10]
   11df4:	cmp	x9, #0x0
   11df8:	cneg	x9, x9, mi  // mi = first
   11dfc:	cbnz	x13, 11e0c <__gmpf_cmp@@Base+0x84>
   11e00:	ldr	x12, [x10, #8]!
   11e04:	sub	x8, x8, #0x1
   11e08:	cbz	x12, 11e00 <__gmpf_cmp@@Base+0x78>
   11e0c:	ldr	x12, [x11]
   11e10:	cbnz	x12, 11e20 <__gmpf_cmp@@Base+0x98>
   11e14:	ldr	x12, [x11, #8]!
   11e18:	sub	x9, x9, #0x1
   11e1c:	cbz	x12, 11e14 <__gmpf_cmp@@Base+0x8c>
   11e20:	cmp	x8, x9
   11e24:	b.le	11e54 <__gmpf_cmp@@Base+0xcc>
   11e28:	add	x8, x10, x8, lsl #3
   11e2c:	sub	x11, x11, #0x8
   11e30:	sub	x8, x8, #0x8
   11e34:	subs	x10, x9, #0x1
   11e38:	b.lt	11dc8 <__gmpf_cmp@@Base+0x40>  // b.tstop
   11e3c:	ldr	x12, [x8], #-8
   11e40:	ldr	x9, [x11, x9, lsl #3]
   11e44:	cmp	x12, x9
   11e48:	mov	x9, x10
   11e4c:	b.eq	11e34 <__gmpf_cmp@@Base+0xac>  // b.none
   11e50:	b	11eb0 <__gmpf_cmp@@Base+0x128>
   11e54:	cmp	x9, x8
   11e58:	b.le	11e88 <__gmpf_cmp@@Base+0x100>
   11e5c:	add	x9, x11, x9, lsl #3
   11e60:	sub	x9, x9, #0x8
   11e64:	sub	x10, x10, #0x8
   11e68:	subs	x11, x8, #0x1
   11e6c:	b.lt	11dc4 <__gmpf_cmp@@Base+0x3c>  // b.tstop
   11e70:	ldr	x8, [x10, x8, lsl #3]
   11e74:	ldr	x12, [x9], #-8
   11e78:	cmp	x8, x12
   11e7c:	mov	x8, x11
   11e80:	b.eq	11e68 <__gmpf_cmp@@Base+0xe0>  // b.none
   11e84:	b	11eb0 <__gmpf_cmp@@Base+0x128>
   11e88:	sub	x9, x11, #0x8
   11e8c:	sub	x10, x10, #0x8
   11e90:	subs	x11, x8, #0x1
   11e94:	b.lt	11eb8 <__gmpf_cmp@@Base+0x130>  // b.tstop
   11e98:	lsl	x8, x8, #3
   11e9c:	ldr	x12, [x10, x8]
   11ea0:	ldr	x8, [x9, x8]
   11ea4:	cmp	x12, x8
   11ea8:	mov	x8, x11
   11eac:	b.eq	11e90 <__gmpf_cmp@@Base+0x108>  // b.none
   11eb0:	b.ls	11dc4 <__gmpf_cmp@@Base+0x3c>  // b.plast
   11eb4:	b	11dc8 <__gmpf_cmp@@Base+0x40>
   11eb8:	mov	w0, wzr
   11ebc:	ret

0000000000011ec0 <__gmpf_cmp_d@@Base>:
   11ec0:	sub	sp, sp, #0x50
   11ec4:	fmov	x8, d0
   11ec8:	mvn	x9, x8
   11ecc:	tst	x9, #0x7ff0000000000000
   11ed0:	stp	x29, x30, [sp, #48]
   11ed4:	str	x19, [sp, #64]
   11ed8:	add	x29, sp, #0x30
   11edc:	b.eq	11f40 <__gmpf_cmp_d@@Base+0x80>  // b.none
   11ee0:	mov	x19, x0
   11ee4:	fcmp	d0, #0.0
   11ee8:	b.ne	11ef4 <__gmpf_cmp_d@@Base+0x34>  // b.any
   11eec:	ldr	w0, [x19, #4]
   11ef0:	b	11f30 <__gmpf_cmp_d@@Base+0x70>
   11ef4:	sub	x8, x29, #0x10
   11ef8:	mov	w9, #0xfffffffe            	// #-2
   11efc:	mov	w10, #0x2                   	// #2
   11f00:	fneg	d1, d0
   11f04:	str	x8, [sp, #24]
   11f08:	csel	w8, w10, w9, ge  // ge = tcont
   11f0c:	fcsel	d0, d0, d1, ge  // ge = tcont
   11f10:	sub	x0, x29, #0x10
   11f14:	str	w8, [sp, #12]
   11f18:	bl	d280 <__gmp_extract_double@plt>
   11f1c:	sxtw	x8, w0
   11f20:	add	x1, sp, #0x8
   11f24:	mov	x0, x19
   11f28:	str	x8, [sp, #16]
   11f2c:	bl	c670 <__gmpf_cmp@plt>
   11f30:	ldr	x19, [sp, #64]
   11f34:	ldp	x29, x30, [sp, #48]
   11f38:	add	sp, sp, #0x50
   11f3c:	ret
   11f40:	tst	x8, #0xfffffffffffff
   11f44:	b.ne	11f58 <__gmpf_cmp_d@@Base+0x98>  // b.any
   11f48:	fcmp	d0, #0.0
   11f4c:	mov	w8, #0xffffffff            	// #-1
   11f50:	csinc	w0, w8, wzr, pl  // pl = nfrst
   11f54:	b	11f30 <__gmpf_cmp_d@@Base+0x70>
   11f58:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000011f5c <__gmpf_cmp_ui@@Base>:
   11f5c:	ldrsw	x8, [x0, #4]
   11f60:	tbnz	w8, #31, 11fac <__gmpf_cmp_ui@@Base+0x50>
   11f64:	cbz	x1, 11fb4 <__gmpf_cmp_ui@@Base+0x58>
   11f68:	ldr	x9, [x0, #8]
   11f6c:	cmp	x9, #0x1
   11f70:	b.ne	11fc0 <__gmpf_cmp_ui@@Base+0x64>  // b.any
   11f74:	ldr	x9, [x0, #16]
   11f78:	sub	x8, x8, #0x1
   11f7c:	ldr	x10, [x9, x8, lsl #3]
   11f80:	cmp	x10, x1
   11f84:	b.ne	11fcc <__gmpf_cmp_ui@@Base+0x70>  // b.any
   11f88:	ldr	x10, [x9]
   11f8c:	cbnz	x10, 11fa0 <__gmpf_cmp_ui@@Base+0x44>
   11f90:	add	x9, x9, #0x8
   11f94:	ldr	x10, [x9], #8
   11f98:	sub	x8, x8, #0x1
   11f9c:	cbz	x10, 11f94 <__gmpf_cmp_ui@@Base+0x38>
   11fa0:	cmp	x8, #0x0
   11fa4:	cset	w0, gt
   11fa8:	ret
   11fac:	mov	w0, #0xffffffff            	// #-1
   11fb0:	ret
   11fb4:	cmp	w8, #0x0
   11fb8:	cset	w0, ne  // ne = any
   11fbc:	ret
   11fc0:	mov	w8, #0xffffffff            	// #-1
   11fc4:	cneg	w0, w8, ge  // ge = tcont
   11fc8:	ret
   11fcc:	mov	w8, #0xffffffff            	// #-1
   11fd0:	cneg	w0, w8, cs  // cs = hs, nlast
   11fd4:	ret

0000000000011fd8 <__gmpf_cmp_si@@Base>:
   11fd8:	ldrsw	x10, [x0, #4]
   11fdc:	lsr	x9, x1, #63
   11fe0:	cmp	w9, w10, lsr #31
   11fe4:	b.ne	12054 <__gmpf_cmp_si@@Base+0x7c>  // b.any
   11fe8:	cbz	w10, 12064 <__gmpf_cmp_si@@Base+0x8c>
   11fec:	mov	x8, x0
   11ff0:	mov	w0, #0x1                   	// #1
   11ff4:	cbz	x1, 12050 <__gmpf_cmp_si@@Base+0x78>
   11ff8:	ldr	x11, [x8, #8]
   11ffc:	cmp	w10, #0x0
   12000:	cneg	w9, w0, lt  // lt = tstop
   12004:	cmp	x1, #0x0
   12008:	cneg	x12, x1, mi  // mi = first
   1200c:	cmp	x11, #0x1
   12010:	b.ne	12070 <__gmpf_cmp_si@@Base+0x98>  // b.any
   12014:	ldr	x11, [x8, #16]
   12018:	cmp	w10, #0x0
   1201c:	cneg	x8, x10, lt  // lt = tstop
   12020:	sub	x8, x8, #0x1
   12024:	ldr	x10, [x11, x8, lsl #3]
   12028:	cmp	x10, x12
   1202c:	b.ne	12078 <__gmpf_cmp_si@@Base+0xa0>  // b.any
   12030:	ldr	x10, [x11]
   12034:	cbnz	x10, 12048 <__gmpf_cmp_si@@Base+0x70>
   12038:	add	x10, x11, #0x8
   1203c:	ldr	x11, [x10], #8
   12040:	sub	x8, x8, #0x1
   12044:	cbz	x11, 1203c <__gmpf_cmp_si@@Base+0x64>
   12048:	cmp	x8, #0x0
   1204c:	csel	w0, w9, wzr, gt
   12050:	ret
   12054:	cmp	w10, #0x0
   12058:	mov	w8, #0x1                   	// #1
   1205c:	cneg	w0, w8, lt  // lt = tstop
   12060:	ret
   12064:	cmp	x1, #0x0
   12068:	csetm	w0, ne  // ne = any
   1206c:	ret
   12070:	cneg	w0, w9, lt  // lt = tstop
   12074:	ret
   12078:	cneg	w0, w9, cc  // cc = lo, ul, last
   1207c:	ret

0000000000012080 <__gmpf_mul_2exp@@Base>:
   12080:	stp	x29, x30, [sp, #-80]!
   12084:	stp	x24, x23, [sp, #32]
   12088:	stp	x22, x21, [sp, #48]
   1208c:	stp	x20, x19, [sp, #64]
   12090:	ldrsw	x24, [x1, #4]
   12094:	mov	x19, x0
   12098:	str	x25, [sp, #16]
   1209c:	mov	x29, sp
   120a0:	cbz	w24, 12170 <__gmpf_mul_2exp@@Base+0xf0>
   120a4:	ldr	x21, [x19, #16]
   120a8:	ldrsw	x22, [x19]
   120ac:	ldp	x25, x1, [x1, #8]
   120b0:	cmp	w24, #0x0
   120b4:	mov	x20, x2
   120b8:	cneg	x23, x24, lt  // lt = tstop
   120bc:	ands	x3, x2, #0x3f
   120c0:	b.eq	120f0 <__gmpf_mul_2exp@@Base+0x70>  // b.none
   120c4:	subs	x8, x23, x22
   120c8:	b.le	12120 <__gmpf_mul_2exp@@Base+0xa0>
   120cc:	add	x1, x1, x8, lsl #3
   120d0:	mov	w8, #0x40                  	// #64
   120d4:	add	x0, x21, #0x8
   120d8:	sub	w3, w8, w3
   120dc:	mov	x2, x22
   120e0:	bl	c1a0 <__gmpn_rshift@plt>
   120e4:	str	x0, [x21]
   120e8:	ldr	x0, [x21, x22, lsl #3]
   120ec:	b	12134 <__gmpf_mul_2exp@@Base+0xb4>
   120f0:	add	x8, x22, #0x1
   120f4:	subs	x8, x23, x8
   120f8:	add	x8, x1, x8, lsl #3
   120fc:	csel	x1, x8, x1, gt
   12100:	csinc	x22, x23, x22, le
   12104:	cmp	x21, x1
   12108:	b.eq	12118 <__gmpf_mul_2exp@@Base+0x98>  // b.none
   1210c:	mov	x0, x21
   12110:	mov	x2, x22
   12114:	bl	ca50 <__gmpn_copyi@plt>
   12118:	add	x8, x25, x20, lsr #6
   1211c:	b	12144 <__gmpf_mul_2exp@@Base+0xc4>
   12120:	mov	x0, x21
   12124:	mov	x2, x23
   12128:	bl	c180 <__gmpn_lshift@plt>
   1212c:	mov	x22, x23
   12130:	str	x0, [x21, x23, lsl #3]
   12134:	cmp	x0, #0x0
   12138:	add	x8, x25, x20, lsr #6
   1213c:	cinc	x22, x22, ne  // ne = any
   12140:	cinc	x8, x8, ne  // ne = any
   12144:	str	x8, [x19, #8]
   12148:	neg	w8, w22
   1214c:	cmp	w24, #0x0
   12150:	csel	x8, x22, x8, ge  // ge = tcont
   12154:	str	w8, [x19, #4]
   12158:	ldp	x20, x19, [sp, #64]
   1215c:	ldp	x22, x21, [sp, #48]
   12160:	ldp	x24, x23, [sp, #32]
   12164:	ldr	x25, [sp, #16]
   12168:	ldp	x29, x30, [sp], #80
   1216c:	ret
   12170:	str	wzr, [x19, #4]
   12174:	str	xzr, [x19, #8]
   12178:	b	12158 <__gmpf_mul_2exp@@Base+0xd8>

000000000001217c <__gmpf_div_2exp@@Base>:
   1217c:	stp	x29, x30, [sp, #-80]!
   12180:	stp	x24, x23, [sp, #32]
   12184:	stp	x22, x21, [sp, #48]
   12188:	stp	x20, x19, [sp, #64]
   1218c:	ldrsw	x24, [x1, #4]
   12190:	mov	x19, x0
   12194:	str	x25, [sp, #16]
   12198:	mov	x29, sp
   1219c:	cbz	w24, 12278 <__gmpf_div_2exp@@Base+0xfc>
   121a0:	ldr	x21, [x19, #16]
   121a4:	ldrsw	x22, [x19]
   121a8:	ldp	x25, x1, [x1, #8]
   121ac:	cmp	w24, #0x0
   121b0:	mov	x20, x2
   121b4:	cneg	x23, x24, lt  // lt = tstop
   121b8:	ands	x3, x2, #0x3f
   121bc:	b.eq	121e4 <__gmpf_div_2exp@@Base+0x68>  // b.none
   121c0:	subs	x8, x23, x22
   121c4:	b.le	12214 <__gmpf_div_2exp@@Base+0x98>
   121c8:	add	x1, x1, x8, lsl #3
   121cc:	add	x0, x21, #0x8
   121d0:	mov	x2, x22
   121d4:	bl	c1a0 <__gmpn_rshift@plt>
   121d8:	str	x0, [x21]
   121dc:	ldr	x0, [x21, x22, lsl #3]
   121e0:	b	12230 <__gmpf_div_2exp@@Base+0xb4>
   121e4:	add	x8, x22, #0x1
   121e8:	subs	x8, x23, x8
   121ec:	add	x8, x1, x8, lsl #3
   121f0:	csel	x1, x8, x1, gt
   121f4:	csinc	x22, x23, x22, le
   121f8:	cmp	x21, x1
   121fc:	b.eq	1220c <__gmpf_div_2exp@@Base+0x90>  // b.none
   12200:	mov	x0, x21
   12204:	mov	x2, x22
   12208:	bl	ca50 <__gmpn_copyi@plt>
   1220c:	sub	x8, x25, x20, lsr #6
   12210:	b	1224c <__gmpf_div_2exp@@Base+0xd0>
   12214:	mov	w8, #0x40                  	// #64
   12218:	sub	w3, w8, w3
   1221c:	mov	x0, x21
   12220:	mov	x2, x23
   12224:	bl	c180 <__gmpn_lshift@plt>
   12228:	mov	x22, x23
   1222c:	str	x0, [x21, x23, lsl #3]
   12230:	lsr	x8, x20, #6
   12234:	sub	x9, x25, x8
   12238:	mvn	x8, x8
   1223c:	cmp	x0, #0x0
   12240:	add	x8, x25, x8
   12244:	cinc	x22, x22, ne  // ne = any
   12248:	csel	x8, x8, x9, eq  // eq = none
   1224c:	str	x8, [x19, #8]
   12250:	neg	w8, w22
   12254:	cmp	w24, #0x0
   12258:	csel	x8, x22, x8, ge  // ge = tcont
   1225c:	str	w8, [x19, #4]
   12260:	ldp	x20, x19, [sp, #64]
   12264:	ldp	x22, x21, [sp, #48]
   12268:	ldp	x24, x23, [sp, #32]
   1226c:	ldr	x25, [sp, #16]
   12270:	ldp	x29, x30, [sp], #80
   12274:	ret
   12278:	str	wzr, [x19, #4]
   1227c:	str	xzr, [x19, #8]
   12280:	b	12260 <__gmpf_div_2exp@@Base+0xe4>

0000000000012284 <__gmpf_abs@@Base>:
   12284:	stp	x29, x30, [sp, #-48]!
   12288:	stp	x20, x19, [sp, #32]
   1228c:	ldr	w8, [x1, #4]
   12290:	mov	x19, x0
   12294:	str	x21, [sp, #16]
   12298:	mov	x29, sp
   1229c:	cmp	w8, #0x0
   122a0:	cneg	w20, w8, mi  // mi = first
   122a4:	cmp	x0, x1
   122a8:	b.eq	122e0 <__gmpf_abs@@Base+0x5c>  // b.none
   122ac:	ldrsw	x8, [x19]
   122b0:	ldr	x9, [x1, #16]
   122b4:	ldr	x0, [x19, #16]
   122b8:	mov	x21, x1
   122bc:	add	x10, x8, #0x1
   122c0:	subs	x10, x20, x10
   122c4:	add	x10, x9, x10, lsl #3
   122c8:	csinc	x20, x20, x8, le
   122cc:	csel	x1, x10, x9, gt
   122d0:	mov	x2, x20
   122d4:	bl	ca50 <__gmpn_copyi@plt>
   122d8:	ldr	x8, [x21, #8]
   122dc:	str	x8, [x19, #8]
   122e0:	str	w20, [x19, #4]
   122e4:	ldp	x20, x19, [sp, #32]
   122e8:	ldr	x21, [sp, #16]
   122ec:	ldp	x29, x30, [sp], #48
   122f0:	ret

00000000000122f4 <__gmpf_neg@@Base>:
   122f4:	stp	x29, x30, [sp, #-48]!
   122f8:	stp	x20, x19, [sp, #32]
   122fc:	ldrsw	x8, [x1, #4]
   12300:	str	x21, [sp, #16]
   12304:	mov	x19, x0
   12308:	cmp	x0, x1
   1230c:	neg	x21, x8
   12310:	mov	x29, sp
   12314:	b.eq	12358 <__gmpf_neg@@Base+0x64>  // b.none
   12318:	ldrsw	x9, [x19]
   1231c:	ldr	x10, [x1, #16]
   12320:	cmp	w8, #0x1
   12324:	ldr	x0, [x19, #16]
   12328:	cneg	x11, x21, ge  // ge = tcont
   1232c:	add	x12, x9, #0x1
   12330:	subs	x12, x11, x12
   12334:	add	x12, x10, x12, lsl #3
   12338:	mov	x20, x1
   1233c:	csinc	x2, x11, x9, le
   12340:	csel	x1, x12, x10, gt
   12344:	cmp	w8, #0x1
   12348:	cneg	x21, x2, ge  // ge = tcont
   1234c:	bl	ca50 <__gmpn_copyi@plt>
   12350:	ldr	x8, [x20, #8]
   12354:	str	x8, [x19, #8]
   12358:	str	w21, [x19, #4]
   1235c:	ldp	x20, x19, [sp, #32]
   12360:	ldr	x21, [sp, #16]
   12364:	ldp	x29, x30, [sp], #48
   12368:	ret

000000000001236c <__gmpf_set_q@@Base>:
   1236c:	stp	x29, x30, [sp, #-96]!
   12370:	stp	x28, x27, [sp, #16]
   12374:	stp	x26, x25, [sp, #32]
   12378:	stp	x24, x23, [sp, #48]
   1237c:	stp	x22, x21, [sp, #64]
   12380:	stp	x20, x19, [sp, #80]
   12384:	mov	x29, sp
   12388:	sub	sp, sp, #0x20
   1238c:	ldrsw	x27, [x1, #4]
   12390:	mov	x19, x0
   12394:	cbz	w27, 1243c <__gmpf_set_q@@Base+0xd0>
   12398:	ldrsw	x20, [x1, #20]
   1239c:	ldr	x8, [x19, #16]
   123a0:	ldrsw	x22, [x19]
   123a4:	cmp	w27, #0x0
   123a8:	cneg	x24, x27, lt  // lt = tstop
   123ac:	stur	x8, [x29, #-24]
   123b0:	sub	x8, x24, x20
   123b4:	add	x8, x8, #0x1
   123b8:	add	x9, x22, #0x1
   123bc:	sub	x28, x9, x8
   123c0:	ldr	x25, [x1, #8]
   123c4:	ldr	x3, [x1, #24]
   123c8:	add	x23, x28, x24
   123cc:	stp	x8, xzr, [x29, #-16]
   123d0:	lsl	x8, x23, #3
   123d4:	add	x1, x8, #0x8
   123d8:	mov	w8, #0x7f00                	// #32512
   123dc:	cmp	x1, x8
   123e0:	stur	x9, [x29, #-32]
   123e4:	b.hi	12448 <__gmpf_set_q@@Base+0xdc>  // b.pmore
   123e8:	add	x9, x1, #0xf
   123ec:	mov	x8, sp
   123f0:	and	x9, x9, #0xfffffffffffffff0
   123f4:	sub	x26, x8, x9
   123f8:	mov	sp, x26
   123fc:	cmp	x28, #0x1
   12400:	b.lt	12464 <__gmpf_set_q@@Base+0xf8>  // b.tstop
   12404:	add	x8, x20, x22
   12408:	sub	x8, x8, x24
   1240c:	lsl	x2, x8, #3
   12410:	mov	x0, x26
   12414:	mov	w1, wzr
   12418:	mov	x21, x3
   1241c:	bl	c5f0 <memset@plt>
   12420:	add	x0, x26, x28, lsl #3
   12424:	mov	x1, x25
   12428:	mov	x2, x24
   1242c:	bl	ca50 <__gmpn_copyi@plt>
   12430:	mov	x3, x21
   12434:	mov	x1, x26
   12438:	b	12468 <__gmpf_set_q@@Base+0xfc>
   1243c:	str	wzr, [x19, #4]
   12440:	str	xzr, [x19, #8]
   12444:	b	124b4 <__gmpf_set_q@@Base+0x148>
   12448:	sub	x0, x29, #0x8
   1244c:	mov	x21, x3
   12450:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   12454:	mov	x3, x21
   12458:	mov	x26, x0
   1245c:	cmp	x28, #0x1
   12460:	b.ge	12404 <__gmpf_set_q@@Base+0x98>  // b.tcont
   12464:	sub	x1, x25, x28, lsl #3
   12468:	ldur	x21, [x29, #-24]
   1246c:	mov	x2, x23
   12470:	mov	x4, x20
   12474:	mov	x5, x26
   12478:	mov	x0, x21
   1247c:	bl	c320 <__gmpn_div_q@plt>
   12480:	ldr	x8, [x21, x22, lsl #3]
   12484:	ldur	x9, [x29, #-32]
   12488:	ldp	x10, x0, [x29, #-16]
   1248c:	cmp	x8, #0x0
   12490:	cset	w8, eq  // eq = none
   12494:	sub	x9, x9, x8
   12498:	sub	x8, x10, x8
   1249c:	str	x8, [x19, #8]
   124a0:	neg	w8, w9
   124a4:	cmp	w27, #0x0
   124a8:	csel	x8, x9, x8, ge  // ge = tcont
   124ac:	str	w8, [x19, #4]
   124b0:	cbnz	x0, 124d4 <__gmpf_set_q@@Base+0x168>
   124b4:	mov	sp, x29
   124b8:	ldp	x20, x19, [sp, #80]
   124bc:	ldp	x22, x21, [sp, #64]
   124c0:	ldp	x24, x23, [sp, #48]
   124c4:	ldp	x26, x25, [sp, #32]
   124c8:	ldp	x28, x27, [sp, #16]
   124cc:	ldp	x29, x30, [sp], #96
   124d0:	ret
   124d4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   124d8:	b	124b4 <__gmpf_set_q@@Base+0x148>

00000000000124dc <__gmpf_get_d@@Base>:
   124dc:	ldrsw	x2, [x0, #4]
   124e0:	cbz	w2, 124fc <__gmpf_get_d@@Base+0x20>
   124e4:	ldp	x8, x0, [x0, #8]
   124e8:	cmp	x2, #0x0
   124ec:	cneg	x1, x2, mi  // mi = first
   124f0:	sub	x8, x8, x1
   124f4:	lsl	x3, x8, #6
   124f8:	b	bf40 <__gmpn_get_d@plt>
   124fc:	fmov	d0, xzr
   12500:	ret

0000000000012504 <__gmpf_get_d_2exp@@Base>:
   12504:	ldrsw	x2, [x1, #4]
   12508:	cbz	w2, 1253c <__gmpf_get_d_2exp@@Base+0x38>
   1250c:	ldp	x9, x8, [x1, #8]
   12510:	cmp	x2, #0x0
   12514:	cneg	x1, x2, mi  // mi = first
   12518:	add	x10, x8, x1, lsl #3
   1251c:	ldur	x10, [x10, #-8]
   12520:	lsl	x9, x9, #6
   12524:	clz	x10, x10
   12528:	sub	x9, x9, x10
   1252c:	sub	x3, x10, x1, lsl #6
   12530:	str	x9, [x0]
   12534:	mov	x0, x8
   12538:	b	bf40 <__gmpn_get_d@plt>
   1253c:	fmov	d0, xzr
   12540:	str	xzr, [x0]
   12544:	ret

0000000000012548 <__gmpf_set_default_prec@@Base>:
   12548:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1254c:	cmp	x0, #0x35
   12550:	mov	w8, #0x35                  	// #53
   12554:	ldr	x9, [x9, #3960]
   12558:	csel	x8, x0, x8, hi  // hi = pmore
   1255c:	add	x8, x8, #0x7f
   12560:	lsr	x8, x8, #6
   12564:	str	x8, [x9]
   12568:	ret

000000000001256c <__gmpf_set_prec@@Base>:
   1256c:	stp	x29, x30, [sp, #-48]!
   12570:	stp	x22, x21, [sp, #16]
   12574:	stp	x20, x19, [sp, #32]
   12578:	cmp	x1, #0x35
   1257c:	mov	w8, #0x35                  	// #53
   12580:	ldrsw	x22, [x0]
   12584:	csel	x8, x1, x8, hi  // hi = pmore
   12588:	add	x8, x8, #0x7f
   1258c:	lsr	x8, x8, #6
   12590:	cmp	x8, x22
   12594:	mov	x29, sp
   12598:	b.eq	12608 <__gmpf_set_prec@@Base+0x9c>  // b.none
   1259c:	ldrsw	x10, [x0, #4]
   125a0:	ldr	x21, [x0, #16]
   125a4:	add	x20, x8, #0x1
   125a8:	mov	x19, x0
   125ac:	cmp	x10, #0x0
   125b0:	cneg	x9, x10, mi  // mi = first
   125b4:	cmp	x9, x20
   125b8:	str	w8, [x0]
   125bc:	b.le	125e4 <__gmpf_set_prec@@Base+0x78>
   125c0:	mvn	x11, x8
   125c4:	cmp	w10, #0x0
   125c8:	add	x9, x21, x9, lsl #3
   125cc:	csinv	x8, x20, x8, ge  // ge = tcont
   125d0:	add	x1, x9, x11, lsl #3
   125d4:	mov	x0, x21
   125d8:	mov	x2, x20
   125dc:	str	w8, [x19, #4]
   125e0:	bl	ca50 <__gmpn_copyi@plt>
   125e4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   125e8:	ldr	x8, [x8, #3792]
   125ec:	lsl	x9, x22, #3
   125f0:	add	x1, x9, #0x8
   125f4:	lsl	x2, x20, #3
   125f8:	ldr	x8, [x8]
   125fc:	mov	x0, x21
   12600:	blr	x8
   12604:	str	x0, [x19, #16]
   12608:	ldp	x20, x19, [sp, #32]
   1260c:	ldp	x22, x21, [sp, #16]
   12610:	ldp	x29, x30, [sp], #48
   12614:	ret

0000000000012618 <__gmpf_set_prec_raw@@Base>:
   12618:	cmp	x1, #0x35
   1261c:	mov	w8, #0x35                  	// #53
   12620:	csel	x8, x1, x8, hi  // hi = pmore
   12624:	add	x8, x8, #0x7f
   12628:	lsr	x8, x8, #6
   1262c:	str	w8, [x0]
   12630:	ret

0000000000012634 <__gmpf_get_default_prec@@Base>:
   12634:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   12638:	ldr	x8, [x8, #3960]
   1263c:	ldr	x8, [x8]
   12640:	lsl	x8, x8, #6
   12644:	sub	x0, x8, #0x40
   12648:	ret

000000000001264c <__gmpf_get_prec@@Base>:
   1264c:	ldrsw	x8, [x0]
   12650:	lsl	x8, x8, #6
   12654:	sub	x0, x8, #0x40
   12658:	ret

000000000001265c <__gmpf_ui_div@@Base>:
   1265c:	stp	x29, x30, [sp, #-96]!
   12660:	stp	x28, x27, [sp, #16]
   12664:	stp	x26, x25, [sp, #32]
   12668:	stp	x24, x23, [sp, #48]
   1266c:	stp	x22, x21, [sp, #64]
   12670:	stp	x20, x19, [sp, #80]
   12674:	mov	x29, sp
   12678:	sub	sp, sp, #0x30
   1267c:	ldrsw	x27, [x2, #4]
   12680:	cbz	w27, 127f4 <__gmpf_ui_div@@Base+0x198>
   12684:	mov	x11, x1
   12688:	mov	x19, x0
   1268c:	cbz	x1, 127c4 <__gmpf_ui_div@@Base+0x168>
   12690:	ldrsw	x12, [x19]
   12694:	ldr	x21, [x19, #16]
   12698:	ldp	x13, x23, [x2, #8]
   1269c:	cmp	w27, #0x0
   126a0:	cneg	x22, x27, lt  // lt = tstop
   126a4:	add	x10, x12, #0x1
   126a8:	add	x20, x22, x10
   126ac:	cmp	x21, x23
   126b0:	sub	x24, x20, #0x1
   126b4:	csel	x8, x22, xzr, eq  // eq = none
   126b8:	add	x9, x24, x22
   126bc:	add	x8, x9, x8
   126c0:	lsl	x1, x8, #3
   126c4:	mov	w8, #0x7f00                	// #32512
   126c8:	cmp	x1, x8
   126cc:	mov	w14, #0x2                   	// #2
   126d0:	stp	x10, xzr, [x29, #-16]
   126d4:	stp	x12, x11, [x29, #-32]
   126d8:	b.hi	127d0 <__gmpf_ui_div@@Base+0x174>  // b.pmore
   126dc:	add	x9, x1, #0xf
   126e0:	mov	x8, sp
   126e4:	and	x9, x9, #0xfffffffffffffff0
   126e8:	sub	x25, x8, x9
   126ec:	mov	sp, x25
   126f0:	sub	x28, x20, #0x2
   126f4:	cmp	x21, x23
   126f8:	add	x26, x25, x22, lsl #3
   126fc:	b.ne	12720 <__gmpf_ui_div@@Base+0xc4>  // b.any
   12700:	add	x23, x26, x24, lsl #3
   12704:	mov	x0, x23
   12708:	mov	x1, x21
   1270c:	mov	x2, x22
   12710:	mov	x20, x13
   12714:	bl	ca50 <__gmpn_copyi@plt>
   12718:	mov	w14, #0x2                   	// #2
   1271c:	mov	x13, x20
   12720:	ldur	x20, [x29, #-32]
   12724:	sub	x8, x14, x13
   12728:	stur	x8, [x29, #-40]
   1272c:	cbz	x28, 12748 <__gmpf_ui_div@@Base+0xec>
   12730:	add	x8, x22, x20
   12734:	lsl	x8, x8, #3
   12738:	add	x0, x25, x22, lsl #3
   1273c:	sub	x2, x8, #0x8
   12740:	mov	w1, wzr
   12744:	bl	c5f0 <memset@plt>
   12748:	ldur	x8, [x29, #-24]
   1274c:	mov	x0, x21
   12750:	mov	x1, x25
   12754:	mov	x2, xzr
   12758:	mov	x3, x26
   1275c:	mov	x4, x24
   12760:	mov	x5, x23
   12764:	mov	x6, x22
   12768:	str	x8, [x26, x28, lsl #3]
   1276c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   12770:	ldr	x8, [x21, x20, lsl #3]
   12774:	ldp	x9, x0, [x29, #-16]
   12778:	ldur	x10, [x29, #-40]
   1277c:	cmp	x8, #0x0
   12780:	cset	w8, eq  // eq = none
   12784:	sub	x9, x9, x8
   12788:	cmp	w27, #0x0
   1278c:	sub	x8, x10, x8
   12790:	neg	w10, w9
   12794:	csel	x9, x9, x10, ge  // ge = tcont
   12798:	str	w9, [x19, #4]
   1279c:	str	x8, [x19, #8]
   127a0:	cbnz	x0, 127ec <__gmpf_ui_div@@Base+0x190>
   127a4:	mov	sp, x29
   127a8:	ldp	x20, x19, [sp, #80]
   127ac:	ldp	x22, x21, [sp, #64]
   127b0:	ldp	x24, x23, [sp, #48]
   127b4:	ldp	x26, x25, [sp, #32]
   127b8:	ldp	x28, x27, [sp, #16]
   127bc:	ldp	x29, x30, [sp], #96
   127c0:	ret
   127c4:	str	wzr, [x19, #4]
   127c8:	str	xzr, [x19, #8]
   127cc:	b	127a4 <__gmpf_ui_div@@Base+0x148>
   127d0:	sub	x0, x29, #0x8
   127d4:	mov	x25, x13
   127d8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   127dc:	mov	w14, #0x2                   	// #2
   127e0:	mov	x13, x25
   127e4:	mov	x25, x0
   127e8:	b	126f0 <__gmpf_ui_div@@Base+0x94>
   127ec:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   127f0:	b	127a4 <__gmpf_ui_div@@Base+0x148>
   127f4:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000127f8 <__gmpf_sqrt_ui@@Base>:
   127f8:	stp	x29, x30, [sp, #-64]!
   127fc:	stp	x24, x23, [sp, #16]
   12800:	stp	x22, x21, [sp, #32]
   12804:	stp	x20, x19, [sp, #48]
   12808:	mov	x29, sp
   1280c:	sub	sp, sp, #0x10
   12810:	mov	x20, x1
   12814:	cmp	x1, #0x1
   12818:	mov	x19, x0
   1281c:	b.ls	128b8 <__gmpf_sqrt_ui@@Base+0xc0>  // b.plast
   12820:	stur	xzr, [x29, #-8]
   12824:	ldr	w23, [x19]
   12828:	mov	w9, #0x7f00                	// #32512
   1282c:	sbfiz	x8, x23, #1, #32
   12830:	sub	x21, x8, #0x1
   12834:	lsl	x1, x21, #3
   12838:	cmp	x1, x9
   1283c:	sub	x24, x8, #0x2
   12840:	b.hi	128cc <__gmpf_sqrt_ui@@Base+0xd4>  // b.pmore
   12844:	add	x9, x1, #0xf
   12848:	mov	x8, sp
   1284c:	and	x9, x9, #0xfffffffffffffff0
   12850:	sub	x22, x8, x9
   12854:	mov	sp, x22
   12858:	cbz	x24, 12874 <__gmpf_sqrt_ui@@Base+0x7c>
   1285c:	sxtw	x8, w23
   12860:	lsl	x8, x8, #4
   12864:	sub	x2, x8, #0x10
   12868:	mov	x0, x22
   1286c:	mov	w1, wzr
   12870:	bl	c5f0 <memset@plt>
   12874:	str	x20, [x22, x24, lsl #3]
   12878:	ldr	x0, [x19, #16]
   1287c:	mov	x1, xzr
   12880:	mov	x2, x22
   12884:	mov	x3, x21
   12888:	bl	d3b0 <__gmpn_sqrtrem@plt>
   1288c:	mov	w8, #0x1                   	// #1
   12890:	str	w23, [x19, #4]
   12894:	str	x8, [x19, #8]
   12898:	ldur	x0, [x29, #-8]
   1289c:	cbnz	x0, 128e0 <__gmpf_sqrt_ui@@Base+0xe8>
   128a0:	mov	sp, x29
   128a4:	ldp	x20, x19, [sp, #48]
   128a8:	ldp	x22, x21, [sp, #32]
   128ac:	ldp	x24, x23, [sp, #16]
   128b0:	ldp	x29, x30, [sp], #64
   128b4:	ret
   128b8:	ldr	x8, [x19, #16]
   128bc:	str	x20, [x19, #8]
   128c0:	str	w20, [x19, #4]
   128c4:	str	x20, [x8]
   128c8:	b	128a0 <__gmpf_sqrt_ui@@Base+0xa8>
   128cc:	sub	x0, x29, #0x8
   128d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   128d4:	mov	x22, x0
   128d8:	cbnz	x24, 1285c <__gmpf_sqrt_ui@@Base+0x64>
   128dc:	b	12874 <__gmpf_sqrt_ui@@Base+0x7c>
   128e0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   128e4:	b	128a0 <__gmpf_sqrt_ui@@Base+0xa8>

00000000000128e8 <__gmpf_ceil@@Base>:
   128e8:	mov	w2, #0x1                   	// #1
   128ec:	b	128f0 <__gmpf_ceil@@Base+0x8>
   128f0:	ldrsw	x10, [x1, #4]
   128f4:	cbz	w10, 129a4 <__gmpf_ceil@@Base+0xbc>
   128f8:	ldr	x11, [x1, #8]
   128fc:	ldr	x8, [x0, #16]
   12900:	mov	w9, w2
   12904:	cmp	x11, #0x0
   12908:	b.le	12988 <__gmpf_ceil@@Base+0xa0>
   1290c:	ldrsw	x14, [x0]
   12910:	str	x11, [x0, #8]
   12914:	cmp	w10, #0x0
   12918:	ldr	x12, [x1, #16]
   1291c:	cneg	x13, x10, lt  // lt = tstop
   12920:	cmp	x13, x11
   12924:	csel	x15, x13, x11, lt  // lt = tstop
   12928:	add	x16, x14, #0x1
   1292c:	cmp	x15, x16
   12930:	add	x11, x12, x13, lsl #3
   12934:	csinc	x2, x15, x14, lt  // lt = tstop
   12938:	eor	w9, w10, w9
   1293c:	sub	x1, x11, x2, lsl #3
   12940:	tbnz	w9, #31, 12968 <__gmpf_ceil@@Base+0x80>
   12944:	cmp	x12, x1
   12948:	b.eq	12968 <__gmpf_ceil@@Base+0x80>  // b.none
   1294c:	lsl	x9, x13, #3
   12950:	sub	x9, x9, x2, lsl #3
   12954:	mov	x14, x12
   12958:	ldr	x15, [x14], #8
   1295c:	cbnz	x15, 129b0 <__gmpf_ceil@@Base+0xc8>
   12960:	subs	x9, x9, #0x8
   12964:	b.ne	12958 <__gmpf_ceil@@Base+0x70>  // b.any
   12968:	neg	x9, x2
   1296c:	cmp	w10, #0x0
   12970:	csel	x9, x2, x9, ge  // ge = tcont
   12974:	cmp	x8, x1
   12978:	str	w9, [x0, #4]
   1297c:	b.eq	129ac <__gmpf_ceil@@Base+0xc4>  // b.none
   12980:	mov	x0, x8
   12984:	b	ca50 <__gmpn_copyi@plt>
   12988:	eor	w10, w10, w9
   1298c:	tbnz	w10, #31, 129a4 <__gmpf_ceil@@Base+0xbc>
   12990:	mov	w10, #0x1                   	// #1
   12994:	str	x10, [x8]
   12998:	str	x10, [x0, #8]
   1299c:	str	w9, [x0, #4]
   129a0:	ret
   129a4:	str	wzr, [x0, #4]
   129a8:	str	xzr, [x0, #8]
   129ac:	ret
   129b0:	ldr	x9, [x1]
   129b4:	adds	x9, x9, #0x1
   129b8:	str	x9, [x8]
   129bc:	b.cc	12a94 <__gmpf_ceil@@Base+0x1ac>  // b.lo, b.ul, b.last
   129c0:	mov	x13, xzr
   129c4:	sub	x12, x2, #0x1
   129c8:	mov	w9, #0x1                   	// #1
   129cc:	cmp	x9, x2
   129d0:	b.ge	12af4 <__gmpf_ceil@@Base+0x20c>  // b.tcont
   129d4:	add	x14, x1, x13
   129d8:	ldr	x14, [x14, #8]
   129dc:	add	x15, x8, x13
   129e0:	add	x9, x9, #0x1
   129e4:	add	x13, x13, #0x8
   129e8:	adds	x14, x14, #0x1
   129ec:	sub	x12, x12, #0x1
   129f0:	str	x14, [x15, #8]
   129f4:	b.cs	129cc <__gmpf_ceil@@Base+0xe4>  // b.hs, b.nlast
   129f8:	cmp	x1, x8
   129fc:	b.eq	12b08 <__gmpf_ceil@@Base+0x220>  // b.none
   12a00:	subs	x14, x2, x9
   12a04:	b.le	12b08 <__gmpf_ceil@@Base+0x220>
   12a08:	cmp	x14, #0x4
   12a0c:	b.cc	12a78 <__gmpf_ceil@@Base+0x190>  // b.lo, b.ul, b.last
   12a10:	add	x15, x8, x13
   12a14:	add	x15, x15, #0x8
   12a18:	cmp	x15, x11
   12a1c:	b.cs	12a34 <__gmpf_ceil@@Base+0x14c>  // b.hs, b.nlast
   12a20:	add	x16, x1, x13
   12a24:	add	x15, x8, x2, lsl #3
   12a28:	add	x16, x16, #0x8
   12a2c:	cmp	x16, x15
   12a30:	b.cc	12a78 <__gmpf_ceil@@Base+0x190>  // b.lo, b.ul, b.last
   12a34:	sub	x16, x2, x9
   12a38:	add	x15, x8, x13
   12a3c:	add	x17, x1, x13
   12a40:	and	x18, x12, #0xfffffffffffffffc
   12a44:	and	x13, x14, #0xfffffffffffffffc
   12a48:	add	x12, x15, #0x18
   12a4c:	add	x15, x17, #0x18
   12a50:	add	x9, x18, x9
   12a54:	and	x16, x16, #0xfffffffffffffffc
   12a58:	ldp	q0, q1, [x15, #-16]
   12a5c:	add	x15, x15, #0x20
   12a60:	subs	x16, x16, #0x4
   12a64:	stp	q0, q1, [x12, #-16]
   12a68:	add	x12, x12, #0x20
   12a6c:	b.ne	12a58 <__gmpf_ceil@@Base+0x170>  // b.any
   12a70:	cmp	x14, x13
   12a74:	b.eq	12b08 <__gmpf_ceil@@Base+0x220>  // b.none
   12a78:	sub	x12, x9, x2
   12a7c:	add	x8, x8, x9, lsl #3
   12a80:	ldr	x9, [x11, x12, lsl #3]
   12a84:	adds	x12, x12, #0x1
   12a88:	str	x9, [x8], #8
   12a8c:	b.cc	12a80 <__gmpf_ceil@@Base+0x198>  // b.lo, b.ul, b.last
   12a90:	b	12b08 <__gmpf_ceil@@Base+0x220>
   12a94:	cmp	x2, #0x2
   12a98:	b.lt	12b08 <__gmpf_ceil@@Base+0x220>  // b.tstop
   12a9c:	cmp	x1, x8
   12aa0:	b.eq	12b08 <__gmpf_ceil@@Base+0x220>  // b.none
   12aa4:	sub	x9, x2, #0x1
   12aa8:	cmp	x9, #0x4
   12aac:	b.cc	12ad4 <__gmpf_ceil@@Base+0x1ec>  // b.lo, b.ul, b.last
   12ab0:	add	x14, x8, #0x8
   12ab4:	sub	x13, x13, x2
   12ab8:	cmp	x14, x11
   12abc:	add	x12, x12, x13, lsl #3
   12ac0:	b.cs	12b1c <__gmpf_ceil@@Base+0x234>  // b.hs, b.nlast
   12ac4:	add	x13, x8, x2, lsl #3
   12ac8:	add	x14, x12, #0x8
   12acc:	cmp	x14, x13
   12ad0:	b.cs	12b1c <__gmpf_ceil@@Base+0x234>  // b.hs, b.nlast
   12ad4:	mov	w12, #0x1                   	// #1
   12ad8:	sub	x9, x12, x2
   12adc:	add	x8, x8, x12, lsl #3
   12ae0:	ldr	x12, [x11, x9, lsl #3]
   12ae4:	adds	x9, x9, #0x1
   12ae8:	str	x12, [x8], #8
   12aec:	b.cc	12ae0 <__gmpf_ceil@@Base+0x1f8>  // b.lo, b.ul, b.last
   12af0:	b	12b08 <__gmpf_ceil@@Base+0x220>
   12af4:	mov	w2, #0x1                   	// #1
   12af8:	str	x2, [x8]
   12afc:	ldr	x8, [x0, #8]
   12b00:	add	x8, x8, #0x1
   12b04:	str	x8, [x0, #8]
   12b08:	neg	w8, w2
   12b0c:	cmp	w10, #0x0
   12b10:	csel	x8, x2, x8, ge  // ge = tcont
   12b14:	str	w8, [x0, #4]
   12b18:	ret
   12b1c:	and	x13, x9, #0xfffffffffffffffc
   12b20:	add	x14, x12, #0x18
   12b24:	orr	x12, x13, #0x1
   12b28:	add	x15, x8, #0x18
   12b2c:	mov	x16, x13
   12b30:	ldp	q0, q1, [x14, #-16]
   12b34:	add	x14, x14, #0x20
   12b38:	subs	x16, x16, #0x4
   12b3c:	stp	q0, q1, [x15, #-16]
   12b40:	add	x15, x15, #0x20
   12b44:	b.ne	12b30 <__gmpf_ceil@@Base+0x248>  // b.any
   12b48:	cmp	x9, x13
   12b4c:	b.eq	12b08 <__gmpf_ceil@@Base+0x220>  // b.none
   12b50:	b	12ad8 <__gmpf_ceil@@Base+0x1f0>

0000000000012b54 <__gmpf_floor@@Base>:
   12b54:	mov	w2, #0xffffffff            	// #-1
   12b58:	b	128f0 <__gmpf_ceil@@Base+0x8>

0000000000012b5c <__gmpf_trunc@@Base>:
   12b5c:	ldr	x9, [x1, #8]
   12b60:	cmp	x9, #0x1
   12b64:	b.lt	12bc8 <__gmpf_trunc@@Base+0x6c>  // b.tstop
   12b68:	ldr	w8, [x1, #4]
   12b6c:	cbz	w8, 12bc8 <__gmpf_trunc@@Base+0x6c>
   12b70:	sxtw	x10, w8
   12b74:	ldrsw	x11, [x0]
   12b78:	cmp	w10, #0x0
   12b7c:	ldr	x12, [x1, #16]
   12b80:	cneg	x13, x10, lt  // lt = tstop
   12b84:	cmp	x13, x9
   12b88:	str	x9, [x0, #8]
   12b8c:	ldr	x8, [x0, #16]
   12b90:	csel	x9, x13, x9, lt  // lt = tstop
   12b94:	add	x14, x11, #0x1
   12b98:	cmp	x9, x14
   12b9c:	add	x12, x12, x13, lsl #3
   12ba0:	csinc	x2, x9, x11, lt  // lt = tstop
   12ba4:	cmp	w10, #0x0
   12ba8:	neg	w9, w2
   12bac:	sub	x1, x12, x2, lsl #3
   12bb0:	csel	x9, x2, x9, ge  // ge = tcont
   12bb4:	cmp	x8, x1
   12bb8:	str	w9, [x0, #4]
   12bbc:	b.eq	12bd0 <__gmpf_trunc@@Base+0x74>  // b.none
   12bc0:	mov	x0, x8
   12bc4:	b	ca50 <__gmpn_copyi@plt>
   12bc8:	str	wzr, [x0, #4]
   12bcc:	str	xzr, [x0, #8]
   12bd0:	ret

0000000000012bd4 <__gmpf_pow_ui@@Base>:
   12bd4:	sub	sp, sp, #0x60
   12bd8:	stp	x22, x21, [sp, #64]
   12bdc:	stp	x20, x19, [sp, #80]
   12be0:	mov	x21, x2
   12be4:	mov	x20, x1
   12be8:	cmp	x2, #0x1
   12bec:	mov	x19, x0
   12bf0:	stp	x29, x30, [sp, #32]
   12bf4:	str	x23, [sp, #48]
   12bf8:	add	x29, sp, #0x20
   12bfc:	b.hi	12c14 <__gmpf_pow_ui@@Base+0x40>  // b.pmore
   12c00:	cbz	x21, 12cd4 <__gmpf_pow_ui@@Base+0x100>
   12c04:	mov	x0, x19
   12c08:	mov	x1, x20
   12c0c:	bl	c150 <__gmpf_set@plt>
   12c10:	b	12ce0 <__gmpf_pow_ui@@Base+0x10c>
   12c14:	mov	x0, x19
   12c18:	clz	x23, x21
   12c1c:	bl	c350 <__gmpf_get_prec@plt>
   12c20:	sub	x8, x0, x23
   12c24:	add	x1, x8, #0x3f
   12c28:	add	x0, sp, #0x8
   12c2c:	bl	ceb0 <__gmpf_init2@plt>
   12c30:	add	x0, sp, #0x8
   12c34:	mov	x1, x20
   12c38:	bl	c150 <__gmpf_set@plt>
   12c3c:	cmp	w23, #0x3d
   12c40:	b.hi	12c94 <__gmpf_pow_ui@@Base+0xc0>  // b.pmore
   12c44:	mov	w8, #0x3e                  	// #62
   12c48:	mov	w9, #0x3f                  	// #63
   12c4c:	sub	w22, w8, w23
   12c50:	sub	w23, w9, w23
   12c54:	b	12c68 <__gmpf_pow_ui@@Base+0x94>
   12c58:	sub	w23, w23, #0x1
   12c5c:	cmp	w23, #0x1
   12c60:	sub	x22, x22, #0x1
   12c64:	b.le	12c94 <__gmpf_pow_ui@@Base+0xc0>
   12c68:	add	x0, sp, #0x8
   12c6c:	add	x1, sp, #0x8
   12c70:	add	x2, sp, #0x8
   12c74:	bl	cd30 <__gmpf_mul@plt>
   12c78:	lsr	x8, x21, x22
   12c7c:	tbz	w8, #0, 12c58 <__gmpf_pow_ui@@Base+0x84>
   12c80:	add	x0, sp, #0x8
   12c84:	add	x1, sp, #0x8
   12c88:	mov	x2, x20
   12c8c:	bl	cd30 <__gmpf_mul@plt>
   12c90:	b	12c58 <__gmpf_pow_ui@@Base+0x84>
   12c94:	tbnz	w21, #0, 12ca8 <__gmpf_pow_ui@@Base+0xd4>
   12c98:	add	x1, sp, #0x8
   12c9c:	add	x2, sp, #0x8
   12ca0:	mov	x0, x19
   12ca4:	b	12cc4 <__gmpf_pow_ui@@Base+0xf0>
   12ca8:	add	x0, sp, #0x8
   12cac:	add	x1, sp, #0x8
   12cb0:	add	x2, sp, #0x8
   12cb4:	bl	cd30 <__gmpf_mul@plt>
   12cb8:	add	x1, sp, #0x8
   12cbc:	mov	x0, x19
   12cc0:	mov	x2, x20
   12cc4:	bl	cd30 <__gmpf_mul@plt>
   12cc8:	add	x0, sp, #0x8
   12ccc:	bl	c330 <__gmpf_clear@plt>
   12cd0:	b	12ce0 <__gmpf_pow_ui@@Base+0x10c>
   12cd4:	mov	w1, #0x1                   	// #1
   12cd8:	mov	x0, x19
   12cdc:	bl	c6a0 <__gmpf_set_ui@plt>
   12ce0:	ldp	x20, x19, [sp, #80]
   12ce4:	ldp	x22, x21, [sp, #64]
   12ce8:	ldr	x23, [sp, #48]
   12cec:	ldp	x29, x30, [sp, #32]
   12cf0:	add	sp, sp, #0x60
   12cf4:	ret

0000000000012cf8 <__gmpf_urandomb@@Base>:
   12cf8:	stp	x29, x30, [sp, #-48]!
   12cfc:	stp	x22, x21, [sp, #16]
   12d00:	stp	x20, x19, [sp, #32]
   12d04:	ldrsw	x8, [x0]
   12d08:	add	x9, x2, #0x3f
   12d0c:	ldr	x10, [x1, #24]
   12d10:	lsr	x9, x9, #6
   12d14:	add	x11, x8, #0x1
   12d18:	cmp	x9, x11
   12d1c:	cset	w12, gt
   12d20:	cmp	x9, #0x0
   12d24:	cset	w13, eq  // eq = none
   12d28:	ldr	x21, [x0, #16]
   12d2c:	orr	w12, w13, w12
   12d30:	ldr	x10, [x10, #8]
   12d34:	lsl	x11, x11, #6
   12d38:	cmp	w12, #0x0
   12d3c:	csel	x22, x11, x2, ne  // ne = any
   12d40:	mov	x19, x0
   12d44:	mov	x0, x1
   12d48:	mov	x1, x21
   12d4c:	mov	x2, x22
   12d50:	mov	x29, sp
   12d54:	csinc	x20, x9, x8, eq  // eq = none
   12d58:	blr	x10
   12d5c:	ands	x8, x22, #0x3f
   12d60:	b.eq	12d7c <__gmpf_urandomb@@Base+0x84>  // b.none
   12d64:	mov	w9, #0x40                  	// #64
   12d68:	sub	w3, w9, w8
   12d6c:	mov	x0, x21
   12d70:	mov	x1, x21
   12d74:	mov	x2, x20
   12d78:	bl	c180 <__gmpn_lshift@plt>
   12d7c:	cbz	x20, 12dac <__gmpf_urandomb@@Base+0xb4>
   12d80:	add	x10, x21, x20, lsl #3
   12d84:	mov	x9, xzr
   12d88:	neg	x8, x20
   12d8c:	sub	x10, x10, #0x8
   12d90:	ldr	x11, [x10, x9, lsl #3]
   12d94:	cbnz	x11, 12db8 <__gmpf_urandomb@@Base+0xc0>
   12d98:	sub	x9, x9, #0x1
   12d9c:	cmn	x20, x9
   12da0:	b.ne	12d90 <__gmpf_urandomb@@Base+0x98>  // b.any
   12da4:	mov	x10, xzr
   12da8:	b	12dc0 <__gmpf_urandomb@@Base+0xc8>
   12dac:	mov	x8, xzr
   12db0:	mov	x10, xzr
   12db4:	b	12dc0 <__gmpf_urandomb@@Base+0xc8>
   12db8:	add	x10, x20, x9
   12dbc:	mov	x8, x9
   12dc0:	str	x8, [x19, #8]
   12dc4:	str	w10, [x19, #4]
   12dc8:	ldp	x20, x19, [sp, #32]
   12dcc:	ldp	x22, x21, [sp, #16]
   12dd0:	ldp	x29, x30, [sp], #48
   12dd4:	ret

0000000000012dd8 <__gmpf_swap@@Base>:
   12dd8:	ldr	x8, [x1]
   12ddc:	ldr	x9, [x0]
   12de0:	str	x8, [x0]
   12de4:	str	x9, [x1]
   12de8:	ldur	q0, [x1, #8]
   12dec:	ldur	q1, [x0, #8]
   12df0:	stur	q0, [x0, #8]
   12df4:	stur	q1, [x1, #8]
   12df8:	ret

0000000000012dfc <__gmpf_fits_sint_p@@Base>:
   12dfc:	ldr	x8, [x0, #8]
   12e00:	cmp	x8, #0x1
   12e04:	b.lt	12e3c <__gmpf_fits_sint_p@@Base+0x40>  // b.tstop
   12e08:	cmp	x8, #0x1
   12e0c:	b.ne	12e44 <__gmpf_fits_sint_p@@Base+0x48>  // b.any
   12e10:	ldrsw	x8, [x0, #4]
   12e14:	ldr	x9, [x0, #16]
   12e18:	cmp	w8, #0x0
   12e1c:	cneg	x8, x8, lt  // lt = tstop
   12e20:	add	x8, x9, x8, lsl #3
   12e24:	ldur	x8, [x8, #-8]
   12e28:	mov	w9, #0x7fffffff            	// #2147483647
   12e2c:	cinc	x9, x9, lt  // lt = tstop
   12e30:	cmp	x8, x9
   12e34:	cset	w0, ls  // ls = plast
   12e38:	ret
   12e3c:	mov	w0, #0x1                   	// #1
   12e40:	ret
   12e44:	mov	w0, wzr
   12e48:	ret

0000000000012e4c <__gmpf_fits_slong_p@@Base>:
   12e4c:	ldr	x8, [x0, #8]
   12e50:	cmp	x8, #0x1
   12e54:	b.lt	12e8c <__gmpf_fits_slong_p@@Base+0x40>  // b.tstop
   12e58:	cmp	x8, #0x1
   12e5c:	b.ne	12e94 <__gmpf_fits_slong_p@@Base+0x48>  // b.any
   12e60:	ldrsw	x8, [x0, #4]
   12e64:	ldr	x9, [x0, #16]
   12e68:	cmp	w8, #0x0
   12e6c:	cneg	x8, x8, lt  // lt = tstop
   12e70:	add	x8, x9, x8, lsl #3
   12e74:	ldur	x8, [x8, #-8]
   12e78:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   12e7c:	cinv	x9, x9, lt  // lt = tstop
   12e80:	cmp	x8, x9
   12e84:	cset	w0, ls  // ls = plast
   12e88:	ret
   12e8c:	mov	w0, #0x1                   	// #1
   12e90:	ret
   12e94:	mov	w0, wzr
   12e98:	ret

0000000000012e9c <__gmpf_fits_sshort_p@@Base>:
   12e9c:	ldr	x8, [x0, #8]
   12ea0:	cmp	x8, #0x1
   12ea4:	b.lt	12edc <__gmpf_fits_sshort_p@@Base+0x40>  // b.tstop
   12ea8:	cmp	x8, #0x1
   12eac:	b.ne	12ee4 <__gmpf_fits_sshort_p@@Base+0x48>  // b.any
   12eb0:	ldrsw	x8, [x0, #4]
   12eb4:	ldr	x9, [x0, #16]
   12eb8:	cmp	w8, #0x0
   12ebc:	cneg	x8, x8, lt  // lt = tstop
   12ec0:	add	x8, x9, x8, lsl #3
   12ec4:	ldur	x8, [x8, #-8]
   12ec8:	mov	w9, #0x7fff                	// #32767
   12ecc:	cinc	x9, x9, lt  // lt = tstop
   12ed0:	cmp	x8, x9
   12ed4:	cset	w0, ls  // ls = plast
   12ed8:	ret
   12edc:	mov	w0, #0x1                   	// #1
   12ee0:	ret
   12ee4:	mov	w0, wzr
   12ee8:	ret

0000000000012eec <__gmpf_fits_uint_p@@Base>:
   12eec:	ldr	x9, [x0, #8]
   12ef0:	cmp	x9, #0x1
   12ef4:	b.lt	12f28 <__gmpf_fits_uint_p@@Base+0x3c>  // b.tstop
   12ef8:	mov	x8, x0
   12efc:	cmp	x9, #0x1
   12f00:	mov	w0, wzr
   12f04:	b.ne	12f24 <__gmpf_fits_uint_p@@Base+0x38>  // b.any
   12f08:	ldr	w9, [x8, #4]
   12f0c:	tbnz	w9, #31, 12f24 <__gmpf_fits_uint_p@@Base+0x38>
   12f10:	ldr	x8, [x8, #16]
   12f14:	add	x8, x8, x9, lsl #3
   12f18:	ldur	w8, [x8, #-4]
   12f1c:	cmp	w8, #0x0
   12f20:	cset	w0, eq  // eq = none
   12f24:	ret
   12f28:	mov	w0, #0x1                   	// #1
   12f2c:	ret

0000000000012f30 <__gmpf_fits_ulong_p@@Base>:
   12f30:	ldr	x8, [x0, #8]
   12f34:	cmp	x8, #0x1
   12f38:	b.lt	12f50 <__gmpf_fits_ulong_p@@Base+0x20>  // b.tstop
   12f3c:	ldr	w9, [x0, #4]
   12f40:	tbnz	w9, #31, 12f58 <__gmpf_fits_ulong_p@@Base+0x28>
   12f44:	cmp	x8, #0x1
   12f48:	cset	w0, eq  // eq = none
   12f4c:	ret
   12f50:	mov	w0, #0x1                   	// #1
   12f54:	ret
   12f58:	mov	w0, wzr
   12f5c:	ret

0000000000012f60 <__gmpf_fits_ushort_p@@Base>:
   12f60:	ldr	x9, [x0, #8]
   12f64:	cmp	x9, #0x1
   12f68:	b.lt	12f9c <__gmpf_fits_ushort_p@@Base+0x3c>  // b.tstop
   12f6c:	mov	x8, x0
   12f70:	cmp	x9, #0x1
   12f74:	mov	w0, wzr
   12f78:	b.ne	12f98 <__gmpf_fits_ushort_p@@Base+0x38>  // b.any
   12f7c:	ldr	w9, [x8, #4]
   12f80:	tbnz	w9, #31, 12f98 <__gmpf_fits_ushort_p@@Base+0x38>
   12f84:	ldr	x8, [x8, #16]
   12f88:	add	x8, x8, x9, lsl #3
   12f8c:	ldur	x8, [x8, #-8]
   12f90:	cmp	x8, #0x10, lsl #12
   12f94:	cset	w0, cc  // cc = lo, ul, last
   12f98:	ret
   12f9c:	mov	w0, #0x1                   	// #1
   12fa0:	ret

0000000000012fa4 <__gmpf_get_si@@Base>:
   12fa4:	ldr	x9, [x0, #8]
   12fa8:	cmp	x9, #0x1
   12fac:	b.lt	12fe0 <__gmpf_get_si@@Base+0x3c>  // b.tstop
   12fb0:	ldrsw	x8, [x0, #4]
   12fb4:	cmp	x8, #0x0
   12fb8:	cneg	x10, x8, mi  // mi = first
   12fbc:	subs	x9, x10, x9
   12fc0:	b.ge	12fe8 <__gmpf_get_si@@Base+0x44>  // b.tcont
   12fc4:	mov	x9, xzr
   12fc8:	cmp	w8, #0x1
   12fcc:	b.ge	12ff8 <__gmpf_get_si@@Base+0x54>  // b.tcont
   12fd0:	sub	x8, x9, #0x1
   12fd4:	orr	x8, x8, #0x8000000000000000
   12fd8:	eor	x0, x8, #0x7fffffffffffffff
   12fdc:	ret
   12fe0:	mov	x0, xzr
   12fe4:	ret
   12fe8:	ldr	x10, [x0, #16]
   12fec:	ldr	x9, [x10, x9, lsl #3]
   12ff0:	cmp	w8, #0x1
   12ff4:	b.lt	12fd0 <__gmpf_get_si@@Base+0x2c>  // b.tstop
   12ff8:	and	x0, x9, #0x7fffffffffffffff
   12ffc:	ret

0000000000013000 <__gmpf_get_ui@@Base>:
   13000:	ldr	x8, [x0, #8]
   13004:	cmp	x8, #0x1
   13008:	b.lt	13020 <__gmpf_get_ui@@Base+0x20>  // b.tstop
   1300c:	ldrsw	x9, [x0, #4]
   13010:	cmp	x9, #0x0
   13014:	cneg	x9, x9, mi  // mi = first
   13018:	subs	x8, x9, x8
   1301c:	b.ge	13028 <__gmpf_get_ui@@Base+0x28>  // b.tcont
   13020:	mov	x0, xzr
   13024:	ret
   13028:	ldr	x9, [x0, #16]
   1302c:	ldr	x0, [x9, x8, lsl #3]
   13030:	ret

0000000000013034 <__gmpf_integer_p@@Base>:
   13034:	ldr	x8, [x0, #8]
   13038:	ldrsw	x9, [x0, #4]
   1303c:	cmp	x8, #0x0
   13040:	b.le	13074 <__gmpf_integer_p@@Base+0x40>
   13044:	ldr	x10, [x0, #16]
   13048:	cmp	x9, #0x0
   1304c:	cneg	x9, x9, mi  // mi = first
   13050:	ldr	x11, [x10]
   13054:	cbnz	x11, 13068 <__gmpf_integer_p@@Base+0x34>
   13058:	add	x10, x10, #0x8
   1305c:	ldr	x11, [x10], #8
   13060:	sub	x9, x9, #0x1
   13064:	cbz	x11, 1305c <__gmpf_integer_p@@Base+0x28>
   13068:	cmp	x9, x8
   1306c:	cset	w0, le
   13070:	ret
   13074:	cmp	w9, #0x0
   13078:	cset	w0, eq  // eq = none
   1307c:	ret

0000000000013080 <__gmpz_abs@@Base>:
   13080:	stp	x29, x30, [sp, #-48]!
   13084:	stp	x20, x19, [sp, #32]
   13088:	ldr	w8, [x1, #4]
   1308c:	mov	x19, x0
   13090:	str	x21, [sp, #16]
   13094:	mov	x29, sp
   13098:	cmp	w8, #0x0
   1309c:	cneg	w20, w8, mi  // mi = first
   130a0:	cmp	x1, x0
   130a4:	b.eq	130c8 <__gmpz_abs@@Base+0x48>  // b.none
   130a8:	ldrsw	x8, [x19]
   130ac:	mov	x21, x1
   130b0:	cmp	x20, x8
   130b4:	b.gt	130dc <__gmpz_abs@@Base+0x5c>
   130b8:	ldr	x0, [x19, #8]
   130bc:	ldr	x1, [x21, #8]
   130c0:	mov	x2, x20
   130c4:	bl	ca50 <__gmpn_copyi@plt>
   130c8:	str	w20, [x19, #4]
   130cc:	ldp	x20, x19, [sp, #32]
   130d0:	ldr	x21, [sp, #16]
   130d4:	ldp	x29, x30, [sp], #48
   130d8:	ret
   130dc:	mov	x0, x19
   130e0:	mov	x1, x20
   130e4:	bl	c080 <__gmpz_realloc@plt>
   130e8:	b	130bc <__gmpz_abs@@Base+0x3c>

00000000000130ec <__gmpz_add@@Base>:
   130ec:	stp	x29, x30, [sp, #-80]!
   130f0:	stp	x26, x25, [sp, #16]
   130f4:	stp	x24, x23, [sp, #32]
   130f8:	stp	x22, x21, [sp, #48]
   130fc:	stp	x20, x19, [sp, #64]
   13100:	ldrsw	x8, [x1, #4]
   13104:	ldrsw	x9, [x2, #4]
   13108:	ldrsw	x10, [x0]
   1310c:	mov	x19, x0
   13110:	cmp	x8, #0x0
   13114:	cneg	x11, x8, mi  // mi = first
   13118:	cmp	x9, #0x0
   1311c:	cneg	x12, x9, mi  // mi = first
   13120:	cmp	x11, x12
   13124:	csel	x20, x12, x11, lt  // lt = tstop
   13128:	csel	x23, x11, x12, lt  // lt = tstop
   1312c:	csel	x25, x8, x9, lt  // lt = tstop
   13130:	csel	x24, x9, x8, lt  // lt = tstop
   13134:	csel	x26, x1, x2, lt  // lt = tstop
   13138:	csel	x22, x2, x1, lt  // lt = tstop
   1313c:	cmp	x20, x10
   13140:	mov	x29, sp
   13144:	b.ge	133f0 <__gmpz_add@@Base+0x304>  // b.tcont
   13148:	ldr	x21, [x19, #8]
   1314c:	ldr	x22, [x22, #8]
   13150:	ldr	x2, [x26, #8]
   13154:	eor	x8, x24, x25
   13158:	tbnz	x8, #63, 1324c <__gmpz_add@@Base+0x160>
   1315c:	cbz	x23, 13198 <__gmpz_add@@Base+0xac>
   13160:	mov	x0, x21
   13164:	mov	x1, x22
   13168:	mov	x3, x23
   1316c:	bl	ca70 <__gmpn_add_n@plt>
   13170:	cbz	x0, 13198 <__gmpz_add@@Base+0xac>
   13174:	mov	w9, #0x1                   	// #1
   13178:	cmp	x23, x20
   1317c:	b.ge	13238 <__gmpz_add@@Base+0x14c>  // b.tcont
   13180:	lsl	x8, x23, #3
   13184:	ldr	x10, [x22, x8]
   13188:	add	x23, x23, #0x1
   1318c:	adds	x10, x10, #0x1
   13190:	str	x10, [x21, x8]
   13194:	b.cs	13178 <__gmpz_add@@Base+0x8c>  // b.hs, b.nlast
   13198:	cmp	x21, x22
   1319c:	mov	x9, xzr
   131a0:	b.eq	13238 <__gmpz_add@@Base+0x14c>  // b.none
   131a4:	subs	x8, x20, x23
   131a8:	b.le	13238 <__gmpz_add@@Base+0x14c>
   131ac:	cmp	x8, #0x4
   131b0:	b.cc	13214 <__gmpz_add@@Base+0x128>  // b.lo, b.ul, b.last
   131b4:	lsl	x10, x23, #3
   131b8:	lsl	x9, x20, #3
   131bc:	add	x11, x21, x10
   131c0:	add	x12, x22, x9
   131c4:	cmp	x11, x12
   131c8:	b.cs	131dc <__gmpz_add@@Base+0xf0>  // b.hs, b.nlast
   131cc:	add	x9, x21, x9
   131d0:	add	x11, x22, x10
   131d4:	cmp	x11, x9
   131d8:	b.cc	13214 <__gmpz_add@@Base+0x128>  // b.lo, b.ul, b.last
   131dc:	and	x9, x8, #0xfffffffffffffffc
   131e0:	add	x11, x10, #0x10
   131e4:	add	x23, x23, x9
   131e8:	add	x10, x22, x11
   131ec:	add	x11, x21, x11
   131f0:	mov	x12, x9
   131f4:	ldp	q0, q1, [x10, #-16]
   131f8:	add	x10, x10, #0x20
   131fc:	subs	x12, x12, #0x4
   13200:	stp	q0, q1, [x11, #-16]
   13204:	add	x11, x11, #0x20
   13208:	b.ne	131f4 <__gmpz_add@@Base+0x108>  // b.any
   1320c:	cmp	x8, x9
   13210:	b.eq	13234 <__gmpz_add@@Base+0x148>  // b.none
   13214:	lsl	x10, x23, #3
   13218:	sub	x8, x20, x23
   1321c:	add	x9, x21, x10
   13220:	add	x10, x22, x10
   13224:	ldr	x11, [x10], #8
   13228:	subs	x8, x8, #0x1
   1322c:	str	x11, [x9], #8
   13230:	b.ne	13224 <__gmpz_add@@Base+0x138>  // b.any
   13234:	mov	x9, xzr
   13238:	add	x8, x9, x20
   1323c:	cmp	x24, #0x0
   13240:	cneg	x8, x8, lt  // lt = tstop
   13244:	str	x9, [x21, x20, lsl #3]
   13248:	b	1339c <__gmpz_add@@Base+0x2b0>
   1324c:	cmp	x20, x23
   13250:	b.ne	132ac <__gmpz_add@@Base+0x1c0>  // b.any
   13254:	sub	x8, x20, #0x1
   13258:	add	x9, x8, #0x1
   1325c:	cmp	x9, #0x1
   13260:	b.lt	13280 <__gmpz_add@@Base+0x194>  // b.tstop
   13264:	lsl	x9, x8, #3
   13268:	ldr	x10, [x22, x9]
   1326c:	ldr	x9, [x2, x9]
   13270:	sub	x8, x8, #0x1
   13274:	cmp	x10, x9
   13278:	b.eq	13258 <__gmpz_add@@Base+0x16c>  // b.none
   1327c:	b.ls	133b8 <__gmpz_add@@Base+0x2cc>  // b.plast
   13280:	mov	x0, x21
   13284:	mov	x1, x22
   13288:	mov	x3, x20
   1328c:	bl	c2d0 <__gmpn_sub_n@plt>
   13290:	sub	x8, x21, #0x8
   13294:	mov	x9, x20
   13298:	subs	x20, x20, #0x1
   1329c:	b.lt	13394 <__gmpz_add@@Base+0x2a8>  // b.tstop
   132a0:	ldr	x10, [x8, x9, lsl #3]
   132a4:	cbz	x10, 13294 <__gmpz_add@@Base+0x1a8>
   132a8:	b	13394 <__gmpz_add@@Base+0x2a8>
   132ac:	cbz	x23, 132e4 <__gmpz_add@@Base+0x1f8>
   132b0:	mov	x0, x21
   132b4:	mov	x1, x22
   132b8:	mov	x3, x23
   132bc:	bl	c2d0 <__gmpn_sub_n@plt>
   132c0:	cbz	x0, 132e4 <__gmpz_add@@Base+0x1f8>
   132c4:	cmp	x23, x20
   132c8:	b.ge	1337c <__gmpz_add@@Base+0x290>  // b.tcont
   132cc:	lsl	x8, x23, #3
   132d0:	ldr	x9, [x22, x8]
   132d4:	add	x23, x23, #0x1
   132d8:	sub	x10, x9, #0x1
   132dc:	str	x10, [x21, x8]
   132e0:	cbz	x9, 132c4 <__gmpz_add@@Base+0x1d8>
   132e4:	cmp	x21, x22
   132e8:	b.eq	1337c <__gmpz_add@@Base+0x290>  // b.none
   132ec:	subs	x8, x20, x23
   132f0:	b.le	1337c <__gmpz_add@@Base+0x290>
   132f4:	cmp	x8, #0x4
   132f8:	b.cc	1335c <__gmpz_add@@Base+0x270>  // b.lo, b.ul, b.last
   132fc:	lsl	x10, x23, #3
   13300:	lsl	x9, x20, #3
   13304:	add	x11, x21, x10
   13308:	add	x12, x22, x9
   1330c:	cmp	x11, x12
   13310:	b.cs	13324 <__gmpz_add@@Base+0x238>  // b.hs, b.nlast
   13314:	add	x9, x21, x9
   13318:	add	x11, x22, x10
   1331c:	cmp	x11, x9
   13320:	b.cc	1335c <__gmpz_add@@Base+0x270>  // b.lo, b.ul, b.last
   13324:	and	x9, x8, #0xfffffffffffffffc
   13328:	add	x11, x10, #0x10
   1332c:	add	x23, x23, x9
   13330:	add	x10, x22, x11
   13334:	add	x11, x21, x11
   13338:	mov	x12, x9
   1333c:	ldp	q0, q1, [x10, #-16]
   13340:	add	x10, x10, #0x20
   13344:	subs	x12, x12, #0x4
   13348:	stp	q0, q1, [x11, #-16]
   1334c:	add	x11, x11, #0x20
   13350:	b.ne	1333c <__gmpz_add@@Base+0x250>  // b.any
   13354:	cmp	x8, x9
   13358:	b.eq	1337c <__gmpz_add@@Base+0x290>  // b.none
   1335c:	lsl	x10, x23, #3
   13360:	sub	x8, x20, x23
   13364:	add	x9, x21, x10
   13368:	add	x10, x22, x10
   1336c:	ldr	x11, [x10], #8
   13370:	subs	x8, x8, #0x1
   13374:	str	x11, [x9], #8
   13378:	b.ne	1336c <__gmpz_add@@Base+0x280>  // b.any
   1337c:	sub	x8, x21, #0x8
   13380:	mov	x9, x20
   13384:	subs	x20, x20, #0x1
   13388:	b.lt	13394 <__gmpz_add@@Base+0x2a8>  // b.tstop
   1338c:	ldr	x10, [x8, x9, lsl #3]
   13390:	cbz	x10, 13380 <__gmpz_add@@Base+0x294>
   13394:	cmp	x24, #0x0
   13398:	cneg	x8, x9, lt  // lt = tstop
   1339c:	str	w8, [x19, #4]
   133a0:	ldp	x20, x19, [sp, #64]
   133a4:	ldp	x22, x21, [sp, #48]
   133a8:	ldp	x24, x23, [sp, #32]
   133ac:	ldp	x26, x25, [sp, #16]
   133b0:	ldp	x29, x30, [sp], #80
   133b4:	ret
   133b8:	mov	x0, x21
   133bc:	mov	x1, x2
   133c0:	mov	x2, x22
   133c4:	mov	x3, x20
   133c8:	bl	c2d0 <__gmpn_sub_n@plt>
   133cc:	sub	x8, x21, #0x8
   133d0:	mov	x9, x20
   133d4:	subs	x20, x20, #0x1
   133d8:	b.lt	133e4 <__gmpz_add@@Base+0x2f8>  // b.tstop
   133dc:	ldr	x10, [x8, x9, lsl #3]
   133e0:	cbz	x10, 133d0 <__gmpz_add@@Base+0x2e4>
   133e4:	cmp	x24, #0x0
   133e8:	cneg	x8, x9, ge  // ge = tcont
   133ec:	b	1339c <__gmpz_add@@Base+0x2b0>
   133f0:	add	x1, x20, #0x1
   133f4:	mov	x0, x19
   133f8:	bl	c080 <__gmpz_realloc@plt>
   133fc:	mov	x21, x0
   13400:	b	1314c <__gmpz_add@@Base+0x60>

0000000000013404 <__gmpz_add_ui@@Base>:
   13404:	stp	x29, x30, [sp, #-64]!
   13408:	stp	x22, x21, [sp, #32]
   1340c:	stp	x20, x19, [sp, #48]
   13410:	str	x23, [sp, #16]
   13414:	ldrsw	x23, [x1, #4]
   13418:	mov	x20, x2
   1341c:	mov	x19, x0
   13420:	mov	x29, sp
   13424:	cbz	w23, 13548 <__gmpz_add_ui@@Base+0x144>
   13428:	ldrsw	x8, [x19]
   1342c:	cmp	w23, #0x0
   13430:	cneg	x22, x23, lt  // lt = tstop
   13434:	mov	x21, x1
   13438:	cmp	x22, x8
   1343c:	b.ge	13618 <__gmpz_add_ui@@Base+0x214>  // b.tcont
   13440:	ldr	x0, [x19, #8]
   13444:	ldr	x8, [x21, #8]
   13448:	tbnz	w23, #31, 1362c <__gmpz_add_ui@@Base+0x228>
   1344c:	ldr	x9, [x8]
   13450:	adds	x9, x9, x20
   13454:	str	x9, [x0]
   13458:	b.cc	13568 <__gmpz_add_ui@@Base+0x164>  // b.lo, b.ul, b.last
   1345c:	mov	x11, xzr
   13460:	sub	x10, x22, #0x1
   13464:	mov	w12, #0x1                   	// #1
   13468:	mov	w9, #0x1                   	// #1
   1346c:	cmp	x9, x22
   13470:	b.ge	135d4 <__gmpz_add_ui@@Base+0x1d0>  // b.tcont
   13474:	add	x13, x8, x11
   13478:	ldr	x13, [x13, #8]
   1347c:	add	x14, x0, x11
   13480:	add	x9, x9, #0x1
   13484:	add	x11, x11, #0x8
   13488:	adds	x13, x13, #0x1
   1348c:	sub	x10, x10, #0x1
   13490:	str	x13, [x14, #8]
   13494:	b.cs	1346c <__gmpz_add_ui@@Base+0x68>  // b.hs, b.nlast
   13498:	cmp	x8, x0
   1349c:	mov	x12, xzr
   134a0:	b.eq	135d4 <__gmpz_add_ui@@Base+0x1d0>  // b.none
   134a4:	subs	x13, x22, x9
   134a8:	b.le	135d4 <__gmpz_add_ui@@Base+0x1d0>
   134ac:	cmp	x13, #0x4
   134b0:	b.cc	13524 <__gmpz_add_ui@@Base+0x120>  // b.lo, b.ul, b.last
   134b4:	add	x14, x0, x11
   134b8:	lsl	x12, x22, #3
   134bc:	add	x14, x14, #0x8
   134c0:	add	x15, x8, x12
   134c4:	cmp	x14, x15
   134c8:	b.cs	134e0 <__gmpz_add_ui@@Base+0xdc>  // b.hs, b.nlast
   134cc:	add	x14, x8, x11
   134d0:	add	x12, x0, x12
   134d4:	add	x14, x14, #0x8
   134d8:	cmp	x14, x12
   134dc:	b.cc	13524 <__gmpz_add_ui@@Base+0x120>  // b.lo, b.ul, b.last
   134e0:	sub	x14, x22, x9
   134e4:	add	x12, x0, x11
   134e8:	add	x15, x8, x11
   134ec:	and	x16, x10, #0xfffffffffffffffc
   134f0:	and	x11, x13, #0xfffffffffffffffc
   134f4:	add	x10, x12, #0x18
   134f8:	add	x12, x15, #0x18
   134fc:	add	x9, x16, x9
   13500:	and	x14, x14, #0xfffffffffffffffc
   13504:	ldp	q0, q1, [x12, #-16]
   13508:	add	x12, x12, #0x20
   1350c:	subs	x14, x14, #0x4
   13510:	stp	q0, q1, [x10, #-16]
   13514:	add	x10, x10, #0x20
   13518:	b.ne	13504 <__gmpz_add_ui@@Base+0x100>  // b.any
   1351c:	cmp	x13, x11
   13520:	b.eq	135d0 <__gmpz_add_ui@@Base+0x1cc>  // b.none
   13524:	lsl	x11, x9, #3
   13528:	sub	x10, x22, x9
   1352c:	add	x9, x0, x11
   13530:	add	x8, x8, x11
   13534:	ldr	x11, [x8], #8
   13538:	subs	x10, x10, #0x1
   1353c:	str	x11, [x9], #8
   13540:	b.ne	13534 <__gmpz_add_ui@@Base+0x130>  // b.any
   13544:	b	135d0 <__gmpz_add_ui@@Base+0x1cc>
   13548:	ldr	w8, [x19]
   1354c:	cmp	w8, #0x0
   13550:	b.le	13800 <__gmpz_add_ui@@Base+0x3fc>
   13554:	ldr	x0, [x19, #8]
   13558:	cmp	x20, #0x0
   1355c:	str	x20, [x0]
   13560:	cset	w8, ne  // ne = any
   13564:	b	137b0 <__gmpz_add_ui@@Base+0x3ac>
   13568:	cmp	x22, #0x2
   1356c:	mov	x12, xzr
   13570:	b.lt	135d4 <__gmpz_add_ui@@Base+0x1d0>  // b.tstop
   13574:	cmp	x8, x0
   13578:	b.eq	135d4 <__gmpz_add_ui@@Base+0x1d0>  // b.none
   1357c:	sub	x9, x22, #0x1
   13580:	cmp	x9, #0x4
   13584:	b.cc	135ac <__gmpz_add_ui@@Base+0x1a8>  // b.lo, b.ul, b.last
   13588:	lsl	x10, x22, #3
   1358c:	add	x11, x0, #0x8
   13590:	add	x12, x8, x10
   13594:	cmp	x11, x12
   13598:	b.cs	135e0 <__gmpz_add_ui@@Base+0x1dc>  // b.hs, b.nlast
   1359c:	add	x10, x0, x10
   135a0:	add	x11, x8, #0x8
   135a4:	cmp	x11, x10
   135a8:	b.cs	135e0 <__gmpz_add_ui@@Base+0x1dc>  // b.hs, b.nlast
   135ac:	mov	w10, #0x1                   	// #1
   135b0:	lsl	x11, x10, #3
   135b4:	sub	x9, x22, x10
   135b8:	add	x10, x0, x11
   135bc:	add	x8, x8, x11
   135c0:	ldr	x11, [x8], #8
   135c4:	subs	x9, x9, #0x1
   135c8:	str	x11, [x10], #8
   135cc:	b.ne	135c0 <__gmpz_add_ui@@Base+0x1bc>  // b.any
   135d0:	mov	x12, xzr
   135d4:	str	x12, [x0, x22, lsl #3]
   135d8:	add	x8, x12, x22
   135dc:	b	137b0 <__gmpz_add_ui@@Base+0x3ac>
   135e0:	and	x11, x9, #0xfffffffffffffffc
   135e4:	add	x12, x8, #0x18
   135e8:	orr	x10, x11, #0x1
   135ec:	add	x13, x0, #0x18
   135f0:	mov	x14, x11
   135f4:	ldp	q0, q1, [x12, #-16]
   135f8:	add	x12, x12, #0x20
   135fc:	subs	x14, x14, #0x4
   13600:	stp	q0, q1, [x13, #-16]
   13604:	add	x13, x13, #0x20
   13608:	b.ne	135f4 <__gmpz_add_ui@@Base+0x1f0>  // b.any
   1360c:	cmp	x9, x11
   13610:	b.eq	135d0 <__gmpz_add_ui@@Base+0x1cc>  // b.none
   13614:	b	135b0 <__gmpz_add_ui@@Base+0x1ac>
   13618:	add	x1, x22, #0x1
   1361c:	mov	x0, x19
   13620:	bl	c080 <__gmpz_realloc@plt>
   13624:	ldr	x8, [x21, #8]
   13628:	tbz	w23, #31, 1344c <__gmpz_add_ui@@Base+0x48>
   1362c:	ldr	x10, [x8]
   13630:	subs	x9, x22, #0x1
   13634:	b.ne	13650 <__gmpz_add_ui@@Base+0x24c>  // b.any
   13638:	cmp	x10, x20
   1363c:	b.cs	13650 <__gmpz_add_ui@@Base+0x24c>  // b.hs, b.nlast
   13640:	sub	x8, x20, x10
   13644:	str	x8, [x0]
   13648:	mov	w8, #0x1                   	// #1
   1364c:	b	137b0 <__gmpz_add_ui@@Base+0x3ac>
   13650:	subs	x10, x10, x20
   13654:	str	x10, [x0]
   13658:	b.cs	1373c <__gmpz_add_ui@@Base+0x338>  // b.hs, b.nlast
   1365c:	mov	x11, xzr
   13660:	mov	w10, #0x1                   	// #1
   13664:	cmp	x10, x22
   13668:	b.ge	1379c <__gmpz_add_ui@@Base+0x398>  // b.tcont
   1366c:	add	x12, x8, x11
   13670:	ldr	x12, [x12, #8]
   13674:	add	x13, x0, x11
   13678:	add	x10, x10, #0x1
   1367c:	add	x11, x11, #0x8
   13680:	sub	x14, x12, #0x1
   13684:	sub	x9, x9, #0x1
   13688:	str	x14, [x13, #8]
   1368c:	cbz	x12, 13664 <__gmpz_add_ui@@Base+0x260>
   13690:	cmp	x8, x0
   13694:	b.eq	1379c <__gmpz_add_ui@@Base+0x398>  // b.none
   13698:	subs	x12, x22, x10
   1369c:	b.le	1379c <__gmpz_add_ui@@Base+0x398>
   136a0:	cmp	x12, #0x4
   136a4:	b.cc	13718 <__gmpz_add_ui@@Base+0x314>  // b.lo, b.ul, b.last
   136a8:	add	x14, x0, x11
   136ac:	lsl	x13, x22, #3
   136b0:	add	x14, x14, #0x8
   136b4:	add	x15, x8, x13
   136b8:	cmp	x14, x15
   136bc:	b.cs	136d4 <__gmpz_add_ui@@Base+0x2d0>  // b.hs, b.nlast
   136c0:	add	x14, x8, x11
   136c4:	add	x13, x0, x13
   136c8:	add	x14, x14, #0x8
   136cc:	cmp	x14, x13
   136d0:	b.cc	13718 <__gmpz_add_ui@@Base+0x314>  // b.lo, b.ul, b.last
   136d4:	sub	x14, x22, x10
   136d8:	add	x13, x0, x11
   136dc:	add	x15, x8, x11
   136e0:	and	x16, x9, #0xfffffffffffffffc
   136e4:	and	x11, x12, #0xfffffffffffffffc
   136e8:	add	x9, x13, #0x18
   136ec:	add	x13, x15, #0x18
   136f0:	add	x10, x16, x10
   136f4:	and	x14, x14, #0xfffffffffffffffc
   136f8:	ldp	q0, q1, [x13, #-16]
   136fc:	add	x13, x13, #0x20
   13700:	subs	x14, x14, #0x4
   13704:	stp	q0, q1, [x9, #-16]
   13708:	add	x9, x9, #0x20
   1370c:	b.ne	136f8 <__gmpz_add_ui@@Base+0x2f4>  // b.any
   13710:	cmp	x12, x11
   13714:	b.eq	1379c <__gmpz_add_ui@@Base+0x398>  // b.none
   13718:	lsl	x11, x10, #3
   1371c:	sub	x9, x22, x10
   13720:	add	x10, x0, x11
   13724:	add	x8, x8, x11
   13728:	ldr	x11, [x8], #8
   1372c:	subs	x9, x9, #0x1
   13730:	str	x11, [x10], #8
   13734:	b.ne	13728 <__gmpz_add_ui@@Base+0x324>  // b.any
   13738:	b	1379c <__gmpz_add_ui@@Base+0x398>
   1373c:	cmp	x22, #0x2
   13740:	b.lt	1379c <__gmpz_add_ui@@Base+0x398>  // b.tstop
   13744:	cmp	x8, x0
   13748:	b.eq	1379c <__gmpz_add_ui@@Base+0x398>  // b.none
   1374c:	cmp	x9, #0x4
   13750:	b.cc	13778 <__gmpz_add_ui@@Base+0x374>  // b.lo, b.ul, b.last
   13754:	lsl	x10, x22, #3
   13758:	add	x11, x0, #0x8
   1375c:	add	x12, x8, x10
   13760:	cmp	x11, x12
   13764:	b.cs	137c8 <__gmpz_add_ui@@Base+0x3c4>  // b.hs, b.nlast
   13768:	add	x10, x0, x10
   1376c:	add	x11, x8, #0x8
   13770:	cmp	x11, x10
   13774:	b.cs	137c8 <__gmpz_add_ui@@Base+0x3c4>  // b.hs, b.nlast
   13778:	mov	w10, #0x1                   	// #1
   1377c:	lsl	x11, x10, #3
   13780:	sub	x9, x22, x10
   13784:	add	x10, x0, x11
   13788:	add	x8, x8, x11
   1378c:	ldr	x11, [x8], #8
   13790:	subs	x9, x9, #0x1
   13794:	str	x11, [x10], #8
   13798:	b.ne	1378c <__gmpz_add_ui@@Base+0x388>  // b.any
   1379c:	add	x8, x0, x22, lsl #3
   137a0:	ldur	x8, [x8, #-8]
   137a4:	cmp	x8, #0x0
   137a8:	cset	w8, eq  // eq = none
   137ac:	sub	x8, x8, x22
   137b0:	str	w8, [x19, #4]
   137b4:	ldp	x20, x19, [sp, #48]
   137b8:	ldp	x22, x21, [sp, #32]
   137bc:	ldr	x23, [sp, #16]
   137c0:	ldp	x29, x30, [sp], #64
   137c4:	ret
   137c8:	and	x11, x9, #0xfffffffffffffffc
   137cc:	add	x12, x8, #0x18
   137d0:	orr	x10, x11, #0x1
   137d4:	add	x13, x0, #0x18
   137d8:	mov	x14, x11
   137dc:	ldp	q0, q1, [x12, #-16]
   137e0:	add	x12, x12, #0x20
   137e4:	subs	x14, x14, #0x4
   137e8:	stp	q0, q1, [x13, #-16]
   137ec:	add	x13, x13, #0x20
   137f0:	b.ne	137dc <__gmpz_add_ui@@Base+0x3d8>  // b.any
   137f4:	cmp	x9, x11
   137f8:	b.eq	1379c <__gmpz_add_ui@@Base+0x398>  // b.none
   137fc:	b	1377c <__gmpz_add_ui@@Base+0x378>
   13800:	mov	w1, #0x1                   	// #1
   13804:	mov	x0, x19
   13808:	bl	c080 <__gmpz_realloc@plt>
   1380c:	b	13558 <__gmpz_add_ui@@Base+0x154>

0000000000013810 <__gmpz_addmul@@Base>:
   13810:	mov	x3, xzr
   13814:	b	13818 <__gmpz_addmul@@Base+0x8>
   13818:	stp	x29, x30, [sp, #-96]!
   1381c:	stp	x28, x27, [sp, #16]
   13820:	stp	x26, x25, [sp, #32]
   13824:	stp	x24, x23, [sp, #48]
   13828:	stp	x22, x21, [sp, #64]
   1382c:	stp	x20, x19, [sp, #80]
   13830:	mov	x29, sp
   13834:	sub	sp, sp, #0x10
   13838:	ldrsw	x8, [x1, #4]
   1383c:	cbz	w8, 13c14 <__gmpz_addmul@@Base+0x404>
   13840:	ldr	w9, [x2, #4]
   13844:	cbz	w9, 13c14 <__gmpz_addmul@@Base+0x404>
   13848:	sxtw	x9, w9
   1384c:	cmp	x9, #0x0
   13850:	cneg	x10, x9, mi  // mi = first
   13854:	cmp	x8, #0x0
   13858:	cneg	x11, x8, mi  // mi = first
   1385c:	cmp	x10, x11
   13860:	csel	x10, x8, x9, gt
   13864:	csel	x8, x9, x8, gt
   13868:	csel	x28, x1, x2, gt
   1386c:	csel	x23, x2, x1, gt
   13870:	cmp	x10, #0x0
   13874:	cneg	x22, x10, mi  // mi = first
   13878:	mov	x19, x0
   1387c:	cmp	x22, #0x1
   13880:	eor	x3, x10, x3
   13884:	b.ne	138a0 <__gmpz_addmul@@Base+0x90>  // b.any
   13888:	ldr	x8, [x28, #8]
   1388c:	mov	x0, x19
   13890:	mov	x1, x23
   13894:	ldr	x2, [x8]
   13898:	bl	cf80 <__gmpz_aorsmul_1@plt>
   1389c:	b	13c14 <__gmpz_addmul@@Base+0x404>
   138a0:	ldpsw	x10, x11, [x19]
   138a4:	cmp	x8, #0x0
   138a8:	cneg	x24, x8, mi  // mi = first
   138ac:	add	x27, x24, x22
   138b0:	cmp	x11, #0x0
   138b4:	cneg	x26, x11, mi  // mi = first
   138b8:	cmp	x26, x27
   138bc:	csel	x9, x26, x27, gt
   138c0:	cmp	x9, x10
   138c4:	eor	x21, x3, x8
   138c8:	b.ge	13bc0 <__gmpz_addmul@@Base+0x3b0>  // b.tcont
   138cc:	ldr	x20, [x19, #8]
   138d0:	eor	x25, x21, x11
   138d4:	cbz	w11, 13be0 <__gmpz_addmul@@Base+0x3d0>
   138d8:	cmp	x27, #0xfe0
   138dc:	lsl	x1, x27, #3
   138e0:	stp	x11, xzr, [x29, #-16]
   138e4:	b.hi	13c34 <__gmpz_addmul@@Base+0x424>  // b.pmore
   138e8:	add	x9, x1, #0xf
   138ec:	mov	x8, sp
   138f0:	and	x9, x9, #0xfffffffffffffff0
   138f4:	sub	x21, x8, x9
   138f8:	mov	sp, x21
   138fc:	ldr	x1, [x23, #8]
   13900:	ldr	x3, [x28, #8]
   13904:	mov	x0, x21
   13908:	mov	x2, x24
   1390c:	mov	x4, x22
   13910:	bl	ccd0 <__gmpn_mul@plt>
   13914:	cmp	x0, #0x0
   13918:	cset	w8, eq  // eq = none
   1391c:	sub	x8, x27, x8
   13920:	cmp	x26, x8
   13924:	tbnz	x25, #63, 13a30 <__gmpz_addmul@@Base+0x220>
   13928:	csel	x23, x26, x8, lt  // lt = tstop
   1392c:	csel	x24, x8, x26, lt  // lt = tstop
   13930:	csel	x22, x21, x20, lt  // lt = tstop
   13934:	cbz	x23, 13980 <__gmpz_addmul@@Base+0x170>
   13938:	cmp	x26, x8
   1393c:	csel	x2, x20, x21, lt  // lt = tstop
   13940:	mov	x0, x20
   13944:	mov	x1, x22
   13948:	mov	x3, x23
   1394c:	bl	ca70 <__gmpn_add_n@plt>
   13950:	cbz	x0, 13980 <__gmpz_addmul@@Base+0x170>
   13954:	ldur	x13, [x29, #-16]
   13958:	mov	w9, #0x1                   	// #1
   1395c:	cmp	x23, x24
   13960:	b.ge	13a24 <__gmpz_addmul@@Base+0x214>  // b.tcont
   13964:	lsl	x8, x23, #3
   13968:	ldr	x10, [x22, x8]
   1396c:	add	x23, x23, #0x1
   13970:	adds	x10, x10, #0x1
   13974:	str	x10, [x20, x8]
   13978:	b.cs	1395c <__gmpz_addmul@@Base+0x14c>  // b.hs, b.nlast
   1397c:	b	13984 <__gmpz_addmul@@Base+0x174>
   13980:	ldur	x13, [x29, #-16]
   13984:	cmp	x20, x22
   13988:	mov	x9, xzr
   1398c:	b.eq	13a24 <__gmpz_addmul@@Base+0x214>  // b.none
   13990:	subs	x8, x24, x23
   13994:	b.le	13a24 <__gmpz_addmul@@Base+0x214>
   13998:	cmp	x8, #0x4
   1399c:	b.cc	13a00 <__gmpz_addmul@@Base+0x1f0>  // b.lo, b.ul, b.last
   139a0:	lsl	x10, x23, #3
   139a4:	lsl	x9, x24, #3
   139a8:	add	x11, x20, x10
   139ac:	add	x12, x22, x9
   139b0:	cmp	x11, x12
   139b4:	b.cs	139c8 <__gmpz_addmul@@Base+0x1b8>  // b.hs, b.nlast
   139b8:	add	x9, x20, x9
   139bc:	add	x11, x22, x10
   139c0:	cmp	x11, x9
   139c4:	b.cc	13a00 <__gmpz_addmul@@Base+0x1f0>  // b.lo, b.ul, b.last
   139c8:	and	x9, x8, #0xfffffffffffffffc
   139cc:	add	x11, x10, #0x10
   139d0:	add	x23, x23, x9
   139d4:	add	x10, x22, x11
   139d8:	add	x11, x20, x11
   139dc:	mov	x12, x9
   139e0:	ldp	q0, q1, [x10, #-16]
   139e4:	add	x10, x10, #0x20
   139e8:	subs	x12, x12, #0x4
   139ec:	stp	q0, q1, [x11, #-16]
   139f0:	add	x11, x11, #0x20
   139f4:	b.ne	139e0 <__gmpz_addmul@@Base+0x1d0>  // b.any
   139f8:	cmp	x8, x9
   139fc:	b.eq	13a20 <__gmpz_addmul@@Base+0x210>  // b.none
   13a00:	lsl	x10, x23, #3
   13a04:	sub	x8, x24, x23
   13a08:	add	x9, x20, x10
   13a0c:	add	x10, x22, x10
   13a10:	ldr	x11, [x10], #8
   13a14:	subs	x8, x8, #0x1
   13a18:	str	x11, [x9], #8
   13a1c:	b.ne	13a10 <__gmpz_addmul@@Base+0x200>  // b.any
   13a20:	mov	x9, xzr
   13a24:	str	x9, [x20, x24, lsl #3]
   13a28:	add	x8, x9, x24
   13a2c:	b	13b8c <__gmpz_addmul@@Base+0x37c>
   13a30:	ldur	x13, [x29, #-16]
   13a34:	neg	x9, x13
   13a38:	b.ge	13a54 <__gmpz_addmul@@Base+0x244>  // b.tcont
   13a3c:	mov	x22, x26
   13a40:	mov	x13, x9
   13a44:	mov	x2, x20
   13a48:	mov	x26, x8
   13a4c:	cbnz	x22, 13aa0 <__gmpz_addmul@@Base+0x290>
   13a50:	b	13adc <__gmpz_addmul@@Base+0x2cc>
   13a54:	b.ne	13a90 <__gmpz_addmul@@Base+0x280>  // b.any
   13a58:	sub	x8, x21, #0x8
   13a5c:	mov	x10, x26
   13a60:	subs	x11, x10, #0x1
   13a64:	b.lt	13a88 <__gmpz_addmul@@Base+0x278>  // b.tstop
   13a68:	lsl	x10, x10, #3
   13a6c:	add	x12, x20, x10
   13a70:	ldur	x12, [x12, #-8]
   13a74:	ldr	x10, [x8, x10]
   13a78:	cmp	x12, x10
   13a7c:	mov	x10, x11
   13a80:	b.eq	13a60 <__gmpz_addmul@@Base+0x250>  // b.none
   13a84:	b.ls	13bac <__gmpz_addmul@@Base+0x39c>  // b.plast
   13a88:	mov	x22, x26
   13a8c:	b	13a94 <__gmpz_addmul@@Base+0x284>
   13a90:	mov	x22, x8
   13a94:	mov	x2, x21
   13a98:	mov	x21, x20
   13a9c:	cbz	x22, 13adc <__gmpz_addmul@@Base+0x2cc>
   13aa0:	mov	x0, x20
   13aa4:	mov	x1, x21
   13aa8:	mov	x3, x22
   13aac:	mov	x23, x13
   13ab0:	bl	c2d0 <__gmpn_sub_n@plt>
   13ab4:	mov	x13, x23
   13ab8:	cbz	x0, 13adc <__gmpz_addmul@@Base+0x2cc>
   13abc:	cmp	x22, x26
   13ac0:	b.ge	13b74 <__gmpz_addmul@@Base+0x364>  // b.tcont
   13ac4:	lsl	x8, x22, #3
   13ac8:	ldr	x9, [x21, x8]
   13acc:	add	x22, x22, #0x1
   13ad0:	sub	x10, x9, #0x1
   13ad4:	str	x10, [x20, x8]
   13ad8:	cbz	x9, 13abc <__gmpz_addmul@@Base+0x2ac>
   13adc:	cmp	x20, x21
   13ae0:	b.eq	13b74 <__gmpz_addmul@@Base+0x364>  // b.none
   13ae4:	subs	x8, x26, x22
   13ae8:	b.le	13b74 <__gmpz_addmul@@Base+0x364>
   13aec:	cmp	x8, #0x4
   13af0:	b.cc	13b54 <__gmpz_addmul@@Base+0x344>  // b.lo, b.ul, b.last
   13af4:	lsl	x10, x22, #3
   13af8:	lsl	x9, x26, #3
   13afc:	add	x11, x20, x10
   13b00:	add	x12, x21, x9
   13b04:	cmp	x11, x12
   13b08:	b.cs	13b1c <__gmpz_addmul@@Base+0x30c>  // b.hs, b.nlast
   13b0c:	add	x9, x20, x9
   13b10:	add	x11, x21, x10
   13b14:	cmp	x11, x9
   13b18:	b.cc	13b54 <__gmpz_addmul@@Base+0x344>  // b.lo, b.ul, b.last
   13b1c:	and	x9, x8, #0xfffffffffffffffc
   13b20:	add	x11, x10, #0x10
   13b24:	add	x22, x22, x9
   13b28:	add	x10, x21, x11
   13b2c:	add	x11, x20, x11
   13b30:	mov	x12, x9
   13b34:	ldp	q0, q1, [x10, #-16]
   13b38:	add	x10, x10, #0x20
   13b3c:	subs	x12, x12, #0x4
   13b40:	stp	q0, q1, [x11, #-16]
   13b44:	add	x11, x11, #0x20
   13b48:	b.ne	13b34 <__gmpz_addmul@@Base+0x324>  // b.any
   13b4c:	cmp	x8, x9
   13b50:	b.eq	13b74 <__gmpz_addmul@@Base+0x364>  // b.none
   13b54:	lsl	x10, x22, #3
   13b58:	sub	x8, x26, x22
   13b5c:	add	x9, x20, x10
   13b60:	add	x10, x21, x10
   13b64:	ldr	x11, [x10], #8
   13b68:	subs	x8, x8, #0x1
   13b6c:	str	x11, [x9], #8
   13b70:	b.ne	13b64 <__gmpz_addmul@@Base+0x354>  // b.any
   13b74:	sub	x9, x20, #0x8
   13b78:	mov	x8, x26
   13b7c:	subs	x26, x26, #0x1
   13b80:	b.lt	13b8c <__gmpz_addmul@@Base+0x37c>  // b.tstop
   13b84:	ldr	x10, [x9, x8, lsl #3]
   13b88:	cbz	x10, 13b78 <__gmpz_addmul@@Base+0x368>
   13b8c:	neg	w9, w8
   13b90:	cmp	x13, #0x0
   13b94:	csel	x8, x8, x9, ge  // ge = tcont
   13b98:	str	w8, [x19, #4]
   13b9c:	ldur	x0, [x29, #-8]
   13ba0:	cbz	x0, 13c14 <__gmpz_addmul@@Base+0x404>
   13ba4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   13ba8:	b	13c14 <__gmpz_addmul@@Base+0x404>
   13bac:	mov	x22, x26
   13bb0:	mov	x13, x9
   13bb4:	mov	x2, x20
   13bb8:	cbnz	x22, 13aa0 <__gmpz_addmul@@Base+0x290>
   13bbc:	b	13adc <__gmpz_addmul@@Base+0x2cc>
   13bc0:	add	x1, x9, #0x1
   13bc4:	mov	x0, x19
   13bc8:	mov	x20, x11
   13bcc:	bl	c080 <__gmpz_realloc@plt>
   13bd0:	mov	x11, x20
   13bd4:	mov	x20, x0
   13bd8:	eor	x25, x21, x11
   13bdc:	cbnz	w11, 138d8 <__gmpz_addmul@@Base+0xc8>
   13be0:	ldr	x1, [x23, #8]
   13be4:	ldr	x3, [x28, #8]
   13be8:	mov	x0, x20
   13bec:	mov	x2, x24
   13bf0:	mov	x4, x22
   13bf4:	bl	ccd0 <__gmpn_mul@plt>
   13bf8:	cmp	x0, #0x0
   13bfc:	cset	w8, eq  // eq = none
   13c00:	sub	x8, x27, x8
   13c04:	neg	w9, w8
   13c08:	cmp	x25, #0x0
   13c0c:	csel	x8, x8, x9, ge  // ge = tcont
   13c10:	str	w8, [x19, #4]
   13c14:	mov	sp, x29
   13c18:	ldp	x20, x19, [sp, #80]
   13c1c:	ldp	x22, x21, [sp, #64]
   13c20:	ldp	x24, x23, [sp, #48]
   13c24:	ldp	x26, x25, [sp, #32]
   13c28:	ldp	x28, x27, [sp, #16]
   13c2c:	ldp	x29, x30, [sp], #96
   13c30:	ret
   13c34:	sub	x0, x29, #0x8
   13c38:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   13c3c:	mov	x21, x0
   13c40:	b	138fc <__gmpz_addmul@@Base+0xec>

0000000000013c44 <__gmpz_submul@@Base>:
   13c44:	mov	x3, #0xffffffffffffffff    	// #-1
   13c48:	b	13818 <__gmpz_addmul@@Base+0x8>

0000000000013c4c <__gmpz_aorsmul_1@@Base>:
   13c4c:	sub	sp, sp, #0x70
   13c50:	stp	x29, x30, [sp, #16]
   13c54:	stp	x28, x27, [sp, #32]
   13c58:	stp	x26, x25, [sp, #48]
   13c5c:	stp	x24, x23, [sp, #64]
   13c60:	stp	x22, x21, [sp, #80]
   13c64:	stp	x20, x19, [sp, #96]
   13c68:	add	x29, sp, #0x10
   13c6c:	cbz	x2, 13f30 <__gmpz_aorsmul_1@@Base+0x2e4>
   13c70:	ldr	w8, [x1, #4]
   13c74:	mov	x26, x1
   13c78:	cbz	w8, 13f30 <__gmpz_aorsmul_1@@Base+0x2e4>
   13c7c:	ldrsw	x25, [x0, #4]
   13c80:	sxtw	x8, w8
   13c84:	cmp	x8, #0x0
   13c88:	mov	x22, x2
   13c8c:	mov	x27, x0
   13c90:	eor	x21, x8, x3
   13c94:	cneg	x24, x8, mi  // mi = first
   13c98:	cbz	w25, 13d18 <__gmpz_aorsmul_1@@Base+0xcc>
   13c9c:	cmp	x25, #0x0
   13ca0:	ldrsw	x8, [x27]
   13ca4:	cneg	x23, x25, mi  // mi = first
   13ca8:	cmp	x24, x23
   13cac:	csel	x20, x23, x24, lt  // lt = tstop
   13cb0:	eor	x19, x21, x25
   13cb4:	cmp	x20, x8
   13cb8:	add	x8, x20, #0x1
   13cbc:	str	x27, [sp, #8]
   13cc0:	b.ge	13f50 <__gmpz_aorsmul_1@@Base+0x304>  // b.tcont
   13cc4:	ldr	x21, [x27, #8]
   13cc8:	ldr	x26, [x26, #8]
   13ccc:	subs	x27, x24, x23
   13cd0:	csel	x28, x23, x24, gt
   13cd4:	tbnz	x19, #63, 13d50 <__gmpz_aorsmul_1@@Base+0x104>
   13cd8:	mov	x0, x21
   13cdc:	mov	x1, x26
   13ce0:	mov	x2, x28
   13ce4:	mov	x3, x22
   13ce8:	bl	d400 <__gmpn_addmul_1@plt>
   13cec:	mov	x4, x0
   13cf0:	cmp	x27, #0x1
   13cf4:	add	x21, x21, x28, lsl #3
   13cf8:	b.lt	13dcc <__gmpz_aorsmul_1@@Base+0x180>  // b.tstop
   13cfc:	add	x1, x26, x28, lsl #3
   13d00:	mov	x0, x21
   13d04:	mov	x2, x27
   13d08:	mov	x3, x22
   13d0c:	bl	d240 <__gmpn_mul_1c@plt>
   13d10:	mov	x4, x0
   13d14:	b	13e9c <__gmpz_aorsmul_1@@Base+0x250>
   13d18:	ldrsw	x8, [x27]
   13d1c:	cmp	x24, x8
   13d20:	b.ge	13f6c <__gmpz_aorsmul_1@@Base+0x320>  // b.tcont
   13d24:	ldr	x20, [x27, #8]
   13d28:	ldr	x1, [x26, #8]
   13d2c:	mov	x0, x20
   13d30:	mov	x2, x24
   13d34:	mov	x3, x22
   13d38:	bl	d490 <__gmpn_mul_1@plt>
   13d3c:	cmp	x0, #0x0
   13d40:	str	x0, [x20, x24, lsl #3]
   13d44:	cinc	x8, x24, ne  // ne = any
   13d48:	mov	x25, x21
   13d4c:	b	13f20 <__gmpz_aorsmul_1@@Base+0x2d4>
   13d50:	mov	x0, x21
   13d54:	mov	x1, x26
   13d58:	mov	x2, x28
   13d5c:	mov	x3, x22
   13d60:	str	x8, [sp]
   13d64:	neg	x19, x25
   13d68:	bl	c9e0 <__gmpn_submul_1@plt>
   13d6c:	subs	x28, x24, x23
   13d70:	mov	x27, x0
   13d74:	b.le	13dd8 <__gmpz_aorsmul_1@@Base+0x18c>
   13d78:	mov	x0, x21
   13d7c:	mov	x1, x21
   13d80:	mov	x2, x23
   13d84:	bl	c290 <__gmpn_com@plt>
   13d88:	ldr	x8, [x21]
   13d8c:	adds	x8, x8, #0x1
   13d90:	str	x8, [x21]
   13d94:	b.cc	13dc0 <__gmpz_aorsmul_1@@Base+0x174>  // b.lo, b.ul, b.last
   13d98:	mov	w8, #0x1                   	// #1
   13d9c:	mov	w9, #0x1                   	// #1
   13da0:	cmp	x9, x23
   13da4:	b.ge	13eb0 <__gmpz_aorsmul_1@@Base+0x264>  // b.tcont
   13da8:	lsl	x10, x9, #3
   13dac:	ldr	x11, [x21, x10]
   13db0:	add	x9, x9, #0x1
   13db4:	adds	x11, x11, #0x1
   13db8:	str	x11, [x21, x10]
   13dbc:	b.cs	13da0 <__gmpz_aorsmul_1@@Base+0x154>  // b.hs, b.nlast
   13dc0:	mov	x25, x19
   13dc4:	mov	x8, xzr
   13dc8:	b	13eb4 <__gmpz_aorsmul_1@@Base+0x268>
   13dcc:	tbnz	x27, #63, 13e5c <__gmpz_aorsmul_1@@Base+0x210>
   13dd0:	mov	x27, xzr
   13dd4:	b	13e9c <__gmpz_aorsmul_1@@Base+0x250>
   13dd8:	b.ne	13e18 <__gmpz_aorsmul_1@@Base+0x1cc>  // b.any
   13ddc:	cbz	x27, 13eec <__gmpz_aorsmul_1@@Base+0x2a0>
   13de0:	sub	x8, x27, #0x1
   13de4:	mov	x0, x21
   13de8:	mov	x1, x21
   13dec:	mov	x2, x20
   13df0:	str	x8, [x21, x20, lsl #3]
   13df4:	bl	c290 <__gmpn_com@plt>
   13df8:	mov	x8, x21
   13dfc:	ldr	x9, [x8]
   13e00:	adds	x9, x9, #0x1
   13e04:	str	x9, [x8], #8
   13e08:	b.cs	13dfc <__gmpz_aorsmul_1@@Base+0x1b0>  // b.hs, b.nlast
   13e0c:	ldp	x20, x27, [sp]
   13e10:	mov	x25, x19
   13e14:	b	13f08 <__gmpz_aorsmul_1@@Base+0x2bc>
   13e18:	add	x8, x21, x24, lsl #3
   13e1c:	ldr	x9, [x8]
   13e20:	subs	x9, x9, x27
   13e24:	str	x9, [x8]
   13e28:	b.cs	13eec <__gmpz_aorsmul_1@@Base+0x2a0>  // b.hs, b.nlast
   13e2c:	sub	x9, x23, x24
   13e30:	mov	w27, #0x1                   	// #1
   13e34:	mov	w10, #0x1                   	// #1
   13e38:	cmp	x10, x9
   13e3c:	b.ge	13de0 <__gmpz_aorsmul_1@@Base+0x194>  // b.tcont
   13e40:	lsl	x11, x10, #3
   13e44:	ldr	x12, [x8, x11]
   13e48:	add	x10, x10, #0x1
   13e4c:	sub	x13, x12, #0x1
   13e50:	str	x13, [x8, x11]
   13e54:	cbz	x12, 13e38 <__gmpz_aorsmul_1@@Base+0x1ec>
   13e58:	b	13eec <__gmpz_aorsmul_1@@Base+0x2a0>
   13e5c:	ldr	x8, [x21]
   13e60:	neg	x27, x27
   13e64:	adds	x8, x8, x4
   13e68:	str	x8, [x21]
   13e6c:	b.cc	13e98 <__gmpz_aorsmul_1@@Base+0x24c>  // b.lo, b.ul, b.last
   13e70:	mov	w4, #0x1                   	// #1
   13e74:	mov	w8, #0x1                   	// #1
   13e78:	cmp	x8, x27
   13e7c:	b.ge	13e9c <__gmpz_aorsmul_1@@Base+0x250>  // b.tcont
   13e80:	lsl	x9, x8, #3
   13e84:	ldr	x10, [x21, x9]
   13e88:	add	x8, x8, #0x1
   13e8c:	adds	x10, x10, #0x1
   13e90:	str	x10, [x21, x9]
   13e94:	b.cs	13e78 <__gmpz_aorsmul_1@@Base+0x22c>  // b.hs, b.nlast
   13e98:	mov	x4, xzr
   13e9c:	str	x4, [x21, x27, lsl #3]
   13ea0:	ldr	x27, [sp, #8]
   13ea4:	cmp	x4, #0x0
   13ea8:	cinc	x8, x20, ne  // ne = any
   13eac:	b	13f20 <__gmpz_aorsmul_1@@Base+0x2d4>
   13eb0:	mov	x25, x19
   13eb4:	adds	x19, x8, x27
   13eb8:	lsl	x8, x23, #3
   13ebc:	cinc	x9, x19, eq  // eq = none
   13ec0:	add	x23, x21, x8
   13ec4:	sub	x4, x9, #0x1
   13ec8:	add	x1, x26, x8
   13ecc:	mov	x0, x23
   13ed0:	mov	x2, x28
   13ed4:	mov	x3, x22
   13ed8:	bl	d240 <__gmpn_mul_1c@plt>
   13edc:	cmp	x0, #0x0
   13ee0:	str	x0, [x21, x20, lsl #3]
   13ee4:	cinc	x20, x20, ne  // ne = any
   13ee8:	cbz	x19, 13ef4 <__gmpz_aorsmul_1@@Base+0x2a8>
   13eec:	ldr	x27, [sp, #8]
   13ef0:	b	13f08 <__gmpz_aorsmul_1@@Base+0x2bc>
   13ef4:	ldr	x27, [sp, #8]
   13ef8:	ldr	x8, [x23]
   13efc:	sub	x9, x8, #0x1
   13f00:	str	x9, [x23], #8
   13f04:	cbz	x8, 13ef8 <__gmpz_aorsmul_1@@Base+0x2ac>
   13f08:	sub	x9, x21, #0x8
   13f0c:	mov	x8, x20
   13f10:	subs	x20, x20, #0x1
   13f14:	b.lt	13f20 <__gmpz_aorsmul_1@@Base+0x2d4>  // b.tstop
   13f18:	ldr	x10, [x9, x8, lsl #3]
   13f1c:	cbz	x10, 13f0c <__gmpz_aorsmul_1@@Base+0x2c0>
   13f20:	neg	w9, w8
   13f24:	cmp	x25, #0x0
   13f28:	csel	x8, x8, x9, ge  // ge = tcont
   13f2c:	str	w8, [x27, #4]
   13f30:	ldp	x20, x19, [sp, #96]
   13f34:	ldp	x22, x21, [sp, #80]
   13f38:	ldp	x24, x23, [sp, #64]
   13f3c:	ldp	x26, x25, [sp, #48]
   13f40:	ldp	x28, x27, [sp, #32]
   13f44:	ldp	x29, x30, [sp, #16]
   13f48:	add	sp, sp, #0x70
   13f4c:	ret
   13f50:	mov	x0, x27
   13f54:	mov	x1, x8
   13f58:	mov	x21, x8
   13f5c:	bl	c080 <__gmpz_realloc@plt>
   13f60:	mov	x8, x21
   13f64:	mov	x21, x0
   13f68:	b	13cc8 <__gmpz_aorsmul_1@@Base+0x7c>
   13f6c:	add	x1, x24, #0x1
   13f70:	mov	x0, x27
   13f74:	bl	c080 <__gmpz_realloc@plt>
   13f78:	mov	x20, x0
   13f7c:	b	13d28 <__gmpz_aorsmul_1@@Base+0xdc>

0000000000013f80 <__gmpz_addmul_ui@@Base>:
   13f80:	mov	x3, xzr
   13f84:	b	cf80 <__gmpz_aorsmul_1@plt>

0000000000013f88 <__gmpz_submul_ui@@Base>:
   13f88:	mov	x3, #0xffffffffffffffff    	// #-1
   13f8c:	b	cf80 <__gmpz_aorsmul_1@plt>

0000000000013f90 <__gmpz_and@@Base>:
   13f90:	stp	x29, x30, [sp, #-96]!
   13f94:	stp	x26, x25, [sp, #32]
   13f98:	stp	x24, x23, [sp, #48]
   13f9c:	stp	x22, x21, [sp, #64]
   13fa0:	stp	x20, x19, [sp, #80]
   13fa4:	ldr	w8, [x1, #4]
   13fa8:	ldr	w9, [x2, #4]
   13fac:	str	x27, [sp, #16]
   13fb0:	mov	x19, x0
   13fb4:	mov	x29, sp
   13fb8:	cmp	w8, w9
   13fbc:	csel	x10, x1, x2, lt  // lt = tstop
   13fc0:	csel	x11, x2, x1, lt  // lt = tstop
   13fc4:	ldr	x23, [x11, #8]
   13fc8:	ldr	x22, [x10, #8]
   13fcc:	csel	w10, w8, w9, lt  // lt = tstop
   13fd0:	sxtw	x27, w10
   13fd4:	csel	w8, w9, w8, lt  // lt = tstop
   13fd8:	tbnz	w10, #31, 14050 <__gmpz_and@@Base+0xc0>
   13fdc:	sub	x9, x27, #0x1
   13fe0:	add	w8, w27, #0x1
   13fe4:	add	x10, x9, #0x1
   13fe8:	cmp	x10, #0x1
   13fec:	b.lt	140f4 <__gmpz_and@@Base+0x164>  // b.tstop
   13ff0:	lsl	x10, x9, #3
   13ff4:	ldr	x11, [x23, x10]
   13ff8:	ldr	x10, [x22, x10]
   13ffc:	sub	x9, x9, #0x1
   14000:	sub	w8, w8, #0x1
   14004:	and	x10, x10, x11
   14008:	cbz	x10, 13fe4 <__gmpz_and@@Base+0x54>
   1400c:	ldrsw	x10, [x19]
   14010:	add	x20, x9, #0x2
   14014:	str	w8, [x19, #4]
   14018:	cmp	x20, x10
   1401c:	b.gt	146d4 <__gmpz_and@@Base+0x744>
   14020:	ldr	x0, [x19, #8]
   14024:	mov	x1, x23
   14028:	mov	x2, x22
   1402c:	mov	x3, x20
   14030:	mov	sp, x29
   14034:	ldp	x20, x19, [sp, #80]
   14038:	ldp	x22, x21, [sp, #64]
   1403c:	ldp	x24, x23, [sp, #48]
   14040:	ldp	x26, x25, [sp, #32]
   14044:	ldr	x27, [sp, #16]
   14048:	ldp	x29, x30, [sp], #96
   1404c:	b	c270 <__gmpn_and_n@plt>
   14050:	sxtw	x20, w8
   14054:	neg	x21, x27
   14058:	str	xzr, [x29, #24]
   1405c:	tbnz	w20, #31, 140fc <__gmpz_and@@Base+0x16c>
   14060:	cmp	x21, #0xfe0
   14064:	lsl	x25, x21, #3
   14068:	b.hi	146e4 <__gmpz_and@@Base+0x754>  // b.pmore
   1406c:	add	x9, x25, #0xf
   14070:	mov	x8, sp
   14074:	and	x9, x9, #0xfffffffffffffff0
   14078:	sub	x24, x8, x9
   1407c:	mov	sp, x24
   14080:	ldr	x8, [x22]
   14084:	sub	x9, x8, #0x1
   14088:	str	x9, [x24]
   1408c:	cbz	x8, 141a0 <__gmpz_and@@Base+0x210>
   14090:	cmn	w27, #0x2
   14094:	b.gt	143a0 <__gmpz_and@@Base+0x410>
   14098:	cmp	x22, x24
   1409c:	b.eq	143a0 <__gmpz_and@@Base+0x410>  // b.none
   140a0:	cmn	w27, #0x5
   140a4:	b.hi	140c8 <__gmpz_and@@Base+0x138>  // b.pmore
   140a8:	add	x8, x24, #0x8
   140ac:	add	x9, x22, x21, lsl #3
   140b0:	cmp	x8, x9
   140b4:	b.cs	14368 <__gmpz_and@@Base+0x3d8>  // b.hs, b.nlast
   140b8:	sub	x8, x24, x27, lsl #3
   140bc:	add	x9, x22, #0x8
   140c0:	cmp	x8, x9
   140c4:	b.ls	14368 <__gmpz_and@@Base+0x3d8>  // b.plast
   140c8:	mov	w8, #0x1                   	// #1
   140cc:	add	x9, x8, x27
   140d0:	lsl	x10, x8, #3
   140d4:	neg	x8, x9
   140d8:	add	x9, x24, x10
   140dc:	add	x10, x22, x10
   140e0:	ldr	x11, [x10], #8
   140e4:	subs	x8, x8, #0x1
   140e8:	str	x11, [x9], #8
   140ec:	b.ne	140e0 <__gmpz_and@@Base+0x150>  // b.any
   140f0:	b	143a0 <__gmpz_and@@Base+0x410>
   140f4:	str	wzr, [x19, #4]
   140f8:	b	146b4 <__gmpz_and@@Base+0x724>
   140fc:	add	x8, x20, x27
   14100:	neg	x1, x8, lsl #3
   14104:	mov	w8, #0x7f00                	// #32512
   14108:	cmp	x1, x8
   1410c:	neg	x24, x20
   14110:	b.hi	146f8 <__gmpz_and@@Base+0x768>  // b.pmore
   14114:	add	x9, x1, #0xf
   14118:	mov	x8, sp
   1411c:	and	x9, x9, #0xfffffffffffffff0
   14120:	sub	x25, x8, x9
   14124:	mov	sp, x25
   14128:	ldr	x8, [x23]
   1412c:	add	x26, x25, x24, lsl #3
   14130:	sub	x9, x8, #0x1
   14134:	str	x9, [x25]
   14138:	cbz	x8, 14284 <__gmpz_and@@Base+0x2f4>
   1413c:	cmn	w20, #0x2
   14140:	b.gt	14480 <__gmpz_and@@Base+0x4f0>
   14144:	cmp	x23, x25
   14148:	b.eq	14480 <__gmpz_and@@Base+0x4f0>  // b.none
   1414c:	cmn	w20, #0x5
   14150:	b.hi	14174 <__gmpz_and@@Base+0x1e4>  // b.pmore
   14154:	add	x8, x25, #0x8
   14158:	add	x9, x23, x24, lsl #3
   1415c:	cmp	x8, x9
   14160:	b.cs	14448 <__gmpz_and@@Base+0x4b8>  // b.hs, b.nlast
   14164:	sub	x8, x25, x20, lsl #3
   14168:	add	x9, x23, #0x8
   1416c:	cmp	x8, x9
   14170:	b.ls	14448 <__gmpz_and@@Base+0x4b8>  // b.plast
   14174:	mov	w8, #0x1                   	// #1
   14178:	add	x9, x8, x20
   1417c:	lsl	x10, x8, #3
   14180:	neg	x8, x9
   14184:	add	x9, x25, x10
   14188:	add	x10, x23, x10
   1418c:	ldr	x11, [x10], #8
   14190:	subs	x8, x8, #0x1
   14194:	str	x11, [x9], #8
   14198:	b.ne	1418c <__gmpz_and@@Base+0x1fc>  // b.any
   1419c:	b	14480 <__gmpz_and@@Base+0x4f0>
   141a0:	mov	x10, xzr
   141a4:	mvn	x9, x27
   141a8:	mov	w8, #0x1                   	// #1
   141ac:	cmp	x8, x21
   141b0:	b.ge	143a0 <__gmpz_and@@Base+0x410>  // b.tcont
   141b4:	add	x11, x22, x10
   141b8:	ldr	x11, [x11, #8]
   141bc:	add	x12, x24, x10
   141c0:	add	x8, x8, #0x1
   141c4:	add	x10, x10, #0x8
   141c8:	sub	x13, x11, #0x1
   141cc:	sub	x9, x9, #0x1
   141d0:	str	x13, [x12, #8]
   141d4:	cbz	x11, 141ac <__gmpz_and@@Base+0x21c>
   141d8:	cmp	x22, x24
   141dc:	b.eq	143a0 <__gmpz_and@@Base+0x410>  // b.none
   141e0:	cmp	x8, x21
   141e4:	b.ge	143a0 <__gmpz_and@@Base+0x410>  // b.tcont
   141e8:	sub	x11, x21, x8
   141ec:	cmp	x11, #0x4
   141f0:	b.cc	1425c <__gmpz_and@@Base+0x2cc>  // b.lo, b.ul, b.last
   141f4:	add	x12, x24, x10
   141f8:	add	x12, x12, #0x8
   141fc:	add	x13, x22, x21, lsl #3
   14200:	cmp	x12, x13
   14204:	b.cs	1421c <__gmpz_and@@Base+0x28c>  // b.hs, b.nlast
   14208:	add	x13, x22, x10
   1420c:	sub	x12, x24, x27, lsl #3
   14210:	add	x13, x13, #0x8
   14214:	cmp	x12, x13
   14218:	b.hi	1425c <__gmpz_and@@Base+0x2cc>  // b.pmore
   1421c:	add	x12, x24, x10
   14220:	add	x13, x22, x10
   14224:	and	x10, x11, #0xfffffffffffffffc
   14228:	and	x14, x9, #0xfffffffffffffffc
   1422c:	add	x9, x12, #0x18
   14230:	add	x12, x13, #0x18
   14234:	add	x8, x14, x8
   14238:	mov	x13, x10
   1423c:	ldp	q0, q1, [x12, #-16]
   14240:	add	x12, x12, #0x20
   14244:	subs	x13, x13, #0x4
   14248:	stp	q0, q1, [x9, #-16]
   1424c:	add	x9, x9, #0x20
   14250:	b.ne	1423c <__gmpz_and@@Base+0x2ac>  // b.any
   14254:	cmp	x11, x10
   14258:	b.eq	143a0 <__gmpz_and@@Base+0x410>  // b.none
   1425c:	add	x9, x8, x27
   14260:	lsl	x10, x8, #3
   14264:	neg	x8, x9
   14268:	add	x9, x24, x10
   1426c:	add	x10, x22, x10
   14270:	ldr	x11, [x10], #8
   14274:	subs	x8, x8, #0x1
   14278:	str	x11, [x9], #8
   1427c:	b.ne	14270 <__gmpz_and@@Base+0x2e0>  // b.any
   14280:	b	143a0 <__gmpz_and@@Base+0x410>
   14284:	mov	x10, xzr
   14288:	mvn	x9, x20
   1428c:	mov	w8, #0x1                   	// #1
   14290:	cmp	x8, x24
   14294:	b.ge	14480 <__gmpz_and@@Base+0x4f0>  // b.tcont
   14298:	add	x11, x23, x10
   1429c:	ldr	x11, [x11, #8]
   142a0:	add	x12, x25, x10
   142a4:	add	x8, x8, #0x1
   142a8:	add	x10, x10, #0x8
   142ac:	sub	x13, x11, #0x1
   142b0:	sub	x9, x9, #0x1
   142b4:	str	x13, [x12, #8]
   142b8:	cbz	x11, 14290 <__gmpz_and@@Base+0x300>
   142bc:	cmp	x23, x25
   142c0:	b.eq	14480 <__gmpz_and@@Base+0x4f0>  // b.none
   142c4:	cmp	x8, x24
   142c8:	b.ge	14480 <__gmpz_and@@Base+0x4f0>  // b.tcont
   142cc:	sub	x11, x24, x8
   142d0:	cmp	x11, #0x4
   142d4:	b.cc	14340 <__gmpz_and@@Base+0x3b0>  // b.lo, b.ul, b.last
   142d8:	add	x12, x25, x10
   142dc:	add	x12, x12, #0x8
   142e0:	add	x13, x23, x24, lsl #3
   142e4:	cmp	x12, x13
   142e8:	b.cs	14300 <__gmpz_and@@Base+0x370>  // b.hs, b.nlast
   142ec:	add	x13, x23, x10
   142f0:	sub	x12, x25, x20, lsl #3
   142f4:	add	x13, x13, #0x8
   142f8:	cmp	x12, x13
   142fc:	b.hi	14340 <__gmpz_and@@Base+0x3b0>  // b.pmore
   14300:	add	x12, x25, x10
   14304:	add	x13, x23, x10
   14308:	and	x10, x11, #0xfffffffffffffffc
   1430c:	and	x14, x9, #0xfffffffffffffffc
   14310:	add	x9, x12, #0x18
   14314:	add	x12, x13, #0x18
   14318:	add	x8, x14, x8
   1431c:	mov	x13, x10
   14320:	ldp	q0, q1, [x12, #-16]
   14324:	add	x12, x12, #0x20
   14328:	subs	x13, x13, #0x4
   1432c:	stp	q0, q1, [x9, #-16]
   14330:	add	x9, x9, #0x20
   14334:	b.ne	14320 <__gmpz_and@@Base+0x390>  // b.any
   14338:	cmp	x11, x10
   1433c:	b.eq	14480 <__gmpz_and@@Base+0x4f0>  // b.none
   14340:	add	x9, x8, x20
   14344:	lsl	x10, x8, #3
   14348:	neg	x8, x9
   1434c:	add	x9, x25, x10
   14350:	add	x10, x23, x10
   14354:	ldr	x11, [x10], #8
   14358:	subs	x8, x8, #0x1
   1435c:	str	x11, [x9], #8
   14360:	b.ne	14354 <__gmpz_and@@Base+0x3c4>  // b.any
   14364:	b	14480 <__gmpz_and@@Base+0x4f0>
   14368:	mvn	x9, x27
   1436c:	and	x10, x9, #0xfffffffffffffffc
   14370:	add	x11, x22, #0x18
   14374:	orr	x8, x10, #0x1
   14378:	add	x12, x24, #0x18
   1437c:	mov	x13, x10
   14380:	ldp	q0, q1, [x11, #-16]
   14384:	add	x11, x11, #0x20
   14388:	subs	x13, x13, #0x4
   1438c:	stp	q0, q1, [x12, #-16]
   14390:	add	x12, x12, #0x20
   14394:	b.ne	14380 <__gmpz_and@@Base+0x3f0>  // b.any
   14398:	cmp	x10, x9
   1439c:	b.ne	140cc <__gmpz_and@@Base+0x13c>  // b.any
   143a0:	cmp	x20, x21
   143a4:	b.le	143e0 <__gmpz_and@@Base+0x450>
   143a8:	ldr	w8, [x19]
   143ac:	cmp	w20, w8
   143b0:	b.gt	14720 <__gmpz_and@@Base+0x790>
   143b4:	ldr	x22, [x19, #8]
   143b8:	mov	x0, x22
   143bc:	mov	x1, x23
   143c0:	mov	x2, x24
   143c4:	mov	x3, x21
   143c8:	bl	c060 <__gmpn_andn_n@plt>
   143cc:	add	x0, x22, x25
   143d0:	add	x1, x23, x25
   143d4:	add	x2, x20, x27
   143d8:	bl	ca50 <__gmpn_copyi@plt>
   143dc:	b	14434 <__gmpz_and@@Base+0x4a4>
   143e0:	sub	x8, x24, #0x8
   143e4:	subs	x9, x20, #0x1
   143e8:	b.lt	14430 <__gmpz_and@@Base+0x4a0>  // b.tstop
   143ec:	lsl	x10, x20, #3
   143f0:	add	x11, x23, x10
   143f4:	ldur	x11, [x11, #-8]
   143f8:	ldr	x10, [x8, x10]
   143fc:	mov	x20, x9
   14400:	bics	xzr, x11, x10
   14404:	b.eq	143e4 <__gmpz_and@@Base+0x454>  // b.none
   14408:	ldrsw	x8, [x19]
   1440c:	add	x20, x9, #0x1
   14410:	cmp	x20, x8
   14414:	b.gt	14734 <__gmpz_and@@Base+0x7a4>
   14418:	ldr	x0, [x19, #8]
   1441c:	mov	x1, x23
   14420:	mov	x2, x24
   14424:	mov	x3, x20
   14428:	bl	c060 <__gmpn_andn_n@plt>
   1442c:	b	14434 <__gmpz_and@@Base+0x4a4>
   14430:	mov	x20, xzr
   14434:	str	w20, [x19, #4]
   14438:	ldr	x0, [x29, #24]
   1443c:	cbz	x0, 146b4 <__gmpz_and@@Base+0x724>
   14440:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   14444:	b	146b4 <__gmpz_and@@Base+0x724>
   14448:	mvn	x9, x20
   1444c:	and	x10, x9, #0xfffffffffffffffc
   14450:	add	x11, x23, #0x18
   14454:	orr	x8, x10, #0x1
   14458:	add	x12, x25, #0x18
   1445c:	mov	x13, x10
   14460:	ldp	q0, q1, [x11, #-16]
   14464:	add	x11, x11, #0x20
   14468:	subs	x13, x13, #0x4
   1446c:	stp	q0, q1, [x12, #-16]
   14470:	add	x12, x12, #0x20
   14474:	b.ne	14460 <__gmpz_and@@Base+0x4d0>  // b.any
   14478:	cmp	x10, x9
   1447c:	b.ne	14178 <__gmpz_and@@Base+0x1e8>  // b.any
   14480:	ldr	x8, [x22]
   14484:	sub	x9, x8, #0x1
   14488:	str	x9, [x26]
   1448c:	cbz	x8, 14504 <__gmpz_and@@Base+0x574>
   14490:	cmn	w27, #0x2
   14494:	b.gt	14644 <__gmpz_and@@Base+0x6b4>
   14498:	cmp	x22, x26
   1449c:	b.eq	14644 <__gmpz_and@@Base+0x6b4>  // b.none
   144a0:	cmn	w27, #0x5
   144a4:	b.hi	144d4 <__gmpz_and@@Base+0x544>  // b.pmore
   144a8:	lsl	x8, x20, #3
   144ac:	sub	x9, x25, x8
   144b0:	add	x9, x9, #0x8
   144b4:	add	x10, x22, x21, lsl #3
   144b8:	cmp	x9, x10
   144bc:	b.cs	14608 <__gmpz_and@@Base+0x678>  // b.hs, b.nlast
   144c0:	add	x8, x8, x27, lsl #3
   144c4:	sub	x8, x25, x8
   144c8:	add	x9, x22, #0x8
   144cc:	cmp	x8, x9
   144d0:	b.ls	14608 <__gmpz_and@@Base+0x678>  // b.plast
   144d4:	mov	w8, #0x1                   	// #1
   144d8:	add	x9, x8, x27
   144dc:	lsl	x10, x8, #3
   144e0:	neg	x8, x9
   144e4:	sub	x9, x10, x20, lsl #3
   144e8:	add	x9, x25, x9
   144ec:	add	x10, x22, x10
   144f0:	ldr	x11, [x10], #8
   144f4:	subs	x8, x8, #0x1
   144f8:	str	x11, [x9], #8
   144fc:	b.ne	144f0 <__gmpz_and@@Base+0x560>  // b.any
   14500:	b	14644 <__gmpz_and@@Base+0x6b4>
   14504:	mov	w8, #0x20                  	// #32
   14508:	sub	x11, x8, x20, lsl #3
   1450c:	add	x8, x11, x25
   14510:	mov	x9, xzr
   14514:	mvn	x10, x27
   14518:	sub	x12, x8, #0x20
   1451c:	mov	w8, #0x1                   	// #1
   14520:	cmp	x8, x21
   14524:	b.ge	14644 <__gmpz_and@@Base+0x6b4>  // b.tcont
   14528:	add	x13, x22, x9
   1452c:	ldr	x13, [x13, #8]
   14530:	add	x14, x12, x9
   14534:	add	x8, x8, #0x1
   14538:	add	x9, x9, #0x8
   1453c:	sub	x15, x13, #0x1
   14540:	sub	x10, x10, #0x1
   14544:	str	x15, [x14, #8]
   14548:	cbz	x13, 14520 <__gmpz_and@@Base+0x590>
   1454c:	cmp	x22, x26
   14550:	b.eq	14644 <__gmpz_and@@Base+0x6b4>  // b.none
   14554:	cmp	x8, x21
   14558:	b.ge	14644 <__gmpz_and@@Base+0x6b4>  // b.tcont
   1455c:	sub	x12, x21, x8
   14560:	cmp	x12, #0x4
   14564:	b.cc	145dc <__gmpz_and@@Base+0x64c>  // b.lo, b.ul, b.last
   14568:	add	x13, x11, x25
   1456c:	add	x13, x13, x9
   14570:	sub	x13, x13, #0x18
   14574:	add	x14, x22, x21, lsl #3
   14578:	cmp	x13, x14
   1457c:	b.cs	14598 <__gmpz_and@@Base+0x608>  // b.hs, b.nlast
   14580:	add	x13, x27, x20
   14584:	add	x14, x22, x9
   14588:	sub	x13, x25, x13, lsl #3
   1458c:	add	x14, x14, #0x8
   14590:	cmp	x13, x14
   14594:	b.hi	145dc <__gmpz_and@@Base+0x64c>  // b.pmore
   14598:	add	x13, x11, x25
   1459c:	add	x14, x22, x9
   145a0:	and	x11, x12, #0xfffffffffffffffc
   145a4:	and	x15, x10, #0xfffffffffffffffc
   145a8:	add	x10, x13, x9
   145ac:	add	x9, x14, #0x18
   145b0:	sub	x10, x10, #0x8
   145b4:	add	x8, x15, x8
   145b8:	mov	x13, x11
   145bc:	ldp	q0, q1, [x9, #-16]
   145c0:	add	x9, x9, #0x20
   145c4:	subs	x13, x13, #0x4
   145c8:	stp	q0, q1, [x10, #-16]
   145cc:	add	x10, x10, #0x20
   145d0:	b.ne	145bc <__gmpz_and@@Base+0x62c>  // b.any
   145d4:	cmp	x12, x11
   145d8:	b.eq	14644 <__gmpz_and@@Base+0x6b4>  // b.none
   145dc:	add	x9, x8, x27
   145e0:	lsl	x10, x8, #3
   145e4:	neg	x8, x9
   145e8:	sub	x9, x10, x20, lsl #3
   145ec:	add	x9, x25, x9
   145f0:	add	x10, x22, x10
   145f4:	ldr	x11, [x10], #8
   145f8:	subs	x8, x8, #0x1
   145fc:	str	x11, [x9], #8
   14600:	b.ne	145f4 <__gmpz_and@@Base+0x664>  // b.any
   14604:	b	14644 <__gmpz_and@@Base+0x6b4>
   14608:	mvn	x9, x27
   1460c:	sub	x12, x25, x20, lsl #3
   14610:	and	x11, x9, #0xfffffffffffffffc
   14614:	add	x10, x22, #0x18
   14618:	orr	x8, x11, #0x1
   1461c:	add	x12, x12, #0x18
   14620:	mov	x13, x11
   14624:	ldp	q0, q1, [x10, #-16]
   14628:	add	x10, x10, #0x20
   1462c:	subs	x13, x13, #0x4
   14630:	stp	q0, q1, [x12, #-16]
   14634:	add	x12, x12, #0x20
   14638:	b.ne	14624 <__gmpz_and@@Base+0x694>  // b.any
   1463c:	cmp	x11, x9
   14640:	b.ne	144d8 <__gmpz_and@@Base+0x548>  // b.any
   14644:	ldrsw	x8, [x19]
   14648:	mov	w9, #0x1                   	// #1
   1464c:	sub	x1, x9, x27
   14650:	cmp	x1, x8
   14654:	b.gt	14708 <__gmpz_and@@Base+0x778>
   14658:	ldr	x22, [x19, #8]
   1465c:	lsl	x8, x24, #3
   14660:	add	x0, x22, x8
   14664:	add	x1, x26, x8
   14668:	sub	x2, x20, x27
   1466c:	bl	ca50 <__gmpn_copyi@plt>
   14670:	mov	x0, x22
   14674:	mov	x1, x25
   14678:	mov	x2, x26
   1467c:	mov	x3, x24
   14680:	bl	cc00 <__gmpn_ior_n@plt>
   14684:	ldr	x0, [x29, #24]
   14688:	cbnz	x0, 14718 <__gmpz_and@@Base+0x788>
   1468c:	mov	x8, x22
   14690:	str	xzr, [x22, x21, lsl #3]
   14694:	ldr	x9, [x8]
   14698:	adds	x9, x9, #0x1
   1469c:	str	x9, [x8], #8
   146a0:	b.cs	14694 <__gmpz_and@@Base+0x704>  // b.hs, b.nlast
   146a4:	lsl	x8, x21, #3
   146a8:	ldr	w8, [x22, x8]
   146ac:	sub	w8, w27, w8
   146b0:	str	w8, [x19, #4]
   146b4:	mov	sp, x29
   146b8:	ldp	x20, x19, [sp, #80]
   146bc:	ldp	x22, x21, [sp, #64]
   146c0:	ldp	x24, x23, [sp, #48]
   146c4:	ldp	x26, x25, [sp, #32]
   146c8:	ldr	x27, [sp, #16]
   146cc:	ldp	x29, x30, [sp], #96
   146d0:	ret
   146d4:	mov	x0, x19
   146d8:	mov	x1, x20
   146dc:	bl	c080 <__gmpz_realloc@plt>
   146e0:	b	14024 <__gmpz_and@@Base+0x94>
   146e4:	add	x0, x29, #0x18
   146e8:	mov	x1, x25
   146ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   146f0:	mov	x24, x0
   146f4:	b	14080 <__gmpz_and@@Base+0xf0>
   146f8:	add	x0, x29, #0x18
   146fc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   14700:	mov	x25, x0
   14704:	b	14128 <__gmpz_and@@Base+0x198>
   14708:	mov	x0, x19
   1470c:	bl	c080 <__gmpz_realloc@plt>
   14710:	mov	x22, x0
   14714:	b	1465c <__gmpz_and@@Base+0x6cc>
   14718:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1471c:	b	1468c <__gmpz_and@@Base+0x6fc>
   14720:	mov	x0, x19
   14724:	mov	x1, x20
   14728:	bl	c080 <__gmpz_realloc@plt>
   1472c:	mov	x22, x0
   14730:	b	143b8 <__gmpz_and@@Base+0x428>
   14734:	mov	x0, x19
   14738:	mov	x1, x20
   1473c:	bl	c080 <__gmpz_realloc@plt>
   14740:	b	1441c <__gmpz_and@@Base+0x48c>

0000000000014744 <__gmpz_array_init@@Base>:
   14744:	stp	x29, x30, [sp, #-48]!
   14748:	stp	x22, x21, [sp, #16]
   1474c:	stp	x20, x19, [sp, #32]
   14750:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   14754:	ldr	x9, [x9, #3840]
   14758:	add	x8, x2, #0x3f
   1475c:	cmp	x2, #0x0
   14760:	csel	x8, x8, x2, lt  // lt = tstop
   14764:	asr	x22, x8, #6
   14768:	ldr	x8, [x9]
   1476c:	add	x21, x22, #0x1
   14770:	mul	x9, x1, x21
   14774:	mov	x19, x0
   14778:	lsl	x0, x9, #3
   1477c:	mov	x29, sp
   14780:	mov	x20, x1
   14784:	blr	x8
   14788:	cmp	x20, #0x1
   1478c:	b.lt	14820 <__gmpz_array_init@@Base+0xdc>  // b.tstop
   14790:	add	w8, w22, #0x2
   14794:	cmp	x20, #0x1
   14798:	lsl	x9, x22, #3
   1479c:	b.ne	147a8 <__gmpz_array_init@@Base+0x64>  // b.any
   147a0:	mov	x10, xzr
   147a4:	b	147f0 <__gmpz_array_init@@Base+0xac>
   147a8:	and	x10, x20, #0xfffffffffffffffe
   147ac:	lsl	x12, x22, #4
   147b0:	add	x11, x9, #0x8
   147b4:	add	x12, x12, #0x10
   147b8:	add	x13, x19, #0x10
   147bc:	mov	x14, x0
   147c0:	mov	x15, x10
   147c4:	add	x16, x14, x11
   147c8:	stp	w8, wzr, [x13, #-16]
   147cc:	stp	w8, wzr, [x13]
   147d0:	stur	x14, [x13, #-8]
   147d4:	subs	x15, x15, #0x2
   147d8:	add	x14, x14, x12
   147dc:	str	x16, [x13, #8]
   147e0:	add	x13, x13, #0x20
   147e4:	b.ne	147c4 <__gmpz_array_init@@Base+0x80>  // b.any
   147e8:	cmp	x10, x20
   147ec:	b.eq	14820 <__gmpz_array_init@@Base+0xdc>  // b.none
   147f0:	add	x12, x19, x10, lsl #4
   147f4:	mul	x13, x10, x21
   147f8:	sub	x11, x20, x10
   147fc:	add	x10, x12, #0x4
   14800:	add	x12, x0, x13, lsl #3
   14804:	add	x9, x9, #0x8
   14808:	stp	w8, wzr, [x10, #-4]
   1480c:	stur	x12, [x10, #4]
   14810:	subs	x11, x11, #0x1
   14814:	add	x10, x10, #0x10
   14818:	add	x12, x12, x9
   1481c:	b.ne	14808 <__gmpz_array_init@@Base+0xc4>  // b.any
   14820:	ldp	x20, x19, [sp, #32]
   14824:	ldp	x22, x21, [sp, #16]
   14828:	ldp	x29, x30, [sp], #48
   1482c:	ret

0000000000014830 <__gmpz_bin_ui@@Base>:
   14830:	sub	sp, sp, #0x70
   14834:	stp	x29, x30, [sp, #48]
   14838:	stp	x22, x21, [sp, #80]
   1483c:	stp	x20, x19, [sp, #96]
   14840:	ldr	w8, [x1, #4]
   14844:	mov	x20, x2
   14848:	mov	x21, x1
   1484c:	mov	x19, x0
   14850:	str	x23, [sp, #64]
   14854:	add	x29, sp, #0x30
   14858:	tbnz	w8, #31, 1488c <__gmpz_bin_ui@@Base+0x5c>
   1485c:	mov	x0, x21
   14860:	mov	x1, x20
   14864:	bl	d1f0 <__gmpz_cmp_ui@plt>
   14868:	tbnz	w0, #31, 14a00 <__gmpz_bin_ui@@Base+0x1d0>
   1486c:	sub	x0, x29, #0x10
   14870:	bl	d250 <__gmpz_init@plt>
   14874:	sub	x0, x29, #0x10
   14878:	mov	x1, x21
   1487c:	mov	x2, x20
   14880:	bl	c120 <__gmpz_sub_ui@plt>
   14884:	mov	x23, xzr
   14888:	b	148b4 <__gmpz_bin_ui@@Base+0x84>
   1488c:	sub	x0, x29, #0x10
   14890:	bl	d250 <__gmpz_init@plt>
   14894:	sub	x0, x29, #0x10
   14898:	mov	w2, #0x1                   	// #1
   1489c:	mov	x1, x21
   148a0:	bl	c8b0 <__gmpz_add_ui@plt>
   148a4:	ldur	w8, [x29, #-12]
   148a8:	and	x23, x20, #0x1
   148ac:	neg	w8, w8
   148b0:	stur	w8, [x29, #-12]
   148b4:	sub	x0, x29, #0x10
   148b8:	mov	x1, x20
   148bc:	bl	d1f0 <__gmpz_cmp_ui@plt>
   148c0:	tbnz	w0, #31, 148e4 <__gmpz_bin_ui@@Base+0xb4>
   148c4:	cmp	x20, #0x1
   148c8:	b.hi	1490c <__gmpz_bin_ui@@Base+0xdc>  // b.pmore
   148cc:	cbz	x20, 14a08 <__gmpz_bin_ui@@Base+0x1d8>
   148d0:	sub	x1, x29, #0x10
   148d4:	mov	w2, #0x1                   	// #1
   148d8:	mov	x0, x19
   148dc:	bl	c8b0 <__gmpz_add_ui@plt>
   148e0:	b	14ba4 <__gmpz_bin_ui@@Base+0x374>
   148e4:	ldur	x8, [x29, #-8]
   148e8:	ldur	w22, [x29, #-12]
   148ec:	sub	x0, x29, #0x10
   148f0:	mov	x1, x20
   148f4:	ldr	x21, [x8]
   148f8:	bl	c170 <__gmpz_set_ui@plt>
   148fc:	cbz	w22, 14a08 <__gmpz_bin_ui@@Base+0x1d8>
   14900:	mov	x20, x21
   14904:	cmp	x20, #0x1
   14908:	b.ls	148cc <__gmpz_bin_ui@@Base+0x9c>  // b.plast
   1490c:	add	x0, sp, #0x10
   14910:	bl	d250 <__gmpz_init@plt>
   14914:	mov	x0, sp
   14918:	bl	d250 <__gmpz_init@plt>
   1491c:	ldp	w8, w21, [x29, #-16]
   14920:	sxtw	x21, w21
   14924:	add	x1, x21, #0x2
   14928:	cmp	w1, w8
   1492c:	b.gt	14bd8 <__gmpz_bin_ui@@Base+0x3a8>
   14930:	ldur	x0, [x29, #-8]
   14934:	add	x8, x0, x21, lsl #3
   14938:	stp	xzr, xzr, [x8]
   1493c:	ldur	x8, [x29, #-8]
   14940:	mov	x9, x8
   14944:	ldr	x10, [x9]
   14948:	adds	x10, x10, #0x1
   1494c:	str	x10, [x9], #8
   14950:	b.cs	14944 <__gmpz_bin_ui@@Base+0x114>  // b.hs, b.nlast
   14954:	ldursw	x9, [x29, #-12]
   14958:	ldr	x8, [x8, x9, lsl #3]
   1495c:	str	wzr, [sp, #20]
   14960:	cmp	x8, #0x0
   14964:	cinc	w8, w9, ne  // ne = any
   14968:	stur	w8, [x29, #-12]
   1496c:	tbz	w20, #0, 149a8 <__gmpz_bin_ui@@Base+0x178>
   14970:	add	x0, sp, #0x10
   14974:	sub	x1, x29, #0x10
   14978:	bl	c420 <__gmpz_set@plt>
   1497c:	ldur	x8, [x29, #-8]
   14980:	mov	x9, x8
   14984:	ldr	x10, [x9]
   14988:	adds	x10, x10, #0x1
   1498c:	str	x10, [x9], #8
   14990:	b.cs	14984 <__gmpz_bin_ui@@Base+0x154>  // b.hs, b.nlast
   14994:	ldursw	x9, [x29, #-12]
   14998:	ldr	x8, [x8, x9, lsl #3]
   1499c:	cmp	x8, #0x0
   149a0:	cinc	w8, w9, ne  // ne = any
   149a4:	stur	w8, [x29, #-12]
   149a8:	lsr	x21, x20, #1
   149ac:	sub	x1, x29, #0x10
   149b0:	mov	x3, sp
   149b4:	mov	x0, x19
   149b8:	mov	x2, x21
   149bc:	bl	14c00 <__gmpz_bin_ui@@Base+0x3d0>
   149c0:	ldp	w8, w22, [x19]
   149c4:	sxtw	x22, w22
   149c8:	add	x1, x22, #0x2
   149cc:	cmp	w1, w8
   149d0:	b.gt	14be4 <__gmpz_bin_ui@@Base+0x3b4>
   149d4:	ldr	x0, [x19, #8]
   149d8:	add	x8, x0, x22, lsl #3
   149dc:	stp	xzr, xzr, [x8]
   149e0:	tbz	w20, #1, 14a78 <__gmpz_bin_ui@@Base+0x248>
   149e4:	ldr	w8, [sp, #20]
   149e8:	cbz	w8, 14a2c <__gmpz_bin_ui@@Base+0x1fc>
   149ec:	add	x0, sp, #0x10
   149f0:	add	x1, sp, #0x10
   149f4:	mov	x2, x19
   149f8:	bl	c4b0 <__gmpz_mul@plt>
   149fc:	b	14a38 <__gmpz_bin_ui@@Base+0x208>
   14a00:	str	wzr, [x19, #4]
   14a04:	b	14bc0 <__gmpz_bin_ui@@Base+0x390>
   14a08:	ldr	w8, [x19]
   14a0c:	mov	w9, #0x1                   	// #1
   14a10:	str	w9, [x19, #4]
   14a14:	cmp	w8, #0x0
   14a18:	b.le	14bf0 <__gmpz_bin_ui@@Base+0x3c0>
   14a1c:	ldr	x0, [x19, #8]
   14a20:	mov	w8, #0x1                   	// #1
   14a24:	str	x8, [x0]
   14a28:	b	14ba4 <__gmpz_bin_ui@@Base+0x374>
   14a2c:	add	x0, sp, #0x10
   14a30:	mov	x1, x19
   14a34:	bl	c420 <__gmpz_set@plt>
   14a38:	ldr	x8, [x19, #8]
   14a3c:	sub	x10, x21, #0x1
   14a40:	ldr	x9, [x8]
   14a44:	adds	x9, x9, x10
   14a48:	str	x9, [x8]
   14a4c:	b.cc	14a64 <__gmpz_bin_ui@@Base+0x234>  // b.lo, b.ul, b.last
   14a50:	add	x9, x8, #0x8
   14a54:	ldr	x10, [x9]
   14a58:	adds	x10, x10, #0x1
   14a5c:	str	x10, [x9], #8
   14a60:	b.cs	14a54 <__gmpz_bin_ui@@Base+0x224>  // b.hs, b.nlast
   14a64:	ldrsw	x9, [x19, #4]
   14a68:	ldr	x8, [x8, x9, lsl #3]
   14a6c:	cmp	x8, #0x0
   14a70:	cinc	w8, w9, ne  // ne = any
   14a74:	str	w8, [x19, #4]
   14a78:	lsr	x22, x20, #2
   14a7c:	cbz	x22, 14b2c <__gmpz_bin_ui@@Base+0x2fc>
   14a80:	mov	x0, sp
   14a84:	sub	x3, x29, #0x10
   14a88:	mov	x1, x19
   14a8c:	mov	x2, x22
   14a90:	bl	14c00 <__gmpz_bin_ui@@Base+0x3d0>
   14a94:	ldr	w8, [sp, #20]
   14a98:	cbz	w8, 14ab8 <__gmpz_bin_ui@@Base+0x288>
   14a9c:	add	x0, sp, #0x10
   14aa0:	add	x1, sp, #0x10
   14aa4:	mov	x2, sp
   14aa8:	bl	c4b0 <__gmpz_mul@plt>
   14aac:	cmp	x20, #0x8
   14ab0:	b.cs	14acc <__gmpz_bin_ui@@Base+0x29c>  // b.hs, b.nlast
   14ab4:	b	14b2c <__gmpz_bin_ui@@Base+0x2fc>
   14ab8:	add	x0, sp, #0x10
   14abc:	mov	x1, sp
   14ac0:	bl	c420 <__gmpz_set@plt>
   14ac4:	cmp	x20, #0x8
   14ac8:	b.cc	14b2c <__gmpz_bin_ui@@Base+0x2fc>  // b.lo, b.ul, b.last
   14acc:	ldr	x8, [x19, #8]
   14ad0:	ldr	x9, [x8]
   14ad4:	subs	x9, x9, x22
   14ad8:	str	x9, [x8]
   14adc:	b.cs	14af4 <__gmpz_bin_ui@@Base+0x2c4>  // b.hs, b.nlast
   14ae0:	add	x9, x8, #0x8
   14ae4:	ldr	x10, [x9]
   14ae8:	sub	x11, x10, #0x1
   14aec:	str	x11, [x9], #8
   14af0:	cbz	x10, 14ae4 <__gmpz_bin_ui@@Base+0x2b4>
   14af4:	ldr	w9, [x19, #4]
   14af8:	sub	x3, x22, #0x1
   14afc:	add	x0, sp, #0x10
   14b00:	mov	x2, sp
   14b04:	sub	w10, w9, #0x1
   14b08:	ldr	x8, [x8, w10, sxtw #3]
   14b0c:	sub	x5, x29, #0x10
   14b10:	mov	x1, x19
   14b14:	mov	x4, xzr
   14b18:	cmp	x8, #0x0
   14b1c:	cset	w8, eq  // eq = none
   14b20:	sub	w8, w9, w8
   14b24:	str	w8, [x19, #4]
   14b28:	bl	14d34 <__gmpz_bin_ui@@Base+0x504>
   14b2c:	and	x8, x21, #0x5555555555555555
   14b30:	sub	x8, x20, x8
   14b34:	lsr	x10, x8, #2
   14b38:	and	x8, x8, #0x3333333333333333
   14b3c:	and	x10, x10, #0x3333333333333333
   14b40:	add	x8, x10, x8
   14b44:	add	x8, x8, x8, lsr #4
   14b48:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   14b4c:	add	x8, x8, x8, lsr #8
   14b50:	add	x8, x8, x8, lsr #16
   14b54:	sub	x9, x20, x21
   14b58:	lsr	x10, x8, #32
   14b5c:	add	w8, w10, w8
   14b60:	sub	x9, x9, x22
   14b64:	sub	x2, x9, w8, uxtb
   14b68:	add	x0, sp, #0x10
   14b6c:	add	x1, sp, #0x10
   14b70:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   14b74:	mov	x0, sp
   14b78:	mov	x1, x20
   14b7c:	mov	w2, wzr
   14b80:	bl	c3d0 <__gmpz_oddfac_1@plt>
   14b84:	add	x1, sp, #0x10
   14b88:	mov	x2, sp
   14b8c:	mov	x0, x19
   14b90:	bl	c3f0 <__gmpz_divexact@plt>
   14b94:	add	x0, sp, #0x10
   14b98:	bl	cb50 <__gmpz_clear@plt>
   14b9c:	mov	x0, sp
   14ba0:	bl	cb50 <__gmpz_clear@plt>
   14ba4:	sub	x0, x29, #0x10
   14ba8:	bl	cb50 <__gmpz_clear@plt>
   14bac:	ldr	w8, [x19, #4]
   14bb0:	neg	w9, w23
   14bb4:	eor	w8, w8, w9
   14bb8:	add	w8, w8, w23
   14bbc:	str	w8, [x19, #4]
   14bc0:	ldp	x20, x19, [sp, #96]
   14bc4:	ldp	x22, x21, [sp, #80]
   14bc8:	ldr	x23, [sp, #64]
   14bcc:	ldp	x29, x30, [sp, #48]
   14bd0:	add	sp, sp, #0x70
   14bd4:	ret
   14bd8:	sub	x0, x29, #0x10
   14bdc:	bl	c080 <__gmpz_realloc@plt>
   14be0:	b	14934 <__gmpz_bin_ui@@Base+0x104>
   14be4:	mov	x0, x19
   14be8:	bl	c080 <__gmpz_realloc@plt>
   14bec:	b	149d8 <__gmpz_bin_ui@@Base+0x1a8>
   14bf0:	mov	w1, #0x1                   	// #1
   14bf4:	mov	x0, x19
   14bf8:	bl	c080 <__gmpz_realloc@plt>
   14bfc:	b	14a20 <__gmpz_bin_ui@@Base+0x1f0>
   14c00:	sub	sp, sp, #0x40
   14c04:	stp	x20, x19, [sp, #48]
   14c08:	sub	x20, x2, #0x1
   14c0c:	mov	x19, x0
   14c10:	mov	x0, x3
   14c14:	mov	x2, x20
   14c18:	stp	x29, x30, [sp, #16]
   14c1c:	stp	x22, x21, [sp, #32]
   14c20:	add	x29, sp, #0x10
   14c24:	mov	x21, x3
   14c28:	mov	x22, x1
   14c2c:	bl	c8b0 <__gmpz_add_ui@plt>
   14c30:	mov	x0, x19
   14c34:	mov	x1, x21
   14c38:	mov	x2, x21
   14c3c:	bl	c4b0 <__gmpz_mul@plt>
   14c40:	mov	x0, x19
   14c44:	mov	x1, x19
   14c48:	mov	x2, x22
   14c4c:	bl	cf90 <__gmpz_add@plt>
   14c50:	ldrsw	x21, [x19, #4]
   14c54:	ldr	x22, [x19, #8]
   14c58:	mov	w3, #0x1                   	// #1
   14c5c:	mov	x2, x21
   14c60:	mov	x0, x22
   14c64:	mov	x1, x22
   14c68:	bl	c1a0 <__gmpn_rshift@plt>
   14c6c:	add	x8, x22, x21, lsl #3
   14c70:	ldur	x8, [x8, #-8]
   14c74:	ldr	w9, [x19, #4]
   14c78:	mov	x10, #0x100000000           	// #4294967296
   14c7c:	cmp	x8, #0x0
   14c80:	cset	w8, eq  // eq = none
   14c84:	sub	w8, w9, w8
   14c88:	cmp	x20, x10
   14c8c:	str	w8, [x19, #4]
   14c90:	b.hi	14cf8 <__gmpz_bin_ui@@Base+0x4c8>  // b.pmore
   14c94:	ldr	x9, [x19, #8]
   14c98:	and	x10, x20, #0x1
   14c9c:	add	x10, x10, x20
   14ca0:	lsr	x12, x20, #1
   14ca4:	ldr	x11, [x9]
   14ca8:	mul	x10, x10, x12
   14cac:	subs	x10, x11, x10
   14cb0:	str	x10, [x9]
   14cb4:	b.cs	14ccc <__gmpz_bin_ui@@Base+0x49c>  // b.hs, b.nlast
   14cb8:	add	x10, x9, #0x8
   14cbc:	ldr	x11, [x10]
   14cc0:	sub	x12, x11, #0x1
   14cc4:	str	x12, [x10], #8
   14cc8:	cbz	x11, 14cbc <__gmpz_bin_ui@@Base+0x48c>
   14ccc:	sub	w10, w8, #0x1
   14cd0:	ldr	x9, [x9, w10, sxtw #3]
   14cd4:	cmp	x9, #0x0
   14cd8:	cset	w9, eq  // eq = none
   14cdc:	sub	w8, w8, w9
   14ce0:	str	w8, [x19, #4]
   14ce4:	ldp	x20, x19, [sp, #48]
   14ce8:	ldp	x22, x21, [sp, #32]
   14cec:	ldp	x29, x30, [sp, #16]
   14cf0:	add	sp, sp, #0x40
   14cf4:	ret
   14cf8:	and	x8, x20, #0x1
   14cfc:	add	x1, x8, x20
   14d00:	mov	x0, sp
   14d04:	bl	ce50 <__gmpz_init_set_ui@plt>
   14d08:	lsr	x2, x20, #1
   14d0c:	mov	x0, sp
   14d10:	mov	x1, sp
   14d14:	bl	c5c0 <__gmpz_mul_ui@plt>
   14d18:	mov	x2, sp
   14d1c:	mov	x0, x19
   14d20:	mov	x1, x19
   14d24:	bl	c260 <__gmpz_sub@plt>
   14d28:	mov	x0, sp
   14d2c:	bl	cb50 <__gmpz_clear@plt>
   14d30:	b	14ce4 <__gmpz_bin_ui@@Base+0x4b4>
   14d34:	sub	sp, sp, #0x60
   14d38:	sub	x8, x3, x4
   14d3c:	stp	x26, x25, [sp, #32]
   14d40:	stp	x22, x21, [sp, #64]
   14d44:	stp	x20, x19, [sp, #80]
   14d48:	mov	x19, x4
   14d4c:	mov	x25, x3
   14d50:	mov	x21, x2
   14d54:	mov	x22, x1
   14d58:	cmp	x8, #0x4
   14d5c:	mov	x20, x0
   14d60:	stp	x29, x30, [sp, #16]
   14d64:	stp	x24, x23, [sp, #48]
   14d68:	add	x29, sp, #0x10
   14d6c:	b.ls	14e84 <__gmpz_bin_ui@@Base+0x654>  // b.plast
   14d70:	add	x8, x19, x25
   14d74:	lsr	x24, x8, #1
   14d78:	add	x26, x24, #0x1
   14d7c:	mov	x0, x20
   14d80:	mov	x1, x22
   14d84:	mov	x2, x21
   14d88:	mov	x3, x25
   14d8c:	mov	x4, x26
   14d90:	mov	x23, x5
   14d94:	bl	14d34 <__gmpz_bin_ui@@Base+0x504>
   14d98:	ldr	x8, [x22, #8]
   14d9c:	mov	w10, #0x2                   	// #2
   14da0:	bfi	x10, x26, #2, #62
   14da4:	lsl	x2, x26, #2
   14da8:	ldr	x9, [x8]
   14dac:	adds	x9, x9, x10
   14db0:	str	x9, [x8]
   14db4:	b.cc	14dcc <__gmpz_bin_ui@@Base+0x59c>  // b.lo, b.ul, b.last
   14db8:	add	x9, x8, #0x8
   14dbc:	ldr	x10, [x9]
   14dc0:	adds	x10, x10, #0x1
   14dc4:	str	x10, [x9], #8
   14dc8:	b.cs	14dbc <__gmpz_bin_ui@@Base+0x58c>  // b.hs, b.nlast
   14dcc:	ldrsw	x9, [x22, #4]
   14dd0:	mov	x0, x21
   14dd4:	mov	x1, x22
   14dd8:	ldr	x8, [x8, x9, lsl #3]
   14ddc:	cmp	x8, #0x0
   14de0:	cinc	w8, w9, ne  // ne = any
   14de4:	str	w8, [x22, #4]
   14de8:	bl	d320 <__gmpz_addmul_ui@plt>
   14dec:	ldr	x8, [x21, #8]
   14df0:	ldr	x9, [x8]
   14df4:	sub	x10, x9, x26
   14df8:	cmp	x9, x24
   14dfc:	str	x10, [x8]
   14e00:	b.hi	14e18 <__gmpz_bin_ui@@Base+0x5e8>  // b.pmore
   14e04:	add	x9, x8, #0x8
   14e08:	ldr	x10, [x9]
   14e0c:	sub	x11, x10, #0x1
   14e10:	str	x11, [x9], #8
   14e14:	cbz	x10, 14e08 <__gmpz_bin_ui@@Base+0x5d8>
   14e18:	ldr	w9, [x21, #4]
   14e1c:	sub	w10, w9, #0x1
   14e20:	ldr	x8, [x8, w10, sxtw #3]
   14e24:	cmp	x8, #0x0
   14e28:	cset	w8, eq  // eq = none
   14e2c:	sub	w8, w9, w8
   14e30:	str	w8, [x21, #4]
   14e34:	cbz	x23, 14f04 <__gmpz_bin_ui@@Base+0x6d4>
   14e38:	mov	x0, x23
   14e3c:	mov	x1, x21
   14e40:	str	wzr, [sp]
   14e44:	bl	c420 <__gmpz_set@plt>
   14e48:	b	14f14 <__gmpz_bin_ui@@Base+0x6e4>
   14e4c:	ldr	w9, [x21, #4]
   14e50:	mov	x0, x20
   14e54:	mov	x1, x20
   14e58:	mov	x2, x21
   14e5c:	sub	w10, w9, #0x1
   14e60:	ldr	x8, [x8, w10, sxtw #3]
   14e64:	cmp	x8, #0x0
   14e68:	cset	w8, eq  // eq = none
   14e6c:	sub	w8, w9, w8
   14e70:	str	w8, [x21, #4]
   14e74:	bl	c4b0 <__gmpz_mul@plt>
   14e78:	sub	x25, x25, #0x1
   14e7c:	cmp	x25, x19
   14e80:	b.ls	14f48 <__gmpz_bin_ui@@Base+0x718>  // b.plast
   14e84:	ldr	x8, [x22, #8]
   14e88:	mov	w10, #0x2                   	// #2
   14e8c:	bfi	x10, x25, #2, #62
   14e90:	lsl	x2, x25, #2
   14e94:	ldr	x9, [x8]
   14e98:	adds	x9, x9, x10
   14e9c:	str	x9, [x8]
   14ea0:	b.cc	14eb8 <__gmpz_bin_ui@@Base+0x688>  // b.lo, b.ul, b.last
   14ea4:	add	x9, x8, #0x8
   14ea8:	ldr	x10, [x9]
   14eac:	adds	x10, x10, #0x1
   14eb0:	str	x10, [x9], #8
   14eb4:	b.cs	14ea8 <__gmpz_bin_ui@@Base+0x678>  // b.hs, b.nlast
   14eb8:	ldrsw	x9, [x22, #4]
   14ebc:	mov	x0, x21
   14ec0:	mov	x1, x22
   14ec4:	ldr	x8, [x8, x9, lsl #3]
   14ec8:	cmp	x8, #0x0
   14ecc:	cinc	w8, w9, ne  // ne = any
   14ed0:	str	w8, [x22, #4]
   14ed4:	bl	d320 <__gmpz_addmul_ui@plt>
   14ed8:	ldr	x8, [x21, #8]
   14edc:	ldr	x9, [x8]
   14ee0:	subs	x9, x9, x25
   14ee4:	str	x9, [x8]
   14ee8:	b.cs	14e4c <__gmpz_bin_ui@@Base+0x61c>  // b.hs, b.nlast
   14eec:	add	x9, x8, #0x8
   14ef0:	ldr	x10, [x9]
   14ef4:	sub	x11, x10, #0x1
   14ef8:	str	x11, [x9], #8
   14efc:	cbz	x10, 14ef0 <__gmpz_bin_ui@@Base+0x6c0>
   14f00:	b	14e4c <__gmpz_bin_ui@@Base+0x61c>
   14f04:	mov	x0, sp
   14f08:	mov	x1, x21
   14f0c:	mov	x23, sp
   14f10:	bl	bf80 <__gmpz_init_set@plt>
   14f14:	mov	x0, x23
   14f18:	mov	x1, x22
   14f1c:	mov	x2, x21
   14f20:	mov	x3, x24
   14f24:	mov	x4, x19
   14f28:	mov	x5, xzr
   14f2c:	bl	14d34 <__gmpz_bin_ui@@Base+0x504>
   14f30:	mov	x0, x20
   14f34:	mov	x1, x20
   14f38:	mov	x2, x23
   14f3c:	bl	c4b0 <__gmpz_mul@plt>
   14f40:	mov	x0, sp
   14f44:	bl	cb50 <__gmpz_clear@plt>
   14f48:	ldp	x20, x19, [sp, #80]
   14f4c:	ldp	x22, x21, [sp, #64]
   14f50:	ldp	x24, x23, [sp, #48]
   14f54:	ldp	x26, x25, [sp, #32]
   14f58:	ldp	x29, x30, [sp, #16]
   14f5c:	add	sp, sp, #0x60
   14f60:	ret

0000000000014f64 <__gmpz_bin_uiui@@Base>:
   14f64:	stp	x29, x30, [sp, #-32]!
   14f68:	stp	x20, x19, [sp, #16]
   14f6c:	subs	x8, x1, x2
   14f70:	mov	x19, x0
   14f74:	mov	x29, sp
   14f78:	b.cc	150ac <__gmpz_bin_uiui@@Base+0x148>  // b.lo, b.ul, b.last
   14f7c:	cmp	x8, x2
   14f80:	csel	x2, x2, x8, hi  // hi = pmore
   14f84:	cmp	x2, #0x1
   14f88:	b.hi	14fbc <__gmpz_bin_uiui@@Base+0x58>  // b.pmore
   14f8c:	ldr	w8, [x19]
   14f90:	cmp	x2, #0x0
   14f94:	csinc	x20, x1, xzr, ne  // ne = any
   14f98:	cmp	w8, #0x0
   14f9c:	b.le	15038 <__gmpz_bin_uiui@@Base+0xd4>
   14fa0:	ldr	x0, [x19, #8]
   14fa4:	mov	w8, #0x1                   	// #1
   14fa8:	str	x20, [x0]
   14fac:	str	w8, [x19, #4]
   14fb0:	ldp	x20, x19, [sp, #16]
   14fb4:	ldp	x29, x30, [sp], #32
   14fb8:	ret
   14fbc:	cmp	x1, #0x43
   14fc0:	b.hi	15048 <__gmpz_bin_uiui@@Base+0xe4>  // b.pmore
   14fc4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   14fc8:	adrp	x10, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   14fcc:	sub	w11, w1, w2
   14fd0:	ldr	x8, [x8, #3848]
   14fd4:	sub	w9, w2, #0x2
   14fd8:	add	x10, x10, #0xf8
   14fdc:	sub	w13, w11, #0x2
   14fe0:	ldr	x9, [x10, w9, uxtw #3]
   14fe4:	ldr	x10, [x10, w13, uxtw #3]
   14fe8:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   14fec:	ldr	x13, [x13, #3992]
   14ff0:	ubfx	x12, x1, #1, #31
   14ff4:	ldr	x8, [x8, x1, lsl #3]
   14ff8:	ubfx	x14, x2, #1, #31
   14ffc:	sub	w12, w12, #0x1
   15000:	sub	w14, w14, #0x1
   15004:	lsr	w11, w11, #1
   15008:	ldrb	w12, [x13, w12, uxtw]
   1500c:	ldrb	w14, [x13, w14, uxtw]
   15010:	sub	w11, w11, #0x1
   15014:	ldrb	w11, [x13, w11, uxtw]
   15018:	mul	x8, x9, x8
   1501c:	ldr	w9, [x19]
   15020:	mul	x8, x8, x10
   15024:	sub	w10, w12, w14
   15028:	sub	w10, w10, w11
   1502c:	cmp	w9, #0x0
   15030:	lsl	x20, x8, x10
   15034:	b.gt	14fa0 <__gmpz_bin_uiui@@Base+0x3c>
   15038:	mov	w1, #0x1                   	// #1
   1503c:	mov	x0, x19
   15040:	bl	c080 <__gmpz_realloc@plt>
   15044:	b	14fa4 <__gmpz_bin_uiui@@Base+0x40>
   15048:	cmp	x2, #0x19
   1504c:	b.hi	15060 <__gmpz_bin_uiui@@Base+0xfc>  // b.pmore
   15050:	mov	x0, x19
   15054:	ldp	x20, x19, [sp, #16]
   15058:	ldp	x29, x30, [sp], #32
   1505c:	b	150b4 <__gmpz_bin_uiui@@Base+0x150>
   15060:	cmp	x2, #0x46
   15064:	b.hi	15078 <__gmpz_bin_uiui@@Base+0x114>  // b.pmore
   15068:	mov	x0, x19
   1506c:	ldp	x20, x19, [sp, #16]
   15070:	ldp	x29, x30, [sp], #32
   15074:	b	152d8 <__gmpz_bin_uiui@@Base+0x374>
   15078:	cmp	x2, #0x200
   1507c:	b.cc	1509c <__gmpz_bin_uiui@@Base+0x138>  // b.lo, b.ul, b.last
   15080:	lsr	x8, x1, #4
   15084:	cmp	x2, x8
   15088:	b.ls	1509c <__gmpz_bin_uiui@@Base+0x138>  // b.plast
   1508c:	mov	x0, x19
   15090:	ldp	x20, x19, [sp, #16]
   15094:	ldp	x29, x30, [sp], #32
   15098:	b	154a0 <__gmpz_bin_uiui@@Base+0x53c>
   1509c:	mov	x0, x19
   150a0:	ldp	x20, x19, [sp, #16]
   150a4:	ldp	x29, x30, [sp], #32
   150a8:	b	1594c <__gmpz_bin_uiui@@Base+0x9e8>
   150ac:	str	wzr, [x19, #4]
   150b0:	b	14fb0 <__gmpz_bin_uiui@@Base+0x4c>
   150b4:	sub	sp, sp, #0x70
   150b8:	stp	x29, x30, [sp, #16]
   150bc:	stp	x28, x27, [sp, #32]
   150c0:	stp	x26, x25, [sp, #48]
   150c4:	stp	x24, x23, [sp, #64]
   150c8:	stp	x22, x21, [sp, #80]
   150cc:	stp	x20, x19, [sp, #96]
   150d0:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   150d4:	ldr	x9, [x9, #3880]
   150d8:	mov	x20, x2
   150dc:	mov	x21, x0
   150e0:	mov	w8, #0x9                   	// #9
   150e4:	add	x29, sp, #0x10
   150e8:	sub	w10, w8, #0x2
   150ec:	ldr	x10, [x9, w10, uxtw #3]
   150f0:	sub	w8, w8, #0x1
   150f4:	cmp	x10, x1
   150f8:	b.cc	150e8 <__gmpz_bin_uiui@@Base+0x184>  // b.lo, b.ul, b.last
   150fc:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15100:	ldr	x10, [x10, #3992]
   15104:	cmp	w8, #0x8
   15108:	mov	w9, #0x8                   	// #8
   1510c:	csel	w26, w8, w9, cc  // cc = lo, ul, last
   15110:	add	x8, x10, x20, lsr #1
   15114:	ldurb	w24, [x8, #-1]
   15118:	sub	x8, x1, x20
   1511c:	cmp	x26, x20
   15120:	add	x22, x8, #0x1
   15124:	b.cs	1523c <__gmpz_bin_uiui@@Base+0x2d8>  // b.hs, b.nlast
   15128:	clz	x8, x1
   1512c:	mov	w9, #0x40                  	// #64
   15130:	sub	x8, x9, x8
   15134:	ldrsw	x9, [x21]
   15138:	mul	x8, x8, x20
   1513c:	lsr	x8, x8, #6
   15140:	add	x1, x8, #0x3
   15144:	cmp	x1, x9
   15148:	str	x21, [sp, #8]
   1514c:	b.gt	152b8 <__gmpz_bin_uiui@@Base+0x354>
   15150:	ldr	x21, [x21, #8]
   15154:	adrp	x27, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15158:	sub	w19, w26, #0x1
   1515c:	add	x27, x27, #0x9a0
   15160:	ldr	x8, [x27, w19, uxtw #3]
   15164:	mov	x0, x22
   15168:	blr	x8
   1516c:	adrp	x28, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15170:	add	x28, x28, #0x47f
   15174:	ldrb	w8, [x28, w19, uxtw]
   15178:	add	x23, x22, x26
   1517c:	sub	w19, w20, w26
   15180:	mov	w2, #0x1                   	// #1
   15184:	sub	w22, w24, w8
   15188:	str	x0, [x21]
   1518c:	cmp	w26, w19
   15190:	csel	w26, w26, w19, cc  // cc = lo, ul, last
   15194:	sub	w25, w26, #0x1
   15198:	ldr	x8, [x27, w25, uxtw #3]
   1519c:	mov	x0, x23
   151a0:	mov	x24, x2
   151a4:	blr	x8
   151a8:	ldrb	w8, [x28, w25, uxtw]
   151ac:	mov	x3, x0
   151b0:	mov	x0, x21
   151b4:	mov	x1, x21
   151b8:	mov	x2, x24
   151bc:	add	x23, x23, x26
   151c0:	sub	w22, w22, w8
   151c4:	bl	d490 <__gmpn_mul_1@plt>
   151c8:	cmp	x0, #0x0
   151cc:	cinc	x2, x24, ne  // ne = any
   151d0:	subs	w19, w19, w26
   151d4:	str	x0, [x21, x24, lsl #3]
   151d8:	b.ne	1518c <__gmpz_bin_uiui@@Base+0x228>  // b.any
   151dc:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   151e0:	ldr	x9, [x9, #3848]
   151e4:	adrp	x10, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   151e8:	lsl	x8, x20, #3
   151ec:	add	x10, x10, #0xf8
   151f0:	ldr	x3, [x9, x8]
   151f4:	add	x8, x8, x10
   151f8:	ldur	x4, [x8, #-16]
   151fc:	mov	x25, x0
   15200:	mov	x0, x21
   15204:	mov	x1, x21
   15208:	mov	w5, w22
   1520c:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   15210:	cmp	x25, #0x0
   15214:	cinc	x9, x24, ne  // ne = any
   15218:	add	x9, x21, x9, lsl #3
   1521c:	ldr	x21, [sp, #8]
   15220:	cinc	w8, w24, ne  // ne = any
   15224:	add	w8, w8, #0x1
   15228:	sub	x9, x9, #0x8
   1522c:	ldr	x10, [x9], #-8
   15230:	sub	w8, w8, #0x1
   15234:	cbz	x10, 1522c <__gmpz_bin_uiui@@Base+0x2c8>
   15238:	b	15294 <__gmpz_bin_uiui@@Base+0x330>
   1523c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15240:	sub	x19, x20, #0x1
   15244:	add	x8, x8, #0x9a0
   15248:	ldr	x8, [x8, x19, lsl #3]
   1524c:	mov	x0, x22
   15250:	blr	x8
   15254:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15258:	add	x8, x8, #0xf8
   1525c:	adrp	x9, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15260:	add	x9, x9, #0x47f
   15264:	add	x8, x8, x20, lsl #3
   15268:	ldrb	w9, [x9, x19]
   1526c:	ldur	x8, [x8, #-16]
   15270:	ldr	w10, [x21]
   15274:	sub	w9, w24, w9
   15278:	mul	x8, x8, x0
   1527c:	cmp	w10, #0x0
   15280:	lsr	x19, x8, x9
   15284:	b.le	152c8 <__gmpz_bin_uiui@@Base+0x364>
   15288:	ldr	x0, [x21, #8]
   1528c:	str	x19, [x0]
   15290:	mov	w8, #0x1                   	// #1
   15294:	str	w8, [x21, #4]
   15298:	ldp	x20, x19, [sp, #96]
   1529c:	ldp	x22, x21, [sp, #80]
   152a0:	ldp	x24, x23, [sp, #64]
   152a4:	ldp	x26, x25, [sp, #48]
   152a8:	ldp	x28, x27, [sp, #32]
   152ac:	ldp	x29, x30, [sp, #16]
   152b0:	add	sp, sp, #0x70
   152b4:	ret
   152b8:	mov	x0, x21
   152bc:	bl	c080 <__gmpz_realloc@plt>
   152c0:	mov	x21, x0
   152c4:	b	15154 <__gmpz_bin_uiui@@Base+0x1f0>
   152c8:	mov	w1, #0x1                   	// #1
   152cc:	mov	x0, x21
   152d0:	bl	c080 <__gmpz_realloc@plt>
   152d4:	b	1528c <__gmpz_bin_uiui@@Base+0x328>
   152d8:	sub	sp, sp, #0x190
   152dc:	stp	x20, x19, [sp, #384]
   152e0:	lsr	x20, x2, #1
   152e4:	stp	x22, x21, [sp, #368]
   152e8:	mov	x21, x2
   152ec:	mov	x22, x1
   152f0:	mov	x19, x0
   152f4:	cmp	x2, #0x33
   152f8:	mov	x2, x20
   152fc:	stp	x29, x30, [sp, #320]
   15300:	str	x28, [sp, #336]
   15304:	stp	x24, x23, [sp, #352]
   15308:	add	x29, sp, #0x140
   1530c:	b.hi	15318 <__gmpz_bin_uiui@@Base+0x3b4>  // b.pmore
   15310:	bl	150b4 <__gmpz_bin_uiui@@Base+0x150>
   15314:	b	1531c <__gmpz_bin_uiui@@Base+0x3b8>
   15318:	bl	152d8 <__gmpz_bin_uiui@@Base+0x374>
   1531c:	sub	x24, x22, x20
   15320:	cmp	x24, #0x43
   15324:	sub	x21, x21, x20
   15328:	b.hi	153c8 <__gmpz_bin_uiui@@Base+0x464>  // b.pmore
   1532c:	ldp	w8, w23, [x19]
   15330:	sxtw	x23, w23
   15334:	cmp	w23, w8
   15338:	b.ge	1548c <__gmpz_bin_uiui@@Base+0x528>  // b.tcont
   1533c:	ldr	x22, [x19, #8]
   15340:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15344:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15348:	ldr	x8, [x8, #3848]
   1534c:	adrp	x10, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15350:	sub	w11, w24, w21
   15354:	ldr	x13, [x13, #3992]
   15358:	sub	w9, w21, #0x2
   1535c:	add	x10, x10, #0xf8
   15360:	sub	w14, w11, #0x2
   15364:	ubfx	x12, x24, #1, #31
   15368:	ldr	x9, [x10, w9, uxtw #3]
   1536c:	ldr	x10, [x10, w14, uxtw #3]
   15370:	ubfx	x14, x21, #1, #31
   15374:	sub	w12, w12, #0x1
   15378:	sub	w14, w14, #0x1
   1537c:	lsr	w11, w11, #1
   15380:	ldr	x8, [x8, x24, lsl #3]
   15384:	ldrb	w12, [x13, w12, uxtw]
   15388:	ldrb	w14, [x13, w14, uxtw]
   1538c:	sub	w11, w11, #0x1
   15390:	ldrb	w11, [x13, w11, uxtw]
   15394:	mul	x8, x9, x8
   15398:	sub	w9, w12, w14
   1539c:	mul	x8, x8, x10
   153a0:	sub	w9, w9, w11
   153a4:	lsl	x3, x8, x9
   153a8:	mov	x0, x22
   153ac:	mov	x1, x22
   153b0:	mov	x2, x23
   153b4:	bl	d490 <__gmpn_mul_1@plt>
   153b8:	cmp	x0, #0x0
   153bc:	str	x0, [x22, x23, lsl #3]
   153c0:	cinc	x23, x23, ne  // ne = any
   153c4:	b	15410 <__gmpz_bin_uiui@@Base+0x4ac>
   153c8:	mov	w8, #0x26                  	// #38
   153cc:	add	x9, sp, #0x10
   153d0:	cmp	x21, #0x19
   153d4:	mov	x0, sp
   153d8:	mov	x1, x24
   153dc:	mov	x2, x21
   153e0:	str	w8, [sp]
   153e4:	str	x9, [sp, #8]
   153e8:	b.hi	153f4 <__gmpz_bin_uiui@@Base+0x490>  // b.pmore
   153ec:	bl	150b4 <__gmpz_bin_uiui@@Base+0x150>
   153f0:	b	153f8 <__gmpz_bin_uiui@@Base+0x494>
   153f4:	bl	152d8 <__gmpz_bin_uiui@@Base+0x374>
   153f8:	mov	x2, sp
   153fc:	mov	x0, x19
   15400:	mov	x1, x19
   15404:	bl	c4b0 <__gmpz_mul@plt>
   15408:	ldr	x22, [x19, #8]
   1540c:	ldrsw	x23, [x19, #4]
   15410:	adrp	x11, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15414:	sub	x8, x21, #0xd
   15418:	adrp	x9, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1541c:	adrp	x10, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   15420:	add	x11, x11, #0x468
   15424:	add	x9, x9, #0x2f8
   15428:	add	x10, x10, #0x3b0
   1542c:	lsl	x12, x8, #3
   15430:	ldrb	w8, [x11, x8]
   15434:	ldr	x3, [x9, x12]
   15438:	ldr	x4, [x10, x12]
   1543c:	cmp	x21, x20
   15440:	cset	w9, ne  // ne = any
   15444:	sub	w5, w8, w9
   15448:	mov	x0, x22
   1544c:	mov	x1, x22
   15450:	mov	x2, x23
   15454:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   15458:	sub	x8, x22, #0x8
   1545c:	ldr	x9, [x8, x23, lsl #3]
   15460:	sub	x23, x23, #0x1
   15464:	cbz	x9, 1545c <__gmpz_bin_uiui@@Base+0x4f8>
   15468:	add	w8, w23, #0x1
   1546c:	str	w8, [x19, #4]
   15470:	ldp	x20, x19, [sp, #384]
   15474:	ldp	x22, x21, [sp, #368]
   15478:	ldp	x24, x23, [sp, #352]
   1547c:	ldr	x28, [sp, #336]
   15480:	ldp	x29, x30, [sp, #320]
   15484:	add	sp, sp, #0x190
   15488:	ret
   1548c:	add	x1, x23, #0x1
   15490:	mov	x0, x19
   15494:	bl	c080 <__gmpz_realloc@plt>
   15498:	mov	x22, x0
   1549c:	b	15340 <__gmpz_bin_uiui@@Base+0x3dc>
   154a0:	stp	x29, x30, [sp, #-64]!
   154a4:	sub	x8, x1, #0x5
   154a8:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   154ac:	movk	x9, #0xaaab
   154b0:	orr	x8, x8, #0x1
   154b4:	str	x23, [sp, #16]
   154b8:	umulh	x23, x8, x9
   154bc:	lsr	x9, x23, #4
   154c0:	lsr	x8, x8, #11
   154c4:	and	x9, x9, #0xffffffffffffff8
   154c8:	stp	x22, x21, [sp, #32]
   154cc:	stp	x20, x19, [sp, #48]
   154d0:	mov	x29, sp
   154d4:	mov	x21, x2
   154d8:	mov	x22, x1
   154dc:	mov	x19, x0
   154e0:	cmp	x8, #0x17c
   154e4:	add	x1, x9, #0x8
   154e8:	str	xzr, [x29, #24]
   154ec:	b.hi	158e8 <__gmpz_bin_uiui@@Base+0x984>  // b.pmore
   154f0:	add	x9, x1, #0xf
   154f4:	mov	x8, sp
   154f8:	and	x9, x9, #0x3ffffffffffffff0
   154fc:	sub	x20, x8, x9
   15500:	mov	sp, x20
   15504:	mov	x0, x20
   15508:	mov	x1, x22
   1550c:	lsr	x23, x23, #1
   15510:	bl	d200 <__gmp_primesieve@plt>
   15514:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15518:	ldr	x8, [x8, #3880]
   1551c:	mov	w10, #0x9                   	// #9
   15520:	sub	w9, w10, #0x2
   15524:	ldr	x9, [x8, w9, uxtw #3]
   15528:	sub	w10, w10, #0x1
   1552c:	cmp	x9, x22
   15530:	b.cc	15520 <__gmpz_bin_uiui@@Base+0x5bc>  // b.lo, b.ul, b.last
   15534:	add	x9, x0, #0x1
   15538:	mov	w10, w10
   1553c:	udiv	x10, x9, x10
   15540:	lsl	x10, x10, #3
   15544:	add	x10, x10, #0x8
   15548:	mov	w11, #0x9                   	// #9
   1554c:	sub	w12, w11, #0x2
   15550:	ldr	x12, [x8, w12, uxtw #3]
   15554:	sub	w11, w11, #0x1
   15558:	cmp	x12, x22
   1555c:	b.cc	1554c <__gmpz_bin_uiui@@Base+0x5e8>  // b.lo, b.ul, b.last
   15560:	mov	w8, w11
   15564:	udiv	x8, x9, x8
   15568:	mov	w11, #0x7f00                	// #32512
   1556c:	lsl	x8, x8, #3
   15570:	cmp	x10, x11
   15574:	add	x1, x8, #0x8
   15578:	b.hi	158f8 <__gmpz_bin_uiui@@Base+0x994>  // b.pmore
   1557c:	add	x9, x1, #0xf
   15580:	mov	x8, sp
   15584:	and	x9, x9, #0xfffffffffffffff0
   15588:	sub	x1, x8, x9
   1558c:	mov	sp, x1
   15590:	lsr	x9, x21, #1
   15594:	and	x9, x9, #0x5555555555555555
   15598:	lsr	x12, x22, #1
   1559c:	sub	x9, x21, x9
   155a0:	mov	x8, #0xffffffffffffffff    	// #-1
   155a4:	sub	x10, x22, x21
   155a8:	and	x13, x12, #0x5555555555555555
   155ac:	lsr	x14, x9, #2
   155b0:	udiv	x11, x8, x22
   155b4:	lsr	x8, x10, #1
   155b8:	sub	x13, x22, x13
   155bc:	and	x9, x9, #0x3333333333333333
   155c0:	and	x14, x14, #0x3333333333333333
   155c4:	and	x8, x8, #0x5555555555555555
   155c8:	add	x9, x14, x9
   155cc:	lsr	x14, x13, #2
   155d0:	sub	x8, x10, x8
   155d4:	and	x13, x13, #0x3333333333333333
   155d8:	and	x14, x14, #0x3333333333333333
   155dc:	add	x13, x14, x13
   155e0:	lsr	x14, x8, #2
   155e4:	and	x8, x8, #0x3333333333333333
   155e8:	and	x14, x14, #0x3333333333333333
   155ec:	add	x9, x9, x9, lsr #4
   155f0:	add	x8, x14, x8
   155f4:	add	x13, x13, x13, lsr #4
   155f8:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   155fc:	add	x8, x8, x8, lsr #4
   15600:	and	x13, x13, #0xf0f0f0f0f0f0f0f
   15604:	add	x9, x9, x9, lsr #8
   15608:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   1560c:	add	x13, x13, x13, lsr #8
   15610:	add	x9, x9, x9, lsr #16
   15614:	add	x8, x8, x8, lsr #8
   15618:	add	x13, x13, x13, lsr #16
   1561c:	lsr	x14, x9, #32
   15620:	add	x8, x8, x8, lsr #16
   15624:	add	w9, w14, w9
   15628:	lsr	x14, x13, #32
   1562c:	add	w13, w14, w13
   15630:	lsr	x14, x8, #32
   15634:	and	x9, x9, #0xff
   15638:	add	w8, w14, w8
   1563c:	sub	x9, x9, w13, uxtb
   15640:	add	x8, x9, w8, uxtb
   15644:	mov	w9, #0x1                   	// #1
   15648:	lsl	x8, x9, x8
   1564c:	cmp	x8, x11
   15650:	b.ls	15664 <__gmpz_bin_uiui@@Base+0x700>  // b.plast
   15654:	str	x8, [x1]
   15658:	mov	w9, #0x1                   	// #1
   1565c:	mov	w8, #0x1                   	// #1
   15660:	b	15668 <__gmpz_bin_uiui@@Base+0x704>
   15664:	mov	x9, xzr
   15668:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1566c:	mov	x14, xzr
   15670:	movk	x13, #0xaaab
   15674:	mov	x16, x21
   15678:	mov	x15, x22
   1567c:	umulh	x17, x16, x13
   15680:	lsr	x17, x17, #1
   15684:	add	x18, x17, x17, lsl #1
   15688:	sub	x16, x16, x18
   1568c:	umulh	x18, x15, x13
   15690:	lsr	x18, x18, #1
   15694:	add	x14, x16, x14
   15698:	add	x16, x18, x18, lsl #1
   1569c:	sub	x16, x15, x16
   156a0:	cmp	x16, x14
   156a4:	add	x16, x8, x8, lsl #1
   156a8:	cset	w14, cc  // cc = lo, ul, last
   156ac:	csel	x8, x16, x8, cc  // cc = lo, ul, last
   156b0:	cmp	x15, #0x8
   156b4:	mov	x16, x17
   156b8:	mov	x15, x18
   156bc:	b.hi	1567c <__gmpz_bin_uiui@@Base+0x718>  // b.pmore
   156c0:	clz	x14, x22
   156c4:	mov	w15, #0x40                  	// #64
   156c8:	sub	w14, w15, w14
   156cc:	mov	w17, #0x1                   	// #1
   156d0:	asr	w14, w14, #1
   156d4:	lsl	x15, x17, x14
   156d8:	lsr	x14, x22, x14
   156dc:	add	x14, x15, x14
   156e0:	lsr	x14, x14, #1
   156e4:	mov	x15, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   156e8:	sub	x14, x14, #0x5
   156ec:	movk	x15, #0xaaab
   156f0:	orr	x14, x14, #0x1
   156f4:	umulh	x14, x14, x15
   156f8:	mov	x13, xzr
   156fc:	mov	x16, xzr
   15700:	lsr	x18, x14, #1
   15704:	mov	w2, #0x7                   	// #7
   15708:	b	15724 <__gmpz_bin_uiui@@Base+0x7c0>
   1570c:	ror	x0, x17, #63
   15710:	add	x13, x13, x17, lsr #63
   15714:	cmp	x14, x18
   15718:	add	x2, x15, #0x3
   1571c:	mov	x17, x0
   15720:	b.cs	157a8 <__gmpz_bin_uiui@@Base+0x844>  // b.hs, b.nlast
   15724:	ldr	x0, [x20, x13, lsl #3]
   15728:	mov	x14, x16
   1572c:	mov	x15, x2
   15730:	add	x16, x16, #0x1
   15734:	tst	x0, x17
   15738:	b.ne	1570c <__gmpz_bin_uiui@@Base+0x7a8>  // b.any
   1573c:	add	x0, x16, x16, lsl #1
   15740:	and	x2, x16, #0x1
   15744:	cmp	x8, x11
   15748:	add	x2, x0, x2
   1574c:	b.ls	15760 <__gmpz_bin_uiui@@Base+0x7fc>  // b.plast
   15750:	add	x0, x9, #0x1
   15754:	str	x8, [x1, x9, lsl #3]
   15758:	mov	x9, x0
   1575c:	mov	w8, #0x1                   	// #1
   15760:	mov	x0, xzr
   15764:	add	x2, x2, #0x1
   15768:	mov	x3, x22
   1576c:	mov	x4, x21
   15770:	udiv	x5, x4, x2
   15774:	udiv	x6, x3, x2
   15778:	msub	x4, x5, x2, x4
   1577c:	msub	x3, x6, x2, x3
   15780:	add	x0, x4, x0
   15784:	cmp	x3, x0
   15788:	csinc	x3, x2, xzr, cc  // cc = lo, ul, last
   1578c:	cset	w0, cc  // cc = lo, ul, last
   15790:	cmp	x6, x2
   15794:	mul	x8, x3, x8
   15798:	mov	x3, x6
   1579c:	mov	x4, x5
   157a0:	b.cs	15770 <__gmpz_bin_uiui@@Base+0x80c>  // b.hs, b.nlast
   157a4:	b	1570c <__gmpz_bin_uiui@@Base+0x7a8>
   157a8:	sub	x12, x12, #0x5
   157ac:	mov	x17, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   157b0:	orr	x12, x12, #0x1
   157b4:	movk	x17, #0xaaab
   157b8:	umulh	x12, x12, x17
   157bc:	lsl	x16, x11, #1
   157c0:	lsr	x12, x12, #1
   157c4:	b	157e4 <__gmpz_bin_uiui@@Base+0x880>
   157c8:	mul	x8, x17, x8
   157cc:	add	x14, x14, #0x1
   157d0:	add	x13, x13, x0, lsr #63
   157d4:	ror	x0, x0, #63
   157d8:	cmp	x14, x12
   157dc:	add	x15, x15, #0x3
   157e0:	b.cs	1582c <__gmpz_bin_uiui@@Base+0x8c8>  // b.hs, b.nlast
   157e4:	ldr	x17, [x20, x13, lsl #3]
   157e8:	tst	x17, x0
   157ec:	b.ne	157cc <__gmpz_bin_uiui@@Base+0x868>  // b.any
   157f0:	and	x17, x14, #0x1
   157f4:	add	x17, x15, x17
   157f8:	udiv	x18, x22, x17
   157fc:	udiv	x2, x21, x17
   15800:	msub	x18, x18, x17, x22
   15804:	msub	x2, x2, x17, x21
   15808:	cmp	x18, x2
   1580c:	b.cs	157cc <__gmpz_bin_uiui@@Base+0x868>  // b.hs, b.nlast
   15810:	cmp	x8, x16
   15814:	b.ls	157c8 <__gmpz_bin_uiui@@Base+0x864>  // b.plast
   15818:	add	x18, x9, #0x1
   1581c:	str	x8, [x1, x9, lsl #3]
   15820:	mov	x9, x18
   15824:	mov	x8, x17
   15828:	b	157cc <__gmpz_bin_uiui@@Base+0x868>
   1582c:	sub	x10, x10, #0x5
   15830:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   15834:	movk	x12, #0xaaab
   15838:	orr	x10, x10, #0x1
   1583c:	umulh	x10, x10, x12
   15840:	lsr	x12, x10, #1
   15844:	mov	w13, #0x1                   	// #1
   15848:	add	x14, x12, #0x1
   1584c:	add	x15, x12, x12, lsl #1
   15850:	and	x11, x11, #0x7fffffffffffffff
   15854:	add	x10, x12, #0x2
   15858:	lsr	x12, x14, #6
   1585c:	lsl	x14, x13, x14
   15860:	add	x13, x15, #0x7
   15864:	b	15884 <__gmpz_bin_uiui@@Base+0x920>
   15868:	mul	x8, x15, x8
   1586c:	add	x12, x12, x14, lsr #63
   15870:	ror	x14, x14, #63
   15874:	cmp	x10, x23
   15878:	add	x10, x10, #0x1
   1587c:	add	x13, x13, #0x3
   15880:	b.hi	158b4 <__gmpz_bin_uiui@@Base+0x950>  // b.pmore
   15884:	ldr	x15, [x20, x12, lsl #3]
   15888:	tst	x15, x14
   1588c:	b.ne	1586c <__gmpz_bin_uiui@@Base+0x908>  // b.any
   15890:	and	x15, x10, #0x1
   15894:	cmp	x8, x11
   15898:	add	x15, x13, x15
   1589c:	b.ls	15868 <__gmpz_bin_uiui@@Base+0x904>  // b.plast
   158a0:	add	x16, x9, #0x1
   158a4:	str	x8, [x1, x9, lsl #3]
   158a8:	mov	x9, x16
   158ac:	mov	x8, x15
   158b0:	b	1586c <__gmpz_bin_uiui@@Base+0x908>
   158b4:	cbz	x9, 15908 <__gmpz_bin_uiui@@Base+0x9a4>
   158b8:	add	x2, x9, #0x1
   158bc:	mov	x0, x19
   158c0:	str	x8, [x1, x9, lsl #3]
   158c4:	bl	cd70 <__gmpz_prodlimbs@plt>
   158c8:	ldr	x0, [x29, #24]
   158cc:	cbnz	x0, 1592c <__gmpz_bin_uiui@@Base+0x9c8>
   158d0:	mov	sp, x29
   158d4:	ldp	x20, x19, [sp, #48]
   158d8:	ldp	x22, x21, [sp, #32]
   158dc:	ldr	x23, [sp, #16]
   158e0:	ldp	x29, x30, [sp], #64
   158e4:	ret
   158e8:	add	x0, x29, #0x18
   158ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   158f0:	mov	x20, x0
   158f4:	b	15504 <__gmpz_bin_uiui@@Base+0x5a0>
   158f8:	add	x0, x29, #0x18
   158fc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15900:	mov	x1, x0
   15904:	b	15590 <__gmpz_bin_uiui@@Base+0x62c>
   15908:	ldr	w9, [x19]
   1590c:	cmp	w9, #0x0
   15910:	b.le	15934 <__gmpz_bin_uiui@@Base+0x9d0>
   15914:	ldr	x0, [x19, #8]
   15918:	str	x8, [x0]
   1591c:	mov	w8, #0x1                   	// #1
   15920:	str	w8, [x19, #4]
   15924:	ldr	x0, [x29, #24]
   15928:	cbz	x0, 158d0 <__gmpz_bin_uiui@@Base+0x96c>
   1592c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   15930:	b	158d0 <__gmpz_bin_uiui@@Base+0x96c>
   15934:	mov	w1, #0x1                   	// #1
   15938:	mov	x0, x19
   1593c:	mov	x20, x8
   15940:	bl	c080 <__gmpz_realloc@plt>
   15944:	mov	x8, x20
   15948:	b	15918 <__gmpz_bin_uiui@@Base+0x9b4>
   1594c:	stp	x29, x30, [sp, #-96]!
   15950:	stp	x28, x27, [sp, #16]
   15954:	stp	x26, x25, [sp, #32]
   15958:	stp	x24, x23, [sp, #48]
   1595c:	stp	x22, x21, [sp, #64]
   15960:	stp	x20, x19, [sp, #80]
   15964:	mov	x29, sp
   15968:	sub	sp, sp, #0x40
   1596c:	lsr	x8, x1, #6
   15970:	add	x8, x8, x8, lsl #1
   15974:	add	x10, x8, #0x3
   15978:	cmp	x8, #0x27
   1597c:	lsr	x8, x10, #1
   15980:	mov	w9, #0x27                  	// #39
   15984:	add	x8, x8, #0x13
   15988:	csel	x8, x9, x8, cc  // cc = lo, ul, last
   1598c:	cmp	x8, x2
   15990:	csel	x19, x8, x2, lt  // lt = tstop
   15994:	lsl	x8, x19, #3
   15998:	mov	x21, x1
   1599c:	add	x1, x8, #0xb0
   159a0:	mov	w8, #0x7f00                	// #32512
   159a4:	mov	x22, x2
   159a8:	cmp	x1, x8
   159ac:	stur	x0, [x29, #-48]
   159b0:	stur	xzr, [x29, #-8]
   159b4:	b.hi	15d80 <__gmpz_bin_uiui@@Base+0xe1c>  // b.pmore
   159b8:	add	x9, x1, #0xf
   159bc:	mov	x8, sp
   159c0:	and	x9, x9, #0xfffffffffffffff0
   159c4:	sub	x20, x8, x9
   159c8:	mov	sp, x20
   159cc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   159d0:	ldr	x8, [x8, #3880]
   159d4:	add	x9, x19, #0x1
   159d8:	mov	w11, #0x9                   	// #9
   159dc:	sub	w10, w11, #0x2
   159e0:	ldr	x10, [x8, w10, uxtw #3]
   159e4:	sub	w11, w11, #0x1
   159e8:	cmp	x10, x21
   159ec:	b.cc	159dc <__gmpz_bin_uiui@@Base+0xa78>  // b.lo, b.ul, b.last
   159f0:	add	x24, x20, x9, lsl #3
   159f4:	mov	w27, #0x9                   	// #9
   159f8:	stur	w11, [x29, #-36]
   159fc:	sub	w9, w27, #0x2
   15a00:	ldr	x9, [x8, w9, uxtw #3]
   15a04:	sub	w27, w27, #0x1
   15a08:	cmp	x9, x22
   15a0c:	b.cc	159fc <__gmpz_bin_uiui@@Base+0xa98>  // b.lo, b.ul, b.last
   15a10:	mov	x9, #0x41ef                	// #16879
   15a14:	movk	x9, #0x7ec2, lsl #16
   15a18:	stur	x21, [x29, #-56]
   15a1c:	sub	x10, x21, x22
   15a20:	movk	x9, #0x8186, lsl #32
   15a24:	adrp	x21, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15a28:	mov	w23, #0x1                   	// #1
   15a2c:	movk	x9, #0x3352, lsl #48
   15a30:	mov	w8, #0x1a                  	// #26
   15a34:	add	x21, x21, #0x9a0
   15a38:	add	x25, x10, #0x1
   15a3c:	mov	w28, #0x1                   	// #1
   15a40:	stur	x10, [x29, #-64]
   15a44:	str	x23, [x20]
   15a48:	stp	x24, x22, [x29, #-32]
   15a4c:	sub	x10, x22, x8
   15a50:	add	x11, x10, #0x1
   15a54:	mov	w12, w27
   15a58:	cmp	x11, x12
   15a5c:	csinc	x19, x12, x10, hi  // hi = pmore
   15a60:	str	x9, [x24]
   15a64:	cbz	x19, 15ae0 <__gmpz_bin_uiui@@Base+0xb7c>
   15a68:	mov	w27, #0x1                   	// #1
   15a6c:	mov	x26, x8
   15a70:	sub	w8, w19, #0x1
   15a74:	ldr	x8, [x21, w8, uxtw #3]
   15a78:	mov	x0, x26
   15a7c:	blr	x8
   15a80:	rbit	x8, x0
   15a84:	clz	x8, x8
   15a88:	and	x19, x19, #0xffffffff
   15a8c:	lsr	x3, x0, x8
   15a90:	mov	x0, x24
   15a94:	mov	x1, x24
   15a98:	mov	x2, x27
   15a9c:	add	x26, x19, x26
   15aa0:	bl	d490 <__gmpn_mul_1@plt>
   15aa4:	sub	x8, x22, x26
   15aa8:	cmp	x0, #0x0
   15aac:	add	x9, x8, #0x1
   15ab0:	str	x0, [x24, x27, lsl #3]
   15ab4:	cinc	x27, x27, ne  // ne = any
   15ab8:	cmp	x19, x9
   15abc:	csinc	x19, x19, x8, cc  // cc = lo, ul, last
   15ac0:	cmp	x27, #0x13
   15ac4:	b.hi	15acc <__gmpz_bin_uiui@@Base+0xb68>  // b.pmore
   15ac8:	cbnz	x19, 15a70 <__gmpz_bin_uiui@@Base+0xb0c>
   15acc:	mov	w8, w19
   15ad0:	stur	w19, [x29, #-12]
   15ad4:	subs	w22, w26, w28
   15ad8:	b.ne	15af4 <__gmpz_bin_uiui@@Base+0xb90>  // b.any
   15adc:	b	15b4c <__gmpz_bin_uiui@@Base+0xbe8>
   15ae0:	stur	wzr, [x29, #-12]
   15ae4:	mov	x26, x8
   15ae8:	mov	w27, #0x1                   	// #1
   15aec:	subs	w22, w26, w28
   15af0:	b.eq	15b4c <__gmpz_bin_uiui@@Base+0xbe8>  // b.none
   15af4:	ldur	w24, [x29, #-36]
   15af8:	adrp	x28, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15afc:	add	x28, x28, #0x9a0
   15b00:	cmp	w24, w22
   15b04:	csel	w21, w24, w22, cc  // cc = lo, ul, last
   15b08:	sub	w8, w21, #0x1
   15b0c:	ldr	x8, [x28, w8, uxtw #3]
   15b10:	mov	x0, x25
   15b14:	blr	x8
   15b18:	rbit	x8, x0
   15b1c:	clz	x8, x8
   15b20:	lsr	x3, x0, x8
   15b24:	mov	x0, x20
   15b28:	mov	x1, x20
   15b2c:	mov	x2, x23
   15b30:	add	x25, x25, x21
   15b34:	bl	d490 <__gmpn_mul_1@plt>
   15b38:	cmp	x0, #0x0
   15b3c:	str	x0, [x20, x23, lsl #3]
   15b40:	cinc	x23, x23, ne  // ne = any
   15b44:	subs	w22, w22, w21
   15b48:	b.ne	15b00 <__gmpz_bin_uiui@@Base+0xb9c>  // b.any
   15b4c:	ldur	x24, [x29, #-32]
   15b50:	add	x9, x20, x23, lsl #3
   15b54:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15b58:	ldur	x9, [x9, #-8]
   15b5c:	ldr	x8, [x24]
   15b60:	add	x10, x24, x27, lsl #3
   15b64:	ldur	x10, [x10, #-8]
   15b68:	ldr	x13, [x13, #3952]
   15b6c:	ubfx	x12, x8, #1, #7
   15b70:	sub	x11, x23, x27
   15b74:	cmp	x9, x10
   15b78:	ldrb	w12, [x13, x12]
   15b7c:	mov	w10, #0x2                   	// #2
   15b80:	cinc	x23, x11, cs  // cs = hs, nlast
   15b84:	cmp	x27, x23
   15b88:	msub	x9, x8, x12, x10
   15b8c:	mul	x9, x9, x12
   15b90:	msub	x10, x9, x8, x10
   15b94:	mul	x9, x9, x10
   15b98:	orr	x10, xzr, #0xfffffffffffffffe
   15b9c:	madd	x8, x9, x8, x10
   15ba0:	csel	x4, x27, x23, lt  // lt = tstop
   15ba4:	mul	x5, x8, x9
   15ba8:	mov	x0, x20
   15bac:	mov	x1, x20
   15bb0:	mov	x2, x23
   15bb4:	mov	x3, x24
   15bb8:	bl	c510 <__gmpn_sbpi1_bdiv_q@plt>
   15bbc:	ldr	x10, [x20]
   15bc0:	cbz	x10, 15be0 <__gmpz_bin_uiui@@Base+0xc7c>
   15bc4:	ldur	x22, [x29, #-24]
   15bc8:	ldur	w27, [x29, #-12]
   15bcc:	adrp	x21, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15bd0:	mov	x8, x20
   15bd4:	mov	x9, x23
   15bd8:	add	x21, x21, #0x9a0
   15bdc:	b	15c0c <__gmpz_bin_uiui@@Base+0xca8>
   15be0:	ldur	x22, [x29, #-24]
   15be4:	ldur	w27, [x29, #-12]
   15be8:	adrp	x21, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   15bec:	mov	x9, x23
   15bf0:	mov	x8, x20
   15bf4:	add	x21, x21, #0x9a0
   15bf8:	subs	x9, x9, #0x1
   15bfc:	str	xzr, [x8]
   15c00:	b.eq	15c28 <__gmpz_bin_uiui@@Base+0xcc4>  // b.none
   15c04:	ldr	x10, [x8, #8]!
   15c08:	cbz	x10, 15bf8 <__gmpz_bin_uiui@@Base+0xc94>
   15c0c:	neg	x10, x10
   15c10:	subs	x2, x9, #0x1
   15c14:	str	x10, [x8]
   15c18:	b.eq	15c28 <__gmpz_bin_uiui@@Base+0xcc4>  // b.none
   15c1c:	add	x0, x8, #0x8
   15c20:	mov	x1, x0
   15c24:	bl	c290 <__gmpn_com@plt>
   15c28:	cbz	w27, 15c54 <__gmpz_bin_uiui@@Base+0xcf0>
   15c2c:	sub	w8, w19, #0x1
   15c30:	ldr	x8, [x21, w8, uxtw #3]
   15c34:	mov	x0, x26
   15c38:	blr	x8
   15c3c:	rbit	x9, x0
   15c40:	clz	x9, x9
   15c44:	add	x8, x26, w19, uxtw
   15c48:	lsr	x9, x0, x9
   15c4c:	mov	x28, x26
   15c50:	b	15a4c <__gmpz_bin_uiui@@Base+0xae8>
   15c54:	ldp	x9, x11, [x29, #-64]
   15c58:	lsr	x8, x9, #1
   15c5c:	and	x8, x8, #0x5555555555555555
   15c60:	lsr	x10, x11, #1
   15c64:	sub	x8, x9, x8
   15c68:	lsr	x9, x22, #1
   15c6c:	and	x10, x10, #0x5555555555555555
   15c70:	and	x9, x9, #0x5555555555555555
   15c74:	sub	x10, x11, x10
   15c78:	lsr	x11, x8, #2
   15c7c:	sub	x9, x22, x9
   15c80:	and	x8, x8, #0x3333333333333333
   15c84:	and	x11, x11, #0x3333333333333333
   15c88:	add	x8, x11, x8
   15c8c:	lsr	x11, x9, #2
   15c90:	and	x9, x9, #0x3333333333333333
   15c94:	and	x11, x11, #0x3333333333333333
   15c98:	add	x9, x11, x9
   15c9c:	lsr	x11, x10, #2
   15ca0:	and	x10, x10, #0x3333333333333333
   15ca4:	and	x11, x11, #0x3333333333333333
   15ca8:	add	x8, x8, x8, lsr #4
   15cac:	add	x10, x11, x10
   15cb0:	add	x9, x9, x9, lsr #4
   15cb4:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   15cb8:	add	x10, x10, x10, lsr #4
   15cbc:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   15cc0:	add	x8, x8, x8, lsr #8
   15cc4:	and	x10, x10, #0xf0f0f0f0f0f0f0f
   15cc8:	add	x9, x9, x9, lsr #8
   15ccc:	add	x8, x8, x8, lsr #16
   15cd0:	add	x10, x10, x10, lsr #8
   15cd4:	add	x9, x9, x9, lsr #16
   15cd8:	lsr	x11, x8, #32
   15cdc:	add	x10, x10, x10, lsr #16
   15ce0:	add	w8, w11, w8
   15ce4:	lsr	x11, x9, #32
   15ce8:	add	w9, w11, w9
   15cec:	lsr	x11, x10, #32
   15cf0:	and	w9, w9, #0xff
   15cf4:	add	w10, w11, w10
   15cf8:	sub	w9, w9, w10, uxtb
   15cfc:	adds	w3, w9, w8, uxtb
   15d00:	b.eq	15d20 <__gmpz_bin_uiui@@Base+0xdbc>  // b.none
   15d04:	mov	x0, x20
   15d08:	mov	x1, x20
   15d0c:	mov	x2, x23
   15d10:	bl	c180 <__gmpn_lshift@plt>
   15d14:	cmp	x0, #0x0
   15d18:	str	x0, [x20, x23, lsl #3]
   15d1c:	cinc	x23, x23, ne  // ne = any
   15d20:	ldur	x19, [x29, #-48]
   15d24:	add	x8, x20, x23, lsl #3
   15d28:	ldur	x8, [x8, #-8]
   15d2c:	ldrsw	x9, [x19]
   15d30:	cmp	x8, #0x0
   15d34:	cset	w8, eq  // eq = none
   15d38:	sub	x21, x23, x8
   15d3c:	cmp	x21, x9
   15d40:	b.gt	15d90 <__gmpz_bin_uiui@@Base+0xe2c>
   15d44:	ldr	x0, [x19, #8]
   15d48:	mov	x1, x20
   15d4c:	mov	x2, x21
   15d50:	str	w21, [x19, #4]
   15d54:	bl	ca50 <__gmpn_copyi@plt>
   15d58:	ldur	x0, [x29, #-8]
   15d5c:	cbnz	x0, 15da0 <__gmpz_bin_uiui@@Base+0xe3c>
   15d60:	mov	sp, x29
   15d64:	ldp	x20, x19, [sp, #80]
   15d68:	ldp	x22, x21, [sp, #64]
   15d6c:	ldp	x24, x23, [sp, #48]
   15d70:	ldp	x26, x25, [sp, #32]
   15d74:	ldp	x28, x27, [sp, #16]
   15d78:	ldp	x29, x30, [sp], #96
   15d7c:	ret
   15d80:	sub	x0, x29, #0x8
   15d84:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15d88:	mov	x20, x0
   15d8c:	b	159cc <__gmpz_bin_uiui@@Base+0xa68>
   15d90:	mov	x0, x19
   15d94:	mov	x1, x21
   15d98:	bl	c080 <__gmpz_realloc@plt>
   15d9c:	b	15d48 <__gmpz_bin_uiui@@Base+0xde4>
   15da0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   15da4:	b	15d60 <__gmpz_bin_uiui@@Base+0xdfc>
   15da8:	ret
   15dac:	add	x9, x0, #0x1
   15db0:	orr	x8, x0, #0x1
   15db4:	lsr	x9, x9, #1
   15db8:	mul	x0, x9, x8
   15dbc:	ret
   15dc0:	add	x8, x0, #0x1
   15dc4:	mul	x8, x8, x0
   15dc8:	lsr	x8, x8, #1
   15dcc:	add	x9, x0, #0x2
   15dd0:	mul	x0, x8, x9
   15dd4:	ret
   15dd8:	add	x8, x0, #0x3
   15ddc:	mul	x8, x8, x0
   15de0:	lsr	x8, x8, #1
   15de4:	add	x9, x8, #0x1
   15de8:	mul	x0, x9, x8
   15dec:	ret
   15df0:	add	x8, x0, #0x3
   15df4:	mul	x8, x8, x0
   15df8:	add	x9, x0, #0x4
   15dfc:	lsr	x8, x8, #1
   15e00:	mul	x9, x8, x9
   15e04:	add	x8, x8, #0x1
   15e08:	mul	x0, x9, x8
   15e0c:	ret
   15e10:	add	x8, x0, #0x5
   15e14:	mul	x8, x8, x0
   15e18:	add	x9, x8, #0x5
   15e1c:	mul	x9, x9, x9
   15e20:	lsr	x9, x9, #3
   15e24:	lsr	x8, x8, #1
   15e28:	mul	x0, x9, x8
   15e2c:	ret
   15e30:	add	x8, x0, #0x5
   15e34:	mul	x8, x8, x0
   15e38:	add	x9, x0, #0x6
   15e3c:	add	x10, x8, #0x5
   15e40:	mul	x8, x8, x9
   15e44:	mul	x9, x10, x10
   15e48:	lsr	x9, x9, #3
   15e4c:	lsr	x8, x8, #1
   15e50:	mul	x0, x9, x8
   15e54:	ret
   15e58:	add	x8, x0, #0x7
   15e5c:	mul	x8, x8, x0
   15e60:	add	x9, x8, #0xa
   15e64:	mul	x9, x9, x8
   15e68:	lsr	x9, x9, #3
   15e6c:	add	x8, x8, x9
   15e70:	add	x8, x8, #0x9
   15e74:	mul	x0, x8, x9
   15e78:	ret

0000000000015e7c <__gmpz_cdiv_q@@Base>:
   15e7c:	stp	x29, x30, [sp, #-64]!
   15e80:	str	x23, [sp, #16]
   15e84:	stp	x22, x21, [sp, #32]
   15e88:	stp	x20, x19, [sp, #48]
   15e8c:	mov	x29, sp
   15e90:	sub	sp, sp, #0x10
   15e94:	ldrsw	x22, [x2, #4]
   15e98:	ldr	w23, [x1, #4]
   15e9c:	mov	x20, x2
   15ea0:	mov	x21, x1
   15ea4:	cmp	x22, #0x0
   15ea8:	cneg	x8, x22, mi  // mi = first
   15eac:	mov	x19, x0
   15eb0:	cmp	x8, #0xfe0
   15eb4:	lsl	x1, x8, #3
   15eb8:	str	xzr, [x29, #24]
   15ebc:	stur	w8, [x29, #-16]
   15ec0:	b.hi	15f30 <__gmpz_cdiv_q@@Base+0xb4>  // b.pmore
   15ec4:	add	x9, x1, #0xf
   15ec8:	mov	x8, sp
   15ecc:	and	x9, x9, #0xfffffffffffffff0
   15ed0:	sub	x0, x8, x9
   15ed4:	mov	sp, x0
   15ed8:	stur	x0, [x29, #-8]
   15edc:	sub	x1, x29, #0x10
   15ee0:	mov	x0, x19
   15ee4:	mov	x2, x21
   15ee8:	mov	x3, x20
   15eec:	bl	bff0 <__gmpz_tdiv_qr@plt>
   15ef0:	eor	w8, w22, w23
   15ef4:	tbnz	w8, #31, 15f10 <__gmpz_cdiv_q@@Base+0x94>
   15ef8:	ldur	w8, [x29, #-12]
   15efc:	cbz	w8, 15f10 <__gmpz_cdiv_q@@Base+0x94>
   15f00:	mov	w2, #0x1                   	// #1
   15f04:	mov	x0, x19
   15f08:	mov	x1, x19
   15f0c:	bl	c8b0 <__gmpz_add_ui@plt>
   15f10:	ldr	x0, [x29, #24]
   15f14:	cbnz	x0, 15f3c <__gmpz_cdiv_q@@Base+0xc0>
   15f18:	mov	sp, x29
   15f1c:	ldp	x20, x19, [sp, #48]
   15f20:	ldp	x22, x21, [sp, #32]
   15f24:	ldr	x23, [sp, #16]
   15f28:	ldp	x29, x30, [sp], #64
   15f2c:	ret
   15f30:	add	x0, x29, #0x18
   15f34:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   15f38:	b	15ed8 <__gmpz_cdiv_q@@Base+0x5c>
   15f3c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   15f40:	b	15f18 <__gmpz_cdiv_q@@Base+0x9c>

0000000000015f44 <__gmpz_cdiv_q_ui@@Base>:
   15f44:	stp	x29, x30, [sp, #-64]!
   15f48:	stp	x24, x23, [sp, #16]
   15f4c:	stp	x22, x21, [sp, #32]
   15f50:	stp	x20, x19, [sp, #48]
   15f54:	mov	x29, sp
   15f58:	cbz	x2, 16014 <__gmpz_cdiv_q_ui@@Base+0xd0>
   15f5c:	ldrsw	x24, [x1, #4]
   15f60:	mov	x23, x1
   15f64:	mov	x19, x0
   15f68:	cbz	w24, 15fe0 <__gmpz_cdiv_q_ui@@Base+0x9c>
   15f6c:	ldrsw	x8, [x19]
   15f70:	cmp	w24, #0x0
   15f74:	cneg	x21, x24, lt  // lt = tstop
   15f78:	mov	x20, x2
   15f7c:	cmp	x21, x8
   15f80:	b.gt	16000 <__gmpz_cdiv_q_ui@@Base+0xbc>
   15f84:	ldr	x22, [x19, #8]
   15f88:	ldr	x2, [x23, #8]
   15f8c:	mov	x0, x22
   15f90:	mov	x1, xzr
   15f94:	mov	x3, x21
   15f98:	mov	x4, x20
   15f9c:	bl	cd00 <__gmpn_divrem_1@plt>
   15fa0:	tbnz	w24, #31, 15fc0 <__gmpz_cdiv_q_ui@@Base+0x7c>
   15fa4:	cbz	x0, 15fc0 <__gmpz_cdiv_q_ui@@Base+0x7c>
   15fa8:	mov	x8, x22
   15fac:	ldr	x9, [x8]
   15fb0:	adds	x9, x9, #0x1
   15fb4:	str	x9, [x8], #8
   15fb8:	b.cs	15fac <__gmpz_cdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   15fbc:	sub	x0, x20, x0
   15fc0:	add	x8, x22, x21, lsl #3
   15fc4:	ldur	x8, [x8, #-8]
   15fc8:	cmp	x8, #0x0
   15fcc:	cset	w8, eq  // eq = none
   15fd0:	sub	w8, w21, w8
   15fd4:	cmp	w24, #0x0
   15fd8:	cneg	w8, w8, lt  // lt = tstop
   15fdc:	b	15fe8 <__gmpz_cdiv_q_ui@@Base+0xa4>
   15fe0:	mov	w8, wzr
   15fe4:	mov	x0, xzr
   15fe8:	str	w8, [x19, #4]
   15fec:	ldp	x20, x19, [sp, #48]
   15ff0:	ldp	x22, x21, [sp, #32]
   15ff4:	ldp	x24, x23, [sp, #16]
   15ff8:	ldp	x29, x30, [sp], #64
   15ffc:	ret
   16000:	mov	x0, x19
   16004:	mov	x1, x21
   16008:	bl	c080 <__gmpz_realloc@plt>
   1600c:	mov	x22, x0
   16010:	b	15f88 <__gmpz_cdiv_q_ui@@Base+0x44>
   16014:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000016018 <__gmpz_cdiv_qr@@Base>:
   16018:	stp	x29, x30, [sp, #-64]!
   1601c:	str	x23, [sp, #16]
   16020:	stp	x22, x21, [sp, #32]
   16024:	stp	x20, x19, [sp, #48]
   16028:	mov	x29, sp
   1602c:	sub	sp, sp, #0x10
   16030:	ldrsw	x23, [x3, #4]
   16034:	mov	x20, x3
   16038:	mov	x22, x2
   1603c:	mov	x19, x1
   16040:	mov	x21, x0
   16044:	cmp	x0, x3
   16048:	str	xzr, [x29, #24]
   1604c:	b.eq	16058 <__gmpz_cdiv_qr@@Base+0x40>  // b.none
   16050:	cmp	x19, x20
   16054:	b.ne	16098 <__gmpz_cdiv_qr@@Base+0x80>  // b.any
   16058:	cmp	x23, #0x0
   1605c:	cneg	x8, x23, mi  // mi = first
   16060:	cmp	x8, #0xfe0
   16064:	lsl	x1, x8, #3
   16068:	stur	w8, [x29, #-16]
   1606c:	b.hi	16108 <__gmpz_cdiv_qr@@Base+0xf0>  // b.pmore
   16070:	add	x9, x1, #0xf
   16074:	mov	x8, sp
   16078:	and	x9, x9, #0xfffffffffffffff0
   1607c:	sub	x0, x8, x9
   16080:	mov	sp, x0
   16084:	stur	x0, [x29, #-8]
   16088:	sub	x0, x29, #0x10
   1608c:	mov	x1, x20
   16090:	bl	c420 <__gmpz_set@plt>
   16094:	sub	x20, x29, #0x10
   16098:	ldr	w8, [x22, #4]
   1609c:	mov	x0, x21
   160a0:	mov	x1, x19
   160a4:	mov	x2, x22
   160a8:	mov	x3, x20
   160ac:	eor	w23, w8, w23
   160b0:	bl	bff0 <__gmpz_tdiv_qr@plt>
   160b4:	tbnz	w23, #31, 160e0 <__gmpz_cdiv_qr@@Base+0xc8>
   160b8:	ldr	w8, [x19, #4]
   160bc:	cbz	w8, 160e0 <__gmpz_cdiv_qr@@Base+0xc8>
   160c0:	mov	w2, #0x1                   	// #1
   160c4:	mov	x0, x21
   160c8:	mov	x1, x21
   160cc:	bl	c8b0 <__gmpz_add_ui@plt>
   160d0:	mov	x0, x19
   160d4:	mov	x1, x19
   160d8:	mov	x2, x20
   160dc:	bl	c260 <__gmpz_sub@plt>
   160e0:	ldr	x0, [x29, #24]
   160e4:	cbnz	x0, 16100 <__gmpz_cdiv_qr@@Base+0xe8>
   160e8:	mov	sp, x29
   160ec:	ldp	x20, x19, [sp, #48]
   160f0:	ldp	x22, x21, [sp, #32]
   160f4:	ldr	x23, [sp, #16]
   160f8:	ldp	x29, x30, [sp], #64
   160fc:	ret
   16100:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   16104:	b	160e8 <__gmpz_cdiv_qr@@Base+0xd0>
   16108:	add	x0, x29, #0x18
   1610c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   16110:	b	16084 <__gmpz_cdiv_qr@@Base+0x6c>

0000000000016114 <__gmpz_cdiv_qr_ui@@Base>:
   16114:	stp	x29, x30, [sp, #-80]!
   16118:	str	x25, [sp, #16]
   1611c:	stp	x24, x23, [sp, #32]
   16120:	stp	x22, x21, [sp, #48]
   16124:	stp	x20, x19, [sp, #64]
   16128:	mov	x29, sp
   1612c:	cbz	x3, 16238 <__gmpz_cdiv_qr_ui@@Base+0x124>
   16130:	ldrsw	x25, [x2, #4]
   16134:	mov	x22, x2
   16138:	mov	x20, x1
   1613c:	mov	x19, x0
   16140:	cbz	w25, 161bc <__gmpz_cdiv_qr_ui@@Base+0xa8>
   16144:	ldrsw	x8, [x19]
   16148:	cmp	w25, #0x0
   1614c:	cneg	x21, x25, lt  // lt = tstop
   16150:	mov	x24, x3
   16154:	cmp	x21, x8
   16158:	b.gt	16214 <__gmpz_cdiv_qr_ui@@Base+0x100>
   1615c:	ldr	x23, [x19, #8]
   16160:	ldr	x2, [x22, #8]
   16164:	mov	x0, x23
   16168:	mov	x1, xzr
   1616c:	mov	x3, x21
   16170:	mov	x4, x24
   16174:	bl	cd00 <__gmpn_divrem_1@plt>
   16178:	mov	x22, x0
   1617c:	cbz	x0, 161d0 <__gmpz_cdiv_qr_ui@@Base+0xbc>
   16180:	tbnz	w25, #31, 1619c <__gmpz_cdiv_qr_ui@@Base+0x88>
   16184:	mov	x8, x23
   16188:	ldr	x9, [x8]
   1618c:	adds	x9, x9, #0x1
   16190:	str	x9, [x8], #8
   16194:	b.cs	16188 <__gmpz_cdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   16198:	sub	x22, x24, x22
   1619c:	ldr	w8, [x20]
   161a0:	cmp	w8, #0x0
   161a4:	b.le	16228 <__gmpz_cdiv_qr_ui@@Base+0x114>
   161a8:	ldr	x0, [x20, #8]
   161ac:	cmp	x22, #0x0
   161b0:	csetm	w8, ne  // ne = any
   161b4:	str	x22, [x0]
   161b8:	b	161d4 <__gmpz_cdiv_qr_ui@@Base+0xc0>
   161bc:	mov	w8, wzr
   161c0:	mov	x22, xzr
   161c4:	str	wzr, [x19, #4]
   161c8:	mov	x19, x20
   161cc:	b	161f4 <__gmpz_cdiv_qr_ui@@Base+0xe0>
   161d0:	mov	w8, wzr
   161d4:	str	w8, [x20, #4]
   161d8:	add	x8, x23, x21, lsl #3
   161dc:	ldur	x8, [x8, #-8]
   161e0:	cmp	x8, #0x0
   161e4:	cset	w8, eq  // eq = none
   161e8:	sub	w8, w21, w8
   161ec:	cmp	w25, #0x0
   161f0:	cneg	w8, w8, lt  // lt = tstop
   161f4:	str	w8, [x19, #4]
   161f8:	mov	x0, x22
   161fc:	ldp	x20, x19, [sp, #64]
   16200:	ldp	x22, x21, [sp, #48]
   16204:	ldp	x24, x23, [sp, #32]
   16208:	ldr	x25, [sp, #16]
   1620c:	ldp	x29, x30, [sp], #80
   16210:	ret
   16214:	mov	x0, x19
   16218:	mov	x1, x21
   1621c:	bl	c080 <__gmpz_realloc@plt>
   16220:	mov	x23, x0
   16224:	b	16160 <__gmpz_cdiv_qr_ui@@Base+0x4c>
   16228:	mov	w1, #0x1                   	// #1
   1622c:	mov	x0, x20
   16230:	bl	c080 <__gmpz_realloc@plt>
   16234:	b	161ac <__gmpz_cdiv_qr_ui@@Base+0x98>
   16238:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000001623c <__gmpz_cdiv_r@@Base>:
   1623c:	stp	x29, x30, [sp, #-48]!
   16240:	stp	x22, x21, [sp, #16]
   16244:	stp	x20, x19, [sp, #32]
   16248:	mov	x29, sp
   1624c:	sub	sp, sp, #0x20
   16250:	ldrsw	x22, [x2, #4]
   16254:	mov	x19, x2
   16258:	mov	x21, x1
   1625c:	mov	x20, x0
   16260:	cmp	x0, x2
   16264:	stur	xzr, [x29, #-24]
   16268:	b.ne	162ac <__gmpz_cdiv_r@@Base+0x70>  // b.any
   1626c:	cmp	x22, #0x0
   16270:	cneg	x8, x22, mi  // mi = first
   16274:	cmp	x8, #0xfe0
   16278:	lsl	x1, x8, #3
   1627c:	stur	w8, [x29, #-16]
   16280:	b.hi	16304 <__gmpz_cdiv_r@@Base+0xc8>  // b.pmore
   16284:	add	x9, x1, #0xf
   16288:	mov	x8, sp
   1628c:	and	x9, x9, #0xfffffffffffffff0
   16290:	sub	x0, x8, x9
   16294:	mov	sp, x0
   16298:	stur	x0, [x29, #-8]
   1629c:	sub	x0, x29, #0x10
   162a0:	mov	x1, x19
   162a4:	bl	c420 <__gmpz_set@plt>
   162a8:	sub	x19, x29, #0x10
   162ac:	mov	x0, x20
   162b0:	mov	x1, x21
   162b4:	mov	x2, x19
   162b8:	bl	ca80 <__gmpz_tdiv_r@plt>
   162bc:	ldr	w8, [x21, #4]
   162c0:	eor	w8, w8, w22
   162c4:	tbnz	w8, #31, 162e0 <__gmpz_cdiv_r@@Base+0xa4>
   162c8:	ldr	w8, [x20, #4]
   162cc:	cbz	w8, 162e0 <__gmpz_cdiv_r@@Base+0xa4>
   162d0:	mov	x0, x20
   162d4:	mov	x1, x20
   162d8:	mov	x2, x19
   162dc:	bl	c260 <__gmpz_sub@plt>
   162e0:	ldur	x0, [x29, #-24]
   162e4:	cbnz	x0, 162fc <__gmpz_cdiv_r@@Base+0xc0>
   162e8:	mov	sp, x29
   162ec:	ldp	x20, x19, [sp, #32]
   162f0:	ldp	x22, x21, [sp, #16]
   162f4:	ldp	x29, x30, [sp], #48
   162f8:	ret
   162fc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   16300:	b	162e8 <__gmpz_cdiv_r@@Base+0xac>
   16304:	sub	x0, x29, #0x18
   16308:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1630c:	b	16298 <__gmpz_cdiv_r@@Base+0x5c>

0000000000016310 <__gmpz_cdiv_r_ui@@Base>:
   16310:	stp	x29, x30, [sp, #-48]!
   16314:	str	x21, [sp, #16]
   16318:	stp	x20, x19, [sp, #32]
   1631c:	mov	x29, sp
   16320:	cbz	x2, 163a0 <__gmpz_cdiv_r_ui@@Base+0x90>
   16324:	ldrsw	x21, [x1, #4]
   16328:	mov	x19, x0
   1632c:	cbz	w21, 16370 <__gmpz_cdiv_r_ui@@Base+0x60>
   16330:	ldr	x0, [x1, #8]
   16334:	cmp	w21, #0x0
   16338:	cneg	x1, x21, lt  // lt = tstop
   1633c:	mov	x20, x2
   16340:	bl	c3e0 <__gmpn_mod_1@plt>
   16344:	cbz	x0, 16370 <__gmpz_cdiv_r_ui@@Base+0x60>
   16348:	ldr	w8, [x19]
   1634c:	sub	x9, x20, x0
   16350:	cmp	w21, #0x0
   16354:	csel	x20, x9, x0, ge  // ge = tcont
   16358:	cmp	w8, #0x0
   1635c:	b.le	16390 <__gmpz_cdiv_r_ui@@Base+0x80>
   16360:	ldr	x0, [x19, #8]
   16364:	mov	w8, #0xffffffff            	// #-1
   16368:	str	x20, [x0]
   1636c:	b	16378 <__gmpz_cdiv_r_ui@@Base+0x68>
   16370:	mov	w8, wzr
   16374:	mov	x20, xzr
   16378:	str	w8, [x19, #4]
   1637c:	mov	x0, x20
   16380:	ldp	x20, x19, [sp, #32]
   16384:	ldr	x21, [sp, #16]
   16388:	ldp	x29, x30, [sp], #48
   1638c:	ret
   16390:	mov	w1, #0x1                   	// #1
   16394:	mov	x0, x19
   16398:	bl	c080 <__gmpz_realloc@plt>
   1639c:	b	16364 <__gmpz_cdiv_r_ui@@Base+0x54>
   163a0:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000163a4 <__gmpz_cdiv_ui@@Base>:
   163a4:	stp	x29, x30, [sp, #-32]!
   163a8:	stp	x20, x19, [sp, #16]
   163ac:	mov	x29, sp
   163b0:	cbz	x1, 163fc <__gmpz_cdiv_ui@@Base+0x58>
   163b4:	ldrsw	x20, [x0, #4]
   163b8:	cbz	w20, 163ec <__gmpz_cdiv_ui@@Base+0x48>
   163bc:	ldr	x0, [x0, #8]
   163c0:	mov	x19, x1
   163c4:	cmp	w20, #0x0
   163c8:	cneg	x1, x20, lt  // lt = tstop
   163cc:	mov	x2, x19
   163d0:	bl	c3e0 <__gmpn_mod_1@plt>
   163d4:	cmp	x0, #0x0
   163d8:	mov	w9, #0xffffffff            	// #-1
   163dc:	sub	x8, x19, x0
   163e0:	ccmp	w20, w9, #0x4, ne  // ne = any
   163e4:	csel	x0, x8, x0, gt
   163e8:	b	163f0 <__gmpz_cdiv_ui@@Base+0x4c>
   163ec:	mov	x0, xzr
   163f0:	ldp	x20, x19, [sp, #16]
   163f4:	ldp	x29, x30, [sp], #32
   163f8:	ret
   163fc:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000016400 <__gmpz_cdiv_q_2exp@@Base>:
   16400:	mov	w3, #0x1                   	// #1
   16404:	b	16408 <__gmpz_cdiv_q_2exp@@Base+0x8>
   16408:	stp	x29, x30, [sp, #-80]!
   1640c:	stp	x26, x25, [sp, #16]
   16410:	stp	x24, x23, [sp, #32]
   16414:	stp	x22, x21, [sp, #48]
   16418:	stp	x20, x19, [sp, #64]
   1641c:	ldrsw	x25, [x1, #4]
   16420:	ldrsw	x8, [x0]
   16424:	lsr	x26, x2, #6
   16428:	mov	x19, x0
   1642c:	cmp	x25, #0x0
   16430:	cneg	x9, x25, mi  // mi = first
   16434:	sub	x20, x9, x26
   16438:	cmp	x20, #0x0
   1643c:	mov	w23, w3
   16440:	mov	x29, sp
   16444:	b.le	164d0 <__gmpz_cdiv_q_2exp@@Base+0xd0>
   16448:	mov	x22, x2
   1644c:	mov	x24, x1
   16450:	cmp	x20, x8
   16454:	b.ge	16580 <__gmpz_cdiv_q_2exp@@Base+0x180>  // b.tcont
   16458:	ldr	x21, [x19, #8]
   1645c:	ldr	x9, [x24, #8]
   16460:	eor	w8, w25, w23
   16464:	mov	x23, xzr
   16468:	tbnz	w8, #31, 16488 <__gmpz_cdiv_q_2exp@@Base+0x88>
   1646c:	cbz	x26, 16488 <__gmpz_cdiv_q_2exp@@Base+0x88>
   16470:	mov	x10, xzr
   16474:	ldr	x23, [x9, x10, lsl #3]
   16478:	add	x10, x10, #0x1
   1647c:	cmp	x10, x26
   16480:	b.cs	16488 <__gmpz_cdiv_q_2exp@@Base+0x88>  // b.hs, b.nlast
   16484:	cbz	x23, 16474 <__gmpz_cdiv_q_2exp@@Base+0x74>
   16488:	ands	x3, x22, #0x3f
   1648c:	add	x1, x9, x26, lsl #3
   16490:	b.eq	164f8 <__gmpz_cdiv_q_2exp@@Base+0xf8>  // b.none
   16494:	mov	w9, #0xffffffff            	// #-1
   16498:	eor	w8, w9, w8, asr #31
   1649c:	mov	x0, x21
   164a0:	mov	x2, x20
   164a4:	sxtw	x22, w8
   164a8:	bl	c1a0 <__gmpn_rshift@plt>
   164ac:	add	x8, x21, x20, lsl #3
   164b0:	ldur	x8, [x8, #-8]
   164b4:	and	x9, x0, x22
   164b8:	orr	x23, x9, x23
   164bc:	cmp	x8, #0x0
   164c0:	cset	w8, eq  // eq = none
   164c4:	sub	x20, x20, x8
   164c8:	cbnz	x23, 16508 <__gmpz_cdiv_q_2exp@@Base+0x108>
   164cc:	b	1655c <__gmpz_cdiv_q_2exp@@Base+0x15c>
   164d0:	cmp	w8, #0x0
   164d4:	b.le	16594 <__gmpz_cdiv_q_2exp@@Base+0x194>
   164d8:	ldr	x0, [x19, #8]
   164dc:	eor	w9, w25, w23
   164e0:	cmp	w9, #0x0
   164e4:	mov	w8, #0x1                   	// #1
   164e8:	ccmp	w25, #0x0, #0x4, ge  // ge = tcont
   164ec:	str	x8, [x0]
   164f0:	csel	w8, wzr, w23, eq  // eq = none
   164f4:	b	16564 <__gmpz_cdiv_q_2exp@@Base+0x164>
   164f8:	mov	x0, x21
   164fc:	mov	x2, x20
   16500:	bl	ca50 <__gmpn_copyi@plt>
   16504:	cbz	x23, 1655c <__gmpz_cdiv_q_2exp@@Base+0x15c>
   16508:	cbz	x20, 16554 <__gmpz_cdiv_q_2exp@@Base+0x154>
   1650c:	ldr	x8, [x21]
   16510:	adds	x8, x8, #0x1
   16514:	str	x8, [x21]
   16518:	b.cc	16544 <__gmpz_cdiv_q_2exp@@Base+0x144>  // b.lo, b.ul, b.last
   1651c:	mov	w8, #0x1                   	// #1
   16520:	mov	w9, #0x1                   	// #1
   16524:	cmp	x9, x20
   16528:	b.ge	16548 <__gmpz_cdiv_q_2exp@@Base+0x148>  // b.tcont
   1652c:	lsl	x10, x9, #3
   16530:	ldr	x11, [x21, x10]
   16534:	add	x9, x9, #0x1
   16538:	adds	x11, x11, #0x1
   1653c:	str	x11, [x21, x10]
   16540:	b.cs	16524 <__gmpz_cdiv_q_2exp@@Base+0x124>  // b.hs, b.nlast
   16544:	mov	x8, xzr
   16548:	str	x8, [x21, x20, lsl #3]
   1654c:	add	x20, x8, x20
   16550:	b	1655c <__gmpz_cdiv_q_2exp@@Base+0x15c>
   16554:	mov	w20, #0x1                   	// #1
   16558:	str	x20, [x21]
   1655c:	cmp	w25, #0x0
   16560:	cneg	w8, w20, lt  // lt = tstop
   16564:	str	w8, [x19, #4]
   16568:	ldp	x20, x19, [sp, #64]
   1656c:	ldp	x22, x21, [sp, #48]
   16570:	ldp	x24, x23, [sp, #32]
   16574:	ldp	x26, x25, [sp, #16]
   16578:	ldp	x29, x30, [sp], #80
   1657c:	ret
   16580:	add	x1, x20, #0x1
   16584:	mov	x0, x19
   16588:	bl	c080 <__gmpz_realloc@plt>
   1658c:	mov	x21, x0
   16590:	b	1645c <__gmpz_cdiv_q_2exp@@Base+0x5c>
   16594:	mov	w1, #0x1                   	// #1
   16598:	mov	x0, x19
   1659c:	bl	c080 <__gmpz_realloc@plt>
   165a0:	b	164dc <__gmpz_cdiv_q_2exp@@Base+0xdc>

00000000000165a4 <__gmpz_fdiv_q_2exp@@Base>:
   165a4:	mov	w3, #0xffffffff            	// #-1
   165a8:	b	16408 <__gmpz_cdiv_q_2exp@@Base+0x8>

00000000000165ac <__gmpz_cdiv_r_2exp@@Base>:
   165ac:	mov	w3, #0x1                   	// #1
   165b0:	b	165b4 <__gmpz_cdiv_r_2exp@@Base+0x8>
   165b4:	stp	x29, x30, [sp, #-80]!
   165b8:	stp	x26, x25, [sp, #16]
   165bc:	stp	x24, x23, [sp, #32]
   165c0:	stp	x22, x21, [sp, #48]
   165c4:	stp	x20, x19, [sp, #64]
   165c8:	ldr	w26, [x1, #4]
   165cc:	mov	x19, x0
   165d0:	mov	x29, sp
   165d4:	cbz	w26, 16754 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   165d8:	mov	x21, x1
   165dc:	ldr	x1, [x1, #8]
   165e0:	sxtw	x22, w26
   165e4:	cmp	x22, #0x0
   165e8:	lsr	x23, x2, #6
   165ec:	and	x24, x2, #0x3f
   165f0:	eor	w8, w26, w3
   165f4:	cneg	x25, x22, mi  // mi = first
   165f8:	tbnz	w8, #31, 1666c <__gmpz_cdiv_r_2exp@@Base+0xc0>
   165fc:	cmp	x25, x23
   16600:	b.le	16638 <__gmpz_cdiv_r_2exp@@Base+0x8c>
   16604:	cbz	x23, 16624 <__gmpz_cdiv_r_2exp@@Base+0x78>
   16608:	mov	x8, x1
   1660c:	mov	x9, x23
   16610:	ldr	x10, [x8]
   16614:	cbnz	x10, 16638 <__gmpz_cdiv_r_2exp@@Base+0x8c>
   16618:	subs	x9, x9, #0x1
   1661c:	add	x8, x8, #0x8
   16620:	b.ne	16610 <__gmpz_cdiv_r_2exp@@Base+0x64>  // b.any
   16624:	ldr	x8, [x1, x23, lsl #3]
   16628:	mov	x9, #0xffffffffffffffff    	// #-1
   1662c:	lsl	x9, x9, x24
   16630:	bics	xzr, x8, x9
   16634:	b.eq	16750 <__gmpz_cdiv_r_2exp@@Base+0x1a4>  // b.none
   16638:	ldrsw	x8, [x19]
   1663c:	add	x20, x23, #0x1
   16640:	cmp	x23, x8
   16644:	b.ge	1677c <__gmpz_cdiv_r_2exp@@Base+0x1d0>  // b.tcont
   16648:	ldr	x21, [x19, #8]
   1664c:	ldr	x10, [x1]
   16650:	cmp	x25, x23
   16654:	neg	x22, x22
   16658:	csel	x25, x20, x25, gt
   1665c:	cbz	x10, 166a8 <__gmpz_cdiv_r_2exp@@Base+0xfc>
   16660:	mov	x8, x21
   16664:	mov	x9, x25
   16668:	b	166c8 <__gmpz_cdiv_r_2exp@@Base+0x11c>
   1666c:	cmp	x19, x21
   16670:	b.eq	16770 <__gmpz_cdiv_r_2exp@@Base+0x1c4>  // b.none
   16674:	ldrsw	x8, [x19]
   16678:	cmp	x25, x23
   1667c:	csinc	x20, x25, x23, le
   16680:	cmp	x20, x8
   16684:	b.gt	16794 <__gmpz_cdiv_r_2exp@@Base+0x1e8>
   16688:	ldr	x21, [x19, #8]
   1668c:	mov	x0, x21
   16690:	mov	x2, x20
   16694:	bl	ca50 <__gmpn_copyi@plt>
   16698:	cmp	x25, x23
   1669c:	mov	x1, x21
   166a0:	b.gt	16704 <__gmpz_cdiv_r_2exp@@Base+0x158>
   166a4:	b	16754 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   166a8:	mov	x9, x25
   166ac:	mov	x8, x21
   166b0:	subs	x9, x9, #0x1
   166b4:	str	xzr, [x8]
   166b8:	b.eq	166e4 <__gmpz_cdiv_r_2exp@@Base+0x138>  // b.none
   166bc:	ldr	x10, [x1, #8]!
   166c0:	add	x8, x8, #0x8
   166c4:	cbz	x10, 166b0 <__gmpz_cdiv_r_2exp@@Base+0x104>
   166c8:	neg	x10, x10
   166cc:	subs	x2, x9, #0x1
   166d0:	str	x10, [x8]
   166d4:	b.eq	166e4 <__gmpz_cdiv_r_2exp@@Base+0x138>  // b.none
   166d8:	add	x0, x8, #0x8
   166dc:	add	x1, x1, #0x8
   166e0:	bl	c290 <__gmpn_com@plt>
   166e4:	cmp	x25, x23
   166e8:	b.hi	16700 <__gmpz_cdiv_r_2exp@@Base+0x154>  // b.pmore
   166ec:	sub	x8, x20, x25
   166f0:	add	x0, x21, x25, lsl #3
   166f4:	lsl	x2, x8, #3
   166f8:	mov	w1, #0xff                  	// #255
   166fc:	bl	c5f0 <memset@plt>
   16700:	mov	x1, x21
   16704:	lsl	x8, x23, #3
   16708:	ldr	x9, [x1, x8]
   1670c:	mov	x10, #0xffffffffffffffff    	// #-1
   16710:	lsl	x10, x10, x24
   16714:	bics	x9, x9, x10
   16718:	str	x9, [x1, x8]
   1671c:	b.eq	16728 <__gmpz_cdiv_r_2exp@@Base+0x17c>  // b.none
   16720:	mov	x8, x23
   16724:	b	16740 <__gmpz_cdiv_r_2exp@@Base+0x194>
   16728:	sub	x9, x1, #0x8
   1672c:	subs	x8, x23, #0x1
   16730:	b.lt	16750 <__gmpz_cdiv_r_2exp@@Base+0x1a4>  // b.tstop
   16734:	ldr	x10, [x9, x23, lsl #3]
   16738:	mov	x23, x8
   1673c:	cbz	x10, 1672c <__gmpz_cdiv_r_2exp@@Base+0x180>
   16740:	mvn	w9, w8
   16744:	cmp	x22, #0x0
   16748:	csinc	w26, w9, w8, lt  // lt = tstop
   1674c:	b	16754 <__gmpz_cdiv_r_2exp@@Base+0x1a8>
   16750:	mov	w26, wzr
   16754:	str	w26, [x19, #4]
   16758:	ldp	x20, x19, [sp, #64]
   1675c:	ldp	x22, x21, [sp, #48]
   16760:	ldp	x24, x23, [sp, #32]
   16764:	ldp	x26, x25, [sp, #16]
   16768:	ldp	x29, x30, [sp], #80
   1676c:	ret
   16770:	cmp	x25, x23
   16774:	b.gt	16704 <__gmpz_cdiv_r_2exp@@Base+0x158>
   16778:	b	16758 <__gmpz_cdiv_r_2exp@@Base+0x1ac>
   1677c:	mov	x0, x19
   16780:	mov	x1, x20
   16784:	bl	c080 <__gmpz_realloc@plt>
   16788:	ldr	x1, [x21, #8]
   1678c:	mov	x21, x0
   16790:	b	1664c <__gmpz_cdiv_r_2exp@@Base+0xa0>
   16794:	mov	x0, x19
   16798:	mov	x21, x1
   1679c:	mov	x1, x20
   167a0:	bl	c080 <__gmpz_realloc@plt>
   167a4:	mov	x1, x21
   167a8:	mov	x21, x0
   167ac:	b	1668c <__gmpz_cdiv_r_2exp@@Base+0xe0>

00000000000167b0 <__gmpz_fdiv_r_2exp@@Base>:
   167b0:	mov	w3, #0xffffffff            	// #-1
   167b4:	b	165b4 <__gmpz_cdiv_r_2exp@@Base+0x8>

00000000000167b8 <__gmpz_clear@@Base>:
   167b8:	ldrsw	x8, [x0]
   167bc:	cbz	w8, 167d8 <__gmpz_clear@@Base+0x20>
   167c0:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   167c4:	ldr	x9, [x9, #4016]
   167c8:	ldr	x0, [x0, #8]
   167cc:	lsl	x1, x8, #3
   167d0:	ldr	x2, [x9]
   167d4:	br	x2
   167d8:	ret

00000000000167dc <__gmpz_clears@@Base>:
   167dc:	sub	sp, sp, #0x100
   167e0:	stp	x29, x30, [sp, #224]
   167e4:	add	x29, sp, #0xe0
   167e8:	mov	x8, #0xffffffffffffffc8    	// #-56
   167ec:	mov	x9, sp
   167f0:	sub	x10, x29, #0x58
   167f4:	movk	x8, #0xff80, lsl #32
   167f8:	add	x11, x29, #0x20
   167fc:	add	x9, x9, #0x80
   16800:	add	x10, x10, #0x38
   16804:	str	x19, [sp, #240]
   16808:	stp	x1, x2, [x29, #-88]
   1680c:	stp	x3, x4, [x29, #-72]
   16810:	stp	x5, x6, [x29, #-56]
   16814:	stur	x7, [x29, #-40]
   16818:	stp	q0, q1, [sp]
   1681c:	stp	q2, q3, [sp, #32]
   16820:	stp	q4, q5, [sp, #64]
   16824:	stp	q6, q7, [sp, #96]
   16828:	stp	x9, x8, [x29, #-16]
   1682c:	stp	x11, x10, [x29, #-32]
   16830:	adrp	x19, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   16834:	ldr	x19, [x19, #4016]
   16838:	b	16850 <__gmpz_clears@@Base+0x74>
   1683c:	ldur	x8, [x29, #-32]
   16840:	add	x9, x8, #0x8
   16844:	stur	x9, [x29, #-32]
   16848:	ldr	x0, [x8]
   1684c:	cbz	x0, 16890 <__gmpz_clears@@Base+0xb4>
   16850:	ldrsw	x8, [x0]
   16854:	cbz	w8, 16868 <__gmpz_clears@@Base+0x8c>
   16858:	ldr	x9, [x19]
   1685c:	ldr	x0, [x0, #8]
   16860:	lsl	x1, x8, #3
   16864:	blr	x9
   16868:	ldursw	x8, [x29, #-8]
   1686c:	tbz	w8, #31, 1683c <__gmpz_clears@@Base+0x60>
   16870:	add	w9, w8, #0x8
   16874:	cmn	w8, #0x8
   16878:	stur	w9, [x29, #-8]
   1687c:	b.gt	1683c <__gmpz_clears@@Base+0x60>
   16880:	ldur	x9, [x29, #-24]
   16884:	add	x8, x9, x8
   16888:	ldr	x0, [x8]
   1688c:	cbnz	x0, 16850 <__gmpz_clears@@Base+0x74>
   16890:	ldr	x19, [sp, #240]
   16894:	ldp	x29, x30, [sp, #224]
   16898:	add	sp, sp, #0x100
   1689c:	ret

00000000000168a0 <__gmpz_clrbit@@Base>:
   168a0:	sub	sp, sp, #0x40
   168a4:	stp	x29, x30, [sp, #16]
   168a8:	stp	x20, x19, [sp, #48]
   168ac:	ldrsw	x8, [x0, #4]
   168b0:	ldr	x19, [x0, #8]
   168b4:	mov	w9, #0x1                   	// #1
   168b8:	str	x21, [sp, #32]
   168bc:	lsr	x20, x1, #6
   168c0:	lsl	x21, x9, x1
   168c4:	add	x29, sp, #0x10
   168c8:	tbnz	w8, #31, 16918 <__gmpz_clrbit@@Base+0x78>
   168cc:	cmp	x20, x8
   168d0:	b.ge	169f0 <__gmpz_clrbit@@Base+0x150>  // b.tcont
   168d4:	lsl	x9, x20, #3
   168d8:	ldr	x10, [x19, x9]
   168dc:	bics	xzr, x10, x21
   168e0:	bic	x11, x10, x21
   168e4:	cinc	x10, x20, eq  // eq = none
   168e8:	cmp	x10, x8
   168ec:	str	x11, [x19, x9]
   168f0:	b.ne	169f0 <__gmpz_clrbit@@Base+0x150>  // b.any
   168f4:	sub	x8, x19, #0x8
   168f8:	subs	x9, x20, #0x1
   168fc:	b.lt	16a2c <__gmpz_clrbit@@Base+0x18c>  // b.tstop
   16900:	ldr	x10, [x8, x20, lsl #3]
   16904:	mov	x20, x9
   16908:	cbz	x10, 168f8 <__gmpz_clrbit@@Base+0x58>
   1690c:	add	x8, x9, #0x1
   16910:	str	w8, [x0, #4]
   16914:	b	169f0 <__gmpz_clrbit@@Base+0x150>
   16918:	neg	x9, x8
   1691c:	cmp	x20, x9
   16920:	b.ge	16954 <__gmpz_clrbit@@Base+0xb4>  // b.tcont
   16924:	mov	x10, xzr
   16928:	ldr	x11, [x19, x10, lsl #3]
   1692c:	add	x10, x10, #0x1
   16930:	cbz	x11, 16928 <__gmpz_clrbit@@Base+0x88>
   16934:	sub	x11, x10, #0x1
   16938:	cmp	x20, x11
   1693c:	b.ls	16988 <__gmpz_clrbit@@Base+0xe8>  // b.plast
   16940:	lsl	x8, x20, #3
   16944:	ldr	x9, [x19, x8]
   16948:	orr	x9, x9, x21
   1694c:	str	x9, [x19, x8]
   16950:	b	169f0 <__gmpz_clrbit@@Base+0x150>
   16954:	ldrsw	x10, [x0]
   16958:	cmp	x20, x10
   1695c:	b.ge	16a04 <__gmpz_clrbit@@Base+0x164>  // b.tcont
   16960:	mvn	w10, w20
   16964:	adds	x8, x20, x8
   16968:	str	w10, [x0, #4]
   1696c:	b.eq	16980 <__gmpz_clrbit@@Base+0xe0>  // b.none
   16970:	add	x0, x19, x9, lsl #3
   16974:	lsl	x2, x8, #3
   16978:	mov	w1, wzr
   1697c:	bl	c5f0 <memset@plt>
   16980:	str	x21, [x19, x20, lsl #3]
   16984:	b	169f0 <__gmpz_clrbit@@Base+0x150>
   16988:	add	x11, x20, #0x1
   1698c:	cmp	x11, x10
   16990:	b.ne	169f0 <__gmpz_clrbit@@Base+0x150>  // b.any
   16994:	lsl	x10, x20, #3
   16998:	ldr	x11, [x19, x10]
   1699c:	sub	x11, x11, #0x1
   169a0:	orr	x11, x11, x21
   169a4:	adds	x11, x11, #0x1
   169a8:	str	x11, [x19, x10]
   169ac:	b.cc	169f0 <__gmpz_clrbit@@Base+0x150>  // b.lo, b.ul, b.last
   169b0:	ldrsw	x10, [x0]
   169b4:	mov	w11, #0x1                   	// #1
   169b8:	sub	x1, x11, x8
   169bc:	cmp	x1, x10
   169c0:	b.gt	16a38 <__gmpz_clrbit@@Base+0x198>
   169c4:	add	x10, x19, x20, lsl #3
   169c8:	add	x10, x10, #0x8
   169cc:	str	xzr, [x19, x9, lsl #3]
   169d0:	ldr	x11, [x10]
   169d4:	adds	x11, x11, #0x1
   169d8:	str	x11, [x10], #8
   169dc:	b.cs	169d0 <__gmpz_clrbit@@Base+0x130>  // b.hs, b.nlast
   169e0:	lsl	x9, x9, #3
   169e4:	ldr	w9, [x19, x9]
   169e8:	sub	w8, w8, w9
   169ec:	str	w8, [x0, #4]
   169f0:	ldp	x20, x19, [sp, #48]
   169f4:	ldr	x21, [sp, #32]
   169f8:	ldp	x29, x30, [sp, #16]
   169fc:	add	sp, sp, #0x40
   16a00:	ret
   16a04:	add	x1, x20, #0x1
   16a08:	str	x0, [x29, #24]
   16a0c:	str	x8, [sp, #8]
   16a10:	mov	x19, x9
   16a14:	bl	c080 <__gmpz_realloc@plt>
   16a18:	mov	x9, x19
   16a1c:	ldr	x8, [sp, #8]
   16a20:	mov	x19, x0
   16a24:	ldr	x0, [x29, #24]
   16a28:	b	16960 <__gmpz_clrbit@@Base+0xc0>
   16a2c:	mov	x8, xzr
   16a30:	str	w8, [x0, #4]
   16a34:	b	169f0 <__gmpz_clrbit@@Base+0x150>
   16a38:	str	x0, [x29, #24]
   16a3c:	mov	x19, x8
   16a40:	mov	x21, x9
   16a44:	bl	c080 <__gmpz_realloc@plt>
   16a48:	mov	x8, x19
   16a4c:	mov	x19, x0
   16a50:	ldr	x0, [x29, #24]
   16a54:	mov	x9, x21
   16a58:	b	169c4 <__gmpz_clrbit@@Base+0x124>

0000000000016a5c <__gmpz_cmp@@Base>:
   16a5c:	ldrsw	x9, [x0, #4]
   16a60:	ldrsw	x10, [x1, #4]
   16a64:	mov	x8, x0
   16a68:	subs	x0, x9, x10
   16a6c:	b.eq	16a74 <__gmpz_cmp@@Base+0x18>  // b.none
   16a70:	ret
   16a74:	ldr	x8, [x8, #8]
   16a78:	ldr	x10, [x1, #8]
   16a7c:	cmp	w9, #0x0
   16a80:	cneg	x11, x9, lt  // lt = tstop
   16a84:	sub	x8, x8, #0x8
   16a88:	sub	x10, x10, #0x8
   16a8c:	subs	x12, x11, #0x1
   16a90:	b.lt	16ab8 <__gmpz_cmp@@Base+0x5c>  // b.tstop
   16a94:	lsl	x11, x11, #3
   16a98:	ldr	x13, [x8, x11]
   16a9c:	ldr	x11, [x10, x11]
   16aa0:	cmp	x13, x11
   16aa4:	mov	x11, x12
   16aa8:	b.eq	16a8c <__gmpz_cmp@@Base+0x30>  // b.none
   16aac:	mov	w8, #0x1                   	// #1
   16ab0:	cneg	w8, w8, ls  // ls = plast
   16ab4:	b	16abc <__gmpz_cmp@@Base+0x60>
   16ab8:	mov	w8, wzr
   16abc:	cmp	w9, #0x0
   16ac0:	cneg	w0, w8, lt  // lt = tstop
   16ac4:	ret

0000000000016ac8 <__gmpz_cmp_d@@Base>:
   16ac8:	sub	sp, sp, #0x40
   16acc:	fmov	x8, d0
   16ad0:	mvn	x9, x8
   16ad4:	tst	x9, #0x7ff0000000000000
   16ad8:	stp	x29, x30, [sp, #16]
   16adc:	str	x21, [sp, #32]
   16ae0:	stp	x20, x19, [sp, #48]
   16ae4:	add	x29, sp, #0x10
   16ae8:	b.eq	16bfc <__gmpz_cmp_d@@Base+0x134>  // b.none
   16aec:	mov	x19, x0
   16af0:	ldr	w0, [x0, #4]
   16af4:	fcmp	d0, #0.0
   16af8:	b.ne	16b10 <__gmpz_cmp_d@@Base+0x48>  // b.any
   16afc:	ldp	x20, x19, [sp, #48]
   16b00:	ldr	x21, [sp, #32]
   16b04:	ldp	x29, x30, [sp, #16]
   16b08:	add	sp, sp, #0x40
   16b0c:	ret
   16b10:	cbz	w0, 16c04 <__gmpz_cmp_d@@Base+0x13c>
   16b14:	sxtw	x21, w0
   16b18:	tbnz	w0, #31, 16b30 <__gmpz_cmp_d@@Base+0x68>
   16b1c:	mov	w20, #0x1                   	// #1
   16b20:	fcmp	d0, #0.0
   16b24:	mov	w0, #0x1                   	// #1
   16b28:	b.pl	16b44 <__gmpz_cmp_d@@Base+0x7c>  // b.nfrst
   16b2c:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16b30:	fcmp	d0, #0.0
   16b34:	b.ge	16b9c <__gmpz_cmp_d@@Base+0xd4>  // b.tcont
   16b38:	fneg	d0, d0
   16b3c:	neg	x21, x21
   16b40:	mov	w20, #0xffffffff            	// #-1
   16b44:	fmov	d1, #1.000000000000000000e+00
   16b48:	fcmp	d0, d1
   16b4c:	b.pl	16b58 <__gmpz_cmp_d@@Base+0x90>  // b.nfrst
   16b50:	mov	w0, w20
   16b54:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16b58:	mov	x0, sp
   16b5c:	bl	d280 <__gmp_extract_double@plt>
   16b60:	sxtw	x8, w0
   16b64:	cmp	x21, x8
   16b68:	b.ne	16ba4 <__gmpz_cmp_d@@Base+0xdc>  // b.any
   16b6c:	ldr	x8, [x19, #8]
   16b70:	ldr	x10, [sp, #8]
   16b74:	add	x9, x8, x21, lsl #3
   16b78:	ldur	x9, [x9, #-8]
   16b7c:	cmp	x9, x10
   16b80:	b.ne	16bec <__gmpz_cmp_d@@Base+0x124>  // b.any
   16b84:	cmp	x21, #0x1
   16b88:	b.ne	16bac <__gmpz_cmp_d@@Base+0xe4>  // b.any
   16b8c:	ldr	x8, [sp]
   16b90:	cmp	x8, #0x0
   16b94:	csneg	w0, wzr, w20, eq  // eq = none
   16b98:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16b9c:	mov	w0, #0xffffffff            	// #-1
   16ba0:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16ba4:	cneg	w0, w20, lt  // lt = tstop
   16ba8:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16bac:	add	x9, x8, x21, lsl #3
   16bb0:	ldur	x9, [x9, #-16]
   16bb4:	ldr	x10, [sp]
   16bb8:	cmp	x9, x10
   16bbc:	b.ne	16bec <__gmpz_cmp_d@@Base+0x124>  // b.any
   16bc0:	cmp	x21, #0x3
   16bc4:	b.lt	16bf4 <__gmpz_cmp_d@@Base+0x12c>  // b.tstop
   16bc8:	sub	x8, x8, #0x18
   16bcc:	ldr	x9, [x8, x21, lsl #3]
   16bd0:	cbnz	x9, 16b50 <__gmpz_cmp_d@@Base+0x88>
   16bd4:	sub	x9, x21, #0x3
   16bd8:	mov	w0, wzr
   16bdc:	sub	x21, x21, #0x1
   16be0:	cmp	x9, #0x1
   16be4:	b.ge	16bcc <__gmpz_cmp_d@@Base+0x104>  // b.tcont
   16be8:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16bec:	cneg	w0, w20, cc  // cc = lo, ul, last
   16bf0:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16bf4:	mov	w0, wzr
   16bf8:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16bfc:	tst	x8, #0xfffffffffffff
   16c00:	b.ne	16c14 <__gmpz_cmp_d@@Base+0x14c>  // b.any
   16c04:	fcmp	d0, #0.0
   16c08:	mov	w8, #0xffffffff            	// #-1
   16c0c:	csinc	w0, w8, wzr, pl  // pl = nfrst
   16c10:	b	16afc <__gmpz_cmp_d@@Base+0x34>
   16c14:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000016c18 <__gmpz_cmp_si@@Base>:
   16c18:	ldr	w8, [x0, #4]
   16c1c:	cmp	x1, #0x0
   16c20:	asr	x9, x1, #63
   16c24:	cinc	x9, x9, gt
   16c28:	cbz	w8, 16c58 <__gmpz_cmp_si@@Base+0x40>
   16c2c:	sxtw	x10, w8
   16c30:	cmp	x9, x10
   16c34:	b.ne	16c58 <__gmpz_cmp_si@@Base+0x40>  // b.any
   16c38:	ldr	x9, [x0, #8]
   16c3c:	cmp	x1, #0x0
   16c40:	cneg	x10, x1, mi  // mi = first
   16c44:	ldr	x9, [x9]
   16c48:	cmp	x9, x10
   16c4c:	b.ne	16c60 <__gmpz_cmp_si@@Base+0x48>  // b.any
   16c50:	mov	w0, wzr
   16c54:	ret
   16c58:	sub	w0, w8, w9
   16c5c:	ret
   16c60:	cneg	w0, w8, ls  // ls = plast
   16c64:	ret

0000000000016c68 <__gmpz_cmp_ui@@Base>:
   16c68:	ldr	w8, [x0, #4]
   16c6c:	cmp	w8, #0x1
   16c70:	b.eq	16c84 <__gmpz_cmp_ui@@Base+0x1c>  // b.none
   16c74:	cbnz	w8, 16c9c <__gmpz_cmp_ui@@Base+0x34>
   16c78:	cmp	x1, #0x0
   16c7c:	csetm	w0, ne  // ne = any
   16c80:	ret
   16c84:	ldr	x8, [x0, #8]
   16c88:	ldr	x8, [x8]
   16c8c:	cmp	x8, x1
   16c90:	b.ls	16cac <__gmpz_cmp_ui@@Base+0x44>  // b.plast
   16c94:	mov	w0, #0x1                   	// #1
   16c98:	ret
   16c9c:	cmp	w8, #0x1
   16ca0:	mov	w8, #0xffffffff            	// #-1
   16ca4:	cneg	w0, w8, ge  // ge = tcont
   16ca8:	ret
   16cac:	csetm	w0, cc  // cc = lo, ul, last
   16cb0:	ret

0000000000016cb4 <__gmpz_cmpabs@@Base>:
   16cb4:	ldr	w9, [x0, #4]
   16cb8:	ldr	w10, [x1, #4]
   16cbc:	mov	x8, x0
   16cc0:	cmp	w9, #0x0
   16cc4:	cneg	w9, w9, mi  // mi = first
   16cc8:	cmp	w10, #0x0
   16ccc:	cneg	w10, w10, mi  // mi = first
   16cd0:	subs	x0, x9, x10
   16cd4:	b.eq	16cdc <__gmpz_cmpabs@@Base+0x28>  // b.none
   16cd8:	ret
   16cdc:	ldr	x8, [x8, #8]
   16ce0:	ldr	x10, [x1, #8]
   16ce4:	sub	x8, x8, #0x8
   16ce8:	sub	x10, x10, #0x8
   16cec:	subs	x11, x9, #0x1
   16cf0:	b.lt	16d18 <__gmpz_cmpabs@@Base+0x64>  // b.tstop
   16cf4:	lsl	x9, x9, #3
   16cf8:	ldr	x12, [x8, x9]
   16cfc:	ldr	x9, [x10, x9]
   16d00:	cmp	x12, x9
   16d04:	mov	x9, x11
   16d08:	b.eq	16cec <__gmpz_cmpabs@@Base+0x38>  // b.none
   16d0c:	mov	w8, #0x1                   	// #1
   16d10:	cneg	w0, w8, ls  // ls = plast
   16d14:	ret
   16d18:	mov	w0, wzr
   16d1c:	ret

0000000000016d20 <__gmpz_cmpabs_d@@Base>:
   16d20:	sub	sp, sp, #0x30
   16d24:	fmov	x8, d0
   16d28:	mvn	x9, x8
   16d2c:	tst	x9, #0x7ff0000000000000
   16d30:	stp	x29, x30, [sp, #16]
   16d34:	stp	x20, x19, [sp, #32]
   16d38:	add	x29, sp, #0x10
   16d3c:	b.eq	16e38 <__gmpz_cmpabs_d@@Base+0x118>  // b.none
   16d40:	ldrsw	x8, [x0, #4]
   16d44:	mov	x19, x0
   16d48:	fcmp	d0, #0.0
   16d4c:	b.ne	16d68 <__gmpz_cmpabs_d@@Base+0x48>  // b.any
   16d50:	cmp	w8, #0x0
   16d54:	cset	w0, ne  // ne = any
   16d58:	ldp	x20, x19, [sp, #32]
   16d5c:	ldp	x29, x30, [sp, #16]
   16d60:	add	sp, sp, #0x30
   16d64:	ret
   16d68:	cbz	w8, 16e40 <__gmpz_cmpabs_d@@Base+0x120>
   16d6c:	cmp	x8, #0x0
   16d70:	fneg	d1, d0
   16d74:	cneg	x20, x8, mi  // mi = first
   16d78:	fcmp	d0, #0.0
   16d7c:	fcsel	d0, d0, d1, ge  // ge = tcont
   16d80:	fmov	d1, #1.000000000000000000e+00
   16d84:	fcmp	d0, d1
   16d88:	b.pl	16d94 <__gmpz_cmpabs_d@@Base+0x74>  // b.nfrst
   16d8c:	mov	w0, #0x1                   	// #1
   16d90:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16d94:	mov	x0, sp
   16d98:	bl	d280 <__gmp_extract_double@plt>
   16d9c:	sxtw	x8, w0
   16da0:	cmp	x20, x8
   16da4:	b.ne	16dd8 <__gmpz_cmpabs_d@@Base+0xb8>  // b.any
   16da8:	ldr	x8, [x19, #8]
   16dac:	ldr	x10, [sp, #8]
   16db0:	add	x9, x8, x20, lsl #3
   16db4:	ldur	x9, [x9, #-8]
   16db8:	cmp	x9, x10
   16dbc:	b.ne	16e24 <__gmpz_cmpabs_d@@Base+0x104>  // b.any
   16dc0:	cmp	x20, #0x1
   16dc4:	b.ne	16de4 <__gmpz_cmpabs_d@@Base+0xc4>  // b.any
   16dc8:	ldr	x8, [sp]
   16dcc:	cmp	x8, #0x0
   16dd0:	csetm	w0, ne  // ne = any
   16dd4:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16dd8:	mov	w8, #0xffffffff            	// #-1
   16ddc:	cneg	w0, w8, ge  // ge = tcont
   16de0:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16de4:	add	x9, x8, x20, lsl #3
   16de8:	ldur	x9, [x9, #-16]
   16dec:	ldr	x10, [sp]
   16df0:	cmp	x9, x10
   16df4:	b.ne	16e24 <__gmpz_cmpabs_d@@Base+0x104>  // b.any
   16df8:	cmp	x20, #0x3
   16dfc:	b.lt	16e30 <__gmpz_cmpabs_d@@Base+0x110>  // b.tstop
   16e00:	sub	x8, x8, #0x18
   16e04:	ldr	x9, [x8, x20, lsl #3]
   16e08:	cbnz	x9, 16d8c <__gmpz_cmpabs_d@@Base+0x6c>
   16e0c:	sub	x9, x20, #0x3
   16e10:	mov	w0, wzr
   16e14:	sub	x20, x20, #0x1
   16e18:	cmp	x9, #0x1
   16e1c:	b.ge	16e04 <__gmpz_cmpabs_d@@Base+0xe4>  // b.tcont
   16e20:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16e24:	mov	w8, #0xffffffff            	// #-1
   16e28:	cneg	w0, w8, cs  // cs = hs, nlast
   16e2c:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16e30:	mov	w0, wzr
   16e34:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16e38:	tst	x8, #0xfffffffffffff
   16e3c:	b.ne	16e48 <__gmpz_cmpabs_d@@Base+0x128>  // b.any
   16e40:	mov	w0, #0xffffffff            	// #-1
   16e44:	b	16d58 <__gmpz_cmpabs_d@@Base+0x38>
   16e48:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000016e4c <__gmpz_cmpabs_ui@@Base>:
   16e4c:	ldrsw	x8, [x0, #4]
   16e50:	cbz	w8, 16e7c <__gmpz_cmpabs_ui@@Base+0x30>
   16e54:	cmp	x8, #0x0
   16e58:	cneg	x8, x8, mi  // mi = first
   16e5c:	cmp	x8, #0x1
   16e60:	b.ne	16e74 <__gmpz_cmpabs_ui@@Base+0x28>  // b.any
   16e64:	ldr	x8, [x0, #8]
   16e68:	ldr	x8, [x8]
   16e6c:	cmp	x8, x1
   16e70:	b.ls	16e88 <__gmpz_cmpabs_ui@@Base+0x3c>  // b.plast
   16e74:	mov	w0, #0x1                   	// #1
   16e78:	ret
   16e7c:	cmp	x1, #0x0
   16e80:	csetm	w0, ne  // ne = any
   16e84:	ret
   16e88:	csetm	w0, cc  // cc = lo, ul, last
   16e8c:	ret

0000000000016e90 <__gmpz_com@@Base>:
   16e90:	stp	x29, x30, [sp, #-48]!
   16e94:	stp	x22, x21, [sp, #16]
   16e98:	stp	x20, x19, [sp, #32]
   16e9c:	ldrsw	x22, [x1, #4]
   16ea0:	mov	x21, x1
   16ea4:	mov	x19, x0
   16ea8:	mov	x29, sp
   16eac:	tbnz	w22, #31, 16fc4 <__gmpz_com@@Base+0x134>
   16eb0:	ldr	w8, [x19]
   16eb4:	cbz	w22, 17250 <__gmpz_com@@Base+0x3c0>
   16eb8:	cmp	w22, w8
   16ebc:	b.ge	1726c <__gmpz_com@@Base+0x3dc>  // b.tcont
   16ec0:	ldr	x0, [x19, #8]
   16ec4:	ldr	x8, [x21, #8]
   16ec8:	ldr	x9, [x8]
   16ecc:	adds	x9, x9, #0x1
   16ed0:	str	x9, [x0]
   16ed4:	b.cc	17054 <__gmpz_com@@Base+0x1c4>  // b.lo, b.ul, b.last
   16ed8:	mov	x11, xzr
   16edc:	sub	x10, x22, #0x1
   16ee0:	mov	w12, #0x1                   	// #1
   16ee4:	mov	w9, #0x1                   	// #1
   16ee8:	cmp	x9, x22
   16eec:	b.ge	170c0 <__gmpz_com@@Base+0x230>  // b.tcont
   16ef0:	add	x13, x8, x11
   16ef4:	ldr	x13, [x13, #8]
   16ef8:	add	x14, x0, x11
   16efc:	add	x9, x9, #0x1
   16f00:	add	x11, x11, #0x8
   16f04:	adds	x13, x13, #0x1
   16f08:	sub	x10, x10, #0x1
   16f0c:	str	x13, [x14, #8]
   16f10:	b.cs	16ee8 <__gmpz_com@@Base+0x58>  // b.hs, b.nlast
   16f14:	cmp	x8, x0
   16f18:	mov	x12, xzr
   16f1c:	b.eq	170c0 <__gmpz_com@@Base+0x230>  // b.none
   16f20:	cmp	x9, x22
   16f24:	b.ge	170c0 <__gmpz_com@@Base+0x230>  // b.tcont
   16f28:	sub	x12, x22, x9
   16f2c:	cmp	x12, #0x4
   16f30:	b.cc	16fa0 <__gmpz_com@@Base+0x110>  // b.lo, b.ul, b.last
   16f34:	add	x14, x0, x11
   16f38:	lsl	x13, x22, #3
   16f3c:	add	x14, x14, #0x8
   16f40:	add	x15, x8, x13
   16f44:	cmp	x14, x15
   16f48:	b.cs	16f60 <__gmpz_com@@Base+0xd0>  // b.hs, b.nlast
   16f4c:	add	x14, x8, x11
   16f50:	add	x13, x0, x13
   16f54:	add	x14, x14, #0x8
   16f58:	cmp	x14, x13
   16f5c:	b.cc	16fa0 <__gmpz_com@@Base+0x110>  // b.lo, b.ul, b.last
   16f60:	add	x13, x0, x11
   16f64:	add	x14, x8, x11
   16f68:	and	x11, x12, #0xfffffffffffffffc
   16f6c:	and	x15, x10, #0xfffffffffffffffc
   16f70:	add	x10, x13, #0x18
   16f74:	add	x13, x14, #0x18
   16f78:	add	x9, x15, x9
   16f7c:	mov	x14, x11
   16f80:	ldp	q0, q1, [x13, #-16]
   16f84:	add	x13, x13, #0x20
   16f88:	subs	x14, x14, #0x4
   16f8c:	stp	q0, q1, [x10, #-16]
   16f90:	add	x10, x10, #0x20
   16f94:	b.ne	16f80 <__gmpz_com@@Base+0xf0>  // b.any
   16f98:	cmp	x12, x11
   16f9c:	b.eq	170bc <__gmpz_com@@Base+0x22c>  // b.none
   16fa0:	lsl	x11, x9, #3
   16fa4:	sub	x10, x22, x9
   16fa8:	add	x9, x0, x11
   16fac:	add	x8, x8, x11
   16fb0:	ldr	x11, [x8], #8
   16fb4:	subs	x10, x10, #0x1
   16fb8:	str	x11, [x9], #8
   16fbc:	b.ne	16fb0 <__gmpz_com@@Base+0x120>  // b.any
   16fc0:	b	170bc <__gmpz_com@@Base+0x22c>
   16fc4:	ldrsw	x8, [x19]
   16fc8:	neg	x20, x22
   16fcc:	cmp	x20, x8
   16fd0:	b.gt	1727c <__gmpz_com@@Base+0x3ec>
   16fd4:	ldr	x0, [x19, #8]
   16fd8:	ldr	x8, [x21, #8]
   16fdc:	ldr	x9, [x8]
   16fe0:	sub	x10, x9, #0x1
   16fe4:	str	x10, [x0]
   16fe8:	cbz	x9, 170d0 <__gmpz_com@@Base+0x240>
   16fec:	cmn	w22, #0x2
   16ff0:	b.gt	17228 <__gmpz_com@@Base+0x398>
   16ff4:	cmp	x8, x0
   16ff8:	b.eq	17228 <__gmpz_com@@Base+0x398>  // b.none
   16ffc:	cmn	w22, #0x5
   17000:	b.hi	17028 <__gmpz_com@@Base+0x198>  // b.pmore
   17004:	lsl	x9, x20, #3
   17008:	add	x10, x0, #0x8
   1700c:	add	x11, x8, x9
   17010:	cmp	x10, x11
   17014:	b.cs	171f0 <__gmpz_com@@Base+0x360>  // b.hs, b.nlast
   17018:	add	x9, x0, x9
   1701c:	add	x10, x8, #0x8
   17020:	cmp	x10, x9
   17024:	b.cs	171f0 <__gmpz_com@@Base+0x360>  // b.hs, b.nlast
   17028:	mov	w9, #0x1                   	// #1
   1702c:	add	x10, x9, x22
   17030:	lsl	x11, x9, #3
   17034:	neg	x9, x10
   17038:	add	x10, x0, x11
   1703c:	add	x8, x8, x11
   17040:	ldr	x11, [x8], #8
   17044:	subs	x9, x9, #0x1
   17048:	str	x11, [x10], #8
   1704c:	b.ne	17040 <__gmpz_com@@Base+0x1b0>  // b.any
   17050:	b	17228 <__gmpz_com@@Base+0x398>
   17054:	cmp	w22, #0x2
   17058:	mov	x12, xzr
   1705c:	b.lt	170c0 <__gmpz_com@@Base+0x230>  // b.tstop
   17060:	cmp	x8, x0
   17064:	b.eq	170c0 <__gmpz_com@@Base+0x230>  // b.none
   17068:	sub	x9, x22, #0x1
   1706c:	cmp	x9, #0x4
   17070:	b.cc	17098 <__gmpz_com@@Base+0x208>  // b.lo, b.ul, b.last
   17074:	lsl	x10, x22, #3
   17078:	add	x11, x0, #0x8
   1707c:	add	x12, x8, x10
   17080:	cmp	x11, x12
   17084:	b.cs	171b8 <__gmpz_com@@Base+0x328>  // b.hs, b.nlast
   17088:	add	x10, x0, x10
   1708c:	add	x11, x8, #0x8
   17090:	cmp	x11, x10
   17094:	b.cs	171b8 <__gmpz_com@@Base+0x328>  // b.hs, b.nlast
   17098:	mov	w10, #0x1                   	// #1
   1709c:	lsl	x11, x10, #3
   170a0:	sub	x9, x22, x10
   170a4:	add	x10, x0, x11
   170a8:	add	x8, x8, x11
   170ac:	ldr	x11, [x8], #8
   170b0:	subs	x9, x9, #0x1
   170b4:	str	x11, [x10], #8
   170b8:	b.ne	170ac <__gmpz_com@@Base+0x21c>  // b.any
   170bc:	mov	x12, xzr
   170c0:	add	w8, w22, w12
   170c4:	str	x12, [x0, x22, lsl #3]
   170c8:	neg	w8, w8
   170cc:	b	1723c <__gmpz_com@@Base+0x3ac>
   170d0:	mov	x11, xzr
   170d4:	mvn	x10, x22
   170d8:	mov	w9, #0x1                   	// #1
   170dc:	cmp	x9, x20
   170e0:	b.ge	17228 <__gmpz_com@@Base+0x398>  // b.tcont
   170e4:	add	x12, x8, x11
   170e8:	ldr	x12, [x12, #8]
   170ec:	add	x13, x0, x11
   170f0:	add	x9, x9, #0x1
   170f4:	add	x11, x11, #0x8
   170f8:	sub	x14, x12, #0x1
   170fc:	sub	x10, x10, #0x1
   17100:	str	x14, [x13, #8]
   17104:	cbz	x12, 170dc <__gmpz_com@@Base+0x24c>
   17108:	cmp	x8, x0
   1710c:	b.eq	17228 <__gmpz_com@@Base+0x398>  // b.none
   17110:	cmp	x9, x20
   17114:	b.ge	17228 <__gmpz_com@@Base+0x398>  // b.tcont
   17118:	sub	x12, x20, x9
   1711c:	cmp	x12, #0x4
   17120:	b.cc	17190 <__gmpz_com@@Base+0x300>  // b.lo, b.ul, b.last
   17124:	add	x14, x0, x11
   17128:	lsl	x13, x20, #3
   1712c:	add	x14, x14, #0x8
   17130:	add	x15, x8, x13
   17134:	cmp	x14, x15
   17138:	b.cs	17150 <__gmpz_com@@Base+0x2c0>  // b.hs, b.nlast
   1713c:	add	x14, x8, x11
   17140:	add	x13, x0, x13
   17144:	add	x14, x14, #0x8
   17148:	cmp	x14, x13
   1714c:	b.cc	17190 <__gmpz_com@@Base+0x300>  // b.lo, b.ul, b.last
   17150:	add	x13, x0, x11
   17154:	add	x14, x8, x11
   17158:	and	x11, x12, #0xfffffffffffffffc
   1715c:	and	x15, x10, #0xfffffffffffffffc
   17160:	add	x10, x13, #0x18
   17164:	add	x13, x14, #0x18
   17168:	add	x9, x15, x9
   1716c:	mov	x14, x11
   17170:	ldp	q0, q1, [x13, #-16]
   17174:	add	x13, x13, #0x20
   17178:	subs	x14, x14, #0x4
   1717c:	stp	q0, q1, [x10, #-16]
   17180:	add	x10, x10, #0x20
   17184:	b.ne	17170 <__gmpz_com@@Base+0x2e0>  // b.any
   17188:	cmp	x12, x11
   1718c:	b.eq	17228 <__gmpz_com@@Base+0x398>  // b.none
   17190:	add	x10, x9, x22
   17194:	lsl	x11, x9, #3
   17198:	neg	x9, x10
   1719c:	add	x10, x0, x11
   171a0:	add	x8, x8, x11
   171a4:	ldr	x11, [x8], #8
   171a8:	subs	x9, x9, #0x1
   171ac:	str	x11, [x10], #8
   171b0:	b.ne	171a4 <__gmpz_com@@Base+0x314>  // b.any
   171b4:	b	17228 <__gmpz_com@@Base+0x398>
   171b8:	and	x11, x9, #0xfffffffffffffffc
   171bc:	add	x12, x8, #0x18
   171c0:	orr	x10, x11, #0x1
   171c4:	add	x13, x0, #0x18
   171c8:	mov	x14, x11
   171cc:	ldp	q0, q1, [x12, #-16]
   171d0:	add	x12, x12, #0x20
   171d4:	subs	x14, x14, #0x4
   171d8:	stp	q0, q1, [x13, #-16]
   171dc:	add	x13, x13, #0x20
   171e0:	b.ne	171cc <__gmpz_com@@Base+0x33c>  // b.any
   171e4:	cmp	x9, x11
   171e8:	b.eq	170bc <__gmpz_com@@Base+0x22c>  // b.none
   171ec:	b	1709c <__gmpz_com@@Base+0x20c>
   171f0:	mvn	x10, x22
   171f4:	and	x11, x10, #0xfffffffffffffffc
   171f8:	add	x12, x8, #0x18
   171fc:	orr	x9, x11, #0x1
   17200:	add	x13, x0, #0x18
   17204:	mov	x14, x11
   17208:	ldp	q0, q1, [x12, #-16]
   1720c:	add	x12, x12, #0x20
   17210:	subs	x14, x14, #0x4
   17214:	stp	q0, q1, [x13, #-16]
   17218:	add	x13, x13, #0x20
   1721c:	b.ne	17208 <__gmpz_com@@Base+0x378>  // b.any
   17220:	cmp	x11, x10
   17224:	b.ne	1702c <__gmpz_com@@Base+0x19c>  // b.any
   17228:	mvn	x8, x22
   1722c:	ldr	x8, [x0, x8, lsl #3]
   17230:	cmp	x8, #0x0
   17234:	csetm	w8, eq  // eq = none
   17238:	sub	w8, w8, w22
   1723c:	str	w8, [x19, #4]
   17240:	ldp	x20, x19, [sp, #32]
   17244:	ldp	x22, x21, [sp, #16]
   17248:	ldp	x29, x30, [sp], #48
   1724c:	ret
   17250:	cmp	w8, #0x0
   17254:	b.le	1728c <__gmpz_com@@Base+0x3fc>
   17258:	ldr	x0, [x19, #8]
   1725c:	mov	w8, #0x1                   	// #1
   17260:	str	x8, [x0]
   17264:	mov	w8, #0xffffffff            	// #-1
   17268:	b	1723c <__gmpz_com@@Base+0x3ac>
   1726c:	add	x1, x22, #0x1
   17270:	mov	x0, x19
   17274:	bl	c080 <__gmpz_realloc@plt>
   17278:	b	16ec4 <__gmpz_com@@Base+0x34>
   1727c:	mov	x0, x19
   17280:	mov	x1, x20
   17284:	bl	c080 <__gmpz_realloc@plt>
   17288:	b	16fd8 <__gmpz_com@@Base+0x148>
   1728c:	mov	w1, #0x1                   	// #1
   17290:	mov	x0, x19
   17294:	bl	c080 <__gmpz_realloc@plt>
   17298:	b	1725c <__gmpz_com@@Base+0x3cc>

000000000001729c <__gmpz_combit@@Base>:
   1729c:	stp	x29, x30, [sp, #-64]!
   172a0:	stp	x22, x21, [sp, #32]
   172a4:	stp	x20, x19, [sp, #48]
   172a8:	ldrsw	x8, [x0, #4]
   172ac:	ldr	x20, [x0, #8]
   172b0:	lsr	x22, x1, #6
   172b4:	mov	w9, #0x1                   	// #1
   172b8:	add	x21, x22, #0x1
   172bc:	str	x23, [sp, #16]
   172c0:	cmp	x21, x8
   172c4:	lsl	x23, x9, x1
   172c8:	mov	x29, sp
   172cc:	b.ge	172e4 <__gmpz_combit@@Base+0x48>  // b.tcont
   172d0:	lsl	x8, x22, #3
   172d4:	ldr	x9, [x20, x8]
   172d8:	eor	x9, x9, x23
   172dc:	str	x9, [x20, x8]
   172e0:	b	173ac <__gmpz_combit@@Base+0x110>
   172e4:	neg	x9, x8
   172e8:	mov	x19, x0
   172ec:	cmp	x22, x9
   172f0:	b.ge	17320 <__gmpz_combit@@Base+0x84>  // b.tcont
   172f4:	cbz	x22, 17310 <__gmpz_combit@@Base+0x74>
   172f8:	lsl	x10, x22, #3
   172fc:	sub	x11, x20, #0x8
   17300:	ldr	x12, [x11, x10]
   17304:	cbnz	x12, 17320 <__gmpz_combit@@Base+0x84>
   17308:	subs	x10, x10, #0x8
   1730c:	b.ne	17300 <__gmpz_combit@@Base+0x64>  // b.any
   17310:	ldr	x10, [x20, x22, lsl #3]
   17314:	sub	x11, x23, #0x1
   17318:	tst	x10, x11
   1731c:	b.eq	173c0 <__gmpz_combit@@Base+0x124>  // b.none
   17320:	cmp	x8, #0x0
   17324:	cneg	x9, x8, mi  // mi = first
   17328:	cmp	x22, x9
   1732c:	b.ge	17370 <__gmpz_combit@@Base+0xd4>  // b.tcont
   17330:	lsl	x10, x22, #3
   17334:	ldr	x11, [x20, x10]
   17338:	eor	x11, x11, x23
   1733c:	cmp	x11, #0x0
   17340:	cinc	x12, x22, eq  // eq = none
   17344:	cmp	x12, x9
   17348:	str	x11, [x20, x10]
   1734c:	b.ne	173ac <__gmpz_combit@@Base+0x110>  // b.any
   17350:	sub	x9, x20, #0x8
   17354:	subs	x10, x22, #0x1
   17358:	b.lt	17480 <__gmpz_combit@@Base+0x1e4>  // b.tstop
   1735c:	ldr	x11, [x9, x22, lsl #3]
   17360:	mov	x22, x10
   17364:	cbz	x11, 17354 <__gmpz_combit@@Base+0xb8>
   17368:	add	x9, x10, #0x1
   1736c:	b	17484 <__gmpz_combit@@Base+0x1e8>
   17370:	ldrsw	x8, [x19]
   17374:	cmp	x22, x8
   17378:	b.ge	1745c <__gmpz_combit@@Base+0x1c0>  // b.tcont
   1737c:	subs	x8, x22, x9
   17380:	b.eq	17394 <__gmpz_combit@@Base+0xf8>  // b.none
   17384:	add	x0, x20, x9, lsl #3
   17388:	lsl	x2, x8, #3
   1738c:	mov	w1, wzr
   17390:	bl	c5f0 <memset@plt>
   17394:	str	x23, [x20, x22, lsl #3]
   17398:	ldr	w8, [x19, #4]
   1739c:	mvn	w9, w22
   173a0:	cmp	w8, #0x0
   173a4:	csel	x8, x21, x9, ge  // ge = tcont
   173a8:	str	w8, [x19, #4]
   173ac:	ldp	x20, x19, [sp, #48]
   173b0:	ldp	x22, x21, [sp, #32]
   173b4:	ldr	x23, [sp, #16]
   173b8:	ldp	x29, x30, [sp], #64
   173bc:	ret
   173c0:	tst	x10, x23
   173c4:	b.eq	17420 <__gmpz_combit@@Base+0x184>  // b.none
   173c8:	ldrsw	x10, [x19]
   173cc:	mov	w11, #0x1                   	// #1
   173d0:	sub	x1, x11, x8
   173d4:	cmp	x1, x10
   173d8:	b.gt	17494 <__gmpz_combit@@Base+0x1f8>
   173dc:	str	xzr, [x20, x9, lsl #3]
   173e0:	lsl	x10, x22, #3
   173e4:	ldr	x11, [x20, x10]
   173e8:	adds	x11, x11, x23
   173ec:	str	x11, [x20, x10]
   173f0:	b.cc	1740c <__gmpz_combit@@Base+0x170>  // b.lo, b.ul, b.last
   173f4:	add	x10, x20, x22, lsl #3
   173f8:	add	x10, x10, #0x8
   173fc:	ldr	x11, [x10]
   17400:	adds	x11, x11, #0x1
   17404:	str	x11, [x10], #8
   17408:	b.cs	173fc <__gmpz_combit@@Base+0x160>  // b.hs, b.nlast
   1740c:	lsl	x9, x9, #3
   17410:	ldr	w9, [x20, x9]
   17414:	sub	w8, w8, w9
   17418:	str	w8, [x19, #4]
   1741c:	b	173ac <__gmpz_combit@@Base+0x110>
   17420:	subs	x9, x10, x23
   17424:	str	x9, [x20, x22, lsl #3]
   17428:	b.cs	17444 <__gmpz_combit@@Base+0x1a8>  // b.hs, b.nlast
   1742c:	add	x9, x20, x22, lsl #3
   17430:	add	x9, x9, #0x8
   17434:	ldr	x10, [x9]
   17438:	sub	x11, x10, #0x1
   1743c:	str	x11, [x9], #8
   17440:	cbz	x10, 17434 <__gmpz_combit@@Base+0x198>
   17444:	mvn	x9, x8
   17448:	ldr	x9, [x20, x9, lsl #3]
   1744c:	cmp	x9, #0x0
   17450:	cinc	w8, w8, eq  // eq = none
   17454:	str	w8, [x19, #4]
   17458:	b	173ac <__gmpz_combit@@Base+0x110>
   1745c:	mov	x0, x19
   17460:	mov	x1, x21
   17464:	mov	x20, x9
   17468:	bl	c080 <__gmpz_realloc@plt>
   1746c:	mov	x9, x20
   17470:	mov	x20, x0
   17474:	subs	x8, x22, x9
   17478:	b.ne	17384 <__gmpz_combit@@Base+0xe8>  // b.any
   1747c:	b	17394 <__gmpz_combit@@Base+0xf8>
   17480:	mov	x9, xzr
   17484:	neg	w10, w9
   17488:	cmp	w8, #0x0
   1748c:	csel	x8, x9, x10, ge  // ge = tcont
   17490:	b	173a8 <__gmpz_combit@@Base+0x10c>
   17494:	mov	x0, x19
   17498:	mov	x20, x8
   1749c:	mov	x21, x9
   174a0:	bl	c080 <__gmpz_realloc@plt>
   174a4:	mov	x9, x21
   174a8:	mov	x8, x20
   174ac:	mov	x20, x0
   174b0:	b	173dc <__gmpz_combit@@Base+0x140>

00000000000174b4 <__gmpz_congruent_p@@Base>:
   174b4:	stp	x29, x30, [sp, #-80]!
   174b8:	str	x25, [sp, #16]
   174bc:	stp	x24, x23, [sp, #32]
   174c0:	stp	x22, x21, [sp, #48]
   174c4:	stp	x20, x19, [sp, #64]
   174c8:	mov	x29, sp
   174cc:	sub	sp, sp, #0x10
   174d0:	ldrsw	x8, [x2, #4]
   174d4:	cbz	w8, 177d8 <__gmpz_congruent_p@@Base+0x324>
   174d8:	ldr	w9, [x0, #4]
   174dc:	ldr	w10, [x1, #4]
   174e0:	cmp	x8, #0x0
   174e4:	cneg	x21, x8, mi  // mi = first
   174e8:	cmp	w9, #0x0
   174ec:	cneg	w8, w9, mi  // mi = first
   174f0:	cmp	w10, #0x0
   174f4:	cneg	w9, w10, mi  // mi = first
   174f8:	cmp	w8, w9
   174fc:	csel	x11, x1, x0, lt  // lt = tstop
   17500:	ldrsw	x8, [x11, #4]
   17504:	csel	x10, x0, x1, lt  // lt = tstop
   17508:	ldr	x20, [x2, #8]
   1750c:	ldrsw	x9, [x10, #4]
   17510:	ldr	x22, [x11, #8]
   17514:	cmp	x8, #0x0
   17518:	cneg	x19, x8, mi  // mi = first
   1751c:	cbz	w9, 17564 <__gmpz_congruent_p@@Base+0xb0>
   17520:	ldr	x2, [x10, #8]
   17524:	ldr	x25, [x20]
   17528:	ldr	x10, [x22]
   1752c:	eor	w8, w9, w8
   17530:	ldr	x23, [x2]
   17534:	cmp	x9, #0x0
   17538:	cneg	x24, x9, mi  // mi = first
   1753c:	neg	x9, x25
   17540:	cmp	w8, #0x0
   17544:	and	x9, x25, x9
   17548:	cneg	x10, x10, lt  // lt = tstop
   1754c:	sub	x9, x9, #0x1
   17550:	sub	x10, x10, x23
   17554:	tst	x9, x10
   17558:	b.eq	17580 <__gmpz_congruent_p@@Base+0xcc>  // b.none
   1755c:	mov	w19, wzr
   17560:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   17564:	mov	x0, x22
   17568:	mov	x1, x19
   1756c:	mov	x2, x20
   17570:	mov	x3, x21
   17574:	bl	d360 <__gmpn_divisible_p@plt>
   17578:	mov	w19, w0
   1757c:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   17580:	cmp	x24, #0x1
   17584:	b.ne	175d0 <__gmpz_congruent_p@@Base+0x11c>  // b.any
   17588:	cmp	x21, #0x1
   1758c:	b.ne	175b8 <__gmpz_congruent_p@@Base+0x104>  // b.any
   17590:	tbz	w8, #31, 17758 <__gmpz_congruent_p@@Base+0x2a4>
   17594:	subs	x8, x25, x23
   17598:	b.cs	17754 <__gmpz_congruent_p@@Base+0x2a0>  // b.hs, b.nlast
   1759c:	clz	x8, x25
   175a0:	lsl	x8, x25, x8
   175a4:	cmp	x23, x8
   175a8:	cset	w9, hi  // hi = pmore
   175ac:	lsl	x8, x8, x9
   175b0:	sub	x23, x8, x23
   175b4:	b	17758 <__gmpz_congruent_p@@Base+0x2a4>
   175b8:	cmp	x21, #0x2
   175bc:	b.ne	175d0 <__gmpz_congruent_p@@Base+0x11c>  // b.any
   175c0:	cbz	x25, 175d0 <__gmpz_congruent_p@@Base+0x11c>
   175c4:	ldr	x10, [x20, #8]
   175c8:	cmp	x10, x9
   175cc:	b.ls	17734 <__gmpz_congruent_p@@Base+0x280>  // b.plast
   175d0:	lsl	x25, x19, #3
   175d4:	cmp	x19, #0xfdf
   175d8:	add	x1, x25, #0x8
   175dc:	str	xzr, [x29, #24]
   175e0:	b.hi	177e8 <__gmpz_congruent_p@@Base+0x334>  // b.pmore
   175e4:	add	x10, x1, #0xf
   175e8:	mov	x9, sp
   175ec:	and	x10, x10, #0xfffffffffffffff0
   175f0:	sub	x23, x9, x10
   175f4:	mov	sp, x23
   175f8:	tbnz	w8, #31, 17808 <__gmpz_congruent_p@@Base+0x354>
   175fc:	cmp	x19, x24
   17600:	b.le	176d8 <__gmpz_congruent_p@@Base+0x224>
   17604:	cbz	x24, 1763c <__gmpz_congruent_p@@Base+0x188>
   17608:	mov	x0, x23
   1760c:	mov	x1, x22
   17610:	mov	x3, x24
   17614:	bl	c2d0 <__gmpn_sub_n@plt>
   17618:	cbz	x0, 1763c <__gmpz_congruent_p@@Base+0x188>
   1761c:	cmp	x24, x19
   17620:	b.ge	17718 <__gmpz_congruent_p@@Base+0x264>  // b.tcont
   17624:	lsl	x8, x24, #3
   17628:	ldr	x9, [x22, x8]
   1762c:	add	x24, x24, #0x1
   17630:	sub	x10, x9, #0x1
   17634:	str	x10, [x23, x8]
   17638:	cbz	x9, 1761c <__gmpz_congruent_p@@Base+0x168>
   1763c:	cmp	x22, x23
   17640:	b.eq	17718 <__gmpz_congruent_p@@Base+0x264>  // b.none
   17644:	subs	x8, x19, x24
   17648:	b.le	17718 <__gmpz_congruent_p@@Base+0x264>
   1764c:	cmp	x8, #0x4
   17650:	b.cc	176b4 <__gmpz_congruent_p@@Base+0x200>  // b.lo, b.ul, b.last
   17654:	lsl	x10, x24, #3
   17658:	add	x9, x23, x10
   1765c:	add	x11, x22, x25
   17660:	cmp	x9, x11
   17664:	b.cs	17678 <__gmpz_congruent_p@@Base+0x1c4>  // b.hs, b.nlast
   17668:	add	x9, x23, x25
   1766c:	add	x11, x22, x10
   17670:	cmp	x9, x11
   17674:	b.hi	176b4 <__gmpz_congruent_p@@Base+0x200>  // b.pmore
   17678:	and	x9, x8, #0xfffffffffffffffc
   1767c:	add	x11, x10, x22
   17680:	add	x12, x10, x23
   17684:	add	x24, x24, x9
   17688:	add	x10, x11, #0x10
   1768c:	add	x11, x12, #0x10
   17690:	mov	x12, x9
   17694:	ldp	q0, q1, [x10, #-16]
   17698:	add	x10, x10, #0x20
   1769c:	subs	x12, x12, #0x4
   176a0:	stp	q0, q1, [x11, #-16]
   176a4:	add	x11, x11, #0x20
   176a8:	b.ne	17694 <__gmpz_congruent_p@@Base+0x1e0>  // b.any
   176ac:	cmp	x8, x9
   176b0:	b.eq	17718 <__gmpz_congruent_p@@Base+0x264>  // b.none
   176b4:	lsl	x10, x24, #3
   176b8:	sub	x8, x19, x24
   176bc:	add	x9, x23, x10
   176c0:	add	x10, x22, x10
   176c4:	ldr	x11, [x10], #8
   176c8:	subs	x8, x8, #0x1
   176cc:	str	x11, [x9], #8
   176d0:	b.ne	176c4 <__gmpz_congruent_p@@Base+0x210>  // b.any
   176d4:	b	17718 <__gmpz_congruent_p@@Base+0x264>
   176d8:	sub	x8, x19, #0x1
   176dc:	add	x9, x8, #0x1
   176e0:	cmp	x9, #0x1
   176e4:	b.lt	17604 <__gmpz_congruent_p@@Base+0x150>  // b.tstop
   176e8:	lsl	x9, x8, #3
   176ec:	ldr	x10, [x22, x9]
   176f0:	ldr	x9, [x2, x9]
   176f4:	sub	x8, x8, #0x1
   176f8:	cmp	x10, x9
   176fc:	b.eq	176dc <__gmpz_congruent_p@@Base+0x228>  // b.none
   17700:	b.hi	17604 <__gmpz_congruent_p@@Base+0x150>  // b.pmore
   17704:	mov	x0, x23
   17708:	mov	x1, x2
   1770c:	mov	x2, x22
   17710:	mov	x3, x19
   17714:	bl	c2d0 <__gmpn_sub_n@plt>
   17718:	sub	x8, x23, #0x8
   1771c:	mov	x1, x19
   17720:	subs	x19, x19, #0x1
   17724:	b.lt	178ec <__gmpz_congruent_p@@Base+0x438>  // b.tstop
   17728:	ldr	x9, [x8, x1, lsl #3]
   1772c:	cbz	x9, 1771c <__gmpz_congruent_p@@Base+0x268>
   17730:	b	178ec <__gmpz_congruent_p@@Base+0x438>
   17734:	rbit	x9, x25
   17738:	clz	x9, x9
   1773c:	lsr	x11, x25, x9
   17740:	neg	x9, x9
   17744:	lsl	x9, x10, x9
   17748:	orr	x25, x9, x11
   1774c:	tbz	w8, #31, 17758 <__gmpz_congruent_p@@Base+0x2a4>
   17750:	b	17594 <__gmpz_congruent_p@@Base+0xe0>
   17754:	mov	x23, x8
   17758:	cmp	x19, #0x28
   1775c:	b.lt	17784 <__gmpz_congruent_p@@Base+0x2d0>  // b.tstop
   17760:	mov	x0, x22
   17764:	mov	x1, x19
   17768:	mov	x2, x25
   1776c:	bl	c3e0 <__gmpn_mod_1@plt>
   17770:	cmp	x23, x25
   17774:	b.cs	177c4 <__gmpz_congruent_p@@Base+0x310>  // b.hs, b.nlast
   17778:	cmp	x0, x23
   1777c:	cset	w19, eq  // eq = none
   17780:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   17784:	rbit	x8, x25
   17788:	clz	x8, x8
   1778c:	tst	x25, #0x1
   17790:	csel	x8, x8, xzr, eq  // eq = none
   17794:	lsr	x20, x25, x8
   17798:	mov	x0, x22
   1779c:	mov	x1, x19
   177a0:	mov	x2, x20
   177a4:	mov	x3, x23
   177a8:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   177ac:	cmp	x0, #0x0
   177b0:	cset	w8, eq  // eq = none
   177b4:	cmp	x0, x20
   177b8:	cset	w9, eq  // eq = none
   177bc:	orr	w19, w8, w9
   177c0:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   177c4:	udiv	x8, x23, x25
   177c8:	msub	x8, x8, x25, x23
   177cc:	cmp	x0, x8
   177d0:	cset	w19, eq  // eq = none
   177d4:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   177d8:	bl	ce70 <__gmpz_cmp@plt>
   177dc:	cmp	w0, #0x0
   177e0:	cset	w19, eq  // eq = none
   177e4:	b	17908 <__gmpz_congruent_p@@Base+0x454>
   177e8:	add	x0, x29, #0x18
   177ec:	stur	x2, [x29, #-8]
   177f0:	mov	w23, w8
   177f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   177f8:	ldur	x2, [x29, #-8]
   177fc:	mov	w8, w23
   17800:	mov	x23, x0
   17804:	tbz	w8, #31, 175fc <__gmpz_congruent_p@@Base+0x148>
   17808:	cbz	x24, 17844 <__gmpz_congruent_p@@Base+0x390>
   1780c:	mov	x0, x23
   17810:	mov	x1, x22
   17814:	mov	x3, x24
   17818:	bl	ca70 <__gmpn_add_n@plt>
   1781c:	cbz	x0, 17844 <__gmpz_congruent_p@@Base+0x390>
   17820:	mov	w9, #0x1                   	// #1
   17824:	cmp	x24, x19
   17828:	b.ge	178e4 <__gmpz_congruent_p@@Base+0x430>  // b.tcont
   1782c:	lsl	x8, x24, #3
   17830:	ldr	x10, [x22, x8]
   17834:	add	x24, x24, #0x1
   17838:	adds	x10, x10, #0x1
   1783c:	str	x10, [x23, x8]
   17840:	b.cs	17824 <__gmpz_congruent_p@@Base+0x370>  // b.hs, b.nlast
   17844:	cmp	x22, x23
   17848:	mov	x9, xzr
   1784c:	b.eq	178e4 <__gmpz_congruent_p@@Base+0x430>  // b.none
   17850:	subs	x8, x19, x24
   17854:	b.le	178e4 <__gmpz_congruent_p@@Base+0x430>
   17858:	cmp	x8, #0x4
   1785c:	b.cc	178c0 <__gmpz_congruent_p@@Base+0x40c>  // b.lo, b.ul, b.last
   17860:	lsl	x10, x24, #3
   17864:	add	x9, x23, x10
   17868:	add	x11, x22, x25
   1786c:	cmp	x9, x11
   17870:	b.cs	17884 <__gmpz_congruent_p@@Base+0x3d0>  // b.hs, b.nlast
   17874:	add	x9, x23, x25
   17878:	add	x11, x22, x10
   1787c:	cmp	x9, x11
   17880:	b.hi	178c0 <__gmpz_congruent_p@@Base+0x40c>  // b.pmore
   17884:	and	x9, x8, #0xfffffffffffffffc
   17888:	add	x11, x10, x22
   1788c:	add	x12, x10, x23
   17890:	add	x24, x24, x9
   17894:	add	x10, x11, #0x10
   17898:	add	x11, x12, #0x10
   1789c:	mov	x12, x9
   178a0:	ldp	q0, q1, [x10, #-16]
   178a4:	add	x10, x10, #0x20
   178a8:	subs	x12, x12, #0x4
   178ac:	stp	q0, q1, [x11, #-16]
   178b0:	add	x11, x11, #0x20
   178b4:	b.ne	178a0 <__gmpz_congruent_p@@Base+0x3ec>  // b.any
   178b8:	cmp	x8, x9
   178bc:	b.eq	178e0 <__gmpz_congruent_p@@Base+0x42c>  // b.none
   178c0:	lsl	x10, x24, #3
   178c4:	sub	x8, x19, x24
   178c8:	add	x9, x23, x10
   178cc:	add	x10, x22, x10
   178d0:	ldr	x11, [x10], #8
   178d4:	subs	x8, x8, #0x1
   178d8:	str	x11, [x9], #8
   178dc:	b.ne	178d0 <__gmpz_congruent_p@@Base+0x41c>  // b.any
   178e0:	mov	x9, xzr
   178e4:	add	x1, x9, x19
   178e8:	str	x9, [x23, x19, lsl #3]
   178ec:	mov	x0, x23
   178f0:	mov	x2, x20
   178f4:	mov	x3, x21
   178f8:	bl	d360 <__gmpn_divisible_p@plt>
   178fc:	ldr	x8, [x29, #24]
   17900:	mov	w19, w0
   17904:	cbnz	x8, 17928 <__gmpz_congruent_p@@Base+0x474>
   17908:	mov	w0, w19
   1790c:	mov	sp, x29
   17910:	ldp	x20, x19, [sp, #64]
   17914:	ldp	x22, x21, [sp, #48]
   17918:	ldp	x24, x23, [sp, #32]
   1791c:	ldr	x25, [sp, #16]
   17920:	ldp	x29, x30, [sp], #80
   17924:	ret
   17928:	mov	x0, x8
   1792c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   17930:	b	17908 <__gmpz_congruent_p@@Base+0x454>

0000000000017934 <__gmpz_congruent_2exp_p@@Base>:
   17934:	ldrsw	x13, [x0, #4]
   17938:	ldrsw	x15, [x1, #4]
   1793c:	cmp	x13, #0x0
   17940:	cneg	x8, x13, mi  // mi = first
   17944:	cmp	x15, #0x0
   17948:	cneg	x9, x15, mi  // mi = first
   1794c:	cmp	x8, x9
   17950:	csel	x11, x9, x8, lt  // lt = tstop
   17954:	csel	x12, x8, x9, lt  // lt = tstop
   17958:	csel	x8, x1, x0, lt  // lt = tstop
   1795c:	ldr	x10, [x8, #8]
   17960:	mov	x8, #0xffffffffffffffff    	// #-1
   17964:	lsl	x8, x8, x2
   17968:	csel	x14, x0, x1, lt  // lt = tstop
   1796c:	lsr	x9, x2, #6
   17970:	mvn	x8, x8
   17974:	cbz	x12, 17a4c <__gmpz_congruent_2exp_p@@Base+0x118>
   17978:	ldr	x14, [x14, #8]
   1797c:	eor	w13, w15, w13
   17980:	tbnz	w13, #31, 179bc <__gmpz_congruent_2exp_p@@Base+0x88>
   17984:	cmp	x12, x9
   17988:	sub	x13, x14, #0x8
   1798c:	csel	x16, x12, x9, cc  // cc = lo, ul, last
   17990:	sub	x15, x10, #0x8
   17994:	subs	x17, x16, #0x1
   17998:	b.lt	17a30 <__gmpz_congruent_2exp_p@@Base+0xfc>  // b.tstop
   1799c:	lsl	x16, x16, #3
   179a0:	ldr	x18, [x15, x16]
   179a4:	ldr	x16, [x13, x16]
   179a8:	cmp	x18, x16
   179ac:	mov	x16, x17
   179b0:	b.eq	17994 <__gmpz_congruent_2exp_p@@Base+0x60>  // b.none
   179b4:	mov	w0, wzr
   179b8:	ret
   179bc:	mov	x15, xzr
   179c0:	and	w13, w2, #0x3f
   179c4:	lsl	x17, x15, #3
   179c8:	ldr	x16, [x10, x17]
   179cc:	ldr	x17, [x14, x17]
   179d0:	cmp	x9, x15
   179d4:	add	x17, x17, x16
   179d8:	b.eq	17a8c <__gmpz_congruent_2exp_p@@Base+0x158>  // b.none
   179dc:	cbnz	x17, 17acc <__gmpz_congruent_2exp_p@@Base+0x198>
   179e0:	add	x15, x15, #0x1
   179e4:	cbz	x16, 179c4 <__gmpz_congruent_2exp_p@@Base+0x90>
   179e8:	cmp	x15, x12
   179ec:	b.cs	17a20 <__gmpz_congruent_2exp_p@@Base+0xec>  // b.hs, b.nlast
   179f0:	lsl	x16, x15, #3
   179f4:	ldr	x17, [x10, x16]
   179f8:	ldr	x16, [x14, x16]
   179fc:	cmp	x15, x9
   17a00:	eor	x16, x16, x17
   17a04:	b.cs	17ad4 <__gmpz_congruent_2exp_p@@Base+0x1a0>  // b.hs, b.nlast
   17a08:	cmn	x16, #0x1
   17a0c:	b.ne	17acc <__gmpz_congruent_2exp_p@@Base+0x198>  // b.any
   17a10:	add	x15, x15, #0x1
   17a14:	cmp	x15, x12
   17a18:	b.cc	179f0 <__gmpz_congruent_2exp_p@@Base+0xbc>  // b.lo, b.ul, b.last
   17a1c:	mov	x15, x12
   17a20:	cmp	x11, x9
   17a24:	b.cs	17a98 <__gmpz_congruent_2exp_p@@Base+0x164>  // b.hs, b.nlast
   17a28:	mov	w0, wzr
   17a2c:	ret
   17a30:	cmp	x12, x9
   17a34:	b.ls	17a4c <__gmpz_congruent_2exp_p@@Base+0x118>  // b.plast
   17a38:	lsl	x9, x9, #3
   17a3c:	ldr	x10, [x10, x9]
   17a40:	ldr	x9, [x14, x9]
   17a44:	sub	x9, x10, x9
   17a48:	b	17a74 <__gmpz_congruent_2exp_p@@Base+0x140>
   17a4c:	cmp	x11, x9
   17a50:	b.ls	17a80 <__gmpz_congruent_2exp_p@@Base+0x14c>  // b.plast
   17a54:	cmp	x12, x9
   17a58:	b.cs	17a70 <__gmpz_congruent_2exp_p@@Base+0x13c>  // b.hs, b.nlast
   17a5c:	ldr	x11, [x10, x12, lsl #3]
   17a60:	cbnz	x11, 17acc <__gmpz_congruent_2exp_p@@Base+0x198>
   17a64:	add	x12, x12, #0x1
   17a68:	cmp	x12, x9
   17a6c:	b.cc	17a5c <__gmpz_congruent_2exp_p@@Base+0x128>  // b.lo, b.ul, b.last
   17a70:	ldr	x9, [x10, x9, lsl #3]
   17a74:	tst	x9, x8
   17a78:	cset	w0, eq  // eq = none
   17a7c:	ret
   17a80:	cmp	x11, x12
   17a84:	cset	w0, eq  // eq = none
   17a88:	ret
   17a8c:	tst	x17, x8
   17a90:	cset	w0, eq  // eq = none
   17a94:	ret
   17a98:	cmp	x15, x9
   17a9c:	b.cs	17ac0 <__gmpz_congruent_2exp_p@@Base+0x18c>  // b.hs, b.nlast
   17aa0:	sub	x12, x9, x15
   17aa4:	add	x14, x10, x15, lsl #3
   17aa8:	ldr	x15, [x14]
   17aac:	cmn	x15, #0x1
   17ab0:	b.ne	17acc <__gmpz_congruent_2exp_p@@Base+0x198>  // b.any
   17ab4:	subs	x12, x12, #0x1
   17ab8:	add	x14, x14, #0x8
   17abc:	b.ne	17aa8 <__gmpz_congruent_2exp_p@@Base+0x174>  // b.any
   17ac0:	cbz	w13, 17ae0 <__gmpz_congruent_2exp_p@@Base+0x1ac>
   17ac4:	cmp	x11, x9
   17ac8:	b.ne	17ae8 <__gmpz_congruent_2exp_p@@Base+0x1b4>  // b.any
   17acc:	mov	w0, wzr
   17ad0:	ret
   17ad4:	bics	xzr, x8, x16
   17ad8:	cset	w0, eq  // eq = none
   17adc:	ret
   17ae0:	mov	w0, #0x1                   	// #1
   17ae4:	ret
   17ae8:	ldr	x9, [x10, x9, lsl #3]
   17aec:	add	x9, x9, #0x1
   17af0:	b	17a74 <__gmpz_congruent_2exp_p@@Base+0x140>

0000000000017af4 <__gmpz_congruent_ui_p@@Base>:
   17af4:	stp	x29, x30, [sp, #-32]!
   17af8:	stp	x20, x19, [sp, #16]
   17afc:	mov	x20, x1
   17b00:	mov	x29, sp
   17b04:	cbz	x2, 17c08 <__gmpz_congruent_ui_p@@Base+0x114>
   17b08:	ldrsw	x1, [x0, #4]
   17b0c:	mov	x19, x2
   17b10:	cbz	w1, 17b40 <__gmpz_congruent_ui_p@@Base+0x4c>
   17b14:	tbz	w1, #31, 17b6c <__gmpz_congruent_ui_p@@Base+0x78>
   17b18:	subs	x8, x19, x20
   17b1c:	neg	x1, x1
   17b20:	b.cs	17b68 <__gmpz_congruent_ui_p@@Base+0x74>  // b.hs, b.nlast
   17b24:	clz	x8, x19
   17b28:	lsl	x8, x19, x8
   17b2c:	cmp	x8, x20
   17b30:	cset	w9, cc  // cc = lo, ul, last
   17b34:	lsl	x8, x8, x9
   17b38:	sub	x20, x8, x20
   17b3c:	b	17b6c <__gmpz_congruent_ui_p@@Base+0x78>
   17b40:	cmp	x19, x20
   17b44:	b.ls	17b54 <__gmpz_congruent_ui_p@@Base+0x60>  // b.plast
   17b48:	cmp	x20, #0x0
   17b4c:	cset	w0, eq  // eq = none
   17b50:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>
   17b54:	udiv	x8, x20, x19
   17b58:	msub	x8, x8, x19, x20
   17b5c:	cmp	x8, #0x0
   17b60:	cset	w0, eq  // eq = none
   17b64:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>
   17b68:	mov	x20, x8
   17b6c:	ldr	x0, [x0, #8]
   17b70:	cmp	x1, #0x28
   17b74:	b.lt	17b94 <__gmpz_congruent_ui_p@@Base+0xa0>  // b.tstop
   17b78:	mov	x2, x19
   17b7c:	bl	c3e0 <__gmpn_mod_1@plt>
   17b80:	cmp	x20, x19
   17b84:	b.cs	17bbc <__gmpz_congruent_ui_p@@Base+0xc8>  // b.hs, b.nlast
   17b88:	cmp	x0, x20
   17b8c:	cset	w0, eq  // eq = none
   17b90:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>
   17b94:	tbnz	w19, #0, 17bdc <__gmpz_congruent_ui_p@@Base+0xe8>
   17b98:	ldr	x8, [x0]
   17b9c:	neg	x9, x19
   17ba0:	and	x9, x9, x19
   17ba4:	sub	x9, x9, #0x1
   17ba8:	sub	x8, x8, x20
   17bac:	tst	x8, x9
   17bb0:	b.eq	17bd0 <__gmpz_congruent_ui_p@@Base+0xdc>  // b.none
   17bb4:	mov	w0, wzr
   17bb8:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>
   17bbc:	udiv	x8, x20, x19
   17bc0:	msub	x8, x8, x19, x20
   17bc4:	cmp	x0, x8
   17bc8:	cset	w0, eq  // eq = none
   17bcc:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>
   17bd0:	rbit	x8, x19
   17bd4:	clz	x8, x8
   17bd8:	lsr	x19, x19, x8
   17bdc:	mov	x2, x19
   17be0:	mov	x3, x20
   17be4:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   17be8:	cmp	x0, #0x0
   17bec:	cset	w8, eq  // eq = none
   17bf0:	cmp	x0, x19
   17bf4:	cset	w9, eq  // eq = none
   17bf8:	orr	w0, w8, w9
   17bfc:	ldp	x20, x19, [sp, #16]
   17c00:	ldp	x29, x30, [sp], #32
   17c04:	ret
   17c08:	mov	x1, x20
   17c0c:	bl	d1f0 <__gmpz_cmp_ui@plt>
   17c10:	cmp	w0, #0x0
   17c14:	cset	w0, eq  // eq = none
   17c18:	b	17bfc <__gmpz_congruent_ui_p@@Base+0x108>

0000000000017c1c <__gmpz_divexact@@Base>:
   17c1c:	stp	x29, x30, [sp, #-80]!
   17c20:	stp	x24, x23, [sp, #32]
   17c24:	stp	x22, x21, [sp, #48]
   17c28:	stp	x20, x19, [sp, #64]
   17c2c:	ldr	w8, [x1, #4]
   17c30:	ldr	w9, [x2, #4]
   17c34:	mov	x19, x0
   17c38:	str	x25, [sp, #16]
   17c3c:	cmp	w8, #0x0
   17c40:	cneg	w23, w8, mi  // mi = first
   17c44:	cmp	w9, #0x0
   17c48:	cneg	w22, w9, mi  // mi = first
   17c4c:	cmp	w23, w22
   17c50:	mov	x29, sp
   17c54:	b.cs	17c60 <__gmpz_divexact@@Base+0x44>  // b.hs, b.nlast
   17c58:	str	wzr, [x19, #4]
   17c5c:	b	17d38 <__gmpz_divexact@@Base+0x11c>
   17c60:	sub	x25, x23, x22
   17c64:	mov	x20, x1
   17c68:	mov	x21, x2
   17c6c:	cmp	x19, x1
   17c70:	add	x1, x25, #0x1
   17c74:	str	xzr, [x29, #24]
   17c78:	b.eq	17c98 <__gmpz_divexact@@Base+0x7c>  // b.none
   17c7c:	cmp	x19, x21
   17c80:	b.eq	17c98 <__gmpz_divexact@@Base+0x7c>  // b.none
   17c84:	ldrsw	x8, [x19]
   17c88:	cmp	x25, x8
   17c8c:	b.ge	17d6c <__gmpz_divexact@@Base+0x150>  // b.tcont
   17c90:	ldr	x24, [x19, #8]
   17c94:	b	17cbc <__gmpz_divexact@@Base+0xa0>
   17c98:	lsl	x1, x1, #3
   17c9c:	mov	w8, #0x7f00                	// #32512
   17ca0:	cmp	x1, x8
   17ca4:	b.hi	17d7c <__gmpz_divexact@@Base+0x160>  // b.pmore
   17ca8:	add	x9, x1, #0xf
   17cac:	mov	x8, sp
   17cb0:	and	x9, x9, #0xfffffffffffffff0
   17cb4:	sub	x24, x8, x9
   17cb8:	mov	sp, x24
   17cbc:	ldr	x1, [x20, #8]
   17cc0:	ldr	x3, [x21, #8]
   17cc4:	mov	x0, x24
   17cc8:	mov	x2, x23
   17ccc:	mov	x4, x22
   17cd0:	bl	c430 <__gmpn_divexact@plt>
   17cd4:	add	x22, x25, #0x1
   17cd8:	mov	x23, x25
   17cdc:	cmp	x22, #0x1
   17ce0:	b.lt	17cf0 <__gmpz_divexact@@Base+0xd4>  // b.tstop
   17ce4:	ldr	x8, [x24, x23, lsl #3]
   17ce8:	sub	x25, x23, #0x1
   17cec:	cbz	x8, 17cd4 <__gmpz_divexact@@Base+0xb8>
   17cf0:	ldr	x0, [x19, #8]
   17cf4:	cmp	x24, x0
   17cf8:	b.eq	17d14 <__gmpz_divexact@@Base+0xf8>  // b.none
   17cfc:	ldrsw	x8, [x19]
   17d00:	cmp	x22, x8
   17d04:	b.gt	17d5c <__gmpz_divexact@@Base+0x140>
   17d08:	mov	x1, x24
   17d0c:	mov	x2, x22
   17d10:	bl	ca50 <__gmpn_copyi@plt>
   17d14:	ldr	w8, [x20, #4]
   17d18:	ldr	w9, [x21, #4]
   17d1c:	eor	w8, w9, w8
   17d20:	mvn	w9, w23
   17d24:	cmp	w8, #0x0
   17d28:	csel	x8, x22, x9, ge  // ge = tcont
   17d2c:	str	w8, [x19, #4]
   17d30:	ldr	x0, [x29, #24]
   17d34:	cbnz	x0, 17d54 <__gmpz_divexact@@Base+0x138>
   17d38:	mov	sp, x29
   17d3c:	ldp	x20, x19, [sp, #64]
   17d40:	ldp	x22, x21, [sp, #48]
   17d44:	ldp	x24, x23, [sp, #32]
   17d48:	ldr	x25, [sp, #16]
   17d4c:	ldp	x29, x30, [sp], #80
   17d50:	ret
   17d54:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   17d58:	b	17d38 <__gmpz_divexact@@Base+0x11c>
   17d5c:	mov	x0, x19
   17d60:	mov	x1, x22
   17d64:	bl	c080 <__gmpz_realloc@plt>
   17d68:	b	17d08 <__gmpz_divexact@@Base+0xec>
   17d6c:	mov	x0, x19
   17d70:	bl	c080 <__gmpz_realloc@plt>
   17d74:	mov	x24, x0
   17d78:	b	17cbc <__gmpz_divexact@@Base+0xa0>
   17d7c:	add	x0, x29, #0x18
   17d80:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   17d84:	mov	x24, x0
   17d88:	b	17cbc <__gmpz_divexact@@Base+0xa0>

0000000000017d8c <__gmpz_divexact_gcd@@Base>:
   17d8c:	stp	x29, x30, [sp, #-64]!
   17d90:	stp	x22, x21, [sp, #32]
   17d94:	stp	x20, x19, [sp, #48]
   17d98:	ldr	w8, [x1, #4]
   17d9c:	mov	x19, x0
   17da0:	str	x23, [sp, #16]
   17da4:	mov	x29, sp
   17da8:	cbz	w8, 17e14 <__gmpz_divexact_gcd@@Base+0x88>
   17dac:	ldr	w8, [x2, #4]
   17db0:	cmp	w8, #0x1
   17db4:	b.ne	17e1c <__gmpz_divexact_gcd@@Base+0x90>  // b.any
   17db8:	ldr	x8, [x2, #8]
   17dbc:	ldr	x20, [x8]
   17dc0:	tbnz	w20, #0, 17ddc <__gmpz_divexact_gcd@@Base+0x50>
   17dc4:	rbit	x8, x20
   17dc8:	clz	x2, x8
   17dcc:	mov	x0, x19
   17dd0:	lsr	x20, x20, x2
   17dd4:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   17dd8:	mov	x1, x19
   17ddc:	cmp	x20, #0x5
   17de0:	b.eq	17e34 <__gmpz_divexact_gcd@@Base+0xa8>  // b.none
   17de4:	cmp	x20, #0x3
   17de8:	b.eq	17e5c <__gmpz_divexact_gcd@@Base+0xd0>  // b.none
   17dec:	cmp	x20, #0x1
   17df0:	b.ne	17eb0 <__gmpz_divexact_gcd@@Base+0x124>  // b.any
   17df4:	cmp	x1, x19
   17df8:	b.eq	17f04 <__gmpz_divexact_gcd@@Base+0x178>  // b.none
   17dfc:	mov	x0, x19
   17e00:	ldp	x20, x19, [sp, #48]
   17e04:	ldp	x22, x21, [sp, #32]
   17e08:	ldr	x23, [sp, #16]
   17e0c:	ldp	x29, x30, [sp], #64
   17e10:	b	c420 <__gmpz_set@plt>
   17e14:	str	wzr, [x19, #4]
   17e18:	b	17f04 <__gmpz_divexact_gcd@@Base+0x178>
   17e1c:	mov	x0, x19
   17e20:	ldp	x20, x19, [sp, #48]
   17e24:	ldp	x22, x21, [sp, #32]
   17e28:	ldr	x23, [sp, #16]
   17e2c:	ldp	x29, x30, [sp], #64
   17e30:	b	c3f0 <__gmpz_divexact@plt>
   17e34:	ldrsw	x22, [x1, #4]
   17e38:	ldrsw	x8, [x19]
   17e3c:	cmp	x22, #0x0
   17e40:	cneg	x20, x22, mi  // mi = first
   17e44:	cmp	x20, x8
   17e48:	b.gt	17f18 <__gmpz_divexact_gcd@@Base+0x18c>
   17e4c:	ldr	x21, [x19, #8]
   17e50:	ldr	x1, [x1, #8]
   17e54:	mov	x3, #0x3333333333333333    	// #3689348814741910323
   17e58:	b	17e80 <__gmpz_divexact_gcd@@Base+0xf4>
   17e5c:	ldrsw	x22, [x1, #4]
   17e60:	ldrsw	x8, [x19]
   17e64:	cmp	x22, #0x0
   17e68:	cneg	x20, x22, mi  // mi = first
   17e6c:	cmp	x20, x8
   17e70:	b.gt	17f34 <__gmpz_divexact_gcd@@Base+0x1a8>
   17e74:	ldr	x21, [x19, #8]
   17e78:	ldr	x1, [x1, #8]
   17e7c:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   17e80:	mov	x0, x21
   17e84:	mov	x2, x20
   17e88:	mov	x4, xzr
   17e8c:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   17e90:	add	x8, x21, x20, lsl #3
   17e94:	ldur	x8, [x8, #-8]
   17e98:	cmp	x8, #0x0
   17e9c:	cset	w8, eq  // eq = none
   17ea0:	sub	x8, x20, x8
   17ea4:	neg	w9, w8
   17ea8:	cmp	w22, #0x0
   17eac:	b	17efc <__gmpz_divexact_gcd@@Base+0x170>
   17eb0:	ldrsw	x23, [x1, #4]
   17eb4:	ldrsw	x8, [x19]
   17eb8:	cmp	x23, #0x0
   17ebc:	cneg	x21, x23, mi  // mi = first
   17ec0:	cmp	x21, x8
   17ec4:	b.gt	17f50 <__gmpz_divexact_gcd@@Base+0x1c4>
   17ec8:	ldr	x22, [x19, #8]
   17ecc:	ldr	x1, [x1, #8]
   17ed0:	mov	x0, x22
   17ed4:	mov	x2, x21
   17ed8:	mov	x3, x20
   17edc:	bl	c770 <__gmpn_divexact_1@plt>
   17ee0:	add	x8, x22, x21, lsl #3
   17ee4:	ldur	x8, [x8, #-8]
   17ee8:	cmp	x8, #0x0
   17eec:	cset	w8, eq  // eq = none
   17ef0:	sub	x8, x21, x8
   17ef4:	neg	w9, w8
   17ef8:	cmp	w23, #0x0
   17efc:	csel	x8, x8, x9, gt
   17f00:	str	w8, [x19, #4]
   17f04:	ldp	x20, x19, [sp, #48]
   17f08:	ldp	x22, x21, [sp, #32]
   17f0c:	ldr	x23, [sp, #16]
   17f10:	ldp	x29, x30, [sp], #64
   17f14:	ret
   17f18:	mov	x0, x19
   17f1c:	mov	x21, x1
   17f20:	mov	x1, x20
   17f24:	bl	c080 <__gmpz_realloc@plt>
   17f28:	mov	x1, x21
   17f2c:	mov	x21, x0
   17f30:	b	17e50 <__gmpz_divexact_gcd@@Base+0xc4>
   17f34:	mov	x0, x19
   17f38:	mov	x21, x1
   17f3c:	mov	x1, x20
   17f40:	bl	c080 <__gmpz_realloc@plt>
   17f44:	mov	x1, x21
   17f48:	mov	x21, x0
   17f4c:	b	17e78 <__gmpz_divexact_gcd@@Base+0xec>
   17f50:	mov	x0, x19
   17f54:	mov	x22, x1
   17f58:	mov	x1, x21
   17f5c:	bl	c080 <__gmpz_realloc@plt>
   17f60:	mov	x1, x22
   17f64:	mov	x22, x0
   17f68:	b	17ecc <__gmpz_divexact_gcd@@Base+0x140>

0000000000017f6c <__gmpz_divexact_ui@@Base>:
   17f6c:	stp	x29, x30, [sp, #-64]!
   17f70:	stp	x24, x23, [sp, #16]
   17f74:	stp	x22, x21, [sp, #32]
   17f78:	stp	x20, x19, [sp, #48]
   17f7c:	mov	x29, sp
   17f80:	cbz	x2, 18014 <__gmpz_divexact_ui@@Base+0xa8>
   17f84:	ldrsw	x24, [x1, #4]
   17f88:	mov	x21, x1
   17f8c:	mov	x19, x0
   17f90:	cbz	w24, 17fe4 <__gmpz_divexact_ui@@Base+0x78>
   17f94:	ldrsw	x8, [x19]
   17f98:	cmp	w24, #0x0
   17f9c:	cneg	x22, x24, lt  // lt = tstop
   17fa0:	mov	x20, x2
   17fa4:	cmp	x22, x8
   17fa8:	b.gt	18000 <__gmpz_divexact_ui@@Base+0x94>
   17fac:	ldr	x23, [x19, #8]
   17fb0:	ldr	x1, [x21, #8]
   17fb4:	mov	x0, x23
   17fb8:	mov	x2, x22
   17fbc:	mov	x3, x20
   17fc0:	bl	c770 <__gmpn_divexact_1@plt>
   17fc4:	add	x8, x23, x22, lsl #3
   17fc8:	ldur	x8, [x8, #-8]
   17fcc:	cmp	x8, #0x0
   17fd0:	cset	w8, eq  // eq = none
   17fd4:	sub	w8, w22, w8
   17fd8:	cmp	w24, #0x0
   17fdc:	cneg	w8, w8, lt  // lt = tstop
   17fe0:	b	17fe8 <__gmpz_divexact_ui@@Base+0x7c>
   17fe4:	mov	w8, wzr
   17fe8:	str	w8, [x19, #4]
   17fec:	ldp	x20, x19, [sp, #48]
   17ff0:	ldp	x22, x21, [sp, #32]
   17ff4:	ldp	x24, x23, [sp, #16]
   17ff8:	ldp	x29, x30, [sp], #64
   17ffc:	ret
   18000:	mov	x0, x19
   18004:	mov	x1, x22
   18008:	bl	c080 <__gmpz_realloc@plt>
   1800c:	mov	x23, x0
   18010:	b	17fb0 <__gmpz_divexact_ui@@Base+0x44>
   18014:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000018018 <__gmpz_divisible_p@@Base>:
   18018:	ldrsw	x8, [x1, #4]
   1801c:	ldrsw	x9, [x0, #4]
   18020:	cbz	w8, 18040 <__gmpz_divisible_p@@Base+0x28>
   18024:	ldr	x0, [x0, #8]
   18028:	ldr	x2, [x1, #8]
   1802c:	cmp	x9, #0x0
   18030:	cneg	x1, x9, mi  // mi = first
   18034:	cmp	x8, #0x0
   18038:	cneg	x3, x8, mi  // mi = first
   1803c:	b	d360 <__gmpn_divisible_p@plt>
   18040:	cmp	w9, #0x0
   18044:	cset	w0, eq  // eq = none
   18048:	ret

000000000001804c <__gmpz_divisible_ui_p@@Base>:
   1804c:	stp	x29, x30, [sp, #-16]!
   18050:	ldrsw	x9, [x0, #4]
   18054:	mov	x8, x0
   18058:	mov	x29, sp
   1805c:	cmp	x9, #0x0
   18060:	cset	w10, eq  // eq = none
   18064:	cmp	x1, #0x0
   18068:	cset	w11, ne  // ne = any
   1806c:	orr	w0, w10, w11
   18070:	cbz	x1, 180dc <__gmpz_divisible_ui_p@@Base+0x90>
   18074:	cbz	w9, 180dc <__gmpz_divisible_ui_p@@Base+0x90>
   18078:	ldr	x0, [x8, #8]
   1807c:	cmp	x9, #0x0
   18080:	mov	x2, x1
   18084:	cneg	x1, x9, mi  // mi = first
   18088:	cmp	x1, #0x28
   1808c:	b.lt	18098 <__gmpz_divisible_ui_p@@Base+0x4c>  // b.tstop
   18090:	bl	c3e0 <__gmpn_mod_1@plt>
   18094:	b	180d4 <__gmpz_divisible_ui_p@@Base+0x88>
   18098:	tbnz	w2, #0, 180cc <__gmpz_divisible_ui_p@@Base+0x80>
   1809c:	ldr	x8, [x0]
   180a0:	neg	x9, x2
   180a4:	and	x9, x9, x2
   180a8:	sub	x9, x9, #0x1
   180ac:	tst	x8, x9
   180b0:	b.eq	180c0 <__gmpz_divisible_ui_p@@Base+0x74>  // b.none
   180b4:	mov	w0, wzr
   180b8:	ldp	x29, x30, [sp], #16
   180bc:	ret
   180c0:	rbit	x8, x2
   180c4:	clz	x8, x8
   180c8:	lsr	x2, x2, x8
   180cc:	mov	x3, xzr
   180d0:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   180d4:	cmp	x0, #0x0
   180d8:	cset	w0, eq  // eq = none
   180dc:	ldp	x29, x30, [sp], #16
   180e0:	ret

00000000000180e4 <__gmpz_divisible_2exp_p@@Base>:
   180e4:	ldr	w8, [x0, #4]
   180e8:	cmp	w8, #0x0
   180ec:	cneg	w9, w8, mi  // mi = first
   180f0:	lsr	x8, x1, #6
   180f4:	cmp	x8, x9
   180f8:	b.cs	18138 <__gmpz_divisible_2exp_p@@Base+0x54>  // b.hs, b.nlast
   180fc:	ldr	x9, [x0, #8]
   18100:	cbz	x8, 18120 <__gmpz_divisible_2exp_p@@Base+0x3c>
   18104:	mov	x10, x9
   18108:	mov	x11, x8
   1810c:	ldr	x12, [x10]
   18110:	cbnz	x12, 18144 <__gmpz_divisible_2exp_p@@Base+0x60>
   18114:	subs	x11, x11, #0x1
   18118:	add	x10, x10, #0x8
   1811c:	b.ne	1810c <__gmpz_divisible_2exp_p@@Base+0x28>  // b.any
   18120:	ldr	x8, [x9, x8, lsl #3]
   18124:	mov	x9, #0xffffffffffffffff    	// #-1
   18128:	lsl	x9, x9, x1
   1812c:	bics	xzr, x8, x9
   18130:	cset	w0, eq  // eq = none
   18134:	ret
   18138:	cmp	w9, #0x0
   1813c:	cset	w0, eq  // eq = none
   18140:	ret
   18144:	mov	w0, wzr
   18148:	ret

000000000001814c <__gmpz_dump@@Base>:
   1814c:	stp	x29, x30, [sp, #-32]!
   18150:	mov	x2, x0
   18154:	mov	w1, #0xa                   	// #10
   18158:	mov	x0, xzr
   1815c:	str	x19, [sp, #16]
   18160:	mov	x29, sp
   18164:	bl	c3a0 <__gmpz_get_str@plt>
   18168:	mov	x19, x0
   1816c:	bl	c9b0 <puts@plt>
   18170:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18174:	ldr	x8, [x8, #4016]
   18178:	ldr	x0, [x8]
   1817c:	str	x0, [x29, #24]
   18180:	mov	x0, x19
   18184:	bl	bf60 <strlen@plt>
   18188:	add	x1, x0, #0x1
   1818c:	mov	x0, x19
   18190:	ldr	x2, [x29, #24]
   18194:	ldr	x19, [sp, #16]
   18198:	ldp	x29, x30, [sp], #32
   1819c:	br	x2

00000000000181a0 <__gmpz_export@@Base>:
   181a0:	sub	sp, sp, #0x70
   181a4:	stp	x29, x30, [sp, #16]
   181a8:	stp	x28, x27, [sp, #32]
   181ac:	stp	x26, x25, [sp, #48]
   181b0:	stp	x24, x23, [sp, #64]
   181b4:	stp	x22, x21, [sp, #80]
   181b8:	stp	x20, x19, [sp, #96]
   181bc:	ldrsw	x9, [x6, #4]
   181c0:	cmp	x1, #0x0
   181c4:	add	x8, sp, #0x8
   181c8:	mov	x19, x0
   181cc:	csel	x8, x8, x1, eq  // eq = none
   181d0:	add	x29, sp, #0x10
   181d4:	cbz	w9, 183c0 <__gmpz_export@@Base+0x220>
   181d8:	ldr	x21, [x6, #8]
   181dc:	cmp	x9, #0x0
   181e0:	cneg	x26, x9, mi  // mi = first
   181e4:	lsl	x10, x3, #3
   181e8:	add	x9, x21, x26, lsl #3
   181ec:	ldur	x9, [x9, #-8]
   181f0:	sub	x27, x10, x5
   181f4:	add	x10, x27, x26, lsl #6
   181f8:	mov	x24, x5
   181fc:	clz	x9, x9
   18200:	mvn	x9, x9
   18204:	add	x28, x9, x10
   18208:	mov	w25, w4
   1820c:	mov	x22, x3
   18210:	mov	w23, w2
   18214:	udiv	x20, x28, x27
   18218:	str	x20, [x8]
   1821c:	cbnz	x19, 18238 <__gmpz_export@@Base+0x98>
   18220:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18224:	ldr	x8, [x8, #3840]
   18228:	mul	x0, x20, x22
   1822c:	ldr	x8, [x8]
   18230:	blr	x8
   18234:	mov	x19, x0
   18238:	cmp	w25, #0x0
   1823c:	csinv	w18, w25, wzr, ne  // ne = any
   18240:	cbz	x24, 183e8 <__gmpz_export@@Base+0x248>
   18244:	cmp	w18, #0x0
   18248:	cneg	x12, x22, lt  // lt = tstop
   1824c:	cmp	w23, #0x0
   18250:	cneg	x13, x22, ge  // ge = tcont
   18254:	cmp	x27, x28
   18258:	b.hi	183c4 <__gmpz_export@@Base+0x224>  // b.pmore
   1825c:	and	w11, w27, #0x7
   18260:	mov	x15, #0xffffffffffffffff    	// #-1
   18264:	sub	x16, x20, #0x1
   18268:	mov	x8, xzr
   1826c:	cmp	w23, #0x0
   18270:	mov	w17, #0x40                  	// #64
   18274:	lsl	x1, x15, x11
   18278:	mul	x2, x16, x22
   1827c:	sub	x0, x22, #0x1
   18280:	sub	w15, w17, w11
   18284:	mvn	x17, x1
   18288:	csel	x1, x2, x8, ge  // ge = tcont
   1828c:	cmp	w18, #0x0
   18290:	lsr	x10, x27, #3
   18294:	sub	x14, x8, w18, sxtw
   18298:	add	x18, x19, x1
   1829c:	csel	x0, x0, x8, ge  // ge = tcont
   182a0:	mov	w9, wzr
   182a4:	add	x12, x12, x13
   182a8:	add	x13, x21, x26, lsl #3
   182ac:	add	x16, x10, #0x1
   182b0:	add	x18, x18, x0
   182b4:	mov	w0, #0x8                   	// #8
   182b8:	mov	x1, x8
   182bc:	b	182d0 <__gmpz_export@@Base+0x130>
   182c0:	add	x8, x8, #0x1
   182c4:	cmp	x8, x20
   182c8:	add	x18, x18, x12
   182cc:	b.cs	183c4 <__gmpz_export@@Base+0x224>  // b.hs, b.nlast
   182d0:	cbz	x10, 18334 <__gmpz_export@@Base+0x194>
   182d4:	mov	x2, x10
   182d8:	b	182f8 <__gmpz_export@@Base+0x158>
   182dc:	strb	w1, [x18]
   182e0:	lsr	x1, x1, #8
   182e4:	mov	w3, #0xfffffff8            	// #-8
   182e8:	add	w9, w9, w3
   182ec:	subs	x2, x2, #0x1
   182f0:	add	x18, x18, x14
   182f4:	b.eq	18334 <__gmpz_export@@Base+0x194>  // b.none
   182f8:	cmp	w9, #0x8
   182fc:	b.ge	182dc <__gmpz_export@@Base+0x13c>  // b.tcont
   18300:	cmp	x21, x13
   18304:	b.eq	18310 <__gmpz_export@@Base+0x170>  // b.none
   18308:	ldr	x3, [x21], #8
   1830c:	b	18318 <__gmpz_export@@Base+0x178>
   18310:	mov	x3, xzr
   18314:	mov	x21, x13
   18318:	lsl	x4, x3, x9
   1831c:	sub	w5, w0, w9
   18320:	orr	w4, w4, w1
   18324:	lsr	x1, x3, x5
   18328:	mov	w3, #0x38                  	// #56
   1832c:	strb	w4, [x18]
   18330:	b	182e8 <__gmpz_export@@Base+0x148>
   18334:	cbz	w11, 18350 <__gmpz_export@@Base+0x1b0>
   18338:	cmp	w9, w11
   1833c:	b.ge	18360 <__gmpz_export@@Base+0x1c0>  // b.tcont
   18340:	cmp	x21, x13
   18344:	b.eq	18374 <__gmpz_export@@Base+0x1d4>  // b.none
   18348:	ldr	x2, [x21], #8
   1834c:	b	1837c <__gmpz_export@@Base+0x1dc>
   18350:	mov	x2, x10
   18354:	cmp	x2, x22
   18358:	b.cs	182c0 <__gmpz_export@@Base+0x120>  // b.hs, b.nlast
   1835c:	b	183a8 <__gmpz_export@@Base+0x208>
   18360:	and	w2, w1, w17
   18364:	lsr	x1, x1, x11
   18368:	strb	w2, [x18]
   1836c:	sub	w9, w9, w11
   18370:	b	18398 <__gmpz_export@@Base+0x1f8>
   18374:	mov	x2, xzr
   18378:	mov	x21, x13
   1837c:	lsl	x3, x2, x9
   18380:	sub	w4, w11, w9
   18384:	orr	w3, w3, w1
   18388:	lsr	x1, x2, x4
   1838c:	and	w2, w3, w17
   18390:	add	w9, w15, w9
   18394:	strb	w2, [x18]
   18398:	add	x18, x18, x14
   1839c:	mov	x2, x16
   183a0:	cmp	x2, x22
   183a4:	b.cs	182c0 <__gmpz_export@@Base+0x120>  // b.hs, b.nlast
   183a8:	sub	x2, x22, x2
   183ac:	strb	wzr, [x18]
   183b0:	subs	x2, x2, #0x1
   183b4:	add	x18, x18, x14
   183b8:	b.ne	183ac <__gmpz_export@@Base+0x20c>  // b.any
   183bc:	b	182c0 <__gmpz_export@@Base+0x120>
   183c0:	str	xzr, [x8]
   183c4:	mov	x0, x19
   183c8:	ldp	x20, x19, [sp, #96]
   183cc:	ldp	x22, x21, [sp, #80]
   183d0:	ldp	x24, x23, [sp, #64]
   183d4:	ldp	x26, x25, [sp, #48]
   183d8:	ldp	x28, x27, [sp, #32]
   183dc:	ldp	x29, x30, [sp, #16]
   183e0:	add	sp, sp, #0x70
   183e4:	ret
   183e8:	cmp	x22, #0x8
   183ec:	b.ne	18244 <__gmpz_export@@Base+0xa4>  // b.any
   183f0:	and	x8, x19, #0x7
   183f4:	cbnz	x8, 18244 <__gmpz_export@@Base+0xa4>
   183f8:	and	w8, w18, w23
   183fc:	cmn	w8, #0x1
   18400:	b.eq	18460 <__gmpz_export@@Base+0x2c0>  // b.none
   18404:	cmp	w23, #0x1
   18408:	b.ne	18474 <__gmpz_export@@Base+0x2d4>  // b.any
   1840c:	cmn	w18, #0x1
   18410:	b.ne	18474 <__gmpz_export@@Base+0x2d4>  // b.any
   18414:	cmp	x20, #0x1
   18418:	b.lt	183c4 <__gmpz_export@@Base+0x224>  // b.tstop
   1841c:	cmp	x20, #0x4
   18420:	add	x10, x21, x20, lsl #3
   18424:	b.cc	1843c <__gmpz_export@@Base+0x29c>  // b.lo, b.ul, b.last
   18428:	cmp	x19, x10
   1842c:	b.cs	185b0 <__gmpz_export@@Base+0x410>  // b.hs, b.nlast
   18430:	add	x8, x19, x20, lsl #3
   18434:	cmp	x8, x21
   18438:	b.ls	185b0 <__gmpz_export@@Base+0x410>  // b.plast
   1843c:	mov	x8, xzr
   18440:	mov	x9, x19
   18444:	sub	x10, x10, #0x8
   18448:	ldr	x11, [x10], #-8
   1844c:	add	x8, x8, #0x1
   18450:	cmp	x8, x20
   18454:	str	x11, [x9], #8
   18458:	b.lt	18448 <__gmpz_export@@Base+0x2a8>  // b.tstop
   1845c:	b	183c4 <__gmpz_export@@Base+0x224>
   18460:	mov	x0, x19
   18464:	mov	x1, x21
   18468:	mov	x2, x20
   1846c:	bl	ca50 <__gmpn_copyi@plt>
   18470:	b	183c4 <__gmpz_export@@Base+0x224>
   18474:	cmn	w23, #0x1
   18478:	b.ne	18514 <__gmpz_export@@Base+0x374>  // b.any
   1847c:	cmp	w18, #0x1
   18480:	b.ne	18514 <__gmpz_export@@Base+0x374>  // b.any
   18484:	cmp	x20, #0x1
   18488:	b.lt	183c4 <__gmpz_export@@Base+0x224>  // b.tstop
   1848c:	cmp	x20, #0x1
   18490:	b.eq	184b0 <__gmpz_export@@Base+0x310>  // b.none
   18494:	lsl	x8, x20, #3
   18498:	add	x9, x21, x8
   1849c:	cmp	x19, x9
   184a0:	b.cs	18600 <__gmpz_export@@Base+0x460>  // b.hs, b.nlast
   184a4:	add	x8, x19, x8
   184a8:	cmp	x8, x21
   184ac:	b.ls	18600 <__gmpz_export@@Base+0x460>  // b.plast
   184b0:	mov	x8, xzr
   184b4:	mov	x9, x21
   184b8:	mov	x10, x19
   184bc:	ldr	x11, [x9], #8
   184c0:	add	x8, x8, #0x1
   184c4:	cmp	x8, x20
   184c8:	lsl	x14, x11, #40
   184cc:	and	x14, x14, #0xff000000000000
   184d0:	lsr	x13, x11, #16
   184d4:	bfi	x14, x11, #56, #8
   184d8:	lsr	x12, x11, #24
   184dc:	bfi	x14, x13, #40, #8
   184e0:	lsr	x13, x11, #8
   184e4:	and	x13, x13, #0xff000000
   184e8:	bfi	x14, x12, #32, #8
   184ec:	orr	x13, x14, x13
   184f0:	lsr	x14, x11, #40
   184f4:	and	x12, x12, #0xff0000
   184f8:	and	x14, x14, #0xff00
   184fc:	orr	x12, x13, x12
   18500:	orr	x12, x12, x14
   18504:	add	x11, x12, x11, lsr #56
   18508:	str	x11, [x10], #8
   1850c:	b.lt	184bc <__gmpz_export@@Base+0x31c>  // b.tstop
   18510:	b	183c4 <__gmpz_export@@Base+0x224>
   18514:	cmp	w23, #0x1
   18518:	b.ne	18244 <__gmpz_export@@Base+0xa4>  // b.any
   1851c:	cmp	w18, #0x1
   18520:	b.ne	18244 <__gmpz_export@@Base+0xa4>  // b.any
   18524:	cmp	x20, #0x1
   18528:	b.lt	183c4 <__gmpz_export@@Base+0x224>  // b.tstop
   1852c:	cmp	x20, #0x1
   18530:	add	x11, x21, x20, lsl #3
   18534:	b.eq	1854c <__gmpz_export@@Base+0x3ac>  // b.none
   18538:	cmp	x19, x11
   1853c:	b.cs	186a4 <__gmpz_export@@Base+0x504>  // b.hs, b.nlast
   18540:	add	x8, x19, x20, lsl #3
   18544:	cmp	x8, x21
   18548:	b.ls	186a4 <__gmpz_export@@Base+0x504>  // b.plast
   1854c:	mov	x8, xzr
   18550:	mov	x9, x19
   18554:	sub	x10, x11, #0x8
   18558:	ldr	x11, [x10], #-8
   1855c:	add	x8, x8, #0x1
   18560:	cmp	x8, x20
   18564:	lsl	x14, x11, #40
   18568:	and	x14, x14, #0xff000000000000
   1856c:	lsr	x13, x11, #16
   18570:	bfi	x14, x11, #56, #8
   18574:	lsr	x12, x11, #24
   18578:	bfi	x14, x13, #40, #8
   1857c:	lsr	x13, x11, #8
   18580:	and	x13, x13, #0xff000000
   18584:	bfi	x14, x12, #32, #8
   18588:	orr	x13, x14, x13
   1858c:	lsr	x14, x11, #40
   18590:	and	x12, x12, #0xff0000
   18594:	and	x14, x14, #0xff00
   18598:	orr	x12, x13, x12
   1859c:	orr	x12, x12, x14
   185a0:	add	x11, x12, x11, lsr #56
   185a4:	str	x11, [x9], #8
   185a8:	b.lt	18558 <__gmpz_export@@Base+0x3b8>  // b.tstop
   185ac:	b	183c4 <__gmpz_export@@Base+0x224>
   185b0:	and	x8, x20, #0xfffffffffffffffc
   185b4:	lsl	x9, x8, #3
   185b8:	mov	x11, xzr
   185bc:	sub	x12, x10, #0x8
   185c0:	sub	x10, x10, x9
   185c4:	add	x9, x19, x9
   185c8:	add	x13, x19, #0x10
   185cc:	sub	x14, x12, x11, lsl #3
   185d0:	ldur	q0, [x14, #-8]
   185d4:	ldur	q1, [x14, #-24]
   185d8:	add	x11, x11, #0x4
   185dc:	cmp	x11, x8
   185e0:	ext	v0.16b, v0.16b, v0.16b, #8
   185e4:	ext	v1.16b, v1.16b, v1.16b, #8
   185e8:	stp	q0, q1, [x13, #-16]
   185ec:	add	x13, x13, #0x20
   185f0:	b.ne	185cc <__gmpz_export@@Base+0x42c>  // b.any
   185f4:	cmp	x20, x8
   185f8:	b.eq	183c4 <__gmpz_export@@Base+0x224>  // b.none
   185fc:	b	18444 <__gmpz_export@@Base+0x2a4>
   18600:	and	x8, x20, #0xfffffffffffffffe
   18604:	lsl	x10, x8, #3
   18608:	mov	x11, xzr
   1860c:	mov	x12, xzr
   18610:	movi	v0.2d, #0xff000000000000
   18614:	movi	v1.2d, #0xff0000000000
   18618:	movi	v2.2d, #0xff00000000
   1861c:	movi	v3.2d, #0xff000000
   18620:	movi	v4.2d, #0xff0000
   18624:	add	x9, x21, x10
   18628:	add	x10, x19, x10
   1862c:	movi	v5.2d, #0xff00
   18630:	ldr	q6, [x21, x11]
   18634:	add	x12, x12, #0x2
   18638:	cmp	x12, x8
   1863c:	shl	v16.2d, v6.2d, #40
   18640:	shl	v7.2d, v6.2d, #56
   18644:	and	v16.16b, v16.16b, v0.16b
   18648:	orr	v7.16b, v16.16b, v7.16b
   1864c:	shl	v16.2d, v6.2d, #24
   18650:	and	v16.16b, v16.16b, v1.16b
   18654:	orr	v7.16b, v7.16b, v16.16b
   18658:	shl	v16.2d, v6.2d, #8
   1865c:	and	v16.16b, v16.16b, v2.16b
   18660:	orr	v7.16b, v7.16b, v16.16b
   18664:	ushr	v16.2d, v6.2d, #8
   18668:	and	v16.16b, v16.16b, v3.16b
   1866c:	orr	v7.16b, v7.16b, v16.16b
   18670:	ushr	v16.2d, v6.2d, #24
   18674:	and	v16.16b, v16.16b, v4.16b
   18678:	orr	v7.16b, v7.16b, v16.16b
   1867c:	ushr	v16.2d, v6.2d, #40
   18680:	and	v16.16b, v16.16b, v5.16b
   18684:	orr	v7.16b, v7.16b, v16.16b
   18688:	usra	v7.2d, v6.2d, #56
   1868c:	str	q7, [x19, x11]
   18690:	add	x11, x11, #0x10
   18694:	b.ne	18630 <__gmpz_export@@Base+0x490>  // b.any
   18698:	cmp	x20, x8
   1869c:	b.ne	184bc <__gmpz_export@@Base+0x31c>  // b.any
   186a0:	b	183c4 <__gmpz_export@@Base+0x224>
   186a4:	and	x8, x20, #0xfffffffffffffffe
   186a8:	lsl	x9, x8, #3
   186ac:	mov	x10, xzr
   186b0:	sub	x12, x11, #0x10
   186b4:	movi	v0.2d, #0xff000000000000
   186b8:	movi	v1.2d, #0xff0000000000
   186bc:	movi	v2.2d, #0xff00000000
   186c0:	movi	v3.2d, #0xff000000
   186c4:	movi	v4.2d, #0xff0000
   186c8:	sub	x11, x11, x9
   186cc:	add	x9, x19, x9
   186d0:	movi	v5.2d, #0xff00
   186d4:	mov	x13, x19
   186d8:	sub	x14, x12, x10, lsl #3
   186dc:	ldr	q6, [x14]
   186e0:	add	x10, x10, #0x2
   186e4:	cmp	x10, x8
   186e8:	ext	v6.16b, v6.16b, v6.16b, #8
   186ec:	shl	v16.2d, v6.2d, #40
   186f0:	shl	v7.2d, v6.2d, #56
   186f4:	and	v16.16b, v16.16b, v0.16b
   186f8:	orr	v7.16b, v16.16b, v7.16b
   186fc:	shl	v16.2d, v6.2d, #24
   18700:	and	v16.16b, v16.16b, v1.16b
   18704:	orr	v7.16b, v7.16b, v16.16b
   18708:	shl	v16.2d, v6.2d, #8
   1870c:	and	v16.16b, v16.16b, v2.16b
   18710:	orr	v7.16b, v7.16b, v16.16b
   18714:	ushr	v16.2d, v6.2d, #8
   18718:	and	v16.16b, v16.16b, v3.16b
   1871c:	orr	v7.16b, v7.16b, v16.16b
   18720:	ushr	v16.2d, v6.2d, #24
   18724:	and	v16.16b, v16.16b, v4.16b
   18728:	orr	v7.16b, v7.16b, v16.16b
   1872c:	ushr	v16.2d, v6.2d, #40
   18730:	and	v16.16b, v16.16b, v5.16b
   18734:	orr	v7.16b, v7.16b, v16.16b
   18738:	usra	v7.2d, v6.2d, #56
   1873c:	str	q7, [x13], #16
   18740:	b.ne	186d8 <__gmpz_export@@Base+0x538>  // b.any
   18744:	cmp	x20, x8
   18748:	b.eq	183c4 <__gmpz_export@@Base+0x224>  // b.none
   1874c:	b	18554 <__gmpz_export@@Base+0x3b4>

0000000000018750 <__gmpz_mfac_uiui@@Base>:
   18750:	stp	x29, x30, [sp, #-64]!
   18754:	str	x23, [sp, #16]
   18758:	stp	x22, x21, [sp, #32]
   1875c:	stp	x20, x19, [sp, #48]
   18760:	mov	x29, sp
   18764:	sub	sp, sp, #0x20
   18768:	mov	x20, x1
   1876c:	subs	x8, x1, #0x3
   18770:	mov	x19, x0
   18774:	b.cc	187e4 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   18778:	sub	x9, x2, #0x1
   1877c:	mov	x22, x2
   18780:	cmp	x8, x9
   18784:	b.cc	187e4 <__gmpz_mfac_uiui@@Base+0x94>  // b.lo, b.ul, b.last
   18788:	add	x0, x29, #0x18
   1878c:	mov	w1, #0x1                   	// #1
   18790:	mov	x2, x22
   18794:	str	x20, [x29, #24]
   18798:	bl	bf90 <__gmpn_gcd_1@plt>
   1879c:	mov	x21, x0
   187a0:	cmp	x0, #0x2
   187a4:	b.cc	187b0 <__gmpz_mfac_uiui@@Base+0x60>  // b.lo, b.ul, b.last
   187a8:	udiv	x20, x20, x21
   187ac:	udiv	x22, x22, x21
   187b0:	cmp	x22, #0x2
   187b4:	b.hi	1880c <__gmpz_mfac_uiui@@Base+0xbc>  // b.pmore
   187b8:	cmp	x22, #0x1
   187bc:	b.ne	1886c <__gmpz_mfac_uiui@@Base+0x11c>  // b.any
   187c0:	cmp	x21, #0x3
   187c4:	b.cc	18954 <__gmpz_mfac_uiui@@Base+0x204>  // b.lo, b.ul, b.last
   187c8:	sub	x0, x29, #0x10
   187cc:	bl	d250 <__gmpz_init@plt>
   187d0:	sub	x0, x29, #0x10
   187d4:	mov	x1, x20
   187d8:	bl	c450 <__gmpz_fac_ui@plt>
   187dc:	str	x20, [x29, #24]
   187e0:	b	18a20 <__gmpz_mfac_uiui@@Base+0x2d0>
   187e4:	ldr	w8, [x19]
   187e8:	cmp	x20, #0x0
   187ec:	cinc	x20, x20, eq  // eq = none
   187f0:	cmp	w8, #0x0
   187f4:	b.le	1898c <__gmpz_mfac_uiui@@Base+0x23c>
   187f8:	ldr	x0, [x19, #8]
   187fc:	mov	w8, #0x1                   	// #1
   18800:	str	x20, [x0]
   18804:	str	w8, [x19, #4]
   18808:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>
   1880c:	udiv	x8, x20, x22
   18810:	cmp	x21, #0x2
   18814:	add	x9, x8, #0x1
   18818:	sub	x8, x20, x22
   1881c:	str	x9, [x29, #24]
   18820:	b.cc	18898 <__gmpz_mfac_uiui@@Base+0x148>  // b.lo, b.ul, b.last
   18824:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18828:	ldr	x10, [x10, #3880]
   1882c:	mov	w11, #0x9                   	// #9
   18830:	sub	w12, w11, #0x2
   18834:	ldr	x12, [x10, w12, uxtw #3]
   18838:	sub	w11, w11, #0x1
   1883c:	cmp	x12, x8
   18840:	b.cc	18830 <__gmpz_mfac_uiui@@Base+0xe0>  // b.lo, b.ul, b.last
   18844:	ldrsw	x12, [x19]
   18848:	mov	w11, w11
   1884c:	udiv	x11, x9, x11
   18850:	add	x11, x11, #0x2
   18854:	cmp	x11, x12
   18858:	b.hi	1899c <__gmpz_mfac_uiui@@Base+0x24c>  // b.pmore
   1885c:	ldr	x23, [x19, #8]
   18860:	cmp	x8, x22
   18864:	b.hi	1891c <__gmpz_mfac_uiui@@Base+0x1cc>  // b.pmore
   18868:	b	189ec <__gmpz_mfac_uiui@@Base+0x29c>
   1886c:	cmp	x21, #0x2
   18870:	b.cc	1896c <__gmpz_mfac_uiui@@Base+0x21c>  // b.lo, b.ul, b.last
   18874:	sub	x0, x29, #0x10
   18878:	bl	d250 <__gmpz_init@plt>
   1887c:	sub	x0, x29, #0x10
   18880:	mov	x1, x20
   18884:	bl	c600 <__gmpz_2fac_ui@plt>
   18888:	lsr	x8, x20, #1
   1888c:	add	x8, x8, #0x1
   18890:	str	x8, [x29, #24]
   18894:	b	18a20 <__gmpz_mfac_uiui@@Base+0x2d0>
   18898:	stur	xzr, [x29, #-32]
   1889c:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   188a0:	ldr	x10, [x10, #3880]
   188a4:	mov	w11, #0x9                   	// #9
   188a8:	sub	w12, w11, #0x2
   188ac:	ldr	x12, [x10, w12, uxtw #3]
   188b0:	sub	w11, w11, #0x1
   188b4:	cmp	x12, x8
   188b8:	b.cc	188a8 <__gmpz_mfac_uiui@@Base+0x158>  // b.lo, b.ul, b.last
   188bc:	mov	w11, w11
   188c0:	udiv	x11, x9, x11
   188c4:	lsl	x11, x11, #3
   188c8:	add	x11, x11, #0x10
   188cc:	mov	w12, #0x9                   	// #9
   188d0:	sub	w13, w12, #0x2
   188d4:	ldr	x13, [x10, w13, uxtw #3]
   188d8:	sub	w12, w12, #0x1
   188dc:	cmp	x13, x8
   188e0:	b.cc	188d0 <__gmpz_mfac_uiui@@Base+0x180>  // b.lo, b.ul, b.last
   188e4:	mov	w10, w12
   188e8:	udiv	x9, x9, x10
   188ec:	mov	w12, #0x7f00                	// #32512
   188f0:	lsl	x9, x9, #3
   188f4:	cmp	x11, x12
   188f8:	add	x1, x9, #0x10
   188fc:	b.hi	189d0 <__gmpz_mfac_uiui@@Base+0x280>  // b.pmore
   18900:	add	x10, x1, #0xf
   18904:	mov	x9, sp
   18908:	and	x10, x10, #0xfffffffffffffff0
   1890c:	sub	x23, x9, x10
   18910:	mov	sp, x23
   18914:	cmp	x8, x22
   18918:	b.ls	189ec <__gmpz_mfac_uiui@@Base+0x29c>  // b.plast
   1891c:	mov	x10, xzr
   18920:	mov	x9, x8
   18924:	b	18938 <__gmpz_mfac_uiui@@Base+0x1e8>
   18928:	mul	x20, x9, x20
   1892c:	sub	x9, x9, x22
   18930:	cmp	x9, x22
   18934:	b.ls	189f4 <__gmpz_mfac_uiui@@Base+0x2a4>  // b.plast
   18938:	umulh	x11, x8, x20
   1893c:	cbz	x11, 18928 <__gmpz_mfac_uiui@@Base+0x1d8>
   18940:	add	x11, x10, #0x1
   18944:	str	x20, [x23, x10, lsl #3]
   18948:	mov	x20, x9
   1894c:	mov	x10, x11
   18950:	b	1892c <__gmpz_mfac_uiui@@Base+0x1dc>
   18954:	cmp	x21, #0x2
   18958:	b.ne	1897c <__gmpz_mfac_uiui@@Base+0x22c>  // b.any
   1895c:	lsl	x1, x20, #1
   18960:	mov	x0, x19
   18964:	bl	c600 <__gmpz_2fac_ui@plt>
   18968:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>
   1896c:	mov	x0, x19
   18970:	mov	x1, x20
   18974:	bl	c600 <__gmpz_2fac_ui@plt>
   18978:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>
   1897c:	mov	x0, x19
   18980:	mov	x1, x20
   18984:	bl	c450 <__gmpz_fac_ui@plt>
   18988:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>
   1898c:	mov	w1, #0x1                   	// #1
   18990:	mov	x0, x19
   18994:	bl	c080 <__gmpz_realloc@plt>
   18998:	b	187fc <__gmpz_mfac_uiui@@Base+0xac>
   1899c:	mov	w11, #0x9                   	// #9
   189a0:	sub	w12, w11, #0x2
   189a4:	ldr	x12, [x10, w12, uxtw #3]
   189a8:	sub	w11, w11, #0x1
   189ac:	cmp	x12, x8
   189b0:	b.cc	189a0 <__gmpz_mfac_uiui@@Base+0x250>  // b.lo, b.ul, b.last
   189b4:	mov	w10, w11
   189b8:	udiv	x9, x9, x10
   189bc:	add	x1, x9, #0x2
   189c0:	mov	x0, x19
   189c4:	mov	x23, x8
   189c8:	bl	c080 <__gmpz_realloc@plt>
   189cc:	b	189dc <__gmpz_mfac_uiui@@Base+0x28c>
   189d0:	sub	x0, x29, #0x20
   189d4:	mov	x23, x8
   189d8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   189dc:	mov	x8, x23
   189e0:	mov	x23, x0
   189e4:	cmp	x8, x22
   189e8:	b.hi	1891c <__gmpz_mfac_uiui@@Base+0x1cc>  // b.pmore
   189ec:	mov	x10, xzr
   189f0:	mov	x9, x8
   189f4:	add	x8, x23, x10, lsl #3
   189f8:	add	x22, x10, #0x2
   189fc:	cmp	x21, #0x2
   18a00:	stp	x9, x20, [x8]
   18a04:	b.cc	18a5c <__gmpz_mfac_uiui@@Base+0x30c>  // b.lo, b.ul, b.last
   18a08:	sub	x0, x29, #0x10
   18a0c:	bl	d250 <__gmpz_init@plt>
   18a10:	sub	x0, x29, #0x10
   18a14:	mov	x1, x23
   18a18:	mov	x2, x22
   18a1c:	bl	cd70 <__gmpz_prodlimbs@plt>
   18a20:	sub	x0, x29, #0x20
   18a24:	bl	d250 <__gmpz_init@plt>
   18a28:	ldr	x2, [x29, #24]
   18a2c:	sub	x0, x29, #0x20
   18a30:	mov	x1, x21
   18a34:	bl	ca10 <__gmpz_ui_pow_ui@plt>
   18a38:	sub	x1, x29, #0x20
   18a3c:	sub	x2, x29, #0x10
   18a40:	mov	x0, x19
   18a44:	bl	c4b0 <__gmpz_mul@plt>
   18a48:	sub	x0, x29, #0x20
   18a4c:	bl	cb50 <__gmpz_clear@plt>
   18a50:	sub	x0, x29, #0x10
   18a54:	bl	cb50 <__gmpz_clear@plt>
   18a58:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>
   18a5c:	mov	x0, x19
   18a60:	mov	x1, x23
   18a64:	mov	x2, x22
   18a68:	bl	cd70 <__gmpz_prodlimbs@plt>
   18a6c:	ldur	x0, [x29, #-32]
   18a70:	cbnz	x0, 18a8c <__gmpz_mfac_uiui@@Base+0x33c>
   18a74:	mov	sp, x29
   18a78:	ldp	x20, x19, [sp, #48]
   18a7c:	ldp	x22, x21, [sp, #32]
   18a80:	ldr	x23, [sp, #16]
   18a84:	ldp	x29, x30, [sp], #64
   18a88:	ret
   18a8c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   18a90:	b	18a74 <__gmpz_mfac_uiui@@Base+0x324>

0000000000018a94 <__gmpz_2fac_ui@@Base>:
   18a94:	stp	x29, x30, [sp, #-32]!
   18a98:	stp	x20, x19, [sp, #16]
   18a9c:	mov	x19, x0
   18aa0:	mov	x29, sp
   18aa4:	tbnz	w1, #0, 18acc <__gmpz_2fac_ui@@Base+0x38>
   18aa8:	sub	x8, x1, #0x1
   18aac:	cmp	x8, #0x50
   18ab0:	lsr	x8, x1, #1
   18ab4:	b.hi	18b08 <__gmpz_2fac_ui@@Base+0x74>  // b.pmore
   18ab8:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18abc:	ldr	x9, [x9, #3992]
   18ac0:	add	x9, x8, x9
   18ac4:	ldurb	w20, [x9, #-1]
   18ac8:	b	18b3c <__gmpz_2fac_ui@@Base+0xa8>
   18acc:	cmp	x1, #0x21
   18ad0:	b.hi	18b68 <__gmpz_2fac_ui@@Base+0xd4>  // b.pmore
   18ad4:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18ad8:	ldr	w9, [x19]
   18adc:	ldr	x10, [x10, #3928]
   18ae0:	lsl	x8, x1, #2
   18ae4:	and	x8, x8, #0xfffffffffffffff8
   18ae8:	cmp	w9, #0x0
   18aec:	ldr	x20, [x10, x8]
   18af0:	b.le	18c44 <__gmpz_2fac_ui@@Base+0x1b0>
   18af4:	ldr	x0, [x19, #8]
   18af8:	mov	w8, #0x1                   	// #1
   18afc:	str	x20, [x0]
   18b00:	str	w8, [x19, #4]
   18b04:	b	18c34 <__gmpz_2fac_ui@@Base+0x1a0>
   18b08:	and	x9, x8, #0x5555555555555555
   18b0c:	sub	x9, x1, x9
   18b10:	lsr	x10, x9, #2
   18b14:	and	x10, x10, #0x3333333333333333
   18b18:	and	x9, x9, #0x3333333333333333
   18b1c:	add	x9, x10, x9
   18b20:	add	x9, x9, x9, lsr #4
   18b24:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   18b28:	add	x9, x9, x9, lsr #8
   18b2c:	add	x9, x9, x9, lsr #16
   18b30:	lsr	x10, x9, #32
   18b34:	add	w9, w10, w9
   18b38:	sub	x20, x1, w9, uxtb
   18b3c:	mov	x0, x19
   18b40:	mov	x1, x8
   18b44:	mov	w2, wzr
   18b48:	bl	c3d0 <__gmpz_oddfac_1@plt>
   18b4c:	mov	x0, x19
   18b50:	mov	x1, x19
   18b54:	mov	x2, x20
   18b58:	mov	sp, x29
   18b5c:	ldp	x20, x19, [sp, #16]
   18b60:	ldp	x29, x30, [sp], #32
   18b64:	b	c6e0 <__gmpz_mul_2exp@plt>
   18b68:	cmp	x1, #0x1d7
   18b6c:	b.ls	18b88 <__gmpz_2fac_ui@@Base+0xf4>  // b.plast
   18b70:	mov	w2, #0x1                   	// #1
   18b74:	mov	x0, x19
   18b78:	mov	sp, x29
   18b7c:	ldp	x20, x19, [sp, #16]
   18b80:	ldp	x29, x30, [sp], #32
   18b84:	b	c3d0 <__gmpz_oddfac_1@plt>
   18b88:	mov	w9, #0xaaab                	// #43691
   18b8c:	and	w8, w1, #0xffff
   18b90:	movk	w9, #0xaaaa, lsl #16
   18b94:	umull	x8, w8, w9
   18b98:	lsr	x8, x8, #32
   18b9c:	and	w8, w8, #0xfff8
   18ba0:	add	w8, w8, #0x8
   18ba4:	and	w8, w8, #0xfff8
   18ba8:	add	w8, w8, #0xf
   18bac:	and	x8, x8, #0x1fff0
   18bb0:	mov	x9, sp
   18bb4:	sub	x8, x9, x8
   18bb8:	mov	sp, x8
   18bbc:	mov	x9, #0xd941                	// #55617
   18bc0:	movk	x9, #0xc030, lsl #16
   18bc4:	movk	x9, #0x2099, lsl #32
   18bc8:	movk	x9, #0x57e2, lsl #48
   18bcc:	sub	x10, x1, #0x2
   18bd0:	cmp	x10, #0x22
   18bd4:	str	x9, [x8]
   18bd8:	mov	w9, #0x1                   	// #1
   18bdc:	b.cc	18c20 <__gmpz_2fac_ui@@Base+0x18c>  // b.lo, b.ul, b.last
   18be0:	mov	x11, #0x3869                	// #14441
   18be4:	movk	x11, #0xfba9, lsl #16
   18be8:	movk	x11, #0xd8f2, lsl #32
   18bec:	movk	x11, #0x8a, lsl #48
   18bf0:	b	18c04 <__gmpz_2fac_ui@@Base+0x170>
   18bf4:	mul	x1, x10, x1
   18bf8:	sub	x10, x10, #0x2
   18bfc:	cmp	x10, #0x21
   18c00:	b.ls	18c20 <__gmpz_2fac_ui@@Base+0x18c>  // b.plast
   18c04:	cmp	x1, x11
   18c08:	b.cc	18bf4 <__gmpz_2fac_ui@@Base+0x160>  // b.lo, b.ul, b.last
   18c0c:	add	x12, x9, #0x1
   18c10:	str	x1, [x8, x9, lsl #3]
   18c14:	mov	x1, x10
   18c18:	mov	x9, x12
   18c1c:	b	18bf8 <__gmpz_2fac_ui@@Base+0x164>
   18c20:	add	x2, x9, #0x1
   18c24:	str	x1, [x8, x9, lsl #3]
   18c28:	mov	x0, x19
   18c2c:	mov	x1, x8
   18c30:	bl	cd70 <__gmpz_prodlimbs@plt>
   18c34:	mov	sp, x29
   18c38:	ldp	x20, x19, [sp, #16]
   18c3c:	ldp	x29, x30, [sp], #32
   18c40:	ret
   18c44:	mov	w1, #0x1                   	// #1
   18c48:	mov	x0, x19
   18c4c:	bl	c080 <__gmpz_realloc@plt>
   18c50:	b	18af8 <__gmpz_2fac_ui@@Base+0x64>

0000000000018c54 <__gmpz_fac_ui@@Base>:
   18c54:	stp	x29, x30, [sp, #-32]!
   18c58:	stp	x20, x19, [sp, #16]
   18c5c:	mov	x20, x1
   18c60:	cmp	x1, #0x14
   18c64:	mov	x19, x0
   18c68:	mov	x29, sp
   18c6c:	b.hi	18c9c <__gmpz_fac_ui@@Base+0x48>  // b.pmore
   18c70:	adrp	x9, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   18c74:	ldr	w8, [x19]
   18c78:	add	x9, x9, #0x488
   18c7c:	ldr	x20, [x9, x20, lsl #3]
   18c80:	cmp	w8, #0x0
   18c84:	b.le	18de0 <__gmpz_fac_ui@@Base+0x18c>
   18c88:	ldr	x0, [x19, #8]
   18c8c:	mov	w8, #0x1                   	// #1
   18c90:	str	x20, [x0]
   18c94:	str	w8, [x19, #4]
   18c98:	b	18dd0 <__gmpz_fac_ui@@Base+0x17c>
   18c9c:	cmp	x20, #0x17
   18ca0:	b.ls	18cd4 <__gmpz_fac_ui@@Base+0x80>  // b.plast
   18ca4:	mov	x0, x19
   18ca8:	mov	x1, x20
   18cac:	mov	w2, wzr
   18cb0:	bl	c3d0 <__gmpz_oddfac_1@plt>
   18cb4:	cmp	x20, #0x51
   18cb8:	lsr	x8, x20, #1
   18cbc:	b.hi	18d6c <__gmpz_fac_ui@@Base+0x118>  // b.pmore
   18cc0:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18cc4:	ldr	x9, [x9, #3992]
   18cc8:	add	x8, x8, x9
   18ccc:	ldurb	w2, [x8, #-1]
   18cd0:	b	18da0 <__gmpz_fac_ui@@Base+0x14c>
   18cd4:	sub	w8, w20, #0x15
   18cd8:	mov	w9, #0xcccd                	// #52429
   18cdc:	and	w8, w8, #0xff
   18ce0:	movk	w9, #0xcccc, lsl #16
   18ce4:	umull	x8, w8, w9
   18ce8:	lsr	x8, x8, #32
   18cec:	and	w8, w8, #0xf8
   18cf0:	add	w8, w8, #0x10
   18cf4:	and	w8, w8, #0xf8
   18cf8:	add	w8, w8, #0xf
   18cfc:	and	x8, x8, #0x1f0
   18d00:	mov	x9, sp
   18d04:	sub	x1, x9, x8
   18d08:	mov	sp, x1
   18d0c:	mov	x8, #0x82b40000            	// #2192834560
   18d10:	movk	x8, #0x677c, lsl #32
   18d14:	sub	x9, x20, #0x1
   18d18:	movk	x8, #0x21c3, lsl #48
   18d1c:	cmp	x9, #0x15
   18d20:	str	x8, [x1]
   18d24:	b.cc	18db8 <__gmpz_fac_ui@@Base+0x164>  // b.lo, b.ul, b.last
   18d28:	mov	x10, #0x3d71                	// #15729
   18d2c:	movk	x10, #0xd70a, lsl #16
   18d30:	movk	x10, #0x70a3, lsl #32
   18d34:	mov	w8, #0x1                   	// #1
   18d38:	movk	x10, #0xa3d, lsl #48
   18d3c:	b	18d50 <__gmpz_fac_ui@@Base+0xfc>
   18d40:	mul	x20, x9, x20
   18d44:	sub	x9, x9, #0x1
   18d48:	cmp	x9, #0x14
   18d4c:	b.ls	18dc0 <__gmpz_fac_ui@@Base+0x16c>  // b.plast
   18d50:	cmp	x20, x10
   18d54:	b.cc	18d40 <__gmpz_fac_ui@@Base+0xec>  // b.lo, b.ul, b.last
   18d58:	add	x11, x8, #0x1
   18d5c:	str	x20, [x1, x8, lsl #3]
   18d60:	mov	x8, x11
   18d64:	mov	x20, x9
   18d68:	b	18d44 <__gmpz_fac_ui@@Base+0xf0>
   18d6c:	and	x8, x8, #0x5555555555555555
   18d70:	sub	x8, x20, x8
   18d74:	lsr	x9, x8, #2
   18d78:	and	x9, x9, #0x3333333333333333
   18d7c:	and	x8, x8, #0x3333333333333333
   18d80:	add	x8, x9, x8
   18d84:	add	x8, x8, x8, lsr #4
   18d88:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   18d8c:	add	x8, x8, x8, lsr #8
   18d90:	add	x8, x8, x8, lsr #16
   18d94:	lsr	x9, x8, #32
   18d98:	add	w8, w9, w8
   18d9c:	sub	x2, x20, w8, uxtb
   18da0:	mov	x0, x19
   18da4:	mov	x1, x19
   18da8:	mov	sp, x29
   18dac:	ldp	x20, x19, [sp, #16]
   18db0:	ldp	x29, x30, [sp], #32
   18db4:	b	c6e0 <__gmpz_mul_2exp@plt>
   18db8:	mov	w20, #0x15                  	// #21
   18dbc:	mov	w8, #0x1                   	// #1
   18dc0:	add	x2, x8, #0x1
   18dc4:	mov	x0, x19
   18dc8:	str	x20, [x1, x8, lsl #3]
   18dcc:	bl	cd70 <__gmpz_prodlimbs@plt>
   18dd0:	mov	sp, x29
   18dd4:	ldp	x20, x19, [sp, #16]
   18dd8:	ldp	x29, x30, [sp], #32
   18ddc:	ret
   18de0:	mov	w1, #0x1                   	// #1
   18de4:	mov	x0, x19
   18de8:	bl	c080 <__gmpz_realloc@plt>
   18dec:	b	18c8c <__gmpz_fac_ui@@Base+0x38>

0000000000018df0 <__gmpz_oddfac_1@@Base>:
   18df0:	stp	x29, x30, [sp, #-96]!
   18df4:	stp	x28, x27, [sp, #16]
   18df8:	stp	x26, x25, [sp, #32]
   18dfc:	stp	x24, x23, [sp, #48]
   18e00:	stp	x22, x21, [sp, #64]
   18e04:	stp	x20, x19, [sp, #80]
   18e08:	mov	x29, sp
   18e0c:	sub	sp, sp, #0x30
   18e10:	mov	x20, x1
   18e14:	cmp	x1, #0x19
   18e18:	mov	x19, x0
   18e1c:	stur	w2, [x29, #-36]
   18e20:	b.hi	18e50 <__gmpz_oddfac_1@@Base+0x60>  // b.pmore
   18e24:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18e28:	ldr	w8, [x19]
   18e2c:	ldr	x9, [x9, #3848]
   18e30:	cmp	w8, #0x0
   18e34:	ldr	x20, [x9, x20, lsl #3]
   18e38:	b.le	19418 <__gmpz_oddfac_1@@Base+0x628>
   18e3c:	ldr	x0, [x19, #8]
   18e40:	mov	w8, #0x1                   	// #1
   18e44:	str	x20, [x0]
   18e48:	str	w8, [x19, #4]
   18e4c:	b	193f8 <__gmpz_oddfac_1@@Base+0x608>
   18e50:	cmp	x20, #0x23
   18e54:	b.cs	18ea8 <__gmpz_oddfac_1@@Base+0xb8>  // b.hs, b.nlast
   18e58:	ldr	w8, [x19]
   18e5c:	cmp	w8, #0x1
   18e60:	b.le	19428 <__gmpz_oddfac_1@@Base+0x638>
   18e64:	ldr	x0, [x19, #8]
   18e68:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18e6c:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18e70:	ldr	x9, [x9, #3928]
   18e74:	ldr	x10, [x10, #3848]
   18e78:	lsl	x8, x20, #2
   18e7c:	sub	x11, x8, #0x4
   18e80:	and	x8, x8, #0xfffffffffffffff8
   18e84:	and	x11, x11, #0xfffffffffffffff8
   18e88:	ldr	x8, [x10, x8]
   18e8c:	ldr	x9, [x9, x11]
   18e90:	mov	w10, #0x2                   	// #2
   18e94:	umulh	x11, x9, x8
   18e98:	mul	x8, x8, x9
   18e9c:	stp	x8, x11, [x0]
   18ea0:	str	w10, [x19, #4]
   18ea4:	b	193f8 <__gmpz_oddfac_1@@Base+0x608>
   18ea8:	cmp	x20, #0xec
   18eac:	b.cc	18ed0 <__gmpz_oddfac_1@@Base+0xe0>  // b.lo, b.ul, b.last
   18eb0:	mov	w25, wzr
   18eb4:	mov	x8, x20
   18eb8:	lsr	x13, x8, #1
   18ebc:	cmp	x8, #0x1d7
   18ec0:	add	w25, w25, #0x1
   18ec4:	mov	x8, x13
   18ec8:	b.hi	18eb8 <__gmpz_oddfac_1@@Base+0xc8>  // b.pmore
   18ecc:	b	18ed8 <__gmpz_oddfac_1@@Base+0xe8>
   18ed0:	mov	w25, wzr
   18ed4:	mov	x13, x20
   18ed8:	mov	w9, #0x4925                	// #18725
   18edc:	and	w8, w13, #0xff
   18ee0:	movk	w9, #0x2492, lsl #16
   18ee4:	umull	x9, w8, w9
   18ee8:	lsr	x9, x9, #32
   18eec:	sub	w8, w8, w9
   18ef0:	add	w8, w9, w8, lsr #1
   18ef4:	lsl	w8, w8, #1
   18ef8:	and	w8, w8, #0x7f8
   18efc:	add	w8, w8, #0x17
   18f00:	and	x8, x8, #0x7f0
   18f04:	mov	x9, sp
   18f08:	sub	x1, x9, x8
   18f0c:	mov	sp, x1
   18f10:	mov	x10, #0x70d0                	// #28880
   18f14:	mov	x12, #0xd941                	// #55617
   18f18:	movk	x10, #0xf752, lsl #16
   18f1c:	movk	x12, #0xc030, lsl #16
   18f20:	movk	x10, #0xb1e5, lsl #32
   18f24:	movk	x12, #0x2099, lsl #32
   18f28:	mov	x8, xzr
   18f2c:	movk	x10, #0x115, lsl #48
   18f30:	mov	w9, #0x1                   	// #1
   18f34:	movk	x12, #0x57e2, lsl #48
   18f38:	b	18f4c <__gmpz_oddfac_1@@Base+0x15c>
   18f3c:	lsl	x10, x10, #1
   18f40:	cmp	x11, #0x45
   18f44:	lsr	x13, x11, #1
   18f48:	b.ls	18f8c <__gmpz_oddfac_1@@Base+0x19c>  // b.plast
   18f4c:	mov	x11, x13
   18f50:	str	x12, [x1, x8, lsl #3]
   18f54:	add	x8, x8, #0x1
   18f58:	mov	w13, #0x23                  	// #35
   18f5c:	b	18f70 <__gmpz_oddfac_1@@Base+0x180>
   18f60:	mul	x9, x9, x13
   18f64:	add	x13, x13, #0x2
   18f68:	cmp	x13, x11
   18f6c:	b.hi	18f3c <__gmpz_oddfac_1@@Base+0x14c>  // b.pmore
   18f70:	cmp	x9, x10
   18f74:	b.ls	18f60 <__gmpz_oddfac_1@@Base+0x170>  // b.plast
   18f78:	add	x14, x8, #0x1
   18f7c:	str	x9, [x1, x8, lsl #3]
   18f80:	mov	x8, x14
   18f84:	mov	x9, x13
   18f88:	b	18f64 <__gmpz_oddfac_1@@Base+0x174>
   18f8c:	lsl	x12, x13, #2
   18f90:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18f94:	adrp	x14, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   18f98:	ldr	x13, [x13, #3928]
   18f9c:	ldr	x14, [x14, #3848]
   18fa0:	lsl	x11, x11, #1
   18fa4:	sub	x12, x12, #0x4
   18fa8:	and	x11, x11, #0xfffffffffffffff8
   18fac:	and	x12, x12, #0xfffffffffffffff8
   18fb0:	ldr	x12, [x13, x12]
   18fb4:	ldr	x11, [x14, x11]
   18fb8:	add	x10, x1, x8, lsl #3
   18fbc:	add	x2, x8, #0x3
   18fc0:	mov	x0, x19
   18fc4:	stp	x9, x12, [x10]
   18fc8:	str	x11, [x10, #16]
   18fcc:	bl	cd70 <__gmpz_prodlimbs@plt>
   18fd0:	cbz	w25, 193f8 <__gmpz_oddfac_1@@Base+0x608>
   18fd4:	lsr	x8, x20, #6
   18fd8:	add	x21, x8, #0x4
   18fdc:	cmp	x8, #0xfdc
   18fe0:	lsl	x1, x21, #3
   18fe4:	stur	xzr, [x29, #-24]
   18fe8:	stur	w21, [x29, #-16]
   18fec:	b.hi	19438 <__gmpz_oddfac_1@@Base+0x648>  // b.pmore
   18ff0:	add	x9, x1, #0xf
   18ff4:	mov	x8, sp
   18ff8:	and	x9, x9, #0x7ffffffffffffff0
   18ffc:	sub	x0, x8, x9
   19000:	mov	sp, x0
   19004:	lsl	x8, x21, #2
   19008:	and	x8, x8, #0x1ffffffffffffff8
   1900c:	add	x8, x0, x8
   19010:	add	x22, x8, #0x8
   19014:	stur	x0, [x29, #-8]
   19018:	sub	x1, x20, #0x1
   1901c:	mov	x0, x22
   19020:	bl	d200 <__gmp_primesieve@plt>
   19024:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   19028:	ldr	x9, [x9, #3880]
   1902c:	mov	w8, #0x9                   	// #9
   19030:	sub	w10, w8, #0x2
   19034:	ldr	x10, [x9, w10, uxtw #3]
   19038:	sub	w8, w8, #0x1
   1903c:	cmp	x10, x20
   19040:	b.cc	19030 <__gmpz_oddfac_1@@Base+0x240>  // b.lo, b.ul, b.last
   19044:	add	x9, x0, #0x1
   19048:	mov	w8, w8
   1904c:	udiv	x8, x9, x8
   19050:	lsl	x8, x8, #3
   19054:	add	x1, x8, #0x8
   19058:	mov	w8, #0x7f00                	// #32512
   1905c:	cmp	x1, x8
   19060:	b.hi	19444 <__gmpz_oddfac_1@@Base+0x654>  // b.pmore
   19064:	add	x9, x1, #0xf
   19068:	mov	x8, sp
   1906c:	and	x9, x9, #0xfffffffffffffff0
   19070:	sub	x23, x8, x9
   19074:	mov	sp, x23
   19078:	mov	x21, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1907c:	mov	w3, #0x1                   	// #1
   19080:	movk	x21, #0xaaab
   19084:	sub	w28, w25, #0x1
   19088:	lsr	x12, x20, x28
   1908c:	and	x8, x12, #0x1
   19090:	neg	x8, x8
   19094:	and	x11, x12, #0xfffffffffffffffe
   19098:	and	x8, x12, x8
   1909c:	sub	x10, x11, #0x1
   190a0:	orr	x9, x8, #0x1
   190a4:	mov	x8, #0xffffffffffffffff    	// #-1
   190a8:	udiv	x8, x8, x10
   190ac:	cmp	x9, x8
   190b0:	b.ls	190c4 <__gmpz_oddfac_1@@Base+0x2d4>  // b.plast
   190b4:	str	x9, [x23]
   190b8:	mov	w10, #0x1                   	// #1
   190bc:	mov	w9, #0x1                   	// #1
   190c0:	b	190c8 <__gmpz_oddfac_1@@Base+0x2d8>
   190c4:	mov	x10, xzr
   190c8:	mov	x13, x11
   190cc:	umulh	x14, x13, x21
   190d0:	lsr	x14, x14, #1
   190d4:	add	x15, x9, x9, lsl #1
   190d8:	tst	x14, #0x1
   190dc:	csel	x9, x9, x15, eq  // eq = none
   190e0:	cmp	x13, #0x8
   190e4:	mov	x13, x14
   190e8:	b.hi	190cc <__gmpz_oddfac_1@@Base+0x2dc>  // b.pmore
   190ec:	clz	x14, x11
   190f0:	mov	w15, #0x40                  	// #64
   190f4:	sub	w14, w15, w14
   190f8:	asr	w14, w14, #1
   190fc:	lsl	x15, x3, x14
   19100:	lsr	x14, x11, x14
   19104:	add	x14, x15, x14
   19108:	lsr	x14, x14, #1
   1910c:	sub	x14, x14, #0x5
   19110:	orr	x14, x14, #0x1
   19114:	umulh	x14, x14, x21
   19118:	mov	x16, xzr
   1911c:	mov	x13, xzr
   19120:	mov	w1, #0x7                   	// #7
   19124:	lsr	x18, x14, #1
   19128:	mov	w0, #0x1                   	// #1
   1912c:	b	19148 <__gmpz_oddfac_1@@Base+0x358>
   19130:	ror	x17, x0, #63
   19134:	add	x13, x13, x0, lsr #63
   19138:	cmp	x14, x18
   1913c:	add	x1, x15, #0x3
   19140:	mov	x0, x17
   19144:	b.cs	191a8 <__gmpz_oddfac_1@@Base+0x3b8>  // b.hs, b.nlast
   19148:	ldr	x17, [x22, x13, lsl #3]
   1914c:	mov	x14, x16
   19150:	mov	x15, x1
   19154:	add	x16, x16, #0x1
   19158:	tst	x17, x0
   1915c:	b.ne	19130 <__gmpz_oddfac_1@@Base+0x340>  // b.any
   19160:	add	x17, x16, x16, lsl #1
   19164:	and	x1, x16, #0x1
   19168:	cmp	x9, x8
   1916c:	add	x17, x17, x1
   19170:	b.ls	19184 <__gmpz_oddfac_1@@Base+0x394>  // b.plast
   19174:	add	x1, x10, #0x1
   19178:	str	x9, [x23, x10, lsl #3]
   1917c:	mov	x10, x1
   19180:	mov	w9, #0x1                   	// #1
   19184:	add	x17, x17, #0x1
   19188:	mov	x1, x11
   1918c:	udiv	x1, x1, x17
   19190:	tst	x1, #0x1
   19194:	csinc	x2, x17, xzr, ne  // ne = any
   19198:	cmp	x1, x17
   1919c:	mul	x9, x2, x9
   191a0:	b.cs	1918c <__gmpz_oddfac_1@@Base+0x39c>  // b.hs, b.nlast
   191a4:	b	19130 <__gmpz_oddfac_1@@Base+0x340>
   191a8:	umulh	x18, x11, x21
   191ac:	lsr	x18, x18, #1
   191b0:	sub	x18, x18, #0x5
   191b4:	orr	x18, x18, #0x1
   191b8:	umulh	x18, x18, x21
   191bc:	add	x16, x8, x8, lsl #1
   191c0:	lsr	x18, x18, #1
   191c4:	b	191e4 <__gmpz_oddfac_1@@Base+0x3f4>
   191c8:	mul	x9, x9, x0
   191cc:	add	x14, x14, #0x1
   191d0:	add	x13, x13, x17, lsr #63
   191d4:	ror	x17, x17, #63
   191d8:	cmp	x14, x18
   191dc:	add	x15, x15, #0x3
   191e0:	b.cs	1921c <__gmpz_oddfac_1@@Base+0x42c>  // b.hs, b.nlast
   191e4:	ldr	x0, [x22, x13, lsl #3]
   191e8:	tst	x0, x17
   191ec:	b.ne	191cc <__gmpz_oddfac_1@@Base+0x3dc>  // b.any
   191f0:	and	x0, x14, #0x1
   191f4:	add	x0, x15, x0
   191f8:	udiv	x1, x11, x0
   191fc:	tbz	w1, #0, 191cc <__gmpz_oddfac_1@@Base+0x3dc>
   19200:	cmp	x9, x16
   19204:	b.ls	191c8 <__gmpz_oddfac_1@@Base+0x3d8>  // b.plast
   19208:	add	x1, x10, #0x1
   1920c:	str	x9, [x23, x10, lsl #3]
   19210:	mov	x10, x1
   19214:	mov	x9, x0
   19218:	b	191cc <__gmpz_oddfac_1@@Base+0x3dc>
   1921c:	lsr	x12, x12, #1
   19220:	sub	x12, x12, #0x5
   19224:	orr	x12, x12, #0x1
   19228:	umulh	x12, x12, x21
   1922c:	sub	x11, x11, #0x5
   19230:	lsr	x12, x12, #1
   19234:	umulh	x11, x11, x21
   19238:	add	x14, x12, #0x1
   1923c:	add	x16, x12, x12, lsl #1
   19240:	lsr	x11, x11, #1
   19244:	add	x13, x12, #0x2
   19248:	lsr	x12, x14, #6
   1924c:	lsl	x15, x3, x14
   19250:	add	x14, x16, #0x7
   19254:	b	19274 <__gmpz_oddfac_1@@Base+0x484>
   19258:	mul	x9, x16, x9
   1925c:	add	x12, x12, x15, lsr #63
   19260:	ror	x15, x15, #63
   19264:	cmp	x13, x11
   19268:	add	x13, x13, #0x1
   1926c:	add	x14, x14, #0x3
   19270:	b.hi	192a4 <__gmpz_oddfac_1@@Base+0x4b4>  // b.pmore
   19274:	ldr	x16, [x22, x12, lsl #3]
   19278:	tst	x16, x15
   1927c:	b.ne	1925c <__gmpz_oddfac_1@@Base+0x46c>  // b.any
   19280:	and	x16, x13, #0x1
   19284:	cmp	x9, x8
   19288:	add	x16, x14, x16
   1928c:	b.ls	19258 <__gmpz_oddfac_1@@Base+0x468>  // b.plast
   19290:	add	x17, x10, #0x1
   19294:	str	x9, [x23, x10, lsl #3]
   19298:	mov	x10, x17
   1929c:	mov	x9, x16
   192a0:	b	1925c <__gmpz_oddfac_1@@Base+0x46c>
   192a4:	cbz	x10, 193a8 <__gmpz_oddfac_1@@Base+0x5b8>
   192a8:	add	x2, x10, #0x1
   192ac:	sub	x0, x29, #0x10
   192b0:	mov	x1, x23
   192b4:	str	x9, [x23, x10, lsl #3]
   192b8:	bl	cd70 <__gmpz_prodlimbs@plt>
   192bc:	stur	xzr, [x29, #-32]
   192c0:	ldur	w8, [x29, #-36]
   192c4:	ldrsw	x24, [x19, #4]
   192c8:	cmp	w25, w8
   192cc:	b.ne	19308 <__gmpz_oddfac_1@@Base+0x518>  // b.any
   192d0:	lsl	x1, x24, #3
   192d4:	mov	w8, #0x7f00                	// #32512
   192d8:	cmp	x1, x8
   192dc:	b.hi	193d0 <__gmpz_oddfac_1@@Base+0x5e0>  // b.pmore
   192e0:	add	x9, x1, #0xf
   192e4:	mov	x8, sp
   192e8:	and	x9, x9, #0xfffffffffffffff0
   192ec:	sub	x25, x8, x9
   192f0:	mov	sp, x25
   192f4:	ldr	x1, [x19, #8]
   192f8:	mov	x0, x25
   192fc:	mov	x2, x24
   19300:	bl	ca50 <__gmpn_copyi@plt>
   19304:	b	19354 <__gmpz_oddfac_1@@Base+0x564>
   19308:	lsl	x1, x24, #4
   1930c:	mov	w8, #0x7f00                	// #32512
   19310:	cmp	x1, x8
   19314:	lsl	x26, x24, #1
   19318:	b.hi	193e0 <__gmpz_oddfac_1@@Base+0x5f0>  // b.pmore
   1931c:	add	x9, x1, #0xf
   19320:	mov	x8, sp
   19324:	and	x9, x9, #0xfffffffffffffff0
   19328:	sub	x25, x8, x9
   1932c:	mov	sp, x25
   19330:	ldr	x1, [x19, #8]
   19334:	mov	x0, x25
   19338:	mov	x2, x24
   1933c:	bl	c8e0 <__gmpn_sqr@plt>
   19340:	add	x8, x25, x26, lsl #3
   19344:	ldur	x8, [x8, #-8]
   19348:	cmp	x8, #0x0
   1934c:	cset	w8, eq  // eq = none
   19350:	sub	x24, x26, x8
   19354:	ldursw	x27, [x29, #-12]
   19358:	ldrsw	x8, [x19]
   1935c:	add	x26, x24, x27
   19360:	cmp	x26, x8
   19364:	b.gt	193b8 <__gmpz_oddfac_1@@Base+0x5c8>
   19368:	ldr	x0, [x19, #8]
   1936c:	ldur	x3, [x29, #-8]
   19370:	mov	x1, x25
   19374:	mov	x2, x24
   19378:	mov	x4, x27
   1937c:	bl	ccd0 <__gmpn_mul@plt>
   19380:	cmp	x0, #0x0
   19384:	cset	w8, eq  // eq = none
   19388:	sub	w8, w26, w8
   1938c:	str	w8, [x19, #4]
   19390:	ldur	x0, [x29, #-32]
   19394:	cbnz	x0, 193c8 <__gmpz_oddfac_1@@Base+0x5d8>
   19398:	mov	w25, w28
   1939c:	mov	w3, #0x1                   	// #1
   193a0:	cbnz	w28, 19084 <__gmpz_oddfac_1@@Base+0x294>
   193a4:	b	193f0 <__gmpz_oddfac_1@@Base+0x600>
   193a8:	ldur	x8, [x29, #-8]
   193ac:	str	x9, [x8]
   193b0:	stur	w3, [x29, #-12]
   193b4:	b	192bc <__gmpz_oddfac_1@@Base+0x4cc>
   193b8:	mov	x0, x19
   193bc:	mov	x1, x26
   193c0:	bl	c080 <__gmpz_realloc@plt>
   193c4:	b	1936c <__gmpz_oddfac_1@@Base+0x57c>
   193c8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   193cc:	b	19398 <__gmpz_oddfac_1@@Base+0x5a8>
   193d0:	sub	x0, x29, #0x20
   193d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   193d8:	mov	x25, x0
   193dc:	b	192f4 <__gmpz_oddfac_1@@Base+0x504>
   193e0:	sub	x0, x29, #0x20
   193e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   193e8:	mov	x25, x0
   193ec:	b	19330 <__gmpz_oddfac_1@@Base+0x540>
   193f0:	ldur	x0, [x29, #-24]
   193f4:	cbnz	x0, 19454 <__gmpz_oddfac_1@@Base+0x664>
   193f8:	mov	sp, x29
   193fc:	ldp	x20, x19, [sp, #80]
   19400:	ldp	x22, x21, [sp, #64]
   19404:	ldp	x24, x23, [sp, #48]
   19408:	ldp	x26, x25, [sp, #32]
   1940c:	ldp	x28, x27, [sp, #16]
   19410:	ldp	x29, x30, [sp], #96
   19414:	ret
   19418:	mov	w1, #0x1                   	// #1
   1941c:	mov	x0, x19
   19420:	bl	c080 <__gmpz_realloc@plt>
   19424:	b	18e40 <__gmpz_oddfac_1@@Base+0x50>
   19428:	mov	w1, #0x2                   	// #2
   1942c:	mov	x0, x19
   19430:	bl	c080 <__gmpz_realloc@plt>
   19434:	b	18e68 <__gmpz_oddfac_1@@Base+0x78>
   19438:	sub	x0, x29, #0x18
   1943c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19440:	b	19004 <__gmpz_oddfac_1@@Base+0x214>
   19444:	sub	x0, x29, #0x18
   19448:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1944c:	mov	x23, x0
   19450:	b	19078 <__gmpz_oddfac_1@@Base+0x288>
   19454:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19458:	b	193f8 <__gmpz_oddfac_1@@Base+0x608>

000000000001945c <__gmpz_prodlimbs@@Base>:
   1945c:	stp	x29, x30, [sp, #-64]!
   19460:	stp	x24, x23, [sp, #16]
   19464:	stp	x22, x21, [sp, #32]
   19468:	stp	x20, x19, [sp, #48]
   1946c:	mov	x29, sp
   19470:	sub	sp, sp, #0x30
   19474:	mov	x20, x1
   19478:	cmp	x2, #0xd
   1947c:	mov	x19, x0
   19480:	b.le	1951c <__gmpz_prodlimbs@@Base+0xc0>
   19484:	lsr	x21, x2, #1
   19488:	sub	x22, x2, x21
   1948c:	lsl	x1, x22, #3
   19490:	mov	w8, #0x7f00                	// #32512
   19494:	cmp	x1, x8
   19498:	stur	xzr, [x29, #-40]
   1949c:	stur	w22, [x29, #-32]
   194a0:	b.hi	195a4 <__gmpz_prodlimbs@@Base+0x148>  // b.pmore
   194a4:	add	x9, x1, #0xf
   194a8:	mov	x8, sp
   194ac:	and	x9, x9, #0xfffffffffffffff0
   194b0:	sub	x0, x8, x9
   194b4:	mov	sp, x0
   194b8:	stur	x0, [x29, #-24]
   194bc:	add	x1, x20, x21, lsl #3
   194c0:	sub	x0, x29, #0x20
   194c4:	mov	x2, x22
   194c8:	stur	x1, [x29, #-8]
   194cc:	stur	w22, [x29, #-16]
   194d0:	bl	cd70 <__gmpz_prodlimbs@plt>
   194d4:	mov	x22, x0
   194d8:	sub	x0, x29, #0x10
   194dc:	mov	x1, x20
   194e0:	mov	x2, x21
   194e4:	bl	cd70 <__gmpz_prodlimbs@plt>
   194e8:	ldrsw	x8, [x19]
   194ec:	add	x20, x0, x22
   194f0:	mov	x21, x0
   194f4:	cmp	x20, x8
   194f8:	b.gt	195b0 <__gmpz_prodlimbs@@Base+0x154>
   194fc:	ldr	x0, [x19, #8]
   19500:	cmp	x21, x22
   19504:	b.ge	195c4 <__gmpz_prodlimbs@@Base+0x168>  // b.tcont
   19508:	ldur	x1, [x29, #-24]
   1950c:	ldur	x3, [x29, #-8]
   19510:	mov	x2, x22
   19514:	mov	x4, x21
   19518:	b	195d4 <__gmpz_prodlimbs@@Base+0x178>
   1951c:	cmp	x2, #0x3
   19520:	b.lt	19568 <__gmpz_prodlimbs@@Base+0x10c>  // b.tstop
   19524:	mov	x22, xzr
   19528:	sub	x23, x2, #0x1
   1952c:	sub	x24, x2, #0x2
   19530:	mov	w21, #0x1                   	// #1
   19534:	add	x8, x20, x22, lsl #3
   19538:	ldr	x3, [x8, #8]
   1953c:	mov	x0, x20
   19540:	mov	x1, x20
   19544:	mov	x2, x21
   19548:	bl	d490 <__gmpn_mul_1@plt>
   1954c:	cmp	x0, #0x0
   19550:	add	x22, x22, #0x1
   19554:	str	x0, [x20, x21, lsl #3]
   19558:	cinc	x21, x21, ne  // ne = any
   1955c:	cmp	x24, x22
   19560:	b.ne	19534 <__gmpz_prodlimbs@@Base+0xd8>  // b.any
   19564:	b	19570 <__gmpz_prodlimbs@@Base+0x114>
   19568:	mov	w21, #0x1                   	// #1
   1956c:	mov	w23, #0x1                   	// #1
   19570:	ldrsw	x8, [x19]
   19574:	cmp	x21, x8
   19578:	b.ge	19610 <__gmpz_prodlimbs@@Base+0x1b4>  // b.tcont
   1957c:	ldr	x22, [x19, #8]
   19580:	ldr	x3, [x20, x23, lsl #3]
   19584:	mov	x0, x22
   19588:	mov	x1, x20
   1958c:	mov	x2, x21
   19590:	bl	d490 <__gmpn_mul_1@plt>
   19594:	cmp	x0, #0x0
   19598:	str	x0, [x22, x21, lsl #3]
   1959c:	cinc	x8, x21, ne  // ne = any
   195a0:	b	195f0 <__gmpz_prodlimbs@@Base+0x194>
   195a4:	sub	x0, x29, #0x28
   195a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   195ac:	b	194b8 <__gmpz_prodlimbs@@Base+0x5c>
   195b0:	mov	x0, x19
   195b4:	mov	x1, x20
   195b8:	bl	c080 <__gmpz_realloc@plt>
   195bc:	cmp	x21, x22
   195c0:	b.lt	19508 <__gmpz_prodlimbs@@Base+0xac>  // b.tstop
   195c4:	ldur	x1, [x29, #-8]
   195c8:	ldur	x3, [x29, #-24]
   195cc:	mov	x2, x21
   195d0:	mov	x4, x22
   195d4:	bl	ccd0 <__gmpn_mul@plt>
   195d8:	mov	x21, x0
   195dc:	ldur	x0, [x29, #-40]
   195e0:	cbnz	x0, 19624 <__gmpz_prodlimbs@@Base+0x1c8>
   195e4:	cmp	x21, #0x0
   195e8:	cset	w8, eq  // eq = none
   195ec:	sub	x8, x20, x8
   195f0:	str	w8, [x19, #4]
   195f4:	sxtw	x0, w8
   195f8:	mov	sp, x29
   195fc:	ldp	x20, x19, [sp, #48]
   19600:	ldp	x22, x21, [sp, #32]
   19604:	ldp	x24, x23, [sp, #16]
   19608:	ldp	x29, x30, [sp], #64
   1960c:	ret
   19610:	add	x1, x21, #0x1
   19614:	mov	x0, x19
   19618:	bl	c080 <__gmpz_realloc@plt>
   1961c:	mov	x22, x0
   19620:	b	19580 <__gmpz_prodlimbs@@Base+0x124>
   19624:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19628:	b	195e4 <__gmpz_prodlimbs@@Base+0x188>

000000000001962c <__gmpz_fdiv_q_ui@@Base>:
   1962c:	stp	x29, x30, [sp, #-64]!
   19630:	stp	x24, x23, [sp, #16]
   19634:	stp	x22, x21, [sp, #32]
   19638:	stp	x20, x19, [sp, #48]
   1963c:	mov	x29, sp
   19640:	cbz	x2, 196fc <__gmpz_fdiv_q_ui@@Base+0xd0>
   19644:	ldrsw	x24, [x1, #4]
   19648:	mov	x23, x1
   1964c:	mov	x19, x0
   19650:	cbz	w24, 196c8 <__gmpz_fdiv_q_ui@@Base+0x9c>
   19654:	ldrsw	x8, [x19]
   19658:	cmp	w24, #0x0
   1965c:	cneg	x21, x24, lt  // lt = tstop
   19660:	mov	x20, x2
   19664:	cmp	x21, x8
   19668:	b.gt	196e8 <__gmpz_fdiv_q_ui@@Base+0xbc>
   1966c:	ldr	x22, [x19, #8]
   19670:	ldr	x2, [x23, #8]
   19674:	mov	x0, x22
   19678:	mov	x1, xzr
   1967c:	mov	x3, x21
   19680:	mov	x4, x20
   19684:	bl	cd00 <__gmpn_divrem_1@plt>
   19688:	tbz	w24, #31, 196a8 <__gmpz_fdiv_q_ui@@Base+0x7c>
   1968c:	cbz	x0, 196a8 <__gmpz_fdiv_q_ui@@Base+0x7c>
   19690:	mov	x8, x22
   19694:	ldr	x9, [x8]
   19698:	adds	x9, x9, #0x1
   1969c:	str	x9, [x8], #8
   196a0:	b.cs	19694 <__gmpz_fdiv_q_ui@@Base+0x68>  // b.hs, b.nlast
   196a4:	sub	x0, x20, x0
   196a8:	add	x8, x22, x21, lsl #3
   196ac:	ldur	x8, [x8, #-8]
   196b0:	cmp	x8, #0x0
   196b4:	cset	w8, eq  // eq = none
   196b8:	sub	w8, w21, w8
   196bc:	cmp	w24, #0x0
   196c0:	cneg	w8, w8, lt  // lt = tstop
   196c4:	b	196d0 <__gmpz_fdiv_q_ui@@Base+0xa4>
   196c8:	mov	w8, wzr
   196cc:	mov	x0, xzr
   196d0:	str	w8, [x19, #4]
   196d4:	ldp	x20, x19, [sp, #48]
   196d8:	ldp	x22, x21, [sp, #32]
   196dc:	ldp	x24, x23, [sp, #16]
   196e0:	ldp	x29, x30, [sp], #64
   196e4:	ret
   196e8:	mov	x0, x19
   196ec:	mov	x1, x21
   196f0:	bl	c080 <__gmpz_realloc@plt>
   196f4:	mov	x22, x0
   196f8:	b	19670 <__gmpz_fdiv_q_ui@@Base+0x44>
   196fc:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019700 <__gmpz_fdiv_qr@@Base>:
   19700:	stp	x29, x30, [sp, #-64]!
   19704:	str	x23, [sp, #16]
   19708:	stp	x22, x21, [sp, #32]
   1970c:	stp	x20, x19, [sp, #48]
   19710:	mov	x29, sp
   19714:	sub	sp, sp, #0x10
   19718:	ldrsw	x23, [x3, #4]
   1971c:	mov	x20, x3
   19720:	mov	x22, x2
   19724:	mov	x19, x1
   19728:	mov	x21, x0
   1972c:	cmp	x0, x3
   19730:	str	xzr, [x29, #24]
   19734:	b.eq	19740 <__gmpz_fdiv_qr@@Base+0x40>  // b.none
   19738:	cmp	x19, x20
   1973c:	b.ne	19780 <__gmpz_fdiv_qr@@Base+0x80>  // b.any
   19740:	cmp	x23, #0x0
   19744:	cneg	x8, x23, mi  // mi = first
   19748:	cmp	x8, #0xfe0
   1974c:	lsl	x1, x8, #3
   19750:	stur	w8, [x29, #-16]
   19754:	b.hi	197f0 <__gmpz_fdiv_qr@@Base+0xf0>  // b.pmore
   19758:	add	x9, x1, #0xf
   1975c:	mov	x8, sp
   19760:	and	x9, x9, #0xfffffffffffffff0
   19764:	sub	x0, x8, x9
   19768:	mov	sp, x0
   1976c:	stur	x0, [x29, #-8]
   19770:	sub	x0, x29, #0x10
   19774:	mov	x1, x20
   19778:	bl	c420 <__gmpz_set@plt>
   1977c:	sub	x20, x29, #0x10
   19780:	ldr	w8, [x22, #4]
   19784:	mov	x0, x21
   19788:	mov	x1, x19
   1978c:	mov	x2, x22
   19790:	mov	x3, x20
   19794:	eor	w23, w8, w23
   19798:	bl	bff0 <__gmpz_tdiv_qr@plt>
   1979c:	tbz	w23, #31, 197c8 <__gmpz_fdiv_qr@@Base+0xc8>
   197a0:	ldr	w8, [x19, #4]
   197a4:	cbz	w8, 197c8 <__gmpz_fdiv_qr@@Base+0xc8>
   197a8:	mov	w2, #0x1                   	// #1
   197ac:	mov	x0, x21
   197b0:	mov	x1, x21
   197b4:	bl	c120 <__gmpz_sub_ui@plt>
   197b8:	mov	x0, x19
   197bc:	mov	x1, x19
   197c0:	mov	x2, x20
   197c4:	bl	cf90 <__gmpz_add@plt>
   197c8:	ldr	x0, [x29, #24]
   197cc:	cbnz	x0, 197e8 <__gmpz_fdiv_qr@@Base+0xe8>
   197d0:	mov	sp, x29
   197d4:	ldp	x20, x19, [sp, #48]
   197d8:	ldp	x22, x21, [sp, #32]
   197dc:	ldr	x23, [sp, #16]
   197e0:	ldp	x29, x30, [sp], #64
   197e4:	ret
   197e8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   197ec:	b	197d0 <__gmpz_fdiv_qr@@Base+0xd0>
   197f0:	add	x0, x29, #0x18
   197f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   197f8:	b	1976c <__gmpz_fdiv_qr@@Base+0x6c>

00000000000197fc <__gmpz_fdiv_qr_ui@@Base>:
   197fc:	stp	x29, x30, [sp, #-80]!
   19800:	str	x25, [sp, #16]
   19804:	stp	x24, x23, [sp, #32]
   19808:	stp	x22, x21, [sp, #48]
   1980c:	stp	x20, x19, [sp, #64]
   19810:	mov	x29, sp
   19814:	cbz	x3, 19920 <__gmpz_fdiv_qr_ui@@Base+0x124>
   19818:	ldrsw	x25, [x2, #4]
   1981c:	mov	x22, x2
   19820:	mov	x20, x1
   19824:	mov	x19, x0
   19828:	cbz	w25, 198a4 <__gmpz_fdiv_qr_ui@@Base+0xa8>
   1982c:	ldrsw	x8, [x19]
   19830:	cmp	w25, #0x0
   19834:	cneg	x21, x25, lt  // lt = tstop
   19838:	mov	x24, x3
   1983c:	cmp	x21, x8
   19840:	b.gt	198fc <__gmpz_fdiv_qr_ui@@Base+0x100>
   19844:	ldr	x23, [x19, #8]
   19848:	ldr	x2, [x22, #8]
   1984c:	mov	x0, x23
   19850:	mov	x1, xzr
   19854:	mov	x3, x21
   19858:	mov	x4, x24
   1985c:	bl	cd00 <__gmpn_divrem_1@plt>
   19860:	mov	x22, x0
   19864:	cbz	x0, 198b8 <__gmpz_fdiv_qr_ui@@Base+0xbc>
   19868:	tbz	w25, #31, 19884 <__gmpz_fdiv_qr_ui@@Base+0x88>
   1986c:	mov	x8, x23
   19870:	ldr	x9, [x8]
   19874:	adds	x9, x9, #0x1
   19878:	str	x9, [x8], #8
   1987c:	b.cs	19870 <__gmpz_fdiv_qr_ui@@Base+0x74>  // b.hs, b.nlast
   19880:	sub	x22, x24, x22
   19884:	ldr	w8, [x20]
   19888:	cmp	w8, #0x0
   1988c:	b.le	19910 <__gmpz_fdiv_qr_ui@@Base+0x114>
   19890:	ldr	x0, [x20, #8]
   19894:	cmp	x22, #0x0
   19898:	cset	w8, ne  // ne = any
   1989c:	str	x22, [x0]
   198a0:	b	198bc <__gmpz_fdiv_qr_ui@@Base+0xc0>
   198a4:	mov	w8, wzr
   198a8:	mov	x22, xzr
   198ac:	str	wzr, [x19, #4]
   198b0:	mov	x19, x20
   198b4:	b	198dc <__gmpz_fdiv_qr_ui@@Base+0xe0>
   198b8:	mov	w8, wzr
   198bc:	str	w8, [x20, #4]
   198c0:	add	x8, x23, x21, lsl #3
   198c4:	ldur	x8, [x8, #-8]
   198c8:	cmp	x8, #0x0
   198cc:	cset	w8, eq  // eq = none
   198d0:	sub	w8, w21, w8
   198d4:	cmp	w25, #0x0
   198d8:	cneg	w8, w8, lt  // lt = tstop
   198dc:	str	w8, [x19, #4]
   198e0:	mov	x0, x22
   198e4:	ldp	x20, x19, [sp, #64]
   198e8:	ldp	x22, x21, [sp, #48]
   198ec:	ldp	x24, x23, [sp, #32]
   198f0:	ldr	x25, [sp, #16]
   198f4:	ldp	x29, x30, [sp], #80
   198f8:	ret
   198fc:	mov	x0, x19
   19900:	mov	x1, x21
   19904:	bl	c080 <__gmpz_realloc@plt>
   19908:	mov	x23, x0
   1990c:	b	19848 <__gmpz_fdiv_qr_ui@@Base+0x4c>
   19910:	mov	w1, #0x1                   	// #1
   19914:	mov	x0, x20
   19918:	bl	c080 <__gmpz_realloc@plt>
   1991c:	b	19894 <__gmpz_fdiv_qr_ui@@Base+0x98>
   19920:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019924 <__gmpz_fdiv_r@@Base>:
   19924:	stp	x29, x30, [sp, #-48]!
   19928:	stp	x22, x21, [sp, #16]
   1992c:	stp	x20, x19, [sp, #32]
   19930:	mov	x29, sp
   19934:	sub	sp, sp, #0x20
   19938:	ldrsw	x22, [x2, #4]
   1993c:	mov	x19, x2
   19940:	mov	x21, x1
   19944:	mov	x20, x0
   19948:	cmp	x0, x2
   1994c:	stur	xzr, [x29, #-24]
   19950:	b.ne	19994 <__gmpz_fdiv_r@@Base+0x70>  // b.any
   19954:	cmp	x22, #0x0
   19958:	cneg	x8, x22, mi  // mi = first
   1995c:	cmp	x8, #0xfe0
   19960:	lsl	x1, x8, #3
   19964:	stur	w8, [x29, #-16]
   19968:	b.hi	199ec <__gmpz_fdiv_r@@Base+0xc8>  // b.pmore
   1996c:	add	x9, x1, #0xf
   19970:	mov	x8, sp
   19974:	and	x9, x9, #0xfffffffffffffff0
   19978:	sub	x0, x8, x9
   1997c:	mov	sp, x0
   19980:	stur	x0, [x29, #-8]
   19984:	sub	x0, x29, #0x10
   19988:	mov	x1, x19
   1998c:	bl	c420 <__gmpz_set@plt>
   19990:	sub	x19, x29, #0x10
   19994:	mov	x0, x20
   19998:	mov	x1, x21
   1999c:	mov	x2, x19
   199a0:	bl	ca80 <__gmpz_tdiv_r@plt>
   199a4:	ldr	w8, [x21, #4]
   199a8:	eor	w8, w8, w22
   199ac:	tbz	w8, #31, 199c8 <__gmpz_fdiv_r@@Base+0xa4>
   199b0:	ldr	w8, [x20, #4]
   199b4:	cbz	w8, 199c8 <__gmpz_fdiv_r@@Base+0xa4>
   199b8:	mov	x0, x20
   199bc:	mov	x1, x20
   199c0:	mov	x2, x19
   199c4:	bl	cf90 <__gmpz_add@plt>
   199c8:	ldur	x0, [x29, #-24]
   199cc:	cbnz	x0, 199e4 <__gmpz_fdiv_r@@Base+0xc0>
   199d0:	mov	sp, x29
   199d4:	ldp	x20, x19, [sp, #32]
   199d8:	ldp	x22, x21, [sp, #16]
   199dc:	ldp	x29, x30, [sp], #48
   199e0:	ret
   199e4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   199e8:	b	199d0 <__gmpz_fdiv_r@@Base+0xac>
   199ec:	sub	x0, x29, #0x18
   199f0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   199f4:	b	19980 <__gmpz_fdiv_r@@Base+0x5c>

00000000000199f8 <__gmpz_fdiv_r_ui@@Base>:
   199f8:	stp	x29, x30, [sp, #-48]!
   199fc:	str	x21, [sp, #16]
   19a00:	stp	x20, x19, [sp, #32]
   19a04:	mov	x29, sp
   19a08:	cbz	x2, 19a88 <__gmpz_fdiv_r_ui@@Base+0x90>
   19a0c:	ldrsw	x21, [x1, #4]
   19a10:	mov	x19, x0
   19a14:	cbz	w21, 19a58 <__gmpz_fdiv_r_ui@@Base+0x60>
   19a18:	ldr	x0, [x1, #8]
   19a1c:	cmp	x21, #0x0
   19a20:	cneg	x1, x21, mi  // mi = first
   19a24:	mov	x20, x2
   19a28:	bl	c3e0 <__gmpn_mod_1@plt>
   19a2c:	cbz	x0, 19a58 <__gmpz_fdiv_r_ui@@Base+0x60>
   19a30:	ldr	w8, [x19]
   19a34:	sub	x9, x20, x0
   19a38:	cmp	w21, #0x0
   19a3c:	csel	x20, x9, x0, lt  // lt = tstop
   19a40:	cmp	w8, #0x0
   19a44:	b.le	19a78 <__gmpz_fdiv_r_ui@@Base+0x80>
   19a48:	ldr	x0, [x19, #8]
   19a4c:	mov	w8, #0x1                   	// #1
   19a50:	str	x20, [x0]
   19a54:	b	19a60 <__gmpz_fdiv_r_ui@@Base+0x68>
   19a58:	mov	w8, wzr
   19a5c:	mov	x20, xzr
   19a60:	str	w8, [x19, #4]
   19a64:	mov	x0, x20
   19a68:	ldp	x20, x19, [sp, #32]
   19a6c:	ldr	x21, [sp, #16]
   19a70:	ldp	x29, x30, [sp], #48
   19a74:	ret
   19a78:	mov	w1, #0x1                   	// #1
   19a7c:	mov	x0, x19
   19a80:	bl	c080 <__gmpz_realloc@plt>
   19a84:	b	19a4c <__gmpz_fdiv_r_ui@@Base+0x54>
   19a88:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019a8c <__gmpz_fdiv_q@@Base>:
   19a8c:	stp	x29, x30, [sp, #-64]!
   19a90:	str	x23, [sp, #16]
   19a94:	stp	x22, x21, [sp, #32]
   19a98:	stp	x20, x19, [sp, #48]
   19a9c:	mov	x29, sp
   19aa0:	sub	sp, sp, #0x10
   19aa4:	ldrsw	x22, [x2, #4]
   19aa8:	ldr	w23, [x1, #4]
   19aac:	mov	x20, x2
   19ab0:	mov	x21, x1
   19ab4:	cmp	x22, #0x0
   19ab8:	cneg	x8, x22, mi  // mi = first
   19abc:	mov	x19, x0
   19ac0:	cmp	x8, #0xfe0
   19ac4:	lsl	x1, x8, #3
   19ac8:	str	xzr, [x29, #24]
   19acc:	stur	w8, [x29, #-16]
   19ad0:	b.hi	19b40 <__gmpz_fdiv_q@@Base+0xb4>  // b.pmore
   19ad4:	add	x9, x1, #0xf
   19ad8:	mov	x8, sp
   19adc:	and	x9, x9, #0xfffffffffffffff0
   19ae0:	sub	x0, x8, x9
   19ae4:	mov	sp, x0
   19ae8:	stur	x0, [x29, #-8]
   19aec:	sub	x1, x29, #0x10
   19af0:	mov	x0, x19
   19af4:	mov	x2, x21
   19af8:	mov	x3, x20
   19afc:	bl	bff0 <__gmpz_tdiv_qr@plt>
   19b00:	eor	w8, w22, w23
   19b04:	tbz	w8, #31, 19b20 <__gmpz_fdiv_q@@Base+0x94>
   19b08:	ldur	w8, [x29, #-12]
   19b0c:	cbz	w8, 19b20 <__gmpz_fdiv_q@@Base+0x94>
   19b10:	mov	w2, #0x1                   	// #1
   19b14:	mov	x0, x19
   19b18:	mov	x1, x19
   19b1c:	bl	c120 <__gmpz_sub_ui@plt>
   19b20:	ldr	x0, [x29, #24]
   19b24:	cbnz	x0, 19b4c <__gmpz_fdiv_q@@Base+0xc0>
   19b28:	mov	sp, x29
   19b2c:	ldp	x20, x19, [sp, #48]
   19b30:	ldp	x22, x21, [sp, #32]
   19b34:	ldr	x23, [sp, #16]
   19b38:	ldp	x29, x30, [sp], #64
   19b3c:	ret
   19b40:	add	x0, x29, #0x18
   19b44:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19b48:	b	19ae8 <__gmpz_fdiv_q@@Base+0x5c>
   19b4c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19b50:	b	19b28 <__gmpz_fdiv_q@@Base+0x9c>

0000000000019b54 <__gmpz_fdiv_ui@@Base>:
   19b54:	stp	x29, x30, [sp, #-32]!
   19b58:	stp	x20, x19, [sp, #16]
   19b5c:	mov	x29, sp
   19b60:	cbz	x1, 19ba8 <__gmpz_fdiv_ui@@Base+0x54>
   19b64:	ldrsw	x20, [x0, #4]
   19b68:	cbz	w20, 19b98 <__gmpz_fdiv_ui@@Base+0x44>
   19b6c:	ldr	x0, [x0, #8]
   19b70:	mov	x19, x1
   19b74:	cmp	x20, #0x0
   19b78:	cneg	x1, x20, mi  // mi = first
   19b7c:	mov	x2, x19
   19b80:	bl	c3e0 <__gmpn_mod_1@plt>
   19b84:	cmp	x0, #0x0
   19b88:	sub	x8, x19, x0
   19b8c:	ccmp	w20, #0x0, #0x0, ne  // ne = any
   19b90:	csel	x0, x8, x0, lt  // lt = tstop
   19b94:	b	19b9c <__gmpz_fdiv_ui@@Base+0x48>
   19b98:	mov	x0, xzr
   19b9c:	ldp	x20, x19, [sp, #16]
   19ba0:	ldp	x29, x30, [sp], #32
   19ba4:	ret
   19ba8:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000019bac <__gmpz_fib_ui@@Base>:
   19bac:	stp	x29, x30, [sp, #-96]!
   19bb0:	stp	x20, x19, [sp, #80]
   19bb4:	mov	x20, x1
   19bb8:	cmp	x1, #0x5d
   19bbc:	mov	x19, x0
   19bc0:	str	x27, [sp, #16]
   19bc4:	stp	x26, x25, [sp, #32]
   19bc8:	stp	x24, x23, [sp, #48]
   19bcc:	stp	x22, x21, [sp, #64]
   19bd0:	mov	x29, sp
   19bd4:	b.hi	19c0c <__gmpz_fib_ui@@Base+0x60>  // b.pmore
   19bd8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   19bdc:	ldr	x8, [x8, #3808]
   19be0:	ldr	w9, [x19]
   19be4:	add	x8, x8, x20, lsl #3
   19be8:	ldr	x21, [x8, #8]
   19bec:	cmp	w9, #0x0
   19bf0:	b.le	19d90 <__gmpz_fib_ui@@Base+0x1e4>
   19bf4:	ldr	x0, [x19, #8]
   19bf8:	cmp	x20, #0x0
   19bfc:	cset	w8, ne  // ne = any
   19c00:	str	x21, [x0]
   19c04:	str	w8, [x19, #4]
   19c08:	b	19d70 <__gmpz_fib_ui@@Base+0x1c4>
   19c0c:	lsr	x8, x20, #6
   19c10:	mov	w9, #0x17                  	// #23
   19c14:	mul	x22, x8, x9
   19c18:	ldrsw	x8, [x19]
   19c1c:	lsr	x9, x22, #6
   19c20:	add	x23, x9, #0x5
   19c24:	lsl	x1, x23, #1
   19c28:	cmp	x1, x8
   19c2c:	lsr	x24, x20, #1
   19c30:	b.gt	19da0 <__gmpz_fib_ui@@Base+0x1f4>
   19c34:	ldr	x21, [x19, #8]
   19c38:	lsr	x8, x22, #8
   19c3c:	cmp	x8, #0x1fa
   19c40:	lsl	x1, x23, #4
   19c44:	str	xzr, [x29, #24]
   19c48:	b.hi	19db0 <__gmpz_fib_ui@@Base+0x204>  // b.pmore
   19c4c:	add	x9, x1, #0xf
   19c50:	mov	x8, sp
   19c54:	and	x9, x9, #0x7ffffffffffffff0
   19c58:	sub	x22, x8, x9
   19c5c:	mov	sp, x22
   19c60:	add	x23, x22, x23, lsl #3
   19c64:	mov	x0, x22
   19c68:	mov	x1, x23
   19c6c:	mov	x2, x24
   19c70:	bl	d070 <__gmpn_fib2_ui@plt>
   19c74:	mov	x24, x0
   19c78:	tbnz	w20, #0, 19cb8 <__gmpz_fib_ui@@Base+0x10c>
   19c7c:	mov	x0, x23
   19c80:	mov	x1, x22
   19c84:	mov	x2, x23
   19c88:	mov	x3, x24
   19c8c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   19c90:	cmp	x0, #0x0
   19c94:	str	x0, [x23, x24, lsl #3]
   19c98:	cinc	x2, x24, ne  // ne = any
   19c9c:	mov	x0, x21
   19ca0:	mov	x1, x23
   19ca4:	mov	x3, x22
   19ca8:	mov	x4, x24
   19cac:	add	x25, x2, x24
   19cb0:	bl	ccd0 <__gmpn_mul@plt>
   19cb4:	b	19d44 <__gmpz_fib_ui@@Base+0x198>
   19cb8:	mov	w3, #0x1                   	// #1
   19cbc:	mov	x0, x21
   19cc0:	mov	x1, x22
   19cc4:	mov	x2, x24
   19cc8:	bl	c180 <__gmpn_lshift@plt>
   19ccc:	mov	x25, x0
   19cd0:	mov	x0, x22
   19cd4:	mov	x1, x21
   19cd8:	mov	x2, x23
   19cdc:	mov	x3, x24
   19ce0:	bl	ca70 <__gmpn_add_n@plt>
   19ce4:	adds	x8, x0, x25
   19ce8:	lsl	x27, x24, #3
   19cec:	mov	x0, x23
   19cf0:	mov	x1, x21
   19cf4:	mov	x2, x23
   19cf8:	mov	x3, x24
   19cfc:	str	x8, [x22, x27]
   19d00:	cinc	x26, x24, ne  // ne = any
   19d04:	bl	c2d0 <__gmpn_sub_n@plt>
   19d08:	sub	x8, x25, x0
   19d0c:	add	x4, x8, x24
   19d10:	mov	x0, x21
   19d14:	mov	x1, x22
   19d18:	mov	x2, x26
   19d1c:	mov	x3, x23
   19d20:	str	x8, [x23, x27]
   19d24:	add	x25, x26, x4
   19d28:	bl	ccd0 <__gmpn_mul@plt>
   19d2c:	ldr	x8, [x21]
   19d30:	tst	x20, #0x2
   19d34:	mov	w9, #0x2                   	// #2
   19d38:	cneg	x9, x9, ne  // ne = any
   19d3c:	add	x8, x8, x9
   19d40:	str	x8, [x21]
   19d44:	cmp	x0, #0x0
   19d48:	cset	w8, eq  // eq = none
   19d4c:	sub	x8, x25, x8
   19d50:	add	x9, x21, x8, lsl #3
   19d54:	ldur	x9, [x9, #-8]
   19d58:	cmp	x9, #0x0
   19d5c:	cset	w9, eq  // eq = none
   19d60:	sub	w8, w8, w9
   19d64:	str	w8, [x19, #4]
   19d68:	ldr	x0, [x29, #24]
   19d6c:	cbnz	x0, 19dc0 <__gmpz_fib_ui@@Base+0x214>
   19d70:	mov	sp, x29
   19d74:	ldp	x20, x19, [sp, #80]
   19d78:	ldp	x22, x21, [sp, #64]
   19d7c:	ldp	x24, x23, [sp, #48]
   19d80:	ldp	x26, x25, [sp, #32]
   19d84:	ldr	x27, [sp, #16]
   19d88:	ldp	x29, x30, [sp], #96
   19d8c:	ret
   19d90:	mov	w1, #0x1                   	// #1
   19d94:	mov	x0, x19
   19d98:	bl	c080 <__gmpz_realloc@plt>
   19d9c:	b	19bf8 <__gmpz_fib_ui@@Base+0x4c>
   19da0:	mov	x0, x19
   19da4:	bl	c080 <__gmpz_realloc@plt>
   19da8:	mov	x21, x0
   19dac:	b	19c38 <__gmpz_fib_ui@@Base+0x8c>
   19db0:	add	x0, x29, #0x18
   19db4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   19db8:	mov	x22, x0
   19dbc:	b	19c60 <__gmpz_fib_ui@@Base+0xb4>
   19dc0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   19dc4:	b	19d70 <__gmpz_fib_ui@@Base+0x1c4>

0000000000019dc8 <__gmpz_fib2_ui@@Base>:
   19dc8:	stp	x29, x30, [sp, #-64]!
   19dcc:	stp	x22, x21, [sp, #32]
   19dd0:	stp	x20, x19, [sp, #48]
   19dd4:	mov	x20, x2
   19dd8:	mov	x19, x1
   19ddc:	cmp	x2, #0x5d
   19de0:	mov	x21, x0
   19de4:	str	x23, [sp, #16]
   19de8:	mov	x29, sp
   19dec:	b.hi	19e44 <__gmpz_fib2_ui@@Base+0x7c>  // b.pmore
   19df0:	adrp	x22, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   19df4:	ldr	x22, [x22, #3808]
   19df8:	ldr	w8, [x21]
   19dfc:	add	x9, x22, x20, lsl #3
   19e00:	ldr	x23, [x9, #8]
   19e04:	cmp	w8, #0x0
   19e08:	b.le	19eb8 <__gmpz_fib2_ui@@Base+0xf0>
   19e0c:	ldr	x0, [x21, #8]
   19e10:	cmp	x20, #0x0
   19e14:	cset	w8, ne  // ne = any
   19e18:	str	x23, [x0]
   19e1c:	str	w8, [x21, #4]
   19e20:	ldr	w8, [x19]
   19e24:	ldr	x21, [x22, x20, lsl #3]
   19e28:	cmp	w8, #0x0
   19e2c:	b.le	19ec8 <__gmpz_fib2_ui@@Base+0x100>
   19e30:	ldr	x0, [x19, #8]
   19e34:	cmp	x20, #0x1
   19e38:	str	x21, [x0]
   19e3c:	cset	w8, ne  // ne = any
   19e40:	b	19ea0 <__gmpz_fib2_ui@@Base+0xd8>
   19e44:	lsr	x8, x20, #5
   19e48:	mov	w9, #0x17                  	// #23
   19e4c:	ldrsw	x10, [x21]
   19e50:	mul	x8, x8, x9
   19e54:	lsr	x8, x8, #6
   19e58:	add	x23, x8, #0x4
   19e5c:	cmp	x23, x10
   19e60:	b.gt	19ed8 <__gmpz_fib2_ui@@Base+0x110>
   19e64:	ldr	x22, [x21, #8]
   19e68:	ldrsw	x8, [x19]
   19e6c:	cmp	x23, x8
   19e70:	b.gt	19eec <__gmpz_fib2_ui@@Base+0x124>
   19e74:	ldr	x23, [x19, #8]
   19e78:	mov	x0, x22
   19e7c:	mov	x1, x23
   19e80:	mov	x2, x20
   19e84:	bl	d070 <__gmpn_fib2_ui@plt>
   19e88:	str	w0, [x21, #4]
   19e8c:	add	x8, x23, x0, lsl #3
   19e90:	ldur	x8, [x8, #-8]
   19e94:	cmp	x8, #0x0
   19e98:	cset	w8, eq  // eq = none
   19e9c:	sub	w8, w0, w8
   19ea0:	str	w8, [x19, #4]
   19ea4:	ldp	x20, x19, [sp, #48]
   19ea8:	ldp	x22, x21, [sp, #32]
   19eac:	ldr	x23, [sp, #16]
   19eb0:	ldp	x29, x30, [sp], #64
   19eb4:	ret
   19eb8:	mov	w1, #0x1                   	// #1
   19ebc:	mov	x0, x21
   19ec0:	bl	c080 <__gmpz_realloc@plt>
   19ec4:	b	19e10 <__gmpz_fib2_ui@@Base+0x48>
   19ec8:	mov	w1, #0x1                   	// #1
   19ecc:	mov	x0, x19
   19ed0:	bl	c080 <__gmpz_realloc@plt>
   19ed4:	b	19e34 <__gmpz_fib2_ui@@Base+0x6c>
   19ed8:	mov	x0, x21
   19edc:	mov	x1, x23
   19ee0:	bl	c080 <__gmpz_realloc@plt>
   19ee4:	mov	x22, x0
   19ee8:	b	19e68 <__gmpz_fib2_ui@@Base+0xa0>
   19eec:	mov	x0, x19
   19ef0:	mov	x1, x23
   19ef4:	bl	c080 <__gmpz_realloc@plt>
   19ef8:	mov	x23, x0
   19efc:	b	19e78 <__gmpz_fib2_ui@@Base+0xb0>

0000000000019f00 <__gmpz_fits_sint_p@@Base>:
   19f00:	ldr	x8, [x0, #8]
   19f04:	ldr	w9, [x0, #4]
   19f08:	ldr	x8, [x8]
   19f0c:	cmn	w9, #0x1
   19f10:	b.eq	19f30 <__gmpz_fits_sint_p@@Base+0x30>  // b.none
   19f14:	cbz	w9, 19f40 <__gmpz_fits_sint_p@@Base+0x40>
   19f18:	cmp	w9, #0x1
   19f1c:	b.ne	19f48 <__gmpz_fits_sint_p@@Base+0x48>  // b.any
   19f20:	lsr	x8, x8, #31
   19f24:	cmp	x8, #0x0
   19f28:	cset	w0, eq  // eq = none
   19f2c:	ret
   19f30:	mov	w9, #0x80000001            	// #-2147483647
   19f34:	cmp	x8, x9
   19f38:	cset	w0, cc  // cc = lo, ul, last
   19f3c:	ret
   19f40:	mov	w0, #0x1                   	// #1
   19f44:	ret
   19f48:	mov	w0, wzr
   19f4c:	ret

0000000000019f50 <__gmpz_fits_slong_p@@Base>:
   19f50:	ldr	x8, [x0, #8]
   19f54:	ldr	w9, [x0, #4]
   19f58:	ldr	x8, [x8]
   19f5c:	cmn	w9, #0x1
   19f60:	b.eq	19f7c <__gmpz_fits_slong_p@@Base+0x2c>  // b.none
   19f64:	cbz	w9, 19f8c <__gmpz_fits_slong_p@@Base+0x3c>
   19f68:	cmp	w9, #0x1
   19f6c:	b.ne	19f94 <__gmpz_fits_slong_p@@Base+0x44>  // b.any
   19f70:	lsr	x8, x8, #63
   19f74:	eor	w0, w8, #0x1
   19f78:	ret
   19f7c:	mov	x9, #0x8000000000000001    	// #-9223372036854775807
   19f80:	cmp	x8, x9
   19f84:	cset	w0, cc  // cc = lo, ul, last
   19f88:	ret
   19f8c:	mov	w0, #0x1                   	// #1
   19f90:	ret
   19f94:	mov	w0, wzr
   19f98:	ret

0000000000019f9c <__gmpz_fits_sshort_p@@Base>:
   19f9c:	ldr	x8, [x0, #8]
   19fa0:	ldr	w9, [x0, #4]
   19fa4:	ldr	x8, [x8]
   19fa8:	cmn	w9, #0x1
   19fac:	b.eq	19fc8 <__gmpz_fits_sshort_p@@Base+0x2c>  // b.none
   19fb0:	cbz	w9, 19fd4 <__gmpz_fits_sshort_p@@Base+0x38>
   19fb4:	cmp	w9, #0x1
   19fb8:	b.ne	19fdc <__gmpz_fits_sshort_p@@Base+0x40>  // b.any
   19fbc:	cmp	x8, #0x8, lsl #12
   19fc0:	cset	w0, cc  // cc = lo, ul, last
   19fc4:	ret
   19fc8:	cmp	x8, #0x8, lsl #12
   19fcc:	cset	w0, ls  // ls = plast
   19fd0:	ret
   19fd4:	mov	w0, #0x1                   	// #1
   19fd8:	ret
   19fdc:	mov	w0, wzr
   19fe0:	ret

0000000000019fe4 <__gmpz_fits_uint_p@@Base>:
   19fe4:	ldr	w8, [x0, #4]
   19fe8:	cbz	w8, 1a008 <__gmpz_fits_uint_p@@Base+0x24>
   19fec:	cmp	w8, #0x1
   19ff0:	b.ne	1a010 <__gmpz_fits_uint_p@@Base+0x2c>  // b.any
   19ff4:	ldr	x8, [x0, #8]
   19ff8:	ldr	w8, [x8, #4]
   19ffc:	cmp	w8, #0x0
   1a000:	cset	w0, eq  // eq = none
   1a004:	ret
   1a008:	mov	w0, #0x1                   	// #1
   1a00c:	ret
   1a010:	mov	w0, wzr
   1a014:	ret

000000000001a018 <__gmpz_fits_ulong_p@@Base>:
   1a018:	ldr	w8, [x0, #4]
   1a01c:	cmp	w8, #0x2
   1a020:	cset	w0, cc  // cc = lo, ul, last
   1a024:	ret

000000000001a028 <__gmpz_fits_ushort_p@@Base>:
   1a028:	ldr	w8, [x0, #4]
   1a02c:	cbz	w8, 1a04c <__gmpz_fits_ushort_p@@Base+0x24>
   1a030:	cmp	w8, #0x1
   1a034:	b.ne	1a054 <__gmpz_fits_ushort_p@@Base+0x2c>  // b.any
   1a038:	ldr	x8, [x0, #8]
   1a03c:	ldr	x8, [x8]
   1a040:	cmp	x8, #0x10, lsl #12
   1a044:	cset	w0, cc  // cc = lo, ul, last
   1a048:	ret
   1a04c:	mov	w0, #0x1                   	// #1
   1a050:	ret
   1a054:	mov	w0, wzr
   1a058:	ret

000000000001a05c <__gmpz_gcd@@Base>:
   1a05c:	stp	x29, x30, [sp, #-96]!
   1a060:	stp	x26, x25, [sp, #32]
   1a064:	stp	x24, x23, [sp, #48]
   1a068:	stp	x22, x21, [sp, #64]
   1a06c:	stp	x20, x19, [sp, #80]
   1a070:	ldr	w8, [x1, #4]
   1a074:	ldr	w9, [x2, #4]
   1a078:	ldr	x22, [x2, #8]
   1a07c:	mov	x19, x0
   1a080:	cmp	w8, #0x0
   1a084:	cneg	w21, w8, mi  // mi = first
   1a088:	cmp	w9, #0x0
   1a08c:	cneg	w20, w9, mi  // mi = first
   1a090:	str	x27, [sp, #16]
   1a094:	mov	x29, sp
   1a098:	cbz	w21, 1a0c4 <__gmpz_gcd@@Base+0x68>
   1a09c:	ldr	x23, [x1, #8]
   1a0a0:	cbz	w20, 1a0f0 <__gmpz_gcd@@Base+0x94>
   1a0a4:	cmp	w21, #0x1
   1a0a8:	b.ne	1a11c <__gmpz_gcd@@Base+0xc0>  // b.any
   1a0ac:	mov	w8, #0x1                   	// #1
   1a0b0:	str	w8, [x19, #4]
   1a0b4:	ldr	x2, [x23]
   1a0b8:	mov	x0, x22
   1a0bc:	mov	x1, x20
   1a0c0:	b	1a138 <__gmpz_gcd@@Base+0xdc>
   1a0c4:	cmp	x19, x2
   1a0c8:	str	w20, [x19, #4]
   1a0cc:	b.eq	1a37c <__gmpz_gcd@@Base+0x320>  // b.none
   1a0d0:	ldrsw	x8, [x19]
   1a0d4:	cmp	x20, x8
   1a0d8:	b.gt	1a3b0 <__gmpz_gcd@@Base+0x354>
   1a0dc:	ldr	x0, [x19, #8]
   1a0e0:	mov	x1, x22
   1a0e4:	mov	x2, x20
   1a0e8:	bl	ca50 <__gmpn_copyi@plt>
   1a0ec:	b	1a37c <__gmpz_gcd@@Base+0x320>
   1a0f0:	cmp	x19, x1
   1a0f4:	str	w21, [x19, #4]
   1a0f8:	b.eq	1a37c <__gmpz_gcd@@Base+0x320>  // b.none
   1a0fc:	ldrsw	x8, [x19]
   1a100:	cmp	x21, x8
   1a104:	b.gt	1a3c0 <__gmpz_gcd@@Base+0x364>
   1a108:	ldr	x0, [x19, #8]
   1a10c:	mov	x1, x23
   1a110:	mov	x2, x21
   1a114:	bl	ca50 <__gmpn_copyi@plt>
   1a118:	b	1a37c <__gmpz_gcd@@Base+0x320>
   1a11c:	cmp	w20, #0x1
   1a120:	b.ne	1a158 <__gmpz_gcd@@Base+0xfc>  // b.any
   1a124:	mov	w8, #0x1                   	// #1
   1a128:	str	w8, [x19, #4]
   1a12c:	ldr	x2, [x22]
   1a130:	mov	x0, x23
   1a134:	mov	x1, x21
   1a138:	bl	bf90 <__gmpn_gcd_1@plt>
   1a13c:	ldr	w8, [x19]
   1a140:	mov	x20, x0
   1a144:	cmp	w8, #0x0
   1a148:	b.le	1a39c <__gmpz_gcd@@Base+0x340>
   1a14c:	ldr	x8, [x19, #8]
   1a150:	str	x20, [x8]
   1a154:	b	1a37c <__gmpz_gcd@@Base+0x320>
   1a158:	sub	x25, x23, #0x8
   1a15c:	str	xzr, [x29, #24]
   1a160:	ldr	x8, [x25, #8]!
   1a164:	cbz	x8, 1a160 <__gmpz_gcd@@Base+0x104>
   1a168:	sub	x9, x25, x23
   1a16c:	asr	x27, x9, #3
   1a170:	sub	x21, x21, x27
   1a174:	rbit	x8, x8
   1a178:	lsl	x1, x21, #3
   1a17c:	mov	w9, #0x7f00                	// #32512
   1a180:	cmp	x1, x9
   1a184:	clz	x24, x8
   1a188:	b.hi	1a3d0 <__gmpz_gcd@@Base+0x374>  // b.pmore
   1a18c:	add	x9, x1, #0xf
   1a190:	mov	x8, sp
   1a194:	and	x9, x9, #0xfffffffffffffff0
   1a198:	sub	x23, x8, x9
   1a19c:	mov	sp, x23
   1a1a0:	mov	x0, x23
   1a1a4:	mov	x1, x25
   1a1a8:	mov	x2, x21
   1a1ac:	cbz	x24, 1a1d0 <__gmpz_gcd@@Base+0x174>
   1a1b0:	mov	w3, w24
   1a1b4:	bl	c1a0 <__gmpn_rshift@plt>
   1a1b8:	add	x8, x23, x21, lsl #3
   1a1bc:	ldur	x8, [x8, #-8]
   1a1c0:	cmp	x8, #0x0
   1a1c4:	cset	w8, eq  // eq = none
   1a1c8:	sub	x21, x21, x8
   1a1cc:	b	1a1d4 <__gmpz_gcd@@Base+0x178>
   1a1d0:	bl	ca50 <__gmpn_copyi@plt>
   1a1d4:	sub	x1, x22, #0x8
   1a1d8:	ldr	x8, [x1, #8]!
   1a1dc:	cbz	x8, 1a1d8 <__gmpz_gcd@@Base+0x17c>
   1a1e0:	sub	x9, x1, x22
   1a1e4:	asr	x26, x9, #3
   1a1e8:	sub	x25, x20, x26
   1a1ec:	rbit	x10, x8
   1a1f0:	lsl	x8, x25, #3
   1a1f4:	mov	w9, #0x7f00                	// #32512
   1a1f8:	cmp	x8, x9
   1a1fc:	clz	x22, x10
   1a200:	b.hi	1a3e0 <__gmpz_gcd@@Base+0x384>  // b.pmore
   1a204:	add	x8, x8, #0xf
   1a208:	mov	x9, sp
   1a20c:	and	x8, x8, #0xfffffffffffffff0
   1a210:	sub	x20, x9, x8
   1a214:	mov	sp, x20
   1a218:	mov	x0, x20
   1a21c:	mov	x2, x25
   1a220:	cbz	x22, 1a24c <__gmpz_gcd@@Base+0x1f0>
   1a224:	mov	w3, w22
   1a228:	bl	c1a0 <__gmpn_rshift@plt>
   1a22c:	add	x8, x20, x25, lsl #3
   1a230:	ldur	x8, [x8, #-8]
   1a234:	cmp	x8, #0x0
   1a238:	cset	w8, eq  // eq = none
   1a23c:	sub	x25, x25, x8
   1a240:	cmp	x27, x26
   1a244:	b.le	1a258 <__gmpz_gcd@@Base+0x1fc>
   1a248:	b	1a264 <__gmpz_gcd@@Base+0x208>
   1a24c:	bl	ca50 <__gmpn_copyi@plt>
   1a250:	cmp	x27, x26
   1a254:	b.gt	1a264 <__gmpz_gcd@@Base+0x208>
   1a258:	b.ge	1a270 <__gmpz_gcd@@Base+0x214>  // b.tcont
   1a25c:	mov	x26, x27
   1a260:	mov	x22, x24
   1a264:	cmp	x21, x25
   1a268:	b.ge	1a284 <__gmpz_gcd@@Base+0x228>  // b.tcont
   1a26c:	b	1a2a0 <__gmpz_gcd@@Base+0x244>
   1a270:	cmp	x24, x22
   1a274:	csel	x22, x24, x22, cc  // cc = lo, ul, last
   1a278:	mov	x26, x27
   1a27c:	cmp	x21, x25
   1a280:	b.lt	1a2a0 <__gmpz_gcd@@Base+0x244>  // b.tstop
   1a284:	b.ne	1a2b8 <__gmpz_gcd@@Base+0x25c>  // b.any
   1a288:	lsl	x8, x21, #3
   1a28c:	sub	x8, x8, #0x8
   1a290:	ldr	x9, [x23, x8]
   1a294:	ldr	x8, [x20, x8]
   1a298:	cmp	x9, x8
   1a29c:	b.cs	1a2b8 <__gmpz_gcd@@Base+0x25c>  // b.hs, b.nlast
   1a2a0:	mov	x0, x20
   1a2a4:	mov	x1, x20
   1a2a8:	mov	x2, x25
   1a2ac:	mov	x3, x23
   1a2b0:	mov	x4, x21
   1a2b4:	b	1a2cc <__gmpz_gcd@@Base+0x270>
   1a2b8:	mov	x0, x20
   1a2bc:	mov	x1, x23
   1a2c0:	mov	x2, x21
   1a2c4:	mov	x3, x20
   1a2c8:	mov	x4, x25
   1a2cc:	bl	cc10 <__gmpn_gcd@plt>
   1a2d0:	mov	x23, x0
   1a2d4:	add	x21, x0, x26
   1a2d8:	cbz	x22, 1a33c <__gmpz_gcd@@Base+0x2e0>
   1a2dc:	add	x8, x20, x23, lsl #3
   1a2e0:	ldur	x8, [x8, #-8]
   1a2e4:	neg	x9, x22
   1a2e8:	ldrsw	x10, [x19]
   1a2ec:	lsr	x8, x8, x9
   1a2f0:	cmp	x8, #0x0
   1a2f4:	cinc	x21, x21, ne  // ne = any
   1a2f8:	cmp	x21, x10
   1a2fc:	b.gt	1a404 <__gmpz_gcd@@Base+0x3a8>
   1a300:	ldr	x24, [x19, #8]
   1a304:	cbz	x26, 1a318 <__gmpz_gcd@@Base+0x2bc>
   1a308:	lsl	x2, x26, #3
   1a30c:	mov	x0, x24
   1a310:	mov	w1, wzr
   1a314:	bl	c5f0 <memset@plt>
   1a318:	add	x24, x24, x26, lsl #3
   1a31c:	mov	x0, x24
   1a320:	mov	x1, x20
   1a324:	mov	x2, x23
   1a328:	mov	w3, w22
   1a32c:	bl	c180 <__gmpn_lshift@plt>
   1a330:	cbz	x0, 1a370 <__gmpz_gcd@@Base+0x314>
   1a334:	str	x0, [x24, x23, lsl #3]
   1a338:	b	1a370 <__gmpz_gcd@@Base+0x314>
   1a33c:	ldrsw	x8, [x19]
   1a340:	cmp	x21, x8
   1a344:	b.gt	1a41c <__gmpz_gcd@@Base+0x3c0>
   1a348:	ldr	x22, [x19, #8]
   1a34c:	cbz	x26, 1a360 <__gmpz_gcd@@Base+0x304>
   1a350:	lsl	x2, x26, #3
   1a354:	mov	x0, x22
   1a358:	mov	w1, wzr
   1a35c:	bl	c5f0 <memset@plt>
   1a360:	add	x0, x22, x26, lsl #3
   1a364:	mov	x1, x20
   1a368:	mov	x2, x23
   1a36c:	bl	ca50 <__gmpn_copyi@plt>
   1a370:	str	w21, [x19, #4]
   1a374:	ldr	x0, [x29, #24]
   1a378:	cbnz	x0, 1a3fc <__gmpz_gcd@@Base+0x3a0>
   1a37c:	mov	sp, x29
   1a380:	ldp	x20, x19, [sp, #80]
   1a384:	ldp	x22, x21, [sp, #64]
   1a388:	ldp	x24, x23, [sp, #48]
   1a38c:	ldp	x26, x25, [sp, #32]
   1a390:	ldr	x27, [sp, #16]
   1a394:	ldp	x29, x30, [sp], #96
   1a398:	ret
   1a39c:	mov	w1, #0x1                   	// #1
   1a3a0:	mov	x0, x19
   1a3a4:	bl	c080 <__gmpz_realloc@plt>
   1a3a8:	str	x20, [x0]
   1a3ac:	b	1a37c <__gmpz_gcd@@Base+0x320>
   1a3b0:	mov	x0, x19
   1a3b4:	mov	x1, x20
   1a3b8:	bl	c080 <__gmpz_realloc@plt>
   1a3bc:	b	1a0e0 <__gmpz_gcd@@Base+0x84>
   1a3c0:	mov	x0, x19
   1a3c4:	mov	x1, x21
   1a3c8:	bl	c080 <__gmpz_realloc@plt>
   1a3cc:	b	1a10c <__gmpz_gcd@@Base+0xb0>
   1a3d0:	add	x0, x29, #0x18
   1a3d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a3d8:	mov	x23, x0
   1a3dc:	b	1a1a0 <__gmpz_gcd@@Base+0x144>
   1a3e0:	add	x0, x29, #0x18
   1a3e4:	mov	x20, x1
   1a3e8:	mov	x1, x8
   1a3ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a3f0:	mov	x1, x20
   1a3f4:	mov	x20, x0
   1a3f8:	b	1a218 <__gmpz_gcd@@Base+0x1bc>
   1a3fc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1a400:	b	1a37c <__gmpz_gcd@@Base+0x320>
   1a404:	mov	x0, x19
   1a408:	mov	x1, x21
   1a40c:	bl	c080 <__gmpz_realloc@plt>
   1a410:	mov	x24, x0
   1a414:	cbnz	x26, 1a308 <__gmpz_gcd@@Base+0x2ac>
   1a418:	b	1a318 <__gmpz_gcd@@Base+0x2bc>
   1a41c:	mov	x0, x19
   1a420:	mov	x1, x21
   1a424:	bl	c080 <__gmpz_realloc@plt>
   1a428:	mov	x22, x0
   1a42c:	cbnz	x26, 1a350 <__gmpz_gcd@@Base+0x2f4>
   1a430:	b	1a360 <__gmpz_gcd@@Base+0x304>

000000000001a434 <__gmpz_gcd_ui@@Base>:
   1a434:	stp	x29, x30, [sp, #-48]!
   1a438:	stp	x22, x21, [sp, #16]
   1a43c:	stp	x20, x19, [sp, #32]
   1a440:	ldr	w8, [x1, #4]
   1a444:	mov	x20, x2
   1a448:	mov	x19, x0
   1a44c:	mov	x29, sp
   1a450:	cmp	w8, #0x0
   1a454:	cneg	w21, w8, mi  // mi = first
   1a458:	cbz	w21, 1a478 <__gmpz_gcd_ui@@Base+0x44>
   1a45c:	mov	x22, x1
   1a460:	cbz	x20, 1a4a0 <__gmpz_gcd_ui@@Base+0x6c>
   1a464:	ldr	x0, [x22, #8]
   1a468:	mov	x1, x21
   1a46c:	mov	x2, x20
   1a470:	bl	bf90 <__gmpn_gcd_1@plt>
   1a474:	mov	x20, x0
   1a478:	cbz	x19, 1a4dc <__gmpz_gcd_ui@@Base+0xa8>
   1a47c:	ldr	w8, [x19]
   1a480:	cmp	w8, #0x0
   1a484:	b.le	1a4f0 <__gmpz_gcd_ui@@Base+0xbc>
   1a488:	ldr	x0, [x19, #8]
   1a48c:	cmp	x20, #0x0
   1a490:	cset	w8, ne  // ne = any
   1a494:	str	x20, [x0]
   1a498:	str	w8, [x19, #4]
   1a49c:	b	1a4dc <__gmpz_gcd_ui@@Base+0xa8>
   1a4a0:	cbz	x19, 1a4cc <__gmpz_gcd_ui@@Base+0x98>
   1a4a4:	cmp	x22, x19
   1a4a8:	b.eq	1a4c8 <__gmpz_gcd_ui@@Base+0x94>  // b.none
   1a4ac:	ldrsw	x8, [x19]
   1a4b0:	cmp	x21, x8
   1a4b4:	b.gt	1a500 <__gmpz_gcd_ui@@Base+0xcc>
   1a4b8:	ldr	x0, [x19, #8]
   1a4bc:	ldr	x1, [x22, #8]
   1a4c0:	mov	x2, x21
   1a4c4:	bl	ca50 <__gmpn_copyi@plt>
   1a4c8:	str	w21, [x19, #4]
   1a4cc:	ldr	x8, [x22, #8]
   1a4d0:	cmp	w21, #0x1
   1a4d4:	ldr	x8, [x8]
   1a4d8:	csel	x20, x8, xzr, eq  // eq = none
   1a4dc:	mov	x0, x20
   1a4e0:	ldp	x20, x19, [sp, #32]
   1a4e4:	ldp	x22, x21, [sp, #16]
   1a4e8:	ldp	x29, x30, [sp], #48
   1a4ec:	ret
   1a4f0:	mov	w1, #0x1                   	// #1
   1a4f4:	mov	x0, x19
   1a4f8:	bl	c080 <__gmpz_realloc@plt>
   1a4fc:	b	1a48c <__gmpz_gcd_ui@@Base+0x58>
   1a500:	mov	x0, x19
   1a504:	mov	x1, x21
   1a508:	bl	c080 <__gmpz_realloc@plt>
   1a50c:	b	1a4b8 <__gmpz_gcd_ui@@Base+0x84>

000000000001a510 <__gmpz_gcdext@@Base>:
   1a510:	stp	x29, x30, [sp, #-96]!
   1a514:	stp	x28, x27, [sp, #16]
   1a518:	stp	x26, x25, [sp, #32]
   1a51c:	stp	x24, x23, [sp, #48]
   1a520:	stp	x22, x21, [sp, #64]
   1a524:	stp	x20, x19, [sp, #80]
   1a528:	mov	x29, sp
   1a52c:	sub	sp, sp, #0x50
   1a530:	ldr	w8, [x3, #4]
   1a534:	ldr	w9, [x4, #4]
   1a538:	mov	x19, x0
   1a53c:	cmp	w8, #0x0
   1a540:	cneg	w8, w8, mi  // mi = first
   1a544:	cmp	w9, #0x0
   1a548:	cneg	w9, w9, mi  // mi = first
   1a54c:	cmp	w8, w9
   1a550:	csel	w26, w8, w9, cc  // cc = lo, ul, last
   1a554:	csel	w23, w9, w8, cc  // cc = lo, ul, last
   1a558:	csel	x21, x3, x4, cc  // cc = lo, ul, last
   1a55c:	csel	x24, x4, x3, cc  // cc = lo, ul, last
   1a560:	csel	x25, x1, x2, cc  // cc = lo, ul, last
   1a564:	csel	x20, x2, x1, cc  // cc = lo, ul, last
   1a568:	cbz	w26, 1a6cc <__gmpz_gcdext@@Base+0x1bc>
   1a56c:	add	x8, x26, x26, lsl #1
   1a570:	add	x8, x23, x8
   1a574:	add	x8, x8, #0x1
   1a578:	cmp	x8, #0xfe0
   1a57c:	lsl	x1, x8, #3
   1a580:	stur	xzr, [x29, #-16]
   1a584:	stur	x25, [x29, #-72]
   1a588:	b.hi	1a74c <__gmpz_gcdext@@Base+0x23c>  // b.pmore
   1a58c:	add	x9, x1, #0xf
   1a590:	mov	x8, sp
   1a594:	and	x9, x9, #0xfffffffff0
   1a598:	sub	x22, x8, x9
   1a59c:	mov	sp, x22
   1a5a0:	lsl	x8, x26, #3
   1a5a4:	add	x27, x22, x8
   1a5a8:	ldr	x1, [x24, #8]
   1a5ac:	add	x9, x27, x8
   1a5b0:	add	x28, x9, #0x8
   1a5b4:	add	x25, x28, x8
   1a5b8:	mov	x0, x25
   1a5bc:	mov	x2, x23
   1a5c0:	bl	ca50 <__gmpn_copyi@plt>
   1a5c4:	ldr	x1, [x21, #8]
   1a5c8:	mov	x0, x28
   1a5cc:	mov	x2, x26
   1a5d0:	bl	ca50 <__gmpn_copyi@plt>
   1a5d4:	sub	x2, x29, #0x8
   1a5d8:	mov	x0, x22
   1a5dc:	mov	x1, x27
   1a5e0:	mov	x3, x25
   1a5e4:	mov	x4, x23
   1a5e8:	mov	x5, x28
   1a5ec:	mov	x6, x26
   1a5f0:	bl	c560 <__gmpn_gcdext@plt>
   1a5f4:	ldur	x8, [x29, #-8]
   1a5f8:	ldr	w9, [x24, #4]
   1a5fc:	ldur	x25, [x29, #-72]
   1a600:	mov	x26, x0
   1a604:	cmp	x8, #0x0
   1a608:	cneg	x28, x8, mi  // mi = first
   1a60c:	cmp	w9, #0x0
   1a610:	cneg	x8, x8, lt  // lt = tstop
   1a614:	stur	x8, [x29, #-8]
   1a618:	cbz	x25, 1a670 <__gmpz_gcdext@@Base+0x160>
   1a61c:	stur	w8, [x29, #-60]
   1a620:	add	x8, x27, x28, lsl #3
   1a624:	add	w9, w23, w28
   1a628:	stur	x8, [x29, #-24]
   1a62c:	add	w8, w9, #0x1
   1a630:	sub	x0, x29, #0x20
   1a634:	sub	x1, x29, #0x40
   1a638:	mov	x2, x24
   1a63c:	stur	x22, [x29, #-40]
   1a640:	stur	w26, [x29, #-44]
   1a644:	stur	x27, [x29, #-56]
   1a648:	stur	w8, [x29, #-32]
   1a64c:	bl	c4b0 <__gmpz_mul@plt>
   1a650:	sub	x0, x29, #0x20
   1a654:	sub	x1, x29, #0x30
   1a658:	sub	x2, x29, #0x20
   1a65c:	bl	c260 <__gmpz_sub@plt>
   1a660:	sub	x1, x29, #0x20
   1a664:	mov	x0, x25
   1a668:	mov	x2, x21
   1a66c:	bl	c3f0 <__gmpz_divexact@plt>
   1a670:	cbz	x20, 1a698 <__gmpz_gcdext@@Base+0x188>
   1a674:	ldrsw	x8, [x20]
   1a678:	cmp	x28, x8
   1a67c:	b.gt	1a75c <__gmpz_gcdext@@Base+0x24c>
   1a680:	ldr	x0, [x20, #8]
   1a684:	mov	x1, x27
   1a688:	mov	x2, x28
   1a68c:	bl	ca50 <__gmpn_copyi@plt>
   1a690:	ldur	x8, [x29, #-8]
   1a694:	str	w8, [x20, #4]
   1a698:	cbz	x19, 1a6bc <__gmpz_gcdext@@Base+0x1ac>
   1a69c:	ldrsw	x8, [x19]
   1a6a0:	cmp	x26, x8
   1a6a4:	b.gt	1a76c <__gmpz_gcdext@@Base+0x25c>
   1a6a8:	ldr	x0, [x19, #8]
   1a6ac:	mov	x1, x22
   1a6b0:	mov	x2, x26
   1a6b4:	bl	ca50 <__gmpn_copyi@plt>
   1a6b8:	str	w26, [x19, #4]
   1a6bc:	ldur	x0, [x29, #-16]
   1a6c0:	cbz	x0, 1a72c <__gmpz_gcdext@@Base+0x21c>
   1a6c4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1a6c8:	b	1a72c <__gmpz_gcdext@@Base+0x21c>
   1a6cc:	ldr	w8, [x24, #4]
   1a6d0:	cmp	w23, #0x0
   1a6d4:	cset	w9, ne  // ne = any
   1a6d8:	cmp	w8, #0x0
   1a6dc:	csinv	w22, w9, wzr, ge  // ge = tcont
   1a6e0:	cbz	x19, 1a704 <__gmpz_gcdext@@Base+0x1f4>
   1a6e4:	ldrsw	x8, [x19]
   1a6e8:	cmp	x23, x8
   1a6ec:	b.gt	1a77c <__gmpz_gcdext@@Base+0x26c>
   1a6f0:	ldr	x0, [x19, #8]
   1a6f4:	ldr	x1, [x24, #8]
   1a6f8:	mov	x2, x23
   1a6fc:	bl	ca50 <__gmpn_copyi@plt>
   1a700:	str	w23, [x19, #4]
   1a704:	cbz	x25, 1a70c <__gmpz_gcdext@@Base+0x1fc>
   1a708:	str	wzr, [x25, #4]
   1a70c:	cbz	x20, 1a72c <__gmpz_gcdext@@Base+0x21c>
   1a710:	ldr	w8, [x20]
   1a714:	str	w22, [x20, #4]
   1a718:	cmp	w8, #0x0
   1a71c:	b.le	1a78c <__gmpz_gcdext@@Base+0x27c>
   1a720:	ldr	x0, [x20, #8]
   1a724:	mov	w8, #0x1                   	// #1
   1a728:	str	x8, [x0]
   1a72c:	mov	sp, x29
   1a730:	ldp	x20, x19, [sp, #80]
   1a734:	ldp	x22, x21, [sp, #64]
   1a738:	ldp	x24, x23, [sp, #48]
   1a73c:	ldp	x26, x25, [sp, #32]
   1a740:	ldp	x28, x27, [sp, #16]
   1a744:	ldp	x29, x30, [sp], #96
   1a748:	ret
   1a74c:	sub	x0, x29, #0x10
   1a750:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1a754:	mov	x22, x0
   1a758:	b	1a5a0 <__gmpz_gcdext@@Base+0x90>
   1a75c:	mov	x0, x20
   1a760:	mov	x1, x28
   1a764:	bl	c080 <__gmpz_realloc@plt>
   1a768:	b	1a684 <__gmpz_gcdext@@Base+0x174>
   1a76c:	mov	x0, x19
   1a770:	mov	x1, x26
   1a774:	bl	c080 <__gmpz_realloc@plt>
   1a778:	b	1a6ac <__gmpz_gcdext@@Base+0x19c>
   1a77c:	mov	x0, x19
   1a780:	mov	x1, x23
   1a784:	bl	c080 <__gmpz_realloc@plt>
   1a788:	b	1a6f4 <__gmpz_gcdext@@Base+0x1e4>
   1a78c:	mov	w1, #0x1                   	// #1
   1a790:	mov	x0, x20
   1a794:	bl	c080 <__gmpz_realloc@plt>
   1a798:	b	1a724 <__gmpz_gcdext@@Base+0x214>

000000000001a79c <__gmpz_get_d@@Base>:
   1a79c:	ldrsw	x2, [x0, #4]
   1a7a0:	cbz	w2, 1a7b8 <__gmpz_get_d@@Base+0x1c>
   1a7a4:	ldr	x0, [x0, #8]
   1a7a8:	cmp	x2, #0x0
   1a7ac:	cneg	x1, x2, mi  // mi = first
   1a7b0:	mov	x3, xzr
   1a7b4:	b	bf40 <__gmpn_get_d@plt>
   1a7b8:	fmov	d0, xzr
   1a7bc:	ret

000000000001a7c0 <__gmpz_get_d_2exp@@Base>:
   1a7c0:	ldrsw	x2, [x1, #4]
   1a7c4:	cbz	w2, 1a7f8 <__gmpz_get_d_2exp@@Base+0x38>
   1a7c8:	ldr	x8, [x1, #8]
   1a7cc:	cmp	x2, #0x0
   1a7d0:	cneg	x1, x2, mi  // mi = first
   1a7d4:	lsl	x10, x1, #6
   1a7d8:	add	x9, x8, x1, lsl #3
   1a7dc:	ldur	x9, [x9, #-8]
   1a7e0:	clz	x9, x9
   1a7e4:	sub	x9, x10, x9
   1a7e8:	neg	x3, x9
   1a7ec:	str	x9, [x0]
   1a7f0:	mov	x0, x8
   1a7f4:	b	bf40 <__gmpn_get_d@plt>
   1a7f8:	fmov	d0, xzr
   1a7fc:	str	xzr, [x0]
   1a800:	ret

000000000001a804 <__gmpz_get_si@@Base>:
   1a804:	ldr	x8, [x0, #8]
   1a808:	ldr	w9, [x0, #4]
   1a80c:	ldr	x8, [x8]
   1a810:	cmp	w9, #0x1
   1a814:	b.lt	1a820 <__gmpz_get_si@@Base+0x1c>  // b.tstop
   1a818:	and	x0, x8, #0x7fffffffffffffff
   1a81c:	ret
   1a820:	tbnz	w9, #31, 1a82c <__gmpz_get_si@@Base+0x28>
   1a824:	mov	x0, xzr
   1a828:	ret
   1a82c:	sub	x8, x8, #0x1
   1a830:	orr	x8, x8, #0x8000000000000000
   1a834:	eor	x0, x8, #0x7fffffffffffffff
   1a838:	ret

000000000001a83c <__gmpz_get_str@@Base>:
   1a83c:	stp	x29, x30, [sp, #-80]!
   1a840:	stp	x24, x23, [sp, #32]
   1a844:	stp	x22, x21, [sp, #48]
   1a848:	stp	x20, x19, [sp, #64]
   1a84c:	ldrsw	x20, [x2, #4]
   1a850:	mov	x22, x2
   1a854:	mov	w21, w1
   1a858:	cmp	w1, #0x2
   1a85c:	mov	x19, x0
   1a860:	str	x25, [sp, #16]
   1a864:	mov	x29, sp
   1a868:	b.lt	1a8d8 <__gmpz_get_str@@Base+0x9c>  // b.tstop
   1a86c:	cmp	w21, #0x25
   1a870:	b.ge	1a8e8 <__gmpz_get_str@@Base+0xac>  // b.tcont
   1a874:	adrp	x25, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1a878:	add	x25, x25, #0xbd
   1a87c:	cbnz	x19, 1a90c <__gmpz_get_str@@Base+0xd0>
   1a880:	cmp	x20, #0x0
   1a884:	cneg	x8, x20, mi  // mi = first
   1a888:	cbz	x8, 1aa0c <__gmpz_get_str@@Base+0x1d0>
   1a88c:	ldr	x9, [x22, #8]
   1a890:	adrp	x11, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1a894:	sub	w10, w21, #0x1
   1a898:	tst	w21, w10
   1a89c:	add	x9, x9, x8, lsl #3
   1a8a0:	ldur	x9, [x9, #-8]
   1a8a4:	ldr	x11, [x11, #3936]
   1a8a8:	lsl	x8, x8, #6
   1a8ac:	mov	w10, #0x28                  	// #40
   1a8b0:	clz	x9, x9
   1a8b4:	sub	x8, x8, x9
   1a8b8:	mov	w9, w21
   1a8bc:	madd	x9, x9, x10, x11
   1a8c0:	b.ne	1aa14 <__gmpz_get_str@@Base+0x1d8>  // b.any
   1a8c4:	ldrsw	x9, [x9, #24]
   1a8c8:	add	x8, x8, x9
   1a8cc:	sub	x8, x8, #0x1
   1a8d0:	udiv	x8, x8, x9
   1a8d4:	b	1aa24 <__gmpz_get_str@@Base+0x1e8>
   1a8d8:	cmn	w21, #0x2
   1a8dc:	b.le	1a8f4 <__gmpz_get_str@@Base+0xb8>
   1a8e0:	mov	w21, #0xa                   	// #10
   1a8e4:	b	1a900 <__gmpz_get_str@@Base+0xc4>
   1a8e8:	cmp	w21, #0x3e
   1a8ec:	b.le	1a900 <__gmpz_get_str@@Base+0xc4>
   1a8f0:	b	1aa60 <__gmpz_get_str@@Base+0x224>
   1a8f4:	cmn	w21, #0x24
   1a8f8:	b.lt	1aa60 <__gmpz_get_str@@Base+0x224>  // b.tstop
   1a8fc:	neg	w21, w21
   1a900:	adrp	x25, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1a904:	add	x25, x25, #0x7e
   1a908:	cbz	x19, 1a880 <__gmpz_get_str@@Base+0x44>
   1a90c:	mov	x23, xzr
   1a910:	mov	x24, x19
   1a914:	tbz	w20, #31, 1a928 <__gmpz_get_str@@Base+0xec>
   1a918:	mov	w8, #0x2d                  	// #45
   1a91c:	mov	x24, x19
   1a920:	strb	w8, [x24], #1
   1a924:	neg	x20, x20
   1a928:	str	xzr, [x29, #24]
   1a92c:	ldr	x2, [x22, #8]
   1a930:	sub	w8, w21, #0x1
   1a934:	tst	w21, w8
   1a938:	b.eq	1a978 <__gmpz_get_str@@Base+0x13c>  // b.none
   1a93c:	lsl	x8, x20, #3
   1a940:	orr	x1, x8, #0x8
   1a944:	mov	w8, #0x7f00                	// #32512
   1a948:	cmp	x1, x8
   1a94c:	b.hi	1aa68 <__gmpz_get_str@@Base+0x22c>  // b.pmore
   1a950:	add	x9, x1, #0xf
   1a954:	mov	x8, sp
   1a958:	and	x9, x9, #0xfffffffffffffff0
   1a95c:	sub	x22, x8, x9
   1a960:	mov	sp, x22
   1a964:	mov	x0, x22
   1a968:	mov	x1, x2
   1a96c:	mov	x2, x20
   1a970:	bl	ca50 <__gmpn_copyi@plt>
   1a974:	mov	x2, x22
   1a978:	mov	x0, x24
   1a97c:	mov	w1, w21
   1a980:	mov	x3, x20
   1a984:	bl	ca90 <__gmpn_get_str@plt>
   1a988:	mov	x20, x0
   1a98c:	cbz	x0, 1a9ac <__gmpz_get_str@@Base+0x170>
   1a990:	mov	x8, x20
   1a994:	mov	x9, x24
   1a998:	ldrb	w10, [x9]
   1a99c:	subs	x8, x8, #0x1
   1a9a0:	ldrb	w10, [x25, x10]
   1a9a4:	strb	w10, [x9], #1
   1a9a8:	b.ne	1a998 <__gmpz_get_str@@Base+0x15c>  // b.any
   1a9ac:	strb	wzr, [x24, x20]
   1a9b0:	ldr	x0, [x29, #24]
   1a9b4:	cbnz	x0, 1aa54 <__gmpz_get_str@@Base+0x218>
   1a9b8:	cbz	x23, 1a9ec <__gmpz_get_str@@Base+0x1b0>
   1a9bc:	sub	x8, x24, x19
   1a9c0:	add	x8, x8, x20
   1a9c4:	add	x2, x8, #0x1
   1a9c8:	cmp	x23, x2
   1a9cc:	b.eq	1a9ec <__gmpz_get_str@@Base+0x1b0>  // b.none
   1a9d0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1a9d4:	ldr	x8, [x8, #3792]
   1a9d8:	mov	x0, x19
   1a9dc:	mov	x1, x23
   1a9e0:	ldr	x8, [x8]
   1a9e4:	blr	x8
   1a9e8:	mov	x19, x0
   1a9ec:	mov	x0, x19
   1a9f0:	mov	sp, x29
   1a9f4:	ldp	x20, x19, [sp, #64]
   1a9f8:	ldp	x22, x21, [sp, #48]
   1a9fc:	ldp	x24, x23, [sp, #32]
   1aa00:	ldr	x25, [sp, #16]
   1aa04:	ldp	x29, x30, [sp], #80
   1aa08:	ret
   1aa0c:	mov	w8, #0x1                   	// #1
   1aa10:	b	1aa24 <__gmpz_get_str@@Base+0x1e8>
   1aa14:	ldr	x9, [x9, #8]
   1aa18:	add	x9, x9, #0x1
   1aa1c:	umulh	x8, x9, x8
   1aa20:	add	x8, x8, #0x1
   1aa24:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1aa28:	ldr	x9, [x9, #3840]
   1aa2c:	ubfx	x10, x20, #31, #1
   1aa30:	add	w10, w10, #0x1
   1aa34:	add	x23, x8, x10
   1aa38:	ldr	x9, [x9]
   1aa3c:	mov	x0, x23
   1aa40:	blr	x9
   1aa44:	mov	x19, x0
   1aa48:	mov	x24, x19
   1aa4c:	tbz	w20, #31, 1a928 <__gmpz_get_str@@Base+0xec>
   1aa50:	b	1a918 <__gmpz_get_str@@Base+0xdc>
   1aa54:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1aa58:	cbnz	x23, 1a9bc <__gmpz_get_str@@Base+0x180>
   1aa5c:	b	1a9ec <__gmpz_get_str@@Base+0x1b0>
   1aa60:	mov	x19, xzr
   1aa64:	b	1a9ec <__gmpz_get_str@@Base+0x1b0>
   1aa68:	add	x0, x29, #0x18
   1aa6c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1aa70:	ldr	x2, [x22, #8]
   1aa74:	mov	x22, x0
   1aa78:	b	1a964 <__gmpz_get_str@@Base+0x128>

000000000001aa7c <__gmpz_get_ui@@Base>:
   1aa7c:	ldr	x8, [x0, #8]
   1aa80:	ldr	w9, [x0, #4]
   1aa84:	ldr	x8, [x8]
   1aa88:	cmp	w9, #0x0
   1aa8c:	csel	x0, xzr, x8, eq  // eq = none
   1aa90:	ret

000000000001aa94 <__gmpz_getlimbn@@Base>:
   1aa94:	tbnz	x1, #63, 1aab8 <__gmpz_getlimbn@@Base+0x24>
   1aa98:	ldr	w8, [x0, #4]
   1aa9c:	cmp	w8, #0x0
   1aaa0:	cneg	w8, w8, mi  // mi = first
   1aaa4:	cmp	x8, x1
   1aaa8:	b.le	1aab8 <__gmpz_getlimbn@@Base+0x24>
   1aaac:	ldr	x8, [x0, #8]
   1aab0:	ldr	x0, [x8, x1, lsl #3]
   1aab4:	ret
   1aab8:	mov	x0, xzr
   1aabc:	ret

000000000001aac0 <__gmpz_hamdist@@Base>:
   1aac0:	stp	x29, x30, [sp, #-80]!
   1aac4:	stp	x24, x23, [sp, #32]
   1aac8:	stp	x22, x21, [sp, #48]
   1aacc:	stp	x20, x19, [sp, #64]
   1aad0:	ldrsw	x12, [x0, #4]
   1aad4:	ldrsw	x9, [x1, #4]
   1aad8:	ldr	x8, [x0, #8]
   1aadc:	ldr	x10, [x1, #8]
   1aae0:	str	x25, [sp, #16]
   1aae4:	mov	x29, sp
   1aae8:	tbnz	w12, #31, 1ab30 <__gmpz_hamdist@@Base+0x70>
   1aaec:	tbnz	w9, #31, 1ab34 <__gmpz_hamdist@@Base+0x74>
   1aaf0:	cmp	w12, w9
   1aaf4:	csel	w11, w12, w9, lt  // lt = tstop
   1aaf8:	csel	w13, w12, w9, gt
   1aafc:	sxtw	x19, w11
   1ab00:	sxtw	x21, w13
   1ab04:	csel	x20, x10, x8, lt  // lt = tstop
   1ab08:	cbz	w11, 1ab3c <__gmpz_hamdist@@Base+0x7c>
   1ab0c:	cmp	w12, w9
   1ab10:	csel	x1, x8, x10, lt  // lt = tstop
   1ab14:	mov	x0, x20
   1ab18:	mov	x2, x19
   1ab1c:	bl	ce40 <__gmpn_hamdist@plt>
   1ab20:	mov	x22, x0
   1ab24:	subs	x1, x21, x19
   1ab28:	b.ne	1ab48 <__gmpz_hamdist@@Base+0x88>  // b.any
   1ab2c:	b	1ace8 <__gmpz_hamdist@@Base+0x228>
   1ab30:	tbnz	w9, #31, 1ab50 <__gmpz_hamdist@@Base+0x90>
   1ab34:	mov	x22, #0xffffffffffffffff    	// #-1
   1ab38:	b	1ace8 <__gmpz_hamdist@@Base+0x228>
   1ab3c:	mov	x22, xzr
   1ab40:	subs	x1, x21, x19
   1ab44:	b.eq	1ace8 <__gmpz_hamdist@@Base+0x228>  // b.none
   1ab48:	add	x0, x20, x19, lsl #3
   1ab4c:	b	1ace0 <__gmpz_hamdist@@Base+0x220>
   1ab50:	mov	x11, xzr
   1ab54:	neg	x24, x12
   1ab58:	neg	x19, x9
   1ab5c:	ldr	x12, [x8, x11]
   1ab60:	ldr	x9, [x10, x11]
   1ab64:	sub	x24, x24, #0x1
   1ab68:	sub	x19, x19, #0x1
   1ab6c:	cbnz	x12, 1ab8c <__gmpz_hamdist@@Base+0xcc>
   1ab70:	add	x11, x11, #0x8
   1ab74:	cbz	x9, 1ab5c <__gmpz_hamdist@@Base+0x9c>
   1ab78:	add	x20, x10, x11
   1ab7c:	add	x21, x8, x11
   1ab80:	mov	x12, x9
   1ab84:	mov	x9, xzr
   1ab88:	b	1aba8 <__gmpz_hamdist@@Base+0xe8>
   1ab8c:	mov	x0, x24
   1ab90:	add	x10, x10, x11
   1ab94:	add	x8, x8, x11
   1ab98:	add	x21, x10, #0x8
   1ab9c:	add	x20, x8, #0x8
   1aba0:	mov	x24, x19
   1aba4:	mov	x19, x0
   1aba8:	neg	x8, x12
   1abac:	neg	x10, x9
   1abb0:	eor	x8, x10, x8
   1abb4:	lsr	x10, x8, #1
   1abb8:	and	x10, x10, #0x5555555555555555
   1abbc:	sub	x8, x8, x10
   1abc0:	lsr	x10, x8, #2
   1abc4:	and	x10, x10, #0x3333333333333333
   1abc8:	and	x8, x8, #0x3333333333333333
   1abcc:	add	x8, x10, x8
   1abd0:	add	x8, x8, x8, lsr #4
   1abd4:	and	x8, x8, #0xf0f0f0f0f0f0f0f
   1abd8:	add	x8, x8, x8, lsr #8
   1abdc:	add	x8, x8, x8, lsr #16
   1abe0:	lsr	x10, x8, #32
   1abe4:	add	w8, w10, w8
   1abe8:	and	x22, x8, #0xff
   1abec:	cbnz	x9, 1ac94 <__gmpz_hamdist@@Base+0x1d4>
   1abf0:	mov	w8, #0x40                  	// #64
   1abf4:	sub	x25, x8, x22
   1abf8:	mov	w8, #0x1                   	// #1
   1abfc:	ldr	x23, [x21], #8
   1ac00:	sub	x25, x25, #0x40
   1ac04:	sub	x8, x8, #0x1
   1ac08:	cbz	x23, 1abfc <__gmpz_hamdist@@Base+0x13c>
   1ac0c:	neg	x9, x8
   1ac10:	cmp	x9, x19
   1ac14:	csneg	x22, x19, x8, ge  // ge = tcont
   1ac18:	add	x24, x24, x8
   1ac1c:	cbz	x22, 1ac40 <__gmpz_hamdist@@Base+0x180>
   1ac20:	mov	x0, x20
   1ac24:	mov	x1, x22
   1ac28:	bl	cd80 <__gmpn_popcount@plt>
   1ac2c:	add	x8, x0, x25
   1ac30:	sub	x19, x19, x22
   1ac34:	neg	x8, x8
   1ac38:	add	x20, x20, x22, lsl #3
   1ac3c:	b	1ac44 <__gmpz_hamdist@@Base+0x184>
   1ac40:	neg	x8, x25
   1ac44:	sub	x24, x24, #0x1
   1ac48:	sub	x9, x23, #0x1
   1ac4c:	cbz	x19, 1ac5c <__gmpz_hamdist@@Base+0x19c>
   1ac50:	ldr	x10, [x20], #8
   1ac54:	sub	x19, x19, #0x1
   1ac58:	eor	x9, x10, x9
   1ac5c:	lsr	x10, x9, #1
   1ac60:	and	x10, x10, #0x5555555555555555
   1ac64:	sub	x9, x9, x10
   1ac68:	lsr	x10, x9, #2
   1ac6c:	and	x10, x10, #0x3333333333333333
   1ac70:	and	x9, x9, #0x3333333333333333
   1ac74:	add	x9, x10, x9
   1ac78:	add	x9, x9, x9, lsr #4
   1ac7c:	and	x9, x9, #0xf0f0f0f0f0f0f0f
   1ac80:	add	x9, x9, x9, lsr #8
   1ac84:	add	x9, x9, x9, lsr #16
   1ac88:	lsr	x10, x9, #32
   1ac8c:	add	w9, w10, w9
   1ac90:	add	x22, x8, w9, uxtb
   1ac94:	cmp	x19, x24
   1ac98:	csel	x23, x19, x24, lt  // lt = tstop
   1ac9c:	cbz	x23, 1acc8 <__gmpz_hamdist@@Base+0x208>
   1aca0:	mov	x0, x20
   1aca4:	mov	x1, x21
   1aca8:	mov	x2, x23
   1acac:	bl	ce40 <__gmpn_hamdist@plt>
   1acb0:	lsl	x8, x23, #3
   1acb4:	add	x22, x0, x22
   1acb8:	sub	x19, x19, x23
   1acbc:	sub	x24, x24, x23
   1acc0:	add	x20, x20, x8
   1acc4:	add	x21, x21, x8
   1acc8:	cbnz	x19, 1acd8 <__gmpz_hamdist@@Base+0x218>
   1accc:	mov	x19, x24
   1acd0:	mov	x20, x21
   1acd4:	cbz	x24, 1ace8 <__gmpz_hamdist@@Base+0x228>
   1acd8:	mov	x0, x20
   1acdc:	mov	x1, x19
   1ace0:	bl	cd80 <__gmpn_popcount@plt>
   1ace4:	add	x22, x0, x22
   1ace8:	mov	x0, x22
   1acec:	ldp	x20, x19, [sp, #64]
   1acf0:	ldp	x22, x21, [sp, #48]
   1acf4:	ldp	x24, x23, [sp, #32]
   1acf8:	ldr	x25, [sp, #16]
   1acfc:	ldp	x29, x30, [sp], #80
   1ad00:	ret

000000000001ad04 <__gmpz_import@@Base>:
   1ad04:	stp	x29, x30, [sp, #-96]!
   1ad08:	stp	x26, x25, [sp, #32]
   1ad0c:	stp	x24, x23, [sp, #48]
   1ad10:	stp	x22, x21, [sp, #64]
   1ad14:	stp	x20, x19, [sp, #80]
   1ad18:	lsl	x8, x3, #3
   1ad1c:	ldrsw	x9, [x0]
   1ad20:	str	x27, [sp, #16]
   1ad24:	sub	x27, x8, x5
   1ad28:	orr	x8, xzr, #0x3f
   1ad2c:	madd	x8, x27, x1, x8
   1ad30:	lsr	x20, x8, #6
   1ad34:	mov	x22, x6
   1ad38:	mov	x25, x5
   1ad3c:	mov	w26, w4
   1ad40:	mov	x23, x3
   1ad44:	mov	x21, x1
   1ad48:	mov	x19, x0
   1ad4c:	cmp	x20, x9
   1ad50:	mov	w24, w2
   1ad54:	mov	x29, sp
   1ad58:	b.gt	1b0ec <__gmpz_import@@Base+0x3e8>
   1ad5c:	ldr	x0, [x19, #8]
   1ad60:	cmp	w26, #0x0
   1ad64:	csinv	w15, w26, wzr, ne  // ne = any
   1ad68:	cbz	x25, 1ae74 <__gmpz_import@@Base+0x170>
   1ad6c:	add	x8, x27, #0x7
   1ad70:	cmp	w15, #0x0
   1ad74:	lsr	x8, x8, #3
   1ad78:	cneg	x9, x8, lt  // lt = tstop
   1ad7c:	cmp	w24, #0x0
   1ad80:	cneg	x10, x23, ge  // ge = tcont
   1ad84:	cbz	x21, 1b0a4 <__gmpz_import@@Base+0x3a0>
   1ad88:	sub	x12, x21, #0x1
   1ad8c:	mov	x8, xzr
   1ad90:	cmp	w24, #0x0
   1ad94:	mul	x12, x12, x23
   1ad98:	csel	x12, x12, x8, ge  // ge = tcont
   1ad9c:	and	w11, w27, #0x7
   1ada0:	add	x14, x22, x12
   1ada4:	mov	x12, #0xffffffffffffffff    	// #-1
   1ada8:	sub	x16, x23, #0x1
   1adac:	cmp	w15, #0x0
   1adb0:	lsl	x12, x12, x11
   1adb4:	csel	x16, x16, x8, ge  // ge = tcont
   1adb8:	mov	w13, wzr
   1adbc:	add	x9, x9, x10
   1adc0:	lsr	x10, x27, #3
   1adc4:	mvn	x12, x12
   1adc8:	add	x14, x14, x16
   1adcc:	sub	x15, x8, w15, sxtw
   1add0:	mov	x16, x8
   1add4:	b	1ade8 <__gmpz_import@@Base+0xe4>
   1add8:	add	x8, x8, #0x1
   1addc:	cmp	x8, x21
   1ade0:	add	x14, x14, x9
   1ade4:	b.eq	1ae68 <__gmpz_import@@Base+0x164>  // b.none
   1ade8:	cbz	x10, 1ae30 <__gmpz_import@@Base+0x12c>
   1adec:	mov	x17, x10
   1adf0:	b	1ae0c <__gmpz_import@@Base+0x108>
   1adf4:	neg	w13, w13
   1adf8:	str	x16, [x0], #8
   1adfc:	lsr	x16, x18, x13
   1ae00:	mov	w13, w1
   1ae04:	subs	x17, x17, #0x1
   1ae08:	b.eq	1ae30 <__gmpz_import@@Base+0x12c>  // b.none
   1ae0c:	ldrb	w18, [x14]
   1ae10:	add	x14, x14, x15
   1ae14:	subs	w1, w13, #0x38
   1ae18:	lsl	x2, x18, x13
   1ae1c:	orr	x16, x2, x16
   1ae20:	b.ge	1adf4 <__gmpz_import@@Base+0xf0>  // b.tcont
   1ae24:	add	w13, w13, #0x8
   1ae28:	subs	x17, x17, #0x1
   1ae2c:	b.ne	1ae0c <__gmpz_import@@Base+0x108>  // b.any
   1ae30:	cbz	w11, 1add8 <__gmpz_import@@Base+0xd4>
   1ae34:	ldrb	w17, [x14]
   1ae38:	add	x14, x14, x15
   1ae3c:	and	x17, x17, x12
   1ae40:	lsl	x1, x17, x13
   1ae44:	add	w13, w13, w11
   1ae48:	subs	w18, w13, #0x40
   1ae4c:	orr	x16, x1, x16
   1ae50:	b.lt	1add8 <__gmpz_import@@Base+0xd4>  // b.tstop
   1ae54:	sub	w13, w11, w18
   1ae58:	str	x16, [x0], #8
   1ae5c:	lsr	x16, x17, x13
   1ae60:	mov	w13, w18
   1ae64:	b	1add8 <__gmpz_import@@Base+0xd4>
   1ae68:	cbz	w13, 1b0a4 <__gmpz_import@@Base+0x3a0>
   1ae6c:	str	x16, [x0]
   1ae70:	b	1b0a4 <__gmpz_import@@Base+0x3a0>
   1ae74:	cmn	w24, #0x1
   1ae78:	cset	w8, eq  // eq = none
   1ae7c:	cmp	x23, #0x8
   1ae80:	cset	w9, eq  // eq = none
   1ae84:	and	w9, w8, w9
   1ae88:	cmp	w9, #0x1
   1ae8c:	and	x8, x22, #0x7
   1ae90:	b.ne	1aeb0 <__gmpz_import@@Base+0x1ac>  // b.any
   1ae94:	cmn	w15, #0x1
   1ae98:	b.ne	1aeb0 <__gmpz_import@@Base+0x1ac>  // b.any
   1ae9c:	cbnz	x8, 1aeb0 <__gmpz_import@@Base+0x1ac>
   1aea0:	mov	x1, x22
   1aea4:	mov	x2, x21
   1aea8:	bl	ca50 <__gmpn_copyi@plt>
   1aeac:	b	1b0a4 <__gmpz_import@@Base+0x3a0>
   1aeb0:	cmp	w15, #0x1
   1aeb4:	cset	w10, ne  // ne = any
   1aeb8:	eor	w9, w9, #0x1
   1aebc:	orr	w9, w9, w10
   1aec0:	tbnz	w9, #0, 1af58 <__gmpz_import@@Base+0x254>
   1aec4:	cbnz	x8, 1af58 <__gmpz_import@@Base+0x254>
   1aec8:	cmp	x21, #0x1
   1aecc:	b.lt	1b0a4 <__gmpz_import@@Base+0x3a0>  // b.tstop
   1aed0:	cmp	x21, #0x1
   1aed4:	b.eq	1aef4 <__gmpz_import@@Base+0x1f0>  // b.none
   1aed8:	lsl	x8, x21, #3
   1aedc:	add	x9, x22, x8
   1aee0:	cmp	x9, x0
   1aee4:	b.ls	1afc4 <__gmpz_import@@Base+0x2c0>  // b.plast
   1aee8:	add	x8, x0, x8
   1aeec:	cmp	x8, x22
   1aef0:	b.ls	1afc4 <__gmpz_import@@Base+0x2c0>  // b.plast
   1aef4:	mov	x10, xzr
   1aef8:	mov	x8, x0
   1aefc:	mov	x9, x22
   1af00:	sub	x10, x21, x10
   1af04:	ldr	x11, [x9], #8
   1af08:	subs	x10, x10, #0x1
   1af0c:	lsl	x14, x11, #40
   1af10:	and	x14, x14, #0xff000000000000
   1af14:	lsr	x13, x11, #16
   1af18:	bfi	x14, x11, #56, #8
   1af1c:	lsr	x12, x11, #24
   1af20:	bfi	x14, x13, #40, #8
   1af24:	lsr	x13, x11, #8
   1af28:	and	x13, x13, #0xff000000
   1af2c:	bfi	x14, x12, #32, #8
   1af30:	orr	x13, x14, x13
   1af34:	lsr	x14, x11, #40
   1af38:	and	x12, x12, #0xff0000
   1af3c:	and	x14, x14, #0xff00
   1af40:	orr	x12, x13, x12
   1af44:	orr	x12, x12, x14
   1af48:	add	x11, x12, x11, lsr #56
   1af4c:	str	x11, [x8], #8
   1af50:	b.ne	1af04 <__gmpz_import@@Base+0x200>  // b.any
   1af54:	b	1b0a4 <__gmpz_import@@Base+0x3a0>
   1af58:	cmp	w24, #0x1
   1af5c:	b.ne	1ad6c <__gmpz_import@@Base+0x68>  // b.any
   1af60:	cmp	x23, #0x8
   1af64:	b.ne	1ad6c <__gmpz_import@@Base+0x68>  // b.any
   1af68:	cmn	w15, #0x1
   1af6c:	b.ne	1ad6c <__gmpz_import@@Base+0x68>  // b.any
   1af70:	cbnz	x8, 1ad6c <__gmpz_import@@Base+0x68>
   1af74:	cmp	x21, #0x1
   1af78:	b.lt	1b0a4 <__gmpz_import@@Base+0x3a0>  // b.tstop
   1af7c:	cmp	x21, #0x4
   1af80:	add	x8, x22, x21, lsl #3
   1af84:	b.cc	1afa4 <__gmpz_import@@Base+0x2a0>  // b.lo, b.ul, b.last
   1af88:	lsl	x9, x21, #3
   1af8c:	add	x10, x22, x9
   1af90:	cmp	x10, x0
   1af94:	b.ls	1b05c <__gmpz_import@@Base+0x358>  // b.plast
   1af98:	add	x9, x0, x9
   1af9c:	cmp	x9, x22
   1afa0:	b.ls	1b05c <__gmpz_import@@Base+0x358>  // b.plast
   1afa4:	mov	x9, xzr
   1afa8:	sub	x8, x8, #0x8
   1afac:	sub	x9, x21, x9
   1afb0:	ldr	x10, [x8], #-8
   1afb4:	subs	x9, x9, #0x1
   1afb8:	str	x10, [x0], #8
   1afbc:	b.ne	1afb0 <__gmpz_import@@Base+0x2ac>  // b.any
   1afc0:	b	1b0a4 <__gmpz_import@@Base+0x3a0>
   1afc4:	and	x10, x21, #0xfffffffffffffffe
   1afc8:	lsl	x9, x10, #3
   1afcc:	movi	v0.2d, #0xff000000000000
   1afd0:	movi	v1.2d, #0xff0000000000
   1afd4:	movi	v2.2d, #0xff00000000
   1afd8:	movi	v3.2d, #0xff000000
   1afdc:	movi	v4.2d, #0xff0000
   1afe0:	add	x8, x0, x9
   1afe4:	add	x9, x22, x9
   1afe8:	movi	v5.2d, #0xff00
   1afec:	mov	x11, x10
   1aff0:	ldr	q6, [x22], #16
   1aff4:	subs	x11, x11, #0x2
   1aff8:	shl	v16.2d, v6.2d, #40
   1affc:	shl	v7.2d, v6.2d, #56
   1b000:	and	v16.16b, v16.16b, v0.16b
   1b004:	orr	v7.16b, v16.16b, v7.16b
   1b008:	shl	v16.2d, v6.2d, #24
   1b00c:	and	v16.16b, v16.16b, v1.16b
   1b010:	orr	v7.16b, v7.16b, v16.16b
   1b014:	shl	v16.2d, v6.2d, #8
   1b018:	and	v16.16b, v16.16b, v2.16b
   1b01c:	orr	v7.16b, v7.16b, v16.16b
   1b020:	ushr	v16.2d, v6.2d, #8
   1b024:	and	v16.16b, v16.16b, v3.16b
   1b028:	orr	v7.16b, v7.16b, v16.16b
   1b02c:	ushr	v16.2d, v6.2d, #24
   1b030:	and	v16.16b, v16.16b, v4.16b
   1b034:	orr	v7.16b, v7.16b, v16.16b
   1b038:	ushr	v16.2d, v6.2d, #40
   1b03c:	and	v16.16b, v16.16b, v5.16b
   1b040:	orr	v7.16b, v7.16b, v16.16b
   1b044:	usra	v7.2d, v6.2d, #56
   1b048:	str	q7, [x0], #16
   1b04c:	b.ne	1aff0 <__gmpz_import@@Base+0x2ec>  // b.any
   1b050:	cmp	x10, x21
   1b054:	b.ne	1af00 <__gmpz_import@@Base+0x1fc>  // b.any
   1b058:	b	1b0a4 <__gmpz_import@@Base+0x3a0>
   1b05c:	and	x9, x21, #0xfffffffffffffffc
   1b060:	add	x11, x22, x21, lsl #3
   1b064:	lsl	x12, x9, #3
   1b068:	add	x10, x0, #0x10
   1b06c:	sub	x8, x8, x12
   1b070:	add	x0, x0, x12
   1b074:	sub	x11, x11, #0x10
   1b078:	mov	x12, x9
   1b07c:	ldp	q1, q0, [x11, #-16]
   1b080:	subs	x12, x12, #0x4
   1b084:	sub	x11, x11, #0x20
   1b088:	ext	v0.16b, v0.16b, v0.16b, #8
   1b08c:	ext	v1.16b, v1.16b, v1.16b, #8
   1b090:	stp	q0, q1, [x10, #-16]
   1b094:	add	x10, x10, #0x20
   1b098:	b.ne	1b07c <__gmpz_import@@Base+0x378>  // b.any
   1b09c:	cmp	x9, x21
   1b0a0:	b.ne	1afa8 <__gmpz_import@@Base+0x2a4>  // b.any
   1b0a4:	ldr	x8, [x19, #8]
   1b0a8:	sub	x8, x8, #0x8
   1b0ac:	subs	x9, x20, #0x1
   1b0b0:	b.lt	1b0c8 <__gmpz_import@@Base+0x3c4>  // b.tstop
   1b0b4:	ldr	x10, [x8, x20, lsl #3]
   1b0b8:	mov	x20, x9
   1b0bc:	cbz	x10, 1b0ac <__gmpz_import@@Base+0x3a8>
   1b0c0:	add	x8, x9, #0x1
   1b0c4:	b	1b0cc <__gmpz_import@@Base+0x3c8>
   1b0c8:	mov	x8, xzr
   1b0cc:	str	w8, [x19, #4]
   1b0d0:	ldp	x20, x19, [sp, #80]
   1b0d4:	ldp	x22, x21, [sp, #64]
   1b0d8:	ldp	x24, x23, [sp, #48]
   1b0dc:	ldp	x26, x25, [sp, #32]
   1b0e0:	ldr	x27, [sp, #16]
   1b0e4:	ldp	x29, x30, [sp], #96
   1b0e8:	ret
   1b0ec:	mov	x0, x19
   1b0f0:	mov	x1, x20
   1b0f4:	bl	c080 <__gmpz_realloc@plt>
   1b0f8:	b	1ad60 <__gmpz_import@@Base+0x5c>

000000000001b0fc <__gmpz_init@@Base>:
   1b0fc:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1b100:	add	x8, x8, #0x530
   1b104:	stp	xzr, x8, [x0]
   1b108:	ret

000000000001b10c <__gmpz_init2@@Base>:
   1b10c:	stp	x29, x30, [sp, #-32]!
   1b110:	cmp	x1, #0x0
   1b114:	cset	w8, ne  // ne = any
   1b118:	sub	x8, x1, x8
   1b11c:	mov	x9, #0x1fffffffc0          	// #137438953408
   1b120:	cmp	x8, x9
   1b124:	stp	x20, x19, [sp, #16]
   1b128:	mov	x29, sp
   1b12c:	b.cs	1b164 <__gmpz_init2@@Base+0x58>  // b.hs, b.nlast
   1b130:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b134:	ldr	x9, [x9, #3840]
   1b138:	lsr	x8, x8, #6
   1b13c:	add	x20, x8, #0x1
   1b140:	mov	x19, x0
   1b144:	ldr	x9, [x9]
   1b148:	lsl	x0, x20, #3
   1b14c:	blr	x9
   1b150:	str	x0, [x19, #8]
   1b154:	stp	w20, wzr, [x19]
   1b158:	ldp	x20, x19, [sp, #16]
   1b15c:	ldp	x29, x30, [sp], #32
   1b160:	ret
   1b164:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b168:	ldr	x8, [x8, #3824]
   1b16c:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1b170:	add	x0, x0, #0x538
   1b174:	mov	w1, #0x1a                  	// #26
   1b178:	ldr	x3, [x8]
   1b17c:	mov	w2, #0x1                   	// #1
   1b180:	bl	ce30 <fwrite@plt>
   1b184:	bl	c900 <abort@plt>

000000000001b188 <__gmpz_inits@@Base>:
   1b188:	sub	sp, sp, #0xe0
   1b18c:	mov	x8, #0xffffffffffffffc8    	// #-56
   1b190:	mov	x9, sp
   1b194:	movk	x8, #0xff80, lsl #32
   1b198:	add	x9, x9, #0x80
   1b19c:	add	x10, sp, #0x88
   1b1a0:	stp	x9, x8, [sp, #208]
   1b1a4:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1b1a8:	add	x11, sp, #0xe0
   1b1ac:	add	x10, x10, #0x38
   1b1b0:	add	x8, x8, #0x558
   1b1b4:	stp	x1, x2, [sp, #136]
   1b1b8:	stp	x3, x4, [sp, #152]
   1b1bc:	stp	x5, x6, [sp, #168]
   1b1c0:	stp	q0, q1, [sp]
   1b1c4:	stp	q2, q3, [sp, #32]
   1b1c8:	stp	q4, q5, [sp, #64]
   1b1cc:	stp	q6, q7, [sp, #96]
   1b1d0:	str	x10, [sp, #200]
   1b1d4:	stp	x7, x11, [sp, #184]
   1b1d8:	b	1b1f0 <__gmpz_inits@@Base+0x68>
   1b1dc:	ldr	x9, [sp, #192]
   1b1e0:	add	x10, x9, #0x8
   1b1e4:	str	x10, [sp, #192]
   1b1e8:	ldr	x0, [x9]
   1b1ec:	cbz	x0, 1b21c <__gmpz_inits@@Base+0x94>
   1b1f0:	stp	xzr, x8, [x0]
   1b1f4:	ldrsw	x9, [sp, #216]
   1b1f8:	tbz	w9, #31, 1b1dc <__gmpz_inits@@Base+0x54>
   1b1fc:	add	w10, w9, #0x8
   1b200:	cmn	w9, #0x8
   1b204:	str	w10, [sp, #216]
   1b208:	b.gt	1b1dc <__gmpz_inits@@Base+0x54>
   1b20c:	ldr	x10, [sp, #200]
   1b210:	add	x9, x10, x9
   1b214:	ldr	x0, [x9]
   1b218:	cbnz	x0, 1b1f0 <__gmpz_inits@@Base+0x68>
   1b21c:	add	sp, sp, #0xe0
   1b220:	ret

000000000001b224 <__gmpz_inp_raw@@Base>:
   1b224:	sub	sp, sp, #0x50
   1b228:	stp	x29, x30, [sp, #16]
   1b22c:	stp	x24, x23, [sp, #32]
   1b230:	stp	x22, x21, [sp, #48]
   1b234:	stp	x20, x19, [sp, #64]
   1b238:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b23c:	ldr	x8, [x8, #3888]
   1b240:	cmp	x1, #0x0
   1b244:	add	x29, sp, #0x10
   1b248:	mov	x19, x0
   1b24c:	ldr	x8, [x8]
   1b250:	sub	x0, x29, #0x4
   1b254:	mov	w2, #0x1                   	// #1
   1b258:	csel	x23, x8, x1, eq  // eq = none
   1b25c:	mov	w1, #0x4                   	// #4
   1b260:	mov	x3, x23
   1b264:	bl	cb90 <fread@plt>
   1b268:	cmp	x0, #0x1
   1b26c:	b.ne	1b3ac <__gmpz_inp_raw@@Base+0x188>  // b.any
   1b270:	ldur	w8, [x29, #-4]
   1b274:	lsl	x8, x8, #32
   1b278:	rev	x8, x8
   1b27c:	lsr	x10, x8, #31
   1b280:	orr	x9, x8, #0xffffffff00000000
   1b284:	cmp	x10, #0x0
   1b288:	csel	x24, x8, x9, eq  // eq = none
   1b28c:	cmp	x24, #0x0
   1b290:	cneg	x20, x24, mi  // mi = first
   1b294:	lsl	x8, x20, #3
   1b298:	add	x8, x8, #0x3f
   1b29c:	lsr	x21, x8, #6
   1b2a0:	cbz	x21, 1b3b4 <__gmpz_inp_raw@@Base+0x190>
   1b2a4:	ldrsw	x8, [x19]
   1b2a8:	cmp	x21, x8
   1b2ac:	b.gt	1b3e4 <__gmpz_inp_raw@@Base+0x1c0>
   1b2b0:	ldr	x22, [x19, #8]
   1b2b4:	add	x8, x22, x21, lsl #3
   1b2b8:	sub	x0, x8, x20
   1b2bc:	mov	w2, #0x1                   	// #1
   1b2c0:	mov	x1, x20
   1b2c4:	mov	x3, x23
   1b2c8:	str	xzr, [x22]
   1b2cc:	bl	cb90 <fread@plt>
   1b2d0:	cmp	x0, #0x1
   1b2d4:	mov	x0, xzr
   1b2d8:	b.ne	1b3cc <__gmpz_inp_raw@@Base+0x1a8>  // b.any
   1b2dc:	add	x8, x21, #0x1
   1b2e0:	lsr	x8, x8, #1
   1b2e4:	cbz	x8, 1b38c <__gmpz_inp_raw@@Base+0x168>
   1b2e8:	add	x9, x22, x21, lsl #3
   1b2ec:	sub	x9, x9, #0x8
   1b2f0:	mov	x10, x22
   1b2f4:	ldr	x11, [x9]
   1b2f8:	ldr	x12, [x10]
   1b2fc:	subs	x8, x8, #0x1
   1b300:	lsl	x15, x11, #40
   1b304:	and	x15, x15, #0xff000000000000
   1b308:	lsr	x14, x11, #16
   1b30c:	bfi	x15, x11, #56, #8
   1b310:	bfi	x15, x14, #40, #8
   1b314:	lsl	x14, x12, #40
   1b318:	lsr	x13, x11, #24
   1b31c:	lsr	x16, x11, #8
   1b320:	and	x14, x14, #0xff000000000000
   1b324:	lsr	x17, x12, #16
   1b328:	bfi	x14, x12, #56, #8
   1b32c:	and	x16, x16, #0xff000000
   1b330:	bfi	x15, x13, #32, #8
   1b334:	bfi	x14, x17, #40, #8
   1b338:	lsr	x17, x12, #24
   1b33c:	orr	x15, x15, x16
   1b340:	lsr	x16, x12, #8
   1b344:	and	x16, x16, #0xff000000
   1b348:	bfi	x14, x17, #32, #8
   1b34c:	and	x13, x13, #0xff0000
   1b350:	orr	x14, x14, x16
   1b354:	orr	x13, x15, x13
   1b358:	and	x15, x17, #0xff0000
   1b35c:	orr	x14, x14, x15
   1b360:	lsr	x15, x11, #40
   1b364:	and	x15, x15, #0xff00
   1b368:	orr	x13, x13, x15
   1b36c:	lsr	x15, x12, #40
   1b370:	and	x15, x15, #0xff00
   1b374:	orr	x14, x14, x15
   1b378:	add	x11, x13, x11, lsr #56
   1b37c:	add	x12, x14, x12, lsr #56
   1b380:	str	x11, [x10], #8
   1b384:	str	x12, [x9], #-8
   1b388:	b.ne	1b2f4 <__gmpz_inp_raw@@Base+0xd0>  // b.any
   1b38c:	sub	x8, x22, #0x8
   1b390:	subs	x9, x21, #0x1
   1b394:	b.lt	1b3b4 <__gmpz_inp_raw@@Base+0x190>  // b.tstop
   1b398:	ldr	x10, [x8, x21, lsl #3]
   1b39c:	mov	x21, x9
   1b3a0:	cbz	x10, 1b390 <__gmpz_inp_raw@@Base+0x16c>
   1b3a4:	add	x8, x9, #0x1
   1b3a8:	b	1b3b8 <__gmpz_inp_raw@@Base+0x194>
   1b3ac:	mov	x0, xzr
   1b3b0:	b	1b3cc <__gmpz_inp_raw@@Base+0x1a8>
   1b3b4:	mov	x8, xzr
   1b3b8:	neg	w9, w8
   1b3bc:	cmp	x24, #0x0
   1b3c0:	csel	x8, x8, x9, ge  // ge = tcont
   1b3c4:	add	x0, x20, #0x4
   1b3c8:	str	w8, [x19, #4]
   1b3cc:	ldp	x20, x19, [sp, #64]
   1b3d0:	ldp	x22, x21, [sp, #48]
   1b3d4:	ldp	x24, x23, [sp, #32]
   1b3d8:	ldp	x29, x30, [sp, #16]
   1b3dc:	add	sp, sp, #0x50
   1b3e0:	ret
   1b3e4:	mov	x0, x19
   1b3e8:	mov	x1, x21
   1b3ec:	bl	c080 <__gmpz_realloc@plt>
   1b3f0:	mov	x22, x0
   1b3f4:	b	1b2b4 <__gmpz_inp_raw@@Base+0x90>

000000000001b3f8 <__gmpz_inp_str@@Base>:
   1b3f8:	stp	x29, x30, [sp, #-64]!
   1b3fc:	str	x23, [sp, #16]
   1b400:	stp	x22, x21, [sp, #32]
   1b404:	stp	x20, x19, [sp, #48]
   1b408:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b40c:	ldr	x8, [x8, #3888]
   1b410:	cmp	x1, #0x0
   1b414:	mov	w19, w2
   1b418:	mov	x21, x0
   1b41c:	ldr	x8, [x8]
   1b420:	mov	x20, xzr
   1b424:	mov	x29, sp
   1b428:	csel	x22, x8, x1, eq  // eq = none
   1b42c:	mov	x0, x22
   1b430:	bl	c7f0 <getc@plt>
   1b434:	mov	w23, w0
   1b438:	add	x20, x20, #0x1
   1b43c:	bl	cae0 <__ctype_b_loc@plt>
   1b440:	ldr	x8, [x0]
   1b444:	ldrh	w8, [x8, w23, sxtw #1]
   1b448:	tbnz	w8, #13, 1b42c <__gmpz_inp_str@@Base+0x34>
   1b44c:	mov	x0, x21
   1b450:	mov	x1, x22
   1b454:	mov	w2, w19
   1b458:	mov	w3, w23
   1b45c:	mov	x4, x20
   1b460:	ldp	x20, x19, [sp, #48]
   1b464:	ldp	x22, x21, [sp, #32]
   1b468:	ldr	x23, [sp, #16]
   1b46c:	ldp	x29, x30, [sp], #64
   1b470:	b	c9d0 <__gmpz_inp_str_nowhite@plt>

000000000001b474 <__gmpz_inp_str_nowhite@@Base>:
   1b474:	sub	sp, sp, #0x70
   1b478:	stp	x29, x30, [sp, #16]
   1b47c:	stp	x28, x27, [sp, #32]
   1b480:	stp	x26, x25, [sp, #48]
   1b484:	stp	x24, x23, [sp, #64]
   1b488:	stp	x22, x21, [sp, #80]
   1b48c:	stp	x20, x19, [sp, #96]
   1b490:	adrp	x28, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b494:	ldr	x28, [x28, #3920]
   1b498:	mov	x20, x4
   1b49c:	mov	w23, w3
   1b4a0:	mov	w22, w2
   1b4a4:	mov	x21, x1
   1b4a8:	mov	x26, x0
   1b4ac:	cmp	w2, #0x25
   1b4b0:	add	x29, sp, #0x10
   1b4b4:	b.lt	1b4c4 <__gmpz_inp_str_nowhite@@Base+0x50>  // b.tstop
   1b4b8:	cmp	w22, #0x3e
   1b4bc:	b.gt	1b548 <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b4c0:	add	x28, x28, #0xd0
   1b4c4:	cmp	w23, #0x2d
   1b4c8:	b.ne	1b4ec <__gmpz_inp_str_nowhite@@Base+0x78>  // b.any
   1b4cc:	mov	x0, x21
   1b4d0:	bl	c7f0 <getc@plt>
   1b4d4:	mov	w23, w0
   1b4d8:	add	x20, x20, #0x1
   1b4dc:	mov	w27, #0x1                   	// #1
   1b4e0:	cmn	w23, #0x1
   1b4e4:	b.ne	1b4f8 <__gmpz_inp_str_nowhite@@Base+0x84>  // b.any
   1b4e8:	b	1b548 <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b4ec:	mov	w27, wzr
   1b4f0:	cmn	w23, #0x1
   1b4f4:	b.eq	1b548 <__gmpz_inp_str_nowhite@@Base+0xd4>  // b.none
   1b4f8:	ldrb	w8, [x28, w23, sxtw]
   1b4fc:	cmp	w22, #0x0
   1b500:	mov	w9, #0xa                   	// #10
   1b504:	csel	w9, w9, w22, eq  // eq = none
   1b508:	cmp	w9, w8
   1b50c:	b.le	1b548 <__gmpz_inp_str_nowhite@@Base+0xd4>
   1b510:	cbnz	w22, 1b554 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b514:	cmp	w23, #0x30
   1b518:	b.ne	1b550 <__gmpz_inp_str_nowhite@@Base+0xdc>  // b.any
   1b51c:	mov	x0, x21
   1b520:	bl	c7f0 <getc@plt>
   1b524:	orr	w8, w0, #0x20
   1b528:	cmp	w8, #0x78
   1b52c:	b.ne	1b6cc <__gmpz_inp_str_nowhite@@Base+0x258>  // b.any
   1b530:	mov	x0, x21
   1b534:	bl	c7f0 <getc@plt>
   1b538:	mov	w23, w0
   1b53c:	add	x20, x20, #0x2
   1b540:	mov	w22, #0x10                  	// #16
   1b544:	b	1b554 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b548:	mov	x20, xzr
   1b54c:	b	1b6a8 <__gmpz_inp_str_nowhite@@Base+0x234>
   1b550:	mov	w22, #0xa                   	// #10
   1b554:	cmp	w23, #0x30
   1b558:	b.ne	1b574 <__gmpz_inp_str_nowhite@@Base+0x100>  // b.any
   1b55c:	mov	x0, x21
   1b560:	bl	c7f0 <getc@plt>
   1b564:	cmp	w0, #0x30
   1b568:	add	x20, x20, #0x1
   1b56c:	b.eq	1b55c <__gmpz_inp_str_nowhite@@Base+0xe8>  // b.none
   1b570:	mov	w23, w0
   1b574:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b578:	ldr	x8, [x8, #3840]
   1b57c:	mov	w0, #0x64                  	// #100
   1b580:	mov	w24, #0x64                  	// #100
   1b584:	ldr	x8, [x8]
   1b588:	blr	x8
   1b58c:	cmn	w23, #0x1
   1b590:	mov	x25, x0
   1b594:	b.eq	1b610 <__gmpz_inp_str_nowhite@@Base+0x19c>  // b.none
   1b598:	str	x26, [sp, #8]
   1b59c:	mov	x26, xzr
   1b5a0:	mov	w24, #0x64                  	// #100
   1b5a4:	str	w27, [sp, #4]
   1b5a8:	b	1b5cc <__gmpz_inp_str_nowhite@@Base+0x158>
   1b5ac:	mov	x0, x21
   1b5b0:	add	x19, x26, #0x1
   1b5b4:	strb	w27, [x25, x26]
   1b5b8:	bl	c7f0 <getc@plt>
   1b5bc:	mov	w23, w0
   1b5c0:	cmn	w0, #0x1
   1b5c4:	mov	x26, x19
   1b5c8:	b.eq	1b61c <__gmpz_inp_str_nowhite@@Base+0x1a8>  // b.none
   1b5cc:	ldrb	w27, [x28, w23, sxtw]
   1b5d0:	cmp	w22, w27
   1b5d4:	b.le	1b618 <__gmpz_inp_str_nowhite@@Base+0x1a4>
   1b5d8:	cmp	x26, x24
   1b5dc:	b.cc	1b5ac <__gmpz_inp_str_nowhite@@Base+0x138>  // b.lo, b.ul, b.last
   1b5e0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b5e4:	ldr	x8, [x8, #3792]
   1b5e8:	add	x9, x24, x24, lsl #1
   1b5ec:	lsr	x23, x9, #1
   1b5f0:	mov	x0, x25
   1b5f4:	ldr	x8, [x8]
   1b5f8:	mov	x1, x24
   1b5fc:	mov	x2, x23
   1b600:	blr	x8
   1b604:	mov	x25, x0
   1b608:	mov	x24, x23
   1b60c:	b	1b5ac <__gmpz_inp_str_nowhite@@Base+0x138>
   1b610:	mov	x19, xzr
   1b614:	b	1b624 <__gmpz_inp_str_nowhite@@Base+0x1b0>
   1b618:	mov	x19, x26
   1b61c:	ldr	x26, [sp, #8]
   1b620:	ldr	w27, [sp, #4]
   1b624:	mov	w0, w23
   1b628:	mov	x1, x21
   1b62c:	bl	cc50 <ungetc@plt>
   1b630:	add	x8, x20, x19
   1b634:	sub	x20, x8, #0x1
   1b638:	cbz	x19, 1b688 <__gmpz_inp_str_nowhite@@Base+0x214>
   1b63c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b640:	ldr	x8, [x8, #3936]
   1b644:	mov	w9, #0x28                  	// #40
   1b648:	smaddl	x8, w22, w9, x8
   1b64c:	ldr	x8, [x8, #16]
   1b650:	ldrsw	x9, [x26]
   1b654:	umulh	x8, x8, x19
   1b658:	ubfx	x8, x8, #3, #58
   1b65c:	add	x1, x8, #0x2
   1b660:	cmp	x1, x9
   1b664:	b.gt	1b6fc <__gmpz_inp_str_nowhite@@Base+0x288>
   1b668:	ldr	x0, [x26, #8]
   1b66c:	mov	x1, x25
   1b670:	mov	x2, x19
   1b674:	mov	w3, w22
   1b678:	bl	c090 <__gmpn_set_str@plt>
   1b67c:	cmp	w27, #0x0
   1b680:	cneg	w8, w0, ne  // ne = any
   1b684:	b	1b68c <__gmpz_inp_str_nowhite@@Base+0x218>
   1b688:	mov	w8, wzr
   1b68c:	str	w8, [x26, #4]
   1b690:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1b694:	ldr	x8, [x8, #4016]
   1b698:	mov	x0, x25
   1b69c:	mov	x1, x24
   1b6a0:	ldr	x8, [x8]
   1b6a4:	blr	x8
   1b6a8:	mov	x0, x20
   1b6ac:	ldp	x20, x19, [sp, #96]
   1b6b0:	ldp	x22, x21, [sp, #80]
   1b6b4:	ldp	x24, x23, [sp, #64]
   1b6b8:	ldp	x26, x25, [sp, #48]
   1b6bc:	ldp	x28, x27, [sp, #32]
   1b6c0:	ldp	x29, x30, [sp, #16]
   1b6c4:	add	sp, sp, #0x70
   1b6c8:	ret
   1b6cc:	cmp	w8, #0x62
   1b6d0:	b.ne	1b6ec <__gmpz_inp_str_nowhite@@Base+0x278>  // b.any
   1b6d4:	mov	x0, x21
   1b6d8:	bl	c7f0 <getc@plt>
   1b6dc:	mov	w23, w0
   1b6e0:	add	x20, x20, #0x2
   1b6e4:	mov	w22, #0x2                   	// #2
   1b6e8:	b	1b554 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b6ec:	mov	w23, w0
   1b6f0:	add	x20, x20, #0x1
   1b6f4:	mov	w22, #0x8                   	// #8
   1b6f8:	b	1b554 <__gmpz_inp_str_nowhite@@Base+0xe0>
   1b6fc:	mov	x0, x26
   1b700:	bl	c080 <__gmpz_realloc@plt>
   1b704:	b	1b668 <__gmpz_inp_str_nowhite@@Base+0x1f4>

000000000001b708 <__gmpz_invert@@Base>:
   1b708:	stp	x29, x30, [sp, #-64]!
   1b70c:	stp	x24, x23, [sp, #16]
   1b710:	stp	x22, x21, [sp, #32]
   1b714:	stp	x20, x19, [sp, #48]
   1b718:	mov	x29, sp
   1b71c:	sub	sp, sp, #0x30
   1b720:	ldr	w8, [x1, #4]
   1b724:	ldr	w9, [x2, #4]
   1b728:	mov	x19, x2
   1b72c:	mov	x21, x1
   1b730:	cmp	w8, #0x0
   1b734:	cneg	w8, w8, mi  // mi = first
   1b738:	cmp	w9, #0x0
   1b73c:	cneg	w9, w9, mi  // mi = first
   1b740:	cmp	x8, x9
   1b744:	csel	x8, x8, x9, hi  // hi = pmore
   1b748:	add	x24, x8, #0x1
   1b74c:	mov	x20, x0
   1b750:	cmp	x8, #0xfdf
   1b754:	lsl	x23, x24, #3
   1b758:	stur	xzr, [x29, #-40]
   1b75c:	stur	w24, [x29, #-16]
   1b760:	b.hi	1b858 <__gmpz_invert@@Base+0x150>  // b.pmore
   1b764:	add	x9, x23, #0xf
   1b768:	mov	x8, sp
   1b76c:	and	x9, x9, #0x1ffffffff0
   1b770:	sub	x8, x8, x9
   1b774:	mov	sp, x8
   1b778:	stur	x8, [x29, #-8]
   1b77c:	mov	x8, sp
   1b780:	sub	x22, x29, #0x20
   1b784:	sub	x0, x8, x9
   1b788:	stur	w24, [x29, #-32]
   1b78c:	mov	sp, x0
   1b790:	stur	x0, [x29, #-24]
   1b794:	sub	x0, x29, #0x10
   1b798:	mov	x1, x22
   1b79c:	mov	x2, xzr
   1b7a0:	mov	x3, x21
   1b7a4:	mov	x4, x19
   1b7a8:	bl	c0c0 <__gmpz_gcdext@plt>
   1b7ac:	ldur	w8, [x29, #-12]
   1b7b0:	cmp	w8, #0x1
   1b7b4:	b.ne	1b7e8 <__gmpz_invert@@Base+0xe0>  // b.any
   1b7b8:	ldur	x8, [x29, #-8]
   1b7bc:	ldr	x8, [x8]
   1b7c0:	cmp	x8, #0x1
   1b7c4:	b.ne	1b7e8 <__gmpz_invert@@Base+0xe0>  // b.any
   1b7c8:	ldur	w8, [x29, #-28]
   1b7cc:	tbnz	w8, #31, 1b7fc <__gmpz_invert@@Base+0xf4>
   1b7d0:	mov	x0, x20
   1b7d4:	mov	x1, x22
   1b7d8:	bl	c420 <__gmpz_set@plt>
   1b7dc:	ldur	x0, [x29, #-40]
   1b7e0:	cbz	x0, 1b81c <__gmpz_invert@@Base+0x114>
   1b7e4:	b	1b850 <__gmpz_invert@@Base+0x148>
   1b7e8:	ldur	x0, [x29, #-40]
   1b7ec:	cbz	x0, 1b820 <__gmpz_invert@@Base+0x118>
   1b7f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1b7f4:	mov	w0, wzr
   1b7f8:	b	1b820 <__gmpz_invert@@Base+0x118>
   1b7fc:	ldr	w8, [x19, #4]
   1b800:	tbnz	w8, #31, 1b838 <__gmpz_invert@@Base+0x130>
   1b804:	mov	x0, x20
   1b808:	mov	x1, x22
   1b80c:	mov	x2, x19
   1b810:	bl	cf90 <__gmpz_add@plt>
   1b814:	ldur	x0, [x29, #-40]
   1b818:	cbnz	x0, 1b850 <__gmpz_invert@@Base+0x148>
   1b81c:	mov	w0, #0x1                   	// #1
   1b820:	mov	sp, x29
   1b824:	ldp	x20, x19, [sp, #48]
   1b828:	ldp	x22, x21, [sp, #32]
   1b82c:	ldp	x24, x23, [sp, #16]
   1b830:	ldp	x29, x30, [sp], #64
   1b834:	ret
   1b838:	mov	x0, x20
   1b83c:	mov	x1, x22
   1b840:	mov	x2, x19
   1b844:	bl	c260 <__gmpz_sub@plt>
   1b848:	ldur	x0, [x29, #-40]
   1b84c:	cbz	x0, 1b81c <__gmpz_invert@@Base+0x114>
   1b850:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1b854:	b	1b81c <__gmpz_invert@@Base+0x114>
   1b858:	sub	x0, x29, #0x28
   1b85c:	mov	x1, x23
   1b860:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1b864:	stur	x0, [x29, #-8]
   1b868:	sub	x0, x29, #0x28
   1b86c:	mov	x1, x23
   1b870:	sub	x22, x29, #0x20
   1b874:	stur	w24, [x29, #-32]
   1b878:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1b87c:	b	1b790 <__gmpz_invert@@Base+0x88>

000000000001b880 <__gmpz_ior@@Base>:
   1b880:	stp	x29, x30, [sp, #-80]!
   1b884:	stp	x26, x25, [sp, #16]
   1b888:	stp	x24, x23, [sp, #32]
   1b88c:	stp	x22, x21, [sp, #48]
   1b890:	stp	x20, x19, [sp, #64]
   1b894:	mov	x29, sp
   1b898:	sub	sp, sp, #0x10
   1b89c:	ldr	w9, [x1, #4]
   1b8a0:	ldr	w10, [x2, #4]
   1b8a4:	ldr	x22, [x0, #8]
   1b8a8:	mov	x19, x0
   1b8ac:	cmp	w9, w10
   1b8b0:	csel	x8, x2, x1, lt  // lt = tstop
   1b8b4:	ldr	x21, [x8, #8]
   1b8b8:	csel	w11, w9, w10, lt  // lt = tstop
   1b8bc:	csel	w24, w10, w9, lt  // lt = tstop
   1b8c0:	sxtw	x23, w11
   1b8c4:	sxtw	x20, w24
   1b8c8:	csel	x26, x1, x2, lt  // lt = tstop
   1b8cc:	tbnz	w11, #31, 1b91c <__gmpz_ior@@Base+0x9c>
   1b8d0:	cmp	x22, x21
   1b8d4:	mov	x0, x21
   1b8d8:	b.eq	1b900 <__gmpz_ior@@Base+0x80>  // b.none
   1b8dc:	ldr	w8, [x19]
   1b8e0:	cmp	w24, w8
   1b8e4:	b.gt	1bf98 <__gmpz_ior@@Base+0x718>
   1b8e8:	lsl	x8, x23, #3
   1b8ec:	add	x0, x22, x8
   1b8f0:	add	x1, x21, x8
   1b8f4:	sub	x2, x20, x23
   1b8f8:	bl	ca50 <__gmpn_copyi@plt>
   1b8fc:	mov	x0, x22
   1b900:	cbz	w23, 1b914 <__gmpz_ior@@Base+0x94>
   1b904:	ldr	x2, [x26, #8]
   1b908:	mov	x1, x21
   1b90c:	mov	x3, x23
   1b910:	bl	cc00 <__gmpn_ior_n@plt>
   1b914:	str	w24, [x19, #4]
   1b918:	b	1c058 <__gmpz_ior@@Base+0x7d8>
   1b91c:	stur	xzr, [x29, #-8]
   1b920:	tbnz	w24, #31, 1b9cc <__gmpz_ior@@Base+0x14c>
   1b924:	ldrsw	x9, [x19]
   1b928:	neg	x25, x23
   1b92c:	cmp	x25, x9
   1b930:	b.gt	1bfac <__gmpz_ior@@Base+0x72c>
   1b934:	cmp	x25, #0xfe0
   1b938:	lsl	x1, x25, #3
   1b93c:	b.hi	1bfc8 <__gmpz_ior@@Base+0x748>  // b.pmore
   1b940:	add	x9, x1, #0xf
   1b944:	mov	x8, sp
   1b948:	and	x9, x9, #0xfffffffffffffff0
   1b94c:	sub	x24, x8, x9
   1b950:	mov	sp, x24
   1b954:	ldr	x8, [x26, #8]
   1b958:	ldr	x9, [x8]
   1b95c:	sub	x10, x9, #0x1
   1b960:	str	x10, [x24]
   1b964:	cbz	x9, 1ba68 <__gmpz_ior@@Base+0x1e8>
   1b968:	cmn	w23, #0x2
   1b96c:	b.gt	1bc68 <__gmpz_ior@@Base+0x3e8>
   1b970:	cmp	x8, x24
   1b974:	b.eq	1bc68 <__gmpz_ior@@Base+0x3e8>  // b.none
   1b978:	cmn	w23, #0x5
   1b97c:	b.hi	1b9a0 <__gmpz_ior@@Base+0x120>  // b.pmore
   1b980:	add	x9, x24, #0x8
   1b984:	add	x10, x8, x25, lsl #3
   1b988:	cmp	x9, x10
   1b98c:	b.cs	1bc30 <__gmpz_ior@@Base+0x3b0>  // b.hs, b.nlast
   1b990:	sub	x9, x24, x23, lsl #3
   1b994:	add	x10, x8, #0x8
   1b998:	cmp	x9, x10
   1b99c:	b.ls	1bc30 <__gmpz_ior@@Base+0x3b0>  // b.plast
   1b9a0:	mov	w9, #0x1                   	// #1
   1b9a4:	add	x10, x9, x23
   1b9a8:	lsl	x11, x9, #3
   1b9ac:	neg	x9, x10
   1b9b0:	add	x10, x24, x11
   1b9b4:	add	x8, x8, x11
   1b9b8:	ldr	x11, [x8], #8
   1b9bc:	subs	x9, x9, #0x1
   1b9c0:	str	x11, [x10], #8
   1b9c4:	b.ne	1b9b8 <__gmpz_ior@@Base+0x138>  // b.any
   1b9c8:	b	1bc68 <__gmpz_ior@@Base+0x3e8>
   1b9cc:	neg	x22, x20
   1b9d0:	cmp	x22, #0x7f0
   1b9d4:	lsl	x1, x22, #4
   1b9d8:	b.hi	1bfd8 <__gmpz_ior@@Base+0x758>  // b.pmore
   1b9dc:	add	x9, x1, #0xf
   1b9e0:	mov	x8, sp
   1b9e4:	and	x9, x9, #0xfffffffffffffff0
   1b9e8:	sub	x1, x8, x9
   1b9ec:	mov	sp, x1
   1b9f0:	ldr	x8, [x21]
   1b9f4:	add	x2, x1, x22, lsl #3
   1b9f8:	sub	x9, x8, #0x1
   1b9fc:	str	x9, [x1]
   1ba00:	cbz	x8, 1bb4c <__gmpz_ior@@Base+0x2cc>
   1ba04:	cmn	w24, #0x2
   1ba08:	b.gt	1bd7c <__gmpz_ior@@Base+0x4fc>
   1ba0c:	cmp	x21, x1
   1ba10:	b.eq	1bd7c <__gmpz_ior@@Base+0x4fc>  // b.none
   1ba14:	cmn	w24, #0x5
   1ba18:	b.hi	1ba3c <__gmpz_ior@@Base+0x1bc>  // b.pmore
   1ba1c:	add	x8, x1, #0x8
   1ba20:	add	x9, x21, x22, lsl #3
   1ba24:	cmp	x8, x9
   1ba28:	b.cs	1bd44 <__gmpz_ior@@Base+0x4c4>  // b.hs, b.nlast
   1ba2c:	sub	x8, x1, x20, lsl #3
   1ba30:	add	x9, x21, #0x8
   1ba34:	cmp	x8, x9
   1ba38:	b.ls	1bd44 <__gmpz_ior@@Base+0x4c4>  // b.plast
   1ba3c:	mov	w8, #0x1                   	// #1
   1ba40:	add	x9, x8, x20
   1ba44:	lsl	x10, x8, #3
   1ba48:	neg	x8, x9
   1ba4c:	add	x9, x1, x10
   1ba50:	add	x10, x21, x10
   1ba54:	ldr	x11, [x10], #8
   1ba58:	subs	x8, x8, #0x1
   1ba5c:	str	x11, [x9], #8
   1ba60:	b.ne	1ba54 <__gmpz_ior@@Base+0x1d4>  // b.any
   1ba64:	b	1bd7c <__gmpz_ior@@Base+0x4fc>
   1ba68:	mov	x11, xzr
   1ba6c:	mvn	x10, x23
   1ba70:	mov	w9, #0x1                   	// #1
   1ba74:	cmp	x9, x25
   1ba78:	b.ge	1bc68 <__gmpz_ior@@Base+0x3e8>  // b.tcont
   1ba7c:	add	x12, x8, x11
   1ba80:	ldr	x12, [x12, #8]
   1ba84:	add	x13, x24, x11
   1ba88:	add	x9, x9, #0x1
   1ba8c:	add	x11, x11, #0x8
   1ba90:	sub	x14, x12, #0x1
   1ba94:	sub	x10, x10, #0x1
   1ba98:	str	x14, [x13, #8]
   1ba9c:	cbz	x12, 1ba74 <__gmpz_ior@@Base+0x1f4>
   1baa0:	cmp	x8, x24
   1baa4:	b.eq	1bc68 <__gmpz_ior@@Base+0x3e8>  // b.none
   1baa8:	cmp	x9, x25
   1baac:	b.ge	1bc68 <__gmpz_ior@@Base+0x3e8>  // b.tcont
   1bab0:	sub	x12, x25, x9
   1bab4:	cmp	x12, #0x4
   1bab8:	b.cc	1bb24 <__gmpz_ior@@Base+0x2a4>  // b.lo, b.ul, b.last
   1babc:	add	x13, x24, x11
   1bac0:	add	x13, x13, #0x8
   1bac4:	add	x14, x8, x25, lsl #3
   1bac8:	cmp	x13, x14
   1bacc:	b.cs	1bae4 <__gmpz_ior@@Base+0x264>  // b.hs, b.nlast
   1bad0:	add	x14, x8, x11
   1bad4:	sub	x13, x24, x23, lsl #3
   1bad8:	add	x14, x14, #0x8
   1badc:	cmp	x13, x14
   1bae0:	b.hi	1bb24 <__gmpz_ior@@Base+0x2a4>  // b.pmore
   1bae4:	add	x13, x24, x11
   1bae8:	add	x14, x8, x11
   1baec:	and	x11, x12, #0xfffffffffffffffc
   1baf0:	and	x15, x10, #0xfffffffffffffffc
   1baf4:	add	x10, x13, #0x18
   1baf8:	add	x13, x14, #0x18
   1bafc:	add	x9, x15, x9
   1bb00:	mov	x14, x11
   1bb04:	ldp	q0, q1, [x13, #-16]
   1bb08:	add	x13, x13, #0x20
   1bb0c:	subs	x14, x14, #0x4
   1bb10:	stp	q0, q1, [x10, #-16]
   1bb14:	add	x10, x10, #0x20
   1bb18:	b.ne	1bb04 <__gmpz_ior@@Base+0x284>  // b.any
   1bb1c:	cmp	x12, x11
   1bb20:	b.eq	1bc68 <__gmpz_ior@@Base+0x3e8>  // b.none
   1bb24:	add	x10, x9, x23
   1bb28:	lsl	x11, x9, #3
   1bb2c:	neg	x9, x10
   1bb30:	add	x10, x24, x11
   1bb34:	add	x8, x8, x11
   1bb38:	ldr	x11, [x8], #8
   1bb3c:	subs	x9, x9, #0x1
   1bb40:	str	x11, [x10], #8
   1bb44:	b.ne	1bb38 <__gmpz_ior@@Base+0x2b8>  // b.any
   1bb48:	b	1bc68 <__gmpz_ior@@Base+0x3e8>
   1bb4c:	mov	x10, xzr
   1bb50:	mvn	x9, x20
   1bb54:	mov	w8, #0x1                   	// #1
   1bb58:	cmp	x8, x22
   1bb5c:	b.ge	1bd7c <__gmpz_ior@@Base+0x4fc>  // b.tcont
   1bb60:	add	x11, x21, x10
   1bb64:	ldr	x11, [x11, #8]
   1bb68:	add	x12, x1, x10
   1bb6c:	add	x8, x8, #0x1
   1bb70:	add	x10, x10, #0x8
   1bb74:	sub	x13, x11, #0x1
   1bb78:	sub	x9, x9, #0x1
   1bb7c:	str	x13, [x12, #8]
   1bb80:	cbz	x11, 1bb58 <__gmpz_ior@@Base+0x2d8>
   1bb84:	cmp	x21, x1
   1bb88:	b.eq	1bd7c <__gmpz_ior@@Base+0x4fc>  // b.none
   1bb8c:	cmp	x8, x22
   1bb90:	b.ge	1bd7c <__gmpz_ior@@Base+0x4fc>  // b.tcont
   1bb94:	sub	x11, x22, x8
   1bb98:	cmp	x11, #0x4
   1bb9c:	b.cc	1bc08 <__gmpz_ior@@Base+0x388>  // b.lo, b.ul, b.last
   1bba0:	add	x12, x1, x10
   1bba4:	add	x12, x12, #0x8
   1bba8:	add	x13, x21, x22, lsl #3
   1bbac:	cmp	x12, x13
   1bbb0:	b.cs	1bbc8 <__gmpz_ior@@Base+0x348>  // b.hs, b.nlast
   1bbb4:	add	x13, x21, x10
   1bbb8:	sub	x12, x1, x20, lsl #3
   1bbbc:	add	x13, x13, #0x8
   1bbc0:	cmp	x12, x13
   1bbc4:	b.hi	1bc08 <__gmpz_ior@@Base+0x388>  // b.pmore
   1bbc8:	add	x12, x1, x10
   1bbcc:	add	x13, x21, x10
   1bbd0:	and	x10, x11, #0xfffffffffffffffc
   1bbd4:	and	x14, x9, #0xfffffffffffffffc
   1bbd8:	add	x9, x12, #0x18
   1bbdc:	add	x12, x13, #0x18
   1bbe0:	add	x8, x14, x8
   1bbe4:	mov	x13, x10
   1bbe8:	ldp	q0, q1, [x12, #-16]
   1bbec:	add	x12, x12, #0x20
   1bbf0:	subs	x13, x13, #0x4
   1bbf4:	stp	q0, q1, [x9, #-16]
   1bbf8:	add	x9, x9, #0x20
   1bbfc:	b.ne	1bbe8 <__gmpz_ior@@Base+0x368>  // b.any
   1bc00:	cmp	x11, x10
   1bc04:	b.eq	1bd7c <__gmpz_ior@@Base+0x4fc>  // b.none
   1bc08:	add	x9, x8, x20
   1bc0c:	lsl	x10, x8, #3
   1bc10:	neg	x8, x9
   1bc14:	add	x9, x1, x10
   1bc18:	add	x10, x21, x10
   1bc1c:	ldr	x11, [x10], #8
   1bc20:	subs	x8, x8, #0x1
   1bc24:	str	x11, [x9], #8
   1bc28:	b.ne	1bc1c <__gmpz_ior@@Base+0x39c>  // b.any
   1bc2c:	b	1bd7c <__gmpz_ior@@Base+0x4fc>
   1bc30:	mvn	x10, x23
   1bc34:	and	x11, x10, #0xfffffffffffffffc
   1bc38:	add	x12, x8, #0x18
   1bc3c:	orr	x9, x11, #0x1
   1bc40:	add	x13, x24, #0x18
   1bc44:	mov	x14, x11
   1bc48:	ldp	q0, q1, [x12, #-16]
   1bc4c:	add	x12, x12, #0x20
   1bc50:	subs	x14, x14, #0x4
   1bc54:	stp	q0, q1, [x13, #-16]
   1bc58:	add	x13, x13, #0x20
   1bc5c:	b.ne	1bc48 <__gmpz_ior@@Base+0x3c8>  // b.any
   1bc60:	cmp	x11, x10
   1bc64:	b.ne	1b9a4 <__gmpz_ior@@Base+0x124>  // b.any
   1bc68:	mvn	x8, x23
   1bc6c:	ldr	x8, [x24, x8, lsl #3]
   1bc70:	cmp	x8, #0x0
   1bc74:	csetm	x8, eq  // eq = none
   1bc78:	sub	x23, x8, x23
   1bc7c:	subs	x2, x23, x20
   1bc80:	b.le	1bca0 <__gmpz_ior@@Base+0x420>
   1bc84:	lsl	x8, x20, #3
   1bc88:	add	x0, x22, x8
   1bc8c:	add	x1, x24, x8
   1bc90:	bl	ca50 <__gmpn_copyi@plt>
   1bc94:	cbz	x23, 1bd24 <__gmpz_ior@@Base+0x4a4>
   1bc98:	cbnz	x20, 1bcd0 <__gmpz_ior@@Base+0x450>
   1bc9c:	b	1bce4 <__gmpz_ior@@Base+0x464>
   1bca0:	sub	x8, x21, #0x8
   1bca4:	sub	x9, x24, #0x8
   1bca8:	subs	x10, x23, #0x1
   1bcac:	b.lt	1bd1c <__gmpz_ior@@Base+0x49c>  // b.tstop
   1bcb0:	lsl	x11, x23, #3
   1bcb4:	ldr	x12, [x8, x11]
   1bcb8:	ldr	x11, [x9, x11]
   1bcbc:	mov	x23, x10
   1bcc0:	bics	xzr, x11, x12
   1bcc4:	b.eq	1bca8 <__gmpz_ior@@Base+0x428>  // b.none
   1bcc8:	add	x23, x10, #0x1
   1bccc:	mov	x20, x23
   1bcd0:	mov	x0, x22
   1bcd4:	mov	x1, x24
   1bcd8:	mov	x2, x21
   1bcdc:	mov	x3, x20
   1bce0:	bl	c060 <__gmpn_andn_n@plt>
   1bce4:	ldr	x8, [x22]
   1bce8:	adds	x8, x8, #0x1
   1bcec:	str	x8, [x22]
   1bcf0:	b.cc	1bd3c <__gmpz_ior@@Base+0x4bc>  // b.lo, b.ul, b.last
   1bcf4:	mov	w8, #0x1                   	// #1
   1bcf8:	cmp	x8, x23
   1bcfc:	b.ge	1bd30 <__gmpz_ior@@Base+0x4b0>  // b.tcont
   1bd00:	lsl	x9, x8, #3
   1bd04:	ldr	x10, [x22, x9]
   1bd08:	add	x8, x8, #0x1
   1bd0c:	adds	x10, x10, #0x1
   1bd10:	str	x10, [x22, x9]
   1bd14:	b.cs	1bcf8 <__gmpz_ior@@Base+0x478>  // b.hs, b.nlast
   1bd18:	b	1bd3c <__gmpz_ior@@Base+0x4bc>
   1bd1c:	mov	x20, x23
   1bd20:	cbnz	x23, 1bc98 <__gmpz_ior@@Base+0x418>
   1bd24:	mov	w23, #0x1                   	// #1
   1bd28:	str	x23, [x22]
   1bd2c:	b	1bd3c <__gmpz_ior@@Base+0x4bc>
   1bd30:	mov	w8, #0x1                   	// #1
   1bd34:	str	x8, [x22, x23, lsl #3]
   1bd38:	add	x23, x23, #0x1
   1bd3c:	neg	w8, w23
   1bd40:	b	1c04c <__gmpz_ior@@Base+0x7cc>
   1bd44:	mvn	x9, x20
   1bd48:	and	x10, x9, #0xfffffffffffffffc
   1bd4c:	add	x11, x21, #0x18
   1bd50:	orr	x8, x10, #0x1
   1bd54:	add	x12, x1, #0x18
   1bd58:	mov	x13, x10
   1bd5c:	ldp	q0, q1, [x11, #-16]
   1bd60:	add	x11, x11, #0x20
   1bd64:	subs	x13, x13, #0x4
   1bd68:	stp	q0, q1, [x12, #-16]
   1bd6c:	add	x12, x12, #0x20
   1bd70:	b.ne	1bd5c <__gmpz_ior@@Base+0x4dc>  // b.any
   1bd74:	cmp	x10, x9
   1bd78:	b.ne	1ba40 <__gmpz_ior@@Base+0x1c0>  // b.any
   1bd7c:	ldr	x8, [x26, #8]
   1bd80:	ldr	x9, [x8]
   1bd84:	sub	x10, x9, #0x1
   1bd88:	str	x10, [x2]
   1bd8c:	cbz	x9, 1bdfc <__gmpz_ior@@Base+0x57c>
   1bd90:	cmn	w24, #0x2
   1bd94:	b.gt	1bf34 <__gmpz_ior@@Base+0x6b4>
   1bd98:	cmp	x8, x2
   1bd9c:	b.eq	1bf34 <__gmpz_ior@@Base+0x6b4>  // b.none
   1bda0:	cmn	w24, #0x5
   1bda4:	b.hi	1bdcc <__gmpz_ior@@Base+0x54c>  // b.pmore
   1bda8:	sub	x13, x1, x20, lsl #3
   1bdac:	add	x9, x13, #0x8
   1bdb0:	add	x10, x8, x22, lsl #3
   1bdb4:	cmp	x9, x10
   1bdb8:	b.cs	1befc <__gmpz_ior@@Base+0x67c>  // b.hs, b.nlast
   1bdbc:	sub	x9, x1, x20, lsl #4
   1bdc0:	add	x10, x8, #0x8
   1bdc4:	cmp	x9, x10
   1bdc8:	b.ls	1befc <__gmpz_ior@@Base+0x67c>  // b.plast
   1bdcc:	mov	w9, #0x1                   	// #1
   1bdd0:	add	x10, x9, x20
   1bdd4:	lsl	x11, x9, #3
   1bdd8:	neg	x9, x10
   1bddc:	sub	x10, x11, x20, lsl #3
   1bde0:	add	x10, x1, x10
   1bde4:	add	x8, x8, x11
   1bde8:	ldr	x11, [x8], #8
   1bdec:	subs	x9, x9, #0x1
   1bdf0:	str	x11, [x10], #8
   1bdf4:	b.ne	1bde8 <__gmpz_ior@@Base+0x568>  // b.any
   1bdf8:	b	1bf34 <__gmpz_ior@@Base+0x6b4>
   1bdfc:	mov	w9, #0x20                  	// #32
   1be00:	sub	x12, x9, x20, lsl #3
   1be04:	add	x9, x12, x1
   1be08:	mov	x10, xzr
   1be0c:	mvn	x11, x20
   1be10:	sub	x13, x9, #0x20
   1be14:	mov	w9, #0x1                   	// #1
   1be18:	cmp	x9, x22
   1be1c:	b.ge	1bf34 <__gmpz_ior@@Base+0x6b4>  // b.tcont
   1be20:	add	x14, x8, x10
   1be24:	ldr	x14, [x14, #8]
   1be28:	add	x15, x13, x10
   1be2c:	add	x9, x9, #0x1
   1be30:	add	x10, x10, #0x8
   1be34:	sub	x16, x14, #0x1
   1be38:	sub	x11, x11, #0x1
   1be3c:	str	x16, [x15, #8]
   1be40:	cbz	x14, 1be18 <__gmpz_ior@@Base+0x598>
   1be44:	cmp	x8, x2
   1be48:	b.eq	1bf34 <__gmpz_ior@@Base+0x6b4>  // b.none
   1be4c:	cmp	x9, x22
   1be50:	b.ge	1bf34 <__gmpz_ior@@Base+0x6b4>  // b.tcont
   1be54:	sub	x13, x22, x9
   1be58:	cmp	x13, #0x4
   1be5c:	b.cc	1bed0 <__gmpz_ior@@Base+0x650>  // b.lo, b.ul, b.last
   1be60:	add	x14, x12, x1
   1be64:	add	x14, x14, x10
   1be68:	sub	x14, x14, #0x18
   1be6c:	add	x15, x8, x22, lsl #3
   1be70:	cmp	x14, x15
   1be74:	b.cs	1be8c <__gmpz_ior@@Base+0x60c>  // b.hs, b.nlast
   1be78:	add	x15, x8, x10
   1be7c:	sub	x14, x1, x20, lsl #4
   1be80:	add	x15, x15, #0x8
   1be84:	cmp	x14, x15
   1be88:	b.hi	1bed0 <__gmpz_ior@@Base+0x650>  // b.pmore
   1be8c:	add	x14, x12, x1
   1be90:	add	x15, x8, x10
   1be94:	and	x12, x13, #0xfffffffffffffffc
   1be98:	and	x16, x11, #0xfffffffffffffffc
   1be9c:	add	x11, x14, x10
   1bea0:	add	x10, x15, #0x18
   1bea4:	sub	x11, x11, #0x8
   1bea8:	add	x9, x16, x9
   1beac:	mov	x14, x12
   1beb0:	ldp	q0, q1, [x10, #-16]
   1beb4:	add	x10, x10, #0x20
   1beb8:	subs	x14, x14, #0x4
   1bebc:	stp	q0, q1, [x11, #-16]
   1bec0:	add	x11, x11, #0x20
   1bec4:	b.ne	1beb0 <__gmpz_ior@@Base+0x630>  // b.any
   1bec8:	cmp	x13, x12
   1becc:	b.eq	1bf34 <__gmpz_ior@@Base+0x6b4>  // b.none
   1bed0:	add	x10, x9, x20
   1bed4:	lsl	x11, x9, #3
   1bed8:	neg	x9, x10
   1bedc:	sub	x10, x11, x20, lsl #3
   1bee0:	add	x10, x1, x10
   1bee4:	add	x8, x8, x11
   1bee8:	ldr	x11, [x8], #8
   1beec:	subs	x9, x9, #0x1
   1bef0:	str	x11, [x10], #8
   1bef4:	b.ne	1bee8 <__gmpz_ior@@Base+0x668>  // b.any
   1bef8:	b	1bf34 <__gmpz_ior@@Base+0x6b4>
   1befc:	mvn	x10, x20
   1bf00:	and	x11, x10, #0xfffffffffffffffc
   1bf04:	add	x12, x8, #0x18
   1bf08:	orr	x9, x11, #0x1
   1bf0c:	add	x13, x13, #0x18
   1bf10:	mov	x14, x11
   1bf14:	ldp	q0, q1, [x12, #-16]
   1bf18:	add	x12, x12, #0x20
   1bf1c:	subs	x14, x14, #0x4
   1bf20:	stp	q0, q1, [x13, #-16]
   1bf24:	add	x13, x13, #0x20
   1bf28:	b.ne	1bf14 <__gmpz_ior@@Base+0x694>  // b.any
   1bf2c:	cmp	x11, x10
   1bf30:	b.ne	1bdd0 <__gmpz_ior@@Base+0x550>  // b.any
   1bf34:	sub	x8, x1, x20, lsl #3
   1bf38:	sub	x9, x1, x20, lsl #4
   1bf3c:	mov	x10, xzr
   1bf40:	sub	x8, x8, #0x8
   1bf44:	sub	x9, x9, #0x8
   1bf48:	add	x21, x22, x10
   1bf4c:	mov	x23, x10
   1bf50:	cmp	x21, #0x1
   1bf54:	b.lt	1bf70 <__gmpz_ior@@Base+0x6f0>  // b.tstop
   1bf58:	lsl	x10, x23, #3
   1bf5c:	ldr	x11, [x8, x10]
   1bf60:	ldr	x10, [x9, x10]
   1bf64:	and	x11, x10, x11
   1bf68:	sub	x10, x23, #0x1
   1bf6c:	cbz	x11, 1bf48 <__gmpz_ior@@Base+0x6c8>
   1bf70:	ldrsw	x8, [x19]
   1bf74:	cmp	x21, x8
   1bf78:	b.ge	1bfe8 <__gmpz_ior@@Base+0x768>  // b.tcont
   1bf7c:	ldr	x22, [x19, #8]
   1bf80:	cmp	x20, x23
   1bf84:	b.ne	1c018 <__gmpz_ior@@Base+0x798>  // b.any
   1bf88:	mov	w8, #0x1                   	// #1
   1bf8c:	str	x8, [x22]
   1bf90:	mov	w8, #0xffffffff            	// #-1
   1bf94:	b	1c04c <__gmpz_ior@@Base+0x7cc>
   1bf98:	mov	x0, x19
   1bf9c:	mov	x1, x20
   1bfa0:	bl	c080 <__gmpz_realloc@plt>
   1bfa4:	mov	x22, x0
   1bfa8:	b	1b8e8 <__gmpz_ior@@Base+0x68>
   1bfac:	mov	x0, x19
   1bfb0:	mov	x1, x25
   1bfb4:	mov	x21, x8
   1bfb8:	bl	c080 <__gmpz_realloc@plt>
   1bfbc:	ldr	x21, [x21, #8]
   1bfc0:	mov	x22, x0
   1bfc4:	b	1b934 <__gmpz_ior@@Base+0xb4>
   1bfc8:	sub	x0, x29, #0x8
   1bfcc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1bfd0:	mov	x24, x0
   1bfd4:	b	1b954 <__gmpz_ior@@Base+0xd4>
   1bfd8:	sub	x0, x29, #0x8
   1bfdc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1bfe0:	mov	x1, x0
   1bfe4:	b	1b9f0 <__gmpz_ior@@Base+0x170>
   1bfe8:	add	x8, x22, x23
   1bfec:	add	x8, x8, #0x1
   1bff0:	mov	x0, x19
   1bff4:	mov	x22, x1
   1bff8:	mov	x1, x8
   1bffc:	mov	x24, x2
   1c000:	bl	c080 <__gmpz_realloc@plt>
   1c004:	mov	x2, x24
   1c008:	mov	x1, x22
   1c00c:	mov	x22, x0
   1c010:	cmp	x20, x23
   1c014:	b.eq	1bf88 <__gmpz_ior@@Base+0x708>  // b.none
   1c018:	mov	x0, x22
   1c01c:	mov	x3, x21
   1c020:	bl	c270 <__gmpn_and_n@plt>
   1c024:	sub	x8, x22, x20, lsl #3
   1c028:	str	xzr, [x8, x23, lsl #3]
   1c02c:	ldr	x9, [x22]
   1c030:	adds	x9, x9, #0x1
   1c034:	str	x9, [x22], #8
   1c038:	b.cs	1c02c <__gmpz_ior@@Base+0x7ac>  // b.hs, b.nlast
   1c03c:	lsl	x9, x23, #3
   1c040:	ldr	w8, [x8, x9]
   1c044:	sub	w8, w20, w8
   1c048:	sub	w8, w8, w23
   1c04c:	str	w8, [x19, #4]
   1c050:	ldur	x0, [x29, #-8]
   1c054:	cbnz	x0, 1c074 <__gmpz_ior@@Base+0x7f4>
   1c058:	mov	sp, x29
   1c05c:	ldp	x20, x19, [sp, #64]
   1c060:	ldp	x22, x21, [sp, #48]
   1c064:	ldp	x24, x23, [sp, #32]
   1c068:	ldp	x26, x25, [sp, #16]
   1c06c:	ldp	x29, x30, [sp], #80
   1c070:	ret
   1c074:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1c078:	b	1c058 <__gmpz_ior@@Base+0x7d8>

000000000001c07c <__gmpz_init_set@@Base>:
   1c07c:	stp	x29, x30, [sp, #-48]!
   1c080:	stp	x22, x21, [sp, #16]
   1c084:	stp	x20, x19, [sp, #32]
   1c088:	ldrsw	x22, [x1, #4]
   1c08c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1c090:	mov	x20, x0
   1c094:	mov	x29, sp
   1c098:	cmp	x22, #0x0
   1c09c:	cneg	x21, x22, mi  // mi = first
   1c0a0:	cmp	x21, #0x1
   1c0a4:	csinc	x8, x21, xzr, gt
   1c0a8:	str	w8, [x0]
   1c0ac:	ldr	x9, [x9, #3840]
   1c0b0:	sbfiz	x0, x8, #3, #32
   1c0b4:	mov	x19, x1
   1c0b8:	ldr	x9, [x9]
   1c0bc:	blr	x9
   1c0c0:	str	x0, [x20, #8]
   1c0c4:	ldr	x1, [x19, #8]
   1c0c8:	mov	x2, x21
   1c0cc:	bl	ca50 <__gmpn_copyi@plt>
   1c0d0:	str	w22, [x20, #4]
   1c0d4:	ldp	x20, x19, [sp, #32]
   1c0d8:	ldp	x22, x21, [sp, #16]
   1c0dc:	ldp	x29, x30, [sp], #48
   1c0e0:	ret

000000000001c0e4 <__gmpz_init_set_d@@Base>:
   1c0e4:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1c0e8:	add	x8, x8, #0x560
   1c0ec:	stp	xzr, x8, [x0]
   1c0f0:	b	c890 <__gmpz_set_d@plt>

000000000001c0f4 <__gmpz_init_set_si@@Base>:
   1c0f4:	stp	x29, x30, [sp, #-32]!
   1c0f8:	mov	w8, #0x1                   	// #1
   1c0fc:	stp	x20, x19, [sp, #16]
   1c100:	str	w8, [x0]
   1c104:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1c108:	ldr	x8, [x8, #3840]
   1c10c:	mov	x19, x0
   1c110:	mov	w0, #0x8                   	// #8
   1c114:	mov	x29, sp
   1c118:	ldr	x8, [x8]
   1c11c:	mov	x20, x1
   1c120:	blr	x8
   1c124:	cmp	x20, #0x0
   1c128:	cneg	x8, x20, mi  // mi = first
   1c12c:	cset	w9, ne  // ne = any
   1c130:	csetm	w10, ne  // ne = any
   1c134:	str	x0, [x19, #8]
   1c138:	str	x8, [x0]
   1c13c:	csel	w8, w9, w10, ge  // ge = tcont
   1c140:	str	w8, [x19, #4]
   1c144:	ldp	x20, x19, [sp, #16]
   1c148:	ldp	x29, x30, [sp], #32
   1c14c:	ret

000000000001c150 <__gmpz_init_set_str@@Base>:
   1c150:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1c154:	add	x8, x8, #0x568
   1c158:	stp	xzr, x8, [x0]
   1c15c:	b	c0d0 <__gmpz_set_str@plt>

000000000001c160 <__gmpz_init_set_ui@@Base>:
   1c160:	stp	x29, x30, [sp, #-32]!
   1c164:	mov	w8, #0x1                   	// #1
   1c168:	stp	x20, x19, [sp, #16]
   1c16c:	str	w8, [x0]
   1c170:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1c174:	ldr	x8, [x8, #3840]
   1c178:	mov	x19, x0
   1c17c:	mov	w0, #0x8                   	// #8
   1c180:	mov	x29, sp
   1c184:	ldr	x8, [x8]
   1c188:	mov	x20, x1
   1c18c:	blr	x8
   1c190:	cmp	x20, #0x0
   1c194:	cset	w8, ne  // ne = any
   1c198:	str	x0, [x19, #8]
   1c19c:	str	x20, [x0]
   1c1a0:	str	w8, [x19, #4]
   1c1a4:	ldp	x20, x19, [sp, #16]
   1c1a8:	ldp	x29, x30, [sp], #32
   1c1ac:	ret

000000000001c1b0 <__gmpz_jacobi@@Base>:
   1c1b0:	stp	x29, x30, [sp, #-80]!
   1c1b4:	stp	x26, x25, [sp, #16]
   1c1b8:	stp	x24, x23, [sp, #32]
   1c1bc:	stp	x22, x21, [sp, #48]
   1c1c0:	stp	x20, x19, [sp, #64]
   1c1c4:	mov	x29, sp
   1c1c8:	sub	sp, sp, #0x10
   1c1cc:	ldr	x8, [x0, #8]
   1c1d0:	ldrsw	x10, [x0, #4]
   1c1d4:	ldrsw	x13, [x1, #4]
   1c1d8:	ldr	x9, [x8]
   1c1dc:	cbz	w13, 1c2d4 <__gmpz_jacobi@@Base+0x124>
   1c1e0:	ldr	x3, [x1, #8]
   1c1e4:	ldr	x11, [x3]
   1c1e8:	cbz	w10, 1c2f0 <__gmpz_jacobi@@Base+0x140>
   1c1ec:	orr	w12, w11, w9
   1c1f0:	tbz	w12, #0, 1c30c <__gmpz_jacobi@@Base+0x15c>
   1c1f4:	and	w12, w10, w13
   1c1f8:	cmp	w13, #0x0
   1c1fc:	lsr	w12, w12, #30
   1c200:	cneg	x4, x13, lt  // lt = tstop
   1c204:	cbz	x11, 1c4c4 <__gmpz_jacobi@@Base+0x314>
   1c208:	rbit	x13, x11
   1c20c:	clz	x20, x13
   1c210:	and	w12, w12, #0x2
   1c214:	cmp	x4, #0x2
   1c218:	lsr	x19, x11, x20
   1c21c:	b.lt	1c24c <__gmpz_jacobi@@Base+0x9c>  // b.tstop
   1c220:	cbz	w20, 1c24c <__gmpz_jacobi@@Base+0x9c>
   1c224:	ldr	x11, [x3, #8]
   1c228:	neg	x13, x20
   1c22c:	cmp	x4, #0x2
   1c230:	lsl	x13, x11, x13
   1c234:	orr	x19, x13, x19
   1c238:	b.ne	1c24c <__gmpz_jacobi@@Base+0x9c>  // b.any
   1c23c:	lsr	x11, x11, x20
   1c240:	cmp	x11, #0x0
   1c244:	mov	w11, #0x1                   	// #1
   1c248:	cinc	x4, x11, ne  // ne = any
   1c24c:	and	w11, w19, w10, asr #31
   1c250:	cmp	w10, #0x0
   1c254:	eor	w26, w11, w12
   1c258:	cneg	x10, x10, lt  // lt = tstop
   1c25c:	cbz	x9, 1c4d4 <__gmpz_jacobi@@Base+0x324>
   1c260:	cmp	x10, x4
   1c264:	b.ge	1c330 <__gmpz_jacobi@@Base+0x180>  // b.tcont
   1c268:	rbit	x11, x9
   1c26c:	clz	x20, x11
   1c270:	cmp	x10, #0x2
   1c274:	lsr	x21, x9, x20
   1c278:	b.lt	1c2a8 <__gmpz_jacobi@@Base+0xf8>  // b.tstop
   1c27c:	cbz	w20, 1c2a8 <__gmpz_jacobi@@Base+0xf8>
   1c280:	ldr	x9, [x8, #8]
   1c284:	neg	x11, x20
   1c288:	cmp	x10, #0x2
   1c28c:	lsl	x11, x9, x11
   1c290:	orr	x21, x11, x21
   1c294:	b.ne	1c2a8 <__gmpz_jacobi@@Base+0xf8>  // b.any
   1c298:	lsr	x9, x9, x20
   1c29c:	cmp	x9, #0x0
   1c2a0:	mov	w9, #0x1                   	// #1
   1c2a4:	cinc	x10, x9, ne  // ne = any
   1c2a8:	and	w9, w21, w19
   1c2ac:	eor	w26, w26, w9
   1c2b0:	mov	x22, x10
   1c2b4:	mov	x23, x8
   1c2b8:	cmp	x22, #0x1
   1c2bc:	b.eq	1c350 <__gmpz_jacobi@@Base+0x1a0>  // b.none
   1c2c0:	cmp	x4, x22, lsl #1
   1c2c4:	stur	xzr, [x29, #-8]
   1c2c8:	b.ge	1c378 <__gmpz_jacobi@@Base+0x1c8>  // b.tcont
   1c2cc:	lsl	x1, x22, #4
   1c2d0:	b	1c380 <__gmpz_jacobi@@Base+0x1d0>
   1c2d4:	cmp	w10, #0x1
   1c2d8:	b.eq	1c2e4 <__gmpz_jacobi@@Base+0x134>  // b.none
   1c2dc:	cmn	w10, #0x1
   1c2e0:	b.ne	1c30c <__gmpz_jacobi@@Base+0x15c>  // b.any
   1c2e4:	cmp	x9, #0x1
   1c2e8:	cset	w19, eq  // eq = none
   1c2ec:	b	1c310 <__gmpz_jacobi@@Base+0x160>
   1c2f0:	cmp	w13, #0x1
   1c2f4:	b.eq	1c300 <__gmpz_jacobi@@Base+0x150>  // b.none
   1c2f8:	cmn	w13, #0x1
   1c2fc:	b.ne	1c30c <__gmpz_jacobi@@Base+0x15c>  // b.any
   1c300:	cmp	x11, #0x1
   1c304:	cset	w19, eq  // eq = none
   1c308:	b	1c310 <__gmpz_jacobi@@Base+0x160>
   1c30c:	mov	w19, wzr
   1c310:	mov	w0, w19
   1c314:	mov	sp, x29
   1c318:	ldp	x20, x19, [sp, #64]
   1c31c:	ldp	x22, x21, [sp, #48]
   1c320:	ldp	x24, x23, [sp, #32]
   1c324:	ldp	x26, x25, [sp, #16]
   1c328:	ldp	x29, x30, [sp], #80
   1c32c:	ret
   1c330:	mov	x21, x19
   1c334:	mov	x19, x9
   1c338:	mov	x22, x4
   1c33c:	mov	x4, x10
   1c340:	mov	x23, x3
   1c344:	mov	x3, x8
   1c348:	cmp	x22, #0x1
   1c34c:	b.ne	1c2c0 <__gmpz_jacobi@@Base+0x110>  // b.any
   1c350:	lsr	x8, x19, #1
   1c354:	eor	w8, w8, w19
   1c358:	and	w8, w8, w20, lsl #1
   1c35c:	cmp	x21, #0x1
   1c360:	eor	w20, w8, w26
   1c364:	b.ne	1c410 <__gmpz_jacobi@@Base+0x260>  // b.any
   1c368:	and	w8, w20, #0x2
   1c36c:	mov	w9, #0x1                   	// #1
   1c370:	sub	w19, w9, w8
   1c374:	b	1c310 <__gmpz_jacobi@@Base+0x160>
   1c378:	lsl	x8, x4, #3
   1c37c:	add	x1, x8, #0x8
   1c380:	mov	w8, #0x7f00                	// #32512
   1c384:	cmp	x1, x8
   1c388:	b.hi	1c4e4 <__gmpz_jacobi@@Base+0x334>  // b.pmore
   1c38c:	add	x9, x1, #0xf
   1c390:	mov	x8, sp
   1c394:	and	x9, x9, #0xfffffffffffffff0
   1c398:	sub	x24, x8, x9
   1c39c:	mov	sp, x24
   1c3a0:	cmp	x4, x22
   1c3a4:	add	x25, x24, x22, lsl #3
   1c3a8:	b.le	1c434 <__gmpz_jacobi@@Base+0x284>
   1c3ac:	mov	x0, x25
   1c3b0:	mov	x1, x24
   1c3b4:	mov	x2, xzr
   1c3b8:	mov	x5, x23
   1c3bc:	mov	x6, x22
   1c3c0:	bl	bf00 <__gmpn_tdiv_qr@plt>
   1c3c4:	cbz	w20, 1c448 <__gmpz_jacobi@@Base+0x298>
   1c3c8:	lsr	x8, x19, #1
   1c3cc:	eor	w8, w8, w19
   1c3d0:	and	w8, w8, w20, lsl #1
   1c3d4:	mov	x0, x25
   1c3d8:	mov	x1, x23
   1c3dc:	mov	x2, x22
   1c3e0:	mov	w3, w20
   1c3e4:	eor	w26, w8, w26
   1c3e8:	bl	c1a0 <__gmpn_rshift@plt>
   1c3ec:	lsl	x8, x22, #3
   1c3f0:	sub	x8, x8, #0x8
   1c3f4:	ldr	x9, [x24, x8]
   1c3f8:	ldr	x8, [x25, x8]
   1c3fc:	orr	x8, x8, x9
   1c400:	cmp	x8, #0x0
   1c404:	cset	w8, eq  // eq = none
   1c408:	sub	x22, x22, x8
   1c40c:	b	1c458 <__gmpz_jacobi@@Base+0x2a8>
   1c410:	cmp	x4, #0x2
   1c414:	b.lt	1c4ac <__gmpz_jacobi@@Base+0x2fc>  // b.tstop
   1c418:	cmp	x4, #0x28
   1c41c:	b.lt	1c490 <__gmpz_jacobi@@Base+0x2e0>  // b.tstop
   1c420:	mov	x0, x3
   1c424:	mov	x1, x4
   1c428:	mov	x2, x21
   1c42c:	bl	c3e0 <__gmpn_mod_1@plt>
   1c430:	b	1c4a8 <__gmpz_jacobi@@Base+0x2f8>
   1c434:	mov	x0, x24
   1c438:	mov	x1, x3
   1c43c:	mov	x2, x22
   1c440:	bl	ca50 <__gmpn_copyi@plt>
   1c444:	cbnz	w20, 1c3c8 <__gmpz_jacobi@@Base+0x218>
   1c448:	mov	x0, x25
   1c44c:	mov	x1, x23
   1c450:	mov	x2, x22
   1c454:	bl	ca50 <__gmpn_copyi@plt>
   1c458:	ldr	w8, [x24]
   1c45c:	and	w3, w21, #0x2
   1c460:	bfxil	w3, w26, #1, #1
   1c464:	mov	x0, x24
   1c468:	bfi	w3, w8, #2, #2
   1c46c:	mov	x1, x25
   1c470:	mov	x2, x22
   1c474:	bl	cea0 <__gmpn_jacobi_n@plt>
   1c478:	ldur	x8, [x29, #-8]
   1c47c:	mov	w19, w0
   1c480:	cbz	x8, 1c310 <__gmpz_jacobi@@Base+0x160>
   1c484:	mov	x0, x8
   1c488:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1c48c:	b	1c310 <__gmpz_jacobi@@Base+0x160>
   1c490:	mov	x0, x3
   1c494:	mov	x1, x4
   1c498:	mov	x2, x21
   1c49c:	mov	x3, xzr
   1c4a0:	eor	w20, w20, w21
   1c4a4:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c4a8:	mov	x19, x0
   1c4ac:	mov	x0, x19
   1c4b0:	mov	x1, x21
   1c4b4:	mov	w2, w20
   1c4b8:	bl	c730 <__gmpn_jacobi_base@plt>
   1c4bc:	mov	w19, w0
   1c4c0:	b	1c310 <__gmpz_jacobi@@Base+0x160>
   1c4c4:	ldr	x11, [x3, #8]!
   1c4c8:	sub	x4, x4, #0x1
   1c4cc:	cbnz	x11, 1c208 <__gmpz_jacobi@@Base+0x58>
   1c4d0:	b	1c4c4 <__gmpz_jacobi@@Base+0x314>
   1c4d4:	ldr	x9, [x8, #8]!
   1c4d8:	sub	x10, x10, #0x1
   1c4dc:	cbnz	x9, 1c260 <__gmpz_jacobi@@Base+0xb0>
   1c4e0:	b	1c4d4 <__gmpz_jacobi@@Base+0x324>
   1c4e4:	sub	x0, x29, #0x8
   1c4e8:	mov	x24, x3
   1c4ec:	mov	x25, x4
   1c4f0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1c4f4:	mov	x4, x25
   1c4f8:	mov	x3, x24
   1c4fc:	mov	x24, x0
   1c500:	b	1c3a0 <__gmpz_jacobi@@Base+0x1f0>

000000000001c504 <__gmpz_si_kronecker@@Base>:
   1c504:	stp	x29, x30, [sp, #-48]!
   1c508:	stp	x20, x19, [sp, #32]
   1c50c:	ldrsw	x11, [x1, #4]
   1c510:	mov	x8, x0
   1c514:	str	x21, [sp, #16]
   1c518:	mov	x29, sp
   1c51c:	cbz	w11, 1c54c <__gmpz_si_kronecker@@Base+0x48>
   1c520:	ldr	x0, [x1, #8]
   1c524:	lsr	x10, x8, #63
   1c528:	and	w9, w10, w11, lsr #31
   1c52c:	cmp	x11, #0x0
   1c530:	ldr	x20, [x0]
   1c534:	lsl	w9, w9, #1
   1c538:	cneg	x1, x11, mi  // mi = first
   1c53c:	tbnz	w20, #0, 1c564 <__gmpz_si_kronecker@@Base+0x60>
   1c540:	tbnz	w8, #0, 1c5b0 <__gmpz_si_kronecker@@Base+0xac>
   1c544:	mov	w0, wzr
   1c548:	b	1c63c <__gmpz_si_kronecker@@Base+0x138>
   1c54c:	cmp	x8, #0x1
   1c550:	cset	w9, eq  // eq = none
   1c554:	cmn	x8, #0x1
   1c558:	cset	w8, eq  // eq = none
   1c55c:	orr	w0, w9, w8
   1c560:	b	1c63c <__gmpz_si_kronecker@@Base+0x138>
   1c564:	and	w10, w20, w10, lsl #1
   1c568:	cmp	x8, #0x0
   1c56c:	cneg	x19, x8, mi  // mi = first
   1c570:	eor	w21, w9, w10
   1c574:	tbnz	w19, #0, 1c598 <__gmpz_si_kronecker@@Base+0x94>
   1c578:	cbz	x19, 1c628 <__gmpz_si_kronecker@@Base+0x124>
   1c57c:	rbit	x8, x8
   1c580:	lsr	x9, x20, #1
   1c584:	clz	x8, x8
   1c588:	eor	w9, w9, w20
   1c58c:	lsr	x19, x19, x8
   1c590:	and	w8, w9, w8, lsl #1
   1c594:	eor	w21, w8, w21
   1c598:	cmp	x19, #0x1
   1c59c:	b.ne	1c5e8 <__gmpz_si_kronecker@@Base+0xe4>  // b.any
   1c5a0:	and	w8, w21, #0x2
   1c5a4:	mov	w9, #0x1                   	// #1
   1c5a8:	sub	w0, w9, w8
   1c5ac:	b	1c63c <__gmpz_si_kronecker@@Base+0x138>
   1c5b0:	cbz	x20, 1c64c <__gmpz_si_kronecker@@Base+0x148>
   1c5b4:	tbnz	w20, #0, 1c5d0 <__gmpz_si_kronecker@@Base+0xcc>
   1c5b8:	mov	x11, #0x8000000000000000    	// #-9223372036854775808
   1c5bc:	cmp	x20, x11
   1c5c0:	b.eq	1c65c <__gmpz_si_kronecker@@Base+0x158>  // b.none
   1c5c4:	rbit	x11, x20
   1c5c8:	clz	x11, x11
   1c5cc:	lsr	x20, x20, x11
   1c5d0:	and	w10, w20, w10, lsl #1
   1c5d4:	cmp	x8, #0x0
   1c5d8:	eor	w21, w9, w10
   1c5dc:	cneg	x19, x8, mi  // mi = first
   1c5e0:	cmp	x19, #0x1
   1c5e4:	b.eq	1c5a0 <__gmpz_si_kronecker@@Base+0x9c>  // b.none
   1c5e8:	cmp	x1, #0x28
   1c5ec:	b.lt	1c5fc <__gmpz_si_kronecker@@Base+0xf8>  // b.tstop
   1c5f0:	mov	x2, x19
   1c5f4:	bl	c3e0 <__gmpn_mod_1@plt>
   1c5f8:	b	1c60c <__gmpz_si_kronecker@@Base+0x108>
   1c5fc:	mov	x2, x19
   1c600:	mov	x3, xzr
   1c604:	eor	w21, w21, w19
   1c608:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c60c:	and	w8, w20, w19
   1c610:	eor	w2, w21, w8
   1c614:	mov	x1, x19
   1c618:	ldp	x20, x19, [sp, #32]
   1c61c:	ldr	x21, [sp, #16]
   1c620:	ldp	x29, x30, [sp], #48
   1c624:	b	c730 <__gmpn_jacobi_base@plt>
   1c628:	cmp	x1, #0x1
   1c62c:	cset	w8, eq  // eq = none
   1c630:	cmp	x20, #0x1
   1c634:	cset	w9, eq  // eq = none
   1c638:	and	w0, w8, w9
   1c63c:	ldp	x20, x19, [sp, #32]
   1c640:	ldr	x21, [sp, #16]
   1c644:	ldp	x29, x30, [sp], #48
   1c648:	ret
   1c64c:	ldr	x20, [x0, #8]!
   1c650:	sub	x1, x1, #0x1
   1c654:	cbnz	x20, 1c5b4 <__gmpz_si_kronecker@@Base+0xb0>
   1c658:	b	1c64c <__gmpz_si_kronecker@@Base+0x148>
   1c65c:	cmp	x1, #0x1
   1c660:	b.ne	1c674 <__gmpz_si_kronecker@@Base+0x170>  // b.any
   1c664:	eor	w8, w8, w8, lsr #1
   1c668:	and	w8, w8, #0x2
   1c66c:	eor	w8, w9, w8
   1c670:	b	1c5a4 <__gmpz_si_kronecker@@Base+0xa0>
   1c674:	ldr	x11, [x0, #8]
   1c678:	lsl	x20, x11, #1
   1c67c:	b	1c5d0 <__gmpz_si_kronecker@@Base+0xcc>

000000000001c680 <__gmpz_ui_kronecker@@Base>:
   1c680:	stp	x29, x30, [sp, #-48]!
   1c684:	stp	x20, x19, [sp, #32]
   1c688:	ldr	w8, [x1, #4]
   1c68c:	mov	x19, x0
   1c690:	str	x21, [sp, #16]
   1c694:	mov	x29, sp
   1c698:	cmp	w8, #0x0
   1c69c:	cneg	w8, w8, mi  // mi = first
   1c6a0:	cbz	w8, 1c6bc <__gmpz_ui_kronecker@@Base+0x3c>
   1c6a4:	ldr	x0, [x1, #8]
   1c6a8:	ldr	x20, [x0]
   1c6ac:	tbnz	w20, #0, 1c6c8 <__gmpz_ui_kronecker@@Base+0x48>
   1c6b0:	tbnz	w19, #0, 1c708 <__gmpz_ui_kronecker@@Base+0x88>
   1c6b4:	mov	w0, wzr
   1c6b8:	b	1c768 <__gmpz_ui_kronecker@@Base+0xe8>
   1c6bc:	cmp	x19, #0x1
   1c6c0:	cset	w0, eq  // eq = none
   1c6c4:	b	1c768 <__gmpz_ui_kronecker@@Base+0xe8>
   1c6c8:	cbz	x19, 1c754 <__gmpz_ui_kronecker@@Base+0xd4>
   1c6cc:	tbnz	w19, #0, 1c738 <__gmpz_ui_kronecker@@Base+0xb8>
   1c6d0:	rbit	x9, x19
   1c6d4:	lsr	x10, x20, #1
   1c6d8:	clz	x9, x9
   1c6dc:	eor	w10, w10, w20
   1c6e0:	lsr	x19, x19, x9
   1c6e4:	and	w21, w10, w9, lsl #1
   1c6e8:	cmp	x19, #0x1
   1c6ec:	b.eq	1c744 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1c6f0:	cmp	w8, #0x28
   1c6f4:	sxtw	x1, w8
   1c6f8:	b.lt	1c778 <__gmpz_ui_kronecker@@Base+0xf8>  // b.tstop
   1c6fc:	mov	x2, x19
   1c700:	bl	c3e0 <__gmpn_mod_1@plt>
   1c704:	b	1c788 <__gmpz_ui_kronecker@@Base+0x108>
   1c708:	cbz	x20, 1c7a4 <__gmpz_ui_kronecker@@Base+0x124>
   1c70c:	tbnz	w20, #0, 1c738 <__gmpz_ui_kronecker@@Base+0xb8>
   1c710:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   1c714:	cmp	x20, x9
   1c718:	b.eq	1c7b4 <__gmpz_ui_kronecker@@Base+0x134>  // b.none
   1c71c:	rbit	x9, x20
   1c720:	clz	x9, x9
   1c724:	mov	w21, wzr
   1c728:	lsr	x20, x20, x9
   1c72c:	cmp	x19, #0x1
   1c730:	b.eq	1c744 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1c734:	b	1c6f0 <__gmpz_ui_kronecker@@Base+0x70>
   1c738:	mov	w21, wzr
   1c73c:	cmp	x19, #0x1
   1c740:	b.ne	1c6f0 <__gmpz_ui_kronecker@@Base+0x70>  // b.any
   1c744:	and	w8, w21, #0x2
   1c748:	mov	w9, #0x1                   	// #1
   1c74c:	sub	w0, w9, w8
   1c750:	b	1c768 <__gmpz_ui_kronecker@@Base+0xe8>
   1c754:	cmp	w8, #0x1
   1c758:	cset	w8, eq  // eq = none
   1c75c:	cmp	x20, #0x1
   1c760:	cset	w9, eq  // eq = none
   1c764:	and	w0, w8, w9
   1c768:	ldp	x20, x19, [sp, #32]
   1c76c:	ldr	x21, [sp, #16]
   1c770:	ldp	x29, x30, [sp], #48
   1c774:	ret
   1c778:	mov	x2, x19
   1c77c:	mov	x3, xzr
   1c780:	eor	w21, w21, w19
   1c784:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c788:	and	w8, w19, w20
   1c78c:	eor	w2, w21, w8
   1c790:	mov	x1, x19
   1c794:	ldp	x20, x19, [sp, #32]
   1c798:	ldr	x21, [sp, #16]
   1c79c:	ldp	x29, x30, [sp], #48
   1c7a0:	b	c730 <__gmpn_jacobi_base@plt>
   1c7a4:	ldr	x20, [x0, #8]!
   1c7a8:	sub	w8, w8, #0x1
   1c7ac:	cbnz	x20, 1c70c <__gmpz_ui_kronecker@@Base+0x8c>
   1c7b0:	b	1c7a4 <__gmpz_ui_kronecker@@Base+0x124>
   1c7b4:	cmp	w8, #0x1
   1c7b8:	b.ne	1c7c8 <__gmpz_ui_kronecker@@Base+0x148>  // b.any
   1c7bc:	eor	w8, w19, w19, lsr #1
   1c7c0:	and	w8, w8, #0x2
   1c7c4:	b	1c748 <__gmpz_ui_kronecker@@Base+0xc8>
   1c7c8:	ldr	x9, [x0, #8]
   1c7cc:	mov	w21, wzr
   1c7d0:	lsl	x20, x9, #1
   1c7d4:	cmp	x19, #0x1
   1c7d8:	b.eq	1c744 <__gmpz_ui_kronecker@@Base+0xc4>  // b.none
   1c7dc:	b	1c6f0 <__gmpz_ui_kronecker@@Base+0x70>

000000000001c7e0 <__gmpz_kronecker_si@@Base>:
   1c7e0:	stp	x29, x30, [sp, #-32]!
   1c7e4:	stp	x20, x19, [sp, #16]
   1c7e8:	ldrsw	x8, [x0, #4]
   1c7ec:	mov	x29, sp
   1c7f0:	cbz	w8, 1c854 <__gmpz_kronecker_si@@Base+0x74>
   1c7f4:	ldr	x0, [x0, #8]
   1c7f8:	ubfx	x9, x8, #31, #1
   1c7fc:	lsr	x10, x1, #63
   1c800:	and	w10, w9, w10
   1c804:	cmp	x1, #0x0
   1c808:	cneg	x19, x1, mi  // mi = first
   1c80c:	lsl	w10, w10, #1
   1c810:	tbnz	w19, #0, 1c83c <__gmpz_kronecker_si@@Base+0x5c>
   1c814:	ldr	x11, [x0]
   1c818:	cbz	x19, 1c890 <__gmpz_kronecker_si@@Base+0xb0>
   1c81c:	tbz	w11, #0, 1c8ac <__gmpz_kronecker_si@@Base+0xcc>
   1c820:	rbit	x12, x1
   1c824:	lsr	x13, x11, #1
   1c828:	clz	x12, x12
   1c82c:	eor	w11, w13, w11
   1c830:	and	w11, w11, w12, lsl #1
   1c834:	lsr	x19, x19, x12
   1c838:	eor	w10, w11, w10
   1c83c:	cmp	x19, #0x1
   1c840:	b.ne	1c86c <__gmpz_kronecker_si@@Base+0x8c>  // b.any
   1c844:	and	w8, w10, #0x2
   1c848:	mov	w9, #0x1                   	// #1
   1c84c:	sub	w0, w9, w8
   1c850:	b	1c8b0 <__gmpz_kronecker_si@@Base+0xd0>
   1c854:	cmp	x1, #0x1
   1c858:	cset	w8, eq  // eq = none
   1c85c:	cmn	x1, #0x1
   1c860:	cset	w9, eq  // eq = none
   1c864:	orr	w0, w8, w9
   1c868:	b	1c8b0 <__gmpz_kronecker_si@@Base+0xd0>
   1c86c:	cmp	x8, #0x0
   1c870:	and	w9, w19, w9, lsl #1
   1c874:	cneg	x1, x8, mi  // mi = first
   1c878:	cmp	x1, #0x28
   1c87c:	eor	w20, w9, w10
   1c880:	b.lt	1c8bc <__gmpz_kronecker_si@@Base+0xdc>  // b.tstop
   1c884:	mov	x2, x19
   1c888:	bl	c3e0 <__gmpn_mod_1@plt>
   1c88c:	b	1c8cc <__gmpz_kronecker_si@@Base+0xec>
   1c890:	cmp	w8, #0x1
   1c894:	b.eq	1c8a0 <__gmpz_kronecker_si@@Base+0xc0>  // b.none
   1c898:	cmn	w8, #0x1
   1c89c:	b.ne	1c8ac <__gmpz_kronecker_si@@Base+0xcc>  // b.any
   1c8a0:	cmp	x11, #0x1
   1c8a4:	cset	w0, eq  // eq = none
   1c8a8:	b	1c8b0 <__gmpz_kronecker_si@@Base+0xd0>
   1c8ac:	mov	w0, wzr
   1c8b0:	ldp	x20, x19, [sp, #16]
   1c8b4:	ldp	x29, x30, [sp], #32
   1c8b8:	ret
   1c8bc:	mov	x2, x19
   1c8c0:	mov	x3, xzr
   1c8c4:	eor	w20, w20, w19
   1c8c8:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c8cc:	mov	x1, x19
   1c8d0:	mov	w2, w20
   1c8d4:	ldp	x20, x19, [sp, #16]
   1c8d8:	ldp	x29, x30, [sp], #32
   1c8dc:	b	c730 <__gmpn_jacobi_base@plt>

000000000001c8e0 <__gmpz_kronecker_ui@@Base>:
   1c8e0:	stp	x29, x30, [sp, #-32]!
   1c8e4:	stp	x20, x19, [sp, #16]
   1c8e8:	ldrsw	x8, [x0, #4]
   1c8ec:	mov	x19, x1
   1c8f0:	mov	x29, sp
   1c8f4:	cbz	w8, 1c954 <__gmpz_kronecker_ui@@Base+0x74>
   1c8f8:	ldr	x0, [x0, #8]
   1c8fc:	tbnz	w19, #0, 1c960 <__gmpz_kronecker_ui@@Base+0x80>
   1c900:	ldr	x9, [x0]
   1c904:	cbz	x19, 1c980 <__gmpz_kronecker_ui@@Base+0xa0>
   1c908:	tbz	w9, #0, 1c998 <__gmpz_kronecker_ui@@Base+0xb8>
   1c90c:	rbit	x10, x19
   1c910:	lsr	x11, x9, #1
   1c914:	clz	x10, x10
   1c918:	eor	w9, w11, w9
   1c91c:	lsr	x19, x19, x10
   1c920:	and	w9, w9, w10, lsl #1
   1c924:	and	w10, w19, w8, lsr #30
   1c928:	and	w10, w10, #0x2
   1c92c:	eor	w20, w9, w10
   1c930:	cmp	x19, #0x1
   1c934:	b.eq	1c970 <__gmpz_kronecker_ui@@Base+0x90>  // b.none
   1c938:	cmp	x8, #0x0
   1c93c:	cneg	x1, x8, mi  // mi = first
   1c940:	cmp	x1, #0x28
   1c944:	b.lt	1c9a8 <__gmpz_kronecker_ui@@Base+0xc8>  // b.tstop
   1c948:	mov	x2, x19
   1c94c:	bl	c3e0 <__gmpn_mod_1@plt>
   1c950:	b	1c9b8 <__gmpz_kronecker_ui@@Base+0xd8>
   1c954:	cmp	x19, #0x1
   1c958:	cset	w0, eq  // eq = none
   1c95c:	b	1c99c <__gmpz_kronecker_ui@@Base+0xbc>
   1c960:	and	w9, w19, w8, lsr #30
   1c964:	and	w20, w9, #0x2
   1c968:	cmp	x19, #0x1
   1c96c:	b.ne	1c938 <__gmpz_kronecker_ui@@Base+0x58>  // b.any
   1c970:	and	w8, w20, #0x2
   1c974:	mov	w9, #0x1                   	// #1
   1c978:	sub	w0, w9, w8
   1c97c:	b	1c99c <__gmpz_kronecker_ui@@Base+0xbc>
   1c980:	cmp	w8, #0x1
   1c984:	b.eq	1c990 <__gmpz_kronecker_ui@@Base+0xb0>  // b.none
   1c988:	cmn	w8, #0x1
   1c98c:	b.ne	1c998 <__gmpz_kronecker_ui@@Base+0xb8>  // b.any
   1c990:	cmp	x9, #0x1
   1c994:	b	1c958 <__gmpz_kronecker_ui@@Base+0x78>
   1c998:	mov	w0, wzr
   1c99c:	ldp	x20, x19, [sp, #16]
   1c9a0:	ldp	x29, x30, [sp], #32
   1c9a4:	ret
   1c9a8:	mov	x2, x19
   1c9ac:	mov	x3, xzr
   1c9b0:	eor	w20, w20, w19
   1c9b4:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   1c9b8:	mov	x1, x19
   1c9bc:	mov	w2, w20
   1c9c0:	ldp	x20, x19, [sp, #16]
   1c9c4:	ldp	x29, x30, [sp], #32
   1c9c8:	b	c730 <__gmpn_jacobi_base@plt>

000000000001c9cc <__gmpz_lcm@@Base>:
   1c9cc:	stp	x29, x30, [sp, #-64]!
   1c9d0:	str	x23, [sp, #16]
   1c9d4:	stp	x22, x21, [sp, #32]
   1c9d8:	stp	x20, x19, [sp, #48]
   1c9dc:	mov	x29, sp
   1c9e0:	sub	sp, sp, #0x10
   1c9e4:	ldrsw	x8, [x1, #4]
   1c9e8:	mov	x19, x0
   1c9ec:	cbz	w8, 1caa0 <__gmpz_lcm@@Base+0xd4>
   1c9f0:	ldr	w9, [x2, #4]
   1c9f4:	mov	x20, x2
   1c9f8:	cbz	w9, 1caa0 <__gmpz_lcm@@Base+0xd4>
   1c9fc:	sxtw	x9, w9
   1ca00:	cmp	x8, #0x0
   1ca04:	cneg	x8, x8, mi  // mi = first
   1ca08:	cmp	x9, #0x0
   1ca0c:	mov	x21, x1
   1ca10:	cneg	x9, x9, mi  // mi = first
   1ca14:	cmp	x8, #0x1
   1ca18:	b.eq	1caa8 <__gmpz_lcm@@Base+0xdc>  // b.none
   1ca1c:	cmp	x9, #0x1
   1ca20:	b.eq	1caa8 <__gmpz_lcm@@Base+0xdc>  // b.none
   1ca24:	cmp	x8, #0xfe0
   1ca28:	lsl	x1, x8, #3
   1ca2c:	str	xzr, [x29, #24]
   1ca30:	stur	w8, [x29, #-16]
   1ca34:	b.hi	1cb20 <__gmpz_lcm@@Base+0x154>  // b.pmore
   1ca38:	add	x9, x1, #0xf
   1ca3c:	mov	x8, sp
   1ca40:	and	x9, x9, #0xfffffffffffffff0
   1ca44:	sub	x0, x8, x9
   1ca48:	mov	sp, x0
   1ca4c:	stur	x0, [x29, #-8]
   1ca50:	sub	x0, x29, #0x10
   1ca54:	mov	x1, x21
   1ca58:	mov	x2, x20
   1ca5c:	bl	cf70 <__gmpz_gcd@plt>
   1ca60:	sub	x0, x29, #0x10
   1ca64:	sub	x2, x29, #0x10
   1ca68:	mov	x1, x21
   1ca6c:	bl	c3f0 <__gmpz_divexact@plt>
   1ca70:	sub	x1, x29, #0x10
   1ca74:	mov	x0, x19
   1ca78:	mov	x2, x20
   1ca7c:	bl	c4b0 <__gmpz_mul@plt>
   1ca80:	ldr	w8, [x19, #4]
   1ca84:	cmp	w8, #0x0
   1ca88:	cneg	w8, w8, mi  // mi = first
   1ca8c:	str	w8, [x19, #4]
   1ca90:	ldr	x0, [x29, #24]
   1ca94:	cbz	x0, 1cb08 <__gmpz_lcm@@Base+0x13c>
   1ca98:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1ca9c:	b	1cb08 <__gmpz_lcm@@Base+0x13c>
   1caa0:	str	wzr, [x19, #4]
   1caa4:	b	1cb08 <__gmpz_lcm@@Base+0x13c>
   1caa8:	ldrsw	x10, [x19]
   1caac:	cmp	x8, #0x1
   1cab0:	csel	x22, x9, x8, eq  // eq = none
   1cab4:	csel	x23, x21, x20, eq  // eq = none
   1cab8:	csel	x20, x20, x21, eq  // eq = none
   1cabc:	cmp	x22, x10
   1cac0:	b.ge	1cb2c <__gmpz_lcm@@Base+0x160>  // b.tcont
   1cac4:	ldr	x8, [x23, #8]
   1cac8:	ldr	x20, [x20, #8]
   1cacc:	mov	x1, x22
   1cad0:	ldr	x21, [x8]
   1cad4:	mov	x0, x20
   1cad8:	mov	x2, x21
   1cadc:	bl	bf90 <__gmpn_gcd_1@plt>
   1cae0:	ldr	x23, [x19, #8]
   1cae4:	udiv	x3, x21, x0
   1cae8:	mov	x1, x20
   1caec:	mov	x2, x22
   1caf0:	mov	x0, x23
   1caf4:	bl	d490 <__gmpn_mul_1@plt>
   1caf8:	cmp	x0, #0x0
   1cafc:	cinc	w8, w22, ne  // ne = any
   1cb00:	str	x0, [x23, x22, lsl #3]
   1cb04:	str	w8, [x19, #4]
   1cb08:	mov	sp, x29
   1cb0c:	ldp	x20, x19, [sp, #48]
   1cb10:	ldp	x22, x21, [sp, #32]
   1cb14:	ldr	x23, [sp, #16]
   1cb18:	ldp	x29, x30, [sp], #64
   1cb1c:	ret
   1cb20:	add	x0, x29, #0x18
   1cb24:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1cb28:	b	1ca4c <__gmpz_lcm@@Base+0x80>
   1cb2c:	add	x1, x22, #0x1
   1cb30:	mov	x0, x19
   1cb34:	bl	c080 <__gmpz_realloc@plt>
   1cb38:	b	1cac4 <__gmpz_lcm@@Base+0xf8>

000000000001cb3c <__gmpz_lcm_ui@@Base>:
   1cb3c:	stp	x29, x30, [sp, #-64]!
   1cb40:	stp	x20, x19, [sp, #48]
   1cb44:	mov	x19, x0
   1cb48:	mov	w8, wzr
   1cb4c:	str	x23, [sp, #16]
   1cb50:	stp	x22, x21, [sp, #32]
   1cb54:	mov	x29, sp
   1cb58:	cbz	x2, 1cbbc <__gmpz_lcm_ui@@Base+0x80>
   1cb5c:	ldr	w9, [x1, #4]
   1cb60:	mov	x22, x1
   1cb64:	cbz	w9, 1cbbc <__gmpz_lcm_ui@@Base+0x80>
   1cb68:	ldrsw	x8, [x19]
   1cb6c:	sxtw	x9, w9
   1cb70:	cmp	x9, #0x0
   1cb74:	cneg	x21, x9, mi  // mi = first
   1cb78:	mov	x20, x2
   1cb7c:	cmp	x21, x8
   1cb80:	b.ge	1cbd4 <__gmpz_lcm_ui@@Base+0x98>  // b.tcont
   1cb84:	ldr	x22, [x22, #8]
   1cb88:	mov	x1, x21
   1cb8c:	mov	x2, x20
   1cb90:	mov	x0, x22
   1cb94:	bl	bf90 <__gmpn_gcd_1@plt>
   1cb98:	ldr	x23, [x19, #8]
   1cb9c:	udiv	x3, x20, x0
   1cba0:	mov	x1, x22
   1cba4:	mov	x2, x21
   1cba8:	mov	x0, x23
   1cbac:	bl	d490 <__gmpn_mul_1@plt>
   1cbb0:	cmp	x0, #0x0
   1cbb4:	cinc	w8, w21, ne  // ne = any
   1cbb8:	str	x0, [x23, x21, lsl #3]
   1cbbc:	str	w8, [x19, #4]
   1cbc0:	ldp	x20, x19, [sp, #48]
   1cbc4:	ldp	x22, x21, [sp, #32]
   1cbc8:	ldr	x23, [sp, #16]
   1cbcc:	ldp	x29, x30, [sp], #64
   1cbd0:	ret
   1cbd4:	add	x1, x21, #0x1
   1cbd8:	mov	x0, x19
   1cbdc:	bl	c080 <__gmpz_realloc@plt>
   1cbe0:	b	1cb84 <__gmpz_lcm_ui@@Base+0x48>

000000000001cbe4 <__gmpz_limbs_finish@@Base>:
   1cbe4:	cmp	x1, #0x0
   1cbe8:	cneg	x9, x1, mi  // mi = first
   1cbec:	mov	x8, x9
   1cbf0:	subs	x9, x9, #0x1
   1cbf4:	b.lt	1cc08 <__gmpz_limbs_finish@@Base+0x24>  // b.tstop
   1cbf8:	ldr	x10, [x0, #8]
   1cbfc:	add	x10, x10, x8, lsl #3
   1cc00:	ldur	x10, [x10, #-8]
   1cc04:	cbz	x10, 1cbec <__gmpz_limbs_finish@@Base+0x8>
   1cc08:	neg	w9, w8
   1cc0c:	cmp	x1, #0x0
   1cc10:	csel	x8, x9, x8, lt  // lt = tstop
   1cc14:	str	w8, [x0, #4]
   1cc18:	ret

000000000001cc1c <__gmpz_limbs_modify@@Base>:
   1cc1c:	ldrsw	x8, [x0]
   1cc20:	cmp	x8, x1
   1cc24:	b.lt	1cc30 <__gmpz_limbs_modify@@Base+0x14>  // b.tstop
   1cc28:	ldr	x0, [x0, #8]
   1cc2c:	ret
   1cc30:	b	c080 <__gmpz_realloc@plt>

000000000001cc34 <__gmpz_limbs_read@@Base>:
   1cc34:	ldr	x0, [x0, #8]
   1cc38:	ret

000000000001cc3c <__gmpz_limbs_write@@Base>:
   1cc3c:	ldrsw	x8, [x0]
   1cc40:	cmp	x8, x1
   1cc44:	b.lt	1cc50 <__gmpz_limbs_write@@Base+0x14>  // b.tstop
   1cc48:	ldr	x0, [x0, #8]
   1cc4c:	ret
   1cc50:	b	c080 <__gmpz_realloc@plt>

000000000001cc54 <__gmpz_lucas_mod@@Base>:
   1cc54:	stp	x29, x30, [sp, #-96]!
   1cc58:	stp	x22, x21, [sp, #64]
   1cc5c:	mov	x21, x1
   1cc60:	mov	w1, #0x1                   	// #1
   1cc64:	str	x27, [sp, #16]
   1cc68:	stp	x26, x25, [sp, #32]
   1cc6c:	stp	x24, x23, [sp, #48]
   1cc70:	stp	x20, x19, [sp, #80]
   1cc74:	mov	x29, sp
   1cc78:	mov	x19, x6
   1cc7c:	mov	x22, x5
   1cc80:	mov	x20, x4
   1cc84:	mov	x25, x3
   1cc88:	mov	x23, x2
   1cc8c:	mov	x26, x0
   1cc90:	bl	c170 <__gmpz_set_ui@plt>
   1cc94:	mov	w1, #0x2                   	// #2
   1cc98:	mov	x0, x20
   1cc9c:	bl	d260 <__gmpz_sizeinbase@plt>
   1cca0:	sub	x27, x0, #0x2
   1cca4:	cmp	x27, x25
   1cca8:	b.cc	1ce8c <__gmpz_lucas_mod@@Base+0x238>  // b.lo, b.ul, b.last
   1ccac:	mov	w1, #0x1                   	// #1
   1ccb0:	mov	x0, x21
   1ccb4:	bl	c170 <__gmpz_set_ui@plt>
   1ccb8:	neg	x24, x23
   1ccbc:	b	1ccec <__gmpz_lucas_mod@@Base+0x98>
   1ccc0:	mov	x0, x21
   1ccc4:	mov	x1, x22
   1ccc8:	mov	x2, x20
   1cccc:	bl	ca80 <__gmpz_tdiv_r@plt>
   1ccd0:	mov	x0, x26
   1ccd4:	mov	x1, x19
   1ccd8:	mov	x2, x20
   1ccdc:	bl	ca80 <__gmpz_tdiv_r@plt>
   1cce0:	sub	x27, x27, #0x1
   1cce4:	cmp	x27, x25
   1cce8:	b.cc	1cda0 <__gmpz_lucas_mod@@Base+0x14c>  // b.lo, b.ul, b.last
   1ccec:	mov	x0, x22
   1ccf0:	mov	x1, x21
   1ccf4:	mov	x2, x21
   1ccf8:	bl	c4b0 <__gmpz_mul@plt>
   1ccfc:	mov	x0, x21
   1cd00:	mov	x1, x26
   1cd04:	mov	x2, x21
   1cd08:	bl	c260 <__gmpz_sub@plt>
   1cd0c:	mov	x0, x19
   1cd10:	mov	x1, x21
   1cd14:	mov	x2, x21
   1cd18:	bl	c4b0 <__gmpz_mul@plt>
   1cd1c:	mov	x0, x21
   1cd20:	mov	x1, x26
   1cd24:	mov	x2, x26
   1cd28:	bl	c4b0 <__gmpz_mul@plt>
   1cd2c:	mov	x0, x19
   1cd30:	mov	x1, x22
   1cd34:	mov	x2, x19
   1cd38:	bl	c260 <__gmpz_sub@plt>
   1cd3c:	mov	x0, x22
   1cd40:	mov	x1, x21
   1cd44:	cmp	x23, #0x1
   1cd48:	b.lt	1cd58 <__gmpz_lucas_mod@@Base+0x104>  // b.tstop
   1cd4c:	mov	x2, x23
   1cd50:	bl	c870 <__gmpz_submul_ui@plt>
   1cd54:	b	1cd60 <__gmpz_lucas_mod@@Base+0x10c>
   1cd58:	mov	x2, x24
   1cd5c:	bl	d320 <__gmpz_addmul_ui@plt>
   1cd60:	mov	x0, x20
   1cd64:	mov	x1, x27
   1cd68:	bl	c480 <__gmpz_tstbit@plt>
   1cd6c:	cbz	w0, 1ccc0 <__gmpz_lucas_mod@@Base+0x6c>
   1cd70:	mov	x0, x19
   1cd74:	mov	x1, x19
   1cd78:	mov	x2, x23
   1cd7c:	bl	cbb0 <__gmpz_mul_si@plt>
   1cd80:	mov	x0, x19
   1cd84:	mov	x1, x22
   1cd88:	mov	x2, x19
   1cd8c:	bl	c260 <__gmpz_sub@plt>
   1cd90:	mov	x0, x22
   1cd94:	mov	x1, x19
   1cd98:	bl	c580 <__gmpz_swap@plt>
   1cd9c:	b	1ccc0 <__gmpz_lucas_mod@@Base+0x6c>
   1cda0:	ldr	w8, [x21, #4]
   1cda4:	cbz	w8, 1ce4c <__gmpz_lucas_mod@@Base+0x1f8>
   1cda8:	neg	x2, x23, lsl #1
   1cdac:	mov	x0, x22
   1cdb0:	mov	x1, x26
   1cdb4:	bl	cbb0 <__gmpz_mul_si@plt>
   1cdb8:	mov	x0, x22
   1cdbc:	mov	x1, x21
   1cdc0:	mov	x2, x22
   1cdc4:	bl	cf90 <__gmpz_add@plt>
   1cdc8:	mov	x0, x26
   1cdcc:	mov	x1, x22
   1cdd0:	mov	x2, x20
   1cdd4:	bl	ca80 <__gmpz_tdiv_r@plt>
   1cdd8:	ldr	w8, [x26, #4]
   1cddc:	cmp	w8, #0x0
   1cde0:	cset	w0, eq  // eq = none
   1cde4:	cmp	x25, #0x2
   1cde8:	b.cc	1ce70 <__gmpz_lucas_mod@@Base+0x21c>  // b.lo, b.ul, b.last
   1cdec:	cbz	w8, 1ce70 <__gmpz_lucas_mod@@Base+0x21c>
   1cdf0:	mov	x0, x19
   1cdf4:	mov	x1, x22
   1cdf8:	mov	x2, x22
   1cdfc:	bl	c4b0 <__gmpz_mul@plt>
   1ce00:	mov	x0, x22
   1ce04:	mov	x1, x21
   1ce08:	mov	x2, x21
   1ce0c:	bl	c4b0 <__gmpz_mul@plt>
   1ce10:	mov	x0, x19
   1ce14:	mov	x1, x19
   1ce18:	mov	x2, x22
   1ce1c:	bl	c260 <__gmpz_sub@plt>
   1ce20:	mov	w2, #0x2                   	// #2
   1ce24:	mov	x0, x19
   1ce28:	mov	x1, x19
   1ce2c:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1ce30:	mov	x0, x19
   1ce34:	mov	x1, x22
   1ce38:	cmp	x23, #0x1
   1ce3c:	b.lt	1ce54 <__gmpz_lucas_mod@@Base+0x200>  // b.tstop
   1ce40:	mov	x2, x23
   1ce44:	bl	d320 <__gmpz_addmul_ui@plt>
   1ce48:	b	1ce5c <__gmpz_lucas_mod@@Base+0x208>
   1ce4c:	mov	w0, #0x1                   	// #1
   1ce50:	b	1ce70 <__gmpz_lucas_mod@@Base+0x21c>
   1ce54:	mov	x2, x24
   1ce58:	bl	c870 <__gmpz_submul_ui@plt>
   1ce5c:	mov	x0, x21
   1ce60:	mov	x1, x19
   1ce64:	mov	x2, x20
   1ce68:	bl	ca80 <__gmpz_tdiv_r@plt>
   1ce6c:	mov	w0, wzr
   1ce70:	ldp	x20, x19, [sp, #80]
   1ce74:	ldp	x22, x21, [sp, #64]
   1ce78:	ldp	x24, x23, [sp, #48]
   1ce7c:	ldp	x26, x25, [sp, #32]
   1ce80:	ldr	x27, [sp, #16]
   1ce84:	ldp	x29, x30, [sp], #96
   1ce88:	ret
   1ce8c:	mov	x0, x21
   1ce90:	mov	x1, x23
   1ce94:	bl	d270 <__gmpz_set_si@plt>
   1ce98:	b	1ce6c <__gmpz_lucas_mod@@Base+0x218>

000000000001ce9c <__gmpz_lucnum_ui@@Base>:
   1ce9c:	stp	x29, x30, [sp, #-96]!
   1cea0:	stp	x20, x19, [sp, #80]
   1cea4:	mov	x20, x1
   1cea8:	cmp	x1, #0x5c
   1ceac:	mov	x19, x0
   1ceb0:	str	x27, [sp, #16]
   1ceb4:	stp	x26, x25, [sp, #32]
   1ceb8:	stp	x24, x23, [sp, #48]
   1cebc:	stp	x22, x21, [sp, #64]
   1cec0:	mov	x29, sp
   1cec4:	b.hi	1cefc <__gmpz_lucnum_ui@@Base+0x60>  // b.pmore
   1cec8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1cecc:	ldr	x8, [x8, #3808]
   1ced0:	ldr	w9, [x19]
   1ced4:	add	x8, x8, x20, lsl #3
   1ced8:	ldp	x8, x10, [x8]
   1cedc:	cmp	w9, #0x0
   1cee0:	add	x20, x10, x8, lsl #1
   1cee4:	b.le	1d154 <__gmpz_lucnum_ui@@Base+0x2b8>
   1cee8:	ldr	x0, [x19, #8]
   1ceec:	mov	w8, #0x1                   	// #1
   1cef0:	str	x20, [x0]
   1cef4:	str	w8, [x19, #4]
   1cef8:	b	1d0bc <__gmpz_lucnum_ui@@Base+0x220>
   1cefc:	lsr	x8, x20, #5
   1cf00:	mov	w9, #0x17                  	// #23
   1cf04:	ldrsw	x10, [x19]
   1cf08:	mul	x8, x8, x9
   1cf0c:	lsr	x24, x8, #6
   1cf10:	add	x22, x24, #0x6
   1cf14:	cmp	x22, x10
   1cf18:	b.gt	1d164 <__gmpz_lucnum_ui@@Base+0x2c8>
   1cf1c:	ldr	x21, [x19, #8]
   1cf20:	mov	w23, #0xf6c0                	// #63168
   1cf24:	movk	w23, #0x3, lsl #16
   1cf28:	cmp	x24, #0xfda
   1cf2c:	lsl	x1, x22, #3
   1cf30:	str	xzr, [x29, #24]
   1cf34:	b.hi	1d178 <__gmpz_lucnum_ui@@Base+0x2dc>  // b.pmore
   1cf38:	add	x9, x1, #0xf
   1cf3c:	mov	x8, sp
   1cf40:	and	x9, x9, #0x7ffffffffffffff0
   1cf44:	sub	x0, x8, x9
   1cf48:	mov	sp, x0
   1cf4c:	mov	w26, wzr
   1cf50:	mov	x22, x21
   1cf54:	mov	x21, x0
   1cf58:	lsr	x2, x20, #1
   1cf5c:	tbnz	w20, #0, 1cfa8 <__gmpz_lucnum_ui@@Base+0x10c>
   1cf60:	cmp	x20, #0xb9
   1cf64:	add	w26, w26, #0x1
   1cf68:	mov	x0, x22
   1cf6c:	mov	x20, x2
   1cf70:	b.hi	1cf50 <__gmpz_lucnum_ui@@Base+0xb4>  // b.pmore
   1cf74:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1cf78:	ldr	x8, [x8, #3808]
   1cf7c:	sbfiz	x9, x2, #3, #32
   1cf80:	mov	w23, #0x1                   	// #1
   1cf84:	mov	x20, x2
   1cf88:	add	x10, x8, x2, lsl #3
   1cf8c:	ldr	x8, [x8, x9]
   1cf90:	ldr	x9, [x10, #8]
   1cf94:	mov	x0, x21
   1cf98:	add	x8, x9, x8, lsl #1
   1cf9c:	str	x8, [x21]
   1cfa0:	mov	x21, x22
   1cfa4:	b	1d0f4 <__gmpz_lucnum_ui@@Base+0x258>
   1cfa8:	lsr	x8, x20, #6
   1cfac:	mov	w9, #0x17                  	// #23
   1cfb0:	mul	x8, x8, x9
   1cfb4:	lsr	x9, x8, #3
   1cfb8:	add	x10, x23, #0x80
   1cfbc:	and	x9, x9, #0xffffffffffffff8
   1cfc0:	cmp	x8, x10
   1cfc4:	add	x1, x9, #0x20
   1cfc8:	b.cs	1d18c <__gmpz_lucnum_ui@@Base+0x2f0>  // b.hs, b.nlast
   1cfcc:	add	x9, x1, #0xf
   1cfd0:	mov	x8, sp
   1cfd4:	and	x9, x9, #0x3ffffffffffffff0
   1cfd8:	sub	x23, x8, x9
   1cfdc:	mov	sp, x23
   1cfe0:	mov	x0, x21
   1cfe4:	mov	x1, x23
   1cfe8:	bl	d070 <__gmpn_fib2_ui@plt>
   1cfec:	lsl	x27, x0, #3
   1cff0:	add	x8, x27, x23
   1cff4:	ldur	x8, [x8, #-8]
   1cff8:	mov	x24, x0
   1cffc:	mov	x1, x23
   1d000:	mov	x2, x21
   1d004:	cmp	x8, #0x0
   1d008:	cset	w8, eq  // eq = none
   1d00c:	sub	x25, x0, x8
   1d010:	mov	x0, x21
   1d014:	mov	x3, x24
   1d018:	bl	cc40 <__gmpn_addlsh1_n@plt>
   1d01c:	cmp	x0, #0x0
   1d020:	cinc	x24, x24, ne  // ne = any
   1d024:	str	x0, [x21, x27]
   1d028:	mov	x0, x22
   1d02c:	mov	x1, x21
   1d030:	mov	x2, x24
   1d034:	mov	x3, x23
   1d038:	mov	x4, x25
   1d03c:	bl	ccd0 <__gmpn_mul@plt>
   1d040:	cmp	x0, #0x0
   1d044:	add	x8, x24, x25
   1d048:	cset	w9, eq  // eq = none
   1d04c:	sub	x23, x8, x9
   1d050:	mov	x0, x22
   1d054:	mov	x1, x22
   1d058:	mov	x2, x22
   1d05c:	mov	x3, x23
   1d060:	bl	cba0 <__gmpn_addlsh2_n@plt>
   1d064:	str	x0, [x22, x23, lsl #3]
   1d068:	ldr	x8, [x22]
   1d06c:	cmp	x0, #0x0
   1d070:	cinc	x23, x23, ne  // ne = any
   1d074:	tbnz	w20, #1, 1d0a0 <__gmpz_lucnum_ui@@Base+0x204>
   1d078:	sub	x9, x8, #0x4
   1d07c:	cmp	x8, #0x3
   1d080:	str	x9, [x22]
   1d084:	b.hi	1d0a8 <__gmpz_lucnum_ui@@Base+0x20c>  // b.pmore
   1d088:	add	x8, x22, #0x8
   1d08c:	ldr	x9, [x8]
   1d090:	sub	x10, x9, #0x1
   1d094:	str	x10, [x8], #8
   1d098:	cbz	x9, 1d08c <__gmpz_lucnum_ui@@Base+0x1f0>
   1d09c:	b	1d0a8 <__gmpz_lucnum_ui@@Base+0x20c>
   1d0a0:	add	x8, x8, #0x4
   1d0a4:	str	x8, [x22]
   1d0a8:	mov	x0, x22
   1d0ac:	cbnz	w26, 1d0f4 <__gmpz_lucnum_ui@@Base+0x258>
   1d0b0:	str	w23, [x19, #4]
   1d0b4:	ldr	x0, [x29, #24]
   1d0b8:	cbnz	x0, 1d184 <__gmpz_lucnum_ui@@Base+0x2e8>
   1d0bc:	mov	sp, x29
   1d0c0:	ldp	x20, x19, [sp, #80]
   1d0c4:	ldp	x22, x21, [sp, #64]
   1d0c8:	ldp	x24, x23, [sp, #48]
   1d0cc:	ldp	x26, x25, [sp, #32]
   1d0d0:	ldr	x27, [sp, #16]
   1d0d4:	ldp	x29, x30, [sp], #96
   1d0d8:	ret
   1d0dc:	add	x8, x8, #0x2
   1d0e0:	mov	x20, xzr
   1d0e4:	str	x8, [x22]
   1d0e8:	subs	w26, w26, #0x1
   1d0ec:	mov	x0, x22
   1d0f0:	b.eq	1d0b0 <__gmpz_lucnum_ui@@Base+0x214>  // b.none
   1d0f4:	mov	x22, x21
   1d0f8:	mov	x21, x0
   1d0fc:	mov	x0, x22
   1d100:	mov	x1, x21
   1d104:	mov	x2, x23
   1d108:	bl	c8e0 <__gmpn_sqr@plt>
   1d10c:	add	x8, x22, x23, lsl #4
   1d110:	ldur	x9, [x8, #-8]
   1d114:	ldr	x8, [x22]
   1d118:	lsl	x10, x23, #1
   1d11c:	cmp	x9, #0x0
   1d120:	cset	w9, eq  // eq = none
   1d124:	sub	x23, x10, x9
   1d128:	tbnz	w20, #0, 1d0dc <__gmpz_lucnum_ui@@Base+0x240>
   1d12c:	sub	x9, x8, #0x2
   1d130:	cmp	x8, #0x1
   1d134:	str	x9, [x22]
   1d138:	b.hi	1d0e8 <__gmpz_lucnum_ui@@Base+0x24c>  // b.pmore
   1d13c:	add	x8, x22, #0x8
   1d140:	ldr	x9, [x8]
   1d144:	sub	x10, x9, #0x1
   1d148:	str	x10, [x8], #8
   1d14c:	cbz	x9, 1d140 <__gmpz_lucnum_ui@@Base+0x2a4>
   1d150:	b	1d0e8 <__gmpz_lucnum_ui@@Base+0x24c>
   1d154:	mov	w1, #0x1                   	// #1
   1d158:	mov	x0, x19
   1d15c:	bl	c080 <__gmpz_realloc@plt>
   1d160:	b	1ceec <__gmpz_lucnum_ui@@Base+0x50>
   1d164:	mov	x0, x19
   1d168:	mov	x1, x22
   1d16c:	bl	c080 <__gmpz_realloc@plt>
   1d170:	mov	x21, x0
   1d174:	b	1cf20 <__gmpz_lucnum_ui@@Base+0x84>
   1d178:	add	x0, x29, #0x18
   1d17c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d180:	b	1cf4c <__gmpz_lucnum_ui@@Base+0xb0>
   1d184:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d188:	b	1d0bc <__gmpz_lucnum_ui@@Base+0x220>
   1d18c:	add	x0, x29, #0x18
   1d190:	mov	x23, x2
   1d194:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d198:	mov	x2, x23
   1d19c:	mov	x23, x0
   1d1a0:	b	1cfe0 <__gmpz_lucnum_ui@@Base+0x144>

000000000001d1a4 <__gmpz_lucnum2_ui@@Base>:
   1d1a4:	stp	x29, x30, [sp, #-80]!
   1d1a8:	stp	x22, x21, [sp, #48]
   1d1ac:	stp	x20, x19, [sp, #64]
   1d1b0:	mov	x20, x2
   1d1b4:	mov	x19, x1
   1d1b8:	cmp	x2, #0x5c
   1d1bc:	mov	x21, x0
   1d1c0:	str	x25, [sp, #16]
   1d1c4:	stp	x24, x23, [sp, #32]
   1d1c8:	mov	x29, sp
   1d1cc:	b.hi	1d238 <__gmpz_lucnum2_ui@@Base+0x94>  // b.pmore
   1d1d0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1d1d4:	ldr	x8, [x8, #3808]
   1d1d8:	ldr	w9, [x21]
   1d1dc:	add	x8, x8, x20, lsl #3
   1d1e0:	ldp	x22, x23, [x8]
   1d1e4:	cmp	w9, #0x0
   1d1e8:	add	x24, x23, x22, lsl #1
   1d1ec:	b.le	1d318 <__gmpz_lucnum2_ui@@Base+0x174>
   1d1f0:	ldr	x0, [x21, #8]
   1d1f4:	mov	w8, #0x1                   	// #1
   1d1f8:	str	x24, [x0]
   1d1fc:	str	w8, [x21, #4]
   1d200:	ldr	w9, [x19]
   1d204:	lsl	x8, x23, #1
   1d208:	sub	x8, x8, x22
   1d20c:	cmp	x20, #0x0
   1d210:	csinc	x21, x8, xzr, ne  // ne = any
   1d214:	cmp	w9, #0x0
   1d218:	b.le	1d328 <__gmpz_lucnum2_ui@@Base+0x184>
   1d21c:	ldr	x0, [x19, #8]
   1d220:	cmp	x20, #0x0
   1d224:	mov	w8, #0xffffffff            	// #-1
   1d228:	cneg	w8, w8, ne  // ne = any
   1d22c:	str	x21, [x0]
   1d230:	str	w8, [x19, #4]
   1d234:	b	1d2fc <__gmpz_lucnum2_ui@@Base+0x158>
   1d238:	lsr	x8, x20, #5
   1d23c:	mov	w9, #0x17                  	// #23
   1d240:	mul	x8, x8, x9
   1d244:	lsr	x23, x8, #6
   1d248:	lsl	x8, x23, #3
   1d24c:	cmp	x23, #0xfdc
   1d250:	add	x1, x8, #0x20
   1d254:	str	xzr, [x29, #24]
   1d258:	b.hi	1d338 <__gmpz_lucnum2_ui@@Base+0x194>  // b.pmore
   1d25c:	add	x9, x1, #0xf
   1d260:	mov	x8, sp
   1d264:	and	x9, x9, #0x7ffffffffffffff0
   1d268:	sub	x22, x8, x9
   1d26c:	mov	sp, x22
   1d270:	ldrsw	x8, [x21]
   1d274:	add	x24, x23, #0x5
   1d278:	cmp	x24, x8
   1d27c:	b.gt	1d348 <__gmpz_lucnum2_ui@@Base+0x1a4>
   1d280:	ldr	x23, [x21, #8]
   1d284:	ldrsw	x8, [x19]
   1d288:	cmp	x24, x8
   1d28c:	b.gt	1d35c <__gmpz_lucnum2_ui@@Base+0x1b8>
   1d290:	ldr	x24, [x19, #8]
   1d294:	mov	x0, x24
   1d298:	mov	x1, x22
   1d29c:	mov	x2, x20
   1d2a0:	bl	d070 <__gmpn_fib2_ui@plt>
   1d2a4:	mov	x20, x0
   1d2a8:	mov	x0, x23
   1d2ac:	mov	x1, x24
   1d2b0:	mov	x2, x22
   1d2b4:	mov	x3, x20
   1d2b8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   1d2bc:	lsl	x25, x20, #3
   1d2c0:	cmp	x0, #0x0
   1d2c4:	str	x0, [x23, x25]
   1d2c8:	cinc	w8, w20, ne  // ne = any
   1d2cc:	mov	x0, x24
   1d2d0:	mov	x1, x22
   1d2d4:	mov	x2, x24
   1d2d8:	mov	x3, x20
   1d2dc:	str	w8, [x21, #4]
   1d2e0:	bl	d090 <__gmpn_rsblsh1_n@plt>
   1d2e4:	cmp	x0, #0x0
   1d2e8:	cinc	w8, w20, ne  // ne = any
   1d2ec:	str	x0, [x24, x25]
   1d2f0:	str	w8, [x19, #4]
   1d2f4:	ldr	x0, [x29, #24]
   1d2f8:	cbnz	x0, 1d370 <__gmpz_lucnum2_ui@@Base+0x1cc>
   1d2fc:	mov	sp, x29
   1d300:	ldp	x20, x19, [sp, #64]
   1d304:	ldp	x22, x21, [sp, #48]
   1d308:	ldp	x24, x23, [sp, #32]
   1d30c:	ldr	x25, [sp, #16]
   1d310:	ldp	x29, x30, [sp], #80
   1d314:	ret
   1d318:	mov	w1, #0x1                   	// #1
   1d31c:	mov	x0, x21
   1d320:	bl	c080 <__gmpz_realloc@plt>
   1d324:	b	1d1f4 <__gmpz_lucnum2_ui@@Base+0x50>
   1d328:	mov	w1, #0x1                   	// #1
   1d32c:	mov	x0, x19
   1d330:	bl	c080 <__gmpz_realloc@plt>
   1d334:	b	1d220 <__gmpz_lucnum2_ui@@Base+0x7c>
   1d338:	add	x0, x29, #0x18
   1d33c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d340:	mov	x22, x0
   1d344:	b	1d270 <__gmpz_lucnum2_ui@@Base+0xcc>
   1d348:	mov	x0, x21
   1d34c:	mov	x1, x24
   1d350:	bl	c080 <__gmpz_realloc@plt>
   1d354:	mov	x23, x0
   1d358:	b	1d284 <__gmpz_lucnum2_ui@@Base+0xe0>
   1d35c:	mov	x0, x19
   1d360:	mov	x1, x24
   1d364:	bl	c080 <__gmpz_realloc@plt>
   1d368:	mov	x24, x0
   1d36c:	b	1d294 <__gmpz_lucnum2_ui@@Base+0xf0>
   1d370:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d374:	b	1d2fc <__gmpz_lucnum2_ui@@Base+0x158>

000000000001d378 <__gmpz_millerrabin@@Base>:
   1d378:	stp	x29, x30, [sp, #-48]!
   1d37c:	stp	x22, x21, [sp, #16]
   1d380:	stp	x20, x19, [sp, #32]
   1d384:	mov	x29, sp
   1d388:	sub	sp, sp, #0x70
   1d38c:	stur	xzr, [x29, #-104]
   1d390:	ldrsw	x8, [x0, #4]
   1d394:	mov	w20, w1
   1d398:	mov	w9, #0x7f00                	// #32512
   1d39c:	mov	x19, x0
   1d3a0:	add	x8, x8, #0x1
   1d3a4:	lsl	x1, x8, #3
   1d3a8:	cmp	x1, x9
   1d3ac:	stur	w8, [x29, #-16]
   1d3b0:	b.hi	1d59c <__gmpz_millerrabin@@Base+0x224>  // b.pmore
   1d3b4:	add	x9, x1, #0xf
   1d3b8:	mov	x8, sp
   1d3bc:	and	x9, x9, #0xfffffffffffffff0
   1d3c0:	sub	x0, x8, x9
   1d3c4:	mov	sp, x0
   1d3c8:	stur	x0, [x29, #-8]
   1d3cc:	sub	x0, x29, #0x10
   1d3d0:	mov	w2, #0x1                   	// #1
   1d3d4:	mov	x1, x19
   1d3d8:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1d3dc:	ldr	w8, [x19, #4]
   1d3e0:	mov	w10, #0x7f00                	// #32512
   1d3e4:	add	w9, w8, #0x1
   1d3e8:	sbfiz	x1, x9, #3, #32
   1d3ec:	cmp	x1, x10
   1d3f0:	stur	w9, [x29, #-32]
   1d3f4:	b.hi	1d5a8 <__gmpz_millerrabin@@Base+0x230>  // b.pmore
   1d3f8:	add	x10, x1, #0xf
   1d3fc:	mov	x9, sp
   1d400:	and	x10, x10, #0xfffffffffffffff0
   1d404:	sub	x0, x9, x10
   1d408:	mov	sp, x0
   1d40c:	lsl	w9, w8, #1
   1d410:	sbfiz	x1, x9, #3, #32
   1d414:	mov	w10, #0x7f00                	// #32512
   1d418:	cmp	x1, x10
   1d41c:	stur	x0, [x29, #-24]
   1d420:	stur	w9, [x29, #-48]
   1d424:	b.hi	1d5b8 <__gmpz_millerrabin@@Base+0x240>  // b.pmore
   1d428:	add	x10, x1, #0xf
   1d42c:	mov	x9, sp
   1d430:	and	x10, x10, #0xfffffffffffffff0
   1d434:	sub	x0, x9, x10
   1d438:	mov	sp, x0
   1d43c:	sbfiz	x1, x8, #3, #32
   1d440:	mov	w9, #0x7f00                	// #32512
   1d444:	cmp	x1, x9
   1d448:	stur	x0, [x29, #-40]
   1d44c:	stur	w8, [x29, #-64]
   1d450:	b.hi	1d5c8 <__gmpz_millerrabin@@Base+0x250>  // b.pmore
   1d454:	add	x9, x1, #0xf
   1d458:	mov	x8, sp
   1d45c:	and	x9, x9, #0xfffffffffffffff0
   1d460:	sub	x0, x8, x9
   1d464:	mov	sp, x0
   1d468:	stur	x0, [x29, #-56]
   1d46c:	sub	x0, x29, #0x10
   1d470:	mov	x1, xzr
   1d474:	bl	bf20 <__gmpz_scan1@plt>
   1d478:	mov	x21, x0
   1d47c:	sub	x0, x29, #0x40
   1d480:	sub	x1, x29, #0x10
   1d484:	mov	x2, x21
   1d488:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   1d48c:	sub	x0, x29, #0x20
   1d490:	mov	w1, #0x2                   	// #2
   1d494:	add	x21, x21, #0x1
   1d498:	bl	c170 <__gmpz_set_ui@plt>
   1d49c:	sub	x1, x29, #0x20
   1d4a0:	sub	x2, x29, #0x30
   1d4a4:	sub	x3, x29, #0x40
   1d4a8:	mov	x0, x19
   1d4ac:	mov	x4, x21
   1d4b0:	bl	1d5dc <__gmpz_millerrabin@@Base+0x264>
   1d4b4:	cbz	w0, 1d4f4 <__gmpz_millerrabin@@Base+0x17c>
   1d4b8:	sub	x1, x29, #0x20
   1d4bc:	sub	x2, x29, #0x30
   1d4c0:	mov	x0, x19
   1d4c4:	bl	c9c0 <__gmpz_stronglucas@plt>
   1d4c8:	cbz	w0, 1d4f4 <__gmpz_millerrabin@@Base+0x17c>
   1d4cc:	ldr	x8, [x19, #8]
   1d4d0:	ldr	w9, [x19, #4]
   1d4d4:	ldr	x8, [x8]
   1d4d8:	lsr	x8, x8, #46
   1d4dc:	cmp	x8, #0x13
   1d4e0:	cset	w8, cc  // cc = lo, ul, last
   1d4e4:	cmp	w9, w8
   1d4e8:	b.ne	1d518 <__gmpz_millerrabin@@Base+0x1a0>  // b.any
   1d4ec:	mov	w20, #0x2                   	// #2
   1d4f0:	b	1d4f8 <__gmpz_millerrabin@@Base+0x180>
   1d4f4:	mov	w20, wzr
   1d4f8:	ldur	x0, [x29, #-104]
   1d4fc:	cbnz	x0, 1d5d4 <__gmpz_millerrabin@@Base+0x25c>
   1d500:	mov	w0, w20
   1d504:	mov	sp, x29
   1d508:	ldp	x20, x19, [sp, #32]
   1d50c:	ldp	x22, x21, [sp, #16]
   1d510:	ldp	x29, x30, [sp], #48
   1d514:	ret
   1d518:	cmp	w20, #0x19
   1d51c:	b.lt	1d594 <__gmpz_millerrabin@@Base+0x21c>  // b.tstop
   1d520:	sub	x0, x29, #0x10
   1d524:	sub	x1, x29, #0x10
   1d528:	mov	w2, #0x2                   	// #2
   1d52c:	sub	w22, w20, #0x18
   1d530:	bl	c120 <__gmpz_sub_ui@plt>
   1d534:	sub	x0, x29, #0x60
   1d538:	bl	c7a0 <__gmp_randinit_default@plt>
   1d53c:	sub	x0, x29, #0x20
   1d540:	sub	x1, x29, #0x60
   1d544:	sub	x2, x29, #0x10
   1d548:	bl	c8f0 <__gmpz_urandomm@plt>
   1d54c:	sub	x0, x29, #0x20
   1d550:	sub	x1, x29, #0x20
   1d554:	mov	w2, #0x3                   	// #3
   1d558:	bl	c8b0 <__gmpz_add_ui@plt>
   1d55c:	sub	x1, x29, #0x20
   1d560:	sub	x2, x29, #0x30
   1d564:	sub	x3, x29, #0x40
   1d568:	mov	x0, x19
   1d56c:	mov	x4, x21
   1d570:	bl	1d5dc <__gmpz_millerrabin@@Base+0x264>
   1d574:	cmp	w22, #0x2
   1d578:	mov	w20, w0
   1d57c:	b.lt	1d588 <__gmpz_millerrabin@@Base+0x210>  // b.tstop
   1d580:	sub	w22, w22, #0x1
   1d584:	cbnz	w20, 1d53c <__gmpz_millerrabin@@Base+0x1c4>
   1d588:	sub	x0, x29, #0x60
   1d58c:	bl	c490 <__gmp_randclear@plt>
   1d590:	b	1d4f8 <__gmpz_millerrabin@@Base+0x180>
   1d594:	mov	w20, #0x1                   	// #1
   1d598:	b	1d4f8 <__gmpz_millerrabin@@Base+0x180>
   1d59c:	sub	x0, x29, #0x68
   1d5a0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d5a4:	b	1d3c8 <__gmpz_millerrabin@@Base+0x50>
   1d5a8:	sub	x0, x29, #0x68
   1d5ac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d5b0:	ldr	w8, [x19, #4]
   1d5b4:	b	1d40c <__gmpz_millerrabin@@Base+0x94>
   1d5b8:	sub	x0, x29, #0x68
   1d5bc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d5c0:	ldr	w8, [x19, #4]
   1d5c4:	b	1d43c <__gmpz_millerrabin@@Base+0xc4>
   1d5c8:	sub	x0, x29, #0x68
   1d5cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d5d0:	b	1d468 <__gmpz_millerrabin@@Base+0xf0>
   1d5d4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d5d8:	b	1d500 <__gmpz_millerrabin@@Base+0x188>
   1d5dc:	stp	x29, x30, [sp, #-64]!
   1d5e0:	stp	x22, x21, [sp, #32]
   1d5e4:	mov	x21, x0
   1d5e8:	stp	x20, x19, [sp, #48]
   1d5ec:	mov	x20, x2
   1d5f0:	mov	x0, x2
   1d5f4:	mov	x2, x3
   1d5f8:	mov	x3, x21
   1d5fc:	str	x23, [sp, #16]
   1d600:	mov	x29, sp
   1d604:	mov	x19, x4
   1d608:	bl	c370 <__gmpz_powm@plt>
   1d60c:	mov	w1, #0x1                   	// #1
   1d610:	mov	x0, x20
   1d614:	mov	w22, #0x1                   	// #1
   1d618:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1d61c:	cbz	w0, 1d71c <__gmpz_millerrabin@@Base+0x3a4>
   1d620:	ldrsw	x8, [x21, #4]
   1d624:	ldr	w9, [x20, #4]
   1d628:	cmp	w9, w8
   1d62c:	b.ne	1d674 <__gmpz_millerrabin@@Base+0x2fc>  // b.any
   1d630:	ldr	x10, [x20, #8]
   1d634:	ldr	x9, [x21, #8]
   1d638:	ldr	x11, [x10]
   1d63c:	ldr	x12, [x9]
   1d640:	eor	x11, x11, #0x1
   1d644:	cmp	x11, x12
   1d648:	b.ne	1d674 <__gmpz_millerrabin@@Base+0x2fc>  // b.any
   1d64c:	sub	x9, x9, #0x8
   1d650:	sub	x10, x10, #0x8
   1d654:	cmp	x8, #0x2
   1d658:	b.lt	1d710 <__gmpz_millerrabin@@Base+0x398>  // b.tstop
   1d65c:	lsl	x11, x8, #3
   1d660:	ldr	x12, [x10, x11]
   1d664:	ldr	x11, [x9, x11]
   1d668:	sub	x8, x8, #0x1
   1d66c:	cmp	x12, x11
   1d670:	b.eq	1d654 <__gmpz_millerrabin@@Base+0x2dc>  // b.none
   1d674:	cmp	x19, #0x2
   1d678:	b.cc	1d718 <__gmpz_millerrabin@@Base+0x3a0>  // b.lo, b.ul, b.last
   1d67c:	mov	w23, #0x1                   	// #1
   1d680:	mov	w2, #0x2                   	// #2
   1d684:	mov	x0, x20
   1d688:	mov	x1, x20
   1d68c:	mov	x3, x21
   1d690:	bl	d2c0 <__gmpz_powm_ui@plt>
   1d694:	ldrsw	x8, [x21, #4]
   1d698:	ldr	w9, [x20, #4]
   1d69c:	cmp	w9, w8
   1d6a0:	b.ne	1d6e8 <__gmpz_millerrabin@@Base+0x370>  // b.any
   1d6a4:	ldr	x10, [x20, #8]
   1d6a8:	ldr	x9, [x21, #8]
   1d6ac:	ldr	x11, [x10]
   1d6b0:	ldr	x12, [x9]
   1d6b4:	eor	x11, x11, #0x1
   1d6b8:	cmp	x11, x12
   1d6bc:	b.ne	1d6e8 <__gmpz_millerrabin@@Base+0x370>  // b.any
   1d6c0:	sub	x9, x9, #0x8
   1d6c4:	sub	x10, x10, #0x8
   1d6c8:	cmp	x8, #0x2
   1d6cc:	b.lt	1d710 <__gmpz_millerrabin@@Base+0x398>  // b.tstop
   1d6d0:	lsl	x11, x8, #3
   1d6d4:	ldr	x12, [x10, x11]
   1d6d8:	ldr	x11, [x9, x11]
   1d6dc:	sub	x8, x8, #0x1
   1d6e0:	cmp	x12, x11
   1d6e4:	b.eq	1d6c8 <__gmpz_millerrabin@@Base+0x350>  // b.none
   1d6e8:	mov	w1, #0x1                   	// #1
   1d6ec:	mov	x0, x20
   1d6f0:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1d6f4:	cmp	w0, #0x1
   1d6f8:	mov	w22, wzr
   1d6fc:	b.lt	1d71c <__gmpz_millerrabin@@Base+0x3a4>  // b.tstop
   1d700:	add	x23, x23, #0x1
   1d704:	cmp	x23, x19
   1d708:	b.ne	1d680 <__gmpz_millerrabin@@Base+0x308>  // b.any
   1d70c:	b	1d71c <__gmpz_millerrabin@@Base+0x3a4>
   1d710:	mov	w22, #0x1                   	// #1
   1d714:	b	1d71c <__gmpz_millerrabin@@Base+0x3a4>
   1d718:	mov	w22, wzr
   1d71c:	mov	w0, w22
   1d720:	ldp	x20, x19, [sp, #48]
   1d724:	ldp	x22, x21, [sp, #32]
   1d728:	ldr	x23, [sp, #16]
   1d72c:	ldp	x29, x30, [sp], #64
   1d730:	ret

000000000001d734 <__gmpz_mod@@Base>:
   1d734:	stp	x29, x30, [sp, #-48]!
   1d738:	stp	x22, x21, [sp, #16]
   1d73c:	stp	x20, x19, [sp, #32]
   1d740:	mov	x29, sp
   1d744:	sub	sp, sp, #0x20
   1d748:	stur	xzr, [x29, #-24]
   1d74c:	ldr	w8, [x2, #4]
   1d750:	mov	x22, x2
   1d754:	mov	x19, x0
   1d758:	mov	x20, x1
   1d75c:	cmp	w8, #0x0
   1d760:	cneg	w21, w8, mi  // mi = first
   1d764:	cmp	x0, x2
   1d768:	b.eq	1d778 <__gmpz_mod@@Base+0x44>  // b.none
   1d76c:	ldr	x8, [x22, #8]
   1d770:	stur	x8, [x29, #-8]
   1d774:	b	1d7a8 <__gmpz_mod@@Base+0x74>
   1d778:	cmp	w21, #0xfe0
   1d77c:	lsl	x1, x21, #3
   1d780:	b.hi	1d7f8 <__gmpz_mod@@Base+0xc4>  // b.pmore
   1d784:	add	x9, x1, #0xf
   1d788:	mov	x8, sp
   1d78c:	and	x9, x9, #0xffffffff0
   1d790:	sub	x0, x8, x9
   1d794:	mov	sp, x0
   1d798:	stur	x0, [x29, #-8]
   1d79c:	ldr	x1, [x22, #8]
   1d7a0:	mov	x2, x21
   1d7a4:	bl	ca50 <__gmpn_copyi@plt>
   1d7a8:	sub	x2, x29, #0x10
   1d7ac:	mov	x0, x19
   1d7b0:	mov	x1, x20
   1d7b4:	stur	w21, [x29, #-12]
   1d7b8:	bl	ca80 <__gmpz_tdiv_r@plt>
   1d7bc:	ldr	w8, [x19, #4]
   1d7c0:	tbz	w8, #31, 1d7d4 <__gmpz_mod@@Base+0xa0>
   1d7c4:	sub	x2, x29, #0x10
   1d7c8:	mov	x0, x19
   1d7cc:	mov	x1, x19
   1d7d0:	bl	cf90 <__gmpz_add@plt>
   1d7d4:	ldur	x0, [x29, #-24]
   1d7d8:	cbnz	x0, 1d7f0 <__gmpz_mod@@Base+0xbc>
   1d7dc:	mov	sp, x29
   1d7e0:	ldp	x20, x19, [sp, #32]
   1d7e4:	ldp	x22, x21, [sp, #16]
   1d7e8:	ldp	x29, x30, [sp], #48
   1d7ec:	ret
   1d7f0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1d7f4:	b	1d7dc <__gmpz_mod@@Base+0xa8>
   1d7f8:	sub	x0, x29, #0x18
   1d7fc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1d800:	b	1d798 <__gmpz_mod@@Base+0x64>

000000000001d804 <__gmpz_mul@@Base>:
   1d804:	stp	x29, x30, [sp, #-96]!
   1d808:	stp	x28, x27, [sp, #16]
   1d80c:	stp	x26, x25, [sp, #32]
   1d810:	stp	x24, x23, [sp, #48]
   1d814:	stp	x22, x21, [sp, #64]
   1d818:	stp	x20, x19, [sp, #80]
   1d81c:	mov	x29, sp
   1d820:	sub	sp, sp, #0x10
   1d824:	ldrsw	x8, [x1, #4]
   1d828:	ldrsw	x9, [x2, #4]
   1d82c:	mov	x19, x0
   1d830:	cmp	x8, #0x0
   1d834:	cneg	x10, x8, mi  // mi = first
   1d838:	cmp	x9, #0x0
   1d83c:	cneg	x11, x9, mi  // mi = first
   1d840:	cmp	x10, x11
   1d844:	csel	x21, x10, x11, lt  // lt = tstop
   1d848:	csel	x20, x11, x10, lt  // lt = tstop
   1d84c:	csel	x23, x1, x2, lt  // lt = tstop
   1d850:	csel	x22, x2, x1, lt  // lt = tstop
   1d854:	cmp	x21, #0x1
   1d858:	eor	w27, w9, w8
   1d85c:	b.eq	1d86c <__gmpz_mul@@Base+0x68>  // b.none
   1d860:	cbnz	x21, 1d8b4 <__gmpz_mul@@Base+0xb0>
   1d864:	str	wzr, [x19, #4]
   1d868:	b	1da10 <__gmpz_mul@@Base+0x20c>
   1d86c:	ldrsw	x8, [x19]
   1d870:	cmp	x20, x8
   1d874:	b.ge	1da78 <__gmpz_mul@@Base+0x274>  // b.tcont
   1d878:	ldr	x21, [x19, #8]
   1d87c:	ldr	x8, [x23, #8]
   1d880:	ldr	x1, [x22, #8]
   1d884:	mov	x0, x21
   1d888:	mov	x2, x20
   1d88c:	ldr	x3, [x8]
   1d890:	bl	d490 <__gmpn_mul_1@plt>
   1d894:	cmp	x0, #0x0
   1d898:	cinc	x8, x20, ne  // ne = any
   1d89c:	neg	w9, w8
   1d8a0:	cmp	w27, #0x0
   1d8a4:	csel	x8, x8, x9, ge  // ge = tcont
   1d8a8:	str	x0, [x21, x20, lsl #3]
   1d8ac:	str	w8, [x19, #4]
   1d8b0:	b	1da10 <__gmpz_mul@@Base+0x20c>
   1d8b4:	ldrsw	x9, [x19]
   1d8b8:	ldr	x22, [x22, #8]
   1d8bc:	ldr	x23, [x23, #8]
   1d8c0:	ldr	x24, [x19, #8]
   1d8c4:	add	x28, x20, x21
   1d8c8:	cmp	x28, x9
   1d8cc:	stp	x9, xzr, [x29, #-16]
   1d8d0:	b.le	1d950 <__gmpz_mul@@Base+0x14c>
   1d8d4:	cbz	w9, 1d900 <__gmpz_mul@@Base+0xfc>
   1d8d8:	cmp	x24, x22
   1d8dc:	b.eq	1d904 <__gmpz_mul@@Base+0x100>  // b.none
   1d8e0:	cmp	x24, x23
   1d8e4:	b.eq	1d904 <__gmpz_mul@@Base+0x100>  // b.none
   1d8e8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1d8ec:	ldr	x8, [x8, #4016]
   1d8f0:	lsl	x1, x9, #3
   1d8f4:	mov	x0, x24
   1d8f8:	ldr	x8, [x8]
   1d8fc:	blr	x8
   1d900:	mov	x24, xzr
   1d904:	str	w28, [x19]
   1d908:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1d90c:	ldr	x8, [x8, #3840]
   1d910:	lsl	x0, x28, #3
   1d914:	ldr	x8, [x8]
   1d918:	blr	x8
   1d91c:	mov	x26, x22
   1d920:	str	x0, [x19, #8]
   1d924:	mov	x25, x23
   1d928:	mov	x22, x0
   1d92c:	cmp	x26, x25
   1d930:	b.eq	1d9b4 <__gmpz_mul@@Base+0x1b0>  // b.none
   1d934:	mov	x0, x22
   1d938:	mov	x1, x26
   1d93c:	mov	x2, x20
   1d940:	mov	x3, x25
   1d944:	mov	x4, x21
   1d948:	bl	ccd0 <__gmpn_mul@plt>
   1d94c:	b	1d9cc <__gmpz_mul@@Base+0x1c8>
   1d950:	cmp	x24, x22
   1d954:	b.eq	1d970 <__gmpz_mul@@Base+0x16c>  // b.none
   1d958:	cmp	x24, x23
   1d95c:	b.eq	1da30 <__gmpz_mul@@Base+0x22c>  // b.none
   1d960:	mov	x26, x22
   1d964:	mov	x25, x23
   1d968:	mov	x22, x24
   1d96c:	b	1d9a8 <__gmpz_mul@@Base+0x1a4>
   1d970:	cmp	x20, #0xfe0
   1d974:	lsl	x1, x20, #3
   1d978:	b.hi	1da94 <__gmpz_mul@@Base+0x290>  // b.pmore
   1d97c:	add	x9, x1, #0xf
   1d980:	mov	x8, sp
   1d984:	and	x9, x9, #0xfffffffffffffff0
   1d988:	sub	x26, x8, x9
   1d98c:	mov	sp, x26
   1d990:	cmp	x22, x23
   1d994:	mov	x0, x26
   1d998:	mov	x1, x22
   1d99c:	mov	x2, x20
   1d9a0:	csel	x25, x26, x23, eq  // eq = none
   1d9a4:	bl	ca50 <__gmpn_copyi@plt>
   1d9a8:	mov	x24, xzr
   1d9ac:	cmp	x26, x25
   1d9b0:	b.ne	1d934 <__gmpz_mul@@Base+0x130>  // b.any
   1d9b4:	mov	x0, x22
   1d9b8:	mov	x1, x26
   1d9bc:	mov	x2, x20
   1d9c0:	bl	c8e0 <__gmpn_sqr@plt>
   1d9c4:	add	x8, x22, x28, lsl #3
   1d9c8:	ldur	x0, [x8, #-8]
   1d9cc:	cmp	x0, #0x0
   1d9d0:	cset	w8, eq  // eq = none
   1d9d4:	sub	x8, x28, x8
   1d9d8:	neg	w9, w8
   1d9dc:	cmp	w27, #0x0
   1d9e0:	csel	x8, x9, x8, lt  // lt = tstop
   1d9e4:	str	w8, [x19, #4]
   1d9e8:	cbz	x24, 1da08 <__gmpz_mul@@Base+0x204>
   1d9ec:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1d9f0:	ldr	x8, [x8, #4016]
   1d9f4:	ldur	x9, [x29, #-16]
   1d9f8:	mov	x0, x24
   1d9fc:	ldr	x8, [x8]
   1da00:	lsl	x1, x9, #3
   1da04:	blr	x8
   1da08:	ldur	x0, [x29, #-8]
   1da0c:	cbnz	x0, 1da8c <__gmpz_mul@@Base+0x288>
   1da10:	mov	sp, x29
   1da14:	ldp	x20, x19, [sp, #80]
   1da18:	ldp	x22, x21, [sp, #64]
   1da1c:	ldp	x24, x23, [sp, #48]
   1da20:	ldp	x26, x25, [sp, #32]
   1da24:	ldp	x28, x27, [sp, #16]
   1da28:	ldp	x29, x30, [sp], #96
   1da2c:	ret
   1da30:	cmp	x21, #0xfe0
   1da34:	lsl	x1, x21, #3
   1da38:	b.hi	1daa4 <__gmpz_mul@@Base+0x2a0>  // b.pmore
   1da3c:	add	x9, x1, #0xf
   1da40:	mov	x8, sp
   1da44:	and	x9, x9, #0xfffffffffffffff0
   1da48:	sub	x25, x8, x9
   1da4c:	mov	sp, x25
   1da50:	mov	x0, x25
   1da54:	mov	x1, x23
   1da58:	mov	x2, x21
   1da5c:	bl	ca50 <__gmpn_copyi@plt>
   1da60:	mov	x24, xzr
   1da64:	mov	x26, x22
   1da68:	mov	x22, x23
   1da6c:	cmp	x26, x25
   1da70:	b.ne	1d934 <__gmpz_mul@@Base+0x130>  // b.any
   1da74:	b	1d9b4 <__gmpz_mul@@Base+0x1b0>
   1da78:	add	x1, x20, #0x1
   1da7c:	mov	x0, x19
   1da80:	bl	c080 <__gmpz_realloc@plt>
   1da84:	mov	x21, x0
   1da88:	b	1d87c <__gmpz_mul@@Base+0x78>
   1da8c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1da90:	b	1da10 <__gmpz_mul@@Base+0x20c>
   1da94:	sub	x0, x29, #0x8
   1da98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1da9c:	mov	x26, x0
   1daa0:	b	1d990 <__gmpz_mul@@Base+0x18c>
   1daa4:	sub	x0, x29, #0x8
   1daa8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1daac:	mov	x25, x0
   1dab0:	b	1da50 <__gmpz_mul@@Base+0x24c>

000000000001dab4 <__gmpz_mul_2exp@@Base>:
   1dab4:	stp	x29, x30, [sp, #-80]!
   1dab8:	stp	x24, x23, [sp, #32]
   1dabc:	stp	x22, x21, [sp, #48]
   1dac0:	stp	x20, x19, [sp, #64]
   1dac4:	ldr	w8, [x1, #4]
   1dac8:	mov	x20, x1
   1dacc:	mov	x19, x0
   1dad0:	str	x25, [sp, #16]
   1dad4:	cmp	w8, #0x0
   1dad8:	cneg	w22, w8, mi  // mi = first
   1dadc:	mov	x29, sp
   1dae0:	cbz	w22, 1db2c <__gmpz_mul_2exp@@Base+0x78>
   1dae4:	ldrsw	x8, [x19]
   1dae8:	lsr	x25, x2, #6
   1daec:	add	x24, x25, x22
   1daf0:	mov	x23, x2
   1daf4:	cmp	x24, x8
   1daf8:	b.ge	1db78 <__gmpz_mul_2exp@@Base+0xc4>  // b.tcont
   1dafc:	ldr	x21, [x19, #8]
   1db00:	ldr	x1, [x20, #8]
   1db04:	ands	x3, x23, #0x3f
   1db08:	add	x0, x21, x25, lsl #3
   1db0c:	mov	x2, x22
   1db10:	b.eq	1db34 <__gmpz_mul_2exp@@Base+0x80>  // b.none
   1db14:	bl	c180 <__gmpn_lshift@plt>
   1db18:	cmp	x0, #0x0
   1db1c:	str	x0, [x21, x24, lsl #3]
   1db20:	cinc	x24, x24, ne  // ne = any
   1db24:	cbnz	x25, 1db3c <__gmpz_mul_2exp@@Base+0x88>
   1db28:	b	1db4c <__gmpz_mul_2exp@@Base+0x98>
   1db2c:	mov	x24, xzr
   1db30:	b	1db4c <__gmpz_mul_2exp@@Base+0x98>
   1db34:	bl	c000 <__gmpn_copyd@plt>
   1db38:	cbz	x25, 1db4c <__gmpz_mul_2exp@@Base+0x98>
   1db3c:	lsl	x2, x25, #3
   1db40:	mov	x0, x21
   1db44:	mov	w1, wzr
   1db48:	bl	c5f0 <memset@plt>
   1db4c:	ldr	w8, [x20, #4]
   1db50:	neg	w9, w24
   1db54:	ldr	x25, [sp, #16]
   1db58:	cmp	w8, #0x0
   1db5c:	csel	x8, x24, x9, ge  // ge = tcont
   1db60:	str	w8, [x19, #4]
   1db64:	ldp	x20, x19, [sp, #64]
   1db68:	ldp	x22, x21, [sp, #48]
   1db6c:	ldp	x24, x23, [sp, #32]
   1db70:	ldp	x29, x30, [sp], #80
   1db74:	ret
   1db78:	add	x1, x24, #0x1
   1db7c:	mov	x0, x19
   1db80:	bl	c080 <__gmpz_realloc@plt>
   1db84:	mov	x21, x0
   1db88:	b	1db00 <__gmpz_mul_2exp@@Base+0x4c>

000000000001db8c <__gmpz_mul_si@@Base>:
   1db8c:	stp	x29, x30, [sp, #-80]!
   1db90:	stp	x20, x19, [sp, #64]
   1db94:	mov	x19, x0
   1db98:	mov	w8, wzr
   1db9c:	str	x25, [sp, #16]
   1dba0:	stp	x24, x23, [sp, #32]
   1dba4:	stp	x22, x21, [sp, #48]
   1dba8:	mov	x29, sp
   1dbac:	cbz	x2, 1dc10 <__gmpz_mul_si@@Base+0x84>
   1dbb0:	ldr	w9, [x1, #4]
   1dbb4:	mov	x21, x1
   1dbb8:	cbz	w9, 1dc10 <__gmpz_mul_si@@Base+0x84>
   1dbbc:	ldrsw	x8, [x19]
   1dbc0:	sxtw	x25, w9
   1dbc4:	cmp	x25, #0x0
   1dbc8:	cneg	x22, x25, mi  // mi = first
   1dbcc:	cmp	x2, #0x0
   1dbd0:	mov	x20, x2
   1dbd4:	cneg	x23, x2, mi  // mi = first
   1dbd8:	cmp	x22, x8
   1dbdc:	b.ge	1dc2c <__gmpz_mul_si@@Base+0xa0>  // b.tcont
   1dbe0:	ldr	x24, [x19, #8]
   1dbe4:	ldr	x1, [x21, #8]
   1dbe8:	mov	x0, x24
   1dbec:	mov	x2, x22
   1dbf0:	mov	x3, x23
   1dbf4:	bl	d490 <__gmpn_mul_1@plt>
   1dbf8:	cmp	x0, #0x0
   1dbfc:	lsr	x8, x20, #63
   1dc00:	cinc	w9, w22, ne  // ne = any
   1dc04:	cmp	w8, w25, lsr #31
   1dc08:	cneg	w8, w9, ne  // ne = any
   1dc0c:	str	x0, [x24, x22, lsl #3]
   1dc10:	str	w8, [x19, #4]
   1dc14:	ldp	x20, x19, [sp, #64]
   1dc18:	ldp	x22, x21, [sp, #48]
   1dc1c:	ldp	x24, x23, [sp, #32]
   1dc20:	ldr	x25, [sp, #16]
   1dc24:	ldp	x29, x30, [sp], #80
   1dc28:	ret
   1dc2c:	add	x1, x22, #0x1
   1dc30:	mov	x0, x19
   1dc34:	bl	c080 <__gmpz_realloc@plt>
   1dc38:	mov	x24, x0
   1dc3c:	b	1dbe4 <__gmpz_mul_si@@Base+0x58>

000000000001dc40 <__gmpz_mul_ui@@Base>:
   1dc40:	stp	x29, x30, [sp, #-64]!
   1dc44:	stp	x20, x19, [sp, #48]
   1dc48:	mov	x19, x0
   1dc4c:	mov	w8, wzr
   1dc50:	stp	x24, x23, [sp, #16]
   1dc54:	stp	x22, x21, [sp, #32]
   1dc58:	mov	x29, sp
   1dc5c:	cbz	x2, 1dcb4 <__gmpz_mul_ui@@Base+0x74>
   1dc60:	ldr	w9, [x1, #4]
   1dc64:	mov	x21, x1
   1dc68:	cbz	w9, 1dcb4 <__gmpz_mul_ui@@Base+0x74>
   1dc6c:	ldrsw	x8, [x19]
   1dc70:	sxtw	x24, w9
   1dc74:	cmp	x24, #0x0
   1dc78:	cneg	x22, x24, mi  // mi = first
   1dc7c:	mov	x20, x2
   1dc80:	cmp	x22, x8
   1dc84:	b.ge	1dccc <__gmpz_mul_ui@@Base+0x8c>  // b.tcont
   1dc88:	ldr	x23, [x19, #8]
   1dc8c:	ldr	x1, [x21, #8]
   1dc90:	mov	x0, x23
   1dc94:	mov	x2, x22
   1dc98:	mov	x3, x20
   1dc9c:	bl	d490 <__gmpn_mul_1@plt>
   1dca0:	cmp	x0, #0x0
   1dca4:	cinc	w8, w22, ne  // ne = any
   1dca8:	cmp	w24, #0x0
   1dcac:	cneg	w8, w8, lt  // lt = tstop
   1dcb0:	str	x0, [x23, x22, lsl #3]
   1dcb4:	str	w8, [x19, #4]
   1dcb8:	ldp	x20, x19, [sp, #48]
   1dcbc:	ldp	x22, x21, [sp, #32]
   1dcc0:	ldp	x24, x23, [sp, #16]
   1dcc4:	ldp	x29, x30, [sp], #64
   1dcc8:	ret
   1dccc:	add	x1, x22, #0x1
   1dcd0:	mov	x0, x19
   1dcd4:	bl	c080 <__gmpz_realloc@plt>
   1dcd8:	mov	x23, x0
   1dcdc:	b	1dc8c <__gmpz_mul_ui@@Base+0x4c>

000000000001dce0 <__gmpz_n_pow_ui@@Base>:
   1dce0:	stp	x29, x30, [sp, #-96]!
   1dce4:	stp	x28, x27, [sp, #16]
   1dce8:	stp	x26, x25, [sp, #32]
   1dcec:	stp	x24, x23, [sp, #48]
   1dcf0:	stp	x22, x21, [sp, #64]
   1dcf4:	stp	x20, x19, [sp, #80]
   1dcf8:	mov	x29, sp
   1dcfc:	sub	sp, sp, #0x40
   1dd00:	mov	x26, x0
   1dd04:	cbz	x3, 1ddd0 <__gmpz_n_pow_ui@@Base+0xf0>
   1dd08:	cbz	x2, 1ddec <__gmpz_n_pow_ui@@Base+0x10c>
   1dd0c:	ldr	x10, [x1]
   1dd10:	ldr	x8, [x26, #8]
   1dd14:	cmp	x2, #0x0
   1dd18:	mov	x21, x3
   1dd1c:	cset	w11, lt  // lt = tstop
   1dd20:	cneg	x23, x2, mi  // mi = first
   1dd24:	mov	x9, xzr
   1dd28:	mov	x22, x1
   1dd2c:	stur	w11, [x29, #-52]
   1dd30:	cbnz	x10, 1dd44 <__gmpz_n_pow_ui@@Base+0x64>
   1dd34:	ldr	x10, [x22, #8]!
   1dd38:	add	x9, x9, x21
   1dd3c:	sub	x23, x23, #0x1
   1dd40:	cbz	x10, 1dd34 <__gmpz_n_pow_ui@@Base+0x54>
   1dd44:	rbit	x11, x10
   1dd48:	clz	x25, x11
   1dd4c:	lsr	x24, x10, x25
   1dd50:	mul	x10, x25, x21
   1dd54:	cmp	x23, #0x2
   1dd58:	add	x27, x9, x10, lsr #6
   1dd5c:	and	x28, x10, #0x3f
   1dd60:	stur	xzr, [x29, #-24]
   1dd64:	b.eq	1ddf4 <__gmpz_n_pow_ui@@Base+0x114>  // b.none
   1dd68:	cmp	x23, #0x1
   1dd6c:	b.eq	1de2c <__gmpz_n_pow_ui@@Base+0x14c>  // b.none
   1dd70:	cmp	x8, x1
   1dd74:	b.eq	1dd7c <__gmpz_n_pow_ui@@Base+0x9c>  // b.none
   1dd78:	cbz	w25, 1de94 <__gmpz_n_pow_ui@@Base+0x1b4>
   1dd7c:	lsl	x1, x23, #3
   1dd80:	mov	w8, #0x7f00                	// #32512
   1dd84:	cmp	x1, x8
   1dd88:	b.hi	1e1c4 <__gmpz_n_pow_ui@@Base+0x4e4>  // b.pmore
   1dd8c:	add	x9, x1, #0xf
   1dd90:	mov	x8, sp
   1dd94:	and	x9, x9, #0xfffffffffffffff0
   1dd98:	sub	x24, x8, x9
   1dd9c:	mov	sp, x24
   1dda0:	mov	x0, x24
   1dda4:	mov	x1, x22
   1dda8:	mov	x2, x23
   1ddac:	cbz	w25, 1de8c <__gmpz_n_pow_ui@@Base+0x1ac>
   1ddb0:	mov	w3, w25
   1ddb4:	bl	c1a0 <__gmpn_rshift@plt>
   1ddb8:	add	x8, x24, x23, lsl #3
   1ddbc:	ldur	x8, [x8, #-8]
   1ddc0:	cmp	x8, #0x0
   1ddc4:	cset	w8, eq  // eq = none
   1ddc8:	sub	x23, x23, x8
   1ddcc:	b	1de90 <__gmpz_n_pow_ui@@Base+0x1b0>
   1ddd0:	ldr	w8, [x26]
   1ddd4:	cmp	w8, #0x0
   1ddd8:	b.le	1e19c <__gmpz_n_pow_ui@@Base+0x4bc>
   1dddc:	ldr	x0, [x26, #8]
   1dde0:	mov	w8, #0x1                   	// #1
   1dde4:	str	x8, [x0]
   1dde8:	b	1e0c8 <__gmpz_n_pow_ui@@Base+0x3e8>
   1ddec:	mov	w8, wzr
   1ddf0:	b	1e0c8 <__gmpz_n_pow_ui@@Base+0x3e8>
   1ddf4:	ldr	x8, [x22, #8]
   1ddf8:	neg	x9, x25
   1ddfc:	cmp	w25, #0x0
   1de00:	lsl	x9, x8, x9
   1de04:	csel	x9, xzr, x9, eq  // eq = none
   1de08:	lsr	x8, x8, x25
   1de0c:	orr	x24, x9, x24
   1de10:	cbz	x8, 1de2c <__gmpz_n_pow_ui@@Base+0x14c>
   1de14:	sub	x22, x29, #0x10
   1de18:	stp	x24, x8, [x29, #-16]
   1de1c:	mov	w23, #0x2                   	// #2
   1de20:	mov	w25, #0x1                   	// #1
   1de24:	mov	x24, x8
   1de28:	b	1dea0 <__gmpz_n_pow_ui@@Base+0x1c0>
   1de2c:	mov	w25, #0x1                   	// #1
   1de30:	mov	x20, x21
   1de34:	lsr	x8, x24, #32
   1de38:	cbnz	x8, 1de84 <__gmpz_n_pow_ui@@Base+0x1a4>
   1de3c:	tst	x20, #0x1
   1de40:	csinc	x8, x24, xzr, ne  // ne = any
   1de44:	lsr	x20, x20, #1
   1de48:	mul	x25, x8, x25
   1de4c:	cbz	x20, 1de5c <__gmpz_n_pow_ui@@Base+0x17c>
   1de50:	mul	x24, x24, x24
   1de54:	lsr	x8, x24, #32
   1de58:	cbz	x8, 1de3c <__gmpz_n_pow_ui@@Base+0x15c>
   1de5c:	mov	w23, #0x1                   	// #1
   1de60:	cbz	x28, 1dea4 <__gmpz_n_pow_ui@@Base+0x1c4>
   1de64:	cmp	x25, #0x1
   1de68:	b.eq	1dea4 <__gmpz_n_pow_ui@@Base+0x1c4>  // b.none
   1de6c:	neg	x8, x28
   1de70:	lsr	x8, x25, x8
   1de74:	cmp	x8, #0x0
   1de78:	csel	x8, x28, xzr, eq  // eq = none
   1de7c:	csel	x28, xzr, x28, eq  // eq = none
   1de80:	lsl	x25, x25, x8
   1de84:	mov	w23, #0x1                   	// #1
   1de88:	b	1dea4 <__gmpz_n_pow_ui@@Base+0x1c4>
   1de8c:	bl	ca50 <__gmpn_copyi@plt>
   1de90:	mov	x22, x24
   1de94:	add	x8, x22, x23, lsl #3
   1de98:	ldur	x24, [x8, #-8]
   1de9c:	mov	w25, #0x1                   	// #1
   1dea0:	mov	x20, x21
   1dea4:	clz	x8, x24
   1dea8:	lsl	x9, x23, #6
   1deac:	sub	x8, x9, x8
   1deb0:	mul	x8, x8, x20
   1deb4:	ldrsw	x9, [x26]
   1deb8:	lsr	x8, x8, #6
   1debc:	add	x19, x8, #0x5
   1dec0:	add	x1, x19, x27
   1dec4:	cmp	x1, x9
   1dec8:	stur	x26, [x29, #-32]
   1decc:	b.gt	1e180 <__gmpz_n_pow_ui@@Base+0x4a0>
   1ded0:	ldr	x26, [x26, #8]
   1ded4:	cbz	x27, 1dee8 <__gmpz_n_pow_ui@@Base+0x208>
   1ded8:	lsl	x2, x27, #3
   1dedc:	mov	x0, x26
   1dee0:	mov	w1, wzr
   1dee4:	bl	c5f0 <memset@plt>
   1dee8:	add	x12, x26, x27, lsl #3
   1deec:	stp	x28, x27, [x29, #-48]
   1def0:	cbz	x20, 1df68 <__gmpz_n_pow_ui@@Base+0x288>
   1def4:	cmp	x23, #0x2
   1def8:	mvn	w8, w20
   1defc:	cset	w9, lt  // lt = tstop
   1df00:	and	x8, x8, #0x1
   1df04:	orr	x8, x8, x9
   1df08:	lsr	x8, x19, x8
   1df0c:	cmp	x8, #0xfe0
   1df10:	lsl	x1, x8, #3
   1df14:	b.hi	1e1ac <__gmpz_n_pow_ui@@Base+0x4cc>  // b.pmore
   1df18:	add	x9, x1, #0xf
   1df1c:	mov	x8, sp
   1df20:	and	x9, x9, #0x7ffffffffffffff0
   1df24:	sub	x27, x8, x9
   1df28:	mov	sp, x27
   1df2c:	clz	x19, x20
   1df30:	mov	w8, #0x3e                  	// #62
   1df34:	cmp	x23, #0x1
   1df38:	sub	w28, w8, w19
   1df3c:	b.ne	1df7c <__gmpz_n_pow_ui@@Base+0x29c>  // b.any
   1df40:	tst	w28, #0x1
   1df44:	csel	x26, x27, x12, eq  // eq = none
   1df48:	cmp	w19, #0x3f
   1df4c:	str	x24, [x26]
   1df50:	b.ne	1dfd8 <__gmpz_n_pow_ui@@Base+0x2f8>  // b.any
   1df54:	mov	w27, #0x1                   	// #1
   1df58:	cmp	x25, #0x1
   1df5c:	b.ne	1e064 <__gmpz_n_pow_ui@@Base+0x384>  // b.any
   1df60:	ldur	w25, [x29, #-52]
   1df64:	b	1e088 <__gmpz_n_pow_ui@@Base+0x3a8>
   1df68:	str	x25, [x12]
   1df6c:	ldur	w25, [x29, #-52]
   1df70:	mov	w27, #0x1                   	// #1
   1df74:	mov	x26, x12
   1df78:	b	1e088 <__gmpz_n_pow_ui@@Base+0x3a8>
   1df7c:	mov	w9, #0x6996                	// #27030
   1df80:	mov	x8, xzr
   1df84:	movk	w9, #0x9669, lsl #16
   1df88:	mov	x10, x20
   1df8c:	and	x11, x10, #0x1f
   1df90:	sxtw	x8, w8
   1df94:	lsr	x11, x9, x11
   1df98:	lsr	x10, x10, #5
   1df9c:	eor	x8, x8, x11
   1dfa0:	cbnz	x10, 1df8c <__gmpz_n_pow_ui@@Base+0x2ac>
   1dfa4:	eor	w24, w28, w8
   1dfa8:	tst	w24, #0x1
   1dfac:	csel	x26, x12, x27, eq  // eq = none
   1dfb0:	mov	x0, x26
   1dfb4:	mov	x1, x22
   1dfb8:	mov	x2, x23
   1dfbc:	stur	x12, [x29, #-64]
   1dfc0:	bl	ca50 <__gmpn_copyi@plt>
   1dfc4:	ldur	w25, [x29, #-52]
   1dfc8:	cmp	w19, #0x3f
   1dfcc:	b.ne	1e0ec <__gmpz_n_pow_ui@@Base+0x40c>  // b.any
   1dfd0:	mov	x27, x23
   1dfd4:	b	1e088 <__gmpz_n_pow_ui@@Base+0x3a8>
   1dfd8:	tst	w28, #0x1
   1dfdc:	mov	w8, #0x3f                  	// #63
   1dfe0:	csel	x22, x12, x27, eq  // eq = none
   1dfe4:	sub	w19, w8, w19
   1dfe8:	mov	w27, #0x1                   	// #1
   1dfec:	mov	x0, x26
   1dff0:	b	1e008 <__gmpz_n_pow_ui@@Base+0x328>
   1dff4:	sub	w19, w19, #0x1
   1dff8:	cmp	w19, #0x0
   1dffc:	sub	x28, x28, #0x1
   1e000:	mov	x0, x26
   1e004:	b.le	1df58 <__gmpz_n_pow_ui@@Base+0x278>
   1e008:	mov	x26, x22
   1e00c:	mov	x22, x0
   1e010:	mov	x0, x26
   1e014:	mov	x1, x22
   1e018:	mov	x2, x27
   1e01c:	bl	c8e0 <__gmpn_sqr@plt>
   1e020:	add	x8, x26, x27, lsl #4
   1e024:	ldur	x8, [x8, #-8]
   1e028:	lsl	x9, x27, #1
   1e02c:	lsr	x10, x20, x28
   1e030:	cmp	x8, #0x0
   1e034:	cset	w8, eq  // eq = none
   1e038:	sub	x27, x9, x8
   1e03c:	tbz	w10, #0, 1dff4 <__gmpz_n_pow_ui@@Base+0x314>
   1e040:	mov	x0, x26
   1e044:	mov	x1, x26
   1e048:	mov	x2, x27
   1e04c:	mov	x3, x24
   1e050:	bl	d490 <__gmpn_mul_1@plt>
   1e054:	cmp	x0, #0x0
   1e058:	str	x0, [x26, x27, lsl #3]
   1e05c:	cinc	x27, x27, ne  // ne = any
   1e060:	b	1dff4 <__gmpz_n_pow_ui@@Base+0x314>
   1e064:	mov	x0, x26
   1e068:	mov	x1, x26
   1e06c:	mov	x2, x27
   1e070:	mov	x3, x25
   1e074:	bl	d490 <__gmpn_mul_1@plt>
   1e078:	ldur	w25, [x29, #-52]
   1e07c:	cmp	x0, #0x0
   1e080:	str	x0, [x26, x27, lsl #3]
   1e084:	cinc	x27, x27, ne  // ne = any
   1e088:	ldur	x0, [x29, #-24]
   1e08c:	cbnz	x0, 1e194 <__gmpz_n_pow_ui@@Base+0x4b4>
   1e090:	ldur	x3, [x29, #-48]
   1e094:	and	w19, w21, w25
   1e098:	cbz	x3, 1e0b8 <__gmpz_n_pow_ui@@Base+0x3d8>
   1e09c:	mov	x0, x26
   1e0a0:	mov	x1, x26
   1e0a4:	mov	x2, x27
   1e0a8:	bl	c180 <__gmpn_lshift@plt>
   1e0ac:	cmp	x0, #0x0
   1e0b0:	str	x0, [x26, x27, lsl #3]
   1e0b4:	cinc	x27, x27, ne  // ne = any
   1e0b8:	ldp	x8, x26, [x29, #-40]
   1e0bc:	cmp	w19, #0x0
   1e0c0:	add	w8, w27, w8
   1e0c4:	cneg	w8, w8, ne  // ne = any
   1e0c8:	str	w8, [x26, #4]
   1e0cc:	mov	sp, x29
   1e0d0:	ldp	x20, x19, [sp, #80]
   1e0d4:	ldp	x22, x21, [sp, #64]
   1e0d8:	ldp	x24, x23, [sp, #48]
   1e0dc:	ldp	x26, x25, [sp, #32]
   1e0e0:	ldp	x28, x27, [sp, #16]
   1e0e4:	ldp	x29, x30, [sp], #96
   1e0e8:	ret
   1e0ec:	ldur	x9, [x29, #-64]
   1e0f0:	tst	w24, #0x1
   1e0f4:	mov	w8, #0x3f                  	// #63
   1e0f8:	sub	w19, w8, w19
   1e0fc:	csel	x24, x27, x9, eq  // eq = none
   1e100:	mov	x27, x23
   1e104:	b	1e140 <__gmpz_n_pow_ui@@Base+0x460>
   1e108:	mov	x0, x26
   1e10c:	mov	x1, x24
   1e110:	mov	x2, x27
   1e114:	mov	x3, x22
   1e118:	mov	x4, x23
   1e11c:	bl	ccd0 <__gmpn_mul@plt>
   1e120:	cmp	x0, #0x0
   1e124:	cset	w8, eq  // eq = none
   1e128:	add	x9, x27, x23
   1e12c:	sub	x27, x9, x8
   1e130:	sub	w19, w19, #0x1
   1e134:	cmp	w19, #0x0
   1e138:	sub	x28, x28, #0x1
   1e13c:	b.le	1e088 <__gmpz_n_pow_ui@@Base+0x3a8>
   1e140:	mov	x0, x24
   1e144:	mov	x1, x26
   1e148:	mov	x2, x27
   1e14c:	bl	c8e0 <__gmpn_sqr@plt>
   1e150:	add	x8, x24, x27, lsl #4
   1e154:	ldur	x8, [x8, #-8]
   1e158:	lsl	x9, x27, #1
   1e15c:	lsr	x10, x20, x28
   1e160:	cmp	x8, #0x0
   1e164:	cset	w8, eq  // eq = none
   1e168:	sub	x27, x9, x8
   1e16c:	tbnz	w10, #0, 1e108 <__gmpz_n_pow_ui@@Base+0x428>
   1e170:	mov	x0, x26
   1e174:	mov	x26, x24
   1e178:	mov	x24, x0
   1e17c:	b	1e130 <__gmpz_n_pow_ui@@Base+0x450>
   1e180:	mov	x0, x26
   1e184:	bl	c080 <__gmpz_realloc@plt>
   1e188:	mov	x26, x0
   1e18c:	cbnz	x27, 1ded8 <__gmpz_n_pow_ui@@Base+0x1f8>
   1e190:	b	1dee8 <__gmpz_n_pow_ui@@Base+0x208>
   1e194:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1e198:	b	1e090 <__gmpz_n_pow_ui@@Base+0x3b0>
   1e19c:	mov	w1, #0x1                   	// #1
   1e1a0:	mov	x0, x26
   1e1a4:	bl	c080 <__gmpz_realloc@plt>
   1e1a8:	b	1dde0 <__gmpz_n_pow_ui@@Base+0x100>
   1e1ac:	sub	x0, x29, #0x18
   1e1b0:	mov	x19, x12
   1e1b4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e1b8:	mov	x12, x19
   1e1bc:	mov	x27, x0
   1e1c0:	b	1df2c <__gmpz_n_pow_ui@@Base+0x24c>
   1e1c4:	sub	x0, x29, #0x18
   1e1c8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e1cc:	mov	x24, x0
   1e1d0:	b	1dda0 <__gmpz_n_pow_ui@@Base+0xc0>

000000000001e1d4 <__gmpz_neg@@Base>:
   1e1d4:	stp	x29, x30, [sp, #-48]!
   1e1d8:	stp	x22, x21, [sp, #16]
   1e1dc:	stp	x20, x19, [sp, #32]
   1e1e0:	ldrsw	x22, [x1, #4]
   1e1e4:	mov	x19, x0
   1e1e8:	cmp	x1, x0
   1e1ec:	mov	x29, sp
   1e1f0:	b.eq	1e21c <__gmpz_neg@@Base+0x48>  // b.none
   1e1f4:	ldrsw	x8, [x19]
   1e1f8:	cmp	x22, #0x0
   1e1fc:	cneg	x21, x22, mi  // mi = first
   1e200:	mov	x20, x1
   1e204:	cmp	x21, x8
   1e208:	b.gt	1e234 <__gmpz_neg@@Base+0x60>
   1e20c:	ldr	x0, [x19, #8]
   1e210:	ldr	x1, [x20, #8]
   1e214:	mov	x2, x21
   1e218:	bl	ca50 <__gmpn_copyi@plt>
   1e21c:	neg	w8, w22
   1e220:	str	w8, [x19, #4]
   1e224:	ldp	x20, x19, [sp, #32]
   1e228:	ldp	x22, x21, [sp, #16]
   1e22c:	ldp	x29, x30, [sp], #48
   1e230:	ret
   1e234:	mov	x0, x19
   1e238:	mov	x1, x21
   1e23c:	bl	c080 <__gmpz_realloc@plt>
   1e240:	b	1e210 <__gmpz_neg@@Base+0x3c>

000000000001e244 <__gmpz_nextprime@@Base>:
   1e244:	stp	x29, x30, [sp, #-96]!
   1e248:	stp	x20, x19, [sp, #80]
   1e24c:	mov	x20, x1
   1e250:	mov	x19, x0
   1e254:	mov	w1, #0x2                   	// #2
   1e258:	mov	x0, x20
   1e25c:	str	x27, [sp, #16]
   1e260:	stp	x26, x25, [sp, #32]
   1e264:	stp	x24, x23, [sp, #48]
   1e268:	stp	x22, x21, [sp, #64]
   1e26c:	mov	x29, sp
   1e270:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1e274:	tbnz	w0, #31, 1e3d8 <__gmpz_nextprime@@Base+0x194>
   1e278:	mov	w2, #0x1                   	// #1
   1e27c:	mov	x0, x19
   1e280:	mov	x1, x20
   1e284:	bl	c8b0 <__gmpz_add_ui@plt>
   1e288:	mov	x0, x19
   1e28c:	mov	x1, xzr
   1e290:	bl	c310 <__gmpz_setbit@plt>
   1e294:	mov	w1, #0x7                   	// #7
   1e298:	mov	x0, x19
   1e29c:	bl	d1f0 <__gmpz_cmp_ui@plt>
   1e2a0:	cmp	w0, #0x1
   1e2a4:	b.lt	1e3b8 <__gmpz_nextprime@@Base+0x174>  // b.tstop
   1e2a8:	ldrsw	x8, [x19, #4]
   1e2ac:	ldr	x9, [x19, #8]
   1e2b0:	add	x9, x9, x8, lsl #3
   1e2b4:	ldur	x9, [x9, #-8]
   1e2b8:	lsl	x8, x8, #6
   1e2bc:	clz	x9, x9
   1e2c0:	sub	x8, x8, x9
   1e2c4:	lsr	x9, x8, #1
   1e2c8:	cmp	x8, #0x14d
   1e2cc:	mov	w8, #0xa6                  	// #166
   1e2d0:	csel	w21, w8, w9, hi  // hi = pmore
   1e2d4:	lsl	x8, x21, #1
   1e2d8:	add	x8, x8, #0xf
   1e2dc:	and	x8, x8, #0x3fffffff0
   1e2e0:	mov	x9, sp
   1e2e4:	sub	x22, x9, x8
   1e2e8:	mov	sp, x22
   1e2ec:	adrp	x24, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1e2f0:	mov	w23, #0xfffe                	// #65534
   1e2f4:	add	x24, x24, #0x570
   1e2f8:	b	1e308 <__gmpz_nextprime@@Base+0xc4>
   1e2fc:	mov	x0, x19
   1e300:	mov	x1, x19
   1e304:	bl	c8b0 <__gmpz_add_ui@plt>
   1e308:	cbz	w21, 1e33c <__gmpz_nextprime@@Base+0xf8>
   1e30c:	mov	x25, x21
   1e310:	mov	x26, x22
   1e314:	mov	x27, x24
   1e318:	mov	w20, #0x3                   	// #3
   1e31c:	mov	x0, x19
   1e320:	mov	x1, x20
   1e324:	bl	bfa0 <__gmpz_tdiv_ui@plt>
   1e328:	strh	w0, [x26], #2
   1e32c:	ldrb	w8, [x27], #1
   1e330:	subs	x25, x25, #0x1
   1e334:	add	x20, x20, x8
   1e338:	b.ne	1e31c <__gmpz_nextprime@@Base+0xd8>  // b.any
   1e33c:	mov	x2, xzr
   1e340:	mov	w20, wzr
   1e344:	b	1e378 <__gmpz_nextprime@@Base+0x134>
   1e348:	mov	x0, x19
   1e34c:	mov	x1, x19
   1e350:	bl	c8b0 <__gmpz_add_ui@plt>
   1e354:	mov	w1, #0x19                  	// #25
   1e358:	mov	x0, x19
   1e35c:	bl	cb80 <__gmpz_millerrabin@plt>
   1e360:	cbnz	w0, 1e3b8 <__gmpz_nextprime@@Base+0x174>
   1e364:	mov	x2, xzr
   1e368:	cmp	w20, w23
   1e36c:	add	w20, w20, #0x2
   1e370:	add	x2, x2, #0x2
   1e374:	b.cs	1e2fc <__gmpz_nextprime@@Base+0xb8>  // b.hs, b.nlast
   1e378:	cbz	w21, 1e348 <__gmpz_nextprime@@Base+0x104>
   1e37c:	mov	x8, x21
   1e380:	mov	x9, x22
   1e384:	mov	x10, x24
   1e388:	mov	w11, #0x3                   	// #3
   1e38c:	ldrh	w12, [x9]
   1e390:	add	x12, x12, w20, uxtw
   1e394:	udiv	x13, x12, x11
   1e398:	msub	x12, x13, x11, x12
   1e39c:	cbz	x12, 1e368 <__gmpz_nextprime@@Base+0x124>
   1e3a0:	ldrb	w12, [x10], #1
   1e3a4:	subs	x8, x8, #0x1
   1e3a8:	add	x9, x9, #0x2
   1e3ac:	add	x11, x11, x12
   1e3b0:	b.ne	1e38c <__gmpz_nextprime@@Base+0x148>  // b.any
   1e3b4:	b	1e348 <__gmpz_nextprime@@Base+0x104>
   1e3b8:	mov	sp, x29
   1e3bc:	ldp	x20, x19, [sp, #80]
   1e3c0:	ldp	x22, x21, [sp, #64]
   1e3c4:	ldp	x24, x23, [sp, #48]
   1e3c8:	ldp	x26, x25, [sp, #32]
   1e3cc:	ldr	x27, [sp, #16]
   1e3d0:	ldp	x29, x30, [sp], #96
   1e3d4:	ret
   1e3d8:	mov	w1, #0x2                   	// #2
   1e3dc:	mov	x0, x19
   1e3e0:	mov	sp, x29
   1e3e4:	ldp	x20, x19, [sp, #80]
   1e3e8:	ldp	x22, x21, [sp, #64]
   1e3ec:	ldp	x24, x23, [sp, #48]
   1e3f0:	ldp	x26, x25, [sp, #32]
   1e3f4:	ldr	x27, [sp, #16]
   1e3f8:	ldp	x29, x30, [sp], #96
   1e3fc:	b	c170 <__gmpz_set_ui@plt>

000000000001e400 <__gmpz_out_raw@@Base>:
   1e400:	stp	x29, x30, [sp, #-80]!
   1e404:	stp	x24, x23, [sp, #32]
   1e408:	stp	x22, x21, [sp, #48]
   1e40c:	stp	x20, x19, [sp, #64]
   1e410:	str	x25, [sp, #16]
   1e414:	ldrsw	x23, [x1, #4]
   1e418:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1e41c:	ldr	x8, [x8, #3840]
   1e420:	mov	x21, x0
   1e424:	cmp	x23, #0x0
   1e428:	cneg	x25, x23, mi  // mi = first
   1e42c:	ldr	x8, [x8]
   1e430:	ubfiz	x24, x25, #3, #58
   1e434:	add	x19, x24, #0x8
   1e438:	mov	x0, x19
   1e43c:	mov	x29, sp
   1e440:	mov	x22, x1
   1e444:	blr	x8
   1e448:	mov	x20, x0
   1e44c:	add	x0, x0, #0x8
   1e450:	cbz	x24, 1e544 <__gmpz_out_raw@@Base+0x144>
   1e454:	ldr	x8, [x22, #8]
   1e458:	cmp	x25, #0x1
   1e45c:	csinc	x10, x25, xzr, gt
   1e460:	cmp	x10, #0x2
   1e464:	add	x9, x0, x24
   1e468:	b.cc	1e550 <__gmpz_out_raw@@Base+0x150>  // b.lo, b.ul, b.last
   1e46c:	cmp	x25, #0x1
   1e470:	csinc	x11, x25, xzr, lt  // lt = tstop
   1e474:	add	x12, x24, x11, lsl #3
   1e478:	sub	x11, x25, x11
   1e47c:	sub	x12, x12, x25, lsl #3
   1e480:	add	x11, x8, x11, lsl #3
   1e484:	add	x12, x20, x12
   1e488:	add	x11, x11, #0x8
   1e48c:	cmp	x12, x11
   1e490:	b.cs	1e49c <__gmpz_out_raw@@Base+0x9c>  // b.hs, b.nlast
   1e494:	cmp	x9, x8
   1e498:	b.hi	1e550 <__gmpz_out_raw@@Base+0x150>  // b.pmore
   1e49c:	and	x11, x10, #0x7ffffffffffffffe
   1e4a0:	lsl	x14, x11, #3
   1e4a4:	sub	x12, x9, #0x10
   1e4a8:	movi	v0.2d, #0xff000000000000
   1e4ac:	movi	v1.2d, #0xff0000000000
   1e4b0:	movi	v2.2d, #0xff00000000
   1e4b4:	movi	v3.2d, #0xff000000
   1e4b8:	movi	v4.2d, #0xff0000
   1e4bc:	sub	x25, x25, x11
   1e4c0:	add	x13, x8, x14
   1e4c4:	sub	x9, x9, x14
   1e4c8:	movi	v5.2d, #0xff00
   1e4cc:	mov	x14, x11
   1e4d0:	ldr	q6, [x8], #16
   1e4d4:	subs	x14, x14, #0x2
   1e4d8:	shl	v16.2d, v6.2d, #40
   1e4dc:	shl	v7.2d, v6.2d, #56
   1e4e0:	and	v16.16b, v16.16b, v0.16b
   1e4e4:	orr	v7.16b, v16.16b, v7.16b
   1e4e8:	shl	v16.2d, v6.2d, #24
   1e4ec:	and	v16.16b, v16.16b, v1.16b
   1e4f0:	orr	v7.16b, v7.16b, v16.16b
   1e4f4:	shl	v16.2d, v6.2d, #8
   1e4f8:	and	v16.16b, v16.16b, v2.16b
   1e4fc:	orr	v7.16b, v7.16b, v16.16b
   1e500:	ushr	v16.2d, v6.2d, #8
   1e504:	and	v16.16b, v16.16b, v3.16b
   1e508:	orr	v7.16b, v7.16b, v16.16b
   1e50c:	ushr	v16.2d, v6.2d, #24
   1e510:	and	v16.16b, v16.16b, v4.16b
   1e514:	orr	v7.16b, v7.16b, v16.16b
   1e518:	ushr	v16.2d, v6.2d, #40
   1e51c:	and	v16.16b, v16.16b, v5.16b
   1e520:	orr	v7.16b, v7.16b, v16.16b
   1e524:	usra	v7.2d, v6.2d, #56
   1e528:	ext	v7.16b, v7.16b, v7.16b, #8
   1e52c:	str	q7, [x12], #-16
   1e530:	b.ne	1e4d0 <__gmpz_out_raw@@Base+0xd0>  // b.any
   1e534:	cmp	x10, x11
   1e538:	b.ne	1e54c <__gmpz_out_raw@@Base+0x14c>  // b.any
   1e53c:	mov	x11, v6.d[1]
   1e540:	b	1e5a8 <__gmpz_out_raw@@Base+0x1a8>
   1e544:	mov	x8, xzr
   1e548:	b	1e5b8 <__gmpz_out_raw@@Base+0x1b8>
   1e54c:	mov	x8, x13
   1e550:	add	x10, x25, #0x1
   1e554:	ldr	x11, [x8], #8
   1e558:	sub	x10, x10, #0x1
   1e55c:	cmp	x10, #0x1
   1e560:	lsl	x14, x11, #40
   1e564:	and	x14, x14, #0xff000000000000
   1e568:	lsr	x13, x11, #16
   1e56c:	bfi	x14, x11, #56, #8
   1e570:	lsr	x12, x11, #24
   1e574:	bfi	x14, x13, #40, #8
   1e578:	lsr	x13, x11, #8
   1e57c:	and	x13, x13, #0xff000000
   1e580:	bfi	x14, x12, #32, #8
   1e584:	orr	x13, x14, x13
   1e588:	lsr	x14, x11, #40
   1e58c:	and	x12, x12, #0xff0000
   1e590:	and	x14, x14, #0xff00
   1e594:	orr	x12, x13, x12
   1e598:	orr	x12, x12, x14
   1e59c:	add	x12, x12, x11, lsr #56
   1e5a0:	str	x12, [x9, #-8]!
   1e5a4:	b.gt	1e554 <__gmpz_out_raw@@Base+0x154>
   1e5a8:	clz	x8, x11
   1e5ac:	lsr	x8, x8, #3
   1e5b0:	add	x0, x9, x8
   1e5b4:	sub	x8, x24, x8
   1e5b8:	cmp	w23, #0x0
   1e5bc:	cneg	w9, w8, lt  // lt = tstop
   1e5c0:	rev	w9, w9
   1e5c4:	str	w9, [x0, #-4]!
   1e5c8:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1e5cc:	ldr	x9, [x9, #3856]
   1e5d0:	add	x22, x8, #0x4
   1e5d4:	cmp	x21, #0x0
   1e5d8:	mov	w2, #0x1                   	// #1
   1e5dc:	ldr	x9, [x9]
   1e5e0:	mov	x1, x22
   1e5e4:	csel	x3, x9, x21, eq  // eq = none
   1e5e8:	bl	ce30 <fwrite@plt>
   1e5ec:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1e5f0:	ldr	x8, [x8, #4016]
   1e5f4:	cmp	x0, #0x1
   1e5f8:	mov	x0, x20
   1e5fc:	mov	x1, x19
   1e600:	ldr	x8, [x8]
   1e604:	csel	x21, x22, xzr, eq  // eq = none
   1e608:	blr	x8
   1e60c:	mov	x0, x21
   1e610:	ldp	x20, x19, [sp, #64]
   1e614:	ldp	x22, x21, [sp, #48]
   1e618:	ldp	x24, x23, [sp, #32]
   1e61c:	ldr	x25, [sp, #16]
   1e620:	ldp	x29, x30, [sp], #80
   1e624:	ret

000000000001e628 <__gmpz_out_str@@Base>:
   1e628:	stp	x29, x30, [sp, #-80]!
   1e62c:	str	x25, [sp, #16]
   1e630:	stp	x24, x23, [sp, #32]
   1e634:	stp	x22, x21, [sp, #48]
   1e638:	stp	x20, x19, [sp, #64]
   1e63c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1e640:	ldr	x8, [x8, #3856]
   1e644:	ldrsw	x21, [x2, #4]
   1e648:	cmp	x0, #0x0
   1e64c:	mov	x23, x2
   1e650:	ldr	x8, [x8]
   1e654:	mov	w20, w1
   1e658:	mov	x29, sp
   1e65c:	csel	x19, x8, x0, eq  // eq = none
   1e660:	cmp	w1, #0x2
   1e664:	b.lt	1e694 <__gmpz_out_str@@Base+0x6c>  // b.tstop
   1e668:	cmp	w20, #0x25
   1e66c:	b.ge	1e6a4 <__gmpz_out_str@@Base+0x7c>  // b.tcont
   1e670:	adrp	x24, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1e674:	add	x24, x24, #0xbd
   1e678:	tbz	w21, #31, 1e6c8 <__gmpz_out_str@@Base+0xa0>
   1e67c:	mov	w0, #0x2d                  	// #45
   1e680:	mov	x1, x19
   1e684:	bl	c210 <fputc@plt>
   1e688:	neg	x21, x21
   1e68c:	mov	w25, #0x1                   	// #1
   1e690:	b	1e6cc <__gmpz_out_str@@Base+0xa4>
   1e694:	cmn	w20, #0x2
   1e698:	b.le	1e6b0 <__gmpz_out_str@@Base+0x88>
   1e69c:	mov	w20, #0xa                   	// #10
   1e6a0:	b	1e6bc <__gmpz_out_str@@Base+0x94>
   1e6a4:	cmp	w20, #0x3e
   1e6a8:	b.le	1e6bc <__gmpz_out_str@@Base+0x94>
   1e6ac:	b	1e83c <__gmpz_out_str@@Base+0x214>
   1e6b0:	cmn	w20, #0x24
   1e6b4:	b.lt	1e83c <__gmpz_out_str@@Base+0x214>  // b.tstop
   1e6b8:	neg	w20, w20
   1e6bc:	adrp	x24, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   1e6c0:	add	x24, x24, #0x7e
   1e6c4:	tbnz	w21, #31, 1e67c <__gmpz_out_str@@Base+0x54>
   1e6c8:	mov	x25, xzr
   1e6cc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1e6d0:	ldr	x8, [x8, #3936]
   1e6d4:	mov	w9, #0x28                  	// #40
   1e6d8:	str	xzr, [x29, #24]
   1e6dc:	umaddl	x8, w20, w9, x8
   1e6e0:	ldr	x8, [x8, #8]
   1e6e4:	lsl	x9, x21, #6
   1e6e8:	umulh	x8, x8, x9
   1e6ec:	add	x1, x8, #0x3
   1e6f0:	mov	w8, #0x7f00                	// #32512
   1e6f4:	cmp	x1, x8
   1e6f8:	b.hi	1e820 <__gmpz_out_str@@Base+0x1f8>  // b.pmore
   1e6fc:	add	x9, x1, #0xf
   1e700:	mov	x8, sp
   1e704:	and	x9, x9, #0xfffffffffffffff0
   1e708:	sub	x22, x8, x9
   1e70c:	mov	sp, x22
   1e710:	ldr	x2, [x23, #8]
   1e714:	sub	w8, w20, #0x1
   1e718:	tst	w20, w8
   1e71c:	b.eq	1e75c <__gmpz_out_str@@Base+0x134>  // b.none
   1e720:	lsl	x8, x21, #3
   1e724:	orr	x1, x8, #0x8
   1e728:	mov	w8, #0x7f00                	// #32512
   1e72c:	cmp	x1, x8
   1e730:	b.hi	1e844 <__gmpz_out_str@@Base+0x21c>  // b.pmore
   1e734:	add	x9, x1, #0xf
   1e738:	mov	x8, sp
   1e73c:	and	x9, x9, #0xfffffffffffffff0
   1e740:	sub	x23, x8, x9
   1e744:	mov	sp, x23
   1e748:	mov	x0, x23
   1e74c:	mov	x1, x2
   1e750:	mov	x2, x21
   1e754:	bl	ca50 <__gmpn_copyi@plt>
   1e758:	mov	x2, x23
   1e75c:	mov	x0, x22
   1e760:	mov	w1, w20
   1e764:	mov	x3, x21
   1e768:	bl	ca90 <__gmpn_get_str@plt>
   1e76c:	mov	x2, x0
   1e770:	cbz	x0, 1e7d4 <__gmpz_out_str@@Base+0x1ac>
   1e774:	cmp	x2, #0x1
   1e778:	b.ne	1e784 <__gmpz_out_str@@Base+0x15c>  // b.any
   1e77c:	mov	x8, xzr
   1e780:	b	1e7b8 <__gmpz_out_str@@Base+0x190>
   1e784:	and	x8, x2, #0xfffffffffffffffe
   1e788:	add	x9, x22, #0x1
   1e78c:	mov	x10, x8
   1e790:	ldurb	w11, [x9, #-1]
   1e794:	ldrb	w12, [x9]
   1e798:	subs	x10, x10, #0x2
   1e79c:	ldrb	w11, [x24, x11]
   1e7a0:	ldrb	w12, [x24, x12]
   1e7a4:	sturb	w11, [x9, #-1]
   1e7a8:	strb	w12, [x9], #2
   1e7ac:	b.ne	1e790 <__gmpz_out_str@@Base+0x168>  // b.any
   1e7b0:	cmp	x2, x8
   1e7b4:	b.eq	1e7d4 <__gmpz_out_str@@Base+0x1ac>  // b.none
   1e7b8:	sub	x9, x2, x8
   1e7bc:	add	x8, x22, x8
   1e7c0:	ldrb	w10, [x8]
   1e7c4:	subs	x9, x9, #0x1
   1e7c8:	ldrb	w10, [x24, x10]
   1e7cc:	strb	w10, [x8], #1
   1e7d0:	b.ne	1e7c0 <__gmpz_out_str@@Base+0x198>  // b.any
   1e7d4:	mov	w1, #0x1                   	// #1
   1e7d8:	mov	x0, x22
   1e7dc:	mov	x3, x19
   1e7e0:	strb	wzr, [x22, x2]
   1e7e4:	bl	ce30 <fwrite@plt>
   1e7e8:	ldr	x8, [x29, #24]
   1e7ec:	add	x20, x0, x25
   1e7f0:	cbnz	x8, 1e830 <__gmpz_out_str@@Base+0x208>
   1e7f4:	mov	x0, x19
   1e7f8:	bl	d4a0 <ferror@plt>
   1e7fc:	cmp	w0, #0x0
   1e800:	csel	x0, x20, xzr, eq  // eq = none
   1e804:	mov	sp, x29
   1e808:	ldp	x20, x19, [sp, #64]
   1e80c:	ldp	x22, x21, [sp, #48]
   1e810:	ldp	x24, x23, [sp, #32]
   1e814:	ldr	x25, [sp, #16]
   1e818:	ldp	x29, x30, [sp], #80
   1e81c:	ret
   1e820:	add	x0, x29, #0x18
   1e824:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e828:	mov	x22, x0
   1e82c:	b	1e710 <__gmpz_out_str@@Base+0xe8>
   1e830:	mov	x0, x8
   1e834:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1e838:	b	1e7f4 <__gmpz_out_str@@Base+0x1cc>
   1e83c:	mov	x0, xzr
   1e840:	b	1e804 <__gmpz_out_str@@Base+0x1dc>
   1e844:	add	x0, x29, #0x18
   1e848:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1e84c:	ldr	x2, [x23, #8]
   1e850:	mov	x23, x0
   1e854:	b	1e748 <__gmpz_out_str@@Base+0x120>

000000000001e858 <__gmpz_perfect_power_p@@Base>:
   1e858:	ldr	x8, [x0, #8]
   1e85c:	ldrsw	x1, [x0, #4]
   1e860:	mov	x0, x8
   1e864:	b	c240 <__gmpn_perfect_power_p@plt>

000000000001e868 <__gmpz_perfect_square_p@@Base>:
   1e868:	ldr	w1, [x0, #4]
   1e86c:	cmp	w1, #0x1
   1e870:	b.lt	1e87c <__gmpz_perfect_square_p@@Base+0x14>  // b.tstop
   1e874:	ldr	x0, [x0, #8]
   1e878:	b	d0b0 <__gmpn_perfect_square_p@plt>
   1e87c:	mvn	w8, w1
   1e880:	lsr	w0, w8, #31
   1e884:	ret

000000000001e888 <__gmpz_popcount@@Base>:
   1e888:	ldr	w1, [x0, #4]
   1e88c:	cmp	w1, #0x1
   1e890:	b.lt	1e89c <__gmpz_popcount@@Base+0x14>  // b.tstop
   1e894:	ldr	x0, [x0, #8]
   1e898:	b	cd80 <__gmpn_popcount@plt>
   1e89c:	sbfx	x0, x1, #31, #1
   1e8a0:	ret

000000000001e8a4 <__gmpz_pow_ui@@Base>:
   1e8a4:	cmp	x2, #0x2
   1e8a8:	b.eq	1e8c4 <__gmpz_pow_ui@@Base+0x20>  // b.none
   1e8ac:	mov	x3, x2
   1e8b0:	cmp	x2, #0x1
   1e8b4:	b.eq	1e8cc <__gmpz_pow_ui@@Base+0x28>  // b.none
   1e8b8:	cbnz	x3, 1e8d0 <__gmpz_pow_ui@@Base+0x2c>
   1e8bc:	mov	w1, #0x1                   	// #1
   1e8c0:	b	c170 <__gmpz_set_ui@plt>
   1e8c4:	mov	x2, x1
   1e8c8:	b	c4b0 <__gmpz_mul@plt>
   1e8cc:	b	c420 <__gmpz_set@plt>
   1e8d0:	ldr	x8, [x1, #8]
   1e8d4:	ldrsw	x2, [x1, #4]
   1e8d8:	mov	x1, x8
   1e8dc:	b	c340 <__gmpz_n_pow_ui@plt>

000000000001e8e0 <__gmpz_powm@@Base>:
   1e8e0:	stp	x29, x30, [sp, #-96]!
   1e8e4:	stp	x28, x27, [sp, #16]
   1e8e8:	stp	x26, x25, [sp, #32]
   1e8ec:	stp	x24, x23, [sp, #48]
   1e8f0:	stp	x22, x21, [sp, #64]
   1e8f4:	stp	x20, x19, [sp, #80]
   1e8f8:	mov	x29, sp
   1e8fc:	sub	sp, sp, #0x50
   1e900:	ldr	w8, [x3, #4]
   1e904:	cmp	w8, #0x0
   1e908:	cneg	w20, w8, mi  // mi = first
   1e90c:	cbz	w20, 1f320 <__gmpz_powm@@Base+0xa40>
   1e910:	ldr	x26, [x3, #8]
   1e914:	stur	xzr, [x29, #-24]
   1e918:	ldrsw	x22, [x2, #4]
   1e91c:	mov	x19, x3
   1e920:	mov	x21, x2
   1e924:	mov	x25, x1
   1e928:	mov	x23, x0
   1e92c:	cmp	w22, #0x0
   1e930:	b.le	1ef0c <__gmpz_powm@@Base+0x62c>
   1e934:	ldr	w8, [x25, #4]
   1e938:	cmp	w8, #0x0
   1e93c:	cneg	w24, w8, mi  // mi = first
   1e940:	cbz	w24, 1ef5c <__gmpz_powm@@Base+0x67c>
   1e944:	ldr	x9, [x21, #8]
   1e948:	cmp	x22, #0x1
   1e94c:	b.ne	1e95c <__gmpz_powm@@Base+0x7c>  // b.any
   1e950:	ldr	x8, [x9]
   1e954:	cmp	x8, #0x1
   1e958:	b.eq	1efa0 <__gmpz_powm@@Base+0x6c0>  // b.none
   1e95c:	ldr	x8, [x26]
   1e960:	mov	x28, xzr
   1e964:	stur	x9, [x29, #-32]
   1e968:	cbz	x8, 1ef70 <__gmpz_powm@@Base+0x690>
   1e96c:	sub	x27, x20, x28
   1e970:	stur	x19, [x29, #-64]
   1e974:	stur	x22, [x29, #-40]
   1e978:	tbnz	w8, #0, 1e9dc <__gmpz_powm@@Base+0xfc>
   1e97c:	lsl	x1, x27, #3
   1e980:	mov	w9, #0x7f00                	// #32512
   1e984:	cmp	x1, x9
   1e988:	b.hi	1eff8 <__gmpz_powm@@Base+0x718>  // b.pmore
   1e98c:	add	x10, x1, #0xf
   1e990:	mov	x9, sp
   1e994:	and	x10, x10, #0xfffffffffffffff0
   1e998:	sub	x21, x9, x10
   1e99c:	mov	sp, x21
   1e9a0:	rbit	x8, x8
   1e9a4:	clz	x3, x8
   1e9a8:	mov	x0, x21
   1e9ac:	mov	x1, x26
   1e9b0:	mov	x2, x27
   1e9b4:	stur	x3, [x29, #-56]
   1e9b8:	bl	c1a0 <__gmpn_rshift@plt>
   1e9bc:	add	x8, x21, x27, lsl #3
   1e9c0:	ldur	x8, [x8, #-8]
   1e9c4:	add	x28, x28, #0x1
   1e9c8:	mov	x26, x21
   1e9cc:	cmp	x8, #0x0
   1e9d0:	cset	w8, eq  // eq = none
   1e9d4:	sub	x27, x27, x8
   1e9d8:	b	1e9e4 <__gmpz_powm@@Base+0x104>
   1e9dc:	cbz	x28, 1eb2c <__gmpz_powm@@Base+0x24c>
   1e9e0:	stur	xzr, [x29, #-56]
   1e9e4:	cmp	x28, x27
   1e9e8:	csel	x0, x28, x27, gt
   1e9ec:	bl	d1e0 <__gmpn_binvert_itch@plt>
   1e9f0:	lsl	x8, x20, #1
   1e9f4:	cmp	x0, x8
   1e9f8:	add	x9, x8, x20
   1e9fc:	csel	x8, x0, x8, gt
   1ea00:	mov	w22, #0x1                   	// #1
   1ea04:	add	x8, x8, x9
   1ea08:	lsl	x1, x8, #3
   1ea0c:	mov	w8, #0x7f00                	// #32512
   1ea10:	mov	x19, x26
   1ea14:	cmp	x1, x8
   1ea18:	b.hi	1ef90 <__gmpz_powm@@Base+0x6b0>  // b.pmore
   1ea1c:	add	x9, x1, #0xf
   1ea20:	mov	x8, sp
   1ea24:	and	x9, x9, #0xfffffffffffffff0
   1ea28:	sub	x21, x8, x9
   1ea2c:	mov	sp, x21
   1ea30:	ldr	x26, [x25, #8]
   1ea34:	ldp	x4, x3, [x29, #-40]
   1ea38:	stur	x25, [x29, #-48]
   1ea3c:	add	x25, x21, x20, lsl #3
   1ea40:	mov	x0, x21
   1ea44:	mov	x1, x26
   1ea48:	mov	x2, x24
   1ea4c:	mov	x5, x19
   1ea50:	mov	x6, x27
   1ea54:	mov	x7, x25
   1ea58:	bl	d000 <__gmpn_powm@plt>
   1ea5c:	cbz	w22, 1ed48 <__gmpz_powm@@Base+0x468>
   1ea60:	cmp	x28, x24
   1ea64:	stur	x23, [x29, #-72]
   1ea68:	b.le	1eac4 <__gmpz_powm@@Base+0x1e4>
   1ea6c:	lsl	x22, x28, #3
   1ea70:	mov	w8, #0x7f00                	// #32512
   1ea74:	cmp	x22, x8
   1ea78:	b.hi	1f09c <__gmpz_powm@@Base+0x7bc>  // b.pmore
   1ea7c:	add	x9, x22, #0xf
   1ea80:	mov	x8, sp
   1ea84:	and	x9, x9, #0xfffffffffffffff0
   1ea88:	sub	x23, x8, x9
   1ea8c:	mov	sp, x23
   1ea90:	mov	x0, x23
   1ea94:	mov	x1, x26
   1ea98:	mov	x2, x24
   1ea9c:	bl	ca50 <__gmpn_copyi@plt>
   1eaa0:	cmp	x28, x24
   1eaa4:	mov	x26, x23
   1eaa8:	b.eq	1eac4 <__gmpz_powm@@Base+0x1e4>  // b.none
   1eaac:	lsl	x8, x24, #3
   1eab0:	add	x0, x23, x8
   1eab4:	sub	x2, x22, x8
   1eab8:	mov	w1, wzr
   1eabc:	bl	c5f0 <memset@plt>
   1eac0:	mov	x26, x23
   1eac4:	ldr	x8, [x26]
   1eac8:	ldur	x3, [x29, #-40]
   1eacc:	tbnz	w8, #0, 1eb10 <__gmpz_powm@@Base+0x230>
   1ead0:	cmp	x3, #0x2
   1ead4:	b.ge	1eb50 <__gmpz_powm@@Base+0x270>  // b.tcont
   1ead8:	ldur	x10, [x29, #-32]
   1eadc:	ldur	x12, [x29, #-56]
   1eae0:	ubfiz	w8, w8, #1, #3
   1eae4:	mov	w9, #0x1213                	// #4627
   1eae8:	ldr	x10, [x10]
   1eaec:	cmp	w12, #0x0
   1eaf0:	cset	w11, ne  // ne = any
   1eaf4:	lsr	w8, w9, w8
   1eaf8:	sub	x9, x28, x11
   1eafc:	and	w8, w8, #0x3
   1eb00:	add	x9, x12, x9, lsl #6
   1eb04:	mul	x8, x10, x8
   1eb08:	cmp	x8, x9
   1eb0c:	b.cs	1eb50 <__gmpz_powm@@Base+0x270>  // b.hs, b.nlast
   1eb10:	ldur	x2, [x29, #-32]
   1eb14:	add	x5, x25, x28, lsl #3
   1eb18:	mov	x0, x25
   1eb1c:	mov	x1, x26
   1eb20:	mov	x4, x28
   1eb24:	bl	c3c0 <__gmpn_powlo@plt>
   1eb28:	b	1eb60 <__gmpz_powm@@Base+0x280>
   1eb2c:	mov	x0, x27
   1eb30:	bl	d1e0 <__gmpn_binvert_itch@plt>
   1eb34:	lsl	x8, x20, #1
   1eb38:	cmp	x0, x8
   1eb3c:	mov	w22, wzr
   1eb40:	csel	x8, x0, x8, gt
   1eb44:	mov	x9, x20
   1eb48:	stur	xzr, [x29, #-56]
   1eb4c:	b	1ea04 <__gmpz_powm@@Base+0x124>
   1eb50:	add	x0, x21, x20, lsl #3
   1eb54:	lsl	x2, x28, #3
   1eb58:	mov	w1, wzr
   1eb5c:	bl	c5f0 <memset@plt>
   1eb60:	cmp	x28, x27
   1eb64:	mov	x26, x19
   1eb68:	b.le	1ebcc <__gmpz_powm@@Base+0x2ec>
   1eb6c:	ldur	x23, [x29, #-72]
   1eb70:	lsl	x19, x28, #3
   1eb74:	mov	w8, #0x7f00                	// #32512
   1eb78:	cmp	x19, x8
   1eb7c:	b.hi	1f0b0 <__gmpz_powm@@Base+0x7d0>  // b.pmore
   1eb80:	add	x9, x19, #0xf
   1eb84:	mov	x8, sp
   1eb88:	and	x9, x9, #0xfffffffffffffff0
   1eb8c:	sub	x22, x8, x9
   1eb90:	mov	sp, x22
   1eb94:	mov	x0, x22
   1eb98:	mov	x1, x26
   1eb9c:	mov	x2, x27
   1eba0:	bl	ca50 <__gmpn_copyi@plt>
   1eba4:	cmp	x28, x27
   1eba8:	mov	x26, x22
   1ebac:	b.eq	1ebd0 <__gmpz_powm@@Base+0x2f0>  // b.none
   1ebb0:	lsl	x8, x27, #3
   1ebb4:	add	x0, x22, x8
   1ebb8:	sub	x2, x19, x8
   1ebbc:	mov	w1, wzr
   1ebc0:	bl	c5f0 <memset@plt>
   1ebc4:	mov	x26, x22
   1ebc8:	b	1ebd0 <__gmpz_powm@@Base+0x2f0>
   1ebcc:	ldur	x23, [x29, #-72]
   1ebd0:	add	x24, x25, x20, lsl #3
   1ebd4:	add	x19, x25, x20, lsl #4
   1ebd8:	mov	x0, x24
   1ebdc:	mov	x1, x26
   1ebe0:	mov	x2, x28
   1ebe4:	mov	x3, x19
   1ebe8:	bl	cd20 <__gmpn_binvert@plt>
   1ebec:	cmp	x28, x27
   1ebf0:	csel	x22, x28, x27, lt  // lt = tstop
   1ebf4:	cbz	x22, 1ec34 <__gmpz_powm@@Base+0x354>
   1ebf8:	mov	x0, x25
   1ebfc:	mov	x1, x25
   1ec00:	mov	x2, x21
   1ec04:	mov	x3, x22
   1ec08:	bl	c2d0 <__gmpn_sub_n@plt>
   1ec0c:	cbz	x0, 1ec34 <__gmpz_powm@@Base+0x354>
   1ec10:	add	x8, x21, x20, lsl #3
   1ec14:	cmp	x22, x28
   1ec18:	b.ge	1ec34 <__gmpz_powm@@Base+0x354>  // b.tcont
   1ec1c:	lsl	x9, x22, #3
   1ec20:	ldr	x10, [x8, x9]
   1ec24:	add	x22, x22, #0x1
   1ec28:	sub	x11, x10, #0x1
   1ec2c:	str	x11, [x8, x9]
   1ec30:	cbz	x10, 1ec14 <__gmpz_powm@@Base+0x334>
   1ec34:	mov	x0, x19
   1ec38:	mov	x1, x24
   1ec3c:	mov	x2, x25
   1ec40:	mov	x3, x28
   1ec44:	bl	cec0 <__gmpn_mullo_n@plt>
   1ec48:	ldur	x11, [x29, #-56]
   1ec4c:	cbz	w11, 1ec68 <__gmpz_powm@@Base+0x388>
   1ec50:	add	x8, x19, x28, lsl #3
   1ec54:	ldur	x9, [x8, #-8]
   1ec58:	mov	x10, #0xffffffffffffffff    	// #-1
   1ec5c:	lsl	x10, x10, x11
   1ec60:	bic	x9, x9, x10
   1ec64:	stur	x9, [x8, #-8]
   1ec68:	mov	x0, x25
   1ec6c:	cmp	x28, x27
   1ec70:	b.le	1ec90 <__gmpz_powm@@Base+0x3b0>
   1ec74:	mov	x1, x19
   1ec78:	mov	x2, x28
   1ec7c:	mov	x3, x26
   1ec80:	mov	x4, x27
   1ec84:	bl	ccd0 <__gmpn_mul@plt>
   1ec88:	cbnz	x27, 1eca8 <__gmpz_powm@@Base+0x3c8>
   1ec8c:	b	1ece4 <__gmpz_powm@@Base+0x404>
   1ec90:	mov	x1, x26
   1ec94:	mov	x2, x27
   1ec98:	mov	x3, x19
   1ec9c:	mov	x4, x28
   1eca0:	bl	ccd0 <__gmpn_mul@plt>
   1eca4:	cbz	x27, 1ece4 <__gmpz_powm@@Base+0x404>
   1eca8:	mov	x0, x21
   1ecac:	mov	x1, x25
   1ecb0:	mov	x2, x21
   1ecb4:	mov	x3, x27
   1ecb8:	bl	ca70 <__gmpn_add_n@plt>
   1ecbc:	cbz	x0, 1ece4 <__gmpz_powm@@Base+0x404>
   1ecc0:	add	x8, x21, x20, lsl #3
   1ecc4:	cmp	x27, x20
   1ecc8:	b.ge	1ed48 <__gmpz_powm@@Base+0x468>  // b.tcont
   1eccc:	lsl	x9, x27, #3
   1ecd0:	ldr	x10, [x8, x9]
   1ecd4:	add	x27, x27, #0x1
   1ecd8:	adds	x10, x10, #0x1
   1ecdc:	str	x10, [x21, x9]
   1ece0:	b.cs	1ecc4 <__gmpz_powm@@Base+0x3e4>  // b.hs, b.nlast
   1ece4:	cmp	x25, x21
   1ece8:	b.eq	1ed48 <__gmpz_powm@@Base+0x468>  // b.none
   1ecec:	cmp	x27, x20
   1ecf0:	b.ge	1ed48 <__gmpz_powm@@Base+0x468>  // b.tcont
   1ecf4:	sub	x8, x20, x27
   1ecf8:	cmp	x8, #0x4
   1ecfc:	b.cc	1ed2c <__gmpz_powm@@Base+0x44c>  // b.lo, b.ul, b.last
   1ed00:	lsl	x9, x27, #3
   1ed04:	add	x10, x21, x9
   1ed08:	add	x11, x21, x20, lsl #4
   1ed0c:	cmp	x10, x11
   1ed10:	lsl	x10, x20, #3
   1ed14:	b.cs	1eedc <__gmpz_powm@@Base+0x5fc>  // b.hs, b.nlast
   1ed18:	add	x9, x9, x10
   1ed1c:	add	x11, x21, x10
   1ed20:	add	x9, x21, x9
   1ed24:	cmp	x9, x11
   1ed28:	b.cs	1eedc <__gmpz_powm@@Base+0x5fc>  // b.hs, b.nlast
   1ed2c:	mov	x9, x27
   1ed30:	sub	x8, x20, x9
   1ed34:	add	x9, x21, x9, lsl #3
   1ed38:	ldr	x10, [x9, x20, lsl #3]
   1ed3c:	subs	x8, x8, #0x1
   1ed40:	str	x10, [x9], #8
   1ed44:	b.ne	1ed38 <__gmpz_powm@@Base+0x458>  // b.any
   1ed48:	ldur	x11, [x29, #-48]
   1ed4c:	ldur	x12, [x29, #-32]
   1ed50:	sub	x22, x21, #0x8
   1ed54:	mov	x8, x20
   1ed58:	subs	x9, x8, #0x1
   1ed5c:	b.lt	1ed7c <__gmpz_powm@@Base+0x49c>  // b.tstop
   1ed60:	ldr	x10, [x22, x8, lsl #3]
   1ed64:	mov	x8, x9
   1ed68:	cbz	x10, 1ed58 <__gmpz_powm@@Base+0x478>
   1ed6c:	add	x24, x9, #0x1
   1ed70:	ldrb	w8, [x12]
   1ed74:	tbnz	w8, #0, 1ed88 <__gmpz_powm@@Base+0x4a8>
   1ed78:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1ed7c:	mov	x24, xzr
   1ed80:	ldrb	w8, [x12]
   1ed84:	tbz	w8, #0, 1ee94 <__gmpz_powm@@Base+0x5b4>
   1ed88:	cbz	x24, 1ee94 <__gmpz_powm@@Base+0x5b4>
   1ed8c:	ldr	w8, [x11, #4]
   1ed90:	tbz	w8, #31, 1ee94 <__gmpz_powm@@Base+0x5b4>
   1ed94:	ldur	x8, [x29, #-64]
   1ed98:	mov	x0, x21
   1ed9c:	mov	x2, x21
   1eda0:	mov	x3, x24
   1eda4:	ldr	x19, [x8, #8]
   1eda8:	mov	x1, x19
   1edac:	bl	c2d0 <__gmpn_sub_n@plt>
   1edb0:	cbz	x0, 1edd4 <__gmpz_powm@@Base+0x4f4>
   1edb4:	cmp	x24, x20
   1edb8:	b.ge	1ee74 <__gmpz_powm@@Base+0x594>  // b.tcont
   1edbc:	lsl	x8, x24, #3
   1edc0:	ldr	x9, [x19, x8]
   1edc4:	add	x24, x24, #0x1
   1edc8:	sub	x10, x9, #0x1
   1edcc:	str	x10, [x21, x8]
   1edd0:	cbz	x9, 1edb4 <__gmpz_powm@@Base+0x4d4>
   1edd4:	cmp	x19, x21
   1edd8:	b.eq	1ee74 <__gmpz_powm@@Base+0x594>  // b.none
   1eddc:	cmp	x24, x20
   1ede0:	b.ge	1ee74 <__gmpz_powm@@Base+0x594>  // b.tcont
   1ede4:	sub	x8, x20, x24
   1ede8:	cmp	x8, #0x4
   1edec:	b.cc	1ee54 <__gmpz_powm@@Base+0x574>  // b.lo, b.ul, b.last
   1edf0:	lsl	x10, x24, #3
   1edf4:	lsl	x9, x20, #3
   1edf8:	add	x11, x21, x10
   1edfc:	add	x12, x19, x9
   1ee00:	cmp	x11, x12
   1ee04:	b.cs	1ee18 <__gmpz_powm@@Base+0x538>  // b.hs, b.nlast
   1ee08:	add	x9, x21, x9
   1ee0c:	add	x11, x19, x10
   1ee10:	cmp	x9, x11
   1ee14:	b.hi	1ee54 <__gmpz_powm@@Base+0x574>  // b.pmore
   1ee18:	and	x9, x8, #0xfffffffffffffffc
   1ee1c:	add	x11, x10, x19
   1ee20:	add	x12, x10, x21
   1ee24:	add	x24, x24, x9
   1ee28:	add	x10, x11, #0x10
   1ee2c:	add	x11, x12, #0x10
   1ee30:	mov	x12, x9
   1ee34:	ldp	q0, q1, [x10, #-16]
   1ee38:	add	x10, x10, #0x20
   1ee3c:	subs	x12, x12, #0x4
   1ee40:	stp	q0, q1, [x11, #-16]
   1ee44:	add	x11, x11, #0x20
   1ee48:	b.ne	1ee34 <__gmpz_powm@@Base+0x554>  // b.any
   1ee4c:	cmp	x8, x9
   1ee50:	b.eq	1ee74 <__gmpz_powm@@Base+0x594>  // b.none
   1ee54:	lsl	x10, x24, #3
   1ee58:	sub	x8, x20, x24
   1ee5c:	add	x9, x21, x10
   1ee60:	add	x10, x19, x10
   1ee64:	ldr	x11, [x10], #8
   1ee68:	subs	x8, x8, #0x1
   1ee6c:	str	x11, [x9], #8
   1ee70:	b.ne	1ee64 <__gmpz_powm@@Base+0x584>  // b.any
   1ee74:	subs	x8, x20, #0x1
   1ee78:	b.lt	1ee90 <__gmpz_powm@@Base+0x5b0>  // b.tstop
   1ee7c:	ldr	x9, [x22, x20, lsl #3]
   1ee80:	mov	x20, x8
   1ee84:	cbz	x9, 1ee74 <__gmpz_powm@@Base+0x594>
   1ee88:	add	x24, x8, #0x1
   1ee8c:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1ee90:	mov	x24, xzr
   1ee94:	ldrsw	x8, [x23]
   1ee98:	cmp	x24, x8
   1ee9c:	b.gt	1ef80 <__gmpz_powm@@Base+0x6a0>
   1eea0:	ldr	x0, [x23, #8]
   1eea4:	mov	x1, x21
   1eea8:	mov	x2, x24
   1eeac:	str	w24, [x23, #4]
   1eeb0:	bl	ca50 <__gmpn_copyi@plt>
   1eeb4:	ldur	x0, [x29, #-24]
   1eeb8:	cbnz	x0, 1ef68 <__gmpz_powm@@Base+0x688>
   1eebc:	mov	sp, x29
   1eec0:	ldp	x20, x19, [sp, #80]
   1eec4:	ldp	x22, x21, [sp, #64]
   1eec8:	ldp	x24, x23, [sp, #48]
   1eecc:	ldp	x26, x25, [sp, #32]
   1eed0:	ldp	x28, x27, [sp, #16]
   1eed4:	ldp	x29, x30, [sp], #96
   1eed8:	ret
   1eedc:	and	x11, x8, #0xfffffffffffffffc
   1eee0:	add	x9, x27, x11
   1eee4:	add	x12, x21, x27, lsl #3
   1eee8:	mov	x13, x11
   1eeec:	add	x14, x12, x10
   1eef0:	ldp	q0, q1, [x14]
   1eef4:	subs	x13, x13, #0x4
   1eef8:	stp	q0, q1, [x12], #32
   1eefc:	b.ne	1eeec <__gmpz_powm@@Base+0x60c>  // b.any
   1ef00:	cmp	x8, x11
   1ef04:	b.eq	1ed48 <__gmpz_powm@@Base+0x468>  // b.none
   1ef08:	b	1ed30 <__gmpz_powm@@Base+0x450>
   1ef0c:	cbz	w22, 1f00c <__gmpz_powm@@Base+0x72c>
   1ef10:	add	x8, x20, #0x1
   1ef14:	cmp	w20, #0xfdf
   1ef18:	lsl	x1, x8, #3
   1ef1c:	stur	w8, [x29, #-16]
   1ef20:	b.hi	1f2e4 <__gmpz_powm@@Base+0xa04>  // b.pmore
   1ef24:	add	x9, x1, #0xf
   1ef28:	mov	x8, sp
   1ef2c:	and	x9, x9, #0x1ffffffff0
   1ef30:	sub	x0, x8, x9
   1ef34:	mov	sp, x0
   1ef38:	stur	x0, [x29, #-8]
   1ef3c:	sub	x0, x29, #0x10
   1ef40:	mov	x1, x25
   1ef44:	mov	x2, x19
   1ef48:	bl	cbd0 <__gmpz_invert@plt>
   1ef4c:	cbz	w0, 1f320 <__gmpz_powm@@Base+0xa40>
   1ef50:	sub	x25, x29, #0x10
   1ef54:	neg	x22, x22
   1ef58:	b	1e934 <__gmpz_powm@@Base+0x54>
   1ef5c:	str	wzr, [x23, #4]
   1ef60:	ldur	x0, [x29, #-24]
   1ef64:	cbz	x0, 1eebc <__gmpz_powm@@Base+0x5dc>
   1ef68:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1ef6c:	b	1eebc <__gmpz_powm@@Base+0x5dc>
   1ef70:	ldr	x8, [x26, #8]!
   1ef74:	add	x28, x28, #0x1
   1ef78:	cbnz	x8, 1e96c <__gmpz_powm@@Base+0x8c>
   1ef7c:	b	1ef70 <__gmpz_powm@@Base+0x690>
   1ef80:	mov	x0, x23
   1ef84:	mov	x1, x24
   1ef88:	bl	c080 <__gmpz_realloc@plt>
   1ef8c:	b	1eea0 <__gmpz_powm@@Base+0x5c0>
   1ef90:	sub	x0, x29, #0x18
   1ef94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1ef98:	mov	x21, x0
   1ef9c:	b	1ea30 <__gmpz_powm@@Base+0x150>
   1efa0:	mov	x27, x25
   1efa4:	cmp	w20, #0xfe0
   1efa8:	lsl	x19, x20, #3
   1efac:	mov	x25, x23
   1efb0:	b.hi	1f2f0 <__gmpz_powm@@Base+0xa10>  // b.pmore
   1efb4:	add	x9, x19, #0xf
   1efb8:	mov	x8, sp
   1efbc:	and	x9, x9, #0xffffffff0
   1efc0:	sub	x21, x8, x9
   1efc4:	mov	sp, x21
   1efc8:	ldr	x23, [x27, #8]
   1efcc:	cmp	w24, w20
   1efd0:	b.cs	1f024 <__gmpz_powm@@Base+0x744>  // b.hs, b.nlast
   1efd4:	mov	x8, x27
   1efd8:	ldr	w8, [x27, #4]
   1efdc:	tbnz	w8, #31, 1f0f4 <__gmpz_powm@@Base+0x814>
   1efe0:	mov	x0, x21
   1efe4:	mov	x1, x23
   1efe8:	mov	x2, x24
   1efec:	bl	ca50 <__gmpn_copyi@plt>
   1eff0:	mov	x23, x25
   1eff4:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1eff8:	sub	x0, x29, #0x18
   1effc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f000:	ldr	x8, [x26]
   1f004:	mov	x21, x0
   1f008:	b	1e9a0 <__gmpz_powm@@Base+0xc0>
   1f00c:	cmp	w20, #0x1
   1f010:	b.ne	1f0c4 <__gmpz_powm@@Base+0x7e4>  // b.any
   1f014:	ldr	x8, [x26]
   1f018:	cmp	x8, #0x1
   1f01c:	cset	w8, ne  // ne = any
   1f020:	b	1f0c8 <__gmpz_powm@@Base+0x7e8>
   1f024:	sub	x8, x24, x20
   1f028:	lsl	x8, x8, #3
   1f02c:	add	x1, x8, #0x8
   1f030:	mov	w8, #0x7f00                	// #32512
   1f034:	cmp	x1, x8
   1f038:	b.hi	1f314 <__gmpz_powm@@Base+0xa34>  // b.pmore
   1f03c:	add	x9, x1, #0xf
   1f040:	mov	x8, sp
   1f044:	and	x9, x9, #0xfffffffffffffff0
   1f048:	sub	x0, x8, x9
   1f04c:	mov	sp, x0
   1f050:	mov	x1, x21
   1f054:	mov	x2, xzr
   1f058:	mov	x3, x23
   1f05c:	mov	x4, x24
   1f060:	mov	x5, x26
   1f064:	mov	x6, x20
   1f068:	bl	bf00 <__gmpn_tdiv_qr@plt>
   1f06c:	sub	x22, x21, #0x8
   1f070:	mov	x9, x20
   1f074:	subs	x8, x9, #0x1
   1f078:	b.lt	1f0e8 <__gmpz_powm@@Base+0x808>  // b.tstop
   1f07c:	ldr	x10, [x22, x9, lsl #3]
   1f080:	mov	x9, x8
   1f084:	cbz	x10, 1f074 <__gmpz_powm@@Base+0x794>
   1f088:	ldr	w9, [x27, #4]
   1f08c:	add	x24, x8, #0x1
   1f090:	tbnz	w9, #31, 1f1f0 <__gmpz_powm@@Base+0x910>
   1f094:	mov	x23, x25
   1f098:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1f09c:	sub	x0, x29, #0x18
   1f0a0:	mov	x1, x22
   1f0a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f0a8:	mov	x23, x0
   1f0ac:	b	1ea90 <__gmpz_powm@@Base+0x1b0>
   1f0b0:	sub	x0, x29, #0x18
   1f0b4:	mov	x1, x19
   1f0b8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f0bc:	mov	x22, x0
   1f0c0:	b	1eb94 <__gmpz_powm@@Base+0x2b4>
   1f0c4:	mov	w8, #0x1                   	// #1
   1f0c8:	ldr	w9, [x23]
   1f0cc:	str	w8, [x23, #4]
   1f0d0:	cmp	w9, #0x0
   1f0d4:	b.le	1f304 <__gmpz_powm@@Base+0xa24>
   1f0d8:	ldr	x0, [x23, #8]
   1f0dc:	mov	w8, #0x1                   	// #1
   1f0e0:	str	x8, [x0]
   1f0e4:	b	1eebc <__gmpz_powm@@Base+0x5dc>
   1f0e8:	mov	x24, xzr
   1f0ec:	mov	x23, x25
   1f0f0:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1f0f4:	mov	x0, x21
   1f0f8:	mov	x1, x26
   1f0fc:	mov	x2, x23
   1f100:	mov	x3, x24
   1f104:	mov	x22, x26
   1f108:	bl	c2d0 <__gmpn_sub_n@plt>
   1f10c:	mov	x11, x26
   1f110:	cbz	x0, 1f134 <__gmpz_powm@@Base+0x854>
   1f114:	cmp	x24, x20
   1f118:	b.cs	1f1d0 <__gmpz_powm@@Base+0x8f0>  // b.hs, b.nlast
   1f11c:	lsl	x8, x24, #3
   1f120:	ldr	x9, [x11, x8]
   1f124:	add	x24, x24, #0x1
   1f128:	sub	x10, x9, #0x1
   1f12c:	str	x10, [x21, x8]
   1f130:	cbz	x9, 1f114 <__gmpz_powm@@Base+0x834>
   1f134:	cmp	x11, x21
   1f138:	b.eq	1f1d0 <__gmpz_powm@@Base+0x8f0>  // b.none
   1f13c:	cmp	x24, x20
   1f140:	b.ge	1f1d0 <__gmpz_powm@@Base+0x8f0>  // b.tcont
   1f144:	sub	x8, x20, x24
   1f148:	cmp	x8, #0x4
   1f14c:	b.cc	1f1b0 <__gmpz_powm@@Base+0x8d0>  // b.lo, b.ul, b.last
   1f150:	lsl	x10, x24, #3
   1f154:	add	x9, x21, x10
   1f158:	add	x11, x22, x19
   1f15c:	cmp	x9, x11
   1f160:	b.cs	1f174 <__gmpz_powm@@Base+0x894>  // b.hs, b.nlast
   1f164:	add	x9, x21, x19
   1f168:	add	x11, x22, x10
   1f16c:	cmp	x9, x11
   1f170:	b.hi	1f1b0 <__gmpz_powm@@Base+0x8d0>  // b.pmore
   1f174:	and	x9, x8, #0xfffffffffffffffc
   1f178:	add	x11, x10, x22
   1f17c:	add	x12, x10, x21
   1f180:	add	x24, x24, x9
   1f184:	add	x10, x11, #0x10
   1f188:	add	x11, x12, #0x10
   1f18c:	mov	x12, x9
   1f190:	ldp	q0, q1, [x10, #-16]
   1f194:	add	x10, x10, #0x20
   1f198:	subs	x12, x12, #0x4
   1f19c:	stp	q0, q1, [x11, #-16]
   1f1a0:	add	x11, x11, #0x20
   1f1a4:	b.ne	1f190 <__gmpz_powm@@Base+0x8b0>  // b.any
   1f1a8:	cmp	x8, x9
   1f1ac:	b.eq	1f1d0 <__gmpz_powm@@Base+0x8f0>  // b.none
   1f1b0:	lsl	x10, x24, #3
   1f1b4:	sub	x8, x20, x24
   1f1b8:	add	x9, x21, x10
   1f1bc:	add	x10, x22, x10
   1f1c0:	ldr	x11, [x10], #8
   1f1c4:	subs	x8, x8, #0x1
   1f1c8:	str	x11, [x9], #8
   1f1cc:	b.ne	1f1c0 <__gmpz_powm@@Base+0x8e0>  // b.any
   1f1d0:	sub	x8, x21, #0x8
   1f1d4:	ldr	x10, [x8, x20, lsl #3]
   1f1d8:	sub	x9, x20, #0x1
   1f1dc:	mov	x20, x9
   1f1e0:	cbz	x10, 1f1d4 <__gmpz_powm@@Base+0x8f4>
   1f1e4:	add	x24, x9, #0x1
   1f1e8:	mov	x23, x25
   1f1ec:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1f1f0:	mov	x0, x21
   1f1f4:	mov	x1, x26
   1f1f8:	mov	x2, x21
   1f1fc:	mov	x3, x24
   1f200:	bl	c2d0 <__gmpn_sub_n@plt>
   1f204:	mov	x11, x26
   1f208:	cbz	x0, 1f22c <__gmpz_powm@@Base+0x94c>
   1f20c:	cmp	x24, x20
   1f210:	b.ge	1f2c8 <__gmpz_powm@@Base+0x9e8>  // b.tcont
   1f214:	lsl	x8, x24, #3
   1f218:	ldr	x9, [x11, x8]
   1f21c:	add	x24, x24, #0x1
   1f220:	sub	x10, x9, #0x1
   1f224:	str	x10, [x21, x8]
   1f228:	cbz	x9, 1f20c <__gmpz_powm@@Base+0x92c>
   1f22c:	cmp	x11, x21
   1f230:	b.eq	1f2c8 <__gmpz_powm@@Base+0x9e8>  // b.none
   1f234:	cmp	x24, x20
   1f238:	b.ge	1f2c8 <__gmpz_powm@@Base+0x9e8>  // b.tcont
   1f23c:	sub	x8, x20, x24
   1f240:	cmp	x8, #0x4
   1f244:	b.cc	1f2a8 <__gmpz_powm@@Base+0x9c8>  // b.lo, b.ul, b.last
   1f248:	lsl	x10, x24, #3
   1f24c:	add	x9, x21, x10
   1f250:	add	x11, x26, x19
   1f254:	cmp	x9, x11
   1f258:	b.cs	1f26c <__gmpz_powm@@Base+0x98c>  // b.hs, b.nlast
   1f25c:	add	x9, x21, x19
   1f260:	add	x11, x26, x10
   1f264:	cmp	x9, x11
   1f268:	b.hi	1f2a8 <__gmpz_powm@@Base+0x9c8>  // b.pmore
   1f26c:	and	x9, x8, #0xfffffffffffffffc
   1f270:	add	x11, x10, x26
   1f274:	add	x12, x10, x21
   1f278:	add	x24, x24, x9
   1f27c:	add	x10, x11, #0x10
   1f280:	add	x11, x12, #0x10
   1f284:	mov	x12, x9
   1f288:	ldp	q0, q1, [x10, #-16]
   1f28c:	add	x10, x10, #0x20
   1f290:	subs	x12, x12, #0x4
   1f294:	stp	q0, q1, [x11, #-16]
   1f298:	add	x11, x11, #0x20
   1f29c:	b.ne	1f288 <__gmpz_powm@@Base+0x9a8>  // b.any
   1f2a0:	cmp	x8, x9
   1f2a4:	b.eq	1f2c8 <__gmpz_powm@@Base+0x9e8>  // b.none
   1f2a8:	lsl	x10, x24, #3
   1f2ac:	sub	x8, x20, x24
   1f2b0:	add	x9, x21, x10
   1f2b4:	add	x10, x26, x10
   1f2b8:	ldr	x11, [x10], #8
   1f2bc:	subs	x8, x8, #0x1
   1f2c0:	str	x11, [x9], #8
   1f2c4:	b.ne	1f2b8 <__gmpz_powm@@Base+0x9d8>  // b.any
   1f2c8:	ldr	x9, [x22, x20, lsl #3]
   1f2cc:	sub	x8, x20, #0x1
   1f2d0:	mov	x20, x8
   1f2d4:	cbz	x9, 1f2c8 <__gmpz_powm@@Base+0x9e8>
   1f2d8:	add	x24, x8, #0x1
   1f2dc:	mov	x23, x25
   1f2e0:	b	1ee94 <__gmpz_powm@@Base+0x5b4>
   1f2e4:	sub	x0, x29, #0x18
   1f2e8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f2ec:	b	1ef38 <__gmpz_powm@@Base+0x658>
   1f2f0:	sub	x0, x29, #0x18
   1f2f4:	mov	x1, x19
   1f2f8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f2fc:	mov	x21, x0
   1f300:	b	1efc8 <__gmpz_powm@@Base+0x6e8>
   1f304:	mov	w1, #0x1                   	// #1
   1f308:	mov	x0, x23
   1f30c:	bl	c080 <__gmpz_realloc@plt>
   1f310:	b	1f0dc <__gmpz_powm@@Base+0x7fc>
   1f314:	sub	x0, x29, #0x18
   1f318:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f31c:	b	1f050 <__gmpz_powm@@Base+0x770>
   1f320:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000001f324 <__gmpz_powm_sec@@Base>:
   1f324:	stp	x29, x30, [sp, #-96]!
   1f328:	stp	x28, x27, [sp, #16]
   1f32c:	stp	x26, x25, [sp, #32]
   1f330:	stp	x24, x23, [sp, #48]
   1f334:	stp	x22, x21, [sp, #64]
   1f338:	stp	x20, x19, [sp, #80]
   1f33c:	mov	x29, sp
   1f340:	sub	sp, sp, #0x10
   1f344:	ldr	w8, [x3, #4]
   1f348:	cmp	w8, #0x0
   1f34c:	cneg	w20, w8, mi  // mi = first
   1f350:	cbz	w20, 1f5f4 <__gmpz_powm_sec@@Base+0x2d0>
   1f354:	ldr	x25, [x3, #8]
   1f358:	mov	x23, x3
   1f35c:	ldr	x8, [x25]
   1f360:	tbz	w8, #0, 1f5f4 <__gmpz_powm_sec@@Base+0x2d0>
   1f364:	ldrsw	x9, [x2, #4]
   1f368:	mov	x22, x2
   1f36c:	mov	x19, x0
   1f370:	cmp	w9, #0x0
   1f374:	b.le	1f57c <__gmpz_powm_sec@@Base+0x258>
   1f378:	ldr	w8, [x1, #4]
   1f37c:	mov	x24, x1
   1f380:	cmp	w8, #0x0
   1f384:	cneg	w26, w8, mi  // mi = first
   1f388:	cbz	w26, 1f5b4 <__gmpz_powm_sec@@Base+0x290>
   1f38c:	lsl	x27, x9, #6
   1f390:	mov	x0, x26
   1f394:	mov	x1, x27
   1f398:	mov	x2, x20
   1f39c:	stur	xzr, [x29, #-8]
   1f3a0:	bl	c230 <__gmpn_sec_powm_itch@plt>
   1f3a4:	add	x8, x0, x20
   1f3a8:	lsl	x1, x8, #3
   1f3ac:	mov	w8, #0x7f00                	// #32512
   1f3b0:	cmp	x1, x8
   1f3b4:	b.hi	1f5bc <__gmpz_powm_sec@@Base+0x298>  // b.pmore
   1f3b8:	add	x9, x1, #0xf
   1f3bc:	mov	x8, sp
   1f3c0:	and	x9, x9, #0xfffffffffffffff0
   1f3c4:	sub	x21, x8, x9
   1f3c8:	mov	sp, x21
   1f3cc:	ldr	x28, [x22, #8]
   1f3d0:	ldr	x1, [x24, #8]
   1f3d4:	add	x7, x21, x20, lsl #3
   1f3d8:	mov	x0, x21
   1f3dc:	mov	x2, x26
   1f3e0:	mov	x3, x28
   1f3e4:	mov	x4, x27
   1f3e8:	mov	x5, x25
   1f3ec:	mov	x6, x20
   1f3f0:	bl	cc60 <__gmpn_sec_powm@plt>
   1f3f4:	sub	x25, x21, #0x8
   1f3f8:	mov	x8, x20
   1f3fc:	subs	x9, x8, #0x1
   1f400:	b.lt	1f420 <__gmpz_powm_sec@@Base+0xfc>  // b.tstop
   1f404:	ldr	x10, [x25, x8, lsl #3]
   1f408:	mov	x8, x9
   1f40c:	cbz	x10, 1f3fc <__gmpz_powm_sec@@Base+0xd8>
   1f410:	add	x22, x9, #0x1
   1f414:	ldrb	w8, [x28]
   1f418:	tbnz	w8, #0, 1f42c <__gmpz_powm_sec@@Base+0x108>
   1f41c:	b	1f534 <__gmpz_powm_sec@@Base+0x210>
   1f420:	mov	x22, xzr
   1f424:	ldrb	w8, [x28]
   1f428:	tbz	w8, #0, 1f534 <__gmpz_powm_sec@@Base+0x210>
   1f42c:	cbz	x22, 1f534 <__gmpz_powm_sec@@Base+0x210>
   1f430:	ldr	w8, [x24, #4]
   1f434:	tbz	w8, #31, 1f534 <__gmpz_powm_sec@@Base+0x210>
   1f438:	ldr	x23, [x23, #8]
   1f43c:	mov	x0, x21
   1f440:	mov	x2, x21
   1f444:	mov	x3, x22
   1f448:	mov	x1, x23
   1f44c:	bl	c2d0 <__gmpn_sub_n@plt>
   1f450:	cbz	x0, 1f474 <__gmpz_powm_sec@@Base+0x150>
   1f454:	cmp	x22, x20
   1f458:	b.ge	1f514 <__gmpz_powm_sec@@Base+0x1f0>  // b.tcont
   1f45c:	lsl	x8, x22, #3
   1f460:	ldr	x9, [x23, x8]
   1f464:	add	x22, x22, #0x1
   1f468:	sub	x10, x9, #0x1
   1f46c:	str	x10, [x21, x8]
   1f470:	cbz	x9, 1f454 <__gmpz_powm_sec@@Base+0x130>
   1f474:	cmp	x23, x21
   1f478:	b.eq	1f514 <__gmpz_powm_sec@@Base+0x1f0>  // b.none
   1f47c:	cmp	x22, x20
   1f480:	b.ge	1f514 <__gmpz_powm_sec@@Base+0x1f0>  // b.tcont
   1f484:	sub	x8, x20, x22
   1f488:	cmp	x8, #0x4
   1f48c:	b.cc	1f4f4 <__gmpz_powm_sec@@Base+0x1d0>  // b.lo, b.ul, b.last
   1f490:	lsl	x10, x22, #3
   1f494:	lsl	x9, x20, #3
   1f498:	add	x11, x21, x10
   1f49c:	add	x12, x23, x9
   1f4a0:	cmp	x11, x12
   1f4a4:	b.cs	1f4b8 <__gmpz_powm_sec@@Base+0x194>  // b.hs, b.nlast
   1f4a8:	add	x9, x21, x9
   1f4ac:	add	x11, x23, x10
   1f4b0:	cmp	x9, x11
   1f4b4:	b.hi	1f4f4 <__gmpz_powm_sec@@Base+0x1d0>  // b.pmore
   1f4b8:	and	x9, x8, #0xfffffffffffffffc
   1f4bc:	add	x11, x10, x23
   1f4c0:	add	x12, x10, x21
   1f4c4:	add	x22, x22, x9
   1f4c8:	add	x10, x11, #0x10
   1f4cc:	add	x11, x12, #0x10
   1f4d0:	mov	x12, x9
   1f4d4:	ldp	q0, q1, [x10, #-16]
   1f4d8:	add	x10, x10, #0x20
   1f4dc:	subs	x12, x12, #0x4
   1f4e0:	stp	q0, q1, [x11, #-16]
   1f4e4:	add	x11, x11, #0x20
   1f4e8:	b.ne	1f4d4 <__gmpz_powm_sec@@Base+0x1b0>  // b.any
   1f4ec:	cmp	x8, x9
   1f4f0:	b.eq	1f514 <__gmpz_powm_sec@@Base+0x1f0>  // b.none
   1f4f4:	lsl	x10, x22, #3
   1f4f8:	sub	x8, x20, x22
   1f4fc:	add	x9, x21, x10
   1f500:	add	x10, x23, x10
   1f504:	ldr	x11, [x10], #8
   1f508:	subs	x8, x8, #0x1
   1f50c:	str	x11, [x9], #8
   1f510:	b.ne	1f504 <__gmpz_powm_sec@@Base+0x1e0>  // b.any
   1f514:	subs	x8, x20, #0x1
   1f518:	b.lt	1f530 <__gmpz_powm_sec@@Base+0x20c>  // b.tstop
   1f51c:	ldr	x9, [x25, x20, lsl #3]
   1f520:	mov	x20, x8
   1f524:	cbz	x9, 1f514 <__gmpz_powm_sec@@Base+0x1f0>
   1f528:	add	x22, x8, #0x1
   1f52c:	b	1f534 <__gmpz_powm_sec@@Base+0x210>
   1f530:	mov	x22, xzr
   1f534:	ldrsw	x8, [x19]
   1f538:	cmp	x22, x8
   1f53c:	b.gt	1f5cc <__gmpz_powm_sec@@Base+0x2a8>
   1f540:	ldr	x0, [x19, #8]
   1f544:	mov	x1, x21
   1f548:	mov	x2, x22
   1f54c:	str	w22, [x19, #4]
   1f550:	bl	ca50 <__gmpn_copyi@plt>
   1f554:	ldur	x0, [x29, #-8]
   1f558:	cbnz	x0, 1f5dc <__gmpz_powm_sec@@Base+0x2b8>
   1f55c:	mov	sp, x29
   1f560:	ldp	x20, x19, [sp, #80]
   1f564:	ldp	x22, x21, [sp, #64]
   1f568:	ldp	x24, x23, [sp, #48]
   1f56c:	ldp	x26, x25, [sp, #32]
   1f570:	ldp	x28, x27, [sp, #16]
   1f574:	ldp	x29, x30, [sp], #96
   1f578:	ret
   1f57c:	cbnz	w9, 1f5f4 <__gmpz_powm_sec@@Base+0x2d0>
   1f580:	ldr	w9, [x19]
   1f584:	cmp	w20, #0x1
   1f588:	cset	w10, ne  // ne = any
   1f58c:	cmp	x8, #0x1
   1f590:	cset	w8, ne  // ne = any
   1f594:	orr	w8, w10, w8
   1f598:	cmp	w9, #0x0
   1f59c:	str	w8, [x19, #4]
   1f5a0:	b.le	1f5e4 <__gmpz_powm_sec@@Base+0x2c0>
   1f5a4:	ldr	x0, [x19, #8]
   1f5a8:	mov	w8, #0x1                   	// #1
   1f5ac:	str	x8, [x0]
   1f5b0:	b	1f55c <__gmpz_powm_sec@@Base+0x238>
   1f5b4:	str	wzr, [x19, #4]
   1f5b8:	b	1f55c <__gmpz_powm_sec@@Base+0x238>
   1f5bc:	sub	x0, x29, #0x8
   1f5c0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1f5c4:	mov	x21, x0
   1f5c8:	b	1f3cc <__gmpz_powm_sec@@Base+0xa8>
   1f5cc:	mov	x0, x19
   1f5d0:	mov	x1, x22
   1f5d4:	bl	c080 <__gmpz_realloc@plt>
   1f5d8:	b	1f540 <__gmpz_powm_sec@@Base+0x21c>
   1f5dc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1f5e0:	b	1f55c <__gmpz_powm_sec@@Base+0x238>
   1f5e4:	mov	w1, #0x1                   	// #1
   1f5e8:	mov	x0, x19
   1f5ec:	bl	c080 <__gmpz_realloc@plt>
   1f5f0:	b	1f5a8 <__gmpz_powm_sec@@Base+0x284>
   1f5f4:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000001f5f8 <__gmpz_powm_ui@@Base>:
   1f5f8:	stp	x29, x30, [sp, #-96]!
   1f5fc:	stp	x28, x27, [sp, #16]
   1f600:	stp	x26, x25, [sp, #32]
   1f604:	stp	x24, x23, [sp, #48]
   1f608:	stp	x22, x21, [sp, #64]
   1f60c:	stp	x20, x19, [sp, #80]
   1f610:	mov	x29, sp
   1f614:	sub	sp, sp, #0x40
   1f618:	mov	x25, x3
   1f61c:	mov	x27, x2
   1f620:	mov	x26, x1
   1f624:	cmp	x2, #0x13
   1f628:	mov	x23, x0
   1f62c:	b.hi	1f664 <__gmpz_powm_ui@@Base+0x6c>  // b.pmore
   1f630:	ldr	w8, [x25, #4]
   1f634:	cmp	w8, #0x0
   1f638:	cneg	w22, w8, mi  // mi = first
   1f63c:	cbz	w22, 1fbb4 <__gmpz_powm_ui@@Base+0x5bc>
   1f640:	ldr	x24, [x25, #8]
   1f644:	cmp	x27, #0x1
   1f648:	b.hi	1f68c <__gmpz_powm_ui@@Base+0x94>  // b.pmore
   1f64c:	b.ne	1f6e8 <__gmpz_powm_ui@@Base+0xf0>  // b.any
   1f650:	mov	x0, x23
   1f654:	mov	x1, x26
   1f658:	mov	x2, x25
   1f65c:	bl	cdf0 <__gmpz_mod@plt>
   1f660:	b	1fb3c <__gmpz_powm_ui@@Base+0x544>
   1f664:	sub	x8, x29, #0x8
   1f668:	mov	w9, #0x1                   	// #1
   1f66c:	sub	x2, x29, #0x18
   1f670:	mov	x0, x23
   1f674:	mov	x1, x26
   1f678:	mov	x3, x25
   1f67c:	stp	x8, x27, [x29, #-16]
   1f680:	stur	w9, [x29, #-20]
   1f684:	bl	c370 <__gmpz_powm@plt>
   1f688:	b	1fb3c <__gmpz_powm_ui@@Base+0x544>
   1f68c:	stur	xzr, [x29, #-8]
   1f690:	sub	x20, x22, #0x1
   1f694:	ldr	x8, [x24, x20, lsl #3]
   1f698:	clz	x28, x8
   1f69c:	cbz	w28, 1f6d8 <__gmpz_powm_ui@@Base+0xe0>
   1f6a0:	cmp	w22, #0xfe0
   1f6a4:	lsl	x1, x22, #3
   1f6a8:	b.hi	1fb64 <__gmpz_powm_ui@@Base+0x56c>  // b.pmore
   1f6ac:	add	x9, x1, #0xf
   1f6b0:	mov	x8, sp
   1f6b4:	and	x9, x9, #0xffffffff0
   1f6b8:	sub	x19, x8, x9
   1f6bc:	mov	sp, x19
   1f6c0:	mov	x0, x19
   1f6c4:	mov	x1, x24
   1f6c8:	mov	x2, x22
   1f6cc:	mov	w3, w28
   1f6d0:	bl	c180 <__gmpn_lshift@plt>
   1f6d4:	mov	x24, x19
   1f6d8:	cmp	w22, #0x1
   1f6dc:	b.ne	1f700 <__gmpz_powm_ui@@Base+0x108>  // b.any
   1f6e0:	mov	x21, xzr
   1f6e4:	b	1f708 <__gmpz_powm_ui@@Base+0x110>
   1f6e8:	cmp	w22, #0x1
   1f6ec:	b.ne	1f9bc <__gmpz_powm_ui@@Base+0x3c4>  // b.any
   1f6f0:	ldr	x8, [x24]
   1f6f4:	cmp	x8, #0x1
   1f6f8:	cset	w8, ne  // ne = any
   1f6fc:	b	1f9c0 <__gmpz_powm_ui@@Base+0x3c8>
   1f700:	add	x8, x24, x22, lsl #3
   1f704:	ldur	x21, [x8, #-16]
   1f708:	ldr	x19, [x24, x20, lsl #3]
   1f70c:	mov	x0, x19
   1f710:	bl	d3f0 <__gmpn_invert_limb@plt>
   1f714:	mul	x8, x0, x19
   1f718:	adds	x8, x8, x21
   1f71c:	b.cc	1f738 <__gmpz_powm_ui@@Base+0x140>  // b.lo, b.ul, b.last
   1f720:	subs	x8, x8, x19
   1f724:	cset	w9, cs  // cs = hs, nlast
   1f728:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   1f72c:	mvn	x9, x9
   1f730:	add	x0, x9, x0
   1f734:	sub	x8, x8, x10
   1f738:	umulh	x9, x21, x0
   1f73c:	adds	x9, x9, x8
   1f740:	stp	x28, x27, [x29, #-40]
   1f744:	b.cc	1f76c <__gmpz_powm_ui@@Base+0x174>  // b.lo, b.ul, b.last
   1f748:	cmp	x9, x19
   1f74c:	sub	x8, x0, #0x1
   1f750:	b.cc	1f770 <__gmpz_powm_ui@@Base+0x178>  // b.lo, b.ul, b.last
   1f754:	mul	x10, x0, x21
   1f758:	cmp	x9, x19
   1f75c:	sub	x11, x0, #0x2
   1f760:	ccmp	x10, x21, #0x2, ls  // ls = plast
   1f764:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   1f768:	b	1f770 <__gmpz_powm_ui@@Base+0x178>
   1f76c:	mov	x8, x0
   1f770:	stur	x8, [x29, #-24]
   1f774:	ldr	w8, [x26, #4]
   1f778:	ldr	x28, [x26, #8]
   1f77c:	cmp	w8, #0x0
   1f780:	cneg	w27, w8, mi  // mi = first
   1f784:	cmp	w27, w22
   1f788:	b.ls	1f7f0 <__gmpz_powm_ui@@Base+0x1f8>  // b.plast
   1f78c:	cmp	w22, #0xfe0
   1f790:	lsl	x1, x22, #3
   1f794:	b.hi	1fb94 <__gmpz_powm_ui@@Base+0x59c>  // b.pmore
   1f798:	add	x9, x1, #0xf
   1f79c:	mov	x8, sp
   1f7a0:	and	x9, x9, #0xffffffff0
   1f7a4:	sub	x19, x8, x9
   1f7a8:	mov	sp, x19
   1f7ac:	sub	x5, x29, #0x18
   1f7b0:	mov	x0, x19
   1f7b4:	mov	x1, x28
   1f7b8:	mov	x2, x27
   1f7bc:	mov	x3, x24
   1f7c0:	mov	x4, x22
   1f7c4:	bl	1fbb8 <__gmpz_powm_ui@@Base+0x5c0>
   1f7c8:	sub	x8, x19, #0x8
   1f7cc:	mov	x10, x22
   1f7d0:	subs	x9, x10, #0x1
   1f7d4:	b.lt	1f9ac <__gmpz_powm_ui@@Base+0x3b4>  // b.tstop
   1f7d8:	ldr	x11, [x8, x10, lsl #3]
   1f7dc:	mov	x10, x9
   1f7e0:	cbz	x11, 1f7d0 <__gmpz_powm_ui@@Base+0x1d8>
   1f7e4:	add	x27, x9, #0x1
   1f7e8:	mov	x28, x19
   1f7ec:	b	1f7f4 <__gmpz_powm_ui@@Base+0x1fc>
   1f7f0:	cbz	w27, 1f9ac <__gmpz_powm_ui@@Base+0x3b4>
   1f7f4:	add	x20, x22, #0x1
   1f7f8:	mov	w8, #0x1                   	// #1
   1f7fc:	add	x9, x20, x22
   1f800:	bfi	x8, x22, #1, #32
   1f804:	add	x8, x9, x8
   1f808:	cmp	x8, #0xfe0
   1f80c:	lsl	x1, x8, #3
   1f810:	stp	x26, x23, [x29, #-56]
   1f814:	stur	x25, [x29, #-64]
   1f818:	b.hi	1fb74 <__gmpz_powm_ui@@Base+0x57c>  // b.pmore
   1f81c:	add	x9, x1, #0xf
   1f820:	mov	x8, sp
   1f824:	and	x9, x9, #0x7ffffffff0
   1f828:	sub	x26, x8, x9
   1f82c:	mov	sp, x26
   1f830:	add	x19, x26, x22, lsl #3
   1f834:	mov	x0, x26
   1f838:	mov	x1, x28
   1f83c:	mov	x2, x27
   1f840:	add	x25, x19, x20, lsl #3
   1f844:	bl	ca50 <__gmpn_copyi@plt>
   1f848:	ldur	x9, [x29, #-32]
   1f84c:	mov	x20, x27
   1f850:	clz	x8, x9
   1f854:	lsl	x21, x9, x8
   1f858:	sub	w23, w8, #0x3f
   1f85c:	b	1f878 <__gmpz_powm_ui@@Base+0x280>
   1f860:	mov	x0, x26
   1f864:	mov	x1, x25
   1f868:	mov	x2, x20
   1f86c:	bl	ca50 <__gmpn_copyi@plt>
   1f870:	adds	w23, w23, #0x1
   1f874:	b.cs	1f93c <__gmpz_powm_ui@@Base+0x344>  // b.hs, b.nlast
   1f878:	mov	x0, x25
   1f87c:	mov	x1, x26
   1f880:	mov	x2, x20
   1f884:	lsl	x21, x21, #1
   1f888:	bl	c8e0 <__gmpn_sqr@plt>
   1f88c:	add	x8, x25, x20, lsl #4
   1f890:	ldur	x8, [x8, #-8]
   1f894:	lsl	x9, x20, #1
   1f898:	cmp	x8, #0x0
   1f89c:	cset	w8, eq  // eq = none
   1f8a0:	sub	x20, x9, x8
   1f8a4:	cmp	x20, x22
   1f8a8:	b.lt	1f8cc <__gmpz_powm_ui@@Base+0x2d4>  // b.tstop
   1f8ac:	sub	x4, x29, #0x18
   1f8b0:	mov	x0, x25
   1f8b4:	mov	x1, x20
   1f8b8:	mov	x2, x24
   1f8bc:	mov	x3, x22
   1f8c0:	mov	x5, x19
   1f8c4:	bl	1fc9c <__gmpz_powm_ui@@Base+0x6a4>
   1f8c8:	mov	x20, x22
   1f8cc:	mov	x0, x26
   1f8d0:	mov	x1, x25
   1f8d4:	mov	x2, x20
   1f8d8:	bl	ca50 <__gmpn_copyi@plt>
   1f8dc:	tbz	x21, #63, 1f870 <__gmpz_powm_ui@@Base+0x278>
   1f8e0:	mov	x0, x25
   1f8e4:	mov	x1, x26
   1f8e8:	mov	x2, x20
   1f8ec:	mov	x3, x28
   1f8f0:	mov	x4, x27
   1f8f4:	bl	ccd0 <__gmpn_mul@plt>
   1f8f8:	add	x8, x20, x27
   1f8fc:	add	x9, x25, x8, lsl #3
   1f900:	ldur	x9, [x9, #-8]
   1f904:	cmp	x9, #0x0
   1f908:	cset	w9, eq  // eq = none
   1f90c:	sub	x20, x8, x9
   1f910:	cmp	x20, x22
   1f914:	b.lt	1f860 <__gmpz_powm_ui@@Base+0x268>  // b.tstop
   1f918:	sub	x4, x29, #0x18
   1f91c:	mov	x0, x25
   1f920:	mov	x1, x20
   1f924:	mov	x2, x24
   1f928:	mov	x3, x22
   1f92c:	mov	x5, x19
   1f930:	bl	1fc9c <__gmpz_powm_ui@@Base+0x6a4>
   1f934:	mov	x20, x22
   1f938:	b	1f860 <__gmpz_powm_ui@@Base+0x268>
   1f93c:	ldur	x27, [x29, #-40]
   1f940:	cbz	w27, 1f9e0 <__gmpz_powm_ui@@Base+0x3e8>
   1f944:	mov	x0, x25
   1f948:	mov	x1, x26
   1f94c:	mov	x2, x20
   1f950:	mov	w3, w27
   1f954:	bl	c180 <__gmpn_lshift@plt>
   1f958:	ldur	x21, [x29, #-48]
   1f95c:	ldur	x23, [x29, #-32]
   1f960:	cmp	x0, #0x0
   1f964:	str	x0, [x25, x20, lsl #3]
   1f968:	cinc	x20, x20, ne  // ne = any
   1f96c:	cmp	x20, x22
   1f970:	b.lt	1f994 <__gmpz_powm_ui@@Base+0x39c>  // b.tstop
   1f974:	sub	x4, x29, #0x18
   1f978:	mov	x0, x25
   1f97c:	mov	x1, x20
   1f980:	mov	x2, x24
   1f984:	mov	x3, x22
   1f988:	mov	x5, x19
   1f98c:	bl	1fc9c <__gmpz_powm_ui@@Base+0x6a4>
   1f990:	mov	x20, x22
   1f994:	mov	x0, x26
   1f998:	mov	x1, x25
   1f99c:	mov	x2, x20
   1f9a0:	mov	w3, w27
   1f9a4:	bl	c1a0 <__gmpn_rshift@plt>
   1f9a8:	b	1f9e8 <__gmpz_powm_ui@@Base+0x3f0>
   1f9ac:	str	wzr, [x23, #4]
   1f9b0:	ldur	x0, [x29, #-8]
   1f9b4:	cbz	x0, 1fb3c <__gmpz_powm_ui@@Base+0x544>
   1f9b8:	b	1fb5c <__gmpz_powm_ui@@Base+0x564>
   1f9bc:	mov	w8, #0x1                   	// #1
   1f9c0:	ldr	w9, [x23]
   1f9c4:	str	w8, [x23, #4]
   1f9c8:	cmp	w9, #0x0
   1f9cc:	b.le	1fba4 <__gmpz_powm_ui@@Base+0x5ac>
   1f9d0:	ldr	x0, [x23, #8]
   1f9d4:	mov	w8, #0x1                   	// #1
   1f9d8:	str	x8, [x0]
   1f9dc:	b	1fb3c <__gmpz_powm_ui@@Base+0x544>
   1f9e0:	ldur	x21, [x29, #-48]
   1f9e4:	ldur	x23, [x29, #-32]
   1f9e8:	sub	x25, x26, #0x8
   1f9ec:	mov	x24, x20
   1f9f0:	subs	x20, x20, #0x1
   1f9f4:	b.lt	1fa00 <__gmpz_powm_ui@@Base+0x408>  // b.tstop
   1f9f8:	ldr	x8, [x25, x24, lsl #3]
   1f9fc:	cbz	x8, 1f9ec <__gmpz_powm_ui@@Base+0x3f4>
   1fa00:	tbz	w23, #0, 1fb14 <__gmpz_powm_ui@@Base+0x51c>
   1fa04:	cbz	x24, 1fb14 <__gmpz_powm_ui@@Base+0x51c>
   1fa08:	ldur	x8, [x29, #-56]
   1fa0c:	ldr	w8, [x8, #4]
   1fa10:	tbz	w8, #31, 1fb14 <__gmpz_powm_ui@@Base+0x51c>
   1fa14:	ldur	x8, [x29, #-64]
   1fa18:	mov	x0, x26
   1fa1c:	mov	x2, x26
   1fa20:	mov	x3, x24
   1fa24:	ldr	x19, [x8, #8]
   1fa28:	mov	x1, x19
   1fa2c:	bl	c2d0 <__gmpn_sub_n@plt>
   1fa30:	cbz	x0, 1fa54 <__gmpz_powm_ui@@Base+0x45c>
   1fa34:	cmp	x24, x22
   1fa38:	b.ge	1faf4 <__gmpz_powm_ui@@Base+0x4fc>  // b.tcont
   1fa3c:	lsl	x8, x24, #3
   1fa40:	ldr	x9, [x19, x8]
   1fa44:	add	x24, x24, #0x1
   1fa48:	sub	x10, x9, #0x1
   1fa4c:	str	x10, [x26, x8]
   1fa50:	cbz	x9, 1fa34 <__gmpz_powm_ui@@Base+0x43c>
   1fa54:	cmp	x19, x26
   1fa58:	b.eq	1faf4 <__gmpz_powm_ui@@Base+0x4fc>  // b.none
   1fa5c:	cmp	x24, x22
   1fa60:	b.ge	1faf4 <__gmpz_powm_ui@@Base+0x4fc>  // b.tcont
   1fa64:	sub	x8, x22, x24
   1fa68:	cmp	x8, #0x4
   1fa6c:	b.cc	1fad4 <__gmpz_powm_ui@@Base+0x4dc>  // b.lo, b.ul, b.last
   1fa70:	lsl	x10, x24, #3
   1fa74:	lsl	x9, x22, #3
   1fa78:	add	x11, x26, x10
   1fa7c:	add	x12, x19, x9
   1fa80:	cmp	x11, x12
   1fa84:	b.cs	1fa98 <__gmpz_powm_ui@@Base+0x4a0>  // b.hs, b.nlast
   1fa88:	add	x9, x26, x9
   1fa8c:	add	x11, x19, x10
   1fa90:	cmp	x9, x11
   1fa94:	b.hi	1fad4 <__gmpz_powm_ui@@Base+0x4dc>  // b.pmore
   1fa98:	and	x9, x8, #0xfffffffffffffffc
   1fa9c:	add	x11, x10, x19
   1faa0:	add	x12, x10, x26
   1faa4:	add	x24, x24, x9
   1faa8:	add	x10, x11, #0x10
   1faac:	add	x11, x12, #0x10
   1fab0:	mov	x12, x9
   1fab4:	ldp	q0, q1, [x10, #-16]
   1fab8:	add	x10, x10, #0x20
   1fabc:	subs	x12, x12, #0x4
   1fac0:	stp	q0, q1, [x11, #-16]
   1fac4:	add	x11, x11, #0x20
   1fac8:	b.ne	1fab4 <__gmpz_powm_ui@@Base+0x4bc>  // b.any
   1facc:	cmp	x8, x9
   1fad0:	b.eq	1faf4 <__gmpz_powm_ui@@Base+0x4fc>  // b.none
   1fad4:	lsl	x10, x24, #3
   1fad8:	sub	x8, x22, x24
   1fadc:	add	x9, x26, x10
   1fae0:	add	x10, x19, x10
   1fae4:	ldr	x11, [x10], #8
   1fae8:	subs	x8, x8, #0x1
   1faec:	str	x11, [x9], #8
   1faf0:	b.ne	1fae4 <__gmpz_powm_ui@@Base+0x4ec>  // b.any
   1faf4:	subs	x8, x22, #0x1
   1faf8:	b.lt	1fb10 <__gmpz_powm_ui@@Base+0x518>  // b.tstop
   1fafc:	ldr	x9, [x25, x22, lsl #3]
   1fb00:	mov	x22, x8
   1fb04:	cbz	x9, 1faf4 <__gmpz_powm_ui@@Base+0x4fc>
   1fb08:	add	x24, x8, #0x1
   1fb0c:	b	1fb14 <__gmpz_powm_ui@@Base+0x51c>
   1fb10:	mov	x24, xzr
   1fb14:	ldrsw	x8, [x21]
   1fb18:	cmp	x24, x8
   1fb1c:	b.gt	1fb84 <__gmpz_powm_ui@@Base+0x58c>
   1fb20:	ldr	x0, [x21, #8]
   1fb24:	mov	x1, x26
   1fb28:	mov	x2, x24
   1fb2c:	str	w24, [x21, #4]
   1fb30:	bl	ca50 <__gmpn_copyi@plt>
   1fb34:	ldur	x0, [x29, #-8]
   1fb38:	cbnz	x0, 1fb5c <__gmpz_powm_ui@@Base+0x564>
   1fb3c:	mov	sp, x29
   1fb40:	ldp	x20, x19, [sp, #80]
   1fb44:	ldp	x22, x21, [sp, #64]
   1fb48:	ldp	x24, x23, [sp, #48]
   1fb4c:	ldp	x26, x25, [sp, #32]
   1fb50:	ldp	x28, x27, [sp, #16]
   1fb54:	ldp	x29, x30, [sp], #96
   1fb58:	ret
   1fb5c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1fb60:	b	1fb3c <__gmpz_powm_ui@@Base+0x544>
   1fb64:	sub	x0, x29, #0x8
   1fb68:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fb6c:	mov	x19, x0
   1fb70:	b	1f6c0 <__gmpz_powm_ui@@Base+0xc8>
   1fb74:	sub	x0, x29, #0x8
   1fb78:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fb7c:	mov	x26, x0
   1fb80:	b	1f830 <__gmpz_powm_ui@@Base+0x238>
   1fb84:	mov	x0, x21
   1fb88:	mov	x1, x24
   1fb8c:	bl	c080 <__gmpz_realloc@plt>
   1fb90:	b	1fb20 <__gmpz_powm_ui@@Base+0x528>
   1fb94:	sub	x0, x29, #0x8
   1fb98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fb9c:	mov	x19, x0
   1fba0:	b	1f7ac <__gmpz_powm_ui@@Base+0x1b4>
   1fba4:	mov	w1, #0x1                   	// #1
   1fba8:	mov	x0, x23
   1fbac:	bl	c080 <__gmpz_realloc@plt>
   1fbb0:	b	1f9d4 <__gmpz_powm_ui@@Base+0x3dc>
   1fbb4:	bl	bfd0 <__gmp_divide_by_zero@plt>
   1fbb8:	stp	x29, x30, [sp, #-80]!
   1fbbc:	stp	x26, x25, [sp, #16]
   1fbc0:	stp	x24, x23, [sp, #32]
   1fbc4:	stp	x22, x21, [sp, #48]
   1fbc8:	stp	x20, x19, [sp, #64]
   1fbcc:	mov	x29, sp
   1fbd0:	sub	sp, sp, #0x10
   1fbd4:	mov	w8, #0x1                   	// #1
   1fbd8:	bfi	x8, x2, #1, #63
   1fbdc:	sub	x8, x8, x4
   1fbe0:	mov	x24, x1
   1fbe4:	lsl	x1, x8, #3
   1fbe8:	mov	w8, #0x7f00                	// #32512
   1fbec:	mov	x21, x5
   1fbf0:	mov	x19, x4
   1fbf4:	mov	x22, x3
   1fbf8:	mov	x23, x2
   1fbfc:	mov	x20, x0
   1fc00:	cmp	x1, x8
   1fc04:	stur	xzr, [x29, #-8]
   1fc08:	b.hi	1fc84 <__gmpz_powm_ui@@Base+0x68c>  // b.pmore
   1fc0c:	add	x9, x1, #0xf
   1fc10:	mov	x8, sp
   1fc14:	and	x9, x9, #0xfffffffffffffff0
   1fc18:	sub	x25, x8, x9
   1fc1c:	mov	sp, x25
   1fc20:	mov	x0, x25
   1fc24:	mov	x1, x24
   1fc28:	mov	x2, x23
   1fc2c:	add	x26, x25, x23, lsl #3
   1fc30:	bl	ca50 <__gmpn_copyi@plt>
   1fc34:	mov	x0, x25
   1fc38:	mov	x1, x23
   1fc3c:	mov	x2, x22
   1fc40:	mov	x3, x19
   1fc44:	mov	x4, x21
   1fc48:	mov	x5, x26
   1fc4c:	bl	1fc9c <__gmpz_powm_ui@@Base+0x6a4>
   1fc50:	mov	x0, x20
   1fc54:	mov	x1, x25
   1fc58:	mov	x2, x19
   1fc5c:	bl	ca50 <__gmpn_copyi@plt>
   1fc60:	ldur	x0, [x29, #-8]
   1fc64:	cbnz	x0, 1fc94 <__gmpz_powm_ui@@Base+0x69c>
   1fc68:	mov	sp, x29
   1fc6c:	ldp	x20, x19, [sp, #64]
   1fc70:	ldp	x22, x21, [sp, #48]
   1fc74:	ldp	x24, x23, [sp, #32]
   1fc78:	ldp	x26, x25, [sp, #16]
   1fc7c:	ldp	x29, x30, [sp], #80
   1fc80:	ret
   1fc84:	sub	x0, x29, #0x8
   1fc88:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fc8c:	mov	x25, x0
   1fc90:	b	1fc20 <__gmpz_powm_ui@@Base+0x628>
   1fc94:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1fc98:	b	1fc68 <__gmpz_powm_ui@@Base+0x670>
   1fc9c:	stp	x29, x30, [sp, #-80]!
   1fca0:	stp	x24, x23, [sp, #32]
   1fca4:	stp	x22, x21, [sp, #48]
   1fca8:	stp	x20, x19, [sp, #64]
   1fcac:	mov	x23, x5
   1fcb0:	mov	x8, x4
   1fcb4:	mov	x22, x2
   1fcb8:	mov	x21, x1
   1fcbc:	cmp	x3, #0x2
   1fcc0:	mov	x19, x0
   1fcc4:	str	x25, [sp, #16]
   1fcc8:	mov	x29, sp
   1fccc:	b.eq	1fd10 <__gmpz_powm_ui@@Base+0x718>  // b.none
   1fcd0:	mov	x20, x3
   1fcd4:	cmp	x3, #0x1
   1fcd8:	b.ne	1fd40 <__gmpz_powm_ui@@Base+0x748>  // b.any
   1fcdc:	ldr	x4, [x22]
   1fce0:	mov	x0, x23
   1fce4:	mov	x1, xzr
   1fce8:	mov	x2, x19
   1fcec:	mov	x3, x21
   1fcf0:	bl	cd00 <__gmpn_divrem_1@plt>
   1fcf4:	str	x0, [x19]
   1fcf8:	ldp	x20, x19, [sp, #64]
   1fcfc:	ldp	x22, x21, [sp, #48]
   1fd00:	ldp	x24, x23, [sp, #32]
   1fd04:	ldr	x25, [sp, #16]
   1fd08:	ldp	x29, x30, [sp], #80
   1fd0c:	ret
   1fd10:	ldp	x5, x4, [x22]
   1fd14:	ldr	x6, [x8]
   1fd18:	mov	x0, x23
   1fd1c:	mov	x1, x19
   1fd20:	mov	x2, x19
   1fd24:	mov	x3, x21
   1fd28:	ldp	x20, x19, [sp, #64]
   1fd2c:	ldp	x22, x21, [sp, #48]
   1fd30:	ldp	x24, x23, [sp, #32]
   1fd34:	ldr	x25, [sp, #16]
   1fd38:	ldp	x29, x30, [sp], #80
   1fd3c:	b	c920 <__gmpn_div_qr_2n_pi1@plt>
   1fd40:	cmp	x20, #0x2a
   1fd44:	b.lt	1fdcc <__gmpz_powm_ui@@Base+0x7d4>  // b.tstop
   1fd48:	sub	x9, x21, x20
   1fd4c:	cmp	x9, #0x29
   1fd50:	b.le	1fdcc <__gmpz_powm_ui@@Base+0x7d4>
   1fd54:	cmp	x21, #0x7cc
   1fd58:	b.lt	1fd9c <__gmpz_powm_ui@@Base+0x7a4>  // b.tstop
   1fd5c:	cmp	x20, #0x62
   1fd60:	b.lt	1fd9c <__gmpz_powm_ui@@Base+0x7a4>  // b.tstop
   1fd64:	mov	x9, #0x200000000000        	// #35184372088832
   1fd68:	mov	x10, #0x800000000000        	// #140737488355328
   1fd6c:	movk	x9, #0x409c, lsl #48
   1fd70:	movk	x10, #0x4058, lsl #48
   1fd74:	scvtf	d0, x20
   1fd78:	scvtf	d1, x21
   1fd7c:	fmov	d2, x9
   1fd80:	fmov	d3, x10
   1fd84:	fmul	d2, d0, d2
   1fd88:	fmul	d3, d1, d3
   1fd8c:	fadd	d2, d3, d2
   1fd90:	fmul	d0, d1, d0
   1fd94:	fcmp	d2, d0
   1fd98:	b.le	1fdfc <__gmpz_powm_ui@@Base+0x804>
   1fd9c:	mov	x0, x23
   1fda0:	mov	x1, x19
   1fda4:	mov	x2, x21
   1fda8:	mov	x3, x22
   1fdac:	mov	x4, x20
   1fdb0:	ldp	x20, x19, [sp, #64]
   1fdb4:	ldp	x22, x21, [sp, #48]
   1fdb8:	ldp	x24, x23, [sp, #32]
   1fdbc:	ldr	x25, [sp, #16]
   1fdc0:	mov	x5, x8
   1fdc4:	ldp	x29, x30, [sp], #80
   1fdc8:	b	c3b0 <__gmpn_dcpi1_div_qr@plt>
   1fdcc:	ldr	x5, [x8]
   1fdd0:	mov	x0, x23
   1fdd4:	mov	x1, x19
   1fdd8:	mov	x2, x21
   1fddc:	mov	x3, x22
   1fde0:	mov	x4, x20
   1fde4:	ldp	x20, x19, [sp, #64]
   1fde8:	ldp	x22, x21, [sp, #48]
   1fdec:	ldp	x24, x23, [sp, #32]
   1fdf0:	ldr	x25, [sp, #16]
   1fdf4:	ldp	x29, x30, [sp], #80
   1fdf8:	b	c640 <__gmpn_sbpi1_div_qr@plt>
   1fdfc:	mov	x0, x21
   1fe00:	mov	x1, x20
   1fe04:	mov	w2, wzr
   1fe08:	str	xzr, [x29, #24]
   1fe0c:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   1fe10:	mov	x24, x0
   1fe14:	lsl	x1, x20, #3
   1fe18:	add	x0, x29, #0x18
   1fe1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fe20:	mov	x25, x0
   1fe24:	lsl	x1, x24, #3
   1fe28:	add	x0, x29, #0x18
   1fe2c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   1fe30:	mov	x6, x0
   1fe34:	mov	x0, x23
   1fe38:	mov	x1, x25
   1fe3c:	mov	x2, x19
   1fe40:	mov	x3, x21
   1fe44:	mov	x4, x22
   1fe48:	mov	x5, x20
   1fe4c:	bl	c960 <__gmpn_mu_div_qr@plt>
   1fe50:	mov	x0, x19
   1fe54:	mov	x1, x25
   1fe58:	mov	x2, x20
   1fe5c:	bl	ca50 <__gmpn_copyi@plt>
   1fe60:	ldr	x0, [x29, #24]
   1fe64:	cbz	x0, 1fcf8 <__gmpz_powm_ui@@Base+0x700>
   1fe68:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   1fe6c:	b	1fcf8 <__gmpz_powm_ui@@Base+0x700>

000000000001fe70 <__gmpz_primorial_ui@@Base>:
   1fe70:	stp	x29, x30, [sp, #-48]!
   1fe74:	stp	x20, x19, [sp, #32]
   1fe78:	mov	x20, x1
   1fe7c:	cmp	x1, #0x4
   1fe80:	mov	x19, x0
   1fe84:	str	x21, [sp, #16]
   1fe88:	mov	x29, sp
   1fe8c:	b.hi	1fec0 <__gmpz_primorial_ui@@Base+0x50>  // b.pmore
   1fe90:	ldr	w8, [x19]
   1fe94:	add	w9, w20, w20, lsl #1
   1fe98:	mov	w10, #0x6c89                	// #27785
   1fe9c:	lsr	w9, w10, w9
   1fea0:	cmp	w8, #0x0
   1fea4:	and	w20, w9, #0x7
   1fea8:	b.le	2000c <__gmpz_primorial_ui@@Base+0x19c>
   1feac:	ldr	x0, [x19, #8]
   1feb0:	mov	w8, #0x1                   	// #1
   1feb4:	str	x20, [x0]
   1feb8:	str	w8, [x19, #4]
   1febc:	b	1ffd8 <__gmpz_primorial_ui@@Base+0x168>
   1fec0:	ldrsw	x9, [x19]
   1fec4:	lsr	x8, x20, #6
   1fec8:	add	x8, x8, x20, lsr #7
   1fecc:	cmp	x8, x9
   1fed0:	b.ge	2001c <__gmpz_primorial_ui@@Base+0x1ac>  // b.tcont
   1fed4:	ldr	x21, [x19, #8]
   1fed8:	mov	x0, x21
   1fedc:	mov	x1, x20
   1fee0:	bl	d200 <__gmp_primesieve@plt>
   1fee4:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   1fee8:	ldr	x9, [x9, #3880]
   1feec:	mov	w8, #0x9                   	// #9
   1fef0:	sub	w10, w8, #0x2
   1fef4:	ldr	x10, [x9, w10, uxtw #3]
   1fef8:	sub	w8, w8, #0x1
   1fefc:	cmp	x10, x20
   1ff00:	b.cc	1fef0 <__gmpz_primorial_ui@@Base+0x80>  // b.lo, b.ul, b.last
   1ff04:	add	x9, x0, #0x1
   1ff08:	mov	w8, w8
   1ff0c:	udiv	x8, x9, x8
   1ff10:	lsl	x8, x8, #3
   1ff14:	add	x1, x8, #0x8
   1ff18:	mov	w8, #0x7f00                	// #32512
   1ff1c:	cmp	x1, x8
   1ff20:	str	xzr, [x29, #24]
   1ff24:	b.hi	20030 <__gmpz_primorial_ui@@Base+0x1c0>  // b.pmore
   1ff28:	add	x9, x1, #0xf
   1ff2c:	mov	x8, sp
   1ff30:	and	x9, x9, #0xfffffffffffffff0
   1ff34:	sub	x1, x8, x9
   1ff38:	mov	sp, x1
   1ff3c:	sub	x12, x20, #0x5
   1ff40:	mov	x13, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   1ff44:	movk	x13, #0xaaab
   1ff48:	orr	x12, x12, #0x1
   1ff4c:	umulh	x12, x12, x13
   1ff50:	mov	x9, xzr
   1ff54:	mov	x8, xzr
   1ff58:	mov	x11, #0xffffffffffffffff    	// #-1
   1ff5c:	mov	w14, #0x1                   	// #1
   1ff60:	mov	w10, #0x6                   	// #6
   1ff64:	lsr	x12, x12, #1
   1ff68:	mov	w13, #0x4                   	// #4
   1ff6c:	b	1ff8c <__gmpz_primorial_ui@@Base+0x11c>
   1ff70:	mul	x10, x15, x10
   1ff74:	add	x11, x11, #0x1
   1ff78:	add	x9, x9, x14, lsr #63
   1ff7c:	ror	x14, x14, #63
   1ff80:	cmp	x11, x12
   1ff84:	add	x13, x13, #0x3
   1ff88:	b.cs	1ffbc <__gmpz_primorial_ui@@Base+0x14c>  // b.hs, b.nlast
   1ff8c:	ldr	x15, [x21, x9, lsl #3]
   1ff90:	tst	x15, x14
   1ff94:	b.ne	1ff74 <__gmpz_primorial_ui@@Base+0x104>  // b.any
   1ff98:	and	x15, x11, #0x1
   1ff9c:	umulh	x16, x20, x10
   1ffa0:	add	x15, x13, x15
   1ffa4:	cbz	x16, 1ff70 <__gmpz_primorial_ui@@Base+0x100>
   1ffa8:	add	x16, x8, #0x1
   1ffac:	str	x10, [x1, x8, lsl #3]
   1ffb0:	mov	x10, x15
   1ffb4:	mov	x8, x16
   1ffb8:	b	1ff74 <__gmpz_primorial_ui@@Base+0x104>
   1ffbc:	cbz	x8, 1ffec <__gmpz_primorial_ui@@Base+0x17c>
   1ffc0:	add	x2, x8, #0x1
   1ffc4:	mov	x0, x19
   1ffc8:	str	x10, [x1, x8, lsl #3]
   1ffcc:	bl	cd70 <__gmpz_prodlimbs@plt>
   1ffd0:	ldr	x0, [x29, #24]
   1ffd4:	cbnz	x0, 20004 <__gmpz_primorial_ui@@Base+0x194>
   1ffd8:	mov	sp, x29
   1ffdc:	ldp	x20, x19, [sp, #32]
   1ffe0:	ldr	x21, [sp, #16]
   1ffe4:	ldp	x29, x30, [sp], #48
   1ffe8:	ret
   1ffec:	ldr	x8, [x19, #8]
   1fff0:	mov	w9, #0x1                   	// #1
   1fff4:	str	x10, [x8]
   1fff8:	str	w9, [x19, #4]
   1fffc:	ldr	x0, [x29, #24]
   20000:	cbz	x0, 1ffd8 <__gmpz_primorial_ui@@Base+0x168>
   20004:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   20008:	b	1ffd8 <__gmpz_primorial_ui@@Base+0x168>
   2000c:	mov	w1, #0x1                   	// #1
   20010:	mov	x0, x19
   20014:	bl	c080 <__gmpz_realloc@plt>
   20018:	b	1feb0 <__gmpz_primorial_ui@@Base+0x40>
   2001c:	add	x1, x8, #0x1
   20020:	mov	x0, x19
   20024:	bl	c080 <__gmpz_realloc@plt>
   20028:	mov	x21, x0
   2002c:	b	1fed8 <__gmpz_primorial_ui@@Base+0x68>
   20030:	add	x0, x29, #0x18
   20034:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   20038:	mov	x1, x0
   2003c:	b	1ff3c <__gmpz_primorial_ui@@Base+0xcc>

0000000000020040 <__gmpz_probab_prime_p@@Base>:
   20040:	sub	sp, sp, #0xb0
   20044:	stp	x20, x19, [sp, #160]
   20048:	mov	w19, w1
   2004c:	mov	w1, #0x4240                	// #16960
   20050:	movk	w1, #0xf, lsl #16
   20054:	stp	x29, x30, [sp, #80]
   20058:	str	x27, [sp, #96]
   2005c:	stp	x26, x25, [sp, #112]
   20060:	stp	x24, x23, [sp, #128]
   20064:	stp	x22, x21, [sp, #144]
   20068:	add	x29, sp, #0x50
   2006c:	mov	x20, x0
   20070:	bl	d1f0 <__gmpz_cmp_ui@plt>
   20074:	cmp	w0, #0x0
   20078:	b.gt	200ac <__gmpz_probab_prime_p@@Base+0x6c>
   2007c:	mov	w1, #0x4240                	// #16960
   20080:	movk	w1, #0xf, lsl #16
   20084:	mov	x0, x20
   20088:	bl	c100 <__gmpz_cmpabs_ui@plt>
   2008c:	cmp	w0, #0x0
   20090:	b.le	20378 <__gmpz_probab_prime_p@@Base+0x338>
   20094:	ldr	x8, [x20, #8]
   20098:	stur	x8, [x29, #-8]
   2009c:	ldr	w8, [x20, #4]
   200a0:	sub	x20, x29, #0x10
   200a4:	neg	w8, w8
   200a8:	stur	w8, [x29, #-12]
   200ac:	ldr	x22, [x20, #8]
   200b0:	ldrsw	x21, [x20, #4]
   200b4:	ldr	w8, [x22]
   200b8:	cmp	x21, #0x0
   200bc:	cset	w9, ne  // ne = any
   200c0:	tst	w8, w9
   200c4:	b.eq	204a0 <__gmpz_probab_prime_p@@Base+0x460>  // b.none
   200c8:	mov	x2, #0x4e1d                	// #19997
   200cc:	movk	x2, #0x30e9, lsl #16
   200d0:	movk	x2, #0xf97c, lsl #32
   200d4:	movk	x2, #0xe221, lsl #48
   200d8:	cmp	w21, #0x14
   200dc:	b.le	200f0 <__gmpz_probab_prime_p@@Base+0xb0>
   200e0:	mov	x0, x22
   200e4:	mov	x1, x21
   200e8:	bl	c3e0 <__gmpn_mod_1@plt>
   200ec:	b	2010c <__gmpz_probab_prime_p@@Base+0xcc>
   200f0:	mov	x3, #0xb36b                	// #45931
   200f4:	movk	x3, #0xc938, lsl #16
   200f8:	movk	x3, #0xe6cf, lsl #32
   200fc:	movk	x3, #0x21cf, lsl #48
   20100:	mov	x0, x22
   20104:	mov	x1, x21
   20108:	bl	c6d0 <__gmpn_preinv_mod_1@plt>
   2010c:	mov	x9, #0x521d                	// #21021
   20110:	movk	x9, #0x8c13, lsl #16
   20114:	mov	x10, #0x304e                	// #12366
   20118:	movk	x9, #0xb2b7, lsl #32
   2011c:	movk	x10, #0xcade, lsl #16
   20120:	movk	x9, #0x21cf, lsl #48
   20124:	movk	x10, #0x873e, lsl #32
   20128:	mul	x9, x0, x9
   2012c:	movk	x10, #0x4d4, lsl #48
   20130:	mov	x8, x0
   20134:	cmp	x9, x10
   20138:	mov	w0, wzr
   2013c:	b.cc	204a4 <__gmpz_probab_prime_p@@Base+0x464>  // b.lo, b.ul, b.last
   20140:	mov	x9, #0x7263                	// #29283
   20144:	movk	x9, #0x3105, lsl #16
   20148:	movk	x9, #0x82b9, lsl #32
   2014c:	movk	x9, #0x5c98, lsl #48
   20150:	umulh	x9, x8, x9
   20154:	sub	x10, x8, x9
   20158:	add	x9, x9, x10, lsr #1
   2015c:	lsr	x9, x9, #5
   20160:	mov	w10, #0x2f                  	// #47
   20164:	msub	x9, x9, x10, x8
   20168:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   2016c:	mov	x9, #0xa0bf                	// #41151
   20170:	movk	x9, #0xe82f, lsl #16
   20174:	movk	x9, #0xfa0b, lsl #32
   20178:	movk	x9, #0xbe82, lsl #48
   2017c:	umulh	x9, x8, x9
   20180:	lsr	x9, x9, #5
   20184:	mov	w10, #0x2b                  	// #43
   20188:	msub	x9, x9, x10, x8
   2018c:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20190:	mov	x9, #0xce0d                	// #52749
   20194:	movk	x9, #0xe0c7, lsl #16
   20198:	movk	x9, #0xc7c, lsl #32
   2019c:	movk	x9, #0xc7ce, lsl #48
   201a0:	umulh	x9, x8, x9
   201a4:	lsr	x9, x9, #5
   201a8:	mov	w10, #0x29                  	// #41
   201ac:	msub	x9, x9, x10, x8
   201b0:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   201b4:	mov	x9, #0x7c8b                	// #31883
   201b8:	movk	x9, #0xdd6, lsl #16
   201bc:	movk	x9, #0xc8a6, lsl #32
   201c0:	movk	x9, #0xdd67, lsl #48
   201c4:	umulh	x9, x8, x9
   201c8:	lsr	x9, x9, #5
   201cc:	mov	w10, #0x25                  	// #37
   201d0:	msub	x9, x9, x10, x8
   201d4:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   201d8:	mov	x9, #0x4211                	// #16913
   201dc:	movk	x9, #0x2108, lsl #16
   201e0:	movk	x9, #0x1084, lsl #32
   201e4:	movk	x9, #0x842, lsl #48
   201e8:	umulh	x9, x8, x9
   201ec:	sub	x10, x8, x9
   201f0:	add	x9, x9, x10, lsr #1
   201f4:	lsr	x9, x9, #4
   201f8:	sub	x9, x9, x9, lsl #5
   201fc:	add	x9, x8, x9
   20200:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20204:	mov	x9, #0x611b                	// #24859
   20208:	movk	x9, #0xa7b9, lsl #16
   2020c:	movk	x9, #0x9611, lsl #32
   20210:	movk	x9, #0x1a7b, lsl #48
   20214:	umulh	x9, x8, x9
   20218:	sub	x10, x8, x9
   2021c:	add	x9, x9, x10, lsr #1
   20220:	lsr	x9, x9, #4
   20224:	mov	w10, #0x1d                  	// #29
   20228:	msub	x9, x9, x10, x8
   2022c:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20230:	mov	x9, #0x42c9                	// #17097
   20234:	movk	x9, #0xb216, lsl #16
   20238:	movk	x9, #0x8590, lsl #32
   2023c:	movk	x9, #0x642c, lsl #48
   20240:	umulh	x9, x8, x9
   20244:	sub	x10, x8, x9
   20248:	add	x9, x9, x10, lsr #1
   2024c:	lsr	x9, x9, #4
   20250:	mov	w10, #0x17                  	// #23
   20254:	msub	x9, x9, x10, x8
   20258:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   2025c:	mov	x9, #0x435f                	// #17247
   20260:	movk	x9, #0xd79, lsl #16
   20264:	movk	x9, #0x35e5, lsl #32
   20268:	movk	x9, #0xd794, lsl #48
   2026c:	umulh	x9, x8, x9
   20270:	lsr	x9, x9, #4
   20274:	mov	w10, #0x13                  	// #19
   20278:	msub	x9, x9, x10, x8
   2027c:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20280:	mov	x9, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   20284:	movk	x9, #0xf0f1
   20288:	umulh	x9, x8, x9
   2028c:	lsr	x9, x9, #4
   20290:	add	x9, x9, x9, lsl #4
   20294:	sub	x9, x8, x9
   20298:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   2029c:	mov	x9, #0x4ec5                	// #20165
   202a0:	movk	x9, #0xc4ec, lsl #16
   202a4:	movk	x9, #0xec4e, lsl #32
   202a8:	movk	x9, #0x4ec4, lsl #48
   202ac:	umulh	x9, x8, x9
   202b0:	lsr	x9, x9, #2
   202b4:	mov	w10, #0xd                   	// #13
   202b8:	msub	x9, x9, x10, x8
   202bc:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   202c0:	mov	x9, #0x8ba3                	// #35747
   202c4:	movk	x9, #0xba2e, lsl #16
   202c8:	movk	x9, #0xa2e8, lsl #32
   202cc:	movk	x9, #0x2e8b, lsl #48
   202d0:	umulh	x9, x8, x9
   202d4:	lsr	x9, x9, #1
   202d8:	mov	w10, #0xb                   	// #11
   202dc:	msub	x9, x9, x10, x8
   202e0:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   202e4:	mov	x9, #0x2493                	// #9363
   202e8:	movk	x9, #0x9249, lsl #16
   202ec:	movk	x9, #0x4924, lsl #32
   202f0:	movk	x9, #0x2492, lsl #48
   202f4:	umulh	x9, x8, x9
   202f8:	sub	x10, x8, x9
   202fc:	add	x9, x9, x10, lsr #1
   20300:	lsr	x9, x9, #2
   20304:	sub	x9, x9, x9, lsl #3
   20308:	add	x9, x8, x9
   2030c:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20310:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   20314:	movk	x9, #0xaaab
   20318:	umulh	x9, x8, x9
   2031c:	lsr	x9, x9, #1
   20320:	add	x9, x9, x9, lsl #1
   20324:	sub	x9, x8, x9
   20328:	cbz	x9, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   2032c:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   20330:	movk	x9, #0xcccd
   20334:	umulh	x9, x8, x9
   20338:	lsr	x9, x9, #2
   2033c:	add	x9, x9, x9, lsl #2
   20340:	sub	x8, x8, x9
   20344:	cbz	x8, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20348:	mov	w1, #0x2                   	// #2
   2034c:	mov	x0, x20
   20350:	bl	d260 <__gmpz_sizeinbase@plt>
   20354:	cmp	x0, #0x3c
   20358:	b.cc	20458 <__gmpz_probab_prime_p@@Base+0x418>  // b.lo, b.ul, b.last
   2035c:	add	x24, sp, #0x4
   20360:	mov	x23, x0
   20364:	mov	w27, wzr
   20368:	mov	w25, #0x3b                  	// #59
   2036c:	sub	x26, x24, #0x4
   20370:	mov	w2, #0x1                   	// #1
   20374:	b	203dc <__gmpz_probab_prime_p@@Base+0x39c>
   20378:	ldr	x8, [x20, #8]
   2037c:	ldr	w9, [x20, #4]
   20380:	ldr	x8, [x8]
   20384:	cmp	w9, #0x0
   20388:	csel	x8, xzr, x8, eq  // eq = none
   2038c:	cmp	x8, #0x1
   20390:	cset	w9, hi  // hi = pmore
   20394:	tst	x8, x9
   20398:	b.eq	20490 <__gmpz_probab_prime_p@@Base+0x450>  // b.none
   2039c:	mov	w9, #0x3                   	// #3
   203a0:	udiv	x10, x8, x9
   203a4:	cmp	x10, x9
   203a8:	b.cc	20498 <__gmpz_probab_prime_p@@Base+0x458>  // b.lo, b.ul, b.last
   203ac:	mul	x10, x10, x9
   203b0:	cmp	x10, x8
   203b4:	add	x9, x9, #0x2
   203b8:	b.ne	203a0 <__gmpz_probab_prime_p@@Base+0x360>  // b.any
   203bc:	b	204a0 <__gmpz_probab_prime_p@@Base+0x460>
   203c0:	mul	x2, x2, x25
   203c4:	add	w8, w27, #0x1
   203c8:	str	w25, [x24, w27, sxtw #2]
   203cc:	mov	w27, w8
   203d0:	add	x25, x25, #0x2
   203d4:	cmp	x25, x23
   203d8:	b.cs	20458 <__gmpz_probab_prime_p@@Base+0x418>  // b.hs, b.nlast
   203dc:	mov	w8, #0x3                   	// #3
   203e0:	udiv	x9, x25, x8
   203e4:	cmp	x9, x8
   203e8:	b.cc	20400 <__gmpz_probab_prime_p@@Base+0x3c0>  // b.lo, b.ul, b.last
   203ec:	mul	x9, x9, x8
   203f0:	cmp	x9, x25
   203f4:	add	x8, x8, #0x2
   203f8:	b.ne	203e0 <__gmpz_probab_prime_p@@Base+0x3a0>  // b.any
   203fc:	b	203d0 <__gmpz_probab_prime_p@@Base+0x390>
   20400:	umulh	x8, x2, x25
   20404:	cbz	x8, 203c0 <__gmpz_probab_prime_p@@Base+0x380>
   20408:	mov	x0, x22
   2040c:	mov	x1, x21
   20410:	cmp	w21, #0x27
   20414:	b.le	20420 <__gmpz_probab_prime_p@@Base+0x3e0>
   20418:	bl	c3e0 <__gmpn_mod_1@plt>
   2041c:	b	20428 <__gmpz_probab_prime_p@@Base+0x3e8>
   20420:	mov	x3, xzr
   20424:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   20428:	sxtw	x8, w27
   2042c:	subs	x9, x8, #0x1
   20430:	b.lt	2044c <__gmpz_probab_prime_p@@Base+0x40c>  // b.tstop
   20434:	ldr	w2, [x26, x8, lsl #2]
   20438:	udiv	x8, x0, x2
   2043c:	msub	x10, x8, x2, x0
   20440:	mov	x8, x9
   20444:	cbnz	x10, 2042c <__gmpz_probab_prime_p@@Base+0x3ec>
   20448:	b	20468 <__gmpz_probab_prime_p@@Base+0x428>
   2044c:	mov	w27, wzr
   20450:	mov	x2, x25
   20454:	b	203c4 <__gmpz_probab_prime_p@@Base+0x384>
   20458:	mov	x0, x20
   2045c:	mov	w1, w19
   20460:	bl	cb80 <__gmpz_millerrabin@plt>
   20464:	b	204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20468:	mov	x0, x22
   2046c:	mov	x1, x21
   20470:	bl	c3e0 <__gmpn_mod_1@plt>
   20474:	cbz	x0, 204a4 <__gmpz_probab_prime_p@@Base+0x464>
   20478:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   2047c:	adrp	x2, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   20480:	add	x0, x0, #0x617
   20484:	add	x2, x2, #0x622
   20488:	mov	w1, #0x83                  	// #131
   2048c:	bl	c6c0 <__gmp_assert_fail@plt>
   20490:	cmp	x8, #0x2
   20494:	b.ne	204a0 <__gmpz_probab_prime_p@@Base+0x460>  // b.any
   20498:	mov	w0, #0x2                   	// #2
   2049c:	b	204a4 <__gmpz_probab_prime_p@@Base+0x464>
   204a0:	mov	w0, wzr
   204a4:	ldp	x20, x19, [sp, #160]
   204a8:	ldp	x22, x21, [sp, #144]
   204ac:	ldp	x24, x23, [sp, #128]
   204b0:	ldp	x26, x25, [sp, #112]
   204b4:	ldr	x27, [sp, #96]
   204b8:	ldp	x29, x30, [sp, #80]
   204bc:	add	sp, sp, #0xb0
   204c0:	ret

00000000000204c4 <__gmpz_random@@Base>:
   204c4:	stp	x29, x30, [sp, #-32]!
   204c8:	stp	x20, x19, [sp, #16]
   204cc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   204d0:	ldr	x8, [x8, #4040]
   204d4:	mov	x20, x1
   204d8:	mov	x19, x0
   204dc:	mov	x29, sp
   204e0:	ldrb	w9, [x8]
   204e4:	cbnz	w9, 204fc <__gmpz_random@@Base+0x38>
   204e8:	mov	w9, #0x1                   	// #1
   204ec:	strb	w9, [x8]
   204f0:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   204f4:	ldr	x0, [x0, #3976]
   204f8:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   204fc:	adrp	x1, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   20500:	ldr	x1, [x1, #3976]
   20504:	cmp	x20, #0x0
   20508:	cneg	x8, x20, mi  // mi = first
   2050c:	lsl	x2, x8, #6
   20510:	mov	x0, x19
   20514:	bl	d430 <__gmpz_urandomb@plt>
   20518:	tbz	x20, #63, 20528 <__gmpz_random@@Base+0x64>
   2051c:	ldr	w8, [x19, #4]
   20520:	neg	w8, w8
   20524:	str	w8, [x19, #4]
   20528:	ldp	x20, x19, [sp, #16]
   2052c:	ldp	x29, x30, [sp], #32
   20530:	ret

0000000000020534 <__gmpz_random2@@Base>:
   20534:	stp	x29, x30, [sp, #-48]!
   20538:	cmp	x1, #0x0
   2053c:	str	x21, [sp, #16]
   20540:	stp	x20, x19, [sp, #32]
   20544:	mov	x19, x1
   20548:	mov	x20, x0
   2054c:	cneg	x21, x1, mi  // mi = first
   20550:	mov	x29, sp
   20554:	cbz	x1, 20570 <__gmpz_random2@@Base+0x3c>
   20558:	ldrsw	x8, [x20]
   2055c:	cmp	x21, x8
   20560:	b.gt	20584 <__gmpz_random2@@Base+0x50>
   20564:	ldr	x0, [x20, #8]
   20568:	mov	x1, x21
   2056c:	bl	d1c0 <__gmpn_random2@plt>
   20570:	str	w19, [x20, #4]
   20574:	ldp	x20, x19, [sp, #32]
   20578:	ldr	x21, [sp, #16]
   2057c:	ldp	x29, x30, [sp], #48
   20580:	ret
   20584:	mov	x0, x20
   20588:	mov	x1, x21
   2058c:	bl	c080 <__gmpz_realloc@plt>
   20590:	b	20568 <__gmpz_random2@@Base+0x34>

0000000000020594 <__gmpz_realloc@@Base>:
   20594:	stp	x29, x30, [sp, #-32]!
   20598:	cmp	x1, #0x1
   2059c:	stp	x20, x19, [sp, #16]
   205a0:	csinc	x20, x1, xzr, gt
   205a4:	mov	w8, #0x80000000            	// #-2147483648
   205a8:	cmp	x20, x8
   205ac:	mov	x29, sp
   205b0:	b.ge	20620 <__gmpz_realloc@@Base+0x8c>  // b.tcont
   205b4:	ldrsw	x8, [x0]
   205b8:	mov	x19, x0
   205bc:	cbz	w8, 205f8 <__gmpz_realloc@@Base+0x64>
   205c0:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   205c4:	ldr	x9, [x9, #3792]
   205c8:	ldr	x0, [x19, #8]
   205cc:	lsl	x1, x8, #3
   205d0:	lsl	x2, x20, #3
   205d4:	ldr	x9, [x9]
   205d8:	blr	x9
   205dc:	ldr	w8, [x19, #4]
   205e0:	cmp	w8, #0x0
   205e4:	cneg	w8, w8, mi  // mi = first
   205e8:	cmp	x20, x8
   205ec:	b.ge	2060c <__gmpz_realloc@@Base+0x78>  // b.tcont
   205f0:	str	wzr, [x19, #4]
   205f4:	b	2060c <__gmpz_realloc@@Base+0x78>
   205f8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   205fc:	ldr	x8, [x8, #3840]
   20600:	lsl	x0, x20, #3
   20604:	ldr	x8, [x8]
   20608:	blr	x8
   2060c:	str	x0, [x19, #8]
   20610:	str	w20, [x19]
   20614:	ldp	x20, x19, [sp, #16]
   20618:	ldp	x29, x30, [sp], #32
   2061c:	ret
   20620:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   20624:	ldr	x8, [x8, #3824]
   20628:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   2062c:	add	x0, x0, #0x538
   20630:	mov	w1, #0x1a                  	// #26
   20634:	ldr	x3, [x8]
   20638:	mov	w2, #0x1                   	// #1
   2063c:	bl	ce30 <fwrite@plt>
   20640:	bl	c900 <abort@plt>

0000000000020644 <__gmpz_realloc2@@Base>:
   20644:	stp	x29, x30, [sp, #-32]!
   20648:	cmp	x1, #0x0
   2064c:	cset	w8, ne  // ne = any
   20650:	sub	x8, x1, x8
   20654:	mov	x9, #0x1fffffffc0          	// #137438953408
   20658:	cmp	x8, x9
   2065c:	stp	x20, x19, [sp, #16]
   20660:	mov	x29, sp
   20664:	b.cs	206e0 <__gmpz_realloc2@@Base+0x9c>  // b.hs, b.nlast
   20668:	ldrsw	x9, [x0]
   2066c:	lsr	x8, x8, #6
   20670:	mov	x19, x0
   20674:	add	x20, x8, #0x1
   20678:	cbz	w9, 206b8 <__gmpz_realloc2@@Base+0x74>
   2067c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   20680:	ldr	x8, [x8, #3792]
   20684:	ldr	x0, [x19, #8]
   20688:	lsl	x1, x9, #3
   2068c:	lsl	x2, x20, #3
   20690:	ldr	x8, [x8]
   20694:	blr	x8
   20698:	ldr	w8, [x19, #4]
   2069c:	str	x0, [x19, #8]
   206a0:	cmp	w8, #0x0
   206a4:	cneg	w8, w8, mi  // mi = first
   206a8:	cmp	x20, x8
   206ac:	b.cs	206d0 <__gmpz_realloc2@@Base+0x8c>  // b.hs, b.nlast
   206b0:	str	wzr, [x19, #4]
   206b4:	b	206d0 <__gmpz_realloc2@@Base+0x8c>
   206b8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   206bc:	ldr	x8, [x8, #3840]
   206c0:	lsl	x0, x20, #3
   206c4:	ldr	x8, [x8]
   206c8:	blr	x8
   206cc:	str	x0, [x19, #8]
   206d0:	str	w20, [x19]
   206d4:	ldp	x20, x19, [sp, #16]
   206d8:	ldp	x29, x30, [sp], #32
   206dc:	ret
   206e0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   206e4:	ldr	x8, [x8, #3824]
   206e8:	adrp	x0, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   206ec:	add	x0, x0, #0x538
   206f0:	mov	w1, #0x1a                  	// #26
   206f4:	ldr	x3, [x8]
   206f8:	mov	w2, #0x1                   	// #1
   206fc:	bl	ce30 <fwrite@plt>
   20700:	bl	c900 <abort@plt>

0000000000020704 <__gmpz_remove@@Base>:
   20704:	stp	x29, x30, [sp, #-80]!
   20708:	stp	x28, x25, [sp, #16]
   2070c:	stp	x24, x23, [sp, #32]
   20710:	stp	x22, x21, [sp, #48]
   20714:	stp	x20, x19, [sp, #64]
   20718:	mov	x29, sp
   2071c:	sub	sp, sp, #0x420
   20720:	ldr	x4, [x2, #8]
   20724:	ldrsw	x9, [x2, #4]
   20728:	ldrsw	x25, [x1, #4]
   2072c:	mov	x21, x1
   20730:	ldr	x8, [x4]
   20734:	cmp	x9, #0x0
   20738:	cneg	x22, x9, mi  // mi = first
   2073c:	mov	x19, x0
   20740:	cmp	x8, #0x1
   20744:	cset	w10, eq  // eq = none
   20748:	cbz	w25, 209b4 <__gmpz_remove@@Base+0x2b0>
   2074c:	cmp	x22, x10
   20750:	b.le	209b4 <__gmpz_remove@@Base+0x2b0>
   20754:	mov	x20, x2
   20758:	and	x24, x9, #0xffffffff
   2075c:	tbnz	w8, #0, 207ac <__gmpz_remove@@Base+0xa8>
   20760:	cmp	x8, #0x2
   20764:	cset	w8, eq  // eq = none
   20768:	cmp	x22, x8
   2076c:	b.ne	20804 <__gmpz_remove@@Base+0x100>  // b.any
   20770:	mov	x0, x21
   20774:	mov	x1, xzr
   20778:	bl	bf20 <__gmpz_scan1@plt>
   2077c:	mov	x20, x0
   20780:	mov	x0, x19
   20784:	mov	x1, x21
   20788:	mov	x2, x20
   2078c:	bl	c650 <__gmpz_fdiv_q_2exp@plt>
   20790:	ubfx	x8, x24, #31, #1
   20794:	tst	x20, x8
   20798:	b.eq	20850 <__gmpz_remove@@Base+0x14c>  // b.none
   2079c:	ldr	w8, [x19, #4]
   207a0:	neg	w8, w8
   207a4:	str	w8, [x19, #4]
   207a8:	b	20850 <__gmpz_remove@@Base+0x14c>
   207ac:	cmp	x25, #0x0
   207b0:	cneg	x23, x25, mi  // mi = first
   207b4:	str	x23, [sp]
   207b8:	ldrsw	x8, [x19]
   207bc:	cmp	x23, x8
   207c0:	b.gt	209cc <__gmpz_remove@@Base+0x2c8>
   207c4:	ldr	x0, [x19, #8]
   207c8:	ldr	x2, [x21, #8]
   207cc:	mov	x1, sp
   207d0:	mov	x6, #0xffffffffffffffff    	// #-1
   207d4:	mov	x3, x23
   207d8:	mov	x5, x22
   207dc:	bl	cd40 <__gmpn_remove@plt>
   207e0:	ldr	x8, [sp]
   207e4:	and	x9, x0, x24, lsr #31
   207e8:	ubfx	x10, x25, #31, #1
   207ec:	cmp	x9, x10
   207f0:	neg	w11, w8
   207f4:	csel	x8, x8, x11, eq  // eq = none
   207f8:	mov	x20, x0
   207fc:	str	w8, [x19, #4]
   20800:	b	20850 <__gmpz_remove@@Base+0x14c>
   20804:	sub	x0, x29, #0x20
   20808:	bl	d250 <__gmpz_init@plt>
   2080c:	sub	x0, x29, #0x10
   20810:	bl	d250 <__gmpz_init@plt>
   20814:	sub	x0, x29, #0x10
   20818:	sub	x1, x29, #0x20
   2081c:	mov	x2, x21
   20820:	mov	x3, x20
   20824:	bl	bff0 <__gmpz_tdiv_qr@plt>
   20828:	ldur	w8, [x29, #-28]
   2082c:	cbz	w8, 20870 <__gmpz_remove@@Base+0x16c>
   20830:	mov	x0, x19
   20834:	mov	x1, x21
   20838:	bl	c420 <__gmpz_set@plt>
   2083c:	mov	x20, xzr
   20840:	sub	x0, x29, #0x10
   20844:	bl	cb50 <__gmpz_clear@plt>
   20848:	sub	x0, x29, #0x20
   2084c:	bl	cb50 <__gmpz_clear@plt>
   20850:	mov	x0, x20
   20854:	add	sp, sp, #0x420
   20858:	ldp	x20, x19, [sp, #64]
   2085c:	ldp	x22, x21, [sp, #48]
   20860:	ldp	x24, x23, [sp, #32]
   20864:	ldp	x28, x25, [sp, #16]
   20868:	ldp	x29, x30, [sp], #80
   2086c:	ret
   20870:	mov	x0, sp
   20874:	mov	x1, x20
   20878:	mov	x22, sp
   2087c:	bl	bf80 <__gmpz_init_set@plt>
   20880:	sub	x1, x29, #0x10
   20884:	mov	x0, x19
   20888:	bl	c580 <__gmpz_swap@plt>
   2088c:	ldr	w8, [x19, #4]
   20890:	ldr	w9, [sp, #4]
   20894:	cmp	w8, #0x0
   20898:	cneg	w8, w8, mi  // mi = first
   2089c:	cmp	w9, #0x0
   208a0:	cneg	w9, w9, mi  // mi = first
   208a4:	lsl	w9, w9, #1
   208a8:	sub	w9, w9, #0x1
   208ac:	cmp	w8, w9
   208b0:	b.ge	208bc <__gmpz_remove@@Base+0x1b8>  // b.tcont
   208b4:	mov	w23, #0x1                   	// #1
   208b8:	b	20944 <__gmpz_remove@@Base+0x240>
   208bc:	add	x20, x22, #0x10
   208c0:	mov	w23, #0x1                   	// #1
   208c4:	mov	x0, x20
   208c8:	sub	x21, x20, #0x10
   208cc:	bl	d250 <__gmpz_init@plt>
   208d0:	mov	x0, x20
   208d4:	mov	x1, x21
   208d8:	mov	x2, x21
   208dc:	bl	c4b0 <__gmpz_mul@plt>
   208e0:	sub	x0, x29, #0x10
   208e4:	sub	x1, x29, #0x20
   208e8:	mov	x2, x19
   208ec:	mov	x3, x20
   208f0:	bl	bff0 <__gmpz_tdiv_qr@plt>
   208f4:	ldur	w8, [x29, #-28]
   208f8:	cbnz	w8, 2093c <__gmpz_remove@@Base+0x238>
   208fc:	sub	x1, x29, #0x10
   20900:	mov	x0, x19
   20904:	bl	c580 <__gmpz_swap@plt>
   20908:	ldr	w8, [x19, #4]
   2090c:	ldr	w9, [x20, #4]
   20910:	add	w23, w23, #0x1
   20914:	add	x20, x20, #0x10
   20918:	cmp	w8, #0x0
   2091c:	cneg	w8, w8, mi  // mi = first
   20920:	cmp	w9, #0x0
   20924:	cneg	w9, w9, mi  // mi = first
   20928:	lsl	w9, w9, #1
   2092c:	sub	w9, w9, #0x1
   20930:	cmp	w8, w9
   20934:	b.ge	208c4 <__gmpz_remove@@Base+0x1c0>  // b.tcont
   20938:	b	20944 <__gmpz_remove@@Base+0x240>
   2093c:	mov	x0, x20
   20940:	bl	cb50 <__gmpz_clear@plt>
   20944:	mov	x8, #0xffffffffffffffff    	// #-1
   20948:	sub	w25, w23, #0x1
   2094c:	lsl	x8, x8, x23
   20950:	add	w24, w23, #0x1
   20954:	add	x21, x22, w25, sxtw #4
   20958:	mvn	x20, x8
   2095c:	mov	w22, #0x1                   	// #1
   20960:	b	20980 <__gmpz_remove@@Base+0x27c>
   20964:	mov	x0, x21
   20968:	bl	cb50 <__gmpz_clear@plt>
   2096c:	sub	w24, w24, #0x1
   20970:	sub	x21, x21, #0x10
   20974:	cmp	w24, #0x1
   20978:	sub	x25, x25, #0x1
   2097c:	b.le	20840 <__gmpz_remove@@Base+0x13c>
   20980:	sub	x0, x29, #0x10
   20984:	sub	x1, x29, #0x20
   20988:	mov	x2, x19
   2098c:	mov	x3, x21
   20990:	bl	bff0 <__gmpz_tdiv_qr@plt>
   20994:	ldur	w8, [x29, #-28]
   20998:	cbnz	w8, 20964 <__gmpz_remove@@Base+0x260>
   2099c:	lsl	x8, x22, x25
   209a0:	sub	x1, x29, #0x10
   209a4:	mov	x0, x19
   209a8:	add	x20, x8, x20
   209ac:	bl	c580 <__gmpz_swap@plt>
   209b0:	b	20964 <__gmpz_remove@@Base+0x260>
   209b4:	cbz	x22, 209e0 <__gmpz_remove@@Base+0x2dc>
   209b8:	mov	x0, x19
   209bc:	mov	x1, x21
   209c0:	bl	c420 <__gmpz_set@plt>
   209c4:	mov	x20, xzr
   209c8:	b	20850 <__gmpz_remove@@Base+0x14c>
   209cc:	mov	x0, x19
   209d0:	mov	x1, x23
   209d4:	bl	c080 <__gmpz_realloc@plt>
   209d8:	ldr	x4, [x20, #8]
   209dc:	b	207c8 <__gmpz_remove@@Base+0xc4>
   209e0:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000209e4 <__gmpz_roinit_n@@Base>:
   209e4:	cmp	x2, #0x0
   209e8:	cneg	x9, x2, mi  // mi = first
   209ec:	mov	x8, x9
   209f0:	subs	x9, x9, #0x1
   209f4:	b.lt	20a04 <__gmpz_roinit_n@@Base+0x20>  // b.tstop
   209f8:	add	x10, x1, x8, lsl #3
   209fc:	ldur	x10, [x10, #-8]
   20a00:	cbz	x10, 209ec <__gmpz_roinit_n@@Base+0x8>
   20a04:	neg	w9, w8
   20a08:	cmp	x2, #0x0
   20a0c:	csel	x8, x9, x8, lt  // lt = tstop
   20a10:	stp	wzr, w8, [x0]
   20a14:	str	x1, [x0, #8]
   20a18:	ret

0000000000020a1c <__gmpz_root@@Base>:
   20a1c:	stp	x29, x30, [sp, #-96]!
   20a20:	stp	x26, x25, [sp, #32]
   20a24:	stp	x24, x23, [sp, #48]
   20a28:	stp	x22, x21, [sp, #64]
   20a2c:	stp	x20, x19, [sp, #80]
   20a30:	ldr	w8, [x1, #4]
   20a34:	mov	x22, x2
   20a38:	mov	x20, x1
   20a3c:	mov	x19, x0
   20a40:	str	x27, [sp, #16]
   20a44:	mov	x29, sp
   20a48:	tbnz	w22, #0, 20a50 <__gmpz_root@@Base+0x34>
   20a4c:	tbnz	w8, #31, 20b94 <__gmpz_root@@Base+0x178>
   20a50:	cbz	x22, 20b98 <__gmpz_root@@Base+0x17c>
   20a54:	sxtw	x26, w8
   20a58:	cbz	w26, 20a94 <__gmpz_root@@Base+0x78>
   20a5c:	cmp	w26, #0x0
   20a60:	cneg	x23, x26, lt  // lt = tstop
   20a64:	sub	x8, x23, #0x1
   20a68:	udiv	x27, x8, x22
   20a6c:	cmp	x20, x19
   20a70:	add	x21, x27, #0x1
   20a74:	str	xzr, [x29, #24]
   20a78:	b.eq	20aa4 <__gmpz_root@@Base+0x88>  // b.none
   20a7c:	cbz	x19, 20aa4 <__gmpz_root@@Base+0x88>
   20a80:	ldrsw	x8, [x19]
   20a84:	cmp	x21, x8
   20a88:	b.gt	20b74 <__gmpz_root@@Base+0x158>
   20a8c:	ldr	x24, [x19, #8]
   20a90:	b	20ac8 <__gmpz_root@@Base+0xac>
   20a94:	cbz	x19, 20b64 <__gmpz_root@@Base+0x148>
   20a98:	str	wzr, [x19, #4]
   20a9c:	mov	w0, #0x1                   	// #1
   20aa0:	b	20b44 <__gmpz_root@@Base+0x128>
   20aa4:	lsl	x1, x21, #3
   20aa8:	mov	w8, #0x7f00                	// #32512
   20aac:	cmp	x1, x8
   20ab0:	b.hi	20b84 <__gmpz_root@@Base+0x168>  // b.pmore
   20ab4:	add	x9, x1, #0xf
   20ab8:	mov	x8, sp
   20abc:	and	x9, x9, #0xfffffffffffffff0
   20ac0:	sub	x24, x8, x9
   20ac4:	mov	sp, x24
   20ac8:	ldr	x25, [x20, #8]
   20acc:	mov	x0, x24
   20ad0:	cmp	x22, #0x1
   20ad4:	b.ne	20af0 <__gmpz_root@@Base+0xd4>  // b.any
   20ad8:	mov	x1, x25
   20adc:	mov	x2, x23
   20ae0:	bl	ca50 <__gmpn_copyi@plt>
   20ae4:	mov	x22, xzr
   20ae8:	cbnz	x19, 20b0c <__gmpz_root@@Base+0xf0>
   20aec:	b	20b34 <__gmpz_root@@Base+0x118>
   20af0:	mov	x1, xzr
   20af4:	mov	x2, x25
   20af8:	mov	x3, x23
   20afc:	mov	x4, x22
   20b00:	bl	c2a0 <__gmpn_rootrem@plt>
   20b04:	mov	x22, x0
   20b08:	cbz	x19, 20b34 <__gmpz_root@@Base+0x118>
   20b0c:	mvn	w8, w27
   20b10:	cmp	w26, #0x0
   20b14:	csel	x8, x21, x8, ge  // ge = tcont
   20b18:	cmp	x20, x19
   20b1c:	str	w8, [x19, #4]
   20b20:	b.ne	20b34 <__gmpz_root@@Base+0x118>  // b.any
   20b24:	mov	x0, x25
   20b28:	mov	x1, x24
   20b2c:	mov	x2, x21
   20b30:	bl	ca50 <__gmpn_copyi@plt>
   20b34:	ldr	x0, [x29, #24]
   20b38:	cbnz	x0, 20b6c <__gmpz_root@@Base+0x150>
   20b3c:	cmp	x22, #0x0
   20b40:	cset	w0, eq  // eq = none
   20b44:	mov	sp, x29
   20b48:	ldp	x20, x19, [sp, #80]
   20b4c:	ldp	x22, x21, [sp, #64]
   20b50:	ldp	x24, x23, [sp, #48]
   20b54:	ldp	x26, x25, [sp, #32]
   20b58:	ldr	x27, [sp, #16]
   20b5c:	ldp	x29, x30, [sp], #96
   20b60:	ret
   20b64:	mov	w0, #0x1                   	// #1
   20b68:	b	20b44 <__gmpz_root@@Base+0x128>
   20b6c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   20b70:	b	20b3c <__gmpz_root@@Base+0x120>
   20b74:	mov	x0, x19
   20b78:	mov	x1, x21
   20b7c:	bl	c080 <__gmpz_realloc@plt>
   20b80:	b	20b8c <__gmpz_root@@Base+0x170>
   20b84:	add	x0, x29, #0x18
   20b88:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   20b8c:	mov	x24, x0
   20b90:	b	20ac8 <__gmpz_root@@Base+0xac>
   20b94:	bl	cff0 <__gmp_sqrt_of_negative@plt>
   20b98:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000020b9c <__gmpz_rootrem@@Base>:
   20b9c:	stp	x29, x30, [sp, #-96]!
   20ba0:	stp	x28, x27, [sp, #16]
   20ba4:	stp	x26, x25, [sp, #32]
   20ba8:	stp	x24, x23, [sp, #48]
   20bac:	stp	x22, x21, [sp, #64]
   20bb0:	stp	x20, x19, [sp, #80]
   20bb4:	mov	x29, sp
   20bb8:	sub	sp, sp, #0x10
   20bbc:	ldr	w8, [x2, #4]
   20bc0:	mov	x23, x3
   20bc4:	mov	x20, x2
   20bc8:	mov	x19, x1
   20bcc:	mov	x21, x0
   20bd0:	tbnz	w23, #0, 20bd8 <__gmpz_rootrem@@Base+0x3c>
   20bd4:	tbnz	w8, #31, 20da8 <__gmpz_rootrem@@Base+0x20c>
   20bd8:	cbz	x23, 20dac <__gmpz_rootrem@@Base+0x210>
   20bdc:	sxtw	x28, w8
   20be0:	cbz	w28, 20c24 <__gmpz_rootrem@@Base+0x88>
   20be4:	cmp	w28, #0x0
   20be8:	cneg	x24, x28, lt  // lt = tstop
   20bec:	sub	x8, x24, #0x1
   20bf0:	udiv	x22, x8, x23
   20bf4:	cmp	x20, x21
   20bf8:	add	x1, x22, #0x1
   20bfc:	stp	x1, xzr, [x29, #-16]
   20c00:	b.eq	20c34 <__gmpz_rootrem@@Base+0x98>  // b.none
   20c04:	cbz	x21, 20c34 <__gmpz_rootrem@@Base+0x98>
   20c08:	ldrsw	x8, [x21]
   20c0c:	cmp	x1, x8
   20c10:	b.gt	20c84 <__gmpz_rootrem@@Base+0xe8>
   20c14:	ldr	x25, [x21, #8]
   20c18:	cmp	x20, x19
   20c1c:	b.ne	20c60 <__gmpz_rootrem@@Base+0xc4>  // b.any
   20c20:	b	20ca4 <__gmpz_rootrem@@Base+0x108>
   20c24:	cbz	x21, 20c2c <__gmpz_rootrem@@Base+0x90>
   20c28:	str	wzr, [x21, #4]
   20c2c:	str	wzr, [x19, #4]
   20c30:	b	20d58 <__gmpz_rootrem@@Base+0x1bc>
   20c34:	lsl	x1, x1, #3
   20c38:	mov	w8, #0x7f00                	// #32512
   20c3c:	cmp	x1, x8
   20c40:	b.hi	20c90 <__gmpz_rootrem@@Base+0xf4>  // b.pmore
   20c44:	add	x9, x1, #0xf
   20c48:	mov	x8, sp
   20c4c:	and	x9, x9, #0xfffffffffffffff0
   20c50:	sub	x25, x8, x9
   20c54:	mov	sp, x25
   20c58:	cmp	x20, x19
   20c5c:	b.eq	20ca4 <__gmpz_rootrem@@Base+0x108>  // b.none
   20c60:	ldrsw	x8, [x19]
   20c64:	cmp	x24, x8
   20c68:	b.gt	20c74 <__gmpz_rootrem@@Base+0xd8>
   20c6c:	ldr	x26, [x19, #8]
   20c70:	b	20cc4 <__gmpz_rootrem@@Base+0x128>
   20c74:	mov	x0, x19
   20c78:	mov	x1, x24
   20c7c:	bl	c080 <__gmpz_realloc@plt>
   20c80:	b	20da0 <__gmpz_rootrem@@Base+0x204>
   20c84:	mov	x0, x21
   20c88:	bl	c080 <__gmpz_realloc@plt>
   20c8c:	b	20c98 <__gmpz_rootrem@@Base+0xfc>
   20c90:	sub	x0, x29, #0x8
   20c94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   20c98:	mov	x25, x0
   20c9c:	cmp	x20, x19
   20ca0:	b.ne	20c60 <__gmpz_rootrem@@Base+0xc4>  // b.any
   20ca4:	cmp	x24, #0xfe0
   20ca8:	lsl	x1, x24, #3
   20cac:	b.hi	20d98 <__gmpz_rootrem@@Base+0x1fc>  // b.pmore
   20cb0:	add	x9, x1, #0xf
   20cb4:	mov	x8, sp
   20cb8:	and	x9, x9, #0xfffffffffffffff0
   20cbc:	sub	x26, x8, x9
   20cc0:	mov	sp, x26
   20cc4:	ldr	x27, [x20, #8]
   20cc8:	mov	x0, x25
   20ccc:	cmp	x23, #0x1
   20cd0:	b.ne	20cf0 <__gmpz_rootrem@@Base+0x154>  // b.any
   20cd4:	mov	x1, x27
   20cd8:	mov	x2, x24
   20cdc:	bl	ca50 <__gmpn_copyi@plt>
   20ce0:	mov	x23, xzr
   20ce4:	ldur	x2, [x29, #-16]
   20ce8:	cbnz	x21, 20d10 <__gmpz_rootrem@@Base+0x174>
   20cec:	b	20d28 <__gmpz_rootrem@@Base+0x18c>
   20cf0:	mov	x1, x26
   20cf4:	mov	x2, x27
   20cf8:	mov	x3, x24
   20cfc:	mov	x4, x23
   20d00:	bl	c2a0 <__gmpn_rootrem@plt>
   20d04:	mov	x23, x0
   20d08:	ldur	x2, [x29, #-16]
   20d0c:	cbz	x21, 20d28 <__gmpz_rootrem@@Base+0x18c>
   20d10:	mvn	w8, w22
   20d14:	cmp	w28, #0x0
   20d18:	csel	x8, x2, x8, ge  // ge = tcont
   20d1c:	cmp	x20, x21
   20d20:	str	w8, [x21, #4]
   20d24:	b.eq	20d78 <__gmpz_rootrem@@Base+0x1dc>  // b.none
   20d28:	cmp	x20, x19
   20d2c:	b.ne	20d40 <__gmpz_rootrem@@Base+0x1a4>  // b.any
   20d30:	mov	x0, x27
   20d34:	mov	x1, x26
   20d38:	mov	x2, x23
   20d3c:	bl	ca50 <__gmpn_copyi@plt>
   20d40:	neg	w8, w23
   20d44:	cmp	w28, #0x0
   20d48:	csel	x8, x23, x8, ge  // ge = tcont
   20d4c:	str	w8, [x19, #4]
   20d50:	ldur	x0, [x29, #-8]
   20d54:	cbnz	x0, 20d90 <__gmpz_rootrem@@Base+0x1f4>
   20d58:	mov	sp, x29
   20d5c:	ldp	x20, x19, [sp, #80]
   20d60:	ldp	x22, x21, [sp, #64]
   20d64:	ldp	x24, x23, [sp, #48]
   20d68:	ldp	x26, x25, [sp, #32]
   20d6c:	ldp	x28, x27, [sp, #16]
   20d70:	ldp	x29, x30, [sp], #96
   20d74:	ret
   20d78:	mov	x0, x27
   20d7c:	mov	x1, x25
   20d80:	bl	ca50 <__gmpn_copyi@plt>
   20d84:	cmp	x20, x19
   20d88:	b.ne	20d40 <__gmpz_rootrem@@Base+0x1a4>  // b.any
   20d8c:	b	20d30 <__gmpz_rootrem@@Base+0x194>
   20d90:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   20d94:	b	20d58 <__gmpz_rootrem@@Base+0x1bc>
   20d98:	sub	x0, x29, #0x8
   20d9c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   20da0:	mov	x26, x0
   20da4:	b	20cc4 <__gmpz_rootrem@@Base+0x128>
   20da8:	bl	cff0 <__gmp_sqrt_of_negative@plt>
   20dac:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000020db0 <__gmpz_rrandomb@@Base>:
   20db0:	stp	x29, x30, [sp, #-96]!
   20db4:	stp	x24, x23, [sp, #48]
   20db8:	add	x24, x2, #0x3f
   20dbc:	stp	x20, x19, [sp, #80]
   20dc0:	mov	x19, x0
   20dc4:	lsr	x20, x24, #6
   20dc8:	str	x27, [sp, #16]
   20dcc:	stp	x26, x25, [sp, #32]
   20dd0:	stp	x22, x21, [sp, #64]
   20dd4:	mov	x29, sp
   20dd8:	cbz	x2, 20f28 <__gmpz_rrandomb@@Base+0x178>
   20ddc:	ldrsw	x8, [x19]
   20de0:	mov	x23, x2
   20de4:	mov	x21, x1
   20de8:	cmp	x20, x8
   20dec:	b.gt	20f48 <__gmpz_rrandomb@@Base+0x198>
   20df0:	ldr	x22, [x19, #8]
   20df4:	neg	w9, w23
   20df8:	mov	x10, #0xffffffffffffffff    	// #-1
   20dfc:	sub	x8, x20, #0x1
   20e00:	lsr	x9, x10, x9
   20e04:	cmp	x24, #0x80
   20e08:	str	x9, [x22, x8, lsl #3]
   20e0c:	b.cc	20e34 <__gmpz_rrandomb@@Base+0x84>  // b.lo, b.ul, b.last
   20e10:	cmp	x20, #0x2
   20e14:	mov	w9, #0x2                   	// #2
   20e18:	csel	x9, x20, x9, cc  // cc = lo, ul, last
   20e1c:	sub	x9, x9, #0x2
   20e20:	sub	x8, x8, x9
   20e24:	add	x0, x22, x9, lsl #3
   20e28:	lsl	x2, x8, #3
   20e2c:	mov	w1, #0xff                  	// #255
   20e30:	bl	c5f0 <memset@plt>
   20e34:	ldr	x8, [x21, #24]
   20e38:	add	x1, x29, #0x18
   20e3c:	mov	w2, #0x20                  	// #32
   20e40:	mov	x0, x21
   20e44:	ldr	x8, [x8, #8]
   20e48:	blr	x8
   20e4c:	ldr	x8, [x29, #24]
   20e50:	add	x24, x22, #0x8
   20e54:	mov	w26, #0x1                   	// #1
   20e58:	and	x8, x8, #0x3
   20e5c:	add	x8, x8, #0x1
   20e60:	udiv	x8, x23, x8
   20e64:	cmp	w8, #0x0
   20e68:	cinc	w25, w8, eq  // eq = none
   20e6c:	b	20e78 <__gmpz_rrandomb@@Base+0xc8>
   20e70:	cmp	x27, x8
   20e74:	b.ls	20f28 <__gmpz_rrandomb@@Base+0x178>  // b.plast
   20e78:	ldr	x8, [x21, #24]
   20e7c:	add	x1, x29, #0x18
   20e80:	mov	w2, #0x20                  	// #32
   20e84:	mov	x0, x21
   20e88:	ldr	x8, [x8, #8]
   20e8c:	blr	x8
   20e90:	ldr	x8, [x29, #24]
   20e94:	udiv	x9, x8, x25
   20e98:	msub	x8, x9, x25, x8
   20e9c:	add	x8, x8, #0x1
   20ea0:	subs	x8, x23, x8
   20ea4:	csel	x27, xzr, x8, cc  // cc = lo, ul, last
   20ea8:	b.ls	20f28 <__gmpz_rrandomb@@Base+0x178>  // b.plast
   20eac:	lsr	x8, x27, #3
   20eb0:	and	x8, x8, #0x1ffffffffffffff8
   20eb4:	ldr	x9, [x22, x8]
   20eb8:	lsl	x10, x26, x27
   20ebc:	add	x1, x29, #0x18
   20ec0:	mov	w2, #0x20                  	// #32
   20ec4:	eor	x9, x9, x10
   20ec8:	str	x9, [x22, x8]
   20ecc:	ldr	x8, [x21, #24]
   20ed0:	mov	x0, x21
   20ed4:	ldr	x8, [x8, #8]
   20ed8:	blr	x8
   20edc:	ldr	x8, [x29, #24]
   20ee0:	udiv	x9, x8, x25
   20ee4:	msub	x8, x9, x25, x8
   20ee8:	add	x8, x8, #0x1
   20eec:	subs	x9, x27, x8
   20ef0:	csel	x23, xzr, x9, cc  // cc = lo, ul, last
   20ef4:	lsr	x9, x23, #6
   20ef8:	lsl	x10, x9, #3
   20efc:	ldr	x11, [x22, x10]
   20f00:	lsl	x12, x26, x23
   20f04:	adds	x11, x11, x12
   20f08:	str	x11, [x22, x10]
   20f0c:	b.cc	20e70 <__gmpz_rrandomb@@Base+0xc0>  // b.lo, b.ul, b.last
   20f10:	add	x9, x24, x9, lsl #3
   20f14:	ldr	x10, [x9]
   20f18:	adds	x10, x10, #0x1
   20f1c:	str	x10, [x9], #8
   20f20:	b.cs	20f14 <__gmpz_rrandomb@@Base+0x164>  // b.hs, b.nlast
   20f24:	b	20e70 <__gmpz_rrandomb@@Base+0xc0>
   20f28:	str	w20, [x19, #4]
   20f2c:	ldp	x20, x19, [sp, #80]
   20f30:	ldp	x22, x21, [sp, #64]
   20f34:	ldp	x24, x23, [sp, #48]
   20f38:	ldp	x26, x25, [sp, #32]
   20f3c:	ldr	x27, [sp, #16]
   20f40:	ldp	x29, x30, [sp], #96
   20f44:	ret
   20f48:	mov	x0, x19
   20f4c:	mov	x1, x20
   20f50:	bl	c080 <__gmpz_realloc@plt>
   20f54:	mov	x22, x0
   20f58:	b	20df4 <__gmpz_rrandomb@@Base+0x44>

0000000000020f5c <__gmpz_scan0@@Base>:
   20f5c:	ldrsw	x13, [x0, #4]
   20f60:	lsr	x12, x1, #6
   20f64:	cmp	x13, #0x0
   20f68:	cneg	x10, x13, mi  // mi = first
   20f6c:	cmp	x12, x10
   20f70:	b.ge	20fcc <__gmpz_scan0@@Base+0x70>  // b.tcont
   20f74:	ldr	x8, [x0, #8]
   20f78:	add	x9, x8, x12, lsl #3
   20f7c:	ldr	x11, [x9]
   20f80:	tbnz	w13, #31, 20fd8 <__gmpz_scan0@@Base+0x7c>
   20f84:	mov	x13, #0xffffffffffffffff    	// #-1
   20f88:	lsl	x13, x13, x1
   20f8c:	orn	x11, x11, x13
   20f90:	cmn	x11, #0x1
   20f94:	b.ne	20fc4 <__gmpz_scan0@@Base+0x68>  // b.any
   20f98:	lsl	x11, x10, #3
   20f9c:	sub	x11, x11, x12, lsl #3
   20fa0:	sub	x12, x11, #0x8
   20fa4:	cbz	x12, 21034 <__gmpz_scan0@@Base+0xd8>
   20fa8:	ldr	x11, [x9, #8]
   20fac:	add	x13, x9, #0x8
   20fb0:	sub	x12, x12, #0x8
   20fb4:	mov	x9, x13
   20fb8:	cmn	x11, #0x1
   20fbc:	b.eq	20fa4 <__gmpz_scan0@@Base+0x48>  // b.none
   20fc0:	mov	x9, x13
   20fc4:	mvn	x11, x11
   20fc8:	b	21020 <__gmpz_scan0@@Base+0xc4>
   20fcc:	cmp	w13, #0x0
   20fd0:	csinv	x0, x1, xzr, ge  // ge = tcont
   20fd4:	ret
   20fd8:	add	x10, x8, x10, lsl #3
   20fdc:	sub	x13, x8, #0x8
   20fe0:	lsl	x12, x12, #3
   20fe4:	cbz	x12, 20ff8 <__gmpz_scan0@@Base+0x9c>
   20fe8:	ldr	x14, [x13, x12]
   20fec:	sub	x12, x12, #0x8
   20ff0:	cbz	x14, 20fe4 <__gmpz_scan0@@Base+0x88>
   20ff4:	b	20ffc <__gmpz_scan0@@Base+0xa0>
   20ff8:	sub	x11, x11, #0x1
   20ffc:	mov	x12, #0xffffffffffffffff    	// #-1
   21000:	lsl	x12, x12, x1
   21004:	ands	x11, x11, x12
   21008:	b.ne	21020 <__gmpz_scan0@@Base+0xc4>  // b.any
   2100c:	add	x11, x9, #0x8
   21010:	cmp	x11, x10
   21014:	b.eq	2103c <__gmpz_scan0@@Base+0xe0>  // b.none
   21018:	ldr	x11, [x9, #8]!
   2101c:	cbz	x11, 21018 <__gmpz_scan0@@Base+0xbc>
   21020:	rbit	x10, x11
   21024:	clz	x10, x10
   21028:	sub	x8, x9, x8
   2102c:	add	x0, x10, x8, lsl #3
   21030:	ret
   21034:	lsl	x0, x10, #6
   21038:	ret
   2103c:	mov	x0, #0xffffffffffffffff    	// #-1
   21040:	ret

0000000000021044 <__gmpz_scan1@@Base>:
   21044:	ldrsw	x13, [x0, #4]
   21048:	lsr	x11, x1, #6
   2104c:	cmp	x13, #0x0
   21050:	cneg	x10, x13, mi  // mi = first
   21054:	cmp	x11, x10
   21058:	b.ge	21098 <__gmpz_scan1@@Base+0x54>  // b.tcont
   2105c:	ldr	x8, [x0, #8]
   21060:	add	x9, x8, x11, lsl #3
   21064:	cbz	x1, 21110 <__gmpz_scan1@@Base+0xcc>
   21068:	ldr	x12, [x9]
   2106c:	tbnz	w13, #31, 210a4 <__gmpz_scan1@@Base+0x60>
   21070:	mov	x11, #0xffffffffffffffff    	// #-1
   21074:	lsl	x11, x11, x1
   21078:	ands	x11, x12, x11
   2107c:	b.ne	21118 <__gmpz_scan1@@Base+0xd4>  // b.any
   21080:	add	x10, x8, x10, lsl #3
   21084:	sub	x10, x10, #0x8
   21088:	cmp	x9, x10
   2108c:	b.ne	2110c <__gmpz_scan1@@Base+0xc8>  // b.any
   21090:	mov	x0, #0xffffffffffffffff    	// #-1
   21094:	ret
   21098:	cmp	w13, #0x0
   2109c:	csinv	x0, x1, xzr, lt  // lt = tstop
   210a0:	ret
   210a4:	cbz	x11, 210c0 <__gmpz_scan1@@Base+0x7c>
   210a8:	lsl	x13, x11, #3
   210ac:	sub	x14, x8, #0x8
   210b0:	ldr	x15, [x14, x13]
   210b4:	cbnz	x15, 210c8 <__gmpz_scan1@@Base+0x84>
   210b8:	subs	x13, x13, #0x8
   210bc:	b.ne	210b0 <__gmpz_scan1@@Base+0x6c>  // b.any
   210c0:	cbz	x12, 2110c <__gmpz_scan1@@Base+0xc8>
   210c4:	sub	x12, x12, #0x1
   210c8:	mov	x13, #0xffffffffffffffff    	// #-1
   210cc:	lsl	x13, x13, x1
   210d0:	orn	x12, x12, x13
   210d4:	cmn	x12, #0x1
   210d8:	b.ne	210fc <__gmpz_scan1@@Base+0xb8>  // b.any
   210dc:	lsl	x12, x10, #3
   210e0:	sub	x11, x12, x11, lsl #3
   210e4:	sub	x11, x11, #0x8
   210e8:	cbz	x11, 21104 <__gmpz_scan1@@Base+0xc0>
   210ec:	ldr	x12, [x9, #8]!
   210f0:	sub	x11, x11, #0x8
   210f4:	cmn	x12, #0x1
   210f8:	b.eq	210e8 <__gmpz_scan1@@Base+0xa4>  // b.none
   210fc:	mvn	x11, x12
   21100:	b	21118 <__gmpz_scan1@@Base+0xd4>
   21104:	lsl	x0, x10, #6
   21108:	ret
   2110c:	add	x9, x9, #0x8
   21110:	ldr	x11, [x9]
   21114:	cbz	x11, 2110c <__gmpz_scan1@@Base+0xc8>
   21118:	rbit	x10, x11
   2111c:	clz	x10, x10
   21120:	sub	x8, x9, x8
   21124:	add	x0, x10, x8, lsl #3
   21128:	ret

000000000002112c <__gmpz_set@@Base>:
   2112c:	stp	x29, x30, [sp, #-48]!
   21130:	stp	x22, x21, [sp, #16]
   21134:	stp	x20, x19, [sp, #32]
   21138:	ldrsw	x22, [x1, #4]
   2113c:	ldrsw	x8, [x0]
   21140:	mov	x20, x1
   21144:	mov	x19, x0
   21148:	cmp	x22, #0x0
   2114c:	cneg	x21, x22, mi  // mi = first
   21150:	cmp	x21, x8
   21154:	mov	x29, sp
   21158:	b.gt	21180 <__gmpz_set@@Base+0x54>
   2115c:	ldr	x0, [x19, #8]
   21160:	ldr	x1, [x20, #8]
   21164:	mov	x2, x21
   21168:	bl	ca50 <__gmpn_copyi@plt>
   2116c:	str	w22, [x19, #4]
   21170:	ldp	x20, x19, [sp, #32]
   21174:	ldp	x22, x21, [sp, #16]
   21178:	ldp	x29, x30, [sp], #48
   2117c:	ret
   21180:	mov	x0, x19
   21184:	mov	x1, x21
   21188:	bl	c080 <__gmpz_realloc@plt>
   2118c:	b	21160 <__gmpz_set@@Base+0x34>

0000000000021190 <__gmpz_set_d@@Base>:
   21190:	sub	sp, sp, #0x50
   21194:	str	d8, [sp, #16]
   21198:	mov	v8.16b, v0.16b
   2119c:	fmov	x8, d8
   211a0:	mvn	x8, x8
   211a4:	tst	x8, #0x7ff0000000000000
   211a8:	stp	x29, x30, [sp, #32]
   211ac:	stp	x22, x21, [sp, #48]
   211b0:	stp	x20, x19, [sp, #64]
   211b4:	add	x29, sp, #0x10
   211b8:	b.eq	21270 <__gmpz_set_d@@Base+0xe0>  // b.none
   211bc:	fneg	d0, d8
   211c0:	fcmp	d8, #0.0
   211c4:	mov	x19, x0
   211c8:	fcsel	d0, d8, d0, ge  // ge = tcont
   211cc:	mov	x0, sp
   211d0:	bl	d280 <__gmp_extract_double@plt>
   211d4:	ldrsw	x8, [x19]
   211d8:	sxtw	x9, w0
   211dc:	bic	x20, x9, x9, asr #63
   211e0:	cmp	x20, x8
   211e4:	b.gt	21258 <__gmpz_set_d@@Base+0xc8>
   211e8:	ldr	x21, [x19, #8]
   211ec:	cbz	x20, 21230 <__gmpz_set_d@@Base+0xa0>
   211f0:	cmp	x20, #0x1
   211f4:	b.eq	21228 <__gmpz_set_d@@Base+0x98>  // b.none
   211f8:	cmp	x20, #0x2
   211fc:	b.eq	2121c <__gmpz_set_d@@Base+0x8c>  // b.none
   21200:	lsl	x8, x20, #3
   21204:	sub	x22, x8, #0x10
   21208:	mov	x0, x21
   2120c:	mov	w1, wzr
   21210:	mov	x2, x22
   21214:	bl	c5f0 <memset@plt>
   21218:	add	x21, x21, x22
   2121c:	ldr	q0, [sp]
   21220:	str	q0, [x21]
   21224:	b	21230 <__gmpz_set_d@@Base+0xa0>
   21228:	ldr	x8, [sp, #8]
   2122c:	str	x8, [x21]
   21230:	neg	w8, w20
   21234:	fcmp	d8, #0.0
   21238:	csel	x8, x8, x20, mi  // mi = first
   2123c:	str	w8, [x19, #4]
   21240:	ldp	x20, x19, [sp, #64]
   21244:	ldp	x22, x21, [sp, #48]
   21248:	ldp	x29, x30, [sp, #32]
   2124c:	ldr	d8, [sp, #16]
   21250:	add	sp, sp, #0x50
   21254:	ret
   21258:	mov	x0, x19
   2125c:	mov	x1, x20
   21260:	bl	c080 <__gmpz_realloc@plt>
   21264:	mov	x21, x0
   21268:	cbnz	x20, 211f0 <__gmpz_set_d@@Base+0x60>
   2126c:	b	21230 <__gmpz_set_d@@Base+0xa0>
   21270:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000021274 <__gmpz_set_f@@Base>:
   21274:	stp	x29, x30, [sp, #-64]!
   21278:	stp	x24, x23, [sp, #16]
   2127c:	stp	x22, x21, [sp, #32]
   21280:	stp	x20, x19, [sp, #48]
   21284:	ldr	x19, [x1, #8]
   21288:	mov	x22, x0
   2128c:	mov	x29, sp
   21290:	cmp	x19, #0x0
   21294:	b.le	212f4 <__gmpz_set_f@@Base+0x80>
   21298:	ldrsw	x8, [x22]
   2129c:	mov	x21, x1
   212a0:	cmp	x19, x8
   212a4:	b.gt	21334 <__gmpz_set_f@@Base+0xc0>
   212a8:	ldr	x20, [x22, #8]
   212ac:	ldrsw	x8, [x21, #4]
   212b0:	ldr	x21, [x21, #16]
   212b4:	neg	w9, w19
   212b8:	cmp	w8, #0x0
   212bc:	csel	x9, x19, x9, ge  // ge = tcont
   212c0:	cmp	x8, #0x0
   212c4:	cneg	x23, x8, mi  // mi = first
   212c8:	subs	x24, x19, x23
   212cc:	str	w9, [x22, #4]
   212d0:	b.le	2130c <__gmpz_set_f@@Base+0x98>
   212d4:	b.eq	212e8 <__gmpz_set_f@@Base+0x74>  // b.none
   212d8:	lsl	x2, x24, #3
   212dc:	mov	x0, x20
   212e0:	mov	w1, wzr
   212e4:	bl	c5f0 <memset@plt>
   212e8:	add	x20, x20, x24, lsl #3
   212ec:	mov	x19, x23
   212f0:	b	21314 <__gmpz_set_f@@Base+0xa0>
   212f4:	str	wzr, [x22, #4]
   212f8:	ldp	x20, x19, [sp, #48]
   212fc:	ldp	x22, x21, [sp, #32]
   21300:	ldp	x24, x23, [sp, #16]
   21304:	ldp	x29, x30, [sp], #64
   21308:	ret
   2130c:	sub	x8, x23, x19
   21310:	add	x21, x21, x8, lsl #3
   21314:	mov	x0, x20
   21318:	mov	x1, x21
   2131c:	mov	x2, x19
   21320:	ldp	x20, x19, [sp, #48]
   21324:	ldp	x22, x21, [sp, #32]
   21328:	ldp	x24, x23, [sp, #16]
   2132c:	ldp	x29, x30, [sp], #64
   21330:	b	ca50 <__gmpn_copyi@plt>
   21334:	mov	x0, x22
   21338:	mov	x1, x19
   2133c:	bl	c080 <__gmpz_realloc@plt>
   21340:	mov	x20, x0
   21344:	b	212ac <__gmpz_set_f@@Base+0x38>

0000000000021348 <__gmpz_set_q@@Base>:
   21348:	add	x2, x1, #0x10
   2134c:	b	c040 <__gmpz_tdiv_q@plt>

0000000000021350 <__gmpz_set_si@@Base>:
   21350:	stp	x29, x30, [sp, #-48]!
   21354:	stp	x20, x19, [sp, #32]
   21358:	ldr	w8, [x0]
   2135c:	cmp	x1, #0x0
   21360:	str	x21, [sp, #16]
   21364:	mov	x19, x0
   21368:	mov	x20, x1
   2136c:	cneg	x21, x1, mi  // mi = first
   21370:	cmp	w8, #0x0
   21374:	mov	x29, sp
   21378:	b.le	213a8 <__gmpz_set_si@@Base+0x58>
   2137c:	ldr	x0, [x19, #8]
   21380:	cmp	x20, #0x0
   21384:	cset	w8, ne  // ne = any
   21388:	csetm	w9, ne  // ne = any
   2138c:	csel	w8, w8, w9, ge  // ge = tcont
   21390:	str	x21, [x0]
   21394:	str	w8, [x19, #4]
   21398:	ldp	x20, x19, [sp, #32]
   2139c:	ldr	x21, [sp, #16]
   213a0:	ldp	x29, x30, [sp], #48
   213a4:	ret
   213a8:	mov	w1, #0x1                   	// #1
   213ac:	mov	x0, x19
   213b0:	bl	c080 <__gmpz_realloc@plt>
   213b4:	b	21380 <__gmpz_set_si@@Base+0x30>

00000000000213b8 <__gmpz_set_str@@Base>:
   213b8:	stp	x29, x30, [sp, #-96]!
   213bc:	str	x27, [sp, #16]
   213c0:	stp	x26, x25, [sp, #32]
   213c4:	stp	x24, x23, [sp, #48]
   213c8:	stp	x22, x21, [sp, #64]
   213cc:	stp	x20, x19, [sp, #80]
   213d0:	adrp	x26, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   213d4:	ldr	x26, [x26, #3920]
   213d8:	mov	w20, w2
   213dc:	mov	x21, x1
   213e0:	mov	x19, x0
   213e4:	cmp	w2, #0x25
   213e8:	mov	x29, sp
   213ec:	b.lt	213fc <__gmpz_set_str@@Base+0x44>  // b.tstop
   213f0:	cmp	w20, #0x3e
   213f4:	b.gt	21538 <__gmpz_set_str@@Base+0x180>
   213f8:	add	x26, x26, #0xd0
   213fc:	bl	cae0 <__ctype_b_loc@plt>
   21400:	ldr	x8, [x0]
   21404:	mov	x22, x0
   21408:	ldrb	w27, [x21], #1
   2140c:	ldrh	w9, [x8, x27, lsl #1]
   21410:	tbnz	w9, #13, 21408 <__gmpz_set_str@@Base+0x50>
   21414:	cmp	w27, #0x2d
   21418:	b.ne	21428 <__gmpz_set_str@@Base+0x70>  // b.any
   2141c:	ldrb	w27, [x21], #1
   21420:	mov	w25, #0x1                   	// #1
   21424:	b	2142c <__gmpz_set_str@@Base+0x74>
   21428:	mov	w25, wzr
   2142c:	ldrb	w9, [x26, w27, uxtw]
   21430:	cmp	w20, #0x0
   21434:	mov	w10, #0xa                   	// #10
   21438:	csel	w10, w10, w20, eq  // eq = none
   2143c:	cmp	w10, w9
   21440:	b.le	21538 <__gmpz_set_str@@Base+0x180>
   21444:	cbnz	w20, 214a4 <__gmpz_set_str@@Base+0xec>
   21448:	cmp	w27, #0x30
   2144c:	b.ne	21474 <__gmpz_set_str@@Base+0xbc>  // b.any
   21450:	mov	x9, x21
   21454:	ldrb	w27, [x9], #1
   21458:	orr	w10, w27, #0x20
   2145c:	cmp	w10, #0x78
   21460:	b.ne	2147c <__gmpz_set_str@@Base+0xc4>  // b.any
   21464:	ldrb	w27, [x21, #1]
   21468:	add	x21, x21, #0x2
   2146c:	mov	w20, #0x10                  	// #16
   21470:	b	214a4 <__gmpz_set_str@@Base+0xec>
   21474:	mov	w20, #0xa                   	// #10
   21478:	b	214a4 <__gmpz_set_str@@Base+0xec>
   2147c:	cmp	w10, #0x62
   21480:	b.ne	21494 <__gmpz_set_str@@Base+0xdc>  // b.any
   21484:	ldrb	w27, [x21, #1]
   21488:	add	x21, x21, #0x2
   2148c:	mov	w20, #0x2                   	// #2
   21490:	b	214a4 <__gmpz_set_str@@Base+0xec>
   21494:	mov	w20, #0x8                   	// #8
   21498:	mov	x21, x9
   2149c:	b	214a4 <__gmpz_set_str@@Base+0xec>
   214a0:	ldrb	w27, [x21], #1
   214a4:	cmp	w27, #0x30
   214a8:	b.eq	214a0 <__gmpz_set_str@@Base+0xe8>  // b.none
   214ac:	ldrh	w9, [x8, w27, uxtw #1]
   214b0:	tbnz	w9, #13, 214a0 <__gmpz_set_str@@Base+0xe8>
   214b4:	cbz	w27, 215b0 <__gmpz_set_str@@Base+0x1f8>
   214b8:	sub	x0, x21, #0x1
   214bc:	str	xzr, [x29, #24]
   214c0:	bl	bf60 <strlen@plt>
   214c4:	add	x1, x0, #0x1
   214c8:	mov	w8, #0x7f01                	// #32513
   214cc:	mov	x24, x0
   214d0:	cmp	x1, x8
   214d4:	b.cs	215d8 <__gmpz_set_str@@Base+0x220>  // b.hs, b.nlast
   214d8:	add	x9, x1, #0xf
   214dc:	mov	x8, sp
   214e0:	and	x9, x9, #0xfffffffffffffff0
   214e4:	sub	x23, x8, x9
   214e8:	mov	sp, x23
   214ec:	mov	x8, x23
   214f0:	cbz	x24, 21540 <__gmpz_set_str@@Base+0x188>
   214f4:	mov	x9, xzr
   214f8:	mov	x8, x23
   214fc:	b	21514 <__gmpz_set_str@@Base+0x15c>
   21500:	strb	w10, [x8], #1
   21504:	ldrb	w27, [x21, x9]
   21508:	add	x9, x9, #0x1
   2150c:	cmp	x24, x9
   21510:	b.eq	21540 <__gmpz_set_str@@Base+0x188>  // b.none
   21514:	ldr	x10, [x22]
   21518:	ldrh	w10, [x10, w27, uxtw #1]
   2151c:	tbnz	w10, #13, 21504 <__gmpz_set_str@@Base+0x14c>
   21520:	mov	w10, w27
   21524:	ldrb	w10, [x26, x10]
   21528:	cmp	w20, w10
   2152c:	b.gt	21500 <__gmpz_set_str@@Base+0x148>
   21530:	ldr	x0, [x29, #24]
   21534:	cbnz	x0, 215f4 <__gmpz_set_str@@Base+0x23c>
   21538:	mov	w0, #0xffffffff            	// #-1
   2153c:	b	215b8 <__gmpz_set_str@@Base+0x200>
   21540:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   21544:	ldr	x9, [x9, #3936]
   21548:	mov	w10, #0x28                  	// #40
   2154c:	sub	x21, x8, x23
   21550:	ldrsw	x8, [x19]
   21554:	smaddl	x9, w20, w10, x9
   21558:	ldr	x9, [x9, #16]
   2155c:	umulh	x9, x9, x21
   21560:	ubfx	x9, x9, #3, #58
   21564:	add	x1, x9, #0x2
   21568:	cmp	x1, x8
   2156c:	b.gt	215e8 <__gmpz_set_str@@Base+0x230>
   21570:	ldr	x0, [x19, #8]
   21574:	mov	x1, x23
   21578:	mov	x2, x21
   2157c:	mov	w3, w20
   21580:	bl	c090 <__gmpn_set_str@plt>
   21584:	neg	w8, w0
   21588:	cmp	w25, #0x0
   2158c:	csel	x8, x0, x8, eq  // eq = none
   21590:	str	w8, [x19, #4]
   21594:	ldr	x8, [x29, #24]
   21598:	mov	w0, wzr
   2159c:	cbz	x8, 215b8 <__gmpz_set_str@@Base+0x200>
   215a0:	mov	x0, x8
   215a4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   215a8:	mov	w0, wzr
   215ac:	b	215b8 <__gmpz_set_str@@Base+0x200>
   215b0:	mov	w0, wzr
   215b4:	str	wzr, [x19, #4]
   215b8:	mov	sp, x29
   215bc:	ldp	x20, x19, [sp, #80]
   215c0:	ldp	x22, x21, [sp, #64]
   215c4:	ldp	x24, x23, [sp, #48]
   215c8:	ldp	x26, x25, [sp, #32]
   215cc:	ldr	x27, [sp, #16]
   215d0:	ldp	x29, x30, [sp], #96
   215d4:	ret
   215d8:	add	x0, x29, #0x18
   215dc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   215e0:	mov	x23, x0
   215e4:	b	214f4 <__gmpz_set_str@@Base+0x13c>
   215e8:	mov	x0, x19
   215ec:	bl	c080 <__gmpz_realloc@plt>
   215f0:	b	21570 <__gmpz_set_str@@Base+0x1b8>
   215f4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   215f8:	mov	w0, #0xffffffff            	// #-1
   215fc:	b	215b8 <__gmpz_set_str@@Base+0x200>

0000000000021600 <__gmpz_set_ui@@Base>:
   21600:	stp	x29, x30, [sp, #-32]!
   21604:	stp	x20, x19, [sp, #16]
   21608:	ldr	w8, [x0]
   2160c:	mov	x19, x0
   21610:	mov	x20, x1
   21614:	mov	x29, sp
   21618:	cmp	w8, #0x0
   2161c:	b.le	21640 <__gmpz_set_ui@@Base+0x40>
   21620:	ldr	x0, [x19, #8]
   21624:	cmp	x20, #0x0
   21628:	cset	w8, ne  // ne = any
   2162c:	str	x20, [x0]
   21630:	str	w8, [x19, #4]
   21634:	ldp	x20, x19, [sp, #16]
   21638:	ldp	x29, x30, [sp], #32
   2163c:	ret
   21640:	mov	w1, #0x1                   	// #1
   21644:	mov	x0, x19
   21648:	bl	c080 <__gmpz_realloc@plt>
   2164c:	b	21624 <__gmpz_set_ui@@Base+0x24>

0000000000021650 <__gmpz_setbit@@Base>:
   21650:	stp	x29, x30, [sp, #-64]!
   21654:	stp	x24, x23, [sp, #16]
   21658:	stp	x22, x21, [sp, #32]
   2165c:	stp	x20, x19, [sp, #48]
   21660:	ldrsw	x23, [x0, #4]
   21664:	ldr	x20, [x0, #8]
   21668:	mov	w8, #0x1                   	// #1
   2166c:	mov	x19, x0
   21670:	lsr	x22, x1, #6
   21674:	lsl	x24, x8, x1
   21678:	mov	x29, sp
   2167c:	tbnz	w23, #31, 2169c <__gmpz_setbit@@Base+0x4c>
   21680:	cmp	x22, x23
   21684:	b.ge	21708 <__gmpz_setbit@@Base+0xb8>  // b.tcont
   21688:	lsl	x8, x22, #3
   2168c:	ldr	x9, [x20, x8]
   21690:	orr	x9, x9, x24
   21694:	str	x9, [x20, x8]
   21698:	b	21738 <__gmpz_setbit@@Base+0xe8>
   2169c:	neg	x8, x23
   216a0:	cmp	x22, x8
   216a4:	b.ge	21738 <__gmpz_setbit@@Base+0xe8>  // b.tcont
   216a8:	mov	x9, xzr
   216ac:	ldr	x10, [x20, x9, lsl #3]
   216b0:	add	x9, x9, #0x1
   216b4:	cbz	x10, 216ac <__gmpz_setbit@@Base+0x5c>
   216b8:	sub	x10, x9, #0x1
   216bc:	cmp	x22, x10
   216c0:	b.ls	2174c <__gmpz_setbit@@Base+0xfc>  // b.plast
   216c4:	lsl	x9, x22, #3
   216c8:	ldr	x10, [x20, x9]
   216cc:	bics	xzr, x10, x24
   216d0:	bic	x11, x10, x24
   216d4:	cinc	x10, x22, eq  // eq = none
   216d8:	cmp	x10, x8
   216dc:	str	x11, [x20, x9]
   216e0:	b.ne	21738 <__gmpz_setbit@@Base+0xe8>  // b.any
   216e4:	sub	x8, x20, #0x8
   216e8:	subs	x9, x22, #0x1
   216ec:	b.lt	217c0 <__gmpz_setbit@@Base+0x170>  // b.tstop
   216f0:	ldr	x10, [x8, x22, lsl #3]
   216f4:	mov	x22, x9
   216f8:	cbz	x10, 216e8 <__gmpz_setbit@@Base+0x98>
   216fc:	add	x8, x9, #0x1
   21700:	neg	w8, w8
   21704:	b	217a4 <__gmpz_setbit@@Base+0x154>
   21708:	ldrsw	x8, [x19]
   2170c:	add	x21, x22, #0x1
   21710:	cmp	x22, x8
   21714:	b.ge	217ac <__gmpz_setbit@@Base+0x15c>  // b.tcont
   21718:	subs	x8, x22, x23
   2171c:	str	w21, [x19, #4]
   21720:	b.eq	21734 <__gmpz_setbit@@Base+0xe4>  // b.none
   21724:	add	x0, x20, x23, lsl #3
   21728:	lsl	x2, x8, #3
   2172c:	mov	w1, wzr
   21730:	bl	c5f0 <memset@plt>
   21734:	str	x24, [x20, x22, lsl #3]
   21738:	ldp	x20, x19, [sp, #48]
   2173c:	ldp	x22, x21, [sp, #32]
   21740:	ldp	x24, x23, [sp, #16]
   21744:	ldp	x29, x30, [sp], #64
   21748:	ret
   2174c:	ldr	x8, [x20, x22, lsl #3]
   21750:	add	x10, x22, #0x1
   21754:	cmp	x10, x9
   21758:	b.ne	21770 <__gmpz_setbit@@Base+0x120>  // b.any
   2175c:	sub	x8, x8, #0x1
   21760:	bic	x8, x8, x24
   21764:	add	x8, x8, #0x1
   21768:	str	x8, [x20, x22, lsl #3]
   2176c:	b	21738 <__gmpz_setbit@@Base+0xe8>
   21770:	subs	x8, x8, x24
   21774:	str	x8, [x20, x22, lsl #3]
   21778:	b.cs	21794 <__gmpz_setbit@@Base+0x144>  // b.hs, b.nlast
   2177c:	add	x8, x20, x22, lsl #3
   21780:	add	x8, x8, #0x8
   21784:	ldr	x9, [x8]
   21788:	sub	x10, x9, #0x1
   2178c:	str	x10, [x8], #8
   21790:	cbz	x9, 21784 <__gmpz_setbit@@Base+0x134>
   21794:	mvn	x8, x23
   21798:	ldr	x8, [x20, x8, lsl #3]
   2179c:	cmp	x8, #0x0
   217a0:	cinc	w8, w23, eq  // eq = none
   217a4:	str	w8, [x19, #4]
   217a8:	b	21738 <__gmpz_setbit@@Base+0xe8>
   217ac:	mov	x0, x19
   217b0:	mov	x1, x21
   217b4:	bl	c080 <__gmpz_realloc@plt>
   217b8:	mov	x20, x0
   217bc:	b	21718 <__gmpz_setbit@@Base+0xc8>
   217c0:	mov	x8, xzr
   217c4:	neg	w8, w8
   217c8:	b	217a4 <__gmpz_setbit@@Base+0x154>

00000000000217cc <__gmpz_size@@Base>:
   217cc:	ldr	w8, [x0, #4]
   217d0:	cmp	w8, #0x0
   217d4:	cneg	w0, w8, mi  // mi = first
   217d8:	ret

00000000000217dc <__gmpz_sizeinbase@@Base>:
   217dc:	ldr	w8, [x0, #4]
   217e0:	cmp	w8, #0x0
   217e4:	cneg	w8, w8, mi  // mi = first
   217e8:	cbz	w8, 2183c <__gmpz_sizeinbase@@Base+0x60>
   217ec:	ldr	x9, [x0, #8]
   217f0:	sub	w10, w8, #0x1
   217f4:	adrp	x11, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   217f8:	mov	w8, w8
   217fc:	ldr	x9, [x9, w10, sxtw #3]
   21800:	ldr	x11, [x11, #3936]
   21804:	sub	w10, w1, #0x1
   21808:	lsl	x8, x8, #6
   2180c:	clz	x9, x9
   21810:	tst	w1, w10
   21814:	sub	x8, x8, x9
   21818:	sxtw	x9, w1
   2181c:	mov	w10, #0x28                  	// #40
   21820:	madd	x9, x9, x10, x11
   21824:	b.ne	21844 <__gmpz_sizeinbase@@Base+0x68>  // b.any
   21828:	ldrsw	x9, [x9, #24]
   2182c:	add	x8, x8, x9
   21830:	sub	x8, x8, #0x1
   21834:	udiv	x0, x8, x9
   21838:	ret
   2183c:	mov	w0, #0x1                   	// #1
   21840:	ret
   21844:	ldr	x9, [x9, #8]
   21848:	add	x9, x9, #0x1
   2184c:	umulh	x8, x9, x8
   21850:	add	x0, x8, #0x1
   21854:	ret

0000000000021858 <__gmpz_sqrt@@Base>:
   21858:	stp	x29, x30, [sp, #-48]!
   2185c:	stp	x22, x21, [sp, #16]
   21860:	stp	x20, x19, [sp, #32]
   21864:	mov	x29, sp
   21868:	sub	sp, sp, #0x10
   2186c:	ldrsw	x19, [x1, #4]
   21870:	cmp	w19, #0x0
   21874:	b.le	21934 <__gmpz_sqrt@@Base+0xdc>
   21878:	add	x8, x19, #0x1
   2187c:	add	x9, x19, #0x2
   21880:	cmp	x8, #0x0
   21884:	csinc	x8, x9, x19, lt  // lt = tstop
   21888:	asr	x21, x8, #1
   2188c:	str	w21, [x0, #4]
   21890:	ldr	x20, [x1, #8]
   21894:	cmp	x0, x1
   21898:	b.eq	218cc <__gmpz_sqrt@@Base+0x74>  // b.none
   2189c:	ldrsw	x8, [x0]
   218a0:	cmp	x21, x8
   218a4:	b.gt	21940 <__gmpz_sqrt@@Base+0xe8>
   218a8:	ldr	x0, [x0, #8]
   218ac:	mov	x1, xzr
   218b0:	mov	x2, x20
   218b4:	mov	x3, x19
   218b8:	mov	sp, x29
   218bc:	ldp	x20, x19, [sp, #32]
   218c0:	ldp	x22, x21, [sp, #16]
   218c4:	ldp	x29, x30, [sp], #48
   218c8:	b	d3b0 <__gmpn_sqrtrem@plt>
   218cc:	lsl	x1, x21, #3
   218d0:	mov	w8, #0x7f00                	// #32512
   218d4:	cmp	x1, x8
   218d8:	stur	xzr, [x29, #-8]
   218dc:	b.hi	2194c <__gmpz_sqrt@@Base+0xf4>  // b.pmore
   218e0:	add	x9, x1, #0xf
   218e4:	mov	x8, sp
   218e8:	and	x9, x9, #0xfffffffffffffff0
   218ec:	sub	x22, x8, x9
   218f0:	mov	sp, x22
   218f4:	mov	x0, x22
   218f8:	mov	x1, xzr
   218fc:	mov	x2, x20
   21900:	mov	x3, x19
   21904:	bl	d3b0 <__gmpn_sqrtrem@plt>
   21908:	mov	x0, x20
   2190c:	mov	x1, x22
   21910:	mov	x2, x21
   21914:	bl	ca50 <__gmpn_copyi@plt>
   21918:	ldur	x0, [x29, #-8]
   2191c:	cbnz	x0, 2195c <__gmpz_sqrt@@Base+0x104>
   21920:	mov	sp, x29
   21924:	ldp	x20, x19, [sp, #32]
   21928:	ldp	x22, x21, [sp, #16]
   2192c:	ldp	x29, x30, [sp], #48
   21930:	ret
   21934:	cbnz	w19, 21964 <__gmpz_sqrt@@Base+0x10c>
   21938:	str	wzr, [x0, #4]
   2193c:	b	21920 <__gmpz_sqrt@@Base+0xc8>
   21940:	mov	x1, x21
   21944:	bl	c080 <__gmpz_realloc@plt>
   21948:	b	218ac <__gmpz_sqrt@@Base+0x54>
   2194c:	sub	x0, x29, #0x8
   21950:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   21954:	mov	x22, x0
   21958:	b	218f4 <__gmpz_sqrt@@Base+0x9c>
   2195c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   21960:	b	21920 <__gmpz_sqrt@@Base+0xc8>
   21964:	bl	cff0 <__gmp_sqrt_of_negative@plt>

0000000000021968 <__gmpz_sqrtrem@@Base>:
   21968:	stp	x29, x30, [sp, #-80]!
   2196c:	stp	x24, x23, [sp, #32]
   21970:	stp	x22, x21, [sp, #48]
   21974:	stp	x20, x19, [sp, #64]
   21978:	ldrsw	x20, [x2, #4]
   2197c:	str	x25, [sp, #16]
   21980:	mov	x19, x1
   21984:	mov	x25, x0
   21988:	cmp	w20, #0x0
   2198c:	mov	x29, sp
   21990:	b.le	21a74 <__gmpz_sqrtrem@@Base+0x10c>
   21994:	ldr	w8, [x19]
   21998:	mov	x21, x2
   2199c:	cmp	w20, w8
   219a0:	b.gt	21a80 <__gmpz_sqrtrem@@Base+0x118>
   219a4:	ldr	x22, [x19, #8]
   219a8:	add	x8, x20, #0x1
   219ac:	add	x9, x20, #0x2
   219b0:	cmp	x8, #0x0
   219b4:	csinc	x8, x9, x20, lt  // lt = tstop
   219b8:	asr	x24, x8, #1
   219bc:	str	w24, [x25, #4]
   219c0:	ldr	x23, [x21, #8]
   219c4:	cmp	x25, x21
   219c8:	b.eq	219f4 <__gmpz_sqrtrem@@Base+0x8c>  // b.none
   219cc:	ldrsw	x8, [x25]
   219d0:	cmp	x24, x8
   219d4:	b.gt	21a94 <__gmpz_sqrtrem@@Base+0x12c>
   219d8:	ldr	x0, [x25, #8]
   219dc:	mov	x1, x22
   219e0:	mov	x2, x23
   219e4:	mov	x3, x20
   219e8:	bl	d3b0 <__gmpn_sqrtrem@plt>
   219ec:	mov	x20, x0
   219f0:	b	21a54 <__gmpz_sqrtrem@@Base+0xec>
   219f4:	lsl	x1, x24, #3
   219f8:	mov	w8, #0x7f00                	// #32512
   219fc:	cmp	x1, x8
   21a00:	str	xzr, [x29, #24]
   21a04:	b.hi	21aa4 <__gmpz_sqrtrem@@Base+0x13c>  // b.pmore
   21a08:	add	x9, x1, #0xf
   21a0c:	mov	x8, sp
   21a10:	and	x9, x9, #0xfffffffffffffff0
   21a14:	sub	x25, x8, x9
   21a18:	mov	sp, x25
   21a1c:	mov	x0, x25
   21a20:	mov	x1, x22
   21a24:	mov	x2, x23
   21a28:	mov	x3, x20
   21a2c:	bl	d3b0 <__gmpn_sqrtrem@plt>
   21a30:	cmp	x19, x21
   21a34:	mov	x20, x0
   21a38:	b.eq	21a4c <__gmpz_sqrtrem@@Base+0xe4>  // b.none
   21a3c:	mov	x0, x23
   21a40:	mov	x1, x25
   21a44:	mov	x2, x24
   21a48:	bl	ca50 <__gmpn_copyi@plt>
   21a4c:	ldr	x0, [x29, #24]
   21a50:	cbnz	x0, 21ab4 <__gmpz_sqrtrem@@Base+0x14c>
   21a54:	str	w20, [x19, #4]
   21a58:	mov	sp, x29
   21a5c:	ldp	x20, x19, [sp, #64]
   21a60:	ldp	x22, x21, [sp, #48]
   21a64:	ldp	x24, x23, [sp, #32]
   21a68:	ldr	x25, [sp, #16]
   21a6c:	ldp	x29, x30, [sp], #80
   21a70:	ret
   21a74:	cbnz	w20, 21abc <__gmpz_sqrtrem@@Base+0x154>
   21a78:	str	wzr, [x25, #4]
   21a7c:	b	21a54 <__gmpz_sqrtrem@@Base+0xec>
   21a80:	mov	x0, x19
   21a84:	mov	x1, x20
   21a88:	bl	c080 <__gmpz_realloc@plt>
   21a8c:	mov	x22, x0
   21a90:	b	219a8 <__gmpz_sqrtrem@@Base+0x40>
   21a94:	mov	x0, x25
   21a98:	mov	x1, x24
   21a9c:	bl	c080 <__gmpz_realloc@plt>
   21aa0:	b	219dc <__gmpz_sqrtrem@@Base+0x74>
   21aa4:	add	x0, x29, #0x18
   21aa8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   21aac:	mov	x25, x0
   21ab0:	b	21a1c <__gmpz_sqrtrem@@Base+0xb4>
   21ab4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   21ab8:	b	21a54 <__gmpz_sqrtrem@@Base+0xec>
   21abc:	bl	cff0 <__gmp_sqrt_of_negative@plt>

0000000000021ac0 <__gmpz_stronglucas@@Base>:
   21ac0:	sub	sp, sp, #0x70
   21ac4:	stp	x29, x30, [sp, #48]
   21ac8:	stp	x24, x23, [sp, #64]
   21acc:	stp	x22, x21, [sp, #80]
   21ad0:	stp	x20, x19, [sp, #96]
   21ad4:	ldr	w8, [x0, #4]
   21ad8:	mov	x19, x1
   21adc:	ldr	x1, [x0, #8]
   21ae0:	add	x29, sp, #0x30
   21ae4:	cmp	w8, #0x0
   21ae8:	mov	x20, x2
   21aec:	cneg	w2, w8, mi  // mi = first
   21af0:	sub	x0, x29, #0x10
   21af4:	bl	cc90 <__gmpz_roinit_n@plt>
   21af8:	ldur	w22, [x29, #-12]
   21afc:	ldur	x21, [x29, #-8]
   21b00:	sxtw	x24, w22
   21b04:	mov	x0, x21
   21b08:	mov	x1, x24
   21b0c:	bl	cf60 <__gmpn_mod_34lsub1@plt>
   21b10:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   21b14:	movk	x8, #0xcccd
   21b18:	umulh	x8, x0, x8
   21b1c:	ubfx	x8, x8, #2, #30
   21b20:	mov	x23, x0
   21b24:	add	w8, w8, w8, lsl #2
   21b28:	sub	w8, w23, w8
   21b2c:	tbnz	w8, #1, 21bd8 <__gmpz_stronglucas@@Base+0x118>
   21b30:	mov	x8, #0x2493                	// #9363
   21b34:	movk	x8, #0x9249, lsl #16
   21b38:	movk	x8, #0x4924, lsl #32
   21b3c:	movk	x8, #0x2492, lsl #48
   21b40:	umulh	x8, x23, x8
   21b44:	sub	x9, x23, x8
   21b48:	add	x8, x8, x9, lsr #1
   21b4c:	lsr	x8, x8, #2
   21b50:	sub	x8, x8, x8, lsl #3
   21b54:	add	x8, x23, x8
   21b58:	sub	x9, x8, #0x1
   21b5c:	tst	x8, x9
   21b60:	b.ne	21bec <__gmpz_stronglucas@@Base+0x12c>  // b.any
   21b64:	sub	x0, x29, #0x10
   21b68:	mov	w1, #0xb                   	// #11
   21b6c:	mov	w24, #0xb                   	// #11
   21b70:	bl	cb30 <__gmpz_kronecker_ui@plt>
   21b74:	cmn	w0, #0x1
   21b78:	b.eq	21bf0 <__gmpz_stronglucas@@Base+0x130>  // b.none
   21b7c:	mov	x8, #0x4ec5                	// #20165
   21b80:	movk	x8, #0xc4ec, lsl #16
   21b84:	movk	x8, #0xec4e, lsl #32
   21b88:	movk	x8, #0x4ec4, lsl #48
   21b8c:	umulh	x8, x23, x8
   21b90:	lsr	x8, x8, #2
   21b94:	mov	w24, #0xd                   	// #13
   21b98:	msub	w8, w8, w24, w23
   21b9c:	sub	w8, w8, w8, lsr #3
   21ba0:	and	x8, x8, #0x7
   21ba4:	cmp	x8, #0x4
   21ba8:	b.hi	21bf0 <__gmpz_stronglucas@@Base+0x130>  // b.pmore
   21bac:	cmp	x8, #0x2
   21bb0:	b.eq	21bf0 <__gmpz_stronglucas@@Base+0x130>  // b.none
   21bb4:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   21bb8:	movk	x8, #0xaaab
   21bbc:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   21bc0:	madd	x8, x23, x8, x9
   21bc4:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   21bc8:	cmp	x8, x9
   21bcc:	b.cs	21d20 <__gmpz_stronglucas@@Base+0x260>  // b.hs, b.nlast
   21bd0:	mov	w24, #0xf                   	// #15
   21bd4:	b	21bf0 <__gmpz_stronglucas@@Base+0x130>
   21bd8:	ldr	x2, [x19, #8]
   21bdc:	mov	x0, x21
   21be0:	mov	x1, x24
   21be4:	bl	d120 <__gmpn_strongfibo@plt>
   21be8:	b	21d08 <__gmpz_stronglucas@@Base+0x248>
   21bec:	mov	w24, #0x7                   	// #7
   21bf0:	lsr	x8, x24, #2
   21bf4:	neg	x9, x8
   21bf8:	tst	x24, #0x2
   21bfc:	sub	x0, x29, #0x10
   21c00:	mov	x1, xzr
   21c04:	csinc	x22, x9, x8, eq  // eq = none
   21c08:	bl	c9a0 <__gmpz_scan0@plt>
   21c0c:	mov	x21, x0
   21c10:	add	x0, sp, #0x10
   21c14:	bl	d250 <__gmpz_init@plt>
   21c18:	mov	x0, sp
   21c1c:	bl	d250 <__gmpz_init@plt>
   21c20:	sub	x4, x29, #0x10
   21c24:	add	x5, sp, #0x10
   21c28:	mov	x6, sp
   21c2c:	mov	x0, x19
   21c30:	mov	x1, x20
   21c34:	mov	x2, x22
   21c38:	mov	x3, x21
   21c3c:	bl	c520 <__gmpz_lucas_mod@plt>
   21c40:	cbnz	w0, 21cf0 <__gmpz_stronglucas@@Base+0x230>
   21c44:	subs	x22, x21, #0x1
   21c48:	b.eq	21e3c <__gmpz_stronglucas@@Base+0x37c>  // b.none
   21c4c:	mov	x0, sp
   21c50:	mov	x1, x19
   21c54:	mov	x2, x19
   21c58:	bl	c4b0 <__gmpz_mul@plt>
   21c5c:	mov	x0, sp
   21c60:	mov	w2, #0x2                   	// #2
   21c64:	mov	x1, x20
   21c68:	bl	c870 <__gmpz_submul_ui@plt>
   21c6c:	mov	x1, sp
   21c70:	sub	x2, x29, #0x10
   21c74:	mov	x0, x19
   21c78:	bl	ca80 <__gmpz_tdiv_r@plt>
   21c7c:	ldr	w8, [x19, #4]
   21c80:	cbz	w8, 21cec <__gmpz_stronglucas@@Base+0x22c>
   21c84:	mov	w21, #0x1                   	// #1
   21c88:	subs	x22, x22, #0x1
   21c8c:	b.eq	21e3c <__gmpz_stronglucas@@Base+0x37c>  // b.none
   21c90:	mov	x0, sp
   21c94:	mov	x1, x20
   21c98:	mov	x2, x20
   21c9c:	bl	c4b0 <__gmpz_mul@plt>
   21ca0:	mov	x1, sp
   21ca4:	sub	x2, x29, #0x10
   21ca8:	mov	x0, x20
   21cac:	bl	ca80 <__gmpz_tdiv_r@plt>
   21cb0:	mov	x0, sp
   21cb4:	mov	x1, x19
   21cb8:	mov	x2, x19
   21cbc:	bl	c4b0 <__gmpz_mul@plt>
   21cc0:	mov	x0, sp
   21cc4:	mov	w2, #0x2                   	// #2
   21cc8:	mov	x1, x20
   21ccc:	bl	c870 <__gmpz_submul_ui@plt>
   21cd0:	mov	x1, sp
   21cd4:	sub	x2, x29, #0x10
   21cd8:	mov	x0, x19
   21cdc:	bl	ca80 <__gmpz_tdiv_r@plt>
   21ce0:	ldr	w8, [x19, #4]
   21ce4:	cbnz	w8, 21c88 <__gmpz_stronglucas@@Base+0x1c8>
   21ce8:	b	21cf0 <__gmpz_stronglucas@@Base+0x230>
   21cec:	mov	w21, #0x1                   	// #1
   21cf0:	add	x0, sp, #0x10
   21cf4:	bl	cb50 <__gmpz_clear@plt>
   21cf8:	mov	x0, sp
   21cfc:	bl	cb50 <__gmpz_clear@plt>
   21d00:	cmp	x21, #0x0
   21d04:	cset	w0, ne  // ne = any
   21d08:	ldp	x20, x19, [sp, #96]
   21d0c:	ldp	x22, x21, [sp, #80]
   21d10:	ldp	x24, x23, [sp, #64]
   21d14:	ldp	x29, x30, [sp, #48]
   21d18:	add	sp, sp, #0x70
   21d1c:	ret
   21d20:	mov	x8, #0xf0f0f0f0f0f0f0f0    	// #-1085102592571150096
   21d24:	movk	x8, #0xf0f1
   21d28:	umulh	x8, x23, x8
   21d2c:	lsr	x8, x8, #4
   21d30:	add	x8, x8, x8, lsl #4
   21d34:	sub	x8, x23, x8
   21d38:	sub	x9, x8, #0x1
   21d3c:	tst	x8, x9
   21d40:	b.eq	21d5c <__gmpz_stronglucas@@Base+0x29c>  // b.none
   21d44:	mov	w24, #0x11                  	// #17
   21d48:	mov	w9, #0x10                  	// #16
   21d4c:	sub	x10, x24, x8
   21d50:	sub	x8, x9, x8
   21d54:	tst	x10, x8
   21d58:	b.ne	21bf0 <__gmpz_stronglucas@@Base+0x130>  // b.any
   21d5c:	cmp	w22, #0x1
   21d60:	b.lt	21e4c <__gmpz_stronglucas@@Base+0x38c>  // b.tstop
   21d64:	mov	x0, x21
   21d68:	mov	x1, x22
   21d6c:	bl	d0b0 <__gmpn_perfect_square_p@plt>
   21d70:	cbnz	w0, 21e58 <__gmpz_stronglucas@@Base+0x398>
   21d74:	cmp	w22, #0x2
   21d78:	b.eq	21db0 <__gmpz_stronglucas@@Base+0x2f0>  // b.none
   21d7c:	cmp	w22, #0x1
   21d80:	b.ne	21dd4 <__gmpz_stronglucas@@Base+0x314>  // b.any
   21d84:	ldr	x8, [x21]
   21d88:	mov	w9, #0x40                  	// #64
   21d8c:	mov	w22, #0x1                   	// #1
   21d90:	clz	x10, x8
   21d94:	sub	w9, w9, w10
   21d98:	asr	w9, w9, #1
   21d9c:	lsl	x10, x22, x9
   21da0:	lsr	x8, x8, x9
   21da4:	add	x8, x10, x8
   21da8:	lsr	x24, x8, #1
   21dac:	b	21dd8 <__gmpz_stronglucas@@Base+0x318>
   21db0:	add	x0, sp, #0x10
   21db4:	mov	w3, #0x2                   	// #2
   21db8:	mov	x1, xzr
   21dbc:	mov	x2, x21
   21dc0:	bl	d3b0 <__gmpn_sqrtrem@plt>
   21dc4:	ldr	x24, [sp, #16]
   21dc8:	ldur	x21, [x29, #-8]
   21dcc:	ldur	w22, [x29, #-12]
   21dd0:	b	21ddc <__gmpz_stronglucas@@Base+0x31c>
   21dd4:	mov	x24, #0xffffffffffffffff    	// #-1
   21dd8:	str	x24, [sp, #16]
   21ddc:	sxtw	x23, w22
   21de0:	mov	w22, #0x13                  	// #19
   21de4:	sub	x8, x22, #0x2
   21de8:	cmp	x8, x24
   21dec:	b.cs	21e44 <__gmpz_stronglucas@@Base+0x384>  // b.hs, b.nlast
   21df0:	mov	x0, x21
   21df4:	mov	x1, x23
   21df8:	mov	x2, x22
   21dfc:	cmp	w23, #0x28
   21e00:	b.lt	21e10 <__gmpz_stronglucas@@Base+0x350>  // b.tstop
   21e04:	bl	c3e0 <__gmpn_mod_1@plt>
   21e08:	mov	w2, wzr
   21e0c:	b	21e1c <__gmpz_stronglucas@@Base+0x35c>
   21e10:	mov	x3, xzr
   21e14:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   21e18:	mov	w2, w22
   21e1c:	cbz	x0, 21d08 <__gmpz_stronglucas@@Base+0x248>
   21e20:	mov	x1, x22
   21e24:	bl	c730 <__gmpn_jacobi_base@plt>
   21e28:	cmp	w0, #0x1
   21e2c:	add	x22, x22, #0x2
   21e30:	b.eq	21de4 <__gmpz_stronglucas@@Base+0x324>  // b.none
   21e34:	sub	x24, x22, #0x2
   21e38:	b	21bf0 <__gmpz_stronglucas@@Base+0x130>
   21e3c:	mov	x21, xzr
   21e40:	b	21cf0 <__gmpz_stronglucas@@Base+0x230>
   21e44:	mov	w0, #0x1                   	// #1
   21e48:	b	21d08 <__gmpz_stronglucas@@Base+0x248>
   21e4c:	mvn	w8, w22
   21e50:	lsr	w0, w8, #31
   21e54:	cbz	w0, 21d74 <__gmpz_stronglucas@@Base+0x2b4>
   21e58:	mov	w0, wzr
   21e5c:	b	21d08 <__gmpz_stronglucas@@Base+0x248>

0000000000021e60 <__gmpz_sub@@Base>:
   21e60:	stp	x29, x30, [sp, #-80]!
   21e64:	stp	x26, x25, [sp, #16]
   21e68:	stp	x24, x23, [sp, #32]
   21e6c:	stp	x22, x21, [sp, #48]
   21e70:	stp	x20, x19, [sp, #64]
   21e74:	ldrsw	x8, [x2, #4]
   21e78:	ldrsw	x9, [x1, #4]
   21e7c:	ldrsw	x10, [x0]
   21e80:	mov	x19, x0
   21e84:	neg	x11, x8
   21e88:	cmp	x9, #0x0
   21e8c:	cneg	x12, x9, mi  // mi = first
   21e90:	cmp	x11, #0x0
   21e94:	cneg	x11, x8, pl  // pl = nfrst
   21e98:	cmp	x12, x11
   21e9c:	csel	x20, x11, x12, lt  // lt = tstop
   21ea0:	csel	x23, x12, x11, lt  // lt = tstop
   21ea4:	csneg	x25, x9, x8, lt  // lt = tstop
   21ea8:	csneg	x24, x9, x8, ge  // ge = tcont
   21eac:	csel	x26, x1, x2, lt  // lt = tstop
   21eb0:	csel	x22, x2, x1, lt  // lt = tstop
   21eb4:	cmp	x20, x10
   21eb8:	mov	x29, sp
   21ebc:	b.ge	22168 <__gmpz_sub@@Base+0x308>  // b.tcont
   21ec0:	ldr	x21, [x19, #8]
   21ec4:	ldr	x22, [x22, #8]
   21ec8:	ldr	x2, [x26, #8]
   21ecc:	eor	x8, x24, x25
   21ed0:	tbnz	x8, #63, 21fc4 <__gmpz_sub@@Base+0x164>
   21ed4:	cbz	x23, 21f10 <__gmpz_sub@@Base+0xb0>
   21ed8:	mov	x0, x21
   21edc:	mov	x1, x22
   21ee0:	mov	x3, x23
   21ee4:	bl	ca70 <__gmpn_add_n@plt>
   21ee8:	cbz	x0, 21f10 <__gmpz_sub@@Base+0xb0>
   21eec:	mov	w9, #0x1                   	// #1
   21ef0:	cmp	x23, x20
   21ef4:	b.ge	21fb0 <__gmpz_sub@@Base+0x150>  // b.tcont
   21ef8:	lsl	x8, x23, #3
   21efc:	ldr	x10, [x22, x8]
   21f00:	add	x23, x23, #0x1
   21f04:	adds	x10, x10, #0x1
   21f08:	str	x10, [x21, x8]
   21f0c:	b.cs	21ef0 <__gmpz_sub@@Base+0x90>  // b.hs, b.nlast
   21f10:	cmp	x21, x22
   21f14:	mov	x9, xzr
   21f18:	b.eq	21fb0 <__gmpz_sub@@Base+0x150>  // b.none
   21f1c:	subs	x8, x20, x23
   21f20:	b.le	21fb0 <__gmpz_sub@@Base+0x150>
   21f24:	cmp	x8, #0x4
   21f28:	b.cc	21f8c <__gmpz_sub@@Base+0x12c>  // b.lo, b.ul, b.last
   21f2c:	lsl	x10, x23, #3
   21f30:	lsl	x9, x20, #3
   21f34:	add	x11, x21, x10
   21f38:	add	x12, x22, x9
   21f3c:	cmp	x11, x12
   21f40:	b.cs	21f54 <__gmpz_sub@@Base+0xf4>  // b.hs, b.nlast
   21f44:	add	x9, x21, x9
   21f48:	add	x11, x22, x10
   21f4c:	cmp	x11, x9
   21f50:	b.cc	21f8c <__gmpz_sub@@Base+0x12c>  // b.lo, b.ul, b.last
   21f54:	and	x9, x8, #0xfffffffffffffffc
   21f58:	add	x11, x10, #0x10
   21f5c:	add	x23, x23, x9
   21f60:	add	x10, x22, x11
   21f64:	add	x11, x21, x11
   21f68:	mov	x12, x9
   21f6c:	ldp	q0, q1, [x10, #-16]
   21f70:	add	x10, x10, #0x20
   21f74:	subs	x12, x12, #0x4
   21f78:	stp	q0, q1, [x11, #-16]
   21f7c:	add	x11, x11, #0x20
   21f80:	b.ne	21f6c <__gmpz_sub@@Base+0x10c>  // b.any
   21f84:	cmp	x8, x9
   21f88:	b.eq	21fac <__gmpz_sub@@Base+0x14c>  // b.none
   21f8c:	lsl	x10, x23, #3
   21f90:	sub	x8, x20, x23
   21f94:	add	x9, x21, x10
   21f98:	add	x10, x22, x10
   21f9c:	ldr	x11, [x10], #8
   21fa0:	subs	x8, x8, #0x1
   21fa4:	str	x11, [x9], #8
   21fa8:	b.ne	21f9c <__gmpz_sub@@Base+0x13c>  // b.any
   21fac:	mov	x9, xzr
   21fb0:	add	x8, x9, x20
   21fb4:	cmp	x24, #0x0
   21fb8:	cneg	x8, x8, lt  // lt = tstop
   21fbc:	str	x9, [x21, x20, lsl #3]
   21fc0:	b	22114 <__gmpz_sub@@Base+0x2b4>
   21fc4:	cmp	x20, x23
   21fc8:	b.ne	22024 <__gmpz_sub@@Base+0x1c4>  // b.any
   21fcc:	sub	x8, x20, #0x1
   21fd0:	add	x9, x8, #0x1
   21fd4:	cmp	x9, #0x1
   21fd8:	b.lt	21ff8 <__gmpz_sub@@Base+0x198>  // b.tstop
   21fdc:	lsl	x9, x8, #3
   21fe0:	ldr	x10, [x22, x9]
   21fe4:	ldr	x9, [x2, x9]
   21fe8:	sub	x8, x8, #0x1
   21fec:	cmp	x10, x9
   21ff0:	b.eq	21fd0 <__gmpz_sub@@Base+0x170>  // b.none
   21ff4:	b.ls	22130 <__gmpz_sub@@Base+0x2d0>  // b.plast
   21ff8:	mov	x0, x21
   21ffc:	mov	x1, x22
   22000:	mov	x3, x20
   22004:	bl	c2d0 <__gmpn_sub_n@plt>
   22008:	sub	x8, x21, #0x8
   2200c:	mov	x9, x20
   22010:	subs	x20, x20, #0x1
   22014:	b.lt	2210c <__gmpz_sub@@Base+0x2ac>  // b.tstop
   22018:	ldr	x10, [x8, x9, lsl #3]
   2201c:	cbz	x10, 2200c <__gmpz_sub@@Base+0x1ac>
   22020:	b	2210c <__gmpz_sub@@Base+0x2ac>
   22024:	cbz	x23, 2205c <__gmpz_sub@@Base+0x1fc>
   22028:	mov	x0, x21
   2202c:	mov	x1, x22
   22030:	mov	x3, x23
   22034:	bl	c2d0 <__gmpn_sub_n@plt>
   22038:	cbz	x0, 2205c <__gmpz_sub@@Base+0x1fc>
   2203c:	cmp	x23, x20
   22040:	b.ge	220f4 <__gmpz_sub@@Base+0x294>  // b.tcont
   22044:	lsl	x8, x23, #3
   22048:	ldr	x9, [x22, x8]
   2204c:	add	x23, x23, #0x1
   22050:	sub	x10, x9, #0x1
   22054:	str	x10, [x21, x8]
   22058:	cbz	x9, 2203c <__gmpz_sub@@Base+0x1dc>
   2205c:	cmp	x21, x22
   22060:	b.eq	220f4 <__gmpz_sub@@Base+0x294>  // b.none
   22064:	subs	x8, x20, x23
   22068:	b.le	220f4 <__gmpz_sub@@Base+0x294>
   2206c:	cmp	x8, #0x4
   22070:	b.cc	220d4 <__gmpz_sub@@Base+0x274>  // b.lo, b.ul, b.last
   22074:	lsl	x10, x23, #3
   22078:	lsl	x9, x20, #3
   2207c:	add	x11, x21, x10
   22080:	add	x12, x22, x9
   22084:	cmp	x11, x12
   22088:	b.cs	2209c <__gmpz_sub@@Base+0x23c>  // b.hs, b.nlast
   2208c:	add	x9, x21, x9
   22090:	add	x11, x22, x10
   22094:	cmp	x11, x9
   22098:	b.cc	220d4 <__gmpz_sub@@Base+0x274>  // b.lo, b.ul, b.last
   2209c:	and	x9, x8, #0xfffffffffffffffc
   220a0:	add	x11, x10, #0x10
   220a4:	add	x23, x23, x9
   220a8:	add	x10, x22, x11
   220ac:	add	x11, x21, x11
   220b0:	mov	x12, x9
   220b4:	ldp	q0, q1, [x10, #-16]
   220b8:	add	x10, x10, #0x20
   220bc:	subs	x12, x12, #0x4
   220c0:	stp	q0, q1, [x11, #-16]
   220c4:	add	x11, x11, #0x20
   220c8:	b.ne	220b4 <__gmpz_sub@@Base+0x254>  // b.any
   220cc:	cmp	x8, x9
   220d0:	b.eq	220f4 <__gmpz_sub@@Base+0x294>  // b.none
   220d4:	lsl	x10, x23, #3
   220d8:	sub	x8, x20, x23
   220dc:	add	x9, x21, x10
   220e0:	add	x10, x22, x10
   220e4:	ldr	x11, [x10], #8
   220e8:	subs	x8, x8, #0x1
   220ec:	str	x11, [x9], #8
   220f0:	b.ne	220e4 <__gmpz_sub@@Base+0x284>  // b.any
   220f4:	sub	x8, x21, #0x8
   220f8:	mov	x9, x20
   220fc:	subs	x20, x20, #0x1
   22100:	b.lt	2210c <__gmpz_sub@@Base+0x2ac>  // b.tstop
   22104:	ldr	x10, [x8, x9, lsl #3]
   22108:	cbz	x10, 220f8 <__gmpz_sub@@Base+0x298>
   2210c:	cmp	x24, #0x0
   22110:	cneg	x8, x9, lt  // lt = tstop
   22114:	str	w8, [x19, #4]
   22118:	ldp	x20, x19, [sp, #64]
   2211c:	ldp	x22, x21, [sp, #48]
   22120:	ldp	x24, x23, [sp, #32]
   22124:	ldp	x26, x25, [sp, #16]
   22128:	ldp	x29, x30, [sp], #80
   2212c:	ret
   22130:	mov	x0, x21
   22134:	mov	x1, x2
   22138:	mov	x2, x22
   2213c:	mov	x3, x20
   22140:	bl	c2d0 <__gmpn_sub_n@plt>
   22144:	sub	x8, x21, #0x8
   22148:	mov	x9, x20
   2214c:	subs	x20, x20, #0x1
   22150:	b.lt	2215c <__gmpz_sub@@Base+0x2fc>  // b.tstop
   22154:	ldr	x10, [x8, x9, lsl #3]
   22158:	cbz	x10, 22148 <__gmpz_sub@@Base+0x2e8>
   2215c:	cmp	x24, #0x0
   22160:	cneg	x8, x9, ge  // ge = tcont
   22164:	b	22114 <__gmpz_sub@@Base+0x2b4>
   22168:	add	x1, x20, #0x1
   2216c:	mov	x0, x19
   22170:	bl	c080 <__gmpz_realloc@plt>
   22174:	mov	x21, x0
   22178:	b	21ec4 <__gmpz_sub@@Base+0x64>

000000000002217c <__gmpz_sub_ui@@Base>:
   2217c:	stp	x29, x30, [sp, #-64]!
   22180:	stp	x22, x21, [sp, #32]
   22184:	stp	x20, x19, [sp, #48]
   22188:	str	x23, [sp, #16]
   2218c:	ldrsw	x23, [x1, #4]
   22190:	mov	x20, x2
   22194:	mov	x19, x0
   22198:	mov	x29, sp
   2219c:	cbz	w23, 221e8 <__gmpz_sub_ui@@Base+0x6c>
   221a0:	ldrsw	x8, [x19]
   221a4:	cmp	x23, #0x0
   221a8:	cneg	x22, x23, mi  // mi = first
   221ac:	mov	x21, x1
   221b0:	cmp	x22, x8
   221b4:	b.ge	223a4 <__gmpz_sub_ui@@Base+0x228>  // b.tcont
   221b8:	ldr	x0, [x19, #8]
   221bc:	ldr	x8, [x21, #8]
   221c0:	tbnz	w23, #31, 223b8 <__gmpz_sub_ui@@Base+0x23c>
   221c4:	ldr	x10, [x8]
   221c8:	subs	x9, x22, #0x1
   221cc:	b.ne	22208 <__gmpz_sub_ui@@Base+0x8c>  // b.any
   221d0:	cmp	x10, x20
   221d4:	b.cs	22208 <__gmpz_sub_ui@@Base+0x8c>  // b.hs, b.nlast
   221d8:	sub	x8, x20, x10
   221dc:	str	x8, [x0]
   221e0:	mov	x8, #0xffffffffffffffff    	// #-1
   221e4:	b	2252c <__gmpz_sub_ui@@Base+0x3b0>
   221e8:	ldr	w8, [x19]
   221ec:	cmp	w8, #0x0
   221f0:	b.le	2257c <__gmpz_sub_ui@@Base+0x400>
   221f4:	ldr	x0, [x19, #8]
   221f8:	cmp	x20, #0x0
   221fc:	str	x20, [x0]
   22200:	csetm	w8, ne  // ne = any
   22204:	b	2252c <__gmpz_sub_ui@@Base+0x3b0>
   22208:	subs	x10, x10, x20
   2220c:	str	x10, [x0]
   22210:	b.cs	222f4 <__gmpz_sub_ui@@Base+0x178>  // b.hs, b.nlast
   22214:	mov	x11, xzr
   22218:	mov	w10, #0x1                   	// #1
   2221c:	cmp	x10, x22
   22220:	b.ge	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.tcont
   22224:	add	x12, x8, x11
   22228:	ldr	x12, [x12, #8]
   2222c:	add	x13, x0, x11
   22230:	add	x10, x10, #0x1
   22234:	add	x11, x11, #0x8
   22238:	sub	x14, x12, #0x1
   2223c:	sub	x9, x9, #0x1
   22240:	str	x14, [x13, #8]
   22244:	cbz	x12, 2221c <__gmpz_sub_ui@@Base+0xa0>
   22248:	cmp	x8, x0
   2224c:	b.eq	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.none
   22250:	subs	x12, x22, x10
   22254:	b.le	22354 <__gmpz_sub_ui@@Base+0x1d8>
   22258:	cmp	x12, #0x4
   2225c:	b.cc	222d0 <__gmpz_sub_ui@@Base+0x154>  // b.lo, b.ul, b.last
   22260:	add	x14, x0, x11
   22264:	lsl	x13, x22, #3
   22268:	add	x14, x14, #0x8
   2226c:	add	x15, x8, x13
   22270:	cmp	x14, x15
   22274:	b.cs	2228c <__gmpz_sub_ui@@Base+0x110>  // b.hs, b.nlast
   22278:	add	x14, x8, x11
   2227c:	add	x13, x0, x13
   22280:	add	x14, x14, #0x8
   22284:	cmp	x14, x13
   22288:	b.cc	222d0 <__gmpz_sub_ui@@Base+0x154>  // b.lo, b.ul, b.last
   2228c:	sub	x14, x22, x10
   22290:	add	x13, x0, x11
   22294:	add	x15, x8, x11
   22298:	and	x16, x9, #0xfffffffffffffffc
   2229c:	and	x11, x12, #0xfffffffffffffffc
   222a0:	add	x9, x13, #0x18
   222a4:	add	x13, x15, #0x18
   222a8:	add	x10, x16, x10
   222ac:	and	x14, x14, #0xfffffffffffffffc
   222b0:	ldp	q0, q1, [x13, #-16]
   222b4:	add	x13, x13, #0x20
   222b8:	subs	x14, x14, #0x4
   222bc:	stp	q0, q1, [x9, #-16]
   222c0:	add	x9, x9, #0x20
   222c4:	b.ne	222b0 <__gmpz_sub_ui@@Base+0x134>  // b.any
   222c8:	cmp	x12, x11
   222cc:	b.eq	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.none
   222d0:	lsl	x11, x10, #3
   222d4:	sub	x9, x22, x10
   222d8:	add	x10, x0, x11
   222dc:	add	x8, x8, x11
   222e0:	ldr	x11, [x8], #8
   222e4:	subs	x9, x9, #0x1
   222e8:	str	x11, [x10], #8
   222ec:	b.ne	222e0 <__gmpz_sub_ui@@Base+0x164>  // b.any
   222f0:	b	22354 <__gmpz_sub_ui@@Base+0x1d8>
   222f4:	cmp	x22, #0x2
   222f8:	b.lt	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.tstop
   222fc:	cmp	x8, x0
   22300:	b.eq	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.none
   22304:	cmp	x9, #0x4
   22308:	b.cc	22330 <__gmpz_sub_ui@@Base+0x1b4>  // b.lo, b.ul, b.last
   2230c:	lsl	x10, x22, #3
   22310:	add	x11, x0, #0x8
   22314:	add	x12, x8, x10
   22318:	cmp	x11, x12
   2231c:	b.cs	2236c <__gmpz_sub_ui@@Base+0x1f0>  // b.hs, b.nlast
   22320:	add	x10, x0, x10
   22324:	add	x11, x8, #0x8
   22328:	cmp	x11, x10
   2232c:	b.cs	2236c <__gmpz_sub_ui@@Base+0x1f0>  // b.hs, b.nlast
   22330:	mov	w10, #0x1                   	// #1
   22334:	lsl	x11, x10, #3
   22338:	sub	x9, x22, x10
   2233c:	add	x10, x0, x11
   22340:	add	x8, x8, x11
   22344:	ldr	x11, [x8], #8
   22348:	subs	x9, x9, #0x1
   2234c:	str	x11, [x10], #8
   22350:	b.ne	22344 <__gmpz_sub_ui@@Base+0x1c8>  // b.any
   22354:	add	x8, x0, x22, lsl #3
   22358:	ldur	x8, [x8, #-8]
   2235c:	cmp	x8, #0x0
   22360:	cset	w8, eq  // eq = none
   22364:	sub	x8, x22, x8
   22368:	b	2252c <__gmpz_sub_ui@@Base+0x3b0>
   2236c:	and	x11, x9, #0xfffffffffffffffc
   22370:	add	x12, x8, #0x18
   22374:	orr	x10, x11, #0x1
   22378:	add	x13, x0, #0x18
   2237c:	mov	x14, x11
   22380:	ldp	q0, q1, [x12, #-16]
   22384:	add	x12, x12, #0x20
   22388:	subs	x14, x14, #0x4
   2238c:	stp	q0, q1, [x13, #-16]
   22390:	add	x13, x13, #0x20
   22394:	b.ne	22380 <__gmpz_sub_ui@@Base+0x204>  // b.any
   22398:	cmp	x9, x11
   2239c:	b.eq	22354 <__gmpz_sub_ui@@Base+0x1d8>  // b.none
   223a0:	b	22334 <__gmpz_sub_ui@@Base+0x1b8>
   223a4:	add	x1, x22, #0x1
   223a8:	mov	x0, x19
   223ac:	bl	c080 <__gmpz_realloc@plt>
   223b0:	ldr	x8, [x21, #8]
   223b4:	tbz	w23, #31, 221c4 <__gmpz_sub_ui@@Base+0x48>
   223b8:	ldr	x9, [x8]
   223bc:	adds	x9, x9, x20
   223c0:	str	x9, [x0]
   223c4:	b.cc	224b4 <__gmpz_sub_ui@@Base+0x338>  // b.lo, b.ul, b.last
   223c8:	mov	x11, xzr
   223cc:	sub	x10, x22, #0x1
   223d0:	mov	w12, #0x1                   	// #1
   223d4:	mov	w9, #0x1                   	// #1
   223d8:	cmp	x9, x22
   223dc:	b.ge	22520 <__gmpz_sub_ui@@Base+0x3a4>  // b.tcont
   223e0:	add	x13, x8, x11
   223e4:	ldr	x13, [x13, #8]
   223e8:	add	x14, x0, x11
   223ec:	add	x9, x9, #0x1
   223f0:	add	x11, x11, #0x8
   223f4:	adds	x13, x13, #0x1
   223f8:	sub	x10, x10, #0x1
   223fc:	str	x13, [x14, #8]
   22400:	b.cs	223d8 <__gmpz_sub_ui@@Base+0x25c>  // b.hs, b.nlast
   22404:	cmp	x8, x0
   22408:	mov	x12, xzr
   2240c:	b.eq	22520 <__gmpz_sub_ui@@Base+0x3a4>  // b.none
   22410:	subs	x13, x22, x9
   22414:	b.le	22520 <__gmpz_sub_ui@@Base+0x3a4>
   22418:	cmp	x13, #0x4
   2241c:	b.cc	22490 <__gmpz_sub_ui@@Base+0x314>  // b.lo, b.ul, b.last
   22420:	add	x14, x0, x11
   22424:	lsl	x12, x22, #3
   22428:	add	x14, x14, #0x8
   2242c:	add	x15, x8, x12
   22430:	cmp	x14, x15
   22434:	b.cs	2244c <__gmpz_sub_ui@@Base+0x2d0>  // b.hs, b.nlast
   22438:	add	x14, x8, x11
   2243c:	add	x12, x0, x12
   22440:	add	x14, x14, #0x8
   22444:	cmp	x14, x12
   22448:	b.cc	22490 <__gmpz_sub_ui@@Base+0x314>  // b.lo, b.ul, b.last
   2244c:	sub	x14, x22, x9
   22450:	add	x12, x0, x11
   22454:	add	x15, x8, x11
   22458:	and	x16, x10, #0xfffffffffffffffc
   2245c:	and	x11, x13, #0xfffffffffffffffc
   22460:	add	x10, x12, #0x18
   22464:	add	x12, x15, #0x18
   22468:	add	x9, x16, x9
   2246c:	and	x14, x14, #0xfffffffffffffffc
   22470:	ldp	q0, q1, [x12, #-16]
   22474:	add	x12, x12, #0x20
   22478:	subs	x14, x14, #0x4
   2247c:	stp	q0, q1, [x10, #-16]
   22480:	add	x10, x10, #0x20
   22484:	b.ne	22470 <__gmpz_sub_ui@@Base+0x2f4>  // b.any
   22488:	cmp	x13, x11
   2248c:	b.eq	2251c <__gmpz_sub_ui@@Base+0x3a0>  // b.none
   22490:	lsl	x11, x9, #3
   22494:	sub	x10, x22, x9
   22498:	add	x9, x0, x11
   2249c:	add	x8, x8, x11
   224a0:	ldr	x11, [x8], #8
   224a4:	subs	x10, x10, #0x1
   224a8:	str	x11, [x9], #8
   224ac:	b.ne	224a0 <__gmpz_sub_ui@@Base+0x324>  // b.any
   224b0:	b	2251c <__gmpz_sub_ui@@Base+0x3a0>
   224b4:	cmp	x22, #0x2
   224b8:	mov	x12, xzr
   224bc:	b.lt	22520 <__gmpz_sub_ui@@Base+0x3a4>  // b.tstop
   224c0:	cmp	x8, x0
   224c4:	b.eq	22520 <__gmpz_sub_ui@@Base+0x3a4>  // b.none
   224c8:	sub	x9, x22, #0x1
   224cc:	cmp	x9, #0x4
   224d0:	b.cc	224f8 <__gmpz_sub_ui@@Base+0x37c>  // b.lo, b.ul, b.last
   224d4:	lsl	x10, x22, #3
   224d8:	add	x11, x0, #0x8
   224dc:	add	x12, x8, x10
   224e0:	cmp	x11, x12
   224e4:	b.cs	22544 <__gmpz_sub_ui@@Base+0x3c8>  // b.hs, b.nlast
   224e8:	add	x10, x0, x10
   224ec:	add	x11, x8, #0x8
   224f0:	cmp	x11, x10
   224f4:	b.cs	22544 <__gmpz_sub_ui@@Base+0x3c8>  // b.hs, b.nlast
   224f8:	mov	w10, #0x1                   	// #1
   224fc:	lsl	x11, x10, #3
   22500:	sub	x9, x22, x10
   22504:	add	x10, x0, x11
   22508:	add	x8, x8, x11
   2250c:	ldr	x11, [x8], #8
   22510:	subs	x9, x9, #0x1
   22514:	str	x11, [x10], #8
   22518:	b.ne	2250c <__gmpz_sub_ui@@Base+0x390>  // b.any
   2251c:	mov	x12, xzr
   22520:	add	x8, x22, x12
   22524:	str	x12, [x0, x22, lsl #3]
   22528:	neg	x8, x8
   2252c:	str	w8, [x19, #4]
   22530:	ldp	x20, x19, [sp, #48]
   22534:	ldp	x22, x21, [sp, #32]
   22538:	ldr	x23, [sp, #16]
   2253c:	ldp	x29, x30, [sp], #64
   22540:	ret
   22544:	and	x11, x9, #0xfffffffffffffffc
   22548:	add	x12, x8, #0x18
   2254c:	orr	x10, x11, #0x1
   22550:	add	x13, x0, #0x18
   22554:	mov	x14, x11
   22558:	ldp	q0, q1, [x12, #-16]
   2255c:	add	x12, x12, #0x20
   22560:	subs	x14, x14, #0x4
   22564:	stp	q0, q1, [x13, #-16]
   22568:	add	x13, x13, #0x20
   2256c:	b.ne	22558 <__gmpz_sub_ui@@Base+0x3dc>  // b.any
   22570:	cmp	x9, x11
   22574:	b.eq	2251c <__gmpz_sub_ui@@Base+0x3a0>  // b.none
   22578:	b	224fc <__gmpz_sub_ui@@Base+0x380>
   2257c:	mov	w1, #0x1                   	// #1
   22580:	mov	x0, x19
   22584:	bl	c080 <__gmpz_realloc@plt>
   22588:	b	221f8 <__gmpz_sub_ui@@Base+0x7c>

000000000002258c <__gmpz_swap@@Base>:
   2258c:	ldr	x8, [x1]
   22590:	ldr	x9, [x0]
   22594:	str	x8, [x0]
   22598:	str	x9, [x1]
   2259c:	ldr	x8, [x0, #8]
   225a0:	ldr	x9, [x1, #8]
   225a4:	str	x8, [x1, #8]
   225a8:	str	x9, [x0, #8]
   225ac:	ret

00000000000225b0 <__gmpz_tdiv_ui@@Base>:
   225b0:	stp	x29, x30, [sp, #-16]!
   225b4:	mov	x29, sp
   225b8:	cbz	x1, 225e8 <__gmpz_tdiv_ui@@Base+0x38>
   225bc:	ldrsw	x8, [x0, #4]
   225c0:	cbz	w8, 225dc <__gmpz_tdiv_ui@@Base+0x2c>
   225c4:	ldr	x0, [x0, #8]
   225c8:	cmp	x8, #0x0
   225cc:	mov	x2, x1
   225d0:	cneg	x1, x8, mi  // mi = first
   225d4:	ldp	x29, x30, [sp], #16
   225d8:	b	c3e0 <__gmpn_mod_1@plt>
   225dc:	mov	x0, xzr
   225e0:	ldp	x29, x30, [sp], #16
   225e4:	ret
   225e8:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000225ec <__gmpz_tdiv_q@@Base>:
   225ec:	stp	x29, x30, [sp, #-96]!
   225f0:	stp	x28, x27, [sp, #16]
   225f4:	stp	x26, x25, [sp, #32]
   225f8:	stp	x24, x23, [sp, #48]
   225fc:	stp	x22, x21, [sp, #64]
   22600:	stp	x20, x19, [sp, #80]
   22604:	mov	x29, sp
   22608:	sub	sp, sp, #0x10
   2260c:	ldrsw	x27, [x1, #4]
   22610:	ldrsw	x28, [x2, #4]
   22614:	cmp	x27, #0x0
   22618:	cneg	x20, x27, mi  // mi = first
   2261c:	cmp	x28, #0x0
   22620:	cneg	x21, x28, mi  // mi = first
   22624:	cbz	x21, 22778 <__gmpz_tdiv_q@@Base+0x18c>
   22628:	mov	x19, x0
   2262c:	sub	x22, x20, x21
   22630:	tbnz	x22, #63, 22724 <__gmpz_tdiv_q@@Base+0x138>
   22634:	ldrsw	x8, [x19]
   22638:	mov	x23, x1
   2263c:	mov	x25, x2
   22640:	add	x1, x22, #0x1
   22644:	cmp	x22, x8
   22648:	stur	x1, [x29, #-16]
   2264c:	b.ge	22748 <__gmpz_tdiv_q@@Base+0x15c>  // b.tcont
   22650:	ldr	x24, [x19, #8]
   22654:	stur	xzr, [x29, #-8]
   22658:	ldr	x25, [x25, #8]
   2265c:	cmp	x25, x24
   22660:	b.ne	22694 <__gmpz_tdiv_q@@Base+0xa8>  // b.any
   22664:	cmp	x21, #0xfe0
   22668:	lsl	x1, x21, #3
   2266c:	b.hi	22768 <__gmpz_tdiv_q@@Base+0x17c>  // b.pmore
   22670:	add	x9, x1, #0xf
   22674:	mov	x8, sp
   22678:	and	x9, x9, #0xfffffffffffffff0
   2267c:	sub	x25, x8, x9
   22680:	mov	sp, x25
   22684:	mov	x0, x25
   22688:	mov	x1, x24
   2268c:	mov	x2, x21
   22690:	bl	ca50 <__gmpn_copyi@plt>
   22694:	lsl	x8, x20, #3
   22698:	cmp	x20, #0xfdf
   2269c:	add	x1, x8, #0x8
   226a0:	b.hi	22758 <__gmpz_tdiv_q@@Base+0x16c>  // b.pmore
   226a4:	add	x9, x1, #0xf
   226a8:	mov	x8, sp
   226ac:	and	x9, x9, #0xfffffffffffffff0
   226b0:	sub	x26, x8, x9
   226b4:	mov	sp, x26
   226b8:	ldr	x1, [x23, #8]
   226bc:	cmp	x1, x24
   226c0:	b.ne	226d8 <__gmpz_tdiv_q@@Base+0xec>  // b.any
   226c4:	mov	x0, x26
   226c8:	mov	x1, x24
   226cc:	mov	x2, x20
   226d0:	bl	ca50 <__gmpn_copyi@plt>
   226d4:	mov	x1, x26
   226d8:	mov	x0, x24
   226dc:	mov	x2, x20
   226e0:	mov	x3, x25
   226e4:	mov	x4, x21
   226e8:	mov	x5, x26
   226ec:	bl	c320 <__gmpn_div_q@plt>
   226f0:	ldr	x8, [x24, x22, lsl #3]
   226f4:	ldp	x10, x0, [x29, #-16]
   226f8:	eor	w9, w28, w27
   226fc:	cmp	x8, #0x0
   22700:	cset	w8, eq  // eq = none
   22704:	sub	x8, x10, x8
   22708:	neg	w10, w8
   2270c:	cmp	w9, #0x0
   22710:	csel	x8, x8, x10, ge  // ge = tcont
   22714:	str	w8, [x19, #4]
   22718:	cbz	x0, 22728 <__gmpz_tdiv_q@@Base+0x13c>
   2271c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   22720:	b	22728 <__gmpz_tdiv_q@@Base+0x13c>
   22724:	str	wzr, [x19, #4]
   22728:	mov	sp, x29
   2272c:	ldp	x20, x19, [sp, #80]
   22730:	ldp	x22, x21, [sp, #64]
   22734:	ldp	x24, x23, [sp, #48]
   22738:	ldp	x26, x25, [sp, #32]
   2273c:	ldp	x28, x27, [sp, #16]
   22740:	ldp	x29, x30, [sp], #96
   22744:	ret
   22748:	mov	x0, x19
   2274c:	bl	c080 <__gmpz_realloc@plt>
   22750:	mov	x24, x0
   22754:	b	22654 <__gmpz_tdiv_q@@Base+0x68>
   22758:	sub	x0, x29, #0x8
   2275c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22760:	mov	x26, x0
   22764:	b	226b8 <__gmpz_tdiv_q@@Base+0xcc>
   22768:	sub	x0, x29, #0x8
   2276c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22770:	mov	x25, x0
   22774:	b	22684 <__gmpz_tdiv_q@@Base+0x98>
   22778:	bl	bfd0 <__gmp_divide_by_zero@plt>

000000000002277c <__gmpz_tdiv_q_2exp@@Base>:
   2277c:	stp	x29, x30, [sp, #-80]!
   22780:	stp	x24, x23, [sp, #32]
   22784:	stp	x22, x21, [sp, #48]
   22788:	stp	x20, x19, [sp, #64]
   2278c:	ldrsw	x24, [x1, #4]
   22790:	str	x25, [sp, #16]
   22794:	lsr	x25, x2, #6
   22798:	mov	x19, x0
   2279c:	cmp	x24, #0x0
   227a0:	cneg	x8, x24, mi  // mi = first
   227a4:	sub	x20, x8, x25
   227a8:	cmp	x20, #0x1
   227ac:	mov	x29, sp
   227b0:	b.lt	22800 <__gmpz_tdiv_q_2exp@@Base+0x84>  // b.tstop
   227b4:	ldrsw	x8, [x19]
   227b8:	mov	x21, x2
   227bc:	mov	x22, x1
   227c0:	cmp	x20, x8
   227c4:	b.gt	2283c <__gmpz_tdiv_q_2exp@@Base+0xc0>
   227c8:	ldr	x23, [x19, #8]
   227cc:	ldr	x8, [x22, #8]
   227d0:	ands	x3, x21, #0x3f
   227d4:	add	x1, x8, x25, lsl #3
   227d8:	b.eq	22808 <__gmpz_tdiv_q_2exp@@Base+0x8c>  // b.none
   227dc:	mov	x0, x23
   227e0:	mov	x2, x20
   227e4:	bl	c1a0 <__gmpn_rshift@plt>
   227e8:	add	x8, x23, x20, lsl #3
   227ec:	ldur	x8, [x8, #-8]
   227f0:	cmp	x8, #0x0
   227f4:	cset	w8, eq  // eq = none
   227f8:	sub	x20, x20, x8
   227fc:	b	22814 <__gmpz_tdiv_q_2exp@@Base+0x98>
   22800:	mov	x20, xzr
   22804:	b	22814 <__gmpz_tdiv_q_2exp@@Base+0x98>
   22808:	mov	x0, x23
   2280c:	mov	x2, x20
   22810:	bl	ca50 <__gmpn_copyi@plt>
   22814:	neg	w8, w20
   22818:	cmp	w24, #0x0
   2281c:	csel	x8, x20, x8, ge  // ge = tcont
   22820:	str	w8, [x19, #4]
   22824:	ldp	x20, x19, [sp, #64]
   22828:	ldp	x22, x21, [sp, #48]
   2282c:	ldp	x24, x23, [sp, #32]
   22830:	ldr	x25, [sp, #16]
   22834:	ldp	x29, x30, [sp], #80
   22838:	ret
   2283c:	mov	x0, x19
   22840:	mov	x1, x20
   22844:	bl	c080 <__gmpz_realloc@plt>
   22848:	mov	x23, x0
   2284c:	b	227cc <__gmpz_tdiv_q_2exp@@Base+0x50>

0000000000022850 <__gmpz_tdiv_q_ui@@Base>:
   22850:	stp	x29, x30, [sp, #-64]!
   22854:	stp	x24, x23, [sp, #16]
   22858:	stp	x22, x21, [sp, #32]
   2285c:	stp	x20, x19, [sp, #48]
   22860:	mov	x29, sp
   22864:	cbz	x2, 22900 <__gmpz_tdiv_q_ui@@Base+0xb0>
   22868:	ldrsw	x24, [x1, #4]
   2286c:	mov	x22, x1
   22870:	mov	x19, x0
   22874:	cbz	w24, 228cc <__gmpz_tdiv_q_ui@@Base+0x7c>
   22878:	ldrsw	x8, [x19]
   2287c:	cmp	w24, #0x0
   22880:	cneg	x21, x24, lt  // lt = tstop
   22884:	mov	x20, x2
   22888:	cmp	x21, x8
   2288c:	b.gt	228ec <__gmpz_tdiv_q_ui@@Base+0x9c>
   22890:	ldr	x23, [x19, #8]
   22894:	ldr	x2, [x22, #8]
   22898:	mov	x0, x23
   2289c:	mov	x1, xzr
   228a0:	mov	x3, x21
   228a4:	mov	x4, x20
   228a8:	bl	cd00 <__gmpn_divrem_1@plt>
   228ac:	add	x8, x23, x21, lsl #3
   228b0:	ldur	x8, [x8, #-8]
   228b4:	cmp	x8, #0x0
   228b8:	cset	w8, eq  // eq = none
   228bc:	sub	w8, w21, w8
   228c0:	cmp	w24, #0x0
   228c4:	cneg	w8, w8, lt  // lt = tstop
   228c8:	b	228d4 <__gmpz_tdiv_q_ui@@Base+0x84>
   228cc:	mov	w8, wzr
   228d0:	mov	x0, xzr
   228d4:	str	w8, [x19, #4]
   228d8:	ldp	x20, x19, [sp, #48]
   228dc:	ldp	x22, x21, [sp, #32]
   228e0:	ldp	x24, x23, [sp, #16]
   228e4:	ldp	x29, x30, [sp], #64
   228e8:	ret
   228ec:	mov	x0, x19
   228f0:	mov	x1, x21
   228f4:	bl	c080 <__gmpz_realloc@plt>
   228f8:	mov	x23, x0
   228fc:	b	22894 <__gmpz_tdiv_q_ui@@Base+0x44>
   22900:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022904 <__gmpz_tdiv_qr@@Base>:
   22904:	stp	x29, x30, [sp, #-96]!
   22908:	stp	x28, x27, [sp, #16]
   2290c:	stp	x26, x25, [sp, #32]
   22910:	stp	x24, x23, [sp, #48]
   22914:	stp	x22, x21, [sp, #64]
   22918:	stp	x20, x19, [sp, #80]
   2291c:	mov	x29, sp
   22920:	sub	sp, sp, #0x20
   22924:	ldrsw	x24, [x2, #4]
   22928:	ldrsw	x28, [x3, #4]
   2292c:	cmp	x24, #0x0
   22930:	cneg	x22, x24, mi  // mi = first
   22934:	cmp	x28, #0x0
   22938:	cneg	x21, x28, mi  // mi = first
   2293c:	cbz	x21, 22b30 <__gmpz_tdiv_qr@@Base+0x22c>
   22940:	ldrsw	x8, [x1]
   22944:	mov	x26, x3
   22948:	mov	x27, x2
   2294c:	mov	x19, x1
   22950:	mov	x25, x0
   22954:	cmp	x21, x8
   22958:	sub	x20, x22, x21
   2295c:	b.gt	22aa4 <__gmpz_tdiv_qr@@Base+0x1a0>
   22960:	ldr	x23, [x19, #8]
   22964:	tbnz	x20, #63, 22ab8 <__gmpz_tdiv_qr@@Base+0x1b4>
   22968:	ldrsw	x8, [x25]
   2296c:	cmp	x20, x8
   22970:	add	x8, x20, #0x1
   22974:	stp	x25, x8, [x29, #-24]
   22978:	b.ge	22afc <__gmpz_tdiv_qr@@Base+0x1f8>  // b.tcont
   2297c:	ldr	x25, [x25, #8]
   22980:	stur	xzr, [x29, #-8]
   22984:	ldr	x26, [x26, #8]
   22988:	ldr	x27, [x27, #8]
   2298c:	stur	x28, [x29, #-32]
   22990:	cmp	x26, x23
   22994:	b.eq	229a0 <__gmpz_tdiv_qr@@Base+0x9c>  // b.none
   22998:	cmp	x26, x25
   2299c:	b.ne	229d8 <__gmpz_tdiv_qr@@Base+0xd4>  // b.any
   229a0:	cmp	x21, #0xfe0
   229a4:	lsl	x1, x21, #3
   229a8:	b.hi	22b10 <__gmpz_tdiv_qr@@Base+0x20c>  // b.pmore
   229ac:	add	x9, x1, #0xf
   229b0:	mov	x8, sp
   229b4:	and	x9, x9, #0xfffffffffffffff0
   229b8:	sub	x28, x8, x9
   229bc:	mov	sp, x28
   229c0:	mov	x0, x28
   229c4:	mov	x1, x26
   229c8:	mov	x2, x21
   229cc:	bl	ca50 <__gmpn_copyi@plt>
   229d0:	mov	x26, x28
   229d4:	ldur	x28, [x29, #-32]
   229d8:	cmp	x27, x23
   229dc:	b.eq	229e8 <__gmpz_tdiv_qr@@Base+0xe4>  // b.none
   229e0:	cmp	x27, x25
   229e4:	b.ne	22a20 <__gmpz_tdiv_qr@@Base+0x11c>  // b.any
   229e8:	cmp	x22, #0xfe0
   229ec:	lsl	x1, x22, #3
   229f0:	b.hi	22b20 <__gmpz_tdiv_qr@@Base+0x21c>  // b.pmore
   229f4:	add	x9, x1, #0xf
   229f8:	mov	x8, sp
   229fc:	and	x9, x9, #0xfffffffffffffff0
   22a00:	sub	x28, x8, x9
   22a04:	mov	sp, x28
   22a08:	mov	x0, x28
   22a0c:	mov	x1, x27
   22a10:	mov	x2, x22
   22a14:	bl	ca50 <__gmpn_copyi@plt>
   22a18:	mov	x27, x28
   22a1c:	ldur	x28, [x29, #-32]
   22a20:	mov	x0, x25
   22a24:	mov	x1, x23
   22a28:	mov	x2, xzr
   22a2c:	mov	x3, x27
   22a30:	mov	x4, x22
   22a34:	mov	x5, x26
   22a38:	mov	x6, x21
   22a3c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   22a40:	ldr	x8, [x25, x20, lsl #3]
   22a44:	ldur	x9, [x29, #-16]
   22a48:	sub	x10, x23, #0x8
   22a4c:	cmp	x8, #0x0
   22a50:	cset	w8, eq  // eq = none
   22a54:	sub	x8, x9, x8
   22a58:	mov	x9, x21
   22a5c:	subs	x21, x21, #0x1
   22a60:	b.lt	22a6c <__gmpz_tdiv_qr@@Base+0x168>  // b.tstop
   22a64:	ldr	x11, [x10, x9, lsl #3]
   22a68:	cbz	x11, 22a58 <__gmpz_tdiv_qr@@Base+0x154>
   22a6c:	eor	w10, w28, w24
   22a70:	cmp	w10, #0x0
   22a74:	ldur	x10, [x29, #-24]
   22a78:	neg	w11, w8
   22a7c:	neg	w12, w9
   22a80:	csel	x8, x8, x11, ge  // ge = tcont
   22a84:	cmp	w24, #0x0
   22a88:	str	w8, [x10, #4]
   22a8c:	csel	x8, x9, x12, ge  // ge = tcont
   22a90:	str	w8, [x19, #4]
   22a94:	ldur	x0, [x29, #-8]
   22a98:	cbz	x0, 22adc <__gmpz_tdiv_qr@@Base+0x1d8>
   22a9c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   22aa0:	b	22adc <__gmpz_tdiv_qr@@Base+0x1d8>
   22aa4:	mov	x0, x19
   22aa8:	mov	x1, x21
   22aac:	bl	c080 <__gmpz_realloc@plt>
   22ab0:	mov	x23, x0
   22ab4:	tbz	x20, #63, 22968 <__gmpz_tdiv_qr@@Base+0x64>
   22ab8:	cmp	x27, x19
   22abc:	b.eq	22ad8 <__gmpz_tdiv_qr@@Base+0x1d4>  // b.none
   22ac0:	ldr	x1, [x27, #8]
   22ac4:	mov	x0, x23
   22ac8:	mov	x2, x22
   22acc:	bl	ca50 <__gmpn_copyi@plt>
   22ad0:	ldr	w8, [x27, #4]
   22ad4:	str	w8, [x19, #4]
   22ad8:	str	wzr, [x25, #4]
   22adc:	mov	sp, x29
   22ae0:	ldp	x20, x19, [sp, #80]
   22ae4:	ldp	x22, x21, [sp, #64]
   22ae8:	ldp	x24, x23, [sp, #48]
   22aec:	ldp	x26, x25, [sp, #32]
   22af0:	ldp	x28, x27, [sp, #16]
   22af4:	ldp	x29, x30, [sp], #96
   22af8:	ret
   22afc:	ldur	x1, [x29, #-16]
   22b00:	mov	x0, x25
   22b04:	bl	c080 <__gmpz_realloc@plt>
   22b08:	mov	x25, x0
   22b0c:	b	22980 <__gmpz_tdiv_qr@@Base+0x7c>
   22b10:	sub	x0, x29, #0x8
   22b14:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22b18:	mov	x28, x0
   22b1c:	b	229c0 <__gmpz_tdiv_qr@@Base+0xbc>
   22b20:	sub	x0, x29, #0x8
   22b24:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22b28:	mov	x28, x0
   22b2c:	b	22a08 <__gmpz_tdiv_qr@@Base+0x104>
   22b30:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022b34 <__gmpz_tdiv_qr_ui@@Base>:
   22b34:	stp	x29, x30, [sp, #-80]!
   22b38:	str	x25, [sp, #16]
   22b3c:	stp	x24, x23, [sp, #32]
   22b40:	stp	x22, x21, [sp, #48]
   22b44:	stp	x20, x19, [sp, #64]
   22b48:	mov	x29, sp
   22b4c:	cbz	x3, 22c40 <__gmpz_tdiv_qr_ui@@Base+0x10c>
   22b50:	ldrsw	x25, [x2, #4]
   22b54:	mov	x24, x2
   22b58:	mov	x20, x1
   22b5c:	mov	x19, x0
   22b60:	cbz	w25, 22bc8 <__gmpz_tdiv_qr_ui@@Base+0x94>
   22b64:	ldrsw	x8, [x19]
   22b68:	cmp	w25, #0x0
   22b6c:	cneg	x21, x25, lt  // lt = tstop
   22b70:	mov	x22, x3
   22b74:	cmp	x21, x8
   22b78:	b.gt	22c1c <__gmpz_tdiv_qr_ui@@Base+0xe8>
   22b7c:	ldr	x23, [x19, #8]
   22b80:	ldr	x2, [x24, #8]
   22b84:	mov	x0, x23
   22b88:	mov	x1, xzr
   22b8c:	mov	x3, x21
   22b90:	mov	x4, x22
   22b94:	bl	cd00 <__gmpn_divrem_1@plt>
   22b98:	mov	x22, x0
   22b9c:	cbz	x0, 22bdc <__gmpz_tdiv_qr_ui@@Base+0xa8>
   22ba0:	ldr	w8, [x20]
   22ba4:	cmp	w25, #0x0
   22ba8:	mov	w9, #0x1                   	// #1
   22bac:	cneg	w9, w9, lt  // lt = tstop
   22bb0:	cmp	w8, #0x0
   22bb4:	str	w9, [x20, #4]
   22bb8:	b.le	22c30 <__gmpz_tdiv_qr_ui@@Base+0xfc>
   22bbc:	ldr	x0, [x20, #8]
   22bc0:	str	x22, [x0]
   22bc4:	b	22be0 <__gmpz_tdiv_qr_ui@@Base+0xac>
   22bc8:	mov	w8, wzr
   22bcc:	mov	x22, xzr
   22bd0:	str	wzr, [x19, #4]
   22bd4:	mov	x19, x20
   22bd8:	b	22bfc <__gmpz_tdiv_qr_ui@@Base+0xc8>
   22bdc:	str	wzr, [x20, #4]
   22be0:	add	x8, x23, x21, lsl #3
   22be4:	ldur	x8, [x8, #-8]
   22be8:	cmp	x8, #0x0
   22bec:	cset	w8, eq  // eq = none
   22bf0:	sub	w8, w21, w8
   22bf4:	cmp	w25, #0x0
   22bf8:	cneg	w8, w8, lt  // lt = tstop
   22bfc:	str	w8, [x19, #4]
   22c00:	mov	x0, x22
   22c04:	ldp	x20, x19, [sp, #64]
   22c08:	ldp	x22, x21, [sp, #48]
   22c0c:	ldp	x24, x23, [sp, #32]
   22c10:	ldr	x25, [sp, #16]
   22c14:	ldp	x29, x30, [sp], #80
   22c18:	ret
   22c1c:	mov	x0, x19
   22c20:	mov	x1, x21
   22c24:	bl	c080 <__gmpz_realloc@plt>
   22c28:	mov	x23, x0
   22c2c:	b	22b80 <__gmpz_tdiv_qr_ui@@Base+0x4c>
   22c30:	mov	w1, #0x1                   	// #1
   22c34:	mov	x0, x20
   22c38:	bl	c080 <__gmpz_realloc@plt>
   22c3c:	b	22bc0 <__gmpz_tdiv_qr_ui@@Base+0x8c>
   22c40:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022c44 <__gmpz_tdiv_r@@Base>:
   22c44:	stp	x29, x30, [sp, #-80]!
   22c48:	stp	x26, x25, [sp, #16]
   22c4c:	stp	x24, x23, [sp, #32]
   22c50:	stp	x22, x21, [sp, #48]
   22c54:	stp	x20, x19, [sp, #64]
   22c58:	mov	x29, sp
   22c5c:	sub	sp, sp, #0x10
   22c60:	ldrsw	x26, [x1, #4]
   22c64:	ldr	w8, [x2, #4]
   22c68:	cmp	x26, #0x0
   22c6c:	cneg	x20, x26, mi  // mi = first
   22c70:	cmp	w8, #0x0
   22c74:	cneg	w21, w8, mi  // mi = first
   22c78:	cbz	w21, 22e4c <__gmpz_tdiv_r@@Base+0x208>
   22c7c:	mov	x24, x1
   22c80:	mov	x19, x0
   22c84:	sub	x23, x20, x21
   22c88:	tbnz	x23, #63, 22ce4 <__gmpz_tdiv_r@@Base+0xa0>
   22c8c:	ldrsw	x8, [x19]
   22c90:	mov	x25, x2
   22c94:	cmp	x21, x8
   22c98:	b.gt	22df0 <__gmpz_tdiv_r@@Base+0x1ac>
   22c9c:	ldr	x22, [x19, #8]
   22ca0:	lsl	x8, x23, #3
   22ca4:	cmp	x23, #0xfdf
   22ca8:	add	x1, x8, #0x8
   22cac:	stur	xzr, [x29, #-8]
   22cb0:	b.hi	22e04 <__gmpz_tdiv_r@@Base+0x1c0>  // b.pmore
   22cb4:	add	x9, x1, #0xf
   22cb8:	mov	x8, sp
   22cbc:	and	x9, x9, #0xfffffffffffffff0
   22cc0:	sub	x23, x8, x9
   22cc4:	mov	sp, x23
   22cc8:	ldr	x25, [x25, #8]
   22ccc:	ldr	x24, [x24, #8]
   22cd0:	cmp	x25, x22
   22cd4:	b.eq	22d10 <__gmpz_tdiv_r@@Base+0xcc>  // b.none
   22cd8:	cmp	x24, x22
   22cdc:	b.ne	22d78 <__gmpz_tdiv_r@@Base+0x134>  // b.any
   22ce0:	b	22d48 <__gmpz_tdiv_r@@Base+0x104>
   22ce4:	cmp	x24, x19
   22ce8:	b.eq	22dd4 <__gmpz_tdiv_r@@Base+0x190>  // b.none
   22cec:	ldrsw	x8, [x19]
   22cf0:	str	w26, [x19, #4]
   22cf4:	cmp	x20, x8
   22cf8:	b.gt	22e1c <__gmpz_tdiv_r@@Base+0x1d8>
   22cfc:	ldr	x0, [x19, #8]
   22d00:	ldr	x1, [x24, #8]
   22d04:	mov	x2, x20
   22d08:	bl	ca50 <__gmpn_copyi@plt>
   22d0c:	b	22dd4 <__gmpz_tdiv_r@@Base+0x190>
   22d10:	cmp	w21, #0xfe0
   22d14:	lsl	x1, x21, #3
   22d18:	b.hi	22e2c <__gmpz_tdiv_r@@Base+0x1e8>  // b.pmore
   22d1c:	add	x9, x1, #0xf
   22d20:	mov	x8, sp
   22d24:	and	x9, x9, #0xffffffff0
   22d28:	sub	x25, x8, x9
   22d2c:	mov	sp, x25
   22d30:	mov	x0, x25
   22d34:	mov	x1, x22
   22d38:	mov	x2, x21
   22d3c:	bl	ca50 <__gmpn_copyi@plt>
   22d40:	cmp	x24, x22
   22d44:	b.ne	22d78 <__gmpz_tdiv_r@@Base+0x134>  // b.any
   22d48:	cmp	x20, #0xfe0
   22d4c:	lsl	x1, x20, #3
   22d50:	b.hi	22e3c <__gmpz_tdiv_r@@Base+0x1f8>  // b.pmore
   22d54:	add	x9, x1, #0xf
   22d58:	mov	x8, sp
   22d5c:	and	x9, x9, #0xfffffffffffffff0
   22d60:	sub	x24, x8, x9
   22d64:	mov	sp, x24
   22d68:	mov	x0, x24
   22d6c:	mov	x1, x22
   22d70:	mov	x2, x20
   22d74:	bl	ca50 <__gmpn_copyi@plt>
   22d78:	mov	x0, x23
   22d7c:	mov	x1, x22
   22d80:	mov	x2, xzr
   22d84:	mov	x3, x24
   22d88:	mov	x4, x20
   22d8c:	mov	x5, x25
   22d90:	mov	x6, x21
   22d94:	bl	bf00 <__gmpn_tdiv_qr@plt>
   22d98:	sub	x8, x22, #0x8
   22d9c:	subs	x9, x21, #0x1
   22da0:	b.lt	22db8 <__gmpz_tdiv_r@@Base+0x174>  // b.tstop
   22da4:	ldr	x10, [x8, x21, lsl #3]
   22da8:	mov	x21, x9
   22dac:	cbz	x10, 22d9c <__gmpz_tdiv_r@@Base+0x158>
   22db0:	add	x8, x9, #0x1
   22db4:	b	22dbc <__gmpz_tdiv_r@@Base+0x178>
   22db8:	mov	x8, xzr
   22dbc:	neg	w9, w8
   22dc0:	cmp	w26, #0x0
   22dc4:	csel	x8, x8, x9, ge  // ge = tcont
   22dc8:	str	w8, [x19, #4]
   22dcc:	ldur	x0, [x29, #-8]
   22dd0:	cbnz	x0, 22e14 <__gmpz_tdiv_r@@Base+0x1d0>
   22dd4:	mov	sp, x29
   22dd8:	ldp	x20, x19, [sp, #64]
   22ddc:	ldp	x22, x21, [sp, #48]
   22de0:	ldp	x24, x23, [sp, #32]
   22de4:	ldp	x26, x25, [sp, #16]
   22de8:	ldp	x29, x30, [sp], #80
   22dec:	ret
   22df0:	mov	x0, x19
   22df4:	mov	x1, x21
   22df8:	bl	c080 <__gmpz_realloc@plt>
   22dfc:	mov	x22, x0
   22e00:	b	22ca0 <__gmpz_tdiv_r@@Base+0x5c>
   22e04:	sub	x0, x29, #0x8
   22e08:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22e0c:	mov	x23, x0
   22e10:	b	22cc8 <__gmpz_tdiv_r@@Base+0x84>
   22e14:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   22e18:	b	22dd4 <__gmpz_tdiv_r@@Base+0x190>
   22e1c:	mov	x0, x19
   22e20:	mov	x1, x20
   22e24:	bl	c080 <__gmpz_realloc@plt>
   22e28:	b	22d00 <__gmpz_tdiv_r@@Base+0xbc>
   22e2c:	sub	x0, x29, #0x8
   22e30:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22e34:	mov	x25, x0
   22e38:	b	22d30 <__gmpz_tdiv_r@@Base+0xec>
   22e3c:	sub	x0, x29, #0x8
   22e40:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   22e44:	mov	x24, x0
   22e48:	b	22d68 <__gmpz_tdiv_r@@Base+0x124>
   22e4c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000022e50 <__gmpz_tdiv_r_2exp@@Base>:
   22e50:	stp	x29, x30, [sp, #-64]!
   22e54:	stp	x22, x21, [sp, #32]
   22e58:	stp	x20, x19, [sp, #48]
   22e5c:	ldr	w8, [x1, #4]
   22e60:	lsr	x21, x2, #6
   22e64:	mov	x20, x1
   22e68:	mov	x19, x0
   22e6c:	cmp	w8, #0x0
   22e70:	cneg	w22, w8, mi  // mi = first
   22e74:	cmp	x21, x22
   22e78:	str	x23, [sp, #16]
   22e7c:	mov	x29, sp
   22e80:	b.cs	22ec0 <__gmpz_tdiv_r_2exp@@Base+0x70>  // b.hs, b.nlast
   22e84:	ldr	x8, [x20, #8]
   22e88:	mov	x10, #0xffffffffffffffff    	// #-1
   22e8c:	lsl	x10, x10, x2
   22e90:	ldr	x9, [x8, x21, lsl #3]
   22e94:	bics	x23, x9, x10
   22e98:	b.eq	22f10 <__gmpz_tdiv_r_2exp@@Base+0xc0>  // b.none
   22e9c:	ldrsw	x8, [x19]
   22ea0:	add	x22, x21, #0x1
   22ea4:	cmp	x21, x8
   22ea8:	b.ge	22f60 <__gmpz_tdiv_r_2exp@@Base+0x110>  // b.tcont
   22eac:	ldr	x8, [x19, #8]
   22eb0:	str	x23, [x8, x21, lsl #3]
   22eb4:	cmp	x19, x20
   22eb8:	b.ne	22ed8 <__gmpz_tdiv_r_2exp@@Base+0x88>  // b.any
   22ebc:	b	22ee8 <__gmpz_tdiv_r_2exp@@Base+0x98>
   22ec0:	ldrsw	x8, [x19]
   22ec4:	cmp	x22, x8
   22ec8:	b.gt	22f50 <__gmpz_tdiv_r_2exp@@Base+0x100>
   22ecc:	mov	x21, x22
   22ed0:	cmp	x19, x20
   22ed4:	b.eq	22ee8 <__gmpz_tdiv_r_2exp@@Base+0x98>  // b.none
   22ed8:	ldr	x0, [x19, #8]
   22edc:	ldr	x1, [x20, #8]
   22ee0:	mov	x2, x21
   22ee4:	bl	ca50 <__gmpn_copyi@plt>
   22ee8:	ldr	w8, [x20, #4]
   22eec:	neg	w9, w22
   22ef0:	ldr	x23, [sp, #16]
   22ef4:	cmp	w8, #0x0
   22ef8:	csel	x8, x22, x9, ge  // ge = tcont
   22efc:	str	w8, [x19, #4]
   22f00:	ldp	x20, x19, [sp, #48]
   22f04:	ldp	x22, x21, [sp, #32]
   22f08:	ldp	x29, x30, [sp], #64
   22f0c:	ret
   22f10:	sub	x8, x8, #0x8
   22f14:	subs	x9, x21, #0x1
   22f18:	b.lt	22f30 <__gmpz_tdiv_r_2exp@@Base+0xe0>  // b.tstop
   22f1c:	ldr	x10, [x8, x21, lsl #3]
   22f20:	mov	x21, x9
   22f24:	cbz	x10, 22f14 <__gmpz_tdiv_r_2exp@@Base+0xc4>
   22f28:	add	x21, x9, #0x1
   22f2c:	b	22f34 <__gmpz_tdiv_r_2exp@@Base+0xe4>
   22f30:	mov	x21, xzr
   22f34:	ldrsw	x8, [x19]
   22f38:	cmp	x21, x8
   22f3c:	b.gt	22f70 <__gmpz_tdiv_r_2exp@@Base+0x120>
   22f40:	mov	x22, x21
   22f44:	cmp	x19, x20
   22f48:	b.ne	22ed8 <__gmpz_tdiv_r_2exp@@Base+0x88>  // b.any
   22f4c:	b	22ee8 <__gmpz_tdiv_r_2exp@@Base+0x98>
   22f50:	mov	x0, x19
   22f54:	mov	x1, x22
   22f58:	bl	c080 <__gmpz_realloc@plt>
   22f5c:	b	22ecc <__gmpz_tdiv_r_2exp@@Base+0x7c>
   22f60:	mov	x0, x19
   22f64:	mov	x1, x22
   22f68:	bl	c080 <__gmpz_realloc@plt>
   22f6c:	b	22eac <__gmpz_tdiv_r_2exp@@Base+0x5c>
   22f70:	mov	x0, x19
   22f74:	mov	x1, x21
   22f78:	bl	c080 <__gmpz_realloc@plt>
   22f7c:	b	22f40 <__gmpz_tdiv_r_2exp@@Base+0xf0>

0000000000022f80 <__gmpz_tdiv_r_ui@@Base>:
   22f80:	stp	x29, x30, [sp, #-48]!
   22f84:	str	x21, [sp, #16]
   22f88:	stp	x20, x19, [sp, #32]
   22f8c:	mov	x29, sp
   22f90:	cbz	x2, 2300c <__gmpz_tdiv_r_ui@@Base+0x8c>
   22f94:	ldrsw	x21, [x1, #4]
   22f98:	mov	x20, x0
   22f9c:	cbz	w21, 22fe0 <__gmpz_tdiv_r_ui@@Base+0x60>
   22fa0:	ldr	x0, [x1, #8]
   22fa4:	cmp	w21, #0x0
   22fa8:	cneg	x1, x21, lt  // lt = tstop
   22fac:	bl	c3e0 <__gmpn_mod_1@plt>
   22fb0:	mov	x19, x0
   22fb4:	cbz	x0, 22fe4 <__gmpz_tdiv_r_ui@@Base+0x64>
   22fb8:	ldr	w8, [x20]
   22fbc:	cmp	w21, #0x0
   22fc0:	mov	w9, #0x1                   	// #1
   22fc4:	cneg	w9, w9, lt  // lt = tstop
   22fc8:	cmp	w8, #0x0
   22fcc:	str	w9, [x20, #4]
   22fd0:	b.le	22ffc <__gmpz_tdiv_r_ui@@Base+0x7c>
   22fd4:	ldr	x0, [x20, #8]
   22fd8:	str	x19, [x0]
   22fdc:	b	22fe8 <__gmpz_tdiv_r_ui@@Base+0x68>
   22fe0:	mov	x19, xzr
   22fe4:	str	wzr, [x20, #4]
   22fe8:	mov	x0, x19
   22fec:	ldp	x20, x19, [sp, #32]
   22ff0:	ldr	x21, [sp, #16]
   22ff4:	ldp	x29, x30, [sp], #48
   22ff8:	ret
   22ffc:	mov	w1, #0x1                   	// #1
   23000:	mov	x0, x20
   23004:	bl	c080 <__gmpz_realloc@plt>
   23008:	b	22fd8 <__gmpz_tdiv_r_ui@@Base+0x58>
   2300c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000023010 <__gmpz_tstbit@@Base>:
   23010:	ldr	w11, [x0, #4]
   23014:	lsr	x10, x1, #6
   23018:	cmp	w11, #0x0
   2301c:	cneg	w8, w11, mi  // mi = first
   23020:	cmp	x10, x8
   23024:	b.cs	23064 <__gmpz_tstbit@@Base+0x54>  // b.hs, b.nlast
   23028:	ldr	x12, [x0, #8]
   2302c:	ldr	x9, [x12, x10, lsl #3]
   23030:	mov	x8, x9
   23034:	tbz	w11, #31, 23058 <__gmpz_tstbit@@Base+0x48>
   23038:	neg	x8, x9
   2303c:	sub	x11, x12, #0x8
   23040:	lsl	x10, x10, #3
   23044:	cbz	x10, 23058 <__gmpz_tstbit@@Base+0x48>
   23048:	ldr	x12, [x11, x10]
   2304c:	sub	x10, x10, #0x8
   23050:	cbz	x12, 23044 <__gmpz_tstbit@@Base+0x34>
   23054:	mvn	x8, x9
   23058:	lsr	x8, x8, x1
   2305c:	and	w0, w8, #0x1
   23060:	ret
   23064:	lsr	w0, w11, #31
   23068:	ret

000000000002306c <__gmpz_ui_pow_ui@@Base>:
   2306c:	sub	sp, sp, #0x20
   23070:	cmp	x1, #0x0
   23074:	mov	x3, x2
   23078:	str	x1, [sp, #8]
   2307c:	cset	w2, ne  // ne = any
   23080:	add	x1, sp, #0x8
   23084:	stp	x29, x30, [sp, #16]
   23088:	add	x29, sp, #0x10
   2308c:	bl	c340 <__gmpz_n_pow_ui@plt>
   23090:	ldp	x29, x30, [sp, #16]
   23094:	add	sp, sp, #0x20
   23098:	ret

000000000002309c <__gmpz_ui_sub@@Base>:
   2309c:	sub	sp, sp, #0x40
   230a0:	stp	x29, x30, [sp, #16]
   230a4:	stp	x22, x21, [sp, #32]
   230a8:	stp	x20, x19, [sp, #48]
   230ac:	ldrsw	x20, [x2, #4]
   230b0:	mov	x22, x2
   230b4:	mov	x21, x1
   230b8:	mov	x19, x0
   230bc:	cmp	w20, #0x2
   230c0:	add	x29, sp, #0x10
   230c4:	b.lt	231d0 <__gmpz_ui_sub@@Base+0x134>  // b.tstop
   230c8:	ldr	w8, [x19]
   230cc:	cmp	w20, w8
   230d0:	b.gt	23488 <__gmpz_ui_sub@@Base+0x3ec>
   230d4:	ldr	x0, [x19, #8]
   230d8:	ldr	x8, [x22, #8]
   230dc:	ldr	x9, [x8]
   230e0:	subs	x9, x9, x21
   230e4:	str	x9, [x0]
   230e8:	b.cs	23208 <__gmpz_ui_sub@@Base+0x16c>  // b.hs, b.nlast
   230ec:	mov	x11, xzr
   230f0:	sub	x10, x20, #0x1
   230f4:	mov	w9, #0x1                   	// #1
   230f8:	cmp	x9, x20
   230fc:	b.ge	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.tcont
   23100:	add	x12, x8, x11
   23104:	ldr	x12, [x12, #8]
   23108:	add	x13, x0, x11
   2310c:	add	x9, x9, #0x1
   23110:	add	x11, x11, #0x8
   23114:	sub	x14, x12, #0x1
   23118:	sub	x10, x10, #0x1
   2311c:	str	x14, [x13, #8]
   23120:	cbz	x12, 230f8 <__gmpz_ui_sub@@Base+0x5c>
   23124:	cmp	x8, x0
   23128:	b.eq	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   2312c:	cmp	x9, x20
   23130:	b.ge	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.tcont
   23134:	sub	x12, x20, x9
   23138:	cmp	x12, #0x4
   2313c:	b.cc	231ac <__gmpz_ui_sub@@Base+0x110>  // b.lo, b.ul, b.last
   23140:	add	x14, x0, x11
   23144:	lsl	x13, x20, #3
   23148:	add	x14, x14, #0x8
   2314c:	add	x15, x8, x13
   23150:	cmp	x14, x15
   23154:	b.cs	2316c <__gmpz_ui_sub@@Base+0xd0>  // b.hs, b.nlast
   23158:	add	x14, x8, x11
   2315c:	add	x13, x0, x13
   23160:	add	x14, x14, #0x8
   23164:	cmp	x14, x13
   23168:	b.cc	231ac <__gmpz_ui_sub@@Base+0x110>  // b.lo, b.ul, b.last
   2316c:	add	x13, x0, x11
   23170:	add	x14, x8, x11
   23174:	and	x11, x12, #0xfffffffffffffffc
   23178:	and	x15, x10, #0xfffffffffffffffc
   2317c:	add	x10, x13, #0x18
   23180:	add	x13, x14, #0x18
   23184:	add	x9, x15, x9
   23188:	mov	x14, x11
   2318c:	ldp	q0, q1, [x13, #-16]
   23190:	add	x13, x13, #0x20
   23194:	subs	x14, x14, #0x4
   23198:	stp	q0, q1, [x10, #-16]
   2319c:	add	x10, x10, #0x20
   231a0:	b.ne	2318c <__gmpz_ui_sub@@Base+0xf0>  // b.any
   231a4:	cmp	x12, x11
   231a8:	b.eq	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   231ac:	lsl	x11, x9, #3
   231b0:	sub	x10, x20, x9
   231b4:	add	x9, x0, x11
   231b8:	add	x8, x8, x11
   231bc:	ldr	x11, [x8], #8
   231c0:	subs	x10, x10, #0x1
   231c4:	str	x11, [x9], #8
   231c8:	b.ne	231bc <__gmpz_ui_sub@@Base+0x120>  // b.any
   231cc:	b	23264 <__gmpz_ui_sub@@Base+0x1c8>
   231d0:	tbnz	w20, #31, 2327c <__gmpz_ui_sub@@Base+0x1e0>
   231d4:	ldr	x8, [x22, #8]
   231d8:	ldr	w9, [x19]
   231dc:	neg	x10, x20
   231e0:	ldr	x8, [x8]
   231e4:	cmp	w9, #0x0
   231e8:	and	x20, x8, x10
   231ec:	b.le	23498 <__gmpz_ui_sub@@Base+0x3fc>
   231f0:	ldr	x0, [x19, #8]
   231f4:	subs	x8, x20, x21
   231f8:	b.ls	234ac <__gmpz_ui_sub@@Base+0x410>  // b.plast
   231fc:	str	x8, [x0]
   23200:	mov	w8, #0xffffffff            	// #-1
   23204:	b	234b8 <__gmpz_ui_sub@@Base+0x41c>
   23208:	cmp	x8, x0
   2320c:	b.eq	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   23210:	sub	x9, x20, #0x1
   23214:	cmp	x9, #0x4
   23218:	b.cc	23240 <__gmpz_ui_sub@@Base+0x1a4>  // b.lo, b.ul, b.last
   2321c:	lsl	x10, x20, #3
   23220:	add	x11, x0, #0x8
   23224:	add	x12, x8, x10
   23228:	cmp	x11, x12
   2322c:	b.cs	23414 <__gmpz_ui_sub@@Base+0x378>  // b.hs, b.nlast
   23230:	add	x10, x0, x10
   23234:	add	x11, x8, #0x8
   23238:	cmp	x11, x10
   2323c:	b.cs	23414 <__gmpz_ui_sub@@Base+0x378>  // b.hs, b.nlast
   23240:	mov	w10, #0x1                   	// #1
   23244:	lsl	x11, x10, #3
   23248:	sub	x9, x20, x10
   2324c:	add	x10, x0, x11
   23250:	add	x8, x8, x11
   23254:	ldr	x11, [x8], #8
   23258:	subs	x9, x9, #0x1
   2325c:	str	x11, [x10], #8
   23260:	b.ne	23254 <__gmpz_ui_sub@@Base+0x1b8>  // b.any
   23264:	add	x8, x0, x20, lsl #3
   23268:	ldur	x8, [x8, #-8]
   2326c:	cmp	x8, #0x0
   23270:	cset	w8, eq  // eq = none
   23274:	sub	w8, w8, w20
   23278:	b	234b8 <__gmpz_ui_sub@@Base+0x41c>
   2327c:	ldrsw	x8, [x19]
   23280:	mov	w9, #0x1                   	// #1
   23284:	sub	x1, x9, x20
   23288:	cmp	x1, x8
   2328c:	neg	x8, x20
   23290:	b.gt	234d0 <__gmpz_ui_sub@@Base+0x434>
   23294:	ldr	x0, [x19, #8]
   23298:	ldr	x9, [x22, #8]
   2329c:	ldr	x10, [x9]
   232a0:	adds	x10, x10, x21
   232a4:	str	x10, [x0]
   232a8:	b.cc	2339c <__gmpz_ui_sub@@Base+0x300>  // b.lo, b.ul, b.last
   232ac:	mov	x12, xzr
   232b0:	mvn	x11, x20
   232b4:	mov	w13, #0x1                   	// #1
   232b8:	mov	w10, #0x1                   	// #1
   232bc:	cmp	x10, x8
   232c0:	b.ge	23408 <__gmpz_ui_sub@@Base+0x36c>  // b.tcont
   232c4:	add	x14, x9, x12
   232c8:	ldr	x14, [x14, #8]
   232cc:	add	x15, x0, x12
   232d0:	add	x10, x10, #0x1
   232d4:	add	x12, x12, #0x8
   232d8:	adds	x14, x14, #0x1
   232dc:	sub	x11, x11, #0x1
   232e0:	str	x14, [x15, #8]
   232e4:	b.cs	232bc <__gmpz_ui_sub@@Base+0x220>  // b.hs, b.nlast
   232e8:	cmp	x9, x0
   232ec:	mov	x13, xzr
   232f0:	b.eq	23408 <__gmpz_ui_sub@@Base+0x36c>  // b.none
   232f4:	cmp	x10, x8
   232f8:	b.ge	23408 <__gmpz_ui_sub@@Base+0x36c>  // b.tcont
   232fc:	sub	x13, x8, x10
   23300:	cmp	x13, #0x4
   23304:	b.cc	23374 <__gmpz_ui_sub@@Base+0x2d8>  // b.lo, b.ul, b.last
   23308:	add	x15, x0, x12
   2330c:	lsl	x14, x8, #3
   23310:	add	x15, x15, #0x8
   23314:	add	x16, x9, x14
   23318:	cmp	x15, x16
   2331c:	b.cs	23334 <__gmpz_ui_sub@@Base+0x298>  // b.hs, b.nlast
   23320:	add	x15, x9, x12
   23324:	add	x14, x0, x14
   23328:	add	x15, x15, #0x8
   2332c:	cmp	x15, x14
   23330:	b.cc	23374 <__gmpz_ui_sub@@Base+0x2d8>  // b.lo, b.ul, b.last
   23334:	add	x14, x0, x12
   23338:	add	x15, x9, x12
   2333c:	and	x12, x13, #0xfffffffffffffffc
   23340:	and	x16, x11, #0xfffffffffffffffc
   23344:	add	x11, x14, #0x18
   23348:	add	x14, x15, #0x18
   2334c:	add	x10, x16, x10
   23350:	mov	x15, x12
   23354:	ldp	q0, q1, [x14, #-16]
   23358:	add	x14, x14, #0x20
   2335c:	subs	x15, x15, #0x4
   23360:	stp	q0, q1, [x11, #-16]
   23364:	add	x11, x11, #0x20
   23368:	b.ne	23354 <__gmpz_ui_sub@@Base+0x2b8>  // b.any
   2336c:	cmp	x13, x12
   23370:	b.eq	23404 <__gmpz_ui_sub@@Base+0x368>  // b.none
   23374:	add	x11, x10, x20
   23378:	lsl	x12, x10, #3
   2337c:	neg	x10, x11
   23380:	add	x11, x0, x12
   23384:	add	x9, x9, x12
   23388:	ldr	x12, [x9], #8
   2338c:	subs	x10, x10, #0x1
   23390:	str	x12, [x11], #8
   23394:	b.ne	23388 <__gmpz_ui_sub@@Base+0x2ec>  // b.any
   23398:	b	23404 <__gmpz_ui_sub@@Base+0x368>
   2339c:	cmn	w20, #0x2
   233a0:	mov	x13, xzr
   233a4:	b.gt	23408 <__gmpz_ui_sub@@Base+0x36c>
   233a8:	cmp	x9, x0
   233ac:	b.eq	23408 <__gmpz_ui_sub@@Base+0x36c>  // b.none
   233b0:	cmn	w20, #0x5
   233b4:	b.hi	233dc <__gmpz_ui_sub@@Base+0x340>  // b.pmore
   233b8:	lsl	x10, x8, #3
   233bc:	add	x11, x0, #0x8
   233c0:	add	x12, x9, x10
   233c4:	cmp	x11, x12
   233c8:	b.cs	2344c <__gmpz_ui_sub@@Base+0x3b0>  // b.hs, b.nlast
   233cc:	add	x10, x0, x10
   233d0:	add	x11, x9, #0x8
   233d4:	cmp	x11, x10
   233d8:	b.cs	2344c <__gmpz_ui_sub@@Base+0x3b0>  // b.hs, b.nlast
   233dc:	mov	w10, #0x1                   	// #1
   233e0:	add	x11, x10, x20
   233e4:	lsl	x12, x10, #3
   233e8:	neg	x10, x11
   233ec:	add	x11, x0, x12
   233f0:	add	x9, x9, x12
   233f4:	ldr	x12, [x9], #8
   233f8:	subs	x10, x10, #0x1
   233fc:	str	x12, [x11], #8
   23400:	b.ne	233f4 <__gmpz_ui_sub@@Base+0x358>  // b.any
   23404:	mov	x13, xzr
   23408:	str	x13, [x0, x8, lsl #3]
   2340c:	sub	w8, w13, w20
   23410:	b	234b8 <__gmpz_ui_sub@@Base+0x41c>
   23414:	and	x11, x9, #0xfffffffffffffffc
   23418:	add	x12, x8, #0x18
   2341c:	orr	x10, x11, #0x1
   23420:	add	x13, x0, #0x18
   23424:	mov	x14, x11
   23428:	ldp	q0, q1, [x12, #-16]
   2342c:	add	x12, x12, #0x20
   23430:	subs	x14, x14, #0x4
   23434:	stp	q0, q1, [x13, #-16]
   23438:	add	x13, x13, #0x20
   2343c:	b.ne	23428 <__gmpz_ui_sub@@Base+0x38c>  // b.any
   23440:	cmp	x9, x11
   23444:	b.eq	23264 <__gmpz_ui_sub@@Base+0x1c8>  // b.none
   23448:	b	23244 <__gmpz_ui_sub@@Base+0x1a8>
   2344c:	mvn	x11, x20
   23450:	and	x12, x11, #0xfffffffffffffffc
   23454:	add	x13, x9, #0x18
   23458:	orr	x10, x12, #0x1
   2345c:	add	x14, x0, #0x18
   23460:	mov	x15, x12
   23464:	ldp	q0, q1, [x13, #-16]
   23468:	add	x13, x13, #0x20
   2346c:	subs	x15, x15, #0x4
   23470:	stp	q0, q1, [x14, #-16]
   23474:	add	x14, x14, #0x20
   23478:	b.ne	23464 <__gmpz_ui_sub@@Base+0x3c8>  // b.any
   2347c:	cmp	x12, x11
   23480:	b.eq	23404 <__gmpz_ui_sub@@Base+0x368>  // b.none
   23484:	b	233e0 <__gmpz_ui_sub@@Base+0x344>
   23488:	mov	x0, x19
   2348c:	mov	x1, x20
   23490:	bl	c080 <__gmpz_realloc@plt>
   23494:	b	230d8 <__gmpz_ui_sub@@Base+0x3c>
   23498:	mov	w1, #0x1                   	// #1
   2349c:	mov	x0, x19
   234a0:	bl	c080 <__gmpz_realloc@plt>
   234a4:	subs	x8, x20, x21
   234a8:	b.hi	231fc <__gmpz_ui_sub@@Base+0x160>  // b.pmore
   234ac:	subs	x8, x21, x20
   234b0:	str	x8, [x0]
   234b4:	cset	w8, ne  // ne = any
   234b8:	str	w8, [x19, #4]
   234bc:	ldp	x20, x19, [sp, #48]
   234c0:	ldp	x22, x21, [sp, #32]
   234c4:	ldp	x29, x30, [sp, #16]
   234c8:	add	sp, sp, #0x40
   234cc:	ret
   234d0:	mov	x0, x19
   234d4:	str	x8, [sp, #8]
   234d8:	bl	c080 <__gmpz_realloc@plt>
   234dc:	ldr	x8, [sp, #8]
   234e0:	b	23298 <__gmpz_ui_sub@@Base+0x1fc>

00000000000234e4 <__gmpz_urandomb@@Base>:
   234e4:	stp	x29, x30, [sp, #-64]!
   234e8:	stp	x22, x21, [sp, #32]
   234ec:	stp	x20, x19, [sp, #48]
   234f0:	ldrsw	x8, [x0]
   234f4:	add	x9, x2, #0x3f
   234f8:	lsr	x20, x9, #6
   234fc:	mov	x19, x0
   23500:	mov	x21, x2
   23504:	cmp	x20, x8
   23508:	mov	x22, x1
   2350c:	str	x23, [sp, #16]
   23510:	mov	x29, sp
   23514:	b.gt	23570 <__gmpz_urandomb@@Base+0x8c>
   23518:	ldr	x23, [x19, #8]
   2351c:	ldr	x8, [x22, #24]
   23520:	mov	x0, x22
   23524:	mov	x1, x23
   23528:	mov	x2, x21
   2352c:	ldr	x8, [x8, #8]
   23530:	blr	x8
   23534:	sub	x8, x23, #0x8
   23538:	subs	x9, x20, #0x1
   2353c:	b.lt	23554 <__gmpz_urandomb@@Base+0x70>  // b.tstop
   23540:	ldr	x10, [x8, x20, lsl #3]
   23544:	mov	x20, x9
   23548:	cbz	x10, 23538 <__gmpz_urandomb@@Base+0x54>
   2354c:	add	x8, x9, #0x1
   23550:	b	23558 <__gmpz_urandomb@@Base+0x74>
   23554:	mov	x8, xzr
   23558:	str	w8, [x19, #4]
   2355c:	ldp	x20, x19, [sp, #48]
   23560:	ldp	x22, x21, [sp, #32]
   23564:	ldr	x23, [sp, #16]
   23568:	ldp	x29, x30, [sp], #64
   2356c:	ret
   23570:	mov	x0, x19
   23574:	mov	x1, x20
   23578:	bl	c080 <__gmpz_realloc@plt>
   2357c:	mov	x23, x0
   23580:	b	2351c <__gmpz_urandomb@@Base+0x38>

0000000000023584 <__gmpz_urandomm@@Base>:
   23584:	stp	x29, x30, [sp, #-80]!
   23588:	stp	x26, x25, [sp, #16]
   2358c:	stp	x24, x23, [sp, #32]
   23590:	stp	x22, x21, [sp, #48]
   23594:	stp	x20, x19, [sp, #64]
   23598:	mov	x29, sp
   2359c:	sub	sp, sp, #0x10
   235a0:	ldr	w8, [x2, #4]
   235a4:	cmp	w8, #0x0
   235a8:	cneg	w20, w8, mi  // mi = first
   235ac:	cbz	w20, 23754 <__gmpz_urandomm@@Base+0x1d0>
   235b0:	ldr	x22, [x2, #8]
   235b4:	sub	x25, x20, #0x1
   235b8:	mov	x21, x1
   235bc:	mov	x19, x0
   235c0:	ldr	x8, [x22, x25, lsl #3]
   235c4:	sub	x9, x8, #0x1
   235c8:	tst	x8, x9
   235cc:	b.ne	235fc <__gmpz_urandomm@@Base+0x78>  // b.any
   235d0:	cmp	w20, #0x1
   235d4:	b.eq	235f4 <__gmpz_urandomm@@Base+0x70>  // b.none
   235d8:	sub	x9, x22, #0x10
   235dc:	mov	x10, x20
   235e0:	ldr	x11, [x9, x10, lsl #3]
   235e4:	cbnz	x11, 235fc <__gmpz_urandomm@@Base+0x78>
   235e8:	sub	x10, x10, #0x1
   235ec:	cmp	x10, #0x1
   235f0:	b.ne	235e0 <__gmpz_urandomm@@Base+0x5c>  // b.any
   235f4:	mov	x9, #0xffffffffffffffff    	// #-1
   235f8:	b	23600 <__gmpz_urandomm@@Base+0x7c>
   235fc:	mov	x9, xzr
   23600:	clz	x8, x8
   23604:	lsl	x10, x20, #6
   23608:	sub	x8, x10, x8
   2360c:	adds	x23, x9, x8
   23610:	b.eq	236f4 <__gmpz_urandomm@@Base+0x170>  // b.none
   23614:	cmp	x19, x2
   23618:	stur	xzr, [x29, #-8]
   2361c:	b.ne	23654 <__gmpz_urandomm@@Base+0xd0>  // b.any
   23620:	cmp	w20, #0xfe0
   23624:	lsl	x1, x20, #3
   23628:	b.hi	23744 <__gmpz_urandomm@@Base+0x1c0>  // b.pmore
   2362c:	add	x9, x1, #0xf
   23630:	mov	x8, sp
   23634:	and	x9, x9, #0xffffffff0
   23638:	sub	x24, x8, x9
   2363c:	mov	sp, x24
   23640:	mov	x0, x24
   23644:	mov	x1, x22
   23648:	mov	x2, x20
   2364c:	bl	ca50 <__gmpn_copyi@plt>
   23650:	mov	x22, x24
   23654:	ldrsw	x8, [x19]
   23658:	cmp	x20, x8
   2365c:	b.gt	23728 <__gmpz_urandomm@@Base+0x1a4>
   23660:	ldr	x24, [x19, #8]
   23664:	mov	w26, #0x50                  	// #80
   23668:	str	xzr, [x24, x25, lsl #3]
   2366c:	b	23678 <__gmpz_urandomm@@Base+0xf4>
   23670:	subs	w26, w26, #0x1
   23674:	b.eq	236c0 <__gmpz_urandomm@@Base+0x13c>  // b.none
   23678:	ldr	x8, [x21, #24]
   2367c:	mov	x0, x21
   23680:	mov	x1, x24
   23684:	mov	x2, x23
   23688:	ldr	x8, [x8, #8]
   2368c:	blr	x8
   23690:	mov	x8, x25
   23694:	add	x9, x8, #0x1
   23698:	cmp	x9, #0x1
   2369c:	b.lt	23670 <__gmpz_urandomm@@Base+0xec>  // b.tstop
   236a0:	lsl	x9, x8, #3
   236a4:	ldr	x10, [x24, x9]
   236a8:	ldr	x9, [x22, x9]
   236ac:	sub	x8, x8, #0x1
   236b0:	cmp	x10, x9
   236b4:	b.eq	23694 <__gmpz_urandomm@@Base+0x110>  // b.none
   236b8:	b.hi	23670 <__gmpz_urandomm@@Base+0xec>  // b.pmore
   236bc:	cbnz	w26, 236d4 <__gmpz_urandomm@@Base+0x150>
   236c0:	mov	x0, x24
   236c4:	mov	x1, x24
   236c8:	mov	x2, x22
   236cc:	mov	x3, x20
   236d0:	bl	c2d0 <__gmpn_sub_n@plt>
   236d4:	sub	x8, x24, #0x8
   236d8:	subs	x9, x20, #0x1
   236dc:	b.lt	236fc <__gmpz_urandomm@@Base+0x178>  // b.tstop
   236e0:	ldr	x10, [x8, x20, lsl #3]
   236e4:	mov	x20, x9
   236e8:	cbz	x10, 236d8 <__gmpz_urandomm@@Base+0x154>
   236ec:	add	x8, x9, #0x1
   236f0:	b	23700 <__gmpz_urandomm@@Base+0x17c>
   236f4:	str	wzr, [x19, #4]
   236f8:	b	2370c <__gmpz_urandomm@@Base+0x188>
   236fc:	mov	x8, xzr
   23700:	str	w8, [x19, #4]
   23704:	ldur	x0, [x29, #-8]
   23708:	cbnz	x0, 2373c <__gmpz_urandomm@@Base+0x1b8>
   2370c:	mov	sp, x29
   23710:	ldp	x20, x19, [sp, #64]
   23714:	ldp	x22, x21, [sp, #48]
   23718:	ldp	x24, x23, [sp, #32]
   2371c:	ldp	x26, x25, [sp, #16]
   23720:	ldp	x29, x30, [sp], #80
   23724:	ret
   23728:	mov	x0, x19
   2372c:	mov	x1, x20
   23730:	bl	c080 <__gmpz_realloc@plt>
   23734:	mov	x24, x0
   23738:	b	23664 <__gmpz_urandomm@@Base+0xe0>
   2373c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   23740:	b	2370c <__gmpz_urandomm@@Base+0x188>
   23744:	sub	x0, x29, #0x8
   23748:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2374c:	mov	x24, x0
   23750:	b	23640 <__gmpz_urandomm@@Base+0xbc>
   23754:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000023758 <__gmpz_xor@@Base>:
   23758:	stp	x29, x30, [sp, #-96]!
   2375c:	stp	x28, x27, [sp, #16]
   23760:	stp	x26, x25, [sp, #32]
   23764:	stp	x24, x23, [sp, #48]
   23768:	stp	x22, x21, [sp, #64]
   2376c:	stp	x20, x19, [sp, #80]
   23770:	mov	x29, sp
   23774:	sub	sp, sp, #0x10
   23778:	ldr	w8, [x1, #4]
   2377c:	ldr	w9, [x2, #4]
   23780:	ldr	x24, [x0, #8]
   23784:	mov	x19, x0
   23788:	cmp	w8, w9
   2378c:	csel	x25, x2, x1, lt  // lt = tstop
   23790:	ldr	x23, [x25, #8]
   23794:	csel	w10, w8, w9, gt
   23798:	csel	w8, w8, w9, lt  // lt = tstop
   2379c:	sxtw	x20, w10
   237a0:	sxtw	x21, w8
   237a4:	csel	x27, x1, x2, lt  // lt = tstop
   237a8:	tbnz	w8, #31, 23814 <__gmpz_xor@@Base+0xbc>
   237ac:	cmp	x24, x23
   237b0:	mov	x22, x23
   237b4:	b.eq	237dc <__gmpz_xor@@Base+0x84>  // b.none
   237b8:	ldr	w8, [x19]
   237bc:	cmp	w20, w8
   237c0:	b.gt	23e7c <__gmpz_xor@@Base+0x724>
   237c4:	lsl	x8, x21, #3
   237c8:	add	x0, x24, x8
   237cc:	add	x1, x23, x8
   237d0:	sub	x2, x20, x21
   237d4:	bl	ca50 <__gmpn_copyi@plt>
   237d8:	mov	x22, x24
   237dc:	cbz	w21, 237f4 <__gmpz_xor@@Base+0x9c>
   237e0:	ldr	x2, [x27, #8]
   237e4:	mov	x0, x22
   237e8:	mov	x1, x23
   237ec:	mov	x3, x21
   237f0:	bl	cb40 <__gmpn_xor_n@plt>
   237f4:	sub	x8, x22, #0x8
   237f8:	mov	x9, x20
   237fc:	subs	x20, x20, #0x1
   23800:	b.lt	2380c <__gmpz_xor@@Base+0xb4>  // b.tstop
   23804:	ldr	x10, [x8, x9, lsl #3]
   23808:	cbz	x10, 237f8 <__gmpz_xor@@Base+0xa0>
   2380c:	str	w9, [x19, #4]
   23810:	b	23e5c <__gmpz_xor@@Base+0x704>
   23814:	neg	x22, x21
   23818:	stur	xzr, [x29, #-8]
   2381c:	tbnz	w20, #31, 238cc <__gmpz_xor@@Base+0x174>
   23820:	ldrsw	x8, [x19]
   23824:	cmp	x22, x20
   23828:	csel	x28, x20, x22, lt  // lt = tstop
   2382c:	cmp	x28, x8
   23830:	b.ge	23e90 <__gmpz_xor@@Base+0x738>  // b.tcont
   23834:	cmp	x22, #0xfe0
   23838:	lsl	x26, x22, #3
   2383c:	b.hi	23ea8 <__gmpz_xor@@Base+0x750>  // b.pmore
   23840:	add	x9, x26, #0xf
   23844:	mov	x8, sp
   23848:	and	x9, x9, #0xfffffffffffffff0
   2384c:	sub	x25, x8, x9
   23850:	mov	sp, x25
   23854:	ldr	x8, [x27, #8]
   23858:	ldr	x9, [x8]
   2385c:	sub	x10, x9, #0x1
   23860:	str	x10, [x25]
   23864:	cbz	x9, 23970 <__gmpz_xor@@Base+0x218>
   23868:	cmn	w21, #0x2
   2386c:	b.gt	23b70 <__gmpz_xor@@Base+0x418>
   23870:	cmp	x8, x25
   23874:	b.eq	23b70 <__gmpz_xor@@Base+0x418>  // b.none
   23878:	cmn	w21, #0x5
   2387c:	b.hi	238a0 <__gmpz_xor@@Base+0x148>  // b.pmore
   23880:	add	x9, x25, #0x8
   23884:	add	x10, x8, x22, lsl #3
   23888:	cmp	x9, x10
   2388c:	b.cs	23b38 <__gmpz_xor@@Base+0x3e0>  // b.hs, b.nlast
   23890:	sub	x9, x25, x21, lsl #3
   23894:	add	x10, x8, #0x8
   23898:	cmp	x9, x10
   2389c:	b.ls	23b38 <__gmpz_xor@@Base+0x3e0>  // b.plast
   238a0:	mov	w9, #0x1                   	// #1
   238a4:	add	x10, x9, x21
   238a8:	lsl	x11, x9, #3
   238ac:	neg	x9, x10
   238b0:	add	x10, x25, x11
   238b4:	add	x8, x8, x11
   238b8:	ldr	x11, [x8], #8
   238bc:	subs	x9, x9, #0x1
   238c0:	str	x11, [x10], #8
   238c4:	b.ne	238b8 <__gmpz_xor@@Base+0x160>  // b.any
   238c8:	b	23b70 <__gmpz_xor@@Base+0x418>
   238cc:	add	x8, x20, x21
   238d0:	neg	x1, x8, lsl #3
   238d4:	mov	w8, #0x7f00                	// #32512
   238d8:	cmp	x1, x8
   238dc:	neg	x24, x20
   238e0:	b.hi	23ec4 <__gmpz_xor@@Base+0x76c>  // b.pmore
   238e4:	add	x9, x1, #0xf
   238e8:	mov	x8, sp
   238ec:	and	x9, x9, #0xfffffffffffffff0
   238f0:	sub	x25, x8, x9
   238f4:	mov	sp, x25
   238f8:	ldr	x8, [x23]
   238fc:	add	x26, x25, x24, lsl #3
   23900:	sub	x9, x8, #0x1
   23904:	str	x9, [x25]
   23908:	cbz	x8, 23a54 <__gmpz_xor@@Base+0x2fc>
   2390c:	cmn	w20, #0x2
   23910:	b.gt	23c34 <__gmpz_xor@@Base+0x4dc>
   23914:	cmp	x23, x25
   23918:	b.eq	23c34 <__gmpz_xor@@Base+0x4dc>  // b.none
   2391c:	cmn	w20, #0x5
   23920:	b.hi	23944 <__gmpz_xor@@Base+0x1ec>  // b.pmore
   23924:	add	x8, x25, #0x8
   23928:	add	x9, x23, x24, lsl #3
   2392c:	cmp	x8, x9
   23930:	b.cs	23bfc <__gmpz_xor@@Base+0x4a4>  // b.hs, b.nlast
   23934:	sub	x8, x25, x20, lsl #3
   23938:	add	x9, x23, #0x8
   2393c:	cmp	x8, x9
   23940:	b.ls	23bfc <__gmpz_xor@@Base+0x4a4>  // b.plast
   23944:	mov	w8, #0x1                   	// #1
   23948:	add	x9, x8, x20
   2394c:	lsl	x10, x8, #3
   23950:	neg	x8, x9
   23954:	add	x9, x25, x10
   23958:	add	x10, x23, x10
   2395c:	ldr	x11, [x10], #8
   23960:	subs	x8, x8, #0x1
   23964:	str	x11, [x9], #8
   23968:	b.ne	2395c <__gmpz_xor@@Base+0x204>  // b.any
   2396c:	b	23c34 <__gmpz_xor@@Base+0x4dc>
   23970:	mov	x11, xzr
   23974:	mvn	x10, x21
   23978:	mov	w9, #0x1                   	// #1
   2397c:	cmp	x9, x22
   23980:	b.ge	23b70 <__gmpz_xor@@Base+0x418>  // b.tcont
   23984:	add	x12, x8, x11
   23988:	ldr	x12, [x12, #8]
   2398c:	add	x13, x25, x11
   23990:	add	x9, x9, #0x1
   23994:	add	x11, x11, #0x8
   23998:	sub	x14, x12, #0x1
   2399c:	sub	x10, x10, #0x1
   239a0:	str	x14, [x13, #8]
   239a4:	cbz	x12, 2397c <__gmpz_xor@@Base+0x224>
   239a8:	cmp	x8, x25
   239ac:	b.eq	23b70 <__gmpz_xor@@Base+0x418>  // b.none
   239b0:	cmp	x9, x22
   239b4:	b.ge	23b70 <__gmpz_xor@@Base+0x418>  // b.tcont
   239b8:	sub	x12, x22, x9
   239bc:	cmp	x12, #0x4
   239c0:	b.cc	23a2c <__gmpz_xor@@Base+0x2d4>  // b.lo, b.ul, b.last
   239c4:	add	x13, x25, x11
   239c8:	add	x13, x13, #0x8
   239cc:	add	x14, x8, x22, lsl #3
   239d0:	cmp	x13, x14
   239d4:	b.cs	239ec <__gmpz_xor@@Base+0x294>  // b.hs, b.nlast
   239d8:	add	x14, x8, x11
   239dc:	sub	x13, x25, x21, lsl #3
   239e0:	add	x14, x14, #0x8
   239e4:	cmp	x13, x14
   239e8:	b.hi	23a2c <__gmpz_xor@@Base+0x2d4>  // b.pmore
   239ec:	add	x13, x25, x11
   239f0:	add	x14, x8, x11
   239f4:	and	x11, x12, #0xfffffffffffffffc
   239f8:	and	x15, x10, #0xfffffffffffffffc
   239fc:	add	x10, x13, #0x18
   23a00:	add	x13, x14, #0x18
   23a04:	add	x9, x15, x9
   23a08:	mov	x14, x11
   23a0c:	ldp	q0, q1, [x13, #-16]
   23a10:	add	x13, x13, #0x20
   23a14:	subs	x14, x14, #0x4
   23a18:	stp	q0, q1, [x10, #-16]
   23a1c:	add	x10, x10, #0x20
   23a20:	b.ne	23a0c <__gmpz_xor@@Base+0x2b4>  // b.any
   23a24:	cmp	x12, x11
   23a28:	b.eq	23b70 <__gmpz_xor@@Base+0x418>  // b.none
   23a2c:	add	x10, x9, x21
   23a30:	lsl	x11, x9, #3
   23a34:	neg	x9, x10
   23a38:	add	x10, x25, x11
   23a3c:	add	x8, x8, x11
   23a40:	ldr	x11, [x8], #8
   23a44:	subs	x9, x9, #0x1
   23a48:	str	x11, [x10], #8
   23a4c:	b.ne	23a40 <__gmpz_xor@@Base+0x2e8>  // b.any
   23a50:	b	23b70 <__gmpz_xor@@Base+0x418>
   23a54:	mov	x10, xzr
   23a58:	mvn	x9, x20
   23a5c:	mov	w8, #0x1                   	// #1
   23a60:	cmp	x8, x24
   23a64:	b.ge	23c34 <__gmpz_xor@@Base+0x4dc>  // b.tcont
   23a68:	add	x11, x23, x10
   23a6c:	ldr	x11, [x11, #8]
   23a70:	add	x12, x25, x10
   23a74:	add	x8, x8, #0x1
   23a78:	add	x10, x10, #0x8
   23a7c:	sub	x13, x11, #0x1
   23a80:	sub	x9, x9, #0x1
   23a84:	str	x13, [x12, #8]
   23a88:	cbz	x11, 23a60 <__gmpz_xor@@Base+0x308>
   23a8c:	cmp	x23, x25
   23a90:	b.eq	23c34 <__gmpz_xor@@Base+0x4dc>  // b.none
   23a94:	cmp	x8, x24
   23a98:	b.ge	23c34 <__gmpz_xor@@Base+0x4dc>  // b.tcont
   23a9c:	sub	x11, x24, x8
   23aa0:	cmp	x11, #0x4
   23aa4:	b.cc	23b10 <__gmpz_xor@@Base+0x3b8>  // b.lo, b.ul, b.last
   23aa8:	add	x12, x25, x10
   23aac:	add	x12, x12, #0x8
   23ab0:	add	x13, x23, x24, lsl #3
   23ab4:	cmp	x12, x13
   23ab8:	b.cs	23ad0 <__gmpz_xor@@Base+0x378>  // b.hs, b.nlast
   23abc:	add	x13, x23, x10
   23ac0:	sub	x12, x25, x20, lsl #3
   23ac4:	add	x13, x13, #0x8
   23ac8:	cmp	x12, x13
   23acc:	b.hi	23b10 <__gmpz_xor@@Base+0x3b8>  // b.pmore
   23ad0:	add	x12, x25, x10
   23ad4:	add	x13, x23, x10
   23ad8:	and	x10, x11, #0xfffffffffffffffc
   23adc:	and	x14, x9, #0xfffffffffffffffc
   23ae0:	add	x9, x12, #0x18
   23ae4:	add	x12, x13, #0x18
   23ae8:	add	x8, x14, x8
   23aec:	mov	x13, x10
   23af0:	ldp	q0, q1, [x12, #-16]
   23af4:	add	x12, x12, #0x20
   23af8:	subs	x13, x13, #0x4
   23afc:	stp	q0, q1, [x9, #-16]
   23b00:	add	x9, x9, #0x20
   23b04:	b.ne	23af0 <__gmpz_xor@@Base+0x398>  // b.any
   23b08:	cmp	x11, x10
   23b0c:	b.eq	23c34 <__gmpz_xor@@Base+0x4dc>  // b.none
   23b10:	add	x9, x8, x20
   23b14:	lsl	x10, x8, #3
   23b18:	neg	x8, x9
   23b1c:	add	x9, x25, x10
   23b20:	add	x10, x23, x10
   23b24:	ldr	x11, [x10], #8
   23b28:	subs	x8, x8, #0x1
   23b2c:	str	x11, [x9], #8
   23b30:	b.ne	23b24 <__gmpz_xor@@Base+0x3cc>  // b.any
   23b34:	b	23c34 <__gmpz_xor@@Base+0x4dc>
   23b38:	mvn	x10, x21
   23b3c:	and	x11, x10, #0xfffffffffffffffc
   23b40:	add	x12, x8, #0x18
   23b44:	orr	x9, x11, #0x1
   23b48:	add	x13, x25, #0x18
   23b4c:	mov	x14, x11
   23b50:	ldp	q0, q1, [x12, #-16]
   23b54:	add	x12, x12, #0x20
   23b58:	subs	x14, x14, #0x4
   23b5c:	stp	q0, q1, [x13, #-16]
   23b60:	add	x13, x13, #0x20
   23b64:	b.ne	23b50 <__gmpz_xor@@Base+0x3f8>  // b.any
   23b68:	cmp	x11, x10
   23b6c:	b.ne	238a4 <__gmpz_xor@@Base+0x14c>  // b.any
   23b70:	subs	x2, x22, x20
   23b74:	b.le	23b90 <__gmpz_xor@@Base+0x438>
   23b78:	lsl	x8, x20, #3
   23b7c:	add	x0, x24, x8
   23b80:	add	x1, x25, x8
   23b84:	bl	ca50 <__gmpn_copyi@plt>
   23b88:	cbnz	w20, 23ba4 <__gmpz_xor@@Base+0x44c>
   23b8c:	b	23bb8 <__gmpz_xor@@Base+0x460>
   23b90:	add	x0, x24, x26
   23b94:	add	x1, x23, x26
   23b98:	add	x2, x20, x21
   23b9c:	bl	ca50 <__gmpn_copyi@plt>
   23ba0:	mov	x20, x22
   23ba4:	mov	x0, x24
   23ba8:	mov	x1, x23
   23bac:	mov	x2, x25
   23bb0:	mov	x3, x20
   23bb4:	bl	cb40 <__gmpn_xor_n@plt>
   23bb8:	ldur	x0, [x29, #-8]
   23bbc:	cbnz	x0, 23ebc <__gmpz_xor@@Base+0x764>
   23bc0:	mov	x8, x24
   23bc4:	str	xzr, [x24, x28, lsl #3]
   23bc8:	ldr	x9, [x8]
   23bcc:	adds	x9, x9, #0x1
   23bd0:	str	x9, [x8], #8
   23bd4:	b.cs	23bc8 <__gmpz_xor@@Base+0x470>  // b.hs, b.nlast
   23bd8:	ldr	x8, [x24, x28, lsl #3]
   23bdc:	add	x9, x8, x28
   23be0:	mvn	w8, w9
   23be4:	add	x9, x24, x9, lsl #3
   23be8:	sub	x9, x9, #0x8
   23bec:	ldr	x10, [x9], #-8
   23bf0:	add	w8, w8, #0x1
   23bf4:	cbz	x10, 23bec <__gmpz_xor@@Base+0x494>
   23bf8:	b	23e58 <__gmpz_xor@@Base+0x700>
   23bfc:	mvn	x9, x20
   23c00:	and	x10, x9, #0xfffffffffffffffc
   23c04:	add	x11, x23, #0x18
   23c08:	orr	x8, x10, #0x1
   23c0c:	add	x12, x25, #0x18
   23c10:	mov	x13, x10
   23c14:	ldp	q0, q1, [x11, #-16]
   23c18:	add	x11, x11, #0x20
   23c1c:	subs	x13, x13, #0x4
   23c20:	stp	q0, q1, [x12, #-16]
   23c24:	add	x12, x12, #0x20
   23c28:	b.ne	23c14 <__gmpz_xor@@Base+0x4bc>  // b.any
   23c2c:	cmp	x10, x9
   23c30:	b.ne	23948 <__gmpz_xor@@Base+0x1f0>  // b.any
   23c34:	ldr	x8, [x27, #8]
   23c38:	ldr	x9, [x8]
   23c3c:	sub	x10, x9, #0x1
   23c40:	str	x10, [x26]
   23c44:	cbz	x9, 23cbc <__gmpz_xor@@Base+0x564>
   23c48:	cmn	w21, #0x2
   23c4c:	b.gt	23dfc <__gmpz_xor@@Base+0x6a4>
   23c50:	cmp	x8, x26
   23c54:	b.eq	23dfc <__gmpz_xor@@Base+0x6a4>  // b.none
   23c58:	cmn	w21, #0x5
   23c5c:	b.hi	23c8c <__gmpz_xor@@Base+0x534>  // b.pmore
   23c60:	lsl	x9, x20, #3
   23c64:	sub	x10, x25, x9
   23c68:	add	x10, x10, #0x8
   23c6c:	add	x11, x8, x22, lsl #3
   23c70:	cmp	x10, x11
   23c74:	b.cs	23dc0 <__gmpz_xor@@Base+0x668>  // b.hs, b.nlast
   23c78:	add	x9, x9, x21, lsl #3
   23c7c:	sub	x9, x25, x9
   23c80:	add	x10, x8, #0x8
   23c84:	cmp	x9, x10
   23c88:	b.ls	23dc0 <__gmpz_xor@@Base+0x668>  // b.plast
   23c8c:	mov	w9, #0x1                   	// #1
   23c90:	add	x10, x9, x21
   23c94:	lsl	x11, x9, #3
   23c98:	neg	x9, x10
   23c9c:	sub	x10, x11, x20, lsl #3
   23ca0:	add	x10, x25, x10
   23ca4:	add	x8, x8, x11
   23ca8:	ldr	x11, [x8], #8
   23cac:	subs	x9, x9, #0x1
   23cb0:	str	x11, [x10], #8
   23cb4:	b.ne	23ca8 <__gmpz_xor@@Base+0x550>  // b.any
   23cb8:	b	23dfc <__gmpz_xor@@Base+0x6a4>
   23cbc:	mov	w9, #0x20                  	// #32
   23cc0:	sub	x12, x9, x20, lsl #3
   23cc4:	add	x9, x12, x25
   23cc8:	mov	x10, xzr
   23ccc:	mvn	x11, x21
   23cd0:	sub	x13, x9, #0x20
   23cd4:	mov	w9, #0x1                   	// #1
   23cd8:	cmp	x9, x22
   23cdc:	b.ge	23dfc <__gmpz_xor@@Base+0x6a4>  // b.tcont
   23ce0:	add	x14, x8, x10
   23ce4:	ldr	x14, [x14, #8]
   23ce8:	add	x15, x13, x10
   23cec:	add	x9, x9, #0x1
   23cf0:	add	x10, x10, #0x8
   23cf4:	sub	x16, x14, #0x1
   23cf8:	sub	x11, x11, #0x1
   23cfc:	str	x16, [x15, #8]
   23d00:	cbz	x14, 23cd8 <__gmpz_xor@@Base+0x580>
   23d04:	cmp	x8, x26
   23d08:	b.eq	23dfc <__gmpz_xor@@Base+0x6a4>  // b.none
   23d0c:	cmp	x9, x22
   23d10:	b.ge	23dfc <__gmpz_xor@@Base+0x6a4>  // b.tcont
   23d14:	sub	x13, x22, x9
   23d18:	cmp	x13, #0x4
   23d1c:	b.cc	23d94 <__gmpz_xor@@Base+0x63c>  // b.lo, b.ul, b.last
   23d20:	add	x14, x12, x25
   23d24:	add	x14, x14, x10
   23d28:	sub	x14, x14, #0x18
   23d2c:	add	x15, x8, x22, lsl #3
   23d30:	cmp	x14, x15
   23d34:	b.cs	23d50 <__gmpz_xor@@Base+0x5f8>  // b.hs, b.nlast
   23d38:	add	x14, x21, x20
   23d3c:	add	x15, x8, x10
   23d40:	sub	x14, x25, x14, lsl #3
   23d44:	add	x15, x15, #0x8
   23d48:	cmp	x14, x15
   23d4c:	b.hi	23d94 <__gmpz_xor@@Base+0x63c>  // b.pmore
   23d50:	add	x14, x12, x25
   23d54:	add	x15, x8, x10
   23d58:	and	x12, x13, #0xfffffffffffffffc
   23d5c:	and	x16, x11, #0xfffffffffffffffc
   23d60:	add	x11, x14, x10
   23d64:	add	x10, x15, #0x18
   23d68:	sub	x11, x11, #0x8
   23d6c:	add	x9, x16, x9
   23d70:	mov	x14, x12
   23d74:	ldp	q0, q1, [x10, #-16]
   23d78:	add	x10, x10, #0x20
   23d7c:	subs	x14, x14, #0x4
   23d80:	stp	q0, q1, [x11, #-16]
   23d84:	add	x11, x11, #0x20
   23d88:	b.ne	23d74 <__gmpz_xor@@Base+0x61c>  // b.any
   23d8c:	cmp	x13, x12
   23d90:	b.eq	23dfc <__gmpz_xor@@Base+0x6a4>  // b.none
   23d94:	add	x10, x9, x21
   23d98:	lsl	x11, x9, #3
   23d9c:	neg	x9, x10
   23da0:	sub	x10, x11, x20, lsl #3
   23da4:	add	x10, x25, x10
   23da8:	add	x8, x8, x11
   23dac:	ldr	x11, [x8], #8
   23db0:	subs	x9, x9, #0x1
   23db4:	str	x11, [x10], #8
   23db8:	b.ne	23dac <__gmpz_xor@@Base+0x654>  // b.any
   23dbc:	b	23dfc <__gmpz_xor@@Base+0x6a4>
   23dc0:	mvn	x10, x21
   23dc4:	sub	x13, x25, x20, lsl #3
   23dc8:	and	x12, x10, #0xfffffffffffffffc
   23dcc:	add	x11, x8, #0x18
   23dd0:	orr	x9, x12, #0x1
   23dd4:	add	x13, x13, #0x18
   23dd8:	mov	x14, x12
   23ddc:	ldp	q0, q1, [x11, #-16]
   23de0:	add	x11, x11, #0x20
   23de4:	subs	x14, x14, #0x4
   23de8:	stp	q0, q1, [x13, #-16]
   23dec:	add	x13, x13, #0x20
   23df0:	b.ne	23ddc <__gmpz_xor@@Base+0x684>  // b.any
   23df4:	cmp	x12, x10
   23df8:	b.ne	23c90 <__gmpz_xor@@Base+0x538>  // b.any
   23dfc:	ldrsw	x8, [x19]
   23e00:	cmp	x22, x8
   23e04:	b.gt	23ed4 <__gmpz_xor@@Base+0x77c>
   23e08:	ldr	x23, [x19, #8]
   23e0c:	lsl	x8, x24, #3
   23e10:	add	x0, x23, x8
   23e14:	add	x1, x26, x8
   23e18:	sub	x2, x20, x21
   23e1c:	bl	ca50 <__gmpn_copyi@plt>
   23e20:	mov	x0, x23
   23e24:	mov	x1, x25
   23e28:	mov	x2, x26
   23e2c:	mov	x3, x24
   23e30:	bl	cb40 <__gmpn_xor_n@plt>
   23e34:	ldur	x0, [x29, #-8]
   23e38:	cbnz	x0, 23ee8 <__gmpz_xor@@Base+0x790>
   23e3c:	sub	x9, x23, #0x8
   23e40:	subs	x10, x22, #0x1
   23e44:	mov	w8, w22
   23e48:	b.lt	23e58 <__gmpz_xor@@Base+0x700>  // b.tstop
   23e4c:	ldr	x11, [x9, x22, lsl #3]
   23e50:	mov	x22, x10
   23e54:	cbz	x11, 23e40 <__gmpz_xor@@Base+0x6e8>
   23e58:	str	w8, [x19, #4]
   23e5c:	mov	sp, x29
   23e60:	ldp	x20, x19, [sp, #80]
   23e64:	ldp	x22, x21, [sp, #64]
   23e68:	ldp	x24, x23, [sp, #48]
   23e6c:	ldp	x26, x25, [sp, #32]
   23e70:	ldp	x28, x27, [sp, #16]
   23e74:	ldp	x29, x30, [sp], #96
   23e78:	ret
   23e7c:	mov	x0, x19
   23e80:	mov	x1, x20
   23e84:	bl	c080 <__gmpz_realloc@plt>
   23e88:	mov	x24, x0
   23e8c:	b	237c4 <__gmpz_xor@@Base+0x6c>
   23e90:	add	x1, x28, #0x1
   23e94:	mov	x0, x19
   23e98:	bl	c080 <__gmpz_realloc@plt>
   23e9c:	ldr	x23, [x25, #8]
   23ea0:	mov	x24, x0
   23ea4:	b	23834 <__gmpz_xor@@Base+0xdc>
   23ea8:	sub	x0, x29, #0x8
   23eac:	mov	x1, x26
   23eb0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   23eb4:	mov	x25, x0
   23eb8:	b	23854 <__gmpz_xor@@Base+0xfc>
   23ebc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   23ec0:	b	23bc0 <__gmpz_xor@@Base+0x468>
   23ec4:	sub	x0, x29, #0x8
   23ec8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   23ecc:	mov	x25, x0
   23ed0:	b	238f8 <__gmpz_xor@@Base+0x1a0>
   23ed4:	mov	x0, x19
   23ed8:	mov	x1, x22
   23edc:	bl	c080 <__gmpz_realloc@plt>
   23ee0:	mov	x23, x0
   23ee4:	b	23e0c <__gmpz_xor@@Base+0x6b4>
   23ee8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   23eec:	b	23e3c <__gmpz_xor@@Base+0x6e4>

0000000000023ef0 <__gmpq_abs@@Base>:
   23ef0:	stp	x29, x30, [sp, #-64]!
   23ef4:	stp	x22, x21, [sp, #32]
   23ef8:	stp	x20, x19, [sp, #48]
   23efc:	ldr	w8, [x1, #4]
   23f00:	mov	x19, x0
   23f04:	str	x23, [sp, #16]
   23f08:	mov	x29, sp
   23f0c:	cmp	w8, #0x0
   23f10:	cneg	w20, w8, mi  // mi = first
   23f14:	cmp	x0, x1
   23f18:	b.eq	23f68 <__gmpq_abs@@Base+0x78>  // b.none
   23f1c:	ldrsw	x8, [x19]
   23f20:	ldr	w23, [x1, #20]
   23f24:	mov	x21, x1
   23f28:	cmp	x20, x8
   23f2c:	sxtw	x22, w23
   23f30:	b.gt	23f80 <__gmpq_abs@@Base+0x90>
   23f34:	ldr	x0, [x19, #8]
   23f38:	ldr	x1, [x21, #8]
   23f3c:	mov	x2, x20
   23f40:	bl	ca50 <__gmpn_copyi@plt>
   23f44:	mov	x0, x19
   23f48:	ldr	w8, [x0, #16]!
   23f4c:	cmp	w23, w8
   23f50:	b.gt	23f90 <__gmpq_abs@@Base+0xa0>
   23f54:	ldr	x0, [x19, #24]
   23f58:	str	w23, [x19, #20]
   23f5c:	ldr	x1, [x21, #24]
   23f60:	mov	x2, x22
   23f64:	bl	ca50 <__gmpn_copyi@plt>
   23f68:	str	w20, [x19, #4]
   23f6c:	ldp	x20, x19, [sp, #48]
   23f70:	ldp	x22, x21, [sp, #32]
   23f74:	ldr	x23, [sp, #16]
   23f78:	ldp	x29, x30, [sp], #64
   23f7c:	ret
   23f80:	mov	x0, x19
   23f84:	mov	x1, x20
   23f88:	bl	c080 <__gmpz_realloc@plt>
   23f8c:	b	23f38 <__gmpq_abs@@Base+0x48>
   23f90:	mov	x1, x22
   23f94:	bl	c080 <__gmpz_realloc@plt>
   23f98:	b	23f58 <__gmpq_abs@@Base+0x68>

0000000000023f9c <__gmpq_add@@Base>:
   23f9c:	adrp	x3, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   23fa0:	ldr	x3, [x3, #4000]
   23fa4:	b	23fa8 <__gmpq_add@@Base+0xc>
   23fa8:	stp	x29, x30, [sp, #-96]!
   23fac:	stp	x28, x27, [sp, #16]
   23fb0:	stp	x26, x25, [sp, #32]
   23fb4:	stp	x24, x23, [sp, #48]
   23fb8:	stp	x22, x21, [sp, #64]
   23fbc:	stp	x20, x19, [sp, #80]
   23fc0:	mov	x29, sp
   23fc4:	sub	sp, sp, #0x50
   23fc8:	ldr	w8, [x1, #4]
   23fcc:	ldr	w9, [x2, #4]
   23fd0:	ldrsw	x26, [x1, #20]
   23fd4:	ldrsw	x25, [x2, #20]
   23fd8:	cmp	w8, #0x0
   23fdc:	cneg	w28, w8, mi  // mi = first
   23fe0:	cmp	w9, #0x0
   23fe4:	cneg	w27, w9, mi  // mi = first
   23fe8:	cmp	x26, x25
   23fec:	csel	x8, x26, x25, lt  // lt = tstop
   23ff0:	mov	x23, x1
   23ff4:	mov	w10, #0x7f00                	// #32512
   23ff8:	lsl	x1, x8, #3
   23ffc:	mov	x21, x3
   24000:	mov	x22, x2
   24004:	mov	x19, x0
   24008:	cmp	x1, x10
   2400c:	stur	xzr, [x29, #-56]
   24010:	stur	w8, [x29, #-16]
   24014:	b.hi	24230 <__gmpq_add@@Base+0x294>  // b.pmore
   24018:	add	x9, x1, #0xf
   2401c:	mov	x8, sp
   24020:	and	x9, x9, #0xfffffffffffffff0
   24024:	sub	x0, x8, x9
   24028:	mov	sp, x0
   2402c:	add	x25, x25, x28
   24030:	lsl	x1, x25, #3
   24034:	mov	w8, #0x7f00                	// #32512
   24038:	add	x24, x23, #0x10
   2403c:	add	x20, x22, #0x10
   24040:	cmp	x1, x8
   24044:	stur	x0, [x29, #-8]
   24048:	stur	w25, [x29, #-32]
   2404c:	b.hi	2423c <__gmpq_add@@Base+0x2a0>  // b.pmore
   24050:	add	x9, x1, #0xf
   24054:	mov	x8, sp
   24058:	and	x9, x9, #0xfffffffffffffff0
   2405c:	sub	x0, x8, x9
   24060:	mov	sp, x0
   24064:	add	x26, x27, x26
   24068:	lsl	x1, x26, #3
   2406c:	mov	w8, #0x7f00                	// #32512
   24070:	cmp	x1, x8
   24074:	stur	x0, [x29, #-24]
   24078:	stur	w26, [x29, #-48]
   2407c:	b.hi	24248 <__gmpq_add@@Base+0x2ac>  // b.pmore
   24080:	add	x9, x1, #0xf
   24084:	mov	x8, sp
   24088:	and	x9, x9, #0xfffffffffffffff0
   2408c:	sub	x0, x8, x9
   24090:	mov	sp, x0
   24094:	stur	x0, [x29, #-40]
   24098:	sub	x0, x29, #0x10
   2409c:	mov	x1, x24
   240a0:	mov	x2, x20
   240a4:	bl	cf70 <__gmpz_gcd@plt>
   240a8:	ldursw	x8, [x29, #-12]
   240ac:	cmp	w8, #0x1
   240b0:	b.ne	24104 <__gmpq_add@@Base+0x168>  // b.any
   240b4:	ldur	x9, [x29, #-8]
   240b8:	ldr	x9, [x9]
   240bc:	cmp	x9, #0x1
   240c0:	b.ne	24104 <__gmpq_add@@Base+0x168>  // b.any
   240c4:	sub	x0, x29, #0x20
   240c8:	mov	x1, x23
   240cc:	mov	x2, x20
   240d0:	bl	c4b0 <__gmpz_mul@plt>
   240d4:	sub	x0, x29, #0x30
   240d8:	mov	x1, x22
   240dc:	mov	x2, x24
   240e0:	bl	c4b0 <__gmpz_mul@plt>
   240e4:	sub	x1, x29, #0x20
   240e8:	sub	x2, x29, #0x30
   240ec:	mov	x0, x19
   240f0:	blr	x21
   240f4:	add	x0, x19, #0x10
   240f8:	mov	x1, x24
   240fc:	mov	x2, x20
   24100:	b	24204 <__gmpq_add@@Base+0x268>
   24104:	cmp	x25, x26
   24108:	csel	x9, x25, x26, gt
   2410c:	sub	w10, w9, w8
   24110:	sub	x8, x9, x8
   24114:	lsl	x8, x8, #3
   24118:	add	x1, x8, #0x10
   2411c:	mov	w8, #0x7f00                	// #32512
   24120:	add	w9, w10, #0x2
   24124:	cmp	x1, x8
   24128:	stur	w9, [x29, #-72]
   2412c:	b.hi	2425c <__gmpq_add@@Base+0x2c0>  // b.pmore
   24130:	add	x9, x1, #0xf
   24134:	mov	x8, sp
   24138:	and	x9, x9, #0xfffffffffffffff0
   2413c:	sub	x0, x8, x9
   24140:	mov	sp, x0
   24144:	stur	x0, [x29, #-64]
   24148:	sub	x0, x29, #0x48
   2414c:	sub	x2, x29, #0x10
   24150:	mov	x1, x20
   24154:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24158:	sub	x0, x29, #0x30
   2415c:	sub	x2, x29, #0x10
   24160:	mov	x1, x24
   24164:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24168:	sub	x0, x29, #0x20
   2416c:	sub	x2, x29, #0x48
   24170:	mov	x1, x23
   24174:	bl	c4b0 <__gmpz_mul@plt>
   24178:	sub	x0, x29, #0x48
   2417c:	sub	x2, x29, #0x30
   24180:	mov	x1, x22
   24184:	bl	c4b0 <__gmpz_mul@plt>
   24188:	sub	x0, x29, #0x48
   2418c:	sub	x1, x29, #0x20
   24190:	sub	x2, x29, #0x48
   24194:	blr	x21
   24198:	sub	x0, x29, #0x10
   2419c:	sub	x1, x29, #0x48
   241a0:	sub	x2, x29, #0x10
   241a4:	bl	cf70 <__gmpz_gcd@plt>
   241a8:	ldur	w8, [x29, #-12]
   241ac:	cmp	w8, #0x1
   241b0:	b.ne	241d4 <__gmpq_add@@Base+0x238>  // b.any
   241b4:	ldur	x8, [x29, #-8]
   241b8:	ldr	x8, [x8]
   241bc:	cmp	x8, #0x1
   241c0:	b.ne	241d4 <__gmpq_add@@Base+0x238>  // b.any
   241c4:	sub	x1, x29, #0x48
   241c8:	mov	x0, x19
   241cc:	bl	c420 <__gmpz_set@plt>
   241d0:	b	241f8 <__gmpq_add@@Base+0x25c>
   241d4:	sub	x1, x29, #0x48
   241d8:	sub	x2, x29, #0x10
   241dc:	mov	x0, x19
   241e0:	bl	ca00 <__gmpz_divexact_gcd@plt>
   241e4:	sub	x0, x29, #0x20
   241e8:	sub	x2, x29, #0x10
   241ec:	mov	x1, x20
   241f0:	bl	ca00 <__gmpz_divexact_gcd@plt>
   241f4:	sub	x20, x29, #0x20
   241f8:	add	x0, x19, #0x10
   241fc:	sub	x2, x29, #0x30
   24200:	mov	x1, x20
   24204:	bl	c4b0 <__gmpz_mul@plt>
   24208:	ldur	x0, [x29, #-56]
   2420c:	cbnz	x0, 24254 <__gmpq_add@@Base+0x2b8>
   24210:	mov	sp, x29
   24214:	ldp	x20, x19, [sp, #80]
   24218:	ldp	x22, x21, [sp, #64]
   2421c:	ldp	x24, x23, [sp, #48]
   24220:	ldp	x26, x25, [sp, #32]
   24224:	ldp	x28, x27, [sp, #16]
   24228:	ldp	x29, x30, [sp], #96
   2422c:	ret
   24230:	sub	x0, x29, #0x38
   24234:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24238:	b	2402c <__gmpq_add@@Base+0x90>
   2423c:	sub	x0, x29, #0x38
   24240:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24244:	b	24064 <__gmpq_add@@Base+0xc8>
   24248:	sub	x0, x29, #0x38
   2424c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24250:	b	24094 <__gmpq_add@@Base+0xf8>
   24254:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24258:	b	24210 <__gmpq_add@@Base+0x274>
   2425c:	sub	x0, x29, #0x38
   24260:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24264:	b	24144 <__gmpq_add@@Base+0x1a8>

0000000000024268 <__gmpq_sub@@Base>:
   24268:	adrp	x3, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2426c:	ldr	x3, [x3, #3832]
   24270:	b	23fa8 <__gmpq_add@@Base+0xc>

0000000000024274 <__gmpq_canonicalize@@Base>:
   24274:	stp	x29, x30, [sp, #-32]!
   24278:	stp	x20, x19, [sp, #16]
   2427c:	mov	x29, sp
   24280:	sub	sp, sp, #0x20
   24284:	ldr	w8, [x0, #20]
   24288:	mov	x19, x0
   2428c:	tbnz	w8, #31, 2429c <__gmpq_canonicalize@@Base+0x28>
   24290:	cbz	w8, 2436c <__gmpq_canonicalize@@Base+0xf8>
   24294:	ldr	w9, [x19, #4]
   24298:	b	242b0 <__gmpq_canonicalize@@Base+0x3c>
   2429c:	ldr	w9, [x19, #4]
   242a0:	neg	w8, w8
   242a4:	str	w8, [x19, #20]
   242a8:	neg	w9, w9
   242ac:	str	w9, [x19, #4]
   242b0:	cmp	w9, #0x0
   242b4:	cneg	w9, w9, mi  // mi = first
   242b8:	cmp	w9, w8
   242bc:	csel	w8, w9, w8, gt
   242c0:	add	w9, w8, #0x1
   242c4:	add	x20, x19, #0x10
   242c8:	cmp	w8, #0xfdf
   242cc:	lsl	x1, x9, #3
   242d0:	stur	xzr, [x29, #-24]
   242d4:	stur	w9, [x29, #-16]
   242d8:	b.hi	24358 <__gmpq_canonicalize@@Base+0xe4>  // b.pmore
   242dc:	add	x9, x1, #0xf
   242e0:	mov	x8, sp
   242e4:	and	x9, x9, #0xffffffff0
   242e8:	sub	x0, x8, x9
   242ec:	mov	sp, x0
   242f0:	stur	x0, [x29, #-8]
   242f4:	sub	x0, x29, #0x10
   242f8:	mov	x1, x19
   242fc:	mov	x2, x20
   24300:	bl	cf70 <__gmpz_gcd@plt>
   24304:	ldur	w8, [x29, #-12]
   24308:	cmp	w8, #0x1
   2430c:	b.ne	24320 <__gmpq_canonicalize@@Base+0xac>  // b.any
   24310:	ldur	x8, [x29, #-8]
   24314:	ldr	x8, [x8]
   24318:	cmp	x8, #0x1
   2431c:	b.eq	24340 <__gmpq_canonicalize@@Base+0xcc>  // b.none
   24320:	sub	x2, x29, #0x10
   24324:	mov	x0, x19
   24328:	mov	x1, x19
   2432c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24330:	sub	x2, x29, #0x10
   24334:	mov	x0, x20
   24338:	mov	x1, x20
   2433c:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24340:	ldur	x0, [x29, #-24]
   24344:	cbnz	x0, 24364 <__gmpq_canonicalize@@Base+0xf0>
   24348:	mov	sp, x29
   2434c:	ldp	x20, x19, [sp, #16]
   24350:	ldp	x29, x30, [sp], #32
   24354:	ret
   24358:	sub	x0, x29, #0x18
   2435c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24360:	b	242f0 <__gmpq_canonicalize@@Base+0x7c>
   24364:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24368:	b	24348 <__gmpq_canonicalize@@Base+0xd4>
   2436c:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000024370 <__gmpq_clear@@Base>:
   24370:	stp	x29, x30, [sp, #-32]!
   24374:	stp	x20, x19, [sp, #16]
   24378:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2437c:	ldrsw	x8, [x0]
   24380:	ldr	x20, [x20, #4016]
   24384:	mov	x19, x0
   24388:	mov	x29, sp
   2438c:	cbz	w8, 243a0 <__gmpq_clear@@Base+0x30>
   24390:	ldr	x9, [x20]
   24394:	ldr	x0, [x19, #8]
   24398:	lsl	x1, x8, #3
   2439c:	blr	x9
   243a0:	ldrsw	x8, [x19, #16]
   243a4:	cbz	w8, 243c0 <__gmpq_clear@@Base+0x50>
   243a8:	ldr	x2, [x20]
   243ac:	ldr	x0, [x19, #24]
   243b0:	ldp	x20, x19, [sp, #16]
   243b4:	lsl	x1, x8, #3
   243b8:	ldp	x29, x30, [sp], #32
   243bc:	br	x2
   243c0:	ldp	x20, x19, [sp, #16]
   243c4:	ldp	x29, x30, [sp], #32
   243c8:	ret

00000000000243cc <__gmpq_clears@@Base>:
   243cc:	sub	sp, sp, #0x100
   243d0:	stp	x29, x30, [sp, #224]
   243d4:	add	x29, sp, #0xe0
   243d8:	mov	x8, #0xffffffffffffffc8    	// #-56
   243dc:	mov	x9, sp
   243e0:	sub	x10, x29, #0x58
   243e4:	movk	x8, #0xff80, lsl #32
   243e8:	add	x11, x29, #0x20
   243ec:	add	x9, x9, #0x80
   243f0:	add	x10, x10, #0x38
   243f4:	stp	x20, x19, [sp, #240]
   243f8:	stp	x1, x2, [x29, #-88]
   243fc:	stp	x3, x4, [x29, #-72]
   24400:	stp	x5, x6, [x29, #-56]
   24404:	stur	x7, [x29, #-40]
   24408:	stp	q0, q1, [sp]
   2440c:	stp	q2, q3, [sp, #32]
   24410:	stp	q4, q5, [sp, #64]
   24414:	stp	q6, q7, [sp, #96]
   24418:	stp	x9, x8, [x29, #-16]
   2441c:	stp	x11, x10, [x29, #-32]
   24420:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   24424:	ldr	x20, [x20, #4016]
   24428:	mov	x19, x0
   2442c:	b	24444 <__gmpq_clears@@Base+0x78>
   24430:	ldur	x8, [x29, #-32]
   24434:	add	x9, x8, #0x8
   24438:	stur	x9, [x29, #-32]
   2443c:	ldr	x19, [x8]
   24440:	cbz	x19, 2449c <__gmpq_clears@@Base+0xd0>
   24444:	ldrsw	x8, [x19]
   24448:	cbz	w8, 2445c <__gmpq_clears@@Base+0x90>
   2444c:	ldr	x9, [x20]
   24450:	ldr	x0, [x19, #8]
   24454:	lsl	x1, x8, #3
   24458:	blr	x9
   2445c:	ldrsw	x8, [x19, #16]
   24460:	cbz	w8, 24474 <__gmpq_clears@@Base+0xa8>
   24464:	ldr	x9, [x20]
   24468:	ldr	x0, [x19, #24]
   2446c:	lsl	x1, x8, #3
   24470:	blr	x9
   24474:	ldursw	x8, [x29, #-8]
   24478:	tbz	w8, #31, 24430 <__gmpq_clears@@Base+0x64>
   2447c:	add	w9, w8, #0x8
   24480:	cmn	w8, #0x8
   24484:	stur	w9, [x29, #-8]
   24488:	b.gt	24430 <__gmpq_clears@@Base+0x64>
   2448c:	ldur	x9, [x29, #-24]
   24490:	add	x8, x9, x8
   24494:	ldr	x19, [x8]
   24498:	cbnz	x19, 24444 <__gmpq_clears@@Base+0x78>
   2449c:	ldp	x20, x19, [sp, #240]
   244a0:	ldp	x29, x30, [sp, #224]
   244a4:	add	sp, sp, #0x100
   244a8:	ret

00000000000244ac <__gmpq_cmp@@Base>:
   244ac:	add	x2, x1, #0x10
   244b0:	b	244b4 <__gmpq_cmp@@Base+0x8>
   244b4:	stp	x29, x30, [sp, #-96]!
   244b8:	str	x27, [sp, #16]
   244bc:	stp	x26, x25, [sp, #32]
   244c0:	stp	x24, x23, [sp, #48]
   244c4:	stp	x22, x21, [sp, #64]
   244c8:	stp	x20, x19, [sp, #80]
   244cc:	mov	x29, sp
   244d0:	sub	sp, sp, #0x10
   244d4:	ldr	w25, [x0, #4]
   244d8:	ldrsw	x11, [x1, #4]
   244dc:	cbz	w25, 2457c <__gmpq_cmp@@Base+0xd0>
   244e0:	cbz	w11, 245a0 <__gmpq_cmp@@Base+0xf4>
   244e4:	eor	w8, w11, w25
   244e8:	tbnz	w8, #31, 245a0 <__gmpq_cmp@@Base+0xf4>
   244ec:	ldrsw	x8, [x2, #4]
   244f0:	ldr	x9, [x2, #8]
   244f4:	ldrsw	x21, [x0, #20]
   244f8:	ldr	x10, [x0, #24]
   244fc:	sxtw	x14, w25
   24500:	add	x9, x9, x8, lsl #3
   24504:	ldur	x12, [x9, #-8]
   24508:	add	x9, x10, x21, lsl #3
   2450c:	ldur	x13, [x9, #-8]
   24510:	cmp	x14, #0x0
   24514:	orr	x9, x12, x8
   24518:	cneg	x4, x14, mi  // mi = first
   2451c:	cmp	x9, #0x1
   24520:	cset	w10, eq  // eq = none
   24524:	orr	x15, x13, x21
   24528:	mov	x19, x1
   2452c:	mov	x20, x0
   24530:	cmp	x15, x10
   24534:	b.ne	24584 <__gmpq_cmp@@Base+0xd8>  // b.any
   24538:	cmp	w25, w11
   2453c:	b.ne	245c4 <__gmpq_cmp@@Base+0x118>  // b.any
   24540:	ldr	x8, [x19, #8]
   24544:	ldr	x9, [x20, #8]
   24548:	sub	x8, x8, #0x8
   2454c:	sub	x9, x9, #0x8
   24550:	subs	x10, x4, #0x1
   24554:	b.lt	245e8 <__gmpq_cmp@@Base+0x13c>  // b.tstop
   24558:	lsl	x11, x4, #3
   2455c:	ldr	x12, [x9, x11]
   24560:	ldr	x11, [x8, x11]
   24564:	mov	x4, x10
   24568:	cmp	x12, x11
   2456c:	b.eq	24550 <__gmpq_cmp@@Base+0xa4>  // b.none
   24570:	mov	w8, #0x1                   	// #1
   24574:	cneg	w8, w8, ls  // ls = plast
   24578:	b	245ec <__gmpq_cmp@@Base+0x140>
   2457c:	neg	w0, w11
   24580:	b	245a4 <__gmpq_cmp@@Base+0xf8>
   24584:	cmp	x11, #0x0
   24588:	cneg	x22, x11, mi  // mi = first
   2458c:	add	x26, x22, x21
   24590:	add	x27, x4, x8
   24594:	add	x11, x26, #0x1
   24598:	cmp	x27, x11
   2459c:	b.le	245cc <__gmpq_cmp@@Base+0x120>
   245a0:	mov	w0, w25
   245a4:	mov	sp, x29
   245a8:	ldp	x20, x19, [sp, #80]
   245ac:	ldp	x22, x21, [sp, #64]
   245b0:	ldp	x24, x23, [sp, #48]
   245b4:	ldp	x26, x25, [sp, #32]
   245b8:	ldr	x27, [sp, #16]
   245bc:	ldp	x29, x30, [sp], #96
   245c0:	ret
   245c4:	sub	w0, w25, w11
   245c8:	b	245a4 <__gmpq_cmp@@Base+0xf8>
   245cc:	add	x11, x26, x10
   245d0:	add	x15, x27, #0x1
   245d4:	cmp	x11, x15
   245d8:	neg	x11, x14
   245dc:	b.le	245f8 <__gmpq_cmp@@Base+0x14c>
   245e0:	mov	w0, w11
   245e4:	b	245a4 <__gmpq_cmp@@Base+0xf8>
   245e8:	mov	w8, wzr
   245ec:	cmp	w25, #0x0
   245f0:	cneg	w0, w8, le
   245f4:	b	245a4 <__gmpq_cmp@@Base+0xf8>
   245f8:	ldr	x23, [x20, #8]
   245fc:	ldr	x14, [x19, #8]
   24600:	lsl	x16, x27, #6
   24604:	clz	x13, x13
   24608:	add	x15, x23, x4, lsl #3
   2460c:	ldur	x15, [x15, #-8]
   24610:	add	x14, x14, x22, lsl #3
   24614:	ldur	x14, [x14, #-8]
   24618:	clz	x12, x12
   2461c:	clz	x15, x15
   24620:	sub	x15, x16, x15
   24624:	lsl	x16, x26, #6
   24628:	clz	x14, x14
   2462c:	sub	x14, x16, x14
   24630:	sub	x13, x14, x13
   24634:	sub	x12, x15, x12
   24638:	add	x14, x13, #0x1
   2463c:	cmp	x12, x14
   24640:	mov	w0, w25
   24644:	b.hi	245a4 <__gmpq_cmp@@Base+0xf8>  // b.pmore
   24648:	add	x10, x13, x10
   2464c:	add	x12, x12, #0x1
   24650:	cmp	x10, x12
   24654:	mov	w0, w11
   24658:	b.hi	245a4 <__gmpq_cmp@@Base+0xf8>  // b.pmore
   2465c:	cmp	x9, #0x1
   24660:	str	xzr, [x29, #24]
   24664:	b.ne	24694 <__gmpq_cmp@@Base+0x1e8>  // b.any
   24668:	lsl	x1, x26, #3
   2466c:	mov	w8, #0x7f00                	// #32512
   24670:	cmp	x1, x8
   24674:	b.hi	2479c <__gmpq_cmp@@Base+0x2f0>  // b.pmore
   24678:	add	x9, x1, #0xf
   2467c:	mov	x8, sp
   24680:	and	x9, x9, #0xfffffffffffffff0
   24684:	sub	x24, x8, x9
   24688:	mov	sp, x24
   2468c:	mov	x8, #0xffffffffffffffff    	// #-1
   24690:	b	246fc <__gmpq_cmp@@Base+0x250>
   24694:	add	x9, x26, x27
   24698:	lsl	x1, x9, #3
   2469c:	mov	w9, #0x7f00                	// #32512
   246a0:	cmp	x1, x9
   246a4:	b.hi	247b4 <__gmpq_cmp@@Base+0x308>  // b.pmore
   246a8:	add	x10, x1, #0xf
   246ac:	mov	x9, sp
   246b0:	and	x10, x10, #0xfffffffffffffff0
   246b4:	sub	x23, x9, x10
   246b8:	mov	sp, x23
   246bc:	cmp	x4, x8
   246c0:	add	x24, x23, x27, lsl #3
   246c4:	b.ge	246dc <__gmpq_cmp@@Base+0x230>  // b.tcont
   246c8:	ldr	x1, [x2, #8]
   246cc:	ldr	x3, [x20, #8]
   246d0:	mov	x0, x23
   246d4:	mov	x2, x8
   246d8:	b	246f0 <__gmpq_cmp@@Base+0x244>
   246dc:	ldr	x1, [x20, #8]
   246e0:	ldr	x3, [x2, #8]
   246e4:	mov	x0, x23
   246e8:	mov	x2, x4
   246ec:	mov	x4, x8
   246f0:	bl	ccd0 <__gmpn_mul@plt>
   246f4:	cmp	x0, #0x0
   246f8:	csetm	x8, eq  // eq = none
   246fc:	cmp	x22, x21
   24700:	add	x27, x27, x8
   24704:	b.ge	24720 <__gmpq_cmp@@Base+0x274>  // b.tcont
   24708:	ldr	x1, [x20, #24]
   2470c:	ldr	x3, [x19, #8]
   24710:	mov	x0, x24
   24714:	mov	x2, x21
   24718:	mov	x4, x22
   2471c:	b	24734 <__gmpq_cmp@@Base+0x288>
   24720:	ldr	x1, [x19, #8]
   24724:	ldr	x3, [x20, #24]
   24728:	mov	x0, x24
   2472c:	mov	x2, x22
   24730:	mov	x4, x21
   24734:	bl	ccd0 <__gmpn_mul@plt>
   24738:	sub	x8, x27, x26
   2473c:	cmp	x0, #0x0
   24740:	cinc	x19, x8, eq  // eq = none
   24744:	cbnz	x19, 24780 <__gmpq_cmp@@Base+0x2d4>
   24748:	sub	x8, x24, #0x8
   2474c:	sub	x9, x23, #0x8
   24750:	subs	x10, x27, #0x1
   24754:	b.lt	2477c <__gmpq_cmp@@Base+0x2d0>  // b.tstop
   24758:	lsl	x11, x27, #3
   2475c:	ldr	x12, [x9, x11]
   24760:	ldr	x11, [x8, x11]
   24764:	mov	x27, x10
   24768:	cmp	x12, x11
   2476c:	b.eq	24750 <__gmpq_cmp@@Base+0x2a4>  // b.none
   24770:	mov	w8, #0x1                   	// #1
   24774:	cneg	x19, x8, ls  // ls = plast
   24778:	b	24780 <__gmpq_cmp@@Base+0x2d4>
   2477c:	mov	x19, xzr
   24780:	ldr	x0, [x29, #24]
   24784:	cbnz	x0, 24794 <__gmpq_cmp@@Base+0x2e8>
   24788:	cmp	w25, #0x0
   2478c:	cneg	w0, w19, lt  // lt = tstop
   24790:	b	245a4 <__gmpq_cmp@@Base+0xf8>
   24794:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24798:	b	24788 <__gmpq_cmp@@Base+0x2dc>
   2479c:	add	x0, x29, #0x18
   247a0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   247a4:	ldr	x23, [x20, #8]
   247a8:	mov	x24, x0
   247ac:	mov	x8, #0xffffffffffffffff    	// #-1
   247b0:	b	246fc <__gmpq_cmp@@Base+0x250>
   247b4:	add	x0, x29, #0x18
   247b8:	stur	x4, [x29, #-8]
   247bc:	mov	x23, x2
   247c0:	mov	x24, x8
   247c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   247c8:	ldur	x4, [x29, #-8]
   247cc:	mov	x8, x24
   247d0:	mov	x2, x23
   247d4:	mov	x23, x0
   247d8:	b	246bc <__gmpq_cmp@@Base+0x210>

00000000000247dc <__gmpq_cmp_z@@Base>:
   247dc:	adrp	x2, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   247e0:	add	x2, x2, #0x9e0
   247e4:	b	244b4 <__gmpq_cmp@@Base+0x8>

00000000000247e8 <__gmpq_cmp_si@@Base>:
   247e8:	tbnz	x1, #63, 247f0 <__gmpq_cmp_si@@Base+0x8>
   247ec:	b	bf10 <__gmpq_cmp_ui@plt>
   247f0:	sub	sp, sp, #0x30
   247f4:	stp	x29, x30, [sp, #32]
   247f8:	ldr	w8, [x0, #4]
   247fc:	add	x29, sp, #0x20
   24800:	tbnz	w8, #31, 2480c <__gmpq_cmp_si@@Base+0x24>
   24804:	mov	w0, #0x1                   	// #1
   24808:	b	2483c <__gmpq_cmp_si@@Base+0x54>
   2480c:	neg	w8, w8
   24810:	str	w8, [sp, #4]
   24814:	ldr	x8, [x0, #8]
   24818:	neg	x1, x1
   2481c:	str	x8, [sp, #8]
   24820:	ldr	w8, [x0, #20]
   24824:	str	w8, [sp, #20]
   24828:	ldr	x8, [x0, #24]
   2482c:	mov	x0, sp
   24830:	str	x8, [sp, #24]
   24834:	bl	bf10 <__gmpq_cmp_ui@plt>
   24838:	neg	w0, w0
   2483c:	ldp	x29, x30, [sp, #32]
   24840:	add	sp, sp, #0x30
   24844:	ret

0000000000024848 <__gmpq_cmp_ui@@Base>:
   24848:	stp	x29, x30, [sp, #-80]!
   2484c:	str	x25, [sp, #16]
   24850:	stp	x24, x23, [sp, #32]
   24854:	stp	x22, x21, [sp, #48]
   24858:	stp	x20, x19, [sp, #64]
   2485c:	mov	x29, sp
   24860:	cbz	x2, 249cc <__gmpq_cmp_ui@@Base+0x184>
   24864:	mov	x22, x0
   24868:	ldr	w0, [x0, #4]
   2486c:	mov	x20, x1
   24870:	cbz	x1, 24984 <__gmpq_cmp_ui@@Base+0x13c>
   24874:	cmp	w0, #0x1
   24878:	b.lt	248b0 <__gmpq_cmp_ui@@Base+0x68>  // b.tstop
   2487c:	ldrsw	x21, [x22, #20]
   24880:	cmp	x20, x2
   24884:	sxtw	x19, w0
   24888:	mov	x3, x2
   2488c:	cinc	x8, x21, hi  // hi = pmore
   24890:	cmp	x8, x19
   24894:	b.lt	24984 <__gmpq_cmp_ui@@Base+0x13c>  // b.tstop
   24898:	cmp	x3, x20
   2489c:	cinc	x8, x19, hi  // hi = pmore
   248a0:	cmp	x8, x21
   248a4:	b.ge	248b8 <__gmpq_cmp_ui@@Base+0x70>  // b.tcont
   248a8:	neg	w0, w0
   248ac:	b	24984 <__gmpq_cmp_ui@@Base+0x13c>
   248b0:	mov	w0, #0xffffffff            	// #-1
   248b4:	b	24984 <__gmpq_cmp_ui@@Base+0x13c>
   248b8:	add	x24, x19, #0x1
   248bc:	add	x8, x21, x24
   248c0:	lsl	x8, x8, #3
   248c4:	add	x1, x8, #0x8
   248c8:	mov	w8, #0x7f00                	// #32512
   248cc:	cmp	x1, x8
   248d0:	str	xzr, [x29, #24]
   248d4:	b.hi	249a0 <__gmpq_cmp_ui@@Base+0x158>  // b.pmore
   248d8:	add	x9, x1, #0xf
   248dc:	mov	x8, sp
   248e0:	and	x9, x9, #0xfffffffffffffff0
   248e4:	sub	x23, x8, x9
   248e8:	mov	sp, x23
   248ec:	ldr	x1, [x22, #8]
   248f0:	mov	x0, x23
   248f4:	mov	x2, x19
   248f8:	add	x24, x23, x24, lsl #3
   248fc:	bl	d490 <__gmpn_mul_1@plt>
   24900:	str	x0, [x23, x19, lsl #3]
   24904:	ldr	x1, [x22, #24]
   24908:	cmp	x0, #0x0
   2490c:	mov	x0, x24
   24910:	mov	x2, x21
   24914:	mov	x3, x20
   24918:	cset	w25, ne  // ne = any
   2491c:	cinc	x22, x19, ne  // ne = any
   24920:	bl	d490 <__gmpn_mul_1@plt>
   24924:	cmp	x0, #0x0
   24928:	cset	w9, ne  // ne = any
   2492c:	sub	w10, w22, w21
   24930:	mov	x8, x0
   24934:	subs	w0, w10, w9
   24938:	str	x8, [x24, x21, lsl #3]
   2493c:	b.ne	2497c <__gmpq_cmp_ui@@Base+0x134>  // b.any
   24940:	lsl	x8, x25, #3
   24944:	bfi	x8, x19, #4, #60
   24948:	add	x8, x23, x8
   2494c:	sub	x9, x23, #0x8
   24950:	subs	x10, x22, #0x1
   24954:	b.lt	24978 <__gmpq_cmp_ui@@Base+0x130>  // b.tstop
   24958:	ldr	x11, [x9, x22, lsl #3]
   2495c:	ldr	x12, [x8], #-8
   24960:	mov	x22, x10
   24964:	cmp	x11, x12
   24968:	b.eq	24950 <__gmpq_cmp_ui@@Base+0x108>  // b.none
   2496c:	mov	w8, #0x1                   	// #1
   24970:	cneg	w0, w8, ls  // ls = plast
   24974:	b	2497c <__gmpq_cmp_ui@@Base+0x134>
   24978:	mov	w0, wzr
   2497c:	ldr	x8, [x29, #24]
   24980:	cbnz	x8, 249b8 <__gmpq_cmp_ui@@Base+0x170>
   24984:	mov	sp, x29
   24988:	ldp	x20, x19, [sp, #64]
   2498c:	ldp	x22, x21, [sp, #48]
   24990:	ldp	x24, x23, [sp, #32]
   24994:	ldr	x25, [sp, #16]
   24998:	ldp	x29, x30, [sp], #80
   2499c:	ret
   249a0:	add	x0, x29, #0x18
   249a4:	mov	x23, x3
   249a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   249ac:	mov	x3, x23
   249b0:	mov	x23, x0
   249b4:	b	248ec <__gmpq_cmp_ui@@Base+0xa4>
   249b8:	mov	x19, x0
   249bc:	mov	x0, x8
   249c0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   249c4:	mov	x0, x19
   249c8:	b	24984 <__gmpq_cmp_ui@@Base+0x13c>
   249cc:	bl	bfd0 <__gmp_divide_by_zero@plt>

00000000000249d0 <__gmpq_div@@Base>:
   249d0:	stp	x29, x30, [sp, #-80]!
   249d4:	str	x25, [sp, #16]
   249d8:	stp	x24, x23, [sp, #32]
   249dc:	stp	x22, x21, [sp, #48]
   249e0:	stp	x20, x19, [sp, #64]
   249e4:	mov	x29, sp
   249e8:	sub	sp, sp, #0x40
   249ec:	ldrsw	x8, [x2, #4]
   249f0:	cbz	w8, 24c90 <__gmpq_div@@Base+0x2c0>
   249f4:	mov	x20, x2
   249f8:	mov	x21, x1
   249fc:	mov	x19, x0
   24a00:	cmp	x0, x2
   24a04:	b.eq	24bf0 <__gmpq_div@@Base+0x220>  // b.none
   24a08:	ldr	w9, [x21, #4]
   24a0c:	cmp	w9, #0x0
   24a10:	cneg	w22, w9, mi  // mi = first
   24a14:	cbz	w22, 24b90 <__gmpq_div@@Base+0x1c0>
   24a18:	cmp	x8, #0x0
   24a1c:	cneg	x23, x8, mi  // mi = first
   24a20:	cmp	x23, x22
   24a24:	csel	x8, x22, x23, gt
   24a28:	cmp	x8, #0xfe0
   24a2c:	lsl	x1, x8, #3
   24a30:	str	xzr, [x29, #24]
   24a34:	stur	w8, [x29, #-16]
   24a38:	b.hi	24c38 <__gmpq_div@@Base+0x268>  // b.pmore
   24a3c:	add	x9, x1, #0xf
   24a40:	mov	x8, sp
   24a44:	and	x9, x9, #0xfffffffffffffff0
   24a48:	sub	x0, x8, x9
   24a4c:	mov	sp, x0
   24a50:	cmp	x23, x22
   24a54:	csel	x8, x22, x23, lt  // lt = tstop
   24a58:	cmp	x8, #0xfe0
   24a5c:	lsl	x1, x8, #3
   24a60:	stur	x0, [x29, #-8]
   24a64:	stur	w8, [x29, #-48]
   24a68:	b.hi	24c44 <__gmpq_div@@Base+0x274>  // b.pmore
   24a6c:	add	x9, x1, #0xf
   24a70:	mov	x8, sp
   24a74:	and	x9, x9, #0xfffffffffffffff0
   24a78:	sub	x0, x8, x9
   24a7c:	mov	sp, x0
   24a80:	stur	x0, [x29, #-40]
   24a84:	ldrsw	x24, [x20, #20]
   24a88:	ldrsw	x25, [x21, #20]
   24a8c:	mov	w9, #0x7f00                	// #32512
   24a90:	add	x23, x20, #0x10
   24a94:	add	x22, x21, #0x10
   24a98:	cmp	x25, x24
   24a9c:	csel	x8, x25, x24, lt  // lt = tstop
   24aa0:	lsl	x1, x8, #3
   24aa4:	cmp	x1, x9
   24aa8:	stur	w8, [x29, #-32]
   24aac:	b.hi	24c50 <__gmpq_div@@Base+0x280>  // b.pmore
   24ab0:	add	x9, x1, #0xf
   24ab4:	mov	x8, sp
   24ab8:	and	x9, x9, #0xfffffffffffffff0
   24abc:	sub	x0, x8, x9
   24ac0:	mov	sp, x0
   24ac4:	cmp	x25, x24
   24ac8:	csel	x8, x25, x24, gt
   24acc:	lsl	x1, x8, #3
   24ad0:	mov	w9, #0x7f00                	// #32512
   24ad4:	cmp	x1, x9
   24ad8:	stur	x0, [x29, #-24]
   24adc:	stur	w8, [x29, #-64]
   24ae0:	b.hi	24c5c <__gmpq_div@@Base+0x28c>  // b.pmore
   24ae4:	add	x9, x1, #0xf
   24ae8:	mov	x8, sp
   24aec:	and	x9, x9, #0xfffffffffffffff0
   24af0:	sub	x0, x8, x9
   24af4:	mov	sp, x0
   24af8:	stur	x0, [x29, #-56]
   24afc:	sub	x0, x29, #0x10
   24b00:	mov	x1, x21
   24b04:	mov	x2, x20
   24b08:	bl	cf70 <__gmpz_gcd@plt>
   24b0c:	sub	x0, x29, #0x20
   24b10:	mov	x1, x23
   24b14:	mov	x2, x22
   24b18:	bl	cf70 <__gmpz_gcd@plt>
   24b1c:	sub	x0, x29, #0x30
   24b20:	sub	x2, x29, #0x10
   24b24:	mov	x1, x21
   24b28:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b2c:	sub	x0, x29, #0x40
   24b30:	sub	x2, x29, #0x20
   24b34:	mov	x1, x23
   24b38:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b3c:	sub	x1, x29, #0x30
   24b40:	sub	x2, x29, #0x40
   24b44:	mov	x0, x19
   24b48:	bl	c4b0 <__gmpz_mul@plt>
   24b4c:	sub	x0, x29, #0x30
   24b50:	sub	x2, x29, #0x10
   24b54:	mov	x1, x20
   24b58:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b5c:	sub	x0, x29, #0x40
   24b60:	sub	x2, x29, #0x20
   24b64:	mov	x1, x22
   24b68:	bl	ca00 <__gmpz_divexact_gcd@plt>
   24b6c:	add	x0, x19, #0x10
   24b70:	sub	x1, x29, #0x30
   24b74:	sub	x2, x29, #0x40
   24b78:	bl	c4b0 <__gmpz_mul@plt>
   24b7c:	ldr	w8, [x19, #20]
   24b80:	tbnz	w8, #31, 24bb8 <__gmpq_div@@Base+0x1e8>
   24b84:	ldr	x0, [x29, #24]
   24b88:	cbz	x0, 24bd4 <__gmpq_div@@Base+0x204>
   24b8c:	b	24c68 <__gmpq_div@@Base+0x298>
   24b90:	mov	x0, x19
   24b94:	ldr	w8, [x0, #16]!
   24b98:	cmp	w8, #0x0
   24b9c:	stur	wzr, [x0, #-12]
   24ba0:	b.le	24c70 <__gmpq_div@@Base+0x2a0>
   24ba4:	ldr	x0, [x19, #24]
   24ba8:	mov	w8, #0x1                   	// #1
   24bac:	str	x8, [x0]
   24bb0:	str	w8, [x19, #20]
   24bb4:	b	24bd4 <__gmpq_div@@Base+0x204>
   24bb8:	ldr	w9, [x19, #4]
   24bbc:	neg	w8, w8
   24bc0:	str	w8, [x19, #20]
   24bc4:	neg	w8, w9
   24bc8:	str	w8, [x19, #4]
   24bcc:	ldr	x0, [x29, #24]
   24bd0:	cbnz	x0, 24c68 <__gmpq_div@@Base+0x298>
   24bd4:	mov	sp, x29
   24bd8:	ldp	x20, x19, [sp, #64]
   24bdc:	ldp	x22, x21, [sp, #48]
   24be0:	ldp	x24, x23, [sp, #32]
   24be4:	ldr	x25, [sp, #16]
   24be8:	ldp	x29, x30, [sp], #80
   24bec:	ret
   24bf0:	cmp	x21, x20
   24bf4:	b.eq	24c7c <__gmpq_div@@Base+0x2ac>  // b.none
   24bf8:	ldr	x9, [x20, #8]
   24bfc:	ldp	w12, w13, [x20, #16]
   24c00:	ldr	x10, [x20, #24]
   24c04:	ldr	w11, [x20]
   24c08:	cmp	w8, #0x0
   24c0c:	cneg	w8, w8, le
   24c10:	str	x9, [x20, #24]
   24c14:	cneg	w9, w13, le
   24c18:	mov	x0, x20
   24c1c:	mov	x1, x20
   24c20:	mov	x2, x21
   24c24:	str	x10, [x20, #8]
   24c28:	stp	w11, w8, [x20, #16]
   24c2c:	stp	w12, w9, [x20]
   24c30:	bl	cfe0 <__gmpq_mul@plt>
   24c34:	b	24bd4 <__gmpq_div@@Base+0x204>
   24c38:	add	x0, x29, #0x18
   24c3c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24c40:	b	24a50 <__gmpq_div@@Base+0x80>
   24c44:	add	x0, x29, #0x18
   24c48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24c4c:	b	24a80 <__gmpq_div@@Base+0xb0>
   24c50:	add	x0, x29, #0x18
   24c54:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24c58:	b	24ac4 <__gmpq_div@@Base+0xf4>
   24c5c:	add	x0, x29, #0x18
   24c60:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24c64:	b	24af8 <__gmpq_div@@Base+0x128>
   24c68:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24c6c:	b	24bd4 <__gmpq_div@@Base+0x204>
   24c70:	mov	w1, #0x1                   	// #1
   24c74:	bl	c080 <__gmpz_realloc@plt>
   24c78:	b	24ba8 <__gmpq_div@@Base+0x1d8>
   24c7c:	mov	w1, #0x1                   	// #1
   24c80:	mov	w2, #0x1                   	// #1
   24c84:	mov	x0, x20
   24c88:	bl	ca60 <__gmpq_set_ui@plt>
   24c8c:	b	24bd4 <__gmpq_div@@Base+0x204>
   24c90:	bl	bfd0 <__gmp_divide_by_zero@plt>

0000000000024c94 <__gmpq_get_d@@Base>:
   24c94:	str	d8, [sp, #-96]!
   24c98:	stp	x29, x30, [sp, #8]
   24c9c:	str	x27, [sp, #24]
   24ca0:	stp	x26, x25, [sp, #32]
   24ca4:	stp	x24, x23, [sp, #48]
   24ca8:	stp	x22, x21, [sp, #64]
   24cac:	stp	x20, x19, [sp, #80]
   24cb0:	mov	x29, sp
   24cb4:	sub	sp, sp, #0x20
   24cb8:	ldrsw	x19, [x0, #4]
   24cbc:	cbz	w19, 24dec <__gmpq_get_d@@Base+0x158>
   24cc0:	ldrsw	x8, [x0, #20]
   24cc4:	cmp	x19, #0x0
   24cc8:	stur	xzr, [x29, #-32]
   24ccc:	cneg	x24, x19, mi  // mi = first
   24cd0:	cmp	x8, #0x0
   24cd4:	ldr	x25, [x0, #8]
   24cd8:	ldr	x21, [x0, #24]
   24cdc:	cneg	x22, x8, mi  // mi = first
   24ce0:	mov	x9, #0xfffffffffffffffe    	// #-2
   24ce4:	sub	x8, x22, x24
   24ce8:	sub	x9, x9, x8
   24cec:	cmn	x8, #0x1
   24cf0:	lsl	x20, x9, #6
   24cf4:	b.lt	24d50 <__gmpq_get_d@@Base+0xbc>  // b.tstop
   24cf8:	add	x27, x8, #0x2
   24cfc:	lsl	x8, x22, #3
   24d00:	cmp	x22, #0xfdd
   24d04:	add	x1, x8, #0x18
   24d08:	b.hi	24dfc <__gmpq_get_d@@Base+0x168>  // b.pmore
   24d0c:	add	x9, x1, #0xf
   24d10:	mov	x8, sp
   24d14:	and	x9, x9, #0xfffffffffffffff0
   24d18:	sub	x23, x8, x9
   24d1c:	mov	sp, x23
   24d20:	add	x26, x22, #0x2
   24d24:	sub	x8, x26, x24
   24d28:	lsl	x2, x8, #3
   24d2c:	mov	x0, x23
   24d30:	mov	w1, wzr
   24d34:	bl	c5f0 <memset@plt>
   24d38:	add	x0, x23, x27, lsl #3
   24d3c:	mov	x1, x25
   24d40:	mov	x2, x24
   24d44:	bl	ca50 <__gmpn_copyi@plt>
   24d48:	mov	x24, x23
   24d4c:	b	24d7c <__gmpq_get_d@@Base+0xe8>
   24d50:	lsl	x8, x22, #3
   24d54:	add	x24, x25, x9, lsl #3
   24d58:	cmp	x22, #0xfdd
   24d5c:	add	x1, x8, #0x18
   24d60:	b.hi	24e0c <__gmpq_get_d@@Base+0x178>  // b.pmore
   24d64:	add	x9, x1, #0xf
   24d68:	mov	x8, sp
   24d6c:	and	x9, x9, #0xfffffffffffffff0
   24d70:	sub	x23, x8, x9
   24d74:	mov	sp, x23
   24d78:	add	x26, x22, #0x2
   24d7c:	sub	x0, x29, #0x18
   24d80:	mov	x1, x24
   24d84:	mov	x2, x26
   24d88:	mov	x3, x21
   24d8c:	mov	x4, x22
   24d90:	mov	x5, x23
   24d94:	bl	c320 <__gmpn_div_q@plt>
   24d98:	ldur	x8, [x29, #-8]
   24d9c:	sub	x0, x29, #0x18
   24da0:	mov	x2, x19
   24da4:	mov	x3, x20
   24da8:	cmp	x8, #0x0
   24dac:	mov	w8, #0x2                   	// #2
   24db0:	cinc	x1, x8, ne  // ne = any
   24db4:	bl	bf40 <__gmpn_get_d@plt>
   24db8:	ldur	x0, [x29, #-32]
   24dbc:	mov	v8.16b, v0.16b
   24dc0:	cbnz	x0, 24df4 <__gmpq_get_d@@Base+0x160>
   24dc4:	mov	v0.16b, v8.16b
   24dc8:	mov	sp, x29
   24dcc:	ldp	x20, x19, [sp, #80]
   24dd0:	ldp	x22, x21, [sp, #64]
   24dd4:	ldp	x24, x23, [sp, #48]
   24dd8:	ldp	x26, x25, [sp, #32]
   24ddc:	ldr	x27, [sp, #24]
   24de0:	ldp	x29, x30, [sp, #8]
   24de4:	ldr	d8, [sp], #96
   24de8:	ret
   24dec:	fmov	d8, xzr
   24df0:	b	24dc4 <__gmpq_get_d@@Base+0x130>
   24df4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   24df8:	b	24dc4 <__gmpq_get_d@@Base+0x130>
   24dfc:	sub	x0, x29, #0x20
   24e00:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24e04:	mov	x23, x0
   24e08:	b	24d20 <__gmpq_get_d@@Base+0x8c>
   24e0c:	sub	x0, x29, #0x20
   24e10:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   24e14:	mov	x23, x0
   24e18:	b	24d78 <__gmpq_get_d@@Base+0xe4>

0000000000024e1c <__gmpq_get_den@@Base>:
   24e1c:	stp	x29, x30, [sp, #-48]!
   24e20:	stp	x22, x21, [sp, #16]
   24e24:	stp	x20, x19, [sp, #32]
   24e28:	ldr	w22, [x1, #20]
   24e2c:	ldr	w8, [x0]
   24e30:	mov	x19, x1
   24e34:	mov	x20, x0
   24e38:	sxtw	x21, w22
   24e3c:	cmp	w22, w8
   24e40:	mov	x29, sp
   24e44:	b.gt	24e68 <__gmpq_get_den@@Base+0x4c>
   24e48:	ldr	x0, [x20, #8]
   24e4c:	str	w22, [x20, #4]
   24e50:	ldr	x1, [x19, #24]
   24e54:	mov	x2, x21
   24e58:	ldp	x20, x19, [sp, #32]
   24e5c:	ldp	x22, x21, [sp, #16]
   24e60:	ldp	x29, x30, [sp], #48
   24e64:	b	ca50 <__gmpn_copyi@plt>
   24e68:	mov	x0, x20
   24e6c:	mov	x1, x21
   24e70:	bl	c080 <__gmpz_realloc@plt>
   24e74:	b	24e4c <__gmpq_get_den@@Base+0x30>

0000000000024e78 <__gmpq_get_num@@Base>:
   24e78:	stp	x29, x30, [sp, #-48]!
   24e7c:	stp	x22, x21, [sp, #16]
   24e80:	stp	x20, x19, [sp, #32]
   24e84:	ldrsw	x22, [x1, #4]
   24e88:	ldrsw	x8, [x0]
   24e8c:	mov	x19, x1
   24e90:	mov	x20, x0
   24e94:	cmp	x22, #0x0
   24e98:	cneg	x21, x22, mi  // mi = first
   24e9c:	cmp	x21, x8
   24ea0:	mov	x29, sp
   24ea4:	b.gt	24ec8 <__gmpq_get_num@@Base+0x50>
   24ea8:	ldr	x0, [x20, #8]
   24eac:	str	w22, [x20, #4]
   24eb0:	ldr	x1, [x19, #8]
   24eb4:	mov	x2, x21
   24eb8:	ldp	x20, x19, [sp, #32]
   24ebc:	ldp	x22, x21, [sp, #16]
   24ec0:	ldp	x29, x30, [sp], #48
   24ec4:	b	ca50 <__gmpn_copyi@plt>
   24ec8:	mov	x0, x20
   24ecc:	mov	x1, x21
   24ed0:	bl	c080 <__gmpz_realloc@plt>
   24ed4:	b	24eac <__gmpq_get_num@@Base+0x34>

0000000000024ed8 <__gmpq_get_str@@Base>:
   24ed8:	stp	x29, x30, [sp, #-64]!
   24edc:	add	w8, w1, #0x24
   24ee0:	cmp	w8, #0x62
   24ee4:	str	x23, [sp, #16]
   24ee8:	stp	x22, x21, [sp, #32]
   24eec:	stp	x20, x19, [sp, #48]
   24ef0:	mov	x29, sp
   24ef4:	b.ls	24f00 <__gmpq_get_str@@Base+0x28>  // b.plast
   24ef8:	mov	x19, xzr
   24efc:	b	24ff0 <__gmpq_get_str@@Base+0x118>
   24f00:	mov	x21, x2
   24f04:	mov	w20, w1
   24f08:	mov	x19, x0
   24f0c:	cbz	x0, 24f18 <__gmpq_get_str@@Base+0x40>
   24f10:	mov	x22, xzr
   24f14:	b	24f80 <__gmpq_get_str@@Base+0xa8>
   24f18:	cmp	w20, #0x0
   24f1c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   24f20:	cneg	w11, w20, mi  // mi = first
   24f24:	mov	w8, #0xa                   	// #10
   24f28:	ldr	x9, [x9, #3936]
   24f2c:	cmp	w11, #0x2
   24f30:	csel	w20, w8, w20, lt  // lt = tstop
   24f34:	ldr	w11, [x21, #4]
   24f38:	cmp	w20, #0x0
   24f3c:	mov	w10, #0x28                  	// #40
   24f40:	cneg	w8, w20, mi  // mi = first
   24f44:	umaddl	x8, w8, w10, x9
   24f48:	ldr	w9, [x21, #20]
   24f4c:	cmp	w11, #0x0
   24f50:	cneg	w10, w11, mi  // mi = first
   24f54:	ldr	x8, [x8, #8]
   24f58:	add	w9, w10, w9
   24f5c:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   24f60:	ldr	x10, [x10, #3840]
   24f64:	sbfiz	x9, x9, #6, #32
   24f68:	umulh	x8, x8, x9
   24f6c:	add	x22, x8, #0x6
   24f70:	ldr	x10, [x10]
   24f74:	mov	x0, x22
   24f78:	blr	x10
   24f7c:	mov	x19, x0
   24f80:	mov	x0, x19
   24f84:	mov	w1, w20
   24f88:	mov	x2, x21
   24f8c:	bl	c3a0 <__gmpz_get_str@plt>
   24f90:	mov	x0, x19
   24f94:	bl	bf60 <strlen@plt>
   24f98:	ldr	w8, [x21, #20]
   24f9c:	cmp	w8, #0x1
   24fa0:	b.ne	24fb4 <__gmpq_get_str@@Base+0xdc>  // b.any
   24fa4:	ldr	x8, [x21, #24]
   24fa8:	ldr	x8, [x8]
   24fac:	cmp	x8, #0x1
   24fb0:	b.eq	24fe0 <__gmpq_get_str@@Base+0x108>  // b.none
   24fb4:	add	x23, x0, #0x1
   24fb8:	add	x2, x21, #0x10
   24fbc:	mov	w8, #0x2f                  	// #47
   24fc0:	add	x21, x19, x23
   24fc4:	strb	w8, [x19, x0]
   24fc8:	mov	x0, x21
   24fcc:	mov	w1, w20
   24fd0:	bl	c3a0 <__gmpz_get_str@plt>
   24fd4:	mov	x0, x21
   24fd8:	bl	bf60 <strlen@plt>
   24fdc:	add	x0, x0, x23
   24fe0:	cbz	x22, 24ff0 <__gmpq_get_str@@Base+0x118>
   24fe4:	add	x2, x0, #0x1
   24fe8:	cmp	x22, x2
   24fec:	b.ne	25008 <__gmpq_get_str@@Base+0x130>  // b.any
   24ff0:	mov	x0, x19
   24ff4:	ldp	x20, x19, [sp, #48]
   24ff8:	ldp	x22, x21, [sp, #32]
   24ffc:	ldr	x23, [sp, #16]
   25000:	ldp	x29, x30, [sp], #64
   25004:	ret
   25008:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2500c:	ldr	x8, [x8, #3792]
   25010:	mov	x0, x19
   25014:	mov	x1, x22
   25018:	ldp	x20, x19, [sp, #48]
   2501c:	ldr	x3, [x8]
   25020:	ldp	x22, x21, [sp, #32]
   25024:	ldr	x23, [sp, #16]
   25028:	ldp	x29, x30, [sp], #64
   2502c:	br	x3

0000000000025030 <__gmpq_init@@Base>:
   25030:	stp	x29, x30, [sp, #-32]!
   25034:	adrp	x8, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   25038:	stp	x20, x19, [sp, #16]
   2503c:	add	x8, x8, #0x688
   25040:	mov	w20, #0x1                   	// #1
   25044:	stp	xzr, x8, [x0]
   25048:	str	w20, [x0, #16]
   2504c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   25050:	ldr	x8, [x8, #3840]
   25054:	mov	x19, x0
   25058:	mov	w0, #0x8                   	// #8
   2505c:	mov	x29, sp
   25060:	ldr	x8, [x8]
   25064:	blr	x8
   25068:	str	x0, [x19, #24]
   2506c:	str	x20, [x0]
   25070:	str	w20, [x19, #20]
   25074:	ldp	x20, x19, [sp, #16]
   25078:	ldp	x29, x30, [sp], #32
   2507c:	ret

0000000000025080 <__gmpq_inits@@Base>:
   25080:	sub	sp, sp, #0xf0
   25084:	stp	x29, x30, [sp, #224]
   25088:	add	x29, sp, #0xe0
   2508c:	mov	x8, #0xffffffffffffffc8    	// #-56
   25090:	mov	x9, sp
   25094:	sub	x10, x29, #0x58
   25098:	movk	x8, #0xff80, lsl #32
   2509c:	add	x11, x29, #0x10
   250a0:	add	x9, x9, #0x80
   250a4:	add	x10, x10, #0x38
   250a8:	stp	x1, x2, [x29, #-88]
   250ac:	stp	x3, x4, [x29, #-72]
   250b0:	stp	x5, x6, [x29, #-56]
   250b4:	stur	x7, [x29, #-40]
   250b8:	stp	q0, q1, [sp]
   250bc:	stp	q2, q3, [sp, #32]
   250c0:	stp	q4, q5, [sp, #64]
   250c4:	stp	q6, q7, [sp, #96]
   250c8:	stp	x9, x8, [x29, #-16]
   250cc:	stp	x11, x10, [x29, #-32]
   250d0:	b	250e8 <__gmpq_inits@@Base+0x68>
   250d4:	ldur	x8, [x29, #-32]
   250d8:	add	x9, x8, #0x8
   250dc:	stur	x9, [x29, #-32]
   250e0:	ldr	x0, [x8]
   250e4:	cbz	x0, 25114 <__gmpq_inits@@Base+0x94>
   250e8:	bl	cdd0 <__gmpq_init@plt>
   250ec:	ldursw	x8, [x29, #-8]
   250f0:	tbz	w8, #31, 250d4 <__gmpq_inits@@Base+0x54>
   250f4:	add	w9, w8, #0x8
   250f8:	cmn	w8, #0x8
   250fc:	stur	w9, [x29, #-8]
   25100:	b.gt	250d4 <__gmpq_inits@@Base+0x54>
   25104:	ldur	x9, [x29, #-24]
   25108:	add	x8, x9, x8
   2510c:	ldr	x0, [x8]
   25110:	cbnz	x0, 250e8 <__gmpq_inits@@Base+0x68>
   25114:	ldp	x29, x30, [sp, #224]
   25118:	add	sp, sp, #0xf0
   2511c:	ret

0000000000025120 <__gmpq_inp_str@@Base>:
   25120:	stp	x29, x30, [sp, #-64]!
   25124:	str	x23, [sp, #16]
   25128:	stp	x22, x21, [sp, #32]
   2512c:	stp	x20, x19, [sp, #48]
   25130:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   25134:	ldr	x8, [x8, #3888]
   25138:	mov	x22, x0
   2513c:	cmp	x1, #0x0
   25140:	mov	w20, w2
   25144:	ldr	x8, [x8]
   25148:	ldr	w9, [x22, #16]!
   2514c:	mov	x19, x0
   25150:	mov	w10, #0x1                   	// #1
   25154:	csel	x21, x8, x1, eq  // eq = none
   25158:	cmp	w9, #0x0
   2515c:	mov	x29, sp
   25160:	str	w10, [x22, #4]
   25164:	b.le	251fc <__gmpq_inp_str@@Base+0xdc>
   25168:	ldr	x0, [x19, #24]
   2516c:	mov	w8, #0x1                   	// #1
   25170:	str	x8, [x0]
   25174:	mov	x0, x19
   25178:	mov	x1, x21
   2517c:	mov	w2, w20
   25180:	bl	d0d0 <__gmpz_inp_str@plt>
   25184:	mov	x23, x0
   25188:	cbz	x0, 251e4 <__gmpq_inp_str@@Base+0xc4>
   2518c:	mov	x0, x21
   25190:	bl	c7f0 <getc@plt>
   25194:	cmp	w0, #0x2f
   25198:	b.ne	251dc <__gmpq_inp_str@@Base+0xbc>  // b.any
   2519c:	mov	x0, x21
   251a0:	bl	c7f0 <getc@plt>
   251a4:	mov	w3, w0
   251a8:	add	x4, x23, #0x2
   251ac:	mov	x0, x22
   251b0:	mov	x1, x21
   251b4:	mov	w2, w20
   251b8:	bl	c9d0 <__gmpz_inp_str_nowhite@plt>
   251bc:	mov	x23, x0
   251c0:	cbnz	x0, 251e4 <__gmpq_inp_str@@Base+0xc4>
   251c4:	ldr	x8, [x19, #24]
   251c8:	mov	w9, #0x1                   	// #1
   251cc:	str	wzr, [x19, #4]
   251d0:	str	w9, [x19, #20]
   251d4:	str	x9, [x8]
   251d8:	b	251e4 <__gmpq_inp_str@@Base+0xc4>
   251dc:	mov	x1, x21
   251e0:	bl	cc50 <ungetc@plt>
   251e4:	mov	x0, x23
   251e8:	ldp	x20, x19, [sp, #48]
   251ec:	ldp	x22, x21, [sp, #32]
   251f0:	ldr	x23, [sp, #16]
   251f4:	ldp	x29, x30, [sp], #64
   251f8:	ret
   251fc:	mov	w1, #0x1                   	// #1
   25200:	mov	x0, x22
   25204:	bl	c080 <__gmpz_realloc@plt>
   25208:	b	2516c <__gmpq_inp_str@@Base+0x4c>

000000000002520c <__gmpq_inv@@Base>:
   2520c:	stp	x29, x30, [sp, #-64]!
   25210:	stp	x22, x21, [sp, #32]
   25214:	stp	x20, x19, [sp, #48]
   25218:	ldrsw	x20, [x1, #4]
   2521c:	ldrsw	x8, [x1, #20]
   25220:	mov	x19, x1
   25224:	mov	x21, x0
   25228:	str	x23, [sp, #16]
   2522c:	mov	x29, sp
   25230:	tbnz	w20, #31, 2523c <__gmpq_inv@@Base+0x30>
   25234:	cbnz	w20, 25244 <__gmpq_inv@@Base+0x38>
   25238:	bl	bfd0 <__gmp_divide_by_zero@plt>
   2523c:	neg	x20, x20
   25240:	neg	x8, x8
   25244:	add	x22, x21, #0x10
   25248:	cmp	x21, x19
   2524c:	str	w20, [x21, #20]
   25250:	str	w8, [x21, #4]
   25254:	b.eq	252a8 <__gmpq_inv@@Base+0x9c>  // b.none
   25258:	ldrsw	x9, [x21]
   2525c:	cmp	x8, #0x0
   25260:	cneg	x23, x8, mi  // mi = first
   25264:	cmp	x23, x9
   25268:	b.gt	252dc <__gmpq_inv@@Base+0xd0>
   2526c:	ldr	x0, [x21, #8]
   25270:	ldr	x1, [x19, #24]
   25274:	mov	x2, x23
   25278:	bl	ca50 <__gmpn_copyi@plt>
   2527c:	ldrsw	x8, [x22]
   25280:	cmp	x20, x8
   25284:	b.gt	252ec <__gmpq_inv@@Base+0xe0>
   25288:	ldr	x0, [x21, #24]
   2528c:	ldr	x1, [x19, #8]
   25290:	mov	x2, x20
   25294:	ldp	x20, x19, [sp, #48]
   25298:	ldp	x22, x21, [sp, #32]
   2529c:	ldr	x23, [sp, #16]
   252a0:	ldp	x29, x30, [sp], #64
   252a4:	b	ca50 <__gmpn_copyi@plt>
   252a8:	ldr	x8, [x19, #24]
   252ac:	ldr	x9, [x19, #8]
   252b0:	ldr	x23, [sp, #16]
   252b4:	str	x8, [x19, #8]
   252b8:	str	x9, [x19, #24]
   252bc:	ldr	w8, [x22]
   252c0:	ldr	w9, [x19]
   252c4:	str	w8, [x19]
   252c8:	str	w9, [x22]
   252cc:	ldp	x20, x19, [sp, #48]
   252d0:	ldp	x22, x21, [sp, #32]
   252d4:	ldp	x29, x30, [sp], #64
   252d8:	ret
   252dc:	mov	x0, x21
   252e0:	mov	x1, x23
   252e4:	bl	c080 <__gmpz_realloc@plt>
   252e8:	b	25270 <__gmpq_inv@@Base+0x64>
   252ec:	mov	x0, x22
   252f0:	mov	x1, x20
   252f4:	bl	c080 <__gmpz_realloc@plt>
   252f8:	b	2528c <__gmpq_inv@@Base+0x80>

00000000000252fc <__gmpq_mul_2exp@@Base>:
   252fc:	mov	x8, x1
   25300:	add	x1, x0, #0x10
   25304:	add	x3, x8, #0x10
   25308:	mov	x4, x2
   2530c:	mov	x2, x8
   25310:	b	25314 <__gmpq_mul_2exp@@Base+0x18>
   25314:	stp	x29, x30, [sp, #-96]!
   25318:	stp	x28, x27, [sp, #16]
   2531c:	stp	x26, x25, [sp, #32]
   25320:	stp	x24, x23, [sp, #48]
   25324:	stp	x22, x21, [sp, #64]
   25328:	stp	x20, x19, [sp, #80]
   2532c:	ldr	x8, [x3, #8]
   25330:	ldrsw	x27, [x3, #4]
   25334:	mov	x20, x4
   25338:	mov	x19, x2
   2533c:	ldr	x26, [x8]
   25340:	cmp	x27, #0x0
   25344:	cneg	x9, x27, mi  // mi = first
   25348:	mov	x21, x1
   2534c:	cmp	x26, #0x0
   25350:	cset	w28, eq  // eq = none
   25354:	cmp	x4, #0x40
   25358:	mov	x22, x0
   2535c:	mov	x23, x8
   25360:	mov	x29, sp
   25364:	b.cc	25388 <__gmpq_mul_2exp@@Base+0x8c>  // b.lo, b.ul, b.last
   25368:	cbnz	x26, 25388 <__gmpq_mul_2exp@@Base+0x8c>
   2536c:	ldr	x26, [x23, #8]!
   25370:	sub	x20, x20, #0x40
   25374:	cmp	x26, #0x0
   25378:	cset	w28, eq  // eq = none
   2537c:	cmp	x20, #0x40
   25380:	b.cc	25388 <__gmpq_mul_2exp@@Base+0x8c>  // b.lo, b.ul, b.last
   25384:	cbz	x26, 2536c <__gmpq_mul_2exp@@Base+0x70>
   25388:	ldrsw	x10, [x21]
   2538c:	sub	x8, x23, x8
   25390:	sub	x24, x9, x8, asr #3
   25394:	cmp	x24, x10
   25398:	b.gt	253f0 <__gmpq_mul_2exp@@Base+0xf4>
   2539c:	ldr	x25, [x21, #8]
   253a0:	cbz	x20, 25404 <__gmpq_mul_2exp@@Base+0x108>
   253a4:	tbnz	w26, #0, 25404 <__gmpq_mul_2exp@@Base+0x108>
   253a8:	rbit	x8, x26
   253ac:	clz	x8, x8
   253b0:	cmp	x8, x20
   253b4:	csel	x8, x8, x20, cc  // cc = lo, ul, last
   253b8:	cmp	w28, #0x0
   253bc:	csel	x26, x20, x8, ne  // ne = any
   253c0:	mov	x0, x25
   253c4:	mov	x1, x23
   253c8:	mov	x2, x24
   253cc:	mov	w3, w26
   253d0:	bl	c1a0 <__gmpn_rshift@plt>
   253d4:	add	x8, x25, x24, lsl #3
   253d8:	ldur	x8, [x8, #-8]
   253dc:	sub	x20, x20, x26
   253e0:	cmp	x8, #0x0
   253e4:	cset	w8, eq  // eq = none
   253e8:	sub	x24, x24, x8
   253ec:	b	2541c <__gmpq_mul_2exp@@Base+0x120>
   253f0:	mov	x0, x21
   253f4:	mov	x1, x24
   253f8:	bl	c080 <__gmpz_realloc@plt>
   253fc:	mov	x25, x0
   25400:	cbnz	x20, 253a4 <__gmpq_mul_2exp@@Base+0xa8>
   25404:	cmp	x23, x25
   25408:	b.eq	2541c <__gmpq_mul_2exp@@Base+0x120>  // b.none
   2540c:	mov	x0, x25
   25410:	mov	x1, x23
   25414:	mov	x2, x24
   25418:	bl	ca50 <__gmpn_copyi@plt>
   2541c:	neg	w8, w24
   25420:	cmp	w27, #0x0
   25424:	csel	x8, x24, x8, ge  // ge = tcont
   25428:	str	w8, [x21, #4]
   2542c:	cbz	x20, 25458 <__gmpq_mul_2exp@@Base+0x15c>
   25430:	mov	x0, x22
   25434:	mov	x1, x19
   25438:	mov	x2, x20
   2543c:	ldp	x20, x19, [sp, #80]
   25440:	ldp	x22, x21, [sp, #64]
   25444:	ldp	x24, x23, [sp, #48]
   25448:	ldp	x26, x25, [sp, #32]
   2544c:	ldp	x28, x27, [sp, #16]
   25450:	ldp	x29, x30, [sp], #96
   25454:	b	c6e0 <__gmpz_mul_2exp@plt>
   25458:	cmp	x22, x19
   2545c:	b.eq	25484 <__gmpq_mul_2exp@@Base+0x188>  // b.none
   25460:	mov	x0, x22
   25464:	mov	x1, x19
   25468:	ldp	x20, x19, [sp, #80]
   2546c:	ldp	x22, x21, [sp, #64]
   25470:	ldp	x24, x23, [sp, #48]
   25474:	ldp	x26, x25, [sp, #32]
   25478:	ldp	x28, x27, [sp, #16]
   2547c:	ldp	x29, x30, [sp], #96
   25480:	b	c420 <__gmpz_set@plt>
   25484:	ldp	x20, x19, [sp, #80]
   25488:	ldp	x22, x21, [sp, #64]
   2548c:	ldp	x24, x23, [sp, #48]
   25490:	ldp	x26, x25, [sp, #32]
   25494:	ldp	x28, x27, [sp, #16]
   25498:	ldp	x29, x30, [sp], #96
   2549c:	ret

00000000000254a0 <__gmpq_div_2exp@@Base>:
   254a0:	stp	x29, x30, [sp, #-16]!
   254a4:	ldr	w8, [x1, #4]
   254a8:	mov	x3, x1
   254ac:	mov	x1, x0
   254b0:	mov	x29, sp
   254b4:	cbz	w8, 254cc <__gmpq_div_2exp@@Base+0x2c>
   254b8:	mov	x4, x2
   254bc:	add	x0, x1, #0x10
   254c0:	add	x2, x3, #0x10
   254c4:	ldp	x29, x30, [sp], #16
   254c8:	b	25314 <__gmpq_mul_2exp@@Base+0x18>
   254cc:	mov	x0, x1
   254d0:	ldr	w8, [x0, #16]!
   254d4:	mov	w9, #0x1                   	// #1
   254d8:	cmp	w8, #0x0
   254dc:	stur	wzr, [x0, #-12]
   254e0:	str	w9, [x0, #4]
   254e4:	b.le	254fc <__gmpq_div_2exp@@Base+0x5c>
   254e8:	ldr	x0, [x1, #24]
   254ec:	mov	w8, #0x1                   	// #1
   254f0:	str	x8, [x0]
   254f4:	ldp	x29, x30, [sp], #16
   254f8:	ret
   254fc:	mov	w1, #0x1                   	// #1
   25500:	bl	c080 <__gmpz_realloc@plt>
   25504:	b	254ec <__gmpq_div_2exp@@Base+0x4c>

0000000000025508 <__gmpq_mul@@Base>:
   25508:	stp	x29, x30, [sp, #-96]!
   2550c:	str	x27, [sp, #16]
   25510:	stp	x26, x25, [sp, #32]
   25514:	stp	x24, x23, [sp, #48]
   25518:	stp	x22, x21, [sp, #64]
   2551c:	stp	x20, x19, [sp, #80]
   25520:	mov	x29, sp
   25524:	sub	sp, sp, #0x40
   25528:	mov	x20, x1
   2552c:	cmp	x1, x2
   25530:	mov	x19, x0
   25534:	b.eq	256c8 <__gmpq_mul@@Base+0x1c0>  // b.none
   25538:	ldr	w8, [x20, #4]
   2553c:	ldr	w9, [x2, #4]
   25540:	mov	x21, x2
   25544:	cmp	w8, #0x0
   25548:	cneg	w26, w8, mi  // mi = first
   2554c:	cmp	w9, #0x0
   25550:	cneg	w24, w9, mi  // mi = first
   25554:	cbz	w26, 256ec <__gmpq_mul@@Base+0x1e4>
   25558:	cbz	w24, 256ec <__gmpq_mul@@Base+0x1e4>
   2555c:	ldrsw	x27, [x21, #20]
   25560:	ldrsw	x25, [x20, #20]
   25564:	mov	w9, #0x7f00                	// #32512
   25568:	str	xzr, [x29, #24]
   2556c:	cmp	x26, x27
   25570:	csel	x8, x26, x27, lt  // lt = tstop
   25574:	lsl	x1, x8, #3
   25578:	cmp	x1, x9
   2557c:	stur	w8, [x29, #-16]
   25580:	b.hi	25730 <__gmpq_mul@@Base+0x228>  // b.pmore
   25584:	add	x9, x1, #0xf
   25588:	mov	x8, sp
   2558c:	and	x9, x9, #0xfffffffffffffff0
   25590:	sub	x0, x8, x9
   25594:	mov	sp, x0
   25598:	cmp	x24, x25
   2559c:	csel	x8, x24, x25, lt  // lt = tstop
   255a0:	lsl	x1, x8, #3
   255a4:	mov	w9, #0x7f00                	// #32512
   255a8:	cmp	x1, x9
   255ac:	stur	x0, [x29, #-8]
   255b0:	stur	w8, [x29, #-32]
   255b4:	b.hi	2573c <__gmpq_mul@@Base+0x234>  // b.pmore
   255b8:	add	x9, x1, #0xf
   255bc:	mov	x8, sp
   255c0:	and	x9, x9, #0xfffffffffffffff0
   255c4:	sub	x0, x8, x9
   255c8:	mov	sp, x0
   255cc:	cmp	x26, x27
   255d0:	csel	x8, x26, x27, gt
   255d4:	add	x22, x20, #0x10
   255d8:	add	x23, x21, #0x10
   255dc:	cmp	x8, #0xfe0
   255e0:	lsl	x1, x8, #3
   255e4:	stur	x0, [x29, #-24]
   255e8:	stur	w8, [x29, #-48]
   255ec:	b.hi	25748 <__gmpq_mul@@Base+0x240>  // b.pmore
   255f0:	add	x9, x1, #0xf
   255f4:	mov	x8, sp
   255f8:	and	x9, x9, #0xfffffffffffffff0
   255fc:	sub	x0, x8, x9
   25600:	mov	sp, x0
   25604:	cmp	x24, x25
   25608:	csel	x8, x24, x25, gt
   2560c:	cmp	x8, #0xfe0
   25610:	lsl	x1, x8, #3
   25614:	stur	x0, [x29, #-40]
   25618:	stur	w8, [x29, #-64]
   2561c:	b.hi	25754 <__gmpq_mul@@Base+0x24c>  // b.pmore
   25620:	add	x9, x1, #0xf
   25624:	mov	x8, sp
   25628:	and	x9, x9, #0xfffffffffffffff0
   2562c:	sub	x0, x8, x9
   25630:	mov	sp, x0
   25634:	stur	x0, [x29, #-56]
   25638:	sub	x0, x29, #0x10
   2563c:	mov	x1, x20
   25640:	mov	x2, x23
   25644:	bl	cf70 <__gmpz_gcd@plt>
   25648:	sub	x0, x29, #0x20
   2564c:	mov	x1, x21
   25650:	mov	x2, x22
   25654:	bl	cf70 <__gmpz_gcd@plt>
   25658:	sub	x0, x29, #0x30
   2565c:	sub	x2, x29, #0x10
   25660:	mov	x1, x20
   25664:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25668:	sub	x0, x29, #0x40
   2566c:	sub	x2, x29, #0x20
   25670:	mov	x1, x21
   25674:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25678:	sub	x1, x29, #0x30
   2567c:	sub	x2, x29, #0x40
   25680:	mov	x0, x19
   25684:	bl	c4b0 <__gmpz_mul@plt>
   25688:	sub	x0, x29, #0x30
   2568c:	sub	x2, x29, #0x10
   25690:	mov	x1, x23
   25694:	bl	ca00 <__gmpz_divexact_gcd@plt>
   25698:	sub	x0, x29, #0x40
   2569c:	sub	x2, x29, #0x20
   256a0:	mov	x1, x22
   256a4:	bl	ca00 <__gmpz_divexact_gcd@plt>
   256a8:	add	x0, x19, #0x10
   256ac:	sub	x1, x29, #0x30
   256b0:	sub	x2, x29, #0x40
   256b4:	bl	c4b0 <__gmpz_mul@plt>
   256b8:	ldr	x0, [x29, #24]
   256bc:	cbz	x0, 25710 <__gmpq_mul@@Base+0x208>
   256c0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   256c4:	b	25710 <__gmpq_mul@@Base+0x208>
   256c8:	mov	x0, x19
   256cc:	mov	x1, x20
   256d0:	mov	x2, x20
   256d4:	bl	c4b0 <__gmpz_mul@plt>
   256d8:	add	x1, x20, #0x10
   256dc:	add	x0, x19, #0x10
   256e0:	mov	x2, x1
   256e4:	bl	c4b0 <__gmpz_mul@plt>
   256e8:	b	25710 <__gmpq_mul@@Base+0x208>
   256ec:	mov	x0, x19
   256f0:	ldr	w8, [x0, #16]!
   256f4:	cmp	w8, #0x0
   256f8:	stur	wzr, [x0, #-12]
   256fc:	b.le	25760 <__gmpq_mul@@Base+0x258>
   25700:	ldr	x0, [x19, #24]
   25704:	mov	w8, #0x1                   	// #1
   25708:	str	x8, [x0]
   2570c:	str	w8, [x19, #20]
   25710:	mov	sp, x29
   25714:	ldp	x20, x19, [sp, #80]
   25718:	ldp	x22, x21, [sp, #64]
   2571c:	ldp	x24, x23, [sp, #48]
   25720:	ldp	x26, x25, [sp, #32]
   25724:	ldr	x27, [sp, #16]
   25728:	ldp	x29, x30, [sp], #96
   2572c:	ret
   25730:	add	x0, x29, #0x18
   25734:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25738:	b	25598 <__gmpq_mul@@Base+0x90>
   2573c:	add	x0, x29, #0x18
   25740:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25744:	b	255cc <__gmpq_mul@@Base+0xc4>
   25748:	add	x0, x29, #0x18
   2574c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   25750:	b	25604 <__gmpq_mul@@Base+0xfc>
   25754:	add	x0, x29, #0x18
   25758:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2575c:	b	25634 <__gmpq_mul@@Base+0x12c>
   25760:	mov	w1, #0x1                   	// #1
   25764:	bl	c080 <__gmpz_realloc@plt>
   25768:	b	25704 <__gmpq_mul@@Base+0x1fc>

000000000002576c <__gmpq_neg@@Base>:
   2576c:	stp	x29, x30, [sp, #-64]!
   25770:	stp	x22, x21, [sp, #32]
   25774:	stp	x20, x19, [sp, #48]
   25778:	ldrsw	x22, [x1, #4]
   2577c:	mov	x19, x0
   25780:	cmp	x1, x0
   25784:	str	x23, [sp, #16]
   25788:	mov	x29, sp
   2578c:	b.eq	257e4 <__gmpq_neg@@Base+0x78>  // b.none
   25790:	ldrsw	x8, [x19]
   25794:	cmp	x22, #0x0
   25798:	cneg	x21, x22, mi  // mi = first
   2579c:	mov	x20, x1
   257a0:	cmp	x21, x8
   257a4:	b.gt	25800 <__gmpq_neg@@Base+0x94>
   257a8:	ldr	x0, [x19, #8]
   257ac:	ldr	x1, [x20, #8]
   257b0:	mov	x2, x21
   257b4:	bl	ca50 <__gmpn_copyi@plt>
   257b8:	mov	x0, x19
   257bc:	ldr	w23, [x20, #20]
   257c0:	ldr	w8, [x0, #16]!
   257c4:	sxtw	x21, w23
   257c8:	cmp	w23, w8
   257cc:	b.gt	25810 <__gmpq_neg@@Base+0xa4>
   257d0:	ldr	x0, [x19, #24]
   257d4:	str	w23, [x19, #20]
   257d8:	ldr	x1, [x20, #24]
   257dc:	mov	x2, x21
   257e0:	bl	ca50 <__gmpn_copyi@plt>
   257e4:	neg	w8, w22
   257e8:	str	w8, [x19, #4]
   257ec:	ldp	x20, x19, [sp, #48]
   257f0:	ldp	x22, x21, [sp, #32]
   257f4:	ldr	x23, [sp, #16]
   257f8:	ldp	x29, x30, [sp], #64
   257fc:	ret
   25800:	mov	x0, x19
   25804:	mov	x1, x21
   25808:	bl	c080 <__gmpz_realloc@plt>
   2580c:	b	257ac <__gmpq_neg@@Base+0x40>
   25810:	mov	x1, x21
   25814:	bl	c080 <__gmpz_realloc@plt>
   25818:	b	257d4 <__gmpq_neg@@Base+0x68>

000000000002581c <__gmpq_out_str@@Base>:
   2581c:	stp	x29, x30, [sp, #-48]!
   25820:	stp	x22, x21, [sp, #16]
   25824:	stp	x20, x19, [sp, #32]
   25828:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2582c:	ldr	x8, [x8, #3856]
   25830:	cmp	x0, #0x0
   25834:	mov	x29, sp
   25838:	mov	x22, x2
   2583c:	ldr	x8, [x8]
   25840:	mov	w21, w1
   25844:	csel	x19, x8, x0, eq  // eq = none
   25848:	mov	x0, x19
   2584c:	bl	c540 <__gmpz_out_str@plt>
   25850:	add	x22, x22, #0x10
   25854:	mov	x20, x0
   25858:	mov	w1, #0x1                   	// #1
   2585c:	mov	x0, x22
   25860:	bl	d1f0 <__gmpz_cmp_ui@plt>
   25864:	cbz	w0, 2588c <__gmpq_out_str@@Base+0x70>
   25868:	mov	w0, #0x2f                  	// #47
   2586c:	mov	x1, x19
   25870:	bl	c1e0 <putc@plt>
   25874:	mov	x0, x19
   25878:	mov	w1, w21
   2587c:	mov	x2, x22
   25880:	bl	c540 <__gmpz_out_str@plt>
   25884:	add	x8, x20, x0
   25888:	add	x20, x8, #0x1
   2588c:	mov	x0, x19
   25890:	bl	d4a0 <ferror@plt>
   25894:	cmp	w0, #0x0
   25898:	csel	x0, x20, xzr, eq  // eq = none
   2589c:	ldp	x20, x19, [sp, #32]
   258a0:	ldp	x22, x21, [sp, #16]
   258a4:	ldp	x29, x30, [sp], #48
   258a8:	ret

00000000000258ac <__gmpq_set@@Base>:
   258ac:	stp	x29, x30, [sp, #-48]!
   258b0:	stp	x20, x19, [sp, #32]
   258b4:	ldrsw	x8, [x1, #4]
   258b8:	ldrsw	x9, [x0]
   258bc:	str	x21, [sp, #16]
   258c0:	mov	x19, x1
   258c4:	cmp	x8, #0x0
   258c8:	cneg	x21, x8, mi  // mi = first
   258cc:	mov	x20, x0
   258d0:	cmp	x21, x9
   258d4:	mov	x29, sp
   258d8:	str	w8, [x0, #4]
   258dc:	b.gt	25924 <__gmpq_set@@Base+0x78>
   258e0:	ldr	x0, [x20, #8]
   258e4:	ldr	x1, [x19, #8]
   258e8:	mov	x2, x21
   258ec:	bl	ca50 <__gmpn_copyi@plt>
   258f0:	mov	x0, x20
   258f4:	ldrsw	x21, [x19, #20]
   258f8:	ldr	w8, [x0, #16]!
   258fc:	cmp	w21, w8
   25900:	str	w21, [x0, #4]
   25904:	b.gt	25934 <__gmpq_set@@Base+0x88>
   25908:	ldr	x0, [x20, #24]
   2590c:	ldr	x1, [x19, #24]
   25910:	mov	x2, x21
   25914:	ldp	x20, x19, [sp, #32]
   25918:	ldr	x21, [sp, #16]
   2591c:	ldp	x29, x30, [sp], #48
   25920:	b	ca50 <__gmpn_copyi@plt>
   25924:	mov	x0, x20
   25928:	mov	x1, x21
   2592c:	bl	c080 <__gmpz_realloc@plt>
   25930:	b	258e4 <__gmpq_set@@Base+0x38>
   25934:	mov	x1, x21
   25938:	bl	c080 <__gmpz_realloc@plt>
   2593c:	b	2590c <__gmpq_set@@Base+0x60>

0000000000025940 <__gmpq_set_den@@Base>:
   25940:	stp	x29, x30, [sp, #-32]!
   25944:	stp	x20, x19, [sp, #16]
   25948:	ldrsw	x9, [x1, #4]
   2594c:	mov	x8, x0
   25950:	ldrsw	x10, [x8, #16]!
   25954:	mov	x19, x1
   25958:	cmp	x9, #0x0
   2595c:	cneg	x20, x9, mi  // mi = first
   25960:	cmp	x20, x10
   25964:	mov	x29, sp
   25968:	str	w9, [x8, #4]
   2596c:	b.gt	25988 <__gmpq_set_den@@Base+0x48>
   25970:	ldr	x0, [x0, #24]
   25974:	ldr	x1, [x19, #8]
   25978:	mov	x2, x20
   2597c:	ldp	x20, x19, [sp, #16]
   25980:	ldp	x29, x30, [sp], #32
   25984:	b	ca50 <__gmpn_copyi@plt>
   25988:	mov	x0, x8
   2598c:	mov	x1, x20
   25990:	bl	c080 <__gmpz_realloc@plt>
   25994:	b	25974 <__gmpq_set_den@@Base+0x34>

0000000000025998 <__gmpq_set_num@@Base>:
   25998:	stp	x29, x30, [sp, #-32]!
   2599c:	stp	x20, x19, [sp, #16]
   259a0:	ldrsw	x8, [x1, #4]
   259a4:	ldrsw	x9, [x0]
   259a8:	mov	x19, x1
   259ac:	mov	x29, sp
   259b0:	cmp	x8, #0x0
   259b4:	cneg	x20, x8, mi  // mi = first
   259b8:	cmp	x20, x9
   259bc:	str	w8, [x0, #4]
   259c0:	b.gt	259dc <__gmpq_set_num@@Base+0x44>
   259c4:	ldr	x0, [x0, #8]
   259c8:	ldr	x1, [x19, #8]
   259cc:	mov	x2, x20
   259d0:	ldp	x20, x19, [sp, #16]
   259d4:	ldp	x29, x30, [sp], #32
   259d8:	b	ca50 <__gmpn_copyi@plt>
   259dc:	mov	x1, x20
   259e0:	bl	c080 <__gmpz_realloc@plt>
   259e4:	b	259c8 <__gmpq_set_num@@Base+0x30>

00000000000259e8 <__gmpq_set_si@@Base>:
   259e8:	stp	x29, x30, [sp, #-48]!
   259ec:	stp	x20, x19, [sp, #32]
   259f0:	mov	x19, x0
   259f4:	stp	x22, x21, [sp, #16]
   259f8:	mov	x29, sp
   259fc:	cbz	x1, 25a34 <__gmpq_set_si@@Base+0x4c>
   25a00:	ldr	w8, [x19]
   25a04:	cmp	x1, #0x0
   25a08:	mov	x20, x2
   25a0c:	mov	x21, x1
   25a10:	cneg	x22, x1, mi  // mi = first
   25a14:	cmp	w8, #0x0
   25a18:	b.le	25a80 <__gmpq_set_si@@Base+0x98>
   25a1c:	ldr	x0, [x19, #8]
   25a20:	cmp	x21, #0x0
   25a24:	mov	w8, #0x1                   	// #1
   25a28:	cneg	w8, w8, le
   25a2c:	str	x22, [x0]
   25a30:	b	25a3c <__gmpq_set_si@@Base+0x54>
   25a34:	mov	w8, wzr
   25a38:	mov	w20, #0x1                   	// #1
   25a3c:	mov	x0, x19
   25a40:	ldr	w9, [x0, #16]!
   25a44:	cmp	w9, #0x0
   25a48:	stur	w8, [x0, #-12]
   25a4c:	b.le	25a74 <__gmpq_set_si@@Base+0x8c>
   25a50:	ldr	x0, [x19, #24]
   25a54:	cmp	x20, #0x0
   25a58:	cset	w8, ne  // ne = any
   25a5c:	str	x20, [x0]
   25a60:	str	w8, [x19, #20]
   25a64:	ldp	x20, x19, [sp, #32]
   25a68:	ldp	x22, x21, [sp, #16]
   25a6c:	ldp	x29, x30, [sp], #48
   25a70:	ret
   25a74:	mov	w1, #0x1                   	// #1
   25a78:	bl	c080 <__gmpz_realloc@plt>
   25a7c:	b	25a54 <__gmpq_set_si@@Base+0x6c>
   25a80:	mov	w1, #0x1                   	// #1
   25a84:	mov	x0, x19
   25a88:	bl	c080 <__gmpz_realloc@plt>
   25a8c:	b	25a20 <__gmpq_set_si@@Base+0x38>

0000000000025a90 <__gmpq_set_str@@Base>:
   25a90:	stp	x29, x30, [sp, #-80]!
   25a94:	stp	x22, x21, [sp, #48]
   25a98:	mov	x21, x1
   25a9c:	stp	x20, x19, [sp, #64]
   25aa0:	mov	x20, x0
   25aa4:	mov	w1, #0x2f                  	// #47
   25aa8:	mov	x0, x21
   25aac:	str	x25, [sp, #16]
   25ab0:	stp	x24, x23, [sp, #32]
   25ab4:	mov	x29, sp
   25ab8:	mov	w19, w2
   25abc:	bl	cda0 <strchr@plt>
   25ac0:	cbz	x0, 25b44 <__gmpq_set_str@@Base+0xb4>
   25ac4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   25ac8:	ldr	x8, [x8, #3840]
   25acc:	sub	x23, x0, x21
   25ad0:	add	x24, x23, #0x1
   25ad4:	mov	x22, x0
   25ad8:	ldr	x8, [x8]
   25adc:	mov	x0, x24
   25ae0:	blr	x8
   25ae4:	mov	x1, x21
   25ae8:	mov	x2, x23
   25aec:	mov	x25, x0
   25af0:	bl	bed0 <memcpy@plt>
   25af4:	mov	x0, x20
   25af8:	mov	x1, x25
   25afc:	mov	w2, w19
   25b00:	strb	wzr, [x25, x23]
   25b04:	bl	c0d0 <__gmpz_set_str@plt>
   25b08:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   25b0c:	ldr	x8, [x8, #4016]
   25b10:	mov	w21, w0
   25b14:	mov	x0, x25
   25b18:	mov	x1, x24
   25b1c:	ldr	x8, [x8]
   25b20:	blr	x8
   25b24:	cbz	w21, 25b74 <__gmpq_set_str@@Base+0xe4>
   25b28:	mov	w0, w21
   25b2c:	ldp	x20, x19, [sp, #64]
   25b30:	ldp	x22, x21, [sp, #48]
   25b34:	ldp	x24, x23, [sp, #32]
   25b38:	ldr	x25, [sp, #16]
   25b3c:	ldp	x29, x30, [sp], #80
   25b40:	ret
   25b44:	mov	x0, x20
   25b48:	ldr	w8, [x0, #16]!
   25b4c:	mov	w9, #0x1                   	// #1
   25b50:	cmp	w8, #0x0
   25b54:	str	w9, [x0, #4]
   25b58:	b.le	25b98 <__gmpq_set_str@@Base+0x108>
   25b5c:	ldr	x0, [x20, #24]
   25b60:	mov	w8, #0x1                   	// #1
   25b64:	str	x8, [x0]
   25b68:	mov	x0, x20
   25b6c:	mov	x1, x21
   25b70:	b	25b7c <__gmpq_set_str@@Base+0xec>
   25b74:	add	x0, x20, #0x10
   25b78:	add	x1, x22, #0x1
   25b7c:	mov	w2, w19
   25b80:	ldp	x20, x19, [sp, #64]
   25b84:	ldp	x22, x21, [sp, #48]
   25b88:	ldp	x24, x23, [sp, #32]
   25b8c:	ldr	x25, [sp, #16]
   25b90:	ldp	x29, x30, [sp], #80
   25b94:	b	c0d0 <__gmpz_set_str@plt>
   25b98:	mov	w1, #0x1                   	// #1
   25b9c:	bl	c080 <__gmpz_realloc@plt>
   25ba0:	b	25b60 <__gmpq_set_str@@Base+0xd0>

0000000000025ba4 <__gmpq_set_ui@@Base>:
   25ba4:	stp	x29, x30, [sp, #-48]!
   25ba8:	stp	x20, x19, [sp, #32]
   25bac:	mov	x19, x0
   25bb0:	str	x21, [sp, #16]
   25bb4:	mov	x29, sp
   25bb8:	cbz	x1, 25be0 <__gmpq_set_ui@@Base+0x3c>
   25bbc:	ldr	w8, [x19]
   25bc0:	mov	x20, x2
   25bc4:	mov	x21, x1
   25bc8:	cmp	w8, #0x0
   25bcc:	b.le	25c2c <__gmpq_set_ui@@Base+0x88>
   25bd0:	ldr	x0, [x19, #8]
   25bd4:	mov	w8, #0x1                   	// #1
   25bd8:	str	x21, [x0]
   25bdc:	b	25be8 <__gmpq_set_ui@@Base+0x44>
   25be0:	mov	w8, wzr
   25be4:	mov	w20, #0x1                   	// #1
   25be8:	mov	x0, x19
   25bec:	ldr	w9, [x0, #16]!
   25bf0:	cmp	w9, #0x0
   25bf4:	stur	w8, [x0, #-12]
   25bf8:	b.le	25c20 <__gmpq_set_ui@@Base+0x7c>
   25bfc:	ldr	x0, [x19, #24]
   25c00:	cmp	x20, #0x0
   25c04:	cset	w8, ne  // ne = any
   25c08:	str	x20, [x0]
   25c0c:	str	w8, [x19, #20]
   25c10:	ldp	x20, x19, [sp, #32]
   25c14:	ldr	x21, [sp, #16]
   25c18:	ldp	x29, x30, [sp], #48
   25c1c:	ret
   25c20:	mov	w1, #0x1                   	// #1
   25c24:	bl	c080 <__gmpz_realloc@plt>
   25c28:	b	25c00 <__gmpq_set_ui@@Base+0x5c>
   25c2c:	mov	w1, #0x1                   	// #1
   25c30:	mov	x0, x19
   25c34:	bl	c080 <__gmpz_realloc@plt>
   25c38:	b	25bd4 <__gmpq_set_ui@@Base+0x30>

0000000000025c3c <__gmpq_equal@@Base>:
   25c3c:	ldrsw	x9, [x0, #4]
   25c40:	ldr	w8, [x1, #4]
   25c44:	cmp	w9, w8
   25c48:	b.ne	25ccc <__gmpq_equal@@Base+0x90>  // b.any
   25c4c:	ldrsw	x8, [x0, #20]
   25c50:	ldr	w10, [x1, #20]
   25c54:	cmp	w8, w10
   25c58:	b.ne	25ccc <__gmpq_equal@@Base+0x90>  // b.any
   25c5c:	cmp	x9, #0x0
   25c60:	cneg	x9, x9, mi  // mi = first
   25c64:	cmp	x9, #0x1
   25c68:	b.lt	25c94 <__gmpq_equal@@Base+0x58>  // b.tstop
   25c6c:	ldr	x10, [x0, #8]
   25c70:	ldr	x11, [x1, #8]
   25c74:	ldr	x12, [x10]
   25c78:	ldr	x13, [x11]
   25c7c:	cmp	x12, x13
   25c80:	b.ne	25ccc <__gmpq_equal@@Base+0x90>  // b.any
   25c84:	subs	x9, x9, #0x1
   25c88:	add	x11, x11, #0x8
   25c8c:	add	x10, x10, #0x8
   25c90:	b.ne	25c74 <__gmpq_equal@@Base+0x38>  // b.any
   25c94:	cmp	w8, #0x1
   25c98:	b.lt	25cc4 <__gmpq_equal@@Base+0x88>  // b.tstop
   25c9c:	ldr	x9, [x0, #24]
   25ca0:	ldr	x10, [x1, #24]
   25ca4:	ldr	x11, [x9]
   25ca8:	ldr	x12, [x10]
   25cac:	cmp	x11, x12
   25cb0:	b.ne	25ccc <__gmpq_equal@@Base+0x90>  // b.any
   25cb4:	subs	x8, x8, #0x1
   25cb8:	add	x10, x10, #0x8
   25cbc:	add	x9, x9, #0x8
   25cc0:	b.ne	25ca4 <__gmpq_equal@@Base+0x68>  // b.any
   25cc4:	mov	w0, #0x1                   	// #1
   25cc8:	ret
   25ccc:	mov	w0, wzr
   25cd0:	ret

0000000000025cd4 <__gmpq_set_z@@Base>:
   25cd4:	stp	x29, x30, [sp, #-48]!
   25cd8:	stp	x20, x19, [sp, #32]
   25cdc:	ldrsw	x8, [x1, #4]
   25ce0:	ldrsw	x9, [x0]
   25ce4:	str	x21, [sp, #16]
   25ce8:	mov	x20, x1
   25cec:	cmp	x8, #0x0
   25cf0:	cneg	x21, x8, mi  // mi = first
   25cf4:	mov	x19, x0
   25cf8:	cmp	x21, x9
   25cfc:	mov	x29, sp
   25d00:	str	w8, [x0, #4]
   25d04:	b.gt	25d48 <__gmpq_set_z@@Base+0x74>
   25d08:	ldr	x0, [x19, #8]
   25d0c:	ldr	x1, [x20, #8]
   25d10:	mov	x2, x21
   25d14:	bl	ca50 <__gmpn_copyi@plt>
   25d18:	mov	x0, x19
   25d1c:	ldr	w8, [x0, #16]!
   25d20:	cmp	w8, #0x0
   25d24:	b.le	25d58 <__gmpq_set_z@@Base+0x84>
   25d28:	ldr	x0, [x19, #24]
   25d2c:	mov	w8, #0x1                   	// #1
   25d30:	str	x8, [x0]
   25d34:	str	w8, [x19, #20]
   25d38:	ldp	x20, x19, [sp, #32]
   25d3c:	ldr	x21, [sp, #16]
   25d40:	ldp	x29, x30, [sp], #48
   25d44:	ret
   25d48:	mov	x0, x19
   25d4c:	mov	x1, x21
   25d50:	bl	c080 <__gmpz_realloc@plt>
   25d54:	b	25d0c <__gmpq_set_z@@Base+0x38>
   25d58:	mov	w1, #0x1                   	// #1
   25d5c:	bl	c080 <__gmpz_realloc@plt>
   25d60:	b	25d2c <__gmpq_set_z@@Base+0x58>

0000000000025d64 <__gmpq_set_d@@Base>:
   25d64:	sub	sp, sp, #0x70
   25d68:	stp	d9, d8, [sp, #16]
   25d6c:	mov	v8.16b, v0.16b
   25d70:	fmov	x8, d8
   25d74:	mvn	x8, x8
   25d78:	tst	x8, #0x7ff0000000000000
   25d7c:	stp	x29, x30, [sp, #32]
   25d80:	str	x25, [sp, #48]
   25d84:	stp	x24, x23, [sp, #64]
   25d88:	stp	x22, x21, [sp, #80]
   25d8c:	stp	x20, x19, [sp, #96]
   25d90:	add	x29, sp, #0x10
   25d94:	b.eq	25fb8 <__gmpq_set_d@@Base+0x254>  // b.none
   25d98:	fneg	d0, d8
   25d9c:	fcmp	d8, #0.0
   25da0:	fcsel	d9, d8, d0, ge  // ge = tcont
   25da4:	mov	x19, x0
   25da8:	mov	x0, sp
   25dac:	mov	v0.16b, v9.16b
   25db0:	bl	d280 <__gmp_extract_double@plt>
   25db4:	cmp	w0, #0x1
   25db8:	sxtw	x20, w0
   25dbc:	b.gt	25df4 <__gmpq_set_d@@Base+0x90>
   25dc0:	fcmp	d9, #0.0
   25dc4:	b.ne	25e84 <__gmpq_set_d@@Base+0x120>  // b.any
   25dc8:	mov	x0, x19
   25dcc:	ldr	w8, [x0, #16]!
   25dd0:	mov	w9, #0x1                   	// #1
   25dd4:	cmp	w8, #0x0
   25dd8:	stur	wzr, [x0, #-12]
   25ddc:	str	w9, [x0, #4]
   25de0:	b.le	25ed4 <__gmpq_set_d@@Base+0x170>
   25de4:	ldr	x0, [x19, #24]
   25de8:	mov	w8, #0x1                   	// #1
   25dec:	str	x8, [x0]
   25df0:	b	25e64 <__gmpq_set_d@@Base+0x100>
   25df4:	ldr	w8, [x19]
   25df8:	cmp	w20, w8
   25dfc:	b.gt	25eac <__gmpq_set_d@@Base+0x148>
   25e00:	ldr	x21, [x19, #8]
   25e04:	cmp	w20, #0x2
   25e08:	b.eq	25e2c <__gmpq_set_d@@Base+0xc8>  // b.none
   25e0c:	subs	x22, x20, #0x2
   25e10:	b.eq	25e28 <__gmpq_set_d@@Base+0xc4>  // b.none
   25e14:	lsl	x8, x20, #3
   25e18:	sub	x2, x8, #0x10
   25e1c:	mov	x0, x21
   25e20:	mov	w1, wzr
   25e24:	bl	c5f0 <memset@plt>
   25e28:	add	x21, x21, x22, lsl #3
   25e2c:	ldr	q0, [sp]
   25e30:	mov	x0, x19
   25e34:	str	q0, [x21]
   25e38:	ldr	w8, [x0, #16]!
   25e3c:	cmp	w8, #0x0
   25e40:	b.le	25ec8 <__gmpq_set_d@@Base+0x164>
   25e44:	ldr	x0, [x19, #24]
   25e48:	mov	w25, #0x1                   	// #1
   25e4c:	str	x25, [x0]
   25e50:	neg	w8, w20
   25e54:	fcmp	d8, #0.0
   25e58:	csel	x8, x8, x20, mi  // mi = first
   25e5c:	str	w25, [x19, #20]
   25e60:	str	w8, [x19, #4]
   25e64:	ldp	x20, x19, [sp, #96]
   25e68:	ldp	x22, x21, [sp, #80]
   25e6c:	ldp	x24, x23, [sp, #64]
   25e70:	ldr	x25, [sp, #48]
   25e74:	ldp	x29, x30, [sp, #32]
   25e78:	ldp	d9, d8, [sp, #16]
   25e7c:	add	sp, sp, #0x70
   25e80:	ret
   25e84:	ldr	w8, [x19]
   25e88:	cmp	w8, #0x1
   25e8c:	b.le	25ee0 <__gmpq_set_d@@Base+0x17c>
   25e90:	ldr	x22, [x19, #8]
   25e94:	ldp	x9, x8, [sp]
   25e98:	cbz	x9, 25ef8 <__gmpq_set_d@@Base+0x194>
   25e9c:	str	x8, [x22, #8]
   25ea0:	mov	w21, #0x2                   	// #2
   25ea4:	mov	x8, x9
   25ea8:	b	25efc <__gmpq_set_d@@Base+0x198>
   25eac:	mov	x0, x19
   25eb0:	mov	x1, x20
   25eb4:	bl	c080 <__gmpz_realloc@plt>
   25eb8:	mov	x21, x0
   25ebc:	cmp	w20, #0x2
   25ec0:	b.ne	25e0c <__gmpq_set_d@@Base+0xa8>  // b.any
   25ec4:	b	25e2c <__gmpq_set_d@@Base+0xc8>
   25ec8:	mov	w1, #0x1                   	// #1
   25ecc:	bl	c080 <__gmpz_realloc@plt>
   25ed0:	b	25e48 <__gmpq_set_d@@Base+0xe4>
   25ed4:	mov	w1, #0x1                   	// #1
   25ed8:	bl	c080 <__gmpz_realloc@plt>
   25edc:	b	25de8 <__gmpq_set_d@@Base+0x84>
   25ee0:	mov	w1, #0x2                   	// #2
   25ee4:	mov	x0, x19
   25ee8:	bl	c080 <__gmpz_realloc@plt>
   25eec:	mov	x22, x0
   25ef0:	ldp	x9, x8, [sp]
   25ef4:	cbnz	x9, 25e9c <__gmpq_set_d@@Base+0x138>
   25ef8:	mov	w21, #0x1                   	// #1
   25efc:	str	x8, [x22]
   25f00:	mov	x0, x19
   25f04:	ldrsw	x8, [x0, #16]!
   25f08:	sub	x24, x21, x20
   25f0c:	add	x20, x24, #0x1
   25f10:	cmp	x20, x8
   25f14:	b.gt	25fa0 <__gmpq_set_d@@Base+0x23c>
   25f18:	ldr	x23, [x19, #24]
   25f1c:	subs	x25, x20, #0x1
   25f20:	b.eq	25f34 <__gmpq_set_d@@Base+0x1d0>  // b.none
   25f24:	lsl	x2, x24, #3
   25f28:	mov	x0, x23
   25f2c:	mov	w1, wzr
   25f30:	bl	c5f0 <memset@plt>
   25f34:	mov	w8, #0x1                   	// #1
   25f38:	str	x8, [x23, x25, lsl #3]
   25f3c:	ldr	x8, [x22]
   25f40:	ldr	x9, [x23]
   25f44:	orr	x8, x9, x8
   25f48:	rbit	x8, x8
   25f4c:	clz	x24, x8
   25f50:	cbz	w24, 25f94 <__gmpq_set_d@@Base+0x230>
   25f54:	mov	x0, x22
   25f58:	mov	x1, x22
   25f5c:	mov	x2, x21
   25f60:	mov	w3, w24
   25f64:	bl	c1a0 <__gmpn_rshift@plt>
   25f68:	add	x8, x22, x21, lsl #3
   25f6c:	ldur	x8, [x8, #-8]
   25f70:	neg	x9, x24
   25f74:	mov	w10, #0x1                   	// #1
   25f78:	add	x11, x23, x20, lsl #3
   25f7c:	cmp	x8, #0x0
   25f80:	lsl	x9, x10, x9
   25f84:	cset	w8, eq  // eq = none
   25f88:	sub	x20, x21, x8
   25f8c:	stur	x9, [x11, #-16]
   25f90:	b	25e50 <__gmpq_set_d@@Base+0xec>
   25f94:	mov	x25, x20
   25f98:	mov	x20, x21
   25f9c:	b	25e50 <__gmpq_set_d@@Base+0xec>
   25fa0:	mov	x1, x20
   25fa4:	bl	c080 <__gmpz_realloc@plt>
   25fa8:	mov	x23, x0
   25fac:	subs	x25, x20, #0x1
   25fb0:	b.ne	25f24 <__gmpq_set_d@@Base+0x1c0>  // b.any
   25fb4:	b	25f34 <__gmpq_set_d@@Base+0x1d0>
   25fb8:	bl	c1b0 <__gmp_invalid_operation@plt>

0000000000025fbc <__gmpq_set_f@@Base>:
   25fbc:	stp	x29, x30, [sp, #-96]!
   25fc0:	stp	x26, x25, [sp, #32]
   25fc4:	stp	x24, x23, [sp, #48]
   25fc8:	stp	x22, x21, [sp, #64]
   25fcc:	stp	x20, x19, [sp, #80]
   25fd0:	ldrsw	x26, [x1, #4]
   25fd4:	mov	x19, x0
   25fd8:	str	x27, [sp, #16]
   25fdc:	mov	x29, sp
   25fe0:	cbz	w26, 26090 <__gmpq_set_f@@Base+0xd4>
   25fe4:	ldp	x21, x22, [x1, #8]
   25fe8:	cmp	w26, #0x0
   25fec:	cneg	x20, x26, lt  // lt = tstop
   25ff0:	ldr	x25, [x22]
   25ff4:	cbnz	x25, 26004 <__gmpq_set_f@@Base+0x48>
   25ff8:	ldr	x25, [x22, #8]!
   25ffc:	sub	x20, x20, #0x1
   26000:	cbz	x25, 25ff8 <__gmpq_set_f@@Base+0x3c>
   26004:	subs	x27, x20, x21
   26008:	b.le	260bc <__gmpq_set_f@@Base+0x100>
   2600c:	ldrsw	x8, [x19]
   26010:	cmp	x20, x8
   26014:	b.gt	26130 <__gmpq_set_f@@Base+0x174>
   26018:	ldr	x24, [x19, #8]
   2601c:	mov	x0, x19
   26020:	ldrsw	x8, [x0, #16]!
   26024:	cmp	x27, x8
   26028:	b.ge	26144 <__gmpq_set_f@@Base+0x188>  // b.tcont
   2602c:	ldr	x23, [x19, #24]
   26030:	tbnz	w25, #0, 26154 <__gmpq_set_f@@Base+0x198>
   26034:	rbit	x8, x25
   26038:	clz	x25, x8
   2603c:	mov	x0, x24
   26040:	mov	x1, x22
   26044:	mov	x2, x20
   26048:	mov	w3, w25
   2604c:	sub	x27, x27, #0x1
   26050:	bl	c1a0 <__gmpn_rshift@plt>
   26054:	sub	x8, x20, #0x1
   26058:	ldr	x9, [x24, x8, lsl #3]
   2605c:	cmp	x9, #0x0
   26060:	cset	w9, eq  // eq = none
   26064:	sub	x20, x20, x9
   26068:	cbz	x27, 26080 <__gmpq_set_f@@Base+0xc4>
   2606c:	sub	x8, x8, x21
   26070:	lsl	x2, x8, #3
   26074:	mov	x0, x23
   26078:	mov	w1, wzr
   2607c:	bl	c5f0 <memset@plt>
   26080:	sub	w8, w25, #0x1
   26084:	mov	x9, #0x8000000000000000    	// #-9223372036854775808
   26088:	lsr	x8, x9, x8
   2608c:	b	2617c <__gmpq_set_f@@Base+0x1c0>
   26090:	mov	x0, x19
   26094:	ldr	w8, [x0, #16]!
   26098:	mov	w9, #0x1                   	// #1
   2609c:	cmp	w8, #0x0
   260a0:	stur	wzr, [x0, #-12]
   260a4:	str	w9, [x0, #4]
   260a8:	b.le	26124 <__gmpq_set_f@@Base+0x168>
   260ac:	ldr	x0, [x19, #24]
   260b0:	mov	w8, #0x1                   	// #1
   260b4:	str	x8, [x0]
   260b8:	b	26198 <__gmpq_set_f@@Base+0x1dc>
   260bc:	ldrsw	x8, [x19]
   260c0:	cmp	x21, x8
   260c4:	b.gt	261b4 <__gmpq_set_f@@Base+0x1f8>
   260c8:	ldr	x23, [x19, #8]
   260cc:	cmp	x20, x21
   260d0:	b.eq	260e8 <__gmpq_set_f@@Base+0x12c>  // b.none
   260d4:	sub	x8, x21, x20
   260d8:	lsl	x2, x8, #3
   260dc:	mov	x0, x23
   260e0:	mov	w1, wzr
   260e4:	bl	c5f0 <memset@plt>
   260e8:	add	x8, x23, x21, lsl #3
   260ec:	sub	x0, x8, x20, lsl #3
   260f0:	mov	x1, x22
   260f4:	mov	x2, x20
   260f8:	bl	ca50 <__gmpn_copyi@plt>
   260fc:	mov	x0, x19
   26100:	ldr	w9, [x0, #16]!
   26104:	neg	w8, w21
   26108:	cmp	w26, #0x0
   2610c:	mov	w10, #0x1                   	// #1
   26110:	csel	x8, x21, x8, ge  // ge = tcont
   26114:	cmp	w9, #0x0
   26118:	stur	w8, [x0, #-12]
   2611c:	str	w10, [x0, #4]
   26120:	b.gt	260ac <__gmpq_set_f@@Base+0xf0>
   26124:	mov	w1, #0x1                   	// #1
   26128:	bl	c080 <__gmpz_realloc@plt>
   2612c:	b	260b0 <__gmpq_set_f@@Base+0xf4>
   26130:	mov	x0, x19
   26134:	mov	x1, x20
   26138:	bl	c080 <__gmpz_realloc@plt>
   2613c:	mov	x24, x0
   26140:	b	2601c <__gmpq_set_f@@Base+0x60>
   26144:	add	x1, x27, #0x1
   26148:	bl	c080 <__gmpz_realloc@plt>
   2614c:	mov	x23, x0
   26150:	tbz	w25, #0, 26034 <__gmpq_set_f@@Base+0x78>
   26154:	mov	x0, x24
   26158:	mov	x1, x22
   2615c:	mov	x2, x20
   26160:	bl	ca50 <__gmpn_copyi@plt>
   26164:	cbz	x27, 26178 <__gmpq_set_f@@Base+0x1bc>
   26168:	lsl	x2, x27, #3
   2616c:	mov	x0, x23
   26170:	mov	w1, wzr
   26174:	bl	c5f0 <memset@plt>
   26178:	mov	w8, #0x1                   	// #1
   2617c:	str	x8, [x23, x27, lsl #3]
   26180:	neg	w8, w20
   26184:	cmp	w26, #0x0
   26188:	add	w9, w27, #0x1
   2618c:	csel	x8, x20, x8, ge  // ge = tcont
   26190:	str	w8, [x19, #4]
   26194:	str	w9, [x19, #20]
   26198:	ldp	x20, x19, [sp, #80]
   2619c:	ldp	x22, x21, [sp, #64]
   261a0:	ldp	x24, x23, [sp, #48]
   261a4:	ldp	x26, x25, [sp, #32]
   261a8:	ldr	x27, [sp, #16]
   261ac:	ldp	x29, x30, [sp], #96
   261b0:	ret
   261b4:	mov	x0, x19
   261b8:	mov	x1, x21
   261bc:	bl	c080 <__gmpz_realloc@plt>
   261c0:	mov	x23, x0
   261c4:	cmp	x20, x21
   261c8:	b.ne	260d4 <__gmpq_set_f@@Base+0x118>  // b.any
   261cc:	b	260e8 <__gmpq_set_f@@Base+0x12c>

00000000000261d0 <__gmpq_swap@@Base>:
   261d0:	ldr	w8, [x1]
   261d4:	ldr	w9, [x0]
   261d8:	str	w8, [x0]
   261dc:	ldr	x8, [x1, #16]
   261e0:	str	w9, [x1]
   261e4:	ldr	x9, [x0, #16]
   261e8:	ldr	w10, [x0, #4]
   261ec:	str	x8, [x0, #16]
   261f0:	ldr	w8, [x1, #4]
   261f4:	str	w8, [x0, #4]
   261f8:	str	w10, [x1, #4]
   261fc:	str	x9, [x1, #16]
   26200:	ldr	x8, [x1, #8]
   26204:	ldr	x9, [x0, #8]
   26208:	str	x8, [x0, #8]
   2620c:	str	x9, [x1, #8]
   26210:	ldr	x8, [x1, #24]
   26214:	ldr	x9, [x0, #24]
   26218:	str	x8, [x0, #24]
   2621c:	str	x9, [x1, #24]
   26220:	ret

0000000000026224 <__gmpn_add@@Base>:
   26224:	stp	x29, x30, [sp, #-48]!
   26228:	stp	x22, x21, [sp, #16]
   2622c:	stp	x20, x19, [sp, #32]
   26230:	mov	x21, x4
   26234:	mov	x22, x2
   26238:	mov	x19, x1
   2623c:	mov	x20, x0
   26240:	mov	x29, sp
   26244:	cbz	x4, 26284 <__gmpn_add@@Base+0x60>
   26248:	mov	x0, x20
   2624c:	mov	x1, x19
   26250:	mov	x2, x3
   26254:	mov	x3, x21
   26258:	bl	ca70 <__gmpn_add_n@plt>
   2625c:	cbz	x0, 26284 <__gmpn_add@@Base+0x60>
   26260:	mov	w0, #0x1                   	// #1
   26264:	cmp	x21, x22
   26268:	b.ge	26328 <__gmpn_add@@Base+0x104>  // b.tcont
   2626c:	lsl	x8, x21, #3
   26270:	ldr	x9, [x19, x8]
   26274:	add	x21, x21, #0x1
   26278:	adds	x9, x9, #0x1
   2627c:	str	x9, [x20, x8]
   26280:	b.cs	26264 <__gmpn_add@@Base+0x40>  // b.hs, b.nlast
   26284:	cmp	x20, x19
   26288:	mov	x0, xzr
   2628c:	b.eq	26328 <__gmpn_add@@Base+0x104>  // b.none
   26290:	cmp	x21, x22
   26294:	b.ge	26328 <__gmpn_add@@Base+0x104>  // b.tcont
   26298:	sub	x8, x22, x21
   2629c:	cmp	x8, #0x4
   262a0:	b.cc	26304 <__gmpn_add@@Base+0xe0>  // b.lo, b.ul, b.last
   262a4:	lsl	x10, x21, #3
   262a8:	lsl	x9, x22, #3
   262ac:	add	x11, x20, x10
   262b0:	add	x12, x19, x9
   262b4:	cmp	x11, x12
   262b8:	b.cs	262cc <__gmpn_add@@Base+0xa8>  // b.hs, b.nlast
   262bc:	add	x9, x20, x9
   262c0:	add	x11, x19, x10
   262c4:	cmp	x11, x9
   262c8:	b.cc	26304 <__gmpn_add@@Base+0xe0>  // b.lo, b.ul, b.last
   262cc:	and	x9, x8, #0xfffffffffffffffc
   262d0:	add	x11, x10, #0x10
   262d4:	add	x21, x21, x9
   262d8:	add	x10, x19, x11
   262dc:	add	x11, x20, x11
   262e0:	mov	x12, x9
   262e4:	ldp	q0, q1, [x10, #-16]
   262e8:	add	x10, x10, #0x20
   262ec:	subs	x12, x12, #0x4
   262f0:	stp	q0, q1, [x11, #-16]
   262f4:	add	x11, x11, #0x20
   262f8:	b.ne	262e4 <__gmpn_add@@Base+0xc0>  // b.any
   262fc:	cmp	x8, x9
   26300:	b.eq	26324 <__gmpn_add@@Base+0x100>  // b.none
   26304:	lsl	x10, x21, #3
   26308:	sub	x8, x22, x21
   2630c:	add	x9, x20, x10
   26310:	add	x10, x19, x10
   26314:	ldr	x11, [x10], #8
   26318:	subs	x8, x8, #0x1
   2631c:	str	x11, [x9], #8
   26320:	b.ne	26314 <__gmpn_add@@Base+0xf0>  // b.any
   26324:	mov	x0, xzr
   26328:	ldp	x20, x19, [sp, #32]
   2632c:	ldp	x22, x21, [sp, #16]
   26330:	ldp	x29, x30, [sp], #48
   26334:	ret

0000000000026338 <__gmpn_add_1@@Base>:
   26338:	ldr	x8, [x1]
   2633c:	adds	x8, x8, x3
   26340:	str	x8, [x0]
   26344:	b.cc	26438 <__gmpn_add_1@@Base+0x100>  // b.lo, b.ul, b.last
   26348:	mov	x11, xzr
   2634c:	sub	x10, x2, #0x1
   26350:	mov	w8, #0x1                   	// #1
   26354:	mov	w9, #0x1                   	// #1
   26358:	cmp	x9, x2
   2635c:	b.ge	26430 <__gmpn_add_1@@Base+0xf8>  // b.tcont
   26360:	add	x12, x1, x11
   26364:	ldr	x12, [x12, #8]
   26368:	add	x13, x0, x11
   2636c:	add	x9, x9, #0x1
   26370:	add	x11, x11, #0x8
   26374:	adds	x12, x12, #0x1
   26378:	sub	x10, x10, #0x1
   2637c:	str	x12, [x13, #8]
   26380:	b.cs	26358 <__gmpn_add_1@@Base+0x20>  // b.hs, b.nlast
   26384:	cmp	x1, x0
   26388:	mov	x8, xzr
   2638c:	b.eq	26430 <__gmpn_add_1@@Base+0xf8>  // b.none
   26390:	cmp	x9, x2
   26394:	b.ge	26430 <__gmpn_add_1@@Base+0xf8>  // b.tcont
   26398:	sub	x8, x2, x9
   2639c:	cmp	x8, #0x4
   263a0:	b.cc	26410 <__gmpn_add_1@@Base+0xd8>  // b.lo, b.ul, b.last
   263a4:	add	x13, x0, x11
   263a8:	lsl	x12, x2, #3
   263ac:	add	x13, x13, #0x8
   263b0:	add	x14, x1, x12
   263b4:	cmp	x13, x14
   263b8:	b.cs	263d0 <__gmpn_add_1@@Base+0x98>  // b.hs, b.nlast
   263bc:	add	x13, x1, x11
   263c0:	add	x12, x0, x12
   263c4:	add	x13, x13, #0x8
   263c8:	cmp	x13, x12
   263cc:	b.cc	26410 <__gmpn_add_1@@Base+0xd8>  // b.lo, b.ul, b.last
   263d0:	add	x12, x0, x11
   263d4:	add	x13, x1, x11
   263d8:	and	x11, x8, #0xfffffffffffffffc
   263dc:	and	x14, x10, #0xfffffffffffffffc
   263e0:	add	x10, x12, #0x18
   263e4:	add	x12, x13, #0x18
   263e8:	add	x9, x14, x9
   263ec:	mov	x13, x11
   263f0:	ldp	q0, q1, [x12, #-16]
   263f4:	add	x12, x12, #0x20
   263f8:	subs	x13, x13, #0x4
   263fc:	stp	q0, q1, [x10, #-16]
   26400:	add	x10, x10, #0x20
   26404:	b.ne	263f0 <__gmpn_add_1@@Base+0xb8>  // b.any
   26408:	cmp	x8, x11
   2640c:	b.eq	264d8 <__gmpn_add_1@@Base+0x1a0>  // b.none
   26410:	lsl	x10, x9, #3
   26414:	sub	x8, x2, x9
   26418:	add	x9, x0, x10
   2641c:	add	x10, x1, x10
   26420:	ldr	x11, [x10], #8
   26424:	subs	x8, x8, #0x1
   26428:	str	x11, [x9], #8
   2642c:	b.ne	26420 <__gmpn_add_1@@Base+0xe8>  // b.any
   26430:	mov	x0, x8
   26434:	ret
   26438:	cmp	x1, x0
   2643c:	mov	x8, xzr
   26440:	b.eq	26430 <__gmpn_add_1@@Base+0xf8>  // b.none
   26444:	cmp	x2, #0x2
   26448:	b.lt	26430 <__gmpn_add_1@@Base+0xf8>  // b.tstop
   2644c:	sub	x8, x2, #0x1
   26450:	cmp	x8, #0x4
   26454:	b.cc	2647c <__gmpn_add_1@@Base+0x144>  // b.lo, b.ul, b.last
   26458:	lsl	x9, x2, #3
   2645c:	add	x10, x0, #0x8
   26460:	add	x11, x1, x9
   26464:	cmp	x10, x11
   26468:	b.cs	264a4 <__gmpn_add_1@@Base+0x16c>  // b.hs, b.nlast
   2646c:	add	x9, x0, x9
   26470:	add	x10, x1, #0x8
   26474:	cmp	x10, x9
   26478:	b.cs	264a4 <__gmpn_add_1@@Base+0x16c>  // b.hs, b.nlast
   2647c:	mov	w9, #0x1                   	// #1
   26480:	lsl	x10, x9, #3
   26484:	sub	x8, x2, x9
   26488:	add	x9, x0, x10
   2648c:	add	x10, x1, x10
   26490:	ldr	x11, [x10], #8
   26494:	subs	x8, x8, #0x1
   26498:	str	x11, [x9], #8
   2649c:	b.ne	26490 <__gmpn_add_1@@Base+0x158>  // b.any
   264a0:	b	26430 <__gmpn_add_1@@Base+0xf8>
   264a4:	and	x10, x8, #0xfffffffffffffffc
   264a8:	add	x11, x1, #0x18
   264ac:	orr	x9, x10, #0x1
   264b0:	add	x12, x0, #0x18
   264b4:	mov	x13, x10
   264b8:	ldp	q0, q1, [x11, #-16]
   264bc:	add	x11, x11, #0x20
   264c0:	subs	x13, x13, #0x4
   264c4:	stp	q0, q1, [x12, #-16]
   264c8:	add	x12, x12, #0x20
   264cc:	b.ne	264b8 <__gmpn_add_1@@Base+0x180>  // b.any
   264d0:	cmp	x8, x10
   264d4:	b.ne	26480 <__gmpn_add_1@@Base+0x148>  // b.any
   264d8:	mov	x8, xzr
   264dc:	mov	x0, x8
   264e0:	ret
   264e4:	nop
   264e8:	nop
   264ec:	nop

00000000000264f0 <__gmpn_add_nc@@Base>:
   264f0:	cmp	x4, #0x1
   264f4:	b	264fc <__gmpn_add_n@@Base+0x4>

00000000000264f8 <__gmpn_add_n@@Base>:
   264f8:	cmn	xzr, xzr
   264fc:	lsr	x18, x3, #2
   26500:	tbz	w3, #0, 26548 <__gmpn_add_n@@Base+0x50>
   26504:	ldr	x7, [x1]
   26508:	ldr	x11, [x2]
   2650c:	adcs	x13, x7, x11
   26510:	str	x13, [x0], #8
   26514:	tbnz	w3, #1, 26530 <__gmpn_add_n@@Base+0x38>
   26518:	cbz	x18, 265ac <__gmpn_add_n@@Base+0xb4>
   2651c:	ldp	x4, x5, [x1, #8]
   26520:	ldp	x8, x9, [x2, #8]
   26524:	sub	x1, x1, #0x8
   26528:	sub	x2, x2, #0x8
   2652c:	b	26584 <__gmpn_add_n@@Base+0x8c>
   26530:	ldp	x6, x7, [x1, #8]
   26534:	ldp	x10, x11, [x2, #8]
   26538:	add	x1, x1, #0x8
   2653c:	add	x2, x2, #0x8
   26540:	cbz	x18, 265a0 <__gmpn_add_n@@Base+0xa8>
   26544:	b	26570 <__gmpn_add_n@@Base+0x78>
   26548:	tbnz	w3, #1, 26560 <__gmpn_add_n@@Base+0x68>
   2654c:	ldp	x4, x5, [x1]
   26550:	ldp	x8, x9, [x2]
   26554:	sub	x1, x1, #0x10
   26558:	sub	x2, x2, #0x10
   2655c:	b	26584 <__gmpn_add_n@@Base+0x8c>
   26560:	ldp	x6, x7, [x1]
   26564:	ldp	x10, x11, [x2]
   26568:	cbz	x18, 265a0 <__gmpn_add_n@@Base+0xa8>
   2656c:	nop
   26570:	ldp	x4, x5, [x1, #16]
   26574:	ldp	x8, x9, [x2, #16]
   26578:	adcs	x12, x6, x10
   2657c:	adcs	x13, x7, x11
   26580:	stp	x12, x13, [x0], #16
   26584:	ldp	x6, x7, [x1, #32]!
   26588:	ldp	x10, x11, [x2, #32]!
   2658c:	adcs	x12, x4, x8
   26590:	adcs	x13, x5, x9
   26594:	stp	x12, x13, [x0], #16
   26598:	sub	x18, x18, #0x1
   2659c:	cbnz	x18, 26570 <__gmpn_add_n@@Base+0x78>
   265a0:	adcs	x12, x6, x10
   265a4:	adcs	x13, x7, x11
   265a8:	stp	x12, x13, [x0]
   265ac:	cset	x0, cs  // cs = hs, nlast
   265b0:	ret

00000000000265b4 <__gmpn_sub@@Base>:
   265b4:	stp	x29, x30, [sp, #-48]!
   265b8:	stp	x22, x21, [sp, #16]
   265bc:	stp	x20, x19, [sp, #32]
   265c0:	mov	x21, x4
   265c4:	mov	x22, x2
   265c8:	mov	x19, x1
   265cc:	mov	x20, x0
   265d0:	mov	x29, sp
   265d4:	cbz	x4, 26614 <__gmpn_sub@@Base+0x60>
   265d8:	mov	x0, x20
   265dc:	mov	x1, x19
   265e0:	mov	x2, x3
   265e4:	mov	x3, x21
   265e8:	bl	c2d0 <__gmpn_sub_n@plt>
   265ec:	cbz	x0, 26614 <__gmpn_sub@@Base+0x60>
   265f0:	mov	w0, #0x1                   	// #1
   265f4:	cmp	x21, x22
   265f8:	b.ge	266b8 <__gmpn_sub@@Base+0x104>  // b.tcont
   265fc:	lsl	x8, x21, #3
   26600:	ldr	x9, [x19, x8]
   26604:	add	x21, x21, #0x1
   26608:	sub	x10, x9, #0x1
   2660c:	str	x10, [x20, x8]
   26610:	cbz	x9, 265f4 <__gmpn_sub@@Base+0x40>
   26614:	cmp	x20, x19
   26618:	mov	x0, xzr
   2661c:	b.eq	266b8 <__gmpn_sub@@Base+0x104>  // b.none
   26620:	cmp	x21, x22
   26624:	b.ge	266b8 <__gmpn_sub@@Base+0x104>  // b.tcont
   26628:	sub	x8, x22, x21
   2662c:	cmp	x8, #0x4
   26630:	b.cc	26694 <__gmpn_sub@@Base+0xe0>  // b.lo, b.ul, b.last
   26634:	lsl	x10, x21, #3
   26638:	lsl	x9, x22, #3
   2663c:	add	x11, x20, x10
   26640:	add	x12, x19, x9
   26644:	cmp	x11, x12
   26648:	b.cs	2665c <__gmpn_sub@@Base+0xa8>  // b.hs, b.nlast
   2664c:	add	x9, x20, x9
   26650:	add	x11, x19, x10
   26654:	cmp	x11, x9
   26658:	b.cc	26694 <__gmpn_sub@@Base+0xe0>  // b.lo, b.ul, b.last
   2665c:	and	x9, x8, #0xfffffffffffffffc
   26660:	add	x11, x10, #0x10
   26664:	add	x21, x21, x9
   26668:	add	x10, x19, x11
   2666c:	add	x11, x20, x11
   26670:	mov	x12, x9
   26674:	ldp	q0, q1, [x10, #-16]
   26678:	add	x10, x10, #0x20
   2667c:	subs	x12, x12, #0x4
   26680:	stp	q0, q1, [x11, #-16]
   26684:	add	x11, x11, #0x20
   26688:	b.ne	26674 <__gmpn_sub@@Base+0xc0>  // b.any
   2668c:	cmp	x8, x9
   26690:	b.eq	266b4 <__gmpn_sub@@Base+0x100>  // b.none
   26694:	lsl	x10, x21, #3
   26698:	sub	x8, x22, x21
   2669c:	add	x9, x20, x10
   266a0:	add	x10, x19, x10
   266a4:	ldr	x11, [x10], #8
   266a8:	subs	x8, x8, #0x1
   266ac:	str	x11, [x9], #8
   266b0:	b.ne	266a4 <__gmpn_sub@@Base+0xf0>  // b.any
   266b4:	mov	x0, xzr
   266b8:	ldp	x20, x19, [sp, #32]
   266bc:	ldp	x22, x21, [sp, #16]
   266c0:	ldp	x29, x30, [sp], #48
   266c4:	ret

00000000000266c8 <__gmpn_sub_1@@Base>:
   266c8:	ldr	x8, [x1]
   266cc:	subs	x8, x8, x3
   266d0:	str	x8, [x0]
   266d4:	b.cs	267c8 <__gmpn_sub_1@@Base+0x100>  // b.hs, b.nlast
   266d8:	mov	x11, xzr
   266dc:	sub	x10, x2, #0x1
   266e0:	mov	w8, #0x1                   	// #1
   266e4:	mov	w9, #0x1                   	// #1
   266e8:	cmp	x9, x2
   266ec:	b.ge	267c0 <__gmpn_sub_1@@Base+0xf8>  // b.tcont
   266f0:	add	x12, x1, x11
   266f4:	ldr	x12, [x12, #8]
   266f8:	add	x13, x0, x11
   266fc:	add	x9, x9, #0x1
   26700:	add	x11, x11, #0x8
   26704:	sub	x14, x12, #0x1
   26708:	sub	x10, x10, #0x1
   2670c:	str	x14, [x13, #8]
   26710:	cbz	x12, 266e8 <__gmpn_sub_1@@Base+0x20>
   26714:	cmp	x1, x0
   26718:	mov	x8, xzr
   2671c:	b.eq	267c0 <__gmpn_sub_1@@Base+0xf8>  // b.none
   26720:	cmp	x9, x2
   26724:	b.ge	267c0 <__gmpn_sub_1@@Base+0xf8>  // b.tcont
   26728:	sub	x8, x2, x9
   2672c:	cmp	x8, #0x4
   26730:	b.cc	267a0 <__gmpn_sub_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26734:	add	x13, x0, x11
   26738:	lsl	x12, x2, #3
   2673c:	add	x13, x13, #0x8
   26740:	add	x14, x1, x12
   26744:	cmp	x13, x14
   26748:	b.cs	26760 <__gmpn_sub_1@@Base+0x98>  // b.hs, b.nlast
   2674c:	add	x13, x1, x11
   26750:	add	x12, x0, x12
   26754:	add	x13, x13, #0x8
   26758:	cmp	x13, x12
   2675c:	b.cc	267a0 <__gmpn_sub_1@@Base+0xd8>  // b.lo, b.ul, b.last
   26760:	add	x12, x0, x11
   26764:	add	x13, x1, x11
   26768:	and	x11, x8, #0xfffffffffffffffc
   2676c:	and	x14, x10, #0xfffffffffffffffc
   26770:	add	x10, x12, #0x18
   26774:	add	x12, x13, #0x18
   26778:	add	x9, x14, x9
   2677c:	mov	x13, x11
   26780:	ldp	q0, q1, [x12, #-16]
   26784:	add	x12, x12, #0x20
   26788:	subs	x13, x13, #0x4
   2678c:	stp	q0, q1, [x10, #-16]
   26790:	add	x10, x10, #0x20
   26794:	b.ne	26780 <__gmpn_sub_1@@Base+0xb8>  // b.any
   26798:	cmp	x8, x11
   2679c:	b.eq	26868 <__gmpn_sub_1@@Base+0x1a0>  // b.none
   267a0:	lsl	x10, x9, #3
   267a4:	sub	x8, x2, x9
   267a8:	add	x9, x0, x10
   267ac:	add	x10, x1, x10
   267b0:	ldr	x11, [x10], #8
   267b4:	subs	x8, x8, #0x1
   267b8:	str	x11, [x9], #8
   267bc:	b.ne	267b0 <__gmpn_sub_1@@Base+0xe8>  // b.any
   267c0:	mov	x0, x8
   267c4:	ret
   267c8:	cmp	x1, x0
   267cc:	mov	x8, xzr
   267d0:	b.eq	267c0 <__gmpn_sub_1@@Base+0xf8>  // b.none
   267d4:	cmp	x2, #0x2
   267d8:	b.lt	267c0 <__gmpn_sub_1@@Base+0xf8>  // b.tstop
   267dc:	sub	x8, x2, #0x1
   267e0:	cmp	x8, #0x4
   267e4:	b.cc	2680c <__gmpn_sub_1@@Base+0x144>  // b.lo, b.ul, b.last
   267e8:	lsl	x9, x2, #3
   267ec:	add	x10, x0, #0x8
   267f0:	add	x11, x1, x9
   267f4:	cmp	x10, x11
   267f8:	b.cs	26834 <__gmpn_sub_1@@Base+0x16c>  // b.hs, b.nlast
   267fc:	add	x9, x0, x9
   26800:	add	x10, x1, #0x8
   26804:	cmp	x10, x9
   26808:	b.cs	26834 <__gmpn_sub_1@@Base+0x16c>  // b.hs, b.nlast
   2680c:	mov	w9, #0x1                   	// #1
   26810:	lsl	x10, x9, #3
   26814:	sub	x8, x2, x9
   26818:	add	x9, x0, x10
   2681c:	add	x10, x1, x10
   26820:	ldr	x11, [x10], #8
   26824:	subs	x8, x8, #0x1
   26828:	str	x11, [x9], #8
   2682c:	b.ne	26820 <__gmpn_sub_1@@Base+0x158>  // b.any
   26830:	b	267c0 <__gmpn_sub_1@@Base+0xf8>
   26834:	and	x10, x8, #0xfffffffffffffffc
   26838:	add	x11, x1, #0x18
   2683c:	orr	x9, x10, #0x1
   26840:	add	x12, x0, #0x18
   26844:	mov	x13, x10
   26848:	ldp	q0, q1, [x11, #-16]
   2684c:	add	x11, x11, #0x20
   26850:	subs	x13, x13, #0x4
   26854:	stp	q0, q1, [x12, #-16]
   26858:	add	x12, x12, #0x20
   2685c:	b.ne	26848 <__gmpn_sub_1@@Base+0x180>  // b.any
   26860:	cmp	x8, x10
   26864:	b.ne	26810 <__gmpn_sub_1@@Base+0x148>  // b.any
   26868:	mov	x8, xzr
   2686c:	mov	x0, x8
   26870:	ret
   26874:	nop
   26878:	nop
   2687c:	nop

0000000000026880 <__gmpn_sub_nc@@Base>:
   26880:	cmp	xzr, x4
   26884:	b	2688c <__gmpn_sub_n@@Base+0x4>

0000000000026888 <__gmpn_sub_n@@Base>:
   26888:	cmp	xzr, xzr
   2688c:	lsr	x18, x3, #2
   26890:	tbz	w3, #0, 268d8 <__gmpn_sub_n@@Base+0x50>
   26894:	ldr	x7, [x1]
   26898:	ldr	x11, [x2]
   2689c:	sbcs	x13, x7, x11
   268a0:	str	x13, [x0], #8
   268a4:	tbnz	w3, #1, 268c0 <__gmpn_sub_n@@Base+0x38>
   268a8:	cbz	x18, 2693c <__gmpn_sub_n@@Base+0xb4>
   268ac:	ldp	x4, x5, [x1, #8]
   268b0:	ldp	x8, x9, [x2, #8]
   268b4:	sub	x1, x1, #0x8
   268b8:	sub	x2, x2, #0x8
   268bc:	b	26914 <__gmpn_sub_n@@Base+0x8c>
   268c0:	ldp	x6, x7, [x1, #8]
   268c4:	ldp	x10, x11, [x2, #8]
   268c8:	add	x1, x1, #0x8
   268cc:	add	x2, x2, #0x8
   268d0:	cbz	x18, 26930 <__gmpn_sub_n@@Base+0xa8>
   268d4:	b	26900 <__gmpn_sub_n@@Base+0x78>
   268d8:	tbnz	w3, #1, 268f0 <__gmpn_sub_n@@Base+0x68>
   268dc:	ldp	x4, x5, [x1]
   268e0:	ldp	x8, x9, [x2]
   268e4:	sub	x1, x1, #0x10
   268e8:	sub	x2, x2, #0x10
   268ec:	b	26914 <__gmpn_sub_n@@Base+0x8c>
   268f0:	ldp	x6, x7, [x1]
   268f4:	ldp	x10, x11, [x2]
   268f8:	cbz	x18, 26930 <__gmpn_sub_n@@Base+0xa8>
   268fc:	nop
   26900:	ldp	x4, x5, [x1, #16]
   26904:	ldp	x8, x9, [x2, #16]
   26908:	sbcs	x12, x6, x10
   2690c:	sbcs	x13, x7, x11
   26910:	stp	x12, x13, [x0], #16
   26914:	ldp	x6, x7, [x1, #32]!
   26918:	ldp	x10, x11, [x2, #32]!
   2691c:	sbcs	x12, x4, x8
   26920:	sbcs	x13, x5, x9
   26924:	stp	x12, x13, [x0], #16
   26928:	sub	x18, x18, #0x1
   2692c:	cbnz	x18, 26900 <__gmpn_sub_n@@Base+0x78>
   26930:	sbcs	x12, x6, x10
   26934:	sbcs	x13, x7, x11
   26938:	stp	x12, x13, [x0]
   2693c:	cset	x0, cc  // cc = lo, ul, last
   26940:	ret
   26944:	nop
   26948:	nop
   2694c:	nop

0000000000026950 <__gmpn_cnd_add_n@@Base>:
   26950:	cmp	x0, #0x1
   26954:	sbc	x0, x0, x0
   26958:	cmn	xzr, xzr
   2695c:	lsr	x18, x4, #2
   26960:	tbz	w4, #0, 269ac <__gmpn_cnd_add_n@@Base+0x5c>
   26964:	ldr	x13, [x3]
   26968:	ldr	x11, [x2]
   2696c:	bic	x7, x13, x0
   26970:	adcs	x9, x11, x7
   26974:	str	x9, [x1]
   26978:	tbnz	w4, #1, 26998 <__gmpn_cnd_add_n@@Base+0x48>
   2697c:	cbz	x18, 26a24 <__gmpn_cnd_add_n@@Base+0xd4>
   26980:	ldp	x12, x13, [x3, #8]
   26984:	ldp	x10, x11, [x2, #8]
   26988:	sub	x2, x2, #0x8
   2698c:	sub	x3, x3, #0x8
   26990:	sub	x1, x1, #0x18
   26994:	b	269ec <__gmpn_cnd_add_n@@Base+0x9c>
   26998:	ldp	x12, x13, [x3, #8]!
   2699c:	ldp	x10, x11, [x2, #8]!
   269a0:	sub	x1, x1, #0x8
   269a4:	cbz	x18, 26a10 <__gmpn_cnd_add_n@@Base+0xc0>
   269a8:	b	269d0 <__gmpn_cnd_add_n@@Base+0x80>
   269ac:	ldp	x12, x13, [x3]
   269b0:	ldp	x10, x11, [x2]
   269b4:	tbnz	w4, #1, 269c8 <__gmpn_cnd_add_n@@Base+0x78>
   269b8:	sub	x2, x2, #0x10
   269bc:	sub	x3, x3, #0x10
   269c0:	sub	x1, x1, #0x20
   269c4:	b	269ec <__gmpn_cnd_add_n@@Base+0x9c>
   269c8:	sub	x1, x1, #0x10
   269cc:	cbz	x18, 26a10 <__gmpn_cnd_add_n@@Base+0xc0>
   269d0:	bic	x6, x12, x0
   269d4:	bic	x7, x13, x0
   269d8:	ldp	x12, x13, [x3, #16]
   269dc:	adcs	x8, x10, x6
   269e0:	adcs	x9, x11, x7
   269e4:	ldp	x10, x11, [x2, #16]
   269e8:	stp	x8, x9, [x1, #16]
   269ec:	bic	x6, x12, x0
   269f0:	bic	x7, x13, x0
   269f4:	ldp	x12, x13, [x3, #32]!
   269f8:	adcs	x8, x10, x6
   269fc:	adcs	x9, x11, x7
   26a00:	ldp	x10, x11, [x2, #32]!
   26a04:	stp	x8, x9, [x1, #32]!
   26a08:	sub	x18, x18, #0x1
   26a0c:	cbnz	x18, 269d0 <__gmpn_cnd_add_n@@Base+0x80>
   26a10:	bic	x6, x12, x0
   26a14:	bic	x7, x13, x0
   26a18:	adcs	x8, x10, x6
   26a1c:	adcs	x9, x11, x7
   26a20:	stp	x8, x9, [x1, #16]
   26a24:	cset	x0, cs  // cs = hs, nlast
   26a28:	ret
   26a2c:	nop

0000000000026a30 <__gmpn_cnd_sub_n@@Base>:
   26a30:	cmp	x0, #0x1
   26a34:	sbc	x0, x0, x0
   26a38:	cmp	xzr, xzr
   26a3c:	lsr	x18, x4, #2
   26a40:	tbz	w4, #0, 26a8c <__gmpn_cnd_sub_n@@Base+0x5c>
   26a44:	ldr	x13, [x3]
   26a48:	ldr	x11, [x2]
   26a4c:	bic	x7, x13, x0
   26a50:	sbcs	x9, x11, x7
   26a54:	str	x9, [x1]
   26a58:	tbnz	w4, #1, 26a78 <__gmpn_cnd_sub_n@@Base+0x48>
   26a5c:	cbz	x18, 26b04 <__gmpn_cnd_sub_n@@Base+0xd4>
   26a60:	ldp	x12, x13, [x3, #8]
   26a64:	ldp	x10, x11, [x2, #8]
   26a68:	sub	x2, x2, #0x8
   26a6c:	sub	x3, x3, #0x8
   26a70:	sub	x1, x1, #0x18
   26a74:	b	26acc <__gmpn_cnd_sub_n@@Base+0x9c>
   26a78:	ldp	x12, x13, [x3, #8]!
   26a7c:	ldp	x10, x11, [x2, #8]!
   26a80:	sub	x1, x1, #0x8
   26a84:	cbz	x18, 26af0 <__gmpn_cnd_sub_n@@Base+0xc0>
   26a88:	b	26ab0 <__gmpn_cnd_sub_n@@Base+0x80>
   26a8c:	ldp	x12, x13, [x3]
   26a90:	ldp	x10, x11, [x2]
   26a94:	tbnz	w4, #1, 26aa8 <__gmpn_cnd_sub_n@@Base+0x78>
   26a98:	sub	x2, x2, #0x10
   26a9c:	sub	x3, x3, #0x10
   26aa0:	sub	x1, x1, #0x20
   26aa4:	b	26acc <__gmpn_cnd_sub_n@@Base+0x9c>
   26aa8:	sub	x1, x1, #0x10
   26aac:	cbz	x18, 26af0 <__gmpn_cnd_sub_n@@Base+0xc0>
   26ab0:	bic	x6, x12, x0
   26ab4:	bic	x7, x13, x0
   26ab8:	ldp	x12, x13, [x3, #16]
   26abc:	sbcs	x8, x10, x6
   26ac0:	sbcs	x9, x11, x7
   26ac4:	ldp	x10, x11, [x2, #16]
   26ac8:	stp	x8, x9, [x1, #16]
   26acc:	bic	x6, x12, x0
   26ad0:	bic	x7, x13, x0
   26ad4:	ldp	x12, x13, [x3, #32]!
   26ad8:	sbcs	x8, x10, x6
   26adc:	sbcs	x9, x11, x7
   26ae0:	ldp	x10, x11, [x2, #32]!
   26ae4:	stp	x8, x9, [x1, #32]!
   26ae8:	sub	x18, x18, #0x1
   26aec:	cbnz	x18, 26ab0 <__gmpn_cnd_sub_n@@Base+0x80>
   26af0:	bic	x6, x12, x0
   26af4:	bic	x7, x13, x0
   26af8:	sbcs	x8, x10, x6
   26afc:	sbcs	x9, x11, x7
   26b00:	stp	x8, x9, [x1, #16]
   26b04:	cset	x0, cc  // cc = lo, ul, last
   26b08:	ret

0000000000026b0c <__gmpn_cnd_swap@@Base>:
   26b0c:	sub	sp, sp, #0x10
   26b10:	cmp	x0, #0x0
   26b14:	csetm	x8, ne  // ne = any
   26b18:	cmp	x3, #0x1
   26b1c:	str	x8, [sp, #8]
   26b20:	b.lt	26b50 <__gmpn_cnd_swap@@Base+0x44>  // b.tstop
   26b24:	ldr	x8, [x1]
   26b28:	ldr	x9, [x2]
   26b2c:	ldr	x10, [sp, #8]
   26b30:	subs	x3, x3, #0x1
   26b34:	eor	x11, x9, x8
   26b38:	and	x10, x11, x10
   26b3c:	eor	x8, x10, x8
   26b40:	eor	x9, x10, x9
   26b44:	str	x8, [x1], #8
   26b48:	str	x9, [x2], #8
   26b4c:	b.ne	26b24 <__gmpn_cnd_swap@@Base+0x18>  // b.any
   26b50:	add	sp, sp, #0x10
   26b54:	ret

0000000000026b58 <__gmpn_neg@@Base>:
   26b58:	stp	x29, x30, [sp, #-16]!
   26b5c:	ldr	x8, [x1]
   26b60:	mov	x29, sp
   26b64:	cbz	x8, 26b9c <__gmpn_neg@@Base+0x44>
   26b68:	neg	x8, x8
   26b6c:	subs	x2, x2, #0x1
   26b70:	str	x8, [x0]
   26b74:	b.eq	26b84 <__gmpn_neg@@Base+0x2c>  // b.none
   26b78:	add	x0, x0, #0x8
   26b7c:	add	x1, x1, #0x8
   26b80:	bl	c290 <__gmpn_com@plt>
   26b84:	mov	w0, #0x1                   	// #1
   26b88:	ldp	x29, x30, [sp], #16
   26b8c:	ret
   26b90:	ldr	x8, [x1, #8]!
   26b94:	add	x0, x0, #0x8
   26b98:	cbnz	x8, 26b68 <__gmpn_neg@@Base+0x10>
   26b9c:	subs	x2, x2, #0x1
   26ba0:	str	xzr, [x0]
   26ba4:	b.ne	26b90 <__gmpn_neg@@Base+0x38>  // b.any
   26ba8:	mov	x0, xzr
   26bac:	ldp	x29, x30, [sp], #16
   26bb0:	ret
   26bb4:	nop
   26bb8:	nop
   26bbc:	nop

0000000000026bc0 <__gmpn_com@@Base>:
   26bc0:	cmp	x2, #0x3
   26bc4:	b.le	26c18 <__gmpn_com@@Base+0x58>
   26bc8:	tbz	w0, #3, 26bdc <__gmpn_com@@Base+0x1c>
   26bcc:	ld1	{v22.1d}, [x1], #8
   26bd0:	sub	x2, x2, #0x1
   26bd4:	mvn	v22.8b, v22.8b
   26bd8:	st1	{v22.1d}, [x0], #8
   26bdc:	ld1	{v26.2d}, [x1], #16
   26be0:	subs	x2, x2, #0x6
   26be4:	b.lt	26c10 <__gmpn_com@@Base+0x50>  // b.tstop
   26be8:	nop
   26bec:	nop
   26bf0:	ld1	{v22.2d}, [x1], #16
   26bf4:	mvn	v26.16b, v26.16b
   26bf8:	st1	{v26.2d}, [x0], #16
   26bfc:	ld1	{v26.2d}, [x1], #16
   26c00:	mvn	v22.16b, v22.16b
   26c04:	st1	{v22.2d}, [x0], #16
   26c08:	subs	x2, x2, #0x4
   26c0c:	b.ge	26bf0 <__gmpn_com@@Base+0x30>  // b.tcont
   26c10:	mvn	v26.16b, v26.16b
   26c14:	st1	{v26.2d}, [x0], #16
   26c18:	tbz	w2, #1, 26c28 <__gmpn_com@@Base+0x68>
   26c1c:	ld1	{v22.2d}, [x1], #16
   26c20:	mvn	v22.16b, v22.16b
   26c24:	st1	{v22.2d}, [x0], #16
   26c28:	tbz	w2, #0, 26c38 <__gmpn_com@@Base+0x78>
   26c2c:	ld1	{v22.1d}, [x1]
   26c30:	mvn	v22.8b, v22.8b
   26c34:	st1	{v22.1d}, [x0]
   26c38:	ret
   26c3c:	nop

0000000000026c40 <__gmpn_mul_1c@@Base>:
   26c40:	cmn	xzr, xzr
   26c44:	b	26c4c <__gmpn_mul_1@@Base+0x4>

0000000000026c48 <__gmpn_mul_1@@Base>:
   26c48:	adds	x4, xzr, xzr
   26c4c:	lsr	x18, x2, #2
   26c50:	tbnz	w2, #0, 26c80 <__gmpn_mul_1@@Base+0x38>
   26c54:	mov	x11, x4
   26c58:	tbz	w2, #1, 26c9c <__gmpn_mul_1@@Base+0x54>
   26c5c:	ldp	x4, x5, [x1]
   26c60:	mul	x8, x4, x3
   26c64:	umulh	x10, x4, x3
   26c68:	cbz	x18, 26c78 <__gmpn_mul_1@@Base+0x30>
   26c6c:	ldp	x6, x7, [x1, #16]!
   26c70:	mul	x9, x5, x3
   26c74:	b	26ce8 <__gmpn_mul_1@@Base+0xa0>
   26c78:	mul	x9, x5, x3
   26c7c:	b	26d2c <__gmpn_mul_1@@Base+0xe4>
   26c80:	ldr	x7, [x1], #8
   26c84:	mul	x9, x7, x3
   26c88:	umulh	x11, x7, x3
   26c8c:	adds	x9, x9, x4
   26c90:	str	x9, [x0], #8
   26c94:	tbnz	w2, #1, 26c5c <__gmpn_mul_1@@Base+0x14>
   26c98:	cbz	x18, 26d3c <__gmpn_mul_1@@Base+0xf4>
   26c9c:	ldp	x6, x7, [x1]
   26ca0:	mul	x8, x6, x3
   26ca4:	umulh	x10, x6, x3
   26ca8:	ldp	x4, x5, [x1, #16]
   26cac:	mul	x9, x7, x3
   26cb0:	adcs	x12, x8, x11
   26cb4:	umulh	x11, x7, x3
   26cb8:	add	x0, x0, #0x10
   26cbc:	sub	x18, x18, #0x1
   26cc0:	cbz	x18, 26d18 <__gmpn_mul_1@@Base+0xd0>
   26cc4:	nop
   26cc8:	nop
   26ccc:	nop
   26cd0:	mul	x8, x4, x3
   26cd4:	ldp	x6, x7, [x1, #32]!
   26cd8:	adcs	x13, x9, x10
   26cdc:	umulh	x10, x4, x3
   26ce0:	mul	x9, x5, x3
   26ce4:	stp	x12, x13, [x0, #-16]
   26ce8:	adcs	x12, x8, x11
   26cec:	umulh	x11, x5, x3
   26cf0:	mul	x8, x6, x3
   26cf4:	ldp	x4, x5, [x1, #16]
   26cf8:	adcs	x13, x9, x10
   26cfc:	umulh	x10, x6, x3
   26d00:	mul	x9, x7, x3
   26d04:	stp	x12, x13, [x0], #32
   26d08:	adcs	x12, x8, x11
   26d0c:	umulh	x11, x7, x3
   26d10:	sub	x18, x18, #0x1
   26d14:	cbnz	x18, 26cd0 <__gmpn_mul_1@@Base+0x88>
   26d18:	mul	x8, x4, x3
   26d1c:	adcs	x13, x9, x10
   26d20:	umulh	x10, x4, x3
   26d24:	mul	x9, x5, x3
   26d28:	stp	x12, x13, [x0, #-16]
   26d2c:	adcs	x12, x8, x11
   26d30:	umulh	x11, x5, x3
   26d34:	adcs	x13, x9, x10
   26d38:	stp	x12, x13, [x0]
   26d3c:	adc	x0, x11, xzr
   26d40:	ret
   26d44:	nop
   26d48:	nop
   26d4c:	nop

0000000000026d50 <__gmpn_addmul_1@@Base>:
   26d50:	adds	x15, xzr, xzr
   26d54:	tbz	w2, #0, 26d74 <__gmpn_addmul_1@@Base+0x24>
   26d58:	ldr	x4, [x1], #8
   26d5c:	mul	x8, x4, x3
   26d60:	umulh	x12, x4, x3
   26d64:	ldr	x4, [x0]
   26d68:	adds	x8, x4, x8
   26d6c:	cinc	x15, x12, cs  // cs = hs, nlast
   26d70:	str	x8, [x0], #8
   26d74:	tbz	w2, #1, 26dac <__gmpn_addmul_1@@Base+0x5c>
   26d78:	ldp	x4, x5, [x1], #16
   26d7c:	mul	x8, x4, x3
   26d80:	umulh	x12, x4, x3
   26d84:	mul	x9, x5, x3
   26d88:	umulh	x13, x5, x3
   26d8c:	adds	x8, x8, x15
   26d90:	adcs	x9, x9, x12
   26d94:	ldp	x4, x5, [x0]
   26d98:	adc	x15, x13, xzr
   26d9c:	adds	x8, x4, x8
   26da0:	adcs	x9, x5, x9
   26da4:	cinc	x15, x15, cs  // cs = hs, nlast
   26da8:	stp	x8, x9, [x0], #16
   26dac:	lsr	x2, x2, #2
   26db0:	cbz	x2, 26dc0 <__gmpn_addmul_1@@Base+0x70>
   26db4:	ldp	x4, x5, [x1], #32
   26db8:	ldp	x6, x7, [x1, #-16]
   26dbc:	b	26df4 <__gmpn_addmul_1@@Base+0xa4>
   26dc0:	mov	x0, x15
   26dc4:	ret
   26dc8:	nop
   26dcc:	nop
   26dd0:	ldp	x4, x5, [x1], #32
   26dd4:	ldp	x6, x7, [x1, #-16]
   26dd8:	adds	x8, x16, x8
   26ddc:	adcs	x9, x17, x9
   26de0:	stp	x8, x9, [x0], #32
   26de4:	adcs	x10, x12, x10
   26de8:	adcs	x11, x13, x11
   26dec:	stp	x10, x11, [x0, #-16]
   26df0:	cinc	x15, x15, cs  // cs = hs, nlast
   26df4:	sub	x2, x2, #0x1
   26df8:	mul	x8, x4, x3
   26dfc:	umulh	x12, x4, x3
   26e00:	mul	x9, x5, x3
   26e04:	umulh	x13, x5, x3
   26e08:	adds	x8, x8, x15
   26e0c:	mul	x10, x6, x3
   26e10:	umulh	x14, x6, x3
   26e14:	adcs	x9, x9, x12
   26e18:	mul	x11, x7, x3
   26e1c:	umulh	x15, x7, x3
   26e20:	adcs	x10, x10, x13
   26e24:	ldp	x16, x17, [x0]
   26e28:	adcs	x11, x11, x14
   26e2c:	ldp	x12, x13, [x0, #16]
   26e30:	adc	x15, x15, xzr
   26e34:	cbnz	x2, 26dd0 <__gmpn_addmul_1@@Base+0x80>
   26e38:	adds	x8, x16, x8
   26e3c:	adcs	x9, x17, x9
   26e40:	adcs	x10, x12, x10
   26e44:	adcs	x11, x13, x11
   26e48:	stp	x8, x9, [x0]
   26e4c:	stp	x10, x11, [x0, #16]
   26e50:	cinc	x0, x15, cs  // cs = hs, nlast
   26e54:	ret
   26e58:	nop
   26e5c:	nop

0000000000026e60 <__gmpn_submul_1@@Base>:
   26e60:	adds	x15, xzr, xzr
   26e64:	tbz	w2, #0, 26e84 <__gmpn_submul_1@@Base+0x24>
   26e68:	ldr	x4, [x1], #8
   26e6c:	mul	x8, x4, x3
   26e70:	umulh	x12, x4, x3
   26e74:	ldr	x4, [x0]
   26e78:	subs	x8, x4, x8
   26e7c:	cinc	x15, x12, cc  // cc = lo, ul, last
   26e80:	str	x8, [x0], #8
   26e84:	tbz	w2, #1, 26ebc <__gmpn_submul_1@@Base+0x5c>
   26e88:	ldp	x4, x5, [x1], #16
   26e8c:	mul	x8, x4, x3
   26e90:	umulh	x12, x4, x3
   26e94:	mul	x9, x5, x3
   26e98:	umulh	x13, x5, x3
   26e9c:	adds	x8, x8, x15
   26ea0:	adcs	x9, x9, x12
   26ea4:	ldp	x4, x5, [x0]
   26ea8:	adc	x15, x13, xzr
   26eac:	subs	x8, x4, x8
   26eb0:	sbcs	x9, x5, x9
   26eb4:	cinc	x15, x15, cc  // cc = lo, ul, last
   26eb8:	stp	x8, x9, [x0], #16
   26ebc:	lsr	x2, x2, #2
   26ec0:	cbz	x2, 26ed0 <__gmpn_submul_1@@Base+0x70>
   26ec4:	ldp	x4, x5, [x1], #32
   26ec8:	ldp	x6, x7, [x1, #-16]
   26ecc:	b	26f04 <__gmpn_submul_1@@Base+0xa4>
   26ed0:	mov	x0, x15
   26ed4:	ret
   26ed8:	nop
   26edc:	nop
   26ee0:	ldp	x4, x5, [x1], #32
   26ee4:	ldp	x6, x7, [x1, #-16]
   26ee8:	subs	x8, x16, x8
   26eec:	sbcs	x9, x17, x9
   26ef0:	stp	x8, x9, [x0], #32
   26ef4:	sbcs	x10, x12, x10
   26ef8:	sbcs	x11, x13, x11
   26efc:	stp	x10, x11, [x0, #-16]
   26f00:	cinc	x15, x15, cc  // cc = lo, ul, last
   26f04:	sub	x2, x2, #0x1
   26f08:	mul	x8, x4, x3
   26f0c:	umulh	x12, x4, x3
   26f10:	mul	x9, x5, x3
   26f14:	umulh	x13, x5, x3
   26f18:	adds	x8, x8, x15
   26f1c:	mul	x10, x6, x3
   26f20:	umulh	x14, x6, x3
   26f24:	adcs	x9, x9, x12
   26f28:	mul	x11, x7, x3
   26f2c:	umulh	x15, x7, x3
   26f30:	adcs	x10, x10, x13
   26f34:	ldp	x16, x17, [x0]
   26f38:	adcs	x11, x11, x14
   26f3c:	ldp	x12, x13, [x0, #16]
   26f40:	adc	x15, x15, xzr
   26f44:	cbnz	x2, 26ee0 <__gmpn_submul_1@@Base+0x80>
   26f48:	subs	x8, x16, x8
   26f4c:	sbcs	x9, x17, x9
   26f50:	sbcs	x10, x12, x10
   26f54:	sbcs	x11, x13, x11
   26f58:	stp	x8, x9, [x0]
   26f5c:	stp	x10, x11, [x0, #16]
   26f60:	cinc	x0, x15, cc  // cc = lo, ul, last
   26f64:	ret

0000000000026f68 <__gmpn_add_err1_n@@Base>:
   26f68:	mov	x8, xzr
   26f6c:	mov	x9, xzr
   26f70:	sub	x10, x4, #0x8
   26f74:	ldr	x11, [x10, x5, lsl #3]
   26f78:	ldr	x12, [x1], #8
   26f7c:	ldr	x13, [x2], #8
   26f80:	adds	x12, x13, x12
   26f84:	cset	w13, cs  // cs = hs, nlast
   26f88:	adds	x12, x12, x6
   26f8c:	cset	w14, cs  // cs = hs, nlast
   26f90:	orr	w6, w13, w14
   26f94:	cmp	w6, #0x0
   26f98:	csel	x11, x11, xzr, ne  // ne = any
   26f9c:	adds	x9, x11, x9
   26fa0:	cinc	x8, x8, cs  // cs = hs, nlast
   26fa4:	subs	x5, x5, #0x1
   26fa8:	str	x12, [x0], #8
   26fac:	b.ne	26f74 <__gmpn_add_err1_n@@Base+0xc>  // b.any
   26fb0:	mov	x0, x6
   26fb4:	stp	x9, x8, [x3]
   26fb8:	ret

0000000000026fbc <__gmpn_add_err2_n@@Base>:
   26fbc:	mov	x8, xzr
   26fc0:	mov	x9, xzr
   26fc4:	mov	x10, xzr
   26fc8:	mov	x11, xzr
   26fcc:	sub	x12, x5, #0x8
   26fd0:	sub	x13, x4, #0x8
   26fd4:	lsl	x14, x6, #3
   26fd8:	ldr	x15, [x13, x14]
   26fdc:	ldr	x14, [x12, x14]
   26fe0:	ldr	x16, [x1], #8
   26fe4:	ldr	x17, [x2], #8
   26fe8:	adds	x16, x17, x16
   26fec:	cset	w17, cs  // cs = hs, nlast
   26ff0:	adds	x16, x16, x7
   26ff4:	cset	w18, cs  // cs = hs, nlast
   26ff8:	orr	w7, w17, w18
   26ffc:	sbfx	x17, x7, #0, #1
   27000:	and	x15, x15, x17
   27004:	and	x14, x14, x17
   27008:	adds	x11, x15, x11
   2700c:	cinc	x10, x10, cs  // cs = hs, nlast
   27010:	adds	x9, x14, x9
   27014:	cinc	x8, x8, cs  // cs = hs, nlast
   27018:	subs	x6, x6, #0x1
   2701c:	str	x16, [x0], #8
   27020:	b.ne	26fd4 <__gmpn_add_err2_n@@Base+0x18>  // b.any
   27024:	mov	x0, x7
   27028:	stp	x11, x10, [x3]
   2702c:	stp	x9, x8, [x3, #16]
   27030:	ret

0000000000027034 <__gmpn_add_err3_n@@Base>:
   27034:	str	x21, [sp, #-32]!
   27038:	mov	x8, x0
   2703c:	ldr	x0, [sp, #32]
   27040:	lsl	x16, x7, #3
   27044:	sub	x18, x16, #0x8
   27048:	mov	x14, xzr
   2704c:	mov	x9, xzr
   27050:	mov	x10, xzr
   27054:	mov	x11, xzr
   27058:	mov	x12, xzr
   2705c:	mov	x13, xzr
   27060:	mov	x15, xzr
   27064:	add	x16, x4, x18
   27068:	add	x17, x5, x18
   2706c:	add	x18, x6, x18
   27070:	stp	x20, x19, [sp, #16]
   27074:	lsl	x5, x14, #3
   27078:	ldr	x4, [x16], #-8
   2707c:	ldr	x6, [x17], #-8
   27080:	ldr	x19, [x18], #-8
   27084:	ldr	x20, [x1, x5]
   27088:	ldr	x21, [x2, x5]
   2708c:	add	x14, x14, #0x1
   27090:	adds	x20, x21, x20
   27094:	cset	w21, cs  // cs = hs, nlast
   27098:	adds	x20, x20, x0
   2709c:	cset	w0, cs  // cs = hs, nlast
   270a0:	orr	w0, w21, w0
   270a4:	sbfx	x21, x0, #0, #1
   270a8:	and	x4, x4, x21
   270ac:	and	x6, x6, x21
   270b0:	adds	x15, x4, x15
   270b4:	and	x19, x19, x21
   270b8:	cinc	x13, x13, cs  // cs = hs, nlast
   270bc:	adds	x12, x6, x12
   270c0:	cinc	x11, x11, cs  // cs = hs, nlast
   270c4:	adds	x10, x19, x10
   270c8:	cinc	x9, x9, cs  // cs = hs, nlast
   270cc:	cmp	x7, x14
   270d0:	str	x20, [x8, x5]
   270d4:	b.ne	27074 <__gmpn_add_err3_n@@Base+0x40>  // b.any
   270d8:	stp	x15, x13, [x3]
   270dc:	stp	x12, x11, [x3, #16]
   270e0:	stp	x10, x9, [x3, #32]
   270e4:	ldp	x20, x19, [sp, #16]
   270e8:	ldr	x21, [sp], #32
   270ec:	ret

00000000000270f0 <__gmpn_sub_err1_n@@Base>:
   270f0:	mov	x8, xzr
   270f4:	mov	x9, xzr
   270f8:	sub	x10, x4, #0x8
   270fc:	ldr	x11, [x10, x5, lsl #3]
   27100:	ldr	x12, [x1], #8
   27104:	ldr	x13, [x2], #8
   27108:	subs	x12, x12, x13
   2710c:	cset	w13, cc  // cc = lo, ul, last
   27110:	subs	x12, x12, x6
   27114:	cset	w14, cc  // cc = lo, ul, last
   27118:	orr	w6, w13, w14
   2711c:	cmp	w6, #0x0
   27120:	csel	x11, x11, xzr, ne  // ne = any
   27124:	adds	x9, x11, x9
   27128:	cinc	x8, x8, cs  // cs = hs, nlast
   2712c:	subs	x5, x5, #0x1
   27130:	str	x12, [x0], #8
   27134:	b.ne	270fc <__gmpn_sub_err1_n@@Base+0xc>  // b.any
   27138:	mov	x0, x6
   2713c:	stp	x9, x8, [x3]
   27140:	ret

0000000000027144 <__gmpn_sub_err2_n@@Base>:
   27144:	mov	x8, xzr
   27148:	mov	x9, xzr
   2714c:	mov	x10, xzr
   27150:	mov	x11, xzr
   27154:	sub	x12, x5, #0x8
   27158:	sub	x13, x4, #0x8
   2715c:	lsl	x14, x6, #3
   27160:	ldr	x15, [x13, x14]
   27164:	ldr	x14, [x12, x14]
   27168:	ldr	x16, [x1], #8
   2716c:	ldr	x17, [x2], #8
   27170:	subs	x16, x16, x17
   27174:	cset	w17, cc  // cc = lo, ul, last
   27178:	subs	x16, x16, x7
   2717c:	cset	w18, cc  // cc = lo, ul, last
   27180:	orr	w7, w17, w18
   27184:	sbfx	x17, x7, #0, #1
   27188:	and	x15, x15, x17
   2718c:	and	x14, x14, x17
   27190:	adds	x11, x15, x11
   27194:	cinc	x10, x10, cs  // cs = hs, nlast
   27198:	adds	x9, x14, x9
   2719c:	cinc	x8, x8, cs  // cs = hs, nlast
   271a0:	subs	x6, x6, #0x1
   271a4:	str	x16, [x0], #8
   271a8:	b.ne	2715c <__gmpn_sub_err2_n@@Base+0x18>  // b.any
   271ac:	mov	x0, x7
   271b0:	stp	x11, x10, [x3]
   271b4:	stp	x9, x8, [x3, #16]
   271b8:	ret

00000000000271bc <__gmpn_sub_err3_n@@Base>:
   271bc:	str	x21, [sp, #-32]!
   271c0:	mov	x8, x0
   271c4:	ldr	x0, [sp, #32]
   271c8:	lsl	x16, x7, #3
   271cc:	sub	x18, x16, #0x8
   271d0:	mov	x14, xzr
   271d4:	mov	x9, xzr
   271d8:	mov	x10, xzr
   271dc:	mov	x11, xzr
   271e0:	mov	x12, xzr
   271e4:	mov	x13, xzr
   271e8:	mov	x15, xzr
   271ec:	add	x16, x4, x18
   271f0:	add	x17, x5, x18
   271f4:	add	x18, x6, x18
   271f8:	stp	x20, x19, [sp, #16]
   271fc:	lsl	x5, x14, #3
   27200:	ldr	x4, [x16], #-8
   27204:	ldr	x6, [x17], #-8
   27208:	ldr	x19, [x18], #-8
   2720c:	ldr	x20, [x1, x5]
   27210:	ldr	x21, [x2, x5]
   27214:	add	x14, x14, #0x1
   27218:	subs	x20, x20, x21
   2721c:	cset	w21, cc  // cc = lo, ul, last
   27220:	subs	x20, x20, x0
   27224:	cset	w0, cc  // cc = lo, ul, last
   27228:	orr	w0, w21, w0
   2722c:	sbfx	x21, x0, #0, #1
   27230:	and	x4, x4, x21
   27234:	and	x6, x6, x21
   27238:	adds	x15, x4, x15
   2723c:	and	x19, x19, x21
   27240:	cinc	x13, x13, cs  // cs = hs, nlast
   27244:	adds	x12, x6, x12
   27248:	cinc	x11, x11, cs  // cs = hs, nlast
   2724c:	adds	x10, x19, x10
   27250:	cinc	x9, x9, cs  // cs = hs, nlast
   27254:	cmp	x7, x14
   27258:	str	x20, [x8, x5]
   2725c:	b.ne	271fc <__gmpn_sub_err3_n@@Base+0x40>  // b.any
   27260:	stp	x15, x13, [x3]
   27264:	stp	x12, x11, [x3, #16]
   27268:	stp	x10, x9, [x3, #32]
   2726c:	ldp	x20, x19, [sp, #16]
   27270:	ldr	x21, [sp], #32
   27274:	ret
   27278:	nop
   2727c:	nop

0000000000027280 <__gmpn_lshift@@Base>:
   27280:	add	x16, x0, x2, lsl #3
   27284:	add	x1, x1, x2, lsl #3
   27288:	neg	x8, x3
   2728c:	lsr	x18, x2, #2
   27290:	tbz	w2, #0, 272d0 <__gmpn_lshift@@Base+0x50>
   27294:	ldur	x4, [x1, #-8]
   27298:	tbnz	w2, #1, 272c0 <__gmpn_lshift@@Base+0x40>
   2729c:	lsr	x0, x4, x8
   272a0:	lsl	x2, x4, x3
   272a4:	cbnz	x18, 272b0 <__gmpn_lshift@@Base+0x30>
   272a8:	stur	x2, [x16, #-8]
   272ac:	ret
   272b0:	ldp	x4, x5, [x1, #-24]
   272b4:	sub	x1, x1, #0x8
   272b8:	add	x16, x16, #0x10
   272bc:	b	27344 <__gmpn_lshift@@Base+0xc4>
   272c0:	lsr	x0, x4, x8
   272c4:	lsl	x2, x4, x3
   272c8:	ldp	x6, x7, [x1, #-24]!
   272cc:	b	27368 <__gmpn_lshift@@Base+0xe8>
   272d0:	ldp	x4, x5, [x1, #-16]
   272d4:	tbz	w2, #1, 27310 <__gmpn_lshift@@Base+0x90>
   272d8:	lsr	x0, x5, x8
   272dc:	lsl	x13, x5, x3
   272e0:	lsr	x10, x4, x8
   272e4:	lsl	x2, x4, x3
   272e8:	cbnz	x18, 272f8 <__gmpn_lshift@@Base+0x78>
   272ec:	orr	x10, x10, x13
   272f0:	stp	x2, x10, [x16, #-16]
   272f4:	ret
   272f8:	ldp	x4, x5, [x1, #-32]
   272fc:	orr	x10, x10, x13
   27300:	stur	x10, [x16, #-8]
   27304:	sub	x1, x1, #0x10
   27308:	add	x16, x16, #0x8
   2730c:	b	27344 <__gmpn_lshift@@Base+0xc4>
   27310:	lsr	x0, x5, x8
   27314:	lsl	x13, x5, x3
   27318:	lsr	x10, x4, x8
   2731c:	lsl	x2, x4, x3
   27320:	ldp	x6, x7, [x1, #-32]!
   27324:	orr	x10, x10, x13
   27328:	str	x10, [x16, #-8]!
   2732c:	b	27364 <__gmpn_lshift@@Base+0xe4>
   27330:	ldp	x4, x5, [x1, #-16]
   27334:	orr	x10, x10, x13
   27338:	orr	x11, x12, x2
   2733c:	stp	x10, x11, [x16, #-16]
   27340:	lsl	x2, x6, x3
   27344:	lsr	x10, x4, x8
   27348:	lsl	x13, x5, x3
   2734c:	lsr	x12, x5, x8
   27350:	ldp	x6, x7, [x1, #-32]!
   27354:	orr	x10, x10, x13
   27358:	orr	x11, x12, x2
   2735c:	stp	x10, x11, [x16, #-32]!
   27360:	lsl	x2, x4, x3
   27364:	sub	x18, x18, #0x1
   27368:	lsr	x10, x6, x8
   2736c:	lsl	x13, x7, x3
   27370:	lsr	x12, x7, x8
   27374:	cbnz	x18, 27330 <__gmpn_lshift@@Base+0xb0>
   27378:	orr	x10, x10, x13
   2737c:	orr	x11, x12, x2
   27380:	lsl	x2, x6, x3
   27384:	stp	x10, x11, [x16, #-16]
   27388:	stur	x2, [x16, #-24]
   2738c:	ret

0000000000027390 <__gmpn_rshift@@Base>:
   27390:	mov	x16, x0
   27394:	neg	x8, x3
   27398:	lsr	x18, x2, #2
   2739c:	tbz	w2, #0, 273e0 <__gmpn_rshift@@Base+0x50>
   273a0:	ldr	x5, [x1]
   273a4:	tbnz	w2, #1, 273cc <__gmpn_rshift@@Base+0x3c>
   273a8:	lsl	x0, x5, x8
   273ac:	lsr	x2, x5, x3
   273b0:	cbnz	x18, 273bc <__gmpn_rshift@@Base+0x2c>
   273b4:	str	x2, [x16]
   273b8:	ret
   273bc:	ldp	x4, x5, [x1, #8]
   273c0:	sub	x1, x1, #0x8
   273c4:	sub	x16, x16, #0x20
   273c8:	b	27454 <__gmpn_rshift@@Base+0xc4>
   273cc:	lsl	x0, x5, x8
   273d0:	lsr	x2, x5, x3
   273d4:	ldp	x6, x7, [x1, #8]!
   273d8:	sub	x16, x16, #0x10
   273dc:	b	27478 <__gmpn_rshift@@Base+0xe8>
   273e0:	ldp	x4, x5, [x1]
   273e4:	tbz	w2, #1, 27418 <__gmpn_rshift@@Base+0x88>
   273e8:	lsl	x0, x4, x8
   273ec:	lsr	x13, x4, x3
   273f0:	lsl	x10, x5, x8
   273f4:	lsr	x2, x5, x3
   273f8:	cbnz	x18, 27408 <__gmpn_rshift@@Base+0x78>
   273fc:	orr	x10, x10, x13
   27400:	stp	x10, x2, [x16]
   27404:	ret
   27408:	ldp	x4, x5, [x1, #16]
   2740c:	orr	x10, x10, x13
   27410:	str	x10, [x16], #-24
   27414:	b	27454 <__gmpn_rshift@@Base+0xc4>
   27418:	lsl	x0, x4, x8
   2741c:	lsr	x13, x4, x3
   27420:	lsl	x10, x5, x8
   27424:	lsr	x2, x5, x3
   27428:	ldp	x6, x7, [x1, #16]!
   2742c:	orr	x10, x10, x13
   27430:	str	x10, [x16], #-8
   27434:	b	27474 <__gmpn_rshift@@Base+0xe4>
   27438:	nop
   2743c:	nop
   27440:	ldp	x4, x5, [x1, #16]
   27444:	orr	x10, x10, x13
   27448:	orr	x11, x12, x2
   2744c:	stp	x11, x10, [x16, #16]
   27450:	lsr	x2, x7, x3
   27454:	lsl	x10, x5, x8
   27458:	lsl	x12, x4, x8
   2745c:	lsr	x13, x4, x3
   27460:	ldp	x6, x7, [x1, #32]!
   27464:	orr	x10, x10, x13
   27468:	orr	x11, x12, x2
   2746c:	stp	x11, x10, [x16, #32]!
   27470:	lsr	x2, x5, x3
   27474:	sub	x18, x18, #0x1
   27478:	lsl	x10, x7, x8
   2747c:	lsl	x12, x6, x8
   27480:	lsr	x13, x6, x3
   27484:	cbnz	x18, 27440 <__gmpn_rshift@@Base+0xb0>
   27488:	orr	x10, x10, x13
   2748c:	orr	x11, x12, x2
   27490:	lsr	x2, x7, x3
   27494:	stp	x11, x10, [x16, #16]
   27498:	str	x2, [x16, #32]
   2749c:	ret

00000000000274a0 <__gmpn_divexact_1@@Base>:
   274a0:	rbit	x8, x3
   274a4:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   274a8:	tst	x3, #0x1
   274ac:	ldr	x9, [x9, #3952]
   274b0:	clz	x10, x8
   274b4:	csel	x8, x10, xzr, eq  // eq = none
   274b8:	lsr	x8, x3, x8
   274bc:	ubfx	x11, x8, #1, #7
   274c0:	ldrb	w9, [x9, x11]
   274c4:	mov	w12, #0x2                   	// #2
   274c8:	msub	x11, x8, x9, x12
   274cc:	mul	x9, x11, x9
   274d0:	msub	x13, x9, x8, x12
   274d4:	ldr	x11, [x1]
   274d8:	mul	x9, x9, x13
   274dc:	msub	x12, x9, x8, x12
   274e0:	mul	x9, x9, x12
   274e4:	cbz	x10, 27544 <__gmpn_divexact_1@@Base+0xa4>
   274e8:	tbnz	w3, #0, 27544 <__gmpn_divexact_1@@Base+0xa4>
   274ec:	cmp	x2, #0x2
   274f0:	b.lt	2758c <__gmpn_divexact_1@@Base+0xec>  // b.tstop
   274f4:	mov	w14, #0x40                  	// #64
   274f8:	mov	x12, xzr
   274fc:	sub	x13, x2, #0x1
   27500:	sub	x14, x14, x10
   27504:	add	x15, x1, #0x8
   27508:	mov	x16, x0
   2750c:	mov	x17, x11
   27510:	ldr	x11, [x15], #8
   27514:	lsr	x17, x17, x10
   27518:	lsl	x18, x11, x14
   2751c:	orr	x17, x18, x17
   27520:	subs	x12, x17, x12
   27524:	mul	x17, x12, x9
   27528:	umulh	x12, x17, x8
   2752c:	cinc	x12, x12, cc  // cc = lo, ul, last
   27530:	subs	x13, x13, #0x1
   27534:	str	x17, [x16], #8
   27538:	mov	x17, x11
   2753c:	b.ne	27510 <__gmpn_divexact_1@@Base+0x70>  // b.any
   27540:	b	27590 <__gmpn_divexact_1@@Base+0xf0>
   27544:	mul	x10, x9, x11
   27548:	cmp	x2, #0x2
   2754c:	str	x10, [x0]
   27550:	b.lt	27588 <__gmpn_divexact_1@@Base+0xe8>  // b.tstop
   27554:	mov	x12, xzr
   27558:	sub	x11, x2, #0x1
   2755c:	add	x13, x0, #0x8
   27560:	add	x14, x1, #0x8
   27564:	ldr	x15, [x14], #8
   27568:	umulh	x10, x10, x8
   2756c:	add	x10, x10, x12
   27570:	subs	x10, x15, x10
   27574:	mul	x10, x10, x9
   27578:	cset	w12, cc  // cc = lo, ul, last
   2757c:	subs	x11, x11, #0x1
   27580:	str	x10, [x13], #8
   27584:	b.ne	27564 <__gmpn_divexact_1@@Base+0xc4>  // b.any
   27588:	ret
   2758c:	mov	x12, xzr
   27590:	lsr	x8, x11, x10
   27594:	sub	x8, x8, x12
   27598:	mul	x8, x8, x9
   2759c:	add	x9, x0, x2, lsl #3
   275a0:	stur	x8, [x9, #-8]
   275a4:	ret

00000000000275a8 <__gmpn_divexact_by3c@@Base>:
   275a8:	stp	x29, x30, [sp, #-16]!
   275ac:	mov	x8, #0x5555555555555555    	// #6148914691236517205
   275b0:	mul	x4, x3, x8
   275b4:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   275b8:	mov	x29, sp
   275bc:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   275c0:	and	x0, x0, #0x3
   275c4:	ldp	x29, x30, [sp], #16
   275c8:	ret

00000000000275cc <__gmpn_divisible_p@@Base>:
   275cc:	stp	x29, x30, [sp, #-80]!
   275d0:	stp	x26, x25, [sp, #16]
   275d4:	stp	x24, x23, [sp, #32]
   275d8:	stp	x22, x21, [sp, #48]
   275dc:	stp	x20, x19, [sp, #64]
   275e0:	mov	x29, sp
   275e4:	sub	sp, sp, #0x10
   275e8:	mov	x20, x1
   275ec:	cmp	x1, x3
   275f0:	b.ge	27600 <__gmpn_divisible_p@@Base+0x34>  // b.tcont
   275f4:	cmp	x20, #0x0
   275f8:	cset	w19, eq  // eq = none
   275fc:	b	27664 <__gmpn_divisible_p@@Base+0x98>
   27600:	mov	x21, x2
   27604:	ldr	x2, [x2]
   27608:	ldr	x8, [x0]
   2760c:	mov	x19, x3
   27610:	mov	x23, x0
   27614:	cbz	x2, 2765c <__gmpn_divisible_p@@Base+0x90>
   27618:	neg	x9, x2
   2761c:	and	x9, x2, x9
   27620:	sub	x9, x9, #0x1
   27624:	tst	x9, x8
   27628:	b.ne	27660 <__gmpn_divisible_p@@Base+0x94>  // b.any
   2762c:	cmp	x19, #0x1
   27630:	b.ne	27684 <__gmpn_divisible_p@@Base+0xb8>  // b.any
   27634:	cmp	x20, #0x28
   27638:	b.lt	27730 <__gmpn_divisible_p@@Base+0x164>  // b.tstop
   2763c:	mov	x0, x23
   27640:	mov	x1, x20
   27644:	b	27768 <__gmpn_divisible_p@@Base+0x19c>
   27648:	ldr	x8, [x23, #8]!
   2764c:	ldr	x2, [x21, #8]!
   27650:	sub	x20, x20, #0x1
   27654:	sub	x19, x19, #0x1
   27658:	cbnz	x2, 27618 <__gmpn_divisible_p@@Base+0x4c>
   2765c:	cbz	x8, 27648 <__gmpn_divisible_p@@Base+0x7c>
   27660:	mov	w19, wzr
   27664:	mov	w0, w19
   27668:	mov	sp, x29
   2766c:	ldp	x20, x19, [sp, #64]
   27670:	ldp	x22, x21, [sp, #48]
   27674:	ldp	x24, x23, [sp, #32]
   27678:	ldp	x26, x25, [sp, #16]
   2767c:	ldp	x29, x30, [sp], #80
   27680:	ret
   27684:	rbit	x8, x2
   27688:	cmp	x19, #0x2
   2768c:	clz	x24, x8
   27690:	b.ne	276a0 <__gmpn_divisible_p@@Base+0xd4>  // b.any
   27694:	ldr	x8, [x21, #8]
   27698:	cmp	x8, x9
   2769c:	b.ls	27748 <__gmpn_divisible_p@@Base+0x17c>  // b.plast
   276a0:	add	x26, x20, #0x1
   276a4:	sub	x8, x20, x19
   276a8:	add	x8, x8, x26
   276ac:	lsl	x8, x8, #3
   276b0:	add	x1, x8, #0x8
   276b4:	mov	w8, #0x7f00                	// #32512
   276b8:	cmp	x1, x8
   276bc:	stur	xzr, [x29, #-8]
   276c0:	b.hi	27780 <__gmpn_divisible_p@@Base+0x1b4>  // b.pmore
   276c4:	add	x9, x1, #0xf
   276c8:	mov	x8, sp
   276cc:	and	x9, x9, #0xfffffffffffffff0
   276d0:	sub	x22, x8, x9
   276d4:	mov	sp, x22
   276d8:	cbz	w24, 27790 <__gmpn_divisible_p@@Base+0x1c4>
   276dc:	lsl	x1, x19, #3
   276e0:	mov	w8, #0x7f00                	// #32512
   276e4:	cmp	x1, x8
   276e8:	b.hi	27938 <__gmpn_divisible_p@@Base+0x36c>  // b.pmore
   276ec:	add	x9, x1, #0xf
   276f0:	mov	x8, sp
   276f4:	and	x9, x9, #0xfffffffffffffff0
   276f8:	sub	x25, x8, x9
   276fc:	mov	sp, x25
   27700:	mov	x0, x25
   27704:	mov	x1, x21
   27708:	mov	x2, x19
   2770c:	mov	w3, w24
   27710:	bl	c1a0 <__gmpn_rshift@plt>
   27714:	mov	x0, x22
   27718:	mov	x1, x23
   2771c:	mov	x2, x20
   27720:	mov	w3, w24
   27724:	bl	c1a0 <__gmpn_rshift@plt>
   27728:	mov	x21, x25
   2772c:	b	277a0 <__gmpn_divisible_p@@Base+0x1d4>
   27730:	rbit	x8, x2
   27734:	clz	x8, x8
   27738:	lsr	x2, x2, x8
   2773c:	mov	x0, x23
   27740:	mov	x1, x20
   27744:	b	27770 <__gmpn_divisible_p@@Base+0x1a4>
   27748:	neg	x10, x24
   2774c:	lsr	x9, x2, x24
   27750:	lsl	x8, x8, x10
   27754:	cmp	x20, #0x27
   27758:	orr	x2, x8, x9
   2775c:	mov	x0, x23
   27760:	mov	x1, x20
   27764:	b.le	27770 <__gmpn_divisible_p@@Base+0x1a4>
   27768:	bl	c3e0 <__gmpn_mod_1@plt>
   2776c:	b	27778 <__gmpn_divisible_p@@Base+0x1ac>
   27770:	mov	x3, xzr
   27774:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   27778:	cmp	x0, #0x0
   2777c:	b	275f8 <__gmpn_divisible_p@@Base+0x2c>
   27780:	sub	x0, x29, #0x8
   27784:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27788:	mov	x22, x0
   2778c:	cbnz	w24, 276dc <__gmpn_divisible_p@@Base+0x110>
   27790:	mov	x0, x22
   27794:	mov	x1, x23
   27798:	mov	x2, x20
   2779c:	bl	ca50 <__gmpn_copyi@plt>
   277a0:	add	x8, x22, x20, lsl #3
   277a4:	add	x9, x21, x19, lsl #3
   277a8:	ldur	x8, [x8, #-8]
   277ac:	ldur	x9, [x9, #-8]
   277b0:	cmp	x8, x9
   277b4:	b.cs	277d4 <__gmpn_divisible_p@@Base+0x208>  // b.hs, b.nlast
   277b8:	cmp	x20, x19
   277bc:	b.ne	277dc <__gmpn_divisible_p@@Base+0x210>  // b.any
   277c0:	ldur	x0, [x29, #-8]
   277c4:	cbz	x0, 27660 <__gmpn_divisible_p@@Base+0x94>
   277c8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   277cc:	mov	w19, wzr
   277d0:	b	27664 <__gmpn_divisible_p@@Base+0x98>
   277d4:	str	xzr, [x22, x20, lsl #3]
   277d8:	mov	x20, x26
   277dc:	add	x23, x22, x26, lsl #3
   277e0:	cmp	x19, #0x27
   277e4:	sub	x24, x20, x19
   277e8:	b.lt	2784c <__gmpn_divisible_p@@Base+0x280>  // b.tstop
   277ec:	cmp	x24, #0x26
   277f0:	b.le	2784c <__gmpn_divisible_p@@Base+0x280>
   277f4:	cmp	x19, #0x326
   277f8:	b.le	2789c <__gmpn_divisible_p@@Base+0x2d0>
   277fc:	mov	x0, x20
   27800:	mov	x1, x19
   27804:	bl	d170 <__gmpn_mu_bdiv_qr_itch@plt>
   27808:	lsl	x1, x0, #3
   2780c:	mov	w8, #0x7f00                	// #32512
   27810:	cmp	x1, x8
   27814:	b.hi	27948 <__gmpn_divisible_p@@Base+0x37c>  // b.pmore
   27818:	add	x9, x1, #0xf
   2781c:	mov	x8, sp
   27820:	and	x9, x9, #0xfffffffffffffff0
   27824:	sub	x6, x8, x9
   27828:	mov	sp, x6
   2782c:	mov	x0, x23
   27830:	mov	x1, x22
   27834:	mov	x2, x22
   27838:	mov	x3, x20
   2783c:	mov	x4, x21
   27840:	mov	x5, x19
   27844:	bl	ccb0 <__gmpn_mu_bdiv_qr@plt>
   27848:	b	278ec <__gmpn_divisible_p@@Base+0x320>
   2784c:	ldr	x8, [x21]
   27850:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   27854:	ldr	x9, [x9, #3952]
   27858:	mov	x0, x23
   2785c:	ubfx	x10, x8, #1, #7
   27860:	mov	x1, x22
   27864:	ldrb	w9, [x9, x10]
   27868:	mov	w10, #0x2                   	// #2
   2786c:	mov	x2, x20
   27870:	mov	x3, x21
   27874:	msub	x11, x8, x9, x10
   27878:	mul	x9, x11, x9
   2787c:	msub	x10, x9, x8, x10
   27880:	mul	x9, x9, x10
   27884:	orr	x10, xzr, #0xfffffffffffffffe
   27888:	madd	x8, x9, x8, x10
   2788c:	mul	x5, x8, x9
   27890:	mov	x4, x19
   27894:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   27898:	b	278e8 <__gmpn_divisible_p@@Base+0x31c>
   2789c:	ldr	x8, [x21]
   278a0:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   278a4:	ldr	x9, [x9, #3952]
   278a8:	mov	x0, x23
   278ac:	ubfx	x10, x8, #1, #7
   278b0:	mov	x1, x22
   278b4:	ldrb	w9, [x9, x10]
   278b8:	mov	w10, #0x2                   	// #2
   278bc:	mov	x2, x20
   278c0:	mov	x3, x21
   278c4:	msub	x11, x8, x9, x10
   278c8:	mul	x9, x11, x9
   278cc:	msub	x10, x9, x8, x10
   278d0:	mul	x9, x9, x10
   278d4:	orr	x10, xzr, #0xfffffffffffffffe
   278d8:	madd	x8, x9, x8, x10
   278dc:	mul	x5, x8, x9
   278e0:	mov	x4, x19
   278e4:	bl	c5e0 <__gmpn_dcpi1_bdiv_qr@plt>
   278e8:	add	x22, x22, x24, lsl #3
   278ec:	sub	x8, x22, #0x8
   278f0:	sub	x9, x21, #0x8
   278f4:	subs	x10, x19, #0x1
   278f8:	b.lt	27924 <__gmpn_divisible_p@@Base+0x358>  // b.tstop
   278fc:	lsl	x11, x19, #3
   27900:	ldr	x12, [x8, x11]
   27904:	ldr	x11, [x9, x11]
   27908:	mov	x19, x10
   2790c:	cmp	x12, x11
   27910:	b.eq	278f4 <__gmpn_divisible_p@@Base+0x328>  // b.none
   27914:	mov	w19, wzr
   27918:	ldur	x0, [x29, #-8]
   2791c:	cbz	x0, 27664 <__gmpn_divisible_p@@Base+0x98>
   27920:	b	27930 <__gmpn_divisible_p@@Base+0x364>
   27924:	mov	w19, #0x1                   	// #1
   27928:	ldur	x0, [x29, #-8]
   2792c:	cbz	x0, 27664 <__gmpn_divisible_p@@Base+0x98>
   27930:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   27934:	b	27664 <__gmpn_divisible_p@@Base+0x98>
   27938:	sub	x0, x29, #0x8
   2793c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27940:	mov	x25, x0
   27944:	b	27700 <__gmpn_divisible_p@@Base+0x134>
   27948:	sub	x0, x29, #0x8
   2794c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27950:	mov	x6, x0
   27954:	b	2782c <__gmpn_divisible_p@@Base+0x260>

0000000000027958 <__gmpn_divrem@@Base>:
   27958:	stp	x29, x30, [sp, #-96]!
   2795c:	stp	x28, x27, [sp, #16]
   27960:	stp	x26, x25, [sp, #32]
   27964:	stp	x24, x23, [sp, #48]
   27968:	stp	x22, x21, [sp, #64]
   2796c:	stp	x20, x19, [sp, #80]
   27970:	mov	x29, sp
   27974:	sub	sp, sp, #0x10
   27978:	mov	x21, x4
   2797c:	mov	x22, x3
   27980:	mov	x20, x2
   27984:	mov	x24, x1
   27988:	cmp	x5, #0x2
   2798c:	mov	x19, x0
   27990:	b.eq	27a2c <__gmpn_divrem@@Base+0xd4>  // b.none
   27994:	mov	x23, x5
   27998:	cmp	x5, #0x1
   2799c:	b.ne	27a60 <__gmpn_divrem@@Base+0x108>  // b.any
   279a0:	add	x25, x22, x24
   279a4:	lsl	x1, x25, #3
   279a8:	mov	w8, #0x7f00                	// #32512
   279ac:	cmp	x1, x8
   279b0:	stur	xzr, [x29, #-8]
   279b4:	b.hi	27adc <__gmpn_divrem@@Base+0x184>  // b.pmore
   279b8:	add	x9, x1, #0xf
   279bc:	mov	x8, sp
   279c0:	and	x9, x9, #0xfffffffffffffff0
   279c4:	sub	x23, x8, x9
   279c8:	mov	sp, x23
   279cc:	ldr	x4, [x21]
   279d0:	mov	x0, x23
   279d4:	mov	x1, x24
   279d8:	mov	x2, x20
   279dc:	mov	x3, x22
   279e0:	bl	cd00 <__gmpn_divrem_1@plt>
   279e4:	str	x0, [x20]
   279e8:	sub	x20, x25, #0x1
   279ec:	mov	x0, x19
   279f0:	mov	x1, x23
   279f4:	mov	x2, x20
   279f8:	bl	ca50 <__gmpn_copyi@plt>
   279fc:	ldur	x0, [x29, #-8]
   27a00:	ldr	x19, [x23, x20, lsl #3]
   27a04:	cbnz	x0, 27ad4 <__gmpn_divrem@@Base+0x17c>
   27a08:	mov	x0, x19
   27a0c:	mov	sp, x29
   27a10:	ldp	x20, x19, [sp, #80]
   27a14:	ldp	x22, x21, [sp, #64]
   27a18:	ldp	x24, x23, [sp, #48]
   27a1c:	ldp	x26, x25, [sp, #32]
   27a20:	ldp	x28, x27, [sp, #16]
   27a24:	ldp	x29, x30, [sp], #96
   27a28:	ret
   27a2c:	mov	x0, x19
   27a30:	mov	x1, x24
   27a34:	mov	x2, x20
   27a38:	mov	x3, x22
   27a3c:	mov	x4, x21
   27a40:	mov	sp, x29
   27a44:	ldp	x20, x19, [sp, #80]
   27a48:	ldp	x22, x21, [sp, #64]
   27a4c:	ldp	x24, x23, [sp, #48]
   27a50:	ldp	x26, x25, [sp, #32]
   27a54:	ldp	x28, x27, [sp, #16]
   27a58:	ldp	x29, x30, [sp], #96
   27a5c:	b	c200 <__gmpn_divrem_2@plt>
   27a60:	stur	xzr, [x29, #-8]
   27a64:	cbnz	x24, 27aec <__gmpn_divrem@@Base+0x194>
   27a68:	sub	x24, x22, x23
   27a6c:	lsl	x8, x24, #3
   27a70:	add	x1, x8, #0x8
   27a74:	mov	w8, #0x7f00                	// #32512
   27a78:	cmp	x1, x8
   27a7c:	b.hi	27b84 <__gmpn_divrem@@Base+0x22c>  // b.pmore
   27a80:	add	x9, x1, #0xf
   27a84:	mov	x8, sp
   27a88:	and	x9, x9, #0xfffffffffffffff0
   27a8c:	sub	x25, x8, x9
   27a90:	mov	sp, x25
   27a94:	mov	x0, x25
   27a98:	mov	x1, x20
   27a9c:	mov	x2, xzr
   27aa0:	mov	x3, x20
   27aa4:	mov	x4, x22
   27aa8:	mov	x5, x21
   27aac:	mov	x6, x23
   27ab0:	bl	bf00 <__gmpn_tdiv_qr@plt>
   27ab4:	mov	x0, x19
   27ab8:	mov	x1, x25
   27abc:	mov	x2, x24
   27ac0:	bl	ca50 <__gmpn_copyi@plt>
   27ac4:	add	x8, x25, x24, lsl #3
   27ac8:	ldur	x0, [x29, #-8]
   27acc:	ldr	x19, [x8]
   27ad0:	cbz	x0, 27a08 <__gmpn_divrem@@Base+0xb0>
   27ad4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   27ad8:	b	27a08 <__gmpn_divrem@@Base+0xb0>
   27adc:	sub	x0, x29, #0x8
   27ae0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27ae4:	mov	x23, x0
   27ae8:	b	279cc <__gmpn_divrem@@Base+0x74>
   27aec:	sub	x8, x22, x23
   27af0:	add	x26, x22, x24
   27af4:	add	x25, x8, x24
   27af8:	add	x8, x26, x25
   27afc:	lsl	x8, x8, #3
   27b00:	add	x1, x8, #0x8
   27b04:	mov	w8, #0x7f00                	// #32512
   27b08:	cmp	x1, x8
   27b0c:	b.hi	27b94 <__gmpn_divrem@@Base+0x23c>  // b.pmore
   27b10:	add	x9, x1, #0xf
   27b14:	mov	x8, sp
   27b18:	and	x9, x9, #0xfffffffffffffff0
   27b1c:	sub	x27, x8, x9
   27b20:	mov	sp, x27
   27b24:	lsl	x24, x24, #3
   27b28:	mov	x0, x27
   27b2c:	mov	w1, wzr
   27b30:	mov	x2, x24
   27b34:	bl	c5f0 <memset@plt>
   27b38:	add	x0, x27, x24
   27b3c:	mov	x1, x20
   27b40:	mov	x2, x22
   27b44:	add	x28, x27, x26, lsl #3
   27b48:	bl	ca50 <__gmpn_copyi@plt>
   27b4c:	mov	x0, x28
   27b50:	mov	x1, x20
   27b54:	mov	x2, xzr
   27b58:	mov	x3, x27
   27b5c:	mov	x4, x26
   27b60:	mov	x5, x21
   27b64:	mov	x6, x23
   27b68:	bl	bf00 <__gmpn_tdiv_qr@plt>
   27b6c:	mov	x0, x19
   27b70:	mov	x1, x28
   27b74:	mov	x2, x25
   27b78:	bl	ca50 <__gmpn_copyi@plt>
   27b7c:	add	x8, x28, x25, lsl #3
   27b80:	b	27ac8 <__gmpn_divrem@@Base+0x170>
   27b84:	sub	x0, x29, #0x8
   27b88:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27b8c:	mov	x25, x0
   27b90:	b	27a94 <__gmpn_divrem@@Base+0x13c>
   27b94:	sub	x0, x29, #0x8
   27b98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   27b9c:	mov	x27, x0
   27ba0:	b	27b24 <__gmpn_divrem@@Base+0x1cc>

0000000000027ba4 <__gmpn_divrem_1@@Base>:
   27ba4:	stp	x29, x30, [sp, #-80]!
   27ba8:	adds	x8, x3, x1
   27bac:	str	x25, [sp, #16]
   27bb0:	stp	x24, x23, [sp, #32]
   27bb4:	stp	x22, x21, [sp, #48]
   27bb8:	stp	x20, x19, [sp, #64]
   27bbc:	mov	x29, sp
   27bc0:	b.eq	27c0c <__gmpn_divrem_1@@Base+0x68>  // b.none
   27bc4:	sub	x9, x8, #0x1
   27bc8:	mov	x20, x4
   27bcc:	mov	x21, x3
   27bd0:	mov	x23, x2
   27bd4:	mov	x19, x1
   27bd8:	add	x24, x0, x9, lsl #3
   27bdc:	tbnz	x4, #63, 27cc8 <__gmpn_divrem_1@@Base+0x124>
   27be0:	cbz	x21, 27c14 <__gmpn_divrem_1@@Base+0x70>
   27be4:	sub	x10, x21, #0x1
   27be8:	ldr	x22, [x23, x10, lsl #3]
   27bec:	cmp	x22, x20
   27bf0:	b.cs	27c14 <__gmpn_divrem_1@@Base+0x70>  // b.hs, b.nlast
   27bf4:	str	xzr, [x24]
   27bf8:	cbz	x9, 281ec <__gmpn_divrem_1@@Base+0x648>
   27bfc:	sub	x24, x24, #0x8
   27c00:	mov	x8, x9
   27c04:	mov	x21, x10
   27c08:	b	27c18 <__gmpn_divrem_1@@Base+0x74>
   27c0c:	mov	x22, xzr
   27c10:	b	281ec <__gmpn_divrem_1@@Base+0x648>
   27c14:	mov	x22, xzr
   27c18:	clz	x25, x20
   27c1c:	cmp	x8, #0x3
   27c20:	lsl	x20, x20, x25
   27c24:	lsl	x22, x22, x25
   27c28:	b.le	27da4 <__gmpn_divrem_1@@Base+0x200>
   27c2c:	mov	x0, x20
   27c30:	bl	d3f0 <__gmpn_invert_limb@plt>
   27c34:	cbz	x21, 28050 <__gmpn_divrem_1@@Base+0x4ac>
   27c38:	add	x8, x23, x21, lsl #3
   27c3c:	ldur	x12, [x8, #-8]
   27c40:	neg	x8, x25
   27c44:	cmp	x21, #0x2
   27c48:	lsr	x8, x12, x8
   27c4c:	orr	x11, x8, x22
   27c50:	b.lt	28008 <__gmpn_divrem_1@@Base+0x464>  // b.tstop
   27c54:	mov	w8, #0x40                  	// #64
   27c58:	sub	x8, x8, x25
   27c5c:	sub	x9, x23, #0x10
   27c60:	ldr	x10, [x9, x21, lsl #3]
   27c64:	lsl	x12, x12, x25
   27c68:	umulh	x13, x11, x0
   27c6c:	lsr	x14, x10, x8
   27c70:	orr	x12, x14, x12
   27c74:	mul	x14, x11, x0
   27c78:	add	x11, x11, #0x1
   27c7c:	adds	x15, x14, x12
   27c80:	adc	x11, x13, x11
   27c84:	msub	x12, x11, x20, x12
   27c88:	cmp	x12, x15
   27c8c:	cset	w14, hi  // hi = pmore
   27c90:	sub	x11, x11, x14
   27c94:	csel	x14, x20, xzr, hi  // hi = pmore
   27c98:	add	x12, x14, x12
   27c9c:	cmp	x12, x20
   27ca0:	sub	x13, x21, #0x2
   27ca4:	cinc	x14, x11, cs  // cs = hs, nlast
   27ca8:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   27cac:	sub	x21, x21, #0x1
   27cb0:	cmp	x13, #0x0
   27cb4:	sub	x11, x12, x11
   27cb8:	str	x14, [x24], #-8
   27cbc:	mov	x12, x10
   27cc0:	b.gt	27c60 <__gmpn_divrem_1@@Base+0xbc>
   27cc4:	b	2800c <__gmpn_divrem_1@@Base+0x468>
   27cc8:	cbz	x21, 27e90 <__gmpn_divrem_1@@Base+0x2ec>
   27ccc:	sub	x21, x21, #0x1
   27cd0:	ldr	x8, [x23, x21, lsl #3]
   27cd4:	cmp	x8, x20
   27cd8:	cset	w10, cs  // cs = hs, nlast
   27cdc:	csel	x11, x20, xzr, cs  // cs = hs, nlast
   27ce0:	str	x10, [x24], #-8
   27ce4:	sub	x22, x8, x11
   27ce8:	mov	x8, x9
   27cec:	cmp	x8, #0x2
   27cf0:	b.le	27e9c <__gmpn_divrem_1@@Base+0x2f8>
   27cf4:	mov	x0, x20
   27cf8:	bl	d3f0 <__gmpn_invert_limb@plt>
   27cfc:	cmp	x21, #0x1
   27d00:	b.lt	27d5c <__gmpn_divrem_1@@Base+0x1b8>  // b.tstop
   27d04:	add	x8, x21, #0x1
   27d08:	sub	x9, x23, #0x10
   27d0c:	ldr	x10, [x9, x8, lsl #3]
   27d10:	umulh	x11, x22, x0
   27d14:	mul	x12, x22, x0
   27d18:	add	x13, x22, #0x1
   27d1c:	adds	x14, x12, x10
   27d20:	adc	x11, x11, x13
   27d24:	msub	x10, x11, x20, x10
   27d28:	cmp	x10, x14
   27d2c:	csel	x13, x20, xzr, hi  // hi = pmore
   27d30:	cset	w12, hi  // hi = pmore
   27d34:	add	x10, x13, x10
   27d38:	sub	x11, x11, x12
   27d3c:	cmp	x10, x20
   27d40:	sub	x8, x8, #0x1
   27d44:	csel	x12, xzr, x20, cc  // cc = lo, ul, last
   27d48:	cinc	x11, x11, cs  // cs = hs, nlast
   27d4c:	cmp	x8, #0x1
   27d50:	sub	x22, x10, x12
   27d54:	str	x11, [x24], #-8
   27d58:	b.gt	27d0c <__gmpn_divrem_1@@Base+0x168>
   27d5c:	cmp	x19, #0x1
   27d60:	b.lt	281ec <__gmpn_divrem_1@@Base+0x648>  // b.tstop
   27d64:	add	x8, x19, #0x1
   27d68:	umulh	x9, x22, x0
   27d6c:	add	x9, x22, x9
   27d70:	add	x9, x9, #0x1
   27d74:	mul	x10, x22, x0
   27d78:	mneg	x11, x9, x20
   27d7c:	cmp	x10, x11
   27d80:	cset	w10, cc  // cc = lo, ul, last
   27d84:	sub	x8, x8, #0x1
   27d88:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   27d8c:	sub	x10, x9, x10
   27d90:	msub	x22, x9, x20, x11
   27d94:	cmp	x8, #0x1
   27d98:	str	x10, [x24], #-8
   27d9c:	b.gt	27d68 <__gmpn_divrem_1@@Base+0x1c4>
   27da0:	b	281ec <__gmpn_divrem_1@@Base+0x648>
   27da4:	cbz	x21, 28134 <__gmpn_divrem_1@@Base+0x590>
   27da8:	add	x8, x23, x21, lsl #3
   27dac:	ldur	x10, [x8, #-8]
   27db0:	neg	x8, x25
   27db4:	cmp	x21, #0x1
   27db8:	lsr	x8, x10, x8
   27dbc:	orr	x13, x8, x22
   27dc0:	b.le	28098 <__gmpn_divrem_1@@Base+0x4f4>
   27dc4:	mov	w11, #0x40                  	// #64
   27dc8:	lsr	x8, x20, #32
   27dcc:	and	x9, x20, #0xffffffff
   27dd0:	sub	x11, x11, x25
   27dd4:	sub	x12, x23, #0x10
   27dd8:	b	27dfc <__gmpn_divrem_1@@Base+0x258>
   27ddc:	mov	x17, x16
   27de0:	sub	x13, x13, x15
   27de4:	orr	x14, x17, x14, lsl #32
   27de8:	sub	x15, x21, #0x2
   27dec:	sub	x21, x21, #0x1
   27df0:	cmp	x15, #0x0
   27df4:	str	x14, [x24], #-8
   27df8:	b.le	280a0 <__gmpn_divrem_1@@Base+0x4fc>
   27dfc:	mov	x14, x10
   27e00:	ldr	x10, [x12, x21, lsl #3]
   27e04:	udiv	x17, x13, x8
   27e08:	lsl	x14, x14, x25
   27e0c:	msub	w16, w17, w8, w13
   27e10:	lsr	x13, x10, x11
   27e14:	orr	x13, x13, x14
   27e18:	mul	x15, x17, x9
   27e1c:	extr	x16, x16, x13, #32
   27e20:	cmp	x16, x15
   27e24:	b.cs	27e4c <__gmpn_divrem_1@@Base+0x2a8>  // b.hs, b.nlast
   27e28:	add	x16, x16, x20
   27e2c:	cmp	x16, x20
   27e30:	sub	x14, x17, #0x1
   27e34:	b.cc	27e50 <__gmpn_divrem_1@@Base+0x2ac>  // b.lo, b.ul, b.last
   27e38:	cmp	x16, x15
   27e3c:	b.cs	27e50 <__gmpn_divrem_1@@Base+0x2ac>  // b.hs, b.nlast
   27e40:	sub	x14, x17, #0x2
   27e44:	add	x16, x16, x20
   27e48:	b	27e50 <__gmpn_divrem_1@@Base+0x2ac>
   27e4c:	mov	x14, x17
   27e50:	sub	x15, x16, x15
   27e54:	udiv	x16, x15, x8
   27e58:	msub	w17, w16, w8, w15
   27e5c:	mul	x15, x16, x9
   27e60:	bfi	x13, x17, #32, #32
   27e64:	cmp	x13, x15
   27e68:	b.cs	27ddc <__gmpn_divrem_1@@Base+0x238>  // b.hs, b.nlast
   27e6c:	add	x13, x13, x20
   27e70:	cmp	x13, x20
   27e74:	sub	x17, x16, #0x1
   27e78:	b.cc	27de0 <__gmpn_divrem_1@@Base+0x23c>  // b.lo, b.ul, b.last
   27e7c:	cmp	x13, x15
   27e80:	b.cs	27de0 <__gmpn_divrem_1@@Base+0x23c>  // b.hs, b.nlast
   27e84:	sub	x17, x16, #0x2
   27e88:	add	x13, x13, x20
   27e8c:	b	27de0 <__gmpn_divrem_1@@Base+0x23c>
   27e90:	mov	x22, xzr
   27e94:	cmp	x8, #0x2
   27e98:	b.gt	27cf4 <__gmpn_divrem_1@@Base+0x150>
   27e9c:	cmp	x21, #0x1
   27ea0:	lsr	x8, x20, #32
   27ea4:	b.lt	27f58 <__gmpn_divrem_1@@Base+0x3b4>  // b.tstop
   27ea8:	and	x9, x20, #0xffffffff
   27eac:	add	x10, x21, #0x1
   27eb0:	sub	x11, x23, #0x10
   27eb4:	b	27ed4 <__gmpn_divrem_1@@Base+0x330>
   27eb8:	mov	x16, x15
   27ebc:	sub	x22, x12, x14
   27ec0:	orr	x12, x16, x13, lsl #32
   27ec4:	sub	x10, x10, #0x1
   27ec8:	cmp	x10, #0x1
   27ecc:	str	x12, [x24], #-8
   27ed0:	b.le	27f58 <__gmpn_divrem_1@@Base+0x3b4>
   27ed4:	ldr	x12, [x11, x10, lsl #3]
   27ed8:	udiv	x16, x22, x8
   27edc:	msub	w13, w16, w8, w22
   27ee0:	mul	x14, x16, x9
   27ee4:	extr	x15, x13, x12, #32
   27ee8:	cmp	x15, x14
   27eec:	b.cs	27f14 <__gmpn_divrem_1@@Base+0x370>  // b.hs, b.nlast
   27ef0:	add	x15, x15, x20
   27ef4:	cmp	x15, x20
   27ef8:	sub	x13, x16, #0x1
   27efc:	b.cc	27f18 <__gmpn_divrem_1@@Base+0x374>  // b.lo, b.ul, b.last
   27f00:	cmp	x15, x14
   27f04:	b.cs	27f18 <__gmpn_divrem_1@@Base+0x374>  // b.hs, b.nlast
   27f08:	sub	x13, x16, #0x2
   27f0c:	add	x15, x15, x20
   27f10:	b	27f18 <__gmpn_divrem_1@@Base+0x374>
   27f14:	mov	x13, x16
   27f18:	sub	x14, x15, x14
   27f1c:	udiv	x15, x14, x8
   27f20:	msub	w16, w15, w8, w14
   27f24:	mul	x14, x15, x9
   27f28:	bfi	x12, x16, #32, #32
   27f2c:	cmp	x12, x14
   27f30:	b.cs	27eb8 <__gmpn_divrem_1@@Base+0x314>  // b.hs, b.nlast
   27f34:	add	x12, x12, x20
   27f38:	cmp	x12, x20
   27f3c:	sub	x16, x15, #0x1
   27f40:	b.cc	27ebc <__gmpn_divrem_1@@Base+0x318>  // b.lo, b.ul, b.last
   27f44:	cmp	x12, x14
   27f48:	b.cs	27ebc <__gmpn_divrem_1@@Base+0x318>  // b.hs, b.nlast
   27f4c:	sub	x16, x15, #0x2
   27f50:	add	x12, x12, x20
   27f54:	b	27ebc <__gmpn_divrem_1@@Base+0x318>
   27f58:	cmp	x19, #0x1
   27f5c:	b.lt	281ec <__gmpn_divrem_1@@Base+0x648>  // b.tstop
   27f60:	and	x9, x20, #0xffffffff
   27f64:	add	x10, x19, #0x1
   27f68:	b	27f88 <__gmpn_divrem_1@@Base+0x3e4>
   27f6c:	mov	x15, x14
   27f70:	orr	x11, x15, x11, lsl #32
   27f74:	sub	x10, x10, #0x1
   27f78:	sub	x22, x13, x12
   27f7c:	cmp	x10, #0x1
   27f80:	str	x11, [x24], #-8
   27f84:	b.le	281ec <__gmpn_divrem_1@@Base+0x648>
   27f88:	udiv	x14, x22, x8
   27f8c:	msub	w11, w14, w8, w22
   27f90:	mul	x12, x14, x9
   27f94:	lsl	x13, x11, #32
   27f98:	cmp	x13, x12
   27f9c:	b.cs	27fc4 <__gmpn_divrem_1@@Base+0x420>  // b.hs, b.nlast
   27fa0:	add	x13, x13, x20
   27fa4:	cmp	x13, x20
   27fa8:	sub	x11, x14, #0x1
   27fac:	b.cc	27fc8 <__gmpn_divrem_1@@Base+0x424>  // b.lo, b.ul, b.last
   27fb0:	cmp	x13, x12
   27fb4:	b.cs	27fc8 <__gmpn_divrem_1@@Base+0x424>  // b.hs, b.nlast
   27fb8:	sub	x11, x14, #0x2
   27fbc:	add	x13, x13, x20
   27fc0:	b	27fc8 <__gmpn_divrem_1@@Base+0x424>
   27fc4:	mov	x11, x14
   27fc8:	sub	x12, x13, x12
   27fcc:	udiv	x14, x12, x8
   27fd0:	msub	w13, w14, w8, w12
   27fd4:	mul	x12, x14, x9
   27fd8:	lsl	x13, x13, #32
   27fdc:	cmp	x13, x12
   27fe0:	b.cs	27f6c <__gmpn_divrem_1@@Base+0x3c8>  // b.hs, b.nlast
   27fe4:	add	x13, x13, x20
   27fe8:	cmp	x13, x20
   27fec:	sub	x15, x14, #0x1
   27ff0:	b.cc	27f70 <__gmpn_divrem_1@@Base+0x3cc>  // b.lo, b.ul, b.last
   27ff4:	cmp	x13, x12
   27ff8:	b.cs	27f70 <__gmpn_divrem_1@@Base+0x3cc>  // b.hs, b.nlast
   27ffc:	sub	x15, x14, #0x2
   28000:	add	x13, x13, x20
   28004:	b	27f70 <__gmpn_divrem_1@@Base+0x3cc>
   28008:	mov	x10, x12
   2800c:	umulh	x8, x11, x0
   28010:	mul	x9, x11, x0
   28014:	lsl	x10, x10, x25
   28018:	add	x11, x11, #0x1
   2801c:	adds	x12, x9, x10
   28020:	adc	x8, x8, x11
   28024:	msub	x9, x8, x20, x10
   28028:	cmp	x9, x12
   2802c:	csel	x11, x20, xzr, hi  // hi = pmore
   28030:	cset	w10, hi  // hi = pmore
   28034:	add	x9, x11, x9
   28038:	sub	x8, x8, x10
   2803c:	cmp	x9, x20
   28040:	cinc	x8, x8, cs  // cs = hs, nlast
   28044:	csel	x10, xzr, x20, cc  // cc = lo, ul, last
   28048:	sub	x22, x9, x10
   2804c:	str	x8, [x24], #-8
   28050:	cmp	x19, #0x1
   28054:	b.lt	281e8 <__gmpn_divrem_1@@Base+0x644>  // b.tstop
   28058:	add	x8, x19, #0x1
   2805c:	umulh	x9, x22, x0
   28060:	add	x9, x22, x9
   28064:	add	x9, x9, #0x1
   28068:	mul	x10, x22, x0
   2806c:	mneg	x11, x9, x20
   28070:	cmp	x10, x11
   28074:	cset	w10, cc  // cc = lo, ul, last
   28078:	sub	x8, x8, #0x1
   2807c:	csel	x11, x20, xzr, cc  // cc = lo, ul, last
   28080:	sub	x10, x9, x10
   28084:	msub	x22, x9, x20, x11
   28088:	cmp	x8, #0x1
   2808c:	str	x10, [x24], #-8
   28090:	b.gt	2805c <__gmpn_divrem_1@@Base+0x4b8>
   28094:	b	281e8 <__gmpn_divrem_1@@Base+0x644>
   28098:	lsr	x8, x20, #32
   2809c:	and	x9, x20, #0xffffffff
   280a0:	udiv	x14, x13, x8
   280a4:	msub	w11, w14, w8, w13
   280a8:	lsl	x10, x10, x25
   280ac:	mul	x12, x14, x9
   280b0:	extr	x13, x11, x10, #32
   280b4:	cmp	x13, x12
   280b8:	b.cs	280e0 <__gmpn_divrem_1@@Base+0x53c>  // b.hs, b.nlast
   280bc:	add	x13, x13, x20
   280c0:	cmp	x13, x20
   280c4:	sub	x11, x14, #0x1
   280c8:	b.cc	280e4 <__gmpn_divrem_1@@Base+0x540>  // b.lo, b.ul, b.last
   280cc:	cmp	x13, x12
   280d0:	b.cs	280e4 <__gmpn_divrem_1@@Base+0x540>  // b.hs, b.nlast
   280d4:	sub	x11, x14, #0x2
   280d8:	add	x13, x13, x20
   280dc:	b	280e4 <__gmpn_divrem_1@@Base+0x540>
   280e0:	mov	x11, x14
   280e4:	sub	x13, x13, x12
   280e8:	udiv	x12, x13, x8
   280ec:	msub	w13, w12, w8, w13
   280f0:	mul	x8, x12, x9
   280f4:	bfi	x10, x13, #32, #32
   280f8:	cmp	x10, x8
   280fc:	b.cs	28124 <__gmpn_divrem_1@@Base+0x580>  // b.hs, b.nlast
   28100:	add	x10, x10, x20
   28104:	cmp	x10, x20
   28108:	sub	x9, x12, #0x1
   2810c:	b.cc	28128 <__gmpn_divrem_1@@Base+0x584>  // b.lo, b.ul, b.last
   28110:	cmp	x10, x8
   28114:	b.cs	28128 <__gmpn_divrem_1@@Base+0x584>  // b.hs, b.nlast
   28118:	sub	x9, x12, #0x2
   2811c:	add	x10, x10, x20
   28120:	b	28128 <__gmpn_divrem_1@@Base+0x584>
   28124:	mov	x9, x12
   28128:	sub	x22, x10, x8
   2812c:	orr	x8, x9, x11, lsl #32
   28130:	str	x8, [x24], #-8
   28134:	cmp	x19, #0x1
   28138:	b.lt	281e8 <__gmpn_divrem_1@@Base+0x644>  // b.tstop
   2813c:	lsr	x8, x20, #32
   28140:	and	x9, x20, #0xffffffff
   28144:	add	x10, x19, #0x1
   28148:	b	28168 <__gmpn_divrem_1@@Base+0x5c4>
   2814c:	mov	x15, x14
   28150:	orr	x11, x15, x11, lsl #32
   28154:	sub	x10, x10, #0x1
   28158:	sub	x22, x13, x12
   2815c:	cmp	x10, #0x1
   28160:	str	x11, [x24], #-8
   28164:	b.le	281e8 <__gmpn_divrem_1@@Base+0x644>
   28168:	udiv	x14, x22, x8
   2816c:	msub	w11, w14, w8, w22
   28170:	mul	x12, x14, x9
   28174:	lsl	x13, x11, #32
   28178:	cmp	x13, x12
   2817c:	b.cs	281a4 <__gmpn_divrem_1@@Base+0x600>  // b.hs, b.nlast
   28180:	add	x13, x13, x20
   28184:	cmp	x13, x20
   28188:	sub	x11, x14, #0x1
   2818c:	b.cc	281a8 <__gmpn_divrem_1@@Base+0x604>  // b.lo, b.ul, b.last
   28190:	cmp	x13, x12
   28194:	b.cs	281a8 <__gmpn_divrem_1@@Base+0x604>  // b.hs, b.nlast
   28198:	sub	x11, x14, #0x2
   2819c:	add	x13, x13, x20
   281a0:	b	281a8 <__gmpn_divrem_1@@Base+0x604>
   281a4:	mov	x11, x14
   281a8:	sub	x12, x13, x12
   281ac:	udiv	x14, x12, x8
   281b0:	msub	w13, w14, w8, w12
   281b4:	mul	x12, x14, x9
   281b8:	lsl	x13, x13, #32
   281bc:	cmp	x13, x12
   281c0:	b.cs	2814c <__gmpn_divrem_1@@Base+0x5a8>  // b.hs, b.nlast
   281c4:	add	x13, x13, x20
   281c8:	cmp	x13, x20
   281cc:	sub	x15, x14, #0x1
   281d0:	b.cc	28150 <__gmpn_divrem_1@@Base+0x5ac>  // b.lo, b.ul, b.last
   281d4:	cmp	x13, x12
   281d8:	b.cs	28150 <__gmpn_divrem_1@@Base+0x5ac>  // b.hs, b.nlast
   281dc:	sub	x15, x14, #0x2
   281e0:	add	x13, x13, x20
   281e4:	b	28150 <__gmpn_divrem_1@@Base+0x5ac>
   281e8:	lsr	x22, x22, x25
   281ec:	mov	x0, x22
   281f0:	ldp	x20, x19, [sp, #64]
   281f4:	ldp	x22, x21, [sp, #48]
   281f8:	ldp	x24, x23, [sp, #32]
   281fc:	ldr	x25, [sp, #16]
   28200:	ldp	x29, x30, [sp], #80
   28204:	ret

0000000000028208 <__gmpn_divrem_2@@Base>:
   28208:	stp	x29, x30, [sp, #-96]!
   2820c:	stp	x28, x27, [sp, #16]
   28210:	stp	x26, x25, [sp, #32]
   28214:	stp	x24, x23, [sp, #48]
   28218:	stp	x22, x21, [sp, #64]
   2821c:	stp	x20, x19, [sp, #80]
   28220:	add	x28, x2, x3, lsl #3
   28224:	ldr	x27, [x28, #-16]!
   28228:	ldp	x25, x20, [x4]
   2822c:	mov	x24, x3
   28230:	mov	x22, x2
   28234:	ldr	x26, [x28, #8]
   28238:	mov	x19, x1
   2823c:	mov	x23, x0
   28240:	mov	x29, sp
   28244:	cmp	x26, x20
   28248:	b.cc	28258 <__gmpn_divrem_2@@Base+0x50>  // b.lo, b.ul, b.last
   2824c:	b.hi	28260 <__gmpn_divrem_2@@Base+0x58>  // b.pmore
   28250:	cmp	x27, x25
   28254:	b.cs	28260 <__gmpn_divrem_2@@Base+0x58>  // b.hs, b.nlast
   28258:	mov	x21, xzr
   2825c:	b	28270 <__gmpn_divrem_2@@Base+0x68>
   28260:	subs	x8, x27, x25
   28264:	sbc	x26, x26, x20
   28268:	mov	w21, #0x1                   	// #1
   2826c:	mov	x27, x8
   28270:	mov	x0, x20
   28274:	bl	d3f0 <__gmpn_invert_limb@plt>
   28278:	mul	x8, x0, x20
   2827c:	adds	x8, x8, x25
   28280:	b.cc	2829c <__gmpn_divrem_2@@Base+0x94>  // b.lo, b.ul, b.last
   28284:	subs	x8, x8, x20
   28288:	cset	w9, cs  // cs = hs, nlast
   2828c:	csel	x10, x20, xzr, cs  // cs = hs, nlast
   28290:	mvn	x9, x9
   28294:	add	x0, x9, x0
   28298:	sub	x8, x8, x10
   2829c:	umulh	x9, x25, x0
   282a0:	adds	x9, x9, x8
   282a4:	b.cc	28358 <__gmpn_divrem_2@@Base+0x150>  // b.lo, b.ul, b.last
   282a8:	cmp	x9, x20
   282ac:	sub	x8, x0, #0x1
   282b0:	b.cs	283a0 <__gmpn_divrem_2@@Base+0x198>  // b.hs, b.nlast
   282b4:	subs	x9, x24, #0x3
   282b8:	b.lt	28364 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   282bc:	add	x10, x23, x19, lsl #3
   282c0:	ldr	x11, [x22, x9, lsl #3]
   282c4:	mul	x12, x26, x8
   282c8:	umulh	x13, x26, x8
   282cc:	adds	x14, x12, x27
   282d0:	adc	x12, x13, x26
   282d4:	msub	x13, x12, x20, x27
   282d8:	subs	x17, x11, x25
   282dc:	sbc	x11, x13, x20
   282e0:	mul	x15, x12, x25
   282e4:	umulh	x16, x25, x12
   282e8:	subs	x13, x17, x15
   282ec:	sbc	x11, x11, x16
   282f0:	cmp	x11, x14
   282f4:	cset	w14, cs  // cs = hs, nlast
   282f8:	csetm	x15, cs  // cs = hs, nlast
   282fc:	sub	x12, x12, x14
   28300:	and	x14, x25, x15
   28304:	and	x15, x20, x15
   28308:	adds	x27, x13, x14
   2830c:	adc	x26, x11, x15
   28310:	cmp	x26, x20
   28314:	add	x11, x12, #0x1
   28318:	b.cs	28334 <__gmpn_divrem_2@@Base+0x12c>  // b.hs, b.nlast
   2831c:	sub	x12, x9, #0x1
   28320:	cmp	x9, #0x0
   28324:	str	x11, [x10, x9, lsl #3]
   28328:	mov	x9, x12
   2832c:	b.gt	282c0 <__gmpn_divrem_2@@Base+0xb8>
   28330:	b	28370 <__gmpn_divrem_2@@Base+0x168>
   28334:	cmp	x27, x25
   28338:	b.cs	28344 <__gmpn_divrem_2@@Base+0x13c>  // b.hs, b.nlast
   2833c:	cmp	x26, x20
   28340:	b.ls	2831c <__gmpn_divrem_2@@Base+0x114>  // b.plast
   28344:	subs	x12, x27, x25
   28348:	sbc	x26, x26, x20
   2834c:	add	x11, x11, #0x1
   28350:	mov	x27, x12
   28354:	b	2831c <__gmpn_divrem_2@@Base+0x114>
   28358:	mov	x8, x0
   2835c:	subs	x9, x24, #0x3
   28360:	b.ge	282bc <__gmpn_divrem_2@@Base+0xb4>  // b.tcont
   28364:	cmp	x19, #0x1
   28368:	b.lt	2837c <__gmpn_divrem_2@@Base+0x174>  // b.tstop
   2836c:	b	283c0 <__gmpn_divrem_2@@Base+0x1b8>
   28370:	mov	x28, x22
   28374:	cmp	x19, #0x1
   28378:	b.ge	283c0 <__gmpn_divrem_2@@Base+0x1b8>  // b.tcont
   2837c:	stp	x27, x26, [x28]
   28380:	mov	x0, x21
   28384:	ldp	x20, x19, [sp, #80]
   28388:	ldp	x22, x21, [sp, #64]
   2838c:	ldp	x24, x23, [sp, #48]
   28390:	ldp	x26, x25, [sp, #32]
   28394:	ldp	x28, x27, [sp, #16]
   28398:	ldp	x29, x30, [sp], #96
   2839c:	ret
   283a0:	mul	x10, x0, x25
   283a4:	cmp	x9, x20
   283a8:	sub	x11, x0, #0x2
   283ac:	ccmp	x10, x25, #0x2, ls  // ls = plast
   283b0:	csel	x8, x8, x11, cc  // cc = lo, ul, last
   283b4:	subs	x9, x24, #0x3
   283b8:	b.lt	28364 <__gmpn_divrem_2@@Base+0x15c>  // b.tstop
   283bc:	b	282bc <__gmpn_divrem_2@@Base+0xb4>
   283c0:	sub	x9, x23, #0x8
   283c4:	mov	x11, xzr
   283c8:	mul	x12, x26, x8
   283cc:	umulh	x13, x26, x8
   283d0:	adds	x14, x12, x27
   283d4:	adc	x12, x13, x26
   283d8:	msub	x13, x12, x20, x27
   283dc:	subs	x17, x11, x25
   283e0:	sbc	x11, x13, x20
   283e4:	mul	x15, x12, x25
   283e8:	umulh	x16, x25, x12
   283ec:	subs	x13, x17, x15
   283f0:	sbc	x11, x11, x16
   283f4:	cmp	x11, x14
   283f8:	cset	w14, cs  // cs = hs, nlast
   283fc:	csetm	x15, cs  // cs = hs, nlast
   28400:	sub	x12, x12, x14
   28404:	and	x14, x25, x15
   28408:	and	x15, x20, x15
   2840c:	adds	x27, x13, x14
   28410:	adc	x26, x11, x15
   28414:	sub	x10, x19, #0x1
   28418:	cmp	x26, x20
   2841c:	add	x11, x12, #0x1
   28420:	b.cs	2843c <__gmpn_divrem_2@@Base+0x234>  // b.hs, b.nlast
   28424:	add	x12, x10, #0x1
   28428:	cmp	x12, #0x1
   2842c:	str	x11, [x9, x19, lsl #3]
   28430:	mov	x19, x10
   28434:	b.gt	283c4 <__gmpn_divrem_2@@Base+0x1bc>
   28438:	b	2837c <__gmpn_divrem_2@@Base+0x174>
   2843c:	cmp	x27, x25
   28440:	b.cs	2844c <__gmpn_divrem_2@@Base+0x244>  // b.hs, b.nlast
   28444:	cmp	x26, x20
   28448:	b.ls	28424 <__gmpn_divrem_2@@Base+0x21c>  // b.plast
   2844c:	subs	x12, x27, x25
   28450:	sbc	x26, x26, x20
   28454:	add	x11, x11, #0x1
   28458:	mov	x27, x12
   2845c:	b	28424 <__gmpn_divrem_2@@Base+0x21c>

0000000000028460 <__gmpn_fib2_ui@@Base>:
   28460:	stp	x29, x30, [sp, #-96]!
   28464:	stp	x24, x23, [sp, #48]
   28468:	stp	x22, x21, [sp, #64]
   2846c:	stp	x20, x19, [sp, #80]
   28470:	mov	x19, x2
   28474:	mov	x20, x1
   28478:	cmp	x2, #0x5e
   2847c:	mov	x21, x0
   28480:	mov	w24, #0x1                   	// #1
   28484:	str	x27, [sp, #16]
   28488:	stp	x26, x25, [sp, #32]
   2848c:	mov	x29, sp
   28490:	b.cc	284b0 <__gmpn_fib2_ui@@Base+0x50>  // b.lo, b.ul, b.last
   28494:	mov	x8, x19
   28498:	lsr	x9, x8, #1
   2849c:	cmp	x8, #0xbb
   284a0:	lsl	x24, x24, #1
   284a4:	mov	x8, x9
   284a8:	b.hi	28498 <__gmpn_fib2_ui@@Base+0x38>  // b.pmore
   284ac:	b	284b4 <__gmpn_fib2_ui@@Base+0x54>
   284b0:	mov	x9, x19
   284b4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   284b8:	ldr	x8, [x8, #3808]
   284bc:	cmp	x24, #0x1
   284c0:	add	x8, x8, x9, lsl #3
   284c4:	ldp	x9, x8, [x8]
   284c8:	str	x9, [x20]
   284cc:	str	x8, [x21]
   284d0:	b.ne	284dc <__gmpn_fib2_ui@@Base+0x7c>  // b.any
   284d4:	mov	w23, #0x1                   	// #1
   284d8:	b	28634 <__gmpn_fib2_ui@@Base+0x1d4>
   284dc:	lsr	x8, x19, #5
   284e0:	mov	w9, #0x17                  	// #23
   284e4:	mul	x8, x8, x9
   284e8:	lsr	x8, x8, #6
   284ec:	lsl	x9, x8, #3
   284f0:	cmp	x8, #0xfdc
   284f4:	add	x1, x9, #0x20
   284f8:	str	xzr, [x29, #24]
   284fc:	b.hi	28658 <__gmpn_fib2_ui@@Base+0x1f8>  // b.pmore
   28500:	add	x9, x1, #0xf
   28504:	mov	x8, sp
   28508:	and	x9, x9, #0x7ffffffffffffff0
   2850c:	sub	x22, x8, x9
   28510:	mov	sp, x22
   28514:	add	x25, x21, #0x8
   28518:	mov	w23, #0x1                   	// #1
   2851c:	b	28550 <__gmpn_fib2_ui@@Base+0xf0>
   28520:	mov	x0, x21
   28524:	mov	x1, x21
   28528:	mov	x2, x20
   2852c:	mov	x3, x23
   28530:	bl	c2d0 <__gmpn_sub_n@plt>
   28534:	add	x8, x21, x23, lsl #3
   28538:	ldur	x8, [x8, #-8]
   2853c:	cmp	x8, #0x0
   28540:	cset	w8, eq  // eq = none
   28544:	sub	x23, x23, x8
   28548:	cmp	x24, #0x1
   2854c:	b.eq	2862c <__gmpn_fib2_ui@@Base+0x1cc>  // b.none
   28550:	mov	x0, x22
   28554:	mov	x1, x21
   28558:	mov	x2, x23
   2855c:	bl	c8e0 <__gmpn_sqr@plt>
   28560:	mov	x0, x21
   28564:	mov	x1, x20
   28568:	mov	x2, x23
   2856c:	bl	c8e0 <__gmpn_sqr@plt>
   28570:	add	x8, x22, x23, lsl #4
   28574:	ldur	x8, [x8, #-8]
   28578:	lsl	x9, x23, #1
   2857c:	mov	x0, x20
   28580:	mov	x1, x22
   28584:	cmp	x8, #0x0
   28588:	cset	w8, eq  // eq = none
   2858c:	sub	x23, x9, x8
   28590:	mov	x2, x21
   28594:	mov	x3, x23
   28598:	bl	ca70 <__gmpn_add_n@plt>
   2859c:	lsl	x26, x23, #3
   285a0:	str	x0, [x20, x26]
   285a4:	ldr	x8, [x21]
   285a8:	tst	x24, x19
   285ac:	cset	w9, ne  // ne = any
   285b0:	mov	x0, x21
   285b4:	orr	x8, x8, x9, lsl #1
   285b8:	mov	x1, x21
   285bc:	mov	x2, x22
   285c0:	mov	x3, x23
   285c4:	cset	w27, eq  // eq = none
   285c8:	str	x8, [x21]
   285cc:	bl	cbe0 <__gmpn_rsblsh2_n@plt>
   285d0:	str	x0, [x21, x26]
   285d4:	ldr	x8, [x21]
   285d8:	adds	x8, x8, w27, uxtw #1
   285dc:	str	x8, [x21]
   285e0:	b.cc	285f8 <__gmpn_fib2_ui@@Base+0x198>  // b.lo, b.ul, b.last
   285e4:	mov	x8, x25
   285e8:	ldr	x9, [x8]
   285ec:	adds	x9, x9, #0x1
   285f0:	str	x9, [x8], #8
   285f4:	b.cs	285e8 <__gmpn_fib2_ui@@Base+0x188>  // b.hs, b.nlast
   285f8:	ldr	x8, [x21, x23, lsl #3]
   285fc:	lsr	x24, x24, #1
   28600:	cmp	x8, #0x0
   28604:	cinc	x23, x23, ne  // ne = any
   28608:	tst	x24, x19
   2860c:	b.eq	28520 <__gmpn_fib2_ui@@Base+0xc0>  // b.none
   28610:	mov	x0, x20
   28614:	mov	x1, x21
   28618:	mov	x2, x20
   2861c:	mov	x3, x23
   28620:	bl	c2d0 <__gmpn_sub_n@plt>
   28624:	cmp	x24, #0x1
   28628:	b.ne	28550 <__gmpn_fib2_ui@@Base+0xf0>  // b.any
   2862c:	ldr	x0, [x29, #24]
   28630:	cbnz	x0, 28668 <__gmpn_fib2_ui@@Base+0x208>
   28634:	mov	x0, x23
   28638:	mov	sp, x29
   2863c:	ldp	x20, x19, [sp, #80]
   28640:	ldp	x22, x21, [sp, #64]
   28644:	ldp	x24, x23, [sp, #48]
   28648:	ldp	x26, x25, [sp, #32]
   2864c:	ldr	x27, [sp, #16]
   28650:	ldp	x29, x30, [sp], #96
   28654:	ret
   28658:	add	x0, x29, #0x18
   2865c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28660:	mov	x22, x0
   28664:	b	28514 <__gmpn_fib2_ui@@Base+0xb4>
   28668:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2866c:	b	28634 <__gmpn_fib2_ui@@Base+0x1d4>

0000000000028670 <__gmpn_fib2m@@Base>:
   28670:	stp	x29, x30, [sp, #-96]!
   28674:	stp	x28, x27, [sp, #16]
   28678:	stp	x26, x25, [sp, #32]
   2867c:	stp	x24, x23, [sp, #48]
   28680:	stp	x22, x21, [sp, #64]
   28684:	stp	x20, x19, [sp, #80]
   28688:	mov	x29, sp
   2868c:	sub	sp, sp, #0x40
   28690:	mov	x11, #0x2c84                	// #11396
   28694:	movk	x11, #0x2164, lsl #16
   28698:	sub	x10, x3, #0x1
   2869c:	movk	x11, #0x590b, lsl #32
   286a0:	ldr	x8, [x2, x10, lsl #3]
   286a4:	mov	w9, #0x5c                  	// #92
   286a8:	movk	x11, #0x2c8, lsl #48
   286ac:	mul	x9, x5, x9
   286b0:	cmp	x5, x11
   286b4:	csinv	x9, x9, xzr, ls  // ls = plast
   286b8:	clz	x13, x8
   286bc:	clz	x14, x9
   286c0:	mov	x19, x5
   286c4:	mov	x21, x1
   286c8:	subs	w11, w14, w13
   286cc:	mov	x23, x0
   286d0:	stur	x4, [x29, #-56]
   286d4:	stur	x2, [x29, #-32]
   286d8:	b.cs	28710 <__gmpn_fib2m@@Base+0xa0>  // b.hs, b.nlast
   286dc:	subs	x12, x3, #0x2
   286e0:	b.lt	28718 <__gmpn_fib2m@@Base+0xa8>  // b.tstop
   286e4:	ldur	x11, [x29, #-32]
   286e8:	sub	w10, w13, w14
   286ec:	lsl	x8, x8, x10
   286f0:	ldr	x13, [x11, x12, lsl #3]
   286f4:	mov	w11, #0x40                  	// #64
   286f8:	sub	w11, w11, w10
   286fc:	neg	w10, w10
   28700:	lsr	x10, x13, x10
   28704:	orr	x8, x10, x8
   28708:	mov	x10, x12
   2870c:	b	2871c <__gmpn_fib2m@@Base+0xac>
   28710:	lsr	x8, x8, x11
   28714:	b	2871c <__gmpn_fib2m@@Base+0xac>
   28718:	mov	w11, wzr
   2871c:	lsl	x10, x10, #6
   28720:	cmp	x8, x9
   28724:	add	x9, x10, w11, sxtw
   28728:	cset	w10, hi  // hi = pmore
   2872c:	lsr	x24, x8, x10
   28730:	mov	x0, x23
   28734:	mov	x1, x21
   28738:	mov	x2, x24
   2873c:	cinc	x22, x9, hi  // hi = pmore
   28740:	bl	d070 <__gmpn_fib2_ui@plt>
   28744:	mov	x25, x0
   28748:	cmp	x0, x19
   2874c:	b.eq	2877c <__gmpn_fib2m@@Base+0x10c>  // b.none
   28750:	sub	x8, x19, x25
   28754:	lsl	x20, x25, #3
   28758:	lsl	x26, x8, #3
   2875c:	add	x0, x23, x20
   28760:	mov	w1, wzr
   28764:	mov	x2, x26
   28768:	bl	c5f0 <memset@plt>
   2876c:	add	x0, x21, x20
   28770:	mov	w1, wzr
   28774:	mov	x2, x26
   28778:	bl	c5f0 <memset@plt>
   2877c:	cbz	x22, 28a18 <__gmpn_fib2m@@Base+0x3a8>
   28780:	cmp	x19, #0x2
   28784:	cset	w8, lt  // lt = tstop
   28788:	bfi	x8, x19, #1, #63
   2878c:	lsl	x1, x8, #3
   28790:	mov	w8, #0x7f00                	// #32512
   28794:	and	w20, w24, #0x1
   28798:	cmp	x1, x8
   2879c:	lsl	x28, x19, #1
   287a0:	stur	xzr, [x29, #-16]
   287a4:	b.hi	28a88 <__gmpn_fib2m@@Base+0x418>  // b.pmore
   287a8:	add	x9, x1, #0xf
   287ac:	mov	x8, sp
   287b0:	and	x9, x9, #0xfffffffffffffff0
   287b4:	sub	x25, x8, x9
   287b8:	mov	sp, x25
   287bc:	ldur	x24, [x29, #-56]
   287c0:	orr	x8, x28, #0x1
   287c4:	add	x9, x23, #0x8
   287c8:	stur	x8, [x29, #-24]
   287cc:	sub	x8, x8, #0x1
   287d0:	lsl	x27, x28, #3
   287d4:	stur	x9, [x29, #-48]
   287d8:	stur	x8, [x29, #-64]
   287dc:	b	2883c <__gmpn_fib2m@@Base+0x1cc>
   287e0:	mov	x1, x23
   287e4:	mov	x2, x21
   287e8:	bl	c2d0 <__gmpn_sub_n@plt>
   287ec:	stur	wzr, [x29, #-36]
   287f0:	mov	x28, x26
   287f4:	ldur	x26, [x29, #-24]
   287f8:	mov	x0, x25
   287fc:	mov	x1, x23
   28800:	mov	x2, xzr
   28804:	mov	x3, x23
   28808:	mov	x4, x26
   2880c:	mov	x5, x24
   28810:	mov	x6, x19
   28814:	bl	bf00 <__gmpn_tdiv_qr@plt>
   28818:	mov	x0, x25
   2881c:	mov	x1, x21
   28820:	mov	x2, xzr
   28824:	mov	x3, x21
   28828:	mov	x4, x26
   2882c:	mov	x5, x24
   28830:	mov	x6, x19
   28834:	bl	bf00 <__gmpn_tdiv_qr@plt>
   28838:	cbz	x22, 28a08 <__gmpn_fib2m@@Base+0x398>
   2883c:	mov	x0, x25
   28840:	mov	x1, x23
   28844:	mov	x2, x19
   28848:	bl	c8e0 <__gmpn_sqr@plt>
   2884c:	mov	x0, x23
   28850:	mov	x1, x21
   28854:	mov	x2, x19
   28858:	bl	c8e0 <__gmpn_sqr@plt>
   2885c:	mov	x0, x21
   28860:	mov	x1, x25
   28864:	mov	x2, x23
   28868:	mov	x3, x28
   2886c:	bl	ca70 <__gmpn_add_n@plt>
   28870:	str	x0, [x21, x27]
   28874:	ldr	x8, [x23]
   28878:	lsl	w20, w20, #1
   2887c:	mov	x0, x23
   28880:	mov	x1, x23
   28884:	orr	x8, x8, x20
   28888:	mov	x2, x25
   2888c:	mov	x3, x28
   28890:	str	x8, [x23]
   28894:	mov	x26, x28
   28898:	bl	cbe0 <__gmpn_rsblsh2_n@plt>
   2889c:	add	x8, x0, #0x1
   288a0:	str	x8, [x23, x27]
   288a4:	ldr	x8, [x23]
   288a8:	eor	w9, w20, #0x2
   288ac:	adds	x8, x8, x9
   288b0:	str	x8, [x23]
   288b4:	b.cc	288cc <__gmpn_fib2m@@Base+0x25c>  // b.lo, b.ul, b.last
   288b8:	ldur	x8, [x29, #-48]
   288bc:	ldr	x9, [x8]
   288c0:	adds	x9, x9, #0x1
   288c4:	str	x9, [x8], #8
   288c8:	b.cs	288bc <__gmpn_fib2m@@Base+0x24c>  // b.hs, b.nlast
   288cc:	ldr	x8, [x23, x27]
   288d0:	sub	x22, x22, #0x1
   288d4:	lsr	x9, x22, #3
   288d8:	and	x9, x9, #0x1ffffffffffffff8
   288dc:	sub	x10, x8, #0x1
   288e0:	str	x10, [x23, x27]
   288e4:	ldur	x10, [x29, #-32]
   288e8:	ldr	x9, [x10, x9]
   288ec:	lsr	x9, x9, x22
   288f0:	tst	w9, #0x1
   288f4:	and	w20, w9, #0x1
   288f8:	csel	x28, x21, x23, ne  // ne = any
   288fc:	cbz	x8, 28940 <__gmpn_fib2m@@Base+0x2d0>
   28900:	ldur	x8, [x29, #-24]
   28904:	cmp	x8, #0x1
   28908:	b.lt	287ec <__gmpn_fib2m@@Base+0x17c>  // b.tstop
   2890c:	ldur	x8, [x29, #-64]
   28910:	lsl	x9, x8, #3
   28914:	ldr	x10, [x23, x9]
   28918:	ldr	x9, [x21, x9]
   2891c:	cmp	x10, x9
   28920:	b.ne	289d0 <__gmpn_fib2m@@Base+0x360>  // b.any
   28924:	add	x9, x8, #0x1
   28928:	sub	x10, x8, #0x1
   2892c:	cmp	x9, #0x1
   28930:	str	xzr, [x28, x8, lsl #3]
   28934:	mov	x8, x10
   28938:	b.gt	28910 <__gmpn_fib2m@@Base+0x2a0>
   2893c:	b	287ec <__gmpn_fib2m@@Base+0x17c>
   28940:	ldr	x24, [x21, x27]
   28944:	cmp	w20, #0x0
   28948:	cset	w8, eq  // eq = none
   2894c:	mov	x0, x28
   28950:	mov	x1, x21
   28954:	mov	x2, x23
   28958:	mov	x3, x26
   2895c:	stur	w8, [x29, #-36]
   28960:	bl	c2d0 <__gmpn_sub_n@plt>
   28964:	sub	x8, x24, x0
   28968:	add	x8, x8, #0x1
   2896c:	str	x8, [x28, x27]
   28970:	cbz	w20, 289f4 <__gmpn_fib2m@@Base+0x384>
   28974:	ldr	x10, [x23]
   28978:	ldur	x24, [x29, #-56]
   2897c:	mov	x8, x23
   28980:	mov	x9, x26
   28984:	mov	x28, x26
   28988:	cbnz	x10, 289a8 <__gmpn_fib2m@@Base+0x338>
   2898c:	mov	x9, x28
   28990:	mov	x8, x23
   28994:	subs	x9, x9, #0x1
   28998:	str	xzr, [x8]
   2899c:	b.eq	289fc <__gmpn_fib2m@@Base+0x38c>  // b.none
   289a0:	ldr	x10, [x8, #8]!
   289a4:	cbz	x10, 28994 <__gmpn_fib2m@@Base+0x324>
   289a8:	neg	x10, x10
   289ac:	subs	x2, x9, #0x1
   289b0:	str	x10, [x8]
   289b4:	b.eq	289c4 <__gmpn_fib2m@@Base+0x354>  // b.none
   289b8:	add	x0, x8, #0x8
   289bc:	mov	x1, x0
   289c0:	bl	c290 <__gmpn_com@plt>
   289c4:	mov	x8, xzr
   289c8:	str	xzr, [x23, x28, lsl #3]
   289cc:	b	287f4 <__gmpn_fib2m@@Base+0x184>
   289d0:	add	x3, x8, #0x1
   289d4:	mov	x0, x28
   289d8:	b.hi	287e0 <__gmpn_fib2m@@Base+0x170>  // b.pmore
   289dc:	mov	x1, x21
   289e0:	mov	x2, x23
   289e4:	bl	c2d0 <__gmpn_sub_n@plt>
   289e8:	mov	w8, #0x1                   	// #1
   289ec:	stur	w8, [x29, #-36]
   289f0:	b	287f0 <__gmpn_fib2m@@Base+0x180>
   289f4:	ldur	x24, [x29, #-56]
   289f8:	b	287f0 <__gmpn_fib2m@@Base+0x180>
   289fc:	mov	w8, #0x1                   	// #1
   28a00:	str	x8, [x23, x28, lsl #3]
   28a04:	b	287f4 <__gmpn_fib2m@@Base+0x184>
   28a08:	ldur	x0, [x29, #-16]
   28a0c:	cbnz	x0, 28a98 <__gmpn_fib2m@@Base+0x428>
   28a10:	ldur	w0, [x29, #-36]
   28a14:	b	28a68 <__gmpn_fib2m@@Base+0x3f8>
   28a18:	cmp	x25, x19
   28a1c:	b.ne	28a64 <__gmpn_fib2m@@Base+0x3f4>  // b.any
   28a20:	ldur	x20, [x29, #-56]
   28a24:	sub	x0, x29, #0x10
   28a28:	mov	x1, x23
   28a2c:	mov	x2, xzr
   28a30:	mov	x3, x23
   28a34:	mov	x4, x19
   28a38:	mov	x5, x20
   28a3c:	mov	x6, x19
   28a40:	bl	bf00 <__gmpn_tdiv_qr@plt>
   28a44:	sub	x0, x29, #0x10
   28a48:	mov	x1, x21
   28a4c:	mov	x2, xzr
   28a50:	mov	x3, x21
   28a54:	mov	x4, x19
   28a58:	mov	x5, x20
   28a5c:	mov	x6, x19
   28a60:	bl	bf00 <__gmpn_tdiv_qr@plt>
   28a64:	mov	w0, wzr
   28a68:	mov	sp, x29
   28a6c:	ldp	x20, x19, [sp, #80]
   28a70:	ldp	x22, x21, [sp, #64]
   28a74:	ldp	x24, x23, [sp, #48]
   28a78:	ldp	x26, x25, [sp, #32]
   28a7c:	ldp	x28, x27, [sp, #16]
   28a80:	ldp	x29, x30, [sp], #96
   28a84:	ret
   28a88:	sub	x0, x29, #0x10
   28a8c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   28a90:	mov	x25, x0
   28a94:	b	287bc <__gmpn_fib2m@@Base+0x14c>
   28a98:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   28a9c:	b	28a10 <__gmpn_fib2m@@Base+0x3a0>

0000000000028aa0 <__gmpn_mod_1@@Base>:
   28aa0:	sub	sp, sp, #0x90
   28aa4:	stp	x29, x30, [sp, #64]
   28aa8:	str	x25, [sp, #80]
   28aac:	stp	x24, x23, [sp, #96]
   28ab0:	stp	x22, x21, [sp, #112]
   28ab4:	stp	x20, x19, [sp, #128]
   28ab8:	add	x29, sp, #0x40
   28abc:	cbz	x1, 28b18 <__gmpn_mod_1@@Base+0x78>
   28ac0:	mov	x20, x2
   28ac4:	mov	x19, x1
   28ac8:	mov	x21, x0
   28acc:	tbnz	x2, #63, 28d84 <__gmpn_mod_1@@Base+0x2e4>
   28ad0:	cmp	x19, #0x5
   28ad4:	b.le	28b20 <__gmpn_mod_1@@Base+0x80>
   28ad8:	cmp	x19, #0x9
   28adc:	b.le	28b44 <__gmpn_mod_1@@Base+0xa4>
   28ae0:	cmp	x19, #0x14
   28ae4:	b.lt	28d44 <__gmpn_mod_1@@Base+0x2a4>  // b.tstop
   28ae8:	lsr	x8, x20, #62
   28aec:	cbnz	x8, 28d44 <__gmpn_mod_1@@Base+0x2a4>
   28af0:	add	x0, sp, #0x8
   28af4:	mov	x1, x20
   28af8:	bl	c680 <__gmpn_mod_1s_4p_cps@plt>
   28afc:	ldr	x8, [sp, #16]
   28b00:	add	x3, sp, #0x8
   28b04:	mov	x0, x21
   28b08:	mov	x1, x19
   28b0c:	lsl	x2, x20, x8
   28b10:	bl	d410 <__gmpn_mod_1s_4p@plt>
   28b14:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28b18:	mov	x0, xzr
   28b1c:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28b20:	sub	x8, x19, #0x1
   28b24:	ldr	x0, [x21, x8, lsl #3]
   28b28:	cmp	x0, x20
   28b2c:	b.cs	28b6c <__gmpn_mod_1@@Base+0xcc>  // b.hs, b.nlast
   28b30:	cbz	x8, 28d68 <__gmpn_mod_1@@Base+0x2c8>
   28b34:	add	x9, x21, x19, lsl #3
   28b38:	ldur	x23, [x9, #-16]
   28b3c:	mov	x19, x8
   28b40:	b	28b74 <__gmpn_mod_1@@Base+0xd4>
   28b44:	add	x0, sp, #0x8
   28b48:	mov	x1, x20
   28b4c:	bl	cac0 <__gmpn_mod_1_1p_cps@plt>
   28b50:	ldr	x8, [sp, #16]
   28b54:	add	x3, sp, #0x8
   28b58:	mov	x0, x21
   28b5c:	mov	x1, x19
   28b60:	lsl	x2, x20, x8
   28b64:	bl	c250 <__gmpn_mod_1_1p@plt>
   28b68:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28b6c:	mov	x23, x0
   28b70:	mov	x0, xzr
   28b74:	clz	x22, x20
   28b78:	mov	w8, #0x40                  	// #64
   28b7c:	sub	x24, x8, x22
   28b80:	neg	x8, x22
   28b84:	lsl	x9, x0, x22
   28b88:	lsr	x8, x23, x8
   28b8c:	lsl	x20, x20, x22
   28b90:	cmp	x19, #0x3
   28b94:	orr	x25, x9, x8
   28b98:	b.le	28c38 <__gmpn_mod_1@@Base+0x198>
   28b9c:	mov	x0, x20
   28ba0:	bl	d3f0 <__gmpn_invert_limb@plt>
   28ba4:	sub	x8, x21, #0x10
   28ba8:	ldr	x9, [x8, x19, lsl #3]
   28bac:	lsl	x10, x23, x22
   28bb0:	umulh	x11, x25, x0
   28bb4:	add	x13, x25, #0x1
   28bb8:	lsr	x12, x9, x24
   28bbc:	orr	x10, x12, x10
   28bc0:	mul	x12, x25, x0
   28bc4:	adds	x14, x12, x10
   28bc8:	adc	x11, x11, x13
   28bcc:	msub	x10, x11, x20, x10
   28bd0:	cmp	x10, x14
   28bd4:	csel	x11, x20, xzr, hi  // hi = pmore
   28bd8:	add	x10, x11, x10
   28bdc:	cmp	x10, x20
   28be0:	sub	x12, x19, #0x2
   28be4:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   28be8:	sub	x19, x19, #0x1
   28bec:	cmp	x12, #0x0
   28bf0:	sub	x25, x10, x11
   28bf4:	mov	x23, x9
   28bf8:	b.gt	28ba8 <__gmpn_mod_1@@Base+0x108>
   28bfc:	umulh	x8, x25, x0
   28c00:	mul	x10, x25, x0
   28c04:	lsl	x9, x9, x22
   28c08:	add	x11, x25, #0x1
   28c0c:	adds	x12, x10, x9
   28c10:	adc	x8, x8, x11
   28c14:	msub	x8, x8, x20, x9
   28c18:	cmp	x8, x12
   28c1c:	csel	x9, x20, xzr, hi  // hi = pmore
   28c20:	add	x8, x9, x8
   28c24:	cmp	x8, x20
   28c28:	csel	x9, xzr, x20, cc  // cc = lo, ul, last
   28c2c:	sub	x8, x8, x9
   28c30:	lsr	x0, x8, x22
   28c34:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28c38:	lsr	x8, x20, #32
   28c3c:	and	x9, x20, #0xffffffff
   28c40:	cmp	x19, #0x1
   28c44:	b.le	28cd8 <__gmpn_mod_1@@Base+0x238>
   28c48:	sub	x10, x21, #0x10
   28c4c:	b	28c64 <__gmpn_mod_1@@Base+0x1c4>
   28c50:	sub	x13, x19, #0x2
   28c54:	sub	x19, x19, #0x1
   28c58:	cmp	x13, #0x0
   28c5c:	sub	x25, x11, x12
   28c60:	b.le	28cd8 <__gmpn_mod_1@@Base+0x238>
   28c64:	mov	x11, x23
   28c68:	ldr	x23, [x10, x19, lsl #3]
   28c6c:	lsl	x11, x11, x22
   28c70:	udiv	x12, x25, x8
   28c74:	msub	w13, w12, w8, w25
   28c78:	lsr	x14, x23, x24
   28c7c:	orr	x11, x14, x11
   28c80:	mul	x12, x12, x9
   28c84:	extr	x13, x13, x11, #32
   28c88:	cmp	x13, x12
   28c8c:	b.cs	28ca4 <__gmpn_mod_1@@Base+0x204>  // b.hs, b.nlast
   28c90:	add	x13, x13, x20
   28c94:	cmp	x13, x12
   28c98:	ccmp	x13, x20, #0x0, cc  // cc = lo, ul, last
   28c9c:	csel	x14, x20, xzr, cs  // cs = hs, nlast
   28ca0:	add	x13, x14, x13
   28ca4:	sub	x12, x13, x12
   28ca8:	udiv	x13, x12, x8
   28cac:	msub	w14, w13, w8, w12
   28cb0:	mul	x12, x13, x9
   28cb4:	bfi	x11, x14, #32, #32
   28cb8:	cmp	x11, x12
   28cbc:	b.cs	28c50 <__gmpn_mod_1@@Base+0x1b0>  // b.hs, b.nlast
   28cc0:	add	x11, x11, x20
   28cc4:	cmp	x11, x12
   28cc8:	ccmp	x11, x20, #0x0, cc  // cc = lo, ul, last
   28ccc:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   28cd0:	add	x11, x13, x11
   28cd4:	b	28c50 <__gmpn_mod_1@@Base+0x1b0>
   28cd8:	udiv	x10, x25, x8
   28cdc:	msub	w12, w10, w8, w25
   28ce0:	mul	x11, x10, x9
   28ce4:	lsl	x10, x23, x22
   28ce8:	extr	x12, x12, x10, #32
   28cec:	cmp	x12, x11
   28cf0:	b.cs	28d08 <__gmpn_mod_1@@Base+0x268>  // b.hs, b.nlast
   28cf4:	add	x12, x12, x20
   28cf8:	cmp	x12, x11
   28cfc:	ccmp	x12, x20, #0x0, cc  // cc = lo, ul, last
   28d00:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   28d04:	add	x12, x13, x12
   28d08:	sub	x11, x12, x11
   28d0c:	udiv	x12, x11, x8
   28d10:	msub	w11, w12, w8, w11
   28d14:	mul	x8, x12, x9
   28d18:	bfi	x10, x11, #32, #32
   28d1c:	cmp	x10, x8
   28d20:	b.cs	28d38 <__gmpn_mod_1@@Base+0x298>  // b.hs, b.nlast
   28d24:	add	x9, x10, x20
   28d28:	cmp	x9, x8
   28d2c:	ccmp	x9, x20, #0x0, cc  // cc = lo, ul, last
   28d30:	csel	x10, x20, xzr, cs  // cs = hs, nlast
   28d34:	add	x10, x10, x9
   28d38:	sub	x8, x10, x8
   28d3c:	lsr	x0, x8, x22
   28d40:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28d44:	add	x0, sp, #0x8
   28d48:	mov	x1, x20
   28d4c:	bl	ccc0 <__gmpn_mod_1s_2p_cps@plt>
   28d50:	ldr	x8, [sp, #16]
   28d54:	add	x3, sp, #0x8
   28d58:	mov	x0, x21
   28d5c:	mov	x1, x19
   28d60:	lsl	x2, x20, x8
   28d64:	bl	ce80 <__gmpn_mod_1s_2p@plt>
   28d68:	ldp	x20, x19, [sp, #128]
   28d6c:	ldp	x22, x21, [sp, #112]
   28d70:	ldp	x24, x23, [sp, #96]
   28d74:	ldr	x25, [sp, #80]
   28d78:	ldp	x29, x30, [sp, #64]
   28d7c:	add	sp, sp, #0x90
   28d80:	ret
   28d84:	cmp	x19, #0x7
   28d88:	b.le	28db0 <__gmpn_mod_1@@Base+0x310>
   28d8c:	add	x0, sp, #0x8
   28d90:	mov	x1, x20
   28d94:	bl	cac0 <__gmpn_mod_1_1p_cps@plt>
   28d98:	add	x3, sp, #0x8
   28d9c:	mov	x0, x21
   28da0:	mov	x1, x19
   28da4:	mov	x2, x20
   28da8:	bl	c250 <__gmpn_mod_1_1p@plt>
   28dac:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28db0:	add	x8, x21, x19, lsl #3
   28db4:	ldur	x8, [x8, #-8]
   28db8:	cmp	x8, x20
   28dbc:	csel	x9, xzr, x20, cc  // cc = lo, ul, last
   28dc0:	cmp	x19, #0x1
   28dc4:	sub	x0, x8, x9
   28dc8:	b.eq	28d68 <__gmpn_mod_1@@Base+0x2c8>  // b.none
   28dcc:	mov	x22, x0
   28dd0:	cmp	x19, #0x3
   28dd4:	b.le	28e34 <__gmpn_mod_1@@Base+0x394>
   28dd8:	mov	x0, x20
   28ddc:	bl	d3f0 <__gmpn_invert_limb@plt>
   28de0:	mov	x8, x0
   28de4:	sub	x9, x21, #0x10
   28de8:	mov	x0, x22
   28dec:	ldr	x10, [x9, x19, lsl #3]
   28df0:	umulh	x11, x0, x8
   28df4:	mul	x12, x0, x8
   28df8:	add	x13, x0, #0x1
   28dfc:	adds	x14, x12, x10
   28e00:	adc	x11, x11, x13
   28e04:	msub	x10, x11, x20, x10
   28e08:	cmp	x10, x14
   28e0c:	csel	x11, x20, xzr, hi  // hi = pmore
   28e10:	add	x10, x11, x10
   28e14:	cmp	x10, x20
   28e18:	sub	x12, x19, #0x2
   28e1c:	csel	x11, xzr, x20, cc  // cc = lo, ul, last
   28e20:	sub	x19, x19, #0x1
   28e24:	cmp	x12, #0x0
   28e28:	sub	x0, x10, x11
   28e2c:	b.gt	28dec <__gmpn_mod_1@@Base+0x34c>
   28e30:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28e34:	cmp	x19, #0x2
   28e38:	b.lt	28ec8 <__gmpn_mod_1@@Base+0x428>  // b.tstop
   28e3c:	lsr	x8, x20, #32
   28e40:	and	x9, x20, #0xffffffff
   28e44:	sub	x10, x21, #0x10
   28e48:	mov	x0, x22
   28e4c:	b	28e64 <__gmpn_mod_1@@Base+0x3c4>
   28e50:	sub	x13, x19, #0x2
   28e54:	sub	x19, x19, #0x1
   28e58:	cmp	x13, #0x0
   28e5c:	sub	x0, x11, x12
   28e60:	b.le	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28e64:	ldr	x11, [x10, x19, lsl #3]
   28e68:	udiv	x12, x0, x8
   28e6c:	msub	w13, w12, w8, w0
   28e70:	mul	x12, x12, x9
   28e74:	extr	x13, x13, x11, #32
   28e78:	cmp	x13, x12
   28e7c:	b.cs	28e94 <__gmpn_mod_1@@Base+0x3f4>  // b.hs, b.nlast
   28e80:	add	x13, x13, x20
   28e84:	cmp	x13, x12
   28e88:	ccmp	x13, x20, #0x0, cc  // cc = lo, ul, last
   28e8c:	csel	x14, x20, xzr, cs  // cs = hs, nlast
   28e90:	add	x13, x14, x13
   28e94:	sub	x12, x13, x12
   28e98:	udiv	x13, x12, x8
   28e9c:	msub	w14, w13, w8, w12
   28ea0:	mul	x12, x13, x9
   28ea4:	bfi	x11, x14, #32, #32
   28ea8:	cmp	x11, x12
   28eac:	b.cs	28e50 <__gmpn_mod_1@@Base+0x3b0>  // b.hs, b.nlast
   28eb0:	add	x11, x11, x20
   28eb4:	cmp	x11, x12
   28eb8:	ccmp	x11, x20, #0x0, cc  // cc = lo, ul, last
   28ebc:	csel	x13, x20, xzr, cs  // cs = hs, nlast
   28ec0:	add	x11, x13, x11
   28ec4:	b	28e50 <__gmpn_mod_1@@Base+0x3b0>
   28ec8:	mov	x0, x22
   28ecc:	b	28d68 <__gmpn_mod_1@@Base+0x2c8>
   28ed0:	nop
   28ed4:	nop
   28ed8:	nop
   28edc:	nop

0000000000028ee0 <__gmpn_mod_34lsub1@@Base>:
   28ee0:	subs	x1, x1, #0x3
   28ee4:	mov	x8, #0x0                   	// #0
   28ee8:	b.lt	28f84 <__gmpn_mod_34lsub1@@Base+0xa4>  // b.tstop
   28eec:	ldp	x2, x3, [x0]
   28ef0:	ldr	x4, [x0, #16]
   28ef4:	add	x0, x0, #0x18
   28ef8:	subs	x1, x1, #0x3
   28efc:	b.lt	28f28 <__gmpn_mod_34lsub1@@Base+0x48>  // b.tstop
   28f00:	cmn	x0, #0x0
   28f04:	ldp	x5, x6, [x0]
   28f08:	ldr	x7, [x0, #16]
   28f0c:	add	x0, x0, #0x18
   28f10:	sub	x1, x1, #0x3
   28f14:	adcs	x2, x2, x5
   28f18:	adcs	x3, x3, x6
   28f1c:	adcs	x4, x4, x7
   28f20:	tbz	x1, #63, 28f04 <__gmpn_mod_34lsub1@@Base+0x24>
   28f24:	adc	x8, xzr, xzr
   28f28:	cmn	x1, #0x2
   28f2c:	mov	x5, #0x0                   	// #0
   28f30:	b.cc	28f38 <__gmpn_mod_34lsub1@@Base+0x58>  // b.lo, b.ul, b.last
   28f34:	ldr	x5, [x0], #8
   28f38:	mov	x6, #0x0                   	// #0
   28f3c:	b.ls	28f44 <__gmpn_mod_34lsub1@@Base+0x64>  // b.plast
   28f40:	ldr	x6, [x0], #8
   28f44:	adds	x2, x2, x5
   28f48:	adcs	x3, x3, x6
   28f4c:	adcs	x4, x4, xzr
   28f50:	adc	x8, x8, xzr
   28f54:	and	x0, x2, #0xffffffffffff
   28f58:	add	x0, x0, x2, lsr #48
   28f5c:	add	x0, x0, x8
   28f60:	lsl	x8, x3, #16
   28f64:	and	x1, x8, #0xffffffffffff
   28f68:	add	x0, x0, x1
   28f6c:	add	x0, x0, x3, lsr #32
   28f70:	lsl	x8, x4, #32
   28f74:	and	x1, x8, #0xffffffffffff
   28f78:	add	x0, x0, x1
   28f7c:	add	x0, x0, x4, lsr #16
   28f80:	ret
   28f84:	cmn	x1, #0x1
   28f88:	b.ne	28f98 <__gmpn_mod_34lsub1@@Base+0xb8>  // b.any
   28f8c:	ldp	x2, x3, [x0]
   28f90:	mov	x4, #0x0                   	// #0
   28f94:	b	28f54 <__gmpn_mod_34lsub1@@Base+0x74>
   28f98:	ldr	x2, [x0]
   28f9c:	and	x0, x2, #0xffffffffffff
   28fa0:	add	x0, x0, x2, lsr #48
   28fa4:	ret

0000000000028fa8 <__gmpn_modexact_1c_odd@@Base>:
   28fa8:	subs	x8, x1, #0x1
   28fac:	b.ne	28fd4 <__gmpn_modexact_1c_odd@@Base+0x2c>  // b.any
   28fb0:	ldr	x8, [x0]
   28fb4:	subs	x9, x8, x3
   28fb8:	b.ls	29044 <__gmpn_modexact_1c_odd@@Base+0x9c>  // b.plast
   28fbc:	udiv	x8, x9, x2
   28fc0:	msub	x8, x8, x2, x9
   28fc4:	sub	x9, x2, x8
   28fc8:	cmp	x8, #0x0
   28fcc:	csel	x0, xzr, x9, eq  // eq = none
   28fd0:	ret
   28fd4:	adrp	x11, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   28fd8:	ldr	x11, [x11, #3952]
   28fdc:	ubfx	x10, x2, #1, #7
   28fe0:	mov	x9, xzr
   28fe4:	ldrb	w10, [x11, x10]
   28fe8:	mov	w11, #0x2                   	// #2
   28fec:	msub	x12, x10, x2, x11
   28ff0:	mul	x10, x12, x10
   28ff4:	msub	x12, x10, x2, x11
   28ff8:	mul	x10, x10, x12
   28ffc:	msub	x11, x10, x2, x11
   29000:	mul	x10, x10, x11
   29004:	ldr	x11, [x0, x9, lsl #3]
   29008:	add	x9, x9, #0x1
   2900c:	subs	x11, x11, x3
   29010:	mul	x11, x11, x10
   29014:	umulh	x11, x11, x2
   29018:	cinc	x3, x11, cc  // cc = lo, ul, last
   2901c:	cmp	x9, x8
   29020:	b.lt	29004 <__gmpn_modexact_1c_odd@@Base+0x5c>  // b.tstop
   29024:	ldr	x8, [x0, x9, lsl #3]
   29028:	cmp	x8, x2
   2902c:	b.ls	29054 <__gmpn_modexact_1c_odd@@Base+0xac>  // b.plast
   29030:	subs	x8, x8, x3
   29034:	mul	x8, x8, x10
   29038:	umulh	x8, x8, x2
   2903c:	cinc	x0, x8, cc  // cc = lo, ul, last
   29040:	ret
   29044:	sub	x8, x3, x8
   29048:	udiv	x9, x8, x2
   2904c:	msub	x0, x9, x2, x8
   29050:	ret
   29054:	subs	x8, x3, x8
   29058:	csel	x9, x2, xzr, cc  // cc = lo, ul, last
   2905c:	add	x0, x8, x9
   29060:	ret

0000000000029064 <__gmpn_preinv_divrem_1@@Base>:
   29064:	sub	x13, x3, #0x1
   29068:	ldr	x12, [x2, x13, lsl #3]
   2906c:	add	x10, x13, x1
   29070:	mov	w8, w6
   29074:	lsl	x9, x4, x6
   29078:	add	x10, x0, x10, lsl #3
   2907c:	cbz	w6, 290a4 <__gmpn_preinv_divrem_1@@Base+0x40>
   29080:	cmp	x12, x4
   29084:	b.cs	2911c <__gmpn_preinv_divrem_1@@Base+0xb8>  // b.hs, b.nlast
   29088:	lsl	x11, x12, x8
   2908c:	str	xzr, [x10], #-8
   29090:	cbz	x13, 291f0 <__gmpn_preinv_divrem_1@@Base+0x18c>
   29094:	add	x12, x2, x3, lsl #3
   29098:	ldur	x12, [x12, #-16]
   2909c:	mov	x3, x13
   290a0:	b	29120 <__gmpn_preinv_divrem_1@@Base+0xbc>
   290a4:	cmp	x12, x9
   290a8:	cset	w13, cs  // cs = hs, nlast
   290ac:	csel	x11, x9, xzr, cs  // cs = hs, nlast
   290b0:	cmp	x3, #0x1
   290b4:	sub	x11, x12, x11
   290b8:	str	x13, [x10], #-8
   290bc:	b.le	291f0 <__gmpn_preinv_divrem_1@@Base+0x18c>
   290c0:	sub	x12, x2, #0x10
   290c4:	ldr	x13, [x12, x3, lsl #3]
   290c8:	umulh	x14, x11, x5
   290cc:	mul	x15, x11, x5
   290d0:	add	x11, x11, #0x1
   290d4:	adds	x16, x15, x13
   290d8:	adc	x11, x14, x11
   290dc:	msub	x13, x11, x9, x13
   290e0:	cmp	x13, x16
   290e4:	cset	w15, hi  // hi = pmore
   290e8:	sub	x11, x11, x15
   290ec:	csel	x15, x9, xzr, hi  // hi = pmore
   290f0:	add	x13, x15, x13
   290f4:	cmp	x13, x9
   290f8:	sub	x14, x3, #0x2
   290fc:	csel	x15, xzr, x9, cc  // cc = lo, ul, last
   29100:	cinc	x16, x11, cs  // cs = hs, nlast
   29104:	sub	x3, x3, #0x1
   29108:	cmp	x14, #0x0
   2910c:	sub	x11, x13, x15
   29110:	str	x16, [x10], #-8
   29114:	b.gt	290c4 <__gmpn_preinv_divrem_1@@Base+0x60>
   29118:	b	291f0 <__gmpn_preinv_divrem_1@@Base+0x18c>
   2911c:	mov	x11, xzr
   29120:	neg	w13, w6
   29124:	lsr	x13, x12, x13
   29128:	cmp	x3, #0x2
   2912c:	orr	x15, x13, x11
   29130:	b.lt	291a8 <__gmpn_preinv_divrem_1@@Base+0x144>  // b.tstop
   29134:	mov	w11, #0x40                  	// #64
   29138:	sub	w11, w11, w6
   2913c:	sub	x13, x2, #0x10
   29140:	ldr	x14, [x13, x3, lsl #3]
   29144:	lsl	x12, x12, x8
   29148:	umulh	x16, x15, x5
   2914c:	mul	x17, x15, x5
   29150:	lsr	x18, x14, x11
   29154:	add	x15, x15, #0x1
   29158:	orr	x12, x18, x12
   2915c:	adds	x0, x17, x12
   29160:	adc	x15, x16, x15
   29164:	msub	x12, x15, x9, x12
   29168:	cmp	x12, x0
   2916c:	csel	x17, x9, xzr, hi  // hi = pmore
   29170:	cset	w16, hi  // hi = pmore
   29174:	add	x12, x17, x12
   29178:	sub	x15, x15, x16
   2917c:	cmp	x12, x9
   29180:	sub	x18, x3, #0x2
   29184:	cinc	x16, x15, cs  // cs = hs, nlast
   29188:	csel	x15, xzr, x9, cc  // cc = lo, ul, last
   2918c:	sub	x3, x3, #0x1
   29190:	cmp	x18, #0x0
   29194:	sub	x15, x12, x15
   29198:	str	x16, [x10], #-8
   2919c:	mov	x12, x14
   291a0:	b.gt	29140 <__gmpn_preinv_divrem_1@@Base+0xdc>
   291a4:	b	291ac <__gmpn_preinv_divrem_1@@Base+0x148>
   291a8:	mov	x14, x12
   291ac:	umulh	x11, x15, x5
   291b0:	mul	x12, x15, x5
   291b4:	lsl	x13, x14, x8
   291b8:	add	x14, x15, #0x1
   291bc:	adds	x15, x12, x13
   291c0:	adc	x11, x11, x14
   291c4:	msub	x12, x11, x9, x13
   291c8:	cmp	x12, x15
   291cc:	csel	x14, x9, xzr, hi  // hi = pmore
   291d0:	cset	w13, hi  // hi = pmore
   291d4:	add	x12, x14, x12
   291d8:	sub	x11, x11, x13
   291dc:	cmp	x12, x9
   291e0:	cinc	x13, x11, cs  // cs = hs, nlast
   291e4:	csel	x11, xzr, x9, cc  // cc = lo, ul, last
   291e8:	sub	x11, x12, x11
   291ec:	str	x13, [x10], #-8
   291f0:	cmp	x1, #0x1
   291f4:	b.lt	2922c <__gmpn_preinv_divrem_1@@Base+0x1c8>  // b.tstop
   291f8:	umulh	x12, x11, x5
   291fc:	mul	x13, x11, x5
   29200:	add	x11, x11, x12
   29204:	add	x11, x11, #0x1
   29208:	mneg	x12, x11, x9
   2920c:	cmp	x13, x12
   29210:	cset	w12, cc  // cc = lo, ul, last
   29214:	csel	x13, x9, xzr, cc  // cc = lo, ul, last
   29218:	sub	x12, x11, x12
   2921c:	msub	x11, x11, x9, x13
   29220:	subs	x1, x1, #0x1
   29224:	str	x12, [x10], #-8
   29228:	b.ne	291f8 <__gmpn_preinv_divrem_1@@Base+0x194>  // b.any
   2922c:	lsr	x0, x11, x8
   29230:	ret

0000000000029234 <__gmpn_preinv_mod_1@@Base>:
   29234:	add	x9, x0, x1, lsl #3
   29238:	ldur	x9, [x9, #-8]
   2923c:	mov	x8, x0
   29240:	cmp	x9, x2
   29244:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29248:	cmp	x1, #0x2
   2924c:	sub	x0, x9, x10
   29250:	b.lt	2929c <__gmpn_preinv_mod_1@@Base+0x68>  // b.tstop
   29254:	sub	x8, x8, #0x10
   29258:	ldr	x9, [x8, x1, lsl #3]
   2925c:	umulh	x10, x0, x3
   29260:	mul	x11, x0, x3
   29264:	add	x12, x0, #0x1
   29268:	adds	x13, x11, x9
   2926c:	adc	x10, x10, x12
   29270:	msub	x9, x10, x2, x9
   29274:	cmp	x9, x13
   29278:	csel	x10, x2, xzr, hi  // hi = pmore
   2927c:	add	x9, x10, x9
   29280:	cmp	x9, x2
   29284:	sub	x11, x1, #0x2
   29288:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   2928c:	sub	x1, x1, #0x1
   29290:	cmp	x11, #0x0
   29294:	sub	x0, x9, x10
   29298:	b.gt	29258 <__gmpn_preinv_mod_1@@Base+0x24>
   2929c:	ret

00000000000292a0 <__gmpn_dump@@Base>:
   292a0:	stp	x29, x30, [sp, #-48]!
   292a4:	stp	x20, x19, [sp, #32]
   292a8:	sub	x20, x0, #0x8
   292ac:	stp	x22, x21, [sp, #16]
   292b0:	mov	x29, sp
   292b4:	subs	x21, x1, #0x1
   292b8:	b.lt	292e0 <__gmpn_dump@@Base+0x40>  // b.tstop
   292bc:	ldr	x8, [x20, x1, lsl #3]
   292c0:	mov	x1, x21
   292c4:	cbz	x8, 292b4 <__gmpn_dump@@Base+0x14>
   292c8:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   292cc:	add	x0, x0, #0x1b0
   292d0:	mov	x1, x8
   292d4:	bl	d2e0 <printf@plt>
   292d8:	cbnz	x21, 292f8 <__gmpn_dump@@Base+0x58>
   292dc:	b	2931c <__gmpn_dump@@Base+0x7c>
   292e0:	cbz	x1, 29330 <__gmpn_dump@@Base+0x90>
   292e4:	add	x8, x0, x1, lsl #3
   292e8:	ldur	x1, [x8, #-8]
   292ec:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   292f0:	add	x0, x0, #0x1b0
   292f4:	bl	d2e0 <printf@plt>
   292f8:	adrp	x19, 5b000 <__gmpn_bases@@Base+0x2678>
   292fc:	add	x19, x19, #0x1b4
   29300:	ldr	x2, [x20, x21, lsl #3]
   29304:	mov	w1, #0x10                  	// #16
   29308:	mov	x0, x19
   2930c:	sub	x22, x21, #0x1
   29310:	bl	d2e0 <printf@plt>
   29314:	mov	x21, x22
   29318:	cbnz	x22, 29300 <__gmpn_dump@@Base+0x60>
   2931c:	ldp	x20, x19, [sp, #32]
   29320:	ldp	x22, x21, [sp, #16]
   29324:	mov	w0, #0xa                   	// #10
   29328:	ldp	x29, x30, [sp], #48
   2932c:	b	d310 <putchar@plt>
   29330:	ldp	x20, x19, [sp, #32]
   29334:	ldp	x22, x21, [sp, #16]
   29338:	adrp	x0, 63000 <__gmp_jacobi_table@@Base+0x7599>
   2933c:	add	x0, x0, #0xc87
   29340:	ldp	x29, x30, [sp], #48
   29344:	b	c9b0 <puts@plt>

0000000000029348 <__gmpn_mod_1_1p_cps@@Base>:
   29348:	stp	x29, x30, [sp, #-48]!
   2934c:	str	x21, [sp, #16]
   29350:	clz	x21, x1
   29354:	stp	x20, x19, [sp, #32]
   29358:	lsl	x20, x1, x21
   2935c:	mov	x19, x0
   29360:	mov	x0, x20
   29364:	mov	x29, sp
   29368:	bl	d3f0 <__gmpn_invert_limb@plt>
   2936c:	stp	x0, x21, [x19]
   29370:	cbz	x21, 29394 <__gmpn_mod_1_1p_cps@@Base+0x4c>
   29374:	neg	x8, x21
   29378:	mov	w9, #0x1                   	// #1
   2937c:	lsr	x8, x0, x8
   29380:	lsl	x9, x9, x21
   29384:	orr	x8, x8, x9
   29388:	mneg	x8, x20, x8
   2938c:	lsr	x8, x8, x21
   29390:	str	x8, [x19, #16]
   29394:	mneg	x8, x20, x0
   29398:	str	x8, [x19, #24]
   2939c:	ldp	x20, x19, [sp, #32]
   293a0:	ldr	x21, [sp, #16]
   293a4:	ldp	x29, x30, [sp], #48
   293a8:	ret

00000000000293ac <__gmpn_mod_1_1p@@Base>:
   293ac:	add	x10, x0, x1, lsl #3
   293b0:	ldp	x9, x11, [x10, #-16]
   293b4:	cmp	x1, #0x3
   293b8:	b.lt	29458 <__gmpn_mod_1_1p@@Base+0xac>  // b.tstop
   293bc:	ldr	x8, [x3, #24]
   293c0:	ldur	x10, [x10, #-24]
   293c4:	umulh	x13, x11, x8
   293c8:	mov	w12, #0x2                   	// #2
   293cc:	mul	x11, x8, x11
   293d0:	adds	x10, x10, x11
   293d4:	cset	w11, cs  // cs = hs, nlast
   293d8:	adds	x9, x13, x9
   293dc:	cset	w13, cs  // cs = hs, nlast
   293e0:	csinc	x12, x12, xzr, cs  // cs = hs, nlast
   293e4:	adds	x9, x9, x11
   293e8:	csel	x11, x13, x12, cc  // cc = lo, ul, last
   293ec:	cmp	x1, #0x3
   293f0:	neg	x13, x11
   293f4:	b.eq	2944c <__gmpn_mod_1_1p@@Base+0xa0>  // b.none
   293f8:	sub	x11, x0, #0x20
   293fc:	mov	w12, #0x2                   	// #2
   29400:	ldr	x15, [x11, x1, lsl #3]
   29404:	and	x13, x13, x8
   29408:	adds	x10, x10, x13
   2940c:	umulh	x14, x9, x8
   29410:	mul	x9, x9, x8
   29414:	csel	x13, x2, xzr, cs  // cs = hs, nlast
   29418:	sub	x13, x10, x13
   2941c:	adds	x10, x15, x9
   29420:	cset	w9, cs  // cs = hs, nlast
   29424:	adds	x13, x14, x13
   29428:	cset	w14, cs  // cs = hs, nlast
   2942c:	csinc	x15, x12, xzr, cs  // cs = hs, nlast
   29430:	adds	x9, x13, x9
   29434:	sub	x13, x1, #0x4
   29438:	csel	x14, x14, x15, cc  // cc = lo, ul, last
   2943c:	sub	x1, x1, #0x1
   29440:	cmp	x13, #0x0
   29444:	neg	x13, x14
   29448:	b.gt	29400 <__gmpn_mod_1_1p@@Base+0x54>
   2944c:	and	x8, x13, x2
   29450:	sub	x11, x9, x8
   29454:	mov	x9, x10
   29458:	ldr	x8, [x3, #8]
   2945c:	cbz	w8, 294c4 <__gmpn_mod_1_1p@@Base+0x118>
   29460:	ldr	x10, [x3, #16]
   29464:	umulh	x13, x11, x10
   29468:	neg	w12, w8
   2946c:	mul	x10, x10, x11
   29470:	adds	x9, x10, x9
   29474:	cinc	x10, x13, cs  // cs = hs, nlast
   29478:	lsr	x11, x9, x12
   2947c:	lsl	x10, x10, x8
   29480:	orr	x10, x10, x11
   29484:	lsl	x9, x9, x8
   29488:	ldr	x11, [x3]
   2948c:	umulh	x12, x10, x11
   29490:	mul	x11, x11, x10
   29494:	add	x10, x10, #0x1
   29498:	adds	x13, x11, x9
   2949c:	adc	x10, x12, x10
   294a0:	msub	x9, x10, x2, x9
   294a4:	cmp	x9, x13
   294a8:	csel	x10, x2, xzr, hi  // hi = pmore
   294ac:	add	x9, x10, x9
   294b0:	cmp	x9, x2
   294b4:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   294b8:	sub	x9, x9, x10
   294bc:	lsr	x0, x9, x8
   294c0:	ret
   294c4:	cmp	x11, x2
   294c8:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   294cc:	sub	x10, x11, x10
   294d0:	b	29488 <__gmpn_mod_1_1p@@Base+0xdc>

00000000000294d4 <__gmpn_mod_1s_2p_cps@@Base>:
   294d4:	stp	x29, x30, [sp, #-48]!
   294d8:	str	x21, [sp, #16]
   294dc:	clz	x21, x1
   294e0:	stp	x20, x19, [sp, #32]
   294e4:	lsl	x20, x1, x21
   294e8:	mov	x19, x0
   294ec:	mov	x0, x20
   294f0:	mov	x29, sp
   294f4:	bl	d3f0 <__gmpn_invert_limb@plt>
   294f8:	neg	x8, x21
   294fc:	mov	w9, #0x1                   	// #1
   29500:	lsr	x8, x0, x8
   29504:	lsl	x9, x9, x21
   29508:	orr	x8, x8, x9
   2950c:	mul	x9, x8, x20
   29510:	mneg	x8, x8, x20
   29514:	lsr	x10, x8, x21
   29518:	umulh	x8, x8, x0
   2951c:	mvn	x8, x8
   29520:	add	x8, x9, x8
   29524:	mneg	x11, x9, x0
   29528:	mul	x8, x8, x20
   2952c:	cmp	x8, x11
   29530:	csel	x9, x20, xzr, hi  // hi = pmore
   29534:	add	x8, x9, x8
   29538:	lsr	x9, x8, x21
   2953c:	umulh	x11, x8, x0
   29540:	mul	x12, x8, x0
   29544:	add	x8, x8, x11
   29548:	mvn	x8, x8
   2954c:	mul	x8, x20, x8
   29550:	cmp	x8, x12
   29554:	stp	x10, x9, [x19, #16]
   29558:	csel	x9, x20, xzr, hi  // hi = pmore
   2955c:	add	x8, x9, x8
   29560:	lsr	x8, x8, x21
   29564:	stp	x0, x21, [x19]
   29568:	str	x8, [x19, #32]
   2956c:	ldp	x20, x19, [sp, #32]
   29570:	ldr	x21, [sp, #16]
   29574:	ldp	x29, x30, [sp], #48
   29578:	ret

000000000002957c <__gmpn_mod_1s_2p@@Base>:
   2957c:	ldp	x8, x9, [x3, #16]
   29580:	ldr	x10, [x3, #32]
   29584:	tbnz	w1, #0, 29630 <__gmpn_mod_1s_2p@@Base+0xb4>
   29588:	add	x12, x0, x1, lsl #3
   2958c:	ldp	x12, x11, [x12, #-16]
   29590:	cmp	x1, #0x4
   29594:	b.lt	295e4 <__gmpn_mod_1s_2p@@Base+0x68>  // b.tstop
   29598:	add	x14, x0, x1, lsl #3
   2959c:	ldp	x14, x15, [x14, #-32]
   295a0:	mov	x13, xzr
   295a4:	mul	x16, x15, x8
   295a8:	umulh	x15, x15, x8
   295ac:	adds	x17, x16, x14
   295b0:	adc	x13, x15, x13
   295b4:	mul	x14, x12, x9
   295b8:	umulh	x12, x12, x9
   295bc:	mul	x15, x11, x10
   295c0:	umulh	x11, x11, x10
   295c4:	adds	x16, x17, x14
   295c8:	adc	x13, x13, x12
   295cc:	sub	x14, x1, #0x4
   295d0:	adds	x12, x15, x16
   295d4:	adc	x11, x11, x13
   295d8:	sub	x1, x1, #0x2
   295dc:	cmp	x14, #0x1
   295e0:	b.gt	29598 <__gmpn_mod_1s_2p@@Base+0x1c>
   295e4:	mul	x10, x11, x8
   295e8:	umulh	x8, x11, x8
   295ec:	ldp	x11, x13, [x3]
   295f0:	mov	x9, xzr
   295f4:	adds	x14, x12, x10
   295f8:	adc	x9, x8, x9
   295fc:	neg	w10, w13
   29600:	lsl	x9, x9, x13
   29604:	lsr	x10, x14, x10
   29608:	orr	x9, x10, x9
   2960c:	umulh	x10, x9, x11
   29610:	mul	x11, x9, x11
   29614:	add	x9, x9, #0x1
   29618:	and	x8, x13, #0xffffffff
   2961c:	lsl	x12, x14, x13
   29620:	adds	x13, x11, x12
   29624:	adc	x9, x10, x9
   29628:	msub	x9, x9, x2, x12
   2962c:	b	29668 <__gmpn_mod_1s_2p@@Base+0xec>
   29630:	subs	x13, x1, #0x1
   29634:	b.ne	29690 <__gmpn_mod_1s_2p@@Base+0x114>  // b.any
   29638:	ldp	x11, x9, [x3]
   2963c:	ldr	x10, [x0]
   29640:	neg	w12, w9
   29644:	and	x8, x9, #0xffffffff
   29648:	lsl	x9, x10, x9
   2964c:	lsr	x10, x10, x12
   29650:	umulh	x12, x10, x11
   29654:	mul	x11, x10, x11
   29658:	add	x10, x10, #0x1
   2965c:	adds	x13, x11, x9
   29660:	adc	x10, x12, x10
   29664:	msub	x9, x10, x2, x9
   29668:	cmp	x9, x13
   2966c:	cset	w10, hi  // hi = pmore
   29670:	cmp	w10, #0x0
   29674:	csel	x10, x2, xzr, ne  // ne = any
   29678:	add	x9, x10, x9
   2967c:	cmp	x9, x2
   29680:	csel	x10, xzr, x2, cc  // cc = lo, ul, last
   29684:	sub	x9, x9, x10
   29688:	lsr	x0, x9, x8
   2968c:	ret
   29690:	add	x12, x0, x1, lsl #3
   29694:	ldp	x12, x15, [x12, #-24]
   29698:	ldr	x14, [x0, x13, lsl #3]
   2969c:	mov	x11, xzr
   296a0:	mov	x1, x13
   296a4:	mul	x17, x15, x8
   296a8:	umulh	x15, x15, x8
   296ac:	adds	x18, x17, x12
   296b0:	adc	x11, x15, x11
   296b4:	mul	x16, x14, x9
   296b8:	umulh	x14, x14, x9
   296bc:	adds	x12, x16, x18
   296c0:	adc	x11, x14, x11
   296c4:	cmp	x1, #0x4
   296c8:	b.ge	29598 <__gmpn_mod_1s_2p@@Base+0x1c>  // b.tcont
   296cc:	b	295e4 <__gmpn_mod_1s_2p@@Base+0x68>

00000000000296d0 <__gmpn_mod_1s_3p_cps@@Base>:
   296d0:	stp	x29, x30, [sp, #-48]!
   296d4:	str	x21, [sp, #16]
   296d8:	clz	x21, x1
   296dc:	stp	x20, x19, [sp, #32]
   296e0:	lsl	x20, x1, x21
   296e4:	mov	x19, x0
   296e8:	mov	x0, x20
   296ec:	mov	x29, sp
   296f0:	bl	d3f0 <__gmpn_invert_limb@plt>
   296f4:	neg	x8, x21
   296f8:	mov	w9, #0x1                   	// #1
   296fc:	lsr	x8, x0, x8
   29700:	lsl	x9, x9, x21
   29704:	orr	x8, x8, x9
   29708:	mul	x9, x8, x20
   2970c:	mneg	x8, x8, x20
   29710:	lsr	x10, x8, x21
   29714:	umulh	x8, x8, x0
   29718:	mvn	x8, x8
   2971c:	add	x8, x9, x8
   29720:	mneg	x11, x9, x0
   29724:	mul	x8, x8, x20
   29728:	cmp	x8, x11
   2972c:	csel	x9, x20, xzr, hi  // hi = pmore
   29730:	add	x8, x9, x8
   29734:	lsr	x9, x8, x21
   29738:	umulh	x11, x8, x0
   2973c:	stp	x10, x9, [x19, #16]
   29740:	mul	x9, x8, x0
   29744:	add	x8, x8, x11
   29748:	mvn	x8, x8
   2974c:	mul	x8, x20, x8
   29750:	cmp	x8, x9
   29754:	csel	x9, x20, xzr, hi  // hi = pmore
   29758:	add	x8, x9, x8
   2975c:	lsr	x9, x8, x21
   29760:	umulh	x10, x8, x0
   29764:	mul	x11, x8, x0
   29768:	add	x8, x8, x10
   2976c:	mvn	x8, x8
   29770:	mul	x8, x20, x8
   29774:	cmp	x8, x11
   29778:	csel	x10, x20, xzr, hi  // hi = pmore
   2977c:	add	x8, x10, x8
   29780:	lsr	x8, x8, x21
   29784:	stp	x0, x21, [x19]
   29788:	stp	x9, x8, [x19, #32]
   2978c:	ldp	x20, x19, [sp, #32]
   29790:	ldr	x21, [sp, #16]
   29794:	ldp	x29, x30, [sp], #48
   29798:	ret

000000000002979c <__gmpn_mod_1s_3p@@Base>:
   2979c:	mov	x12, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   297a0:	ldp	x8, x9, [x3, #16]
   297a4:	ldp	x10, x11, [x3, #32]
   297a8:	movk	x12, #0xaaab
   297ac:	mul	x12, x1, x12
   297b0:	lsr	x12, x12, #62
   297b4:	cmp	w12, #0x2
   297b8:	b.eq	29808 <__gmpn_mod_1s_3p@@Base+0x6c>  // b.none
   297bc:	cmp	w12, #0x1
   297c0:	b.eq	29820 <__gmpn_mod_1s_3p@@Base+0x84>  // b.none
   297c4:	cbnz	w12, 2983c <__gmpn_mod_1s_3p@@Base+0xa0>
   297c8:	add	x13, x0, x1, lsl #3
   297cc:	ldp	x14, x13, [x13, #-16]
   297d0:	mov	x12, xzr
   297d4:	sub	x1, x1, #0x3
   297d8:	ldr	x15, [x0, x1, lsl #3]
   297dc:	mul	x16, x14, x8
   297e0:	umulh	x14, x14, x8
   297e4:	adds	x18, x16, x15
   297e8:	adc	x12, x14, x12
   297ec:	mul	x17, x13, x9
   297f0:	umulh	x14, x13, x9
   297f4:	adds	x13, x17, x18
   297f8:	adc	x12, x14, x12
   297fc:	cmp	x1, #0x3
   29800:	b.ge	29844 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   29804:	b	298a0 <__gmpn_mod_1s_3p@@Base+0x104>
   29808:	sub	x1, x1, #0x1
   2980c:	ldr	x13, [x0, x1, lsl #3]
   29810:	mov	x12, xzr
   29814:	cmp	x1, #0x3
   29818:	b.ge	29844 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   2981c:	b	298a0 <__gmpn_mod_1s_3p@@Base+0x104>
   29820:	add	x12, x0, x1, lsl #3
   29824:	sub	x1, x1, #0x2
   29828:	ldur	x12, [x12, #-8]
   2982c:	ldr	x13, [x0, x1, lsl #3]
   29830:	cmp	x1, #0x3
   29834:	b.ge	29844 <__gmpn_mod_1s_3p@@Base+0xa8>  // b.tcont
   29838:	b	298a0 <__gmpn_mod_1s_3p@@Base+0x104>
   2983c:	cmp	x1, #0x3
   29840:	b.lt	298a0 <__gmpn_mod_1s_3p@@Base+0x104>  // b.tstop
   29844:	add	x15, x0, x1, lsl #3
   29848:	ldp	x18, x17, [x15, #-24]
   2984c:	ldur	x15, [x15, #-8]
   29850:	mov	x14, xzr
   29854:	mul	x16, x13, x10
   29858:	mul	x4, x17, x8
   2985c:	umulh	x17, x17, x8
   29860:	adds	x5, x4, x18
   29864:	adc	x14, x17, x14
   29868:	umulh	x13, x13, x10
   2986c:	mul	x17, x12, x11
   29870:	umulh	x12, x12, x11
   29874:	mul	x18, x15, x9
   29878:	umulh	x15, x15, x9
   2987c:	adds	x4, x5, x18
   29880:	adc	x14, x14, x15
   29884:	adds	x15, x4, x16
   29888:	adc	x14, x14, x13
   2988c:	adds	x13, x17, x15
   29890:	adc	x12, x12, x14
   29894:	cmp	x1, #0x5
   29898:	sub	x1, x1, #0x3
   2989c:	b.gt	29844 <__gmpn_mod_1s_3p@@Base+0xa8>
   298a0:	mul	x10, x12, x8
   298a4:	umulh	x8, x12, x8
   298a8:	ldp	x12, x11, [x3]
   298ac:	mov	x9, xzr
   298b0:	adds	x14, x13, x10
   298b4:	adc	x8, x8, x9
   298b8:	neg	w10, w11
   298bc:	lsl	x8, x8, x11
   298c0:	lsr	x10, x14, x10
   298c4:	orr	x8, x10, x8
   298c8:	umulh	x10, x8, x12
   298cc:	mul	x12, x8, x12
   298d0:	add	x8, x8, #0x1
   298d4:	lsl	x13, x14, x11
   298d8:	adds	x14, x12, x13
   298dc:	adc	x8, x10, x8
   298e0:	msub	x8, x8, x2, x13
   298e4:	cmp	x8, x14
   298e8:	csel	x10, x2, x9, hi  // hi = pmore
   298ec:	add	x8, x10, x8
   298f0:	cmp	x8, x2
   298f4:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   298f8:	sub	x8, x8, x9
   298fc:	lsr	x0, x8, x11
   29900:	ret

0000000000029904 <__gmpn_mod_1s_4p_cps@@Base>:
   29904:	stp	x29, x30, [sp, #-48]!
   29908:	str	x21, [sp, #16]
   2990c:	clz	x21, x1
   29910:	stp	x20, x19, [sp, #32]
   29914:	lsl	x20, x1, x21
   29918:	mov	x19, x0
   2991c:	mov	x0, x20
   29920:	mov	x29, sp
   29924:	bl	d3f0 <__gmpn_invert_limb@plt>
   29928:	neg	x8, x21
   2992c:	mov	w9, #0x1                   	// #1
   29930:	lsr	x8, x0, x8
   29934:	lsl	x9, x9, x21
   29938:	orr	x8, x8, x9
   2993c:	mul	x9, x8, x20
   29940:	mneg	x8, x8, x20
   29944:	lsr	x10, x8, x21
   29948:	umulh	x8, x8, x0
   2994c:	mvn	x8, x8
   29950:	add	x8, x9, x8
   29954:	mneg	x11, x9, x0
   29958:	mul	x8, x8, x20
   2995c:	cmp	x8, x11
   29960:	csel	x9, x20, xzr, hi  // hi = pmore
   29964:	add	x8, x9, x8
   29968:	lsr	x9, x8, x21
   2996c:	umulh	x11, x8, x0
   29970:	mul	x12, x8, x0
   29974:	add	x8, x8, x11
   29978:	mvn	x8, x8
   2997c:	mul	x8, x20, x8
   29980:	cmp	x8, x12
   29984:	stp	x10, x9, [x19, #16]
   29988:	csel	x9, x20, xzr, hi  // hi = pmore
   2998c:	add	x8, x9, x8
   29990:	lsr	x9, x8, x21
   29994:	umulh	x10, x8, x0
   29998:	mul	x11, x8, x0
   2999c:	add	x8, x8, x10
   299a0:	mvn	x8, x8
   299a4:	mul	x8, x20, x8
   299a8:	cmp	x8, x11
   299ac:	csel	x10, x20, xzr, hi  // hi = pmore
   299b0:	add	x8, x10, x8
   299b4:	lsr	x10, x8, x21
   299b8:	umulh	x11, x8, x0
   299bc:	mul	x12, x8, x0
   299c0:	add	x8, x8, x11
   299c4:	mvn	x8, x8
   299c8:	mul	x8, x20, x8
   299cc:	cmp	x8, x12
   299d0:	stp	x9, x10, [x19, #32]
   299d4:	csel	x9, x20, xzr, hi  // hi = pmore
   299d8:	add	x8, x9, x8
   299dc:	lsr	x8, x8, x21
   299e0:	stp	x0, x21, [x19]
   299e4:	str	x8, [x19, #48]
   299e8:	ldp	x20, x19, [sp, #32]
   299ec:	ldr	x21, [sp, #16]
   299f0:	ldp	x29, x30, [sp], #48
   299f4:	ret

00000000000299f8 <__gmpn_mod_1s_4p@@Base>:
   299f8:	ldp	x8, x9, [x3, #16]
   299fc:	ldp	x10, x11, [x3, #32]
   29a00:	ldr	x12, [x3, #48]
   29a04:	adrp	x14, 5b000 <__gmpn_bases@@Base+0x2678>
   29a08:	and	x13, x1, #0x3
   29a0c:	add	x14, x14, #0x1ba
   29a10:	adr	x15, 29a20 <__gmpn_mod_1s_4p@@Base+0x28>
   29a14:	ldrb	w16, [x14, x13]
   29a18:	add	x15, x15, x16, lsl #2
   29a1c:	br	x15
   29a20:	add	x14, x0, x1, lsl #3
   29a24:	ldp	x16, x18, [x14, #-24]
   29a28:	ldur	x14, [x14, #-8]
   29a2c:	mov	x13, xzr
   29a30:	sub	x15, x1, #0x4
   29a34:	ldr	x17, [x0, x15, lsl #3]
   29a38:	mul	x1, x16, x8
   29a3c:	umulh	x16, x16, x8
   29a40:	adds	x4, x1, x17
   29a44:	adc	x13, x16, x13
   29a48:	mul	x16, x18, x9
   29a4c:	umulh	x17, x18, x9
   29a50:	adds	x18, x4, x16
   29a54:	adc	x13, x13, x17
   29a58:	mul	x16, x14, x10
   29a5c:	umulh	x17, x14, x10
   29a60:	adds	x14, x16, x18
   29a64:	adc	x13, x17, x13
   29a68:	cmp	x15, #0x4
   29a6c:	b.ge	29ae4 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   29a70:	b	29b58 <__gmpn_mod_1s_4p@@Base+0x160>
   29a74:	add	x13, x0, x1, lsl #3
   29a78:	sub	x15, x1, #0x2
   29a7c:	ldur	x13, [x13, #-8]
   29a80:	ldr	x14, [x0, x15, lsl #3]
   29a84:	cmp	x15, #0x4
   29a88:	b.ge	29ae4 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   29a8c:	b	29b58 <__gmpn_mod_1s_4p@@Base+0x160>
   29a90:	add	x14, x0, x1, lsl #3
   29a94:	ldp	x16, x14, [x14, #-16]
   29a98:	mov	x13, xzr
   29a9c:	sub	x15, x1, #0x3
   29aa0:	ldr	x17, [x0, x15, lsl #3]
   29aa4:	mul	x18, x16, x8
   29aa8:	umulh	x16, x16, x8
   29aac:	adds	x4, x18, x17
   29ab0:	adc	x13, x16, x13
   29ab4:	mul	x1, x14, x9
   29ab8:	umulh	x16, x14, x9
   29abc:	adds	x14, x1, x4
   29ac0:	adc	x13, x16, x13
   29ac4:	cmp	x15, #0x4
   29ac8:	b.ge	29ae4 <__gmpn_mod_1s_4p@@Base+0xec>  // b.tcont
   29acc:	b	29b58 <__gmpn_mod_1s_4p@@Base+0x160>
   29ad0:	sub	x15, x1, #0x1
   29ad4:	ldr	x14, [x0, x15, lsl #3]
   29ad8:	mov	x13, xzr
   29adc:	cmp	x15, #0x4
   29ae0:	b.lt	29b58 <__gmpn_mod_1s_4p@@Base+0x160>  // b.tstop
   29ae4:	add	x16, x15, #0x4
   29ae8:	add	x15, x0, x15, lsl #3
   29aec:	sub	x15, x15, #0x10
   29af0:	ldp	x0, x18, [x15, #-16]
   29af4:	mov	x17, xzr
   29af8:	sub	x16, x16, #0x4
   29afc:	mul	x1, x18, x8
   29b00:	umulh	x18, x18, x8
   29b04:	adds	x5, x1, x0
   29b08:	adc	x17, x18, x17
   29b0c:	ldp	x4, x18, [x15], #-32
   29b10:	umulh	x1, x4, x9
   29b14:	mul	x0, x4, x9
   29b18:	adds	x4, x5, x0
   29b1c:	adc	x17, x17, x1
   29b20:	mul	x0, x18, x10
   29b24:	umulh	x18, x18, x10
   29b28:	adds	x1, x4, x0
   29b2c:	adc	x17, x17, x18
   29b30:	mul	x18, x14, x11
   29b34:	umulh	x14, x14, x11
   29b38:	mul	x0, x13, x12
   29b3c:	umulh	x13, x13, x12
   29b40:	adds	x4, x1, x18
   29b44:	adc	x17, x17, x14
   29b48:	adds	x14, x0, x4
   29b4c:	adc	x13, x13, x17
   29b50:	cmp	x16, #0x7
   29b54:	b.gt	29af0 <__gmpn_mod_1s_4p@@Base+0xf8>
   29b58:	ldp	x12, x11, [x3]
   29b5c:	mul	x10, x13, x8
   29b60:	umulh	x8, x13, x8
   29b64:	mov	x9, xzr
   29b68:	adds	x13, x14, x10
   29b6c:	adc	x8, x8, x9
   29b70:	neg	w10, w11
   29b74:	lsl	x8, x8, x11
   29b78:	lsr	x10, x13, x10
   29b7c:	orr	x8, x10, x8
   29b80:	umulh	x10, x8, x12
   29b84:	mul	x12, x8, x12
   29b88:	add	x8, x8, #0x1
   29b8c:	lsl	x13, x13, x11
   29b90:	adds	x14, x12, x13
   29b94:	adc	x8, x10, x8
   29b98:	msub	x8, x8, x2, x13
   29b9c:	cmp	x8, x14
   29ba0:	csel	x10, x2, x9, hi  // hi = pmore
   29ba4:	add	x8, x10, x8
   29ba8:	cmp	x8, x2
   29bac:	csel	x9, x9, x2, cc  // cc = lo, ul, last
   29bb0:	sub	x8, x8, x9
   29bb4:	lsr	x0, x8, x11
   29bb8:	ret
   29bbc:	nop

0000000000029bc0 <__gmpn_lshiftc@@Base>:
   29bc0:	add	x16, x0, x2, lsl #3
   29bc4:	add	x1, x1, x2, lsl #3
   29bc8:	neg	x8, x3
   29bcc:	lsr	x18, x2, #2
   29bd0:	tbz	w2, #0, 29c14 <__gmpn_lshiftc@@Base+0x54>
   29bd4:	ldur	x4, [x1, #-8]
   29bd8:	tbnz	w2, #1, 29c04 <__gmpn_lshiftc@@Base+0x44>
   29bdc:	lsr	x0, x4, x8
   29be0:	lsl	x2, x4, x3
   29be4:	cbnz	x18, 29bf4 <__gmpn_lshiftc@@Base+0x34>
   29be8:	mvn	x2, x2
   29bec:	stur	x2, [x16, #-8]
   29bf0:	ret
   29bf4:	ldp	x4, x5, [x1, #-24]
   29bf8:	sub	x1, x1, #0x8
   29bfc:	add	x16, x16, #0x10
   29c00:	b	29c94 <__gmpn_lshiftc@@Base+0xd4>
   29c04:	lsr	x0, x4, x8
   29c08:	lsl	x2, x4, x3
   29c0c:	ldp	x6, x7, [x1, #-24]!
   29c10:	b	29cb8 <__gmpn_lshiftc@@Base+0xf8>
   29c14:	ldp	x4, x5, [x1, #-16]
   29c18:	tbz	w2, #1, 29c58 <__gmpn_lshiftc@@Base+0x98>
   29c1c:	lsr	x0, x5, x8
   29c20:	lsl	x13, x5, x3
   29c24:	lsr	x10, x4, x8
   29c28:	lsl	x2, x4, x3
   29c2c:	cbnz	x18, 29c40 <__gmpn_lshiftc@@Base+0x80>
   29c30:	eon	x10, x10, x13
   29c34:	mvn	x2, x2
   29c38:	stp	x2, x10, [x16, #-16]
   29c3c:	ret
   29c40:	ldp	x4, x5, [x1, #-32]
   29c44:	eon	x10, x10, x13
   29c48:	stur	x10, [x16, #-8]
   29c4c:	sub	x1, x1, #0x10
   29c50:	add	x16, x16, #0x8
   29c54:	b	29c94 <__gmpn_lshiftc@@Base+0xd4>
   29c58:	lsr	x0, x5, x8
   29c5c:	lsl	x13, x5, x3
   29c60:	lsr	x10, x4, x8
   29c64:	lsl	x2, x4, x3
   29c68:	ldp	x6, x7, [x1, #-32]!
   29c6c:	eon	x10, x10, x13
   29c70:	str	x10, [x16, #-8]!
   29c74:	b	29cb4 <__gmpn_lshiftc@@Base+0xf4>
   29c78:	nop
   29c7c:	nop
   29c80:	ldp	x4, x5, [x1, #-16]
   29c84:	eon	x10, x10, x13
   29c88:	eon	x11, x12, x2
   29c8c:	stp	x10, x11, [x16, #-16]
   29c90:	lsl	x2, x6, x3
   29c94:	lsr	x10, x4, x8
   29c98:	lsl	x13, x5, x3
   29c9c:	lsr	x12, x5, x8
   29ca0:	ldp	x6, x7, [x1, #-32]!
   29ca4:	eon	x10, x10, x13
   29ca8:	eon	x11, x12, x2
   29cac:	stp	x10, x11, [x16, #-32]!
   29cb0:	lsl	x2, x4, x3
   29cb4:	sub	x18, x18, #0x1
   29cb8:	lsr	x10, x6, x8
   29cbc:	lsl	x13, x7, x3
   29cc0:	lsr	x12, x7, x8
   29cc4:	cbnz	x18, 29c80 <__gmpn_lshiftc@@Base+0xc0>
   29cc8:	eon	x10, x10, x13
   29ccc:	eon	x11, x12, x2
   29cd0:	lsl	x2, x6, x3
   29cd4:	stp	x10, x11, [x16, #-16]
   29cd8:	mvn	x2, x2
   29cdc:	stur	x2, [x16, #-24]
   29ce0:	ret

0000000000029ce4 <__gmpn_mul@@Base>:
   29ce4:	stp	x29, x30, [sp, #-96]!
   29ce8:	stp	x28, x27, [sp, #16]
   29cec:	stp	x26, x25, [sp, #32]
   29cf0:	stp	x24, x23, [sp, #48]
   29cf4:	stp	x22, x21, [sp, #64]
   29cf8:	stp	x20, x19, [sp, #80]
   29cfc:	mov	x29, sp
   29d00:	sub	sp, sp, #0xa0
   29d04:	mov	x19, x4
   29d08:	mov	x28, x3
   29d0c:	mov	x20, x2
   29d10:	mov	x23, x1
   29d14:	cmp	x2, #0xd
   29d18:	mov	x21, x0
   29d1c:	b.le	29e28 <__gmpn_mul@@Base+0x144>
   29d20:	cmp	x20, x19
   29d24:	b.ne	29d40 <__gmpn_mul@@Base+0x5c>  // b.any
   29d28:	mov	x0, x21
   29d2c:	mov	x1, x23
   29d30:	mov	x2, x28
   29d34:	mov	x3, x20
   29d38:	bl	c990 <__gmpn_mul_n@plt>
   29d3c:	b	29e40 <__gmpn_mul@@Base+0x15c>
   29d40:	cmp	x19, #0xd
   29d44:	b.gt	29e6c <__gmpn_mul@@Base+0x188>
   29d48:	cmp	x20, #0x1f5
   29d4c:	b.lt	29e28 <__gmpn_mul@@Base+0x144>  // b.tstop
   29d50:	cmp	x19, #0x1
   29d54:	b.eq	29e28 <__gmpn_mul@@Base+0x144>  // b.none
   29d58:	mov	w2, #0x1f4                 	// #500
   29d5c:	mov	x0, x21
   29d60:	mov	x1, x23
   29d64:	mov	x3, x28
   29d68:	mov	x4, x19
   29d6c:	bl	c550 <__gmpn_mul_basecase@plt>
   29d70:	add	x24, x21, #0xfa0
   29d74:	sub	x0, x29, #0x70
   29d78:	mov	x1, x24
   29d7c:	mov	x2, x19
   29d80:	bl	ca50 <__gmpn_copyi@plt>
   29d84:	sub	x22, x20, #0x1f4
   29d88:	cmp	x20, #0x3e9
   29d8c:	add	x23, x23, #0xfa0
   29d90:	b.lt	2a024 <__gmpn_mul@@Base+0x340>  // b.tstop
   29d94:	add	x8, x21, x19, lsl #3
   29d98:	add	x25, x8, #0xfa8
   29d9c:	lsl	x26, x19, #3
   29da0:	mov	x21, x24
   29da4:	b	29dd4 <__gmpn_mul@@Base+0xf0>
   29da8:	add	x21, x21, #0xfa0
   29dac:	sub	x0, x29, #0x70
   29db0:	mov	x1, x21
   29db4:	mov	x2, x19
   29db8:	bl	ca50 <__gmpn_copyi@plt>
   29dbc:	sub	x20, x22, #0x1f4
   29dc0:	add	x23, x23, #0xfa0
   29dc4:	cmp	x22, #0x3e8
   29dc8:	add	x25, x25, #0xfa0
   29dcc:	mov	x22, x20
   29dd0:	b.le	2a02c <__gmpn_mul@@Base+0x348>
   29dd4:	mov	w2, #0x1f4                 	// #500
   29dd8:	mov	x0, x21
   29ddc:	mov	x1, x23
   29de0:	mov	x3, x28
   29de4:	mov	x4, x19
   29de8:	bl	c550 <__gmpn_mul_basecase@plt>
   29dec:	sub	x2, x29, #0x70
   29df0:	mov	x0, x21
   29df4:	mov	x1, x21
   29df8:	mov	x3, x19
   29dfc:	bl	ca70 <__gmpn_add_n@plt>
   29e00:	ldr	x8, [x21, x26]
   29e04:	adds	x8, x8, x0
   29e08:	str	x8, [x21, x26]
   29e0c:	b.cc	29da8 <__gmpn_mul@@Base+0xc4>  // b.lo, b.ul, b.last
   29e10:	mov	x8, x25
   29e14:	ldr	x9, [x8]
   29e18:	adds	x9, x9, #0x1
   29e1c:	str	x9, [x8], #8
   29e20:	b.cs	29e14 <__gmpn_mul@@Base+0x130>  // b.hs, b.nlast
   29e24:	b	29da8 <__gmpn_mul@@Base+0xc4>
   29e28:	mov	x0, x21
   29e2c:	mov	x1, x23
   29e30:	mov	x2, x20
   29e34:	mov	x3, x28
   29e38:	mov	x4, x19
   29e3c:	bl	c550 <__gmpn_mul_basecase@plt>
   29e40:	add	x8, x19, x20
   29e44:	add	x8, x21, x8, lsl #3
   29e48:	ldur	x0, [x8, #-8]
   29e4c:	mov	sp, x29
   29e50:	ldp	x20, x19, [sp, #80]
   29e54:	ldp	x22, x21, [sp, #64]
   29e58:	ldp	x24, x23, [sp, #48]
   29e5c:	ldp	x26, x25, [sp, #32]
   29e60:	ldp	x28, x27, [sp, #16]
   29e64:	ldp	x29, x30, [sp], #96
   29e68:	ret
   29e6c:	cmp	x19, #0x30
   29e70:	stur	x28, [x29, #-120]
   29e74:	b.le	29eb8 <__gmpn_mul@@Base+0x1d4>
   29e78:	add	x8, x19, x20
   29e7c:	mov	w9, #0x1900                	// #6400
   29e80:	cmp	x8, x9
   29e84:	b.lt	29f18 <__gmpn_mul@@Base+0x234>  // b.tstop
   29e88:	add	x25, x19, x19, lsl #1
   29e8c:	cmp	x25, #0xc7f
   29e90:	b.le	29f18 <__gmpn_mul@@Base+0x234>
   29e94:	cmp	x20, x19, lsl #3
   29e98:	b.ge	2a434 <__gmpn_mul@@Base+0x750>  // b.tcont
   29e9c:	mov	x0, x21
   29ea0:	mov	x1, x23
   29ea4:	mov	x2, x20
   29ea8:	mov	x3, x28
   29eac:	mov	x4, x19
   29eb0:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   29eb4:	b	29e40 <__gmpn_mul@@Base+0x15c>
   29eb8:	add	x8, x19, x19, lsl #3
   29ebc:	cmp	x8, #0x0
   29ec0:	cinc	x8, x8, lt  // lt = tstop
   29ec4:	lsl	x8, x8, #2
   29ec8:	and	x8, x8, #0xfffffffffffffff8
   29ecc:	add	x8, x8, #0x40f
   29ed0:	and	x8, x8, #0xfffffffffffffff0
   29ed4:	mov	x9, sp
   29ed8:	sub	x5, x9, x8
   29edc:	mov	sp, x5
   29ee0:	add	x26, x19, x19, lsl #1
   29ee4:	cmp	x26, x20
   29ee8:	b.le	2a04c <__gmpn_mul@@Base+0x368>
   29eec:	lsl	x8, x20, #2
   29ef0:	add	x9, x19, x19, lsl #2
   29ef4:	cmp	x8, x9
   29ef8:	b.ge	2a17c <__gmpn_mul@@Base+0x498>  // b.tcont
   29efc:	mov	x0, x21
   29f00:	mov	x1, x23
   29f04:	mov	x2, x20
   29f08:	mov	x3, x28
   29f0c:	mov	x4, x19
   29f10:	bl	d450 <__gmpn_toom22_mul@plt>
   29f14:	b	29e40 <__gmpn_mul@@Base+0x15c>
   29f18:	cmp	x19, #0x52
   29f1c:	b.lt	29fa8 <__gmpn_mul@@Base+0x2c4>  // b.tstop
   29f20:	add	x9, x20, x20, lsl #1
   29f24:	add	x9, x9, #0xc
   29f28:	cmp	x9, x19, lsl #2
   29f2c:	b.ge	29fa8 <__gmpn_mul@@Base+0x2c4>  // b.tcont
   29f30:	cmp	x19, #0xac
   29f34:	stur	xzr, [x29, #-112]
   29f38:	b.le	2a51c <__gmpn_mul@@Base+0x838>
   29f3c:	cmp	x19, #0xeb
   29f40:	b.le	2a6b4 <__gmpn_mul@@Base+0x9d0>
   29f44:	mov	x9, #0x4925                	// #18725
   29f48:	movk	x9, #0x2492, lsl #16
   29f4c:	movk	x9, #0x9249, lsl #32
   29f50:	lsr	x8, x8, #1
   29f54:	movk	x9, #0x4924, lsl #48
   29f58:	umulh	x8, x8, x9
   29f5c:	mov	w10, #0x78                  	// #120
   29f60:	lsr	x8, x8, #1
   29f64:	mul	x8, x8, x10
   29f68:	add	x1, x8, #0xd68
   29f6c:	mov	w8, #0x7f00                	// #32512
   29f70:	cmp	x1, x8
   29f74:	b.hi	2a790 <__gmpn_mul@@Base+0xaac>  // b.pmore
   29f78:	add	x9, x1, #0xf
   29f7c:	mov	x8, sp
   29f80:	and	x9, x9, #0xfffffffffffffff0
   29f84:	sub	x5, x8, x9
   29f88:	mov	sp, x5
   29f8c:	mov	x0, x21
   29f90:	mov	x1, x23
   29f94:	mov	x2, x20
   29f98:	mov	x3, x28
   29f9c:	mov	x4, x19
   29fa0:	bl	cb20 <__gmpn_toom8h_mul@plt>
   29fa4:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   29fa8:	lsl	x8, x19, #5
   29fac:	add	x1, x8, #0x200
   29fb0:	mov	w8, #0x7f00                	// #32512
   29fb4:	cmp	x1, x8
   29fb8:	stur	xzr, [x29, #-112]
   29fbc:	b.hi	2a770 <__gmpn_mul@@Base+0xa8c>  // b.pmore
   29fc0:	add	x9, x1, #0xf
   29fc4:	mov	x8, sp
   29fc8:	and	x9, x9, #0xfffffffffffffff0
   29fcc:	sub	x8, x8, x9
   29fd0:	stur	x8, [x29, #-128]
   29fd4:	mov	sp, x8
   29fd8:	lsl	x9, x20, #1
   29fdc:	add	x8, x19, x19, lsl #2
   29fe0:	cmp	x9, x8
   29fe4:	stur	x8, [x29, #-136]
   29fe8:	b.ge	2a124 <__gmpn_mul@@Base+0x440>  // b.tcont
   29fec:	add	x8, x20, x20, lsl #1
   29ff0:	lsl	x11, x19, #3
   29ff4:	lsl	x10, x8, #1
   29ff8:	sub	x8, x11, x19
   29ffc:	cmp	x10, x8
   2a000:	b.ge	2a284 <__gmpn_mul@@Base+0x5a0>  // b.tcont
   2a004:	ldur	x5, [x29, #-128]
   2a008:	mov	x0, x21
   2a00c:	mov	x1, x23
   2a010:	mov	x2, x20
   2a014:	mov	x3, x28
   2a018:	mov	x4, x19
   2a01c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   2a020:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a024:	mov	x21, x24
   2a028:	mov	x20, x22
   2a02c:	mov	x0, x21
   2a030:	cmp	x20, x19
   2a034:	b.le	2a0cc <__gmpn_mul@@Base+0x3e8>
   2a038:	mov	x1, x23
   2a03c:	mov	x2, x20
   2a040:	mov	x3, x28
   2a044:	mov	x4, x19
   2a048:	b	2a0dc <__gmpn_mul@@Base+0x3f8>
   2a04c:	mov	x8, sp
   2a050:	sub	x22, x8, x19, lsl #5
   2a054:	mov	sp, x22
   2a058:	lsl	x27, x19, #1
   2a05c:	mov	x0, x21
   2a060:	mov	x1, x23
   2a064:	mov	x2, x27
   2a068:	mov	x3, x28
   2a06c:	mov	x4, x19
   2a070:	stur	x5, [x29, #-128]
   2a074:	bl	d480 <__gmpn_toom42_mul@plt>
   2a078:	lsl	x8, x19, #4
   2a07c:	sub	x20, x20, x27
   2a080:	add	x25, x21, x8
   2a084:	cmp	x20, x26
   2a088:	add	x23, x23, x8
   2a08c:	stp	x8, x22, [x29, #-144]
   2a090:	b.ge	2a1a8 <__gmpn_mul@@Base+0x4c4>  // b.tcont
   2a094:	ldur	x5, [x29, #-128]
   2a098:	lsl	x8, x20, #2
   2a09c:	add	x9, x19, x19, lsl #2
   2a0a0:	cmp	x8, x9
   2a0a4:	lsl	x24, x19, #3
   2a0a8:	b.ge	2a258 <__gmpn_mul@@Base+0x574>  // b.tcont
   2a0ac:	ldur	x26, [x29, #-136]
   2a0b0:	ldur	x3, [x29, #-120]
   2a0b4:	mov	x1, x23
   2a0b8:	mov	x2, x20
   2a0bc:	mov	x0, x26
   2a0c0:	mov	x4, x19
   2a0c4:	bl	d450 <__gmpn_toom22_mul@plt>
   2a0c8:	b	2a4c0 <__gmpn_mul@@Base+0x7dc>
   2a0cc:	mov	x1, x28
   2a0d0:	mov	x2, x19
   2a0d4:	mov	x3, x23
   2a0d8:	mov	x4, x20
   2a0dc:	bl	c550 <__gmpn_mul_basecase@plt>
   2a0e0:	sub	x2, x29, #0x70
   2a0e4:	mov	x0, x21
   2a0e8:	mov	x1, x21
   2a0ec:	mov	x3, x19
   2a0f0:	bl	ca70 <__gmpn_add_n@plt>
   2a0f4:	lsl	x8, x19, #3
   2a0f8:	ldr	x9, [x21, x8]
   2a0fc:	adds	x9, x9, x0
   2a100:	str	x9, [x21, x8]
   2a104:	b.cc	29e40 <__gmpn_mul@@Base+0x15c>  // b.lo, b.ul, b.last
   2a108:	add	x8, x21, x19, lsl #3
   2a10c:	add	x8, x8, #0x8
   2a110:	ldr	x9, [x8]
   2a114:	adds	x9, x9, #0x1
   2a118:	str	x9, [x8], #8
   2a11c:	b.cs	2a110 <__gmpn_mul@@Base+0x42c>  // b.hs, b.nlast
   2a120:	b	29e40 <__gmpn_mul@@Base+0x15c>
   2a124:	mov	w8, #0x1c                  	// #28
   2a128:	mul	x8, x19, x8
   2a12c:	and	x1, x8, #0xfffffffffffffff8
   2a130:	mov	w8, #0x7f00                	// #32512
   2a134:	cmp	x1, x8
   2a138:	b.hi	2a780 <__gmpn_mul@@Base+0xa9c>  // b.pmore
   2a13c:	add	x9, x1, #0xf
   2a140:	mov	x8, sp
   2a144:	and	x9, x9, #0xfffffffffffffff0
   2a148:	sub	x25, x8, x9
   2a14c:	mov	sp, x25
   2a150:	lsl	x27, x19, #1
   2a154:	cmp	x19, #0x4f
   2a158:	mov	x0, x21
   2a15c:	mov	x1, x23
   2a160:	mov	x2, x27
   2a164:	mov	x3, x28
   2a168:	mov	x4, x19
   2a16c:	b.le	2a2b8 <__gmpn_mul@@Base+0x5d4>
   2a170:	ldur	x5, [x29, #-128]
   2a174:	bl	c740 <__gmpn_toom63_mul@plt>
   2a178:	b	2a2c0 <__gmpn_mul@@Base+0x5dc>
   2a17c:	lsl	x9, x19, #3
   2a180:	sub	x9, x9, x19
   2a184:	mov	x0, x21
   2a188:	mov	x1, x23
   2a18c:	mov	x2, x20
   2a190:	mov	x3, x28
   2a194:	mov	x4, x19
   2a198:	cmp	x8, x9
   2a19c:	b.ge	2a4b4 <__gmpn_mul@@Base+0x7d0>  // b.tcont
   2a1a0:	bl	c850 <__gmpn_toom32_mul@plt>
   2a1a4:	b	29e40 <__gmpn_mul@@Base+0x15c>
   2a1a8:	add	x8, x22, x19, lsl #3
   2a1ac:	ldur	x5, [x29, #-128]
   2a1b0:	stur	x8, [x29, #-152]
   2a1b4:	mov	w8, #0x18                  	// #24
   2a1b8:	madd	x8, x19, x8, x21
   2a1bc:	add	x28, x8, #0x8
   2a1c0:	lsl	x22, x27, #3
   2a1c4:	b	2a1e8 <__gmpn_mul@@Base+0x504>
   2a1c8:	ldur	x8, [x29, #-144]
   2a1cc:	ldur	x5, [x29, #-128]
   2a1d0:	sub	x20, x20, x27
   2a1d4:	add	x25, x25, x22
   2a1d8:	add	x23, x23, x22
   2a1dc:	cmp	x20, x26
   2a1e0:	add	x28, x28, x8
   2a1e4:	b.lt	2a098 <__gmpn_mul@@Base+0x3b4>  // b.tstop
   2a1e8:	ldur	x21, [x29, #-136]
   2a1ec:	ldur	x3, [x29, #-120]
   2a1f0:	mov	x1, x23
   2a1f4:	mov	x2, x27
   2a1f8:	mov	x0, x21
   2a1fc:	mov	x4, x19
   2a200:	bl	d480 <__gmpn_toom42_mul@plt>
   2a204:	mov	x0, x25
   2a208:	mov	x1, x25
   2a20c:	mov	x2, x21
   2a210:	mov	x3, x19
   2a214:	bl	ca70 <__gmpn_add_n@plt>
   2a218:	ldur	x1, [x29, #-152]
   2a21c:	add	x24, x25, x19, lsl #3
   2a220:	mov	x21, x0
   2a224:	mov	x0, x24
   2a228:	mov	x2, x27
   2a22c:	bl	ca50 <__gmpn_copyi@plt>
   2a230:	ldr	x8, [x24]
   2a234:	adds	x8, x8, x21
   2a238:	str	x8, [x24]
   2a23c:	b.cc	2a1c8 <__gmpn_mul@@Base+0x4e4>  // b.lo, b.ul, b.last
   2a240:	mov	x8, x28
   2a244:	ldr	x9, [x8]
   2a248:	adds	x9, x9, #0x1
   2a24c:	str	x9, [x8], #8
   2a250:	b.cs	2a244 <__gmpn_mul@@Base+0x560>  // b.hs, b.nlast
   2a254:	b	2a1c8 <__gmpn_mul@@Base+0x4e4>
   2a258:	ldur	x26, [x29, #-136]
   2a25c:	ldur	x3, [x29, #-120]
   2a260:	sub	x9, x24, x19
   2a264:	cmp	x8, x9
   2a268:	mov	x0, x26
   2a26c:	mov	x1, x23
   2a270:	mov	x2, x20
   2a274:	mov	x4, x19
   2a278:	b.ge	2a4bc <__gmpn_mul@@Base+0x7d8>  // b.tcont
   2a27c:	bl	c850 <__gmpn_toom32_mul@plt>
   2a280:	b	2a4c0 <__gmpn_mul@@Base+0x7dc>
   2a284:	add	x11, x19, x19, lsl #1
   2a288:	cmp	x9, x11
   2a28c:	b.ge	2a554 <__gmpn_mul@@Base+0x870>  // b.tcont
   2a290:	cmp	x19, #0x50
   2a294:	b.le	2a574 <__gmpn_mul@@Base+0x890>
   2a298:	ldur	x5, [x29, #-128]
   2a29c:	mov	x0, x21
   2a2a0:	mov	x1, x23
   2a2a4:	mov	x2, x20
   2a2a8:	mov	x3, x28
   2a2ac:	mov	x4, x19
   2a2b0:	bl	cf00 <__gmpn_toom43_mul@plt>
   2a2b4:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a2b8:	ldur	x5, [x29, #-128]
   2a2bc:	bl	d480 <__gmpn_toom42_mul@plt>
   2a2c0:	ldur	x8, [x29, #-136]
   2a2c4:	lsl	x22, x27, #3
   2a2c8:	sub	x20, x20, x27
   2a2cc:	add	x26, x21, x22
   2a2d0:	cmp	x8, x20, lsl #1
   2a2d4:	add	x23, x23, x22
   2a2d8:	b.le	2a2fc <__gmpn_mul@@Base+0x618>
   2a2dc:	mov	x0, x25
   2a2e0:	cmp	x20, x19
   2a2e4:	b.ge	2a3c0 <__gmpn_mul@@Base+0x6dc>  // b.tcont
   2a2e8:	ldur	x1, [x29, #-120]
   2a2ec:	mov	x2, x19
   2a2f0:	mov	x3, x23
   2a2f4:	mov	x4, x20
   2a2f8:	b	2a3d0 <__gmpn_mul@@Base+0x6ec>
   2a2fc:	add	x8, x25, x19, lsl #3
   2a300:	stur	x8, [x29, #-144]
   2a304:	mov	w8, #0x18                  	// #24
   2a308:	madd	x8, x19, x8, x21
   2a30c:	add	x28, x8, #0x8
   2a310:	lsl	x8, x19, #4
   2a314:	stur	x8, [x29, #-152]
   2a318:	b	2a33c <__gmpn_mul@@Base+0x658>
   2a31c:	ldur	x8, [x29, #-136]
   2a320:	sub	x20, x20, x27
   2a324:	add	x26, x26, x22
   2a328:	add	x23, x23, x22
   2a32c:	cmp	x8, x20, lsl #1
   2a330:	ldur	x8, [x29, #-152]
   2a334:	add	x28, x28, x8
   2a338:	b.gt	2a2dc <__gmpn_mul@@Base+0x5f8>
   2a33c:	mov	x0, x25
   2a340:	mov	x1, x23
   2a344:	mov	x2, x27
   2a348:	cmp	x19, #0x4f
   2a34c:	b.le	2a360 <__gmpn_mul@@Base+0x67c>
   2a350:	ldp	x5, x3, [x29, #-128]
   2a354:	mov	x4, x19
   2a358:	bl	c740 <__gmpn_toom63_mul@plt>
   2a35c:	b	2a36c <__gmpn_mul@@Base+0x688>
   2a360:	ldp	x5, x3, [x29, #-128]
   2a364:	mov	x4, x19
   2a368:	bl	d480 <__gmpn_toom42_mul@plt>
   2a36c:	mov	x0, x26
   2a370:	mov	x1, x26
   2a374:	mov	x2, x25
   2a378:	mov	x3, x19
   2a37c:	bl	ca70 <__gmpn_add_n@plt>
   2a380:	ldur	x1, [x29, #-144]
   2a384:	add	x24, x26, x19, lsl #3
   2a388:	mov	x21, x0
   2a38c:	mov	x0, x24
   2a390:	mov	x2, x27
   2a394:	bl	ca50 <__gmpn_copyi@plt>
   2a398:	ldr	x8, [x24]
   2a39c:	adds	x8, x8, x21
   2a3a0:	str	x8, [x24]
   2a3a4:	b.cc	2a31c <__gmpn_mul@@Base+0x638>  // b.lo, b.ul, b.last
   2a3a8:	mov	x8, x28
   2a3ac:	ldr	x9, [x8]
   2a3b0:	adds	x9, x9, #0x1
   2a3b4:	str	x9, [x8], #8
   2a3b8:	b.cs	2a3ac <__gmpn_mul@@Base+0x6c8>  // b.hs, b.nlast
   2a3bc:	b	2a31c <__gmpn_mul@@Base+0x638>
   2a3c0:	ldur	x3, [x29, #-120]
   2a3c4:	mov	x1, x23
   2a3c8:	mov	x2, x20
   2a3cc:	mov	x4, x19
   2a3d0:	bl	ccd0 <__gmpn_mul@plt>
   2a3d4:	mov	x0, x26
   2a3d8:	mov	x1, x26
   2a3dc:	mov	x2, x25
   2a3e0:	mov	x3, x19
   2a3e4:	bl	ca70 <__gmpn_add_n@plt>
   2a3e8:	lsl	x23, x19, #3
   2a3ec:	add	x22, x26, x23
   2a3f0:	mov	x21, x0
   2a3f4:	add	x1, x25, x23
   2a3f8:	mov	x0, x22
   2a3fc:	mov	x2, x20
   2a400:	bl	ca50 <__gmpn_copyi@plt>
   2a404:	ldr	x8, [x22]
   2a408:	adds	x8, x8, x21
   2a40c:	str	x8, [x22]
   2a410:	b.cc	2a42c <__gmpn_mul@@Base+0x748>  // b.lo, b.ul, b.last
   2a414:	add	x8, x23, #0x8
   2a418:	ldr	x9, [x26, x8]
   2a41c:	adds	x9, x9, #0x1
   2a420:	str	x9, [x26, x8]
   2a424:	add	x8, x8, #0x8
   2a428:	b.cs	2a418 <__gmpn_mul@@Base+0x734>  // b.hs, b.nlast
   2a42c:	mov	x21, x26
   2a430:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a434:	lsl	x26, x19, #3
   2a438:	add	x8, x26, x19
   2a43c:	lsl	x8, x8, #2
   2a440:	and	x1, x8, #0xfffffffffffffff8
   2a444:	sub	x0, x29, #0x70
   2a448:	stur	xzr, [x29, #-112]
   2a44c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a450:	mov	x24, x0
   2a454:	mov	x0, x21
   2a458:	mov	x1, x23
   2a45c:	mov	x2, x25
   2a460:	mov	x3, x28
   2a464:	mov	x4, x19
   2a468:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   2a46c:	lsl	x22, x25, #3
   2a470:	sub	x20, x20, x25
   2a474:	sub	x9, x26, x19
   2a478:	add	x8, x21, x22
   2a47c:	cmp	x9, x20, lsl #1
   2a480:	add	x23, x23, x22
   2a484:	stur	x26, [x29, #-152]
   2a488:	stur	x9, [x29, #-128]
   2a48c:	b.le	2a594 <__gmpn_mul@@Base+0x8b0>
   2a490:	mov	x21, x8
   2a494:	mov	x0, x24
   2a498:	cmp	x20, x19
   2a49c:	b.ge	2a644 <__gmpn_mul@@Base+0x960>  // b.tcont
   2a4a0:	mov	x1, x28
   2a4a4:	mov	x2, x19
   2a4a8:	mov	x3, x23
   2a4ac:	mov	x4, x20
   2a4b0:	b	2a654 <__gmpn_mul@@Base+0x970>
   2a4b4:	bl	d480 <__gmpn_toom42_mul@plt>
   2a4b8:	b	29e40 <__gmpn_mul@@Base+0x15c>
   2a4bc:	bl	d480 <__gmpn_toom42_mul@plt>
   2a4c0:	mov	x0, x25
   2a4c4:	mov	x1, x25
   2a4c8:	mov	x2, x26
   2a4cc:	mov	x3, x19
   2a4d0:	bl	ca70 <__gmpn_add_n@plt>
   2a4d4:	add	x22, x25, x24
   2a4d8:	mov	x21, x0
   2a4dc:	add	x1, x26, x24
   2a4e0:	mov	x0, x22
   2a4e4:	mov	x2, x20
   2a4e8:	bl	ca50 <__gmpn_copyi@plt>
   2a4ec:	ldr	x8, [x22]
   2a4f0:	adds	x8, x8, x21
   2a4f4:	str	x8, [x22]
   2a4f8:	b.cc	2a514 <__gmpn_mul@@Base+0x830>  // b.lo, b.ul, b.last
   2a4fc:	add	x8, x24, #0x8
   2a500:	ldr	x9, [x25, x8]
   2a504:	adds	x9, x9, #0x1
   2a508:	str	x9, [x25, x8]
   2a50c:	add	x8, x8, #0x8
   2a510:	b.cs	2a500 <__gmpn_mul@@Base+0x81c>  // b.hs, b.nlast
   2a514:	mov	x21, x25
   2a518:	b	29e40 <__gmpn_mul@@Base+0x15c>
   2a51c:	mov	w8, #0x18                  	// #24
   2a520:	mul	x8, x20, x8
   2a524:	add	x8, x8, #0x20f
   2a528:	and	x8, x8, #0xfffffffffffffff0
   2a52c:	mov	x9, sp
   2a530:	sub	x5, x9, x8
   2a534:	mov	sp, x5
   2a538:	mov	x0, x21
   2a53c:	mov	x1, x23
   2a540:	mov	x2, x20
   2a544:	mov	x3, x28
   2a548:	mov	x4, x19
   2a54c:	bl	c720 <__gmpn_toom44_mul@plt>
   2a550:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a554:	mov	w9, #0xb                   	// #11
   2a558:	mul	x9, x19, x9
   2a55c:	cmp	x10, x9
   2a560:	b.ge	2a6f4 <__gmpn_mul@@Base+0xa10>  // b.tcont
   2a564:	cmp	x8, x20, lsl #2
   2a568:	b.le	2a71c <__gmpn_mul@@Base+0xa38>
   2a56c:	cmp	x19, #0x4b
   2a570:	b.gt	2a724 <__gmpn_mul@@Base+0xa40>
   2a574:	ldur	x5, [x29, #-128]
   2a578:	mov	x0, x21
   2a57c:	mov	x1, x23
   2a580:	mov	x2, x20
   2a584:	mov	x3, x28
   2a588:	mov	x4, x19
   2a58c:	bl	c850 <__gmpn_toom32_mul@plt>
   2a590:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a594:	add	x9, x24, x19, lsl #3
   2a598:	stur	x9, [x29, #-136]
   2a59c:	add	x9, x21, x19, lsl #5
   2a5a0:	add	x10, x19, x19, lsl #1
   2a5a4:	add	x26, x9, #0x8
   2a5a8:	lsl	x9, x10, #3
   2a5ac:	mov	x21, x8
   2a5b0:	stur	x9, [x29, #-144]
   2a5b4:	b	2a5d8 <__gmpn_mul@@Base+0x8f4>
   2a5b8:	ldp	x8, x28, [x29, #-128]
   2a5bc:	sub	x20, x20, x25
   2a5c0:	add	x21, x21, x22
   2a5c4:	add	x23, x23, x22
   2a5c8:	cmp	x8, x20, lsl #1
   2a5cc:	ldur	x8, [x29, #-144]
   2a5d0:	add	x26, x26, x8
   2a5d4:	b.gt	2a494 <__gmpn_mul@@Base+0x7b0>
   2a5d8:	mov	x0, x24
   2a5dc:	mov	x1, x23
   2a5e0:	mov	x2, x25
   2a5e4:	mov	x3, x28
   2a5e8:	mov	x4, x19
   2a5ec:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   2a5f0:	mov	x0, x21
   2a5f4:	mov	x1, x21
   2a5f8:	mov	x2, x24
   2a5fc:	mov	x3, x19
   2a600:	bl	ca70 <__gmpn_add_n@plt>
   2a604:	ldur	x1, [x29, #-136]
   2a608:	add	x28, x21, x19, lsl #3
   2a60c:	mov	x27, x0
   2a610:	mov	x0, x28
   2a614:	mov	x2, x25
   2a618:	bl	ca50 <__gmpn_copyi@plt>
   2a61c:	ldr	x8, [x28]
   2a620:	adds	x8, x8, x27
   2a624:	str	x8, [x28]
   2a628:	b.cc	2a5b8 <__gmpn_mul@@Base+0x8d4>  // b.lo, b.ul, b.last
   2a62c:	mov	x8, x26
   2a630:	ldr	x9, [x8]
   2a634:	adds	x9, x9, #0x1
   2a638:	str	x9, [x8], #8
   2a63c:	b.cs	2a630 <__gmpn_mul@@Base+0x94c>  // b.hs, b.nlast
   2a640:	b	2a5b8 <__gmpn_mul@@Base+0x8d4>
   2a644:	mov	x1, x23
   2a648:	mov	x2, x20
   2a64c:	mov	x3, x28
   2a650:	mov	x4, x19
   2a654:	bl	ccd0 <__gmpn_mul@plt>
   2a658:	mov	x0, x21
   2a65c:	mov	x1, x21
   2a660:	mov	x2, x24
   2a664:	mov	x3, x19
   2a668:	bl	ca70 <__gmpn_add_n@plt>
   2a66c:	ldur	x8, [x29, #-152]
   2a670:	mov	x22, x0
   2a674:	mov	x2, x20
   2a678:	add	x23, x21, x8
   2a67c:	add	x1, x24, x8
   2a680:	mov	x0, x23
   2a684:	bl	ca50 <__gmpn_copyi@plt>
   2a688:	ldr	x8, [x23]
   2a68c:	adds	x8, x8, x22
   2a690:	str	x8, [x23]
   2a694:	b.cc	2a760 <__gmpn_mul@@Base+0xa7c>  // b.lo, b.ul, b.last
   2a698:	add	x8, x21, x19, lsl #3
   2a69c:	add	x8, x8, #0x8
   2a6a0:	ldr	x9, [x8]
   2a6a4:	adds	x9, x9, #0x1
   2a6a8:	str	x9, [x8], #8
   2a6ac:	b.cs	2a6a0 <__gmpn_mul@@Base+0x9bc>  // b.hs, b.nlast
   2a6b0:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a6b4:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   2a6b8:	movk	x9, #0xcccd
   2a6bc:	umulh	x8, x8, x9
   2a6c0:	lsr	x8, x8, #3
   2a6c4:	mov	w9, #0x60                  	// #96
   2a6c8:	mov	x10, sp
   2a6cc:	msub	x8, x8, x9, x10
   2a6d0:	sub	x5, x8, #0xc60
   2a6d4:	mov	sp, x5
   2a6d8:	mov	x0, x21
   2a6dc:	mov	x1, x23
   2a6e0:	mov	x2, x20
   2a6e4:	mov	x3, x28
   2a6e8:	mov	x4, x19
   2a6ec:	bl	cc20 <__gmpn_toom6h_mul@plt>
   2a6f0:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a6f4:	cmp	x19, #0x4f
   2a6f8:	b.le	2a744 <__gmpn_mul@@Base+0xa60>
   2a6fc:	ldur	x5, [x29, #-128]
   2a700:	mov	x0, x21
   2a704:	mov	x1, x23
   2a708:	mov	x2, x20
   2a70c:	mov	x3, x28
   2a710:	mov	x4, x19
   2a714:	bl	c740 <__gmpn_toom63_mul@plt>
   2a718:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a71c:	cmp	x19, #0x50
   2a720:	b.le	2a744 <__gmpn_mul@@Base+0xa60>
   2a724:	ldur	x5, [x29, #-128]
   2a728:	mov	x0, x21
   2a72c:	mov	x1, x23
   2a730:	mov	x2, x20
   2a734:	mov	x3, x28
   2a738:	mov	x4, x19
   2a73c:	bl	ca40 <__gmpn_toom53_mul@plt>
   2a740:	b	2a760 <__gmpn_mul@@Base+0xa7c>
   2a744:	ldur	x5, [x29, #-128]
   2a748:	mov	x0, x21
   2a74c:	mov	x1, x23
   2a750:	mov	x2, x20
   2a754:	mov	x3, x28
   2a758:	mov	x4, x19
   2a75c:	bl	d480 <__gmpn_toom42_mul@plt>
   2a760:	ldur	x0, [x29, #-112]
   2a764:	cbz	x0, 29e40 <__gmpn_mul@@Base+0x15c>
   2a768:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2a76c:	b	29e40 <__gmpn_mul@@Base+0x15c>
   2a770:	sub	x0, x29, #0x70
   2a774:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a778:	stur	x0, [x29, #-128]
   2a77c:	b	29fd8 <__gmpn_mul@@Base+0x2f4>
   2a780:	sub	x0, x29, #0x70
   2a784:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a788:	mov	x25, x0
   2a78c:	b	2a150 <__gmpn_mul@@Base+0x46c>
   2a790:	sub	x0, x29, #0x70
   2a794:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a798:	mov	x5, x0
   2a79c:	b	29f8c <__gmpn_mul@@Base+0x2a8>

000000000002a7a0 <__gmpn_fft_best_k@@Base>:
   2a7a0:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   2a7a4:	add	x8, x8, #0x220
   2a7a8:	mov	w9, #0x1d8                 	// #472
   2a7ac:	smaddl	x9, w1, w9, x8
   2a7b0:	ldr	w10, [x9], #4
   2a7b4:	lsr	w8, w10, #27
   2a7b8:	ldr	w10, [x9], #4
   2a7bc:	and	x11, x10, #0x7ffffff
   2a7c0:	lsl	x11, x11, x8
   2a7c4:	cmp	x11, x0
   2a7c8:	b.lt	2a7b4 <__gmpn_fft_best_k@@Base+0x14>  // b.tstop
   2a7cc:	mov	w0, w8
   2a7d0:	ret

000000000002a7d4 <__gmpn_fft_next_size@@Base>:
   2a7d4:	sub	x8, x0, #0x1
   2a7d8:	asr	x8, x8, x1
   2a7dc:	add	x8, x8, #0x1
   2a7e0:	lsl	x0, x8, x1
   2a7e4:	ret

000000000002a7e8 <__gmpn_mul_fft@@Base>:
   2a7e8:	sub	sp, sp, #0xd0
   2a7ec:	sub	x9, x1, #0x1
   2a7f0:	asr	x10, x9, x6
   2a7f4:	cmp	x2, x4
   2a7f8:	add	x10, x10, #0x1
   2a7fc:	cset	w8, eq  // eq = none
   2a800:	cmp	x3, x5
   2a804:	lsl	x10, x10, x6
   2a808:	stp	x29, x30, [sp, #112]
   2a80c:	add	x29, sp, #0x70
   2a810:	cset	w9, eq  // eq = none
   2a814:	cmp	x10, x1
   2a818:	stp	x28, x27, [sp, #128]
   2a81c:	stp	x26, x25, [sp, #144]
   2a820:	stp	x24, x23, [sp, #160]
   2a824:	stp	x22, x21, [sp, #176]
   2a828:	stp	x20, x19, [sp, #192]
   2a82c:	stp	x2, x3, [x29, #-48]
   2a830:	b.ne	2ac6c <__gmpn_mul_fft@@Base+0x484>  // b.any
   2a834:	add	w23, w6, #0x1
   2a838:	mov	x25, x1
   2a83c:	mov	x24, x0
   2a840:	lsl	x21, x1, #6
   2a844:	sbfiz	x1, x23, #3, #32
   2a848:	sub	x0, x29, #0x8
   2a84c:	mov	x19, x5
   2a850:	mov	x27, x4
   2a854:	and	w26, w8, w9
   2a858:	mov	w20, w6
   2a85c:	stp	x6, xzr, [x29, #-16]
   2a860:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a864:	mov	w8, #0x8                   	// #8
   2a868:	mov	x22, x0
   2a86c:	lsl	x1, x8, x20
   2a870:	sub	x0, x29, #0x8
   2a874:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2a878:	ldur	x5, [x29, #-16]
   2a87c:	stur	x24, [x29, #-32]
   2a880:	tbnz	w5, #31, 2aa34 <__gmpn_mul_fft@@Base+0x24c>
   2a884:	mov	x9, xzr
   2a888:	mov	w8, w23
   2a88c:	mov	w10, #0x1                   	// #1
   2a890:	str	x0, [x22, x9, lsl #3]
   2a894:	lsl	x11, x10, x9
   2a898:	add	x9, x9, #0x1
   2a89c:	cmp	x8, x9
   2a8a0:	add	x0, x0, x11, lsl #2
   2a8a4:	b.ne	2a890 <__gmpn_mul_fft@@Base+0xa8>  // b.any
   2a8a8:	ldr	x9, [x22]
   2a8ac:	cmp	w5, #0x0
   2a8b0:	str	wzr, [x9]
   2a8b4:	b.le	2aa3c <__gmpn_mul_fft@@Base+0x254>
   2a8b8:	mov	w9, #0x1                   	// #1
   2a8bc:	mov	w10, #0x1                   	// #1
   2a8c0:	b	2a8d4 <__gmpn_mul_fft@@Base+0xec>
   2a8c4:	add	x9, x9, #0x1
   2a8c8:	cmp	x9, x8
   2a8cc:	lsl	w10, w10, #1
   2a8d0:	b.eq	2a9e0 <__gmpn_mul_fft@@Base+0x1f8>  // b.none
   2a8d4:	cbz	w10, 2a8c4 <__gmpn_mul_fft@@Base+0xdc>
   2a8d8:	add	x12, x22, x9, lsl #3
   2a8dc:	ldr	x11, [x22, x9, lsl #3]
   2a8e0:	ldur	x12, [x12, #-8]
   2a8e4:	sxtw	x13, w10
   2a8e8:	cmp	w10, #0x8
   2a8ec:	mov	w14, w10
   2a8f0:	mov	x15, xzr
   2a8f4:	b.cs	2a934 <__gmpn_mul_fft@@Base+0x14c>  // b.hs, b.nlast
   2a8f8:	add	x16, x15, x13
   2a8fc:	sub	x13, x14, x15
   2a900:	lsl	x15, x15, #2
   2a904:	add	x14, x11, x16, lsl #2
   2a908:	add	x11, x11, x15
   2a90c:	add	x12, x12, x15
   2a910:	ldr	w15, [x12], #4
   2a914:	mov	w16, #0x1                   	// #1
   2a918:	subs	x13, x13, #0x1
   2a91c:	lsl	w17, w15, #1
   2a920:	bfi	w16, w15, #1, #31
   2a924:	str	w17, [x11], #4
   2a928:	str	w16, [x14], #4
   2a92c:	b.ne	2a910 <__gmpn_mul_fft@@Base+0x128>  // b.any
   2a930:	b	2a8c4 <__gmpn_mul_fft@@Base+0xdc>
   2a934:	add	x18, x13, x14
   2a938:	lsl	x16, x14, #2
   2a93c:	add	x1, x11, x18, lsl #2
   2a940:	add	x17, x11, x13, lsl #2
   2a944:	add	x0, x11, x16
   2a948:	cmp	x11, x1
   2a94c:	add	x2, x12, x16
   2a950:	cset	w3, cc  // cc = lo, ul, last
   2a954:	cmp	x17, x0
   2a958:	cset	w4, cc  // cc = lo, ul, last
   2a95c:	cmp	x11, x2
   2a960:	cset	w16, cc  // cc = lo, ul, last
   2a964:	cmp	x12, x0
   2a968:	cset	w18, cc  // cc = lo, ul, last
   2a96c:	cmp	x17, x2
   2a970:	cset	w17, cc  // cc = lo, ul, last
   2a974:	cmp	x12, x1
   2a978:	and	w2, w3, w4
   2a97c:	cset	w0, cc  // cc = lo, ul, last
   2a980:	tbnz	w2, #0, 2a8f8 <__gmpn_mul_fft@@Base+0x110>
   2a984:	and	w16, w16, w18
   2a988:	tbnz	w16, #0, 2a8f8 <__gmpn_mul_fft@@Base+0x110>
   2a98c:	and	w16, w17, w0
   2a990:	tbnz	w16, #0, 2a8f8 <__gmpn_mul_fft@@Base+0x110>
   2a994:	and	x15, x14, #0xfffffff8
   2a998:	add	x16, x12, #0x10
   2a99c:	lsl	x17, x13, #2
   2a9a0:	mov	x18, x15
   2a9a4:	mov	x0, x11
   2a9a8:	ldp	q0, q1, [x16, #-16]
   2a9ac:	add	x1, x0, x17
   2a9b0:	add	x16, x16, #0x20
   2a9b4:	subs	x18, x18, #0x8
   2a9b8:	shl	v0.4s, v0.4s, #1
   2a9bc:	shl	v1.4s, v1.4s, #1
   2a9c0:	stp	q0, q1, [x0], #32
   2a9c4:	orr	v0.4s, #0x1
   2a9c8:	orr	v1.4s, #0x1
   2a9cc:	stp	q0, q1, [x1]
   2a9d0:	b.ne	2a9a8 <__gmpn_mul_fft@@Base+0x1c0>  // b.any
   2a9d4:	cmp	x15, x14
   2a9d8:	b.eq	2a8c4 <__gmpn_mul_fft@@Base+0xdc>  // b.none
   2a9dc:	b	2a8f8 <__gmpn_mul_fft@@Base+0x110>
   2a9e0:	mov	w9, #0x1                   	// #1
   2a9e4:	asr	x8, x21, x20
   2a9e8:	lsl	x28, x9, x20
   2a9ec:	sub	x9, x8, #0x1
   2a9f0:	add	x10, x8, #0x3e
   2a9f4:	cmp	x9, #0x0
   2a9f8:	csel	x9, x10, x9, lt  // lt = tstop
   2a9fc:	asr	x9, x9, #6
   2aa00:	mov	x15, x19
   2aa04:	cmp	w5, #0x1
   2aa08:	add	x19, x9, #0x1
   2aa0c:	mov	w9, #0x40                  	// #64
   2aa10:	b.lt	2aa68 <__gmpn_mul_fft@@Base+0x280>  // b.tstop
   2aa14:	mov	w10, w5
   2aa18:	mov	x11, x9
   2aa1c:	cmp	w10, #0x2
   2aa20:	lsr	x9, x9, #1
   2aa24:	b.lt	2aa68 <__gmpn_mul_fft@@Base+0x280>  // b.tstop
   2aa28:	sub	w10, w10, #0x1
   2aa2c:	tbz	w11, #1, 2aa18 <__gmpn_mul_fft@@Base+0x230>
   2aa30:	b	2aa68 <__gmpn_mul_fft@@Base+0x280>
   2aa34:	ldr	x8, [x22]
   2aa38:	str	wzr, [x8]
   2aa3c:	mov	w9, #0x1                   	// #1
   2aa40:	asr	x8, x21, x20
   2aa44:	lsl	x28, x9, x20
   2aa48:	sub	x9, x8, #0x1
   2aa4c:	add	x10, x8, #0x3e
   2aa50:	cmp	x9, #0x0
   2aa54:	csel	x9, x10, x9, lt  // lt = tstop
   2aa58:	asr	x9, x9, #6
   2aa5c:	mov	x15, x19
   2aa60:	add	x19, x9, #0x1
   2aa64:	mov	w9, #0x40                  	// #64
   2aa68:	lsl	x8, x8, #1
   2aa6c:	add	x8, x8, w5, sxtw
   2aa70:	lsl	x9, x9, x20
   2aa74:	add	x8, x8, #0x2
   2aa78:	sdiv	x8, x8, x9
   2aa7c:	add	x8, x8, #0x1
   2aa80:	mul	x24, x8, x9
   2aa84:	add	x8, x24, #0x3f
   2aa88:	cmp	x24, #0x0
   2aa8c:	mov	w10, #0x13c                 	// #316
   2aa90:	mov	w11, #0x110                 	// #272
   2aa94:	csel	x8, x8, x24, lt  // lt = tstop
   2aa98:	cmp	w26, #0x0
   2aa9c:	asr	x21, x8, #6
   2aaa0:	csel	x8, x11, x10, ne  // ne = any
   2aaa4:	cmp	x21, x8
   2aaa8:	str	x27, [sp, #40]
   2aaac:	b.ge	2ab64 <__gmpn_mul_fft@@Base+0x37c>  // b.tcont
   2aab0:	cmp	x21, x25
   2aab4:	str	x15, [sp, #48]
   2aab8:	stur	w26, [x29, #-52]
   2aabc:	stur	x25, [x29, #-24]
   2aac0:	b.ge	2ac84 <__gmpn_mul_fft@@Base+0x49c>  // b.tcont
   2aac4:	add	x25, x21, #0x1
   2aac8:	lsl	x1, x25, #4
   2aacc:	sub	x0, x29, #0x8
   2aad0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2aad4:	lsl	x8, x25, x20
   2aad8:	asr	x24, x24, x20
   2aadc:	lsl	x20, x8, #3
   2aae0:	mov	x23, x0
   2aae4:	sub	x0, x29, #0x8
   2aae8:	mov	x1, x20
   2aaec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2aaf0:	lsl	x26, x28, #3
   2aaf4:	mov	x27, x0
   2aaf8:	sub	x0, x29, #0x8
   2aafc:	mov	x1, x26
   2ab00:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ab04:	ldp	x4, x5, [x29, #-48]
   2ab08:	mov	x25, x0
   2ab0c:	mov	x0, x27
   2ab10:	mov	x1, x25
   2ab14:	mov	x2, x28
   2ab18:	mov	x3, x21
   2ab1c:	mov	x6, x19
   2ab20:	mov	x7, x24
   2ab24:	str	x23, [sp]
   2ab28:	bl	2ac9c <__gmpn_mul_fft@@Base+0x4b4>
   2ab2c:	ldur	w27, [x29, #-52]
   2ab30:	cbz	w27, 2abc0 <__gmpn_mul_fft@@Base+0x3d8>
   2ab34:	sub	x8, x28, #0x1
   2ab38:	madd	x8, x8, x19, x21
   2ab3c:	lsl	x8, x8, #3
   2ab40:	add	x1, x8, #0x8
   2ab44:	sub	x0, x29, #0x8
   2ab48:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ab4c:	mov	x20, x0
   2ab50:	sub	x0, x29, #0x8
   2ab54:	mov	x1, x26
   2ab58:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ab5c:	mov	x26, x0
   2ab60:	b	2ac04 <__gmpn_mul_fft@@Base+0x41c>
   2ab64:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   2ab68:	add	x8, x8, #0x220
   2ab6c:	mov	w9, #0x1d8                 	// #472
   2ab70:	umaddl	x8, w26, w9, x8
   2ab74:	ldr	w9, [x8], #4
   2ab78:	mov	w10, #0x1                   	// #1
   2ab7c:	mov	x11, x8
   2ab80:	mov	w13, w9
   2ab84:	lsr	w12, w13, #27
   2ab88:	ldr	w13, [x11], #4
   2ab8c:	and	x14, x13, #0x7ffffff
   2ab90:	lsl	x14, x14, x12
   2ab94:	cmp	x14, x21
   2ab98:	b.lt	2ab84 <__gmpn_mul_fft@@Base+0x39c>  // b.tstop
   2ab9c:	lsl	x11, x10, x12
   2aba0:	sub	x12, x11, #0x1
   2aba4:	tst	x12, x21
   2aba8:	b.eq	2aab0 <__gmpn_mul_fft@@Base+0x2c8>  // b.none
   2abac:	add	x12, x12, x21
   2abb0:	neg	x11, x11
   2abb4:	and	x21, x12, x11
   2abb8:	lsl	x24, x21, #6
   2abbc:	b	2ab7c <__gmpn_mul_fft@@Base+0x394>
   2abc0:	sub	x0, x29, #0x8
   2abc4:	mov	x1, x20
   2abc8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2abcc:	mov	x20, x0
   2abd0:	sub	x0, x29, #0x8
   2abd4:	mov	x1, x26
   2abd8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2abdc:	ldp	x4, x5, [sp, #40]
   2abe0:	mov	x26, x0
   2abe4:	mov	x0, x20
   2abe8:	mov	x1, x26
   2abec:	mov	x2, x28
   2abf0:	mov	x3, x21
   2abf4:	mov	x6, x19
   2abf8:	mov	x7, x24
   2abfc:	str	x23, [sp]
   2ac00:	bl	2ac9c <__gmpn_mul_fft@@Base+0x4b4>
   2ac04:	ldp	x0, x1, [x29, #-32]
   2ac08:	ldur	x2, [x29, #-16]
   2ac0c:	mov	x3, x25
   2ac10:	mov	x4, x26
   2ac14:	mov	x5, x20
   2ac18:	mov	x6, x21
   2ac1c:	mov	x7, x19
   2ac20:	str	w27, [sp, #24]
   2ac24:	stp	x22, x23, [sp, #8]
   2ac28:	str	x24, [sp]
   2ac2c:	bl	2b070 <__gmpn_mul_fft@@Base+0x888>
   2ac30:	ldur	x8, [x29, #-8]
   2ac34:	mov	x19, x0
   2ac38:	cbnz	x8, 2ac60 <__gmpn_mul_fft@@Base+0x478>
   2ac3c:	mov	x0, x19
   2ac40:	ldp	x20, x19, [sp, #192]
   2ac44:	ldp	x22, x21, [sp, #176]
   2ac48:	ldp	x24, x23, [sp, #160]
   2ac4c:	ldp	x26, x25, [sp, #144]
   2ac50:	ldp	x28, x27, [sp, #128]
   2ac54:	ldp	x29, x30, [sp, #112]
   2ac58:	add	sp, sp, #0xd0
   2ac5c:	ret
   2ac60:	mov	x0, x8
   2ac64:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2ac68:	b	2ac3c <__gmpn_mul_fft@@Base+0x454>
   2ac6c:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2ac70:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2ac74:	add	x0, x0, #0x1be
   2ac78:	add	x2, x2, #0x1c8
   2ac7c:	mov	w1, #0x365                 	// #869
   2ac80:	bl	c6c0 <__gmp_assert_fail@plt>
   2ac84:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2ac88:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2ac8c:	add	x0, x0, #0x1be
   2ac90:	add	x2, x2, #0x1eb
   2ac94:	mov	w1, #0x38b                 	// #907
   2ac98:	bl	c6c0 <__gmp_assert_fail@plt>
   2ac9c:	sub	sp, sp, #0x90
   2aca0:	stp	x26, x25, [sp, #80]
   2aca4:	mul	x26, x6, x2
   2aca8:	stp	x29, x30, [sp, #48]
   2acac:	stp	x28, x27, [sp, #64]
   2acb0:	stp	x20, x19, [sp, #128]
   2acb4:	add	x29, sp, #0x30
   2acb8:	mov	x19, x5
   2acbc:	mov	x28, x4
   2acc0:	mov	x25, x0
   2acc4:	cmp	x26, x5
   2acc8:	stp	x24, x23, [sp, #96]
   2accc:	stp	x22, x21, [sp, #112]
   2acd0:	stp	x1, x7, [sp, #8]
   2acd4:	str	x3, [sp]
   2acd8:	stp	x6, xzr, [x29, #-16]
   2acdc:	str	x2, [sp, #24]
   2ace0:	b.ge	2af50 <__gmpn_mul_fft@@Base+0x768>  // b.tcont
   2ace4:	sub	x22, x19, x26
   2ace8:	add	x19, x26, #0x1
   2acec:	lsl	x1, x19, #3
   2acf0:	sub	x0, x29, #0x8
   2acf4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2acf8:	mov	x27, x0
   2acfc:	subs	x20, x22, x26
   2ad00:	add	x2, x28, x26, lsl #3
   2ad04:	b.le	2ad7c <__gmpn_mul_fft@@Base+0x594>
   2ad08:	mov	x0, x27
   2ad0c:	mov	x1, x28
   2ad10:	mov	x3, x26
   2ad14:	bl	c2d0 <__gmpn_sub_n@plt>
   2ad18:	mov	x22, x0
   2ad1c:	cmp	x20, x26
   2ad20:	add	x28, x28, x26, lsl #4
   2ad24:	b.le	2ae80 <__gmpn_mul_fft@@Base+0x698>
   2ad28:	mov	w8, wzr
   2ad2c:	mov	w21, wzr
   2ad30:	lsl	x23, x26, #3
   2ad34:	b	2ad5c <__gmpn_mul_fft@@Base+0x574>
   2ad38:	bl	ca70 <__gmpn_add_n@plt>
   2ad3c:	sub	x22, x22, x0
   2ad40:	eor	w21, w21, #0x1
   2ad44:	sub	x20, x20, x26
   2ad48:	cmp	w21, #0x0
   2ad4c:	cset	w8, ne  // ne = any
   2ad50:	cmp	x20, x26
   2ad54:	add	x28, x28, x23
   2ad58:	b.le	2ade8 <__gmpn_mul_fft@@Base+0x600>
   2ad5c:	mov	x0, x27
   2ad60:	mov	x1, x27
   2ad64:	mov	x2, x28
   2ad68:	mov	x3, x26
   2ad6c:	tbz	w8, #0, 2ad38 <__gmpn_mul_fft@@Base+0x550>
   2ad70:	bl	c2d0 <__gmpn_sub_n@plt>
   2ad74:	add	x22, x0, x22
   2ad78:	b	2ad40 <__gmpn_mul_fft@@Base+0x558>
   2ad7c:	cbz	x22, 2adb4 <__gmpn_mul_fft@@Base+0x5cc>
   2ad80:	mov	x0, x27
   2ad84:	mov	x1, x28
   2ad88:	mov	x3, x22
   2ad8c:	bl	c2d0 <__gmpn_sub_n@plt>
   2ad90:	cbz	x0, 2adb4 <__gmpn_mul_fft@@Base+0x5cc>
   2ad94:	cmp	x22, x26
   2ad98:	b.ge	2af08 <__gmpn_mul_fft@@Base+0x720>  // b.tcont
   2ad9c:	lsl	x8, x22, #3
   2ada0:	ldr	x9, [x28, x8]
   2ada4:	add	x22, x22, #0x1
   2ada8:	sub	x10, x9, #0x1
   2adac:	str	x10, [x27, x8]
   2adb0:	cbz	x9, 2ad94 <__gmpn_mul_fft@@Base+0x5ac>
   2adb4:	cmp	x27, x28
   2adb8:	mov	x8, xzr
   2adbc:	b.eq	2af48 <__gmpn_mul_fft@@Base+0x760>  // b.none
   2adc0:	cmp	x22, x26
   2adc4:	b.ge	2af48 <__gmpn_mul_fft@@Base+0x760>  // b.tcont
   2adc8:	lsl	x8, x22, #3
   2adcc:	lsl	x9, x26, #3
   2add0:	add	x0, x27, x8
   2add4:	add	x1, x28, x8
   2add8:	sub	x2, x9, x8
   2addc:	bl	bed0 <memcpy@plt>
   2ade0:	mov	x8, xzr
   2ade4:	b	2af48 <__gmpn_mul_fft@@Base+0x760>
   2ade8:	cbz	w21, 2ae80 <__gmpn_mul_fft@@Base+0x698>
   2adec:	cbz	x20, 2ae2c <__gmpn_mul_fft@@Base+0x644>
   2adf0:	mov	x0, x27
   2adf4:	mov	x1, x27
   2adf8:	mov	x2, x28
   2adfc:	mov	x3, x20
   2ae00:	bl	c2d0 <__gmpn_sub_n@plt>
   2ae04:	cbz	x0, 2ae2c <__gmpn_mul_fft@@Base+0x644>
   2ae08:	mov	w8, #0x1                   	// #1
   2ae0c:	cmp	x20, x26
   2ae10:	b.ge	2ae30 <__gmpn_mul_fft@@Base+0x648>  // b.tcont
   2ae14:	lsl	x9, x20, #3
   2ae18:	ldr	x10, [x27, x9]
   2ae1c:	add	x20, x20, #0x1
   2ae20:	sub	x11, x10, #0x1
   2ae24:	str	x11, [x27, x9]
   2ae28:	cbz	x10, 2ae0c <__gmpn_mul_fft@@Base+0x624>
   2ae2c:	mov	x8, xzr
   2ae30:	add	x9, x8, x22
   2ae34:	tbz	x9, #63, 2aecc <__gmpn_mul_fft@@Base+0x6e4>
   2ae38:	ldr	x10, [x27]
   2ae3c:	neg	x11, x9
   2ae40:	mov	x8, xzr
   2ae44:	add	x9, x10, x9
   2ae48:	cmp	x10, x11
   2ae4c:	str	x9, [x27]
   2ae50:	b.cs	2af48 <__gmpn_mul_fft@@Base+0x760>  // b.hs, b.nlast
   2ae54:	mov	w8, #0x1                   	// #1
   2ae58:	mov	w9, #0x1                   	// #1
   2ae5c:	cmp	x9, x26
   2ae60:	b.ge	2af48 <__gmpn_mul_fft@@Base+0x760>  // b.tcont
   2ae64:	lsl	x10, x9, #3
   2ae68:	ldr	x11, [x27, x10]
   2ae6c:	add	x9, x9, #0x1
   2ae70:	sub	x12, x11, #0x1
   2ae74:	str	x12, [x27, x10]
   2ae78:	cbz	x11, 2ae5c <__gmpn_mul_fft@@Base+0x674>
   2ae7c:	b	2af44 <__gmpn_mul_fft@@Base+0x75c>
   2ae80:	cbz	x20, 2aec0 <__gmpn_mul_fft@@Base+0x6d8>
   2ae84:	mov	x0, x27
   2ae88:	mov	x1, x27
   2ae8c:	mov	x2, x28
   2ae90:	mov	x3, x20
   2ae94:	bl	ca70 <__gmpn_add_n@plt>
   2ae98:	cbz	x0, 2aec0 <__gmpn_mul_fft@@Base+0x6d8>
   2ae9c:	mov	w8, #0x1                   	// #1
   2aea0:	cmp	x20, x26
   2aea4:	b.ge	2aec4 <__gmpn_mul_fft@@Base+0x6dc>  // b.tcont
   2aea8:	lsl	x9, x20, #3
   2aeac:	ldr	x10, [x27, x9]
   2aeb0:	add	x20, x20, #0x1
   2aeb4:	adds	x10, x10, #0x1
   2aeb8:	str	x10, [x27, x9]
   2aebc:	b.cs	2aea0 <__gmpn_mul_fft@@Base+0x6b8>  // b.hs, b.nlast
   2aec0:	mov	x8, xzr
   2aec4:	sub	x9, x22, x8
   2aec8:	tbnz	x9, #63, 2ae38 <__gmpn_mul_fft@@Base+0x650>
   2aecc:	ldr	x8, [x27]
   2aed0:	adds	x8, x8, x9
   2aed4:	str	x8, [x27]
   2aed8:	b.cc	2af44 <__gmpn_mul_fft@@Base+0x75c>  // b.lo, b.ul, b.last
   2aedc:	mov	w8, #0x1                   	// #1
   2aee0:	mov	w9, #0x1                   	// #1
   2aee4:	cmp	x9, x26
   2aee8:	b.ge	2af48 <__gmpn_mul_fft@@Base+0x760>  // b.tcont
   2aeec:	lsl	x10, x9, #3
   2aef0:	ldr	x11, [x27, x10]
   2aef4:	add	x9, x9, #0x1
   2aef8:	adds	x11, x11, #0x1
   2aefc:	str	x11, [x27, x10]
   2af00:	b.cs	2aee4 <__gmpn_mul_fft@@Base+0x6fc>  // b.hs, b.nlast
   2af04:	b	2af44 <__gmpn_mul_fft@@Base+0x75c>
   2af08:	ldr	x8, [x27]
   2af0c:	adds	x8, x8, #0x1
   2af10:	str	x8, [x27]
   2af14:	b.cc	2af44 <__gmpn_mul_fft@@Base+0x75c>  // b.lo, b.ul, b.last
   2af18:	mov	w9, #0x1                   	// #1
   2af1c:	cmp	x9, x26
   2af20:	b.ge	2b048 <__gmpn_mul_fft@@Base+0x860>  // b.tcont
   2af24:	lsl	x10, x9, #3
   2af28:	ldr	x11, [x27, x10]
   2af2c:	mov	x8, xzr
   2af30:	add	x9, x9, #0x1
   2af34:	adds	x11, x11, #0x1
   2af38:	str	x11, [x27, x10]
   2af3c:	b.cs	2af1c <__gmpn_mul_fft@@Base+0x734>  // b.hs, b.nlast
   2af40:	b	2af48 <__gmpn_mul_fft@@Base+0x760>
   2af44:	mov	x8, xzr
   2af48:	mov	x28, x27
   2af4c:	str	x8, [x27, x26, lsl #3]
   2af50:	ldr	x8, [sp, #24]
   2af54:	subs	x24, x8, #0x1
   2af58:	b.lt	2b01c <__gmpn_mul_fft@@Base+0x834>  // b.tstop
   2af5c:	ldr	x8, [sp]
   2af60:	ldr	x22, [x29, #96]
   2af64:	mov	x20, xzr
   2af68:	mov	x23, xzr
   2af6c:	add	x21, x8, #0x1
   2af70:	lsl	x8, x8, #3
   2af74:	add	x26, x8, #0x8
   2af78:	b	2afb4 <__gmpn_mul_fft@@Base+0x7cc>
   2af7c:	ldur	x8, [x29, #-16]
   2af80:	ldr	x3, [sp]
   2af84:	mov	x0, x25
   2af88:	mov	x1, x22
   2af8c:	mov	x2, x20
   2af90:	add	x28, x28, x8, lsl #3
   2af94:	bl	2c054 <__gmpn_mul_fft@@Base+0x186c>
   2af98:	ldr	x8, [sp, #24]
   2af9c:	add	x23, x23, #0x1
   2afa0:	add	x25, x25, x26
   2afa4:	cmp	x8, x23
   2afa8:	ldr	x8, [sp, #16]
   2afac:	add	x20, x20, x8
   2afb0:	b.eq	2b01c <__gmpn_mul_fft@@Base+0x834>  // b.none
   2afb4:	ldr	x8, [sp, #8]
   2afb8:	cmp	x19, #0x1
   2afbc:	str	x25, [x8, x23, lsl #3]
   2afc0:	b.lt	2b004 <__gmpn_mul_fft@@Base+0x81c>  // b.tstop
   2afc4:	ldur	x8, [x29, #-16]
   2afc8:	mov	x0, x22
   2afcc:	mov	x1, x28
   2afd0:	cmp	x19, x8
   2afd4:	ccmp	x23, x24, #0x0, ge  // ge = tcont
   2afd8:	csel	x27, x8, x19, lt  // lt = tstop
   2afdc:	mov	x2, x27
   2afe0:	sub	x19, x19, x27
   2afe4:	bl	ca50 <__gmpn_copyi@plt>
   2afe8:	subs	x8, x21, x27
   2afec:	b.eq	2af7c <__gmpn_mul_fft@@Base+0x794>  // b.none
   2aff0:	add	x0, x22, x27, lsl #3
   2aff4:	lsl	x2, x8, #3
   2aff8:	mov	w1, wzr
   2affc:	bl	c5f0 <memset@plt>
   2b000:	b	2af7c <__gmpn_mul_fft@@Base+0x794>
   2b004:	cbz	x21, 2af98 <__gmpn_mul_fft@@Base+0x7b0>
   2b008:	mov	x0, x25
   2b00c:	mov	w1, wzr
   2b010:	mov	x2, x26
   2b014:	bl	c5f0 <memset@plt>
   2b018:	b	2af98 <__gmpn_mul_fft@@Base+0x7b0>
   2b01c:	cbnz	x19, 2b058 <__gmpn_mul_fft@@Base+0x870>
   2b020:	ldur	x0, [x29, #-8]
   2b024:	cbnz	x0, 2b050 <__gmpn_mul_fft@@Base+0x868>
   2b028:	ldp	x20, x19, [sp, #128]
   2b02c:	ldp	x22, x21, [sp, #112]
   2b030:	ldp	x24, x23, [sp, #96]
   2b034:	ldp	x26, x25, [sp, #80]
   2b038:	ldp	x28, x27, [sp, #64]
   2b03c:	ldp	x29, x30, [sp, #48]
   2b040:	add	sp, sp, #0x90
   2b044:	ret
   2b048:	mov	w8, #0x1                   	// #1
   2b04c:	b	2af48 <__gmpn_mul_fft@@Base+0x760>
   2b050:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2b054:	b	2b028 <__gmpn_mul_fft@@Base+0x840>
   2b058:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2b05c:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2b060:	add	x0, x0, #0x1be
   2b064:	add	x2, x2, #0x1f7
   2b068:	mov	w1, #0x2e7                 	// #743
   2b06c:	bl	c6c0 <__gmp_assert_fail@plt>
   2b070:	sub	sp, sp, #0x160
   2b074:	stp	x29, x30, [sp, #256]
   2b078:	add	x29, sp, #0x100
   2b07c:	ldp	x10, x8, [x29, #96]
   2b080:	stp	x26, x25, [sp, #288]
   2b084:	ldr	x26, [x29, #112]
   2b088:	stp	x20, x19, [sp, #336]
   2b08c:	ldr	w20, [x29, #120]
   2b090:	mov	w9, #0x1                   	// #1
   2b094:	stp	x28, x27, [sp, #272]
   2b098:	lsl	x28, x9, x2
   2b09c:	add	x19, x8, w2, sxtw #3
   2b0a0:	lsl	x25, x10, #1
   2b0a4:	stp	x24, x23, [sp, #304]
   2b0a8:	stp	x22, x21, [sp, #320]
   2b0ac:	stp	x7, x5, [x29, #-64]
   2b0b0:	mov	x21, x6
   2b0b4:	mov	x24, x4
   2b0b8:	mov	x22, x3
   2b0bc:	mov	x27, x1
   2b0c0:	str	x0, [sp, #64]
   2b0c4:	str	x2, [sp, #80]
   2b0c8:	mov	w5, #0x1                   	// #1
   2b0cc:	mov	x0, x3
   2b0d0:	mov	x1, x28
   2b0d4:	mov	x2, x19
   2b0d8:	mov	x3, x25
   2b0dc:	mov	x4, x6
   2b0e0:	mov	x6, x26
   2b0e4:	str	x10, [sp, #56]
   2b0e8:	bl	2c32c <__gmpn_mul_fft@@Base+0x1b44>
   2b0ec:	cbnz	w20, 2b110 <__gmpn_mul_fft@@Base+0x928>
   2b0f0:	mov	w5, #0x1                   	// #1
   2b0f4:	mov	x0, x24
   2b0f8:	mov	x1, x28
   2b0fc:	mov	x2, x19
   2b100:	mov	x3, x25
   2b104:	mov	x4, x21
   2b108:	mov	x6, x26
   2b10c:	bl	2c32c <__gmpn_mul_fft@@Base+0x1b44>
   2b110:	cmp	w20, #0x0
   2b114:	csel	x17, x22, x24, ne  // ne = any
   2b118:	cmp	x17, x22
   2b11c:	mov	w8, #0x13c                 	// #316
   2b120:	mov	w9, #0x110                 	// #272
   2b124:	cset	w10, eq  // eq = none
   2b128:	stur	w10, [x29, #-96]
   2b12c:	csel	x10, x9, x8, eq  // eq = none
   2b130:	cmp	x10, x21
   2b134:	lsl	x16, x21, #3
   2b138:	mov	x15, x21
   2b13c:	stp	x21, xzr, [x29, #-24]
   2b140:	str	x27, [sp, #72]
   2b144:	str	x16, [sp, #104]
   2b148:	stp	x28, x24, [x29, #-40]
   2b14c:	stur	x22, [x29, #-48]
   2b150:	str	x25, [sp, #48]
   2b154:	stur	x17, [x29, #-72]
   2b158:	str	x26, [sp, #40]
   2b15c:	b.le	2b2e0 <__gmpn_mul_fft@@Base+0xaf8>
   2b160:	lsl	x1, x15, #4
   2b164:	sub	x0, x29, #0x10
   2b168:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b16c:	ldr	x8, [sp, #80]
   2b170:	ldur	x3, [x29, #-24]
   2b174:	cmp	w8, #0x3f
   2b178:	b.eq	2b568 <__gmpn_mul_fft@@Base+0xd80>  // b.none
   2b17c:	ldur	x22, [x29, #-48]
   2b180:	ldur	x28, [x29, #-72]
   2b184:	mov	x19, x0
   2b188:	mov	x25, xzr
   2b18c:	lsl	x27, x3, #1
   2b190:	add	x20, x0, x3, lsl #3
   2b194:	b	2b1b0 <__gmpn_mul_fft@@Base+0x9c8>
   2b198:	mov	w8, #0x1                   	// #1
   2b19c:	ldur	x9, [x29, #-40]
   2b1a0:	add	x25, x25, #0x1
   2b1a4:	str	x8, [x21, x3, lsl #3]
   2b1a8:	cmp	x9, x25
   2b1ac:	b.le	2b568 <__gmpn_mul_fft@@Base+0xd80>
   2b1b0:	ldr	x21, [x22], #8
   2b1b4:	ldr	x23, [x28], #8
   2b1b8:	ldur	x8, [x29, #-48]
   2b1bc:	ldur	x9, [x29, #-72]
   2b1c0:	mov	x0, x19
   2b1c4:	cmp	x9, x8
   2b1c8:	b.eq	2b1e0 <__gmpn_mul_fft@@Base+0x9f8>  // b.none
   2b1cc:	mov	x1, x23
   2b1d0:	mov	x2, x21
   2b1d4:	mov	x24, x3
   2b1d8:	bl	c990 <__gmpn_mul_n@plt>
   2b1dc:	b	2b1f0 <__gmpn_mul_fft@@Base+0xa08>
   2b1e0:	mov	x1, x21
   2b1e4:	mov	x2, x3
   2b1e8:	mov	x24, x3
   2b1ec:	bl	c8e0 <__gmpn_sqr@plt>
   2b1f0:	ldr	x8, [x21, x24, lsl #3]
   2b1f4:	mov	x3, x24
   2b1f8:	cbz	x8, 2b2d0 <__gmpn_mul_fft@@Base+0xae8>
   2b1fc:	mov	x0, x20
   2b200:	mov	x1, x20
   2b204:	mov	x2, x23
   2b208:	bl	ca70 <__gmpn_add_n@plt>
   2b20c:	ldur	x3, [x29, #-24]
   2b210:	mov	x24, x0
   2b214:	ldr	x8, [x23, x3, lsl #3]
   2b218:	cbz	x8, 2b23c <__gmpn_mul_fft@@Base+0xa54>
   2b21c:	mov	x0, x20
   2b220:	mov	x1, x20
   2b224:	mov	x2, x21
   2b228:	bl	ca70 <__gmpn_add_n@plt>
   2b22c:	ldur	x3, [x29, #-24]
   2b230:	add	x9, x0, x24
   2b234:	ldr	x8, [x21, x3, lsl #3]
   2b238:	add	x24, x9, x8
   2b23c:	cbz	x24, 2b274 <__gmpn_mul_fft@@Base+0xa8c>
   2b240:	ldr	x8, [x19]
   2b244:	adds	x8, x8, x24
   2b248:	str	x8, [x19]
   2b24c:	b.cc	2b274 <__gmpn_mul_fft@@Base+0xa8c>  // b.lo, b.ul, b.last
   2b250:	mov	w8, #0x1                   	// #1
   2b254:	cmp	x8, x27
   2b258:	b.ge	2b274 <__gmpn_mul_fft@@Base+0xa8c>  // b.tcont
   2b25c:	lsl	x9, x8, #3
   2b260:	ldr	x10, [x19, x9]
   2b264:	add	x8, x8, #0x1
   2b268:	adds	x10, x10, #0x1
   2b26c:	str	x10, [x19, x9]
   2b270:	b.cs	2b254 <__gmpn_mul_fft@@Base+0xa6c>  // b.hs, b.nlast
   2b274:	mov	x0, x21
   2b278:	mov	x1, x19
   2b27c:	mov	x2, x20
   2b280:	bl	c2d0 <__gmpn_sub_n@plt>
   2b284:	cbz	x0, 2b2c4 <__gmpn_mul_fft@@Base+0xadc>
   2b288:	ldr	x8, [x21]
   2b28c:	adds	x8, x8, #0x1
   2b290:	str	x8, [x21]
   2b294:	b.cc	2b2c4 <__gmpn_mul_fft@@Base+0xadc>  // b.lo, b.ul, b.last
   2b298:	ldur	x3, [x29, #-24]
   2b29c:	mov	w8, #0x1                   	// #1
   2b2a0:	cmp	x8, x3
   2b2a4:	b.ge	2b198 <__gmpn_mul_fft@@Base+0x9b0>  // b.tcont
   2b2a8:	lsl	x9, x8, #3
   2b2ac:	ldr	x10, [x21, x9]
   2b2b0:	add	x8, x8, #0x1
   2b2b4:	adds	x10, x10, #0x1
   2b2b8:	str	x10, [x21, x9]
   2b2bc:	b.cs	2b2a0 <__gmpn_mul_fft@@Base+0xab8>  // b.hs, b.nlast
   2b2c0:	b	2b2c8 <__gmpn_mul_fft@@Base+0xae0>
   2b2c4:	ldur	x3, [x29, #-24]
   2b2c8:	mov	x8, xzr
   2b2cc:	b	2b19c <__gmpn_mul_fft@@Base+0x9b4>
   2b2d0:	mov	x24, xzr
   2b2d4:	ldr	x8, [x23, x3, lsl #3]
   2b2d8:	cbnz	x8, 2b21c <__gmpn_mul_fft@@Base+0xa34>
   2b2dc:	b	2b23c <__gmpn_mul_fft@@Base+0xa54>
   2b2e0:	cmp	x17, x22
   2b2e4:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   2b2e8:	add	x8, x8, #0x220
   2b2ec:	cset	w9, eq  // eq = none
   2b2f0:	mov	w11, #0x1d8                 	// #472
   2b2f4:	umaddl	x8, w9, w11, x8
   2b2f8:	ldr	w9, [x8], #4
   2b2fc:	mov	x11, x8
   2b300:	mov	w12, w9
   2b304:	lsr	w19, w12, #27
   2b308:	ldr	w12, [x11], #4
   2b30c:	and	x13, x12, #0x7ffffff
   2b310:	lsl	x13, x13, x19
   2b314:	cmp	x13, x15
   2b318:	b.lt	2b304 <__gmpn_mul_fft@@Base+0xb1c>  // b.tstop
   2b31c:	mov	w11, #0x1                   	// #1
   2b320:	lsl	x18, x11, x19
   2b324:	sub	x11, x18, #0x1
   2b328:	tst	x11, x15
   2b32c:	b.ne	2c024 <__gmpn_mul_fft@@Base+0x183c>  // b.any
   2b330:	lsl	x12, x15, #6
   2b334:	cmp	x18, #0x40
   2b338:	mov	w11, #0x40                  	// #64
   2b33c:	add	w13, w19, #0x2
   2b340:	asr	x12, x12, x19
   2b344:	csel	x11, x18, x11, gt
   2b348:	add	x12, x13, x12, lsl #1
   2b34c:	add	x12, x12, x11
   2b350:	sdiv	x12, x12, x11
   2b354:	mul	x21, x12, x11
   2b358:	add	x11, x21, #0x3f
   2b35c:	cmp	x21, #0x0
   2b360:	csel	x11, x11, x21, lt  // lt = tstop
   2b364:	asr	x23, x11, #6
   2b368:	cmp	x23, x10
   2b36c:	b.ge	2b5f8 <__gmpn_mul_fft@@Base+0xe10>  // b.tcont
   2b370:	cmp	x23, x15
   2b374:	b.ge	2c03c <__gmpn_mul_fft@@Base+0x1854>  // b.tcont
   2b378:	lsl	x20, x18, #3
   2b37c:	sub	x0, x29, #0x10
   2b380:	mov	x1, x20
   2b384:	asr	x22, x15, x19
   2b388:	stur	x18, [x29, #-120]
   2b38c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b390:	stur	x0, [x29, #-88]
   2b394:	sub	x0, x29, #0x10
   2b398:	mov	x1, x20
   2b39c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3a0:	add	x20, x23, #0x1
   2b3a4:	lsl	x8, x20, #1
   2b3a8:	lsl	x8, x8, x19
   2b3ac:	mov	x24, x0
   2b3b0:	lsl	x1, x8, #3
   2b3b4:	sub	x0, x29, #0x10
   2b3b8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3bc:	stur	x0, [x29, #-112]
   2b3c0:	lsl	x1, x20, #4
   2b3c4:	sub	x0, x29, #0x10
   2b3c8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3cc:	add	w25, w19, #0x1
   2b3d0:	str	x0, [sp, #128]
   2b3d4:	lsl	x8, x20, x19
   2b3d8:	lsl	w1, w25, #3
   2b3dc:	sub	x0, x29, #0x10
   2b3e0:	stur	x8, [x29, #-80]
   2b3e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3e8:	mov	w8, #0x8                   	// #8
   2b3ec:	mov	x20, x0
   2b3f0:	lsl	x1, x8, x19
   2b3f4:	sub	x0, x29, #0x10
   2b3f8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2b3fc:	mov	x8, xzr
   2b400:	mov	w9, #0x1                   	// #1
   2b404:	str	x0, [x20, x8, lsl #3]
   2b408:	lsl	x10, x9, x8
   2b40c:	add	x8, x8, #0x1
   2b410:	cmp	x25, x8
   2b414:	add	x0, x0, x10, lsl #2
   2b418:	b.ne	2b404 <__gmpn_mul_fft@@Base+0xc1c>  // b.any
   2b41c:	ldr	x8, [x20]
   2b420:	ldur	x3, [x29, #-24]
   2b424:	str	wzr, [x8]
   2b428:	cbz	w19, 2b554 <__gmpn_mul_fft@@Base+0xd6c>
   2b42c:	mov	w8, #0x1                   	// #1
   2b430:	mov	w9, #0x1                   	// #1
   2b434:	b	2b448 <__gmpn_mul_fft@@Base+0xc60>
   2b438:	add	x8, x8, #0x1
   2b43c:	cmp	x8, x25
   2b440:	lsl	w9, w9, #1
   2b444:	b.eq	2b554 <__gmpn_mul_fft@@Base+0xd6c>  // b.none
   2b448:	cbz	w9, 2b438 <__gmpn_mul_fft@@Base+0xc50>
   2b44c:	add	x11, x20, x8, lsl #3
   2b450:	ldr	x10, [x20, x8, lsl #3]
   2b454:	ldur	x11, [x11, #-8]
   2b458:	sxtw	x12, w9
   2b45c:	cmp	w9, #0x8
   2b460:	mov	w13, w9
   2b464:	mov	x14, xzr
   2b468:	b.cs	2b4a8 <__gmpn_mul_fft@@Base+0xcc0>  // b.hs, b.nlast
   2b46c:	add	x15, x14, x12
   2b470:	sub	x12, x13, x14
   2b474:	lsl	x14, x14, #2
   2b478:	add	x13, x10, x15, lsl #2
   2b47c:	add	x10, x10, x14
   2b480:	add	x11, x11, x14
   2b484:	ldr	w14, [x11], #4
   2b488:	mov	w15, #0x1                   	// #1
   2b48c:	subs	x12, x12, #0x1
   2b490:	lsl	w16, w14, #1
   2b494:	bfi	w15, w14, #1, #31
   2b498:	str	w16, [x10], #4
   2b49c:	str	w15, [x13], #4
   2b4a0:	b.ne	2b484 <__gmpn_mul_fft@@Base+0xc9c>  // b.any
   2b4a4:	b	2b438 <__gmpn_mul_fft@@Base+0xc50>
   2b4a8:	add	x17, x12, x13
   2b4ac:	lsl	x15, x13, #2
   2b4b0:	add	x0, x10, x17, lsl #2
   2b4b4:	add	x16, x10, x12, lsl #2
   2b4b8:	add	x18, x10, x15
   2b4bc:	cmp	x10, x0
   2b4c0:	add	x1, x11, x15
   2b4c4:	cset	w2, cc  // cc = lo, ul, last
   2b4c8:	cmp	x16, x18
   2b4cc:	cset	w4, cc  // cc = lo, ul, last
   2b4d0:	cmp	x10, x1
   2b4d4:	cset	w15, cc  // cc = lo, ul, last
   2b4d8:	cmp	x11, x18
   2b4dc:	cset	w17, cc  // cc = lo, ul, last
   2b4e0:	cmp	x16, x1
   2b4e4:	cset	w16, cc  // cc = lo, ul, last
   2b4e8:	cmp	x11, x0
   2b4ec:	and	w1, w2, w4
   2b4f0:	cset	w18, cc  // cc = lo, ul, last
   2b4f4:	tbnz	w1, #0, 2b46c <__gmpn_mul_fft@@Base+0xc84>
   2b4f8:	and	w15, w15, w17
   2b4fc:	tbnz	w15, #0, 2b46c <__gmpn_mul_fft@@Base+0xc84>
   2b500:	and	w15, w16, w18
   2b504:	tbnz	w15, #0, 2b46c <__gmpn_mul_fft@@Base+0xc84>
   2b508:	and	x14, x13, #0xfffffff8
   2b50c:	add	x15, x11, #0x10
   2b510:	lsl	x16, x12, #2
   2b514:	mov	x17, x14
   2b518:	mov	x18, x10
   2b51c:	ldp	q0, q1, [x15, #-16]
   2b520:	add	x0, x18, x16
   2b524:	add	x15, x15, #0x20
   2b528:	subs	x17, x17, #0x8
   2b52c:	shl	v0.4s, v0.4s, #1
   2b530:	shl	v1.4s, v1.4s, #1
   2b534:	stp	q0, q1, [x18], #32
   2b538:	orr	v0.4s, #0x1
   2b53c:	orr	v1.4s, #0x1
   2b540:	stp	q0, q1, [x0]
   2b544:	b.ne	2b51c <__gmpn_mul_fft@@Base+0xd34>  // b.any
   2b548:	cmp	x14, x13
   2b54c:	b.eq	2b438 <__gmpn_mul_fft@@Base+0xc50>  // b.none
   2b550:	b	2b46c <__gmpn_mul_fft@@Base+0xc84>
   2b554:	ldr	x8, [sp, #80]
   2b558:	str	x20, [sp, #120]
   2b55c:	stur	x22, [x29, #-104]
   2b560:	cmp	w8, #0x3f
   2b564:	b.ne	2b640 <__gmpn_mul_fft@@Base+0xe58>  // b.any
   2b568:	ldr	x8, [sp, #80]
   2b56c:	ldur	x0, [x29, #-16]
   2b570:	sxtw	x19, w8
   2b574:	cbnz	x0, 2c018 <__gmpn_mul_fft@@Base+0x1830>
   2b578:	ldp	x27, x25, [x29, #-48]
   2b57c:	ldp	x26, x2, [sp, #40]
   2b580:	mov	x0, x27
   2b584:	mov	x1, x25
   2b588:	mov	x4, x26
   2b58c:	bl	2c5e4 <__gmpn_mul_fft@@Base+0x1dfc>
   2b590:	ldr	x23, [sp, #104]
   2b594:	ldur	x3, [x29, #-24]
   2b598:	add	x8, x26, x23
   2b59c:	add	x20, x8, #0x8
   2b5a0:	ldp	x24, x8, [x29, #-32]
   2b5a4:	mov	x0, x20
   2b5a8:	str	x20, [x24]
   2b5ac:	ldr	x1, [x27]
   2b5b0:	lsl	x8, x8, #7
   2b5b4:	sub	x19, x8, x19
   2b5b8:	mov	x2, x19
   2b5bc:	bl	2c054 <__gmpn_mul_fft@@Base+0x186c>
   2b5c0:	ldur	x3, [x29, #-24]
   2b5c4:	ldr	x8, [x20, x23]
   2b5c8:	ldr	x22, [sp, #72]
   2b5cc:	ldr	x28, [sp, #56]
   2b5d0:	cbz	x8, 2b84c <__gmpn_mul_fft@@Base+0x1064>
   2b5d4:	mov	x8, x20
   2b5d8:	ldr	x9, [x8]
   2b5dc:	sub	x10, x9, #0x1
   2b5e0:	str	x10, [x8], #8
   2b5e4:	cbz	x9, 2b5d8 <__gmpn_mul_fft@@Base+0xdf0>
   2b5e8:	ldr	x8, [x20, x3, lsl #3]
   2b5ec:	cbz	x8, 2b82c <__gmpn_mul_fft@@Base+0x1044>
   2b5f0:	mov	x8, xzr
   2b5f4:	b	2b848 <__gmpn_mul_fft@@Base+0x1060>
   2b5f8:	mov	w10, #0x1                   	// #1
   2b5fc:	mov	x11, x8
   2b600:	mov	w13, w9
   2b604:	lsr	w12, w13, #27
   2b608:	ldr	w13, [x11], #4
   2b60c:	and	x14, x13, #0x7ffffff
   2b610:	lsl	x14, x14, x12
   2b614:	cmp	x14, x23
   2b618:	b.lt	2b604 <__gmpn_mul_fft@@Base+0xe1c>  // b.tstop
   2b61c:	lsl	x11, x10, x12
   2b620:	sub	x12, x11, #0x1
   2b624:	tst	x12, x23
   2b628:	b.eq	2b370 <__gmpn_mul_fft@@Base+0xb88>  // b.none
   2b62c:	add	x12, x12, x23
   2b630:	neg	x11, x11
   2b634:	and	x23, x12, x11
   2b638:	lsl	x21, x23, #6
   2b63c:	b	2b5fc <__gmpn_mul_fft@@Base+0xe14>
   2b640:	ldur	x8, [x29, #-112]
   2b644:	ldur	x9, [x29, #-80]
   2b648:	asr	x27, x21, x19
   2b64c:	ldur	x28, [x29, #-48]
   2b650:	ldur	x21, [x29, #-72]
   2b654:	add	x26, x8, x9, lsl #3
   2b658:	ldur	x8, [x29, #-104]
   2b65c:	mov	x25, xzr
   2b660:	stp	x27, x19, [sp, #88]
   2b664:	lsl	x8, x8, x19
   2b668:	add	x8, x8, #0x1
   2b66c:	str	x8, [sp, #112]
   2b670:	b	2b704 <__gmpn_mul_fft@@Base+0xf1c>
   2b674:	mov	x1, x22
   2b678:	mov	x25, x22
   2b67c:	ldur	x22, [x29, #-104]
   2b680:	ldr	x4, [x28]
   2b684:	ldr	x20, [sp, #128]
   2b688:	ldp	x2, x0, [x29, #-120]
   2b68c:	ldr	x5, [sp, #112]
   2b690:	mov	x3, x23
   2b694:	mov	x6, x22
   2b698:	mov	x7, x27
   2b69c:	str	x20, [sp]
   2b6a0:	bl	2ac9c <__gmpn_mul_fft@@Base+0x4b4>
   2b6a4:	ldur	w8, [x29, #-96]
   2b6a8:	ldr	x0, [x28]
   2b6ac:	mov	w2, w19
   2b6b0:	mov	x3, x25
   2b6b4:	str	w8, [sp, #24]
   2b6b8:	ldr	x8, [sp, #120]
   2b6bc:	mov	x4, x24
   2b6c0:	mov	x5, x26
   2b6c4:	mov	x6, x23
   2b6c8:	stp	x8, x20, [sp, #8]
   2b6cc:	ldur	x20, [x29, #-24]
   2b6d0:	mov	x7, x22
   2b6d4:	str	x27, [sp]
   2b6d8:	mov	x1, x20
   2b6dc:	bl	2b070 <__gmpn_mul_fft@@Base+0x888>
   2b6e0:	ldr	x8, [x28], #8
   2b6e4:	ldur	x25, [x29, #-80]
   2b6e8:	ldur	x9, [x29, #-40]
   2b6ec:	mov	x3, x20
   2b6f0:	add	x21, x21, #0x8
   2b6f4:	add	x25, x25, #0x1
   2b6f8:	cmp	x9, x25
   2b6fc:	str	x0, [x8, x3, lsl #3]
   2b700:	b.le	2b568 <__gmpn_mul_fft@@Base+0xd80>
   2b704:	ldr	x20, [x28]
   2b708:	ldur	x22, [x29, #-88]
   2b70c:	ldr	x8, [x20, x3, lsl #3]
   2b710:	cbz	x8, 2b75c <__gmpn_mul_fft@@Base+0xf74>
   2b714:	mov	x8, x20
   2b718:	ldr	x9, [x8]
   2b71c:	sub	x10, x9, #0x1
   2b720:	str	x10, [x8], #8
   2b724:	cbz	x9, 2b718 <__gmpn_mul_fft@@Base+0xf30>
   2b728:	ldr	x9, [x20, x3, lsl #3]
   2b72c:	cmp	x9, #0x0
   2b730:	cset	w8, eq  // eq = none
   2b734:	cbnz	x9, 2b758 <__gmpn_mul_fft@@Base+0xf70>
   2b738:	cbz	x3, 2b758 <__gmpn_mul_fft@@Base+0xf70>
   2b73c:	ldr	x2, [sp, #104]
   2b740:	mov	x0, x20
   2b744:	mov	w1, wzr
   2b748:	bl	c5f0 <memset@plt>
   2b74c:	ldr	x27, [sp, #88]
   2b750:	ldur	x3, [x29, #-24]
   2b754:	mov	w8, #0x1                   	// #1
   2b758:	str	x8, [x20, x3, lsl #3]
   2b75c:	ldur	x8, [x29, #-48]
   2b760:	ldur	x9, [x29, #-72]
   2b764:	stur	x25, [x29, #-80]
   2b768:	cmp	x9, x8
   2b76c:	b.eq	2b674 <__gmpn_mul_fft@@Base+0xe8c>  // b.none
   2b770:	ldr	x20, [x21]
   2b774:	ldr	x8, [x20, x3, lsl #3]
   2b778:	cbz	x8, 2b7c4 <__gmpn_mul_fft@@Base+0xfdc>
   2b77c:	mov	x8, x20
   2b780:	ldr	x9, [x8]
   2b784:	sub	x10, x9, #0x1
   2b788:	str	x10, [x8], #8
   2b78c:	cbz	x9, 2b780 <__gmpn_mul_fft@@Base+0xf98>
   2b790:	ldr	x9, [x20, x3, lsl #3]
   2b794:	cmp	x9, #0x0
   2b798:	cset	w8, eq  // eq = none
   2b79c:	cbnz	x9, 2b7c0 <__gmpn_mul_fft@@Base+0xfd8>
   2b7a0:	cbz	x3, 2b7c0 <__gmpn_mul_fft@@Base+0xfd8>
   2b7a4:	ldr	x2, [sp, #104]
   2b7a8:	mov	x0, x20
   2b7ac:	mov	w1, wzr
   2b7b0:	bl	c5f0 <memset@plt>
   2b7b4:	ldr	x27, [sp, #88]
   2b7b8:	ldur	x3, [x29, #-24]
   2b7bc:	mov	w8, #0x1                   	// #1
   2b7c0:	str	x8, [x20, x3, lsl #3]
   2b7c4:	ldp	x19, x0, [x29, #-120]
   2b7c8:	mov	x1, x22
   2b7cc:	ldr	x25, [sp, #112]
   2b7d0:	ldur	x22, [x29, #-104]
   2b7d4:	ldr	x4, [x28]
   2b7d8:	ldr	x20, [sp, #128]
   2b7dc:	mov	x2, x19
   2b7e0:	mov	x3, x23
   2b7e4:	mov	x5, x25
   2b7e8:	mov	x6, x22
   2b7ec:	mov	x7, x27
   2b7f0:	str	x20, [sp]
   2b7f4:	bl	2ac9c <__gmpn_mul_fft@@Base+0x4b4>
   2b7f8:	ldr	x4, [x21]
   2b7fc:	mov	x5, x25
   2b800:	ldur	x25, [x29, #-88]
   2b804:	mov	x0, x26
   2b808:	mov	x1, x24
   2b80c:	mov	x2, x19
   2b810:	mov	x3, x23
   2b814:	mov	x6, x22
   2b818:	mov	x7, x27
   2b81c:	str	x20, [sp]
   2b820:	bl	2ac9c <__gmpn_mul_fft@@Base+0x4b4>
   2b824:	ldr	x19, [sp, #96]
   2b828:	b	2b6a4 <__gmpn_mul_fft@@Base+0xebc>
   2b82c:	cbz	x3, 2b844 <__gmpn_mul_fft@@Base+0x105c>
   2b830:	mov	x0, x20
   2b834:	mov	w1, wzr
   2b838:	mov	x2, x23
   2b83c:	bl	c5f0 <memset@plt>
   2b840:	ldur	x3, [x29, #-24]
   2b844:	mov	w8, #0x1                   	// #1
   2b848:	str	x8, [x20, x3, lsl #3]
   2b84c:	cmp	x25, #0x2
   2b850:	b.lt	2b8e0 <__gmpn_mul_fft@@Base+0x10f8>  // b.tstop
   2b854:	mov	w21, #0x1                   	// #1
   2b858:	b	2b86c <__gmpn_mul_fft@@Base+0x1084>
   2b85c:	str	x8, [x20, x3, lsl #3]
   2b860:	add	x21, x21, #0x1
   2b864:	cmp	x21, x25
   2b868:	b.eq	2b8e0 <__gmpn_mul_fft@@Base+0x10f8>  // b.none
   2b86c:	lsl	x8, x21, #3
   2b870:	add	x9, x27, x8
   2b874:	ldur	x20, [x9, #-8]
   2b878:	str	x20, [x24, x8]
   2b87c:	ldr	x1, [x9]
   2b880:	sub	x8, x25, x21
   2b884:	msub	x2, x8, x28, x19
   2b888:	mov	x0, x20
   2b88c:	bl	2c054 <__gmpn_mul_fft@@Base+0x186c>
   2b890:	ldur	x3, [x29, #-24]
   2b894:	ldr	x8, [x20, x3, lsl #3]
   2b898:	cbz	x8, 2b860 <__gmpn_mul_fft@@Base+0x1078>
   2b89c:	mov	x8, x20
   2b8a0:	ldr	x9, [x8]
   2b8a4:	sub	x10, x9, #0x1
   2b8a8:	str	x10, [x8], #8
   2b8ac:	cbz	x9, 2b8a0 <__gmpn_mul_fft@@Base+0x10b8>
   2b8b0:	ldr	x9, [x20, x3, lsl #3]
   2b8b4:	cmp	x9, #0x0
   2b8b8:	cset	w8, eq  // eq = none
   2b8bc:	cbnz	x9, 2b85c <__gmpn_mul_fft@@Base+0x1074>
   2b8c0:	cbz	x3, 2b85c <__gmpn_mul_fft@@Base+0x1074>
   2b8c4:	mov	x0, x20
   2b8c8:	mov	w1, wzr
   2b8cc:	mov	x2, x23
   2b8d0:	bl	c5f0 <memset@plt>
   2b8d4:	ldur	x3, [x29, #-24]
   2b8d8:	mov	w8, #0x1                   	// #1
   2b8dc:	b	2b85c <__gmpn_mul_fft@@Base+0x1074>
   2b8e0:	adds	x8, x3, #0x1
   2b8e4:	stur	x8, [x29, #-80]
   2b8e8:	b.cs	2b900 <__gmpn_mul_fft@@Base+0x1118>  // b.hs, b.nlast
   2b8ec:	add	x2, x23, #0x8
   2b8f0:	mov	x0, x26
   2b8f4:	mov	w1, wzr
   2b8f8:	bl	c5f0 <memset@plt>
   2b8fc:	ldur	x3, [x29, #-24]
   2b900:	ldp	x8, x20, [x29, #-64]
   2b904:	sub	x9, x25, #0x1
   2b908:	stur	x9, [x29, #-72]
   2b90c:	mul	x27, x9, x8
   2b910:	add	x19, x27, x3
   2b914:	adds	x28, x19, #0x1
   2b918:	b.cs	2b930 <__gmpn_mul_fft@@Base+0x1148>  // b.hs, b.nlast
   2b91c:	lsl	x2, x28, #3
   2b920:	mov	x0, x20
   2b924:	mov	w1, wzr
   2b928:	bl	c5f0 <memset@plt>
   2b92c:	ldur	x3, [x29, #-24]
   2b930:	ldr	x8, [sp, #80]
   2b934:	lsl	x25, x22, #1
   2b938:	cmp	w8, #0x3f
   2b93c:	b.eq	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.none
   2b940:	ldur	x9, [x29, #-64]
   2b944:	stp	x25, x28, [x29, #-112]
   2b948:	add	x21, x20, x27, lsl #3
   2b94c:	mov	x25, x27
   2b950:	lsl	x8, x9, #1
   2b954:	stur	x8, [x29, #-88]
   2b958:	add	x8, x3, x27
   2b95c:	add	x27, x20, x28, lsl #3
   2b960:	ldur	x28, [x29, #-72]
   2b964:	add	x8, x20, x8, lsl #3
   2b968:	neg	x23, x9, lsl #3
   2b96c:	add	x24, x8, #0x10
   2b970:	stur	xzr, [x29, #-48]
   2b974:	stur	x19, [x29, #-120]
   2b978:	stur	x25, [x29, #-96]
   2b97c:	b	2b9a8 <__gmpn_mul_fft@@Base+0x11c0>
   2b980:	ldur	x20, [x29, #-56]
   2b984:	ldur	x8, [x29, #-64]
   2b988:	cmp	x28, #0x0
   2b98c:	sub	x28, x28, #0x1
   2b990:	add	x24, x24, x23
   2b994:	sub	x19, x19, x8
   2b998:	sub	x25, x25, x8
   2b99c:	add	x21, x21, x23
   2b9a0:	add	x27, x27, x23
   2b9a4:	b.le	2bb34 <__gmpn_mul_fft@@Base+0x134c>
   2b9a8:	ldur	x8, [x29, #-40]
   2b9ac:	ldp	x3, x9, [x29, #-80]
   2b9b0:	add	x20, x20, x25, lsl #3
   2b9b4:	mov	x0, x20
   2b9b8:	sub	x8, x8, x28
   2b9bc:	and	x22, x8, x9
   2b9c0:	ldur	x8, [x29, #-32]
   2b9c4:	mov	x1, x20
   2b9c8:	ldr	x2, [x8, x22, lsl #3]
   2b9cc:	bl	ca70 <__gmpn_add_n@plt>
   2b9d0:	cbz	x0, 2ba24 <__gmpn_mul_fft@@Base+0x123c>
   2b9d4:	ldur	x14, [x29, #-24]
   2b9d8:	add	x8, x20, x14, lsl #3
   2b9dc:	ldr	x9, [x8, #8]
   2b9e0:	adds	x9, x9, #0x1
   2b9e4:	str	x9, [x8, #8]
   2b9e8:	b.cc	2ba1c <__gmpn_mul_fft@@Base+0x1234>  // b.lo, b.ul, b.last
   2b9ec:	ldur	x8, [x29, #-96]
   2b9f0:	mov	x9, xzr
   2b9f4:	sub	x8, x8, x25
   2b9f8:	add	x10, x9, #0x1
   2b9fc:	cmp	x10, x8
   2ba00:	b.ge	2ba2c <__gmpn_mul_fft@@Base+0x1244>  // b.tcont
   2ba04:	lsl	x9, x9, #3
   2ba08:	ldr	x11, [x24, x9]
   2ba0c:	adds	x11, x11, #0x1
   2ba10:	str	x11, [x24, x9]
   2ba14:	mov	x9, x10
   2ba18:	b.cs	2b9f8 <__gmpn_mul_fft@@Base+0x1210>  // b.hs, b.nlast
   2ba1c:	mov	x8, xzr
   2ba20:	b	2ba30 <__gmpn_mul_fft@@Base+0x1248>
   2ba24:	ldur	x14, [x29, #-24]
   2ba28:	b	2ba3c <__gmpn_mul_fft@@Base+0x1254>
   2ba2c:	mov	w8, #0x1                   	// #1
   2ba30:	ldur	x9, [x29, #-48]
   2ba34:	add	x9, x8, x9
   2ba38:	stur	x9, [x29, #-48]
   2ba3c:	ldur	x9, [x29, #-88]
   2ba40:	add	x8, x28, #0x1
   2ba44:	str	x8, [x26, x9, lsl #3]
   2ba48:	ldur	x8, [x29, #-32]
   2ba4c:	mov	x9, x14
   2ba50:	ldr	x8, [x8, x22, lsl #3]
   2ba54:	add	x10, x9, #0x1
   2ba58:	cmp	x10, #0x1
   2ba5c:	b.lt	2b980 <__gmpn_mul_fft@@Base+0x1198>  // b.tstop
   2ba60:	lsl	x10, x9, #3
   2ba64:	ldr	x11, [x8, x10]
   2ba68:	ldr	x10, [x26, x10]
   2ba6c:	sub	x9, x9, #0x1
   2ba70:	cmp	x11, x10
   2ba74:	b.eq	2ba54 <__gmpn_mul_fft@@Base+0x126c>  // b.none
   2ba78:	b.ls	2b980 <__gmpn_mul_fft@@Base+0x1198>  // b.plast
   2ba7c:	ldr	x8, [x20]
   2ba80:	sub	x9, x8, #0x1
   2ba84:	str	x9, [x20]
   2ba88:	cbz	x8, 2ba98 <__gmpn_mul_fft@@Base+0x12b0>
   2ba8c:	ldur	x20, [x29, #-56]
   2ba90:	mov	x8, xzr
   2ba94:	b	2bad4 <__gmpn_mul_fft@@Base+0x12ec>
   2ba98:	ldur	x8, [x29, #-104]
   2ba9c:	ldur	x20, [x29, #-56]
   2baa0:	mov	w9, #0x1                   	// #1
   2baa4:	sub	x8, x8, x25
   2baa8:	cmp	x9, x8
   2baac:	b.ge	2bad0 <__gmpn_mul_fft@@Base+0x12e8>  // b.tcont
   2bab0:	lsl	x10, x9, #3
   2bab4:	ldr	x11, [x21, x10]
   2bab8:	add	x9, x9, #0x1
   2babc:	sub	x12, x11, #0x1
   2bac0:	str	x12, [x21, x10]
   2bac4:	cbz	x11, 2baa8 <__gmpn_mul_fft@@Base+0x12c0>
   2bac8:	mov	x8, xzr
   2bacc:	b	2bad4 <__gmpn_mul_fft@@Base+0x12ec>
   2bad0:	mov	x8, #0xffffffffffffffff    	// #-1
   2bad4:	lsl	x9, x19, #3
   2bad8:	ldr	x10, [x20, x9]
   2badc:	ldur	x11, [x29, #-48]
   2bae0:	add	x8, x8, x11
   2bae4:	sub	x11, x10, #0x1
   2bae8:	str	x11, [x20, x9]
   2baec:	cbnz	x10, 2bb1c <__gmpn_mul_fft@@Base+0x1334>
   2baf0:	ldur	x9, [x29, #-104]
   2baf4:	sub	x9, x9, x19
   2baf8:	add	x11, x10, #0x1
   2bafc:	cmp	x11, x9
   2bb00:	b.ge	2bb2c <__gmpn_mul_fft@@Base+0x1344>  // b.tcont
   2bb04:	lsl	x10, x10, #3
   2bb08:	ldr	x12, [x27, x10]
   2bb0c:	sub	x13, x12, #0x1
   2bb10:	str	x13, [x27, x10]
   2bb14:	mov	x10, x11
   2bb18:	cbz	x12, 2baf8 <__gmpn_mul_fft@@Base+0x1310>
   2bb1c:	mov	x9, xzr
   2bb20:	add	x8, x8, x9
   2bb24:	stur	x8, [x29, #-48]
   2bb28:	b	2b984 <__gmpn_mul_fft@@Base+0x119c>
   2bb2c:	mov	x9, #0xffffffffffffffff    	// #-1
   2bb30:	b	2bb20 <__gmpn_mul_fft@@Base+0x1338>
   2bb34:	ldur	x8, [x29, #-48]
   2bb38:	cmp	x8, #0x1
   2bb3c:	b.eq	2bbac <__gmpn_mul_fft@@Base+0x13c4>  // b.none
   2bb40:	ldr	x22, [sp, #72]
   2bb44:	ldr	x23, [sp, #104]
   2bb48:	ldp	x28, x27, [x29, #-104]
   2bb4c:	ldp	x13, x25, [x29, #-120]
   2bb50:	cmn	x8, #0x1
   2bb54:	b.ne	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.any
   2bb58:	add	x8, x20, x28, lsl #3
   2bb5c:	sub	x8, x8, x22, lsl #3
   2bb60:	ldr	x9, [x8]
   2bb64:	adds	x9, x9, #0x1
   2bb68:	str	x9, [x8]
   2bb6c:	b.cc	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.lo, b.ul, b.last
   2bb70:	add	x9, x14, x27
   2bb74:	sub	x9, x9, x22
   2bb78:	add	x9, x20, x9, lsl #3
   2bb7c:	mov	x10, xzr
   2bb80:	add	x9, x9, #0x10
   2bb84:	add	x11, x10, #0x1
   2bb88:	cmp	x11, x22
   2bb8c:	b.ge	2bc68 <__gmpn_mul_fft@@Base+0x1480>  // b.tcont
   2bb90:	lsl	x10, x10, #3
   2bb94:	ldr	x12, [x9, x10]
   2bb98:	adds	x12, x12, #0x1
   2bb9c:	str	x12, [x9, x10]
   2bba0:	mov	x10, x11
   2bba4:	b.cs	2bb84 <__gmpn_mul_fft@@Base+0x139c>  // b.hs, b.nlast
   2bba8:	b	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>
   2bbac:	ldp	x25, x28, [x29, #-112]
   2bbb0:	ldr	x22, [sp, #72]
   2bbb4:	ldr	x23, [sp, #104]
   2bbb8:	ldur	x27, [x29, #-96]
   2bbbc:	cmp	x28, x25
   2bbc0:	add	x8, x20, x28, lsl #3
   2bbc4:	b.ge	2bc14 <__gmpn_mul_fft@@Base+0x142c>  // b.tcont
   2bbc8:	sub	x8, x8, x22, lsl #3
   2bbcc:	ldr	x9, [x8]
   2bbd0:	sub	x10, x9, #0x1
   2bbd4:	str	x10, [x8]
   2bbd8:	cbnz	x9, 2bcb0 <__gmpn_mul_fft@@Base+0x14c8>
   2bbdc:	add	x8, x14, x27
   2bbe0:	sub	x8, x8, x22
   2bbe4:	add	x8, x20, x8, lsl #3
   2bbe8:	add	x8, x8, #0x10
   2bbec:	add	x10, x9, #0x1
   2bbf0:	cmp	x10, x22
   2bbf4:	b.ge	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.tcont
   2bbf8:	lsl	x9, x9, #3
   2bbfc:	ldr	x11, [x8, x9]
   2bc00:	sub	x12, x11, #0x1
   2bc04:	str	x12, [x8, x9]
   2bc08:	mov	x9, x10
   2bc0c:	cbz	x11, 2bbec <__gmpn_mul_fft@@Base+0x1404>
   2bc10:	b	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>
   2bc14:	sub	x8, x8, x25, lsl #3
   2bc18:	ldr	x9, [x8]
   2bc1c:	adds	x9, x9, #0x1
   2bc20:	str	x9, [x8]
   2bc24:	b.cc	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.lo, b.ul, b.last
   2bc28:	mov	w9, #0x1                   	// #1
   2bc2c:	b	2bc44 <__gmpn_mul_fft@@Base+0x145c>
   2bc30:	ldr	x9, [x8]
   2bc34:	adds	x9, x9, #0x1
   2bc38:	str	x9, [x8]
   2bc3c:	mov	w9, #0x1                   	// #1
   2bc40:	b.cc	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>  // b.lo, b.ul, b.last
   2bc44:	cmp	x9, x25
   2bc48:	b.ge	2bc30 <__gmpn_mul_fft@@Base+0x1448>  // b.tcont
   2bc4c:	lsl	x10, x9, #3
   2bc50:	ldr	x11, [x8, x10]
   2bc54:	add	x9, x9, #0x1
   2bc58:	adds	x11, x11, #0x1
   2bc5c:	str	x11, [x8, x10]
   2bc60:	b.cs	2bc44 <__gmpn_mul_fft@@Base+0x145c>  // b.hs, b.nlast
   2bc64:	b	2bcb0 <__gmpn_mul_fft@@Base+0x14c8>
   2bc68:	ldur	x9, [x8, #-8]
   2bc6c:	sub	x10, x9, #0x1
   2bc70:	stur	x10, [x8, #-8]
   2bc74:	cbnz	x9, 2bca0 <__gmpn_mul_fft@@Base+0x14b8>
   2bc78:	b	2bc94 <__gmpn_mul_fft@@Base+0x14ac>
   2bc7c:	lsl	x9, x9, #3
   2bc80:	ldr	x11, [x8, x9]
   2bc84:	sub	x12, x11, #0x1
   2bc88:	str	x12, [x8, x9]
   2bc8c:	mov	x9, x10
   2bc90:	cbnz	x11, 2bca0 <__gmpn_mul_fft@@Base+0x14b8>
   2bc94:	add	x10, x9, #0x1
   2bc98:	cmp	x10, x22
   2bc9c:	b.le	2bc7c <__gmpn_mul_fft@@Base+0x1494>
   2bca0:	lsl	x8, x13, #3
   2bca4:	ldr	x9, [x20, x8]
   2bca8:	sub	x9, x9, #0x1
   2bcac:	str	x9, [x20, x8]
   2bcb0:	sub	x19, x28, x25
   2bcb4:	cmp	x19, #0x1
   2bcb8:	b.lt	2bd8c <__gmpn_mul_fft@@Base+0x15a4>  // b.tstop
   2bcbc:	ldr	x21, [sp, #64]
   2bcc0:	add	x2, x20, x25, lsl #3
   2bcc4:	mov	x1, x20
   2bcc8:	mov	x3, x19
   2bccc:	mov	x0, x21
   2bcd0:	bl	ca70 <__gmpn_add_n@plt>
   2bcd4:	lsl	x8, x19, #3
   2bcd8:	ldr	x9, [x20, x8]
   2bcdc:	sub	x13, x22, x19
   2bce0:	adds	x9, x9, x0
   2bce4:	str	x9, [x21, x8]
   2bce8:	b.cc	2bdac <__gmpn_mul_fft@@Base+0x15c4>  // b.lo, b.ul, b.last
   2bcec:	ldur	x1, [x29, #-24]
   2bcf0:	add	x9, x25, x22
   2bcf4:	sub	x10, x27, x25
   2bcf8:	sub	x9, x9, x27
   2bcfc:	lsl	x10, x10, #3
   2bd00:	sub	x11, x9, x1
   2bd04:	mov	x8, xzr
   2bd08:	add	x12, x21, x10
   2bd0c:	add	x10, x20, x10
   2bd10:	sub	x9, x11, #0x2
   2bd14:	mov	w20, #0x1                   	// #1
   2bd18:	add	x8, x8, #0x1
   2bd1c:	cmp	x8, x13
   2bd20:	b.ge	2bf54 <__gmpn_mul_fft@@Base+0x176c>  // b.tcont
   2bd24:	add	x14, x10, x23
   2bd28:	ldr	x14, [x14, #16]
   2bd2c:	add	x15, x12, x23
   2bd30:	add	x12, x12, #0x8
   2bd34:	add	x10, x10, #0x8
   2bd38:	adds	x14, x14, #0x1
   2bd3c:	sub	x9, x9, #0x1
   2bd40:	str	x14, [x15, #16]
   2bd44:	b.cs	2bd18 <__gmpn_mul_fft@@Base+0x1530>  // b.hs, b.nlast
   2bd48:	ldur	x14, [x29, #-56]
   2bd4c:	mov	x20, xzr
   2bd50:	cmp	x14, x21
   2bd54:	b.eq	2bf54 <__gmpn_mul_fft@@Base+0x176c>  // b.none
   2bd58:	add	x14, x8, #0x1
   2bd5c:	cmp	x14, x13
   2bd60:	mov	x19, x22
   2bd64:	b.ge	2bf58 <__gmpn_mul_fft@@Base+0x1770>  // b.tcont
   2bd68:	add	x13, x22, x22, lsl #1
   2bd6c:	sub	x15, x13, x27
   2bd70:	sub	x15, x15, x1
   2bd74:	sub	x15, x15, x8
   2bd78:	sub	x15, x15, #0x2
   2bd7c:	cmp	x15, #0x4
   2bd80:	b.cs	2bea4 <__gmpn_mul_fft@@Base+0x16bc>  // b.hs, b.nlast
   2bd84:	ldur	x0, [x29, #-56]
   2bd88:	b	2bf20 <__gmpn_mul_fft@@Base+0x1738>
   2bd8c:	ldr	x21, [sp, #64]
   2bd90:	mov	x1, x20
   2bd94:	mov	x2, x22
   2bd98:	sub	x19, x28, x22
   2bd9c:	mov	x0, x21
   2bda0:	bl	ca50 <__gmpn_copyi@plt>
   2bda4:	mov	x20, xzr
   2bda8:	b	2bf58 <__gmpn_mul_fft@@Base+0x1770>
   2bdac:	ldur	x16, [x29, #-24]
   2bdb0:	cmp	x20, x21
   2bdb4:	mov	x20, xzr
   2bdb8:	b.eq	2bf54 <__gmpn_mul_fft@@Base+0x176c>  // b.none
   2bdbc:	cmp	x13, #0x2
   2bdc0:	mov	x19, x22
   2bdc4:	b.lt	2bf58 <__gmpn_mul_fft@@Base+0x1770>  // b.tstop
   2bdc8:	add	x8, x22, x22, lsl #1
   2bdcc:	sub	x9, x8, x27
   2bdd0:	sub	x9, x9, x16
   2bdd4:	sub	x9, x9, #0x2
   2bdd8:	cmp	x9, #0x4
   2bddc:	b.cs	2bdec <__gmpn_mul_fft@@Base+0x1604>  // b.hs, b.nlast
   2bde0:	ldur	x15, [x29, #-56]
   2bde4:	mov	w10, #0x1                   	// #1
   2bde8:	b	2be70 <__gmpn_mul_fft@@Base+0x1688>
   2bdec:	add	x10, x27, x16
   2bdf0:	ldur	x15, [x29, #-56]
   2bdf4:	sub	x10, x10, x22, lsl #1
   2bdf8:	lsl	x10, x10, #3
   2bdfc:	lsl	x11, x22, #3
   2be00:	add	x10, x10, #0x10
   2be04:	add	x12, x21, x10
   2be08:	add	x13, x15, x11
   2be0c:	cmp	x12, x13
   2be10:	b.cs	2be2c <__gmpn_mul_fft@@Base+0x1644>  // b.hs, b.nlast
   2be14:	add	x11, x21, x11
   2be18:	add	x10, x15, x10
   2be1c:	cmp	x10, x11
   2be20:	b.cs	2be2c <__gmpn_mul_fft@@Base+0x1644>  // b.hs, b.nlast
   2be24:	mov	w10, #0x1                   	// #1
   2be28:	b	2be70 <__gmpn_mul_fft@@Base+0x1688>
   2be2c:	add	x12, x16, x27
   2be30:	sub	x12, x12, x22, lsl #1
   2be34:	lsl	x12, x12, #3
   2be38:	and	x11, x9, #0xfffffffffffffffc
   2be3c:	add	x13, x12, #0x20
   2be40:	orr	x10, x11, #0x1
   2be44:	add	x12, x15, x13
   2be48:	add	x13, x21, x13
   2be4c:	mov	x14, x11
   2be50:	ldp	q0, q1, [x12, #-16]
   2be54:	add	x12, x12, #0x20
   2be58:	subs	x14, x14, #0x4
   2be5c:	stp	q0, q1, [x13, #-16]
   2be60:	add	x13, x13, #0x20
   2be64:	b.ne	2be50 <__gmpn_mul_fft@@Base+0x1668>  // b.any
   2be68:	cmp	x9, x11
   2be6c:	b.eq	2bf50 <__gmpn_mul_fft@@Base+0x1768>  // b.none
   2be70:	add	x9, x10, x16
   2be74:	add	x9, x9, x27
   2be78:	add	x9, x9, #0x1
   2be7c:	sub	x8, x9, x8
   2be80:	sub	x9, x9, x22, lsl #1
   2be84:	lsl	x10, x9, #3
   2be88:	add	x9, x21, x10
   2be8c:	add	x10, x15, x10
   2be90:	ldr	x11, [x10], #8
   2be94:	adds	x8, x8, #0x1
   2be98:	str	x11, [x9], #8
   2be9c:	b.cc	2be90 <__gmpn_mul_fft@@Base+0x16a8>  // b.lo, b.ul, b.last
   2bea0:	b	2bf50 <__gmpn_mul_fft@@Base+0x1768>
   2bea4:	ldur	x0, [x29, #-56]
   2bea8:	add	x17, x12, x23
   2beac:	lsl	x16, x22, #3
   2beb0:	add	x17, x17, #0x10
   2beb4:	add	x18, x0, x16
   2beb8:	cmp	x17, x18
   2bebc:	b.cs	2bed4 <__gmpn_mul_fft@@Base+0x16ec>  // b.hs, b.nlast
   2bec0:	add	x17, x10, x23
   2bec4:	add	x16, x21, x16
   2bec8:	add	x17, x17, #0x10
   2becc:	cmp	x17, x16
   2bed0:	b.cc	2bf20 <__gmpn_mul_fft@@Base+0x1738>  // b.lo, b.ul, b.last
   2bed4:	add	x12, x12, x23
   2bed8:	sub	x11, x11, x8
   2bedc:	and	x16, x9, #0xfffffffffffffffc
   2bee0:	add	x14, x10, x23
   2bee4:	add	x9, x12, #0x20
   2bee8:	sub	x12, x11, #0x2
   2beec:	add	x8, x16, x8
   2bef0:	and	x10, x15, #0xfffffffffffffffc
   2bef4:	add	x11, x14, #0x20
   2bef8:	add	x14, x8, #0x1
   2befc:	and	x8, x12, #0xfffffffffffffffc
   2bf00:	ldp	q0, q1, [x11, #-16]
   2bf04:	add	x11, x11, #0x20
   2bf08:	subs	x8, x8, #0x4
   2bf0c:	stp	q0, q1, [x9, #-16]
   2bf10:	add	x9, x9, #0x20
   2bf14:	b.ne	2bf00 <__gmpn_mul_fft@@Base+0x1718>  // b.any
   2bf18:	cmp	x15, x10
   2bf1c:	b.eq	2bf50 <__gmpn_mul_fft@@Base+0x1768>  // b.none
   2bf20:	add	x8, x14, x1
   2bf24:	add	x8, x8, x27
   2bf28:	add	x9, x8, #0x1
   2bf2c:	sub	x8, x9, x13
   2bf30:	sub	x9, x9, x22, lsl #1
   2bf34:	lsl	x10, x9, #3
   2bf38:	add	x9, x21, x10
   2bf3c:	add	x10, x0, x10
   2bf40:	ldr	x11, [x10], #8
   2bf44:	adds	x8, x8, #0x1
   2bf48:	str	x11, [x9], #8
   2bf4c:	b.cc	2bf40 <__gmpn_mul_fft@@Base+0x1758>  // b.lo, b.ul, b.last
   2bf50:	mov	x20, xzr
   2bf54:	mov	x19, x22
   2bf58:	ldur	x8, [x29, #-56]
   2bf5c:	mov	x0, x21
   2bf60:	mov	x1, x21
   2bf64:	mov	x3, x19
   2bf68:	add	x2, x8, x22, lsl #3
   2bf6c:	bl	c2d0 <__gmpn_sub_n@plt>
   2bf70:	add	x9, x21, x19, lsl #3
   2bf74:	ldr	x8, [x9]
   2bf78:	subs	x8, x8, x0
   2bf7c:	str	x8, [x9]
   2bf80:	b.cs	2bfb0 <__gmpn_mul_fft@@Base+0x17c8>  // b.hs, b.nlast
   2bf84:	sub	x10, x22, x19
   2bf88:	mov	w8, #0x1                   	// #1
   2bf8c:	mov	w11, #0x1                   	// #1
   2bf90:	cmp	x11, x10
   2bf94:	b.ge	2bfb4 <__gmpn_mul_fft@@Base+0x17cc>  // b.tcont
   2bf98:	lsl	x12, x11, #3
   2bf9c:	ldr	x13, [x9, x12]
   2bfa0:	add	x11, x11, #0x1
   2bfa4:	sub	x14, x13, #0x1
   2bfa8:	str	x14, [x9, x12]
   2bfac:	cbz	x13, 2bf90 <__gmpn_mul_fft@@Base+0x17a8>
   2bfb0:	mov	x8, xzr
   2bfb4:	subs	x0, x20, x8
   2bfb8:	b.pl	2bff8 <__gmpn_mul_fft@@Base+0x1810>  // b.nfrst
   2bfbc:	ldr	x8, [x21]
   2bfc0:	adds	x8, x8, #0x1
   2bfc4:	str	x8, [x21]
   2bfc8:	b.cc	2bff4 <__gmpn_mul_fft@@Base+0x180c>  // b.lo, b.ul, b.last
   2bfcc:	mov	w0, #0x1                   	// #1
   2bfd0:	mov	w8, #0x1                   	// #1
   2bfd4:	cmp	x8, x22
   2bfd8:	b.ge	2bff8 <__gmpn_mul_fft@@Base+0x1810>  // b.tcont
   2bfdc:	lsl	x9, x8, #3
   2bfe0:	ldr	x10, [x21, x9]
   2bfe4:	add	x8, x8, #0x1
   2bfe8:	adds	x10, x10, #0x1
   2bfec:	str	x10, [x21, x9]
   2bff0:	b.cs	2bfd4 <__gmpn_mul_fft@@Base+0x17ec>  // b.hs, b.nlast
   2bff4:	mov	x0, xzr
   2bff8:	ldp	x20, x19, [sp, #336]
   2bffc:	ldp	x22, x21, [sp, #320]
   2c000:	ldp	x24, x23, [sp, #304]
   2c004:	ldp	x26, x25, [sp, #288]
   2c008:	ldp	x28, x27, [sp, #272]
   2c00c:	ldp	x29, x30, [sp, #256]
   2c010:	add	sp, sp, #0x160
   2c014:	ret
   2c018:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2c01c:	ldur	x3, [x29, #-24]
   2c020:	b	2b578 <__gmpn_mul_fft@@Base+0xd90>
   2c024:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2c028:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2c02c:	add	x0, x0, #0x1be
   2c030:	add	x2, x2, #0x1ff
   2c034:	mov	w1, #0x1d9                 	// #473
   2c038:	bl	c6c0 <__gmp_assert_fail@plt>
   2c03c:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2c040:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2c044:	add	x0, x0, #0x1be
   2c048:	add	x2, x2, #0x213
   2c04c:	mov	w1, #0x1ef                 	// #495
   2c050:	bl	c6c0 <__gmp_assert_fail@plt>
   2c054:	stp	x29, x30, [sp, #-80]!
   2c058:	stp	x24, x23, [sp, #32]
   2c05c:	lsr	x24, x2, #6
   2c060:	stp	x22, x21, [sp, #48]
   2c064:	stp	x20, x19, [sp, #64]
   2c068:	mov	x19, x3
   2c06c:	mov	x22, x1
   2c070:	mov	x20, x0
   2c074:	subs	x21, x24, x3
   2c078:	and	w23, w2, #0x3f
   2c07c:	add	x8, x1, x3, lsl #3
   2c080:	str	x25, [sp, #16]
   2c084:	mov	x29, sp
   2c088:	b.ge	2c0c8 <__gmpn_mul_fft@@Base+0x18e0>  // b.tcont
   2c08c:	sub	x1, x8, x24, lsl #3
   2c090:	add	x2, x24, #0x1
   2c094:	mov	x0, x20
   2c098:	cbz	w23, 2c100 <__gmpn_mul_fft@@Base+0x1918>
   2c09c:	mov	w3, w23
   2c0a0:	bl	d160 <__gmpn_lshiftc@plt>
   2c0a4:	add	x0, x20, x24, lsl #3
   2c0a8:	ldr	x8, [x0]
   2c0ac:	sub	x2, x19, x24
   2c0b0:	mov	x1, x22
   2c0b4:	mov	w3, w23
   2c0b8:	mvn	x21, x8
   2c0bc:	bl	c180 <__gmpn_lshift@plt>
   2c0c0:	cbnz	x24, 2c120 <__gmpn_mul_fft@@Base+0x1938>
   2c0c4:	b	2c238 <__gmpn_mul_fft@@Base+0x1a50>
   2c0c8:	sub	x1, x8, x21, lsl #3
   2c0cc:	cbz	w23, 2c164 <__gmpn_mul_fft@@Base+0x197c>
   2c0d0:	add	x2, x21, #0x1
   2c0d4:	mov	x0, x20
   2c0d8:	mov	w3, w23
   2c0dc:	bl	c180 <__gmpn_lshift@plt>
   2c0e0:	add	x0, x20, x21, lsl #3
   2c0e4:	ldr	x25, [x0]
   2c0e8:	sub	x2, x19, x21
   2c0ec:	mov	x1, x22
   2c0f0:	mov	w3, w23
   2c0f4:	bl	d160 <__gmpn_lshiftc@plt>
   2c0f8:	add	x8, x0, #0x1
   2c0fc:	b	2c188 <__gmpn_mul_fft@@Base+0x19a0>
   2c100:	bl	c290 <__gmpn_com@plt>
   2c104:	ldr	x21, [x22, x19, lsl #3]
   2c108:	add	x0, x20, x24, lsl #3
   2c10c:	sub	x2, x19, x24
   2c110:	mov	x1, x22
   2c114:	bl	ca50 <__gmpn_copyi@plt>
   2c118:	mov	x0, xzr
   2c11c:	cbz	x24, 2c238 <__gmpn_mul_fft@@Base+0x1a50>
   2c120:	cbz	x0, 2c1f8 <__gmpn_mul_fft@@Base+0x1a10>
   2c124:	sub	x0, x0, #0x1
   2c128:	ldr	x8, [x20]
   2c12c:	subs	x8, x8, x0
   2c130:	str	x8, [x20]
   2c134:	b.cs	2c234 <__gmpn_mul_fft@@Base+0x1a4c>  // b.hs, b.nlast
   2c138:	mov	w0, #0x2                   	// #2
   2c13c:	mov	w8, #0x1                   	// #1
   2c140:	cmp	x8, x24
   2c144:	b.cs	2c238 <__gmpn_mul_fft@@Base+0x1a50>  // b.hs, b.nlast
   2c148:	lsl	x9, x8, #3
   2c14c:	ldr	x10, [x20, x9]
   2c150:	add	x8, x8, #0x1
   2c154:	sub	x11, x10, #0x1
   2c158:	str	x11, [x20, x9]
   2c15c:	cbz	x10, 2c140 <__gmpn_mul_fft@@Base+0x1958>
   2c160:	b	2c234 <__gmpn_mul_fft@@Base+0x1a4c>
   2c164:	mov	x0, x20
   2c168:	mov	x2, x21
   2c16c:	bl	ca50 <__gmpn_copyi@plt>
   2c170:	ldr	x25, [x22, x19, lsl #3]
   2c174:	add	x0, x20, x21, lsl #3
   2c178:	sub	x2, x19, x21
   2c17c:	mov	x1, x22
   2c180:	bl	c290 <__gmpn_com@plt>
   2c184:	mov	w8, #0x1                   	// #1
   2c188:	str	xzr, [x20, x19, lsl #3]
   2c18c:	ldr	x9, [x20]
   2c190:	adds	x8, x9, x8
   2c194:	str	x8, [x20]
   2c198:	b.cc	2c1b0 <__gmpn_mul_fft@@Base+0x19c8>  // b.lo, b.ul, b.last
   2c19c:	add	x8, x20, #0x8
   2c1a0:	ldr	x9, [x8]
   2c1a4:	adds	x9, x9, #0x1
   2c1a8:	str	x9, [x8], #8
   2c1ac:	b.cs	2c1a0 <__gmpn_mul_fft@@Base+0x19b8>  // b.hs, b.nlast
   2c1b0:	adds	x9, x25, #0x1
   2c1b4:	cset	w8, cs  // cs = hs, nlast
   2c1b8:	add	x10, x20, x21, lsl #3
   2c1bc:	lsl	x11, x8, #3
   2c1c0:	ldr	x12, [x10, x11]
   2c1c4:	csinc	x9, x9, xzr, cc  // cc = lo, ul, last
   2c1c8:	adds	x9, x12, x9
   2c1cc:	str	x9, [x10, x11]
   2c1d0:	b.cc	2c314 <__gmpn_mul_fft@@Base+0x1b2c>  // b.lo, b.ul, b.last
   2c1d4:	add	x8, x24, x8
   2c1d8:	sub	x8, x8, x19
   2c1dc:	add	x8, x20, x8, lsl #3
   2c1e0:	add	x8, x8, #0x8
   2c1e4:	ldr	x9, [x8]
   2c1e8:	adds	x9, x9, #0x1
   2c1ec:	str	x9, [x8], #8
   2c1f0:	b.cs	2c1e4 <__gmpn_mul_fft@@Base+0x19fc>  // b.hs, b.nlast
   2c1f4:	b	2c314 <__gmpn_mul_fft@@Base+0x1b2c>
   2c1f8:	ldr	x8, [x20]
   2c1fc:	adds	x8, x8, #0x1
   2c200:	str	x8, [x20]
   2c204:	b.cc	2c234 <__gmpn_mul_fft@@Base+0x1a4c>  // b.lo, b.ul, b.last
   2c208:	mov	w0, #0x1                   	// #1
   2c20c:	mov	w8, #0x1                   	// #1
   2c210:	cmp	x8, x19
   2c214:	b.ge	2c128 <__gmpn_mul_fft@@Base+0x1940>  // b.tcont
   2c218:	lsl	x9, x8, #3
   2c21c:	ldr	x10, [x20, x9]
   2c220:	add	x8, x8, #0x1
   2c224:	adds	x10, x10, #0x1
   2c228:	str	x10, [x20, x9]
   2c22c:	b.cs	2c210 <__gmpn_mul_fft@@Base+0x1a28>  // b.hs, b.nlast
   2c230:	b	2c238 <__gmpn_mul_fft@@Base+0x1a50>
   2c234:	mov	w0, #0x1                   	// #1
   2c238:	add	x8, x20, x24, lsl #3
   2c23c:	ldr	x10, [x8]
   2c240:	sub	x9, x19, x24
   2c244:	subs	x10, x10, x0
   2c248:	str	x10, [x8]
   2c24c:	b.cs	2c274 <__gmpn_mul_fft@@Base+0x1a8c>  // b.hs, b.nlast
   2c250:	mov	w10, #0x1                   	// #1
   2c254:	cmp	x10, x9
   2c258:	b.ge	2c27c <__gmpn_mul_fft@@Base+0x1a94>  // b.tcont
   2c25c:	lsl	x11, x10, #3
   2c260:	ldr	x12, [x8, x11]
   2c264:	add	x10, x10, #0x1
   2c268:	sub	x13, x12, #0x1
   2c26c:	str	x13, [x8, x11]
   2c270:	cbz	x12, 2c254 <__gmpn_mul_fft@@Base+0x1a6c>
   2c274:	mov	x10, xzr
   2c278:	b	2c280 <__gmpn_mul_fft@@Base+0x1a98>
   2c27c:	mov	x10, #0xffffffffffffffff    	// #-1
   2c280:	str	x10, [x20, x19, lsl #3]
   2c284:	ldr	x10, [x8]
   2c288:	subs	x10, x10, x21
   2c28c:	str	x10, [x8]
   2c290:	b.cs	2c2bc <__gmpn_mul_fft@@Base+0x1ad4>  // b.hs, b.nlast
   2c294:	mov	w10, #0x1                   	// #1
   2c298:	mov	w11, #0x1                   	// #1
   2c29c:	cmp	x11, x9
   2c2a0:	b.ge	2c2c0 <__gmpn_mul_fft@@Base+0x1ad8>  // b.tcont
   2c2a4:	lsl	x12, x11, #3
   2c2a8:	ldr	x13, [x8, x12]
   2c2ac:	add	x11, x11, #0x1
   2c2b0:	sub	x14, x13, #0x1
   2c2b4:	str	x14, [x8, x12]
   2c2b8:	cbz	x13, 2c29c <__gmpn_mul_fft@@Base+0x1ab4>
   2c2bc:	mov	x10, xzr
   2c2c0:	lsl	x8, x19, #3
   2c2c4:	ldr	x9, [x20, x8]
   2c2c8:	subs	x9, x9, x10
   2c2cc:	str	x9, [x20, x8]
   2c2d0:	b.pl	2c314 <__gmpn_mul_fft@@Base+0x1b2c>  // b.nfrst
   2c2d4:	ldr	x8, [x20]
   2c2d8:	adds	x8, x8, #0x1
   2c2dc:	str	x8, [x20]
   2c2e0:	b.cc	2c30c <__gmpn_mul_fft@@Base+0x1b24>  // b.lo, b.ul, b.last
   2c2e4:	mov	w8, #0x1                   	// #1
   2c2e8:	mov	w9, #0x1                   	// #1
   2c2ec:	cmp	x9, x19
   2c2f0:	b.ge	2c310 <__gmpn_mul_fft@@Base+0x1b28>  // b.tcont
   2c2f4:	lsl	x10, x9, #3
   2c2f8:	ldr	x11, [x20, x10]
   2c2fc:	add	x9, x9, #0x1
   2c300:	adds	x11, x11, #0x1
   2c304:	str	x11, [x20, x10]
   2c308:	b.cs	2c2ec <__gmpn_mul_fft@@Base+0x1b04>  // b.hs, b.nlast
   2c30c:	mov	x8, xzr
   2c310:	str	x8, [x20, x19, lsl #3]
   2c314:	ldp	x20, x19, [sp, #64]
   2c318:	ldp	x22, x21, [sp, #48]
   2c31c:	ldp	x24, x23, [sp, #32]
   2c320:	ldr	x25, [sp, #16]
   2c324:	ldp	x29, x30, [sp], #80
   2c328:	ret
   2c32c:	sub	sp, sp, #0x70
   2c330:	stp	x26, x25, [sp, #48]
   2c334:	stp	x22, x21, [sp, #80]
   2c338:	stp	x20, x19, [sp, #96]
   2c33c:	mov	x21, x6
   2c340:	mov	x25, x5
   2c344:	mov	x19, x4
   2c348:	cmp	x1, #0x2
   2c34c:	mov	x20, x0
   2c350:	stp	x29, x30, [sp, #16]
   2c354:	stp	x28, x27, [sp, #32]
   2c358:	stp	x24, x23, [sp, #64]
   2c35c:	add	x29, sp, #0x10
   2c360:	b.ne	2c3f4 <__gmpn_mul_fft@@Base+0x1c0c>  // b.any
   2c364:	ldr	x1, [x20]
   2c368:	add	x22, x19, #0x1
   2c36c:	mov	x0, x21
   2c370:	mov	x2, x22
   2c374:	bl	ca50 <__gmpn_copyi@plt>
   2c378:	ldr	x0, [x20]
   2c37c:	lsl	x23, x25, #3
   2c380:	ldr	x2, [x20, x23]
   2c384:	mov	x3, x22
   2c388:	mov	x1, x0
   2c38c:	bl	ca70 <__gmpn_add_n@plt>
   2c390:	ldr	x0, [x20, x23]
   2c394:	mov	x1, x21
   2c398:	mov	x3, x22
   2c39c:	mov	x2, x0
   2c3a0:	bl	c2d0 <__gmpn_sub_n@plt>
   2c3a4:	ldr	x8, [x20]
   2c3a8:	ldr	x9, [x8, x19, lsl #3]
   2c3ac:	cmp	x9, #0x2
   2c3b0:	b.cc	2c56c <__gmpn_mul_fft@@Base+0x1d84>  // b.lo, b.ul, b.last
   2c3b4:	ldr	x10, [x8]
   2c3b8:	sub	x9, x9, #0x1
   2c3bc:	subs	x9, x10, x9
   2c3c0:	str	x9, [x8]
   2c3c4:	mov	w9, #0x1                   	// #1
   2c3c8:	b.cs	2c568 <__gmpn_mul_fft@@Base+0x1d80>  // b.hs, b.nlast
   2c3cc:	mov	w10, #0x1                   	// #1
   2c3d0:	cmp	x10, x19
   2c3d4:	b.ge	2c564 <__gmpn_mul_fft@@Base+0x1d7c>  // b.tcont
   2c3d8:	lsl	x11, x10, #3
   2c3dc:	ldr	x12, [x8, x11]
   2c3e0:	add	x10, x10, #0x1
   2c3e4:	sub	x13, x12, #0x1
   2c3e8:	str	x13, [x8, x11]
   2c3ec:	cbz	x12, 2c3d0 <__gmpn_mul_fft@@Base+0x1be8>
   2c3f0:	b	2c568 <__gmpn_mul_fft@@Base+0x1d80>
   2c3f4:	mov	x27, x2
   2c3f8:	ldr	x28, [x27], #-8
   2c3fc:	asr	x22, x1, #1
   2c400:	lsl	x24, x25, #1
   2c404:	mov	x23, x3
   2c408:	mov	x26, x1
   2c40c:	lsl	x3, x3, #1
   2c410:	mov	x0, x20
   2c414:	mov	x1, x22
   2c418:	mov	x2, x27
   2c41c:	mov	x4, x19
   2c420:	mov	x5, x24
   2c424:	mov	x6, x21
   2c428:	str	x3, [sp]
   2c42c:	bl	2c32c <__gmpn_mul_fft@@Base+0x1b44>
   2c430:	ldr	x3, [sp]
   2c434:	add	x0, x20, x25, lsl #3
   2c438:	mov	x1, x22
   2c43c:	mov	x2, x27
   2c440:	mov	x4, x19
   2c444:	mov	x5, x24
   2c448:	mov	x6, x21
   2c44c:	stp	x24, x22, [sp]
   2c450:	bl	2c32c <__gmpn_mul_fft@@Base+0x1b44>
   2c454:	cmp	x26, #0x2
   2c458:	b.lt	2c5c4 <__gmpn_mul_fft@@Base+0x1ddc>  // b.tstop
   2c45c:	mov	x26, xzr
   2c460:	lsl	x27, x25, #3
   2c464:	lsl	x22, x19, #3
   2c468:	b	2c48c <__gmpn_mul_fft@@Base+0x1ca4>
   2c46c:	ldr	x8, [sp, #8]
   2c470:	add	x26, x26, #0x1
   2c474:	add	x28, x28, #0x8
   2c478:	mov	x23, x24
   2c47c:	cmp	x26, x8
   2c480:	ldr	x8, [sp]
   2c484:	add	x20, x20, x8, lsl #3
   2c488:	b.ge	2c5c4 <__gmpn_mul_fft@@Base+0x1ddc>  // b.tcont
   2c48c:	ldrsw	x8, [x28]
   2c490:	ldr	x1, [x20, x27]
   2c494:	mov	x0, x21
   2c498:	mov	x3, x19
   2c49c:	mul	x2, x8, x23
   2c4a0:	mov	x24, x23
   2c4a4:	bl	2c054 <__gmpn_mul_fft@@Base+0x186c>
   2c4a8:	ldr	x1, [x20]
   2c4ac:	ldr	x25, [x20, x27]
   2c4b0:	ldr	x9, [x21, x22]
   2c4b4:	mov	x2, x21
   2c4b8:	ldr	x8, [x1, x22]
   2c4bc:	mov	x0, x25
   2c4c0:	mov	x3, x19
   2c4c4:	sub	x23, x8, x9
   2c4c8:	bl	c2d0 <__gmpn_sub_n@plt>
   2c4cc:	sub	x8, x23, x0
   2c4d0:	neg	x9, x8
   2c4d4:	and	x9, x9, x8, asr #63
   2c4d8:	add	x8, x9, x8
   2c4dc:	str	x8, [x25, x22]
   2c4e0:	ldr	x8, [x25]
   2c4e4:	adds	x8, x8, x9
   2c4e8:	str	x8, [x25]
   2c4ec:	b.cc	2c504 <__gmpn_mul_fft@@Base+0x1d1c>  // b.lo, b.ul, b.last
   2c4f0:	add	x8, x25, #0x8
   2c4f4:	ldr	x9, [x8]
   2c4f8:	adds	x9, x9, #0x1
   2c4fc:	str	x9, [x8], #8
   2c500:	b.cs	2c4f4 <__gmpn_mul_fft@@Base+0x1d0c>  // b.hs, b.nlast
   2c504:	ldr	x25, [x20]
   2c508:	ldr	x9, [x21, x22]
   2c50c:	mov	x2, x21
   2c510:	mov	x3, x19
   2c514:	ldr	x8, [x25, x22]
   2c518:	mov	x0, x25
   2c51c:	mov	x1, x25
   2c520:	add	x23, x9, x8
   2c524:	bl	ca70 <__gmpn_add_n@plt>
   2c528:	adds	x8, x23, x0
   2c52c:	sub	x9, x8, #0x1
   2c530:	csel	x9, xzr, x9, eq  // eq = none
   2c534:	sub	x8, x8, x9
   2c538:	str	x8, [x25, x22]
   2c53c:	ldr	x8, [x25]
   2c540:	subs	x8, x8, x9
   2c544:	str	x8, [x25]
   2c548:	b.cs	2c46c <__gmpn_mul_fft@@Base+0x1c84>  // b.hs, b.nlast
   2c54c:	add	x8, x25, #0x8
   2c550:	ldr	x9, [x8]
   2c554:	sub	x10, x9, #0x1
   2c558:	str	x10, [x8], #8
   2c55c:	cbz	x9, 2c550 <__gmpn_mul_fft@@Base+0x1d68>
   2c560:	b	2c46c <__gmpn_mul_fft@@Base+0x1c84>
   2c564:	mov	x9, xzr
   2c568:	str	x9, [x8, x19, lsl #3]
   2c56c:	cbz	x0, 2c5c4 <__gmpn_mul_fft@@Base+0x1ddc>
   2c570:	ldr	x8, [x20, x25, lsl #3]
   2c574:	mov	x9, xzr
   2c578:	ldr	x10, [x8, x19, lsl #3]
   2c57c:	ldr	x11, [x8]
   2c580:	neg	x12, x10
   2c584:	sub	x10, x11, x10
   2c588:	cmp	x10, x12
   2c58c:	str	x10, [x8]
   2c590:	b.cs	2c5c0 <__gmpn_mul_fft@@Base+0x1dd8>  // b.hs, b.nlast
   2c594:	mov	w9, #0x1                   	// #1
   2c598:	mov	w10, #0x1                   	// #1
   2c59c:	cmp	x10, x19
   2c5a0:	b.ge	2c5c0 <__gmpn_mul_fft@@Base+0x1dd8>  // b.tcont
   2c5a4:	lsl	x11, x10, #3
   2c5a8:	ldr	x12, [x8, x11]
   2c5ac:	add	x10, x10, #0x1
   2c5b0:	adds	x12, x12, #0x1
   2c5b4:	str	x12, [x8, x11]
   2c5b8:	b.cs	2c59c <__gmpn_mul_fft@@Base+0x1db4>  // b.hs, b.nlast
   2c5bc:	mov	x9, xzr
   2c5c0:	str	x9, [x8, x19, lsl #3]
   2c5c4:	ldp	x20, x19, [sp, #96]
   2c5c8:	ldp	x22, x21, [sp, #80]
   2c5cc:	ldp	x24, x23, [sp, #64]
   2c5d0:	ldp	x26, x25, [sp, #48]
   2c5d4:	ldp	x28, x27, [sp, #32]
   2c5d8:	ldp	x29, x30, [sp, #16]
   2c5dc:	add	sp, sp, #0x70
   2c5e0:	ret
   2c5e4:	stp	x29, x30, [sp, #-96]!
   2c5e8:	stp	x22, x21, [sp, #64]
   2c5ec:	stp	x20, x19, [sp, #80]
   2c5f0:	mov	x21, x4
   2c5f4:	mov	x19, x3
   2c5f8:	cmp	x1, #0x2
   2c5fc:	mov	x20, x0
   2c600:	stp	x28, x27, [sp, #16]
   2c604:	stp	x26, x25, [sp, #32]
   2c608:	stp	x24, x23, [sp, #48]
   2c60c:	mov	x29, sp
   2c610:	b.ne	2c69c <__gmpn_mul_fft@@Base+0x1eb4>  // b.any
   2c614:	ldr	x1, [x20]
   2c618:	add	x22, x19, #0x1
   2c61c:	mov	x0, x21
   2c620:	mov	x2, x22
   2c624:	bl	ca50 <__gmpn_copyi@plt>
   2c628:	ldp	x0, x2, [x20]
   2c62c:	mov	x3, x22
   2c630:	mov	x1, x0
   2c634:	bl	ca70 <__gmpn_add_n@plt>
   2c638:	ldr	x0, [x20, #8]
   2c63c:	mov	x1, x21
   2c640:	mov	x3, x22
   2c644:	mov	x2, x0
   2c648:	bl	c2d0 <__gmpn_sub_n@plt>
   2c64c:	ldr	x8, [x20]
   2c650:	ldr	x9, [x8, x19, lsl #3]
   2c654:	cmp	x9, #0x2
   2c658:	b.cc	2c7dc <__gmpn_mul_fft@@Base+0x1ff4>  // b.lo, b.ul, b.last
   2c65c:	ldr	x10, [x8]
   2c660:	sub	x9, x9, #0x1
   2c664:	subs	x9, x10, x9
   2c668:	str	x9, [x8]
   2c66c:	mov	w9, #0x1                   	// #1
   2c670:	b.cs	2c7d8 <__gmpn_mul_fft@@Base+0x1ff0>  // b.hs, b.nlast
   2c674:	mov	w10, #0x1                   	// #1
   2c678:	cmp	x10, x19
   2c67c:	b.ge	2c7d4 <__gmpn_mul_fft@@Base+0x1fec>  // b.tcont
   2c680:	lsl	x11, x10, #3
   2c684:	ldr	x12, [x8, x11]
   2c688:	add	x10, x10, #0x1
   2c68c:	sub	x13, x12, #0x1
   2c690:	str	x13, [x8, x11]
   2c694:	cbz	x12, 2c678 <__gmpn_mul_fft@@Base+0x1e90>
   2c698:	b	2c7d8 <__gmpn_mul_fft@@Base+0x1ff0>
   2c69c:	asr	x23, x1, #1
   2c6a0:	lsl	x25, x2, #1
   2c6a4:	mov	x22, x2
   2c6a8:	mov	x24, x1
   2c6ac:	mov	x0, x20
   2c6b0:	mov	x1, x23
   2c6b4:	mov	x2, x25
   2c6b8:	mov	x3, x19
   2c6bc:	mov	x4, x21
   2c6c0:	bl	2c5e4 <__gmpn_mul_fft@@Base+0x1dfc>
   2c6c4:	add	x0, x20, x23, lsl #3
   2c6c8:	mov	x1, x23
   2c6cc:	mov	x2, x25
   2c6d0:	mov	x3, x19
   2c6d4:	mov	x4, x21
   2c6d8:	bl	2c5e4 <__gmpn_mul_fft@@Base+0x1dfc>
   2c6dc:	cmp	x24, #0x2
   2c6e0:	b.lt	2c834 <__gmpn_mul_fft@@Base+0x204c>  // b.tstop
   2c6e4:	mov	x25, xzr
   2c6e8:	lsl	x26, x23, #3
   2c6ec:	lsl	x27, x19, #3
   2c6f0:	b	2c704 <__gmpn_mul_fft@@Base+0x1f1c>
   2c6f4:	add	x25, x25, #0x1
   2c6f8:	cmp	x25, x23
   2c6fc:	add	x20, x20, #0x8
   2c700:	b.ge	2c834 <__gmpn_mul_fft@@Base+0x204c>  // b.tcont
   2c704:	ldr	x1, [x20, x26]
   2c708:	mul	x2, x25, x22
   2c70c:	mov	x0, x21
   2c710:	mov	x3, x19
   2c714:	bl	2c054 <__gmpn_mul_fft@@Base+0x186c>
   2c718:	ldr	x1, [x20]
   2c71c:	ldr	x24, [x20, x26]
   2c720:	ldr	x9, [x21, x27]
   2c724:	mov	x2, x21
   2c728:	ldr	x8, [x1, x27]
   2c72c:	mov	x0, x24
   2c730:	mov	x3, x19
   2c734:	sub	x28, x8, x9
   2c738:	bl	c2d0 <__gmpn_sub_n@plt>
   2c73c:	sub	x8, x28, x0
   2c740:	neg	x9, x8
   2c744:	and	x9, x9, x8, asr #63
   2c748:	add	x8, x9, x8
   2c74c:	str	x8, [x24, x27]
   2c750:	ldr	x8, [x24]
   2c754:	adds	x8, x8, x9
   2c758:	str	x8, [x24]
   2c75c:	b.cc	2c774 <__gmpn_mul_fft@@Base+0x1f8c>  // b.lo, b.ul, b.last
   2c760:	add	x8, x24, #0x8
   2c764:	ldr	x9, [x8]
   2c768:	adds	x9, x9, #0x1
   2c76c:	str	x9, [x8], #8
   2c770:	b.cs	2c764 <__gmpn_mul_fft@@Base+0x1f7c>  // b.hs, b.nlast
   2c774:	ldr	x24, [x20]
   2c778:	ldr	x9, [x21, x27]
   2c77c:	mov	x2, x21
   2c780:	mov	x3, x19
   2c784:	ldr	x8, [x24, x27]
   2c788:	mov	x0, x24
   2c78c:	mov	x1, x24
   2c790:	add	x28, x9, x8
   2c794:	bl	ca70 <__gmpn_add_n@plt>
   2c798:	adds	x8, x28, x0
   2c79c:	sub	x9, x8, #0x1
   2c7a0:	csel	x9, xzr, x9, eq  // eq = none
   2c7a4:	sub	x8, x8, x9
   2c7a8:	str	x8, [x24, x27]
   2c7ac:	ldr	x8, [x24]
   2c7b0:	subs	x8, x8, x9
   2c7b4:	str	x8, [x24]
   2c7b8:	b.cs	2c6f4 <__gmpn_mul_fft@@Base+0x1f0c>  // b.hs, b.nlast
   2c7bc:	add	x8, x24, #0x8
   2c7c0:	ldr	x9, [x8]
   2c7c4:	sub	x10, x9, #0x1
   2c7c8:	str	x10, [x8], #8
   2c7cc:	cbz	x9, 2c7c0 <__gmpn_mul_fft@@Base+0x1fd8>
   2c7d0:	b	2c6f4 <__gmpn_mul_fft@@Base+0x1f0c>
   2c7d4:	mov	x9, xzr
   2c7d8:	str	x9, [x8, x19, lsl #3]
   2c7dc:	cbz	x0, 2c834 <__gmpn_mul_fft@@Base+0x204c>
   2c7e0:	ldr	x8, [x20, #8]
   2c7e4:	mov	x9, xzr
   2c7e8:	ldr	x10, [x8, x19, lsl #3]
   2c7ec:	ldr	x11, [x8]
   2c7f0:	neg	x12, x10
   2c7f4:	sub	x10, x11, x10
   2c7f8:	cmp	x10, x12
   2c7fc:	str	x10, [x8]
   2c800:	b.cs	2c830 <__gmpn_mul_fft@@Base+0x2048>  // b.hs, b.nlast
   2c804:	mov	w9, #0x1                   	// #1
   2c808:	mov	w10, #0x1                   	// #1
   2c80c:	cmp	x10, x19
   2c810:	b.ge	2c830 <__gmpn_mul_fft@@Base+0x2048>  // b.tcont
   2c814:	lsl	x11, x10, #3
   2c818:	ldr	x12, [x8, x11]
   2c81c:	add	x10, x10, #0x1
   2c820:	adds	x12, x12, #0x1
   2c824:	str	x12, [x8, x11]
   2c828:	b.cs	2c80c <__gmpn_mul_fft@@Base+0x2024>  // b.hs, b.nlast
   2c82c:	mov	x9, xzr
   2c830:	str	x9, [x8, x19, lsl #3]
   2c834:	ldp	x20, x19, [sp, #80]
   2c838:	ldp	x22, x21, [sp, #64]
   2c83c:	ldp	x24, x23, [sp, #48]
   2c840:	ldp	x26, x25, [sp, #32]
   2c844:	ldp	x28, x27, [sp, #16]
   2c848:	ldp	x29, x30, [sp], #96
   2c84c:	ret

000000000002c850 <__gmpn_mul_n@@Base>:
   2c850:	stp	x29, x30, [sp, #-32]!
   2c854:	stp	x28, x19, [sp, #16]
   2c858:	mov	x29, sp
   2c85c:	sub	sp, sp, #0x720
   2c860:	mov	x4, x3
   2c864:	mov	x3, x2
   2c868:	cmp	x4, #0xd
   2c86c:	mov	x19, sp
   2c870:	b.le	2c8b0 <__gmpn_mul_n@@Base+0x60>
   2c874:	cmp	x4, #0x30
   2c878:	b.le	2c8c4 <__gmpn_mul_n@@Base+0x74>
   2c87c:	cmp	x4, #0x51
   2c880:	b.le	2c8d4 <__gmpn_mul_n@@Base+0x84>
   2c884:	cmp	x4, #0xac
   2c888:	b.le	2c8fc <__gmpn_mul_n@@Base+0xac>
   2c88c:	cmp	x4, #0xeb
   2c890:	b.le	2c924 <__gmpn_mul_n@@Base+0xd4>
   2c894:	cmp	x4, #0xc7f
   2c898:	b.le	2c940 <__gmpn_mul_n@@Base+0xf0>
   2c89c:	mov	x2, x4
   2c8a0:	mov	sp, x29
   2c8a4:	ldp	x28, x19, [sp, #16]
   2c8a8:	ldp	x29, x30, [sp], #32
   2c8ac:	b	cca0 <__gmpn_nussbaumer_mul@plt>
   2c8b0:	mov	x2, x4
   2c8b4:	mov	sp, x29
   2c8b8:	ldp	x28, x19, [sp, #16]
   2c8bc:	ldp	x29, x30, [sp], #32
   2c8c0:	b	c550 <__gmpn_mul_basecase@plt>
   2c8c4:	add	x5, x19, #0x20
   2c8c8:	mov	x2, x4
   2c8cc:	bl	d450 <__gmpn_toom22_mul@plt>
   2c8d0:	b	2c984 <__gmpn_mul_n@@Base+0x134>
   2c8d4:	mov	w8, #0x18                  	// #24
   2c8d8:	mul	x8, x4, x8
   2c8dc:	add	x8, x8, #0x20f
   2c8e0:	and	x8, x8, #0xfffffffffffffff0
   2c8e4:	mov	x9, sp
   2c8e8:	sub	x5, x9, x8
   2c8ec:	mov	sp, x5
   2c8f0:	mov	x2, x4
   2c8f4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   2c8f8:	b	2c984 <__gmpn_mul_n@@Base+0x134>
   2c8fc:	mov	w8, #0x18                  	// #24
   2c900:	mul	x8, x4, x8
   2c904:	add	x8, x8, #0x20f
   2c908:	and	x8, x8, #0xfffffffffffffff0
   2c90c:	mov	x9, sp
   2c910:	sub	x5, x9, x8
   2c914:	mov	sp, x5
   2c918:	mov	x2, x4
   2c91c:	bl	c720 <__gmpn_toom44_mul@plt>
   2c920:	b	2c984 <__gmpn_mul_n@@Base+0x134>
   2c924:	mov	x8, sp
   2c928:	sub	x8, x8, x4, lsl #4
   2c92c:	sub	x5, x8, #0xc00
   2c930:	mov	sp, x5
   2c934:	mov	x2, x4
   2c938:	bl	cc20 <__gmpn_toom6h_mul@plt>
   2c93c:	b	2c984 <__gmpn_mul_n@@Base+0x134>
   2c940:	lsl	x8, x4, #4
   2c944:	sub	x8, x8, x4
   2c948:	add	x8, x8, #0xcf0
   2c94c:	and	x8, x8, #0xfffffffffffffff8
   2c950:	mov	w9, #0x7f00                	// #32512
   2c954:	cmp	x8, x9
   2c958:	str	xzr, [x19, #32]
   2c95c:	b.hi	2c994 <__gmpn_mul_n@@Base+0x144>  // b.pmore
   2c960:	add	x8, x8, #0xf
   2c964:	mov	x9, sp
   2c968:	and	x8, x8, #0xfffffffffffffff0
   2c96c:	sub	x5, x9, x8
   2c970:	mov	sp, x5
   2c974:	mov	x2, x4
   2c978:	bl	cb20 <__gmpn_toom8h_mul@plt>
   2c97c:	ldr	x0, [x19, #32]
   2c980:	cbnz	x0, 2c9bc <__gmpn_mul_n@@Base+0x16c>
   2c984:	mov	sp, x29
   2c988:	ldp	x28, x19, [sp, #16]
   2c98c:	ldp	x29, x30, [sp], #32
   2c990:	ret
   2c994:	stp	x1, x0, [x19, #16]
   2c998:	add	x0, x19, #0x20
   2c99c:	mov	x1, x8
   2c9a0:	stp	x3, x4, [x19]
   2c9a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2c9a8:	ldp	x4, x1, [x19, #8]
   2c9ac:	ldr	x3, [x19]
   2c9b0:	mov	x5, x0
   2c9b4:	ldr	x0, [x19, #24]
   2c9b8:	b	2c974 <__gmpn_mul_n@@Base+0x124>
   2c9bc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2c9c0:	b	2c984 <__gmpn_mul_n@@Base+0x134>

000000000002c9c4 <__gmpn_sqr@@Base>:
   2c9c4:	stp	x29, x30, [sp, #-32]!
   2c9c8:	stp	x28, x19, [sp, #16]
   2c9cc:	mov	x29, sp
   2c9d0:	sub	sp, sp, #0x840
   2c9d4:	mov	x8, x1
   2c9d8:	cmp	x2, #0x11
   2c9dc:	mov	x19, sp
   2c9e0:	b.le	2ca28 <__gmpn_sqr@@Base+0x64>
   2c9e4:	cmp	x2, #0x42
   2c9e8:	b.le	2ca3c <__gmpn_sqr@@Base+0x78>
   2c9ec:	cmp	x2, #0xa5
   2c9f0:	b.le	2ca4c <__gmpn_sqr@@Base+0x88>
   2c9f4:	cmp	x2, #0xdd
   2c9f8:	b.le	2ca74 <__gmpn_sqr@@Base+0xb0>
   2c9fc:	cmp	x2, #0x14c
   2ca00:	b.le	2ca9c <__gmpn_sqr@@Base+0xd8>
   2ca04:	cmp	x2, #0xa7f
   2ca08:	b.le	2cab8 <__gmpn_sqr@@Base+0xf4>
   2ca0c:	mov	x1, x8
   2ca10:	mov	x3, x8
   2ca14:	mov	x4, x2
   2ca18:	mov	sp, x29
   2ca1c:	ldp	x28, x19, [sp, #16]
   2ca20:	ldp	x29, x30, [sp], #32
   2ca24:	b	cca0 <__gmpn_nussbaumer_mul@plt>
   2ca28:	mov	x1, x8
   2ca2c:	mov	sp, x29
   2ca30:	ldp	x28, x19, [sp, #16]
   2ca34:	ldp	x29, x30, [sp], #32
   2ca38:	b	c190 <__gmpn_sqr_basecase@plt>
   2ca3c:	add	x3, x19, #0x20
   2ca40:	mov	x1, x8
   2ca44:	bl	c050 <__gmpn_toom2_sqr@plt>
   2ca48:	b	2cafc <__gmpn_sqr@@Base+0x138>
   2ca4c:	mov	w9, #0x18                  	// #24
   2ca50:	mul	x9, x2, x9
   2ca54:	add	x9, x9, #0x20f
   2ca58:	and	x9, x9, #0xfffffffffffffff0
   2ca5c:	mov	x10, sp
   2ca60:	sub	x3, x10, x9
   2ca64:	mov	sp, x3
   2ca68:	mov	x1, x8
   2ca6c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   2ca70:	b	2cafc <__gmpn_sqr@@Base+0x138>
   2ca74:	mov	w9, #0x18                  	// #24
   2ca78:	mul	x9, x2, x9
   2ca7c:	add	x9, x9, #0x20f
   2ca80:	and	x9, x9, #0xfffffffffffffff0
   2ca84:	mov	x10, sp
   2ca88:	sub	x3, x10, x9
   2ca8c:	mov	sp, x3
   2ca90:	mov	x1, x8
   2ca94:	bl	c220 <__gmpn_toom4_sqr@plt>
   2ca98:	b	2cafc <__gmpn_sqr@@Base+0x138>
   2ca9c:	mov	x9, sp
   2caa0:	sub	x9, x9, x2, lsl #4
   2caa4:	sub	x3, x9, #0xc00
   2caa8:	mov	sp, x3
   2caac:	mov	x1, x8
   2cab0:	bl	d470 <__gmpn_toom6_sqr@plt>
   2cab4:	b	2cafc <__gmpn_sqr@@Base+0x138>
   2cab8:	lsl	x9, x2, #4
   2cabc:	sub	x9, x9, x2
   2cac0:	add	x9, x9, #0xd50
   2cac4:	and	x1, x9, #0xfffffffffffffff8
   2cac8:	mov	w9, #0x7f00                	// #32512
   2cacc:	cmp	x1, x9
   2cad0:	str	xzr, [x19, #32]
   2cad4:	b.hi	2cb0c <__gmpn_sqr@@Base+0x148>  // b.pmore
   2cad8:	add	x10, x1, #0xf
   2cadc:	mov	x9, sp
   2cae0:	and	x10, x10, #0xfffffffffffffff0
   2cae4:	sub	x3, x9, x10
   2cae8:	mov	sp, x3
   2caec:	mov	x1, x8
   2caf0:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   2caf4:	ldr	x0, [x19, #32]
   2caf8:	cbnz	x0, 2cb2c <__gmpn_sqr@@Base+0x168>
   2cafc:	mov	sp, x29
   2cb00:	ldp	x28, x19, [sp, #16]
   2cb04:	ldp	x29, x30, [sp], #32
   2cb08:	ret
   2cb0c:	stp	x2, x0, [x19, #16]
   2cb10:	add	x0, x19, #0x20
   2cb14:	str	x8, [x19, #8]
   2cb18:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2cb1c:	ldp	x8, x2, [x19, #8]
   2cb20:	mov	x3, x0
   2cb24:	ldr	x0, [x19, #24]
   2cb28:	b	2caec <__gmpn_sqr@@Base+0x128>
   2cb2c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2cb30:	b	2cafc <__gmpn_sqr@@Base+0x138>

000000000002cb34 <__gmpn_mul_basecase@@Base>:
   2cb34:	stp	x29, x30, [sp, #-64]!
   2cb38:	stp	x22, x21, [sp, #32]
   2cb3c:	stp	x20, x19, [sp, #48]
   2cb40:	str	x23, [sp, #16]
   2cb44:	mov	x23, x3
   2cb48:	ldr	x3, [x3]
   2cb4c:	mov	x29, sp
   2cb50:	mov	x22, x4
   2cb54:	mov	x19, x2
   2cb58:	mov	x20, x1
   2cb5c:	mov	x21, x0
   2cb60:	bl	d490 <__gmpn_mul_1@plt>
   2cb64:	cmp	x22, #0x2
   2cb68:	str	x0, [x21, x19, lsl #3]
   2cb6c:	b.lt	2cba4 <__gmpn_mul_basecase@@Base+0x70>  // b.tstop
   2cb70:	add	x21, x21, #0x8
   2cb74:	add	x23, x23, #0x8
   2cb78:	add	x22, x22, #0x1
   2cb7c:	ldr	x3, [x23], #8
   2cb80:	mov	x0, x21
   2cb84:	mov	x1, x20
   2cb88:	mov	x2, x19
   2cb8c:	bl	d400 <__gmpn_addmul_1@plt>
   2cb90:	sub	x22, x22, #0x1
   2cb94:	str	x0, [x21, x19, lsl #3]
   2cb98:	cmp	x22, #0x2
   2cb9c:	add	x21, x21, #0x8
   2cba0:	b.gt	2cb7c <__gmpn_mul_basecase@@Base+0x48>
   2cba4:	ldp	x20, x19, [sp, #48]
   2cba8:	ldp	x22, x21, [sp, #32]
   2cbac:	ldr	x23, [sp, #16]
   2cbb0:	ldp	x29, x30, [sp], #64
   2cbb4:	ret

000000000002cbb8 <__gmpn_sqr_basecase@@Base>:
   2cbb8:	stp	x29, x30, [sp, #-64]!
   2cbbc:	stp	x24, x23, [sp, #16]
   2cbc0:	stp	x20, x19, [sp, #48]
   2cbc4:	mov	x19, x2
   2cbc8:	mov	x20, x1
   2cbcc:	subs	x2, x2, #0x1
   2cbd0:	mov	x23, x0
   2cbd4:	stp	x22, x21, [sp, #32]
   2cbd8:	mov	x29, sp
   2cbdc:	b.ne	2cc04 <__gmpn_sqr_basecase@@Base+0x4c>  // b.any
   2cbe0:	ldr	x8, [x20]
   2cbe4:	umulh	x9, x8, x8
   2cbe8:	mul	x8, x8, x8
   2cbec:	stp	x8, x9, [x23]
   2cbf0:	ldp	x20, x19, [sp, #48]
   2cbf4:	ldp	x22, x21, [sp, #32]
   2cbf8:	ldp	x24, x23, [sp, #16]
   2cbfc:	ldp	x29, x30, [sp], #64
   2cc00:	ret
   2cc04:	mov	x1, x20
   2cc08:	ldr	x3, [x1], #8
   2cc0c:	add	x22, x23, #0x8
   2cc10:	mov	x0, x22
   2cc14:	bl	d490 <__gmpn_mul_1@plt>
   2cc18:	subs	x21, x19, #0x2
   2cc1c:	str	x0, [x23, x19, lsl #3]
   2cc20:	b.eq	2cc68 <__gmpn_sqr_basecase@@Base+0xb0>  // b.none
   2cc24:	add	x8, x23, x19, lsl #3
   2cc28:	mov	x24, xzr
   2cc2c:	add	x22, x23, #0x18
   2cc30:	add	x23, x8, #0x8
   2cc34:	add	x8, x20, x24
   2cc38:	ldr	x3, [x8, #8]
   2cc3c:	add	x1, x8, #0x10
   2cc40:	mov	x0, x22
   2cc44:	mov	x2, x21
   2cc48:	bl	d400 <__gmpn_addmul_1@plt>
   2cc4c:	str	x0, [x23, x24]
   2cc50:	subs	x21, x21, #0x1
   2cc54:	add	x22, x22, #0x10
   2cc58:	add	x24, x24, #0x8
   2cc5c:	b.ne	2cc34 <__gmpn_sqr_basecase@@Base+0x7c>  // b.any
   2cc60:	sub	x22, x22, #0x10
   2cc64:	add	x20, x20, x24
   2cc68:	sub	x8, x22, x19, lsl #4
   2cc6c:	sub	x9, x20, x19, lsl #3
   2cc70:	mov	x3, x19
   2cc74:	ldp	x20, x19, [sp, #48]
   2cc78:	ldp	x22, x21, [sp, #32]
   2cc7c:	ldp	x24, x23, [sp, #16]
   2cc80:	add	x0, x8, #0x18
   2cc84:	add	x1, x8, #0x20
   2cc88:	add	x2, x9, #0x10
   2cc8c:	ldp	x29, x30, [sp], #64
   2cc90:	b	d330 <__gmpn_sqr_diag_addlsh1@plt>

000000000002cc94 <__gmpn_nussbaumer_mul@@Base>:
   2cc94:	stp	x29, x30, [sp, #-64]!
   2cc98:	stp	x24, x23, [sp, #16]
   2cc9c:	stp	x22, x21, [sp, #32]
   2cca0:	stp	x20, x19, [sp, #48]
   2cca4:	mov	x29, sp
   2cca8:	sub	sp, sp, #0x10
   2ccac:	mov	x22, x4
   2ccb0:	mov	x23, x3
   2ccb4:	mov	x19, x2
   2ccb8:	mov	x20, x1
   2ccbc:	mov	x21, x0
   2ccc0:	cmp	x1, x3
   2ccc4:	stur	xzr, [x29, #-8]
   2ccc8:	b.ne	2cd2c <__gmpn_nussbaumer_mul@@Base+0x98>  // b.any
   2cccc:	cmp	x19, x22
   2ccd0:	b.ne	2cd2c <__gmpn_nussbaumer_mul@@Base+0x98>  // b.any
   2ccd4:	lsl	x0, x19, #1
   2ccd8:	bl	c5d0 <__gmpn_sqrmod_bnm1_next_size@plt>
   2ccdc:	cmp	x19, x0, asr #1
   2cce0:	csel	x8, x19, xzr, gt
   2cce4:	add	x8, x0, x8
   2cce8:	lsl	x8, x8, #3
   2ccec:	add	x1, x8, #0x18
   2ccf0:	mov	w8, #0x7f00                	// #32512
   2ccf4:	mov	x22, x0
   2ccf8:	cmp	x1, x8
   2ccfc:	b.hi	2cdbc <__gmpn_nussbaumer_mul@@Base+0x128>  // b.pmore
   2cd00:	add	x9, x1, #0xf
   2cd04:	mov	x8, sp
   2cd08:	and	x9, x9, #0xfffffffffffffff0
   2cd0c:	sub	x4, x8, x9
   2cd10:	mov	sp, x4
   2cd14:	mov	x0, x21
   2cd18:	mov	x1, x22
   2cd1c:	mov	x2, x20
   2cd20:	mov	x3, x19
   2cd24:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   2cd28:	b	2cd94 <__gmpn_nussbaumer_mul@@Base+0x100>
   2cd2c:	add	x0, x22, x19
   2cd30:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   2cd34:	asr	x8, x0, #1
   2cd38:	cmp	x8, x22
   2cd3c:	csel	x9, x0, x8, lt  // lt = tstop
   2cd40:	cmp	x8, x19
   2cd44:	csel	x8, x9, xzr, lt  // lt = tstop
   2cd48:	add	x8, x0, x8
   2cd4c:	lsl	x8, x8, #3
   2cd50:	add	x1, x8, #0x20
   2cd54:	mov	w8, #0x7f00                	// #32512
   2cd58:	mov	x24, x0
   2cd5c:	cmp	x1, x8
   2cd60:	b.hi	2cdcc <__gmpn_nussbaumer_mul@@Base+0x138>  // b.pmore
   2cd64:	add	x9, x1, #0xf
   2cd68:	mov	x8, sp
   2cd6c:	and	x9, x9, #0xfffffffffffffff0
   2cd70:	sub	x6, x8, x9
   2cd74:	mov	sp, x6
   2cd78:	mov	x0, x21
   2cd7c:	mov	x1, x24
   2cd80:	mov	x2, x20
   2cd84:	mov	x3, x19
   2cd88:	mov	x4, x23
   2cd8c:	mov	x5, x22
   2cd90:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   2cd94:	ldur	x0, [x29, #-8]
   2cd98:	cbnz	x0, 2cdb4 <__gmpn_nussbaumer_mul@@Base+0x120>
   2cd9c:	mov	sp, x29
   2cda0:	ldp	x20, x19, [sp, #48]
   2cda4:	ldp	x22, x21, [sp, #32]
   2cda8:	ldp	x24, x23, [sp, #16]
   2cdac:	ldp	x29, x30, [sp], #64
   2cdb0:	ret
   2cdb4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2cdb8:	b	2cd9c <__gmpn_nussbaumer_mul@@Base+0x108>
   2cdbc:	sub	x0, x29, #0x8
   2cdc0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2cdc4:	mov	x4, x0
   2cdc8:	b	2cd14 <__gmpn_nussbaumer_mul@@Base+0x80>
   2cdcc:	sub	x0, x29, #0x8
   2cdd0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2cdd4:	mov	x6, x0
   2cdd8:	b	2cd78 <__gmpn_nussbaumer_mul@@Base+0xe4>

000000000002cddc <__gmpn_mulmid_basecase@@Base>:
   2cddc:	stp	x29, x30, [sp, #-80]!
   2cde0:	stp	x26, x25, [sp, #16]
   2cde4:	stp	x24, x23, [sp, #32]
   2cde8:	stp	x22, x21, [sp, #48]
   2cdec:	stp	x20, x19, [sp, #64]
   2cdf0:	mov	x23, x3
   2cdf4:	ldr	x3, [x3]
   2cdf8:	sub	x26, x4, #0x1
   2cdfc:	sub	x20, x2, x26
   2ce00:	mov	x24, x1
   2ce04:	add	x1, x1, x26, lsl #3
   2ce08:	mov	x2, x20
   2ce0c:	mov	x29, sp
   2ce10:	mov	x22, x4
   2ce14:	mov	x19, x0
   2ce18:	bl	d490 <__gmpn_mul_1@plt>
   2ce1c:	mov	x21, x0
   2ce20:	mov	x25, xzr
   2ce24:	cbz	x26, 2ce68 <__gmpn_mulmid_basecase@@Base+0x8c>
   2ce28:	add	x8, x24, x22, lsl #3
   2ce2c:	sub	x22, x8, #0x10
   2ce30:	add	x23, x23, #0x8
   2ce34:	ldr	x3, [x23], #8
   2ce38:	mov	x0, x19
   2ce3c:	mov	x1, x22
   2ce40:	mov	x2, x20
   2ce44:	bl	d400 <__gmpn_addmul_1@plt>
   2ce48:	mov	x9, xzr
   2ce4c:	adds	x8, x21, x0
   2ce50:	adc	x25, x25, x9
   2ce54:	sub	x26, x26, #0x1
   2ce58:	sub	x22, x22, #0x8
   2ce5c:	mov	x21, x8
   2ce60:	cbnz	x26, 2ce34 <__gmpn_mulmid_basecase@@Base+0x58>
   2ce64:	mov	x21, x8
   2ce68:	add	x8, x19, x20, lsl #3
   2ce6c:	stp	x21, x25, [x8]
   2ce70:	ldp	x20, x19, [sp, #64]
   2ce74:	ldp	x22, x21, [sp, #48]
   2ce78:	ldp	x24, x23, [sp, #32]
   2ce7c:	ldp	x26, x25, [sp, #16]
   2ce80:	ldp	x29, x30, [sp], #80
   2ce84:	ret

000000000002ce88 <__gmpn_toom42_mulmid@@Base>:
   2ce88:	sub	sp, sp, #0x110
   2ce8c:	cmp	x3, #0x0
   2ce90:	stp	x22, x21, [sp, #240]
   2ce94:	cinc	x22, x3, lt  // lt = tstop
   2ce98:	stp	x24, x23, [sp, #224]
   2ce9c:	and	x8, x3, #0x1
   2cea0:	asr	x24, x22, #1
   2cea4:	stp	x20, x19, [sp, #256]
   2cea8:	add	x23, x1, x8, lsl #3
   2ceac:	lsl	x20, x24, #3
   2ceb0:	stp	x28, x27, [sp, #192]
   2ceb4:	stp	x26, x25, [sp, #208]
   2ceb8:	add	x28, x4, #0x10
   2cebc:	add	x26, x23, x20
   2cec0:	add	x25, x2, x20
   2cec4:	mov	x19, x2
   2cec8:	mov	x21, x0
   2cecc:	str	x4, [sp, #48]
   2ced0:	str	x3, [sp, #64]
   2ced4:	sub	x5, x24, #0x1
   2ced8:	add	x3, sp, #0x48
   2cedc:	mov	x0, x28
   2cee0:	mov	x1, x23
   2cee4:	mov	x2, x26
   2cee8:	mov	x4, x25
   2ceec:	mov	x6, xzr
   2cef0:	stp	x29, x30, [sp, #176]
   2cef4:	add	x29, sp, #0xb0
   2cef8:	str	x8, [sp, #8]
   2cefc:	bl	ccf0 <__gmpn_add_err1_n@plt>
   2cf00:	add	x8, x28, x20
   2cf04:	lsl	x27, x24, #4
   2cf08:	mov	x7, x0
   2cf0c:	str	x8, [sp, #16]
   2cf10:	sub	x0, x8, #0x8
   2cf14:	add	x8, x23, x27
   2cf18:	stp	x26, x20, [sp, #24]
   2cf1c:	sub	x20, x26, #0x8
   2cf20:	sub	x26, x8, #0x8
   2cf24:	add	x8, sp, #0x48
   2cf28:	add	x3, x8, #0x10
   2cf2c:	mov	x1, x20
   2cf30:	mov	x2, x26
   2cf34:	mov	x4, x25
   2cf38:	mov	x5, x19
   2cf3c:	mov	x6, x24
   2cf40:	and	x22, x22, #0xfffffffffffffffe
   2cf44:	bl	c630 <__gmpn_add_err2_n@plt>
   2cf48:	add	x8, x28, x27
   2cf4c:	str	x22, [sp, #40]
   2cf50:	add	x22, x22, x24
   2cf54:	mov	x6, x0
   2cf58:	sub	x0, x8, #0x8
   2cf5c:	add	x8, x23, x22, lsl #3
   2cf60:	sub	x2, x8, #0x8
   2cf64:	add	x8, sp, #0x48
   2cf68:	add	x3, x8, #0x30
   2cf6c:	mov	x1, x26
   2cf70:	mov	x4, x19
   2cf74:	mov	x5, x24
   2cf78:	str	x28, [sp, #56]
   2cf7c:	str	x23, [sp]
   2cf80:	bl	ccf0 <__gmpn_add_err1_n@plt>
   2cf84:	sub	x8, x19, #0x8
   2cf88:	mov	x9, x24
   2cf8c:	subs	x10, x9, #0x1
   2cf90:	b.lt	2cfb0 <__gmpn_toom42_mulmid@@Base+0x128>  // b.tstop
   2cf94:	ldr	x11, [x8, x27]
   2cf98:	ldr	x9, [x8, x9, lsl #3]
   2cf9c:	sub	x27, x27, #0x8
   2cfa0:	cmp	x11, x9
   2cfa4:	mov	x9, x10
   2cfa8:	b.eq	2cf8c <__gmpn_toom42_mulmid@@Base+0x104>  // b.none
   2cfac:	b.ls	2cfec <__gmpn_toom42_mulmid@@Base+0x164>  // b.plast
   2cfb0:	add	x9, x21, x24, lsl #3
   2cfb4:	add	x8, sp, #0x48
   2cfb8:	add	x27, x9, #0x10
   2cfbc:	add	x3, x8, #0x40
   2cfc0:	mov	x0, x27
   2cfc4:	mov	x1, x25
   2cfc8:	mov	x2, x19
   2cfcc:	mov	x4, x20
   2cfd0:	mov	x5, x26
   2cfd4:	mov	x6, x24
   2cfd8:	mov	x7, xzr
   2cfdc:	mov	x23, x9
   2cfe0:	bl	d370 <__gmpn_sub_err2_n@plt>
   2cfe4:	mov	w26, wzr
   2cfe8:	b	2d024 <__gmpn_toom42_mulmid@@Base+0x19c>
   2cfec:	add	x9, x21, x24, lsl #3
   2cff0:	add	x8, sp, #0x48
   2cff4:	add	x27, x9, #0x10
   2cff8:	add	x3, x8, #0x40
   2cffc:	mov	x0, x27
   2d000:	mov	x1, x19
   2d004:	mov	x2, x25
   2d008:	mov	x4, x20
   2d00c:	mov	x5, x26
   2d010:	mov	x6, x24
   2d014:	mov	x7, xzr
   2d018:	mov	x23, x9
   2d01c:	bl	d370 <__gmpn_sub_err2_n@plt>
   2d020:	mov	w26, #0x1                   	// #1
   2d024:	ldr	x8, [sp, #64]
   2d028:	cmp	x8, #0x27
   2d02c:	ldr	x8, [sp, #40]
   2d030:	b.gt	2d0ac <__gmpn_toom42_mulmid@@Base+0x224>
   2d034:	ldr	x1, [sp, #56]
   2d038:	sub	x20, x8, #0x1
   2d03c:	mov	x0, x21
   2d040:	mov	x2, x20
   2d044:	mov	x3, x25
   2d048:	mov	x4, x24
   2d04c:	mov	x28, x8
   2d050:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d054:	add	x8, x21, x24, lsl #3
   2d058:	ldp	x9, x8, [x8]
   2d05c:	ldp	x10, x11, [sp, #88]
   2d060:	ldr	x22, [sp, #48]
   2d064:	ldr	x1, [sp, #24]
   2d068:	mov	x2, x20
   2d06c:	adds	x9, x9, x10
   2d070:	cinc	x8, x8, cs  // cs = hs, nlast
   2d074:	add	x8, x8, x11
   2d078:	mov	x0, x22
   2d07c:	mov	x3, x27
   2d080:	mov	x4, x24
   2d084:	stp	x9, x8, [sp, #88]
   2d088:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d08c:	ldr	x1, [sp, #16]
   2d090:	mov	x0, x23
   2d094:	mov	x2, x20
   2d098:	mov	x3, x19
   2d09c:	mov	x4, x24
   2d0a0:	mov	x25, x23
   2d0a4:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d0a8:	b	2d124 <__gmpn_toom42_mulmid@@Base+0x29c>
   2d0ac:	ldp	x9, x1, [sp, #48]
   2d0b0:	mov	x28, x8
   2d0b4:	mov	x0, x21
   2d0b8:	mov	x2, x25
   2d0bc:	add	x8, x9, x22, lsl #3
   2d0c0:	add	x20, x8, #0x8
   2d0c4:	mov	x3, x24
   2d0c8:	mov	x4, x20
   2d0cc:	mov	x22, x9
   2d0d0:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d0d4:	add	x8, x21, x24, lsl #3
   2d0d8:	ldp	x9, x8, [x8]
   2d0dc:	ldp	x10, x11, [sp, #88]
   2d0e0:	ldr	x1, [sp, #24]
   2d0e4:	mov	x0, x22
   2d0e8:	mov	x2, x27
   2d0ec:	adds	x9, x9, x10
   2d0f0:	cinc	x8, x8, cs  // cs = hs, nlast
   2d0f4:	add	x8, x8, x11
   2d0f8:	mov	x3, x24
   2d0fc:	mov	x4, x20
   2d100:	stp	x9, x8, [sp, #88]
   2d104:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d108:	ldr	x1, [sp, #16]
   2d10c:	mov	x0, x23
   2d110:	mov	x2, x19
   2d114:	mov	x3, x24
   2d118:	mov	x4, x20
   2d11c:	mov	x25, x23
   2d120:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d124:	ldr	x8, [sp, #72]
   2d128:	ldp	x9, x10, [x21]
   2d12c:	ldr	x14, [sp, #32]
   2d130:	subs	x8, x9, x8
   2d134:	str	x8, [x21]
   2d138:	ldr	x8, [sp, #80]
   2d13c:	cinc	x8, x8, cc  // cc = lo, ul, last
   2d140:	subs	x8, x10, x8
   2d144:	str	x8, [x21, #8]
   2d148:	b.cc	2d3d0 <__gmpn_toom42_mulmid@@Base+0x548>  // b.lo, b.ul, b.last
   2d14c:	ldp	x8, x9, [sp, #88]
   2d150:	ldp	x10, x12, [sp, #104]
   2d154:	ldr	x11, [x21, x14]
   2d158:	subs	x8, x8, x10
   2d15c:	cset	w10, cc  // cc = lo, ul, last
   2d160:	adds	x11, x11, x8
   2d164:	add	x8, x24, #0x1
   2d168:	lsl	x8, x8, #3
   2d16c:	str	x11, [x21, x14]
   2d170:	ldr	x11, [x21, x8]
   2d174:	sub	x9, x9, x12
   2d178:	sub	x9, x9, x10
   2d17c:	cinc	x9, x9, cs  // cs = hs, nlast
   2d180:	adds	x10, x9, x11
   2d184:	asr	x9, x9, #63
   2d188:	cinc	x9, x9, cs  // cs = hs, nlast
   2d18c:	str	x10, [x21, x8]
   2d190:	cbnz	x9, 2d424 <__gmpn_toom42_mulmid@@Base+0x59c>
   2d194:	lsl	x9, x28, #3
   2d198:	ldr	x10, [sp, #120]
   2d19c:	ldr	x11, [x21, x9]
   2d1a0:	adds	x10, x10, x11
   2d1a4:	orr	x11, x9, #0x8
   2d1a8:	str	x10, [x21, x9]
   2d1ac:	ldr	x9, [x21, x11]
   2d1b0:	ldr	x10, [sp, #128]
   2d1b4:	add	x9, x10, x9
   2d1b8:	cinc	x9, x9, cs  // cs = hs, nlast
   2d1bc:	str	x9, [x21, x11]
   2d1c0:	ldr	x9, [sp, #136]
   2d1c4:	ldp	x10, x11, [x22]
   2d1c8:	adds	x9, x9, x10
   2d1cc:	str	x9, [x22]
   2d1d0:	ldr	x9, [sp, #144]
   2d1d4:	cinc	x10, x11, cs  // cs = hs, nlast
   2d1d8:	add	x9, x10, x9
   2d1dc:	cmp	x9, x11
   2d1e0:	str	x9, [x22, #8]
   2d1e4:	b.cc	2d464 <__gmpn_toom42_mulmid@@Base+0x5dc>  // b.lo, b.ul, b.last
   2d1e8:	ldr	x9, [x22, x14]
   2d1ec:	ldr	x10, [sp, #152]
   2d1f0:	subs	x9, x9, x10
   2d1f4:	str	x9, [x22, x14]
   2d1f8:	ldr	x9, [x22, x8]
   2d1fc:	ldr	x10, [sp, #160]
   2d200:	cset	w11, cc  // cc = lo, ul, last
   2d204:	sub	x9, x9, x10
   2d208:	sub	x9, x9, x11
   2d20c:	str	x9, [x22, x8]
   2d210:	ldr	x8, [x27]
   2d214:	lsr	x9, x9, #63
   2d218:	cbz	w26, 2d338 <__gmpn_toom42_mulmid@@Base+0x4b0>
   2d21c:	subs	x8, x8, x9
   2d220:	str	x8, [x27]
   2d224:	b.cs	2d24c <__gmpn_toom42_mulmid@@Base+0x3c4>  // b.hs, b.nlast
   2d228:	mov	w8, #0x1                   	// #1
   2d22c:	cmp	x8, x24
   2d230:	b.ge	2d24c <__gmpn_toom42_mulmid@@Base+0x3c4>  // b.tcont
   2d234:	lsl	x9, x8, #3
   2d238:	ldr	x10, [x27, x9]
   2d23c:	add	x8, x8, #0x1
   2d240:	sub	x11, x10, #0x1
   2d244:	str	x11, [x27, x9]
   2d248:	cbz	x10, 2d22c <__gmpn_toom42_mulmid@@Base+0x3a4>
   2d24c:	adds	x20, x24, #0x2
   2d250:	b.eq	2d294 <__gmpn_toom42_mulmid@@Base+0x40c>  // b.none
   2d254:	mov	x0, x21
   2d258:	mov	x1, x21
   2d25c:	mov	x2, x22
   2d260:	mov	x3, x20
   2d264:	bl	ca70 <__gmpn_add_n@plt>
   2d268:	cbz	x0, 2d294 <__gmpn_toom42_mulmid@@Base+0x40c>
   2d26c:	add	x8, x28, #0x2
   2d270:	mov	x9, x20
   2d274:	cmp	x9, x8
   2d278:	b.ge	2d294 <__gmpn_toom42_mulmid@@Base+0x40c>  // b.tcont
   2d27c:	lsl	x10, x9, #3
   2d280:	ldr	x11, [x21, x10]
   2d284:	add	x9, x9, #0x1
   2d288:	adds	x11, x11, #0x1
   2d28c:	str	x11, [x21, x10]
   2d290:	b.cs	2d274 <__gmpn_toom42_mulmid@@Base+0x3ec>  // b.hs, b.nlast
   2d294:	mov	x0, x25
   2d298:	mov	x1, x25
   2d29c:	mov	x2, x22
   2d2a0:	mov	x3, x20
   2d2a4:	bl	c2d0 <__gmpn_sub_n@plt>
   2d2a8:	ldr	x8, [sp, #8]
   2d2ac:	cbz	x8, 2d318 <__gmpn_toom42_mulmid@@Base+0x490>
   2d2b0:	ldr	x23, [sp, #64]
   2d2b4:	ldr	x22, [sp]
   2d2b8:	mov	x0, x21
   2d2bc:	sub	x20, x23, #0x1
   2d2c0:	ldr	x3, [x19, x20, lsl #3]
   2d2c4:	sub	x1, x22, #0x8
   2d2c8:	mov	x2, x23
   2d2cc:	bl	d400 <__gmpn_addmul_1@plt>
   2d2d0:	lsl	x8, x23, #3
   2d2d4:	mov	x3, x19
   2d2d8:	add	x19, x21, x8
   2d2dc:	ldr	x9, [x19]
   2d2e0:	add	x8, x22, x8
   2d2e4:	sub	x1, x8, #0x8
   2d2e8:	mov	x2, x20
   2d2ec:	adds	x9, x9, x0
   2d2f0:	cset	w10, cs  // cs = hs, nlast
   2d2f4:	add	x0, sp, #0x48
   2d2f8:	mov	x4, x20
   2d2fc:	stp	x9, x10, [x19]
   2d300:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d304:	sub	x0, x19, #0x8
   2d308:	add	x2, sp, #0x48
   2d30c:	mov	w3, #0x3                   	// #3
   2d310:	mov	x1, x0
   2d314:	bl	ca70 <__gmpn_add_n@plt>
   2d318:	ldp	x20, x19, [sp, #256]
   2d31c:	ldp	x22, x21, [sp, #240]
   2d320:	ldp	x24, x23, [sp, #224]
   2d324:	ldp	x26, x25, [sp, #208]
   2d328:	ldp	x28, x27, [sp, #192]
   2d32c:	ldp	x29, x30, [sp, #176]
   2d330:	add	sp, sp, #0x110
   2d334:	ret
   2d338:	adds	x8, x9, x8
   2d33c:	str	x8, [x27]
   2d340:	b.cc	2d368 <__gmpn_toom42_mulmid@@Base+0x4e0>  // b.lo, b.ul, b.last
   2d344:	mov	w8, #0x1                   	// #1
   2d348:	cmp	x8, x24
   2d34c:	b.ge	2d368 <__gmpn_toom42_mulmid@@Base+0x4e0>  // b.tcont
   2d350:	lsl	x9, x8, #3
   2d354:	ldr	x10, [x27, x9]
   2d358:	add	x8, x8, #0x1
   2d35c:	adds	x10, x10, #0x1
   2d360:	str	x10, [x27, x9]
   2d364:	b.cs	2d348 <__gmpn_toom42_mulmid@@Base+0x4c0>  // b.hs, b.nlast
   2d368:	adds	x20, x24, #0x2
   2d36c:	b.eq	2d3b0 <__gmpn_toom42_mulmid@@Base+0x528>  // b.none
   2d370:	mov	x0, x21
   2d374:	mov	x1, x21
   2d378:	mov	x2, x22
   2d37c:	mov	x3, x20
   2d380:	bl	c2d0 <__gmpn_sub_n@plt>
   2d384:	cbz	x0, 2d3b0 <__gmpn_toom42_mulmid@@Base+0x528>
   2d388:	add	x8, x28, #0x2
   2d38c:	mov	x9, x20
   2d390:	cmp	x9, x8
   2d394:	b.ge	2d3b0 <__gmpn_toom42_mulmid@@Base+0x528>  // b.tcont
   2d398:	lsl	x10, x9, #3
   2d39c:	ldr	x11, [x21, x10]
   2d3a0:	add	x9, x9, #0x1
   2d3a4:	sub	x12, x11, #0x1
   2d3a8:	str	x12, [x21, x10]
   2d3ac:	cbz	x11, 2d390 <__gmpn_toom42_mulmid@@Base+0x508>
   2d3b0:	mov	x0, x25
   2d3b4:	mov	x1, x25
   2d3b8:	mov	x2, x22
   2d3bc:	mov	x3, x20
   2d3c0:	bl	ca70 <__gmpn_add_n@plt>
   2d3c4:	ldr	x8, [sp, #8]
   2d3c8:	cbnz	x8, 2d2b0 <__gmpn_toom42_mulmid@@Base+0x428>
   2d3cc:	b	2d318 <__gmpn_toom42_mulmid@@Base+0x490>
   2d3d0:	ldr	x8, [sp, #64]
   2d3d4:	cmp	x8, #0x6
   2d3d8:	b.lt	2d4a4 <__gmpn_toom42_mulmid@@Base+0x61c>  // b.tstop
   2d3dc:	ldr	x8, [x21, #16]
   2d3e0:	sub	x9, x8, #0x1
   2d3e4:	str	x9, [x21, #16]
   2d3e8:	cbnz	x8, 2d41c <__gmpn_toom42_mulmid@@Base+0x594>
   2d3ec:	sub	x9, x24, #0x2
   2d3f0:	mov	w10, #0x3                   	// #3
   2d3f4:	mov	w8, #0x1                   	// #1
   2d3f8:	sub	x11, x10, #0x2
   2d3fc:	cmp	x11, x9
   2d400:	b.ge	2d4a8 <__gmpn_toom42_mulmid@@Base+0x620>  // b.tcont
   2d404:	lsl	x11, x10, #3
   2d408:	ldr	x12, [x21, x11]
   2d40c:	add	x10, x10, #0x1
   2d410:	sub	x13, x12, #0x1
   2d414:	str	x13, [x21, x11]
   2d418:	cbz	x12, 2d3f8 <__gmpn_toom42_mulmid@@Base+0x570>
   2d41c:	mov	x8, xzr
   2d420:	b	2d4a8 <__gmpn_toom42_mulmid@@Base+0x620>
   2d424:	cmp	x9, #0x1
   2d428:	b.ne	2d4c0 <__gmpn_toom42_mulmid@@Base+0x638>  // b.any
   2d42c:	ldr	x9, [x27]
   2d430:	adds	x9, x9, #0x1
   2d434:	str	x9, [x27]
   2d438:	b.cc	2d194 <__gmpn_toom42_mulmid@@Base+0x30c>  // b.lo, b.ul, b.last
   2d43c:	mov	w9, #0x1                   	// #1
   2d440:	cmp	x9, x24
   2d444:	b.ge	2d194 <__gmpn_toom42_mulmid@@Base+0x30c>  // b.tcont
   2d448:	lsl	x10, x9, #3
   2d44c:	ldr	x11, [x27, x10]
   2d450:	add	x9, x9, #0x1
   2d454:	adds	x11, x11, #0x1
   2d458:	str	x11, [x27, x10]
   2d45c:	b.cs	2d440 <__gmpn_toom42_mulmid@@Base+0x5b8>  // b.hs, b.nlast
   2d460:	b	2d194 <__gmpn_toom42_mulmid@@Base+0x30c>
   2d464:	ldr	x10, [sp, #56]
   2d468:	ldr	x9, [x10]
   2d46c:	adds	x9, x9, #0x1
   2d470:	str	x9, [x10]
   2d474:	b.cc	2d1e8 <__gmpn_toom42_mulmid@@Base+0x360>  // b.lo, b.ul, b.last
   2d478:	mov	w9, #0x3                   	// #3
   2d47c:	sub	x10, x9, #0x2
   2d480:	cmp	x10, x24
   2d484:	b.ge	2d1e8 <__gmpn_toom42_mulmid@@Base+0x360>  // b.tcont
   2d488:	lsl	x10, x9, #3
   2d48c:	ldr	x11, [x22, x10]
   2d490:	add	x9, x9, #0x1
   2d494:	adds	x11, x11, #0x1
   2d498:	str	x11, [x22, x10]
   2d49c:	b.cs	2d47c <__gmpn_toom42_mulmid@@Base+0x5f4>  // b.hs, b.nlast
   2d4a0:	b	2d1e8 <__gmpn_toom42_mulmid@@Base+0x360>
   2d4a4:	mov	w8, #0x1                   	// #1
   2d4a8:	ldp	x9, x10, [sp, #88]
   2d4ac:	subs	x8, x9, x8
   2d4b0:	cset	w9, cc  // cc = lo, ul, last
   2d4b4:	sub	x9, x10, x9
   2d4b8:	stp	x8, x9, [sp, #88]
   2d4bc:	b	2d150 <__gmpn_toom42_mulmid@@Base+0x2c8>
   2d4c0:	ldr	x9, [x27]
   2d4c4:	sub	x10, x9, #0x1
   2d4c8:	str	x10, [x27]
   2d4cc:	cbnz	x9, 2d194 <__gmpn_toom42_mulmid@@Base+0x30c>
   2d4d0:	mov	w9, #0x1                   	// #1
   2d4d4:	cmp	x9, x24
   2d4d8:	b.ge	2d194 <__gmpn_toom42_mulmid@@Base+0x30c>  // b.tcont
   2d4dc:	lsl	x10, x9, #3
   2d4e0:	ldr	x11, [x27, x10]
   2d4e4:	add	x9, x9, #0x1
   2d4e8:	sub	x12, x11, #0x1
   2d4ec:	str	x12, [x27, x10]
   2d4f0:	cbz	x11, 2d4d4 <__gmpn_toom42_mulmid@@Base+0x64c>
   2d4f4:	b	2d194 <__gmpn_toom42_mulmid@@Base+0x30c>

000000000002d4f8 <__gmpn_mulmid_n@@Base>:
   2d4f8:	stp	x29, x30, [sp, #-48]!
   2d4fc:	stp	x22, x21, [sp, #16]
   2d500:	stp	x20, x19, [sp, #32]
   2d504:	mov	x29, sp
   2d508:	sub	sp, sp, #0x10
   2d50c:	mov	x19, x3
   2d510:	mov	x20, x2
   2d514:	mov	x21, x1
   2d518:	cmp	x3, #0x13
   2d51c:	mov	x22, x0
   2d520:	b.gt	2d550 <__gmpn_mulmid_n@@Base+0x58>
   2d524:	lsl	x8, x19, #1
   2d528:	sub	x2, x8, #0x1
   2d52c:	mov	x0, x22
   2d530:	mov	x1, x21
   2d534:	mov	x3, x20
   2d538:	mov	x4, x19
   2d53c:	mov	sp, x29
   2d540:	ldp	x20, x19, [sp, #32]
   2d544:	ldp	x22, x21, [sp, #16]
   2d548:	ldp	x29, x30, [sp], #48
   2d54c:	b	c460 <__gmpn_mulmid_basecase@plt>
   2d550:	mov	w8, #0x18                  	// #24
   2d554:	orr	x9, xzr, #0x200
   2d558:	madd	x1, x19, x8, x9
   2d55c:	mov	w8, #0x7f00                	// #32512
   2d560:	cmp	x1, x8
   2d564:	stur	xzr, [x29, #-8]
   2d568:	b.hi	2d5b0 <__gmpn_mulmid_n@@Base+0xb8>  // b.pmore
   2d56c:	add	x9, x1, #0xf
   2d570:	mov	x8, sp
   2d574:	and	x9, x9, #0xfffffffffffffff0
   2d578:	sub	x4, x8, x9
   2d57c:	mov	sp, x4
   2d580:	mov	x0, x22
   2d584:	mov	x1, x21
   2d588:	mov	x2, x20
   2d58c:	mov	x3, x19
   2d590:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d594:	ldur	x0, [x29, #-8]
   2d598:	cbnz	x0, 2d5c0 <__gmpn_mulmid_n@@Base+0xc8>
   2d59c:	mov	sp, x29
   2d5a0:	ldp	x20, x19, [sp, #32]
   2d5a4:	ldp	x22, x21, [sp, #16]
   2d5a8:	ldp	x29, x30, [sp], #48
   2d5ac:	ret
   2d5b0:	sub	x0, x29, #0x8
   2d5b4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2d5b8:	mov	x4, x0
   2d5bc:	b	2d580 <__gmpn_mulmid_n@@Base+0x88>
   2d5c0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2d5c4:	b	2d59c <__gmpn_mulmid_n@@Base+0xa4>

000000000002d5c8 <__gmpn_mulmid@@Base>:
   2d5c8:	stp	x29, x30, [sp, #-96]!
   2d5cc:	stp	x28, x27, [sp, #16]
   2d5d0:	stp	x26, x25, [sp, #32]
   2d5d4:	stp	x24, x23, [sp, #48]
   2d5d8:	stp	x22, x21, [sp, #64]
   2d5dc:	stp	x20, x19, [sp, #80]
   2d5e0:	mov	x29, sp
   2d5e4:	sub	sp, sp, #0x30
   2d5e8:	mov	x19, x4
   2d5ec:	mov	x22, x3
   2d5f0:	mov	x24, x2
   2d5f4:	mov	x20, x1
   2d5f8:	cmp	x4, #0x13
   2d5fc:	mov	x21, x0
   2d600:	b.gt	2d640 <__gmpn_mulmid@@Base+0x78>
   2d604:	cmp	x24, #0xdb
   2d608:	b.gt	2d670 <__gmpn_mulmid@@Base+0xa8>
   2d60c:	mov	x0, x21
   2d610:	mov	x1, x20
   2d614:	mov	x2, x24
   2d618:	mov	x3, x22
   2d61c:	mov	x4, x19
   2d620:	mov	sp, x29
   2d624:	ldp	x20, x19, [sp, #80]
   2d628:	ldp	x22, x21, [sp, #64]
   2d62c:	ldp	x24, x23, [sp, #48]
   2d630:	ldp	x26, x25, [sp, #32]
   2d634:	ldp	x28, x27, [sp, #16]
   2d638:	ldp	x29, x30, [sp], #96
   2d63c:	b	c460 <__gmpn_mulmid_basecase@plt>
   2d640:	sub	x28, x24, x19
   2d644:	cmp	x28, #0x13
   2d648:	b.ge	2d724 <__gmpn_mulmid@@Base+0x15c>  // b.tcont
   2d64c:	cmp	x19, #0xdb
   2d650:	b.gt	2d864 <__gmpn_mulmid@@Base+0x29c>
   2d654:	mov	x0, x21
   2d658:	mov	x1, x20
   2d65c:	mov	x2, x24
   2d660:	mov	x3, x22
   2d664:	mov	x4, x19
   2d668:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d66c:	b	2da64 <__gmpn_mulmid@@Base+0x49c>
   2d670:	mov	w8, #0xdd                  	// #221
   2d674:	mov	w2, #0xdc                  	// #220
   2d678:	mov	x0, x21
   2d67c:	mov	x1, x20
   2d680:	mov	x3, x22
   2d684:	mov	x4, x19
   2d688:	sub	x25, x8, x19
   2d68c:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d690:	sub	x23, x24, x25
   2d694:	cmp	x23, #0xdc
   2d698:	b.lt	2d940 <__gmpn_mulmid@@Base+0x378>  // b.tstop
   2d69c:	lsl	x8, x19, #3
   2d6a0:	mov	w9, #0x6e8                 	// #1768
   2d6a4:	mov	w10, #0x6f8                 	// #1784
   2d6a8:	sub	x26, x9, x8
   2d6ac:	sub	x8, x10, x8
   2d6b0:	stur	x8, [x29, #-16]
   2d6b4:	b	2d6c8 <__gmpn_mulmid@@Base+0x100>
   2d6b8:	sub	x23, x23, x25
   2d6bc:	cmp	x23, #0xdb
   2d6c0:	mov	x21, x24
   2d6c4:	b.le	2d944 <__gmpn_mulmid@@Base+0x37c>
   2d6c8:	add	x24, x21, x26
   2d6cc:	ldp	x28, x27, [x24]
   2d6d0:	add	x20, x20, x25, lsl #3
   2d6d4:	mov	w2, #0xdc                  	// #220
   2d6d8:	mov	x0, x24
   2d6dc:	mov	x1, x20
   2d6e0:	mov	x3, x22
   2d6e4:	mov	x4, x19
   2d6e8:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d6ec:	ldp	x8, x9, [x24]
   2d6f0:	adds	x8, x8, x28
   2d6f4:	str	x8, [x24]
   2d6f8:	cinc	x8, x27, cs  // cs = hs, nlast
   2d6fc:	adds	x8, x9, x8
   2d700:	str	x8, [x24, #8]
   2d704:	b.cc	2d6b8 <__gmpn_mulmid@@Base+0xf0>  // b.lo, b.ul, b.last
   2d708:	ldur	x8, [x29, #-16]
   2d70c:	ldr	x9, [x21, x8]
   2d710:	adds	x9, x9, #0x1
   2d714:	str	x9, [x21, x8]
   2d718:	add	x8, x8, #0x8
   2d71c:	b.cs	2d70c <__gmpn_mulmid@@Base+0x144>  // b.hs, b.nlast
   2d720:	b	2d6b8 <__gmpn_mulmid@@Base+0xf0>
   2d724:	add	x23, x28, #0x1
   2d728:	subs	x26, x23, x19
   2d72c:	b.ge	2d9ac <__gmpn_mulmid@@Base+0x3e4>  // b.tcont
   2d730:	add	x8, x23, x23, lsl #1
   2d734:	add	x8, x28, x8
   2d738:	lsl	x8, x8, #3
   2d73c:	add	x1, x8, #0x218
   2d740:	mov	w8, #0x7f00                	// #32512
   2d744:	cmp	x1, x8
   2d748:	add	x8, x28, #0x3
   2d74c:	stp	x8, xzr, [x29, #-16]
   2d750:	b.hi	2db08 <__gmpn_mulmid@@Base+0x540>  // b.pmore
   2d754:	add	x9, x1, #0xf
   2d758:	mov	x8, sp
   2d75c:	and	x9, x9, #0xfffffffffffffff0
   2d760:	sub	x26, x8, x9
   2d764:	mov	sp, x26
   2d768:	add	x8, x26, x23, lsl #3
   2d76c:	sub	x27, x19, x23
   2d770:	add	x4, x8, #0x10
   2d774:	add	x2, x22, x27, lsl #3
   2d778:	mov	x0, x21
   2d77c:	mov	x1, x20
   2d780:	mov	x3, x23
   2d784:	mov	x25, x2
   2d788:	stur	x4, [x29, #-40]
   2d78c:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d790:	cmp	x27, x28
   2d794:	b.le	2d830 <__gmpn_mulmid@@Base+0x268>
   2d798:	lsl	x11, x24, #3
   2d79c:	lsl	x8, x19, #3
   2d7a0:	sub	x10, x11, x8
   2d7a4:	mov	w9, #0x18                  	// #24
   2d7a8:	add	x10, x10, #0x8
   2d7ac:	ldur	x25, [x29, #-40]
   2d7b0:	sub	x8, x8, x11
   2d7b4:	mul	x9, x19, x9
   2d7b8:	stp	x10, x28, [x29, #-32]
   2d7bc:	mov	x10, x24
   2d7c0:	sub	x24, x8, #0x8
   2d7c4:	sub	x8, x9, x10, lsl #4
   2d7c8:	sub	x28, x8, #0x10
   2d7cc:	stur	x11, [x29, #-48]
   2d7d0:	ldur	x8, [x29, #-32]
   2d7d4:	add	x2, x22, x28
   2d7d8:	mov	x0, x26
   2d7dc:	mov	x3, x23
   2d7e0:	add	x20, x20, x8
   2d7e4:	mov	x1, x20
   2d7e8:	mov	x4, x25
   2d7ec:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d7f0:	ldur	x3, [x29, #-16]
   2d7f4:	mov	x0, x21
   2d7f8:	mov	x1, x21
   2d7fc:	mov	x2, x26
   2d800:	bl	ca70 <__gmpn_add_n@plt>
   2d804:	ldur	x8, [x29, #-24]
   2d808:	sub	x27, x27, x23
   2d80c:	add	x22, x22, x24
   2d810:	cmp	x27, x8
   2d814:	b.gt	2d7d0 <__gmpn_mulmid@@Base+0x208>
   2d818:	ldur	x9, [x29, #-48]
   2d81c:	lsl	x8, x19, #4
   2d820:	ldur	x28, [x29, #-24]
   2d824:	sub	x8, x8, x9
   2d828:	add	x8, x8, x22
   2d82c:	sub	x25, x8, #0x8
   2d830:	ldur	x19, [x29, #-16]
   2d834:	cbz	x27, 2d930 <__gmpn_mulmid@@Base+0x368>
   2d838:	add	x1, x20, x23, lsl #3
   2d83c:	sub	x3, x25, x27, lsl #3
   2d840:	add	x2, x27, x28
   2d844:	mov	x0, x26
   2d848:	mov	x4, x27
   2d84c:	bl	c700 <__gmpn_mulmid@plt>
   2d850:	mov	x0, x21
   2d854:	mov	x1, x21
   2d858:	mov	x2, x26
   2d85c:	mov	x3, x19
   2d860:	b	2d92c <__gmpn_mulmid@@Base+0x364>
   2d864:	add	x23, x28, #0x3
   2d868:	lsl	x1, x23, #3
   2d86c:	mov	w8, #0x7f00                	// #32512
   2d870:	cmp	x1, x8
   2d874:	stur	xzr, [x29, #-8]
   2d878:	b.hi	2db18 <__gmpn_mulmid@@Base+0x550>  // b.pmore
   2d87c:	add	x9, x1, #0xf
   2d880:	mov	x8, sp
   2d884:	and	x9, x9, #0xfffffffffffffff0
   2d888:	sub	x25, x8, x9
   2d88c:	mov	sp, x25
   2d890:	sub	x26, x19, #0xdc
   2d894:	add	x22, x22, x26, lsl #3
   2d898:	sub	x24, x24, x26
   2d89c:	mov	w4, #0xdc                  	// #220
   2d8a0:	mov	x0, x21
   2d8a4:	mov	x1, x20
   2d8a8:	mov	x2, x24
   2d8ac:	mov	x3, x22
   2d8b0:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d8b4:	cmp	x19, #0x1b8
   2d8b8:	b.lt	2d900 <__gmpn_mulmid@@Base+0x338>  // b.tstop
   2d8bc:	add	x20, x20, #0x6e0
   2d8c0:	sub	x22, x22, #0x6e0
   2d8c4:	mov	w4, #0xdc                  	// #220
   2d8c8:	mov	x0, x25
   2d8cc:	mov	x1, x20
   2d8d0:	mov	x2, x24
   2d8d4:	mov	x3, x22
   2d8d8:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d8dc:	mov	x0, x21
   2d8e0:	mov	x1, x21
   2d8e4:	mov	x2, x25
   2d8e8:	mov	x3, x23
   2d8ec:	bl	ca70 <__gmpn_add_n@plt>
   2d8f0:	sub	x19, x19, #0xdc
   2d8f4:	cmp	x19, #0x1b7
   2d8f8:	b.gt	2d8bc <__gmpn_mulmid@@Base+0x2f4>
   2d8fc:	sub	x26, x19, #0xdc
   2d900:	cbz	x26, 2d930 <__gmpn_mulmid@@Base+0x368>
   2d904:	add	x1, x20, #0x6e0
   2d908:	sub	x3, x22, x26, lsl #3
   2d90c:	add	x2, x26, x28
   2d910:	mov	x0, x25
   2d914:	mov	x4, x26
   2d918:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d91c:	mov	x0, x21
   2d920:	mov	x1, x21
   2d924:	mov	x2, x25
   2d928:	mov	x3, x23
   2d92c:	bl	ca70 <__gmpn_add_n@plt>
   2d930:	ldur	x0, [x29, #-8]
   2d934:	cbz	x0, 2da64 <__gmpn_mulmid@@Base+0x49c>
   2d938:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2d93c:	b	2da64 <__gmpn_mulmid@@Base+0x49c>
   2d940:	mov	x24, x21
   2d944:	cmp	x23, x19
   2d948:	b.lt	2da64 <__gmpn_mulmid@@Base+0x49c>  // b.tstop
   2d94c:	lsl	x8, x25, #3
   2d950:	add	x21, x24, x8
   2d954:	ldp	x25, x26, [x21]
   2d958:	add	x1, x20, x8
   2d95c:	mov	x0, x21
   2d960:	mov	x2, x23
   2d964:	mov	x3, x22
   2d968:	mov	x4, x19
   2d96c:	bl	c460 <__gmpn_mulmid_basecase@plt>
   2d970:	ldp	x8, x9, [x21]
   2d974:	adds	x8, x8, x25
   2d978:	str	x8, [x21]
   2d97c:	cinc	x8, x26, cs  // cs = hs, nlast
   2d980:	adds	x8, x9, x8
   2d984:	str	x8, [x21, #8]
   2d988:	b.cc	2da64 <__gmpn_mulmid@@Base+0x49c>  // b.lo, b.ul, b.last
   2d98c:	mov	w8, #0xdf                  	// #223
   2d990:	sub	x8, x8, x19
   2d994:	add	x8, x24, x8, lsl #3
   2d998:	ldr	x9, [x8]
   2d99c:	adds	x9, x9, #0x1
   2d9a0:	str	x9, [x8], #8
   2d9a4:	b.cs	2d998 <__gmpn_mulmid@@Base+0x3d0>  // b.hs, b.nlast
   2d9a8:	b	2da64 <__gmpn_mulmid@@Base+0x49c>
   2d9ac:	mov	w8, #0x18                  	// #24
   2d9b0:	orr	x9, xzr, #0x200
   2d9b4:	madd	x1, x19, x8, x9
   2d9b8:	mov	w8, #0x7f00                	// #32512
   2d9bc:	cmp	x1, x8
   2d9c0:	stur	xzr, [x29, #-8]
   2d9c4:	b.hi	2db28 <__gmpn_mulmid@@Base+0x560>  // b.pmore
   2d9c8:	add	x9, x1, #0xf
   2d9cc:	mov	x8, sp
   2d9d0:	and	x9, x9, #0xfffffffffffffff0
   2d9d4:	sub	x24, x8, x9
   2d9d8:	mov	sp, x24
   2d9dc:	mov	x0, x21
   2d9e0:	mov	x1, x20
   2d9e4:	mov	x2, x22
   2d9e8:	mov	x3, x19
   2d9ec:	mov	x4, x24
   2d9f0:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2d9f4:	cmp	x26, x19
   2d9f8:	lsl	x27, x19, #3
   2d9fc:	b.ge	2da84 <__gmpn_mulmid@@Base+0x4bc>  // b.tcont
   2da00:	mov	x24, x21
   2da04:	ldur	x0, [x29, #-8]
   2da08:	cbnz	x0, 2db38 <__gmpn_mulmid@@Base+0x570>
   2da0c:	cbz	x26, 2da64 <__gmpn_mulmid@@Base+0x49c>
   2da10:	add	x21, x24, x27
   2da14:	ldp	x25, x26, [x21]
   2da18:	add	x1, x20, x27
   2da1c:	sub	x2, x23, #0x1
   2da20:	mov	x0, x21
   2da24:	mov	x3, x22
   2da28:	mov	x4, x19
   2da2c:	bl	c700 <__gmpn_mulmid@plt>
   2da30:	ldp	x8, x9, [x21]
   2da34:	adds	x8, x8, x25
   2da38:	str	x8, [x21]
   2da3c:	cinc	x8, x26, cs  // cs = hs, nlast
   2da40:	adds	x8, x9, x8
   2da44:	str	x8, [x21, #8]
   2da48:	b.cc	2da64 <__gmpn_mulmid@@Base+0x49c>  // b.lo, b.ul, b.last
   2da4c:	add	x8, x24, x19, lsl #3
   2da50:	add	x8, x8, #0x10
   2da54:	ldr	x9, [x8]
   2da58:	adds	x9, x9, #0x1
   2da5c:	str	x9, [x8], #8
   2da60:	b.cs	2da54 <__gmpn_mulmid@@Base+0x48c>  // b.hs, b.nlast
   2da64:	mov	sp, x29
   2da68:	ldp	x20, x19, [sp, #80]
   2da6c:	ldp	x22, x21, [sp, #64]
   2da70:	ldp	x24, x23, [sp, #48]
   2da74:	ldp	x26, x25, [sp, #32]
   2da78:	ldp	x28, x27, [sp, #16]
   2da7c:	ldp	x29, x30, [sp], #96
   2da80:	ret
   2da84:	add	x8, x27, #0x10
   2da88:	stp	x8, x24, [x29, #-24]
   2da8c:	b	2daa0 <__gmpn_mulmid@@Base+0x4d8>
   2da90:	sub	x26, x23, x19
   2da94:	cmp	x26, x19
   2da98:	mov	x21, x24
   2da9c:	b.lt	2da04 <__gmpn_mulmid@@Base+0x43c>  // b.tstop
   2daa0:	add	x24, x21, x27
   2daa4:	ldur	x4, [x29, #-16]
   2daa8:	ldr	x28, [x21, x27]
   2daac:	ldr	x25, [x24, #8]
   2dab0:	add	x20, x20, x27
   2dab4:	mov	x0, x24
   2dab8:	mov	x1, x20
   2dabc:	mov	x2, x22
   2dac0:	mov	x3, x19
   2dac4:	mov	x23, x26
   2dac8:	bl	c790 <__gmpn_toom42_mulmid@plt>
   2dacc:	ldr	x8, [x21, x27]
   2dad0:	adds	x8, x8, x28
   2dad4:	str	x8, [x21, x27]
   2dad8:	ldr	x8, [x24, #8]
   2dadc:	cinc	x9, x25, cs  // cs = hs, nlast
   2dae0:	adds	x8, x8, x9
   2dae4:	str	x8, [x24, #8]
   2dae8:	b.cc	2da90 <__gmpn_mulmid@@Base+0x4c8>  // b.lo, b.ul, b.last
   2daec:	ldur	x8, [x29, #-24]
   2daf0:	ldr	x9, [x21, x8]
   2daf4:	adds	x9, x9, #0x1
   2daf8:	str	x9, [x21, x8]
   2dafc:	add	x8, x8, #0x8
   2db00:	b.cs	2daf0 <__gmpn_mulmid@@Base+0x528>  // b.hs, b.nlast
   2db04:	b	2da90 <__gmpn_mulmid@@Base+0x4c8>
   2db08:	sub	x0, x29, #0x8
   2db0c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2db10:	mov	x26, x0
   2db14:	b	2d768 <__gmpn_mulmid@@Base+0x1a0>
   2db18:	sub	x0, x29, #0x8
   2db1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2db20:	mov	x25, x0
   2db24:	b	2d890 <__gmpn_mulmid@@Base+0x2c8>
   2db28:	sub	x0, x29, #0x8
   2db2c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2db30:	mov	x24, x0
   2db34:	b	2d9dc <__gmpn_mulmid@@Base+0x414>
   2db38:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2db3c:	cbnz	x26, 2da10 <__gmpn_mulmid@@Base+0x448>
   2db40:	b	2da64 <__gmpn_mulmid@@Base+0x49c>

000000000002db44 <__gmpn_random@@Base>:
   2db44:	stp	x29, x30, [sp, #-48]!
   2db48:	str	x21, [sp, #16]
   2db4c:	stp	x20, x19, [sp, #32]
   2db50:	mov	x29, sp
   2db54:	cbz	x1, 2dbd0 <__gmpn_random@@Base+0x8c>
   2db58:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2db5c:	ldr	x8, [x8, #4040]
   2db60:	mov	x20, x1
   2db64:	mov	x21, x0
   2db68:	ldrb	w9, [x8]
   2db6c:	cbnz	w9, 2db84 <__gmpn_random@@Base+0x40>
   2db70:	mov	w9, #0x1                   	// #1
   2db74:	strb	w9, [x8]
   2db78:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2db7c:	ldr	x0, [x0, #3976]
   2db80:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   2db84:	adrp	x19, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2db88:	ldr	x19, [x19, #3976]
   2db8c:	lsl	x2, x20, #6
   2db90:	mov	x1, x21
   2db94:	ldr	x8, [x19, #24]
   2db98:	mov	x0, x19
   2db9c:	ldr	x8, [x8, #8]
   2dba0:	blr	x8
   2dba4:	add	x20, x21, x20, lsl #3
   2dba8:	ldr	x8, [x20, #-8]!
   2dbac:	cbnz	x8, 2dbd0 <__gmpn_random@@Base+0x8c>
   2dbb0:	ldr	x8, [x19, #24]
   2dbb4:	mov	w2, #0x40                  	// #64
   2dbb8:	mov	x0, x19
   2dbbc:	mov	x1, x20
   2dbc0:	ldr	x8, [x8, #8]
   2dbc4:	blr	x8
   2dbc8:	ldr	x8, [x20]
   2dbcc:	cbz	x8, 2dbb0 <__gmpn_random@@Base+0x6c>
   2dbd0:	ldp	x20, x19, [sp, #32]
   2dbd4:	ldr	x21, [sp, #16]
   2dbd8:	ldp	x29, x30, [sp], #48
   2dbdc:	ret

000000000002dbe0 <__gmpn_random2@@Base>:
   2dbe0:	sub	sp, sp, #0x60
   2dbe4:	stp	x29, x30, [sp, #16]
   2dbe8:	str	x25, [sp, #32]
   2dbec:	stp	x24, x23, [sp, #48]
   2dbf0:	stp	x22, x21, [sp, #64]
   2dbf4:	stp	x20, x19, [sp, #80]
   2dbf8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2dbfc:	ldr	x8, [x8, #4040]
   2dc00:	mov	x21, x1
   2dc04:	mov	x19, x0
   2dc08:	add	x29, sp, #0x10
   2dc0c:	ldrb	w9, [x8]
   2dc10:	cbnz	w9, 2dc28 <__gmpn_random2@@Base+0x48>
   2dc14:	mov	w9, #0x1                   	// #1
   2dc18:	strb	w9, [x8]
   2dc1c:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2dc20:	ldr	x0, [x0, #3976]
   2dc24:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   2dc28:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2dc2c:	ldr	x20, [x20, #3976]
   2dc30:	add	x1, sp, #0x8
   2dc34:	mov	w2, #0x20                  	// #32
   2dc38:	ldr	x8, [x20, #24]
   2dc3c:	mov	x0, x20
   2dc40:	ldr	x8, [x8, #8]
   2dc44:	blr	x8
   2dc48:	ldr	x8, [sp, #8]
   2dc4c:	lsl	x9, x21, #6
   2dc50:	mov	x10, #0xffffffffffffffff    	// #-1
   2dc54:	and	x8, x8, #0x3f
   2dc58:	sub	x21, x9, x8
   2dc5c:	add	x11, x21, #0x3f
   2dc60:	neg	w8, w21
   2dc64:	lsr	x9, x11, #6
   2dc68:	lsr	x10, x10, x8
   2dc6c:	sub	x8, x9, #0x1
   2dc70:	cmp	x11, #0x80
   2dc74:	str	x10, [x19, x8, lsl #3]
   2dc78:	b.cc	2dca0 <__gmpn_random2@@Base+0xc0>  // b.lo, b.ul, b.last
   2dc7c:	cmp	x9, #0x2
   2dc80:	mov	w10, #0x2                   	// #2
   2dc84:	csel	x9, x9, x10, cc  // cc = lo, ul, last
   2dc88:	sub	x9, x9, #0x2
   2dc8c:	sub	x8, x8, x9
   2dc90:	add	x0, x19, x9, lsl #3
   2dc94:	lsl	x2, x8, #3
   2dc98:	mov	w1, #0xff                  	// #255
   2dc9c:	bl	c5f0 <memset@plt>
   2dca0:	ldr	x8, [x20, #24]
   2dca4:	add	x1, x29, #0x18
   2dca8:	mov	w2, #0x20                  	// #32
   2dcac:	mov	x0, x20
   2dcb0:	ldr	x8, [x8, #8]
   2dcb4:	blr	x8
   2dcb8:	ldr	x8, [x29, #24]
   2dcbc:	add	x22, x19, #0x8
   2dcc0:	mov	w24, #0x1                   	// #1
   2dcc4:	and	x8, x8, #0x3
   2dcc8:	add	x8, x8, #0x1
   2dccc:	udiv	x8, x21, x8
   2dcd0:	cmp	w8, #0x0
   2dcd4:	cinc	w23, w8, eq  // eq = none
   2dcd8:	b	2dce4 <__gmpn_random2@@Base+0x104>
   2dcdc:	cmp	x25, x8
   2dce0:	b.ls	2dd94 <__gmpn_random2@@Base+0x1b4>  // b.plast
   2dce4:	ldr	x8, [x20, #24]
   2dce8:	add	x1, x29, #0x18
   2dcec:	mov	w2, #0x20                  	// #32
   2dcf0:	mov	x0, x20
   2dcf4:	ldr	x8, [x8, #8]
   2dcf8:	blr	x8
   2dcfc:	ldr	x8, [x29, #24]
   2dd00:	udiv	x9, x8, x23
   2dd04:	msub	x8, x9, x23, x8
   2dd08:	add	x8, x8, #0x1
   2dd0c:	subs	x8, x21, x8
   2dd10:	csel	x25, xzr, x8, cc  // cc = lo, ul, last
   2dd14:	b.ls	2dd94 <__gmpn_random2@@Base+0x1b4>  // b.plast
   2dd18:	lsr	x8, x25, #3
   2dd1c:	and	x8, x8, #0x1ffffffffffffff8
   2dd20:	ldr	x9, [x19, x8]
   2dd24:	lsl	x10, x24, x25
   2dd28:	add	x1, x29, #0x18
   2dd2c:	mov	w2, #0x20                  	// #32
   2dd30:	eor	x9, x9, x10
   2dd34:	str	x9, [x19, x8]
   2dd38:	ldr	x8, [x20, #24]
   2dd3c:	mov	x0, x20
   2dd40:	ldr	x8, [x8, #8]
   2dd44:	blr	x8
   2dd48:	ldr	x8, [x29, #24]
   2dd4c:	udiv	x9, x8, x23
   2dd50:	msub	x8, x9, x23, x8
   2dd54:	add	x8, x8, #0x1
   2dd58:	subs	x9, x25, x8
   2dd5c:	csel	x21, xzr, x9, cc  // cc = lo, ul, last
   2dd60:	lsr	x9, x21, #6
   2dd64:	lsl	x10, x9, #3
   2dd68:	ldr	x11, [x19, x10]
   2dd6c:	lsl	x12, x24, x21
   2dd70:	adds	x11, x11, x12
   2dd74:	str	x11, [x19, x10]
   2dd78:	b.cc	2dcdc <__gmpn_random2@@Base+0xfc>  // b.lo, b.ul, b.last
   2dd7c:	add	x9, x22, x9, lsl #3
   2dd80:	ldr	x10, [x9]
   2dd84:	adds	x10, x10, #0x1
   2dd88:	str	x10, [x9], #8
   2dd8c:	b.cs	2dd80 <__gmpn_random2@@Base+0x1a0>  // b.hs, b.nlast
   2dd90:	b	2dcdc <__gmpn_random2@@Base+0xfc>
   2dd94:	ldp	x20, x19, [sp, #80]
   2dd98:	ldp	x22, x21, [sp, #64]
   2dd9c:	ldp	x24, x23, [sp, #48]
   2dda0:	ldr	x25, [sp, #32]
   2dda4:	ldp	x29, x30, [sp, #16]
   2dda8:	add	sp, sp, #0x60
   2ddac:	ret

000000000002ddb0 <__gmpn_pow_1@@Base>:
   2ddb0:	stp	x29, x30, [sp, #-80]!
   2ddb4:	stp	x20, x19, [sp, #64]
   2ddb8:	mov	x19, x2
   2ddbc:	mov	x20, x1
   2ddc0:	cmp	x3, #0x2
   2ddc4:	stp	x26, x25, [sp, #16]
   2ddc8:	stp	x24, x23, [sp, #32]
   2ddcc:	stp	x22, x21, [sp, #48]
   2ddd0:	mov	x29, sp
   2ddd4:	b.cs	2ddec <__gmpn_pow_1@@Base+0x3c>  // b.hs, b.nlast
   2ddd8:	cbz	x3, 2df70 <__gmpn_pow_1@@Base+0x1c0>
   2dddc:	mov	x1, x20
   2dde0:	mov	x2, x19
   2dde4:	bl	ca50 <__gmpn_copyi@plt>
   2dde8:	b	2df78 <__gmpn_pow_1@@Base+0x1c8>
   2ddec:	mov	x9, xzr
   2ddf0:	mov	w8, #0x40                  	// #64
   2ddf4:	mov	x10, x3
   2ddf8:	sxtw	x9, w9
   2ddfc:	eor	x9, x9, x10
   2de00:	lsr	x10, x10, #1
   2de04:	sub	w8, w8, #0x1
   2de08:	cbnz	x10, 2ddf8 <__gmpn_pow_1@@Base+0x48>
   2de0c:	lsl	x25, x3, x8
   2de10:	cmp	x19, #0x1
   2de14:	sub	w26, w8, #0x3e
   2de18:	b.ne	2deac <__gmpn_pow_1@@Base+0xfc>  // b.any
   2de1c:	ldr	x20, [x20]
   2de20:	tst	w8, #0x1
   2de24:	csel	x21, x0, x4, eq  // eq = none
   2de28:	umulh	x9, x20, x20
   2de2c:	mul	x10, x20, x20
   2de30:	csel	x8, x4, x0, eq  // eq = none
   2de34:	stp	x10, x9, [x21]
   2de38:	cmp	x9, #0x0
   2de3c:	mov	w9, #0x1                   	// #1
   2de40:	cinc	x19, x9, ne  // ne = any
   2de44:	lsl	x25, x25, #1
   2de48:	mov	x22, x8
   2de4c:	tbz	x25, #63, 2de70 <__gmpn_pow_1@@Base+0xc0>
   2de50:	mov	x0, x21
   2de54:	mov	x1, x21
   2de58:	mov	x2, x19
   2de5c:	mov	x3, x20
   2de60:	bl	d490 <__gmpn_mul_1@plt>
   2de64:	cmp	x0, #0x0
   2de68:	str	x0, [x21, x19, lsl #3]
   2de6c:	cinc	x19, x19, ne  // ne = any
   2de70:	cbz	w26, 2df78 <__gmpn_pow_1@@Base+0x1c8>
   2de74:	mov	x0, x22
   2de78:	mov	x1, x21
   2de7c:	mov	x2, x19
   2de80:	bl	c8e0 <__gmpn_sqr@plt>
   2de84:	add	x8, x22, x19, lsl #4
   2de88:	ldur	x8, [x8, #-8]
   2de8c:	lsl	x9, x19, #1
   2de90:	add	w26, w26, #0x1
   2de94:	cmp	x8, #0x0
   2de98:	cset	w8, eq  // eq = none
   2de9c:	sub	x19, x9, x8
   2dea0:	mov	x8, x21
   2dea4:	mov	x21, x22
   2dea8:	b	2de44 <__gmpn_pow_1@@Base+0x94>
   2deac:	eor	w8, w8, w9
   2deb0:	tst	w8, #0x1
   2deb4:	csel	x23, x4, x0, eq  // eq = none
   2deb8:	csel	x21, x0, x4, eq  // eq = none
   2debc:	mov	x0, x23
   2dec0:	mov	x1, x20
   2dec4:	mov	x2, x19
   2dec8:	bl	c8e0 <__gmpn_sqr@plt>
   2decc:	add	x8, x23, x19, lsl #4
   2ded0:	ldur	x8, [x8, #-8]
   2ded4:	lsl	x9, x19, #1
   2ded8:	cmp	x8, #0x0
   2dedc:	cset	w8, eq  // eq = none
   2dee0:	sub	x22, x9, x8
   2dee4:	lsl	x25, x25, #1
   2dee8:	tbnz	x25, #63, 2defc <__gmpn_pow_1@@Base+0x14c>
   2deec:	mov	x24, x21
   2def0:	mov	x21, x23
   2def4:	cbnz	w26, 2df2c <__gmpn_pow_1@@Base+0x17c>
   2def8:	b	2df68 <__gmpn_pow_1@@Base+0x1b8>
   2defc:	mov	x0, x21
   2df00:	mov	x1, x23
   2df04:	mov	x2, x22
   2df08:	mov	x3, x20
   2df0c:	mov	x4, x19
   2df10:	add	x24, x22, x19
   2df14:	bl	ccd0 <__gmpn_mul@plt>
   2df18:	cmp	x0, #0x0
   2df1c:	cset	w8, eq  // eq = none
   2df20:	sub	x22, x24, x8
   2df24:	mov	x24, x23
   2df28:	cbz	w26, 2df68 <__gmpn_pow_1@@Base+0x1b8>
   2df2c:	mov	x0, x24
   2df30:	mov	x1, x21
   2df34:	mov	x2, x22
   2df38:	bl	c8e0 <__gmpn_sqr@plt>
   2df3c:	add	x8, x24, x22, lsl #4
   2df40:	ldur	x8, [x8, #-8]
   2df44:	lsl	x9, x22, #1
   2df48:	add	w26, w26, #0x1
   2df4c:	mov	x23, x24
   2df50:	cmp	x8, #0x0
   2df54:	cset	w8, eq  // eq = none
   2df58:	sub	x22, x9, x8
   2df5c:	lsl	x25, x25, #1
   2df60:	tbz	x25, #63, 2deec <__gmpn_pow_1@@Base+0x13c>
   2df64:	b	2defc <__gmpn_pow_1@@Base+0x14c>
   2df68:	mov	x19, x22
   2df6c:	b	2df78 <__gmpn_pow_1@@Base+0x1c8>
   2df70:	mov	w19, #0x1                   	// #1
   2df74:	str	x19, [x0]
   2df78:	mov	x0, x19
   2df7c:	ldp	x20, x19, [sp, #64]
   2df80:	ldp	x22, x21, [sp, #48]
   2df84:	ldp	x24, x23, [sp, #32]
   2df88:	ldp	x26, x25, [sp, #16]
   2df8c:	ldp	x29, x30, [sp], #80
   2df90:	ret

000000000002df94 <__gmpn_rootrem@@Base>:
   2df94:	stp	x29, x30, [sp, #-64]!
   2df98:	stp	x24, x23, [sp, #16]
   2df9c:	stp	x22, x21, [sp, #32]
   2dfa0:	stp	x20, x19, [sp, #48]
   2dfa4:	mov	x29, sp
   2dfa8:	sub	sp, sp, #0x10
   2dfac:	cmp	x4, #0x2
   2dfb0:	mov	x20, x0
   2dfb4:	b.eq	2e0b8 <__gmpn_rootrem@@Base+0x124>  // b.none
   2dfb8:	mov	x19, x4
   2dfbc:	cbnz	x1, 2e094 <__gmpn_rootrem@@Base+0x100>
   2dfc0:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   2dfc4:	add	x8, x3, #0x2
   2dfc8:	movk	x9, #0x5556
   2dfcc:	smulh	x8, x8, x9
   2dfd0:	add	x8, x8, x8, lsr #63
   2dfd4:	cmp	x8, x19
   2dfd8:	b.ls	2e094 <__gmpn_rootrem@@Base+0x100>  // b.plast
   2dfdc:	sub	x8, x3, #0x1
   2dfe0:	add	x21, x19, x3
   2dfe4:	udiv	x24, x8, x19
   2dfe8:	add	x8, x21, x24
   2dfec:	lsl	x8, x8, #3
   2dff0:	add	x1, x8, #0x10
   2dff4:	mov	w8, #0x7f00                	// #32512
   2dff8:	cmp	x1, x8
   2dffc:	stur	xzr, [x29, #-8]
   2e000:	b.hi	2e0d4 <__gmpn_rootrem@@Base+0x140>  // b.pmore
   2e004:	add	x9, x1, #0xf
   2e008:	mov	x8, sp
   2e00c:	and	x9, x9, #0xfffffffffffffff0
   2e010:	sub	x22, x8, x9
   2e014:	mov	sp, x22
   2e018:	lsl	x23, x19, #3
   2e01c:	add	x0, x22, x23
   2e020:	mov	x1, x2
   2e024:	mov	x2, x3
   2e028:	bl	ca50 <__gmpn_copyi@plt>
   2e02c:	mov	x0, x22
   2e030:	mov	w1, wzr
   2e034:	mov	x2, x23
   2e038:	bl	c5f0 <memset@plt>
   2e03c:	add	x23, x22, x21, lsl #3
   2e040:	mov	w5, #0x1                   	// #1
   2e044:	mov	x0, x23
   2e048:	mov	x1, xzr
   2e04c:	mov	x2, x22
   2e050:	mov	x3, x21
   2e054:	mov	x4, x19
   2e058:	bl	2e0fc <__gmpn_rootrem@@Base+0x168>
   2e05c:	mov	x19, x0
   2e060:	add	x1, x23, #0x8
   2e064:	add	x2, x24, #0x1
   2e068:	mov	x0, x20
   2e06c:	bl	ca50 <__gmpn_copyi@plt>
   2e070:	ldur	x0, [x29, #-8]
   2e074:	cbnz	x0, 2e0f4 <__gmpn_rootrem@@Base+0x160>
   2e078:	mov	x0, x19
   2e07c:	mov	sp, x29
   2e080:	ldp	x20, x19, [sp, #48]
   2e084:	ldp	x22, x21, [sp, #32]
   2e088:	ldp	x24, x23, [sp, #16]
   2e08c:	ldp	x29, x30, [sp], #64
   2e090:	ret
   2e094:	mov	x0, x20
   2e098:	mov	x4, x19
   2e09c:	mov	w5, wzr
   2e0a0:	mov	sp, x29
   2e0a4:	ldp	x20, x19, [sp, #48]
   2e0a8:	ldp	x22, x21, [sp, #32]
   2e0ac:	ldp	x24, x23, [sp, #16]
   2e0b0:	ldp	x29, x30, [sp], #64
   2e0b4:	b	2e0fc <__gmpn_rootrem@@Base+0x168>
   2e0b8:	mov	x0, x20
   2e0bc:	mov	sp, x29
   2e0c0:	ldp	x20, x19, [sp, #48]
   2e0c4:	ldp	x22, x21, [sp, #32]
   2e0c8:	ldp	x24, x23, [sp, #16]
   2e0cc:	ldp	x29, x30, [sp], #64
   2e0d0:	b	d3b0 <__gmpn_sqrtrem@plt>
   2e0d4:	sub	x0, x29, #0x8
   2e0d8:	mov	x22, x3
   2e0dc:	mov	x23, x2
   2e0e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2e0e4:	mov	x2, x23
   2e0e8:	mov	x3, x22
   2e0ec:	mov	x22, x0
   2e0f0:	b	2e018 <__gmpn_rootrem@@Base+0x84>
   2e0f4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2e0f8:	b	2e078 <__gmpn_rootrem@@Base+0xe4>
   2e0fc:	stp	x29, x30, [sp, #-96]!
   2e100:	stp	x28, x27, [sp, #16]
   2e104:	stp	x26, x25, [sp, #32]
   2e108:	stp	x24, x23, [sp, #48]
   2e10c:	stp	x22, x21, [sp, #64]
   2e110:	stp	x20, x19, [sp, #80]
   2e114:	mov	x29, sp
   2e118:	sub	sp, sp, #0x2d0
   2e11c:	sub	x8, x3, #0x1
   2e120:	ldr	x11, [x2, x8, lsl #3]
   2e124:	lsl	x9, x3, #6
   2e128:	mov	x24, x3
   2e12c:	mov	x27, x2
   2e130:	clz	x10, x11
   2e134:	add	w12, w10, #0x1
   2e138:	sub	x9, x9, x12
   2e13c:	mov	x16, x1
   2e140:	cmp	x9, x4
   2e144:	mov	x19, sp
   2e148:	b.cs	2e1cc <__gmpn_rootrem@@Base+0x238>  // b.hs, b.nlast
   2e14c:	mov	w9, #0x1                   	// #1
   2e150:	str	x9, [x0]
   2e154:	ldr	x9, [x27]
   2e158:	cbz	x16, 2e8b4 <__gmpn_rootrem@@Base+0x920>
   2e15c:	sub	x10, x9, #0x1
   2e160:	str	x10, [x16]
   2e164:	cbz	x9, 2e97c <__gmpn_rootrem@@Base+0x9e8>
   2e168:	cmp	x27, x16
   2e16c:	b.eq	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.none
   2e170:	cmp	x24, #0x2
   2e174:	b.lt	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.tstop
   2e178:	cmp	x8, #0x4
   2e17c:	b.cc	2e1a4 <__gmpn_rootrem@@Base+0x210>  // b.lo, b.ul, b.last
   2e180:	lsl	x9, x24, #3
   2e184:	add	x10, x16, #0x8
   2e188:	add	x11, x27, x9
   2e18c:	cmp	x10, x11
   2e190:	b.cs	2ea60 <__gmpn_rootrem@@Base+0xacc>  // b.hs, b.nlast
   2e194:	add	x9, x16, x9
   2e198:	add	x10, x27, #0x8
   2e19c:	cmp	x10, x9
   2e1a0:	b.cs	2ea60 <__gmpn_rootrem@@Base+0xacc>  // b.hs, b.nlast
   2e1a4:	mov	w9, #0x1                   	// #1
   2e1a8:	lsl	x11, x9, #3
   2e1ac:	sub	x10, x24, x9
   2e1b0:	add	x9, x16, x11
   2e1b4:	add	x11, x27, x11
   2e1b8:	ldr	x12, [x11], #8
   2e1bc:	subs	x10, x10, #0x1
   2e1c0:	str	x12, [x9], #8
   2e1c4:	b.ne	2e1b8 <__gmpn_rootrem@@Base+0x224>  // b.any
   2e1c8:	b	2ea94 <__gmpn_rootrem@@Base+0xb00>
   2e1cc:	mov	x28, x4
   2e1d0:	cmp	w12, #0x40
   2e1d4:	b.ne	2e1e4 <__gmpn_rootrem@@Base+0x250>  // b.any
   2e1d8:	add	x8, x27, x24, lsl #3
   2e1dc:	ldur	x8, [x8, #-16]
   2e1e0:	b	2e208 <__gmpn_rootrem@@Base+0x274>
   2e1e4:	cmp	x24, #0x1
   2e1e8:	cset	w13, ne  // ne = any
   2e1ec:	sub	x8, x8, x13
   2e1f0:	ldr	x8, [x27, x8, lsl #3]
   2e1f4:	lsl	x11, x11, x12
   2e1f8:	mov	w12, #0x3f                  	// #63
   2e1fc:	sub	w10, w12, w10
   2e200:	lsr	x8, x8, x10
   2e204:	orr	x8, x8, x11
   2e208:	lsr	x8, x8, #56
   2e20c:	lsr	x10, x9, #56
   2e210:	cbnz	x10, 2ebe0 <__gmpn_rootrem@@Base+0xc4c>
   2e214:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   2e218:	add	x10, x10, #0x5f4
   2e21c:	ldrb	w8, [x10, x8]
   2e220:	bfi	x8, x9, #8, #56
   2e224:	udiv	x9, x8, x28
   2e228:	lsr	x8, x9, #8
   2e22c:	and	x9, x9, #0xff
   2e230:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   2e234:	add	x10, x10, #0x6f4
   2e238:	ldrb	w9, [x10, x9]
   2e23c:	and	x20, x8, #0xffffffff
   2e240:	sub	x10, x28, #0x1
   2e244:	cmp	x20, #0x8
   2e248:	orr	x8, x9, #0x100
   2e24c:	str	x8, [x0]
   2e250:	str	x20, [x19, #192]
   2e254:	stp	x10, x0, [x19, #152]
   2e258:	str	w5, [x19, #20]
   2e25c:	b.ls	2e2d4 <__gmpn_rootrem@@Base+0x340>  // b.plast
   2e260:	lsr	x9, x10, #2
   2e264:	mov	w10, #0x43                  	// #67
   2e268:	add	x11, x19, #0xc0
   2e26c:	clz	x9, x9
   2e270:	mov	x22, xzr
   2e274:	sub	x9, x10, x9
   2e278:	add	x10, x11, #0x8
   2e27c:	mov	x25, x20
   2e280:	add	x11, x25, x9
   2e284:	sub	x12, x25, #0x1
   2e288:	cmp	x25, x9
   2e28c:	lsr	x11, x11, #1
   2e290:	csel	x25, x11, x12, hi  // hi = pmore
   2e294:	str	x25, [x10, x22, lsl #3]
   2e298:	cmp	x25, #0x8
   2e29c:	add	x22, x22, #0x1
   2e2a0:	b.hi	2e280 <__gmpn_rootrem@@Base+0x2ec>  // b.pmore
   2e2a4:	mov	w9, #0x8                   	// #8
   2e2a8:	sub	x9, x9, x25
   2e2ac:	lsr	x8, x8, x9
   2e2b0:	cmp	w22, #0x40
   2e2b4:	str	x8, [x0]
   2e2b8:	b.ls	2e2ec <__gmpn_rootrem@@Base+0x358>  // b.plast
   2e2bc:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   2e2c0:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   2e2c4:	add	x0, x0, #0x5d8
   2e2c8:	add	x2, x2, #0x5e2
   2e2cc:	mov	w1, #0x11b                 	// #283
   2e2d0:	bl	c6c0 <__gmp_assert_fail@plt>
   2e2d4:	mov	w9, #0x8                   	// #8
   2e2d8:	sub	x9, x9, x20
   2e2dc:	lsr	x8, x8, x9
   2e2e0:	mov	w22, wzr
   2e2e4:	str	x8, [x0]
   2e2e8:	mov	x25, x20
   2e2ec:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   2e2f0:	ldr	d0, [x8, #1488]
   2e2f4:	ucvtf	d1, x28
   2e2f8:	mov	x8, #0x3f90000000000000    	// #4580160821035794432
   2e2fc:	add	x21, x24, #0x1
   2e300:	fmul	d0, d1, d0
   2e304:	fmov	d1, x8
   2e308:	fmul	d0, d0, d1
   2e30c:	fcvtzs	x8, d0
   2e310:	add	x8, x24, x8
   2e314:	add	x23, x8, #0x2
   2e318:	add	x8, x21, x23, lsl #1
   2e31c:	lsl	x1, x8, #3
   2e320:	mov	w8, #0x7f00                	// #32512
   2e324:	cmp	x1, x8
   2e328:	str	xzr, [x19, #184]
   2e32c:	b.hi	2ec00 <__gmpn_rootrem@@Base+0xc6c>  // b.pmore
   2e330:	add	x9, x1, #0xf
   2e334:	mov	x8, sp
   2e338:	and	x9, x9, #0xfffffffffffffff0
   2e33c:	sub	x0, x8, x9
   2e340:	mov	sp, x0
   2e344:	add	x8, x0, x21, lsl #3
   2e348:	cmp	x16, #0x0
   2e34c:	str	x8, [x19, #168]
   2e350:	add	x8, x8, x23, lsl #3
   2e354:	csel	x21, x0, x16, eq  // eq = none
   2e358:	str	x27, [x19, #120]
   2e35c:	str	x8, [x19, #176]
   2e360:	str	x16, [x19, #8]
   2e364:	str	x24, [x19, #72]
   2e368:	str	x28, [x19, #48]
   2e36c:	str	x0, [x19, #32]
   2e370:	cbz	w22, 2e8bc <__gmpn_rootrem@@Base+0x928>
   2e374:	ldr	x23, [x19, #160]
   2e378:	mul	x8, x20, x28
   2e37c:	add	x9, x0, x24, lsl #4
   2e380:	mov	w10, w22
   2e384:	sub	x28, x21, #0x8
   2e388:	str	x9, [x19, #40]
   2e38c:	add	x9, x21, #0x8
   2e390:	sub	x26, x8, x25
   2e394:	mov	w24, #0x1                   	// #1
   2e398:	str	x9, [x19, #24]
   2e39c:	str	x21, [x19, #88]
   2e3a0:	b	2e3d0 <__gmpn_rootrem@@Base+0x43c>
   2e3a4:	ldr	x8, [x19, #80]
   2e3a8:	mov	x9, #0xffffffffffffffff    	// #-1
   2e3ac:	mvn	w8, w8
   2e3b0:	lsr	x8, x9, x8
   2e3b4:	str	x8, [x20, x27, lsl #3]
   2e3b8:	ldr	x8, [x22]
   2e3bc:	ldp	x9, x10, [x19, #96]
   2e3c0:	ldr	x23, [x19, #160]
   2e3c4:	orr	x8, x8, x9
   2e3c8:	str	x8, [x22]
   2e3cc:	cbz	w10, 2e8c4 <__gmpn_rootrem@@Base+0x930>
   2e3d0:	ldr	x22, [x19, #152]
   2e3d4:	ldr	x8, [x19, #72]
   2e3d8:	mov	x0, x21
   2e3dc:	str	x10, [x19, #128]
   2e3e0:	msub	x26, x25, x22, x26
   2e3e4:	lsr	x25, x26, #6
   2e3e8:	sub	x20, x8, x25
   2e3ec:	ldr	x8, [x19, #120]
   2e3f0:	ands	x3, x26, #0x3f
   2e3f4:	mov	x2, x20
   2e3f8:	add	x1, x8, x25, lsl #3
   2e3fc:	b.eq	2e408 <__gmpn_rootrem@@Base+0x474>  // b.none
   2e400:	bl	c1a0 <__gmpn_rshift@plt>
   2e404:	b	2e40c <__gmpn_rootrem@@Base+0x478>
   2e408:	bl	ca50 <__gmpn_copyi@plt>
   2e40c:	add	x8, x21, x20, lsl #3
   2e410:	ldur	x8, [x8, #-8]
   2e414:	cmp	x8, #0x0
   2e418:	cset	w8, eq  // eq = none
   2e41c:	csetm	x9, eq  // eq = none
   2e420:	sub	x20, x20, x8
   2e424:	str	x9, [x19, #112]
   2e428:	lsl	x8, x9, #3
   2e42c:	ldr	x9, [x19, #40]
   2e430:	sub	x8, x8, x25, lsl #3
   2e434:	add	x8, x9, x8
   2e438:	stp	x25, x8, [x19, #136]
   2e43c:	ldp	x27, x21, [x19, #168]
   2e440:	mov	x1, x23
   2e444:	mov	x2, x24
   2e448:	mov	x3, x22
   2e44c:	mov	x0, x21
   2e450:	mov	x4, x27
   2e454:	mov	x25, x24
   2e458:	bl	d210 <__gmpn_pow_1@plt>
   2e45c:	mov	x22, x0
   2e460:	mov	x0, x27
   2e464:	mov	x1, x21
   2e468:	mov	x2, x22
   2e46c:	mov	x3, x23
   2e470:	mov	x4, x24
   2e474:	bl	ccd0 <__gmpn_mul@plt>
   2e478:	add	x8, x22, x24
   2e47c:	add	x9, x27, x8, lsl #3
   2e480:	ldur	x9, [x9, #-8]
   2e484:	mov	x10, x23
   2e488:	cmp	x9, #0x0
   2e48c:	cset	w9, eq  // eq = none
   2e490:	sub	x23, x8, x9
   2e494:	cmp	x23, x20
   2e498:	b.gt	2e4d0 <__gmpn_rootrem@@Base+0x53c>
   2e49c:	b.ne	2e87c <__gmpn_rootrem@@Base+0x8e8>  // b.any
   2e4a0:	ldr	x8, [x19, #144]
   2e4a4:	ldr	x21, [x19, #88]
   2e4a8:	mov	x9, x20
   2e4ac:	subs	x10, x9, #0x1
   2e4b0:	b.lt	2e4f0 <__gmpn_rootrem@@Base+0x55c>  // b.tstop
   2e4b4:	ldr	x11, [x8], #-8
   2e4b8:	ldr	x9, [x28, x9, lsl #3]
   2e4bc:	cmp	x11, x9
   2e4c0:	mov	x9, x10
   2e4c4:	b.eq	2e4ac <__gmpn_rootrem@@Base+0x518>  // b.none
   2e4c8:	ldr	x10, [x19, #160]
   2e4cc:	b.ls	2e884 <__gmpn_rootrem@@Base+0x8f0>  // b.plast
   2e4d0:	ldr	x22, [x19, #152]
   2e4d4:	mov	x8, x10
   2e4d8:	mov	x23, x10
   2e4dc:	ldr	x9, [x8]
   2e4e0:	sub	x10, x9, #0x1
   2e4e4:	str	x10, [x8], #8
   2e4e8:	cbz	x9, 2e4dc <__gmpn_rootrem@@Base+0x548>
   2e4ec:	b	2e43c <__gmpn_rootrem@@Base+0x4a8>
   2e4f0:	mov	w8, #0x1                   	// #1
   2e4f4:	mov	x23, x20
   2e4f8:	ldr	x9, [x19, #128]
   2e4fc:	add	x10, x19, #0xc0
   2e500:	sub	x11, x26, #0x1
   2e504:	sub	x12, x9, #0x1
   2e508:	ldr	x9, [x10, x9, lsl #3]
   2e50c:	ldr	x10, [x10, x12, lsl #3]
   2e510:	str	x12, [x19, #104]
   2e514:	sub	x25, x10, x9
   2e518:	sub	x26, x26, x25
   2e51c:	lsr	x9, x11, #6
   2e520:	lsr	x11, x26, #6
   2e524:	sub	x9, x9, x11
   2e528:	lsr	x27, x25, #6
   2e52c:	add	x10, x9, #0x1
   2e530:	str	x26, [x19, #128]
   2e534:	tbz	w8, #0, 2e548 <__gmpn_rootrem@@Base+0x5b4>
   2e538:	str	xzr, [x19, #144]
   2e53c:	str	xzr, [x19, #64]
   2e540:	str	x27, [x19, #112]
   2e544:	b	2e678 <__gmpn_rootrem@@Base+0x6e4>
   2e548:	str	x11, [x19, #80]
   2e54c:	str	x10, [x19, #96]
   2e550:	cbz	x23, 2e58c <__gmpn_rootrem@@Base+0x5f8>
   2e554:	ldr	x2, [x19, #168]
   2e558:	mov	x0, x21
   2e55c:	mov	x1, x21
   2e560:	mov	x3, x23
   2e564:	bl	c2d0 <__gmpn_sub_n@plt>
   2e568:	cbz	x0, 2e58c <__gmpn_rootrem@@Base+0x5f8>
   2e56c:	cmp	x23, x20
   2e570:	b.ge	2e58c <__gmpn_rootrem@@Base+0x5f8>  // b.tcont
   2e574:	lsl	x8, x23, #3
   2e578:	ldr	x9, [x21, x8]
   2e57c:	add	x23, x23, #0x1
   2e580:	sub	x10, x9, #0x1
   2e584:	str	x10, [x21, x8]
   2e588:	cbz	x9, 2e56c <__gmpn_rootrem@@Base+0x5d8>
   2e58c:	ldr	x26, [x19, #72]
   2e590:	ldr	x23, [x19, #112]
   2e594:	ldr	x9, [x19, #136]
   2e598:	mov	x20, xzr
   2e59c:	str	x24, [x19, #144]
   2e5a0:	add	x8, x26, x23
   2e5a4:	sub	x8, x8, x9
   2e5a8:	add	x8, x28, x8, lsl #3
   2e5ac:	ldr	x9, [x8, x20, lsl #3]
   2e5b0:	sub	x20, x20, #0x1
   2e5b4:	cbz	x9, 2e5ac <__gmpn_rootrem@@Base+0x618>
   2e5b8:	ldr	x24, [x19, #136]
   2e5bc:	add	x8, x26, x23
   2e5c0:	and	x3, x25, #0x3f
   2e5c4:	add	x0, x21, x27, lsl #3
   2e5c8:	sub	x8, x8, x24
   2e5cc:	add	x8, x8, x20
   2e5d0:	add	x2, x8, #0x1
   2e5d4:	mov	x1, x21
   2e5d8:	str	x0, [x19, #56]
   2e5dc:	cbz	x3, 2e60c <__gmpn_rootrem@@Base+0x678>
   2e5e0:	bl	c180 <__gmpn_lshift@plt>
   2e5e4:	add	x8, x26, x27
   2e5e8:	add	x8, x8, x23
   2e5ec:	sub	x9, x8, x24
   2e5f0:	add	x8, x9, x20
   2e5f4:	cbz	x0, 2e63c <__gmpn_rootrem@@Base+0x6a8>
   2e5f8:	ldr	x10, [x19, #24]
   2e5fc:	add	x9, x10, x9, lsl #3
   2e600:	str	x0, [x9, x20, lsl #3]
   2e604:	add	x9, x8, #0x2
   2e608:	b	2e640 <__gmpn_rootrem@@Base+0x6ac>
   2e60c:	bl	c000 <__gmpn_copyd@plt>
   2e610:	add	x8, x26, x27
   2e614:	add	x8, x8, x23
   2e618:	ldr	x12, [x19, #56]
   2e61c:	sub	x8, x8, x24
   2e620:	ldr	x26, [x19, #128]
   2e624:	ldr	x24, [x19, #144]
   2e628:	ldr	x10, [x19, #96]
   2e62c:	ldr	x11, [x19, #80]
   2e630:	add	x8, x8, x20
   2e634:	add	x9, x8, #0x1
   2e638:	b	2e654 <__gmpn_rootrem@@Base+0x6c0>
   2e63c:	add	x9, x8, #0x1
   2e640:	ldr	x26, [x19, #128]
   2e644:	ldr	x24, [x19, #144]
   2e648:	ldr	x10, [x19, #96]
   2e64c:	ldr	x11, [x19, #80]
   2e650:	ldr	x12, [x19, #56]
   2e654:	ldr	x12, [x12]
   2e658:	sub	x8, x10, #0x1
   2e65c:	cmp	x8, x27
   2e660:	str	x9, [x19, #112]
   2e664:	str	x12, [x19, #144]
   2e668:	b.le	2e678 <__gmpn_rootrem@@Base+0x6e4>
   2e66c:	add	x8, x21, x27, lsl #3
   2e670:	ldr	x8, [x8, #8]
   2e674:	str	x8, [x19, #64]
   2e678:	ldr	x8, [x19, #120]
   2e67c:	ldr	x23, [x19, #160]
   2e680:	ands	x3, x26, #0x3f
   2e684:	mov	x0, x21
   2e688:	add	x1, x8, x11, lsl #3
   2e68c:	mov	x20, x10
   2e690:	mov	x2, x10
   2e694:	b.eq	2e6a0 <__gmpn_rootrem@@Base+0x70c>  // b.none
   2e698:	bl	c1a0 <__gmpn_rshift@plt>
   2e69c:	b	2e6a4 <__gmpn_rootrem@@Base+0x710>
   2e6a0:	bl	ca50 <__gmpn_copyi@plt>
   2e6a4:	and	x8, x25, #0x3f
   2e6a8:	str	x8, [x19, #136]
   2e6ac:	lsl	x8, x27, #3
   2e6b0:	ldr	x9, [x21, x8]
   2e6b4:	mov	w10, #0x1                   	// #1
   2e6b8:	lsl	x11, x10, x25
   2e6bc:	str	x11, [x19, #56]
   2e6c0:	sub	x11, x11, #0x1
   2e6c4:	and	x9, x9, x11
   2e6c8:	ldr	x11, [x19, #144]
   2e6cc:	sub	x10, x20, #0x1
   2e6d0:	cmp	x10, x27
   2e6d4:	orr	x9, x9, x11
   2e6d8:	str	x9, [x21, x8]
   2e6dc:	b.le	2e6ec <__gmpn_rootrem@@Base+0x758>
   2e6e0:	ldr	x9, [x19, #64]
   2e6e4:	add	x8, x21, x27, lsl #3
   2e6e8:	str	x9, [x8, #8]
   2e6ec:	ldr	x20, [x19, #176]
   2e6f0:	ldr	x3, [x19, #48]
   2e6f4:	mov	x2, x22
   2e6f8:	mov	x0, x20
   2e6fc:	mov	x1, x20
   2e700:	bl	d490 <__gmpn_mul_1@plt>
   2e704:	ldr	x3, [x19, #136]
   2e708:	cmp	x0, #0x0
   2e70c:	str	x0, [x20, x22, lsl #3]
   2e710:	cinc	x20, x22, ne  // ne = any
   2e714:	add	x22, x23, x27, lsl #3
   2e718:	cbz	x3, 2e740 <__gmpn_rootrem@@Base+0x7ac>
   2e71c:	mov	x0, x22
   2e720:	mov	x1, x23
   2e724:	mov	x2, x24
   2e728:	bl	c180 <__gmpn_lshift@plt>
   2e72c:	add	x24, x27, x24
   2e730:	cbz	x0, 2e75c <__gmpn_rootrem@@Base+0x7c8>
   2e734:	str	x0, [x23, x24, lsl #3]
   2e738:	add	x24, x24, #0x1
   2e73c:	b	2e75c <__gmpn_rootrem@@Base+0x7c8>
   2e740:	mov	x0, x22
   2e744:	mov	x1, x23
   2e748:	mov	x2, x24
   2e74c:	str	x25, [x19, #96]
   2e750:	bl	c000 <__gmpn_copyd@plt>
   2e754:	ldr	x25, [x19, #96]
   2e758:	add	x24, x27, x24
   2e75c:	str	x24, [x19, #144]
   2e760:	ldr	x8, [x22]
   2e764:	ldr	x2, [x19, #112]
   2e768:	str	x8, [x19, #96]
   2e76c:	sub	x8, x25, #0x1
   2e770:	subs	x23, x2, x20
   2e774:	lsr	x27, x8, #6
   2e778:	b.lt	2e894 <__gmpn_rootrem@@Base+0x900>  // b.tstop
   2e77c:	add	x26, x27, #0x1
   2e780:	cmp	x23, x26
   2e784:	str	x8, [x19, #80]
   2e788:	b.gt	2e7c0 <__gmpn_rootrem@@Base+0x82c>
   2e78c:	mov	x24, x25
   2e790:	mov	x25, x22
   2e794:	ldp	x22, x3, [x19, #168]
   2e798:	ldr	x5, [x19, #32]
   2e79c:	mov	x1, x21
   2e7a0:	mov	x4, x20
   2e7a4:	mov	x0, x22
   2e7a8:	bl	c320 <__gmpn_div_q@plt>
   2e7ac:	ldr	x8, [x22, x23, lsl #3]
   2e7b0:	mov	x22, x25
   2e7b4:	mov	x25, x24
   2e7b8:	cmp	x8, #0x0
   2e7bc:	cinc	x23, x23, ne  // ne = any
   2e7c0:	ldr	x8, [x19, #136]
   2e7c4:	cmp	x26, x23
   2e7c8:	b.ge	2e7fc <__gmpn_rootrem@@Base+0x868>  // b.tcont
   2e7cc:	ldr	x20, [x19, #160]
   2e7d0:	ldr	x26, [x19, #128]
   2e7d4:	ldr	x24, [x19, #144]
   2e7d8:	cbz	x27, 2e3a4 <__gmpn_rootrem@@Base+0x410>
   2e7dc:	lsl	x2, x27, #3
   2e7e0:	mov	w1, #0xff                  	// #255
   2e7e4:	mov	x0, x20
   2e7e8:	bl	c5f0 <memset@plt>
   2e7ec:	cmp	x27, #0x1
   2e7f0:	b.ne	2e84c <__gmpn_rootrem@@Base+0x8b8>  // b.any
   2e7f4:	mov	w8, #0x1                   	// #1
   2e7f8:	b	2e868 <__gmpn_rootrem@@Base+0x8d4>
   2e7fc:	b.ne	2e818 <__gmpn_rootrem@@Base+0x884>  // b.any
   2e800:	cbz	x8, 2e818 <__gmpn_rootrem@@Base+0x884>
   2e804:	ldr	x8, [x19, #168]
   2e808:	ldr	x9, [x19, #56]
   2e80c:	ldr	x8, [x8, x27, lsl #3]
   2e810:	cmp	x8, x9
   2e814:	b.cs	2e7cc <__gmpn_rootrem@@Base+0x838>  // b.hs, b.nlast
   2e818:	ldp	x0, x1, [x19, #160]
   2e81c:	mov	x2, x23
   2e820:	bl	ca50 <__gmpn_copyi@plt>
   2e824:	subs	x8, x26, x23
   2e828:	ldr	x26, [x19, #128]
   2e82c:	ldr	x24, [x19, #144]
   2e830:	b.eq	2e3b8 <__gmpn_rootrem@@Base+0x424>  // b.none
   2e834:	ldr	x9, [x19, #160]
   2e838:	lsl	x2, x8, #3
   2e83c:	mov	w1, wzr
   2e840:	add	x0, x9, x23, lsl #3
   2e844:	bl	c5f0 <memset@plt>
   2e848:	b	2e3b8 <__gmpn_rootrem@@Base+0x424>
   2e84c:	and	x9, x27, #0x3fffffffffffffe
   2e850:	orr	x8, x27, #0x1
   2e854:	mov	x10, x9
   2e858:	subs	x10, x10, #0x2
   2e85c:	b.ne	2e858 <__gmpn_rootrem@@Base+0x8c4>  // b.any
   2e860:	cmp	x27, x9
   2e864:	b.eq	2e3a4 <__gmpn_rootrem@@Base+0x410>  // b.none
   2e868:	sub	x8, x27, x8
   2e86c:	add	x8, x8, #0x1
   2e870:	subs	x8, x8, #0x1
   2e874:	b.ne	2e870 <__gmpn_rootrem@@Base+0x8dc>  // b.any
   2e878:	b	2e3a4 <__gmpn_rootrem@@Base+0x410>
   2e87c:	mov	w8, wzr
   2e880:	b	2e88c <__gmpn_rootrem@@Base+0x8f8>
   2e884:	mov	w8, wzr
   2e888:	mov	x23, x20
   2e88c:	ldr	x21, [x19, #88]
   2e890:	b	2e4f8 <__gmpn_rootrem@@Base+0x564>
   2e894:	ldr	x0, [x19, #160]
   2e898:	lsl	x8, x27, #3
   2e89c:	add	x2, x8, #0x8
   2e8a0:	mov	w1, wzr
   2e8a4:	bl	c5f0 <memset@plt>
   2e8a8:	ldr	x26, [x19, #128]
   2e8ac:	ldr	x24, [x19, #144]
   2e8b0:	b	2e3b8 <__gmpn_rootrem@@Base+0x424>
   2e8b4:	cmp	x9, #0x1
   2e8b8:	b	2ea9c <__gmpn_rootrem@@Base+0xb08>
   2e8bc:	ldr	x23, [x19, #160]
   2e8c0:	mov	w24, #0x1                   	// #1
   2e8c4:	ldr	w8, [x19, #20]
   2e8c8:	ldr	x9, [x19, #112]
   2e8cc:	cbz	w8, 2e8dc <__gmpn_rootrem@@Base+0x948>
   2e8d0:	ldr	x8, [x23]
   2e8d4:	cmp	x8, #0x1
   2e8d8:	b.hi	2e964 <__gmpn_rootrem@@Base+0x9d0>  // b.pmore
   2e8dc:	ldr	x23, [x19, #72]
   2e8e0:	ldr	x25, [x19, #120]
   2e8e4:	ldr	x26, [x19, #48]
   2e8e8:	add	x8, x25, x23, lsl #3
   2e8ec:	sub	x21, x8, #0x8
   2e8f0:	ldr	x8, [x19, #32]
   2e8f4:	add	x22, x8, x23, lsl #4
   2e8f8:	ldp	x27, x0, [x19, #160]
   2e8fc:	ldr	x4, [x19, #176]
   2e900:	mov	x2, x24
   2e904:	mov	x3, x26
   2e908:	mov	x1, x27
   2e90c:	bl	d210 <__gmpn_pow_1@plt>
   2e910:	cmp	x0, x23
   2e914:	b.gt	2e94c <__gmpn_rootrem@@Base+0x9b8>
   2e918:	b.ne	2eac8 <__gmpn_rootrem@@Base+0xb34>  // b.any
   2e91c:	mov	x8, xzr
   2e920:	add	x9, x23, x8
   2e924:	cmp	x9, #0x1
   2e928:	b.lt	2e960 <__gmpn_rootrem@@Base+0x9cc>  // b.tstop
   2e92c:	lsl	x9, x8, #3
   2e930:	ldr	x10, [x22, x9]
   2e934:	ldr	x9, [x21, x9]
   2e938:	sub	x8, x8, #0x1
   2e93c:	cmp	x10, x9
   2e940:	b.eq	2e920 <__gmpn_rootrem@@Base+0x98c>  // b.none
   2e944:	ldr	x27, [x19, #160]
   2e948:	b.ls	2ead0 <__gmpn_rootrem@@Base+0xb3c>  // b.plast
   2e94c:	ldr	x9, [x27]
   2e950:	sub	x10, x9, #0x1
   2e954:	str	x10, [x27], #8
   2e958:	cbz	x9, 2e94c <__gmpn_rootrem@@Base+0x9b8>
   2e95c:	b	2e8f8 <__gmpn_rootrem@@Base+0x964>
   2e960:	mov	x9, xzr
   2e964:	ldr	x0, [x19, #184]
   2e968:	cbz	x0, 2eaa4 <__gmpn_rootrem@@Base+0xb10>
   2e96c:	mov	x20, x9
   2e970:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2e974:	mov	x9, x20
   2e978:	b	2eaa4 <__gmpn_rootrem@@Base+0xb10>
   2e97c:	mov	x10, xzr
   2e980:	mov	w9, #0x1                   	// #1
   2e984:	mov	x11, x8
   2e988:	cmp	x9, x24
   2e98c:	b.ge	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.tcont
   2e990:	add	x12, x27, x10
   2e994:	ldr	x12, [x12, #8]
   2e998:	add	x13, x16, x10
   2e99c:	add	x9, x9, #0x1
   2e9a0:	add	x10, x10, #0x8
   2e9a4:	sub	x14, x12, #0x1
   2e9a8:	sub	x11, x11, #0x1
   2e9ac:	str	x14, [x13, #8]
   2e9b0:	cbz	x12, 2e988 <__gmpn_rootrem@@Base+0x9f4>
   2e9b4:	cmp	x27, x16
   2e9b8:	b.eq	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.none
   2e9bc:	cmp	x9, x24
   2e9c0:	b.ge	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.tcont
   2e9c4:	sub	x12, x24, x9
   2e9c8:	cmp	x12, #0x4
   2e9cc:	b.cc	2ea3c <__gmpn_rootrem@@Base+0xaa8>  // b.lo, b.ul, b.last
   2e9d0:	add	x14, x16, x10
   2e9d4:	lsl	x13, x24, #3
   2e9d8:	add	x14, x14, #0x8
   2e9dc:	add	x15, x27, x13
   2e9e0:	cmp	x14, x15
   2e9e4:	b.cs	2e9fc <__gmpn_rootrem@@Base+0xa68>  // b.hs, b.nlast
   2e9e8:	add	x14, x27, x10
   2e9ec:	add	x13, x16, x13
   2e9f0:	add	x14, x14, #0x8
   2e9f4:	cmp	x14, x13
   2e9f8:	b.cc	2ea3c <__gmpn_rootrem@@Base+0xaa8>  // b.lo, b.ul, b.last
   2e9fc:	add	x13, x16, x10
   2ea00:	add	x14, x27, x10
   2ea04:	and	x10, x12, #0xfffffffffffffffc
   2ea08:	and	x15, x11, #0xfffffffffffffffc
   2ea0c:	add	x11, x13, #0x18
   2ea10:	add	x13, x14, #0x18
   2ea14:	add	x9, x15, x9
   2ea18:	mov	x14, x10
   2ea1c:	ldp	q0, q1, [x13, #-16]
   2ea20:	add	x13, x13, #0x20
   2ea24:	subs	x14, x14, #0x4
   2ea28:	stp	q0, q1, [x11, #-16]
   2ea2c:	add	x11, x11, #0x20
   2ea30:	b.ne	2ea1c <__gmpn_rootrem@@Base+0xa88>  // b.any
   2ea34:	cmp	x12, x10
   2ea38:	b.eq	2ea94 <__gmpn_rootrem@@Base+0xb00>  // b.none
   2ea3c:	lsl	x11, x9, #3
   2ea40:	sub	x10, x24, x9
   2ea44:	add	x9, x16, x11
   2ea48:	add	x11, x27, x11
   2ea4c:	ldr	x12, [x11], #8
   2ea50:	subs	x10, x10, #0x1
   2ea54:	str	x12, [x9], #8
   2ea58:	b.ne	2ea4c <__gmpn_rootrem@@Base+0xab8>  // b.any
   2ea5c:	b	2ea94 <__gmpn_rootrem@@Base+0xb00>
   2ea60:	and	x10, x8, #0xfffffffffffffffc
   2ea64:	add	x11, x27, #0x18
   2ea68:	orr	x9, x10, #0x1
   2ea6c:	add	x12, x16, #0x18
   2ea70:	mov	x13, x10
   2ea74:	ldp	q0, q1, [x11, #-16]
   2ea78:	add	x11, x11, #0x20
   2ea7c:	subs	x13, x13, #0x4
   2ea80:	stp	q0, q1, [x12, #-16]
   2ea84:	add	x12, x12, #0x20
   2ea88:	b.ne	2ea74 <__gmpn_rootrem@@Base+0xae0>  // b.any
   2ea8c:	cmp	x8, x10
   2ea90:	b.ne	2e1a8 <__gmpn_rootrem@@Base+0x214>  // b.any
   2ea94:	ldr	x8, [x16, x8, lsl #3]
   2ea98:	cmp	x8, #0x0
   2ea9c:	cset	w8, eq  // eq = none
   2eaa0:	sub	x9, x24, x8
   2eaa4:	mov	x0, x9
   2eaa8:	mov	sp, x29
   2eaac:	ldp	x20, x19, [sp, #80]
   2eab0:	ldp	x22, x21, [sp, #64]
   2eab4:	ldp	x24, x23, [sp, #48]
   2eab8:	ldp	x26, x25, [sp, #32]
   2eabc:	ldp	x28, x27, [sp, #16]
   2eac0:	ldp	x29, x30, [sp], #96
   2eac4:	ret
   2eac8:	mov	x20, x0
   2eacc:	b	2ead4 <__gmpn_rootrem@@Base+0xb40>
   2ead0:	mov	x20, x23
   2ead4:	ldr	x0, [x19, #8]
   2ead8:	cbz	x0, 2eb1c <__gmpn_rootrem@@Base+0xb88>
   2eadc:	cbz	x20, 2eb28 <__gmpn_rootrem@@Base+0xb94>
   2eae0:	ldr	x2, [x19, #168]
   2eae4:	mov	x1, x25
   2eae8:	mov	x3, x20
   2eaec:	bl	c2d0 <__gmpn_sub_n@plt>
   2eaf0:	cbz	x0, 2eb24 <__gmpn_rootrem@@Base+0xb90>
   2eaf4:	ldr	x0, [x19, #8]
   2eaf8:	cmp	x20, x23
   2eafc:	b.ge	2ebc4 <__gmpn_rootrem@@Base+0xc30>  // b.tcont
   2eb00:	lsl	x8, x20, #3
   2eb04:	ldr	x9, [x25, x8]
   2eb08:	add	x20, x20, #0x1
   2eb0c:	sub	x10, x9, #0x1
   2eb10:	str	x10, [x0, x8]
   2eb14:	cbz	x9, 2eaf8 <__gmpn_rootrem@@Base+0xb64>
   2eb18:	b	2eb28 <__gmpn_rootrem@@Base+0xb94>
   2eb1c:	mov	w9, #0x1                   	// #1
   2eb20:	b	2e964 <__gmpn_rootrem@@Base+0x9d0>
   2eb24:	ldr	x0, [x19, #8]
   2eb28:	cmp	x0, x25
   2eb2c:	b.eq	2ebc4 <__gmpn_rootrem@@Base+0xc30>  // b.none
   2eb30:	cmp	x20, x23
   2eb34:	b.ge	2ebc4 <__gmpn_rootrem@@Base+0xc30>  // b.tcont
   2eb38:	sub	x8, x23, x20
   2eb3c:	cmp	x8, #0x4
   2eb40:	b.cc	2eba4 <__gmpn_rootrem@@Base+0xc10>  // b.lo, b.ul, b.last
   2eb44:	lsl	x10, x20, #3
   2eb48:	lsl	x9, x23, #3
   2eb4c:	add	x11, x0, x10
   2eb50:	add	x12, x25, x9
   2eb54:	cmp	x11, x12
   2eb58:	b.cs	2eb6c <__gmpn_rootrem@@Base+0xbd8>  // b.hs, b.nlast
   2eb5c:	add	x9, x0, x9
   2eb60:	add	x11, x25, x10
   2eb64:	cmp	x11, x9
   2eb68:	b.cc	2eba4 <__gmpn_rootrem@@Base+0xc10>  // b.lo, b.ul, b.last
   2eb6c:	and	x9, x8, #0xfffffffffffffffc
   2eb70:	add	x11, x10, #0x10
   2eb74:	add	x20, x20, x9
   2eb78:	add	x10, x25, x11
   2eb7c:	add	x11, x0, x11
   2eb80:	mov	x12, x9
   2eb84:	ldp	q0, q1, [x10, #-16]
   2eb88:	add	x10, x10, #0x20
   2eb8c:	subs	x12, x12, #0x4
   2eb90:	stp	q0, q1, [x11, #-16]
   2eb94:	add	x11, x11, #0x20
   2eb98:	b.ne	2eb84 <__gmpn_rootrem@@Base+0xbf0>  // b.any
   2eb9c:	cmp	x8, x9
   2eba0:	b.eq	2ebc4 <__gmpn_rootrem@@Base+0xc30>  // b.none
   2eba4:	lsl	x10, x20, #3
   2eba8:	sub	x8, x23, x20
   2ebac:	add	x9, x0, x10
   2ebb0:	add	x10, x25, x10
   2ebb4:	ldr	x11, [x10], #8
   2ebb8:	subs	x8, x8, #0x1
   2ebbc:	str	x11, [x9], #8
   2ebc0:	b.ne	2ebb4 <__gmpn_rootrem@@Base+0xc20>  // b.any
   2ebc4:	sub	x8, x0, #0x8
   2ebc8:	ldr	x10, [x8, x23, lsl #3]
   2ebcc:	sub	x9, x23, #0x1
   2ebd0:	mov	x23, x9
   2ebd4:	cbz	x10, 2ebc8 <__gmpn_rootrem@@Base+0xc34>
   2ebd8:	add	x9, x9, #0x1
   2ebdc:	b	2e964 <__gmpn_rootrem@@Base+0x9d0>
   2ebe0:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   2ebe4:	add	x10, x10, #0x5f4
   2ebe8:	ldrb	w10, [x10, x8]
   2ebec:	udiv	x8, x9, x28
   2ebf0:	msub	x9, x8, x28, x9
   2ebf4:	bfi	x10, x9, #8, #56
   2ebf8:	udiv	x9, x10, x28
   2ebfc:	b	2e230 <__gmpn_rootrem@@Base+0x29c>
   2ec00:	add	x0, x19, #0xb8
   2ec04:	mov	x26, x16
   2ec08:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2ec0c:	mov	x16, x26
   2ec10:	b	2e344 <__gmpn_rootrem@@Base+0x3b0>

000000000002ec14 <__gmpn_sqrtrem@@Base>:
   2ec14:	stp	x29, x30, [sp, #-96]!
   2ec18:	str	x27, [sp, #16]
   2ec1c:	stp	x26, x25, [sp, #32]
   2ec20:	stp	x24, x23, [sp, #48]
   2ec24:	stp	x22, x21, [sp, #64]
   2ec28:	stp	x20, x19, [sp, #80]
   2ec2c:	mov	x29, sp
   2ec30:	sub	sp, sp, #0x10
   2ec34:	add	x8, x2, x3, lsl #3
   2ec38:	ldur	x9, [x8, #-8]
   2ec3c:	mov	x23, x2
   2ec40:	mov	x20, x1
   2ec44:	mov	x19, x0
   2ec48:	clz	x8, x9
   2ec4c:	lsr	x10, x9, #62
   2ec50:	ubfx	x8, x8, #1, #31
   2ec54:	cmp	x10, #0x0
   2ec58:	csel	w22, wzr, w8, ne  // ne = any
   2ec5c:	cmp	x3, #0x2
   2ec60:	b.eq	2ed1c <__gmpn_sqrtrem@@Base+0x108>  // b.none
   2ec64:	mov	x24, x3
   2ec68:	cmp	x3, #0x1
   2ec6c:	b.ne	2ee30 <__gmpn_sqrtrem@@Base+0x21c>  // b.any
   2ec70:	mov	x8, #0x1ffff00000000       	// #562945658454016
   2ec74:	movk	x8, #0xfffd, lsl #16
   2ec78:	cbz	w22, 2eee0 <__gmpn_sqrtrem@@Base+0x2cc>
   2ec7c:	lsl	w10, w22, #1
   2ec80:	lsl	x10, x9, x10
   2ec84:	adrp	x11, 5b000 <__gmpn_bases@@Base+0x2678>
   2ec88:	lsr	x12, x10, #55
   2ec8c:	add	x11, x11, #0x7f4
   2ec90:	sub	w12, w12, #0x80
   2ec94:	ldrb	w11, [x11, w12, uxtw]
   2ec98:	lsr	x13, x10, #31
   2ec9c:	lsr	x14, x10, #24
   2eca0:	mov	x12, #0xffffff0000000000    	// #-1099511627776
   2eca4:	orr	x11, x11, #0x100
   2eca8:	mul	x13, x11, x13
   2ecac:	msub	x8, x13, x11, x8
   2ecb0:	asr	x8, x8, #16
   2ecb4:	mul	x8, x8, x11
   2ecb8:	asr	x8, x8, #18
   2ecbc:	add	x8, x8, x11, lsl #16
   2ecc0:	mul	x11, x8, x14
   2ecc4:	lsl	x13, x10, #14
   2ecc8:	lsr	x14, x11, #25
   2eccc:	msub	x13, x14, x14, x13
   2ecd0:	add	x12, x13, x12
   2ecd4:	asr	x12, x12, #24
   2ecd8:	mul	x8, x12, x8
   2ecdc:	add	x8, x11, x8, asr #15
   2ece0:	lsr	x11, x8, #32
   2ece4:	mul	x8, x11, x11
   2ece8:	lsl	x12, x11, #1
   2ecec:	sub	x13, x10, #0x1
   2ecf0:	add	x12, x8, x12
   2ecf4:	mov	w14, #0x1                   	// #1
   2ecf8:	cmp	x12, x13
   2ecfc:	bfi	x14, x11, #1, #32
   2ed00:	cinc	x12, x11, ls  // ls = plast
   2ed04:	csneg	x11, xzr, x14, hi  // hi = pmore
   2ed08:	lsr	x12, x12, x22
   2ed0c:	str	x12, [x19]
   2ed10:	cbz	x20, 2f078 <__gmpn_sqrtrem@@Base+0x464>
   2ed14:	msub	x8, x12, x12, x9
   2ed18:	b	2ef74 <__gmpn_sqrtrem@@Base+0x360>
   2ed1c:	ldr	x10, [x23]
   2ed20:	mov	x11, #0x1ffff00000000       	// #562945658454016
   2ed24:	cmp	x20, #0x0
   2ed28:	sub	x8, x29, #0x10
   2ed2c:	movk	x11, #0xfffd, lsl #16
   2ed30:	csel	x8, x8, x20, eq  // eq = none
   2ed34:	cbz	w22, 2ef7c <__gmpn_sqrtrem@@Base+0x368>
   2ed38:	lsl	w12, w22, #1
   2ed3c:	neg	w14, w12
   2ed40:	lsl	x9, x9, x12
   2ed44:	lsr	x14, x10, x14
   2ed48:	orr	x9, x14, x9
   2ed4c:	adrp	x13, 5b000 <__gmpn_bases@@Base+0x2678>
   2ed50:	lsr	x14, x9, #55
   2ed54:	add	x13, x13, #0x7f4
   2ed58:	sub	w14, w14, #0x80
   2ed5c:	ldrb	w13, [x13, w14, uxtw]
   2ed60:	lsr	x15, x9, #31
   2ed64:	lsr	x16, x9, #24
   2ed68:	mov	x14, #0xffffff0000000000    	// #-1099511627776
   2ed6c:	orr	x13, x13, #0x100
   2ed70:	mul	x15, x13, x15
   2ed74:	msub	x11, x15, x13, x11
   2ed78:	asr	x11, x11, #16
   2ed7c:	mul	x11, x11, x13
   2ed80:	asr	x11, x11, #18
   2ed84:	add	x11, x11, x13, lsl #16
   2ed88:	mul	x13, x11, x16
   2ed8c:	lsl	x15, x9, #14
   2ed90:	lsr	x16, x13, #25
   2ed94:	msub	x15, x16, x16, x15
   2ed98:	add	x14, x15, x14
   2ed9c:	asr	x14, x14, #24
   2eda0:	mul	x11, x14, x11
   2eda4:	add	x11, x13, x11, asr #15
   2eda8:	lsr	x11, x11, #32
   2edac:	mul	x13, x11, x11
   2edb0:	lsl	x14, x11, #1
   2edb4:	mov	w16, #0x1                   	// #1
   2edb8:	sub	x15, x9, #0x1
   2edbc:	add	x14, x13, x14
   2edc0:	bfi	x16, x11, #1, #32
   2edc4:	cmp	x14, x15
   2edc8:	str	x9, [x8, #8]
   2edcc:	sub	x9, x9, x13
   2edd0:	csneg	x13, xzr, x16, hi  // hi = pmore
   2edd4:	lsl	x12, x10, x12
   2edd8:	add	x9, x13, x9
   2eddc:	cinc	x11, x11, ls  // ls = plast
   2ede0:	extr	x9, x9, x12, #33
   2ede4:	udiv	x13, x9, x11
   2ede8:	sub	x13, x13, x13, lsr #32
   2edec:	msub	x9, x11, x13, x9
   2edf0:	orr	x11, x13, x11, lsl #32
   2edf4:	mul	x13, x13, x13
   2edf8:	bfi	x12, x9, #33, #31
   2edfc:	cmp	x12, x13
   2ee00:	lsr	x14, x9, #31
   2ee04:	cset	w9, cc  // cc = lo, ul, last
   2ee08:	sub	w9, w14, w9
   2ee0c:	asr	w9, w9, #31
   2ee10:	add	x9, x11, w9, sxtw
   2ee14:	lsr	x9, x9, x22
   2ee18:	str	x9, [x19]
   2ee1c:	msub	x9, x9, x9, x10
   2ee20:	cmp	x9, #0x0
   2ee24:	str	x9, [x8]
   2ee28:	cset	w20, ne  // ne = any
   2ee2c:	b	2f258 <__gmpn_sqrtrem@@Base+0x644>
   2ee30:	add	x25, x24, #0x1
   2ee34:	add	x8, x24, #0x2
   2ee38:	cmp	x25, #0x0
   2ee3c:	csinc	x8, x8, x24, lt  // lt = tstop
   2ee40:	asr	x21, x8, #1
   2ee44:	cbnz	x20, 2ee70 <__gmpn_sqrtrem@@Base+0x25c>
   2ee48:	cmp	x24, #0x9
   2ee4c:	b.lt	2ee70 <__gmpn_sqrtrem@@Base+0x25c>  // b.tstop
   2ee50:	and	w4, w24, #0x1
   2ee54:	mov	x0, x19
   2ee58:	mov	x1, x23
   2ee5c:	mov	x2, x21
   2ee60:	mov	w3, w22
   2ee64:	bl	2f2a4 <__gmpn_sqrtrem@@Base+0x690>
   2ee68:	sxtw	x20, w0
   2ee6c:	b	2f258 <__gmpn_sqrtrem@@Base+0x644>
   2ee70:	and	x27, x24, #0x1
   2ee74:	orr	x8, x27, x22
   2ee78:	stur	xzr, [x29, #-16]
   2ee7c:	cbz	x8, 2f08c <__gmpn_sqrtrem@@Base+0x478>
   2ee80:	add	x8, x25, #0x3
   2ee84:	cmp	x25, #0x0
   2ee88:	lsl	x26, x21, #1
   2ee8c:	csel	x8, x8, x25, lt  // lt = tstop
   2ee90:	add	x8, x26, x8, lsr #2
   2ee94:	lsl	x8, x8, #3
   2ee98:	add	x1, x8, #0x8
   2ee9c:	mov	w8, #0x7f00                	// #32512
   2eea0:	cmp	x1, x8
   2eea4:	b.hi	2f284 <__gmpn_sqrtrem@@Base+0x670>  // b.pmore
   2eea8:	add	x9, x1, #0xf
   2eeac:	mov	x8, sp
   2eeb0:	and	x9, x9, #0xfffffffffffffff0
   2eeb4:	sub	x25, x8, x9
   2eeb8:	mov	sp, x25
   2eebc:	add	x26, x25, x26, lsl #3
   2eec0:	add	x0, x25, x27, lsl #3
   2eec4:	str	xzr, [x25]
   2eec8:	cbz	w22, 2f11c <__gmpn_sqrtrem@@Base+0x508>
   2eecc:	lsl	w3, w22, #1
   2eed0:	mov	x1, x23
   2eed4:	mov	x2, x24
   2eed8:	bl	c180 <__gmpn_lshift@plt>
   2eedc:	b	2f128 <__gmpn_sqrtrem@@Base+0x514>
   2eee0:	lsr	x10, x9, #55
   2eee4:	adrp	x11, 5b000 <__gmpn_bases@@Base+0x2678>
   2eee8:	add	x11, x11, #0x7f4
   2eeec:	sub	w10, w10, #0x80
   2eef0:	ldrb	w10, [x11, w10, uxtw]
   2eef4:	lsr	x11, x9, #31
   2eef8:	lsr	x12, x9, #24
   2eefc:	lsl	x13, x9, #14
   2ef00:	orr	x10, x10, #0x100
   2ef04:	mul	x11, x10, x11
   2ef08:	msub	x8, x11, x10, x8
   2ef0c:	asr	x8, x8, #16
   2ef10:	mul	x8, x8, x10
   2ef14:	asr	x8, x8, #18
   2ef18:	add	x8, x8, x10, lsl #16
   2ef1c:	mul	x10, x8, x12
   2ef20:	lsr	x12, x10, #25
   2ef24:	mov	x11, #0xffffff0000000000    	// #-1099511627776
   2ef28:	msub	x12, x12, x12, x13
   2ef2c:	add	x11, x12, x11
   2ef30:	asr	x11, x11, #24
   2ef34:	mul	x8, x11, x8
   2ef38:	add	x8, x10, x8, asr #15
   2ef3c:	lsr	x8, x8, #32
   2ef40:	mul	x10, x8, x8
   2ef44:	lsl	x11, x8, #1
   2ef48:	sub	x13, x9, #0x1
   2ef4c:	mov	w12, #0x1                   	// #1
   2ef50:	add	x11, x10, x11
   2ef54:	bfi	x12, x8, #1, #32
   2ef58:	cmp	x11, x13
   2ef5c:	sub	x9, x9, x10
   2ef60:	cinc	x10, x8, ls  // ls = plast
   2ef64:	csneg	x8, xzr, x12, hi  // hi = pmore
   2ef68:	add	x8, x8, x9
   2ef6c:	str	x10, [x19]
   2ef70:	cbz	x20, 2f080 <__gmpn_sqrtrem@@Base+0x46c>
   2ef74:	str	x8, [x20]
   2ef78:	b	2f080 <__gmpn_sqrtrem@@Base+0x46c>
   2ef7c:	ldr	x9, [x23, #8]
   2ef80:	adrp	x12, 5b000 <__gmpn_bases@@Base+0x2678>
   2ef84:	add	x12, x12, #0x7f4
   2ef88:	lsr	x13, x9, #55
   2ef8c:	sub	w13, w13, #0x80
   2ef90:	ldrb	w12, [x12, w13, uxtw]
   2ef94:	lsr	x14, x9, #31
   2ef98:	lsr	x15, x9, #24
   2ef9c:	mov	x13, #0xffffff0000000000    	// #-1099511627776
   2efa0:	orr	x12, x12, #0x100
   2efa4:	mul	x14, x12, x14
   2efa8:	msub	x11, x14, x12, x11
   2efac:	asr	x11, x11, #16
   2efb0:	mul	x11, x11, x12
   2efb4:	asr	x11, x11, #18
   2efb8:	add	x11, x11, x12, lsl #16
   2efbc:	mul	x12, x11, x15
   2efc0:	lsl	x14, x9, #14
   2efc4:	lsr	x15, x12, #25
   2efc8:	msub	x14, x15, x15, x14
   2efcc:	add	x13, x14, x13
   2efd0:	asr	x13, x13, #24
   2efd4:	mul	x11, x13, x11
   2efd8:	add	x11, x12, x11, asr #15
   2efdc:	lsr	x11, x11, #32
   2efe0:	mul	x12, x11, x11
   2efe4:	lsl	x13, x11, #1
   2efe8:	mov	w15, #0x1                   	// #1
   2efec:	sub	x14, x9, #0x1
   2eff0:	add	x13, x12, x13
   2eff4:	bfi	x15, x11, #1, #32
   2eff8:	cmp	x13, x14
   2effc:	sub	x9, x9, x12
   2f000:	cinc	x12, x11, ls  // ls = plast
   2f004:	csneg	x11, xzr, x15, hi  // hi = pmore
   2f008:	add	x9, x11, x9
   2f00c:	str	x9, [x8]
   2f010:	extr	x9, x9, x10, #33
   2f014:	udiv	x11, x9, x12
   2f018:	sub	x13, x11, x11, lsr #32
   2f01c:	msub	x9, x12, x13, x9
   2f020:	lsr	x14, x9, #31
   2f024:	bfi	x10, x9, #33, #31
   2f028:	mul	x9, x13, x13
   2f02c:	subs	x11, x10, x9
   2f030:	cset	w9, cc  // cc = lo, ul, last
   2f034:	subs	w9, w14, w9
   2f038:	orr	x10, x13, x12, lsl #32
   2f03c:	b.pl	2f054 <__gmpn_sqrtrem@@Base+0x440>  // b.nfrst
   2f040:	adds	x11, x11, x10
   2f044:	sub	x10, x10, #0x1
   2f048:	cinc	w9, w9, cs  // cs = hs, nlast
   2f04c:	adds	x11, x11, x10
   2f050:	cinc	w9, w9, cs  // cs = hs, nlast
   2f054:	str	x11, [x8]
   2f058:	str	x10, [x19]
   2f05c:	ldr	x10, [x8]
   2f060:	sxtw	x9, w9
   2f064:	str	x9, [x8, #8]
   2f068:	orr	x8, x10, x9
   2f06c:	cmp	x8, #0x0
   2f070:	cinc	x20, x9, ne  // ne = any
   2f074:	b	2f258 <__gmpn_sqrtrem@@Base+0x644>
   2f078:	sub	x8, x10, x8
   2f07c:	add	x8, x8, x11
   2f080:	cmp	x8, #0x0
   2f084:	cset	w20, ne  // ne = any
   2f088:	b	2f258 <__gmpn_sqrtrem@@Base+0x644>
   2f08c:	cmp	x20, x23
   2f090:	b.eq	2f0c4 <__gmpn_sqrtrem@@Base+0x4b0>  // b.none
   2f094:	cbnz	x20, 2f0b0 <__gmpn_sqrtrem@@Base+0x49c>
   2f098:	lsl	x8, x24, #3
   2f09c:	add	x8, x8, #0xf
   2f0a0:	mov	x9, sp
   2f0a4:	and	x8, x8, #0xfffffffffffffff0
   2f0a8:	sub	x20, x9, x8
   2f0ac:	mov	sp, x20
   2f0b0:	mov	x0, x20
   2f0b4:	mov	x1, x23
   2f0b8:	mov	x2, x24
   2f0bc:	bl	ca50 <__gmpn_copyi@plt>
   2f0c0:	mov	x23, x20
   2f0c4:	add	x8, x25, #0x3
   2f0c8:	cmp	x25, #0x0
   2f0cc:	csel	x8, x8, x25, lt  // lt = tstop
   2f0d0:	lsl	x8, x8, #1
   2f0d4:	and	x8, x8, #0xfffffffffffffff8
   2f0d8:	add	x1, x8, #0x8
   2f0dc:	mov	w8, #0x7f00                	// #32512
   2f0e0:	cmp	x1, x8
   2f0e4:	b.hi	2f294 <__gmpn_sqrtrem@@Base+0x680>  // b.pmore
   2f0e8:	add	x9, x1, #0xf
   2f0ec:	mov	x8, sp
   2f0f0:	and	x9, x9, #0xfffffffffffffff0
   2f0f4:	sub	x4, x8, x9
   2f0f8:	mov	sp, x4
   2f0fc:	mov	x0, x19
   2f100:	mov	x1, x23
   2f104:	mov	x2, x21
   2f108:	mov	x3, xzr
   2f10c:	bl	2f758 <__gmpn_sqrtrem@@Base+0xb44>
   2f110:	add	x19, x0, x21
   2f114:	str	x0, [x23, x21, lsl #3]
   2f118:	b	2f238 <__gmpn_sqrtrem@@Base+0x624>
   2f11c:	mov	x1, x23
   2f120:	mov	x2, x24
   2f124:	bl	ca50 <__gmpn_copyi@plt>
   2f128:	add	w22, w22, w27, lsl #5
   2f12c:	mov	x8, #0xffffffffffffffff    	// #-1
   2f130:	mov	x9, #0xfffffffffffffffe    	// #-2
   2f134:	lsl	x27, x8, x22
   2f138:	sub	x8, x9, x27
   2f13c:	cmp	x20, #0x0
   2f140:	csel	x3, x8, xzr, eq  // eq = none
   2f144:	mov	x0, x19
   2f148:	mov	x1, x25
   2f14c:	mov	x2, x21
   2f150:	mov	x4, x26
   2f154:	bl	2f758 <__gmpn_sqrtrem@@Base+0xb44>
   2f158:	ldr	x8, [x19]
   2f15c:	mov	x23, x0
   2f160:	mov	x0, x25
   2f164:	mov	x1, x19
   2f168:	bic	x26, x8, x27
   2f16c:	lsl	x3, x26, #1
   2f170:	mov	x2, x21
   2f174:	str	x26, [x29, #24]
   2f178:	bl	d400 <__gmpn_addmul_1@plt>
   2f17c:	add	x23, x0, x23
   2f180:	add	x1, x29, #0x18
   2f184:	mov	w2, #0x1                   	// #1
   2f188:	mov	x0, x25
   2f18c:	mov	x3, x26
   2f190:	bl	c9e0 <__gmpn_submul_1@plt>
   2f194:	cmp	x24, #0x3
   2f198:	b.lt	2f1e0 <__gmpn_sqrtrem@@Base+0x5cc>  // b.tstop
   2f19c:	ldr	x8, [x25, #8]
   2f1a0:	subs	x8, x8, x0
   2f1a4:	str	x8, [x25, #8]
   2f1a8:	b.cs	2f1dc <__gmpn_sqrtrem@@Base+0x5c8>  // b.hs, b.nlast
   2f1ac:	sub	x8, x21, #0x1
   2f1b0:	mov	w9, #0x2                   	// #2
   2f1b4:	mov	w0, #0x1                   	// #1
   2f1b8:	sub	x10, x9, #0x1
   2f1bc:	cmp	x10, x8
   2f1c0:	b.ge	2f1e0 <__gmpn_sqrtrem@@Base+0x5cc>  // b.tcont
   2f1c4:	lsl	x10, x9, #3
   2f1c8:	ldr	x11, [x25, x10]
   2f1cc:	add	x9, x9, #0x1
   2f1d0:	sub	x12, x11, #0x1
   2f1d4:	str	x12, [x25, x10]
   2f1d8:	cbz	x11, 2f1b8 <__gmpn_sqrtrem@@Base+0x5a4>
   2f1dc:	mov	x0, xzr
   2f1e0:	sub	x23, x23, x0
   2f1e4:	mov	x0, x19
   2f1e8:	mov	x1, x19
   2f1ec:	mov	x2, x21
   2f1f0:	mov	w3, w22
   2f1f4:	bl	c1a0 <__gmpn_rshift@plt>
   2f1f8:	cmp	x20, #0x0
   2f1fc:	str	x23, [x25, x21, lsl #3]
   2f200:	lsl	w8, w22, #1
   2f204:	csel	x23, x25, x20, eq  // eq = none
   2f208:	cmp	w22, #0x20
   2f20c:	add	x9, x25, #0x8
   2f210:	sub	w10, w8, #0x40
   2f214:	cinc	x19, x21, cc  // cc = lo, ul, last
   2f218:	csel	w3, w8, w10, cc  // cc = lo, ul, last
   2f21c:	csel	x1, x25, x9, cc  // cc = lo, ul, last
   2f220:	mov	x0, x23
   2f224:	mov	x2, x19
   2f228:	cbz	w3, 2f234 <__gmpn_sqrtrem@@Base+0x620>
   2f22c:	bl	c1a0 <__gmpn_rshift@plt>
   2f230:	b	2f238 <__gmpn_sqrtrem@@Base+0x624>
   2f234:	bl	ca50 <__gmpn_copyi@plt>
   2f238:	sub	x8, x23, #0x8
   2f23c:	mov	x20, x19
   2f240:	subs	x19, x19, #0x1
   2f244:	b.lt	2f250 <__gmpn_sqrtrem@@Base+0x63c>  // b.tstop
   2f248:	ldr	x9, [x8, x20, lsl #3]
   2f24c:	cbz	x9, 2f23c <__gmpn_sqrtrem@@Base+0x628>
   2f250:	ldur	x0, [x29, #-16]
   2f254:	cbnz	x0, 2f27c <__gmpn_sqrtrem@@Base+0x668>
   2f258:	mov	x0, x20
   2f25c:	mov	sp, x29
   2f260:	ldp	x20, x19, [sp, #80]
   2f264:	ldp	x22, x21, [sp, #64]
   2f268:	ldp	x24, x23, [sp, #48]
   2f26c:	ldp	x26, x25, [sp, #32]
   2f270:	ldr	x27, [sp, #16]
   2f274:	ldp	x29, x30, [sp], #96
   2f278:	ret
   2f27c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2f280:	b	2f258 <__gmpn_sqrtrem@@Base+0x644>
   2f284:	sub	x0, x29, #0x10
   2f288:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2f28c:	mov	x25, x0
   2f290:	b	2eebc <__gmpn_sqrtrem@@Base+0x2a8>
   2f294:	sub	x0, x29, #0x10
   2f298:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2f29c:	mov	x4, x0
   2f2a0:	b	2f0fc <__gmpn_sqrtrem@@Base+0x4e8>
   2f2a4:	stp	x29, x30, [sp, #-96]!
   2f2a8:	stp	x28, x27, [sp, #16]
   2f2ac:	stp	x26, x25, [sp, #32]
   2f2b0:	stp	x24, x23, [sp, #48]
   2f2b4:	stp	x22, x21, [sp, #64]
   2f2b8:	stp	x20, x19, [sp, #80]
   2f2bc:	mov	x29, sp
   2f2c0:	sub	sp, sp, #0x40
   2f2c4:	sub	x8, x2, #0x1
   2f2c8:	cmp	x8, #0x0
   2f2cc:	csel	x8, x2, x8, lt  // lt = tstop
   2f2d0:	asr	x23, x8, #1
   2f2d4:	add	x8, x23, x2, lsl #1
   2f2d8:	lsl	x8, x8, #3
   2f2dc:	mov	x24, x1
   2f2e0:	add	x1, x8, #0x20
   2f2e4:	mov	w8, #0x7f00                	// #32512
   2f2e8:	mov	w27, w3
   2f2ec:	mov	x19, x2
   2f2f0:	mov	x20, x0
   2f2f4:	cmp	x1, x8
   2f2f8:	sub	x8, x2, x23
   2f2fc:	stp	x8, xzr, [x29, #-16]
   2f300:	b.hi	2f740 <__gmpn_sqrtrem@@Base+0xb2c>  // b.pmore
   2f304:	add	x9, x1, #0xf
   2f308:	mov	x8, sp
   2f30c:	and	x9, x9, #0xfffffffffffffff0
   2f310:	sub	x25, x8, x9
   2f314:	mov	sp, x25
   2f318:	add	x8, x25, x19, lsl #3
   2f31c:	add	x22, x8, #0x8
   2f320:	stur	w4, [x29, #-28]
   2f324:	stur	x24, [x29, #-56]
   2f328:	cbz	w27, 2f36c <__gmpn_sqrtrem@@Base+0x758>
   2f32c:	ldur	x28, [x29, #-16]
   2f330:	add	w8, w4, #0x1
   2f334:	mov	x9, #0xfffffffffffffff8    	// #-8
   2f338:	cmp	x23, x8
   2f33c:	add	x10, x24, x23, lsl #3
   2f340:	csel	x8, x9, xzr, gt
   2f344:	add	x11, x19, x28
   2f348:	add	x0, x22, x8
   2f34c:	add	x8, x10, x8
   2f350:	cinc	x9, x11, gt
   2f354:	sub	x8, x8, w4, uxtw #3
   2f358:	add	x2, x9, #0x1
   2f35c:	sub	x1, x8, #0x8
   2f360:	lsl	w3, w27, #1
   2f364:	bl	c180 <__gmpn_lshift@plt>
   2f368:	b	2f38c <__gmpn_sqrtrem@@Base+0x778>
   2f36c:	ldur	x28, [x29, #-16]
   2f370:	add	x8, x24, x23, lsl #3
   2f374:	sub	x8, x8, w4, uxtw #3
   2f378:	sub	x1, x8, #0x8
   2f37c:	add	x9, x19, x28
   2f380:	add	x2, x9, #0x1
   2f384:	mov	x0, x22
   2f388:	bl	ca50 <__gmpn_copyi@plt>
   2f38c:	lsl	x8, x23, #3
   2f390:	add	x24, x20, x8
   2f394:	stur	x8, [x29, #-48]
   2f398:	add	x8, x22, x8
   2f39c:	mov	x26, x20
   2f3a0:	add	x20, x8, #0x8
   2f3a4:	mov	x0, x24
   2f3a8:	mov	x1, x20
   2f3ac:	mov	x2, x28
   2f3b0:	mov	x3, xzr
   2f3b4:	mov	x4, x25
   2f3b8:	stur	x22, [x29, #-24]
   2f3bc:	bl	2f758 <__gmpn_sqrtrem@@Base+0xb44>
   2f3c0:	mov	x22, x0
   2f3c4:	cbz	x0, 2f3dc <__gmpn_sqrtrem@@Base+0x7c8>
   2f3c8:	mov	x0, x20
   2f3cc:	mov	x1, x20
   2f3d0:	mov	x2, x24
   2f3d4:	mov	x3, x28
   2f3d8:	bl	c2d0 <__gmpn_sub_n@plt>
   2f3dc:	ldur	x1, [x29, #-24]
   2f3e0:	add	x2, x19, #0x1
   2f3e4:	mov	x3, x24
   2f3e8:	mov	x4, x28
   2f3ec:	add	x8, x1, x19, lsl #3
   2f3f0:	add	x20, x8, #0x8
   2f3f4:	mov	x0, x20
   2f3f8:	mov	x5, x25
   2f3fc:	stur	x25, [x29, #-40]
   2f400:	bl	2fae4 <__gmpn_sqrtrem@@Base+0xed0>
   2f404:	add	x21, x23, #0x1
   2f408:	ldr	x8, [x20, x21, lsl #3]
   2f40c:	add	x25, x8, x22
   2f410:	cmp	x25, #0x2
   2f414:	b.cc	2f454 <__gmpn_sqrtrem@@Base+0x840>  // b.lo, b.ul, b.last
   2f418:	ldur	x2, [x29, #-48]
   2f41c:	mov	w1, #0xff                  	// #255
   2f420:	mov	x0, x26
   2f424:	bl	c5f0 <memset@plt>
   2f428:	ldur	w13, [x29, #-28]
   2f42c:	mov	w22, #0x1                   	// #1
   2f430:	mov	w25, w27
   2f434:	ldur	x0, [x29, #-8]
   2f438:	cbz	x0, 2f4b8 <__gmpn_sqrtrem@@Base+0x8a4>
   2f43c:	mov	w20, w13
   2f440:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2f444:	mov	w13, w20
   2f448:	orr	w8, w13, w25
   2f44c:	cbnz	w8, 2f4c0 <__gmpn_sqrtrem@@Base+0x8ac>
   2f450:	b	2f4dc <__gmpn_sqrtrem@@Base+0x8c8>
   2f454:	add	x28, x20, #0x8
   2f458:	mov	w3, #0x1                   	// #1
   2f45c:	mov	x0, x26
   2f460:	mov	x1, x28
   2f464:	mov	x2, x23
   2f468:	mov	w22, #0x1                   	// #1
   2f46c:	bl	c1a0 <__gmpn_rshift@plt>
   2f470:	add	x8, x26, x23, lsl #3
   2f474:	ldur	x9, [x8, #-8]
   2f478:	ldur	w13, [x29, #-28]
   2f47c:	mov	w10, #0x40                  	// #64
   2f480:	mvn	w11, w27
   2f484:	orr	x9, x9, x25, lsl #63
   2f488:	stur	x9, [x8, #-8]
   2f48c:	ldp	x8, x9, [x20]
   2f490:	lsr	w10, w10, w13
   2f494:	add	w10, w10, w11
   2f498:	mov	x11, #0xffffffffffffffff    	// #-1
   2f49c:	lsr	x10, x11, x10
   2f4a0:	and	x9, x9, x10
   2f4a4:	mov	w25, w27
   2f4a8:	orr	x8, x9, x8, lsr #3
   2f4ac:	cbz	x8, 2f500 <__gmpn_sqrtrem@@Base+0x8ec>
   2f4b0:	ldur	x0, [x29, #-8]
   2f4b4:	cbnz	x0, 2f43c <__gmpn_sqrtrem@@Base+0x828>
   2f4b8:	orr	w8, w13, w25
   2f4bc:	cbz	w8, 2f4dc <__gmpn_sqrtrem@@Base+0x8c8>
   2f4c0:	cmp	w13, #0x0
   2f4c4:	cset	w8, ne  // ne = any
   2f4c8:	add	w3, w25, w8, lsl #5
   2f4cc:	mov	x0, x26
   2f4d0:	mov	x1, x26
   2f4d4:	mov	x2, x19
   2f4d8:	bl	c1a0 <__gmpn_rshift@plt>
   2f4dc:	mov	w0, w22
   2f4e0:	mov	sp, x29
   2f4e4:	ldp	x20, x19, [sp, #80]
   2f4e8:	ldp	x22, x21, [sp, #64]
   2f4ec:	ldp	x24, x23, [sp, #48]
   2f4f0:	ldp	x26, x25, [sp, #32]
   2f4f4:	ldp	x28, x27, [sp, #16]
   2f4f8:	ldp	x29, x30, [sp], #96
   2f4fc:	ret
   2f500:	ldur	x20, [x29, #-40]
   2f504:	ldur	x27, [x29, #-16]
   2f508:	mov	x1, x24
   2f50c:	mov	x3, x28
   2f510:	mov	x0, x20
   2f514:	mov	x2, x27
   2f518:	mov	x4, x21
   2f51c:	bl	ccd0 <__gmpn_mul@plt>
   2f520:	ldur	x8, [x29, #-24]
   2f524:	mov	x2, x20
   2f528:	mov	x3, x27
   2f52c:	add	x22, x8, #0x8
   2f530:	mov	x0, x22
   2f534:	mov	x1, x22
   2f538:	bl	c2d0 <__gmpn_sub_n@plt>
   2f53c:	lsl	x21, x27, #3
   2f540:	ldr	x8, [x22, x21]
   2f544:	lsl	x20, x19, #4
   2f548:	subs	x8, x8, x0
   2f54c:	str	x8, [x22, x21]
   2f550:	b.cs	2f574 <__gmpn_sqrtrem@@Base+0x960>  // b.hs, b.nlast
   2f554:	ldur	x9, [x29, #-40]
   2f558:	sub	x8, x20, x23, lsl #3
   2f55c:	add	x8, x8, x9
   2f560:	add	x8, x8, #0x18
   2f564:	ldr	x9, [x8]
   2f568:	sub	x10, x9, #0x1
   2f56c:	str	x10, [x8], #8
   2f570:	cbz	x9, 2f564 <__gmpn_sqrtrem@@Base+0x950>
   2f574:	ldur	x10, [x29, #-40]
   2f578:	ldur	w13, [x29, #-28]
   2f57c:	ldur	x28, [x29, #-56]
   2f580:	mov	x8, xzr
   2f584:	add	x9, x10, x19, lsl #3
   2f588:	add	x10, x10, x19, lsl #4
   2f58c:	sub	x9, x9, #0x8
   2f590:	add	x10, x10, #0x8
   2f594:	add	x11, x23, x8
   2f598:	cmp	x11, #0x1
   2f59c:	b.lt	2f62c <__gmpn_sqrtrem@@Base+0xa18>  // b.tstop
   2f5a0:	lsl	x11, x8, #3
   2f5a4:	ldr	x12, [x10, x11]
   2f5a8:	ldr	x11, [x9, x11]
   2f5ac:	sub	x8, x8, #0x1
   2f5b0:	cmp	x12, x11
   2f5b4:	b.eq	2f594 <__gmpn_sqrtrem@@Base+0x980>  // b.none
   2f5b8:	b.hi	2f62c <__gmpn_sqrtrem@@Base+0xa18>  // b.pmore
   2f5bc:	ldur	x3, [x29, #-16]
   2f5c0:	mov	x0, x22
   2f5c4:	mov	x1, x22
   2f5c8:	mov	x2, x24
   2f5cc:	bl	cc40 <__gmpn_addlsh1_n@plt>
   2f5d0:	ldr	x8, [x22, x21]
   2f5d4:	adds	x8, x8, x0
   2f5d8:	str	x8, [x22, x21]
   2f5dc:	b.cc	2f614 <__gmpn_sqrtrem@@Base+0xa00>  // b.lo, b.ul, b.last
   2f5e0:	ldur	x9, [x29, #-40]
   2f5e4:	sub	x8, x20, x23, lsl #3
   2f5e8:	add	x8, x9, x8
   2f5ec:	mov	w9, #0x3                   	// #3
   2f5f0:	sub	x10, x9, #0x2
   2f5f4:	cmp	x10, x23
   2f5f8:	b.ge	2f614 <__gmpn_sqrtrem@@Base+0xa00>  // b.tcont
   2f5fc:	lsl	x10, x9, #3
   2f600:	ldr	x11, [x8, x10]
   2f604:	add	x9, x9, #0x1
   2f608:	adds	x11, x11, #0x1
   2f60c:	str	x11, [x8, x10]
   2f610:	b.cs	2f5f0 <__gmpn_sqrtrem@@Base+0x9dc>  // b.hs, b.nlast
   2f614:	ldur	w13, [x29, #-28]
   2f618:	mov	x8, x26
   2f61c:	ldr	x9, [x8]
   2f620:	sub	x10, x9, #0x1
   2f624:	str	x10, [x8], #8
   2f628:	cbz	x9, 2f61c <__gmpn_sqrtrem@@Base+0xa08>
   2f62c:	ldur	x10, [x29, #-40]
   2f630:	ldp	x21, x8, [x29, #-24]
   2f634:	sub	x9, x20, x23, lsl #3
   2f638:	add	x9, x9, x10
   2f63c:	sub	x8, x8, x23
   2f640:	add	x9, x9, #0x8
   2f644:	ldr	x10, [x9]
   2f648:	cbnz	x10, 2f720 <__gmpn_sqrtrem@@Base+0xb0c>
   2f64c:	sub	x8, x8, #0x1
   2f650:	sub	x9, x9, #0x8
   2f654:	cbnz	x8, 2f644 <__gmpn_sqrtrem@@Base+0xa30>
   2f658:	ldur	x20, [x29, #-40]
   2f65c:	mov	x1, x26
   2f660:	mov	x2, x23
   2f664:	mov	x0, x20
   2f668:	bl	c8e0 <__gmpn_sqr@plt>
   2f66c:	ldur	x10, [x29, #-48]
   2f670:	ldur	w13, [x29, #-28]
   2f674:	add	x9, x20, x23, lsl #4
   2f678:	mov	x8, xzr
   2f67c:	add	x10, x10, x19, lsl #3
   2f680:	add	x10, x10, x20
   2f684:	sub	x9, x9, #0x8
   2f688:	add	x10, x10, #0x8
   2f68c:	add	x11, x23, x8
   2f690:	cmp	x11, #0x1
   2f694:	b.lt	2f6b4 <__gmpn_sqrtrem@@Base+0xaa0>  // b.tstop
   2f698:	lsl	x11, x8, #3
   2f69c:	ldr	x12, [x10, x11]
   2f6a0:	ldr	x11, [x9, x11]
   2f6a4:	sub	x8, x8, #0x1
   2f6a8:	cmp	x12, x11
   2f6ac:	b.eq	2f68c <__gmpn_sqrtrem@@Base+0xa78>  // b.none
   2f6b0:	b	2f708 <__gmpn_sqrtrem@@Base+0xaf4>
   2f6b4:	cbz	w25, 2f6d4 <__gmpn_sqrtrem@@Base+0xac0>
   2f6b8:	lsl	w3, w25, #1
   2f6bc:	mov	x0, x21
   2f6c0:	mov	x1, x28
   2f6c4:	mov	x2, x23
   2f6c8:	bl	c180 <__gmpn_lshift@plt>
   2f6cc:	ldur	w13, [x29, #-28]
   2f6d0:	b	2f6d8 <__gmpn_sqrtrem@@Base+0xac4>
   2f6d4:	mov	x21, x28
   2f6d8:	ldur	x8, [x29, #-40]
   2f6dc:	sub	x10, x23, w13, uxtw
   2f6e0:	sub	x9, x21, #0x8
   2f6e4:	add	x8, x8, x23, lsl #3
   2f6e8:	sub	x8, x8, #0x8
   2f6ec:	subs	x11, x10, #0x1
   2f6f0:	b.lt	2f730 <__gmpn_sqrtrem@@Base+0xb1c>  // b.tstop
   2f6f4:	ldr	x10, [x9, x10, lsl #3]
   2f6f8:	ldr	x12, [x8], #-8
   2f6fc:	cmp	x10, x12
   2f700:	mov	x10, x11
   2f704:	b.eq	2f6ec <__gmpn_sqrtrem@@Base+0xad8>  // b.none
   2f708:	b.hi	2f720 <__gmpn_sqrtrem@@Base+0xb0c>  // b.pmore
   2f70c:	mov	x8, x26
   2f710:	ldr	x9, [x8]
   2f714:	sub	x10, x9, #0x1
   2f718:	str	x10, [x8], #8
   2f71c:	cbz	x9, 2f710 <__gmpn_sqrtrem@@Base+0xafc>
   2f720:	mov	w22, #0x1                   	// #1
   2f724:	ldur	x0, [x29, #-8]
   2f728:	cbz	x0, 2f4b8 <__gmpn_sqrtrem@@Base+0x8a4>
   2f72c:	b	2f43c <__gmpn_sqrtrem@@Base+0x828>
   2f730:	mov	w22, wzr
   2f734:	ldur	x0, [x29, #-8]
   2f738:	cbz	x0, 2f4b8 <__gmpn_sqrtrem@@Base+0x8a4>
   2f73c:	b	2f43c <__gmpn_sqrtrem@@Base+0x828>
   2f740:	sub	x0, x29, #0x8
   2f744:	mov	w22, w4
   2f748:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2f74c:	mov	w4, w22
   2f750:	mov	x25, x0
   2f754:	b	2f318 <__gmpn_sqrtrem@@Base+0x704>
   2f758:	sub	sp, sp, #0x90
   2f75c:	cmp	x2, #0x0
   2f760:	cinc	x15, x2, lt  // lt = tstop
   2f764:	stp	x24, x23, [sp, #96]
   2f768:	asr	x24, x15, #1
   2f76c:	stp	x28, x27, [sp, #64]
   2f770:	sub	x27, x2, x24
   2f774:	stp	x29, x30, [sp, #48]
   2f778:	stp	x26, x25, [sp, #80]
   2f77c:	stp	x22, x21, [sp, #112]
   2f780:	stp	x20, x19, [sp, #128]
   2f784:	add	x29, sp, #0x30
   2f788:	mov	x19, x2
   2f78c:	mov	x21, x1
   2f790:	mov	x20, x0
   2f794:	add	x23, x0, x24, lsl #3
   2f798:	cmp	x27, #0x1
   2f79c:	add	x25, x1, x24, lsl #4
   2f7a0:	stur	x3, [x29, #-8]
   2f7a4:	str	x15, [sp, #24]
   2f7a8:	b.ne	2f8a4 <__gmpn_sqrtrem@@Base+0xc90>  // b.any
   2f7ac:	ldp	x9, x8, [x25]
   2f7b0:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   2f7b4:	add	x10, x10, #0x7f4
   2f7b8:	mov	x11, #0x1ffff00000000       	// #562945658454016
   2f7bc:	lsr	x12, x8, #55
   2f7c0:	sub	w12, w12, #0x80
   2f7c4:	ldrb	w10, [x10, w12, uxtw]
   2f7c8:	lsr	x13, x8, #31
   2f7cc:	movk	x11, #0xfffd, lsl #16
   2f7d0:	lsr	x14, x8, #24
   2f7d4:	orr	x10, x10, #0x100
   2f7d8:	mul	x13, x10, x13
   2f7dc:	msub	x11, x13, x10, x11
   2f7e0:	asr	x11, x11, #16
   2f7e4:	mul	x11, x11, x10
   2f7e8:	asr	x11, x11, #18
   2f7ec:	add	x10, x11, x10, lsl #16
   2f7f0:	mul	x11, x10, x14
   2f7f4:	lsl	x13, x8, #14
   2f7f8:	lsr	x14, x11, #25
   2f7fc:	mov	x12, #0xffffff0000000000    	// #-1099511627776
   2f800:	msub	x13, x14, x14, x13
   2f804:	add	x12, x13, x12
   2f808:	asr	x12, x12, #24
   2f80c:	mul	x10, x12, x10
   2f810:	add	x10, x11, x10, asr #15
   2f814:	lsr	x10, x10, #32
   2f818:	mul	x11, x10, x10
   2f81c:	lsl	x12, x10, #1
   2f820:	mov	w14, #0x1                   	// #1
   2f824:	sub	x13, x8, #0x1
   2f828:	add	x12, x11, x12
   2f82c:	bfi	x14, x10, #1, #32
   2f830:	cmp	x12, x13
   2f834:	sub	x8, x8, x11
   2f838:	cinc	x11, x10, ls  // ls = plast
   2f83c:	csneg	x10, xzr, x14, hi  // hi = pmore
   2f840:	add	x8, x10, x8
   2f844:	str	x8, [x25]
   2f848:	extr	x8, x8, x9, #33
   2f84c:	udiv	x10, x8, x11
   2f850:	sub	x12, x10, x10, lsr #32
   2f854:	msub	x8, x11, x12, x8
   2f858:	lsr	x13, x8, #31
   2f85c:	bfi	x9, x8, #33, #31
   2f860:	mul	x8, x12, x12
   2f864:	subs	x10, x9, x8
   2f868:	cset	w8, cc  // cc = lo, ul, last
   2f86c:	subs	w9, w13, w8
   2f870:	orr	x8, x12, x11, lsl #32
   2f874:	mov	x22, x4
   2f878:	b.pl	2f890 <__gmpn_sqrtrem@@Base+0xc7c>  // b.nfrst
   2f87c:	adds	x10, x10, x8
   2f880:	sub	x8, x8, #0x1
   2f884:	cinc	w9, w9, cs  // cs = hs, nlast
   2f888:	adds	x10, x10, x8
   2f88c:	cinc	w9, w9, cs  // cs = hs, nlast
   2f890:	str	x10, [x25]
   2f894:	str	x8, [x23]
   2f898:	sxtw	x26, w9
   2f89c:	cbnz	x26, 2f8c4 <__gmpn_sqrtrem@@Base+0xcb0>
   2f8a0:	b	2f8d8 <__gmpn_sqrtrem@@Base+0xcc4>
   2f8a4:	mov	x0, x23
   2f8a8:	mov	x1, x25
   2f8ac:	mov	x2, x27
   2f8b0:	mov	x3, xzr
   2f8b4:	mov	x22, x4
   2f8b8:	bl	2f758 <__gmpn_sqrtrem@@Base+0xb44>
   2f8bc:	mov	x26, x0
   2f8c0:	cbz	x26, 2f8d8 <__gmpn_sqrtrem@@Base+0xcc4>
   2f8c4:	mov	x0, x25
   2f8c8:	mov	x1, x25
   2f8cc:	mov	x2, x23
   2f8d0:	mov	x3, x27
   2f8d4:	bl	c2d0 <__gmpn_sub_n@plt>
   2f8d8:	mov	x6, x27
   2f8dc:	lsl	x27, x24, #3
   2f8e0:	add	x28, x21, x27
   2f8e4:	mov	x0, x22
   2f8e8:	mov	x1, x28
   2f8ec:	mov	x2, xzr
   2f8f0:	mov	x3, x28
   2f8f4:	mov	x4, x19
   2f8f8:	mov	x5, x23
   2f8fc:	str	x25, [sp, #8]
   2f900:	stur	x19, [x29, #-16]
   2f904:	str	x6, [sp, #16]
   2f908:	bl	bf00 <__gmpn_tdiv_qr@plt>
   2f90c:	ldr	x8, [x22, x27]
   2f910:	ldr	x25, [x22]
   2f914:	mov	w3, #0x1                   	// #1
   2f918:	mov	x0, x20
   2f91c:	mov	x1, x22
   2f920:	mov	x2, x24
   2f924:	add	x19, x8, x26
   2f928:	mov	w26, #0x1                   	// #1
   2f92c:	bl	c1a0 <__gmpn_rshift@plt>
   2f930:	add	x8, x27, x20
   2f934:	ldur	x9, [x8, #-8]
   2f938:	orr	x9, x9, x19, lsl #63
   2f93c:	stur	x9, [x8, #-8]
   2f940:	ldr	x8, [x20]
   2f944:	ldur	x9, [x29, #-8]
   2f948:	tst	x8, x9
   2f94c:	b.ne	2fac0 <__gmpn_sqrtrem@@Base+0xeac>  // b.any
   2f950:	ldr	x8, [sp, #24]
   2f954:	and	x26, x8, #0xfffffffffffffffe
   2f958:	lsr	x8, x19, #1
   2f95c:	stur	x8, [x29, #-8]
   2f960:	tbnz	w25, #0, 2f974 <__gmpn_sqrtrem@@Base+0xd60>
   2f964:	ldur	x19, [x29, #-16]
   2f968:	ldr	x22, [sp, #16]
   2f96c:	mov	x28, xzr
   2f970:	b	2f994 <__gmpn_sqrtrem@@Base+0xd80>
   2f974:	ldr	x22, [sp, #16]
   2f978:	mov	x0, x28
   2f97c:	mov	x1, x28
   2f980:	mov	x2, x23
   2f984:	mov	x3, x22
   2f988:	bl	ca70 <__gmpn_add_n@plt>
   2f98c:	ldur	x19, [x29, #-16]
   2f990:	sxtw	x28, w0
   2f994:	add	x27, x21, x19, lsl #3
   2f998:	mov	x0, x27
   2f99c:	mov	x1, x20
   2f9a0:	mov	x2, x24
   2f9a4:	bl	c8e0 <__gmpn_sqr@plt>
   2f9a8:	mov	x0, x21
   2f9ac:	mov	x1, x21
   2f9b0:	mov	x2, x27
   2f9b4:	mov	x3, x26
   2f9b8:	bl	c2d0 <__gmpn_sub_n@plt>
   2f9bc:	ldur	x11, [x29, #-8]
   2f9c0:	cmp	x24, x22
   2f9c4:	add	w8, w0, w11
   2f9c8:	sxtw	x8, w8
   2f9cc:	b.eq	2f9e4 <__gmpn_sqrtrem@@Base+0xdd0>  // b.none
   2f9d0:	ldr	x10, [sp, #8]
   2f9d4:	ldr	x9, [x10]
   2f9d8:	subs	x8, x9, x8
   2f9dc:	str	x8, [x10]
   2f9e0:	cset	w8, cc  // cc = lo, ul, last
   2f9e4:	sub	x24, x28, x8
   2f9e8:	tbz	w24, #31, 2fabc <__gmpn_sqrtrem@@Base+0xea8>
   2f9ec:	ldr	x8, [x23]
   2f9f0:	adds	x8, x8, x11
   2f9f4:	str	x8, [x23]
   2f9f8:	b.cc	2fa24 <__gmpn_sqrtrem@@Base+0xe10>  // b.lo, b.ul, b.last
   2f9fc:	mov	w8, #0x1                   	// #1
   2fa00:	mov	w25, #0x2                   	// #2
   2fa04:	cmp	x8, x22
   2fa08:	b.ge	2fa28 <__gmpn_sqrtrem@@Base+0xe14>  // b.tcont
   2fa0c:	lsl	x9, x8, #3
   2fa10:	ldr	x10, [x23, x9]
   2fa14:	add	x8, x8, #0x1
   2fa18:	adds	x10, x10, #0x1
   2fa1c:	str	x10, [x23, x9]
   2fa20:	b.cs	2fa04 <__gmpn_sqrtrem@@Base+0xdf0>  // b.hs, b.nlast
   2fa24:	mov	x25, xzr
   2fa28:	mov	x0, x21
   2fa2c:	mov	x1, x21
   2fa30:	mov	x2, x20
   2fa34:	mov	x3, x19
   2fa38:	bl	cc40 <__gmpn_addlsh1_n@plt>
   2fa3c:	ldr	x9, [x21]
   2fa40:	add	x8, x25, x24
   2fa44:	add	x8, x8, x0
   2fa48:	sub	x10, x9, #0x1
   2fa4c:	str	x10, [x21]
   2fa50:	cbnz	x9, 2fa7c <__gmpn_sqrtrem@@Base+0xe68>
   2fa54:	mov	w9, #0x1                   	// #1
   2fa58:	mov	w10, #0x1                   	// #1
   2fa5c:	cmp	x10, x19
   2fa60:	b.ge	2fa80 <__gmpn_sqrtrem@@Base+0xe6c>  // b.tcont
   2fa64:	lsl	x11, x10, #3
   2fa68:	ldr	x12, [x21, x11]
   2fa6c:	add	x10, x10, #0x1
   2fa70:	sub	x13, x12, #0x1
   2fa74:	str	x13, [x21, x11]
   2fa78:	cbz	x12, 2fa5c <__gmpn_sqrtrem@@Base+0xe48>
   2fa7c:	mov	x9, xzr
   2fa80:	ldr	x10, [x20]
   2fa84:	sxtw	x8, w8
   2fa88:	sub	x24, x8, x9
   2fa8c:	sub	x8, x10, #0x1
   2fa90:	str	x8, [x20]
   2fa94:	cbnz	x10, 2fabc <__gmpn_sqrtrem@@Base+0xea8>
   2fa98:	mov	w8, #0x1                   	// #1
   2fa9c:	cmp	x8, x19
   2faa0:	b.ge	2fabc <__gmpn_sqrtrem@@Base+0xea8>  // b.tcont
   2faa4:	lsl	x9, x8, #3
   2faa8:	ldr	x10, [x20, x9]
   2faac:	add	x8, x8, #0x1
   2fab0:	sub	x11, x10, #0x1
   2fab4:	str	x11, [x20, x9]
   2fab8:	cbz	x10, 2fa9c <__gmpn_sqrtrem@@Base+0xe88>
   2fabc:	sxtw	x26, w24
   2fac0:	mov	x0, x26
   2fac4:	ldp	x20, x19, [sp, #128]
   2fac8:	ldp	x22, x21, [sp, #112]
   2facc:	ldp	x24, x23, [sp, #96]
   2fad0:	ldp	x26, x25, [sp, #80]
   2fad4:	ldp	x28, x27, [sp, #64]
   2fad8:	ldp	x29, x30, [sp, #48]
   2fadc:	add	sp, sp, #0x90
   2fae0:	ret
   2fae4:	stp	x29, x30, [sp, #-80]!
   2fae8:	stp	x26, x25, [sp, #16]
   2faec:	stp	x24, x23, [sp, #32]
   2faf0:	stp	x22, x21, [sp, #48]
   2faf4:	stp	x20, x19, [sp, #64]
   2faf8:	mov	x29, sp
   2fafc:	sub	sp, sp, #0x10
   2fb00:	mov	x21, x0
   2fb04:	mov	x0, x5
   2fb08:	mov	x24, x5
   2fb0c:	mov	x19, x4
   2fb10:	mov	x22, x3
   2fb14:	mov	x20, x2
   2fb18:	mov	x23, x1
   2fb1c:	bl	ca50 <__gmpn_copyi@plt>
   2fb20:	add	x26, x22, x19, lsl #3
   2fb24:	ldur	x25, [x26, #-8]
   2fb28:	mov	x0, x25
   2fb2c:	bl	d3f0 <__gmpn_invert_limb@plt>
   2fb30:	ldur	x8, [x26, #-16]
   2fb34:	mul	x9, x0, x25
   2fb38:	adds	x9, x9, x8
   2fb3c:	b.cc	2fb58 <__gmpn_sqrtrem@@Base+0xf44>  // b.lo, b.ul, b.last
   2fb40:	subs	x9, x9, x25
   2fb44:	cset	w10, cs  // cs = hs, nlast
   2fb48:	csel	x11, x25, xzr, cs  // cs = hs, nlast
   2fb4c:	mvn	x10, x10
   2fb50:	add	x0, x10, x0
   2fb54:	sub	x9, x9, x11
   2fb58:	umulh	x10, x8, x0
   2fb5c:	adds	x9, x10, x9
   2fb60:	b.cc	2fb88 <__gmpn_sqrtrem@@Base+0xf74>  // b.lo, b.ul, b.last
   2fb64:	cmp	x9, x25
   2fb68:	sub	x5, x0, #0x1
   2fb6c:	b.cc	2fb8c <__gmpn_sqrtrem@@Base+0xf78>  // b.lo, b.ul, b.last
   2fb70:	mul	x10, x0, x8
   2fb74:	cmp	x9, x25
   2fb78:	sub	x11, x0, #0x2
   2fb7c:	ccmp	x10, x8, #0x2, ls  // ls = plast
   2fb80:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   2fb84:	b	2fb8c <__gmpn_sqrtrem@@Base+0xf78>
   2fb88:	mov	x5, x0
   2fb8c:	cmp	x19, #0x97
   2fb90:	stur	x5, [x29, #-8]
   2fb94:	b.le	2fc08 <__gmpn_sqrtrem@@Base+0xff4>
   2fb98:	cmp	x19, #0x3e5
   2fb9c:	b.le	2fc24 <__gmpn_sqrtrem@@Base+0x1010>
   2fba0:	mov	x0, x20
   2fba4:	mov	x1, x19
   2fba8:	mov	w2, wzr
   2fbac:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   2fbb0:	lsl	x1, x0, #3
   2fbb4:	mov	w8, #0x7f00                	// #32512
   2fbb8:	cmp	x1, x8
   2fbbc:	stur	xzr, [x29, #-16]
   2fbc0:	b.hi	2fc68 <__gmpn_sqrtrem@@Base+0x1054>  // b.pmore
   2fbc4:	add	x9, x1, #0xf
   2fbc8:	mov	x8, sp
   2fbcc:	and	x9, x9, #0xfffffffffffffff0
   2fbd0:	sub	x5, x8, x9
   2fbd4:	mov	sp, x5
   2fbd8:	mov	x0, x21
   2fbdc:	mov	x1, x23
   2fbe0:	mov	x2, x20
   2fbe4:	mov	x3, x22
   2fbe8:	mov	x4, x19
   2fbec:	bl	c710 <__gmpn_mu_divappr_q@plt>
   2fbf0:	ldur	x8, [x29, #-16]
   2fbf4:	mov	x22, x0
   2fbf8:	cbz	x8, 2fc44 <__gmpn_sqrtrem@@Base+0x1030>
   2fbfc:	mov	x0, x8
   2fc00:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2fc04:	b	2fc44 <__gmpn_sqrtrem@@Base+0x1030>
   2fc08:	mov	x0, x21
   2fc0c:	mov	x1, x24
   2fc10:	mov	x2, x20
   2fc14:	mov	x3, x22
   2fc18:	mov	x4, x19
   2fc1c:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   2fc20:	b	2fc40 <__gmpn_sqrtrem@@Base+0x102c>
   2fc24:	sub	x5, x29, #0x8
   2fc28:	mov	x0, x21
   2fc2c:	mov	x1, x24
   2fc30:	mov	x2, x20
   2fc34:	mov	x3, x22
   2fc38:	mov	x4, x19
   2fc3c:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   2fc40:	mov	x22, x0
   2fc44:	sub	x8, x20, x19
   2fc48:	str	x22, [x21, x8, lsl #3]
   2fc4c:	mov	sp, x29
   2fc50:	ldp	x20, x19, [sp, #64]
   2fc54:	ldp	x22, x21, [sp, #48]
   2fc58:	ldp	x24, x23, [sp, #32]
   2fc5c:	ldp	x26, x25, [sp, #16]
   2fc60:	ldp	x29, x30, [sp], #80
   2fc64:	ret
   2fc68:	sub	x0, x29, #0x10
   2fc6c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2fc70:	mov	x5, x0
   2fc74:	b	2fbd8 <__gmpn_sqrtrem@@Base+0xfc4>

000000000002fc78 <__gmpn_sizeinbase@@Base>:
   2fc78:	cbz	x1, 2fcc4 <__gmpn_sizeinbase@@Base+0x4c>
   2fc7c:	add	x8, x0, x1, lsl #3
   2fc80:	ldur	x8, [x8, #-8]
   2fc84:	adrp	x11, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2fc88:	ldr	x11, [x11, #3936]
   2fc8c:	lsl	x9, x1, #6
   2fc90:	sub	w10, w2, #0x1
   2fc94:	clz	x8, x8
   2fc98:	tst	w2, w10
   2fc9c:	sub	x8, x9, x8
   2fca0:	sxtw	x9, w2
   2fca4:	mov	w10, #0x28                  	// #40
   2fca8:	madd	x9, x9, x10, x11
   2fcac:	b.ne	2fccc <__gmpn_sizeinbase@@Base+0x54>  // b.any
   2fcb0:	ldrsw	x9, [x9, #24]
   2fcb4:	add	x8, x8, x9
   2fcb8:	sub	x8, x8, #0x1
   2fcbc:	udiv	x0, x8, x9
   2fcc0:	ret
   2fcc4:	mov	w0, #0x1                   	// #1
   2fcc8:	ret
   2fccc:	ldr	x9, [x9, #8]
   2fcd0:	add	x9, x9, #0x1
   2fcd4:	umulh	x8, x9, x8
   2fcd8:	add	x0, x8, #0x1
   2fcdc:	ret

000000000002fce0 <__gmpn_get_str@@Base>:
   2fce0:	stp	x29, x30, [sp, #-80]!
   2fce4:	stp	x28, x25, [sp, #16]
   2fce8:	stp	x24, x23, [sp, #32]
   2fcec:	stp	x22, x21, [sp, #48]
   2fcf0:	stp	x20, x19, [sp, #64]
   2fcf4:	mov	x29, sp
   2fcf8:	sub	sp, sp, #0xa10
   2fcfc:	mov	x19, x0
   2fd00:	cbz	x3, 2fdc4 <__gmpn_get_str@@Base+0xe4>
   2fd04:	sub	w8, w1, #0x1
   2fd08:	mov	x21, x3
   2fd0c:	mov	x20, x2
   2fd10:	mov	w22, w1
   2fd14:	tst	w1, w8
   2fd18:	b.ne	2fdd0 <__gmpn_get_str@@Base+0xf0>  // b.any
   2fd1c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2fd20:	ldr	x9, [x9, #3936]
   2fd24:	mov	w11, #0x28                  	// #40
   2fd28:	sub	x8, x21, #0x1
   2fd2c:	ldr	x10, [x20, x8, lsl #3]
   2fd30:	smaddl	x9, w22, w11, x9
   2fd34:	ldr	x9, [x9, #24]
   2fd38:	lsl	x12, x21, #6
   2fd3c:	clz	x13, x10
   2fd40:	sub	x12, x12, x13
   2fd44:	sxtw	x13, w9
   2fd48:	udiv	x13, x12, x13
   2fd4c:	msub	w13, w13, w9, w12
   2fd50:	mov	w11, #0xffffffff            	// #-1
   2fd54:	cmp	w13, #0x0
   2fd58:	sub	w13, w9, w13
   2fd5c:	lsl	w11, w11, w9
   2fd60:	sub	w12, w12, w8, lsl #6
   2fd64:	csel	w13, wzr, w13, eq  // eq = none
   2fd68:	add	w14, w12, w13
   2fd6c:	eor	w12, w11, #0xff
   2fd70:	mov	x11, x19
   2fd74:	subs	w13, w14, w9
   2fd78:	b.mi	2fd90 <__gmpn_get_str@@Base+0xb0>  // b.first
   2fd7c:	lsr	x14, x10, x13
   2fd80:	and	w14, w14, w12
   2fd84:	subs	w13, w13, w9
   2fd88:	strb	w14, [x11], #1
   2fd8c:	b.pl	2fd7c <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   2fd90:	subs	x8, x8, #0x1
   2fd94:	b.lt	2fe6c <__gmpn_get_str@@Base+0x18c>  // b.tstop
   2fd98:	neg	w14, w13
   2fd9c:	lsl	x14, x10, x14
   2fda0:	ldr	x10, [x20, x8, lsl #3]
   2fda4:	and	w15, w14, w12
   2fda8:	add	w14, w13, #0x40
   2fdac:	lsr	x13, x10, x13
   2fdb0:	orr	w13, w13, w15
   2fdb4:	strb	w13, [x11], #1
   2fdb8:	subs	w13, w14, w9
   2fdbc:	b.pl	2fd7c <__gmpn_get_str@@Base+0x9c>  // b.nfrst
   2fdc0:	b	2fd90 <__gmpn_get_str@@Base+0xb0>
   2fdc4:	strb	wzr, [x19]
   2fdc8:	mov	w19, #0x1                   	// #1
   2fdcc:	b	2fe90 <__gmpn_get_str@@Base+0x1b0>
   2fdd0:	cmp	x21, #0x1c
   2fdd4:	b.le	2fe74 <__gmpn_get_str@@Base+0x194>
   2fdd8:	lsl	x23, x21, #3
   2fddc:	add	x1, x23, #0x400
   2fde0:	add	x0, sp, #0x8
   2fde4:	str	xzr, [sp, #8]
   2fde8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2fdec:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   2fdf0:	ldr	x8, [x8, #3936]
   2fdf4:	mov	w24, #0x28                  	// #40
   2fdf8:	lsl	x10, x21, #6
   2fdfc:	mov	x1, x0
   2fe00:	smaddl	x8, w22, w24, x8
   2fe04:	ldr	x9, [x8, #8]
   2fe08:	ldrsw	x8, [x8]
   2fe0c:	umulh	x9, x9, x10
   2fe10:	add	x0, sp, #0x10
   2fe14:	mov	w3, w22
   2fe18:	udiv	x8, x9, x8
   2fe1c:	add	x2, x8, #0x1
   2fe20:	add	x25, sp, #0x10
   2fe24:	bl	cb10 <__gmpn_compute_powtab@plt>
   2fe28:	mov	x22, x0
   2fe2c:	add	x1, x23, #0x200
   2fe30:	add	x0, sp, #0x8
   2fe34:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   2fe38:	mov	x5, x0
   2fe3c:	smaddl	x4, w22, w24, x25
   2fe40:	mov	x0, x19
   2fe44:	mov	x1, xzr
   2fe48:	mov	x2, x20
   2fe4c:	mov	x3, x21
   2fe50:	bl	302e4 <__gmpn_get_str@@Base+0x604>
   2fe54:	ldr	x8, [sp, #8]
   2fe58:	sub	x19, x0, x19
   2fe5c:	cbz	x8, 2fe90 <__gmpn_get_str@@Base+0x1b0>
   2fe60:	mov	x0, x8
   2fe64:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   2fe68:	b	2fe90 <__gmpn_get_str@@Base+0x1b0>
   2fe6c:	sub	x19, x11, x19
   2fe70:	b	2fe90 <__gmpn_get_str@@Base+0x1b0>
   2fe74:	mov	x0, x19
   2fe78:	mov	x1, xzr
   2fe7c:	mov	x2, x20
   2fe80:	mov	x3, x21
   2fe84:	mov	w4, w22
   2fe88:	bl	2feb0 <__gmpn_get_str@@Base+0x1d0>
   2fe8c:	sub	x19, x0, x19
   2fe90:	mov	x0, x19
   2fe94:	add	sp, sp, #0xa10
   2fe98:	ldp	x20, x19, [sp, #64]
   2fe9c:	ldp	x22, x21, [sp, #48]
   2fea0:	ldp	x24, x23, [sp, #32]
   2fea4:	ldp	x28, x25, [sp, #16]
   2fea8:	ldp	x29, x30, [sp], #80
   2feac:	ret
   2feb0:	stp	x29, x30, [sp, #-96]!
   2feb4:	stp	x28, x27, [sp, #16]
   2feb8:	stp	x26, x25, [sp, #32]
   2febc:	stp	x24, x23, [sp, #48]
   2fec0:	stp	x22, x21, [sp, #64]
   2fec4:	stp	x20, x19, [sp, #80]
   2fec8:	mov	x29, sp
   2fecc:	sub	sp, sp, #0x5a0
   2fed0:	mov	x21, x3
   2fed4:	mov	x20, x1
   2fed8:	cmp	w4, #0xa
   2fedc:	mov	x19, x0
   2fee0:	b.ne	30100 <__gmpn_get_str@@Base+0x420>  // b.any
   2fee4:	add	x23, sp, #0x10
   2fee8:	add	x22, x23, #0x8
   2feec:	mov	x0, x22
   2fef0:	mov	x1, x2
   2fef4:	mov	x2, x21
   2fef8:	bl	ca50 <__gmpn_copyi@plt>
   2fefc:	add	x8, sp, #0xf8
   2ff00:	cmp	x21, #0x2
   2ff04:	add	x26, x8, #0x49d
   2ff08:	b.lt	300cc <__gmpn_get_str@@Base+0x3ec>  // b.tstop
   2ff0c:	mov	w24, #0xa                   	// #10
   2ff10:	mov	w25, #0x64                  	// #100
   2ff14:	mov	w27, #0x3e8                 	// #1000
   2ff18:	mov	w28, #0x2710                	// #10000
   2ff1c:	mov	x5, #0xc34a                	// #49994
   2ff20:	mov	x4, #0x89e80000            	// #2313682944
   2ff24:	movk	x5, #0x6d2a, lsl #16
   2ff28:	movk	x4, #0x2304, lsl #32
   2ff2c:	movk	x5, #0x94fb, lsl #32
   2ff30:	add	x0, sp, #0x10
   2ff34:	mov	w1, #0x1                   	// #1
   2ff38:	movk	x4, #0x8ac7, lsl #48
   2ff3c:	movk	x5, #0xd83c, lsl #48
   2ff40:	mov	x2, x22
   2ff44:	mov	x3, x21
   2ff48:	mov	w6, wzr
   2ff4c:	bl	cce0 <__gmpn_preinv_divrem_1@plt>
   2ff50:	ldr	x8, [x23, x21, lsl #3]
   2ff54:	ldr	x9, [sp, #16]
   2ff58:	cmp	x8, #0x0
   2ff5c:	add	x8, x9, #0x1
   2ff60:	umulh	x11, x8, x24
   2ff64:	cset	w9, eq  // eq = none
   2ff68:	strb	w11, [x26, #-19]!
   2ff6c:	mul	x11, x8, x25
   2ff70:	add	x10, x8, x8, lsl #2
   2ff74:	sub	x21, x21, x9
   2ff78:	mul	x9, x8, x27
   2ff7c:	mul	x8, x8, x28
   2ff80:	umulh	x11, x11, x24
   2ff84:	strb	w11, [x26, #2]
   2ff88:	lsr	x11, x8, #2
   2ff8c:	umulh	x9, x9, x24
   2ff90:	add	x8, x11, x8, lsr #4
   2ff94:	strb	w9, [x26, #3]
   2ff98:	lsl	x9, x8, #1
   2ff9c:	ubfx	x8, x8, #59, #4
   2ffa0:	strb	w8, [x26, #4]
   2ffa4:	and	x8, x9, #0xffffffffffffffe
   2ffa8:	add	x8, x8, x8, lsl #2
   2ffac:	lsl	x9, x8, #1
   2ffb0:	ubfx	x8, x8, #59, #4
   2ffb4:	strb	w8, [x26, #5]
   2ffb8:	and	x8, x9, #0xffffffffffffffc
   2ffbc:	add	x8, x8, x8, lsl #2
   2ffc0:	lsl	x9, x8, #1
   2ffc4:	ubfx	x8, x8, #59, #4
   2ffc8:	strb	w8, [x26, #6]
   2ffcc:	and	x8, x9, #0xffffffffffffff8
   2ffd0:	add	x8, x8, x8, lsl #2
   2ffd4:	lsl	x9, x8, #1
   2ffd8:	ubfx	x8, x8, #59, #4
   2ffdc:	strb	w8, [x26, #7]
   2ffe0:	and	x8, x9, #0xffffffffffffff0
   2ffe4:	add	x8, x8, x8, lsl #2
   2ffe8:	lsl	x9, x8, #1
   2ffec:	ubfx	x8, x8, #59, #4
   2fff0:	strb	w8, [x26, #8]
   2fff4:	and	x8, x9, #0xfffffffffffffe0
   2fff8:	add	x8, x8, x8, lsl #2
   2fffc:	lsl	x9, x8, #1
   30000:	ubfx	x8, x8, #59, #4
   30004:	strb	w8, [x26, #9]
   30008:	and	x8, x9, #0xfffffffffffffc0
   3000c:	add	x8, x8, x8, lsl #2
   30010:	lsl	x9, x8, #1
   30014:	ubfx	x8, x8, #59, #4
   30018:	strb	w8, [x26, #10]
   3001c:	and	x8, x9, #0xfffffffffffff80
   30020:	add	x8, x8, x8, lsl #2
   30024:	lsl	x9, x8, #1
   30028:	ubfx	x8, x8, #59, #4
   3002c:	strb	w8, [x26, #11]
   30030:	and	x8, x9, #0xfffffffffffff00
   30034:	add	x8, x8, x8, lsl #2
   30038:	lsl	x9, x8, #1
   3003c:	ubfx	x8, x8, #59, #4
   30040:	strb	w8, [x26, #12]
   30044:	and	x8, x9, #0xffffffffffffe00
   30048:	add	x8, x8, x8, lsl #2
   3004c:	lsl	x9, x8, #1
   30050:	ubfx	x8, x8, #59, #4
   30054:	strb	w8, [x26, #13]
   30058:	and	x8, x9, #0xffffffffffffc00
   3005c:	add	x8, x8, x8, lsl #2
   30060:	lsl	x9, x8, #1
   30064:	ubfx	x8, x8, #59, #4
   30068:	strb	w8, [x26, #14]
   3006c:	and	x8, x9, #0xffffffffffff800
   30070:	add	x8, x8, x8, lsl #2
   30074:	lsl	x9, x8, #1
   30078:	ubfx	x8, x8, #59, #4
   3007c:	strb	w8, [x26, #15]
   30080:	and	x8, x9, #0xffffffffffff000
   30084:	add	x8, x8, x8, lsl #2
   30088:	lsl	x9, x8, #1
   3008c:	ubfx	x8, x8, #59, #4
   30090:	strb	w8, [x26, #16]
   30094:	and	x8, x9, #0xfffffffffffe000
   30098:	add	x8, x8, x8, lsl #2
   3009c:	lsl	x9, x8, #1
   300a0:	ubfx	x8, x8, #59, #4
   300a4:	strb	w8, [x26, #17]
   300a8:	and	x8, x9, #0xfffffffffffc000
   300ac:	add	x8, x8, x8, lsl #2
   300b0:	lsl	x10, x10, #1
   300b4:	ubfx	x8, x8, #59, #4
   300b8:	cmp	x21, #0x1
   300bc:	umulh	x10, x10, x24
   300c0:	strb	w10, [x26, #1]
   300c4:	strb	w8, [x26, #18]
   300c8:	b.gt	2ff1c <__gmpn_get_str@@Base+0x23c>
   300cc:	ldr	x8, [sp, #24]
   300d0:	cbz	x8, 301e0 <__gmpn_get_str@@Base+0x500>
   300d4:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   300d8:	movk	x9, #0xcccd
   300dc:	mov	w10, #0xfffffff6            	// #-10
   300e0:	umulh	x11, x8, x9
   300e4:	lsr	x11, x11, #3
   300e8:	madd	w12, w11, w10, w8
   300ec:	cmp	x8, #0xa
   300f0:	strb	w12, [x26, #-1]!
   300f4:	mov	x8, x11
   300f8:	b.cs	300e0 <__gmpn_get_str@@Base+0x400>  // b.hs, b.nlast
   300fc:	b	301e0 <__gmpn_get_str@@Base+0x500>
   30100:	add	x8, sp, #0x10
   30104:	add	x22, x8, #0x8
   30108:	mov	w23, w4
   3010c:	mov	x0, x22
   30110:	mov	x1, x2
   30114:	mov	x2, x21
   30118:	sxtw	x27, w23
   3011c:	bl	ca50 <__gmpn_copyi@plt>
   30120:	add	x8, sp, #0xf8
   30124:	cmp	x21, #0x2
   30128:	add	x26, x8, #0x49d
   3012c:	b.lt	301c0 <__gmpn_get_str@@Base+0x4e0>  // b.tstop
   30130:	str	x22, [sp, #8]
   30134:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   30138:	ldr	x8, [x8, #3936]
   3013c:	mov	w9, #0x28                  	// #40
   30140:	smaddl	x8, w23, w9, x8
   30144:	ldr	w28, [x8]
   30148:	ldp	x23, x8, [x8, #24]
   3014c:	neg	x22, x28
   30150:	clz	x25, x23
   30154:	neg	x24, x28, lsl #1
   30158:	str	x8, [sp]
   3015c:	ldp	x5, x2, [sp]
   30160:	add	x0, sp, #0x10
   30164:	mov	w1, #0x1                   	// #1
   30168:	mov	x3, x21
   3016c:	mov	x4, x23
   30170:	mov	w6, w25
   30174:	bl	cce0 <__gmpn_preinv_divrem_1@plt>
   30178:	add	x8, sp, #0x10
   3017c:	ldr	x9, [x8, x21, lsl #3]
   30180:	ldr	x10, [sp, #16]
   30184:	add	x8, x26, x22
   30188:	add	x26, x26, x24
   3018c:	cmp	x9, #0x0
   30190:	add	x10, x10, #0x1
   30194:	csetm	x9, eq  // eq = none
   30198:	mov	w11, w28
   3019c:	umulh	x12, x10, x27
   301a0:	mul	x10, x10, x27
   301a4:	subs	w11, w11, #0x1
   301a8:	strb	w12, [x8], #1
   301ac:	add	x26, x26, #0x1
   301b0:	b.ne	3019c <__gmpn_get_str@@Base+0x4bc>  // b.any
   301b4:	add	x21, x21, x9
   301b8:	cmp	x21, #0x1
   301bc:	b.gt	3015c <__gmpn_get_str@@Base+0x47c>
   301c0:	ldr	x8, [sp, #24]
   301c4:	cbz	x8, 301e0 <__gmpn_get_str@@Base+0x500>
   301c8:	udiv	x9, x8, x27
   301cc:	msub	w10, w9, w27, w8
   301d0:	cmp	x8, x27
   301d4:	strb	w10, [x26, #-1]!
   301d8:	mov	x8, x9
   301dc:	b.cs	301c8 <__gmpn_get_str@@Base+0x4e8>  // b.hs, b.nlast
   301e0:	add	x8, sp, #0xf8
   301e4:	add	x23, x8, #0x49d
   301e8:	sub	x22, x23, x26
   301ec:	cmp	x22, x20
   301f0:	b.cs	30244 <__gmpn_get_str@@Base+0x564>  // b.hs, b.nlast
   301f4:	add	x8, x26, x20
   301f8:	sub	x21, x8, x23
   301fc:	mov	x0, x19
   30200:	mov	w1, wzr
   30204:	mov	x2, x21
   30208:	bl	c5f0 <memset@plt>
   3020c:	cmp	x21, #0x1
   30210:	b.ls	30234 <__gmpn_get_str@@Base+0x554>  // b.plast
   30214:	and	x8, x21, #0xfffffffffffffffe
   30218:	add	x19, x19, x8
   3021c:	sub	x20, x20, x8
   30220:	mov	x9, x8
   30224:	subs	x9, x9, #0x2
   30228:	b.ne	30224 <__gmpn_get_str@@Base+0x544>  // b.any
   3022c:	cmp	x21, x8
   30230:	b.eq	30244 <__gmpn_get_str@@Base+0x564>  // b.none
   30234:	sub	x20, x20, #0x1
   30238:	cmp	x22, x20
   3023c:	add	x19, x19, #0x1
   30240:	b.cc	30234 <__gmpn_get_str@@Base+0x554>  // b.lo, b.ul, b.last
   30244:	cbz	x22, 30280 <__gmpn_get_str@@Base+0x5a0>
   30248:	cmp	x22, #0x1f
   3024c:	b.ls	30264 <__gmpn_get_str@@Base+0x584>  // b.plast
   30250:	cmp	x19, x23
   30254:	b.cs	30288 <__gmpn_get_str@@Base+0x5a8>  // b.hs, b.nlast
   30258:	add	x8, x19, x22
   3025c:	cmp	x26, x8
   30260:	b.cs	30288 <__gmpn_get_str@@Base+0x5a8>  // b.hs, b.nlast
   30264:	mov	x0, x19
   30268:	mov	x8, x22
   3026c:	ldrb	w9, [x26], #1
   30270:	subs	x8, x8, #0x1
   30274:	strb	w9, [x0], #1
   30278:	b.ne	3026c <__gmpn_get_str@@Base+0x58c>  // b.any
   3027c:	b	302c4 <__gmpn_get_str@@Base+0x5e4>
   30280:	mov	x0, x19
   30284:	b	302c4 <__gmpn_get_str@@Base+0x5e4>
   30288:	and	x9, x22, #0xffffffffffffffe0
   3028c:	and	x8, x22, #0x1f
   30290:	add	x10, x26, #0x10
   30294:	add	x0, x19, x9
   30298:	add	x26, x26, x9
   3029c:	add	x11, x19, #0x10
   302a0:	mov	x12, x9
   302a4:	ldp	q0, q1, [x10, #-16]
   302a8:	add	x10, x10, #0x20
   302ac:	subs	x12, x12, #0x20
   302b0:	stp	q0, q1, [x11, #-16]
   302b4:	add	x11, x11, #0x20
   302b8:	b.ne	302a4 <__gmpn_get_str@@Base+0x5c4>  // b.any
   302bc:	cmp	x22, x9
   302c0:	b.ne	3026c <__gmpn_get_str@@Base+0x58c>  // b.any
   302c4:	add	sp, sp, #0x5a0
   302c8:	ldp	x20, x19, [sp, #80]
   302cc:	ldp	x22, x21, [sp, #64]
   302d0:	ldp	x24, x23, [sp, #48]
   302d4:	ldp	x26, x25, [sp, #32]
   302d8:	ldp	x28, x27, [sp, #16]
   302dc:	ldp	x29, x30, [sp], #96
   302e0:	ret
   302e4:	stp	x29, x30, [sp, #-96]!
   302e8:	stp	x24, x23, [sp, #48]
   302ec:	stp	x22, x21, [sp, #64]
   302f0:	stp	x20, x19, [sp, #80]
   302f4:	mov	x24, x4
   302f8:	mov	x22, x3
   302fc:	mov	x21, x2
   30300:	mov	x20, x1
   30304:	cmp	x3, #0xf
   30308:	mov	x19, x0
   3030c:	str	x27, [sp, #16]
   30310:	stp	x26, x25, [sp, #32]
   30314:	mov	x29, sp
   30318:	b.lt	303f4 <__gmpn_get_str@@Base+0x714>  // b.tstop
   3031c:	mov	x23, x5
   30320:	sub	x26, x21, #0x8
   30324:	b	30390 <__gmpn_get_str@@Base+0x6b0>
   30328:	sub	x25, x22, x8
   3032c:	mov	x22, x10
   30330:	add	x1, x21, x8, lsl #3
   30334:	mov	x0, x23
   30338:	mov	x2, xzr
   3033c:	mov	x3, x1
   30340:	mov	x4, x25
   30344:	mov	x6, x24
   30348:	bl	bf00 <__gmpn_tdiv_qr@plt>
   3034c:	sub	x8, x25, x24
   30350:	ldr	x9, [x23, x8, lsl #3]
   30354:	cmp	x9, #0x0
   30358:	cinc	x3, x8, ne  // ne = any
   3035c:	cbz	x20, 303ec <__gmpn_get_str@@Base+0x70c>
   30360:	ldr	x8, [x27, #24]
   30364:	sub	x1, x20, x8
   30368:	sub	x24, x27, #0x28
   3036c:	add	x5, x23, x3, lsl #3
   30370:	mov	x0, x19
   30374:	mov	x2, x23
   30378:	mov	x4, x24
   3037c:	bl	302e4 <__gmpn_get_str@@Base+0x604>
   30380:	ldr	x20, [x27, #24]
   30384:	cmp	x22, #0xe
   30388:	mov	x19, x0
   3038c:	b.le	303f4 <__gmpn_get_str@@Base+0x714>
   30390:	add	x9, x26, x22, lsl #3
   30394:	mov	x27, x24
   30398:	b	303a0 <__gmpn_get_str@@Base+0x6c0>
   3039c:	sub	x27, x27, #0x28
   303a0:	ldp	x24, x8, [x27, #8]
   303a4:	add	x10, x8, x24
   303a8:	cmp	x10, x22
   303ac:	b.gt	3039c <__gmpn_get_str@@Base+0x6bc>
   303b0:	ldr	x5, [x27]
   303b4:	b.ne	30328 <__gmpn_get_str@@Base+0x648>  // b.any
   303b8:	sub	x25, x22, x8
   303bc:	sub	x10, x5, #0x8
   303c0:	mov	x11, x9
   303c4:	mov	x12, x25
   303c8:	subs	x13, x12, #0x1
   303cc:	b.lt	30330 <__gmpn_get_str@@Base+0x650>  // b.tstop
   303d0:	ldr	x14, [x11], #-8
   303d4:	ldr	x12, [x10, x12, lsl #3]
   303d8:	cmp	x14, x12
   303dc:	mov	x12, x13
   303e0:	b.eq	303c8 <__gmpn_get_str@@Base+0x6e8>  // b.none
   303e4:	b.ls	3039c <__gmpn_get_str@@Base+0x6bc>  // b.plast
   303e8:	b	30330 <__gmpn_get_str@@Base+0x650>
   303ec:	mov	x1, xzr
   303f0:	b	30368 <__gmpn_get_str@@Base+0x688>
   303f4:	cbz	x22, 30428 <__gmpn_get_str@@Base+0x748>
   303f8:	ldr	w4, [x24, #32]
   303fc:	mov	x0, x19
   30400:	mov	x1, x20
   30404:	mov	x2, x21
   30408:	mov	x3, x22
   3040c:	ldp	x20, x19, [sp, #80]
   30410:	ldp	x22, x21, [sp, #64]
   30414:	ldp	x24, x23, [sp, #48]
   30418:	ldp	x26, x25, [sp, #32]
   3041c:	ldr	x27, [sp, #16]
   30420:	ldp	x29, x30, [sp], #96
   30424:	b	2feb0 <__gmpn_get_str@@Base+0x1d0>
   30428:	cbz	x20, 30478 <__gmpn_get_str@@Base+0x798>
   3042c:	mov	x0, x19
   30430:	mov	w1, wzr
   30434:	mov	x2, x20
   30438:	bl	c5f0 <memset@plt>
   3043c:	cmp	x20, #0x1
   30440:	b.ne	3044c <__gmpn_get_str@@Base+0x76c>  // b.any
   30444:	mov	x8, x20
   30448:	b	3046c <__gmpn_get_str@@Base+0x78c>
   3044c:	and	x9, x20, #0xfffffffffffffffe
   30450:	add	x19, x19, x9
   30454:	and	x8, x20, #0x1
   30458:	mov	x10, x9
   3045c:	subs	x10, x10, #0x2
   30460:	b.ne	3045c <__gmpn_get_str@@Base+0x77c>  // b.any
   30464:	cmp	x20, x9
   30468:	b.eq	30478 <__gmpn_get_str@@Base+0x798>  // b.none
   3046c:	subs	x8, x8, #0x1
   30470:	add	x19, x19, #0x1
   30474:	b.ne	3046c <__gmpn_get_str@@Base+0x78c>  // b.any
   30478:	mov	x0, x19
   3047c:	ldp	x20, x19, [sp, #80]
   30480:	ldp	x22, x21, [sp, #64]
   30484:	ldp	x24, x23, [sp, #48]
   30488:	ldp	x26, x25, [sp, #32]
   3048c:	ldr	x27, [sp, #16]
   30490:	ldp	x29, x30, [sp], #96
   30494:	ret

0000000000030498 <__gmpn_set_str@@Base>:
   30498:	stp	x29, x30, [sp, #-96]!
   3049c:	str	x28, [sp, #16]
   304a0:	stp	x26, x25, [sp, #32]
   304a4:	stp	x24, x23, [sp, #48]
   304a8:	stp	x22, x21, [sp, #64]
   304ac:	stp	x20, x19, [sp, #80]
   304b0:	mov	x29, sp
   304b4:	sub	sp, sp, #0xa00
   304b8:	sub	w8, w3, #0x1
   304bc:	mov	w22, w3
   304c0:	mov	x21, x2
   304c4:	mov	x19, x1
   304c8:	tst	w3, w8
   304cc:	mov	x20, x0
   304d0:	b.ne	304ec <__gmpn_set_str@@Base+0x54>  // b.any
   304d4:	add	x8, x19, x21
   304d8:	sub	x8, x8, #0x1
   304dc:	cmp	x8, x19
   304e0:	b.cs	30580 <__gmpn_set_str@@Base+0xe8>  // b.hs, b.nlast
   304e4:	mov	x0, xzr
   304e8:	b	30620 <__gmpn_set_str@@Base+0x188>
   304ec:	cmp	x21, #0x717
   304f0:	b.ls	305e0 <__gmpn_set_str@@Base+0x148>  // b.plast
   304f4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   304f8:	ldr	x8, [x8, #3936]
   304fc:	mov	w24, #0x28                  	// #40
   30500:	smull	x9, w22, w24
   30504:	add	x0, x29, #0x18
   30508:	ldrsw	x8, [x8, x9]
   3050c:	str	xzr, [x29, #24]
   30510:	udiv	x8, x21, x8
   30514:	lsl	x25, x8, #3
   30518:	add	x1, x25, #0x408
   3051c:	add	x23, x8, #0x1
   30520:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   30524:	mov	x1, x0
   30528:	mov	x0, sp
   3052c:	mov	x2, x23
   30530:	mov	w3, w22
   30534:	mov	x26, sp
   30538:	bl	cb10 <__gmpn_compute_powtab@plt>
   3053c:	madd	x22, x0, x24, x26
   30540:	add	x1, x25, #0x208
   30544:	add	x0, x29, #0x18
   30548:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3054c:	mov	x4, x0
   30550:	mov	x0, x20
   30554:	mov	x1, x19
   30558:	mov	x2, x21
   3055c:	mov	x3, x22
   30560:	bl	c360 <__gmpn_dc_set_str@plt>
   30564:	ldr	x8, [x29, #24]
   30568:	cbz	x8, 30620 <__gmpn_set_str@@Base+0x188>
   3056c:	mov	x19, x0
   30570:	mov	x0, x8
   30574:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   30578:	mov	x0, x19
   3057c:	b	30620 <__gmpn_set_str@@Base+0x188>
   30580:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   30584:	ldr	x10, [x10, #3936]
   30588:	mov	w12, #0x28                  	// #40
   3058c:	mov	w11, wzr
   30590:	mov	x9, xzr
   30594:	smaddl	x10, w22, w12, x10
   30598:	ldr	w10, [x10, #24]
   3059c:	mov	x0, xzr
   305a0:	b	305b0 <__gmpn_set_str@@Base+0x118>
   305a4:	sub	x8, x8, #0x1
   305a8:	cmp	x8, x19
   305ac:	b.cc	30610 <__gmpn_set_str@@Base+0x178>  // b.lo, b.ul, b.last
   305b0:	ldrb	w12, [x8]
   305b4:	lsl	x14, x12, x11
   305b8:	add	w11, w11, w10
   305bc:	subs	w13, w11, #0x40
   305c0:	orr	x9, x14, x9
   305c4:	b.lt	305a4 <__gmpn_set_str@@Base+0x10c>  // b.tstop
   305c8:	str	x9, [x20, x0, lsl #3]
   305cc:	sub	w9, w10, w13
   305d0:	add	x0, x0, #0x1
   305d4:	lsr	w9, w12, w9
   305d8:	mov	w11, w13
   305dc:	b	305a4 <__gmpn_set_str@@Base+0x10c>
   305e0:	mov	x0, x20
   305e4:	mov	x1, x19
   305e8:	mov	x2, x21
   305ec:	mov	w3, w22
   305f0:	add	sp, sp, #0xa00
   305f4:	ldp	x20, x19, [sp, #80]
   305f8:	ldp	x22, x21, [sp, #64]
   305fc:	ldp	x24, x23, [sp, #48]
   30600:	ldp	x26, x25, [sp, #32]
   30604:	ldr	x28, [sp, #16]
   30608:	ldp	x29, x30, [sp], #96
   3060c:	b	c110 <__gmpn_bc_set_str@plt>
   30610:	cbz	x9, 30620 <__gmpn_set_str@@Base+0x188>
   30614:	add	x8, x0, #0x1
   30618:	str	x9, [x20, x0, lsl #3]
   3061c:	mov	x0, x8
   30620:	add	sp, sp, #0xa00
   30624:	ldp	x20, x19, [sp, #80]
   30628:	ldp	x22, x21, [sp, #64]
   3062c:	ldp	x24, x23, [sp, #48]
   30630:	ldp	x26, x25, [sp, #32]
   30634:	ldr	x28, [sp, #16]
   30638:	ldp	x29, x30, [sp], #96
   3063c:	ret

0000000000030640 <__gmpn_bc_set_str@@Base>:
   30640:	sub	sp, sp, #0x70
   30644:	stp	x29, x30, [sp, #16]
   30648:	stp	x28, x27, [sp, #32]
   3064c:	stp	x26, x25, [sp, #48]
   30650:	stp	x24, x23, [sp, #64]
   30654:	stp	x22, x21, [sp, #80]
   30658:	stp	x20, x19, [sp, #96]
   3065c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   30660:	ldr	x8, [x8, #3936]
   30664:	mov	w9, #0x28                  	// #40
   30668:	mov	x21, x1
   3066c:	ldrb	w4, [x21], #1
   30670:	smaddl	x8, w3, w9, x8
   30674:	ldrsw	x26, [x8]
   30678:	mov	w23, w3
   3067c:	mov	x22, x2
   30680:	mov	x19, x0
   30684:	cmp	x26, x2
   30688:	sxtw	x25, w23
   3068c:	mov	x20, xzr
   30690:	add	x29, sp, #0x10
   30694:	b.cs	307c4 <__gmpn_bc_set_str@@Base+0x184>  // b.hs, b.nlast
   30698:	ldr	x8, [x8, #24]
   3069c:	mov	w24, #0xa                   	// #10
   306a0:	mov	x27, x26
   306a4:	str	x8, [sp, #8]
   306a8:	sub	w8, w26, #0x1
   306ac:	sxtw	x28, w8
   306b0:	b	306d0 <__gmpn_bc_set_str@@Base+0x90>
   306b4:	cbz	x4, 307bc <__gmpn_bc_set_str@@Base+0x17c>
   306b8:	str	x4, [x19]
   306bc:	mov	w20, #0x1                   	// #1
   306c0:	ldrb	w4, [x21], #1
   306c4:	add	x27, x27, x26
   306c8:	cmp	x27, x22
   306cc:	b.cs	307c8 <__gmpn_bc_set_str@@Base+0x188>  // b.hs, b.nlast
   306d0:	cmp	w23, #0xa
   306d4:	b.ne	30770 <__gmpn_bc_set_str@@Base+0x130>  // b.any
   306d8:	ldrb	w8, [x21]
   306dc:	ldrb	w9, [x21, #1]
   306e0:	ldrb	w10, [x21, #2]
   306e4:	ldrb	w11, [x21, #3]
   306e8:	madd	x8, x4, x24, x8
   306ec:	ldrb	w12, [x21, #4]
   306f0:	madd	x8, x8, x24, x9
   306f4:	ldrb	w9, [x21, #5]
   306f8:	madd	x8, x8, x24, x10
   306fc:	ldrb	w10, [x21, #6]
   30700:	madd	x8, x8, x24, x11
   30704:	ldrb	w11, [x21, #7]
   30708:	madd	x8, x8, x24, x12
   3070c:	ldrb	w12, [x21, #8]
   30710:	madd	x8, x8, x24, x9
   30714:	ldrb	w9, [x21, #9]
   30718:	madd	x8, x8, x24, x10
   3071c:	ldrb	w10, [x21, #10]
   30720:	madd	x8, x8, x24, x11
   30724:	ldrb	w11, [x21, #11]
   30728:	madd	x8, x8, x24, x12
   3072c:	ldrb	w12, [x21, #12]
   30730:	madd	x8, x8, x24, x9
   30734:	ldrb	w9, [x21, #13]
   30738:	madd	x8, x8, x24, x10
   3073c:	ldrb	w10, [x21, #14]
   30740:	madd	x8, x8, x24, x11
   30744:	ldrb	w11, [x21, #15]
   30748:	madd	x8, x8, x24, x12
   3074c:	ldrb	w12, [x21, #16]
   30750:	madd	x8, x8, x24, x9
   30754:	ldrb	w9, [x21, #17]
   30758:	madd	x8, x8, x24, x10
   3075c:	madd	x8, x8, x24, x11
   30760:	madd	x8, x8, x24, x12
   30764:	madd	x4, x8, x24, x9
   30768:	add	x21, x21, #0x12
   3076c:	b	30790 <__gmpn_bc_set_str@@Base+0x150>
   30770:	cbz	w28, 30790 <__gmpn_bc_set_str@@Base+0x150>
   30774:	mov	x8, x21
   30778:	mov	x9, x28
   3077c:	ldrb	w10, [x8], #1
   30780:	subs	x9, x9, #0x1
   30784:	madd	x4, x4, x25, x10
   30788:	b.ne	3077c <__gmpn_bc_set_str@@Base+0x13c>  // b.any
   3078c:	add	x21, x21, x28
   30790:	cbz	x20, 306b4 <__gmpn_bc_set_str@@Base+0x74>
   30794:	ldr	x3, [sp, #8]
   30798:	mov	x0, x19
   3079c:	mov	x1, x19
   307a0:	mov	x2, x20
   307a4:	bl	d240 <__gmpn_mul_1c@plt>
   307a8:	cbz	x0, 306c0 <__gmpn_bc_set_str@@Base+0x80>
   307ac:	add	x8, x20, #0x1
   307b0:	str	x0, [x19, x20, lsl #3]
   307b4:	mov	x20, x8
   307b8:	b	306c0 <__gmpn_bc_set_str@@Base+0x80>
   307bc:	mov	x20, xzr
   307c0:	b	306c0 <__gmpn_bc_set_str@@Base+0x80>
   307c4:	mov	x27, x26
   307c8:	cmp	w23, #0xa
   307cc:	b.ne	3080c <__gmpn_bc_set_str@@Base+0x1cc>  // b.any
   307d0:	sub	x8, x22, x27
   307d4:	add	x9, x8, #0x12
   307d8:	cmp	x9, #0x1
   307dc:	b.lt	30848 <__gmpn_bc_set_str@@Base+0x208>  // b.tstop
   307e0:	add	x8, x8, #0x13
   307e4:	mov	w9, #0xa                   	// #10
   307e8:	mov	w3, #0xa                   	// #10
   307ec:	ldrb	w10, [x21], #1
   307f0:	add	x11, x3, x3, lsl #2
   307f4:	sub	x8, x8, #0x1
   307f8:	cmp	x8, #0x1
   307fc:	madd	x4, x4, x9, x10
   30800:	lsl	x3, x11, #1
   30804:	b.gt	307ec <__gmpn_bc_set_str@@Base+0x1ac>
   30808:	b	3084c <__gmpn_bc_set_str@@Base+0x20c>
   3080c:	add	x8, x26, x22
   30810:	mvn	x9, x27
   30814:	add	x8, x8, x9
   30818:	cmp	x8, #0x1
   3081c:	b.lt	30874 <__gmpn_bc_set_str@@Base+0x234>  // b.tstop
   30820:	add	x8, x22, x26
   30824:	sub	x8, x8, x27
   30828:	mov	x3, x25
   3082c:	ldrb	w9, [x21], #1
   30830:	sub	x8, x8, #0x1
   30834:	cmp	x8, #0x1
   30838:	mul	x3, x3, x25
   3083c:	madd	x4, x4, x25, x9
   30840:	b.gt	3082c <__gmpn_bc_set_str@@Base+0x1ec>
   30844:	b	3084c <__gmpn_bc_set_str@@Base+0x20c>
   30848:	mov	w3, #0xa                   	// #10
   3084c:	cbz	x20, 3087c <__gmpn_bc_set_str@@Base+0x23c>
   30850:	mov	x0, x19
   30854:	mov	x1, x19
   30858:	mov	x2, x20
   3085c:	bl	d240 <__gmpn_mul_1c@plt>
   30860:	cbz	x0, 30890 <__gmpn_bc_set_str@@Base+0x250>
   30864:	add	x8, x20, #0x1
   30868:	str	x0, [x19, x20, lsl #3]
   3086c:	mov	x20, x8
   30870:	b	30890 <__gmpn_bc_set_str@@Base+0x250>
   30874:	mov	x3, x25
   30878:	cbnz	x20, 30850 <__gmpn_bc_set_str@@Base+0x210>
   3087c:	cbz	x4, 3088c <__gmpn_bc_set_str@@Base+0x24c>
   30880:	str	x4, [x19]
   30884:	mov	w20, #0x1                   	// #1
   30888:	b	30890 <__gmpn_bc_set_str@@Base+0x250>
   3088c:	mov	x20, xzr
   30890:	mov	x0, x20
   30894:	ldp	x20, x19, [sp, #96]
   30898:	ldp	x22, x21, [sp, #80]
   3089c:	ldp	x24, x23, [sp, #64]
   308a0:	ldp	x26, x25, [sp, #48]
   308a4:	ldp	x28, x27, [sp, #32]
   308a8:	ldp	x29, x30, [sp, #16]
   308ac:	add	sp, sp, #0x70
   308b0:	ret

00000000000308b4 <__gmpn_dc_set_str@@Base>:
   308b4:	stp	x29, x30, [sp, #-80]!
   308b8:	stp	x26, x25, [sp, #16]
   308bc:	stp	x24, x23, [sp, #32]
   308c0:	stp	x22, x21, [sp, #48]
   308c4:	stp	x20, x19, [sp, #64]
   308c8:	ldr	x22, [x3, #24]
   308cc:	mov	x20, x4
   308d0:	mov	x23, x2
   308d4:	mov	x24, x1
   308d8:	cmp	x22, x2
   308dc:	mov	x19, x0
   308e0:	mov	x29, sp
   308e4:	b.cs	308f0 <__gmpn_dc_set_str@@Base+0x3c>  // b.hs, b.nlast
   308e8:	mov	x25, x3
   308ec:	b	30910 <__gmpn_dc_set_str@@Base+0x5c>
   308f0:	mov	x8, x3
   308f4:	cmp	x23, #0x313
   308f8:	b.ls	30990 <__gmpn_dc_set_str@@Base+0xdc>  // b.plast
   308fc:	ldur	x22, [x8, #-16]
   30900:	sub	x25, x8, #0x28
   30904:	mov	x8, x25
   30908:	cmp	x22, x23
   3090c:	b.cs	308f4 <__gmpn_dc_set_str@@Base+0x40>  // b.hs, b.nlast
   30910:	sub	x2, x23, x22
   30914:	cmp	x2, #0x313
   30918:	b.ls	30934 <__gmpn_dc_set_str@@Base+0x80>  // b.plast
   3091c:	sub	x3, x25, #0x28
   30920:	mov	x0, x20
   30924:	mov	x1, x24
   30928:	mov	x4, x19
   3092c:	bl	c360 <__gmpn_dc_set_str@plt>
   30930:	b	30944 <__gmpn_dc_set_str@@Base+0x90>
   30934:	ldr	w3, [x25, #32]
   30938:	mov	x0, x20
   3093c:	mov	x1, x24
   30940:	bl	c110 <__gmpn_bc_set_str@plt>
   30944:	ldp	x4, x26, [x25, #8]
   30948:	mov	x21, x0
   3094c:	cbz	x0, 3097c <__gmpn_dc_set_str@@Base+0xc8>
   30950:	ldr	x3, [x25]
   30954:	cmp	x4, x21
   30958:	add	x0, x19, x26, lsl #3
   3095c:	b.le	309b8 <__gmpn_dc_set_str@@Base+0x104>
   30960:	mov	x1, x3
   30964:	mov	x2, x4
   30968:	mov	x3, x20
   3096c:	mov	x4, x21
   30970:	bl	ccd0 <__gmpn_mul@plt>
   30974:	cbnz	x26, 309c8 <__gmpn_dc_set_str@@Base+0x114>
   30978:	b	309d8 <__gmpn_dc_set_str@@Base+0x124>
   3097c:	add	x8, x26, x4
   30980:	adds	x8, x8, #0x1
   30984:	b.eq	309d8 <__gmpn_dc_set_str@@Base+0x124>  // b.none
   30988:	lsl	x2, x8, #3
   3098c:	b	309cc <__gmpn_dc_set_str@@Base+0x118>
   30990:	ldr	w3, [x3, #32]
   30994:	mov	x0, x19
   30998:	mov	x1, x24
   3099c:	mov	x2, x23
   309a0:	ldp	x20, x19, [sp, #64]
   309a4:	ldp	x22, x21, [sp, #48]
   309a8:	ldp	x24, x23, [sp, #32]
   309ac:	ldp	x26, x25, [sp, #16]
   309b0:	ldp	x29, x30, [sp], #80
   309b4:	b	c110 <__gmpn_bc_set_str@plt>
   309b8:	mov	x1, x20
   309bc:	mov	x2, x21
   309c0:	bl	ccd0 <__gmpn_mul@plt>
   309c4:	cbz	x26, 309d8 <__gmpn_dc_set_str@@Base+0x124>
   309c8:	lsl	x2, x26, #3
   309cc:	mov	x0, x19
   309d0:	mov	w1, wzr
   309d4:	bl	c5f0 <memset@plt>
   309d8:	add	x8, x24, x23
   309dc:	cmp	x22, #0x313
   309e0:	sub	x1, x8, x22
   309e4:	b.ls	30a14 <__gmpn_dc_set_str@@Base+0x160>  // b.plast
   309e8:	ldr	x8, [x25, #8]
   309ec:	sub	x3, x25, #0x28
   309f0:	mov	x0, x20
   309f4:	mov	x2, x22
   309f8:	add	x8, x20, x8, lsl #3
   309fc:	add	x8, x8, x26, lsl #3
   30a00:	add	x4, x8, #0x8
   30a04:	bl	c360 <__gmpn_dc_set_str@plt>
   30a08:	mov	x22, x0
   30a0c:	cbnz	x0, 30a2c <__gmpn_dc_set_str@@Base+0x178>
   30a10:	b	30a6c <__gmpn_dc_set_str@@Base+0x1b8>
   30a14:	ldr	w3, [x25, #32]
   30a18:	mov	x0, x20
   30a1c:	mov	x2, x22
   30a20:	bl	c110 <__gmpn_bc_set_str@plt>
   30a24:	mov	x22, x0
   30a28:	cbz	x0, 30a6c <__gmpn_dc_set_str@@Base+0x1b8>
   30a2c:	mov	x0, x19
   30a30:	mov	x1, x19
   30a34:	mov	x2, x20
   30a38:	mov	x3, x22
   30a3c:	bl	ca70 <__gmpn_add_n@plt>
   30a40:	lsl	x8, x22, #3
   30a44:	ldr	x9, [x19, x8]
   30a48:	adds	x9, x9, x0
   30a4c:	str	x9, [x19, x8]
   30a50:	b.cc	30a6c <__gmpn_dc_set_str@@Base+0x1b8>  // b.lo, b.ul, b.last
   30a54:	add	x8, x19, x22, lsl #3
   30a58:	add	x8, x8, #0x8
   30a5c:	ldr	x9, [x8]
   30a60:	adds	x9, x9, #0x1
   30a64:	str	x9, [x8], #8
   30a68:	b.cs	30a5c <__gmpn_dc_set_str@@Base+0x1a8>  // b.hs, b.nlast
   30a6c:	ldr	x8, [x25, #8]
   30a70:	add	x9, x26, x21
   30a74:	ldp	x22, x21, [sp, #48]
   30a78:	ldp	x24, x23, [sp, #32]
   30a7c:	add	x8, x9, x8
   30a80:	add	x9, x19, x8, lsl #3
   30a84:	ldur	x9, [x9, #-8]
   30a88:	ldp	x20, x19, [sp, #64]
   30a8c:	ldp	x26, x25, [sp, #16]
   30a90:	cmp	x9, #0x0
   30a94:	cset	w9, eq  // eq = none
   30a98:	sub	x0, x8, x9
   30a9c:	ldp	x29, x30, [sp], #80
   30aa0:	ret

0000000000030aa4 <__gmpn_compute_powtab@@Base>:
   30aa4:	str	d14, [sp, #-160]!
   30aa8:	stp	d13, d12, [sp, #16]
   30aac:	stp	d11, d10, [sp, #32]
   30ab0:	stp	d9, d8, [sp, #48]
   30ab4:	stp	x29, x30, [sp, #64]
   30ab8:	stp	x28, x27, [sp, #80]
   30abc:	stp	x26, x25, [sp, #96]
   30ac0:	stp	x24, x23, [sp, #112]
   30ac4:	stp	x22, x21, [sp, #128]
   30ac8:	stp	x20, x19, [sp, #144]
   30acc:	mov	x29, sp
   30ad0:	sub	sp, sp, #0x230
   30ad4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   30ad8:	ldr	x8, [x8, #3936]
   30adc:	mov	w9, #0x28                  	// #40
   30ae0:	smull	x9, w3, w9
   30ae4:	mov	x21, x1
   30ae8:	ldrsw	x27, [x8, x9]
   30aec:	add	x9, x2, #0x1
   30af0:	lsr	x11, x9, #1
   30af4:	mov	x20, x0
   30af8:	cmp	x11, #0x1
   30afc:	sxtw	x9, w3
   30b00:	mov	x10, xzr
   30b04:	str	x3, [x29, #8]
   30b08:	b.ne	30b18 <__gmpn_compute_powtab@@Base+0x74>  // b.any
   30b0c:	mov	w14, #0x1                   	// #1
   30b10:	str	x27, [sp, #40]
   30b14:	b	30d8c <__gmpn_compute_powtab@@Base+0x2e8>
   30b18:	mov	x13, #0xffffffffffffffff    	// #-1
   30b1c:	add	x12, sp, #0x28
   30b20:	mov	x14, x11
   30b24:	mul	x15, x14, x27
   30b28:	add	x14, x14, #0x1
   30b2c:	lsr	x14, x14, #1
   30b30:	str	x15, [x12, x10, lsl #3]
   30b34:	add	x10, x10, #0x1
   30b38:	cmp	x14, #0x1
   30b3c:	add	x13, x13, #0x1
   30b40:	b.ne	30b24 <__gmpn_compute_powtab@@Base+0x80>  // b.any
   30b44:	add	x12, sp, #0x28
   30b48:	subs	x14, x10, #0x1
   30b4c:	str	x27, [x12, x10, lsl #3]
   30b50:	b.ne	30b60 <__gmpn_compute_powtab@@Base+0xbc>  // b.any
   30b54:	mov	w14, #0x1                   	// #1
   30b58:	mov	w11, #0x1                   	// #1
   30b5c:	b	30d8c <__gmpn_compute_powtab@@Base+0x2e8>
   30b60:	cmp	x14, #0x7
   30b64:	sub	x12, x2, #0x1
   30b68:	b.hi	30b7c <__gmpn_compute_powtab@@Base+0xd8>  // b.pmore
   30b6c:	mov	w15, #0x1                   	// #1
   30b70:	mov	x13, x14
   30b74:	mov	w14, #0x1                   	// #1
   30b78:	b	30d30 <__gmpn_compute_powtab@@Base+0x28c>
   30b7c:	adrp	x17, 5b000 <__gmpn_bases@@Base+0x2678>
   30b80:	adrp	x16, 5b000 <__gmpn_bases@@Base+0x2678>
   30b84:	ldr	q20, [x17, #2448]
   30b88:	adrp	x17, 5b000 <__gmpn_bases@@Base+0x2678>
   30b8c:	ldr	q18, [x16, #2432]
   30b90:	ldr	q3, [x17, #2464]
   30b94:	mov	x17, #0xfffffffffffffffc    	// #-4
   30b98:	dup	v5.2d, x17
   30b9c:	mov	x17, #0xfffffffffffffffb    	// #-5
   30ba0:	mvn	x13, x13
   30ba4:	dup	v6.2d, x17
   30ba8:	mov	w17, #0x1                   	// #1
   30bac:	dup	v19.2d, x14
   30bb0:	dup	v16.2d, x17
   30bb4:	mov	x17, #0xfffffffffffffff8    	// #-8
   30bb8:	orr	x13, x13, #0x7
   30bbc:	and	x15, x14, #0x7ffffffffffffff8
   30bc0:	dup	v0.2d, x12
   30bc4:	dup	v1.2d, x11
   30bc8:	and	x16, x14, #0xfffffffffffffff8
   30bcc:	movi	v2.2d, #0x0
   30bd0:	movi	v4.2d, #0xffffffffffffffff
   30bd4:	movi	v7.4s, #0x1
   30bd8:	dup	v17.2d, x17
   30bdc:	add	x13, x13, x10
   30be0:	add	v18.2d, v19.2d, v18.2d
   30be4:	add	v19.2d, v19.2d, v20.2d
   30be8:	mov	v20.16b, v3.16b
   30bec:	movi	v21.2d, #0x0
   30bf0:	add	v26.2d, v19.2d, v5.2d
   30bf4:	neg	v29.2d, v19.2d
   30bf8:	add	v27.2d, v18.2d, v5.2d
   30bfc:	neg	v28.2d, v18.2d
   30c00:	ushl	v29.2d, v0.2d, v29.2d
   30c04:	neg	v26.2d, v26.2d
   30c08:	ushl	v28.2d, v0.2d, v28.2d
   30c0c:	neg	v27.2d, v27.2d
   30c10:	ushl	v26.2d, v0.2d, v26.2d
   30c14:	add	v30.2d, v29.2d, v16.2d
   30c18:	add	v25.2d, v19.2d, v4.2d
   30c1c:	add	v23.2d, v19.2d, v6.2d
   30c20:	ushl	v27.2d, v0.2d, v27.2d
   30c24:	add	v31.2d, v28.2d, v16.2d
   30c28:	add	v8.2d, v26.2d, v16.2d
   30c2c:	and	v11.16b, v30.16b, v16.16b
   30c30:	add	v24.2d, v18.2d, v4.2d
   30c34:	add	v22.2d, v18.2d, v6.2d
   30c38:	add	v9.2d, v27.2d, v16.2d
   30c3c:	and	v10.16b, v31.16b, v16.16b
   30c40:	ushl	v25.2d, v30.2d, v25.2d
   30c44:	and	v13.16b, v8.16b, v16.16b
   30c48:	cmeq	v11.2d, v11.2d, #0
   30c4c:	ushl	v23.2d, v8.2d, v23.2d
   30c50:	cmhi	v29.2d, v29.2d, v16.2d
   30c54:	xtn	v12.2s, v30.2d
   30c58:	ushl	v24.2d, v31.2d, v24.2d
   30c5c:	cmhi	v26.2d, v26.2d, v16.2d
   30c60:	and	v30.16b, v9.16b, v16.16b
   30c64:	cmeq	v10.2d, v10.2d, #0
   30c68:	xtn	v14.2s, v8.2d
   30c6c:	ushl	v22.2d, v9.2d, v22.2d
   30c70:	cmeq	v25.2d, v1.2d, v25.2d
   30c74:	xtn	v8.2s, v11.2d
   30c78:	cmeq	v11.2d, v13.2d, #0
   30c7c:	cmeq	v23.2d, v1.2d, v23.2d
   30c80:	cmhi	v28.2d, v28.2d, v16.2d
   30c84:	xtn	v29.2s, v29.2d
   30c88:	cmhi	v27.2d, v27.2d, v16.2d
   30c8c:	cmeq	v24.2d, v1.2d, v24.2d
   30c90:	xtn	v26.2s, v26.2d
   30c94:	cmeq	v30.2d, v30.2d, #0
   30c98:	xtn	v25.2s, v25.2d
   30c9c:	cmeq	v22.2d, v1.2d, v22.2d
   30ca0:	xtn2	v8.4s, v10.2d
   30ca4:	xtn	v10.2s, v11.2d
   30ca8:	xtn	v23.2s, v23.2d
   30cac:	xtn2	v10.4s, v30.2d
   30cb0:	xtn2	v12.4s, v31.2d
   30cb4:	xtn2	v14.4s, v9.2d
   30cb8:	xtn2	v25.4s, v24.2d
   30cbc:	xtn2	v23.4s, v22.2d
   30cc0:	xtn2	v29.4s, v28.2d
   30cc4:	xtn2	v26.4s, v27.2d
   30cc8:	and	v27.16b, v29.16b, v8.16b
   30ccc:	and	v26.16b, v26.16b, v10.16b
   30cd0:	and	v27.16b, v27.16b, v7.16b
   30cd4:	and	v26.16b, v26.16b, v7.16b
   30cd8:	bic	v22.16b, v12.16b, v8.16b
   30cdc:	bic	v24.16b, v14.16b, v10.16b
   30ce0:	ushl	v27.4s, v12.4s, v27.4s
   30ce4:	ushl	v26.4s, v14.4s, v26.4s
   30ce8:	bsl	v25.16b, v22.16b, v27.16b
   30cec:	bsl	v23.16b, v24.16b, v26.16b
   30cf0:	add	v18.2d, v18.2d, v17.2d
   30cf4:	subs	x16, x16, #0x8
   30cf8:	add	v3.4s, v22.4s, v3.4s
   30cfc:	add	v2.4s, v24.4s, v2.4s
   30d00:	add	v20.4s, v25.4s, v20.4s
   30d04:	add	v21.4s, v23.4s, v21.4s
   30d08:	add	v19.2d, v19.2d, v17.2d
   30d0c:	b.ne	30bf0 <__gmpn_compute_powtab@@Base+0x14c>  // b.any
   30d10:	add	v0.4s, v21.4s, v20.4s
   30d14:	add	v1.4s, v2.4s, v3.4s
   30d18:	addv	s0, v0.4s
   30d1c:	addv	s1, v1.4s
   30d20:	cmp	x14, x15
   30d24:	fmov	w14, s0
   30d28:	fmov	w15, s1
   30d2c:	b.eq	30d74 <__gmpn_compute_powtab@@Base+0x2d0>  // b.none
   30d30:	lsr	x16, x12, x13
   30d34:	add	x17, x16, #0x1
   30d38:	tst	x17, #0x1
   30d3c:	cset	w18, eq  // eq = none
   30d40:	csel	w0, wzr, w17, eq  // eq = none
   30d44:	subs	x1, x13, #0x1
   30d48:	cmp	x16, #0x1
   30d4c:	cset	w16, hi  // hi = pmore
   30d50:	lsl	x1, x17, x1
   30d54:	and	w16, w16, w18
   30d58:	cmp	x11, x1
   30d5c:	lsl	w16, w17, w16
   30d60:	csel	w16, w0, w16, eq  // eq = none
   30d64:	subs	x13, x13, #0x1
   30d68:	add	w15, w0, w15
   30d6c:	add	w14, w16, w14
   30d70:	b.gt	30d30 <__gmpn_compute_powtab@@Base+0x28c>
   30d74:	mov	w11, #0x9f                  	// #159
   30d78:	mov	w12, #0x851f                	// #34079
   30d7c:	mul	w11, w15, w11
   30d80:	movk	w12, #0x51eb, lsl #16
   30d84:	umull	x11, w11, w12
   30d88:	lsr	x11, x11, #37
   30d8c:	mov	w12, #0x28                  	// #40
   30d90:	madd	x8, x9, x12, x8
   30d94:	ldr	x3, [x8, #24]
   30d98:	cmp	w14, w11
   30d9c:	cneg	x9, x10, hi  // hi = pmore
   30da0:	str	x9, [sp]
   30da4:	str	x3, [sp, #16]
   30da8:	tbnz	x9, #63, 30e40 <__gmpn_compute_powtab@@Base+0x39c>
   30dac:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   30db0:	ldr	q0, [x8, #2464]
   30db4:	ldr	x22, [x29, #8]
   30db8:	add	x25, x21, #0x8
   30dbc:	mov	w2, #0x1                   	// #1
   30dc0:	mov	x0, x25
   30dc4:	mov	x1, x21
   30dc8:	str	x3, [x21]
   30dcc:	str	x21, [x20]
   30dd0:	str	x27, [x20, #24]
   30dd4:	str	w22, [x20, #32]
   30dd8:	stur	q0, [x20, #8]
   30ddc:	add	x24, x21, #0x18
   30de0:	mov	w19, #0x1                   	// #1
   30de4:	bl	d490 <__gmpn_mul_1@plt>
   30de8:	ldr	x8, [x21, #8]
   30dec:	lsl	x9, x27, #1
   30df0:	ldr	x10, [sp]
   30df4:	str	x0, [x21, #16]
   30df8:	cmp	x8, #0x0
   30dfc:	cset	w26, eq  // eq = none
   30e00:	cinc	x23, x19, ne  // ne = any
   30e04:	add	x25, x25, w26, uxtw #3
   30e08:	str	w22, [x20, #72]
   30e0c:	stp	x25, x23, [x20, #40]
   30e10:	stp	x26, x9, [x20, #56]
   30e14:	ldr	x28, [sp, #40]
   30e18:	lsl	x8, x27, x10
   30e1c:	mov	x11, x9
   30e20:	cmp	x28, x8
   30e24:	b.ne	30fec <__gmpn_compute_powtab@@Base+0x548>  // b.any
   30e28:	add	x8, x20, #0x50
   30e2c:	mov	x9, #0xfffffffffffffffe    	// #-2
   30e30:	adds	x19, x10, x9
   30e34:	b.pl	31088 <__gmpn_compute_powtab@@Base+0x5e4>  // b.nfrst
   30e38:	mov	x0, x10
   30e3c:	b	311d8 <__gmpn_compute_powtab@@Base+0x734>
   30e40:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   30e44:	ldr	q0, [x8, #2464]
   30e48:	ldr	x8, [x29, #8]
   30e4c:	mvn	x23, x9
   30e50:	lsr	x9, x3, #19
   30e54:	mov	x24, x21
   30e58:	str	x9, [sp, #32]
   30e5c:	neg	x9, x3
   30e60:	str	x3, [x24], #8
   30e64:	str	w8, [x20, #32]
   30e68:	and	x8, x3, x9
   30e6c:	mov	x28, xzr
   30e70:	add	x10, x20, #0x30
   30e74:	mov	w26, #0x1                   	// #1
   30e78:	sub	x19, x8, #0x1
   30e7c:	str	x21, [x20]
   30e80:	str	x27, [x20, #24]
   30e84:	stur	q0, [x20, #8]
   30e88:	str	x27, [sp, #24]
   30e8c:	b	30ebc <__gmpn_compute_powtab@@Base+0x418>
   30e90:	mov	x21, x24
   30e94:	stp	x21, x25, [x20, #40]
   30e98:	ldr	x10, [x29, #8]
   30e9c:	stp	x28, x27, [x20, #56]
   30ea0:	subs	x23, x23, #0x1
   30ea4:	mov	x24, x8
   30ea8:	str	w10, [x20, #72]
   30eac:	add	x10, x22, #0x28
   30eb0:	mov	x20, x9
   30eb4:	mov	x26, x25
   30eb8:	b.mi	30f98 <__gmpn_compute_powtab@@Base+0x4f4>  // b.first
   30ebc:	mov	x0, x24
   30ec0:	mov	x1, x21
   30ec4:	mov	x2, x26
   30ec8:	mov	x22, x10
   30ecc:	lsl	x25, x26, #1
   30ed0:	bl	c8e0 <__gmpn_sqr@plt>
   30ed4:	sub	x8, x25, #0x1
   30ed8:	ldr	x9, [x24, x8, lsl #3]
   30edc:	add	x10, sp, #0x28
   30ee0:	ldr	x10, [x10, x23, lsl #3]
   30ee4:	lsl	x27, x27, #1
   30ee8:	cmp	x9, #0x0
   30eec:	csel	x25, x8, x25, eq  // eq = none
   30ef0:	cmp	x27, x10
   30ef4:	b.eq	30f48 <__gmpn_compute_powtab@@Base+0x4a4>  // b.none
   30ef8:	ldr	x8, [x29, #8]
   30efc:	cmp	w8, #0xa
   30f00:	b.ne	30f80 <__gmpn_compute_powtab@@Base+0x4dc>  // b.any
   30f04:	mov	x4, #0xce15                	// #52757
   30f08:	ldr	x3, [sp, #32]
   30f0c:	movk	x4, #0x6559, lsl #16
   30f10:	movk	x4, #0x7250, lsl #32
   30f14:	movk	x4, #0x26b1, lsl #48
   30f18:	mov	w5, #0x13                  	// #19
   30f1c:	mov	x0, x24
   30f20:	mov	x1, x24
   30f24:	mov	x2, x25
   30f28:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   30f2c:	add	x8, x24, x25, lsl #3
   30f30:	ldur	x8, [x8, #-8]
   30f34:	cmp	x8, #0x0
   30f38:	cset	w8, eq  // eq = none
   30f3c:	sub	x25, x25, x8
   30f40:	ldr	x8, [sp, #24]
   30f44:	sub	x27, x27, x8
   30f48:	ldr	x10, [x24]
   30f4c:	add	x9, x20, #0x28
   30f50:	add	x8, x24, x26, lsl #4
   30f54:	lsl	x28, x28, #1
   30f58:	cbnz	x10, 30e90 <__gmpn_compute_powtab@@Base+0x3ec>
   30f5c:	mov	x21, x24
   30f60:	ldr	x10, [x21, #8]!
   30f64:	tst	x10, x19
   30f68:	b.ne	30e90 <__gmpn_compute_powtab@@Base+0x3ec>  // b.any
   30f6c:	sub	x25, x25, #0x1
   30f70:	add	x28, x28, #0x1
   30f74:	mov	x24, x21
   30f78:	cbz	x10, 30f60 <__gmpn_compute_powtab@@Base+0x4bc>
   30f7c:	b	30e94 <__gmpn_compute_powtab@@Base+0x3f0>
   30f80:	ldr	x3, [sp, #16]
   30f84:	mov	x0, x24
   30f88:	mov	x1, x24
   30f8c:	mov	x2, x25
   30f90:	bl	c770 <__gmpn_divexact_1@plt>
   30f94:	b	30f2c <__gmpn_compute_powtab@@Base+0x488>
   30f98:	ldr	x9, [sp]
   30f9c:	cmp	x9, #0x0
   30fa0:	neg	x0, x9
   30fa4:	b.gt	311d8 <__gmpn_compute_powtab@@Base+0x734>
   30fa8:	mov	w8, #0x1                   	// #1
   30fac:	sub	x8, x8, x9
   30fb0:	ldp	x9, x12, [x22, #-8]
   30fb4:	ldr	x11, [x22, #8]
   30fb8:	sub	x8, x8, #0x1
   30fbc:	ldr	x10, [x9]
   30fc0:	cmp	x10, #0x0
   30fc4:	cset	w10, eq  // eq = none
   30fc8:	cinc	x11, x11, eq  // eq = none
   30fcc:	add	x9, x9, w10, uxtw #3
   30fd0:	sub	x10, x12, x10
   30fd4:	cmp	x8, #0x0
   30fd8:	stp	x9, x10, [x22, #-8]
   30fdc:	str	x11, [x22, #8]
   30fe0:	sub	x22, x22, #0x28
   30fe4:	b.gt	30fb0 <__gmpn_compute_powtab@@Base+0x50c>
   30fe8:	b	311d8 <__gmpn_compute_powtab@@Base+0x734>
   30fec:	add	x22, x27, x27, lsl #1
   30ff0:	sub	x8, x10, #0x2
   30ff4:	lsl	x8, x22, x8
   30ff8:	cmp	x8, x28
   30ffc:	b.ls	3101c <__gmpn_compute_powtab@@Base+0x578>  // b.plast
   31000:	ldr	x8, [x25]
   31004:	add	x19, x21, #0x30
   31008:	mov	x22, x11
   3100c:	str	x8, [x21, #24]
   31010:	ldr	x8, [x25, #8]
   31014:	str	x8, [x21, #32]
   31018:	b	3105c <__gmpn_compute_powtab@@Base+0x5b8>
   3101c:	ldr	x3, [sp, #16]
   31020:	mov	x0, x24
   31024:	mov	x1, x25
   31028:	mov	x2, x23
   3102c:	add	x19, x21, #0x38
   31030:	bl	d490 <__gmpn_mul_1@plt>
   31034:	str	x0, [x24, x23, lsl #3]
   31038:	ldr	x8, [x21, #24]
   3103c:	ldr	x10, [sp]
   31040:	cmp	x0, #0x0
   31044:	cinc	x9, x23, ne  // ne = any
   31048:	cmp	x8, #0x0
   3104c:	cset	w8, eq  // eq = none
   31050:	cinc	x26, x26, eq  // eq = none
   31054:	add	x24, x24, w8, uxtw #3
   31058:	sub	x23, x9, x8
   3105c:	stp	x24, x23, [x20, #80]
   31060:	ldr	x8, [x29, #8]
   31064:	mov	x25, x24
   31068:	mov	x11, x22
   3106c:	mov	x9, #0xfffffffffffffffd    	// #-3
   31070:	str	w8, [x20, #112]
   31074:	add	x8, x20, #0x78
   31078:	mov	x24, x19
   3107c:	stp	x26, x22, [x20, #96]
   31080:	adds	x19, x10, x9
   31084:	b.mi	30e38 <__gmpn_compute_powtab@@Base+0x394>  // b.first
   31088:	add	x9, sp, #0x28
   3108c:	stp	x27, x26, [sp, #24]
   31090:	add	x9, x9, #0x8
   31094:	sub	x27, x8, #0x28
   31098:	mov	x26, x11
   3109c:	str	x9, [sp, #8]
   310a0:	mov	x0, x24
   310a4:	mov	x1, x25
   310a8:	mov	x2, x23
   310ac:	lsl	x21, x23, #1
   310b0:	add	x22, x24, x23, lsl #4
   310b4:	bl	c8e0 <__gmpn_sqr@plt>
   310b8:	ldur	x8, [x22, #-8]
   310bc:	ldr	x10, [sp, #24]
   310c0:	ldr	x9, [x24]
   310c4:	lsl	x26, x26, #1
   310c8:	cmp	x8, #0x0
   310cc:	cset	w8, eq  // eq = none
   310d0:	mov	x11, x26
   310d4:	add	x20, x26, x10
   310d8:	sub	x26, x21, x8
   310dc:	ldr	x8, [sp, #32]
   310e0:	cmp	x9, #0x0
   310e4:	lsl	x10, x20, x19
   310e8:	cset	w21, eq  // eq = none
   310ec:	cmp	x10, x28
   310f0:	add	x25, x24, w21, uxtw #3
   310f4:	sub	x23, x26, x21
   310f8:	bfi	x21, x8, #1, #63
   310fc:	b.ls	3110c <__gmpn_compute_powtab@@Base+0x668>  // b.plast
   31100:	mov	x28, x21
   31104:	mov	x26, x11
   31108:	b	31148 <__gmpn_compute_powtab@@Base+0x6a4>
   3110c:	ldr	x3, [sp, #16]
   31110:	mov	x0, x25
   31114:	mov	x1, x25
   31118:	mov	x2, x23
   3111c:	bl	d490 <__gmpn_mul_1@plt>
   31120:	str	x0, [x24, x26, lsl #3]
   31124:	ldr	x8, [x25]
   31128:	cmp	x0, #0x0
   3112c:	cinc	x9, x23, ne  // ne = any
   31130:	mov	x26, x20
   31134:	cmp	x8, #0x0
   31138:	cset	w8, eq  // eq = none
   3113c:	cinc	x28, x21, eq  // eq = none
   31140:	add	x25, x25, w8, uxtw #3
   31144:	sub	x23, x9, x8
   31148:	stp	x25, x23, [x27, #40]
   3114c:	ldr	x8, [x29, #8]
   31150:	stp	x28, x26, [x27, #56]
   31154:	ldr	x9, [sp, #8]
   31158:	str	w8, [x27, #72]
   3115c:	ldr	x8, [x27, #24]
   31160:	ldr	x24, [x9, x19, lsl #3]
   31164:	cmp	x8, x24
   31168:	b.cs	311b8 <__gmpn_compute_powtab@@Base+0x714>  // b.hs, b.nlast
   3116c:	ldp	x21, x20, [x27]
   31170:	ldr	x3, [sp, #16]
   31174:	mov	x0, x21
   31178:	mov	x1, x21
   3117c:	mov	x2, x20
   31180:	bl	d490 <__gmpn_mul_1@plt>
   31184:	str	x0, [x21, x20, lsl #3]
   31188:	str	x24, [x27, #24]
   3118c:	ldr	x8, [x21]
   31190:	ldr	x9, [x27, #16]
   31194:	cmp	x0, #0x0
   31198:	cinc	x10, x20, ne  // ne = any
   3119c:	cmp	x8, #0x0
   311a0:	cset	w8, eq  // eq = none
   311a4:	cinc	x9, x9, eq  // eq = none
   311a8:	add	x11, x21, w8, uxtw #3
   311ac:	sub	x8, x10, x8
   311b0:	stp	x11, x8, [x27]
   311b4:	str	x9, [x27, #16]
   311b8:	subs	x19, x19, #0x1
   311bc:	b.lt	311d4 <__gmpn_compute_powtab@@Base+0x730>  // b.tstop
   311c0:	str	x28, [sp, #32]
   311c4:	ldr	x28, [sp, #40]
   311c8:	add	x24, x22, #0x10
   311cc:	add	x27, x27, #0x28
   311d0:	b	310a0 <__gmpn_compute_powtab@@Base+0x5fc>
   311d4:	ldr	x0, [sp]
   311d8:	add	sp, sp, #0x230
   311dc:	ldp	x20, x19, [sp, #144]
   311e0:	ldp	x22, x21, [sp, #128]
   311e4:	ldp	x24, x23, [sp, #112]
   311e8:	ldp	x26, x25, [sp, #96]
   311ec:	ldp	x28, x27, [sp, #80]
   311f0:	ldp	x29, x30, [sp, #64]
   311f4:	ldp	d9, d8, [sp, #48]
   311f8:	ldp	d11, d10, [sp, #32]
   311fc:	ldp	d13, d12, [sp, #16]
   31200:	ldr	d14, [sp], #160
   31204:	ret

0000000000031208 <__gmpn_scan0@@Base>:
   31208:	lsr	x8, x1, #3
   3120c:	and	x8, x8, #0x1ffffffffffffff8
   31210:	add	x8, x0, x8
   31214:	ldr	x9, [x8], #8
   31218:	mov	x10, #0xffffffffffffffff    	// #-1
   3121c:	lsl	x10, x10, x1
   31220:	bics	x9, x10, x9
   31224:	b.ne	31238 <__gmpn_scan0@@Base+0x30>  // b.any
   31228:	ldr	x9, [x8], #8
   3122c:	cmn	x9, #0x1
   31230:	b.eq	31228 <__gmpn_scan0@@Base+0x20>  // b.none
   31234:	mvn	x9, x9
   31238:	rbit	x9, x9
   3123c:	clz	x9, x9
   31240:	sub	x8, x8, x0
   31244:	orr	x9, x9, #0xffffffffffffffc0
   31248:	add	x0, x9, x8, lsl #3
   3124c:	ret

0000000000031250 <__gmpn_scan1@@Base>:
   31250:	lsr	x8, x1, #3
   31254:	and	x8, x8, #0x1ffffffffffffff8
   31258:	add	x8, x0, x8
   3125c:	ldr	x9, [x8], #8
   31260:	mov	x10, #0xffffffffffffffff    	// #-1
   31264:	lsl	x10, x10, x1
   31268:	ands	x9, x9, x10
   3126c:	b.ne	31278 <__gmpn_scan1@@Base+0x28>  // b.any
   31270:	ldr	x9, [x8], #8
   31274:	cbz	x9, 31270 <__gmpn_scan1@@Base+0x20>
   31278:	rbit	x9, x9
   3127c:	clz	x9, x9
   31280:	sub	x8, x8, x0
   31284:	orr	x9, x9, #0xffffffffffffffc0
   31288:	add	x0, x9, x8, lsl #3
   3128c:	ret

0000000000031290 <__gmpn_popcount@@Base>:
   31290:	mov	x11, #0x1fff                	// #8191
   31294:	cmp	x1, x11
   31298:	b.hi	31374 <__gmpn_popcount@@Base+0xe4>  // b.pmore
   3129c:	movi	v4.16b, #0x0
   312a0:	movi	v5.16b, #0x0
   312a4:	tbz	w1, #0, 312b8 <__gmpn_popcount@@Base+0x28>
   312a8:	sub	x1, x1, #0x1
   312ac:	ld1	{v0.1d}, [x0], #8
   312b0:	cnt	v6.16b, v0.16b
   312b4:	uadalp	v4.8h, v6.16b
   312b8:	tbz	w1, #1, 312cc <__gmpn_popcount@@Base+0x3c>
   312bc:	sub	x1, x1, #0x2
   312c0:	ld1	{v0.2d}, [x0], #16
   312c4:	cnt	v6.16b, v0.16b
   312c8:	uadalp	v4.8h, v6.16b
   312cc:	tbz	w1, #2, 312f0 <__gmpn_popcount@@Base+0x60>
   312d0:	subs	x1, x1, #0x4
   312d4:	ld1	{v0.2d, v1.2d}, [x0], #32
   312d8:	b.ls	31348 <__gmpn_popcount@@Base+0xb8>  // b.plast
   312dc:	ld1	{v2.2d, v3.2d}, [x0], #32
   312e0:	sub	x1, x1, #0x4
   312e4:	cnt	v6.16b, v0.16b
   312e8:	cnt	v7.16b, v1.16b
   312ec:	b	31324 <__gmpn_popcount@@Base+0x94>
   312f0:	subs	x1, x1, #0x8
   312f4:	b.cc	3135c <__gmpn_popcount@@Base+0xcc>  // b.lo, b.ul, b.last
   312f8:	ld1	{v2.2d, v3.2d}, [x0], #32
   312fc:	ld1	{v0.2d, v1.2d}, [x0], #32
   31300:	cnt	v6.16b, v2.16b
   31304:	cnt	v7.16b, v3.16b
   31308:	subs	x1, x1, #0x8
   3130c:	b.cc	31340 <__gmpn_popcount@@Base+0xb0>  // b.lo, b.ul, b.last
   31310:	ld1	{v2.2d, v3.2d}, [x0], #32
   31314:	uadalp	v4.8h, v6.16b
   31318:	cnt	v6.16b, v0.16b
   3131c:	uadalp	v5.8h, v7.16b
   31320:	cnt	v7.16b, v1.16b
   31324:	ld1	{v0.2d, v1.2d}, [x0], #32
   31328:	subs	x1, x1, #0x8
   3132c:	uadalp	v4.8h, v6.16b
   31330:	cnt	v6.16b, v2.16b
   31334:	uadalp	v5.8h, v7.16b
   31338:	cnt	v7.16b, v3.16b
   3133c:	b.cs	31310 <__gmpn_popcount@@Base+0x80>  // b.hs, b.nlast
   31340:	uadalp	v4.8h, v6.16b
   31344:	uadalp	v5.8h, v7.16b
   31348:	cnt	v6.16b, v0.16b
   3134c:	cnt	v7.16b, v1.16b
   31350:	uadalp	v4.8h, v6.16b
   31354:	uadalp	v5.8h, v7.16b
   31358:	add	v4.8h, v4.8h, v5.8h
   3135c:	uaddlp	v4.4s, v4.8h
   31360:	uaddlp	v4.2d, v4.4s
   31364:	mov	x0, v4.d[0]
   31368:	mov	x1, v4.d[1]
   3136c:	add	x0, x0, x1
   31370:	ret
   31374:	mov	x8, x30
   31378:	mov	x7, x1
   3137c:	mov	x4, #0x0                   	// #0
   31380:	mov	x9, #0xff80                	// #65408
   31384:	mov	x10, #0x1ff0                	// #8176
   31388:	add	x5, x0, x9
   3138c:	mov	x1, #0x1fe8                	// #8168
   31390:	movi	v4.16b, #0x0
   31394:	movi	v5.16b, #0x0
   31398:	bl	312f8 <__gmpn_popcount@@Base+0x68>
   3139c:	add	x4, x4, x0
   313a0:	mov	x0, x5
   313a4:	sub	x7, x7, x10
   313a8:	cmp	x7, x11
   313ac:	b.hi	31388 <__gmpn_popcount@@Base+0xf8>  // b.pmore
   313b0:	mov	x1, x7
   313b4:	bl	3129c <__gmpn_popcount@@Base+0xc>
   313b8:	add	x0, x4, x0
   313bc:	mov	x30, x8
   313c0:	ret
   313c4:	nop

00000000000313c8 <__gmpn_hamdist@@Base>:
   313c8:	mov	x11, #0x1fff                	// #8191
   313cc:	cmp	x2, x11
   313d0:	b.hi	314fc <__gmpn_hamdist@@Base+0x134>  // b.pmore
   313d4:	movi	v4.16b, #0x0
   313d8:	movi	v5.16b, #0x0
   313dc:	tbz	w2, #0, 313f8 <__gmpn_hamdist@@Base+0x30>
   313e0:	sub	x2, x2, #0x1
   313e4:	ld1	{v0.1d}, [x0], #8
   313e8:	ld1	{v16.1d}, [x1], #8
   313ec:	eor	v0.16b, v0.16b, v16.16b
   313f0:	cnt	v6.16b, v0.16b
   313f4:	uadalp	v4.8h, v6.16b
   313f8:	tbz	w2, #1, 31414 <__gmpn_hamdist@@Base+0x4c>
   313fc:	sub	x2, x2, #0x2
   31400:	ld1	{v0.2d}, [x0], #16
   31404:	ld1	{v16.2d}, [x1], #16
   31408:	eor	v0.16b, v0.16b, v16.16b
   3140c:	cnt	v6.16b, v0.16b
   31410:	uadalp	v4.8h, v6.16b
   31414:	tbz	w2, #2, 31448 <__gmpn_hamdist@@Base+0x80>
   31418:	subs	x2, x2, #0x4
   3141c:	ld1	{v0.2d, v1.2d}, [x0], #32
   31420:	ld1	{v16.2d, v17.2d}, [x1], #32
   31424:	b.ls	314c8 <__gmpn_hamdist@@Base+0x100>  // b.plast
   31428:	ld1	{v2.2d, v3.2d}, [x0], #32
   3142c:	ld1	{v18.2d, v19.2d}, [x1], #32
   31430:	eor	v0.16b, v0.16b, v16.16b
   31434:	eor	v1.16b, v1.16b, v17.16b
   31438:	sub	x2, x2, #0x4
   3143c:	cnt	v6.16b, v0.16b
   31440:	cnt	v7.16b, v1.16b
   31444:	b	31498 <__gmpn_hamdist@@Base+0xd0>
   31448:	subs	x2, x2, #0x8
   3144c:	b.cc	314e4 <__gmpn_hamdist@@Base+0x11c>  // b.lo, b.ul, b.last
   31450:	ld1	{v2.2d, v3.2d}, [x0], #32
   31454:	ld1	{v0.2d, v1.2d}, [x0], #32
   31458:	ld1	{v18.2d, v19.2d}, [x1], #32
   3145c:	ld1	{v16.2d, v17.2d}, [x1], #32
   31460:	eor	v2.16b, v2.16b, v18.16b
   31464:	eor	v3.16b, v3.16b, v19.16b
   31468:	cnt	v6.16b, v2.16b
   3146c:	cnt	v7.16b, v3.16b
   31470:	subs	x2, x2, #0x8
   31474:	b.cc	314c0 <__gmpn_hamdist@@Base+0xf8>  // b.lo, b.ul, b.last
   31478:	ld1	{v2.2d, v3.2d}, [x0], #32
   3147c:	ld1	{v18.2d, v19.2d}, [x1], #32
   31480:	eor	v0.16b, v0.16b, v16.16b
   31484:	eor	v1.16b, v1.16b, v17.16b
   31488:	uadalp	v4.8h, v6.16b
   3148c:	cnt	v6.16b, v0.16b
   31490:	uadalp	v5.8h, v7.16b
   31494:	cnt	v7.16b, v1.16b
   31498:	ld1	{v0.2d, v1.2d}, [x0], #32
   3149c:	ld1	{v16.2d, v17.2d}, [x1], #32
   314a0:	eor	v2.16b, v2.16b, v18.16b
   314a4:	eor	v3.16b, v3.16b, v19.16b
   314a8:	subs	x2, x2, #0x8
   314ac:	uadalp	v4.8h, v6.16b
   314b0:	cnt	v6.16b, v2.16b
   314b4:	uadalp	v5.8h, v7.16b
   314b8:	cnt	v7.16b, v3.16b
   314bc:	b.cs	31478 <__gmpn_hamdist@@Base+0xb0>  // b.hs, b.nlast
   314c0:	uadalp	v4.8h, v6.16b
   314c4:	uadalp	v5.8h, v7.16b
   314c8:	eor	v0.16b, v0.16b, v16.16b
   314cc:	eor	v1.16b, v1.16b, v17.16b
   314d0:	cnt	v6.16b, v0.16b
   314d4:	cnt	v7.16b, v1.16b
   314d8:	uadalp	v4.8h, v6.16b
   314dc:	uadalp	v5.8h, v7.16b
   314e0:	add	v4.8h, v4.8h, v5.8h
   314e4:	uaddlp	v4.4s, v4.8h
   314e8:	uaddlp	v4.2d, v4.4s
   314ec:	mov	x0, v4.d[0]
   314f0:	mov	x1, v4.d[1]
   314f4:	add	x0, x0, x1
   314f8:	ret
   314fc:	mov	x8, x30
   31500:	mov	x7, x2
   31504:	mov	x4, #0x0                   	// #0
   31508:	mov	x9, #0xff80                	// #65408
   3150c:	mov	x10, #0x1ff0                	// #8176
   31510:	add	x5, x0, x9
   31514:	add	x6, x1, x9
   31518:	mov	x2, #0x1fe8                	// #8168
   3151c:	movi	v4.16b, #0x0
   31520:	movi	v5.16b, #0x0
   31524:	bl	31450 <__gmpn_hamdist@@Base+0x88>
   31528:	add	x4, x4, x0
   3152c:	mov	x0, x5
   31530:	mov	x1, x6
   31534:	sub	x7, x7, x10
   31538:	cmp	x7, x11
   3153c:	b.hi	31510 <__gmpn_hamdist@@Base+0x148>  // b.pmore
   31540:	mov	x2, x7
   31544:	bl	313d4 <__gmpn_hamdist@@Base+0xc>
   31548:	add	x0, x4, x0
   3154c:	mov	x30, x8
   31550:	ret

0000000000031554 <__gmpn_cmp@@Base>:
   31554:	sub	x8, x0, #0x8
   31558:	sub	x9, x1, #0x8
   3155c:	subs	x10, x2, #0x1
   31560:	b.lt	31588 <__gmpn_cmp@@Base+0x34>  // b.tstop
   31564:	lsl	x11, x2, #3
   31568:	ldr	x12, [x8, x11]
   3156c:	ldr	x11, [x9, x11]
   31570:	mov	x2, x10
   31574:	cmp	x12, x11
   31578:	b.eq	3155c <__gmpn_cmp@@Base+0x8>  // b.none
   3157c:	mov	w8, #0x1                   	// #1
   31580:	cneg	w0, w8, ls  // ls = plast
   31584:	ret
   31588:	mov	w0, wzr
   3158c:	ret

0000000000031590 <__gmpn_zero_p@@Base>:
   31590:	sub	x8, x0, #0x8
   31594:	ldr	x9, [x8, x1, lsl #3]
   31598:	cbnz	x9, 315ac <__gmpn_zero_p@@Base+0x1c>
   3159c:	sub	x1, x1, #0x1
   315a0:	cbnz	x1, 31594 <__gmpn_zero_p@@Base+0x4>
   315a4:	mov	w0, #0x1                   	// #1
   315a8:	ret
   315ac:	mov	w0, wzr
   315b0:	ret

00000000000315b4 <__gmpn_perfect_square_p@@Base>:
   315b4:	stp	x29, x30, [sp, #-32]!
   315b8:	stp	x20, x19, [sp, #16]
   315bc:	mov	x29, sp
   315c0:	sub	sp, sp, #0x10
   315c4:	ldr	x8, [x0]
   315c8:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   315cc:	add	x10, x10, #0x9b0
   315d0:	ubfx	x9, x8, #6, #2
   315d4:	ldr	x9, [x10, x9, lsl #3]
   315d8:	lsr	x8, x9, x8
   315dc:	tbz	w8, #0, 31760 <__gmpn_perfect_square_p@@Base+0x1ac>
   315e0:	mov	x19, x0
   315e4:	mov	x20, x1
   315e8:	bl	cf60 <__gmpn_mod_34lsub1@plt>
   315ec:	mov	x9, #0x2fd3                	// #12243
   315f0:	and	x8, x0, #0xffffffffffff
   315f4:	movk	x9, #0xd2fd, lsl #16
   315f8:	movk	x9, #0xfd2f, lsl #32
   315fc:	add	x8, x8, x0, lsr #48
   31600:	mul	x9, x8, x9
   31604:	mov	w10, #0x5b                  	// #91
   31608:	and	x9, x9, #0x1ffffffffffff
   3160c:	mul	x9, x9, x10
   31610:	mov	x10, #0x20e1                	// #8417
   31614:	movk	x10, #0x9538, lsl #16
   31618:	mov	w11, #0x1240                	// #4672
   3161c:	lsr	x9, x9, #49
   31620:	movk	x10, #0xa206, lsl #32
   31624:	movk	w11, #0x219, lsl #16
   31628:	cmp	w9, #0x40
   3162c:	movk	x10, #0x8850, lsl #48
   31630:	csel	x10, x10, x11, cc  // cc = lo, ul, last
   31634:	lsr	x9, x10, x9
   31638:	tbz	w9, #0, 31760 <__gmpn_perfect_square_p@@Base+0x1ac>
   3163c:	mov	x9, #0xfcfd                	// #64765
   31640:	movk	x9, #0xfcfc, lsl #16
   31644:	movk	x9, #0xfcfc, lsl #32
   31648:	mul	x9, x8, x9
   3164c:	mov	w10, #0x55                  	// #85
   31650:	and	x9, x9, #0x1ffffffffffff
   31654:	mul	x9, x9, x10
   31658:	mov	x10, #0xa105                	// #41221
   3165c:	movk	x10, #0x4206, lsl #16
   31660:	mov	w11, #0x2158                	// #8536
   31664:	lsr	x9, x9, #49
   31668:	movk	x10, #0x8c4b, lsl #32
   3166c:	movk	w11, #0x8, lsl #16
   31670:	cmp	w9, #0x40
   31674:	movk	x10, #0x10b4, lsl #48
   31678:	csel	x10, x10, x11, cc  // cc = lo, ul, last
   3167c:	lsr	x9, x10, x9
   31680:	tbz	w9, #0, 31760 <__gmpn_perfect_square_p@@Base+0x1ac>
   31684:	mov	x9, #0x8e39                	// #36409
   31688:	movk	x9, #0x38e3, lsl #16
   3168c:	movk	x9, #0xe38e, lsl #32
   31690:	mul	x9, x8, x9
   31694:	and	x9, x9, #0x1ffffffffffff
   31698:	add	x9, x9, x9, lsl #3
   3169c:	lsr	x9, x9, #49
   316a0:	mov	w10, #0x93                  	// #147
   316a4:	lsr	x9, x10, x9
   316a8:	tbz	w9, #0, 31760 <__gmpn_perfect_square_p@@Base+0x1ac>
   316ac:	mov	x9, #0xa3a1                	// #41889
   316b0:	movk	x9, #0x5f02, lsl #16
   316b4:	movk	x9, #0xfd5c, lsl #32
   316b8:	mul	x8, x8, x9
   316bc:	mov	w10, #0x61                  	// #97
   316c0:	and	x8, x8, #0x1ffffffffffff
   316c4:	mov	x9, #0x1b5f                	// #7007
   316c8:	mov	x11, #0x8b47                	// #35655
   316cc:	mul	x8, x8, x10
   316d0:	movk	x9, #0x8b45, lsl #16
   316d4:	movk	x11, #0xeb62, lsl #16
   316d8:	lsr	x8, x8, #49
   316dc:	movk	x9, #0x981b, lsl #32
   316e0:	movk	x11, #0x1, lsl #32
   316e4:	cmp	w8, #0x40
   316e8:	movk	x9, #0x6067, lsl #48
   316ec:	csel	x9, x9, x11, cc  // cc = lo, ul, last
   316f0:	lsr	x8, x9, x8
   316f4:	tbz	w8, #0, 31760 <__gmpn_perfect_square_p@@Base+0x1ac>
   316f8:	add	x8, x20, #0x1
   316fc:	add	x9, x20, #0x2
   31700:	cmp	x8, #0x0
   31704:	csinc	x8, x9, x20, lt  // lt = tstop
   31708:	lsl	x8, x8, #2
   3170c:	and	x1, x8, #0xfffffffffffffff8
   31710:	mov	w8, #0x7f00                	// #32512
   31714:	cmp	x1, x8
   31718:	stur	xzr, [x29, #-8]
   3171c:	b.hi	31778 <__gmpn_perfect_square_p@@Base+0x1c4>  // b.pmore
   31720:	add	x9, x1, #0xf
   31724:	mov	x8, sp
   31728:	and	x9, x9, #0xfffffffffffffff0
   3172c:	sub	x0, x8, x9
   31730:	mov	sp, x0
   31734:	mov	x1, xzr
   31738:	mov	x2, x19
   3173c:	mov	x3, x20
   31740:	bl	d3b0 <__gmpn_sqrtrem@plt>
   31744:	ldur	x8, [x29, #-8]
   31748:	cmp	x0, #0x0
   3174c:	cset	w19, eq  // eq = none
   31750:	cbz	x8, 31764 <__gmpn_perfect_square_p@@Base+0x1b0>
   31754:	mov	x0, x8
   31758:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3175c:	b	31764 <__gmpn_perfect_square_p@@Base+0x1b0>
   31760:	mov	w19, wzr
   31764:	mov	w0, w19
   31768:	mov	sp, x29
   3176c:	ldp	x20, x19, [sp, #16]
   31770:	ldp	x29, x30, [sp], #32
   31774:	ret
   31778:	sub	x0, x29, #0x8
   3177c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   31780:	b	31734 <__gmpn_perfect_square_p@@Base+0x180>

0000000000031784 <__gmpn_perfect_power_p@@Base>:
   31784:	stp	x29, x30, [sp, #-80]!
   31788:	stp	x26, x25, [sp, #16]
   3178c:	stp	x24, x23, [sp, #32]
   31790:	stp	x22, x21, [sp, #48]
   31794:	stp	x20, x19, [sp, #64]
   31798:	mov	x29, sp
   3179c:	sub	sp, sp, #0x30
   317a0:	mov	x19, x1
   317a4:	mov	x20, x0
   317a8:	mov	x22, x1
   317ac:	stur	x1, [x29, #-8]
   317b0:	tbnz	x1, #63, 317bc <__gmpn_perfect_power_p@@Base+0x38>
   317b4:	cbnz	x22, 317c8 <__gmpn_perfect_power_p@@Base+0x44>
   317b8:	b	317dc <__gmpn_perfect_power_p@@Base+0x58>
   317bc:	neg	x22, x19
   317c0:	stur	x22, [x29, #-8]
   317c4:	cbz	x22, 317dc <__gmpn_perfect_power_p@@Base+0x58>
   317c8:	cmp	x22, #0x1
   317cc:	b.ne	317e4 <__gmpn_perfect_power_p@@Base+0x60>  // b.any
   317d0:	ldr	x8, [x20]
   317d4:	cmp	x8, #0x1
   317d8:	b.ne	317e4 <__gmpn_perfect_power_p@@Base+0x60>  // b.any
   317dc:	mov	w19, #0x1                   	// #1
   317e0:	b	31a68 <__gmpn_perfect_power_p@@Base+0x2e4>
   317e4:	mov	x0, x20
   317e8:	mov	x1, xzr
   317ec:	stur	xzr, [x29, #-40]
   317f0:	bl	d150 <__gmpn_scan1@plt>
   317f4:	mov	x23, x0
   317f8:	cbz	x0, 3180c <__gmpn_perfect_power_p@@Base+0x88>
   317fc:	subs	x8, x23, #0x1
   31800:	b.ne	31814 <__gmpn_perfect_power_p@@Base+0x90>  // b.any
   31804:	mov	w19, wzr
   31808:	b	31a68 <__gmpn_perfect_power_p@@Base+0x2e4>
   3180c:	mov	x26, x23
   31810:	b	318c4 <__gmpn_perfect_power_p@@Base+0x140>
   31814:	lsr	x9, x23, #6
   31818:	add	x10, x9, #0x1
   3181c:	cmp	x10, x22
   31820:	b.ne	3184c <__gmpn_perfect_power_p@@Base+0xc8>  // b.any
   31824:	ldr	x10, [x20, x9, lsl #3]
   31828:	sub	x11, x10, #0x1
   3182c:	tst	x10, x11
   31830:	b.ne	3184c <__gmpn_perfect_power_p@@Base+0xc8>  // b.any
   31834:	cmp	x19, #0x0
   31838:	cset	w9, ge  // ge = tcont
   3183c:	tst	x23, x8
   31840:	cset	w8, ne  // ne = any
   31844:	orr	w19, w8, w9
   31848:	b	31a68 <__gmpn_perfect_power_p@@Base+0x2e4>
   3184c:	and	x24, x23, #0x3f
   31850:	sub	x22, x22, x9
   31854:	add	x20, x20, x9, lsl #3
   31858:	stur	x22, [x29, #-8]
   3185c:	cbz	x24, 318c0 <__gmpn_perfect_power_p@@Base+0x13c>
   31860:	lsl	x1, x22, #3
   31864:	mov	w8, #0x7f00                	// #32512
   31868:	cmp	x1, x8
   3186c:	b.hi	31a90 <__gmpn_perfect_power_p@@Base+0x30c>  // b.pmore
   31870:	add	x9, x1, #0xf
   31874:	mov	x8, sp
   31878:	and	x9, x9, #0xfffffffffffffff0
   3187c:	sub	x21, x8, x9
   31880:	mov	sp, x21
   31884:	mov	x0, x21
   31888:	mov	x1, x20
   3188c:	mov	x2, x22
   31890:	mov	w3, w24
   31894:	bl	c1a0 <__gmpn_rshift@plt>
   31898:	ldur	x8, [x29, #-8]
   3189c:	mov	w26, #0x1                   	// #1
   318a0:	mov	x20, x21
   318a4:	add	x9, x21, x8, lsl #3
   318a8:	ldur	x9, [x9, #-8]
   318ac:	cmp	x9, #0x0
   318b0:	cset	w9, eq  // eq = none
   318b4:	sub	x22, x8, x9
   318b8:	stur	x22, [x29, #-8]
   318bc:	b	318c4 <__gmpn_perfect_power_p@@Base+0x140>
   318c0:	mov	x26, xzr
   318c4:	cmp	x22, #0x64
   318c8:	mov	w8, #0x2                   	// #2
   318cc:	cset	w9, gt
   318d0:	csinc	x8, x8, xzr, gt
   318d4:	cmp	x22, #0x14
   318d8:	csel	x25, x9, x8, le
   318dc:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   318e0:	add	x8, x8, #0x9d8
   318e4:	ldrh	w24, [x8, x25, lsl #1]
   318e8:	sub	x3, x29, #0x1c
   318ec:	mov	x0, x20
   318f0:	mov	x1, x22
   318f4:	mov	x2, x24
   318f8:	stur	x23, [x29, #-16]
   318fc:	stur	wzr, [x29, #-28]
   31900:	bl	c610 <__gmpn_trialdiv@plt>
   31904:	cbz	x0, 319f4 <__gmpn_perfect_power_p@@Base+0x270>
   31908:	cbnz	x26, 31930 <__gmpn_perfect_power_p@@Base+0x1ac>
   3190c:	lsl	x1, x22, #3
   31910:	mov	w8, #0x7f00                	// #32512
   31914:	cmp	x1, x8
   31918:	b.hi	31aa4 <__gmpn_perfect_power_p@@Base+0x320>  // b.pmore
   3191c:	add	x9, x1, #0xf
   31920:	mov	x8, sp
   31924:	and	x9, x9, #0xfffffffffffffff0
   31928:	sub	x21, x8, x9
   3192c:	mov	sp, x21
   31930:	adrp	x22, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   31934:	ldr	x22, [x22, #3952]
   31938:	mov	w23, #0x2                   	// #2
   3193c:	b	31958 <__gmpn_perfect_power_p@@Base+0x1d4>
   31940:	sub	x3, x29, #0x1c
   31944:	mov	x0, x21
   31948:	mov	x2, x24
   3194c:	bl	c610 <__gmpn_trialdiv@plt>
   31950:	mov	x20, x21
   31954:	cbz	x0, 31a04 <__gmpn_perfect_power_p@@Base+0x280>
   31958:	ubfx	x8, x0, #1, #7
   3195c:	ldrb	w8, [x22, x8]
   31960:	ldur	x3, [x29, #-8]
   31964:	sub	x1, x29, #0x8
   31968:	sub	x4, x29, #0x18
   3196c:	msub	x9, x0, x8, x23
   31970:	mul	x8, x9, x8
   31974:	msub	x9, x8, x0, x23
   31978:	mul	x8, x8, x9
   3197c:	msub	x9, x8, x0, x23
   31980:	mul	x8, x8, x9
   31984:	mov	w5, #0x1                   	// #1
   31988:	mov	x6, #0xffffffffffffffff    	// #-1
   3198c:	mov	x0, x21
   31990:	mov	x2, x20
   31994:	stur	x8, [x29, #-24]
   31998:	bl	cd40 <__gmpn_remove@plt>
   3199c:	ldur	x8, [x29, #-16]
   319a0:	mov	x2, x0
   319a4:	cbz	x8, 319b8 <__gmpn_perfect_power_p@@Base+0x234>
   319a8:	sub	x0, x29, #0x10
   319ac:	mov	w1, #0x1                   	// #1
   319b0:	bl	bf90 <__gmpn_gcd_1@plt>
   319b4:	mov	x2, x0
   319b8:	subs	x8, x2, #0x1
   319bc:	stur	x2, [x29, #-16]
   319c0:	b.eq	319fc <__gmpn_perfect_power_p@@Base+0x278>  // b.none
   319c4:	ldur	x1, [x29, #-8]
   319c8:	cmp	x1, #0x1
   319cc:	b.ne	31940 <__gmpn_perfect_power_p@@Base+0x1bc>  // b.any
   319d0:	ldr	x9, [x21]
   319d4:	cmp	x9, #0x1
   319d8:	b.ne	31940 <__gmpn_perfect_power_p@@Base+0x1bc>  // b.any
   319dc:	cmp	x19, #0x0
   319e0:	cset	w9, ge  // ge = tcont
   319e4:	tst	x2, x8
   319e8:	cset	w8, ne  // ne = any
   319ec:	orr	w19, w8, w9
   319f0:	b	31a60 <__gmpn_perfect_power_p@@Base+0x2dc>
   319f4:	mov	x21, x20
   319f8:	b	31a08 <__gmpn_perfect_power_p@@Base+0x284>
   319fc:	mov	w19, wzr
   31a00:	b	31a60 <__gmpn_perfect_power_p@@Base+0x2dc>
   31a04:	ldp	x23, x22, [x29, #-16]
   31a08:	add	x8, x21, x22, lsl #3
   31a0c:	ldur	x8, [x8, #-8]
   31a10:	adrp	x9, 5b000 <__gmpn_bases@@Base+0x2678>
   31a14:	add	x9, x9, #0x9e0
   31a18:	ldr	d0, [x9, x25, lsl #3]
   31a1c:	adrp	x10, 5b000 <__gmpn_bases@@Base+0x2678>
   31a20:	lsl	x9, x22, #6
   31a24:	ldr	d1, [x10, #2512]
   31a28:	clz	x8, x8
   31a2c:	sub	x4, x9, x8
   31a30:	ucvtf	d2, x4
   31a34:	fmul	d0, d0, d2
   31a38:	fadd	d0, d0, d1
   31a3c:	fcvtzu	x8, d0
   31a40:	lsr	x5, x19, #63
   31a44:	add	x2, x8, #0x1
   31a48:	mov	x0, x21
   31a4c:	mov	x1, x22
   31a50:	mov	x3, x23
   31a54:	stur	x2, [x29, #-24]
   31a58:	bl	31abc <__gmpn_perfect_power_p@@Base+0x338>
   31a5c:	mov	w19, w0
   31a60:	ldur	x0, [x29, #-40]
   31a64:	cbnz	x0, 31a88 <__gmpn_perfect_power_p@@Base+0x304>
   31a68:	mov	w0, w19
   31a6c:	mov	sp, x29
   31a70:	ldp	x20, x19, [sp, #64]
   31a74:	ldp	x22, x21, [sp, #48]
   31a78:	ldp	x24, x23, [sp, #32]
   31a7c:	ldp	x26, x25, [sp, #16]
   31a80:	ldp	x29, x30, [sp], #80
   31a84:	ret
   31a88:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   31a8c:	b	31a68 <__gmpn_perfect_power_p@@Base+0x2e4>
   31a90:	sub	x0, x29, #0x28
   31a94:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   31a98:	ldur	x22, [x29, #-8]
   31a9c:	mov	x21, x0
   31aa0:	b	31884 <__gmpn_perfect_power_p@@Base+0x100>
   31aa4:	mov	x22, x0
   31aa8:	sub	x0, x29, #0x28
   31aac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   31ab0:	mov	x21, x0
   31ab4:	mov	x0, x22
   31ab8:	b	31930 <__gmpn_perfect_power_p@@Base+0x1ac>
   31abc:	stp	x29, x30, [sp, #-96]!
   31ac0:	stp	x28, x27, [sp, #16]
   31ac4:	stp	x26, x25, [sp, #32]
   31ac8:	stp	x24, x23, [sp, #48]
   31acc:	stp	x22, x21, [sp, #64]
   31ad0:	stp	x20, x19, [sp, #80]
   31ad4:	mov	x29, sp
   31ad8:	sub	sp, sp, #0x240
   31adc:	mov	x19, sp
   31ae0:	mov	x22, x0
   31ae4:	add	x0, x19, #0x18
   31ae8:	str	w5, [x19, #12]
   31aec:	mov	x20, x4
   31af0:	mov	x23, x3
   31af4:	mov	x25, x2
   31af8:	mov	x21, x1
   31afc:	str	xzr, [x19, #16]
   31b00:	bl	d1a0 <__gmp_init_primesieve@plt>
   31b04:	mov	w8, #0x38                  	// #56
   31b08:	mul	x1, x21, x8
   31b0c:	mov	w8, #0x7f00                	// #32512
   31b10:	cmp	x1, x8
   31b14:	add	x26, x20, #0x3
   31b18:	b.hi	31c84 <__gmpn_perfect_power_p@@Base+0x500>  // b.pmore
   31b1c:	add	x9, x1, #0xf
   31b20:	mov	x8, sp
   31b24:	and	x9, x9, #0xfffffffffffffff0
   31b28:	sub	x24, x8, x9
   31b2c:	mov	sp, x24
   31b30:	lsl	x2, x21, #3
   31b34:	str	x26, [x19]
   31b38:	lsr	x28, x26, #1
   31b3c:	add	x26, x24, x2
   31b40:	add	x27, x26, x2
   31b44:	cbz	x21, 31b54 <__gmpn_perfect_power_p@@Base+0x3d0>
   31b48:	mov	x0, x26
   31b4c:	mov	w1, wzr
   31b50:	bl	c5f0 <memset@plt>
   31b54:	sub	x8, x28, #0x1
   31b58:	lsr	x28, x8, #6
   31b5c:	add	x2, x28, #0x1
   31b60:	mov	x0, x24
   31b64:	mov	x1, x22
   31b68:	mov	x3, x27
   31b6c:	bl	cd20 <__gmpn_binvert@plt>
   31b70:	ldr	x8, [x19]
   31b74:	ubfx	x8, x8, #1, #6
   31b78:	cbz	x8, 31b94 <__gmpn_perfect_power_p@@Base+0x410>
   31b7c:	lsl	x9, x28, #3
   31b80:	ldr	x10, [x24, x9]
   31b84:	mov	x11, #0xffffffffffffffff    	// #-1
   31b88:	lsl	x8, x11, x8
   31b8c:	bic	x8, x10, x8
   31b90:	str	x8, [x24, x9]
   31b94:	ldr	w8, [x19, #12]
   31b98:	cbz	w8, 31ba4 <__gmpn_perfect_power_p@@Base+0x420>
   31b9c:	add	x0, x19, #0x18
   31ba0:	bl	cc80 <__gmp_nextprime@plt>
   31ba4:	cbz	x23, 31c44 <__gmpn_perfect_power_p@@Base+0x4c0>
   31ba8:	add	x8, x23, #0x1
   31bac:	cmp	x8, x25
   31bb0:	add	x0, x19, #0x18
   31bb4:	csinc	x25, x25, x23, hi  // hi = pmore
   31bb8:	bl	cc80 <__gmp_nextprime@plt>
   31bbc:	cmp	x0, x25
   31bc0:	b.cs	31c54 <__gmpn_perfect_power_p@@Base+0x4d0>  // b.hs, b.nlast
   31bc4:	mov	x2, x0
   31bc8:	b	31be0 <__gmpn_perfect_power_p@@Base+0x45c>
   31bcc:	add	x0, x19, #0x18
   31bd0:	bl	cc80 <__gmp_nextprime@plt>
   31bd4:	mov	x2, x0
   31bd8:	cmp	x0, x25
   31bdc:	b.cs	31c54 <__gmpn_perfect_power_p@@Base+0x4d0>  // b.hs, b.nlast
   31be0:	udiv	x8, x23, x2
   31be4:	msub	x8, x8, x2, x23
   31be8:	cbnz	x8, 31bcc <__gmpn_perfect_power_p@@Base+0x448>
   31bec:	mov	x0, x26
   31bf0:	mov	x1, x22
   31bf4:	mov	x3, x24
   31bf8:	mov	x4, x21
   31bfc:	mov	x5, x20
   31c00:	mov	x6, x27
   31c04:	bl	31c94 <__gmpn_perfect_power_p@@Base+0x510>
   31c08:	cbz	w0, 31bcc <__gmpn_perfect_power_p@@Base+0x448>
   31c0c:	mov	w20, #0x1                   	// #1
   31c10:	ldr	x0, [x19, #16]
   31c14:	cbz	x0, 31c60 <__gmpn_perfect_power_p@@Base+0x4dc>
   31c18:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   31c1c:	b	31c60 <__gmpn_perfect_power_p@@Base+0x4dc>
   31c20:	mov	x2, x0
   31c24:	mov	x0, x26
   31c28:	mov	x1, x22
   31c2c:	mov	x3, x24
   31c30:	mov	x4, x21
   31c34:	mov	x5, x20
   31c38:	mov	x6, x27
   31c3c:	bl	31c94 <__gmpn_perfect_power_p@@Base+0x510>
   31c40:	cbnz	w0, 31c0c <__gmpn_perfect_power_p@@Base+0x488>
   31c44:	add	x0, x19, #0x18
   31c48:	bl	cc80 <__gmp_nextprime@plt>
   31c4c:	cmp	x0, x25
   31c50:	b.cc	31c20 <__gmpn_perfect_power_p@@Base+0x49c>  // b.lo, b.ul, b.last
   31c54:	mov	w20, wzr
   31c58:	ldr	x0, [x19, #16]
   31c5c:	cbnz	x0, 31c18 <__gmpn_perfect_power_p@@Base+0x494>
   31c60:	mov	w0, w20
   31c64:	mov	sp, x29
   31c68:	ldp	x20, x19, [sp, #80]
   31c6c:	ldp	x22, x21, [sp, #64]
   31c70:	ldp	x24, x23, [sp, #48]
   31c74:	ldp	x26, x25, [sp, #32]
   31c78:	ldp	x28, x27, [sp, #16]
   31c7c:	ldp	x29, x30, [sp], #96
   31c80:	ret
   31c84:	add	x0, x19, #0x10
   31c88:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   31c8c:	mov	x24, x0
   31c90:	b	31b30 <__gmpn_perfect_power_p@@Base+0x3ac>
   31c94:	stp	x29, x30, [sp, #-96]!
   31c98:	stp	x24, x23, [sp, #48]
   31c9c:	stp	x22, x21, [sp, #64]
   31ca0:	stp	x20, x19, [sp, #80]
   31ca4:	mov	x20, x6
   31ca8:	mov	x21, x5
   31cac:	mov	x22, x4
   31cb0:	mov	x23, x1
   31cb4:	cmp	x2, #0x2
   31cb8:	mov	x19, x0
   31cbc:	str	x27, [sp, #16]
   31cc0:	stp	x26, x25, [sp, #32]
   31cc4:	mov	x29, sp
   31cc8:	b.ne	31d34 <__gmpn_perfect_power_p@@Base+0x5b0>  // b.any
   31ccc:	add	x8, x21, #0x1
   31cd0:	lsr	x25, x8, #1
   31cd4:	lsr	x26, x8, #7
   31cd8:	mov	x0, x19
   31cdc:	mov	x1, x3
   31ce0:	mov	x2, x25
   31ce4:	mov	x3, x20
   31ce8:	add	x24, x26, #0x1
   31cec:	bl	c750 <__gmpn_bsqrtinv@plt>
   31cf0:	cbz	w0, 31dc8 <__gmpn_perfect_power_p@@Base+0x644>
   31cf4:	lsl	x27, x26, #3
   31cf8:	ldr	x8, [x19, x27]
   31cfc:	mov	x9, #0xffffffffffffffff    	// #-1
   31d00:	lsl	x9, x9, x25
   31d04:	mvn	x25, x9
   31d08:	bic	x8, x8, x9
   31d0c:	str	x8, [x19, x27]
   31d10:	mov	x8, x26
   31d14:	add	x9, x8, #0x1
   31d18:	cmp	x9, #0x1
   31d1c:	b.lt	31de0 <__gmpn_perfect_power_p@@Base+0x65c>  // b.tstop
   31d20:	ldr	x9, [x19, x8, lsl #3]
   31d24:	sub	x8, x8, #0x1
   31d28:	cbz	x9, 31d14 <__gmpn_perfect_power_p@@Base+0x590>
   31d2c:	add	x3, x8, #0x2
   31d30:	b	31de4 <__gmpn_perfect_power_p@@Base+0x660>
   31d34:	sub	x8, x21, #0x1
   31d38:	udiv	x8, x8, x2
   31d3c:	lsr	x24, x8, #6
   31d40:	mov	x25, x2
   31d44:	add	x26, x24, #0x1
   31d48:	mov	x0, x19
   31d4c:	mov	x1, x3
   31d50:	mov	x2, x26
   31d54:	mov	x3, x25
   31d58:	mov	x4, x20
   31d5c:	add	w27, w8, #0x1
   31d60:	bl	c6b0 <__gmpn_brootinv@plt>
   31d64:	ands	x8, x27, #0x3f
   31d68:	b.eq	31d84 <__gmpn_perfect_power_p@@Base+0x600>  // b.none
   31d6c:	lsl	x9, x24, #3
   31d70:	ldr	x10, [x19, x9]
   31d74:	mov	x11, #0xffffffffffffffff    	// #-1
   31d78:	lsl	x8, x11, x8
   31d7c:	bic	x8, x10, x8
   31d80:	str	x8, [x19, x9]
   31d84:	mov	x24, x26
   31d88:	subs	x26, x26, #0x1
   31d8c:	b.lt	31d9c <__gmpn_perfect_power_p@@Base+0x618>  // b.tstop
   31d90:	ldr	x8, [x19, x26, lsl #3]
   31d94:	cbz	x8, 31d84 <__gmpn_perfect_power_p@@Base+0x600>
   31d98:	b	31da0 <__gmpn_perfect_power_p@@Base+0x61c>
   31d9c:	mov	x24, xzr
   31da0:	mov	x0, x23
   31da4:	mov	x1, x22
   31da8:	mov	x2, x19
   31dac:	mov	x3, x24
   31db0:	mov	x4, x25
   31db4:	mov	x5, x21
   31db8:	mov	x6, x20
   31dbc:	bl	31eb0 <__gmpn_perfect_power_p@@Base+0x72c>
   31dc0:	cbnz	w0, 31e04 <__gmpn_perfect_power_p@@Base+0x680>
   31dc4:	cbz	x24, 31dd8 <__gmpn_perfect_power_p@@Base+0x654>
   31dc8:	lsl	x2, x24, #3
   31dcc:	mov	x0, x19
   31dd0:	mov	w1, wzr
   31dd4:	bl	c5f0 <memset@plt>
   31dd8:	mov	w0, wzr
   31ddc:	b	31e08 <__gmpn_perfect_power_p@@Base+0x684>
   31de0:	mov	x3, xzr
   31de4:	mov	w4, #0x2                   	// #2
   31de8:	mov	x0, x23
   31dec:	mov	x1, x22
   31df0:	mov	x2, x19
   31df4:	mov	x5, x21
   31df8:	mov	x6, x20
   31dfc:	bl	31eb0 <__gmpn_perfect_power_p@@Base+0x72c>
   31e00:	cbz	w0, 31e24 <__gmpn_perfect_power_p@@Base+0x6a0>
   31e04:	mov	w0, #0x1                   	// #1
   31e08:	ldp	x20, x19, [sp, #80]
   31e0c:	ldp	x22, x21, [sp, #64]
   31e10:	ldp	x24, x23, [sp, #48]
   31e14:	ldp	x26, x25, [sp, #32]
   31e18:	ldr	x27, [sp, #16]
   31e1c:	ldp	x29, x30, [sp], #96
   31e20:	ret
   31e24:	ldr	x9, [x19]
   31e28:	cbz	x9, 31e34 <__gmpn_perfect_power_p@@Base+0x6b0>
   31e2c:	mov	x8, x19
   31e30:	b	31e4c <__gmpn_perfect_power_p@@Base+0x6c8>
   31e34:	mov	x8, x19
   31e38:	subs	x24, x24, #0x1
   31e3c:	str	xzr, [x8]
   31e40:	b.eq	31e68 <__gmpn_perfect_power_p@@Base+0x6e4>  // b.none
   31e44:	ldr	x9, [x8, #8]!
   31e48:	cbz	x9, 31e38 <__gmpn_perfect_power_p@@Base+0x6b4>
   31e4c:	neg	x9, x9
   31e50:	subs	x2, x24, #0x1
   31e54:	str	x9, [x8]
   31e58:	b.eq	31e68 <__gmpn_perfect_power_p@@Base+0x6e4>  // b.none
   31e5c:	add	x0, x8, #0x8
   31e60:	mov	x1, x0
   31e64:	bl	c290 <__gmpn_com@plt>
   31e68:	ldr	x8, [x19, x27]
   31e6c:	and	x8, x8, x25
   31e70:	str	x8, [x19, x27]
   31e74:	add	x8, x26, #0x1
   31e78:	cmp	x8, #0x1
   31e7c:	b.lt	31e94 <__gmpn_perfect_power_p@@Base+0x710>  // b.tstop
   31e80:	ldr	x8, [x19, x26, lsl #3]
   31e84:	sub	x26, x26, #0x1
   31e88:	cbz	x8, 31e74 <__gmpn_perfect_power_p@@Base+0x6f0>
   31e8c:	add	x24, x26, #0x2
   31e90:	b	31e98 <__gmpn_perfect_power_p@@Base+0x714>
   31e94:	mov	x24, xzr
   31e98:	mov	w4, #0x2                   	// #2
   31e9c:	mov	x0, x23
   31ea0:	mov	x1, x22
   31ea4:	mov	x2, x19
   31ea8:	mov	x3, x24
   31eac:	b	31db4 <__gmpn_perfect_power_p@@Base+0x630>
   31eb0:	stp	x29, x30, [sp, #-96]!
   31eb4:	stp	x28, x27, [sp, #16]
   31eb8:	stp	x26, x25, [sp, #32]
   31ebc:	stp	x24, x23, [sp, #48]
   31ec0:	stp	x22, x21, [sp, #64]
   31ec4:	stp	x20, x19, [sp, #80]
   31ec8:	mov	x29, sp
   31ecc:	sub	sp, sp, #0x10
   31ed0:	mov	x20, x6
   31ed4:	mov	x24, x5
   31ed8:	mov	x22, x3
   31edc:	mov	x23, x2
   31ee0:	mov	x19, x1
   31ee4:	mov	x21, x0
   31ee8:	cmp	x3, #0x1
   31eec:	stur	x4, [x29, #-8]
   31ef0:	b.ne	31f28 <__gmpn_perfect_power_p@@Base+0x7a4>  // b.any
   31ef4:	ldr	x8, [x23]
   31ef8:	cmp	x8, #0x1
   31efc:	b.ne	31f28 <__gmpn_perfect_power_p@@Base+0x7a4>  // b.any
   31f00:	mov	w24, wzr
   31f04:	mov	w0, w24
   31f08:	mov	sp, x29
   31f0c:	ldp	x20, x19, [sp, #80]
   31f10:	ldp	x22, x21, [sp, #64]
   31f14:	ldp	x24, x23, [sp, #48]
   31f18:	ldp	x26, x25, [sp, #32]
   31f1c:	ldp	x28, x27, [sp, #16]
   31f20:	ldp	x29, x30, [sp], #96
   31f24:	ret
   31f28:	asr	x8, x19, #1
   31f2c:	add	x26, x8, #0x1
   31f30:	cmp	x26, #0x2
   31f34:	b.cc	31f98 <__gmpn_perfect_power_p@@Base+0x814>  // b.lo, b.ul, b.last
   31f38:	sub	x27, x21, #0x8
   31f3c:	sub	x28, x20, #0x8
   31f40:	mov	w25, #0x1                   	// #1
   31f44:	add	x5, x20, x25, lsl #3
   31f48:	sub	x2, x29, #0x8
   31f4c:	mov	w3, #0x1                   	// #1
   31f50:	mov	x0, x20
   31f54:	mov	x1, x23
   31f58:	mov	x4, x25
   31f5c:	bl	c3c0 <__gmpn_powlo@plt>
   31f60:	mov	x8, x25
   31f64:	subs	x9, x8, #0x1
   31f68:	b.lt	31f88 <__gmpn_perfect_power_p@@Base+0x804>  // b.tstop
   31f6c:	lsl	x8, x8, #3
   31f70:	ldr	x10, [x28, x8]
   31f74:	ldr	x8, [x27, x8]
   31f78:	cmp	x10, x8
   31f7c:	mov	x8, x9
   31f80:	b.eq	31f64 <__gmpn_perfect_power_p@@Base+0x7e0>  // b.none
   31f84:	b	31f00 <__gmpn_perfect_power_p@@Base+0x77c>
   31f88:	lsl	x25, x25, #1
   31f8c:	cmp	x25, x26
   31f90:	b.cc	31f44 <__gmpn_perfect_power_p@@Base+0x7c0>  // b.lo, b.ul, b.last
   31f94:	ldur	x4, [x29, #-8]
   31f98:	add	x8, x23, x22, lsl #3
   31f9c:	ldur	x8, [x8, #-8]
   31fa0:	sub	x11, x24, #0x1
   31fa4:	mov	w24, wzr
   31fa8:	clz	x8, x8
   31fac:	mvn	x8, x8
   31fb0:	add	x10, x8, x22, lsl #6
   31fb4:	mul	x8, x10, x4
   31fb8:	cmp	x8, #0x0
   31fbc:	sub	x8, x8, #0x1
   31fc0:	cset	w9, eq  // eq = none
   31fc4:	cmp	x8, x11
   31fc8:	b.hi	31f04 <__gmpn_perfect_power_p@@Base+0x780>  // b.pmore
   31fcc:	umulh	x10, x4, x10
   31fd0:	cmp	x10, x9
   31fd4:	b.ne	31f04 <__gmpn_perfect_power_p@@Base+0x780>  // b.any
   31fd8:	adds	x8, x8, x4
   31fdc:	b.cs	3208c <__gmpn_perfect_power_p@@Base+0x908>  // b.hs, b.nlast
   31fe0:	lsr	x8, x8, #6
   31fe4:	lsl	x9, x8, #3
   31fe8:	cmp	x8, #0xfde
   31fec:	add	x1, x9, #0x10
   31ff0:	stur	xzr, [x29, #-16]
   31ff4:	b.hi	32078 <__gmpn_perfect_power_p@@Base+0x8f4>  // b.pmore
   31ff8:	add	x9, x1, #0xf
   31ffc:	mov	x8, sp
   32000:	and	x9, x9, #0x7ffffffffffffff0
   32004:	sub	x8, x8, x9
   32008:	mov	sp, x8
   3200c:	mov	x0, x20
   32010:	mov	x1, x23
   32014:	mov	x2, x22
   32018:	mov	x3, x4
   3201c:	mov	x4, x8
   32020:	bl	d210 <__gmpn_pow_1@plt>
   32024:	cmp	x0, x19
   32028:	b.ne	32054 <__gmpn_perfect_power_p@@Base+0x8d0>  // b.any
   3202c:	sub	x8, x21, #0x8
   32030:	sub	x9, x20, #0x8
   32034:	subs	x10, x19, #0x1
   32038:	b.lt	32064 <__gmpn_perfect_power_p@@Base+0x8e0>  // b.tstop
   3203c:	lsl	x11, x19, #3
   32040:	ldr	x12, [x9, x11]
   32044:	ldr	x11, [x8, x11]
   32048:	mov	x19, x10
   3204c:	cmp	x12, x11
   32050:	b.eq	32034 <__gmpn_perfect_power_p@@Base+0x8b0>  // b.none
   32054:	mov	w24, wzr
   32058:	ldur	x0, [x29, #-16]
   3205c:	cbz	x0, 31f04 <__gmpn_perfect_power_p@@Base+0x780>
   32060:	b	32070 <__gmpn_perfect_power_p@@Base+0x8ec>
   32064:	mov	w24, #0x1                   	// #1
   32068:	ldur	x0, [x29, #-16]
   3206c:	cbz	x0, 31f04 <__gmpn_perfect_power_p@@Base+0x780>
   32070:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32074:	b	31f04 <__gmpn_perfect_power_p@@Base+0x780>
   32078:	sub	x0, x29, #0x10
   3207c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32080:	ldur	x4, [x29, #-8]
   32084:	mov	x8, x0
   32088:	b	3200c <__gmpn_perfect_power_p@@Base+0x888>
   3208c:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   32090:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   32094:	add	x0, x0, #0x9f8
   32098:	add	x2, x2, #0xa02
   3209c:	mov	w1, #0x60                  	// #96
   320a0:	bl	c6c0 <__gmp_assert_fail@plt>

00000000000320a4 <__gmpn_strongfibo@@Base>:
   320a4:	stp	x29, x30, [sp, #-96]!
   320a8:	stp	x28, x27, [sp, #16]
   320ac:	stp	x26, x25, [sp, #32]
   320b0:	stp	x24, x23, [sp, #48]
   320b4:	stp	x22, x21, [sp, #64]
   320b8:	stp	x20, x19, [sp, #80]
   320bc:	mov	x29, sp
   320c0:	sub	sp, sp, #0x10
   320c4:	mov	x19, x1
   320c8:	mov	x1, xzr
   320cc:	mov	x25, x2
   320d0:	mov	x20, x0
   320d4:	bl	c690 <__gmpn_scan0@plt>
   320d8:	lsr	x8, x0, #6
   320dc:	mov	x21, x0
   320e0:	sub	x22, x19, x8
   320e4:	and	w3, w21, #0x3f
   320e8:	add	x1, x20, x8, lsl #3
   320ec:	mov	x0, x25
   320f0:	mov	x2, x22
   320f4:	cbz	w3, 323fc <__gmpn_strongfibo@@Base+0x358>
   320f8:	bl	c1a0 <__gmpn_rshift@plt>
   320fc:	ldr	x8, [x25]
   32100:	add	x9, x25, x22, lsl #3
   32104:	mov	w10, #0x7f00                	// #32512
   32108:	orr	x8, x8, #0x1
   3210c:	str	x8, [x25]
   32110:	ldur	x8, [x9, #-8]
   32114:	lsl	x9, x19, #5
   32118:	add	x1, x9, #0x30
   3211c:	stur	xzr, [x29, #-8]
   32120:	cmp	x8, #0x0
   32124:	cset	w8, eq  // eq = none
   32128:	cmp	x1, x10
   3212c:	sub	x26, x22, x8
   32130:	b.hi	32404 <__gmpn_strongfibo@@Base+0x360>  // b.pmore
   32134:	add	x9, x1, #0xf
   32138:	mov	x8, sp
   3213c:	and	x9, x9, #0xfffffffffffffff0
   32140:	sub	x22, x8, x9
   32144:	mov	sp, x22
   32148:	add	x8, x22, x19, lsl #4
   3214c:	add	x24, x8, #0x18
   32150:	mov	x0, x24
   32154:	mov	x1, x22
   32158:	mov	x2, x25
   3215c:	mov	x3, x26
   32160:	mov	x4, x20
   32164:	mov	x5, x19
   32168:	lsl	x23, x19, #1
   3216c:	bl	d050 <__gmpn_fib2m@plt>
   32170:	mov	w9, #0x18                  	// #24
   32174:	madd	x9, x19, x9, x22
   32178:	mov	x8, xzr
   3217c:	add	x9, x9, #0x10
   32180:	ldr	x10, [x9, x8, lsl #3]
   32184:	cbnz	x10, 32198 <__gmpn_strongfibo@@Base+0xf4>
   32188:	sub	x8, x8, #0x1
   3218c:	cmn	x19, x8
   32190:	b.ne	32180 <__gmpn_strongfibo@@Base+0xdc>  // b.any
   32194:	b	323cc <__gmpn_strongfibo@@Base+0x328>
   32198:	cbz	w0, 321d8 <__gmpn_strongfibo@@Base+0x134>
   3219c:	mov	x0, x24
   321a0:	mov	x1, x24
   321a4:	mov	x2, x22
   321a8:	mov	x3, x19
   321ac:	bl	d090 <__gmpn_rsblsh1_n@plt>
   321b0:	mov	x25, x0
   321b4:	cmp	x0, #0x2
   321b8:	b.cc	321f0 <__gmpn_strongfibo@@Base+0x14c>  // b.lo, b.ul, b.last
   321bc:	mov	x0, x24
   321c0:	mov	x1, x24
   321c4:	mov	x2, x20
   321c8:	mov	x3, x19
   321cc:	bl	ca70 <__gmpn_add_n@plt>
   321d0:	add	x25, x0, x25
   321d4:	b	321f0 <__gmpn_strongfibo@@Base+0x14c>
   321d8:	mov	x0, x24
   321dc:	mov	x1, x24
   321e0:	mov	x2, x22
   321e4:	mov	x3, x19
   321e8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   321ec:	mov	x25, x0
   321f0:	mov	w8, #0x18                  	// #24
   321f4:	madd	x8, x19, x8, x22
   321f8:	add	x26, x8, #0x10
   321fc:	b	32218 <__gmpn_strongfibo@@Base+0x174>
   32200:	mov	x0, x24
   32204:	mov	x1, x24
   32208:	mov	x2, x20
   3220c:	mov	x3, x19
   32210:	bl	c2d0 <__gmpn_sub_n@plt>
   32214:	sub	x25, x25, x0
   32218:	cbnz	x25, 32200 <__gmpn_strongfibo@@Base+0x15c>
   3221c:	mov	x8, x26
   32220:	mov	x9, x19
   32224:	subs	x10, x9, #0x1
   32228:	b.lt	32200 <__gmpn_strongfibo@@Base+0x15c>  // b.tstop
   3222c:	add	x9, x20, x9, lsl #3
   32230:	ldr	x11, [x8], #-8
   32234:	ldur	x9, [x9, #-8]
   32238:	cmp	x11, x9
   3223c:	mov	x9, x10
   32240:	b.eq	32224 <__gmpn_strongfibo@@Base+0x180>  // b.none
   32244:	b.hi	32200 <__gmpn_strongfibo@@Base+0x15c>  // b.pmore
   32248:	mov	w8, #0x18                  	// #24
   3224c:	madd	x8, x19, x8, x22
   32250:	mov	x10, xzr
   32254:	neg	x11, x19, lsl #3
   32258:	add	x9, x8, #0x10
   3225c:	mov	x12, x23
   32260:	mov	x8, x10
   32264:	add	x10, x19, x10
   32268:	cmp	x10, #0x1
   3226c:	mov	x25, x12
   32270:	mov	x26, x11
   32274:	b.lt	32290 <__gmpn_strongfibo@@Base+0x1ec>  // b.tstop
   32278:	ldr	x13, [x9, x8, lsl #3]
   3227c:	sub	x12, x25, #0x2
   32280:	sub	x10, x8, #0x1
   32284:	add	x11, x26, #0x10
   32288:	cbz	x13, 32260 <__gmpn_strongfibo@@Base+0x1bc>
   3228c:	b	32298 <__gmpn_strongfibo@@Base+0x1f4>
   32290:	cmn	x19, x8
   32294:	b.eq	323cc <__gmpn_strongfibo@@Base+0x328>  // b.none
   32298:	cmp	x21, #0x1
   3229c:	b.eq	32414 <__gmpn_strongfibo@@Base+0x370>  // b.none
   322a0:	add	x2, x19, x8
   322a4:	mov	x0, x22
   322a8:	mov	x1, x24
   322ac:	bl	c8e0 <__gmpn_sqr@plt>
   322b0:	ldr	x8, [x22]
   322b4:	cmp	x25, x19
   322b8:	orr	x8, x8, #0x2
   322bc:	str	x8, [x22]
   322c0:	b.lt	32428 <__gmpn_strongfibo@@Base+0x384>  // b.tstop
   322c4:	mov	x0, x24
   322c8:	mov	x1, x22
   322cc:	mov	x2, xzr
   322d0:	mov	x3, x22
   322d4:	mov	x4, x25
   322d8:	mov	x5, x20
   322dc:	mov	x6, x19
   322e0:	bl	bf00 <__gmpn_tdiv_qr@plt>
   322e4:	sub	x8, x22, #0x8
   322e8:	mov	x9, x19
   322ec:	ldr	x10, [x8, x9, lsl #3]
   322f0:	cbnz	x10, 3230c <__gmpn_strongfibo@@Base+0x268>
   322f4:	sub	x9, x9, #0x1
   322f8:	cbnz	x9, 322ec <__gmpn_strongfibo@@Base+0x248>
   322fc:	mov	w21, #0x1                   	// #1
   32300:	ldur	x0, [x29, #-8]
   32304:	cbz	x0, 323d4 <__gmpn_strongfibo@@Base+0x330>
   32308:	b	32420 <__gmpn_strongfibo@@Base+0x37c>
   3230c:	subs	x26, x21, #0x2
   32310:	b.eq	32414 <__gmpn_strongfibo@@Base+0x370>  // b.none
   32314:	add	x8, x22, x19, lsl #3
   32318:	add	x24, x8, #0x8
   3231c:	sub	x27, x19, #0x1
   32320:	add	x25, x24, x23, lsl #3
   32324:	add	x28, x22, #0x8
   32328:	mov	x0, x24
   3232c:	mov	x1, x22
   32330:	mov	x2, x19
   32334:	bl	c8e0 <__gmpn_sqr@plt>
   32338:	mov	x0, x25
   3233c:	mov	x1, x22
   32340:	mov	x2, xzr
   32344:	mov	x3, x24
   32348:	mov	x4, x23
   3234c:	mov	x5, x20
   32350:	mov	x6, x19
   32354:	bl	bf00 <__gmpn_tdiv_qr@plt>
   32358:	ldr	x8, [x22]
   3235c:	cmp	x8, #0x4
   32360:	b.hi	32384 <__gmpn_strongfibo@@Base+0x2e0>  // b.pmore
   32364:	cmp	x19, #0x1
   32368:	b.eq	323c4 <__gmpn_strongfibo@@Base+0x320>  // b.none
   3236c:	mov	x9, x27
   32370:	ldr	x10, [x22, x9, lsl #3]
   32374:	cbnz	x10, 3239c <__gmpn_strongfibo@@Base+0x2f8>
   32378:	sub	x9, x9, #0x1
   3237c:	cbnz	x9, 32370 <__gmpn_strongfibo@@Base+0x2cc>
   32380:	b	323c4 <__gmpn_strongfibo@@Base+0x320>
   32384:	sub	x8, x8, #0x2
   32388:	str	x8, [x22]
   3238c:	subs	x26, x26, #0x1
   32390:	mov	x21, xzr
   32394:	b.ne	32328 <__gmpn_strongfibo@@Base+0x284>  // b.any
   32398:	b	323cc <__gmpn_strongfibo@@Base+0x328>
   3239c:	sub	x9, x8, #0x2
   323a0:	cmp	x8, #0x1
   323a4:	str	x9, [x22]
   323a8:	b.hi	3238c <__gmpn_strongfibo@@Base+0x2e8>  // b.pmore
   323ac:	mov	x8, x28
   323b0:	ldr	x9, [x8]
   323b4:	sub	x10, x9, #0x1
   323b8:	str	x10, [x8], #8
   323bc:	cbz	x9, 323b0 <__gmpn_strongfibo@@Base+0x30c>
   323c0:	b	3238c <__gmpn_strongfibo@@Base+0x2e8>
   323c4:	cmp	x8, #0x2
   323c8:	csel	x21, x26, xzr, eq  // eq = none
   323cc:	ldur	x0, [x29, #-8]
   323d0:	cbnz	x0, 32420 <__gmpn_strongfibo@@Base+0x37c>
   323d4:	cmp	x21, #0x0
   323d8:	cset	w0, ne  // ne = any
   323dc:	mov	sp, x29
   323e0:	ldp	x20, x19, [sp, #80]
   323e4:	ldp	x22, x21, [sp, #64]
   323e8:	ldp	x24, x23, [sp, #48]
   323ec:	ldp	x26, x25, [sp, #32]
   323f0:	ldp	x28, x27, [sp, #16]
   323f4:	ldp	x29, x30, [sp], #96
   323f8:	ret
   323fc:	bl	ca50 <__gmpn_copyi@plt>
   32400:	b	320fc <__gmpn_strongfibo@@Base+0x58>
   32404:	sub	x0, x29, #0x8
   32408:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3240c:	mov	x22, x0
   32410:	b	32148 <__gmpn_strongfibo@@Base+0xa4>
   32414:	mov	x21, xzr
   32418:	ldur	x0, [x29, #-8]
   3241c:	cbz	x0, 323d4 <__gmpn_strongfibo@@Base+0x330>
   32420:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32424:	b	323d4 <__gmpn_strongfibo@@Base+0x330>
   32428:	add	x0, x22, x25, lsl #3
   3242c:	mov	w1, wzr
   32430:	mov	x2, x26
   32434:	bl	c5f0 <memset@plt>
   32438:	b	322e4 <__gmpn_strongfibo@@Base+0x240>
   3243c:	nop

0000000000032440 <__gmpn_gcd_11@@Base>:
   32440:	subs	x3, x0, x1
   32444:	b.eq	3246c <__gmpn_gcd_11@@Base+0x2c>  // b.none
   32448:	nop
   3244c:	nop
   32450:	rbit	x12, x3
   32454:	clz	x12, x12
   32458:	cneg	x3, x3, cc  // cc = lo, ul, last
   3245c:	csel	x0, x1, x0, cs  // cs = hs, nlast
   32460:	lsr	x1, x3, x12
   32464:	subs	x3, x0, x1
   32468:	b.ne	32450 <__gmpn_gcd_11@@Base+0x10>  // b.any
   3246c:	ret

0000000000032470 <__gmpn_gcd_22@@Base>:
   32470:	subs	x5, x1, x3
   32474:	cbz	x5, 324e8 <__gmpn_gcd_22@@Base+0x78>
   32478:	sbcs	x6, x0, x2
   3247c:	rbit	x7, x5
   32480:	cneg	x5, x5, cc  // cc = lo, ul, last
   32484:	cinv	x6, x6, cc  // cc = lo, ul, last
   32488:	csel	x3, x3, x1, cs  // cs = hs, nlast
   3248c:	csel	x2, x2, x0, cs  // cs = hs, nlast
   32490:	clz	x7, x7
   32494:	neg	x8, x7
   32498:	lsr	x1, x5, x7
   3249c:	lsl	x14, x6, x8
   324a0:	lsr	x0, x6, x7
   324a4:	orr	x1, x1, x14
   324a8:	orr	x11, x0, x2
   324ac:	cbnz	x11, 32470 <__gmpn_gcd_22@@Base>
   324b0:	subs	x4, x1, x3
   324b4:	b.eq	324dc <__gmpn_gcd_22@@Base+0x6c>  // b.none
   324b8:	nop
   324bc:	nop
   324c0:	rbit	x12, x4
   324c4:	clz	x12, x12
   324c8:	cneg	x4, x4, cc  // cc = lo, ul, last
   324cc:	csel	x1, x3, x1, cs  // cs = hs, nlast
   324d0:	lsr	x3, x4, x12
   324d4:	subs	x4, x1, x3
   324d8:	b.ne	324c0 <__gmpn_gcd_22@@Base+0x50>  // b.any
   324dc:	mov	x0, x1
   324e0:	mov	x1, #0x0                   	// #0
   324e4:	ret
   324e8:	subs	x5, x0, x2
   324ec:	b.eq	32500 <__gmpn_gcd_22@@Base+0x90>  // b.none
   324f0:	mov	x6, #0x0                   	// #0
   324f4:	rbit	x7, x5
   324f8:	cneg	x5, x5, cc  // cc = lo, ul, last
   324fc:	b	32488 <__gmpn_gcd_22@@Base+0x18>
   32500:	mov	x0, x3
   32504:	mov	x1, x2
   32508:	ret

000000000003250c <__gmpn_gcd_1@@Base>:
   3250c:	stp	x29, x30, [sp, #-32]!
   32510:	stp	x20, x19, [sp, #16]
   32514:	ldr	x9, [x0]
   32518:	rbit	x8, x2
   3251c:	clz	x8, x8
   32520:	cmp	x1, #0x2
   32524:	rbit	x10, x9
   32528:	lsr	x19, x2, x8
   3252c:	clz	x10, x10
   32530:	mov	x29, sp
   32534:	b.lt	3255c <__gmpn_gcd_1@@Base+0x50>  // b.tstop
   32538:	cmp	x8, x10
   3253c:	ccmp	x9, #0x0, #0x4, cs  // cs = hs, nlast
   32540:	csel	x20, x8, x10, eq  // eq = none
   32544:	mov	x2, x19
   32548:	cmp	x1, #0x27
   3254c:	b.le	32598 <__gmpn_gcd_1@@Base+0x8c>
   32550:	bl	c3e0 <__gmpn_mod_1@plt>
   32554:	cbnz	x0, 325a4 <__gmpn_gcd_1@@Base+0x98>
   32558:	b	325bc <__gmpn_gcd_1@@Base+0xb0>
   3255c:	lsr	x9, x9, x10
   32560:	cmp	x8, x10
   32564:	csel	x20, x8, x10, cc  // cc = lo, ul, last
   32568:	cmp	x19, x9
   3256c:	csel	x0, x19, x9, hi  // hi = pmore
   32570:	csel	x19, x9, x19, hi  // hi = pmore
   32574:	cmp	x19, x0, lsr #16
   32578:	b.cs	325b0 <__gmpn_gcd_1@@Base+0xa4>  // b.hs, b.nlast
   3257c:	udiv	x8, x0, x19
   32580:	msub	x8, x8, x19, x0
   32584:	cbz	x8, 325bc <__gmpn_gcd_1@@Base+0xb0>
   32588:	rbit	x9, x8
   3258c:	clz	x9, x9
   32590:	lsr	x0, x8, x9
   32594:	b	325b0 <__gmpn_gcd_1@@Base+0xa4>
   32598:	mov	x3, xzr
   3259c:	bl	c7e0 <__gmpn_modexact_1c_odd@plt>
   325a0:	cbz	x0, 325bc <__gmpn_gcd_1@@Base+0xb0>
   325a4:	rbit	x8, x0
   325a8:	clz	x8, x8
   325ac:	lsr	x0, x0, x8
   325b0:	mov	x1, x19
   325b4:	bl	d340 <__gmpn_gcd_11@plt>
   325b8:	mov	x19, x0
   325bc:	lsl	x0, x19, x20
   325c0:	ldp	x20, x19, [sp, #16]
   325c4:	ldp	x29, x30, [sp], #32
   325c8:	ret

00000000000325cc <__gmpn_gcd@@Base>:
   325cc:	stp	x29, x30, [sp, #-96]!
   325d0:	stp	x28, x27, [sp, #16]
   325d4:	stp	x26, x25, [sp, #32]
   325d8:	stp	x24, x23, [sp, #48]
   325dc:	stp	x22, x21, [sp, #64]
   325e0:	stp	x20, x19, [sp, #80]
   325e4:	mov	x29, sp
   325e8:	sub	sp, sp, #0x50
   325ec:	sub	x8, x2, x4
   325f0:	cmp	x8, x4
   325f4:	mov	x22, x4
   325f8:	mov	x20, x3
   325fc:	mov	x24, x2
   32600:	mov	x21, x1
   32604:	csinc	x23, x4, x8, lt  // lt = tstop
   32608:	cmp	x4, #0x14a
   3260c:	mov	x19, x0
   32610:	b.lt	32668 <__gmpn_gcd@@Base+0x9c>  // b.tstop
   32614:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   32618:	lsl	x8, x22, #1
   3261c:	movk	x9, #0x5556
   32620:	smulh	x8, x8, x9
   32624:	add	x25, x8, x8, lsr #63
   32628:	sub	x0, x22, x25
   3262c:	add	x8, x0, #0x1
   32630:	add	x9, x0, #0x2
   32634:	cmp	x8, #0x0
   32638:	csinc	x8, x9, x0, lt  // lt = tstop
   3263c:	lsl	x8, x8, #1
   32640:	and	x26, x8, #0xfffffffffffffffc
   32644:	bl	c590 <__gmpn_hgcd_itch@plt>
   32648:	add	x8, x25, x22
   3264c:	sub	x9, x8, #0x1
   32650:	cmp	x0, x8
   32654:	csel	x8, x9, x0, lt  // lt = tstop
   32658:	add	x8, x26, x8
   3265c:	add	x8, x8, #0x4
   32660:	cmp	x8, x23
   32664:	csel	x23, x8, x23, gt
   32668:	lsl	x1, x23, #3
   3266c:	mov	w8, #0x7f00                	// #32512
   32670:	cmp	x1, x8
   32674:	stur	xzr, [x29, #-24]
   32678:	b.hi	326e8 <__gmpn_gcd@@Base+0x11c>  // b.pmore
   3267c:	add	x9, x1, #0xf
   32680:	mov	x8, sp
   32684:	and	x9, x9, #0xfffffffffffffff0
   32688:	sub	x23, x8, x9
   3268c:	mov	sp, x23
   32690:	cmp	x24, x22
   32694:	b.le	326fc <__gmpn_gcd@@Base+0x130>
   32698:	mov	x0, x23
   3269c:	mov	x1, x21
   326a0:	mov	x2, xzr
   326a4:	mov	x3, x21
   326a8:	mov	x4, x24
   326ac:	mov	x5, x20
   326b0:	mov	x6, x22
   326b4:	bl	bf00 <__gmpn_tdiv_qr@plt>
   326b8:	sub	x8, x21, #0x8
   326bc:	mov	x9, x22
   326c0:	ldr	x10, [x8, x9, lsl #3]
   326c4:	cbnz	x10, 326fc <__gmpn_gcd@@Base+0x130>
   326c8:	sub	x9, x9, #0x1
   326cc:	cbnz	x9, 326c0 <__gmpn_gcd@@Base+0xf4>
   326d0:	mov	x0, x19
   326d4:	mov	x1, x20
   326d8:	mov	x2, x22
   326dc:	bl	ca50 <__gmpn_copyi@plt>
   326e0:	stur	x22, [x29, #-8]
   326e4:	b	32958 <__gmpn_gcd@@Base+0x38c>
   326e8:	sub	x0, x29, #0x18
   326ec:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   326f0:	mov	x23, x0
   326f4:	cmp	x24, x22
   326f8:	b.gt	32698 <__gmpn_gcd@@Base+0xcc>
   326fc:	cmp	x22, #0x14a
   32700:	stur	x19, [x29, #-16]
   32704:	b.lt	327d4 <__gmpn_gcd@@Base+0x208>  // b.tstop
   32708:	mov	x28, #0x5555555555555555    	// #6148914691236517205
   3270c:	adrp	x24, 32000 <__gmpn_perfect_power_p@@Base+0x87c>
   32710:	movk	x28, #0x5556
   32714:	add	x24, x24, #0x98c
   32718:	b	32744 <__gmpn_gcd@@Base+0x178>
   3271c:	add	x1, x0, x25
   32720:	sub	x0, x29, #0x48
   32724:	mov	x2, x21
   32728:	mov	x3, x20
   3272c:	mov	x4, x25
   32730:	mov	x5, x26
   32734:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   32738:	mov	x22, x0
   3273c:	cmp	x22, #0x149
   32740:	b.le	327d4 <__gmpn_gcd@@Base+0x208>
   32744:	lsl	x8, x22, #1
   32748:	smulh	x8, x8, x28
   3274c:	add	x25, x8, x8, lsr #63
   32750:	sub	x27, x22, x25
   32754:	add	x8, x27, #0x1
   32758:	add	x9, x27, #0x2
   3275c:	cmp	x8, #0x0
   32760:	csinc	x8, x9, x27, lt  // lt = tstop
   32764:	lsl	x8, x8, #4
   32768:	sub	x0, x29, #0x48
   3276c:	mov	x1, x27
   32770:	mov	x2, x23
   32774:	and	x26, x8, #0xffffffffffffffe0
   32778:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   3277c:	add	x9, x26, x23
   32780:	lsl	x8, x25, #3
   32784:	add	x26, x9, #0x20
   32788:	add	x0, x21, x8
   3278c:	add	x1, x20, x8
   32790:	sub	x3, x29, #0x48
   32794:	mov	x2, x27
   32798:	mov	x4, x26
   3279c:	bl	cde0 <__gmpn_hgcd@plt>
   327a0:	cmp	x0, #0x1
   327a4:	b.ge	3271c <__gmpn_gcd@@Base+0x150>  // b.tcont
   327a8:	sub	x5, x29, #0x10
   327ac:	mov	x0, x21
   327b0:	mov	x1, x20
   327b4:	mov	x2, x22
   327b8:	mov	x3, xzr
   327bc:	mov	x4, x24
   327c0:	mov	x6, x23
   327c4:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   327c8:	mov	x22, x0
   327cc:	cbnz	x0, 3273c <__gmpn_gcd@@Base+0x170>
   327d0:	b	32958 <__gmpn_gcd@@Base+0x38c>
   327d4:	cmp	x22, #0x3
   327d8:	b.lt	328c8 <__gmpn_gcd@@Base+0x2fc>  // b.tstop
   327dc:	adrp	x24, 32000 <__gmpn_perfect_power_p@@Base+0x87c>
   327e0:	add	x24, x24, #0x98c
   327e4:	b	32818 <__gmpn_gcd@@Base+0x24c>
   327e8:	sub	x0, x29, #0x48
   327ec:	mov	x1, x23
   327f0:	mov	x2, x21
   327f4:	mov	x3, x20
   327f8:	mov	x4, x22
   327fc:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   32800:	mov	x22, x0
   32804:	mov	x0, x21
   32808:	mov	x21, x23
   3280c:	mov	x23, x0
   32810:	cmp	x22, #0x2
   32814:	b.le	328c8 <__gmpn_gcd@@Base+0x2fc>
   32818:	lsl	x8, x22, #3
   3281c:	sub	x9, x8, #0x8
   32820:	ldr	x0, [x21, x9]
   32824:	ldr	x2, [x20, x9]
   32828:	orr	x9, x2, x0
   3282c:	tbnz	x9, #63, 32884 <__gmpn_gcd@@Base+0x2b8>
   32830:	sub	x10, x8, #0x10
   32834:	sub	x8, x8, #0x18
   32838:	ldr	x12, [x21, x10]
   3283c:	ldr	x14, [x21, x8]
   32840:	ldr	x10, [x20, x10]
   32844:	ldr	x8, [x20, x8]
   32848:	clz	x9, x9
   3284c:	neg	x13, x9
   32850:	lsl	x11, x0, x9
   32854:	lsl	x15, x2, x9
   32858:	lsr	x16, x12, x13
   3285c:	lsl	x12, x12, x9
   32860:	lsr	x14, x14, x13
   32864:	lsl	x9, x10, x9
   32868:	lsr	x10, x10, x13
   3286c:	lsr	x8, x8, x13
   32870:	orr	x0, x16, x11
   32874:	orr	x1, x14, x12
   32878:	orr	x2, x10, x15
   3287c:	orr	x3, x8, x9
   32880:	b	32890 <__gmpn_gcd@@Base+0x2c4>
   32884:	sub	x8, x8, #0x10
   32888:	ldr	x1, [x21, x8]
   3288c:	ldr	x3, [x20, x8]
   32890:	sub	x4, x29, #0x48
   32894:	bl	c5a0 <__gmpn_hgcd2@plt>
   32898:	cbnz	w0, 327e8 <__gmpn_gcd@@Base+0x21c>
   3289c:	sub	x5, x29, #0x10
   328a0:	mov	x0, x21
   328a4:	mov	x1, x20
   328a8:	mov	x2, x22
   328ac:	mov	x3, xzr
   328b0:	mov	x4, x24
   328b4:	mov	x6, x23
   328b8:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   328bc:	mov	x22, x0
   328c0:	cbnz	x0, 32810 <__gmpn_gcd@@Base+0x244>
   328c4:	b	32958 <__gmpn_gcd@@Base+0x38c>
   328c8:	ldr	x8, [x21]
   328cc:	tst	x8, #0x1
   328d0:	csel	x11, x21, x20, eq  // eq = none
   328d4:	csel	x9, x20, x21, eq  // eq = none
   328d8:	ldr	x8, [x9]
   328dc:	ldr	x10, [x11]
   328e0:	cmp	x22, #0x1
   328e4:	b.ne	32908 <__gmpn_gcd@@Base+0x33c>  // b.any
   328e8:	rbit	x9, x10
   328ec:	clz	x9, x9
   328f0:	lsr	x1, x10, x9
   328f4:	mov	x0, x8
   328f8:	bl	d340 <__gmpn_gcd_11@plt>
   328fc:	str	x0, [x19]
   32900:	mov	w8, #0x1                   	// #1
   32904:	b	32954 <__gmpn_gcd@@Base+0x388>
   32908:	ldr	x11, [x11, #8]
   3290c:	cmp	x10, #0x0
   32910:	csel	x3, x11, x10, eq  // eq = none
   32914:	csel	x2, xzr, x11, eq  // eq = none
   32918:	tbnz	w3, #0, 32938 <__gmpn_gcd@@Base+0x36c>
   3291c:	rbit	x10, x3
   32920:	clz	x10, x10
   32924:	neg	x11, x10
   32928:	lsr	x12, x3, x10
   3292c:	lsl	x11, x2, x11
   32930:	orr	x3, x11, x12
   32934:	lsr	x2, x2, x10
   32938:	ldr	x0, [x9, #8]
   3293c:	mov	x1, x8
   32940:	bl	c030 <__gmpn_gcd_22@plt>
   32944:	cmp	x1, #0x0
   32948:	mov	w8, #0x1                   	// #1
   3294c:	cinc	x8, x8, ne  // ne = any
   32950:	stp	x0, x1, [x19]
   32954:	stur	x8, [x29, #-8]
   32958:	ldur	x0, [x29, #-24]
   3295c:	cbnz	x0, 32984 <__gmpn_gcd@@Base+0x3b8>
   32960:	ldur	x0, [x29, #-8]
   32964:	mov	sp, x29
   32968:	ldp	x20, x19, [sp, #80]
   3296c:	ldp	x22, x21, [sp, #64]
   32970:	ldp	x24, x23, [sp, #48]
   32974:	ldp	x26, x25, [sp, #32]
   32978:	ldp	x28, x27, [sp, #16]
   3297c:	ldp	x29, x30, [sp], #96
   32980:	ret
   32984:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32988:	b	32960 <__gmpn_gcd@@Base+0x394>
   3298c:	stp	x29, x30, [sp, #-32]!
   32990:	stp	x20, x19, [sp, #16]
   32994:	mov	x20, x0
   32998:	ldr	x0, [x0]
   3299c:	mov	x29, sp
   329a0:	mov	x19, x2
   329a4:	bl	ca50 <__gmpn_copyi@plt>
   329a8:	str	x19, [x20, #8]
   329ac:	ldp	x20, x19, [sp, #16]
   329b0:	ldp	x29, x30, [sp], #32
   329b4:	ret

00000000000329b8 <__gmpn_gcdext_1@@Base>:
   329b8:	mov	x8, xzr
   329bc:	mov	x10, xzr
   329c0:	mov	w9, #0x1                   	// #1
   329c4:	cmp	x2, x3
   329c8:	mov	w11, #0x1                   	// #1
   329cc:	b.cc	329e4 <__gmpn_gcdext_1@@Base+0x2c>  // b.lo, b.ul, b.last
   329d0:	udiv	x12, x2, x3
   329d4:	msub	x2, x12, x3, x2
   329d8:	cbz	x2, 329fc <__gmpn_gcdext_1@@Base+0x44>
   329dc:	msub	x9, x12, x10, x9
   329e0:	msub	x8, x12, x11, x8
   329e4:	udiv	x12, x3, x2
   329e8:	msub	x3, x12, x2, x3
   329ec:	cbz	x3, 32a08 <__gmpn_gcdext_1@@Base+0x50>
   329f0:	msub	x10, x12, x9, x10
   329f4:	msub	x11, x12, x8, x11
   329f8:	b	329d0 <__gmpn_gcdext_1@@Base+0x18>
   329fc:	mov	x9, x10
   32a00:	mov	x8, x11
   32a04:	mov	x2, x3
   32a08:	str	x9, [x0]
   32a0c:	mov	x0, x2
   32a10:	str	x8, [x1]
   32a14:	ret

0000000000032a18 <__gmpn_gcdext@@Base>:
   32a18:	stp	x29, x30, [sp, #-96]!
   32a1c:	stp	x28, x27, [sp, #16]
   32a20:	stp	x26, x25, [sp, #32]
   32a24:	stp	x24, x23, [sp, #48]
   32a28:	stp	x22, x21, [sp, #64]
   32a2c:	stp	x20, x19, [sp, #80]
   32a30:	mov	x29, sp
   32a34:	sub	sp, sp, #0xf0
   32a38:	mov	w8, #0x3                   	// #3
   32a3c:	sub	x9, x4, x6
   32a40:	bfi	x8, x6, #2, #62
   32a44:	cmp	x9, x8
   32a48:	mov	x23, x6
   32a4c:	mov	x24, x5
   32a50:	mov	x19, x4
   32a54:	mov	x27, x3
   32a58:	mov	x25, x2
   32a5c:	mov	x26, x1
   32a60:	mov	x21, x0
   32a64:	add	x28, x6, #0x1
   32a68:	csinc	x22, x8, x9, lt  // lt = tstop
   32a6c:	cmp	x6, #0xf2
   32a70:	stur	xzr, [x29, #-80]
   32a74:	b.lt	32b18 <__gmpn_gcdext@@Base+0x100>  // b.tstop
   32a78:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   32a7c:	movk	x9, #0xaaab
   32a80:	umulh	x9, x23, x9
   32a84:	lsr	x8, x23, #1
   32a88:	lsr	x9, x9, #1
   32a8c:	cmp	x8, x9
   32a90:	csel	x10, x8, x9, lt  // lt = tstop
   32a94:	stur	x28, [x29, #-136]
   32a98:	mov	x28, x24
   32a9c:	mov	x24, x25
   32aa0:	mov	x25, x23
   32aa4:	sub	x0, x25, x10
   32aa8:	csel	x23, x8, x9, gt
   32aac:	add	x8, x0, #0x1
   32ab0:	add	x9, x0, #0x2
   32ab4:	cmp	x8, #0x0
   32ab8:	csinc	x8, x9, x0, lt  // lt = tstop
   32abc:	lsl	x8, x8, #1
   32ac0:	and	x8, x8, #0xfffffffffffffffc
   32ac4:	mov	x20, x26
   32ac8:	add	x26, x8, #0x4
   32acc:	bl	c590 <__gmpn_hgcd_itch@plt>
   32ad0:	add	x8, x23, x25
   32ad4:	sub	x9, x8, #0x1
   32ad8:	cmp	x0, x8
   32adc:	csel	x8, x9, x0, lt  // lt = tstop
   32ae0:	add	x8, x8, x26
   32ae4:	mov	x23, x25
   32ae8:	mov	x25, x24
   32aec:	mov	x24, x28
   32af0:	ldur	x28, [x29, #-136]
   32af4:	cmp	x8, x22
   32af8:	csel	x8, x8, x22, gt
   32afc:	cmp	x8, #0x6a1
   32b00:	mov	w9, #0x6a1                 	// #1697
   32b04:	csel	x8, x8, x9, gt
   32b08:	stur	x26, [x29, #-160]
   32b0c:	mov	x26, x20
   32b10:	add	x22, x8, x28, lsl #1
   32b14:	b	32b18 <__gmpn_gcdext@@Base+0x100>
   32b18:	lsl	x1, x22, #3
   32b1c:	mov	w8, #0x7f00                	// #32512
   32b20:	cmp	x1, x8
   32b24:	b.hi	32ba0 <__gmpn_gcdext@@Base+0x188>  // b.pmore
   32b28:	add	x9, x1, #0xf
   32b2c:	mov	x8, sp
   32b30:	and	x9, x9, #0xfffffffffffffff0
   32b34:	sub	x22, x8, x9
   32b38:	mov	sp, x22
   32b3c:	cmp	x19, x23
   32b40:	b.le	32bb4 <__gmpn_gcdext@@Base+0x19c>
   32b44:	mov	x0, x22
   32b48:	mov	x1, x27
   32b4c:	mov	x2, xzr
   32b50:	mov	x3, x27
   32b54:	mov	x4, x19
   32b58:	mov	x5, x24
   32b5c:	mov	x6, x23
   32b60:	bl	bf00 <__gmpn_tdiv_qr@plt>
   32b64:	sub	x8, x27, #0x8
   32b68:	mov	x9, x23
   32b6c:	ldr	x10, [x8, x9, lsl #3]
   32b70:	cbnz	x10, 32bb4 <__gmpn_gcdext@@Base+0x19c>
   32b74:	sub	x9, x9, #0x1
   32b78:	cbnz	x9, 32b6c <__gmpn_gcdext@@Base+0x154>
   32b7c:	mov	x0, x21
   32b80:	mov	x1, x24
   32b84:	mov	x2, x23
   32b88:	bl	ca50 <__gmpn_copyi@plt>
   32b8c:	str	xzr, [x25]
   32b90:	ldur	x0, [x29, #-80]
   32b94:	cbnz	x0, 3357c <__gmpn_gcdext@@Base+0xb64>
   32b98:	mov	x20, x23
   32b9c:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   32ba0:	sub	x0, x29, #0x50
   32ba4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   32ba8:	mov	x22, x0
   32bac:	cmp	x19, x23
   32bb0:	b.gt	32b44 <__gmpn_gcdext@@Base+0x12c>
   32bb4:	cmp	x23, #0xf1
   32bb8:	b.le	32cc0 <__gmpn_gcdext@@Base+0x2a8>
   32bbc:	cbz	x28, 32bd4 <__gmpn_gcdext@@Base+0x1bc>
   32bc0:	lsl	x8, x23, #4
   32bc4:	add	x2, x8, #0x10
   32bc8:	mov	x0, x22
   32bcc:	mov	w1, wzr
   32bd0:	bl	c5f0 <memset@plt>
   32bd4:	cmp	x23, #0x0
   32bd8:	lsl	x8, x28, #3
   32bdc:	cinc	x9, x23, lt  // lt = tstop
   32be0:	stur	x21, [x29, #-176]
   32be4:	stur	x21, [x29, #-72]
   32be8:	add	x21, x22, x8
   32bec:	asr	x19, x9, #1
   32bf0:	stp	x26, x25, [x29, #-56]
   32bf4:	mov	x20, x26
   32bf8:	add	x28, x21, x8
   32bfc:	mov	x26, x25
   32c00:	sub	x25, x23, x19
   32c04:	sub	x0, x29, #0x80
   32c08:	mov	x1, x25
   32c0c:	mov	x2, x28
   32c10:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   32c14:	lsl	x8, x19, #3
   32c18:	add	x0, x27, x8
   32c1c:	add	x1, x24, x8
   32c20:	ldur	x8, [x29, #-160]
   32c24:	sub	x3, x29, #0x80
   32c28:	mov	x2, x25
   32c2c:	add	x8, x28, x8, lsl #3
   32c30:	mov	x4, x8
   32c34:	mov	x25, x8
   32c38:	bl	cde0 <__gmpn_hgcd@plt>
   32c3c:	cmp	x0, #0x1
   32c40:	stur	x21, [x29, #-136]
   32c44:	stur	x28, [x29, #-160]
   32c48:	b.lt	32cf8 <__gmpn_gcdext@@Base+0x2e0>  // b.tstop
   32c4c:	add	x1, x0, x19
   32c50:	sub	x0, x29, #0x80
   32c54:	mov	x2, x27
   32c58:	mov	x3, x24
   32c5c:	mov	x4, x19
   32c60:	mov	x5, x25
   32c64:	stp	x20, x26, [x29, #-200]
   32c68:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   32c6c:	ldur	x1, [x29, #-96]
   32c70:	ldur	x2, [x29, #-120]
   32c74:	mov	x19, x0
   32c78:	mov	x0, x22
   32c7c:	bl	ca50 <__gmpn_copyi@plt>
   32c80:	ldur	x1, [x29, #-88]
   32c84:	ldur	x2, [x29, #-120]
   32c88:	mov	x0, x21
   32c8c:	bl	ca50 <__gmpn_copyi@plt>
   32c90:	ldur	x8, [x29, #-120]
   32c94:	sub	x9, x22, #0x8
   32c98:	add	x10, x22, x23, lsl #3
   32c9c:	lsl	x11, x8, #3
   32ca0:	ldr	x12, [x9, x11]
   32ca4:	ldr	x13, [x10, x11]
   32ca8:	sub	x11, x8, #0x1
   32cac:	mov	x8, x11
   32cb0:	orr	x12, x13, x12
   32cb4:	cbz	x12, 32c9c <__gmpn_gcdext@@Base+0x284>
   32cb8:	add	x28, x11, #0x1
   32cbc:	b	32d40 <__gmpn_gcdext@@Base+0x328>
   32cc0:	mov	x0, x21
   32cc4:	mov	x1, x26
   32cc8:	mov	x2, x25
   32ccc:	mov	x3, x27
   32cd0:	mov	x4, x24
   32cd4:	mov	x5, x23
   32cd8:	mov	x6, x22
   32cdc:	bl	d1b0 <__gmpn_gcdext_lehmer_n@plt>
   32ce0:	ldur	x8, [x29, #-80]
   32ce4:	mov	x20, x0
   32ce8:	cbz	x8, 33558 <__gmpn_gcdext@@Base+0xb40>
   32cec:	mov	x0, x8
   32cf0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   32cf4:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   32cf8:	mov	w8, #0x1                   	// #1
   32cfc:	add	x9, x28, x23, lsl #3
   32d00:	str	x8, [x21]
   32d04:	stp	x21, x9, [x29, #-24]
   32d08:	stp	x8, x22, [x29, #-40]
   32d0c:	adrp	x4, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   32d10:	ldr	x4, [x4, #3968]
   32d14:	sub	x5, x29, #0x48
   32d18:	mov	x0, x27
   32d1c:	mov	x1, x24
   32d20:	mov	x2, x23
   32d24:	mov	x3, xzr
   32d28:	mov	x6, x28
   32d2c:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   32d30:	cbz	x0, 330b4 <__gmpn_gcdext@@Base+0x69c>
   32d34:	ldur	x28, [x29, #-40]
   32d38:	mov	x19, x0
   32d3c:	stp	x20, x26, [x29, #-200]
   32d40:	cmp	x19, #0xf2
   32d44:	stur	x23, [x29, #-184]
   32d48:	stur	x24, [x29, #-152]
   32d4c:	b.lt	32fa8 <__gmpn_gcdext@@Base+0x590>  // b.tstop
   32d50:	add	x21, x22, x23, lsl #3
   32d54:	sub	x23, x22, #0x8
   32d58:	mov	x20, x25
   32d5c:	stur	x25, [x29, #-168]
   32d60:	stur	x22, [x29, #-144]
   32d64:	b	32d84 <__gmpn_gcdext@@Base+0x36c>
   32d68:	lsl	x9, x8, #3
   32d6c:	str	x25, [x24, x9]
   32d70:	str	x0, [x22, x9]
   32d74:	mov	x22, x24
   32d78:	add	x28, x8, #0x1
   32d7c:	cmp	x19, #0xf1
   32d80:	b.le	32fa8 <__gmpn_gcdext@@Base+0x590>
   32d84:	mov	x8, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   32d88:	movk	x8, #0xaaab
   32d8c:	ldur	x22, [x29, #-160]
   32d90:	umulh	x8, x19, x8
   32d94:	lsr	x25, x8, #1
   32d98:	sub	x26, x19, x25
   32d9c:	sub	x0, x29, #0x80
   32da0:	mov	x1, x26
   32da4:	mov	x2, x22
   32da8:	mov	x24, x28
   32dac:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   32db0:	mov	x28, x19
   32db4:	ldur	x19, [x29, #-152]
   32db8:	lsl	x8, x25, #3
   32dbc:	add	x0, x27, x8
   32dc0:	sub	x3, x29, #0x80
   32dc4:	add	x1, x19, x8
   32dc8:	mov	x2, x26
   32dcc:	mov	x4, x20
   32dd0:	bl	cde0 <__gmpn_hgcd@plt>
   32dd4:	cmp	x0, #0x1
   32dd8:	b.lt	32e4c <__gmpn_gcdext@@Base+0x434>  // b.tstop
   32ddc:	add	x1, x0, x25
   32de0:	sub	x0, x29, #0x80
   32de4:	mov	x2, x27
   32de8:	mov	x3, x19
   32dec:	mov	x4, x25
   32df0:	mov	x5, x20
   32df4:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   32df8:	ldur	x22, [x29, #-144]
   32dfc:	mov	x19, x0
   32e00:	mov	x0, x20
   32e04:	mov	x2, x24
   32e08:	mov	x1, x22
   32e0c:	mov	x28, x24
   32e10:	bl	ca50 <__gmpn_copyi@plt>
   32e14:	ldp	x4, x3, [x29, #-120]
   32e18:	add	x26, x20, x24, lsl #3
   32e1c:	mov	x0, x26
   32e20:	cmp	x4, x24
   32e24:	b.ge	32ea0 <__gmpn_gcdext@@Base+0x488>  // b.tcont
   32e28:	mov	x1, x20
   32e2c:	mov	x2, x28
   32e30:	bl	ccd0 <__gmpn_mul@plt>
   32e34:	ldur	x3, [x29, #-96]
   32e38:	ldur	x4, [x29, #-120]
   32e3c:	ldur	x1, [x29, #-136]
   32e40:	mov	x0, x22
   32e44:	mov	x2, x28
   32e48:	b	32ec8 <__gmpn_gcdext@@Base+0x4b0>
   32e4c:	ldur	x9, [x29, #-136]
   32e50:	add	x8, x22, x28, lsl #3
   32e54:	adrp	x4, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   32e58:	sub	x5, x29, #0x48
   32e5c:	stp	x9, x8, [x29, #-24]
   32e60:	ldur	x8, [x29, #-144]
   32e64:	mov	x0, x27
   32e68:	mov	x1, x19
   32e6c:	mov	x2, x28
   32e70:	stp	x24, x8, [x29, #-40]
   32e74:	ldr	x4, [x4, #3968]
   32e78:	mov	x3, xzr
   32e7c:	mov	x6, x22
   32e80:	mov	x25, x27
   32e84:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   32e88:	cbz	x0, 330b4 <__gmpn_gcdext@@Base+0x69c>
   32e8c:	ldur	x28, [x29, #-40]
   32e90:	ldur	x22, [x29, #-144]
   32e94:	mov	x19, x0
   32e98:	mov	x27, x25
   32e9c:	b	32d7c <__gmpn_gcdext@@Base+0x364>
   32ea0:	mov	x1, x3
   32ea4:	mov	x2, x4
   32ea8:	mov	x3, x20
   32eac:	mov	x4, x28
   32eb0:	bl	ccd0 <__gmpn_mul@plt>
   32eb4:	ldur	x1, [x29, #-96]
   32eb8:	ldur	x2, [x29, #-120]
   32ebc:	ldur	x3, [x29, #-136]
   32ec0:	mov	x0, x22
   32ec4:	mov	x4, x28
   32ec8:	bl	ccd0 <__gmpn_mul@plt>
   32ecc:	ldur	x8, [x29, #-120]
   32ed0:	mov	x0, x22
   32ed4:	mov	x1, x22
   32ed8:	mov	x2, x26
   32edc:	add	x3, x8, x28
   32ee0:	mov	x24, x22
   32ee4:	bl	ca70 <__gmpn_add_n@plt>
   32ee8:	ldur	x4, [x29, #-120]
   32eec:	ldur	x3, [x29, #-88]
   32ef0:	mov	x25, x0
   32ef4:	mov	x0, x26
   32ef8:	cmp	x4, x28
   32efc:	b.ge	32f28 <__gmpn_gcdext@@Base+0x510>  // b.tcont
   32f00:	ldur	x22, [x29, #-136]
   32f04:	mov	x2, x28
   32f08:	mov	x1, x22
   32f0c:	bl	ccd0 <__gmpn_mul@plt>
   32f10:	ldur	x3, [x29, #-104]
   32f14:	ldur	x4, [x29, #-120]
   32f18:	mov	x0, x22
   32f1c:	mov	x1, x20
   32f20:	mov	x2, x28
   32f24:	b	32f54 <__gmpn_gcdext@@Base+0x53c>
   32f28:	ldur	x22, [x29, #-136]
   32f2c:	mov	x1, x3
   32f30:	mov	x2, x4
   32f34:	mov	x4, x28
   32f38:	mov	x3, x22
   32f3c:	bl	ccd0 <__gmpn_mul@plt>
   32f40:	ldur	x1, [x29, #-104]
   32f44:	ldur	x2, [x29, #-120]
   32f48:	mov	x0, x22
   32f4c:	mov	x3, x20
   32f50:	mov	x4, x28
   32f54:	bl	ccd0 <__gmpn_mul@plt>
   32f58:	ldur	x8, [x29, #-120]
   32f5c:	mov	x0, x22
   32f60:	mov	x1, x22
   32f64:	mov	x2, x26
   32f68:	add	x3, x8, x28
   32f6c:	bl	ca70 <__gmpn_add_n@plt>
   32f70:	ldur	x8, [x29, #-120]
   32f74:	orr	x9, x0, x25
   32f78:	add	x8, x8, x28
   32f7c:	cbnz	x9, 32d68 <__gmpn_gcdext@@Base+0x350>
   32f80:	lsl	x9, x8, #3
   32f84:	ldr	x10, [x23, x9]
   32f88:	ldr	x11, [x21, x9]
   32f8c:	sub	x9, x8, #0x1
   32f90:	mov	x8, x9
   32f94:	orr	x10, x11, x10
   32f98:	cbz	x10, 32f80 <__gmpn_gcdext@@Base+0x568>
   32f9c:	add	x28, x9, #0x1
   32fa0:	mov	x22, x24
   32fa4:	b	32d7c <__gmpn_gcdext@@Base+0x364>
   32fa8:	ldur	x23, [x29, #-184]
   32fac:	ldur	x24, [x29, #-152]
   32fb0:	sub	x11, x19, #0x1
   32fb4:	mov	x8, x11
   32fb8:	add	x9, x8, #0x1
   32fbc:	cmp	x9, #0x1
   32fc0:	b.lt	33008 <__gmpn_gcdext@@Base+0x5f0>  // b.tstop
   32fc4:	lsl	x9, x8, #3
   32fc8:	ldr	x10, [x27, x9]
   32fcc:	ldr	x9, [x24, x9]
   32fd0:	sub	x8, x8, #0x1
   32fd4:	cmp	x10, x9
   32fd8:	b.eq	32fb8 <__gmpn_gcdext@@Base+0x5a0>  // b.none
   32fdc:	cmp	x28, #0x1
   32fe0:	b.ne	330c4 <__gmpn_gcdext@@Base+0x6ac>  // b.any
   32fe4:	ldr	x8, [x22]
   32fe8:	cbnz	x8, 330c4 <__gmpn_gcdext@@Base+0x6ac>
   32fec:	ldur	x0, [x29, #-176]
   32ff0:	ldp	x1, x2, [x29, #-200]
   32ff4:	ldur	x6, [x29, #-160]
   32ff8:	mov	x3, x27
   32ffc:	mov	x4, x24
   33000:	mov	x5, x19
   33004:	b	32cdc <__gmpn_gcdext@@Base+0x2c4>
   33008:	ldur	x0, [x29, #-176]
   3300c:	mov	x1, x27
   33010:	mov	x2, x19
   33014:	bl	ca50 <__gmpn_copyi@plt>
   33018:	sub	x8, x22, #0x8
   3301c:	add	x9, x22, x23, lsl #3
   33020:	mov	x10, x28
   33024:	subs	x11, x10, #0x1
   33028:	b.lt	33048 <__gmpn_gcdext@@Base+0x630>  // b.tstop
   3302c:	lsl	x10, x10, #3
   33030:	ldr	x12, [x8, x10]
   33034:	ldr	x10, [x9, x10]
   33038:	cmp	x12, x10
   3303c:	mov	x10, x11
   33040:	b.eq	33024 <__gmpn_gcdext@@Base+0x60c>  // b.none
   33044:	b.ls	33074 <__gmpn_gcdext@@Base+0x65c>  // b.plast
   33048:	add	x8, x22, x23, lsl #3
   3304c:	ldr	x10, [x8, x28, lsl #3]
   33050:	sub	x9, x28, #0x1
   33054:	mov	x28, x9
   33058:	cbz	x10, 3304c <__gmpn_gcdext@@Base+0x634>
   3305c:	ldur	x0, [x29, #-200]
   33060:	ldur	x1, [x29, #-136]
   33064:	add	x20, x9, #0x1
   33068:	mov	x2, x20
   3306c:	bl	ca50 <__gmpn_copyi@plt>
   33070:	b	3309c <__gmpn_gcdext@@Base+0x684>
   33074:	mov	x20, x28
   33078:	subs	x28, x28, #0x1
   3307c:	b.lt	33088 <__gmpn_gcdext@@Base+0x670>  // b.tstop
   33080:	ldr	x9, [x8, x20, lsl #3]
   33084:	cbz	x9, 33074 <__gmpn_gcdext@@Base+0x65c>
   33088:	ldur	x0, [x29, #-200]
   3308c:	mov	x1, x22
   33090:	mov	x2, x20
   33094:	bl	ca50 <__gmpn_copyi@plt>
   33098:	neg	x20, x20
   3309c:	ldur	x8, [x29, #-192]
   330a0:	str	x20, [x8]
   330a4:	ldur	x0, [x29, #-80]
   330a8:	cbnz	x0, 33588 <__gmpn_gcdext@@Base+0xb70>
   330ac:	mov	x20, x19
   330b0:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   330b4:	ldur	x0, [x29, #-80]
   330b8:	cbnz	x0, 33594 <__gmpn_gcdext@@Base+0xb7c>
   330bc:	ldur	x20, [x29, #-64]
   330c0:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   330c4:	ldur	x26, [x29, #-160]
   330c8:	lsl	x21, x19, #3
   330cc:	mov	x1, x27
   330d0:	mov	x2, x19
   330d4:	add	x20, x26, x21
   330d8:	mov	x0, x20
   330dc:	stur	x11, [x29, #-216]
   330e0:	bl	ca50 <__gmpn_copyi@plt>
   330e4:	add	x25, x20, x21
   330e8:	mov	x0, x25
   330ec:	mov	x1, x24
   330f0:	mov	x2, x19
   330f4:	bl	ca50 <__gmpn_copyi@plt>
   330f8:	ldur	x0, [x29, #-176]
   330fc:	add	x6, x20, x19, lsl #4
   33100:	sub	x2, x29, #0x80
   33104:	mov	x1, x26
   33108:	mov	x3, x20
   3310c:	mov	x4, x25
   33110:	mov	x5, x19
   33114:	stur	x20, [x29, #-168]
   33118:	bl	d1b0 <__gmpn_gcdext_lehmer_n@plt>
   3311c:	mov	x20, x0
   33120:	sub	x8, x22, #0x8
   33124:	mov	x9, x28
   33128:	mov	x26, x9
   3312c:	subs	x9, x9, #0x1
   33130:	b.lt	3313c <__gmpn_gcdext@@Base+0x724>  // b.tstop
   33134:	ldr	x10, [x8, x26, lsl #3]
   33138:	cbz	x10, 33128 <__gmpn_gcdext@@Base+0x710>
   3313c:	ldur	x11, [x29, #-128]
   33140:	cbz	x11, 331bc <__gmpn_gcdext@@Base+0x7a4>
   33144:	cmp	x11, #0x0
   33148:	mov	w9, #0x18                  	// #24
   3314c:	lsl	x10, x23, #4
   33150:	add	x0, x25, #0x8
   33154:	cneg	x25, x11, mi  // mi = first
   33158:	madd	x9, x19, x9, x10
   3315c:	add	x9, x9, x25, lsl #3
   33160:	add	x9, x9, x22
   33164:	stp	x11, x10, [x29, #-232]
   33168:	add	x10, x9, #0x10
   3316c:	add	x9, x27, x19, lsl #3
   33170:	mov	x8, xzr
   33174:	sub	x9, x9, #0x8
   33178:	stur	x10, [x29, #-240]
   3317c:	add	x4, x19, x8
   33180:	mov	x21, x8
   33184:	cmp	x4, #0x1
   33188:	mov	x23, x10
   3318c:	b.lt	331a0 <__gmpn_gcdext@@Base+0x788>  // b.tstop
   33190:	ldr	x11, [x9, x21, lsl #3]
   33194:	sub	x10, x23, #0x8
   33198:	sub	x8, x21, #0x1
   3319c:	cbz	x11, 3317c <__gmpn_gcdext@@Base+0x764>
   331a0:	cmp	x4, x25
   331a4:	stur	x0, [x29, #-208]
   331a8:	b.ge	331e4 <__gmpn_gcdext@@Base+0x7cc>  // b.tcont
   331ac:	ldur	x1, [x29, #-160]
   331b0:	mov	x2, x25
   331b4:	mov	x3, x27
   331b8:	b	331f4 <__gmpn_gcdext@@Base+0x7dc>
   331bc:	ldur	x0, [x29, #-200]
   331c0:	mov	x1, x22
   331c4:	mov	x2, x26
   331c8:	bl	ca50 <__gmpn_copyi@plt>
   331cc:	ldur	x9, [x29, #-192]
   331d0:	neg	x8, x26
   331d4:	str	x8, [x9]
   331d8:	ldur	x0, [x29, #-80]
   331dc:	cbz	x0, 33558 <__gmpn_gcdext@@Base+0xb40>
   331e0:	b	32cf0 <__gmpn_gcdext@@Base+0x2d8>
   331e4:	ldur	x3, [x29, #-160]
   331e8:	mov	x1, x27
   331ec:	mov	x2, x4
   331f0:	mov	x4, x25
   331f4:	bl	ccd0 <__gmpn_mul@plt>
   331f8:	ldp	x9, x27, [x29, #-232]
   331fc:	add	x8, x25, x19
   33200:	cmp	x9, #0x1
   33204:	mov	x9, x25
   33208:	add	x25, x8, x21
   3320c:	b.lt	33280 <__gmpn_gcdext@@Base+0x868>  // b.tstop
   33210:	cbz	x20, 33268 <__gmpn_gcdext@@Base+0x850>
   33214:	ldur	x0, [x29, #-208]
   33218:	ldur	x2, [x29, #-176]
   3321c:	mov	x3, x20
   33220:	mov	x1, x0
   33224:	bl	c2d0 <__gmpn_sub_n@plt>
   33228:	cbz	x0, 33268 <__gmpn_gcdext@@Base+0x850>
   3322c:	add	x8, x27, x19, lsl #4
   33230:	add	x8, x22, x8
   33234:	add	x9, x20, #0x3
   33238:	sub	x10, x9, #0x3
   3323c:	cmp	x10, x25
   33240:	b.ge	33268 <__gmpn_gcdext@@Base+0x850>  // b.tcont
   33244:	lsl	x10, x9, #3
   33248:	ldr	x11, [x8, x10]
   3324c:	add	x9, x9, #0x1
   33250:	sub	x12, x11, #0x1
   33254:	str	x12, [x8, x10]
   33258:	cbz	x11, 33238 <__gmpn_gcdext@@Base+0x820>
   3325c:	b	33268 <__gmpn_gcdext@@Base+0x850>
   33260:	ldr	x8, [x23], #-8
   33264:	cbnz	x8, 332ec <__gmpn_gcdext@@Base+0x8d4>
   33268:	mov	x19, x25
   3326c:	subs	x25, x25, #0x1
   33270:	b.ge	33260 <__gmpn_gcdext@@Base+0x848>  // b.tcont
   33274:	cbnz	x19, 332ec <__gmpn_gcdext@@Base+0x8d4>
   33278:	ldur	x25, [x29, #-168]
   3327c:	b	3334c <__gmpn_gcdext@@Base+0x934>
   33280:	mov	x23, x9
   33284:	cbz	x20, 332d0 <__gmpn_gcdext@@Base+0x8b8>
   33288:	ldur	x0, [x29, #-208]
   3328c:	ldur	x2, [x29, #-176]
   33290:	mov	x3, x20
   33294:	mov	x1, x0
   33298:	bl	ca70 <__gmpn_add_n@plt>
   3329c:	cbz	x0, 332d0 <__gmpn_gcdext@@Base+0x8b8>
   332a0:	add	x8, x27, x19, lsl #4
   332a4:	add	x8, x22, x8
   332a8:	add	x9, x20, #0x3
   332ac:	sub	x10, x9, #0x3
   332b0:	cmp	x10, x25
   332b4:	b.ge	332d0 <__gmpn_gcdext@@Base+0x8b8>  // b.tcont
   332b8:	lsl	x10, x9, #3
   332bc:	ldr	x11, [x8, x10]
   332c0:	add	x9, x9, #0x1
   332c4:	adds	x11, x11, #0x1
   332c8:	str	x11, [x8, x10]
   332cc:	b.cs	332ac <__gmpn_gcdext@@Base+0x894>  // b.hs, b.nlast
   332d0:	ldur	x8, [x29, #-240]
   332d4:	add	x9, x23, x19
   332d8:	ldr	x8, [x8, x21, lsl #3]
   332dc:	cmp	x8, #0x0
   332e0:	cset	w8, eq  // eq = none
   332e4:	sub	x8, x9, x8
   332e8:	add	x19, x8, x21
   332ec:	ldur	x10, [x29, #-216]
   332f0:	add	x8, x27, x19, lsl #3
   332f4:	add	x8, x8, x22
   332f8:	add	x8, x8, #0x10
   332fc:	add	x4, x10, #0x1
   33300:	mov	x21, x10
   33304:	cmp	x4, #0x1
   33308:	mov	x23, x8
   3330c:	b.lt	33320 <__gmpn_gcdext@@Base+0x908>  // b.tstop
   33310:	ldr	x9, [x24, x21, lsl #3]
   33314:	sub	x10, x21, #0x1
   33318:	add	x8, x23, #0x8
   3331c:	cbz	x9, 332fc <__gmpn_gcdext@@Base+0x8e4>
   33320:	ldur	x25, [x29, #-168]
   33324:	ldur	x1, [x29, #-208]
   33328:	mov	x2, x19
   3332c:	mov	x3, x24
   33330:	mov	x0, x25
   33334:	bl	c430 <__gmpn_divexact@plt>
   33338:	ldr	x8, [x23]
   3333c:	cmp	x8, #0x0
   33340:	cset	w8, eq  // eq = none
   33344:	sub	x8, x19, x8
   33348:	sub	x19, x8, x21
   3334c:	ldur	x4, [x29, #-128]
   33350:	ldur	x8, [x29, #-184]
   33354:	ldur	x3, [x29, #-160]
   33358:	stur	x20, [x29, #-152]
   3335c:	cmp	x4, #0x0
   33360:	b.le	3336c <__gmpn_gcdext@@Base+0x954>
   33364:	mov	w20, wzr
   33368:	b	33378 <__gmpn_gcdext@@Base+0x960>
   3336c:	neg	x4, x4
   33370:	mov	w20, #0x1                   	// #1
   33374:	stur	x4, [x29, #-128]
   33378:	add	x8, x22, x8, lsl #3
   3337c:	mov	x24, x28
   33380:	subs	x28, x28, #0x1
   33384:	b.lt	33390 <__gmpn_gcdext@@Base+0x978>  // b.tstop
   33388:	ldr	x9, [x8, x24, lsl #3]
   3338c:	cbz	x9, 3337c <__gmpn_gcdext@@Base+0x964>
   33390:	cmp	x4, x24
   33394:	b.le	333b8 <__gmpn_gcdext@@Base+0x9a0>
   33398:	ldur	x27, [x29, #-200]
   3339c:	ldur	x28, [x29, #-136]
   333a0:	mov	x1, x3
   333a4:	mov	x2, x4
   333a8:	mov	x0, x27
   333ac:	mov	x3, x28
   333b0:	mov	x4, x24
   333b4:	b	333cc <__gmpn_gcdext@@Base+0x9b4>
   333b8:	ldur	x27, [x29, #-200]
   333bc:	ldur	x28, [x29, #-136]
   333c0:	mov	x2, x24
   333c4:	mov	x0, x27
   333c8:	mov	x1, x28
   333cc:	bl	ccd0 <__gmpn_mul@plt>
   333d0:	ldur	x8, [x29, #-128]
   333d4:	ldur	x23, [x29, #-192]
   333d8:	add	x9, x8, x24
   333dc:	add	x9, x27, x9, lsl #3
   333e0:	ldur	x9, [x9, #-8]
   333e4:	cmp	x9, #0x0
   333e8:	cset	w9, eq  // eq = none
   333ec:	sub	x8, x8, x9
   333f0:	cmp	x19, #0x1
   333f4:	add	x24, x8, x24
   333f8:	b.lt	33540 <__gmpn_gcdext@@Base+0xb28>  // b.tstop
   333fc:	mov	x0, x28
   33400:	cmp	x19, x26
   33404:	b.le	3341c <__gmpn_gcdext@@Base+0xa04>
   33408:	mov	x1, x25
   3340c:	mov	x2, x19
   33410:	mov	x3, x22
   33414:	mov	x4, x26
   33418:	b	3342c <__gmpn_gcdext@@Base+0xa14>
   3341c:	mov	x1, x22
   33420:	mov	x2, x26
   33424:	mov	x3, x25
   33428:	mov	x4, x19
   3342c:	bl	ccd0 <__gmpn_mul@plt>
   33430:	add	x8, x19, x26
   33434:	add	x8, x28, x8, lsl #3
   33438:	ldur	x8, [x8, #-8]
   3343c:	cmp	x8, #0x0
   33440:	cset	w8, eq  // eq = none
   33444:	sub	x8, x19, x8
   33448:	add	x25, x8, x26
   3344c:	csetm	x21, eq  // eq = none
   33450:	cmp	x25, x24
   33454:	b.le	334ec <__gmpn_gcdext@@Base+0xad4>
   33458:	cbz	x24, 334a4 <__gmpn_gcdext@@Base+0xa8c>
   3345c:	mov	x0, x27
   33460:	mov	x1, x28
   33464:	mov	x2, x27
   33468:	mov	x3, x24
   3346c:	bl	ca70 <__gmpn_add_n@plt>
   33470:	cbz	x0, 334a4 <__gmpn_gcdext@@Base+0xa8c>
   33474:	ldur	x8, [x29, #-184]
   33478:	add	x8, x22, x8, lsl #3
   3347c:	add	x9, x8, #0x8
   33480:	mov	w8, #0x1                   	// #1
   33484:	cmp	x24, x25
   33488:	b.ge	334e4 <__gmpn_gcdext@@Base+0xacc>  // b.tcont
   3348c:	lsl	x10, x24, #3
   33490:	ldr	x11, [x9, x10]
   33494:	add	x24, x24, #0x1
   33498:	adds	x11, x11, #0x1
   3349c:	str	x11, [x27, x10]
   334a0:	b.cs	33484 <__gmpn_gcdext@@Base+0xa6c>  // b.hs, b.nlast
   334a4:	cmp	x28, x27
   334a8:	mov	x8, xzr
   334ac:	b.eq	334e4 <__gmpn_gcdext@@Base+0xacc>  // b.none
   334b0:	cmp	x25, x24
   334b4:	b.le	334e4 <__gmpn_gcdext@@Base+0xacc>
   334b8:	ldur	x8, [x29, #-184]
   334bc:	add	x9, x19, x21
   334c0:	sub	x9, x9, x24
   334c4:	add	x0, x27, x24, lsl #3
   334c8:	add	x8, x24, x8
   334cc:	add	x8, x22, x8, lsl #3
   334d0:	add	x1, x8, #0x8
   334d4:	add	x8, x9, x26
   334d8:	lsl	x2, x8, #3
   334dc:	bl	bed0 <memcpy@plt>
   334e0:	mov	x8, xzr
   334e4:	mov	x24, x25
   334e8:	b	33538 <__gmpn_gcdext@@Base+0xb20>
   334ec:	neg	x8, x8
   334f0:	cmp	x8, x26
   334f4:	b.eq	33534 <__gmpn_gcdext@@Base+0xb1c>  // b.none
   334f8:	mov	x0, x27
   334fc:	mov	x1, x27
   33500:	mov	x2, x28
   33504:	mov	x3, x25
   33508:	bl	ca70 <__gmpn_add_n@plt>
   3350c:	cbz	x0, 33534 <__gmpn_gcdext@@Base+0xb1c>
   33510:	mov	w8, #0x1                   	// #1
   33514:	cmp	x25, x24
   33518:	b.ge	33538 <__gmpn_gcdext@@Base+0xb20>  // b.tcont
   3351c:	lsl	x9, x25, #3
   33520:	ldr	x10, [x27, x9]
   33524:	add	x25, x25, #0x1
   33528:	adds	x10, x10, #0x1
   3352c:	str	x10, [x27, x9]
   33530:	b.cs	33514 <__gmpn_gcdext@@Base+0xafc>  // b.hs, b.nlast
   33534:	mov	x8, xzr
   33538:	str	x8, [x27, x24, lsl #3]
   3353c:	add	x24, x8, x24
   33540:	cmp	w20, #0x0
   33544:	cneg	x8, x24, ne  // ne = any
   33548:	str	x8, [x23]
   3354c:	ldur	x0, [x29, #-80]
   33550:	ldur	x20, [x29, #-152]
   33554:	cbnz	x0, 32cf0 <__gmpn_gcdext@@Base+0x2d8>
   33558:	mov	x0, x20
   3355c:	mov	sp, x29
   33560:	ldp	x20, x19, [sp, #80]
   33564:	ldp	x22, x21, [sp, #64]
   33568:	ldp	x24, x23, [sp, #48]
   3356c:	ldp	x26, x25, [sp, #32]
   33570:	ldp	x28, x27, [sp, #16]
   33574:	ldp	x29, x30, [sp], #96
   33578:	ret
   3357c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   33580:	mov	x20, x23
   33584:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   33588:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3358c:	mov	x20, x19
   33590:	b	33558 <__gmpn_gcdext@@Base+0xb40>
   33594:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   33598:	b	330bc <__gmpn_gcdext@@Base+0x6a4>

000000000003359c <__gmpn_gcd_subdiv_step@@Base>:
   3359c:	sub	sp, sp, #0x70
   335a0:	add	x9, x0, x2, lsl #3
   335a4:	stp	x24, x23, [sp, #64]
   335a8:	stp	x22, x21, [sp, #80]
   335ac:	stp	x20, x19, [sp, #96]
   335b0:	mov	x21, x6
   335b4:	mov	x20, x5
   335b8:	mov	x19, x4
   335bc:	mov	x24, x3
   335c0:	mov	x8, x0
   335c4:	mov	x11, xzr
   335c8:	sub	x10, x9, #0x8
   335cc:	stp	x29, x30, [sp, #16]
   335d0:	stp	x28, x27, [sp, #32]
   335d4:	stp	x26, x25, [sp, #48]
   335d8:	add	x29, sp, #0x10
   335dc:	add	x23, x2, x11
   335e0:	mov	x9, x11
   335e4:	cmp	x23, #0x1
   335e8:	b.lt	3360c <__gmpn_gcd_subdiv_step@@Base+0x70>  // b.tstop
   335ec:	ldr	x12, [x10, x9, lsl #3]
   335f0:	sub	x11, x9, #0x1
   335f4:	cbz	x12, 335dc <__gmpn_gcd_subdiv_step@@Base+0x40>
   335f8:	b	3360c <__gmpn_gcd_subdiv_step@@Base+0x70>
   335fc:	add	x9, x1, x10, lsl #3
   33600:	ldur	x12, [x9, #-8]
   33604:	add	x9, x11, #0x1
   33608:	cbnz	x12, 3361c <__gmpn_gcd_subdiv_step@@Base+0x80>
   3360c:	mov	x10, x2
   33610:	subs	x2, x2, #0x1
   33614:	mov	x11, x9
   33618:	b.ge	335fc <__gmpn_gcd_subdiv_step@@Base+0x60>  // b.tcont
   3361c:	cbz	x11, 33634 <__gmpn_gcd_subdiv_step@@Base+0x98>
   33620:	cmp	x23, x10
   33624:	csel	x22, x10, x23, gt
   33628:	csel	x23, x23, x10, gt
   3362c:	cset	w27, gt
   33630:	b	33668 <__gmpn_gcd_subdiv_step@@Base+0xcc>
   33634:	sub	x9, x1, #0x8
   33638:	mov	x10, x23
   3363c:	subs	x11, x10, #0x1
   33640:	b.lt	3383c <__gmpn_gcd_subdiv_step@@Base+0x2a0>  // b.tstop
   33644:	lsl	x10, x10, #3
   33648:	add	x12, x8, x10
   3364c:	ldur	x12, [x12, #-8]
   33650:	ldr	x10, [x9, x10]
   33654:	cmp	x12, x10
   33658:	mov	x10, x11
   3365c:	b.eq	3363c <__gmpn_gcd_subdiv_step@@Base+0xa0>  // b.none
   33660:	cset	w27, hi  // hi = pmore
   33664:	mov	x22, x23
   33668:	cmp	w27, #0x0
   3366c:	csel	x25, x8, x1, ne  // ne = any
   33670:	csel	x26, x1, x8, ne  // ne = any
   33674:	cmp	x22, x24
   33678:	b.le	3374c <__gmpn_gcd_subdiv_step@@Base+0x1b0>
   3367c:	cbz	x22, 336cc <__gmpn_gcd_subdiv_step@@Base+0x130>
   33680:	mov	x0, x25
   33684:	mov	x1, x25
   33688:	mov	x2, x26
   3368c:	mov	x3, x22
   33690:	bl	c2d0 <__gmpn_sub_n@plt>
   33694:	cbz	x0, 336cc <__gmpn_gcd_subdiv_step@@Base+0x130>
   33698:	mov	x8, x22
   3369c:	cmp	x8, x23
   336a0:	b.ge	336cc <__gmpn_gcd_subdiv_step@@Base+0x130>  // b.tcont
   336a4:	lsl	x9, x8, #3
   336a8:	ldr	x10, [x25, x9]
   336ac:	add	x8, x8, #0x1
   336b0:	sub	x11, x10, #0x1
   336b4:	str	x11, [x25, x9]
   336b8:	cbz	x10, 3369c <__gmpn_gcd_subdiv_step@@Base+0x100>
   336bc:	b	336cc <__gmpn_gcd_subdiv_step@@Base+0x130>
   336c0:	add	x8, x25, x28, lsl #3
   336c4:	ldur	x8, [x8, #-8]
   336c8:	cbnz	x8, 336d8 <__gmpn_gcd_subdiv_step@@Base+0x13c>
   336cc:	mov	x28, x23
   336d0:	subs	x23, x23, #0x1
   336d4:	b.ge	336c0 <__gmpn_gcd_subdiv_step@@Base+0x124>  // b.tcont
   336d8:	cmp	x28, x24
   336dc:	b.le	33760 <__gmpn_gcd_subdiv_step@@Base+0x1c4>
   336e0:	cmp	x22, x28
   336e4:	str	x19, [sp, #8]
   336e8:	b.ne	33884 <__gmpn_gcd_subdiv_step@@Base+0x2e8>  // b.any
   336ec:	sub	x8, x26, #0x8
   336f0:	mov	x9, x22
   336f4:	subs	x10, x9, #0x1
   336f8:	b.lt	33948 <__gmpn_gcd_subdiv_step@@Base+0x3ac>  // b.tstop
   336fc:	lsl	x9, x9, #3
   33700:	ldr	x11, [x8, x9]
   33704:	add	x9, x25, x9
   33708:	ldur	x9, [x9, #-8]
   3370c:	cmp	x11, x9
   33710:	mov	x9, x10
   33714:	b.eq	336f4 <__gmpn_gcd_subdiv_step@@Base+0x158>  // b.none
   33718:	ldr	x8, [sp, #8]
   3371c:	adrp	x3, 5b000 <__gmpn_bases@@Base+0x2678>
   33720:	add	x3, x3, #0xa10
   33724:	mov	w4, #0x1                   	// #1
   33728:	mov	x0, x20
   3372c:	mov	x1, xzr
   33730:	mov	x2, xzr
   33734:	mov	w5, w27
   33738:	mov	x19, x21
   3373c:	cset	w21, hi  // hi = pmore
   33740:	blr	x8
   33744:	mov	x23, x22
   33748:	b	338bc <__gmpn_gcd_subdiv_step@@Base+0x320>
   3374c:	cbnz	x24, 3385c <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   33750:	eor	w5, w27, #0x1
   33754:	mov	x0, x20
   33758:	mov	x1, x25
   3375c:	b	3384c <__gmpn_gcd_subdiv_step@@Base+0x2b0>
   33760:	cbz	x28, 3379c <__gmpn_gcd_subdiv_step@@Base+0x200>
   33764:	mov	x0, x25
   33768:	mov	x1, x26
   3376c:	mov	x2, x25
   33770:	mov	x3, x28
   33774:	bl	ca70 <__gmpn_add_n@plt>
   33778:	cbz	x0, 3379c <__gmpn_gcd_subdiv_step@@Base+0x200>
   3377c:	cmp	x28, x22
   33780:	b.ge	3396c <__gmpn_gcd_subdiv_step@@Base+0x3d0>  // b.tcont
   33784:	lsl	x8, x28, #3
   33788:	ldr	x9, [x26, x8]
   3378c:	add	x28, x28, #0x1
   33790:	adds	x9, x9, #0x1
   33794:	str	x9, [x25, x8]
   33798:	b.cs	3377c <__gmpn_gcd_subdiv_step@@Base+0x1e0>  // b.hs, b.nlast
   3379c:	cmp	x25, x26
   337a0:	mov	x23, xzr
   337a4:	b.eq	33860 <__gmpn_gcd_subdiv_step@@Base+0x2c4>  // b.none
   337a8:	subs	x8, x22, x28
   337ac:	b.le	33860 <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   337b0:	cmp	x8, #0x4
   337b4:	b.cc	33818 <__gmpn_gcd_subdiv_step@@Base+0x27c>  // b.lo, b.ul, b.last
   337b8:	lsl	x10, x28, #3
   337bc:	lsl	x9, x22, #3
   337c0:	add	x11, x25, x10
   337c4:	add	x12, x26, x9
   337c8:	cmp	x11, x12
   337cc:	b.cs	337e0 <__gmpn_gcd_subdiv_step@@Base+0x244>  // b.hs, b.nlast
   337d0:	add	x9, x25, x9
   337d4:	add	x11, x26, x10
   337d8:	cmp	x11, x9
   337dc:	b.cc	33818 <__gmpn_gcd_subdiv_step@@Base+0x27c>  // b.lo, b.ul, b.last
   337e0:	and	x9, x8, #0xfffffffffffffffc
   337e4:	add	x11, x10, #0x10
   337e8:	add	x28, x28, x9
   337ec:	add	x10, x26, x11
   337f0:	add	x11, x25, x11
   337f4:	mov	x12, x9
   337f8:	ldp	q0, q1, [x10, #-16]
   337fc:	add	x10, x10, #0x20
   33800:	subs	x12, x12, #0x4
   33804:	stp	q0, q1, [x11, #-16]
   33808:	add	x11, x11, #0x20
   3380c:	b.ne	337f8 <__gmpn_gcd_subdiv_step@@Base+0x25c>  // b.any
   33810:	cmp	x8, x9
   33814:	b.eq	3385c <__gmpn_gcd_subdiv_step@@Base+0x2c0>  // b.none
   33818:	lsl	x10, x28, #3
   3381c:	sub	x8, x22, x28
   33820:	add	x9, x25, x10
   33824:	add	x10, x26, x10
   33828:	ldr	x11, [x10], #8
   3382c:	subs	x8, x8, #0x1
   33830:	str	x11, [x9], #8
   33834:	b.ne	33828 <__gmpn_gcd_subdiv_step@@Base+0x28c>  // b.any
   33838:	b	3385c <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   3383c:	cbnz	x24, 3385c <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   33840:	mov	w5, #0xffffffff            	// #-1
   33844:	mov	x0, x20
   33848:	mov	x1, x8
   3384c:	mov	x2, x23
   33850:	mov	x3, xzr
   33854:	mov	x4, xzr
   33858:	blr	x19
   3385c:	mov	x23, xzr
   33860:	mov	x0, x23
   33864:	ldp	x20, x19, [sp, #96]
   33868:	ldp	x22, x21, [sp, #80]
   3386c:	ldp	x24, x23, [sp, #64]
   33870:	ldp	x26, x25, [sp, #48]
   33874:	ldp	x28, x27, [sp, #32]
   33878:	ldp	x29, x30, [sp, #16]
   3387c:	add	sp, sp, #0x70
   33880:	ret
   33884:	ldr	x8, [sp, #8]
   33888:	adrp	x3, 5b000 <__gmpn_bases@@Base+0x2678>
   3388c:	add	x3, x3, #0xa10
   33890:	mov	w4, #0x1                   	// #1
   33894:	mov	x0, x20
   33898:	mov	x1, xzr
   3389c:	mov	x2, xzr
   338a0:	mov	w5, w27
   338a4:	mov	x19, x21
   338a8:	blr	x8
   338ac:	cmp	x22, x28
   338b0:	csel	x23, x28, x22, gt
   338b4:	csel	x22, x22, x28, gt
   338b8:	cset	w21, gt
   338bc:	cmp	w21, #0x0
   338c0:	csel	x28, x26, x25, ne  // ne = any
   338c4:	csel	x26, x25, x26, ne  // ne = any
   338c8:	mov	x0, x19
   338cc:	mov	x1, x28
   338d0:	mov	x2, xzr
   338d4:	mov	x3, x28
   338d8:	mov	x4, x22
   338dc:	mov	x5, x26
   338e0:	mov	x6, x23
   338e4:	eor	w25, w27, w21
   338e8:	bl	bf00 <__gmpn_tdiv_qr@plt>
   338ec:	ldr	x10, [sp, #8]
   338f0:	sub	x8, x22, x23
   338f4:	add	x22, x8, #0x1
   338f8:	mov	x8, x23
   338fc:	mov	x27, x8
   33900:	subs	x8, x8, #0x1
   33904:	b.lt	3391c <__gmpn_gcd_subdiv_step@@Base+0x380>  // b.tstop
   33908:	add	x9, x28, x27, lsl #3
   3390c:	ldur	x9, [x9, #-8]
   33910:	cbz	x9, 338fc <__gmpn_gcd_subdiv_step@@Base+0x360>
   33914:	mov	w8, #0x1                   	// #1
   33918:	b	33920 <__gmpn_gcd_subdiv_step@@Base+0x384>
   3391c:	mov	w8, wzr
   33920:	cmp	x27, x24
   33924:	b.le	339a0 <__gmpn_gcd_subdiv_step@@Base+0x404>
   33928:	mov	x0, x20
   3392c:	mov	x1, xzr
   33930:	mov	x2, xzr
   33934:	mov	x3, x19
   33938:	mov	x4, x22
   3393c:	mov	w5, w25
   33940:	blr	x10
   33944:	b	33860 <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   33948:	cmp	x24, #0x1
   3394c:	b.lt	3397c <__gmpn_gcd_subdiv_step@@Base+0x3e0>  // b.tstop
   33950:	adrp	x3, 5b000 <__gmpn_bases@@Base+0x2678>
   33954:	add	x3, x3, #0xa10
   33958:	mov	w4, #0x1                   	// #1
   3395c:	mov	x0, x20
   33960:	mov	x1, xzr
   33964:	mov	x2, xzr
   33968:	b	33990 <__gmpn_gcd_subdiv_step@@Base+0x3f4>
   3396c:	mov	w8, #0x1                   	// #1
   33970:	mov	x23, xzr
   33974:	str	x8, [x25, x22, lsl #3]
   33978:	b	33860 <__gmpn_gcd_subdiv_step@@Base+0x2c4>
   3397c:	mov	x0, x20
   33980:	mov	x1, x25
   33984:	mov	x2, x22
   33988:	mov	x3, xzr
   3398c:	mov	x4, xzr
   33990:	mov	w5, w27
   33994:	ldr	x8, [sp, #8]
   33998:	blr	x8
   3399c:	b	3385c <__gmpn_gcd_subdiv_step@@Base+0x2c0>
   339a0:	cbz	x24, 33a74 <__gmpn_gcd_subdiv_step@@Base+0x4d8>
   339a4:	cbz	w8, 33a90 <__gmpn_gcd_subdiv_step@@Base+0x4f4>
   339a8:	mov	x0, x28
   339ac:	mov	x1, x26
   339b0:	mov	x2, x28
   339b4:	mov	x3, x27
   339b8:	bl	ca70 <__gmpn_add_n@plt>
   339bc:	cbz	x0, 339e0 <__gmpn_gcd_subdiv_step@@Base+0x444>
   339c0:	cmp	x27, x23
   339c4:	b.ge	33aa4 <__gmpn_gcd_subdiv_step@@Base+0x508>  // b.tcont
   339c8:	lsl	x8, x27, #3
   339cc:	ldr	x9, [x26, x8]
   339d0:	add	x27, x27, #0x1
   339d4:	adds	x9, x9, #0x1
   339d8:	str	x9, [x28, x8]
   339dc:	b.cs	339c0 <__gmpn_gcd_subdiv_step@@Base+0x424>  // b.hs, b.nlast
   339e0:	cmp	x28, x26
   339e4:	b.eq	33ab4 <__gmpn_gcd_subdiv_step@@Base+0x518>  // b.none
   339e8:	subs	x8, x23, x27
   339ec:	b.le	33ab4 <__gmpn_gcd_subdiv_step@@Base+0x518>
   339f0:	cmp	x8, #0x4
   339f4:	b.cc	33a58 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.lo, b.ul, b.last
   339f8:	lsl	x10, x27, #3
   339fc:	lsl	x9, x23, #3
   33a00:	add	x11, x28, x10
   33a04:	add	x12, x26, x9
   33a08:	cmp	x11, x12
   33a0c:	b.cs	33a20 <__gmpn_gcd_subdiv_step@@Base+0x484>  // b.hs, b.nlast
   33a10:	add	x9, x28, x9
   33a14:	add	x11, x26, x10
   33a18:	cmp	x11, x9
   33a1c:	b.cc	33a58 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.lo, b.ul, b.last
   33a20:	and	x9, x8, #0xfffffffffffffffc
   33a24:	add	x11, x10, #0x10
   33a28:	add	x27, x27, x9
   33a2c:	add	x10, x26, x11
   33a30:	add	x11, x28, x11
   33a34:	mov	x12, x9
   33a38:	ldp	q0, q1, [x10, #-16]
   33a3c:	add	x10, x10, #0x20
   33a40:	subs	x12, x12, #0x4
   33a44:	stp	q0, q1, [x11, #-16]
   33a48:	add	x11, x11, #0x20
   33a4c:	b.ne	33a38 <__gmpn_gcd_subdiv_step@@Base+0x49c>  // b.any
   33a50:	cmp	x8, x9
   33a54:	b.eq	33ab4 <__gmpn_gcd_subdiv_step@@Base+0x518>  // b.none
   33a58:	lsl	x8, x27, #3
   33a5c:	ldr	x9, [x26, x8]
   33a60:	add	x27, x27, #0x1
   33a64:	cmp	x23, x27
   33a68:	str	x9, [x28, x8]
   33a6c:	b.ne	33a58 <__gmpn_gcd_subdiv_step@@Base+0x4bc>  // b.any
   33a70:	b	33ab4 <__gmpn_gcd_subdiv_step@@Base+0x518>
   33a74:	mov	x0, x20
   33a78:	mov	x1, x26
   33a7c:	mov	x2, x23
   33a80:	mov	x3, x19
   33a84:	mov	x4, x22
   33a88:	mov	w5, w25
   33a8c:	b	33994 <__gmpn_gcd_subdiv_step@@Base+0x3f8>
   33a90:	mov	x0, x28
   33a94:	mov	x1, x26
   33a98:	mov	x2, x23
   33a9c:	bl	ca50 <__gmpn_copyi@plt>
   33aa0:	b	33ab4 <__gmpn_gcd_subdiv_step@@Base+0x518>
   33aa4:	add	x8, x23, #0x1
   33aa8:	mov	w9, #0x1                   	// #1
   33aac:	str	x9, [x28, x23, lsl #3]
   33ab0:	mov	x23, x8
   33ab4:	mov	x8, x19
   33ab8:	ldr	x9, [x8]
   33abc:	sub	x10, x9, #0x1
   33ac0:	str	x10, [x8], #8
   33ac4:	cbz	x9, 33ab8 <__gmpn_gcd_subdiv_step@@Base+0x51c>
   33ac8:	ldr	x10, [sp, #8]
   33acc:	b	33928 <__gmpn_gcd_subdiv_step@@Base+0x38c>

0000000000033ad0 <__gmpn_gcdext_hook@@Base>:
   33ad0:	stp	x29, x30, [sp, #-96]!
   33ad4:	stp	x26, x25, [sp, #32]
   33ad8:	stp	x24, x23, [sp, #48]
   33adc:	stp	x22, x21, [sp, #64]
   33ae0:	stp	x20, x19, [sp, #80]
   33ae4:	ldr	x20, [x0, #32]
   33ae8:	mov	w21, w5
   33aec:	mov	x19, x0
   33af0:	str	x27, [sp, #16]
   33af4:	mov	x29, sp
   33af8:	cbz	x1, 33b44 <__gmpn_gcdext_hook@@Base+0x74>
   33afc:	ldr	x0, [x19]
   33b00:	mov	x22, x2
   33b04:	bl	ca50 <__gmpn_copyi@plt>
   33b08:	str	x22, [x19, #8]
   33b0c:	tbz	w21, #31, 33bc0 <__gmpn_gcdext_hook@@Base+0xf0>
   33b10:	sub	x8, x20, #0x1
   33b14:	add	x9, x8, #0x1
   33b18:	cmp	x9, #0x1
   33b1c:	b.lt	33bbc <__gmpn_gcdext_hook@@Base+0xec>  // b.tstop
   33b20:	ldp	x9, x10, [x19, #40]
   33b24:	lsl	x11, x8, #3
   33b28:	sub	x8, x8, #0x1
   33b2c:	ldr	x9, [x9, x11]
   33b30:	ldr	x10, [x10, x11]
   33b34:	cmp	x9, x10
   33b38:	b.eq	33b14 <__gmpn_gcdext_hook@@Base+0x44>  // b.none
   33b3c:	cset	w21, ls  // ls = plast
   33b40:	b	33bc0 <__gmpn_gcdext_hook@@Base+0xf0>
   33b44:	add	x10, x3, x4, lsl #3
   33b48:	ldp	x8, x9, [x19, #40]
   33b4c:	ldur	x10, [x10, #-8]
   33b50:	cmp	w21, #0x0
   33b54:	mov	x25, x4
   33b58:	csel	x21, x8, x9, eq  // eq = none
   33b5c:	csel	x8, x9, x8, eq  // eq = none
   33b60:	cmp	x10, #0x0
   33b64:	cset	w9, eq  // eq = none
   33b68:	sub	x4, x4, x9
   33b6c:	csetm	x26, eq  // eq = none
   33b70:	cmp	x4, #0x1
   33b74:	b.ne	33b9c <__gmpn_gcdext_hook@@Base+0xcc>  // b.any
   33b78:	ldr	x3, [x3]
   33b7c:	mov	x0, x21
   33b80:	cmp	x3, #0x1
   33b84:	b.ne	33c34 <__gmpn_gcdext_hook@@Base+0x164>  // b.any
   33b88:	mov	x1, x21
   33b8c:	mov	x2, x8
   33b90:	mov	x3, x20
   33b94:	bl	ca70 <__gmpn_add_n@plt>
   33b98:	b	33de8 <__gmpn_gcdext_hook@@Base+0x318>
   33b9c:	sub	x9, x8, #0x8
   33ba0:	mov	x10, x20
   33ba4:	mov	x23, x10
   33ba8:	subs	x10, x10, #0x1
   33bac:	b.lt	33c0c <__gmpn_gcdext_hook@@Base+0x13c>  // b.tstop
   33bb0:	ldr	x11, [x9, x23, lsl #3]
   33bb4:	cbz	x11, 33ba4 <__gmpn_gcdext_hook@@Base+0xd4>
   33bb8:	b	33c10 <__gmpn_gcdext_hook@@Base+0x140>
   33bbc:	mov	w21, wzr
   33bc0:	cmp	w21, #0x0
   33bc4:	mov	w8, #0x30                  	// #48
   33bc8:	mov	w9, #0x28                  	// #40
   33bcc:	csel	x8, x9, x8, ne  // ne = any
   33bd0:	ldr	x1, [x19, x8]
   33bd4:	mov	x22, x20
   33bd8:	subs	x20, x20, #0x1
   33bdc:	b.lt	33bec <__gmpn_gcdext_hook@@Base+0x11c>  // b.tstop
   33be0:	add	x8, x1, x22, lsl #3
   33be4:	ldur	x8, [x8, #-8]
   33be8:	cbz	x8, 33bd4 <__gmpn_gcdext_hook@@Base+0x104>
   33bec:	ldr	x0, [x19, #16]
   33bf0:	mov	x2, x22
   33bf4:	bl	ca50 <__gmpn_copyi@plt>
   33bf8:	ldr	x8, [x19, #24]
   33bfc:	cmp	w21, #0x0
   33c00:	cneg	x9, x22, ne  // ne = any
   33c04:	str	x9, [x8]
   33c08:	b	33df8 <__gmpn_gcdext_hook@@Base+0x328>
   33c0c:	cbz	x23, 33df8 <__gmpn_gcdext_hook@@Base+0x328>
   33c10:	ldr	x24, [x19, #56]
   33c14:	cmp	x4, x23
   33c18:	mov	x0, x24
   33c1c:	b.le	33c44 <__gmpn_gcdext_hook@@Base+0x174>
   33c20:	mov	x1, x3
   33c24:	mov	x2, x4
   33c28:	mov	x3, x8
   33c2c:	mov	x4, x23
   33c30:	b	33c4c <__gmpn_gcdext_hook@@Base+0x17c>
   33c34:	mov	x1, x8
   33c38:	mov	x2, x20
   33c3c:	bl	d400 <__gmpn_addmul_1@plt>
   33c40:	b	33de8 <__gmpn_gcdext_hook@@Base+0x318>
   33c44:	mov	x1, x8
   33c48:	mov	x2, x23
   33c4c:	bl	ccd0 <__gmpn_mul@plt>
   33c50:	lsl	x8, x25, #3
   33c54:	add	x8, x8, x26, lsl #3
   33c58:	add	x8, x8, x24
   33c5c:	add	x8, x8, x23, lsl #3
   33c60:	ldur	x8, [x8, #-8]
   33c64:	add	x9, x25, x26
   33c68:	cmp	x8, #0x0
   33c6c:	cset	w8, eq  // eq = none
   33c70:	sub	x9, x9, x8
   33c74:	add	x22, x9, x23
   33c78:	csetm	x27, eq  // eq = none
   33c7c:	cmp	x22, x20
   33c80:	b.ge	33cd8 <__gmpn_gcdext_hook@@Base+0x208>  // b.tcont
   33c84:	sub	x8, x26, x8
   33c88:	add	x8, x8, x25
   33c8c:	add	x8, x8, x23
   33c90:	cbz	x8, 33cd0 <__gmpn_gcdext_hook@@Base+0x200>
   33c94:	mov	x0, x21
   33c98:	mov	x1, x21
   33c9c:	mov	x2, x24
   33ca0:	mov	x3, x22
   33ca4:	bl	ca70 <__gmpn_add_n@plt>
   33ca8:	cbz	x0, 33de8 <__gmpn_gcdext_hook@@Base+0x318>
   33cac:	mov	w0, #0x1                   	// #1
   33cb0:	cmp	x22, x20
   33cb4:	b.ge	33de8 <__gmpn_gcdext_hook@@Base+0x318>  // b.tcont
   33cb8:	lsl	x8, x22, #3
   33cbc:	ldr	x9, [x21, x8]
   33cc0:	add	x22, x22, #0x1
   33cc4:	adds	x9, x9, #0x1
   33cc8:	str	x9, [x21, x8]
   33ccc:	b.cs	33cb0 <__gmpn_gcdext_hook@@Base+0x1e0>  // b.hs, b.nlast
   33cd0:	mov	x0, xzr
   33cd4:	b	33de8 <__gmpn_gcdext_hook@@Base+0x318>
   33cd8:	cbz	x20, 33d18 <__gmpn_gcdext_hook@@Base+0x248>
   33cdc:	mov	x0, x21
   33ce0:	mov	x1, x24
   33ce4:	mov	x2, x21
   33ce8:	mov	x3, x20
   33cec:	bl	ca70 <__gmpn_add_n@plt>
   33cf0:	cbz	x0, 33d18 <__gmpn_gcdext_hook@@Base+0x248>
   33cf4:	mov	w0, #0x1                   	// #1
   33cf8:	cmp	x20, x22
   33cfc:	b.ge	33de4 <__gmpn_gcdext_hook@@Base+0x314>  // b.tcont
   33d00:	lsl	x8, x20, #3
   33d04:	ldr	x9, [x24, x8]
   33d08:	add	x20, x20, #0x1
   33d0c:	adds	x9, x9, #0x1
   33d10:	str	x9, [x21, x8]
   33d14:	b.cs	33cf8 <__gmpn_gcdext_hook@@Base+0x228>  // b.hs, b.nlast
   33d18:	cmp	x21, x24
   33d1c:	mov	x0, xzr
   33d20:	b.eq	33de4 <__gmpn_gcdext_hook@@Base+0x314>  // b.none
   33d24:	cmp	x20, x22
   33d28:	b.ge	33de4 <__gmpn_gcdext_hook@@Base+0x314>  // b.tcont
   33d2c:	sub	x11, x27, x20
   33d30:	add	x8, x25, x26
   33d34:	add	x9, x11, x8
   33d38:	add	x9, x9, x23
   33d3c:	cmp	x9, #0x4
   33d40:	b.cc	33db8 <__gmpn_gcdext_hook@@Base+0x2e8>  // b.lo, b.ul, b.last
   33d44:	add	x10, x27, x8
   33d48:	add	x10, x10, x23
   33d4c:	lsl	x12, x20, #3
   33d50:	lsl	x10, x10, #3
   33d54:	add	x13, x21, x12
   33d58:	add	x14, x24, x10
   33d5c:	cmp	x13, x14
   33d60:	b.cs	33d74 <__gmpn_gcdext_hook@@Base+0x2a4>  // b.hs, b.nlast
   33d64:	add	x10, x21, x10
   33d68:	add	x13, x24, x12
   33d6c:	cmp	x13, x10
   33d70:	b.cc	33db8 <__gmpn_gcdext_hook@@Base+0x2e8>  // b.lo, b.ul, b.last
   33d74:	add	x14, x25, x26
   33d78:	add	x11, x11, x14
   33d7c:	and	x10, x9, #0xfffffffffffffffc
   33d80:	add	x13, x12, #0x10
   33d84:	add	x11, x11, x23
   33d88:	add	x20, x20, x10
   33d8c:	add	x12, x24, x13
   33d90:	add	x13, x21, x13
   33d94:	and	x11, x11, #0xfffffffffffffffc
   33d98:	ldp	q0, q1, [x12, #-16]
   33d9c:	add	x12, x12, #0x20
   33da0:	subs	x11, x11, #0x4
   33da4:	stp	q0, q1, [x13, #-16]
   33da8:	add	x13, x13, #0x20
   33dac:	b.ne	33d98 <__gmpn_gcdext_hook@@Base+0x2c8>  // b.any
   33db0:	cmp	x9, x10
   33db4:	b.eq	33de0 <__gmpn_gcdext_hook@@Base+0x310>  // b.none
   33db8:	sub	x9, x27, x20
   33dbc:	lsl	x10, x20, #3
   33dc0:	add	x9, x9, x8
   33dc4:	add	x8, x21, x10
   33dc8:	add	x9, x9, x23
   33dcc:	add	x10, x24, x10
   33dd0:	ldr	x11, [x10], #8
   33dd4:	subs	x9, x9, #0x1
   33dd8:	str	x11, [x8], #8
   33ddc:	b.ne	33dd0 <__gmpn_gcdext_hook@@Base+0x300>  // b.any
   33de0:	mov	x0, xzr
   33de4:	mov	x20, x22
   33de8:	cmp	x0, #0x0
   33dec:	cinc	x8, x20, ne  // ne = any
   33df0:	str	x0, [x21, x20, lsl #3]
   33df4:	str	x8, [x19, #32]
   33df8:	ldp	x20, x19, [sp, #80]
   33dfc:	ldp	x22, x21, [sp, #64]
   33e00:	ldp	x24, x23, [sp, #48]
   33e04:	ldp	x26, x25, [sp, #32]
   33e08:	ldr	x27, [sp, #16]
   33e0c:	ldp	x29, x30, [sp], #96
   33e10:	ret

0000000000033e14 <__gmpn_gcdext_lehmer_n@@Base>:
   33e14:	sub	sp, sp, #0xe0
   33e18:	stp	x28, x27, [sp, #144]
   33e1c:	stp	x26, x25, [sp, #160]
   33e20:	stp	x24, x23, [sp, #176]
   33e24:	stp	x20, x19, [sp, #208]
   33e28:	mov	x19, x6
   33e2c:	mov	x26, x5
   33e30:	mov	x27, x4
   33e34:	mov	x28, x3
   33e38:	mov	x23, x2
   33e3c:	mov	x24, x1
   33e40:	adds	x20, x5, #0x1
   33e44:	mov	x25, x0
   33e48:	stp	x29, x30, [sp, #128]
   33e4c:	stp	x22, x21, [sp, #192]
   33e50:	add	x29, sp, #0x80
   33e54:	b.cs	33e70 <__gmpn_gcdext_lehmer_n@@Base+0x5c>  // b.hs, b.nlast
   33e58:	mov	w8, #0x18                  	// #24
   33e5c:	orr	x9, xzr, #0x18
   33e60:	madd	x2, x26, x8, x9
   33e64:	mov	x0, x19
   33e68:	mov	w1, wzr
   33e6c:	bl	c5f0 <memset@plt>
   33e70:	lsl	x8, x20, #3
   33e74:	mov	w21, #0x1                   	// #1
   33e78:	cmp	x26, #0x2
   33e7c:	add	x22, x19, x8
   33e80:	str	x21, [x22]
   33e84:	stp	x25, x24, [sp, #8]
   33e88:	str	x25, [sp, #64]
   33e8c:	stp	x24, x23, [sp, #80]
   33e90:	str	x23, [sp]
   33e94:	b.lt	34008 <__gmpn_gcdext_lehmer_n@@Base+0x1f4>  // b.tstop
   33e98:	add	x23, x22, x8
   33e9c:	mov	w21, #0x1                   	// #1
   33ea0:	add	x20, x23, x20, lsl #3
   33ea4:	mov	x25, x26
   33ea8:	mov	x24, x19
   33eac:	b	33f08 <__gmpn_gcdext_lehmer_n@@Base+0xf4>
   33eb0:	add	x0, sp, #0x20
   33eb4:	mov	x1, x20
   33eb8:	mov	x2, x28
   33ebc:	mov	x3, x27
   33ec0:	mov	x4, x25
   33ec4:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   33ec8:	mov	x25, x0
   33ecc:	add	x0, sp, #0x20
   33ed0:	mov	x1, x23
   33ed4:	mov	x2, x24
   33ed8:	mov	x3, x22
   33edc:	mov	x4, x21
   33ee0:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   33ee4:	mov	x21, x0
   33ee8:	mov	x0, x24
   33eec:	mov	x1, x28
   33ef0:	mov	x28, x20
   33ef4:	mov	x24, x23
   33ef8:	mov	x20, x1
   33efc:	mov	x23, x0
   33f00:	cmp	x25, #0x1
   33f04:	b.le	3400c <__gmpn_gcdext_lehmer_n@@Base+0x1f8>
   33f08:	lsl	x9, x25, #3
   33f0c:	sub	x8, x9, #0x8
   33f10:	ldr	x0, [x28, x8]
   33f14:	ldr	x2, [x27, x8]
   33f18:	orr	x8, x2, x0
   33f1c:	tbnz	x8, #63, 33f5c <__gmpn_gcdext_lehmer_n@@Base+0x148>
   33f20:	cmp	x25, #0x2
   33f24:	clz	x8, x8
   33f28:	b.ne	33f6c <__gmpn_gcdext_lehmer_n@@Base+0x158>  // b.any
   33f2c:	ldp	x10, x9, [x28]
   33f30:	ldp	x13, x12, [x27]
   33f34:	neg	x11, x8
   33f38:	lsl	x9, x9, x8
   33f3c:	lsr	x14, x10, x11
   33f40:	lsl	x1, x10, x8
   33f44:	lsl	x10, x12, x8
   33f48:	lsr	x11, x13, x11
   33f4c:	orr	x0, x14, x9
   33f50:	orr	x2, x11, x10
   33f54:	lsl	x3, x13, x8
   33f58:	b	33fb8 <__gmpn_gcdext_lehmer_n@@Base+0x1a4>
   33f5c:	sub	x8, x9, #0x10
   33f60:	ldr	x1, [x28, x8]
   33f64:	ldr	x3, [x27, x8]
   33f68:	b	33fb8 <__gmpn_gcdext_lehmer_n@@Base+0x1a4>
   33f6c:	sub	x11, x9, #0x10
   33f70:	sub	x9, x9, #0x18
   33f74:	ldr	x14, [x28, x11]
   33f78:	ldr	x15, [x28, x9]
   33f7c:	ldr	x11, [x27, x11]
   33f80:	ldr	x9, [x27, x9]
   33f84:	neg	x12, x8
   33f88:	lsl	x10, x0, x8
   33f8c:	lsl	x13, x2, x8
   33f90:	lsr	x16, x14, x12
   33f94:	lsl	x14, x14, x8
   33f98:	lsr	x15, x15, x12
   33f9c:	lsl	x8, x11, x8
   33fa0:	lsr	x11, x11, x12
   33fa4:	lsr	x9, x9, x12
   33fa8:	orr	x0, x16, x10
   33fac:	orr	x1, x15, x14
   33fb0:	orr	x2, x11, x13
   33fb4:	orr	x3, x9, x8
   33fb8:	add	x4, sp, #0x20
   33fbc:	bl	c5a0 <__gmpn_hgcd2@plt>
   33fc0:	cbnz	w0, 33eb0 <__gmpn_gcdext_lehmer_n@@Base+0x9c>
   33fc4:	stp	x22, x23, [sp, #112]
   33fc8:	stp	x21, x24, [sp, #96]
   33fcc:	adrp	x4, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   33fd0:	ldr	x4, [x4, #3968]
   33fd4:	add	x5, sp, #0x40
   33fd8:	mov	x0, x28
   33fdc:	mov	x1, x27
   33fe0:	mov	x2, x25
   33fe4:	mov	x3, xzr
   33fe8:	mov	x6, x20
   33fec:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   33ff0:	cbz	x0, 3412c <__gmpn_gcdext_lehmer_n@@Base+0x318>
   33ff4:	ldr	x21, [sp, #96]
   33ff8:	mov	x25, x0
   33ffc:	cmp	x25, #0x1
   34000:	b.gt	33f08 <__gmpn_gcdext_lehmer_n@@Base+0xf4>
   34004:	b	3400c <__gmpn_gcdext_lehmer_n@@Base+0x1f8>
   34008:	mov	x24, x19
   3400c:	ldr	x2, [x28]
   34010:	cbz	x2, 34218 <__gmpn_gcdext_lehmer_n@@Base+0x404>
   34014:	ldr	x3, [x27]
   34018:	ldp	x20, x23, [sp, #8]
   3401c:	cbz	x3, 34230 <__gmpn_gcdext_lehmer_n@@Base+0x41c>
   34020:	cmp	x2, x3
   34024:	b.ne	340c0 <__gmpn_gcdext_lehmer_n@@Base+0x2ac>  // b.any
   34028:	add	x8, x19, x26, lsl #3
   3402c:	mov	x9, x21
   34030:	str	x2, [x20]
   34034:	subs	x10, x9, #0x1
   34038:	b.lt	3405c <__gmpn_gcdext_lehmer_n@@Base+0x248>  // b.tstop
   3403c:	lsl	x9, x9, #3
   34040:	add	x11, x24, x9
   34044:	ldur	x11, [x11, #-8]
   34048:	ldr	x9, [x8, x9]
   3404c:	cmp	x11, x9
   34050:	mov	x9, x10
   34054:	b.eq	34034 <__gmpn_gcdext_lehmer_n@@Base+0x220>  // b.none
   34058:	b.ls	34088 <__gmpn_gcdext_lehmer_n@@Base+0x274>  // b.plast
   3405c:	add	x8, x19, x26, lsl #3
   34060:	ldr	x10, [x8, x21, lsl #3]
   34064:	sub	x9, x21, #0x1
   34068:	mov	x21, x9
   3406c:	cbz	x10, 34060 <__gmpn_gcdext_lehmer_n@@Base+0x24c>
   34070:	add	x19, x9, #0x1
   34074:	mov	x0, x23
   34078:	mov	x1, x22
   3407c:	mov	x2, x19
   34080:	bl	ca50 <__gmpn_copyi@plt>
   34084:	b	340b4 <__gmpn_gcdext_lehmer_n@@Base+0x2a0>
   34088:	mov	x19, x21
   3408c:	subs	x21, x21, #0x1
   34090:	b.lt	340a0 <__gmpn_gcdext_lehmer_n@@Base+0x28c>  // b.tstop
   34094:	add	x8, x24, x19, lsl #3
   34098:	ldur	x8, [x8, #-8]
   3409c:	cbz	x8, 34088 <__gmpn_gcdext_lehmer_n@@Base+0x274>
   340a0:	mov	x0, x23
   340a4:	mov	x1, x24
   340a8:	mov	x2, x19
   340ac:	bl	ca50 <__gmpn_copyi@plt>
   340b0:	neg	x19, x19
   340b4:	ldr	x8, [sp]
   340b8:	str	x19, [x8]
   340bc:	b	341f4 <__gmpn_gcdext_lehmer_n@@Base+0x3e0>
   340c0:	add	x0, sp, #0x20
   340c4:	add	x1, sp, #0x18
   340c8:	bl	d140 <__gmpn_gcdext_1@plt>
   340cc:	str	x0, [x20]
   340d0:	ldr	x3, [sp, #32]
   340d4:	cbz	x3, 34104 <__gmpn_gcdext_lehmer_n@@Base+0x2f0>
   340d8:	ldr	x8, [sp, #24]
   340dc:	cbz	x8, 34134 <__gmpn_gcdext_lehmer_n@@Base+0x320>
   340e0:	cmp	x3, #0x1
   340e4:	b.lt	34164 <__gmpn_gcdext_lehmer_n@@Base+0x350>  // b.tstop
   340e8:	neg	x8, x8
   340ec:	mov	w20, wzr
   340f0:	str	x8, [sp, #24]
   340f4:	b	34170 <__gmpn_gcdext_lehmer_n@@Base+0x35c>
   340f8:	add	x8, x24, x19, lsl #3
   340fc:	ldur	x8, [x8, #-8]
   34100:	cbnz	x8, 34110 <__gmpn_gcdext_lehmer_n@@Base+0x2fc>
   34104:	mov	x19, x21
   34108:	subs	x21, x21, #0x1
   3410c:	b.ge	340f8 <__gmpn_gcdext_lehmer_n@@Base+0x2e4>  // b.tcont
   34110:	mov	x0, x23
   34114:	mov	x1, x24
   34118:	mov	x2, x19
   3411c:	bl	ca50 <__gmpn_copyi@plt>
   34120:	ldr	x11, [sp]
   34124:	neg	x19, x19
   34128:	b	341f0 <__gmpn_gcdext_lehmer_n@@Base+0x3dc>
   3412c:	ldr	x0, [sp, #72]
   34130:	b	341f8 <__gmpn_gcdext_lehmer_n@@Base+0x3e4>
   34134:	add	x8, x19, x26, lsl #3
   34138:	mov	x19, x21
   3413c:	subs	x21, x21, #0x1
   34140:	b.lt	3414c <__gmpn_gcdext_lehmer_n@@Base+0x338>  // b.tstop
   34144:	ldr	x9, [x8, x19, lsl #3]
   34148:	cbz	x9, 34138 <__gmpn_gcdext_lehmer_n@@Base+0x324>
   3414c:	mov	x0, x23
   34150:	mov	x1, x22
   34154:	mov	x2, x19
   34158:	bl	ca50 <__gmpn_copyi@plt>
   3415c:	ldr	x11, [sp]
   34160:	b	341f0 <__gmpn_gcdext_lehmer_n@@Base+0x3dc>
   34164:	neg	x3, x3
   34168:	mov	w20, #0x1                   	// #1
   3416c:	str	x3, [sp, #32]
   34170:	mov	x0, x23
   34174:	mov	x1, x22
   34178:	mov	x2, x21
   3417c:	bl	d490 <__gmpn_mul_1@plt>
   34180:	ldr	x3, [sp, #24]
   34184:	mov	x19, x0
   34188:	mov	x0, x23
   3418c:	mov	x1, x24
   34190:	mov	x2, x21
   34194:	bl	d400 <__gmpn_addmul_1@plt>
   34198:	orr	x8, x0, x19
   3419c:	cbz	x8, 341c4 <__gmpn_gcdext_lehmer_n@@Base+0x3b0>
   341a0:	ldr	x11, [sp]
   341a4:	adds	x9, x0, x19
   341a8:	add	x8, x21, #0x1
   341ac:	str	x9, [x23, x21, lsl #3]
   341b0:	b.cc	341cc <__gmpn_gcdext_lehmer_n@@Base+0x3b8>  // b.lo, b.ul, b.last
   341b4:	add	x21, x21, #0x2
   341b8:	mov	w9, #0x1                   	// #1
   341bc:	str	x9, [x23, x8, lsl #3]
   341c0:	b	341d0 <__gmpn_gcdext_lehmer_n@@Base+0x3bc>
   341c4:	ldr	x11, [sp]
   341c8:	b	341d0 <__gmpn_gcdext_lehmer_n@@Base+0x3bc>
   341cc:	mov	x21, x8
   341d0:	sub	x8, x23, #0x8
   341d4:	ldr	x10, [x8, x21, lsl #3]
   341d8:	sub	x9, x21, #0x1
   341dc:	mov	x21, x9
   341e0:	cbz	x10, 341d4 <__gmpn_gcdext_lehmer_n@@Base+0x3c0>
   341e4:	mvn	x8, x9
   341e8:	cmp	w20, #0x0
   341ec:	csinc	x19, x8, x9, ne  // ne = any
   341f0:	str	x19, [x11]
   341f4:	mov	w0, #0x1                   	// #1
   341f8:	ldp	x20, x19, [sp, #208]
   341fc:	ldp	x22, x21, [sp, #192]
   34200:	ldp	x24, x23, [sp, #176]
   34204:	ldp	x26, x25, [sp, #160]
   34208:	ldp	x28, x27, [sp, #144]
   3420c:	ldp	x29, x30, [sp, #128]
   34210:	add	sp, sp, #0xe0
   34214:	ret
   34218:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   3421c:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   34220:	add	x0, x0, #0xa18
   34224:	add	x2, x2, #0xa28
   34228:	mov	w1, #0xf9                  	// #249
   3422c:	bl	c6c0 <__gmp_assert_fail@plt>
   34230:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   34234:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   34238:	add	x0, x0, #0xa18
   3423c:	add	x2, x2, #0xa32
   34240:	mov	w1, #0xfa                  	// #250
   34244:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000034248 <__gmpn_div_q@@Base>:
   34248:	stp	x29, x30, [sp, #-96]!
   3424c:	stp	x28, x27, [sp, #16]
   34250:	stp	x26, x25, [sp, #32]
   34254:	stp	x24, x23, [sp, #48]
   34258:	stp	x22, x21, [sp, #64]
   3425c:	stp	x20, x19, [sp, #80]
   34260:	mov	x29, sp
   34264:	sub	sp, sp, #0x60
   34268:	stur	xzr, [x29, #-16]
   3426c:	subs	x25, x4, #0x1
   34270:	ldr	x28, [x3, x25, lsl #3]
   34274:	mov	x21, x2
   34278:	mov	x2, x1
   3427c:	mov	x19, x0
   34280:	b.ne	3429c <__gmpn_div_q@@Base+0x54>  // b.any
   34284:	mov	x0, x19
   34288:	mov	x1, xzr
   3428c:	mov	x3, x21
   34290:	mov	x4, x28
   34294:	bl	cd00 <__gmpn_divrem_1@plt>
   34298:	b	34cc4 <__gmpn_div_q@@Base+0xa7c>
   3429c:	sub	x22, x21, x4
   342a0:	add	x8, x22, #0x6
   342a4:	mov	x23, x5
   342a8:	mov	x20, x4
   342ac:	mov	x24, x3
   342b0:	cmp	x8, x4
   342b4:	b.ge	3439c <__gmpn_div_q@@Base+0x154>  // b.tcont
   342b8:	add	x8, x22, #0x2
   342bc:	stp	x22, x8, [x29, #-48]
   342c0:	lsl	x26, x8, #3
   342c4:	mov	w8, #0x7f00                	// #32512
   342c8:	cmp	x26, x8
   342cc:	add	x25, x22, #0x1
   342d0:	mov	x22, x24
   342d4:	stur	x23, [x29, #-24]
   342d8:	b.hi	3485c <__gmpn_div_q@@Base+0x614>  // b.pmore
   342dc:	add	x9, x26, #0xf
   342e0:	mov	x8, sp
   342e4:	and	x9, x9, #0xfffffffffffffff0
   342e8:	sub	x8, x8, x9
   342ec:	stur	x8, [x29, #-56]
   342f0:	mov	sp, x8
   342f4:	ldur	x8, [x29, #-24]
   342f8:	mov	w27, #0x1                   	// #1
   342fc:	bfi	x27, x25, #1, #63
   34300:	lsl	x24, x27, #3
   34304:	cmp	x8, x2
   34308:	stur	x25, [x29, #-32]
   3430c:	b.ne	34338 <__gmpn_div_q@@Base+0xf0>  // b.any
   34310:	add	x1, x24, #0x8
   34314:	mov	w8, #0x7f00                	// #32512
   34318:	cmp	x1, x8
   3431c:	b.hi	34984 <__gmpn_div_q@@Base+0x73c>  // b.pmore
   34320:	add	x9, x1, #0xf
   34324:	mov	x8, sp
   34328:	and	x9, x9, #0xfffffffffffffff0
   3432c:	sub	x8, x8, x9
   34330:	stur	x8, [x29, #-24]
   34334:	mov	sp, x8
   34338:	stp	x2, x22, [x29, #-80]
   3433c:	tbnz	x28, #63, 349a0 <__gmpn_div_q@@Base+0x758>
   34340:	ldur	x25, [x29, #-24]
   34344:	clz	x28, x28
   34348:	add	x8, x2, x21, lsl #3
   3434c:	sub	x1, x8, x24
   34350:	mov	x0, x25
   34354:	mov	x2, x27
   34358:	mov	w3, w28
   3435c:	bl	c180 <__gmpn_lshift@plt>
   34360:	cmp	x0, #0x0
   34364:	mov	w8, #0x7f00                	// #32512
   34368:	cset	w9, ne  // ne = any
   3436c:	cinc	x23, x27, ne  // ne = any
   34370:	cmp	x26, x8
   34374:	str	x0, [x25, x24]
   34378:	stur	x9, [x29, #-88]
   3437c:	stur	x0, [x29, #-64]
   34380:	b.hi	3442c <__gmpn_div_q@@Base+0x1e4>  // b.pmore
   34384:	add	x9, x26, #0xf
   34388:	mov	x8, sp
   3438c:	and	x9, x9, #0xfffffffffffffff0
   34390:	sub	x27, x8, x9
   34394:	mov	sp, x27
   34398:	b	3443c <__gmpn_div_q@@Base+0x1f4>
   3439c:	tbnz	x28, #63, 34878 <__gmpn_div_q@@Base+0x630>
   343a0:	clz	x27, x28
   343a4:	mov	x0, x23
   343a8:	mov	x1, x2
   343ac:	mov	x2, x21
   343b0:	mov	w3, w27
   343b4:	bl	c180 <__gmpn_lshift@plt>
   343b8:	cmp	x0, #0x0
   343bc:	lsl	x1, x20, #3
   343c0:	mov	w8, #0x7f00                	// #32512
   343c4:	cinc	x28, x21, ne  // ne = any
   343c8:	cmp	x1, x8
   343cc:	stur	x0, [x29, #-24]
   343d0:	str	x0, [x23, x21, lsl #3]
   343d4:	b.hi	348c0 <__gmpn_div_q@@Base+0x678>  // b.pmore
   343d8:	add	x9, x1, #0xf
   343dc:	mov	x8, sp
   343e0:	and	x9, x9, #0xfffffffffffffff0
   343e4:	sub	x26, x8, x9
   343e8:	mov	sp, x26
   343ec:	mov	x0, x26
   343f0:	mov	x1, x24
   343f4:	mov	x2, x20
   343f8:	mov	w3, w27
   343fc:	bl	c180 <__gmpn_lshift@plt>
   34400:	cmp	x20, #0x2
   34404:	b.ne	344f8 <__gmpn_div_q@@Base+0x2b0>  // b.any
   34408:	mov	x0, x19
   3440c:	mov	x1, xzr
   34410:	mov	x2, x23
   34414:	mov	x3, x28
   34418:	mov	x4, x26
   3441c:	bl	c200 <__gmpn_divrem_2@plt>
   34420:	ldur	x25, [x29, #-24]
   34424:	cbnz	x25, 34cbc <__gmpn_div_q@@Base+0xa74>
   34428:	b	34854 <__gmpn_div_q@@Base+0x60c>
   3442c:	sub	x0, x29, #0x10
   34430:	mov	x1, x26
   34434:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34438:	mov	x27, x0
   3443c:	mov	x24, x22
   34440:	add	x8, x22, x20, lsl #3
   34444:	ldp	x22, x26, [x29, #-48]
   34448:	mov	x9, #0xfffffffffffffffe    	// #-2
   3444c:	mov	x0, x27
   34450:	mov	w3, w28
   34454:	sub	x9, x9, x22
   34458:	add	x1, x8, x9, lsl #3
   3445c:	mov	x2, x26
   34460:	bl	c180 <__gmpn_lshift@plt>
   34464:	sub	x8, x20, x22
   34468:	add	x8, x24, x8, lsl #3
   3446c:	ldur	x8, [x8, #-24]
   34470:	ldr	x9, [x27]
   34474:	ldp	x25, x24, [x29, #-32]
   34478:	neg	x10, x28
   3447c:	lsr	x8, x8, x10
   34480:	orr	x8, x9, x8
   34484:	str	x8, [x27]
   34488:	cbz	x22, 345b8 <__gmpn_div_q@@Base+0x370>
   3448c:	cmp	x22, #0x95
   34490:	b.le	345e0 <__gmpn_div_q@@Base+0x398>
   34494:	cmp	x22, #0x3e3
   34498:	b.le	346b4 <__gmpn_div_q@@Base+0x46c>
   3449c:	mov	x0, x23
   344a0:	mov	x1, x26
   344a4:	mov	w2, wzr
   344a8:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   344ac:	lsl	x1, x0, #3
   344b0:	mov	w8, #0x7f00                	// #32512
   344b4:	cmp	x1, x8
   344b8:	b.hi	34ba0 <__gmpn_div_q@@Base+0x958>  // b.pmore
   344bc:	add	x9, x1, #0xf
   344c0:	mov	x8, sp
   344c4:	and	x9, x9, #0xfffffffffffffff0
   344c8:	sub	x5, x8, x9
   344cc:	mov	sp, x5
   344d0:	ldur	x22, [x29, #-56]
   344d4:	mov	x1, x24
   344d8:	mov	x2, x23
   344dc:	mov	x3, x27
   344e0:	mov	x0, x22
   344e4:	mov	x4, x26
   344e8:	bl	c710 <__gmpn_mu_divappr_q@plt>
   344ec:	ldur	x8, [x29, #-64]
   344f0:	cbnz	x8, 347d0 <__gmpn_div_q@@Base+0x588>
   344f4:	b	34c30 <__gmpn_div_q@@Base+0x9e8>
   344f8:	cmp	x20, #0x98
   344fc:	b.lt	34648 <__gmpn_div_q@@Base+0x400>  // b.tstop
   34500:	sub	x8, x28, x20
   34504:	cmp	x8, #0x97
   34508:	b.le	34648 <__gmpn_div_q@@Base+0x400>
   3450c:	cmp	x21, #0x7cc
   34510:	b.lt	3454c <__gmpn_div_q@@Base+0x304>  // b.tstop
   34514:	mov	x8, #0x200000000000        	// #35184372088832
   34518:	mov	x9, #0x800000000000        	// #140737488355328
   3451c:	movk	x8, #0x409c, lsl #48
   34520:	movk	x9, #0x4058, lsl #48
   34524:	scvtf	d0, x20
   34528:	scvtf	d1, x21
   3452c:	fmov	d2, x8
   34530:	fmov	d3, x9
   34534:	fmul	d2, d0, d2
   34538:	fmul	d3, d1, d3
   3453c:	fadd	d2, d3, d2
   34540:	fmul	d0, d1, d0
   34544:	fcmp	d2, d0
   34548:	b.le	34800 <__gmpn_div_q@@Base+0x5b8>
   3454c:	ldr	x21, [x26, x25, lsl #3]
   34550:	mov	x0, x21
   34554:	bl	d3f0 <__gmpn_invert_limb@plt>
   34558:	add	x8, x26, x20, lsl #3
   3455c:	ldur	x8, [x8, #-16]
   34560:	mul	x9, x0, x21
   34564:	adds	x9, x9, x8
   34568:	b.cc	34584 <__gmpn_div_q@@Base+0x33c>  // b.lo, b.ul, b.last
   3456c:	subs	x9, x9, x21
   34570:	cset	w10, cs  // cs = hs, nlast
   34574:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   34578:	mvn	x10, x10
   3457c:	add	x0, x10, x0
   34580:	sub	x9, x9, x11
   34584:	ldur	x25, [x29, #-24]
   34588:	umulh	x10, x8, x0
   3458c:	adds	x10, x10, x9
   34590:	b.cc	34774 <__gmpn_div_q@@Base+0x52c>  // b.lo, b.ul, b.last
   34594:	cmp	x10, x21
   34598:	sub	x9, x0, #0x1
   3459c:	b.cc	34778 <__gmpn_div_q@@Base+0x530>  // b.lo, b.ul, b.last
   345a0:	mul	x11, x0, x8
   345a4:	cmp	x10, x21
   345a8:	sub	x12, x0, #0x2
   345ac:	ccmp	x11, x8, #0x2, ls  // ls = plast
   345b0:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   345b4:	b	34778 <__gmpn_div_q@@Base+0x530>
   345b8:	ldur	x22, [x29, #-56]
   345bc:	mov	x1, xzr
   345c0:	mov	x2, x24
   345c4:	mov	x3, x23
   345c8:	mov	x0, x22
   345cc:	mov	x4, x27
   345d0:	bl	c200 <__gmpn_divrem_2@plt>
   345d4:	ldur	x8, [x29, #-64]
   345d8:	cbnz	x8, 347d0 <__gmpn_div_q@@Base+0x588>
   345dc:	b	34c30 <__gmpn_div_q@@Base+0x9e8>
   345e0:	ldr	x26, [x27, x25, lsl #3]
   345e4:	mov	x0, x26
   345e8:	bl	d3f0 <__gmpn_invert_limb@plt>
   345ec:	ldr	x8, [x27, x22, lsl #3]
   345f0:	mul	x9, x0, x26
   345f4:	adds	x9, x9, x8
   345f8:	b.cc	34614 <__gmpn_div_q@@Base+0x3cc>  // b.lo, b.ul, b.last
   345fc:	subs	x9, x9, x26
   34600:	cset	w10, cs  // cs = hs, nlast
   34604:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   34608:	mvn	x10, x10
   3460c:	add	x0, x10, x0
   34610:	sub	x9, x9, x11
   34614:	ldur	x22, [x29, #-56]
   34618:	umulh	x10, x8, x0
   3461c:	adds	x9, x10, x9
   34620:	b.cc	3471c <__gmpn_div_q@@Base+0x4d4>  // b.lo, b.ul, b.last
   34624:	cmp	x9, x26
   34628:	sub	x5, x0, #0x1
   3462c:	b.cc	34720 <__gmpn_div_q@@Base+0x4d8>  // b.lo, b.ul, b.last
   34630:	mul	x10, x0, x8
   34634:	cmp	x9, x26
   34638:	sub	x11, x0, #0x2
   3463c:	ccmp	x10, x8, #0x2, ls  // ls = plast
   34640:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   34644:	b	34720 <__gmpn_div_q@@Base+0x4d8>
   34648:	ldr	x21, [x26, x25, lsl #3]
   3464c:	mov	x0, x21
   34650:	bl	d3f0 <__gmpn_invert_limb@plt>
   34654:	add	x8, x26, x20, lsl #3
   34658:	ldur	x8, [x8, #-16]
   3465c:	mul	x9, x0, x21
   34660:	adds	x9, x9, x8
   34664:	b.cc	34680 <__gmpn_div_q@@Base+0x438>  // b.lo, b.ul, b.last
   34668:	subs	x9, x9, x21
   3466c:	cset	w10, cs  // cs = hs, nlast
   34670:	csel	x11, x21, xzr, cs  // cs = hs, nlast
   34674:	mvn	x10, x10
   34678:	add	x0, x10, x0
   3467c:	sub	x9, x9, x11
   34680:	ldur	x25, [x29, #-24]
   34684:	umulh	x10, x8, x0
   34688:	adds	x9, x10, x9
   3468c:	b.cc	3474c <__gmpn_div_q@@Base+0x504>  // b.lo, b.ul, b.last
   34690:	cmp	x9, x21
   34694:	sub	x5, x0, #0x1
   34698:	b.cc	34750 <__gmpn_div_q@@Base+0x508>  // b.lo, b.ul, b.last
   3469c:	mul	x10, x0, x8
   346a0:	cmp	x9, x21
   346a4:	sub	x11, x0, #0x2
   346a8:	ccmp	x10, x8, #0x2, ls  // ls = plast
   346ac:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   346b0:	b	34750 <__gmpn_div_q@@Base+0x508>
   346b4:	ldr	x26, [x27, x25, lsl #3]
   346b8:	mov	x0, x26
   346bc:	bl	d3f0 <__gmpn_invert_limb@plt>
   346c0:	ldr	x8, [x27, x22, lsl #3]
   346c4:	mul	x9, x0, x26
   346c8:	adds	x9, x9, x8
   346cc:	b.cc	346e8 <__gmpn_div_q@@Base+0x4a0>  // b.lo, b.ul, b.last
   346d0:	subs	x9, x9, x26
   346d4:	cset	w10, cs  // cs = hs, nlast
   346d8:	csel	x11, x26, xzr, cs  // cs = hs, nlast
   346dc:	mvn	x10, x10
   346e0:	add	x0, x10, x0
   346e4:	sub	x9, x9, x11
   346e8:	ldur	x22, [x29, #-56]
   346ec:	umulh	x10, x8, x0
   346f0:	adds	x10, x10, x9
   346f4:	b.cc	347a0 <__gmpn_div_q@@Base+0x558>  // b.lo, b.ul, b.last
   346f8:	cmp	x10, x26
   346fc:	sub	x9, x0, #0x1
   34700:	b.cc	347a4 <__gmpn_div_q@@Base+0x55c>  // b.lo, b.ul, b.last
   34704:	mul	x11, x0, x8
   34708:	cmp	x10, x26
   3470c:	sub	x12, x0, #0x2
   34710:	ccmp	x11, x8, #0x2, ls  // ls = plast
   34714:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   34718:	b	347a4 <__gmpn_div_q@@Base+0x55c>
   3471c:	mov	x5, x0
   34720:	ldur	x26, [x29, #-40]
   34724:	mov	x0, x22
   34728:	mov	x1, x24
   3472c:	mov	x2, x23
   34730:	mov	x3, x27
   34734:	mov	x4, x26
   34738:	stur	x5, [x29, #-8]
   3473c:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   34740:	ldur	x8, [x29, #-64]
   34744:	cbnz	x8, 347d0 <__gmpn_div_q@@Base+0x588>
   34748:	b	34c30 <__gmpn_div_q@@Base+0x9e8>
   3474c:	mov	x5, x0
   34750:	mov	x0, x19
   34754:	mov	x1, x23
   34758:	mov	x2, x28
   3475c:	mov	x3, x26
   34760:	mov	x4, x20
   34764:	stur	x5, [x29, #-8]
   34768:	bl	cee0 <__gmpn_sbpi1_div_q@plt>
   3476c:	cbnz	x25, 34cbc <__gmpn_div_q@@Base+0xa74>
   34770:	b	34854 <__gmpn_div_q@@Base+0x60c>
   34774:	mov	x9, x0
   34778:	sub	x5, x29, #0x8
   3477c:	mov	x0, x19
   34780:	mov	x1, x23
   34784:	mov	x2, x28
   34788:	mov	x3, x26
   3478c:	mov	x4, x20
   34790:	stur	x9, [x29, #-8]
   34794:	bl	caa0 <__gmpn_dcpi1_div_q@plt>
   34798:	cbnz	x25, 34cbc <__gmpn_div_q@@Base+0xa74>
   3479c:	b	34854 <__gmpn_div_q@@Base+0x60c>
   347a0:	mov	x9, x0
   347a4:	ldur	x26, [x29, #-40]
   347a8:	sub	x5, x29, #0x8
   347ac:	mov	x0, x22
   347b0:	mov	x1, x24
   347b4:	mov	x2, x23
   347b8:	mov	x3, x27
   347bc:	mov	x4, x26
   347c0:	stur	x9, [x29, #-8]
   347c4:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   347c8:	ldur	x8, [x29, #-64]
   347cc:	cbz	x8, 34c30 <__gmpn_div_q@@Base+0x9e8>
   347d0:	cbz	x0, 34c34 <__gmpn_div_q@@Base+0x9ec>
   347d4:	cmp	x23, x26
   347d8:	b.le	34c34 <__gmpn_div_q@@Base+0x9ec>
   347dc:	ldur	x8, [x29, #-88]
   347e0:	mov	w1, #0xff                  	// #255
   347e4:	mov	x0, x22
   347e8:	add	x8, x8, x21
   347ec:	sub	x8, x8, x20
   347f0:	lsl	x8, x8, #3
   347f4:	add	x2, x8, #0x8
   347f8:	bl	c5f0 <memset@plt>
   347fc:	b	34c34 <__gmpn_div_q@@Base+0x9ec>
   34800:	mov	x0, x28
   34804:	mov	x1, x20
   34808:	mov	w2, wzr
   3480c:	bl	cfd0 <__gmpn_mu_div_q_itch@plt>
   34810:	lsl	x1, x0, #3
   34814:	mov	w8, #0x7f00                	// #32512
   34818:	cmp	x1, x8
   3481c:	b.hi	34d88 <__gmpn_div_q@@Base+0xb40>  // b.pmore
   34820:	add	x9, x1, #0xf
   34824:	mov	x8, sp
   34828:	and	x9, x9, #0xfffffffffffffff0
   3482c:	sub	x5, x8, x9
   34830:	mov	sp, x5
   34834:	ldur	x25, [x29, #-24]
   34838:	mov	x0, x19
   3483c:	mov	x1, x23
   34840:	mov	x2, x28
   34844:	mov	x3, x26
   34848:	mov	x4, x20
   3484c:	bl	c2e0 <__gmpn_mu_div_q@plt>
   34850:	cbnz	x25, 34cbc <__gmpn_div_q@@Base+0xa74>
   34854:	str	x0, [x19, x22, lsl #3]
   34858:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   3485c:	sub	x0, x29, #0x10
   34860:	mov	x1, x26
   34864:	mov	x24, x2
   34868:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3486c:	mov	x2, x24
   34870:	stur	x0, [x29, #-56]
   34874:	b	342f4 <__gmpn_div_q@@Base+0xac>
   34878:	cmp	x23, x2
   3487c:	b.eq	34898 <__gmpn_div_q@@Base+0x650>  // b.none
   34880:	mov	x0, x23
   34884:	mov	x1, x2
   34888:	mov	x25, x2
   3488c:	mov	x2, x21
   34890:	bl	ca50 <__gmpn_copyi@plt>
   34894:	mov	x2, x25
   34898:	cmp	x20, #0x2
   3489c:	b.ne	348d0 <__gmpn_div_q@@Base+0x688>  // b.any
   348a0:	mov	x0, x19
   348a4:	mov	x1, xzr
   348a8:	mov	x2, x23
   348ac:	mov	x3, x21
   348b0:	mov	x4, x24
   348b4:	bl	c200 <__gmpn_divrem_2@plt>
   348b8:	str	x0, [x19, x22, lsl #3]
   348bc:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   348c0:	sub	x0, x29, #0x10
   348c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   348c8:	mov	x26, x0
   348cc:	b	343ec <__gmpn_div_q@@Base+0x1a4>
   348d0:	cmp	x20, #0x98
   348d4:	b.lt	34ab0 <__gmpn_div_q@@Base+0x868>  // b.tstop
   348d8:	cmp	x22, #0x97
   348dc:	b.le	34ab0 <__gmpn_div_q@@Base+0x868>
   348e0:	cmp	x21, #0x7cc
   348e4:	b.lt	34920 <__gmpn_div_q@@Base+0x6d8>  // b.tstop
   348e8:	mov	x8, #0x200000000000        	// #35184372088832
   348ec:	mov	x9, #0x800000000000        	// #140737488355328
   348f0:	movk	x8, #0x409c, lsl #48
   348f4:	movk	x9, #0x4058, lsl #48
   348f8:	scvtf	d0, x20
   348fc:	scvtf	d1, x21
   34900:	fmov	d2, x8
   34904:	fmov	d3, x9
   34908:	fmul	d2, d0, d2
   3490c:	fmul	d3, d1, d3
   34910:	fadd	d2, d3, d2
   34914:	fmul	d0, d1, d0
   34918:	fcmp	d2, d0
   3491c:	b.le	34d30 <__gmpn_div_q@@Base+0xae8>
   34920:	mov	x0, x28
   34924:	bl	d3f0 <__gmpn_invert_limb@plt>
   34928:	add	x8, x24, x20, lsl #3
   3492c:	ldur	x8, [x8, #-16]
   34930:	mul	x9, x0, x28
   34934:	adds	x9, x9, x8
   34938:	b.cc	34954 <__gmpn_div_q@@Base+0x70c>  // b.lo, b.ul, b.last
   3493c:	subs	x9, x9, x28
   34940:	cset	w10, cs  // cs = hs, nlast
   34944:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   34948:	mvn	x10, x10
   3494c:	add	x0, x10, x0
   34950:	sub	x9, x9, x11
   34954:	umulh	x10, x8, x0
   34958:	adds	x10, x10, x9
   3495c:	b.cc	34bd8 <__gmpn_div_q@@Base+0x990>  // b.lo, b.ul, b.last
   34960:	cmp	x10, x28
   34964:	sub	x9, x0, #0x1
   34968:	b.cc	34bdc <__gmpn_div_q@@Base+0x994>  // b.lo, b.ul, b.last
   3496c:	mul	x11, x0, x8
   34970:	cmp	x10, x28
   34974:	sub	x12, x0, #0x2
   34978:	ccmp	x11, x8, #0x2, ls  // ls = plast
   3497c:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   34980:	b	34bdc <__gmpn_div_q@@Base+0x994>
   34984:	sub	x0, x29, #0x10
   34988:	mov	x25, x2
   3498c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34990:	mov	x2, x25
   34994:	stur	x0, [x29, #-24]
   34998:	stp	x2, x22, [x29, #-80]
   3499c:	tbz	x28, #63, 34340 <__gmpn_div_q@@Base+0xf8>
   349a0:	ldur	x0, [x29, #-24]
   349a4:	add	x8, x2, x21, lsl #3
   349a8:	sub	x1, x8, x27, lsl #3
   349ac:	mov	x2, x27
   349b0:	bl	ca50 <__gmpn_copyi@plt>
   349b4:	add	x8, x22, x20, lsl #3
   349b8:	ldur	x22, [x29, #-48]
   349bc:	mov	x9, #0xfffffffffffffffe    	// #-2
   349c0:	sub	x9, x9, x22
   349c4:	add	x25, x8, x9, lsl #3
   349c8:	cbz	x22, 34a30 <__gmpn_div_q@@Base+0x7e8>
   349cc:	cmp	x22, #0x95
   349d0:	b.le	34a50 <__gmpn_div_q@@Base+0x808>
   349d4:	cmp	x22, #0x3e3
   349d8:	b.le	34b14 <__gmpn_div_q@@Base+0x8cc>
   349dc:	ldur	x1, [x29, #-40]
   349e0:	mov	x0, x27
   349e4:	mov	w2, wzr
   349e8:	bl	c0e0 <__gmpn_mu_divappr_q_itch@plt>
   349ec:	lsl	x1, x0, #3
   349f0:	mov	w8, #0x7f00                	// #32512
   349f4:	cmp	x1, x8
   349f8:	b.hi	34d98 <__gmpn_div_q@@Base+0xb50>  // b.pmore
   349fc:	add	x9, x1, #0xf
   34a00:	mov	x8, sp
   34a04:	and	x9, x9, #0xfffffffffffffff0
   34a08:	sub	x5, x8, x9
   34a0c:	mov	sp, x5
   34a10:	ldur	x22, [x29, #-56]
   34a14:	ldur	x1, [x29, #-24]
   34a18:	ldur	x4, [x29, #-40]
   34a1c:	mov	x2, x27
   34a20:	mov	x0, x22
   34a24:	mov	x3, x25
   34a28:	bl	c710 <__gmpn_mu_divappr_q@plt>
   34a2c:	b	34c2c <__gmpn_div_q@@Base+0x9e4>
   34a30:	ldur	x22, [x29, #-56]
   34a34:	ldur	x2, [x29, #-24]
   34a38:	mov	x1, xzr
   34a3c:	mov	x3, x27
   34a40:	mov	x0, x22
   34a44:	mov	x4, x25
   34a48:	bl	c200 <__gmpn_divrem_2@plt>
   34a4c:	b	34c2c <__gmpn_div_q@@Base+0x9e4>
   34a50:	mov	x0, x28
   34a54:	bl	d3f0 <__gmpn_invert_limb@plt>
   34a58:	ldr	x8, [x25, x22, lsl #3]
   34a5c:	mul	x9, x0, x28
   34a60:	adds	x9, x9, x8
   34a64:	b.cc	34a80 <__gmpn_div_q@@Base+0x838>  // b.lo, b.ul, b.last
   34a68:	subs	x9, x9, x28
   34a6c:	cset	w10, cs  // cs = hs, nlast
   34a70:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   34a74:	mvn	x10, x10
   34a78:	add	x0, x10, x0
   34a7c:	sub	x9, x9, x11
   34a80:	umulh	x10, x8, x0
   34a84:	adds	x9, x10, x9
   34a88:	b.cc	34b78 <__gmpn_div_q@@Base+0x930>  // b.lo, b.ul, b.last
   34a8c:	cmp	x9, x28
   34a90:	sub	x5, x0, #0x1
   34a94:	b.cc	34b7c <__gmpn_div_q@@Base+0x934>  // b.lo, b.ul, b.last
   34a98:	mul	x10, x0, x8
   34a9c:	cmp	x9, x28
   34aa0:	sub	x11, x0, #0x2
   34aa4:	ccmp	x10, x8, #0x2, ls  // ls = plast
   34aa8:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   34aac:	b	34b7c <__gmpn_div_q@@Base+0x934>
   34ab0:	mov	x0, x28
   34ab4:	bl	d3f0 <__gmpn_invert_limb@plt>
   34ab8:	add	x8, x24, x20, lsl #3
   34abc:	ldur	x8, [x8, #-16]
   34ac0:	mul	x9, x0, x28
   34ac4:	adds	x9, x9, x8
   34ac8:	b.cc	34ae4 <__gmpn_div_q@@Base+0x89c>  // b.lo, b.ul, b.last
   34acc:	subs	x9, x9, x28
   34ad0:	cset	w10, cs  // cs = hs, nlast
   34ad4:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   34ad8:	mvn	x10, x10
   34adc:	add	x0, x10, x0
   34ae0:	sub	x9, x9, x11
   34ae4:	umulh	x10, x8, x0
   34ae8:	adds	x9, x10, x9
   34aec:	b.cc	34bb0 <__gmpn_div_q@@Base+0x968>  // b.lo, b.ul, b.last
   34af0:	cmp	x9, x28
   34af4:	sub	x5, x0, #0x1
   34af8:	b.cc	34bb4 <__gmpn_div_q@@Base+0x96c>  // b.lo, b.ul, b.last
   34afc:	mul	x10, x0, x8
   34b00:	cmp	x9, x28
   34b04:	sub	x11, x0, #0x2
   34b08:	ccmp	x10, x8, #0x2, ls  // ls = plast
   34b0c:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   34b10:	b	34bb4 <__gmpn_div_q@@Base+0x96c>
   34b14:	mov	x0, x28
   34b18:	bl	d3f0 <__gmpn_invert_limb@plt>
   34b1c:	ldur	x8, [x29, #-48]
   34b20:	mul	x9, x0, x28
   34b24:	ldr	x8, [x25, x8, lsl #3]
   34b28:	adds	x9, x9, x8
   34b2c:	b.cc	34b48 <__gmpn_div_q@@Base+0x900>  // b.lo, b.ul, b.last
   34b30:	subs	x9, x9, x28
   34b34:	cset	w10, cs  // cs = hs, nlast
   34b38:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   34b3c:	mvn	x10, x10
   34b40:	add	x0, x10, x0
   34b44:	sub	x9, x9, x11
   34b48:	umulh	x10, x8, x0
   34b4c:	adds	x10, x10, x9
   34b50:	b.cc	34c04 <__gmpn_div_q@@Base+0x9bc>  // b.lo, b.ul, b.last
   34b54:	cmp	x10, x28
   34b58:	sub	x9, x0, #0x1
   34b5c:	b.cc	34c08 <__gmpn_div_q@@Base+0x9c0>  // b.lo, b.ul, b.last
   34b60:	mul	x11, x0, x8
   34b64:	cmp	x10, x28
   34b68:	sub	x12, x0, #0x2
   34b6c:	ccmp	x11, x8, #0x2, ls  // ls = plast
   34b70:	csel	x9, x9, x12, cc  // cc = lo, ul, last
   34b74:	b	34c08 <__gmpn_div_q@@Base+0x9c0>
   34b78:	mov	x5, x0
   34b7c:	ldur	x22, [x29, #-56]
   34b80:	ldur	x1, [x29, #-24]
   34b84:	ldur	x4, [x29, #-40]
   34b88:	mov	x2, x27
   34b8c:	mov	x0, x22
   34b90:	mov	x3, x25
   34b94:	stur	x5, [x29, #-8]
   34b98:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   34b9c:	b	34c2c <__gmpn_div_q@@Base+0x9e4>
   34ba0:	sub	x0, x29, #0x10
   34ba4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34ba8:	mov	x5, x0
   34bac:	b	344d0 <__gmpn_div_q@@Base+0x288>
   34bb0:	mov	x5, x0
   34bb4:	mov	x0, x19
   34bb8:	mov	x1, x23
   34bbc:	mov	x2, x21
   34bc0:	mov	x3, x24
   34bc4:	mov	x4, x20
   34bc8:	stur	x5, [x29, #-8]
   34bcc:	bl	cee0 <__gmpn_sbpi1_div_q@plt>
   34bd0:	str	x0, [x19, x22, lsl #3]
   34bd4:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   34bd8:	mov	x9, x0
   34bdc:	sub	x5, x29, #0x8
   34be0:	mov	x0, x19
   34be4:	mov	x1, x23
   34be8:	mov	x2, x21
   34bec:	mov	x3, x24
   34bf0:	mov	x4, x20
   34bf4:	stur	x9, [x29, #-8]
   34bf8:	bl	caa0 <__gmpn_dcpi1_div_q@plt>
   34bfc:	str	x0, [x19, x22, lsl #3]
   34c00:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   34c04:	mov	x9, x0
   34c08:	ldur	x22, [x29, #-56]
   34c0c:	ldur	x1, [x29, #-24]
   34c10:	ldur	x4, [x29, #-40]
   34c14:	sub	x5, x29, #0x8
   34c18:	mov	x0, x22
   34c1c:	mov	x2, x27
   34c20:	mov	x3, x25
   34c24:	stur	x9, [x29, #-8]
   34c28:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   34c2c:	ldur	x25, [x29, #-32]
   34c30:	str	x0, [x22, x25, lsl #3]
   34c34:	add	x23, x22, #0x8
   34c38:	mov	x0, x19
   34c3c:	mov	x1, x23
   34c40:	mov	x2, x25
   34c44:	bl	ca50 <__gmpn_copyi@plt>
   34c48:	ldr	x8, [x22]
   34c4c:	cmp	x8, #0x4
   34c50:	b.hi	34cbc <__gmpn_div_q@@Base+0xa74>  // b.pmore
   34c54:	add	x22, x21, #0x1
   34c58:	lsl	x1, x22, #3
   34c5c:	mov	w8, #0x7f00                	// #32512
   34c60:	cmp	x1, x8
   34c64:	b.hi	34d20 <__gmpn_div_q@@Base+0xad8>  // b.pmore
   34c68:	add	x9, x1, #0xf
   34c6c:	mov	x8, sp
   34c70:	and	x9, x9, #0xfffffffffffffff0
   34c74:	sub	x25, x8, x9
   34c78:	mov	sp, x25
   34c7c:	ldur	x1, [x29, #-72]
   34c80:	ldur	x4, [x29, #-32]
   34c84:	mov	x0, x25
   34c88:	mov	x2, x20
   34c8c:	mov	x3, x23
   34c90:	bl	ccd0 <__gmpn_mul@plt>
   34c94:	ldr	x8, [x25, x21, lsl #3]
   34c98:	cmp	x8, #0x0
   34c9c:	cset	w8, eq  // eq = none
   34ca0:	sub	x8, x22, x8
   34ca4:	cmp	x8, x21
   34ca8:	b.le	34ce4 <__gmpn_div_q@@Base+0xa9c>
   34cac:	ldr	x8, [x19]
   34cb0:	sub	x9, x8, #0x1
   34cb4:	str	x9, [x19], #8
   34cb8:	cbz	x8, 34cac <__gmpn_div_q@@Base+0xa64>
   34cbc:	ldur	x0, [x29, #-16]
   34cc0:	cbnz	x0, 34d18 <__gmpn_div_q@@Base+0xad0>
   34cc4:	mov	sp, x29
   34cc8:	ldp	x20, x19, [sp, #80]
   34ccc:	ldp	x22, x21, [sp, #64]
   34cd0:	ldp	x24, x23, [sp, #48]
   34cd4:	ldp	x26, x25, [sp, #32]
   34cd8:	ldp	x28, x27, [sp, #16]
   34cdc:	ldp	x29, x30, [sp], #96
   34ce0:	ret
   34ce4:	ldur	x9, [x29, #-80]
   34ce8:	sub	x8, x25, #0x8
   34cec:	sub	x9, x9, #0x8
   34cf0:	subs	x10, x21, #0x1
   34cf4:	b.lt	34cbc <__gmpn_div_q@@Base+0xa74>  // b.tstop
   34cf8:	lsl	x11, x21, #3
   34cfc:	ldr	x12, [x9, x11]
   34d00:	ldr	x11, [x8, x11]
   34d04:	mov	x21, x10
   34d08:	cmp	x12, x11
   34d0c:	b.eq	34cf0 <__gmpn_div_q@@Base+0xaa8>  // b.none
   34d10:	b.ls	34cac <__gmpn_div_q@@Base+0xa64>  // b.plast
   34d14:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   34d18:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   34d1c:	b	34cc4 <__gmpn_div_q@@Base+0xa7c>
   34d20:	sub	x0, x29, #0x10
   34d24:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34d28:	mov	x25, x0
   34d2c:	b	34c7c <__gmpn_div_q@@Base+0xa34>
   34d30:	mov	x25, x2
   34d34:	mov	x0, x21
   34d38:	mov	x1, x20
   34d3c:	mov	w2, wzr
   34d40:	bl	cfd0 <__gmpn_mu_div_q_itch@plt>
   34d44:	lsl	x1, x0, #3
   34d48:	mov	w8, #0x7f00                	// #32512
   34d4c:	cmp	x1, x8
   34d50:	b.hi	34da8 <__gmpn_div_q@@Base+0xb60>  // b.pmore
   34d54:	add	x9, x1, #0xf
   34d58:	mov	x8, sp
   34d5c:	and	x9, x9, #0xfffffffffffffff0
   34d60:	sub	x5, x8, x9
   34d64:	mov	sp, x5
   34d68:	mov	x0, x19
   34d6c:	mov	x1, x25
   34d70:	mov	x2, x21
   34d74:	mov	x3, x24
   34d78:	mov	x4, x20
   34d7c:	bl	c2e0 <__gmpn_mu_div_q@plt>
   34d80:	str	x0, [x19, x22, lsl #3]
   34d84:	b	34cbc <__gmpn_div_q@@Base+0xa74>
   34d88:	sub	x0, x29, #0x10
   34d8c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34d90:	mov	x5, x0
   34d94:	b	34834 <__gmpn_div_q@@Base+0x5ec>
   34d98:	sub	x0, x29, #0x10
   34d9c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34da0:	mov	x5, x0
   34da4:	b	34a10 <__gmpn_div_q@@Base+0x7c8>
   34da8:	sub	x0, x29, #0x10
   34dac:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   34db0:	mov	x5, x0
   34db4:	b	34d68 <__gmpn_div_q@@Base+0xb20>

0000000000034db8 <__gmpn_tdiv_qr@@Base>:
   34db8:	stp	x29, x30, [sp, #-96]!
   34dbc:	stp	x28, x27, [sp, #16]
   34dc0:	stp	x26, x25, [sp, #32]
   34dc4:	stp	x24, x23, [sp, #48]
   34dc8:	stp	x22, x21, [sp, #64]
   34dcc:	stp	x20, x19, [sp, #80]
   34dd0:	mov	x29, sp
   34dd4:	sub	sp, sp, #0x50
   34dd8:	cbnz	x2, 35900 <__gmpn_tdiv_qr@@Base+0xb48>
   34ddc:	mov	x21, x6
   34de0:	mov	x22, x5
   34de4:	mov	x25, x4
   34de8:	mov	x26, x3
   34dec:	mov	x28, x1
   34df0:	mov	x20, x0
   34df4:	subs	x19, x6, #0x1
   34df8:	b.eq	34fd8 <__gmpn_tdiv_qr@@Base+0x220>  // b.none
   34dfc:	cmp	x21, #0x2
   34e00:	b.eq	34f1c <__gmpn_tdiv_qr@@Base+0x164>  // b.none
   34e04:	cbz	x21, 35918 <__gmpn_tdiv_qr@@Base+0xb60>
   34e08:	stur	xzr, [x29, #-8]
   34e0c:	add	x8, x26, x25, lsl #3
   34e10:	ldur	x27, [x8, #-8]
   34e14:	ldr	x23, [x22, x19, lsl #3]
   34e18:	sub	x9, x25, x21
   34e1c:	str	xzr, [x20, x9, lsl #3]
   34e20:	cmp	x27, x23
   34e24:	cinc	x24, x25, cs  // cs = hs, nlast
   34e28:	cset	w8, cs  // cs = hs, nlast
   34e2c:	cmp	x24, x21, lsl #1
   34e30:	b.ge	34ff8 <__gmpn_tdiv_qr@@Base+0x240>  // b.tcont
   34e34:	adds	x24, x9, x8
   34e38:	b.eq	350ec <__gmpn_tdiv_qr@@Base+0x334>  // b.none
   34e3c:	ldr	x8, [x22, x19, lsl #3]
   34e40:	sub	x19, x21, x24
   34e44:	lsl	x10, x24, #3
   34e48:	stp	x22, x28, [x29, #-40]
   34e4c:	stp	x10, x26, [x29, #-56]
   34e50:	tbnz	x8, #63, 35260 <__gmpn_tdiv_qr@@Base+0x4a8>
   34e54:	mov	w9, #0x7f00                	// #32512
   34e58:	clz	x8, x8
   34e5c:	cmp	x10, x9
   34e60:	stp	x19, x8, [x29, #-80]
   34e64:	mov	x19, x26
   34e68:	b.hi	3589c <__gmpn_tdiv_qr@@Base+0xae4>  // b.pmore
   34e6c:	add	x9, x10, #0xf
   34e70:	mov	x8, sp
   34e74:	and	x9, x9, #0xfffffffffffffff0
   34e78:	sub	x28, x8, x9
   34e7c:	mov	sp, x28
   34e80:	ldur	x8, [x29, #-80]
   34e84:	mov	x0, x28
   34e88:	mov	x2, x24
   34e8c:	add	x26, x22, x8, lsl #3
   34e90:	ldur	x22, [x29, #-72]
   34e94:	mov	x1, x26
   34e98:	mov	w3, w22
   34e9c:	bl	c180 <__gmpn_lshift@plt>
   34ea0:	ldur	x8, [x26, #-8]
   34ea4:	ldr	x10, [x28]
   34ea8:	neg	x9, x22
   34eac:	mov	w1, #0x8                   	// #8
   34eb0:	lsr	x8, x8, x9
   34eb4:	mov	w11, #0x7f00                	// #32512
   34eb8:	bfi	x1, x24, #4, #60
   34ebc:	orr	x8, x10, x8
   34ec0:	cmp	x1, x11
   34ec4:	stur	x28, [x29, #-64]
   34ec8:	str	x8, [x28]
   34ecc:	lsl	x28, x24, #1
   34ed0:	b.hi	358b0 <__gmpn_tdiv_qr@@Base+0xaf8>  // b.pmore
   34ed4:	add	x9, x1, #0xf
   34ed8:	mov	x8, sp
   34edc:	and	x9, x9, #0xfffffffffffffff0
   34ee0:	sub	x26, x8, x9
   34ee4:	mov	sp, x26
   34ee8:	ldur	x22, [x29, #-72]
   34eec:	add	x8, x19, x25, lsl #3
   34ef0:	sub	x1, x8, x28, lsl #3
   34ef4:	mov	x0, x26
   34ef8:	mov	x2, x28
   34efc:	mov	w3, w22
   34f00:	bl	c180 <__gmpn_lshift@plt>
   34f04:	cmp	x27, x23
   34f08:	b.cc	352c4 <__gmpn_tdiv_qr@@Base+0x50c>  // b.lo, b.ul, b.last
   34f0c:	ldur	x19, [x29, #-80]
   34f10:	str	x0, [x26, x28, lsl #3]
   34f14:	add	x26, x26, #0x8
   34f18:	b	3534c <__gmpn_tdiv_qr@@Base+0x594>
   34f1c:	stur	xzr, [x29, #-8]
   34f20:	ldr	x8, [x22, #8]
   34f24:	tbnz	x8, #63, 3508c <__gmpn_tdiv_qr@@Base+0x2d4>
   34f28:	ldr	x9, [x22]
   34f2c:	clz	x21, x8
   34f30:	lsl	x10, x25, #3
   34f34:	neg	x12, x21
   34f38:	mov	w11, #0x7f00                	// #32512
   34f3c:	lsl	x8, x8, x21
   34f40:	add	x1, x10, #0x8
   34f44:	lsr	x10, x9, x12
   34f48:	mov	w19, #0x40                  	// #64
   34f4c:	cmp	x1, x11
   34f50:	lsl	x9, x9, x21
   34f54:	orr	x8, x10, x8
   34f58:	stp	x9, x8, [x29, #-24]
   34f5c:	b.hi	35710 <__gmpn_tdiv_qr@@Base+0x958>  // b.pmore
   34f60:	add	x9, x1, #0xf
   34f64:	mov	x8, sp
   34f68:	and	x9, x9, #0xfffffffffffffff0
   34f6c:	sub	x22, x8, x9
   34f70:	mov	sp, x22
   34f74:	mov	x0, x22
   34f78:	mov	x1, x26
   34f7c:	mov	x2, x25
   34f80:	mov	w3, w21
   34f84:	sub	x19, x19, x21
   34f88:	bl	c180 <__gmpn_lshift@plt>
   34f8c:	cmp	x0, #0x0
   34f90:	mov	x23, x0
   34f94:	str	x0, [x22, x25, lsl #3]
   34f98:	cinc	x3, x25, ne  // ne = any
   34f9c:	sub	x4, x29, #0x18
   34fa0:	mov	x0, x20
   34fa4:	mov	x1, xzr
   34fa8:	mov	x2, x22
   34fac:	bl	c200 <__gmpn_divrem_2@plt>
   34fb0:	cbnz	x23, 34fbc <__gmpn_tdiv_qr@@Base+0x204>
   34fb4:	add	x8, x20, x25, lsl #3
   34fb8:	stur	x0, [x8, #-16]
   34fbc:	ldp	x8, x9, [x22]
   34fc0:	lsr	x8, x8, x21
   34fc4:	lsl	x10, x9, x19
   34fc8:	lsr	x9, x9, x21
   34fcc:	orr	x8, x10, x8
   34fd0:	stp	x8, x9, [x28]
   34fd4:	b	3586c <__gmpn_tdiv_qr@@Base+0xab4>
   34fd8:	ldr	x4, [x22]
   34fdc:	mov	x0, x20
   34fe0:	mov	x1, xzr
   34fe4:	mov	x2, x26
   34fe8:	mov	x3, x25
   34fec:	bl	cd00 <__gmpn_divrem_1@plt>
   34ff0:	str	x0, [x28]
   34ff4:	b	35874 <__gmpn_tdiv_qr@@Base+0xabc>
   34ff8:	ldr	x8, [x22, x19, lsl #3]
   34ffc:	tbnz	x8, #63, 35100 <__gmpn_tdiv_qr@@Base+0x348>
   35000:	lsl	x1, x21, #3
   35004:	mov	w9, #0x7f00                	// #32512
   35008:	mov	x23, x26
   3500c:	cmp	x1, x9
   35010:	clz	x26, x8
   35014:	stur	x28, [x29, #-32]
   35018:	b.hi	35730 <__gmpn_tdiv_qr@@Base+0x978>  // b.pmore
   3501c:	add	x9, x1, #0xf
   35020:	mov	x8, sp
   35024:	and	x9, x9, #0xfffffffffffffff0
   35028:	sub	x28, x8, x9
   3502c:	mov	sp, x28
   35030:	mov	x0, x28
   35034:	mov	x1, x22
   35038:	mov	x2, x21
   3503c:	mov	w3, w26
   35040:	bl	c180 <__gmpn_lshift@plt>
   35044:	lsl	x8, x25, #3
   35048:	add	x1, x8, #0x8
   3504c:	mov	w8, #0x7f00                	// #32512
   35050:	cmp	x1, x8
   35054:	b.hi	35740 <__gmpn_tdiv_qr@@Base+0x988>  // b.pmore
   35058:	add	x9, x1, #0xf
   3505c:	mov	x8, sp
   35060:	and	x9, x9, #0xfffffffffffffff0
   35064:	sub	x27, x8, x9
   35068:	mov	sp, x27
   3506c:	mov	x0, x27
   35070:	mov	x1, x23
   35074:	mov	x2, x25
   35078:	mov	w3, w26
   3507c:	bl	c180 <__gmpn_lshift@plt>
   35080:	mov	x22, x28
   35084:	ldur	x28, [x29, #-32]
   35088:	b	35140 <__gmpn_tdiv_qr@@Base+0x388>
   3508c:	lsl	x1, x25, #3
   35090:	mov	w8, #0x7f00                	// #32512
   35094:	cmp	x1, x8
   35098:	b.hi	35720 <__gmpn_tdiv_qr@@Base+0x968>  // b.pmore
   3509c:	add	x9, x1, #0xf
   350a0:	mov	x8, sp
   350a4:	and	x9, x9, #0xfffffffffffffff0
   350a8:	sub	x21, x8, x9
   350ac:	mov	sp, x21
   350b0:	mov	x0, x21
   350b4:	mov	x1, x26
   350b8:	mov	x2, x25
   350bc:	bl	ca50 <__gmpn_copyi@plt>
   350c0:	mov	x0, x20
   350c4:	mov	x1, xzr
   350c8:	mov	x2, x21
   350cc:	mov	x3, x25
   350d0:	mov	x4, x22
   350d4:	bl	c200 <__gmpn_divrem_2@plt>
   350d8:	add	x8, x20, x25, lsl #3
   350dc:	stur	x0, [x8, #-16]
   350e0:	ldr	q0, [x21]
   350e4:	str	q0, [x28]
   350e8:	b	3586c <__gmpn_tdiv_qr@@Base+0xab4>
   350ec:	mov	x0, x28
   350f0:	mov	x1, x26
   350f4:	mov	x2, x21
   350f8:	bl	ca50 <__gmpn_copyi@plt>
   350fc:	b	35874 <__gmpn_tdiv_qr@@Base+0xabc>
   35100:	lsl	x8, x25, #3
   35104:	add	x1, x8, #0x8
   35108:	mov	w8, #0x7f00                	// #32512
   3510c:	cmp	x1, x8
   35110:	b.hi	358c0 <__gmpn_tdiv_qr@@Base+0xb08>  // b.pmore
   35114:	add	x9, x1, #0xf
   35118:	mov	x8, sp
   3511c:	and	x9, x9, #0xfffffffffffffff0
   35120:	sub	x27, x8, x9
   35124:	mov	sp, x27
   35128:	mov	x0, x27
   3512c:	mov	x1, x26
   35130:	mov	x2, x25
   35134:	bl	ca50 <__gmpn_copyi@plt>
   35138:	mov	x0, xzr
   3513c:	mov	w26, wzr
   35140:	str	x0, [x27, x25, lsl #3]
   35144:	ldr	x23, [x22, x19, lsl #3]
   35148:	mov	x0, x23
   3514c:	bl	d3f0 <__gmpn_invert_limb@plt>
   35150:	add	x8, x22, x21, lsl #3
   35154:	ldur	x8, [x8, #-16]
   35158:	mul	x9, x0, x23
   3515c:	adds	x9, x9, x8
   35160:	b.cc	3517c <__gmpn_tdiv_qr@@Base+0x3c4>  // b.lo, b.ul, b.last
   35164:	subs	x9, x9, x23
   35168:	cset	w10, cs  // cs = hs, nlast
   3516c:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   35170:	mvn	x10, x10
   35174:	add	x0, x10, x0
   35178:	sub	x9, x9, x11
   3517c:	umulh	x10, x8, x0
   35180:	adds	x9, x10, x9
   35184:	b.cc	351ac <__gmpn_tdiv_qr@@Base+0x3f4>  // b.lo, b.ul, b.last
   35188:	cmp	x9, x23
   3518c:	sub	x5, x0, #0x1
   35190:	b.cc	351b0 <__gmpn_tdiv_qr@@Base+0x3f8>  // b.lo, b.ul, b.last
   35194:	mul	x10, x0, x8
   35198:	cmp	x9, x23
   3519c:	sub	x11, x0, #0x2
   351a0:	ccmp	x10, x8, #0x2, ls  // ls = plast
   351a4:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   351a8:	b	351b0 <__gmpn_tdiv_qr@@Base+0x3f8>
   351ac:	mov	x5, x0
   351b0:	cmp	x21, #0x29
   351b4:	stur	x5, [x29, #-24]
   351b8:	b.le	35224 <__gmpn_tdiv_qr@@Base+0x46c>
   351bc:	cmp	x21, #0x62
   351c0:	b.lt	35204 <__gmpn_tdiv_qr@@Base+0x44c>  // b.tstop
   351c4:	cmp	x24, #0x7cc
   351c8:	b.lt	35204 <__gmpn_tdiv_qr@@Base+0x44c>  // b.tstop
   351cc:	mov	x8, #0x200000000000        	// #35184372088832
   351d0:	mov	x9, #0x800000000000        	// #140737488355328
   351d4:	movk	x8, #0x409c, lsl #48
   351d8:	movk	x9, #0x4058, lsl #48
   351dc:	scvtf	d0, x21
   351e0:	scvtf	d1, x24
   351e4:	fmov	d2, x8
   351e8:	fmov	d3, x9
   351ec:	fmul	d2, d0, d2
   351f0:	fmul	d3, d1, d3
   351f4:	fadd	d2, d2, d3
   351f8:	fmul	d0, d0, d1
   351fc:	fcmp	d2, d0
   35200:	b.le	352f0 <__gmpn_tdiv_qr@@Base+0x538>
   35204:	sub	x5, x29, #0x18
   35208:	mov	x0, x20
   3520c:	mov	x1, x27
   35210:	mov	x2, x24
   35214:	mov	x3, x22
   35218:	mov	x4, x21
   3521c:	bl	c3b0 <__gmpn_dcpi1_div_qr@plt>
   35220:	b	3523c <__gmpn_tdiv_qr@@Base+0x484>
   35224:	mov	x0, x20
   35228:	mov	x1, x27
   3522c:	mov	x2, x24
   35230:	mov	x3, x22
   35234:	mov	x4, x21
   35238:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   3523c:	mov	x0, x28
   35240:	mov	x1, x27
   35244:	mov	x2, x21
   35248:	cbz	w26, 35258 <__gmpn_tdiv_qr@@Base+0x4a0>
   3524c:	mov	w3, w26
   35250:	bl	c1a0 <__gmpn_rshift@plt>
   35254:	b	3586c <__gmpn_tdiv_qr@@Base+0xab4>
   35258:	bl	ca50 <__gmpn_copyi@plt>
   3525c:	b	3586c <__gmpn_tdiv_qr@@Base+0xab4>
   35260:	add	x8, x22, x19, lsl #3
   35264:	mov	w1, #0x8                   	// #8
   35268:	stur	x8, [x29, #-64]
   3526c:	bfi	x1, x24, #4, #60
   35270:	mov	w8, #0x7f00                	// #32512
   35274:	cmp	x1, x8
   35278:	lsl	x28, x24, #1
   3527c:	b.hi	358d0 <__gmpn_tdiv_qr@@Base+0xb18>  // b.pmore
   35280:	add	x9, x1, #0xf
   35284:	mov	x8, sp
   35288:	and	x9, x9, #0xfffffffffffffff0
   3528c:	sub	x26, x8, x9
   35290:	mov	sp, x26
   35294:	ldur	x8, [x29, #-48]
   35298:	mov	x0, x26
   3529c:	mov	x2, x28
   352a0:	add	x8, x8, x25, lsl #3
   352a4:	sub	x1, x8, x28, lsl #3
   352a8:	bl	ca50 <__gmpn_copyi@plt>
   352ac:	cmp	x27, x23
   352b0:	b.cc	35348 <__gmpn_tdiv_qr@@Base+0x590>  // b.lo, b.ul, b.last
   352b4:	mov	w22, wzr
   352b8:	str	xzr, [x26, x28, lsl #3]
   352bc:	add	x26, x26, #0x8
   352c0:	b	3534c <__gmpn_tdiv_qr@@Base+0x594>
   352c4:	mvn	x8, x28
   352c8:	add	x8, x8, x25
   352cc:	ldr	x8, [x19, x8, lsl #3]
   352d0:	ldr	x9, [x26]
   352d4:	mov	w10, #0x40                  	// #64
   352d8:	sub	x10, x10, x22
   352dc:	ldur	x19, [x29, #-80]
   352e0:	lsr	x8, x8, x10
   352e4:	orr	x8, x9, x8
   352e8:	str	x8, [x26]
   352ec:	b	3534c <__gmpn_tdiv_qr@@Base+0x594>
   352f0:	mov	x0, x24
   352f4:	mov	x1, x21
   352f8:	mov	w2, wzr
   352fc:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   35300:	lsl	x1, x0, #3
   35304:	mov	w8, #0x7f00                	// #32512
   35308:	cmp	x1, x8
   3530c:	b.hi	358e0 <__gmpn_tdiv_qr@@Base+0xb28>  // b.pmore
   35310:	add	x9, x1, #0xf
   35314:	mov	x8, sp
   35318:	and	x9, x9, #0xfffffffffffffff0
   3531c:	sub	x6, x8, x9
   35320:	mov	sp, x6
   35324:	mov	x0, x20
   35328:	mov	x1, x28
   3532c:	mov	x2, x27
   35330:	mov	x3, x24
   35334:	mov	x4, x22
   35338:	mov	x5, x21
   3533c:	bl	c960 <__gmpn_mu_div_qr@plt>
   35340:	mov	x27, x28
   35344:	b	3523c <__gmpn_tdiv_qr@@Base+0x484>
   35348:	mov	w22, wzr
   3534c:	ldur	x27, [x29, #-64]
   35350:	cmp	x24, #0x2
   35354:	b.eq	353ac <__gmpn_tdiv_qr@@Base+0x5f4>  // b.none
   35358:	cmp	x24, #0x1
   3535c:	b.ne	353c8 <__gmpn_tdiv_qr@@Base+0x610>  // b.any
   35360:	ldr	x9, [x27]
   35364:	ldp	x8, x10, [x26]
   35368:	lsr	x12, x9, #32
   3536c:	udiv	x15, x10, x12
   35370:	and	x11, x9, #0xffffffff
   35374:	msub	w10, w15, w12, w10
   35378:	mul	x13, x15, x11
   3537c:	extr	x14, x10, x8, #32
   35380:	cmp	x14, x13
   35384:	b.cs	35430 <__gmpn_tdiv_qr@@Base+0x678>  // b.hs, b.nlast
   35388:	add	x14, x14, x9
   3538c:	cmp	x14, x9
   35390:	sub	x10, x15, #0x1
   35394:	b.cc	35434 <__gmpn_tdiv_qr@@Base+0x67c>  // b.lo, b.ul, b.last
   35398:	cmp	x14, x13
   3539c:	b.cs	35434 <__gmpn_tdiv_qr@@Base+0x67c>  // b.hs, b.nlast
   353a0:	sub	x10, x15, #0x2
   353a4:	add	x14, x14, x9
   353a8:	b	35434 <__gmpn_tdiv_qr@@Base+0x67c>
   353ac:	mov	w3, #0x4                   	// #4
   353b0:	mov	x0, x20
   353b4:	mov	x1, xzr
   353b8:	mov	x2, x26
   353bc:	mov	x4, x27
   353c0:	bl	c200 <__gmpn_divrem_2@plt>
   353c4:	b	3557c <__gmpn_tdiv_qr@@Base+0x7c4>
   353c8:	add	x23, x27, x24, lsl #3
   353cc:	ldur	x28, [x23, #-8]
   353d0:	mov	x0, x28
   353d4:	bl	d3f0 <__gmpn_invert_limb@plt>
   353d8:	ldur	x8, [x23, #-16]
   353dc:	mul	x9, x0, x28
   353e0:	adds	x9, x9, x8
   353e4:	b.cc	35400 <__gmpn_tdiv_qr@@Base+0x648>  // b.lo, b.ul, b.last
   353e8:	subs	x9, x9, x28
   353ec:	cset	w10, cs  // cs = hs, nlast
   353f0:	csel	x11, x28, xzr, cs  // cs = hs, nlast
   353f4:	mvn	x10, x10
   353f8:	add	x0, x10, x0
   353fc:	sub	x9, x9, x11
   35400:	umulh	x10, x8, x0
   35404:	adds	x9, x10, x9
   35408:	b.cc	354ac <__gmpn_tdiv_qr@@Base+0x6f4>  // b.lo, b.ul, b.last
   3540c:	cmp	x9, x28
   35410:	sub	x5, x0, #0x1
   35414:	b.cc	354b0 <__gmpn_tdiv_qr@@Base+0x6f8>  // b.lo, b.ul, b.last
   35418:	mul	x10, x0, x8
   3541c:	cmp	x9, x28
   35420:	sub	x11, x0, #0x2
   35424:	ccmp	x10, x8, #0x2, ls  // ls = plast
   35428:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   3542c:	b	354b0 <__gmpn_tdiv_qr@@Base+0x6f8>
   35430:	mov	x10, x15
   35434:	sub	x14, x14, x13
   35438:	udiv	x13, x14, x12
   3543c:	msub	w12, w13, w12, w14
   35440:	mul	x11, x13, x11
   35444:	bfi	x8, x12, #32, #32
   35448:	cmp	x8, x11
   3544c:	b.cs	35478 <__gmpn_tdiv_qr@@Base+0x6c0>  // b.hs, b.nlast
   35450:	ldur	x14, [x29, #-56]
   35454:	add	x8, x8, x9
   35458:	cmp	x8, x9
   3545c:	sub	x12, x13, #0x1
   35460:	b.cc	35480 <__gmpn_tdiv_qr@@Base+0x6c8>  // b.lo, b.ul, b.last
   35464:	cmp	x8, x11
   35468:	b.cs	35480 <__gmpn_tdiv_qr@@Base+0x6c8>  // b.hs, b.nlast
   3546c:	sub	x12, x13, #0x2
   35470:	add	x8, x8, x9
   35474:	b	35480 <__gmpn_tdiv_qr@@Base+0x6c8>
   35478:	ldur	x14, [x29, #-56]
   3547c:	mov	x12, x13
   35480:	sub	x8, x8, x11
   35484:	orr	x9, x12, x10, lsl #32
   35488:	str	x8, [x26]
   3548c:	str	x9, [x20]
   35490:	cmp	x19, #0x2
   35494:	b.lt	35588 <__gmpn_tdiv_qr@@Base+0x7d0>  // b.tstop
   35498:	ldur	x10, [x29, #-40]
   3549c:	add	x8, x10, x19, lsl #3
   354a0:	ldur	x8, [x8, #-16]
   354a4:	lsr	x8, x8, #1
   354a8:	b	35590 <__gmpn_tdiv_qr@@Base+0x7d8>
   354ac:	mov	x5, x0
   354b0:	cmp	x24, #0x29
   354b4:	stur	x5, [x29, #-24]
   354b8:	b.le	35544 <__gmpn_tdiv_qr@@Base+0x78c>
   354bc:	cmp	x24, #0x3e5
   354c0:	lsl	x28, x24, #1
   354c4:	b.le	35560 <__gmpn_tdiv_qr@@Base+0x7a8>
   354c8:	mov	x0, x28
   354cc:	mov	x1, x24
   354d0:	mov	w2, wzr
   354d4:	bl	d010 <__gmpn_mu_div_qr_itch@plt>
   354d8:	lsl	x1, x0, #3
   354dc:	mov	w8, #0x7f00                	// #32512
   354e0:	cmp	x1, x8
   354e4:	b.hi	358f0 <__gmpn_tdiv_qr@@Base+0xb38>  // b.pmore
   354e8:	add	x9, x1, #0xf
   354ec:	mov	x8, sp
   354f0:	and	x9, x9, #0xfffffffffffffff0
   354f4:	sub	x6, x8, x9
   354f8:	mov	sp, x6
   354fc:	ldur	x9, [x29, #-32]
   35500:	ldur	x10, [x29, #-48]
   35504:	sub	x8, x25, x24
   35508:	mov	x0, x20
   3550c:	add	x8, x9, x8, lsl #3
   35510:	cmp	x10, x9
   35514:	csel	x25, x8, x9, eq  // eq = none
   35518:	mov	x1, x25
   3551c:	mov	x2, x26
   35520:	mov	x3, x28
   35524:	mov	x4, x27
   35528:	mov	x5, x24
   3552c:	bl	c960 <__gmpn_mu_div_qr@plt>
   35530:	mov	x0, x26
   35534:	mov	x1, x25
   35538:	mov	x2, x24
   3553c:	bl	ca50 <__gmpn_copyi@plt>
   35540:	b	3557c <__gmpn_tdiv_qr@@Base+0x7c4>
   35544:	lsl	x2, x24, #1
   35548:	mov	x0, x20
   3554c:	mov	x1, x26
   35550:	mov	x3, x27
   35554:	mov	x4, x24
   35558:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   3555c:	b	3557c <__gmpn_tdiv_qr@@Base+0x7c4>
   35560:	sub	x5, x29, #0x18
   35564:	mov	x0, x20
   35568:	mov	x1, x26
   3556c:	mov	x2, x28
   35570:	mov	x3, x27
   35574:	mov	x4, x24
   35578:	bl	c3b0 <__gmpn_dcpi1_div_qr@plt>
   3557c:	ldur	x14, [x29, #-56]
   35580:	cmp	x19, #0x2
   35584:	b.ge	35498 <__gmpn_tdiv_qr@@Base+0x6e0>  // b.tcont
   35588:	ldur	x10, [x29, #-40]
   3558c:	mov	x8, xzr
   35590:	sub	x25, x19, #0x1
   35594:	ldr	x10, [x10, x25, lsl #3]
   35598:	sub	x11, x14, #0x8
   3559c:	ldr	x12, [x20, x11]
   355a0:	ldr	x11, [x26, x11]
   355a4:	mvn	w9, w22
   355a8:	lsl	x10, x10, x22
   355ac:	lsr	x8, x8, x9
   355b0:	orr	x8, x10, x8
   355b4:	umulh	x8, x8, x12
   355b8:	cmp	x11, x8
   355bc:	mov	x28, x24
   355c0:	b.cs	355fc <__gmpn_tdiv_qr@@Base+0x844>  // b.hs, b.nlast
   355c4:	mov	x8, x20
   355c8:	ldr	x9, [x8]
   355cc:	sub	x10, x9, #0x1
   355d0:	str	x10, [x8], #8
   355d4:	cbz	x9, 355c8 <__gmpn_tdiv_qr@@Base+0x810>
   355d8:	mov	x0, x26
   355dc:	mov	x1, x26
   355e0:	mov	x2, x27
   355e4:	mov	x3, x24
   355e8:	bl	ca70 <__gmpn_add_n@plt>
   355ec:	mov	x28, x24
   355f0:	cbz	x0, 355fc <__gmpn_tdiv_qr@@Base+0x844>
   355f4:	add	x28, x24, #0x1
   355f8:	str	x0, [x26, x24, lsl #3]
   355fc:	cbz	w22, 35674 <__gmpn_tdiv_qr@@Base+0x8bc>
   35600:	mov	w8, #0x40                  	// #64
   35604:	sub	w3, w8, w22
   35608:	mov	x0, x26
   3560c:	mov	x1, x26
   35610:	mov	x2, x28
   35614:	bl	c180 <__gmpn_lshift@plt>
   35618:	ldur	x9, [x29, #-48]
   3561c:	lsl	x8, x25, #3
   35620:	ldr	x10, [x26]
   35624:	mov	x11, #0xffffffffffffffff    	// #-1
   35628:	ldr	x9, [x9, x8]
   3562c:	lsr	x11, x11, x22
   35630:	ldur	x22, [x29, #-40]
   35634:	mov	x27, x0
   35638:	and	x9, x9, x11
   3563c:	orr	x9, x10, x9
   35640:	str	x9, [x26]
   35644:	ldr	x8, [x22, x8]
   35648:	mov	x0, x26
   3564c:	mov	x1, x20
   35650:	mov	x2, x24
   35654:	and	x3, x8, x11
   35658:	bl	c9e0 <__gmpn_submul_1@plt>
   3565c:	cmp	x24, x28
   35660:	b.ne	35684 <__gmpn_tdiv_qr@@Base+0x8cc>  // b.any
   35664:	subs	x8, x27, x0
   35668:	cset	w23, cc  // cc = lo, ul, last
   3566c:	add	x28, x24, #0x1
   35670:	b	35694 <__gmpn_tdiv_qr@@Base+0x8dc>
   35674:	ldur	x22, [x29, #-40]
   35678:	mov	x23, xzr
   3567c:	mov	x25, x19
   35680:	b	35698 <__gmpn_tdiv_qr@@Base+0x8e0>
   35684:	ldr	x8, [x26, x24, lsl #3]
   35688:	subs	x8, x8, x0
   3568c:	b.cc	3591c <__gmpn_tdiv_qr@@Base+0xb64>  // b.lo, b.ul, b.last
   35690:	mov	x23, xzr
   35694:	str	x8, [x26, x24, lsl #3]
   35698:	lsl	x1, x21, #3
   3569c:	mov	w8, #0x7f00                	// #32512
   356a0:	cmp	x1, x8
   356a4:	b.hi	35750 <__gmpn_tdiv_qr@@Base+0x998>  // b.pmore
   356a8:	add	x9, x1, #0xf
   356ac:	mov	x8, sp
   356b0:	and	x9, x9, #0xfffffffffffffff0
   356b4:	sub	x27, x8, x9
   356b8:	mov	sp, x27
   356bc:	cmp	x25, x24
   356c0:	b.ge	35764 <__gmpn_tdiv_qr@@Base+0x9ac>  // b.tcont
   356c4:	cbz	x25, 356e0 <__gmpn_tdiv_qr@@Base+0x928>
   356c8:	mov	x0, x27
   356cc:	mov	x1, x20
   356d0:	mov	x2, x24
   356d4:	mov	x3, x22
   356d8:	mov	x4, x25
   356dc:	b	35778 <__gmpn_tdiv_qr@@Base+0x9c0>
   356e0:	ldur	x0, [x29, #-32]
   356e4:	mov	x1, x26
   356e8:	mov	x2, x28
   356ec:	bl	ca50 <__gmpn_copyi@plt>
   356f0:	cmp	x28, x21
   356f4:	b.eq	35844 <__gmpn_tdiv_qr@@Base+0xa8c>  // b.none
   356f8:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   356fc:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   35700:	add	x0, x0, #0xa3c
   35704:	add	x2, x2, #0xa5e
   35708:	mov	w1, #0x169                 	// #361
   3570c:	bl	c6c0 <__gmp_assert_fail@plt>
   35710:	sub	x0, x29, #0x8
   35714:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35718:	mov	x22, x0
   3571c:	b	34f74 <__gmpn_tdiv_qr@@Base+0x1bc>
   35720:	sub	x0, x29, #0x8
   35724:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35728:	mov	x21, x0
   3572c:	b	350b0 <__gmpn_tdiv_qr@@Base+0x2f8>
   35730:	sub	x0, x29, #0x8
   35734:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35738:	mov	x28, x0
   3573c:	b	35030 <__gmpn_tdiv_qr@@Base+0x278>
   35740:	sub	x0, x29, #0x8
   35744:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35748:	mov	x27, x0
   3574c:	b	3506c <__gmpn_tdiv_qr@@Base+0x2b4>
   35750:	sub	x0, x29, #0x8
   35754:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35758:	mov	x27, x0
   3575c:	cmp	x25, x24
   35760:	b.lt	356c4 <__gmpn_tdiv_qr@@Base+0x90c>  // b.tstop
   35764:	mov	x0, x27
   35768:	mov	x1, x22
   3576c:	mov	x2, x25
   35770:	mov	x3, x20
   35774:	mov	x4, x24
   35778:	bl	ccd0 <__gmpn_mul@plt>
   3577c:	add	x2, x27, x25, lsl #3
   35780:	mov	x0, x26
   35784:	mov	x1, x26
   35788:	mov	x3, x24
   3578c:	bl	c2d0 <__gmpn_sub_n@plt>
   35790:	cbz	x0, 357b8 <__gmpn_tdiv_qr@@Base+0xa00>
   35794:	mov	w19, #0x1                   	// #1
   35798:	cmp	x24, x28
   3579c:	b.ge	357c4 <__gmpn_tdiv_qr@@Base+0xa0c>  // b.tcont
   357a0:	lsl	x8, x24, #3
   357a4:	ldr	x9, [x26, x8]
   357a8:	add	x24, x24, #0x1
   357ac:	sub	x10, x9, #0x1
   357b0:	str	x10, [x26, x8]
   357b4:	cbz	x9, 35798 <__gmpn_tdiv_qr@@Base+0x9e0>
   357b8:	mov	x22, x23
   357bc:	mov	x19, xzr
   357c0:	b	357c8 <__gmpn_tdiv_qr@@Base+0xa10>
   357c4:	mov	x22, x23
   357c8:	ldur	x23, [x29, #-32]
   357cc:	sub	x2, x21, x25
   357d0:	mov	x1, x26
   357d4:	add	x24, x23, x25, lsl #3
   357d8:	mov	x0, x24
   357dc:	bl	ca50 <__gmpn_copyi@plt>
   357e0:	ldur	x1, [x29, #-48]
   357e4:	mov	x0, x23
   357e8:	mov	x2, x27
   357ec:	mov	x3, x25
   357f0:	orr	x19, x19, x22
   357f4:	bl	c2d0 <__gmpn_sub_n@plt>
   357f8:	ldr	x8, [x24]
   357fc:	subs	x8, x8, x0
   35800:	str	x8, [x24]
   35804:	b.cs	35838 <__gmpn_tdiv_qr@@Base+0xa80>  // b.hs, b.nlast
   35808:	ldur	x22, [x29, #-40]
   3580c:	mov	w8, #0x1                   	// #1
   35810:	mov	w9, #0x1                   	// #1
   35814:	cmp	x9, x28
   35818:	b.ge	35840 <__gmpn_tdiv_qr@@Base+0xa88>  // b.tcont
   3581c:	lsl	x10, x9, #3
   35820:	ldr	x11, [x24, x10]
   35824:	add	x9, x9, #0x1
   35828:	sub	x12, x11, #0x1
   3582c:	str	x12, [x24, x10]
   35830:	cbz	x11, 35814 <__gmpn_tdiv_qr@@Base+0xa5c>
   35834:	b	3583c <__gmpn_tdiv_qr@@Base+0xa84>
   35838:	ldur	x22, [x29, #-40]
   3583c:	mov	x8, xzr
   35840:	orr	x23, x19, x8
   35844:	cbz	x23, 3586c <__gmpn_tdiv_qr@@Base+0xab4>
   35848:	ldr	x8, [x20]
   3584c:	sub	x9, x8, #0x1
   35850:	str	x9, [x20], #8
   35854:	cbz	x8, 35848 <__gmpn_tdiv_qr@@Base+0xa90>
   35858:	ldur	x0, [x29, #-32]
   3585c:	mov	x2, x22
   35860:	mov	x3, x21
   35864:	mov	x1, x0
   35868:	bl	ca70 <__gmpn_add_n@plt>
   3586c:	ldur	x0, [x29, #-8]
   35870:	cbnz	x0, 35894 <__gmpn_tdiv_qr@@Base+0xadc>
   35874:	mov	sp, x29
   35878:	ldp	x20, x19, [sp, #80]
   3587c:	ldp	x22, x21, [sp, #64]
   35880:	ldp	x24, x23, [sp, #48]
   35884:	ldp	x26, x25, [sp, #32]
   35888:	ldp	x28, x27, [sp, #16]
   3588c:	ldp	x29, x30, [sp], #96
   35890:	ret
   35894:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   35898:	b	35874 <__gmpn_tdiv_qr@@Base+0xabc>
   3589c:	sub	x0, x29, #0x8
   358a0:	mov	x1, x10
   358a4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358a8:	mov	x28, x0
   358ac:	b	34e80 <__gmpn_tdiv_qr@@Base+0xc8>
   358b0:	sub	x0, x29, #0x8
   358b4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358b8:	mov	x26, x0
   358bc:	b	34ee8 <__gmpn_tdiv_qr@@Base+0x130>
   358c0:	sub	x0, x29, #0x8
   358c4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358c8:	mov	x27, x0
   358cc:	b	35128 <__gmpn_tdiv_qr@@Base+0x370>
   358d0:	sub	x0, x29, #0x8
   358d4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358d8:	mov	x26, x0
   358dc:	b	35294 <__gmpn_tdiv_qr@@Base+0x4dc>
   358e0:	sub	x0, x29, #0x8
   358e4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358e8:	mov	x6, x0
   358ec:	b	35324 <__gmpn_tdiv_qr@@Base+0x56c>
   358f0:	sub	x0, x29, #0x8
   358f4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   358f8:	mov	x6, x0
   358fc:	b	354fc <__gmpn_tdiv_qr@@Base+0x744>
   35900:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   35904:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   35908:	add	x0, x0, #0xa3c
   3590c:	add	x2, x2, #0xa46
   35910:	mov	w1, #0x32                  	// #50
   35914:	bl	c6c0 <__gmp_assert_fail@plt>
   35918:	bl	bfd0 <__gmp_divide_by_zero@plt>
   3591c:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   35920:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   35924:	add	x0, x0, #0xa3c
   35928:	add	x2, x2, #0xa4f
   3592c:	mov	w1, #0x154                 	// #340
   35930:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000035934 <__gmpn_jacobi_base@@Base>:
   35934:	cbz	x0, 359b0 <__gmpn_jacobi_base@@Base+0x7c>
   35938:	lsr	x8, x1, #1
   3593c:	rbit	x9, x0
   35940:	clz	x9, x9
   35944:	eor	w10, w8, w1, lsr #2
   35948:	and	w10, w10, w9
   3594c:	lsr	x11, x0, x9
   35950:	eor	w9, w10, w2, asr #1
   35954:	lsr	x10, x11, #1
   35958:	subs	x11, x10, x8
   3595c:	b.eq	359b4 <__gmpn_jacobi_base@@Base+0x80>  // b.none
   35960:	asr	x12, x11, #63
   35964:	and	w10, w10, w8
   35968:	and	w10, w10, w12
   3596c:	eor	w9, w9, w10
   35970:	and	x10, x12, x11
   35974:	add	x8, x10, x8
   35978:	rbit	x10, x11
   3597c:	eor	x11, x12, x11
   35980:	clz	x10, x10
   35984:	sub	x11, x11, x12
   35988:	lsr	x12, x8, #1
   3598c:	add	w10, w10, #0x1
   35990:	eor	w12, w12, w8
   35994:	and	w12, w10, w12
   35998:	eor	w9, w9, w12
   3599c:	lsr	x10, x11, x10
   359a0:	cbnz	x8, 35958 <__gmpn_jacobi_base@@Base+0x24>
   359a4:	ubfiz	w8, w9, #1, #1
   359a8:	mov	w9, #0x1                   	// #1
   359ac:	sub	w0, w9, w8
   359b0:	ret
   359b4:	mov	w0, wzr
   359b8:	ret

00000000000359bc <__gmpn_jacobi_2@@Base>:
   359bc:	mov	x8, x0
   359c0:	ldr	x10, [x8, #8]
   359c4:	ldp	x9, x8, [x1]
   359c8:	ldr	x0, [x0]
   359cc:	lsl	w2, w2, #1
   359d0:	cmp	x9, #0x1
   359d4:	b.ne	359dc <__gmpn_jacobi_2@@Base+0x20>  // b.any
   359d8:	cbz	x8, 35b7c <__gmpn_jacobi_2@@Base+0x1c0>
   359dc:	cbz	x0, 35ac4 <__gmpn_jacobi_2@@Base+0x108>
   359e0:	tbnz	w0, #0, 35a0c <__gmpn_jacobi_2@@Base+0x50>
   359e4:	rbit	x11, x0
   359e8:	clz	x11, x11
   359ec:	eor	w12, w9, w9, lsr #1
   359f0:	neg	x13, x11
   359f4:	lsr	x14, x0, x11
   359f8:	and	w12, w12, w11, lsl #1
   359fc:	lsl	x13, x10, x13
   35a00:	lsr	x10, x10, x11
   35a04:	orr	x0, x13, x14
   35a08:	eor	w2, w2, w12
   35a0c:	cbz	x10, 35afc <__gmpn_jacobi_2@@Base+0x140>
   35a10:	cbz	x8, 35c08 <__gmpn_jacobi_2@@Base+0x24c>
   35a14:	cmp	x10, x8
   35a18:	b.ls	35a58 <__gmpn_jacobi_2@@Base+0x9c>  // b.plast
   35a1c:	eor	x11, x9, x9, lsr #1
   35a20:	subs	x13, x0, x9
   35a24:	sbc	x12, x10, x8
   35a28:	cbz	x13, 35b18 <__gmpn_jacobi_2@@Base+0x15c>
   35a2c:	rbit	x10, x13
   35a30:	clz	x10, x10
   35a34:	neg	x15, x10
   35a38:	and	w14, w11, w10, lsl #1
   35a3c:	lsr	x13, x13, x10
   35a40:	lsr	x10, x12, x10
   35a44:	lsl	x12, x12, x15
   35a48:	eor	w2, w2, w14
   35a4c:	cmp	x10, x8
   35a50:	orr	x0, x12, x13
   35a54:	b.hi	35a20 <__gmpn_jacobi_2@@Base+0x64>  // b.pmore
   35a58:	cmp	x10, x8
   35a5c:	b.eq	35c10 <__gmpn_jacobi_2@@Base+0x254>  // b.none
   35a60:	and	w11, w0, w9
   35a64:	eor	w2, w2, w11
   35a68:	cbz	x10, 35b08 <__gmpn_jacobi_2@@Base+0x14c>
   35a6c:	cmp	x8, x10
   35a70:	b.ls	35ab4 <__gmpn_jacobi_2@@Base+0xf8>  // b.plast
   35a74:	eor	x11, x0, x0, lsr #1
   35a78:	subs	x12, x9, x0
   35a7c:	sbc	x9, x8, x10
   35a80:	cbz	x12, 35b48 <__gmpn_jacobi_2@@Base+0x18c>
   35a84:	rbit	x8, x12
   35a88:	clz	x8, x8
   35a8c:	neg	x14, x8
   35a90:	and	w13, w11, w8, lsl #1
   35a94:	lsr	x12, x12, x8
   35a98:	lsr	x8, x9, x8
   35a9c:	lsl	x9, x9, x14
   35aa0:	eor	w2, w2, w13
   35aa4:	cmp	x8, x10
   35aa8:	orr	x9, x9, x12
   35aac:	b.hi	35a78 <__gmpn_jacobi_2@@Base+0xbc>  // b.pmore
   35ab0:	and	w11, w9, w0
   35ab4:	cmp	x10, x8
   35ab8:	eor	w2, w2, w11
   35abc:	b.ne	35a10 <__gmpn_jacobi_2@@Base+0x54>  // b.any
   35ac0:	b	35c14 <__gmpn_jacobi_2@@Base+0x258>
   35ac4:	cbz	x10, 35c64 <__gmpn_jacobi_2@@Base+0x2a8>
   35ac8:	rbit	x11, x10
   35acc:	clz	x11, x11
   35ad0:	lsr	x12, x9, #1
   35ad4:	lsl	w13, w11, #1
   35ad8:	eor	w12, w12, w9
   35adc:	lsr	x1, x10, x11
   35ae0:	orr	w10, w13, #0x80
   35ae4:	and	w10, w10, w12
   35ae8:	cmp	x1, #0x1
   35aec:	eor	w10, w2, w10
   35af0:	b.ne	35b8c <__gmpn_jacobi_2@@Base+0x1d0>  // b.any
   35af4:	and	w8, w10, #0x2
   35af8:	b	35b80 <__gmpn_jacobi_2@@Base+0x1c4>
   35afc:	cbz	x8, 35c00 <__gmpn_jacobi_2@@Base+0x244>
   35b00:	and	w10, w0, w9
   35b04:	eor	w2, w2, w10
   35b08:	mov	x1, x0
   35b0c:	cmp	x1, #0x1
   35b10:	b.eq	35b7c <__gmpn_jacobi_2@@Base+0x1c0>  // b.none
   35b14:	b	35b94 <__gmpn_jacobi_2@@Base+0x1d8>
   35b18:	rbit	x10, x12
   35b1c:	clz	x10, x10
   35b20:	lsl	w13, w10, #1
   35b24:	lsr	x1, x12, x10
   35b28:	add	w10, w13, #0x80
   35b2c:	and	w12, w1, w9
   35b30:	and	w10, w10, w11
   35b34:	eor	w11, w2, w12
   35b38:	eor	w2, w11, w10
   35b3c:	cmp	x1, #0x1
   35b40:	b.eq	35b7c <__gmpn_jacobi_2@@Base+0x1c0>  // b.none
   35b44:	b	35b94 <__gmpn_jacobi_2@@Base+0x1d8>
   35b48:	rbit	x8, x9
   35b4c:	clz	x8, x8
   35b50:	lsl	w12, w8, #1
   35b54:	lsr	x1, x9, x8
   35b58:	add	w8, w12, #0x80
   35b5c:	and	w9, w1, w0
   35b60:	and	w8, w8, w11
   35b64:	eor	w9, w2, w9
   35b68:	eor	w2, w9, w8
   35b6c:	mov	x9, x0
   35b70:	mov	x8, x10
   35b74:	cmp	x1, #0x1
   35b78:	b.ne	35b94 <__gmpn_jacobi_2@@Base+0x1d8>  // b.any
   35b7c:	and	w8, w2, #0x2
   35b80:	mov	w9, #0x1                   	// #1
   35b84:	sub	w0, w9, w8
   35b88:	ret
   35b8c:	and	w11, w1, w9
   35b90:	eor	w2, w10, w11
   35b94:	cbz	x8, 35bd4 <__gmpn_jacobi_2@@Base+0x218>
   35b98:	eor	x10, x1, x1, lsr #1
   35b9c:	subs	x11, x9, x1
   35ba0:	cset	w9, cc  // cc = lo, ul, last
   35ba4:	sub	x9, x8, x9
   35ba8:	cbz	x11, 35bdc <__gmpn_jacobi_2@@Base+0x220>
   35bac:	rbit	x8, x11
   35bb0:	clz	x12, x8
   35bb4:	neg	x13, x12
   35bb8:	lsr	x11, x11, x12
   35bbc:	lsr	x8, x9, x12
   35bc0:	and	w12, w10, w12, lsl #1
   35bc4:	lsl	x9, x9, x13
   35bc8:	orr	x9, x9, x11
   35bcc:	eor	w2, w2, w12
   35bd0:	cbnz	x8, 35b9c <__gmpn_jacobi_2@@Base+0x1e0>
   35bd4:	mov	x0, x9
   35bd8:	b	c730 <__gmpn_jacobi_base@plt>
   35bdc:	cbz	x9, 35c64 <__gmpn_jacobi_2@@Base+0x2a8>
   35be0:	rbit	x8, x9
   35be4:	clz	x8, x8
   35be8:	lsl	w11, w8, #1
   35bec:	orr	w11, w11, #0x80
   35bf0:	and	w10, w11, w10
   35bf4:	eor	w2, w2, w10
   35bf8:	lsr	x0, x9, x8
   35bfc:	b	c730 <__gmpn_jacobi_base@plt>
   35c00:	mov	x1, x9
   35c04:	b	c730 <__gmpn_jacobi_base@plt>
   35c08:	mov	x1, x9
   35c0c:	b	35b6c <__gmpn_jacobi_2@@Base+0x1b0>
   35c10:	mov	x10, x8
   35c14:	cmp	x0, x9
   35c18:	csel	x11, x0, x9, cc  // cc = lo, ul, last
   35c1c:	csel	x8, x9, x0, cc  // cc = lo, ul, last
   35c20:	subs	x8, x8, x11
   35c24:	b.eq	35c64 <__gmpn_jacobi_2@@Base+0x2a8>  // b.none
   35c28:	and	w12, w9, w0
   35c2c:	cmp	x0, x9
   35c30:	rbit	x9, x8
   35c34:	lsr	x13, x11, #1
   35c38:	csel	w12, w12, wzr, cc  // cc = lo, ul, last
   35c3c:	clz	x9, x9
   35c40:	eor	w13, w13, w11
   35c44:	eor	w12, w12, w2
   35c48:	and	w13, w13, w9, lsl #1
   35c4c:	lsr	x1, x8, x9
   35c50:	cmp	x1, #0x1
   35c54:	eor	w8, w12, w13
   35c58:	b.ne	35c6c <__gmpn_jacobi_2@@Base+0x2b0>  // b.any
   35c5c:	and	w8, w8, #0x2
   35c60:	b	35b80 <__gmpn_jacobi_2@@Base+0x1c4>
   35c64:	mov	w0, wzr
   35c68:	ret
   35c6c:	and	w9, w1, w11
   35c70:	eor	w2, w8, w9
   35c74:	mov	x8, x10
   35c78:	mov	x9, x11
   35c7c:	cbnz	x8, 35b98 <__gmpn_jacobi_2@@Base+0x1dc>
   35c80:	b	35bd4 <__gmpn_jacobi_2@@Base+0x218>

0000000000035c84 <__gmpn_jacobi_n@@Base>:
   35c84:	stp	x29, x30, [sp, #-96]!
   35c88:	str	x27, [sp, #16]
   35c8c:	stp	x26, x25, [sp, #32]
   35c90:	stp	x24, x23, [sp, #48]
   35c94:	stp	x22, x21, [sp, #64]
   35c98:	stp	x20, x19, [sp, #80]
   35c9c:	mov	x29, sp
   35ca0:	sub	sp, sp, #0x40
   35ca4:	mov	x21, x2
   35ca8:	mov	x19, x1
   35cac:	mov	x20, x0
   35cb0:	cmp	x2, #0x14a
   35cb4:	mov	x8, x2
   35cb8:	str	w3, [x29, #28]
   35cbc:	b.lt	35d14 <__gmpn_jacobi_n@@Base+0x90>  // b.tstop
   35cc0:	mov	x9, #0x5555555555555555    	// #6148914691236517205
   35cc4:	lsl	x8, x21, #1
   35cc8:	movk	x9, #0x5556
   35ccc:	smulh	x8, x8, x9
   35cd0:	add	x22, x8, x8, lsr #63
   35cd4:	sub	x0, x21, x22
   35cd8:	add	x8, x0, #0x1
   35cdc:	add	x9, x0, #0x2
   35ce0:	cmp	x8, #0x0
   35ce4:	csinc	x8, x9, x0, lt  // lt = tstop
   35ce8:	lsl	x8, x8, #1
   35cec:	and	x23, x8, #0xfffffffffffffffc
   35cf0:	bl	c590 <__gmpn_hgcd_itch@plt>
   35cf4:	add	x8, x22, x21
   35cf8:	sub	x9, x8, #0x1
   35cfc:	cmp	x0, x8
   35d00:	csel	x8, x9, x0, lt  // lt = tstop
   35d04:	add	x8, x23, x8
   35d08:	add	x8, x8, #0x4
   35d0c:	cmp	x8, x21
   35d10:	csel	x8, x8, x21, gt
   35d14:	lsl	x1, x8, #3
   35d18:	mov	w8, #0x7f00                	// #32512
   35d1c:	cmp	x1, x8
   35d20:	stur	xzr, [x29, #-8]
   35d24:	b.hi	35e14 <__gmpn_jacobi_n@@Base+0x190>  // b.pmore
   35d28:	add	x9, x1, #0xf
   35d2c:	mov	x8, sp
   35d30:	and	x9, x9, #0xfffffffffffffff0
   35d34:	sub	x22, x8, x9
   35d38:	mov	sp, x22
   35d3c:	cmp	x21, #0x14a
   35d40:	b.lt	35e28 <__gmpn_jacobi_n@@Base+0x1a4>  // b.tstop
   35d44:	mov	x27, #0x5555555555555555    	// #6148914691236517205
   35d48:	adrp	x23, 35000 <__gmpn_tdiv_qr@@Base+0x248>
   35d4c:	movk	x27, #0x5556
   35d50:	add	x23, x23, #0xfec
   35d54:	b	35d80 <__gmpn_jacobi_n@@Base+0xfc>
   35d58:	add	x1, x0, x24
   35d5c:	sub	x0, x29, #0x38
   35d60:	mov	x2, x20
   35d64:	mov	x3, x19
   35d68:	mov	x4, x24
   35d6c:	mov	x5, x25
   35d70:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   35d74:	mov	x21, x0
   35d78:	cmp	x21, #0x149
   35d7c:	b.le	35e28 <__gmpn_jacobi_n@@Base+0x1a4>
   35d80:	lsl	x8, x21, #1
   35d84:	smulh	x8, x8, x27
   35d88:	add	x24, x8, x8, lsr #63
   35d8c:	sub	x26, x21, x24
   35d90:	add	x8, x26, #0x1
   35d94:	add	x9, x26, #0x2
   35d98:	cmp	x8, #0x0
   35d9c:	csinc	x8, x9, x26, lt  // lt = tstop
   35da0:	lsl	x8, x8, #4
   35da4:	sub	x0, x29, #0x38
   35da8:	mov	x1, x26
   35dac:	mov	x2, x22
   35db0:	and	x25, x8, #0xffffffffffffffe0
   35db4:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   35db8:	add	x9, x25, x22
   35dbc:	lsl	x8, x24, #3
   35dc0:	add	x25, x9, #0x20
   35dc4:	add	x0, x20, x8
   35dc8:	add	x1, x19, x8
   35dcc:	sub	x3, x29, #0x38
   35dd0:	add	x4, x29, #0x1c
   35dd4:	mov	x2, x26
   35dd8:	mov	x5, x25
   35ddc:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   35de0:	cmp	x0, #0x1
   35de4:	b.ge	35d58 <__gmpn_jacobi_n@@Base+0xd4>  // b.tcont
   35de8:	add	x5, x29, #0x1c
   35dec:	mov	x0, x20
   35df0:	mov	x1, x19
   35df4:	mov	x2, x21
   35df8:	mov	x3, xzr
   35dfc:	mov	x4, x23
   35e00:	mov	x6, x22
   35e04:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   35e08:	mov	x21, x0
   35e0c:	cbnz	x0, 35d78 <__gmpn_jacobi_n@@Base+0xf4>
   35e10:	b	35f1c <__gmpn_jacobi_n@@Base+0x298>
   35e14:	sub	x0, x29, #0x8
   35e18:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   35e1c:	mov	x22, x0
   35e20:	cmp	x21, #0x14a
   35e24:	b.ge	35d44 <__gmpn_jacobi_n@@Base+0xc0>  // b.tcont
   35e28:	cmp	x21, #0x3
   35e2c:	b.lt	35f40 <__gmpn_jacobi_n@@Base+0x2bc>  // b.tstop
   35e30:	adrp	x23, 35000 <__gmpn_tdiv_qr@@Base+0x248>
   35e34:	add	x23, x23, #0xfec
   35e38:	b	35e6c <__gmpn_jacobi_n@@Base+0x1e8>
   35e3c:	sub	x0, x29, #0x38
   35e40:	mov	x1, x22
   35e44:	mov	x2, x20
   35e48:	mov	x3, x19
   35e4c:	mov	x4, x21
   35e50:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   35e54:	mov	x21, x0
   35e58:	mov	x0, x20
   35e5c:	mov	x20, x22
   35e60:	mov	x22, x0
   35e64:	cmp	x21, #0x2
   35e68:	b.le	35f40 <__gmpn_jacobi_n@@Base+0x2bc>
   35e6c:	lsl	x8, x21, #3
   35e70:	sub	x9, x8, #0x8
   35e74:	ldr	x0, [x20, x9]
   35e78:	ldr	x2, [x19, x9]
   35e7c:	orr	x9, x2, x0
   35e80:	tbnz	x9, #63, 35ed8 <__gmpn_jacobi_n@@Base+0x254>
   35e84:	sub	x10, x8, #0x10
   35e88:	sub	x8, x8, #0x18
   35e8c:	ldr	x12, [x20, x10]
   35e90:	ldr	x14, [x20, x8]
   35e94:	ldr	x10, [x19, x10]
   35e98:	ldr	x8, [x19, x8]
   35e9c:	clz	x9, x9
   35ea0:	neg	x13, x9
   35ea4:	lsl	x11, x0, x9
   35ea8:	lsl	x15, x2, x9
   35eac:	lsr	x16, x12, x13
   35eb0:	lsl	x12, x12, x9
   35eb4:	lsr	x14, x14, x13
   35eb8:	lsl	x9, x10, x9
   35ebc:	lsr	x10, x10, x13
   35ec0:	lsr	x8, x8, x13
   35ec4:	orr	x0, x16, x11
   35ec8:	orr	x1, x14, x12
   35ecc:	orr	x2, x10, x15
   35ed0:	orr	x3, x8, x9
   35ed4:	b	35ee4 <__gmpn_jacobi_n@@Base+0x260>
   35ed8:	sub	x8, x8, #0x10
   35edc:	ldr	x1, [x20, x8]
   35ee0:	ldr	x3, [x19, x8]
   35ee4:	sub	x4, x29, #0x38
   35ee8:	add	x5, x29, #0x1c
   35eec:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   35ef0:	cbnz	w0, 35e3c <__gmpn_jacobi_n@@Base+0x1b8>
   35ef4:	add	x5, x29, #0x1c
   35ef8:	mov	x0, x20
   35efc:	mov	x1, x19
   35f00:	mov	x2, x21
   35f04:	mov	x3, xzr
   35f08:	mov	x4, x23
   35f0c:	mov	x6, x22
   35f10:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   35f14:	mov	x21, x0
   35f18:	cbnz	x0, 35e64 <__gmpn_jacobi_n@@Base+0x1e0>
   35f1c:	ldur	x0, [x29, #-8]
   35f20:	cbnz	x0, 35fe4 <__gmpn_jacobi_n@@Base+0x360>
   35f24:	ldr	w8, [x29, #28]
   35f28:	mov	w9, #0x1                   	// #1
   35f2c:	ubfiz	w10, w8, #1, #1
   35f30:	sub	w9, w9, w10
   35f34:	cmp	w8, #0x1f
   35f38:	csel	w19, wzr, w9, eq  // eq = none
   35f3c:	b	35fb4 <__gmpn_jacobi_n@@Base+0x330>
   35f40:	ldr	w8, [x29, #28]
   35f44:	cmp	w8, #0xf
   35f48:	csel	x1, x20, x19, hi  // hi = pmore
   35f4c:	csel	x0, x19, x20, hi  // hi = pmore
   35f50:	cmp	x21, #0x1
   35f54:	b.ne	35f84 <__gmpn_jacobi_n@@Base+0x300>  // b.any
   35f58:	ldr	x19, [x0]
   35f5c:	ldur	x0, [x29, #-8]
   35f60:	ldr	x20, [x1]
   35f64:	cbnz	x0, 35fd8 <__gmpn_jacobi_n@@Base+0x354>
   35f68:	cmp	x20, #0x1
   35f6c:	lsl	w2, w8, #1
   35f70:	b.ne	35fa4 <__gmpn_jacobi_n@@Base+0x320>  // b.any
   35f74:	and	w8, w2, #0x2
   35f78:	mov	w9, #0x1                   	// #1
   35f7c:	sub	w19, w9, w8
   35f80:	b	35fb4 <__gmpn_jacobi_n@@Base+0x330>
   35f84:	and	w2, w8, #0x1
   35f88:	bl	cab0 <__gmpn_jacobi_2@plt>
   35f8c:	ldur	x8, [x29, #-8]
   35f90:	mov	w19, w0
   35f94:	cbz	x8, 35fb4 <__gmpn_jacobi_n@@Base+0x330>
   35f98:	mov	x0, x8
   35f9c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   35fa0:	b	35fb4 <__gmpn_jacobi_n@@Base+0x330>
   35fa4:	mov	x0, x19
   35fa8:	mov	x1, x20
   35fac:	bl	c730 <__gmpn_jacobi_base@plt>
   35fb0:	mov	w19, w0
   35fb4:	mov	w0, w19
   35fb8:	mov	sp, x29
   35fbc:	ldp	x20, x19, [sp, #80]
   35fc0:	ldp	x22, x21, [sp, #64]
   35fc4:	ldp	x24, x23, [sp, #48]
   35fc8:	ldp	x26, x25, [sp, #32]
   35fcc:	ldr	x27, [sp, #16]
   35fd0:	ldp	x29, x30, [sp], #96
   35fd4:	ret
   35fd8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   35fdc:	ldr	w8, [x29, #28]
   35fe0:	b	35f68 <__gmpn_jacobi_n@@Base+0x2e4>
   35fe4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   35fe8:	b	35f24 <__gmpn_jacobi_n@@Base+0x2a0>
   35fec:	cbz	x1, 36004 <__gmpn_jacobi_n@@Base+0x380>
   35ff0:	cmp	x2, #0x1
   35ff4:	b.ne	3602c <__gmpn_jacobi_n@@Base+0x3a8>  // b.any
   35ff8:	ldr	x8, [x1]
   35ffc:	cmp	x8, #0x1
   36000:	b.ne	3602c <__gmpn_jacobi_n@@Base+0x3a8>  // b.any
   36004:	cbz	x3, 36034 <__gmpn_jacobi_n@@Base+0x3b0>
   36008:	ldr	w8, [x0]
   3600c:	ldr	w9, [x3]
   36010:	lsl	w8, w8, #3
   36014:	add	w8, w8, w5, lsl #2
   36018:	bfxil	w8, w9, #0, #2
   3601c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   36020:	ldr	x9, [x9, #3872]
   36024:	ldrb	w8, [x9, w8, uxtw]
   36028:	b	36030 <__gmpn_jacobi_n@@Base+0x3ac>
   3602c:	mov	w8, #0x1f                  	// #31
   36030:	str	w8, [x0]
   36034:	ret

0000000000036038 <__gmpn_get_d@@Base>:
   36038:	fmov	d0, xzr
   3603c:	cbz	x1, 360c8 <__gmpn_get_d@@Base+0x90>
   36040:	mov	x9, #0x7fffffffffffffff    	// #9223372036854775807
   36044:	lsl	x8, x1, #6
   36048:	sub	x9, x9, x3
   3604c:	cmp	x8, x9
   36050:	b.hi	360cc <__gmpn_get_d@@Base+0x94>  // b.pmore
   36054:	add	x9, x0, x1, lsl #3
   36058:	ldur	x10, [x9, #-8]
   3605c:	add	x8, x8, x3
   36060:	cmp	x1, #0x2
   36064:	clz	x11, x10
   36068:	mvn	x12, x11
   3606c:	add	x8, x8, x12
   36070:	lsl	x10, x10, x11
   36074:	b.lt	36090 <__gmpn_get_d@@Base+0x58>  // b.tstop
   36078:	cmp	w11, #0xc
   3607c:	b.cc	36090 <__gmpn_get_d@@Base+0x58>  // b.lo, b.ul, b.last
   36080:	ldur	x9, [x9, #-16]
   36084:	neg	x11, x11
   36088:	lsr	x9, x9, x11
   3608c:	orr	x10, x9, x10
   36090:	cmp	x8, #0x3ff
   36094:	b.gt	360cc <__gmpn_get_d@@Base+0x94>
   36098:	cmn	x8, #0x3ff
   3609c:	lsr	x9, x10, #11
   360a0:	b.le	360dc <__gmpn_get_d@@Base+0xa4>
   360a4:	lsr	x10, x10, #43
   360a8:	mov	x11, #0x3ff0000000000000    	// #4607182418800017408
   360ac:	and	x12, x2, #0x8000000000000000
   360b0:	add	x8, x11, x8, lsl #52
   360b4:	bfxil	x12, x9, #0, #32
   360b8:	and	x8, x8, #0x7ff0000000000000
   360bc:	bfi	x12, x10, #32, #20
   360c0:	orr	x8, x12, x8
   360c4:	fmov	d0, x8
   360c8:	ret
   360cc:	mov	x10, xzr
   360d0:	mov	x9, xzr
   360d4:	mov	w8, #0x400                 	// #1024
   360d8:	b	360a8 <__gmpn_get_d@@Base+0x70>
   360dc:	cmn	x8, #0x432
   360e0:	b.lt	360c8 <__gmpn_get_d@@Base+0x90>  // b.tstop
   360e4:	mov	w10, #0xfffffc02            	// #-1022
   360e8:	sub	w8, w10, w8
   360ec:	lsr	x9, x9, x8
   360f0:	lsr	x10, x9, #32
   360f4:	mov	x8, #0xfffffffffffffc01    	// #-1023
   360f8:	b	360a8 <__gmpn_get_d@@Base+0x70>

00000000000360fc <__gmpn_matrix22_mul_itch@@Base>:
   360fc:	cmp	x0, #0xa
   36100:	b.lt	3611c <__gmpn_matrix22_mul_itch@@Base+0x20>  // b.tstop
   36104:	cmp	x1, #0x9
   36108:	b.le	3611c <__gmpn_matrix22_mul_itch@@Base+0x20>
   3610c:	add	x8, x1, x0
   36110:	add	x8, x8, x8, lsl #1
   36114:	add	x0, x8, #0x5
   36118:	ret
   3611c:	add	x8, x0, x0, lsl #1
   36120:	add	x0, x8, x1, lsl #1
   36124:	ret

0000000000036128 <__gmpn_matrix22_mul@@Base>:
   36128:	sub	sp, sp, #0xd0
   3612c:	stp	x29, x30, [sp, #112]
   36130:	add	x29, sp, #0x70
   36134:	stp	x28, x27, [sp, #128]
   36138:	stp	x26, x25, [sp, #144]
   3613c:	stp	x24, x23, [sp, #160]
   36140:	stp	x22, x21, [sp, #176]
   36144:	stp	x20, x19, [sp, #192]
   36148:	stur	x6, [x29, #-24]
   3614c:	stur	x5, [x29, #-40]
   36150:	str	x2, [sp, #32]
   36154:	ldp	x23, x19, [x29, #104]
   36158:	ldr	x24, [x29, #96]
   3615c:	mov	x28, x7
   36160:	mov	x21, x4
   36164:	mov	x20, x3
   36168:	mov	x25, x1
   3616c:	cmp	x4, #0xa
   36170:	mov	x22, x0
   36174:	stur	x24, [x29, #-48]
   36178:	b.lt	361dc <__gmpn_matrix22_mul@@Base+0xb4>  // b.tstop
   3617c:	cmp	x23, #0x9
   36180:	b.le	361dc <__gmpn_matrix22_mul@@Base+0xb4>
   36184:	add	x8, x21, #0x1
   36188:	str	x8, [sp, #48]
   3618c:	add	x9, x23, #0x1
   36190:	add	x10, x23, x21
   36194:	add	x8, x19, x8, lsl #3
   36198:	str	x10, [sp, #40]
   3619c:	add	x10, x10, #0x1
   361a0:	add	x0, x8, x9, lsl #3
   361a4:	str	x9, [sp, #24]
   361a8:	stur	x8, [x29, #-32]
   361ac:	cmp	x21, x23
   361b0:	add	x8, x0, x10, lsl #3
   361b4:	stur	x8, [x29, #-8]
   361b8:	str	x19, [sp, #56]
   361bc:	stur	x0, [x29, #-16]
   361c0:	str	x10, [sp, #16]
   361c4:	b.lt	3626c <__gmpn_matrix22_mul@@Base+0x144>  // b.tstop
   361c8:	mov	x1, x25
   361cc:	mov	x2, x21
   361d0:	mov	x3, x28
   361d4:	mov	x4, x23
   361d8:	b	3627c <__gmpn_matrix22_mul@@Base+0x154>
   361dc:	lsl	x8, x21, #3
   361e0:	add	x26, x19, x8
   361e4:	add	x8, x26, x8
   361e8:	add	x8, x8, x23, lsl #3
   361ec:	mov	x0, x19
   361f0:	mov	x1, x22
   361f4:	mov	x2, x21
   361f8:	stur	x8, [x29, #-8]
   361fc:	add	x24, x23, x21
   36200:	bl	ca50 <__gmpn_copyi@plt>
   36204:	mov	x0, x26
   36208:	cmp	x21, x23
   3620c:	b.ge	3631c <__gmpn_matrix22_mul@@Base+0x1f4>  // b.tcont
   36210:	ldur	x1, [x29, #-40]
   36214:	mov	x2, x23
   36218:	mov	x3, x22
   3621c:	mov	x4, x21
   36220:	bl	ccd0 <__gmpn_mul@plt>
   36224:	ldur	x0, [x29, #-8]
   36228:	ldur	x1, [x29, #-48]
   3622c:	mov	x2, x23
   36230:	mov	x3, x25
   36234:	mov	x4, x21
   36238:	bl	ccd0 <__gmpn_mul@plt>
   3623c:	mov	x0, x22
   36240:	mov	x1, x28
   36244:	mov	x2, x23
   36248:	mov	x3, x25
   3624c:	mov	x4, x21
   36250:	bl	ccd0 <__gmpn_mul@plt>
   36254:	ldur	x1, [x29, #-24]
   36258:	mov	x0, x25
   3625c:	mov	x2, x23
   36260:	mov	x3, x19
   36264:	mov	x4, x21
   36268:	b	36374 <__gmpn_matrix22_mul@@Base+0x24c>
   3626c:	mov	x1, x28
   36270:	mov	x2, x23
   36274:	mov	x3, x25
   36278:	mov	x4, x21
   3627c:	bl	ccd0 <__gmpn_mul@plt>
   36280:	ldr	x19, [sp, #32]
   36284:	sub	x26, x21, #0x1
   36288:	mov	x8, x26
   3628c:	add	x9, x8, #0x1
   36290:	cmp	x9, #0x1
   36294:	b.lt	362b4 <__gmpn_matrix22_mul@@Base+0x18c>  // b.tstop
   36298:	lsl	x9, x8, #3
   3629c:	ldr	x10, [x20, x9]
   362a0:	ldr	x9, [x19, x9]
   362a4:	sub	x8, x8, #0x1
   362a8:	cmp	x10, x9
   362ac:	b.eq	3628c <__gmpn_matrix22_mul@@Base+0x164>  // b.none
   362b0:	b.ls	364e0 <__gmpn_matrix22_mul@@Base+0x3b8>  // b.plast
   362b4:	mov	x2, x19
   362b8:	mov	x0, x20
   362bc:	mov	x1, x20
   362c0:	mov	x3, x21
   362c4:	bl	c2d0 <__gmpn_sub_n@plt>
   362c8:	mov	x0, x25
   362cc:	mov	x1, x25
   362d0:	mov	x2, x20
   362d4:	mov	x3, x21
   362d8:	bl	ca70 <__gmpn_add_n@plt>
   362dc:	str	x0, [x25, x21, lsl #3]
   362e0:	cbz	x0, 364d4 <__gmpn_matrix22_mul@@Base+0x3ac>
   362e4:	ldr	x27, [sp, #56]
   362e8:	mov	x24, x0
   362ec:	mov	x1, x25
   362f0:	mov	x2, x22
   362f4:	mov	x0, x27
   362f8:	mov	x3, x21
   362fc:	bl	c2d0 <__gmpn_sub_n@plt>
   36300:	mov	w8, #0x1                   	// #1
   36304:	str	wzr, [sp, #12]
   36308:	str	w8, [sp, #32]
   3630c:	sub	x0, x24, x0
   36310:	ldur	x24, [x29, #-48]
   36314:	mov	w26, wzr
   36318:	b	365fc <__gmpn_matrix22_mul@@Base+0x4d4>
   3631c:	ldur	x3, [x29, #-40]
   36320:	mov	x1, x22
   36324:	mov	x2, x21
   36328:	mov	x4, x23
   3632c:	bl	ccd0 <__gmpn_mul@plt>
   36330:	ldur	x0, [x29, #-8]
   36334:	ldur	x3, [x29, #-48]
   36338:	mov	x1, x25
   3633c:	mov	x2, x21
   36340:	mov	x4, x23
   36344:	bl	ccd0 <__gmpn_mul@plt>
   36348:	mov	x0, x22
   3634c:	mov	x1, x25
   36350:	mov	x2, x21
   36354:	mov	x3, x28
   36358:	mov	x4, x23
   3635c:	bl	ccd0 <__gmpn_mul@plt>
   36360:	ldur	x3, [x29, #-24]
   36364:	mov	x0, x25
   36368:	mov	x1, x19
   3636c:	mov	x2, x21
   36370:	mov	x4, x23
   36374:	bl	ccd0 <__gmpn_mul@plt>
   36378:	mov	x0, x22
   3637c:	mov	x1, x22
   36380:	mov	x2, x26
   36384:	mov	x3, x24
   36388:	bl	ca70 <__gmpn_add_n@plt>
   3638c:	ldur	x2, [x29, #-8]
   36390:	lsl	x27, x24, #3
   36394:	str	x0, [x22, x27]
   36398:	mov	x0, x25
   3639c:	mov	x1, x25
   363a0:	mov	x3, x24
   363a4:	bl	ca70 <__gmpn_add_n@plt>
   363a8:	ldr	x22, [sp, #32]
   363ac:	str	x0, [x25, x27]
   363b0:	mov	x0, x19
   363b4:	mov	x2, x21
   363b8:	mov	x1, x22
   363bc:	bl	ca50 <__gmpn_copyi@plt>
   363c0:	mov	x0, x26
   363c4:	cmp	x21, x23
   363c8:	b.ge	36428 <__gmpn_matrix22_mul@@Base+0x300>  // b.tcont
   363cc:	ldur	x1, [x29, #-40]
   363d0:	mov	x2, x23
   363d4:	mov	x3, x22
   363d8:	mov	x4, x21
   363dc:	bl	ccd0 <__gmpn_mul@plt>
   363e0:	ldur	x0, [x29, #-8]
   363e4:	ldur	x1, [x29, #-48]
   363e8:	mov	x2, x23
   363ec:	mov	x3, x20
   363f0:	mov	x4, x21
   363f4:	bl	ccd0 <__gmpn_mul@plt>
   363f8:	mov	x0, x22
   363fc:	mov	x1, x28
   36400:	mov	x2, x23
   36404:	mov	x3, x20
   36408:	mov	x4, x21
   3640c:	bl	ccd0 <__gmpn_mul@plt>
   36410:	ldur	x1, [x29, #-24]
   36414:	mov	x0, x20
   36418:	mov	x2, x23
   3641c:	mov	x3, x19
   36420:	mov	x4, x21
   36424:	b	36480 <__gmpn_matrix22_mul@@Base+0x358>
   36428:	ldur	x3, [x29, #-40]
   3642c:	mov	x1, x22
   36430:	mov	x2, x21
   36434:	mov	x4, x23
   36438:	bl	ccd0 <__gmpn_mul@plt>
   3643c:	ldur	x0, [x29, #-8]
   36440:	ldur	x3, [x29, #-48]
   36444:	mov	x1, x20
   36448:	mov	x2, x21
   3644c:	mov	x4, x23
   36450:	bl	ccd0 <__gmpn_mul@plt>
   36454:	mov	x0, x22
   36458:	mov	x1, x20
   3645c:	mov	x2, x21
   36460:	mov	x3, x28
   36464:	mov	x4, x23
   36468:	bl	ccd0 <__gmpn_mul@plt>
   3646c:	ldur	x3, [x29, #-24]
   36470:	mov	x0, x20
   36474:	mov	x1, x19
   36478:	mov	x2, x21
   3647c:	mov	x4, x23
   36480:	bl	ccd0 <__gmpn_mul@plt>
   36484:	mov	x0, x22
   36488:	mov	x1, x22
   3648c:	mov	x2, x26
   36490:	mov	x3, x24
   36494:	bl	ca70 <__gmpn_add_n@plt>
   36498:	ldur	x2, [x29, #-8]
   3649c:	str	x0, [x22, x27]
   364a0:	mov	x0, x20
   364a4:	mov	x1, x20
   364a8:	mov	x3, x24
   364ac:	bl	ca70 <__gmpn_add_n@plt>
   364b0:	str	x0, [x20, x27]
   364b4:	ldp	x20, x19, [sp, #192]
   364b8:	ldp	x22, x21, [sp, #176]
   364bc:	ldp	x24, x23, [sp, #160]
   364c0:	ldp	x26, x25, [sp, #144]
   364c4:	ldp	x28, x27, [sp, #128]
   364c8:	ldp	x29, x30, [sp, #112]
   364cc:	add	sp, sp, #0xd0
   364d0:	ret
   364d4:	str	wzr, [sp, #12]
   364d8:	ldur	x24, [x29, #-48]
   364dc:	b	36540 <__gmpn_matrix22_mul@@Base+0x418>
   364e0:	mov	x0, x20
   364e4:	mov	x1, x19
   364e8:	mov	x2, x20
   364ec:	mov	x3, x21
   364f0:	bl	c2d0 <__gmpn_sub_n@plt>
   364f4:	mov	x8, x26
   364f8:	add	x9, x8, #0x1
   364fc:	cmp	x9, #0x1
   36500:	b.lt	36520 <__gmpn_matrix22_mul@@Base+0x3f8>  // b.tstop
   36504:	lsl	x9, x8, #3
   36508:	ldr	x10, [x25, x9]
   3650c:	ldr	x9, [x20, x9]
   36510:	sub	x8, x8, #0x1
   36514:	cmp	x10, x9
   36518:	b.eq	364f8 <__gmpn_matrix22_mul@@Base+0x3d0>  // b.none
   3651c:	b.ls	365bc <__gmpn_matrix22_mul@@Base+0x494>  // b.plast
   36520:	mov	x0, x25
   36524:	mov	x1, x25
   36528:	mov	x2, x20
   3652c:	mov	x3, x21
   36530:	bl	c2d0 <__gmpn_sub_n@plt>
   36534:	mov	w8, #0x1                   	// #1
   36538:	str	xzr, [x25, x21, lsl #3]
   3653c:	str	w8, [sp, #12]
   36540:	add	x8, x26, #0x1
   36544:	cmp	x8, #0x1
   36548:	b.lt	36568 <__gmpn_matrix22_mul@@Base+0x440>  // b.tstop
   3654c:	lsl	x8, x26, #3
   36550:	ldr	x9, [x22, x8]
   36554:	ldr	x8, [x25, x8]
   36558:	sub	x26, x26, #0x1
   3655c:	cmp	x9, x8
   36560:	b.eq	36540 <__gmpn_matrix22_mul@@Base+0x418>  // b.none
   36564:	b.ls	36590 <__gmpn_matrix22_mul@@Base+0x468>  // b.plast
   36568:	ldr	x27, [sp, #56]
   3656c:	mov	x1, x22
   36570:	mov	x2, x25
   36574:	mov	x3, x21
   36578:	mov	x0, x27
   3657c:	bl	c2d0 <__gmpn_sub_n@plt>
   36580:	mov	x0, xzr
   36584:	mov	w26, wzr
   36588:	str	wzr, [sp, #32]
   3658c:	b	365fc <__gmpn_matrix22_mul@@Base+0x4d4>
   36590:	ldr	x27, [sp, #56]
   36594:	mov	x1, x25
   36598:	mov	x2, x22
   3659c:	mov	x3, x21
   365a0:	mov	x0, x27
   365a4:	bl	c2d0 <__gmpn_sub_n@plt>
   365a8:	mov	x0, xzr
   365ac:	mov	w26, wzr
   365b0:	mov	w8, #0x1                   	// #1
   365b4:	str	w8, [sp, #32]
   365b8:	b	365fc <__gmpn_matrix22_mul@@Base+0x4d4>
   365bc:	mov	x0, x25
   365c0:	mov	x1, x20
   365c4:	mov	x2, x25
   365c8:	mov	x3, x21
   365cc:	bl	c2d0 <__gmpn_sub_n@plt>
   365d0:	ldr	x27, [sp, #56]
   365d4:	mov	x1, x25
   365d8:	mov	x2, x22
   365dc:	mov	x3, x21
   365e0:	mov	x0, x27
   365e4:	str	xzr, [x25, x21, lsl #3]
   365e8:	bl	ca70 <__gmpn_add_n@plt>
   365ec:	mov	w8, #0x1                   	// #1
   365f0:	str	wzr, [sp, #32]
   365f4:	mov	w26, #0x1                   	// #1
   365f8:	str	w8, [sp, #12]
   365fc:	ldur	x3, [x29, #-40]
   36600:	cmp	x21, x23
   36604:	str	x0, [x27, x21, lsl #3]
   36608:	b.lt	36620 <__gmpn_matrix22_mul@@Base+0x4f8>  // b.tstop
   3660c:	ldur	x0, [x29, #-8]
   36610:	mov	x1, x22
   36614:	mov	x2, x21
   36618:	mov	x4, x23
   3661c:	b	36634 <__gmpn_matrix22_mul@@Base+0x50c>
   36620:	ldur	x0, [x29, #-8]
   36624:	mov	x1, x3
   36628:	mov	x2, x23
   3662c:	mov	x3, x22
   36630:	mov	x4, x21
   36634:	bl	ccd0 <__gmpn_mul@plt>
   36638:	ldr	x27, [sp, #40]
   3663c:	ldp	x1, x2, [x29, #-16]
   36640:	mov	x0, x22
   36644:	mov	x3, x27
   36648:	bl	ca70 <__gmpn_add_n@plt>
   3664c:	sub	x8, x23, #0x1
   36650:	str	x0, [x22, x27, lsl #3]
   36654:	add	x9, x8, #0x1
   36658:	cmp	x9, #0x1
   3665c:	b.lt	3667c <__gmpn_matrix22_mul@@Base+0x554>  // b.tstop
   36660:	lsl	x9, x8, #3
   36664:	ldr	x10, [x24, x9]
   36668:	ldr	x9, [x28, x9]
   3666c:	sub	x8, x8, #0x1
   36670:	cmp	x10, x9
   36674:	b.eq	36654 <__gmpn_matrix22_mul@@Base+0x52c>  // b.none
   36678:	b.ls	36698 <__gmpn_matrix22_mul@@Base+0x570>  // b.plast
   3667c:	ldur	x0, [x29, #-32]
   36680:	mov	x1, x24
   36684:	mov	x2, x28
   36688:	mov	x3, x23
   3668c:	bl	c2d0 <__gmpn_sub_n@plt>
   36690:	mov	w22, wzr
   36694:	b	366b0 <__gmpn_matrix22_mul@@Base+0x588>
   36698:	ldur	x0, [x29, #-32]
   3669c:	mov	x1, x28
   366a0:	mov	x2, x24
   366a4:	mov	x3, x23
   366a8:	bl	c2d0 <__gmpn_sub_n@plt>
   366ac:	mov	w22, #0x1                   	// #1
   366b0:	ldur	x24, [x29, #-24]
   366b4:	mov	w27, w26
   366b8:	cmp	x21, x23
   366bc:	b.lt	366dc <__gmpn_matrix22_mul@@Base+0x5b4>  // b.tstop
   366c0:	ldur	x28, [x29, #-32]
   366c4:	ldur	x0, [x29, #-8]
   366c8:	mov	x1, x20
   366cc:	mov	x2, x21
   366d0:	mov	x3, x28
   366d4:	mov	x4, x23
   366d8:	b	366f4 <__gmpn_matrix22_mul@@Base+0x5cc>
   366dc:	ldur	x28, [x29, #-32]
   366e0:	ldur	x0, [x29, #-8]
   366e4:	mov	x2, x23
   366e8:	mov	x3, x20
   366ec:	mov	x1, x28
   366f0:	mov	x4, x21
   366f4:	bl	ccd0 <__gmpn_mul@plt>
   366f8:	ldur	x8, [x29, #-8]
   366fc:	ldr	x9, [sp, #40]
   36700:	str	xzr, [x8, x9, lsl #3]
   36704:	cbz	w22, 36758 <__gmpn_matrix22_mul@@Base+0x630>
   36708:	ldr	x8, [sp, #56]
   3670c:	mov	x9, x23
   36710:	add	x8, x8, x21, lsl #3
   36714:	subs	x10, x9, #0x1
   36718:	b.lt	3673c <__gmpn_matrix22_mul@@Base+0x614>  // b.tstop
   3671c:	lsl	x9, x9, #3
   36720:	add	x11, x24, x9
   36724:	ldur	x11, [x11, #-8]
   36728:	ldr	x9, [x8, x9]
   3672c:	cmp	x11, x9
   36730:	mov	x9, x10
   36734:	b.eq	36714 <__gmpn_matrix22_mul@@Base+0x5ec>  // b.none
   36738:	b.ls	36798 <__gmpn_matrix22_mul@@Base+0x670>  // b.plast
   3673c:	mov	x0, x28
   36740:	mov	x1, x24
   36744:	mov	x2, x28
   36748:	mov	x3, x23
   3674c:	bl	c2d0 <__gmpn_sub_n@plt>
   36750:	mov	w26, wzr
   36754:	b	367b0 <__gmpn_matrix22_mul@@Base+0x688>
   36758:	mov	x0, x28
   3675c:	mov	x1, x28
   36760:	mov	x2, x24
   36764:	mov	x3, x23
   36768:	bl	ca70 <__gmpn_add_n@plt>
   3676c:	add	x24, x28, x23, lsl #3
   36770:	str	x0, [x24]
   36774:	cbz	x0, 367f4 <__gmpn_matrix22_mul@@Base+0x6cc>
   36778:	mov	x0, x20
   3677c:	cmp	x23, x21
   36780:	b.ge	36800 <__gmpn_matrix22_mul@@Base+0x6d8>  // b.tcont
   36784:	ldr	x4, [sp, #24]
   36788:	mov	x1, x25
   3678c:	mov	x2, x21
   36790:	mov	x3, x28
   36794:	b	36810 <__gmpn_matrix22_mul@@Base+0x6e8>
   36798:	mov	x0, x28
   3679c:	mov	x1, x28
   367a0:	mov	x2, x24
   367a4:	mov	x3, x23
   367a8:	bl	c2d0 <__gmpn_sub_n@plt>
   367ac:	mov	w26, #0x1                   	// #1
   367b0:	ldr	x8, [sp, #48]
   367b4:	add	x24, x28, x23, lsl #3
   367b8:	str	xzr, [x24]
   367bc:	mov	x0, x20
   367c0:	cmp	x8, x23
   367c4:	b.ge	367dc <__gmpn_matrix22_mul@@Base+0x6b4>  // b.tcont
   367c8:	mov	x1, x28
   367cc:	mov	x2, x23
   367d0:	mov	x3, x25
   367d4:	mov	x4, x8
   367d8:	b	367ec <__gmpn_matrix22_mul@@Base+0x6c4>
   367dc:	mov	x1, x25
   367e0:	mov	x2, x8
   367e4:	mov	x3, x28
   367e8:	mov	x4, x23
   367ec:	bl	ccd0 <__gmpn_mul@plt>
   367f0:	b	36834 <__gmpn_matrix22_mul@@Base+0x70c>
   367f4:	ldr	x8, [sp, #48]
   367f8:	mov	w26, wzr
   367fc:	b	367bc <__gmpn_matrix22_mul@@Base+0x694>
   36800:	ldr	x2, [sp, #24]
   36804:	mov	x1, x28
   36808:	mov	x3, x25
   3680c:	mov	x4, x21
   36810:	bl	ccd0 <__gmpn_mul@plt>
   36814:	ldr	x8, [x25, x21, lsl #3]
   36818:	cbz	x8, 36830 <__gmpn_matrix22_mul@@Base+0x708>
   3681c:	ldr	x3, [sp, #24]
   36820:	add	x0, x20, x21, lsl #3
   36824:	mov	x1, x0
   36828:	mov	x2, x28
   3682c:	bl	ca70 <__gmpn_add_n@plt>
   36830:	mov	w26, wzr
   36834:	ldur	x12, [x29, #-16]
   36838:	ldr	x3, [sp, #16]
   3683c:	ldr	x8, [sp, #40]
   36840:	cmp	w27, w26
   36844:	str	xzr, [x12, x8, lsl #3]
   36848:	b.ne	36860 <__gmpn_matrix22_mul@@Base+0x738>  // b.any
   3684c:	mov	x0, x20
   36850:	mov	x1, x20
   36854:	mov	x2, x12
   36858:	bl	ca70 <__gmpn_add_n@plt>
   3685c:	b	368a4 <__gmpn_matrix22_mul@@Base+0x77c>
   36860:	ldr	x9, [sp, #56]
   36864:	add	x8, x23, x21
   36868:	add	x9, x9, x8, lsl #4
   3686c:	add	x9, x9, #0x10
   36870:	add	x10, x8, #0x1
   36874:	cmp	x10, #0x1
   36878:	b.lt	36894 <__gmpn_matrix22_mul@@Base+0x76c>  // b.tstop
   3687c:	ldr	x10, [x9], #-8
   36880:	ldr	x11, [x20, x8, lsl #3]
   36884:	sub	x8, x8, #0x1
   36888:	cmp	x10, x11
   3688c:	b.eq	36870 <__gmpn_matrix22_mul@@Base+0x748>  // b.none
   36890:	b.ls	368cc <__gmpn_matrix22_mul@@Base+0x7a4>  // b.plast
   36894:	mov	x0, x20
   36898:	mov	x1, x12
   3689c:	mov	x2, x20
   368a0:	bl	c2d0 <__gmpn_sub_n@plt>
   368a4:	mov	w28, wzr
   368a8:	ldur	x2, [x29, #-40]
   368ac:	cbz	w26, 368e8 <__gmpn_matrix22_mul@@Base+0x7c0>
   368b0:	ldur	x24, [x29, #-32]
   368b4:	mov	x3, x23
   368b8:	mov	x0, x24
   368bc:	mov	x1, x24
   368c0:	bl	ca70 <__gmpn_add_n@plt>
   368c4:	str	x0, [x24, x23, lsl #3]
   368c8:	b	36980 <__gmpn_matrix22_mul@@Base+0x858>
   368cc:	mov	x0, x20
   368d0:	mov	x1, x20
   368d4:	mov	x2, x12
   368d8:	bl	c2d0 <__gmpn_sub_n@plt>
   368dc:	mov	w28, #0x1                   	// #1
   368e0:	ldur	x2, [x29, #-40]
   368e4:	cbnz	w26, 368b0 <__gmpn_matrix22_mul@@Base+0x788>
   368e8:	ldr	x8, [x24]
   368ec:	cbz	x8, 3691c <__gmpn_matrix22_mul@@Base+0x7f4>
   368f0:	ldur	x24, [x29, #-32]
   368f4:	mov	x3, x23
   368f8:	mov	x0, x24
   368fc:	mov	x1, x24
   36900:	bl	c2d0 <__gmpn_sub_n@plt>
   36904:	lsl	x8, x23, #3
   36908:	ldr	x9, [x24, x8]
   3690c:	mov	w26, wzr
   36910:	sub	x9, x9, x0
   36914:	str	x9, [x24, x8]
   36918:	b	36980 <__gmpn_matrix22_mul@@Base+0x858>
   3691c:	ldr	x8, [sp, #56]
   36920:	mov	x9, x23
   36924:	add	x8, x8, x21, lsl #3
   36928:	subs	x10, x9, #0x1
   3692c:	b.lt	36950 <__gmpn_matrix22_mul@@Base+0x828>  // b.tstop
   36930:	lsl	x9, x9, #3
   36934:	ldr	x11, [x8, x9]
   36938:	add	x9, x2, x9
   3693c:	ldur	x9, [x9, #-8]
   36940:	cmp	x11, x9
   36944:	mov	x9, x10
   36948:	b.eq	36928 <__gmpn_matrix22_mul@@Base+0x800>  // b.none
   3694c:	b.ls	36968 <__gmpn_matrix22_mul@@Base+0x840>  // b.plast
   36950:	ldur	x0, [x29, #-32]
   36954:	mov	x3, x23
   36958:	mov	x1, x0
   3695c:	bl	c2d0 <__gmpn_sub_n@plt>
   36960:	mov	w26, wzr
   36964:	b	36980 <__gmpn_matrix22_mul@@Base+0x858>
   36968:	ldur	x0, [x29, #-32]
   3696c:	mov	x1, x2
   36970:	mov	x3, x23
   36974:	mov	x2, x0
   36978:	bl	c2d0 <__gmpn_sub_n@plt>
   3697c:	mov	w26, #0x1                   	// #1
   36980:	ldur	x0, [x29, #-16]
   36984:	cmp	x23, x21
   36988:	b.ge	369a0 <__gmpn_matrix22_mul@@Base+0x878>  // b.tcont
   3698c:	ldur	x3, [x29, #-32]
   36990:	ldr	x4, [sp, #24]
   36994:	mov	x1, x19
   36998:	mov	x2, x21
   3699c:	b	369b0 <__gmpn_matrix22_mul@@Base+0x888>
   369a0:	ldur	x1, [x29, #-32]
   369a4:	ldr	x2, [sp, #24]
   369a8:	mov	x3, x19
   369ac:	mov	x4, x21
   369b0:	bl	ccd0 <__gmpn_mul@plt>
   369b4:	ldr	w8, [sp, #12]
   369b8:	mov	x0, x25
   369bc:	eor	w22, w8, w22
   369c0:	cbz	w27, 369d8 <__gmpn_matrix22_mul@@Base+0x8b0>
   369c4:	mov	x1, x19
   369c8:	mov	x2, x25
   369cc:	mov	x3, x21
   369d0:	bl	c2d0 <__gmpn_sub_n@plt>
   369d4:	b	369f8 <__gmpn_matrix22_mul@@Base+0x8d0>
   369d8:	mov	x1, x25
   369dc:	mov	x2, x19
   369e0:	mov	x3, x21
   369e4:	bl	ca70 <__gmpn_add_n@plt>
   369e8:	lsl	x8, x21, #3
   369ec:	ldr	x9, [x25, x8]
   369f0:	add	x9, x9, x0
   369f4:	str	x9, [x25, x8]
   369f8:	ldr	x8, [sp, #48]
   369fc:	ldur	x12, [x29, #-16]
   36a00:	eor	w24, w22, #0x1
   36a04:	cmp	w28, w26
   36a08:	add	x22, x8, x23
   36a0c:	b.ne	36a48 <__gmpn_matrix22_mul@@Base+0x920>  // b.any
   36a10:	mov	x0, x19
   36a14:	mov	x1, x20
   36a18:	mov	x2, x12
   36a1c:	mov	x3, x22
   36a20:	bl	ca70 <__gmpn_add_n@plt>
   36a24:	mov	w26, w28
   36a28:	cmp	w28, w24
   36a2c:	b.ne	36aa0 <__gmpn_matrix22_mul@@Base+0x978>  // b.any
   36a30:	ldur	x2, [x29, #-8]
   36a34:	mov	x0, x20
   36a38:	mov	x1, x20
   36a3c:	mov	x3, x22
   36a40:	bl	ca70 <__gmpn_add_n@plt>
   36a44:	b	36b3c <__gmpn_matrix22_mul@@Base+0xa14>
   36a48:	ldr	x9, [sp, #56]
   36a4c:	add	x8, x23, x21
   36a50:	add	x9, x9, x8, lsl #4
   36a54:	add	x9, x9, #0x10
   36a58:	add	x10, x8, #0x1
   36a5c:	cmp	x10, #0x1
   36a60:	b.lt	36a7c <__gmpn_matrix22_mul@@Base+0x954>  // b.tstop
   36a64:	ldr	x10, [x20, x8, lsl #3]
   36a68:	ldr	x11, [x9], #-8
   36a6c:	sub	x8, x8, #0x1
   36a70:	cmp	x10, x11
   36a74:	b.eq	36a58 <__gmpn_matrix22_mul@@Base+0x930>  // b.none
   36a78:	b.ls	36af8 <__gmpn_matrix22_mul@@Base+0x9d0>  // b.plast
   36a7c:	mov	x0, x19
   36a80:	mov	x1, x20
   36a84:	mov	x2, x12
   36a88:	mov	x3, x22
   36a8c:	bl	c2d0 <__gmpn_sub_n@plt>
   36a90:	mov	w8, wzr
   36a94:	eor	w26, wzr, w28
   36a98:	cmp	w28, w24
   36a9c:	b.eq	36a30 <__gmpn_matrix22_mul@@Base+0x908>  // b.none
   36aa0:	ldr	x10, [sp, #56]
   36aa4:	add	x8, x23, x21
   36aa8:	mov	w9, #0x18                  	// #24
   36aac:	madd	x9, x8, x9, x10
   36ab0:	add	x9, x9, #0x18
   36ab4:	add	x10, x8, #0x1
   36ab8:	cmp	x10, #0x1
   36abc:	b.lt	36ad8 <__gmpn_matrix22_mul@@Base+0x9b0>  // b.tstop
   36ac0:	ldr	x10, [x20, x8, lsl #3]
   36ac4:	ldr	x11, [x9], #-8
   36ac8:	sub	x8, x8, #0x1
   36acc:	cmp	x10, x11
   36ad0:	b.eq	36ab4 <__gmpn_matrix22_mul@@Base+0x98c>  // b.none
   36ad4:	b.ls	36b20 <__gmpn_matrix22_mul@@Base+0x9f8>  // b.plast
   36ad8:	ldur	x2, [x29, #-8]
   36adc:	mov	x0, x20
   36ae0:	mov	x1, x20
   36ae4:	mov	x3, x22
   36ae8:	bl	c2d0 <__gmpn_sub_n@plt>
   36aec:	mov	w8, wzr
   36af0:	eor	w24, wzr, w28
   36af4:	b	36b3c <__gmpn_matrix22_mul@@Base+0xa14>
   36af8:	mov	x0, x19
   36afc:	mov	x1, x12
   36b00:	mov	x2, x20
   36b04:	mov	x3, x22
   36b08:	bl	c2d0 <__gmpn_sub_n@plt>
   36b0c:	mov	w8, #0x1                   	// #1
   36b10:	eor	w26, w8, w28
   36b14:	cmp	w28, w24
   36b18:	b.eq	36a30 <__gmpn_matrix22_mul@@Base+0x908>  // b.none
   36b1c:	b	36aa0 <__gmpn_matrix22_mul@@Base+0x978>
   36b20:	ldur	x1, [x29, #-8]
   36b24:	mov	x0, x20
   36b28:	mov	x2, x20
   36b2c:	mov	x3, x22
   36b30:	bl	c2d0 <__gmpn_sub_n@plt>
   36b34:	mov	w8, #0x1                   	// #1
   36b38:	eor	w24, w8, w28
   36b3c:	ldr	x8, [sp, #48]
   36b40:	ldp	x28, x0, [x29, #-24]
   36b44:	cmp	x8, x23
   36b48:	b.ge	36b60 <__gmpn_matrix22_mul@@Base+0xa38>  // b.tcont
   36b4c:	ldr	x3, [sp, #56]
   36b50:	mov	x1, x28
   36b54:	mov	x2, x23
   36b58:	mov	x4, x8
   36b5c:	b	36b70 <__gmpn_matrix22_mul@@Base+0xa48>
   36b60:	ldr	x1, [sp, #56]
   36b64:	mov	x2, x8
   36b68:	mov	x3, x28
   36b6c:	mov	x4, x23
   36b70:	bl	ccd0 <__gmpn_mul@plt>
   36b74:	ldur	x0, [x29, #-32]
   36b78:	ldur	x1, [x29, #-48]
   36b7c:	mov	x2, x28
   36b80:	mov	x3, x23
   36b84:	mov	x28, x0
   36b88:	bl	ca70 <__gmpn_add_n@plt>
   36b8c:	cmp	x21, x23
   36b90:	str	x0, [x28, x23, lsl #3]
   36b94:	b.ge	36bb0 <__gmpn_matrix22_mul@@Base+0xa88>  // b.tcont
   36b98:	ldur	x0, [x29, #-8]
   36b9c:	ldr	x2, [sp, #24]
   36ba0:	ldr	x4, [sp, #48]
   36ba4:	mov	x1, x28
   36ba8:	mov	x3, x25
   36bac:	b	36bc4 <__gmpn_matrix22_mul@@Base+0xa9c>
   36bb0:	ldur	x0, [x29, #-8]
   36bb4:	ldr	x2, [sp, #48]
   36bb8:	ldr	x4, [sp, #24]
   36bbc:	mov	x1, x25
   36bc0:	mov	x3, x28
   36bc4:	bl	ccd0 <__gmpn_mul@plt>
   36bc8:	ldr	w8, [sp, #32]
   36bcc:	ldur	x2, [x29, #-16]
   36bd0:	cmp	w24, w8
   36bd4:	b.ne	36c34 <__gmpn_matrix22_mul@@Base+0xb0c>  // b.any
   36bd8:	mov	x0, x25
   36bdc:	mov	x1, x20
   36be0:	mov	x3, x22
   36be4:	bl	ca70 <__gmpn_add_n@plt>
   36be8:	mov	x0, x20
   36bec:	cbz	w24, 36c90 <__gmpn_matrix22_mul@@Base+0xb68>
   36bf0:	ldur	x1, [x29, #-8]
   36bf4:	mov	x2, x20
   36bf8:	mov	x3, x22
   36bfc:	bl	ca70 <__gmpn_add_n@plt>
   36c00:	mov	x0, x19
   36c04:	cbz	w26, 36ca8 <__gmpn_matrix22_mul@@Base+0xb80>
   36c08:	ldur	x1, [x29, #-8]
   36c0c:	mov	x2, x19
   36c10:	mov	x3, x22
   36c14:	ldp	x20, x19, [sp, #192]
   36c18:	ldp	x22, x21, [sp, #176]
   36c1c:	ldp	x24, x23, [sp, #160]
   36c20:	ldp	x26, x25, [sp, #144]
   36c24:	ldp	x28, x27, [sp, #128]
   36c28:	ldp	x29, x30, [sp, #112]
   36c2c:	add	sp, sp, #0xd0
   36c30:	b	ca70 <__gmpn_add_n@plt>
   36c34:	ldr	x9, [sp, #56]
   36c38:	add	x8, x23, x21
   36c3c:	add	x9, x9, x8, lsl #4
   36c40:	add	x9, x9, #0x10
   36c44:	add	x10, x8, #0x1
   36c48:	cmp	x10, #0x1
   36c4c:	b.lt	36c68 <__gmpn_matrix22_mul@@Base+0xb40>  // b.tstop
   36c50:	ldr	x10, [x20, x8, lsl #3]
   36c54:	ldr	x11, [x9], #-8
   36c58:	sub	x8, x8, #0x1
   36c5c:	cmp	x10, x11
   36c60:	b.eq	36c44 <__gmpn_matrix22_mul@@Base+0xb1c>  // b.none
   36c64:	b.ls	36c74 <__gmpn_matrix22_mul@@Base+0xb4c>  // b.plast
   36c68:	mov	x0, x25
   36c6c:	mov	x1, x20
   36c70:	b	36c80 <__gmpn_matrix22_mul@@Base+0xb58>
   36c74:	mov	x0, x25
   36c78:	mov	x1, x2
   36c7c:	mov	x2, x20
   36c80:	mov	x3, x22
   36c84:	bl	c2d0 <__gmpn_sub_n@plt>
   36c88:	mov	x0, x20
   36c8c:	cbnz	w24, 36bf0 <__gmpn_matrix22_mul@@Base+0xac8>
   36c90:	ldur	x1, [x29, #-8]
   36c94:	mov	x2, x20
   36c98:	mov	x3, x22
   36c9c:	bl	c2d0 <__gmpn_sub_n@plt>
   36ca0:	mov	x0, x19
   36ca4:	cbnz	w26, 36c08 <__gmpn_matrix22_mul@@Base+0xae0>
   36ca8:	ldur	x1, [x29, #-8]
   36cac:	mov	x2, x19
   36cb0:	mov	x3, x22
   36cb4:	ldp	x20, x19, [sp, #192]
   36cb8:	ldp	x22, x21, [sp, #176]
   36cbc:	ldp	x24, x23, [sp, #160]
   36cc0:	ldp	x26, x25, [sp, #144]
   36cc4:	ldp	x28, x27, [sp, #128]
   36cc8:	ldp	x29, x30, [sp, #112]
   36ccc:	add	sp, sp, #0xd0
   36cd0:	b	c2d0 <__gmpn_sub_n@plt>

0000000000036cd4 <__gmpn_matrix22_mul1_inverse_vector@@Base>:
   36cd4:	stp	x29, x30, [sp, #-64]!
   36cd8:	stp	x22, x21, [sp, #32]
   36cdc:	stp	x20, x19, [sp, #48]
   36ce0:	mov	x20, x3
   36ce4:	ldr	x3, [x0, #24]
   36ce8:	str	x23, [sp, #16]
   36cec:	mov	x21, x2
   36cf0:	mov	x22, x0
   36cf4:	mov	x23, x1
   36cf8:	mov	x0, x1
   36cfc:	mov	x1, x2
   36d00:	mov	x2, x4
   36d04:	mov	x29, sp
   36d08:	mov	x19, x4
   36d0c:	bl	d490 <__gmpn_mul_1@plt>
   36d10:	ldr	x3, [x22, #8]
   36d14:	mov	x0, x23
   36d18:	mov	x1, x20
   36d1c:	mov	x2, x19
   36d20:	bl	c9e0 <__gmpn_submul_1@plt>
   36d24:	ldr	x3, [x22]
   36d28:	mov	x0, x20
   36d2c:	mov	x1, x20
   36d30:	mov	x2, x19
   36d34:	bl	d490 <__gmpn_mul_1@plt>
   36d38:	ldr	x3, [x22, #16]
   36d3c:	mov	x0, x20
   36d40:	mov	x1, x21
   36d44:	mov	x2, x19
   36d48:	bl	c9e0 <__gmpn_submul_1@plt>
   36d4c:	lsl	x8, x19, #3
   36d50:	sub	x8, x8, #0x8
   36d54:	ldr	x9, [x23, x8]
   36d58:	ldr	x8, [x20, x8]
   36d5c:	ldp	x22, x21, [sp, #32]
   36d60:	ldr	x23, [sp, #16]
   36d64:	orr	x8, x8, x9
   36d68:	cmp	x8, #0x0
   36d6c:	cset	w8, eq  // eq = none
   36d70:	sub	x0, x19, x8
   36d74:	ldp	x20, x19, [sp, #48]
   36d78:	ldp	x29, x30, [sp], #64
   36d7c:	ret

0000000000036d80 <__gmpn_hgcd_matrix_init@@Base>:
   36d80:	stp	x29, x30, [sp, #-48]!
   36d84:	add	x8, x1, #0x1
   36d88:	add	x9, x1, #0x2
   36d8c:	cmp	x8, #0x0
   36d90:	csinc	x8, x9, x1, lt  // lt = tstop
   36d94:	asr	x8, x8, #1
   36d98:	str	x21, [sp, #16]
   36d9c:	stp	x20, x19, [sp, #32]
   36da0:	mov	x19, x2
   36da4:	mov	x20, x0
   36da8:	mov	w10, #0x1                   	// #1
   36dac:	adds	x21, x8, #0x1
   36db0:	mov	x29, sp
   36db4:	stp	x21, x10, [x0]
   36db8:	b.cs	36dd0 <__gmpn_hgcd_matrix_init@@Base+0x50>  // b.hs, b.nlast
   36dbc:	lsl	x8, x8, #5
   36dc0:	add	x2, x8, #0x20
   36dc4:	mov	x0, x19
   36dc8:	mov	w1, wzr
   36dcc:	bl	c5f0 <memset@plt>
   36dd0:	add	x8, x19, x21, lsl #3
   36dd4:	mov	w10, #0x18                  	// #24
   36dd8:	stp	x19, x8, [x20, #16]
   36ddc:	mul	x8, x21, x10
   36de0:	add	x9, x19, x21, lsl #4
   36de4:	mov	w11, #0x1                   	// #1
   36de8:	add	x10, x19, x8
   36dec:	stp	x9, x10, [x20, #32]
   36df0:	str	x11, [x19, x8]
   36df4:	str	x11, [x19]
   36df8:	ldp	x20, x19, [sp, #32]
   36dfc:	ldr	x21, [sp, #16]
   36e00:	ldp	x29, x30, [sp], #48
   36e04:	ret

0000000000036e08 <__gmpn_hgcd_matrix_update_q@@Base>:
   36e08:	sub	sp, sp, #0x80
   36e0c:	stp	x24, x23, [sp, #80]
   36e10:	stp	x22, x21, [sp, #96]
   36e14:	mov	x23, x1
   36e18:	cmp	x2, #0x1
   36e1c:	mov	x22, x0
   36e20:	stp	x29, x30, [sp, #32]
   36e24:	stp	x28, x27, [sp, #48]
   36e28:	stp	x26, x25, [sp, #64]
   36e2c:	stp	x20, x19, [sp, #112]
   36e30:	add	x29, sp, #0x20
   36e34:	b.ne	36eb4 <__gmpn_hgcd_matrix_update_q@@Base+0xac>  // b.any
   36e38:	ldr	x19, [x23]
   36e3c:	ldr	x2, [x22, #8]!
   36e40:	mov	w8, w3
   36e44:	mov	w9, #0x1                   	// #1
   36e48:	lsl	x21, x8, #3
   36e4c:	sub	w8, w9, w3
   36e50:	lsl	x20, x8, #3
   36e54:	add	x23, x22, #0x8
   36e58:	ldr	x0, [x23, x21]
   36e5c:	ldr	x1, [x23, x20]
   36e60:	mov	x3, x19
   36e64:	bl	d400 <__gmpn_addmul_1@plt>
   36e68:	add	x24, x22, #0x18
   36e6c:	ldr	x8, [x24, x21]
   36e70:	ldr	x1, [x24, x20]
   36e74:	ldr	x2, [x22]
   36e78:	mov	x20, x0
   36e7c:	mov	x0, x8
   36e80:	mov	x3, x19
   36e84:	bl	d400 <__gmpn_addmul_1@plt>
   36e88:	ldr	x8, [x23, x21]
   36e8c:	ldr	x9, [x22]
   36e90:	str	x20, [x8, x9, lsl #3]
   36e94:	ldr	x8, [x24, x21]
   36e98:	ldr	x9, [x22]
   36e9c:	str	x0, [x8, x9, lsl #3]
   36ea0:	ldr	x8, [x22]
   36ea4:	orr	x9, x0, x20
   36ea8:	cmp	x9, #0x0
   36eac:	cinc	x8, x8, ne  // ne = any
   36eb0:	b	37204 <__gmpn_hgcd_matrix_update_q@@Base+0x3fc>
   36eb4:	mov	w8, #0x1                   	// #1
   36eb8:	sub	w8, w8, w3
   36ebc:	mov	x14, x22
   36ec0:	add	x10, x22, w8, uxtw #3
   36ec4:	ldr	x9, [x14, #8]!
   36ec8:	ldr	x8, [x10, #16]
   36ecc:	mov	x21, x4
   36ed0:	mov	x19, x2
   36ed4:	add	x26, x10, #0x20
   36ed8:	sub	x10, x8, #0x8
   36edc:	lsl	x11, x9, #3
   36ee0:	mov	x12, x9
   36ee4:	add	x28, x19, x12
   36ee8:	mov	x20, x12
   36eec:	cmp	x28, x9
   36ef0:	mov	x27, x11
   36ef4:	b.le	36f18 <__gmpn_hgcd_matrix_update_q@@Base+0x110>
   36ef8:	ldr	x11, [x10, x20, lsl #3]
   36efc:	cbnz	x11, 36f18 <__gmpn_hgcd_matrix_update_q@@Base+0x110>
   36f00:	ldr	x11, [x26]
   36f04:	sub	x12, x20, #0x1
   36f08:	add	x11, x11, x20, lsl #3
   36f0c:	ldur	x13, [x11, #-8]
   36f10:	sub	x11, x27, #0x8
   36f14:	cbz	x13, 36ee4 <__gmpn_hgcd_matrix_update_q@@Base+0xdc>
   36f18:	cmp	x20, x19
   36f1c:	mov	w24, w3
   36f20:	mov	x0, x21
   36f24:	stur	x14, [x29, #-8]
   36f28:	b.ge	36f40 <__gmpn_hgcd_matrix_update_q@@Base+0x138>  // b.tcont
   36f2c:	mov	x1, x23
   36f30:	mov	x2, x19
   36f34:	mov	x3, x8
   36f38:	mov	x4, x20
   36f3c:	b	36f50 <__gmpn_hgcd_matrix_update_q@@Base+0x148>
   36f40:	mov	x1, x8
   36f44:	mov	x2, x20
   36f48:	mov	x3, x23
   36f4c:	mov	x4, x19
   36f50:	bl	ccd0 <__gmpn_mul@plt>
   36f54:	str	x24, [sp, #16]
   36f58:	add	x8, x22, x24, lsl #3
   36f5c:	ldr	x24, [x8, #16]!
   36f60:	ldr	x25, [x22, #8]
   36f64:	str	x8, [sp, #8]
   36f68:	cbz	x25, 36fa8 <__gmpn_hgcd_matrix_update_q@@Base+0x1a0>
   36f6c:	mov	x0, x24
   36f70:	mov	x1, x21
   36f74:	mov	x2, x24
   36f78:	mov	x3, x25
   36f7c:	bl	ca70 <__gmpn_add_n@plt>
   36f80:	cbz	x0, 36fa8 <__gmpn_hgcd_matrix_update_q@@Base+0x1a0>
   36f84:	mov	w10, #0x1                   	// #1
   36f88:	cmp	x25, x28
   36f8c:	b.ge	37054 <__gmpn_hgcd_matrix_update_q@@Base+0x24c>  // b.tcont
   36f90:	lsl	x8, x25, #3
   36f94:	ldr	x9, [x21, x8]
   36f98:	add	x25, x25, #0x1
   36f9c:	adds	x9, x9, #0x1
   36fa0:	str	x9, [x24, x8]
   36fa4:	b.cs	36f88 <__gmpn_hgcd_matrix_update_q@@Base+0x180>  // b.hs, b.nlast
   36fa8:	cmp	x24, x21
   36fac:	mov	x10, xzr
   36fb0:	b.eq	37054 <__gmpn_hgcd_matrix_update_q@@Base+0x24c>  // b.none
   36fb4:	cmp	x28, x25
   36fb8:	b.le	37054 <__gmpn_hgcd_matrix_update_q@@Base+0x24c>
   36fbc:	sub	x10, x19, x25
   36fc0:	add	x8, x10, x20
   36fc4:	cmp	x8, #0x4
   36fc8:	b.cc	37038 <__gmpn_hgcd_matrix_update_q@@Base+0x230>  // b.lo, b.ul, b.last
   36fcc:	lsl	x9, x19, #3
   36fd0:	lsl	x11, x25, #3
   36fd4:	add	x13, x21, x9
   36fd8:	add	x12, x24, x11
   36fdc:	add	x13, x13, x27
   36fe0:	cmp	x12, x13
   36fe4:	b.cs	36ffc <__gmpn_hgcd_matrix_update_q@@Base+0x1f4>  // b.hs, b.nlast
   36fe8:	add	x9, x24, x9
   36fec:	add	x9, x9, x27
   36ff0:	add	x12, x21, x11
   36ff4:	cmp	x12, x9
   36ff8:	b.cc	37038 <__gmpn_hgcd_matrix_update_q@@Base+0x230>  // b.lo, b.ul, b.last
   36ffc:	and	x9, x8, #0xfffffffffffffffc
   37000:	add	x11, x11, #0x10
   37004:	add	x12, x10, x20
   37008:	add	x25, x25, x9
   3700c:	add	x10, x21, x11
   37010:	add	x11, x24, x11
   37014:	and	x12, x12, #0xfffffffffffffffc
   37018:	ldp	q0, q1, [x10, #-16]
   3701c:	add	x10, x10, #0x20
   37020:	subs	x12, x12, #0x4
   37024:	stp	q0, q1, [x11, #-16]
   37028:	add	x11, x11, #0x20
   3702c:	b.ne	37018 <__gmpn_hgcd_matrix_update_q@@Base+0x210>  // b.any
   37030:	cmp	x8, x9
   37034:	b.eq	37050 <__gmpn_hgcd_matrix_update_q@@Base+0x248>  // b.none
   37038:	lsl	x8, x25, #3
   3703c:	ldr	x9, [x21, x8]
   37040:	add	x25, x25, #0x1
   37044:	cmp	x28, x25
   37048:	str	x9, [x24, x8]
   3704c:	b.ne	37038 <__gmpn_hgcd_matrix_update_q@@Base+0x230>  // b.any
   37050:	mov	x10, xzr
   37054:	ldr	x3, [x26]
   37058:	mov	x25, x10
   3705c:	mov	x0, x21
   37060:	cmp	x20, x19
   37064:	b.ge	37078 <__gmpn_hgcd_matrix_update_q@@Base+0x270>  // b.tcont
   37068:	mov	x1, x23
   3706c:	mov	x2, x19
   37070:	mov	x4, x20
   37074:	b	37088 <__gmpn_hgcd_matrix_update_q@@Base+0x280>
   37078:	mov	x1, x3
   3707c:	mov	x2, x20
   37080:	mov	x3, x23
   37084:	mov	x4, x19
   37088:	bl	ccd0 <__gmpn_mul@plt>
   3708c:	ldr	x8, [sp, #16]
   37090:	add	x24, x22, x8, lsl #3
   37094:	ldr	x23, [x24, #32]!
   37098:	ldr	x22, [x22, #8]
   3709c:	cbz	x22, 370dc <__gmpn_hgcd_matrix_update_q@@Base+0x2d4>
   370a0:	mov	x0, x23
   370a4:	mov	x1, x21
   370a8:	mov	x2, x23
   370ac:	mov	x3, x22
   370b0:	bl	ca70 <__gmpn_add_n@plt>
   370b4:	cbz	x0, 370dc <__gmpn_hgcd_matrix_update_q@@Base+0x2d4>
   370b8:	mov	w8, #0x1                   	// #1
   370bc:	cmp	x22, x28
   370c0:	b.ge	3719c <__gmpn_hgcd_matrix_update_q@@Base+0x394>  // b.tcont
   370c4:	lsl	x9, x22, #3
   370c8:	ldr	x10, [x21, x9]
   370cc:	add	x22, x22, #0x1
   370d0:	adds	x10, x10, #0x1
   370d4:	str	x10, [x23, x9]
   370d8:	b.cs	370bc <__gmpn_hgcd_matrix_update_q@@Base+0x2b4>  // b.hs, b.nlast
   370dc:	cmp	x23, x21
   370e0:	mov	x8, xzr
   370e4:	b.eq	3719c <__gmpn_hgcd_matrix_update_q@@Base+0x394>  // b.none
   370e8:	ldr	x14, [sp, #8]
   370ec:	cmp	x28, x22
   370f0:	b.le	371a0 <__gmpn_hgcd_matrix_update_q@@Base+0x398>
   370f4:	sub	x10, x19, x22
   370f8:	add	x8, x10, x20
   370fc:	cmp	x8, #0x4
   37100:	b.cc	37170 <__gmpn_hgcd_matrix_update_q@@Base+0x368>  // b.lo, b.ul, b.last
   37104:	lsl	x9, x19, #3
   37108:	lsl	x11, x22, #3
   3710c:	add	x13, x21, x9
   37110:	add	x12, x23, x11
   37114:	add	x13, x13, x27
   37118:	cmp	x12, x13
   3711c:	b.cs	37134 <__gmpn_hgcd_matrix_update_q@@Base+0x32c>  // b.hs, b.nlast
   37120:	add	x9, x23, x9
   37124:	add	x9, x9, x27
   37128:	add	x12, x21, x11
   3712c:	cmp	x12, x9
   37130:	b.cc	37170 <__gmpn_hgcd_matrix_update_q@@Base+0x368>  // b.lo, b.ul, b.last
   37134:	and	x9, x8, #0xfffffffffffffffc
   37138:	add	x11, x11, #0x10
   3713c:	add	x12, x10, x20
   37140:	add	x22, x22, x9
   37144:	add	x10, x21, x11
   37148:	add	x11, x23, x11
   3714c:	and	x12, x12, #0xfffffffffffffffc
   37150:	ldp	q0, q1, [x10, #-16]
   37154:	add	x10, x10, #0x20
   37158:	subs	x12, x12, #0x4
   3715c:	stp	q0, q1, [x11, #-16]
   37160:	add	x11, x11, #0x20
   37164:	b.ne	37150 <__gmpn_hgcd_matrix_update_q@@Base+0x348>  // b.any
   37168:	cmp	x8, x9
   3716c:	b.eq	37194 <__gmpn_hgcd_matrix_update_q@@Base+0x38c>  // b.none
   37170:	lsl	x10, x22, #3
   37174:	sub	x8, x22, x19
   37178:	add	x9, x23, x10
   3717c:	add	x10, x21, x10
   37180:	ldr	x11, [x10], #8
   37184:	add	x8, x8, #0x1
   37188:	cmp	x20, x8
   3718c:	str	x11, [x9], #8
   37190:	b.ne	37180 <__gmpn_hgcd_matrix_update_q@@Base+0x378>  // b.any
   37194:	mov	x8, xzr
   37198:	b	371a0 <__gmpn_hgcd_matrix_update_q@@Base+0x398>
   3719c:	ldr	x14, [sp, #8]
   371a0:	ldr	x9, [x14]
   371a4:	orr	x10, x8, x25
   371a8:	cbz	x10, 371cc <__gmpn_hgcd_matrix_update_q@@Base+0x3c4>
   371ac:	lsl	x10, x19, #3
   371b0:	add	x9, x9, x10
   371b4:	str	x25, [x9, x27]
   371b8:	ldr	x9, [x24]
   371bc:	add	x9, x9, x10
   371c0:	str	x8, [x9, x20, lsl #3]
   371c4:	mov	w8, #0x1                   	// #1
   371c8:	b	371f8 <__gmpn_hgcd_matrix_update_q@@Base+0x3f0>
   371cc:	ldr	x10, [x24]
   371d0:	lsl	x8, x19, #3
   371d4:	add	x9, x9, x8
   371d8:	add	x9, x9, x27
   371dc:	add	x8, x10, x8
   371e0:	add	x8, x8, x20, lsl #3
   371e4:	ldur	x9, [x9, #-8]
   371e8:	ldur	x8, [x8, #-8]
   371ec:	orr	x8, x8, x9
   371f0:	cmp	x8, #0x0
   371f4:	csetm	x8, eq  // eq = none
   371f8:	ldur	x22, [x29, #-8]
   371fc:	add	x8, x8, x19
   37200:	add	x8, x8, x20
   37204:	str	x8, [x22]
   37208:	ldp	x20, x19, [sp, #112]
   3720c:	ldp	x22, x21, [sp, #96]
   37210:	ldp	x24, x23, [sp, #80]
   37214:	ldp	x26, x25, [sp, #64]
   37218:	ldp	x28, x27, [sp, #48]
   3721c:	ldp	x29, x30, [sp, #32]
   37220:	add	sp, sp, #0x80
   37224:	ret

0000000000037228 <__gmpn_hgcd_matrix_mul_1@@Base>:
   37228:	stp	x29, x30, [sp, #-48]!
   3722c:	stp	x22, x21, [sp, #16]
   37230:	stp	x20, x19, [sp, #32]
   37234:	mov	x19, x2
   37238:	ldp	x2, x8, [x0, #8]
   3723c:	mov	x20, x0
   37240:	mov	x21, x1
   37244:	mov	x0, x19
   37248:	mov	x1, x8
   3724c:	mov	x29, sp
   37250:	bl	ca50 <__gmpn_copyi@plt>
   37254:	ldp	x1, x3, [x20, #16]
   37258:	ldr	x4, [x20, #8]
   3725c:	mov	x0, x21
   37260:	mov	x2, x19
   37264:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   37268:	ldr	x1, [x20, #32]
   3726c:	ldr	x2, [x20, #8]
   37270:	mov	x22, x0
   37274:	mov	x0, x19
   37278:	bl	ca50 <__gmpn_copyi@plt>
   3727c:	ldp	x1, x3, [x20, #32]
   37280:	ldr	x4, [x20, #8]
   37284:	mov	x0, x21
   37288:	mov	x2, x19
   3728c:	bl	d440 <__gmpn_hgcd_mul_matrix1_vector@plt>
   37290:	cmp	x22, x0
   37294:	csel	x8, x22, x0, gt
   37298:	str	x8, [x20, #8]
   3729c:	ldp	x20, x19, [sp, #32]
   372a0:	ldp	x22, x21, [sp, #16]
   372a4:	ldp	x29, x30, [sp], #48
   372a8:	ret

00000000000372ac <__gmpn_hgcd_matrix_mul@@Base>:
   372ac:	sub	sp, sp, #0x40
   372b0:	stp	x29, x30, [sp, #32]
   372b4:	stp	x20, x19, [sp, #48]
   372b8:	mov	x20, x1
   372bc:	mov	x19, x0
   372c0:	ldp	x1, x8, [x0, #24]
   372c4:	ldp	x10, x5, [x20, #8]
   372c8:	ldr	x3, [x0, #40]
   372cc:	ldr	x0, [x0, #16]
   372d0:	ldr	x4, [x19, #8]
   372d4:	ldp	x6, x7, [x20, #24]
   372d8:	ldr	x9, [x20, #40]
   372dc:	stp	x10, x2, [sp, #8]
   372e0:	mov	x2, x8
   372e4:	add	x29, sp, #0x20
   372e8:	str	x9, [sp]
   372ec:	bl	c010 <__gmpn_matrix22_mul@plt>
   372f0:	ldr	x8, [x20, #8]
   372f4:	ldp	x9, x10, [x19, #8]
   372f8:	ldp	x11, x12, [x19, #24]
   372fc:	ldr	x13, [x19, #40]
   37300:	add	x8, x8, x9
   37304:	lsl	x9, x8, #3
   37308:	ldr	x14, [x10, x9]
   3730c:	ldr	x15, [x11, x9]
   37310:	ldr	x16, [x12, x9]
   37314:	ldr	x9, [x13, x9]
   37318:	orr	x14, x15, x14
   3731c:	orr	x14, x14, x16
   37320:	orr	x9, x14, x9
   37324:	cmp	x9, #0x0
   37328:	cset	w9, eq  // eq = none
   3732c:	sub	x8, x8, x9
   37330:	lsl	x9, x8, #3
   37334:	ldr	x14, [x10, x9]
   37338:	ldr	x15, [x11, x9]
   3733c:	ldr	x16, [x12, x9]
   37340:	ldr	x9, [x13, x9]
   37344:	orr	x14, x15, x14
   37348:	orr	x14, x14, x16
   3734c:	orr	x9, x14, x9
   37350:	cmp	x9, #0x0
   37354:	cset	w9, eq  // eq = none
   37358:	sub	x8, x8, x9
   3735c:	lsl	x9, x8, #3
   37360:	ldr	x10, [x10, x9]
   37364:	ldr	x11, [x11, x9]
   37368:	ldr	x12, [x12, x9]
   3736c:	ldr	x9, [x13, x9]
   37370:	orr	x10, x11, x10
   37374:	orr	x10, x10, x12
   37378:	orr	x9, x10, x9
   3737c:	cmp	x9, #0x0
   37380:	cset	w9, eq  // eq = none
   37384:	sub	x8, x8, x9
   37388:	add	x8, x8, #0x1
   3738c:	str	x8, [x19, #8]
   37390:	ldp	x20, x19, [sp, #48]
   37394:	ldp	x29, x30, [sp, #32]
   37398:	add	sp, sp, #0x40
   3739c:	ret

00000000000373a0 <__gmpn_hgcd_matrix_adjust@@Base>:
   373a0:	sub	sp, sp, #0x70
   373a4:	stp	x29, x30, [sp, #16]
   373a8:	stp	x28, x27, [sp, #32]
   373ac:	stp	x26, x25, [sp, #48]
   373b0:	stp	x24, x23, [sp, #64]
   373b4:	stp	x22, x21, [sp, #80]
   373b8:	stp	x20, x19, [sp, #96]
   373bc:	mov	x22, x4
   373c0:	ldr	x4, [x0, #8]
   373c4:	mov	x20, x3
   373c8:	ldr	x3, [x0, #40]
   373cc:	add	x25, x5, x22, lsl #3
   373d0:	mov	x26, x5
   373d4:	mov	x21, x2
   373d8:	mov	x19, x1
   373dc:	mov	x23, x0
   373e0:	cmp	x4, x22
   373e4:	add	x24, x25, x4, lsl #3
   373e8:	mov	x0, x5
   373ec:	add	x29, sp, #0x10
   373f0:	str	x24, [sp]
   373f4:	b.ge	3741c <__gmpn_hgcd_matrix_adjust@@Base+0x7c>  // b.tcont
   373f8:	mov	x1, x21
   373fc:	mov	x2, x22
   37400:	bl	ccd0 <__gmpn_mul@plt>
   37404:	ldr	x3, [x23, #32]
   37408:	ldr	x4, [x23, #8]
   3740c:	mov	x0, x24
   37410:	mov	x1, x21
   37414:	mov	x2, x22
   37418:	b	37444 <__gmpn_hgcd_matrix_adjust@@Base+0xa4>
   3741c:	mov	x1, x3
   37420:	mov	x2, x4
   37424:	mov	x3, x21
   37428:	mov	x4, x22
   3742c:	bl	ccd0 <__gmpn_mul@plt>
   37430:	ldr	x1, [x23, #32]
   37434:	ldr	x2, [x23, #8]
   37438:	mov	x0, x24
   3743c:	mov	x3, x21
   37440:	mov	x4, x22
   37444:	bl	ccd0 <__gmpn_mul@plt>
   37448:	mov	x0, x21
   3744c:	mov	x1, x26
   37450:	mov	x2, x22
   37454:	bl	ca50 <__gmpn_copyi@plt>
   37458:	ldr	x27, [x23, #8]
   3745c:	sub	x24, x19, x22
   37460:	cbz	x27, 374a0 <__gmpn_hgcd_matrix_adjust@@Base+0x100>
   37464:	add	x28, x21, x22, lsl #3
   37468:	mov	x0, x28
   3746c:	mov	x1, x28
   37470:	mov	x2, x25
   37474:	mov	x3, x27
   37478:	bl	ca70 <__gmpn_add_n@plt>
   3747c:	cbz	x0, 374a0 <__gmpn_hgcd_matrix_adjust@@Base+0x100>
   37480:	cmp	x27, x24
   37484:	b.ge	37660 <__gmpn_hgcd_matrix_adjust@@Base+0x2c0>  // b.tcont
   37488:	lsl	x8, x27, #3
   3748c:	ldr	x9, [x28, x8]
   37490:	add	x27, x27, #0x1
   37494:	adds	x9, x9, #0x1
   37498:	str	x9, [x28, x8]
   3749c:	b.cs	37480 <__gmpn_hgcd_matrix_adjust@@Base+0xe0>  // b.hs, b.nlast
   374a0:	str	xzr, [sp, #8]
   374a4:	ldr	x4, [x23, #8]
   374a8:	ldr	x3, [x23, #24]
   374ac:	mov	x0, x26
   374b0:	cmp	x4, x22
   374b4:	b.ge	374c4 <__gmpn_hgcd_matrix_adjust@@Base+0x124>  // b.tcont
   374b8:	mov	x1, x20
   374bc:	mov	x2, x22
   374c0:	b	374d4 <__gmpn_hgcd_matrix_adjust@@Base+0x134>
   374c4:	mov	x1, x3
   374c8:	mov	x2, x4
   374cc:	mov	x3, x20
   374d0:	mov	x4, x22
   374d4:	bl	ccd0 <__gmpn_mul@plt>
   374d8:	ldr	x8, [x23, #8]
   374dc:	adds	x27, x8, x22
   374e0:	b.eq	37520 <__gmpn_hgcd_matrix_adjust@@Base+0x180>  // b.none
   374e4:	mov	x0, x21
   374e8:	mov	x1, x21
   374ec:	mov	x2, x26
   374f0:	mov	x3, x27
   374f4:	bl	c2d0 <__gmpn_sub_n@plt>
   374f8:	cbz	x0, 37520 <__gmpn_hgcd_matrix_adjust@@Base+0x180>
   374fc:	mov	w28, #0x1                   	// #1
   37500:	cmp	x27, x19
   37504:	b.ge	37524 <__gmpn_hgcd_matrix_adjust@@Base+0x184>  // b.tcont
   37508:	lsl	x8, x27, #3
   3750c:	ldr	x9, [x21, x8]
   37510:	add	x27, x27, #0x1
   37514:	sub	x10, x9, #0x1
   37518:	str	x10, [x21, x8]
   3751c:	cbz	x9, 37500 <__gmpn_hgcd_matrix_adjust@@Base+0x160>
   37520:	mov	x28, xzr
   37524:	ldp	x4, x3, [x23, #8]
   37528:	mov	x0, x26
   3752c:	cmp	x4, x22
   37530:	b.ge	37540 <__gmpn_hgcd_matrix_adjust@@Base+0x1a0>  // b.tcont
   37534:	mov	x1, x20
   37538:	mov	x2, x22
   3753c:	b	37550 <__gmpn_hgcd_matrix_adjust@@Base+0x1b0>
   37540:	mov	x1, x3
   37544:	mov	x2, x4
   37548:	mov	x3, x20
   3754c:	mov	x4, x22
   37550:	bl	ccd0 <__gmpn_mul@plt>
   37554:	mov	x0, x20
   37558:	mov	x1, x26
   3755c:	mov	x2, x22
   37560:	bl	ca50 <__gmpn_copyi@plt>
   37564:	ldr	x26, [x23, #8]
   37568:	cbz	x26, 375bc <__gmpn_hgcd_matrix_adjust@@Base+0x21c>
   3756c:	add	x27, x20, x22, lsl #3
   37570:	mov	x0, x27
   37574:	mov	x1, x27
   37578:	mov	x2, x25
   3757c:	mov	x3, x26
   37580:	bl	ca70 <__gmpn_add_n@plt>
   37584:	cbz	x0, 375bc <__gmpn_hgcd_matrix_adjust@@Base+0x21c>
   37588:	ldr	x10, [sp, #8]
   3758c:	mov	w25, #0x1                   	// #1
   37590:	cmp	x26, x24
   37594:	b.ge	375b4 <__gmpn_hgcd_matrix_adjust@@Base+0x214>  // b.tcont
   37598:	lsl	x8, x26, #3
   3759c:	ldr	x9, [x27, x8]
   375a0:	add	x26, x26, #0x1
   375a4:	adds	x9, x9, #0x1
   375a8:	str	x9, [x27, x8]
   375ac:	b.cs	37590 <__gmpn_hgcd_matrix_adjust@@Base+0x1f0>  // b.hs, b.nlast
   375b0:	mov	x25, xzr
   375b4:	ldr	x2, [sp]
   375b8:	b	375c4 <__gmpn_hgcd_matrix_adjust@@Base+0x224>
   375bc:	ldp	x2, x10, [sp]
   375c0:	mov	x25, xzr
   375c4:	ldr	x8, [x23, #8]
   375c8:	sub	x23, x10, x28
   375cc:	adds	x22, x8, x22
   375d0:	b.eq	3760c <__gmpn_hgcd_matrix_adjust@@Base+0x26c>  // b.none
   375d4:	mov	x0, x20
   375d8:	mov	x1, x20
   375dc:	mov	x3, x22
   375e0:	bl	c2d0 <__gmpn_sub_n@plt>
   375e4:	cbz	x0, 3760c <__gmpn_hgcd_matrix_adjust@@Base+0x26c>
   375e8:	mov	w8, #0x1                   	// #1
   375ec:	cmp	x22, x19
   375f0:	b.ge	37610 <__gmpn_hgcd_matrix_adjust@@Base+0x270>  // b.tcont
   375f4:	lsl	x9, x22, #3
   375f8:	ldr	x10, [x20, x9]
   375fc:	add	x22, x22, #0x1
   37600:	sub	x11, x10, #0x1
   37604:	str	x11, [x20, x9]
   37608:	cbz	x10, 375ec <__gmpn_hgcd_matrix_adjust@@Base+0x24c>
   3760c:	mov	x8, xzr
   37610:	sub	x8, x25, x8
   37614:	orr	x9, x8, x23
   37618:	cbz	x9, 37630 <__gmpn_hgcd_matrix_adjust@@Base+0x290>
   3761c:	lsl	x9, x19, #3
   37620:	add	x19, x19, #0x1
   37624:	str	x23, [x21, x9]
   37628:	str	x8, [x20, x9]
   3762c:	b	3763c <__gmpn_hgcd_matrix_adjust@@Base+0x29c>
   37630:	sub	x8, x19, #0x1
   37634:	ldr	x9, [x21, x8, lsl #3]
   37638:	cbz	x9, 3766c <__gmpn_hgcd_matrix_adjust@@Base+0x2cc>
   3763c:	mov	x0, x19
   37640:	ldp	x20, x19, [sp, #96]
   37644:	ldp	x22, x21, [sp, #80]
   37648:	ldp	x24, x23, [sp, #64]
   3764c:	ldp	x26, x25, [sp, #48]
   37650:	ldp	x28, x27, [sp, #32]
   37654:	ldp	x29, x30, [sp, #16]
   37658:	add	sp, sp, #0x70
   3765c:	ret
   37660:	mov	w8, #0x1                   	// #1
   37664:	str	x8, [sp, #8]
   37668:	b	374a4 <__gmpn_hgcd_matrix_adjust@@Base+0x104>
   3766c:	ldr	x9, [x20, x8, lsl #3]
   37670:	cmp	x9, #0x0
   37674:	csel	x0, x8, x19, eq  // eq = none
   37678:	b	37640 <__gmpn_hgcd_matrix_adjust@@Base+0x2a0>

000000000003767c <__gmpn_hgcd2@@Base>:
   3767c:	cmp	x0, #0x2
   37680:	mov	w8, wzr
   37684:	b.cc	37934 <__gmpn_hgcd2@@Base+0x2b8>  // b.lo, b.ul, b.last
   37688:	cmp	x2, #0x2
   3768c:	b.cc	37934 <__gmpn_hgcd2@@Base+0x2b8>  // b.lo, b.ul, b.last
   37690:	cmp	x0, x2
   37694:	b.hi	376a4 <__gmpn_hgcd2@@Base+0x28>  // b.pmore
   37698:	b.ne	376cc <__gmpn_hgcd2@@Base+0x50>  // b.any
   3769c:	cmp	x1, x3
   376a0:	b.ls	376cc <__gmpn_hgcd2@@Base+0x50>  // b.plast
   376a4:	subs	x10, x1, x3
   376a8:	sbc	x0, x0, x2
   376ac:	cmp	x0, #0x2
   376b0:	b.cs	376bc <__gmpn_hgcd2@@Base+0x40>  // b.hs, b.nlast
   376b4:	mov	w0, wzr
   376b8:	ret
   376bc:	mov	x8, xzr
   376c0:	mov	w9, #0x1                   	// #1
   376c4:	mov	x1, x10
   376c8:	b	376f0 <__gmpn_hgcd2@@Base+0x74>
   376cc:	subs	x10, x3, x1
   376d0:	sbc	x2, x2, x0
   376d4:	cmp	x2, #0x2
   376d8:	b.cs	376e4 <__gmpn_hgcd2@@Base+0x68>  // b.hs, b.nlast
   376dc:	mov	w0, wzr
   376e0:	ret
   376e4:	mov	x9, xzr
   376e8:	mov	w8, #0x1                   	// #1
   376ec:	mov	x3, x10
   376f0:	mov	w10, #0x1                   	// #1
   376f4:	cmp	x0, x2
   376f8:	mov	w11, #0x1                   	// #1
   376fc:	b.cc	377cc <__gmpn_hgcd2@@Base+0x150>  // b.lo, b.ul, b.last
   37700:	cmp	x0, x2
   37704:	b.eq	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.none
   37708:	lsr	x12, x0, #32
   3770c:	cbz	x12, 3789c <__gmpn_hgcd2@@Base+0x220>
   37710:	mov	x12, x1
   37714:	subs	x1, x12, x3
   37718:	sbc	x0, x0, x2
   3771c:	cmp	x0, #0x2
   37720:	b.cc	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.lo, b.ul, b.last
   37724:	cmp	x0, x2
   37728:	b.ls	377c4 <__gmpn_hgcd2@@Base+0x148>  // b.plast
   3772c:	clz	x14, x0
   37730:	clz	x12, x2
   37734:	mov	w16, #0x3f                  	// #63
   37738:	sub	w17, w12, w14
   3773c:	lsr	x13, x3, #1
   37740:	sub	w16, w16, w17
   37744:	mvn	w18, w12
   37748:	lsl	x5, x2, x17
   3774c:	lsr	x13, x13, x16
   37750:	mov	x15, xzr
   37754:	lsl	x12, x3, x17
   37758:	add	x13, x13, x5
   3775c:	add	w14, w18, w14
   37760:	cmp	x1, x12
   37764:	cset	w16, cs  // cs = hs, nlast
   37768:	cmp	x0, x13
   3776c:	cset	w17, hi  // hi = pmore
   37770:	csel	w17, w16, w17, eq  // eq = none
   37774:	sbfx	x16, x17, #0, #1
   37778:	bfi	x17, x15, #1, #63
   3777c:	and	x15, x12, x16
   37780:	and	x18, x13, x16
   37784:	subs	x16, x1, x15
   37788:	sbc	x0, x0, x18
   3778c:	extr	x12, x13, x12, #1
   37790:	adds	w14, w14, #0x1
   37794:	lsr	x13, x13, #1
   37798:	mov	x1, x16
   3779c:	mov	x15, x17
   377a0:	b.cc	37760 <__gmpn_hgcd2@@Base+0xe4>  // b.lo, b.ul, b.last
   377a4:	cmp	x0, #0x1
   377a8:	cinc	x12, x17, hi  // hi = pmore
   377ac:	cmp	x0, #0x2
   377b0:	madd	x11, x12, x8, x11
   377b4:	madd	x9, x12, x10, x9
   377b8:	b.cc	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.lo, b.ul, b.last
   377bc:	mov	x1, x16
   377c0:	b	377cc <__gmpn_hgcd2@@Base+0x150>
   377c4:	add	x9, x9, x10
   377c8:	add	x11, x11, x8
   377cc:	cmp	x0, x2
   377d0:	b.eq	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.none
   377d4:	lsr	x12, x2, #32
   377d8:	cbz	x12, 378a8 <__gmpn_hgcd2@@Base+0x22c>
   377dc:	mov	x12, x3
   377e0:	subs	x3, x12, x1
   377e4:	sbc	x2, x2, x0
   377e8:	cmp	x2, #0x2
   377ec:	b.cc	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.lo, b.ul, b.last
   377f0:	cmp	x2, x0
   377f4:	b.ls	37890 <__gmpn_hgcd2@@Base+0x214>  // b.plast
   377f8:	clz	x14, x2
   377fc:	clz	x12, x0
   37800:	mov	w16, #0x3f                  	// #63
   37804:	sub	w17, w12, w14
   37808:	lsr	x13, x1, #1
   3780c:	sub	w16, w16, w17
   37810:	mvn	w18, w12
   37814:	lsl	x5, x0, x17
   37818:	lsr	x13, x13, x16
   3781c:	mov	x15, xzr
   37820:	lsl	x12, x1, x17
   37824:	add	x13, x13, x5
   37828:	add	w14, w18, w14
   3782c:	cmp	x3, x12
   37830:	cset	w16, cs  // cs = hs, nlast
   37834:	cmp	x2, x13
   37838:	cset	w17, hi  // hi = pmore
   3783c:	csel	w17, w16, w17, eq  // eq = none
   37840:	sbfx	x16, x17, #0, #1
   37844:	bfi	x17, x15, #1, #63
   37848:	and	x15, x12, x16
   3784c:	and	x18, x13, x16
   37850:	subs	x16, x3, x15
   37854:	sbc	x2, x2, x18
   37858:	extr	x12, x13, x12, #1
   3785c:	adds	w14, w14, #0x1
   37860:	lsr	x13, x13, #1
   37864:	mov	x3, x16
   37868:	mov	x15, x17
   3786c:	b.cc	3782c <__gmpn_hgcd2@@Base+0x1b0>  // b.lo, b.ul, b.last
   37870:	cmp	x2, #0x1
   37874:	cinc	x12, x17, hi  // hi = pmore
   37878:	cmp	x2, #0x2
   3787c:	madd	x8, x12, x11, x8
   37880:	madd	x10, x12, x9, x10
   37884:	b.cc	37928 <__gmpn_hgcd2@@Base+0x2ac>  // b.lo, b.ul, b.last
   37888:	mov	x3, x16
   3788c:	b	37700 <__gmpn_hgcd2@@Base+0x84>
   37890:	add	x10, x9, x10
   37894:	add	x8, x11, x8
   37898:	b	37700 <__gmpn_hgcd2@@Base+0x84>
   3789c:	extr	x12, x0, x1, #32
   378a0:	extr	x13, x2, x3, #32
   378a4:	b	378b4 <__gmpn_hgcd2@@Base+0x238>
   378a8:	extr	x12, x0, x1, #32
   378ac:	extr	x13, x2, x3, #32
   378b0:	b	378f4 <__gmpn_hgcd2@@Base+0x278>
   378b4:	sub	x12, x12, x13
   378b8:	lsr	x14, x12, #33
   378bc:	cbz	x14, 37928 <__gmpn_hgcd2@@Base+0x2ac>
   378c0:	cmp	x12, x13
   378c4:	b.ls	378ec <__gmpn_hgcd2@@Base+0x270>  // b.plast
   378c8:	udiv	x14, x12, x13
   378cc:	msub	x12, x14, x13, x12
   378d0:	lsr	x15, x12, #33
   378d4:	cmp	x15, #0x0
   378d8:	cinc	x14, x14, ne  // ne = any
   378dc:	madd	x11, x14, x8, x11
   378e0:	madd	x9, x14, x10, x9
   378e4:	cbnz	x15, 378f4 <__gmpn_hgcd2@@Base+0x278>
   378e8:	b	37928 <__gmpn_hgcd2@@Base+0x2ac>
   378ec:	add	x9, x9, x10
   378f0:	add	x11, x11, x8
   378f4:	sub	x13, x13, x12
   378f8:	lsr	x14, x13, #33
   378fc:	cbz	x14, 37928 <__gmpn_hgcd2@@Base+0x2ac>
   37900:	cmp	x13, x12
   37904:	b.ls	3793c <__gmpn_hgcd2@@Base+0x2c0>  // b.plast
   37908:	udiv	x14, x13, x12
   3790c:	msub	x13, x14, x12, x13
   37910:	lsr	x15, x13, #33
   37914:	cmp	x15, #0x0
   37918:	cinc	x14, x14, ne  // ne = any
   3791c:	madd	x8, x14, x11, x8
   37920:	madd	x10, x14, x9, x10
   37924:	cbnz	x15, 378b4 <__gmpn_hgcd2@@Base+0x238>
   37928:	stp	x8, x11, [x4, #16]
   3792c:	mov	w8, #0x1                   	// #1
   37930:	stp	x10, x9, [x4]
   37934:	mov	w0, w8
   37938:	ret
   3793c:	add	x10, x9, x10
   37940:	add	x8, x11, x8
   37944:	b	378b4 <__gmpn_hgcd2@@Base+0x238>

0000000000037948 <__gmpn_hgcd_mul_matrix1_vector@@Base>:
   37948:	stp	x29, x30, [sp, #-64]!
   3794c:	stp	x24, x23, [sp, #16]
   37950:	stp	x22, x21, [sp, #32]
   37954:	stp	x20, x19, [sp, #48]
   37958:	mov	x20, x3
   3795c:	ldr	x3, [x0]
   37960:	mov	x21, x2
   37964:	mov	x22, x0
   37968:	mov	x23, x1
   3796c:	mov	x0, x1
   37970:	mov	x1, x2
   37974:	mov	x2, x4
   37978:	mov	x29, sp
   3797c:	mov	x19, x4
   37980:	bl	d490 <__gmpn_mul_1@plt>
   37984:	ldr	x3, [x22, #16]
   37988:	mov	x24, x0
   3798c:	mov	x0, x23
   37990:	mov	x1, x20
   37994:	mov	x2, x19
   37998:	bl	d400 <__gmpn_addmul_1@plt>
   3799c:	ldr	x3, [x22, #24]
   379a0:	add	x24, x0, x24
   379a4:	mov	x0, x20
   379a8:	mov	x1, x20
   379ac:	mov	x2, x19
   379b0:	bl	d490 <__gmpn_mul_1@plt>
   379b4:	ldr	x3, [x22, #8]
   379b8:	mov	x22, x0
   379bc:	mov	x0, x20
   379c0:	mov	x1, x21
   379c4:	mov	x2, x19
   379c8:	bl	d400 <__gmpn_addmul_1@plt>
   379cc:	add	x8, x0, x22
   379d0:	lsl	x9, x19, #3
   379d4:	orr	x10, x8, x24
   379d8:	str	x24, [x23, x9]
   379dc:	cmp	x10, #0x0
   379e0:	str	x8, [x20, x9]
   379e4:	cinc	x0, x19, ne  // ne = any
   379e8:	ldp	x20, x19, [sp, #48]
   379ec:	ldp	x22, x21, [sp, #32]
   379f0:	ldp	x24, x23, [sp, #16]
   379f4:	ldp	x29, x30, [sp], #64
   379f8:	ret

00000000000379fc <__gmpn_hgcd_step@@Base>:
   379fc:	sub	sp, sp, #0x60
   37a00:	lsl	x8, x0, #3
   37a04:	stp	x29, x30, [sp, #32]
   37a08:	stp	x24, x23, [sp, #48]
   37a0c:	stp	x22, x21, [sp, #64]
   37a10:	stp	x20, x19, [sp, #80]
   37a14:	sub	x9, x8, #0x8
   37a18:	mov	x21, x2
   37a1c:	mov	x20, x0
   37a20:	ldr	x0, [x1, x9]
   37a24:	ldr	x2, [x2, x9]
   37a28:	add	x9, x3, #0x1
   37a2c:	mov	x19, x5
   37a30:	mov	x23, x4
   37a34:	mov	x22, x1
   37a38:	mov	x24, x3
   37a3c:	cmp	x9, x20
   37a40:	orr	x9, x2, x0
   37a44:	add	x29, sp, #0x20
   37a48:	b.ne	37a58 <__gmpn_hgcd_step@@Base+0x5c>  // b.any
   37a4c:	cmp	x9, #0x4
   37a50:	b.cs	37ab0 <__gmpn_hgcd_step@@Base+0xb4>  // b.hs, b.nlast
   37a54:	b	37b04 <__gmpn_hgcd_step@@Base+0x108>
   37a58:	tbnz	x9, #63, 37ab0 <__gmpn_hgcd_step@@Base+0xb4>
   37a5c:	sub	x10, x8, #0x10
   37a60:	sub	x8, x8, #0x18
   37a64:	ldr	x12, [x22, x10]
   37a68:	ldr	x14, [x22, x8]
   37a6c:	ldr	x10, [x21, x10]
   37a70:	ldr	x8, [x21, x8]
   37a74:	clz	x9, x9
   37a78:	neg	x13, x9
   37a7c:	lsl	x11, x0, x9
   37a80:	lsl	x15, x2, x9
   37a84:	lsr	x16, x12, x13
   37a88:	lsl	x12, x12, x9
   37a8c:	lsr	x14, x14, x13
   37a90:	lsl	x9, x10, x9
   37a94:	lsr	x10, x10, x13
   37a98:	lsr	x8, x8, x13
   37a9c:	orr	x0, x16, x11
   37aa0:	orr	x1, x14, x12
   37aa4:	orr	x2, x10, x15
   37aa8:	orr	x3, x8, x9
   37aac:	b	37abc <__gmpn_hgcd_step@@Base+0xc0>
   37ab0:	sub	x8, x8, #0x10
   37ab4:	ldr	x1, [x22, x8]
   37ab8:	ldr	x3, [x21, x8]
   37abc:	mov	x4, sp
   37ac0:	bl	c5a0 <__gmpn_hgcd2@plt>
   37ac4:	cbz	w0, 37b04 <__gmpn_hgcd_step@@Base+0x108>
   37ac8:	mov	x1, sp
   37acc:	mov	x0, x23
   37ad0:	mov	x2, x19
   37ad4:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   37ad8:	mov	x0, x19
   37adc:	mov	x1, x22
   37ae0:	mov	x2, x20
   37ae4:	bl	ca50 <__gmpn_copyi@plt>
   37ae8:	mov	x0, sp
   37aec:	mov	x1, x22
   37af0:	mov	x2, x19
   37af4:	mov	x3, x21
   37af8:	mov	x4, x20
   37afc:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   37b00:	b	37b28 <__gmpn_hgcd_step@@Base+0x12c>
   37b04:	adrp	x4, 37000 <__gmpn_hgcd_matrix_update_q@@Base+0x1f8>
   37b08:	add	x4, x4, #0xb40
   37b0c:	mov	x0, x22
   37b10:	mov	x1, x21
   37b14:	mov	x2, x20
   37b18:	mov	x3, x24
   37b1c:	mov	x5, x23
   37b20:	mov	x6, x19
   37b24:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   37b28:	ldp	x20, x19, [sp, #80]
   37b2c:	ldp	x22, x21, [sp, #64]
   37b30:	ldp	x24, x23, [sp, #48]
   37b34:	ldp	x29, x30, [sp, #32]
   37b38:	add	sp, sp, #0x60
   37b3c:	ret
   37b40:	add	x9, x3, x4, lsl #3
   37b44:	mov	x8, x4
   37b48:	add	x4, x9, #0x8
   37b4c:	subs	x8, x8, #0x1
   37b50:	b.lt	37b70 <__gmpn_hgcd_step@@Base+0x174>  // b.tstop
   37b54:	ldur	x9, [x4, #-16]
   37b58:	sub	x4, x4, #0x8
   37b5c:	cbz	x9, 37b4c <__gmpn_hgcd_step@@Base+0x150>
   37b60:	add	x2, x8, #0x1
   37b64:	mov	x1, x3
   37b68:	mov	w3, w5
   37b6c:	b	d020 <__gmpn_hgcd_matrix_update_q@plt>
   37b70:	ret

0000000000037b74 <__gmpn_hgcd_reduce_itch@@Base>:
   37b74:	stp	x29, x30, [sp, #-48]!
   37b78:	str	x21, [sp, #16]
   37b7c:	cmp	x0, #0x68e
   37b80:	sub	x21, x0, x1
   37b84:	stp	x20, x19, [sp, #32]
   37b88:	mov	x29, sp
   37b8c:	b.le	37ba0 <__gmpn_hgcd_reduce_itch@@Base+0x2c>
   37b90:	mov	x0, x21
   37b94:	bl	c590 <__gmpn_hgcd_itch@plt>
   37b98:	add	x0, x0, x21, lsl #1
   37b9c:	b	37bc0 <__gmpn_hgcd_reduce_itch@@Base+0x4c>
   37ba0:	mov	x20, x0
   37ba4:	mov	x0, x21
   37ba8:	mov	x19, x1
   37bac:	bl	c590 <__gmpn_hgcd_itch@plt>
   37bb0:	add	x8, x20, x19
   37bb4:	sub	x8, x8, #0x1
   37bb8:	cmp	x0, x8
   37bbc:	csel	x0, x8, x0, lt  // lt = tstop
   37bc0:	ldp	x20, x19, [sp, #32]
   37bc4:	ldr	x21, [sp, #16]
   37bc8:	ldp	x29, x30, [sp], #48
   37bcc:	ret

0000000000037bd0 <__gmpn_hgcd_reduce@@Base>:
   37bd0:	stp	x29, x30, [sp, #-80]!
   37bd4:	stp	x24, x23, [sp, #32]
   37bd8:	stp	x22, x21, [sp, #48]
   37bdc:	stp	x20, x19, [sp, #64]
   37be0:	mov	x22, x5
   37be4:	mov	x24, x4
   37be8:	mov	x23, x3
   37bec:	mov	x19, x2
   37bf0:	mov	x20, x1
   37bf4:	mov	x21, x0
   37bf8:	cmp	x3, #0x68e
   37bfc:	add	x8, x1, x4, lsl #3
   37c00:	str	x25, [sp, #16]
   37c04:	mov	x29, sp
   37c08:	b.le	37c80 <__gmpn_hgcd_reduce@@Base+0xb0>
   37c0c:	sub	x25, x23, x24
   37c10:	mov	x0, x22
   37c14:	mov	x1, x8
   37c18:	mov	x2, x25
   37c1c:	bl	ca50 <__gmpn_copyi@plt>
   37c20:	add	x8, x22, x23, lsl #3
   37c24:	lsl	x9, x24, #3
   37c28:	sub	x24, x8, x9
   37c2c:	add	x1, x19, x9
   37c30:	mov	x0, x24
   37c34:	mov	x2, x25
   37c38:	bl	ca50 <__gmpn_copyi@plt>
   37c3c:	add	x4, x22, x25, lsl #4
   37c40:	mov	x0, x22
   37c44:	mov	x1, x24
   37c48:	mov	x2, x25
   37c4c:	mov	x3, x21
   37c50:	bl	cd50 <__gmpn_hgcd_appr@plt>
   37c54:	cbz	w0, 37cd0 <__gmpn_hgcd_reduce@@Base+0x100>
   37c58:	mov	x0, x21
   37c5c:	mov	x1, x20
   37c60:	mov	x2, x19
   37c64:	mov	x3, x23
   37c68:	ldp	x20, x19, [sp, #64]
   37c6c:	ldp	x22, x21, [sp, #48]
   37c70:	ldp	x24, x23, [sp, #32]
   37c74:	ldr	x25, [sp, #16]
   37c78:	ldp	x29, x30, [sp], #80
   37c7c:	b	37cec <__gmpn_hgcd_reduce@@Base+0x11c>
   37c80:	add	x1, x19, x24, lsl #3
   37c84:	sub	x2, x23, x24
   37c88:	mov	x0, x8
   37c8c:	mov	x3, x21
   37c90:	mov	x4, x22
   37c94:	bl	cde0 <__gmpn_hgcd@plt>
   37c98:	cmp	x0, #0x1
   37c9c:	b.lt	37cd0 <__gmpn_hgcd_reduce@@Base+0x100>  // b.tstop
   37ca0:	add	x1, x0, x24
   37ca4:	mov	x0, x21
   37ca8:	mov	x2, x20
   37cac:	mov	x3, x19
   37cb0:	mov	x4, x24
   37cb4:	mov	x5, x22
   37cb8:	ldp	x20, x19, [sp, #64]
   37cbc:	ldp	x22, x21, [sp, #48]
   37cc0:	ldp	x24, x23, [sp, #32]
   37cc4:	ldr	x25, [sp, #16]
   37cc8:	ldp	x29, x30, [sp], #80
   37ccc:	b	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   37cd0:	ldp	x20, x19, [sp, #64]
   37cd4:	ldp	x22, x21, [sp, #48]
   37cd8:	ldp	x24, x23, [sp, #32]
   37cdc:	ldr	x25, [sp, #16]
   37ce0:	mov	x0, xzr
   37ce4:	ldp	x29, x30, [sp], #80
   37ce8:	ret
   37cec:	stp	x29, x30, [sp, #-96]!
   37cf0:	stp	x28, x27, [sp, #16]
   37cf4:	stp	x26, x25, [sp, #32]
   37cf8:	stp	x24, x23, [sp, #48]
   37cfc:	stp	x22, x21, [sp, #64]
   37d00:	stp	x20, x19, [sp, #80]
   37d04:	mov	x29, sp
   37d08:	sub	sp, sp, #0x60
   37d0c:	mov	x23, x3
   37d10:	mov	x19, x2
   37d14:	mov	x20, x1
   37d18:	mov	x22, x0
   37d1c:	mov	x8, x3
   37d20:	mov	x3, x8
   37d24:	subs	x8, x8, #0x1
   37d28:	b.lt	37d34 <__gmpn_hgcd_reduce@@Base+0x164>  // b.tstop
   37d2c:	ldr	x9, [x20, x8, lsl #3]
   37d30:	cbz	x9, 37d20 <__gmpn_hgcd_reduce@@Base+0x150>
   37d34:	mov	x9, x23
   37d38:	mov	x8, x9
   37d3c:	subs	x9, x9, #0x1
   37d40:	b.lt	37d4c <__gmpn_hgcd_reduce@@Base+0x17c>  // b.tstop
   37d44:	ldr	x10, [x19, x9, lsl #3]
   37d48:	cbz	x10, 37d38 <__gmpn_hgcd_reduce@@Base+0x168>
   37d4c:	ldr	x25, [x22, #8]
   37d50:	mov	x9, x25
   37d54:	mov	x21, x9
   37d58:	subs	x9, x9, #0x1
   37d5c:	b.lt	37d70 <__gmpn_hgcd_reduce@@Base+0x1a0>  // b.tstop
   37d60:	ldr	x10, [x22, #16]
   37d64:	add	x10, x10, x21, lsl #3
   37d68:	ldur	x10, [x10, #-8]
   37d6c:	cbz	x10, 37d54 <__gmpn_hgcd_reduce@@Base+0x184>
   37d70:	sub	x12, x3, x21
   37d74:	mov	x9, x25
   37d78:	stur	x21, [x29, #-32]
   37d7c:	mov	x26, x9
   37d80:	subs	x9, x9, #0x1
   37d84:	b.lt	37d98 <__gmpn_hgcd_reduce@@Base+0x1c8>  // b.tstop
   37d88:	ldr	x10, [x22, #24]
   37d8c:	add	x10, x10, x26, lsl #3
   37d90:	ldur	x10, [x10, #-8]
   37d94:	cbz	x10, 37d7c <__gmpn_hgcd_reduce@@Base+0x1ac>
   37d98:	sub	x13, x3, x26
   37d9c:	mov	x9, x25
   37da0:	stur	x26, [x29, #-24]
   37da4:	mov	x5, x9
   37da8:	subs	x9, x9, #0x1
   37dac:	b.lt	37dc0 <__gmpn_hgcd_reduce@@Base+0x1f0>  // b.tstop
   37db0:	ldr	x10, [x22, #32]
   37db4:	add	x10, x10, x5, lsl #3
   37db8:	ldur	x10, [x10, #-8]
   37dbc:	cbz	x10, 37da4 <__gmpn_hgcd_reduce@@Base+0x1d4>
   37dc0:	sub	x14, x8, x5
   37dc4:	mov	x10, x25
   37dc8:	stur	x5, [x29, #-16]
   37dcc:	mov	x9, x10
   37dd0:	subs	x10, x10, #0x1
   37dd4:	b.lt	37de8 <__gmpn_hgcd_reduce@@Base+0x218>  // b.tstop
   37dd8:	ldr	x11, [x22, #40]
   37ddc:	add	x11, x11, x9, lsl #3
   37de0:	ldur	x11, [x11, #-8]
   37de4:	cbz	x11, 37dcc <__gmpn_hgcd_reduce@@Base+0x1fc>
   37de8:	stur	x9, [x29, #-8]
   37dec:	stur	xzr, [x29, #-40]
   37df0:	cbz	x26, 37ed4 <__gmpn_hgcd_reduce@@Base+0x304>
   37df4:	cbz	x5, 37ee8 <__gmpn_hgcd_reduce@@Base+0x318>
   37df8:	sub	x9, x8, x9
   37dfc:	cmp	x12, x14
   37e00:	csel	x8, x12, x14, lt  // lt = tstop
   37e04:	cmp	x13, x9
   37e08:	stp	x9, x14, [x29, #-88]
   37e0c:	csel	x9, x13, x9, lt  // lt = tstop
   37e10:	cmp	x8, x9
   37e14:	csel	x8, x8, x9, gt
   37e18:	add	x0, x8, #0x2
   37e1c:	stp	x13, x12, [x29, #-72]
   37e20:	stur	x8, [x29, #-56]
   37e24:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   37e28:	asr	x8, x0, #1
   37e2c:	cmp	x8, x25
   37e30:	csel	x10, x0, x8, lt  // lt = tstop
   37e34:	cmp	x8, x0
   37e38:	add	x9, x0, x0, lsl #1
   37e3c:	csel	x8, x10, xzr, lt  // lt = tstop
   37e40:	add	x8, x9, x8
   37e44:	lsl	x8, x8, #3
   37e48:	add	x1, x8, #0x20
   37e4c:	mov	w8, #0x7f00                	// #32512
   37e50:	mov	x24, x0
   37e54:	cmp	x1, x8
   37e58:	b.hi	381c4 <__gmpn_hgcd_reduce@@Base+0x5f4>  // b.pmore
   37e5c:	add	x9, x1, #0xf
   37e60:	mov	x8, sp
   37e64:	and	x9, x9, #0xfffffffffffffff0
   37e68:	sub	x25, x8, x9
   37e6c:	mov	sp, x25
   37e70:	lsl	x8, x24, #3
   37e74:	add	x9, x25, x8
   37e78:	cmp	x24, x23
   37e7c:	add	x28, x9, x8
   37e80:	stur	x9, [x29, #-48]
   37e84:	b.ge	37f78 <__gmpn_hgcd_reduce@@Base+0x3a8>  // b.tcont
   37e88:	subs	x27, x23, x24
   37e8c:	mov	x23, x24
   37e90:	b.eq	37f78 <__gmpn_hgcd_reduce@@Base+0x3a8>  // b.none
   37e94:	add	x2, x20, x24, lsl #3
   37e98:	mov	x0, x20
   37e9c:	mov	x1, x20
   37ea0:	mov	x3, x27
   37ea4:	bl	ca70 <__gmpn_add_n@plt>
   37ea8:	cbz	x0, 37f20 <__gmpn_hgcd_reduce@@Base+0x350>
   37eac:	mov	x8, x27
   37eb0:	cmp	x8, x24
   37eb4:	b.ge	37f0c <__gmpn_hgcd_reduce@@Base+0x33c>  // b.tcont
   37eb8:	lsl	x9, x8, #3
   37ebc:	ldr	x10, [x20, x9]
   37ec0:	add	x8, x8, #0x1
   37ec4:	adds	x10, x10, #0x1
   37ec8:	str	x10, [x20, x9]
   37ecc:	b.cs	37eb0 <__gmpn_hgcd_reduce@@Base+0x2e0>  // b.hs, b.nlast
   37ed0:	b	37f20 <__gmpn_hgcd_reduce@@Base+0x350>
   37ed4:	ldr	x4, [x22, #32]
   37ed8:	mov	x0, x19
   37edc:	mov	x1, x8
   37ee0:	mov	x2, x20
   37ee4:	b	37f00 <__gmpn_hgcd_reduce@@Base+0x330>
   37ee8:	ldr	x4, [x22, #24]
   37eec:	mov	x0, x20
   37ef0:	mov	x1, x3
   37ef4:	mov	x2, x19
   37ef8:	mov	x3, x8
   37efc:	mov	x5, x26
   37f00:	bl	381dc <__gmpn_hgcd_reduce@@Base+0x60c>
   37f04:	mov	x19, x0
   37f08:	b	381a0 <__gmpn_hgcd_reduce@@Base+0x5d0>
   37f0c:	mov	x8, x20
   37f10:	ldr	x9, [x8]
   37f14:	adds	x9, x9, #0x1
   37f18:	str	x9, [x8], #8
   37f1c:	b.cs	37f10 <__gmpn_hgcd_reduce@@Base+0x340>  // b.hs, b.nlast
   37f20:	add	x2, x19, x24, lsl #3
   37f24:	mov	x0, x19
   37f28:	mov	x1, x19
   37f2c:	mov	x3, x27
   37f30:	bl	ca70 <__gmpn_add_n@plt>
   37f34:	mov	x23, x24
   37f38:	cbz	x0, 37f78 <__gmpn_hgcd_reduce@@Base+0x3a8>
   37f3c:	cmp	x27, x24
   37f40:	b.ge	37f60 <__gmpn_hgcd_reduce@@Base+0x390>  // b.tcont
   37f44:	lsl	x8, x27, #3
   37f48:	ldr	x9, [x19, x8]
   37f4c:	add	x27, x27, #0x1
   37f50:	adds	x9, x9, #0x1
   37f54:	str	x9, [x19, x8]
   37f58:	b.cs	37f3c <__gmpn_hgcd_reduce@@Base+0x36c>  // b.hs, b.nlast
   37f5c:	b	37f74 <__gmpn_hgcd_reduce@@Base+0x3a4>
   37f60:	mov	x8, x19
   37f64:	ldr	x9, [x8]
   37f68:	adds	x9, x9, #0x1
   37f6c:	str	x9, [x8], #8
   37f70:	b.cs	37f64 <__gmpn_hgcd_reduce@@Base+0x394>  // b.hs, b.nlast
   37f74:	mov	x23, x24
   37f78:	ldur	x27, [x29, #-8]
   37f7c:	ldr	x4, [x22, #40]
   37f80:	mov	x0, x25
   37f84:	mov	x1, x24
   37f88:	mov	x2, x20
   37f8c:	mov	x3, x23
   37f90:	mov	x5, x27
   37f94:	mov	x6, x28
   37f98:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   37f9c:	ldr	x4, [x22, #24]
   37fa0:	ldur	x0, [x29, #-48]
   37fa4:	mov	x1, x24
   37fa8:	mov	x2, x19
   37fac:	mov	x3, x23
   37fb0:	mov	x5, x26
   37fb4:	mov	x6, x28
   37fb8:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   37fbc:	add	x8, x27, x23
   37fc0:	cmp	x8, x24
   37fc4:	b.ge	37fe8 <__gmpn_hgcd_reduce@@Base+0x418>  // b.tcont
   37fc8:	sub	x8, x24, x23
   37fcc:	subs	x8, x8, x27
   37fd0:	b.eq	37fe8 <__gmpn_hgcd_reduce@@Base+0x418>  // b.none
   37fd4:	add	x9, x23, x27
   37fd8:	add	x0, x25, x9, lsl #3
   37fdc:	lsl	x2, x8, #3
   37fe0:	mov	w1, wzr
   37fe4:	bl	c5f0 <memset@plt>
   37fe8:	add	x8, x23, x26
   37fec:	cmp	x8, x24
   37ff0:	ldur	x8, [x29, #-56]
   37ff4:	add	x8, x8, #0x1
   37ff8:	stur	x8, [x29, #-56]
   37ffc:	b.ge	38024 <__gmpn_hgcd_reduce@@Base+0x454>  // b.tcont
   38000:	sub	x8, x24, x23
   38004:	subs	x8, x8, x26
   38008:	b.eq	38024 <__gmpn_hgcd_reduce@@Base+0x454>  // b.none
   3800c:	add	x9, x24, x23
   38010:	add	x9, x9, x26
   38014:	add	x0, x25, x9, lsl #3
   38018:	lsl	x2, x8, #3
   3801c:	mov	w1, wzr
   38020:	bl	c5f0 <memset@plt>
   38024:	ldur	x27, [x29, #-48]
   38028:	mov	x0, x25
   3802c:	mov	x1, x25
   38030:	mov	x3, x24
   38034:	mov	x2, x27
   38038:	bl	c2d0 <__gmpn_sub_n@plt>
   3803c:	ldr	x8, [x25]
   38040:	subs	x8, x8, x0
   38044:	str	x8, [x25]
   38048:	b.cs	38060 <__gmpn_hgcd_reduce@@Base+0x490>  // b.hs, b.nlast
   3804c:	add	x8, x25, #0x8
   38050:	ldr	x9, [x8]
   38054:	sub	x10, x9, #0x1
   38058:	str	x10, [x8], #8
   3805c:	cbz	x9, 38050 <__gmpn_hgcd_reduce@@Base+0x480>
   38060:	ldur	x26, [x29, #-16]
   38064:	ldr	x4, [x22, #32]
   38068:	mov	x0, x27
   3806c:	mov	x1, x24
   38070:	mov	x2, x20
   38074:	mov	x3, x23
   38078:	mov	x5, x26
   3807c:	mov	x6, x28
   38080:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   38084:	ldur	x2, [x29, #-56]
   38088:	mov	x0, x20
   3808c:	mov	x1, x25
   38090:	bl	ca50 <__gmpn_copyi@plt>
   38094:	ldr	x4, [x22, #16]
   38098:	mov	x0, x25
   3809c:	mov	x1, x24
   380a0:	mov	x2, x19
   380a4:	mov	x3, x23
   380a8:	mov	x5, x21
   380ac:	mov	x6, x28
   380b0:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   380b4:	add	x8, x26, x23
   380b8:	cmp	x8, x24
   380bc:	b.ge	380e4 <__gmpn_hgcd_reduce@@Base+0x514>  // b.tcont
   380c0:	sub	x8, x24, x23
   380c4:	subs	x8, x8, x26
   380c8:	b.eq	380e4 <__gmpn_hgcd_reduce@@Base+0x514>  // b.none
   380cc:	add	x9, x24, x23
   380d0:	add	x9, x9, x26
   380d4:	add	x0, x25, x9, lsl #3
   380d8:	lsl	x2, x8, #3
   380dc:	mov	w1, wzr
   380e0:	bl	c5f0 <memset@plt>
   380e4:	add	x8, x23, x21
   380e8:	cmp	x8, x24
   380ec:	b.ge	38110 <__gmpn_hgcd_reduce@@Base+0x540>  // b.tcont
   380f0:	sub	x8, x24, x23
   380f4:	subs	x8, x8, x21
   380f8:	b.eq	38110 <__gmpn_hgcd_reduce@@Base+0x540>  // b.none
   380fc:	add	x9, x23, x21
   38100:	add	x0, x25, x9, lsl #3
   38104:	lsl	x2, x8, #3
   38108:	mov	w1, wzr
   3810c:	bl	c5f0 <memset@plt>
   38110:	mov	x0, x25
   38114:	mov	x1, x25
   38118:	mov	x2, x27
   3811c:	mov	x3, x24
   38120:	bl	c2d0 <__gmpn_sub_n@plt>
   38124:	ldr	x8, [x25]
   38128:	subs	x8, x8, x0
   3812c:	str	x8, [x25]
   38130:	b.cs	38148 <__gmpn_hgcd_reduce@@Base+0x578>  // b.hs, b.nlast
   38134:	add	x8, x25, #0x8
   38138:	ldr	x9, [x8]
   3813c:	sub	x10, x9, #0x1
   38140:	str	x10, [x8], #8
   38144:	cbz	x9, 38138 <__gmpn_hgcd_reduce@@Base+0x568>
   38148:	ldur	x2, [x29, #-56]
   3814c:	mov	x0, x19
   38150:	mov	x1, x25
   38154:	bl	ca50 <__gmpn_copyi@plt>
   38158:	ldur	x8, [x29, #-64]
   3815c:	ldp	x10, x9, [x29, #-88]
   38160:	cmp	x8, x9
   38164:	csel	x8, x8, x9, lt  // lt = tstop
   38168:	ldur	x9, [x29, #-72]
   3816c:	cmp	x9, x10
   38170:	csel	x9, x9, x10, lt  // lt = tstop
   38174:	cmp	x8, x9
   38178:	csel	x8, x8, x9, gt
   3817c:	lsl	x9, x8, #3
   38180:	ldr	x10, [x20, x9]
   38184:	ldr	x9, [x19, x9]
   38188:	sub	x8, x8, #0x1
   3818c:	orr	x9, x9, x10
   38190:	cbz	x9, 3817c <__gmpn_hgcd_reduce@@Base+0x5ac>
   38194:	ldur	x0, [x29, #-40]
   38198:	add	x19, x8, #0x2
   3819c:	cbnz	x0, 381d4 <__gmpn_hgcd_reduce@@Base+0x604>
   381a0:	mov	x0, x19
   381a4:	mov	sp, x29
   381a8:	ldp	x20, x19, [sp, #80]
   381ac:	ldp	x22, x21, [sp, #64]
   381b0:	ldp	x24, x23, [sp, #48]
   381b4:	ldp	x26, x25, [sp, #32]
   381b8:	ldp	x28, x27, [sp, #16]
   381bc:	ldp	x29, x30, [sp], #96
   381c0:	ret
   381c4:	sub	x0, x29, #0x28
   381c8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   381cc:	mov	x25, x0
   381d0:	b	37e70 <__gmpn_hgcd_reduce@@Base+0x2a0>
   381d4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   381d8:	b	381a0 <__gmpn_hgcd_reduce@@Base+0x5d0>
   381dc:	stp	x29, x30, [sp, #-80]!
   381e0:	stp	x26, x25, [sp, #16]
   381e4:	stp	x24, x23, [sp, #32]
   381e8:	stp	x22, x21, [sp, #48]
   381ec:	stp	x20, x19, [sp, #64]
   381f0:	mov	x29, sp
   381f4:	sub	sp, sp, #0x10
   381f8:	add	x26, x5, x3
   381fc:	mov	x20, x1
   38200:	lsl	x1, x26, #3
   38204:	mov	w8, #0x7f00                	// #32512
   38208:	mov	x22, x5
   3820c:	mov	x23, x4
   38210:	mov	x19, x3
   38214:	mov	x24, x2
   38218:	mov	x21, x0
   3821c:	cmp	x1, x8
   38220:	stur	xzr, [x29, #-8]
   38224:	b.hi	382dc <__gmpn_hgcd_reduce@@Base+0x70c>  // b.pmore
   38228:	add	x9, x1, #0xf
   3822c:	mov	x8, sp
   38230:	and	x9, x9, #0xfffffffffffffff0
   38234:	sub	x25, x8, x9
   38238:	mov	sp, x25
   3823c:	mov	x0, x25
   38240:	mov	x1, x24
   38244:	mov	x2, x19
   38248:	mov	x3, x23
   3824c:	mov	x4, x22
   38250:	bl	ccd0 <__gmpn_mul@plt>
   38254:	cmp	x26, x20
   38258:	cset	w8, gt
   3825c:	subs	x22, x26, x8
   38260:	b.eq	3829c <__gmpn_hgcd_reduce@@Base+0x6cc>  // b.none
   38264:	mov	x0, x21
   38268:	mov	x1, x21
   3826c:	mov	x2, x25
   38270:	mov	x3, x22
   38274:	bl	c2d0 <__gmpn_sub_n@plt>
   38278:	cbz	x0, 3829c <__gmpn_hgcd_reduce@@Base+0x6cc>
   3827c:	cmp	x22, x20
   38280:	b.ge	3829c <__gmpn_hgcd_reduce@@Base+0x6cc>  // b.tcont
   38284:	lsl	x8, x22, #3
   38288:	ldr	x9, [x21, x8]
   3828c:	add	x22, x22, #0x1
   38290:	sub	x10, x9, #0x1
   38294:	str	x10, [x21, x8]
   38298:	cbz	x9, 3827c <__gmpn_hgcd_reduce@@Base+0x6ac>
   3829c:	ldur	x0, [x29, #-8]
   382a0:	cbnz	x0, 382ec <__gmpn_hgcd_reduce@@Base+0x71c>
   382a4:	sub	x8, x21, #0x8
   382a8:	mov	x0, x20
   382ac:	cmp	x20, x19
   382b0:	b.le	382c0 <__gmpn_hgcd_reduce@@Base+0x6f0>
   382b4:	ldr	x9, [x8, x0, lsl #3]
   382b8:	sub	x20, x0, #0x1
   382bc:	cbz	x9, 382a8 <__gmpn_hgcd_reduce@@Base+0x6d8>
   382c0:	mov	sp, x29
   382c4:	ldp	x20, x19, [sp, #64]
   382c8:	ldp	x22, x21, [sp, #48]
   382cc:	ldp	x24, x23, [sp, #32]
   382d0:	ldp	x26, x25, [sp, #16]
   382d4:	ldp	x29, x30, [sp], #80
   382d8:	ret
   382dc:	sub	x0, x29, #0x8
   382e0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   382e4:	mov	x25, x0
   382e8:	b	3823c <__gmpn_hgcd_reduce@@Base+0x66c>
   382ec:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   382f0:	b	382a4 <__gmpn_hgcd_reduce@@Base+0x6d4>

00000000000382f4 <__gmpn_hgcd_itch@@Base>:
   382f4:	cmp	x0, #0x65
   382f8:	b.lt	38354 <__gmpn_hgcd_itch@@Base+0x60>  // b.tstop
   382fc:	mov	x9, #0xd70b                	// #55051
   38300:	movk	x9, #0x70a3, lsl #16
   38304:	movk	x9, #0xa3d, lsl #32
   38308:	sub	x8, x0, #0x1
   3830c:	movk	x9, #0xa3d7, lsl #48
   38310:	smulh	x9, x8, x9
   38314:	add	x8, x9, x8
   38318:	add	x9, x0, #0x3
   3831c:	add	x11, x0, #0x6
   38320:	cmp	x9, #0x0
   38324:	csel	x9, x11, x9, lt  // lt = tstop
   38328:	asr	x11, x8, #6
   3832c:	add	x8, x11, x8, lsr #63
   38330:	mov	w10, #0x40                  	// #64
   38334:	clz	x8, x8
   38338:	sub	w8, w10, w8
   3833c:	mov	w10, #0x16                  	// #22
   38340:	mov	w11, #0x14                  	// #20
   38344:	asr	x9, x9, #2
   38348:	mul	w8, w8, w10
   3834c:	madd	x8, x9, x11, x8
   38350:	add	x0, x8, #0x65
   38354:	ret

0000000000038358 <__gmpn_hgcd@@Base>:
   38358:	sub	sp, sp, #0x90
   3835c:	cmp	x2, #0x0
   38360:	cinc	x8, x2, lt  // lt = tstop
   38364:	stp	x26, x25, [sp, #80]
   38368:	asr	x25, x8, #1
   3836c:	stp	x22, x21, [sp, #112]
   38370:	add	x22, x25, #0x1
   38374:	cmp	x22, x2
   38378:	stp	x29, x30, [sp, #48]
   3837c:	stp	x28, x27, [sp, #64]
   38380:	stp	x24, x23, [sp, #96]
   38384:	stp	x20, x19, [sp, #128]
   38388:	add	x29, sp, #0x30
   3838c:	b.ge	38424 <__gmpn_hgcd@@Base+0xcc>  // b.tcont
   38390:	mov	x19, x4
   38394:	mov	x20, x3
   38398:	mov	x24, x2
   3839c:	mov	x21, x1
   383a0:	mov	x23, x0
   383a4:	cmp	x2, #0x64
   383a8:	b.le	3842c <__gmpn_hgcd@@Base+0xd4>
   383ac:	add	x8, x24, x24, lsl #1
   383b0:	add	x9, x8, #0x3
   383b4:	cmp	x8, #0x0
   383b8:	csel	x8, x9, x8, lt  // lt = tstop
   383bc:	asr	x8, x8, #2
   383c0:	mov	x0, x20
   383c4:	mov	x1, x23
   383c8:	mov	x2, x21
   383cc:	mov	x3, x24
   383d0:	mov	x4, x25
   383d4:	mov	x5, x19
   383d8:	add	x26, x8, #0x1
   383dc:	bl	d2f0 <__gmpn_hgcd_reduce@plt>
   383e0:	cmp	x0, #0x0
   383e4:	cset	w8, ne  // ne = any
   383e8:	csel	x0, x24, x0, eq  // eq = none
   383ec:	mov	x24, x0
   383f0:	cmp	x0, x26
   383f4:	mov	w28, w8
   383f8:	b.le	3849c <__gmpn_hgcd@@Base+0x144>
   383fc:	mov	x0, x24
   38400:	mov	x1, x23
   38404:	mov	x2, x21
   38408:	mov	x3, x22
   3840c:	mov	x4, x20
   38410:	mov	x5, x19
   38414:	bl	c2b0 <__gmpn_hgcd_step@plt>
   38418:	mov	w8, #0x1                   	// #1
   3841c:	cbnz	x0, 383ec <__gmpn_hgcd@@Base+0x94>
   38420:	b	38474 <__gmpn_hgcd@@Base+0x11c>
   38424:	mov	x0, xzr
   38428:	b	3847c <__gmpn_hgcd@@Base+0x124>
   3842c:	mov	w28, wzr
   38430:	mov	x0, x24
   38434:	mov	x1, x23
   38438:	mov	x2, x21
   3843c:	mov	x3, x22
   38440:	mov	x4, x20
   38444:	mov	x5, x19
   38448:	bl	c2b0 <__gmpn_hgcd_step@plt>
   3844c:	cbz	x0, 38474 <__gmpn_hgcd@@Base+0x11c>
   38450:	mov	x1, x23
   38454:	mov	x2, x21
   38458:	mov	x3, x22
   3845c:	mov	x4, x20
   38460:	mov	x5, x19
   38464:	mov	x24, x0
   38468:	bl	c2b0 <__gmpn_hgcd_step@plt>
   3846c:	cbnz	x0, 38450 <__gmpn_hgcd@@Base+0xf8>
   38470:	mov	w28, #0x1                   	// #1
   38474:	cmp	w28, #0x0
   38478:	csel	x0, xzr, x24, eq  // eq = none
   3847c:	ldp	x20, x19, [sp, #128]
   38480:	ldp	x22, x21, [sp, #112]
   38484:	ldp	x24, x23, [sp, #96]
   38488:	ldp	x26, x25, [sp, #80]
   3848c:	ldp	x28, x27, [sp, #64]
   38490:	ldp	x29, x30, [sp, #48]
   38494:	add	sp, sp, #0x90
   38498:	ret
   3849c:	add	x8, x25, #0x3
   384a0:	cmp	x24, x8
   384a4:	b.le	38430 <__gmpn_hgcd@@Base+0xd8>
   384a8:	lsl	x8, x22, #1
   384ac:	sub	x8, x8, x24
   384b0:	add	x25, x8, #0x1
   384b4:	sub	x27, x24, x25
   384b8:	add	x8, x27, #0x1
   384bc:	add	x9, x27, #0x2
   384c0:	cmp	x8, #0x0
   384c4:	csinc	x8, x9, x27, lt  // lt = tstop
   384c8:	lsl	x8, x8, #4
   384cc:	mov	x0, sp
   384d0:	mov	x1, x27
   384d4:	mov	x2, x19
   384d8:	and	x26, x8, #0xffffffffffffffe0
   384dc:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   384e0:	add	x9, x26, x19
   384e4:	lsl	x8, x25, #3
   384e8:	add	x26, x9, #0x20
   384ec:	add	x0, x23, x8
   384f0:	add	x1, x21, x8
   384f4:	mov	x3, sp
   384f8:	mov	x2, x27
   384fc:	mov	x4, x26
   38500:	bl	cde0 <__gmpn_hgcd@plt>
   38504:	cmp	x0, #0x1
   38508:	b.lt	38430 <__gmpn_hgcd@@Base+0xd8>  // b.tstop
   3850c:	add	x1, x0, x25
   38510:	mov	x0, sp
   38514:	mov	x2, x23
   38518:	mov	x3, x21
   3851c:	mov	x4, x25
   38520:	mov	x5, x26
   38524:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   38528:	mov	x24, x0
   3852c:	mov	x1, sp
   38530:	mov	x0, x20
   38534:	mov	x2, x26
   38538:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   3853c:	mov	w28, #0x1                   	// #1
   38540:	b	38430 <__gmpn_hgcd@@Base+0xd8>

0000000000038544 <__gmpn_hgcd_appr_itch@@Base>:
   38544:	cmp	x0, #0x68
   38548:	b.lt	385a4 <__gmpn_hgcd_appr_itch@@Base+0x60>  // b.tstop
   3854c:	mov	x9, #0x13e3                	// #5091
   38550:	movk	x9, #0x2548, lsl #16
   38554:	movk	x9, #0x65e7, lsl #32
   38558:	sub	x8, x0, #0x1
   3855c:	movk	x9, #0x9f11, lsl #48
   38560:	smulh	x9, x8, x9
   38564:	add	x8, x9, x8
   38568:	add	x9, x0, #0x3
   3856c:	add	x11, x0, #0x6
   38570:	cmp	x9, #0x0
   38574:	csel	x9, x11, x9, lt  // lt = tstop
   38578:	asr	x11, x8, #6
   3857c:	add	x8, x11, x8, lsr #63
   38580:	mov	w10, #0x40                  	// #64
   38584:	clz	x8, x8
   38588:	sub	w8, w10, w8
   3858c:	mov	w10, #0x16                  	// #22
   38590:	mov	w11, #0x14                  	// #20
   38594:	asr	x9, x9, #2
   38598:	mul	w8, w8, w10
   3859c:	madd	x8, x9, x11, x8
   385a0:	add	x0, x8, #0x65
   385a4:	ret

00000000000385a8 <__gmpn_hgcd_appr@@Base>:
   385a8:	sub	sp, sp, #0x90
   385ac:	cmp	x2, #0x3
   385b0:	stp	x29, x30, [sp, #48]
   385b4:	stp	x28, x27, [sp, #64]
   385b8:	stp	x26, x25, [sp, #80]
   385bc:	stp	x24, x23, [sp, #96]
   385c0:	stp	x22, x21, [sp, #112]
   385c4:	stp	x20, x19, [sp, #128]
   385c8:	add	x29, sp, #0x30
   385cc:	b.ge	385d8 <__gmpn_hgcd_appr@@Base+0x30>  // b.tcont
   385d0:	mov	w24, wzr
   385d4:	b	388f8 <__gmpn_hgcd_appr@@Base+0x350>
   385d8:	lsr	x26, x2, #1
   385dc:	mov	x20, x4
   385e0:	mov	x19, x3
   385e4:	mov	x25, x2
   385e8:	mov	x21, x1
   385ec:	mov	x22, x0
   385f0:	cmp	x2, #0x67
   385f4:	add	x23, x26, #0x1
   385f8:	b.le	38674 <__gmpn_hgcd_appr@@Base+0xcc>
   385fc:	add	x8, x25, x25, lsl #1
   38600:	add	x9, x8, #0x3
   38604:	cmp	x8, #0x0
   38608:	csel	x8, x9, x8, lt  // lt = tstop
   3860c:	asr	x8, x8, #2
   38610:	mov	x0, x19
   38614:	mov	x1, x22
   38618:	mov	x2, x21
   3861c:	mov	x3, x25
   38620:	mov	x4, x26
   38624:	mov	x5, x20
   38628:	add	x27, x8, #0x1
   3862c:	bl	d2f0 <__gmpn_hgcd_reduce@plt>
   38630:	cmp	x0, #0x0
   38634:	cset	w8, ne  // ne = any
   38638:	csel	x25, x25, x0, eq  // eq = none
   3863c:	cmp	x25, x27
   38640:	mov	w24, w8
   38644:	b.le	38838 <__gmpn_hgcd_appr@@Base+0x290>
   38648:	mov	x0, x25
   3864c:	mov	x1, x22
   38650:	mov	x2, x21
   38654:	mov	x3, x23
   38658:	mov	x4, x19
   3865c:	mov	x5, x20
   38660:	bl	c2b0 <__gmpn_hgcd_step@plt>
   38664:	mov	x25, x0
   38668:	mov	w8, #0x1                   	// #1
   3866c:	cbnz	x0, 3863c <__gmpn_hgcd_appr@@Base+0x94>
   38670:	b	388f8 <__gmpn_hgcd_appr@@Base+0x350>
   38674:	mov	x0, x25
   38678:	mov	x1, x22
   3867c:	mov	x2, x21
   38680:	mov	x3, x23
   38684:	mov	x4, x19
   38688:	mov	x5, x20
   3868c:	bl	c2b0 <__gmpn_hgcd_step@plt>
   38690:	mov	w26, wzr
   38694:	cbnz	x0, 386d4 <__gmpn_hgcd_appr@@Base+0x12c>
   38698:	mov	w24, wzr
   3869c:	cbnz	w26, 38788 <__gmpn_hgcd_appr@@Base+0x1e0>
   386a0:	b	38808 <__gmpn_hgcd_appr@@Base+0x260>
   386a4:	mov	w26, wzr
   386a8:	mov	x25, x0
   386ac:	cmp	x25, #0x2
   386b0:	b.le	38780 <__gmpn_hgcd_appr@@Base+0x1d8>
   386b4:	mov	x0, x25
   386b8:	mov	x1, x22
   386bc:	mov	x2, x21
   386c0:	mov	x3, x23
   386c4:	mov	x4, x19
   386c8:	mov	x5, x20
   386cc:	bl	c2b0 <__gmpn_hgcd_step@plt>
   386d0:	cbz	x0, 38780 <__gmpn_hgcd_appr@@Base+0x1d8>
   386d4:	lsl	w8, w26, #1
   386d8:	add	x9, x8, x0, lsl #6
   386dc:	add	x9, x9, #0x40
   386e0:	cmp	x9, x23, lsl #7
   386e4:	b.gt	386a8 <__gmpn_hgcd_appr@@Base+0x100>
   386e8:	lsl	x9, x23, #1
   386ec:	sub	x9, x9, x0
   386f0:	lsl	x9, x9, #6
   386f4:	sub	x8, x9, x8
   386f8:	add	x9, x8, #0x3f
   386fc:	cmp	x8, #0x0
   38700:	csel	x8, x9, x8, lt  // lt = tstop
   38704:	cbz	w26, 38734 <__gmpn_hgcd_appr@@Base+0x18c>
   38708:	sub	w26, w26, #0x1
   3870c:	mov	x9, x23
   38710:	asr	x8, x8, #6
   38714:	lsl	x10, x8, #3
   38718:	sub	x25, x0, x8
   3871c:	add	x22, x22, x10
   38720:	add	x21, x21, x10
   38724:	sub	x23, x9, x8
   38728:	cmp	x25, #0x2
   3872c:	b.gt	386b4 <__gmpn_hgcd_appr@@Base+0x10c>
   38730:	b	38780 <__gmpn_hgcd_appr@@Base+0x1d8>
   38734:	add	x9, x23, #0x1
   38738:	cmp	x9, x0
   3873c:	b.eq	386a4 <__gmpn_hgcd_appr@@Base+0xfc>  // b.none
   38740:	sub	x10, x0, #0x1
   38744:	mov	x11, x10
   38748:	ldr	x12, [x22, x11, lsl #3]
   3874c:	cbnz	x12, 38760 <__gmpn_hgcd_appr@@Base+0x1b8>
   38750:	sub	x11, x11, #0x1
   38754:	cmp	x23, x11
   38758:	b.ne	38748 <__gmpn_hgcd_appr@@Base+0x1a0>  // b.any
   3875c:	b	386a4 <__gmpn_hgcd_appr@@Base+0xfc>
   38760:	ldr	x11, [x21, x10, lsl #3]
   38764:	cbnz	x11, 38778 <__gmpn_hgcd_appr@@Base+0x1d0>
   38768:	sub	x10, x10, #0x1
   3876c:	cmp	x23, x10
   38770:	b.ne	38760 <__gmpn_hgcd_appr@@Base+0x1b8>  // b.any
   38774:	b	386a4 <__gmpn_hgcd_appr@@Base+0xfc>
   38778:	mov	w26, #0x3f                  	// #63
   3877c:	b	38710 <__gmpn_hgcd_appr@@Base+0x168>
   38780:	mov	w24, #0x1                   	// #1
   38784:	cbz	w26, 38808 <__gmpn_hgcd_appr@@Base+0x260>
   38788:	mov	w8, #0x40                  	// #64
   3878c:	sub	w26, w8, w26
   38790:	mov	x0, x22
   38794:	mov	x1, x22
   38798:	mov	x2, x25
   3879c:	mov	w3, w26
   387a0:	bl	c1a0 <__gmpn_rshift@plt>
   387a4:	str	x0, [x22, #-8]!
   387a8:	mov	x0, x21
   387ac:	mov	x1, x21
   387b0:	mov	x2, x25
   387b4:	mov	w3, w26
   387b8:	bl	c1a0 <__gmpn_rshift@plt>
   387bc:	str	x0, [x21, #-8]!
   387c0:	lsl	x8, x25, #3
   387c4:	ldr	x9, [x22, x8]
   387c8:	ldr	x8, [x21, x8]
   387cc:	orr	x8, x8, x9
   387d0:	cmp	x8, #0x0
   387d4:	cinc	x25, x25, ne  // ne = any
   387d8:	cmp	x25, #0x3
   387dc:	b.lt	38808 <__gmpn_hgcd_appr@@Base+0x260>  // b.tstop
   387e0:	mov	x0, x25
   387e4:	mov	x1, x22
   387e8:	mov	x2, x21
   387ec:	mov	x3, x23
   387f0:	mov	x4, x19
   387f4:	mov	x5, x20
   387f8:	bl	c2b0 <__gmpn_hgcd_step@plt>
   387fc:	mov	x25, x0
   38800:	cbnz	x0, 387d8 <__gmpn_hgcd_appr@@Base+0x230>
   38804:	b	388f4 <__gmpn_hgcd_appr@@Base+0x34c>
   38808:	cmp	x25, #0x2
   3880c:	b.ne	388f8 <__gmpn_hgcd_appr@@Base+0x350>  // b.any
   38810:	ldp	x1, x0, [x22]
   38814:	ldp	x3, x2, [x21]
   38818:	mov	x4, sp
   3881c:	bl	c5a0 <__gmpn_hgcd2@plt>
   38820:	cbz	w0, 388f8 <__gmpn_hgcd_appr@@Base+0x350>
   38824:	mov	x1, sp
   38828:	mov	x0, x19
   3882c:	mov	x2, x20
   38830:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   38834:	b	388f4 <__gmpn_hgcd_appr@@Base+0x34c>
   38838:	add	x8, x26, #0x3
   3883c:	cmp	x25, x8
   38840:	b.le	388b8 <__gmpn_hgcd_appr@@Base+0x310>
   38844:	lsl	x8, x23, #1
   38848:	sub	x8, x8, x25
   3884c:	add	x26, x8, #0x1
   38850:	sub	x27, x25, x26
   38854:	add	x8, x27, #0x1
   38858:	add	x9, x27, #0x2
   3885c:	cmp	x8, #0x0
   38860:	csinc	x8, x9, x27, lt  // lt = tstop
   38864:	lsl	x8, x8, #4
   38868:	mov	x0, sp
   3886c:	mov	x1, x27
   38870:	mov	x2, x20
   38874:	and	x28, x8, #0xffffffffffffffe0
   38878:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   3887c:	add	x9, x28, x20
   38880:	lsl	x8, x26, #3
   38884:	add	x26, x9, #0x20
   38888:	add	x0, x22, x8
   3888c:	add	x1, x21, x8
   38890:	mov	x3, sp
   38894:	mov	x2, x27
   38898:	mov	x4, x26
   3889c:	bl	cd50 <__gmpn_hgcd_appr@plt>
   388a0:	cbz	w0, 388b8 <__gmpn_hgcd_appr@@Base+0x310>
   388a4:	mov	x1, sp
   388a8:	mov	x0, x19
   388ac:	mov	x2, x26
   388b0:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   388b4:	b	388f4 <__gmpn_hgcd_appr@@Base+0x34c>
   388b8:	mov	x0, x25
   388bc:	mov	x1, x22
   388c0:	mov	x2, x21
   388c4:	mov	x3, x23
   388c8:	mov	x4, x19
   388cc:	mov	x5, x20
   388d0:	bl	c2b0 <__gmpn_hgcd_step@plt>
   388d4:	cbz	x0, 388f8 <__gmpn_hgcd_appr@@Base+0x350>
   388d8:	mov	x1, x22
   388dc:	mov	x2, x21
   388e0:	mov	x3, x23
   388e4:	mov	x4, x19
   388e8:	mov	x5, x20
   388ec:	bl	c2b0 <__gmpn_hgcd_step@plt>
   388f0:	cbnz	x0, 388d8 <__gmpn_hgcd_appr@@Base+0x330>
   388f4:	mov	w24, #0x1                   	// #1
   388f8:	mov	w0, w24
   388fc:	ldp	x20, x19, [sp, #128]
   38900:	ldp	x22, x21, [sp, #112]
   38904:	ldp	x24, x23, [sp, #96]
   38908:	ldp	x26, x25, [sp, #80]
   3890c:	ldp	x28, x27, [sp, #64]
   38910:	ldp	x29, x30, [sp, #48]
   38914:	add	sp, sp, #0x90
   38918:	ret

000000000003891c <__gmpn_hgcd2_jacobi@@Base>:
   3891c:	sub	sp, sp, #0x80
   38920:	stp	x22, x21, [sp, #96]
   38924:	mov	x22, x0
   38928:	cmp	x0, #0x2
   3892c:	mov	w0, wzr
   38930:	stp	x29, x30, [sp, #32]
   38934:	stp	x28, x27, [sp, #48]
   38938:	stp	x26, x25, [sp, #64]
   3893c:	stp	x24, x23, [sp, #80]
   38940:	stp	x20, x19, [sp, #112]
   38944:	add	x29, sp, #0x20
   38948:	b.cc	38d50 <__gmpn_hgcd2_jacobi@@Base+0x434>  // b.lo, b.ul, b.last
   3894c:	mov	x21, x2
   38950:	cmp	x2, #0x2
   38954:	b.cc	38d50 <__gmpn_hgcd2_jacobi@@Base+0x434>  // b.lo, b.ul, b.last
   38958:	ldr	w8, [x5]
   3895c:	mov	x23, x3
   38960:	mov	x24, x1
   38964:	cmp	x22, x21
   38968:	b.hi	38978 <__gmpn_hgcd2_jacobi@@Base+0x5c>  // b.pmore
   3896c:	b.ne	389a0 <__gmpn_hgcd2_jacobi@@Base+0x84>  // b.any
   38970:	cmp	x24, x23
   38974:	b.ls	389a0 <__gmpn_hgcd2_jacobi@@Base+0x84>  // b.plast
   38978:	subs	x10, x24, x23
   3897c:	sbc	x22, x22, x21
   38980:	cmp	x22, #0x2
   38984:	b.cc	389b0 <__gmpn_hgcd2_jacobi@@Base+0x94>  // b.lo, b.ul, b.last
   38988:	stp	x4, x5, [sp]
   3898c:	mov	x26, xzr
   38990:	mov	w9, #0x5                   	// #5
   38994:	mov	w25, #0x1                   	// #1
   38998:	mov	x24, x10
   3899c:	b	389cc <__gmpn_hgcd2_jacobi@@Base+0xb0>
   389a0:	subs	x10, x23, x24
   389a4:	sbc	x21, x21, x22
   389a8:	cmp	x21, #0x2
   389ac:	b.cs	389b8 <__gmpn_hgcd2_jacobi@@Base+0x9c>  // b.hs, b.nlast
   389b0:	mov	w0, wzr
   389b4:	b	38d50 <__gmpn_hgcd2_jacobi@@Base+0x434>
   389b8:	mov	x25, xzr
   389bc:	mov	w9, #0x1                   	// #1
   389c0:	mov	x23, x10
   389c4:	mov	w26, #0x1                   	// #1
   389c8:	stp	x4, x5, [sp]
   389cc:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   389d0:	ldr	x20, [x20, #3872]
   389d4:	bfi	w9, w8, #3, #29
   389d8:	mov	w27, #0x1                   	// #1
   389dc:	cmp	x22, x21
   389e0:	ldrb	w19, [x20, w9, uxtw]
   389e4:	mov	w28, #0x1                   	// #1
   389e8:	b.cc	38a7c <__gmpn_hgcd2_jacobi@@Base+0x160>  // b.lo, b.ul, b.last
   389ec:	cmp	x22, x21
   389f0:	b.eq	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   389f4:	lsr	x8, x22, #32
   389f8:	cbz	x8, 38b04 <__gmpn_hgcd2_jacobi@@Base+0x1e8>
   389fc:	subs	x2, x24, x23
   38a00:	sbc	x22, x22, x21
   38a04:	cmp	x22, #0x2
   38a08:	b.cc	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.lo, b.ul, b.last
   38a0c:	cmp	x22, x21
   38a10:	b.ls	38a58 <__gmpn_hgcd2_jacobi@@Base+0x13c>  // b.plast
   38a14:	add	x0, sp, #0x10
   38a18:	mov	x1, x22
   38a1c:	mov	x3, x21
   38a20:	mov	x4, x23
   38a24:	bl	38d70 <__gmpn_hgcd2_jacobi@@Base+0x454>
   38a28:	ldr	x22, [sp, #24]
   38a2c:	cmp	x22, #0x2
   38a30:	b.cc	38b1c <__gmpn_hgcd2_jacobi@@Base+0x200>  // b.lo, b.ul, b.last
   38a34:	add	x8, x0, #0x1
   38a38:	and	w9, w8, #0x3
   38a3c:	bfi	w9, w19, #3, #8
   38a40:	orr	x9, x9, #0x4
   38a44:	ldr	x2, [sp, #16]
   38a48:	ldrb	w19, [x20, x9]
   38a4c:	mul	x9, x8, x26
   38a50:	mul	x8, x8, x27
   38a54:	b	38a70 <__gmpn_hgcd2_jacobi@@Base+0x154>
   38a58:	lsl	w8, w19, #3
   38a5c:	mov	w9, #0x5                   	// #5
   38a60:	orr	x8, x8, x9
   38a64:	ldrb	w19, [x20, x8]
   38a68:	mov	x8, x27
   38a6c:	mov	x9, x26
   38a70:	add	x28, x9, x28
   38a74:	add	x25, x8, x25
   38a78:	mov	x24, x2
   38a7c:	cmp	x22, x21
   38a80:	b.eq	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   38a84:	lsr	x8, x21, #32
   38a88:	cbz	x8, 38b10 <__gmpn_hgcd2_jacobi@@Base+0x1f4>
   38a8c:	mov	x8, x23
   38a90:	subs	x23, x8, x24
   38a94:	sbc	x21, x21, x22
   38a98:	cmp	x21, #0x2
   38a9c:	b.cc	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.lo, b.ul, b.last
   38aa0:	cmp	x21, x22
   38aa4:	b.ls	38aec <__gmpn_hgcd2_jacobi@@Base+0x1d0>  // b.plast
   38aa8:	add	x0, sp, #0x10
   38aac:	mov	x1, x21
   38ab0:	mov	x2, x23
   38ab4:	mov	x3, x22
   38ab8:	mov	x4, x24
   38abc:	bl	38d70 <__gmpn_hgcd2_jacobi@@Base+0x454>
   38ac0:	ldr	x21, [sp, #24]
   38ac4:	cmp	x21, #0x2
   38ac8:	b.cc	38b38 <__gmpn_hgcd2_jacobi@@Base+0x21c>  // b.lo, b.ul, b.last
   38acc:	add	x8, x0, #0x1
   38ad0:	and	w9, w8, #0x3
   38ad4:	bfi	w9, w19, #3, #29
   38ad8:	ldr	x23, [sp, #16]
   38adc:	ldrb	w19, [x20, w9, uxtw]
   38ae0:	madd	x26, x8, x28, x26
   38ae4:	madd	x27, x8, x25, x27
   38ae8:	b	389ec <__gmpn_hgcd2_jacobi@@Base+0xd0>
   38aec:	lsl	w8, w19, #3
   38af0:	orr	x8, x8, #0x1
   38af4:	ldrb	w19, [x20, x8]
   38af8:	add	x27, x25, x27
   38afc:	add	x26, x28, x26
   38b00:	b	389ec <__gmpn_hgcd2_jacobi@@Base+0xd0>
   38b04:	extr	x8, x22, x24, #32
   38b08:	extr	x9, x21, x23, #32
   38b0c:	b	38b50 <__gmpn_hgcd2_jacobi@@Base+0x234>
   38b10:	extr	x8, x22, x24, #32
   38b14:	extr	x9, x21, x23, #32
   38b18:	b	38c30 <__gmpn_hgcd2_jacobi@@Base+0x314>
   38b1c:	and	w8, w0, #0x3
   38b20:	bfi	w8, w19, #3, #8
   38b24:	orr	x8, x8, #0x4
   38b28:	ldrb	w19, [x20, x8]
   38b2c:	madd	x28, x0, x26, x28
   38b30:	madd	x25, x0, x27, x25
   38b34:	b	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>
   38b38:	and	w8, w0, #0x3
   38b3c:	bfi	w8, w19, #3, #29
   38b40:	ldrb	w19, [x20, w8, uxtw]
   38b44:	madd	x26, x0, x28, x26
   38b48:	madd	x27, x0, x25, x27
   38b4c:	b	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>
   38b50:	subs	x8, x8, x9
   38b54:	b.eq	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   38b58:	lsr	x10, x8, #33
   38b5c:	cbz	x10, 38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>
   38b60:	cmp	x8, x9
   38b64:	b.ls	38bb0 <__gmpn_hgcd2_jacobi@@Base+0x294>  // b.plast
   38b68:	tbnz	x8, #63, 38bc4 <__gmpn_hgcd2_jacobi@@Base+0x2a8>
   38b6c:	mov	w11, wzr
   38b70:	mov	x12, x9
   38b74:	lsl	x12, x12, #1
   38b78:	cmp	x12, x8
   38b7c:	sub	w11, w11, #0x1
   38b80:	b.ls	38b74 <__gmpn_hgcd2_jacobi@@Base+0x258>  // b.plast
   38b84:	mov	x13, xzr
   38b88:	lsr	x12, x12, #1
   38b8c:	cmp	x8, x12
   38b90:	cset	w10, cs  // cs = hs, nlast
   38b94:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   38b98:	bfi	x10, x13, #1, #63
   38b9c:	adds	w11, w11, #0x1
   38ba0:	sub	x8, x8, x14
   38ba4:	mov	x13, x10
   38ba8:	b.cc	38b88 <__gmpn_hgcd2_jacobi@@Base+0x26c>  // b.lo, b.ul, b.last
   38bac:	b	38c04 <__gmpn_hgcd2_jacobi@@Base+0x2e8>
   38bb0:	mov	w10, #0x5                   	// #5
   38bb4:	bfi	w10, w19, #3, #8
   38bb8:	mov	x11, x27
   38bbc:	mov	x12, x26
   38bc0:	b	38c24 <__gmpn_hgcd2_jacobi@@Base+0x308>
   38bc4:	mov	w11, #0x1                   	// #1
   38bc8:	mov	x12, x9
   38bcc:	tbnz	x9, #63, 38bdc <__gmpn_hgcd2_jacobi@@Base+0x2c0>
   38bd0:	lsl	x12, x12, #1
   38bd4:	add	w11, w11, #0x1
   38bd8:	tbz	x12, #63, 38bd0 <__gmpn_hgcd2_jacobi@@Base+0x2b4>
   38bdc:	mov	x13, xzr
   38be0:	cmp	x8, x12
   38be4:	cset	w10, cs  // cs = hs, nlast
   38be8:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   38bec:	bfi	x10, x13, #1, #63
   38bf0:	sub	x8, x8, x14
   38bf4:	subs	w11, w11, #0x1
   38bf8:	lsr	x12, x12, #1
   38bfc:	mov	x13, x10
   38c00:	b.ne	38be0 <__gmpn_hgcd2_jacobi@@Base+0x2c4>  // b.any
   38c04:	lsr	x11, x8, #33
   38c08:	cbz	x11, 38d0c <__gmpn_hgcd2_jacobi@@Base+0x3f0>
   38c0c:	add	x11, x10, #0x1
   38c10:	and	w10, w11, #0x3
   38c14:	bfi	w10, w19, #3, #8
   38c18:	mul	x12, x11, x26
   38c1c:	orr	w10, w10, #0x4
   38c20:	mul	x11, x11, x27
   38c24:	ldrb	w19, [x20, w10, uxtw]
   38c28:	add	x28, x12, x28
   38c2c:	add	x25, x11, x25
   38c30:	subs	x9, x9, x8
   38c34:	b.eq	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>  // b.none
   38c38:	lsr	x10, x9, #33
   38c3c:	cbz	x10, 38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>
   38c40:	cmp	x9, x8
   38c44:	b.ls	38c90 <__gmpn_hgcd2_jacobi@@Base+0x374>  // b.plast
   38c48:	tbnz	x9, #63, 38ca8 <__gmpn_hgcd2_jacobi@@Base+0x38c>
   38c4c:	mov	w10, wzr
   38c50:	mov	x12, x8
   38c54:	lsl	x12, x12, #1
   38c58:	cmp	x12, x9
   38c5c:	sub	w10, w10, #0x1
   38c60:	b.ls	38c54 <__gmpn_hgcd2_jacobi@@Base+0x338>  // b.plast
   38c64:	mov	x13, xzr
   38c68:	lsr	x12, x12, #1
   38c6c:	cmp	x9, x12
   38c70:	cset	w11, cs  // cs = hs, nlast
   38c74:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   38c78:	bfi	x11, x13, #1, #63
   38c7c:	adds	w10, w10, #0x1
   38c80:	sub	x9, x9, x14
   38c84:	mov	x13, x11
   38c88:	b.cc	38c68 <__gmpn_hgcd2_jacobi@@Base+0x34c>  // b.lo, b.ul, b.last
   38c8c:	b	38ce8 <__gmpn_hgcd2_jacobi@@Base+0x3cc>
   38c90:	lsl	w10, w19, #3
   38c94:	orr	x10, x10, #0x1
   38c98:	ldrb	w19, [x20, x10]
   38c9c:	add	x27, x25, x27
   38ca0:	add	x26, x28, x26
   38ca4:	b	38b50 <__gmpn_hgcd2_jacobi@@Base+0x234>
   38ca8:	mov	w10, #0x1                   	// #1
   38cac:	mov	x12, x8
   38cb0:	tbnz	x8, #63, 38cc0 <__gmpn_hgcd2_jacobi@@Base+0x3a4>
   38cb4:	lsl	x12, x12, #1
   38cb8:	add	w10, w10, #0x1
   38cbc:	tbz	x12, #63, 38cb4 <__gmpn_hgcd2_jacobi@@Base+0x398>
   38cc0:	mov	x13, xzr
   38cc4:	cmp	x9, x12
   38cc8:	cset	w11, cs  // cs = hs, nlast
   38ccc:	csel	x14, xzr, x12, cc  // cc = lo, ul, last
   38cd0:	bfi	x11, x13, #1, #63
   38cd4:	sub	x9, x9, x14
   38cd8:	subs	w10, w10, #0x1
   38cdc:	lsr	x12, x12, #1
   38ce0:	mov	x13, x11
   38ce4:	b.ne	38cc4 <__gmpn_hgcd2_jacobi@@Base+0x3a8>  // b.any
   38ce8:	lsr	x10, x9, #33
   38cec:	cbz	x10, 38d28 <__gmpn_hgcd2_jacobi@@Base+0x40c>
   38cf0:	add	x10, x11, #0x1
   38cf4:	and	w11, w10, #0x3
   38cf8:	bfi	w11, w19, #3, #29
   38cfc:	ldrb	w19, [x20, w11, uxtw]
   38d00:	madd	x26, x10, x28, x26
   38d04:	madd	x27, x10, x25, x27
   38d08:	b	38b50 <__gmpn_hgcd2_jacobi@@Base+0x234>
   38d0c:	and	w8, w10, #0x3
   38d10:	bfi	w8, w19, #3, #8
   38d14:	orr	x8, x8, #0x4
   38d18:	ldrb	w19, [x20, x8]
   38d1c:	madd	x28, x10, x26, x28
   38d20:	madd	x25, x10, x27, x25
   38d24:	b	38d3c <__gmpn_hgcd2_jacobi@@Base+0x420>
   38d28:	and	w8, w11, #0x3
   38d2c:	bfi	w8, w19, #3, #29
   38d30:	ldrb	w19, [x20, w8, uxtw]
   38d34:	madd	x26, x11, x28, x26
   38d38:	madd	x27, x11, x25, x27
   38d3c:	ldp	x9, x8, [sp]
   38d40:	mov	w0, #0x1                   	// #1
   38d44:	stp	x27, x25, [x9]
   38d48:	stp	x26, x28, [x9, #16]
   38d4c:	str	w19, [x8]
   38d50:	ldp	x20, x19, [sp, #112]
   38d54:	ldp	x22, x21, [sp, #96]
   38d58:	ldp	x24, x23, [sp, #80]
   38d5c:	ldp	x26, x25, [sp, #64]
   38d60:	ldp	x28, x27, [sp, #48]
   38d64:	ldp	x29, x30, [sp, #32]
   38d68:	add	sp, sp, #0x80
   38d6c:	ret
   38d70:	tbnz	x1, #63, 38dec <__gmpn_hgcd2_jacobi@@Base+0x4d0>
   38d74:	mov	w9, wzr
   38d78:	b	38d88 <__gmpn_hgcd2_jacobi@@Base+0x46c>
   38d7c:	extr	x3, x3, x4, #63
   38d80:	lsl	x4, x4, #1
   38d84:	sub	w9, w9, #0x1
   38d88:	cmp	x3, x1
   38d8c:	b.cc	38d7c <__gmpn_hgcd2_jacobi@@Base+0x460>  // b.lo, b.ul, b.last
   38d90:	b.ne	38d9c <__gmpn_hgcd2_jacobi@@Base+0x480>  // b.any
   38d94:	cmp	x4, x2
   38d98:	b.ls	38d7c <__gmpn_hgcd2_jacobi@@Base+0x460>  // b.plast
   38d9c:	mov	x8, xzr
   38da0:	cbnz	w9, 38dc8 <__gmpn_hgcd2_jacobi@@Base+0x4ac>
   38da4:	stp	x2, x1, [x0]
   38da8:	mov	x0, x8
   38dac:	ret
   38db0:	subs	x10, x2, x4
   38db4:	sbc	x1, x1, x3
   38db8:	orr	x8, x8, #0x1
   38dbc:	mov	x2, x10
   38dc0:	adds	w9, w9, #0x1
   38dc4:	b.cs	38da4 <__gmpn_hgcd2_jacobi@@Base+0x488>  // b.hs, b.nlast
   38dc8:	extr	x4, x3, x4, #1
   38dcc:	lsr	x3, x3, #1
   38dd0:	cmp	x1, x3
   38dd4:	lsl	x8, x8, #1
   38dd8:	b.hi	38db0 <__gmpn_hgcd2_jacobi@@Base+0x494>  // b.pmore
   38ddc:	b.ne	38dc0 <__gmpn_hgcd2_jacobi@@Base+0x4a4>  // b.any
   38de0:	cmp	x2, x4
   38de4:	b.cs	38db0 <__gmpn_hgcd2_jacobi@@Base+0x494>  // b.hs, b.nlast
   38de8:	b	38dc0 <__gmpn_hgcd2_jacobi@@Base+0x4a4>
   38dec:	mov	w9, #0x1                   	// #1
   38df0:	tbnz	x3, #63, 38e04 <__gmpn_hgcd2_jacobi@@Base+0x4e8>
   38df4:	extr	x3, x3, x4, #63
   38df8:	lsl	x4, x4, #1
   38dfc:	add	w9, w9, #0x1
   38e00:	tbz	x3, #63, 38df4 <__gmpn_hgcd2_jacobi@@Base+0x4d8>
   38e04:	mov	x8, xzr
   38e08:	b	38e2c <__gmpn_hgcd2_jacobi@@Base+0x510>
   38e0c:	subs	x10, x2, x4
   38e10:	sbc	x1, x1, x3
   38e14:	orr	x8, x8, #0x1
   38e18:	mov	x2, x10
   38e1c:	extr	x4, x3, x4, #1
   38e20:	subs	w9, w9, #0x1
   38e24:	lsr	x3, x3, #1
   38e28:	b.eq	38da4 <__gmpn_hgcd2_jacobi@@Base+0x488>  // b.none
   38e2c:	cmp	x1, x3
   38e30:	lsl	x8, x8, #1
   38e34:	b.hi	38e0c <__gmpn_hgcd2_jacobi@@Base+0x4f0>  // b.pmore
   38e38:	b.ne	38e1c <__gmpn_hgcd2_jacobi@@Base+0x500>  // b.any
   38e3c:	cmp	x2, x4
   38e40:	b.cs	38e0c <__gmpn_hgcd2_jacobi@@Base+0x4f0>  // b.hs, b.nlast
   38e44:	b	38e1c <__gmpn_hgcd2_jacobi@@Base+0x500>

0000000000038e48 <__gmpn_hgcd_jacobi@@Base>:
   38e48:	sub	sp, sp, #0xa0
   38e4c:	cmp	x2, #0x0
   38e50:	cinc	x8, x2, lt  // lt = tstop
   38e54:	stp	x26, x25, [sp, #96]
   38e58:	asr	x26, x8, #1
   38e5c:	stp	x24, x23, [sp, #112]
   38e60:	add	x23, x26, #0x1
   38e64:	cmp	x23, x2
   38e68:	stp	x29, x30, [sp, #64]
   38e6c:	stp	x28, x27, [sp, #80]
   38e70:	stp	x22, x21, [sp, #128]
   38e74:	stp	x20, x19, [sp, #144]
   38e78:	add	x29, sp, #0x40
   38e7c:	b.ge	38f04 <__gmpn_hgcd_jacobi@@Base+0xbc>  // b.tcont
   38e80:	mov	x19, x5
   38e84:	mov	x20, x4
   38e88:	mov	x21, x3
   38e8c:	mov	x25, x2
   38e90:	mov	x22, x1
   38e94:	mov	x24, x0
   38e98:	cmp	x2, #0x64
   38e9c:	b.le	38f0c <__gmpn_hgcd_jacobi@@Base+0xc4>
   38ea0:	add	x8, x25, x25, lsl #1
   38ea4:	lsl	x9, x26, #3
   38ea8:	add	x10, x8, #0x3
   38eac:	cmp	x8, #0x0
   38eb0:	add	x0, x24, x9
   38eb4:	add	x1, x22, x9
   38eb8:	csel	x8, x10, x8, lt  // lt = tstop
   38ebc:	sub	x2, x25, x26
   38ec0:	mov	x3, x21
   38ec4:	mov	x4, x20
   38ec8:	mov	x5, x19
   38ecc:	asr	x27, x8, #2
   38ed0:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   38ed4:	cmp	x0, #0x1
   38ed8:	b.lt	38f60 <__gmpn_hgcd_jacobi@@Base+0x118>  // b.tstop
   38edc:	add	x1, x0, x26
   38ee0:	mov	x0, x21
   38ee4:	mov	x2, x24
   38ee8:	mov	x3, x22
   38eec:	mov	x4, x26
   38ef0:	mov	x5, x19
   38ef4:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   38ef8:	mov	x25, x0
   38efc:	mov	w8, #0x1                   	// #1
   38f00:	b	38f64 <__gmpn_hgcd_jacobi@@Base+0x11c>
   38f04:	mov	x0, xzr
   38f08:	b	38fac <__gmpn_hgcd_jacobi@@Base+0x164>
   38f0c:	mov	w27, wzr
   38f10:	mov	x0, x25
   38f14:	mov	x1, x24
   38f18:	mov	x2, x22
   38f1c:	mov	x3, x23
   38f20:	mov	x4, x21
   38f24:	mov	x5, x20
   38f28:	mov	x6, x19
   38f2c:	bl	39084 <__gmpn_hgcd_jacobi@@Base+0x23c>
   38f30:	cbz	x0, 38fa4 <__gmpn_hgcd_jacobi@@Base+0x15c>
   38f34:	mov	x1, x24
   38f38:	mov	x2, x22
   38f3c:	mov	x3, x23
   38f40:	mov	x4, x21
   38f44:	mov	x5, x20
   38f48:	mov	x6, x19
   38f4c:	mov	x25, x0
   38f50:	bl	39084 <__gmpn_hgcd_jacobi@@Base+0x23c>
   38f54:	cbnz	x0, 38f34 <__gmpn_hgcd_jacobi@@Base+0xec>
   38f58:	mov	w27, #0x1                   	// #1
   38f5c:	b	38fa4 <__gmpn_hgcd_jacobi@@Base+0x15c>
   38f60:	mov	w8, wzr
   38f64:	add	x28, x27, #0x1
   38f68:	mov	x0, x25
   38f6c:	mov	x25, x0
   38f70:	cmp	x0, x28
   38f74:	mov	w27, w8
   38f78:	b.le	38fcc <__gmpn_hgcd_jacobi@@Base+0x184>
   38f7c:	mov	x0, x25
   38f80:	mov	x1, x24
   38f84:	mov	x2, x22
   38f88:	mov	x3, x23
   38f8c:	mov	x4, x21
   38f90:	mov	x5, x20
   38f94:	mov	x6, x19
   38f98:	bl	39084 <__gmpn_hgcd_jacobi@@Base+0x23c>
   38f9c:	mov	w8, #0x1                   	// #1
   38fa0:	cbnz	x0, 38f6c <__gmpn_hgcd_jacobi@@Base+0x124>
   38fa4:	cmp	w27, #0x0
   38fa8:	csel	x0, xzr, x25, eq  // eq = none
   38fac:	ldp	x20, x19, [sp, #144]
   38fb0:	ldp	x22, x21, [sp, #128]
   38fb4:	ldp	x24, x23, [sp, #112]
   38fb8:	ldp	x26, x25, [sp, #96]
   38fbc:	ldp	x28, x27, [sp, #80]
   38fc0:	ldp	x29, x30, [sp, #64]
   38fc4:	add	sp, sp, #0xa0
   38fc8:	ret
   38fcc:	add	x8, x26, #0x3
   38fd0:	cmp	x25, x8
   38fd4:	b.le	38f10 <__gmpn_hgcd_jacobi@@Base+0xc8>
   38fd8:	lsl	x8, x23, #1
   38fdc:	sub	x8, x8, x25
   38fe0:	add	x26, x8, #0x1
   38fe4:	sub	x28, x25, x26
   38fe8:	add	x8, x28, #0x1
   38fec:	add	x9, x28, #0x2
   38ff0:	cmp	x8, #0x0
   38ff4:	csinc	x8, x9, x28, lt  // lt = tstop
   38ff8:	lsl	x8, x8, #4
   38ffc:	and	x8, x8, #0xffffffffffffffe0
   39000:	add	x0, sp, #0x10
   39004:	mov	x1, x28
   39008:	mov	x2, x19
   3900c:	str	x8, [sp, #8]
   39010:	bl	c830 <__gmpn_hgcd_matrix_init@plt>
   39014:	ldr	x9, [sp, #8]
   39018:	lsl	x8, x26, #3
   3901c:	add	x0, x24, x8
   39020:	add	x1, x22, x8
   39024:	add	x9, x9, x19
   39028:	add	x8, x9, #0x20
   3902c:	add	x3, sp, #0x10
   39030:	mov	x2, x28
   39034:	mov	x4, x20
   39038:	mov	x5, x8
   3903c:	mov	x28, x8
   39040:	bl	d390 <__gmpn_hgcd_jacobi@plt>
   39044:	cmp	x0, #0x1
   39048:	b.lt	38f10 <__gmpn_hgcd_jacobi@@Base+0xc8>  // b.tstop
   3904c:	add	x1, x0, x26
   39050:	add	x0, sp, #0x10
   39054:	mov	x2, x24
   39058:	mov	x3, x22
   3905c:	mov	x4, x26
   39060:	mov	x5, x28
   39064:	bl	c8a0 <__gmpn_hgcd_matrix_adjust@plt>
   39068:	mov	x25, x0
   3906c:	add	x1, sp, #0x10
   39070:	mov	x0, x21
   39074:	mov	x2, x28
   39078:	bl	cfa0 <__gmpn_hgcd_matrix_mul@plt>
   3907c:	mov	w27, #0x1                   	// #1
   39080:	b	38f10 <__gmpn_hgcd_jacobi@@Base+0xc8>
   39084:	sub	sp, sp, #0x80
   39088:	lsl	x8, x0, #3
   3908c:	stp	x29, x30, [sp, #48]
   39090:	stp	x24, x23, [sp, #80]
   39094:	stp	x22, x21, [sp, #96]
   39098:	stp	x20, x19, [sp, #112]
   3909c:	sub	x9, x8, #0x8
   390a0:	mov	x20, x2
   390a4:	mov	x21, x0
   390a8:	ldr	x0, [x1, x9]
   390ac:	ldr	x2, [x2, x9]
   390b0:	add	x9, x3, #0x1
   390b4:	str	x25, [sp, #64]
   390b8:	mov	x19, x6
   390bc:	mov	x25, x5
   390c0:	mov	x23, x4
   390c4:	mov	x22, x1
   390c8:	mov	x24, x3
   390cc:	cmp	x9, x21
   390d0:	orr	x9, x2, x0
   390d4:	add	x29, sp, #0x30
   390d8:	b.ne	390e8 <__gmpn_hgcd_jacobi@@Base+0x2a0>  // b.any
   390dc:	cmp	x9, #0x4
   390e0:	b.cs	39140 <__gmpn_hgcd_jacobi@@Base+0x2f8>  // b.hs, b.nlast
   390e4:	b	39198 <__gmpn_hgcd_jacobi@@Base+0x350>
   390e8:	tbnz	x9, #63, 39140 <__gmpn_hgcd_jacobi@@Base+0x2f8>
   390ec:	sub	x10, x8, #0x10
   390f0:	sub	x8, x8, #0x18
   390f4:	ldr	x12, [x22, x10]
   390f8:	ldr	x14, [x22, x8]
   390fc:	ldr	x10, [x20, x10]
   39100:	ldr	x8, [x20, x8]
   39104:	clz	x9, x9
   39108:	neg	x13, x9
   3910c:	lsl	x11, x0, x9
   39110:	lsl	x15, x2, x9
   39114:	lsr	x16, x12, x13
   39118:	lsl	x12, x12, x9
   3911c:	lsr	x14, x14, x13
   39120:	lsl	x9, x10, x9
   39124:	lsr	x10, x10, x13
   39128:	lsr	x8, x8, x13
   3912c:	orr	x0, x16, x11
   39130:	orr	x1, x14, x12
   39134:	orr	x2, x10, x15
   39138:	orr	x3, x8, x9
   3913c:	b	3914c <__gmpn_hgcd_jacobi@@Base+0x304>
   39140:	sub	x8, x8, #0x10
   39144:	ldr	x1, [x22, x8]
   39148:	ldr	x3, [x20, x8]
   3914c:	add	x4, sp, #0x10
   39150:	mov	x5, x25
   39154:	bl	caf0 <__gmpn_hgcd2_jacobi@plt>
   39158:	cbz	w0, 39198 <__gmpn_hgcd_jacobi@@Base+0x350>
   3915c:	add	x1, sp, #0x10
   39160:	mov	x0, x23
   39164:	mov	x2, x19
   39168:	bl	c780 <__gmpn_hgcd_matrix_mul_1@plt>
   3916c:	mov	x0, x19
   39170:	mov	x1, x22
   39174:	mov	x2, x21
   39178:	bl	ca50 <__gmpn_copyi@plt>
   3917c:	add	x0, sp, #0x10
   39180:	mov	x1, x22
   39184:	mov	x2, x19
   39188:	mov	x3, x20
   3918c:	mov	x4, x21
   39190:	bl	c4f0 <__gmpn_matrix22_mul1_inverse_vector@plt>
   39194:	b	391c0 <__gmpn_hgcd_jacobi@@Base+0x378>
   39198:	adrp	x4, 39000 <__gmpn_hgcd_jacobi@@Base+0x1b8>
   3919c:	add	x4, x4, #0x1dc
   391a0:	mov	x5, sp
   391a4:	mov	x0, x22
   391a8:	mov	x1, x20
   391ac:	mov	x2, x21
   391b0:	mov	x3, x24
   391b4:	mov	x6, x19
   391b8:	stp	x23, x25, [sp]
   391bc:	bl	d2b0 <__gmpn_gcd_subdiv_step@plt>
   391c0:	ldp	x20, x19, [sp, #112]
   391c4:	ldp	x22, x21, [sp, #96]
   391c8:	ldp	x24, x23, [sp, #80]
   391cc:	ldr	x25, [sp, #64]
   391d0:	ldp	x29, x30, [sp, #48]
   391d4:	add	sp, sp, #0x80
   391d8:	ret
   391dc:	stp	x29, x30, [sp, #-48]!
   391e0:	add	x9, x3, x4, lsl #3
   391e4:	str	x21, [sp, #16]
   391e8:	stp	x20, x19, [sp, #32]
   391ec:	mov	w19, w5
   391f0:	mov	x8, x4
   391f4:	mov	x20, x3
   391f8:	mov	x21, x0
   391fc:	add	x4, x9, #0x8
   39200:	mov	x29, sp
   39204:	subs	x8, x8, #0x1
   39208:	b.lt	39254 <__gmpn_hgcd_jacobi@@Base+0x40c>  // b.tstop
   3920c:	ldur	x9, [x4, #-16]
   39210:	sub	x4, x4, #0x8
   39214:	cbz	x9, 39204 <__gmpn_hgcd_jacobi@@Base+0x3bc>
   39218:	ldr	x0, [x21]
   3921c:	add	x2, x8, #0x1
   39220:	mov	x1, x20
   39224:	mov	w3, w19
   39228:	bl	d020 <__gmpn_hgcd_matrix_update_q@plt>
   3922c:	ldr	x8, [x21, #8]
   39230:	ldr	w10, [x20]
   39234:	ldr	w9, [x8]
   39238:	lsl	w9, w9, #3
   3923c:	add	w9, w9, w19, lsl #2
   39240:	bfxil	w9, w10, #0, #2
   39244:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   39248:	ldr	x10, [x10, #3872]
   3924c:	ldrb	w9, [x10, w9, uxtw]
   39250:	str	w9, [x8]
   39254:	ldp	x20, x19, [sp, #32]
   39258:	ldr	x21, [sp, #16]
   3925c:	ldp	x29, x30, [sp], #48
   39260:	ret

0000000000039264 <__gmpn_mullo_n@@Base>:
   39264:	stp	x29, x30, [sp, #-64]!
   39268:	stp	x22, x21, [sp, #32]
   3926c:	stp	x20, x19, [sp, #48]
   39270:	mov	x19, x3
   39274:	mov	x20, x2
   39278:	mov	x22, x1
   3927c:	cmp	x3, #0x25
   39280:	mov	x21, x0
   39284:	str	x23, [sp, #16]
   39288:	mov	x29, sp
   3928c:	b.le	392f0 <__gmpn_mullo_n@@Base+0x8c>
   39290:	lsl	x1, x19, #4
   39294:	mov	w8, #0x7f00                	// #32512
   39298:	cmp	x1, x8
   3929c:	str	xzr, [x29, #24]
   392a0:	b.hi	39350 <__gmpn_mullo_n@@Base+0xec>  // b.pmore
   392a4:	add	x9, x1, #0xf
   392a8:	mov	x8, sp
   392ac:	and	x9, x9, #0xfffffffffffffff0
   392b0:	sub	x23, x8, x9
   392b4:	mov	sp, x23
   392b8:	mov	w8, #0x186c                	// #6252
   392bc:	cmp	x19, x8
   392c0:	b.le	39318 <__gmpn_mullo_n@@Base+0xb4>
   392c4:	mov	x0, x23
   392c8:	mov	x1, x22
   392cc:	mov	x2, x19
   392d0:	mov	x3, x20
   392d4:	mov	x4, x19
   392d8:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   392dc:	mov	x0, x21
   392e0:	mov	x1, x23
   392e4:	mov	x2, x19
   392e8:	bl	ca50 <__gmpn_copyi@plt>
   392ec:	b	39330 <__gmpn_mullo_n@@Base+0xcc>
   392f0:	mov	x0, x21
   392f4:	mov	x1, x22
   392f8:	mov	x2, x20
   392fc:	mov	x3, x19
   39300:	mov	sp, x29
   39304:	ldp	x20, x19, [sp, #48]
   39308:	ldp	x22, x21, [sp, #32]
   3930c:	ldr	x23, [sp, #16]
   39310:	ldp	x29, x30, [sp], #64
   39314:	b	d290 <__gmpn_mullo_basecase@plt>
   39318:	mov	x0, x21
   3931c:	mov	x1, x22
   39320:	mov	x2, x20
   39324:	mov	x3, x19
   39328:	mov	x4, x23
   3932c:	bl	39368 <__gmpn_mullo_n@@Base+0x104>
   39330:	ldr	x0, [x29, #24]
   39334:	cbnz	x0, 39360 <__gmpn_mullo_n@@Base+0xfc>
   39338:	mov	sp, x29
   3933c:	ldp	x20, x19, [sp, #48]
   39340:	ldp	x22, x21, [sp, #32]
   39344:	ldr	x23, [sp, #16]
   39348:	ldp	x29, x30, [sp], #64
   3934c:	ret
   39350:	add	x0, x29, #0x18
   39354:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   39358:	mov	x23, x0
   3935c:	b	392b8 <__gmpn_mullo_n@@Base+0x54>
   39360:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   39364:	b	39338 <__gmpn_mullo_n@@Base+0xd4>
   39368:	stp	x29, x30, [sp, #-80]!
   3936c:	stp	x24, x23, [sp, #32]
   39370:	stp	x22, x21, [sp, #48]
   39374:	stp	x20, x19, [sp, #64]
   39378:	mov	x21, x4
   3937c:	mov	x24, x3
   39380:	mov	x20, x2
   39384:	mov	x19, x1
   39388:	cmp	x3, #0x45
   3938c:	mov	x23, x0
   39390:	str	x25, [sp, #16]
   39394:	mov	x29, sp
   39398:	b.le	393c0 <__gmpn_mullo_n@@Base+0x15c>
   3939c:	cmp	x24, #0x68
   393a0:	b.le	393e4 <__gmpn_mullo_n@@Base+0x180>
   393a4:	cmp	x24, #0x105
   393a8:	b.le	39400 <__gmpn_mullo_n@@Base+0x19c>
   393ac:	mov	x8, #0xcccccccccccccccc    	// #-3689348814741910324
   393b0:	movk	x8, #0xcccd
   393b4:	umulh	x8, x24, x8
   393b8:	lsr	x22, x8, #3
   393bc:	b	39428 <__gmpn_mullo_n@@Base+0x1c4>
   393c0:	mov	x9, #0xe38f                	// #58255
   393c4:	movk	x9, #0x8e38, lsl #16
   393c8:	mov	w8, #0xb                   	// #11
   393cc:	movk	x9, #0x38e3, lsl #32
   393d0:	mul	x8, x24, x8
   393d4:	movk	x9, #0xe38e, lsl #48
   393d8:	umulh	x8, x8, x9
   393dc:	lsr	x22, x8, #5
   393e0:	b	39428 <__gmpn_mullo_n@@Base+0x1c4>
   393e4:	add	w8, w24, w24, lsl #3
   393e8:	mov	w9, #0xcccd                	// #52429
   393ec:	and	w8, w8, #0xffff
   393f0:	movk	w9, #0xcccc, lsl #16
   393f4:	umull	x8, w8, w9
   393f8:	lsr	x22, x8, #37
   393fc:	b	39428 <__gmpn_mullo_n@@Base+0x1c4>
   39400:	lsl	w8, w24, #3
   39404:	sub	w8, w8, w24
   39408:	mov	w9, #0x41a5                	// #16805
   3940c:	and	w8, w8, #0xffff
   39410:	movk	w9, #0xa41a, lsl #16
   39414:	umull	x9, w8, w9
   39418:	lsr	x9, x9, #32
   3941c:	sub	w8, w8, w9
   39420:	add	w8, w9, w8, lsr #1
   39424:	lsr	w22, w8, #5
   39428:	sub	x25, x24, x22
   3942c:	mov	x0, x21
   39430:	mov	x1, x19
   39434:	mov	x2, x20
   39438:	mov	x3, x25
   3943c:	bl	c990 <__gmpn_mul_n@plt>
   39440:	mov	x0, x23
   39444:	mov	x1, x21
   39448:	mov	x2, x25
   3944c:	bl	ca50 <__gmpn_copyi@plt>
   39450:	add	x24, x21, x24, lsl #3
   39454:	cmp	x22, #0x25
   39458:	add	x1, x19, x25, lsl #3
   3945c:	mov	x0, x24
   39460:	mov	x2, x20
   39464:	mov	x3, x22
   39468:	b.ls	39478 <__gmpn_mullo_n@@Base+0x214>  // b.plast
   3946c:	mov	x4, x24
   39470:	bl	39368 <__gmpn_mullo_n@@Base+0x104>
   39474:	b	3947c <__gmpn_mullo_n@@Base+0x218>
   39478:	bl	d290 <__gmpn_mullo_basecase@plt>
   3947c:	lsl	x25, x25, #3
   39480:	add	x23, x23, x25
   39484:	add	x1, x21, x25
   39488:	mov	x0, x23
   3948c:	mov	x2, x24
   39490:	mov	x3, x22
   39494:	bl	ca70 <__gmpn_add_n@plt>
   39498:	cmp	x22, #0x25
   3949c:	add	x2, x20, x25
   394a0:	mov	x0, x24
   394a4:	mov	x1, x19
   394a8:	mov	x3, x22
   394ac:	b.ls	394bc <__gmpn_mullo_n@@Base+0x258>  // b.plast
   394b0:	mov	x4, x24
   394b4:	bl	39368 <__gmpn_mullo_n@@Base+0x104>
   394b8:	b	394c0 <__gmpn_mullo_n@@Base+0x25c>
   394bc:	bl	d290 <__gmpn_mullo_basecase@plt>
   394c0:	mov	x0, x23
   394c4:	mov	x1, x23
   394c8:	mov	x2, x24
   394cc:	mov	x3, x22
   394d0:	ldp	x20, x19, [sp, #64]
   394d4:	ldp	x22, x21, [sp, #48]
   394d8:	ldp	x24, x23, [sp, #32]
   394dc:	ldr	x25, [sp, #16]
   394e0:	ldp	x29, x30, [sp], #80
   394e4:	b	ca70 <__gmpn_add_n@plt>

00000000000394e8 <__gmpn_mullo_basecase@@Base>:
   394e8:	stp	x29, x30, [sp, #-80]!
   394ec:	stp	x24, x23, [sp, #32]
   394f0:	stp	x22, x21, [sp, #48]
   394f4:	stp	x20, x19, [sp, #64]
   394f8:	mov	x22, x2
   394fc:	subs	x2, x3, #0x1
   39500:	ldr	x8, [x1]
   39504:	ldr	x9, [x22, x2, lsl #3]
   39508:	mov	x19, x0
   3950c:	str	x25, [sp, #16]
   39510:	mov	x29, sp
   39514:	mul	x24, x9, x8
   39518:	b.eq	39584 <__gmpn_mullo_basecase@@Base+0x9c>  // b.none
   3951c:	ldr	x23, [x22]
   39520:	ldr	x25, [x1, x2, lsl #3]
   39524:	mov	x21, x3
   39528:	mov	x0, x19
   3952c:	mov	x3, x23
   39530:	mov	x20, x1
   39534:	bl	d490 <__gmpn_mul_1@plt>
   39538:	add	x8, x0, x24
   3953c:	cmp	x21, #0x3
   39540:	madd	x24, x25, x23, x8
   39544:	add	x19, x19, #0x8
   39548:	b.lt	39584 <__gmpn_mullo_basecase@@Base+0x9c>  // b.tstop
   3954c:	sub	x21, x21, #0x2
   39550:	add	x23, x22, #0x8
   39554:	ldr	x22, [x23], #8
   39558:	ldr	x25, [x20, x21, lsl #3]
   3955c:	mov	x0, x19
   39560:	mov	x1, x20
   39564:	mov	x2, x21
   39568:	mov	x3, x22
   3956c:	bl	d400 <__gmpn_addmul_1@plt>
   39570:	add	x8, x0, x24
   39574:	subs	x21, x21, #0x1
   39578:	madd	x24, x25, x22, x8
   3957c:	add	x19, x19, #0x8
   39580:	b.gt	39554 <__gmpn_mullo_basecase@@Base+0x6c>
   39584:	str	x24, [x19]
   39588:	ldp	x20, x19, [sp, #64]
   3958c:	ldp	x22, x21, [sp, #48]
   39590:	ldp	x24, x23, [sp, #32]
   39594:	ldr	x25, [sp, #16]
   39598:	ldp	x29, x30, [sp], #80
   3959c:	ret

00000000000395a0 <__gmpn_sqrlo@@Base>:
   395a0:	stp	x29, x30, [sp, #-80]!
   395a4:	stp	x22, x21, [sp, #48]
   395a8:	stp	x20, x19, [sp, #64]
   395ac:	mov	x21, x2
   395b0:	mov	x20, x1
   395b4:	cmp	x2, #0x3
   395b8:	mov	x19, x0
   395bc:	str	x25, [sp, #16]
   395c0:	stp	x24, x23, [sp, #32]
   395c4:	mov	x29, sp
   395c8:	b.le	39634 <__gmpn_sqrlo@@Base+0x94>
   395cc:	cmp	x21, #0x42
   395d0:	b.le	39660 <__gmpn_sqrlo@@Base+0xc0>
   395d4:	lsl	x1, x21, #4
   395d8:	mov	w8, #0x7f00                	// #32512
   395dc:	cmp	x1, x8
   395e0:	str	xzr, [x29, #24]
   395e4:	b.hi	397a4 <__gmpn_sqrlo@@Base+0x204>  // b.pmore
   395e8:	add	x9, x1, #0xf
   395ec:	mov	x8, sp
   395f0:	and	x9, x9, #0xfffffffffffffff0
   395f4:	sub	x22, x8, x9
   395f8:	mov	sp, x22
   395fc:	mov	w8, #0x1477                	// #5239
   39600:	cmp	x21, x8
   39604:	b.le	39688 <__gmpn_sqrlo@@Base+0xe8>
   39608:	mov	x0, x22
   3960c:	mov	x1, x20
   39610:	mov	x2, x21
   39614:	mov	x3, x20
   39618:	mov	x4, x21
   3961c:	bl	cca0 <__gmpn_nussbaumer_mul@plt>
   39620:	mov	x0, x19
   39624:	mov	x1, x22
   39628:	mov	x2, x21
   3962c:	bl	ca50 <__gmpn_copyi@plt>
   39630:	b	39780 <__gmpn_sqrlo@@Base+0x1e0>
   39634:	mov	x0, x19
   39638:	mov	x1, x20
   3963c:	mov	x2, x20
   39640:	mov	x3, x21
   39644:	mov	sp, x29
   39648:	ldp	x20, x19, [sp, #64]
   3964c:	ldp	x22, x21, [sp, #48]
   39650:	ldp	x24, x23, [sp, #32]
   39654:	ldr	x25, [sp, #16]
   39658:	ldp	x29, x30, [sp], #80
   3965c:	b	d290 <__gmpn_mullo_basecase@plt>
   39660:	mov	x0, x19
   39664:	mov	x1, x20
   39668:	mov	x2, x21
   3966c:	mov	sp, x29
   39670:	ldp	x20, x19, [sp, #64]
   39674:	ldp	x22, x21, [sp, #48]
   39678:	ldp	x24, x23, [sp, #32]
   3967c:	ldr	x25, [sp, #16]
   39680:	ldp	x29, x30, [sp], #80
   39684:	b	c0b0 <__gmpn_sqrlo_basecase@plt>
   39688:	cmp	x21, #0x5f
   3968c:	b.le	396b0 <__gmpn_sqrlo@@Base+0x110>
   39690:	cmp	x21, #0xd5
   39694:	b.le	396d0 <__gmpn_sqrlo@@Base+0x130>
   39698:	cmp	x21, #0x171
   3969c:	b.le	396ec <__gmpn_sqrlo@@Base+0x14c>
   396a0:	mov	w9, #0xcccd                	// #52429
   396a4:	and	w8, w21, #0xffff
   396a8:	movk	w9, #0xcccc, lsl #16
   396ac:	b	396c4 <__gmpn_sqrlo@@Base+0x124>
   396b0:	mov	w8, #0xb                   	// #11
   396b4:	mul	w8, w21, w8
   396b8:	mov	w9, #0x8e39                	// #36409
   396bc:	and	w8, w8, #0xffff
   396c0:	movk	w9, #0x38e3, lsl #16
   396c4:	umull	x8, w8, w9
   396c8:	lsr	x8, x8, #35
   396cc:	b	39714 <__gmpn_sqrlo@@Base+0x174>
   396d0:	add	w8, w21, w21, lsl #3
   396d4:	mov	w9, #0xcccd                	// #52429
   396d8:	and	w8, w8, #0xffff
   396dc:	movk	w9, #0xcccc, lsl #16
   396e0:	umull	x8, w8, w9
   396e4:	lsr	x8, x8, #37
   396e8:	b	39714 <__gmpn_sqrlo@@Base+0x174>
   396ec:	lsl	w8, w21, #3
   396f0:	sub	w8, w8, w21
   396f4:	mov	w9, #0x41a5                	// #16805
   396f8:	and	w8, w8, #0xffff
   396fc:	movk	w9, #0xa41a, lsl #16
   39700:	umull	x9, w8, w9
   39704:	lsr	x9, x9, #32
   39708:	sub	w8, w8, w9
   3970c:	add	w8, w9, w8, lsr #1
   39710:	lsr	w8, w8, #5
   39714:	and	x23, x8, #0xffff
   39718:	sub	x24, x21, x23
   3971c:	mov	x0, x22
   39720:	mov	x1, x20
   39724:	mov	x2, x24
   39728:	and	w25, w8, #0xffff
   3972c:	bl	c8e0 <__gmpn_sqr@plt>
   39730:	mov	x0, x19
   39734:	mov	x1, x22
   39738:	mov	x2, x24
   3973c:	bl	ca50 <__gmpn_copyi@plt>
   39740:	add	x21, x22, x21, lsl #3
   39744:	cmp	w25, #0x25
   39748:	add	x1, x20, x24, lsl #3
   3974c:	mov	x0, x21
   39750:	mov	x2, x20
   39754:	mov	x3, x23
   39758:	b.ls	39764 <__gmpn_sqrlo@@Base+0x1c4>  // b.plast
   3975c:	bl	cec0 <__gmpn_mullo_n@plt>
   39760:	b	39768 <__gmpn_sqrlo@@Base+0x1c8>
   39764:	bl	d290 <__gmpn_mullo_basecase@plt>
   39768:	lsl	x8, x24, #3
   3976c:	add	x0, x19, x8
   39770:	add	x1, x22, x8
   39774:	mov	x2, x21
   39778:	mov	x3, x23
   3977c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   39780:	ldr	x0, [x29, #24]
   39784:	cbnz	x0, 397b4 <__gmpn_sqrlo@@Base+0x214>
   39788:	mov	sp, x29
   3978c:	ldp	x20, x19, [sp, #64]
   39790:	ldp	x22, x21, [sp, #48]
   39794:	ldp	x24, x23, [sp, #32]
   39798:	ldr	x25, [sp, #16]
   3979c:	ldp	x29, x30, [sp], #80
   397a0:	ret
   397a4:	add	x0, x29, #0x18
   397a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   397ac:	mov	x22, x0
   397b0:	b	395fc <__gmpn_sqrlo@@Base+0x5c>
   397b4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   397b8:	b	39788 <__gmpn_sqrlo@@Base+0x1e8>

00000000000397bc <__gmpn_sqrlo_basecase@@Base>:
   397bc:	stp	x29, x30, [sp, #-96]!
   397c0:	stp	x28, x27, [sp, #16]
   397c4:	stp	x26, x25, [sp, #32]
   397c8:	stp	x24, x23, [sp, #48]
   397cc:	stp	x22, x21, [sp, #64]
   397d0:	stp	x20, x19, [sp, #80]
   397d4:	mov	x29, sp
   397d8:	sub	sp, sp, #0x240
   397dc:	ldr	x24, [x1]
   397e0:	sub	x19, x2, #0x1
   397e4:	ldr	x20, [x1, x19, lsl #3]
   397e8:	sub	x25, x2, #0x2
   397ec:	mov	x27, x2
   397f0:	mov	x23, x1
   397f4:	mov	x21, x0
   397f8:	add	x1, x1, #0x8
   397fc:	add	x0, sp, #0x28
   39800:	mov	x2, x25
   39804:	mov	x3, x24
   39808:	add	x22, sp, #0x28
   3980c:	bl	d490 <__gmpn_mul_1@plt>
   39810:	cmp	x27, #0x5
   39814:	madd	x28, x20, x24, x0
   39818:	b.lt	398ac <__gmpn_sqrlo_basecase@@Base+0xf0>  // b.tstop
   3981c:	add	x8, x23, x27, lsl #3
   39820:	stp	x25, x27, [sp, #8]
   39824:	stp	x23, x21, [sp, #24]
   39828:	mov	x20, xzr
   3982c:	add	x24, x22, #0x10
   39830:	sub	x25, x27, #0x4
   39834:	add	x26, x23, #0x10
   39838:	sub	x23, x8, #0x10
   3983c:	mov	w22, #0x3                   	// #3
   39840:	ldur	x27, [x26, #-8]
   39844:	mov	x21, x19
   39848:	ldr	x19, [x23, x20, lsl #3]
   3984c:	mov	x0, x24
   39850:	mov	x1, x26
   39854:	mov	x2, x25
   39858:	mov	x3, x27
   3985c:	bl	d400 <__gmpn_addmul_1@plt>
   39860:	add	x8, x0, x28
   39864:	add	x22, x22, #0x2
   39868:	add	x24, x24, #0x10
   3986c:	sub	x25, x25, #0x2
   39870:	add	x26, x26, #0x8
   39874:	madd	x28, x19, x27, x8
   39878:	mov	x19, x21
   3987c:	cmp	x22, x21
   39880:	sub	x20, x20, #0x1
   39884:	b.lt	39840 <__gmpn_sqrlo_basecase@@Base+0x84>  // b.tstop
   39888:	ldp	x23, x21, [sp, #24]
   3988c:	ldp	x25, x27, [sp, #8]
   39890:	mov	w8, #0x1                   	// #1
   39894:	sub	x8, x8, x20
   39898:	tbz	w19, #0, 398b4 <__gmpn_sqrlo_basecase@@Base+0xf8>
   3989c:	add	x8, x23, x8, lsl #3
   398a0:	ldp	x9, x8, [x8]
   398a4:	mul	x8, x8, x9
   398a8:	b	398b8 <__gmpn_sqrlo_basecase@@Base+0xfc>
   398ac:	mov	w8, #0x1                   	// #1
   398b0:	tbnz	w19, #0, 3989c <__gmpn_sqrlo_basecase@@Base+0xe0>
   398b4:	mov	x8, xzr
   398b8:	add	x8, x8, x28
   398bc:	add	x9, sp, #0x28
   398c0:	cmp	x27, #0x2
   398c4:	str	x8, [x9, x25, lsl #3]
   398c8:	asr	x8, x27, #1
   398cc:	b.lt	398f8 <__gmpn_sqrlo_basecase@@Base+0x13c>  // b.tstop
   398d0:	mov	x9, xzr
   398d4:	add	x10, x21, #0x8
   398d8:	ldr	x11, [x23, x9, lsl #3]
   398dc:	add	x9, x9, #0x1
   398e0:	umulh	x12, x11, x11
   398e4:	cmp	x9, x8
   398e8:	mul	x11, x11, x11
   398ec:	stp	x11, x12, [x10, #-8]
   398f0:	add	x10, x10, #0x10
   398f4:	b.lt	398d8 <__gmpn_sqrlo_basecase@@Base+0x11c>  // b.tstop
   398f8:	tbz	w27, #0, 39908 <__gmpn_sqrlo_basecase@@Base+0x14c>
   398fc:	ldr	x8, [x23, x8, lsl #3]
   39900:	mul	x8, x8, x8
   39904:	str	x8, [x21, x19, lsl #3]
   39908:	add	x0, x21, #0x8
   3990c:	add	x2, sp, #0x28
   39910:	mov	x1, x0
   39914:	mov	x3, x19
   39918:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3991c:	add	sp, sp, #0x240
   39920:	ldp	x20, x19, [sp, #80]
   39924:	ldp	x22, x21, [sp, #64]
   39928:	ldp	x24, x23, [sp, #48]
   3992c:	ldp	x26, x25, [sp, #32]
   39930:	ldp	x28, x27, [sp, #16]
   39934:	ldp	x29, x30, [sp], #96
   39938:	ret

000000000003993c <__gmpn_toom22_mul@@Base>:
   3993c:	sub	sp, sp, #0x80
   39940:	stp	x22, x21, [sp, #96]
   39944:	asr	x21, x2, #1
   39948:	sub	x22, x2, x21
   3994c:	stp	x29, x30, [sp, #32]
   39950:	stp	x28, x27, [sp, #48]
   39954:	stp	x26, x25, [sp, #64]
   39958:	stp	x24, x23, [sp, #80]
   3995c:	stp	x20, x19, [sp, #112]
   39960:	add	x29, sp, #0x20
   39964:	mov	x28, x4
   39968:	mov	x26, x3
   3996c:	mov	x20, x2
   39970:	mov	x27, x1
   39974:	mov	x19, x0
   39978:	sub	x25, x4, x22
   3997c:	add	x24, x0, x22, lsl #3
   39980:	subs	x12, x21, x22
   39984:	add	x2, x1, x21, lsl #3
   39988:	stur	x5, [x29, #-8]
   3998c:	str	x12, [sp, #8]
   39990:	b.ne	39a90 <__gmpn_toom22_mul@@Base+0x154>  // b.any
   39994:	lsl	x8, x21, #4
   39998:	sub	x8, x8, #0x8
   3999c:	mov	x10, x21
   399a0:	subs	x9, x10, #0x1
   399a4:	b.lt	399c8 <__gmpn_toom22_mul@@Base+0x8c>  // b.tstop
   399a8:	add	x10, x27, x10, lsl #3
   399ac:	ldur	x10, [x10, #-8]
   399b0:	ldr	x11, [x27, x8]
   399b4:	sub	x8, x8, #0x8
   399b8:	cmp	x10, x11
   399bc:	mov	x10, x9
   399c0:	b.eq	399a0 <__gmpn_toom22_mul@@Base+0x64>  // b.none
   399c4:	b.ls	39e94 <__gmpn_toom22_mul@@Base+0x558>  // b.plast
   399c8:	mov	x0, x19
   399cc:	mov	x1, x27
   399d0:	mov	x3, x21
   399d4:	bl	c2d0 <__gmpn_sub_n@plt>
   399d8:	stur	wzr, [x29, #-12]
   399dc:	subs	x23, x22, x25
   399e0:	b.eq	39ac0 <__gmpn_toom22_mul@@Base+0x184>  // b.none
   399e4:	lsl	x9, x20, #3
   399e8:	add	x8, x28, x21, lsl #1
   399ec:	sub	x9, x9, x21, lsl #3
   399f0:	sub	x8, x8, x20, lsl #1
   399f4:	sub	x9, x9, #0x8
   399f8:	ldr	x10, [x26, x9]
   399fc:	cbnz	x10, 39a44 <__gmpn_toom22_mul@@Base+0x108>
   39a00:	adds	x8, x8, #0x1
   39a04:	sub	x9, x9, #0x8
   39a08:	b.cc	399f8 <__gmpn_toom22_mul@@Base+0xbc>  // b.lo, b.ul, b.last
   39a0c:	lsl	x8, x28, #3
   39a10:	add	x1, x26, x22, lsl #3
   39a14:	sub	x8, x8, #0x8
   39a18:	mov	x10, x25
   39a1c:	subs	x9, x10, #0x1
   39a20:	b.lt	39a44 <__gmpn_toom22_mul@@Base+0x108>  // b.tstop
   39a24:	add	x10, x26, x10, lsl #3
   39a28:	ldur	x10, [x10, #-8]
   39a2c:	ldr	x11, [x26, x8]
   39a30:	sub	x8, x8, #0x8
   39a34:	cmp	x10, x11
   39a38:	mov	x10, x9
   39a3c:	b.eq	39a1c <__gmpn_toom22_mul@@Base+0xe0>  // b.none
   39a40:	b.ls	39ee8 <__gmpn_toom22_mul@@Base+0x5ac>  // b.plast
   39a44:	cbz	x25, 39b80 <__gmpn_toom22_mul@@Base+0x244>
   39a48:	add	x2, x26, x22, lsl #3
   39a4c:	mov	x0, x24
   39a50:	mov	x1, x26
   39a54:	mov	x3, x25
   39a58:	bl	c2d0 <__gmpn_sub_n@plt>
   39a5c:	ldur	x23, [x29, #-8]
   39a60:	mov	x8, x25
   39a64:	cbz	x0, 39b88 <__gmpn_toom22_mul@@Base+0x24c>
   39a68:	add	x9, x19, x28, lsl #3
   39a6c:	mov	x8, x25
   39a70:	cmp	x8, x22
   39a74:	b.ge	39c38 <__gmpn_toom22_mul@@Base+0x2fc>  // b.tcont
   39a78:	ldr	x10, [x26, x8, lsl #3]
   39a7c:	add	x8, x8, #0x1
   39a80:	sub	x11, x10, #0x1
   39a84:	str	x11, [x9], #8
   39a88:	cbz	x10, 39a70 <__gmpn_toom22_mul@@Base+0x134>
   39a8c:	b	39b88 <__gmpn_toom22_mul@@Base+0x24c>
   39a90:	ldr	x23, [x2]
   39a94:	cbz	x23, 39b30 <__gmpn_toom22_mul@@Base+0x1f4>
   39a98:	add	x2, x27, x22, lsl #3
   39a9c:	mov	x0, x19
   39aa0:	mov	x1, x27
   39aa4:	mov	x3, x21
   39aa8:	bl	c2d0 <__gmpn_sub_n@plt>
   39aac:	sub	x8, x23, x0
   39ab0:	stur	wzr, [x29, #-12]
   39ab4:	str	x8, [x19, x21, lsl #3]
   39ab8:	subs	x23, x22, x25
   39abc:	b.ne	399e4 <__gmpn_toom22_mul@@Base+0xa8>  // b.any
   39ac0:	ldur	x23, [x29, #-8]
   39ac4:	lsl	x9, x20, #4
   39ac8:	add	x2, x26, x22, lsl #3
   39acc:	sub	x8, x26, #0x8
   39ad0:	sub	x9, x9, x21, lsl #4
   39ad4:	mov	x10, x22
   39ad8:	subs	x11, x10, #0x1
   39adc:	b.lt	39afc <__gmpn_toom22_mul@@Base+0x1c0>  // b.tstop
   39ae0:	ldr	x10, [x8, x10, lsl #3]
   39ae4:	ldr	x12, [x8, x9]
   39ae8:	sub	x9, x9, #0x8
   39aec:	cmp	x10, x12
   39af0:	mov	x10, x11
   39af4:	b.eq	39ad8 <__gmpn_toom22_mul@@Base+0x19c>  // b.none
   39af8:	b.ls	39ebc <__gmpn_toom22_mul@@Base+0x580>  // b.plast
   39afc:	mov	x0, x24
   39b00:	mov	x1, x26
   39b04:	mov	x3, x22
   39b08:	bl	c2d0 <__gmpn_sub_n@plt>
   39b0c:	cmp	x22, #0xd
   39b10:	b.gt	39c40 <__gmpn_toom22_mul@@Base+0x304>
   39b14:	mov	x0, x23
   39b18:	mov	x1, x19
   39b1c:	mov	x2, x22
   39b20:	mov	x3, x24
   39b24:	mov	x4, x22
   39b28:	bl	c550 <__gmpn_mul_basecase@plt>
   39b2c:	b	39c5c <__gmpn_toom22_mul@@Base+0x320>
   39b30:	lsl	x8, x20, #3
   39b34:	add	x1, x27, x22, lsl #3
   39b38:	sub	x8, x8, #0x8
   39b3c:	mov	x10, x21
   39b40:	subs	x9, x10, #0x1
   39b44:	b.lt	39a98 <__gmpn_toom22_mul@@Base+0x15c>  // b.tstop
   39b48:	add	x10, x27, x10, lsl #3
   39b4c:	ldur	x10, [x10, #-8]
   39b50:	ldr	x11, [x27, x8]
   39b54:	sub	x8, x8, #0x8
   39b58:	cmp	x10, x11
   39b5c:	mov	x10, x9
   39b60:	b.eq	39b40 <__gmpn_toom22_mul@@Base+0x204>  // b.none
   39b64:	b.hi	39a98 <__gmpn_toom22_mul@@Base+0x15c>  // b.pmore
   39b68:	mov	x0, x19
   39b6c:	mov	x2, x27
   39b70:	mov	x3, x21
   39b74:	bl	c2d0 <__gmpn_sub_n@plt>
   39b78:	str	xzr, [x19, x21, lsl #3]
   39b7c:	b	39ea8 <__gmpn_toom22_mul@@Base+0x56c>
   39b80:	ldur	x23, [x29, #-8]
   39b84:	mov	x8, xzr
   39b88:	cmp	x24, x26
   39b8c:	b.eq	39c38 <__gmpn_toom22_mul@@Base+0x2fc>  // b.none
   39b90:	cmp	x8, x22
   39b94:	b.ge	39c38 <__gmpn_toom22_mul@@Base+0x2fc>  // b.tcont
   39b98:	sub	x9, x20, x8
   39b9c:	sub	x9, x9, x21
   39ba0:	cmp	x9, #0x4
   39ba4:	b.cc	39c10 <__gmpn_toom22_mul@@Base+0x2d4>  // b.lo, b.ul, b.last
   39ba8:	add	x10, x8, x20
   39bac:	sub	x10, x10, x21
   39bb0:	add	x12, x19, x10, lsl #3
   39bb4:	add	x10, x26, x22, lsl #3
   39bb8:	cmp	x12, x10
   39bbc:	add	x11, x26, x8, lsl #3
   39bc0:	b.cs	39bdc <__gmpn_toom22_mul@@Base+0x2a0>  // b.hs, b.nlast
   39bc4:	lsl	x10, x20, #1
   39bc8:	and	x13, x20, #0x1ffffffffffffffe
   39bcc:	sub	x10, x10, x13
   39bd0:	add	x10, x19, x10, lsl #3
   39bd4:	cmp	x11, x10
   39bd8:	b.cc	39c10 <__gmpn_toom22_mul@@Base+0x2d4>  // b.lo, b.ul, b.last
   39bdc:	and	x10, x9, #0xfffffffffffffffc
   39be0:	add	x11, x11, #0x10
   39be4:	add	x8, x8, x10
   39be8:	add	x12, x12, #0x10
   39bec:	mov	x13, x10
   39bf0:	ldp	q0, q1, [x11, #-16]
   39bf4:	subs	x13, x13, #0x4
   39bf8:	add	x11, x11, #0x20
   39bfc:	stp	q0, q1, [x12, #-16]
   39c00:	add	x12, x12, #0x20
   39c04:	b.ne	39bf0 <__gmpn_toom22_mul@@Base+0x2b4>  // b.any
   39c08:	cmp	x9, x10
   39c0c:	b.eq	39c38 <__gmpn_toom22_mul@@Base+0x2fc>  // b.none
   39c10:	add	x10, x8, x20
   39c14:	add	x9, x8, x21
   39c18:	sub	x10, x10, x21
   39c1c:	sub	x9, x9, x20
   39c20:	add	x10, x19, x10, lsl #3
   39c24:	add	x8, x26, x8, lsl #3
   39c28:	ldr	x11, [x8], #8
   39c2c:	adds	x9, x9, #0x1
   39c30:	str	x11, [x10], #8
   39c34:	b.cc	39c28 <__gmpn_toom22_mul@@Base+0x2ec>  // b.lo, b.ul, b.last
   39c38:	cmp	x22, #0xd
   39c3c:	b.le	39b14 <__gmpn_toom22_mul@@Base+0x1d8>
   39c40:	add	x5, x23, x22, lsl #4
   39c44:	mov	x0, x23
   39c48:	mov	x1, x19
   39c4c:	mov	x2, x22
   39c50:	mov	x3, x24
   39c54:	mov	x4, x22
   39c58:	bl	d450 <__gmpn_toom22_mul@plt>
   39c5c:	cmp	x21, x25
   39c60:	lsl	x28, x22, #1
   39c64:	b.le	39cac <__gmpn_toom22_mul@@Base+0x370>
   39c68:	cmp	x25, #0xd
   39c6c:	b.le	39cf8 <__gmpn_toom22_mul@@Base+0x3bc>
   39c70:	add	x8, x25, x25, lsl #2
   39c74:	lsl	x9, x22, #4
   39c78:	lsl	x10, x22, #3
   39c7c:	add	x0, x19, x9
   39c80:	add	x1, x27, x10
   39c84:	add	x3, x26, x10
   39c88:	cmp	x8, x21, lsl #2
   39c8c:	add	x5, x23, x9
   39c90:	mov	x2, x21
   39c94:	mov	x4, x25
   39c98:	b.gt	39cd0 <__gmpn_toom22_mul@@Base+0x394>
   39c9c:	bl	c850 <__gmpn_toom32_mul@plt>
   39ca0:	cmp	x22, #0xd
   39ca4:	b.le	39cdc <__gmpn_toom22_mul@@Base+0x3a0>
   39ca8:	b	39d28 <__gmpn_toom22_mul@@Base+0x3ec>
   39cac:	lsl	x8, x22, #3
   39cb0:	add	x0, x19, x22, lsl #4
   39cb4:	cmp	x20, #0x1b
   39cb8:	add	x1, x27, x8
   39cbc:	add	x3, x26, x8
   39cc0:	b.le	39d14 <__gmpn_toom22_mul@@Base+0x3d8>
   39cc4:	add	x5, x23, x28, lsl #3
   39cc8:	mov	x2, x21
   39ccc:	mov	x4, x21
   39cd0:	bl	d450 <__gmpn_toom22_mul@plt>
   39cd4:	cmp	x22, #0xd
   39cd8:	b.gt	39d28 <__gmpn_toom22_mul@@Base+0x3ec>
   39cdc:	mov	x0, x19
   39ce0:	mov	x1, x27
   39ce4:	mov	x2, x22
   39ce8:	mov	x3, x26
   39cec:	mov	x4, x22
   39cf0:	bl	c550 <__gmpn_mul_basecase@plt>
   39cf4:	b	39d44 <__gmpn_toom22_mul@@Base+0x408>
   39cf8:	lsl	x8, x22, #3
   39cfc:	add	x0, x19, x22, lsl #4
   39d00:	add	x1, x27, x8
   39d04:	add	x3, x26, x8
   39d08:	mov	x2, x21
   39d0c:	mov	x4, x25
   39d10:	b	39d1c <__gmpn_toom22_mul@@Base+0x3e0>
   39d14:	mov	x2, x21
   39d18:	mov	x4, x21
   39d1c:	bl	c550 <__gmpn_mul_basecase@plt>
   39d20:	cmp	x22, #0xd
   39d24:	b.le	39cdc <__gmpn_toom22_mul@@Base+0x3a0>
   39d28:	add	x5, x23, x22, lsl #4
   39d2c:	mov	x0, x19
   39d30:	mov	x1, x27
   39d34:	mov	x2, x22
   39d38:	mov	x3, x26
   39d3c:	mov	x4, x22
   39d40:	bl	d450 <__gmpn_toom22_mul@plt>
   39d44:	add	x26, x19, x28, lsl #3
   39d48:	mov	x0, x26
   39d4c:	mov	x1, x24
   39d50:	mov	x2, x26
   39d54:	mov	x3, x22
   39d58:	bl	ca70 <__gmpn_add_n@plt>
   39d5c:	mov	x27, x0
   39d60:	mov	x0, x24
   39d64:	mov	x1, x26
   39d68:	mov	x2, x19
   39d6c:	mov	x3, x22
   39d70:	bl	ca70 <__gmpn_add_n@plt>
   39d74:	ldr	x8, [sp, #8]
   39d78:	adds	x23, x8, x25
   39d7c:	mov	x25, x0
   39d80:	b.eq	39dc0 <__gmpn_toom22_mul@@Base+0x484>  // b.none
   39d84:	add	x2, x26, x22, lsl #3
   39d88:	mov	x0, x26
   39d8c:	mov	x1, x26
   39d90:	mov	x3, x23
   39d94:	bl	ca70 <__gmpn_add_n@plt>
   39d98:	cbz	x0, 39dc0 <__gmpn_toom22_mul@@Base+0x484>
   39d9c:	mov	w8, #0x1                   	// #1
   39da0:	cmp	x23, x22
   39da4:	b.ge	39dc4 <__gmpn_toom22_mul@@Base+0x488>  // b.tcont
   39da8:	lsl	x9, x23, #3
   39dac:	ldr	x10, [x26, x9]
   39db0:	add	x23, x23, #0x1
   39db4:	adds	x10, x10, #0x1
   39db8:	str	x10, [x26, x9]
   39dbc:	b.cs	39da0 <__gmpn_toom22_mul@@Base+0x464>  // b.hs, b.nlast
   39dc0:	mov	x8, xzr
   39dc4:	add	x23, x8, x27
   39dc8:	ldur	w8, [x29, #-12]
   39dcc:	cbz	w8, 39dec <__gmpn_toom22_mul@@Base+0x4b0>
   39dd0:	ldur	x2, [x29, #-8]
   39dd4:	mov	x0, x24
   39dd8:	mov	x1, x24
   39ddc:	mov	x3, x28
   39de0:	bl	ca70 <__gmpn_add_n@plt>
   39de4:	add	x8, x0, x23
   39de8:	b	39e0c <__gmpn_toom22_mul@@Base+0x4d0>
   39dec:	ldur	x2, [x29, #-8]
   39df0:	mov	x0, x24
   39df4:	mov	x1, x24
   39df8:	mov	x3, x28
   39dfc:	bl	c2d0 <__gmpn_sub_n@plt>
   39e00:	sub	x8, x23, x0
   39e04:	cmn	x8, #0x1
   39e08:	b.eq	39f38 <__gmpn_toom22_mul@@Base+0x5fc>  // b.none
   39e0c:	ldr	x9, [x26]
   39e10:	add	x10, x25, x27
   39e14:	adds	x9, x9, x10
   39e18:	str	x9, [x26]
   39e1c:	b.cc	39e38 <__gmpn_toom22_mul@@Base+0x4fc>  // b.lo, b.ul, b.last
   39e20:	add	x9, x19, x28, lsl #3
   39e24:	add	x9, x9, #0x8
   39e28:	ldr	x10, [x9]
   39e2c:	adds	x10, x10, #0x1
   39e30:	str	x10, [x9], #8
   39e34:	b.cs	39e28 <__gmpn_toom22_mul@@Base+0x4ec>  // b.hs, b.nlast
   39e38:	mov	w9, #0x18                  	// #24
   39e3c:	mul	x9, x22, x9
   39e40:	ldr	x10, [x19, x9]
   39e44:	adds	x8, x10, x8
   39e48:	str	x8, [x19, x9]
   39e4c:	b.cc	39e74 <__gmpn_toom22_mul@@Base+0x538>  // b.lo, b.ul, b.last
   39e50:	add	x8, x20, x20, lsl #1
   39e54:	add	x9, x21, x21, lsl #1
   39e58:	sub	x8, x8, x9
   39e5c:	add	x8, x19, x8, lsl #3
   39e60:	add	x8, x8, #0x8
   39e64:	ldr	x9, [x8]
   39e68:	adds	x9, x9, #0x1
   39e6c:	str	x9, [x8], #8
   39e70:	b.cs	39e64 <__gmpn_toom22_mul@@Base+0x528>  // b.hs, b.nlast
   39e74:	ldp	x20, x19, [sp, #112]
   39e78:	ldp	x22, x21, [sp, #96]
   39e7c:	ldp	x24, x23, [sp, #80]
   39e80:	ldp	x26, x25, [sp, #64]
   39e84:	ldp	x28, x27, [sp, #48]
   39e88:	ldp	x29, x30, [sp, #32]
   39e8c:	add	sp, sp, #0x80
   39e90:	ret
   39e94:	mov	x0, x19
   39e98:	mov	x1, x2
   39e9c:	mov	x2, x27
   39ea0:	mov	x3, x21
   39ea4:	bl	c2d0 <__gmpn_sub_n@plt>
   39ea8:	mov	w8, #0x1                   	// #1
   39eac:	stur	w8, [x29, #-12]
   39eb0:	subs	x23, x22, x25
   39eb4:	b.eq	39ac0 <__gmpn_toom22_mul@@Base+0x184>  // b.none
   39eb8:	b	399e4 <__gmpn_toom22_mul@@Base+0xa8>
   39ebc:	mov	x0, x24
   39ec0:	mov	x1, x2
   39ec4:	mov	x2, x26
   39ec8:	mov	x3, x22
   39ecc:	bl	c2d0 <__gmpn_sub_n@plt>
   39ed0:	ldur	w8, [x29, #-12]
   39ed4:	eor	w8, w8, #0x1
   39ed8:	stur	w8, [x29, #-12]
   39edc:	cmp	x22, #0xd
   39ee0:	b.le	39b14 <__gmpn_toom22_mul@@Base+0x1d8>
   39ee4:	b	39c40 <__gmpn_toom22_mul@@Base+0x304>
   39ee8:	mov	x0, x24
   39eec:	mov	x2, x26
   39ef0:	mov	x3, x25
   39ef4:	bl	c2d0 <__gmpn_sub_n@plt>
   39ef8:	cbz	x23, 39f1c <__gmpn_toom22_mul@@Base+0x5e0>
   39efc:	lsl	x8, x20, #1
   39f00:	sub	x8, x8, x28
   39f04:	and	x9, x20, #0x1ffffffffffffffe
   39f08:	sub	x8, x8, x9
   39f0c:	add	x0, x19, x28, lsl #3
   39f10:	lsl	x2, x8, #3
   39f14:	mov	w1, wzr
   39f18:	bl	c5f0 <memset@plt>
   39f1c:	ldur	w8, [x29, #-12]
   39f20:	ldur	x23, [x29, #-8]
   39f24:	eor	w8, w8, #0x1
   39f28:	stur	w8, [x29, #-12]
   39f2c:	cmp	x22, #0xd
   39f30:	b.gt	39c40 <__gmpn_toom22_mul@@Base+0x304>
   39f34:	b	39b14 <__gmpn_toom22_mul@@Base+0x1d8>
   39f38:	lsl	x2, x22, #3
   39f3c:	mov	x0, x26
   39f40:	mov	w1, wzr
   39f44:	bl	c5f0 <memset@plt>
   39f48:	b	39e74 <__gmpn_toom22_mul@@Base+0x538>

0000000000039f4c <__gmpn_toom32_mul@@Base>:
   39f4c:	sub	sp, sp, #0xd0
   39f50:	add	x8, x4, x4, lsl #1
   39f54:	stp	x29, x30, [sp, #112]
   39f58:	stp	x28, x27, [sp, #128]
   39f5c:	stp	x26, x25, [sp, #144]
   39f60:	stp	x24, x23, [sp, #160]
   39f64:	stp	x22, x21, [sp, #176]
   39f68:	stp	x20, x19, [sp, #192]
   39f6c:	add	x29, sp, #0x70
   39f70:	mov	x24, x4
   39f74:	mov	x26, x3
   39f78:	mov	x21, x2
   39f7c:	mov	x27, x1
   39f80:	cmp	x8, x2, lsl #1
   39f84:	mov	x19, x0
   39f88:	stur	x5, [x29, #-16]
   39f8c:	b.le	39f9c <__gmpn_toom32_mul@@Base+0x50>
   39f90:	sub	x8, x24, #0x1
   39f94:	asr	x28, x8, #1
   39f98:	b	39fb0 <__gmpn_toom32_mul@@Base+0x64>
   39f9c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   39fa0:	sub	x8, x21, #0x1
   39fa4:	movk	x9, #0xaaab
   39fa8:	umulh	x8, x8, x9
   39fac:	lsr	x28, x8, #1
   39fb0:	add	x22, x28, #0x1
   39fb4:	lsl	x25, x22, #1
   39fb8:	sub	x8, x24, x22
   39fbc:	stur	x8, [x29, #-8]
   39fc0:	sub	x23, x21, x25
   39fc4:	add	x2, x27, x22, lsl #4
   39fc8:	lsl	x8, x22, #3
   39fcc:	stp	x8, x23, [x29, #-48]
   39fd0:	str	x21, [sp, #8]
   39fd4:	str	x2, [sp, #24]
   39fd8:	cbz	x23, 3a020 <__gmpn_toom32_mul@@Base+0xd4>
   39fdc:	mov	x0, x19
   39fe0:	mov	x1, x27
   39fe4:	mov	x3, x23
   39fe8:	bl	ca70 <__gmpn_add_n@plt>
   39fec:	mov	x8, x23
   39ff0:	cbz	x0, 3a024 <__gmpn_toom32_mul@@Base+0xd8>
   39ff4:	mov	w20, #0x1                   	// #1
   39ff8:	mov	x8, x23
   39ffc:	cmp	x8, x28
   3a000:	b.gt	3a104 <__gmpn_toom32_mul@@Base+0x1b8>
   3a004:	lsl	x9, x8, #3
   3a008:	ldr	x10, [x27, x9]
   3a00c:	add	x8, x8, #0x1
   3a010:	adds	x10, x10, #0x1
   3a014:	str	x10, [x19, x9]
   3a018:	b.cs	39ffc <__gmpn_toom32_mul@@Base+0xb0>  // b.hs, b.nlast
   3a01c:	b	3a024 <__gmpn_toom32_mul@@Base+0xd8>
   3a020:	mov	x8, xzr
   3a024:	ldur	x23, [x29, #-48]
   3a028:	cmp	x19, x27
   3a02c:	b.eq	3a0c8 <__gmpn_toom32_mul@@Base+0x17c>  // b.none
   3a030:	cmp	x8, x28
   3a034:	b.gt	3a0c8 <__gmpn_toom32_mul@@Base+0x17c>
   3a038:	sub	x9, x28, x8
   3a03c:	add	x9, x9, #0x1
   3a040:	cmp	x9, #0x4
   3a044:	b.cc	3a0a4 <__gmpn_toom32_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3a048:	lsl	x11, x8, #3
   3a04c:	add	x10, x19, x11
   3a050:	add	x12, x27, x23
   3a054:	cmp	x10, x12
   3a058:	b.cs	3a06c <__gmpn_toom32_mul@@Base+0x120>  // b.hs, b.nlast
   3a05c:	add	x10, x19, x23
   3a060:	add	x12, x27, x11
   3a064:	cmp	x12, x10
   3a068:	b.cc	3a0a4 <__gmpn_toom32_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3a06c:	and	x10, x9, #0xfffffffffffffffc
   3a070:	add	x12, x11, #0x10
   3a074:	add	x8, x8, x10
   3a078:	add	x11, x27, x12
   3a07c:	add	x12, x19, x12
   3a080:	mov	x13, x10
   3a084:	ldp	q0, q1, [x11, #-16]
   3a088:	add	x11, x11, #0x20
   3a08c:	subs	x13, x13, #0x4
   3a090:	stp	q0, q1, [x12, #-16]
   3a094:	add	x12, x12, #0x20
   3a098:	b.ne	3a084 <__gmpn_toom32_mul@@Base+0x138>  // b.any
   3a09c:	cmp	x9, x10
   3a0a0:	b.eq	3a0c8 <__gmpn_toom32_mul@@Base+0x17c>  // b.none
   3a0a4:	sub	x9, x28, x8
   3a0a8:	lsl	x10, x8, #3
   3a0ac:	add	x8, x9, #0x1
   3a0b0:	add	x9, x19, x10
   3a0b4:	add	x10, x27, x10
   3a0b8:	ldr	x11, [x10], #8
   3a0bc:	subs	x8, x8, #0x1
   3a0c0:	str	x11, [x9], #8
   3a0c4:	b.ne	3a0b8 <__gmpn_toom32_mul@@Base+0x16c>  // b.any
   3a0c8:	add	x8, x27, x28, lsl #4
   3a0cc:	add	x1, x27, x22, lsl #3
   3a0d0:	add	x8, x8, #0x8
   3a0d4:	mov	x9, x28
   3a0d8:	add	x10, x9, #0x1
   3a0dc:	cmp	x10, #0x1
   3a0e0:	b.lt	3a0fc <__gmpn_toom32_mul@@Base+0x1b0>  // b.tstop
   3a0e4:	ldr	x10, [x19, x9, lsl #3]
   3a0e8:	ldr	x11, [x8], #-8
   3a0ec:	sub	x9, x9, #0x1
   3a0f0:	cmp	x10, x11
   3a0f4:	b.eq	3a0d8 <__gmpn_toom32_mul@@Base+0x18c>  // b.none
   3a0f8:	b.ls	3a384 <__gmpn_toom32_mul@@Base+0x438>  // b.plast
   3a0fc:	mov	x20, xzr
   3a100:	b	3a108 <__gmpn_toom32_mul@@Base+0x1bc>
   3a104:	ldur	x23, [x29, #-48]
   3a108:	add	x0, x19, x25, lsl #3
   3a10c:	add	x2, x27, x22, lsl #3
   3a110:	mov	x1, x19
   3a114:	mov	x3, x22
   3a118:	bl	c2d0 <__gmpn_sub_n@plt>
   3a11c:	sub	x8, x20, x0
   3a120:	stur	wzr, [x29, #-28]
   3a124:	str	x8, [sp, #56]
   3a128:	add	x2, x27, x23
   3a12c:	mov	x0, x19
   3a130:	mov	x1, x19
   3a134:	mov	x3, x22
   3a138:	str	x27, [sp, #40]
   3a13c:	bl	ca70 <__gmpn_add_n@plt>
   3a140:	ldur	x8, [x29, #-8]
   3a144:	mov	x9, x23
   3a148:	add	x27, x0, x20
   3a14c:	add	x21, x19, x23
   3a150:	subs	x23, x22, x8
   3a154:	add	x2, x26, x9
   3a158:	stur	x21, [x29, #-24]
   3a15c:	str	x2, [sp, #48]
   3a160:	b.ne	3a1cc <__gmpn_toom32_mul@@Base+0x280>  // b.any
   3a164:	mov	x0, x21
   3a168:	mov	x1, x26
   3a16c:	mov	x3, x22
   3a170:	bl	ca70 <__gmpn_add_n@plt>
   3a174:	ldur	x23, [x29, #-16]
   3a178:	mov	w8, #0x8                   	// #8
   3a17c:	mov	x20, x0
   3a180:	bfi	x8, x28, #4, #60
   3a184:	mov	x9, x28
   3a188:	add	x10, x9, #0x1
   3a18c:	cmp	x10, #0x1
   3a190:	b.lt	3a1b0 <__gmpn_toom32_mul@@Base+0x264>  // b.tstop
   3a194:	ldr	x10, [x26, x9, lsl #3]
   3a198:	ldr	x11, [x26, x8]
   3a19c:	sub	x9, x9, #0x1
   3a1a0:	sub	x8, x8, #0x8
   3a1a4:	cmp	x10, x11
   3a1a8:	b.eq	3a188 <__gmpn_toom32_mul@@Base+0x23c>  // b.none
   3a1ac:	b.ls	3a46c <__gmpn_toom32_mul@@Base+0x520>  // b.plast
   3a1b0:	ldr	x2, [sp, #48]
   3a1b4:	mov	w8, #0x18                  	// #24
   3a1b8:	madd	x0, x22, x8, x19
   3a1bc:	mov	x1, x26
   3a1c0:	mov	x3, x22
   3a1c4:	bl	c2d0 <__gmpn_sub_n@plt>
   3a1c8:	b	3a4dc <__gmpn_toom32_mul@@Base+0x590>
   3a1cc:	ldur	x8, [x29, #-8]
   3a1d0:	cbz	x8, 3a218 <__gmpn_toom32_mul@@Base+0x2cc>
   3a1d4:	ldur	x20, [x29, #-8]
   3a1d8:	mov	x0, x21
   3a1dc:	mov	x1, x26
   3a1e0:	mov	x3, x20
   3a1e4:	bl	ca70 <__gmpn_add_n@plt>
   3a1e8:	mov	x8, x20
   3a1ec:	cbz	x0, 3a218 <__gmpn_toom32_mul@@Base+0x2cc>
   3a1f0:	ldur	x8, [x29, #-8]
   3a1f4:	add	x9, x19, x24, lsl #3
   3a1f8:	mov	w20, #0x1                   	// #1
   3a1fc:	cmp	x8, x28
   3a200:	b.gt	3a2c8 <__gmpn_toom32_mul@@Base+0x37c>
   3a204:	ldr	x10, [x26, x8, lsl #3]
   3a208:	add	x8, x8, #0x1
   3a20c:	adds	x10, x10, #0x1
   3a210:	str	x10, [x9], #8
   3a214:	b.cs	3a1fc <__gmpn_toom32_mul@@Base+0x2b0>  // b.hs, b.nlast
   3a218:	cmp	x21, x26
   3a21c:	mov	x20, xzr
   3a220:	b.eq	3a2c8 <__gmpn_toom32_mul@@Base+0x37c>  // b.none
   3a224:	cmp	x8, x28
   3a228:	b.gt	3a2c8 <__gmpn_toom32_mul@@Base+0x37c>
   3a22c:	sub	x9, x28, x8
   3a230:	add	x9, x9, #0x1
   3a234:	cmp	x9, #0x4
   3a238:	b.cc	3a29c <__gmpn_toom32_mul@@Base+0x350>  // b.lo, b.ul, b.last
   3a23c:	add	x10, x8, x28
   3a240:	add	x12, x19, x10, lsl #3
   3a244:	add	x10, x12, #0x8
   3a248:	add	x11, x26, x22, lsl #3
   3a24c:	cmp	x10, x11
   3a250:	add	x11, x26, x8, lsl #3
   3a254:	b.cs	3a268 <__gmpn_toom32_mul@@Base+0x31c>  // b.hs, b.nlast
   3a258:	add	x10, x19, x28, lsl #4
   3a25c:	add	x10, x10, #0x10
   3a260:	cmp	x11, x10
   3a264:	b.cc	3a29c <__gmpn_toom32_mul@@Base+0x350>  // b.lo, b.ul, b.last
   3a268:	and	x10, x9, #0xfffffffffffffffc
   3a26c:	add	x11, x11, #0x10
   3a270:	add	x8, x8, x10
   3a274:	add	x12, x12, #0x18
   3a278:	mov	x13, x10
   3a27c:	ldp	q0, q1, [x11, #-16]
   3a280:	subs	x13, x13, #0x4
   3a284:	add	x11, x11, #0x20
   3a288:	stp	q0, q1, [x12, #-16]
   3a28c:	add	x12, x12, #0x20
   3a290:	b.ne	3a27c <__gmpn_toom32_mul@@Base+0x330>  // b.any
   3a294:	cmp	x9, x10
   3a298:	b.eq	3a2c4 <__gmpn_toom32_mul@@Base+0x378>  // b.none
   3a29c:	add	x10, x8, x28
   3a2a0:	sub	x9, x28, x8
   3a2a4:	add	x10, x19, x10, lsl #3
   3a2a8:	add	x9, x9, #0x1
   3a2ac:	add	x10, x10, #0x8
   3a2b0:	add	x8, x26, x8, lsl #3
   3a2b4:	ldr	x11, [x8], #8
   3a2b8:	subs	x9, x9, #0x1
   3a2bc:	str	x11, [x10], #8
   3a2c0:	b.ne	3a2b4 <__gmpn_toom32_mul@@Base+0x368>  // b.any
   3a2c4:	mov	x20, xzr
   3a2c8:	sub	x8, x24, x28, lsl #1
   3a2cc:	sub	x8, x8, #0x2
   3a2d0:	lsl	x9, x28, #3
   3a2d4:	ldr	x10, [x26, x9]
   3a2d8:	cbnz	x10, 3a320 <__gmpn_toom32_mul@@Base+0x3d4>
   3a2dc:	adds	x8, x8, #0x1
   3a2e0:	sub	x9, x9, #0x8
   3a2e4:	b.cc	3a2d4 <__gmpn_toom32_mul@@Base+0x388>  // b.lo, b.ul, b.last
   3a2e8:	sub	x8, x24, x28
   3a2ec:	lsl	x9, x24, #3
   3a2f0:	sub	x8, x8, #0x2
   3a2f4:	sub	x9, x9, #0x8
   3a2f8:	add	x10, x8, #0x1
   3a2fc:	cmp	x10, #0x1
   3a300:	b.lt	3a320 <__gmpn_toom32_mul@@Base+0x3d4>  // b.tstop
   3a304:	ldr	x10, [x26, x8, lsl #3]
   3a308:	ldr	x11, [x26, x9]
   3a30c:	sub	x8, x8, #0x1
   3a310:	sub	x9, x9, #0x8
   3a314:	cmp	x10, x11
   3a318:	b.eq	3a2f8 <__gmpn_toom32_mul@@Base+0x3ac>  // b.none
   3a31c:	b.ls	3a48c <__gmpn_toom32_mul@@Base+0x540>  // b.plast
   3a320:	mov	w8, #0x18                  	// #24
   3a324:	madd	x21, x22, x8, x19
   3a328:	ldur	x8, [x29, #-8]
   3a32c:	cbz	x8, 3a3a8 <__gmpn_toom32_mul@@Base+0x45c>
   3a330:	ldur	x23, [x29, #-8]
   3a334:	ldr	x2, [sp, #48]
   3a338:	mov	x0, x21
   3a33c:	mov	x1, x26
   3a340:	mov	x3, x23
   3a344:	bl	c2d0 <__gmpn_sub_n@plt>
   3a348:	mov	x8, x23
   3a34c:	ldur	x23, [x29, #-16]
   3a350:	cbz	x0, 3a3ac <__gmpn_toom32_mul@@Base+0x460>
   3a354:	add	x8, x24, x28, lsl #1
   3a358:	add	x8, x19, x8, lsl #3
   3a35c:	add	x9, x8, #0x10
   3a360:	ldur	x8, [x29, #-8]
   3a364:	cmp	x8, x28
   3a368:	b.gt	3a464 <__gmpn_toom32_mul@@Base+0x518>
   3a36c:	ldr	x10, [x26, x8, lsl #3]
   3a370:	add	x8, x8, #0x1
   3a374:	sub	x11, x10, #0x1
   3a378:	str	x11, [x9], #8
   3a37c:	cbz	x10, 3a364 <__gmpn_toom32_mul@@Base+0x418>
   3a380:	b	3a3ac <__gmpn_toom32_mul@@Base+0x460>
   3a384:	add	x0, x19, x25, lsl #3
   3a388:	mov	x2, x19
   3a38c:	mov	x3, x22
   3a390:	bl	c2d0 <__gmpn_sub_n@plt>
   3a394:	mov	w8, #0x1                   	// #1
   3a398:	mov	x20, xzr
   3a39c:	str	xzr, [sp, #56]
   3a3a0:	stur	w8, [x29, #-28]
   3a3a4:	b	3a128 <__gmpn_toom32_mul@@Base+0x1dc>
   3a3a8:	ldur	x23, [x29, #-16]
   3a3ac:	cmp	x21, x26
   3a3b0:	b.eq	3a464 <__gmpn_toom32_mul@@Base+0x518>  // b.none
   3a3b4:	cmp	x8, x28
   3a3b8:	b.gt	3a464 <__gmpn_toom32_mul@@Base+0x518>
   3a3bc:	ldur	x21, [x29, #-24]
   3a3c0:	sub	x9, x28, x8
   3a3c4:	add	x9, x9, #0x1
   3a3c8:	cmp	x9, #0x4
   3a3cc:	b.cc	3a434 <__gmpn_toom32_mul@@Base+0x4e8>  // b.lo, b.ul, b.last
   3a3d0:	add	x10, x28, x28, lsl #1
   3a3d4:	add	x10, x8, x10
   3a3d8:	add	x12, x19, x10, lsl #3
   3a3dc:	add	x10, x12, #0x18
   3a3e0:	add	x11, x26, x22, lsl #3
   3a3e4:	cmp	x10, x11
   3a3e8:	add	x11, x26, x8, lsl #3
   3a3ec:	b.cs	3a400 <__gmpn_toom32_mul@@Base+0x4b4>  // b.hs, b.nlast
   3a3f0:	add	x10, x19, x28, lsl #5
   3a3f4:	add	x10, x10, #0x20
   3a3f8:	cmp	x11, x10
   3a3fc:	b.cc	3a434 <__gmpn_toom32_mul@@Base+0x4e8>  // b.lo, b.ul, b.last
   3a400:	and	x10, x9, #0xfffffffffffffffc
   3a404:	add	x11, x11, #0x10
   3a408:	add	x8, x8, x10
   3a40c:	add	x12, x12, #0x28
   3a410:	mov	x13, x10
   3a414:	ldp	q0, q1, [x11, #-16]
   3a418:	subs	x13, x13, #0x4
   3a41c:	add	x11, x11, #0x20
   3a420:	stp	q0, q1, [x12, #-16]
   3a424:	add	x12, x12, #0x20
   3a428:	b.ne	3a414 <__gmpn_toom32_mul@@Base+0x4c8>  // b.any
   3a42c:	cmp	x9, x10
   3a430:	b.eq	3a4dc <__gmpn_toom32_mul@@Base+0x590>  // b.none
   3a434:	add	x10, x28, x28, lsl #1
   3a438:	add	x10, x8, x10
   3a43c:	sub	x9, x28, x8
   3a440:	add	x10, x19, x10, lsl #3
   3a444:	add	x9, x9, #0x1
   3a448:	add	x10, x10, #0x18
   3a44c:	add	x8, x26, x8, lsl #3
   3a450:	ldr	x11, [x8], #8
   3a454:	subs	x9, x9, #0x1
   3a458:	str	x11, [x10], #8
   3a45c:	b.ne	3a450 <__gmpn_toom32_mul@@Base+0x504>  // b.any
   3a460:	b	3a4dc <__gmpn_toom32_mul@@Base+0x590>
   3a464:	ldur	x21, [x29, #-24]
   3a468:	b	3a4dc <__gmpn_toom32_mul@@Base+0x590>
   3a46c:	ldr	x1, [sp, #48]
   3a470:	mov	w8, #0x18                  	// #24
   3a474:	madd	x0, x22, x8, x19
   3a478:	mov	x2, x26
   3a47c:	mov	x3, x22
   3a480:	bl	c2d0 <__gmpn_sub_n@plt>
   3a484:	ldur	w8, [x29, #-28]
   3a488:	b	3a4d4 <__gmpn_toom32_mul@@Base+0x588>
   3a48c:	ldr	x1, [sp, #48]
   3a490:	ldur	x3, [x29, #-8]
   3a494:	mov	w8, #0x18                  	// #24
   3a498:	madd	x21, x22, x8, x19
   3a49c:	mov	x0, x21
   3a4a0:	mov	x2, x26
   3a4a4:	bl	c2d0 <__gmpn_sub_n@plt>
   3a4a8:	cbz	x23, 3a4cc <__gmpn_toom32_mul@@Base+0x580>
   3a4ac:	ldur	x8, [x29, #-8]
   3a4b0:	mov	w1, wzr
   3a4b4:	add	x0, x21, x8, lsl #3
   3a4b8:	lsl	x8, x28, #1
   3a4bc:	sub	x8, x8, x24
   3a4c0:	lsl	x8, x8, #3
   3a4c4:	add	x2, x8, #0x10
   3a4c8:	bl	c5f0 <memset@plt>
   3a4cc:	ldur	w8, [x29, #-28]
   3a4d0:	ldp	x21, x23, [x29, #-24]
   3a4d4:	eor	w8, w8, #0x1
   3a4d8:	stur	w8, [x29, #-28]
   3a4dc:	mov	x0, x23
   3a4e0:	mov	x1, x19
   3a4e4:	mov	x2, x21
   3a4e8:	mov	x3, x22
   3a4ec:	bl	c990 <__gmpn_mul_n@plt>
   3a4f0:	cmp	x27, #0x2
   3a4f4:	b.eq	3a8cc <__gmpn_toom32_mul@@Base+0x980>  // b.none
   3a4f8:	cmp	x27, #0x1
   3a4fc:	b.ne	3a8ec <__gmpn_toom32_mul@@Base+0x9a0>  // b.any
   3a500:	add	x0, x23, x22, lsl #3
   3a504:	mov	x1, x0
   3a508:	mov	x2, x21
   3a50c:	mov	x3, x22
   3a510:	bl	ca70 <__gmpn_add_n@plt>
   3a514:	add	x21, x0, x20
   3a518:	cbz	x20, 3a534 <__gmpn_toom32_mul@@Base+0x5e8>
   3a51c:	add	x0, x23, x22, lsl #3
   3a520:	mov	x1, x0
   3a524:	mov	x2, x19
   3a528:	mov	x3, x22
   3a52c:	bl	ca70 <__gmpn_add_n@plt>
   3a530:	add	x21, x0, x21
   3a534:	mov	x8, x23
   3a538:	mov	x27, x25
   3a53c:	lsl	x25, x25, #3
   3a540:	add	x23, x22, x22, lsl #1
   3a544:	str	x21, [x8, x25]
   3a548:	add	x20, x19, x25
   3a54c:	add	x21, x19, x23, lsl #3
   3a550:	mov	x0, x19
   3a554:	mov	x1, x20
   3a558:	mov	x2, x21
   3a55c:	mov	x3, x22
   3a560:	bl	c990 <__gmpn_mul_n@plt>
   3a564:	ldr	x8, [sp, #56]
   3a568:	cbz	x8, 3a584 <__gmpn_toom32_mul@@Base+0x638>
   3a56c:	ldur	x0, [x29, #-24]
   3a570:	mov	x2, x21
   3a574:	mov	x3, x22
   3a578:	mov	x1, x0
   3a57c:	bl	ca70 <__gmpn_add_n@plt>
   3a580:	b	3a588 <__gmpn_toom32_mul@@Base+0x63c>
   3a584:	mov	x0, xzr
   3a588:	ldur	w8, [x29, #-28]
   3a58c:	orr	x3, x27, #0x1
   3a590:	str	x28, [sp, #56]
   3a594:	str	x0, [x20]
   3a598:	str	x24, [sp, #16]
   3a59c:	cbz	w8, 3a5b8 <__gmpn_toom32_mul@@Base+0x66c>
   3a5a0:	ldur	x28, [x29, #-16]
   3a5a4:	mov	x2, x19
   3a5a8:	mov	x0, x28
   3a5ac:	mov	x1, x28
   3a5b0:	bl	c840 <__gmpn_rsh1sub_n@plt>
   3a5b4:	b	3a5cc <__gmpn_toom32_mul@@Base+0x680>
   3a5b8:	ldur	x28, [x29, #-16]
   3a5bc:	mov	x2, x19
   3a5c0:	mov	x0, x28
   3a5c4:	mov	x1, x28
   3a5c8:	bl	c950 <__gmpn_rsh1add_n@plt>
   3a5cc:	ldr	x8, [x20]
   3a5d0:	add	x24, x28, x22, lsl #3
   3a5d4:	mov	x0, x20
   3a5d8:	mov	x1, x28
   3a5dc:	mov	x2, x24
   3a5e0:	mov	x3, x22
   3a5e4:	str	x8, [sp, #32]
   3a5e8:	bl	ca70 <__gmpn_add_n@plt>
   3a5ec:	ldr	x8, [x28, x25]
   3a5f0:	ldr	x9, [x24]
   3a5f4:	add	x8, x8, x0
   3a5f8:	add	x8, x9, x8
   3a5fc:	str	x8, [x24]
   3a600:	ldr	x9, [x28, x25]
   3a604:	ldr	x28, [sp, #56]
   3a608:	add	x9, x9, x0
   3a60c:	cmp	x8, x9
   3a610:	b.cs	3a630 <__gmpn_toom32_mul@@Base+0x6e4>  // b.hs, b.nlast
   3a614:	ldur	x8, [x29, #-16]
   3a618:	add	x8, x8, x28, lsl #3
   3a61c:	add	x8, x8, #0x10
   3a620:	ldr	x9, [x8]
   3a624:	adds	x9, x9, #0x1
   3a628:	str	x9, [x8], #8
   3a62c:	b.cs	3a620 <__gmpn_toom32_mul@@Base+0x6d4>  // b.hs, b.nlast
   3a630:	ldur	w8, [x29, #-28]
   3a634:	mov	x25, x27
   3a638:	cbz	w8, 3a6ac <__gmpn_toom32_mul@@Base+0x760>
   3a63c:	ldur	x27, [x29, #-16]
   3a640:	mov	x2, x19
   3a644:	mov	x3, x22
   3a648:	mov	x0, x27
   3a64c:	mov	x1, x27
   3a650:	bl	ca70 <__gmpn_add_n@plt>
   3a654:	ldur	x2, [x29, #-24]
   3a658:	mov	x4, x0
   3a65c:	mov	x0, x20
   3a660:	mov	x1, x20
   3a664:	mov	x3, x22
   3a668:	bl	ce90 <__gmpn_add_nc@plt>
   3a66c:	ldur	x10, [x29, #-48]
   3a670:	ldr	x9, [sp, #32]
   3a674:	ldr	x8, [x27, x10]
   3a678:	add	x9, x0, x9
   3a67c:	adds	x8, x8, x9
   3a680:	str	x8, [x27, x10]
   3a684:	ldur	x27, [x29, #-40]
   3a688:	b.cc	3a718 <__gmpn_toom32_mul@@Base+0x7cc>  // b.lo, b.ul, b.last
   3a68c:	ldur	x8, [x29, #-16]
   3a690:	add	x8, x8, x28, lsl #3
   3a694:	add	x8, x8, #0x10
   3a698:	ldr	x9, [x8]
   3a69c:	adds	x9, x9, #0x1
   3a6a0:	str	x9, [x8], #8
   3a6a4:	b.cs	3a698 <__gmpn_toom32_mul@@Base+0x74c>  // b.hs, b.nlast
   3a6a8:	b	3a718 <__gmpn_toom32_mul@@Base+0x7cc>
   3a6ac:	ldur	x27, [x29, #-16]
   3a6b0:	mov	x2, x19
   3a6b4:	mov	x3, x22
   3a6b8:	mov	x0, x27
   3a6bc:	mov	x1, x27
   3a6c0:	bl	c2d0 <__gmpn_sub_n@plt>
   3a6c4:	ldur	x2, [x29, #-24]
   3a6c8:	mov	x4, x0
   3a6cc:	mov	x0, x20
   3a6d0:	mov	x1, x20
   3a6d4:	mov	x3, x22
   3a6d8:	bl	c760 <__gmpn_sub_nc@plt>
   3a6dc:	ldur	x10, [x29, #-48]
   3a6e0:	ldr	x9, [sp, #32]
   3a6e4:	ldr	x8, [x27, x10]
   3a6e8:	add	x9, x0, x9
   3a6ec:	subs	x8, x8, x9
   3a6f0:	str	x8, [x27, x10]
   3a6f4:	ldur	x27, [x29, #-40]
   3a6f8:	b.cs	3a718 <__gmpn_toom32_mul@@Base+0x7cc>  // b.hs, b.nlast
   3a6fc:	ldur	x8, [x29, #-16]
   3a700:	add	x8, x8, x28, lsl #3
   3a704:	add	x8, x8, #0x10
   3a708:	ldr	x9, [x8]
   3a70c:	sub	x10, x9, #0x1
   3a710:	str	x10, [x8], #8
   3a714:	cbz	x9, 3a708 <__gmpn_toom32_mul@@Base+0x7bc>
   3a718:	ldr	x1, [sp, #40]
   3a71c:	mov	x0, x19
   3a720:	mov	x2, x26
   3a724:	mov	x3, x22
   3a728:	bl	c990 <__gmpn_mul_n@plt>
   3a72c:	ldur	x8, [x29, #-8]
   3a730:	mov	x0, x21
   3a734:	str	x23, [sp]
   3a738:	cmp	x27, x8
   3a73c:	b.le	3a754 <__gmpn_toom32_mul@@Base+0x808>
   3a740:	ldr	x1, [sp, #24]
   3a744:	ldr	x3, [sp, #48]
   3a748:	ldur	x4, [x29, #-8]
   3a74c:	mov	x2, x27
   3a750:	b	3a764 <__gmpn_toom32_mul@@Base+0x818>
   3a754:	ldr	x1, [sp, #48]
   3a758:	ldur	x2, [x29, #-8]
   3a75c:	ldr	x3, [sp, #24]
   3a760:	mov	x4, x27
   3a764:	bl	ccd0 <__gmpn_mul@plt>
   3a768:	ldur	x27, [x29, #-24]
   3a76c:	mov	x2, x21
   3a770:	mov	x3, x22
   3a774:	mov	x0, x27
   3a778:	mov	x1, x27
   3a77c:	bl	c2d0 <__gmpn_sub_n@plt>
   3a780:	ldur	x28, [x29, #-16]
   3a784:	mov	x26, x0
   3a788:	mov	x0, x20
   3a78c:	mov	x1, x20
   3a790:	ldr	x23, [x28, x25, lsl #3]
   3a794:	mov	x2, x19
   3a798:	mov	x3, x22
   3a79c:	mov	x4, x26
   3a7a0:	bl	c760 <__gmpn_sub_nc@plt>
   3a7a4:	mov	x4, x0
   3a7a8:	mov	x0, x21
   3a7ac:	mov	x1, x24
   3a7b0:	mov	x2, x27
   3a7b4:	mov	x3, x22
   3a7b8:	bl	c760 <__gmpn_sub_nc@plt>
   3a7bc:	mov	x24, x0
   3a7c0:	cbz	x22, 3a818 <__gmpn_toom32_mul@@Base+0x8cc>
   3a7c4:	mov	x0, x27
   3a7c8:	mov	x1, x27
   3a7cc:	mov	x2, x28
   3a7d0:	mov	x3, x22
   3a7d4:	bl	ca70 <__gmpn_add_n@plt>
   3a7d8:	cbz	x0, 3a818 <__gmpn_toom32_mul@@Base+0x8cc>
   3a7dc:	ldr	x28, [sp, #56]
   3a7e0:	ldr	x13, [sp]
   3a7e4:	mov	w8, #0x1                   	// #1
   3a7e8:	mov	x9, x20
   3a7ec:	mov	x12, x22
   3a7f0:	cmp	x22, x13
   3a7f4:	b.ge	3a810 <__gmpn_toom32_mul@@Base+0x8c4>  // b.tcont
   3a7f8:	ldr	x11, [x9]
   3a7fc:	add	x22, x22, #0x1
   3a800:	adds	x11, x11, #0x1
   3a804:	str	x11, [x9], #8
   3a808:	b.cs	3a7f0 <__gmpn_toom32_mul@@Base+0x8a4>  // b.hs, b.nlast
   3a80c:	mov	x8, xzr
   3a810:	mov	x22, x12
   3a814:	b	3a820 <__gmpn_toom32_mul@@Base+0x8d4>
   3a818:	ldr	x28, [sp, #56]
   3a81c:	mov	x8, xzr
   3a820:	ldur	x9, [x29, #-8]
   3a824:	ldur	x10, [x29, #-40]
   3a828:	add	x9, x10, x9
   3a82c:	cmp	x9, x22
   3a830:	b.le	3a934 <__gmpn_toom32_mul@@Base+0x9e8>
   3a834:	add	x10, x23, x26
   3a838:	subs	x23, x9, x22
   3a83c:	sub	x9, x10, x24
   3a840:	add	x24, x9, x8
   3a844:	add	x21, x19, x22, lsl #5
   3a848:	b.eq	3a894 <__gmpn_toom32_mul@@Base+0x948>  // b.none
   3a84c:	mov	x0, x20
   3a850:	mov	x1, x20
   3a854:	mov	x2, x21
   3a858:	mov	x3, x23
   3a85c:	bl	c2d0 <__gmpn_sub_n@plt>
   3a860:	cbz	x0, 3a894 <__gmpn_toom32_mul@@Base+0x948>
   3a864:	ldp	x9, x8, [sp, #8]
   3a868:	add	x8, x8, x9
   3a86c:	sub	x8, x8, x28, lsl #1
   3a870:	add	x8, x19, x8, lsl #3
   3a874:	sub	x8, x8, #0x10
   3a878:	cmp	x23, x25
   3a87c:	b.ge	3a8f8 <__gmpn_toom32_mul@@Base+0x9ac>  // b.tcont
   3a880:	ldr	x9, [x8]
   3a884:	add	x23, x23, #0x1
   3a888:	sub	x10, x9, #0x1
   3a88c:	str	x10, [x8], #8
   3a890:	cbz	x9, 3a878 <__gmpn_toom32_mul@@Base+0x92c>
   3a894:	mov	x8, xzr
   3a898:	adds	x8, x24, x8
   3a89c:	b.mi	3a904 <__gmpn_toom32_mul@@Base+0x9b8>  // b.first
   3a8a0:	ldr	x9, [x21]
   3a8a4:	adds	x8, x9, x8
   3a8a8:	str	x8, [x21]
   3a8ac:	b.cc	3a934 <__gmpn_toom32_mul@@Base+0x9e8>  // b.lo, b.ul, b.last
   3a8b0:	add	x8, x19, x28, lsl #5
   3a8b4:	add	x8, x8, #0x28
   3a8b8:	ldr	x9, [x8]
   3a8bc:	adds	x9, x9, #0x1
   3a8c0:	str	x9, [x8], #8
   3a8c4:	b.cs	3a8b8 <__gmpn_toom32_mul@@Base+0x96c>  // b.hs, b.nlast
   3a8c8:	b	3a934 <__gmpn_toom32_mul@@Base+0x9e8>
   3a8cc:	add	x0, x23, x22, lsl #3
   3a8d0:	mov	x1, x0
   3a8d4:	mov	x2, x21
   3a8d8:	mov	x3, x22
   3a8dc:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3a8e0:	add	x21, x0, x20, lsl #1
   3a8e4:	cbnz	x20, 3a51c <__gmpn_toom32_mul@@Base+0x5d0>
   3a8e8:	b	3a534 <__gmpn_toom32_mul@@Base+0x5e8>
   3a8ec:	mov	x21, xzr
   3a8f0:	cbnz	x20, 3a51c <__gmpn_toom32_mul@@Base+0x5d0>
   3a8f4:	b	3a534 <__gmpn_toom32_mul@@Base+0x5e8>
   3a8f8:	mov	x8, #0xffffffffffffffff    	// #-1
   3a8fc:	adds	x8, x24, x8
   3a900:	b.pl	3a8a0 <__gmpn_toom32_mul@@Base+0x954>  // b.nfrst
   3a904:	ldr	x9, [x21]
   3a908:	neg	x10, x8
   3a90c:	add	x8, x9, x8
   3a910:	cmp	x9, x10
   3a914:	str	x8, [x21]
   3a918:	b.cs	3a934 <__gmpn_toom32_mul@@Base+0x9e8>  // b.hs, b.nlast
   3a91c:	add	x8, x19, x28, lsl #5
   3a920:	add	x8, x8, #0x28
   3a924:	ldr	x9, [x8]
   3a928:	sub	x10, x9, #0x1
   3a92c:	str	x10, [x8], #8
   3a930:	cbz	x9, 3a924 <__gmpn_toom32_mul@@Base+0x9d8>
   3a934:	ldp	x20, x19, [sp, #192]
   3a938:	ldp	x22, x21, [sp, #176]
   3a93c:	ldp	x24, x23, [sp, #160]
   3a940:	ldp	x26, x25, [sp, #144]
   3a944:	ldp	x28, x27, [sp, #128]
   3a948:	ldp	x29, x30, [sp, #112]
   3a94c:	add	sp, sp, #0xd0
   3a950:	ret

000000000003a954 <__gmpn_toom42_mul@@Base>:
   3a954:	stp	x29, x30, [sp, #-96]!
   3a958:	stp	x28, x27, [sp, #16]
   3a95c:	stp	x26, x25, [sp, #32]
   3a960:	stp	x24, x23, [sp, #48]
   3a964:	stp	x22, x21, [sp, #64]
   3a968:	stp	x20, x19, [sp, #80]
   3a96c:	mov	x29, sp
   3a970:	sub	sp, sp, #0x70
   3a974:	add	x8, x2, #0x3
   3a978:	add	x9, x4, #0x1
   3a97c:	cmp	x2, x4, lsl #1
   3a980:	asr	x8, x8, #2
   3a984:	asr	x9, x9, #1
   3a988:	mov	w10, #0x30                  	// #48
   3a98c:	csel	x23, x9, x8, lt  // lt = tstop
   3a990:	mul	x8, x23, x10
   3a994:	stp	x0, x1, [x29, #-24]
   3a998:	add	x22, x23, x23, lsl #1
   3a99c:	add	x1, x8, #0x28
   3a9a0:	mov	w8, #0x7f00                	// #32512
   3a9a4:	mov	x19, x4
   3a9a8:	mov	x21, x3
   3a9ac:	mov	x20, x2
   3a9b0:	sub	x4, x2, x22
   3a9b4:	cmp	x1, x8
   3a9b8:	stur	x5, [x29, #-48]
   3a9bc:	stur	xzr, [x29, #-8]
   3a9c0:	stur	x4, [x29, #-32]
   3a9c4:	b.hi	3b12c <__gmpn_toom42_mul@@Base+0x7d8>  // b.pmore
   3a9c8:	add	x9, x1, #0xf
   3a9cc:	mov	x8, sp
   3a9d0:	and	x9, x9, #0xfffffffffffffff0
   3a9d4:	sub	x26, x8, x9
   3a9d8:	mov	sp, x26
   3a9dc:	add	x8, x23, #0x1
   3a9e0:	stur	x8, [x29, #-88]
   3a9e4:	lsl	x8, x8, #3
   3a9e8:	ldp	x5, x2, [x29, #-24]
   3a9ec:	add	x1, x26, x8
   3a9f0:	add	x27, x1, x8
   3a9f4:	add	x9, x27, x8
   3a9f8:	mov	x0, x26
   3a9fc:	mov	x3, x23
   3aa00:	stur	x19, [x29, #-56]
   3aa04:	sub	x25, x19, x23
   3aa08:	stp	x1, x9, [x29, #-112]
   3aa0c:	add	x28, x9, x8
   3aa10:	mov	x19, x4
   3aa14:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3aa18:	and	w8, w0, #0x1
   3aa1c:	stur	w8, [x29, #-76]
   3aa20:	ldur	x8, [x29, #-16]
   3aa24:	mov	x0, x27
   3aa28:	mov	x3, x19
   3aa2c:	add	x24, x8, x23, lsl #4
   3aa30:	ldur	x8, [x29, #-16]
   3aa34:	mov	x1, x24
   3aa38:	add	x2, x8, x22, lsl #3
   3aa3c:	stur	x2, [x29, #-96]
   3aa40:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3aa44:	ldur	x11, [x29, #-16]
   3aa48:	subs	x8, x23, x19
   3aa4c:	mov	x22, x0
   3aa50:	b.eq	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3aa54:	ldur	x9, [x29, #-32]
   3aa58:	lsl	x9, x9, #3
   3aa5c:	ldr	x10, [x24, x9]
   3aa60:	adds	x10, x10, x22
   3aa64:	str	x10, [x27, x9]
   3aa68:	b.cc	3aad4 <__gmpn_toom42_mul@@Base+0x180>  // b.lo, b.ul, b.last
   3aa6c:	lsl	x9, x20, #3
   3aa70:	sub	x10, x20, x23
   3aa74:	sub	x9, x9, x23, lsl #3
   3aa78:	add	x10, x11, x10, lsl #3
   3aa7c:	add	x9, x9, x26
   3aa80:	mov	w22, #0x1                   	// #1
   3aa84:	add	x1, x10, #0x8
   3aa88:	add	x0, x9, #0x18
   3aa8c:	mov	w9, #0x1                   	// #1
   3aa90:	cmp	x9, x8
   3aa94:	b.ge	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.tcont
   3aa98:	ldr	x10, [x1], #8
   3aa9c:	add	x9, x9, #0x1
   3aaa0:	adds	x10, x10, #0x1
   3aaa4:	str	x10, [x0], #8
   3aaa8:	b.cs	3aa90 <__gmpn_toom42_mul@@Base+0x13c>  // b.hs, b.nlast
   3aaac:	cmp	x24, x27
   3aab0:	mov	x22, xzr
   3aab4:	b.eq	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3aab8:	cmp	x9, x8
   3aabc:	b.ge	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.tcont
   3aac0:	lsl	x8, x23, #2
   3aac4:	sub	x8, x8, x20
   3aac8:	sub	x8, x8, x9
   3aacc:	lsl	x2, x8, #3
   3aad0:	b	3ab10 <__gmpn_toom42_mul@@Base+0x1bc>
   3aad4:	cmp	x8, #0x2
   3aad8:	mov	x22, xzr
   3aadc:	b.lt	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.tstop
   3aae0:	cmp	x24, x27
   3aae4:	b.eq	3ab1c <__gmpn_toom42_mul@@Base+0x1c8>  // b.none
   3aae8:	lsl	x8, x20, #3
   3aaec:	sub	x9, x20, x23
   3aaf0:	mvn	x10, x20
   3aaf4:	sub	x8, x8, x23, lsl #3
   3aaf8:	add	x9, x11, x9, lsl #3
   3aafc:	add	x10, x10, x23, lsl #2
   3ab00:	add	x8, x8, x26
   3ab04:	add	x1, x9, #0x8
   3ab08:	add	x0, x8, #0x18
   3ab0c:	lsl	x2, x10, #3
   3ab10:	bl	bed0 <memcpy@plt>
   3ab14:	ldur	x11, [x29, #-16]
   3ab18:	mov	x22, xzr
   3ab1c:	mov	x19, x28
   3ab20:	add	x8, x28, x23, lsl #3
   3ab24:	lsl	x28, x23, #3
   3ab28:	add	x1, x11, x28
   3ab2c:	mov	x0, x27
   3ab30:	mov	x2, x27
   3ab34:	mov	x3, x23
   3ab38:	stur	x8, [x29, #-40]
   3ab3c:	mov	x24, x11
   3ab40:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ab44:	add	x20, x0, x22, lsl #1
   3ab48:	mov	x0, x27
   3ab4c:	mov	x1, x24
   3ab50:	mov	x2, x27
   3ab54:	mov	x3, x23
   3ab58:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ab5c:	add	x8, x0, x20, lsl #1
   3ab60:	subs	x22, x23, x25
   3ab64:	add	x24, x21, x28
   3ab68:	str	x8, [x27, x28]
   3ab6c:	stp	x24, x28, [x29, #-72]
   3ab70:	b.ne	3abe4 <__gmpn_toom42_mul@@Base+0x290>  // b.any
   3ab74:	ldur	x28, [x29, #-104]
   3ab78:	mov	x1, x21
   3ab7c:	mov	x2, x24
   3ab80:	mov	x3, x23
   3ab84:	mov	x0, x28
   3ab88:	bl	ca70 <__gmpn_add_n@plt>
   3ab8c:	ldp	x20, x22, [x29, #-56]
   3ab90:	lsl	x8, x23, #4
   3ab94:	sub	x8, x8, #0x8
   3ab98:	mov	x10, x23
   3ab9c:	str	x0, [x28, x23, lsl #3]
   3aba0:	subs	x9, x10, #0x1
   3aba4:	b.lt	3abc8 <__gmpn_toom42_mul@@Base+0x274>  // b.tstop
   3aba8:	add	x10, x21, x10, lsl #3
   3abac:	ldur	x10, [x10, #-8]
   3abb0:	ldr	x11, [x21, x8]
   3abb4:	sub	x8, x8, #0x8
   3abb8:	cmp	x10, x11
   3abbc:	mov	x10, x9
   3abc0:	b.eq	3aba0 <__gmpn_toom42_mul@@Base+0x24c>  // b.none
   3abc4:	b.ls	3ad84 <__gmpn_toom42_mul@@Base+0x430>  // b.plast
   3abc8:	mov	x0, x19
   3abcc:	mov	x1, x21
   3abd0:	mov	x2, x24
   3abd4:	mov	x3, x23
   3abd8:	bl	c2d0 <__gmpn_sub_n@plt>
   3abdc:	cbnz	x25, 3ada8 <__gmpn_toom42_mul@@Base+0x454>
   3abe0:	b	3ae54 <__gmpn_toom42_mul@@Base+0x500>
   3abe4:	ldur	x28, [x29, #-104]
   3abe8:	ldur	x20, [x29, #-56]
   3abec:	cbz	x25, 3ac44 <__gmpn_toom42_mul@@Base+0x2f0>
   3abf0:	mov	x0, x28
   3abf4:	mov	x1, x21
   3abf8:	mov	x2, x24
   3abfc:	mov	x3, x25
   3ac00:	bl	ca70 <__gmpn_add_n@plt>
   3ac04:	mov	x8, x25
   3ac08:	cbz	x0, 3ac48 <__gmpn_toom42_mul@@Base+0x2f4>
   3ac0c:	lsl	x8, x23, #4
   3ac10:	add	x8, x8, x20, lsl #3
   3ac14:	add	x8, x8, x26
   3ac18:	add	x10, x8, #0x18
   3ac1c:	mov	w9, #0x1                   	// #1
   3ac20:	mov	x8, x25
   3ac24:	cmp	x8, x23
   3ac28:	b.ge	3ac84 <__gmpn_toom42_mul@@Base+0x330>  // b.tcont
   3ac2c:	ldr	x11, [x21, x8, lsl #3]
   3ac30:	add	x8, x8, #0x1
   3ac34:	adds	x11, x11, #0x1
   3ac38:	str	x11, [x10], #8
   3ac3c:	b.cs	3ac24 <__gmpn_toom42_mul@@Base+0x2d0>  // b.hs, b.nlast
   3ac40:	b	3ac48 <__gmpn_toom42_mul@@Base+0x2f4>
   3ac44:	mov	x8, xzr
   3ac48:	cmp	x28, x21
   3ac4c:	mov	x9, xzr
   3ac50:	b.eq	3ac84 <__gmpn_toom42_mul@@Base+0x330>  // b.none
   3ac54:	cmp	x8, x23
   3ac58:	b.ge	3ac84 <__gmpn_toom42_mul@@Base+0x330>  // b.tcont
   3ac5c:	mov	w9, #0x18                  	// #24
   3ac60:	lsl	x8, x8, #3
   3ac64:	madd	x9, x23, x9, x8
   3ac68:	add	x9, x9, x26
   3ac6c:	add	x0, x9, #0x18
   3ac70:	ldur	x9, [x29, #-64]
   3ac74:	add	x1, x21, x8
   3ac78:	sub	x2, x9, x8
   3ac7c:	bl	bed0 <memcpy@plt>
   3ac80:	mov	x9, xzr
   3ac84:	ldur	x10, [x29, #-64]
   3ac88:	sub	x8, x20, x23, lsl #1
   3ac8c:	str	x9, [x28, x10]
   3ac90:	sub	x9, x10, #0x8
   3ac94:	ldr	x10, [x21, x9]
   3ac98:	cbnz	x10, 3ace0 <__gmpn_toom42_mul@@Base+0x38c>
   3ac9c:	adds	x8, x8, #0x1
   3aca0:	sub	x9, x9, #0x8
   3aca4:	b.cc	3ac94 <__gmpn_toom42_mul@@Base+0x340>  // b.lo, b.ul, b.last
   3aca8:	ldur	x8, [x29, #-56]
   3acac:	mov	x10, x25
   3acb0:	lsl	x24, x8, #3
   3acb4:	sub	x8, x24, #0x8
   3acb8:	subs	x9, x10, #0x1
   3acbc:	b.lt	3ace0 <__gmpn_toom42_mul@@Base+0x38c>  // b.tstop
   3acc0:	add	x10, x21, x10, lsl #3
   3acc4:	ldur	x10, [x10, #-8]
   3acc8:	ldr	x11, [x21, x8]
   3accc:	sub	x8, x8, #0x8
   3acd0:	cmp	x10, x11
   3acd4:	mov	x10, x9
   3acd8:	b.eq	3acb8 <__gmpn_toom42_mul@@Base+0x364>  // b.none
   3acdc:	b.ls	3ae04 <__gmpn_toom42_mul@@Base+0x4b0>  // b.plast
   3ace0:	cbz	x25, 3ad40 <__gmpn_toom42_mul@@Base+0x3ec>
   3ace4:	ldur	x24, [x29, #-72]
   3ace8:	mov	x0, x19
   3acec:	mov	x1, x21
   3acf0:	mov	x3, x25
   3acf4:	mov	x2, x24
   3acf8:	bl	c2d0 <__gmpn_sub_n@plt>
   3acfc:	ldp	x20, x22, [x29, #-56]
   3ad00:	mov	x8, x25
   3ad04:	cbz	x0, 3ad4c <__gmpn_toom42_mul@@Base+0x3f8>
   3ad08:	mov	w8, #0x18                  	// #24
   3ad0c:	mul	x8, x23, x8
   3ad10:	add	x8, x8, x20, lsl #3
   3ad14:	add	x8, x8, x26
   3ad18:	add	x9, x8, #0x20
   3ad1c:	mov	x8, x25
   3ad20:	cmp	x8, x23
   3ad24:	b.ge	3ada4 <__gmpn_toom42_mul@@Base+0x450>  // b.tcont
   3ad28:	ldr	x10, [x21, x8, lsl #3]
   3ad2c:	add	x8, x8, #0x1
   3ad30:	sub	x11, x10, #0x1
   3ad34:	str	x11, [x9], #8
   3ad38:	cbz	x10, 3ad20 <__gmpn_toom42_mul@@Base+0x3cc>
   3ad3c:	b	3ad4c <__gmpn_toom42_mul@@Base+0x3f8>
   3ad40:	ldp	x20, x22, [x29, #-56]
   3ad44:	ldur	x24, [x29, #-72]
   3ad48:	mov	x8, xzr
   3ad4c:	cmp	x19, x21
   3ad50:	b.eq	3ada4 <__gmpn_toom42_mul@@Base+0x450>  // b.none
   3ad54:	cmp	x8, x23
   3ad58:	b.ge	3ada4 <__gmpn_toom42_mul@@Base+0x450>  // b.tcont
   3ad5c:	lsl	x8, x8, #3
   3ad60:	add	x9, x8, x23, lsl #5
   3ad64:	add	x9, x9, x26
   3ad68:	add	x0, x9, #0x20
   3ad6c:	ldur	x9, [x29, #-64]
   3ad70:	add	x1, x21, x8
   3ad74:	sub	x2, x9, x8
   3ad78:	bl	bed0 <memcpy@plt>
   3ad7c:	cbnz	x25, 3ada8 <__gmpn_toom42_mul@@Base+0x454>
   3ad80:	b	3ae54 <__gmpn_toom42_mul@@Base+0x500>
   3ad84:	mov	x0, x19
   3ad88:	mov	x1, x24
   3ad8c:	mov	x2, x21
   3ad90:	mov	x3, x23
   3ad94:	bl	c2d0 <__gmpn_sub_n@plt>
   3ad98:	ldur	w8, [x29, #-76]
   3ad9c:	eor	w8, w8, #0x1
   3ada0:	stur	w8, [x29, #-76]
   3ada4:	cbz	x25, 3ae54 <__gmpn_toom42_mul@@Base+0x500>
   3ada8:	ldur	x0, [x29, #-40]
   3adac:	mov	x1, x28
   3adb0:	mov	x2, x24
   3adb4:	mov	x3, x25
   3adb8:	bl	ca70 <__gmpn_add_n@plt>
   3adbc:	mov	x8, x25
   3adc0:	cbz	x0, 3ae58 <__gmpn_toom42_mul@@Base+0x504>
   3adc4:	lsl	x8, x20, #3
   3adc8:	add	x9, x8, x23, lsl #5
   3adcc:	add	x8, x8, x23, lsl #4
   3add0:	add	x9, x9, x26
   3add4:	add	x8, x8, x26
   3add8:	add	x9, x9, #0x20
   3addc:	add	x10, x8, #0x18
   3ade0:	mov	x8, x25
   3ade4:	cmp	x8, x23
   3ade8:	b.gt	3af4c <__gmpn_toom42_mul@@Base+0x5f8>
   3adec:	ldr	x11, [x10], #8
   3adf0:	add	x8, x8, #0x1
   3adf4:	adds	x11, x11, #0x1
   3adf8:	str	x11, [x9], #8
   3adfc:	b.cs	3ade4 <__gmpn_toom42_mul@@Base+0x490>  // b.hs, b.nlast
   3ae00:	b	3ae58 <__gmpn_toom42_mul@@Base+0x504>
   3ae04:	ldur	x1, [x29, #-72]
   3ae08:	mov	x0, x19
   3ae0c:	mov	x2, x21
   3ae10:	mov	x3, x25
   3ae14:	bl	c2d0 <__gmpn_sub_n@plt>
   3ae18:	cbz	x22, 3ae3c <__gmpn_toom42_mul@@Base+0x4e8>
   3ae1c:	mov	w8, #0x18                  	// #24
   3ae20:	madd	x8, x23, x8, x24
   3ae24:	lsl	x9, x23, #4
   3ae28:	add	x8, x8, x26
   3ae2c:	add	x0, x8, #0x20
   3ae30:	sub	x2, x9, x24
   3ae34:	mov	w1, wzr
   3ae38:	bl	c5f0 <memset@plt>
   3ae3c:	ldur	w8, [x29, #-76]
   3ae40:	ldp	x20, x22, [x29, #-56]
   3ae44:	ldur	x24, [x29, #-72]
   3ae48:	eor	w8, w8, #0x1
   3ae4c:	stur	w8, [x29, #-76]
   3ae50:	cbnz	x25, 3ada8 <__gmpn_toom42_mul@@Base+0x454>
   3ae54:	mov	x8, xzr
   3ae58:	ldur	x9, [x29, #-40]
   3ae5c:	cmp	x9, x28
   3ae60:	b.eq	3af4c <__gmpn_toom42_mul@@Base+0x5f8>  // b.none
   3ae64:	cmp	x8, x23
   3ae68:	b.gt	3af4c <__gmpn_toom42_mul@@Base+0x5f8>
   3ae6c:	sub	x9, x23, x8
   3ae70:	add	x9, x9, #0x1
   3ae74:	cmp	x9, #0x4
   3ae78:	b.cc	3af10 <__gmpn_toom42_mul@@Base+0x5bc>  // b.lo, b.ul, b.last
   3ae7c:	mov	w10, #0x28                  	// #40
   3ae80:	lsl	x11, x8, #3
   3ae84:	madd	x10, x23, x10, x11
   3ae88:	add	x12, x26, x23, lsl #5
   3ae8c:	add	x10, x10, x26
   3ae90:	add	x10, x10, #0x20
   3ae94:	add	x12, x12, #0x20
   3ae98:	cmp	x10, x12
   3ae9c:	b.cs	3aec4 <__gmpn_toom42_mul@@Base+0x570>  // b.hs, b.nlast
   3aea0:	mov	w12, #0x18                  	// #24
   3aea4:	mov	w10, #0x30                  	// #48
   3aea8:	madd	x12, x23, x12, x11
   3aeac:	madd	x10, x23, x10, x26
   3aeb0:	add	x12, x12, x26
   3aeb4:	add	x10, x10, #0x28
   3aeb8:	add	x12, x12, #0x18
   3aebc:	cmp	x12, x10
   3aec0:	b.cc	3af10 <__gmpn_toom42_mul@@Base+0x5bc>  // b.lo, b.ul, b.last
   3aec4:	mov	w12, #0x18                  	// #24
   3aec8:	mov	w13, #0x28                  	// #40
   3aecc:	madd	x12, x23, x12, x11
   3aed0:	madd	x11, x23, x13, x11
   3aed4:	and	x10, x9, #0xfffffffffffffffc
   3aed8:	add	x12, x12, x26
   3aedc:	add	x13, x11, x26
   3aee0:	add	x8, x8, x10
   3aee4:	add	x11, x12, #0x28
   3aee8:	add	x12, x13, #0x30
   3aeec:	mov	x13, x10
   3aef0:	ldp	q0, q1, [x11, #-16]
   3aef4:	subs	x13, x13, #0x4
   3aef8:	add	x11, x11, #0x20
   3aefc:	stp	q0, q1, [x12, #-16]
   3af00:	add	x12, x12, #0x20
   3af04:	b.ne	3aef0 <__gmpn_toom42_mul@@Base+0x59c>  // b.any
   3af08:	cmp	x9, x10
   3af0c:	b.eq	3af4c <__gmpn_toom42_mul@@Base+0x5f8>  // b.none
   3af10:	sub	x9, x23, x8
   3af14:	mov	w10, #0x28                  	// #40
   3af18:	lsl	x11, x8, #3
   3af1c:	mov	w12, #0x18                  	// #24
   3af20:	add	x8, x9, #0x1
   3af24:	madd	x9, x23, x10, x11
   3af28:	madd	x10, x23, x12, x11
   3af2c:	add	x9, x9, x26
   3af30:	add	x10, x10, x26
   3af34:	add	x9, x9, #0x20
   3af38:	add	x10, x10, #0x18
   3af3c:	ldr	x11, [x10], #8
   3af40:	subs	x8, x8, #0x1
   3af44:	str	x11, [x9], #8
   3af48:	b.ne	3af3c <__gmpn_toom42_mul@@Base+0x5e8>  // b.any
   3af4c:	ldur	x20, [x29, #-112]
   3af50:	mov	x0, x22
   3af54:	mov	x2, x19
   3af58:	mov	x3, x23
   3af5c:	mov	x1, x20
   3af60:	lsl	x24, x23, #1
   3af64:	bl	c990 <__gmpn_mul_n@plt>
   3af68:	ldr	x8, [x20, x23, lsl #3]
   3af6c:	cbz	x8, 3af88 <__gmpn_toom42_mul@@Base+0x634>
   3af70:	add	x0, x22, x23, lsl #3
   3af74:	mov	x1, x0
   3af78:	mov	x2, x19
   3af7c:	mov	x3, x23
   3af80:	bl	ca70 <__gmpn_add_n@plt>
   3af84:	b	3af8c <__gmpn_toom42_mul@@Base+0x638>
   3af88:	mov	x0, xzr
   3af8c:	ldur	x2, [x29, #-40]
   3af90:	ldur	x3, [x29, #-88]
   3af94:	add	x19, x22, x24, lsl #3
   3af98:	str	x0, [x19], #8
   3af9c:	mov	x0, x19
   3afa0:	mov	x1, x27
   3afa4:	bl	c990 <__gmpn_mul_n@plt>
   3afa8:	ldp	x4, x8, [x29, #-32]
   3afac:	add	x22, x8, x23, lsl #5
   3afb0:	cmp	x4, x25
   3afb4:	mov	x0, x22
   3afb8:	b.le	3afd0 <__gmpn_toom42_mul@@Base+0x67c>
   3afbc:	ldur	x1, [x29, #-96]
   3afc0:	ldur	x3, [x29, #-72]
   3afc4:	mov	x2, x4
   3afc8:	mov	x4, x25
   3afcc:	b	3afdc <__gmpn_toom42_mul@@Base+0x688>
   3afd0:	ldur	x1, [x29, #-72]
   3afd4:	ldur	x3, [x29, #-96]
   3afd8:	mov	x2, x25
   3afdc:	bl	ccd0 <__gmpn_mul@plt>
   3afe0:	ldur	x8, [x29, #-24]
   3afe4:	ldr	x20, [x22]
   3afe8:	mov	x1, x26
   3afec:	mov	x2, x28
   3aff0:	add	x27, x8, x24, lsl #3
   3aff4:	mov	x0, x27
   3aff8:	mov	x3, x23
   3affc:	bl	c990 <__gmpn_mul_n@plt>
   3b000:	ldr	x8, [x26, x23, lsl #3]
   3b004:	cmp	x8, #0x3
   3b008:	b.eq	3b048 <__gmpn_toom42_mul@@Base+0x6f4>  // b.none
   3b00c:	cmp	x8, #0x2
   3b010:	b.eq	3b078 <__gmpn_toom42_mul@@Base+0x724>  // b.none
   3b014:	cmp	x8, #0x1
   3b018:	b.ne	3b0a4 <__gmpn_toom42_mul@@Base+0x750>  // b.any
   3b01c:	ldur	x8, [x29, #-64]
   3b020:	mov	x2, x28
   3b024:	mov	x3, x23
   3b028:	ldr	x22, [x28, x8]
   3b02c:	add	x0, x27, x8
   3b030:	mov	x1, x0
   3b034:	bl	ca70 <__gmpn_add_n@plt>
   3b038:	add	x22, x0, x22
   3b03c:	ldr	x8, [x28, x23, lsl #3]
   3b040:	cbnz	x8, 3b0b0 <__gmpn_toom42_mul@@Base+0x75c>
   3b044:	b	3b0c8 <__gmpn_toom42_mul@@Base+0x774>
   3b048:	ldur	x9, [x29, #-64]
   3b04c:	mov	w3, #0x3                   	// #3
   3b050:	mov	x1, x28
   3b054:	mov	x2, x23
   3b058:	ldr	x8, [x28, x9]
   3b05c:	add	x0, x27, x9
   3b060:	add	x22, x8, x8, lsl #1
   3b064:	bl	d400 <__gmpn_addmul_1@plt>
   3b068:	add	x22, x22, x0
   3b06c:	ldr	x8, [x28, x23, lsl #3]
   3b070:	cbnz	x8, 3b0b0 <__gmpn_toom42_mul@@Base+0x75c>
   3b074:	b	3b0c8 <__gmpn_toom42_mul@@Base+0x774>
   3b078:	ldur	x8, [x29, #-64]
   3b07c:	mov	x2, x28
   3b080:	mov	x3, x23
   3b084:	ldr	x22, [x28, x8]
   3b088:	add	x0, x27, x8
   3b08c:	mov	x1, x0
   3b090:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3b094:	add	x22, x0, x22, lsl #1
   3b098:	ldr	x8, [x28, x23, lsl #3]
   3b09c:	cbnz	x8, 3b0b0 <__gmpn_toom42_mul@@Base+0x75c>
   3b0a0:	b	3b0c8 <__gmpn_toom42_mul@@Base+0x774>
   3b0a4:	mov	x22, xzr
   3b0a8:	ldr	x8, [x28, x23, lsl #3]
   3b0ac:	cbz	x8, 3b0c8 <__gmpn_toom42_mul@@Base+0x774>
   3b0b0:	add	x0, x27, x23, lsl #3
   3b0b4:	mov	x1, x0
   3b0b8:	mov	x2, x26
   3b0bc:	mov	x3, x23
   3b0c0:	bl	ca70 <__gmpn_add_n@plt>
   3b0c4:	add	x22, x0, x22
   3b0c8:	str	x22, [x27, x24, lsl #3]
   3b0cc:	ldp	x22, x1, [x29, #-24]
   3b0d0:	mov	x2, x21
   3b0d4:	mov	x3, x23
   3b0d8:	mov	x0, x22
   3b0dc:	bl	c990 <__gmpn_mul_n@plt>
   3b0e0:	ldur	x8, [x29, #-32]
   3b0e4:	ldur	x2, [x29, #-48]
   3b0e8:	ldur	w5, [x29, #-76]
   3b0ec:	mov	x0, x22
   3b0f0:	add	x4, x8, x25
   3b0f4:	mov	x1, x19
   3b0f8:	mov	x3, x23
   3b0fc:	mov	x6, x20
   3b100:	bl	ca20 <__gmpn_toom_interpolate_5pts@plt>
   3b104:	ldur	x0, [x29, #-8]
   3b108:	cbnz	x0, 3b140 <__gmpn_toom42_mul@@Base+0x7ec>
   3b10c:	mov	sp, x29
   3b110:	ldp	x20, x19, [sp, #80]
   3b114:	ldp	x22, x21, [sp, #64]
   3b118:	ldp	x24, x23, [sp, #48]
   3b11c:	ldp	x26, x25, [sp, #32]
   3b120:	ldp	x28, x27, [sp, #16]
   3b124:	ldp	x29, x30, [sp], #96
   3b128:	ret
   3b12c:	sub	x0, x29, #0x8
   3b130:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3b134:	ldur	x4, [x29, #-32]
   3b138:	mov	x26, x0
   3b13c:	b	3a9dc <__gmpn_toom42_mul@@Base+0x88>
   3b140:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3b144:	b	3b10c <__gmpn_toom42_mul@@Base+0x7b8>

000000000003b148 <__gmpn_toom52_mul@@Base>:
   3b148:	sub	sp, sp, #0xf0
   3b14c:	add	x8, x4, x4, lsl #2
   3b150:	stp	x28, x27, [sp, #160]
   3b154:	stp	x22, x21, [sp, #208]
   3b158:	stp	x20, x19, [sp, #224]
   3b15c:	mov	x28, x5
   3b160:	mov	x19, x4
   3b164:	mov	x20, x3
   3b168:	mov	x22, x1
   3b16c:	cmp	x8, x2, lsl #1
   3b170:	mov	x21, x0
   3b174:	stp	x29, x30, [sp, #144]
   3b178:	stp	x26, x25, [sp, #176]
   3b17c:	stp	x24, x23, [sp, #192]
   3b180:	add	x29, sp, #0x90
   3b184:	b.le	3b194 <__gmpn_toom52_mul@@Base+0x4c>
   3b188:	sub	x8, x19, #0x1
   3b18c:	asr	x26, x8, #1
   3b190:	b	3b1a8 <__gmpn_toom52_mul@@Base+0x60>
   3b194:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   3b198:	sub	x8, x2, #0x1
   3b19c:	movk	x9, #0xcccd
   3b1a0:	umulh	x8, x8, x9
   3b1a4:	lsr	x26, x8, #2
   3b1a8:	add	x23, x26, #0x1
   3b1ac:	add	x8, x23, x23, lsl #1
   3b1b0:	lsl	x10, x23, #2
   3b1b4:	add	x9, x28, x23, lsl #5
   3b1b8:	lsl	x8, x8, #3
   3b1bc:	str	x10, [sp, #8]
   3b1c0:	stur	x9, [x29, #-64]
   3b1c4:	add	x1, x9, #0x20
   3b1c8:	add	x9, x21, x8
   3b1cc:	add	x8, x28, x8
   3b1d0:	add	x27, x9, #0x18
   3b1d4:	sub	x5, x2, x10
   3b1d8:	add	x6, x8, #0x18
   3b1dc:	mov	w2, #0x4                   	// #4
   3b1e0:	mov	x0, x27
   3b1e4:	mov	x3, x22
   3b1e8:	mov	x4, x23
   3b1ec:	sub	x24, x19, x23
   3b1f0:	str	x1, [sp, #72]
   3b1f4:	stur	x5, [x29, #-40]
   3b1f8:	str	x6, [sp, #64]
   3b1fc:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3b200:	mov	x3, x24
   3b204:	mov	x24, x23
   3b208:	and	w25, w0, #0x2
   3b20c:	subs	x8, x23, x3
   3b210:	add	x23, x20, x23, lsl #3
   3b214:	stur	x3, [x29, #-8]
   3b218:	stur	x24, [x29, #-24]
   3b21c:	stur	x27, [x29, #-56]
   3b220:	str	x22, [sp, #56]
   3b224:	stur	x19, [x29, #-32]
   3b228:	b.ne	3b2a0 <__gmpn_toom52_mul@@Base+0x158>  // b.any
   3b22c:	mov	x0, x21
   3b230:	mov	x1, x20
   3b234:	mov	x2, x23
   3b238:	mov	x3, x24
   3b23c:	bl	ca70 <__gmpn_add_n@plt>
   3b240:	mov	w8, #0x8                   	// #8
   3b244:	bfi	x8, x26, #4, #60
   3b248:	mov	x9, x26
   3b24c:	str	x0, [x21, x24, lsl #3]
   3b250:	add	x10, x9, #0x1
   3b254:	cmp	x10, #0x1
   3b258:	b.lt	3b278 <__gmpn_toom52_mul@@Base+0x130>  // b.tstop
   3b25c:	ldr	x10, [x20, x9, lsl #3]
   3b260:	ldr	x11, [x20, x8]
   3b264:	sub	x9, x9, #0x1
   3b268:	sub	x8, x8, #0x8
   3b26c:	cmp	x10, x11
   3b270:	b.eq	3b250 <__gmpn_toom52_mul@@Base+0x108>  // b.none
   3b274:	b.ls	3b4e4 <__gmpn_toom52_mul@@Base+0x39c>  // b.plast
   3b278:	add	x8, x28, x24, lsl #4
   3b27c:	add	x0, x8, #0x10
   3b280:	mov	x1, x20
   3b284:	mov	x2, x23
   3b288:	mov	x3, x24
   3b28c:	lsl	x27, x24, #1
   3b290:	bl	c2d0 <__gmpn_sub_n@plt>
   3b294:	ldur	x3, [x29, #-8]
   3b298:	mov	w16, w25
   3b29c:	b	3b55c <__gmpn_toom52_mul@@Base+0x414>
   3b2a0:	stur	x8, [x29, #-16]
   3b2a4:	cbz	x3, 3b2f0 <__gmpn_toom52_mul@@Base+0x1a8>
   3b2a8:	mov	x0, x21
   3b2ac:	mov	x1, x20
   3b2b0:	mov	x2, x23
   3b2b4:	bl	ca70 <__gmpn_add_n@plt>
   3b2b8:	ldur	x3, [x29, #-8]
   3b2bc:	mov	x8, x3
   3b2c0:	cbz	x0, 3b2f4 <__gmpn_toom52_mul@@Base+0x1ac>
   3b2c4:	mov	w9, #0x1                   	// #1
   3b2c8:	mov	x8, x3
   3b2cc:	cmp	x8, x26
   3b2d0:	b.gt	3b3a0 <__gmpn_toom52_mul@@Base+0x258>
   3b2d4:	lsl	x10, x8, #3
   3b2d8:	ldr	x11, [x20, x10]
   3b2dc:	add	x8, x8, #0x1
   3b2e0:	adds	x11, x11, #0x1
   3b2e4:	str	x11, [x21, x10]
   3b2e8:	b.cs	3b2cc <__gmpn_toom52_mul@@Base+0x184>  // b.hs, b.nlast
   3b2ec:	b	3b2f4 <__gmpn_toom52_mul@@Base+0x1ac>
   3b2f0:	mov	x8, xzr
   3b2f4:	cmp	x21, x20
   3b2f8:	mov	x9, xzr
   3b2fc:	b.eq	3b3a0 <__gmpn_toom52_mul@@Base+0x258>  // b.none
   3b300:	cmp	x8, x26
   3b304:	b.gt	3b3a0 <__gmpn_toom52_mul@@Base+0x258>
   3b308:	sub	x9, x26, x8
   3b30c:	add	x9, x9, #0x1
   3b310:	cmp	x9, #0x4
   3b314:	b.cc	3b378 <__gmpn_toom52_mul@@Base+0x230>  // b.lo, b.ul, b.last
   3b318:	lsl	x11, x8, #3
   3b31c:	lsl	x10, x24, #3
   3b320:	add	x12, x21, x11
   3b324:	add	x13, x20, x10
   3b328:	cmp	x12, x13
   3b32c:	b.cs	3b340 <__gmpn_toom52_mul@@Base+0x1f8>  // b.hs, b.nlast
   3b330:	add	x10, x21, x10
   3b334:	add	x12, x20, x11
   3b338:	cmp	x12, x10
   3b33c:	b.cc	3b378 <__gmpn_toom52_mul@@Base+0x230>  // b.lo, b.ul, b.last
   3b340:	and	x10, x9, #0xfffffffffffffffc
   3b344:	add	x12, x11, #0x10
   3b348:	add	x8, x8, x10
   3b34c:	add	x11, x20, x12
   3b350:	add	x12, x21, x12
   3b354:	mov	x13, x10
   3b358:	ldp	q0, q1, [x11, #-16]
   3b35c:	add	x11, x11, #0x20
   3b360:	subs	x13, x13, #0x4
   3b364:	stp	q0, q1, [x12, #-16]
   3b368:	add	x12, x12, #0x20
   3b36c:	b.ne	3b358 <__gmpn_toom52_mul@@Base+0x210>  // b.any
   3b370:	cmp	x9, x10
   3b374:	b.eq	3b39c <__gmpn_toom52_mul@@Base+0x254>  // b.none
   3b378:	sub	x9, x26, x8
   3b37c:	lsl	x10, x8, #3
   3b380:	add	x8, x9, #0x1
   3b384:	add	x9, x21, x10
   3b388:	add	x10, x20, x10
   3b38c:	ldr	x11, [x10], #8
   3b390:	subs	x8, x8, #0x1
   3b394:	str	x11, [x9], #8
   3b398:	b.ne	3b38c <__gmpn_toom52_mul@@Base+0x244>  // b.any
   3b39c:	mov	x9, xzr
   3b3a0:	sub	x8, x19, x26, lsl #1
   3b3a4:	str	x9, [x21, x24, lsl #3]
   3b3a8:	sub	x8, x8, #0x2
   3b3ac:	lsl	x9, x26, #3
   3b3b0:	ldr	x10, [x20, x9]
   3b3b4:	cbnz	x10, 3b3fc <__gmpn_toom52_mul@@Base+0x2b4>
   3b3b8:	adds	x8, x8, #0x1
   3b3bc:	sub	x9, x9, #0x8
   3b3c0:	b.cc	3b3b0 <__gmpn_toom52_mul@@Base+0x268>  // b.lo, b.ul, b.last
   3b3c4:	sub	x8, x19, x26
   3b3c8:	lsl	x9, x19, #3
   3b3cc:	sub	x8, x8, #0x2
   3b3d0:	sub	x9, x9, #0x8
   3b3d4:	add	x10, x8, #0x1
   3b3d8:	cmp	x10, #0x1
   3b3dc:	b.lt	3b3fc <__gmpn_toom52_mul@@Base+0x2b4>  // b.tstop
   3b3e0:	ldr	x10, [x20, x8, lsl #3]
   3b3e4:	ldr	x11, [x20, x9]
   3b3e8:	sub	x8, x8, #0x1
   3b3ec:	sub	x9, x9, #0x8
   3b3f0:	cmp	x10, x11
   3b3f4:	b.eq	3b3d4 <__gmpn_toom52_mul@@Base+0x28c>  // b.none
   3b3f8:	b.ls	3b508 <__gmpn_toom52_mul@@Base+0x3c0>  // b.plast
   3b3fc:	add	x8, x28, x24, lsl #4
   3b400:	lsl	x27, x24, #1
   3b404:	add	x22, x8, #0x10
   3b408:	cbz	x3, 3b458 <__gmpn_toom52_mul@@Base+0x310>
   3b40c:	mov	x0, x22
   3b410:	mov	x1, x20
   3b414:	mov	x2, x23
   3b418:	bl	c2d0 <__gmpn_sub_n@plt>
   3b41c:	ldur	x3, [x29, #-8]
   3b420:	mov	x8, x3
   3b424:	cbz	x0, 3b45c <__gmpn_toom52_mul@@Base+0x314>
   3b428:	add	x8, x26, x19
   3b42c:	add	x8, x28, x8, lsl #3
   3b430:	add	x9, x8, #0x18
   3b434:	mov	x8, x3
   3b438:	cmp	x8, x26
   3b43c:	b.gt	3b4dc <__gmpn_toom52_mul@@Base+0x394>
   3b440:	ldr	x10, [x20, x8, lsl #3]
   3b444:	add	x8, x8, #0x1
   3b448:	sub	x11, x10, #0x1
   3b44c:	str	x11, [x9], #8
   3b450:	cbz	x10, 3b438 <__gmpn_toom52_mul@@Base+0x2f0>
   3b454:	b	3b45c <__gmpn_toom52_mul@@Base+0x314>
   3b458:	mov	x8, xzr
   3b45c:	cmp	x22, x20
   3b460:	b.eq	3b4dc <__gmpn_toom52_mul@@Base+0x394>  // b.none
   3b464:	cmp	x8, x26
   3b468:	b.gt	3b4dc <__gmpn_toom52_mul@@Base+0x394>
   3b46c:	sub	x9, x26, x8
   3b470:	add	x9, x9, #0x1
   3b474:	cmp	x9, #0x4
   3b478:	b.cc	3b4ac <__gmpn_toom52_mul@@Base+0x364>  // b.lo, b.ul, b.last
   3b47c:	add	x10, x8, x26, lsl #1
   3b480:	add	x12, x28, x10, lsl #3
   3b484:	add	x10, x12, #0x20
   3b488:	add	x11, x20, x24, lsl #3
   3b48c:	cmp	x10, x11
   3b490:	add	x11, x20, x8, lsl #3
   3b494:	b.cs	3ba2c <__gmpn_toom52_mul@@Base+0x8e4>  // b.hs, b.nlast
   3b498:	mov	w10, #0x18                  	// #24
   3b49c:	madd	x10, x26, x10, x28
   3b4a0:	add	x10, x10, #0x28
   3b4a4:	cmp	x11, x10
   3b4a8:	b.cs	3ba2c <__gmpn_toom52_mul@@Base+0x8e4>  // b.hs, b.nlast
   3b4ac:	mov	w16, w25
   3b4b0:	add	x10, x8, x26, lsl #1
   3b4b4:	sub	x9, x26, x8
   3b4b8:	add	x10, x28, x10, lsl #3
   3b4bc:	add	x9, x9, #0x1
   3b4c0:	add	x10, x10, #0x20
   3b4c4:	add	x8, x20, x8, lsl #3
   3b4c8:	ldr	x11, [x8], #8
   3b4cc:	subs	x9, x9, #0x1
   3b4d0:	str	x11, [x10], #8
   3b4d4:	b.ne	3b4c8 <__gmpn_toom52_mul@@Base+0x380>  // b.any
   3b4d8:	b	3b55c <__gmpn_toom52_mul@@Base+0x414>
   3b4dc:	mov	w16, w25
   3b4e0:	b	3b55c <__gmpn_toom52_mul@@Base+0x414>
   3b4e4:	add	x8, x28, x24, lsl #4
   3b4e8:	add	x0, x8, #0x10
   3b4ec:	mov	x1, x23
   3b4f0:	mov	x2, x20
   3b4f4:	mov	x3, x24
   3b4f8:	lsl	x27, x24, #1
   3b4fc:	bl	c2d0 <__gmpn_sub_n@plt>
   3b500:	ldur	x3, [x29, #-8]
   3b504:	b	3b554 <__gmpn_toom52_mul@@Base+0x40c>
   3b508:	add	x8, x28, x24, lsl #4
   3b50c:	add	x22, x8, #0x10
   3b510:	mov	x0, x22
   3b514:	mov	x1, x23
   3b518:	mov	x2, x20
   3b51c:	lsl	x27, x24, #1
   3b520:	mov	x24, x3
   3b524:	bl	c2d0 <__gmpn_sub_n@plt>
   3b528:	ldur	x8, [x29, #-16]
   3b52c:	cbz	x8, 3b54c <__gmpn_toom52_mul@@Base+0x404>
   3b530:	lsl	x8, x26, #1
   3b534:	sub	x8, x8, x19
   3b538:	lsl	x8, x8, #3
   3b53c:	add	x0, x22, x24, lsl #3
   3b540:	add	x2, x8, #0x10
   3b544:	mov	w1, wzr
   3b548:	bl	c5f0 <memset@plt>
   3b54c:	mov	x3, x24
   3b550:	ldur	x24, [x29, #-24]
   3b554:	mov	w16, w25
   3b558:	orr	w16, w25, #0x1
   3b55c:	add	x8, x21, x27, lsl #3
   3b560:	mov	x22, x23
   3b564:	add	x15, x8, #0x10
   3b568:	add	x25, x26, #0x2
   3b56c:	mov	w19, w16
   3b570:	str	x8, [sp, #40]
   3b574:	cbz	x3, 3b5dc <__gmpn_toom52_mul@@Base+0x494>
   3b578:	mov	x0, x15
   3b57c:	mov	x1, x21
   3b580:	mov	x2, x22
   3b584:	mov	x23, x15
   3b588:	bl	ca70 <__gmpn_add_n@plt>
   3b58c:	ldur	x3, [x29, #-8]
   3b590:	mov	w16, w19
   3b594:	mov	x15, x23
   3b598:	mov	x8, x3
   3b59c:	cbz	x0, 3b5e0 <__gmpn_toom52_mul@@Base+0x498>
   3b5a0:	ldur	x8, [x29, #-32]
   3b5a4:	add	x8, x27, x8
   3b5a8:	lsl	x8, x8, #3
   3b5ac:	sub	x8, x8, x26, lsl #3
   3b5b0:	add	x9, x8, #0x8
   3b5b4:	mov	x8, x3
   3b5b8:	cmp	x8, x25
   3b5bc:	b.ge	3b684 <__gmpn_toom52_mul@@Base+0x53c>  // b.tcont
   3b5c0:	ldr	x10, [x21, x8, lsl #3]
   3b5c4:	add	x8, x8, #0x1
   3b5c8:	adds	x10, x10, #0x1
   3b5cc:	str	x10, [x21, x9]
   3b5d0:	add	x9, x9, #0x8
   3b5d4:	b.cs	3b5b8 <__gmpn_toom52_mul@@Base+0x470>  // b.hs, b.nlast
   3b5d8:	b	3b5e0 <__gmpn_toom52_mul@@Base+0x498>
   3b5dc:	mov	x8, xzr
   3b5e0:	cmp	x15, x21
   3b5e4:	b.eq	3b684 <__gmpn_toom52_mul@@Base+0x53c>  // b.none
   3b5e8:	cmp	x8, x25
   3b5ec:	b.ge	3b684 <__gmpn_toom52_mul@@Base+0x53c>  // b.tcont
   3b5f0:	sub	x9, x26, x8
   3b5f4:	add	x10, x9, #0x2
   3b5f8:	cmp	x10, #0x4
   3b5fc:	lsl	x9, x27, #3
   3b600:	b.cc	3b660 <__gmpn_toom52_mul@@Base+0x518>  // b.lo, b.ul, b.last
   3b604:	add	x11, x27, x8
   3b608:	add	x11, x21, x11, lsl #3
   3b60c:	add	x11, x11, #0x10
   3b610:	add	x12, x21, x25, lsl #3
   3b614:	cmp	x11, x12
   3b618:	add	x11, x21, x8, lsl #3
   3b61c:	b.cs	3b634 <__gmpn_toom52_mul@@Base+0x4ec>  // b.hs, b.nlast
   3b620:	add	x12, x27, x26
   3b624:	add	x12, x21, x12, lsl #3
   3b628:	add	x12, x12, #0x20
   3b62c:	cmp	x11, x12
   3b630:	b.cc	3b660 <__gmpn_toom52_mul@@Base+0x518>  // b.lo, b.ul, b.last
   3b634:	and	x12, x10, #0xfffffffffffffffc
   3b638:	add	x8, x8, x12
   3b63c:	mov	x13, x12
   3b640:	ldp	q0, q1, [x11]
   3b644:	add	x14, x11, x9
   3b648:	subs	x13, x13, #0x4
   3b64c:	add	x11, x11, #0x20
   3b650:	stp	q0, q1, [x14, #16]
   3b654:	b.ne	3b640 <__gmpn_toom52_mul@@Base+0x4f8>  // b.any
   3b658:	cmp	x10, x12
   3b65c:	b.eq	3b684 <__gmpn_toom52_mul@@Base+0x53c>  // b.none
   3b660:	sub	x10, x26, x8
   3b664:	add	x9, x9, #0x10
   3b668:	add	x10, x10, #0x2
   3b66c:	add	x8, x21, x8, lsl #3
   3b670:	ldr	x11, [x8]
   3b674:	subs	x10, x10, #0x1
   3b678:	str	x11, [x8, x9]
   3b67c:	add	x8, x8, #0x8
   3b680:	b.ne	3b670 <__gmpn_toom52_mul@@Base+0x528>  // b.any
   3b684:	add	x8, x21, x24, lsl #3
   3b688:	add	x17, x8, #0x8
   3b68c:	str	x25, [sp, #32]
   3b690:	stur	x28, [x29, #-16]
   3b694:	stur	x22, [x29, #-48]
   3b698:	str	x20, [sp, #48]
   3b69c:	stp	x17, x15, [sp, #16]
   3b6a0:	tbnz	w16, #0, 3b710 <__gmpn_toom52_mul@@Base+0x5c8>
   3b6a4:	add	x8, x28, x27, lsl #3
   3b6a8:	subs	x13, x24, x3
   3b6ac:	add	x25, x8, #0x10
   3b6b0:	mov	x23, x22
   3b6b4:	str	xzr, [x17, x24, lsl #3]
   3b6b8:	b.ne	3b788 <__gmpn_toom52_mul@@Base+0x640>  // b.any
   3b6bc:	add	x8, x20, x26, lsl #4
   3b6c0:	add	x8, x8, #0x8
   3b6c4:	add	x9, x26, #0x1
   3b6c8:	cmp	x9, #0x1
   3b6cc:	b.lt	3b6e8 <__gmpn_toom52_mul@@Base+0x5a0>  // b.tstop
   3b6d0:	ldr	x9, [x25, x26, lsl #3]
   3b6d4:	ldr	x10, [x8], #-8
   3b6d8:	sub	x26, x26, #0x1
   3b6dc:	cmp	x9, x10
   3b6e0:	b.eq	3b6c4 <__gmpn_toom52_mul@@Base+0x57c>  // b.none
   3b6e4:	b.ls	3ba00 <__gmpn_toom52_mul@@Base+0x8b8>  // b.plast
   3b6e8:	mov	x0, x17
   3b6ec:	mov	x1, x25
   3b6f0:	mov	x2, x23
   3b6f4:	mov	x3, x24
   3b6f8:	mov	x28, x27
   3b6fc:	mov	x27, x21
   3b700:	mov	w20, w16
   3b704:	bl	c2d0 <__gmpn_sub_n@plt>
   3b708:	ldr	x14, [sp, #8]
   3b70c:	b	3bab8 <__gmpn_toom52_mul@@Base+0x970>
   3b710:	add	x8, x28, x27, lsl #3
   3b714:	add	x25, x8, #0x10
   3b718:	cbz	x3, 3b858 <__gmpn_toom52_mul@@Base+0x710>
   3b71c:	mov	x0, x17
   3b720:	mov	x1, x25
   3b724:	mov	x2, x22
   3b728:	mov	x23, x17
   3b72c:	bl	ca70 <__gmpn_add_n@plt>
   3b730:	ldur	x12, [x29, #-8]
   3b734:	mov	x17, x23
   3b738:	mov	w16, w19
   3b73c:	mov	x8, x12
   3b740:	cbz	x0, 3b85c <__gmpn_toom52_mul@@Base+0x714>
   3b744:	ldur	x9, [x29, #-32]
   3b748:	add	x8, x21, x9, lsl #3
   3b74c:	add	x9, x27, x9
   3b750:	add	x10, x8, #0x8
   3b754:	sub	x8, x9, x26
   3b758:	add	x8, x28, x8, lsl #3
   3b75c:	add	x11, x8, #0x8
   3b760:	mov	w9, #0x1                   	// #1
   3b764:	mov	x8, x12
   3b768:	cmp	x8, x26
   3b76c:	b.gt	3b92c <__gmpn_toom52_mul@@Base+0x7e4>
   3b770:	ldr	x12, [x11], #8
   3b774:	add	x8, x8, #0x1
   3b778:	adds	x12, x12, #0x1
   3b77c:	str	x12, [x10], #8
   3b780:	b.cs	3b768 <__gmpn_toom52_mul@@Base+0x620>  // b.hs, b.nlast
   3b784:	b	3b85c <__gmpn_toom52_mul@@Base+0x714>
   3b788:	ldur	x8, [x29, #-32]
   3b78c:	add	x9, x27, x26
   3b790:	add	x9, x28, x9, lsl #3
   3b794:	add	x9, x9, #0x10
   3b798:	sub	x8, x8, x26, lsl #1
   3b79c:	sub	x8, x8, #0x2
   3b7a0:	ldr	x10, [x9]
   3b7a4:	cbnz	x10, 3b7ec <__gmpn_toom52_mul@@Base+0x6a4>
   3b7a8:	adds	x8, x8, #0x1
   3b7ac:	sub	x9, x9, #0x8
   3b7b0:	b.cc	3b7a0 <__gmpn_toom52_mul@@Base+0x658>  // b.lo, b.ul, b.last
   3b7b4:	ldur	x9, [x29, #-32]
   3b7b8:	mov	x10, x3
   3b7bc:	add	x8, x20, x9, lsl #3
   3b7c0:	add	x9, x27, x9
   3b7c4:	sub	x9, x9, x26
   3b7c8:	sub	x8, x8, #0x8
   3b7cc:	add	x9, x28, x9, lsl #3
   3b7d0:	subs	x10, x10, #0x1
   3b7d4:	b.lt	3b7ec <__gmpn_toom52_mul@@Base+0x6a4>  // b.tstop
   3b7d8:	ldr	x11, [x9], #-8
   3b7dc:	ldr	x12, [x8], #-8
   3b7e0:	cmp	x11, x12
   3b7e4:	b.eq	3b7d0 <__gmpn_toom52_mul@@Base+0x688>  // b.none
   3b7e8:	b.ls	3ba68 <__gmpn_toom52_mul@@Base+0x920>  // b.plast
   3b7ec:	cbz	x3, 3b944 <__gmpn_toom52_mul@@Base+0x7fc>
   3b7f0:	mov	x0, x17
   3b7f4:	mov	x1, x25
   3b7f8:	mov	x2, x23
   3b7fc:	mov	x22, x17
   3b800:	bl	c2d0 <__gmpn_sub_n@plt>
   3b804:	ldur	x11, [x29, #-8]
   3b808:	mov	x17, x22
   3b80c:	mov	w16, w19
   3b810:	mov	x8, x11
   3b814:	cbz	x0, 3b948 <__gmpn_toom52_mul@@Base+0x800>
   3b818:	ldur	x9, [x29, #-32]
   3b81c:	add	x8, x21, x9, lsl #3
   3b820:	add	x10, x27, x9
   3b824:	add	x9, x8, #0x8
   3b828:	sub	x8, x10, x26
   3b82c:	add	x8, x28, x8, lsl #3
   3b830:	add	x10, x8, #0x8
   3b834:	mov	x8, x11
   3b838:	cmp	x8, x26
   3b83c:	b.gt	3b9ec <__gmpn_toom52_mul@@Base+0x8a4>
   3b840:	ldr	x11, [x10], #8
   3b844:	add	x8, x8, #0x1
   3b848:	sub	x12, x11, #0x1
   3b84c:	str	x12, [x9], #8
   3b850:	cbz	x11, 3b838 <__gmpn_toom52_mul@@Base+0x6f0>
   3b854:	b	3b948 <__gmpn_toom52_mul@@Base+0x800>
   3b858:	mov	x8, xzr
   3b85c:	cmp	x17, x25
   3b860:	mov	x9, xzr
   3b864:	b.eq	3b92c <__gmpn_toom52_mul@@Base+0x7e4>  // b.none
   3b868:	ldr	x14, [sp, #8]
   3b86c:	cmp	x8, x26
   3b870:	b.gt	3b930 <__gmpn_toom52_mul@@Base+0x7e8>
   3b874:	sub	x9, x26, x8
   3b878:	add	x9, x9, #0x1
   3b87c:	cmp	x9, #0x4
   3b880:	b.cc	3b8f4 <__gmpn_toom52_mul@@Base+0x7ac>  // b.lo, b.ul, b.last
   3b884:	add	x10, x8, x26
   3b888:	add	x13, x27, x26
   3b88c:	add	x12, x21, x10, lsl #3
   3b890:	add	x10, x28, x13, lsl #3
   3b894:	add	x11, x27, x8
   3b898:	add	x13, x12, #0x10
   3b89c:	add	x10, x10, #0x18
   3b8a0:	cmp	x13, x10
   3b8a4:	add	x11, x28, x11, lsl #3
   3b8a8:	b.cs	3b8c0 <__gmpn_toom52_mul@@Base+0x778>  // b.hs, b.nlast
   3b8ac:	add	x10, x21, x26, lsl #4
   3b8b0:	add	x10, x10, #0x18
   3b8b4:	add	x13, x11, #0x10
   3b8b8:	cmp	x13, x10
   3b8bc:	b.cc	3b8f4 <__gmpn_toom52_mul@@Base+0x7ac>  // b.lo, b.ul, b.last
   3b8c0:	and	x10, x9, #0xfffffffffffffffc
   3b8c4:	add	x11, x11, #0x20
   3b8c8:	add	x8, x8, x10
   3b8cc:	add	x12, x12, #0x20
   3b8d0:	mov	x13, x10
   3b8d4:	ldp	q0, q1, [x11, #-16]
   3b8d8:	subs	x13, x13, #0x4
   3b8dc:	add	x11, x11, #0x20
   3b8e0:	stp	q0, q1, [x12, #-16]
   3b8e4:	add	x12, x12, #0x20
   3b8e8:	b.ne	3b8d4 <__gmpn_toom52_mul@@Base+0x78c>  // b.any
   3b8ec:	cmp	x9, x10
   3b8f0:	b.eq	3b924 <__gmpn_toom52_mul@@Base+0x7dc>  // b.none
   3b8f4:	sub	x9, x26, x8
   3b8f8:	add	x10, x8, x26
   3b8fc:	add	x11, x27, x8
   3b900:	add	x8, x9, #0x1
   3b904:	add	x9, x21, x10, lsl #3
   3b908:	add	x10, x28, x11, lsl #3
   3b90c:	add	x9, x9, #0x10
   3b910:	add	x10, x10, #0x10
   3b914:	ldr	x11, [x10], #8
   3b918:	subs	x8, x8, #0x1
   3b91c:	str	x11, [x9], #8
   3b920:	b.ne	3b914 <__gmpn_toom52_mul@@Base+0x7cc>  // b.any
   3b924:	mov	x9, xzr
   3b928:	b	3b930 <__gmpn_toom52_mul@@Base+0x7e8>
   3b92c:	ldr	x14, [sp, #8]
   3b930:	mov	x28, x27
   3b934:	mov	x27, x21
   3b938:	str	x9, [x17, x24, lsl #3]
   3b93c:	eor	w20, w16, #0x2
   3b940:	b	3bab8 <__gmpn_toom52_mul@@Base+0x970>
   3b944:	mov	x8, xzr
   3b948:	cmp	x17, x25
   3b94c:	b.eq	3b9ec <__gmpn_toom52_mul@@Base+0x8a4>  // b.none
   3b950:	cmp	x8, x26
   3b954:	b.gt	3b9ec <__gmpn_toom52_mul@@Base+0x8a4>
   3b958:	sub	x9, x26, x8
   3b95c:	add	x9, x9, #0x1
   3b960:	cmp	x9, #0x4
   3b964:	b.cc	3b9a4 <__gmpn_toom52_mul@@Base+0x85c>  // b.lo, b.ul, b.last
   3b968:	add	x10, x8, x26
   3b96c:	add	x13, x27, x26
   3b970:	add	x12, x21, x10, lsl #3
   3b974:	add	x10, x28, x13, lsl #3
   3b978:	add	x11, x27, x8
   3b97c:	add	x13, x12, #0x10
   3b980:	add	x10, x10, #0x18
   3b984:	cmp	x13, x10
   3b988:	add	x11, x28, x11, lsl #3
   3b98c:	b.cs	3bc00 <__gmpn_toom52_mul@@Base+0xab8>  // b.hs, b.nlast
   3b990:	add	x10, x21, x26, lsl #4
   3b994:	add	x10, x10, #0x18
   3b998:	add	x13, x11, #0x10
   3b99c:	cmp	x13, x10
   3b9a0:	b.cs	3bc00 <__gmpn_toom52_mul@@Base+0xab8>  // b.hs, b.nlast
   3b9a4:	ldr	x14, [sp, #8]
   3b9a8:	sub	x9, x26, x8
   3b9ac:	add	x10, x8, x26
   3b9b0:	mov	x12, x28
   3b9b4:	add	x11, x27, x8
   3b9b8:	add	x8, x9, #0x1
   3b9bc:	add	x9, x21, x10, lsl #3
   3b9c0:	add	x10, x12, x11, lsl #3
   3b9c4:	mov	w20, w16
   3b9c8:	mov	x28, x27
   3b9cc:	mov	x27, x21
   3b9d0:	add	x9, x9, #0x10
   3b9d4:	add	x10, x10, #0x10
   3b9d8:	ldr	x11, [x10], #8
   3b9dc:	subs	x8, x8, #0x1
   3b9e0:	str	x11, [x9], #8
   3b9e4:	b.ne	3b9d8 <__gmpn_toom52_mul@@Base+0x890>  // b.any
   3b9e8:	b	3bab8 <__gmpn_toom52_mul@@Base+0x970>
   3b9ec:	ldr	x14, [sp, #8]
   3b9f0:	mov	x28, x27
   3b9f4:	mov	x27, x21
   3b9f8:	mov	w20, w16
   3b9fc:	b	3bab8 <__gmpn_toom52_mul@@Base+0x970>
   3ba00:	mov	x0, x17
   3ba04:	mov	x1, x23
   3ba08:	mov	x2, x25
   3ba0c:	mov	x3, x24
   3ba10:	mov	x28, x27
   3ba14:	mov	x27, x21
   3ba18:	mov	w20, w16
   3ba1c:	bl	c2d0 <__gmpn_sub_n@plt>
   3ba20:	ldr	x14, [sp, #8]
   3ba24:	eor	w20, w20, #0x2
   3ba28:	b	3bab8 <__gmpn_toom52_mul@@Base+0x970>
   3ba2c:	and	x10, x9, #0xfffffffffffffffc
   3ba30:	add	x11, x11, #0x10
   3ba34:	add	x8, x8, x10
   3ba38:	add	x12, x12, #0x30
   3ba3c:	mov	x13, x10
   3ba40:	mov	w16, w25
   3ba44:	ldp	q0, q1, [x11, #-16]
   3ba48:	subs	x13, x13, #0x4
   3ba4c:	add	x11, x11, #0x20
   3ba50:	stp	q0, q1, [x12, #-16]
   3ba54:	add	x12, x12, #0x20
   3ba58:	b.ne	3ba44 <__gmpn_toom52_mul@@Base+0x8fc>  // b.any
   3ba5c:	cmp	x9, x10
   3ba60:	b.eq	3b55c <__gmpn_toom52_mul@@Base+0x414>  // b.none
   3ba64:	b	3b4b0 <__gmpn_toom52_mul@@Base+0x368>
   3ba68:	mov	x0, x17
   3ba6c:	mov	x1, x23
   3ba70:	mov	x2, x25
   3ba74:	mov	x28, x27
   3ba78:	mov	x27, x21
   3ba7c:	mov	x21, x17
   3ba80:	mov	x20, x3
   3ba84:	mov	x22, x13
   3ba88:	bl	c2d0 <__gmpn_sub_n@plt>
   3ba8c:	cbz	x22, 3bab0 <__gmpn_toom52_mul@@Base+0x968>
   3ba90:	ldur	x9, [x29, #-32]
   3ba94:	lsl	x8, x26, #1
   3ba98:	add	x0, x21, x20, lsl #3
   3ba9c:	mov	w1, wzr
   3baa0:	sub	x8, x8, x9
   3baa4:	lsl	x8, x8, #3
   3baa8:	add	x2, x8, #0x10
   3baac:	bl	c5f0 <memset@plt>
   3bab0:	ldr	x14, [sp, #8]
   3bab4:	eor	w20, w19, #0x2
   3bab8:	lsl	x19, x14, #3
   3babc:	mov	x21, x27
   3bac0:	add	x8, x27, x19
   3bac4:	ldp	x22, x25, [sp, #56]
   3bac8:	mov	x4, x24
   3bacc:	mov	x23, x24
   3bad0:	ldur	x24, [x29, #-40]
   3bad4:	ldur	x27, [x29, #-16]
   3bad8:	add	x26, x8, #0x20
   3badc:	mov	w2, #0x4                   	// #4
   3bae0:	mov	x0, x26
   3bae4:	mov	x1, x25
   3bae8:	mov	x3, x22
   3baec:	mov	x5, x24
   3baf0:	mov	x6, x27
   3baf4:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3baf8:	and	w8, w0, #0x1
   3bafc:	eor	w8, w8, w20
   3bb00:	stur	w8, [x29, #-32]
   3bb04:	ldr	x20, [sp, #32]
   3bb08:	add	x28, x27, x28, lsl #3
   3bb0c:	add	x3, x28, #0x10
   3bb10:	mov	x0, x27
   3bb14:	mov	x1, x25
   3bb18:	mov	x2, x20
   3bb1c:	mov	x4, x23
   3bb20:	bl	ccd0 <__gmpn_mul@plt>
   3bb24:	ldr	x1, [sp, #72]
   3bb28:	ldr	x2, [sp, #16]
   3bb2c:	add	x28, x28, #0x8
   3bb30:	mov	x0, x28
   3bb34:	mov	x3, x20
   3bb38:	bl	c990 <__gmpn_mul_n@plt>
   3bb3c:	ldp	x8, x1, [x29, #-64]
   3bb40:	ldr	x2, [sp, #24]
   3bb44:	mov	x3, x20
   3bb48:	add	x27, x8, #0x10
   3bb4c:	mov	x0, x27
   3bb50:	bl	c990 <__gmpn_mul_n@plt>
   3bb54:	ldr	x0, [sp, #40]
   3bb58:	mov	x1, x26
   3bb5c:	mov	x2, x21
   3bb60:	mov	x3, x20
   3bb64:	bl	c990 <__gmpn_mul_n@plt>
   3bb68:	ldur	x9, [x29, #-8]
   3bb6c:	mov	w8, #0x28                  	// #40
   3bb70:	madd	x0, x23, x8, x21
   3bb74:	add	x3, x22, x19
   3bb78:	cmp	x24, x9
   3bb7c:	b.le	3bb98 <__gmpn_toom52_mul@@Base+0xa50>
   3bb80:	mov	x1, x3
   3bb84:	ldur	x3, [x29, #-48]
   3bb88:	mov	x2, x24
   3bb8c:	mov	x19, x9
   3bb90:	mov	x4, x9
   3bb94:	b	3bba8 <__gmpn_toom52_mul@@Base+0xa60>
   3bb98:	ldur	x1, [x29, #-48]
   3bb9c:	mov	x2, x9
   3bba0:	mov	x19, x9
   3bba4:	mov	x4, x24
   3bba8:	bl	ccd0 <__gmpn_mul@plt>
   3bbac:	ldur	x20, [x29, #-24]
   3bbb0:	ldr	x2, [sp, #48]
   3bbb4:	mov	x0, x21
   3bbb8:	mov	x1, x22
   3bbbc:	mov	x3, x20
   3bbc0:	bl	c990 <__gmpn_mul_n@plt>
   3bbc4:	add	x6, x24, x19
   3bbc8:	mov	x0, x21
   3bbcc:	mov	x1, x20
   3bbd0:	ldur	w2, [x29, #-32]
   3bbd4:	ldur	x3, [x29, #-16]
   3bbd8:	mov	x4, x28
   3bbdc:	mov	x5, x27
   3bbe0:	ldp	x20, x19, [sp, #224]
   3bbe4:	ldp	x22, x21, [sp, #208]
   3bbe8:	ldp	x24, x23, [sp, #192]
   3bbec:	ldp	x26, x25, [sp, #176]
   3bbf0:	ldp	x28, x27, [sp, #160]
   3bbf4:	ldp	x29, x30, [sp, #144]
   3bbf8:	add	sp, sp, #0xf0
   3bbfc:	b	c910 <__gmpn_toom_interpolate_6pts@plt>
   3bc00:	and	x10, x9, #0xfffffffffffffffc
   3bc04:	add	x11, x11, #0x20
   3bc08:	add	x8, x8, x10
   3bc0c:	add	x12, x12, #0x20
   3bc10:	mov	x13, x10
   3bc14:	ldp	q0, q1, [x11, #-16]
   3bc18:	subs	x13, x13, #0x4
   3bc1c:	add	x11, x11, #0x20
   3bc20:	stp	q0, q1, [x12, #-16]
   3bc24:	add	x12, x12, #0x20
   3bc28:	b.ne	3bc14 <__gmpn_toom52_mul@@Base+0xacc>  // b.any
   3bc2c:	ldr	x14, [sp, #8]
   3bc30:	cmp	x9, x10
   3bc34:	b.ne	3b9a8 <__gmpn_toom52_mul@@Base+0x860>  // b.any
   3bc38:	b	3b9f0 <__gmpn_toom52_mul@@Base+0x8a8>

000000000003bc3c <__gmpn_toom62_mul@@Base>:
   3bc3c:	stp	x29, x30, [sp, #-96]!
   3bc40:	stp	x28, x27, [sp, #16]
   3bc44:	stp	x26, x25, [sp, #32]
   3bc48:	stp	x24, x23, [sp, #48]
   3bc4c:	stp	x22, x21, [sp, #64]
   3bc50:	stp	x20, x19, [sp, #80]
   3bc54:	mov	x29, sp
   3bc58:	sub	sp, sp, #0xd0
   3bc5c:	add	x8, x4, x4, lsl #1
   3bc60:	mov	x25, x3
   3bc64:	mov	x23, x1
   3bc68:	cmp	x8, x2
   3bc6c:	mov	x20, x0
   3bc70:	stur	x5, [x29, #-40]
   3bc74:	b.le	3bc84 <__gmpn_toom62_mul@@Base+0x48>
   3bc78:	sub	x8, x4, #0x1
   3bc7c:	asr	x24, x8, #1
   3bc80:	b	3bc98 <__gmpn_toom62_mul@@Base+0x5c>
   3bc84:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3bc88:	sub	x8, x2, #0x1
   3bc8c:	movk	x9, #0xaaab
   3bc90:	umulh	x8, x8, x9
   3bc94:	lsr	x24, x8, #2
   3bc98:	add	x21, x24, #0x1
   3bc9c:	add	x9, x24, #0x2
   3bca0:	lsl	x10, x21, #2
   3bca4:	sub	x8, x4, x21
   3bca8:	stp	x9, x8, [x29, #-56]
   3bcac:	add	x8, x10, x21
   3bcb0:	stur	x8, [x29, #-64]
   3bcb4:	sub	x19, x2, x8
   3bcb8:	lsl	x8, x9, #3
   3bcbc:	add	x8, x8, #0xf
   3bcc0:	mov	x9, sp
   3bcc4:	and	x8, x8, #0xfffffffffffffff0
   3bcc8:	sub	x0, x9, x8
   3bccc:	stur	x4, [x29, #-104]
   3bcd0:	stur	x10, [x29, #-200]
   3bcd4:	stur	x2, [x29, #-24]
   3bcd8:	mov	sp, x0
   3bcdc:	mov	x9, sp
   3bce0:	sub	x1, x9, x8
   3bce4:	mov	sp, x1
   3bce8:	mov	x9, sp
   3bcec:	sub	x22, x9, x8
   3bcf0:	mov	sp, x22
   3bcf4:	mov	x9, sp
   3bcf8:	sub	x27, x9, x8
   3bcfc:	mov	sp, x27
   3bd00:	mov	x9, sp
   3bd04:	sub	x28, x9, x8
   3bd08:	mov	sp, x28
   3bd0c:	mov	x9, sp
   3bd10:	sub	x9, x9, x8
   3bd14:	stur	x9, [x29, #-152]
   3bd18:	mov	sp, x9
   3bd1c:	lsl	x26, x21, #3
   3bd20:	add	x10, x26, #0xf
   3bd24:	mov	x9, sp
   3bd28:	and	x10, x10, #0xfffffffffffffff0
   3bd2c:	sub	x9, x9, x10
   3bd30:	stp	x9, x1, [x29, #-136]
   3bd34:	mov	sp, x9
   3bd38:	mov	x9, sp
   3bd3c:	sub	x9, x9, x8
   3bd40:	stur	x9, [x29, #-32]
   3bd44:	mov	sp, x9
   3bd48:	mov	x9, sp
   3bd4c:	sub	x9, x9, x8
   3bd50:	stur	x9, [x29, #-184]
   3bd54:	mov	sp, x9
   3bd58:	mov	x9, sp
   3bd5c:	sub	x8, x9, x8
   3bd60:	stur	x8, [x29, #-176]
   3bd64:	mov	sp, x8
   3bd68:	mov	w2, #0x5                   	// #5
   3bd6c:	mov	x3, x23
   3bd70:	mov	x4, x21
   3bd74:	mov	x5, x19
   3bd78:	mov	x6, x20
   3bd7c:	stur	x0, [x29, #-88]
   3bd80:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3bd84:	stur	w0, [x29, #-120]
   3bd88:	mov	w2, #0x5                   	// #5
   3bd8c:	mov	x0, x22
   3bd90:	mov	x1, x27
   3bd94:	mov	x3, x23
   3bd98:	mov	x4, x21
   3bd9c:	mov	x5, x19
   3bda0:	mov	x6, x20
   3bda4:	stur	x22, [x29, #-160]
   3bda8:	stur	x27, [x29, #-144]
   3bdac:	stur	x20, [x29, #-72]
   3bdb0:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3bdb4:	stur	w0, [x29, #-116]
   3bdb8:	add	x1, x23, x26
   3bdbc:	mov	x0, x28
   3bdc0:	mov	x2, x23
   3bdc4:	mov	x3, x21
   3bdc8:	stur	x26, [x29, #-80]
   3bdcc:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3bdd0:	stur	x23, [x29, #-16]
   3bdd4:	ldur	x8, [x29, #-16]
   3bdd8:	mov	x23, x0
   3bddc:	mov	x0, x28
   3bde0:	mov	x2, x28
   3bde4:	add	x1, x8, x21, lsl #4
   3bde8:	mov	x3, x21
   3bdec:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3bdf0:	add	x20, x0, x23, lsl #1
   3bdf4:	ldur	x23, [x29, #-16]
   3bdf8:	mov	w8, #0x18                  	// #24
   3bdfc:	mov	x0, x28
   3be00:	mov	x2, x28
   3be04:	madd	x1, x21, x8, x23
   3be08:	mov	x3, x21
   3be0c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be10:	add	x20, x0, x20, lsl #1
   3be14:	add	x1, x23, x21, lsl #5
   3be18:	mov	x0, x28
   3be1c:	mov	x2, x28
   3be20:	mov	x3, x21
   3be24:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be28:	cmp	x24, x19
   3be2c:	add	x26, x0, x20, lsl #1
   3be30:	stur	x19, [x29, #-112]
   3be34:	b.ge	3be5c <__gmpn_toom62_mul@@Base+0x220>  // b.tcont
   3be38:	ldur	x8, [x29, #-64]
   3be3c:	mov	x0, x28
   3be40:	mov	x2, x28
   3be44:	mov	x3, x21
   3be48:	add	x1, x23, x8, lsl #3
   3be4c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be50:	add	x8, x0, x26, lsl #1
   3be54:	str	x8, [x28, x21, lsl #3]
   3be58:	b	3bed0 <__gmpn_toom62_mul@@Base+0x294>
   3be5c:	ldur	x8, [x29, #-64]
   3be60:	mov	x0, x28
   3be64:	mov	x2, x28
   3be68:	mov	x3, x19
   3be6c:	add	x1, x23, x8, lsl #3
   3be70:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3be74:	add	x20, x28, x19, lsl #3
   3be78:	mov	x23, x0
   3be7c:	sub	x2, x21, x19
   3be80:	mov	w3, #0x1                   	// #1
   3be84:	mov	x0, x20
   3be88:	mov	x1, x20
   3be8c:	bl	c180 <__gmpn_lshift@plt>
   3be90:	add	x8, x0, x26, lsl #1
   3be94:	str	x8, [x28, x21, lsl #3]
   3be98:	ldr	x8, [x20]
   3be9c:	adds	x8, x8, x23
   3bea0:	str	x8, [x20]
   3bea4:	b.cc	3bed0 <__gmpn_toom62_mul@@Base+0x294>  // b.lo, b.ul, b.last
   3bea8:	ldur	x8, [x29, #-24]
   3beac:	mov	w9, #0x28                  	// #40
   3beb0:	lsl	x8, x8, #3
   3beb4:	msub	x8, x24, x9, x8
   3beb8:	add	x8, x8, x28
   3bebc:	sub	x8, x8, #0x20
   3bec0:	ldr	x9, [x8]
   3bec4:	adds	x9, x9, #0x1
   3bec8:	str	x9, [x8], #8
   3becc:	b.cs	3bec0 <__gmpn_toom62_mul@@Base+0x284>  // b.hs, b.nlast
   3bed0:	ldp	x19, x23, [x29, #-56]
   3bed4:	stur	x28, [x29, #-168]
   3bed8:	ldur	x28, [x29, #-136]
   3bedc:	ldur	x27, [x29, #-152]
   3bee0:	ldur	x26, [x29, #-184]
   3bee4:	subs	x20, x21, x23
   3bee8:	add	x22, x25, x21, lsl #3
   3beec:	stur	x22, [x29, #-96]
   3bef0:	b.ne	3bf68 <__gmpn_toom62_mul@@Base+0x32c>  // b.any
   3bef4:	mov	x0, x27
   3bef8:	mov	x1, x25
   3befc:	mov	x2, x22
   3bf00:	mov	x3, x21
   3bf04:	bl	ca70 <__gmpn_add_n@plt>
   3bf08:	mov	w8, #0x8                   	// #8
   3bf0c:	bfi	x8, x24, #4, #60
   3bf10:	mov	x9, x24
   3bf14:	str	x0, [x27, x21, lsl #3]
   3bf18:	add	x10, x9, #0x1
   3bf1c:	cmp	x10, #0x1
   3bf20:	b.lt	3bf40 <__gmpn_toom62_mul@@Base+0x304>  // b.tstop
   3bf24:	ldr	x10, [x25, x9, lsl #3]
   3bf28:	ldr	x11, [x25, x8]
   3bf2c:	sub	x9, x9, #0x1
   3bf30:	sub	x8, x8, #0x8
   3bf34:	cmp	x10, x11
   3bf38:	b.eq	3bf18 <__gmpn_toom62_mul@@Base+0x2dc>  // b.none
   3bf3c:	b.ls	3c0e4 <__gmpn_toom62_mul@@Base+0x4a8>  // b.plast
   3bf40:	mov	x0, x28
   3bf44:	mov	x1, x25
   3bf48:	mov	x2, x22
   3bf4c:	mov	x3, x21
   3bf50:	bl	c2d0 <__gmpn_sub_n@plt>
   3bf54:	ldur	x12, [x29, #-32]
   3bf58:	mov	w20, wzr
   3bf5c:	cbnz	x23, 3c104 <__gmpn_toom62_mul@@Base+0x4c8>
   3bf60:	mov	x8, xzr
   3bf64:	b	3c148 <__gmpn_toom62_mul@@Base+0x50c>
   3bf68:	cbz	x23, 3bfb4 <__gmpn_toom62_mul@@Base+0x378>
   3bf6c:	mov	x0, x27
   3bf70:	mov	x1, x25
   3bf74:	mov	x2, x22
   3bf78:	mov	x3, x23
   3bf7c:	bl	ca70 <__gmpn_add_n@plt>
   3bf80:	mov	x8, x23
   3bf84:	cbz	x0, 3bfb8 <__gmpn_toom62_mul@@Base+0x37c>
   3bf88:	mov	w9, #0x1                   	// #1
   3bf8c:	mov	x8, x23
   3bf90:	cmp	x8, x24
   3bf94:	b.gt	3bfe8 <__gmpn_toom62_mul@@Base+0x3ac>
   3bf98:	lsl	x10, x8, #3
   3bf9c:	ldr	x11, [x25, x10]
   3bfa0:	add	x8, x8, #0x1
   3bfa4:	adds	x11, x11, #0x1
   3bfa8:	str	x11, [x27, x10]
   3bfac:	b.cs	3bf90 <__gmpn_toom62_mul@@Base+0x354>  // b.hs, b.nlast
   3bfb0:	b	3bfb8 <__gmpn_toom62_mul@@Base+0x37c>
   3bfb4:	mov	x8, xzr
   3bfb8:	cmp	x27, x25
   3bfbc:	mov	x9, xzr
   3bfc0:	b.eq	3bfe8 <__gmpn_toom62_mul@@Base+0x3ac>  // b.none
   3bfc4:	cmp	x8, x24
   3bfc8:	b.gt	3bfe8 <__gmpn_toom62_mul@@Base+0x3ac>
   3bfcc:	lsl	x9, x8, #3
   3bfd0:	sub	x8, x21, x8
   3bfd4:	add	x0, x27, x9
   3bfd8:	add	x1, x25, x9
   3bfdc:	lsl	x2, x8, #3
   3bfe0:	bl	bed0 <memcpy@plt>
   3bfe4:	mov	x9, xzr
   3bfe8:	ldur	x8, [x29, #-104]
   3bfec:	str	x9, [x27, x21, lsl #3]
   3bff0:	lsl	x9, x24, #3
   3bff4:	sub	x8, x8, x24, lsl #1
   3bff8:	sub	x8, x8, #0x2
   3bffc:	ldr	x10, [x25, x9]
   3c000:	cbnz	x10, 3c04c <__gmpn_toom62_mul@@Base+0x410>
   3c004:	adds	x8, x8, #0x1
   3c008:	sub	x9, x9, #0x8
   3c00c:	b.cc	3bffc <__gmpn_toom62_mul@@Base+0x3c0>  // b.lo, b.ul, b.last
   3c010:	ldur	x9, [x29, #-104]
   3c014:	sub	x8, x9, x24
   3c018:	lsl	x22, x9, #3
   3c01c:	sub	x8, x8, #0x2
   3c020:	sub	x9, x22, #0x8
   3c024:	add	x10, x8, #0x1
   3c028:	cmp	x10, #0x1
   3c02c:	b.lt	3c04c <__gmpn_toom62_mul@@Base+0x410>  // b.tstop
   3c030:	ldr	x10, [x25, x8, lsl #3]
   3c034:	ldr	x11, [x25, x9]
   3c038:	sub	x8, x8, #0x1
   3c03c:	sub	x9, x9, #0x8
   3c040:	cmp	x10, x11
   3c044:	b.eq	3c024 <__gmpn_toom62_mul@@Base+0x3e8>  // b.none
   3c048:	b.ls	3c328 <__gmpn_toom62_mul@@Base+0x6ec>  // b.plast
   3c04c:	cbz	x23, 3c09c <__gmpn_toom62_mul@@Base+0x460>
   3c050:	ldur	x22, [x29, #-96]
   3c054:	mov	x0, x28
   3c058:	mov	x1, x25
   3c05c:	mov	x3, x23
   3c060:	mov	x2, x22
   3c064:	bl	c2d0 <__gmpn_sub_n@plt>
   3c068:	ldur	x12, [x29, #-32]
   3c06c:	mov	x8, x23
   3c070:	cbz	x0, 3c0a8 <__gmpn_toom62_mul@@Base+0x46c>
   3c074:	mov	x8, x23
   3c078:	cmp	x8, x24
   3c07c:	b.gt	3c0d8 <__gmpn_toom62_mul@@Base+0x49c>
   3c080:	lsl	x9, x8, #3
   3c084:	ldr	x10, [x25, x9]
   3c088:	add	x8, x8, #0x1
   3c08c:	sub	x11, x10, #0x1
   3c090:	str	x11, [x28, x9]
   3c094:	cbz	x10, 3c078 <__gmpn_toom62_mul@@Base+0x43c>
   3c098:	b	3c0a8 <__gmpn_toom62_mul@@Base+0x46c>
   3c09c:	ldur	x22, [x29, #-96]
   3c0a0:	ldur	x12, [x29, #-32]
   3c0a4:	mov	x8, xzr
   3c0a8:	cmp	x28, x25
   3c0ac:	mov	w20, wzr
   3c0b0:	b.eq	3c100 <__gmpn_toom62_mul@@Base+0x4c4>  // b.none
   3c0b4:	cmp	x8, x24
   3c0b8:	b.gt	3c100 <__gmpn_toom62_mul@@Base+0x4c4>
   3c0bc:	lsl	x9, x8, #3
   3c0c0:	sub	x8, x21, x8
   3c0c4:	add	x0, x28, x9
   3c0c8:	add	x1, x25, x9
   3c0cc:	lsl	x2, x8, #3
   3c0d0:	bl	bed0 <memcpy@plt>
   3c0d4:	ldur	x12, [x29, #-32]
   3c0d8:	mov	w20, wzr
   3c0dc:	cbnz	x23, 3c104 <__gmpn_toom62_mul@@Base+0x4c8>
   3c0e0:	b	3bf60 <__gmpn_toom62_mul@@Base+0x324>
   3c0e4:	mov	x0, x28
   3c0e8:	mov	x1, x22
   3c0ec:	mov	x2, x25
   3c0f0:	mov	x3, x21
   3c0f4:	bl	c2d0 <__gmpn_sub_n@plt>
   3c0f8:	ldur	x12, [x29, #-32]
   3c0fc:	mov	w20, #0x2                   	// #2
   3c100:	cbz	x23, 3bf60 <__gmpn_toom62_mul@@Base+0x324>
   3c104:	mov	x0, x12
   3c108:	mov	x1, x27
   3c10c:	mov	x2, x22
   3c110:	mov	x3, x23
   3c114:	bl	ca70 <__gmpn_add_n@plt>
   3c118:	ldur	x12, [x29, #-32]
   3c11c:	mov	x8, x23
   3c120:	cbz	x0, 3c148 <__gmpn_toom62_mul@@Base+0x50c>
   3c124:	mov	x8, x23
   3c128:	cmp	x8, x19
   3c12c:	b.ge	3c164 <__gmpn_toom62_mul@@Base+0x528>  // b.tcont
   3c130:	lsl	x9, x8, #3
   3c134:	ldr	x10, [x27, x9]
   3c138:	add	x8, x8, #0x1
   3c13c:	adds	x10, x10, #0x1
   3c140:	str	x10, [x12, x9]
   3c144:	b.cs	3c128 <__gmpn_toom62_mul@@Base+0x4ec>  // b.hs, b.nlast
   3c148:	subs	x9, x19, x8
   3c14c:	b.le	3c164 <__gmpn_toom62_mul@@Base+0x528>
   3c150:	lsl	x8, x8, #3
   3c154:	add	x0, x12, x8
   3c158:	add	x1, x27, x8
   3c15c:	lsl	x2, x9, #3
   3c160:	bl	bed0 <memcpy@plt>
   3c164:	lsl	x8, x21, #1
   3c168:	stur	x8, [x29, #-24]
   3c16c:	cbz	w20, 3c1bc <__gmpn_toom62_mul@@Base+0x580>
   3c170:	cbz	x23, 3c210 <__gmpn_toom62_mul@@Base+0x5d4>
   3c174:	mov	x0, x26
   3c178:	mov	x1, x28
   3c17c:	mov	x2, x22
   3c180:	mov	x3, x23
   3c184:	bl	ca70 <__gmpn_add_n@plt>
   3c188:	mov	x8, x23
   3c18c:	cbz	x0, 3c214 <__gmpn_toom62_mul@@Base+0x5d8>
   3c190:	mov	w9, #0x1                   	// #1
   3c194:	mov	x8, x23
   3c198:	cmp	x8, x24
   3c19c:	b.gt	3c238 <__gmpn_toom62_mul@@Base+0x5fc>
   3c1a0:	lsl	x10, x8, #3
   3c1a4:	ldr	x11, [x28, x10]
   3c1a8:	add	x8, x8, #0x1
   3c1ac:	adds	x11, x11, #0x1
   3c1b0:	str	x11, [x26, x10]
   3c1b4:	b.cs	3c198 <__gmpn_toom62_mul@@Base+0x55c>  // b.hs, b.nlast
   3c1b8:	b	3c214 <__gmpn_toom62_mul@@Base+0x5d8>
   3c1bc:	cmp	x24, x23
   3c1c0:	b.ge	3c250 <__gmpn_toom62_mul@@Base+0x614>  // b.tcont
   3c1c4:	ldur	x19, [x29, #-200]
   3c1c8:	add	x8, x25, x24, lsl #4
   3c1cc:	add	x8, x8, #0x8
   3c1d0:	add	x9, x24, #0x1
   3c1d4:	cmp	x9, #0x1
   3c1d8:	b.lt	3c1f4 <__gmpn_toom62_mul@@Base+0x5b8>  // b.tstop
   3c1dc:	ldr	x9, [x28, x24, lsl #3]
   3c1e0:	ldr	x10, [x8], #-8
   3c1e4:	sub	x24, x24, #0x1
   3c1e8:	cmp	x9, x10
   3c1ec:	b.eq	3c1d0 <__gmpn_toom62_mul@@Base+0x594>  // b.none
   3c1f0:	b.ls	3c37c <__gmpn_toom62_mul@@Base+0x740>  // b.plast
   3c1f4:	mov	x0, x26
   3c1f8:	mov	x1, x28
   3c1fc:	mov	x2, x22
   3c200:	mov	x3, x21
   3c204:	bl	c2d0 <__gmpn_sub_n@plt>
   3c208:	stur	wzr, [x29, #-188]
   3c20c:	b	3c398 <__gmpn_toom62_mul@@Base+0x75c>
   3c210:	mov	x8, xzr
   3c214:	cmp	x8, x24
   3c218:	b.gt	3c234 <__gmpn_toom62_mul@@Base+0x5f8>
   3c21c:	lsl	x9, x8, #3
   3c220:	sub	x8, x21, x8
   3c224:	add	x0, x26, x9
   3c228:	add	x1, x28, x9
   3c22c:	lsl	x2, x8, #3
   3c230:	bl	bed0 <memcpy@plt>
   3c234:	mov	x9, xzr
   3c238:	ldur	x24, [x29, #-80]
   3c23c:	ldur	x19, [x29, #-200]
   3c240:	orr	w8, w20, #0x1
   3c244:	str	x9, [x26, x21, lsl #3]
   3c248:	stur	w8, [x29, #-188]
   3c24c:	b	3c3a0 <__gmpn_toom62_mul@@Base+0x764>
   3c250:	ldur	x8, [x29, #-104]
   3c254:	add	x9, x28, x24, lsl #3
   3c258:	sub	x8, x8, x24, lsl #1
   3c25c:	sub	x8, x8, #0x2
   3c260:	ldr	x10, [x9]
   3c264:	cbnz	x10, 3c2ac <__gmpn_toom62_mul@@Base+0x670>
   3c268:	adds	x8, x8, #0x1
   3c26c:	sub	x9, x9, #0x8
   3c270:	b.cc	3c260 <__gmpn_toom62_mul@@Base+0x624>  // b.lo, b.ul, b.last
   3c274:	ldur	x8, [x29, #-104]
   3c278:	sub	x9, x28, #0x10
   3c27c:	sub	x10, x8, x24
   3c280:	add	x8, x25, x8, lsl #3
   3c284:	sub	x8, x8, #0x8
   3c288:	sub	x11, x10, #0x1
   3c28c:	cmp	x11, #0x1
   3c290:	b.lt	3c2ac <__gmpn_toom62_mul@@Base+0x670>  // b.tstop
   3c294:	ldr	x10, [x9, x10, lsl #3]
   3c298:	ldr	x12, [x8], #-8
   3c29c:	cmp	x10, x12
   3c2a0:	mov	x10, x11
   3c2a4:	b.eq	3c288 <__gmpn_toom62_mul@@Base+0x64c>  // b.none
   3c2a8:	b.ls	3c62c <__gmpn_toom62_mul@@Base+0x9f0>  // b.plast
   3c2ac:	cbz	x23, 3c2f4 <__gmpn_toom62_mul@@Base+0x6b8>
   3c2b0:	mov	x0, x26
   3c2b4:	mov	x1, x28
   3c2b8:	mov	x2, x22
   3c2bc:	mov	x3, x23
   3c2c0:	bl	c2d0 <__gmpn_sub_n@plt>
   3c2c4:	mov	x8, x23
   3c2c8:	cbz	x0, 3c2f8 <__gmpn_toom62_mul@@Base+0x6bc>
   3c2cc:	mov	x8, x23
   3c2d0:	cmp	x8, x24
   3c2d4:	b.gt	3c318 <__gmpn_toom62_mul@@Base+0x6dc>
   3c2d8:	lsl	x9, x8, #3
   3c2dc:	ldr	x10, [x28, x9]
   3c2e0:	add	x8, x8, #0x1
   3c2e4:	sub	x11, x10, #0x1
   3c2e8:	str	x11, [x26, x9]
   3c2ec:	cbz	x10, 3c2d0 <__gmpn_toom62_mul@@Base+0x694>
   3c2f0:	b	3c2f8 <__gmpn_toom62_mul@@Base+0x6bc>
   3c2f4:	mov	x8, xzr
   3c2f8:	cmp	x8, x24
   3c2fc:	b.gt	3c318 <__gmpn_toom62_mul@@Base+0x6dc>
   3c300:	lsl	x9, x8, #3
   3c304:	sub	x8, x21, x8
   3c308:	add	x0, x26, x9
   3c30c:	add	x1, x28, x9
   3c310:	lsl	x2, x8, #3
   3c314:	bl	bed0 <memcpy@plt>
   3c318:	ldur	x24, [x29, #-80]
   3c31c:	ldur	x19, [x29, #-200]
   3c320:	stur	wzr, [x29, #-188]
   3c324:	b	3c39c <__gmpn_toom62_mul@@Base+0x760>
   3c328:	ldur	x1, [x29, #-96]
   3c32c:	mov	x0, x28
   3c330:	mov	x2, x25
   3c334:	mov	x3, x23
   3c338:	bl	c2d0 <__gmpn_sub_n@plt>
   3c33c:	cbz	x20, 3c368 <__gmpn_toom62_mul@@Base+0x72c>
   3c340:	ldur	x10, [x29, #-104]
   3c344:	sub	x8, x22, x24, lsl #3
   3c348:	lsl	x9, x24, #1
   3c34c:	add	x8, x8, x28
   3c350:	sub	x9, x9, x10
   3c354:	sub	x0, x8, #0x8
   3c358:	lsl	x8, x9, #3
   3c35c:	add	x2, x8, #0x10
   3c360:	mov	w1, wzr
   3c364:	bl	c5f0 <memset@plt>
   3c368:	ldur	x22, [x29, #-96]
   3c36c:	ldur	x12, [x29, #-32]
   3c370:	mov	w20, #0x2                   	// #2
   3c374:	cbnz	x23, 3c104 <__gmpn_toom62_mul@@Base+0x4c8>
   3c378:	b	3bf60 <__gmpn_toom62_mul@@Base+0x324>
   3c37c:	mov	x0, x26
   3c380:	mov	x1, x22
   3c384:	mov	x2, x28
   3c388:	mov	x3, x21
   3c38c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c390:	mov	w8, #0x1                   	// #1
   3c394:	stur	w8, [x29, #-188]
   3c398:	ldur	x24, [x29, #-80]
   3c39c:	str	xzr, [x26, x21, lsl #3]
   3c3a0:	mov	x23, x26
   3c3a4:	ldur	x26, [x29, #-176]
   3c3a8:	ldr	x20, [x27, x24]
   3c3ac:	mov	x1, x27
   3c3b0:	mov	x2, x25
   3c3b4:	mov	x0, x26
   3c3b8:	mov	x3, x21
   3c3bc:	bl	ca70 <__gmpn_add_n@plt>
   3c3c0:	add	x8, x0, x20
   3c3c4:	ldp	x22, x2, [x29, #-40]
   3c3c8:	ldur	x20, [x29, #-56]
   3c3cc:	ldur	x1, [x29, #-160]
   3c3d0:	str	x8, [x26, x24]
   3c3d4:	mov	x0, x22
   3c3d8:	mov	x3, x20
   3c3dc:	bl	c990 <__gmpn_mul_n@plt>
   3c3e0:	ldur	x8, [x29, #-24]
   3c3e4:	ldur	x1, [x29, #-144]
   3c3e8:	mov	x2, x23
   3c3ec:	mov	x3, x20
   3c3f0:	add	x8, x22, x8, lsl #3
   3c3f4:	add	x0, x8, #0x8
   3c3f8:	stur	x0, [x29, #-32]
   3c3fc:	bl	c990 <__gmpn_mul_n@plt>
   3c400:	ldur	x1, [x29, #-168]
   3c404:	add	x8, x22, x19, lsl #3
   3c408:	add	x0, x8, #0x10
   3c40c:	mov	x2, x26
   3c410:	mov	x3, x20
   3c414:	mov	x23, x0
   3c418:	bl	c990 <__gmpn_mul_n@plt>
   3c41c:	add	x20, x21, x21, lsl #1
   3c420:	add	x8, x22, x20, lsl #4
   3c424:	ldur	x22, [x29, #-128]
   3c428:	add	x19, x8, #0x18
   3c42c:	mov	x0, x19
   3c430:	mov	x2, x28
   3c434:	mov	x1, x22
   3c438:	mov	x3, x21
   3c43c:	bl	c990 <__gmpn_mul_n@plt>
   3c440:	ldr	x8, [x22, x24]
   3c444:	stur	x23, [x29, #-104]
   3c448:	cmp	x8, #0x2
   3c44c:	b.eq	3c470 <__gmpn_toom62_mul@@Base+0x834>  // b.none
   3c450:	cmp	x8, #0x1
   3c454:	b.ne	3c488 <__gmpn_toom62_mul@@Base+0x84c>  // b.any
   3c458:	add	x0, x19, x21, lsl #3
   3c45c:	mov	x1, x0
   3c460:	mov	x2, x28
   3c464:	mov	x3, x21
   3c468:	bl	ca70 <__gmpn_add_n@plt>
   3c46c:	b	3c48c <__gmpn_toom62_mul@@Base+0x850>
   3c470:	add	x0, x19, x21, lsl #3
   3c474:	mov	x1, x0
   3c478:	mov	x2, x28
   3c47c:	mov	x3, x21
   3c480:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3c484:	b	3c48c <__gmpn_toom62_mul@@Base+0x850>
   3c488:	mov	x0, xzr
   3c48c:	ldur	x8, [x29, #-24]
   3c490:	ldp	x9, x28, [x29, #-72]
   3c494:	ldur	x26, [x29, #-88]
   3c498:	mov	x2, x27
   3c49c:	lsl	x8, x8, #3
   3c4a0:	add	x22, x9, x8
   3c4a4:	str	x0, [x19, x8]
   3c4a8:	mov	x0, x22
   3c4ac:	mov	x1, x26
   3c4b0:	mov	x3, x21
   3c4b4:	bl	c990 <__gmpn_mul_n@plt>
   3c4b8:	ldr	x26, [x26, x21, lsl #3]
   3c4bc:	cbz	x26, 3c540 <__gmpn_toom62_mul@@Base+0x904>
   3c4c0:	cmp	x26, #0x2
   3c4c4:	b.eq	3c4f0 <__gmpn_toom62_mul@@Base+0x8b4>  // b.none
   3c4c8:	cmp	x26, #0x1
   3c4cc:	b.ne	3c510 <__gmpn_toom62_mul@@Base+0x8d4>  // b.any
   3c4d0:	ldr	x26, [x27, x24]
   3c4d4:	add	x0, x22, x24
   3c4d8:	mov	x1, x0
   3c4dc:	mov	x2, x27
   3c4e0:	mov	x3, x21
   3c4e4:	bl	ca70 <__gmpn_add_n@plt>
   3c4e8:	add	x26, x0, x26
   3c4ec:	b	3c540 <__gmpn_toom62_mul@@Base+0x904>
   3c4f0:	ldr	x26, [x27, x24]
   3c4f4:	add	x0, x22, x24
   3c4f8:	mov	x1, x0
   3c4fc:	mov	x2, x27
   3c500:	mov	x3, x21
   3c504:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3c508:	add	x26, x0, x26, lsl #1
   3c50c:	b	3c540 <__gmpn_toom62_mul@@Base+0x904>
   3c510:	ldur	x8, [x29, #-80]
   3c514:	mov	x24, x28
   3c518:	mov	x1, x27
   3c51c:	mov	x2, x21
   3c520:	ldr	x28, [x27, x8]
   3c524:	ldur	x8, [x29, #-80]
   3c528:	mov	x3, x26
   3c52c:	add	x0, x22, x8
   3c530:	bl	d400 <__gmpn_addmul_1@plt>
   3c534:	madd	x26, x28, x26, x0
   3c538:	mov	x28, x24
   3c53c:	ldur	x24, [x29, #-80]
   3c540:	ldur	w9, [x29, #-120]
   3c544:	ldr	x8, [x27, x21, lsl #3]
   3c548:	lsl	x23, x20, #1
   3c54c:	and	w27, w9, #0x2
   3c550:	cbz	x8, 3c56c <__gmpn_toom62_mul@@Base+0x930>
   3c554:	ldur	x2, [x29, #-88]
   3c558:	add	x0, x22, x21, lsl #3
   3c55c:	mov	x1, x0
   3c560:	mov	x3, x21
   3c564:	bl	ca70 <__gmpn_add_n@plt>
   3c568:	add	x26, x0, x26
   3c56c:	ldur	w8, [x29, #-116]
   3c570:	mov	x2, x25
   3c574:	mov	x3, x21
   3c578:	bfxil	w27, w8, #0, #1
   3c57c:	ldur	x8, [x29, #-24]
   3c580:	str	x26, [x22, x8, lsl #3]
   3c584:	ldur	x22, [x29, #-72]
   3c588:	ldur	x26, [x29, #-16]
   3c58c:	mov	x0, x22
   3c590:	mov	x1, x26
   3c594:	bl	c990 <__gmpn_mul_n@plt>
   3c598:	add	x0, x22, x23, lsl #3
   3c59c:	ldur	x23, [x29, #-48]
   3c5a0:	ldur	x25, [x29, #-112]
   3c5a4:	add	x3, x26, x28, lsl #3
   3c5a8:	cmp	x25, x23
   3c5ac:	b.le	3c5c4 <__gmpn_toom62_mul@@Base+0x988>
   3c5b0:	mov	x1, x3
   3c5b4:	ldur	x3, [x29, #-96]
   3c5b8:	mov	x2, x25
   3c5bc:	mov	x4, x23
   3c5c0:	b	3c5d0 <__gmpn_toom62_mul@@Base+0x994>
   3c5c4:	ldur	x1, [x29, #-96]
   3c5c8:	mov	x2, x23
   3c5cc:	mov	x4, x25
   3c5d0:	bl	ccd0 <__gmpn_mul@plt>
   3c5d4:	ldur	w8, [x29, #-188]
   3c5d8:	ldur	x5, [x29, #-40]
   3c5dc:	add	x7, x25, x23
   3c5e0:	eor	w2, w8, w27
   3c5e4:	add	x8, x5, x24, lsl #3
   3c5e8:	add	x8, x8, #0x20
   3c5ec:	str	x8, [sp, #-16]!
   3c5f0:	ldur	x3, [x29, #-32]
   3c5f4:	ldur	x6, [x29, #-104]
   3c5f8:	mov	x0, x22
   3c5fc:	mov	x1, x21
   3c600:	mov	x4, x19
   3c604:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   3c608:	add	sp, sp, #0x10
   3c60c:	mov	sp, x29
   3c610:	ldp	x20, x19, [sp, #80]
   3c614:	ldp	x22, x21, [sp, #64]
   3c618:	ldp	x24, x23, [sp, #48]
   3c61c:	ldp	x26, x25, [sp, #32]
   3c620:	ldp	x28, x27, [sp, #16]
   3c624:	ldp	x29, x30, [sp], #96
   3c628:	ret
   3c62c:	mov	x0, x26
   3c630:	mov	x1, x22
   3c634:	mov	x2, x28
   3c638:	mov	x3, x23
   3c63c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c640:	cmp	x19, x23
   3c644:	ldur	x19, [x29, #-200]
   3c648:	b.eq	3c678 <__gmpn_toom62_mul@@Base+0xa3c>  // b.none
   3c64c:	ldur	x10, [x29, #-104]
   3c650:	lsl	x9, x24, #1
   3c654:	mov	w1, wzr
   3c658:	lsl	x8, x10, #3
   3c65c:	sub	x8, x8, x24, lsl #3
   3c660:	sub	x9, x9, x10
   3c664:	add	x8, x8, x26
   3c668:	lsl	x9, x9, #3
   3c66c:	sub	x0, x8, #0x8
   3c670:	add	x2, x9, #0x18
   3c674:	bl	c5f0 <memset@plt>
   3c678:	ldur	x24, [x29, #-80]
   3c67c:	mov	w8, #0x1                   	// #1
   3c680:	stur	w8, [x29, #-188]
   3c684:	b	3c3a0 <__gmpn_toom62_mul@@Base+0x764>

000000000003c688 <__gmpn_toom33_mul@@Base>:
   3c688:	sub	sp, sp, #0xe0
   3c68c:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3c690:	add	x8, x2, #0x2
   3c694:	movk	x9, #0xaaab
   3c698:	umulh	x8, x8, x9
   3c69c:	lsr	x14, x8, #1
   3c6a0:	add	x8, x5, x14, lsl #5
   3c6a4:	stp	x29, x30, [sp, #128]
   3c6a8:	stp	x26, x25, [sp, #160]
   3c6ac:	add	x29, sp, #0x80
   3c6b0:	lsl	x26, x14, #4
   3c6b4:	add	x8, x8, #0x20
   3c6b8:	stp	x28, x27, [sp, #144]
   3c6bc:	stp	x24, x23, [sp, #176]
   3c6c0:	stp	x22, x21, [sp, #192]
   3c6c4:	stp	x20, x19, [sp, #208]
   3c6c8:	stur	x3, [x29, #-40]
   3c6cc:	lsl	x21, x14, #1
   3c6d0:	add	x9, x0, x14, lsl #3
   3c6d4:	str	x2, [sp, #24]
   3c6d8:	stur	x8, [x29, #-24]
   3c6dc:	add	x8, x5, x26
   3c6e0:	mov	x19, x5
   3c6e4:	mov	x28, x4
   3c6e8:	mov	x23, x1
   3c6ec:	mov	x20, x0
   3c6f0:	sub	x24, x2, x21
   3c6f4:	str	x8, [sp, #56]
   3c6f8:	add	x8, x9, #0x8
   3c6fc:	add	x2, x1, x26
   3c700:	lsl	x22, x14, #3
   3c704:	str	x9, [sp, #8]
   3c708:	str	x8, [sp, #64]
   3c70c:	stur	x14, [x29, #-8]
   3c710:	stur	x2, [x29, #-56]
   3c714:	cbz	x24, 3c760 <__gmpn_toom33_mul@@Base+0xd8>
   3c718:	mov	x0, x19
   3c71c:	mov	x1, x23
   3c720:	mov	x3, x24
   3c724:	bl	ca70 <__gmpn_add_n@plt>
   3c728:	ldur	x14, [x29, #-8]
   3c72c:	mov	x8, x24
   3c730:	cbz	x0, 3c764 <__gmpn_toom33_mul@@Base+0xdc>
   3c734:	mov	w27, #0x1                   	// #1
   3c738:	mov	x8, x24
   3c73c:	cmp	x8, x14
   3c740:	b.ge	3c804 <__gmpn_toom33_mul@@Base+0x17c>  // b.tcont
   3c744:	lsl	x9, x8, #3
   3c748:	ldr	x10, [x23, x9]
   3c74c:	add	x8, x8, #0x1
   3c750:	adds	x10, x10, #0x1
   3c754:	str	x10, [x19, x9]
   3c758:	b.cs	3c73c <__gmpn_toom33_mul@@Base+0xb4>  // b.hs, b.nlast
   3c75c:	b	3c764 <__gmpn_toom33_mul@@Base+0xdc>
   3c760:	mov	x8, xzr
   3c764:	cmp	x19, x23
   3c768:	mov	x27, xzr
   3c76c:	b.eq	3c804 <__gmpn_toom33_mul@@Base+0x17c>  // b.none
   3c770:	subs	x9, x14, x8
   3c774:	b.le	3c804 <__gmpn_toom33_mul@@Base+0x17c>
   3c778:	cmp	x9, #0x4
   3c77c:	b.cc	3c7e0 <__gmpn_toom33_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3c780:	lsl	x11, x8, #3
   3c784:	add	x10, x19, x11
   3c788:	add	x12, x23, x22
   3c78c:	cmp	x10, x12
   3c790:	b.cs	3c7a8 <__gmpn_toom33_mul@@Base+0x120>  // b.hs, b.nlast
   3c794:	add	x10, x19, x22
   3c798:	add	x12, x23, x11
   3c79c:	cmp	x12, x10
   3c7a0:	mov	x13, x22
   3c7a4:	b.cc	3c7e0 <__gmpn_toom33_mul@@Base+0x158>  // b.lo, b.ul, b.last
   3c7a8:	and	x10, x9, #0xfffffffffffffffc
   3c7ac:	add	x12, x11, #0x10
   3c7b0:	add	x8, x8, x10
   3c7b4:	add	x11, x23, x12
   3c7b8:	add	x12, x19, x12
   3c7bc:	mov	x13, x10
   3c7c0:	ldp	q0, q1, [x11, #-16]
   3c7c4:	add	x11, x11, #0x20
   3c7c8:	subs	x13, x13, #0x4
   3c7cc:	stp	q0, q1, [x12, #-16]
   3c7d0:	add	x12, x12, #0x20
   3c7d4:	b.ne	3c7c0 <__gmpn_toom33_mul@@Base+0x138>  // b.any
   3c7d8:	cmp	x9, x10
   3c7dc:	b.eq	3c800 <__gmpn_toom33_mul@@Base+0x178>  // b.none
   3c7e0:	lsl	x10, x8, #3
   3c7e4:	sub	x9, x14, x8
   3c7e8:	add	x8, x19, x10
   3c7ec:	add	x10, x23, x10
   3c7f0:	ldr	x11, [x10], #8
   3c7f4:	subs	x9, x9, #0x1
   3c7f8:	str	x11, [x8], #8
   3c7fc:	b.ne	3c7f0 <__gmpn_toom33_mul@@Base+0x168>  // b.any
   3c800:	mov	x27, xzr
   3c804:	sub	x8, x28, x21
   3c808:	stur	x24, [x29, #-48]
   3c80c:	stp	x28, x21, [sp, #32]
   3c810:	stur	x8, [x29, #-16]
   3c814:	ldr	x8, [sp, #56]
   3c818:	add	x24, x23, x22
   3c81c:	mov	x1, x19
   3c820:	mov	x2, x24
   3c824:	add	x8, x8, #0x10
   3c828:	str	x8, [sp, #48]
   3c82c:	ldur	x28, [x29, #-24]
   3c830:	ldur	x3, [x29, #-8]
   3c834:	mov	x25, x22
   3c838:	mov	x0, x28
   3c83c:	bl	ca70 <__gmpn_add_n@plt>
   3c840:	add	x8, x0, x27
   3c844:	stur	x19, [x29, #-32]
   3c848:	ldur	x21, [x29, #-8]
   3c84c:	str	x8, [x28, x22]
   3c850:	ldr	x28, [sp, #64]
   3c854:	cbz	x27, 3c948 <__gmpn_toom33_mul@@Base+0x2c0>
   3c858:	ldr	x25, [sp, #48]
   3c85c:	mov	x1, x19
   3c860:	mov	x2, x24
   3c864:	mov	x3, x21
   3c868:	mov	x0, x25
   3c86c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c870:	sub	x8, x27, x0
   3c874:	str	wzr, [sp, #20]
   3c878:	str	x8, [x25, x21, lsl #3]
   3c87c:	ldp	x1, x19, [x29, #-56]
   3c880:	ldur	x24, [x29, #-24]
   3c884:	mov	x0, x28
   3c888:	add	x26, x20, x26
   3c88c:	mov	x3, x19
   3c890:	mov	x2, x24
   3c894:	bl	ca70 <__gmpn_add_n@plt>
   3c898:	mov	x27, x22
   3c89c:	ldur	x22, [x29, #-40]
   3c8a0:	subs	x14, x21, x19
   3c8a4:	b.eq	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3c8a8:	lsl	x8, x19, #3
   3c8ac:	ldr	x9, [x24, x8]
   3c8b0:	adds	x9, x9, x0
   3c8b4:	str	x9, [x28, x8]
   3c8b8:	b.cc	3c9a0 <__gmpn_toom33_mul@@Base+0x318>  // b.lo, b.ul, b.last
   3c8bc:	ldur	x8, [x29, #-32]
   3c8c0:	ldr	x11, [sp, #24]
   3c8c4:	add	x9, x21, x21, lsl #1
   3c8c8:	sub	x12, x20, x21, lsl #3
   3c8cc:	add	x10, x8, x21, lsl #4
   3c8d0:	mvn	x8, x11
   3c8d4:	lsl	x13, x11, #3
   3c8d8:	mov	w0, #0x1                   	// #1
   3c8dc:	add	x11, x8, x9
   3c8e0:	mov	w8, #0x1                   	// #1
   3c8e4:	cmp	x8, x14
   3c8e8:	b.ge	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.tcont
   3c8ec:	add	x15, x10, x13
   3c8f0:	ldr	x15, [x15, #40]
   3c8f4:	add	x16, x12, x13
   3c8f8:	add	x8, x8, #0x1
   3c8fc:	add	x12, x12, #0x8
   3c900:	add	x10, x10, #0x8
   3c904:	adds	x15, x15, #0x1
   3c908:	sub	x11, x11, #0x1
   3c90c:	str	x15, [x16, #16]
   3c910:	b.cs	3c8e4 <__gmpn_toom33_mul@@Base+0x25c>  // b.hs, b.nlast
   3c914:	cmp	x24, x28
   3c918:	mov	x0, xzr
   3c91c:	b.eq	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3c920:	cmp	x8, x14
   3c924:	b.ge	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.tcont
   3c928:	ldr	x0, [sp, #24]
   3c92c:	add	x14, x21, x21, lsl #1
   3c930:	sub	x15, x14, x0
   3c934:	sub	x15, x15, x8
   3c938:	cmp	x15, #0x4
   3c93c:	b.cs	3ca90 <__gmpn_toom33_mul@@Base+0x408>  // b.hs, b.nlast
   3c940:	ldur	x18, [x29, #-32]
   3c944:	b	3cb14 <__gmpn_toom33_mul@@Base+0x48c>
   3c948:	add	x8, x23, x21, lsl #4
   3c94c:	sub	x8, x8, #0x8
   3c950:	mov	x10, x21
   3c954:	subs	x9, x10, #0x1
   3c958:	b.lt	3c858 <__gmpn_toom33_mul@@Base+0x1d0>  // b.tstop
   3c95c:	add	x10, x19, x10, lsl #3
   3c960:	ldur	x10, [x10, #-8]
   3c964:	ldr	x11, [x8], #-8
   3c968:	cmp	x10, x11
   3c96c:	mov	x10, x9
   3c970:	b.eq	3c954 <__gmpn_toom33_mul@@Base+0x2cc>  // b.none
   3c974:	b.hi	3c858 <__gmpn_toom33_mul@@Base+0x1d0>  // b.pmore
   3c978:	ldr	x25, [sp, #48]
   3c97c:	mov	x1, x24
   3c980:	mov	x2, x19
   3c984:	mov	x3, x21
   3c988:	mov	x0, x25
   3c98c:	bl	c2d0 <__gmpn_sub_n@plt>
   3c990:	mov	w9, #0x1                   	// #1
   3c994:	mov	x8, xzr
   3c998:	str	w9, [sp, #20]
   3c99c:	b	3c878 <__gmpn_toom33_mul@@Base+0x1f0>
   3c9a0:	cmp	x24, x28
   3c9a4:	mov	x0, xzr
   3c9a8:	b.eq	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.none
   3c9ac:	cmp	x14, #0x2
   3c9b0:	b.lt	3cb4c <__gmpn_toom33_mul@@Base+0x4c4>  // b.tstop
   3c9b4:	ldr	x16, [sp, #24]
   3c9b8:	add	x8, x21, x21, lsl #1
   3c9bc:	mvn	x9, x16
   3c9c0:	add	x9, x8, x9
   3c9c4:	cmp	x9, #0x4
   3c9c8:	b.cs	3c9d8 <__gmpn_toom33_mul@@Base+0x350>  // b.hs, b.nlast
   3c9cc:	ldur	x15, [x29, #-32]
   3c9d0:	mov	w10, #0x1                   	// #1
   3c9d4:	b	3ca58 <__gmpn_toom33_mul@@Base+0x3d0>
   3c9d8:	ldur	x15, [x29, #-32]
   3c9dc:	sub	x10, x16, x21
   3c9e0:	mov	w12, #0x28                  	// #40
   3c9e4:	add	x13, x20, x10, lsl #3
   3c9e8:	madd	x10, x21, x12, x15
   3c9ec:	add	x11, x16, x21, lsl #1
   3c9f0:	add	x12, x13, #0x10
   3c9f4:	add	x10, x10, #0x20
   3c9f8:	cmp	x12, x10
   3c9fc:	add	x10, x15, x11, lsl #3
   3ca00:	b.cs	3ca24 <__gmpn_toom33_mul@@Base+0x39c>  // b.hs, b.nlast
   3ca04:	mov	w11, #0x8                   	// #8
   3ca08:	bfi	x11, x21, #4, #60
   3ca0c:	add	x11, x20, x11
   3ca10:	add	x12, x10, #0x28
   3ca14:	cmp	x12, x11
   3ca18:	b.cs	3ca24 <__gmpn_toom33_mul@@Base+0x39c>  // b.hs, b.nlast
   3ca1c:	mov	w10, #0x1                   	// #1
   3ca20:	b	3ca58 <__gmpn_toom33_mul@@Base+0x3d0>
   3ca24:	and	x11, x9, #0xfffffffffffffffc
   3ca28:	add	x12, x10, #0x38
   3ca2c:	orr	x10, x11, #0x1
   3ca30:	add	x13, x13, #0x20
   3ca34:	mov	x14, x11
   3ca38:	ldp	q0, q1, [x12, #-16]
   3ca3c:	subs	x14, x14, #0x4
   3ca40:	add	x12, x12, #0x20
   3ca44:	stp	q0, q1, [x13, #-16]
   3ca48:	add	x13, x13, #0x20
   3ca4c:	b.ne	3ca38 <__gmpn_toom33_mul@@Base+0x3b0>  // b.any
   3ca50:	cmp	x9, x11
   3ca54:	b.eq	3cb48 <__gmpn_toom33_mul@@Base+0x4c0>  // b.none
   3ca58:	add	x9, x10, x16
   3ca5c:	sub	x8, x8, x10
   3ca60:	sub	x10, x9, x21
   3ca64:	add	x9, x9, x21, lsl #1
   3ca68:	add	x10, x20, x10, lsl #3
   3ca6c:	add	x11, x15, x9, lsl #3
   3ca70:	sub	x8, x8, x16
   3ca74:	add	x9, x10, #0x8
   3ca78:	add	x10, x11, #0x20
   3ca7c:	ldr	x11, [x10], #8
   3ca80:	subs	x8, x8, #0x1
   3ca84:	str	x11, [x9], #8
   3ca88:	b.ne	3ca7c <__gmpn_toom33_mul@@Base+0x3f4>  // b.any
   3ca8c:	b	3cb48 <__gmpn_toom33_mul@@Base+0x4c0>
   3ca90:	ldur	x18, [x29, #-32]
   3ca94:	mov	w17, #0x28                  	// #40
   3ca98:	add	x16, x12, x13
   3ca9c:	add	x16, x16, #0x10
   3caa0:	madd	x17, x21, x17, x18
   3caa4:	add	x17, x17, #0x20
   3caa8:	cmp	x16, x17
   3caac:	b.cs	3cacc <__gmpn_toom33_mul@@Base+0x444>  // b.hs, b.nlast
   3cab0:	mov	w16, #0x8                   	// #8
   3cab4:	add	x17, x10, x13
   3cab8:	bfi	x16, x21, #4, #60
   3cabc:	add	x16, x20, x16
   3cac0:	add	x17, x17, #0x28
   3cac4:	cmp	x17, x16
   3cac8:	b.cc	3cb14 <__gmpn_toom33_mul@@Base+0x48c>  // b.lo, b.ul, b.last
   3cacc:	add	x12, x12, x13
   3cad0:	sub	x16, x9, x0
   3cad4:	add	x13, x10, x13
   3cad8:	and	x17, x11, #0xfffffffffffffffc
   3cadc:	add	x10, x12, #0x20
   3cae0:	sub	x12, x16, x8
   3cae4:	and	x9, x15, #0xfffffffffffffffc
   3cae8:	add	x11, x13, #0x38
   3caec:	add	x8, x17, x8
   3caf0:	and	x12, x12, #0xfffffffffffffffc
   3caf4:	ldp	q0, q1, [x11, #-16]
   3caf8:	subs	x12, x12, #0x4
   3cafc:	add	x11, x11, #0x20
   3cb00:	stp	q0, q1, [x10, #-16]
   3cb04:	add	x10, x10, #0x20
   3cb08:	b.ne	3caf4 <__gmpn_toom33_mul@@Base+0x46c>  // b.any
   3cb0c:	cmp	x15, x9
   3cb10:	b.eq	3cb48 <__gmpn_toom33_mul@@Base+0x4c0>  // b.none
   3cb14:	sub	x9, x14, x8
   3cb18:	add	x10, x8, x0
   3cb1c:	sub	x8, x9, x0
   3cb20:	sub	x9, x10, x21
   3cb24:	add	x10, x10, x21, lsl #1
   3cb28:	add	x9, x20, x9, lsl #3
   3cb2c:	add	x10, x18, x10, lsl #3
   3cb30:	add	x9, x9, #0x8
   3cb34:	add	x10, x10, #0x20
   3cb38:	ldr	x11, [x10], #8
   3cb3c:	subs	x8, x8, #0x1
   3cb40:	str	x11, [x9], #8
   3cb44:	b.ne	3cb38 <__gmpn_toom33_mul@@Base+0x4b0>  // b.any
   3cb48:	mov	x0, xzr
   3cb4c:	ldr	x8, [x24, x27]
   3cb50:	ldur	x19, [x29, #-32]
   3cb54:	mov	w9, #0x18                  	// #24
   3cb58:	str	x26, [sp]
   3cb5c:	add	x25, x26, #0x10
   3cb60:	add	x26, x8, x0
   3cb64:	mov	x0, x28
   3cb68:	mov	x1, x23
   3cb6c:	mov	x2, x28
   3cb70:	mov	x3, x21
   3cb74:	madd	x24, x21, x9, x19
   3cb78:	bl	d090 <__gmpn_rsblsh1_n@plt>
   3cb7c:	add	x8, x0, x26, lsl #1
   3cb80:	str	x8, [x28, x27]
   3cb84:	ldr	x8, [sp, #40]
   3cb88:	ldur	x21, [x29, #-16]
   3cb8c:	add	x2, x22, x8, lsl #3
   3cb90:	str	x2, [sp, #40]
   3cb94:	cbz	x21, 3cbe0 <__gmpn_toom33_mul@@Base+0x558>
   3cb98:	mov	x0, x19
   3cb9c:	mov	x1, x22
   3cba0:	mov	x3, x21
   3cba4:	bl	ca70 <__gmpn_add_n@plt>
   3cba8:	ldur	x14, [x29, #-8]
   3cbac:	mov	x8, x21
   3cbb0:	cbz	x0, 3cbe8 <__gmpn_toom33_mul@@Base+0x560>
   3cbb4:	mov	w26, #0x1                   	// #1
   3cbb8:	mov	x8, x21
   3cbbc:	cmp	x8, x14
   3cbc0:	b.ge	3cc84 <__gmpn_toom33_mul@@Base+0x5fc>  // b.tcont
   3cbc4:	lsl	x9, x8, #3
   3cbc8:	ldr	x10, [x22, x9]
   3cbcc:	add	x8, x8, #0x1
   3cbd0:	adds	x10, x10, #0x1
   3cbd4:	str	x10, [x19, x9]
   3cbd8:	b.cs	3cbbc <__gmpn_toom33_mul@@Base+0x534>  // b.hs, b.nlast
   3cbdc:	b	3cbe8 <__gmpn_toom33_mul@@Base+0x560>
   3cbe0:	ldur	x14, [x29, #-8]
   3cbe4:	mov	x8, xzr
   3cbe8:	cmp	x19, x22
   3cbec:	mov	x26, xzr
   3cbf0:	b.eq	3cc84 <__gmpn_toom33_mul@@Base+0x5fc>  // b.none
   3cbf4:	subs	x9, x14, x8
   3cbf8:	b.le	3cc84 <__gmpn_toom33_mul@@Base+0x5fc>
   3cbfc:	cmp	x9, #0x4
   3cc00:	b.cc	3cc60 <__gmpn_toom33_mul@@Base+0x5d8>  // b.lo, b.ul, b.last
   3cc04:	lsl	x11, x8, #3
   3cc08:	add	x10, x19, x11
   3cc0c:	add	x12, x22, x27
   3cc10:	cmp	x10, x12
   3cc14:	b.cs	3cc28 <__gmpn_toom33_mul@@Base+0x5a0>  // b.hs, b.nlast
   3cc18:	add	x10, x19, x27
   3cc1c:	add	x12, x22, x11
   3cc20:	cmp	x12, x10
   3cc24:	b.cc	3cc60 <__gmpn_toom33_mul@@Base+0x5d8>  // b.lo, b.ul, b.last
   3cc28:	and	x10, x9, #0xfffffffffffffffc
   3cc2c:	add	x12, x11, #0x10
   3cc30:	add	x8, x8, x10
   3cc34:	add	x11, x22, x12
   3cc38:	add	x12, x19, x12
   3cc3c:	mov	x13, x10
   3cc40:	ldp	q0, q1, [x11, #-16]
   3cc44:	add	x11, x11, #0x20
   3cc48:	subs	x13, x13, #0x4
   3cc4c:	stp	q0, q1, [x12, #-16]
   3cc50:	add	x12, x12, #0x20
   3cc54:	b.ne	3cc40 <__gmpn_toom33_mul@@Base+0x5b8>  // b.any
   3cc58:	cmp	x9, x10
   3cc5c:	b.eq	3cc80 <__gmpn_toom33_mul@@Base+0x5f8>  // b.none
   3cc60:	lsl	x10, x8, #3
   3cc64:	sub	x9, x14, x8
   3cc68:	add	x8, x19, x10
   3cc6c:	add	x10, x22, x10
   3cc70:	ldr	x11, [x10], #8
   3cc74:	subs	x9, x9, #0x1
   3cc78:	str	x11, [x8], #8
   3cc7c:	b.ne	3cc70 <__gmpn_toom33_mul@@Base+0x5e8>  // b.any
   3cc80:	mov	x26, xzr
   3cc84:	ldur	x3, [x29, #-8]
   3cc88:	add	x28, x24, #0x18
   3cc8c:	add	x24, x22, x27
   3cc90:	mov	x0, x20
   3cc94:	mov	x1, x19
   3cc98:	mov	x2, x24
   3cc9c:	bl	ca70 <__gmpn_add_n@plt>
   3cca0:	ldur	x10, [x29, #-8]
   3cca4:	add	x8, x0, x26
   3cca8:	str	x8, [x20, x27]
   3ccac:	cbz	x26, 3ce24 <__gmpn_toom33_mul@@Base+0x79c>
   3ccb0:	ldur	x3, [x29, #-8]
   3ccb4:	mov	x0, x28
   3ccb8:	mov	x1, x19
   3ccbc:	mov	x2, x24
   3ccc0:	bl	c2d0 <__gmpn_sub_n@plt>
   3ccc4:	ldur	x9, [x29, #-8]
   3ccc8:	sub	x8, x26, x0
   3cccc:	str	x8, [x28, x9, lsl #3]
   3ccd0:	ldur	x21, [x29, #-16]
   3ccd4:	ldr	x1, [sp, #40]
   3ccd8:	mov	x0, x25
   3ccdc:	mov	x2, x20
   3cce0:	mov	x3, x21
   3cce4:	lsl	x26, x9, #2
   3cce8:	bl	ca70 <__gmpn_add_n@plt>
   3ccec:	ldur	x16, [x29, #-8]
   3ccf0:	subs	x11, x16, x21
   3ccf4:	b.eq	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3ccf8:	ldur	x8, [x29, #-16]
   3ccfc:	lsl	x8, x8, #3
   3cd00:	ldr	x9, [x20, x8]
   3cd04:	adds	x9, x9, x0
   3cd08:	str	x9, [x25, x8]
   3cd0c:	b.cc	3ce7c <__gmpn_toom33_mul@@Base+0x7f4>  // b.lo, b.ul, b.last
   3cd10:	ldr	x10, [sp, #32]
   3cd14:	add	x9, x16, x16, lsl #1
   3cd18:	mov	w0, #0x1                   	// #1
   3cd1c:	lsl	x8, x10, #3
   3cd20:	mvn	x10, x10
   3cd24:	add	x12, x8, #0x18
   3cd28:	sub	x8, x8, x16, lsl #4
   3cd2c:	add	x10, x10, x9
   3cd30:	add	x13, x8, #0x8
   3cd34:	mov	w8, #0x1                   	// #1
   3cd38:	cmp	x8, x11
   3cd3c:	b.ge	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.tcont
   3cd40:	ldr	x14, [x20, x13]
   3cd44:	add	x8, x8, #0x1
   3cd48:	add	x13, x13, #0x8
   3cd4c:	sub	x10, x10, #0x1
   3cd50:	adds	x14, x14, #0x1
   3cd54:	str	x14, [x20, x12]
   3cd58:	add	x12, x12, #0x8
   3cd5c:	b.cs	3cd38 <__gmpn_toom33_mul@@Base+0x6b0>  // b.hs, b.nlast
   3cd60:	cmp	x25, x20
   3cd64:	mov	x0, xzr
   3cd68:	b.eq	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3cd6c:	cmp	x8, x11
   3cd70:	b.ge	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.tcont
   3cd74:	ldr	x14, [sp, #32]
   3cd78:	add	x11, x16, x16, lsl #1
   3cd7c:	sub	x14, x11, x14
   3cd80:	sub	x14, x14, x8
   3cd84:	cmp	x14, #0x4
   3cd88:	b.cc	3cdf4 <__gmpn_toom33_mul@@Base+0x76c>  // b.lo, b.ul, b.last
   3cd8c:	ldr	x17, [sp, #8]
   3cd90:	add	x15, x20, x12
   3cd94:	cmp	x15, x17
   3cd98:	b.cs	3cdb4 <__gmpn_toom33_mul@@Base+0x72c>  // b.hs, b.nlast
   3cd9c:	mov	w15, #0x18                  	// #24
   3cda0:	madd	x15, x16, x15, x20
   3cda4:	add	x15, x15, #0x10
   3cda8:	add	x13, x20, x13
   3cdac:	cmp	x13, x15
   3cdb0:	b.cc	3cdf4 <__gmpn_toom33_mul@@Base+0x76c>  // b.lo, b.ul, b.last
   3cdb4:	ldr	x13, [sp, #32]
   3cdb8:	and	x10, x10, #0xfffffffffffffffc
   3cdbc:	add	x12, x20, x12
   3cdc0:	sub	x13, x9, x13
   3cdc4:	sub	x13, x13, x8
   3cdc8:	and	x9, x14, #0xfffffffffffffffc
   3cdcc:	add	x8, x10, x8
   3cdd0:	and	x10, x13, #0xfffffffffffffffc
   3cdd4:	neg	x13, x16, lsl #4
   3cdd8:	add	x15, x12, x13
   3cddc:	ldp	q0, q1, [x15, #-16]
   3cde0:	subs	x10, x10, #0x4
   3cde4:	stp	q0, q1, [x12], #32
   3cde8:	b.ne	3cdd8 <__gmpn_toom33_mul@@Base+0x750>  // b.any
   3cdec:	cmp	x14, x9
   3cdf0:	b.eq	3cf0c <__gmpn_toom33_mul@@Base+0x884>  // b.none
   3cdf4:	sub	x9, x11, x8
   3cdf8:	ldr	x11, [sp, #32]
   3cdfc:	add	x10, x8, x11
   3ce00:	sub	x8, x9, x11
   3ce04:	add	x9, x20, x10, lsl #3
   3ce08:	neg	x10, x16, lsl #4
   3ce0c:	ldr	x11, [x9, x10]
   3ce10:	subs	x8, x8, #0x1
   3ce14:	str	x11, [x9, #16]
   3ce18:	add	x9, x9, #0x8
   3ce1c:	b.ne	3ce0c <__gmpn_toom33_mul@@Base+0x784>  // b.any
   3ce20:	b	3cf0c <__gmpn_toom33_mul@@Base+0x884>
   3ce24:	add	x8, x22, x10, lsl #4
   3ce28:	sub	x8, x8, #0x8
   3ce2c:	subs	x9, x10, #0x1
   3ce30:	b.lt	3ccb0 <__gmpn_toom33_mul@@Base+0x628>  // b.tstop
   3ce34:	add	x10, x19, x10, lsl #3
   3ce38:	ldur	x10, [x10, #-8]
   3ce3c:	ldr	x11, [x8], #-8
   3ce40:	cmp	x10, x11
   3ce44:	mov	x10, x9
   3ce48:	b.eq	3ce2c <__gmpn_toom33_mul@@Base+0x7a4>  // b.none
   3ce4c:	b.hi	3ccb0 <__gmpn_toom33_mul@@Base+0x628>  // b.pmore
   3ce50:	ldur	x3, [x29, #-8]
   3ce54:	mov	x0, x28
   3ce58:	mov	x1, x24
   3ce5c:	mov	x2, x19
   3ce60:	bl	c2d0 <__gmpn_sub_n@plt>
   3ce64:	ldr	w8, [sp, #20]
   3ce68:	ldur	x9, [x29, #-8]
   3ce6c:	eor	w8, w8, #0x1
   3ce70:	str	xzr, [x28, x9, lsl #3]
   3ce74:	str	w8, [sp, #20]
   3ce78:	b	3ccd0 <__gmpn_toom33_mul@@Base+0x648>
   3ce7c:	cmp	x25, x20
   3ce80:	mov	x0, xzr
   3ce84:	b.eq	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.none
   3ce88:	cmp	x11, #0x2
   3ce8c:	b.lt	3cf10 <__gmpn_toom33_mul@@Base+0x888>  // b.tstop
   3ce90:	ldr	x12, [sp, #32]
   3ce94:	add	x8, x16, x16, lsl #1
   3ce98:	mvn	x9, x12
   3ce9c:	add	x9, x8, x9
   3cea0:	cmp	x9, #0x4
   3cea4:	b.cc	3cedc <__gmpn_toom33_mul@@Base+0x854>  // b.lo, b.ul, b.last
   3cea8:	ldr	x11, [sp, #8]
   3ceac:	add	x10, x20, x12, lsl #3
   3ceb0:	add	x10, x10, #0x18
   3ceb4:	cmp	x10, x11
   3ceb8:	b.cs	3d058 <__gmpn_toom33_mul@@Base+0x9d0>  // b.hs, b.nlast
   3cebc:	mov	w11, #0x18                  	// #24
   3cec0:	sub	x12, x12, x16, lsl #1
   3cec4:	madd	x11, x16, x11, x20
   3cec8:	add	x12, x20, x12, lsl #3
   3cecc:	add	x11, x11, #0x10
   3ced0:	add	x12, x12, #0x8
   3ced4:	cmp	x12, x11
   3ced8:	b.cs	3d058 <__gmpn_toom33_mul@@Base+0x9d0>  // b.hs, b.nlast
   3cedc:	mov	w11, #0x1                   	// #1
   3cee0:	ldr	x10, [sp, #32]
   3cee4:	sub	x8, x8, x11
   3cee8:	add	x9, x11, x10
   3ceec:	sub	x8, x8, x10
   3cef0:	add	x9, x20, x9, lsl #3
   3cef4:	neg	x10, x16, lsl #4
   3cef8:	ldr	x11, [x9, x10]
   3cefc:	subs	x8, x8, #0x1
   3cf00:	str	x11, [x9, #16]
   3cf04:	add	x9, x9, #0x8
   3cf08:	b.ne	3cef8 <__gmpn_toom33_mul@@Base+0x870>  // b.any
   3cf0c:	mov	x0, xzr
   3cf10:	ldr	x8, [x20, x27]
   3cf14:	mov	x1, x22
   3cf18:	mov	x2, x25
   3cf1c:	mov	x3, x16
   3cf20:	add	x24, x8, x0
   3cf24:	mov	x0, x25
   3cf28:	str	x23, [sp, #8]
   3cf2c:	mov	x21, x16
   3cf30:	bl	d090 <__gmpn_rsblsh1_n@plt>
   3cf34:	add	x8, x0, x24, lsl #1
   3cf38:	mov	w9, #0x28                  	// #40
   3cf3c:	ldr	x1, [sp, #48]
   3cf40:	str	x8, [x25, x27]
   3cf44:	madd	x8, x21, x9, x19
   3cf48:	add	x24, x21, #0x1
   3cf4c:	add	x27, x8, #0x28
   3cf50:	mov	x0, x19
   3cf54:	mov	x2, x24
   3cf58:	mov	x3, x28
   3cf5c:	mov	x4, x24
   3cf60:	mov	x5, x27
   3cf64:	bl	d450 <__gmpn_toom22_mul@plt>
   3cf68:	ldp	x8, x1, [sp, #56]
   3cf6c:	mov	x2, x24
   3cf70:	mov	x3, x25
   3cf74:	mov	x4, x24
   3cf78:	add	x28, x8, #0x8
   3cf7c:	mov	x0, x28
   3cf80:	mov	x5, x27
   3cf84:	bl	d450 <__gmpn_toom22_mul@plt>
   3cf88:	ldp	x9, x8, [sp, #24]
   3cf8c:	add	x25, x20, x26, lsl #3
   3cf90:	mov	x23, x20
   3cf94:	mov	x0, x25
   3cf98:	cmp	x9, x8
   3cf9c:	b.le	3cfbc <__gmpn_toom33_mul@@Base+0x934>
   3cfa0:	ldp	x1, x19, [x29, #-56]
   3cfa4:	ldur	x21, [x29, #-16]
   3cfa8:	ldr	x3, [sp, #40]
   3cfac:	mov	x2, x19
   3cfb0:	mov	x4, x21
   3cfb4:	bl	ccd0 <__gmpn_mul@plt>
   3cfb8:	b	3cfd8 <__gmpn_toom33_mul@@Base+0x950>
   3cfbc:	ldp	x1, x19, [x29, #-56]
   3cfc0:	ldr	x3, [sp, #40]
   3cfc4:	mov	x5, x27
   3cfc8:	mov	x2, x19
   3cfcc:	mov	x4, x19
   3cfd0:	bl	d450 <__gmpn_toom22_mul@plt>
   3cfd4:	ldur	x21, [x29, #-16]
   3cfd8:	ldr	x0, [sp]
   3cfdc:	ldur	x1, [x29, #-24]
   3cfe0:	ldp	x26, x22, [x25]
   3cfe4:	mov	x2, x24
   3cfe8:	mov	x3, x23
   3cfec:	mov	x4, x24
   3cff0:	mov	x5, x27
   3cff4:	bl	d450 <__gmpn_toom22_mul@plt>
   3cff8:	str	x22, [x25, #8]
   3cffc:	ldur	x22, [x29, #-8]
   3d000:	ldr	x1, [sp, #8]
   3d004:	ldur	x3, [x29, #-40]
   3d008:	mov	x0, x23
   3d00c:	mov	x2, x22
   3d010:	mov	x4, x22
   3d014:	mov	x5, x27
   3d018:	bl	d450 <__gmpn_toom22_mul@plt>
   3d01c:	add	x4, x19, x21
   3d020:	mov	x0, x23
   3d024:	mov	x1, x28
   3d028:	ldur	x2, [x29, #-32]
   3d02c:	mov	x3, x22
   3d030:	ldr	w5, [sp, #20]
   3d034:	mov	x6, x26
   3d038:	ldp	x20, x19, [sp, #208]
   3d03c:	ldp	x22, x21, [sp, #192]
   3d040:	ldp	x24, x23, [sp, #176]
   3d044:	ldp	x26, x25, [sp, #160]
   3d048:	ldp	x28, x27, [sp, #144]
   3d04c:	ldp	x29, x30, [sp, #128]
   3d050:	add	sp, sp, #0xe0
   3d054:	b	ca20 <__gmpn_toom_interpolate_5pts@plt>
   3d058:	and	x12, x9, #0xfffffffffffffffc
   3d05c:	orr	x11, x12, #0x1
   3d060:	neg	x13, x16, lsl #4
   3d064:	mov	x14, x12
   3d068:	add	x15, x10, x13
   3d06c:	ldp	q0, q1, [x15, #-16]
   3d070:	subs	x14, x14, #0x4
   3d074:	stp	q0, q1, [x10], #32
   3d078:	b.ne	3d068 <__gmpn_toom33_mul@@Base+0x9e0>  // b.any
   3d07c:	cmp	x9, x12
   3d080:	b.eq	3cf0c <__gmpn_toom33_mul@@Base+0x884>  // b.none
   3d084:	b	3cee0 <__gmpn_toom33_mul@@Base+0x858>

000000000003d088 <__gmpn_toom43_mul@@Base>:
   3d088:	sub	sp, sp, #0xf0
   3d08c:	add	x8, x2, x2, lsl #1
   3d090:	stp	x28, x27, [sp, #160]
   3d094:	stp	x22, x21, [sp, #208]
   3d098:	stp	x20, x19, [sp, #224]
   3d09c:	mov	x19, x5
   3d0a0:	mov	x27, x4
   3d0a4:	mov	x20, x3
   3d0a8:	mov	x10, x1
   3d0ac:	cmp	x8, x4, lsl #2
   3d0b0:	mov	x21, x0
   3d0b4:	stp	x29, x30, [sp, #144]
   3d0b8:	stp	x26, x25, [sp, #176]
   3d0bc:	stp	x24, x23, [sp, #192]
   3d0c0:	add	x29, sp, #0x90
   3d0c4:	b.ge	3d0e0 <__gmpn_toom43_mul@@Base+0x58>  // b.tcont
   3d0c8:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3d0cc:	sub	x8, x27, #0x1
   3d0d0:	movk	x9, #0xaaab
   3d0d4:	umulh	x8, x8, x9
   3d0d8:	lsr	x25, x8, #1
   3d0dc:	b	3d0e8 <__gmpn_toom43_mul@@Base+0x60>
   3d0e0:	sub	x8, x2, #0x1
   3d0e4:	asr	x25, x8, #2
   3d0e8:	add	x23, x25, #0x1
   3d0ec:	lsl	x11, x23, #1
   3d0f0:	add	x9, x19, x23, lsl #5
   3d0f4:	add	x8, x11, x23
   3d0f8:	str	x11, [sp, #40]
   3d0fc:	str	x9, [sp, #64]
   3d100:	sub	x4, x2, x8
   3d104:	stur	x8, [x29, #-64]
   3d108:	lsl	x8, x8, #3
   3d10c:	add	x1, x9, #0x20
   3d110:	add	x9, x21, x8
   3d114:	add	x8, x19, x8
   3d118:	add	x0, x9, #0x18
   3d11c:	add	x5, x8, #0x18
   3d120:	mov	x2, x10
   3d124:	mov	x3, x23
   3d128:	sub	x24, x27, x11
   3d12c:	str	x0, [sp, #72]
   3d130:	str	x1, [sp, #56]
   3d134:	stp	x4, x10, [x29, #-16]
   3d138:	stur	x5, [x29, #-24]
   3d13c:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3d140:	and	w8, w0, #0x2
   3d144:	lsl	x22, x23, #4
   3d148:	stur	w8, [x29, #-48]
   3d14c:	add	x8, x19, x22
   3d150:	lsl	x26, x23, #3
   3d154:	add	x28, x8, #0x10
   3d158:	add	x1, x20, x26
   3d15c:	mov	w3, #0x1                   	// #1
   3d160:	mov	x0, x28
   3d164:	mov	x2, x23
   3d168:	str	x8, [sp, #48]
   3d16c:	stur	x1, [x29, #-32]
   3d170:	bl	c180 <__gmpn_lshift@plt>
   3d174:	str	x0, [x28, x26]
   3d178:	add	x1, x20, x22
   3d17c:	mov	w3, #0x2                   	// #2
   3d180:	mov	x0, x19
   3d184:	mov	x2, x24
   3d188:	stur	x26, [x29, #-56]
   3d18c:	stur	x1, [x29, #-40]
   3d190:	bl	c180 <__gmpn_lshift@plt>
   3d194:	mov	x22, x0
   3d198:	mov	x0, x19
   3d19c:	mov	x1, x19
   3d1a0:	mov	x2, x20
   3d1a4:	mov	x3, x24
   3d1a8:	bl	ca70 <__gmpn_add_n@plt>
   3d1ac:	subs	x15, x23, x24
   3d1b0:	add	x9, x0, x22
   3d1b4:	b.eq	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3d1b8:	lsl	x8, x24, #3
   3d1bc:	ldr	x10, [x20, x8]
   3d1c0:	adds	x9, x10, x9
   3d1c4:	str	x9, [x19, x8]
   3d1c8:	b.cc	3d300 <__gmpn_toom43_mul@@Base+0x278>  // b.lo, b.ul, b.last
   3d1cc:	add	x9, x25, x25, lsl #1
   3d1d0:	lsl	x10, x25, #4
   3d1d4:	sub	x11, x9, x27
   3d1d8:	mov	x8, xzr
   3d1dc:	lsl	x12, x27, #3
   3d1e0:	sub	x14, x19, x10
   3d1e4:	sub	x13, x20, x10
   3d1e8:	add	x10, x11, #0x2
   3d1ec:	mov	w9, #0x1                   	// #1
   3d1f0:	add	x8, x8, #0x1
   3d1f4:	cmp	x8, x15
   3d1f8:	b.ge	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.tcont
   3d1fc:	add	x16, x13, x12
   3d200:	ldur	x16, [x16, #-8]
   3d204:	add	x17, x14, x12
   3d208:	add	x14, x14, #0x8
   3d20c:	add	x13, x13, #0x8
   3d210:	adds	x16, x16, #0x1
   3d214:	sub	x10, x10, #0x1
   3d218:	stur	x16, [x17, #-8]
   3d21c:	b.cs	3d1f0 <__gmpn_toom43_mul@@Base+0x168>  // b.hs, b.nlast
   3d220:	cmp	x20, x19
   3d224:	mov	x9, xzr
   3d228:	b.eq	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3d22c:	add	x16, x8, #0x1
   3d230:	cmp	x16, x15
   3d234:	b.ge	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.tcont
   3d238:	add	x9, x25, x25, lsl #1
   3d23c:	sub	x15, x9, x27
   3d240:	sub	x15, x15, x8
   3d244:	add	x15, x15, #0x2
   3d248:	cmp	x15, #0x4
   3d24c:	b.cc	3d2cc <__gmpn_toom43_mul@@Base+0x244>  // b.lo, b.ul, b.last
   3d250:	ldur	x18, [x29, #-56]
   3d254:	add	x17, x14, x12
   3d258:	sub	x17, x17, #0x8
   3d25c:	add	x18, x20, x18
   3d260:	cmp	x17, x18
   3d264:	b.cs	3d280 <__gmpn_toom43_mul@@Base+0x1f8>  // b.hs, b.nlast
   3d268:	ldur	x17, [x29, #-56]
   3d26c:	add	x18, x13, x12
   3d270:	sub	x18, x18, #0x8
   3d274:	add	x17, x19, x17
   3d278:	cmp	x18, x17
   3d27c:	b.cc	3d2cc <__gmpn_toom43_mul@@Base+0x244>  // b.lo, b.ul, b.last
   3d280:	add	x14, x14, x12
   3d284:	add	x12, x13, x12
   3d288:	sub	x13, x11, x8
   3d28c:	and	x16, x10, #0xfffffffffffffffc
   3d290:	add	x13, x13, #0x2
   3d294:	add	x8, x16, x8
   3d298:	and	x11, x15, #0xfffffffffffffffc
   3d29c:	add	x10, x14, #0x8
   3d2a0:	add	x12, x12, #0x8
   3d2a4:	add	x16, x8, #0x1
   3d2a8:	and	x8, x13, #0xfffffffffffffffc
   3d2ac:	ldp	q0, q1, [x12, #-16]
   3d2b0:	subs	x8, x8, #0x4
   3d2b4:	add	x12, x12, #0x20
   3d2b8:	stp	q0, q1, [x10, #-16]
   3d2bc:	add	x10, x10, #0x20
   3d2c0:	b.ne	3d2ac <__gmpn_toom43_mul@@Base+0x224>  // b.any
   3d2c4:	cmp	x15, x11
   3d2c8:	b.eq	3d390 <__gmpn_toom43_mul@@Base+0x308>  // b.none
   3d2cc:	add	x8, x16, x27
   3d2d0:	sub	x9, x8, x9
   3d2d4:	sub	x10, x8, x25, lsl #1
   3d2d8:	sub	x8, x9, #0x3
   3d2dc:	lsl	x9, x10, #3
   3d2e0:	sub	x10, x9, #0x10
   3d2e4:	add	x9, x19, x10
   3d2e8:	add	x10, x20, x10
   3d2ec:	ldr	x11, [x10], #8
   3d2f0:	adds	x8, x8, #0x1
   3d2f4:	str	x11, [x9], #8
   3d2f8:	b.cc	3d2ec <__gmpn_toom43_mul@@Base+0x264>  // b.lo, b.ul, b.last
   3d2fc:	b	3d390 <__gmpn_toom43_mul@@Base+0x308>
   3d300:	cmp	x20, x19
   3d304:	mov	x9, xzr
   3d308:	b.eq	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.none
   3d30c:	cmp	x15, #0x2
   3d310:	b.lt	3d394 <__gmpn_toom43_mul@@Base+0x30c>  // b.tstop
   3d314:	add	x8, x25, x25, lsl #1
   3d318:	sub	x9, x8, x27
   3d31c:	add	x9, x9, #0x2
   3d320:	cmp	x9, #0x4
   3d324:	b.cc	3d35c <__gmpn_toom43_mul@@Base+0x2d4>  // b.lo, b.ul, b.last
   3d328:	mov	x10, #0xffffffffffffffff    	// #-1
   3d32c:	ldur	x13, [x29, #-56]
   3d330:	eor	x10, x10, x25, lsl #1
   3d334:	add	x10, x10, x27
   3d338:	lsl	x10, x10, #3
   3d33c:	add	x11, x19, x10
   3d340:	add	x12, x20, x13
   3d344:	cmp	x11, x12
   3d348:	b.cs	3d71c <__gmpn_toom43_mul@@Base+0x694>  // b.hs, b.nlast
   3d34c:	add	x11, x19, x13
   3d350:	add	x10, x20, x10
   3d354:	cmp	x10, x11
   3d358:	b.cs	3d71c <__gmpn_toom43_mul@@Base+0x694>  // b.hs, b.nlast
   3d35c:	mov	w10, #0x1                   	// #1
   3d360:	add	x9, x10, x27
   3d364:	sub	x8, x9, x8
   3d368:	sub	x9, x9, x25, lsl #1
   3d36c:	lsl	x9, x9, #3
   3d370:	sub	x10, x9, #0x10
   3d374:	sub	x8, x8, #0x3
   3d378:	add	x9, x19, x10
   3d37c:	add	x10, x20, x10
   3d380:	ldr	x11, [x10], #8
   3d384:	adds	x8, x8, #0x1
   3d388:	str	x11, [x9], #8
   3d38c:	b.cc	3d380 <__gmpn_toom43_mul@@Base+0x2f8>  // b.lo, b.ul, b.last
   3d390:	mov	x9, xzr
   3d394:	ldr	x8, [sp, #40]
   3d398:	add	x22, x25, #0x2
   3d39c:	mov	x1, x19
   3d3a0:	mov	x2, x28
   3d3a4:	add	x8, x21, x8, lsl #3
   3d3a8:	add	x0, x8, #0x10
   3d3ac:	mov	x3, x22
   3d3b0:	str	x27, [sp, #8]
   3d3b4:	lsl	x27, x23, #2
   3d3b8:	str	x9, [x19, x23, lsl #3]
   3d3bc:	stp	x0, x8, [sp, #32]
   3d3c0:	bl	ca70 <__gmpn_add_n@plt>
   3d3c4:	mov	w8, #0x18                  	// #24
   3d3c8:	mul	x8, x25, x8
   3d3cc:	add	x8, x8, #0x28
   3d3d0:	mov	x9, x23
   3d3d4:	add	x10, x9, #0x1
   3d3d8:	cmp	x10, #0x1
   3d3dc:	b.lt	3d3fc <__gmpn_toom43_mul@@Base+0x374>  // b.tstop
   3d3e0:	ldr	x10, [x19, x9, lsl #3]
   3d3e4:	ldr	x11, [x19, x8]
   3d3e8:	sub	x9, x9, #0x1
   3d3ec:	sub	x8, x8, #0x8
   3d3f0:	cmp	x10, x11
   3d3f4:	b.eq	3d3d4 <__gmpn_toom43_mul@@Base+0x34c>  // b.none
   3d3f8:	b.ls	3d41c <__gmpn_toom43_mul@@Base+0x394>  // b.plast
   3d3fc:	add	x8, x21, x23, lsl #3
   3d400:	add	x0, x8, #0x8
   3d404:	mov	x1, x19
   3d408:	mov	x2, x28
   3d40c:	mov	x3, x22
   3d410:	str	x0, [sp, #24]
   3d414:	bl	c2d0 <__gmpn_sub_n@plt>
   3d418:	b	3d444 <__gmpn_toom43_mul@@Base+0x3bc>
   3d41c:	add	x8, x21, x23, lsl #3
   3d420:	add	x0, x8, #0x8
   3d424:	mov	x1, x28
   3d428:	mov	x2, x19
   3d42c:	mov	x3, x22
   3d430:	str	x0, [sp, #24]
   3d434:	bl	c2d0 <__gmpn_sub_n@plt>
   3d438:	ldur	w8, [x29, #-48]
   3d43c:	eor	w8, w8, #0x2
   3d440:	stur	w8, [x29, #-48]
   3d444:	add	x8, x21, x27, lsl #3
   3d448:	add	x0, x8, #0x20
   3d44c:	str	x0, [sp, #16]
   3d450:	ldp	x1, x4, [x29, #-24]
   3d454:	ldur	x2, [x29, #-8]
   3d458:	mov	x3, x23
   3d45c:	mov	x5, x19
   3d460:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3d464:	and	w26, w0, #0x1
   3d468:	cbz	x24, 3d4bc <__gmpn_toom43_mul@@Base+0x434>
   3d46c:	ldur	x2, [x29, #-40]
   3d470:	mov	x0, x28
   3d474:	mov	x1, x20
   3d478:	mov	x3, x24
   3d47c:	bl	ca70 <__gmpn_add_n@plt>
   3d480:	mov	x8, x24
   3d484:	cbz	x0, 3d4c0 <__gmpn_toom43_mul@@Base+0x438>
   3d488:	ldr	x8, [sp, #8]
   3d48c:	mov	w27, #0x1                   	// #1
   3d490:	add	x8, x19, x8, lsl #3
   3d494:	add	x9, x8, #0x10
   3d498:	mov	x8, x24
   3d49c:	cmp	x8, x25
   3d4a0:	b.gt	3d57c <__gmpn_toom43_mul@@Base+0x4f4>
   3d4a4:	ldr	x10, [x20, x8, lsl #3]
   3d4a8:	add	x8, x8, #0x1
   3d4ac:	adds	x10, x10, #0x1
   3d4b0:	str	x10, [x9], #8
   3d4b4:	b.cs	3d49c <__gmpn_toom43_mul@@Base+0x414>  // b.hs, b.nlast
   3d4b8:	b	3d4c0 <__gmpn_toom43_mul@@Base+0x438>
   3d4bc:	mov	x8, xzr
   3d4c0:	cmp	x28, x20
   3d4c4:	mov	x27, xzr
   3d4c8:	b.eq	3d57c <__gmpn_toom43_mul@@Base+0x4f4>  // b.none
   3d4cc:	ldur	w14, [x29, #-48]
   3d4d0:	cmp	x8, x25
   3d4d4:	b.gt	3d580 <__gmpn_toom43_mul@@Base+0x4f8>
   3d4d8:	sub	x9, x25, x8
   3d4dc:	add	x9, x9, #0x1
   3d4e0:	cmp	x9, #0x4
   3d4e4:	b.cc	3d54c <__gmpn_toom43_mul@@Base+0x4c4>  // b.lo, b.ul, b.last
   3d4e8:	add	x10, x8, x25, lsl #1
   3d4ec:	add	x12, x19, x10, lsl #3
   3d4f0:	add	x10, x12, #0x20
   3d4f4:	add	x11, x20, x23, lsl #3
   3d4f8:	cmp	x10, x11
   3d4fc:	add	x11, x20, x8, lsl #3
   3d500:	b.cs	3d518 <__gmpn_toom43_mul@@Base+0x490>  // b.hs, b.nlast
   3d504:	mov	w10, #0x18                  	// #24
   3d508:	madd	x10, x25, x10, x19
   3d50c:	add	x10, x10, #0x28
   3d510:	cmp	x11, x10
   3d514:	b.cc	3d54c <__gmpn_toom43_mul@@Base+0x4c4>  // b.lo, b.ul, b.last
   3d518:	and	x10, x9, #0xfffffffffffffffc
   3d51c:	add	x11, x11, #0x10
   3d520:	add	x8, x8, x10
   3d524:	add	x12, x12, #0x30
   3d528:	mov	x13, x10
   3d52c:	ldp	q0, q1, [x11, #-16]
   3d530:	subs	x13, x13, #0x4
   3d534:	add	x11, x11, #0x20
   3d538:	stp	q0, q1, [x12, #-16]
   3d53c:	add	x12, x12, #0x20
   3d540:	b.ne	3d52c <__gmpn_toom43_mul@@Base+0x4a4>  // b.any
   3d544:	cmp	x9, x10
   3d548:	b.eq	3d574 <__gmpn_toom43_mul@@Base+0x4ec>  // b.none
   3d54c:	add	x10, x8, x25, lsl #1
   3d550:	sub	x9, x25, x8
   3d554:	add	x10, x19, x10, lsl #3
   3d558:	add	x9, x9, #0x1
   3d55c:	add	x10, x10, #0x20
   3d560:	add	x8, x20, x8, lsl #3
   3d564:	ldr	x11, [x8], #8
   3d568:	subs	x9, x9, #0x1
   3d56c:	str	x11, [x10], #8
   3d570:	b.ne	3d564 <__gmpn_toom43_mul@@Base+0x4dc>  // b.any
   3d574:	mov	x27, xzr
   3d578:	b	3d580 <__gmpn_toom43_mul@@Base+0x4f8>
   3d57c:	ldur	w14, [x29, #-48]
   3d580:	stur	x24, [x29, #-48]
   3d584:	eor	w24, w26, w14
   3d588:	ldur	x26, [x29, #-56]
   3d58c:	ldur	x2, [x29, #-32]
   3d590:	mov	x0, x21
   3d594:	mov	x1, x28
   3d598:	mov	x3, x23
   3d59c:	str	x27, [x28, x26]
   3d5a0:	bl	ca70 <__gmpn_add_n@plt>
   3d5a4:	add	x8, x0, x27
   3d5a8:	str	x8, [x21, x26]
   3d5ac:	ldr	x8, [x28, x26]
   3d5b0:	str	x21, [sp, #8]
   3d5b4:	cbz	x8, 3d6c8 <__gmpn_toom43_mul@@Base+0x640>
   3d5b8:	ldur	x2, [x29, #-32]
   3d5bc:	mov	x0, x28
   3d5c0:	mov	x1, x28
   3d5c4:	mov	x3, x23
   3d5c8:	bl	c2d0 <__gmpn_sub_n@plt>
   3d5cc:	ldr	x8, [x28, x26]
   3d5d0:	mov	w27, w24
   3d5d4:	sub	x8, x8, x0
   3d5d8:	str	x8, [x28, x26]
   3d5dc:	ldur	x1, [x29, #-24]
   3d5e0:	mov	x0, x19
   3d5e4:	mov	x2, x28
   3d5e8:	mov	x3, x22
   3d5ec:	bl	c990 <__gmpn_mul_n@plt>
   3d5f0:	ldp	x8, x1, [sp, #48]
   3d5f4:	ldr	x2, [sp, #24]
   3d5f8:	mov	x3, x22
   3d5fc:	add	x28, x8, #0x8
   3d600:	mov	x0, x28
   3d604:	bl	c990 <__gmpn_mul_n@plt>
   3d608:	ldp	x8, x1, [sp, #64]
   3d60c:	ldr	x2, [sp, #32]
   3d610:	mov	x3, x22
   3d614:	add	x26, x8, #0x10
   3d618:	mov	x0, x26
   3d61c:	bl	c990 <__gmpn_mul_n@plt>
   3d620:	ldp	x21, x1, [sp, #8]
   3d624:	ldr	x0, [sp, #40]
   3d628:	mov	x3, x22
   3d62c:	mov	x2, x21
   3d630:	bl	c990 <__gmpn_mul_n@plt>
   3d634:	mov	w8, #0x28                  	// #40
   3d638:	ldp	x25, x22, [x29, #-16]
   3d63c:	ldur	x24, [x29, #-48]
   3d640:	madd	x0, x23, x8, x21
   3d644:	ldur	x8, [x29, #-64]
   3d648:	cmp	x25, x24
   3d64c:	add	x3, x22, x8, lsl #3
   3d650:	b.le	3d668 <__gmpn_toom43_mul@@Base+0x5e0>
   3d654:	mov	x1, x3
   3d658:	ldur	x3, [x29, #-40]
   3d65c:	mov	x2, x25
   3d660:	mov	x4, x24
   3d664:	b	3d674 <__gmpn_toom43_mul@@Base+0x5ec>
   3d668:	ldur	x1, [x29, #-40]
   3d66c:	mov	x2, x24
   3d670:	mov	x4, x25
   3d674:	bl	ccd0 <__gmpn_mul@plt>
   3d678:	mov	x0, x21
   3d67c:	mov	x1, x22
   3d680:	mov	x2, x20
   3d684:	mov	x3, x23
   3d688:	bl	c990 <__gmpn_mul_n@plt>
   3d68c:	add	x6, x24, x25
   3d690:	mov	x0, x21
   3d694:	mov	x1, x23
   3d698:	mov	w2, w27
   3d69c:	mov	x3, x19
   3d6a0:	mov	x4, x28
   3d6a4:	mov	x5, x26
   3d6a8:	ldp	x20, x19, [sp, #224]
   3d6ac:	ldp	x22, x21, [sp, #208]
   3d6b0:	ldp	x24, x23, [sp, #192]
   3d6b4:	ldp	x26, x25, [sp, #176]
   3d6b8:	ldp	x28, x27, [sp, #160]
   3d6bc:	ldp	x29, x30, [sp, #144]
   3d6c0:	add	sp, sp, #0xf0
   3d6c4:	b	c910 <__gmpn_toom_interpolate_6pts@plt>
   3d6c8:	mov	w9, #0x18                  	// #24
   3d6cc:	add	x8, x20, x25, lsl #4
   3d6d0:	madd	x9, x25, x9, x19
   3d6d4:	add	x8, x8, #0x8
   3d6d8:	add	x9, x9, #0x20
   3d6dc:	mov	x10, x23
   3d6e0:	subs	x10, x10, #0x1
   3d6e4:	b.lt	3d5b8 <__gmpn_toom43_mul@@Base+0x530>  // b.tstop
   3d6e8:	ldr	x11, [x9], #-8
   3d6ec:	ldr	x12, [x8], #-8
   3d6f0:	cmp	x11, x12
   3d6f4:	b.eq	3d6e0 <__gmpn_toom43_mul@@Base+0x658>  // b.none
   3d6f8:	b.hi	3d5b8 <__gmpn_toom43_mul@@Base+0x530>  // b.pmore
   3d6fc:	ldur	x1, [x29, #-32]
   3d700:	mov	x0, x28
   3d704:	mov	x2, x28
   3d708:	mov	x3, x23
   3d70c:	bl	c2d0 <__gmpn_sub_n@plt>
   3d710:	mov	w27, w24
   3d714:	eor	w27, w24, #0x1
   3d718:	b	3d5dc <__gmpn_toom43_mul@@Base+0x554>
   3d71c:	sub	x12, x27, x25, lsl #1
   3d720:	lsl	x12, x12, #3
   3d724:	and	x11, x9, #0xfffffffffffffffc
   3d728:	add	x13, x12, #0x8
   3d72c:	orr	x10, x11, #0x1
   3d730:	add	x12, x20, x13
   3d734:	add	x13, x19, x13
   3d738:	mov	x14, x11
   3d73c:	ldp	q0, q1, [x12, #-16]
   3d740:	subs	x14, x14, #0x4
   3d744:	add	x12, x12, #0x20
   3d748:	stp	q0, q1, [x13, #-16]
   3d74c:	add	x13, x13, #0x20
   3d750:	b.ne	3d73c <__gmpn_toom43_mul@@Base+0x6b4>  // b.any
   3d754:	cmp	x9, x11
   3d758:	b.eq	3d390 <__gmpn_toom43_mul@@Base+0x308>  // b.none
   3d75c:	b	3d360 <__gmpn_toom43_mul@@Base+0x2d8>

000000000003d760 <__gmpn_toom53_mul@@Base>:
   3d760:	stp	x29, x30, [sp, #-96]!
   3d764:	stp	x28, x27, [sp, #16]
   3d768:	stp	x26, x25, [sp, #32]
   3d76c:	stp	x24, x23, [sp, #48]
   3d770:	stp	x22, x21, [sp, #64]
   3d774:	stp	x20, x19, [sp, #80]
   3d778:	mov	x29, sp
   3d77c:	sub	sp, sp, #0xb0
   3d780:	add	x8, x2, x2, lsl #1
   3d784:	add	x9, x4, x4, lsl #2
   3d788:	cmp	x8, x9
   3d78c:	mov	w10, #0x5                   	// #5
   3d790:	mov	w11, #0x3                   	// #3
   3d794:	csel	x8, x4, x2, lt  // lt = tstop
   3d798:	csel	x9, x11, x10, lt  // lt = tstop
   3d79c:	sub	x8, x8, #0x1
   3d7a0:	udiv	x11, x8, x9
   3d7a4:	add	x19, x11, #0x2
   3d7a8:	add	x21, x11, #0x1
   3d7ac:	add	x8, x19, x19, lsl #2
   3d7b0:	mov	x25, x1
   3d7b4:	lsl	x9, x21, #2
   3d7b8:	lsl	x10, x21, #1
   3d7bc:	lsl	x1, x8, #4
   3d7c0:	mov	w8, #0x7f00                	// #32512
   3d7c4:	mov	x20, x0
   3d7c8:	stur	x9, [x29, #-88]
   3d7cc:	sub	x23, x2, x9
   3d7d0:	sub	x9, x4, x10
   3d7d4:	cmp	x1, x8
   3d7d8:	stur	x5, [x29, #-72]
   3d7dc:	stur	x3, [x29, #-24]
   3d7e0:	stp	x2, x10, [x29, #-56]
   3d7e4:	stp	x4, x11, [x29, #-176]
   3d7e8:	stur	x9, [x29, #-40]
   3d7ec:	stur	xzr, [x29, #-16]
   3d7f0:	b.hi	3dea0 <__gmpn_toom53_mul@@Base+0x740>  // b.pmore
   3d7f4:	add	x9, x1, #0xf
   3d7f8:	mov	x8, sp
   3d7fc:	and	x9, x9, #0xfffffffffffffff0
   3d800:	sub	x0, x8, x9
   3d804:	mov	sp, x0
   3d808:	stur	x19, [x29, #-80]
   3d80c:	lsl	x19, x19, #3
   3d810:	add	x1, x0, x19
   3d814:	add	x24, x1, x19
   3d818:	add	x22, x24, x19
   3d81c:	add	x28, x22, x19
   3d820:	add	x26, x28, x19
   3d824:	add	x8, x26, x19
   3d828:	mov	w2, #0x4                   	// #4
   3d82c:	mov	x3, x25
   3d830:	mov	x4, x21
   3d834:	mov	x5, x23
   3d838:	mov	x6, x20
   3d83c:	stp	x0, x8, [x29, #-112]
   3d840:	add	x27, x8, x19
   3d844:	stur	x1, [x29, #-136]
   3d848:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3d84c:	and	w8, w0, #0x2
   3d850:	mov	w2, #0x4                   	// #4
   3d854:	mov	x0, x24
   3d858:	mov	x1, x22
   3d85c:	mov	x3, x25
   3d860:	mov	x4, x21
   3d864:	mov	x5, x23
   3d868:	mov	x6, x20
   3d86c:	stur	w8, [x29, #-28]
   3d870:	stp	x24, x22, [x29, #-160]
   3d874:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3d878:	stur	w0, [x29, #-64]
   3d87c:	add	x1, x25, x21, lsl #3
   3d880:	mov	x0, x28
   3d884:	mov	x2, x25
   3d888:	mov	x3, x21
   3d88c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d890:	ldur	x8, [x29, #-48]
   3d894:	mov	x22, x0
   3d898:	mov	x0, x28
   3d89c:	mov	x2, x28
   3d8a0:	add	x1, x25, x8, lsl #3
   3d8a4:	mov	x3, x21
   3d8a8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d8ac:	mov	w8, #0x18                  	// #24
   3d8b0:	add	x22, x0, x22, lsl #1
   3d8b4:	madd	x1, x21, x8, x25
   3d8b8:	mov	x0, x28
   3d8bc:	mov	x2, x28
   3d8c0:	mov	x3, x21
   3d8c4:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d8c8:	ldur	x24, [x29, #-168]
   3d8cc:	add	x22, x0, x22, lsl #1
   3d8d0:	stur	x23, [x29, #-128]
   3d8d4:	stur	x28, [x29, #-144]
   3d8d8:	cmp	x24, x23
   3d8dc:	b.ge	3d904 <__gmpn_toom53_mul@@Base+0x1a4>  // b.tcont
   3d8e0:	ldur	x8, [x29, #-88]
   3d8e4:	mov	x0, x28
   3d8e8:	mov	x2, x28
   3d8ec:	mov	x3, x21
   3d8f0:	add	x1, x25, x8, lsl #3
   3d8f4:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d8f8:	add	x8, x0, x22, lsl #1
   3d8fc:	str	x8, [x28, x21, lsl #3]
   3d900:	b	3d978 <__gmpn_toom53_mul@@Base+0x218>
   3d904:	ldur	x8, [x29, #-88]
   3d908:	mov	x0, x28
   3d90c:	mov	x2, x28
   3d910:	mov	x3, x23
   3d914:	add	x1, x25, x8, lsl #3
   3d918:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3d91c:	mov	x8, x23
   3d920:	add	x23, x28, x23, lsl #3
   3d924:	stur	x0, [x29, #-96]
   3d928:	sub	x2, x21, x8
   3d92c:	mov	w3, #0x1                   	// #1
   3d930:	mov	x0, x23
   3d934:	mov	x1, x23
   3d938:	bl	c180 <__gmpn_lshift@plt>
   3d93c:	add	x8, x0, x22, lsl #1
   3d940:	str	x8, [x28, x21, lsl #3]
   3d944:	ldr	x8, [x23]
   3d948:	ldur	x9, [x29, #-96]
   3d94c:	adds	x8, x8, x9
   3d950:	str	x8, [x23]
   3d954:	b.cc	3d978 <__gmpn_toom53_mul@@Base+0x218>  // b.lo, b.ul, b.last
   3d958:	ldur	x8, [x29, #-112]
   3d95c:	ldur	x9, [x29, #-56]
   3d960:	add	x8, x8, x9, lsl #3
   3d964:	add	x8, x8, #0x28
   3d968:	ldr	x9, [x8]
   3d96c:	adds	x9, x9, #0x1
   3d970:	str	x9, [x8], #8
   3d974:	b.cs	3d968 <__gmpn_toom53_mul@@Base+0x208>  // b.hs, b.nlast
   3d978:	add	x8, x27, x19
   3d97c:	stur	x8, [x29, #-56]
   3d980:	ldur	w8, [x29, #-28]
   3d984:	ldur	w9, [x29, #-64]
   3d988:	ldur	x22, [x29, #-24]
   3d98c:	lsl	x23, x21, #3
   3d990:	stur	x25, [x29, #-120]
   3d994:	bfxil	w8, w9, #0, #1
   3d998:	stur	w8, [x29, #-28]
   3d99c:	ldp	x8, x28, [x29, #-48]
   3d9a0:	add	x2, x22, x8, lsl #3
   3d9a4:	stur	x2, [x29, #-64]
   3d9a8:	cbz	x28, 3d9e4 <__gmpn_toom53_mul@@Base+0x284>
   3d9ac:	mov	x0, x26
   3d9b0:	mov	x1, x22
   3d9b4:	mov	x3, x28
   3d9b8:	bl	ca70 <__gmpn_add_n@plt>
   3d9bc:	cbz	x0, 3d9e4 <__gmpn_toom53_mul@@Base+0x284>
   3d9c0:	ldur	x28, [x29, #-40]
   3d9c4:	cmp	x28, x24
   3d9c8:	b.gt	3da54 <__gmpn_toom53_mul@@Base+0x2f4>
   3d9cc:	lsl	x9, x28, #3
   3d9d0:	ldr	x10, [x22, x9]
   3d9d4:	add	x28, x28, #0x1
   3d9d8:	adds	x10, x10, #0x1
   3d9dc:	str	x10, [x26, x9]
   3d9e0:	b.cs	3d9c4 <__gmpn_toom53_mul@@Base+0x264>  // b.hs, b.nlast
   3d9e4:	cmp	x26, x22
   3d9e8:	b.eq	3da1c <__gmpn_toom53_mul@@Base+0x2bc>  // b.none
   3d9ec:	cmp	x28, x24
   3d9f0:	b.gt	3da1c <__gmpn_toom53_mul@@Base+0x2bc>
   3d9f4:	mov	w9, #0x28                  	// #40
   3d9f8:	lsl	x10, x28, #3
   3d9fc:	madd	x9, x24, x9, x10
   3da00:	add	x1, x22, x10
   3da04:	ldur	x10, [x29, #-112]
   3da08:	sub	x8, x21, x28
   3da0c:	lsl	x2, x8, #3
   3da10:	add	x9, x9, x10
   3da14:	add	x0, x9, #0x50
   3da18:	bl	bed0 <memcpy@plt>
   3da1c:	ldur	x28, [x29, #-64]
   3da20:	add	x1, x22, x23
   3da24:	mov	x8, x21
   3da28:	str	xzr, [x26, x23]
   3da2c:	subs	x8, x8, #0x1
   3da30:	b.lt	3da4c <__gmpn_toom53_mul@@Base+0x2ec>  // b.tstop
   3da34:	lsl	x9, x8, #3
   3da38:	ldr	x10, [x26, x9]
   3da3c:	ldr	x9, [x1, x9]
   3da40:	cmp	x10, x9
   3da44:	b.eq	3da2c <__gmpn_toom53_mul@@Base+0x2cc>  // b.none
   3da48:	b.ls	3de78 <__gmpn_toom53_mul@@Base+0x718>  // b.plast
   3da4c:	mov	x22, xzr
   3da50:	b	3da5c <__gmpn_toom53_mul@@Base+0x2fc>
   3da54:	mov	w22, #0x1                   	// #1
   3da58:	str	x22, [x26, x21, lsl #3]
   3da5c:	ldur	x25, [x29, #-24]
   3da60:	ldur	x28, [x29, #-104]
   3da64:	mov	x1, x26
   3da68:	mov	x3, x21
   3da6c:	add	x2, x25, x23
   3da70:	mov	x0, x28
   3da74:	bl	c2d0 <__gmpn_sub_n@plt>
   3da78:	sub	x8, x22, x0
   3da7c:	str	x8, [x28, x23]
   3da80:	ldur	x28, [x29, #-64]
   3da84:	mov	x22, x25
   3da88:	ldur	x8, [x29, #-56]
   3da8c:	mov	x0, x26
   3da90:	mov	x1, x26
   3da94:	mov	x3, x21
   3da98:	add	x8, x8, x19
   3da9c:	mov	x19, x22
   3daa0:	add	x22, x22, x23
   3daa4:	mov	x2, x22
   3daa8:	stur	x8, [x29, #-96]
   3daac:	bl	ca70 <__gmpn_add_n@plt>
   3dab0:	ldr	x8, [x26, x23]
   3dab4:	ldur	x25, [x29, #-40]
   3dab8:	mov	x1, x19
   3dabc:	mov	x2, x28
   3dac0:	add	x8, x8, x0
   3dac4:	mov	x0, x27
   3dac8:	mov	x3, x25
   3dacc:	str	x8, [x26, x23]
   3dad0:	bl	cba0 <__gmpn_addlsh2_n@plt>
   3dad4:	ldur	x19, [x29, #-80]
   3dad8:	cmp	x24, x25
   3dadc:	b.lt	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.tstop
   3dae0:	ldur	x9, [x29, #-24]
   3dae4:	lsl	x8, x25, #3
   3dae8:	add	x11, x27, x8
   3daec:	add	x10, x9, x8
   3daf0:	ldr	x12, [x10]
   3daf4:	sub	x9, x21, x25
   3daf8:	adds	x8, x12, x0
   3dafc:	str	x8, [x11]
   3db00:	b.cc	3db94 <__gmpn_toom53_mul@@Base+0x434>  // b.lo, b.ul, b.last
   3db04:	mov	w0, #0x1                   	// #1
   3db08:	mov	w8, #0x1                   	// #1
   3db0c:	cmp	x8, x9
   3db10:	b.ge	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.tcont
   3db14:	lsl	x12, x8, #3
   3db18:	ldr	x13, [x10, x12]
   3db1c:	add	x8, x8, #0x1
   3db20:	adds	x13, x13, #0x1
   3db24:	str	x13, [x11, x12]
   3db28:	b.cs	3db0c <__gmpn_toom53_mul@@Base+0x3ac>  // b.hs, b.nlast
   3db2c:	ldur	x10, [x29, #-24]
   3db30:	mov	x0, xzr
   3db34:	cmp	x27, x10
   3db38:	b.eq	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.none
   3db3c:	cmp	x8, x9
   3db40:	b.ge	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.tcont
   3db44:	ldur	x13, [x29, #-176]
   3db48:	ldur	x12, [x29, #-24]
   3db4c:	lsl	x11, x24, #1
   3db50:	mov	w9, #0x28                  	// #40
   3db54:	add	x10, x8, x13
   3db58:	sub	x10, x10, x11
   3db5c:	add	x10, x12, x10, lsl #3
   3db60:	sub	x1, x10, #0x10
   3db64:	ldur	x10, [x29, #-112]
   3db68:	mul	x9, x24, x9
   3db6c:	add	x11, x11, x24
   3db70:	add	x9, x9, x8, lsl #3
   3db74:	sub	x8, x11, x8
   3db78:	add	x9, x9, x13, lsl #3
   3db7c:	sub	x8, x8, x13
   3db80:	add	x9, x9, x10
   3db84:	lsl	x8, x8, #3
   3db88:	add	x0, x9, #0x60
   3db8c:	add	x2, x8, #0x18
   3db90:	b	3dbec <__gmpn_toom53_mul@@Base+0x48c>
   3db94:	cmp	x9, #0x2
   3db98:	mov	x0, xzr
   3db9c:	b.lt	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.tstop
   3dba0:	ldur	x8, [x29, #-24]
   3dba4:	cmp	x27, x8
   3dba8:	b.eq	3dbf4 <__gmpn_toom53_mul@@Base+0x494>  // b.none
   3dbac:	ldur	x11, [x29, #-176]
   3dbb0:	mov	w8, #0x28                  	// #40
   3dbb4:	lsl	x9, x24, #1
   3dbb8:	mul	x8, x24, x8
   3dbbc:	mvn	x10, x9
   3dbc0:	add	x9, x9, x24
   3dbc4:	add	x8, x8, x11, lsl #3
   3dbc8:	add	x10, x10, x11
   3dbcc:	sub	x9, x9, x11
   3dbd0:	ldur	x11, [x29, #-112]
   3dbd4:	lsl	x9, x9, #3
   3dbd8:	add	x2, x9, #0x10
   3dbdc:	add	x8, x8, x11
   3dbe0:	ldur	x11, [x29, #-24]
   3dbe4:	add	x0, x8, #0x68
   3dbe8:	add	x1, x11, x10, lsl #3
   3dbec:	bl	bed0 <memcpy@plt>
   3dbf0:	mov	x0, xzr
   3dbf4:	str	x0, [x27, x23]
   3dbf8:	mov	w3, #0x1                   	// #1
   3dbfc:	mov	x0, x20
   3dc00:	mov	x1, x22
   3dc04:	mov	x2, x21
   3dc08:	bl	c180 <__gmpn_lshift@plt>
   3dc0c:	mov	x8, x19
   3dc10:	str	x0, [x20, x23]
   3dc14:	subs	x8, x8, #0x1
   3dc18:	b.lt	3dc34 <__gmpn_toom53_mul@@Base+0x4d4>  // b.tstop
   3dc1c:	lsl	x9, x8, #3
   3dc20:	ldr	x10, [x27, x9]
   3dc24:	ldr	x9, [x20, x9]
   3dc28:	cmp	x10, x9
   3dc2c:	b.eq	3dc14 <__gmpn_toom53_mul@@Base+0x4b4>  // b.none
   3dc30:	b.ls	3dc4c <__gmpn_toom53_mul@@Base+0x4ec>  // b.plast
   3dc34:	ldur	x0, [x29, #-56]
   3dc38:	mov	x1, x27
   3dc3c:	mov	x2, x20
   3dc40:	mov	x3, x19
   3dc44:	bl	c2d0 <__gmpn_sub_n@plt>
   3dc48:	b	3dc6c <__gmpn_toom53_mul@@Base+0x50c>
   3dc4c:	ldur	x0, [x29, #-56]
   3dc50:	mov	x1, x20
   3dc54:	mov	x2, x27
   3dc58:	mov	x3, x19
   3dc5c:	bl	c2d0 <__gmpn_sub_n@plt>
   3dc60:	ldur	w8, [x29, #-28]
   3dc64:	eor	w8, w8, #0x1
   3dc68:	stur	w8, [x29, #-28]
   3dc6c:	mov	x0, x27
   3dc70:	mov	x1, x27
   3dc74:	mov	x2, x20
   3dc78:	mov	x3, x19
   3dc7c:	bl	ca70 <__gmpn_add_n@plt>
   3dc80:	ldur	x19, [x29, #-96]
   3dc84:	ldur	x2, [x29, #-24]
   3dc88:	mov	x1, x22
   3dc8c:	mov	x3, x21
   3dc90:	mov	x0, x19
   3dc94:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3dc98:	cmp	x24, x25
   3dc9c:	mov	x22, x0
   3dca0:	mov	x0, x19
   3dca4:	mov	x1, x28
   3dca8:	mov	x2, x19
   3dcac:	b.lt	3dd00 <__gmpn_toom53_mul@@Base+0x5a0>  // b.tstop
   3dcb0:	mov	x3, x25
   3dcb4:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3dcb8:	add	x28, x19, x25, lsl #3
   3dcbc:	mov	x24, x0
   3dcc0:	sub	x2, x21, x25
   3dcc4:	mov	w3, #0x1                   	// #1
   3dcc8:	mov	x0, x28
   3dccc:	mov	x1, x28
   3dcd0:	bl	c180 <__gmpn_lshift@plt>
   3dcd4:	add	x8, x0, x22, lsl #1
   3dcd8:	str	x8, [x19, x21, lsl #3]
   3dcdc:	ldr	x8, [x28]
   3dce0:	adds	x8, x8, x24
   3dce4:	str	x8, [x28]
   3dce8:	b.cc	3dd10 <__gmpn_toom53_mul@@Base+0x5b0>  // b.lo, b.ul, b.last
   3dcec:	ldr	x8, [x28, #8]!
   3dcf0:	adds	x8, x8, #0x1
   3dcf4:	str	x8, [x28]
   3dcf8:	b.cs	3dcec <__gmpn_toom53_mul@@Base+0x58c>  // b.hs, b.nlast
   3dcfc:	b	3dd10 <__gmpn_toom53_mul@@Base+0x5b0>
   3dd00:	mov	x3, x21
   3dd04:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3dd08:	add	x8, x0, x22, lsl #1
   3dd0c:	str	x8, [x19, x21, lsl #3]
   3dd10:	ldp	x25, x19, [x29, #-80]
   3dd14:	ldur	x1, [x29, #-160]
   3dd18:	mov	x2, x27
   3dd1c:	mov	x0, x19
   3dd20:	mov	x3, x25
   3dd24:	bl	c990 <__gmpn_mul_n@plt>
   3dd28:	ldp	x2, x8, [x29, #-56]
   3dd2c:	ldur	x1, [x29, #-152]
   3dd30:	mov	x3, x25
   3dd34:	lsl	x24, x8, #3
   3dd38:	add	x8, x19, x24
   3dd3c:	add	x22, x8, #0x8
   3dd40:	mov	x0, x22
   3dd44:	bl	c990 <__gmpn_mul_n@plt>
   3dd48:	ldp	x2, x8, [x29, #-96]
   3dd4c:	ldur	x1, [x29, #-144]
   3dd50:	mov	x3, x25
   3dd54:	lsl	x28, x8, #3
   3dd58:	add	x8, x19, x28
   3dd5c:	add	x27, x8, #0x10
   3dd60:	mov	x0, x27
   3dd64:	bl	c990 <__gmpn_mul_n@plt>
   3dd68:	add	x8, x21, x21, lsl #1
   3dd6c:	lsl	x25, x8, #4
   3dd70:	ldur	x1, [x29, #-136]
   3dd74:	ldur	x2, [x29, #-104]
   3dd78:	add	x8, x19, x25
   3dd7c:	add	x19, x8, #0x18
   3dd80:	str	xzr, [x19, x24]
   3dd84:	ldr	x8, [x1, x23]
   3dd88:	ldr	x9, [x2, x23]
   3dd8c:	mov	x0, x19
   3dd90:	orr	x8, x9, x8
   3dd94:	cmp	x8, #0x0
   3dd98:	cinc	x3, x21, ne  // ne = any
   3dd9c:	bl	c990 <__gmpn_mul_n@plt>
   3dda0:	ldur	x1, [x29, #-112]
   3dda4:	add	x0, x20, x24
   3dda8:	str	xzr, [x0, x24]
   3ddac:	ldr	x9, [x26, x23]
   3ddb0:	ldr	x8, [x1, x23]
   3ddb4:	mov	x2, x26
   3ddb8:	orr	x8, x9, x8
   3ddbc:	cmp	x8, #0x0
   3ddc0:	cinc	x3, x21, ne  // ne = any
   3ddc4:	bl	c990 <__gmpn_mul_n@plt>
   3ddc8:	ldur	x23, [x29, #-120]
   3ddcc:	ldur	x2, [x29, #-24]
   3ddd0:	mov	x0, x20
   3ddd4:	mov	x3, x21
   3ddd8:	mov	x1, x23
   3dddc:	bl	c990 <__gmpn_mul_n@plt>
   3dde0:	add	x0, x20, x25
   3dde4:	ldur	x24, [x29, #-128]
   3dde8:	ldur	x25, [x29, #-40]
   3ddec:	add	x3, x23, x28
   3ddf0:	cmp	x24, x25
   3ddf4:	b.le	3de0c <__gmpn_toom53_mul@@Base+0x6ac>
   3ddf8:	mov	x1, x3
   3ddfc:	ldur	x3, [x29, #-64]
   3de00:	mov	x2, x24
   3de04:	mov	x4, x25
   3de08:	b	3de18 <__gmpn_toom53_mul@@Base+0x6b8>
   3de0c:	ldur	x1, [x29, #-64]
   3de10:	mov	x2, x25
   3de14:	mov	x4, x24
   3de18:	bl	ccd0 <__gmpn_mul@plt>
   3de1c:	ldur	x5, [x29, #-72]
   3de20:	add	x7, x24, x25
   3de24:	add	x8, x5, x21, lsl #6
   3de28:	add	x8, x8, #0x20
   3de2c:	str	x8, [sp, #-16]!
   3de30:	ldur	w2, [x29, #-28]
   3de34:	mov	x0, x20
   3de38:	mov	x1, x21
   3de3c:	mov	x3, x22
   3de40:	mov	x4, x19
   3de44:	mov	x6, x27
   3de48:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   3de4c:	add	sp, sp, #0x10
   3de50:	ldur	x0, [x29, #-16]
   3de54:	cbnz	x0, 3deac <__gmpn_toom53_mul@@Base+0x74c>
   3de58:	mov	sp, x29
   3de5c:	ldp	x20, x19, [sp, #80]
   3de60:	ldp	x22, x21, [sp, #64]
   3de64:	ldp	x24, x23, [sp, #48]
   3de68:	ldp	x26, x25, [sp, #32]
   3de6c:	ldp	x28, x27, [sp, #16]
   3de70:	ldp	x29, x30, [sp], #96
   3de74:	ret
   3de78:	ldur	x25, [x29, #-104]
   3de7c:	mov	x2, x26
   3de80:	mov	x3, x21
   3de84:	mov	x0, x25
   3de88:	bl	c2d0 <__gmpn_sub_n@plt>
   3de8c:	ldur	w8, [x29, #-28]
   3de90:	str	xzr, [x25, x21, lsl #3]
   3de94:	eor	w8, w8, #0x2
   3de98:	stur	w8, [x29, #-28]
   3de9c:	b	3da88 <__gmpn_toom53_mul@@Base+0x328>
   3dea0:	sub	x0, x29, #0x10
   3dea4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   3dea8:	b	3d808 <__gmpn_toom53_mul@@Base+0xa8>
   3deac:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   3deb0:	b	3de58 <__gmpn_toom53_mul@@Base+0x6f8>

000000000003deb4 <__gmpn_toom54_mul@@Base>:
   3deb4:	sub	sp, sp, #0xc0
   3deb8:	add	x8, x4, x4, lsl #2
   3debc:	stp	x29, x30, [sp, #96]
   3dec0:	stp	x24, x23, [sp, #144]
   3dec4:	stp	x20, x19, [sp, #176]
   3dec8:	add	x29, sp, #0x60
   3decc:	mov	x24, x3
   3ded0:	mov	x19, x1
   3ded4:	cmp	x8, x2, lsl #2
   3ded8:	mov	x20, x0
   3dedc:	stp	x28, x27, [sp, #112]
   3dee0:	stp	x26, x25, [sp, #128]
   3dee4:	stp	x22, x21, [sp, #160]
   3dee8:	stur	x5, [x29, #-32]
   3deec:	b.le	3def8 <__gmpn_toom54_mul@@Base+0x44>
   3def0:	sub	x8, x4, #0x1
   3def4:	b	3df08 <__gmpn_toom54_mul@@Base+0x54>
   3def8:	mov	x9, #0xcccccccccccccccc    	// #-3689348814741910324
   3defc:	sub	x8, x2, #0x1
   3df00:	movk	x9, #0xcccd
   3df04:	umulh	x8, x8, x9
   3df08:	lsr	x23, x8, #2
   3df0c:	add	x21, x23, #0x1
   3df10:	add	x26, x21, x21, lsl #1
   3df14:	mov	w8, #0x28                  	// #40
   3df18:	lsl	x25, x26, #3
   3df1c:	lsl	x9, x21, #2
   3df20:	madd	x8, x21, x8, x20
   3df24:	add	x27, x20, x25
   3df28:	sub	x5, x2, x9
   3df2c:	sub	x28, x4, x26
   3df30:	add	x0, x8, #0x10
   3df34:	mov	w2, #0x4                   	// #4
   3df38:	mov	w6, #0x2                   	// #2
   3df3c:	mov	x1, x27
   3df40:	mov	x3, x19
   3df44:	mov	x4, x21
   3df48:	mov	x7, x20
   3df4c:	str	x9, [sp, #40]
   3df50:	stur	x5, [x29, #-40]
   3df54:	str	x28, [sp, #16]
   3df58:	stur	x0, [x29, #-24]
   3df5c:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3df60:	lsl	x8, x26, #1
   3df64:	stur	x19, [x29, #-16]
   3df68:	str	x8, [sp, #48]
   3df6c:	add	x8, x20, x26, lsl #4
   3df70:	add	x9, x20, x21, lsl #5
   3df74:	str	x26, [sp, #32]
   3df78:	add	x26, x8, #0x18
   3df7c:	add	x19, x9, #0x8
   3df80:	mov	w22, w0
   3df84:	mov	w2, #0x3                   	// #3
   3df88:	mov	w6, #0x2                   	// #2
   3df8c:	mov	x0, x26
   3df90:	mov	x1, x19
   3df94:	mov	x3, x24
   3df98:	mov	x4, x21
   3df9c:	mov	x5, x28
   3dfa0:	mov	x7, x20
   3dfa4:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3dfa8:	eor	w8, w0, w22
   3dfac:	add	x22, x23, #0x2
   3dfb0:	mov	x0, x20
   3dfb4:	mov	x1, x27
   3dfb8:	mov	x2, x19
   3dfbc:	mov	x3, x22
   3dfc0:	stur	x24, [x29, #-8]
   3dfc4:	str	w8, [sp, #12]
   3dfc8:	bl	c990 <__gmpn_mul_n@plt>
   3dfcc:	ldp	x28, x24, [x29, #-32]
   3dfd0:	mov	x2, x26
   3dfd4:	mov	x3, x22
   3dfd8:	add	x8, x28, x25
   3dfdc:	add	x25, x8, #0x8
   3dfe0:	mov	x0, x25
   3dfe4:	mov	x1, x24
   3dfe8:	bl	c990 <__gmpn_mul_n@plt>
   3dfec:	ldr	w3, [sp, #12]
   3dff0:	mov	w23, #0x1                   	// #1
   3dff4:	bfi	x23, x21, #1, #63
   3dff8:	mov	w5, #0x2                   	// #2
   3dffc:	mov	w6, #0x4                   	// #4
   3e000:	mov	x0, x25
   3e004:	mov	x1, x23
   3e008:	mov	x2, x20
   3e00c:	mov	x4, x21
   3e010:	str	x25, [sp, #24]
   3e014:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e018:	ldur	x3, [x29, #-16]
   3e01c:	ldur	x5, [x29, #-40]
   3e020:	mov	w2, #0x4                   	// #4
   3e024:	mov	x0, x24
   3e028:	mov	x1, x27
   3e02c:	mov	x4, x21
   3e030:	mov	x6, x20
   3e034:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3e038:	ldr	x25, [sp, #16]
   3e03c:	ldur	x2, [x29, #-8]
   3e040:	mov	w24, w0
   3e044:	mov	x0, x26
   3e048:	mov	x1, x19
   3e04c:	mov	x3, x21
   3e050:	mov	x4, x25
   3e054:	mov	x5, x20
   3e058:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3e05c:	eor	w8, w0, w24
   3e060:	mov	x0, x20
   3e064:	mov	x1, x27
   3e068:	mov	x2, x19
   3e06c:	mov	x3, x22
   3e070:	str	w8, [sp, #12]
   3e074:	bl	c990 <__gmpn_mul_n@plt>
   3e078:	ldur	x24, [x29, #-24]
   3e07c:	mov	x0, x28
   3e080:	mov	x2, x26
   3e084:	mov	x3, x22
   3e088:	mov	x1, x24
   3e08c:	bl	c990 <__gmpn_mul_n@plt>
   3e090:	ldr	w3, [sp, #12]
   3e094:	mov	x0, x28
   3e098:	mov	x1, x23
   3e09c:	mov	x2, x20
   3e0a0:	mov	x4, x21
   3e0a4:	mov	w5, wzr
   3e0a8:	mov	w6, wzr
   3e0ac:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e0b0:	ldur	x28, [x29, #-40]
   3e0b4:	ldur	x3, [x29, #-16]
   3e0b8:	mov	w2, #0x4                   	// #4
   3e0bc:	mov	x0, x24
   3e0c0:	mov	x1, x27
   3e0c4:	mov	x4, x21
   3e0c8:	mov	x5, x28
   3e0cc:	mov	x6, x20
   3e0d0:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3e0d4:	ldur	x2, [x29, #-8]
   3e0d8:	mov	w24, w0
   3e0dc:	mov	x0, x26
   3e0e0:	mov	x1, x19
   3e0e4:	mov	x3, x21
   3e0e8:	mov	x4, x25
   3e0ec:	mov	x5, x20
   3e0f0:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3e0f4:	eor	w24, w0, w24
   3e0f8:	mov	x0, x20
   3e0fc:	mov	x1, x27
   3e100:	mov	x2, x19
   3e104:	mov	x3, x22
   3e108:	bl	c990 <__gmpn_mul_n@plt>
   3e10c:	mov	x3, x22
   3e110:	ldp	x1, x22, [x29, #-24]
   3e114:	mov	x0, x27
   3e118:	mov	x2, x26
   3e11c:	bl	c990 <__gmpn_mul_n@plt>
   3e120:	ldur	x19, [x29, #-8]
   3e124:	mov	w5, #0x1                   	// #1
   3e128:	mov	w6, #0x2                   	// #2
   3e12c:	mov	x0, x27
   3e130:	mov	x1, x23
   3e134:	mov	x2, x20
   3e138:	mov	w3, w24
   3e13c:	mov	x4, x21
   3e140:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e144:	mov	x0, x20
   3e148:	mov	x1, x22
   3e14c:	mov	x2, x19
   3e150:	mov	x3, x21
   3e154:	bl	c990 <__gmpn_mul_n@plt>
   3e158:	mov	w8, #0x38                  	// #56
   3e15c:	cmp	x28, x25
   3e160:	madd	x0, x21, x8, x20
   3e164:	b.le	3e184 <__gmpn_toom54_mul@@Base+0x2d0>
   3e168:	ldr	x8, [sp, #40]
   3e16c:	mov	x2, x28
   3e170:	mov	x4, x25
   3e174:	add	x1, x22, x8, lsl #3
   3e178:	ldr	x8, [sp, #32]
   3e17c:	add	x3, x19, x8, lsl #3
   3e180:	b	3e19c <__gmpn_toom54_mul@@Base+0x2e8>
   3e184:	ldr	x8, [sp, #32]
   3e188:	mov	x2, x25
   3e18c:	mov	x4, x28
   3e190:	add	x1, x19, x8, lsl #3
   3e194:	ldr	x8, [sp, #40]
   3e198:	add	x3, x22, x8, lsl #3
   3e19c:	bl	ccd0 <__gmpn_mul@plt>
   3e1a0:	ldr	x8, [sp, #48]
   3e1a4:	ldur	x3, [x29, #-32]
   3e1a8:	add	x4, x28, x25
   3e1ac:	mov	x0, x20
   3e1b0:	mov	x1, x21
   3e1b4:	ldr	x2, [sp, #24]
   3e1b8:	ldp	x20, x19, [sp, #176]
   3e1bc:	ldp	x22, x21, [sp, #160]
   3e1c0:	ldp	x24, x23, [sp, #144]
   3e1c4:	ldp	x26, x25, [sp, #128]
   3e1c8:	ldp	x28, x27, [sp, #112]
   3e1cc:	ldp	x29, x30, [sp, #96]
   3e1d0:	add	x8, x3, x8, lsl #3
   3e1d4:	add	x5, x8, #0x10
   3e1d8:	add	sp, sp, #0xc0
   3e1dc:	b	c7b0 <__gmpn_toom_interpolate_8pts@plt>

000000000003e1e0 <__gmpn_toom63_mul@@Base>:
   3e1e0:	sub	sp, sp, #0xe0
   3e1e4:	lsl	x8, x4, #1
   3e1e8:	cmp	x8, x2
   3e1ec:	mov	w9, #0x6                   	// #6
   3e1f0:	mov	w10, #0x3                   	// #3
   3e1f4:	csel	x8, x4, x2, gt
   3e1f8:	csel	x9, x10, x9, gt
   3e1fc:	sub	x8, x8, #0x1
   3e200:	stp	x26, x25, [sp, #160]
   3e204:	udiv	x26, x8, x9
   3e208:	stp	x22, x21, [sp, #192]
   3e20c:	add	x21, x26, #0x1
   3e210:	stp	x29, x30, [sp, #128]
   3e214:	add	x29, sp, #0x80
   3e218:	add	x8, x21, x21, lsl #2
   3e21c:	lsl	x9, x21, #1
   3e220:	stp	x28, x27, [sp, #144]
   3e224:	stp	x24, x23, [sp, #176]
   3e228:	stp	x20, x19, [sp, #208]
   3e22c:	stur	x5, [x29, #-16]
   3e230:	mov	x20, x0
   3e234:	sub	x5, x2, x8
   3e238:	stp	x8, x9, [sp, #32]
   3e23c:	add	x8, x0, x8, lsl #3
   3e240:	add	x22, x9, x21
   3e244:	mov	x25, x3
   3e248:	mov	x3, x1
   3e24c:	sub	x19, x4, x9
   3e250:	add	x0, x8, #0x10
   3e254:	add	x1, x20, x22, lsl #3
   3e258:	mov	w2, #0x5                   	// #5
   3e25c:	mov	w6, #0x2                   	// #2
   3e260:	mov	x4, x21
   3e264:	mov	x7, x20
   3e268:	stp	x0, x1, [x29, #-48]
   3e26c:	stp	x3, x5, [x29, #-32]
   3e270:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3e274:	lsl	x23, x21, #3
   3e278:	add	x24, x25, x23
   3e27c:	mov	w28, w0
   3e280:	mov	w3, #0x2                   	// #2
   3e284:	mov	x0, x20
   3e288:	mov	x1, x24
   3e28c:	mov	x2, x21
   3e290:	bl	c180 <__gmpn_lshift@plt>
   3e294:	lsl	x8, x22, #1
   3e298:	str	x8, [sp, #16]
   3e29c:	add	x8, x20, x22, lsl #4
   3e2a0:	add	x27, x8, #0x18
   3e2a4:	str	x0, [x20, x23]
   3e2a8:	add	x1, x25, x21, lsl #4
   3e2ac:	mov	w3, #0x4                   	// #4
   3e2b0:	mov	x0, x27
   3e2b4:	mov	x2, x19
   3e2b8:	str	x22, [sp, #64]
   3e2bc:	stur	x1, [x29, #-56]
   3e2c0:	bl	c180 <__gmpn_lshift@plt>
   3e2c4:	cmp	x21, x19
   3e2c8:	str	x0, [x27, x19, lsl #3]
   3e2cc:	stur	x19, [x29, #-8]
   3e2d0:	stp	x24, x21, [sp, #48]
   3e2d4:	b.ne	3e308 <__gmpn_toom63_mul@@Base+0x128>  // b.any
   3e2d8:	mov	x0, x27
   3e2dc:	mov	x1, x27
   3e2e0:	mov	x2, x25
   3e2e4:	mov	x3, x21
   3e2e8:	bl	ca70 <__gmpn_add_n@plt>
   3e2ec:	ldr	x8, [x27, x23]
   3e2f0:	add	x9, x20, x21, lsl #5
   3e2f4:	add	x19, x9, #0x8
   3e2f8:	add	x22, x26, #0x2
   3e2fc:	add	x8, x8, x0
   3e300:	str	x8, [x27, x23]
   3e304:	b	3e428 <__gmpn_toom63_mul@@Base+0x248>
   3e308:	adds	x19, x19, #0x1
   3e30c:	b.cc	3e318 <__gmpn_toom63_mul@@Base+0x138>  // b.lo, b.ul, b.last
   3e310:	mov	x19, xzr
   3e314:	b	3e354 <__gmpn_toom63_mul@@Base+0x174>
   3e318:	mov	x0, x27
   3e31c:	mov	x1, x25
   3e320:	mov	x2, x27
   3e324:	mov	x3, x19
   3e328:	bl	ca70 <__gmpn_add_n@plt>
   3e32c:	cbz	x0, 3e354 <__gmpn_toom63_mul@@Base+0x174>
   3e330:	mov	w8, #0x1                   	// #1
   3e334:	cmp	x19, x26
   3e338:	b.gt	3e410 <__gmpn_toom63_mul@@Base+0x230>
   3e33c:	lsl	x9, x19, #3
   3e340:	ldr	x10, [x25, x9]
   3e344:	add	x19, x19, #0x1
   3e348:	adds	x10, x10, #0x1
   3e34c:	str	x10, [x27, x9]
   3e350:	b.cs	3e334 <__gmpn_toom63_mul@@Base+0x154>  // b.hs, b.nlast
   3e354:	cmp	x27, x25
   3e358:	mov	x8, xzr
   3e35c:	b.eq	3e410 <__gmpn_toom63_mul@@Base+0x230>  // b.none
   3e360:	cmp	x19, x26
   3e364:	b.gt	3e410 <__gmpn_toom63_mul@@Base+0x230>
   3e368:	sub	x8, x26, x19
   3e36c:	add	x8, x8, #0x1
   3e370:	cmp	x8, #0x4
   3e374:	b.cs	3e380 <__gmpn_toom63_mul@@Base+0x1a0>  // b.hs, b.nlast
   3e378:	mov	x9, x19
   3e37c:	b	3e3f4 <__gmpn_toom63_mul@@Base+0x214>
   3e380:	mov	w9, #0x6                   	// #6
   3e384:	madd	x9, x26, x9, x19
   3e388:	add	x9, x20, x9, lsl #3
   3e38c:	add	x9, x9, #0x48
   3e390:	add	x10, x25, x21, lsl #3
   3e394:	cmp	x9, x10
   3e398:	add	x12, x25, x19, lsl #3
   3e39c:	b.cs	3e3bc <__gmpn_toom63_mul@@Base+0x1dc>  // b.hs, b.nlast
   3e3a0:	mov	w9, #0x38                  	// #56
   3e3a4:	madd	x9, x26, x9, x20
   3e3a8:	add	x9, x9, #0x50
   3e3ac:	cmp	x12, x9
   3e3b0:	b.cs	3e3bc <__gmpn_toom63_mul@@Base+0x1dc>  // b.hs, b.nlast
   3e3b4:	mov	x9, x19
   3e3b8:	b	3e3f4 <__gmpn_toom63_mul@@Base+0x214>
   3e3bc:	and	x10, x8, #0xfffffffffffffffc
   3e3c0:	mov	x11, xzr
   3e3c4:	add	x9, x19, x10
   3e3c8:	add	x12, x12, #0x10
   3e3cc:	ldp	q0, q1, [x12, #-16]
   3e3d0:	add	x13, x19, x11
   3e3d4:	add	x11, x11, #0x4
   3e3d8:	add	x13, x27, x13, lsl #3
   3e3dc:	cmp	x11, x10
   3e3e0:	add	x12, x12, #0x20
   3e3e4:	stp	q0, q1, [x13]
   3e3e8:	b.ne	3e3cc <__gmpn_toom63_mul@@Base+0x1ec>  // b.any
   3e3ec:	cmp	x8, x10
   3e3f0:	b.eq	3e40c <__gmpn_toom63_mul@@Base+0x22c>  // b.none
   3e3f4:	lsl	x8, x9, #3
   3e3f8:	ldr	x10, [x25, x8]
   3e3fc:	cmp	x9, x26
   3e400:	add	x9, x9, #0x1
   3e404:	str	x10, [x27, x8]
   3e408:	b.ne	3e3f4 <__gmpn_toom63_mul@@Base+0x214>  // b.any
   3e40c:	mov	x8, xzr
   3e410:	str	x8, [x27, x21, lsl #3]
   3e414:	add	x8, x20, x21, lsl #5
   3e418:	add	x22, x26, #0x2
   3e41c:	cmp	x22, #0x1
   3e420:	add	x19, x8, #0x8
   3e424:	b.lt	3e620 <__gmpn_toom63_mul@@Base+0x440>  // b.tstop
   3e428:	sub	x8, x22, #0x1
   3e42c:	mov	x9, x22
   3e430:	sub	x9, x9, #0x1
   3e434:	ldr	x10, [x27, x9, lsl #3]
   3e438:	ldr	x11, [x20, x8, lsl #3]
   3e43c:	cmp	x10, x11
   3e440:	b.ne	3e460 <__gmpn_toom63_mul@@Base+0x280>  // b.any
   3e444:	add	x10, x8, #0x1
   3e448:	sub	x11, x8, #0x1
   3e44c:	cmp	x10, #0x1
   3e450:	str	xzr, [x19, x8, lsl #3]
   3e454:	mov	x8, x11
   3e458:	b.gt	3e430 <__gmpn_toom63_mul@@Base+0x250>
   3e45c:	b	3e478 <__gmpn_toom63_mul@@Base+0x298>
   3e460:	add	x3, x8, #0x1
   3e464:	mov	x0, x19
   3e468:	b.ls	3e480 <__gmpn_toom63_mul@@Base+0x2a0>  // b.plast
   3e46c:	mov	x1, x27
   3e470:	mov	x2, x20
   3e474:	bl	c2d0 <__gmpn_sub_n@plt>
   3e478:	mov	w23, wzr
   3e47c:	b	3e490 <__gmpn_toom63_mul@@Base+0x2b0>
   3e480:	mov	x1, x20
   3e484:	mov	x2, x27
   3e488:	bl	c2d0 <__gmpn_sub_n@plt>
   3e48c:	mov	w23, #0xffffffff            	// #-1
   3e490:	mov	w8, #0x1                   	// #1
   3e494:	str	w8, [sp, #8]
   3e498:	mov	x0, x27
   3e49c:	mov	x1, x27
   3e4a0:	mov	x2, x20
   3e4a4:	mov	x3, x22
   3e4a8:	bl	ca70 <__gmpn_add_n@plt>
   3e4ac:	eor	w8, w23, w28
   3e4b0:	str	w8, [sp, #12]
   3e4b4:	ldur	x24, [x29, #-40]
   3e4b8:	mov	x0, x20
   3e4bc:	mov	x2, x19
   3e4c0:	mov	x3, x22
   3e4c4:	mov	x1, x24
   3e4c8:	bl	c990 <__gmpn_mul_n@plt>
   3e4cc:	ldur	x23, [x29, #-16]
   3e4d0:	ldr	x8, [sp, #64]
   3e4d4:	ldur	x21, [x29, #-48]
   3e4d8:	mov	x2, x27
   3e4dc:	mov	x3, x22
   3e4e0:	add	x8, x23, x8, lsl #3
   3e4e4:	add	x28, x8, #0x8
   3e4e8:	mov	x0, x28
   3e4ec:	mov	x1, x21
   3e4f0:	str	x22, [sp, #64]
   3e4f4:	bl	c990 <__gmpn_mul_n@plt>
   3e4f8:	ldr	x8, [sp, #40]
   3e4fc:	ldr	x22, [sp, #56]
   3e500:	ldr	w3, [sp, #12]
   3e504:	mov	w5, #0x2                   	// #2
   3e508:	orr	x1, x8, #0x1
   3e50c:	mov	w6, #0x4                   	// #4
   3e510:	mov	x0, x28
   3e514:	mov	x2, x20
   3e518:	mov	x4, x22
   3e51c:	str	x28, [sp, #24]
   3e520:	str	x1, [sp, #40]
   3e524:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e528:	ldp	x3, x5, [x29, #-32]
   3e52c:	mov	w2, #0x5                   	// #5
   3e530:	mov	x0, x21
   3e534:	mov	x1, x24
   3e538:	mov	x4, x22
   3e53c:	mov	x6, x20
   3e540:	mov	x21, x22
   3e544:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3e548:	ldr	x8, [sp, #16]
   3e54c:	mov	w24, w0
   3e550:	add	x8, x23, x8, lsl #3
   3e554:	ldur	x23, [x29, #-8]
   3e558:	add	x28, x8, #0x10
   3e55c:	cbz	x23, 3e5ac <__gmpn_toom63_mul@@Base+0x3cc>
   3e560:	ldur	x2, [x29, #-56]
   3e564:	mov	x0, x28
   3e568:	mov	x1, x25
   3e56c:	mov	x3, x23
   3e570:	bl	ca70 <__gmpn_add_n@plt>
   3e574:	ldr	x22, [sp, #48]
   3e578:	mov	x8, x23
   3e57c:	cbz	x0, 3e5b4 <__gmpn_toom63_mul@@Base+0x3d4>
   3e580:	ldur	x8, [x29, #-8]
   3e584:	mov	w23, #0x1                   	// #1
   3e588:	cmp	x8, x26
   3e58c:	b.gt	3e680 <__gmpn_toom63_mul@@Base+0x4a0>
   3e590:	lsl	x9, x8, #3
   3e594:	ldr	x10, [x25, x9]
   3e598:	add	x8, x8, #0x1
   3e59c:	adds	x10, x10, #0x1
   3e5a0:	str	x10, [x28, x9]
   3e5a4:	b.cs	3e588 <__gmpn_toom63_mul@@Base+0x3a8>  // b.hs, b.nlast
   3e5a8:	b	3e5b4 <__gmpn_toom63_mul@@Base+0x3d4>
   3e5ac:	ldr	x22, [sp, #48]
   3e5b0:	mov	x8, xzr
   3e5b4:	cmp	x28, x25
   3e5b8:	mov	x23, xzr
   3e5bc:	b.eq	3e680 <__gmpn_toom63_mul@@Base+0x4a0>  // b.none
   3e5c0:	cmp	x8, x26
   3e5c4:	b.gt	3e680 <__gmpn_toom63_mul@@Base+0x4a0>
   3e5c8:	sub	x9, x26, x8
   3e5cc:	add	x9, x9, #0x1
   3e5d0:	cmp	x9, #0x4
   3e5d4:	b.cs	3e5e0 <__gmpn_toom63_mul@@Base+0x400>  // b.hs, b.nlast
   3e5d8:	mov	x10, x8
   3e5dc:	b	3e664 <__gmpn_toom63_mul@@Base+0x484>
   3e5e0:	ldur	x12, [x29, #-16]
   3e5e4:	mov	w10, #0x6                   	// #6
   3e5e8:	madd	x10, x26, x10, x8
   3e5ec:	add	x11, x25, x21, lsl #3
   3e5f0:	add	x10, x12, x10, lsl #3
   3e5f4:	add	x10, x10, #0x40
   3e5f8:	cmp	x10, x11
   3e5fc:	add	x13, x25, x8, lsl #3
   3e600:	b.cs	3e62c <__gmpn_toom63_mul@@Base+0x44c>  // b.hs, b.nlast
   3e604:	mov	w10, #0x38                  	// #56
   3e608:	madd	x10, x26, x10, x12
   3e60c:	add	x10, x10, #0x48
   3e610:	cmp	x13, x10
   3e614:	b.cs	3e62c <__gmpn_toom63_mul@@Base+0x44c>  // b.hs, b.nlast
   3e618:	mov	x10, x8
   3e61c:	b	3e664 <__gmpn_toom63_mul@@Base+0x484>
   3e620:	str	wzr, [sp, #8]
   3e624:	mov	w23, wzr
   3e628:	b	3e498 <__gmpn_toom63_mul@@Base+0x2b8>
   3e62c:	and	x11, x9, #0xfffffffffffffffc
   3e630:	mov	x12, xzr
   3e634:	add	x10, x8, x11
   3e638:	add	x13, x13, #0x10
   3e63c:	ldp	q0, q1, [x13, #-16]
   3e640:	add	x14, x8, x12
   3e644:	add	x12, x12, #0x4
   3e648:	add	x14, x28, x14, lsl #3
   3e64c:	cmp	x12, x11
   3e650:	add	x13, x13, #0x20
   3e654:	stp	q0, q1, [x14]
   3e658:	b.ne	3e63c <__gmpn_toom63_mul@@Base+0x45c>  // b.any
   3e65c:	cmp	x9, x11
   3e660:	b.eq	3e67c <__gmpn_toom63_mul@@Base+0x49c>  // b.none
   3e664:	lsl	x8, x10, #3
   3e668:	ldr	x9, [x25, x8]
   3e66c:	cmp	x10, x26
   3e670:	add	x10, x10, #0x1
   3e674:	str	x9, [x28, x8]
   3e678:	b.ne	3e664 <__gmpn_toom63_mul@@Base+0x484>  // b.any
   3e67c:	mov	x23, xzr
   3e680:	mov	x0, x27
   3e684:	mov	x1, x28
   3e688:	mov	x2, x22
   3e68c:	mov	x3, x21
   3e690:	bl	ca70 <__gmpn_add_n@plt>
   3e694:	add	x8, x0, x23
   3e698:	str	x8, [x27, x21, lsl #3]
   3e69c:	cbz	x23, 3e7c4 <__gmpn_toom63_mul@@Base+0x5e4>
   3e6a0:	mov	x0, x19
   3e6a4:	mov	x1, x28
   3e6a8:	mov	x2, x22
   3e6ac:	mov	x3, x21
   3e6b0:	str	w24, [sp, #12]
   3e6b4:	bl	c2d0 <__gmpn_sub_n@plt>
   3e6b8:	sub	x8, x23, x0
   3e6bc:	str	x8, [x19, x21, lsl #3]
   3e6c0:	mov	x23, x21
   3e6c4:	ldur	x24, [x29, #-40]
   3e6c8:	ldr	x21, [sp, #64]
   3e6cc:	mov	x0, x20
   3e6d0:	mov	x2, x19
   3e6d4:	mov	x1, x24
   3e6d8:	mov	x3, x21
   3e6dc:	str	x19, [sp, #16]
   3e6e0:	bl	c990 <__gmpn_mul_n@plt>
   3e6e4:	ldur	x19, [x29, #-16]
   3e6e8:	ldur	x22, [x29, #-48]
   3e6ec:	mov	x2, x27
   3e6f0:	mov	x3, x21
   3e6f4:	mov	x0, x19
   3e6f8:	mov	x1, x22
   3e6fc:	bl	c990 <__gmpn_mul_n@plt>
   3e700:	ldr	x1, [sp, #40]
   3e704:	ldr	w3, [sp, #12]
   3e708:	mov	x0, x19
   3e70c:	mov	x2, x20
   3e710:	mov	x4, x23
   3e714:	mov	w5, wzr
   3e718:	mov	w6, wzr
   3e71c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e720:	ldp	x3, x5, [x29, #-32]
   3e724:	mov	w2, #0x5                   	// #5
   3e728:	mov	x0, x22
   3e72c:	mov	x1, x24
   3e730:	mov	x4, x23
   3e734:	mov	x6, x20
   3e738:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3e73c:	ldr	x1, [sp, #48]
   3e740:	str	w0, [sp, #12]
   3e744:	mov	w3, #0x1                   	// #1
   3e748:	mov	x0, x20
   3e74c:	mov	x2, x23
   3e750:	bl	c180 <__gmpn_lshift@plt>
   3e754:	ldur	x22, [x29, #-8]
   3e758:	ldur	x1, [x29, #-56]
   3e75c:	str	x0, [x20, x23, lsl #3]
   3e760:	mov	w3, #0x2                   	// #2
   3e764:	mov	x0, x27
   3e768:	mov	x2, x22
   3e76c:	bl	c180 <__gmpn_lshift@plt>
   3e770:	cmp	x23, x22
   3e774:	str	x0, [x27, x22, lsl #3]
   3e778:	b.ne	3e7a8 <__gmpn_toom63_mul@@Base+0x5c8>  // b.any
   3e77c:	mov	x0, x27
   3e780:	mov	x1, x27
   3e784:	mov	x2, x25
   3e788:	mov	x3, x23
   3e78c:	bl	ca70 <__gmpn_add_n@plt>
   3e790:	ldr	x8, [x27, x23, lsl #3]
   3e794:	ldr	x24, [sp, #64]
   3e798:	ldr	w14, [sp, #8]
   3e79c:	ldr	x19, [sp, #16]
   3e7a0:	add	x8, x8, x0
   3e7a4:	b	3e910 <__gmpn_toom63_mul@@Base+0x730>
   3e7a8:	ldr	x24, [sp, #64]
   3e7ac:	ldr	x19, [sp, #16]
   3e7b0:	adds	x22, x22, #0x1
   3e7b4:	b.cc	3e80c <__gmpn_toom63_mul@@Base+0x62c>  // b.lo, b.ul, b.last
   3e7b8:	ldr	w14, [sp, #8]
   3e7bc:	mov	x22, xzr
   3e7c0:	b	3e854 <__gmpn_toom63_mul@@Base+0x674>
   3e7c4:	mov	x8, x21
   3e7c8:	subs	x8, x8, #0x1
   3e7cc:	b.lt	3e6a0 <__gmpn_toom63_mul@@Base+0x4c0>  // b.tstop
   3e7d0:	lsl	x9, x8, #3
   3e7d4:	ldr	x10, [x28, x9]
   3e7d8:	ldr	x9, [x22, x9]
   3e7dc:	cmp	x10, x9
   3e7e0:	b.eq	3e7c8 <__gmpn_toom63_mul@@Base+0x5e8>  // b.none
   3e7e4:	b.hi	3e6a0 <__gmpn_toom63_mul@@Base+0x4c0>  // b.pmore
   3e7e8:	mov	x0, x19
   3e7ec:	mov	x1, x22
   3e7f0:	mov	x2, x28
   3e7f4:	mov	x3, x21
   3e7f8:	bl	c2d0 <__gmpn_sub_n@plt>
   3e7fc:	mvn	w24, w24
   3e800:	str	xzr, [x19, x21, lsl #3]
   3e804:	str	w24, [sp, #12]
   3e808:	b	3e6c0 <__gmpn_toom63_mul@@Base+0x4e0>
   3e80c:	mov	x0, x27
   3e810:	mov	x1, x25
   3e814:	mov	x2, x27
   3e818:	mov	x3, x22
   3e81c:	bl	ca70 <__gmpn_add_n@plt>
   3e820:	cbz	x0, 3e850 <__gmpn_toom63_mul@@Base+0x670>
   3e824:	ldr	w14, [sp, #8]
   3e828:	mov	w8, #0x1                   	// #1
   3e82c:	cmp	x22, x26
   3e830:	b.gt	3e910 <__gmpn_toom63_mul@@Base+0x730>
   3e834:	lsl	x9, x22, #3
   3e838:	ldr	x10, [x25, x9]
   3e83c:	add	x22, x22, #0x1
   3e840:	adds	x10, x10, #0x1
   3e844:	str	x10, [x27, x9]
   3e848:	b.cs	3e82c <__gmpn_toom63_mul@@Base+0x64c>  // b.hs, b.nlast
   3e84c:	b	3e854 <__gmpn_toom63_mul@@Base+0x674>
   3e850:	ldr	w14, [sp, #8]
   3e854:	cmp	x27, x25
   3e858:	mov	x8, xzr
   3e85c:	b.eq	3e910 <__gmpn_toom63_mul@@Base+0x730>  // b.none
   3e860:	cmp	x22, x26
   3e864:	b.gt	3e910 <__gmpn_toom63_mul@@Base+0x730>
   3e868:	sub	x8, x26, x22
   3e86c:	add	x8, x8, #0x1
   3e870:	cmp	x8, #0x4
   3e874:	b.cs	3e880 <__gmpn_toom63_mul@@Base+0x6a0>  // b.hs, b.nlast
   3e878:	mov	x9, x22
   3e87c:	b	3e8f4 <__gmpn_toom63_mul@@Base+0x714>
   3e880:	mov	w9, #0x6                   	// #6
   3e884:	madd	x9, x26, x9, x22
   3e888:	add	x9, x20, x9, lsl #3
   3e88c:	add	x9, x9, #0x48
   3e890:	add	x10, x25, x23, lsl #3
   3e894:	cmp	x9, x10
   3e898:	add	x12, x25, x22, lsl #3
   3e89c:	b.cs	3e8bc <__gmpn_toom63_mul@@Base+0x6dc>  // b.hs, b.nlast
   3e8a0:	mov	w9, #0x38                  	// #56
   3e8a4:	madd	x9, x26, x9, x20
   3e8a8:	add	x9, x9, #0x50
   3e8ac:	cmp	x12, x9
   3e8b0:	b.cs	3e8bc <__gmpn_toom63_mul@@Base+0x6dc>  // b.hs, b.nlast
   3e8b4:	mov	x9, x22
   3e8b8:	b	3e8f4 <__gmpn_toom63_mul@@Base+0x714>
   3e8bc:	and	x10, x8, #0xfffffffffffffffc
   3e8c0:	mov	x11, xzr
   3e8c4:	add	x9, x22, x10
   3e8c8:	add	x12, x12, #0x10
   3e8cc:	ldp	q0, q1, [x12, #-16]
   3e8d0:	add	x13, x22, x11
   3e8d4:	add	x11, x11, #0x4
   3e8d8:	add	x13, x27, x13, lsl #3
   3e8dc:	cmp	x11, x10
   3e8e0:	add	x12, x12, #0x20
   3e8e4:	stp	q0, q1, [x13]
   3e8e8:	b.ne	3e8cc <__gmpn_toom63_mul@@Base+0x6ec>  // b.any
   3e8ec:	cmp	x8, x10
   3e8f0:	b.eq	3e90c <__gmpn_toom63_mul@@Base+0x72c>  // b.none
   3e8f4:	lsl	x8, x9, #3
   3e8f8:	ldr	x10, [x25, x8]
   3e8fc:	cmp	x9, x26
   3e900:	add	x9, x9, #0x1
   3e904:	str	x10, [x27, x8]
   3e908:	b.ne	3e8f4 <__gmpn_toom63_mul@@Base+0x714>  // b.any
   3e90c:	mov	x8, xzr
   3e910:	str	x8, [x27, x23, lsl #3]
   3e914:	cbz	w14, 3e968 <__gmpn_toom63_mul@@Base+0x788>
   3e918:	sub	x8, x24, #0x1
   3e91c:	mov	x9, x24
   3e920:	sub	x9, x9, #0x1
   3e924:	ldr	x10, [x27, x9, lsl #3]
   3e928:	ldr	x11, [x20, x8, lsl #3]
   3e92c:	cmp	x10, x11
   3e930:	b.ne	3e950 <__gmpn_toom63_mul@@Base+0x770>  // b.any
   3e934:	add	x10, x8, #0x1
   3e938:	sub	x11, x8, #0x1
   3e93c:	cmp	x10, #0x1
   3e940:	str	xzr, [x19, x8, lsl #3]
   3e944:	mov	x8, x11
   3e948:	b.gt	3e920 <__gmpn_toom63_mul@@Base+0x740>
   3e94c:	b	3e968 <__gmpn_toom63_mul@@Base+0x788>
   3e950:	add	x3, x8, #0x1
   3e954:	mov	x0, x19
   3e958:	b.ls	3ea70 <__gmpn_toom63_mul@@Base+0x890>  // b.plast
   3e95c:	mov	x1, x27
   3e960:	mov	x2, x20
   3e964:	bl	c2d0 <__gmpn_sub_n@plt>
   3e968:	mov	w22, wzr
   3e96c:	mov	x0, x27
   3e970:	mov	x1, x27
   3e974:	mov	x2, x20
   3e978:	mov	x3, x24
   3e97c:	bl	ca70 <__gmpn_add_n@plt>
   3e980:	mov	x21, x24
   3e984:	ldur	x24, [x29, #-40]
   3e988:	ldr	w8, [sp, #12]
   3e98c:	mov	x0, x20
   3e990:	mov	x2, x19
   3e994:	mov	x1, x24
   3e998:	mov	x3, x21
   3e99c:	eor	w22, w22, w8
   3e9a0:	bl	c990 <__gmpn_mul_n@plt>
   3e9a4:	ldur	x1, [x29, #-48]
   3e9a8:	mov	x0, x24
   3e9ac:	mov	x2, x27
   3e9b0:	mov	x3, x21
   3e9b4:	bl	c990 <__gmpn_mul_n@plt>
   3e9b8:	ldr	x21, [sp, #56]
   3e9bc:	ldr	x1, [sp, #40]
   3e9c0:	mov	w5, #0x1                   	// #1
   3e9c4:	mov	w6, #0x2                   	// #2
   3e9c8:	mov	x0, x24
   3e9cc:	mov	x2, x20
   3e9d0:	mov	w3, w22
   3e9d4:	mov	x4, x21
   3e9d8:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3e9dc:	ldur	x24, [x29, #-32]
   3e9e0:	mov	x0, x20
   3e9e4:	mov	x2, x25
   3e9e8:	mov	x3, x21
   3e9ec:	mov	x1, x24
   3e9f0:	bl	c990 <__gmpn_mul_n@plt>
   3e9f4:	mov	w8, #0x38                  	// #56
   3e9f8:	ldur	x19, [x29, #-24]
   3e9fc:	ldur	x22, [x29, #-8]
   3ea00:	madd	x0, x21, x8, x20
   3ea04:	ldr	x8, [sp, #32]
   3ea08:	cmp	x19, x22
   3ea0c:	add	x3, x24, x8, lsl #3
   3ea10:	b.le	3ea28 <__gmpn_toom63_mul@@Base+0x848>
   3ea14:	mov	x1, x3
   3ea18:	ldur	x3, [x29, #-56]
   3ea1c:	mov	x2, x19
   3ea20:	mov	x4, x22
   3ea24:	b	3ea34 <__gmpn_toom63_mul@@Base+0x854>
   3ea28:	ldur	x1, [x29, #-56]
   3ea2c:	mov	x2, x22
   3ea30:	mov	x4, x19
   3ea34:	bl	ccd0 <__gmpn_mul@plt>
   3ea38:	add	x4, x19, x22
   3ea3c:	mov	x0, x20
   3ea40:	mov	x1, x21
   3ea44:	ldr	x2, [sp, #24]
   3ea48:	ldur	x3, [x29, #-16]
   3ea4c:	mov	x5, x28
   3ea50:	ldp	x20, x19, [sp, #208]
   3ea54:	ldp	x22, x21, [sp, #192]
   3ea58:	ldp	x24, x23, [sp, #176]
   3ea5c:	ldp	x26, x25, [sp, #160]
   3ea60:	ldp	x28, x27, [sp, #144]
   3ea64:	ldp	x29, x30, [sp, #128]
   3ea68:	add	sp, sp, #0xe0
   3ea6c:	b	c7b0 <__gmpn_toom_interpolate_8pts@plt>
   3ea70:	mov	x1, x20
   3ea74:	mov	x2, x27
   3ea78:	bl	c2d0 <__gmpn_sub_n@plt>
   3ea7c:	mov	w22, #0xffffffff            	// #-1
   3ea80:	b	3e96c <__gmpn_toom63_mul@@Base+0x78c>

000000000003ea84 <__gmpn_toom44_mul@@Base>:
   3ea84:	sub	sp, sp, #0xf0
   3ea88:	stp	x26, x25, [sp, #176]
   3ea8c:	mov	x25, x2
   3ea90:	stp	x20, x19, [sp, #224]
   3ea94:	add	x19, x25, #0x3
   3ea98:	stp	x22, x21, [sp, #208]
   3ea9c:	asr	x21, x19, #2
   3eaa0:	lsl	x10, x21, #1
   3eaa4:	add	x8, x0, x21, lsl #3
   3eaa8:	add	x9, x5, x21, lsl #6
   3eaac:	stp	x4, x10, [sp, #64]
   3eab0:	add	x10, x10, x21
   3eab4:	stp	x28, x27, [sp, #160]
   3eab8:	add	x22, x8, #0x8
   3eabc:	sub	x8, x25, x10
   3eac0:	add	x28, x9, #0x28
   3eac4:	stp	x29, x30, [sp, #144]
   3eac8:	stp	x24, x23, [sp, #192]
   3eacc:	add	x29, sp, #0x90
   3ead0:	mov	x23, x5
   3ead4:	mov	x26, x3
   3ead8:	mov	x2, x1
   3eadc:	sub	x24, x4, x10
   3eae0:	mov	x1, x22
   3eae4:	mov	x3, x21
   3eae8:	mov	x4, x8
   3eaec:	mov	x5, x28
   3eaf0:	mov	x20, x0
   3eaf4:	stur	x10, [x29, #-8]
   3eaf8:	stur	x2, [x29, #-48]
   3eafc:	stur	x8, [x29, #-24]
   3eb00:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3eb04:	and	x8, x19, #0xfffffffffffffffc
   3eb08:	add	x9, x20, x21, lsl #4
   3eb0c:	str	x8, [sp, #24]
   3eb10:	add	x8, x20, x8, lsl #3
   3eb14:	add	x27, x8, #0x10
   3eb18:	add	x19, x9, #0x10
   3eb1c:	str	w0, [sp, #20]
   3eb20:	mov	x0, x27
   3eb24:	mov	x1, x19
   3eb28:	mov	x2, x26
   3eb2c:	mov	x3, x21
   3eb30:	mov	x4, x24
   3eb34:	mov	x5, x28
   3eb38:	str	x9, [sp, #40]
   3eb3c:	stp	x24, x26, [x29, #-40]
   3eb40:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   3eb44:	add	x24, x21, #0x1
   3eb48:	str	w0, [sp, #16]
   3eb4c:	cmp	x25, #0xbc
   3eb50:	mov	x0, x23
   3eb54:	mov	x1, x20
   3eb58:	mov	x2, x24
   3eb5c:	mov	x3, x27
   3eb60:	mov	x4, x24
   3eb64:	mov	x5, x28
   3eb68:	stp	x24, x25, [x29, #-64]
   3eb6c:	stur	x23, [x29, #-16]
   3eb70:	str	x22, [sp, #48]
   3eb74:	str	x19, [sp, #32]
   3eb78:	b.le	3ebac <__gmpn_toom44_mul@@Base+0x128>
   3eb7c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3eb80:	ldr	x8, [sp, #72]
   3eb84:	mov	x1, x22
   3eb88:	mov	x2, x24
   3eb8c:	mov	x3, x19
   3eb90:	add	x8, x23, x8, lsl #3
   3eb94:	add	x0, x8, #0x8
   3eb98:	mov	x4, x24
   3eb9c:	mov	x5, x28
   3eba0:	str	x0, [sp, #56]
   3eba4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3eba8:	b	3ebd8 <__gmpn_toom44_mul@@Base+0x154>
   3ebac:	bl	d450 <__gmpn_toom22_mul@plt>
   3ebb0:	ldr	x8, [sp, #72]
   3ebb4:	mov	x1, x22
   3ebb8:	mov	x2, x24
   3ebbc:	mov	x3, x19
   3ebc0:	add	x8, x23, x8, lsl #3
   3ebc4:	add	x0, x8, #0x8
   3ebc8:	mov	x4, x24
   3ebcc:	mov	x5, x28
   3ebd0:	str	x0, [sp, #56]
   3ebd4:	bl	d450 <__gmpn_toom22_mul@plt>
   3ebd8:	ldur	x25, [x29, #-48]
   3ebdc:	mov	x0, x20
   3ebe0:	mov	x3, x21
   3ebe4:	mov	x22, x28
   3ebe8:	add	x1, x25, x21, lsl #3
   3ebec:	mov	x2, x25
   3ebf0:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ebf4:	ldr	x19, [sp, #72]
   3ebf8:	mov	x23, x0
   3ebfc:	mov	x0, x20
   3ec00:	mov	x2, x20
   3ec04:	add	x1, x25, x19, lsl #3
   3ec08:	mov	x3, x21
   3ec0c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ec10:	ldur	x28, [x29, #-24]
   3ec14:	add	x26, x0, x23, lsl #1
   3ec18:	subs	x24, x21, x28
   3ec1c:	b.le	3ec88 <__gmpn_toom44_mul@@Base+0x204>
   3ec20:	ldur	x8, [x29, #-8]
   3ec24:	mov	x0, x20
   3ec28:	mov	x2, x20
   3ec2c:	mov	x3, x28
   3ec30:	add	x1, x25, x8, lsl #3
   3ec34:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ec38:	mov	x8, x28
   3ec3c:	add	x23, x20, x8, lsl #3
   3ec40:	mov	x28, x0
   3ec44:	mov	w3, #0x1                   	// #1
   3ec48:	mov	x0, x23
   3ec4c:	mov	x1, x23
   3ec50:	mov	x2, x24
   3ec54:	bl	c180 <__gmpn_lshift@plt>
   3ec58:	add	x8, x0, x26, lsl #1
   3ec5c:	str	x8, [x20, x21, lsl #3]
   3ec60:	ldr	x8, [x23]
   3ec64:	adds	x8, x8, x28
   3ec68:	str	x8, [x23]
   3ec6c:	b.cc	3eca8 <__gmpn_toom44_mul@@Base+0x224>  // b.lo, b.ul, b.last
   3ec70:	add	x8, x23, #0x8
   3ec74:	ldr	x9, [x8]
   3ec78:	adds	x9, x9, #0x1
   3ec7c:	str	x9, [x8], #8
   3ec80:	b.cs	3ec74 <__gmpn_toom44_mul@@Base+0x1f0>  // b.hs, b.nlast
   3ec84:	b	3eca8 <__gmpn_toom44_mul@@Base+0x224>
   3ec88:	ldur	x8, [x29, #-8]
   3ec8c:	mov	x0, x20
   3ec90:	mov	x2, x20
   3ec94:	mov	x3, x21
   3ec98:	add	x1, x25, x8, lsl #3
   3ec9c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3eca0:	add	x8, x0, x26, lsl #1
   3eca4:	str	x8, [x20, x21, lsl #3]
   3eca8:	ldp	w9, w8, [sp, #16]
   3ecac:	ldur	x26, [x29, #-32]
   3ecb0:	mov	x0, x27
   3ecb4:	mov	x3, x21
   3ecb8:	eor	w8, w9, w8
   3ecbc:	add	x1, x26, x21, lsl #3
   3ecc0:	mov	x2, x26
   3ecc4:	str	w8, [sp, #20]
   3ecc8:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3eccc:	mov	x23, x0
   3ecd0:	add	x1, x26, x19, lsl #3
   3ecd4:	mov	x0, x27
   3ecd8:	mov	x2, x27
   3ecdc:	mov	x3, x21
   3ece0:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ece4:	ldur	x28, [x29, #-40]
   3ece8:	add	x25, x0, x23, lsl #1
   3ecec:	subs	x24, x21, x28
   3ecf0:	b.le	3ed80 <__gmpn_toom44_mul@@Base+0x2fc>
   3ecf4:	ldur	x8, [x29, #-8]
   3ecf8:	mov	x0, x27
   3ecfc:	mov	x2, x27
   3ed00:	mov	x3, x28
   3ed04:	add	x1, x26, x8, lsl #3
   3ed08:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ed0c:	ldur	x8, [x29, #-40]
   3ed10:	mov	x23, x0
   3ed14:	mov	w3, #0x1                   	// #1
   3ed18:	mov	x2, x24
   3ed1c:	add	x28, x27, x8, lsl #3
   3ed20:	mov	x0, x28
   3ed24:	mov	x1, x28
   3ed28:	bl	c180 <__gmpn_lshift@plt>
   3ed2c:	add	x8, x0, x25, lsl #1
   3ed30:	str	x8, [x27, x21, lsl #3]
   3ed34:	ldr	x8, [x28]
   3ed38:	ldur	x19, [x29, #-24]
   3ed3c:	ldr	x25, [sp, #48]
   3ed40:	ldr	x10, [sp, #24]
   3ed44:	adds	x8, x8, x23
   3ed48:	str	x8, [x28]
   3ed4c:	ldur	x28, [x29, #-40]
   3ed50:	b.cc	3edac <__gmpn_toom44_mul@@Base+0x328>  // b.lo, b.ul, b.last
   3ed54:	ldr	x8, [sp, #64]
   3ed58:	ldur	x9, [x29, #-8]
   3ed5c:	add	x8, x8, x10
   3ed60:	sub	x8, x8, x9
   3ed64:	add	x8, x20, x8, lsl #3
   3ed68:	add	x8, x8, #0x18
   3ed6c:	ldr	x9, [x8]
   3ed70:	adds	x9, x9, #0x1
   3ed74:	str	x9, [x8], #8
   3ed78:	b.cs	3ed6c <__gmpn_toom44_mul@@Base+0x2e8>  // b.hs, b.nlast
   3ed7c:	b	3edac <__gmpn_toom44_mul@@Base+0x328>
   3ed80:	ldur	x8, [x29, #-8]
   3ed84:	mov	x0, x27
   3ed88:	mov	x2, x27
   3ed8c:	mov	x3, x21
   3ed90:	add	x1, x26, x8, lsl #3
   3ed94:	bl	cc40 <__gmpn_addlsh1_n@plt>
   3ed98:	add	x8, x0, x25, lsl #1
   3ed9c:	ldur	x19, [x29, #-24]
   3eda0:	ldr	x25, [sp, #48]
   3eda4:	ldr	x10, [sp, #24]
   3eda8:	str	x8, [x27, x21, lsl #3]
   3edac:	ldr	w8, [sp, #20]
   3edb0:	ldp	x9, x26, [x29, #-56]
   3edb4:	mov	x1, x20
   3edb8:	and	w23, w8, #0x1
   3edbc:	ldur	x8, [x29, #-16]
   3edc0:	cmp	x9, #0xbc
   3edc4:	add	x8, x8, x10, lsl #3
   3edc8:	add	x0, x8, #0x10
   3edcc:	stur	x0, [x29, #-40]
   3edd0:	b.le	3edec <__gmpn_toom44_mul@@Base+0x368>
   3edd4:	ldur	x2, [x29, #-64]
   3edd8:	mov	x3, x27
   3eddc:	mov	x5, x22
   3ede0:	mov	x4, x2
   3ede4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3ede8:	b	3ee00 <__gmpn_toom44_mul@@Base+0x37c>
   3edec:	ldur	x2, [x29, #-64]
   3edf0:	mov	x3, x27
   3edf4:	mov	x5, x22
   3edf8:	mov	x4, x2
   3edfc:	bl	d450 <__gmpn_toom22_mul@plt>
   3ee00:	mov	x0, x20
   3ee04:	mov	x1, x25
   3ee08:	mov	x2, x26
   3ee0c:	mov	x3, x21
   3ee10:	mov	x4, x19
   3ee14:	mov	x5, x22
   3ee18:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3ee1c:	ldr	x19, [sp, #32]
   3ee20:	ldur	x2, [x29, #-32]
   3ee24:	and	w8, w0, #0x2
   3ee28:	mov	x0, x27
   3ee2c:	mov	x1, x19
   3ee30:	mov	x3, x21
   3ee34:	mov	x4, x28
   3ee38:	mov	x5, x22
   3ee3c:	orr	w24, w8, w23
   3ee40:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3ee44:	and	w8, w0, #0x2
   3ee48:	eor	w8, w24, w8
   3ee4c:	str	w8, [sp, #72]
   3ee50:	ldur	x8, [x29, #-16]
   3ee54:	ldur	x24, [x29, #-56]
   3ee58:	add	x23, x21, x21, lsl #1
   3ee5c:	mov	x1, x25
   3ee60:	add	x8, x8, x23, lsl #4
   3ee64:	cmp	x24, #0xbc
   3ee68:	add	x0, x8, #0x18
   3ee6c:	str	x0, [sp, #24]
   3ee70:	b.le	3eed4 <__gmpn_toom44_mul@@Base+0x450>
   3ee74:	ldur	x25, [x29, #-64]
   3ee78:	mov	x3, x19
   3ee7c:	mov	x5, x22
   3ee80:	mov	x2, x25
   3ee84:	mov	x4, x25
   3ee88:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3ee8c:	ldr	x0, [sp, #40]
   3ee90:	mov	x1, x20
   3ee94:	mov	x2, x25
   3ee98:	mov	x3, x27
   3ee9c:	mov	x4, x25
   3eea0:	mov	x5, x22
   3eea4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3eea8:	cmp	x24, #0xc0
   3eeac:	b.le	3ef08 <__gmpn_toom44_mul@@Base+0x484>
   3eeb0:	ldur	x19, [x29, #-32]
   3eeb4:	mov	x0, x20
   3eeb8:	mov	x1, x26
   3eebc:	mov	x2, x21
   3eec0:	mov	x3, x19
   3eec4:	mov	x4, x21
   3eec8:	mov	x5, x22
   3eecc:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3eed0:	b	3ef28 <__gmpn_toom44_mul@@Base+0x4a4>
   3eed4:	ldur	x25, [x29, #-64]
   3eed8:	mov	x3, x19
   3eedc:	mov	x5, x22
   3eee0:	mov	x2, x25
   3eee4:	mov	x4, x25
   3eee8:	bl	d450 <__gmpn_toom22_mul@plt>
   3eeec:	ldr	x0, [sp, #40]
   3eef0:	mov	x1, x20
   3eef4:	mov	x2, x25
   3eef8:	mov	x3, x27
   3eefc:	mov	x4, x25
   3ef00:	mov	x5, x22
   3ef04:	bl	d450 <__gmpn_toom22_mul@plt>
   3ef08:	ldur	x19, [x29, #-32]
   3ef0c:	mov	x0, x20
   3ef10:	mov	x1, x26
   3ef14:	mov	x2, x21
   3ef18:	mov	x3, x19
   3ef1c:	mov	x4, x21
   3ef20:	mov	x5, x22
   3ef24:	bl	d450 <__gmpn_toom22_mul@plt>
   3ef28:	ldr	x8, [sp, #64]
   3ef2c:	ldur	x25, [x29, #-24]
   3ef30:	cmp	x24, x8
   3ef34:	lsl	x8, x23, #1
   3ef38:	add	x0, x20, x8, lsl #3
   3ef3c:	b.le	3ef60 <__gmpn_toom44_mul@@Base+0x4dc>
   3ef40:	ldur	x8, [x29, #-8]
   3ef44:	mov	x2, x25
   3ef48:	mov	x4, x28
   3ef4c:	lsl	x8, x8, #3
   3ef50:	add	x1, x26, x8
   3ef54:	add	x3, x19, x8
   3ef58:	bl	ccd0 <__gmpn_mul@plt>
   3ef5c:	b	3ef90 <__gmpn_toom44_mul@@Base+0x50c>
   3ef60:	ldur	x8, [x29, #-8]
   3ef64:	cmp	x25, #0x30
   3ef68:	mov	x2, x25
   3ef6c:	mov	x4, x25
   3ef70:	lsl	x8, x8, #3
   3ef74:	add	x1, x26, x8
   3ef78:	add	x3, x19, x8
   3ef7c:	mov	x5, x22
   3ef80:	b.le	3ef8c <__gmpn_toom44_mul@@Base+0x508>
   3ef84:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3ef88:	b	3ef90 <__gmpn_toom44_mul@@Base+0x50c>
   3ef8c:	bl	d450 <__gmpn_toom22_mul@plt>
   3ef90:	ldr	w2, [sp, #72]
   3ef94:	ldr	x3, [sp, #56]
   3ef98:	ldr	x4, [sp, #24]
   3ef9c:	ldur	x5, [x29, #-16]
   3efa0:	ldur	x6, [x29, #-40]
   3efa4:	add	x7, x25, x28
   3efa8:	mov	x0, x20
   3efac:	mov	x1, x21
   3efb0:	str	x22, [sp]
   3efb4:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   3efb8:	ldp	x20, x19, [sp, #224]
   3efbc:	ldp	x22, x21, [sp, #208]
   3efc0:	ldp	x24, x23, [sp, #192]
   3efc4:	ldp	x26, x25, [sp, #176]
   3efc8:	ldp	x28, x27, [sp, #160]
   3efcc:	ldp	x29, x30, [sp, #144]
   3efd0:	add	sp, sp, #0xf0
   3efd4:	ret

000000000003efd8 <__gmpn_toom6h_mul@@Base>:
   3efd8:	sub	sp, sp, #0xd0
   3efdc:	add	x8, x2, x2, lsl #4
   3efe0:	add	x9, x4, x4, lsl #3
   3efe4:	stp	x29, x30, [sp, #112]
   3efe8:	stp	x20, x19, [sp, #192]
   3efec:	add	x29, sp, #0x70
   3eff0:	cmp	x8, x9, lsl #1
   3eff4:	mov	x20, x0
   3eff8:	stp	x28, x27, [sp, #128]
   3effc:	stp	x26, x25, [sp, #144]
   3f000:	stp	x24, x23, [sp, #160]
   3f004:	stp	x22, x21, [sp, #176]
   3f008:	str	x5, [sp, #56]
   3f00c:	stp	x3, x1, [x29, #-16]
   3f010:	b.ge	3fa04 <__gmpn_toom6h_mul@@Base+0xa2c>  // b.tcont
   3f014:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3f018:	sub	x8, x2, #0x1
   3f01c:	movk	x9, #0xaaab
   3f020:	umulh	x8, x8, x9
   3f024:	lsr	x8, x8, #2
   3f028:	add	x23, x8, #0x1
   3f02c:	add	x8, x23, x23, lsl #2
   3f030:	sub	x5, x2, x8
   3f034:	sub	x8, x4, x8
   3f038:	str	wzr, [sp, #44]
   3f03c:	mov	w15, #0x5                   	// #5
   3f040:	stur	x8, [x29, #-24]
   3f044:	mov	w26, #0x5                   	// #5
   3f048:	add	x8, x23, x23, lsl #3
   3f04c:	lsl	x19, x8, #3
   3f050:	ldur	x3, [x29, #-8]
   3f054:	mov	w9, #0x38                  	// #56
   3f058:	add	x8, x20, x19
   3f05c:	add	x28, x8, #0x10
   3f060:	madd	x27, x23, x9, x20
   3f064:	mov	w6, #0x1                   	// #1
   3f068:	mov	x0, x28
   3f06c:	mov	x1, x27
   3f070:	mov	w2, w15
   3f074:	mov	x4, x23
   3f078:	mov	x7, x20
   3f07c:	stur	x5, [x29, #-32]
   3f080:	stur	x15, [x29, #-48]
   3f084:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3f088:	ldr	x21, [sp, #56]
   3f08c:	ldp	x5, x3, [x29, #-24]
   3f090:	add	x9, x20, x23, lsl #6
   3f094:	add	x25, x9, #0x8
   3f098:	add	x8, x21, x19
   3f09c:	add	x24, x8, #0x18
   3f0a0:	mov	w22, w0
   3f0a4:	mov	w6, #0x1                   	// #1
   3f0a8:	mov	x0, x24
   3f0ac:	mov	x1, x25
   3f0b0:	mov	w2, w26
   3f0b4:	mov	x4, x23
   3f0b8:	mov	x7, x20
   3f0bc:	stur	x26, [x29, #-40]
   3f0c0:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3f0c4:	eor	w8, w0, w22
   3f0c8:	str	w8, [sp, #32]
   3f0cc:	mov	w8, #0x50                  	// #80
   3f0d0:	cmp	x23, #0x2f
   3f0d4:	add	x19, x23, #0x1
   3f0d8:	madd	x8, x23, x8, x21
   3f0dc:	b.le	3f134 <__gmpn_toom6h_mul@@Base+0x15c>
   3f0e0:	cmp	x23, #0x50
   3f0e4:	b.le	3f178 <__gmpn_toom6h_mul@@Base+0x1a0>
   3f0e8:	add	x26, x8, #0x20
   3f0ec:	cmp	x23, #0xab
   3f0f0:	mov	x22, x28
   3f0f4:	mov	x0, x20
   3f0f8:	mov	x1, x27
   3f0fc:	mov	x2, x19
   3f100:	mov	x3, x25
   3f104:	mov	x4, x19
   3f108:	mov	x5, x26
   3f10c:	b.le	3f1bc <__gmpn_toom6h_mul@@Base+0x1e4>
   3f110:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f114:	mov	x0, x21
   3f118:	mov	x1, x22
   3f11c:	mov	x2, x19
   3f120:	mov	x3, x24
   3f124:	mov	x4, x19
   3f128:	mov	x5, x26
   3f12c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f130:	b	3f1dc <__gmpn_toom6h_mul@@Base+0x204>
   3f134:	add	x26, x8, #0x20
   3f138:	mov	x0, x20
   3f13c:	mov	x1, x27
   3f140:	mov	x2, x19
   3f144:	mov	x3, x25
   3f148:	mov	x4, x19
   3f14c:	mov	x5, x26
   3f150:	bl	d450 <__gmpn_toom22_mul@plt>
   3f154:	mov	x0, x21
   3f158:	mov	x1, x28
   3f15c:	mov	x2, x19
   3f160:	mov	x3, x24
   3f164:	mov	x4, x19
   3f168:	mov	x5, x26
   3f16c:	bl	d450 <__gmpn_toom22_mul@plt>
   3f170:	mov	x22, x28
   3f174:	b	3f1dc <__gmpn_toom6h_mul@@Base+0x204>
   3f178:	add	x26, x8, #0x20
   3f17c:	mov	x0, x20
   3f180:	mov	x1, x27
   3f184:	mov	x2, x19
   3f188:	mov	x3, x25
   3f18c:	mov	x4, x19
   3f190:	mov	x5, x26
   3f194:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f198:	mov	x0, x21
   3f19c:	mov	x1, x28
   3f1a0:	mov	x2, x19
   3f1a4:	mov	x3, x24
   3f1a8:	mov	x4, x19
   3f1ac:	mov	x5, x26
   3f1b0:	mov	x22, x28
   3f1b4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f1b8:	b	3f1dc <__gmpn_toom6h_mul@@Base+0x204>
   3f1bc:	bl	c720 <__gmpn_toom44_mul@plt>
   3f1c0:	mov	x0, x21
   3f1c4:	mov	x1, x22
   3f1c8:	mov	x2, x19
   3f1cc:	mov	x3, x24
   3f1d0:	mov	x4, x19
   3f1d4:	mov	x5, x26
   3f1d8:	bl	c720 <__gmpn_toom44_mul@plt>
   3f1dc:	ldr	w6, [sp, #44]
   3f1e0:	ldr	w3, [sp, #32]
   3f1e4:	mov	w1, #0x1                   	// #1
   3f1e8:	bfi	x1, x23, #1, #63
   3f1ec:	add	w5, w6, #0x1
   3f1f0:	mov	x0, x21
   3f1f4:	mov	x2, x20
   3f1f8:	mov	x4, x23
   3f1fc:	str	x1, [sp, #48]
   3f200:	str	w5, [sp, #28]
   3f204:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f208:	ldur	x3, [x29, #-8]
   3f20c:	ldur	x5, [x29, #-32]
   3f210:	mov	x0, x22
   3f214:	mov	x1, x27
   3f218:	ldur	x2, [x29, #-48]
   3f21c:	mov	x4, x23
   3f220:	mov	x6, x20
   3f224:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3f228:	ldur	x2, [x29, #-40]
   3f22c:	mov	w26, w0
   3f230:	mov	x0, x24
   3f234:	mov	x1, x25
   3f238:	cmp	w2, #0x3
   3f23c:	b.eq	3fa28 <__gmpn_toom6h_mul@@Base+0xa50>  // b.none
   3f240:	ldp	x5, x3, [x29, #-24]
   3f244:	mov	x4, x23
   3f248:	mov	x6, x20
   3f24c:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3f250:	eor	w8, w0, w26
   3f254:	str	w8, [sp, #16]
   3f258:	mov	w8, #0x50                  	// #80
   3f25c:	cmp	x23, #0x2f
   3f260:	madd	x8, x23, x8, x21
   3f264:	b.le	3f2c0 <__gmpn_toom6h_mul@@Base+0x2e8>
   3f268:	cmp	x23, #0x50
   3f26c:	b.le	3f308 <__gmpn_toom6h_mul@@Base+0x330>
   3f270:	add	x26, x8, #0x20
   3f274:	cmp	x23, #0xab
   3f278:	mov	x0, x20
   3f27c:	mov	x1, x27
   3f280:	mov	x2, x19
   3f284:	mov	x3, x25
   3f288:	mov	x4, x19
   3f28c:	mov	x5, x26
   3f290:	b.le	3f350 <__gmpn_toom6h_mul@@Base+0x378>
   3f294:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f298:	add	x28, x23, x23, lsl #1
   3f29c:	add	x8, x21, x28, lsl #3
   3f2a0:	add	x0, x8, #0x8
   3f2a4:	mov	x1, x22
   3f2a8:	mov	x2, x19
   3f2ac:	mov	x3, x24
   3f2b0:	mov	x4, x19
   3f2b4:	mov	x5, x26
   3f2b8:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f2bc:	b	3f378 <__gmpn_toom6h_mul@@Base+0x3a0>
   3f2c0:	add	x26, x8, #0x20
   3f2c4:	mov	x0, x20
   3f2c8:	mov	x1, x27
   3f2cc:	mov	x2, x19
   3f2d0:	mov	x3, x25
   3f2d4:	mov	x4, x19
   3f2d8:	mov	x5, x26
   3f2dc:	bl	d450 <__gmpn_toom22_mul@plt>
   3f2e0:	add	x28, x23, x23, lsl #1
   3f2e4:	add	x8, x21, x28, lsl #3
   3f2e8:	add	x0, x8, #0x8
   3f2ec:	mov	x1, x22
   3f2f0:	mov	x2, x19
   3f2f4:	mov	x3, x24
   3f2f8:	mov	x4, x19
   3f2fc:	mov	x5, x26
   3f300:	bl	d450 <__gmpn_toom22_mul@plt>
   3f304:	b	3f378 <__gmpn_toom6h_mul@@Base+0x3a0>
   3f308:	add	x26, x8, #0x20
   3f30c:	mov	x0, x20
   3f310:	mov	x1, x27
   3f314:	mov	x2, x19
   3f318:	mov	x3, x25
   3f31c:	mov	x4, x19
   3f320:	mov	x5, x26
   3f324:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f328:	add	x28, x23, x23, lsl #1
   3f32c:	add	x8, x21, x28, lsl #3
   3f330:	add	x0, x8, #0x8
   3f334:	mov	x1, x22
   3f338:	mov	x2, x19
   3f33c:	mov	x3, x24
   3f340:	mov	x4, x19
   3f344:	mov	x5, x26
   3f348:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f34c:	b	3f378 <__gmpn_toom6h_mul@@Base+0x3a0>
   3f350:	bl	c720 <__gmpn_toom44_mul@plt>
   3f354:	add	x28, x23, x23, lsl #1
   3f358:	add	x8, x21, x28, lsl #3
   3f35c:	add	x0, x8, #0x8
   3f360:	mov	x1, x22
   3f364:	mov	x2, x19
   3f368:	mov	x3, x24
   3f36c:	mov	x4, x19
   3f370:	mov	x5, x26
   3f374:	bl	c720 <__gmpn_toom44_mul@plt>
   3f378:	ldr	x1, [sp, #48]
   3f37c:	ldr	w3, [sp, #16]
   3f380:	ldur	x26, [x29, #-32]
   3f384:	add	x8, x21, x28, lsl #3
   3f388:	add	x0, x8, #0x8
   3f38c:	mov	x2, x20
   3f390:	mov	x4, x23
   3f394:	mov	w5, wzr
   3f398:	mov	w6, wzr
   3f39c:	str	x28, [sp, #8]
   3f3a0:	str	x0, [sp, #32]
   3f3a4:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f3a8:	ldur	x3, [x29, #-8]
   3f3ac:	mov	w6, #0x2                   	// #2
   3f3b0:	mov	x0, x22
   3f3b4:	mov	x1, x27
   3f3b8:	ldur	x2, [x29, #-48]
   3f3bc:	mov	x4, x23
   3f3c0:	mov	x5, x26
   3f3c4:	mov	x7, x20
   3f3c8:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3f3cc:	ldp	x5, x3, [x29, #-24]
   3f3d0:	mov	w26, w0
   3f3d4:	mov	w6, #0x2                   	// #2
   3f3d8:	mov	x0, x24
   3f3dc:	mov	x1, x25
   3f3e0:	ldur	x2, [x29, #-40]
   3f3e4:	mov	x4, x23
   3f3e8:	mov	x7, x20
   3f3ec:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3f3f0:	eor	w8, w0, w26
   3f3f4:	cmp	x23, #0x2f
   3f3f8:	str	w8, [sp, #4]
   3f3fc:	mov	w8, #0x50                  	// #80
   3f400:	b.le	3f468 <__gmpn_toom6h_mul@@Base+0x490>
   3f404:	cmp	x23, #0x50
   3f408:	b.le	3f4b8 <__gmpn_toom6h_mul@@Base+0x4e0>
   3f40c:	ldr	x21, [sp, #56]
   3f410:	cmp	x23, #0xab
   3f414:	mov	x0, x20
   3f418:	mov	x1, x27
   3f41c:	madd	x8, x23, x8, x21
   3f420:	add	x26, x8, #0x20
   3f424:	mov	x2, x19
   3f428:	mov	x3, x25
   3f42c:	mov	x4, x19
   3f430:	mov	x5, x26
   3f434:	b.le	3f50c <__gmpn_toom6h_mul@@Base+0x534>
   3f438:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f43c:	add	x8, x23, x23, lsl #1
   3f440:	lsl	x28, x8, #1
   3f444:	add	x8, x21, x8, lsl #4
   3f448:	add	x0, x8, #0x10
   3f44c:	mov	x1, x22
   3f450:	mov	x2, x19
   3f454:	mov	x3, x24
   3f458:	mov	x4, x19
   3f45c:	mov	x5, x26
   3f460:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f464:	b	3f538 <__gmpn_toom6h_mul@@Base+0x560>
   3f468:	madd	x8, x23, x8, x21
   3f46c:	add	x26, x8, #0x20
   3f470:	mov	x0, x20
   3f474:	mov	x1, x27
   3f478:	mov	x2, x19
   3f47c:	mov	x3, x25
   3f480:	mov	x4, x19
   3f484:	mov	x5, x26
   3f488:	bl	d450 <__gmpn_toom22_mul@plt>
   3f48c:	add	x8, x23, x23, lsl #1
   3f490:	lsl	x28, x8, #1
   3f494:	add	x8, x21, x8, lsl #4
   3f498:	add	x0, x8, #0x10
   3f49c:	mov	x1, x22
   3f4a0:	mov	x2, x19
   3f4a4:	mov	x3, x24
   3f4a8:	mov	x4, x19
   3f4ac:	mov	x5, x26
   3f4b0:	bl	d450 <__gmpn_toom22_mul@plt>
   3f4b4:	b	3f538 <__gmpn_toom6h_mul@@Base+0x560>
   3f4b8:	ldr	x21, [sp, #56]
   3f4bc:	mov	x0, x20
   3f4c0:	mov	x1, x27
   3f4c4:	mov	x2, x19
   3f4c8:	madd	x8, x23, x8, x21
   3f4cc:	add	x26, x8, #0x20
   3f4d0:	mov	x3, x25
   3f4d4:	mov	x4, x19
   3f4d8:	mov	x5, x26
   3f4dc:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f4e0:	add	x8, x23, x23, lsl #1
   3f4e4:	lsl	x28, x8, #1
   3f4e8:	add	x8, x21, x8, lsl #4
   3f4ec:	add	x0, x8, #0x10
   3f4f0:	mov	x1, x22
   3f4f4:	mov	x2, x19
   3f4f8:	mov	x3, x24
   3f4fc:	mov	x4, x19
   3f500:	mov	x5, x26
   3f504:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f508:	b	3f538 <__gmpn_toom6h_mul@@Base+0x560>
   3f50c:	bl	c720 <__gmpn_toom44_mul@plt>
   3f510:	add	x8, x23, x23, lsl #1
   3f514:	lsl	x28, x8, #1
   3f518:	add	x8, x21, x8, lsl #4
   3f51c:	add	x0, x8, #0x10
   3f520:	mov	x1, x22
   3f524:	mov	x2, x19
   3f528:	mov	x3, x24
   3f52c:	mov	x4, x19
   3f530:	mov	x5, x26
   3f534:	bl	c720 <__gmpn_toom44_mul@plt>
   3f538:	ldr	x1, [sp, #48]
   3f53c:	ldr	w3, [sp, #4]
   3f540:	ldur	x26, [x29, #-32]
   3f544:	add	x8, x21, x28, lsl #3
   3f548:	add	x0, x8, #0x10
   3f54c:	mov	w5, #0x2                   	// #2
   3f550:	mov	w6, #0x4                   	// #4
   3f554:	mov	x2, x20
   3f558:	mov	x4, x23
   3f55c:	str	x0, [sp, #16]
   3f560:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f564:	ldur	x3, [x29, #-8]
   3f568:	mov	w6, #0x2                   	// #2
   3f56c:	mov	x0, x22
   3f570:	mov	x1, x27
   3f574:	ldur	x2, [x29, #-48]
   3f578:	mov	x4, x23
   3f57c:	mov	x5, x26
   3f580:	mov	x7, x20
   3f584:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3f588:	ldp	x5, x3, [x29, #-24]
   3f58c:	mov	w26, w0
   3f590:	mov	w6, #0x2                   	// #2
   3f594:	mov	x0, x24
   3f598:	mov	x1, x25
   3f59c:	ldur	x2, [x29, #-40]
   3f5a0:	mov	x4, x23
   3f5a4:	mov	x7, x20
   3f5a8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3f5ac:	cmp	x23, #0x2f
   3f5b0:	mov	x28, x22
   3f5b4:	mov	x22, x27
   3f5b8:	mov	x27, x25
   3f5bc:	eor	w25, w0, w26
   3f5c0:	str	w25, [sp, #4]
   3f5c4:	b.le	3f62c <__gmpn_toom6h_mul@@Base+0x654>
   3f5c8:	cmp	x23, #0x50
   3f5cc:	b.le	3f680 <__gmpn_toom6h_mul@@Base+0x6a8>
   3f5d0:	ldp	x25, x9, [sp, #48]
   3f5d4:	mov	w8, #0x50                  	// #80
   3f5d8:	mov	x21, x24
   3f5dc:	cmp	x23, #0xab
   3f5e0:	madd	x8, x23, x8, x9
   3f5e4:	add	x26, x8, #0x20
   3f5e8:	mov	x0, x20
   3f5ec:	mov	x1, x22
   3f5f0:	mov	x2, x19
   3f5f4:	mov	x3, x27
   3f5f8:	mov	x4, x19
   3f5fc:	mov	x5, x26
   3f600:	b.le	3f6d8 <__gmpn_toom6h_mul@@Base+0x700>
   3f604:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f608:	ldr	x24, [sp, #8]
   3f60c:	mov	x1, x28
   3f610:	mov	x2, x19
   3f614:	mov	x3, x21
   3f618:	add	x0, x20, x24, lsl #3
   3f61c:	mov	x4, x19
   3f620:	mov	x5, x26
   3f624:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f628:	b	3f6fc <__gmpn_toom6h_mul@@Base+0x724>
   3f62c:	mov	w8, #0x50                  	// #80
   3f630:	madd	x8, x23, x8, x21
   3f634:	add	x26, x8, #0x20
   3f638:	mov	x0, x20
   3f63c:	mov	x1, x22
   3f640:	mov	x2, x19
   3f644:	mov	x3, x27
   3f648:	mov	x4, x19
   3f64c:	mov	x5, x26
   3f650:	bl	d450 <__gmpn_toom22_mul@plt>
   3f654:	mov	x21, x24
   3f658:	ldr	x24, [sp, #8]
   3f65c:	mov	x1, x28
   3f660:	mov	x2, x19
   3f664:	mov	x3, x21
   3f668:	add	x0, x20, x24, lsl #3
   3f66c:	mov	x4, x19
   3f670:	mov	x5, x26
   3f674:	bl	d450 <__gmpn_toom22_mul@plt>
   3f678:	ldr	x25, [sp, #48]
   3f67c:	b	3f6fc <__gmpn_toom6h_mul@@Base+0x724>
   3f680:	ldr	x9, [sp, #56]
   3f684:	mov	w8, #0x50                  	// #80
   3f688:	mov	x0, x20
   3f68c:	mov	x1, x22
   3f690:	madd	x8, x23, x8, x9
   3f694:	add	x26, x8, #0x20
   3f698:	mov	x2, x19
   3f69c:	mov	x3, x27
   3f6a0:	mov	x4, x19
   3f6a4:	mov	x5, x26
   3f6a8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f6ac:	mov	x21, x24
   3f6b0:	ldr	x24, [sp, #8]
   3f6b4:	mov	x1, x28
   3f6b8:	mov	x2, x19
   3f6bc:	mov	x3, x21
   3f6c0:	add	x0, x20, x24, lsl #3
   3f6c4:	mov	x4, x19
   3f6c8:	mov	x5, x26
   3f6cc:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f6d0:	ldr	x25, [sp, #48]
   3f6d4:	b	3f6fc <__gmpn_toom6h_mul@@Base+0x724>
   3f6d8:	bl	c720 <__gmpn_toom44_mul@plt>
   3f6dc:	ldr	x24, [sp, #8]
   3f6e0:	mov	x1, x28
   3f6e4:	mov	x2, x19
   3f6e8:	mov	x3, x21
   3f6ec:	add	x0, x20, x24, lsl #3
   3f6f0:	mov	x4, x19
   3f6f4:	mov	x5, x26
   3f6f8:	bl	c720 <__gmpn_toom44_mul@plt>
   3f6fc:	ldr	w8, [sp, #28]
   3f700:	ldr	w3, [sp, #4]
   3f704:	add	x0, x20, x24, lsl #3
   3f708:	mov	x1, x25
   3f70c:	lsl	w5, w8, #1
   3f710:	ldr	w8, [sp, #44]
   3f714:	mov	x2, x20
   3f718:	mov	x4, x23
   3f71c:	lsl	w6, w8, #1
   3f720:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f724:	ldur	x3, [x29, #-8]
   3f728:	ldur	x5, [x29, #-32]
   3f72c:	mov	x0, x28
   3f730:	mov	x1, x22
   3f734:	ldur	x2, [x29, #-48]
   3f738:	mov	x4, x23
   3f73c:	mov	x6, x20
   3f740:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3f744:	ldp	x5, x3, [x29, #-24]
   3f748:	mov	w26, w0
   3f74c:	mov	x0, x21
   3f750:	mov	x1, x27
   3f754:	ldur	x2, [x29, #-40]
   3f758:	mov	x4, x23
   3f75c:	mov	x6, x20
   3f760:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3f764:	cmp	x23, #0x2f
   3f768:	eor	w10, w0, w26
   3f76c:	mov	x24, x21
   3f770:	b.le	3f824 <__gmpn_toom6h_mul@@Base+0x84c>
   3f774:	ldr	x9, [sp, #56]
   3f778:	mov	x11, x20
   3f77c:	cmp	x23, #0x51
   3f780:	b.lt	3f894 <__gmpn_toom6h_mul@@Base+0x8bc>  // b.tstop
   3f784:	mov	w8, #0x50                  	// #80
   3f788:	madd	x8, x23, x8, x9
   3f78c:	add	x26, x8, #0x20
   3f790:	mov	x20, x22
   3f794:	mov	w21, w10
   3f798:	cmp	x23, #0xab
   3f79c:	mov	x22, x11
   3f7a0:	mov	x0, x11
   3f7a4:	mov	x1, x20
   3f7a8:	mov	x2, x19
   3f7ac:	mov	x3, x27
   3f7b0:	mov	x4, x19
   3f7b4:	mov	x5, x26
   3f7b8:	b.le	3f92c <__gmpn_toom6h_mul@@Base+0x954>
   3f7bc:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f7c0:	mov	x0, x20
   3f7c4:	mov	x1, x28
   3f7c8:	mov	x2, x19
   3f7cc:	mov	x3, x24
   3f7d0:	mov	x4, x19
   3f7d4:	mov	x5, x26
   3f7d8:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f7dc:	ldr	x1, [sp, #48]
   3f7e0:	mov	w5, #0x1                   	// #1
   3f7e4:	mov	w6, #0x2                   	// #2
   3f7e8:	mov	x0, x20
   3f7ec:	mov	x2, x22
   3f7f0:	mov	w3, w21
   3f7f4:	mov	x4, x23
   3f7f8:	mov	x20, x22
   3f7fc:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f800:	cmp	x23, #0xac
   3f804:	b.eq	3f978 <__gmpn_toom6h_mul@@Base+0x9a0>  // b.none
   3f808:	ldp	x3, x1, [x29, #-16]
   3f80c:	mov	x0, x20
   3f810:	mov	x2, x23
   3f814:	mov	x4, x23
   3f818:	mov	x5, x24
   3f81c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   3f820:	b	3f9ac <__gmpn_toom6h_mul@@Base+0x9d4>
   3f824:	ldr	x9, [sp, #56]
   3f828:	mov	w8, #0x50                  	// #80
   3f82c:	mov	x0, x20
   3f830:	mov	x1, x22
   3f834:	madd	x8, x23, x8, x9
   3f838:	add	x26, x8, #0x20
   3f83c:	mov	x2, x19
   3f840:	mov	x3, x27
   3f844:	mov	x4, x19
   3f848:	mov	x5, x26
   3f84c:	mov	w21, w10
   3f850:	bl	d450 <__gmpn_toom22_mul@plt>
   3f854:	mov	x0, x22
   3f858:	mov	x1, x28
   3f85c:	mov	x2, x19
   3f860:	mov	x3, x24
   3f864:	mov	x4, x19
   3f868:	mov	x5, x26
   3f86c:	bl	d450 <__gmpn_toom22_mul@plt>
   3f870:	mov	w5, #0x1                   	// #1
   3f874:	mov	w6, #0x2                   	// #2
   3f878:	mov	x0, x22
   3f87c:	mov	x1, x25
   3f880:	mov	x2, x20
   3f884:	mov	w3, w21
   3f888:	mov	x4, x23
   3f88c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f890:	b	3f910 <__gmpn_toom6h_mul@@Base+0x938>
   3f894:	mov	w8, #0x50                  	// #80
   3f898:	madd	x8, x23, x8, x9
   3f89c:	add	x26, x8, #0x20
   3f8a0:	mov	x0, x11
   3f8a4:	mov	x1, x22
   3f8a8:	mov	x2, x19
   3f8ac:	mov	x3, x27
   3f8b0:	mov	x4, x19
   3f8b4:	mov	x5, x26
   3f8b8:	mov	x21, x11
   3f8bc:	mov	x20, x22
   3f8c0:	mov	w22, w10
   3f8c4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f8c8:	mov	x0, x20
   3f8cc:	mov	x1, x28
   3f8d0:	mov	x2, x19
   3f8d4:	mov	x3, x24
   3f8d8:	mov	x4, x19
   3f8dc:	mov	x5, x26
   3f8e0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f8e4:	ldr	x1, [sp, #48]
   3f8e8:	mov	w5, #0x1                   	// #1
   3f8ec:	mov	w6, #0x2                   	// #2
   3f8f0:	mov	x0, x20
   3f8f4:	mov	x2, x21
   3f8f8:	mov	w3, w22
   3f8fc:	mov	x4, x23
   3f900:	mov	x20, x21
   3f904:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f908:	cmp	x23, #0x30
   3f90c:	b.gt	3f994 <__gmpn_toom6h_mul@@Base+0x9bc>
   3f910:	ldp	x3, x1, [x29, #-16]
   3f914:	mov	x0, x20
   3f918:	mov	x2, x23
   3f91c:	mov	x4, x23
   3f920:	mov	x5, x24
   3f924:	bl	d450 <__gmpn_toom22_mul@plt>
   3f928:	b	3f9ac <__gmpn_toom6h_mul@@Base+0x9d4>
   3f92c:	bl	c720 <__gmpn_toom44_mul@plt>
   3f930:	mov	x0, x20
   3f934:	mov	x1, x28
   3f938:	mov	x2, x19
   3f93c:	mov	x3, x24
   3f940:	mov	x4, x19
   3f944:	mov	x5, x26
   3f948:	bl	c720 <__gmpn_toom44_mul@plt>
   3f94c:	ldr	x1, [sp, #48]
   3f950:	mov	w5, #0x1                   	// #1
   3f954:	mov	w6, #0x2                   	// #2
   3f958:	mov	x0, x20
   3f95c:	mov	x2, x22
   3f960:	mov	w3, w21
   3f964:	mov	x4, x23
   3f968:	mov	x20, x22
   3f96c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3f970:	cmp	x23, #0x51
   3f974:	b.le	3f994 <__gmpn_toom6h_mul@@Base+0x9bc>
   3f978:	ldp	x3, x1, [x29, #-16]
   3f97c:	mov	x0, x20
   3f980:	mov	x2, x23
   3f984:	mov	x4, x23
   3f988:	mov	x5, x24
   3f98c:	bl	c720 <__gmpn_toom44_mul@plt>
   3f990:	b	3f9ac <__gmpn_toom6h_mul@@Base+0x9d4>
   3f994:	ldp	x3, x1, [x29, #-16]
   3f998:	mov	x0, x20
   3f99c:	mov	x2, x23
   3f9a0:	mov	x4, x23
   3f9a4:	mov	x5, x24
   3f9a8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   3f9ac:	ldr	w21, [sp, #44]
   3f9b0:	ldp	x22, x25, [x29, #-32]
   3f9b4:	ldur	x9, [x29, #-40]
   3f9b8:	ldr	x19, [sp, #32]
   3f9bc:	ldr	x26, [sp, #16]
   3f9c0:	cbnz	w21, 3fa3c <__gmpn_toom6h_mul@@Base+0xa64>
   3f9c4:	add	x5, x25, x22
   3f9c8:	mov	x0, x20
   3f9cc:	mov	x1, x26
   3f9d0:	mov	x2, x19
   3f9d4:	ldr	x3, [sp, #56]
   3f9d8:	mov	x4, x23
   3f9dc:	mov	w6, w21
   3f9e0:	mov	x7, x24
   3f9e4:	ldp	x20, x19, [sp, #192]
   3f9e8:	ldp	x22, x21, [sp, #176]
   3f9ec:	ldp	x24, x23, [sp, #160]
   3f9f0:	ldp	x26, x25, [sp, #144]
   3f9f4:	ldp	x28, x27, [sp, #128]
   3f9f8:	ldp	x29, x30, [sp, #112]
   3f9fc:	add	sp, sp, #0xd0
   3fa00:	b	bfb0 <__gmpn_toom_interpolate_12pts@plt>
   3fa04:	mov	w9, #0x5a                  	// #90
   3fa08:	mov	w10, #0x77                  	// #119
   3fa0c:	mul	x9, x2, x9
   3fa10:	mul	x10, x4, x10
   3fa14:	cmp	x9, x10
   3fa18:	b.ge	3fa78 <__gmpn_toom6h_mul@@Base+0xaa0>  // b.tcont
   3fa1c:	mov	w8, #0x6                   	// #6
   3fa20:	mov	w9, #0x7                   	// #7
   3fa24:	b	3fafc <__gmpn_toom6h_mul@@Base+0xb24>
   3fa28:	ldp	x4, x2, [x29, #-24]
   3fa2c:	mov	x3, x23
   3fa30:	mov	x5, x20
   3fa34:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   3fa38:	b	3f250 <__gmpn_toom6h_mul@@Base+0x278>
   3fa3c:	mov	w8, #0x58                  	// #88
   3fa40:	cmp	x22, x25
   3fa44:	madd	x0, x23, x8, x20
   3fa48:	b.le	3fa9c <__gmpn_toom6h_mul@@Base+0xac4>
   3fa4c:	ldur	x8, [x29, #-48]
   3fa50:	ldur	x10, [x29, #-8]
   3fa54:	mul	x9, x23, x9
   3fa58:	mov	x2, x22
   3fa5c:	mov	w8, w8
   3fa60:	mul	x8, x23, x8
   3fa64:	add	x1, x10, x8, lsl #3
   3fa68:	ldur	x8, [x29, #-16]
   3fa6c:	mov	x4, x25
   3fa70:	add	x3, x8, x9, lsl #3
   3fa74:	b	3fac4 <__gmpn_toom6h_mul@@Base+0xaec>
   3fa78:	mov	w9, #0x55                  	// #85
   3fa7c:	mov	w10, #0x7e                  	// #126
   3fa80:	mul	x9, x2, x9
   3fa84:	mul	x10, x4, x10
   3fa88:	cmp	x9, x10
   3fa8c:	b.ge	3facc <__gmpn_toom6h_mul@@Base+0xaf4>  // b.tcont
   3fa90:	mov	w8, #0x5                   	// #5
   3fa94:	mov	w9, #0x7                   	// #7
   3fa98:	b	3fafc <__gmpn_toom6h_mul@@Base+0xb24>
   3fa9c:	mul	x8, x23, x9
   3faa0:	ldur	x9, [x29, #-48]
   3faa4:	ldur	x10, [x29, #-16]
   3faa8:	mov	x2, x25
   3faac:	mov	x4, x22
   3fab0:	mov	w9, w9
   3fab4:	add	x1, x10, x8, lsl #3
   3fab8:	mul	x8, x23, x9
   3fabc:	ldur	x9, [x29, #-8]
   3fac0:	add	x3, x9, x8, lsl #3
   3fac4:	bl	ccd0 <__gmpn_mul@plt>
   3fac8:	b	3f9c4 <__gmpn_toom6h_mul@@Base+0x9ec>
   3facc:	add	x9, x2, x2, lsl #3
   3fad0:	lsl	x9, x9, #1
   3fad4:	add	x10, x4, x4, lsl #4
   3fad8:	cmp	x9, x10, lsl #1
   3fadc:	mov	w9, #0x8                   	// #8
   3fae0:	b.ge	3faec <__gmpn_toom6h_mul@@Base+0xb14>  // b.tcont
   3fae4:	mov	w8, #0x5                   	// #5
   3fae8:	b	3fafc <__gmpn_toom6h_mul@@Base+0xb24>
   3faec:	add	x10, x4, x4, lsl #3
   3faf0:	cmp	x8, x10, lsl #2
   3faf4:	cinc	w9, w9, ge  // ge = tcont
   3faf8:	mov	w8, #0x4                   	// #4
   3fafc:	mov	w12, w9
   3fb00:	mul	x11, x8, x2
   3fb04:	mul	x13, x12, x4
   3fb08:	cmp	x11, x13
   3fb0c:	csel	x11, x4, x2, lt  // lt = tstop
   3fb10:	csel	x12, x8, x12, lt  // lt = tstop
   3fb14:	sub	x11, x11, #0x1
   3fb18:	sub	w15, w9, #0x1
   3fb1c:	udiv	x11, x11, x12
   3fb20:	sub	w26, w8, #0x1
   3fb24:	sxtw	x14, w15
   3fb28:	add	x23, x11, #0x1
   3fb2c:	eor	w10, w8, w9
   3fb30:	msub	x5, x23, x14, x2
   3fb34:	msub	x21, x23, x26, x4
   3fb38:	stur	x21, [x29, #-24]
   3fb3c:	tbnz	w10, #0, 3fb48 <__gmpn_toom6h_mul@@Base+0xb70>
   3fb40:	str	wzr, [sp, #44]
   3fb44:	b	3f048 <__gmpn_toom6h_mul@@Base+0x70>
   3fb48:	cmp	x5, #0x0
   3fb4c:	b.le	3fb64 <__gmpn_toom6h_mul@@Base+0xb8c>
   3fb50:	cmp	x21, #0x0
   3fb54:	b.le	3fb74 <__gmpn_toom6h_mul@@Base+0xb9c>
   3fb58:	mov	w8, #0x1                   	// #1
   3fb5c:	str	w8, [sp, #44]
   3fb60:	b	3f048 <__gmpn_toom6h_mul@@Base+0x70>
   3fb64:	str	wzr, [sp, #44]
   3fb68:	sub	w15, w9, #0x2
   3fb6c:	add	x5, x5, x23
   3fb70:	b	3f048 <__gmpn_toom6h_mul@@Base+0x70>
   3fb74:	mov	x9, x21
   3fb78:	sub	w26, w8, #0x2
   3fb7c:	add	x9, x21, x23
   3fb80:	str	wzr, [sp, #44]
   3fb84:	stur	x9, [x29, #-24]
   3fb88:	b	3f048 <__gmpn_toom6h_mul@@Base+0x70>

000000000003fb8c <__gmpn_toom6_sqr@@Base>:
   3fb8c:	sub	sp, sp, #0x90
   3fb90:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   3fb94:	sub	x8, x2, #0x1
   3fb98:	movk	x9, #0xaaab
   3fb9c:	umulh	x8, x8, x9
   3fba0:	stp	x20, x19, [sp, #128]
   3fba4:	lsr	x19, x8, #2
   3fba8:	stp	x22, x21, [sp, #112]
   3fbac:	add	x22, x19, #0x1
   3fbb0:	add	x9, x22, x22, lsl #3
   3fbb4:	add	x8, x22, x22, lsl #2
   3fbb8:	lsl	x21, x9, #3
   3fbbc:	mov	w10, #0x38                  	// #56
   3fbc0:	sub	x5, x2, x8
   3fbc4:	add	x8, x0, x21
   3fbc8:	stp	x28, x27, [sp, #64]
   3fbcc:	stp	x26, x25, [sp, #80]
   3fbd0:	mov	x20, x0
   3fbd4:	add	x27, x8, #0x10
   3fbd8:	madd	x25, x22, x10, x0
   3fbdc:	stp	x24, x23, [sp, #96]
   3fbe0:	mov	x24, x3
   3fbe4:	mov	x3, x1
   3fbe8:	mov	w2, #0x5                   	// #5
   3fbec:	mov	w6, #0x1                   	// #1
   3fbf0:	mov	x0, x27
   3fbf4:	mov	x1, x25
   3fbf8:	mov	x4, x22
   3fbfc:	mov	x7, x20
   3fc00:	stp	x29, x30, [sp, #48]
   3fc04:	add	x29, sp, #0x30
   3fc08:	mov	x23, x3
   3fc0c:	mov	x26, x5
   3fc10:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3fc14:	add	x8, x24, x21
   3fc18:	add	x28, x19, #0x2
   3fc1c:	mov	x19, x24
   3fc20:	add	x24, x8, #0x18
   3fc24:	mov	x0, x20
   3fc28:	mov	x1, x25
   3fc2c:	mov	x2, x28
   3fc30:	mov	x3, x24
   3fc34:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fc38:	mov	x0, x19
   3fc3c:	mov	x1, x27
   3fc40:	mov	x2, x28
   3fc44:	mov	x3, x24
   3fc48:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fc4c:	mov	w1, #0x1                   	// #1
   3fc50:	bfi	x1, x22, #1, #63
   3fc54:	mov	w5, #0x1                   	// #1
   3fc58:	mov	x0, x19
   3fc5c:	mov	x2, x20
   3fc60:	mov	w3, wzr
   3fc64:	mov	x4, x22
   3fc68:	mov	w6, wzr
   3fc6c:	str	x1, [sp, #8]
   3fc70:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fc74:	mov	w2, #0x5                   	// #5
   3fc78:	mov	x0, x27
   3fc7c:	mov	x1, x25
   3fc80:	mov	x3, x23
   3fc84:	mov	x4, x22
   3fc88:	mov	x5, x26
   3fc8c:	mov	x6, x20
   3fc90:	mov	x21, x26
   3fc94:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   3fc98:	mov	x0, x20
   3fc9c:	mov	x1, x25
   3fca0:	mov	x2, x28
   3fca4:	mov	x3, x24
   3fca8:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fcac:	add	x8, x22, x22, lsl #1
   3fcb0:	lsl	x8, x8, #3
   3fcb4:	str	x8, [sp, #24]
   3fcb8:	add	x8, x19, x8
   3fcbc:	add	x26, x8, #0x8
   3fcc0:	mov	x0, x26
   3fcc4:	mov	x1, x27
   3fcc8:	mov	x2, x28
   3fccc:	mov	x3, x24
   3fcd0:	stp	x19, x26, [x29, #-16]
   3fcd4:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fcd8:	mov	x0, x26
   3fcdc:	ldr	x26, [sp, #8]
   3fce0:	mov	x2, x20
   3fce4:	mov	w3, wzr
   3fce8:	mov	x4, x22
   3fcec:	mov	x1, x26
   3fcf0:	mov	w5, wzr
   3fcf4:	mov	w6, wzr
   3fcf8:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fcfc:	mov	w2, #0x5                   	// #5
   3fd00:	mov	w6, #0x2                   	// #2
   3fd04:	mov	x0, x27
   3fd08:	mov	x1, x25
   3fd0c:	mov	x3, x23
   3fd10:	mov	x4, x22
   3fd14:	mov	x5, x21
   3fd18:	mov	x7, x20
   3fd1c:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   3fd20:	mov	x0, x20
   3fd24:	mov	x1, x25
   3fd28:	mov	x2, x28
   3fd2c:	mov	x3, x24
   3fd30:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fd34:	mov	w8, #0x30                  	// #48
   3fd38:	madd	x8, x22, x8, x19
   3fd3c:	add	x19, x8, #0x10
   3fd40:	mov	x0, x19
   3fd44:	mov	x1, x27
   3fd48:	mov	x2, x28
   3fd4c:	mov	x3, x24
   3fd50:	str	x19, [sp, #16]
   3fd54:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fd58:	mov	w5, #0x2                   	// #2
   3fd5c:	mov	w6, #0x4                   	// #4
   3fd60:	mov	x0, x19
   3fd64:	mov	x1, x26
   3fd68:	mov	x2, x20
   3fd6c:	mov	w3, wzr
   3fd70:	mov	x4, x22
   3fd74:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fd78:	mov	w2, #0x5                   	// #5
   3fd7c:	mov	w6, #0x2                   	// #2
   3fd80:	mov	x0, x27
   3fd84:	mov	x1, x25
   3fd88:	mov	x3, x23
   3fd8c:	mov	x4, x22
   3fd90:	mov	x5, x21
   3fd94:	mov	x7, x20
   3fd98:	mov	x19, x23
   3fd9c:	mov	x23, x21
   3fda0:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3fda4:	mov	x0, x20
   3fda8:	mov	x1, x25
   3fdac:	mov	x2, x28
   3fdb0:	mov	x3, x24
   3fdb4:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fdb8:	ldr	x8, [sp, #24]
   3fdbc:	mov	x1, x27
   3fdc0:	mov	x2, x28
   3fdc4:	mov	x3, x24
   3fdc8:	add	x21, x20, x8
   3fdcc:	mov	x0, x21
   3fdd0:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fdd4:	mov	w5, #0x2                   	// #2
   3fdd8:	mov	x0, x21
   3fddc:	mov	x1, x26
   3fde0:	mov	x2, x20
   3fde4:	mov	w3, wzr
   3fde8:	mov	x4, x22
   3fdec:	mov	w6, wzr
   3fdf0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fdf4:	mov	w2, #0x5                   	// #5
   3fdf8:	mov	x0, x27
   3fdfc:	mov	x1, x25
   3fe00:	mov	x3, x19
   3fe04:	mov	x4, x22
   3fe08:	mov	x5, x23
   3fe0c:	mov	x6, x20
   3fe10:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   3fe14:	mov	x0, x20
   3fe18:	mov	x1, x25
   3fe1c:	mov	x2, x28
   3fe20:	mov	x3, x24
   3fe24:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fe28:	mov	x0, x25
   3fe2c:	mov	x1, x27
   3fe30:	mov	x2, x28
   3fe34:	mov	x3, x24
   3fe38:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fe3c:	mov	w5, #0x1                   	// #1
   3fe40:	mov	w6, #0x2                   	// #2
   3fe44:	mov	x0, x25
   3fe48:	mov	x1, x26
   3fe4c:	mov	x2, x20
   3fe50:	mov	w3, wzr
   3fe54:	mov	x4, x22
   3fe58:	bl	c970 <__gmpn_toom_couple_handling@plt>
   3fe5c:	mov	x0, x20
   3fe60:	mov	x1, x19
   3fe64:	mov	x2, x22
   3fe68:	mov	x3, x24
   3fe6c:	bl	c050 <__gmpn_toom2_sqr@plt>
   3fe70:	lsl	x5, x23, #1
   3fe74:	mov	x0, x20
   3fe78:	ldr	x1, [sp, #16]
   3fe7c:	ldp	x3, x2, [x29, #-16]
   3fe80:	mov	x4, x22
   3fe84:	mov	x7, x24
   3fe88:	ldp	x20, x19, [sp, #128]
   3fe8c:	ldp	x22, x21, [sp, #112]
   3fe90:	ldp	x24, x23, [sp, #96]
   3fe94:	ldp	x26, x25, [sp, #80]
   3fe98:	ldp	x28, x27, [sp, #64]
   3fe9c:	ldp	x29, x30, [sp, #48]
   3fea0:	mov	w6, wzr
   3fea4:	add	sp, sp, #0x90
   3fea8:	b	bfb0 <__gmpn_toom_interpolate_12pts@plt>

000000000003feac <__gmpn_toom8h_mul@@Base>:
   3feac:	sub	sp, sp, #0x100
   3feb0:	stp	x29, x30, [sp, #160]
   3feb4:	stp	x28, x27, [sp, #176]
   3feb8:	stp	x20, x19, [sp, #240]
   3febc:	add	x29, sp, #0xa0
   3fec0:	mov	x27, x5
   3fec4:	mov	x16, x1
   3fec8:	cmp	x2, x4
   3fecc:	mov	x20, x0
   3fed0:	stp	x26, x25, [sp, #192]
   3fed4:	stp	x24, x23, [sp, #208]
   3fed8:	stp	x22, x21, [sp, #224]
   3fedc:	stp	x1, x3, [x29, #-16]
   3fee0:	str	x0, [sp, #32]
   3fee4:	b.ne	41040 <__gmpn_toom8h_mul@@Base+0x1194>  // b.any
   3fee8:	sub	x8, x2, #0x1
   3feec:	asr	x8, x8, #3
   3fef0:	add	x23, x8, #0x1
   3fef4:	lsl	x8, x23, #3
   3fef8:	sub	x8, x8, x23
   3fefc:	mov	w10, wzr
   3ff00:	mov	w15, #0x7                   	// #7
   3ff04:	sub	x5, x2, x8
   3ff08:	sub	x26, x4, x8
   3ff0c:	mov	w28, #0x7                   	// #7
   3ff10:	mov	w8, #0xd                   	// #13
   3ff14:	mul	x19, x23, x8
   3ff18:	add	x8, x20, x19, lsl #3
   3ff1c:	mov	w9, #0x58                  	// #88
   3ff20:	add	x25, x8, #0x10
   3ff24:	madd	x1, x23, x9, x20
   3ff28:	mov	w6, #0x3                   	// #3
   3ff2c:	mov	x0, x25
   3ff30:	mov	w2, w15
   3ff34:	mov	x3, x16
   3ff38:	mov	x4, x23
   3ff3c:	mov	x7, x20
   3ff40:	stp	x5, x26, [x29, #-32]
   3ff44:	stur	w10, [x29, #-76]
   3ff48:	str	x1, [sp, #72]
   3ff4c:	stur	x15, [x29, #-48]
   3ff50:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3ff54:	add	x8, x23, x23, lsl #1
   3ff58:	lsl	x8, x8, #5
   3ff5c:	ldur	x3, [x29, #-8]
   3ff60:	add	x9, x27, x8
   3ff64:	add	x8, x20, x8
   3ff68:	add	x21, x9, #0x20
   3ff6c:	add	x24, x8, #0x8
   3ff70:	mov	w22, w0
   3ff74:	mov	w6, #0x3                   	// #3
   3ff78:	mov	x0, x21
   3ff7c:	mov	x1, x24
   3ff80:	mov	w2, w28
   3ff84:	mov	x4, x23
   3ff88:	mov	x5, x26
   3ff8c:	mov	x7, x20
   3ff90:	stur	x28, [x29, #-56]
   3ff94:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   3ff98:	eor	w28, w0, w22
   3ff9c:	cmp	x23, #0x2f
   3ffa0:	add	x22, x23, #0x1
   3ffa4:	stur	x21, [x29, #-64]
   3ffa8:	stp	x24, x25, [sp, #48]
   3ffac:	stur	x19, [x29, #-40]
   3ffb0:	str	x27, [sp, #64]
   3ffb4:	b.le	4002c <__gmpn_toom8h_mul@@Base+0x180>
   3ffb8:	ldr	x9, [sp, #72]
   3ffbc:	cmp	x23, #0x50
   3ffc0:	b.le	40070 <__gmpn_toom8h_mul@@Base+0x1c4>
   3ffc4:	ldp	x25, x27, [sp, #56]
   3ffc8:	ldur	x8, [x29, #-40]
   3ffcc:	cmp	x23, #0xab
   3ffd0:	add	x8, x27, x8, lsl #3
   3ffd4:	b.le	400cc <__gmpn_toom8h_mul@@Base+0x220>
   3ffd8:	cmp	x23, #0xea
   3ffdc:	add	x26, x8, #0x28
   3ffe0:	b.le	40118 <__gmpn_toom8h_mul@@Base+0x26c>
   3ffe4:	ldr	x20, [sp, #32]
   3ffe8:	ldr	x24, [sp, #48]
   3ffec:	mov	x1, x9
   3fff0:	mov	x2, x22
   3fff4:	mov	x0, x20
   3fff8:	mov	x3, x24
   3fffc:	mov	x4, x22
   40000:	mov	x5, x26
   40004:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40008:	ldur	x21, [x29, #-64]
   4000c:	mov	x0, x27
   40010:	mov	x1, x25
   40014:	mov	x2, x22
   40018:	mov	x3, x21
   4001c:	mov	x4, x22
   40020:	mov	x5, x26
   40024:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40028:	b	4015c <__gmpn_toom8h_mul@@Base+0x2b0>
   4002c:	ldr	x1, [sp, #72]
   40030:	add	x8, x27, x19, lsl #3
   40034:	add	x26, x8, #0x28
   40038:	mov	x0, x20
   4003c:	mov	x2, x22
   40040:	mov	x3, x24
   40044:	mov	x4, x22
   40048:	mov	x5, x26
   4004c:	bl	d450 <__gmpn_toom22_mul@plt>
   40050:	mov	x0, x27
   40054:	mov	x1, x25
   40058:	mov	x2, x22
   4005c:	mov	x3, x21
   40060:	mov	x4, x22
   40064:	mov	x5, x26
   40068:	bl	d450 <__gmpn_toom22_mul@plt>
   4006c:	b	4015c <__gmpn_toom8h_mul@@Base+0x2b0>
   40070:	ldur	x8, [x29, #-40]
   40074:	ldr	x27, [sp, #64]
   40078:	ldr	x20, [sp, #32]
   4007c:	ldr	x24, [sp, #48]
   40080:	mov	x1, x9
   40084:	add	x8, x27, x8, lsl #3
   40088:	add	x26, x8, #0x28
   4008c:	mov	x0, x20
   40090:	mov	x2, x22
   40094:	mov	x3, x24
   40098:	mov	x4, x22
   4009c:	mov	x5, x26
   400a0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   400a4:	ldr	x25, [sp, #56]
   400a8:	ldur	x21, [x29, #-64]
   400ac:	mov	x0, x27
   400b0:	mov	x2, x22
   400b4:	mov	x1, x25
   400b8:	mov	x3, x21
   400bc:	mov	x4, x22
   400c0:	mov	x5, x26
   400c4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   400c8:	b	4015c <__gmpn_toom8h_mul@@Base+0x2b0>
   400cc:	ldr	x20, [sp, #32]
   400d0:	ldr	x24, [sp, #48]
   400d4:	add	x26, x8, #0x28
   400d8:	mov	x1, x9
   400dc:	mov	x0, x20
   400e0:	mov	x2, x22
   400e4:	mov	x3, x24
   400e8:	mov	x4, x22
   400ec:	mov	x5, x26
   400f0:	bl	c720 <__gmpn_toom44_mul@plt>
   400f4:	ldur	x21, [x29, #-64]
   400f8:	mov	x0, x27
   400fc:	mov	x1, x25
   40100:	mov	x2, x22
   40104:	mov	x3, x21
   40108:	mov	x4, x22
   4010c:	mov	x5, x26
   40110:	bl	c720 <__gmpn_toom44_mul@plt>
   40114:	b	4015c <__gmpn_toom8h_mul@@Base+0x2b0>
   40118:	ldr	x20, [sp, #32]
   4011c:	ldr	x24, [sp, #48]
   40120:	mov	x1, x9
   40124:	mov	x2, x22
   40128:	mov	x0, x20
   4012c:	mov	x3, x24
   40130:	mov	x4, x22
   40134:	mov	x5, x26
   40138:	bl	cc20 <__gmpn_toom6h_mul@plt>
   4013c:	ldur	x21, [x29, #-64]
   40140:	mov	x0, x27
   40144:	mov	x1, x25
   40148:	mov	x2, x22
   4014c:	mov	x3, x21
   40150:	mov	x4, x22
   40154:	mov	x5, x26
   40158:	bl	cc20 <__gmpn_toom6h_mul@plt>
   4015c:	ldur	w6, [x29, #-76]
   40160:	mov	w19, #0x1                   	// #1
   40164:	bfi	x19, x23, #1, #63
   40168:	mov	x0, x27
   4016c:	add	w8, w6, #0x1
   40170:	bfi	w6, w6, #1, #1
   40174:	add	w5, w6, #0x3
   40178:	mov	x1, x19
   4017c:	mov	x2, x20
   40180:	mov	w3, w28
   40184:	mov	x4, x23
   40188:	str	w8, [sp, #44]
   4018c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40190:	ldr	x1, [sp, #72]
   40194:	ldur	x3, [x29, #-16]
   40198:	ldur	x5, [x29, #-32]
   4019c:	mov	w6, #0x2                   	// #2
   401a0:	mov	x0, x25
   401a4:	ldur	x2, [x29, #-48]
   401a8:	mov	x4, x23
   401ac:	mov	x7, x20
   401b0:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   401b4:	ldur	x3, [x29, #-8]
   401b8:	ldur	x5, [x29, #-24]
   401bc:	mov	w28, w0
   401c0:	mov	w6, #0x2                   	// #2
   401c4:	mov	x0, x21
   401c8:	mov	x1, x24
   401cc:	ldur	x2, [x29, #-56]
   401d0:	mov	x4, x23
   401d4:	mov	x7, x20
   401d8:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   401dc:	cmp	x23, #0x2f
   401e0:	eor	w9, w0, w28
   401e4:	stur	w9, [x29, #-72]
   401e8:	b.le	40264 <__gmpn_toom8h_mul@@Base+0x3b8>
   401ec:	ldr	x1, [sp, #72]
   401f0:	cmp	x23, #0x50
   401f4:	b.le	402b8 <__gmpn_toom8h_mul@@Base+0x40c>
   401f8:	ldur	x8, [x29, #-40]
   401fc:	cmp	x23, #0xab
   40200:	b.le	40314 <__gmpn_toom8h_mul@@Base+0x468>
   40204:	ldr	x27, [sp, #64]
   40208:	cmp	x23, #0xea
   4020c:	add	x8, x27, x8, lsl #3
   40210:	add	x26, x8, #0x28
   40214:	b.le	4036c <__gmpn_toom8h_mul@@Base+0x4c0>
   40218:	ldr	x20, [sp, #32]
   4021c:	ldr	x24, [sp, #48]
   40220:	mov	x2, x22
   40224:	mov	x4, x22
   40228:	mov	x0, x20
   4022c:	mov	x3, x24
   40230:	mov	x5, x26
   40234:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40238:	ldr	x25, [sp, #56]
   4023c:	add	x28, x23, x23, lsl #1
   40240:	add	x8, x27, x28, lsl #3
   40244:	add	x0, x8, #0x8
   40248:	mov	x1, x25
   4024c:	mov	x2, x22
   40250:	mov	x3, x21
   40254:	mov	x4, x22
   40258:	mov	x5, x26
   4025c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40260:	b	403b4 <__gmpn_toom8h_mul@@Base+0x508>
   40264:	ldur	x8, [x29, #-40]
   40268:	ldr	x1, [sp, #72]
   4026c:	mov	x0, x20
   40270:	mov	x2, x22
   40274:	add	x8, x27, x8, lsl #3
   40278:	add	x26, x8, #0x28
   4027c:	mov	x3, x24
   40280:	mov	x4, x22
   40284:	mov	x5, x26
   40288:	bl	d450 <__gmpn_toom22_mul@plt>
   4028c:	add	x28, x23, x23, lsl #1
   40290:	mov	x5, x26
   40294:	ldur	w26, [x29, #-76]
   40298:	add	x8, x27, x28, lsl #3
   4029c:	add	x0, x8, #0x8
   402a0:	mov	x1, x25
   402a4:	mov	x2, x22
   402a8:	mov	x3, x21
   402ac:	mov	x4, x22
   402b0:	bl	d450 <__gmpn_toom22_mul@plt>
   402b4:	b	403b8 <__gmpn_toom8h_mul@@Base+0x50c>
   402b8:	ldur	x8, [x29, #-40]
   402bc:	ldr	x27, [sp, #64]
   402c0:	ldr	x20, [sp, #32]
   402c4:	ldr	x24, [sp, #48]
   402c8:	mov	x2, x22
   402cc:	add	x8, x27, x8, lsl #3
   402d0:	add	x26, x8, #0x28
   402d4:	mov	x0, x20
   402d8:	mov	x3, x24
   402dc:	mov	x4, x22
   402e0:	mov	x5, x26
   402e4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   402e8:	ldr	x25, [sp, #56]
   402ec:	add	x28, x23, x23, lsl #1
   402f0:	add	x8, x27, x28, lsl #3
   402f4:	add	x0, x8, #0x8
   402f8:	mov	x1, x25
   402fc:	mov	x2, x22
   40300:	mov	x3, x21
   40304:	mov	x4, x22
   40308:	mov	x5, x26
   4030c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40310:	b	403b4 <__gmpn_toom8h_mul@@Base+0x508>
   40314:	ldr	x27, [sp, #64]
   40318:	ldr	x20, [sp, #32]
   4031c:	ldr	x24, [sp, #48]
   40320:	mov	x2, x22
   40324:	add	x8, x27, x8, lsl #3
   40328:	add	x26, x8, #0x28
   4032c:	mov	x0, x20
   40330:	mov	x3, x24
   40334:	mov	x4, x22
   40338:	mov	x5, x26
   4033c:	bl	c720 <__gmpn_toom44_mul@plt>
   40340:	ldr	x25, [sp, #56]
   40344:	add	x28, x23, x23, lsl #1
   40348:	add	x8, x27, x28, lsl #3
   4034c:	add	x0, x8, #0x8
   40350:	mov	x1, x25
   40354:	mov	x2, x22
   40358:	mov	x3, x21
   4035c:	mov	x4, x22
   40360:	mov	x5, x26
   40364:	bl	c720 <__gmpn_toom44_mul@plt>
   40368:	b	403b4 <__gmpn_toom8h_mul@@Base+0x508>
   4036c:	ldr	x20, [sp, #32]
   40370:	ldr	x24, [sp, #48]
   40374:	mov	x2, x22
   40378:	mov	x4, x22
   4037c:	mov	x0, x20
   40380:	mov	x3, x24
   40384:	mov	x5, x26
   40388:	bl	cc20 <__gmpn_toom6h_mul@plt>
   4038c:	ldr	x25, [sp, #56]
   40390:	add	x28, x23, x23, lsl #1
   40394:	add	x8, x27, x28, lsl #3
   40398:	add	x0, x8, #0x8
   4039c:	mov	x1, x25
   403a0:	mov	x2, x22
   403a4:	mov	x3, x21
   403a8:	mov	x4, x22
   403ac:	mov	x5, x26
   403b0:	bl	cc20 <__gmpn_toom6h_mul@plt>
   403b4:	ldur	w26, [x29, #-76]
   403b8:	ldr	w9, [sp, #44]
   403bc:	ldur	w3, [x29, #-72]
   403c0:	add	x8, x27, x28, lsl #3
   403c4:	add	x0, x8, #0x8
   403c8:	lsl	w5, w9, #1
   403cc:	lsl	w6, w26, #1
   403d0:	mov	x1, x19
   403d4:	mov	x2, x20
   403d8:	mov	x4, x23
   403dc:	stp	x28, x0, [sp, #16]
   403e0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   403e4:	ldr	x28, [sp, #72]
   403e8:	ldur	x3, [x29, #-16]
   403ec:	ldur	x5, [x29, #-32]
   403f0:	mov	x0, x25
   403f4:	mov	x1, x28
   403f8:	ldur	x2, [x29, #-48]
   403fc:	mov	x4, x23
   40400:	mov	x6, x20
   40404:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   40408:	ldur	x3, [x29, #-8]
   4040c:	ldur	x5, [x29, #-24]
   40410:	mov	w26, w0
   40414:	mov	x0, x21
   40418:	mov	x1, x24
   4041c:	ldur	x2, [x29, #-56]
   40420:	mov	x4, x23
   40424:	mov	x6, x20
   40428:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   4042c:	cmp	x23, #0x2f
   40430:	eor	w8, w0, w26
   40434:	stur	x19, [x29, #-72]
   40438:	str	w8, [sp, #12]
   4043c:	b.le	404bc <__gmpn_toom8h_mul@@Base+0x610>
   40440:	cmp	x23, #0x50
   40444:	b.le	40518 <__gmpn_toom8h_mul@@Base+0x66c>
   40448:	ldp	x25, x19, [sp, #56]
   4044c:	cmp	x23, #0xab
   40450:	b.le	40580 <__gmpn_toom8h_mul@@Base+0x6d4>
   40454:	ldur	x8, [x29, #-40]
   40458:	cmp	x23, #0xea
   4045c:	add	x8, x19, x8, lsl #3
   40460:	add	x26, x8, #0x28
   40464:	b.le	405e0 <__gmpn_toom8h_mul@@Base+0x734>
   40468:	ldr	x20, [sp, #32]
   4046c:	mov	x1, x28
   40470:	mov	x2, x22
   40474:	mov	x3, x24
   40478:	mov	x0, x20
   4047c:	mov	x4, x22
   40480:	mov	x5, x26
   40484:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40488:	ldur	x21, [x29, #-64]
   4048c:	add	x8, x23, x23, lsl #1
   40490:	mov	x27, x24
   40494:	lsl	x24, x8, #1
   40498:	add	x8, x19, x8, lsl #4
   4049c:	add	x0, x8, #0x10
   404a0:	mov	x1, x25
   404a4:	mov	x2, x22
   404a8:	mov	x3, x21
   404ac:	mov	x4, x22
   404b0:	mov	x5, x26
   404b4:	bl	cb20 <__gmpn_toom8h_mul@plt>
   404b8:	b	40630 <__gmpn_toom8h_mul@@Base+0x784>
   404bc:	ldur	x8, [x29, #-40]
   404c0:	mov	x0, x20
   404c4:	mov	x1, x28
   404c8:	mov	x2, x22
   404cc:	add	x8, x27, x8, lsl #3
   404d0:	add	x26, x8, #0x28
   404d4:	mov	x3, x24
   404d8:	mov	x4, x22
   404dc:	mov	x5, x26
   404e0:	bl	d450 <__gmpn_toom22_mul@plt>
   404e4:	add	x8, x23, x23, lsl #1
   404e8:	mov	x19, x27
   404ec:	mov	x27, x24
   404f0:	lsl	x24, x8, #1
   404f4:	add	x8, x19, x8, lsl #4
   404f8:	add	x0, x8, #0x10
   404fc:	mov	x1, x25
   40500:	mov	x2, x22
   40504:	mov	x3, x21
   40508:	mov	x4, x22
   4050c:	mov	x5, x26
   40510:	bl	d450 <__gmpn_toom22_mul@plt>
   40514:	b	40630 <__gmpn_toom8h_mul@@Base+0x784>
   40518:	ldur	x8, [x29, #-40]
   4051c:	ldr	x19, [sp, #64]
   40520:	ldr	x20, [sp, #32]
   40524:	mov	x1, x28
   40528:	mov	x2, x22
   4052c:	add	x8, x19, x8, lsl #3
   40530:	add	x26, x8, #0x28
   40534:	mov	x0, x20
   40538:	mov	x3, x24
   4053c:	mov	x4, x22
   40540:	mov	x5, x26
   40544:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40548:	ldr	x25, [sp, #56]
   4054c:	ldur	x21, [x29, #-64]
   40550:	add	x8, x23, x23, lsl #1
   40554:	mov	x27, x24
   40558:	lsl	x24, x8, #1
   4055c:	add	x8, x19, x8, lsl #4
   40560:	add	x0, x8, #0x10
   40564:	mov	x1, x25
   40568:	mov	x2, x22
   4056c:	mov	x3, x21
   40570:	mov	x4, x22
   40574:	mov	x5, x26
   40578:	bl	c0a0 <__gmpn_toom33_mul@plt>
   4057c:	b	40630 <__gmpn_toom8h_mul@@Base+0x784>
   40580:	ldur	x8, [x29, #-40]
   40584:	ldr	x20, [sp, #32]
   40588:	mov	x1, x28
   4058c:	mov	x2, x22
   40590:	add	x8, x19, x8, lsl #3
   40594:	add	x26, x8, #0x28
   40598:	mov	x0, x20
   4059c:	mov	x3, x24
   405a0:	mov	x4, x22
   405a4:	mov	x5, x26
   405a8:	bl	c720 <__gmpn_toom44_mul@plt>
   405ac:	ldur	x21, [x29, #-64]
   405b0:	add	x8, x23, x23, lsl #1
   405b4:	mov	x27, x24
   405b8:	lsl	x24, x8, #1
   405bc:	add	x8, x19, x8, lsl #4
   405c0:	add	x0, x8, #0x10
   405c4:	mov	x1, x25
   405c8:	mov	x2, x22
   405cc:	mov	x3, x21
   405d0:	mov	x4, x22
   405d4:	mov	x5, x26
   405d8:	bl	c720 <__gmpn_toom44_mul@plt>
   405dc:	b	40630 <__gmpn_toom8h_mul@@Base+0x784>
   405e0:	ldr	x20, [sp, #32]
   405e4:	mov	x1, x28
   405e8:	mov	x2, x22
   405ec:	mov	x3, x24
   405f0:	mov	x0, x20
   405f4:	mov	x4, x22
   405f8:	mov	x5, x26
   405fc:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40600:	ldur	x21, [x29, #-64]
   40604:	add	x8, x23, x23, lsl #1
   40608:	mov	x27, x24
   4060c:	lsl	x24, x8, #1
   40610:	add	x8, x19, x8, lsl #4
   40614:	add	x0, x8, #0x10
   40618:	mov	x1, x25
   4061c:	mov	x2, x22
   40620:	mov	x3, x21
   40624:	mov	x4, x22
   40628:	mov	x5, x26
   4062c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40630:	ldur	x1, [x29, #-72]
   40634:	ldr	w3, [sp, #12]
   40638:	ldur	x26, [x29, #-32]
   4063c:	add	x8, x19, x24, lsl #3
   40640:	add	x0, x8, #0x10
   40644:	mov	w5, #0x1                   	// #1
   40648:	mov	w6, #0x2                   	// #2
   4064c:	mov	x2, x20
   40650:	mov	x4, x23
   40654:	str	x0, [sp, #72]
   40658:	bl	c970 <__gmpn_toom_couple_handling@plt>
   4065c:	ldur	x3, [x29, #-16]
   40660:	mov	w6, #0x3                   	// #3
   40664:	mov	x0, x25
   40668:	mov	x1, x28
   4066c:	ldur	x2, [x29, #-48]
   40670:	mov	x4, x23
   40674:	mov	x5, x26
   40678:	mov	x7, x20
   4067c:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   40680:	ldur	x3, [x29, #-8]
   40684:	ldur	x5, [x29, #-24]
   40688:	mov	w26, w0
   4068c:	mov	w6, #0x3                   	// #3
   40690:	mov	x0, x21
   40694:	mov	x1, x27
   40698:	ldur	x2, [x29, #-56]
   4069c:	mov	x4, x23
   406a0:	mov	x7, x20
   406a4:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   406a8:	cmp	x23, #0x2f
   406ac:	eor	w9, w0, w26
   406b0:	str	w9, [sp, #12]
   406b4:	b.le	40730 <__gmpn_toom8h_mul@@Base+0x884>
   406b8:	ldur	x8, [x29, #-40]
   406bc:	cmp	x23, #0x50
   406c0:	b.le	40784 <__gmpn_toom8h_mul@@Base+0x8d8>
   406c4:	ldr	x25, [sp, #56]
   406c8:	cmp	x23, #0xab
   406cc:	add	x8, x19, x8, lsl #3
   406d0:	b.le	407dc <__gmpn_toom8h_mul@@Base+0x930>
   406d4:	cmp	x23, #0xea
   406d8:	add	x26, x8, #0x28
   406dc:	mov	x24, x19
   406e0:	mov	x0, x20
   406e4:	mov	x1, x28
   406e8:	mov	x2, x22
   406ec:	b.le	4082c <__gmpn_toom8h_mul@@Base+0x980>
   406f0:	ldr	x27, [sp, #48]
   406f4:	mov	x4, x22
   406f8:	mov	x5, x26
   406fc:	mov	x3, x27
   40700:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40704:	ldur	x21, [x29, #-64]
   40708:	add	x19, x23, x23, lsl #3
   4070c:	add	x8, x24, x19, lsl #3
   40710:	add	x0, x8, #0x18
   40714:	mov	x1, x25
   40718:	mov	x2, x22
   4071c:	mov	x3, x21
   40720:	mov	x4, x22
   40724:	mov	x5, x26
   40728:	bl	cb20 <__gmpn_toom8h_mul@plt>
   4072c:	b	40868 <__gmpn_toom8h_mul@@Base+0x9bc>
   40730:	ldur	x8, [x29, #-40]
   40734:	mov	x0, x20
   40738:	mov	x1, x28
   4073c:	mov	x2, x22
   40740:	add	x8, x19, x8, lsl #3
   40744:	add	x26, x8, #0x28
   40748:	mov	x3, x27
   4074c:	mov	x4, x22
   40750:	mov	x5, x26
   40754:	bl	d450 <__gmpn_toom22_mul@plt>
   40758:	mov	x24, x19
   4075c:	add	x19, x23, x23, lsl #3
   40760:	add	x8, x24, x19, lsl #3
   40764:	add	x0, x8, #0x18
   40768:	mov	x1, x25
   4076c:	mov	x2, x22
   40770:	mov	x3, x21
   40774:	mov	x4, x22
   40778:	mov	x5, x26
   4077c:	bl	d450 <__gmpn_toom22_mul@plt>
   40780:	b	40868 <__gmpn_toom8h_mul@@Base+0x9bc>
   40784:	add	x8, x19, x8, lsl #3
   40788:	add	x26, x8, #0x28
   4078c:	mov	x0, x20
   40790:	mov	x1, x28
   40794:	mov	x2, x22
   40798:	mov	x3, x27
   4079c:	mov	x4, x22
   407a0:	mov	x5, x26
   407a4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   407a8:	ldr	x25, [sp, #56]
   407ac:	ldur	x21, [x29, #-64]
   407b0:	mov	x24, x19
   407b4:	add	x19, x23, x23, lsl #3
   407b8:	add	x8, x24, x19, lsl #3
   407bc:	add	x0, x8, #0x18
   407c0:	mov	x1, x25
   407c4:	mov	x2, x22
   407c8:	mov	x3, x21
   407cc:	mov	x4, x22
   407d0:	mov	x5, x26
   407d4:	bl	c0a0 <__gmpn_toom33_mul@plt>
   407d8:	b	40868 <__gmpn_toom8h_mul@@Base+0x9bc>
   407dc:	add	x26, x8, #0x28
   407e0:	mov	x0, x20
   407e4:	mov	x1, x28
   407e8:	mov	x2, x22
   407ec:	mov	x3, x27
   407f0:	mov	x4, x22
   407f4:	mov	x5, x26
   407f8:	bl	c720 <__gmpn_toom44_mul@plt>
   407fc:	ldur	x21, [x29, #-64]
   40800:	mov	x24, x19
   40804:	add	x19, x23, x23, lsl #3
   40808:	add	x8, x24, x19, lsl #3
   4080c:	add	x0, x8, #0x18
   40810:	mov	x1, x25
   40814:	mov	x2, x22
   40818:	mov	x3, x21
   4081c:	mov	x4, x22
   40820:	mov	x5, x26
   40824:	bl	c720 <__gmpn_toom44_mul@plt>
   40828:	b	40868 <__gmpn_toom8h_mul@@Base+0x9bc>
   4082c:	ldr	x27, [sp, #48]
   40830:	mov	x4, x22
   40834:	mov	x5, x26
   40838:	mov	x3, x27
   4083c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40840:	ldur	x21, [x29, #-64]
   40844:	add	x19, x23, x23, lsl #3
   40848:	add	x8, x24, x19, lsl #3
   4084c:	add	x0, x8, #0x18
   40850:	mov	x1, x25
   40854:	mov	x2, x22
   40858:	mov	x3, x21
   4085c:	mov	x4, x22
   40860:	mov	x5, x26
   40864:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40868:	ldur	x1, [x29, #-72]
   4086c:	ldr	w3, [sp, #12]
   40870:	ldur	x26, [x29, #-32]
   40874:	add	x8, x24, x19, lsl #3
   40878:	add	x0, x8, #0x18
   4087c:	mov	w5, #0x3                   	// #3
   40880:	mov	w6, #0x6                   	// #6
   40884:	mov	x2, x20
   40888:	mov	x4, x23
   4088c:	str	x0, [sp, #32]
   40890:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40894:	ldur	x3, [x29, #-16]
   40898:	mov	w6, #0x1                   	// #1
   4089c:	mov	x0, x25
   408a0:	mov	x1, x28
   408a4:	ldur	x2, [x29, #-48]
   408a8:	mov	x4, x23
   408ac:	mov	x5, x26
   408b0:	mov	x7, x20
   408b4:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   408b8:	ldur	x3, [x29, #-8]
   408bc:	ldur	x5, [x29, #-24]
   408c0:	mov	w26, w0
   408c4:	mov	w6, #0x1                   	// #1
   408c8:	mov	x0, x21
   408cc:	mov	x1, x27
   408d0:	ldur	x2, [x29, #-56]
   408d4:	mov	x4, x23
   408d8:	mov	x7, x20
   408dc:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   408e0:	cmp	x23, #0x2f
   408e4:	eor	w9, w0, w26
   408e8:	mov	x19, x24
   408ec:	str	w9, [sp, #12]
   408f0:	b.le	40964 <__gmpn_toom8h_mul@@Base+0xab8>
   408f4:	ldur	x24, [x29, #-16]
   408f8:	ldur	x8, [x29, #-40]
   408fc:	mov	x27, x28
   40900:	cmp	x23, #0x50
   40904:	b.le	409bc <__gmpn_toom8h_mul@@Base+0xb10>
   40908:	ldp	x21, x25, [sp, #48]
   4090c:	cmp	x23, #0xab
   40910:	add	x8, x19, x8, lsl #3
   40914:	b.le	40a0c <__gmpn_toom8h_mul@@Base+0xb60>
   40918:	ldr	x28, [sp, #16]
   4091c:	add	x26, x8, #0x28
   40920:	cmp	x23, #0xea
   40924:	mov	x0, x20
   40928:	mov	x1, x27
   4092c:	mov	x2, x22
   40930:	mov	x3, x21
   40934:	mov	x4, x22
   40938:	mov	x5, x26
   4093c:	b.le	40a50 <__gmpn_toom8h_mul@@Base+0xba4>
   40940:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40944:	ldur	x3, [x29, #-64]
   40948:	add	x0, x20, x28, lsl #3
   4094c:	mov	x1, x25
   40950:	mov	x2, x22
   40954:	mov	x4, x22
   40958:	mov	x5, x26
   4095c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40960:	b	40a70 <__gmpn_toom8h_mul@@Base+0xbc4>
   40964:	ldur	x8, [x29, #-40]
   40968:	mov	x0, x20
   4096c:	mov	x1, x28
   40970:	mov	x2, x22
   40974:	add	x8, x19, x8, lsl #3
   40978:	add	x26, x8, #0x28
   4097c:	mov	x3, x27
   40980:	mov	x4, x22
   40984:	mov	x5, x26
   40988:	bl	d450 <__gmpn_toom22_mul@plt>
   4098c:	mov	x3, x21
   40990:	mov	x21, x27
   40994:	mov	x27, x28
   40998:	ldr	x28, [sp, #16]
   4099c:	mov	x1, x25
   409a0:	mov	x2, x22
   409a4:	mov	x4, x22
   409a8:	add	x0, x20, x28, lsl #3
   409ac:	mov	x5, x26
   409b0:	bl	d450 <__gmpn_toom22_mul@plt>
   409b4:	ldur	x24, [x29, #-16]
   409b8:	b	40a70 <__gmpn_toom8h_mul@@Base+0xbc4>
   409bc:	ldr	x21, [sp, #48]
   409c0:	add	x8, x19, x8, lsl #3
   409c4:	add	x26, x8, #0x28
   409c8:	mov	x0, x20
   409cc:	mov	x1, x27
   409d0:	mov	x2, x22
   409d4:	mov	x3, x21
   409d8:	mov	x4, x22
   409dc:	mov	x5, x26
   409e0:	bl	c0a0 <__gmpn_toom33_mul@plt>
   409e4:	ldr	x28, [sp, #16]
   409e8:	ldr	x25, [sp, #56]
   409ec:	ldur	x3, [x29, #-64]
   409f0:	mov	x2, x22
   409f4:	add	x0, x20, x28, lsl #3
   409f8:	mov	x1, x25
   409fc:	mov	x4, x22
   40a00:	mov	x5, x26
   40a04:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40a08:	b	40a70 <__gmpn_toom8h_mul@@Base+0xbc4>
   40a0c:	add	x26, x8, #0x28
   40a10:	mov	x0, x20
   40a14:	mov	x1, x27
   40a18:	mov	x2, x22
   40a1c:	mov	x3, x21
   40a20:	mov	x4, x22
   40a24:	mov	x5, x26
   40a28:	bl	c720 <__gmpn_toom44_mul@plt>
   40a2c:	ldr	x28, [sp, #16]
   40a30:	ldur	x3, [x29, #-64]
   40a34:	mov	x1, x25
   40a38:	mov	x2, x22
   40a3c:	add	x0, x20, x28, lsl #3
   40a40:	mov	x4, x22
   40a44:	mov	x5, x26
   40a48:	bl	c720 <__gmpn_toom44_mul@plt>
   40a4c:	b	40a70 <__gmpn_toom8h_mul@@Base+0xbc4>
   40a50:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40a54:	ldur	x3, [x29, #-64]
   40a58:	add	x0, x20, x28, lsl #3
   40a5c:	mov	x1, x25
   40a60:	mov	x2, x22
   40a64:	mov	x4, x22
   40a68:	mov	x5, x26
   40a6c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40a70:	ldur	w6, [x29, #-76]
   40a74:	ldur	x1, [x29, #-72]
   40a78:	ldr	w3, [sp, #12]
   40a7c:	ldr	w5, [sp, #44]
   40a80:	ldur	x26, [x29, #-32]
   40a84:	add	x0, x20, x28, lsl #3
   40a88:	mov	x2, x20
   40a8c:	mov	x4, x23
   40a90:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40a94:	mov	x0, x25
   40a98:	mov	x1, x27
   40a9c:	ldur	x2, [x29, #-48]
   40aa0:	mov	x3, x24
   40aa4:	mov	x4, x23
   40aa8:	mov	x5, x26
   40aac:	mov	x6, x20
   40ab0:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   40ab4:	ldur	x2, [x29, #-56]
   40ab8:	mov	w28, w0
   40abc:	cmp	w2, #0x3
   40ac0:	b.eq	41078 <__gmpn_toom8h_mul@@Base+0x11cc>  // b.none
   40ac4:	ldur	x0, [x29, #-64]
   40ac8:	ldur	x3, [x29, #-8]
   40acc:	ldur	x5, [x29, #-24]
   40ad0:	mov	x1, x21
   40ad4:	mov	x4, x23
   40ad8:	mov	x6, x20
   40adc:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   40ae0:	cmp	x23, #0x2f
   40ae4:	eor	w28, w0, w28
   40ae8:	b.le	40b5c <__gmpn_toom8h_mul@@Base+0xcb0>
   40aec:	cmp	x23, #0x50
   40af0:	b.le	40bb4 <__gmpn_toom8h_mul@@Base+0xd08>
   40af4:	cmp	x23, #0xab
   40af8:	b.le	40c0c <__gmpn_toom8h_mul@@Base+0xd60>
   40afc:	ldur	x8, [x29, #-40]
   40b00:	cmp	x23, #0xea
   40b04:	mov	x0, x20
   40b08:	mov	x1, x27
   40b0c:	add	x8, x19, x8, lsl #3
   40b10:	add	x8, x8, #0x28
   40b14:	mov	x2, x22
   40b18:	mov	x3, x21
   40b1c:	mov	x4, x22
   40b20:	mov	x5, x8
   40b24:	str	x8, [sp, #56]
   40b28:	b.le	40c64 <__gmpn_toom8h_mul@@Base+0xdb8>
   40b2c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40b30:	ldur	x24, [x29, #-64]
   40b34:	ldr	x5, [sp, #56]
   40b38:	lsl	x8, x23, #3
   40b3c:	sub	x19, x8, x23
   40b40:	add	x0, x20, x19, lsl #3
   40b44:	mov	x1, x25
   40b48:	mov	x2, x22
   40b4c:	mov	x3, x24
   40b50:	mov	x4, x22
   40b54:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40b58:	b	40c90 <__gmpn_toom8h_mul@@Base+0xde4>
   40b5c:	ldur	x8, [x29, #-40]
   40b60:	mov	x0, x20
   40b64:	mov	x1, x27
   40b68:	mov	x2, x22
   40b6c:	add	x8, x19, x8, lsl #3
   40b70:	add	x26, x8, #0x28
   40b74:	mov	x3, x21
   40b78:	mov	x4, x22
   40b7c:	mov	x5, x26
   40b80:	bl	d450 <__gmpn_toom22_mul@plt>
   40b84:	ldur	x24, [x29, #-64]
   40b88:	lsl	x8, x23, #3
   40b8c:	mov	x5, x26
   40b90:	ldur	x26, [x29, #-32]
   40b94:	sub	x19, x8, x23
   40b98:	add	x0, x20, x19, lsl #3
   40b9c:	mov	x1, x25
   40ba0:	mov	x2, x22
   40ba4:	mov	x3, x24
   40ba8:	mov	x4, x22
   40bac:	bl	d450 <__gmpn_toom22_mul@plt>
   40bb0:	b	40c90 <__gmpn_toom8h_mul@@Base+0xde4>
   40bb4:	ldur	x8, [x29, #-40]
   40bb8:	mov	x0, x20
   40bbc:	mov	x1, x27
   40bc0:	mov	x2, x22
   40bc4:	add	x8, x19, x8, lsl #3
   40bc8:	add	x26, x8, #0x28
   40bcc:	mov	x3, x21
   40bd0:	mov	x4, x22
   40bd4:	mov	x5, x26
   40bd8:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40bdc:	ldur	x24, [x29, #-64]
   40be0:	lsl	x8, x23, #3
   40be4:	mov	x5, x26
   40be8:	ldur	x26, [x29, #-32]
   40bec:	sub	x19, x8, x23
   40bf0:	add	x0, x20, x19, lsl #3
   40bf4:	mov	x1, x25
   40bf8:	mov	x2, x22
   40bfc:	mov	x3, x24
   40c00:	mov	x4, x22
   40c04:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40c08:	b	40c90 <__gmpn_toom8h_mul@@Base+0xde4>
   40c0c:	ldur	x8, [x29, #-40]
   40c10:	mov	x0, x20
   40c14:	mov	x1, x27
   40c18:	mov	x2, x22
   40c1c:	add	x8, x19, x8, lsl #3
   40c20:	add	x26, x8, #0x28
   40c24:	mov	x3, x21
   40c28:	mov	x4, x22
   40c2c:	mov	x5, x26
   40c30:	bl	c720 <__gmpn_toom44_mul@plt>
   40c34:	ldur	x24, [x29, #-64]
   40c38:	lsl	x8, x23, #3
   40c3c:	mov	x5, x26
   40c40:	ldur	x26, [x29, #-32]
   40c44:	sub	x19, x8, x23
   40c48:	add	x0, x20, x19, lsl #3
   40c4c:	mov	x1, x25
   40c50:	mov	x2, x22
   40c54:	mov	x3, x24
   40c58:	mov	x4, x22
   40c5c:	bl	c720 <__gmpn_toom44_mul@plt>
   40c60:	b	40c90 <__gmpn_toom8h_mul@@Base+0xde4>
   40c64:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40c68:	ldur	x24, [x29, #-64]
   40c6c:	ldr	x5, [sp, #56]
   40c70:	lsl	x8, x23, #3
   40c74:	sub	x19, x8, x23
   40c78:	add	x0, x20, x19, lsl #3
   40c7c:	mov	x1, x25
   40c80:	mov	x2, x22
   40c84:	mov	x3, x24
   40c88:	mov	x4, x22
   40c8c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40c90:	add	x0, x20, x19, lsl #3
   40c94:	ldur	x19, [x29, #-72]
   40c98:	mov	x2, x20
   40c9c:	mov	w3, w28
   40ca0:	mov	x4, x23
   40ca4:	mov	x1, x19
   40ca8:	mov	w5, wzr
   40cac:	mov	w6, wzr
   40cb0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40cb4:	ldur	x3, [x29, #-16]
   40cb8:	mov	w6, #0x2                   	// #2
   40cbc:	mov	x0, x25
   40cc0:	mov	x1, x27
   40cc4:	ldur	x2, [x29, #-48]
   40cc8:	mov	x4, x23
   40ccc:	mov	x5, x26
   40cd0:	mov	x7, x20
   40cd4:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   40cd8:	ldur	x3, [x29, #-8]
   40cdc:	ldur	x5, [x29, #-24]
   40ce0:	mov	w26, w0
   40ce4:	mov	w6, #0x2                   	// #2
   40ce8:	mov	x0, x24
   40cec:	mov	x1, x21
   40cf0:	ldur	x2, [x29, #-56]
   40cf4:	mov	x4, x23
   40cf8:	mov	x7, x20
   40cfc:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   40d00:	cmp	x23, #0x2f
   40d04:	eor	w28, w0, w26
   40d08:	b.le	40dc0 <__gmpn_toom8h_mul@@Base+0xf14>
   40d0c:	ldr	x9, [sp, #64]
   40d10:	cmp	x23, #0x51
   40d14:	b.lt	40e38 <__gmpn_toom8h_mul@@Base+0xf8c>  // b.tstop
   40d18:	ldur	x8, [x29, #-40]
   40d1c:	ldr	x3, [sp, #48]
   40d20:	cmp	x23, #0xac
   40d24:	add	x8, x9, x8, lsl #3
   40d28:	b.lt	40ed0 <__gmpn_toom8h_mul@@Base+0x1024>  // b.tstop
   40d2c:	add	x19, x8, #0x28
   40d30:	cmp	x23, #0xea
   40d34:	mov	x0, x20
   40d38:	mov	x1, x27
   40d3c:	mov	x2, x22
   40d40:	mov	x4, x22
   40d44:	mov	x5, x19
   40d48:	b.le	40f5c <__gmpn_toom8h_mul@@Base+0x10b0>
   40d4c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40d50:	ldur	x26, [x29, #-64]
   40d54:	mov	x0, x27
   40d58:	mov	x1, x25
   40d5c:	mov	x2, x22
   40d60:	mov	x3, x26
   40d64:	mov	x4, x22
   40d68:	mov	x5, x19
   40d6c:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40d70:	ldur	x1, [x29, #-72]
   40d74:	mov	w5, #0x2                   	// #2
   40d78:	mov	w6, #0x4                   	// #4
   40d7c:	mov	x0, x27
   40d80:	mov	x2, x20
   40d84:	mov	w3, w28
   40d88:	mov	x4, x23
   40d8c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40d90:	ldur	w21, [x29, #-76]
   40d94:	ldp	x22, x25, [x29, #-32]
   40d98:	cmp	x23, #0xeb
   40d9c:	b.eq	40fb0 <__gmpn_toom8h_mul@@Base+0x1104>  // b.none
   40da0:	ldp	x28, x3, [x29, #-16]
   40da4:	mov	x0, x20
   40da8:	mov	x2, x23
   40dac:	mov	x4, x23
   40db0:	mov	x1, x28
   40db4:	mov	x5, x26
   40db8:	bl	cb20 <__gmpn_toom8h_mul@plt>
   40dbc:	b	40fec <__gmpn_toom8h_mul@@Base+0x1140>
   40dc0:	ldur	x8, [x29, #-40]
   40dc4:	ldr	x9, [sp, #64]
   40dc8:	mov	x0, x20
   40dcc:	mov	x1, x27
   40dd0:	mov	x2, x22
   40dd4:	add	x8, x9, x8, lsl #3
   40dd8:	add	x26, x8, #0x28
   40ddc:	mov	x3, x21
   40de0:	mov	x4, x22
   40de4:	mov	x5, x26
   40de8:	bl	d450 <__gmpn_toom22_mul@plt>
   40dec:	mov	x0, x27
   40df0:	mov	x1, x25
   40df4:	mov	x2, x22
   40df8:	mov	x3, x24
   40dfc:	mov	x4, x22
   40e00:	mov	x5, x26
   40e04:	bl	d450 <__gmpn_toom22_mul@plt>
   40e08:	mov	w5, #0x2                   	// #2
   40e0c:	mov	w6, #0x4                   	// #4
   40e10:	mov	x0, x27
   40e14:	mov	x1, x19
   40e18:	mov	x2, x20
   40e1c:	mov	w3, w28
   40e20:	mov	x4, x23
   40e24:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40e28:	ldur	w21, [x29, #-76]
   40e2c:	ldp	x22, x25, [x29, #-32]
   40e30:	mov	x26, x24
   40e34:	b	40eb0 <__gmpn_toom8h_mul@@Base+0x1004>
   40e38:	ldur	x8, [x29, #-40]
   40e3c:	ldr	x3, [sp, #48]
   40e40:	mov	x0, x20
   40e44:	mov	x1, x27
   40e48:	add	x8, x9, x8, lsl #3
   40e4c:	add	x26, x8, #0x28
   40e50:	mov	x2, x22
   40e54:	mov	x4, x22
   40e58:	mov	x5, x26
   40e5c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40e60:	ldur	x3, [x29, #-64]
   40e64:	mov	x0, x27
   40e68:	mov	x1, x25
   40e6c:	mov	x2, x22
   40e70:	mov	x4, x22
   40e74:	mov	x5, x26
   40e78:	mov	x26, x3
   40e7c:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40e80:	ldur	x1, [x29, #-72]
   40e84:	mov	w5, #0x2                   	// #2
   40e88:	mov	w6, #0x4                   	// #4
   40e8c:	mov	x0, x27
   40e90:	mov	x2, x20
   40e94:	mov	w3, w28
   40e98:	mov	x4, x23
   40e9c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40ea0:	ldur	w21, [x29, #-76]
   40ea4:	ldp	x22, x25, [x29, #-32]
   40ea8:	cmp	x23, #0x30
   40eac:	b.gt	40f3c <__gmpn_toom8h_mul@@Base+0x1090>
   40eb0:	ldp	x28, x3, [x29, #-16]
   40eb4:	mov	x0, x20
   40eb8:	mov	x2, x23
   40ebc:	mov	x4, x23
   40ec0:	mov	x1, x28
   40ec4:	mov	x5, x26
   40ec8:	bl	d450 <__gmpn_toom22_mul@plt>
   40ecc:	b	40fec <__gmpn_toom8h_mul@@Base+0x1140>
   40ed0:	add	x26, x8, #0x28
   40ed4:	mov	x0, x20
   40ed8:	mov	x1, x27
   40edc:	mov	x2, x22
   40ee0:	mov	x4, x22
   40ee4:	mov	x5, x26
   40ee8:	bl	c720 <__gmpn_toom44_mul@plt>
   40eec:	ldur	x3, [x29, #-64]
   40ef0:	mov	x0, x27
   40ef4:	mov	x1, x25
   40ef8:	mov	x2, x22
   40efc:	mov	x4, x22
   40f00:	mov	x5, x26
   40f04:	mov	x26, x3
   40f08:	bl	c720 <__gmpn_toom44_mul@plt>
   40f0c:	ldur	x1, [x29, #-72]
   40f10:	mov	w5, #0x2                   	// #2
   40f14:	mov	w6, #0x4                   	// #4
   40f18:	mov	x0, x27
   40f1c:	mov	x2, x20
   40f20:	mov	w3, w28
   40f24:	mov	x4, x23
   40f28:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40f2c:	ldur	w21, [x29, #-76]
   40f30:	ldp	x22, x25, [x29, #-32]
   40f34:	cmp	x23, #0x51
   40f38:	b.gt	40fd0 <__gmpn_toom8h_mul@@Base+0x1124>
   40f3c:	ldp	x28, x3, [x29, #-16]
   40f40:	mov	x0, x20
   40f44:	mov	x2, x23
   40f48:	mov	x4, x23
   40f4c:	mov	x1, x28
   40f50:	mov	x5, x26
   40f54:	bl	c0a0 <__gmpn_toom33_mul@plt>
   40f58:	b	40fec <__gmpn_toom8h_mul@@Base+0x1140>
   40f5c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40f60:	ldur	x26, [x29, #-64]
   40f64:	mov	x0, x27
   40f68:	mov	x1, x25
   40f6c:	mov	x2, x22
   40f70:	mov	x3, x26
   40f74:	mov	x4, x22
   40f78:	mov	x5, x19
   40f7c:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40f80:	ldur	x1, [x29, #-72]
   40f84:	mov	w5, #0x2                   	// #2
   40f88:	mov	w6, #0x4                   	// #4
   40f8c:	mov	x0, x27
   40f90:	mov	x2, x20
   40f94:	mov	w3, w28
   40f98:	mov	x4, x23
   40f9c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   40fa0:	ldur	w21, [x29, #-76]
   40fa4:	ldp	x22, x25, [x29, #-32]
   40fa8:	cmp	x23, #0xac
   40fac:	b.le	40fd0 <__gmpn_toom8h_mul@@Base+0x1124>
   40fb0:	ldp	x28, x3, [x29, #-16]
   40fb4:	mov	x0, x20
   40fb8:	mov	x2, x23
   40fbc:	mov	x4, x23
   40fc0:	mov	x1, x28
   40fc4:	mov	x5, x26
   40fc8:	bl	cc20 <__gmpn_toom6h_mul@plt>
   40fcc:	b	40fec <__gmpn_toom8h_mul@@Base+0x1140>
   40fd0:	ldp	x28, x3, [x29, #-16]
   40fd4:	mov	x0, x20
   40fd8:	mov	x2, x23
   40fdc:	mov	x4, x23
   40fe0:	mov	x1, x28
   40fe4:	mov	x5, x26
   40fe8:	bl	c720 <__gmpn_toom44_mul@plt>
   40fec:	ldp	x19, x27, [sp, #24]
   40ff0:	ldr	x24, [sp, #72]
   40ff4:	cbnz	w21, 41098 <__gmpn_toom8h_mul@@Base+0x11ec>
   40ff8:	ldr	x4, [sp, #64]
   40ffc:	add	x6, x25, x22
   41000:	mov	x0, x20
   41004:	mov	x1, x27
   41008:	mov	x2, x24
   4100c:	mov	x3, x19
   41010:	mov	x5, x23
   41014:	mov	w7, w21
   41018:	str	x26, [sp]
   4101c:	bl	d350 <__gmpn_toom_interpolate_16pts@plt>
   41020:	ldp	x20, x19, [sp, #240]
   41024:	ldp	x22, x21, [sp, #224]
   41028:	ldp	x24, x23, [sp, #208]
   4102c:	ldp	x26, x25, [sp, #192]
   41030:	ldp	x28, x27, [sp, #176]
   41034:	ldp	x29, x30, [sp, #160]
   41038:	add	sp, sp, #0x100
   4103c:	ret
   41040:	add	x9, x2, x2, lsl #2
   41044:	asr	x8, x4, #1
   41048:	mov	w10, #0x15                  	// #21
   4104c:	lsl	x9, x9, #1
   41050:	mul	x10, x8, x10
   41054:	cmp	x9, x10
   41058:	b.lt	3fee8 <__gmpn_toom8h_mul@@Base+0x3c>  // b.tstop
   4105c:	mov	w10, #0xd                   	// #13
   41060:	mul	x10, x2, x10
   41064:	cmp	x10, x4, lsl #4
   41068:	b.ge	410fc <__gmpn_toom8h_mul@@Base+0x1250>  // b.tcont
   4106c:	mov	w8, #0x8                   	// #8
   41070:	mov	w9, #0x9                   	// #9
   41074:	b	411cc <__gmpn_toom8h_mul@@Base+0x1320>
   41078:	ldur	x0, [x29, #-64]
   4107c:	ldur	x2, [x29, #-8]
   41080:	ldur	x4, [x29, #-24]
   41084:	mov	x1, x21
   41088:	mov	x3, x23
   4108c:	mov	x5, x20
   41090:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   41094:	b	40ae0 <__gmpn_toom8h_mul@@Base+0xc34>
   41098:	mov	w8, #0x78                  	// #120
   4109c:	cmp	x22, x25
   410a0:	madd	x0, x23, x8, x20
   410a4:	b.le	410d0 <__gmpn_toom8h_mul@@Base+0x1224>
   410a8:	ldp	x9, x8, [x29, #-56]
   410ac:	mov	x2, x22
   410b0:	mov	x4, x25
   410b4:	mov	w8, w8
   410b8:	mul	x8, x23, x8
   410bc:	add	x1, x28, x8, lsl #3
   410c0:	ldur	x8, [x29, #-8]
   410c4:	mul	x9, x23, x9
   410c8:	add	x3, x8, x9, lsl #3
   410cc:	b	410f4 <__gmpn_toom8h_mul@@Base+0x1248>
   410d0:	ldp	x8, x9, [x29, #-56]
   410d4:	ldur	x10, [x29, #-8]
   410d8:	mov	x2, x25
   410dc:	mov	x4, x22
   410e0:	mul	x8, x23, x8
   410e4:	mov	w9, w9
   410e8:	add	x1, x10, x8, lsl #3
   410ec:	mul	x8, x23, x9
   410f0:	add	x3, x28, x8, lsl #3
   410f4:	bl	ccd0 <__gmpn_mul@plt>
   410f8:	b	40ff8 <__gmpn_toom8h_mul@@Base+0x114c>
   410fc:	mov	w10, #0x1b                  	// #27
   41100:	mul	x10, x8, x10
   41104:	cmp	x9, x10
   41108:	b.ge	41118 <__gmpn_toom8h_mul@@Base+0x126c>  // b.tcont
   4110c:	mov	w8, #0x7                   	// #7
   41110:	mov	w9, #0x9                   	// #9
   41114:	b	411cc <__gmpn_toom8h_mul@@Base+0x1320>
   41118:	add	x8, x8, x8, lsl #5
   4111c:	cmp	x9, x8
   41120:	b.ge	41130 <__gmpn_toom8h_mul@@Base+0x1284>  // b.tcont
   41124:	mov	w8, #0x7                   	// #7
   41128:	mov	w9, #0xa                   	// #10
   4112c:	b	411c4 <__gmpn_toom8h_mul@@Base+0x1318>
   41130:	lsl	x9, x4, #3
   41134:	lsl	x8, x2, #2
   41138:	sub	x9, x9, x4
   4113c:	cmp	x8, x9
   41140:	b.ge	41150 <__gmpn_toom8h_mul@@Base+0x12a4>  // b.tcont
   41144:	mov	w8, #0x6                   	// #6
   41148:	mov	w9, #0xa                   	// #10
   4114c:	b	411c4 <__gmpn_toom8h_mul@@Base+0x1318>
   41150:	add	x9, x2, x2, lsl #1
   41154:	mov	w10, #0xd                   	// #13
   41158:	lsl	x9, x9, #1
   4115c:	mul	x10, x4, x10
   41160:	cmp	x9, x10
   41164:	b.ge	41174 <__gmpn_toom8h_mul@@Base+0x12c8>  // b.tcont
   41168:	mov	w8, #0x6                   	// #6
   4116c:	mov	w9, #0xb                   	// #11
   41170:	b	411c4 <__gmpn_toom8h_mul@@Base+0x1318>
   41174:	add	x9, x4, x4, lsl #3
   41178:	cmp	x8, x9
   4117c:	b.ge	4118c <__gmpn_toom8h_mul@@Base+0x12e0>  // b.tcont
   41180:	mov	w8, #0x5                   	// #5
   41184:	mov	w9, #0xb                   	// #11
   41188:	b	411c4 <__gmpn_toom8h_mul@@Base+0x1318>
   4118c:	lsl	x8, x2, #3
   41190:	sub	x8, x8, x2
   41194:	add	x9, x4, x4, lsl #2
   41198:	cmp	x8, x9, lsl #2
   4119c:	mov	w9, #0xc                   	// #12
   411a0:	b.ge	411ac <__gmpn_toom8h_mul@@Base+0x1300>  // b.tcont
   411a4:	mov	w8, #0x5                   	// #5
   411a8:	b	411c4 <__gmpn_toom8h_mul@@Base+0x1318>
   411ac:	mov	w10, #0x1c                  	// #28
   411b0:	add	x8, x2, x2, lsl #3
   411b4:	mul	x10, x4, x10
   411b8:	cmp	x8, x10
   411bc:	cinc	w9, w9, ge  // ge = tcont
   411c0:	mov	w8, #0x4                   	// #4
   411c4:	ldur	x16, [x29, #-16]
   411c8:	ldr	x20, [sp, #32]
   411cc:	mov	w12, w9
   411d0:	mul	x11, x8, x2
   411d4:	mul	x13, x12, x4
   411d8:	cmp	x11, x13
   411dc:	csel	x11, x4, x2, lt  // lt = tstop
   411e0:	csel	x12, x8, x12, lt  // lt = tstop
   411e4:	sub	x11, x11, #0x1
   411e8:	sub	w14, w8, #0x1
   411ec:	udiv	x11, x11, x12
   411f0:	sub	w15, w9, #0x1
   411f4:	mov	x28, x14
   411f8:	sxtw	x14, w14
   411fc:	add	x23, x11, #0x1
   41200:	add	w10, w8, w9
   41204:	msub	x5, x23, x15, x2
   41208:	msub	x26, x23, x14, x4
   4120c:	tbnz	w10, #0, 41218 <__gmpn_toom8h_mul@@Base+0x136c>
   41210:	mov	w10, wzr
   41214:	b	3ff10 <__gmpn_toom8h_mul@@Base+0x64>
   41218:	cmp	x5, #0x0
   4121c:	b.le	41230 <__gmpn_toom8h_mul@@Base+0x1384>
   41220:	cmp	x26, #0x0
   41224:	b.le	41240 <__gmpn_toom8h_mul@@Base+0x1394>
   41228:	mov	w10, #0x1                   	// #1
   4122c:	b	3ff10 <__gmpn_toom8h_mul@@Base+0x64>
   41230:	mov	w10, wzr
   41234:	sub	w15, w9, #0x2
   41238:	add	x5, x5, x23
   4123c:	b	3ff10 <__gmpn_toom8h_mul@@Base+0x64>
   41240:	mov	w10, wzr
   41244:	sub	w28, w8, #0x2
   41248:	add	x26, x26, x23
   4124c:	b	3ff10 <__gmpn_toom8h_mul@@Base+0x64>

0000000000041250 <__gmpn_toom8_sqr@@Base>:
   41250:	sub	sp, sp, #0xb0
   41254:	sub	x8, x2, #0x1
   41258:	stp	x22, x21, [sp, #144]
   4125c:	asr	x22, x8, #3
   41260:	add	x21, x22, #0x1
   41264:	mov	w9, #0x68                  	// #104
   41268:	lsl	x8, x21, #3
   4126c:	mov	w10, #0x58                  	// #88
   41270:	madd	x9, x21, x9, x0
   41274:	sub	x8, x8, x21
   41278:	stp	x26, x25, [sp, #112]
   4127c:	stp	x24, x23, [sp, #128]
   41280:	stp	x20, x19, [sp, #160]
   41284:	mov	x20, x0
   41288:	add	x26, x9, #0x10
   4128c:	sub	x23, x2, x8
   41290:	madd	x25, x21, x10, x0
   41294:	stp	x29, x30, [sp, #80]
   41298:	add	x29, sp, #0x50
   4129c:	mov	x19, x3
   412a0:	mov	x24, x2
   412a4:	mov	x3, x1
   412a8:	mov	w2, #0x7                   	// #7
   412ac:	mov	w6, #0x3                   	// #3
   412b0:	mov	x0, x26
   412b4:	mov	x1, x25
   412b8:	mov	x4, x21
   412bc:	mov	x5, x23
   412c0:	mov	x7, x20
   412c4:	stp	x28, x27, [sp, #96]
   412c8:	str	x8, [sp, #40]
   412cc:	stur	x3, [x29, #-8]
   412d0:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   412d4:	mov	w8, #0x60                  	// #96
   412d8:	cmp	x24, #0x208
   412dc:	add	x27, x22, #0x2
   412e0:	madd	x8, x21, x8, x19
   412e4:	b.le	41330 <__gmpn_toom8_sqr@@Base+0xe0>
   412e8:	cmp	x24, #0x520
   412ec:	b.le	41360 <__gmpn_toom8_sqr@@Base+0x110>
   412f0:	cmp	x24, #0x6e0
   412f4:	b.le	41390 <__gmpn_toom8_sqr@@Base+0x140>
   412f8:	add	x28, x8, #0x20
   412fc:	cmp	x24, #0xa58
   41300:	mov	x0, x20
   41304:	mov	x1, x25
   41308:	mov	x2, x27
   4130c:	mov	x3, x28
   41310:	b.le	413c0 <__gmpn_toom8_sqr@@Base+0x170>
   41314:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41318:	mov	x0, x19
   4131c:	mov	x1, x26
   41320:	mov	x2, x27
   41324:	mov	x3, x28
   41328:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   4132c:	b	413d8 <__gmpn_toom8_sqr@@Base+0x188>
   41330:	add	x28, x8, #0x20
   41334:	mov	x0, x20
   41338:	mov	x1, x25
   4133c:	mov	x2, x27
   41340:	mov	x3, x28
   41344:	bl	c050 <__gmpn_toom2_sqr@plt>
   41348:	mov	x0, x19
   4134c:	mov	x1, x26
   41350:	mov	x2, x27
   41354:	mov	x3, x28
   41358:	bl	c050 <__gmpn_toom2_sqr@plt>
   4135c:	b	413d8 <__gmpn_toom8_sqr@@Base+0x188>
   41360:	add	x28, x8, #0x20
   41364:	mov	x0, x20
   41368:	mov	x1, x25
   4136c:	mov	x2, x27
   41370:	mov	x3, x28
   41374:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41378:	mov	x0, x19
   4137c:	mov	x1, x26
   41380:	mov	x2, x27
   41384:	mov	x3, x28
   41388:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   4138c:	b	413d8 <__gmpn_toom8_sqr@@Base+0x188>
   41390:	add	x28, x8, #0x20
   41394:	mov	x0, x20
   41398:	mov	x1, x25
   4139c:	mov	x2, x27
   413a0:	mov	x3, x28
   413a4:	bl	c220 <__gmpn_toom4_sqr@plt>
   413a8:	mov	x0, x19
   413ac:	mov	x1, x26
   413b0:	mov	x2, x27
   413b4:	mov	x3, x28
   413b8:	bl	c220 <__gmpn_toom4_sqr@plt>
   413bc:	b	413d8 <__gmpn_toom8_sqr@@Base+0x188>
   413c0:	bl	d470 <__gmpn_toom6_sqr@plt>
   413c4:	mov	x0, x19
   413c8:	mov	x1, x26
   413cc:	mov	x2, x27
   413d0:	mov	x3, x28
   413d4:	bl	d470 <__gmpn_toom6_sqr@plt>
   413d8:	mov	w28, #0x1                   	// #1
   413dc:	bfi	x28, x21, #1, #63
   413e0:	mov	w5, #0x3                   	// #3
   413e4:	mov	x0, x19
   413e8:	mov	x1, x28
   413ec:	mov	x2, x20
   413f0:	mov	w3, wzr
   413f4:	mov	x4, x21
   413f8:	mov	w6, wzr
   413fc:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41400:	ldur	x3, [x29, #-8]
   41404:	mov	w2, #0x7                   	// #7
   41408:	mov	w6, #0x2                   	// #2
   4140c:	mov	x0, x26
   41410:	mov	x1, x25
   41414:	mov	x4, x21
   41418:	mov	x5, x23
   4141c:	mov	x7, x20
   41420:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   41424:	mov	w8, #0x60                  	// #96
   41428:	madd	x8, x21, x8, x19
   4142c:	cmp	x24, #0x208
   41430:	b.le	41488 <__gmpn_toom8_sqr@@Base+0x238>
   41434:	cmp	x24, #0x520
   41438:	b.le	414c4 <__gmpn_toom8_sqr@@Base+0x274>
   4143c:	cmp	x24, #0x6e0
   41440:	b.le	41500 <__gmpn_toom8_sqr@@Base+0x2b0>
   41444:	add	x8, x8, #0x20
   41448:	cmp	x24, #0xa58
   4144c:	mov	x0, x20
   41450:	mov	x1, x25
   41454:	mov	x2, x27
   41458:	mov	x3, x8
   4145c:	stur	x8, [x29, #-16]
   41460:	b.le	4153c <__gmpn_toom8_sqr@@Base+0x2ec>
   41464:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41468:	ldur	x3, [x29, #-16]
   4146c:	add	x22, x21, x21, lsl #1
   41470:	add	x8, x19, x22, lsl #3
   41474:	add	x0, x8, #0x8
   41478:	mov	x1, x26
   4147c:	mov	x2, x27
   41480:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41484:	b	4155c <__gmpn_toom8_sqr@@Base+0x30c>
   41488:	add	x22, x8, #0x20
   4148c:	mov	x0, x20
   41490:	mov	x1, x25
   41494:	mov	x2, x27
   41498:	mov	x3, x22
   4149c:	bl	c050 <__gmpn_toom2_sqr@plt>
   414a0:	add	x9, x21, x21, lsl #1
   414a4:	add	x8, x19, x9, lsl #3
   414a8:	add	x0, x8, #0x8
   414ac:	mov	x1, x26
   414b0:	mov	x2, x27
   414b4:	mov	x3, x22
   414b8:	mov	x22, x9
   414bc:	bl	c050 <__gmpn_toom2_sqr@plt>
   414c0:	b	4155c <__gmpn_toom8_sqr@@Base+0x30c>
   414c4:	add	x22, x8, #0x20
   414c8:	mov	x0, x20
   414cc:	mov	x1, x25
   414d0:	mov	x2, x27
   414d4:	mov	x3, x22
   414d8:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   414dc:	add	x9, x21, x21, lsl #1
   414e0:	add	x8, x19, x9, lsl #3
   414e4:	add	x0, x8, #0x8
   414e8:	mov	x1, x26
   414ec:	mov	x2, x27
   414f0:	mov	x3, x22
   414f4:	mov	x22, x9
   414f8:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   414fc:	b	4155c <__gmpn_toom8_sqr@@Base+0x30c>
   41500:	add	x22, x8, #0x20
   41504:	mov	x0, x20
   41508:	mov	x1, x25
   4150c:	mov	x2, x27
   41510:	mov	x3, x22
   41514:	bl	c220 <__gmpn_toom4_sqr@plt>
   41518:	add	x9, x21, x21, lsl #1
   4151c:	add	x8, x19, x9, lsl #3
   41520:	add	x0, x8, #0x8
   41524:	mov	x1, x26
   41528:	mov	x2, x27
   4152c:	mov	x3, x22
   41530:	mov	x22, x9
   41534:	bl	c220 <__gmpn_toom4_sqr@plt>
   41538:	b	4155c <__gmpn_toom8_sqr@@Base+0x30c>
   4153c:	bl	d470 <__gmpn_toom6_sqr@plt>
   41540:	ldur	x3, [x29, #-16]
   41544:	add	x22, x21, x21, lsl #1
   41548:	add	x8, x19, x22, lsl #3
   4154c:	add	x0, x8, #0x8
   41550:	mov	x1, x26
   41554:	mov	x2, x27
   41558:	bl	d470 <__gmpn_toom6_sqr@plt>
   4155c:	add	x8, x19, x22, lsl #3
   41560:	str	x22, [sp, #24]
   41564:	add	x22, x8, #0x8
   41568:	mov	w5, #0x2                   	// #2
   4156c:	mov	x0, x22
   41570:	mov	x1, x28
   41574:	mov	x2, x20
   41578:	mov	w3, wzr
   4157c:	mov	x4, x21
   41580:	mov	w6, wzr
   41584:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41588:	ldur	x3, [x29, #-8]
   4158c:	mov	w2, #0x7                   	// #7
   41590:	mov	x0, x26
   41594:	mov	x1, x25
   41598:	mov	x4, x21
   4159c:	mov	x5, x23
   415a0:	mov	x6, x20
   415a4:	bl	c530 <__gmpn_toom_eval_pm2@plt>
   415a8:	mov	w8, #0x60                  	// #96
   415ac:	cmp	x24, #0x208
   415b0:	madd	x8, x21, x8, x19
   415b4:	stur	x23, [x29, #-16]
   415b8:	str	x22, [sp, #32]
   415bc:	b.le	41618 <__gmpn_toom8_sqr@@Base+0x3c8>
   415c0:	cmp	x24, #0x520
   415c4:	b.le	41654 <__gmpn_toom8_sqr@@Base+0x404>
   415c8:	cmp	x24, #0x6e0
   415cc:	b.le	41690 <__gmpn_toom8_sqr@@Base+0x440>
   415d0:	add	x8, x8, #0x20
   415d4:	cmp	x24, #0xa58
   415d8:	mov	x0, x20
   415dc:	mov	x1, x25
   415e0:	mov	x2, x27
   415e4:	mov	x22, x8
   415e8:	mov	x3, x8
   415ec:	b.le	416cc <__gmpn_toom8_sqr@@Base+0x47c>
   415f0:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   415f4:	add	x8, x21, x21, lsl #1
   415f8:	lsl	x23, x8, #1
   415fc:	add	x8, x19, x8, lsl #4
   41600:	add	x0, x8, #0x10
   41604:	mov	x1, x26
   41608:	mov	x2, x27
   4160c:	mov	x3, x22
   41610:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41614:	b	416f0 <__gmpn_toom8_sqr@@Base+0x4a0>
   41618:	add	x22, x8, #0x20
   4161c:	mov	x0, x20
   41620:	mov	x1, x25
   41624:	mov	x2, x27
   41628:	mov	x3, x22
   4162c:	bl	c050 <__gmpn_toom2_sqr@plt>
   41630:	add	x8, x21, x21, lsl #1
   41634:	lsl	x23, x8, #1
   41638:	add	x8, x19, x8, lsl #4
   4163c:	add	x0, x8, #0x10
   41640:	mov	x1, x26
   41644:	mov	x2, x27
   41648:	mov	x3, x22
   4164c:	bl	c050 <__gmpn_toom2_sqr@plt>
   41650:	b	416f0 <__gmpn_toom8_sqr@@Base+0x4a0>
   41654:	add	x22, x8, #0x20
   41658:	mov	x0, x20
   4165c:	mov	x1, x25
   41660:	mov	x2, x27
   41664:	mov	x3, x22
   41668:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   4166c:	add	x8, x21, x21, lsl #1
   41670:	lsl	x23, x8, #1
   41674:	add	x8, x19, x8, lsl #4
   41678:	add	x0, x8, #0x10
   4167c:	mov	x1, x26
   41680:	mov	x2, x27
   41684:	mov	x3, x22
   41688:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   4168c:	b	416f0 <__gmpn_toom8_sqr@@Base+0x4a0>
   41690:	add	x22, x8, #0x20
   41694:	mov	x0, x20
   41698:	mov	x1, x25
   4169c:	mov	x2, x27
   416a0:	mov	x3, x22
   416a4:	bl	c220 <__gmpn_toom4_sqr@plt>
   416a8:	add	x8, x21, x21, lsl #1
   416ac:	lsl	x23, x8, #1
   416b0:	add	x8, x19, x8, lsl #4
   416b4:	add	x0, x8, #0x10
   416b8:	mov	x1, x26
   416bc:	mov	x2, x27
   416c0:	mov	x3, x22
   416c4:	bl	c220 <__gmpn_toom4_sqr@plt>
   416c8:	b	416f0 <__gmpn_toom8_sqr@@Base+0x4a0>
   416cc:	bl	d470 <__gmpn_toom6_sqr@plt>
   416d0:	add	x8, x21, x21, lsl #1
   416d4:	lsl	x23, x8, #1
   416d8:	add	x8, x19, x8, lsl #4
   416dc:	add	x0, x8, #0x10
   416e0:	mov	x1, x26
   416e4:	mov	x2, x27
   416e8:	mov	x3, x22
   416ec:	bl	d470 <__gmpn_toom6_sqr@plt>
   416f0:	add	x8, x19, x23, lsl #3
   416f4:	add	x22, x8, #0x10
   416f8:	mov	w5, #0x1                   	// #1
   416fc:	mov	w6, #0x2                   	// #2
   41700:	mov	x0, x22
   41704:	mov	x1, x28
   41708:	mov	x2, x20
   4170c:	mov	w3, wzr
   41710:	mov	x4, x21
   41714:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41718:	ldp	x5, x3, [x29, #-16]
   4171c:	mov	w2, #0x7                   	// #7
   41720:	mov	w6, #0x3                   	// #3
   41724:	mov	x0, x26
   41728:	mov	x1, x25
   4172c:	mov	x4, x21
   41730:	mov	x7, x20
   41734:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   41738:	mov	w8, #0x60                  	// #96
   4173c:	cmp	x24, #0x208
   41740:	madd	x8, x21, x8, x19
   41744:	stur	x22, [x29, #-24]
   41748:	b.le	417a0 <__gmpn_toom8_sqr@@Base+0x550>
   4174c:	cmp	x24, #0x520
   41750:	b.le	417d8 <__gmpn_toom8_sqr@@Base+0x588>
   41754:	cmp	x24, #0x6e0
   41758:	b.le	41810 <__gmpn_toom8_sqr@@Base+0x5c0>
   4175c:	add	x8, x8, #0x20
   41760:	cmp	x24, #0xa58
   41764:	mov	x0, x20
   41768:	mov	x1, x25
   4176c:	mov	x2, x27
   41770:	mov	x22, x8
   41774:	mov	x3, x8
   41778:	b.le	41848 <__gmpn_toom8_sqr@@Base+0x5f8>
   4177c:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41780:	add	x23, x21, x21, lsl #3
   41784:	add	x8, x19, x23, lsl #3
   41788:	add	x0, x8, #0x18
   4178c:	mov	x1, x26
   41790:	mov	x2, x27
   41794:	mov	x3, x22
   41798:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   4179c:	b	41868 <__gmpn_toom8_sqr@@Base+0x618>
   417a0:	add	x22, x8, #0x20
   417a4:	mov	x0, x20
   417a8:	mov	x1, x25
   417ac:	mov	x2, x27
   417b0:	mov	x3, x22
   417b4:	bl	c050 <__gmpn_toom2_sqr@plt>
   417b8:	add	x23, x21, x21, lsl #3
   417bc:	add	x8, x19, x23, lsl #3
   417c0:	add	x0, x8, #0x18
   417c4:	mov	x1, x26
   417c8:	mov	x2, x27
   417cc:	mov	x3, x22
   417d0:	bl	c050 <__gmpn_toom2_sqr@plt>
   417d4:	b	41868 <__gmpn_toom8_sqr@@Base+0x618>
   417d8:	add	x22, x8, #0x20
   417dc:	mov	x0, x20
   417e0:	mov	x1, x25
   417e4:	mov	x2, x27
   417e8:	mov	x3, x22
   417ec:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   417f0:	add	x23, x21, x21, lsl #3
   417f4:	add	x8, x19, x23, lsl #3
   417f8:	add	x0, x8, #0x18
   417fc:	mov	x1, x26
   41800:	mov	x2, x27
   41804:	mov	x3, x22
   41808:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   4180c:	b	41868 <__gmpn_toom8_sqr@@Base+0x618>
   41810:	add	x22, x8, #0x20
   41814:	mov	x0, x20
   41818:	mov	x1, x25
   4181c:	mov	x2, x27
   41820:	mov	x3, x22
   41824:	bl	c220 <__gmpn_toom4_sqr@plt>
   41828:	add	x23, x21, x21, lsl #3
   4182c:	add	x8, x19, x23, lsl #3
   41830:	add	x0, x8, #0x18
   41834:	mov	x1, x26
   41838:	mov	x2, x27
   4183c:	mov	x3, x22
   41840:	bl	c220 <__gmpn_toom4_sqr@plt>
   41844:	b	41868 <__gmpn_toom8_sqr@@Base+0x618>
   41848:	bl	d470 <__gmpn_toom6_sqr@plt>
   4184c:	add	x23, x21, x21, lsl #3
   41850:	add	x8, x19, x23, lsl #3
   41854:	add	x0, x8, #0x18
   41858:	mov	x1, x26
   4185c:	mov	x2, x27
   41860:	mov	x3, x22
   41864:	bl	d470 <__gmpn_toom6_sqr@plt>
   41868:	add	x8, x19, x23, lsl #3
   4186c:	add	x22, x8, #0x18
   41870:	mov	w5, #0x3                   	// #3
   41874:	mov	w6, #0x6                   	// #6
   41878:	mov	x0, x22
   4187c:	mov	x1, x28
   41880:	mov	x2, x20
   41884:	mov	w3, wzr
   41888:	mov	x4, x21
   4188c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41890:	ldp	x23, x3, [x29, #-16]
   41894:	mov	w2, #0x7                   	// #7
   41898:	mov	w6, #0x1                   	// #1
   4189c:	mov	x0, x26
   418a0:	mov	x1, x25
   418a4:	mov	x4, x21
   418a8:	mov	x5, x23
   418ac:	mov	x7, x20
   418b0:	bl	d0e0 <__gmpn_toom_eval_pm2rexp@plt>
   418b4:	mov	w8, #0x60                  	// #96
   418b8:	cmp	x24, #0x208
   418bc:	madd	x8, x21, x8, x19
   418c0:	stur	x22, [x29, #-32]
   418c4:	b.le	41914 <__gmpn_toom8_sqr@@Base+0x6c4>
   418c8:	cmp	x24, #0x520
   418cc:	b.le	4194c <__gmpn_toom8_sqr@@Base+0x6fc>
   418d0:	cmp	x24, #0x6e0
   418d4:	b.le	41984 <__gmpn_toom8_sqr@@Base+0x734>
   418d8:	add	x8, x8, #0x20
   418dc:	cmp	x24, #0xa58
   418e0:	mov	x0, x20
   418e4:	mov	x1, x25
   418e8:	mov	x2, x27
   418ec:	mov	x3, x8
   418f0:	str	x8, [sp, #16]
   418f4:	b.le	419bc <__gmpn_toom8_sqr@@Base+0x76c>
   418f8:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   418fc:	ldp	x3, x22, [sp, #16]
   41900:	mov	x1, x26
   41904:	mov	x2, x27
   41908:	add	x0, x20, x22, lsl #3
   4190c:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41910:	b	419d4 <__gmpn_toom8_sqr@@Base+0x784>
   41914:	add	x22, x8, #0x20
   41918:	mov	x0, x20
   4191c:	mov	x1, x25
   41920:	mov	x2, x27
   41924:	mov	x3, x22
   41928:	bl	c050 <__gmpn_toom2_sqr@plt>
   4192c:	ldr	x8, [sp, #24]
   41930:	mov	x1, x26
   41934:	mov	x2, x27
   41938:	mov	x3, x22
   4193c:	add	x0, x20, x8, lsl #3
   41940:	mov	x22, x8
   41944:	bl	c050 <__gmpn_toom2_sqr@plt>
   41948:	b	419d4 <__gmpn_toom8_sqr@@Base+0x784>
   4194c:	add	x22, x8, #0x20
   41950:	mov	x0, x20
   41954:	mov	x1, x25
   41958:	mov	x2, x27
   4195c:	mov	x3, x22
   41960:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41964:	ldr	x8, [sp, #24]
   41968:	mov	x1, x26
   4196c:	mov	x2, x27
   41970:	mov	x3, x22
   41974:	add	x0, x20, x8, lsl #3
   41978:	mov	x22, x8
   4197c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41980:	b	419d4 <__gmpn_toom8_sqr@@Base+0x784>
   41984:	add	x22, x8, #0x20
   41988:	mov	x0, x20
   4198c:	mov	x1, x25
   41990:	mov	x2, x27
   41994:	mov	x3, x22
   41998:	bl	c220 <__gmpn_toom4_sqr@plt>
   4199c:	ldr	x8, [sp, #24]
   419a0:	mov	x1, x26
   419a4:	mov	x2, x27
   419a8:	mov	x3, x22
   419ac:	add	x0, x20, x8, lsl #3
   419b0:	mov	x22, x8
   419b4:	bl	c220 <__gmpn_toom4_sqr@plt>
   419b8:	b	419d4 <__gmpn_toom8_sqr@@Base+0x784>
   419bc:	bl	d470 <__gmpn_toom6_sqr@plt>
   419c0:	ldp	x3, x22, [sp, #16]
   419c4:	mov	x1, x26
   419c8:	mov	x2, x27
   419cc:	add	x0, x20, x22, lsl #3
   419d0:	bl	d470 <__gmpn_toom6_sqr@plt>
   419d4:	add	x0, x20, x22, lsl #3
   419d8:	mov	w5, #0x1                   	// #1
   419dc:	mov	x1, x28
   419e0:	mov	x2, x20
   419e4:	mov	w3, wzr
   419e8:	mov	x4, x21
   419ec:	mov	w6, wzr
   419f0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   419f4:	ldur	x3, [x29, #-8]
   419f8:	mov	w2, #0x7                   	// #7
   419fc:	mov	x0, x26
   41a00:	mov	x1, x25
   41a04:	mov	x4, x21
   41a08:	mov	x5, x23
   41a0c:	mov	x6, x20
   41a10:	bl	cfc0 <__gmpn_toom_eval_pm1@plt>
   41a14:	mov	w8, #0x60                  	// #96
   41a18:	madd	x8, x21, x8, x19
   41a1c:	cmp	x24, #0x208
   41a20:	b.le	41a74 <__gmpn_toom8_sqr@@Base+0x824>
   41a24:	cmp	x24, #0x520
   41a28:	b.le	41aac <__gmpn_toom8_sqr@@Base+0x85c>
   41a2c:	cmp	x24, #0x6e0
   41a30:	b.le	41ae4 <__gmpn_toom8_sqr@@Base+0x894>
   41a34:	add	x8, x8, #0x20
   41a38:	cmp	x24, #0xa58
   41a3c:	mov	x0, x20
   41a40:	mov	x1, x25
   41a44:	mov	x2, x27
   41a48:	mov	x3, x8
   41a4c:	str	x8, [sp, #24]
   41a50:	b.le	41b1c <__gmpn_toom8_sqr@@Base+0x8cc>
   41a54:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41a58:	ldr	x22, [sp, #40]
   41a5c:	ldr	x3, [sp, #24]
   41a60:	mov	x1, x26
   41a64:	mov	x2, x27
   41a68:	add	x0, x20, x22, lsl #3
   41a6c:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41a70:	b	41b38 <__gmpn_toom8_sqr@@Base+0x8e8>
   41a74:	add	x22, x8, #0x20
   41a78:	mov	x0, x20
   41a7c:	mov	x1, x25
   41a80:	mov	x2, x27
   41a84:	mov	x3, x22
   41a88:	bl	c050 <__gmpn_toom2_sqr@plt>
   41a8c:	ldr	x8, [sp, #40]
   41a90:	mov	x1, x26
   41a94:	mov	x2, x27
   41a98:	mov	x3, x22
   41a9c:	add	x0, x20, x8, lsl #3
   41aa0:	mov	x22, x8
   41aa4:	bl	c050 <__gmpn_toom2_sqr@plt>
   41aa8:	b	41b38 <__gmpn_toom8_sqr@@Base+0x8e8>
   41aac:	add	x22, x8, #0x20
   41ab0:	mov	x0, x20
   41ab4:	mov	x1, x25
   41ab8:	mov	x2, x27
   41abc:	mov	x3, x22
   41ac0:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41ac4:	ldr	x8, [sp, #40]
   41ac8:	mov	x1, x26
   41acc:	mov	x2, x27
   41ad0:	mov	x3, x22
   41ad4:	add	x0, x20, x8, lsl #3
   41ad8:	mov	x22, x8
   41adc:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41ae0:	b	41b38 <__gmpn_toom8_sqr@@Base+0x8e8>
   41ae4:	add	x22, x8, #0x20
   41ae8:	mov	x0, x20
   41aec:	mov	x1, x25
   41af0:	mov	x2, x27
   41af4:	mov	x3, x22
   41af8:	bl	c220 <__gmpn_toom4_sqr@plt>
   41afc:	ldr	x8, [sp, #40]
   41b00:	mov	x1, x26
   41b04:	mov	x2, x27
   41b08:	mov	x3, x22
   41b0c:	add	x0, x20, x8, lsl #3
   41b10:	mov	x22, x8
   41b14:	bl	c220 <__gmpn_toom4_sqr@plt>
   41b18:	b	41b38 <__gmpn_toom8_sqr@@Base+0x8e8>
   41b1c:	bl	d470 <__gmpn_toom6_sqr@plt>
   41b20:	ldr	x22, [sp, #40]
   41b24:	ldr	x3, [sp, #24]
   41b28:	mov	x1, x26
   41b2c:	mov	x2, x27
   41b30:	add	x0, x20, x22, lsl #3
   41b34:	bl	d470 <__gmpn_toom6_sqr@plt>
   41b38:	add	x0, x20, x22, lsl #3
   41b3c:	mov	x1, x28
   41b40:	mov	x2, x20
   41b44:	mov	w3, wzr
   41b48:	mov	x4, x21
   41b4c:	mov	w5, wzr
   41b50:	mov	w6, wzr
   41b54:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41b58:	ldur	x3, [x29, #-8]
   41b5c:	mov	w2, #0x7                   	// #7
   41b60:	mov	w6, #0x2                   	// #2
   41b64:	mov	x0, x26
   41b68:	mov	x1, x25
   41b6c:	mov	x4, x21
   41b70:	mov	x5, x23
   41b74:	mov	x7, x20
   41b78:	bl	c390 <__gmpn_toom_eval_pm2exp@plt>
   41b7c:	add	x8, x21, x21, lsl #1
   41b80:	lsl	x23, x8, #2
   41b84:	add	x8, x19, x8, lsl #5
   41b88:	cmp	x24, #0x208
   41b8c:	b.le	41c1c <__gmpn_toom8_sqr@@Base+0x9cc>
   41b90:	cmp	x24, #0x521
   41b94:	b.lt	41c88 <__gmpn_toom8_sqr@@Base+0xa38>  // b.tstop
   41b98:	cmp	x24, #0x6e1
   41b9c:	b.lt	41ce4 <__gmpn_toom8_sqr@@Base+0xa94>  // b.tstop
   41ba0:	add	x22, x8, #0x20
   41ba4:	cmp	x24, #0xa58
   41ba8:	mov	x0, x20
   41bac:	mov	x1, x25
   41bb0:	mov	x2, x27
   41bb4:	mov	x3, x22
   41bb8:	b.le	41d58 <__gmpn_toom8_sqr@@Base+0xb08>
   41bbc:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41bc0:	mov	x0, x25
   41bc4:	mov	x1, x26
   41bc8:	mov	x2, x27
   41bcc:	mov	x3, x22
   41bd0:	mov	x26, x22
   41bd4:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41bd8:	mov	w5, #0x2                   	// #2
   41bdc:	mov	w6, #0x4                   	// #4
   41be0:	mov	x0, x25
   41be4:	mov	x1, x28
   41be8:	mov	x2, x20
   41bec:	mov	w3, wzr
   41bf0:	mov	x4, x21
   41bf4:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41bf8:	ldr	x22, [sp, #32]
   41bfc:	cmp	x24, #0xa60
   41c00:	b.le	41da0 <__gmpn_toom8_sqr@@Base+0xb50>
   41c04:	ldur	x1, [x29, #-8]
   41c08:	mov	x0, x20
   41c0c:	mov	x2, x21
   41c10:	mov	x3, x26
   41c14:	bl	d4b0 <__gmpn_toom8_sqr@plt>
   41c18:	b	41dd0 <__gmpn_toom8_sqr@@Base+0xb80>
   41c1c:	add	x22, x8, #0x20
   41c20:	mov	x0, x20
   41c24:	mov	x1, x25
   41c28:	mov	x2, x27
   41c2c:	mov	x3, x22
   41c30:	bl	c050 <__gmpn_toom2_sqr@plt>
   41c34:	mov	x0, x25
   41c38:	mov	x1, x26
   41c3c:	mov	x2, x27
   41c40:	mov	x3, x22
   41c44:	bl	c050 <__gmpn_toom2_sqr@plt>
   41c48:	mov	w5, #0x2                   	// #2
   41c4c:	mov	w6, #0x4                   	// #4
   41c50:	mov	x0, x25
   41c54:	mov	x1, x28
   41c58:	mov	x2, x20
   41c5c:	mov	w3, wzr
   41c60:	mov	x4, x21
   41c64:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41c68:	ldr	x22, [sp, #32]
   41c6c:	ldur	x1, [x29, #-8]
   41c70:	add	x8, x19, x23, lsl #3
   41c74:	add	x3, x8, #0x20
   41c78:	mov	x0, x20
   41c7c:	mov	x2, x21
   41c80:	bl	c050 <__gmpn_toom2_sqr@plt>
   41c84:	b	41dd0 <__gmpn_toom8_sqr@@Base+0xb80>
   41c88:	add	x22, x8, #0x20
   41c8c:	mov	x0, x20
   41c90:	mov	x1, x25
   41c94:	mov	x2, x27
   41c98:	mov	x3, x22
   41c9c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41ca0:	mov	x0, x25
   41ca4:	mov	x1, x26
   41ca8:	mov	x2, x27
   41cac:	mov	x3, x22
   41cb0:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41cb4:	mov	w5, #0x2                   	// #2
   41cb8:	mov	w6, #0x4                   	// #4
   41cbc:	mov	x0, x25
   41cc0:	mov	x1, x28
   41cc4:	mov	x2, x20
   41cc8:	mov	w3, wzr
   41ccc:	mov	x4, x21
   41cd0:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41cd4:	ldr	x22, [sp, #32]
   41cd8:	cmp	x24, #0x210
   41cdc:	b.le	41c6c <__gmpn_toom8_sqr@@Base+0xa1c>
   41ce0:	b	41d3c <__gmpn_toom8_sqr@@Base+0xaec>
   41ce4:	add	x22, x8, #0x20
   41ce8:	mov	x0, x20
   41cec:	mov	x1, x25
   41cf0:	mov	x2, x27
   41cf4:	mov	x3, x22
   41cf8:	bl	c220 <__gmpn_toom4_sqr@plt>
   41cfc:	mov	x0, x25
   41d00:	mov	x1, x26
   41d04:	mov	x2, x27
   41d08:	mov	x3, x22
   41d0c:	bl	c220 <__gmpn_toom4_sqr@plt>
   41d10:	mov	w5, #0x2                   	// #2
   41d14:	mov	w6, #0x4                   	// #4
   41d18:	mov	x0, x25
   41d1c:	mov	x1, x28
   41d20:	mov	x2, x20
   41d24:	mov	w3, wzr
   41d28:	mov	x4, x21
   41d2c:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41d30:	ldr	x22, [sp, #32]
   41d34:	cmp	x24, #0x528
   41d38:	b.gt	41db8 <__gmpn_toom8_sqr@@Base+0xb68>
   41d3c:	ldur	x1, [x29, #-8]
   41d40:	add	x8, x19, x23, lsl #3
   41d44:	add	x3, x8, #0x20
   41d48:	mov	x0, x20
   41d4c:	mov	x2, x21
   41d50:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   41d54:	b	41dd0 <__gmpn_toom8_sqr@@Base+0xb80>
   41d58:	bl	d470 <__gmpn_toom6_sqr@plt>
   41d5c:	mov	x0, x25
   41d60:	mov	x1, x26
   41d64:	mov	x2, x27
   41d68:	mov	x3, x22
   41d6c:	mov	x26, x22
   41d70:	bl	d470 <__gmpn_toom6_sqr@plt>
   41d74:	mov	w5, #0x2                   	// #2
   41d78:	mov	w6, #0x4                   	// #4
   41d7c:	mov	x0, x25
   41d80:	mov	x1, x28
   41d84:	mov	x2, x20
   41d88:	mov	w3, wzr
   41d8c:	mov	x4, x21
   41d90:	bl	c970 <__gmpn_toom_couple_handling@plt>
   41d94:	ldr	x22, [sp, #32]
   41d98:	cmp	x24, #0x6e8
   41d9c:	b.le	41db8 <__gmpn_toom8_sqr@@Base+0xb68>
   41da0:	ldur	x1, [x29, #-8]
   41da4:	mov	x0, x20
   41da8:	mov	x2, x21
   41dac:	mov	x3, x26
   41db0:	bl	d470 <__gmpn_toom6_sqr@plt>
   41db4:	b	41dd0 <__gmpn_toom8_sqr@@Base+0xb80>
   41db8:	ldur	x1, [x29, #-8]
   41dbc:	add	x8, x19, x23, lsl #3
   41dc0:	add	x3, x8, #0x20
   41dc4:	mov	x0, x20
   41dc8:	mov	x2, x21
   41dcc:	bl	c220 <__gmpn_toom4_sqr@plt>
   41dd0:	ldur	x8, [x29, #-16]
   41dd4:	ldp	x1, x2, [x29, #-32]
   41dd8:	mov	x0, x20
   41ddc:	mov	x3, x22
   41de0:	lsl	x6, x8, #1
   41de4:	add	x8, x19, x23, lsl #3
   41de8:	add	x8, x8, #0x20
   41dec:	mov	x4, x19
   41df0:	mov	x5, x21
   41df4:	mov	w7, wzr
   41df8:	str	x8, [sp]
   41dfc:	bl	d350 <__gmpn_toom_interpolate_16pts@plt>
   41e00:	ldp	x20, x19, [sp, #160]
   41e04:	ldp	x22, x21, [sp, #144]
   41e08:	ldp	x24, x23, [sp, #128]
   41e0c:	ldp	x26, x25, [sp, #112]
   41e10:	ldp	x28, x27, [sp, #96]
   41e14:	ldp	x29, x30, [sp, #80]
   41e18:	add	sp, sp, #0xb0
   41e1c:	ret

0000000000041e20 <__gmpn_toom_couple_handling@@Base>:
   41e20:	stp	x29, x30, [sp, #-64]!
   41e24:	stp	x20, x19, [sp, #48]
   41e28:	mov	x19, x0
   41e2c:	stp	x24, x23, [sp, #16]
   41e30:	stp	x22, x21, [sp, #32]
   41e34:	mov	w23, w6
   41e38:	mov	w24, w5
   41e3c:	mov	x21, x4
   41e40:	mov	x22, x2
   41e44:	mov	x20, x1
   41e48:	mov	x0, x2
   41e4c:	mov	x1, x19
   41e50:	mov	x29, sp
   41e54:	cbz	w3, 41e64 <__gmpn_toom_couple_handling@@Base+0x44>
   41e58:	mov	x3, x20
   41e5c:	bl	c840 <__gmpn_rsh1sub_n@plt>
   41e60:	b	41e6c <__gmpn_toom_couple_handling@@Base+0x4c>
   41e64:	mov	x3, x20
   41e68:	bl	c950 <__gmpn_rsh1add_n@plt>
   41e6c:	mov	x0, x19
   41e70:	mov	x1, x19
   41e74:	mov	x2, x22
   41e78:	mov	x3, x20
   41e7c:	cmp	w24, #0x1
   41e80:	b.ne	41e8c <__gmpn_toom_couple_handling@@Base+0x6c>  // b.any
   41e84:	bl	c840 <__gmpn_rsh1sub_n@plt>
   41e88:	b	41eac <__gmpn_toom_couple_handling@@Base+0x8c>
   41e8c:	bl	c2d0 <__gmpn_sub_n@plt>
   41e90:	cmp	w24, #0x1
   41e94:	b.lt	41eac <__gmpn_toom_couple_handling@@Base+0x8c>  // b.tstop
   41e98:	mov	x0, x19
   41e9c:	mov	x1, x19
   41ea0:	mov	x2, x20
   41ea4:	mov	w3, w24
   41ea8:	bl	c1a0 <__gmpn_rshift@plt>
   41eac:	cmp	w23, #0x1
   41eb0:	b.lt	41ec8 <__gmpn_toom_couple_handling@@Base+0xa8>  // b.tstop
   41eb4:	mov	x0, x22
   41eb8:	mov	x1, x22
   41ebc:	mov	x2, x20
   41ec0:	mov	w3, w23
   41ec4:	bl	c1a0 <__gmpn_rshift@plt>
   41ec8:	lsl	x23, x21, #3
   41ecc:	add	x0, x19, x23
   41ed0:	sub	x3, x20, x21
   41ed4:	mov	x1, x0
   41ed8:	mov	x2, x22
   41edc:	bl	ca70 <__gmpn_add_n@plt>
   41ee0:	lsl	x10, x20, #3
   41ee4:	add	x14, x19, x10
   41ee8:	add	x8, x22, x10
   41eec:	str	x0, [x14]
   41ef0:	sub	x15, x8, x23
   41ef4:	ldr	x9, [x15]
   41ef8:	adds	x9, x9, x0
   41efc:	str	x9, [x14]
   41f00:	b.cc	41fec <__gmpn_toom_couple_handling@@Base+0x1cc>  // b.lo, b.ul, b.last
   41f04:	neg	x9, x21
   41f08:	sub	x11, x21, #0x1
   41f0c:	add	x12, x22, x9, lsl #3
   41f10:	mov	w9, #0x1                   	// #1
   41f14:	mov	x13, x19
   41f18:	cmp	x9, x21
   41f1c:	b.ge	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.tcont
   41f20:	add	x16, x12, x10
   41f24:	ldr	x16, [x16, #8]
   41f28:	add	x17, x13, x10
   41f2c:	add	x9, x9, #0x1
   41f30:	add	x13, x13, #0x8
   41f34:	add	x12, x12, #0x8
   41f38:	adds	x16, x16, #0x1
   41f3c:	sub	x11, x11, #0x1
   41f40:	str	x16, [x17, #8]
   41f44:	b.cs	41f18 <__gmpn_toom_couple_handling@@Base+0xf8>  // b.hs, b.nlast
   41f48:	cmp	x15, x14
   41f4c:	b.eq	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   41f50:	cmp	x9, x21
   41f54:	b.ge	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.tcont
   41f58:	sub	x14, x21, x9
   41f5c:	cmp	x14, #0x4
   41f60:	b.cc	41fcc <__gmpn_toom_couple_handling@@Base+0x1ac>  // b.lo, b.ul, b.last
   41f64:	add	x15, x13, x10
   41f68:	add	x15, x15, #0x8
   41f6c:	cmp	x15, x8
   41f70:	b.cs	41f8c <__gmpn_toom_couple_handling@@Base+0x16c>  // b.hs, b.nlast
   41f74:	add	x15, x21, x20
   41f78:	add	x16, x12, x10
   41f7c:	add	x15, x19, x15, lsl #3
   41f80:	add	x16, x16, #0x8
   41f84:	cmp	x16, x15
   41f88:	b.cc	41fcc <__gmpn_toom_couple_handling@@Base+0x1ac>  // b.lo, b.ul, b.last
   41f8c:	add	x13, x13, x10
   41f90:	add	x12, x12, x10
   41f94:	and	x10, x14, #0xfffffffffffffffc
   41f98:	and	x15, x11, #0xfffffffffffffffc
   41f9c:	add	x11, x13, #0x18
   41fa0:	add	x12, x12, #0x18
   41fa4:	add	x9, x15, x9
   41fa8:	mov	x13, x10
   41fac:	ldp	q0, q1, [x12, #-16]
   41fb0:	subs	x13, x13, #0x4
   41fb4:	add	x12, x12, #0x20
   41fb8:	stp	q0, q1, [x11, #-16]
   41fbc:	add	x11, x11, #0x20
   41fc0:	b.ne	41fac <__gmpn_toom_couple_handling@@Base+0x18c>  // b.any
   41fc4:	cmp	x14, x10
   41fc8:	b.eq	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   41fcc:	sub	x10, x9, x21
   41fd0:	add	x9, x9, x20
   41fd4:	add	x9, x19, x9, lsl #3
   41fd8:	ldr	x11, [x8, x10, lsl #3]
   41fdc:	adds	x10, x10, #0x1
   41fe0:	str	x11, [x9], #8
   41fe4:	b.cc	41fd8 <__gmpn_toom_couple_handling@@Base+0x1b8>  // b.lo, b.ul, b.last
   41fe8:	b	42050 <__gmpn_toom_couple_handling@@Base+0x230>
   41fec:	cmp	x21, #0x2
   41ff0:	b.lt	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.tstop
   41ff4:	cmp	x15, x14
   41ff8:	b.eq	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   41ffc:	sub	x9, x21, #0x1
   42000:	cmp	x9, #0x4
   42004:	b.cc	42030 <__gmpn_toom_couple_handling@@Base+0x210>  // b.lo, b.ul, b.last
   42008:	add	x10, x20, #0x1
   4200c:	add	x11, x19, x10, lsl #3
   42010:	cmp	x11, x8
   42014:	b.cs	42064 <__gmpn_toom_couple_handling@@Base+0x244>  // b.hs, b.nlast
   42018:	add	x11, x21, x20
   4201c:	sub	x10, x10, x21
   42020:	add	x11, x19, x11, lsl #3
   42024:	add	x10, x22, x10, lsl #3
   42028:	cmp	x10, x11
   4202c:	b.cs	42064 <__gmpn_toom_couple_handling@@Base+0x244>  // b.hs, b.nlast
   42030:	mov	w10, #0x1                   	// #1
   42034:	sub	x9, x10, x21
   42038:	add	x10, x10, x20
   4203c:	add	x10, x19, x10, lsl #3
   42040:	ldr	x11, [x8, x9, lsl #3]
   42044:	adds	x9, x9, #0x1
   42048:	str	x11, [x10], #8
   4204c:	b.cc	42040 <__gmpn_toom_couple_handling@@Base+0x220>  // b.lo, b.ul, b.last
   42050:	ldp	x20, x19, [sp, #48]
   42054:	ldp	x22, x21, [sp, #32]
   42058:	ldp	x24, x23, [sp, #16]
   4205c:	ldp	x29, x30, [sp], #64
   42060:	ret
   42064:	add	x13, x20, #0x3
   42068:	and	x11, x9, #0xfffffffffffffffc
   4206c:	sub	x12, x13, x21
   42070:	orr	x10, x11, #0x1
   42074:	add	x12, x22, x12, lsl #3
   42078:	add	x13, x19, x13, lsl #3
   4207c:	mov	x14, x11
   42080:	ldp	q0, q1, [x12, #-16]
   42084:	subs	x14, x14, #0x4
   42088:	add	x12, x12, #0x20
   4208c:	stp	q0, q1, [x13, #-16]
   42090:	add	x13, x13, #0x20
   42094:	b.ne	42080 <__gmpn_toom_couple_handling@@Base+0x260>  // b.any
   42098:	cmp	x9, x11
   4209c:	b.eq	42050 <__gmpn_toom_couple_handling@@Base+0x230>  // b.none
   420a0:	b	42034 <__gmpn_toom_couple_handling@@Base+0x214>

00000000000420a4 <__gmpn_toom2_sqr@@Base>:
   420a4:	sub	sp, sp, #0x70
   420a8:	stp	x22, x21, [sp, #80]
   420ac:	asr	x21, x2, #1
   420b0:	sub	x22, x2, x21
   420b4:	stp	x28, x27, [sp, #32]
   420b8:	stp	x24, x23, [sp, #64]
   420bc:	stp	x20, x19, [sp, #96]
   420c0:	mov	x27, x3
   420c4:	mov	x28, x2
   420c8:	mov	x24, x1
   420cc:	mov	x19, x0
   420d0:	cmp	x21, x22
   420d4:	add	x2, x1, x21, lsl #3
   420d8:	stp	x29, x30, [sp, #16]
   420dc:	stp	x26, x25, [sp, #48]
   420e0:	add	x29, sp, #0x10
   420e4:	b.ne	42128 <__gmpn_toom2_sqr@@Base+0x84>  // b.any
   420e8:	lsl	x8, x21, #4
   420ec:	sub	x8, x8, #0x8
   420f0:	mov	x10, x21
   420f4:	subs	x9, x10, #0x1
   420f8:	b.lt	4211c <__gmpn_toom2_sqr@@Base+0x78>  // b.tstop
   420fc:	add	x10, x24, x10, lsl #3
   42100:	ldur	x10, [x10, #-8]
   42104:	ldr	x11, [x24, x8]
   42108:	sub	x8, x8, #0x8
   4210c:	cmp	x10, x11
   42110:	mov	x10, x9
   42114:	b.eq	420f4 <__gmpn_toom2_sqr@@Base+0x50>  // b.none
   42118:	b.ls	421c4 <__gmpn_toom2_sqr@@Base+0x120>  // b.plast
   4211c:	mov	x0, x19
   42120:	mov	x1, x24
   42124:	b	421d0 <__gmpn_toom2_sqr@@Base+0x12c>
   42128:	ldr	x23, [x2]
   4212c:	cbz	x23, 42158 <__gmpn_toom2_sqr@@Base+0xb4>
   42130:	add	x2, x24, x22, lsl #3
   42134:	mov	x0, x19
   42138:	mov	x1, x24
   4213c:	mov	x3, x21
   42140:	bl	c2d0 <__gmpn_sub_n@plt>
   42144:	sub	x8, x23, x0
   42148:	str	x8, [x19, x21, lsl #3]
   4214c:	cmp	x22, #0x11
   42150:	b.le	421ac <__gmpn_toom2_sqr@@Base+0x108>
   42154:	b	421e0 <__gmpn_toom2_sqr@@Base+0x13c>
   42158:	lsl	x8, x28, #3
   4215c:	add	x1, x24, x22, lsl #3
   42160:	sub	x8, x8, #0x8
   42164:	mov	x10, x21
   42168:	subs	x9, x10, #0x1
   4216c:	b.lt	42130 <__gmpn_toom2_sqr@@Base+0x8c>  // b.tstop
   42170:	add	x10, x24, x10, lsl #3
   42174:	ldur	x10, [x10, #-8]
   42178:	ldr	x11, [x24, x8]
   4217c:	sub	x8, x8, #0x8
   42180:	cmp	x10, x11
   42184:	mov	x10, x9
   42188:	b.eq	42168 <__gmpn_toom2_sqr@@Base+0xc4>  // b.none
   4218c:	b.hi	42130 <__gmpn_toom2_sqr@@Base+0x8c>  // b.pmore
   42190:	mov	x0, x19
   42194:	mov	x2, x24
   42198:	mov	x3, x21
   4219c:	bl	c2d0 <__gmpn_sub_n@plt>
   421a0:	str	xzr, [x19, x21, lsl #3]
   421a4:	cmp	x22, #0x11
   421a8:	b.gt	421e0 <__gmpn_toom2_sqr@@Base+0x13c>
   421ac:	mov	x0, x27
   421b0:	mov	x1, x19
   421b4:	mov	x2, x22
   421b8:	bl	c190 <__gmpn_sqr_basecase@plt>
   421bc:	lsl	x25, x22, #1
   421c0:	b	421f8 <__gmpn_toom2_sqr@@Base+0x154>
   421c4:	mov	x0, x19
   421c8:	mov	x1, x2
   421cc:	mov	x2, x24
   421d0:	mov	x3, x21
   421d4:	bl	c2d0 <__gmpn_sub_n@plt>
   421d8:	cmp	x22, #0x11
   421dc:	b.le	421ac <__gmpn_toom2_sqr@@Base+0x108>
   421e0:	add	x3, x27, x22, lsl #4
   421e4:	mov	x0, x27
   421e8:	mov	x1, x19
   421ec:	mov	x2, x22
   421f0:	lsl	x25, x22, #1
   421f4:	bl	c050 <__gmpn_toom2_sqr@plt>
   421f8:	add	x26, x19, x25, lsl #3
   421fc:	cmp	x28, #0x23
   42200:	add	x1, x24, x22, lsl #3
   42204:	b.le	42238 <__gmpn_toom2_sqr@@Base+0x194>
   42208:	add	x3, x27, x25, lsl #3
   4220c:	mov	x0, x26
   42210:	mov	x2, x21
   42214:	bl	c050 <__gmpn_toom2_sqr@plt>
   42218:	cmp	x22, #0x11
   4221c:	b.gt	4224c <__gmpn_toom2_sqr@@Base+0x1a8>
   42220:	mov	x0, x19
   42224:	mov	x1, x24
   42228:	mov	x2, x22
   4222c:	mov	x20, x27
   42230:	bl	c190 <__gmpn_sqr_basecase@plt>
   42234:	b	42264 <__gmpn_toom2_sqr@@Base+0x1c0>
   42238:	mov	x0, x26
   4223c:	mov	x2, x21
   42240:	bl	c190 <__gmpn_sqr_basecase@plt>
   42244:	cmp	x22, #0x11
   42248:	b.le	42220 <__gmpn_toom2_sqr@@Base+0x17c>
   4224c:	add	x3, x27, x25, lsl #3
   42250:	mov	x0, x19
   42254:	mov	x1, x24
   42258:	mov	x2, x22
   4225c:	mov	x20, x27
   42260:	bl	c050 <__gmpn_toom2_sqr@plt>
   42264:	add	x27, x19, x22, lsl #3
   42268:	mov	x0, x26
   4226c:	mov	x1, x27
   42270:	mov	x2, x26
   42274:	mov	x3, x22
   42278:	bl	ca70 <__gmpn_add_n@plt>
   4227c:	mov	x24, x0
   42280:	mov	x0, x27
   42284:	mov	x1, x26
   42288:	mov	x2, x19
   4228c:	mov	x3, x22
   42290:	bl	ca70 <__gmpn_add_n@plt>
   42294:	and	x8, x28, #0xfffffffffffffffe
   42298:	str	x28, [sp, #8]
   4229c:	subs	x23, x8, x22
   422a0:	mov	x28, x0
   422a4:	b.eq	422e4 <__gmpn_toom2_sqr@@Base+0x240>  // b.none
   422a8:	add	x2, x26, x22, lsl #3
   422ac:	mov	x0, x26
   422b0:	mov	x1, x26
   422b4:	mov	x3, x23
   422b8:	bl	ca70 <__gmpn_add_n@plt>
   422bc:	cbz	x0, 422e4 <__gmpn_toom2_sqr@@Base+0x240>
   422c0:	mov	w8, #0x1                   	// #1
   422c4:	cmp	x23, x22
   422c8:	b.ge	422e8 <__gmpn_toom2_sqr@@Base+0x244>  // b.tcont
   422cc:	lsl	x9, x23, #3
   422d0:	ldr	x10, [x26, x9]
   422d4:	add	x23, x23, #0x1
   422d8:	adds	x10, x10, #0x1
   422dc:	str	x10, [x26, x9]
   422e0:	b.cs	422c4 <__gmpn_toom2_sqr@@Base+0x220>  // b.hs, b.nlast
   422e4:	mov	x8, xzr
   422e8:	mov	x0, x27
   422ec:	mov	x1, x27
   422f0:	mov	x2, x20
   422f4:	mov	x3, x25
   422f8:	add	x23, x8, x24
   422fc:	bl	c2d0 <__gmpn_sub_n@plt>
   42300:	sub	x8, x23, x0
   42304:	cmp	x8, #0x3
   42308:	b.cs	42398 <__gmpn_toom2_sqr@@Base+0x2f4>  // b.hs, b.nlast
   4230c:	ldr	x9, [x26]
   42310:	add	x10, x28, x24
   42314:	adds	x9, x9, x10
   42318:	str	x9, [x26]
   4231c:	b.cc	42338 <__gmpn_toom2_sqr@@Base+0x294>  // b.lo, b.ul, b.last
   42320:	add	x9, x19, x25, lsl #3
   42324:	add	x9, x9, #0x8
   42328:	ldr	x10, [x9]
   4232c:	adds	x10, x10, #0x1
   42330:	str	x10, [x9], #8
   42334:	b.cs	42328 <__gmpn_toom2_sqr@@Base+0x284>  // b.hs, b.nlast
   42338:	mov	w9, #0x18                  	// #24
   4233c:	mul	x9, x22, x9
   42340:	ldr	x10, [x19, x9]
   42344:	adds	x8, x10, x8
   42348:	str	x8, [x19, x9]
   4234c:	ldr	x8, [sp, #8]
   42350:	b.cc	42378 <__gmpn_toom2_sqr@@Base+0x2d4>  // b.lo, b.ul, b.last
   42354:	add	x8, x8, x8, lsl #1
   42358:	add	x9, x21, x21, lsl #1
   4235c:	sub	x8, x8, x9
   42360:	add	x8, x19, x8, lsl #3
   42364:	add	x8, x8, #0x8
   42368:	ldr	x9, [x8]
   4236c:	adds	x9, x9, #0x1
   42370:	str	x9, [x8], #8
   42374:	b.cs	42368 <__gmpn_toom2_sqr@@Base+0x2c4>  // b.hs, b.nlast
   42378:	ldp	x20, x19, [sp, #96]
   4237c:	ldp	x22, x21, [sp, #80]
   42380:	ldp	x24, x23, [sp, #64]
   42384:	ldp	x26, x25, [sp, #48]
   42388:	ldp	x28, x27, [sp, #32]
   4238c:	ldp	x29, x30, [sp, #16]
   42390:	add	sp, sp, #0x70
   42394:	ret
   42398:	lsl	x2, x22, #3
   4239c:	mov	x0, x26
   423a0:	mov	w1, wzr
   423a4:	bl	c5f0 <memset@plt>
   423a8:	b	42378 <__gmpn_toom2_sqr@@Base+0x2d4>

00000000000423ac <__gmpn_toom3_sqr@@Base>:
   423ac:	sub	sp, sp, #0xa0
   423b0:	mov	x9, #0xaaaaaaaaaaaaaaaa    	// #-6148914691236517206
   423b4:	add	x8, x2, #0x2
   423b8:	movk	x9, #0xaaab
   423bc:	umulh	x8, x8, x9
   423c0:	stp	x22, x21, [sp, #128]
   423c4:	lsr	x21, x8, #1
   423c8:	lsl	x11, x21, #1
   423cc:	add	x8, x3, x21, lsl #5
   423d0:	lsl	x9, x21, #4
   423d4:	add	x10, x0, x21, lsl #3
   423d8:	stp	x29, x30, [sp, #64]
   423dc:	stp	x26, x25, [sp, #96]
   423e0:	stp	x24, x23, [sp, #112]
   423e4:	stp	x20, x19, [sp, #144]
   423e8:	add	x29, sp, #0x40
   423ec:	mov	x19, x3
   423f0:	mov	x22, x1
   423f4:	stp	x11, x0, [sp, #24]
   423f8:	str	x2, [sp]
   423fc:	subs	x11, x2, x11
   42400:	add	x24, x8, #0x20
   42404:	add	x20, x3, x9
   42408:	add	x26, x10, #0x8
   4240c:	add	x2, x1, x9
   42410:	lsl	x25, x21, #3
   42414:	stp	x28, x27, [sp, #80]
   42418:	stp	x2, x11, [x29, #-16]
   4241c:	b.eq	42464 <__gmpn_toom3_sqr@@Base+0xb8>  // b.none
   42420:	ldur	x23, [x29, #-8]
   42424:	mov	x0, x19
   42428:	mov	x1, x22
   4242c:	mov	x3, x23
   42430:	bl	ca70 <__gmpn_add_n@plt>
   42434:	cbz	x0, 42468 <__gmpn_toom3_sqr@@Base+0xbc>
   42438:	ldur	x23, [x29, #-8]
   4243c:	mov	w27, #0x1                   	// #1
   42440:	cmp	x23, x21
   42444:	b.ge	42504 <__gmpn_toom3_sqr@@Base+0x158>  // b.tcont
   42448:	lsl	x9, x23, #3
   4244c:	ldr	x10, [x22, x9]
   42450:	add	x23, x23, #0x1
   42454:	adds	x10, x10, #0x1
   42458:	str	x10, [x19, x9]
   4245c:	b.cs	42440 <__gmpn_toom3_sqr@@Base+0x94>  // b.hs, b.nlast
   42460:	b	42468 <__gmpn_toom3_sqr@@Base+0xbc>
   42464:	mov	x23, xzr
   42468:	cmp	x19, x22
   4246c:	mov	x27, xzr
   42470:	b.eq	42504 <__gmpn_toom3_sqr@@Base+0x158>  // b.none
   42474:	subs	x9, x21, x23
   42478:	b.le	42504 <__gmpn_toom3_sqr@@Base+0x158>
   4247c:	cmp	x9, #0x4
   42480:	b.cc	424e0 <__gmpn_toom3_sqr@@Base+0x134>  // b.lo, b.ul, b.last
   42484:	lsl	x11, x23, #3
   42488:	add	x10, x19, x11
   4248c:	add	x12, x22, x25
   42490:	cmp	x10, x12
   42494:	b.cs	424a8 <__gmpn_toom3_sqr@@Base+0xfc>  // b.hs, b.nlast
   42498:	add	x10, x19, x25
   4249c:	add	x12, x22, x11
   424a0:	cmp	x12, x10
   424a4:	b.cc	424e0 <__gmpn_toom3_sqr@@Base+0x134>  // b.lo, b.ul, b.last
   424a8:	and	x10, x9, #0xfffffffffffffffc
   424ac:	add	x12, x11, #0x10
   424b0:	add	x23, x23, x10
   424b4:	add	x11, x22, x12
   424b8:	add	x12, x19, x12
   424bc:	mov	x13, x10
   424c0:	ldp	q0, q1, [x11, #-16]
   424c4:	add	x11, x11, #0x20
   424c8:	subs	x13, x13, #0x4
   424cc:	stp	q0, q1, [x12, #-16]
   424d0:	add	x12, x12, #0x20
   424d4:	b.ne	424c0 <__gmpn_toom3_sqr@@Base+0x114>  // b.any
   424d8:	cmp	x9, x10
   424dc:	b.eq	42500 <__gmpn_toom3_sqr@@Base+0x154>  // b.none
   424e0:	lsl	x10, x23, #3
   424e4:	sub	x9, x21, x23
   424e8:	add	x8, x19, x10
   424ec:	add	x10, x22, x10
   424f0:	ldr	x11, [x10], #8
   424f4:	subs	x9, x9, #0x1
   424f8:	str	x11, [x8], #8
   424fc:	b.ne	424f0 <__gmpn_toom3_sqr@@Base+0x144>  // b.any
   42500:	mov	x27, xzr
   42504:	mov	x23, x26
   42508:	add	x26, x22, x25
   4250c:	add	x8, x20, #0x10
   42510:	mov	x0, x24
   42514:	mov	x1, x19
   42518:	mov	x2, x26
   4251c:	mov	x3, x21
   42520:	mov	x28, x20
   42524:	stur	x8, [x29, #-24]
   42528:	bl	ca70 <__gmpn_add_n@plt>
   4252c:	add	x8, x0, x27
   42530:	mov	x20, x24
   42534:	str	x25, [sp, #16]
   42538:	str	x8, [x24, x25]
   4253c:	cbz	x27, 426e4 <__gmpn_toom3_sqr@@Base+0x338>
   42540:	ldur	x24, [x29, #-24]
   42544:	mov	x1, x19
   42548:	mov	x2, x26
   4254c:	mov	x3, x21
   42550:	mov	x0, x24
   42554:	bl	c2d0 <__gmpn_sub_n@plt>
   42558:	sub	x8, x27, x0
   4255c:	lsl	x9, x21, #2
   42560:	str	x9, [sp, #8]
   42564:	str	x8, [x24, x21, lsl #3]
   42568:	mov	x25, x20
   4256c:	mov	x2, x20
   42570:	ldp	x1, x20, [x29, #-16]
   42574:	mov	x0, x23
   42578:	mov	x3, x20
   4257c:	bl	ca70 <__gmpn_add_n@plt>
   42580:	subs	x14, x21, x20
   42584:	b.eq	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   42588:	ldur	x8, [x29, #-8]
   4258c:	lsl	x8, x8, #3
   42590:	ldr	x9, [x25, x8]
   42594:	adds	x9, x9, x0
   42598:	str	x9, [x23, x8]
   4259c:	b.cc	42734 <__gmpn_toom3_sqr@@Base+0x388>  // b.lo, b.ul, b.last
   425a0:	ldr	x8, [sp, #32]
   425a4:	ldr	x11, [sp]
   425a8:	add	x9, x21, x21, lsl #1
   425ac:	add	x10, x19, x21, lsl #4
   425b0:	sub	x12, x8, x21, lsl #3
   425b4:	mvn	x8, x11
   425b8:	lsl	x13, x11, #3
   425bc:	mov	w0, #0x1                   	// #1
   425c0:	add	x11, x8, x9
   425c4:	mov	w8, #0x1                   	// #1
   425c8:	cmp	x8, x14
   425cc:	b.ge	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tcont
   425d0:	add	x15, x10, x13
   425d4:	ldr	x15, [x15, #40]
   425d8:	add	x16, x12, x13
   425dc:	add	x8, x8, #0x1
   425e0:	add	x12, x12, #0x8
   425e4:	add	x10, x10, #0x8
   425e8:	adds	x15, x15, #0x1
   425ec:	sub	x11, x11, #0x1
   425f0:	str	x15, [x16, #16]
   425f4:	b.cs	425c8 <__gmpn_toom3_sqr@@Base+0x21c>  // b.hs, b.nlast
   425f8:	cmp	x25, x23
   425fc:	mov	x0, xzr
   42600:	b.eq	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   42604:	cmp	x8, x14
   42608:	b.ge	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tcont
   4260c:	ldr	x0, [sp]
   42610:	add	x14, x21, x21, lsl #1
   42614:	sub	x15, x14, x0
   42618:	sub	x15, x15, x8
   4261c:	cmp	x15, #0x4
   42620:	b.cc	426a8 <__gmpn_toom3_sqr@@Base+0x2fc>  // b.lo, b.ul, b.last
   42624:	mov	w17, #0x28                  	// #40
   42628:	add	x16, x12, x13
   4262c:	madd	x17, x21, x17, x19
   42630:	add	x16, x16, #0x10
   42634:	add	x17, x17, #0x20
   42638:	cmp	x16, x17
   4263c:	b.cs	42660 <__gmpn_toom3_sqr@@Base+0x2b4>  // b.hs, b.nlast
   42640:	ldr	x18, [sp, #32]
   42644:	mov	w16, #0x8                   	// #8
   42648:	add	x17, x10, x13
   4264c:	bfi	x16, x21, #4, #60
   42650:	add	x16, x18, x16
   42654:	add	x17, x17, #0x28
   42658:	cmp	x17, x16
   4265c:	b.cc	426a8 <__gmpn_toom3_sqr@@Base+0x2fc>  // b.lo, b.ul, b.last
   42660:	add	x12, x12, x13
   42664:	sub	x16, x9, x0
   42668:	add	x13, x10, x13
   4266c:	and	x17, x11, #0xfffffffffffffffc
   42670:	add	x10, x12, #0x20
   42674:	sub	x12, x16, x8
   42678:	and	x9, x15, #0xfffffffffffffffc
   4267c:	add	x11, x13, #0x38
   42680:	add	x8, x17, x8
   42684:	and	x12, x12, #0xfffffffffffffffc
   42688:	ldp	q0, q1, [x11, #-16]
   4268c:	subs	x12, x12, #0x4
   42690:	add	x11, x11, #0x20
   42694:	stp	q0, q1, [x10, #-16]
   42698:	add	x10, x10, #0x20
   4269c:	b.ne	42688 <__gmpn_toom3_sqr@@Base+0x2dc>  // b.any
   426a0:	cmp	x15, x9
   426a4:	b.eq	427e4 <__gmpn_toom3_sqr@@Base+0x438>  // b.none
   426a8:	ldr	x11, [sp, #32]
   426ac:	sub	x9, x14, x8
   426b0:	add	x10, x8, x0
   426b4:	sub	x8, x9, x0
   426b8:	sub	x9, x10, x21
   426bc:	add	x10, x10, x21, lsl #1
   426c0:	add	x9, x11, x9, lsl #3
   426c4:	add	x10, x19, x10, lsl #3
   426c8:	add	x9, x9, #0x8
   426cc:	add	x10, x10, #0x20
   426d0:	ldr	x11, [x10], #8
   426d4:	subs	x8, x8, #0x1
   426d8:	str	x11, [x9], #8
   426dc:	b.ne	426d0 <__gmpn_toom3_sqr@@Base+0x324>  // b.any
   426e0:	b	427e4 <__gmpn_toom3_sqr@@Base+0x438>
   426e4:	add	x8, x22, x21, lsl #4
   426e8:	sub	x8, x8, #0x8
   426ec:	mov	x10, x21
   426f0:	subs	x9, x10, #0x1
   426f4:	b.lt	42540 <__gmpn_toom3_sqr@@Base+0x194>  // b.tstop
   426f8:	add	x10, x19, x10, lsl #3
   426fc:	ldur	x10, [x10, #-8]
   42700:	ldr	x11, [x8], #-8
   42704:	cmp	x10, x11
   42708:	mov	x10, x9
   4270c:	b.eq	426f0 <__gmpn_toom3_sqr@@Base+0x344>  // b.none
   42710:	b.hi	42540 <__gmpn_toom3_sqr@@Base+0x194>  // b.pmore
   42714:	ldur	x24, [x29, #-24]
   42718:	mov	x1, x26
   4271c:	mov	x2, x19
   42720:	mov	x3, x21
   42724:	mov	x0, x24
   42728:	bl	c2d0 <__gmpn_sub_n@plt>
   4272c:	mov	x8, xzr
   42730:	b	4255c <__gmpn_toom3_sqr@@Base+0x1b0>
   42734:	cmp	x25, x23
   42738:	mov	x0, xzr
   4273c:	b.eq	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.none
   42740:	cmp	x14, #0x2
   42744:	b.lt	427e8 <__gmpn_toom3_sqr@@Base+0x43c>  // b.tstop
   42748:	ldr	x15, [sp]
   4274c:	add	x8, x21, x21, lsl #1
   42750:	mvn	x9, x15
   42754:	add	x9, x8, x9
   42758:	cmp	x9, #0x4
   4275c:	b.cc	427a8 <__gmpn_toom3_sqr@@Base+0x3fc>  // b.lo, b.ul, b.last
   42760:	ldr	x13, [sp, #32]
   42764:	sub	x10, x15, x21
   42768:	mov	w12, #0x28                  	// #40
   4276c:	add	x11, x15, x21, lsl #1
   42770:	add	x13, x13, x10, lsl #3
   42774:	madd	x10, x21, x12, x19
   42778:	add	x12, x13, #0x10
   4277c:	add	x10, x10, #0x20
   42780:	cmp	x12, x10
   42784:	add	x10, x19, x11, lsl #3
   42788:	b.cs	428f0 <__gmpn_toom3_sqr@@Base+0x544>  // b.hs, b.nlast
   4278c:	ldr	x12, [sp, #32]
   42790:	mov	w11, #0x8                   	// #8
   42794:	bfi	x11, x21, #4, #60
   42798:	add	x11, x12, x11
   4279c:	add	x12, x10, #0x28
   427a0:	cmp	x12, x11
   427a4:	b.cs	428f0 <__gmpn_toom3_sqr@@Base+0x544>  // b.hs, b.nlast
   427a8:	mov	w10, #0x1                   	// #1
   427ac:	ldr	x11, [sp, #32]
   427b0:	add	x9, x10, x15
   427b4:	sub	x8, x8, x10
   427b8:	sub	x10, x9, x21
   427bc:	add	x9, x9, x21, lsl #1
   427c0:	add	x10, x11, x10, lsl #3
   427c4:	add	x11, x19, x9, lsl #3
   427c8:	sub	x8, x8, x15
   427cc:	add	x9, x10, #0x8
   427d0:	add	x10, x11, #0x20
   427d4:	ldr	x11, [x10], #8
   427d8:	subs	x8, x8, #0x1
   427dc:	str	x11, [x9], #8
   427e0:	b.ne	427d4 <__gmpn_toom3_sqr@@Base+0x428>  // b.any
   427e4:	mov	x0, xzr
   427e8:	ldr	x24, [sp, #16]
   427ec:	str	x25, [sp]
   427f0:	mov	x1, x22
   427f4:	mov	x2, x23
   427f8:	ldr	x8, [x25, x24]
   427fc:	mov	x3, x21
   42800:	add	x25, x8, x0
   42804:	mov	x0, x23
   42808:	bl	d090 <__gmpn_rsblsh1_n@plt>
   4280c:	add	x8, x0, x25, lsl #1
   42810:	mov	w9, #0x28                  	// #40
   42814:	ldur	x1, [x29, #-24]
   42818:	str	x8, [x23, x24]
   4281c:	madd	x8, x21, x9, x19
   42820:	add	x25, x21, #0x1
   42824:	add	x27, x8, #0x28
   42828:	mov	x0, x19
   4282c:	mov	x2, x25
   42830:	mov	x3, x27
   42834:	bl	c050 <__gmpn_toom2_sqr@plt>
   42838:	add	x28, x28, #0x8
   4283c:	mov	x0, x28
   42840:	mov	x1, x23
   42844:	mov	x2, x25
   42848:	mov	x3, x27
   4284c:	bl	c050 <__gmpn_toom2_sqr@plt>
   42850:	ldr	x24, [sp, #32]
   42854:	ldr	x8, [sp, #8]
   42858:	ldp	x1, x23, [x29, #-16]
   4285c:	mov	x3, x27
   42860:	add	x26, x24, x8, lsl #3
   42864:	mov	x0, x26
   42868:	mov	x2, x23
   4286c:	bl	c050 <__gmpn_toom2_sqr@plt>
   42870:	ldr	x8, [x26]
   42874:	mov	x20, x19
   42878:	mov	x19, x23
   4287c:	ldr	x23, [x26, #8]
   42880:	stur	x8, [x29, #-16]
   42884:	ldr	x8, [sp, #24]
   42888:	ldr	x1, [sp]
   4288c:	mov	x2, x25
   42890:	mov	x3, x27
   42894:	add	x0, x24, x8, lsl #3
   42898:	bl	c050 <__gmpn_toom2_sqr@plt>
   4289c:	mov	x0, x24
   428a0:	mov	x1, x22
   428a4:	mov	x2, x21
   428a8:	mov	x3, x27
   428ac:	str	x23, [x26, #8]
   428b0:	bl	c050 <__gmpn_toom2_sqr@plt>
   428b4:	lsl	x4, x19, #1
   428b8:	mov	x0, x24
   428bc:	mov	x1, x28
   428c0:	mov	x2, x20
   428c4:	mov	x3, x21
   428c8:	ldur	x6, [x29, #-16]
   428cc:	ldp	x20, x19, [sp, #144]
   428d0:	ldp	x22, x21, [sp, #128]
   428d4:	ldp	x24, x23, [sp, #112]
   428d8:	ldp	x26, x25, [sp, #96]
   428dc:	ldp	x28, x27, [sp, #80]
   428e0:	ldp	x29, x30, [sp, #64]
   428e4:	mov	w5, wzr
   428e8:	add	sp, sp, #0xa0
   428ec:	b	ca20 <__gmpn_toom_interpolate_5pts@plt>
   428f0:	and	x11, x9, #0xfffffffffffffffc
   428f4:	add	x12, x10, #0x38
   428f8:	orr	x10, x11, #0x1
   428fc:	add	x13, x13, #0x20
   42900:	mov	x14, x11
   42904:	ldp	q0, q1, [x12, #-16]
   42908:	subs	x14, x14, #0x4
   4290c:	add	x12, x12, #0x20
   42910:	stp	q0, q1, [x13, #-16]
   42914:	add	x13, x13, #0x20
   42918:	b.ne	42904 <__gmpn_toom3_sqr@@Base+0x558>  // b.any
   4291c:	cmp	x9, x11
   42920:	b.eq	427e4 <__gmpn_toom3_sqr@@Base+0x438>  // b.none
   42924:	b	427ac <__gmpn_toom3_sqr@@Base+0x400>

0000000000042928 <__gmpn_toom4_sqr@@Base>:
   42928:	sub	sp, sp, #0xb0
   4292c:	add	x8, x2, #0x3
   42930:	stp	x24, x23, [sp, #128]
   42934:	stp	x22, x21, [sp, #144]
   42938:	asr	x21, x8, #2
   4293c:	and	x23, x8, #0xfffffffffffffffc
   42940:	add	x8, x0, x23, lsl #3
   42944:	add	x9, x3, x21, lsl #6
   42948:	stp	x26, x25, [sp, #112]
   4294c:	mov	x24, x1
   42950:	add	x10, x21, x21, lsl #1
   42954:	add	x25, x8, #0x10
   42958:	add	x22, x9, #0x28
   4295c:	stp	x29, x30, [sp, #80]
   42960:	stp	x20, x19, [sp, #160]
   42964:	add	x29, sp, #0x50
   42968:	mov	x19, x3
   4296c:	mov	x26, x2
   42970:	sub	x4, x2, x10
   42974:	mov	x1, x25
   42978:	mov	x2, x24
   4297c:	mov	x3, x21
   42980:	mov	x5, x22
   42984:	stp	x28, x27, [sp, #96]
   42988:	mov	x20, x0
   4298c:	stp	x4, x10, [x29, #-24]
   42990:	bl	cd60 <__gmpn_toom_eval_dgr3_pm2@plt>
   42994:	add	x27, x21, #0x1
   42998:	cmp	x26, #0x104
   4299c:	mov	x0, x19
   429a0:	mov	x1, x20
   429a4:	mov	x2, x27
   429a8:	mov	x3, x22
   429ac:	str	x26, [sp, #40]
   429b0:	stur	x19, [x29, #-8]
   429b4:	stur	x25, [x29, #-32]
   429b8:	str	x23, [sp, #32]
   429bc:	b.le	429e8 <__gmpn_toom4_sqr@@Base+0xc0>
   429c0:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   429c4:	add	x8, x19, x21, lsl #4
   429c8:	add	x0, x8, #0x8
   429cc:	mov	x1, x25
   429d0:	mov	x2, x27
   429d4:	mov	x3, x22
   429d8:	lsl	x28, x21, #1
   429dc:	str	x0, [sp, #24]
   429e0:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   429e4:	b	42a0c <__gmpn_toom4_sqr@@Base+0xe4>
   429e8:	bl	c050 <__gmpn_toom2_sqr@plt>
   429ec:	add	x8, x19, x21, lsl #4
   429f0:	add	x0, x8, #0x8
   429f4:	mov	x1, x25
   429f8:	mov	x2, x27
   429fc:	mov	x3, x22
   42a00:	lsl	x28, x21, #1
   42a04:	str	x0, [sp, #24]
   42a08:	bl	c050 <__gmpn_toom2_sqr@plt>
   42a0c:	add	x1, x24, x21, lsl #3
   42a10:	mov	x0, x20
   42a14:	mov	x2, x24
   42a18:	mov	x3, x21
   42a1c:	bl	cc40 <__gmpn_addlsh1_n@plt>
   42a20:	mov	x26, x0
   42a24:	add	x1, x24, x28, lsl #3
   42a28:	mov	x0, x20
   42a2c:	mov	x2, x20
   42a30:	mov	x3, x21
   42a34:	mov	x23, x28
   42a38:	bl	cc40 <__gmpn_addlsh1_n@plt>
   42a3c:	ldur	x19, [x29, #-24]
   42a40:	add	x28, x0, x26, lsl #1
   42a44:	subs	x25, x21, x19
   42a48:	b.le	42ac0 <__gmpn_toom4_sqr@@Base+0x198>
   42a4c:	ldur	x8, [x29, #-16]
   42a50:	mov	x0, x20
   42a54:	mov	x2, x20
   42a58:	mov	x3, x19
   42a5c:	add	x1, x24, x8, lsl #3
   42a60:	bl	cc40 <__gmpn_addlsh1_n@plt>
   42a64:	add	x26, x20, x19, lsl #3
   42a68:	str	x0, [sp, #16]
   42a6c:	mov	w3, #0x1                   	// #1
   42a70:	mov	x0, x26
   42a74:	mov	x1, x26
   42a78:	mov	x2, x25
   42a7c:	bl	c180 <__gmpn_lshift@plt>
   42a80:	add	x8, x0, x28, lsl #1
   42a84:	str	x8, [x20, x21, lsl #3]
   42a88:	ldr	x8, [x26]
   42a8c:	ldr	x9, [sp, #16]
   42a90:	ldur	x25, [x29, #-8]
   42a94:	mov	x28, x23
   42a98:	adds	x8, x8, x9
   42a9c:	str	x8, [x26]
   42aa0:	ldp	x10, x19, [sp, #32]
   42aa4:	b.cc	42aec <__gmpn_toom4_sqr@@Base+0x1c4>  // b.lo, b.ul, b.last
   42aa8:	add	x8, x26, #0x8
   42aac:	ldr	x9, [x8]
   42ab0:	adds	x9, x9, #0x1
   42ab4:	str	x9, [x8], #8
   42ab8:	b.cs	42aac <__gmpn_toom4_sqr@@Base+0x184>  // b.hs, b.nlast
   42abc:	b	42aec <__gmpn_toom4_sqr@@Base+0x1c4>
   42ac0:	ldur	x8, [x29, #-16]
   42ac4:	mov	x0, x20
   42ac8:	mov	x2, x20
   42acc:	mov	x3, x21
   42ad0:	add	x1, x24, x8, lsl #3
   42ad4:	bl	cc40 <__gmpn_addlsh1_n@plt>
   42ad8:	add	x8, x0, x28, lsl #1
   42adc:	str	x8, [x20, x21, lsl #3]
   42ae0:	ldur	x25, [x29, #-8]
   42ae4:	ldp	x10, x19, [sp, #32]
   42ae8:	mov	x28, x23
   42aec:	add	x8, x25, x10, lsl #3
   42af0:	cmp	x19, #0x104
   42af4:	add	x0, x8, #0x10
   42af8:	mov	x1, x20
   42afc:	mov	x2, x27
   42b00:	mov	x3, x22
   42b04:	str	x0, [sp, #40]
   42b08:	b.le	42b88 <__gmpn_toom4_sqr@@Base+0x260>
   42b0c:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b10:	ldp	x26, x23, [x29, #-32]
   42b14:	mov	x0, x20
   42b18:	mov	x2, x24
   42b1c:	mov	x3, x21
   42b20:	mov	x1, x26
   42b24:	mov	x4, x23
   42b28:	mov	x5, x22
   42b2c:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   42b30:	add	x0, x20, x28, lsl #3
   42b34:	mov	x1, x20
   42b38:	mov	x2, x27
   42b3c:	mov	x3, x22
   42b40:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b44:	add	x8, x21, x21, lsl #1
   42b48:	lsl	x28, x8, #1
   42b4c:	add	x8, x25, x8, lsl #4
   42b50:	add	x25, x8, #0x18
   42b54:	mov	x0, x25
   42b58:	mov	x1, x26
   42b5c:	mov	x2, x27
   42b60:	mov	x3, x22
   42b64:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b68:	cmp	x19, #0x108
   42b6c:	b.le	42be4 <__gmpn_toom4_sqr@@Base+0x2bc>
   42b70:	mov	x0, x20
   42b74:	mov	x1, x24
   42b78:	mov	x2, x21
   42b7c:	mov	x3, x22
   42b80:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42b84:	b	42bf8 <__gmpn_toom4_sqr@@Base+0x2d0>
   42b88:	bl	c050 <__gmpn_toom2_sqr@plt>
   42b8c:	ldp	x26, x23, [x29, #-32]
   42b90:	mov	x0, x20
   42b94:	mov	x2, x24
   42b98:	mov	x3, x21
   42b9c:	mov	x1, x26
   42ba0:	mov	x4, x23
   42ba4:	mov	x5, x22
   42ba8:	bl	c280 <__gmpn_toom_eval_dgr3_pm1@plt>
   42bac:	add	x0, x20, x28, lsl #3
   42bb0:	mov	x1, x20
   42bb4:	mov	x2, x27
   42bb8:	mov	x3, x22
   42bbc:	bl	c050 <__gmpn_toom2_sqr@plt>
   42bc0:	add	x8, x21, x21, lsl #1
   42bc4:	lsl	x28, x8, #1
   42bc8:	add	x8, x25, x8, lsl #4
   42bcc:	add	x25, x8, #0x18
   42bd0:	mov	x0, x25
   42bd4:	mov	x1, x26
   42bd8:	mov	x2, x27
   42bdc:	mov	x3, x22
   42be0:	bl	c050 <__gmpn_toom2_sqr@plt>
   42be4:	mov	x0, x20
   42be8:	mov	x1, x24
   42bec:	mov	x2, x21
   42bf0:	mov	x3, x22
   42bf4:	bl	c050 <__gmpn_toom2_sqr@plt>
   42bf8:	ldur	x8, [x29, #-16]
   42bfc:	add	x0, x20, x28, lsl #3
   42c00:	cmp	x23, #0x42
   42c04:	mov	x2, x23
   42c08:	add	x1, x24, x8, lsl #3
   42c0c:	mov	x3, x22
   42c10:	b.le	42c1c <__gmpn_toom4_sqr@@Base+0x2f4>
   42c14:	bl	d2a0 <__gmpn_toom3_sqr@plt>
   42c18:	b	42c20 <__gmpn_toom4_sqr@@Base+0x2f8>
   42c1c:	bl	c050 <__gmpn_toom2_sqr@plt>
   42c20:	ldr	x3, [sp, #24]
   42c24:	ldr	x6, [sp, #40]
   42c28:	ldur	x5, [x29, #-8]
   42c2c:	lsl	x7, x23, #1
   42c30:	mov	x0, x20
   42c34:	mov	x1, x21
   42c38:	mov	w2, wzr
   42c3c:	mov	x4, x25
   42c40:	str	x22, [sp]
   42c44:	bl	c810 <__gmpn_toom_interpolate_7pts@plt>
   42c48:	ldp	x20, x19, [sp, #160]
   42c4c:	ldp	x22, x21, [sp, #144]
   42c50:	ldp	x24, x23, [sp, #128]
   42c54:	ldp	x26, x25, [sp, #112]
   42c58:	ldp	x28, x27, [sp, #96]
   42c5c:	ldp	x29, x30, [sp, #80]
   42c60:	add	sp, sp, #0xb0
   42c64:	ret

0000000000042c68 <__gmpn_toom_eval_dgr3_pm1@@Base>:
   42c68:	stp	x29, x30, [sp, #-80]!
   42c6c:	stp	x24, x23, [sp, #32]
   42c70:	mov	x23, x2
   42c74:	stp	x22, x21, [sp, #48]
   42c78:	mov	x21, x1
   42c7c:	add	x2, x2, x3, lsl #4
   42c80:	mov	x1, x23
   42c84:	str	x25, [sp, #16]
   42c88:	stp	x20, x19, [sp, #64]
   42c8c:	mov	x29, sp
   42c90:	mov	x19, x5
   42c94:	mov	x24, x4
   42c98:	mov	x22, x3
   42c9c:	mov	x20, x0
   42ca0:	bl	ca70 <__gmpn_add_n@plt>
   42ca4:	lsl	x8, x22, #3
   42ca8:	add	x25, x23, x8
   42cac:	str	x0, [x20, x8]
   42cb0:	cbz	x24, 42cf4 <__gmpn_toom_eval_dgr3_pm1@@Base+0x8c>
   42cb4:	mov	w8, #0x18                  	// #24
   42cb8:	madd	x2, x22, x8, x23
   42cbc:	mov	x0, x19
   42cc0:	mov	x1, x25
   42cc4:	mov	x3, x24
   42cc8:	bl	ca70 <__gmpn_add_n@plt>
   42ccc:	cbz	x0, 42cf4 <__gmpn_toom_eval_dgr3_pm1@@Base+0x8c>
   42cd0:	mov	w8, #0x1                   	// #1
   42cd4:	cmp	x24, x22
   42cd8:	b.ge	42d94 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.tcont
   42cdc:	lsl	x9, x24, #3
   42ce0:	ldr	x10, [x25, x9]
   42ce4:	add	x24, x24, #0x1
   42ce8:	adds	x10, x10, #0x1
   42cec:	str	x10, [x19, x9]
   42cf0:	b.cs	42cd4 <__gmpn_toom_eval_dgr3_pm1@@Base+0x6c>  // b.hs, b.nlast
   42cf4:	cmp	x25, x19
   42cf8:	mov	x8, xzr
   42cfc:	b.eq	42d94 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.none
   42d00:	cmp	x24, x22
   42d04:	b.ge	42d94 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>  // b.tcont
   42d08:	sub	x8, x22, x24
   42d0c:	cmp	x8, #0x4
   42d10:	b.cc	42d74 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.lo, b.ul, b.last
   42d14:	add	x11, x19, x24, lsl #3
   42d18:	add	x9, x24, x22
   42d1c:	add	x10, x23, x22, lsl #4
   42d20:	cmp	x11, x10
   42d24:	add	x10, x23, x9, lsl #3
   42d28:	b.cs	42d38 <__gmpn_toom_eval_dgr3_pm1@@Base+0xd0>  // b.hs, b.nlast
   42d2c:	add	x9, x19, x22, lsl #3
   42d30:	cmp	x10, x9
   42d34:	b.cc	42d74 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.lo, b.ul, b.last
   42d38:	and	x9, x8, #0xfffffffffffffffc
   42d3c:	add	x10, x10, #0x10
   42d40:	add	x24, x24, x9
   42d44:	add	x11, x11, #0x10
   42d48:	mov	x12, x9
   42d4c:	ldp	q0, q1, [x10, #-16]
   42d50:	add	x10, x10, #0x20
   42d54:	subs	x12, x12, #0x4
   42d58:	stp	q0, q1, [x11, #-16]
   42d5c:	add	x11, x11, #0x20
   42d60:	b.ne	42d4c <__gmpn_toom_eval_dgr3_pm1@@Base+0xe4>  // b.any
   42d64:	cmp	x8, x9
   42d68:	b.ne	42d74 <__gmpn_toom_eval_dgr3_pm1@@Base+0x10c>  // b.any
   42d6c:	mov	x8, xzr
   42d70:	b	42d94 <__gmpn_toom_eval_dgr3_pm1@@Base+0x12c>
   42d74:	add	x10, x24, x22
   42d78:	sub	x8, x22, x24
   42d7c:	add	x9, x19, x24, lsl #3
   42d80:	add	x10, x23, x10, lsl #3
   42d84:	ldr	x11, [x10], #8
   42d88:	subs	x8, x8, #0x1
   42d8c:	str	x11, [x9], #8
   42d90:	b.ne	42d84 <__gmpn_toom_eval_dgr3_pm1@@Base+0x11c>  // b.any
   42d94:	add	x23, x22, #0x1
   42d98:	str	x8, [x19, x22, lsl #3]
   42d9c:	add	x8, x22, #0x1
   42da0:	cmp	x8, #0x1
   42da4:	b.lt	42dc4 <__gmpn_toom_eval_dgr3_pm1@@Base+0x15c>  // b.tstop
   42da8:	lsl	x8, x22, #3
   42dac:	ldr	x9, [x20, x8]
   42db0:	ldr	x8, [x19, x8]
   42db4:	sub	x22, x22, #0x1
   42db8:	cmp	x9, x8
   42dbc:	b.eq	42d9c <__gmpn_toom_eval_dgr3_pm1@@Base+0x134>  // b.none
   42dc0:	b.ls	42de0 <__gmpn_toom_eval_dgr3_pm1@@Base+0x178>  // b.plast
   42dc4:	mov	x0, x21
   42dc8:	mov	x1, x20
   42dcc:	mov	x2, x19
   42dd0:	mov	x3, x23
   42dd4:	bl	c2d0 <__gmpn_sub_n@plt>
   42dd8:	mov	w21, wzr
   42ddc:	b	42df8 <__gmpn_toom_eval_dgr3_pm1@@Base+0x190>
   42de0:	mov	x0, x21
   42de4:	mov	x1, x19
   42de8:	mov	x2, x20
   42dec:	mov	x3, x23
   42df0:	bl	c2d0 <__gmpn_sub_n@plt>
   42df4:	mov	w21, #0xffffffff            	// #-1
   42df8:	mov	x0, x20
   42dfc:	mov	x1, x20
   42e00:	mov	x2, x19
   42e04:	mov	x3, x23
   42e08:	bl	ca70 <__gmpn_add_n@plt>
   42e0c:	mov	w0, w21
   42e10:	ldp	x20, x19, [sp, #64]
   42e14:	ldp	x22, x21, [sp, #48]
   42e18:	ldp	x24, x23, [sp, #32]
   42e1c:	ldr	x25, [sp, #16]
   42e20:	ldp	x29, x30, [sp], #80
   42e24:	ret

0000000000042e28 <__gmpn_toom_eval_dgr3_pm2@@Base>:
   42e28:	stp	x29, x30, [sp, #-80]!
   42e2c:	stp	x24, x23, [sp, #32]
   42e30:	mov	x23, x2
   42e34:	stp	x22, x21, [sp, #48]
   42e38:	mov	x21, x1
   42e3c:	add	x2, x2, x3, lsl #4
   42e40:	mov	x1, x23
   42e44:	str	x25, [sp, #16]
   42e48:	stp	x20, x19, [sp, #64]
   42e4c:	mov	x29, sp
   42e50:	mov	x19, x5
   42e54:	mov	x24, x4
   42e58:	mov	x22, x3
   42e5c:	mov	x20, x0
   42e60:	bl	cba0 <__gmpn_addlsh2_n@plt>
   42e64:	lsl	x8, x22, #3
   42e68:	mov	w9, #0x18                  	// #24
   42e6c:	add	x25, x23, x8
   42e70:	str	x0, [x20, x8]
   42e74:	madd	x2, x22, x9, x23
   42e78:	mov	x0, x19
   42e7c:	mov	x1, x25
   42e80:	mov	x3, x24
   42e84:	bl	cba0 <__gmpn_addlsh2_n@plt>
   42e88:	subs	x13, x22, x24
   42e8c:	b.le	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>
   42e90:	lsl	x8, x24, #3
   42e94:	add	x9, x25, x8
   42e98:	ldr	x11, [x9]
   42e9c:	add	x10, x19, x8
   42ea0:	adds	x8, x11, x0
   42ea4:	str	x8, [x10]
   42ea8:	b.cc	42f9c <__gmpn_toom_eval_dgr3_pm2@@Base+0x174>  // b.lo, b.ul, b.last
   42eac:	mvn	x8, x24
   42eb0:	mov	x11, xzr
   42eb4:	mov	w0, #0x1                   	// #1
   42eb8:	add	x12, x8, x22
   42ebc:	mov	w8, #0x1                   	// #1
   42ec0:	cmp	x8, x13
   42ec4:	b.ge	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tcont
   42ec8:	add	x14, x9, x11
   42ecc:	ldr	x14, [x14, #8]
   42ed0:	add	x15, x10, x11
   42ed4:	add	x8, x8, #0x1
   42ed8:	add	x11, x11, #0x8
   42edc:	adds	x14, x14, #0x1
   42ee0:	sub	x12, x12, #0x1
   42ee4:	str	x14, [x15, #8]
   42ee8:	b.cs	42ec0 <__gmpn_toom_eval_dgr3_pm2@@Base+0x98>  // b.hs, b.nlast
   42eec:	cmp	x25, x19
   42ef0:	mov	x0, xzr
   42ef4:	b.eq	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.none
   42ef8:	cmp	x8, x13
   42efc:	b.ge	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tcont
   42f00:	sub	x13, x13, x8
   42f04:	cmp	x13, #0x4
   42f08:	b.cc	42f74 <__gmpn_toom_eval_dgr3_pm2@@Base+0x14c>  // b.lo, b.ul, b.last
   42f0c:	add	x14, x10, x11
   42f10:	add	x14, x14, #0x8
   42f14:	add	x15, x23, x22, lsl #4
   42f18:	cmp	x14, x15
   42f1c:	b.cs	42f34 <__gmpn_toom_eval_dgr3_pm2@@Base+0x10c>  // b.hs, b.nlast
   42f20:	add	x15, x9, x11
   42f24:	add	x14, x19, x22, lsl #3
   42f28:	add	x15, x15, #0x8
   42f2c:	cmp	x15, x14
   42f30:	b.cc	42f74 <__gmpn_toom_eval_dgr3_pm2@@Base+0x14c>  // b.lo, b.ul, b.last
   42f34:	add	x10, x10, x11
   42f38:	add	x11, x9, x11
   42f3c:	and	x9, x13, #0xfffffffffffffffc
   42f40:	and	x12, x12, #0xfffffffffffffffc
   42f44:	add	x10, x10, #0x18
   42f48:	add	x11, x11, #0x18
   42f4c:	add	x8, x12, x8
   42f50:	mov	x12, x9
   42f54:	ldp	q0, q1, [x11, #-16]
   42f58:	subs	x12, x12, #0x4
   42f5c:	add	x11, x11, #0x20
   42f60:	stp	q0, q1, [x10, #-16]
   42f64:	add	x10, x10, #0x20
   42f68:	b.ne	42f54 <__gmpn_toom_eval_dgr3_pm2@@Base+0x12c>  // b.any
   42f6c:	cmp	x13, x9
   42f70:	b.eq	43014 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>  // b.none
   42f74:	add	x10, x8, x24
   42f78:	sub	x8, x10, x22
   42f7c:	add	x9, x19, x10, lsl #3
   42f80:	add	x10, x10, x22
   42f84:	add	x10, x23, x10, lsl #3
   42f88:	ldr	x11, [x10], #8
   42f8c:	adds	x8, x8, #0x1
   42f90:	str	x11, [x9], #8
   42f94:	b.cc	42f88 <__gmpn_toom_eval_dgr3_pm2@@Base+0x160>  // b.lo, b.ul, b.last
   42f98:	b	43014 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>
   42f9c:	cmp	x25, x19
   42fa0:	mov	x0, xzr
   42fa4:	b.eq	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.none
   42fa8:	cmp	x13, #0x2
   42fac:	b.lt	43018 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1f0>  // b.tstop
   42fb0:	mvn	x8, x24
   42fb4:	add	x8, x8, x22
   42fb8:	cmp	x8, #0x4
   42fbc:	b.cc	42fec <__gmpn_toom_eval_dgr3_pm2@@Base+0x1c4>  // b.lo, b.ul, b.last
   42fc0:	add	x12, x19, x24, lsl #3
   42fc4:	add	x9, x24, x22
   42fc8:	add	x10, x12, #0x8
   42fcc:	add	x11, x23, x22, lsl #4
   42fd0:	cmp	x10, x11
   42fd4:	add	x9, x23, x9, lsl #3
   42fd8:	b.cs	430c0 <__gmpn_toom_eval_dgr3_pm2@@Base+0x298>  // b.hs, b.nlast
   42fdc:	add	x10, x19, x22, lsl #3
   42fe0:	add	x11, x9, #0x8
   42fe4:	cmp	x11, x10
   42fe8:	b.cs	430c0 <__gmpn_toom_eval_dgr3_pm2@@Base+0x298>  // b.hs, b.nlast
   42fec:	mov	w9, #0x1                   	// #1
   42ff0:	add	x10, x9, x24
   42ff4:	sub	x8, x10, x22
   42ff8:	add	x9, x19, x10, lsl #3
   42ffc:	add	x10, x10, x22
   43000:	add	x10, x23, x10, lsl #3
   43004:	ldr	x11, [x10], #8
   43008:	adds	x8, x8, #0x1
   4300c:	str	x11, [x9], #8
   43010:	b.cc	43004 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1dc>  // b.lo, b.ul, b.last
   43014:	mov	x0, xzr
   43018:	add	x23, x22, #0x1
   4301c:	str	x0, [x19, x22, lsl #3]
   43020:	mov	w3, #0x1                   	// #1
   43024:	mov	x0, x19
   43028:	mov	x1, x19
   4302c:	mov	x2, x23
   43030:	bl	c180 <__gmpn_lshift@plt>
   43034:	add	x8, x22, #0x1
   43038:	cmp	x8, #0x1
   4303c:	b.lt	4305c <__gmpn_toom_eval_dgr3_pm2@@Base+0x234>  // b.tstop
   43040:	lsl	x8, x22, #3
   43044:	ldr	x9, [x20, x8]
   43048:	ldr	x8, [x19, x8]
   4304c:	sub	x22, x22, #0x1
   43050:	cmp	x9, x8
   43054:	b.eq	43034 <__gmpn_toom_eval_dgr3_pm2@@Base+0x20c>  // b.none
   43058:	b.ls	43078 <__gmpn_toom_eval_dgr3_pm2@@Base+0x250>  // b.plast
   4305c:	mov	x0, x21
   43060:	mov	x1, x20
   43064:	mov	x2, x19
   43068:	mov	x3, x23
   4306c:	bl	c2d0 <__gmpn_sub_n@plt>
   43070:	mov	w21, wzr
   43074:	b	43090 <__gmpn_toom_eval_dgr3_pm2@@Base+0x268>
   43078:	mov	x0, x21
   4307c:	mov	x1, x19
   43080:	mov	x2, x20
   43084:	mov	x3, x23
   43088:	bl	c2d0 <__gmpn_sub_n@plt>
   4308c:	mov	w21, #0xffffffff            	// #-1
   43090:	mov	x0, x20
   43094:	mov	x1, x20
   43098:	mov	x2, x19
   4309c:	mov	x3, x23
   430a0:	bl	ca70 <__gmpn_add_n@plt>
   430a4:	mov	w0, w21
   430a8:	ldp	x20, x19, [sp, #64]
   430ac:	ldp	x22, x21, [sp, #48]
   430b0:	ldp	x24, x23, [sp, #32]
   430b4:	ldr	x25, [sp, #16]
   430b8:	ldp	x29, x30, [sp], #80
   430bc:	ret
   430c0:	and	x10, x8, #0xfffffffffffffffc
   430c4:	add	x11, x9, #0x18
   430c8:	orr	x9, x10, #0x1
   430cc:	add	x12, x12, #0x18
   430d0:	mov	x13, x10
   430d4:	ldp	q0, q1, [x11, #-16]
   430d8:	subs	x13, x13, #0x4
   430dc:	add	x11, x11, #0x20
   430e0:	stp	q0, q1, [x12, #-16]
   430e4:	add	x12, x12, #0x20
   430e8:	b.ne	430d4 <__gmpn_toom_eval_dgr3_pm2@@Base+0x2ac>  // b.any
   430ec:	cmp	x8, x10
   430f0:	b.eq	43014 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1ec>  // b.none
   430f4:	b	42ff0 <__gmpn_toom_eval_dgr3_pm2@@Base+0x1c8>

00000000000430f8 <__gmpn_toom_eval_pm1@@Base>:
   430f8:	stp	x29, x30, [sp, #-80]!
   430fc:	stp	x26, x25, [sp, #16]
   43100:	stp	x24, x23, [sp, #32]
   43104:	stp	x22, x21, [sp, #48]
   43108:	mov	x25, x3
   4310c:	mov	w24, w2
   43110:	mov	x21, x1
   43114:	add	x2, x3, x4, lsl #4
   43118:	mov	x1, x3
   4311c:	mov	x3, x4
   43120:	stp	x20, x19, [sp, #64]
   43124:	mov	x29, sp
   43128:	mov	x19, x6
   4312c:	mov	x23, x5
   43130:	mov	x22, x4
   43134:	mov	x20, x0
   43138:	bl	ca70 <__gmpn_add_n@plt>
   4313c:	cmp	w24, #0x5
   43140:	str	x0, [x20, x22, lsl #3]
   43144:	b.cc	431a8 <__gmpn_toom_eval_pm1@@Base+0xb0>  // b.lo, b.ul, b.last
   43148:	mov	w26, #0x4                   	// #4
   4314c:	b	4315c <__gmpn_toom_eval_pm1@@Base+0x64>
   43150:	add	w26, w26, #0x2
   43154:	cmp	w26, w24
   43158:	b.cs	431a8 <__gmpn_toom_eval_pm1@@Base+0xb0>  // b.hs, b.nlast
   4315c:	cbz	x22, 43150 <__gmpn_toom_eval_pm1@@Base+0x58>
   43160:	mov	w8, w26
   43164:	mul	x8, x8, x22
   43168:	add	x2, x25, x8, lsl #3
   4316c:	mov	x0, x20
   43170:	mov	x1, x20
   43174:	mov	x3, x22
   43178:	bl	ca70 <__gmpn_add_n@plt>
   4317c:	cbz	x0, 43150 <__gmpn_toom_eval_pm1@@Base+0x58>
   43180:	mov	x8, x22
   43184:	cmp	x8, x22
   43188:	b.gt	43150 <__gmpn_toom_eval_pm1@@Base+0x58>
   4318c:	lsl	x9, x8, #3
   43190:	ldr	x10, [x20, x9]
   43194:	add	x8, x8, #0x1
   43198:	adds	x10, x10, #0x1
   4319c:	str	x10, [x20, x9]
   431a0:	b.cs	43184 <__gmpn_toom_eval_pm1@@Base+0x8c>  // b.hs, b.nlast
   431a4:	b	43150 <__gmpn_toom_eval_pm1@@Base+0x58>
   431a8:	lsl	x26, x22, #3
   431ac:	mov	w8, #0x18                  	// #24
   431b0:	add	x1, x25, x26
   431b4:	madd	x2, x22, x8, x25
   431b8:	mov	x0, x19
   431bc:	mov	x3, x22
   431c0:	bl	ca70 <__gmpn_add_n@plt>
   431c4:	cmp	w24, #0x6
   431c8:	str	x0, [x19, x26]
   431cc:	b.cc	43230 <__gmpn_toom_eval_pm1@@Base+0x138>  // b.lo, b.ul, b.last
   431d0:	mov	w26, #0x5                   	// #5
   431d4:	b	431e4 <__gmpn_toom_eval_pm1@@Base+0xec>
   431d8:	add	w26, w26, #0x2
   431dc:	cmp	w26, w24
   431e0:	b.cs	43230 <__gmpn_toom_eval_pm1@@Base+0x138>  // b.hs, b.nlast
   431e4:	cbz	x22, 431d8 <__gmpn_toom_eval_pm1@@Base+0xe0>
   431e8:	mov	w8, w26
   431ec:	mul	x8, x8, x22
   431f0:	add	x2, x25, x8, lsl #3
   431f4:	mov	x0, x19
   431f8:	mov	x1, x19
   431fc:	mov	x3, x22
   43200:	bl	ca70 <__gmpn_add_n@plt>
   43204:	cbz	x0, 431d8 <__gmpn_toom_eval_pm1@@Base+0xe0>
   43208:	mov	x8, x22
   4320c:	cmp	x8, x22
   43210:	b.gt	431d8 <__gmpn_toom_eval_pm1@@Base+0xe0>
   43214:	lsl	x9, x8, #3
   43218:	ldr	x10, [x19, x9]
   4321c:	add	x8, x8, #0x1
   43220:	adds	x10, x10, #0x1
   43224:	str	x10, [x19, x9]
   43228:	b.cs	4320c <__gmpn_toom_eval_pm1@@Base+0x114>  // b.hs, b.nlast
   4322c:	b	431d8 <__gmpn_toom_eval_pm1@@Base+0xe0>
   43230:	mov	w8, w24
   43234:	mul	x8, x8, x22
   43238:	add	x26, x22, #0x1
   4323c:	add	x2, x25, x8, lsl #3
   43240:	tbnz	w24, #0, 43280 <__gmpn_toom_eval_pm1@@Base+0x188>
   43244:	cbz	x23, 432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   43248:	mov	x0, x20
   4324c:	mov	x1, x20
   43250:	mov	x3, x23
   43254:	bl	ca70 <__gmpn_add_n@plt>
   43258:	cbz	x0, 432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   4325c:	cmp	x23, x22
   43260:	b.gt	432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   43264:	lsl	x8, x23, #3
   43268:	ldr	x9, [x20, x8]
   4326c:	add	x23, x23, #0x1
   43270:	adds	x9, x9, #0x1
   43274:	str	x9, [x20, x8]
   43278:	b.cs	4325c <__gmpn_toom_eval_pm1@@Base+0x164>  // b.hs, b.nlast
   4327c:	b	432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   43280:	cbz	x23, 432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   43284:	mov	x0, x19
   43288:	mov	x1, x19
   4328c:	mov	x3, x23
   43290:	bl	ca70 <__gmpn_add_n@plt>
   43294:	cbz	x0, 432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   43298:	cmp	x23, x22
   4329c:	b.gt	432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   432a0:	lsl	x8, x23, #3
   432a4:	ldr	x9, [x19, x8]
   432a8:	add	x23, x23, #0x1
   432ac:	adds	x9, x9, #0x1
   432b0:	str	x9, [x19, x8]
   432b4:	b.cs	43298 <__gmpn_toom_eval_pm1@@Base+0x1a0>  // b.hs, b.nlast
   432b8:	b	432d4 <__gmpn_toom_eval_pm1@@Base+0x1dc>
   432bc:	lsl	x8, x22, #3
   432c0:	ldr	x9, [x20, x8]
   432c4:	ldr	x8, [x19, x8]
   432c8:	sub	x22, x22, #0x1
   432cc:	cmp	x9, x8
   432d0:	b.ne	432e4 <__gmpn_toom_eval_pm1@@Base+0x1ec>  // b.any
   432d4:	add	x8, x22, #0x1
   432d8:	cmp	x8, #0x1
   432dc:	b.ge	432bc <__gmpn_toom_eval_pm1@@Base+0x1c4>  // b.tcont
   432e0:	b	432e8 <__gmpn_toom_eval_pm1@@Base+0x1f0>
   432e4:	b.ls	43304 <__gmpn_toom_eval_pm1@@Base+0x20c>  // b.plast
   432e8:	mov	x0, x21
   432ec:	mov	x1, x20
   432f0:	mov	x2, x19
   432f4:	mov	x3, x26
   432f8:	bl	c2d0 <__gmpn_sub_n@plt>
   432fc:	mov	w21, wzr
   43300:	b	4331c <__gmpn_toom_eval_pm1@@Base+0x224>
   43304:	mov	x0, x21
   43308:	mov	x1, x19
   4330c:	mov	x2, x20
   43310:	mov	x3, x26
   43314:	bl	c2d0 <__gmpn_sub_n@plt>
   43318:	mov	w21, #0xffffffff            	// #-1
   4331c:	mov	x0, x20
   43320:	mov	x1, x20
   43324:	mov	x2, x19
   43328:	mov	x3, x26
   4332c:	bl	ca70 <__gmpn_add_n@plt>
   43330:	mov	w0, w21
   43334:	ldp	x20, x19, [sp, #64]
   43338:	ldp	x22, x21, [sp, #48]
   4333c:	ldp	x24, x23, [sp, #32]
   43340:	ldp	x26, x25, [sp, #16]
   43344:	ldp	x29, x30, [sp], #80
   43348:	ret

000000000004334c <__gmpn_toom_eval_pm2@@Base>:
   4334c:	stp	x29, x30, [sp, #-96]!
   43350:	stp	x28, x27, [sp, #16]
   43354:	sub	w28, w2, #0x2
   43358:	mov	w8, w2
   4335c:	mul	x9, x28, x4
   43360:	mul	x8, x8, x4
   43364:	add	x27, x3, x9, lsl #3
   43368:	stp	x24, x23, [sp, #48]
   4336c:	stp	x22, x21, [sp, #64]
   43370:	mov	x23, x3
   43374:	mov	w24, w2
   43378:	mov	x21, x1
   4337c:	add	x2, x3, x8, lsl #3
   43380:	mov	x1, x27
   43384:	mov	x3, x5
   43388:	stp	x26, x25, [sp, #32]
   4338c:	stp	x20, x19, [sp, #80]
   43390:	mov	x29, sp
   43394:	mov	x19, x6
   43398:	mov	x25, x5
   4339c:	mov	x22, x4
   433a0:	mov	x20, x0
   433a4:	bl	cba0 <__gmpn_addlsh2_n@plt>
   433a8:	subs	x13, x22, x25
   433ac:	mov	x26, x0
   433b0:	b.eq	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   433b4:	lsl	x8, x25, #3
   433b8:	add	x9, x27, x8
   433bc:	ldr	x11, [x9]
   433c0:	add	x10, x20, x8
   433c4:	adds	x8, x11, x26
   433c8:	str	x8, [x10]
   433cc:	b.cc	434c8 <__gmpn_toom_eval_pm2@@Base+0x17c>  // b.lo, b.ul, b.last
   433d0:	mvn	x8, x25
   433d4:	mov	x11, xzr
   433d8:	mov	w26, #0x1                   	// #1
   433dc:	add	x12, x8, x22
   433e0:	mov	w8, #0x1                   	// #1
   433e4:	cmp	x8, x13
   433e8:	b.ge	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tcont
   433ec:	add	x14, x9, x11
   433f0:	ldr	x14, [x14, #8]
   433f4:	add	x15, x10, x11
   433f8:	add	x8, x8, #0x1
   433fc:	add	x11, x11, #0x8
   43400:	adds	x14, x14, #0x1
   43404:	sub	x12, x12, #0x1
   43408:	str	x14, [x15, #8]
   4340c:	b.cs	433e4 <__gmpn_toom_eval_pm2@@Base+0x98>  // b.hs, b.nlast
   43410:	cmp	x27, x20
   43414:	mov	x26, xzr
   43418:	b.eq	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   4341c:	cmp	x8, x13
   43420:	b.ge	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tcont
   43424:	sub	x13, x13, x8
   43428:	cmp	x13, #0x4
   4342c:	b.cc	434a0 <__gmpn_toom_eval_pm2@@Base+0x154>  // b.lo, b.ul, b.last
   43430:	add	x15, x28, #0x1
   43434:	add	x14, x10, x11
   43438:	mul	x15, x15, x22
   4343c:	add	x14, x14, #0x8
   43440:	add	x15, x23, x15, lsl #3
   43444:	cmp	x14, x15
   43448:	b.cs	43460 <__gmpn_toom_eval_pm2@@Base+0x114>  // b.hs, b.nlast
   4344c:	add	x15, x9, x11
   43450:	add	x14, x20, x22, lsl #3
   43454:	add	x15, x15, #0x8
   43458:	cmp	x15, x14
   4345c:	b.cc	434a0 <__gmpn_toom_eval_pm2@@Base+0x154>  // b.lo, b.ul, b.last
   43460:	add	x10, x10, x11
   43464:	add	x11, x9, x11
   43468:	and	x9, x13, #0xfffffffffffffffc
   4346c:	and	x12, x12, #0xfffffffffffffffc
   43470:	add	x10, x10, #0x18
   43474:	add	x11, x11, #0x18
   43478:	add	x8, x12, x8
   4347c:	mov	x12, x9
   43480:	ldp	q0, q1, [x11, #-16]
   43484:	subs	x12, x12, #0x4
   43488:	add	x11, x11, #0x20
   4348c:	stp	q0, q1, [x10, #-16]
   43490:	add	x10, x10, #0x20
   43494:	b.ne	43480 <__gmpn_toom_eval_pm2@@Base+0x134>  // b.any
   43498:	cmp	x13, x9
   4349c:	b.eq	43548 <__gmpn_toom_eval_pm2@@Base+0x1fc>  // b.none
   434a0:	add	x10, x8, x25
   434a4:	sub	x8, x10, x22
   434a8:	add	x9, x20, x10, lsl #3
   434ac:	madd	x10, x22, x28, x10
   434b0:	add	x10, x23, x10, lsl #3
   434b4:	ldr	x11, [x10], #8
   434b8:	adds	x8, x8, #0x1
   434bc:	str	x11, [x9], #8
   434c0:	b.cc	434b4 <__gmpn_toom_eval_pm2@@Base+0x168>  // b.lo, b.ul, b.last
   434c4:	b	43548 <__gmpn_toom_eval_pm2@@Base+0x1fc>
   434c8:	cmp	x27, x20
   434cc:	mov	x26, xzr
   434d0:	b.eq	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.none
   434d4:	cmp	x13, #0x2
   434d8:	b.lt	4354c <__gmpn_toom_eval_pm2@@Base+0x200>  // b.tstop
   434dc:	mvn	x8, x25
   434e0:	add	x8, x8, x22
   434e4:	cmp	x8, #0x4
   434e8:	b.cc	43520 <__gmpn_toom_eval_pm2@@Base+0x1d4>  // b.lo, b.ul, b.last
   434ec:	add	x9, x28, #0x1
   434f0:	add	x12, x20, x25, lsl #3
   434f4:	mul	x9, x9, x22
   434f8:	add	x10, x12, #0x8
   434fc:	add	x9, x23, x9, lsl #3
   43500:	cmp	x10, x9
   43504:	b.cs	436a0 <__gmpn_toom_eval_pm2@@Base+0x354>  // b.hs, b.nlast
   43508:	madd	x10, x28, x22, x25
   4350c:	add	x10, x23, x10, lsl #3
   43510:	add	x9, x20, x22, lsl #3
   43514:	add	x10, x10, #0x8
   43518:	cmp	x10, x9
   4351c:	b.cs	436a0 <__gmpn_toom_eval_pm2@@Base+0x354>  // b.hs, b.nlast
   43520:	mov	w9, #0x1                   	// #1
   43524:	add	x10, x9, x25
   43528:	sub	x8, x10, x22
   4352c:	add	x9, x20, x10, lsl #3
   43530:	madd	x10, x22, x28, x10
   43534:	add	x10, x23, x10, lsl #3
   43538:	ldr	x11, [x10], #8
   4353c:	adds	x8, x8, #0x1
   43540:	str	x11, [x9], #8
   43544:	b.cc	43538 <__gmpn_toom_eval_pm2@@Base+0x1ec>  // b.lo, b.ul, b.last
   43548:	mov	x26, xzr
   4354c:	cmp	w24, #0x4
   43550:	b.mi	4357c <__gmpn_toom_eval_pm2@@Base+0x230>  // b.first
   43554:	sub	w28, w28, #0x2
   43558:	mul	x8, x28, x22
   4355c:	add	x1, x23, x8, lsl #3
   43560:	mov	x0, x20
   43564:	mov	x2, x20
   43568:	mov	x3, x22
   4356c:	bl	cba0 <__gmpn_addlsh2_n@plt>
   43570:	cmp	w28, #0x1
   43574:	add	x26, x0, x26, lsl #2
   43578:	b.gt	43554 <__gmpn_toom_eval_pm2@@Base+0x208>
   4357c:	sub	w25, w24, #0x1
   43580:	sub	w8, w24, #0x3
   43584:	mul	x8, x8, x22
   43588:	mul	x9, x25, x22
   4358c:	add	x1, x23, x8, lsl #3
   43590:	add	x2, x23, x9, lsl #3
   43594:	mov	x0, x19
   43598:	mov	x3, x22
   4359c:	str	x26, [x20, x22, lsl #3]
   435a0:	bl	cba0 <__gmpn_addlsh2_n@plt>
   435a4:	subs	w26, w24, #0x5
   435a8:	mov	x24, x0
   435ac:	b.mi	435dc <__gmpn_toom_eval_pm2@@Base+0x290>  // b.first
   435b0:	mov	w8, w26
   435b4:	mul	x8, x8, x22
   435b8:	add	x1, x23, x8, lsl #3
   435bc:	mov	x0, x19
   435c0:	mov	x2, x19
   435c4:	mov	x3, x22
   435c8:	bl	cba0 <__gmpn_addlsh2_n@plt>
   435cc:	cmp	w26, #0x1
   435d0:	sub	w26, w26, #0x2
   435d4:	add	x24, x0, x24, lsl #2
   435d8:	b.gt	435b0 <__gmpn_toom_eval_pm2@@Base+0x264>
   435dc:	add	x23, x22, #0x1
   435e0:	mov	w3, #0x1                   	// #1
   435e4:	str	x24, [x19, x22, lsl #3]
   435e8:	tbnz	w25, #0, 435f8 <__gmpn_toom_eval_pm2@@Base+0x2ac>
   435ec:	mov	x0, x20
   435f0:	mov	x1, x20
   435f4:	b	43600 <__gmpn_toom_eval_pm2@@Base+0x2b4>
   435f8:	mov	x0, x19
   435fc:	mov	x1, x19
   43600:	mov	x2, x23
   43604:	bl	c180 <__gmpn_lshift@plt>
   43608:	and	w24, w25, #0x1
   4360c:	add	x8, x22, #0x1
   43610:	cmp	x8, #0x1
   43614:	b.lt	43634 <__gmpn_toom_eval_pm2@@Base+0x2e8>  // b.tstop
   43618:	lsl	x8, x22, #3
   4361c:	ldr	x9, [x20, x8]
   43620:	ldr	x8, [x19, x8]
   43624:	sub	x22, x22, #0x1
   43628:	cmp	x9, x8
   4362c:	b.eq	4360c <__gmpn_toom_eval_pm2@@Base+0x2c0>  // b.none
   43630:	b.ls	43650 <__gmpn_toom_eval_pm2@@Base+0x304>  // b.plast
   43634:	mov	x0, x21
   43638:	mov	x1, x20
   4363c:	mov	x2, x19
   43640:	mov	x3, x23
   43644:	bl	c2d0 <__gmpn_sub_n@plt>
   43648:	mov	w21, wzr
   4364c:	b	43668 <__gmpn_toom_eval_pm2@@Base+0x31c>
   43650:	mov	x0, x21
   43654:	mov	x1, x19
   43658:	mov	x2, x20
   4365c:	mov	x3, x23
   43660:	bl	c2d0 <__gmpn_sub_n@plt>
   43664:	mov	w21, #0xffffffff            	// #-1
   43668:	mov	x0, x20
   4366c:	mov	x1, x20
   43670:	mov	x2, x19
   43674:	mov	x3, x23
   43678:	bl	ca70 <__gmpn_add_n@plt>
   4367c:	sub	w8, w24, #0x1
   43680:	eor	w0, w21, w8
   43684:	ldp	x20, x19, [sp, #80]
   43688:	ldp	x22, x21, [sp, #64]
   4368c:	ldp	x24, x23, [sp, #48]
   43690:	ldp	x26, x25, [sp, #32]
   43694:	ldp	x28, x27, [sp, #16]
   43698:	ldp	x29, x30, [sp], #96
   4369c:	ret
   436a0:	madd	x11, x22, x28, x25
   436a4:	and	x10, x8, #0xfffffffffffffffc
   436a8:	add	x11, x23, x11, lsl #3
   436ac:	orr	x9, x10, #0x1
   436b0:	add	x11, x11, #0x18
   436b4:	add	x12, x12, #0x18
   436b8:	mov	x13, x10
   436bc:	ldp	q0, q1, [x11, #-16]
   436c0:	subs	x13, x13, #0x4
   436c4:	add	x11, x11, #0x20
   436c8:	stp	q0, q1, [x12, #-16]
   436cc:	add	x12, x12, #0x20
   436d0:	b.ne	436bc <__gmpn_toom_eval_pm2@@Base+0x370>  // b.any
   436d4:	cmp	x8, x10
   436d8:	b.eq	43548 <__gmpn_toom_eval_pm2@@Base+0x1fc>  // b.none
   436dc:	b	43524 <__gmpn_toom_eval_pm2@@Base+0x1d8>

00000000000436e0 <__gmpn_toom_eval_pm2exp@@Base>:
   436e0:	sub	sp, sp, #0x70
   436e4:	stp	x28, x27, [sp, #32]
   436e8:	lsl	w27, w6, #1
   436ec:	stp	x29, x30, [sp, #16]
   436f0:	stp	x26, x25, [sp, #48]
   436f4:	stp	x24, x23, [sp, #64]
   436f8:	stp	x22, x21, [sp, #80]
   436fc:	stp	x20, x19, [sp, #96]
   43700:	add	x29, sp, #0x10
   43704:	mov	x26, x3
   43708:	mov	w24, w2
   4370c:	mov	x21, x1
   43710:	mov	x20, x0
   43714:	add	x1, x3, x4, lsl #4
   43718:	mov	x0, x7
   4371c:	mov	x2, x4
   43720:	mov	w3, w27
   43724:	mov	x19, x7
   43728:	str	x5, [sp]
   4372c:	mov	x22, x4
   43730:	stur	w6, [x29, #-4]
   43734:	bl	c180 <__gmpn_lshift@plt>
   43738:	lsl	x23, x22, #3
   4373c:	str	x0, [x20, x23]
   43740:	mov	x0, x20
   43744:	mov	x1, x26
   43748:	mov	x2, x19
   4374c:	mov	x3, x22
   43750:	bl	ca70 <__gmpn_add_n@plt>
   43754:	ldr	x8, [x20, x23]
   43758:	cmp	w24, #0x5
   4375c:	add	x8, x8, x0
   43760:	str	x8, [x20, x23]
   43764:	b.cc	437cc <__gmpn_toom_eval_pm2exp@@Base+0xec>  // b.lo, b.ul, b.last
   43768:	ldur	w8, [x29, #-4]
   4376c:	mov	w25, #0x4                   	// #4
   43770:	lsl	w28, w8, #2
   43774:	mov	w8, w25
   43778:	mul	x8, x8, x22
   4377c:	add	x1, x26, x8, lsl #3
   43780:	mov	x0, x19
   43784:	mov	x2, x22
   43788:	mov	w3, w28
   4378c:	bl	c180 <__gmpn_lshift@plt>
   43790:	ldr	x8, [x20, x23]
   43794:	mov	x1, x20
   43798:	mov	x2, x19
   4379c:	mov	x3, x22
   437a0:	add	x8, x8, x0
   437a4:	mov	x0, x20
   437a8:	str	x8, [x20, x23]
   437ac:	bl	ca70 <__gmpn_add_n@plt>
   437b0:	ldr	x8, [x20, x23]
   437b4:	add	w25, w25, #0x2
   437b8:	cmp	w25, w24
   437bc:	add	w28, w28, w27
   437c0:	add	x8, x8, x0
   437c4:	str	x8, [x20, x23]
   437c8:	b.cc	43774 <__gmpn_toom_eval_pm2exp@@Base+0x94>  // b.lo, b.ul, b.last
   437cc:	ldur	w3, [x29, #-4]
   437d0:	add	x1, x26, x23
   437d4:	mov	x0, x19
   437d8:	mov	x2, x22
   437dc:	bl	c180 <__gmpn_lshift@plt>
   437e0:	cmp	w24, #0x4
   437e4:	str	x0, [x19, x23]
   437e8:	b.cc	43850 <__gmpn_toom_eval_pm2exp@@Base+0x170>  // b.lo, b.ul, b.last
   437ec:	ldur	w8, [x29, #-4]
   437f0:	mov	w25, #0x3                   	// #3
   437f4:	add	w28, w27, w8
   437f8:	mov	w8, w25
   437fc:	mul	x8, x8, x22
   43800:	add	x1, x26, x8, lsl #3
   43804:	mov	x0, x21
   43808:	mov	x2, x22
   4380c:	mov	w3, w28
   43810:	bl	c180 <__gmpn_lshift@plt>
   43814:	ldr	x8, [x19, x23]
   43818:	mov	x1, x19
   4381c:	mov	x2, x21
   43820:	mov	x3, x22
   43824:	add	x8, x8, x0
   43828:	mov	x0, x19
   4382c:	str	x8, [x19, x23]
   43830:	bl	ca70 <__gmpn_add_n@plt>
   43834:	ldr	x8, [x19, x23]
   43838:	add	w25, w25, #0x2
   4383c:	cmp	w25, w24
   43840:	add	w28, w28, w27
   43844:	add	x8, x8, x0
   43848:	str	x8, [x19, x23]
   4384c:	b.cc	437f8 <__gmpn_toom_eval_pm2exp@@Base+0x118>  // b.lo, b.ul, b.last
   43850:	mov	w8, w24
   43854:	mul	x8, x8, x22
   43858:	add	x1, x26, x8, lsl #3
   4385c:	ldur	w8, [x29, #-4]
   43860:	ldr	x23, [sp]
   43864:	mov	x0, x21
   43868:	mul	w3, w8, w24
   4386c:	mov	x2, x23
   43870:	bl	c180 <__gmpn_lshift@plt>
   43874:	str	x0, [x21, x23, lsl #3]
   43878:	add	x25, x22, #0x1
   4387c:	add	x23, x23, #0x1
   43880:	tbnz	w24, #0, 438c4 <__gmpn_toom_eval_pm2exp@@Base+0x1e4>
   43884:	cbz	x23, 4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   43888:	mov	x0, x20
   4388c:	mov	x1, x20
   43890:	mov	x2, x21
   43894:	mov	x3, x23
   43898:	bl	ca70 <__gmpn_add_n@plt>
   4389c:	cbz	x0, 4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438a0:	cmp	x23, x22
   438a4:	b.gt	4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438a8:	lsl	x8, x23, #3
   438ac:	ldr	x9, [x20, x8]
   438b0:	add	x23, x23, #0x1
   438b4:	adds	x9, x9, #0x1
   438b8:	str	x9, [x20, x8]
   438bc:	b.cs	438a0 <__gmpn_toom_eval_pm2exp@@Base+0x1c0>  // b.hs, b.nlast
   438c0:	b	4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438c4:	cbz	x23, 4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438c8:	mov	x0, x19
   438cc:	mov	x1, x19
   438d0:	mov	x2, x21
   438d4:	mov	x3, x23
   438d8:	bl	ca70 <__gmpn_add_n@plt>
   438dc:	cbz	x0, 4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438e0:	cmp	x23, x22
   438e4:	b.gt	4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   438e8:	lsl	x8, x23, #3
   438ec:	ldr	x9, [x19, x8]
   438f0:	add	x23, x23, #0x1
   438f4:	adds	x9, x9, #0x1
   438f8:	str	x9, [x19, x8]
   438fc:	b.cs	438e0 <__gmpn_toom_eval_pm2exp@@Base+0x200>  // b.hs, b.nlast
   43900:	b	4391c <__gmpn_toom_eval_pm2exp@@Base+0x23c>
   43904:	lsl	x8, x22, #3
   43908:	ldr	x9, [x20, x8]
   4390c:	ldr	x8, [x19, x8]
   43910:	sub	x22, x22, #0x1
   43914:	cmp	x9, x8
   43918:	b.ne	4392c <__gmpn_toom_eval_pm2exp@@Base+0x24c>  // b.any
   4391c:	add	x8, x22, #0x1
   43920:	cmp	x8, #0x1
   43924:	b.ge	43904 <__gmpn_toom_eval_pm2exp@@Base+0x224>  // b.tcont
   43928:	b	43930 <__gmpn_toom_eval_pm2exp@@Base+0x250>
   4392c:	b.ls	4394c <__gmpn_toom_eval_pm2exp@@Base+0x26c>  // b.plast
   43930:	mov	x0, x21
   43934:	mov	x1, x20
   43938:	mov	x2, x19
   4393c:	mov	x3, x25
   43940:	bl	c2d0 <__gmpn_sub_n@plt>
   43944:	mov	w21, wzr
   43948:	b	43964 <__gmpn_toom_eval_pm2exp@@Base+0x284>
   4394c:	mov	x0, x21
   43950:	mov	x1, x19
   43954:	mov	x2, x20
   43958:	mov	x3, x25
   4395c:	bl	c2d0 <__gmpn_sub_n@plt>
   43960:	mov	w21, #0xffffffff            	// #-1
   43964:	mov	x0, x20
   43968:	mov	x1, x20
   4396c:	mov	x2, x19
   43970:	mov	x3, x25
   43974:	bl	ca70 <__gmpn_add_n@plt>
   43978:	mov	w0, w21
   4397c:	ldp	x20, x19, [sp, #96]
   43980:	ldp	x22, x21, [sp, #80]
   43984:	ldp	x24, x23, [sp, #64]
   43988:	ldp	x26, x25, [sp, #48]
   4398c:	ldp	x28, x27, [sp, #32]
   43990:	ldp	x29, x30, [sp, #16]
   43994:	add	sp, sp, #0x70
   43998:	ret

000000000004399c <__gmpn_toom_eval_pm2rexp@@Base>:
   4399c:	sub	sp, sp, #0x80
   439a0:	stp	x24, x23, [sp, #80]
   439a4:	mov	x23, x3
   439a8:	stp	x22, x21, [sp, #96]
   439ac:	mov	w24, w2
   439b0:	mov	x21, x1
   439b4:	mul	w3, w6, w2
   439b8:	mov	x1, x23
   439bc:	mov	x2, x4
   439c0:	stp	x29, x30, [sp, #32]
   439c4:	stp	x28, x27, [sp, #48]
   439c8:	stp	x26, x25, [sp, #64]
   439cc:	stp	x20, x19, [sp, #112]
   439d0:	add	x29, sp, #0x20
   439d4:	mov	x19, x7
   439d8:	mov	w28, w6
   439dc:	mov	x26, x5
   439e0:	mov	x22, x4
   439e4:	mov	x20, x0
   439e8:	bl	c180 <__gmpn_lshift@plt>
   439ec:	lsl	x27, x22, #3
   439f0:	sub	w25, w24, #0x1
   439f4:	str	x0, [x20, x27]
   439f8:	add	x1, x23, x27
   439fc:	mul	w3, w25, w28
   43a00:	mov	x0, x19
   43a04:	mov	x2, x22
   43a08:	stur	w28, [x29, #-12]
   43a0c:	bl	c180 <__gmpn_lshift@plt>
   43a10:	mov	w8, w24
   43a14:	mul	x8, x8, x22
   43a18:	add	x2, x23, x8, lsl #3
   43a1c:	str	x0, [x19, x27]
   43a20:	tbnz	w24, #0, 43a64 <__gmpn_toom_eval_pm2rexp@@Base+0xc8>
   43a24:	mov	x28, x23
   43a28:	cbz	x26, 43ae4 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   43a2c:	mov	x0, x20
   43a30:	mov	x1, x20
   43a34:	mov	x3, x26
   43a38:	bl	ca70 <__gmpn_add_n@plt>
   43a3c:	cbz	x0, 43ae4 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   43a40:	cmp	x26, x22
   43a44:	b.gt	43ae4 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   43a48:	lsl	x8, x26, #3
   43a4c:	ldr	x9, [x20, x8]
   43a50:	add	x26, x26, #0x1
   43a54:	adds	x9, x9, #0x1
   43a58:	str	x9, [x20, x8]
   43a5c:	b.cs	43a40 <__gmpn_toom_eval_pm2rexp@@Base+0xa4>  // b.hs, b.nlast
   43a60:	b	43ae4 <__gmpn_toom_eval_pm2rexp@@Base+0x148>
   43a64:	cbz	x26, 43a9c <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   43a68:	mov	x0, x19
   43a6c:	mov	x1, x19
   43a70:	mov	x3, x26
   43a74:	bl	ca70 <__gmpn_add_n@plt>
   43a78:	cbz	x0, 43a9c <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   43a7c:	cmp	x26, x22
   43a80:	b.gt	43a9c <__gmpn_toom_eval_pm2rexp@@Base+0x100>
   43a84:	lsl	x8, x26, #3
   43a88:	ldr	x9, [x19, x8]
   43a8c:	add	x26, x26, #0x1
   43a90:	adds	x9, x9, #0x1
   43a94:	str	x9, [x19, x8]
   43a98:	b.cs	43a7c <__gmpn_toom_eval_pm2rexp@@Base+0xe0>  // b.hs, b.nlast
   43a9c:	ldur	w3, [x29, #-12]
   43aa0:	mov	w8, w25
   43aa4:	mul	x8, x8, x22
   43aa8:	add	x1, x23, x8, lsl #3
   43aac:	mov	x0, x21
   43ab0:	mov	x2, x22
   43ab4:	mov	x28, x23
   43ab8:	bl	c180 <__gmpn_lshift@plt>
   43abc:	mov	x26, x0
   43ac0:	mov	x0, x20
   43ac4:	mov	x1, x20
   43ac8:	mov	x2, x21
   43acc:	mov	x3, x22
   43ad0:	bl	ca70 <__gmpn_add_n@plt>
   43ad4:	ldr	x8, [x20, x27]
   43ad8:	add	x9, x0, x26
   43adc:	add	x8, x9, x8
   43ae0:	str	x8, [x20, x27]
   43ae4:	cmp	w25, #0x3
   43ae8:	add	x8, x22, #0x1
   43aec:	str	x8, [sp, #8]
   43af0:	b.cc	43bc8 <__gmpn_toom_eval_pm2rexp@@Base+0x22c>  // b.lo, b.ul, b.last
   43af4:	ldur	w10, [x29, #-12]
   43af8:	sub	w8, w24, #0x2
   43afc:	mov	w23, w25
   43b00:	mov	w26, wzr
   43b04:	lsl	w9, w10, #1
   43b08:	stur	w9, [x29, #-4]
   43b0c:	sub	w9, w24, #0x3
   43b10:	mul	w8, w10, w8
   43b14:	stur	w8, [x29, #-8]
   43b18:	mul	w8, w10, w9
   43b1c:	mov	w25, #0x2                   	// #2
   43b20:	stur	w8, [x29, #-12]
   43b24:	mov	w8, w25
   43b28:	mul	x8, x8, x22
   43b2c:	add	x1, x28, x8, lsl #3
   43b30:	ldur	w8, [x29, #-8]
   43b34:	mov	x0, x21
   43b38:	mov	x2, x22
   43b3c:	add	w3, w8, w26
   43b40:	bl	c180 <__gmpn_lshift@plt>
   43b44:	mov	x24, x0
   43b48:	mov	x0, x20
   43b4c:	mov	x1, x20
   43b50:	mov	x2, x21
   43b54:	mov	x3, x22
   43b58:	bl	ca70 <__gmpn_add_n@plt>
   43b5c:	ldr	x9, [x20, x27]
   43b60:	add	x8, x0, x24
   43b64:	add	w10, w25, #0x1
   43b68:	mul	x10, x10, x22
   43b6c:	add	x8, x8, x9
   43b70:	str	x8, [x20, x27]
   43b74:	ldur	w8, [x29, #-12]
   43b78:	add	x1, x28, x10, lsl #3
   43b7c:	mov	x0, x21
   43b80:	mov	x2, x22
   43b84:	add	w3, w8, w26
   43b88:	bl	c180 <__gmpn_lshift@plt>
   43b8c:	mov	x24, x0
   43b90:	mov	x0, x19
   43b94:	mov	x1, x19
   43b98:	mov	x2, x21
   43b9c:	mov	x3, x22
   43ba0:	bl	ca70 <__gmpn_add_n@plt>
   43ba4:	ldr	x8, [x19, x27]
   43ba8:	add	x9, x0, x24
   43bac:	add	w25, w25, #0x2
   43bb0:	cmp	w25, w23
   43bb4:	add	x8, x9, x8
   43bb8:	str	x8, [x19, x27]
   43bbc:	ldur	w8, [x29, #-4]
   43bc0:	sub	w26, w26, w8
   43bc4:	b.cc	43b24 <__gmpn_toom_eval_pm2rexp@@Base+0x188>  // b.lo, b.ul, b.last
   43bc8:	add	x8, x22, #0x1
   43bcc:	cmp	x8, #0x1
   43bd0:	b.lt	43bf0 <__gmpn_toom_eval_pm2rexp@@Base+0x254>  // b.tstop
   43bd4:	lsl	x8, x22, #3
   43bd8:	ldr	x9, [x20, x8]
   43bdc:	ldr	x8, [x19, x8]
   43be0:	sub	x22, x22, #0x1
   43be4:	cmp	x9, x8
   43be8:	b.eq	43bc8 <__gmpn_toom_eval_pm2rexp@@Base+0x22c>  // b.none
   43bec:	b.ls	43c10 <__gmpn_toom_eval_pm2rexp@@Base+0x274>  // b.plast
   43bf0:	ldr	x22, [sp, #8]
   43bf4:	mov	x0, x21
   43bf8:	mov	x1, x20
   43bfc:	mov	x2, x19
   43c00:	mov	x3, x22
   43c04:	bl	c2d0 <__gmpn_sub_n@plt>
   43c08:	mov	w21, wzr
   43c0c:	b	43c2c <__gmpn_toom_eval_pm2rexp@@Base+0x290>
   43c10:	ldr	x22, [sp, #8]
   43c14:	mov	x0, x21
   43c18:	mov	x1, x19
   43c1c:	mov	x2, x20
   43c20:	mov	x3, x22
   43c24:	bl	c2d0 <__gmpn_sub_n@plt>
   43c28:	mov	w21, #0xffffffff            	// #-1
   43c2c:	mov	x0, x20
   43c30:	mov	x1, x20
   43c34:	mov	x2, x19
   43c38:	mov	x3, x22
   43c3c:	bl	ca70 <__gmpn_add_n@plt>
   43c40:	mov	w0, w21
   43c44:	ldp	x20, x19, [sp, #112]
   43c48:	ldp	x22, x21, [sp, #96]
   43c4c:	ldp	x24, x23, [sp, #80]
   43c50:	ldp	x26, x25, [sp, #64]
   43c54:	ldp	x28, x27, [sp, #48]
   43c58:	ldp	x29, x30, [sp, #32]
   43c5c:	add	sp, sp, #0x80
   43c60:	ret

0000000000043c64 <__gmpn_toom_interpolate_5pts@@Base>:
   43c64:	sub	sp, sp, #0x80
   43c68:	stp	x20, x19, [sp, #112]
   43c6c:	lsl	x20, x3, #3
   43c70:	stp	x24, x23, [sp, #80]
   43c74:	add	x23, x0, x20
   43c78:	stp	x28, x27, [sp, #48]
   43c7c:	stp	x26, x25, [sp, #64]
   43c80:	mov	w27, #0x1                   	// #1
   43c84:	add	x25, x23, x20
   43c88:	stp	x29, x30, [sp, #32]
   43c8c:	stp	x22, x21, [sp, #96]
   43c90:	add	x29, sp, #0x20
   43c94:	bfi	x27, x3, #1, #63
   43c98:	add	x22, x25, x20
   43c9c:	mov	x24, x4
   43ca0:	mov	x19, x3
   43ca4:	mov	x28, x2
   43ca8:	mov	x21, x1
   43cac:	lsl	x8, x3, #1
   43cb0:	stur	x0, [x29, #-8]
   43cb4:	add	x26, x22, x20
   43cb8:	mov	x0, x1
   43cbc:	mov	x3, x27
   43cc0:	stp	x8, x6, [sp, #8]
   43cc4:	cbz	w5, 43cfc <__gmpn_toom_interpolate_5pts@@Base+0x98>
   43cc8:	bl	ca70 <__gmpn_add_n@plt>
   43ccc:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   43cd0:	mov	x0, x21
   43cd4:	mov	x1, x21
   43cd8:	mov	x2, x27
   43cdc:	mov	x4, xzr
   43ce0:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   43ce4:	mov	x0, x28
   43ce8:	mov	x1, x25
   43cec:	mov	x2, x28
   43cf0:	mov	x3, x27
   43cf4:	bl	c950 <__gmpn_rsh1add_n@plt>
   43cf8:	b	43d2c <__gmpn_toom_interpolate_5pts@@Base+0xc8>
   43cfc:	bl	c2d0 <__gmpn_sub_n@plt>
   43d00:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   43d04:	mov	x0, x21
   43d08:	mov	x1, x21
   43d0c:	mov	x2, x27
   43d10:	mov	x4, xzr
   43d14:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   43d18:	mov	x0, x28
   43d1c:	mov	x1, x25
   43d20:	mov	x2, x28
   43d24:	mov	x3, x27
   43d28:	bl	c840 <__gmpn_rsh1sub_n@plt>
   43d2c:	ldur	x2, [x29, #-8]
   43d30:	ldr	x3, [sp, #8]
   43d34:	mov	x0, x25
   43d38:	mov	x1, x25
   43d3c:	bl	c2d0 <__gmpn_sub_n@plt>
   43d40:	ldr	x8, [x26]
   43d44:	mov	x1, x21
   43d48:	mov	x2, x25
   43d4c:	mov	x3, x27
   43d50:	sub	x8, x8, x0
   43d54:	mov	x0, x21
   43d58:	str	x8, [x26]
   43d5c:	bl	c840 <__gmpn_rsh1sub_n@plt>
   43d60:	mov	x0, x25
   43d64:	mov	x1, x25
   43d68:	mov	x2, x28
   43d6c:	mov	x3, x27
   43d70:	bl	c2d0 <__gmpn_sub_n@plt>
   43d74:	mov	x0, x23
   43d78:	mov	x1, x23
   43d7c:	mov	x2, x28
   43d80:	mov	x3, x27
   43d84:	bl	ca70 <__gmpn_add_n@plt>
   43d88:	ldr	x8, [x22, #8]
   43d8c:	adds	x8, x8, x0
   43d90:	str	x8, [x22, #8]
   43d94:	b.cc	43db8 <__gmpn_toom_interpolate_5pts@@Base+0x154>  // b.lo, b.ul, b.last
   43d98:	ldur	x9, [x29, #-8]
   43d9c:	mov	w8, #0x18                  	// #24
   43da0:	madd	x8, x19, x8, x9
   43da4:	add	x8, x8, #0x10
   43da8:	ldr	x9, [x8]
   43dac:	adds	x9, x9, #0x1
   43db0:	str	x9, [x8], #8
   43db4:	b.cs	43da8 <__gmpn_toom_interpolate_5pts@@Base+0x144>  // b.hs, b.nlast
   43db8:	ldr	x8, [x26]
   43dbc:	mov	x0, x21
   43dc0:	mov	x1, x21
   43dc4:	mov	x2, x26
   43dc8:	str	x8, [sp, #8]
   43dcc:	ldr	x8, [sp, #16]
   43dd0:	mov	x3, x24
   43dd4:	str	x8, [x26]
   43dd8:	bl	c440 <__gmpn_sublsh1_n@plt>
   43ddc:	lsl	x28, x24, #3
   43de0:	ldr	x8, [x21, x28]
   43de4:	subs	x8, x8, x0
   43de8:	str	x8, [x21, x28]
   43dec:	b.cs	43e08 <__gmpn_toom_interpolate_5pts@@Base+0x1a4>  // b.hs, b.nlast
   43df0:	add	x8, x21, x24, lsl #3
   43df4:	add	x8, x8, #0x8
   43df8:	ldr	x9, [x8]
   43dfc:	sub	x10, x9, #0x1
   43e00:	str	x10, [x8], #8
   43e04:	cbz	x9, 43df8 <__gmpn_toom_interpolate_5pts@@Base+0x194>
   43e08:	add	x3, x19, #0x1
   43e0c:	cmp	x3, x24
   43e10:	add	x2, x21, x19, lsl #3
   43e14:	mov	x0, x26
   43e18:	mov	x1, x26
   43e1c:	b.ge	43f4c <__gmpn_toom_interpolate_5pts@@Base+0x2e8>  // b.tcont
   43e20:	bl	ca70 <__gmpn_add_n@plt>
   43e24:	lsl	x8, x27, #3
   43e28:	ldr	x9, [x22, x8]
   43e2c:	adds	x9, x9, x0
   43e30:	str	x9, [x22, x8]
   43e34:	b.cc	43e58 <__gmpn_toom_interpolate_5pts@@Base+0x1f4>  // b.lo, b.ul, b.last
   43e38:	ldur	x9, [x29, #-8]
   43e3c:	mov	w8, #0x28                  	// #40
   43e40:	madd	x8, x19, x8, x9
   43e44:	add	x8, x8, #0x10
   43e48:	ldr	x9, [x8]
   43e4c:	adds	x9, x9, #0x1
   43e50:	str	x9, [x8], #8
   43e54:	b.cs	43e48 <__gmpn_toom_interpolate_5pts@@Base+0x1e4>  // b.hs, b.nlast
   43e58:	mov	x0, x25
   43e5c:	mov	x1, x25
   43e60:	mov	x2, x26
   43e64:	mov	x3, x24
   43e68:	bl	c2d0 <__gmpn_sub_n@plt>
   43e6c:	ldr	x8, [sp, #8]
   43e70:	ldr	x27, [x26]
   43e74:	str	x8, [x26]
   43e78:	ldr	x8, [x25, x28]
   43e7c:	subs	x8, x8, x0
   43e80:	str	x8, [x25, x28]
   43e84:	b.cs	43ea8 <__gmpn_toom_interpolate_5pts@@Base+0x244>  // b.hs, b.nlast
   43e88:	ldur	x9, [x29, #-8]
   43e8c:	add	x8, x24, x19, lsl #1
   43e90:	add	x8, x9, x8, lsl #3
   43e94:	add	x8, x8, #0x8
   43e98:	ldr	x9, [x8]
   43e9c:	sub	x10, x9, #0x1
   43ea0:	str	x10, [x8], #8
   43ea4:	cbz	x9, 43e98 <__gmpn_toom_interpolate_5pts@@Base+0x234>
   43ea8:	mov	x0, x23
   43eac:	mov	x1, x23
   43eb0:	mov	x2, x21
   43eb4:	mov	x3, x19
   43eb8:	bl	c2d0 <__gmpn_sub_n@plt>
   43ebc:	ldr	x8, [x23, x20]
   43ec0:	subs	x8, x8, x0
   43ec4:	str	x8, [x23, x20]
   43ec8:	b.cs	43ee8 <__gmpn_toom_interpolate_5pts@@Base+0x284>  // b.hs, b.nlast
   43ecc:	ldur	x8, [x29, #-8]
   43ed0:	add	x8, x8, x19, lsl #4
   43ed4:	add	x8, x8, #0x8
   43ed8:	ldr	x9, [x8]
   43edc:	sub	x10, x9, #0x1
   43ee0:	str	x10, [x8], #8
   43ee4:	cbz	x9, 43ed8 <__gmpn_toom_interpolate_5pts@@Base+0x274>
   43ee8:	mov	x0, x22
   43eec:	mov	x1, x22
   43ef0:	mov	x2, x21
   43ef4:	mov	x3, x19
   43ef8:	bl	ca70 <__gmpn_add_n@plt>
   43efc:	ldr	x8, [x22, x20]
   43f00:	add	x8, x8, x0
   43f04:	adds	x8, x8, x27
   43f08:	str	x8, [x22, x20]
   43f0c:	b.cc	43f2c <__gmpn_toom_interpolate_5pts@@Base+0x2c8>  // b.lo, b.ul, b.last
   43f10:	ldur	x8, [x29, #-8]
   43f14:	add	x8, x8, x19, lsl #5
   43f18:	add	x8, x8, #0x8
   43f1c:	ldr	x9, [x8]
   43f20:	adds	x9, x9, #0x1
   43f24:	str	x9, [x8], #8
   43f28:	b.cs	43f1c <__gmpn_toom_interpolate_5pts@@Base+0x2b8>  // b.hs, b.nlast
   43f2c:	ldp	x20, x19, [sp, #112]
   43f30:	ldp	x22, x21, [sp, #96]
   43f34:	ldp	x24, x23, [sp, #80]
   43f38:	ldp	x26, x25, [sp, #64]
   43f3c:	ldp	x28, x27, [sp, #48]
   43f40:	ldp	x29, x30, [sp, #32]
   43f44:	add	sp, sp, #0x80
   43f48:	ret
   43f4c:	mov	x3, x24
   43f50:	bl	ca70 <__gmpn_add_n@plt>
   43f54:	b	43e58 <__gmpn_toom_interpolate_5pts@@Base+0x1f4>

0000000000043f58 <__gmpn_toom_interpolate_6pts@@Base>:
   43f58:	sub	sp, sp, #0x90
   43f5c:	stp	x26, x25, [sp, #80]
   43f60:	mov	w25, #0x1                   	// #1
   43f64:	stp	x29, x30, [sp, #48]
   43f68:	stp	x28, x27, [sp, #64]
   43f6c:	stp	x24, x23, [sp, #96]
   43f70:	stp	x22, x21, [sp, #112]
   43f74:	stp	x20, x19, [sp, #128]
   43f78:	add	x29, sp, #0x30
   43f7c:	mov	x23, x5
   43f80:	mov	x27, x4
   43f84:	mov	x22, x3
   43f88:	mov	w24, w2
   43f8c:	mov	x20, x1
   43f90:	mov	x19, x0
   43f94:	lsl	x28, x1, #1
   43f98:	bfi	x25, x1, #1, #63
   43f9c:	mov	x0, x4
   43fa0:	mov	x1, x5
   43fa4:	stur	x6, [x29, #-8]
   43fa8:	tbnz	w2, #1, 43fbc <__gmpn_toom_interpolate_6pts@@Base+0x64>
   43fac:	mov	x2, x27
   43fb0:	mov	x3, x25
   43fb4:	bl	c2d0 <__gmpn_sub_n@plt>
   43fb8:	b	43fc8 <__gmpn_toom_interpolate_6pts@@Base+0x70>
   43fbc:	mov	x2, x27
   43fc0:	mov	x3, x25
   43fc4:	bl	ca70 <__gmpn_add_n@plt>
   43fc8:	mov	w3, #0x2                   	// #2
   43fcc:	mov	x0, x27
   43fd0:	mov	x1, x27
   43fd4:	mov	x2, x25
   43fd8:	bl	c1a0 <__gmpn_rshift@plt>
   43fdc:	mov	x0, x23
   43fe0:	mov	x1, x23
   43fe4:	mov	x2, x19
   43fe8:	mov	x3, x28
   43fec:	bl	c2d0 <__gmpn_sub_n@plt>
   43ff0:	lsl	x21, x28, #3
   43ff4:	ldr	x8, [x23, x21]
   43ff8:	mov	w3, #0x1                   	// #1
   43ffc:	mov	x1, x23
   44000:	mov	x2, x25
   44004:	sub	x8, x8, x0
   44008:	mov	x0, x23
   4400c:	str	x8, [x23, x21]
   44010:	bl	c1a0 <__gmpn_rshift@plt>
   44014:	mov	x0, x23
   44018:	mov	x1, x23
   4401c:	mov	x2, x27
   44020:	mov	x3, x25
   44024:	bl	c840 <__gmpn_rsh1sub_n@plt>
   44028:	add	x26, x19, x21
   4402c:	mov	x0, x22
   44030:	mov	x1, x26
   44034:	mov	x2, x22
   44038:	mov	x3, x25
   4403c:	tbnz	w24, #0, 44048 <__gmpn_toom_interpolate_6pts@@Base+0xf0>
   44040:	bl	c840 <__gmpn_rsh1sub_n@plt>
   44044:	b	4404c <__gmpn_toom_interpolate_6pts@@Base+0xf4>
   44048:	bl	c950 <__gmpn_rsh1add_n@plt>
   4404c:	mov	x0, x27
   44050:	mov	x1, x27
   44054:	mov	x2, x22
   44058:	mov	x3, x25
   4405c:	bl	c2d0 <__gmpn_sub_n@plt>
   44060:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   44064:	mov	x0, x27
   44068:	mov	x1, x27
   4406c:	mov	x2, x25
   44070:	mov	x4, xzr
   44074:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   44078:	mov	x0, x26
   4407c:	mov	x1, x26
   44080:	mov	x2, x22
   44084:	mov	x3, x25
   44088:	bl	c2d0 <__gmpn_sub_n@plt>
   4408c:	mov	x0, x26
   44090:	mov	x1, x26
   44094:	mov	x2, x19
   44098:	mov	x3, x28
   4409c:	str	x28, [sp, #8]
   440a0:	bl	c2d0 <__gmpn_sub_n@plt>
   440a4:	ldr	x8, [x26, x21]
   440a8:	mov	x1, x23
   440ac:	mov	x2, x26
   440b0:	mov	x3, x25
   440b4:	sub	x8, x8, x0
   440b8:	mov	x0, x23
   440bc:	str	x8, [x26, x21]
   440c0:	bl	c2d0 <__gmpn_sub_n@plt>
   440c4:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   440c8:	mov	x0, x23
   440cc:	mov	x1, x23
   440d0:	mov	x2, x25
   440d4:	mov	x4, xzr
   440d8:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   440dc:	add	x28, x19, x20, lsl #3
   440e0:	mov	x0, x28
   440e4:	mov	x1, x28
   440e8:	mov	x2, x22
   440ec:	mov	x3, x25
   440f0:	bl	ca70 <__gmpn_add_n@plt>
   440f4:	mov	w8, #0x18                  	// #24
   440f8:	madd	x25, x20, x8, x19
   440fc:	ldr	x8, [x25, #8]
   44100:	adds	x8, x8, x0
   44104:	str	x8, [x25, #8]
   44108:	b.cc	44128 <__gmpn_toom_interpolate_6pts@@Base+0x1d0>  // b.lo, b.ul, b.last
   4410c:	mov	w8, #0x18                  	// #24
   44110:	madd	x8, x20, x8, x19
   44114:	add	x8, x8, #0x10
   44118:	ldr	x9, [x8]
   4411c:	adds	x9, x9, #0x1
   44120:	str	x9, [x8], #8
   44124:	b.cs	44118 <__gmpn_toom_interpolate_6pts@@Base+0x1c0>  // b.hs, b.nlast
   44128:	ldur	x24, [x29, #-8]
   4412c:	mov	w8, #0x28                  	// #40
   44130:	madd	x22, x20, x8, x19
   44134:	mov	x0, x27
   44138:	mov	x1, x27
   4413c:	mov	x2, x22
   44140:	mov	x3, x24
   44144:	bl	c160 <__gmpn_sublsh2_n@plt>
   44148:	lsl	x9, x24, #3
   4414c:	ldr	x8, [x27, x9]
   44150:	stur	x9, [x29, #-16]
   44154:	subs	x8, x8, x0
   44158:	str	x8, [x27, x9]
   4415c:	b.cs	4417c <__gmpn_toom_interpolate_6pts@@Base+0x224>  // b.hs, b.nlast
   44160:	ldur	x8, [x29, #-8]
   44164:	add	x8, x27, x8, lsl #3
   44168:	add	x8, x8, #0x8
   4416c:	ldr	x9, [x8]
   44170:	sub	x10, x9, #0x1
   44174:	str	x10, [x8], #8
   44178:	cbz	x9, 4416c <__gmpn_toom_interpolate_6pts@@Base+0x214>
   4417c:	mov	x0, x28
   44180:	mov	x1, x28
   44184:	mov	x2, x27
   44188:	mov	x3, x20
   4418c:	bl	c2d0 <__gmpn_sub_n@plt>
   44190:	ldr	x8, [x26]
   44194:	subs	x8, x8, x0
   44198:	str	x8, [x26]
   4419c:	b.cs	441b8 <__gmpn_toom_interpolate_6pts@@Base+0x260>  // b.hs, b.nlast
   441a0:	add	x8, x19, x20, lsl #4
   441a4:	add	x8, x8, #0x8
   441a8:	ldr	x9, [x8]
   441ac:	sub	x10, x9, #0x1
   441b0:	str	x10, [x8], #8
   441b4:	cbz	x9, 441a8 <__gmpn_toom_interpolate_6pts@@Base+0x250>
   441b8:	ldr	x8, [x26, x21]
   441bc:	mov	x0, x25
   441c0:	mov	x1, x25
   441c4:	mov	x2, x27
   441c8:	mov	x3, x20
   441cc:	str	x8, [sp, #24]
   441d0:	bl	ca70 <__gmpn_add_n@plt>
   441d4:	ldr	x24, [x27, x21]
   441d8:	add	x28, x19, x20, lsl #5
   441dc:	lsl	x21, x20, #3
   441e0:	str	x0, [sp, #16]
   441e4:	add	x2, x27, x21
   441e8:	mov	x0, x28
   441ec:	mov	x1, x23
   441f0:	mov	x3, x20
   441f4:	bl	ca70 <__gmpn_add_n@plt>
   441f8:	add	x2, x23, x21
   441fc:	ldr	x8, [x2]
   44200:	add	x9, x0, x24
   44204:	adds	x8, x8, x9
   44208:	str	x8, [x2]
   4420c:	b.cc	44228 <__gmpn_toom_interpolate_6pts@@Base+0x2d0>  // b.lo, b.ul, b.last
   44210:	add	x8, x23, x20, lsl #3
   44214:	add	x8, x8, #0x8
   44218:	ldr	x9, [x8]
   4421c:	adds	x9, x9, #0x1
   44220:	str	x9, [x8], #8
   44224:	b.cs	44218 <__gmpn_toom_interpolate_6pts@@Base+0x2c0>  // b.hs, b.nlast
   44228:	ldur	x27, [x29, #-8]
   4422c:	cmp	x27, x20
   44230:	b.le	443dc <__gmpn_toom_interpolate_6pts@@Base+0x484>
   44234:	ldr	x8, [sp, #8]
   44238:	mov	x0, x22
   4423c:	mov	x1, x22
   44240:	mov	x3, x20
   44244:	ldr	x23, [x23, x8, lsl #3]
   44248:	bl	ca70 <__gmpn_add_n@plt>
   4424c:	add	x23, x0, x23
   44250:	ldp	x9, x8, [sp, #16]
   44254:	add	x3, x27, x20
   44258:	mov	x0, x26
   4425c:	mov	x1, x26
   44260:	mov	x2, x28
   44264:	add	x24, x9, x8
   44268:	bl	c2d0 <__gmpn_sub_n@plt>
   4426c:	sub	x8, x27, #0x1
   44270:	lsl	x8, x8, #3
   44274:	ldr	x9, [x22, x8]
   44278:	mov	w10, #0x1                   	// #1
   4427c:	cmp	x27, x20
   44280:	str	x10, [x22, x8]
   44284:	sub	x9, x9, #0x1
   44288:	b.le	442c0 <__gmpn_toom_interpolate_6pts@@Base+0x368>
   4428c:	subs	x10, x24, x23
   44290:	b.ls	44324 <__gmpn_toom_interpolate_6pts@@Base+0x3cc>  // b.plast
   44294:	ldr	x11, [x28]
   44298:	adds	x10, x11, x10
   4429c:	str	x10, [x28]
   442a0:	b.cc	44350 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.lo, b.ul, b.last
   442a4:	add	x10, x19, x20, lsl #5
   442a8:	add	x10, x10, #0x8
   442ac:	ldr	x11, [x10]
   442b0:	adds	x11, x11, #0x1
   442b4:	str	x11, [x10], #8
   442b8:	b.cs	442ac <__gmpn_toom_interpolate_6pts@@Base+0x354>  // b.hs, b.nlast
   442bc:	b	44350 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>
   442c0:	ldr	x10, [x28]
   442c4:	adds	x10, x10, x24
   442c8:	str	x10, [x28]
   442cc:	b.cc	442e8 <__gmpn_toom_interpolate_6pts@@Base+0x390>  // b.lo, b.ul, b.last
   442d0:	add	x10, x19, x20, lsl #5
   442d4:	add	x10, x10, #0x8
   442d8:	ldr	x11, [x10]
   442dc:	adds	x11, x11, #0x1
   442e0:	str	x11, [x10], #8
   442e4:	b.cs	442d8 <__gmpn_toom_interpolate_6pts@@Base+0x380>  // b.hs, b.nlast
   442e8:	ldur	x12, [x29, #-16]
   442ec:	add	x11, x0, x23
   442f0:	ldr	x10, [x25, x12]
   442f4:	subs	x10, x10, x11
   442f8:	str	x10, [x25, x12]
   442fc:	b.cs	443b0 <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.hs, b.nlast
   44300:	add	x10, x20, x20, lsl #1
   44304:	add	x10, x27, x10
   44308:	add	x10, x19, x10, lsl #3
   4430c:	add	x10, x10, #0x8
   44310:	ldr	x11, [x10]
   44314:	sub	x12, x11, #0x1
   44318:	str	x12, [x10], #8
   4431c:	cbz	x11, 44310 <__gmpn_toom_interpolate_6pts@@Base+0x3b8>
   44320:	b	443b0 <__gmpn_toom_interpolate_6pts@@Base+0x458>
   44324:	ldr	x10, [x28]
   44328:	sub	x11, x23, x24
   4432c:	subs	x10, x10, x11
   44330:	str	x10, [x28]
   44334:	b.cs	44350 <__gmpn_toom_interpolate_6pts@@Base+0x3f8>  // b.hs, b.nlast
   44338:	add	x10, x19, x20, lsl #5
   4433c:	add	x10, x10, #0x8
   44340:	ldr	x11, [x10]
   44344:	sub	x12, x11, #0x1
   44348:	str	x12, [x10], #8
   4434c:	cbz	x11, 44340 <__gmpn_toom_interpolate_6pts@@Base+0x3e8>
   44350:	ldur	x11, [x29, #-16]
   44354:	ldr	x10, [x25, x11]
   44358:	subs	x10, x10, x0
   4435c:	str	x10, [x25, x11]
   44360:	b.cs	44384 <__gmpn_toom_interpolate_6pts@@Base+0x42c>  // b.hs, b.nlast
   44364:	add	x10, x20, x20, lsl #1
   44368:	add	x10, x27, x10
   4436c:	add	x10, x19, x10, lsl #3
   44370:	add	x10, x10, #0x8
   44374:	ldr	x11, [x10]
   44378:	sub	x12, x11, #0x1
   4437c:	str	x12, [x10], #8
   44380:	cbz	x11, 44374 <__gmpn_toom_interpolate_6pts@@Base+0x41c>
   44384:	ldr	x10, [x22, x21]
   44388:	adds	x10, x10, x23
   4438c:	str	x10, [x22, x21]
   44390:	b.cc	443b0 <__gmpn_toom_interpolate_6pts@@Base+0x458>  // b.lo, b.ul, b.last
   44394:	mov	w10, #0x30                  	// #48
   44398:	madd	x10, x20, x10, x19
   4439c:	add	x10, x10, #0x8
   443a0:	ldr	x11, [x10]
   443a4:	adds	x11, x11, #0x1
   443a8:	str	x11, [x10], #8
   443ac:	b.cs	443a0 <__gmpn_toom_interpolate_6pts@@Base+0x448>  // b.hs, b.nlast
   443b0:	ldr	x10, [x22, x8]
   443b4:	add	x9, x9, x10
   443b8:	str	x9, [x22, x8]
   443bc:	ldp	x20, x19, [sp, #128]
   443c0:	ldp	x22, x21, [sp, #112]
   443c4:	ldp	x24, x23, [sp, #96]
   443c8:	ldp	x26, x25, [sp, #80]
   443cc:	ldp	x28, x27, [sp, #64]
   443d0:	ldp	x29, x30, [sp, #48]
   443d4:	add	sp, sp, #0x90
   443d8:	ret
   443dc:	mov	x0, x22
   443e0:	mov	x1, x22
   443e4:	mov	x3, x27
   443e8:	bl	ca70 <__gmpn_add_n@plt>
   443ec:	mov	x23, x0
   443f0:	b	44250 <__gmpn_toom_interpolate_6pts@@Base+0x2f8>

00000000000443f4 <__gmpn_toom_interpolate_7pts@@Base>:
   443f4:	sub	sp, sp, #0x90
   443f8:	stp	x29, x30, [sp, #48]
   443fc:	stp	x28, x27, [sp, #64]
   44400:	stp	x26, x25, [sp, #80]
   44404:	stp	x24, x23, [sp, #96]
   44408:	stp	x22, x21, [sp, #112]
   4440c:	stp	x20, x19, [sp, #128]
   44410:	add	x29, sp, #0x30
   44414:	ldr	x8, [x29, #96]
   44418:	mov	w27, #0x1                   	// #1
   4441c:	bfi	x27, x1, #1, #63
   44420:	mov	x26, x3
   44424:	mov	w21, w2
   44428:	mov	x19, x1
   4442c:	mov	x20, x0
   44430:	lsl	x25, x1, #1
   44434:	mov	x0, x6
   44438:	mov	x1, x6
   4443c:	mov	x2, x5
   44440:	mov	x3, x27
   44444:	mov	x23, x6
   44448:	mov	x22, x5
   4444c:	mov	x24, x4
   44450:	stp	x8, x7, [x29, #-16]
   44454:	bl	ca70 <__gmpn_add_n@plt>
   44458:	mov	x0, x26
   4445c:	str	w21, [sp, #12]
   44460:	tbnz	w21, #0, 4447c <__gmpn_toom_interpolate_7pts@@Base+0x88>
   44464:	mov	x1, x22
   44468:	mov	x2, x26
   4446c:	mov	x3, x27
   44470:	bl	c840 <__gmpn_rsh1sub_n@plt>
   44474:	cbnz	x19, 44490 <__gmpn_toom_interpolate_7pts@@Base+0x9c>
   44478:	b	444d0 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   4447c:	mov	x1, x26
   44480:	mov	x2, x22
   44484:	mov	x3, x27
   44488:	bl	c950 <__gmpn_rsh1add_n@plt>
   4448c:	cbz	x19, 444d0 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   44490:	mov	x0, x22
   44494:	mov	x1, x22
   44498:	mov	x2, x20
   4449c:	mov	x3, x25
   444a0:	bl	c2d0 <__gmpn_sub_n@plt>
   444a4:	cbz	x0, 444d0 <__gmpn_toom_interpolate_7pts@@Base+0xdc>
   444a8:	add	x9, x22, x27, lsl #3
   444ac:	mov	x8, xzr
   444b0:	sub	x9, x9, #0x8
   444b4:	cmp	x8, #0x8
   444b8:	b.eq	444d0 <__gmpn_toom_interpolate_7pts@@Base+0xdc>  // b.none
   444bc:	ldr	x10, [x9, x8]
   444c0:	sub	x11, x10, #0x1
   444c4:	str	x11, [x9, x8]
   444c8:	add	x8, x8, #0x8
   444cc:	cbz	x10, 444b4 <__gmpn_toom_interpolate_7pts@@Base+0xc0>
   444d0:	mov	x0, x22
   444d4:	mov	x1, x22
   444d8:	mov	x2, x26
   444dc:	mov	x3, x27
   444e0:	mov	x28, x20
   444e4:	str	x25, [sp, #16]
   444e8:	bl	c2d0 <__gmpn_sub_n@plt>
   444ec:	mov	w3, #0x2                   	// #2
   444f0:	mov	x0, x22
   444f4:	mov	x1, x22
   444f8:	mov	x2, x27
   444fc:	bl	c1a0 <__gmpn_rshift@plt>
   44500:	mov	w8, #0x30                  	// #48
   44504:	madd	x1, x19, x8, x20
   44508:	ldur	x21, [x29, #-16]
   4450c:	str	x1, [sp, #24]
   44510:	ldur	x25, [x29, #-8]
   44514:	mov	w3, #0x4                   	// #4
   44518:	mov	x0, x21
   4451c:	mov	x2, x25
   44520:	bl	c180 <__gmpn_lshift@plt>
   44524:	adds	x28, x25, #0x1
   44528:	str	x0, [x21, x25, lsl #3]
   4452c:	b.cs	44568 <__gmpn_toom_interpolate_7pts@@Base+0x174>  // b.hs, b.nlast
   44530:	ldur	x2, [x29, #-16]
   44534:	mov	x0, x22
   44538:	mov	x1, x22
   4453c:	mov	x3, x28
   44540:	bl	c2d0 <__gmpn_sub_n@plt>
   44544:	cbz	x0, 44568 <__gmpn_toom_interpolate_7pts@@Base+0x174>
   44548:	cmp	x28, x27
   4454c:	b.ge	44568 <__gmpn_toom_interpolate_7pts@@Base+0x174>  // b.tcont
   44550:	lsl	x8, x28, #3
   44554:	ldr	x9, [x22, x8]
   44558:	add	x28, x28, #0x1
   4455c:	sub	x10, x9, #0x1
   44560:	str	x10, [x22, x8]
   44564:	cbz	x9, 44548 <__gmpn_toom_interpolate_7pts@@Base+0x154>
   44568:	ldr	x25, [sp, #16]
   4456c:	ldr	w8, [sp, #12]
   44570:	add	x28, x20, x25, lsl #3
   44574:	tbnz	w8, #1, 44590 <__gmpn_toom_interpolate_7pts@@Base+0x19c>
   44578:	mov	x0, x24
   4457c:	mov	x1, x28
   44580:	mov	x2, x24
   44584:	mov	x3, x27
   44588:	bl	c840 <__gmpn_rsh1sub_n@plt>
   4458c:	b	445a4 <__gmpn_toom_interpolate_7pts@@Base+0x1b0>
   44590:	mov	x0, x24
   44594:	mov	x1, x24
   44598:	mov	x2, x28
   4459c:	mov	x3, x27
   445a0:	bl	c950 <__gmpn_rsh1add_n@plt>
   445a4:	ldur	x21, [x29, #-8]
   445a8:	mov	x0, x28
   445ac:	mov	x1, x28
   445b0:	mov	x2, x24
   445b4:	mov	x3, x27
   445b8:	bl	c2d0 <__gmpn_sub_n@plt>
   445bc:	mov	w3, #0x41                  	// #65
   445c0:	mov	x0, x23
   445c4:	mov	x1, x28
   445c8:	mov	x2, x27
   445cc:	bl	c9e0 <__gmpn_submul_1@plt>
   445d0:	cbz	x21, 4460c <__gmpn_toom_interpolate_7pts@@Base+0x218>
   445d4:	ldr	x2, [sp, #24]
   445d8:	mov	x0, x28
   445dc:	mov	x1, x28
   445e0:	mov	x3, x21
   445e4:	bl	c2d0 <__gmpn_sub_n@plt>
   445e8:	cbz	x0, 4460c <__gmpn_toom_interpolate_7pts@@Base+0x218>
   445ec:	cmp	x21, x27
   445f0:	b.ge	4460c <__gmpn_toom_interpolate_7pts@@Base+0x218>  // b.tcont
   445f4:	lsl	x9, x21, #3
   445f8:	ldr	x10, [x28, x9]
   445fc:	add	x21, x21, #0x1
   44600:	sub	x11, x10, #0x1
   44604:	str	x11, [x28, x9]
   44608:	cbz	x10, 445ec <__gmpn_toom_interpolate_7pts@@Base+0x1f8>
   4460c:	cbz	x19, 4464c <__gmpn_toom_interpolate_7pts@@Base+0x258>
   44610:	mov	x0, x28
   44614:	mov	x1, x28
   44618:	mov	x2, x20
   4461c:	mov	x3, x25
   44620:	bl	c2d0 <__gmpn_sub_n@plt>
   44624:	cbz	x0, 4464c <__gmpn_toom_interpolate_7pts@@Base+0x258>
   44628:	mov	x8, xzr
   4462c:	add	x9, x20, x19, lsl #5
   44630:	cmp	x8, #0x8
   44634:	b.eq	4464c <__gmpn_toom_interpolate_7pts@@Base+0x258>  // b.none
   44638:	ldr	x10, [x9, x8]
   4463c:	sub	x11, x10, #0x1
   44640:	str	x11, [x9, x8]
   44644:	add	x8, x8, #0x8
   44648:	cbz	x10, 44630 <__gmpn_toom_interpolate_7pts@@Base+0x23c>
   4464c:	mov	w3, #0x2d                  	// #45
   44650:	mov	x0, x23
   44654:	mov	x1, x28
   44658:	mov	x2, x27
   4465c:	bl	d400 <__gmpn_addmul_1@plt>
   44660:	mov	w3, #0x1                   	// #1
   44664:	mov	x0, x23
   44668:	mov	x1, x23
   4466c:	mov	x2, x27
   44670:	bl	c1a0 <__gmpn_rshift@plt>
   44674:	mov	x0, x22
   44678:	mov	x1, x22
   4467c:	mov	x2, x28
   44680:	mov	x3, x27
   44684:	bl	c2d0 <__gmpn_sub_n@plt>
   44688:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   4468c:	mov	x0, x22
   44690:	mov	x1, x22
   44694:	mov	x2, x27
   44698:	mov	x4, xzr
   4469c:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   446a0:	mov	x0, x28
   446a4:	mov	x1, x28
   446a8:	mov	x2, x22
   446ac:	mov	x3, x27
   446b0:	bl	c2d0 <__gmpn_sub_n@plt>
   446b4:	mov	x0, x26
   446b8:	mov	x1, x23
   446bc:	mov	x2, x26
   446c0:	mov	x3, x27
   446c4:	bl	c2d0 <__gmpn_sub_n@plt>
   446c8:	ldur	x21, [x29, #-16]
   446cc:	mov	w3, #0x3                   	// #3
   446d0:	mov	x1, x24
   446d4:	mov	x2, x27
   446d8:	mov	x0, x21
   446dc:	bl	c180 <__gmpn_lshift@plt>
   446e0:	mov	x0, x23
   446e4:	mov	x1, x23
   446e8:	mov	x2, x21
   446ec:	mov	x3, x27
   446f0:	bl	c2d0 <__gmpn_sub_n@plt>
   446f4:	mov	x4, #0x8e39                	// #36409
   446f8:	movk	x4, #0x38e3, lsl #16
   446fc:	movk	x4, #0xe38e, lsl #32
   44700:	mov	w3, #0x9                   	// #9
   44704:	movk	x4, #0x8e38, lsl #48
   44708:	mov	x0, x23
   4470c:	mov	x1, x23
   44710:	mov	x2, x27
   44714:	mov	w5, wzr
   44718:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4471c:	mov	x0, x24
   44720:	mov	x1, x24
   44724:	mov	x2, x23
   44728:	mov	x3, x27
   4472c:	bl	c2d0 <__gmpn_sub_n@plt>
   44730:	mov	x3, #0x1111111111111111    	// #1229782938247303441
   44734:	mov	x0, x26
   44738:	mov	x1, x26
   4473c:	mov	x2, x27
   44740:	mov	x4, xzr
   44744:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   44748:	mov	x0, x26
   4474c:	mov	x1, x26
   44750:	mov	x2, x23
   44754:	mov	x3, x27
   44758:	bl	c950 <__gmpn_rsh1add_n@plt>
   4475c:	lsl	x21, x25, #3
   44760:	ldr	x8, [x26, x21]
   44764:	mov	x0, x23
   44768:	mov	x1, x23
   4476c:	mov	x2, x26
   44770:	and	x8, x8, #0x7fffffffffffffff
   44774:	mov	x3, x27
   44778:	str	x8, [x26, x21]
   4477c:	bl	c2d0 <__gmpn_sub_n@plt>
   44780:	lsl	x25, x19, #3
   44784:	add	x0, x20, x25
   44788:	mov	x1, x0
   4478c:	mov	x2, x26
   44790:	mov	x3, x27
   44794:	bl	ca70 <__gmpn_add_n@plt>
   44798:	add	x8, x28, x25
   4479c:	ldr	x9, [x8, #8]
   447a0:	adds	x9, x9, x0
   447a4:	str	x9, [x8, #8]
   447a8:	b.cc	447c8 <__gmpn_toom_interpolate_7pts@@Base+0x3d4>  // b.lo, b.ul, b.last
   447ac:	mov	w8, #0x18                  	// #24
   447b0:	madd	x8, x19, x8, x20
   447b4:	add	x8, x8, #0x10
   447b8:	ldr	x9, [x8]
   447bc:	adds	x9, x9, #0x1
   447c0:	str	x9, [x8], #8
   447c4:	b.cs	447b8 <__gmpn_toom_interpolate_7pts@@Base+0x3c4>  // b.hs, b.nlast
   447c8:	mov	w8, #0x18                  	// #24
   447cc:	madd	x0, x19, x8, x20
   447d0:	mov	x1, x0
   447d4:	mov	x2, x24
   447d8:	mov	x3, x19
   447dc:	bl	ca70 <__gmpn_add_n@plt>
   447e0:	add	x1, x24, x19, lsl #3
   447e4:	ldr	x8, [x28, x21]
   447e8:	ldr	x9, [x1]
   447ec:	add	x8, x8, x0
   447f0:	add	x8, x9, x8
   447f4:	str	x8, [x1]
   447f8:	ldr	x9, [x28, x21]
   447fc:	add	x9, x9, x0
   44800:	cmp	x8, x9
   44804:	b.cs	4481c <__gmpn_toom_interpolate_7pts@@Base+0x428>  // b.hs, b.nlast
   44808:	add	x8, x1, #0x8
   4480c:	ldr	x9, [x8]
   44810:	adds	x9, x9, #0x1
   44814:	str	x9, [x8], #8
   44818:	b.cs	4480c <__gmpn_toom_interpolate_7pts@@Base+0x418>  // b.hs, b.nlast
   4481c:	add	x0, x20, x19, lsl #5
   44820:	mov	x2, x22
   44824:	mov	x3, x19
   44828:	bl	ca70 <__gmpn_add_n@plt>
   4482c:	add	x1, x22, x19, lsl #3
   44830:	ldr	x8, [x24, x21]
   44834:	ldr	x9, [x1]
   44838:	add	x8, x8, x0
   4483c:	add	x8, x9, x8
   44840:	str	x8, [x1]
   44844:	ldr	x9, [x24, x21]
   44848:	add	x9, x9, x0
   4484c:	cmp	x8, x9
   44850:	b.cs	44868 <__gmpn_toom_interpolate_7pts@@Base+0x474>  // b.hs, b.nlast
   44854:	add	x8, x1, #0x8
   44858:	ldr	x9, [x8]
   4485c:	adds	x9, x9, #0x1
   44860:	str	x9, [x8], #8
   44864:	b.cs	44858 <__gmpn_toom_interpolate_7pts@@Base+0x464>  // b.hs, b.nlast
   44868:	mov	w8, #0x28                  	// #40
   4486c:	madd	x0, x19, x8, x20
   44870:	mov	x2, x23
   44874:	mov	x3, x19
   44878:	bl	ca70 <__gmpn_add_n@plt>
   4487c:	add	x2, x23, x19, lsl #3
   44880:	ldr	x8, [x22, x21]
   44884:	ldr	x9, [x2]
   44888:	add	x8, x8, x0
   4488c:	add	x8, x9, x8
   44890:	str	x8, [x2]
   44894:	ldr	x9, [x22, x21]
   44898:	add	x9, x9, x0
   4489c:	cmp	x8, x9
   448a0:	b.cs	448b8 <__gmpn_toom_interpolate_7pts@@Base+0x4c4>  // b.hs, b.nlast
   448a4:	add	x8, x2, #0x8
   448a8:	ldr	x9, [x8]
   448ac:	adds	x9, x9, #0x1
   448b0:	str	x9, [x8], #8
   448b4:	b.cs	448a8 <__gmpn_toom_interpolate_7pts@@Base+0x4b4>  // b.hs, b.nlast
   448b8:	ldur	x8, [x29, #-8]
   448bc:	add	x3, x19, #0x1
   448c0:	cmp	x3, x8
   448c4:	b.ge	44928 <__gmpn_toom_interpolate_7pts@@Base+0x534>  // b.tcont
   448c8:	ldr	x0, [sp, #24]
   448cc:	mov	x1, x0
   448d0:	bl	ca70 <__gmpn_add_n@plt>
   448d4:	mov	w8, #0x38                  	// #56
   448d8:	madd	x8, x19, x8, x20
   448dc:	ldr	x9, [x8, #8]
   448e0:	adds	x9, x9, x0
   448e4:	str	x9, [x8, #8]
   448e8:	b.cc	44908 <__gmpn_toom_interpolate_7pts@@Base+0x514>  // b.lo, b.ul, b.last
   448ec:	mov	w8, #0x38                  	// #56
   448f0:	madd	x8, x19, x8, x20
   448f4:	add	x8, x8, #0x10
   448f8:	ldr	x9, [x8]
   448fc:	adds	x9, x9, #0x1
   44900:	str	x9, [x8], #8
   44904:	b.cs	448f8 <__gmpn_toom_interpolate_7pts@@Base+0x504>  // b.hs, b.nlast
   44908:	ldp	x20, x19, [sp, #128]
   4490c:	ldp	x22, x21, [sp, #112]
   44910:	ldp	x24, x23, [sp, #96]
   44914:	ldp	x26, x25, [sp, #80]
   44918:	ldp	x28, x27, [sp, #64]
   4491c:	ldp	x29, x30, [sp, #48]
   44920:	add	sp, sp, #0x90
   44924:	ret
   44928:	ldr	x0, [sp, #24]
   4492c:	ldp	x20, x19, [sp, #128]
   44930:	ldp	x22, x21, [sp, #112]
   44934:	ldp	x24, x23, [sp, #96]
   44938:	ldp	x26, x25, [sp, #80]
   4493c:	ldp	x28, x27, [sp, #64]
   44940:	ldp	x29, x30, [sp, #48]
   44944:	mov	x1, x0
   44948:	mov	x3, x8
   4494c:	add	sp, sp, #0x90
   44950:	b	ca70 <__gmpn_add_n@plt>

0000000000044954 <__gmpn_toom_interpolate_8pts@@Base>:
   44954:	sub	sp, sp, #0x90
   44958:	stp	x29, x30, [sp, #48]
   4495c:	stp	x28, x27, [sp, #64]
   44960:	stp	x26, x25, [sp, #80]
   44964:	stp	x24, x23, [sp, #96]
   44968:	stp	x22, x21, [sp, #112]
   4496c:	stp	x20, x19, [sp, #128]
   44970:	add	x25, x2, x1, lsl #3
   44974:	ldr	x8, [x0]
   44978:	ldr	x9, [x25]
   4497c:	mov	w10, #0x38                  	// #56
   44980:	mov	x23, x5
   44984:	mov	x26, x3
   44988:	sub	x8, x9, x8, lsr #4
   4498c:	str	x8, [x25]
   44990:	ldr	x8, [x0]
   44994:	mov	x21, x2
   44998:	mov	x19, x1
   4499c:	add	x29, sp, #0x30
   449a0:	cmp	x9, x8, lsr #4
   449a4:	madd	x8, x1, x10, x0
   449a8:	stp	x8, x4, [sp, #16]
   449ac:	b.cs	449c4 <__gmpn_toom_interpolate_8pts@@Base+0x70>  // b.hs, b.nlast
   449b0:	add	x8, x25, #0x8
   449b4:	ldr	x9, [x8]
   449b8:	sub	x10, x9, #0x1
   449bc:	str	x10, [x8], #8
   449c0:	cbz	x9, 449b4 <__gmpn_toom_interpolate_8pts@@Base+0x60>
   449c4:	add	x8, x19, x19, lsl #1
   449c8:	str	x8, [sp, #8]
   449cc:	lsl	x8, x19, #1
   449d0:	add	x22, x0, #0x8
   449d4:	sub	x24, x8, #0x1
   449d8:	stp	x0, x8, [x29, #-16]
   449dc:	mov	w3, #0x3c                  	// #60
   449e0:	mov	x0, x23
   449e4:	mov	x1, x22
   449e8:	mov	x2, x24
   449ec:	bl	c180 <__gmpn_lshift@plt>
   449f0:	mov	x27, x0
   449f4:	mov	x0, x25
   449f8:	mov	x1, x25
   449fc:	mov	x2, x23
   44a00:	mov	x3, x24
   44a04:	bl	c2d0 <__gmpn_sub_n@plt>
   44a08:	add	x8, x25, x19, lsl #4
   44a0c:	ldur	x9, [x8, #-8]
   44a10:	add	x10, x0, x27
   44a14:	subs	x9, x9, x10
   44a18:	stur	x9, [x8, #-8]
   44a1c:	b.cs	44a30 <__gmpn_toom_interpolate_8pts@@Base+0xdc>  // b.hs, b.nlast
   44a20:	ldr	x9, [x8]
   44a24:	sub	x10, x9, #0x1
   44a28:	str	x10, [x8], #8
   44a2c:	cbz	x9, 44a20 <__gmpn_toom_interpolate_8pts@@Base+0xcc>
   44a30:	ldr	x20, [sp, #24]
   44a34:	ldur	x8, [x29, #-16]
   44a38:	ldp	x9, x1, [sp, #8]
   44a3c:	mov	w3, #0xc                   	// #12
   44a40:	mov	x0, x23
   44a44:	mov	x2, x20
   44a48:	add	x28, x8, x9, lsl #3
   44a4c:	bl	c180 <__gmpn_lshift@plt>
   44a50:	mov	x27, x0
   44a54:	mov	x0, x21
   44a58:	mov	x1, x21
   44a5c:	mov	x2, x23
   44a60:	mov	x3, x20
   44a64:	bl	c2d0 <__gmpn_sub_n@plt>
   44a68:	lsl	x20, x20, #3
   44a6c:	ldr	x8, [x21, x20]
   44a70:	add	x9, x0, x27
   44a74:	subs	x8, x8, x9
   44a78:	str	x8, [x21, x20]
   44a7c:	b.cs	44a9c <__gmpn_toom_interpolate_8pts@@Base+0x148>  // b.hs, b.nlast
   44a80:	ldr	x8, [sp, #24]
   44a84:	add	x8, x21, x8, lsl #3
   44a88:	add	x8, x8, #0x8
   44a8c:	ldr	x9, [x8]
   44a90:	sub	x10, x9, #0x1
   44a94:	str	x10, [x8], #8
   44a98:	cbz	x9, 44a8c <__gmpn_toom_interpolate_8pts@@Base+0x138>
   44a9c:	ldur	x10, [x29, #-16]
   44aa0:	add	x27, x28, x19, lsl #3
   44aa4:	ldr	x9, [x27]
   44aa8:	ldr	x8, [x10]
   44aac:	sub	x8, x9, x8, lsr #2
   44ab0:	str	x8, [x27]
   44ab4:	ldr	x8, [x10]
   44ab8:	cmp	x9, x8, lsr #2
   44abc:	b.cs	44adc <__gmpn_toom_interpolate_8pts@@Base+0x188>  // b.hs, b.nlast
   44ac0:	ldur	x8, [x29, #-16]
   44ac4:	add	x8, x8, x19, lsl #5
   44ac8:	add	x8, x8, #0x8
   44acc:	ldr	x9, [x8]
   44ad0:	sub	x10, x9, #0x1
   44ad4:	str	x10, [x8], #8
   44ad8:	cbz	x9, 44acc <__gmpn_toom_interpolate_8pts@@Base+0x178>
   44adc:	mov	w3, #0x3e                  	// #62
   44ae0:	mov	x0, x23
   44ae4:	mov	x1, x22
   44ae8:	mov	x2, x24
   44aec:	bl	c180 <__gmpn_lshift@plt>
   44af0:	mov	x22, x0
   44af4:	mov	x0, x27
   44af8:	mov	x1, x27
   44afc:	mov	x2, x23
   44b00:	mov	x3, x24
   44b04:	bl	c2d0 <__gmpn_sub_n@plt>
   44b08:	ldur	x8, [x29, #-8]
   44b0c:	add	x10, x0, x22
   44b10:	add	x8, x27, x8, lsl #3
   44b14:	ldur	x9, [x8, #-8]
   44b18:	subs	x9, x9, x10
   44b1c:	stur	x9, [x8, #-8]
   44b20:	b.cs	44b34 <__gmpn_toom_interpolate_8pts@@Base+0x1e0>  // b.hs, b.nlast
   44b24:	ldr	x9, [x8]
   44b28:	sub	x10, x9, #0x1
   44b2c:	str	x10, [x8], #8
   44b30:	cbz	x9, 44b24 <__gmpn_toom_interpolate_8pts@@Base+0x1d0>
   44b34:	ldp	x1, x24, [sp, #16]
   44b38:	mov	w3, #0x6                   	// #6
   44b3c:	mov	x0, x23
   44b40:	mov	x2, x24
   44b44:	bl	c180 <__gmpn_lshift@plt>
   44b48:	mov	x22, x0
   44b4c:	mov	x0, x28
   44b50:	mov	x1, x28
   44b54:	mov	x2, x23
   44b58:	mov	x3, x24
   44b5c:	bl	c2d0 <__gmpn_sub_n@plt>
   44b60:	ldr	x8, [x28, x20]
   44b64:	ldur	x24, [x29, #-16]
   44b68:	add	x9, x0, x22
   44b6c:	subs	x8, x8, x9
   44b70:	str	x8, [x28, x20]
   44b74:	b.cs	44b9c <__gmpn_toom_interpolate_8pts@@Base+0x248>  // b.hs, b.nlast
   44b78:	ldr	x9, [sp, #24]
   44b7c:	add	x8, x19, x19, lsl #1
   44b80:	add	x8, x9, x8
   44b84:	add	x8, x24, x8, lsl #3
   44b88:	add	x8, x8, #0x8
   44b8c:	ldr	x9, [x8]
   44b90:	sub	x10, x9, #0x1
   44b94:	str	x10, [x8], #8
   44b98:	cbz	x9, 44b8c <__gmpn_toom_interpolate_8pts@@Base+0x238>
   44b9c:	ldur	x3, [x29, #-8]
   44ba0:	add	x23, x26, x19, lsl #3
   44ba4:	mov	x0, x23
   44ba8:	mov	x1, x23
   44bac:	mov	x2, x24
   44bb0:	bl	c2d0 <__gmpn_sub_n@plt>
   44bb4:	ldp	x8, x2, [sp, #8]
   44bb8:	ldr	x3, [sp, #24]
   44bbc:	mov	x1, x26
   44bc0:	lsl	x9, x8, #3
   44bc4:	ldr	x8, [x26, x9]
   44bc8:	str	x9, [sp]
   44bcc:	sub	x8, x8, x0
   44bd0:	mov	x0, x26
   44bd4:	str	x8, [x26, x9]
   44bd8:	bl	c2d0 <__gmpn_sub_n@plt>
   44bdc:	ldr	x8, [x26, x20]
   44be0:	subs	x8, x8, x0
   44be4:	str	x8, [x26, x20]
   44be8:	b.cs	44c08 <__gmpn_toom_interpolate_8pts@@Base+0x2b4>  // b.hs, b.nlast
   44bec:	ldr	x8, [sp, #24]
   44bf0:	add	x8, x26, x8, lsl #3
   44bf4:	add	x8, x8, #0x8
   44bf8:	ldr	x9, [x8]
   44bfc:	sub	x10, x9, #0x1
   44c00:	str	x10, [x8], #8
   44c04:	cbz	x9, 44bf8 <__gmpn_toom_interpolate_8pts@@Base+0x2a4>
   44c08:	ldr	x8, [sp, #8]
   44c0c:	mov	x0, x21
   44c10:	mov	x1, x21
   44c14:	mov	x2, x28
   44c18:	add	x22, x8, #0x1
   44c1c:	mov	x3, x22
   44c20:	bl	c2d0 <__gmpn_sub_n@plt>
   44c24:	mov	w3, #0x2                   	// #2
   44c28:	mov	x0, x21
   44c2c:	mov	x1, x21
   44c30:	mov	x2, x22
   44c34:	bl	c1a0 <__gmpn_rshift@plt>
   44c38:	mov	x0, x28
   44c3c:	mov	x1, x28
   44c40:	mov	x2, x26
   44c44:	mov	x3, x22
   44c48:	bl	c2d0 <__gmpn_sub_n@plt>
   44c4c:	mov	x0, x21
   44c50:	mov	x1, x21
   44c54:	mov	x2, x28
   44c58:	mov	x3, x22
   44c5c:	bl	c2d0 <__gmpn_sub_n@plt>
   44c60:	mov	x4, #0x4fa5                	// #20389
   44c64:	movk	x4, #0xa4fa, lsl #16
   44c68:	movk	x4, #0xfa4f, lsl #32
   44c6c:	mov	w3, #0x2d                  	// #45
   44c70:	movk	x4, #0x4fa4, lsl #48
   44c74:	mov	x0, x21
   44c78:	mov	x1, x21
   44c7c:	mov	x2, x22
   44c80:	mov	w5, wzr
   44c84:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   44c88:	mov	x3, #0x5555555555555555    	// #6148914691236517205
   44c8c:	mov	x0, x28
   44c90:	mov	x1, x28
   44c94:	mov	x2, x22
   44c98:	mov	x4, xzr
   44c9c:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   44ca0:	mov	x0, x28
   44ca4:	mov	x1, x28
   44ca8:	mov	x2, x21
   44cac:	mov	x3, x22
   44cb0:	bl	c160 <__gmpn_sublsh2_n@plt>
   44cb4:	add	x22, x24, x19, lsl #3
   44cb8:	mov	x0, x22
   44cbc:	mov	x1, x22
   44cc0:	mov	x2, x26
   44cc4:	mov	x3, x19
   44cc8:	bl	ca70 <__gmpn_add_n@plt>
   44ccc:	mov	x24, x0
   44cd0:	mov	x0, x22
   44cd4:	mov	x1, x22
   44cd8:	mov	x2, x28
   44cdc:	mov	x3, x19
   44ce0:	bl	c2d0 <__gmpn_sub_n@plt>
   44ce4:	sub	x8, x24, x0
   44ce8:	cmp	x8, #0x1
   44cec:	b.lt	44d08 <__gmpn_toom_interpolate_8pts@@Base+0x3b4>  // b.tstop
   44cf0:	mov	x8, x23
   44cf4:	ldr	x9, [x8]
   44cf8:	adds	x9, x9, #0x1
   44cfc:	str	x9, [x8], #8
   44d00:	b.cs	44cf4 <__gmpn_toom_interpolate_8pts@@Base+0x3a0>  // b.hs, b.nlast
   44d04:	mov	x8, xzr
   44d08:	ldp	x9, x10, [x29, #-16]
   44d0c:	neg	x4, x8
   44d10:	mov	x1, x23
   44d14:	mov	x2, x27
   44d18:	lsl	x20, x10, #3
   44d1c:	add	x0, x9, x20
   44d20:	mov	x3, x19
   44d24:	bl	c760 <__gmpn_sub_nc@plt>
   44d28:	add	x2, x26, x20
   44d2c:	ldr	x8, [x2]
   44d30:	subs	x8, x8, x0
   44d34:	str	x8, [x2]
   44d38:	b.cs	44d54 <__gmpn_toom_interpolate_8pts@@Base+0x400>  // b.hs, b.nlast
   44d3c:	add	x8, x26, x19, lsl #4
   44d40:	add	x8, x8, #0x8
   44d44:	ldr	x9, [x8]
   44d48:	sub	x10, x9, #0x1
   44d4c:	str	x10, [x8], #8
   44d50:	cbz	x9, 44d44 <__gmpn_toom_interpolate_8pts@@Base+0x3f0>
   44d54:	add	x22, x19, #0x1
   44d58:	mov	x0, x28
   44d5c:	mov	x1, x28
   44d60:	mov	x3, x22
   44d64:	bl	ca70 <__gmpn_add_n@plt>
   44d68:	ldur	x8, [x29, #-8]
   44d6c:	mov	x23, x0
   44d70:	mov	x2, x21
   44d74:	mov	x3, x19
   44d78:	add	x24, x28, x8, lsl #3
   44d7c:	mov	x0, x24
   44d80:	mov	x1, x24
   44d84:	bl	ca70 <__gmpn_add_n@plt>
   44d88:	ldr	x26, [sp]
   44d8c:	mov	x1, x28
   44d90:	mov	x2, x24
   44d94:	mov	x3, x22
   44d98:	ldr	x8, [x28, x26]
   44d9c:	add	x8, x8, x0
   44da0:	mov	x0, x28
   44da4:	str	x8, [x28, x26]
   44da8:	bl	c2d0 <__gmpn_sub_n@plt>
   44dac:	subs	x8, x23, x0
   44db0:	b.mi	45060 <__gmpn_toom_interpolate_8pts@@Base+0x70c>  // b.first
   44db4:	ldr	x9, [x27, #8]
   44db8:	ldur	x20, [x29, #-16]
   44dbc:	adds	x8, x9, x8
   44dc0:	str	x8, [x27, #8]
   44dc4:	b.cc	44de0 <__gmpn_toom_interpolate_8pts@@Base+0x48c>  // b.lo, b.ul, b.last
   44dc8:	add	x8, x20, x19, lsl #5
   44dcc:	add	x8, x8, #0x10
   44dd0:	ldr	x9, [x8]
   44dd4:	adds	x9, x9, #0x1
   44dd8:	str	x9, [x8], #8
   44ddc:	b.cs	44dd0 <__gmpn_toom_interpolate_8pts@@Base+0x47c>  // b.hs, b.nlast
   44de0:	ldur	x8, [x29, #-8]
   44de4:	add	x0, x20, x19, lsl #5
   44de8:	mov	x1, x27
   44dec:	mov	x2, x25
   44df0:	orr	x3, x8, #0x1
   44df4:	bl	c2d0 <__gmpn_sub_n@plt>
   44df8:	mov	w8, #0x30                  	// #48
   44dfc:	madd	x9, x19, x8, x20
   44e00:	ldr	x8, [x9]
   44e04:	ldr	x10, [x25]
   44e08:	adds	x8, x10, x8
   44e0c:	str	x8, [x9]
   44e10:	b.cc	44f00 <__gmpn_toom_interpolate_8pts@@Base+0x5ac>  // b.lo, b.ul, b.last
   44e14:	mov	x11, xzr
   44e18:	sub	x10, x19, #0x1
   44e1c:	mov	w8, #0x1                   	// #1
   44e20:	cmp	x8, x19
   44e24:	b.ge	44f88 <__gmpn_toom_interpolate_8pts@@Base+0x634>  // b.tcont
   44e28:	add	x12, x25, x11
   44e2c:	ldr	x12, [x12, #8]
   44e30:	add	x13, x9, x11
   44e34:	add	x8, x8, #0x1
   44e38:	add	x11, x11, #0x8
   44e3c:	adds	x12, x12, #0x1
   44e40:	sub	x10, x10, #0x1
   44e44:	str	x12, [x13, #8]
   44e48:	b.cs	44e20 <__gmpn_toom_interpolate_8pts@@Base+0x4cc>  // b.hs, b.nlast
   44e4c:	cmp	x25, x9
   44e50:	b.eq	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   44e54:	cmp	x8, x19
   44e58:	b.ge	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.tcont
   44e5c:	sub	x12, x19, x8
   44e60:	cmp	x12, #0x4
   44e64:	b.cc	44ed4 <__gmpn_toom_interpolate_8pts@@Base+0x580>  // b.lo, b.ul, b.last
   44e68:	add	x13, x9, x11
   44e6c:	add	x13, x13, #0x8
   44e70:	add	x14, x21, x19, lsl #4
   44e74:	cmp	x13, x14
   44e78:	b.cs	44e94 <__gmpn_toom_interpolate_8pts@@Base+0x540>  // b.hs, b.nlast
   44e7c:	mov	w13, #0x38                  	// #56
   44e80:	add	x14, x25, x11
   44e84:	madd	x13, x19, x13, x20
   44e88:	add	x14, x14, #0x8
   44e8c:	cmp	x14, x13
   44e90:	b.cc	44ed4 <__gmpn_toom_interpolate_8pts@@Base+0x580>  // b.lo, b.ul, b.last
   44e94:	add	x13, x9, x11
   44e98:	add	x11, x25, x11
   44e9c:	and	x9, x12, #0xfffffffffffffffc
   44ea0:	and	x14, x10, #0xfffffffffffffffc
   44ea4:	add	x10, x13, #0x18
   44ea8:	add	x11, x11, #0x18
   44eac:	add	x8, x14, x8
   44eb0:	mov	x13, x9
   44eb4:	ldp	q0, q1, [x11, #-16]
   44eb8:	add	x11, x11, #0x20
   44ebc:	subs	x13, x13, #0x4
   44ec0:	stp	q0, q1, [x10, #-16]
   44ec4:	add	x10, x10, #0x20
   44ec8:	b.ne	44eb4 <__gmpn_toom_interpolate_8pts@@Base+0x560>  // b.any
   44ecc:	cmp	x12, x9
   44ed0:	b.eq	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   44ed4:	mov	w10, #0x6                   	// #6
   44ed8:	sub	x9, x19, x8
   44edc:	add	x11, x8, x19
   44ee0:	madd	x8, x19, x10, x8
   44ee4:	add	x8, x20, x8, lsl #3
   44ee8:	add	x10, x21, x11, lsl #3
   44eec:	ldr	x11, [x10], #8
   44ef0:	subs	x9, x9, #0x1
   44ef4:	str	x11, [x8], #8
   44ef8:	b.ne	44eec <__gmpn_toom_interpolate_8pts@@Base+0x598>  // b.any
   44efc:	b	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>
   44f00:	cmp	x19, #0x2
   44f04:	b.lt	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.tstop
   44f08:	cmp	x25, x9
   44f0c:	b.eq	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   44f10:	sub	x8, x19, #0x1
   44f14:	cmp	x8, #0x4
   44f18:	b.cc	44f50 <__gmpn_toom_interpolate_8pts@@Base+0x5fc>  // b.lo, b.ul, b.last
   44f1c:	add	x9, x19, x19, lsl #1
   44f20:	mov	w10, #0x8                   	// #8
   44f24:	bfi	x10, x9, #4, #60
   44f28:	add	x9, x20, x10
   44f2c:	add	x10, x21, x19, lsl #4
   44f30:	cmp	x9, x10
   44f34:	add	x9, x21, x19, lsl #3
   44f38:	b.cs	45020 <__gmpn_toom_interpolate_8pts@@Base+0x6cc>  // b.hs, b.nlast
   44f3c:	mov	w10, #0x38                  	// #56
   44f40:	madd	x10, x19, x10, x20
   44f44:	add	x11, x9, #0x8
   44f48:	cmp	x11, x10
   44f4c:	b.cs	45020 <__gmpn_toom_interpolate_8pts@@Base+0x6cc>  // b.hs, b.nlast
   44f50:	mov	w9, #0x1                   	// #1
   44f54:	mov	w10, #0x6                   	// #6
   44f58:	sub	x8, x19, x9
   44f5c:	add	x11, x9, x19
   44f60:	madd	x9, x19, x10, x9
   44f64:	add	x9, x20, x9, lsl #3
   44f68:	add	x10, x21, x11, lsl #3
   44f6c:	ldr	x11, [x10], #8
   44f70:	subs	x8, x8, #0x1
   44f74:	str	x11, [x9], #8
   44f78:	b.ne	44f6c <__gmpn_toom_interpolate_8pts@@Base+0x618>  // b.any
   44f7c:	ldur	x8, [x29, #-8]
   44f80:	add	x2, x21, x8, lsl #3
   44f84:	b	44fa4 <__gmpn_toom_interpolate_8pts@@Base+0x650>
   44f88:	ldur	x8, [x29, #-8]
   44f8c:	add	x2, x21, x8, lsl #3
   44f90:	mov	x8, x2
   44f94:	ldr	x9, [x8]
   44f98:	adds	x9, x9, #0x1
   44f9c:	str	x9, [x8], #8
   44fa0:	b.cs	44f94 <__gmpn_toom_interpolate_8pts@@Base+0x640>  // b.hs, b.nlast
   44fa4:	ldr	x0, [sp, #16]
   44fa8:	mov	x3, x19
   44fac:	mov	x1, x0
   44fb0:	bl	ca70 <__gmpn_add_n@plt>
   44fb4:	ldr	x8, [sp, #24]
   44fb8:	cmp	x8, x19
   44fbc:	b.eq	45000 <__gmpn_toom_interpolate_8pts@@Base+0x6ac>  // b.none
   44fc0:	lsl	x8, x19, #6
   44fc4:	ldr	x9, [x21, x26]
   44fc8:	ldr	x10, [x20, x8]
   44fcc:	add	x9, x9, x0
   44fd0:	add	x9, x10, x9
   44fd4:	str	x9, [x20, x8]
   44fd8:	ldr	x8, [x21, x26]
   44fdc:	add	x8, x8, x0
   44fe0:	cmp	x9, x8
   44fe4:	b.cs	45000 <__gmpn_toom_interpolate_8pts@@Base+0x6ac>  // b.hs, b.nlast
   44fe8:	add	x8, x20, x19, lsl #6
   44fec:	add	x8, x8, #0x8
   44ff0:	ldr	x9, [x8]
   44ff4:	adds	x9, x9, #0x1
   44ff8:	str	x9, [x8], #8
   44ffc:	b.cs	44ff0 <__gmpn_toom_interpolate_8pts@@Base+0x69c>  // b.hs, b.nlast
   45000:	ldp	x20, x19, [sp, #128]
   45004:	ldp	x22, x21, [sp, #112]
   45008:	ldp	x24, x23, [sp, #96]
   4500c:	ldp	x26, x25, [sp, #80]
   45010:	ldp	x28, x27, [sp, #64]
   45014:	ldp	x29, x30, [sp, #48]
   45018:	add	sp, sp, #0x90
   4501c:	ret
   45020:	mov	w12, #0x30                  	// #48
   45024:	and	x10, x8, #0xfffffffffffffffc
   45028:	madd	x12, x19, x12, x20
   4502c:	add	x11, x9, #0x18
   45030:	orr	x9, x10, #0x1
   45034:	add	x12, x12, #0x18
   45038:	mov	x13, x10
   4503c:	ldp	q0, q1, [x11, #-16]
   45040:	add	x11, x11, #0x20
   45044:	subs	x13, x13, #0x4
   45048:	stp	q0, q1, [x12, #-16]
   4504c:	add	x12, x12, #0x20
   45050:	b.ne	4503c <__gmpn_toom_interpolate_8pts@@Base+0x6e8>  // b.any
   45054:	cmp	x8, x10
   45058:	b.eq	44f7c <__gmpn_toom_interpolate_8pts@@Base+0x628>  // b.none
   4505c:	b	44f54 <__gmpn_toom_interpolate_8pts@@Base+0x600>
   45060:	ldur	x20, [x29, #-16]
   45064:	add	x8, x20, x19, lsl #5
   45068:	add	x8, x8, #0x8
   4506c:	ldr	x9, [x8]
   45070:	sub	x10, x9, #0x1
   45074:	str	x10, [x8], #8
   45078:	cbz	x9, 4506c <__gmpn_toom_interpolate_8pts@@Base+0x718>
   4507c:	b	44de0 <__gmpn_toom_interpolate_8pts@@Base+0x48c>

0000000000045080 <__gmpn_toom_interpolate_12pts@@Base>:
   45080:	sub	sp, sp, #0xd0
   45084:	stp	x29, x30, [sp, #112]
   45088:	stp	x28, x27, [sp, #128]
   4508c:	stp	x26, x25, [sp, #144]
   45090:	stp	x20, x19, [sp, #192]
   45094:	add	x29, sp, #0x70
   45098:	mov	x27, x7
   4509c:	mov	x28, x5
   450a0:	mov	x20, x4
   450a4:	mov	x25, x0
   450a8:	add	x11, x4, x4, lsl #1
   450ac:	lsl	x26, x4, #3
   450b0:	stp	x24, x23, [sp, #160]
   450b4:	stp	x22, x21, [sp, #176]
   450b8:	stp	x3, x2, [x29, #-32]
   450bc:	stur	x1, [x29, #-8]
   450c0:	stur	x0, [x29, #-40]
   450c4:	str	w6, [sp, #36]
   450c8:	str	x11, [sp, #56]
   450cc:	cbz	w6, 45320 <__gmpn_toom_interpolate_12pts@@Base+0x2a0>
   450d0:	ldur	x19, [x29, #-24]
   450d4:	mov	w8, #0x58                  	// #88
   450d8:	madd	x21, x20, x8, x25
   450dc:	mov	x2, x21
   450e0:	mov	x0, x19
   450e4:	mov	x1, x19
   450e8:	mov	x3, x28
   450ec:	bl	c2d0 <__gmpn_sub_n@plt>
   450f0:	lsl	x22, x28, #3
   450f4:	ldr	x8, [x19, x22]
   450f8:	subs	x8, x8, x0
   450fc:	str	x8, [x19, x22]
   45100:	b.cs	45120 <__gmpn_toom_interpolate_12pts@@Base+0xa0>  // b.hs, b.nlast
   45104:	ldur	x8, [x29, #-24]
   45108:	add	x8, x8, x28, lsl #3
   4510c:	add	x8, x8, #0x8
   45110:	ldr	x9, [x8]
   45114:	sub	x10, x9, #0x1
   45118:	str	x10, [x8], #8
   4511c:	cbz	x9, 45110 <__gmpn_toom_interpolate_12pts@@Base+0x90>
   45120:	mov	w8, #0x38                  	// #56
   45124:	mov	w3, #0xa                   	// #10
   45128:	mov	x0, x27
   4512c:	mov	x1, x21
   45130:	mov	x2, x28
   45134:	madd	x19, x20, x8, x25
   45138:	bl	c180 <__gmpn_lshift@plt>
   4513c:	mov	x23, x0
   45140:	mov	x0, x19
   45144:	mov	x1, x19
   45148:	mov	x2, x27
   4514c:	mov	x3, x28
   45150:	bl	c2d0 <__gmpn_sub_n@plt>
   45154:	ldr	x8, [x19, x22]
   45158:	add	x9, x0, x23
   4515c:	subs	x8, x8, x9
   45160:	str	x8, [x19, x22]
   45164:	b.cs	45188 <__gmpn_toom_interpolate_12pts@@Base+0x108>  // b.hs, b.nlast
   45168:	sub	x8, x26, x20
   4516c:	add	x8, x28, x8
   45170:	add	x8, x25, x8, lsl #3
   45174:	add	x8, x8, #0x8
   45178:	ldr	x9, [x8]
   4517c:	sub	x10, x9, #0x1
   45180:	str	x10, [x8], #8
   45184:	cbz	x9, 45178 <__gmpn_toom_interpolate_12pts@@Base+0xf8>
   45188:	ldur	x10, [x29, #-32]
   4518c:	ldr	x8, [x21]
   45190:	ldr	x9, [x10]
   45194:	sub	x8, x9, x8, lsr #2
   45198:	str	x8, [x10]
   4519c:	ldr	x8, [x21]
   451a0:	cmp	x9, x8, lsr #2
   451a4:	b.cs	451c0 <__gmpn_toom_interpolate_12pts@@Base+0x140>  // b.hs, b.nlast
   451a8:	ldur	x8, [x29, #-32]
   451ac:	add	x8, x8, #0x8
   451b0:	ldr	x9, [x8]
   451b4:	sub	x10, x9, #0x1
   451b8:	str	x10, [x8], #8
   451bc:	cbz	x9, 451b0 <__gmpn_toom_interpolate_12pts@@Base+0x130>
   451c0:	sub	x23, x28, #0x1
   451c4:	add	x1, x21, #0x8
   451c8:	mov	w3, #0x3e                  	// #62
   451cc:	mov	x0, x27
   451d0:	mov	x2, x23
   451d4:	stur	x1, [x29, #-16]
   451d8:	bl	c180 <__gmpn_lshift@plt>
   451dc:	ldur	x24, [x29, #-32]
   451e0:	mov	x19, x0
   451e4:	mov	x2, x27
   451e8:	mov	x3, x23
   451ec:	mov	x0, x24
   451f0:	mov	x1, x24
   451f4:	bl	c2d0 <__gmpn_sub_n@plt>
   451f8:	add	x8, x24, x28, lsl #3
   451fc:	ldur	x9, [x8, #-8]
   45200:	add	x10, x0, x19
   45204:	subs	x9, x9, x10
   45208:	stur	x9, [x8, #-8]
   4520c:	b.cs	45220 <__gmpn_toom_interpolate_12pts@@Base+0x1a0>  // b.hs, b.nlast
   45210:	ldr	x9, [x8]
   45214:	sub	x10, x9, #0x1
   45218:	str	x10, [x8], #8
   4521c:	cbz	x9, 45210 <__gmpn_toom_interpolate_12pts@@Base+0x190>
   45220:	mov	w3, #0x14                  	// #20
   45224:	mov	x0, x27
   45228:	mov	x1, x21
   4522c:	mov	x2, x28
   45230:	bl	c180 <__gmpn_lshift@plt>
   45234:	ldur	x24, [x29, #-8]
   45238:	mov	x19, x0
   4523c:	mov	x2, x27
   45240:	mov	x3, x28
   45244:	mov	x0, x24
   45248:	mov	x1, x24
   4524c:	bl	c2d0 <__gmpn_sub_n@plt>
   45250:	ldr	x8, [x24, x22]
   45254:	add	x9, x0, x19
   45258:	subs	x8, x8, x9
   4525c:	str	x8, [x24, x22]
   45260:	b.cs	45280 <__gmpn_toom_interpolate_12pts@@Base+0x200>  // b.hs, b.nlast
   45264:	ldur	x8, [x29, #-8]
   45268:	add	x8, x8, x28, lsl #3
   4526c:	add	x8, x8, #0x8
   45270:	ldr	x9, [x8]
   45274:	sub	x10, x9, #0x1
   45278:	str	x10, [x8], #8
   4527c:	cbz	x9, 45270 <__gmpn_toom_interpolate_12pts@@Base+0x1f0>
   45280:	ldr	x8, [sp, #56]
   45284:	add	x25, x25, x8, lsl #3
   45288:	ldr	x8, [x21]
   4528c:	ldr	x9, [x25]
   45290:	sub	x8, x9, x8, lsr #4
   45294:	str	x8, [x25]
   45298:	ldr	x8, [x21]
   4529c:	cmp	x9, x8, lsr #4
   452a0:	b.cs	452c4 <__gmpn_toom_interpolate_12pts@@Base+0x244>  // b.hs, b.nlast
   452a4:	ldur	x9, [x29, #-40]
   452a8:	mov	w8, #0x18                  	// #24
   452ac:	madd	x8, x20, x8, x9
   452b0:	add	x8, x8, #0x8
   452b4:	ldr	x9, [x8]
   452b8:	sub	x10, x9, #0x1
   452bc:	str	x10, [x8], #8
   452c0:	cbz	x9, 452b4 <__gmpn_toom_interpolate_12pts@@Base+0x234>
   452c4:	ldur	x1, [x29, #-16]
   452c8:	mov	w3, #0x3c                  	// #60
   452cc:	mov	x0, x27
   452d0:	mov	x2, x23
   452d4:	bl	c180 <__gmpn_lshift@plt>
   452d8:	mov	x19, x0
   452dc:	mov	x0, x25
   452e0:	mov	x1, x25
   452e4:	mov	x2, x27
   452e8:	mov	x3, x23
   452ec:	bl	c2d0 <__gmpn_sub_n@plt>
   452f0:	add	x8, x25, x28, lsl #3
   452f4:	ldur	x9, [x8, #-8]
   452f8:	ldur	x25, [x29, #-40]
   452fc:	ldr	x11, [sp, #56]
   45300:	add	x10, x0, x19
   45304:	subs	x9, x9, x10
   45308:	stur	x9, [x8, #-8]
   4530c:	b.cs	45320 <__gmpn_toom_interpolate_12pts@@Base+0x2a0>  // b.hs, b.nlast
   45310:	ldr	x9, [x8]
   45314:	sub	x10, x9, #0x1
   45318:	str	x10, [x8], #8
   4531c:	cbz	x9, 45310 <__gmpn_toom_interpolate_12pts@@Base+0x290>
   45320:	lsl	x22, x11, #3
   45324:	lsl	x24, x20, #1
   45328:	str	x28, [sp, #40]
   4532c:	add	x28, x25, x22
   45330:	mov	w3, #0x14                  	// #20
   45334:	mov	x0, x27
   45338:	mov	x1, x25
   4533c:	mov	x2, x24
   45340:	add	x21, x11, #0x1
   45344:	add	x19, x28, x26
   45348:	bl	c180 <__gmpn_lshift@plt>
   4534c:	mov	x23, x0
   45350:	mov	x0, x19
   45354:	mov	x1, x19
   45358:	mov	x2, x27
   4535c:	mov	x3, x24
   45360:	stur	x24, [x29, #-16]
   45364:	bl	c2d0 <__gmpn_sub_n@plt>
   45368:	ldr	x8, [x28, x22]
   4536c:	ldur	x10, [x29, #-8]
   45370:	add	x9, x0, x23
   45374:	stur	x28, [x29, #-48]
   45378:	sub	x8, x8, x9
   4537c:	add	x23, x10, x26
   45380:	str	x8, [x28, x22]
   45384:	ldr	x8, [x25]
   45388:	ldr	x9, [x23]
   4538c:	sub	x8, x9, x8, lsr #4
   45390:	str	x8, [x23]
   45394:	ldr	x8, [x25]
   45398:	cmp	x9, x8, lsr #4
   4539c:	b.cs	453bc <__gmpn_toom_interpolate_12pts@@Base+0x33c>  // b.hs, b.nlast
   453a0:	ldur	x8, [x29, #-8]
   453a4:	add	x8, x8, x20, lsl #3
   453a8:	add	x8, x8, #0x8
   453ac:	ldr	x9, [x8]
   453b0:	sub	x10, x9, #0x1
   453b4:	str	x10, [x8], #8
   453b8:	cbz	x9, 453ac <__gmpn_toom_interpolate_12pts@@Base+0x32c>
   453bc:	str	x20, [sp, #24]
   453c0:	add	x1, x25, #0x8
   453c4:	ldur	x25, [x29, #-16]
   453c8:	mov	w3, #0x3c                  	// #60
   453cc:	mov	x0, x27
   453d0:	str	x1, [sp, #8]
   453d4:	sub	x24, x25, #0x1
   453d8:	mov	x2, x24
   453dc:	bl	c180 <__gmpn_lshift@plt>
   453e0:	mov	x19, x0
   453e4:	mov	x0, x23
   453e8:	mov	x1, x23
   453ec:	mov	x2, x27
   453f0:	mov	x3, x24
   453f4:	bl	c2d0 <__gmpn_sub_n@plt>
   453f8:	add	x8, x23, x25, lsl #3
   453fc:	ldur	x9, [x8, #-8]
   45400:	add	x10, x0, x19
   45404:	str	x23, [sp]
   45408:	subs	x9, x9, x10
   4540c:	stur	x9, [x8, #-8]
   45410:	b.cs	45424 <__gmpn_toom_interpolate_12pts@@Base+0x3a4>  // b.hs, b.nlast
   45414:	ldr	x9, [x8]
   45418:	sub	x10, x9, #0x1
   4541c:	str	x10, [x8], #8
   45420:	cbz	x9, 45414 <__gmpn_toom_interpolate_12pts@@Base+0x394>
   45424:	str	x27, [sp, #48]
   45428:	ldur	x28, [x29, #-8]
   4542c:	ldur	x19, [x29, #-48]
   45430:	mov	x0, x27
   45434:	mov	x3, x21
   45438:	mov	x1, x28
   4543c:	mov	x2, x19
   45440:	bl	ca70 <__gmpn_add_n@plt>
   45444:	mov	x0, x19
   45448:	mov	x1, x19
   4544c:	mov	x2, x28
   45450:	mov	x3, x21
   45454:	bl	c2d0 <__gmpn_sub_n@plt>
   45458:	ldp	x25, x23, [x29, #-40]
   4545c:	mov	x20, x26
   45460:	mov	w3, #0xa                   	// #10
   45464:	mov	x0, x28
   45468:	add	x19, x23, x26
   4546c:	ldur	x26, [x29, #-16]
   45470:	mov	x1, x25
   45474:	mov	x2, x26
   45478:	bl	c180 <__gmpn_lshift@plt>
   4547c:	mov	x27, x0
   45480:	mov	x0, x19
   45484:	mov	x1, x19
   45488:	mov	x2, x28
   4548c:	mov	x3, x26
   45490:	mov	x19, x20
   45494:	bl	c2d0 <__gmpn_sub_n@plt>
   45498:	ldr	x8, [x23, x22]
   4549c:	ldr	x20, [sp, #24]
   454a0:	add	x9, x0, x27
   454a4:	mov	w10, #0x38                  	// #56
   454a8:	sub	x8, x8, x9
   454ac:	madd	x27, x20, x10, x25
   454b0:	str	x19, [sp, #16]
   454b4:	add	x19, x27, x19
   454b8:	str	x8, [x23, x22]
   454bc:	ldr	x8, [x25]
   454c0:	ldr	x9, [x19]
   454c4:	sub	x8, x9, x8, lsr #2
   454c8:	str	x8, [x19]
   454cc:	ldr	x8, [x25]
   454d0:	cmp	x9, x8, lsr #2
   454d4:	b.cs	454f4 <__gmpn_toom_interpolate_12pts@@Base+0x474>  // b.hs, b.nlast
   454d8:	ldur	x8, [x29, #-40]
   454dc:	add	x8, x8, x20, lsl #6
   454e0:	add	x8, x8, #0x8
   454e4:	ldr	x9, [x8]
   454e8:	sub	x10, x9, #0x1
   454ec:	str	x10, [x8], #8
   454f0:	cbz	x9, 454e4 <__gmpn_toom_interpolate_12pts@@Base+0x464>
   454f4:	ldur	x26, [x29, #-8]
   454f8:	ldr	x1, [sp, #8]
   454fc:	mov	w3, #0x3e                  	// #62
   45500:	mov	x2, x24
   45504:	mov	x0, x26
   45508:	bl	c180 <__gmpn_lshift@plt>
   4550c:	mov	x25, x0
   45510:	mov	x0, x19
   45514:	mov	x1, x19
   45518:	mov	x2, x26
   4551c:	mov	x3, x24
   45520:	bl	c2d0 <__gmpn_sub_n@plt>
   45524:	ldur	x8, [x29, #-16]
   45528:	add	x10, x0, x25
   4552c:	add	x8, x19, x8, lsl #3
   45530:	ldur	x9, [x8, #-8]
   45534:	subs	x9, x9, x10
   45538:	stur	x9, [x8, #-8]
   4553c:	b.cs	45550 <__gmpn_toom_interpolate_12pts@@Base+0x4d0>  // b.hs, b.nlast
   45540:	ldr	x9, [x8]
   45544:	sub	x10, x9, #0x1
   45548:	str	x10, [x8], #8
   4554c:	cbz	x9, 45540 <__gmpn_toom_interpolate_12pts@@Base+0x4c0>
   45550:	ldur	x25, [x29, #-8]
   45554:	ldur	x19, [x29, #-32]
   45558:	mov	x2, x27
   4555c:	mov	x3, x21
   45560:	mov	x0, x25
   45564:	mov	x1, x19
   45568:	bl	c2d0 <__gmpn_sub_n@plt>
   4556c:	mov	x0, x27
   45570:	mov	x1, x27
   45574:	mov	x2, x19
   45578:	mov	x3, x21
   4557c:	bl	ca70 <__gmpn_add_n@plt>
   45580:	ldp	x19, x3, [x29, #-24]
   45584:	ldur	x2, [x29, #-40]
   45588:	add	x0, x19, x20, lsl #3
   4558c:	mov	x1, x0
   45590:	mov	x23, x0
   45594:	bl	c2d0 <__gmpn_sub_n@plt>
   45598:	ldr	x8, [x19, x22]
   4559c:	ldur	x28, [x29, #-48]
   455a0:	mov	w3, #0x101                 	// #257
   455a4:	mov	x1, x25
   455a8:	sub	x8, x8, x0
   455ac:	mov	x0, x28
   455b0:	mov	x2, x21
   455b4:	str	x8, [x19, x22]
   455b8:	bl	c9e0 <__gmpn_submul_1@plt>
   455bc:	mov	x4, #0x771b                	// #30491
   455c0:	movk	x4, #0x53e3, lsl #16
   455c4:	movk	x4, #0xc705, lsl #32
   455c8:	mov	w3, #0xb13                 	// #2835
   455cc:	movk	x4, #0x938c, lsl #48
   455d0:	mov	w5, #0x2                   	// #2
   455d4:	mov	x0, x28
   455d8:	mov	x1, x28
   455dc:	mov	x2, x21
   455e0:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   455e4:	ldr	x8, [x28, x22]
   455e8:	lsr	x9, x8, #61
   455ec:	cbz	x9, 455fc <__gmpn_toom_interpolate_12pts@@Base+0x57c>
   455f0:	ldr	x9, [sp, #56]
   455f4:	orr	x8, x8, #0xc000000000000000
   455f8:	str	x8, [x28, x9, lsl #3]
   455fc:	ldur	x25, [x29, #-8]
   45600:	mov	w3, #0x3c                  	// #60
   45604:	mov	x1, x28
   45608:	mov	x2, x21
   4560c:	mov	x0, x25
   45610:	bl	d400 <__gmpn_addmul_1@plt>
   45614:	mov	x3, #0x101010101010101     	// #72340172838076673
   45618:	mov	x0, x25
   4561c:	mov	x1, x25
   45620:	mov	x2, x21
   45624:	mov	x4, xzr
   45628:	bl	c2c0 <__gmpn_bdiv_dbm1c@plt>
   4562c:	ldp	x24, x26, [x29, #-32]
   45630:	mov	w3, #0x5                   	// #5
   45634:	mov	x2, x21
   45638:	mov	x0, x24
   4563c:	mov	x1, x26
   45640:	bl	c180 <__gmpn_lshift@plt>
   45644:	mov	x0, x27
   45648:	mov	x1, x27
   4564c:	mov	x2, x24
   45650:	mov	x3, x21
   45654:	bl	c2d0 <__gmpn_sub_n@plt>
   45658:	ldr	x19, [sp, #48]
   4565c:	mov	w3, #0x64                  	// #100
   45660:	mov	x1, x27
   45664:	mov	x2, x21
   45668:	mov	x0, x19
   4566c:	bl	c9e0 <__gmpn_submul_1@plt>
   45670:	mov	w3, #0x9                   	// #9
   45674:	mov	x0, x24
   45678:	mov	x1, x26
   4567c:	mov	x2, x21
   45680:	bl	c180 <__gmpn_lshift@plt>
   45684:	mov	x0, x19
   45688:	mov	x1, x19
   4568c:	mov	x2, x24
   45690:	mov	x3, x21
   45694:	bl	c2d0 <__gmpn_sub_n@plt>
   45698:	mov	x4, #0x4c35                	// #19509
   4569c:	movk	x4, #0x9f31, lsl #16
   456a0:	movk	x4, #0xd44, lsl #32
   456a4:	mov	w3, #0xa61d                	// #42525
   456a8:	movk	x4, #0xe7b4, lsl #48
   456ac:	mov	x0, x19
   456b0:	mov	x1, x19
   456b4:	mov	x2, x21
   456b8:	mov	w5, wzr
   456bc:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   456c0:	mov	w3, #0xe1                  	// #225
   456c4:	mov	x0, x27
   456c8:	mov	x1, x19
   456cc:	mov	x2, x21
   456d0:	bl	c9e0 <__gmpn_submul_1@plt>
   456d4:	mov	x4, #0x8e39                	// #36409
   456d8:	movk	x4, #0x38e3, lsl #16
   456dc:	movk	x4, #0xe38e, lsl #32
   456e0:	mov	w3, #0x9                   	// #9
   456e4:	movk	x4, #0x8e38, lsl #48
   456e8:	mov	w5, #0x2                   	// #2
   456ec:	mov	x0, x27
   456f0:	mov	x1, x27
   456f4:	mov	x2, x21
   456f8:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   456fc:	mov	x0, x26
   45700:	mov	x1, x26
   45704:	mov	x2, x27
   45708:	mov	x3, x21
   4570c:	bl	c2d0 <__gmpn_sub_n@plt>
   45710:	mov	x0, x28
   45714:	mov	x1, x27
   45718:	mov	x2, x28
   4571c:	mov	x3, x21
   45720:	bl	c840 <__gmpn_rsh1sub_n@plt>
   45724:	ldr	x8, [x28, x22]
   45728:	mov	x0, x27
   4572c:	mov	x1, x27
   45730:	mov	x2, x28
   45734:	and	x8, x8, #0x7fffffffffffffff
   45738:	mov	x3, x21
   4573c:	str	x8, [x28, x22]
   45740:	bl	c2d0 <__gmpn_sub_n@plt>
   45744:	mov	x0, x25
   45748:	mov	x1, x25
   4574c:	mov	x2, x19
   45750:	mov	x3, x21
   45754:	bl	c950 <__gmpn_rsh1add_n@plt>
   45758:	ldr	x8, [x25, x22]
   4575c:	mov	x0, x26
   45760:	mov	x1, x26
   45764:	mov	x2, x19
   45768:	and	x8, x8, #0x7fffffffffffffff
   4576c:	mov	x3, x21
   45770:	str	x8, [x25, x22]
   45774:	bl	c2d0 <__gmpn_sub_n@plt>
   45778:	mov	x0, x19
   4577c:	mov	x1, x19
   45780:	mov	x2, x25
   45784:	mov	x3, x21
   45788:	bl	c2d0 <__gmpn_sub_n@plt>
   4578c:	ldr	x21, [sp, #16]
   45790:	ldur	x26, [x29, #-40]
   45794:	mov	x2, x25
   45798:	mov	x3, x20
   4579c:	add	x0, x26, x21
   457a0:	mov	x1, x0
   457a4:	bl	ca70 <__gmpn_add_n@plt>
   457a8:	ldr	x8, [x25, x21]
   457ac:	ldur	x9, [x29, #-16]
   457b0:	adds	x8, x8, x0
   457b4:	add	x9, x26, x9, lsl #3
   457b8:	str	x8, [x9]
   457bc:	b.cc	458c4 <__gmpn_toom_interpolate_12pts@@Base+0x844>  // b.lo, b.ul, b.last
   457c0:	ldur	x22, [x29, #-48]
   457c4:	ldr	x15, [sp]
   457c8:	mov	x11, xzr
   457cc:	sub	x10, x20, #0x1
   457d0:	mov	w4, #0x1                   	// #1
   457d4:	mov	w8, #0x1                   	// #1
   457d8:	mov	x24, x23
   457dc:	cmp	x8, x20
   457e0:	b.ge	45964 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.tcont
   457e4:	add	x12, x15, x11
   457e8:	ldr	x12, [x12, #8]
   457ec:	add	x13, x9, x11
   457f0:	add	x8, x8, #0x1
   457f4:	add	x11, x11, #0x8
   457f8:	adds	x12, x12, #0x1
   457fc:	sub	x10, x10, #0x1
   45800:	str	x12, [x13, #8]
   45804:	b.cs	457dc <__gmpn_toom_interpolate_12pts@@Base+0x75c>  // b.hs, b.nlast
   45808:	cmp	x15, x9
   4580c:	mov	x4, xzr
   45810:	b.eq	45964 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.none
   45814:	cmp	x8, x20
   45818:	b.ge	45964 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.tcont
   4581c:	sub	x12, x20, x8
   45820:	cmp	x12, #0x4
   45824:	b.cc	45898 <__gmpn_toom_interpolate_12pts@@Base+0x818>  // b.lo, b.ul, b.last
   45828:	ldur	x14, [x29, #-8]
   4582c:	add	x13, x9, x11
   45830:	add	x13, x13, #0x8
   45834:	add	x14, x14, x20, lsl #4
   45838:	cmp	x13, x14
   4583c:	b.cs	45858 <__gmpn_toom_interpolate_12pts@@Base+0x7d8>  // b.hs, b.nlast
   45840:	mov	w13, #0x18                  	// #24
   45844:	add	x14, x15, x11
   45848:	madd	x13, x20, x13, x26
   4584c:	add	x14, x14, #0x8
   45850:	cmp	x14, x13
   45854:	b.cc	45898 <__gmpn_toom_interpolate_12pts@@Base+0x818>  // b.lo, b.ul, b.last
   45858:	add	x13, x9, x11
   4585c:	add	x11, x15, x11
   45860:	and	x9, x12, #0xfffffffffffffffc
   45864:	and	x14, x10, #0xfffffffffffffffc
   45868:	add	x10, x13, #0x18
   4586c:	add	x11, x11, #0x18
   45870:	add	x8, x14, x8
   45874:	mov	x13, x9
   45878:	ldp	q0, q1, [x11, #-16]
   4587c:	add	x11, x11, #0x20
   45880:	subs	x13, x13, #0x4
   45884:	stp	q0, q1, [x10, #-16]
   45888:	add	x10, x10, #0x20
   4588c:	b.ne	45878 <__gmpn_toom_interpolate_12pts@@Base+0x7f8>  // b.any
   45890:	cmp	x12, x9
   45894:	b.eq	45954 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>  // b.none
   45898:	add	x10, x8, x20, lsl #1
   4589c:	sub	x9, x20, x8
   458a0:	add	x11, x8, x20
   458a4:	add	x8, x26, x10, lsl #3
   458a8:	ldur	x10, [x29, #-8]
   458ac:	add	x10, x10, x11, lsl #3
   458b0:	ldr	x11, [x10], #8
   458b4:	subs	x9, x9, #0x1
   458b8:	str	x11, [x8], #8
   458bc:	b.ne	458b0 <__gmpn_toom_interpolate_12pts@@Base+0x830>  // b.any
   458c0:	b	45954 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>
   458c4:	cmp	x20, #0x2
   458c8:	mov	x4, xzr
   458cc:	b.lt	4595c <__gmpn_toom_interpolate_12pts@@Base+0x8dc>  // b.tstop
   458d0:	ldr	x8, [sp]
   458d4:	ldur	x22, [x29, #-48]
   458d8:	mov	x24, x23
   458dc:	cmp	x8, x9
   458e0:	b.eq	45964 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>  // b.none
   458e4:	sub	x8, x20, #0x1
   458e8:	cmp	x8, #0x4
   458ec:	b.cc	45928 <__gmpn_toom_interpolate_12pts@@Base+0x8a8>  // b.lo, b.ul, b.last
   458f0:	ldur	x11, [x29, #-8]
   458f4:	mov	w9, #0x8                   	// #8
   458f8:	lsl	x10, x20, #4
   458fc:	bfi	x9, x20, #4, #60
   45900:	add	x9, x26, x9
   45904:	add	x10, x11, x10
   45908:	cmp	x9, x10
   4590c:	add	x9, x11, x20, lsl #3
   45910:	b.cs	45f78 <__gmpn_toom_interpolate_12pts@@Base+0xef8>  // b.hs, b.nlast
   45914:	mov	w10, #0x18                  	// #24
   45918:	madd	x10, x20, x10, x26
   4591c:	add	x11, x9, #0x8
   45920:	cmp	x11, x10
   45924:	b.cs	45f78 <__gmpn_toom_interpolate_12pts@@Base+0xef8>  // b.hs, b.nlast
   45928:	mov	w9, #0x1                   	// #1
   4592c:	add	x10, x9, x20, lsl #1
   45930:	sub	x8, x20, x9
   45934:	add	x11, x9, x20
   45938:	add	x9, x26, x10, lsl #3
   4593c:	ldur	x10, [x29, #-8]
   45940:	add	x10, x10, x11, lsl #3
   45944:	ldr	x11, [x10], #8
   45948:	subs	x8, x8, #0x1
   4594c:	str	x11, [x9], #8
   45950:	b.ne	45944 <__gmpn_toom_interpolate_12pts@@Base+0x8c4>  // b.any
   45954:	mov	x4, xzr
   45958:	b	45964 <__gmpn_toom_interpolate_12pts@@Base+0x8e4>
   4595c:	ldur	x22, [x29, #-48]
   45960:	mov	x24, x23
   45964:	ldr	x23, [sp, #56]
   45968:	ldp	x8, x9, [x29, #-16]
   4596c:	mov	x0, x22
   45970:	mov	x1, x22
   45974:	mov	x3, x20
   45978:	ldr	x19, [x9, x23, lsl #3]
   4597c:	add	x2, x9, x8, lsl #3
   45980:	bl	ce90 <__gmpn_add_nc@plt>
   45984:	ldr	x8, [x22, x21]
   45988:	add	x9, x0, x19
   4598c:	adds	x8, x8, x9
   45990:	str	x8, [x22, x21]
   45994:	b.cc	459b0 <__gmpn_toom_interpolate_12pts@@Base+0x930>  // b.lo, b.ul, b.last
   45998:	add	x8, x26, x20, lsl #5
   4599c:	add	x8, x8, #0x8
   459a0:	ldr	x9, [x8]
   459a4:	adds	x9, x9, #0x1
   459a8:	str	x9, [x8], #8
   459ac:	b.cs	459a0 <__gmpn_toom_interpolate_12pts@@Base+0x920>  // b.hs, b.nlast
   459b0:	ldur	x19, [x29, #-24]
   459b4:	mov	w8, #0x28                  	// #40
   459b8:	madd	x0, x20, x8, x26
   459bc:	mov	x1, x0
   459c0:	mov	x2, x19
   459c4:	mov	x3, x20
   459c8:	bl	ca70 <__gmpn_add_n@plt>
   459cc:	add	x8, x20, x20, lsl #1
   459d0:	add	x10, x26, x8, lsl #4
   459d4:	ldr	x9, [x10]
   459d8:	ldr	x22, [sp, #40]
   459dc:	lsl	x8, x8, #1
   459e0:	add	x9, x9, x0
   459e4:	str	x9, [x10]
   459e8:	ldr	x11, [x19, x20, lsl #3]
   459ec:	adds	x9, x11, x9
   459f0:	str	x9, [x10]
   459f4:	b.cc	45ae8 <__gmpn_toom_interpolate_12pts@@Base+0xa68>  // b.lo, b.ul, b.last
   459f8:	mov	x12, xzr
   459fc:	sub	x11, x20, #0x1
   45a00:	mov	w4, #0x1                   	// #1
   45a04:	mov	w9, #0x1                   	// #1
   45a08:	cmp	x9, x20
   45a0c:	b.ge	45b64 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tcont
   45a10:	add	x13, x24, x12
   45a14:	ldr	x13, [x13, #8]
   45a18:	add	x14, x10, x12
   45a1c:	add	x9, x9, #0x1
   45a20:	add	x12, x12, #0x8
   45a24:	adds	x13, x13, #0x1
   45a28:	sub	x11, x11, #0x1
   45a2c:	str	x13, [x14, #8]
   45a30:	b.cs	45a08 <__gmpn_toom_interpolate_12pts@@Base+0x988>  // b.hs, b.nlast
   45a34:	cmp	x24, x10
   45a38:	mov	x4, xzr
   45a3c:	b.eq	45b64 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.none
   45a40:	cmp	x9, x20
   45a44:	b.ge	45b64 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tcont
   45a48:	sub	x13, x20, x9
   45a4c:	cmp	x13, #0x4
   45a50:	b.cc	45abc <__gmpn_toom_interpolate_12pts@@Base+0xa3c>  // b.lo, b.ul, b.last
   45a54:	ldur	x15, [x29, #-24]
   45a58:	add	x14, x10, x12
   45a5c:	add	x14, x14, #0x8
   45a60:	add	x15, x15, x20, lsl #4
   45a64:	cmp	x14, x15
   45a68:	b.cs	45a7c <__gmpn_toom_interpolate_12pts@@Base+0x9fc>  // b.hs, b.nlast
   45a6c:	add	x14, x24, x12
   45a70:	add	x14, x14, #0x8
   45a74:	cmp	x14, x27
   45a78:	b.cc	45abc <__gmpn_toom_interpolate_12pts@@Base+0xa3c>  // b.lo, b.ul, b.last
   45a7c:	add	x14, x10, x12
   45a80:	add	x12, x24, x12
   45a84:	and	x10, x13, #0xfffffffffffffffc
   45a88:	and	x15, x11, #0xfffffffffffffffc
   45a8c:	add	x11, x14, #0x18
   45a90:	add	x12, x12, #0x18
   45a94:	add	x9, x15, x9
   45a98:	mov	x14, x10
   45a9c:	ldp	q0, q1, [x12, #-16]
   45aa0:	add	x12, x12, #0x20
   45aa4:	subs	x14, x14, #0x4
   45aa8:	stp	q0, q1, [x11, #-16]
   45aac:	add	x11, x11, #0x20
   45ab0:	b.ne	45a9c <__gmpn_toom_interpolate_12pts@@Base+0xa1c>  // b.any
   45ab4:	cmp	x13, x10
   45ab8:	b.eq	45b60 <__gmpn_toom_interpolate_12pts@@Base+0xae0>  // b.none
   45abc:	ldur	x11, [x29, #-24]
   45ac0:	sub	x10, x20, x9
   45ac4:	add	x8, x9, x8
   45ac8:	add	x9, x9, x20
   45acc:	add	x8, x26, x8, lsl #3
   45ad0:	add	x9, x11, x9, lsl #3
   45ad4:	ldr	x11, [x9], #8
   45ad8:	subs	x10, x10, #0x1
   45adc:	str	x11, [x8], #8
   45ae0:	b.ne	45ad4 <__gmpn_toom_interpolate_12pts@@Base+0xa54>  // b.any
   45ae4:	b	45b60 <__gmpn_toom_interpolate_12pts@@Base+0xae0>
   45ae8:	cmp	x20, #0x2
   45aec:	mov	x4, xzr
   45af0:	b.lt	45b64 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.tstop
   45af4:	cmp	x24, x10
   45af8:	b.eq	45b64 <__gmpn_toom_interpolate_12pts@@Base+0xae4>  // b.none
   45afc:	sub	x9, x20, #0x1
   45b00:	cmp	x9, #0x4
   45b04:	b.cc	45b34 <__gmpn_toom_interpolate_12pts@@Base+0xab4>  // b.lo, b.ul, b.last
   45b08:	ldur	x12, [x29, #-24]
   45b0c:	lsl	x10, x8, #3
   45b10:	orr	x10, x10, #0x8
   45b14:	add	x10, x26, x10
   45b18:	add	x11, x12, x20, lsl #4
   45b1c:	cmp	x10, x11
   45b20:	add	x10, x12, x20, lsl #3
   45b24:	b.cs	45fb4 <__gmpn_toom_interpolate_12pts@@Base+0xf34>  // b.hs, b.nlast
   45b28:	add	x11, x10, #0x8
   45b2c:	cmp	x11, x27
   45b30:	b.cs	45fb4 <__gmpn_toom_interpolate_12pts@@Base+0xf34>  // b.hs, b.nlast
   45b34:	mov	w10, #0x1                   	// #1
   45b38:	ldur	x11, [x29, #-24]
   45b3c:	sub	x9, x20, x10
   45b40:	add	x8, x10, x8
   45b44:	add	x10, x10, x20
   45b48:	add	x8, x26, x8, lsl #3
   45b4c:	add	x10, x11, x10, lsl #3
   45b50:	ldr	x11, [x10], #8
   45b54:	subs	x9, x9, #0x1
   45b58:	str	x11, [x8], #8
   45b5c:	b.ne	45b50 <__gmpn_toom_interpolate_12pts@@Base+0xad0>  // b.any
   45b60:	mov	x4, xzr
   45b64:	ldp	x9, x8, [x29, #-24]
   45b68:	mov	x0, x27
   45b6c:	mov	x1, x27
   45b70:	mov	x3, x20
   45b74:	ldr	x19, [x9, x23, lsl #3]
   45b78:	add	x2, x9, x8, lsl #3
   45b7c:	bl	ce90 <__gmpn_add_nc@plt>
   45b80:	lsl	x8, x20, #6
   45b84:	ldr	x9, [x26, x8]
   45b88:	add	x10, x0, x19
   45b8c:	adds	x9, x9, x10
   45b90:	str	x9, [x26, x8]
   45b94:	b.cc	45bb0 <__gmpn_toom_interpolate_12pts@@Base+0xb30>  // b.lo, b.ul, b.last
   45b98:	add	x8, x26, x21, lsl #3
   45b9c:	add	x8, x8, #0x8
   45ba0:	ldr	x9, [x8]
   45ba4:	adds	x9, x9, #0x1
   45ba8:	str	x9, [x8], #8
   45bac:	b.cs	45ba0 <__gmpn_toom_interpolate_12pts@@Base+0xb20>  // b.hs, b.nlast
   45bb0:	ldr	x21, [sp, #48]
   45bb4:	mov	w8, #0x48                  	// #72
   45bb8:	madd	x0, x20, x8, x26
   45bbc:	mov	x1, x0
   45bc0:	mov	x2, x21
   45bc4:	mov	x3, x20
   45bc8:	bl	ca70 <__gmpn_add_n@plt>
   45bcc:	mov	w8, #0x50                  	// #80
   45bd0:	madd	x9, x20, x8, x26
   45bd4:	ldr	x8, [x9]
   45bd8:	ldr	w12, [sp, #36]
   45bdc:	add	x10, x8, x0
   45be0:	str	x10, [x9]
   45be4:	add	x8, x21, x20, lsl #3
   45be8:	ldr	x11, [x8]
   45bec:	add	x11, x11, x10
   45bf0:	str	x11, [x9]
   45bf4:	cbz	w12, 45cf4 <__gmpn_toom_interpolate_12pts@@Base+0xc74>
   45bf8:	cmp	x11, x10
   45bfc:	b.cs	45df0 <__gmpn_toom_interpolate_12pts@@Base+0xd70>  // b.hs, b.nlast
   45c00:	mov	x12, xzr
   45c04:	sub	x11, x20, #0x1
   45c08:	mov	w4, #0x1                   	// #1
   45c0c:	mov	w10, #0x1                   	// #1
   45c10:	cmp	x10, x20
   45c14:	b.ge	45e74 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tcont
   45c18:	add	x13, x8, x12
   45c1c:	ldr	x13, [x13, #8]
   45c20:	add	x14, x9, x12
   45c24:	add	x10, x10, #0x1
   45c28:	add	x12, x12, #0x8
   45c2c:	adds	x13, x13, #0x1
   45c30:	sub	x11, x11, #0x1
   45c34:	str	x13, [x14, #8]
   45c38:	b.cs	45c10 <__gmpn_toom_interpolate_12pts@@Base+0xb90>  // b.hs, b.nlast
   45c3c:	cmp	x8, x9
   45c40:	mov	x4, xzr
   45c44:	b.eq	45e74 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.none
   45c48:	cmp	x10, x20
   45c4c:	b.ge	45e74 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tcont
   45c50:	sub	x13, x20, x10
   45c54:	cmp	x13, #0x4
   45c58:	b.cc	45cc8 <__gmpn_toom_interpolate_12pts@@Base+0xc48>  // b.lo, b.ul, b.last
   45c5c:	add	x14, x9, x12
   45c60:	add	x14, x14, #0x8
   45c64:	add	x15, x21, x20, lsl #4
   45c68:	cmp	x14, x15
   45c6c:	b.cs	45c88 <__gmpn_toom_interpolate_12pts@@Base+0xc08>  // b.hs, b.nlast
   45c70:	mov	w14, #0x58                  	// #88
   45c74:	add	x15, x8, x12
   45c78:	madd	x14, x20, x14, x26
   45c7c:	add	x15, x15, #0x8
   45c80:	cmp	x15, x14
   45c84:	b.cc	45cc8 <__gmpn_toom_interpolate_12pts@@Base+0xc48>  // b.lo, b.ul, b.last
   45c88:	add	x9, x9, x12
   45c8c:	add	x12, x8, x12
   45c90:	and	x8, x13, #0xfffffffffffffffc
   45c94:	and	x14, x11, #0xfffffffffffffffc
   45c98:	add	x9, x9, #0x18
   45c9c:	add	x11, x12, #0x18
   45ca0:	add	x10, x14, x10
   45ca4:	mov	x12, x8
   45ca8:	ldp	q0, q1, [x11, #-16]
   45cac:	add	x11, x11, #0x20
   45cb0:	subs	x12, x12, #0x4
   45cb4:	stp	q0, q1, [x9, #-16]
   45cb8:	add	x9, x9, #0x20
   45cbc:	b.ne	45ca8 <__gmpn_toom_interpolate_12pts@@Base+0xc28>  // b.any
   45cc0:	cmp	x13, x8
   45cc4:	b.eq	45e70 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>  // b.none
   45cc8:	mov	w9, #0xa                   	// #10
   45ccc:	add	x11, x10, x20
   45cd0:	madd	x9, x20, x9, x10
   45cd4:	sub	x8, x20, x10
   45cd8:	add	x9, x26, x9, lsl #3
   45cdc:	add	x10, x21, x11, lsl #3
   45ce0:	ldr	x11, [x10], #8
   45ce4:	subs	x8, x8, #0x1
   45ce8:	str	x11, [x9], #8
   45cec:	b.ne	45ce0 <__gmpn_toom_interpolate_12pts@@Base+0xc60>  // b.any
   45cf0:	b	45e70 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>
   45cf4:	cmp	x11, x10
   45cf8:	b.cs	45ed8 <__gmpn_toom_interpolate_12pts@@Base+0xe58>  // b.hs, b.nlast
   45cfc:	mov	x12, xzr
   45d00:	sub	x11, x22, #0x1
   45d04:	mov	w10, #0x1                   	// #1
   45d08:	cmp	x10, x22
   45d0c:	b.ge	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tcont
   45d10:	add	x13, x8, x12
   45d14:	ldr	x13, [x13, #8]
   45d18:	add	x14, x9, x12
   45d1c:	add	x10, x10, #0x1
   45d20:	add	x12, x12, #0x8
   45d24:	adds	x13, x13, #0x1
   45d28:	sub	x11, x11, #0x1
   45d2c:	str	x13, [x14, #8]
   45d30:	b.cs	45d08 <__gmpn_toom_interpolate_12pts@@Base+0xc88>  // b.hs, b.nlast
   45d34:	cmp	x8, x9
   45d38:	b.eq	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   45d3c:	cmp	x10, x22
   45d40:	b.ge	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tcont
   45d44:	sub	x13, x22, x10
   45d48:	cmp	x13, #0x4
   45d4c:	b.cc	45dc4 <__gmpn_toom_interpolate_12pts@@Base+0xd44>  // b.lo, b.ul, b.last
   45d50:	add	x14, x9, x12
   45d54:	add	x15, x22, x20
   45d58:	add	x14, x14, #0x8
   45d5c:	add	x15, x21, x15, lsl #3
   45d60:	cmp	x14, x15
   45d64:	b.cs	45d84 <__gmpn_toom_interpolate_12pts@@Base+0xd04>  // b.hs, b.nlast
   45d68:	mov	w14, #0xa                   	// #10
   45d6c:	add	x15, x8, x12
   45d70:	madd	x14, x20, x14, x22
   45d74:	add	x14, x26, x14, lsl #3
   45d78:	add	x15, x15, #0x8
   45d7c:	cmp	x15, x14
   45d80:	b.cc	45dc4 <__gmpn_toom_interpolate_12pts@@Base+0xd44>  // b.lo, b.ul, b.last
   45d84:	add	x9, x9, x12
   45d88:	add	x12, x8, x12
   45d8c:	and	x8, x13, #0xfffffffffffffffc
   45d90:	and	x14, x11, #0xfffffffffffffffc
   45d94:	add	x9, x9, #0x18
   45d98:	add	x11, x12, #0x18
   45d9c:	add	x10, x14, x10
   45da0:	mov	x12, x8
   45da4:	ldp	q0, q1, [x11, #-16]
   45da8:	add	x11, x11, #0x20
   45dac:	subs	x12, x12, #0x4
   45db0:	stp	q0, q1, [x9, #-16]
   45db4:	add	x9, x9, #0x20
   45db8:	b.ne	45da4 <__gmpn_toom_interpolate_12pts@@Base+0xd24>  // b.any
   45dbc:	cmp	x13, x8
   45dc0:	b.eq	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   45dc4:	mov	w9, #0xa                   	// #10
   45dc8:	add	x11, x10, x20
   45dcc:	madd	x9, x20, x9, x10
   45dd0:	sub	x8, x22, x10
   45dd4:	add	x9, x26, x9, lsl #3
   45dd8:	add	x10, x21, x11, lsl #3
   45ddc:	ldr	x11, [x10], #8
   45de0:	subs	x8, x8, #0x1
   45de4:	str	x11, [x9], #8
   45de8:	b.ne	45ddc <__gmpn_toom_interpolate_12pts@@Base+0xd5c>  // b.any
   45dec:	b	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>
   45df0:	cmp	x20, #0x2
   45df4:	mov	x4, xzr
   45df8:	b.lt	45e74 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.tstop
   45dfc:	cmp	x8, x9
   45e00:	b.eq	45e74 <__gmpn_toom_interpolate_12pts@@Base+0xdf4>  // b.none
   45e04:	sub	x8, x20, #0x1
   45e08:	cmp	x8, #0x4
   45e0c:	b.cc	45e44 <__gmpn_toom_interpolate_12pts@@Base+0xdc4>  // b.lo, b.ul, b.last
   45e10:	add	x9, x20, x20, lsl #2
   45e14:	mov	w10, #0x8                   	// #8
   45e18:	bfi	x10, x9, #4, #60
   45e1c:	add	x9, x26, x10
   45e20:	add	x10, x21, x20, lsl #4
   45e24:	cmp	x9, x10
   45e28:	add	x9, x21, x20, lsl #3
   45e2c:	b.cs	45ff0 <__gmpn_toom_interpolate_12pts@@Base+0xf70>  // b.hs, b.nlast
   45e30:	mov	w10, #0x58                  	// #88
   45e34:	madd	x10, x20, x10, x26
   45e38:	add	x11, x9, #0x8
   45e3c:	cmp	x11, x10
   45e40:	b.cs	45ff0 <__gmpn_toom_interpolate_12pts@@Base+0xf70>  // b.hs, b.nlast
   45e44:	mov	w9, #0x1                   	// #1
   45e48:	mov	w10, #0xa                   	// #10
   45e4c:	sub	x8, x20, x9
   45e50:	add	x11, x9, x20
   45e54:	madd	x9, x20, x10, x9
   45e58:	add	x9, x26, x9, lsl #3
   45e5c:	add	x10, x21, x11, lsl #3
   45e60:	ldr	x11, [x10], #8
   45e64:	subs	x8, x8, #0x1
   45e68:	str	x11, [x9], #8
   45e6c:	b.ne	45e60 <__gmpn_toom_interpolate_12pts@@Base+0xde0>  // b.any
   45e70:	mov	x4, xzr
   45e74:	cmp	x22, x20
   45e78:	b.le	46070 <__gmpn_toom_interpolate_12pts@@Base+0xff0>
   45e7c:	mov	w8, #0x58                  	// #88
   45e80:	madd	x0, x20, x8, x26
   45e84:	ldur	x8, [x29, #-16]
   45e88:	ldr	x19, [x21, x23, lsl #3]
   45e8c:	mov	x1, x0
   45e90:	mov	x3, x20
   45e94:	add	x2, x21, x8, lsl #3
   45e98:	bl	ce90 <__gmpn_add_nc@plt>
   45e9c:	add	x8, x20, x20, lsl #1
   45ea0:	lsl	x9, x8, #5
   45ea4:	ldr	x10, [x26, x9]
   45ea8:	add	x11, x0, x19
   45eac:	adds	x10, x10, x11
   45eb0:	str	x10, [x26, x9]
   45eb4:	b.cc	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.lo, b.ul, b.last
   45eb8:	lsl	x8, x8, #2
   45ebc:	add	x8, x26, x8, lsl #3
   45ec0:	add	x8, x8, #0x8
   45ec4:	ldr	x9, [x8]
   45ec8:	adds	x9, x9, #0x1
   45ecc:	str	x9, [x8], #8
   45ed0:	b.cs	45ec4 <__gmpn_toom_interpolate_12pts@@Base+0xe44>  // b.hs, b.nlast
   45ed4:	b	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>
   45ed8:	cmp	x22, #0x2
   45edc:	b.lt	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.tstop
   45ee0:	cmp	x8, x9
   45ee4:	b.eq	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   45ee8:	sub	x8, x22, #0x1
   45eec:	cmp	x8, #0x4
   45ef0:	b.cc	45f2c <__gmpn_toom_interpolate_12pts@@Base+0xeac>  // b.lo, b.ul, b.last
   45ef4:	add	x10, x20, x20, lsl #2
   45ef8:	mov	w9, #0x8                   	// #8
   45efc:	add	x11, x22, x20
   45f00:	bfi	x9, x10, #4, #60
   45f04:	add	x9, x26, x9
   45f08:	add	x11, x21, x11, lsl #3
   45f0c:	cmp	x9, x11
   45f10:	add	x9, x21, x20, lsl #3
   45f14:	b.cs	46030 <__gmpn_toom_interpolate_12pts@@Base+0xfb0>  // b.hs, b.nlast
   45f18:	add	x10, x22, x10, lsl #1
   45f1c:	add	x10, x26, x10, lsl #3
   45f20:	add	x11, x9, #0x8
   45f24:	cmp	x11, x10
   45f28:	b.cs	46030 <__gmpn_toom_interpolate_12pts@@Base+0xfb0>  // b.hs, b.nlast
   45f2c:	mov	w9, #0x1                   	// #1
   45f30:	mov	w10, #0xa                   	// #10
   45f34:	sub	x8, x22, x9
   45f38:	add	x11, x9, x20
   45f3c:	madd	x9, x20, x10, x9
   45f40:	add	x9, x26, x9, lsl #3
   45f44:	add	x10, x21, x11, lsl #3
   45f48:	ldr	x11, [x10], #8
   45f4c:	subs	x8, x8, #0x1
   45f50:	str	x11, [x9], #8
   45f54:	b.ne	45f48 <__gmpn_toom_interpolate_12pts@@Base+0xec8>  // b.any
   45f58:	ldp	x20, x19, [sp, #192]
   45f5c:	ldp	x22, x21, [sp, #176]
   45f60:	ldp	x24, x23, [sp, #160]
   45f64:	ldp	x26, x25, [sp, #144]
   45f68:	ldp	x28, x27, [sp, #128]
   45f6c:	ldp	x29, x30, [sp, #112]
   45f70:	add	sp, sp, #0xd0
   45f74:	ret
   45f78:	and	x10, x8, #0xfffffffffffffffc
   45f7c:	add	x12, x26, x20, lsl #4
   45f80:	add	x11, x9, #0x18
   45f84:	orr	x9, x10, #0x1
   45f88:	add	x12, x12, #0x18
   45f8c:	mov	x13, x10
   45f90:	ldp	q0, q1, [x11, #-16]
   45f94:	add	x11, x11, #0x20
   45f98:	subs	x13, x13, #0x4
   45f9c:	stp	q0, q1, [x12, #-16]
   45fa0:	add	x12, x12, #0x20
   45fa4:	b.ne	45f90 <__gmpn_toom_interpolate_12pts@@Base+0xf10>  // b.any
   45fa8:	cmp	x8, x10
   45fac:	b.eq	45954 <__gmpn_toom_interpolate_12pts@@Base+0x8d4>  // b.none
   45fb0:	b	4592c <__gmpn_toom_interpolate_12pts@@Base+0x8ac>
   45fb4:	and	x11, x9, #0xfffffffffffffffc
   45fb8:	add	x13, x26, x8, lsl #3
   45fbc:	add	x12, x10, #0x18
   45fc0:	orr	x10, x11, #0x1
   45fc4:	add	x13, x13, #0x18
   45fc8:	mov	x14, x11
   45fcc:	ldp	q0, q1, [x12, #-16]
   45fd0:	add	x12, x12, #0x20
   45fd4:	subs	x14, x14, #0x4
   45fd8:	stp	q0, q1, [x13, #-16]
   45fdc:	add	x13, x13, #0x20
   45fe0:	b.ne	45fcc <__gmpn_toom_interpolate_12pts@@Base+0xf4c>  // b.any
   45fe4:	cmp	x9, x11
   45fe8:	b.eq	45b60 <__gmpn_toom_interpolate_12pts@@Base+0xae0>  // b.none
   45fec:	b	45b38 <__gmpn_toom_interpolate_12pts@@Base+0xab8>
   45ff0:	mov	w12, #0x50                  	// #80
   45ff4:	and	x10, x8, #0xfffffffffffffffc
   45ff8:	madd	x12, x20, x12, x26
   45ffc:	add	x11, x9, #0x18
   46000:	orr	x9, x10, #0x1
   46004:	add	x12, x12, #0x18
   46008:	mov	x13, x10
   4600c:	ldp	q0, q1, [x11, #-16]
   46010:	add	x11, x11, #0x20
   46014:	subs	x13, x13, #0x4
   46018:	stp	q0, q1, [x12, #-16]
   4601c:	add	x12, x12, #0x20
   46020:	b.ne	4600c <__gmpn_toom_interpolate_12pts@@Base+0xf8c>  // b.any
   46024:	cmp	x8, x10
   46028:	b.eq	45e70 <__gmpn_toom_interpolate_12pts@@Base+0xdf0>  // b.none
   4602c:	b	45e48 <__gmpn_toom_interpolate_12pts@@Base+0xdc8>
   46030:	mov	w12, #0x50                  	// #80
   46034:	and	x10, x8, #0xfffffffffffffffc
   46038:	madd	x12, x20, x12, x26
   4603c:	add	x11, x9, #0x18
   46040:	orr	x9, x10, #0x1
   46044:	add	x12, x12, #0x18
   46048:	mov	x13, x10
   4604c:	ldp	q0, q1, [x11, #-16]
   46050:	add	x11, x11, #0x20
   46054:	subs	x13, x13, #0x4
   46058:	stp	q0, q1, [x12, #-16]
   4605c:	add	x12, x12, #0x20
   46060:	b.ne	4604c <__gmpn_toom_interpolate_12pts@@Base+0xfcc>  // b.any
   46064:	cmp	x8, x10
   46068:	b.eq	45f58 <__gmpn_toom_interpolate_12pts@@Base+0xed8>  // b.none
   4606c:	b	45f30 <__gmpn_toom_interpolate_12pts@@Base+0xeb0>
   46070:	mov	w8, #0x58                  	// #88
   46074:	madd	x0, x20, x8, x26
   46078:	ldur	x8, [x29, #-16]
   4607c:	mov	x3, x22
   46080:	ldp	x20, x19, [sp, #192]
   46084:	ldp	x24, x23, [sp, #160]
   46088:	add	x2, x21, x8, lsl #3
   4608c:	ldp	x22, x21, [sp, #176]
   46090:	ldp	x26, x25, [sp, #144]
   46094:	ldp	x28, x27, [sp, #128]
   46098:	ldp	x29, x30, [sp, #112]
   4609c:	mov	x1, x0
   460a0:	add	sp, sp, #0xd0
   460a4:	b	ce90 <__gmpn_add_nc@plt>

00000000000460a8 <__gmpn_toom_interpolate_16pts@@Base>:
   460a8:	sub	sp, sp, #0xe0
   460ac:	stp	x29, x30, [sp, #128]
   460b0:	add	x29, sp, #0x80
   460b4:	stp	x28, x27, [sp, #144]
   460b8:	stp	x26, x25, [sp, #160]
   460bc:	stp	x24, x23, [sp, #176]
   460c0:	stp	x22, x21, [sp, #192]
   460c4:	stp	x20, x19, [sp, #208]
   460c8:	stur	x4, [x29, #-40]
   460cc:	stp	x2, x3, [x29, #-24]
   460d0:	str	x1, [sp, #24]
   460d4:	ldr	x22, [x29, #96]
   460d8:	mov	x19, x5
   460dc:	mov	x24, x0
   460e0:	add	x28, x5, x5, lsl #1
   460e4:	lsl	x27, x5, #3
   460e8:	stur	x0, [x29, #-56]
   460ec:	str	x28, [sp, #64]
   460f0:	str	w7, [sp, #12]
   460f4:	stur	x22, [x29, #-32]
   460f8:	str	x6, [sp, #16]
   460fc:	cbz	w7, 4646c <__gmpn_toom_interpolate_16pts@@Base+0x3c4>
   46100:	mov	w8, #0x38                  	// #56
   46104:	mov	w9, #0x78                  	// #120
   46108:	madd	x20, x19, x8, x24
   4610c:	madd	x21, x19, x9, x24
   46110:	mov	x0, x20
   46114:	mov	x1, x20
   46118:	mov	x2, x21
   4611c:	mov	x3, x6
   46120:	mov	x25, x6
   46124:	bl	c2d0 <__gmpn_sub_n@plt>
   46128:	lsl	x24, x25, #3
   4612c:	ldr	x8, [x20, x24]
   46130:	subs	x8, x8, x0
   46134:	str	x8, [x20, x24]
   46138:	b.cs	46160 <__gmpn_toom_interpolate_16pts@@Base+0xb8>  // b.hs, b.nlast
   4613c:	ldur	x9, [x29, #-56]
   46140:	sub	x8, x27, x19
   46144:	add	x8, x25, x8
   46148:	add	x8, x9, x8, lsl #3
   4614c:	add	x8, x8, #0x8
   46150:	ldr	x9, [x8]
   46154:	sub	x10, x9, #0x1
   46158:	str	x10, [x8], #8
   4615c:	cbz	x9, 46150 <__gmpn_toom_interpolate_16pts@@Base+0xa8>
   46160:	mov	w3, #0xe                   	// #14
   46164:	mov	x0, x22
   46168:	mov	x1, x21
   4616c:	mov	x2, x25
   46170:	bl	c180 <__gmpn_lshift@plt>
   46174:	ldur	x23, [x29, #-24]
   46178:	mov	x20, x0
   4617c:	mov	x2, x22
   46180:	mov	x3, x25
   46184:	mov	x0, x23
   46188:	mov	x1, x23
   4618c:	bl	c2d0 <__gmpn_sub_n@plt>
   46190:	ldr	x8, [x23, x24]
   46194:	add	x9, x0, x20
   46198:	subs	x8, x8, x9
   4619c:	str	x8, [x23, x24]
   461a0:	b.cs	461c0 <__gmpn_toom_interpolate_16pts@@Base+0x118>  // b.hs, b.nlast
   461a4:	ldur	x8, [x29, #-24]
   461a8:	add	x8, x8, x25, lsl #3
   461ac:	add	x8, x8, #0x8
   461b0:	ldr	x9, [x8]
   461b4:	sub	x10, x9, #0x1
   461b8:	str	x10, [x8], #8
   461bc:	cbz	x9, 461b0 <__gmpn_toom_interpolate_16pts@@Base+0x108>
   461c0:	ldur	x8, [x29, #-56]
   461c4:	add	x20, x8, x28, lsl #3
   461c8:	ldr	x8, [x21]
   461cc:	ldr	x9, [x20]
   461d0:	sub	x8, x9, x8, lsr #2
   461d4:	str	x8, [x20]
   461d8:	ldr	x8, [x21]
   461dc:	cmp	x9, x8, lsr #2
   461e0:	b.cs	46204 <__gmpn_toom_interpolate_16pts@@Base+0x15c>  // b.hs, b.nlast
   461e4:	ldur	x9, [x29, #-56]
   461e8:	mov	w8, #0x18                  	// #24
   461ec:	madd	x8, x19, x8, x9
   461f0:	add	x8, x8, #0x8
   461f4:	ldr	x9, [x8]
   461f8:	sub	x10, x9, #0x1
   461fc:	str	x10, [x8], #8
   46200:	cbz	x9, 461f4 <__gmpn_toom_interpolate_16pts@@Base+0x14c>
   46204:	add	x26, x21, #0x8
   46208:	sub	x23, x25, #0x1
   4620c:	mov	w3, #0x3e                  	// #62
   46210:	mov	x0, x22
   46214:	mov	x1, x26
   46218:	mov	x2, x23
   4621c:	bl	c180 <__gmpn_lshift@plt>
   46220:	mov	x28, x0
   46224:	mov	x0, x20
   46228:	mov	x1, x20
   4622c:	mov	x2, x22
   46230:	mov	x3, x23
   46234:	bl	c2d0 <__gmpn_sub_n@plt>
   46238:	add	x8, x20, x25, lsl #3
   4623c:	ldur	x9, [x8, #-8]
   46240:	add	x10, x0, x28
   46244:	subs	x9, x9, x10
   46248:	stur	x9, [x8, #-8]
   4624c:	b.cs	46260 <__gmpn_toom_interpolate_16pts@@Base+0x1b8>  // b.hs, b.nlast
   46250:	ldr	x9, [x8]
   46254:	sub	x10, x9, #0x1
   46258:	str	x10, [x8], #8
   4625c:	cbz	x9, 46250 <__gmpn_toom_interpolate_16pts@@Base+0x1a8>
   46260:	mov	w8, #0xb                   	// #11
   46264:	mul	x9, x19, x8
   46268:	ldur	x8, [x29, #-56]
   4626c:	mov	w3, #0x1c                  	// #28
   46270:	mov	x0, x22
   46274:	mov	x1, x21
   46278:	mov	x2, x25
   4627c:	str	x9, [sp, #48]
   46280:	add	x20, x8, x9, lsl #3
   46284:	bl	c180 <__gmpn_lshift@plt>
   46288:	mov	x28, x0
   4628c:	mov	x0, x20
   46290:	mov	x1, x20
   46294:	mov	x2, x22
   46298:	mov	x3, x25
   4629c:	bl	c2d0 <__gmpn_sub_n@plt>
   462a0:	ldr	x8, [x20, x24]
   462a4:	add	x9, x0, x28
   462a8:	subs	x8, x8, x9
   462ac:	str	x8, [x20, x24]
   462b0:	b.cs	462d8 <__gmpn_toom_interpolate_16pts@@Base+0x230>  // b.hs, b.nlast
   462b4:	ldur	x9, [x29, #-56]
   462b8:	mov	w8, #0xb                   	// #11
   462bc:	madd	x8, x19, x8, x25
   462c0:	add	x8, x9, x8, lsl #3
   462c4:	add	x8, x8, #0x8
   462c8:	ldr	x9, [x8]
   462cc:	sub	x10, x9, #0x1
   462d0:	str	x10, [x8], #8
   462d4:	cbz	x9, 462c8 <__gmpn_toom_interpolate_16pts@@Base+0x220>
   462d8:	ldur	x10, [x29, #-16]
   462dc:	ldr	x8, [x21]
   462e0:	ldr	x28, [sp, #64]
   462e4:	ldr	x9, [x10]
   462e8:	sub	x8, x9, x8, lsr #4
   462ec:	str	x8, [x10]
   462f0:	ldr	x8, [x21]
   462f4:	cmp	x9, x8, lsr #4
   462f8:	b.cs	46314 <__gmpn_toom_interpolate_16pts@@Base+0x26c>  // b.hs, b.nlast
   462fc:	ldur	x8, [x29, #-16]
   46300:	add	x8, x8, #0x8
   46304:	ldr	x9, [x8]
   46308:	sub	x10, x9, #0x1
   4630c:	str	x10, [x8], #8
   46310:	cbz	x9, 46304 <__gmpn_toom_interpolate_16pts@@Base+0x25c>
   46314:	mov	w3, #0x3c                  	// #60
   46318:	mov	x0, x22
   4631c:	mov	x1, x26
   46320:	mov	x2, x23
   46324:	stur	x26, [x29, #-8]
   46328:	bl	c180 <__gmpn_lshift@plt>
   4632c:	ldur	x26, [x29, #-16]
   46330:	mov	x20, x0
   46334:	mov	x2, x22
   46338:	mov	x3, x23
   4633c:	mov	x0, x26
   46340:	mov	x1, x26
   46344:	bl	c2d0 <__gmpn_sub_n@plt>
   46348:	add	x8, x26, x25, lsl #3
   4634c:	ldur	x9, [x8, #-8]
   46350:	add	x10, x0, x20
   46354:	subs	x9, x9, x10
   46358:	stur	x9, [x8, #-8]
   4635c:	b.cs	46370 <__gmpn_toom_interpolate_16pts@@Base+0x2c8>  // b.hs, b.nlast
   46360:	ldr	x9, [x8]
   46364:	sub	x10, x9, #0x1
   46368:	str	x10, [x8], #8
   4636c:	cbz	x9, 46360 <__gmpn_toom_interpolate_16pts@@Base+0x2b8>
   46370:	ldur	x26, [x29, #-32]
   46374:	mov	w3, #0x2a                  	// #42
   46378:	mov	x1, x21
   4637c:	mov	x2, x25
   46380:	mov	x0, x26
   46384:	bl	c180 <__gmpn_lshift@plt>
   46388:	ldr	x22, [sp, #24]
   4638c:	mov	x20, x0
   46390:	mov	x2, x26
   46394:	mov	x3, x25
   46398:	mov	x0, x22
   4639c:	mov	x1, x22
   463a0:	bl	c2d0 <__gmpn_sub_n@plt>
   463a4:	ldr	x8, [x22, x24]
   463a8:	add	x9, x0, x20
   463ac:	subs	x8, x8, x9
   463b0:	str	x8, [x22, x24]
   463b4:	b.cs	463d4 <__gmpn_toom_interpolate_16pts@@Base+0x32c>  // b.hs, b.nlast
   463b8:	ldr	x8, [sp, #24]
   463bc:	add	x8, x8, x25, lsl #3
   463c0:	add	x8, x8, #0x8
   463c4:	ldr	x9, [x8]
   463c8:	sub	x10, x9, #0x1
   463cc:	str	x10, [x8], #8
   463d0:	cbz	x9, 463c4 <__gmpn_toom_interpolate_16pts@@Base+0x31c>
   463d4:	ldur	x10, [x29, #-40]
   463d8:	ldr	x8, [x21]
   463dc:	ldur	x24, [x29, #-56]
   463e0:	ldr	x9, [x10]
   463e4:	sub	x8, x9, x8, lsr #6
   463e8:	str	x8, [x10]
   463ec:	ldr	x8, [x21]
   463f0:	cmp	x9, x8, lsr #6
   463f4:	b.cs	46410 <__gmpn_toom_interpolate_16pts@@Base+0x368>  // b.hs, b.nlast
   463f8:	ldur	x8, [x29, #-40]
   463fc:	add	x8, x8, #0x8
   46400:	ldr	x9, [x8]
   46404:	sub	x10, x9, #0x1
   46408:	str	x10, [x8], #8
   4640c:	cbz	x9, 46400 <__gmpn_toom_interpolate_16pts@@Base+0x358>
   46410:	ldur	x1, [x29, #-8]
   46414:	mov	w3, #0x3a                  	// #58
   46418:	mov	x0, x26
   4641c:	mov	x2, x23
   46420:	bl	c180 <__gmpn_lshift@plt>
   46424:	ldur	x21, [x29, #-40]
   46428:	mov	x20, x0
   4642c:	mov	x2, x26
   46430:	mov	x3, x23
   46434:	mov	x0, x21
   46438:	mov	x1, x21
   4643c:	bl	c2d0 <__gmpn_sub_n@plt>
   46440:	add	x8, x21, x25, lsl #3
   46444:	ldur	x9, [x8, #-8]
   46448:	add	x10, x0, x20
   4644c:	subs	x9, x9, x10
   46450:	stur	x9, [x8, #-8]
   46454:	b.cs	46478 <__gmpn_toom_interpolate_16pts@@Base+0x3d0>  // b.hs, b.nlast
   46458:	ldr	x9, [x8]
   4645c:	sub	x10, x9, #0x1
   46460:	str	x10, [x8], #8
   46464:	cbz	x9, 46458 <__gmpn_toom_interpolate_16pts@@Base+0x3b0>
   46468:	b	46478 <__gmpn_toom_interpolate_16pts@@Base+0x3d0>
   4646c:	mov	w8, #0xb                   	// #11
   46470:	mul	x8, x19, x8
   46474:	str	x8, [sp, #48]
   46478:	ldur	x26, [x29, #-32]
   4647c:	ldur	x23, [x29, #-16]
   46480:	lsl	x25, x19, #1
   46484:	mov	w3, #0x1c                  	// #28
   46488:	mov	x0, x26
   4648c:	mov	x1, x24
   46490:	mov	x2, x25
   46494:	add	x21, x28, #0x1
   46498:	mov	x22, x28
   4649c:	mov	x28, x27
   464a0:	add	x27, x23, x27
   464a4:	bl	c180 <__gmpn_lshift@plt>
   464a8:	mov	x20, x0
   464ac:	mov	x0, x27
   464b0:	mov	x1, x27
   464b4:	mov	x2, x26
   464b8:	mov	x3, x25
   464bc:	str	x27, [sp]
   464c0:	stur	x25, [x29, #-8]
   464c4:	bl	c2d0 <__gmpn_sub_n@plt>
   464c8:	lsl	x25, x22, #3
   464cc:	ldr	x8, [x23, x25]
   464d0:	ldr	x22, [sp, #48]
   464d4:	add	x9, x0, x20
   464d8:	sub	x8, x8, x9
   464dc:	add	x10, x24, x22, lsl #3
   464e0:	add	x27, x10, x28
   464e4:	str	x8, [x23, x25]
   464e8:	ldr	x8, [x24]
   464ec:	ldr	x9, [x27]
   464f0:	str	x10, [sp, #56]
   464f4:	stur	x28, [x29, #-48]
   464f8:	sub	x8, x9, x8, lsr #4
   464fc:	str	x8, [x27]
   46500:	ldr	x8, [x24]
   46504:	cmp	x9, x8, lsr #4
   46508:	b.cs	46528 <__gmpn_toom_interpolate_16pts@@Base+0x480>  // b.hs, b.nlast
   4650c:	add	x8, x22, x19
   46510:	add	x8, x24, x8, lsl #3
   46514:	add	x8, x8, #0x8
   46518:	ldr	x9, [x8]
   4651c:	sub	x10, x9, #0x1
   46520:	str	x10, [x8], #8
   46524:	cbz	x9, 46518 <__gmpn_toom_interpolate_16pts@@Base+0x470>
   46528:	ldur	x20, [x29, #-8]
   4652c:	ldur	x22, [x29, #-32]
   46530:	add	x1, x24, #0x8
   46534:	mov	w3, #0x3c                  	// #60
   46538:	sub	x23, x20, #0x1
   4653c:	mov	x0, x22
   46540:	mov	x2, x23
   46544:	str	x1, [sp, #32]
   46548:	bl	c180 <__gmpn_lshift@plt>
   4654c:	mov	x28, x0
   46550:	mov	x0, x27
   46554:	mov	x1, x27
   46558:	mov	x2, x22
   4655c:	mov	x3, x23
   46560:	bl	c2d0 <__gmpn_sub_n@plt>
   46564:	add	x8, x27, x20, lsl #3
   46568:	ldur	x9, [x8, #-8]
   4656c:	add	x10, x0, x28
   46570:	subs	x9, x9, x10
   46574:	stur	x9, [x8, #-8]
   46578:	b.cs	4658c <__gmpn_toom_interpolate_16pts@@Base+0x4e4>  // b.hs, b.nlast
   4657c:	ldr	x9, [x8]
   46580:	sub	x10, x9, #0x1
   46584:	str	x10, [x8], #8
   46588:	cbz	x9, 4657c <__gmpn_toom_interpolate_16pts@@Base+0x4d4>
   4658c:	ldur	x26, [x29, #-16]
   46590:	ldr	x20, [sp, #56]
   46594:	mov	x0, x22
   46598:	mov	x3, x21
   4659c:	mov	x1, x26
   465a0:	mov	x2, x20
   465a4:	bl	c2d0 <__gmpn_sub_n@plt>
   465a8:	mov	x0, x20
   465ac:	mov	x1, x20
   465b0:	mov	x2, x26
   465b4:	mov	x3, x21
   465b8:	bl	ca70 <__gmpn_add_n@plt>
   465bc:	ldur	x22, [x29, #-8]
   465c0:	ldur	x8, [x29, #-48]
   465c4:	add	x27, x24, x25
   465c8:	mov	w3, #0xe                   	// #14
   465cc:	mov	x0, x26
   465d0:	mov	x1, x24
   465d4:	mov	x2, x22
   465d8:	add	x28, x27, x8
   465dc:	bl	c180 <__gmpn_lshift@plt>
   465e0:	mov	x3, x22
   465e4:	ldur	x22, [x29, #-48]
   465e8:	mov	x20, x24
   465ec:	mov	x24, x0
   465f0:	mov	x0, x28
   465f4:	mov	x1, x28
   465f8:	mov	x2, x26
   465fc:	mov	x26, x27
   46600:	bl	c2d0 <__gmpn_sub_n@plt>
   46604:	ldr	x8, [x27, x25]
   46608:	ldur	x10, [x29, #-24]
   4660c:	add	x9, x0, x24
   46610:	sub	x8, x8, x9
   46614:	add	x10, x10, x22
   46618:	str	x8, [x27, x25]
   4661c:	ldr	x8, [x20]
   46620:	ldr	x9, [x10]
   46624:	str	x10, [sp, #40]
   46628:	sub	x8, x9, x8, lsr #2
   4662c:	str	x8, [x10]
   46630:	ldr	x8, [x20]
   46634:	cmp	x9, x8, lsr #2
   46638:	b.cs	46658 <__gmpn_toom_interpolate_16pts@@Base+0x5b0>  // b.hs, b.nlast
   4663c:	ldur	x8, [x29, #-24]
   46640:	add	x8, x8, x19, lsl #3
   46644:	add	x8, x8, #0x8
   46648:	ldr	x9, [x8]
   4664c:	sub	x10, x9, #0x1
   46650:	str	x10, [x8], #8
   46654:	cbz	x9, 46648 <__gmpn_toom_interpolate_16pts@@Base+0x5a0>
   46658:	ldur	x20, [x29, #-16]
   4665c:	ldr	x1, [sp, #32]
   46660:	mov	w3, #0x3e                  	// #62
   46664:	mov	x2, x23
   46668:	mov	x0, x20
   4666c:	bl	c180 <__gmpn_lshift@plt>
   46670:	ldr	x27, [sp, #40]
   46674:	mov	x24, x0
   46678:	mov	x2, x20
   4667c:	mov	x3, x23
   46680:	mov	x0, x27
   46684:	mov	x1, x27
   46688:	bl	c2d0 <__gmpn_sub_n@plt>
   4668c:	ldur	x8, [x29, #-8]
   46690:	add	x10, x0, x24
   46694:	add	x8, x27, x8, lsl #3
   46698:	ldur	x9, [x8, #-8]
   4669c:	subs	x9, x9, x10
   466a0:	stur	x9, [x8, #-8]
   466a4:	b.cs	466b8 <__gmpn_toom_interpolate_16pts@@Base+0x610>  // b.hs, b.nlast
   466a8:	ldr	x9, [x8]
   466ac:	sub	x10, x9, #0x1
   466b0:	str	x10, [x8], #8
   466b4:	cbz	x9, 466a8 <__gmpn_toom_interpolate_16pts@@Base+0x600>
   466b8:	ldp	x28, x0, [x29, #-24]
   466bc:	mov	x2, x26
   466c0:	mov	x3, x21
   466c4:	mov	x1, x28
   466c8:	bl	ca70 <__gmpn_add_n@plt>
   466cc:	mov	x0, x26
   466d0:	mov	x1, x26
   466d4:	mov	x2, x28
   466d8:	mov	x3, x21
   466dc:	str	x26, [sp, #48]
   466e0:	bl	c2d0 <__gmpn_sub_n@plt>
   466e4:	ldur	x27, [x29, #-40]
   466e8:	ldur	x24, [x29, #-56]
   466ec:	mov	w3, #0x2a                  	// #42
   466f0:	mov	x0, x28
   466f4:	add	x26, x27, x22
   466f8:	ldur	x22, [x29, #-8]
   466fc:	mov	x1, x24
   46700:	mov	x2, x22
   46704:	bl	c180 <__gmpn_lshift@plt>
   46708:	mov	x20, x0
   4670c:	mov	x0, x26
   46710:	mov	x1, x26
   46714:	mov	x2, x28
   46718:	mov	x3, x22
   4671c:	bl	c2d0 <__gmpn_sub_n@plt>
   46720:	ldr	x8, [x27, x25]
   46724:	ldr	x28, [sp, #24]
   46728:	ldur	x10, [x29, #-48]
   4672c:	add	x9, x0, x20
   46730:	sub	x8, x8, x9
   46734:	str	x8, [x27, x25]
   46738:	add	x22, x28, x10
   4673c:	ldr	x8, [x24]
   46740:	ldr	x9, [x22]
   46744:	sub	x8, x9, x8, lsr #6
   46748:	str	x8, [x22]
   4674c:	ldr	x8, [x24]
   46750:	cmp	x9, x8, lsr #6
   46754:	b.cs	46770 <__gmpn_toom_interpolate_16pts@@Base+0x6c8>  // b.hs, b.nlast
   46758:	add	x8, x28, x19, lsl #3
   4675c:	add	x8, x8, #0x8
   46760:	ldr	x9, [x8]
   46764:	sub	x10, x9, #0x1
   46768:	str	x10, [x8], #8
   4676c:	cbz	x9, 46760 <__gmpn_toom_interpolate_16pts@@Base+0x6b8>
   46770:	ldur	x26, [x29, #-24]
   46774:	ldr	x1, [sp, #32]
   46778:	mov	w3, #0x3a                  	// #58
   4677c:	mov	x2, x23
   46780:	mov	x0, x26
   46784:	bl	c180 <__gmpn_lshift@plt>
   46788:	mov	x20, x0
   4678c:	mov	x0, x22
   46790:	mov	x1, x22
   46794:	mov	x2, x26
   46798:	mov	x3, x23
   4679c:	bl	c2d0 <__gmpn_sub_n@plt>
   467a0:	ldur	x8, [x29, #-8]
   467a4:	add	x10, x0, x20
   467a8:	add	x8, x22, x8, lsl #3
   467ac:	ldur	x9, [x8, #-8]
   467b0:	subs	x9, x9, x10
   467b4:	stur	x9, [x8, #-8]
   467b8:	b.cs	467cc <__gmpn_toom_interpolate_16pts@@Base+0x724>  // b.hs, b.nlast
   467bc:	ldr	x9, [x8]
   467c0:	sub	x10, x9, #0x1
   467c4:	str	x10, [x8], #8
   467c8:	cbz	x9, 467bc <__gmpn_toom_interpolate_16pts@@Base+0x714>
   467cc:	ldur	x26, [x29, #-24]
   467d0:	ldur	x20, [x29, #-40]
   467d4:	mov	x2, x28
   467d8:	mov	x3, x21
   467dc:	mov	x0, x26
   467e0:	mov	x1, x20
   467e4:	bl	c2d0 <__gmpn_sub_n@plt>
   467e8:	mov	x0, x28
   467ec:	mov	x1, x28
   467f0:	mov	x2, x20
   467f4:	mov	x3, x21
   467f8:	bl	ca70 <__gmpn_add_n@plt>
   467fc:	mov	w8, #0x38                  	// #56
   46800:	ldur	x3, [x29, #-8]
   46804:	madd	x23, x19, x8, x24
   46808:	add	x0, x23, x19, lsl #3
   4680c:	mov	x1, x0
   46810:	mov	x2, x24
   46814:	bl	c2d0 <__gmpn_sub_n@plt>
   46818:	ldr	x8, [x23, x25]
   4681c:	ldur	x20, [x29, #-32]
   46820:	ldr	x27, [sp, #48]
   46824:	mov	w3, #0x404                 	// #1028
   46828:	sub	x8, x8, x0
   4682c:	mov	x0, x20
   46830:	mov	x1, x27
   46834:	mov	x2, x21
   46838:	str	x8, [x23, x25]
   4683c:	bl	c9e0 <__gmpn_submul_1@plt>
   46840:	mov	w3, #0x514                 	// #1300
   46844:	mov	x0, x26
   46848:	mov	x1, x20
   4684c:	mov	x2, x21
   46850:	bl	c9e0 <__gmpn_submul_1@plt>
   46854:	mov	w3, #0x1010                	// #4112
   46858:	movk	w3, #0x10, lsl #16
   4685c:	mov	x0, x26
   46860:	mov	x1, x27
   46864:	mov	x2, x21
   46868:	bl	c9e0 <__gmpn_submul_1@plt>
   4686c:	mov	x4, #0x275b                	// #10075
   46870:	mov	x3, #0xb0d3                	// #45267
   46874:	movk	x4, #0x6864, lsl #16
   46878:	movk	x3, #0x313f, lsl #16
   4687c:	movk	x4, #0x993a, lsl #32
   46880:	movk	x3, #0xb, lsl #32
   46884:	movk	x4, #0x6db, lsl #48
   46888:	mov	x0, x26
   4688c:	mov	x1, x26
   46890:	mov	x2, x21
   46894:	mov	w5, wzr
   46898:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4689c:	mov	w3, #0xc403                	// #50179
   468a0:	movk	w3, #0xbf, lsl #16
   468a4:	mov	x0, x20
   468a8:	mov	x1, x26
   468ac:	mov	x2, x21
   468b0:	bl	c9e0 <__gmpn_submul_1@plt>
   468b4:	mov	x4, #0x771b                	// #30491
   468b8:	movk	x4, #0x53e3, lsl #16
   468bc:	movk	x4, #0xc705, lsl #32
   468c0:	mov	w3, #0xb13                 	// #2835
   468c4:	movk	x4, #0x938c, lsl #48
   468c8:	mov	w5, #0x6                   	// #6
   468cc:	mov	x0, x20
   468d0:	mov	x1, x20
   468d4:	mov	x2, x21
   468d8:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   468dc:	ldr	x8, [x20, x25]
   468e0:	lsr	x9, x8, #57
   468e4:	cbz	x9, 468f4 <__gmpn_toom_interpolate_16pts@@Base+0x84c>
   468e8:	ldr	x9, [sp, #64]
   468ec:	orr	x8, x8, #0xfc00000000000000
   468f0:	str	x8, [x20, x9, lsl #3]
   468f4:	str	x22, [sp, #32]
   468f8:	ldr	x22, [sp, #48]
   468fc:	ldur	x1, [x29, #-24]
   46900:	mov	w3, #0xfff                 	// #4095
   46904:	mov	x2, x21
   46908:	mov	x0, x22
   4690c:	bl	c9e0 <__gmpn_submul_1@plt>
   46910:	mov	w3, #0xf0                  	// #240
   46914:	mov	x0, x22
   46918:	mov	x1, x20
   4691c:	mov	x2, x21
   46920:	bl	d400 <__gmpn_addmul_1@plt>
   46924:	mov	x4, #0xfefefefefefefefe    	// #-72340172838076674
   46928:	mov	w3, #0xff                  	// #255
   4692c:	movk	x4, #0xfeff
   46930:	mov	w5, #0x2                   	// #2
   46934:	mov	x0, x22
   46938:	mov	x1, x22
   4693c:	mov	x2, x21
   46940:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46944:	ldr	x8, [sp, #64]
   46948:	ldur	x27, [x29, #-48]
   4694c:	ldr	x8, [x22, x8, lsl #3]
   46950:	lsr	x9, x8, #61
   46954:	cbz	x9, 46964 <__gmpn_toom_interpolate_16pts@@Base+0x8bc>
   46958:	ldr	x9, [sp, #64]
   4695c:	orr	x8, x8, #0xc000000000000000
   46960:	str	x8, [x22, x9, lsl #3]
   46964:	ldur	x26, [x29, #-40]
   46968:	mov	w3, #0x7                   	// #7
   4696c:	mov	x1, x23
   46970:	mov	x2, x21
   46974:	mov	x0, x26
   46978:	bl	c180 <__gmpn_lshift@plt>
   4697c:	ldur	x20, [x29, #-16]
   46980:	mov	x2, x26
   46984:	mov	x3, x21
   46988:	mov	x0, x20
   4698c:	mov	x1, x20
   46990:	bl	c2d0 <__gmpn_sub_n@plt>
   46994:	mov	w3, #0xd                   	// #13
   46998:	mov	x0, x26
   4699c:	mov	x1, x23
   469a0:	mov	x2, x21
   469a4:	bl	c180 <__gmpn_lshift@plt>
   469a8:	ldr	x22, [sp, #56]
   469ac:	mov	x2, x26
   469b0:	mov	x3, x21
   469b4:	mov	x0, x22
   469b8:	mov	x1, x22
   469bc:	bl	c2d0 <__gmpn_sub_n@plt>
   469c0:	mov	w3, #0x190                 	// #400
   469c4:	mov	x0, x22
   469c8:	mov	x1, x20
   469cc:	mov	x2, x21
   469d0:	bl	c9e0 <__gmpn_submul_1@plt>
   469d4:	mov	w3, #0x13                  	// #19
   469d8:	mov	x0, x26
   469dc:	mov	x1, x23
   469e0:	mov	x2, x21
   469e4:	bl	c180 <__gmpn_lshift@plt>
   469e8:	mov	x2, x26
   469ec:	ldr	x26, [sp, #48]
   469f0:	mov	x0, x28
   469f4:	mov	x1, x28
   469f8:	mov	x3, x21
   469fc:	bl	c2d0 <__gmpn_sub_n@plt>
   46a00:	mov	w3, #0x594                 	// #1428
   46a04:	mov	x0, x28
   46a08:	mov	x1, x22
   46a0c:	mov	x2, x21
   46a10:	bl	c9e0 <__gmpn_submul_1@plt>
   46a14:	mov	w3, #0xb900                	// #47360
   46a18:	movk	w3, #0x1, lsl #16
   46a1c:	mov	x0, x28
   46a20:	mov	x1, x20
   46a24:	mov	x2, x21
   46a28:	bl	c9e0 <__gmpn_submul_1@plt>
   46a2c:	mov	x4, #0xcb25                	// #52005
   46a30:	mov	x3, #0x58ad                	// #22701
   46a34:	movk	x4, #0x6fc4, lsl #16
   46a38:	movk	x3, #0xd916, lsl #16
   46a3c:	movk	x4, #0x9a07, lsl #32
   46a40:	movk	x3, #0xa, lsl #32
   46a44:	movk	x4, #0x1b64, lsl #48
   46a48:	mov	x0, x28
   46a4c:	mov	x1, x28
   46a50:	mov	x2, x21
   46a54:	mov	w5, wzr
   46a58:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46a5c:	mov	w3, #0xa671                	// #42609
   46a60:	movk	w3, #0xe7, lsl #16
   46a64:	mov	x0, x22
   46a68:	mov	x1, x28
   46a6c:	mov	x2, x21
   46a70:	bl	c9e0 <__gmpn_submul_1@plt>
   46a74:	mov	x4, #0x4c35                	// #19509
   46a78:	movk	x4, #0x9f31, lsl #16
   46a7c:	movk	x4, #0xd44, lsl #32
   46a80:	mov	w3, #0xa61d                	// #42525
   46a84:	movk	x4, #0xe7b4, lsl #48
   46a88:	mov	w5, #0x4                   	// #4
   46a8c:	mov	x0, x22
   46a90:	mov	x1, x22
   46a94:	mov	x2, x21
   46a98:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46a9c:	mov	w3, #0xf81                 	// #3969
   46aa0:	mov	x0, x20
   46aa4:	mov	x1, x28
   46aa8:	mov	x2, x21
   46aac:	bl	c9e0 <__gmpn_submul_1@plt>
   46ab0:	mov	w3, #0x384                 	// #900
   46ab4:	mov	x0, x20
   46ab8:	mov	x1, x22
   46abc:	mov	x2, x21
   46ac0:	bl	c9e0 <__gmpn_submul_1@plt>
   46ac4:	mov	x4, #0x8e39                	// #36409
   46ac8:	movk	x4, #0x38e3, lsl #16
   46acc:	movk	x4, #0xe38e, lsl #32
   46ad0:	mov	w3, #0x9                   	// #9
   46ad4:	movk	x4, #0x8e38, lsl #48
   46ad8:	mov	w5, #0x4                   	// #4
   46adc:	mov	x0, x20
   46ae0:	mov	x1, x20
   46ae4:	mov	x2, x21
   46ae8:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   46aec:	mov	x0, x23
   46af0:	mov	x1, x23
   46af4:	mov	x2, x28
   46af8:	mov	x3, x21
   46afc:	bl	c2d0 <__gmpn_sub_n@plt>
   46b00:	mov	x0, x23
   46b04:	mov	x1, x23
   46b08:	mov	x2, x20
   46b0c:	mov	x3, x21
   46b10:	bl	c2d0 <__gmpn_sub_n@plt>
   46b14:	mov	x0, x23
   46b18:	mov	x1, x23
   46b1c:	mov	x2, x22
   46b20:	mov	x3, x21
   46b24:	bl	c2d0 <__gmpn_sub_n@plt>
   46b28:	mov	x0, x26
   46b2c:	mov	x1, x22
   46b30:	mov	x2, x26
   46b34:	mov	x3, x21
   46b38:	bl	c950 <__gmpn_rsh1add_n@plt>
   46b3c:	ldr	x8, [x26, x25]
   46b40:	mov	x0, x22
   46b44:	mov	x1, x22
   46b48:	mov	x2, x26
   46b4c:	and	x8, x8, #0x7fffffffffffffff
   46b50:	mov	x3, x21
   46b54:	str	x8, [x26, x25]
   46b58:	bl	c2d0 <__gmpn_sub_n@plt>
   46b5c:	ldur	x22, [x29, #-32]
   46b60:	mov	x1, x20
   46b64:	mov	x3, x21
   46b68:	mov	x0, x22
   46b6c:	mov	x2, x22
   46b70:	bl	c840 <__gmpn_rsh1sub_n@plt>
   46b74:	ldr	x8, [x22, x25]
   46b78:	mov	x0, x20
   46b7c:	mov	x1, x20
   46b80:	mov	x2, x22
   46b84:	and	x8, x8, #0x7fffffffffffffff
   46b88:	mov	x3, x21
   46b8c:	str	x8, [x22, x25]
   46b90:	bl	c2d0 <__gmpn_sub_n@plt>
   46b94:	ldur	x20, [x29, #-24]
   46b98:	mov	x1, x28
   46b9c:	mov	x3, x21
   46ba0:	mov	x0, x20
   46ba4:	mov	x2, x20
   46ba8:	bl	c950 <__gmpn_rsh1add_n@plt>
   46bac:	ldr	x8, [x20, x25]
   46bb0:	mov	x0, x28
   46bb4:	mov	x1, x28
   46bb8:	mov	x2, x20
   46bbc:	and	x8, x8, #0x7fffffffffffffff
   46bc0:	mov	x3, x21
   46bc4:	str	x8, [x20, x25]
   46bc8:	bl	c2d0 <__gmpn_sub_n@plt>
   46bcc:	add	x0, x24, x27
   46bd0:	mov	x1, x0
   46bd4:	mov	x2, x20
   46bd8:	mov	x3, x19
   46bdc:	bl	ca70 <__gmpn_add_n@plt>
   46be0:	ldr	x8, [x20, x27]
   46be4:	ldur	x9, [x29, #-8]
   46be8:	adds	x8, x8, x0
   46bec:	add	x9, x24, x9, lsl #3
   46bf0:	str	x8, [x9]
   46bf4:	b.cc	46cf8 <__gmpn_toom_interpolate_16pts@@Base+0xc50>  // b.lo, b.ul, b.last
   46bf8:	ldr	x21, [sp, #16]
   46bfc:	ldp	x22, x15, [sp, #32]
   46c00:	mov	x11, xzr
   46c04:	sub	x10, x19, #0x1
   46c08:	mov	w4, #0x1                   	// #1
   46c0c:	mov	w8, #0x1                   	// #1
   46c10:	cmp	x8, x19
   46c14:	b.ge	46d94 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.tcont
   46c18:	add	x12, x15, x11
   46c1c:	ldr	x12, [x12, #8]
   46c20:	add	x13, x9, x11
   46c24:	add	x8, x8, #0x1
   46c28:	add	x11, x11, #0x8
   46c2c:	adds	x12, x12, #0x1
   46c30:	sub	x10, x10, #0x1
   46c34:	str	x12, [x13, #8]
   46c38:	b.cs	46c10 <__gmpn_toom_interpolate_16pts@@Base+0xb68>  // b.hs, b.nlast
   46c3c:	cmp	x15, x9
   46c40:	mov	x4, xzr
   46c44:	b.eq	46d94 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.none
   46c48:	cmp	x8, x19
   46c4c:	b.ge	46d94 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.tcont
   46c50:	sub	x12, x19, x8
   46c54:	cmp	x12, #0x4
   46c58:	b.cc	46ccc <__gmpn_toom_interpolate_16pts@@Base+0xc24>  // b.lo, b.ul, b.last
   46c5c:	ldur	x14, [x29, #-24]
   46c60:	add	x13, x9, x11
   46c64:	add	x13, x13, #0x8
   46c68:	add	x14, x14, x19, lsl #4
   46c6c:	cmp	x13, x14
   46c70:	b.cs	46c8c <__gmpn_toom_interpolate_16pts@@Base+0xbe4>  // b.hs, b.nlast
   46c74:	mov	w13, #0x18                  	// #24
   46c78:	add	x14, x15, x11
   46c7c:	madd	x13, x19, x13, x24
   46c80:	add	x14, x14, #0x8
   46c84:	cmp	x14, x13
   46c88:	b.cc	46ccc <__gmpn_toom_interpolate_16pts@@Base+0xc24>  // b.lo, b.ul, b.last
   46c8c:	add	x13, x9, x11
   46c90:	add	x11, x15, x11
   46c94:	and	x9, x12, #0xfffffffffffffffc
   46c98:	and	x14, x10, #0xfffffffffffffffc
   46c9c:	add	x10, x13, #0x18
   46ca0:	add	x11, x11, #0x18
   46ca4:	add	x8, x14, x8
   46ca8:	mov	x13, x9
   46cac:	ldp	q0, q1, [x11, #-16]
   46cb0:	add	x11, x11, #0x20
   46cb4:	subs	x13, x13, #0x4
   46cb8:	stp	q0, q1, [x10, #-16]
   46cbc:	add	x10, x10, #0x20
   46cc0:	b.ne	46cac <__gmpn_toom_interpolate_16pts@@Base+0xc04>  // b.any
   46cc4:	cmp	x12, x9
   46cc8:	b.eq	46d88 <__gmpn_toom_interpolate_16pts@@Base+0xce0>  // b.none
   46ccc:	add	x10, x8, x19, lsl #1
   46cd0:	sub	x9, x19, x8
   46cd4:	add	x11, x8, x19
   46cd8:	add	x8, x24, x10, lsl #3
   46cdc:	ldur	x10, [x29, #-24]
   46ce0:	add	x10, x10, x11, lsl #3
   46ce4:	ldr	x11, [x10], #8
   46ce8:	subs	x9, x9, #0x1
   46cec:	str	x11, [x8], #8
   46cf0:	b.ne	46ce4 <__gmpn_toom_interpolate_16pts@@Base+0xc3c>  // b.any
   46cf4:	b	46d88 <__gmpn_toom_interpolate_16pts@@Base+0xce0>
   46cf8:	ldr	x21, [sp, #16]
   46cfc:	ldr	x8, [sp, #40]
   46d00:	cmp	x19, #0x2
   46d04:	mov	x4, xzr
   46d08:	b.lt	46d90 <__gmpn_toom_interpolate_16pts@@Base+0xce8>  // b.tstop
   46d0c:	ldr	x22, [sp, #32]
   46d10:	cmp	x8, x9
   46d14:	b.eq	46d94 <__gmpn_toom_interpolate_16pts@@Base+0xcec>  // b.none
   46d18:	sub	x8, x19, #0x1
   46d1c:	cmp	x8, #0x4
   46d20:	b.cc	46d5c <__gmpn_toom_interpolate_16pts@@Base+0xcb4>  // b.lo, b.ul, b.last
   46d24:	ldur	x11, [x29, #-24]
   46d28:	mov	w9, #0x8                   	// #8
   46d2c:	lsl	x10, x19, #4
   46d30:	bfi	x9, x19, #4, #60
   46d34:	add	x9, x24, x9
   46d38:	add	x10, x11, x10
   46d3c:	cmp	x9, x10
   46d40:	add	x9, x11, x19, lsl #3
   46d44:	b.cs	475e0 <__gmpn_toom_interpolate_16pts@@Base+0x1538>  // b.hs, b.nlast
   46d48:	mov	w10, #0x18                  	// #24
   46d4c:	madd	x10, x19, x10, x24
   46d50:	add	x11, x9, #0x8
   46d54:	cmp	x11, x10
   46d58:	b.cs	475e0 <__gmpn_toom_interpolate_16pts@@Base+0x1538>  // b.hs, b.nlast
   46d5c:	mov	w9, #0x1                   	// #1
   46d60:	add	x10, x9, x19, lsl #1
   46d64:	sub	x8, x19, x9
   46d68:	add	x11, x9, x19
   46d6c:	add	x9, x24, x10, lsl #3
   46d70:	ldur	x10, [x29, #-24]
   46d74:	add	x10, x10, x11, lsl #3
   46d78:	ldr	x11, [x10], #8
   46d7c:	subs	x8, x8, #0x1
   46d80:	str	x11, [x9], #8
   46d84:	b.ne	46d78 <__gmpn_toom_interpolate_16pts@@Base+0xcd0>  // b.any
   46d88:	mov	x4, xzr
   46d8c:	b	46d94 <__gmpn_toom_interpolate_16pts@@Base+0xcec>
   46d90:	ldr	x22, [sp, #32]
   46d94:	ldr	x25, [sp, #64]
   46d98:	ldur	x9, [x29, #-24]
   46d9c:	ldur	x8, [x29, #-8]
   46da0:	mov	x0, x26
   46da4:	mov	x1, x26
   46da8:	ldr	x20, [x9, x25, lsl #3]
   46dac:	add	x2, x9, x8, lsl #3
   46db0:	mov	x3, x19
   46db4:	bl	ce90 <__gmpn_add_nc@plt>
   46db8:	lsl	x8, x19, #5
   46dbc:	ldr	x9, [x24, x8]
   46dc0:	add	x10, x0, x20
   46dc4:	adds	x9, x9, x10
   46dc8:	str	x9, [x24, x8]
   46dcc:	b.cc	46de8 <__gmpn_toom_interpolate_16pts@@Base+0xd40>  // b.lo, b.ul, b.last
   46dd0:	add	x8, x24, x19, lsl #5
   46dd4:	add	x8, x8, #0x8
   46dd8:	ldr	x9, [x8]
   46ddc:	adds	x9, x9, #0x1
   46de0:	str	x9, [x8], #8
   46de4:	b.cs	46dd8 <__gmpn_toom_interpolate_16pts@@Base+0xd30>  // b.hs, b.nlast
   46de8:	ldur	x20, [x29, #-32]
   46dec:	mov	w8, #0x28                  	// #40
   46df0:	madd	x0, x19, x8, x24
   46df4:	mov	x1, x0
   46df8:	mov	x2, x20
   46dfc:	mov	x3, x19
   46e00:	bl	ca70 <__gmpn_add_n@plt>
   46e04:	add	x8, x19, x19, lsl #1
   46e08:	add	x10, x24, x8, lsl #4
   46e0c:	ldr	x9, [x10]
   46e10:	add	x11, x20, x19, lsl #3
   46e14:	lsl	x8, x8, #1
   46e18:	add	x9, x9, x0
   46e1c:	str	x9, [x10]
   46e20:	ldr	x12, [x11]
   46e24:	adds	x9, x12, x9
   46e28:	str	x9, [x10]
   46e2c:	b.cc	46f28 <__gmpn_toom_interpolate_16pts@@Base+0xe80>  // b.lo, b.ul, b.last
   46e30:	mov	x13, xzr
   46e34:	sub	x12, x19, #0x1
   46e38:	mov	w4, #0x1                   	// #1
   46e3c:	mov	w9, #0x1                   	// #1
   46e40:	cmp	x9, x19
   46e44:	b.ge	46fac <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tcont
   46e48:	add	x14, x11, x13
   46e4c:	ldr	x14, [x14, #8]
   46e50:	add	x15, x10, x13
   46e54:	add	x9, x9, #0x1
   46e58:	add	x13, x13, #0x8
   46e5c:	adds	x14, x14, #0x1
   46e60:	sub	x12, x12, #0x1
   46e64:	str	x14, [x15, #8]
   46e68:	b.cs	46e40 <__gmpn_toom_interpolate_16pts@@Base+0xd98>  // b.hs, b.nlast
   46e6c:	cmp	x11, x10
   46e70:	mov	x4, xzr
   46e74:	b.eq	46fac <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.none
   46e78:	cmp	x9, x19
   46e7c:	b.ge	46fac <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tcont
   46e80:	sub	x14, x19, x9
   46e84:	cmp	x14, #0x4
   46e88:	b.cc	46efc <__gmpn_toom_interpolate_16pts@@Base+0xe54>  // b.lo, b.ul, b.last
   46e8c:	ldur	x16, [x29, #-32]
   46e90:	add	x15, x10, x13
   46e94:	add	x15, x15, #0x8
   46e98:	add	x16, x16, x19, lsl #4
   46e9c:	cmp	x15, x16
   46ea0:	b.cs	46ebc <__gmpn_toom_interpolate_16pts@@Base+0xe14>  // b.hs, b.nlast
   46ea4:	mov	w15, #0x38                  	// #56
   46ea8:	add	x16, x11, x13
   46eac:	madd	x15, x19, x15, x24
   46eb0:	add	x16, x16, #0x8
   46eb4:	cmp	x16, x15
   46eb8:	b.cc	46efc <__gmpn_toom_interpolate_16pts@@Base+0xe54>  // b.lo, b.ul, b.last
   46ebc:	add	x15, x10, x13
   46ec0:	add	x13, x11, x13
   46ec4:	and	x10, x14, #0xfffffffffffffffc
   46ec8:	and	x16, x12, #0xfffffffffffffffc
   46ecc:	add	x11, x15, #0x18
   46ed0:	add	x12, x13, #0x18
   46ed4:	add	x9, x16, x9
   46ed8:	mov	x13, x10
   46edc:	ldp	q0, q1, [x12, #-16]
   46ee0:	add	x12, x12, #0x20
   46ee4:	subs	x13, x13, #0x4
   46ee8:	stp	q0, q1, [x11, #-16]
   46eec:	add	x11, x11, #0x20
   46ef0:	b.ne	46edc <__gmpn_toom_interpolate_16pts@@Base+0xe34>  // b.any
   46ef4:	cmp	x14, x10
   46ef8:	b.eq	46fa8 <__gmpn_toom_interpolate_16pts@@Base+0xf00>  // b.none
   46efc:	ldur	x11, [x29, #-32]
   46f00:	sub	x10, x19, x9
   46f04:	add	x8, x9, x8
   46f08:	add	x9, x9, x19
   46f0c:	add	x8, x24, x8, lsl #3
   46f10:	add	x9, x11, x9, lsl #3
   46f14:	ldr	x11, [x9], #8
   46f18:	subs	x10, x10, #0x1
   46f1c:	str	x11, [x8], #8
   46f20:	b.ne	46f14 <__gmpn_toom_interpolate_16pts@@Base+0xe6c>  // b.any
   46f24:	b	46fa8 <__gmpn_toom_interpolate_16pts@@Base+0xf00>
   46f28:	cmp	x19, #0x2
   46f2c:	mov	x4, xzr
   46f30:	b.lt	46fac <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.tstop
   46f34:	cmp	x11, x10
   46f38:	b.eq	46fac <__gmpn_toom_interpolate_16pts@@Base+0xf04>  // b.none
   46f3c:	sub	x9, x19, #0x1
   46f40:	cmp	x9, #0x4
   46f44:	b.cc	46f7c <__gmpn_toom_interpolate_16pts@@Base+0xed4>  // b.lo, b.ul, b.last
   46f48:	ldur	x12, [x29, #-32]
   46f4c:	lsl	x10, x8, #3
   46f50:	orr	x10, x10, #0x8
   46f54:	add	x10, x24, x10
   46f58:	add	x11, x12, x19, lsl #4
   46f5c:	cmp	x10, x11
   46f60:	add	x10, x12, x19, lsl #3
   46f64:	b.cs	4761c <__gmpn_toom_interpolate_16pts@@Base+0x1574>  // b.hs, b.nlast
   46f68:	mov	w11, #0x38                  	// #56
   46f6c:	madd	x11, x19, x11, x24
   46f70:	add	x12, x10, #0x8
   46f74:	cmp	x12, x11
   46f78:	b.cs	4761c <__gmpn_toom_interpolate_16pts@@Base+0x1574>  // b.hs, b.nlast
   46f7c:	mov	w10, #0x1                   	// #1
   46f80:	ldur	x11, [x29, #-32]
   46f84:	sub	x9, x19, x10
   46f88:	add	x8, x10, x8
   46f8c:	add	x10, x10, x19
   46f90:	add	x8, x24, x8, lsl #3
   46f94:	add	x10, x11, x10, lsl #3
   46f98:	ldr	x11, [x10], #8
   46f9c:	subs	x9, x9, #0x1
   46fa0:	str	x11, [x8], #8
   46fa4:	b.ne	46f98 <__gmpn_toom_interpolate_16pts@@Base+0xef0>  // b.any
   46fa8:	mov	x4, xzr
   46fac:	ldur	x9, [x29, #-32]
   46fb0:	ldur	x8, [x29, #-8]
   46fb4:	mov	x0, x23
   46fb8:	mov	x1, x23
   46fbc:	ldr	x20, [x9, x25, lsl #3]
   46fc0:	add	x2, x9, x8, lsl #3
   46fc4:	mov	x3, x19
   46fc8:	bl	ce90 <__gmpn_add_nc@plt>
   46fcc:	lsl	x8, x19, #6
   46fd0:	ldr	x9, [x24, x8]
   46fd4:	add	x10, x0, x20
   46fd8:	adds	x9, x9, x10
   46fdc:	str	x9, [x24, x8]
   46fe0:	b.cc	46ffc <__gmpn_toom_interpolate_16pts@@Base+0xf54>  // b.lo, b.ul, b.last
   46fe4:	add	x8, x24, x27, lsl #3
   46fe8:	add	x8, x8, #0x8
   46fec:	ldr	x9, [x8]
   46ff0:	adds	x9, x9, #0x1
   46ff4:	str	x9, [x8], #8
   46ff8:	b.cs	46fec <__gmpn_toom_interpolate_16pts@@Base+0xf44>  // b.hs, b.nlast
   46ffc:	ldur	x20, [x29, #-16]
   47000:	mov	w8, #0x48                  	// #72
   47004:	madd	x0, x19, x8, x24
   47008:	mov	x1, x0
   4700c:	mov	x2, x20
   47010:	mov	x3, x19
   47014:	bl	ca70 <__gmpn_add_n@plt>
   47018:	mov	w8, #0x50                  	// #80
   4701c:	madd	x9, x19, x8, x24
   47020:	ldr	x8, [x9]
   47024:	add	x8, x8, x0
   47028:	str	x8, [x9]
   4702c:	ldr	x10, [x20, x19, lsl #3]
   47030:	adds	x8, x10, x8
   47034:	str	x8, [x9]
   47038:	b.cc	4713c <__gmpn_toom_interpolate_16pts@@Base+0x1094>  // b.lo, b.ul, b.last
   4703c:	ldr	x15, [sp]
   47040:	mov	x11, xzr
   47044:	sub	x10, x19, #0x1
   47048:	mov	w4, #0x1                   	// #1
   4704c:	mov	w8, #0x1                   	// #1
   47050:	cmp	x8, x19
   47054:	b.ge	471cc <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tcont
   47058:	add	x12, x15, x11
   4705c:	ldr	x12, [x12, #8]
   47060:	add	x13, x9, x11
   47064:	add	x8, x8, #0x1
   47068:	add	x11, x11, #0x8
   4706c:	adds	x12, x12, #0x1
   47070:	sub	x10, x10, #0x1
   47074:	str	x12, [x13, #8]
   47078:	b.cs	47050 <__gmpn_toom_interpolate_16pts@@Base+0xfa8>  // b.hs, b.nlast
   4707c:	cmp	x15, x9
   47080:	mov	x4, xzr
   47084:	b.eq	471cc <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.none
   47088:	cmp	x8, x19
   4708c:	b.ge	471cc <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tcont
   47090:	sub	x12, x19, x8
   47094:	cmp	x12, #0x4
   47098:	b.cc	4710c <__gmpn_toom_interpolate_16pts@@Base+0x1064>  // b.lo, b.ul, b.last
   4709c:	ldur	x14, [x29, #-16]
   470a0:	add	x13, x9, x11
   470a4:	add	x13, x13, #0x8
   470a8:	add	x14, x14, x19, lsl #4
   470ac:	cmp	x13, x14
   470b0:	b.cs	470cc <__gmpn_toom_interpolate_16pts@@Base+0x1024>  // b.hs, b.nlast
   470b4:	mov	w13, #0x58                  	// #88
   470b8:	add	x14, x15, x11
   470bc:	madd	x13, x19, x13, x24
   470c0:	add	x14, x14, #0x8
   470c4:	cmp	x14, x13
   470c8:	b.cc	4710c <__gmpn_toom_interpolate_16pts@@Base+0x1064>  // b.lo, b.ul, b.last
   470cc:	add	x13, x9, x11
   470d0:	add	x11, x15, x11
   470d4:	and	x9, x12, #0xfffffffffffffffc
   470d8:	and	x14, x10, #0xfffffffffffffffc
   470dc:	add	x10, x13, #0x18
   470e0:	add	x11, x11, #0x18
   470e4:	add	x8, x14, x8
   470e8:	mov	x13, x9
   470ec:	ldp	q0, q1, [x11, #-16]
   470f0:	add	x11, x11, #0x20
   470f4:	subs	x13, x13, #0x4
   470f8:	stp	q0, q1, [x10, #-16]
   470fc:	add	x10, x10, #0x20
   47100:	b.ne	470ec <__gmpn_toom_interpolate_16pts@@Base+0x1044>  // b.any
   47104:	cmp	x12, x9
   47108:	b.eq	471c8 <__gmpn_toom_interpolate_16pts@@Base+0x1120>  // b.none
   4710c:	mov	w10, #0xa                   	// #10
   47110:	sub	x9, x19, x8
   47114:	add	x11, x8, x19
   47118:	madd	x8, x19, x10, x8
   4711c:	ldur	x10, [x29, #-16]
   47120:	add	x8, x24, x8, lsl #3
   47124:	add	x10, x10, x11, lsl #3
   47128:	ldr	x11, [x10], #8
   4712c:	subs	x9, x9, #0x1
   47130:	str	x11, [x8], #8
   47134:	b.ne	47128 <__gmpn_toom_interpolate_16pts@@Base+0x1080>  // b.any
   47138:	b	471c8 <__gmpn_toom_interpolate_16pts@@Base+0x1120>
   4713c:	ldr	x8, [sp]
   47140:	cmp	x19, #0x2
   47144:	mov	x4, xzr
   47148:	b.lt	471cc <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.tstop
   4714c:	cmp	x8, x9
   47150:	b.eq	471cc <__gmpn_toom_interpolate_16pts@@Base+0x1124>  // b.none
   47154:	sub	x8, x19, #0x1
   47158:	cmp	x8, #0x4
   4715c:	b.cc	47198 <__gmpn_toom_interpolate_16pts@@Base+0x10f0>  // b.lo, b.ul, b.last
   47160:	ldur	x11, [x29, #-16]
   47164:	add	x9, x19, x19, lsl #2
   47168:	mov	w10, #0x8                   	// #8
   4716c:	bfi	x10, x9, #4, #60
   47170:	add	x9, x24, x10
   47174:	add	x10, x11, x19, lsl #4
   47178:	cmp	x9, x10
   4717c:	add	x9, x11, x19, lsl #3
   47180:	b.cs	47658 <__gmpn_toom_interpolate_16pts@@Base+0x15b0>  // b.hs, b.nlast
   47184:	mov	w10, #0x58                  	// #88
   47188:	madd	x10, x19, x10, x24
   4718c:	add	x11, x9, #0x8
   47190:	cmp	x11, x10
   47194:	b.cs	47658 <__gmpn_toom_interpolate_16pts@@Base+0x15b0>  // b.hs, b.nlast
   47198:	mov	w9, #0x1                   	// #1
   4719c:	mov	w10, #0xa                   	// #10
   471a0:	sub	x8, x19, x9
   471a4:	add	x11, x9, x19
   471a8:	madd	x9, x19, x10, x9
   471ac:	ldur	x10, [x29, #-16]
   471b0:	add	x9, x24, x9, lsl #3
   471b4:	add	x10, x10, x11, lsl #3
   471b8:	ldr	x11, [x10], #8
   471bc:	subs	x8, x8, #0x1
   471c0:	str	x11, [x9], #8
   471c4:	b.ne	471b8 <__gmpn_toom_interpolate_16pts@@Base+0x1110>  // b.any
   471c8:	mov	x4, xzr
   471cc:	ldp	x8, x9, [x29, #-16]
   471d0:	ldr	x0, [sp, #56]
   471d4:	mov	x3, x19
   471d8:	ldr	x20, [x8, x25, lsl #3]
   471dc:	add	x2, x8, x9, lsl #3
   471e0:	mov	x1, x0
   471e4:	bl	ce90 <__gmpn_add_nc@plt>
   471e8:	mov	w8, #0x60                  	// #96
   471ec:	mul	x8, x19, x8
   471f0:	ldr	x9, [x24, x8]
   471f4:	add	x10, x0, x20
   471f8:	adds	x9, x9, x10
   471fc:	str	x9, [x24, x8]
   47200:	b.cc	47220 <__gmpn_toom_interpolate_16pts@@Base+0x1178>  // b.lo, b.ul, b.last
   47204:	mov	w8, #0x60                  	// #96
   47208:	madd	x8, x19, x8, x24
   4720c:	add	x8, x8, #0x8
   47210:	ldr	x9, [x8]
   47214:	adds	x9, x9, #0x1
   47218:	str	x9, [x8], #8
   4721c:	b.cs	47210 <__gmpn_toom_interpolate_16pts@@Base+0x1168>  // b.hs, b.nlast
   47220:	mov	w8, #0x68                  	// #104
   47224:	madd	x0, x19, x8, x24
   47228:	mov	x1, x0
   4722c:	mov	x2, x28
   47230:	mov	x3, x19
   47234:	bl	ca70 <__gmpn_add_n@plt>
   47238:	mov	w8, #0x70                  	// #112
   4723c:	madd	x8, x19, x8, x24
   47240:	ldr	x9, [x8]
   47244:	ldr	w11, [sp, #12]
   47248:	add	x9, x9, x0
   4724c:	str	x9, [x8]
   47250:	ldr	x10, [x28, x19, lsl #3]
   47254:	add	x10, x10, x9
   47258:	str	x10, [x8]
   4725c:	cbz	w11, 4735c <__gmpn_toom_interpolate_16pts@@Base+0x12b4>
   47260:	cmp	x10, x9
   47264:	b.cs	47458 <__gmpn_toom_interpolate_16pts@@Base+0x13b0>  // b.hs, b.nlast
   47268:	mov	x11, xzr
   4726c:	sub	x10, x19, #0x1
   47270:	mov	w4, #0x1                   	// #1
   47274:	mov	w9, #0x1                   	// #1
   47278:	cmp	x9, x19
   4727c:	b.ge	474dc <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tcont
   47280:	add	x12, x22, x11
   47284:	ldr	x12, [x12, #8]
   47288:	add	x13, x8, x11
   4728c:	add	x9, x9, #0x1
   47290:	add	x11, x11, #0x8
   47294:	adds	x12, x12, #0x1
   47298:	sub	x10, x10, #0x1
   4729c:	str	x12, [x13, #8]
   472a0:	b.cs	47278 <__gmpn_toom_interpolate_16pts@@Base+0x11d0>  // b.hs, b.nlast
   472a4:	cmp	x22, x8
   472a8:	mov	x4, xzr
   472ac:	b.eq	474dc <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.none
   472b0:	cmp	x9, x19
   472b4:	b.ge	474dc <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tcont
   472b8:	sub	x12, x19, x9
   472bc:	cmp	x12, #0x4
   472c0:	b.cc	47330 <__gmpn_toom_interpolate_16pts@@Base+0x1288>  // b.lo, b.ul, b.last
   472c4:	add	x13, x8, x11
   472c8:	add	x13, x13, #0x8
   472cc:	add	x14, x28, x19, lsl #4
   472d0:	cmp	x13, x14
   472d4:	b.cs	472f0 <__gmpn_toom_interpolate_16pts@@Base+0x1248>  // b.hs, b.nlast
   472d8:	mov	w13, #0x78                  	// #120
   472dc:	add	x14, x22, x11
   472e0:	madd	x13, x19, x13, x24
   472e4:	add	x14, x14, #0x8
   472e8:	cmp	x14, x13
   472ec:	b.cc	47330 <__gmpn_toom_interpolate_16pts@@Base+0x1288>  // b.lo, b.ul, b.last
   472f0:	add	x13, x8, x11
   472f4:	add	x11, x22, x11
   472f8:	and	x8, x12, #0xfffffffffffffffc
   472fc:	and	x14, x10, #0xfffffffffffffffc
   47300:	add	x10, x13, #0x18
   47304:	add	x11, x11, #0x18
   47308:	add	x9, x14, x9
   4730c:	mov	x13, x8
   47310:	ldp	q0, q1, [x11, #-16]
   47314:	add	x11, x11, #0x20
   47318:	subs	x13, x13, #0x4
   4731c:	stp	q0, q1, [x10, #-16]
   47320:	add	x10, x10, #0x20
   47324:	b.ne	47310 <__gmpn_toom_interpolate_16pts@@Base+0x1268>  // b.any
   47328:	cmp	x12, x8
   4732c:	b.eq	474d8 <__gmpn_toom_interpolate_16pts@@Base+0x1430>  // b.none
   47330:	mov	w10, #0xe                   	// #14
   47334:	sub	x8, x19, x9
   47338:	add	x11, x9, x19
   4733c:	madd	x9, x19, x10, x9
   47340:	add	x9, x24, x9, lsl #3
   47344:	add	x10, x28, x11, lsl #3
   47348:	ldr	x11, [x10], #8
   4734c:	subs	x8, x8, #0x1
   47350:	str	x11, [x9], #8
   47354:	b.ne	47348 <__gmpn_toom_interpolate_16pts@@Base+0x12a0>  // b.any
   47358:	b	474d8 <__gmpn_toom_interpolate_16pts@@Base+0x1430>
   4735c:	cmp	x10, x9
   47360:	b.cs	47538 <__gmpn_toom_interpolate_16pts@@Base+0x1490>  // b.hs, b.nlast
   47364:	mov	x11, xzr
   47368:	sub	x10, x21, #0x1
   4736c:	mov	w9, #0x1                   	// #1
   47370:	cmp	x9, x21
   47374:	b.ge	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tcont
   47378:	add	x12, x22, x11
   4737c:	ldr	x12, [x12, #8]
   47380:	add	x13, x8, x11
   47384:	add	x9, x9, #0x1
   47388:	add	x11, x11, #0x8
   4738c:	adds	x12, x12, #0x1
   47390:	sub	x10, x10, #0x1
   47394:	str	x12, [x13, #8]
   47398:	b.cs	47370 <__gmpn_toom_interpolate_16pts@@Base+0x12c8>  // b.hs, b.nlast
   4739c:	cmp	x22, x8
   473a0:	b.eq	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   473a4:	cmp	x9, x21
   473a8:	b.ge	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tcont
   473ac:	sub	x12, x21, x9
   473b0:	cmp	x12, #0x4
   473b4:	b.cc	4742c <__gmpn_toom_interpolate_16pts@@Base+0x1384>  // b.lo, b.ul, b.last
   473b8:	add	x13, x8, x11
   473bc:	add	x14, x21, x19
   473c0:	add	x13, x13, #0x8
   473c4:	add	x14, x28, x14, lsl #3
   473c8:	cmp	x13, x14
   473cc:	b.cs	473ec <__gmpn_toom_interpolate_16pts@@Base+0x1344>  // b.hs, b.nlast
   473d0:	mov	w13, #0xe                   	// #14
   473d4:	add	x14, x22, x11
   473d8:	madd	x13, x19, x13, x21
   473dc:	add	x13, x24, x13, lsl #3
   473e0:	add	x14, x14, #0x8
   473e4:	cmp	x14, x13
   473e8:	b.cc	4742c <__gmpn_toom_interpolate_16pts@@Base+0x1384>  // b.lo, b.ul, b.last
   473ec:	add	x13, x8, x11
   473f0:	add	x11, x22, x11
   473f4:	and	x8, x12, #0xfffffffffffffffc
   473f8:	and	x14, x10, #0xfffffffffffffffc
   473fc:	add	x10, x13, #0x18
   47400:	add	x11, x11, #0x18
   47404:	add	x9, x14, x9
   47408:	mov	x13, x8
   4740c:	ldp	q0, q1, [x11, #-16]
   47410:	add	x11, x11, #0x20
   47414:	subs	x13, x13, #0x4
   47418:	stp	q0, q1, [x10, #-16]
   4741c:	add	x10, x10, #0x20
   47420:	b.ne	4740c <__gmpn_toom_interpolate_16pts@@Base+0x1364>  // b.any
   47424:	cmp	x12, x8
   47428:	b.eq	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   4742c:	mov	w10, #0xe                   	// #14
   47430:	sub	x8, x21, x9
   47434:	add	x11, x9, x19
   47438:	madd	x9, x19, x10, x9
   4743c:	add	x9, x24, x9, lsl #3
   47440:	add	x10, x28, x11, lsl #3
   47444:	ldr	x11, [x10], #8
   47448:	subs	x8, x8, #0x1
   4744c:	str	x11, [x9], #8
   47450:	b.ne	47444 <__gmpn_toom_interpolate_16pts@@Base+0x139c>  // b.any
   47454:	b	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>
   47458:	cmp	x19, #0x2
   4745c:	mov	x4, xzr
   47460:	b.lt	474dc <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.tstop
   47464:	cmp	x22, x8
   47468:	b.eq	474dc <__gmpn_toom_interpolate_16pts@@Base+0x1434>  // b.none
   4746c:	sub	x8, x19, #0x1
   47470:	cmp	x8, #0x4
   47474:	b.cc	474ac <__gmpn_toom_interpolate_16pts@@Base+0x1404>  // b.lo, b.ul, b.last
   47478:	mov	w9, #0x70                  	// #112
   4747c:	mul	x9, x19, x9
   47480:	orr	x9, x9, #0x8
   47484:	add	x9, x24, x9
   47488:	add	x10, x28, x19, lsl #4
   4748c:	cmp	x9, x10
   47490:	add	x9, x28, x19, lsl #3
   47494:	b.cs	47698 <__gmpn_toom_interpolate_16pts@@Base+0x15f0>  // b.hs, b.nlast
   47498:	mov	w10, #0x78                  	// #120
   4749c:	madd	x10, x19, x10, x24
   474a0:	add	x11, x9, #0x8
   474a4:	cmp	x11, x10
   474a8:	b.cs	47698 <__gmpn_toom_interpolate_16pts@@Base+0x15f0>  // b.hs, b.nlast
   474ac:	mov	w9, #0x1                   	// #1
   474b0:	mov	w10, #0xe                   	// #14
   474b4:	sub	x8, x19, x9
   474b8:	add	x11, x9, x19
   474bc:	madd	x9, x19, x10, x9
   474c0:	add	x9, x24, x9, lsl #3
   474c4:	add	x10, x28, x11, lsl #3
   474c8:	ldr	x11, [x10], #8
   474cc:	subs	x8, x8, #0x1
   474d0:	str	x11, [x9], #8
   474d4:	b.ne	474c8 <__gmpn_toom_interpolate_16pts@@Base+0x1420>  // b.any
   474d8:	mov	x4, xzr
   474dc:	cmp	x21, x19
   474e0:	b.le	47718 <__gmpn_toom_interpolate_16pts@@Base+0x1670>
   474e4:	mov	w8, #0x78                  	// #120
   474e8:	madd	x0, x19, x8, x24
   474ec:	ldur	x8, [x29, #-8]
   474f0:	ldr	x20, [x28, x25, lsl #3]
   474f4:	mov	x1, x0
   474f8:	mov	x3, x19
   474fc:	add	x2, x28, x8, lsl #3
   47500:	bl	ce90 <__gmpn_add_nc@plt>
   47504:	lsl	x8, x19, #7
   47508:	ldr	x9, [x24, x8]
   4750c:	add	x10, x0, x20
   47510:	adds	x9, x9, x10
   47514:	str	x9, [x24, x8]
   47518:	b.cc	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.lo, b.ul, b.last
   4751c:	add	x8, x24, x19, lsl #7
   47520:	add	x8, x8, #0x8
   47524:	ldr	x9, [x8]
   47528:	adds	x9, x9, #0x1
   4752c:	str	x9, [x8], #8
   47530:	b.cs	47524 <__gmpn_toom_interpolate_16pts@@Base+0x147c>  // b.hs, b.nlast
   47534:	b	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>
   47538:	cmp	x21, #0x2
   4753c:	b.lt	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.tstop
   47540:	cmp	x22, x8
   47544:	b.eq	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   47548:	sub	x8, x21, #0x1
   4754c:	cmp	x8, #0x4
   47550:	b.cc	47594 <__gmpn_toom_interpolate_16pts@@Base+0x14ec>  // b.lo, b.ul, b.last
   47554:	mov	w9, #0xe                   	// #14
   47558:	mul	x10, x19, x9
   4755c:	mov	w11, #0x8                   	// #8
   47560:	lsr	x9, x10, #1
   47564:	add	x12, x21, x19
   47568:	bfi	x11, x9, #4, #60
   4756c:	add	x9, x24, x11
   47570:	add	x11, x28, x12, lsl #3
   47574:	cmp	x9, x11
   47578:	add	x9, x28, x19, lsl #3
   4757c:	b.cs	476d8 <__gmpn_toom_interpolate_16pts@@Base+0x1630>  // b.hs, b.nlast
   47580:	add	x10, x10, x21
   47584:	add	x10, x24, x10, lsl #3
   47588:	add	x11, x9, #0x8
   4758c:	cmp	x11, x10
   47590:	b.cs	476d8 <__gmpn_toom_interpolate_16pts@@Base+0x1630>  // b.hs, b.nlast
   47594:	mov	w9, #0x1                   	// #1
   47598:	mov	w10, #0xe                   	// #14
   4759c:	sub	x8, x21, x9
   475a0:	add	x11, x9, x19
   475a4:	madd	x9, x19, x10, x9
   475a8:	add	x9, x24, x9, lsl #3
   475ac:	add	x10, x28, x11, lsl #3
   475b0:	ldr	x11, [x10], #8
   475b4:	subs	x8, x8, #0x1
   475b8:	str	x11, [x9], #8
   475bc:	b.ne	475b0 <__gmpn_toom_interpolate_16pts@@Base+0x1508>  // b.any
   475c0:	ldp	x20, x19, [sp, #208]
   475c4:	ldp	x22, x21, [sp, #192]
   475c8:	ldp	x24, x23, [sp, #176]
   475cc:	ldp	x26, x25, [sp, #160]
   475d0:	ldp	x28, x27, [sp, #144]
   475d4:	ldp	x29, x30, [sp, #128]
   475d8:	add	sp, sp, #0xe0
   475dc:	ret
   475e0:	and	x10, x8, #0xfffffffffffffffc
   475e4:	add	x12, x24, x19, lsl #4
   475e8:	add	x11, x9, #0x18
   475ec:	orr	x9, x10, #0x1
   475f0:	add	x12, x12, #0x18
   475f4:	mov	x13, x10
   475f8:	ldp	q0, q1, [x11, #-16]
   475fc:	add	x11, x11, #0x20
   47600:	subs	x13, x13, #0x4
   47604:	stp	q0, q1, [x12, #-16]
   47608:	add	x12, x12, #0x20
   4760c:	b.ne	475f8 <__gmpn_toom_interpolate_16pts@@Base+0x1550>  // b.any
   47610:	cmp	x8, x10
   47614:	b.eq	46d88 <__gmpn_toom_interpolate_16pts@@Base+0xce0>  // b.none
   47618:	b	46d60 <__gmpn_toom_interpolate_16pts@@Base+0xcb8>
   4761c:	and	x11, x9, #0xfffffffffffffffc
   47620:	add	x13, x24, x8, lsl #3
   47624:	add	x12, x10, #0x18
   47628:	orr	x10, x11, #0x1
   4762c:	add	x13, x13, #0x18
   47630:	mov	x14, x11
   47634:	ldp	q0, q1, [x12, #-16]
   47638:	add	x12, x12, #0x20
   4763c:	subs	x14, x14, #0x4
   47640:	stp	q0, q1, [x13, #-16]
   47644:	add	x13, x13, #0x20
   47648:	b.ne	47634 <__gmpn_toom_interpolate_16pts@@Base+0x158c>  // b.any
   4764c:	cmp	x9, x11
   47650:	b.eq	46fa8 <__gmpn_toom_interpolate_16pts@@Base+0xf00>  // b.none
   47654:	b	46f80 <__gmpn_toom_interpolate_16pts@@Base+0xed8>
   47658:	mov	w12, #0x50                  	// #80
   4765c:	and	x10, x8, #0xfffffffffffffffc
   47660:	madd	x12, x19, x12, x24
   47664:	add	x11, x9, #0x18
   47668:	orr	x9, x10, #0x1
   4766c:	add	x12, x12, #0x18
   47670:	mov	x13, x10
   47674:	ldp	q0, q1, [x11, #-16]
   47678:	add	x11, x11, #0x20
   4767c:	subs	x13, x13, #0x4
   47680:	stp	q0, q1, [x12, #-16]
   47684:	add	x12, x12, #0x20
   47688:	b.ne	47674 <__gmpn_toom_interpolate_16pts@@Base+0x15cc>  // b.any
   4768c:	cmp	x8, x10
   47690:	b.eq	471c8 <__gmpn_toom_interpolate_16pts@@Base+0x1120>  // b.none
   47694:	b	4719c <__gmpn_toom_interpolate_16pts@@Base+0x10f4>
   47698:	mov	w12, #0x70                  	// #112
   4769c:	and	x10, x8, #0xfffffffffffffffc
   476a0:	madd	x12, x19, x12, x24
   476a4:	add	x11, x9, #0x18
   476a8:	orr	x9, x10, #0x1
   476ac:	add	x12, x12, #0x18
   476b0:	mov	x13, x10
   476b4:	ldp	q0, q1, [x11, #-16]
   476b8:	add	x11, x11, #0x20
   476bc:	subs	x13, x13, #0x4
   476c0:	stp	q0, q1, [x12, #-16]
   476c4:	add	x12, x12, #0x20
   476c8:	b.ne	476b4 <__gmpn_toom_interpolate_16pts@@Base+0x160c>  // b.any
   476cc:	cmp	x8, x10
   476d0:	b.eq	474d8 <__gmpn_toom_interpolate_16pts@@Base+0x1430>  // b.none
   476d4:	b	474b0 <__gmpn_toom_interpolate_16pts@@Base+0x1408>
   476d8:	mov	w12, #0x70                  	// #112
   476dc:	and	x10, x8, #0xfffffffffffffffc
   476e0:	madd	x12, x19, x12, x24
   476e4:	add	x11, x9, #0x18
   476e8:	orr	x9, x10, #0x1
   476ec:	add	x12, x12, #0x18
   476f0:	mov	x13, x10
   476f4:	ldp	q0, q1, [x11, #-16]
   476f8:	add	x11, x11, #0x20
   476fc:	subs	x13, x13, #0x4
   47700:	stp	q0, q1, [x12, #-16]
   47704:	add	x12, x12, #0x20
   47708:	b.ne	476f4 <__gmpn_toom_interpolate_16pts@@Base+0x164c>  // b.any
   4770c:	cmp	x8, x10
   47710:	b.eq	475c0 <__gmpn_toom_interpolate_16pts@@Base+0x1518>  // b.none
   47714:	b	47598 <__gmpn_toom_interpolate_16pts@@Base+0x14f0>
   47718:	mov	w8, #0x78                  	// #120
   4771c:	madd	x0, x19, x8, x24
   47720:	ldur	x8, [x29, #-8]
   47724:	mov	x3, x21
   47728:	ldp	x20, x19, [sp, #208]
   4772c:	ldp	x22, x21, [sp, #192]
   47730:	add	x2, x28, x8, lsl #3
   47734:	ldp	x24, x23, [sp, #176]
   47738:	ldp	x26, x25, [sp, #160]
   4773c:	ldp	x28, x27, [sp, #144]
   47740:	ldp	x29, x30, [sp, #128]
   47744:	mov	x1, x0
   47748:	add	sp, sp, #0xe0
   4774c:	b	ce90 <__gmpn_add_nc@plt>

0000000000047750 <__gmpn_ni_invertappr@@Base>:
   47750:	stp	x29, x30, [sp, #-96]!
   47754:	stp	x28, x27, [sp, #16]
   47758:	stp	x26, x25, [sp, #32]
   4775c:	stp	x24, x23, [sp, #48]
   47760:	stp	x22, x21, [sp, #64]
   47764:	stp	x20, x19, [sp, #80]
   47768:	mov	x29, sp
   4776c:	sub	sp, sp, #0x1b0
   47770:	mov	x19, sp
   47774:	mov	x20, x3
   47778:	mov	x25, x2
   4777c:	mov	x24, x1
   47780:	mov	x23, x0
   47784:	mov	x22, xzr
   47788:	add	x8, x19, #0x68
   4778c:	mov	x21, x2
   47790:	asr	x9, x21, #1
   47794:	str	x21, [x8, x22, lsl #3]
   47798:	cmp	x21, #0x143
   4779c:	add	x21, x9, #0x1
   477a0:	add	x22, x22, #0x1
   477a4:	b.gt	47790 <__gmpn_ni_invertappr@@Base+0x40>
   477a8:	lsl	x26, x25, #3
   477ac:	mvn	x8, x9
   477b0:	add	x9, x24, x26
   477b4:	add	x28, x23, x26
   477b8:	lsl	x8, x8, #3
   477bc:	add	x0, x28, x8
   477c0:	add	x1, x9, x8
   477c4:	mov	x2, x21
   477c8:	mov	x3, x20
   477cc:	str	x9, [x19, #48]
   477d0:	bl	47c78 <__gmpn_ni_invertappr@@Base+0x528>
   477d4:	add	x0, x25, #0x1
   477d8:	str	xzr, [x19, #96]
   477dc:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   477e0:	asr	x8, x0, #1
   477e4:	cmp	x8, x25, asr #1
   477e8:	csel	x9, x8, x0, gt
   477ec:	cmp	x8, x25
   477f0:	csel	x8, x9, xzr, lt  // lt = tstop
   477f4:	add	x8, x0, x8
   477f8:	lsl	x8, x8, #3
   477fc:	add	x1, x8, #0x20
   47800:	mov	w8, #0x7f00                	// #32512
   47804:	cmp	x1, x8
   47808:	b.hi	47c60 <__gmpn_ni_invertappr@@Base+0x510>  // b.pmore
   4780c:	add	x9, x1, #0xf
   47810:	mov	x8, sp
   47814:	and	x9, x9, #0xfffffffffffffff0
   47818:	sub	x8, x8, x9
   4781c:	str	x8, [x19, #16]
   47820:	mov	sp, x8
   47824:	add	x8, x20, #0x8
   47828:	stp	x8, x28, [x19, #32]
   4782c:	add	x8, x26, x24
   47830:	ldr	x24, [x19, #48]
   47834:	sub	x9, x20, #0x8
   47838:	str	x9, [x19, #8]
   4783c:	add	x9, x26, x23
   47840:	sub	x8, x8, #0x8
   47844:	str	x8, [x19, #64]
   47848:	add	x8, x9, #0x8
   4784c:	str	x8, [x19, #24]
   47850:	b	47860 <__gmpn_ni_invertappr@@Base+0x110>
   47854:	ldr	x24, [x19, #48]
   47858:	mov	x21, x23
   4785c:	cbz	x22, 47c20 <__gmpn_ni_invertappr@@Base+0x4d0>
   47860:	sub	x22, x22, #0x1
   47864:	add	x8, x19, #0x68
   47868:	ldr	x23, [x8, x22, lsl #3]
   4786c:	neg	x8, x21
   47870:	str	x8, [x19, #88]
   47874:	add	x0, x23, #0x1
   47878:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4787c:	add	x8, x23, x21
   47880:	sub	x26, x24, x23, lsl #3
   47884:	cmp	x0, x8
   47888:	sub	x4, x28, x21, lsl #3
   4788c:	str	x4, [x19, #80]
   47890:	b.le	478cc <__gmpn_ni_invertappr@@Base+0x17c>
   47894:	mov	x0, x20
   47898:	mov	x1, x26
   4789c:	mov	x2, x23
   478a0:	mov	x3, x4
   478a4:	mov	x4, x21
   478a8:	bl	ccd0 <__gmpn_mul@plt>
   478ac:	add	x0, x20, x21, lsl #3
   478b0:	sub	x8, x23, x21
   478b4:	add	x3, x8, #0x1
   478b8:	mov	x1, x0
   478bc:	mov	x2, x26
   478c0:	bl	ca70 <__gmpn_add_n@plt>
   478c4:	mov	w8, #0x1                   	// #1
   478c8:	b	479a8 <__gmpn_ni_invertappr@@Base+0x258>
   478cc:	ldr	x6, [x19, #16]
   478d0:	mov	x25, x0
   478d4:	mov	x0, x20
   478d8:	mov	x1, x25
   478dc:	mov	x2, x26
   478e0:	mov	x3, x23
   478e4:	mov	x5, x21
   478e8:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   478ec:	add	x27, x20, x21, lsl #3
   478f0:	sub	x28, x25, x21
   478f4:	mov	x0, x27
   478f8:	mov	x1, x27
   478fc:	mov	x2, x26
   47900:	mov	x3, x28
   47904:	bl	ca70 <__gmpn_add_n@plt>
   47908:	sub	x3, x23, x28
   4790c:	mov	x4, x0
   47910:	sub	x2, x24, x3, lsl #3
   47914:	mov	x0, x20
   47918:	mov	x1, x20
   4791c:	bl	ce90 <__gmpn_add_nc@plt>
   47920:	lsl	x8, x25, #3
   47924:	add	x9, x27, x23, lsl #3
   47928:	mov	w11, #0x1                   	// #1
   4792c:	str	x11, [x20, x8]
   47930:	sub	x9, x9, x8
   47934:	ldr	x10, [x9]
   47938:	sub	x11, x11, x0
   4793c:	subs	x10, x10, x11
   47940:	str	x10, [x9]
   47944:	b.cs	47968 <__gmpn_ni_invertappr@@Base+0x218>  // b.hs, b.nlast
   47948:	ldr	x10, [x19, #32]
   4794c:	add	x9, x21, x23
   47950:	sub	x9, x9, x25
   47954:	add	x9, x10, x9, lsl #3
   47958:	ldr	x10, [x9]
   4795c:	sub	x11, x10, #0x1
   47960:	str	x11, [x9], #8
   47964:	cbz	x10, 47958 <__gmpn_ni_invertappr@@Base+0x208>
   47968:	ldr	x9, [x20, x8]
   4796c:	ldr	x10, [x20]
   47970:	add	x9, x9, x10
   47974:	sub	x9, x9, #0x1
   47978:	str	x9, [x20]
   4797c:	ldr	x8, [x20, x8]
   47980:	mov	w9, #0x1                   	// #1
   47984:	sub	x8, x9, x8
   47988:	cmp	x10, x8
   4798c:	b.cs	479a4 <__gmpn_ni_invertappr@@Base+0x254>  // b.hs, b.nlast
   47990:	ldr	x8, [x19, #32]
   47994:	ldr	x9, [x8]
   47998:	sub	x10, x9, #0x1
   4799c:	str	x10, [x8], #8
   479a0:	cbz	x9, 47994 <__gmpn_ni_invertappr@@Base+0x244>
   479a4:	mov	x8, xzr
   479a8:	add	x25, x20, x23, lsl #3
   479ac:	neg	x9, x23
   479b0:	ldr	x28, [x25]
   479b4:	str	x9, [x19, #72]
   479b8:	ldr	x9, [x19, #88]
   479bc:	cmp	x28, #0x1
   479c0:	lsl	x27, x9, #3
   479c4:	b.hi	47a1c <__gmpn_ni_invertappr@@Base+0x2cc>  // b.pmore
   479c8:	cbz	x28, 47aa8 <__gmpn_ni_invertappr@@Base+0x358>
   479cc:	add	x8, x28, #0x1
   479d0:	str	x8, [x19, #56]
   479d4:	ldr	x8, [x19, #64]
   479d8:	mov	x10, x23
   479dc:	subs	x9, x10, #0x1
   479e0:	b.lt	47a90 <__gmpn_ni_invertappr@@Base+0x340>  // b.tstop
   479e4:	add	x10, x20, x10, lsl #3
   479e8:	ldur	x10, [x10, #-8]
   479ec:	ldr	x11, [x8], #-8
   479f0:	cmp	x10, x11
   479f4:	mov	x10, x9
   479f8:	b.eq	479dc <__gmpn_ni_invertappr@@Base+0x28c>  // b.none
   479fc:	b.ls	47a90 <__gmpn_ni_invertappr@@Base+0x340>  // b.plast
   47a00:	mov	x0, x20
   47a04:	mov	x1, x20
   47a08:	mov	x2, x26
   47a0c:	mov	x3, x23
   47a10:	bl	c440 <__gmpn_sublsh1_n@plt>
   47a14:	add	x8, x28, #0x2
   47a18:	b	47aac <__gmpn_ni_invertappr@@Base+0x35c>
   47a1c:	ldr	x9, [x20]
   47a20:	subs	x8, x9, x8
   47a24:	str	x8, [x20]
   47a28:	b.cs	47a40 <__gmpn_ni_invertappr@@Base+0x2f0>  // b.hs, b.nlast
   47a2c:	ldr	x8, [x19, #32]
   47a30:	ldr	x9, [x8]
   47a34:	sub	x10, x9, #0x1
   47a38:	str	x10, [x8], #8
   47a3c:	cbz	x9, 47a30 <__gmpn_ni_invertappr@@Base+0x2e0>
   47a40:	ldr	x8, [x25]
   47a44:	ldr	x28, [x19, #80]
   47a48:	cmn	x8, #0x1
   47a4c:	b.eq	47a78 <__gmpn_ni_invertappr@@Base+0x328>  // b.none
   47a50:	mov	x8, x28
   47a54:	ldr	x9, [x8]
   47a58:	adds	x9, x9, #0x1
   47a5c:	str	x9, [x8], #8
   47a60:	b.cs	47a54 <__gmpn_ni_invertappr@@Base+0x304>  // b.hs, b.nlast
   47a64:	mov	x0, x20
   47a68:	mov	x1, x20
   47a6c:	mov	x2, x26
   47a70:	mov	x3, x23
   47a74:	bl	ca70 <__gmpn_add_n@plt>
   47a78:	add	x8, x20, x23, lsl #4
   47a7c:	add	x0, x8, x27
   47a80:	add	x1, x25, x27
   47a84:	mov	x2, x21
   47a88:	bl	c290 <__gmpn_com@plt>
   47a8c:	b	47b8c <__gmpn_ni_invertappr@@Base+0x43c>
   47a90:	mov	x0, x20
   47a94:	mov	x1, x20
   47a98:	mov	x2, x26
   47a9c:	mov	x3, x23
   47aa0:	bl	c2d0 <__gmpn_sub_n@plt>
   47aa4:	b	47ab0 <__gmpn_ni_invertappr@@Base+0x360>
   47aa8:	mov	w8, #0x1                   	// #1
   47aac:	str	x8, [x19, #56]
   47ab0:	ldr	x8, [x19, #64]
   47ab4:	ldr	x28, [x19, #80]
   47ab8:	mov	x10, x23
   47abc:	subs	x9, x10, #0x1
   47ac0:	b.lt	47b00 <__gmpn_ni_invertappr@@Base+0x3b0>  // b.tstop
   47ac4:	add	x10, x20, x10, lsl #3
   47ac8:	ldur	x10, [x10, #-8]
   47acc:	ldr	x11, [x8], #-8
   47ad0:	cmp	x10, x11
   47ad4:	mov	x10, x9
   47ad8:	b.eq	47abc <__gmpn_ni_invertappr@@Base+0x36c>  // b.none
   47adc:	b.ls	47b00 <__gmpn_ni_invertappr@@Base+0x3b0>  // b.plast
   47ae0:	mov	x0, x25
   47ae4:	mov	x1, x20
   47ae8:	mov	x2, x26
   47aec:	mov	x3, x23
   47af0:	bl	d090 <__gmpn_rsblsh1_n@plt>
   47af4:	ldr	x9, [x19, #56]
   47af8:	add	x9, x9, #0x1
   47afc:	b	47b60 <__gmpn_ni_invertappr@@Base+0x410>
   47b00:	add	x8, x20, x23, lsl #4
   47b04:	add	x0, x8, x27
   47b08:	ldr	x8, [x19, #64]
   47b0c:	ldr	x12, [x19, #8]
   47b10:	add	x1, x24, x27
   47b14:	add	x2, x25, x27
   47b18:	sub	x9, x23, x21
   47b1c:	add	x8, x8, x27
   47b20:	subs	x10, x9, #0x1
   47b24:	b.lt	47b48 <__gmpn_ni_invertappr@@Base+0x3f8>  // b.tstop
   47b28:	ldr	x9, [x12, x9, lsl #3]
   47b2c:	ldr	x11, [x8], #-8
   47b30:	cmp	x9, x11
   47b34:	mov	x9, x10
   47b38:	b.eq	47b20 <__gmpn_ni_invertappr@@Base+0x3d0>  // b.none
   47b3c:	mov	w8, #0x1                   	// #1
   47b40:	cneg	w8, w8, ls  // ls = plast
   47b44:	b	47b4c <__gmpn_ni_invertappr@@Base+0x3fc>
   47b48:	mov	w8, wzr
   47b4c:	cmp	w8, #0x0
   47b50:	cset	w4, gt
   47b54:	mov	x3, x21
   47b58:	bl	c760 <__gmpn_sub_nc@plt>
   47b5c:	ldr	x9, [x19, #56]
   47b60:	ldr	x8, [x28]
   47b64:	subs	x8, x8, x9
   47b68:	str	x8, [x28]
   47b6c:	b.cs	47b8c <__gmpn_ni_invertappr@@Base+0x43c>  // b.hs, b.nlast
   47b70:	ldr	x8, [x19, #24]
   47b74:	ldr	x9, [x19, #88]
   47b78:	add	x8, x8, x9, lsl #3
   47b7c:	ldr	x9, [x8]
   47b80:	sub	x10, x9, #0x1
   47b84:	str	x10, [x8], #8
   47b88:	cbz	x9, 47b7c <__gmpn_ni_invertappr@@Base+0x42c>
   47b8c:	add	x8, x20, x23, lsl #4
   47b90:	add	x26, x8, x27
   47b94:	mov	x0, x20
   47b98:	mov	x1, x26
   47b9c:	mov	x2, x28
   47ba0:	mov	x3, x21
   47ba4:	bl	c990 <__gmpn_mul_n@plt>
   47ba8:	lsl	x28, x21, #3
   47bac:	lsl	x24, x21, #1
   47bb0:	add	x0, x20, x28
   47bb4:	sub	x3, x24, x23
   47bb8:	mov	x1, x0
   47bbc:	mov	x2, x26
   47bc0:	bl	ca70 <__gmpn_add_n@plt>
   47bc4:	ldr	x8, [x19, #72]
   47bc8:	add	x2, x25, x28
   47bcc:	ldr	x28, [x19, #40]
   47bd0:	add	x26, x24, x21
   47bd4:	lsl	x8, x8, #3
   47bd8:	add	x9, x20, x26, lsl #3
   47bdc:	mov	x4, x0
   47be0:	add	x0, x28, x8
   47be4:	add	x1, x9, x8
   47be8:	sub	x3, x23, x21
   47bec:	bl	ce90 <__gmpn_add_nc@plt>
   47bf0:	ldr	x8, [x28, x27]
   47bf4:	adds	x8, x8, x0
   47bf8:	str	x8, [x28, x27]
   47bfc:	b.cc	47854 <__gmpn_ni_invertappr@@Base+0x104>  // b.lo, b.ul, b.last
   47c00:	ldr	x8, [x19, #24]
   47c04:	ldr	x9, [x19, #88]
   47c08:	add	x8, x8, x9, lsl #3
   47c0c:	ldr	x9, [x8]
   47c10:	adds	x9, x9, #0x1
   47c14:	str	x9, [x8], #8
   47c18:	b.cs	47c0c <__gmpn_ni_invertappr@@Base+0x4bc>  // b.hs, b.nlast
   47c1c:	b	47854 <__gmpn_ni_invertappr@@Base+0x104>
   47c20:	mvn	x8, x23
   47c24:	add	x8, x26, x8
   47c28:	ldr	x8, [x20, x8, lsl #3]
   47c2c:	ldr	x0, [x19, #96]
   47c30:	cmn	x8, #0x8
   47c34:	cset	w20, hi  // hi = pmore
   47c38:	cbnz	x0, 47c70 <__gmpn_ni_invertappr@@Base+0x520>
   47c3c:	mov	x0, x20
   47c40:	mov	sp, x29
   47c44:	ldp	x20, x19, [sp, #80]
   47c48:	ldp	x22, x21, [sp, #64]
   47c4c:	ldp	x24, x23, [sp, #48]
   47c50:	ldp	x26, x25, [sp, #32]
   47c54:	ldp	x28, x27, [sp, #16]
   47c58:	ldp	x29, x30, [sp], #96
   47c5c:	ret
   47c60:	add	x0, x19, #0x60
   47c64:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   47c68:	str	x0, [x19, #16]
   47c6c:	b	47824 <__gmpn_ni_invertappr@@Base+0xd4>
   47c70:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   47c74:	b	47c3c <__gmpn_ni_invertappr@@Base+0x4ec>
   47c78:	stp	x29, x30, [sp, #-64]!
   47c7c:	stp	x20, x19, [sp, #48]
   47c80:	mov	x20, x1
   47c84:	cmp	x2, #0x1
   47c88:	mov	x19, x0
   47c8c:	stp	x24, x23, [sp, #16]
   47c90:	stp	x22, x21, [sp, #32]
   47c94:	mov	x29, sp
   47c98:	b.ne	47cb4 <__gmpn_ni_invertappr@@Base+0x564>  // b.any
   47c9c:	ldr	x0, [x20]
   47ca0:	bl	d3f0 <__gmpn_invert_limb@plt>
   47ca4:	mov	x8, x0
   47ca8:	mov	x0, xzr
   47cac:	str	x8, [x19]
   47cb0:	b	47da0 <__gmpn_ni_invertappr@@Base+0x650>
   47cb4:	lsl	x23, x2, #3
   47cb8:	mov	x22, x2
   47cbc:	mov	w1, #0xff                  	// #255
   47cc0:	mov	x0, x3
   47cc4:	mov	x2, x23
   47cc8:	mov	x21, x3
   47ccc:	bl	c5f0 <memset@plt>
   47cd0:	add	x0, x21, x23
   47cd4:	mov	x1, x20
   47cd8:	mov	x2, x22
   47cdc:	bl	c290 <__gmpn_com@plt>
   47ce0:	cmp	x22, #0x2
   47ce4:	b.ne	47d08 <__gmpn_ni_invertappr@@Base+0x5b8>  // b.any
   47ce8:	mov	w3, #0x4                   	// #4
   47cec:	mov	x0, x19
   47cf0:	mov	x1, xzr
   47cf4:	mov	x2, x21
   47cf8:	mov	x4, x20
   47cfc:	bl	c200 <__gmpn_divrem_2@plt>
   47d00:	mov	x0, xzr
   47d04:	b	47da0 <__gmpn_ni_invertappr@@Base+0x650>
   47d08:	add	x24, x20, x22, lsl #3
   47d0c:	ldur	x23, [x24, #-8]
   47d10:	mov	x0, x23
   47d14:	bl	d3f0 <__gmpn_invert_limb@plt>
   47d18:	ldur	x8, [x24, #-16]
   47d1c:	mul	x9, x0, x23
   47d20:	adds	x9, x9, x8
   47d24:	b.cc	47d40 <__gmpn_ni_invertappr@@Base+0x5f0>  // b.lo, b.ul, b.last
   47d28:	subs	x9, x9, x23
   47d2c:	cset	w10, cs  // cs = hs, nlast
   47d30:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   47d34:	mvn	x10, x10
   47d38:	add	x0, x10, x0
   47d3c:	sub	x9, x9, x11
   47d40:	umulh	x10, x8, x0
   47d44:	adds	x9, x10, x9
   47d48:	b.cc	47d70 <__gmpn_ni_invertappr@@Base+0x620>  // b.lo, b.ul, b.last
   47d4c:	cmp	x9, x23
   47d50:	sub	x5, x0, #0x1
   47d54:	b.cc	47d74 <__gmpn_ni_invertappr@@Base+0x624>  // b.lo, b.ul, b.last
   47d58:	mul	x10, x0, x8
   47d5c:	cmp	x9, x23
   47d60:	sub	x11, x0, #0x2
   47d64:	ccmp	x10, x8, #0x2, ls  // ls = plast
   47d68:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   47d6c:	b	47d74 <__gmpn_ni_invertappr@@Base+0x624>
   47d70:	mov	x5, x0
   47d74:	lsl	x2, x22, #1
   47d78:	mov	x0, x19
   47d7c:	mov	x1, x21
   47d80:	mov	x3, x20
   47d84:	mov	x4, x22
   47d88:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   47d8c:	mov	w0, #0x1                   	// #1
   47d90:	ldr	x8, [x19]
   47d94:	sub	x9, x8, #0x1
   47d98:	str	x9, [x19], #8
   47d9c:	cbz	x8, 47d90 <__gmpn_ni_invertappr@@Base+0x640>
   47da0:	ldp	x20, x19, [sp, #48]
   47da4:	ldp	x22, x21, [sp, #32]
   47da8:	ldp	x24, x23, [sp, #16]
   47dac:	ldp	x29, x30, [sp], #64
   47db0:	ret

0000000000047db4 <__gmpn_invertappr@@Base>:
   47db4:	cmp	x2, #0xa2
   47db8:	b.le	47dc0 <__gmpn_invertappr@@Base+0xc>
   47dbc:	b	d180 <__gmpn_ni_invertappr@plt>
   47dc0:	b	47c78 <__gmpn_ni_invertappr@@Base+0x528>

0000000000047dc4 <__gmpn_invert@@Base>:
   47dc4:	stp	x29, x30, [sp, #-64]!
   47dc8:	stp	x20, x19, [sp, #48]
   47dcc:	mov	x20, x1
   47dd0:	cmp	x2, #0x1
   47dd4:	mov	x19, x0
   47dd8:	stp	x24, x23, [sp, #16]
   47ddc:	stp	x22, x21, [sp, #32]
   47de0:	mov	x29, sp
   47de4:	b.ne	47df8 <__gmpn_invert@@Base+0x34>  // b.any
   47de8:	ldr	x0, [x20]
   47dec:	bl	d3f0 <__gmpn_invert_limb@plt>
   47df0:	str	x0, [x19]
   47df4:	b	47e20 <__gmpn_invert@@Base+0x5c>
   47df8:	mov	x21, x3
   47dfc:	mov	x22, x2
   47e00:	cmp	x2, #0xa1
   47e04:	b.le	47e34 <__gmpn_invert@@Base+0x70>
   47e08:	mov	x0, x19
   47e0c:	mov	x1, x20
   47e10:	mov	x2, x22
   47e14:	mov	x3, x21
   47e18:	bl	d180 <__gmpn_ni_invertappr@plt>
   47e1c:	cbnz	x0, 47f1c <__gmpn_invert@@Base+0x158>
   47e20:	ldp	x20, x19, [sp, #48]
   47e24:	ldp	x22, x21, [sp, #32]
   47e28:	ldp	x24, x23, [sp, #16]
   47e2c:	ldp	x29, x30, [sp], #64
   47e30:	ret
   47e34:	lsl	x23, x22, #3
   47e38:	mov	w1, #0xff                  	// #255
   47e3c:	mov	x0, x21
   47e40:	mov	x2, x23
   47e44:	bl	c5f0 <memset@plt>
   47e48:	add	x0, x21, x23
   47e4c:	mov	x1, x20
   47e50:	mov	x2, x22
   47e54:	bl	c290 <__gmpn_com@plt>
   47e58:	cmp	x22, #0x2
   47e5c:	b.ne	47e88 <__gmpn_invert@@Base+0xc4>  // b.any
   47e60:	mov	x0, x19
   47e64:	mov	x2, x21
   47e68:	mov	x4, x20
   47e6c:	ldp	x20, x19, [sp, #48]
   47e70:	ldp	x22, x21, [sp, #32]
   47e74:	ldp	x24, x23, [sp, #16]
   47e78:	mov	w3, #0x4                   	// #4
   47e7c:	mov	x1, xzr
   47e80:	ldp	x29, x30, [sp], #64
   47e84:	b	c200 <__gmpn_divrem_2@plt>
   47e88:	add	x24, x20, x22, lsl #3
   47e8c:	ldur	x23, [x24, #-8]
   47e90:	mov	x0, x23
   47e94:	bl	d3f0 <__gmpn_invert_limb@plt>
   47e98:	ldur	x8, [x24, #-16]
   47e9c:	mul	x9, x0, x23
   47ea0:	adds	x9, x9, x8
   47ea4:	b.cc	47ec0 <__gmpn_invert@@Base+0xfc>  // b.lo, b.ul, b.last
   47ea8:	subs	x9, x9, x23
   47eac:	cset	w10, cs  // cs = hs, nlast
   47eb0:	csel	x11, x23, xzr, cs  // cs = hs, nlast
   47eb4:	mvn	x10, x10
   47eb8:	add	x0, x10, x0
   47ebc:	sub	x9, x9, x11
   47ec0:	umulh	x10, x8, x0
   47ec4:	adds	x9, x10, x9
   47ec8:	b.cc	47ef0 <__gmpn_invert@@Base+0x12c>  // b.lo, b.ul, b.last
   47ecc:	cmp	x9, x23
   47ed0:	sub	x5, x0, #0x1
   47ed4:	b.cc	47ef4 <__gmpn_invert@@Base+0x130>  // b.lo, b.ul, b.last
   47ed8:	mul	x10, x0, x8
   47edc:	cmp	x9, x23
   47ee0:	sub	x11, x0, #0x2
   47ee4:	ccmp	x10, x8, #0x2, ls  // ls = plast
   47ee8:	csel	x5, x5, x11, cc  // cc = lo, ul, last
   47eec:	b	47ef4 <__gmpn_invert@@Base+0x130>
   47ef0:	mov	x5, x0
   47ef4:	lsl	x2, x22, #1
   47ef8:	mov	x0, x19
   47efc:	mov	x1, x21
   47f00:	mov	x3, x20
   47f04:	mov	x4, x22
   47f08:	ldp	x20, x19, [sp, #48]
   47f0c:	ldp	x22, x21, [sp, #32]
   47f10:	ldp	x24, x23, [sp, #16]
   47f14:	ldp	x29, x30, [sp], #64
   47f18:	b	cee0 <__gmpn_sbpi1_div_q@plt>
   47f1c:	mov	x0, x21
   47f20:	mov	x1, x19
   47f24:	mov	x2, x20
   47f28:	mov	x3, x22
   47f2c:	bl	c990 <__gmpn_mul_n@plt>
   47f30:	mov	x0, x21
   47f34:	mov	x1, x21
   47f38:	mov	x2, x20
   47f3c:	mov	x3, x22
   47f40:	bl	ca70 <__gmpn_add_n@plt>
   47f44:	cbz	x0, 47f8c <__gmpn_invert@@Base+0x1c8>
   47f48:	mov	x4, x0
   47f4c:	add	x0, x21, x22, lsl #3
   47f50:	mov	x1, x0
   47f54:	mov	x2, x20
   47f58:	mov	x3, x22
   47f5c:	bl	ce90 <__gmpn_add_nc@plt>
   47f60:	eor	x8, x0, #0x1
   47f64:	ldr	x9, [x19]
   47f68:	adds	x8, x9, x8
   47f6c:	str	x8, [x19]
   47f70:	b.cc	47e20 <__gmpn_invert@@Base+0x5c>  // b.lo, b.ul, b.last
   47f74:	add	x8, x19, #0x8
   47f78:	ldr	x9, [x8]
   47f7c:	adds	x9, x9, #0x1
   47f80:	str	x9, [x8], #8
   47f84:	b.cs	47f78 <__gmpn_invert@@Base+0x1b4>  // b.hs, b.nlast
   47f88:	b	47e20 <__gmpn_invert@@Base+0x5c>
   47f8c:	mov	w8, #0x1                   	// #1
   47f90:	b	47f64 <__gmpn_invert@@Base+0x1a0>

0000000000047f94 <__gmpn_binvert_itch@@Base>:
   47f94:	stp	x29, x30, [sp, #-32]!
   47f98:	str	x19, [sp, #16]
   47f9c:	mov	x29, sp
   47fa0:	mov	x19, x0
   47fa4:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   47fa8:	add	x8, x19, #0x1
   47fac:	asr	x9, x0, #1
   47fb0:	cmp	x9, x8, asr #1
   47fb4:	csel	x8, x0, x9, lt  // lt = tstop
   47fb8:	cmp	x9, x19
   47fbc:	ldr	x19, [sp, #16]
   47fc0:	csel	x8, x8, xzr, lt  // lt = tstop
   47fc4:	add	x8, x8, x0, lsl #1
   47fc8:	add	x0, x8, #0x4
   47fcc:	ldp	x29, x30, [sp], #32
   47fd0:	ret

0000000000047fd4 <__gmpn_binvert@@Base>:
   47fd4:	sub	sp, sp, #0x1b0
   47fd8:	stp	x28, x27, [sp, #352]
   47fdc:	stp	x22, x21, [sp, #400]
   47fe0:	stp	x20, x19, [sp, #416]
   47fe4:	mov	x19, x3
   47fe8:	mov	x20, x2
   47fec:	mov	x21, x1
   47ff0:	cmp	x2, #0xc2
   47ff4:	mov	x22, x0
   47ff8:	add	x27, sp, #0x8
   47ffc:	stp	x29, x30, [sp, #336]
   48000:	stp	x26, x25, [sp, #368]
   48004:	stp	x24, x23, [sp, #384]
   48008:	add	x29, sp, #0x150
   4800c:	b.lt	480a0 <__gmpn_binvert@@Base+0xcc>  // b.tstop
   48010:	mov	x8, x20
   48014:	add	x9, x8, #0x1
   48018:	asr	x23, x9, #1
   4801c:	cmp	x8, #0x182
   48020:	str	x8, [x27], #8
   48024:	mov	x8, x23
   48028:	b.gt	48014 <__gmpn_binvert@@Base+0x40>
   4802c:	cbz	x23, 48040 <__gmpn_binvert@@Base+0x6c>
   48030:	lsl	x2, x23, #3
   48034:	mov	x0, x19
   48038:	mov	w1, wzr
   4803c:	bl	c5f0 <memset@plt>
   48040:	mov	w8, #0x1                   	// #1
   48044:	str	x8, [x19]
   48048:	ldr	x8, [x21]
   4804c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   48050:	ldr	x9, [x9, #3952]
   48054:	orr	x11, xzr, #0xfffffffffffffffe
   48058:	ubfx	x10, x8, #1, #7
   4805c:	cmp	x23, #0x5c
   48060:	ldrb	w9, [x9, x10]
   48064:	mov	w10, #0x2                   	// #2
   48068:	mov	x0, x22
   4806c:	mov	x1, x19
   48070:	msub	x12, x8, x9, x10
   48074:	mul	x9, x12, x9
   48078:	msub	x10, x9, x8, x10
   4807c:	mul	x9, x9, x10
   48080:	madd	x8, x9, x8, x11
   48084:	mul	x5, x8, x9
   48088:	mov	x2, x23
   4808c:	mov	x3, x21
   48090:	mov	x4, x23
   48094:	b.le	480ac <__gmpn_binvert@@Base+0xd8>
   48098:	bl	ce10 <__gmpn_dcpi1_bdiv_q@plt>
   4809c:	b	480b0 <__gmpn_binvert@@Base+0xdc>
   480a0:	mov	x23, x20
   480a4:	cbnz	x23, 48030 <__gmpn_binvert@@Base+0x5c>
   480a8:	b	48040 <__gmpn_binvert@@Base+0x6c>
   480ac:	bl	c510 <__gmpn_sbpi1_bdiv_q@plt>
   480b0:	ldr	x10, [x22]
   480b4:	mov	x8, x22
   480b8:	mov	x9, x23
   480bc:	cbnz	x10, 480dc <__gmpn_binvert@@Base+0x108>
   480c0:	mov	x9, x23
   480c4:	mov	x8, x22
   480c8:	subs	x9, x9, #0x1
   480cc:	str	xzr, [x8]
   480d0:	b.eq	480f8 <__gmpn_binvert@@Base+0x124>  // b.none
   480d4:	ldr	x10, [x8, #8]!
   480d8:	cbz	x10, 480c8 <__gmpn_binvert@@Base+0xf4>
   480dc:	neg	x10, x10
   480e0:	subs	x2, x9, #0x1
   480e4:	str	x10, [x8]
   480e8:	b.eq	480f8 <__gmpn_binvert@@Base+0x124>  // b.none
   480ec:	add	x0, x8, #0x8
   480f0:	mov	x1, x0
   480f4:	bl	c290 <__gmpn_com@plt>
   480f8:	cmp	x23, x20
   480fc:	b.ge	4833c <__gmpn_binvert@@Base+0x368>  // b.tcont
   48100:	add	x28, x19, #0x8
   48104:	b	48114 <__gmpn_binvert@@Base+0x140>
   48108:	cmp	x24, x20
   4810c:	mov	x23, x24
   48110:	b.ge	4833c <__gmpn_binvert@@Base+0x368>  // b.tcont
   48114:	ldr	x24, [x27, #-8]!
   48118:	mov	x0, x24
   4811c:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   48120:	mov	x25, x0
   48124:	add	x26, x19, x0, lsl #3
   48128:	mov	x0, x19
   4812c:	mov	x1, x25
   48130:	mov	x2, x21
   48134:	mov	x3, x24
   48138:	mov	x4, x22
   4813c:	mov	x5, x23
   48140:	mov	x6, x26
   48144:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   48148:	ldr	x8, [x19]
   4814c:	sub	x9, x24, x25
   48150:	add	x10, x9, x23
   48154:	sub	x9, x8, #0x1
   48158:	str	x9, [x26]
   4815c:	cbz	x8, 481c4 <__gmpn_binvert@@Base+0x1f0>
   48160:	cbz	x25, 482e0 <__gmpn_binvert@@Base+0x30c>
   48164:	cmp	x10, #0x2
   48168:	b.lt	482e0 <__gmpn_binvert@@Base+0x30c>  // b.tstop
   4816c:	add	x8, x23, x24
   48170:	mvn	x9, x25
   48174:	add	x9, x9, x8
   48178:	cmp	x9, #0x4
   4817c:	b.cc	4819c <__gmpn_binvert@@Base+0x1c8>  // b.lo, b.ul, b.last
   48180:	add	x11, x28, x25, lsl #3
   48184:	add	x10, x19, x10, lsl #3
   48188:	cmp	x11, x10
   4818c:	b.cs	482a4 <__gmpn_binvert@@Base+0x2d0>  // b.hs, b.nlast
   48190:	add	x10, x19, x8, lsl #3
   48194:	cmp	x28, x10
   48198:	b.cs	482a4 <__gmpn_binvert@@Base+0x2d0>  // b.hs, b.nlast
   4819c:	mov	w10, #0x1                   	// #1
   481a0:	sub	x8, x8, x25
   481a4:	sub	x8, x8, x10
   481a8:	add	x9, x19, x10, lsl #3
   481ac:	ldr	x10, [x9]
   481b0:	subs	x8, x8, #0x1
   481b4:	str	x10, [x9, x25, lsl #3]
   481b8:	add	x9, x9, #0x8
   481bc:	b.ne	481ac <__gmpn_binvert@@Base+0x1d8>  // b.any
   481c0:	b	482e0 <__gmpn_binvert@@Base+0x30c>
   481c4:	add	x8, x23, x24
   481c8:	sub	x8, x8, x25
   481cc:	mov	x11, x28
   481d0:	sub	x13, x8, #0x2
   481d4:	mov	w8, #0x1                   	// #1
   481d8:	cmp	x8, x10
   481dc:	b.ge	482e0 <__gmpn_binvert@@Base+0x30c>  // b.tcont
   481e0:	mov	x12, x13
   481e4:	lsl	x13, x8, #3
   481e8:	ldr	x14, [x19, x13]
   481ec:	mov	x9, x11
   481f0:	add	x8, x8, #0x1
   481f4:	add	x11, x11, #0x8
   481f8:	sub	x15, x14, #0x1
   481fc:	str	x15, [x26, x13]
   48200:	sub	x13, x12, #0x1
   48204:	cbz	x14, 481d8 <__gmpn_binvert@@Base+0x204>
   48208:	cbz	x25, 482e0 <__gmpn_binvert@@Base+0x30c>
   4820c:	cmp	x8, x10
   48210:	b.ge	482e0 <__gmpn_binvert@@Base+0x30c>  // b.tcont
   48214:	add	x14, x23, x24
   48218:	sub	x11, x14, x25
   4821c:	sub	x13, x11, x8
   48220:	cmp	x13, #0x4
   48224:	b.cc	48284 <__gmpn_binvert@@Base+0x2b0>  // b.lo, b.ul, b.last
   48228:	add	x15, x25, x8
   4822c:	add	x15, x19, x15, lsl #3
   48230:	add	x10, x19, x10, lsl #3
   48234:	cmp	x15, x10
   48238:	b.cs	4824c <__gmpn_binvert@@Base+0x278>  // b.hs, b.nlast
   4823c:	add	x10, x19, x14, lsl #3
   48240:	add	x14, x19, x8, lsl #3
   48244:	cmp	x14, x10
   48248:	b.cc	48284 <__gmpn_binvert@@Base+0x2b0>  // b.lo, b.ul, b.last
   4824c:	and	x10, x13, #0xfffffffffffffffc
   48250:	lsl	x14, x25, #3
   48254:	add	x8, x8, x10
   48258:	and	x12, x12, #0xfffffffffffffffc
   4825c:	ldur	q0, [x9, #8]
   48260:	ldur	q1, [x9, #24]
   48264:	add	x15, x9, x14
   48268:	subs	x12, x12, #0x4
   4826c:	add	x9, x9, #0x20
   48270:	stur	q0, [x15, #8]
   48274:	stur	q1, [x15, #24]
   48278:	b.ne	4825c <__gmpn_binvert@@Base+0x288>  // b.any
   4827c:	cmp	x13, x10
   48280:	b.eq	482e0 <__gmpn_binvert@@Base+0x30c>  // b.none
   48284:	sub	x9, x11, x8
   48288:	add	x8, x19, x8, lsl #3
   4828c:	ldr	x10, [x8]
   48290:	subs	x9, x9, #0x1
   48294:	str	x10, [x8, x25, lsl #3]
   48298:	add	x8, x8, #0x8
   4829c:	b.ne	4828c <__gmpn_binvert@@Base+0x2b8>  // b.any
   482a0:	b	482e0 <__gmpn_binvert@@Base+0x30c>
   482a4:	and	x11, x9, #0xfffffffffffffffc
   482a8:	orr	x10, x11, #0x1
   482ac:	lsl	x12, x25, #3
   482b0:	mov	x13, x11
   482b4:	mov	x14, x19
   482b8:	ldur	q0, [x14, #8]
   482bc:	ldur	q1, [x14, #24]
   482c0:	add	x15, x14, x12
   482c4:	subs	x13, x13, #0x4
   482c8:	add	x14, x14, #0x20
   482cc:	stur	q0, [x15, #8]
   482d0:	stur	q1, [x15, #24]
   482d4:	b.ne	482b8 <__gmpn_binvert@@Base+0x2e4>  // b.any
   482d8:	cmp	x9, x11
   482dc:	b.ne	481a0 <__gmpn_binvert@@Base+0x1cc>  // b.any
   482e0:	lsl	x8, x23, #3
   482e4:	add	x25, x22, x8
   482e8:	sub	x23, x24, x23
   482ec:	add	x2, x19, x8
   482f0:	mov	x0, x25
   482f4:	mov	x1, x22
   482f8:	mov	x3, x23
   482fc:	bl	cec0 <__gmpn_mullo_n@plt>
   48300:	ldr	x8, [x25]
   48304:	cbnz	x8, 4831c <__gmpn_binvert@@Base+0x348>
   48308:	subs	x23, x23, #0x1
   4830c:	str	xzr, [x25]
   48310:	b.eq	48108 <__gmpn_binvert@@Base+0x134>  // b.none
   48314:	ldr	x8, [x25, #8]!
   48318:	cbz	x8, 48308 <__gmpn_binvert@@Base+0x334>
   4831c:	neg	x8, x8
   48320:	subs	x2, x23, #0x1
   48324:	str	x8, [x25]
   48328:	b.eq	48108 <__gmpn_binvert@@Base+0x134>  // b.none
   4832c:	add	x0, x25, #0x8
   48330:	mov	x1, x0
   48334:	bl	c290 <__gmpn_com@plt>
   48338:	b	48108 <__gmpn_binvert@@Base+0x134>
   4833c:	ldp	x20, x19, [sp, #416]
   48340:	ldp	x22, x21, [sp, #400]
   48344:	ldp	x24, x23, [sp, #384]
   48348:	ldp	x26, x25, [sp, #368]
   4834c:	ldp	x28, x27, [sp, #352]
   48350:	ldp	x29, x30, [sp, #336]
   48354:	add	sp, sp, #0x1b0
   48358:	ret

000000000004835c <__gmpn_bc_mulmod_bnm1@@Base>:
   4835c:	stp	x29, x30, [sp, #-48]!
   48360:	stp	x20, x19, [sp, #32]
   48364:	mov	x19, x0
   48368:	mov	x0, x4
   4836c:	str	x21, [sp, #16]
   48370:	mov	x29, sp
   48374:	mov	x20, x4
   48378:	mov	x21, x3
   4837c:	bl	c990 <__gmpn_mul_n@plt>
   48380:	add	x2, x20, x21, lsl #3
   48384:	mov	x0, x19
   48388:	mov	x1, x20
   4838c:	mov	x3, x21
   48390:	bl	ca70 <__gmpn_add_n@plt>
   48394:	ldr	x8, [x19]
   48398:	adds	x8, x8, x0
   4839c:	str	x8, [x19]
   483a0:	b.cc	483b8 <__gmpn_bc_mulmod_bnm1@@Base+0x5c>  // b.lo, b.ul, b.last
   483a4:	add	x8, x19, #0x8
   483a8:	ldr	x9, [x8]
   483ac:	adds	x9, x9, #0x1
   483b0:	str	x9, [x8], #8
   483b4:	b.cs	483a8 <__gmpn_bc_mulmod_bnm1@@Base+0x4c>  // b.hs, b.nlast
   483b8:	ldp	x20, x19, [sp, #32]
   483bc:	ldr	x21, [sp, #16]
   483c0:	ldp	x29, x30, [sp], #48
   483c4:	ret

00000000000483c8 <__gmpn_mulmod_bnm1@@Base>:
   483c8:	sub	sp, sp, #0x80
   483cc:	stp	x28, x27, [sp, #48]
   483d0:	stp	x26, x25, [sp, #64]
   483d4:	stp	x22, x21, [sp, #96]
   483d8:	stp	x20, x19, [sp, #112]
   483dc:	mov	x20, x6
   483e0:	mov	x22, x5
   483e4:	mov	x25, x4
   483e8:	mov	x28, x3
   483ec:	mov	x26, x2
   483f0:	mov	x21, x1
   483f4:	cmp	x1, #0xa
   483f8:	mov	x19, x0
   483fc:	stp	x29, x30, [sp, #32]
   48400:	stp	x24, x23, [sp, #80]
   48404:	add	x29, sp, #0x20
   48408:	b.lt	48478 <__gmpn_mulmod_bnm1@@Base+0xb0>  // b.tstop
   4840c:	tbnz	w21, #0, 48478 <__gmpn_mulmod_bnm1@@Base+0xb0>
   48410:	lsr	x24, x21, #1
   48414:	cmp	x24, x28
   48418:	lsl	x23, x24, #3
   4841c:	stur	x28, [x29, #-8]
   48420:	str	x23, [sp, #16]
   48424:	b.ge	48980 <__gmpn_mulmod_bnm1@@Base+0x5b8>  // b.tcont
   48428:	subs	x28, x28, x24
   4842c:	add	x2, x26, x24, lsl #3
   48430:	str	x2, [sp, #8]
   48434:	b.eq	484d0 <__gmpn_mulmod_bnm1@@Base+0x108>  // b.none
   48438:	mov	x0, x20
   4843c:	mov	x1, x26
   48440:	mov	x3, x28
   48444:	bl	ca70 <__gmpn_add_n@plt>
   48448:	mov	x8, x28
   4844c:	cbz	x0, 484d4 <__gmpn_mulmod_bnm1@@Base+0x10c>
   48450:	mov	x8, x28
   48454:	cmp	x8, x24
   48458:	b.ge	4856c <__gmpn_mulmod_bnm1@@Base+0x1a4>  // b.tcont
   4845c:	lsl	x9, x8, #3
   48460:	ldr	x10, [x26, x9]
   48464:	add	x8, x8, #0x1
   48468:	adds	x10, x10, #0x1
   4846c:	str	x10, [x20, x9]
   48470:	b.cs	48454 <__gmpn_mulmod_bnm1@@Base+0x8c>  // b.hs, b.nlast
   48474:	b	484d4 <__gmpn_mulmod_bnm1@@Base+0x10c>
   48478:	cmp	x22, x21
   4847c:	b.lt	489b0 <__gmpn_mulmod_bnm1@@Base+0x5e8>  // b.tstop
   48480:	mov	x0, x20
   48484:	mov	x1, x26
   48488:	mov	x2, x25
   4848c:	mov	x3, x21
   48490:	bl	c990 <__gmpn_mul_n@plt>
   48494:	add	x2, x20, x21, lsl #3
   48498:	mov	x0, x19
   4849c:	mov	x1, x20
   484a0:	mov	x3, x21
   484a4:	bl	ca70 <__gmpn_add_n@plt>
   484a8:	ldr	x8, [x19]
   484ac:	adds	x8, x8, x0
   484b0:	str	x8, [x19]
   484b4:	b.cc	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.lo, b.ul, b.last
   484b8:	add	x8, x19, #0x8
   484bc:	ldr	x9, [x8]
   484c0:	adds	x9, x9, #0x1
   484c4:	str	x9, [x8], #8
   484c8:	b.cs	484bc <__gmpn_mulmod_bnm1@@Base+0xf4>  // b.hs, b.nlast
   484cc:	b	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>
   484d0:	mov	x8, xzr
   484d4:	cmp	x20, x26
   484d8:	b.eq	48580 <__gmpn_mulmod_bnm1@@Base+0x1b8>  // b.none
   484dc:	subs	x9, x24, x8
   484e0:	b.le	48580 <__gmpn_mulmod_bnm1@@Base+0x1b8>
   484e4:	cmp	x9, #0x4
   484e8:	b.cc	48548 <__gmpn_mulmod_bnm1@@Base+0x180>  // b.lo, b.ul, b.last
   484ec:	lsl	x11, x8, #3
   484f0:	add	x10, x20, x11
   484f4:	add	x12, x26, x23
   484f8:	cmp	x10, x12
   484fc:	b.cs	48510 <__gmpn_mulmod_bnm1@@Base+0x148>  // b.hs, b.nlast
   48500:	add	x10, x20, x23
   48504:	add	x12, x26, x11
   48508:	cmp	x12, x10
   4850c:	b.cc	48548 <__gmpn_mulmod_bnm1@@Base+0x180>  // b.lo, b.ul, b.last
   48510:	and	x10, x9, #0xfffffffffffffffc
   48514:	add	x12, x11, #0x10
   48518:	add	x8, x8, x10
   4851c:	add	x11, x26, x12
   48520:	add	x12, x20, x12
   48524:	mov	x13, x10
   48528:	ldp	q0, q1, [x11, #-16]
   4852c:	add	x11, x11, #0x20
   48530:	subs	x13, x13, #0x4
   48534:	stp	q0, q1, [x12, #-16]
   48538:	add	x12, x12, #0x20
   4853c:	b.ne	48528 <__gmpn_mulmod_bnm1@@Base+0x160>  // b.any
   48540:	cmp	x9, x10
   48544:	b.eq	48580 <__gmpn_mulmod_bnm1@@Base+0x1b8>  // b.none
   48548:	lsl	x10, x8, #3
   4854c:	sub	x9, x24, x8
   48550:	add	x8, x20, x10
   48554:	add	x10, x26, x10
   48558:	ldr	x11, [x10], #8
   4855c:	subs	x9, x9, #0x1
   48560:	str	x11, [x8], #8
   48564:	b.ne	48558 <__gmpn_mulmod_bnm1@@Base+0x190>  // b.any
   48568:	b	48580 <__gmpn_mulmod_bnm1@@Base+0x1b8>
   4856c:	mov	x8, x20
   48570:	ldr	x9, [x8]
   48574:	adds	x9, x9, #0x1
   48578:	str	x9, [x8], #8
   4857c:	b.cs	48570 <__gmpn_mulmod_bnm1@@Base+0x1a8>  // b.hs, b.nlast
   48580:	cmp	x24, x22
   48584:	add	x27, x20, x24, lsl #3
   48588:	b.ge	48ac8 <__gmpn_mulmod_bnm1@@Base+0x700>  // b.tcont
   4858c:	subs	x23, x22, x24
   48590:	b.eq	485cc <__gmpn_mulmod_bnm1@@Base+0x204>  // b.none
   48594:	add	x2, x25, x24, lsl #3
   48598:	mov	x0, x27
   4859c:	mov	x1, x25
   485a0:	mov	x3, x23
   485a4:	bl	ca70 <__gmpn_add_n@plt>
   485a8:	cbz	x0, 485cc <__gmpn_mulmod_bnm1@@Base+0x204>
   485ac:	add	x8, x20, x22, lsl #3
   485b0:	cmp	x23, x24
   485b4:	b.ge	48664 <__gmpn_mulmod_bnm1@@Base+0x29c>  // b.tcont
   485b8:	ldr	x9, [x25, x23, lsl #3]
   485bc:	add	x23, x23, #0x1
   485c0:	adds	x9, x9, #0x1
   485c4:	str	x9, [x8], #8
   485c8:	b.cs	485b0 <__gmpn_mulmod_bnm1@@Base+0x1e8>  // b.hs, b.nlast
   485cc:	cmp	x27, x25
   485d0:	b.eq	48678 <__gmpn_mulmod_bnm1@@Base+0x2b0>  // b.none
   485d4:	subs	x8, x24, x23
   485d8:	b.le	48678 <__gmpn_mulmod_bnm1@@Base+0x2b0>
   485dc:	cmp	x8, #0x4
   485e0:	b.cc	48640 <__gmpn_mulmod_bnm1@@Base+0x278>  // b.lo, b.ul, b.last
   485e4:	add	x9, x23, x24
   485e8:	add	x11, x20, x9, lsl #3
   485ec:	add	x9, x25, x24, lsl #3
   485f0:	cmp	x11, x9
   485f4:	add	x10, x25, x23, lsl #3
   485f8:	b.cs	4860c <__gmpn_mulmod_bnm1@@Base+0x244>  // b.hs, b.nlast
   485fc:	and	x9, x21, #0x1ffffffffffffffe
   48600:	add	x9, x20, x9, lsl #3
   48604:	cmp	x10, x9
   48608:	b.cc	48640 <__gmpn_mulmod_bnm1@@Base+0x278>  // b.lo, b.ul, b.last
   4860c:	and	x9, x8, #0xfffffffffffffffc
   48610:	add	x10, x10, #0x10
   48614:	add	x23, x23, x9
   48618:	add	x11, x11, #0x10
   4861c:	mov	x12, x9
   48620:	ldp	q0, q1, [x10, #-16]
   48624:	add	x10, x10, #0x20
   48628:	subs	x12, x12, #0x4
   4862c:	stp	q0, q1, [x11, #-16]
   48630:	add	x11, x11, #0x20
   48634:	b.ne	48620 <__gmpn_mulmod_bnm1@@Base+0x258>  // b.any
   48638:	cmp	x8, x9
   4863c:	b.eq	48678 <__gmpn_mulmod_bnm1@@Base+0x2b0>  // b.none
   48640:	add	x9, x23, x24
   48644:	sub	x8, x24, x23
   48648:	add	x9, x20, x9, lsl #3
   4864c:	add	x10, x25, x23, lsl #3
   48650:	ldr	x11, [x10], #8
   48654:	subs	x8, x8, #0x1
   48658:	str	x11, [x9], #8
   4865c:	b.ne	48650 <__gmpn_mulmod_bnm1@@Base+0x288>  // b.any
   48660:	b	48678 <__gmpn_mulmod_bnm1@@Base+0x2b0>
   48664:	mov	x8, x27
   48668:	ldr	x9, [x8]
   4866c:	adds	x9, x9, #0x1
   48670:	str	x9, [x8], #8
   48674:	b.cs	48668 <__gmpn_mulmod_bnm1@@Base+0x2a0>  // b.hs, b.nlast
   48678:	add	x6, x27, x24, lsl #3
   4867c:	mov	x5, x24
   48680:	mov	x0, x19
   48684:	mov	x1, x24
   48688:	mov	x2, x20
   4868c:	mov	x3, x24
   48690:	mov	x4, x27
   48694:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   48698:	and	x23, x21, #0xfffffffffffffffe
   4869c:	add	x8, x20, x23, lsl #3
   486a0:	add	x27, x8, #0x10
   486a4:	cbz	x28, 486ec <__gmpn_mulmod_bnm1@@Base+0x324>
   486a8:	ldr	x2, [sp, #8]
   486ac:	mov	x0, x27
   486b0:	mov	x1, x26
   486b4:	mov	x3, x28
   486b8:	bl	c2d0 <__gmpn_sub_n@plt>
   486bc:	cbz	x0, 486ec <__gmpn_mulmod_bnm1@@Base+0x324>
   486c0:	ldur	x8, [x29, #-8]
   486c4:	add	x8, x8, x24
   486c8:	add	x8, x20, x8, lsl #3
   486cc:	add	x8, x8, #0x10
   486d0:	cmp	x28, x24
   486d4:	b.ge	48960 <__gmpn_mulmod_bnm1@@Base+0x598>  // b.tcont
   486d8:	ldr	x9, [x26, x28, lsl #3]
   486dc:	add	x28, x28, #0x1
   486e0:	sub	x10, x9, #0x1
   486e4:	str	x10, [x8], #8
   486e8:	cbz	x9, 486d0 <__gmpn_mulmod_bnm1@@Base+0x308>
   486ec:	cmp	x27, x26
   486f0:	b.eq	4878c <__gmpn_mulmod_bnm1@@Base+0x3c4>  // b.none
   486f4:	subs	x8, x24, x28
   486f8:	b.le	4878c <__gmpn_mulmod_bnm1@@Base+0x3c4>
   486fc:	cmp	x8, #0x4
   48700:	b.cc	48768 <__gmpn_mulmod_bnm1@@Base+0x3a0>  // b.lo, b.ul, b.last
   48704:	add	x9, x28, x23
   48708:	add	x11, x20, x9, lsl #3
   4870c:	add	x9, x11, #0x10
   48710:	add	x10, x26, x24, lsl #3
   48714:	cmp	x9, x10
   48718:	add	x10, x26, x28, lsl #3
   4871c:	b.cs	48734 <__gmpn_mulmod_bnm1@@Base+0x36c>  // b.hs, b.nlast
   48720:	mov	w9, #0x18                  	// #24
   48724:	madd	x9, x24, x9, x20
   48728:	add	x9, x9, #0x10
   4872c:	cmp	x10, x9
   48730:	b.cc	48768 <__gmpn_mulmod_bnm1@@Base+0x3a0>  // b.lo, b.ul, b.last
   48734:	and	x9, x8, #0xfffffffffffffffc
   48738:	add	x10, x10, #0x10
   4873c:	add	x28, x28, x9
   48740:	add	x11, x11, #0x20
   48744:	mov	x12, x9
   48748:	ldp	q0, q1, [x10, #-16]
   4874c:	subs	x12, x12, #0x4
   48750:	add	x10, x10, #0x20
   48754:	stp	q0, q1, [x11, #-16]
   48758:	add	x11, x11, #0x20
   4875c:	b.ne	48748 <__gmpn_mulmod_bnm1@@Base+0x380>  // b.any
   48760:	cmp	x8, x9
   48764:	b.eq	4878c <__gmpn_mulmod_bnm1@@Base+0x3c4>  // b.none
   48768:	add	x9, x28, x23
   4876c:	add	x9, x20, x9, lsl #3
   48770:	sub	x8, x24, x28
   48774:	add	x9, x9, #0x10
   48778:	add	x10, x26, x28, lsl #3
   4877c:	ldr	x11, [x10], #8
   48780:	subs	x8, x8, #0x1
   48784:	str	x11, [x9], #8
   48788:	b.ne	4877c <__gmpn_mulmod_bnm1@@Base+0x3b4>  // b.any
   4878c:	mov	x8, xzr
   48790:	str	xzr, [x27, x24, lsl #3]
   48794:	cmp	x24, x22
   48798:	add	x8, x8, x24
   4879c:	str	x8, [sp, #8]
   487a0:	b.ge	48ad8 <__gmpn_mulmod_bnm1@@Base+0x710>  // b.tcont
   487a4:	add	x8, x27, x24, lsl #3
   487a8:	subs	x28, x22, x24
   487ac:	add	x26, x8, #0x8
   487b0:	b.eq	487f8 <__gmpn_mulmod_bnm1@@Base+0x430>  // b.none
   487b4:	add	x2, x25, x24, lsl #3
   487b8:	mov	x0, x26
   487bc:	mov	x1, x25
   487c0:	mov	x3, x28
   487c4:	bl	c2d0 <__gmpn_sub_n@plt>
   487c8:	cbz	x0, 487f8 <__gmpn_mulmod_bnm1@@Base+0x430>
   487cc:	add	x8, x22, x23
   487d0:	add	x8, x20, x8, lsl #3
   487d4:	add	x8, x8, #0x18
   487d8:	mov	w9, #0x1                   	// #1
   487dc:	cmp	x28, x24
   487e0:	b.ge	488a8 <__gmpn_mulmod_bnm1@@Base+0x4e0>  // b.tcont
   487e4:	ldr	x10, [x25, x28, lsl #3]
   487e8:	add	x28, x28, #0x1
   487ec:	sub	x11, x10, #0x1
   487f0:	str	x11, [x8], #8
   487f4:	cbz	x10, 487dc <__gmpn_mulmod_bnm1@@Base+0x414>
   487f8:	cmp	x26, x25
   487fc:	mov	x9, xzr
   48800:	b.eq	488a8 <__gmpn_mulmod_bnm1@@Base+0x4e0>  // b.none
   48804:	subs	x8, x24, x28
   48808:	b.le	488a8 <__gmpn_mulmod_bnm1@@Base+0x4e0>
   4880c:	cmp	x8, #0x4
   48810:	b.cc	4887c <__gmpn_mulmod_bnm1@@Base+0x4b4>  // b.lo, b.ul, b.last
   48814:	add	x9, x24, x24, lsl #1
   48818:	add	x9, x28, x9
   4881c:	add	x11, x20, x9, lsl #3
   48820:	add	x9, x11, #0x18
   48824:	add	x10, x25, x24, lsl #3
   48828:	cmp	x9, x10
   4882c:	add	x10, x25, x28, lsl #3
   48830:	b.cs	48848 <__gmpn_mulmod_bnm1@@Base+0x480>  // b.hs, b.nlast
   48834:	mov	w9, #0x18                  	// #24
   48838:	bfi	x9, x24, #5, #59
   4883c:	add	x9, x20, x9
   48840:	cmp	x10, x9
   48844:	b.cc	4887c <__gmpn_mulmod_bnm1@@Base+0x4b4>  // b.lo, b.ul, b.last
   48848:	and	x9, x8, #0xfffffffffffffffc
   4884c:	add	x10, x10, #0x10
   48850:	add	x28, x28, x9
   48854:	add	x11, x11, #0x28
   48858:	mov	x12, x9
   4885c:	ldp	q0, q1, [x10, #-16]
   48860:	subs	x12, x12, #0x4
   48864:	add	x10, x10, #0x20
   48868:	stp	q0, q1, [x11, #-16]
   4886c:	add	x11, x11, #0x20
   48870:	b.ne	4885c <__gmpn_mulmod_bnm1@@Base+0x494>  // b.any
   48874:	cmp	x8, x9
   48878:	b.eq	488a4 <__gmpn_mulmod_bnm1@@Base+0x4dc>  // b.none
   4887c:	add	x9, x24, x24, lsl #1
   48880:	add	x9, x28, x9
   48884:	add	x9, x20, x9, lsl #3
   48888:	sub	x8, x24, x28
   4888c:	add	x9, x9, #0x18
   48890:	add	x10, x25, x28, lsl #3
   48894:	ldr	x11, [x10], #8
   48898:	subs	x8, x8, #0x1
   4889c:	str	x11, [x9], #8
   488a0:	b.ne	48894 <__gmpn_mulmod_bnm1@@Base+0x4cc>  // b.any
   488a4:	mov	x9, xzr
   488a8:	lsl	x8, x21, #3
   488ac:	orr	x8, x8, #0x8
   488b0:	str	xzr, [x27, x8]
   488b4:	ldr	x8, [x26]
   488b8:	adds	x8, x8, x9
   488bc:	str	x8, [x26]
   488c0:	b.cc	488e0 <__gmpn_mulmod_bnm1@@Base+0x518>  // b.lo, b.ul, b.last
   488c4:	mov	w8, #0x18                  	// #24
   488c8:	madd	x8, x24, x8, x20
   488cc:	add	x8, x8, #0x20
   488d0:	ldr	x9, [x8]
   488d4:	adds	x9, x9, #0x1
   488d8:	str	x9, [x8], #8
   488dc:	b.cs	488d0 <__gmpn_mulmod_bnm1@@Base+0x508>  // b.hs, b.nlast
   488e0:	ldr	x8, [x26, x24, lsl #3]
   488e4:	add	x28, x8, x24
   488e8:	cmp	x21, #0x278
   488ec:	b.lt	48ae8 <__gmpn_mulmod_bnm1@@Base+0x720>  // b.tstop
   488f0:	mov	x0, x24
   488f4:	mov	w1, wzr
   488f8:	bl	cad0 <__gmpn_fft_best_k@plt>
   488fc:	mov	w8, #0xffffffff            	// #-1
   48900:	lsl	w8, w8, w0
   48904:	mvn	w8, w8
   48908:	sxtw	x9, w8
   4890c:	mov	w6, w0
   48910:	tst	x24, x9
   48914:	b.eq	4892c <__gmpn_mulmod_bnm1@@Base+0x564>  // b.none
   48918:	sbfx	x9, x8, #1, #31
   4891c:	asr	w8, w8, #1
   48920:	tst	x24, x9
   48924:	sub	w6, w6, #0x1
   48928:	b.ne	48918 <__gmpn_mulmod_bnm1@@Base+0x550>  // b.any
   4892c:	cmp	w6, #0x4
   48930:	b.lt	48ae8 <__gmpn_mulmod_bnm1@@Base+0x720>  // b.tstop
   48934:	ldr	x3, [sp, #8]
   48938:	mov	x0, x20
   4893c:	mov	x1, x24
   48940:	mov	x2, x27
   48944:	mov	x4, x26
   48948:	mov	x5, x28
   4894c:	bl	c300 <__gmpn_mul_fft@plt>
   48950:	ldur	x26, [x29, #-8]
   48954:	ldr	x27, [sp, #16]
   48958:	str	x0, [x20, x24, lsl #3]
   4895c:	b	48b58 <__gmpn_mulmod_bnm1@@Base+0x790>
   48960:	mov	x8, x27
   48964:	str	xzr, [x27, x24, lsl #3]
   48968:	ldr	x9, [x8]
   4896c:	adds	x9, x9, #0x1
   48970:	str	x9, [x8], #8
   48974:	b.cs	48968 <__gmpn_mulmod_bnm1@@Base+0x5a0>  // b.hs, b.nlast
   48978:	ldr	x8, [x27, x24, lsl #3]
   4897c:	b	48794 <__gmpn_mulmod_bnm1@@Base+0x3cc>
   48980:	mov	x0, x19
   48984:	mov	x1, x24
   48988:	mov	x2, x26
   4898c:	mov	x3, x28
   48990:	mov	x4, x25
   48994:	mov	x5, x22
   48998:	mov	x6, x20
   4899c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   489a0:	mov	x27, x26
   489a4:	mov	x26, x25
   489a8:	str	x28, [sp, #8]
   489ac:	b	48adc <__gmpn_mulmod_bnm1@@Base+0x714>
   489b0:	add	x23, x22, x28
   489b4:	cmp	x23, x21
   489b8:	b.le	48d4c <__gmpn_mulmod_bnm1@@Base+0x984>
   489bc:	mov	x0, x20
   489c0:	mov	x1, x26
   489c4:	mov	x2, x28
   489c8:	mov	x3, x25
   489cc:	mov	x4, x22
   489d0:	bl	ccd0 <__gmpn_mul@plt>
   489d4:	subs	x22, x23, x21
   489d8:	b.eq	48a14 <__gmpn_mulmod_bnm1@@Base+0x64c>  // b.none
   489dc:	add	x2, x20, x21, lsl #3
   489e0:	mov	x0, x19
   489e4:	mov	x1, x20
   489e8:	mov	x3, x22
   489ec:	bl	ca70 <__gmpn_add_n@plt>
   489f0:	cbz	x0, 48a14 <__gmpn_mulmod_bnm1@@Base+0x64c>
   489f4:	cmp	x22, x21
   489f8:	b.ge	48ab4 <__gmpn_mulmod_bnm1@@Base+0x6ec>  // b.tcont
   489fc:	lsl	x8, x22, #3
   48a00:	ldr	x9, [x20, x8]
   48a04:	add	x22, x22, #0x1
   48a08:	adds	x9, x9, #0x1
   48a0c:	str	x9, [x19, x8]
   48a10:	b.cs	489f4 <__gmpn_mulmod_bnm1@@Base+0x62c>  // b.hs, b.nlast
   48a14:	cmp	x19, x20
   48a18:	b.eq	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.none
   48a1c:	cmp	x22, x21
   48a20:	b.ge	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.tcont
   48a24:	sub	x8, x21, x22
   48a28:	cmp	x8, #0x4
   48a2c:	b.cc	48a90 <__gmpn_mulmod_bnm1@@Base+0x6c8>  // b.lo, b.ul, b.last
   48a30:	lsl	x10, x22, #3
   48a34:	lsl	x9, x21, #3
   48a38:	add	x11, x19, x10
   48a3c:	add	x12, x20, x9
   48a40:	cmp	x11, x12
   48a44:	b.cs	48a58 <__gmpn_mulmod_bnm1@@Base+0x690>  // b.hs, b.nlast
   48a48:	add	x9, x19, x9
   48a4c:	add	x11, x20, x10
   48a50:	cmp	x11, x9
   48a54:	b.cc	48a90 <__gmpn_mulmod_bnm1@@Base+0x6c8>  // b.lo, b.ul, b.last
   48a58:	and	x9, x8, #0xfffffffffffffffc
   48a5c:	add	x11, x10, #0x10
   48a60:	add	x22, x22, x9
   48a64:	add	x10, x20, x11
   48a68:	add	x11, x19, x11
   48a6c:	mov	x12, x9
   48a70:	ldp	q0, q1, [x10, #-16]
   48a74:	add	x10, x10, #0x20
   48a78:	subs	x12, x12, #0x4
   48a7c:	stp	q0, q1, [x11, #-16]
   48a80:	add	x11, x11, #0x20
   48a84:	b.ne	48a70 <__gmpn_mulmod_bnm1@@Base+0x6a8>  // b.any
   48a88:	cmp	x8, x9
   48a8c:	b.eq	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.none
   48a90:	lsl	x10, x22, #3
   48a94:	sub	x8, x21, x22
   48a98:	add	x9, x19, x10
   48a9c:	add	x10, x20, x10
   48aa0:	ldr	x11, [x10], #8
   48aa4:	subs	x8, x8, #0x1
   48aa8:	str	x11, [x9], #8
   48aac:	b.ne	48aa0 <__gmpn_mulmod_bnm1@@Base+0x6d8>  // b.any
   48ab0:	b	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>
   48ab4:	ldr	x8, [x19]
   48ab8:	adds	x8, x8, #0x1
   48abc:	str	x8, [x19], #8
   48ac0:	b.cs	48ab4 <__gmpn_mulmod_bnm1@@Base+0x6ec>  // b.hs, b.nlast
   48ac4:	b	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>
   48ac8:	mov	x6, x27
   48acc:	mov	x5, x22
   48ad0:	mov	x27, x25
   48ad4:	b	48680 <__gmpn_mulmod_bnm1@@Base+0x2b8>
   48ad8:	mov	x26, x25
   48adc:	mov	x28, x22
   48ae0:	cmp	x21, #0x278
   48ae4:	b.ge	488f0 <__gmpn_mulmod_bnm1@@Base+0x528>  // b.tcont
   48ae8:	cmp	x26, x25
   48aec:	b.eq	48cb0 <__gmpn_mulmod_bnm1@@Base+0x8e8>  // b.none
   48af0:	add	x3, x24, #0x1
   48af4:	mov	x0, x20
   48af8:	mov	x1, x27
   48afc:	mov	x2, x26
   48b00:	bl	c990 <__gmpn_mul_n@plt>
   48b04:	and	x8, x21, #0x1ffffffffffffffe
   48b08:	ldr	x25, [x20, x8, lsl #3]
   48b0c:	add	x23, x20, x24, lsl #3
   48b10:	mov	x0, x20
   48b14:	mov	x1, x20
   48b18:	mov	x2, x23
   48b1c:	mov	x3, x24
   48b20:	bl	c2d0 <__gmpn_sub_n@plt>
   48b24:	str	xzr, [x23]
   48b28:	ldr	x8, [x20]
   48b2c:	ldur	x26, [x29, #-8]
   48b30:	ldr	x27, [sp, #16]
   48b34:	add	x9, x0, x25
   48b38:	adds	x8, x8, x9
   48b3c:	str	x8, [x20]
   48b40:	b.cc	48b58 <__gmpn_mulmod_bnm1@@Base+0x790>  // b.lo, b.ul, b.last
   48b44:	add	x8, x20, #0x8
   48b48:	ldr	x9, [x8]
   48b4c:	adds	x9, x9, #0x1
   48b50:	str	x9, [x8], #8
   48b54:	b.cs	48b48 <__gmpn_mulmod_bnm1@@Base+0x780>  // b.hs, b.nlast
   48b58:	ldr	x23, [x20, x27]
   48b5c:	mov	x0, x19
   48b60:	mov	x1, x19
   48b64:	mov	x2, x20
   48b68:	mov	x3, x24
   48b6c:	bl	c950 <__gmpn_rsh1add_n@plt>
   48b70:	mov	x8, xzr
   48b74:	add	x9, x27, x19
   48b78:	add	x11, x0, x23
   48b7c:	ldur	x10, [x9, #-8]
   48b80:	lsr	x12, x11, #1
   48b84:	lsl	x11, x11, #63
   48b88:	adds	x13, x10, x11
   48b8c:	adc	x8, x12, x8
   48b90:	stur	x13, [x9, #-8]
   48b94:	ldr	x9, [x19]
   48b98:	adds	x8, x9, x8
   48b9c:	str	x8, [x19]
   48ba0:	b.cc	48bb8 <__gmpn_mulmod_bnm1@@Base+0x7f0>  // b.lo, b.ul, b.last
   48ba4:	add	x8, x19, #0x8
   48ba8:	ldr	x9, [x8]
   48bac:	adds	x9, x9, #0x1
   48bb0:	str	x9, [x8], #8
   48bb4:	b.cs	48ba8 <__gmpn_mulmod_bnm1@@Base+0x7e0>  // b.hs, b.nlast
   48bb8:	add	x23, x22, x26
   48bbc:	cmp	x23, x21
   48bc0:	b.lt	48c24 <__gmpn_mulmod_bnm1@@Base+0x85c>  // b.tstop
   48bc4:	ldr	x21, [x20, x27]
   48bc8:	add	x0, x19, x27
   48bcc:	mov	x1, x19
   48bd0:	mov	x2, x20
   48bd4:	mov	x3, x24
   48bd8:	bl	c2d0 <__gmpn_sub_n@plt>
   48bdc:	ldr	x8, [x19]
   48be0:	add	x9, x0, x21
   48be4:	subs	x8, x8, x9
   48be8:	str	x8, [x19]
   48bec:	b.cs	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.hs, b.nlast
   48bf0:	add	x8, x19, #0x8
   48bf4:	ldr	x9, [x8]
   48bf8:	sub	x10, x9, #0x1
   48bfc:	str	x10, [x8], #8
   48c00:	cbz	x9, 48bf4 <__gmpn_mulmod_bnm1@@Base+0x82c>
   48c04:	ldp	x20, x19, [sp, #112]
   48c08:	ldp	x22, x21, [sp, #96]
   48c0c:	ldp	x24, x23, [sp, #80]
   48c10:	ldp	x26, x25, [sp, #64]
   48c14:	ldp	x28, x27, [sp, #48]
   48c18:	ldp	x29, x30, [sp, #32]
   48c1c:	add	sp, sp, #0x80
   48c20:	ret
   48c24:	add	x0, x19, x27
   48c28:	sub	x3, x23, x24
   48c2c:	mov	x1, x19
   48c30:	mov	x2, x20
   48c34:	neg	x25, x24
   48c38:	bl	c2d0 <__gmpn_sub_n@plt>
   48c3c:	lsl	x8, x26, #3
   48c40:	lsl	x9, x22, #3
   48c44:	add	x11, x20, x8
   48c48:	ldr	x24, [x20, x27]
   48c4c:	lsl	x10, x25, #3
   48c50:	add	x8, x19, x8
   48c54:	add	x11, x11, x9
   48c58:	mov	x4, x0
   48c5c:	add	x8, x8, x9
   48c60:	add	x0, x11, x10
   48c64:	add	x1, x8, x10
   48c68:	sub	x3, x21, x23
   48c6c:	mov	x2, x0
   48c70:	bl	c760 <__gmpn_sub_nc@plt>
   48c74:	ldr	x8, [x19]
   48c78:	add	x9, x0, x24
   48c7c:	subs	x8, x8, x9
   48c80:	str	x8, [x19]
   48c84:	b.cs	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.hs, b.nlast
   48c88:	mov	w8, #0x1                   	// #1
   48c8c:	cmp	x8, x23
   48c90:	b.ge	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>  // b.tcont
   48c94:	lsl	x9, x8, #3
   48c98:	ldr	x10, [x19, x9]
   48c9c:	add	x8, x8, #0x1
   48ca0:	sub	x11, x10, #0x1
   48ca4:	str	x11, [x19, x9]
   48ca8:	cbz	x10, 48c8c <__gmpn_mulmod_bnm1@@Base+0x8c4>
   48cac:	b	48c04 <__gmpn_mulmod_bnm1@@Base+0x83c>
   48cb0:	ldr	x23, [sp, #8]
   48cb4:	mov	x0, x20
   48cb8:	mov	x1, x27
   48cbc:	mov	x3, x25
   48cc0:	mov	x2, x23
   48cc4:	mov	x4, x28
   48cc8:	bl	ccd0 <__gmpn_mul@plt>
   48ccc:	sub	x8, x23, x24
   48cd0:	add	x8, x8, x28
   48cd4:	ldur	x26, [x29, #-8]
   48cd8:	ldr	x27, [sp, #16]
   48cdc:	cmp	x8, x24
   48ce0:	cset	w9, gt
   48ce4:	subs	x25, x8, x9
   48ce8:	add	x23, x20, x24, lsl #3
   48cec:	b.eq	48d28 <__gmpn_mulmod_bnm1@@Base+0x960>  // b.none
   48cf0:	mov	x0, x20
   48cf4:	mov	x1, x20
   48cf8:	mov	x2, x23
   48cfc:	mov	x3, x25
   48d00:	bl	c2d0 <__gmpn_sub_n@plt>
   48d04:	cbz	x0, 48d28 <__gmpn_mulmod_bnm1@@Base+0x960>
   48d08:	cmp	x25, x24
   48d0c:	b.ge	48d30 <__gmpn_mulmod_bnm1@@Base+0x968>  // b.tcont
   48d10:	lsl	x8, x25, #3
   48d14:	ldr	x9, [x20, x8]
   48d18:	add	x25, x25, #0x1
   48d1c:	sub	x10, x9, #0x1
   48d20:	str	x10, [x20, x8]
   48d24:	cbz	x9, 48d08 <__gmpn_mulmod_bnm1@@Base+0x940>
   48d28:	str	xzr, [x23]
   48d2c:	b	48b58 <__gmpn_mulmod_bnm1@@Base+0x790>
   48d30:	mov	x8, x20
   48d34:	str	xzr, [x23]
   48d38:	ldr	x9, [x8]
   48d3c:	adds	x9, x9, #0x1
   48d40:	str	x9, [x8], #8
   48d44:	b.cs	48d38 <__gmpn_mulmod_bnm1@@Base+0x970>  // b.hs, b.nlast
   48d48:	b	48b58 <__gmpn_mulmod_bnm1@@Base+0x790>
   48d4c:	mov	x0, x19
   48d50:	mov	x1, x26
   48d54:	mov	x2, x28
   48d58:	mov	x3, x25
   48d5c:	mov	x4, x22
   48d60:	ldp	x20, x19, [sp, #112]
   48d64:	ldp	x22, x21, [sp, #96]
   48d68:	ldp	x24, x23, [sp, #80]
   48d6c:	ldp	x26, x25, [sp, #64]
   48d70:	ldp	x28, x27, [sp, #48]
   48d74:	ldp	x29, x30, [sp, #32]
   48d78:	add	sp, sp, #0x80
   48d7c:	b	ccd0 <__gmpn_mul@plt>

0000000000048d80 <__gmpn_mulmod_bnm1_next_size@@Base>:
   48d80:	cmp	x0, #0xa
   48d84:	b.lt	48dd8 <__gmpn_mulmod_bnm1_next_size@@Base+0x58>  // b.tstop
   48d88:	cmp	x0, #0x24
   48d8c:	b.le	48ddc <__gmpn_mulmod_bnm1_next_size@@Base+0x5c>
   48d90:	cmp	x0, #0x48
   48d94:	b.le	48de8 <__gmpn_mulmod_bnm1_next_size@@Base+0x68>
   48d98:	cmp	x0, #0x276
   48d9c:	b.le	48df4 <__gmpn_mulmod_bnm1_next_size@@Base+0x74>
   48da0:	stp	x29, x30, [sp, #-32]!
   48da4:	add	x8, x0, #0x1
   48da8:	str	x19, [sp, #16]
   48dac:	asr	x19, x8, #1
   48db0:	mov	x0, x19
   48db4:	mov	w1, wzr
   48db8:	mov	x29, sp
   48dbc:	bl	cad0 <__gmpn_fft_best_k@plt>
   48dc0:	mov	w1, w0
   48dc4:	mov	x0, x19
   48dc8:	bl	d1d0 <__gmpn_fft_next_size@plt>
   48dcc:	ldr	x19, [sp, #16]
   48dd0:	lsl	x0, x0, #1
   48dd4:	ldp	x29, x30, [sp], #32
   48dd8:	ret
   48ddc:	add	x8, x0, #0x1
   48de0:	and	x0, x8, #0xfffffffffffffffe
   48de4:	ret
   48de8:	add	x8, x0, #0x3
   48dec:	and	x0, x8, #0xfffffffffffffffc
   48df0:	ret
   48df4:	add	x8, x0, #0x7
   48df8:	and	x0, x8, #0xfffffffffffffff8
   48dfc:	ret

0000000000048e00 <__gmpn_sqrmod_bnm1@@Base>:
   48e00:	sub	sp, sp, #0x70
   48e04:	stp	x28, x27, [sp, #32]
   48e08:	stp	x24, x23, [sp, #64]
   48e0c:	stp	x22, x21, [sp, #80]
   48e10:	stp	x20, x19, [sp, #96]
   48e14:	mov	x20, x4
   48e18:	mov	x27, x3
   48e1c:	mov	x24, x2
   48e20:	mov	x21, x1
   48e24:	cmp	x1, #0xb
   48e28:	mov	x19, x0
   48e2c:	stp	x29, x30, [sp, #16]
   48e30:	stp	x26, x25, [sp, #48]
   48e34:	add	x29, sp, #0x10
   48e38:	b.lt	48ea8 <__gmpn_sqrmod_bnm1@@Base+0xa8>  // b.tstop
   48e3c:	tbnz	w21, #0, 48ea8 <__gmpn_sqrmod_bnm1@@Base+0xa8>
   48e40:	lsr	x23, x21, #1
   48e44:	cmp	x23, x27
   48e48:	lsl	x28, x23, #3
   48e4c:	b.ge	492a0 <__gmpn_sqrmod_bnm1@@Base+0x4a0>  // b.tcont
   48e50:	add	x26, x20, x28
   48e54:	str	x27, [sp, #8]
   48e58:	subs	x25, x27, x23
   48e5c:	add	x27, x24, x28
   48e60:	b.eq	48efc <__gmpn_sqrmod_bnm1@@Base+0xfc>  // b.none
   48e64:	mov	x0, x20
   48e68:	mov	x1, x24
   48e6c:	mov	x2, x27
   48e70:	mov	x3, x25
   48e74:	bl	ca70 <__gmpn_add_n@plt>
   48e78:	mov	x8, x25
   48e7c:	cbz	x0, 48f00 <__gmpn_sqrmod_bnm1@@Base+0x100>
   48e80:	mov	x8, x25
   48e84:	cmp	x8, x23
   48e88:	b.ge	48f98 <__gmpn_sqrmod_bnm1@@Base+0x198>  // b.tcont
   48e8c:	lsl	x9, x8, #3
   48e90:	ldr	x10, [x24, x9]
   48e94:	add	x8, x8, #0x1
   48e98:	adds	x10, x10, #0x1
   48e9c:	str	x10, [x20, x9]
   48ea0:	b.cs	48e84 <__gmpn_sqrmod_bnm1@@Base+0x84>  // b.hs, b.nlast
   48ea4:	b	48f00 <__gmpn_sqrmod_bnm1@@Base+0x100>
   48ea8:	cmp	x27, x21
   48eac:	b.lt	492c4 <__gmpn_sqrmod_bnm1@@Base+0x4c4>  // b.tstop
   48eb0:	mov	x0, x20
   48eb4:	mov	x1, x24
   48eb8:	mov	x2, x21
   48ebc:	bl	c8e0 <__gmpn_sqr@plt>
   48ec0:	add	x2, x20, x21, lsl #3
   48ec4:	mov	x0, x19
   48ec8:	mov	x1, x20
   48ecc:	mov	x3, x21
   48ed0:	bl	ca70 <__gmpn_add_n@plt>
   48ed4:	ldr	x8, [x19]
   48ed8:	adds	x8, x8, x0
   48edc:	str	x8, [x19]
   48ee0:	b.cc	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.lo, b.ul, b.last
   48ee4:	add	x8, x19, #0x8
   48ee8:	ldr	x9, [x8]
   48eec:	adds	x9, x9, #0x1
   48ef0:	str	x9, [x8], #8
   48ef4:	b.cs	48ee8 <__gmpn_sqrmod_bnm1@@Base+0xe8>  // b.hs, b.nlast
   48ef8:	b	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>
   48efc:	mov	x8, xzr
   48f00:	cmp	x20, x24
   48f04:	b.eq	48fac <__gmpn_sqrmod_bnm1@@Base+0x1ac>  // b.none
   48f08:	subs	x9, x23, x8
   48f0c:	b.le	48fac <__gmpn_sqrmod_bnm1@@Base+0x1ac>
   48f10:	cmp	x9, #0x4
   48f14:	b.cc	48f74 <__gmpn_sqrmod_bnm1@@Base+0x174>  // b.lo, b.ul, b.last
   48f18:	lsl	x11, x8, #3
   48f1c:	add	x10, x20, x11
   48f20:	add	x12, x24, x28
   48f24:	cmp	x10, x12
   48f28:	b.cs	48f3c <__gmpn_sqrmod_bnm1@@Base+0x13c>  // b.hs, b.nlast
   48f2c:	add	x10, x20, x28
   48f30:	add	x12, x24, x11
   48f34:	cmp	x12, x10
   48f38:	b.cc	48f74 <__gmpn_sqrmod_bnm1@@Base+0x174>  // b.lo, b.ul, b.last
   48f3c:	and	x10, x9, #0xfffffffffffffffc
   48f40:	add	x12, x11, #0x10
   48f44:	add	x8, x8, x10
   48f48:	add	x11, x24, x12
   48f4c:	add	x12, x20, x12
   48f50:	mov	x13, x10
   48f54:	ldp	q0, q1, [x11, #-16]
   48f58:	add	x11, x11, #0x20
   48f5c:	subs	x13, x13, #0x4
   48f60:	stp	q0, q1, [x12, #-16]
   48f64:	add	x12, x12, #0x20
   48f68:	b.ne	48f54 <__gmpn_sqrmod_bnm1@@Base+0x154>  // b.any
   48f6c:	cmp	x9, x10
   48f70:	b.eq	48fac <__gmpn_sqrmod_bnm1@@Base+0x1ac>  // b.none
   48f74:	lsl	x10, x8, #3
   48f78:	sub	x9, x23, x8
   48f7c:	add	x8, x20, x10
   48f80:	add	x10, x24, x10
   48f84:	ldr	x11, [x10], #8
   48f88:	subs	x9, x9, #0x1
   48f8c:	str	x11, [x8], #8
   48f90:	b.ne	48f84 <__gmpn_sqrmod_bnm1@@Base+0x184>  // b.any
   48f94:	b	48fac <__gmpn_sqrmod_bnm1@@Base+0x1ac>
   48f98:	mov	x8, x20
   48f9c:	ldr	x9, [x8]
   48fa0:	adds	x9, x9, #0x1
   48fa4:	str	x9, [x8], #8
   48fa8:	b.cs	48f9c <__gmpn_sqrmod_bnm1@@Base+0x19c>  // b.hs, b.nlast
   48fac:	mov	x0, x19
   48fb0:	mov	x1, x23
   48fb4:	mov	x2, x20
   48fb8:	mov	x3, x23
   48fbc:	mov	x4, x26
   48fc0:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   48fc4:	and	x22, x21, #0xfffffffffffffffe
   48fc8:	add	x8, x20, x22, lsl #3
   48fcc:	add	x26, x8, #0x10
   48fd0:	cbz	x25, 4901c <__gmpn_sqrmod_bnm1@@Base+0x21c>
   48fd4:	mov	x0, x26
   48fd8:	mov	x1, x24
   48fdc:	mov	x2, x27
   48fe0:	mov	x3, x25
   48fe4:	bl	c2d0 <__gmpn_sub_n@plt>
   48fe8:	cbz	x0, 4902c <__gmpn_sqrmod_bnm1@@Base+0x22c>
   48fec:	ldr	x27, [sp, #8]
   48ff0:	add	x8, x27, x23
   48ff4:	add	x8, x20, x8, lsl #3
   48ff8:	add	x8, x8, #0x10
   48ffc:	cmp	x25, x23
   49000:	b.ge	4927c <__gmpn_sqrmod_bnm1@@Base+0x47c>  // b.tcont
   49004:	ldr	x9, [x24, x25, lsl #3]
   49008:	add	x25, x25, #0x1
   4900c:	sub	x10, x9, #0x1
   49010:	str	x10, [x8], #8
   49014:	cbz	x9, 48ffc <__gmpn_sqrmod_bnm1@@Base+0x1fc>
   49018:	b	49020 <__gmpn_sqrmod_bnm1@@Base+0x220>
   4901c:	ldr	x27, [sp, #8]
   49020:	cmp	x26, x24
   49024:	b.ne	49038 <__gmpn_sqrmod_bnm1@@Base+0x238>  // b.any
   49028:	b	490d0 <__gmpn_sqrmod_bnm1@@Base+0x2d0>
   4902c:	ldr	x27, [sp, #8]
   49030:	cmp	x26, x24
   49034:	b.eq	490d0 <__gmpn_sqrmod_bnm1@@Base+0x2d0>  // b.none
   49038:	subs	x8, x23, x25
   4903c:	b.le	490d0 <__gmpn_sqrmod_bnm1@@Base+0x2d0>
   49040:	cmp	x8, #0x4
   49044:	b.cc	490ac <__gmpn_sqrmod_bnm1@@Base+0x2ac>  // b.lo, b.ul, b.last
   49048:	add	x9, x25, x22
   4904c:	add	x11, x20, x9, lsl #3
   49050:	add	x9, x11, #0x10
   49054:	add	x10, x24, x23, lsl #3
   49058:	cmp	x9, x10
   4905c:	add	x10, x24, x25, lsl #3
   49060:	b.cs	49078 <__gmpn_sqrmod_bnm1@@Base+0x278>  // b.hs, b.nlast
   49064:	mov	w9, #0x18                  	// #24
   49068:	madd	x9, x23, x9, x20
   4906c:	add	x9, x9, #0x10
   49070:	cmp	x10, x9
   49074:	b.cc	490ac <__gmpn_sqrmod_bnm1@@Base+0x2ac>  // b.lo, b.ul, b.last
   49078:	and	x9, x8, #0xfffffffffffffffc
   4907c:	add	x10, x10, #0x10
   49080:	add	x25, x25, x9
   49084:	add	x11, x11, #0x20
   49088:	mov	x12, x9
   4908c:	ldp	q0, q1, [x10, #-16]
   49090:	subs	x12, x12, #0x4
   49094:	add	x10, x10, #0x20
   49098:	stp	q0, q1, [x11, #-16]
   4909c:	add	x11, x11, #0x20
   490a0:	b.ne	4908c <__gmpn_sqrmod_bnm1@@Base+0x28c>  // b.any
   490a4:	cmp	x8, x9
   490a8:	b.eq	490d0 <__gmpn_sqrmod_bnm1@@Base+0x2d0>  // b.none
   490ac:	add	x9, x25, x22
   490b0:	add	x9, x20, x9, lsl #3
   490b4:	sub	x8, x23, x25
   490b8:	add	x9, x9, #0x10
   490bc:	add	x10, x24, x25, lsl #3
   490c0:	ldr	x11, [x10], #8
   490c4:	subs	x8, x8, #0x1
   490c8:	str	x11, [x9], #8
   490cc:	b.ne	490c0 <__gmpn_sqrmod_bnm1@@Base+0x2c0>  // b.any
   490d0:	mov	x8, xzr
   490d4:	str	xzr, [x26, x23, lsl #3]
   490d8:	add	x25, xzr, x23
   490dc:	cmp	x21, #0x278
   490e0:	b.lt	4914c <__gmpn_sqrmod_bnm1@@Base+0x34c>  // b.tstop
   490e4:	mov	w1, #0x1                   	// #1
   490e8:	mov	x0, x23
   490ec:	bl	cad0 <__gmpn_fft_best_k@plt>
   490f0:	mov	w8, #0xffffffff            	// #-1
   490f4:	lsl	w8, w8, w0
   490f8:	mvn	w8, w8
   490fc:	sxtw	x9, w8
   49100:	mov	w6, w0
   49104:	tst	x23, x9
   49108:	b.eq	49120 <__gmpn_sqrmod_bnm1@@Base+0x320>  // b.none
   4910c:	sbfx	x9, x8, #1, #31
   49110:	asr	w8, w8, #1
   49114:	tst	x23, x9
   49118:	sub	w6, w6, #0x1
   4911c:	b.ne	4910c <__gmpn_sqrmod_bnm1@@Base+0x30c>  // b.any
   49120:	cmp	w6, #0x4
   49124:	b.lt	4914c <__gmpn_sqrmod_bnm1@@Base+0x34c>  // b.tstop
   49128:	mov	x0, x20
   4912c:	mov	x1, x23
   49130:	mov	x2, x26
   49134:	mov	x3, x25
   49138:	mov	x4, x26
   4913c:	mov	x5, x25
   49140:	bl	c300 <__gmpn_mul_fft@plt>
   49144:	str	x0, [x20, x23, lsl #3]
   49148:	b	491b0 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   4914c:	cmp	x26, x24
   49150:	b.eq	4944c <__gmpn_sqrmod_bnm1@@Base+0x64c>  // b.none
   49154:	add	x2, x23, #0x1
   49158:	mov	x0, x20
   4915c:	mov	x1, x26
   49160:	bl	c8e0 <__gmpn_sqr@plt>
   49164:	and	x8, x21, #0x1ffffffffffffffe
   49168:	ldr	x22, [x20, x8, lsl #3]
   4916c:	add	x24, x20, x23, lsl #3
   49170:	mov	x0, x20
   49174:	mov	x1, x20
   49178:	mov	x2, x24
   4917c:	mov	x3, x23
   49180:	bl	c2d0 <__gmpn_sub_n@plt>
   49184:	str	xzr, [x24]
   49188:	ldr	x8, [x20]
   4918c:	add	x9, x0, x22
   49190:	adds	x8, x8, x9
   49194:	str	x8, [x20]
   49198:	b.cc	491b0 <__gmpn_sqrmod_bnm1@@Base+0x3b0>  // b.lo, b.ul, b.last
   4919c:	add	x8, x20, #0x8
   491a0:	ldr	x9, [x8]
   491a4:	adds	x9, x9, #0x1
   491a8:	str	x9, [x8], #8
   491ac:	b.cs	491a0 <__gmpn_sqrmod_bnm1@@Base+0x3a0>  // b.hs, b.nlast
   491b0:	ldr	x22, [x20, x28]
   491b4:	mov	x0, x19
   491b8:	mov	x1, x19
   491bc:	mov	x2, x20
   491c0:	mov	x3, x23
   491c4:	bl	c950 <__gmpn_rsh1add_n@plt>
   491c8:	mov	x8, xzr
   491cc:	add	x9, x28, x19
   491d0:	add	x11, x0, x22
   491d4:	ldur	x10, [x9, #-8]
   491d8:	lsr	x12, x11, #1
   491dc:	lsl	x11, x11, #63
   491e0:	adds	x13, x10, x11
   491e4:	adc	x8, x12, x8
   491e8:	stur	x13, [x9, #-8]
   491ec:	ldr	x9, [x19]
   491f0:	adds	x8, x9, x8
   491f4:	str	x8, [x19]
   491f8:	b.cc	49210 <__gmpn_sqrmod_bnm1@@Base+0x410>  // b.lo, b.ul, b.last
   491fc:	add	x8, x19, #0x8
   49200:	ldr	x9, [x8]
   49204:	adds	x9, x9, #0x1
   49208:	str	x9, [x8], #8
   4920c:	b.cs	49200 <__gmpn_sqrmod_bnm1@@Base+0x400>  // b.hs, b.nlast
   49210:	lsl	x22, x27, #1
   49214:	cmp	x22, x21
   49218:	b.lt	493d4 <__gmpn_sqrmod_bnm1@@Base+0x5d4>  // b.tstop
   4921c:	ldr	x21, [x20, x28]
   49220:	add	x0, x19, x28
   49224:	mov	x1, x19
   49228:	mov	x2, x20
   4922c:	mov	x3, x23
   49230:	bl	c2d0 <__gmpn_sub_n@plt>
   49234:	ldr	x8, [x19]
   49238:	add	x9, x0, x21
   4923c:	subs	x8, x8, x9
   49240:	str	x8, [x19]
   49244:	b.cs	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.hs, b.nlast
   49248:	add	x8, x19, #0x8
   4924c:	ldr	x9, [x8]
   49250:	sub	x10, x9, #0x1
   49254:	str	x10, [x8], #8
   49258:	cbz	x9, 4924c <__gmpn_sqrmod_bnm1@@Base+0x44c>
   4925c:	ldp	x20, x19, [sp, #96]
   49260:	ldp	x22, x21, [sp, #80]
   49264:	ldp	x24, x23, [sp, #64]
   49268:	ldp	x26, x25, [sp, #48]
   4926c:	ldp	x28, x27, [sp, #32]
   49270:	ldp	x29, x30, [sp, #16]
   49274:	add	sp, sp, #0x70
   49278:	ret
   4927c:	mov	x8, x26
   49280:	str	xzr, [x26, x23, lsl #3]
   49284:	ldr	x9, [x8]
   49288:	adds	x9, x9, #0x1
   4928c:	str	x9, [x8], #8
   49290:	b.cs	49284 <__gmpn_sqrmod_bnm1@@Base+0x484>  // b.hs, b.nlast
   49294:	ldr	x8, [x26, x23, lsl #3]
   49298:	add	x25, x8, x23
   4929c:	b	490dc <__gmpn_sqrmod_bnm1@@Base+0x2dc>
   492a0:	mov	x0, x19
   492a4:	mov	x1, x23
   492a8:	mov	x2, x24
   492ac:	mov	x3, x27
   492b0:	mov	x4, x20
   492b4:	bl	bf50 <__gmpn_sqrmod_bnm1@plt>
   492b8:	mov	x26, x24
   492bc:	mov	x25, x27
   492c0:	b	490dc <__gmpn_sqrmod_bnm1@@Base+0x2dc>
   492c4:	lsl	x22, x27, #1
   492c8:	cmp	x22, x21
   492cc:	b.le	494c8 <__gmpn_sqrmod_bnm1@@Base+0x6c8>
   492d0:	mov	x0, x20
   492d4:	mov	x1, x24
   492d8:	mov	x2, x27
   492dc:	bl	c8e0 <__gmpn_sqr@plt>
   492e0:	subs	x22, x22, x21
   492e4:	b.eq	49320 <__gmpn_sqrmod_bnm1@@Base+0x520>  // b.none
   492e8:	add	x2, x20, x21, lsl #3
   492ec:	mov	x0, x19
   492f0:	mov	x1, x20
   492f4:	mov	x3, x22
   492f8:	bl	ca70 <__gmpn_add_n@plt>
   492fc:	cbz	x0, 49320 <__gmpn_sqrmod_bnm1@@Base+0x520>
   49300:	cmp	x22, x21
   49304:	b.ge	493c0 <__gmpn_sqrmod_bnm1@@Base+0x5c0>  // b.tcont
   49308:	lsl	x8, x22, #3
   4930c:	ldr	x9, [x20, x8]
   49310:	add	x22, x22, #0x1
   49314:	adds	x9, x9, #0x1
   49318:	str	x9, [x19, x8]
   4931c:	b.cs	49300 <__gmpn_sqrmod_bnm1@@Base+0x500>  // b.hs, b.nlast
   49320:	cmp	x19, x20
   49324:	b.eq	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.none
   49328:	cmp	x22, x21
   4932c:	b.ge	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.tcont
   49330:	sub	x8, x21, x22
   49334:	cmp	x8, #0x4
   49338:	b.cc	4939c <__gmpn_sqrmod_bnm1@@Base+0x59c>  // b.lo, b.ul, b.last
   4933c:	lsl	x10, x22, #3
   49340:	lsl	x9, x21, #3
   49344:	add	x11, x19, x10
   49348:	add	x12, x20, x9
   4934c:	cmp	x11, x12
   49350:	b.cs	49364 <__gmpn_sqrmod_bnm1@@Base+0x564>  // b.hs, b.nlast
   49354:	add	x9, x19, x9
   49358:	add	x11, x20, x10
   4935c:	cmp	x11, x9
   49360:	b.cc	4939c <__gmpn_sqrmod_bnm1@@Base+0x59c>  // b.lo, b.ul, b.last
   49364:	and	x9, x8, #0xfffffffffffffffc
   49368:	add	x11, x10, #0x10
   4936c:	add	x22, x22, x9
   49370:	add	x10, x20, x11
   49374:	add	x11, x19, x11
   49378:	mov	x12, x9
   4937c:	ldp	q0, q1, [x10, #-16]
   49380:	add	x10, x10, #0x20
   49384:	subs	x12, x12, #0x4
   49388:	stp	q0, q1, [x11, #-16]
   4938c:	add	x11, x11, #0x20
   49390:	b.ne	4937c <__gmpn_sqrmod_bnm1@@Base+0x57c>  // b.any
   49394:	cmp	x8, x9
   49398:	b.eq	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.none
   4939c:	lsl	x10, x22, #3
   493a0:	sub	x8, x21, x22
   493a4:	add	x9, x19, x10
   493a8:	add	x10, x20, x10
   493ac:	ldr	x11, [x10], #8
   493b0:	subs	x8, x8, #0x1
   493b4:	str	x11, [x9], #8
   493b8:	b.ne	493ac <__gmpn_sqrmod_bnm1@@Base+0x5ac>  // b.any
   493bc:	b	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>
   493c0:	ldr	x8, [x19]
   493c4:	adds	x8, x8, #0x1
   493c8:	str	x8, [x19], #8
   493cc:	b.cs	493c0 <__gmpn_sqrmod_bnm1@@Base+0x5c0>  // b.hs, b.nlast
   493d0:	b	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>
   493d4:	add	x0, x19, x28
   493d8:	sub	x3, x22, x23
   493dc:	mov	x1, x19
   493e0:	mov	x2, x20
   493e4:	bl	c2d0 <__gmpn_sub_n@plt>
   493e8:	lsl	x8, x22, #3
   493ec:	ldr	x23, [x20, x28]
   493f0:	add	x9, x20, x8
   493f4:	mov	x4, x0
   493f8:	add	x8, x19, x8
   493fc:	sub	x0, x9, x28
   49400:	sub	x1, x8, x28
   49404:	sub	x3, x21, x22
   49408:	mov	x2, x0
   4940c:	bl	c760 <__gmpn_sub_nc@plt>
   49410:	ldr	x8, [x19]
   49414:	add	x9, x0, x23
   49418:	subs	x8, x8, x9
   4941c:	str	x8, [x19]
   49420:	b.cs	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.hs, b.nlast
   49424:	mov	w8, #0x1                   	// #1
   49428:	cmp	x8, x22
   4942c:	b.ge	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>  // b.tcont
   49430:	lsl	x9, x8, #3
   49434:	ldr	x10, [x19, x9]
   49438:	add	x8, x8, #0x1
   4943c:	sub	x11, x10, #0x1
   49440:	str	x11, [x19, x9]
   49444:	cbz	x10, 49428 <__gmpn_sqrmod_bnm1@@Base+0x628>
   49448:	b	4925c <__gmpn_sqrmod_bnm1@@Base+0x45c>
   4944c:	mov	x0, x20
   49450:	mov	x1, x24
   49454:	mov	x2, x27
   49458:	bl	c8e0 <__gmpn_sqr@plt>
   4945c:	lsl	x8, x27, #1
   49460:	subs	x25, x8, x23
   49464:	add	x24, x20, x23, lsl #3
   49468:	b.eq	494a4 <__gmpn_sqrmod_bnm1@@Base+0x6a4>  // b.none
   4946c:	mov	x0, x20
   49470:	mov	x1, x20
   49474:	mov	x2, x24
   49478:	mov	x3, x25
   4947c:	bl	c2d0 <__gmpn_sub_n@plt>
   49480:	cbz	x0, 494a4 <__gmpn_sqrmod_bnm1@@Base+0x6a4>
   49484:	cmp	x25, x23
   49488:	b.ge	494ac <__gmpn_sqrmod_bnm1@@Base+0x6ac>  // b.tcont
   4948c:	lsl	x8, x25, #3
   49490:	ldr	x9, [x20, x8]
   49494:	add	x25, x25, #0x1
   49498:	sub	x10, x9, #0x1
   4949c:	str	x10, [x20, x8]
   494a0:	cbz	x9, 49484 <__gmpn_sqrmod_bnm1@@Base+0x684>
   494a4:	str	xzr, [x24]
   494a8:	b	491b0 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   494ac:	mov	x8, x20
   494b0:	str	xzr, [x24]
   494b4:	ldr	x9, [x8]
   494b8:	adds	x9, x9, #0x1
   494bc:	str	x9, [x8], #8
   494c0:	b.cs	494b4 <__gmpn_sqrmod_bnm1@@Base+0x6b4>  // b.hs, b.nlast
   494c4:	b	491b0 <__gmpn_sqrmod_bnm1@@Base+0x3b0>
   494c8:	mov	x0, x19
   494cc:	mov	x1, x24
   494d0:	mov	x2, x27
   494d4:	ldp	x20, x19, [sp, #96]
   494d8:	ldp	x22, x21, [sp, #80]
   494dc:	ldp	x24, x23, [sp, #64]
   494e0:	ldp	x26, x25, [sp, #48]
   494e4:	ldp	x28, x27, [sp, #32]
   494e8:	ldp	x29, x30, [sp, #16]
   494ec:	add	sp, sp, #0x70
   494f0:	b	c8e0 <__gmpn_sqr@plt>

00000000000494f4 <__gmpn_sqrmod_bnm1_next_size@@Base>:
   494f4:	cmp	x0, #0xb
   494f8:	b.lt	4954c <__gmpn_sqrmod_bnm1_next_size@@Base+0x58>  // b.tstop
   494fc:	cmp	x0, #0x28
   49500:	b.le	49550 <__gmpn_sqrmod_bnm1_next_size@@Base+0x5c>
   49504:	cmp	x0, #0x50
   49508:	b.le	4955c <__gmpn_sqrmod_bnm1_next_size@@Base+0x68>
   4950c:	cmp	x0, #0x21e
   49510:	b.le	49568 <__gmpn_sqrmod_bnm1_next_size@@Base+0x74>
   49514:	stp	x29, x30, [sp, #-32]!
   49518:	add	x8, x0, #0x1
   4951c:	str	x19, [sp, #16]
   49520:	asr	x19, x8, #1
   49524:	mov	w1, #0x1                   	// #1
   49528:	mov	x0, x19
   4952c:	mov	x29, sp
   49530:	bl	cad0 <__gmpn_fft_best_k@plt>
   49534:	mov	w1, w0
   49538:	mov	x0, x19
   4953c:	bl	d1d0 <__gmpn_fft_next_size@plt>
   49540:	ldr	x19, [sp, #16]
   49544:	lsl	x0, x0, #1
   49548:	ldp	x29, x30, [sp], #32
   4954c:	ret
   49550:	add	x8, x0, #0x1
   49554:	and	x0, x8, #0xfffffffffffffffe
   49558:	ret
   4955c:	add	x8, x0, #0x3
   49560:	and	x0, x8, #0xfffffffffffffffc
   49564:	ret
   49568:	add	x8, x0, #0x7
   4956c:	and	x0, x8, #0xfffffffffffffff8
   49570:	ret

0000000000049574 <__gmpn_div_qr_1@@Base>:
   49574:	stp	x29, x30, [sp, #-80]!
   49578:	stp	x24, x23, [sp, #32]
   4957c:	stp	x22, x21, [sp, #48]
   49580:	stp	x20, x19, [sp, #64]
   49584:	mov	x19, x4
   49588:	mov	x22, x2
   4958c:	mov	x24, x1
   49590:	mov	x20, x0
   49594:	stp	x26, x25, [sp, #16]
   49598:	mov	x29, sp
   4959c:	tbnz	x4, #63, 49618 <__gmpn_div_qr_1@@Base+0xa4>
   495a0:	sub	x23, x3, #0x1
   495a4:	ldr	x25, [x22, x23, lsl #3]
   495a8:	clz	x21, x19
   495ac:	mov	x0, x20
   495b0:	mov	x1, x22
   495b4:	mov	x2, x23
   495b8:	mov	w3, w21
   495bc:	lsl	x19, x19, x21
   495c0:	lsl	x26, x25, x21
   495c4:	bl	c180 <__gmpn_lshift@plt>
   495c8:	neg	x9, x21
   495cc:	lsr	x10, x19, #32
   495d0:	lsr	x9, x25, x9
   495d4:	udiv	x14, x9, x10
   495d8:	orr	x8, x26, x0
   495dc:	and	x11, x19, #0xffffffff
   495e0:	msub	w9, w14, w10, w9
   495e4:	mul	x12, x14, x11
   495e8:	extr	x13, x9, x8, #32
   495ec:	cmp	x13, x12
   495f0:	b.cs	49674 <__gmpn_div_qr_1@@Base+0x100>  // b.hs, b.nlast
   495f4:	add	x13, x13, x19
   495f8:	cmp	x13, x19
   495fc:	sub	x9, x14, #0x1
   49600:	b.cc	49678 <__gmpn_div_qr_1@@Base+0x104>  // b.lo, b.ul, b.last
   49604:	cmp	x13, x12
   49608:	b.cs	49678 <__gmpn_div_qr_1@@Base+0x104>  // b.hs, b.nlast
   4960c:	sub	x9, x14, #0x2
   49610:	add	x13, x13, x19
   49614:	b	49678 <__gmpn_div_qr_1@@Base+0x104>
   49618:	sub	x23, x3, #0x1
   4961c:	ldr	x8, [x22, x23, lsl #3]
   49620:	cmp	x8, x19
   49624:	csel	x10, x19, xzr, cs  // cs = hs, nlast
   49628:	cset	w9, cs  // cs = hs, nlast
   4962c:	cmp	x3, #0xd
   49630:	sub	x25, x8, x10
   49634:	str	x9, [x24]
   49638:	b.le	49784 <__gmpn_div_qr_1@@Base+0x210>
   4963c:	mov	x0, x19
   49640:	bl	d3f0 <__gmpn_invert_limb@plt>
   49644:	mov	x5, x0
   49648:	mov	x0, x20
   4964c:	mov	x1, x22
   49650:	mov	x2, x23
   49654:	mov	x3, x25
   49658:	mov	x4, x19
   4965c:	ldp	x20, x19, [sp, #64]
   49660:	ldp	x22, x21, [sp, #48]
   49664:	ldp	x24, x23, [sp, #32]
   49668:	ldp	x26, x25, [sp, #16]
   4966c:	ldp	x29, x30, [sp], #80
   49670:	b	c470 <__gmpn_div_qr_1n_pi1@plt>
   49674:	mov	x9, x14
   49678:	sub	x13, x13, x12
   4967c:	udiv	x12, x13, x10
   49680:	msub	w13, w12, w10, w13
   49684:	mul	x10, x12, x11
   49688:	bfi	x8, x13, #32, #32
   4968c:	cmp	x8, x10
   49690:	b.cs	496b8 <__gmpn_div_qr_1@@Base+0x144>  // b.hs, b.nlast
   49694:	add	x8, x8, x19
   49698:	cmp	x8, x19
   4969c:	sub	x11, x12, #0x1
   496a0:	b.cc	496bc <__gmpn_div_qr_1@@Base+0x148>  // b.lo, b.ul, b.last
   496a4:	cmp	x8, x10
   496a8:	b.cs	496bc <__gmpn_div_qr_1@@Base+0x148>  // b.hs, b.nlast
   496ac:	sub	x11, x12, #0x2
   496b0:	add	x8, x8, x19
   496b4:	b	496bc <__gmpn_div_qr_1@@Base+0x148>
   496b8:	mov	x11, x12
   496bc:	sub	x25, x8, x10
   496c0:	orr	x8, x11, x9, lsl #32
   496c4:	str	x8, [x24]
   496c8:	mov	x22, x20
   496cc:	subs	x8, x23, #0x1
   496d0:	b.lt	49790 <__gmpn_div_qr_1@@Base+0x21c>  // b.tstop
   496d4:	lsr	x9, x19, #32
   496d8:	and	x10, x19, #0xffffffff
   496dc:	b	49700 <__gmpn_div_qr_1@@Base+0x18c>
   496e0:	mov	x15, x14
   496e4:	orr	x12, x15, x12, lsl #32
   496e8:	add	x14, x8, #0x1
   496ec:	str	x12, [x20, x8, lsl #3]
   496f0:	sub	x8, x8, #0x1
   496f4:	cmp	x14, #0x1
   496f8:	sub	x25, x11, x13
   496fc:	b.le	49790 <__gmpn_div_qr_1@@Base+0x21c>
   49700:	ldr	x11, [x22, x8, lsl #3]
   49704:	udiv	x15, x25, x9
   49708:	msub	w12, w15, w9, w25
   4970c:	mul	x13, x15, x10
   49710:	extr	x14, x12, x11, #32
   49714:	cmp	x14, x13
   49718:	b.cs	49740 <__gmpn_div_qr_1@@Base+0x1cc>  // b.hs, b.nlast
   4971c:	add	x14, x14, x19
   49720:	cmp	x14, x19
   49724:	sub	x12, x15, #0x1
   49728:	b.cc	49744 <__gmpn_div_qr_1@@Base+0x1d0>  // b.lo, b.ul, b.last
   4972c:	cmp	x14, x13
   49730:	b.cs	49744 <__gmpn_div_qr_1@@Base+0x1d0>  // b.hs, b.nlast
   49734:	sub	x12, x15, #0x2
   49738:	add	x14, x14, x19
   4973c:	b	49744 <__gmpn_div_qr_1@@Base+0x1d0>
   49740:	mov	x12, x15
   49744:	sub	x13, x14, x13
   49748:	udiv	x14, x13, x9
   4974c:	msub	w15, w14, w9, w13
   49750:	mul	x13, x14, x10
   49754:	bfi	x11, x15, #32, #32
   49758:	cmp	x11, x13
   4975c:	b.cs	496e0 <__gmpn_div_qr_1@@Base+0x16c>  // b.hs, b.nlast
   49760:	add	x11, x11, x19
   49764:	cmp	x11, x19
   49768:	sub	x15, x14, #0x1
   4976c:	b.cc	496e4 <__gmpn_div_qr_1@@Base+0x170>  // b.lo, b.ul, b.last
   49770:	cmp	x11, x13
   49774:	b.cs	496e4 <__gmpn_div_qr_1@@Base+0x170>  // b.hs, b.nlast
   49778:	sub	x15, x14, #0x2
   4977c:	add	x11, x11, x19
   49780:	b	496e4 <__gmpn_div_qr_1@@Base+0x170>
   49784:	mov	x21, xzr
   49788:	subs	x8, x23, #0x1
   4978c:	b.ge	496d4 <__gmpn_div_qr_1@@Base+0x160>  // b.tcont
   49790:	lsr	x0, x25, x21
   49794:	ldp	x20, x19, [sp, #64]
   49798:	ldp	x22, x21, [sp, #48]
   4979c:	ldp	x24, x23, [sp, #32]
   497a0:	ldp	x26, x25, [sp, #16]
   497a4:	ldp	x29, x30, [sp], #80
   497a8:	ret

00000000000497ac <__gmpn_div_qr_1n_pi1@@Base>:
   497ac:	str	x19, [sp, #-16]!
   497b0:	cmp	x2, #0x1
   497b4:	b.ne	497fc <__gmpn_div_qr_1n_pi1@@Base+0x50>  // b.any
   497b8:	ldr	x8, [x1]
   497bc:	mul	x10, x5, x3
   497c0:	umulh	x9, x3, x5
   497c4:	add	x11, x3, #0x1
   497c8:	adds	x12, x10, x8
   497cc:	adc	x10, x9, x11
   497d0:	msub	x8, x10, x4, x8
   497d4:	cmp	x8, x12
   497d8:	csel	x9, x4, xzr, hi  // hi = pmore
   497dc:	cset	w11, hi  // hi = pmore
   497e0:	add	x8, x9, x8
   497e4:	subs	x9, x8, x4
   497e8:	sub	x10, x10, x11
   497ec:	b.cc	499b8 <__gmpn_div_qr_1n_pi1@@Base+0x20c>  // b.lo, b.ul, b.last
   497f0:	add	x10, x10, #0x1
   497f4:	mov	x8, x9
   497f8:	b	499b8 <__gmpn_div_qr_1n_pi1@@Base+0x20c>
   497fc:	umulh	x10, x5, x3
   49800:	lsl	x11, x2, #3
   49804:	add	x10, x10, x3
   49808:	sub	x11, x11, #0x8
   4980c:	sub	x15, x2, #0x2
   49810:	ldr	x12, [x1, x11]
   49814:	str	x10, [x0, x11]
   49818:	ldr	x10, [x1, x15, lsl #3]
   4981c:	mul	x9, x4, x5
   49820:	mneg	x9, x9, x3
   49824:	mneg	x8, x4, x5
   49828:	adds	x10, x10, x9
   4982c:	mov	w11, #0x2                   	// #2
   49830:	umulh	x13, x8, x3
   49834:	cset	w9, cs  // cs = hs, nlast
   49838:	adds	x12, x12, x13
   4983c:	cset	w13, cs  // cs = hs, nlast
   49840:	csinc	x14, x11, xzr, cs  // cs = hs, nlast
   49844:	adds	x11, x12, x9
   49848:	csel	x16, x13, x14, cc  // cc = lo, ul, last
   4984c:	subs	x17, x2, #0x3
   49850:	mul	x18, x5, x3
   49854:	b.lt	49920 <__gmpn_div_qr_1n_pi1@@Base+0x174>  // b.tstop
   49858:	add	x12, x0, #0x10
   4985c:	add	x13, x0, x2, lsl #3
   49860:	mov	w14, #0x2                   	// #2
   49864:	b	4989c <__gmpn_div_qr_1n_pi1@@Base+0xf0>
   49868:	ldr	x18, [x1, x15, lsl #3]
   4986c:	sub	x17, x15, #0x1
   49870:	sub	x13, x13, #0x8
   49874:	adds	x10, x18, x10
   49878:	cset	w18, cs  // cs = hs, nlast
   4987c:	adds	x11, x16, x11
   49880:	cset	w16, cs  // cs = hs, nlast
   49884:	csinc	x2, x14, xzr, cs  // cs = hs, nlast
   49888:	adds	x11, x11, x18
   4988c:	csel	x16, x16, x2, cc  // cc = lo, ul, last
   49890:	cmp	x15, #0x0
   49894:	mov	x18, x9
   49898:	b.le	49924 <__gmpn_div_qr_1n_pi1@@Base+0x178>
   4989c:	neg	x3, x16
   498a0:	mov	x2, xzr
   498a4:	and	x7, x3, x5
   498a8:	adds	x19, x7, x11
   498ac:	adc	x16, x16, x2
   498b0:	umulh	x6, x11, x5
   498b4:	adds	x7, x19, x6
   498b8:	adc	x16, x16, x2
   498bc:	and	x3, x3, x8
   498c0:	adds	x6, x7, x18
   498c4:	adc	x16, x16, x2
   498c8:	adds	x18, x10, x3
   498cc:	cset	w10, cs  // cs = hs, nlast
   498d0:	csel	x3, x4, xzr, cs  // cs = hs, nlast
   498d4:	adds	x7, x6, x10
   498d8:	adc	x2, x16, x2
   498dc:	str	x7, [x0, x15, lsl #3]
   498e0:	mov	x15, x17
   498e4:	lsl	x17, x17, #3
   498e8:	ldr	x6, [x12, x17]
   498ec:	mul	x9, x11, x5
   498f0:	umulh	x16, x11, x8
   498f4:	mul	x10, x11, x8
   498f8:	sub	x11, x18, x3
   498fc:	adds	x18, x6, x2
   49900:	str	x18, [x12, x17]
   49904:	b.cc	49868 <__gmpn_div_qr_1n_pi1@@Base+0xbc>  // b.lo, b.ul, b.last
   49908:	mov	x17, x13
   4990c:	ldr	x18, [x17]
   49910:	adds	x18, x18, #0x1
   49914:	str	x18, [x17], #8
   49918:	b.cs	4990c <__gmpn_div_qr_1n_pi1@@Base+0x160>  // b.hs, b.nlast
   4991c:	b	49868 <__gmpn_div_qr_1n_pi1@@Base+0xbc>
   49920:	mov	x9, x18
   49924:	mov	x12, xzr
   49928:	cmp	x16, #0x0
   4992c:	csel	x14, x4, x12, ne  // ne = any
   49930:	mov	w8, #0x2                   	// #2
   49934:	sub	x11, x11, x14
   49938:	cset	w14, ne  // ne = any
   4993c:	csinc	x8, x8, xzr, ne  // ne = any
   49940:	cmp	x11, x4
   49944:	csel	x14, x14, x8, cc  // cc = lo, ul, last
   49948:	csel	x8, x4, x12, cs  // cs = hs, nlast
   4994c:	sub	x8, x11, x8
   49950:	umulh	x11, x8, x5
   49954:	mul	x15, x8, x5
   49958:	add	x8, x8, #0x1
   4995c:	adds	x16, x15, x10
   49960:	adc	x8, x11, x8
   49964:	msub	x10, x8, x4, x10
   49968:	cmp	x10, x16
   4996c:	csel	x15, x4, x12, hi  // hi = pmore
   49970:	ldr	x13, [x0, #8]
   49974:	cset	w11, hi  // hi = pmore
   49978:	add	x10, x15, x10
   4997c:	sub	x8, x8, x11
   49980:	cmp	x10, x4
   49984:	cinc	x11, x8, cs  // cs = hs, nlast
   49988:	csel	x8, x12, x4, cc  // cc = lo, ul, last
   4998c:	sub	x8, x10, x8
   49990:	adds	x10, x9, x11
   49994:	adc	x9, x14, x12
   49998:	adds	x9, x13, x9
   4999c:	str	x9, [x0, #8]
   499a0:	b.cc	499b8 <__gmpn_div_qr_1n_pi1@@Base+0x20c>  // b.lo, b.ul, b.last
   499a4:	add	x9, x0, #0x10
   499a8:	ldr	x11, [x9]
   499ac:	adds	x11, x11, #0x1
   499b0:	str	x11, [x9], #8
   499b4:	b.cs	499a8 <__gmpn_div_qr_1n_pi1@@Base+0x1fc>  // b.hs, b.nlast
   499b8:	str	x10, [x0]
   499bc:	mov	x0, x8
   499c0:	ldr	x19, [sp], #16
   499c4:	ret

00000000000499c8 <__gmpn_div_qr_2@@Base>:
   499c8:	stp	x29, x30, [sp, #-80]!
   499cc:	str	x25, [sp, #16]
   499d0:	stp	x24, x23, [sp, #32]
   499d4:	stp	x22, x21, [sp, #48]
   499d8:	stp	x20, x19, [sp, #64]
   499dc:	ldp	x23, x25, [x4]
   499e0:	mov	x19, x3
   499e4:	mov	x20, x2
   499e8:	mov	x21, x1
   499ec:	mov	x22, x0
   499f0:	mov	x29, sp
   499f4:	tbnz	x25, #63, 49aa4 <__gmpn_div_qr_2@@Base+0xdc>
   499f8:	clz	x24, x25
   499fc:	neg	x9, x24
   49a00:	lsl	x8, x25, x24
   49a04:	lsr	x9, x23, x9
   49a08:	orr	x25, x9, x8
   49a0c:	mov	x0, x25
   49a10:	lsl	x23, x23, x24
   49a14:	bl	d3f0 <__gmpn_invert_limb@plt>
   49a18:	mul	x8, x0, x25
   49a1c:	adds	x8, x8, x23
   49a20:	b.cc	49a3c <__gmpn_div_qr_2@@Base+0x74>  // b.lo, b.ul, b.last
   49a24:	subs	x8, x8, x25
   49a28:	cset	w9, cs  // cs = hs, nlast
   49a2c:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   49a30:	mvn	x9, x9
   49a34:	add	x0, x9, x0
   49a38:	sub	x8, x8, x10
   49a3c:	umulh	x9, x23, x0
   49a40:	adds	x8, x9, x8
   49a44:	b.cc	49a6c <__gmpn_div_qr_2@@Base+0xa4>  // b.lo, b.ul, b.last
   49a48:	cmp	x8, x25
   49a4c:	sub	x7, x0, #0x1
   49a50:	b.cc	49a70 <__gmpn_div_qr_2@@Base+0xa8>  // b.lo, b.ul, b.last
   49a54:	mul	x9, x0, x23
   49a58:	cmp	x8, x25
   49a5c:	sub	x10, x0, #0x2
   49a60:	ccmp	x9, x23, #0x2, ls  // ls = plast
   49a64:	csel	x7, x7, x10, cc  // cc = lo, ul, last
   49a68:	b	49a70 <__gmpn_div_qr_2@@Base+0xa8>
   49a6c:	mov	x7, x0
   49a70:	mov	x0, x22
   49a74:	mov	x1, x21
   49a78:	mov	x2, x20
   49a7c:	mov	x3, x19
   49a80:	mov	x4, x25
   49a84:	mov	x5, x23
   49a88:	mov	w6, w24
   49a8c:	ldp	x20, x19, [sp, #64]
   49a90:	ldp	x22, x21, [sp, #48]
   49a94:	ldp	x24, x23, [sp, #32]
   49a98:	ldr	x25, [sp, #16]
   49a9c:	ldp	x29, x30, [sp], #80
   49aa0:	b	cf20 <__gmpn_div_qr_2u_pi1@plt>
   49aa4:	mov	x0, x25
   49aa8:	bl	d3f0 <__gmpn_invert_limb@plt>
   49aac:	mul	x8, x0, x25
   49ab0:	adds	x8, x8, x23
   49ab4:	b.cc	49ad0 <__gmpn_div_qr_2@@Base+0x108>  // b.lo, b.ul, b.last
   49ab8:	subs	x8, x8, x25
   49abc:	cset	w9, cs  // cs = hs, nlast
   49ac0:	csel	x10, x25, xzr, cs  // cs = hs, nlast
   49ac4:	mvn	x9, x9
   49ac8:	add	x0, x9, x0
   49acc:	sub	x8, x8, x10
   49ad0:	umulh	x9, x23, x0
   49ad4:	adds	x8, x9, x8
   49ad8:	b.cc	49b00 <__gmpn_div_qr_2@@Base+0x138>  // b.lo, b.ul, b.last
   49adc:	cmp	x8, x25
   49ae0:	sub	x6, x0, #0x1
   49ae4:	b.cc	49b04 <__gmpn_div_qr_2@@Base+0x13c>  // b.lo, b.ul, b.last
   49ae8:	mul	x9, x0, x23
   49aec:	cmp	x8, x25
   49af0:	sub	x10, x0, #0x2
   49af4:	ccmp	x9, x23, #0x2, ls  // ls = plast
   49af8:	csel	x6, x6, x10, cc  // cc = lo, ul, last
   49afc:	b	49b04 <__gmpn_div_qr_2@@Base+0x13c>
   49b00:	mov	x6, x0
   49b04:	mov	x0, x22
   49b08:	mov	x1, x21
   49b0c:	mov	x2, x20
   49b10:	mov	x3, x19
   49b14:	mov	x4, x25
   49b18:	mov	x5, x23
   49b1c:	ldp	x20, x19, [sp, #64]
   49b20:	ldp	x22, x21, [sp, #48]
   49b24:	ldp	x24, x23, [sp, #32]
   49b28:	ldr	x25, [sp, #16]
   49b2c:	ldp	x29, x30, [sp], #80
   49b30:	b	c920 <__gmpn_div_qr_2n_pi1@plt>

0000000000049b34 <__gmpn_div_qr_2n_pi1@@Base>:
   49b34:	add	x8, x2, x3, lsl #3
   49b38:	ldp	x10, x9, [x8, #-16]
   49b3c:	cmp	x9, x4
   49b40:	b.cc	49b50 <__gmpn_div_qr_2n_pi1@@Base+0x1c>  // b.lo, b.ul, b.last
   49b44:	b.hi	49b60 <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.pmore
   49b48:	cmp	x10, x5
   49b4c:	b.cs	49b60 <__gmpn_div_qr_2n_pi1@@Base+0x2c>  // b.hs, b.nlast
   49b50:	mov	x8, xzr
   49b54:	subs	x11, x3, #0x3
   49b58:	b.ge	49ba0 <__gmpn_div_qr_2n_pi1@@Base+0x6c>  // b.tcont
   49b5c:	b	49c10 <__gmpn_div_qr_2n_pi1@@Base+0xdc>
   49b60:	subs	x11, x10, x5
   49b64:	sbc	x9, x9, x4
   49b68:	mov	w8, #0x1                   	// #1
   49b6c:	mov	x10, x11
   49b70:	subs	x11, x3, #0x3
   49b74:	b.lt	49c10 <__gmpn_div_qr_2n_pi1@@Base+0xdc>  // b.tstop
   49b78:	b	49ba0 <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   49b7c:	cmp	x10, x5
   49b80:	b.cs	49b8c <__gmpn_div_qr_2n_pi1@@Base+0x58>  // b.hs, b.nlast
   49b84:	cmp	x9, x4
   49b88:	b.ls	49bfc <__gmpn_div_qr_2n_pi1@@Base+0xc8>  // b.plast
   49b8c:	subs	x13, x10, x5
   49b90:	sbc	x9, x9, x4
   49b94:	add	x12, x12, #0x1
   49b98:	mov	x10, x13
   49b9c:	b	49bfc <__gmpn_div_qr_2n_pi1@@Base+0xc8>
   49ba0:	mul	x13, x9, x6
   49ba4:	ldr	x12, [x2, x11, lsl #3]
   49ba8:	umulh	x14, x9, x6
   49bac:	adds	x15, x13, x10
   49bb0:	adc	x9, x14, x9
   49bb4:	msub	x10, x9, x4, x10
   49bb8:	mul	x13, x9, x5
   49bbc:	umulh	x14, x5, x9
   49bc0:	subs	x16, x12, x5
   49bc4:	sbc	x10, x10, x4
   49bc8:	subs	x12, x16, x13
   49bcc:	sbc	x13, x10, x14
   49bd0:	cmp	x13, x15
   49bd4:	cset	w10, cs  // cs = hs, nlast
   49bd8:	csetm	x14, cs  // cs = hs, nlast
   49bdc:	sub	x15, x9, x10
   49be0:	and	x9, x14, x5
   49be4:	and	x14, x14, x4
   49be8:	adds	x10, x12, x9
   49bec:	adc	x9, x13, x14
   49bf0:	cmp	x9, x4
   49bf4:	add	x12, x15, #0x1
   49bf8:	b.cs	49b7c <__gmpn_div_qr_2n_pi1@@Base+0x48>  // b.hs, b.nlast
   49bfc:	sub	x13, x11, #0x1
   49c00:	cmp	x11, #0x0
   49c04:	str	x12, [x0, x11, lsl #3]
   49c08:	mov	x11, x13
   49c0c:	b.gt	49ba0 <__gmpn_div_qr_2n_pi1@@Base+0x6c>
   49c10:	mov	x0, x8
   49c14:	stp	x10, x9, [x1]
   49c18:	ret

0000000000049c1c <__gmpn_div_qr_2u_pi1@@Base>:
   49c1c:	add	x8, x2, x3, lsl #3
   49c20:	ldp	x8, x12, [x8, #-16]
   49c24:	neg	w11, w6
   49c28:	mov	w10, #0x40                  	// #64
   49c2c:	mov	w9, w6
   49c30:	lsr	x13, x12, x11
   49c34:	lsl	x12, x12, x6
   49c38:	lsr	x11, x8, x11
   49c3c:	lsl	x8, x8, x6
   49c40:	orr	x11, x11, x12
   49c44:	mul	x12, x13, x7
   49c48:	umulh	x14, x13, x7
   49c4c:	adds	x15, x12, x11
   49c50:	adc	x12, x14, x13
   49c54:	msub	x11, x12, x4, x11
   49c58:	subs	x16, x8, x5
   49c5c:	sbc	x8, x11, x4
   49c60:	mul	x13, x12, x5
   49c64:	umulh	x14, x5, x12
   49c68:	subs	x11, x16, x13
   49c6c:	sbc	x8, x8, x14
   49c70:	cmp	x8, x15
   49c74:	cset	w13, cs  // cs = hs, nlast
   49c78:	csetm	x14, cs  // cs = hs, nlast
   49c7c:	sub	x13, x12, x13
   49c80:	and	x15, x14, x5
   49c84:	and	x14, x14, x4
   49c88:	adds	x12, x11, x15
   49c8c:	adc	x11, x8, x14
   49c90:	sub	w10, w10, w6
   49c94:	cmp	x11, x4
   49c98:	add	x8, x13, #0x1
   49c9c:	b.cs	49d68 <__gmpn_div_qr_2u_pi1@@Base+0x14c>  // b.hs, b.nlast
   49ca0:	subs	x13, x3, #0x3
   49ca4:	b.lt	49d4c <__gmpn_div_qr_2u_pi1@@Base+0x130>  // b.tstop
   49ca8:	ldr	x14, [x2, x13, lsl #3]
   49cac:	mul	x15, x11, x7
   49cb0:	umulh	x16, x11, x7
   49cb4:	lsr	x17, x14, x10
   49cb8:	orr	x12, x17, x12
   49cbc:	lsl	x14, x14, x9
   49cc0:	adds	x17, x15, x12
   49cc4:	adc	x11, x16, x11
   49cc8:	msub	x12, x11, x4, x12
   49ccc:	mul	x15, x11, x5
   49cd0:	umulh	x16, x5, x11
   49cd4:	subs	x18, x14, x5
   49cd8:	sbc	x12, x12, x4
   49cdc:	subs	x14, x18, x15
   49ce0:	sbc	x15, x12, x16
   49ce4:	cmp	x15, x17
   49ce8:	cset	w12, cs  // cs = hs, nlast
   49cec:	csetm	x16, cs  // cs = hs, nlast
   49cf0:	sub	x17, x11, x12
   49cf4:	and	x11, x16, x5
   49cf8:	and	x16, x16, x4
   49cfc:	adds	x12, x14, x11
   49d00:	adc	x11, x15, x16
   49d04:	cmp	x11, x4
   49d08:	add	x14, x17, #0x1
   49d0c:	b.cs	49d28 <__gmpn_div_qr_2u_pi1@@Base+0x10c>  // b.hs, b.nlast
   49d10:	sub	x15, x13, #0x1
   49d14:	cmp	x13, #0x0
   49d18:	str	x14, [x0, x13, lsl #3]
   49d1c:	mov	x13, x15
   49d20:	b.gt	49ca8 <__gmpn_div_qr_2u_pi1@@Base+0x8c>
   49d24:	b	49d4c <__gmpn_div_qr_2u_pi1@@Base+0x130>
   49d28:	cmp	x12, x5
   49d2c:	b.cs	49d38 <__gmpn_div_qr_2u_pi1@@Base+0x11c>  // b.hs, b.nlast
   49d30:	cmp	x11, x4
   49d34:	b.ls	49d10 <__gmpn_div_qr_2u_pi1@@Base+0xf4>  // b.plast
   49d38:	subs	x15, x12, x5
   49d3c:	sbc	x11, x11, x4
   49d40:	add	x14, x14, #0x1
   49d44:	mov	x12, x15
   49d48:	b	49d10 <__gmpn_div_qr_2u_pi1@@Base+0xf4>
   49d4c:	lsr	x12, x12, x9
   49d50:	lsl	x10, x11, x10
   49d54:	lsr	x9, x11, x9
   49d58:	orr	x10, x10, x12
   49d5c:	mov	x0, x8
   49d60:	stp	x10, x9, [x1]
   49d64:	ret
   49d68:	cmp	x12, x5
   49d6c:	b.cs	49d78 <__gmpn_div_qr_2u_pi1@@Base+0x15c>  // b.hs, b.nlast
   49d70:	cmp	x11, x4
   49d74:	b.ls	49ca0 <__gmpn_div_qr_2u_pi1@@Base+0x84>  // b.plast
   49d78:	subs	x13, x12, x5
   49d7c:	sbc	x11, x11, x4
   49d80:	add	x8, x8, #0x1
   49d84:	mov	x12, x13
   49d88:	b	49ca0 <__gmpn_div_qr_2u_pi1@@Base+0x84>

0000000000049d8c <__gmpn_sbpi1_div_q@@Base>:
   49d8c:	sub	sp, sp, #0xe0
   49d90:	stp	x28, x27, [sp, #144]
   49d94:	sub	x27, x2, x4
   49d98:	add	x8, x27, #0x1
   49d9c:	str	x8, [sp, #16]
   49da0:	subs	x8, x4, x8
   49da4:	stp	x26, x25, [sp, #160]
   49da8:	stp	x20, x19, [sp, #208]
   49dac:	add	x19, x1, x2, lsl #3
   49db0:	csinc	x25, x4, x27, le
   49db4:	str	x8, [sp]
   49db8:	add	x8, x3, x8, lsl #3
   49dbc:	stp	x29, x30, [sp, #128]
   49dc0:	stp	x24, x23, [sp, #176]
   49dc4:	stp	x22, x21, [sp, #192]
   49dc8:	add	x29, sp, #0x80
   49dcc:	mov	x21, x4
   49dd0:	mov	x20, x2
   49dd4:	mov	x22, x0
   49dd8:	mov	x9, x25
   49ddc:	csel	x23, x8, x3, gt
   49de0:	sub	x0, x19, x25, lsl #3
   49de4:	sub	x8, x19, #0x8
   49de8:	stur	x5, [x29, #-24]
   49dec:	stp	x3, x1, [sp, #24]
   49df0:	str	x25, [sp, #64]
   49df4:	subs	x10, x9, #0x1
   49df8:	b.lt	49e1c <__gmpn_sbpi1_div_q@@Base+0x90>  // b.tstop
   49dfc:	add	x9, x23, x9, lsl #3
   49e00:	ldr	x11, [x8], #-8
   49e04:	ldur	x12, [x9, #-8]
   49e08:	mov	x9, x10
   49e0c:	cmp	x11, x12
   49e10:	b.eq	49df4 <__gmpn_sbpi1_div_q@@Base+0x68>  // b.none
   49e14:	cmp	x11, x12
   49e18:	b.ls	49e3c <__gmpn_sbpi1_div_q@@Base+0xb0>  // b.plast
   49e1c:	ldr	x3, [sp, #64]
   49e20:	mov	x1, x0
   49e24:	mov	x2, x23
   49e28:	bl	c2d0 <__gmpn_sub_n@plt>
   49e2c:	mov	w26, #0x1                   	// #1
   49e30:	mov	w8, #0x1                   	// #1
   49e34:	str	w8, [sp, #12]
   49e38:	b	49e44 <__gmpn_sbpi1_div_q@@Base+0xb8>
   49e3c:	mov	x26, xzr
   49e40:	str	wzr, [sp, #12]
   49e44:	sub	x8, x25, #0x2
   49e48:	sub	x9, x25, #0x1
   49e4c:	ldr	x15, [x23, x9, lsl #3]
   49e50:	stp	x8, x23, [x29, #-40]
   49e54:	ldr	x24, [x23, x8, lsl #3]
   49e58:	ldur	x23, [x19, #-8]
   49e5c:	subs	x8, x27, x25
   49e60:	str	x21, [sp, #56]
   49e64:	stp	x24, x15, [x29, #-16]
   49e68:	stur	x25, [x29, #-56]
   49e6c:	b.mi	4a09c <__gmpn_sbpi1_div_q@@Base+0x310>  // b.first
   49e70:	sub	x24, x22, x21, lsl #3
   49e74:	ldr	x21, [sp, #32]
   49e78:	stur	x9, [x29, #-48]
   49e7c:	mov	x9, x25
   49e80:	stp	x27, x26, [sp, #40]
   49e84:	add	x27, x8, #0x1
   49e88:	mvn	x8, x9
   49e8c:	lsl	x25, x20, #3
   49e90:	add	x19, x21, x8, lsl #3
   49e94:	cmp	x23, x15
   49e98:	add	x28, x21, x25
   49e9c:	b.eq	49f50 <__gmpn_sbpi1_div_q@@Base+0x1c4>  // b.none
   49ea0:	ldp	x10, x16, [x29, #-24]
   49ea4:	ldp	x11, x8, [x28, #-24]
   49ea8:	mul	x9, x23, x10
   49eac:	umulh	x10, x23, x10
   49eb0:	adds	x12, x9, x8
   49eb4:	adc	x9, x10, x23
   49eb8:	msub	x8, x9, x15, x8
   49ebc:	subs	x14, x11, x16
   49ec0:	sbc	x8, x8, x15
   49ec4:	mul	x10, x9, x16
   49ec8:	umulh	x13, x16, x9
   49ecc:	subs	x11, x14, x10
   49ed0:	sbc	x8, x8, x13
   49ed4:	cmp	x8, x12
   49ed8:	cset	w10, cs  // cs = hs, nlast
   49edc:	csetm	x12, cs  // cs = hs, nlast
   49ee0:	sub	x9, x9, x10
   49ee4:	and	x10, x16, x12
   49ee8:	and	x12, x15, x12
   49eec:	adds	x26, x11, x10
   49ef0:	adc	x23, x8, x12
   49ef4:	cmp	x23, x15
   49ef8:	add	x20, x9, #0x1
   49efc:	b.cs	49f84 <__gmpn_sbpi1_div_q@@Base+0x1f8>  // b.hs, b.nlast
   49f00:	ldp	x2, x1, [x29, #-40]
   49f04:	add	x22, x19, x25
   49f08:	mov	x0, x22
   49f0c:	mov	x3, x20
   49f10:	bl	c9e0 <__gmpn_submul_1@plt>
   49f14:	subs	x8, x26, x0
   49f18:	cset	w9, cc  // cc = lo, ul, last
   49f1c:	subs	x23, x23, x9
   49f20:	stur	x8, [x28, #-24]
   49f24:	b.cc	49fb0 <__gmpn_sbpi1_div_q@@Base+0x224>  // b.lo, b.ul, b.last
   49f28:	ldur	x15, [x29, #-8]
   49f2c:	sub	x27, x27, #0x1
   49f30:	add	x8, x24, x25
   49f34:	sub	x24, x24, #0x8
   49f38:	sub	x21, x21, #0x8
   49f3c:	cmp	x27, #0x0
   49f40:	sub	x19, x19, #0x8
   49f44:	stur	x20, [x8, #-8]
   49f48:	b.gt	49e94 <__gmpn_sbpi1_div_q@@Base+0x108>
   49f4c:	b	49fd8 <__gmpn_sbpi1_div_q@@Base+0x24c>
   49f50:	ldur	x8, [x28, #-16]
   49f54:	ldur	x9, [x29, #-16]
   49f58:	cmp	x8, x9
   49f5c:	b.ne	49ea0 <__gmpn_sbpi1_div_q@@Base+0x114>  // b.any
   49f60:	ldur	x1, [x29, #-32]
   49f64:	ldr	x2, [sp, #64]
   49f68:	add	x0, x19, x25
   49f6c:	mov	x3, #0xffffffffffffffff    	// #-1
   49f70:	mov	x20, #0xffffffffffffffff    	// #-1
   49f74:	bl	c9e0 <__gmpn_submul_1@plt>
   49f78:	ldur	x15, [x29, #-8]
   49f7c:	ldur	x23, [x28, #-16]
   49f80:	b	49f2c <__gmpn_sbpi1_div_q@@Base+0x1a0>
   49f84:	ldur	x8, [x29, #-16]
   49f88:	cmp	x26, x8
   49f8c:	b.cs	49f98 <__gmpn_sbpi1_div_q@@Base+0x20c>  // b.hs, b.nlast
   49f90:	cmp	x23, x15
   49f94:	b.ls	49f00 <__gmpn_sbpi1_div_q@@Base+0x174>  // b.plast
   49f98:	ldur	x9, [x29, #-16]
   49f9c:	subs	x8, x26, x9
   49fa0:	sbc	x23, x23, x15
   49fa4:	add	x20, x20, #0x1
   49fa8:	mov	x26, x8
   49fac:	b	49f00 <__gmpn_sbpi1_div_q@@Base+0x174>
   49fb0:	ldur	x2, [x29, #-32]
   49fb4:	ldur	x3, [x29, #-48]
   49fb8:	mov	x0, x22
   49fbc:	mov	x1, x22
   49fc0:	bl	ca70 <__gmpn_add_n@plt>
   49fc4:	ldur	x15, [x29, #-8]
   49fc8:	sub	x20, x20, #0x1
   49fcc:	add	x8, x23, x15
   49fd0:	add	x23, x8, x0
   49fd4:	b	49f2c <__gmpn_sbpi1_div_q@@Base+0x1a0>
   49fd8:	add	x8, x24, x25
   49fdc:	stur	x8, [x29, #-40]
   49fe0:	add	x8, x21, x25
   49fe4:	ldp	x27, x26, [sp, #40]
   49fe8:	ldur	x25, [x29, #-56]
   49fec:	ldur	x24, [x29, #-16]
   49ff0:	sub	x22, x8, #0x10
   49ff4:	cmp	x25, #0x2
   49ff8:	b.lt	4a0b0 <__gmpn_sbpi1_div_q@@Base+0x324>  // b.tstop
   49ffc:	cmp	x23, x15
   4a000:	cset	w8, cs  // cs = hs, nlast
   4a004:	cmp	x25, #0x2
   4a008:	b.ne	4a0c0 <__gmpn_sbpi1_div_q@@Base+0x334>  // b.any
   4a00c:	ldur	x25, [x29, #-32]
   4a010:	ldr	x21, [sp, #56]
   4a014:	mov	x10, #0xffffffffffffffff    	// #-1
   4a018:	mov	x19, x22
   4a01c:	sub	x22, x22, #0x8
   4a020:	stur	x10, [x29, #-48]
   4a024:	cbnz	w8, 4a294 <__gmpn_sbpi1_div_q@@Base+0x508>
   4a028:	ldur	x10, [x29, #-24]
   4a02c:	ldr	x8, [x19]
   4a030:	ldr	x11, [x22]
   4a034:	ldur	x25, [x29, #-56]
   4a038:	mul	x9, x23, x10
   4a03c:	umulh	x10, x23, x10
   4a040:	adds	x12, x9, x8
   4a044:	adc	x9, x10, x23
   4a048:	msub	x8, x9, x15, x8
   4a04c:	mul	x10, x9, x24
   4a050:	umulh	x13, x24, x9
   4a054:	subs	x14, x11, x24
   4a058:	sbc	x8, x8, x15
   4a05c:	subs	x11, x14, x10
   4a060:	sbc	x10, x8, x13
   4a064:	cmp	x10, x12
   4a068:	cset	w8, cs  // cs = hs, nlast
   4a06c:	csetm	x12, cs  // cs = hs, nlast
   4a070:	sub	x9, x9, x8
   4a074:	and	x13, x24, x12
   4a078:	and	x12, x15, x12
   4a07c:	adds	x8, x11, x13
   4a080:	adc	x23, x10, x12
   4a084:	cmp	x23, x15
   4a088:	add	x20, x9, #0x1
   4a08c:	b.cs	4a370 <__gmpn_sbpi1_div_q@@Base+0x5e4>  // b.hs, b.nlast
   4a090:	str	x8, [x22]
   4a094:	str	x23, [x19]
   4a098:	b	4a2bc <__gmpn_sbpi1_div_q@@Base+0x530>
   4a09c:	add	x8, x22, x27, lsl #3
   4a0a0:	stur	x8, [x29, #-40]
   4a0a4:	sub	x22, x19, #0x10
   4a0a8:	cmp	x25, #0x2
   4a0ac:	b.ge	49ffc <__gmpn_sbpi1_div_q@@Base+0x270>  // b.tcont
   4a0b0:	ldr	x21, [sp, #56]
   4a0b4:	mov	x8, #0xffffffffffffffff    	// #-1
   4a0b8:	stur	x8, [x29, #-48]
   4a0bc:	b	4a2c8 <__gmpn_sbpi1_div_q@@Base+0x53c>
   4a0c0:	mov	w9, #0x1                   	// #1
   4a0c4:	sub	x9, x9, x25
   4a0c8:	ldur	x25, [x29, #-32]
   4a0cc:	ldr	x2, [sp, #64]
   4a0d0:	stp	x27, x26, [sp, #40]
   4a0d4:	mov	x21, xzr
   4a0d8:	mov	x26, x22
   4a0dc:	add	x28, x22, x9, lsl #3
   4a0e0:	mov	x9, #0xffffffffffffffff    	// #-1
   4a0e4:	stur	x9, [x29, #-48]
   4a0e8:	b	4a144 <__gmpn_sbpi1_div_q@@Base+0x3b8>
   4a0ec:	mov	x3, #0xffffffffffffffff    	// #-1
   4a0f0:	mov	x0, x28
   4a0f4:	mov	x1, x25
   4a0f8:	mov	x20, #0xffffffffffffffff    	// #-1
   4a0fc:	mov	x27, x2
   4a100:	bl	c9e0 <__gmpn_submul_1@plt>
   4a104:	cmp	x23, x0
   4a108:	b.ne	4a1e8 <__gmpn_sbpi1_div_q@@Base+0x45c>  // b.any
   4a10c:	ldur	x15, [x29, #-8]
   4a110:	ldr	x23, [x19]
   4a114:	ldp	x9, x8, [x29, #-48]
   4a118:	mov	x2, x27
   4a11c:	add	x25, x25, #0x8
   4a120:	sub	x2, x27, #0x1
   4a124:	and	x9, x9, x15
   4a128:	add	x8, x8, x21
   4a12c:	cmp	x23, x9
   4a130:	sub	x21, x21, #0x8
   4a134:	stur	x20, [x8, #-8]
   4a138:	cset	w8, cs  // cs = hs, nlast
   4a13c:	cmp	x22, #0x1
   4a140:	b.le	4a274 <__gmpn_sbpi1_div_q@@Base+0x4e8>
   4a144:	sub	x22, x2, #0x2
   4a148:	add	x19, x26, x21
   4a14c:	tbnz	w8, #0, 4a0ec <__gmpn_sbpi1_div_q@@Base+0x360>
   4a150:	ldur	x10, [x29, #-24]
   4a154:	ldp	x11, x8, [x19, #-8]
   4a158:	mov	x27, x2
   4a15c:	mul	x9, x23, x10
   4a160:	umulh	x10, x23, x10
   4a164:	adds	x12, x9, x8
   4a168:	adc	x9, x10, x23
   4a16c:	msub	x8, x9, x15, x8
   4a170:	subs	x14, x11, x24
   4a174:	sbc	x8, x8, x15
   4a178:	mul	x10, x9, x24
   4a17c:	umulh	x13, x24, x9
   4a180:	subs	x11, x14, x10
   4a184:	sbc	x8, x8, x13
   4a188:	cmp	x8, x12
   4a18c:	cset	w10, cs  // cs = hs, nlast
   4a190:	csetm	x12, cs  // cs = hs, nlast
   4a194:	sub	x9, x9, x10
   4a198:	and	x10, x24, x12
   4a19c:	and	x12, x15, x12
   4a1a0:	adds	x24, x11, x10
   4a1a4:	adc	x23, x8, x12
   4a1a8:	cmp	x23, x15
   4a1ac:	add	x20, x9, #0x1
   4a1b0:	b.cs	4a214 <__gmpn_sbpi1_div_q@@Base+0x488>  // b.hs, b.nlast
   4a1b4:	mov	x0, x28
   4a1b8:	mov	x1, x25
   4a1bc:	mov	x2, x22
   4a1c0:	mov	x3, x20
   4a1c4:	bl	c9e0 <__gmpn_submul_1@plt>
   4a1c8:	subs	x8, x24, x0
   4a1cc:	cset	w9, cc  // cc = lo, ul, last
   4a1d0:	subs	x23, x23, x9
   4a1d4:	stur	x8, [x19, #-8]
   4a1d8:	b.cc	4a240 <__gmpn_sbpi1_div_q@@Base+0x4b4>  // b.lo, b.ul, b.last
   4a1dc:	ldur	x15, [x29, #-8]
   4a1e0:	ldur	x24, [x29, #-16]
   4a1e4:	b	4a114 <__gmpn_sbpi1_div_q@@Base+0x388>
   4a1e8:	ldur	x8, [x29, #-48]
   4a1ec:	and	x8, x0, x8
   4a1f0:	cmp	x23, x8
   4a1f4:	b.cs	4a268 <__gmpn_sbpi1_div_q@@Base+0x4dc>  // b.hs, b.nlast
   4a1f8:	mov	x0, x28
   4a1fc:	mov	x1, x28
   4a200:	mov	x2, x25
   4a204:	mov	x3, x27
   4a208:	bl	ca70 <__gmpn_add_n@plt>
   4a20c:	mov	x20, #0xfffffffffffffffe    	// #-2
   4a210:	b	4a10c <__gmpn_sbpi1_div_q@@Base+0x380>
   4a214:	ldur	x8, [x29, #-16]
   4a218:	cmp	x24, x8
   4a21c:	b.cs	4a228 <__gmpn_sbpi1_div_q@@Base+0x49c>  // b.hs, b.nlast
   4a220:	cmp	x23, x15
   4a224:	b.ls	4a1b4 <__gmpn_sbpi1_div_q@@Base+0x428>  // b.plast
   4a228:	ldur	x9, [x29, #-16]
   4a22c:	subs	x8, x24, x9
   4a230:	sbc	x23, x23, x15
   4a234:	add	x20, x20, #0x1
   4a238:	mov	x24, x8
   4a23c:	b	4a1b4 <__gmpn_sbpi1_div_q@@Base+0x428>
   4a240:	sub	x3, x27, #0x1
   4a244:	mov	x0, x28
   4a248:	mov	x1, x28
   4a24c:	mov	x2, x25
   4a250:	bl	ca70 <__gmpn_add_n@plt>
   4a254:	ldur	x15, [x29, #-8]
   4a258:	sub	x20, x20, #0x1
   4a25c:	add	x8, x23, x15
   4a260:	add	x23, x8, x0
   4a264:	b	4a1e0 <__gmpn_sbpi1_div_q@@Base+0x454>
   4a268:	stur	xzr, [x29, #-48]
   4a26c:	mov	x20, #0xffffffffffffffff    	// #-1
   4a270:	b	4a10c <__gmpn_sbpi1_div_q@@Base+0x380>
   4a274:	ldur	x9, [x29, #-40]
   4a278:	add	x19, x26, x21
   4a27c:	ldr	x27, [sp, #40]
   4a280:	sub	x22, x19, #0x8
   4a284:	add	x9, x9, x21
   4a288:	ldp	x26, x21, [sp, #48]
   4a28c:	stur	x9, [x29, #-40]
   4a290:	cbz	w8, 4a028 <__gmpn_sbpi1_div_q@@Base+0x29c>
   4a294:	mov	w2, #0x2                   	// #2
   4a298:	mov	x3, #0xffffffffffffffff    	// #-1
   4a29c:	mov	x0, x22
   4a2a0:	mov	x1, x25
   4a2a4:	mov	x20, #0xffffffffffffffff    	// #-1
   4a2a8:	bl	c9e0 <__gmpn_submul_1@plt>
   4a2ac:	cmp	x23, x0
   4a2b0:	b.ne	4a394 <__gmpn_sbpi1_div_q@@Base+0x608>  // b.any
   4a2b4:	ldr	x23, [x19]
   4a2b8:	ldur	x25, [x29, #-56]
   4a2bc:	ldur	x8, [x29, #-40]
   4a2c0:	str	x20, [x8, #-8]!
   4a2c4:	stur	x8, [x29, #-40]
   4a2c8:	ldr	x8, [x22, #8]
   4a2cc:	cmp	x8, x23
   4a2d0:	b.ne	4a590 <__gmpn_sbpi1_div_q@@Base+0x804>  // b.any
   4a2d4:	ldur	x8, [x29, #-48]
   4a2d8:	and	x8, x8, x21
   4a2dc:	cmp	x23, x8
   4a2e0:	b.cc	4a308 <__gmpn_sbpi1_div_q@@Base+0x57c>  // b.lo, b.ul, b.last
   4a2e4:	mov	x0, x26
   4a2e8:	ldp	x20, x19, [sp, #208]
   4a2ec:	ldp	x22, x21, [sp, #192]
   4a2f0:	ldp	x24, x23, [sp, #176]
   4a2f4:	ldp	x26, x25, [sp, #160]
   4a2f8:	ldp	x28, x27, [sp, #144]
   4a2fc:	ldp	x29, x30, [sp, #128]
   4a300:	add	sp, sp, #0xe0
   4a304:	ret
   4a308:	cmp	x21, #0x3
   4a30c:	b.lt	4a3d4 <__gmpn_sbpi1_div_q@@Base+0x648>  // b.tstop
   4a310:	mov	x24, x21
   4a314:	ldr	x21, [x22]
   4a318:	mov	x8, x22
   4a31c:	subs	x22, x25, #0x3
   4a320:	b.lt	4a3c8 <__gmpn_sbpi1_div_q@@Base+0x63c>  // b.tstop
   4a324:	mov	x25, x8
   4a328:	sub	x19, x8, #0x8
   4a32c:	mov	w20, #0x1                   	// #1
   4a330:	b	4a348 <__gmpn_sbpi1_div_q@@Base+0x5bc>
   4a334:	cmp	x22, #0x0
   4a338:	sub	x22, x22, #0x1
   4a33c:	add	x20, x20, #0x1
   4a340:	sub	x19, x19, #0x8
   4a344:	b.le	4a3cc <__gmpn_sbpi1_div_q@@Base+0x640>
   4a348:	ldp	x8, x1, [x29, #-40]
   4a34c:	mov	x0, x19
   4a350:	mov	x2, x20
   4a354:	ldr	x3, [x8, x22, lsl #3]
   4a358:	bl	c9e0 <__gmpn_submul_1@plt>
   4a35c:	subs	x21, x21, x0
   4a360:	b.cs	4a334 <__gmpn_sbpi1_div_q@@Base+0x5a8>  // b.hs, b.nlast
   4a364:	cbz	x23, 4a4b8 <__gmpn_sbpi1_div_q@@Base+0x72c>
   4a368:	sub	x23, x23, #0x1
   4a36c:	b	4a334 <__gmpn_sbpi1_div_q@@Base+0x5a8>
   4a370:	cmp	x8, x24
   4a374:	b.cs	4a380 <__gmpn_sbpi1_div_q@@Base+0x5f4>  // b.hs, b.nlast
   4a378:	cmp	x23, x15
   4a37c:	b.ls	4a090 <__gmpn_sbpi1_div_q@@Base+0x304>  // b.plast
   4a380:	subs	x9, x8, x24
   4a384:	sbc	x23, x23, x15
   4a388:	add	x20, x20, #0x1
   4a38c:	mov	x8, x9
   4a390:	b	4a090 <__gmpn_sbpi1_div_q@@Base+0x304>
   4a394:	ldur	x8, [x29, #-48]
   4a398:	and	x8, x0, x8
   4a39c:	cmp	x23, x8
   4a3a0:	b.cs	4a4ac <__gmpn_sbpi1_div_q@@Base+0x720>  // b.hs, b.nlast
   4a3a4:	ldr	x8, [x19]
   4a3a8:	mov	x20, #0xfffffffffffffffe    	// #-2
   4a3ac:	ldp	x9, x10, [x25]
   4a3b0:	ldr	x11, [x22]
   4a3b4:	adds	x12, x11, x9
   4a3b8:	adc	x8, x8, x10
   4a3bc:	str	x8, [x19]
   4a3c0:	str	x12, [x22]
   4a3c4:	b	4a2b4 <__gmpn_sbpi1_div_q@@Base+0x528>
   4a3c8:	mov	x25, x8
   4a3cc:	str	x21, [x25]
   4a3d0:	mov	x21, x24
   4a3d4:	ldr	x8, [sp, #16]
   4a3d8:	cmp	x8, x21
   4a3dc:	b.ge	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tcont
   4a3e0:	ldr	w8, [sp, #12]
   4a3e4:	cbz	w8, 4a408 <__gmpn_sbpi1_div_q@@Base+0x67c>
   4a3e8:	ldp	x2, x8, [sp, #24]
   4a3ec:	ldr	x3, [sp]
   4a3f0:	add	x0, x8, x27, lsl #3
   4a3f4:	mov	x1, x0
   4a3f8:	bl	c2d0 <__gmpn_sub_n@plt>
   4a3fc:	cbz	x0, 4a408 <__gmpn_sbpi1_div_q@@Base+0x67c>
   4a400:	cbz	x23, 4a4fc <__gmpn_sbpi1_div_q@@Base+0x770>
   4a404:	sub	x23, x23, #0x1
   4a408:	cbz	x27, 4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a40c:	sub	x19, x21, x27
   4a410:	subs	x20, x19, #0x2
   4a414:	b.lt	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tstop
   4a418:	ldr	x8, [sp, #32]
   4a41c:	mov	x9, x21
   4a420:	add	x21, x8, x27, lsl #3
   4a424:	add	x22, x8, x9, lsl #3
   4a428:	b	4a43c <__gmpn_sbpi1_div_q@@Base+0x6b0>
   4a42c:	cmp	x20, #0x0
   4a430:	sub	x20, x20, #0x1
   4a434:	sub	x22, x22, #0x8
   4a438:	b.le	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a43c:	ldr	x8, [sp, #24]
   4a440:	lsl	x24, x20, #3
   4a444:	ldur	x1, [x29, #-40]
   4a448:	mov	x2, x27
   4a44c:	ldr	x3, [x8, x24]
   4a450:	ldr	x8, [sp, #32]
   4a454:	add	x0, x8, x24
   4a458:	bl	c9e0 <__gmpn_submul_1@plt>
   4a45c:	ldr	x8, [x21, x24]
   4a460:	subs	x8, x8, x0
   4a464:	str	x8, [x21, x24]
   4a468:	b.cs	4a42c <__gmpn_sbpi1_div_q@@Base+0x6a0>  // b.hs, b.nlast
   4a46c:	mvn	x8, x20
   4a470:	add	x8, x19, x8
   4a474:	mov	x9, #0xffffffffffffffff    	// #-1
   4a478:	add	x10, x9, #0x2
   4a47c:	cmp	x10, x8
   4a480:	b.ge	4a4a0 <__gmpn_sbpi1_div_q@@Base+0x714>  // b.tcont
   4a484:	lsl	x10, x9, #3
   4a488:	ldr	x11, [x22, x10]
   4a48c:	add	x9, x9, #0x1
   4a490:	sub	x12, x11, #0x1
   4a494:	str	x12, [x22, x10]
   4a498:	cbz	x11, 4a478 <__gmpn_sbpi1_div_q@@Base+0x6ec>
   4a49c:	b	4a42c <__gmpn_sbpi1_div_q@@Base+0x6a0>
   4a4a0:	cbz	x23, 4a550 <__gmpn_sbpi1_div_q@@Base+0x7c4>
   4a4a4:	sub	x23, x23, #0x1
   4a4a8:	b	4a42c <__gmpn_sbpi1_div_q@@Base+0x6a0>
   4a4ac:	stur	xzr, [x29, #-48]
   4a4b0:	mov	x20, #0xffffffffffffffff    	// #-1
   4a4b4:	b	4a2b4 <__gmpn_sbpi1_div_q@@Base+0x528>
   4a4b8:	ldur	x10, [x29, #-40]
   4a4bc:	ldr	x8, [x10]
   4a4c0:	sub	x9, x8, #0x1
   4a4c4:	str	x9, [x10]
   4a4c8:	cbnz	x8, 4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a4cc:	ldur	x13, [x29, #-40]
   4a4d0:	mov	w8, #0x1                   	// #1
   4a4d4:	mov	x12, x27
   4a4d8:	cmp	x8, x12
   4a4dc:	b.ge	4a5a8 <__gmpn_sbpi1_div_q@@Base+0x81c>  // b.tcont
   4a4e0:	lsl	x9, x8, #3
   4a4e4:	ldr	x10, [x13, x9]
   4a4e8:	add	x8, x8, #0x1
   4a4ec:	sub	x11, x10, #0x1
   4a4f0:	str	x11, [x13, x9]
   4a4f4:	cbz	x10, 4a4d8 <__gmpn_sbpi1_div_q@@Base+0x74c>
   4a4f8:	b	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a4fc:	cbz	x27, 4a548 <__gmpn_sbpi1_div_q@@Base+0x7bc>
   4a500:	ldur	x10, [x29, #-40]
   4a504:	ldr	x8, [x10]
   4a508:	sub	x9, x8, #0x1
   4a50c:	str	x9, [x10]
   4a510:	cbnz	x8, 4a544 <__gmpn_sbpi1_div_q@@Base+0x7b8>
   4a514:	ldur	x13, [x29, #-40]
   4a518:	mov	x12, x27
   4a51c:	mov	w0, #0x1                   	// #1
   4a520:	mov	w8, #0x1                   	// #1
   4a524:	cmp	x8, x12
   4a528:	b.ge	4a548 <__gmpn_sbpi1_div_q@@Base+0x7bc>  // b.tcont
   4a52c:	lsl	x9, x8, #3
   4a530:	ldr	x10, [x13, x9]
   4a534:	add	x8, x8, #0x1
   4a538:	sub	x11, x10, #0x1
   4a53c:	str	x11, [x13, x9]
   4a540:	cbz	x10, 4a524 <__gmpn_sbpi1_div_q@@Base+0x798>
   4a544:	mov	x0, xzr
   4a548:	sub	x26, x26, x0
   4a54c:	b	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a550:	ldur	x10, [x29, #-40]
   4a554:	ldr	x8, [x10]
   4a558:	sub	x9, x8, #0x1
   4a55c:	str	x9, [x10]
   4a560:	cbnz	x8, 4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a564:	ldur	x12, [x29, #-40]
   4a568:	mov	w8, #0x1                   	// #1
   4a56c:	cmp	x8, x27
   4a570:	b.ge	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>  // b.tcont
   4a574:	lsl	x9, x8, #3
   4a578:	ldr	x10, [x12, x9]
   4a57c:	add	x8, x8, #0x1
   4a580:	sub	x11, x10, #0x1
   4a584:	str	x11, [x12, x9]
   4a588:	cbz	x10, 4a56c <__gmpn_sbpi1_div_q@@Base+0x7e0>
   4a58c:	b	4a2e4 <__gmpn_sbpi1_div_q@@Base+0x558>
   4a590:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4a594:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4a598:	add	x0, x0, #0xb37
   4a59c:	add	x2, x2, #0xb45
   4a5a0:	mov	w1, #0xc5                  	// #197
   4a5a4:	bl	c6c0 <__gmp_assert_fail@plt>
   4a5a8:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4a5ac:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4a5b0:	add	x0, x0, #0xb37
   4a5b4:	add	x2, x2, #0xb51
   4a5b8:	mov	w1, #0xf8                  	// #248
   4a5bc:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004a5c0 <__gmpn_sbpi1_div_qr@@Base>:
   4a5c0:	sub	sp, sp, #0xb0
   4a5c4:	stp	x20, x19, [sp, #160]
   4a5c8:	add	x20, x1, x2, lsl #3
   4a5cc:	stp	x29, x30, [sp, #80]
   4a5d0:	stp	x28, x27, [sp, #96]
   4a5d4:	stp	x24, x23, [sp, #128]
   4a5d8:	stp	x22, x21, [sp, #144]
   4a5dc:	add	x29, sp, #0x50
   4a5e0:	mov	x21, x4
   4a5e4:	mov	x22, x2
   4a5e8:	mov	x23, x1
   4a5ec:	mov	x27, x0
   4a5f0:	sub	x0, x20, x4, lsl #3
   4a5f4:	sub	x8, x20, #0x8
   4a5f8:	mov	x9, x4
   4a5fc:	mov	x12, x3
   4a600:	stp	x26, x25, [sp, #112]
   4a604:	stp	x5, x3, [x29, #-16]
   4a608:	subs	x10, x9, #0x1
   4a60c:	b.lt	4a62c <__gmpn_sbpi1_div_qr@@Base+0x6c>  // b.tstop
   4a610:	add	x9, x12, x9, lsl #3
   4a614:	ldr	x11, [x8], #-8
   4a618:	ldur	x9, [x9, #-8]
   4a61c:	cmp	x11, x9
   4a620:	mov	x9, x10
   4a624:	b.eq	4a608 <__gmpn_sbpi1_div_qr@@Base+0x48>  // b.none
   4a628:	b.ls	4a644 <__gmpn_sbpi1_div_qr@@Base+0x84>  // b.plast
   4a62c:	ldur	x2, [x29, #-8]
   4a630:	mov	x1, x0
   4a634:	mov	x3, x21
   4a638:	bl	c2d0 <__gmpn_sub_n@plt>
   4a63c:	mov	w0, #0x1                   	// #1
   4a640:	b	4a648 <__gmpn_sbpi1_div_qr@@Base+0x88>
   4a644:	mov	x0, xzr
   4a648:	ldur	x19, [x20, #-8]
   4a64c:	sub	x8, x22, x21
   4a650:	cmp	x8, #0x1
   4a654:	b.lt	4a7f8 <__gmpn_sbpi1_div_qr@@Base+0x238>  // b.tstop
   4a658:	ldur	x10, [x29, #-8]
   4a65c:	stp	x0, x22, [sp, #24]
   4a660:	sub	x11, x21, #0x2
   4a664:	sub	x12, x21, #0x1
   4a668:	ldr	x16, [x10, x12, lsl #3]
   4a66c:	ldr	x10, [x10, x11, lsl #3]
   4a670:	mvn	x9, x21
   4a674:	lsl	x9, x9, #3
   4a678:	lsl	x28, x22, #3
   4a67c:	add	x20, x27, x9
   4a680:	add	x25, x23, x9
   4a684:	add	x22, x8, #0x1
   4a688:	str	x21, [sp, #8]
   4a68c:	str	x12, [sp, #16]
   4a690:	str	x11, [sp, #40]
   4a694:	stp	x16, x10, [x29, #-32]
   4a698:	cmp	x19, x16
   4a69c:	add	x26, x23, x28
   4a6a0:	b.eq	4a75c <__gmpn_sbpi1_div_qr@@Base+0x19c>  // b.none
   4a6a4:	ldp	x15, x10, [x29, #-24]
   4a6a8:	ldp	x11, x8, [x26, #-24]
   4a6ac:	mul	x9, x19, x10
   4a6b0:	umulh	x10, x19, x10
   4a6b4:	adds	x12, x9, x8
   4a6b8:	adc	x9, x10, x19
   4a6bc:	msub	x8, x9, x16, x8
   4a6c0:	subs	x14, x11, x15
   4a6c4:	sbc	x8, x8, x16
   4a6c8:	mul	x10, x9, x15
   4a6cc:	umulh	x13, x15, x9
   4a6d0:	subs	x11, x14, x10
   4a6d4:	sbc	x8, x8, x13
   4a6d8:	cmp	x8, x12
   4a6dc:	cset	w10, cs  // cs = hs, nlast
   4a6e0:	csetm	x12, cs  // cs = hs, nlast
   4a6e4:	sub	x9, x9, x10
   4a6e8:	and	x10, x15, x12
   4a6ec:	and	x12, x16, x12
   4a6f0:	adds	x21, x11, x10
   4a6f4:	adc	x19, x8, x12
   4a6f8:	cmp	x19, x16
   4a6fc:	add	x27, x9, #0x1
   4a700:	b.cs	4a790 <__gmpn_sbpi1_div_qr@@Base+0x1d0>  // b.hs, b.nlast
   4a704:	ldur	x1, [x29, #-8]
   4a708:	ldr	x2, [sp, #40]
   4a70c:	mov	x24, x28
   4a710:	add	x28, x25, x28
   4a714:	mov	x0, x28
   4a718:	mov	x3, x27
   4a71c:	bl	c9e0 <__gmpn_submul_1@plt>
   4a720:	subs	x8, x21, x0
   4a724:	cset	w9, cc  // cc = lo, ul, last
   4a728:	subs	x19, x19, x9
   4a72c:	stur	x8, [x26, #-24]
   4a730:	b.cc	4a7bc <__gmpn_sbpi1_div_qr@@Base+0x1fc>  // b.lo, b.ul, b.last
   4a734:	ldur	x16, [x29, #-32]
   4a738:	mov	x28, x24
   4a73c:	sub	x22, x22, #0x1
   4a740:	str	x27, [x20, x28]
   4a744:	sub	x20, x20, #0x8
   4a748:	sub	x23, x23, #0x8
   4a74c:	cmp	x22, #0x1
   4a750:	sub	x25, x25, #0x8
   4a754:	b.gt	4a698 <__gmpn_sbpi1_div_qr@@Base+0xd8>
   4a758:	b	4a7e8 <__gmpn_sbpi1_div_qr@@Base+0x228>
   4a75c:	ldur	x8, [x26, #-16]
   4a760:	ldur	x9, [x29, #-24]
   4a764:	cmp	x8, x9
   4a768:	b.ne	4a6a4 <__gmpn_sbpi1_div_qr@@Base+0xe4>  // b.any
   4a76c:	ldur	x1, [x29, #-8]
   4a770:	ldr	x2, [sp, #8]
   4a774:	add	x0, x25, x28
   4a778:	mov	x3, #0xffffffffffffffff    	// #-1
   4a77c:	mov	x27, #0xffffffffffffffff    	// #-1
   4a780:	bl	c9e0 <__gmpn_submul_1@plt>
   4a784:	ldur	x16, [x29, #-32]
   4a788:	ldur	x19, [x26, #-16]
   4a78c:	b	4a73c <__gmpn_sbpi1_div_qr@@Base+0x17c>
   4a790:	ldur	x8, [x29, #-24]
   4a794:	cmp	x21, x8
   4a798:	b.cs	4a7a4 <__gmpn_sbpi1_div_qr@@Base+0x1e4>  // b.hs, b.nlast
   4a79c:	cmp	x19, x16
   4a7a0:	b.ls	4a704 <__gmpn_sbpi1_div_qr@@Base+0x144>  // b.plast
   4a7a4:	ldur	x9, [x29, #-24]
   4a7a8:	subs	x8, x21, x9
   4a7ac:	sbc	x19, x19, x16
   4a7b0:	add	x27, x27, #0x1
   4a7b4:	mov	x21, x8
   4a7b8:	b	4a704 <__gmpn_sbpi1_div_qr@@Base+0x144>
   4a7bc:	ldur	x2, [x29, #-8]
   4a7c0:	ldr	x3, [sp, #16]
   4a7c4:	mov	x0, x28
   4a7c8:	mov	x1, x28
   4a7cc:	bl	ca70 <__gmpn_add_n@plt>
   4a7d0:	ldur	x16, [x29, #-32]
   4a7d4:	sub	x27, x27, #0x1
   4a7d8:	mov	x28, x24
   4a7dc:	add	x8, x19, x16
   4a7e0:	add	x19, x8, x0
   4a7e4:	b	4a73c <__gmpn_sbpi1_div_qr@@Base+0x17c>
   4a7e8:	ldp	x0, x8, [sp, #24]
   4a7ec:	add	x8, x23, x8, lsl #3
   4a7f0:	sub	x8, x8, #0x10
   4a7f4:	b	4a7fc <__gmpn_sbpi1_div_qr@@Base+0x23c>
   4a7f8:	sub	x8, x20, #0x10
   4a7fc:	str	x19, [x8, #8]
   4a800:	ldp	x20, x19, [sp, #160]
   4a804:	ldp	x22, x21, [sp, #144]
   4a808:	ldp	x24, x23, [sp, #128]
   4a80c:	ldp	x26, x25, [sp, #112]
   4a810:	ldp	x28, x27, [sp, #96]
   4a814:	ldp	x29, x30, [sp, #80]
   4a818:	add	sp, sp, #0xb0
   4a81c:	ret

000000000004a820 <__gmpn_sbpi1_divappr_q@@Base>:
   4a820:	sub	sp, sp, #0xa0
   4a824:	stp	x22, x21, [sp, #128]
   4a828:	sub	x22, x2, x4
   4a82c:	add	x8, x22, #0x1
   4a830:	subs	x8, x4, x8
   4a834:	stp	x20, x19, [sp, #144]
   4a838:	add	x20, x1, x2, lsl #3
   4a83c:	add	x8, x3, x8, lsl #3
   4a840:	csinc	x12, x4, x22, le
   4a844:	stp	x29, x30, [sp, #64]
   4a848:	stp	x28, x27, [sp, #80]
   4a84c:	stp	x26, x25, [sp, #96]
   4a850:	stp	x24, x23, [sp, #112]
   4a854:	add	x29, sp, #0x40
   4a858:	mov	x27, x4
   4a85c:	mov	x26, x2
   4a860:	mov	x23, x1
   4a864:	mov	x25, x0
   4a868:	csel	x15, x8, x3, gt
   4a86c:	sub	x0, x20, x12, lsl #3
   4a870:	sub	x8, x20, #0x8
   4a874:	mov	x9, x12
   4a878:	stur	x5, [x29, #-24]
   4a87c:	str	x15, [sp, #32]
   4a880:	subs	x10, x9, #0x1
   4a884:	b.lt	4a8a4 <__gmpn_sbpi1_divappr_q@@Base+0x84>  // b.tstop
   4a888:	add	x9, x15, x9, lsl #3
   4a88c:	ldr	x11, [x8], #-8
   4a890:	ldur	x9, [x9, #-8]
   4a894:	cmp	x11, x9
   4a898:	mov	x9, x10
   4a89c:	b.eq	4a880 <__gmpn_sbpi1_divappr_q@@Base+0x60>  // b.none
   4a8a0:	b.ls	4a8c8 <__gmpn_sbpi1_divappr_q@@Base+0xa8>  // b.plast
   4a8a4:	mov	x1, x0
   4a8a8:	mov	x2, x15
   4a8ac:	mov	x3, x12
   4a8b0:	mov	x19, x12
   4a8b4:	bl	c2d0 <__gmpn_sub_n@plt>
   4a8b8:	ldr	x15, [sp, #32]
   4a8bc:	mov	x12, x19
   4a8c0:	mov	w19, #0x1                   	// #1
   4a8c4:	b	4a8cc <__gmpn_sbpi1_divappr_q@@Base+0xac>
   4a8c8:	mov	x19, xzr
   4a8cc:	sub	x8, x12, #0x2
   4a8d0:	sub	x9, x12, #0x1
   4a8d4:	ldr	x16, [x15, x9, lsl #3]
   4a8d8:	str	x8, [sp, #24]
   4a8dc:	ldr	x8, [x15, x8, lsl #3]
   4a8e0:	stp	x8, x16, [x29, #-16]
   4a8e4:	ldur	x21, [x20, #-8]
   4a8e8:	subs	x8, x22, x12
   4a8ec:	b.mi	4aafc <__gmpn_sbpi1_divappr_q@@Base+0x2dc>  // b.first
   4a8f0:	add	x22, x8, #0x1
   4a8f4:	mvn	x8, x12
   4a8f8:	sub	x24, x25, x27, lsl #3
   4a8fc:	lsl	x25, x26, #3
   4a900:	add	x20, x23, x8, lsl #3
   4a904:	stp	x9, x12, [sp]
   4a908:	str	x19, [sp, #16]
   4a90c:	ldur	x17, [x29, #-16]
   4a910:	cmp	x21, x16
   4a914:	add	x28, x23, x25
   4a918:	b.eq	4a9d4 <__gmpn_sbpi1_divappr_q@@Base+0x1b4>  // b.none
   4a91c:	ldur	x10, [x29, #-24]
   4a920:	ldp	x11, x8, [x28, #-24]
   4a924:	mul	x9, x21, x10
   4a928:	umulh	x10, x21, x10
   4a92c:	adds	x12, x9, x8
   4a930:	adc	x9, x10, x21
   4a934:	msub	x8, x9, x16, x8
   4a938:	subs	x14, x11, x17
   4a93c:	sbc	x8, x8, x16
   4a940:	mul	x10, x9, x17
   4a944:	umulh	x13, x17, x9
   4a948:	subs	x11, x14, x10
   4a94c:	sbc	x8, x8, x13
   4a950:	cmp	x8, x12
   4a954:	cset	w10, cs  // cs = hs, nlast
   4a958:	csetm	x12, cs  // cs = hs, nlast
   4a95c:	sub	x9, x9, x10
   4a960:	and	x10, x17, x12
   4a964:	and	x12, x16, x12
   4a968:	adds	x19, x11, x10
   4a96c:	adc	x21, x8, x12
   4a970:	cmp	x21, x16
   4a974:	add	x26, x9, #0x1
   4a978:	b.cs	4aa04 <__gmpn_sbpi1_divappr_q@@Base+0x1e4>  // b.hs, b.nlast
   4a97c:	ldr	x2, [sp, #24]
   4a980:	add	x27, x20, x25
   4a984:	mov	x0, x27
   4a988:	mov	x1, x15
   4a98c:	mov	x3, x26
   4a990:	bl	c9e0 <__gmpn_submul_1@plt>
   4a994:	subs	x8, x19, x0
   4a998:	cset	w9, cc  // cc = lo, ul, last
   4a99c:	subs	x21, x21, x9
   4a9a0:	stur	x8, [x28, #-24]
   4a9a4:	b.cc	4aa30 <__gmpn_sbpi1_divappr_q@@Base+0x210>  // b.lo, b.ul, b.last
   4a9a8:	ldur	x16, [x29, #-8]
   4a9ac:	ldr	x15, [sp, #32]
   4a9b0:	sub	x22, x22, #0x1
   4a9b4:	add	x8, x24, x25
   4a9b8:	sub	x24, x24, #0x8
   4a9bc:	sub	x23, x23, #0x8
   4a9c0:	cmp	x22, #0x0
   4a9c4:	sub	x20, x20, #0x8
   4a9c8:	stur	x26, [x8, #-8]
   4a9cc:	b.gt	4a90c <__gmpn_sbpi1_divappr_q@@Base+0xec>
   4a9d0:	b	4aa58 <__gmpn_sbpi1_divappr_q@@Base+0x238>
   4a9d4:	ldur	x8, [x28, #-16]
   4a9d8:	cmp	x8, x17
   4a9dc:	b.ne	4a91c <__gmpn_sbpi1_divappr_q@@Base+0xfc>  // b.any
   4a9e0:	ldr	x2, [sp, #8]
   4a9e4:	add	x0, x20, x25
   4a9e8:	mov	x3, #0xffffffffffffffff    	// #-1
   4a9ec:	mov	x1, x15
   4a9f0:	mov	x26, #0xffffffffffffffff    	// #-1
   4a9f4:	bl	c9e0 <__gmpn_submul_1@plt>
   4a9f8:	ldur	x16, [x29, #-8]
   4a9fc:	ldur	x21, [x28, #-16]
   4aa00:	b	4a9ac <__gmpn_sbpi1_divappr_q@@Base+0x18c>
   4aa04:	ldur	x8, [x29, #-16]
   4aa08:	cmp	x19, x8
   4aa0c:	b.cs	4aa18 <__gmpn_sbpi1_divappr_q@@Base+0x1f8>  // b.hs, b.nlast
   4aa10:	cmp	x21, x16
   4aa14:	b.ls	4a97c <__gmpn_sbpi1_divappr_q@@Base+0x15c>  // b.plast
   4aa18:	ldur	x9, [x29, #-16]
   4aa1c:	subs	x8, x19, x9
   4aa20:	sbc	x21, x21, x16
   4aa24:	add	x26, x26, #0x1
   4aa28:	mov	x19, x8
   4aa2c:	b	4a97c <__gmpn_sbpi1_divappr_q@@Base+0x15c>
   4aa30:	ldr	x2, [sp, #32]
   4aa34:	ldr	x3, [sp]
   4aa38:	mov	x0, x27
   4aa3c:	mov	x1, x27
   4aa40:	bl	ca70 <__gmpn_add_n@plt>
   4aa44:	ldur	x16, [x29, #-8]
   4aa48:	sub	x26, x26, #0x1
   4aa4c:	add	x8, x21, x16
   4aa50:	add	x21, x8, x0
   4aa54:	b	4a9ac <__gmpn_sbpi1_divappr_q@@Base+0x18c>
   4aa58:	ldp	x12, x19, [sp, #8]
   4aa5c:	add	x8, x23, x25
   4aa60:	add	x28, x24, x25
   4aa64:	sub	x26, x8, #0x10
   4aa68:	cmp	x12, #0x2
   4aa6c:	b.lt	4ab0c <__gmpn_sbpi1_divappr_q@@Base+0x2ec>  // b.tstop
   4aa70:	cmp	x21, x16
   4aa74:	cset	w8, cs  // cs = hs, nlast
   4aa78:	cmp	x12, #0x2
   4aa7c:	b.ne	4ab14 <__gmpn_sbpi1_divappr_q@@Base+0x2f4>  // b.any
   4aa80:	sub	x22, x26, #0x8
   4aa84:	mov	x23, #0xffffffffffffffff    	// #-1
   4aa88:	cbnz	w8, 4ad00 <__gmpn_sbpi1_divappr_q@@Base+0x4e0>
   4aa8c:	ldp	x10, x15, [x29, #-24]
   4aa90:	ldr	x8, [x26]
   4aa94:	ldr	x11, [x22]
   4aa98:	mul	x9, x21, x10
   4aa9c:	umulh	x10, x21, x10
   4aaa0:	adds	x12, x9, x8
   4aaa4:	adc	x9, x10, x21
   4aaa8:	msub	x8, x9, x16, x8
   4aaac:	mul	x10, x9, x15
   4aab0:	umulh	x13, x15, x9
   4aab4:	subs	x14, x11, x15
   4aab8:	sbc	x8, x8, x16
   4aabc:	subs	x11, x14, x10
   4aac0:	sbc	x10, x8, x13
   4aac4:	cmp	x10, x12
   4aac8:	cset	w8, cs  // cs = hs, nlast
   4aacc:	csetm	x12, cs  // cs = hs, nlast
   4aad0:	sub	x9, x9, x8
   4aad4:	and	x13, x15, x12
   4aad8:	and	x12, x16, x12
   4aadc:	adds	x8, x11, x13
   4aae0:	adc	x21, x10, x12
   4aae4:	cmp	x21, x16
   4aae8:	add	x20, x9, #0x1
   4aaec:	b.cs	4ad6c <__gmpn_sbpi1_divappr_q@@Base+0x54c>  // b.hs, b.nlast
   4aaf0:	str	x21, [x26]
   4aaf4:	str	x8, [x22]
   4aaf8:	b	4ad38 <__gmpn_sbpi1_divappr_q@@Base+0x518>
   4aafc:	add	x28, x25, x22, lsl #3
   4ab00:	sub	x26, x20, #0x10
   4ab04:	cmp	x12, #0x2
   4ab08:	b.ge	4aa70 <__gmpn_sbpi1_divappr_q@@Base+0x250>  // b.tcont
   4ab0c:	mov	x22, x26
   4ab10:	b	4ad3c <__gmpn_sbpi1_divappr_q@@Base+0x51c>
   4ab14:	mov	w9, #0x1                   	// #1
   4ab18:	sub	x9, x9, x12
   4ab1c:	mov	x27, xzr
   4ab20:	add	x9, x26, x9, lsl #3
   4ab24:	mov	x23, #0xffffffffffffffff    	// #-1
   4ab28:	str	x19, [sp, #16]
   4ab2c:	str	x9, [sp, #32]
   4ab30:	b	4ab94 <__gmpn_sbpi1_divappr_q@@Base+0x374>
   4ab34:	ldr	x0, [sp, #32]
   4ab38:	mov	x3, #0xffffffffffffffff    	// #-1
   4ab3c:	mov	x1, x15
   4ab40:	mov	x2, x12
   4ab44:	mov	x25, #0xffffffffffffffff    	// #-1
   4ab48:	mov	x22, x15
   4ab4c:	mov	x19, x12
   4ab50:	bl	c9e0 <__gmpn_submul_1@plt>
   4ab54:	cmp	x21, x0
   4ab58:	b.ne	4ac54 <__gmpn_sbpi1_divappr_q@@Base+0x434>  // b.any
   4ab5c:	ldur	x16, [x29, #-8]
   4ab60:	ldr	x21, [x20]
   4ab64:	mov	x12, x19
   4ab68:	and	x9, x23, x16
   4ab6c:	add	x8, x28, x27
   4ab70:	mov	x15, x22
   4ab74:	cmp	x21, x9
   4ab78:	add	x15, x22, #0x8
   4ab7c:	sub	x27, x27, #0x8
   4ab80:	stur	x25, [x8, #-8]
   4ab84:	cset	w8, cs  // cs = hs, nlast
   4ab88:	cmp	x24, #0x1
   4ab8c:	sub	x12, x12, #0x1
   4ab90:	b.le	4acec <__gmpn_sbpi1_divappr_q@@Base+0x4cc>
   4ab94:	sub	x24, x12, #0x2
   4ab98:	add	x20, x26, x27
   4ab9c:	tbnz	w8, #0, 4ab34 <__gmpn_sbpi1_divappr_q@@Base+0x314>
   4aba0:	str	x23, [sp, #24]
   4aba4:	ldp	x10, x17, [x29, #-24]
   4aba8:	ldp	x11, x8, [x20, #-8]
   4abac:	mov	x23, x26
   4abb0:	mov	x26, x28
   4abb4:	mul	x9, x21, x10
   4abb8:	mov	x28, x12
   4abbc:	umulh	x10, x21, x10
   4abc0:	adds	x12, x9, x8
   4abc4:	adc	x9, x10, x21
   4abc8:	msub	x8, x9, x16, x8
   4abcc:	subs	x14, x11, x17
   4abd0:	sbc	x8, x8, x16
   4abd4:	mul	x10, x9, x17
   4abd8:	umulh	x13, x17, x9
   4abdc:	subs	x11, x14, x10
   4abe0:	sbc	x8, x8, x13
   4abe4:	cmp	x8, x12
   4abe8:	cset	w10, cs  // cs = hs, nlast
   4abec:	csetm	x12, cs  // cs = hs, nlast
   4abf0:	sub	x9, x9, x10
   4abf4:	and	x10, x17, x12
   4abf8:	and	x12, x16, x12
   4abfc:	adds	x19, x11, x10
   4ac00:	adc	x21, x8, x12
   4ac04:	cmp	x21, x16
   4ac08:	add	x25, x9, #0x1
   4ac0c:	b.cs	4acb4 <__gmpn_sbpi1_divappr_q@@Base+0x494>  // b.hs, b.nlast
   4ac10:	ldr	x0, [sp, #32]
   4ac14:	mov	x1, x15
   4ac18:	mov	x2, x24
   4ac1c:	mov	x3, x25
   4ac20:	mov	x22, x15
   4ac24:	bl	c9e0 <__gmpn_submul_1@plt>
   4ac28:	subs	x8, x19, x0
   4ac2c:	cset	w9, cc  // cc = lo, ul, last
   4ac30:	subs	x21, x21, x9
   4ac34:	stur	x8, [x20, #-8]
   4ac38:	b.cc	4ac88 <__gmpn_sbpi1_divappr_q@@Base+0x468>  // b.lo, b.ul, b.last
   4ac3c:	ldur	x16, [x29, #-8]
   4ac40:	mov	x12, x28
   4ac44:	mov	x28, x26
   4ac48:	mov	x26, x23
   4ac4c:	ldr	x23, [sp, #24]
   4ac50:	b	4ab68 <__gmpn_sbpi1_divappr_q@@Base+0x348>
   4ac54:	and	x8, x0, x23
   4ac58:	cmp	x21, x8
   4ac5c:	b.cs	4ace0 <__gmpn_sbpi1_divappr_q@@Base+0x4c0>  // b.hs, b.nlast
   4ac60:	ldr	x0, [sp, #32]
   4ac64:	mov	x2, x22
   4ac68:	mov	x3, x19
   4ac6c:	mov	x1, x0
   4ac70:	bl	ca70 <__gmpn_add_n@plt>
   4ac74:	ldur	x16, [x29, #-8]
   4ac78:	ldr	x21, [x20]
   4ac7c:	mov	x12, x19
   4ac80:	mov	x25, #0xfffffffffffffffe    	// #-2
   4ac84:	b	4ab68 <__gmpn_sbpi1_divappr_q@@Base+0x348>
   4ac88:	ldr	x0, [sp, #32]
   4ac8c:	sub	x3, x28, #0x1
   4ac90:	mov	x2, x22
   4ac94:	mov	x1, x0
   4ac98:	bl	ca70 <__gmpn_add_n@plt>
   4ac9c:	ldur	x16, [x29, #-8]
   4aca0:	mov	x12, x28
   4aca4:	sub	x25, x25, #0x1
   4aca8:	add	x8, x21, x16
   4acac:	add	x21, x8, x0
   4acb0:	b	4ac44 <__gmpn_sbpi1_divappr_q@@Base+0x424>
   4acb4:	ldur	x8, [x29, #-16]
   4acb8:	cmp	x19, x8
   4acbc:	b.cs	4acc8 <__gmpn_sbpi1_divappr_q@@Base+0x4a8>  // b.hs, b.nlast
   4acc0:	cmp	x21, x16
   4acc4:	b.ls	4ac10 <__gmpn_sbpi1_divappr_q@@Base+0x3f0>  // b.plast
   4acc8:	ldur	x9, [x29, #-16]
   4accc:	subs	x8, x19, x9
   4acd0:	sbc	x21, x21, x16
   4acd4:	add	x25, x25, #0x1
   4acd8:	mov	x19, x8
   4acdc:	b	4ac10 <__gmpn_sbpi1_divappr_q@@Base+0x3f0>
   4ace0:	mov	x23, xzr
   4ace4:	mov	x25, #0xffffffffffffffff    	// #-1
   4ace8:	b	4ab5c <__gmpn_sbpi1_divappr_q@@Base+0x33c>
   4acec:	ldr	x19, [sp, #16]
   4acf0:	add	x26, x26, x27
   4acf4:	sub	x22, x26, #0x8
   4acf8:	add	x28, x28, x27
   4acfc:	cbz	w8, 4aa8c <__gmpn_sbpi1_divappr_q@@Base+0x26c>
   4ad00:	mov	w2, #0x2                   	// #2
   4ad04:	mov	x3, #0xffffffffffffffff    	// #-1
   4ad08:	mov	x0, x22
   4ad0c:	mov	x1, x15
   4ad10:	mov	x24, x23
   4ad14:	mov	x20, #0xffffffffffffffff    	// #-1
   4ad18:	mov	x23, x15
   4ad1c:	bl	c9e0 <__gmpn_submul_1@plt>
   4ad20:	cmp	x21, x0
   4ad24:	b.eq	4ad34 <__gmpn_sbpi1_divappr_q@@Base+0x514>  // b.none
   4ad28:	and	x8, x0, x24
   4ad2c:	cmp	x21, x8
   4ad30:	b.cc	4ad90 <__gmpn_sbpi1_divappr_q@@Base+0x570>  // b.lo, b.ul, b.last
   4ad34:	ldr	x21, [x26]
   4ad38:	stur	x20, [x28, #-8]
   4ad3c:	ldr	x8, [x22, #8]
   4ad40:	cmp	x8, x21
   4ad44:	b.ne	4adb4 <__gmpn_sbpi1_divappr_q@@Base+0x594>  // b.any
   4ad48:	mov	x0, x19
   4ad4c:	ldp	x20, x19, [sp, #144]
   4ad50:	ldp	x22, x21, [sp, #128]
   4ad54:	ldp	x24, x23, [sp, #112]
   4ad58:	ldp	x26, x25, [sp, #96]
   4ad5c:	ldp	x28, x27, [sp, #80]
   4ad60:	ldp	x29, x30, [sp, #64]
   4ad64:	add	sp, sp, #0xa0
   4ad68:	ret
   4ad6c:	cmp	x8, x15
   4ad70:	b.cs	4ad7c <__gmpn_sbpi1_divappr_q@@Base+0x55c>  // b.hs, b.nlast
   4ad74:	cmp	x21, x16
   4ad78:	b.ls	4aaf0 <__gmpn_sbpi1_divappr_q@@Base+0x2d0>  // b.plast
   4ad7c:	subs	x9, x8, x15
   4ad80:	sbc	x21, x21, x16
   4ad84:	add	x20, x20, #0x1
   4ad88:	mov	x8, x9
   4ad8c:	b	4aaf0 <__gmpn_sbpi1_divappr_q@@Base+0x2d0>
   4ad90:	ldr	x8, [x26]
   4ad94:	mov	x20, #0xfffffffffffffffe    	// #-2
   4ad98:	ldp	x9, x10, [x23]
   4ad9c:	ldr	x11, [x22]
   4ada0:	adds	x12, x11, x9
   4ada4:	adc	x8, x8, x10
   4ada8:	str	x8, [x26]
   4adac:	str	x12, [x22]
   4adb0:	b	4ad34 <__gmpn_sbpi1_divappr_q@@Base+0x514>
   4adb4:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4adb8:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4adbc:	add	x0, x0, #0xb59
   4adc0:	add	x2, x2, #0xb45
   4adc4:	mov	w1, #0xc3                  	// #195
   4adc8:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004adcc <__gmpn_dcpi1_div_q@@Base>:
   4adcc:	stp	x29, x30, [sp, #-96]!
   4add0:	stp	x28, x27, [sp, #16]
   4add4:	stp	x26, x25, [sp, #32]
   4add8:	stp	x24, x23, [sp, #48]
   4addc:	stp	x22, x21, [sp, #64]
   4ade0:	stp	x20, x19, [sp, #80]
   4ade4:	mov	x29, sp
   4ade8:	sub	sp, sp, #0x10
   4adec:	add	x28, x2, #0x1
   4adf0:	mov	x25, x1
   4adf4:	lsl	x1, x28, #3
   4adf8:	mov	w8, #0x7f00                	// #32512
   4adfc:	mov	x20, x5
   4ae00:	mov	x21, x4
   4ae04:	mov	x26, x3
   4ae08:	mov	x22, x2
   4ae0c:	mov	x19, x0
   4ae10:	cmp	x1, x8
   4ae14:	stur	xzr, [x29, #-8]
   4ae18:	b.hi	4b024 <__gmpn_dcpi1_div_q@@Base+0x258>  // b.pmore
   4ae1c:	add	x9, x1, #0xf
   4ae20:	mov	x8, sp
   4ae24:	and	x9, x9, #0xfffffffffffffff0
   4ae28:	sub	x27, x8, x9
   4ae2c:	mov	sp, x27
   4ae30:	add	x0, x27, #0x8
   4ae34:	mov	x1, x25
   4ae38:	mov	x2, x22
   4ae3c:	bl	ca50 <__gmpn_copyi@plt>
   4ae40:	sub	x23, x22, x21
   4ae44:	lsl	x8, x23, #3
   4ae48:	add	x1, x8, #0x8
   4ae4c:	mov	w8, #0x7f00                	// #32512
   4ae50:	cmp	x1, x8
   4ae54:	str	xzr, [x27]
   4ae58:	b.hi	4b034 <__gmpn_dcpi1_div_q@@Base+0x268>  // b.pmore
   4ae5c:	add	x9, x1, #0xf
   4ae60:	mov	x8, sp
   4ae64:	and	x9, x9, #0xfffffffffffffff0
   4ae68:	sub	x24, x8, x9
   4ae6c:	mov	sp, x24
   4ae70:	mov	x0, x24
   4ae74:	mov	x1, x27
   4ae78:	mov	x2, x28
   4ae7c:	mov	x3, x26
   4ae80:	mov	x4, x21
   4ae84:	mov	x5, x20
   4ae88:	bl	c4d0 <__gmpn_dcpi1_divappr_q@plt>
   4ae8c:	ldr	x8, [x24]
   4ae90:	mov	x20, x0
   4ae94:	cbz	x8, 4aea4 <__gmpn_dcpi1_div_q@@Base+0xd8>
   4ae98:	add	x1, x24, #0x8
   4ae9c:	mov	x0, x19
   4aea0:	b	4af74 <__gmpn_dcpi1_div_q@@Base+0x1a8>
   4aea4:	cmp	x23, x21
   4aea8:	add	x28, x24, #0x8
   4aeac:	mov	x0, x27
   4aeb0:	b.le	4aed0 <__gmpn_dcpi1_div_q@@Base+0x104>
   4aeb4:	mov	x1, x28
   4aeb8:	mov	x2, x23
   4aebc:	mov	x3, x26
   4aec0:	mov	x4, x21
   4aec4:	bl	ccd0 <__gmpn_mul@plt>
   4aec8:	cbnz	x20, 4aee8 <__gmpn_dcpi1_div_q@@Base+0x11c>
   4aecc:	b	4af00 <__gmpn_dcpi1_div_q@@Base+0x134>
   4aed0:	mov	x1, x26
   4aed4:	mov	x2, x21
   4aed8:	mov	x3, x28
   4aedc:	mov	x4, x23
   4aee0:	bl	ccd0 <__gmpn_mul@plt>
   4aee4:	cbz	x20, 4af00 <__gmpn_dcpi1_div_q@@Base+0x134>
   4aee8:	add	x0, x27, x23, lsl #3
   4aeec:	mov	x1, x0
   4aef0:	mov	x2, x26
   4aef4:	mov	x3, x21
   4aef8:	bl	ca70 <__gmpn_add_n@plt>
   4aefc:	cbnz	x0, 4af30 <__gmpn_dcpi1_div_q@@Base+0x164>
   4af00:	sub	x8, x25, #0x8
   4af04:	sub	x9, x27, #0x8
   4af08:	mov	x10, x22
   4af0c:	subs	x11, x10, #0x1
   4af10:	b.lt	4af6c <__gmpn_dcpi1_div_q@@Base+0x1a0>  // b.tstop
   4af14:	lsl	x10, x10, #3
   4af18:	ldr	x12, [x9, x10]
   4af1c:	ldr	x10, [x8, x10]
   4af20:	cmp	x12, x10
   4af24:	mov	x10, x11
   4af28:	b.eq	4af0c <__gmpn_dcpi1_div_q@@Base+0x140>  // b.none
   4af2c:	b.ls	4af6c <__gmpn_dcpi1_div_q@@Base+0x1a0>  // b.plast
   4af30:	ldr	x8, [x28]
   4af34:	sub	x9, x8, #0x1
   4af38:	str	x9, [x19]
   4af3c:	cbz	x8, 4afa8 <__gmpn_dcpi1_div_q@@Base+0x1dc>
   4af40:	cmp	x23, #0x2
   4af44:	mov	x8, xzr
   4af48:	b.lt	4b010 <__gmpn_dcpi1_div_q@@Base+0x244>  // b.tstop
   4af4c:	cmp	x28, x19
   4af50:	b.eq	4b010 <__gmpn_dcpi1_div_q@@Base+0x244>  // b.none
   4af54:	mvn	x8, x21
   4af58:	add	x8, x8, x22
   4af5c:	add	x0, x19, #0x8
   4af60:	add	x1, x24, #0x10
   4af64:	lsl	x2, x8, #3
   4af68:	b	4b008 <__gmpn_dcpi1_div_q@@Base+0x23c>
   4af6c:	mov	x0, x19
   4af70:	mov	x1, x28
   4af74:	mov	x2, x23
   4af78:	bl	ca50 <__gmpn_copyi@plt>
   4af7c:	ldur	x0, [x29, #-8]
   4af80:	cbnz	x0, 4b01c <__gmpn_dcpi1_div_q@@Base+0x250>
   4af84:	mov	x0, x20
   4af88:	mov	sp, x29
   4af8c:	ldp	x20, x19, [sp, #80]
   4af90:	ldp	x22, x21, [sp, #64]
   4af94:	ldp	x24, x23, [sp, #48]
   4af98:	ldp	x26, x25, [sp, #32]
   4af9c:	ldp	x28, x27, [sp, #16]
   4afa0:	ldp	x29, x30, [sp], #96
   4afa4:	ret
   4afa8:	mov	x9, xzr
   4afac:	add	x11, x24, #0x10
   4afb0:	mov	w8, #0x1                   	// #1
   4afb4:	mov	w10, #0x1                   	// #1
   4afb8:	cmp	x10, x23
   4afbc:	b.ge	4b010 <__gmpn_dcpi1_div_q@@Base+0x244>  // b.tcont
   4afc0:	ldr	x12, [x11, x9]
   4afc4:	add	x13, x19, x9
   4afc8:	add	x10, x10, #0x1
   4afcc:	add	x9, x9, #0x8
   4afd0:	sub	x14, x12, #0x1
   4afd4:	str	x14, [x13, #8]
   4afd8:	cbz	x12, 4afb8 <__gmpn_dcpi1_div_q@@Base+0x1ec>
   4afdc:	cmp	x28, x19
   4afe0:	mov	x8, xzr
   4afe4:	b.eq	4b010 <__gmpn_dcpi1_div_q@@Base+0x244>  // b.none
   4afe8:	cmp	x10, x23
   4afec:	b.ge	4b010 <__gmpn_dcpi1_div_q@@Base+0x244>  // b.tcont
   4aff0:	add	x8, x19, x9
   4aff4:	add	x9, x24, x9
   4aff8:	sub	x10, x23, x10
   4affc:	add	x0, x8, #0x8
   4b000:	add	x1, x9, #0x10
   4b004:	lsl	x2, x10, #3
   4b008:	bl	bed0 <memcpy@plt>
   4b00c:	mov	x8, xzr
   4b010:	sub	x20, x20, x8
   4b014:	ldur	x0, [x29, #-8]
   4b018:	cbz	x0, 4af84 <__gmpn_dcpi1_div_q@@Base+0x1b8>
   4b01c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4b020:	b	4af84 <__gmpn_dcpi1_div_q@@Base+0x1b8>
   4b024:	sub	x0, x29, #0x8
   4b028:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4b02c:	mov	x27, x0
   4b030:	b	4ae30 <__gmpn_dcpi1_div_q@@Base+0x64>
   4b034:	sub	x0, x29, #0x8
   4b038:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4b03c:	mov	x24, x0
   4b040:	b	4ae70 <__gmpn_dcpi1_div_q@@Base+0xa4>

000000000004b044 <__gmpn_dcpi1_div_qr_n@@Base>:
   4b044:	sub	sp, sp, #0x80
   4b048:	stp	x24, x23, [sp, #80]
   4b04c:	asr	x23, x3, #1
   4b050:	stp	x26, x25, [sp, #64]
   4b054:	sub	x25, x3, x23
   4b058:	and	x8, x3, #0xfffffffffffffffe
   4b05c:	stp	x28, x27, [sp, #48]
   4b060:	stp	x22, x21, [sp, #96]
   4b064:	stp	x20, x19, [sp, #112]
   4b068:	mov	x26, x5
   4b06c:	mov	x19, x3
   4b070:	mov	x20, x2
   4b074:	mov	x21, x1
   4b078:	mov	x22, x0
   4b07c:	add	x27, x0, x23, lsl #3
   4b080:	cmp	x25, #0x29
   4b084:	add	x1, x1, x8, lsl #3
   4b088:	stp	x29, x30, [sp, #32]
   4b08c:	add	x29, sp, #0x20
   4b090:	stp	x8, x4, [sp, #8]
   4b094:	b.le	4b0b0 <__gmpn_dcpi1_div_qr_n@@Base+0x6c>
   4b098:	add	x2, x20, x23, lsl #3
   4b09c:	mov	x0, x27
   4b0a0:	mov	x3, x25
   4b0a4:	mov	x5, x26
   4b0a8:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4b0ac:	b	4b0c8 <__gmpn_dcpi1_div_qr_n@@Base+0x84>
   4b0b0:	ldr	x5, [x4]
   4b0b4:	lsl	x2, x25, #1
   4b0b8:	add	x3, x20, x23, lsl #3
   4b0bc:	mov	x0, x27
   4b0c0:	mov	x4, x25
   4b0c4:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4b0c8:	mov	x24, x0
   4b0cc:	mov	x0, x26
   4b0d0:	mov	x1, x27
   4b0d4:	mov	x2, x25
   4b0d8:	mov	x3, x20
   4b0dc:	mov	x4, x23
   4b0e0:	bl	ccd0 <__gmpn_mul@plt>
   4b0e4:	add	x28, x21, x23, lsl #3
   4b0e8:	mov	x0, x28
   4b0ec:	mov	x1, x28
   4b0f0:	mov	x2, x26
   4b0f4:	mov	x3, x19
   4b0f8:	stur	x26, [x29, #-8]
   4b0fc:	bl	c2d0 <__gmpn_sub_n@plt>
   4b100:	mov	x26, x0
   4b104:	cbz	x24, 4b120 <__gmpn_dcpi1_div_qr_n@@Base+0xdc>
   4b108:	add	x0, x21, x19, lsl #3
   4b10c:	mov	x1, x0
   4b110:	mov	x2, x20
   4b114:	mov	x3, x23
   4b118:	bl	c2d0 <__gmpn_sub_n@plt>
   4b11c:	add	x26, x0, x26
   4b120:	cbnz	x26, 4b17c <__gmpn_dcpi1_div_qr_n@@Base+0x138>
   4b124:	lsl	x8, x25, #3
   4b128:	cmp	x19, #0x53
   4b12c:	add	x1, x21, x8
   4b130:	add	x3, x20, x8
   4b134:	b.le	4b1bc <__gmpn_dcpi1_div_qr_n@@Base+0x178>
   4b138:	ldur	x26, [x29, #-8]
   4b13c:	ldr	x4, [sp, #16]
   4b140:	mov	x0, x22
   4b144:	mov	x2, x3
   4b148:	mov	x3, x23
   4b14c:	mov	x5, x26
   4b150:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4b154:	b	4b1d4 <__gmpn_dcpi1_div_qr_n@@Base+0x190>
   4b158:	mov	x8, xzr
   4b15c:	mov	x0, x28
   4b160:	mov	x1, x28
   4b164:	mov	x2, x20
   4b168:	mov	x3, x19
   4b16c:	sub	x24, x24, x8
   4b170:	bl	ca70 <__gmpn_add_n@plt>
   4b174:	subs	x26, x26, x0
   4b178:	b.eq	4b124 <__gmpn_dcpi1_div_qr_n@@Base+0xe0>  // b.none
   4b17c:	ldr	x8, [x27]
   4b180:	sub	x9, x8, #0x1
   4b184:	str	x9, [x27]
   4b188:	cbnz	x8, 4b158 <__gmpn_dcpi1_div_qr_n@@Base+0x114>
   4b18c:	mov	w8, #0x1                   	// #1
   4b190:	cmp	x8, x25
   4b194:	b.ge	4b1b4 <__gmpn_dcpi1_div_qr_n@@Base+0x170>  // b.tcont
   4b198:	lsl	x9, x8, #3
   4b19c:	ldr	x10, [x27, x9]
   4b1a0:	add	x8, x8, #0x1
   4b1a4:	sub	x11, x10, #0x1
   4b1a8:	str	x11, [x27, x9]
   4b1ac:	cbz	x10, 4b190 <__gmpn_dcpi1_div_qr_n@@Base+0x14c>
   4b1b0:	b	4b158 <__gmpn_dcpi1_div_qr_n@@Base+0x114>
   4b1b4:	mov	w8, #0x1                   	// #1
   4b1b8:	b	4b15c <__gmpn_dcpi1_div_qr_n@@Base+0x118>
   4b1bc:	ldp	x2, x8, [sp, #8]
   4b1c0:	mov	x0, x22
   4b1c4:	mov	x4, x23
   4b1c8:	ldr	x5, [x8]
   4b1cc:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4b1d0:	ldur	x26, [x29, #-8]
   4b1d4:	mov	x27, x0
   4b1d8:	mov	x0, x26
   4b1dc:	mov	x1, x20
   4b1e0:	mov	x2, x25
   4b1e4:	mov	x3, x22
   4b1e8:	mov	x4, x23
   4b1ec:	bl	ccd0 <__gmpn_mul@plt>
   4b1f0:	mov	x0, x21
   4b1f4:	mov	x1, x21
   4b1f8:	mov	x2, x26
   4b1fc:	mov	x3, x19
   4b200:	bl	c2d0 <__gmpn_sub_n@plt>
   4b204:	mov	x26, x0
   4b208:	cbz	x27, 4b224 <__gmpn_dcpi1_div_qr_n@@Base+0x1e0>
   4b20c:	mov	x0, x28
   4b210:	mov	x1, x28
   4b214:	mov	x2, x20
   4b218:	mov	x3, x25
   4b21c:	bl	c2d0 <__gmpn_sub_n@plt>
   4b220:	add	x26, x0, x26
   4b224:	cbnz	x26, 4b268 <__gmpn_dcpi1_div_qr_n@@Base+0x224>
   4b228:	mov	x0, x24
   4b22c:	ldp	x20, x19, [sp, #112]
   4b230:	ldp	x22, x21, [sp, #96]
   4b234:	ldp	x24, x23, [sp, #80]
   4b238:	ldp	x26, x25, [sp, #64]
   4b23c:	ldp	x28, x27, [sp, #48]
   4b240:	ldp	x29, x30, [sp, #32]
   4b244:	add	sp, sp, #0x80
   4b248:	ret
   4b24c:	mov	x0, x21
   4b250:	mov	x1, x21
   4b254:	mov	x2, x20
   4b258:	mov	x3, x19
   4b25c:	bl	ca70 <__gmpn_add_n@plt>
   4b260:	subs	x26, x26, x0
   4b264:	b.eq	4b228 <__gmpn_dcpi1_div_qr_n@@Base+0x1e4>  // b.none
   4b268:	ldr	x8, [x22]
   4b26c:	sub	x9, x8, #0x1
   4b270:	str	x9, [x22]
   4b274:	cbnz	x8, 4b24c <__gmpn_dcpi1_div_qr_n@@Base+0x208>
   4b278:	mov	w8, #0x1                   	// #1
   4b27c:	cmp	x8, x23
   4b280:	b.ge	4b24c <__gmpn_dcpi1_div_qr_n@@Base+0x208>  // b.tcont
   4b284:	lsl	x9, x8, #3
   4b288:	ldr	x10, [x22, x9]
   4b28c:	add	x8, x8, #0x1
   4b290:	sub	x11, x10, #0x1
   4b294:	str	x11, [x22, x9]
   4b298:	cbz	x10, 4b27c <__gmpn_dcpi1_div_qr_n@@Base+0x238>
   4b29c:	b	4b24c <__gmpn_dcpi1_div_qr_n@@Base+0x208>

000000000004b2a0 <__gmpn_dcpi1_div_qr@@Base>:
   4b2a0:	stp	x29, x30, [sp, #-96]!
   4b2a4:	stp	x28, x27, [sp, #16]
   4b2a8:	stp	x26, x25, [sp, #32]
   4b2ac:	stp	x24, x23, [sp, #48]
   4b2b0:	stp	x22, x21, [sp, #64]
   4b2b4:	stp	x20, x19, [sp, #80]
   4b2b8:	mov	x29, sp
   4b2bc:	sub	sp, sp, #0x50
   4b2c0:	lsl	x25, x4, #3
   4b2c4:	mov	w8, #0x7f00                	// #32512
   4b2c8:	mov	x19, x4
   4b2cc:	mov	x20, x3
   4b2d0:	mov	x26, x2
   4b2d4:	mov	x23, x1
   4b2d8:	mov	x21, x0
   4b2dc:	cmp	x25, x8
   4b2e0:	stur	xzr, [x29, #-8]
   4b2e4:	stur	x5, [x29, #-24]
   4b2e8:	b.hi	4b7b4 <__gmpn_dcpi1_div_qr@@Base+0x514>  // b.pmore
   4b2ec:	add	x9, x25, #0xf
   4b2f0:	mov	x8, sp
   4b2f4:	and	x9, x9, #0xfffffffffffffff0
   4b2f8:	sub	x27, x8, x9
   4b2fc:	mov	sp, x27
   4b300:	sub	x22, x26, x19
   4b304:	add	x9, x23, x26, lsl #3
   4b308:	cmp	x22, x19
   4b30c:	add	x28, x20, x19, lsl #3
   4b310:	b.le	4b3c0 <__gmpn_dcpi1_div_qr@@Base+0x120>
   4b314:	add	x8, x25, x21
   4b318:	stp	x23, x27, [x29, #-40]
   4b31c:	add	x10, x21, x22, lsl #3
   4b320:	add	x11, x8, #0x8
   4b324:	mov	x8, x25
   4b328:	mov	x27, x22
   4b32c:	sub	x27, x27, x19
   4b330:	mov	x24, x8
   4b334:	mov	x23, x11
   4b338:	add	x8, x8, x25
   4b33c:	cmp	x27, x19
   4b340:	add	x11, x11, x25
   4b344:	b.gt	4b32c <__gmpn_dcpi1_div_qr@@Base+0x8c>
   4b348:	lsl	x11, x27, #3
   4b34c:	cmp	x27, #0x2
   4b350:	sub	x10, x10, x11
   4b354:	sub	x26, x9, x11
   4b358:	stur	x10, [x29, #-16]
   4b35c:	b.eq	4b440 <__gmpn_dcpi1_div_qr@@Base+0x1a0>  // b.none
   4b360:	cmp	x27, #0x1
   4b364:	b.ne	4b460 <__gmpn_dcpi1_div_qr@@Base+0x1c0>  // b.any
   4b368:	ldur	x9, [x29, #-40]
   4b36c:	sub	x14, x26, x19, lsl #3
   4b370:	neg	x13, x19
   4b374:	add	x0, x14, #0x8
   4b378:	mov	x10, x19
   4b37c:	stp	x14, x13, [x29, #-56]
   4b380:	subs	x11, x10, #0x1
   4b384:	b.lt	4b3a8 <__gmpn_dcpi1_div_qr@@Base+0x108>  // b.tstop
   4b388:	add	x10, x20, x10, lsl #3
   4b38c:	ldr	x12, [x9, x8]
   4b390:	ldur	x10, [x10, #-8]
   4b394:	sub	x9, x9, #0x8
   4b398:	cmp	x12, x10
   4b39c:	mov	x10, x11
   4b3a0:	b.eq	4b380 <__gmpn_dcpi1_div_qr@@Base+0xe0>  // b.none
   4b3a4:	b.ls	4b66c <__gmpn_dcpi1_div_qr@@Base+0x3cc>  // b.plast
   4b3a8:	mov	x1, x0
   4b3ac:	mov	x2, x20
   4b3b0:	mov	x3, x19
   4b3b4:	bl	c2d0 <__gmpn_sub_n@plt>
   4b3b8:	mov	w25, #0x1                   	// #1
   4b3bc:	b	4b670 <__gmpn_dcpi1_div_qr@@Base+0x3d0>
   4b3c0:	lsl	x10, x22, #3
   4b3c4:	sub	x23, x9, x10
   4b3c8:	neg	x8, x22
   4b3cc:	cmp	x22, #0x29
   4b3d0:	sub	x1, x23, x10
   4b3d4:	b.le	4b3f4 <__gmpn_dcpi1_div_qr@@Base+0x154>
   4b3d8:	ldur	x4, [x29, #-24]
   4b3dc:	add	x2, x28, x8, lsl #3
   4b3e0:	mov	x0, x21
   4b3e4:	mov	x3, x22
   4b3e8:	mov	x5, x27
   4b3ec:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4b3f0:	b	4b410 <__gmpn_dcpi1_div_qr@@Base+0x170>
   4b3f4:	ldur	x9, [x29, #-24]
   4b3f8:	lsl	x2, x22, #1
   4b3fc:	add	x3, x28, x8, lsl #3
   4b400:	mov	x0, x21
   4b404:	ldr	x5, [x9]
   4b408:	mov	x4, x22
   4b40c:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4b410:	mov	x25, x0
   4b414:	cmp	x22, x19
   4b418:	b.eq	4b788 <__gmpn_dcpi1_div_qr@@Base+0x4e8>  // b.none
   4b41c:	sub	x26, x19, x22
   4b420:	mov	x0, x27
   4b424:	cmp	x22, x26
   4b428:	b.le	4b48c <__gmpn_dcpi1_div_qr@@Base+0x1ec>
   4b42c:	mov	x1, x21
   4b430:	mov	x2, x22
   4b434:	mov	x3, x20
   4b438:	mov	x4, x26
   4b43c:	b	4b49c <__gmpn_dcpi1_div_qr@@Base+0x1fc>
   4b440:	ldur	x0, [x29, #-16]
   4b444:	sub	x2, x26, #0x10
   4b448:	sub	x4, x28, #0x10
   4b44c:	mov	w3, #0x4                   	// #4
   4b450:	mov	x1, xzr
   4b454:	bl	c200 <__gmpn_divrem_2@plt>
   4b458:	ldur	x28, [x29, #-24]
   4b45c:	b	4b560 <__gmpn_dcpi1_div_qr@@Base+0x2c0>
   4b460:	neg	x8, x27
   4b464:	cmp	x27, #0x29
   4b468:	add	x1, x26, x8, lsl #3
   4b46c:	b.le	4b544 <__gmpn_dcpi1_div_qr@@Base+0x2a4>
   4b470:	add	x2, x28, x8, lsl #3
   4b474:	ldp	x28, x0, [x29, #-24]
   4b478:	ldur	x5, [x29, #-32]
   4b47c:	mov	x3, x27
   4b480:	mov	x4, x28
   4b484:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4b488:	b	4b560 <__gmpn_dcpi1_div_qr@@Base+0x2c0>
   4b48c:	mov	x1, x20
   4b490:	mov	x2, x26
   4b494:	mov	x3, x21
   4b498:	mov	x4, x22
   4b49c:	bl	ccd0 <__gmpn_mul@plt>
   4b4a0:	sub	x24, x23, x19, lsl #3
   4b4a4:	mov	x0, x24
   4b4a8:	mov	x1, x24
   4b4ac:	mov	x2, x27
   4b4b0:	mov	x3, x19
   4b4b4:	bl	c2d0 <__gmpn_sub_n@plt>
   4b4b8:	mov	x23, x0
   4b4bc:	cbz	x25, 4b4d8 <__gmpn_dcpi1_div_qr@@Base+0x238>
   4b4c0:	add	x0, x24, x22, lsl #3
   4b4c4:	mov	x1, x0
   4b4c8:	mov	x2, x20
   4b4cc:	mov	x3, x26
   4b4d0:	bl	c2d0 <__gmpn_sub_n@plt>
   4b4d4:	add	x23, x0, x23
   4b4d8:	cbnz	x23, 4b504 <__gmpn_dcpi1_div_qr@@Base+0x264>
   4b4dc:	b	4b788 <__gmpn_dcpi1_div_qr@@Base+0x4e8>
   4b4e0:	mov	x8, xzr
   4b4e4:	mov	x0, x24
   4b4e8:	mov	x1, x24
   4b4ec:	mov	x2, x20
   4b4f0:	mov	x3, x19
   4b4f4:	sub	x25, x25, x8
   4b4f8:	bl	ca70 <__gmpn_add_n@plt>
   4b4fc:	subs	x23, x23, x0
   4b500:	b.eq	4b788 <__gmpn_dcpi1_div_qr@@Base+0x4e8>  // b.none
   4b504:	ldr	x8, [x21]
   4b508:	sub	x9, x8, #0x1
   4b50c:	str	x9, [x21]
   4b510:	cbnz	x8, 4b4e0 <__gmpn_dcpi1_div_qr@@Base+0x240>
   4b514:	mov	w8, #0x1                   	// #1
   4b518:	cmp	x8, x22
   4b51c:	b.ge	4b53c <__gmpn_dcpi1_div_qr@@Base+0x29c>  // b.tcont
   4b520:	lsl	x9, x8, #3
   4b524:	ldr	x10, [x21, x9]
   4b528:	add	x8, x8, #0x1
   4b52c:	sub	x11, x10, #0x1
   4b530:	str	x11, [x21, x9]
   4b534:	cbz	x10, 4b518 <__gmpn_dcpi1_div_qr@@Base+0x278>
   4b538:	b	4b4e0 <__gmpn_dcpi1_div_qr@@Base+0x240>
   4b53c:	mov	w8, #0x1                   	// #1
   4b540:	b	4b4e4 <__gmpn_dcpi1_div_qr@@Base+0x244>
   4b544:	ldp	x9, x0, [x29, #-24]
   4b548:	lsl	x2, x27, #1
   4b54c:	add	x3, x28, x8, lsl #3
   4b550:	mov	x4, x27
   4b554:	ldr	x5, [x9]
   4b558:	mov	x28, x9
   4b55c:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4b560:	mov	x25, x0
   4b564:	cmp	x27, x19
   4b568:	b.ne	4b574 <__gmpn_dcpi1_div_qr@@Base+0x2d4>  // b.any
   4b56c:	neg	x9, x19
   4b570:	b	4b744 <__gmpn_dcpi1_div_qr@@Base+0x4a4>
   4b574:	sub	x8, x19, x27
   4b578:	cmp	x27, x8
   4b57c:	stur	x8, [x29, #-56]
   4b580:	b.le	4b5a0 <__gmpn_dcpi1_div_qr@@Base+0x300>
   4b584:	ldur	x28, [x29, #-32]
   4b588:	ldur	x1, [x29, #-16]
   4b58c:	mov	x2, x27
   4b590:	mov	x3, x20
   4b594:	mov	x0, x28
   4b598:	mov	x4, x8
   4b59c:	b	4b5b8 <__gmpn_dcpi1_div_qr@@Base+0x318>
   4b5a0:	ldur	x28, [x29, #-32]
   4b5a4:	ldur	x3, [x29, #-16]
   4b5a8:	mov	x1, x20
   4b5ac:	mov	x2, x8
   4b5b0:	mov	x0, x28
   4b5b4:	mov	x4, x27
   4b5b8:	bl	ccd0 <__gmpn_mul@plt>
   4b5bc:	sub	x26, x26, x19, lsl #3
   4b5c0:	neg	x8, x19
   4b5c4:	mov	x0, x26
   4b5c8:	mov	x1, x26
   4b5cc:	mov	x2, x28
   4b5d0:	mov	x3, x19
   4b5d4:	stur	x8, [x29, #-48]
   4b5d8:	bl	c2d0 <__gmpn_sub_n@plt>
   4b5dc:	mov	x28, x0
   4b5e0:	cbz	x25, 4b5fc <__gmpn_dcpi1_div_qr@@Base+0x35c>
   4b5e4:	ldur	x3, [x29, #-56]
   4b5e8:	add	x0, x26, x27, lsl #3
   4b5ec:	mov	x1, x0
   4b5f0:	mov	x2, x20
   4b5f4:	bl	c2d0 <__gmpn_sub_n@plt>
   4b5f8:	add	x28, x0, x28
   4b5fc:	cbnz	x28, 4b628 <__gmpn_dcpi1_div_qr@@Base+0x388>
   4b600:	b	4b73c <__gmpn_dcpi1_div_qr@@Base+0x49c>
   4b604:	mov	x8, xzr
   4b608:	mov	x0, x26
   4b60c:	mov	x1, x26
   4b610:	mov	x2, x20
   4b614:	mov	x3, x19
   4b618:	sub	x25, x25, x8
   4b61c:	bl	ca70 <__gmpn_add_n@plt>
   4b620:	subs	x28, x28, x0
   4b624:	b.eq	4b73c <__gmpn_dcpi1_div_qr@@Base+0x49c>  // b.none
   4b628:	ldur	x10, [x29, #-16]
   4b62c:	ldr	x8, [x10]
   4b630:	sub	x9, x8, #0x1
   4b634:	str	x9, [x10]
   4b638:	cbnz	x8, 4b604 <__gmpn_dcpi1_div_qr@@Base+0x364>
   4b63c:	mov	x8, x23
   4b640:	mov	w9, #0x1                   	// #1
   4b644:	cmp	x9, x27
   4b648:	b.ge	4b664 <__gmpn_dcpi1_div_qr@@Base+0x3c4>  // b.tcont
   4b64c:	ldr	x10, [x8]
   4b650:	add	x9, x9, #0x1
   4b654:	sub	x11, x10, #0x1
   4b658:	str	x11, [x8], #8
   4b65c:	cbz	x10, 4b644 <__gmpn_dcpi1_div_qr@@Base+0x3a4>
   4b660:	b	4b604 <__gmpn_dcpi1_div_qr@@Base+0x364>
   4b664:	mov	w8, #0x1                   	// #1
   4b668:	b	4b608 <__gmpn_dcpi1_div_qr@@Base+0x368>
   4b66c:	mov	x25, xzr
   4b670:	ldp	x10, x9, [x26, #-8]
   4b674:	ldp	x8, x23, [x28, #-16]
   4b678:	cmp	x9, x23
   4b67c:	b.ne	4b688 <__gmpn_dcpi1_div_qr@@Base+0x3e8>  // b.any
   4b680:	cmp	x10, x8
   4b684:	b.eq	4b7f4 <__gmpn_dcpi1_div_qr@@Base+0x554>  // b.none
   4b688:	ldur	x11, [x29, #-24]
   4b68c:	ldur	x12, [x26, #-16]
   4b690:	ldr	x11, [x11]
   4b694:	mul	x13, x11, x9
   4b698:	umulh	x11, x9, x11
   4b69c:	adds	x14, x13, x10
   4b6a0:	adc	x9, x11, x9
   4b6a4:	msub	x10, x9, x23, x10
   4b6a8:	subs	x15, x12, x8
   4b6ac:	sbc	x10, x10, x23
   4b6b0:	mul	x11, x9, x8
   4b6b4:	umulh	x13, x8, x9
   4b6b8:	subs	x12, x15, x11
   4b6bc:	sbc	x10, x10, x13
   4b6c0:	cmp	x10, x14
   4b6c4:	cset	w11, cs  // cs = hs, nlast
   4b6c8:	csetm	x13, cs  // cs = hs, nlast
   4b6cc:	sub	x9, x9, x11
   4b6d0:	and	x11, x8, x13
   4b6d4:	and	x13, x23, x13
   4b6d8:	adds	x14, x12, x11
   4b6dc:	adc	x28, x10, x13
   4b6e0:	cmp	x28, x23
   4b6e4:	add	x3, x9, #0x1
   4b6e8:	b.cs	4b7d0 <__gmpn_dcpi1_div_qr@@Base+0x530>  // b.hs, b.nlast
   4b6ec:	cmp	x19, #0x3
   4b6f0:	b.lt	4b72c <__gmpn_dcpi1_div_qr@@Base+0x48c>  // b.tstop
   4b6f4:	ldur	x0, [x29, #-56]
   4b6f8:	sub	x2, x19, #0x2
   4b6fc:	mov	x1, x20
   4b700:	stp	x28, x3, [x29, #-72]
   4b704:	mov	x28, x14
   4b708:	bl	c9e0 <__gmpn_submul_1@plt>
   4b70c:	subs	x8, x28, x0
   4b710:	ldur	x28, [x29, #-72]
   4b714:	cset	w9, cc  // cc = lo, ul, last
   4b718:	stur	x8, [x26, #-16]
   4b71c:	subs	x28, x28, x9
   4b720:	b.cc	4b818 <__gmpn_dcpi1_div_qr@@Base+0x578>  // b.lo, b.ul, b.last
   4b724:	ldur	x3, [x29, #-64]
   4b728:	b	4b730 <__gmpn_dcpi1_div_qr@@Base+0x490>
   4b72c:	stur	x14, [x26, #-16]
   4b730:	ldur	x8, [x29, #-16]
   4b734:	stur	x28, [x26, #-8]
   4b738:	str	x3, [x8]
   4b73c:	ldur	x28, [x29, #-24]
   4b740:	ldur	x9, [x29, #-48]
   4b744:	sub	x22, x22, x27
   4b748:	add	x8, x19, x9, lsl #1
   4b74c:	lsl	x23, x9, #3
   4b750:	ldp	x9, x27, [x29, #-40]
   4b754:	add	x21, x21, x23
   4b758:	add	x26, x9, x8, lsl #3
   4b75c:	add	x0, x21, x24
   4b760:	add	x1, x26, x24
   4b764:	mov	x2, x20
   4b768:	mov	x3, x19
   4b76c:	mov	x4, x28
   4b770:	mov	x5, x27
   4b774:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4b778:	sub	x22, x22, x19
   4b77c:	cmp	x22, #0x0
   4b780:	add	x24, x24, x23
   4b784:	b.gt	4b75c <__gmpn_dcpi1_div_qr@@Base+0x4bc>
   4b788:	ldur	x0, [x29, #-8]
   4b78c:	cbnz	x0, 4b7c8 <__gmpn_dcpi1_div_qr@@Base+0x528>
   4b790:	mov	x0, x25
   4b794:	mov	sp, x29
   4b798:	ldp	x20, x19, [sp, #80]
   4b79c:	ldp	x22, x21, [sp, #64]
   4b7a0:	ldp	x24, x23, [sp, #48]
   4b7a4:	ldp	x26, x25, [sp, #32]
   4b7a8:	ldp	x28, x27, [sp, #16]
   4b7ac:	ldp	x29, x30, [sp], #96
   4b7b0:	ret
   4b7b4:	sub	x0, x29, #0x8
   4b7b8:	mov	x1, x25
   4b7bc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4b7c0:	mov	x27, x0
   4b7c4:	b	4b300 <__gmpn_dcpi1_div_qr@@Base+0x60>
   4b7c8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4b7cc:	b	4b790 <__gmpn_dcpi1_div_qr@@Base+0x4f0>
   4b7d0:	cmp	x14, x8
   4b7d4:	b.cs	4b7e0 <__gmpn_dcpi1_div_qr@@Base+0x540>  // b.hs, b.nlast
   4b7d8:	cmp	x28, x23
   4b7dc:	b.ls	4b6ec <__gmpn_dcpi1_div_qr@@Base+0x44c>  // b.plast
   4b7e0:	subs	x9, x14, x8
   4b7e4:	sbc	x28, x28, x23
   4b7e8:	add	x3, x3, #0x1
   4b7ec:	mov	x14, x9
   4b7f0:	b	4b6ec <__gmpn_dcpi1_div_qr@@Base+0x44c>
   4b7f4:	ldur	x0, [x29, #-56]
   4b7f8:	mov	x3, #0xffffffffffffffff    	// #-1
   4b7fc:	mov	x1, x20
   4b800:	mov	x2, x19
   4b804:	mov	x23, #0xffffffffffffffff    	// #-1
   4b808:	bl	c9e0 <__gmpn_submul_1@plt>
   4b80c:	ldur	x8, [x29, #-16]
   4b810:	str	x23, [x8]
   4b814:	b	4b73c <__gmpn_dcpi1_div_qr@@Base+0x49c>
   4b818:	ldur	x0, [x29, #-56]
   4b81c:	sub	x3, x19, #0x1
   4b820:	mov	x2, x20
   4b824:	mov	x1, x0
   4b828:	bl	ca70 <__gmpn_add_n@plt>
   4b82c:	ldur	x3, [x29, #-64]
   4b830:	add	x8, x28, x23
   4b834:	add	x28, x8, x0
   4b838:	cmp	x3, #0x0
   4b83c:	cset	w8, eq  // eq = none
   4b840:	sub	x25, x25, x8
   4b844:	sub	x3, x3, #0x1
   4b848:	b	4b730 <__gmpn_dcpi1_div_qr@@Base+0x490>

000000000004b84c <__gmpn_dcpi1_divappr_q@@Base>:
   4b84c:	stp	x29, x30, [sp, #-96]!
   4b850:	stp	x28, x27, [sp, #16]
   4b854:	stp	x26, x25, [sp, #32]
   4b858:	stp	x24, x23, [sp, #48]
   4b85c:	stp	x22, x21, [sp, #64]
   4b860:	stp	x20, x19, [sp, #80]
   4b864:	mov	x29, sp
   4b868:	sub	sp, sp, #0x50
   4b86c:	sub	x23, x2, x4
   4b870:	mov	x12, x5
   4b874:	mov	x28, x2
   4b878:	mov	x22, x0
   4b87c:	cmp	x23, x4
   4b880:	add	x16, x3, x4, lsl #3
   4b884:	stur	x3, [x29, #-8]
   4b888:	b.ge	4b8ec <__gmpn_dcpi1_divappr_q@@Base+0xa0>  // b.tcont
   4b88c:	add	x4, x23, #0x1
   4b890:	lsl	x10, x4, #3
   4b894:	add	x10, x10, #0xf
   4b898:	add	x9, x1, x28, lsl #3
   4b89c:	mov	x11, sp
   4b8a0:	and	x10, x10, #0xfffffffffffffff0
   4b8a4:	neg	x8, x23
   4b8a8:	sub	x9, x9, x23, lsl #3
   4b8ac:	sub	x20, x11, x10
   4b8b0:	mov	sp, x20
   4b8b4:	cmp	x23, #0x97
   4b8b8:	b.le	4b9d4 <__gmpn_dcpi1_divappr_q@@Base+0x188>
   4b8bc:	mov	x11, sp
   4b8c0:	sub	x5, x11, x10
   4b8c4:	mov	sp, x5
   4b8c8:	add	x8, x9, x8, lsl #3
   4b8cc:	mvn	x9, x23
   4b8d0:	sub	x1, x8, #0x10
   4b8d4:	add	x2, x16, x9, lsl #3
   4b8d8:	mov	x0, x20
   4b8dc:	mov	x3, x4
   4b8e0:	mov	x4, x12
   4b8e4:	bl	4bdc4 <__gmpn_dcpi1_divappr_q@@Base+0x578>
   4b8e8:	b	4b9f4 <__gmpn_dcpi1_divappr_q@@Base+0x1a8>
   4b8ec:	lsl	x19, x4, #3
   4b8f0:	lsl	x8, x28, #3
   4b8f4:	lsl	x11, x4, #4
   4b8f8:	add	x27, x1, x19
   4b8fc:	sub	x8, x8, x11
   4b900:	mov	x20, x4
   4b904:	mov	x21, xzr
   4b908:	mov	x24, xzr
   4b90c:	add	x10, x23, #0x1
   4b910:	sub	x14, x4, #0x1
   4b914:	add	x9, x8, #0x10
   4b918:	mov	x8, x27
   4b91c:	stur	x12, [x29, #-24]
   4b920:	sub	x21, x21, x20
   4b924:	add	x26, x10, x21
   4b928:	add	x24, x24, x19
   4b92c:	add	x8, x8, x19
   4b930:	cmp	x26, x20
   4b934:	sub	x9, x9, x11
   4b938:	b.gt	4b920 <__gmpn_dcpi1_divappr_q@@Base+0xd4>
   4b93c:	add	x11, x19, #0xf
   4b940:	add	x23, x22, x24
   4b944:	mov	x10, sp
   4b948:	and	x11, x11, #0xfffffffffffffff0
   4b94c:	sub	x22, x23, #0x8
   4b950:	add	x13, x27, x24
   4b954:	sub	x10, x10, x11
   4b958:	stur	x10, [x29, #-16]
   4b95c:	mov	sp, x10
   4b960:	cmp	x26, #0x2
   4b964:	stp	x13, x1, [x29, #-40]
   4b968:	b.eq	4ba0c <__gmpn_dcpi1_divappr_q@@Base+0x1c0>  // b.none
   4b96c:	cmp	x26, #0x1
   4b970:	b.ne	4ba28 <__gmpn_dcpi1_divappr_q@@Base+0x1dc>  // b.any
   4b974:	ldur	x2, [x29, #-8]
   4b978:	add	x0, x1, x24
   4b97c:	mov	x9, xzr
   4b980:	sub	x15, x0, #0x8
   4b984:	add	x10, x2, x14, lsl #3
   4b988:	stp	x14, x15, [x29, #-56]
   4b98c:	add	x11, x20, x9
   4b990:	cmp	x11, #0x1
   4b994:	b.lt	4b9b8 <__gmpn_dcpi1_divappr_q@@Base+0x16c>  // b.tstop
   4b998:	lsl	x11, x9, #3
   4b99c:	add	x12, x8, x11
   4b9a0:	ldur	x12, [x12, #-8]
   4b9a4:	ldr	x11, [x10, x11]
   4b9a8:	sub	x9, x9, #0x1
   4b9ac:	cmp	x12, x11
   4b9b0:	b.eq	4b98c <__gmpn_dcpi1_divappr_q@@Base+0x140>  // b.none
   4b9b4:	b.ls	4bb98 <__gmpn_dcpi1_divappr_q@@Base+0x34c>  // b.plast
   4b9b8:	mov	x1, x0
   4b9bc:	mov	x3, x20
   4b9c0:	mov	x25, x16
   4b9c4:	bl	c2d0 <__gmpn_sub_n@plt>
   4b9c8:	mov	x16, x25
   4b9cc:	mov	w25, #0x1                   	// #1
   4b9d0:	b	4bb9c <__gmpn_dcpi1_divappr_q@@Base+0x350>
   4b9d4:	ldr	x5, [x12]
   4b9d8:	add	x8, x9, x8, lsl #3
   4b9dc:	sub	x1, x8, #0x10
   4b9e0:	mvn	x8, x23
   4b9e4:	lsl	x2, x4, #1
   4b9e8:	add	x3, x16, x8, lsl #3
   4b9ec:	mov	x0, x20
   4b9f0:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   4b9f4:	mov	x25, x0
   4b9f8:	add	x1, x20, #0x8
   4b9fc:	mov	x0, x22
   4ba00:	mov	x2, x23
   4ba04:	bl	ca50 <__gmpn_copyi@plt>
   4ba08:	b	4bd2c <__gmpn_dcpi1_divappr_q@@Base+0x4e0>
   4ba0c:	sub	x2, x13, #0x18
   4ba10:	sub	x4, x16, #0x10
   4ba14:	mov	w3, #0x4                   	// #4
   4ba18:	mov	x0, x22
   4ba1c:	mov	x1, xzr
   4ba20:	bl	c200 <__gmpn_divrem_2@plt>
   4ba24:	b	4ba90 <__gmpn_dcpi1_divappr_q@@Base+0x244>
   4ba28:	cmp	x26, #0x29
   4ba2c:	sub	x1, x1, x9
   4ba30:	b.le	4ba60 <__gmpn_dcpi1_divappr_q@@Base+0x214>
   4ba34:	ldp	x5, x9, [x29, #-16]
   4ba38:	lsl	x8, x20, #1
   4ba3c:	sub	x8, x8, x28
   4ba40:	ldur	x4, [x29, #-24]
   4ba44:	add	x8, x9, x8, lsl #3
   4ba48:	add	x8, x8, x24
   4ba4c:	sub	x2, x8, #0x8
   4ba50:	mov	x0, x22
   4ba54:	mov	x3, x26
   4ba58:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4ba5c:	b	4ba90 <__gmpn_dcpi1_divappr_q@@Base+0x244>
   4ba60:	ldur	x9, [x29, #-24]
   4ba64:	lsl	x8, x20, #1
   4ba68:	sub	x8, x8, x28
   4ba6c:	lsl	x2, x26, #1
   4ba70:	ldr	x5, [x9]
   4ba74:	ldur	x9, [x29, #-8]
   4ba78:	mov	x0, x22
   4ba7c:	mov	x4, x26
   4ba80:	add	x8, x9, x8, lsl #3
   4ba84:	add	x8, x8, x24
   4ba88:	sub	x3, x8, #0x8
   4ba8c:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4ba90:	mvn	x8, x28
   4ba94:	add	x8, x8, x20, lsl #1
   4ba98:	mov	x25, x0
   4ba9c:	cmp	x8, x21
   4baa0:	b.eq	4bc6c <__gmpn_dcpi1_divappr_q@@Base+0x420>  // b.none
   4baa4:	lsl	x8, x20, #1
   4baa8:	sub	x8, x8, x28
   4baac:	mvn	x9, x21
   4bab0:	add	x8, x9, x8
   4bab4:	cmp	x26, x8
   4bab8:	stur	x8, [x29, #-48]
   4babc:	b.le	4bad4 <__gmpn_dcpi1_divappr_q@@Base+0x288>
   4bac0:	ldp	x28, x3, [x29, #-16]
   4bac4:	mov	x1, x22
   4bac8:	mov	x2, x26
   4bacc:	mov	x4, x8
   4bad0:	b	4bae4 <__gmpn_dcpi1_divappr_q@@Base+0x298>
   4bad4:	ldp	x28, x1, [x29, #-16]
   4bad8:	mov	x2, x8
   4badc:	mov	x3, x22
   4bae0:	mov	x4, x26
   4bae4:	mov	x0, x28
   4bae8:	bl	ccd0 <__gmpn_mul@plt>
   4baec:	ldur	x8, [x29, #-32]
   4baf0:	mov	x2, x28
   4baf4:	mov	x3, x20
   4baf8:	add	x8, x8, x24
   4bafc:	sub	x27, x8, #0x8
   4bb00:	mov	x0, x27
   4bb04:	mov	x1, x27
   4bb08:	bl	c2d0 <__gmpn_sub_n@plt>
   4bb0c:	mov	x28, x0
   4bb10:	cbz	x25, 4bb2c <__gmpn_dcpi1_divappr_q@@Base+0x2e0>
   4bb14:	ldur	x2, [x29, #-8]
   4bb18:	ldur	x3, [x29, #-48]
   4bb1c:	add	x0, x27, x26, lsl #3
   4bb20:	mov	x1, x0
   4bb24:	bl	c2d0 <__gmpn_sub_n@plt>
   4bb28:	add	x28, x0, x28
   4bb2c:	cbnz	x28, 4bb58 <__gmpn_dcpi1_divappr_q@@Base+0x30c>
   4bb30:	b	4bc6c <__gmpn_dcpi1_divappr_q@@Base+0x420>
   4bb34:	mov	x8, xzr
   4bb38:	ldur	x2, [x29, #-8]
   4bb3c:	mov	x0, x27
   4bb40:	mov	x1, x27
   4bb44:	mov	x3, x20
   4bb48:	sub	x25, x25, x8
   4bb4c:	bl	ca70 <__gmpn_add_n@plt>
   4bb50:	subs	x28, x28, x0
   4bb54:	b.eq	4bc6c <__gmpn_dcpi1_divappr_q@@Base+0x420>  // b.none
   4bb58:	ldur	x8, [x23, #-8]
   4bb5c:	sub	x9, x8, #0x1
   4bb60:	stur	x9, [x23, #-8]
   4bb64:	cbnz	x8, 4bb34 <__gmpn_dcpi1_divappr_q@@Base+0x2e8>
   4bb68:	mov	x8, x23
   4bb6c:	mov	w9, #0x1                   	// #1
   4bb70:	cmp	x9, x26
   4bb74:	b.ge	4bb90 <__gmpn_dcpi1_divappr_q@@Base+0x344>  // b.tcont
   4bb78:	ldr	x10, [x8]
   4bb7c:	add	x9, x9, #0x1
   4bb80:	sub	x11, x10, #0x1
   4bb84:	str	x11, [x8], #8
   4bb88:	cbz	x10, 4bb70 <__gmpn_dcpi1_divappr_q@@Base+0x324>
   4bb8c:	b	4bb34 <__gmpn_dcpi1_divappr_q@@Base+0x2e8>
   4bb90:	mov	w8, #0x1                   	// #1
   4bb94:	b	4bb38 <__gmpn_dcpi1_divappr_q@@Base+0x2ec>
   4bb98:	mov	x25, xzr
   4bb9c:	ldur	x8, [x29, #-40]
   4bba0:	add	x17, x27, x24
   4bba4:	ldur	x10, [x17, #-16]
   4bba8:	ldur	x9, [x8, #-8]
   4bbac:	ldp	x8, x28, [x16, #-16]
   4bbb0:	cmp	x9, x28
   4bbb4:	b.ne	4bbc0 <__gmpn_dcpi1_divappr_q@@Base+0x374>  // b.any
   4bbb8:	cmp	x10, x8
   4bbbc:	b.eq	4bd74 <__gmpn_dcpi1_divappr_q@@Base+0x528>  // b.none
   4bbc0:	ldur	x11, [x29, #-24]
   4bbc4:	ldur	x12, [x17, #-24]
   4bbc8:	ldr	x11, [x11]
   4bbcc:	mul	x13, x11, x9
   4bbd0:	umulh	x11, x9, x11
   4bbd4:	adds	x14, x13, x10
   4bbd8:	adc	x9, x11, x9
   4bbdc:	msub	x10, x9, x28, x10
   4bbe0:	subs	x15, x12, x8
   4bbe4:	sbc	x10, x10, x28
   4bbe8:	mul	x11, x9, x8
   4bbec:	umulh	x13, x8, x9
   4bbf0:	subs	x12, x15, x11
   4bbf4:	sbc	x10, x10, x13
   4bbf8:	cmp	x10, x14
   4bbfc:	cset	w11, cs  // cs = hs, nlast
   4bc00:	csetm	x13, cs  // cs = hs, nlast
   4bc04:	sub	x9, x9, x11
   4bc08:	and	x11, x8, x13
   4bc0c:	and	x13, x28, x13
   4bc10:	adds	x26, x12, x11
   4bc14:	adc	x27, x10, x13
   4bc18:	cmp	x27, x28
   4bc1c:	add	x3, x9, #0x1
   4bc20:	b.cs	4bd50 <__gmpn_dcpi1_divappr_q@@Base+0x504>  // b.hs, b.nlast
   4bc24:	cmp	x20, #0x3
   4bc28:	b.lt	4bc60 <__gmpn_dcpi1_divappr_q@@Base+0x414>  // b.tstop
   4bc2c:	ldur	x0, [x29, #-48]
   4bc30:	ldur	x1, [x29, #-8]
   4bc34:	sub	x2, x20, #0x2
   4bc38:	stp	x3, x17, [x29, #-72]
   4bc3c:	bl	c9e0 <__gmpn_submul_1@plt>
   4bc40:	ldur	x17, [x29, #-64]
   4bc44:	subs	x8, x26, x0
   4bc48:	cset	w9, cc  // cc = lo, ul, last
   4bc4c:	subs	x27, x27, x9
   4bc50:	stur	x8, [x17, #-24]
   4bc54:	b.cc	4bd94 <__gmpn_dcpi1_divappr_q@@Base+0x548>  // b.lo, b.ul, b.last
   4bc58:	ldur	x3, [x29, #-72]
   4bc5c:	b	4bc64 <__gmpn_dcpi1_divappr_q@@Base+0x418>
   4bc60:	stur	x26, [x17, #-24]
   4bc64:	stur	x27, [x17, #-16]
   4bc68:	stur	x3, [x23, #-8]
   4bc6c:	neg	x21, x21
   4bc70:	cmp	x21, x20
   4bc74:	neg	x10, x20
   4bc78:	b.le	4bcd0 <__gmpn_dcpi1_divappr_q@@Base+0x484>
   4bc7c:	ldp	x8, x27, [x29, #-32]
   4bc80:	ldp	x23, x28, [x29, #-16]
   4bc84:	neg	x26, x20, lsl #3
   4bc88:	stur	x10, [x29, #-40]
   4bc8c:	add	x8, x8, x24
   4bc90:	sub	x24, x8, #0x8
   4bc94:	add	x22, x22, x26
   4bc98:	add	x24, x24, x26
   4bc9c:	mov	x0, x22
   4bca0:	mov	x1, x24
   4bca4:	mov	x2, x28
   4bca8:	mov	x3, x20
   4bcac:	mov	x4, x27
   4bcb0:	mov	x5, x23
   4bcb4:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4bcb8:	sub	x21, x21, x20
   4bcbc:	cmp	x21, x20
   4bcc0:	b.gt	4bc94 <__gmpn_dcpi1_divappr_q@@Base+0x448>
   4bcc4:	ldur	x10, [x29, #-40]
   4bcc8:	add	x8, x24, x19
   4bccc:	b	4bce0 <__gmpn_dcpi1_divappr_q@@Base+0x494>
   4bcd0:	ldur	x8, [x29, #-40]
   4bcd4:	ldur	x27, [x29, #-24]
   4bcd8:	ldur	x28, [x29, #-8]
   4bcdc:	sub	x8, x8, #0x8
   4bce0:	mov	w9, #0x1                   	// #1
   4bce4:	ldur	x5, [x29, #-16]
   4bce8:	lsl	x10, x10, #3
   4bcec:	ldr	x19, [x22]
   4bcf0:	sub	x9, x9, x21
   4bcf4:	add	x26, x22, x9, lsl #3
   4bcf8:	add	x8, x8, x10
   4bcfc:	add	x1, x8, x10
   4bd00:	mov	x0, x26
   4bd04:	mov	x2, x28
   4bd08:	mov	x3, x20
   4bd0c:	mov	x4, x27
   4bd10:	sub	x24, x21, #0x1
   4bd14:	bl	4bdc4 <__gmpn_dcpi1_divappr_q@@Base+0x578>
   4bd18:	add	x1, x26, #0x8
   4bd1c:	mov	x0, x26
   4bd20:	mov	x2, x24
   4bd24:	bl	ca50 <__gmpn_copyi@plt>
   4bd28:	str	x19, [x22]
   4bd2c:	mov	x0, x25
   4bd30:	mov	sp, x29
   4bd34:	ldp	x20, x19, [sp, #80]
   4bd38:	ldp	x22, x21, [sp, #64]
   4bd3c:	ldp	x24, x23, [sp, #48]
   4bd40:	ldp	x26, x25, [sp, #32]
   4bd44:	ldp	x28, x27, [sp, #16]
   4bd48:	ldp	x29, x30, [sp], #96
   4bd4c:	ret
   4bd50:	cmp	x26, x8
   4bd54:	b.cs	4bd60 <__gmpn_dcpi1_divappr_q@@Base+0x514>  // b.hs, b.nlast
   4bd58:	cmp	x27, x28
   4bd5c:	b.ls	4bc24 <__gmpn_dcpi1_divappr_q@@Base+0x3d8>  // b.plast
   4bd60:	subs	x9, x26, x8
   4bd64:	sbc	x27, x27, x28
   4bd68:	add	x3, x3, #0x1
   4bd6c:	mov	x26, x9
   4bd70:	b	4bc24 <__gmpn_dcpi1_divappr_q@@Base+0x3d8>
   4bd74:	ldur	x0, [x29, #-48]
   4bd78:	ldur	x1, [x29, #-8]
   4bd7c:	mov	x3, #0xffffffffffffffff    	// #-1
   4bd80:	mov	x2, x20
   4bd84:	mov	x26, #0xffffffffffffffff    	// #-1
   4bd88:	bl	c9e0 <__gmpn_submul_1@plt>
   4bd8c:	stur	x26, [x23, #-8]
   4bd90:	b	4bc6c <__gmpn_dcpi1_divappr_q@@Base+0x420>
   4bd94:	ldp	x3, x0, [x29, #-56]
   4bd98:	ldur	x2, [x29, #-8]
   4bd9c:	mov	x1, x0
   4bda0:	bl	ca70 <__gmpn_add_n@plt>
   4bda4:	ldp	x3, x17, [x29, #-72]
   4bda8:	add	x8, x27, x28
   4bdac:	add	x27, x8, x0
   4bdb0:	cmp	x3, #0x0
   4bdb4:	cset	w8, eq  // eq = none
   4bdb8:	sub	x25, x25, x8
   4bdbc:	sub	x3, x3, #0x1
   4bdc0:	b	4bc64 <__gmpn_dcpi1_divappr_q@@Base+0x418>
   4bdc4:	sub	sp, sp, #0x80
   4bdc8:	stp	x20, x19, [sp, #112]
   4bdcc:	asr	x20, x3, #1
   4bdd0:	stp	x28, x27, [sp, #48]
   4bdd4:	sub	x28, x3, x20
   4bdd8:	and	x8, x3, #0xfffffffffffffffe
   4bddc:	stp	x29, x30, [sp, #32]
   4bde0:	stp	x26, x25, [sp, #64]
   4bde4:	stp	x24, x23, [sp, #80]
   4bde8:	stp	x22, x21, [sp, #96]
   4bdec:	add	x29, sp, #0x20
   4bdf0:	mov	x23, x5
   4bdf4:	mov	x21, x3
   4bdf8:	mov	x25, x2
   4bdfc:	mov	x27, x1
   4be00:	add	x19, x0, x20, lsl #3
   4be04:	cmp	x28, #0x29
   4be08:	add	x1, x1, x8, lsl #3
   4be0c:	stur	x0, [x29, #-8]
   4be10:	stp	x8, x4, [sp, #8]
   4be14:	b.le	4be30 <__gmpn_dcpi1_divappr_q@@Base+0x5e4>
   4be18:	add	x2, x25, x20, lsl #3
   4be1c:	mov	x0, x19
   4be20:	mov	x3, x28
   4be24:	mov	x5, x23
   4be28:	bl	d3e0 <__gmpn_dcpi1_div_qr_n@plt>
   4be2c:	b	4be48 <__gmpn_dcpi1_divappr_q@@Base+0x5fc>
   4be30:	ldr	x5, [x4]
   4be34:	lsl	x2, x28, #1
   4be38:	add	x3, x25, x20, lsl #3
   4be3c:	mov	x0, x19
   4be40:	mov	x4, x28
   4be44:	bl	c640 <__gmpn_sbpi1_div_qr@plt>
   4be48:	mov	x24, x0
   4be4c:	mov	x0, x23
   4be50:	mov	x1, x19
   4be54:	mov	x2, x28
   4be58:	mov	x3, x25
   4be5c:	mov	x4, x20
   4be60:	bl	ccd0 <__gmpn_mul@plt>
   4be64:	add	x26, x27, x20, lsl #3
   4be68:	mov	x0, x26
   4be6c:	mov	x1, x26
   4be70:	mov	x2, x23
   4be74:	mov	x3, x21
   4be78:	bl	c2d0 <__gmpn_sub_n@plt>
   4be7c:	mov	x22, x0
   4be80:	cbz	x24, 4be9c <__gmpn_dcpi1_divappr_q@@Base+0x650>
   4be84:	add	x0, x27, x21, lsl #3
   4be88:	mov	x1, x0
   4be8c:	mov	x2, x25
   4be90:	mov	x3, x20
   4be94:	bl	c2d0 <__gmpn_sub_n@plt>
   4be98:	add	x22, x0, x22
   4be9c:	cbnz	x22, 4befc <__gmpn_dcpi1_divappr_q@@Base+0x6b0>
   4bea0:	lsl	x8, x28, #3
   4bea4:	cmp	x21, #0x12f
   4bea8:	add	x1, x27, x8
   4beac:	add	x3, x25, x8
   4beb0:	b.le	4bf3c <__gmpn_dcpi1_divappr_q@@Base+0x6f0>
   4beb4:	ldur	x19, [x29, #-8]
   4beb8:	ldr	x4, [sp, #16]
   4bebc:	mov	x2, x3
   4bec0:	mov	x3, x20
   4bec4:	mov	x0, x19
   4bec8:	mov	x5, x23
   4becc:	bl	4bdc4 <__gmpn_dcpi1_divappr_q@@Base+0x578>
   4bed0:	cbnz	x0, 4bf58 <__gmpn_dcpi1_divappr_q@@Base+0x70c>
   4bed4:	b	4bf60 <__gmpn_dcpi1_divappr_q@@Base+0x714>
   4bed8:	mov	x8, xzr
   4bedc:	mov	x0, x26
   4bee0:	mov	x1, x26
   4bee4:	mov	x2, x25
   4bee8:	mov	x3, x21
   4beec:	sub	x24, x24, x8
   4bef0:	bl	ca70 <__gmpn_add_n@plt>
   4bef4:	subs	x22, x22, x0
   4bef8:	b.eq	4bea0 <__gmpn_dcpi1_divappr_q@@Base+0x654>  // b.none
   4befc:	ldr	x8, [x19]
   4bf00:	sub	x9, x8, #0x1
   4bf04:	str	x9, [x19]
   4bf08:	cbnz	x8, 4bed8 <__gmpn_dcpi1_divappr_q@@Base+0x68c>
   4bf0c:	mov	w8, #0x1                   	// #1
   4bf10:	cmp	x8, x28
   4bf14:	b.ge	4bf34 <__gmpn_dcpi1_divappr_q@@Base+0x6e8>  // b.tcont
   4bf18:	lsl	x9, x8, #3
   4bf1c:	ldr	x10, [x19, x9]
   4bf20:	add	x8, x8, #0x1
   4bf24:	sub	x11, x10, #0x1
   4bf28:	str	x11, [x19, x9]
   4bf2c:	cbz	x10, 4bf10 <__gmpn_dcpi1_divappr_q@@Base+0x6c4>
   4bf30:	b	4bed8 <__gmpn_dcpi1_divappr_q@@Base+0x68c>
   4bf34:	mov	w8, #0x1                   	// #1
   4bf38:	b	4bedc <__gmpn_dcpi1_divappr_q@@Base+0x690>
   4bf3c:	ldp	x2, x8, [sp, #8]
   4bf40:	ldur	x19, [x29, #-8]
   4bf44:	mov	x4, x20
   4bf48:	ldr	x5, [x8]
   4bf4c:	mov	x0, x19
   4bf50:	bl	c6f0 <__gmpn_sbpi1_divappr_q@plt>
   4bf54:	cbz	x0, 4bf60 <__gmpn_dcpi1_divappr_q@@Base+0x714>
   4bf58:	cmp	x21, #0x2
   4bf5c:	b.ge	4bf84 <__gmpn_dcpi1_divappr_q@@Base+0x738>  // b.tcont
   4bf60:	mov	x0, x24
   4bf64:	ldp	x20, x19, [sp, #112]
   4bf68:	ldp	x22, x21, [sp, #96]
   4bf6c:	ldp	x24, x23, [sp, #80]
   4bf70:	ldp	x26, x25, [sp, #64]
   4bf74:	ldp	x28, x27, [sp, #48]
   4bf78:	ldp	x29, x30, [sp, #32]
   4bf7c:	add	sp, sp, #0x80
   4bf80:	ret
   4bf84:	cmp	x20, #0x1
   4bf88:	csinc	x8, x20, xzr, gt
   4bf8c:	lsl	x2, x8, #3
   4bf90:	mov	w1, #0xff                  	// #255
   4bf94:	mov	x0, x19
   4bf98:	bl	c5f0 <memset@plt>
   4bf9c:	b	4bf60 <__gmpn_dcpi1_divappr_q@@Base+0x714>

000000000004bfa0 <__gmpn_mu_div_qr@@Base>:
   4bfa0:	sub	sp, sp, #0x80
   4bfa4:	stp	x22, x21, [sp, #96]
   4bfa8:	sub	x22, x3, x5
   4bfac:	add	x8, x22, #0x64
   4bfb0:	stp	x28, x27, [sp, #48]
   4bfb4:	stp	x26, x25, [sp, #64]
   4bfb8:	stp	x24, x23, [sp, #80]
   4bfbc:	stp	x20, x19, [sp, #112]
   4bfc0:	mov	x25, x6
   4bfc4:	mov	x19, x5
   4bfc8:	mov	x26, x3
   4bfcc:	mov	x27, x2
   4bfd0:	cmp	x8, x5
   4bfd4:	mov	x23, x0
   4bfd8:	stp	x29, x30, [sp, #32]
   4bfdc:	add	x29, sp, #0x20
   4bfe0:	b.ge	4c064 <__gmpn_mu_div_qr@@Base+0xc4>  // b.tcont
   4bfe4:	mov	w20, #0x1                   	// #1
   4bfe8:	lsl	x8, x26, #3
   4bfec:	bfi	x20, x22, #1, #63
   4bff0:	add	x9, x4, x19, lsl #3
   4bff4:	add	x21, x22, #0x1
   4bff8:	mvn	x10, x22
   4bffc:	add	x11, x1, x8
   4c000:	add	x8, x27, x8
   4c004:	lsl	x12, x20, #3
   4c008:	mov	x28, x4
   4c00c:	str	x1, [sp, #16]
   4c010:	sub	x1, x11, x12
   4c014:	sub	x2, x8, x12
   4c018:	add	x4, x9, x10, lsl #3
   4c01c:	mov	x0, x23
   4c020:	mov	x3, x20
   4c024:	mov	x5, x21
   4c028:	mov	x6, x25
   4c02c:	str	x1, [sp, #8]
   4c030:	bl	4c194 <__gmpn_mu_div_qr@@Base+0x1f4>
   4c034:	str	x21, [sp]
   4c038:	sub	x21, x19, x21
   4c03c:	cmp	x21, x22
   4c040:	mov	x24, x0
   4c044:	mov	x0, x25
   4c048:	stur	x28, [x29, #-8]
   4c04c:	b.le	4c098 <__gmpn_mu_div_qr@@Base+0xf8>
   4c050:	mov	x1, x28
   4c054:	mov	x2, x21
   4c058:	mov	x3, x23
   4c05c:	mov	x4, x22
   4c060:	b	4c0a8 <__gmpn_mu_div_qr@@Base+0x108>
   4c064:	mov	x0, x23
   4c068:	mov	x2, x27
   4c06c:	mov	x3, x26
   4c070:	mov	x5, x19
   4c074:	mov	x6, x25
   4c078:	ldp	x20, x19, [sp, #112]
   4c07c:	ldp	x22, x21, [sp, #96]
   4c080:	ldp	x24, x23, [sp, #80]
   4c084:	ldp	x26, x25, [sp, #64]
   4c088:	ldp	x28, x27, [sp, #48]
   4c08c:	ldp	x29, x30, [sp, #32]
   4c090:	add	sp, sp, #0x80
   4c094:	b	4c194 <__gmpn_mu_div_qr@@Base+0x1f4>
   4c098:	mov	x1, x23
   4c09c:	mov	x2, x22
   4c0a0:	mov	x3, x28
   4c0a4:	mov	x4, x21
   4c0a8:	bl	ccd0 <__gmpn_mul@plt>
   4c0ac:	mov	x28, x24
   4c0b0:	ldr	x24, [sp]
   4c0b4:	neg	x8, x20
   4c0b8:	str	x8, [sp]
   4c0bc:	cbz	x28, 4c0d8 <__gmpn_mu_div_qr@@Base+0x138>
   4c0c0:	ldur	x2, [x29, #-8]
   4c0c4:	add	x0, x25, x22, lsl #3
   4c0c8:	mov	x1, x0
   4c0cc:	mov	x3, x21
   4c0d0:	bl	ca70 <__gmpn_add_n@plt>
   4c0d4:	b	4c0dc <__gmpn_mu_div_qr@@Base+0x13c>
   4c0d8:	mov	x0, xzr
   4c0dc:	sub	x3, x26, x20
   4c0e0:	ldr	x20, [sp, #16]
   4c0e4:	add	x8, x25, x19, lsl #3
   4c0e8:	stur	x0, [x8, #-8]
   4c0ec:	mov	x1, x27
   4c0f0:	mov	x0, x20
   4c0f4:	mov	x2, x25
   4c0f8:	bl	c2d0 <__gmpn_sub_n@plt>
   4c0fc:	mov	x4, x0
   4c100:	ldp	x9, x0, [sp]
   4c104:	add	x8, x25, x26, lsl #3
   4c108:	mov	x3, x24
   4c10c:	add	x2, x8, x9, lsl #3
   4c110:	mov	x1, x0
   4c114:	bl	c760 <__gmpn_sub_nc@plt>
   4c118:	cbz	x0, 4c170 <__gmpn_mu_div_qr@@Base+0x1d0>
   4c11c:	ldr	x8, [x23]
   4c120:	sub	x9, x8, #0x1
   4c124:	str	x9, [x23]
   4c128:	cbnz	x8, 4c154 <__gmpn_mu_div_qr@@Base+0x1b4>
   4c12c:	mov	w8, #0x1                   	// #1
   4c130:	mov	w9, #0x1                   	// #1
   4c134:	cmp	x9, x22
   4c138:	b.ge	4c158 <__gmpn_mu_div_qr@@Base+0x1b8>  // b.tcont
   4c13c:	lsl	x10, x9, #3
   4c140:	ldr	x11, [x23, x10]
   4c144:	add	x9, x9, #0x1
   4c148:	sub	x12, x11, #0x1
   4c14c:	str	x12, [x23, x10]
   4c150:	cbz	x11, 4c134 <__gmpn_mu_div_qr@@Base+0x194>
   4c154:	mov	x8, xzr
   4c158:	ldur	x2, [x29, #-8]
   4c15c:	mov	x0, x20
   4c160:	mov	x1, x20
   4c164:	mov	x3, x19
   4c168:	sub	x28, x28, x8
   4c16c:	bl	ca70 <__gmpn_add_n@plt>
   4c170:	mov	x0, x28
   4c174:	ldp	x20, x19, [sp, #112]
   4c178:	ldp	x22, x21, [sp, #96]
   4c17c:	ldp	x24, x23, [sp, #80]
   4c180:	ldp	x26, x25, [sp, #64]
   4c184:	ldp	x28, x27, [sp, #48]
   4c188:	ldp	x29, x30, [sp, #32]
   4c18c:	add	sp, sp, #0x80
   4c190:	ret
   4c194:	sub	sp, sp, #0x70
   4c198:	stp	x26, x25, [sp, #48]
   4c19c:	sub	x26, x3, x5
   4c1a0:	stp	x24, x23, [sp, #64]
   4c1a4:	stp	x22, x21, [sp, #80]
   4c1a8:	stp	x20, x19, [sp, #96]
   4c1ac:	mov	x19, x6
   4c1b0:	mov	x20, x5
   4c1b4:	mov	x21, x4
   4c1b8:	mov	x22, x3
   4c1bc:	mov	x23, x2
   4c1c0:	mov	x24, x1
   4c1c4:	cmp	x26, x5
   4c1c8:	mov	x25, x0
   4c1cc:	stp	x29, x30, [sp, #16]
   4c1d0:	stp	x28, x27, [sp, #32]
   4c1d4:	add	x29, sp, #0x10
   4c1d8:	b.le	4c1ec <__gmpn_mu_div_qr@@Base+0x24c>
   4c1dc:	sub	x8, x26, #0x1
   4c1e0:	sdiv	x9, x8, x20
   4c1e4:	add	x9, x9, #0x1
   4c1e8:	b	4c200 <__gmpn_mu_div_qr@@Base+0x260>
   4c1ec:	add	x8, x26, x26, lsl #1
   4c1f0:	cmp	x8, x20
   4c1f4:	b.le	4c208 <__gmpn_mu_div_qr@@Base+0x268>
   4c1f8:	sub	x8, x26, #0x1
   4c1fc:	mov	w9, #0x2                   	// #2
   4c200:	sdiv	x8, x8, x9
   4c204:	add	x26, x8, #0x1
   4c208:	add	x28, x19, x26, lsl #3
   4c20c:	cmp	x26, x20
   4c210:	add	x27, x28, #0x8
   4c214:	b.ne	4c258 <__gmpn_mu_div_qr@@Base+0x2b8>  // b.any
   4c218:	add	x0, x27, #0x8
   4c21c:	mov	x1, x21
   4c220:	mov	x2, x20
   4c224:	bl	ca50 <__gmpn_copyi@plt>
   4c228:	add	x9, x27, x20, lsl #3
   4c22c:	mov	w8, #0x1                   	// #1
   4c230:	add	x2, x20, #0x1
   4c234:	add	x3, x9, #0x8
   4c238:	mov	x0, x19
   4c23c:	mov	x1, x27
   4c240:	str	x8, [x27]
   4c244:	bl	d060 <__gmpn_invertappr@plt>
   4c248:	add	x1, x19, #0x8
   4c24c:	mov	x0, x19
   4c250:	mov	x2, x20
   4c254:	b	4c3fc <__gmpn_mu_div_qr@@Base+0x45c>
   4c258:	add	x8, x21, x20, lsl #3
   4c25c:	mvn	x9, x26
   4c260:	add	x13, x8, x9, lsl #3
   4c264:	ldr	x9, [x13]
   4c268:	adds	x9, x9, #0x1
   4c26c:	str	x9, [x27]
   4c270:	b.cc	4c370 <__gmpn_mu_div_qr@@Base+0x3d0>  // b.lo, b.ul, b.last
   4c274:	sub	x10, x20, x26
   4c278:	mov	x15, xzr
   4c27c:	mov	x9, xzr
   4c280:	add	x14, x21, x10, lsl #3
   4c284:	mov	x11, x26
   4c288:	add	x12, x15, #0x1
   4c28c:	cmp	x12, x26
   4c290:	b.gt	4c448 <__gmpn_mu_div_qr@@Base+0x4a8>
   4c294:	lsl	x15, x15, #3
   4c298:	ldr	x16, [x14, x15]
   4c29c:	add	x15, x28, x15
   4c2a0:	add	x9, x9, #0x8
   4c2a4:	sub	x11, x11, #0x1
   4c2a8:	adds	x16, x16, #0x1
   4c2ac:	str	x16, [x15, #16]
   4c2b0:	mov	x15, x12
   4c2b4:	b.cs	4c288 <__gmpn_mu_div_qr@@Base+0x2e8>  // b.hs, b.nlast
   4c2b8:	cmp	x13, x27
   4c2bc:	b.eq	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4c2c0:	cmp	x12, x26
   4c2c4:	b.ge	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.tcont
   4c2c8:	sub	x13, x26, x12
   4c2cc:	cmp	x13, #0x4
   4c2d0:	add	x14, x12, #0x1
   4c2d4:	b.cc	4c348 <__gmpn_mu_div_qr@@Base+0x3a8>  // b.lo, b.ul, b.last
   4c2d8:	add	x15, x28, x9
   4c2dc:	add	x15, x15, #0x10
   4c2e0:	cmp	x15, x8
   4c2e4:	b.cs	4c300 <__gmpn_mu_div_qr@@Base+0x360>  // b.hs, b.nlast
   4c2e8:	add	x15, x19, x26, lsl #4
   4c2ec:	add	x16, x21, x10, lsl #3
   4c2f0:	add	x15, x15, #0x10
   4c2f4:	add	x16, x16, x9
   4c2f8:	cmp	x16, x15
   4c2fc:	b.cc	4c348 <__gmpn_mu_div_qr@@Base+0x3a8>  // b.lo, b.ul, b.last
   4c300:	add	x15, x21, x10, lsl #3
   4c304:	and	x16, x11, #0xfffffffffffffffc
   4c308:	add	x14, x28, x9
   4c30c:	and	x10, x13, #0xfffffffffffffffc
   4c310:	add	x9, x15, x9
   4c314:	add	x12, x16, x12
   4c318:	add	x11, x14, #0x20
   4c31c:	add	x9, x9, #0x10
   4c320:	add	x14, x12, #0x1
   4c324:	mov	x12, x10
   4c328:	ldp	q0, q1, [x9, #-16]
   4c32c:	subs	x12, x12, #0x4
   4c330:	add	x9, x9, #0x20
   4c334:	stp	q0, q1, [x11, #-16]
   4c338:	add	x11, x11, #0x20
   4c33c:	b.ne	4c328 <__gmpn_mu_div_qr@@Base+0x388>  // b.any
   4c340:	cmp	x13, x10
   4c344:	b.eq	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4c348:	add	x10, x14, x26
   4c34c:	mvn	x9, x26
   4c350:	add	x10, x19, x10, lsl #3
   4c354:	add	x9, x9, x14
   4c358:	add	x10, x10, #0x8
   4c35c:	ldr	x11, [x8, x9, lsl #3]
   4c360:	adds	x9, x9, #0x1
   4c364:	str	x11, [x10], #8
   4c368:	b.cc	4c35c <__gmpn_mu_div_qr@@Base+0x3bc>  // b.lo, b.ul, b.last
   4c36c:	b	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>
   4c370:	cmp	x26, #0x1
   4c374:	b.lt	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.tstop
   4c378:	cmp	x13, x27
   4c37c:	b.eq	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4c380:	cmp	x26, #0x4
   4c384:	b.cc	4c3b0 <__gmpn_mu_div_qr@@Base+0x410>  // b.lo, b.ul, b.last
   4c388:	add	x12, x19, x26, lsl #3
   4c38c:	add	x9, x12, #0x10
   4c390:	sub	x10, x20, x26
   4c394:	cmp	x9, x8
   4c398:	add	x9, x21, x10, lsl #3
   4c39c:	b.cs	4c460 <__gmpn_mu_div_qr@@Base+0x4c0>  // b.hs, b.nlast
   4c3a0:	add	x10, x19, x26, lsl #4
   4c3a4:	add	x10, x10, #0x10
   4c3a8:	cmp	x9, x10
   4c3ac:	b.cs	4c460 <__gmpn_mu_div_qr@@Base+0x4c0>  // b.hs, b.nlast
   4c3b0:	mov	w9, #0x1                   	// #1
   4c3b4:	mvn	x10, x26
   4c3b8:	add	x11, x9, x26
   4c3bc:	add	x9, x10, x9
   4c3c0:	add	x10, x19, x11, lsl #3
   4c3c4:	add	x10, x10, #0x8
   4c3c8:	ldr	x11, [x8, x9, lsl #3]
   4c3cc:	adds	x9, x9, #0x1
   4c3d0:	str	x11, [x10], #8
   4c3d4:	b.cc	4c3c8 <__gmpn_mu_div_qr@@Base+0x428>  // b.lo, b.ul, b.last
   4c3d8:	add	x8, x27, x26, lsl #3
   4c3dc:	add	x2, x26, #0x1
   4c3e0:	add	x3, x8, #0x8
   4c3e4:	mov	x0, x19
   4c3e8:	mov	x1, x27
   4c3ec:	bl	d060 <__gmpn_invertappr@plt>
   4c3f0:	add	x1, x19, #0x8
   4c3f4:	mov	x0, x19
   4c3f8:	mov	x2, x26
   4c3fc:	bl	ca50 <__gmpn_copyi@plt>
   4c400:	mov	x0, x25
   4c404:	mov	x1, x24
   4c408:	mov	x2, x23
   4c40c:	mov	x3, x22
   4c410:	mov	x4, x21
   4c414:	mov	x5, x20
   4c418:	mov	x6, x19
   4c41c:	mov	x7, x26
   4c420:	str	x28, [sp]
   4c424:	bl	d080 <__gmpn_preinv_mu_div_qr@plt>
   4c428:	ldp	x20, x19, [sp, #96]
   4c42c:	ldp	x22, x21, [sp, #80]
   4c430:	ldp	x24, x23, [sp, #64]
   4c434:	ldp	x26, x25, [sp, #48]
   4c438:	ldp	x28, x27, [sp, #32]
   4c43c:	ldp	x29, x30, [sp, #16]
   4c440:	add	sp, sp, #0x70
   4c444:	ret
   4c448:	cbz	x26, 4c400 <__gmpn_mu_div_qr@@Base+0x460>
   4c44c:	lsl	x2, x26, #3
   4c450:	mov	x0, x19
   4c454:	mov	w1, wzr
   4c458:	bl	c5f0 <memset@plt>
   4c45c:	b	4c400 <__gmpn_mu_div_qr@@Base+0x460>
   4c460:	and	x10, x26, #0xfffffffffffffffc
   4c464:	add	x11, x9, #0x10
   4c468:	orr	x9, x10, #0x1
   4c46c:	add	x12, x12, #0x20
   4c470:	mov	x13, x10
   4c474:	ldp	q0, q1, [x11, #-16]
   4c478:	add	x11, x11, #0x20
   4c47c:	subs	x13, x13, #0x4
   4c480:	stp	q0, q1, [x12, #-16]
   4c484:	add	x12, x12, #0x20
   4c488:	b.ne	4c474 <__gmpn_mu_div_qr@@Base+0x4d4>  // b.any
   4c48c:	cmp	x26, x10
   4c490:	b.eq	4c3d8 <__gmpn_mu_div_qr@@Base+0x438>  // b.none
   4c494:	b	4c3b4 <__gmpn_mu_div_qr@@Base+0x414>

000000000004c498 <__gmpn_preinv_mu_div_qr@@Base>:
   4c498:	sub	sp, sp, #0xc0
   4c49c:	stp	x29, x30, [sp, #96]
   4c4a0:	add	x29, sp, #0x60
   4c4a4:	stp	x28, x27, [sp, #112]
   4c4a8:	stp	x24, x23, [sp, #144]
   4c4ac:	ldr	x24, [x29, #96]
   4c4b0:	sub	x28, x3, x5
   4c4b4:	lsl	x8, x28, #3
   4c4b8:	stp	x26, x25, [sp, #128]
   4c4bc:	mov	x23, x1
   4c4c0:	add	x1, x2, x8
   4c4c4:	add	x26, x0, x8
   4c4c8:	add	x8, x2, x3, lsl #3
   4c4cc:	stp	x22, x21, [sp, #160]
   4c4d0:	stp	x20, x19, [sp, #176]
   4c4d4:	mov	x19, x7
   4c4d8:	mov	x27, x6
   4c4dc:	mov	x21, x5
   4c4e0:	mov	x22, x4
   4c4e4:	sub	x8, x8, #0x8
   4c4e8:	mov	x9, x5
   4c4ec:	stur	x1, [x29, #-8]
   4c4f0:	subs	x10, x9, #0x1
   4c4f4:	b.lt	4c514 <__gmpn_preinv_mu_div_qr@@Base+0x7c>  // b.tstop
   4c4f8:	add	x9, x22, x9, lsl #3
   4c4fc:	ldr	x11, [x8], #-8
   4c500:	ldur	x9, [x9, #-8]
   4c504:	cmp	x11, x9
   4c508:	mov	x9, x10
   4c50c:	b.eq	4c4f0 <__gmpn_preinv_mu_div_qr@@Base+0x58>  // b.none
   4c510:	b.ls	4c538 <__gmpn_preinv_mu_div_qr@@Base+0xa0>  // b.plast
   4c514:	mov	x0, x23
   4c518:	mov	x2, x22
   4c51c:	mov	x3, x21
   4c520:	bl	c2d0 <__gmpn_sub_n@plt>
   4c524:	mov	w8, #0x1                   	// #1
   4c528:	str	x8, [sp]
   4c52c:	cmp	x28, #0x1
   4c530:	b.ge	4c550 <__gmpn_preinv_mu_div_qr@@Base+0xb8>  // b.tcont
   4c534:	b	4c83c <__gmpn_preinv_mu_div_qr@@Base+0x3a4>
   4c538:	mov	x0, x23
   4c53c:	mov	x2, x21
   4c540:	bl	ca50 <__gmpn_copyi@plt>
   4c544:	str	xzr, [sp]
   4c548:	cmp	x28, #0x1
   4c54c:	b.lt	4c83c <__gmpn_preinv_mu_div_qr@@Base+0x3a4>  // b.tstop
   4c550:	add	x8, x23, x21, lsl #3
   4c554:	stur	x8, [x29, #-32]
   4c558:	add	x8, x21, #0x1
   4c55c:	str	x8, [sp, #32]
   4c560:	sub	x8, x23, #0x8
   4c564:	str	x8, [sp, #16]
   4c568:	add	x8, x24, #0x8
   4c56c:	neg	x25, x21
   4c570:	str	x8, [sp, #8]
   4c574:	sub	x8, x21, #0x1
   4c578:	stur	x8, [x29, #-40]
   4c57c:	str	x25, [sp, #24]
   4c580:	b	4c58c <__gmpn_preinv_mu_div_qr@@Base+0xf4>
   4c584:	cmp	x28, #0x0
   4c588:	b.le	4c83c <__gmpn_preinv_mu_div_qr@@Base+0x3a4>
   4c58c:	subs	x8, x19, x28
   4c590:	add	x8, x27, x8, lsl #3
   4c594:	csel	x27, x8, x27, gt
   4c598:	ldur	x8, [x29, #-32]
   4c59c:	csel	x19, x28, x19, gt
   4c5a0:	lsl	x20, x19, #3
   4c5a4:	mov	x0, x24
   4c5a8:	sub	x25, x8, x20
   4c5ac:	mov	x1, x25
   4c5b0:	mov	x2, x27
   4c5b4:	mov	x3, x19
   4c5b8:	sub	x26, x26, x20
   4c5bc:	bl	c990 <__gmpn_mul_n@plt>
   4c5c0:	add	x1, x24, x20
   4c5c4:	mov	x0, x26
   4c5c8:	mov	x2, x25
   4c5cc:	mov	x3, x19
   4c5d0:	stur	x1, [x29, #-24]
   4c5d4:	bl	ca70 <__gmpn_add_n@plt>
   4c5d8:	cbnz	x0, 4c860 <__gmpn_preinv_mu_div_qr@@Base+0x3c8>
   4c5dc:	cmp	x19, #0x11
   4c5e0:	stur	x28, [x29, #-16]
   4c5e4:	b.le	4c694 <__gmpn_preinv_mu_div_qr@@Base+0x1fc>
   4c5e8:	ldr	x0, [sp, #32]
   4c5ec:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4c5f0:	mov	x28, x0
   4c5f4:	add	x6, x24, x0, lsl #3
   4c5f8:	mov	x0, x24
   4c5fc:	mov	x1, x28
   4c600:	mov	x2, x22
   4c604:	mov	x3, x21
   4c608:	mov	x4, x26
   4c60c:	mov	x5, x19
   4c610:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4c614:	add	x8, x19, x21
   4c618:	sub	x25, x8, x28
   4c61c:	cmp	x25, #0x1
   4c620:	b.lt	4c730 <__gmpn_preinv_mu_div_qr@@Base+0x298>  // b.tstop
   4c624:	neg	x8, x19
   4c628:	stp	x8, x27, [sp, #40]
   4c62c:	ldur	x8, [x29, #-32]
   4c630:	lsl	x27, x25, #3
   4c634:	mov	x0, x24
   4c638:	mov	x1, x24
   4c63c:	sub	x2, x8, x27
   4c640:	mov	x3, x25
   4c644:	bl	c2d0 <__gmpn_sub_n@plt>
   4c648:	add	x8, x24, x27
   4c64c:	ldr	x9, [x8]
   4c650:	subs	x9, x9, x0
   4c654:	str	x9, [x8]
   4c658:	b.cs	4c6b0 <__gmpn_preinv_mu_div_qr@@Base+0x218>  // b.hs, b.nlast
   4c65c:	ldr	x27, [sp, #48]
   4c660:	ldr	x14, [sp, #24]
   4c664:	sub	x9, x28, x25
   4c668:	mov	w10, #0x1                   	// #1
   4c66c:	cmp	x10, x9
   4c670:	b.ge	4c6c0 <__gmpn_preinv_mu_div_qr@@Base+0x228>  // b.tcont
   4c674:	lsl	x11, x10, #3
   4c678:	ldr	x12, [x8, x11]
   4c67c:	add	x10, x10, #0x1
   4c680:	sub	x13, x12, #0x1
   4c684:	str	x13, [x8, x11]
   4c688:	cbz	x12, 4c66c <__gmpn_preinv_mu_div_qr@@Base+0x1d4>
   4c68c:	mov	x8, xzr
   4c690:	b	4c6c4 <__gmpn_preinv_mu_div_qr@@Base+0x22c>
   4c694:	mov	x0, x24
   4c698:	mov	x1, x22
   4c69c:	mov	x2, x21
   4c6a0:	mov	x3, x26
   4c6a4:	mov	x4, x19
   4c6a8:	bl	ccd0 <__gmpn_mul@plt>
   4c6ac:	b	4c730 <__gmpn_preinv_mu_div_qr@@Base+0x298>
   4c6b0:	ldr	x27, [sp, #48]
   4c6b4:	ldr	x14, [sp, #24]
   4c6b8:	mov	x8, xzr
   4c6bc:	b	4c6c4 <__gmpn_preinv_mu_div_qr@@Base+0x22c>
   4c6c0:	mov	w8, #0x1                   	// #1
   4c6c4:	ldr	x9, [sp, #16]
   4c6c8:	ldr	x10, [sp, #40]
   4c6cc:	add	x9, x9, x10, lsl #3
   4c6d0:	add	x10, x14, x28
   4c6d4:	cmp	x10, #0x1
   4c6d8:	b.lt	4c700 <__gmpn_preinv_mu_div_qr@@Base+0x268>  // b.tstop
   4c6dc:	lsl	x10, x28, #3
   4c6e0:	ldr	x11, [x9, x10]
   4c6e4:	add	x10, x24, x10
   4c6e8:	ldur	x10, [x10, #-8]
   4c6ec:	sub	x28, x28, #0x1
   4c6f0:	cmp	x11, x10
   4c6f4:	b.eq	4c6d0 <__gmpn_preinv_mu_div_qr@@Base+0x238>  // b.none
   4c6f8:	cset	w9, ls  // ls = plast
   4c6fc:	b	4c704 <__gmpn_preinv_mu_div_qr@@Base+0x26c>
   4c700:	mov	x9, xzr
   4c704:	subs	x8, x9, x8
   4c708:	b.cc	4c878 <__gmpn_preinv_mu_div_qr@@Base+0x3e0>  // b.lo, b.ul, b.last
   4c70c:	ldr	x9, [x24]
   4c710:	adds	x8, x9, x8
   4c714:	str	x8, [x24]
   4c718:	b.cc	4c730 <__gmpn_preinv_mu_div_qr@@Base+0x298>  // b.lo, b.ul, b.last
   4c71c:	ldr	x8, [sp, #8]
   4c720:	ldr	x9, [x8]
   4c724:	adds	x9, x9, #0x1
   4c728:	str	x9, [x8], #8
   4c72c:	b.cs	4c720 <__gmpn_preinv_mu_div_qr@@Base+0x288>  // b.hs, b.nlast
   4c730:	subs	x8, x21, x19
   4c734:	ldr	x8, [x23, x8, lsl #3]
   4c738:	ldr	x9, [x24, x21, lsl #3]
   4c73c:	ldp	x28, x1, [x29, #-16]
   4c740:	subs	x25, x21, x19
   4c744:	sub	x1, x1, x20
   4c748:	sub	x20, x8, x9
   4c74c:	stur	x1, [x29, #-8]
   4c750:	b.ne	4c76c <__gmpn_preinv_mu_div_qr@@Base+0x2d4>  // b.any
   4c754:	mov	x0, x23
   4c758:	mov	x2, x24
   4c75c:	mov	x3, x21
   4c760:	bl	c2d0 <__gmpn_sub_n@plt>
   4c764:	mov	x25, x0
   4c768:	b	4c7a8 <__gmpn_preinv_mu_div_qr@@Base+0x310>
   4c76c:	mov	x0, x24
   4c770:	mov	x2, x24
   4c774:	mov	x3, x19
   4c778:	bl	c2d0 <__gmpn_sub_n@plt>
   4c77c:	mov	x4, x0
   4c780:	ldur	x0, [x29, #-24]
   4c784:	mov	x1, x23
   4c788:	mov	x3, x25
   4c78c:	mov	x2, x0
   4c790:	bl	c760 <__gmpn_sub_nc@plt>
   4c794:	mov	x25, x0
   4c798:	mov	x0, x23
   4c79c:	mov	x1, x24
   4c7a0:	mov	x2, x21
   4c7a4:	bl	ca50 <__gmpn_copyi@plt>
   4c7a8:	subs	x20, x20, x25
   4c7ac:	sub	x28, x28, x19
   4c7b0:	b.eq	4c7e4 <__gmpn_preinv_mu_div_qr@@Base+0x34c>  // b.none
   4c7b4:	mov	x8, x26
   4c7b8:	ldr	x9, [x8]
   4c7bc:	adds	x9, x9, #0x1
   4c7c0:	str	x9, [x8], #8
   4c7c4:	b.cs	4c7b8 <__gmpn_preinv_mu_div_qr@@Base+0x320>  // b.hs, b.nlast
   4c7c8:	mov	x0, x23
   4c7cc:	mov	x1, x23
   4c7d0:	mov	x2, x22
   4c7d4:	mov	x3, x21
   4c7d8:	bl	c2d0 <__gmpn_sub_n@plt>
   4c7dc:	subs	x20, x20, x0
   4c7e0:	b.ne	4c7b4 <__gmpn_preinv_mu_div_qr@@Base+0x31c>  // b.any
   4c7e4:	ldur	x8, [x29, #-40]
   4c7e8:	add	x9, x8, #0x1
   4c7ec:	cmp	x9, #0x1
   4c7f0:	b.lt	4c810 <__gmpn_preinv_mu_div_qr@@Base+0x378>  // b.tstop
   4c7f4:	lsl	x9, x8, #3
   4c7f8:	ldr	x10, [x23, x9]
   4c7fc:	ldr	x9, [x22, x9]
   4c800:	sub	x8, x8, #0x1
   4c804:	cmp	x10, x9
   4c808:	b.eq	4c7e8 <__gmpn_preinv_mu_div_qr@@Base+0x350>  // b.none
   4c80c:	b.ls	4c584 <__gmpn_preinv_mu_div_qr@@Base+0xec>  // b.plast
   4c810:	mov	x8, x26
   4c814:	ldr	x9, [x8]
   4c818:	adds	x9, x9, #0x1
   4c81c:	str	x9, [x8], #8
   4c820:	b.cs	4c814 <__gmpn_preinv_mu_div_qr@@Base+0x37c>  // b.hs, b.nlast
   4c824:	mov	x0, x23
   4c828:	mov	x1, x23
   4c82c:	mov	x2, x22
   4c830:	mov	x3, x21
   4c834:	bl	c2d0 <__gmpn_sub_n@plt>
   4c838:	b	4c584 <__gmpn_preinv_mu_div_qr@@Base+0xec>
   4c83c:	ldr	x0, [sp]
   4c840:	ldp	x20, x19, [sp, #176]
   4c844:	ldp	x22, x21, [sp, #160]
   4c848:	ldp	x24, x23, [sp, #144]
   4c84c:	ldp	x26, x25, [sp, #128]
   4c850:	ldp	x28, x27, [sp, #112]
   4c854:	ldp	x29, x30, [sp, #96]
   4c858:	add	sp, sp, #0xc0
   4c85c:	ret
   4c860:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4c864:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4c868:	add	x0, x0, #0xb6b
   4c86c:	add	x2, x2, #0xb51
   4c870:	mov	w1, #0x118                 	// #280
   4c874:	bl	c6c0 <__gmp_assert_fail@plt>
   4c878:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4c87c:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4c880:	add	x0, x0, #0xb6b
   4c884:	add	x2, x2, #0xb77
   4c888:	mov	w1, #0x12c                 	// #300
   4c88c:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004c890 <__gmpn_mu_div_qr_itch@@Base>:
   4c890:	stp	x29, x30, [sp, #-32]!
   4c894:	stp	x20, x19, [sp, #16]
   4c898:	sub	x20, x0, x1
   4c89c:	mov	x19, x1
   4c8a0:	cmp	x20, x1
   4c8a4:	mov	x29, sp
   4c8a8:	cbz	w2, 4c8bc <__gmpn_mu_div_qr_itch@@Base+0x2c>
   4c8ac:	csel	x8, x19, x20, gt
   4c8b0:	sub	x8, x8, #0x1
   4c8b4:	sxtw	x9, w2
   4c8b8:	b	4c8cc <__gmpn_mu_div_qr_itch@@Base+0x3c>
   4c8bc:	b.le	4c918 <__gmpn_mu_div_qr_itch@@Base+0x88>
   4c8c0:	sub	x8, x20, #0x1
   4c8c4:	sdiv	x9, x8, x19
   4c8c8:	add	x9, x9, #0x1
   4c8cc:	sdiv	x8, x8, x9
   4c8d0:	add	x20, x8, #0x1
   4c8d4:	add	x0, x19, #0x1
   4c8d8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4c8dc:	asr	x8, x0, #1
   4c8e0:	cmp	x8, x20
   4c8e4:	csel	x10, x0, x8, lt  // lt = tstop
   4c8e8:	cmp	x8, x19
   4c8ec:	csel	x8, x10, xzr, lt  // lt = tstop
   4c8f0:	add	x9, x20, x20, lsl #1
   4c8f4:	add	x8, x8, x0, lsl #1
   4c8f8:	add	x8, x8, #0x4
   4c8fc:	add	x9, x9, #0x4
   4c900:	cmp	x9, x8
   4c904:	csel	x8, x9, x8, gt
   4c908:	add	x0, x8, x20
   4c90c:	ldp	x20, x19, [sp, #16]
   4c910:	ldp	x29, x30, [sp], #32
   4c914:	ret
   4c918:	add	x8, x20, x20, lsl #1
   4c91c:	cmp	x8, x19
   4c920:	b.le	4c8d4 <__gmpn_mu_div_qr_itch@@Base+0x44>
   4c924:	sub	x8, x20, #0x1
   4c928:	cmp	x8, #0x0
   4c92c:	csel	x8, x20, x8, lt  // lt = tstop
   4c930:	asr	x8, x8, #1
   4c934:	b	4c8d0 <__gmpn_mu_div_qr_itch@@Base+0x40>

000000000004c938 <__gmpn_preinv_mu_div_qr_itch@@Base>:
   4c938:	stp	x29, x30, [sp, #-32]!
   4c93c:	add	x0, x1, #0x1
   4c940:	stp	x20, x19, [sp, #16]
   4c944:	mov	x29, sp
   4c948:	mov	x19, x2
   4c94c:	mov	x20, x1
   4c950:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4c954:	asr	x8, x0, #1
   4c958:	cmp	x8, x19
   4c95c:	csel	x9, x0, x8, lt  // lt = tstop
   4c960:	cmp	x8, x20
   4c964:	ldp	x20, x19, [sp, #16]
   4c968:	csel	x8, x9, xzr, lt  // lt = tstop
   4c96c:	add	x8, x8, x0, lsl #1
   4c970:	add	x0, x8, #0x4
   4c974:	ldp	x29, x30, [sp], #32
   4c978:	ret

000000000004c97c <__gmpn_mu_divappr_q@@Base>:
   4c97c:	sub	sp, sp, #0xf0
   4c980:	stp	x28, x27, [sp, #160]
   4c984:	sub	x28, x2, x4
   4c988:	add	x8, x28, #0x1
   4c98c:	stp	x26, x25, [sp, #176]
   4c990:	stp	x24, x23, [sp, #192]
   4c994:	stp	x22, x21, [sp, #208]
   4c998:	stp	x20, x19, [sp, #224]
   4c99c:	mov	x19, x5
   4c9a0:	mov	x21, x4
   4c9a4:	mov	x20, x3
   4c9a8:	mov	x24, x2
   4c9ac:	mov	x25, x1
   4c9b0:	cmp	x8, x4
   4c9b4:	mov	x27, x0
   4c9b8:	stp	x29, x30, [sp, #144]
   4c9bc:	add	x29, sp, #0x90
   4c9c0:	b.ge	4c9e0 <__gmpn_mu_divappr_q@@Base+0x64>  // b.tcont
   4c9c4:	sub	x9, x21, x8
   4c9c8:	lsl	x10, x9, #3
   4c9cc:	sub	x24, x24, x9
   4c9d0:	add	x25, x25, x10
   4c9d4:	add	x20, x20, x10
   4c9d8:	mov	x21, x8
   4c9dc:	b	4c9f8 <__gmpn_mu_divappr_q@@Base+0x7c>
   4c9e0:	cmp	x28, x21
   4c9e4:	b.le	4c9f8 <__gmpn_mu_divappr_q@@Base+0x7c>
   4c9e8:	sub	x8, x28, #0x1
   4c9ec:	sdiv	x9, x8, x21
   4c9f0:	add	x9, x9, #0x1
   4c9f4:	b	4ca0c <__gmpn_mu_divappr_q@@Base+0x90>
   4c9f8:	add	x8, x28, x28, lsl #1
   4c9fc:	cmp	x8, x21
   4ca00:	b.le	4ca14 <__gmpn_mu_divappr_q@@Base+0x98>
   4ca04:	sub	x8, x28, #0x1
   4ca08:	mov	w9, #0x2                   	// #2
   4ca0c:	sdiv	x8, x8, x9
   4ca10:	add	x28, x8, #0x1
   4ca14:	add	x23, x19, x28, lsl #3
   4ca18:	subs	x9, x21, x28
   4ca1c:	add	x22, x23, #0x8
   4ca20:	b.ne	4ca68 <__gmpn_mu_divappr_q@@Base+0xec>  // b.any
   4ca24:	add	x0, x22, #0x8
   4ca28:	mov	x1, x20
   4ca2c:	mov	x2, x21
   4ca30:	mov	x26, x19
   4ca34:	bl	ca50 <__gmpn_copyi@plt>
   4ca38:	add	x9, x22, x21, lsl #3
   4ca3c:	mov	w8, #0x1                   	// #1
   4ca40:	add	x2, x21, #0x1
   4ca44:	add	x3, x9, #0x8
   4ca48:	mov	x0, x19
   4ca4c:	mov	x1, x22
   4ca50:	str	x8, [x22]
   4ca54:	bl	d060 <__gmpn_invertappr@plt>
   4ca58:	add	x1, x19, #0x8
   4ca5c:	mov	x0, x19
   4ca60:	mov	x2, x21
   4ca64:	b	4cc0c <__gmpn_mu_divappr_q@@Base+0x290>
   4ca68:	add	x8, x20, x21, lsl #3
   4ca6c:	mvn	x10, x28
   4ca70:	add	x13, x8, x10, lsl #3
   4ca74:	ldr	x10, [x13]
   4ca78:	adds	x10, x10, #0x1
   4ca7c:	str	x10, [x22]
   4ca80:	b.cc	4cb7c <__gmpn_mu_divappr_q@@Base+0x200>  // b.lo, b.ul, b.last
   4ca84:	mov	x15, xzr
   4ca88:	mov	x10, xzr
   4ca8c:	add	x14, x20, x9, lsl #3
   4ca90:	mov	x11, x28
   4ca94:	add	x12, x15, #0x1
   4ca98:	cmp	x12, x28
   4ca9c:	b.gt	4d0ac <__gmpn_mu_divappr_q@@Base+0x730>
   4caa0:	lsl	x15, x15, #3
   4caa4:	ldr	x16, [x14, x15]
   4caa8:	add	x15, x23, x15
   4caac:	add	x10, x10, #0x8
   4cab0:	sub	x11, x11, #0x1
   4cab4:	adds	x16, x16, #0x1
   4cab8:	str	x16, [x15, #16]
   4cabc:	mov	x15, x12
   4cac0:	b.cs	4ca94 <__gmpn_mu_divappr_q@@Base+0x118>  // b.hs, b.nlast
   4cac4:	cmp	x13, x22
   4cac8:	b.eq	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4cacc:	cmp	x12, x28
   4cad0:	b.ge	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.tcont
   4cad4:	sub	x13, x28, x12
   4cad8:	cmp	x13, #0x4
   4cadc:	add	x14, x12, #0x1
   4cae0:	b.cc	4cb54 <__gmpn_mu_divappr_q@@Base+0x1d8>  // b.lo, b.ul, b.last
   4cae4:	add	x15, x23, x10
   4cae8:	add	x15, x15, #0x10
   4caec:	cmp	x15, x8
   4caf0:	b.cs	4cb0c <__gmpn_mu_divappr_q@@Base+0x190>  // b.hs, b.nlast
   4caf4:	add	x15, x19, x28, lsl #4
   4caf8:	add	x16, x20, x9, lsl #3
   4cafc:	add	x15, x15, #0x10
   4cb00:	add	x16, x16, x10
   4cb04:	cmp	x16, x15
   4cb08:	b.cc	4cb54 <__gmpn_mu_divappr_q@@Base+0x1d8>  // b.lo, b.ul, b.last
   4cb0c:	add	x15, x20, x9, lsl #3
   4cb10:	and	x16, x11, #0xfffffffffffffffc
   4cb14:	add	x14, x23, x10
   4cb18:	and	x9, x13, #0xfffffffffffffffc
   4cb1c:	add	x10, x15, x10
   4cb20:	add	x12, x16, x12
   4cb24:	add	x11, x14, #0x20
   4cb28:	add	x10, x10, #0x10
   4cb2c:	add	x14, x12, #0x1
   4cb30:	mov	x12, x9
   4cb34:	ldp	q0, q1, [x10, #-16]
   4cb38:	subs	x12, x12, #0x4
   4cb3c:	add	x10, x10, #0x20
   4cb40:	stp	q0, q1, [x11, #-16]
   4cb44:	add	x11, x11, #0x20
   4cb48:	b.ne	4cb34 <__gmpn_mu_divappr_q@@Base+0x1b8>  // b.any
   4cb4c:	cmp	x13, x9
   4cb50:	b.eq	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4cb54:	add	x10, x14, x28
   4cb58:	mvn	x9, x28
   4cb5c:	add	x10, x19, x10, lsl #3
   4cb60:	add	x9, x9, x14
   4cb64:	add	x10, x10, #0x8
   4cb68:	ldr	x11, [x8, x9, lsl #3]
   4cb6c:	adds	x9, x9, #0x1
   4cb70:	str	x11, [x10], #8
   4cb74:	b.cc	4cb68 <__gmpn_mu_divappr_q@@Base+0x1ec>  // b.lo, b.ul, b.last
   4cb78:	b	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>
   4cb7c:	cmp	x28, #0x1
   4cb80:	b.lt	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.tstop
   4cb84:	cmp	x13, x22
   4cb88:	b.eq	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4cb8c:	cmp	x28, #0x4
   4cb90:	b.cc	4cbbc <__gmpn_mu_divappr_q@@Base+0x240>  // b.lo, b.ul, b.last
   4cb94:	add	x12, x19, x28, lsl #3
   4cb98:	add	x10, x12, #0x10
   4cb9c:	cmp	x10, x8
   4cba0:	add	x9, x20, x9, lsl #3
   4cba4:	b.cs	4d0c4 <__gmpn_mu_divappr_q@@Base+0x748>  // b.hs, b.nlast
   4cba8:	add	x10, x19, x28, lsl #4
   4cbac:	add	x10, x10, #0x10
   4cbb0:	cmp	x9, x10
   4cbb4:	mov	x11, x19
   4cbb8:	b.cs	4d0c4 <__gmpn_mu_divappr_q@@Base+0x748>  // b.hs, b.nlast
   4cbbc:	mov	w9, #0x1                   	// #1
   4cbc0:	mvn	x10, x28
   4cbc4:	add	x11, x9, x28
   4cbc8:	add	x9, x10, x9
   4cbcc:	add	x10, x19, x11, lsl #3
   4cbd0:	add	x10, x10, #0x8
   4cbd4:	ldr	x11, [x8, x9, lsl #3]
   4cbd8:	adds	x9, x9, #0x1
   4cbdc:	str	x11, [x10], #8
   4cbe0:	b.cc	4cbd4 <__gmpn_mu_divappr_q@@Base+0x258>  // b.lo, b.ul, b.last
   4cbe4:	add	x8, x22, x28, lsl #3
   4cbe8:	add	x2, x28, #0x1
   4cbec:	add	x3, x8, #0x8
   4cbf0:	mov	x0, x19
   4cbf4:	mov	x1, x22
   4cbf8:	mov	x26, x19
   4cbfc:	bl	d060 <__gmpn_invertappr@plt>
   4cc00:	add	x1, x19, #0x8
   4cc04:	mov	x0, x19
   4cc08:	mov	x2, x28
   4cc0c:	bl	ca50 <__gmpn_copyi@plt>
   4cc10:	sub	x9, x24, x21
   4cc14:	add	x8, x25, x24, lsl #3
   4cc18:	lsl	x22, x9, #3
   4cc1c:	str	x9, [sp, #64]
   4cc20:	add	x24, x25, x22
   4cc24:	add	x25, x27, x22
   4cc28:	sub	x8, x8, #0x8
   4cc2c:	mov	x9, x21
   4cc30:	subs	x10, x9, #0x1
   4cc34:	b.lt	4cc54 <__gmpn_mu_divappr_q@@Base+0x2d8>  // b.tstop
   4cc38:	add	x9, x20, x9, lsl #3
   4cc3c:	ldr	x11, [x8], #-8
   4cc40:	ldur	x9, [x9, #-8]
   4cc44:	cmp	x11, x9
   4cc48:	mov	x9, x10
   4cc4c:	b.eq	4cc30 <__gmpn_mu_divappr_q@@Base+0x2b4>  // b.none
   4cc50:	b.ls	4cc7c <__gmpn_mu_divappr_q@@Base+0x300>  // b.plast
   4cc54:	mov	x0, x23
   4cc58:	mov	x1, x24
   4cc5c:	mov	x2, x20
   4cc60:	mov	x3, x21
   4cc64:	bl	c2d0 <__gmpn_sub_n@plt>
   4cc68:	mov	w8, wzr
   4cc6c:	mov	w0, #0x1                   	// #1
   4cc70:	ldr	x10, [sp, #64]
   4cc74:	cbnz	x10, 4cc9c <__gmpn_mu_divappr_q@@Base+0x320>
   4cc78:	b	4d08c <__gmpn_mu_divappr_q@@Base+0x710>
   4cc7c:	mov	x0, x23
   4cc80:	mov	x1, x24
   4cc84:	mov	x2, x21
   4cc88:	bl	ca50 <__gmpn_copyi@plt>
   4cc8c:	mov	x0, xzr
   4cc90:	mov	w8, #0x1                   	// #1
   4cc94:	ldr	x10, [sp, #64]
   4cc98:	cbz	x10, 4d08c <__gmpn_mu_divappr_q@@Base+0x710>
   4cc9c:	cmp	x10, #0x1
   4cca0:	str	w8, [sp, #20]
   4cca4:	str	x0, [sp, #24]
   4cca8:	str	x22, [sp, #8]
   4ccac:	b.lt	4cff8 <__gmpn_mu_divappr_q@@Base+0x67c>  // b.tstop
   4ccb0:	add	x8, x23, x21, lsl #3
   4ccb4:	stur	x8, [x29, #-8]
   4ccb8:	add	x8, x21, #0x1
   4ccbc:	stur	x8, [x29, #-64]
   4ccc0:	add	x8, x19, x21, lsl #4
   4ccc4:	sub	x9, x19, #0x8
   4ccc8:	add	x11, x21, x28
   4cccc:	add	x8, x8, #0x8
   4ccd0:	str	x9, [sp, #56]
   4ccd4:	add	x9, x19, x28, lsl #3
   4ccd8:	str	x8, [sp, #40]
   4ccdc:	add	x8, x19, x11, lsl #3
   4cce0:	stur	x19, [x29, #-16]
   4cce4:	add	x8, x8, #0x8
   4cce8:	sub	x19, x9, #0x8
   4ccec:	mov	x22, x28
   4ccf0:	stur	x24, [x29, #-24]
   4ccf4:	str	x11, [sp, #48]
   4ccf8:	str	x8, [sp, #32]
   4ccfc:	str	x28, [sp, #72]
   4cd00:	stur	x19, [x29, #-56]
   4cd04:	b	4cd14 <__gmpn_mu_divappr_q@@Base+0x398>
   4cd08:	ldur	x10, [x29, #-40]
   4cd0c:	cmp	x10, #0x0
   4cd10:	b.le	4cff8 <__gmpn_mu_divappr_q@@Base+0x67c>
   4cd14:	ldp	x2, x28, [x29, #-16]
   4cd18:	subs	x8, x22, x10
   4cd1c:	csel	x22, x10, x22, gt
   4cd20:	mov	x19, x21
   4cd24:	lsl	x21, x22, #3
   4cd28:	add	x8, x2, x8, lsl #3
   4cd2c:	sub	x26, x28, x21
   4cd30:	csel	x2, x8, x2, gt
   4cd34:	mov	x0, x28
   4cd38:	mov	x1, x26
   4cd3c:	mov	x3, x22
   4cd40:	sub	x25, x25, x21
   4cd44:	stur	x2, [x29, #-16]
   4cd48:	mov	x24, x10
   4cd4c:	bl	c990 <__gmpn_mul_n@plt>
   4cd50:	add	x28, x28, x21
   4cd54:	mov	x0, x25
   4cd58:	mov	x1, x28
   4cd5c:	mov	x2, x26
   4cd60:	mov	x3, x22
   4cd64:	stur	x21, [x29, #-32]
   4cd68:	bl	ca70 <__gmpn_add_n@plt>
   4cd6c:	cbnz	x0, 4d0fc <__gmpn_mu_divappr_q@@Base+0x780>
   4cd70:	subs	x24, x24, x22
   4cd74:	b.eq	4cff8 <__gmpn_mu_divappr_q@@Base+0x67c>  // b.none
   4cd78:	cmp	x22, #0x11
   4cd7c:	stp	x28, x24, [x29, #-48]
   4cd80:	b.le	4ce40 <__gmpn_mu_divappr_q@@Base+0x4c4>
   4cd84:	ldur	x0, [x29, #-64]
   4cd88:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4cd8c:	ldur	x26, [x29, #-8]
   4cd90:	mov	x28, x0
   4cd94:	mov	x1, x28
   4cd98:	mov	x2, x20
   4cd9c:	add	x6, x26, x0, lsl #3
   4cda0:	mov	x0, x26
   4cda4:	mov	x3, x19
   4cda8:	mov	x4, x25
   4cdac:	mov	x5, x22
   4cdb0:	mov	x21, x19
   4cdb4:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4cdb8:	add	x8, x22, x19
   4cdbc:	mov	x10, x26
   4cdc0:	sub	x26, x8, x28
   4cdc4:	cmp	x26, #0x1
   4cdc8:	b.lt	4cee0 <__gmpn_mu_divappr_q@@Base+0x564>  // b.tstop
   4cdcc:	lsl	x27, x26, #3
   4cdd0:	sub	x2, x10, x27
   4cdd4:	mov	x0, x10
   4cdd8:	mov	x1, x10
   4cddc:	mov	x3, x26
   4cde0:	mov	x19, x10
   4cde4:	bl	c2d0 <__gmpn_sub_n@plt>
   4cde8:	ldr	x8, [x19, x27]
   4cdec:	subs	x8, x8, x0
   4cdf0:	str	x8, [x19, x27]
   4cdf4:	b.cs	4ce38 <__gmpn_mu_divappr_q@@Base+0x4bc>  // b.hs, b.nlast
   4cdf8:	ldr	x9, [sp, #72]
   4cdfc:	ldr	x11, [sp, #40]
   4ce00:	mov	x10, xzr
   4ce04:	sub	x8, x28, x26
   4ce08:	add	x9, x9, x22
   4ce0c:	sub	x9, x9, x28
   4ce10:	add	x9, x11, x9, lsl #3
   4ce14:	add	x11, x10, #0x1
   4ce18:	cmp	x11, x8
   4ce1c:	b.ge	4ce68 <__gmpn_mu_divappr_q@@Base+0x4ec>  // b.tcont
   4ce20:	lsl	x10, x10, #3
   4ce24:	ldr	x12, [x9, x10]
   4ce28:	sub	x13, x12, #0x1
   4ce2c:	str	x13, [x9, x10]
   4ce30:	mov	x10, x11
   4ce34:	cbz	x12, 4ce14 <__gmpn_mu_divappr_q@@Base+0x498>
   4ce38:	mov	x8, xzr
   4ce3c:	b	4ce6c <__gmpn_mu_divappr_q@@Base+0x4f0>
   4ce40:	ldur	x26, [x29, #-8]
   4ce44:	mov	x1, x20
   4ce48:	mov	x2, x19
   4ce4c:	mov	x3, x25
   4ce50:	mov	x0, x26
   4ce54:	mov	x4, x22
   4ce58:	mov	x21, x19
   4ce5c:	bl	ccd0 <__gmpn_mul@plt>
   4ce60:	mov	x10, x26
   4ce64:	b	4cee0 <__gmpn_mu_divappr_q@@Base+0x564>
   4ce68:	mov	w8, #0x1                   	// #1
   4ce6c:	ldr	x11, [sp, #72]
   4ce70:	ldp	x10, x12, [sp, #48]
   4ce74:	sub	x9, x28, x21
   4ce78:	add	x11, x11, x28
   4ce7c:	add	x10, x10, x28
   4ce80:	sub	x11, x11, x22
   4ce84:	add	x10, x12, x10, lsl #3
   4ce88:	add	x11, x12, x11, lsl #3
   4ce8c:	subs	x9, x9, #0x1
   4ce90:	b.lt	4ceac <__gmpn_mu_divappr_q@@Base+0x530>  // b.tstop
   4ce94:	ldr	x12, [x11], #-8
   4ce98:	ldr	x13, [x10], #-8
   4ce9c:	cmp	x12, x13
   4cea0:	b.eq	4ce8c <__gmpn_mu_divappr_q@@Base+0x510>  // b.none
   4cea4:	cset	w9, ls  // ls = plast
   4cea8:	b	4ceb0 <__gmpn_mu_divappr_q@@Base+0x534>
   4ceac:	mov	x9, xzr
   4ceb0:	ldur	x10, [x29, #-8]
   4ceb4:	subs	x8, x9, x8
   4ceb8:	b.cc	4d114 <__gmpn_mu_divappr_q@@Base+0x798>  // b.lo, b.ul, b.last
   4cebc:	ldr	x9, [x10]
   4cec0:	adds	x8, x9, x8
   4cec4:	str	x8, [x10]
   4cec8:	b.cc	4cee0 <__gmpn_mu_divappr_q@@Base+0x564>  // b.lo, b.ul, b.last
   4cecc:	ldr	x8, [sp, #32]
   4ced0:	ldr	x9, [x8]
   4ced4:	adds	x9, x9, #0x1
   4ced8:	str	x9, [x8], #8
   4cedc:	b.cs	4ced0 <__gmpn_mu_divappr_q@@Base+0x554>  // b.hs, b.nlast
   4cee0:	subs	x8, x21, x22
   4cee4:	ldr	x8, [x23, x8, lsl #3]
   4cee8:	ldr	x9, [x10, x21, lsl #3]
   4ceec:	ldp	x11, x1, [x29, #-32]
   4cef0:	subs	x26, x21, x22
   4cef4:	sub	x28, x8, x9
   4cef8:	sub	x1, x1, x11
   4cefc:	stur	x1, [x29, #-24]
   4cf00:	b.ne	4cf1c <__gmpn_mu_divappr_q@@Base+0x5a0>  // b.any
   4cf04:	mov	x0, x23
   4cf08:	mov	x2, x10
   4cf0c:	mov	x3, x21
   4cf10:	bl	c2d0 <__gmpn_sub_n@plt>
   4cf14:	mov	x24, x0
   4cf18:	b	4cf5c <__gmpn_mu_divappr_q@@Base+0x5e0>
   4cf1c:	mov	x0, x10
   4cf20:	mov	x2, x10
   4cf24:	mov	x3, x22
   4cf28:	mov	x27, x10
   4cf2c:	bl	c2d0 <__gmpn_sub_n@plt>
   4cf30:	mov	x4, x0
   4cf34:	ldur	x0, [x29, #-48]
   4cf38:	mov	x1, x23
   4cf3c:	mov	x3, x26
   4cf40:	mov	x2, x0
   4cf44:	bl	c760 <__gmpn_sub_nc@plt>
   4cf48:	mov	x24, x0
   4cf4c:	mov	x0, x23
   4cf50:	mov	x1, x27
   4cf54:	mov	x2, x21
   4cf58:	bl	ca50 <__gmpn_copyi@plt>
   4cf5c:	ldur	x19, [x29, #-56]
   4cf60:	subs	x26, x28, x24
   4cf64:	b.eq	4cf9c <__gmpn_mu_divappr_q@@Base+0x620>  // b.none
   4cf68:	mov	x8, x25
   4cf6c:	ldr	x9, [x8]
   4cf70:	adds	x9, x9, #0x1
   4cf74:	str	x9, [x8], #8
   4cf78:	b.cs	4cf6c <__gmpn_mu_divappr_q@@Base+0x5f0>  // b.hs, b.nlast
   4cf7c:	mov	x0, x23
   4cf80:	mov	x1, x23
   4cf84:	mov	x2, x20
   4cf88:	mov	x3, x21
   4cf8c:	bl	c2d0 <__gmpn_sub_n@plt>
   4cf90:	subs	x26, x26, x0
   4cf94:	b.ne	4cf68 <__gmpn_mu_divappr_q@@Base+0x5ec>  // b.any
   4cf98:	mov	x24, x0
   4cf9c:	mov	x8, x21
   4cfa0:	subs	x9, x8, #0x1
   4cfa4:	b.lt	4cfc8 <__gmpn_mu_divappr_q@@Base+0x64c>  // b.tstop
   4cfa8:	lsl	x8, x8, #3
   4cfac:	ldr	x10, [x19, x8]
   4cfb0:	add	x8, x20, x8
   4cfb4:	ldur	x8, [x8, #-8]
   4cfb8:	cmp	x10, x8
   4cfbc:	mov	x8, x9
   4cfc0:	b.eq	4cfa0 <__gmpn_mu_divappr_q@@Base+0x624>  // b.none
   4cfc4:	b.ls	4cd08 <__gmpn_mu_divappr_q@@Base+0x38c>  // b.plast
   4cfc8:	mov	x8, x25
   4cfcc:	ldr	x9, [x8]
   4cfd0:	adds	x9, x9, #0x1
   4cfd4:	str	x9, [x8], #8
   4cfd8:	b.cs	4cfcc <__gmpn_mu_divappr_q@@Base+0x650>  // b.hs, b.nlast
   4cfdc:	mov	x0, x23
   4cfe0:	mov	x1, x23
   4cfe4:	mov	x2, x20
   4cfe8:	mov	x3, x21
   4cfec:	bl	c2d0 <__gmpn_sub_n@plt>
   4cff0:	mov	x24, x0
   4cff4:	b	4cd08 <__gmpn_mu_divappr_q@@Base+0x38c>
   4cff8:	ldr	x8, [x25]
   4cffc:	ldr	x19, [sp, #24]
   4d000:	add	x9, x8, #0x3
   4d004:	cmn	x8, #0x3
   4d008:	str	x9, [x25]
   4d00c:	b.cc	4d040 <__gmpn_mu_divappr_q@@Base+0x6c4>  // b.lo, b.ul, b.last
   4d010:	ldr	x11, [sp, #64]
   4d014:	mov	w8, #0x1                   	// #1
   4d018:	cmp	x8, x11
   4d01c:	b.ge	4d04c <__gmpn_mu_divappr_q@@Base+0x6d0>  // b.tcont
   4d020:	lsl	x9, x8, #3
   4d024:	ldr	x10, [x25, x9]
   4d028:	add	x8, x8, #0x1
   4d02c:	adds	x10, x10, #0x1
   4d030:	str	x10, [x25, x9]
   4d034:	b.cs	4d018 <__gmpn_mu_divappr_q@@Base+0x69c>  // b.hs, b.nlast
   4d038:	mov	x8, xzr
   4d03c:	b	4d050 <__gmpn_mu_divappr_q@@Base+0x6d4>
   4d040:	ldr	x11, [sp, #64]
   4d044:	mov	x8, xzr
   4d048:	b	4d050 <__gmpn_mu_divappr_q@@Base+0x6d4>
   4d04c:	mov	x8, #0xffffffffffffffff    	// #-1
   4d050:	ldr	w9, [sp, #20]
   4d054:	cmp	x24, x8
   4d058:	cset	w8, eq  // eq = none
   4d05c:	orr	w8, w9, w8
   4d060:	csinc	x9, x19, xzr, eq  // eq = none
   4d064:	cmp	w8, #0x0
   4d068:	csel	x0, x9, x19, ne  // ne = any
   4d06c:	tbnz	w8, #0, 4d08c <__gmpn_mu_divappr_q@@Base+0x710>
   4d070:	cmp	x11, #0x1
   4d074:	b.lt	4d08c <__gmpn_mu_divappr_q@@Base+0x710>  // b.tstop
   4d078:	ldr	x2, [sp, #8]
   4d07c:	mov	w1, #0xff                  	// #255
   4d080:	mov	x0, x25
   4d084:	bl	c5f0 <memset@plt>
   4d088:	mov	x0, x19
   4d08c:	ldp	x20, x19, [sp, #224]
   4d090:	ldp	x22, x21, [sp, #208]
   4d094:	ldp	x24, x23, [sp, #192]
   4d098:	ldp	x26, x25, [sp, #176]
   4d09c:	ldp	x28, x27, [sp, #160]
   4d0a0:	ldp	x29, x30, [sp, #144]
   4d0a4:	add	sp, sp, #0xf0
   4d0a8:	ret
   4d0ac:	cbz	x28, 4cc10 <__gmpn_mu_divappr_q@@Base+0x294>
   4d0b0:	lsl	x2, x28, #3
   4d0b4:	mov	x0, x19
   4d0b8:	mov	w1, wzr
   4d0bc:	bl	c5f0 <memset@plt>
   4d0c0:	b	4cc10 <__gmpn_mu_divappr_q@@Base+0x294>
   4d0c4:	and	x10, x28, #0xfffffffffffffffc
   4d0c8:	add	x11, x9, #0x10
   4d0cc:	orr	x9, x10, #0x1
   4d0d0:	add	x12, x12, #0x20
   4d0d4:	mov	x13, x10
   4d0d8:	ldp	q0, q1, [x11, #-16]
   4d0dc:	add	x11, x11, #0x20
   4d0e0:	subs	x13, x13, #0x4
   4d0e4:	stp	q0, q1, [x12, #-16]
   4d0e8:	add	x12, x12, #0x20
   4d0ec:	b.ne	4d0d8 <__gmpn_mu_divappr_q@@Base+0x75c>  // b.any
   4d0f0:	cmp	x28, x10
   4d0f4:	b.eq	4cbe4 <__gmpn_mu_divappr_q@@Base+0x268>  // b.none
   4d0f8:	b	4cbc0 <__gmpn_mu_divappr_q@@Base+0x244>
   4d0fc:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4d100:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4d104:	add	x0, x0, #0xb80
   4d108:	add	x2, x2, #0xb51
   4d10c:	mov	w1, #0xd0                  	// #208
   4d110:	bl	c6c0 <__gmp_assert_fail@plt>
   4d114:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   4d118:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   4d11c:	add	x0, x0, #0xb80
   4d120:	add	x2, x2, #0xb77
   4d124:	mov	w1, #0xe6                  	// #230
   4d128:	bl	c6c0 <__gmp_assert_fail@plt>

000000000004d12c <__gmpn_mu_divappr_q_itch@@Base>:
   4d12c:	stp	x29, x30, [sp, #-32]!
   4d130:	stp	x20, x19, [sp, #16]
   4d134:	sub	x20, x0, x1
   4d138:	add	x8, x20, #0x1
   4d13c:	cmp	x8, x1
   4d140:	csinc	x19, x1, x20, ge  // ge = tcont
   4d144:	mov	x29, sp
   4d148:	cbz	w2, 4d160 <__gmpn_mu_divappr_q_itch@@Base+0x34>
   4d14c:	cmp	x19, x20
   4d150:	csel	x8, x19, x20, lt  // lt = tstop
   4d154:	sub	x8, x8, #0x1
   4d158:	sxtw	x9, w2
   4d15c:	b	4d174 <__gmpn_mu_divappr_q_itch@@Base+0x48>
   4d160:	cmp	x20, x19
   4d164:	b.le	4d1c4 <__gmpn_mu_divappr_q_itch@@Base+0x98>
   4d168:	sub	x8, x20, #0x1
   4d16c:	sdiv	x9, x8, x19
   4d170:	add	x9, x9, #0x1
   4d174:	sdiv	x8, x8, x9
   4d178:	add	x20, x8, #0x1
   4d17c:	add	x0, x19, #0x1
   4d180:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4d184:	asr	x8, x0, #1
   4d188:	cmp	x8, x20
   4d18c:	csel	x10, x0, x8, lt  // lt = tstop
   4d190:	cmp	x8, x19
   4d194:	csel	x8, x10, xzr, lt  // lt = tstop
   4d198:	add	x10, x19, x0, lsl #1
   4d19c:	add	x9, x20, x20, lsl #1
   4d1a0:	add	x8, x10, x8
   4d1a4:	add	x9, x9, #0x4
   4d1a8:	add	x8, x8, #0x4
   4d1ac:	cmp	x8, x9
   4d1b0:	csel	x8, x8, x9, gt
   4d1b4:	add	x0, x8, x20
   4d1b8:	ldp	x20, x19, [sp, #16]
   4d1bc:	ldp	x29, x30, [sp], #32
   4d1c0:	ret
   4d1c4:	add	x8, x20, x20, lsl #1
   4d1c8:	cmp	x8, x19
   4d1cc:	b.le	4d17c <__gmpn_mu_divappr_q_itch@@Base+0x50>
   4d1d0:	sub	x8, x20, #0x1
   4d1d4:	cmp	x8, #0x0
   4d1d8:	csel	x8, x20, x8, lt  // lt = tstop
   4d1dc:	asr	x8, x8, #1
   4d1e0:	b	4d178 <__gmpn_mu_divappr_q_itch@@Base+0x4c>

000000000004d1e4 <__gmpn_mu_div_q@@Base>:
   4d1e4:	sub	sp, sp, #0x80
   4d1e8:	stp	x22, x21, [sp, #96]
   4d1ec:	sub	x21, x2, x4
   4d1f0:	stp	x29, x30, [sp, #32]
   4d1f4:	add	x29, sp, #0x20
   4d1f8:	add	x22, x21, #0x1
   4d1fc:	stp	x28, x27, [sp, #48]
   4d200:	stp	x26, x25, [sp, #64]
   4d204:	mov	x26, x1
   4d208:	mov	x28, x0
   4d20c:	lsl	x1, x22, #3
   4d210:	sub	x0, x29, #0x8
   4d214:	stp	x24, x23, [sp, #80]
   4d218:	stp	x20, x19, [sp, #112]
   4d21c:	mov	x19, x5
   4d220:	mov	x24, x4
   4d224:	mov	x27, x3
   4d228:	mov	x23, x2
   4d22c:	stur	xzr, [x29, #-8]
   4d230:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4d234:	cmp	x21, x24
   4d238:	mov	x25, x0
   4d23c:	str	x0, [sp, #16]
   4d240:	b.ge	4d298 <__gmpn_mu_div_q@@Base+0xb4>  // b.tcont
   4d244:	lsl	x9, x21, #1
   4d248:	mov	x10, #0xfffffffffffffffe    	// #-2
   4d24c:	add	x8, x26, x23, lsl #3
   4d250:	add	x11, x27, x24, lsl #3
   4d254:	mvn	x12, x21
   4d258:	add	x2, x9, #0x2
   4d25c:	sub	x9, x10, x9
   4d260:	add	x1, x8, x9, lsl #3
   4d264:	add	x3, x11, x12, lsl #3
   4d268:	mov	x0, x25
   4d26c:	mov	x4, x22
   4d270:	mov	x5, x19
   4d274:	bl	c710 <__gmpn_mu_divappr_q@plt>
   4d278:	ldr	x8, [x25]
   4d27c:	mov	x20, x0
   4d280:	cmp	x8, #0x7
   4d284:	b.cc	4d324 <__gmpn_mu_div_q@@Base+0x140>  // b.lo, b.ul, b.last
   4d288:	ldr	x8, [sp, #16]
   4d28c:	mov	x0, x28
   4d290:	add	x1, x8, #0x8
   4d294:	b	4d4e4 <__gmpn_mu_div_q@@Base+0x300>
   4d298:	add	x22, x23, #0x1
   4d29c:	lsl	x1, x22, #3
   4d2a0:	sub	x0, x29, #0x8
   4d2a4:	str	x28, [sp, #8]
   4d2a8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4d2ac:	add	x20, x0, #0x8
   4d2b0:	mov	x28, x0
   4d2b4:	mov	x0, x20
   4d2b8:	mov	x1, x26
   4d2bc:	mov	x2, x23
   4d2c0:	bl	ca50 <__gmpn_copyi@plt>
   4d2c4:	lsl	x8, x23, #3
   4d2c8:	add	x9, x20, x8
   4d2cc:	sub	x0, x9, x24, lsl #3
   4d2d0:	add	x8, x28, x8
   4d2d4:	mov	x9, x24
   4d2d8:	str	xzr, [x28]
   4d2dc:	subs	x10, x9, #0x1
   4d2e0:	b.lt	4d304 <__gmpn_mu_div_q@@Base+0x120>  // b.tstop
   4d2e4:	add	x9, x27, x9, lsl #3
   4d2e8:	ldr	x11, [x8], #-8
   4d2ec:	ldur	x12, [x9, #-8]
   4d2f0:	mov	x9, x10
   4d2f4:	cmp	x11, x12
   4d2f8:	b.eq	4d2dc <__gmpn_mu_div_q@@Base+0xf8>  // b.none
   4d2fc:	cmp	x11, x12
   4d300:	b.ls	4d3d8 <__gmpn_mu_div_q@@Base+0x1f4>  // b.plast
   4d304:	mov	x1, x0
   4d308:	mov	x2, x27
   4d30c:	mov	x3, x24
   4d310:	bl	c2d0 <__gmpn_sub_n@plt>
   4d314:	mov	w8, #0x1                   	// #1
   4d318:	mov	w20, #0x1                   	// #1
   4d31c:	str	x8, [sp]
   4d320:	b	4d3e0 <__gmpn_mu_div_q@@Base+0x1fc>
   4d324:	lsl	x1, x23, #3
   4d328:	sub	x0, x29, #0x8
   4d32c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4d330:	ldr	x25, [sp, #16]
   4d334:	mov	x1, x27
   4d338:	mov	x2, x24
   4d33c:	mov	x4, x21
   4d340:	add	x19, x25, #0x8
   4d344:	mov	x3, x19
   4d348:	mov	x22, x0
   4d34c:	bl	ccd0 <__gmpn_mul@plt>
   4d350:	cbz	x20, 4d36c <__gmpn_mu_div_q@@Base+0x188>
   4d354:	add	x0, x22, x21, lsl #3
   4d358:	mov	x1, x0
   4d35c:	mov	x2, x27
   4d360:	mov	x3, x24
   4d364:	bl	ca70 <__gmpn_add_n@plt>
   4d368:	cbnz	x0, 4d39c <__gmpn_mu_div_q@@Base+0x1b8>
   4d36c:	sub	x8, x26, #0x8
   4d370:	sub	x9, x22, #0x8
   4d374:	mov	x10, x23
   4d378:	subs	x11, x10, #0x1
   4d37c:	b.lt	4d4dc <__gmpn_mu_div_q@@Base+0x2f8>  // b.tstop
   4d380:	lsl	x10, x10, #3
   4d384:	ldr	x12, [x9, x10]
   4d388:	ldr	x10, [x8, x10]
   4d38c:	cmp	x12, x10
   4d390:	mov	x10, x11
   4d394:	b.eq	4d378 <__gmpn_mu_div_q@@Base+0x194>  // b.none
   4d398:	b.ls	4d4dc <__gmpn_mu_div_q@@Base+0x2f8>  // b.plast
   4d39c:	ldr	x8, [x19]
   4d3a0:	sub	x9, x8, #0x1
   4d3a4:	str	x9, [x28]
   4d3a8:	cbz	x8, 4d518 <__gmpn_mu_div_q@@Base+0x334>
   4d3ac:	cmp	x21, #0x2
   4d3b0:	mov	x8, xzr
   4d3b4:	b.lt	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.tstop
   4d3b8:	cmp	x19, x28
   4d3bc:	b.eq	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.none
   4d3c0:	mvn	x8, x24
   4d3c4:	add	x8, x8, x23
   4d3c8:	add	x0, x28, #0x8
   4d3cc:	add	x1, x25, #0x10
   4d3d0:	lsl	x2, x8, #3
   4d3d4:	b	4d5e0 <__gmpn_mu_div_q@@Base+0x3fc>
   4d3d8:	str	xzr, [sp]
   4d3dc:	mov	w20, wzr
   4d3e0:	mov	x0, x25
   4d3e4:	mov	x1, x28
   4d3e8:	mov	x2, x22
   4d3ec:	mov	x3, x27
   4d3f0:	mov	x4, x24
   4d3f4:	mov	x5, x19
   4d3f8:	bl	c710 <__gmpn_mu_divappr_q@plt>
   4d3fc:	cbz	x0, 4d404 <__gmpn_mu_div_q@@Base+0x220>
   4d400:	tbz	x21, #63, 4d604 <__gmpn_mu_div_q@@Base+0x420>
   4d404:	ldr	x8, [x25], #8
   4d408:	cmp	x8, #0x5
   4d40c:	b.cc	4d430 <__gmpn_mu_div_q@@Base+0x24c>  // b.lo, b.ul, b.last
   4d410:	ldr	x0, [sp, #8]
   4d414:	mov	x1, x25
   4d418:	mov	x2, x21
   4d41c:	bl	ca50 <__gmpn_copyi@plt>
   4d420:	ldr	x20, [sp]
   4d424:	ldur	x0, [x29, #-8]
   4d428:	cbz	x0, 4d4f4 <__gmpn_mu_div_q@@Base+0x310>
   4d42c:	b	4d5fc <__gmpn_mu_div_q@@Base+0x418>
   4d430:	mov	x0, x28
   4d434:	mov	x1, x25
   4d438:	mov	x2, x21
   4d43c:	mov	x3, x27
   4d440:	mov	x4, x24
   4d444:	bl	ccd0 <__gmpn_mul@plt>
   4d448:	cbz	w20, 4d464 <__gmpn_mu_div_q@@Base+0x280>
   4d44c:	add	x0, x28, x21, lsl #3
   4d450:	mov	x1, x0
   4d454:	mov	x2, x27
   4d458:	mov	x3, x24
   4d45c:	bl	ca70 <__gmpn_add_n@plt>
   4d460:	cbnz	x0, 4d494 <__gmpn_mu_div_q@@Base+0x2b0>
   4d464:	sub	x8, x26, #0x8
   4d468:	sub	x9, x28, #0x8
   4d46c:	mov	x10, x23
   4d470:	subs	x11, x10, #0x1
   4d474:	b.lt	4d410 <__gmpn_mu_div_q@@Base+0x22c>  // b.tstop
   4d478:	lsl	x10, x10, #3
   4d47c:	ldr	x12, [x9, x10]
   4d480:	ldr	x10, [x8, x10]
   4d484:	cmp	x12, x10
   4d488:	mov	x10, x11
   4d48c:	b.eq	4d470 <__gmpn_mu_div_q@@Base+0x28c>  // b.none
   4d490:	b.ls	4d410 <__gmpn_mu_div_q@@Base+0x22c>  // b.plast
   4d494:	ldr	x8, [x25]
   4d498:	ldr	x15, [sp, #8]
   4d49c:	sub	x9, x8, #0x1
   4d4a0:	str	x9, [x15]
   4d4a4:	cbz	x8, 4d574 <__gmpn_mu_div_q@@Base+0x390>
   4d4a8:	cmp	x21, #0x2
   4d4ac:	mov	x8, xzr
   4d4b0:	b.lt	4d5ec <__gmpn_mu_div_q@@Base+0x408>  // b.tstop
   4d4b4:	ldr	x20, [sp]
   4d4b8:	cmp	x25, x15
   4d4bc:	b.eq	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.none
   4d4c0:	ldr	x8, [sp, #16]
   4d4c4:	add	x0, x15, #0x8
   4d4c8:	add	x1, x8, #0x10
   4d4cc:	mvn	x8, x24
   4d4d0:	add	x8, x8, x23
   4d4d4:	lsl	x2, x8, #3
   4d4d8:	b	4d5e0 <__gmpn_mu_div_q@@Base+0x3fc>
   4d4dc:	mov	x0, x28
   4d4e0:	mov	x1, x19
   4d4e4:	mov	x2, x21
   4d4e8:	bl	ca50 <__gmpn_copyi@plt>
   4d4ec:	ldur	x0, [x29, #-8]
   4d4f0:	cbnz	x0, 4d5fc <__gmpn_mu_div_q@@Base+0x418>
   4d4f4:	mov	x0, x20
   4d4f8:	ldp	x20, x19, [sp, #112]
   4d4fc:	ldp	x22, x21, [sp, #96]
   4d500:	ldp	x24, x23, [sp, #80]
   4d504:	ldp	x26, x25, [sp, #64]
   4d508:	ldp	x28, x27, [sp, #48]
   4d50c:	ldp	x29, x30, [sp, #32]
   4d510:	add	sp, sp, #0x80
   4d514:	ret
   4d518:	mov	x9, xzr
   4d51c:	add	x11, x25, #0x10
   4d520:	mov	w8, #0x1                   	// #1
   4d524:	mov	w10, #0x1                   	// #1
   4d528:	cmp	x10, x21
   4d52c:	b.ge	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.tcont
   4d530:	ldr	x12, [x11, x9]
   4d534:	add	x13, x28, x9
   4d538:	add	x10, x10, #0x1
   4d53c:	add	x9, x9, #0x8
   4d540:	sub	x14, x12, #0x1
   4d544:	str	x14, [x13, #8]
   4d548:	cbz	x12, 4d528 <__gmpn_mu_div_q@@Base+0x344>
   4d54c:	cmp	x19, x28
   4d550:	mov	x8, xzr
   4d554:	b.eq	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.none
   4d558:	cmp	x10, x21
   4d55c:	b.ge	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.tcont
   4d560:	add	x8, x28, x9
   4d564:	add	x9, x25, x9
   4d568:	sub	x10, x21, x10
   4d56c:	add	x0, x8, #0x8
   4d570:	b	4d5d8 <__gmpn_mu_div_q@@Base+0x3f4>
   4d574:	ldr	x8, [sp, #16]
   4d578:	mov	x9, xzr
   4d57c:	mov	w10, #0x1                   	// #1
   4d580:	add	x11, x8, #0x10
   4d584:	mov	w8, #0x1                   	// #1
   4d588:	cmp	x10, x21
   4d58c:	b.ge	4d5ec <__gmpn_mu_div_q@@Base+0x408>  // b.tcont
   4d590:	ldr	x12, [x11, x9]
   4d594:	add	x13, x15, x9
   4d598:	add	x10, x10, #0x1
   4d59c:	add	x9, x9, #0x8
   4d5a0:	sub	x14, x12, #0x1
   4d5a4:	str	x14, [x13, #8]
   4d5a8:	cbz	x12, 4d588 <__gmpn_mu_div_q@@Base+0x3a4>
   4d5ac:	cmp	x25, x15
   4d5b0:	mov	x8, xzr
   4d5b4:	b.eq	4d5ec <__gmpn_mu_div_q@@Base+0x408>  // b.none
   4d5b8:	ldr	x20, [sp]
   4d5bc:	cmp	x10, x21
   4d5c0:	b.ge	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>  // b.tcont
   4d5c4:	ldr	x11, [sp, #16]
   4d5c8:	add	x8, x15, x9
   4d5cc:	sub	x10, x21, x10
   4d5d0:	add	x0, x8, #0x8
   4d5d4:	add	x9, x11, x9
   4d5d8:	add	x1, x9, #0x10
   4d5dc:	lsl	x2, x10, #3
   4d5e0:	bl	bed0 <memcpy@plt>
   4d5e4:	mov	x8, xzr
   4d5e8:	b	4d5f0 <__gmpn_mu_div_q@@Base+0x40c>
   4d5ec:	ldr	x20, [sp]
   4d5f0:	sub	x20, x20, x8
   4d5f4:	ldur	x0, [x29, #-8]
   4d5f8:	cbz	x0, 4d4f4 <__gmpn_mu_div_q@@Base+0x310>
   4d5fc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4d600:	b	4d4f4 <__gmpn_mu_div_q@@Base+0x310>
   4d604:	sub	x8, x22, x24
   4d608:	lsl	x2, x8, #3
   4d60c:	mov	w1, #0xff                  	// #255
   4d610:	mov	x0, x25
   4d614:	bl	c5f0 <memset@plt>
   4d618:	b	4d404 <__gmpn_mu_div_q@@Base+0x220>

000000000004d61c <__gmpn_mu_div_q_itch@@Base>:
   4d61c:	sub	x8, x0, x1
   4d620:	lsl	x9, x8, #1
   4d624:	cmp	x8, x1
   4d628:	add	x9, x9, #0x2
   4d62c:	csinc	x1, x1, x8, ge  // ge = tcont
   4d630:	csinc	x0, x9, x0, lt  // lt = tstop
   4d634:	b	c0e0 <__gmpn_mu_divappr_q_itch@plt>

000000000004d638 <__gmpn_bdiv_q_1@@Base>:
   4d638:	rbit	x6, x3
   4d63c:	clz	x5, x6
   4d640:	lsr	x3, x3, x5
   4d644:	adrp	x7, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4d648:	ubfx	x6, x3, #1, #7
   4d64c:	ldr	x7, [x7, #3952]
   4d650:	ldrb	w6, [x7, x6]
   4d654:	ubfiz	x7, x6, #1, #8
   4d658:	umull	x6, w6, w6
   4d65c:	msub	x6, x6, x3, x7
   4d660:	lsl	x7, x6, #1
   4d664:	mul	x6, x6, x6
   4d668:	msub	x6, x6, x3, x7
   4d66c:	lsl	x7, x6, #1
   4d670:	mul	x6, x6, x6
   4d674:	msub	x4, x6, x3, x7
   4d678:	b	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4d67c:	nop

000000000004d680 <__gmpn_pi1_bdiv_q_1@@Base>:
   4d680:	sub	x2, x2, #0x1
   4d684:	subs	x6, x6, x6
   4d688:	ldr	x9, [x1], #8
   4d68c:	cbz	x5, 4d6d4 <__gmpn_pi1_bdiv_q_1@@Base+0x54>
   4d690:	lsr	x12, x9, x5
   4d694:	cbz	x2, 4d6c4 <__gmpn_pi1_bdiv_q_1@@Base+0x44>
   4d698:	neg	x8, x5
   4d69c:	ldr	x9, [x1], #8
   4d6a0:	lsl	x7, x9, x8
   4d6a4:	orr	x7, x7, x12
   4d6a8:	sbcs	x6, x7, x6
   4d6ac:	mul	x7, x6, x4
   4d6b0:	str	x7, [x0], #8
   4d6b4:	lsr	x12, x9, x5
   4d6b8:	umulh	x6, x7, x3
   4d6bc:	sub	x2, x2, #0x1
   4d6c0:	cbnz	x2, 4d69c <__gmpn_pi1_bdiv_q_1@@Base+0x1c>
   4d6c4:	sbcs	x6, x12, x6
   4d6c8:	mul	x6, x6, x4
   4d6cc:	str	x6, [x0]
   4d6d0:	ret
   4d6d4:	mul	x5, x9, x4
   4d6d8:	str	x5, [x0], #8
   4d6dc:	cbz	x2, 4d6fc <__gmpn_pi1_bdiv_q_1@@Base+0x7c>
   4d6e0:	ldr	x9, [x1], #8
   4d6e4:	umulh	x5, x5, x3
   4d6e8:	sbcs	x5, x9, x5
   4d6ec:	mul	x5, x5, x4
   4d6f0:	str	x5, [x0], #8
   4d6f4:	sub	x2, x2, #0x1
   4d6f8:	cbnz	x2, 4d6e0 <__gmpn_pi1_bdiv_q_1@@Base+0x60>
   4d6fc:	ret

000000000004d700 <__gmpn_sbpi1_bdiv_q@@Base>:
   4d700:	stp	x29, x30, [sp, #-96]!
   4d704:	stp	x26, x25, [sp, #32]
   4d708:	stp	x24, x23, [sp, #48]
   4d70c:	stp	x22, x21, [sp, #64]
   4d710:	stp	x20, x19, [sp, #80]
   4d714:	mov	x20, x5
   4d718:	mov	x23, x4
   4d71c:	mov	x21, x3
   4d720:	mov	x22, x1
   4d724:	subs	x26, x2, x4
   4d728:	mov	x19, x0
   4d72c:	stp	x28, x27, [sp, #16]
   4d730:	mov	x29, sp
   4d734:	b.le	4d7d0 <__gmpn_sbpi1_bdiv_q@@Base+0xd0>
   4d738:	ldr	x8, [x22]
   4d73c:	mvn	x9, x23
   4d740:	add	x25, x9, x2
   4d744:	mov	x0, x22
   4d748:	mul	x24, x8, x20
   4d74c:	mov	x1, x21
   4d750:	mov	x2, x23
   4d754:	mov	x3, x24
   4d758:	bl	d400 <__gmpn_addmul_1@plt>
   4d75c:	cmp	x25, #0x1
   4d760:	lsl	x25, x23, #3
   4d764:	mov	x27, xzr
   4d768:	b.lt	4d7b8 <__gmpn_sbpi1_bdiv_q@@Base+0xb8>  // b.tstop
   4d76c:	mov	w28, #0x2                   	// #2
   4d770:	str	x24, [x19], #8
   4d774:	ldr	x8, [x22, x25]
   4d778:	adds	x9, x0, x27
   4d77c:	cset	w10, cs  // cs = hs, nlast
   4d780:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   4d784:	adds	x8, x8, x9
   4d788:	str	x8, [x22, x25]
   4d78c:	ldr	x8, [x22, #8]!
   4d790:	mov	x1, x21
   4d794:	mov	x2, x23
   4d798:	csel	x27, x10, x11, cc  // cc = lo, ul, last
   4d79c:	mul	x24, x8, x20
   4d7a0:	mov	x0, x22
   4d7a4:	mov	x3, x24
   4d7a8:	bl	d400 <__gmpn_addmul_1@plt>
   4d7ac:	sub	x26, x26, #0x1
   4d7b0:	cmp	x26, #0x1
   4d7b4:	b.gt	4d770 <__gmpn_sbpi1_bdiv_q@@Base+0x70>
   4d7b8:	str	x24, [x19], #8
   4d7bc:	ldr	x8, [x22, x25]
   4d7c0:	add	x9, x0, x27
   4d7c4:	add	x8, x9, x8
   4d7c8:	str	x8, [x22, x25]
   4d7cc:	add	x22, x22, #0x8
   4d7d0:	ldr	x8, [x22]
   4d7d4:	cmp	x23, #0x2
   4d7d8:	mul	x24, x8, x20
   4d7dc:	b.lt	4d80c <__gmpn_sbpi1_bdiv_q@@Base+0x10c>  // b.tstop
   4d7e0:	mov	x0, x22
   4d7e4:	mov	x1, x21
   4d7e8:	mov	x2, x23
   4d7ec:	mov	x3, x24
   4d7f0:	bl	d400 <__gmpn_addmul_1@plt>
   4d7f4:	str	x24, [x19], #8
   4d7f8:	ldr	x8, [x22, #8]!
   4d7fc:	cmp	x23, #0x2
   4d800:	sub	x23, x23, #0x1
   4d804:	mul	x24, x8, x20
   4d808:	b.gt	4d7e0 <__gmpn_sbpi1_bdiv_q@@Base+0xe0>
   4d80c:	str	x24, [x19]
   4d810:	ldp	x20, x19, [sp, #80]
   4d814:	ldp	x22, x21, [sp, #64]
   4d818:	ldp	x24, x23, [sp, #48]
   4d81c:	ldp	x26, x25, [sp, #32]
   4d820:	ldp	x28, x27, [sp, #16]
   4d824:	ldp	x29, x30, [sp], #96
   4d828:	ret

000000000004d82c <__gmpn_sbpi1_bdiv_qr@@Base>:
   4d82c:	stp	x29, x30, [sp, #-96]!
   4d830:	cmp	x2, x4
   4d834:	stp	x28, x27, [sp, #16]
   4d838:	stp	x26, x25, [sp, #32]
   4d83c:	stp	x24, x23, [sp, #48]
   4d840:	stp	x22, x21, [sp, #64]
   4d844:	stp	x20, x19, [sp, #80]
   4d848:	mov	x29, sp
   4d84c:	b.eq	4d8c4 <__gmpn_sbpi1_bdiv_qr@@Base+0x98>  // b.none
   4d850:	mov	x19, x5
   4d854:	mov	x20, x4
   4d858:	mov	x21, x3
   4d85c:	mov	x22, x2
   4d860:	mov	x23, x1
   4d864:	mov	x24, x0
   4d868:	mov	x25, xzr
   4d86c:	lsl	x27, x4, #3
   4d870:	mov	w28, #0x2                   	// #2
   4d874:	ldr	x8, [x23]
   4d878:	mov	x0, x23
   4d87c:	mov	x1, x21
   4d880:	mov	x2, x20
   4d884:	mul	x26, x8, x19
   4d888:	mov	x3, x26
   4d88c:	bl	d400 <__gmpn_addmul_1@plt>
   4d890:	str	x26, [x24], #8
   4d894:	ldr	x9, [x23, x27]
   4d898:	adds	x8, x0, x25
   4d89c:	sub	x22, x22, #0x1
   4d8a0:	cset	w10, cs  // cs = hs, nlast
   4d8a4:	csinc	x11, x28, xzr, cs  // cs = hs, nlast
   4d8a8:	adds	x8, x9, x8
   4d8ac:	csel	x25, x10, x11, cc  // cc = lo, ul, last
   4d8b0:	str	x8, [x23, x27]
   4d8b4:	cmp	x20, x22
   4d8b8:	add	x23, x23, #0x8
   4d8bc:	b.ne	4d874 <__gmpn_sbpi1_bdiv_qr@@Base+0x48>  // b.any
   4d8c0:	b	4d8c8 <__gmpn_sbpi1_bdiv_qr@@Base+0x9c>
   4d8c4:	mov	x25, xzr
   4d8c8:	mov	x0, x25
   4d8cc:	ldp	x20, x19, [sp, #80]
   4d8d0:	ldp	x22, x21, [sp, #64]
   4d8d4:	ldp	x24, x23, [sp, #48]
   4d8d8:	ldp	x26, x25, [sp, #32]
   4d8dc:	ldp	x28, x27, [sp, #16]
   4d8e0:	ldp	x29, x30, [sp], #96
   4d8e4:	ret

000000000004d8e8 <__gmpn_sbpi1_bdiv_r@@Base>:
   4d8e8:	stp	x29, x30, [sp, #-80]!
   4d8ec:	cmp	x1, x3
   4d8f0:	stp	x26, x25, [sp, #16]
   4d8f4:	stp	x24, x23, [sp, #32]
   4d8f8:	stp	x22, x21, [sp, #48]
   4d8fc:	stp	x20, x19, [sp, #64]
   4d900:	mov	x29, sp
   4d904:	b.eq	4d970 <__gmpn_sbpi1_bdiv_r@@Base+0x88>  // b.none
   4d908:	mov	x19, x4
   4d90c:	mov	x20, x3
   4d910:	mov	x21, x2
   4d914:	mov	x22, x1
   4d918:	mov	x23, x0
   4d91c:	mov	x24, xzr
   4d920:	lsl	x25, x3, #3
   4d924:	mov	w26, #0x2                   	// #2
   4d928:	ldr	x8, [x23]
   4d92c:	mov	x0, x23
   4d930:	mov	x1, x21
   4d934:	mov	x2, x20
   4d938:	mul	x3, x8, x19
   4d93c:	bl	d400 <__gmpn_addmul_1@plt>
   4d940:	ldr	x9, [x23, x25]
   4d944:	adds	x8, x0, x24
   4d948:	sub	x22, x22, #0x1
   4d94c:	cset	w10, cs  // cs = hs, nlast
   4d950:	csinc	x11, x26, xzr, cs  // cs = hs, nlast
   4d954:	adds	x8, x8, x9
   4d958:	csel	x24, x10, x11, cc  // cc = lo, ul, last
   4d95c:	str	x8, [x23, x25]
   4d960:	cmp	x20, x22
   4d964:	add	x23, x23, #0x8
   4d968:	b.ne	4d928 <__gmpn_sbpi1_bdiv_r@@Base+0x40>  // b.any
   4d96c:	b	4d974 <__gmpn_sbpi1_bdiv_r@@Base+0x8c>
   4d970:	mov	x24, xzr
   4d974:	mov	x0, x24
   4d978:	ldp	x20, x19, [sp, #64]
   4d97c:	ldp	x22, x21, [sp, #48]
   4d980:	ldp	x24, x23, [sp, #32]
   4d984:	ldp	x26, x25, [sp, #16]
   4d988:	ldp	x29, x30, [sp], #80
   4d98c:	ret

000000000004d990 <__gmpn_dcpi1_bdiv_q@@Base>:
   4d990:	stp	x29, x30, [sp, #-96]!
   4d994:	stp	x28, x27, [sp, #16]
   4d998:	stp	x26, x25, [sp, #32]
   4d99c:	stp	x24, x23, [sp, #48]
   4d9a0:	stp	x22, x21, [sp, #64]
   4d9a4:	stp	x20, x19, [sp, #80]
   4d9a8:	mov	x29, sp
   4d9ac:	sub	sp, sp, #0x20
   4d9b0:	lsl	x28, x4, #3
   4d9b4:	add	x9, x28, #0xf
   4d9b8:	mov	x8, sp
   4d9bc:	and	x9, x9, #0xfffffffffffffff0
   4d9c0:	mov	x25, x2
   4d9c4:	mov	x24, x1
   4d9c8:	sub	x21, x8, x9
   4d9cc:	stur	x3, [x29, #-8]
   4d9d0:	mov	sp, x21
   4d9d4:	cmp	x2, x4
   4d9d8:	b.le	4da4c <__gmpn_dcpi1_bdiv_q@@Base+0xbc>
   4d9dc:	lsl	x10, x25, #3
   4d9e0:	sub	x11, x10, x28
   4d9e4:	add	x9, x24, x10
   4d9e8:	add	x11, x11, x21
   4d9ec:	mov	x22, x4
   4d9f0:	neg	x8, x28
   4d9f4:	add	x10, x9, #0x8
   4d9f8:	add	x11, x11, #0x8
   4d9fc:	mov	x26, x25
   4da00:	stur	x5, [x29, #-16]
   4da04:	sub	x26, x26, x22
   4da08:	mov	x19, x10
   4da0c:	mov	x23, x9
   4da10:	mov	x20, x11
   4da14:	add	x10, x10, x8
   4da18:	add	x9, x9, x8
   4da1c:	cmp	x26, x22
   4da20:	add	x11, x11, x8
   4da24:	b.gt	4da04 <__gmpn_dcpi1_bdiv_q@@Base+0x74>
   4da28:	cmp	x26, #0x26
   4da2c:	stur	x0, [x29, #-24]
   4da30:	b.le	4da68 <__gmpn_dcpi1_bdiv_q@@Base+0xd8>
   4da34:	ldp	x4, x2, [x29, #-16]
   4da38:	mov	x1, x24
   4da3c:	mov	x3, x26
   4da40:	mov	x5, x21
   4da44:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4da48:	b	4da7c <__gmpn_dcpi1_bdiv_q@@Base+0xec>
   4da4c:	mov	x1, x24
   4da50:	cmp	x25, #0x5c
   4da54:	b.le	4da94 <__gmpn_dcpi1_bdiv_q@@Base+0x104>
   4da58:	ldur	x2, [x29, #-8]
   4da5c:	mov	x3, x25
   4da60:	mov	x4, x5
   4da64:	b	4dba8 <__gmpn_dcpi1_bdiv_q@@Base+0x218>
   4da68:	ldp	x5, x3, [x29, #-16]
   4da6c:	lsl	x2, x26, #1
   4da70:	mov	x1, x24
   4da74:	mov	x4, x26
   4da78:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4da7c:	mov	x27, x0
   4da80:	cmp	x26, x22
   4da84:	lsl	x11, x26, #3
   4da88:	b.ne	4dac0 <__gmpn_dcpi1_bdiv_q@@Base+0x130>  // b.any
   4da8c:	sub	x20, x25, x26
   4da90:	b	4db7c <__gmpn_dcpi1_bdiv_q@@Base+0x1ec>
   4da94:	ldur	x3, [x29, #-8]
   4da98:	mov	x2, x25
   4da9c:	mov	x4, x25
   4daa0:	mov	sp, x29
   4daa4:	ldp	x20, x19, [sp, #80]
   4daa8:	ldp	x22, x21, [sp, #64]
   4daac:	ldp	x24, x23, [sp, #48]
   4dab0:	ldp	x26, x25, [sp, #32]
   4dab4:	ldp	x28, x27, [sp, #16]
   4dab8:	ldp	x29, x30, [sp], #96
   4dabc:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>
   4dac0:	ldur	x8, [x29, #-8]
   4dac4:	sub	x4, x22, x26
   4dac8:	cmp	x26, x4
   4dacc:	mov	x0, x21
   4dad0:	add	x3, x8, x26, lsl #3
   4dad4:	stur	x11, [x29, #-32]
   4dad8:	b.le	4dae8 <__gmpn_dcpi1_bdiv_q@@Base+0x158>
   4dadc:	ldur	x1, [x29, #-24]
   4dae0:	mov	x2, x26
   4dae4:	b	4daf8 <__gmpn_dcpi1_bdiv_q@@Base+0x168>
   4dae8:	mov	x1, x3
   4daec:	ldur	x3, [x29, #-24]
   4daf0:	mov	x2, x4
   4daf4:	mov	x4, x26
   4daf8:	bl	ccd0 <__gmpn_mul@plt>
   4dafc:	ldur	x11, [x29, #-32]
   4db00:	ldr	x8, [x21, x11]
   4db04:	adds	x8, x8, x27
   4db08:	str	x8, [x21, x11]
   4db0c:	b.cc	4db20 <__gmpn_dcpi1_bdiv_q@@Base+0x190>  // b.lo, b.ul, b.last
   4db10:	ldr	x8, [x20]
   4db14:	adds	x8, x8, #0x1
   4db18:	str	x8, [x20], #8
   4db1c:	b.cs	4db10 <__gmpn_dcpi1_bdiv_q@@Base+0x180>  // b.hs, b.nlast
   4db20:	sub	x20, x25, x26
   4db24:	cbz	x22, 4db78 <__gmpn_dcpi1_bdiv_q@@Base+0x1e8>
   4db28:	add	x25, x24, x26, lsl #3
   4db2c:	mov	x0, x25
   4db30:	mov	x1, x25
   4db34:	mov	x2, x21
   4db38:	mov	x3, x22
   4db3c:	bl	ca70 <__gmpn_add_n@plt>
   4db40:	cbz	x0, 4dc40 <__gmpn_dcpi1_bdiv_q@@Base+0x2b0>
   4db44:	ldp	x10, x26, [x29, #-24]
   4db48:	ldur	x11, [x29, #-32]
   4db4c:	mov	x8, x22
   4db50:	cmp	x8, x20
   4db54:	b.ge	4db6c <__gmpn_dcpi1_bdiv_q@@Base+0x1dc>  // b.tcont
   4db58:	ldr	x9, [x25, x8, lsl #3]
   4db5c:	add	x8, x8, #0x1
   4db60:	adds	x9, x9, #0x1
   4db64:	str	x9, [x23], #8
   4db68:	b.cs	4db50 <__gmpn_dcpi1_bdiv_q@@Base+0x1c0>  // b.hs, b.nlast
   4db6c:	ldur	x25, [x29, #-8]
   4db70:	mov	x27, xzr
   4db74:	b	4db84 <__gmpn_dcpi1_bdiv_q@@Base+0x1f4>
   4db78:	mov	x27, xzr
   4db7c:	ldp	x26, x25, [x29, #-16]
   4db80:	ldur	x10, [x29, #-24]
   4db84:	add	x24, x24, x11
   4db88:	cmp	x20, x22
   4db8c:	add	x23, x10, x11
   4db90:	b.gt	4dbfc <__gmpn_dcpi1_bdiv_q@@Base+0x26c>
   4db94:	mov	x0, x23
   4db98:	mov	x1, x24
   4db9c:	mov	x2, x25
   4dba0:	mov	x3, x22
   4dba4:	mov	x4, x26
   4dba8:	mov	x5, x21
   4dbac:	bl	4dc50 <__gmpn_dcpi1_bdiv_q@@Base+0x2c0>
   4dbb0:	mov	sp, x29
   4dbb4:	ldp	x20, x19, [sp, #80]
   4dbb8:	ldp	x22, x21, [sp, #64]
   4dbbc:	ldp	x24, x23, [sp, #48]
   4dbc0:	ldp	x26, x25, [sp, #32]
   4dbc4:	ldp	x28, x27, [sp, #16]
   4dbc8:	ldp	x29, x30, [sp], #96
   4dbcc:	ret
   4dbd0:	mov	x0, x23
   4dbd4:	mov	x2, x25
   4dbd8:	mov	x3, x22
   4dbdc:	mov	x4, x26
   4dbe0:	mov	x5, x21
   4dbe4:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4dbe8:	mov	x27, x0
   4dbec:	add	x23, x23, x22, lsl #3
   4dbf0:	cmp	x20, x22
   4dbf4:	add	x19, x19, x28
   4dbf8:	b.le	4db94 <__gmpn_dcpi1_bdiv_q@@Base+0x204>
   4dbfc:	mov	x1, x24
   4dc00:	add	x24, x24, x22, lsl #3
   4dc04:	ldr	x8, [x24]
   4dc08:	sub	x20, x20, x22
   4dc0c:	adds	x8, x8, x27
   4dc10:	str	x8, [x24]
   4dc14:	b.cc	4dbd0 <__gmpn_dcpi1_bdiv_q@@Base+0x240>  // b.lo, b.ul, b.last
   4dc18:	mov	x8, xzr
   4dc1c:	add	x9, x8, #0x1
   4dc20:	cmp	x9, x20
   4dc24:	b.ge	4dbd0 <__gmpn_dcpi1_bdiv_q@@Base+0x240>  // b.tcont
   4dc28:	ldr	x10, [x24, x9, lsl #3]
   4dc2c:	adds	x10, x10, #0x1
   4dc30:	str	x10, [x19, x8, lsl #3]
   4dc34:	mov	x8, x9
   4dc38:	b.cs	4dc1c <__gmpn_dcpi1_bdiv_q@@Base+0x28c>  // b.hs, b.nlast
   4dc3c:	b	4dbd0 <__gmpn_dcpi1_bdiv_q@@Base+0x240>
   4dc40:	ldp	x26, x25, [x29, #-16]
   4dc44:	ldp	x11, x10, [x29, #-32]
   4dc48:	mov	x27, xzr
   4dc4c:	b	4db84 <__gmpn_dcpi1_bdiv_q@@Base+0x1f4>
   4dc50:	stp	x29, x30, [sp, #-96]!
   4dc54:	stp	x26, x25, [sp, #32]
   4dc58:	stp	x22, x21, [sp, #64]
   4dc5c:	stp	x20, x19, [sp, #80]
   4dc60:	mov	x19, x4
   4dc64:	mov	x25, x3
   4dc68:	mov	x20, x2
   4dc6c:	mov	x21, x1
   4dc70:	cmp	x3, #0x5d
   4dc74:	mov	x22, x0
   4dc78:	stp	x28, x27, [sp, #16]
   4dc7c:	stp	x24, x23, [sp, #48]
   4dc80:	mov	x29, sp
   4dc84:	b.lt	4dd30 <__gmpn_dcpi1_bdiv_q@@Base+0x3a0>  // b.tstop
   4dc88:	mov	x23, x5
   4dc8c:	b	4dca4 <__gmpn_dcpi1_bdiv_q@@Base+0x314>
   4dc90:	add	x22, x22, x28
   4dc94:	cmp	x24, #0x5c
   4dc98:	add	x21, x21, x28
   4dc9c:	mov	x25, x24
   4dca0:	b.le	4dd34 <__gmpn_dcpi1_bdiv_q@@Base+0x3a4>
   4dca4:	lsr	x26, x25, #1
   4dca8:	mov	x0, x22
   4dcac:	mov	x1, x21
   4dcb0:	mov	x2, x20
   4dcb4:	mov	x3, x26
   4dcb8:	mov	x4, x19
   4dcbc:	mov	x5, x23
   4dcc0:	sub	x24, x25, x26
   4dcc4:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4dcc8:	lsl	x28, x24, #3
   4dccc:	mov	x27, x0
   4dcd0:	add	x2, x20, x28
   4dcd4:	mov	x0, x23
   4dcd8:	mov	x1, x22
   4dcdc:	mov	x3, x26
   4dce0:	bl	cec0 <__gmpn_mullo_n@plt>
   4dce4:	add	x0, x21, x28
   4dce8:	mov	x1, x0
   4dcec:	mov	x2, x23
   4dcf0:	mov	x3, x26
   4dcf4:	bl	ca70 <__gmpn_add_n@plt>
   4dcf8:	cmp	x26, x24
   4dcfc:	lsl	x28, x26, #3
   4dd00:	b.ge	4dc90 <__gmpn_dcpi1_bdiv_q@@Base+0x300>  // b.tcont
   4dd04:	ldr	x3, [x20, x28]
   4dd08:	add	x0, x21, x28
   4dd0c:	mov	x1, x22
   4dd10:	mov	x2, x26
   4dd14:	bl	d400 <__gmpn_addmul_1@plt>
   4dd18:	add	x8, x21, x25, lsl #3
   4dd1c:	ldur	x9, [x8, #-8]
   4dd20:	add	x10, x0, x27
   4dd24:	add	x9, x10, x9
   4dd28:	stur	x9, [x8, #-8]
   4dd2c:	b	4dc90 <__gmpn_dcpi1_bdiv_q@@Base+0x300>
   4dd30:	mov	x24, x25
   4dd34:	mov	x0, x22
   4dd38:	mov	x1, x21
   4dd3c:	mov	x2, x24
   4dd40:	mov	x3, x20
   4dd44:	mov	x4, x24
   4dd48:	mov	x5, x19
   4dd4c:	ldp	x20, x19, [sp, #80]
   4dd50:	ldp	x22, x21, [sp, #64]
   4dd54:	ldp	x24, x23, [sp, #48]
   4dd58:	ldp	x26, x25, [sp, #32]
   4dd5c:	ldp	x28, x27, [sp, #16]
   4dd60:	ldp	x29, x30, [sp], #96
   4dd64:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>

000000000004dd68 <__gmpn_dcpi1_bdiv_qr_n_itch@@Base>:
   4dd68:	ret

000000000004dd6c <__gmpn_dcpi1_bdiv_qr_n@@Base>:
   4dd6c:	stp	x29, x30, [sp, #-96]!
   4dd70:	stp	x24, x23, [sp, #48]
   4dd74:	asr	x23, x3, #1
   4dd78:	stp	x26, x25, [sp, #32]
   4dd7c:	stp	x22, x21, [sp, #64]
   4dd80:	stp	x20, x19, [sp, #80]
   4dd84:	mov	x19, x5
   4dd88:	mov	x25, x4
   4dd8c:	mov	x20, x3
   4dd90:	mov	x24, x2
   4dd94:	mov	x21, x1
   4dd98:	mov	x26, x0
   4dd9c:	cmp	x3, #0x4d
   4dda0:	sub	x22, x3, x23
   4dda4:	stp	x28, x27, [sp, #16]
   4dda8:	mov	x29, sp
   4ddac:	b.le	4ddd0 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x64>
   4ddb0:	mov	x0, x26
   4ddb4:	mov	x1, x21
   4ddb8:	mov	x2, x24
   4ddbc:	mov	x3, x23
   4ddc0:	mov	x4, x25
   4ddc4:	mov	x5, x19
   4ddc8:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4ddcc:	b	4ddec <__gmpn_dcpi1_bdiv_qr_n@@Base+0x80>
   4ddd0:	and	x2, x20, #0xfffffffffffffffe
   4ddd4:	mov	x0, x26
   4ddd8:	mov	x1, x21
   4dddc:	mov	x3, x24
   4dde0:	mov	x4, x23
   4dde4:	mov	x5, x25
   4dde8:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4ddec:	lsl	x28, x23, #3
   4ddf0:	mov	x27, x0
   4ddf4:	add	x1, x24, x28
   4ddf8:	mov	x0, x19
   4ddfc:	mov	x2, x22
   4de00:	mov	x3, x26
   4de04:	mov	x4, x23
   4de08:	bl	ccd0 <__gmpn_mul@plt>
   4de0c:	ldr	x8, [x19, x28]
   4de10:	adds	x8, x8, x27
   4de14:	str	x8, [x19, x28]
   4de18:	b.cc	4de34 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xc8>  // b.lo, b.ul, b.last
   4de1c:	add	x8, x19, x23, lsl #3
   4de20:	add	x8, x8, #0x8
   4de24:	ldr	x9, [x8]
   4de28:	adds	x9, x9, #0x1
   4de2c:	str	x9, [x8], #8
   4de30:	b.cs	4de24 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xb8>  // b.hs, b.nlast
   4de34:	add	x27, x21, x23, lsl #3
   4de38:	cbz	x20, 4de80 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x114>
   4de3c:	mov	x0, x27
   4de40:	mov	x1, x27
   4de44:	mov	x2, x19
   4de48:	mov	x3, x20
   4de4c:	bl	ca70 <__gmpn_add_n@plt>
   4de50:	cbz	x0, 4de80 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x114>
   4de54:	add	x8, x22, x20
   4de58:	mov	w28, #0x1                   	// #1
   4de5c:	mov	x9, x20
   4de60:	cmp	x9, x8
   4de64:	b.ge	4de84 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x118>  // b.tcont
   4de68:	lsl	x10, x9, #3
   4de6c:	ldr	x11, [x27, x10]
   4de70:	add	x9, x9, #0x1
   4de74:	adds	x11, x11, #0x1
   4de78:	str	x11, [x27, x10]
   4de7c:	b.cs	4de60 <__gmpn_dcpi1_bdiv_qr_n@@Base+0xf4>  // b.hs, b.nlast
   4de80:	mov	x28, xzr
   4de84:	cmp	x22, #0x26
   4de88:	add	x26, x26, x23, lsl #3
   4de8c:	b.le	4deb0 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x144>
   4de90:	mov	x0, x26
   4de94:	mov	x1, x27
   4de98:	mov	x2, x24
   4de9c:	mov	x3, x22
   4dea0:	mov	x4, x25
   4dea4:	mov	x5, x19
   4dea8:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4deac:	b	4decc <__gmpn_dcpi1_bdiv_qr_n@@Base+0x160>
   4deb0:	lsl	x2, x22, #1
   4deb4:	mov	x0, x26
   4deb8:	mov	x1, x27
   4debc:	mov	x3, x24
   4dec0:	mov	x4, x22
   4dec4:	mov	x5, x25
   4dec8:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4decc:	lsl	x27, x22, #3
   4ded0:	mov	x25, x0
   4ded4:	add	x3, x24, x27
   4ded8:	mov	x0, x19
   4dedc:	mov	x1, x26
   4dee0:	mov	x2, x22
   4dee4:	mov	x4, x23
   4dee8:	bl	ccd0 <__gmpn_mul@plt>
   4deec:	ldr	x8, [x19, x27]
   4def0:	adds	x8, x8, x25
   4def4:	str	x8, [x19, x27]
   4def8:	b.cc	4df14 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x1a8>  // b.lo, b.ul, b.last
   4defc:	add	x8, x19, x22, lsl #3
   4df00:	add	x8, x8, #0x8
   4df04:	ldr	x9, [x8]
   4df08:	adds	x9, x9, #0x1
   4df0c:	str	x9, [x8], #8
   4df10:	b.cs	4df04 <__gmpn_dcpi1_bdiv_qr_n@@Base+0x198>  // b.hs, b.nlast
   4df14:	add	x0, x21, x20, lsl #3
   4df18:	mov	x1, x0
   4df1c:	mov	x2, x19
   4df20:	mov	x3, x20
   4df24:	bl	ca70 <__gmpn_add_n@plt>
   4df28:	add	x0, x0, x28
   4df2c:	ldp	x20, x19, [sp, #80]
   4df30:	ldp	x22, x21, [sp, #64]
   4df34:	ldp	x24, x23, [sp, #48]
   4df38:	ldp	x26, x25, [sp, #32]
   4df3c:	ldp	x28, x27, [sp, #16]
   4df40:	ldp	x29, x30, [sp], #96
   4df44:	ret

000000000004df48 <__gmpn_dcpi1_bdiv_qr@@Base>:
   4df48:	stp	x29, x30, [sp, #-96]!
   4df4c:	stp	x28, x27, [sp, #16]
   4df50:	stp	x26, x25, [sp, #32]
   4df54:	stp	x24, x23, [sp, #48]
   4df58:	stp	x22, x21, [sp, #64]
   4df5c:	stp	x20, x19, [sp, #80]
   4df60:	mov	x29, sp
   4df64:	sub	sp, sp, #0x30
   4df68:	lsl	x28, x4, #3
   4df6c:	add	x9, x28, #0xf
   4df70:	mov	x8, sp
   4df74:	and	x9, x9, #0xfffffffffffffff0
   4df78:	mov	x19, x4
   4df7c:	mov	x21, x3
   4df80:	mov	x24, x2
   4df84:	sub	x20, x8, x9
   4df88:	stur	x5, [x29, #-8]
   4df8c:	mov	sp, x20
   4df90:	sub	x25, x2, x4
   4df94:	cmp	x25, x4
   4df98:	b.le	4e024 <__gmpn_dcpi1_bdiv_qr@@Base+0xdc>
   4df9c:	mov	x12, x21
   4dfa0:	lsl	x11, x24, #1
   4dfa4:	lsl	x10, x19, #1
   4dfa8:	mov	x26, xzr
   4dfac:	neg	x8, x28
   4dfb0:	lsl	x13, x24, #3
   4dfb4:	stp	x20, x12, [x29, #-24]
   4dfb8:	sub	x21, x20, x28
   4dfbc:	neg	x9, x1
   4dfc0:	sub	x22, x28, x12
   4dfc4:	sub	x20, x28, x1
   4dfc8:	sub	x2, x11, x10
   4dfcc:	sub	x23, x28, x0
   4dfd0:	sub	x26, x26, x19
   4dfd4:	add	x27, x25, x26
   4dfd8:	add	x21, x21, x8
   4dfdc:	add	x9, x9, x28
   4dfe0:	sub	x2, x2, x10
   4dfe4:	add	x22, x22, x28
   4dfe8:	add	x20, x20, x28
   4dfec:	cmp	x27, x19
   4dff0:	add	x23, x23, x28
   4dff4:	b.gt	4dfd0 <__gmpn_dcpi1_bdiv_qr@@Base+0x88>
   4dff8:	sub	x25, x13, x9
   4dffc:	cmp	x27, #0x26
   4e000:	sub	x8, x10, x24
   4e004:	stp	x0, x13, [x29, #-40]
   4e008:	b.le	4e04c <__gmpn_dcpi1_bdiv_qr@@Base+0x104>
   4e00c:	ldp	x2, x4, [x29, #-16]
   4e010:	ldur	x5, [x29, #-24]
   4e014:	mov	x3, x27
   4e018:	mov	x24, x8
   4e01c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4e020:	b	4e05c <__gmpn_dcpi1_bdiv_qr@@Base+0x114>
   4e024:	cmp	x25, #0x26
   4e028:	b.le	4e07c <__gmpn_dcpi1_bdiv_qr@@Base+0x134>
   4e02c:	ldur	x4, [x29, #-8]
   4e030:	mov	x2, x21
   4e034:	mov	x3, x25
   4e038:	mov	x5, x20
   4e03c:	mov	x23, x0
   4e040:	mov	x26, x1
   4e044:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4e048:	b	4e098 <__gmpn_dcpi1_bdiv_qr@@Base+0x150>
   4e04c:	ldp	x3, x5, [x29, #-16]
   4e050:	mov	x4, x27
   4e054:	mov	x24, x8
   4e058:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4e05c:	ldur	x12, [x29, #-32]
   4e060:	mov	x8, x24
   4e064:	mov	x24, x0
   4e068:	subs	x4, x8, x26
   4e06c:	b.ne	4e0ac <__gmpn_dcpi1_bdiv_qr@@Base+0x164>  // b.any
   4e070:	ldur	x27, [x29, #-24]
   4e074:	mov	x21, xzr
   4e078:	b	4e20c <__gmpn_dcpi1_bdiv_qr@@Base+0x2c4>
   4e07c:	ldur	x5, [x29, #-8]
   4e080:	lsl	x2, x25, #1
   4e084:	mov	x3, x21
   4e088:	mov	x4, x25
   4e08c:	mov	x23, x0
   4e090:	mov	x26, x1
   4e094:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4e098:	mov	x22, x0
   4e09c:	cmp	x25, x19
   4e0a0:	b.ne	4e0c8 <__gmpn_dcpi1_bdiv_qr@@Base+0x180>  // b.any
   4e0a4:	mov	x8, xzr
   4e0a8:	b	4e1f8 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b0>
   4e0ac:	cmp	x27, x4
   4e0b0:	sub	x3, x12, x22
   4e0b4:	b.le	4e0e8 <__gmpn_dcpi1_bdiv_qr@@Base+0x1a0>
   4e0b8:	ldur	x0, [x29, #-24]
   4e0bc:	ldur	x1, [x29, #-40]
   4e0c0:	mov	x2, x27
   4e0c4:	b	4e0fc <__gmpn_dcpi1_bdiv_qr@@Base+0x1b4>
   4e0c8:	sub	x4, x19, x25
   4e0cc:	cmp	x25, x4
   4e0d0:	add	x3, x21, x25, lsl #3
   4e0d4:	mov	x0, x20
   4e0d8:	b.le	4e180 <__gmpn_dcpi1_bdiv_qr@@Base+0x238>
   4e0dc:	mov	x1, x23
   4e0e0:	mov	x2, x25
   4e0e4:	b	4e190 <__gmpn_dcpi1_bdiv_qr@@Base+0x248>
   4e0e8:	ldur	x0, [x29, #-24]
   4e0ec:	mov	x1, x3
   4e0f0:	ldur	x3, [x29, #-40]
   4e0f4:	mov	x2, x4
   4e0f8:	mov	x4, x27
   4e0fc:	bl	ccd0 <__gmpn_mul@plt>
   4e100:	ldur	x12, [x29, #-32]
   4e104:	ldr	x8, [x21, x12]
   4e108:	adds	x8, x8, x24
   4e10c:	str	x8, [x21, x12]
   4e110:	b.cc	4e12c <__gmpn_dcpi1_bdiv_qr@@Base+0x1e4>  // b.lo, b.ul, b.last
   4e114:	add	x8, x21, x12
   4e118:	add	x8, x8, #0x8
   4e11c:	ldr	x9, [x8]
   4e120:	adds	x9, x9, #0x1
   4e124:	str	x9, [x8], #8
   4e128:	b.cs	4e11c <__gmpn_dcpi1_bdiv_qr@@Base+0x1d4>  // b.hs, b.nlast
   4e12c:	ldur	x27, [x29, #-24]
   4e130:	cbz	x19, 4e204 <__gmpn_dcpi1_bdiv_qr@@Base+0x2bc>
   4e134:	sub	x0, x12, x20
   4e138:	mov	x1, x0
   4e13c:	mov	x2, x27
   4e140:	mov	x3, x19
   4e144:	bl	ca70 <__gmpn_add_n@plt>
   4e148:	cbz	x0, 4e200 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b8>
   4e14c:	ldur	x12, [x29, #-32]
   4e150:	sub	x8, x19, x26
   4e154:	mov	w21, #0x1                   	// #1
   4e158:	mov	x9, x25
   4e15c:	mov	x10, x19
   4e160:	cmp	x10, x8
   4e164:	b.ge	4e208 <__gmpn_dcpi1_bdiv_qr@@Base+0x2c0>  // b.tcont
   4e168:	ldr	x11, [x9]
   4e16c:	add	x10, x10, #0x1
   4e170:	adds	x11, x11, #0x1
   4e174:	str	x11, [x9], #8
   4e178:	b.cs	4e160 <__gmpn_dcpi1_bdiv_qr@@Base+0x218>  // b.hs, b.nlast
   4e17c:	b	4e204 <__gmpn_dcpi1_bdiv_qr@@Base+0x2bc>
   4e180:	mov	x1, x3
   4e184:	mov	x2, x4
   4e188:	mov	x3, x23
   4e18c:	mov	x4, x25
   4e190:	bl	ccd0 <__gmpn_mul@plt>
   4e194:	lsl	x8, x25, #3
   4e198:	ldr	x9, [x20, x8]
   4e19c:	adds	x9, x9, x22
   4e1a0:	str	x9, [x20, x8]
   4e1a4:	b.cc	4e1c8 <__gmpn_dcpi1_bdiv_qr@@Base+0x280>  // b.lo, b.ul, b.last
   4e1a8:	lsl	x8, x24, #3
   4e1ac:	sub	x8, x8, x19, lsl #3
   4e1b0:	add	x8, x8, x20
   4e1b4:	add	x8, x8, #0x8
   4e1b8:	ldr	x9, [x8]
   4e1bc:	adds	x9, x9, #0x1
   4e1c0:	str	x9, [x8], #8
   4e1c4:	b.cs	4e1b8 <__gmpn_dcpi1_bdiv_qr@@Base+0x270>  // b.hs, b.nlast
   4e1c8:	cbz	x19, 4e1f0 <__gmpn_dcpi1_bdiv_qr@@Base+0x2a8>
   4e1cc:	add	x0, x26, x25, lsl #3
   4e1d0:	mov	x1, x0
   4e1d4:	mov	x2, x20
   4e1d8:	mov	x3, x19
   4e1dc:	bl	ca70 <__gmpn_add_n@plt>
   4e1e0:	cmp	x0, #0x0
   4e1e4:	mov	x22, xzr
   4e1e8:	cset	w8, ne  // ne = any
   4e1ec:	b	4e1f8 <__gmpn_dcpi1_bdiv_qr@@Base+0x2b0>
   4e1f0:	mov	x8, xzr
   4e1f4:	mov	x22, xzr
   4e1f8:	add	x0, x22, x8
   4e1fc:	b	4e2a4 <__gmpn_dcpi1_bdiv_qr@@Base+0x35c>
   4e200:	ldur	x12, [x29, #-32]
   4e204:	mov	x21, xzr
   4e208:	mov	x24, xzr
   4e20c:	sub	x1, x12, x20
   4e210:	neg	x20, x26
   4e214:	ldur	x26, [x29, #-16]
   4e218:	sub	x23, x12, x23
   4e21c:	b	4e25c <__gmpn_dcpi1_bdiv_qr@@Base+0x314>
   4e220:	mov	x8, xzr
   4e224:	ldur	x4, [x29, #-8]
   4e228:	mov	x0, x23
   4e22c:	mov	x2, x26
   4e230:	mov	x3, x19
   4e234:	mov	x5, x27
   4e238:	add	x21, x8, x21
   4e23c:	bl	d300 <__gmpn_dcpi1_bdiv_qr_n@plt>
   4e240:	sub	x20, x20, x19
   4e244:	mov	x24, x0
   4e248:	add	x23, x23, x19, lsl #3
   4e24c:	cmp	x20, #0x0
   4e250:	add	x25, x25, x28
   4e254:	mov	x1, x22
   4e258:	b.le	4e2a0 <__gmpn_dcpi1_bdiv_qr@@Base+0x358>
   4e25c:	add	x22, x1, x19, lsl #3
   4e260:	ldr	x8, [x22]
   4e264:	adds	x8, x8, x24
   4e268:	str	x8, [x22]
   4e26c:	b.cc	4e220 <__gmpn_dcpi1_bdiv_qr@@Base+0x2d8>  // b.lo, b.ul, b.last
   4e270:	mov	w8, #0x1                   	// #1
   4e274:	cmp	x8, x20
   4e278:	b.ge	4e298 <__gmpn_dcpi1_bdiv_qr@@Base+0x350>  // b.tcont
   4e27c:	lsl	x9, x8, #3
   4e280:	ldr	x10, [x25, x9]
   4e284:	add	x8, x8, #0x1
   4e288:	adds	x10, x10, #0x1
   4e28c:	str	x10, [x25, x9]
   4e290:	b.cs	4e274 <__gmpn_dcpi1_bdiv_qr@@Base+0x32c>  // b.hs, b.nlast
   4e294:	b	4e220 <__gmpn_dcpi1_bdiv_qr@@Base+0x2d8>
   4e298:	mov	w8, #0x1                   	// #1
   4e29c:	b	4e224 <__gmpn_dcpi1_bdiv_qr@@Base+0x2dc>
   4e2a0:	add	x0, x21, x24
   4e2a4:	mov	sp, x29
   4e2a8:	ldp	x20, x19, [sp, #80]
   4e2ac:	ldp	x22, x21, [sp, #64]
   4e2b0:	ldp	x24, x23, [sp, #48]
   4e2b4:	ldp	x26, x25, [sp, #32]
   4e2b8:	ldp	x28, x27, [sp, #16]
   4e2bc:	ldp	x29, x30, [sp], #96
   4e2c0:	ret

000000000004e2c4 <__gmpn_mu_bdiv_q@@Base>:
   4e2c4:	sub	sp, sp, #0xe0
   4e2c8:	stp	x28, x27, [sp, #144]
   4e2cc:	stp	x26, x25, [sp, #160]
   4e2d0:	stp	x22, x21, [sp, #192]
   4e2d4:	stp	x20, x19, [sp, #208]
   4e2d8:	mov	x28, x5
   4e2dc:	mov	x25, x3
   4e2e0:	mov	x27, x2
   4e2e4:	mov	x19, x1
   4e2e8:	cmp	x2, x4
   4e2ec:	mov	x21, x0
   4e2f0:	stp	x29, x30, [sp, #128]
   4e2f4:	stp	x24, x23, [sp, #176]
   4e2f8:	add	x29, sp, #0x80
   4e2fc:	b.le	4e520 <__gmpn_mu_bdiv_q@@Base+0x25c>
   4e300:	sub	x8, x27, #0x1
   4e304:	sdiv	x9, x8, x4
   4e308:	add	x9, x9, #0x1
   4e30c:	sdiv	x20, x8, x9
   4e310:	add	x26, x20, #0x1
   4e314:	add	x24, x28, x26, lsl #3
   4e318:	mov	x0, x28
   4e31c:	mov	x1, x25
   4e320:	mov	x2, x26
   4e324:	mov	x3, x24
   4e328:	mov	x22, x4
   4e32c:	stur	x25, [x29, #-40]
   4e330:	bl	cd20 <__gmpn_binvert@plt>
   4e334:	mov	x0, x24
   4e338:	mov	x1, x19
   4e33c:	mov	x2, x22
   4e340:	bl	ca50 <__gmpn_copyi@plt>
   4e344:	mov	x0, x21
   4e348:	mov	x1, x24
   4e34c:	mov	x2, x28
   4e350:	mov	x3, x26
   4e354:	add	x25, x19, x22, lsl #3
   4e358:	bl	cec0 <__gmpn_mullo_n@plt>
   4e35c:	sub	x23, x27, x26
   4e360:	cmp	x23, x26
   4e364:	lsl	x11, x22, #3
   4e368:	lsl	x12, x26, #3
   4e36c:	stp	x22, x20, [x29, #-32]
   4e370:	str	x27, [sp, #32]
   4e374:	stur	x28, [x29, #-16]
   4e378:	stp	x11, x21, [sp, #16]
   4e37c:	str	x12, [sp, #8]
   4e380:	b.le	4e5e4 <__gmpn_mu_bdiv_q@@Base+0x320>
   4e384:	add	x8, x26, x22
   4e388:	stur	x8, [x29, #-56]
   4e38c:	add	x8, x24, x12
   4e390:	str	x8, [sp, #64]
   4e394:	sub	x8, x22, x26
   4e398:	add	x27, x24, x11
   4e39c:	str	x8, [sp, #56]
   4e3a0:	mvn	x8, x20
   4e3a4:	mov	x10, x28
   4e3a8:	add	x9, x20, x22
   4e3ac:	add	x12, x27, x12
   4e3b0:	add	x8, x27, x8, lsl #3
   4e3b4:	str	x12, [sp, #48]
   4e3b8:	stur	x8, [x29, #-48]
   4e3bc:	add	x8, x10, x9, lsl #4
   4e3c0:	mov	w28, wzr
   4e3c4:	add	x11, x27, x11
   4e3c8:	add	x8, x8, #0x18
   4e3cc:	mov	x19, x21
   4e3d0:	stur	x11, [x29, #-8]
   4e3d4:	str	x8, [sp, #40]
   4e3d8:	b	4e424 <__gmpn_mu_bdiv_q@@Base+0x160>
   4e3dc:	ldur	x20, [x29, #-24]
   4e3e0:	mov	x24, x21
   4e3e4:	ldur	x0, [x29, #-48]
   4e3e8:	ldur	x2, [x29, #-8]
   4e3ec:	sxtw	x4, w28
   4e3f0:	mov	x1, x25
   4e3f4:	mov	x3, x26
   4e3f8:	bl	c760 <__gmpn_sub_nc@plt>
   4e3fc:	ldur	x2, [x29, #-16]
   4e400:	mov	x28, x0
   4e404:	mov	x0, x19
   4e408:	mov	x1, x24
   4e40c:	mov	x3, x26
   4e410:	add	x25, x25, x26, lsl #3
   4e414:	bl	cec0 <__gmpn_mullo_n@plt>
   4e418:	sub	x23, x23, x26
   4e41c:	cmp	x23, x26
   4e420:	b.le	4e5ec <__gmpn_mu_bdiv_q@@Base+0x328>
   4e424:	mov	x21, x24
   4e428:	cmp	x20, #0x10
   4e42c:	b.le	4e4b8 <__gmpn_mu_bdiv_q@@Base+0x1f4>
   4e430:	mov	x0, x22
   4e434:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e438:	ldur	x2, [x29, #-40]
   4e43c:	mov	x3, x22
   4e440:	mov	x22, x0
   4e444:	add	x20, x27, x0, lsl #3
   4e448:	mov	x0, x27
   4e44c:	mov	x1, x22
   4e450:	mov	x4, x19
   4e454:	mov	x5, x26
   4e458:	mov	x6, x20
   4e45c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4e460:	ldur	x8, [x29, #-56]
   4e464:	sub	x24, x8, x22
   4e468:	cmp	x24, #0x1
   4e46c:	b.lt	4e4d0 <__gmpn_mu_bdiv_q@@Base+0x20c>  // b.tstop
   4e470:	mov	x0, x20
   4e474:	mov	x1, x27
   4e478:	mov	x2, x21
   4e47c:	mov	x3, x24
   4e480:	bl	c2d0 <__gmpn_sub_n@plt>
   4e484:	lsl	x8, x24, #3
   4e488:	ldr	x9, [x27, x8]
   4e48c:	sxtw	x10, w0
   4e490:	subs	x9, x9, x10
   4e494:	str	x9, [x27, x8]
   4e498:	b.cs	4e4d0 <__gmpn_mu_bdiv_q@@Base+0x20c>  // b.hs, b.nlast
   4e49c:	ldr	x8, [sp, #40]
   4e4a0:	sub	x8, x8, x22, lsl #3
   4e4a4:	ldr	x9, [x8]
   4e4a8:	sub	x10, x9, #0x1
   4e4ac:	str	x10, [x8], #8
   4e4b0:	cbz	x9, 4e4a4 <__gmpn_mu_bdiv_q@@Base+0x1e0>
   4e4b4:	b	4e4d0 <__gmpn_mu_bdiv_q@@Base+0x20c>
   4e4b8:	ldur	x1, [x29, #-40]
   4e4bc:	mov	x0, x27
   4e4c0:	mov	x2, x22
   4e4c4:	mov	x3, x19
   4e4c8:	mov	x4, x26
   4e4cc:	bl	ccd0 <__gmpn_mul@plt>
   4e4d0:	ldur	x22, [x29, #-32]
   4e4d4:	add	x19, x19, x26, lsl #3
   4e4d8:	cmp	x26, x22
   4e4dc:	b.eq	4e3dc <__gmpn_mu_bdiv_q@@Base+0x118>  // b.none
   4e4e0:	ldp	x3, x1, [sp, #56]
   4e4e4:	ldr	x2, [sp, #48]
   4e4e8:	mov	x0, x21
   4e4ec:	mov	x24, x21
   4e4f0:	bl	c2d0 <__gmpn_sub_n@plt>
   4e4f4:	ldur	x20, [x29, #-24]
   4e4f8:	add	w28, w28, w0
   4e4fc:	cmp	w28, #0x2
   4e500:	b.ne	4e3e4 <__gmpn_mu_bdiv_q@@Base+0x120>  // b.any
   4e504:	ldur	x8, [x29, #-8]
   4e508:	ldr	x9, [x8]
   4e50c:	adds	x9, x9, #0x1
   4e510:	str	x9, [x8], #8
   4e514:	b.cs	4e508 <__gmpn_mu_bdiv_q@@Base+0x244>  // b.hs, b.nlast
   4e518:	mov	w28, #0x1                   	// #1
   4e51c:	b	4e3e4 <__gmpn_mu_bdiv_q@@Base+0x120>
   4e520:	asr	x22, x27, #1
   4e524:	sub	x24, x27, x22
   4e528:	add	x23, x28, x24, lsl #3
   4e52c:	mov	x0, x28
   4e530:	mov	x1, x25
   4e534:	mov	x2, x24
   4e538:	mov	x3, x23
   4e53c:	bl	cd20 <__gmpn_binvert@plt>
   4e540:	mov	x0, x21
   4e544:	mov	x1, x19
   4e548:	mov	x2, x28
   4e54c:	mov	x3, x24
   4e550:	bl	cec0 <__gmpn_mullo_n@plt>
   4e554:	cmp	x24, #0x11
   4e558:	b.le	4e698 <__gmpn_mu_bdiv_q@@Base+0x3d4>
   4e55c:	mov	x0, x27
   4e560:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e564:	mov	x2, x25
   4e568:	mov	x25, x0
   4e56c:	add	x6, x23, x0, lsl #3
   4e570:	mov	x0, x23
   4e574:	mov	x1, x25
   4e578:	mov	x3, x27
   4e57c:	mov	x4, x21
   4e580:	mov	x5, x24
   4e584:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4e588:	add	x8, x24, x27
   4e58c:	sub	x8, x8, x25
   4e590:	cmp	x8, #0x1
   4e594:	b.lt	4e7a8 <__gmpn_mu_bdiv_q@@Base+0x4e4>  // b.tstop
   4e598:	lsl	x9, x27, #1
   4e59c:	add	x10, x25, x22
   4e5a0:	sub	x10, x9, x10
   4e5a4:	add	x9, x9, x27
   4e5a8:	add	x10, x19, x10, lsl #3
   4e5ac:	sub	x11, x9, x25
   4e5b0:	sub	x9, x10, #0x8
   4e5b4:	sub	x10, x11, x22, lsl #1
   4e5b8:	add	x10, x28, x10, lsl #3
   4e5bc:	sub	x10, x10, #0x8
   4e5c0:	mov	x11, x8
   4e5c4:	subs	x11, x11, #0x1
   4e5c8:	b.lt	4e76c <__gmpn_mu_bdiv_q@@Base+0x4a8>  // b.tstop
   4e5cc:	ldr	x12, [x10], #-8
   4e5d0:	ldr	x13, [x9], #-8
   4e5d4:	cmp	x12, x13
   4e5d8:	b.eq	4e5c4 <__gmpn_mu_bdiv_q@@Base+0x300>  // b.none
   4e5dc:	cset	w9, ls  // ls = plast
   4e5e0:	b	4e770 <__gmpn_mu_bdiv_q@@Base+0x4ac>
   4e5e4:	mov	w28, wzr
   4e5e8:	mov	x19, x21
   4e5ec:	cmp	x20, #0x10
   4e5f0:	b.le	4e6b4 <__gmpn_mu_bdiv_q@@Base+0x3f0>
   4e5f4:	mov	x0, x22
   4e5f8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e5fc:	mov	x20, x22
   4e600:	ldur	x2, [x29, #-40]
   4e604:	add	x27, x24, x20, lsl #3
   4e608:	mov	x22, x0
   4e60c:	add	x21, x27, x0, lsl #3
   4e610:	mov	x0, x27
   4e614:	mov	x1, x22
   4e618:	mov	x3, x20
   4e61c:	mov	x4, x19
   4e620:	mov	x5, x26
   4e624:	mov	x6, x21
   4e628:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4e62c:	add	x8, x26, x20
   4e630:	sub	x20, x8, x22
   4e634:	cmp	x20, #0x1
   4e638:	b.lt	4e6cc <__gmpn_mu_bdiv_q@@Base+0x408>  // b.tstop
   4e63c:	mov	x0, x21
   4e640:	mov	x1, x27
   4e644:	mov	x2, x24
   4e648:	mov	x3, x20
   4e64c:	bl	c2d0 <__gmpn_sub_n@plt>
   4e650:	lsl	x8, x20, #3
   4e654:	ldr	x9, [x27, x8]
   4e658:	sxtw	x10, w0
   4e65c:	subs	x9, x9, x10
   4e660:	str	x9, [x27, x8]
   4e664:	b.cs	4e6cc <__gmpn_mu_bdiv_q@@Base+0x408>  // b.hs, b.nlast
   4e668:	ldp	x9, x8, [x29, #-32]
   4e66c:	add	x8, x8, x9
   4e670:	ldur	x9, [x29, #-16]
   4e674:	lsl	x8, x8, #1
   4e678:	sub	x8, x8, x22
   4e67c:	add	x8, x9, x8, lsl #3
   4e680:	add	x8, x8, #0x18
   4e684:	ldr	x9, [x8]
   4e688:	sub	x10, x9, #0x1
   4e68c:	str	x10, [x8], #8
   4e690:	cbz	x9, 4e684 <__gmpn_mu_bdiv_q@@Base+0x3c0>
   4e694:	b	4e6cc <__gmpn_mu_bdiv_q@@Base+0x408>
   4e698:	mov	x0, x23
   4e69c:	mov	x1, x25
   4e6a0:	mov	x2, x27
   4e6a4:	mov	x3, x21
   4e6a8:	mov	x4, x24
   4e6ac:	bl	ccd0 <__gmpn_mul@plt>
   4e6b0:	b	4e7a8 <__gmpn_mu_bdiv_q@@Base+0x4e4>
   4e6b4:	ldur	x1, [x29, #-40]
   4e6b8:	add	x0, x24, x22, lsl #3
   4e6bc:	mov	x2, x22
   4e6c0:	mov	x3, x19
   4e6c4:	mov	x4, x26
   4e6c8:	bl	ccd0 <__gmpn_mul@plt>
   4e6cc:	ldur	x8, [x29, #-32]
   4e6d0:	add	x9, x19, x26, lsl #3
   4e6d4:	stur	x9, [x29, #-8]
   4e6d8:	subs	x20, x26, x8
   4e6dc:	b.ne	4e6ec <__gmpn_mu_bdiv_q@@Base+0x428>  // b.any
   4e6e0:	ldp	x21, x27, [sp, #24]
   4e6e4:	ldr	x10, [sp, #16]
   4e6e8:	b	4e738 <__gmpn_mu_bdiv_q@@Base+0x474>
   4e6ec:	ldr	x9, [sp, #8]
   4e6f0:	add	x22, x24, x8, lsl #3
   4e6f4:	sub	x3, x8, x26
   4e6f8:	mov	x0, x24
   4e6fc:	add	x1, x24, x9
   4e700:	add	x2, x22, x9
   4e704:	mov	x19, x8
   4e708:	bl	c2d0 <__gmpn_sub_n@plt>
   4e70c:	ldp	x21, x27, [sp, #24]
   4e710:	ldr	x10, [sp, #16]
   4e714:	add	w28, w28, w0
   4e718:	cmp	w28, #0x2
   4e71c:	b.ne	4e738 <__gmpn_mu_bdiv_q@@Base+0x474>  // b.any
   4e720:	add	x8, x22, x19, lsl #3
   4e724:	ldr	x9, [x8]
   4e728:	adds	x9, x9, #0x1
   4e72c:	str	x9, [x8], #8
   4e730:	b.cs	4e724 <__gmpn_mu_bdiv_q@@Base+0x460>  // b.hs, b.nlast
   4e734:	mov	w28, #0x1                   	// #1
   4e738:	ldur	x9, [x29, #-24]
   4e73c:	add	x8, x24, x10
   4e740:	add	x3, x20, x23
   4e744:	add	x2, x8, x10
   4e748:	mvn	x9, x9
   4e74c:	add	x0, x8, x9, lsl #3
   4e750:	sxtw	x4, w28
   4e754:	mov	x1, x25
   4e758:	bl	c760 <__gmpn_sub_nc@plt>
   4e75c:	ldp	x2, x0, [x29, #-16]
   4e760:	mov	x1, x24
   4e764:	mov	x3, x23
   4e768:	b	4e7d0 <__gmpn_mu_bdiv_q@@Base+0x50c>
   4e76c:	mov	x9, xzr
   4e770:	lsl	x8, x8, #3
   4e774:	ldr	x10, [x23, x8]
   4e778:	subs	x9, x10, x9
   4e77c:	str	x9, [x23, x8]
   4e780:	b.cs	4e7a8 <__gmpn_mu_bdiv_q@@Base+0x4e4>  // b.hs, b.nlast
   4e784:	add	x8, x27, x27, lsl #1
   4e788:	sub	x8, x8, x25
   4e78c:	sub	x8, x8, x22, lsl #1
   4e790:	add	x8, x28, x8, lsl #3
   4e794:	add	x8, x8, #0x8
   4e798:	ldr	x9, [x8]
   4e79c:	sub	x10, x9, #0x1
   4e7a0:	str	x10, [x8], #8
   4e7a4:	cbz	x9, 4e798 <__gmpn_mu_bdiv_q@@Base+0x4d4>
   4e7a8:	lsl	x20, x24, #3
   4e7ac:	add	x1, x19, x20
   4e7b0:	add	x2, x23, x20
   4e7b4:	mov	x0, x23
   4e7b8:	mov	x3, x22
   4e7bc:	bl	c2d0 <__gmpn_sub_n@plt>
   4e7c0:	add	x0, x21, x20
   4e7c4:	mov	x1, x23
   4e7c8:	mov	x2, x28
   4e7cc:	mov	x3, x22
   4e7d0:	bl	cec0 <__gmpn_mullo_n@plt>
   4e7d4:	ldr	x8, [x21]
   4e7d8:	cbnz	x8, 4e7f0 <__gmpn_mu_bdiv_q@@Base+0x52c>
   4e7dc:	subs	x27, x27, #0x1
   4e7e0:	str	xzr, [x21]
   4e7e4:	b.eq	4e828 <__gmpn_mu_bdiv_q@@Base+0x564>  // b.none
   4e7e8:	ldr	x8, [x21, #8]!
   4e7ec:	cbz	x8, 4e7dc <__gmpn_mu_bdiv_q@@Base+0x518>
   4e7f0:	neg	x8, x8
   4e7f4:	subs	x2, x27, #0x1
   4e7f8:	str	x8, [x21]
   4e7fc:	b.eq	4e828 <__gmpn_mu_bdiv_q@@Base+0x564>  // b.none
   4e800:	add	x0, x21, #0x8
   4e804:	ldp	x20, x19, [sp, #208]
   4e808:	ldp	x22, x21, [sp, #192]
   4e80c:	ldp	x24, x23, [sp, #176]
   4e810:	ldp	x26, x25, [sp, #160]
   4e814:	ldp	x28, x27, [sp, #144]
   4e818:	ldp	x29, x30, [sp, #128]
   4e81c:	mov	x1, x0
   4e820:	add	sp, sp, #0xe0
   4e824:	b	c290 <__gmpn_com@plt>
   4e828:	ldp	x20, x19, [sp, #208]
   4e82c:	ldp	x22, x21, [sp, #192]
   4e830:	ldp	x24, x23, [sp, #176]
   4e834:	ldp	x26, x25, [sp, #160]
   4e838:	ldp	x28, x27, [sp, #144]
   4e83c:	ldp	x29, x30, [sp, #128]
   4e840:	add	sp, sp, #0xe0
   4e844:	ret

000000000004e848 <__gmpn_mu_bdiv_q_itch@@Base>:
   4e848:	stp	x29, x30, [sp, #-48]!
   4e84c:	str	x21, [sp, #16]
   4e850:	mov	x21, x0
   4e854:	cmp	x0, x1
   4e858:	stp	x20, x19, [sp, #32]
   4e85c:	mov	x29, sp
   4e860:	b.le	4e8ac <__gmpn_mu_bdiv_q_itch@@Base+0x64>
   4e864:	sub	x8, x21, #0x1
   4e868:	sdiv	x9, x8, x1
   4e86c:	add	x9, x9, #0x1
   4e870:	sdiv	x21, x8, x9
   4e874:	mov	x20, x1
   4e878:	cmp	x21, #0x10
   4e87c:	add	x19, x21, #0x1
   4e880:	b.le	4e8e0 <__gmpn_mu_bdiv_q_itch@@Base+0x98>
   4e884:	mov	x0, x20
   4e888:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e88c:	asr	x8, x0, #1
   4e890:	cmp	x8, x21
   4e894:	csel	x9, x8, x0, gt
   4e898:	cmp	x8, x20
   4e89c:	csel	x8, x9, xzr, lt  // lt = tstop
   4e8a0:	add	x8, x0, x8
   4e8a4:	add	x8, x8, #0x4
   4e8a8:	b	4e8e8 <__gmpn_mu_bdiv_q_itch@@Base+0xa0>
   4e8ac:	sub	x19, x21, x21, asr #1
   4e8b0:	cmp	x19, #0x11
   4e8b4:	b.le	4e8f4 <__gmpn_mu_bdiv_q_itch@@Base+0xac>
   4e8b8:	mov	x0, x21
   4e8bc:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4e8c0:	asr	x8, x0, #1
   4e8c4:	cmp	x8, x19
   4e8c8:	csel	x9, x0, x8, lt  // lt = tstop
   4e8cc:	cmp	x8, x21
   4e8d0:	csel	x8, x9, xzr, lt  // lt = tstop
   4e8d4:	add	x8, x0, x8
   4e8d8:	add	x8, x8, #0x4
   4e8dc:	b	4e8fc <__gmpn_mu_bdiv_q_itch@@Base+0xb4>
   4e8e0:	mov	x8, xzr
   4e8e4:	add	x0, x19, x20
   4e8e8:	add	x9, x0, x20
   4e8ec:	add	x20, x9, x8
   4e8f0:	b	4e900 <__gmpn_mu_bdiv_q_itch@@Base+0xb8>
   4e8f4:	mov	x8, xzr
   4e8f8:	add	x0, x19, x21
   4e8fc:	add	x20, x8, x0
   4e900:	mov	x0, x19
   4e904:	bl	d1e0 <__gmpn_binvert_itch@plt>
   4e908:	cmp	x20, x0
   4e90c:	csel	x8, x20, x0, gt
   4e910:	add	x0, x8, x19
   4e914:	ldp	x20, x19, [sp, #32]
   4e918:	ldr	x21, [sp, #16]
   4e91c:	ldp	x29, x30, [sp], #48
   4e920:	ret

000000000004e924 <__gmpn_mu_bdiv_qr@@Base>:
   4e924:	sub	sp, sp, #0xd0
   4e928:	stp	x24, x23, [sp, #160]
   4e92c:	sub	x24, x3, x5
   4e930:	stp	x29, x30, [sp, #112]
   4e934:	stp	x28, x27, [sp, #128]
   4e938:	stp	x26, x25, [sp, #144]
   4e93c:	stp	x22, x21, [sp, #176]
   4e940:	stp	x20, x19, [sp, #192]
   4e944:	add	x29, sp, #0x70
   4e948:	mov	x21, x6
   4e94c:	mov	x23, x5
   4e950:	mov	x28, x4
   4e954:	mov	x25, x2
   4e958:	cmp	x24, x5
   4e95c:	mov	x20, x0
   4e960:	stp	x5, x6, [x29, #-40]
   4e964:	stur	x1, [x29, #-8]
   4e968:	stur	x4, [x29, #-48]
   4e96c:	b.le	4eb60 <__gmpn_mu_bdiv_qr@@Base+0x23c>
   4e970:	sub	x8, x24, #0x1
   4e974:	sdiv	x9, x8, x23
   4e978:	add	x9, x9, #0x1
   4e97c:	sdiv	x27, x8, x9
   4e980:	mov	x19, x1
   4e984:	mov	x1, x28
   4e988:	add	x28, x27, #0x1
   4e98c:	add	x22, x21, x28, lsl #3
   4e990:	mov	x0, x21
   4e994:	mov	x2, x28
   4e998:	mov	x3, x22
   4e99c:	bl	cd20 <__gmpn_binvert@plt>
   4e9a0:	mov	x0, x19
   4e9a4:	mov	x1, x25
   4e9a8:	mov	x2, x23
   4e9ac:	bl	ca50 <__gmpn_copyi@plt>
   4e9b0:	cmp	x24, x28
   4e9b4:	add	x25, x25, x23, lsl #3
   4e9b8:	mov	x26, xzr
   4e9bc:	stur	x22, [x29, #-16]
   4e9c0:	str	x27, [sp, #56]
   4e9c4:	b.le	4ec44 <__gmpn_mu_bdiv_qr@@Base+0x320>
   4e9c8:	add	x8, x28, x23
   4e9cc:	str	x8, [sp, #40]
   4e9d0:	lsl	x8, x28, #3
   4e9d4:	sub	x9, x23, x28
   4e9d8:	str	x9, [sp, #32]
   4e9dc:	lsl	x9, x23, #3
   4e9e0:	add	x12, x19, x8
   4e9e4:	add	x8, x22, x8
   4e9e8:	stp	x8, x12, [sp, #16]
   4e9ec:	add	x8, x22, x9
   4e9f0:	mvn	x10, x27
   4e9f4:	add	x11, x23, x27, lsl #1
   4e9f8:	stur	x8, [x29, #-24]
   4e9fc:	add	x8, x19, x9
   4ea00:	add	x9, x21, x11, lsl #3
   4ea04:	add	x8, x8, x10, lsl #3
   4ea08:	str	x8, [sp, #48]
   4ea0c:	add	x8, x9, #0x18
   4ea10:	mov	x22, x20
   4ea14:	mov	x27, x24
   4ea18:	str	x8, [sp, #8]
   4ea1c:	b	4ea50 <__gmpn_mu_bdiv_qr@@Base+0x12c>
   4ea20:	ldur	x19, [x29, #-8]
   4ea24:	ldur	x21, [x29, #-32]
   4ea28:	ldr	x0, [sp, #48]
   4ea2c:	ldur	x2, [x29, #-24]
   4ea30:	mov	x1, x25
   4ea34:	mov	x3, x28
   4ea38:	mov	x4, x26
   4ea3c:	bl	c760 <__gmpn_sub_nc@plt>
   4ea40:	mov	x26, x0
   4ea44:	cmp	x27, x28
   4ea48:	add	x25, x25, x28, lsl #3
   4ea4c:	b.le	4ec4c <__gmpn_mu_bdiv_qr@@Base+0x328>
   4ea50:	mov	x0, x22
   4ea54:	mov	x1, x19
   4ea58:	mov	x2, x21
   4ea5c:	mov	x3, x28
   4ea60:	bl	cec0 <__gmpn_mullo_n@plt>
   4ea64:	ldr	x8, [sp, #56]
   4ea68:	cmp	x8, #0x10
   4ea6c:	b.le	4eaf4 <__gmpn_mu_bdiv_qr@@Base+0x1d0>
   4ea70:	mov	x0, x23
   4ea74:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4ea78:	mov	x21, x0
   4ea7c:	ldur	x0, [x29, #-16]
   4ea80:	ldur	x2, [x29, #-48]
   4ea84:	mov	x1, x21
   4ea88:	mov	x3, x23
   4ea8c:	add	x19, x0, x21, lsl #3
   4ea90:	mov	x4, x22
   4ea94:	mov	x5, x28
   4ea98:	mov	x6, x19
   4ea9c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4eaa0:	ldr	x8, [sp, #40]
   4eaa4:	sub	x23, x8, x21
   4eaa8:	cmp	x23, #0x1
   4eaac:	b.lt	4eb0c <__gmpn_mu_bdiv_qr@@Base+0x1e8>  // b.tstop
   4eab0:	mov	x0, x19
   4eab4:	ldp	x19, x2, [x29, #-16]
   4eab8:	mov	x3, x23
   4eabc:	mov	x1, x19
   4eac0:	bl	c2d0 <__gmpn_sub_n@plt>
   4eac4:	lsl	x8, x23, #3
   4eac8:	ldr	x9, [x19, x8]
   4eacc:	subs	x9, x9, x0
   4ead0:	str	x9, [x19, x8]
   4ead4:	b.cs	4eb0c <__gmpn_mu_bdiv_qr@@Base+0x1e8>  // b.hs, b.nlast
   4ead8:	ldr	x8, [sp, #8]
   4eadc:	sub	x8, x8, x21, lsl #3
   4eae0:	ldr	x9, [x8]
   4eae4:	sub	x10, x9, #0x1
   4eae8:	str	x10, [x8], #8
   4eaec:	cbz	x9, 4eae0 <__gmpn_mu_bdiv_qr@@Base+0x1bc>
   4eaf0:	b	4eb0c <__gmpn_mu_bdiv_qr@@Base+0x1e8>
   4eaf4:	ldur	x0, [x29, #-16]
   4eaf8:	ldur	x1, [x29, #-48]
   4eafc:	mov	x2, x23
   4eb00:	mov	x3, x22
   4eb04:	mov	x4, x28
   4eb08:	bl	ccd0 <__gmpn_mul@plt>
   4eb0c:	ldur	x23, [x29, #-40]
   4eb10:	add	x22, x22, x28, lsl #3
   4eb14:	sub	x27, x27, x28
   4eb18:	cmp	x28, x23
   4eb1c:	b.eq	4ea20 <__gmpn_mu_bdiv_qr@@Base+0xfc>  // b.none
   4eb20:	ldur	x19, [x29, #-8]
   4eb24:	ldp	x2, x1, [sp, #16]
   4eb28:	ldr	x3, [sp, #32]
   4eb2c:	mov	x0, x19
   4eb30:	bl	c2d0 <__gmpn_sub_n@plt>
   4eb34:	ldur	x21, [x29, #-32]
   4eb38:	add	x26, x0, x26
   4eb3c:	cmp	x26, #0x2
   4eb40:	b.ne	4ea28 <__gmpn_mu_bdiv_qr@@Base+0x104>  // b.any
   4eb44:	ldur	x8, [x29, #-24]
   4eb48:	ldr	x9, [x8]
   4eb4c:	adds	x9, x9, #0x1
   4eb50:	str	x9, [x8], #8
   4eb54:	b.cs	4eb48 <__gmpn_mu_bdiv_qr@@Base+0x224>  // b.hs, b.nlast
   4eb58:	mov	w26, #0x1                   	// #1
   4eb5c:	b	4ea28 <__gmpn_mu_bdiv_qr@@Base+0x104>
   4eb60:	asr	x22, x24, #1
   4eb64:	sub	x26, x24, x22
   4eb68:	add	x27, x21, x26, lsl #3
   4eb6c:	stur	x3, [x29, #-24]
   4eb70:	mov	x0, x21
   4eb74:	mov	x1, x28
   4eb78:	mov	x2, x26
   4eb7c:	mov	x3, x27
   4eb80:	bl	cd20 <__gmpn_binvert@plt>
   4eb84:	mov	x0, x20
   4eb88:	mov	x1, x25
   4eb8c:	mov	x2, x21
   4eb90:	mov	x3, x26
   4eb94:	bl	cec0 <__gmpn_mullo_n@plt>
   4eb98:	cmp	x26, #0x11
   4eb9c:	b.le	4ed0c <__gmpn_mu_bdiv_qr@@Base+0x3e8>
   4eba0:	mov	x0, x23
   4eba4:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4eba8:	mov	x21, x0
   4ebac:	mov	x19, x23
   4ebb0:	add	x23, x27, x0, lsl #3
   4ebb4:	mov	x0, x27
   4ebb8:	mov	x1, x21
   4ebbc:	mov	x2, x28
   4ebc0:	mov	x3, x19
   4ebc4:	mov	x4, x20
   4ebc8:	mov	x5, x26
   4ebcc:	mov	x6, x23
   4ebd0:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4ebd4:	add	x8, x26, x19
   4ebd8:	sub	x19, x8, x21
   4ebdc:	cmp	x19, #0x1
   4ebe0:	b.lt	4ed24 <__gmpn_mu_bdiv_qr@@Base+0x400>  // b.tstop
   4ebe4:	mov	x0, x23
   4ebe8:	mov	x1, x27
   4ebec:	mov	x2, x25
   4ebf0:	mov	x3, x19
   4ebf4:	bl	c2d0 <__gmpn_sub_n@plt>
   4ebf8:	lsl	x8, x19, #3
   4ebfc:	ldr	x9, [x27, x8]
   4ec00:	subs	x9, x9, x0
   4ec04:	str	x9, [x27, x8]
   4ec08:	b.cs	4ed24 <__gmpn_mu_bdiv_qr@@Base+0x400>  // b.hs, b.nlast
   4ec0c:	ldur	x8, [x29, #-24]
   4ec10:	ldur	x9, [x29, #-40]
   4ec14:	lsl	x8, x8, #1
   4ec18:	add	x9, x21, x9
   4ec1c:	sub	x8, x8, x9
   4ec20:	ldur	x9, [x29, #-32]
   4ec24:	sub	x8, x8, x22, lsl #1
   4ec28:	add	x8, x9, x8, lsl #3
   4ec2c:	add	x8, x8, #0x8
   4ec30:	ldr	x9, [x8]
   4ec34:	sub	x10, x9, #0x1
   4ec38:	str	x10, [x8], #8
   4ec3c:	cbz	x9, 4ec30 <__gmpn_mu_bdiv_qr@@Base+0x30c>
   4ec40:	b	4ed24 <__gmpn_mu_bdiv_qr@@Base+0x400>
   4ec44:	mov	x27, x24
   4ec48:	mov	x22, x20
   4ec4c:	mov	x0, x22
   4ec50:	mov	x1, x19
   4ec54:	mov	x2, x21
   4ec58:	mov	x3, x27
   4ec5c:	bl	cec0 <__gmpn_mullo_n@plt>
   4ec60:	cmp	x27, #0x11
   4ec64:	b.le	4edf4 <__gmpn_mu_bdiv_qr@@Base+0x4d0>
   4ec68:	mov	x0, x23
   4ec6c:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4ec70:	mov	x21, x0
   4ec74:	ldur	x0, [x29, #-16]
   4ec78:	ldur	x28, [x29, #-48]
   4ec7c:	mov	x19, x23
   4ec80:	mov	x1, x21
   4ec84:	add	x23, x0, x21, lsl #3
   4ec88:	mov	x2, x28
   4ec8c:	mov	x3, x19
   4ec90:	mov	x4, x22
   4ec94:	mov	x5, x27
   4ec98:	mov	x6, x23
   4ec9c:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4eca0:	add	x8, x27, x19
   4eca4:	sub	x19, x8, x21
   4eca8:	cmp	x19, #0x1
   4ecac:	b.lt	4ee10 <__gmpn_mu_bdiv_qr@@Base+0x4ec>  // b.tstop
   4ecb0:	ldp	x22, x2, [x29, #-16]
   4ecb4:	mov	x0, x23
   4ecb8:	mov	x3, x19
   4ecbc:	mov	x1, x22
   4ecc0:	bl	c2d0 <__gmpn_sub_n@plt>
   4ecc4:	lsl	x8, x19, #3
   4ecc8:	ldr	x9, [x22, x8]
   4eccc:	subs	x9, x9, x0
   4ecd0:	str	x9, [x22, x8]
   4ecd4:	b.cs	4ee10 <__gmpn_mu_bdiv_qr@@Base+0x4ec>  // b.hs, b.nlast
   4ecd8:	ldr	x8, [sp, #56]
   4ecdc:	ldur	x9, [x29, #-40]
   4ece0:	add	x8, x27, x8
   4ece4:	add	x8, x8, x9
   4ece8:	ldur	x9, [x29, #-32]
   4ecec:	sub	x8, x8, x21
   4ecf0:	add	x8, x9, x8, lsl #3
   4ecf4:	add	x8, x8, #0x10
   4ecf8:	ldr	x9, [x8]
   4ecfc:	sub	x10, x9, #0x1
   4ed00:	str	x10, [x8], #8
   4ed04:	cbz	x9, 4ecf8 <__gmpn_mu_bdiv_qr@@Base+0x3d4>
   4ed08:	b	4ee10 <__gmpn_mu_bdiv_qr@@Base+0x4ec>
   4ed0c:	mov	x0, x27
   4ed10:	mov	x1, x28
   4ed14:	mov	x2, x23
   4ed18:	mov	x3, x20
   4ed1c:	mov	x4, x26
   4ed20:	bl	ccd0 <__gmpn_mul@plt>
   4ed24:	ldur	x21, [x29, #-8]
   4ed28:	ldur	x23, [x29, #-40]
   4ed2c:	lsl	x8, x26, #3
   4ed30:	add	x1, x25, x8
   4ed34:	add	x2, x27, x8
   4ed38:	mov	x0, x21
   4ed3c:	mov	x3, x23
   4ed40:	add	x19, x20, x8
   4ed44:	bl	c2d0 <__gmpn_sub_n@plt>
   4ed48:	ldur	x2, [x29, #-32]
   4ed4c:	stur	x0, [x29, #-16]
   4ed50:	mov	x0, x19
   4ed54:	mov	x1, x21
   4ed58:	mov	x3, x22
   4ed5c:	bl	cec0 <__gmpn_mullo_n@plt>
   4ed60:	cmp	x24, #0x23
   4ed64:	b.le	4ee24 <__gmpn_mu_bdiv_qr@@Base+0x500>
   4ed68:	mov	x0, x23
   4ed6c:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4ed70:	mov	x2, x28
   4ed74:	mov	x28, x0
   4ed78:	add	x21, x27, x0, lsl #3
   4ed7c:	mov	x0, x27
   4ed80:	mov	x1, x28
   4ed84:	mov	x3, x23
   4ed88:	mov	x4, x19
   4ed8c:	mov	x5, x22
   4ed90:	mov	x6, x21
   4ed94:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   4ed98:	add	x8, x22, x23
   4ed9c:	sub	x19, x8, x28
   4eda0:	cmp	x19, #0x1
   4eda4:	b.lt	4ee3c <__gmpn_mu_bdiv_qr@@Base+0x518>  // b.tstop
   4eda8:	ldur	x2, [x29, #-8]
   4edac:	mov	x0, x21
   4edb0:	mov	x1, x27
   4edb4:	mov	x3, x19
   4edb8:	bl	c2d0 <__gmpn_sub_n@plt>
   4edbc:	lsl	x8, x19, #3
   4edc0:	ldr	x9, [x27, x8]
   4edc4:	subs	x9, x9, x0
   4edc8:	str	x9, [x27, x8]
   4edcc:	b.cs	4ee3c <__gmpn_mu_bdiv_qr@@Base+0x518>  // b.hs, b.nlast
   4edd0:	ldp	x9, x8, [x29, #-32]
   4edd4:	sub	x8, x8, x28
   4edd8:	add	x8, x9, x8, lsl #3
   4eddc:	add	x8, x8, #0x8
   4ede0:	ldr	x9, [x8]
   4ede4:	sub	x10, x9, #0x1
   4ede8:	str	x10, [x8], #8
   4edec:	cbz	x9, 4ede0 <__gmpn_mu_bdiv_qr@@Base+0x4bc>
   4edf0:	b	4ee3c <__gmpn_mu_bdiv_qr@@Base+0x518>
   4edf4:	ldur	x28, [x29, #-48]
   4edf8:	ldur	x0, [x29, #-16]
   4edfc:	mov	x2, x23
   4ee00:	mov	x3, x22
   4ee04:	mov	x1, x28
   4ee08:	mov	x4, x27
   4ee0c:	bl	ccd0 <__gmpn_mul@plt>
   4ee10:	ldur	x23, [x29, #-40]
   4ee14:	cmp	x27, x23
   4ee18:	b.ne	4eea4 <__gmpn_mu_bdiv_qr@@Base+0x580>  // b.any
   4ee1c:	ldp	x21, x19, [x29, #-16]
   4ee20:	b	4eee4 <__gmpn_mu_bdiv_qr@@Base+0x5c0>
   4ee24:	mov	x0, x27
   4ee28:	mov	x1, x28
   4ee2c:	mov	x2, x23
   4ee30:	mov	x3, x19
   4ee34:	mov	x4, x22
   4ee38:	bl	ccd0 <__gmpn_mul@plt>
   4ee3c:	ldur	x19, [x29, #-8]
   4ee40:	ldur	x8, [x29, #-32]
   4ee44:	sub	x3, x23, x22
   4ee48:	add	x1, x19, x22, lsl #3
   4ee4c:	add	x2, x8, x24, lsl #3
   4ee50:	mov	x0, x19
   4ee54:	bl	c2d0 <__gmpn_sub_n@plt>
   4ee58:	ldur	x8, [x29, #-16]
   4ee5c:	ldur	x28, [x29, #-48]
   4ee60:	add	x4, x0, x8
   4ee64:	cmp	x4, #0x2
   4ee68:	b.ne	4ee84 <__gmpn_mu_bdiv_qr@@Base+0x560>  // b.any
   4ee6c:	add	x8, x27, x23, lsl #3
   4ee70:	mov	w4, #0x1                   	// #1
   4ee74:	ldr	x9, [x8]
   4ee78:	adds	x9, x9, #0x1
   4ee7c:	str	x9, [x8], #8
   4ee80:	b.cs	4ee74 <__gmpn_mu_bdiv_qr@@Base+0x550>  // b.hs, b.nlast
   4ee84:	lsl	x8, x23, #3
   4ee88:	add	x9, x19, x8
   4ee8c:	add	x10, x25, x8
   4ee90:	sub	x0, x9, x22, lsl #3
   4ee94:	add	x1, x10, x26, lsl #3
   4ee98:	add	x2, x27, x8
   4ee9c:	mov	x3, x22
   4eea0:	b	4ef00 <__gmpn_mu_bdiv_qr@@Base+0x5dc>
   4eea4:	ldp	x21, x19, [x29, #-16]
   4eea8:	lsl	x8, x27, #3
   4eeac:	sub	x3, x23, x27
   4eeb0:	add	x1, x19, x8
   4eeb4:	add	x2, x21, x8
   4eeb8:	mov	x0, x19
   4eebc:	bl	c2d0 <__gmpn_sub_n@plt>
   4eec0:	add	x26, x0, x26
   4eec4:	cmp	x26, #0x2
   4eec8:	b.ne	4eee4 <__gmpn_mu_bdiv_qr@@Base+0x5c0>  // b.any
   4eecc:	add	x8, x21, x23, lsl #3
   4eed0:	mov	w26, #0x1                   	// #1
   4eed4:	ldr	x9, [x8]
   4eed8:	adds	x9, x9, #0x1
   4eedc:	str	x9, [x8], #8
   4eee0:	b.cs	4eed4 <__gmpn_mu_bdiv_qr@@Base+0x5b0>  // b.hs, b.nlast
   4eee4:	lsl	x8, x23, #3
   4eee8:	add	x9, x19, x8
   4eeec:	sub	x0, x9, x27, lsl #3
   4eef0:	add	x2, x21, x8
   4eef4:	mov	x1, x25
   4eef8:	mov	x3, x27
   4eefc:	mov	x4, x26
   4ef00:	bl	c760 <__gmpn_sub_nc@plt>
   4ef04:	ldr	x8, [x20]
   4ef08:	mov	x21, x0
   4ef0c:	cbz	x8, 4ef6c <__gmpn_mu_bdiv_qr@@Base+0x648>
   4ef10:	neg	x8, x8
   4ef14:	subs	x2, x24, #0x1
   4ef18:	str	x8, [x20]
   4ef1c:	b.eq	4ef2c <__gmpn_mu_bdiv_qr@@Base+0x608>  // b.none
   4ef20:	add	x0, x20, #0x8
   4ef24:	mov	x1, x0
   4ef28:	bl	c290 <__gmpn_com@plt>
   4ef2c:	mov	x0, x19
   4ef30:	mov	x1, x19
   4ef34:	mov	x2, x28
   4ef38:	mov	x3, x23
   4ef3c:	bl	ca70 <__gmpn_add_n@plt>
   4ef40:	sub	x0, x0, x21
   4ef44:	ldp	x20, x19, [sp, #192]
   4ef48:	ldp	x22, x21, [sp, #176]
   4ef4c:	ldp	x24, x23, [sp, #160]
   4ef50:	ldp	x26, x25, [sp, #144]
   4ef54:	ldp	x28, x27, [sp, #128]
   4ef58:	ldp	x29, x30, [sp, #112]
   4ef5c:	add	sp, sp, #0xd0
   4ef60:	ret
   4ef64:	ldr	x8, [x20, #8]!
   4ef68:	cbnz	x8, 4ef10 <__gmpn_mu_bdiv_qr@@Base+0x5ec>
   4ef6c:	subs	x24, x24, #0x1
   4ef70:	str	xzr, [x20]
   4ef74:	b.ne	4ef64 <__gmpn_mu_bdiv_qr@@Base+0x640>  // b.any
   4ef78:	mov	x0, xzr
   4ef7c:	b	4ef44 <__gmpn_mu_bdiv_qr@@Base+0x620>

000000000004ef80 <__gmpn_mu_bdiv_qr_itch@@Base>:
   4ef80:	stp	x29, x30, [sp, #-48]!
   4ef84:	sub	x8, x0, x1
   4ef88:	stp	x20, x19, [sp, #32]
   4ef8c:	mov	x20, x1
   4ef90:	cmp	x8, x1
   4ef94:	stp	x22, x21, [sp, #16]
   4ef98:	mov	x29, sp
   4ef9c:	b.le	4efc8 <__gmpn_mu_bdiv_qr_itch@@Base+0x48>
   4efa0:	sub	x8, x8, #0x1
   4efa4:	sdiv	x9, x8, x20
   4efa8:	add	x9, x9, #0x1
   4efac:	sdiv	x8, x8, x9
   4efb0:	add	x19, x8, #0x1
   4efb4:	cmp	x19, #0x11
   4efb8:	b.gt	4efd4 <__gmpn_mu_bdiv_qr_itch@@Base+0x54>
   4efbc:	mov	x22, xzr
   4efc0:	add	x21, x19, x20
   4efc4:	b	4effc <__gmpn_mu_bdiv_qr_itch@@Base+0x7c>
   4efc8:	sub	x19, x8, x8, asr #1
   4efcc:	cmp	x19, #0x11
   4efd0:	b.le	4efbc <__gmpn_mu_bdiv_qr_itch@@Base+0x3c>
   4efd4:	mov	x0, x20
   4efd8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4efdc:	asr	x8, x0, #1
   4efe0:	cmp	x8, x19
   4efe4:	csel	x9, x0, x8, lt  // lt = tstop
   4efe8:	cmp	x8, x20
   4efec:	csel	x8, x9, xzr, lt  // lt = tstop
   4eff0:	add	x8, x0, x8
   4eff4:	mov	x21, x0
   4eff8:	add	x22, x8, #0x4
   4effc:	mov	x0, x19
   4f000:	bl	d1e0 <__gmpn_binvert_itch@plt>
   4f004:	add	x8, x21, x22
   4f008:	cmp	x8, x0
   4f00c:	csel	x8, x8, x0, gt
   4f010:	add	x0, x8, x19
   4f014:	ldp	x20, x19, [sp, #32]
   4f018:	ldp	x22, x21, [sp, #16]
   4f01c:	ldp	x29, x30, [sp], #48
   4f020:	ret

000000000004f024 <__gmpn_bdiv_q@@Base>:
   4f024:	stp	x29, x30, [sp, #-64]!
   4f028:	str	x23, [sp, #16]
   4f02c:	stp	x22, x21, [sp, #32]
   4f030:	stp	x20, x19, [sp, #48]
   4f034:	mov	x21, x5
   4f038:	mov	x19, x4
   4f03c:	mov	x20, x3
   4f040:	mov	x22, x2
   4f044:	cmp	x4, #0x5c
   4f048:	mov	x23, x0
   4f04c:	mov	x29, sp
   4f050:	b.le	4f084 <__gmpn_bdiv_q@@Base+0x60>
   4f054:	cmp	x19, #0x39b
   4f058:	b.le	4f0ec <__gmpn_bdiv_q@@Base+0xc8>
   4f05c:	mov	x0, x23
   4f060:	mov	x2, x22
   4f064:	mov	x3, x20
   4f068:	mov	x4, x19
   4f06c:	mov	x5, x21
   4f070:	ldp	x20, x19, [sp, #48]
   4f074:	ldp	x22, x21, [sp, #32]
   4f078:	ldr	x23, [sp, #16]
   4f07c:	ldp	x29, x30, [sp], #64
   4f080:	b	d3c0 <__gmpn_mu_bdiv_q@plt>
   4f084:	mov	x0, x21
   4f088:	mov	x2, x22
   4f08c:	bl	ca50 <__gmpn_copyi@plt>
   4f090:	ldr	x8, [x20]
   4f094:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f098:	ldr	x9, [x9, #3952]
   4f09c:	mov	x0, x23
   4f0a0:	ubfx	x10, x8, #1, #7
   4f0a4:	mov	x1, x21
   4f0a8:	ldrb	w9, [x9, x10]
   4f0ac:	mov	w10, #0x2                   	// #2
   4f0b0:	mov	x2, x22
   4f0b4:	mov	x3, x20
   4f0b8:	msub	x11, x8, x9, x10
   4f0bc:	mul	x9, x11, x9
   4f0c0:	msub	x10, x9, x8, x10
   4f0c4:	mov	x4, x19
   4f0c8:	ldp	x20, x19, [sp, #48]
   4f0cc:	ldp	x22, x21, [sp, #32]
   4f0d0:	ldr	x23, [sp, #16]
   4f0d4:	mul	x9, x9, x10
   4f0d8:	orr	x10, xzr, #0xfffffffffffffffe
   4f0dc:	madd	x8, x9, x8, x10
   4f0e0:	mul	x5, x8, x9
   4f0e4:	ldp	x29, x30, [sp], #64
   4f0e8:	b	c510 <__gmpn_sbpi1_bdiv_q@plt>
   4f0ec:	mov	x0, x21
   4f0f0:	mov	x2, x22
   4f0f4:	bl	ca50 <__gmpn_copyi@plt>
   4f0f8:	ldr	x8, [x20]
   4f0fc:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f100:	ldr	x9, [x9, #3952]
   4f104:	mov	x0, x23
   4f108:	ubfx	x10, x8, #1, #7
   4f10c:	mov	x1, x21
   4f110:	ldrb	w9, [x9, x10]
   4f114:	mov	w10, #0x2                   	// #2
   4f118:	mov	x2, x22
   4f11c:	mov	x3, x20
   4f120:	msub	x11, x8, x9, x10
   4f124:	mul	x9, x11, x9
   4f128:	msub	x10, x9, x8, x10
   4f12c:	mov	x4, x19
   4f130:	ldp	x20, x19, [sp, #48]
   4f134:	ldp	x22, x21, [sp, #32]
   4f138:	ldr	x23, [sp, #16]
   4f13c:	mul	x9, x9, x10
   4f140:	orr	x10, xzr, #0xfffffffffffffffe
   4f144:	madd	x8, x9, x8, x10
   4f148:	mul	x5, x8, x9
   4f14c:	ldp	x29, x30, [sp], #64
   4f150:	b	ce10 <__gmpn_dcpi1_bdiv_q@plt>

000000000004f154 <__gmpn_bdiv_q_itch@@Base>:
   4f154:	cmp	x1, #0x39c
   4f158:	b.lt	4f160 <__gmpn_bdiv_q_itch@@Base+0xc>  // b.tstop
   4f15c:	b	c880 <__gmpn_mu_bdiv_q_itch@plt>
   4f160:	ret

000000000004f164 <__gmpn_bdiv_qr@@Base>:
   4f164:	stp	x29, x30, [sp, #-64]!
   4f168:	stp	x24, x23, [sp, #16]
   4f16c:	stp	x22, x21, [sp, #32]
   4f170:	stp	x20, x19, [sp, #48]
   4f174:	mov	x20, x6
   4f178:	mov	x19, x5
   4f17c:	mov	x23, x4
   4f180:	mov	x22, x3
   4f184:	mov	x21, x1
   4f188:	cmp	x5, #0x27
   4f18c:	mov	x24, x0
   4f190:	mov	x29, sp
   4f194:	b.lt	4f1d8 <__gmpn_bdiv_qr@@Base+0x74>  // b.tstop
   4f198:	sub	x8, x22, x19
   4f19c:	cmp	x8, #0x26
   4f1a0:	b.le	4f1d8 <__gmpn_bdiv_qr@@Base+0x74>
   4f1a4:	cmp	x19, #0x326
   4f1a8:	b.le	4f238 <__gmpn_bdiv_qr@@Base+0xd4>
   4f1ac:	mov	x0, x24
   4f1b0:	mov	x1, x21
   4f1b4:	mov	x3, x22
   4f1b8:	mov	x4, x23
   4f1bc:	mov	x5, x19
   4f1c0:	mov	x6, x20
   4f1c4:	ldp	x20, x19, [sp, #48]
   4f1c8:	ldp	x22, x21, [sp, #32]
   4f1cc:	ldp	x24, x23, [sp, #16]
   4f1d0:	ldp	x29, x30, [sp], #64
   4f1d4:	b	ccb0 <__gmpn_mu_bdiv_qr@plt>
   4f1d8:	mov	x0, x20
   4f1dc:	mov	x1, x2
   4f1e0:	mov	x2, x22
   4f1e4:	bl	ca50 <__gmpn_copyi@plt>
   4f1e8:	ldr	x8, [x23]
   4f1ec:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f1f0:	ldr	x9, [x9, #3952]
   4f1f4:	mov	x0, x24
   4f1f8:	ubfx	x10, x8, #1, #7
   4f1fc:	mov	x1, x20
   4f200:	ldrb	w9, [x9, x10]
   4f204:	mov	w10, #0x2                   	// #2
   4f208:	mov	x2, x22
   4f20c:	mov	x3, x23
   4f210:	msub	x11, x8, x9, x10
   4f214:	mul	x9, x11, x9
   4f218:	msub	x10, x9, x8, x10
   4f21c:	mul	x9, x9, x10
   4f220:	orr	x10, xzr, #0xfffffffffffffffe
   4f224:	madd	x8, x9, x8, x10
   4f228:	mul	x5, x8, x9
   4f22c:	mov	x4, x19
   4f230:	bl	c820 <__gmpn_sbpi1_bdiv_qr@plt>
   4f234:	b	4f294 <__gmpn_bdiv_qr@@Base+0x130>
   4f238:	mov	x0, x20
   4f23c:	mov	x1, x2
   4f240:	mov	x2, x22
   4f244:	bl	ca50 <__gmpn_copyi@plt>
   4f248:	ldr	x8, [x23]
   4f24c:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f250:	ldr	x9, [x9, #3952]
   4f254:	mov	x0, x24
   4f258:	ubfx	x10, x8, #1, #7
   4f25c:	mov	x1, x20
   4f260:	ldrb	w9, [x9, x10]
   4f264:	mov	w10, #0x2                   	// #2
   4f268:	mov	x2, x22
   4f26c:	mov	x3, x23
   4f270:	msub	x11, x8, x9, x10
   4f274:	mul	x9, x11, x9
   4f278:	msub	x10, x9, x8, x10
   4f27c:	mul	x9, x9, x10
   4f280:	orr	x10, xzr, #0xfffffffffffffffe
   4f284:	madd	x8, x9, x8, x10
   4f288:	mul	x5, x8, x9
   4f28c:	mov	x4, x19
   4f290:	bl	c5e0 <__gmpn_dcpi1_bdiv_qr@plt>
   4f294:	add	x8, x20, x22, lsl #3
   4f298:	mov	x23, x0
   4f29c:	sub	x1, x8, x19, lsl #3
   4f2a0:	mov	x0, x21
   4f2a4:	mov	x2, x19
   4f2a8:	bl	ca50 <__gmpn_copyi@plt>
   4f2ac:	mov	x0, x23
   4f2b0:	ldp	x20, x19, [sp, #48]
   4f2b4:	ldp	x22, x21, [sp, #32]
   4f2b8:	ldp	x24, x23, [sp, #16]
   4f2bc:	ldp	x29, x30, [sp], #64
   4f2c0:	ret

000000000004f2c4 <__gmpn_bdiv_qr_itch@@Base>:
   4f2c4:	cmp	x1, #0x327
   4f2c8:	b.lt	4f2d0 <__gmpn_bdiv_qr_itch@@Base+0xc>  // b.tstop
   4f2cc:	b	d170 <__gmpn_mu_bdiv_qr_itch@plt>
   4f2d0:	ret

000000000004f2d4 <__gmpn_broot_invm1@@Base>:
   4f2d4:	stp	x29, x30, [sp, #-96]!
   4f2d8:	stp	x28, x27, [sp, #16]
   4f2dc:	stp	x26, x25, [sp, #32]
   4f2e0:	stp	x24, x23, [sp, #48]
   4f2e4:	stp	x22, x21, [sp, #64]
   4f2e8:	stp	x20, x19, [sp, #80]
   4f2ec:	mov	x29, sp
   4f2f0:	sub	sp, sp, #0x440
   4f2f4:	mov	x23, x1
   4f2f8:	lsl	x1, x2, #5
   4f2fc:	mov	w8, #0x7f00                	// #32512
   4f300:	mov	x19, sp
   4f304:	mov	x20, x3
   4f308:	mov	x25, x2
   4f30c:	cmp	x1, x8
   4f310:	stp	x0, xzr, [x19, #24]
   4f314:	b.hi	4f5c4 <__gmpn_broot_invm1@@Base+0x2f0>  // b.pmore
   4f318:	add	x9, x1, #0xf
   4f31c:	mov	x8, sp
   4f320:	and	x9, x9, #0xfffffffffffffff0
   4f324:	sub	x22, x8, x9
   4f328:	mov	sp, x22
   4f32c:	add	x5, x22, x25, lsl #3
   4f330:	sub	x8, x20, #0x1
   4f334:	add	x2, x19, #0x30
   4f338:	mov	w3, #0x1                   	// #1
   4f33c:	mov	x0, x22
   4f340:	mov	x1, x23
   4f344:	mov	x4, x25
   4f348:	str	x8, [x19, #48]
   4f34c:	mov	w21, #0x1                   	// #1
   4f350:	str	x5, [x19, #16]
   4f354:	bl	c3c0 <__gmpn_powlo@plt>
   4f358:	adrp	x11, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f35c:	ldr	w9, [x23]
   4f360:	ldr	x11, [x11, #3952]
   4f364:	ubfx	x10, x20, #1, #7
   4f368:	mov	w13, #0x2                   	// #2
   4f36c:	lsl	w12, w9, #1
   4f370:	ldrb	w11, [x11, x10]
   4f374:	eor	w9, w12, w9, lsl #2
   4f378:	and	w9, w9, w20, lsl #2
   4f37c:	ldr	x8, [x22]
   4f380:	msub	x12, x11, x20, x13
   4f384:	mul	x11, x12, x11
   4f388:	msub	x12, x11, x20, x13
   4f38c:	and	x9, x9, #0x8
   4f390:	mul	x11, x11, x12
   4f394:	orr	x12, x9, #0x1
   4f398:	msub	x9, x11, x20, x13
   4f39c:	mul	x24, x11, x9
   4f3a0:	ands	x10, x20, #0x7f
   4f3a4:	mul	x11, x24, x12
   4f3a8:	add	x9, x20, #0x1
   4f3ac:	b.eq	4f3cc <__gmpn_broot_invm1@@Base+0xf8>  // b.none
   4f3b0:	mov	w21, #0x1                   	// #1
   4f3b4:	tst	x10, #0x1
   4f3b8:	csinc	x13, x12, xzr, ne  // ne = any
   4f3bc:	lsr	x10, x10, #1
   4f3c0:	mul	x21, x13, x21
   4f3c4:	mul	x12, x12, x12
   4f3c8:	cbnz	x10, 4f3b4 <__gmpn_broot_invm1@@Base+0xe0>
   4f3cc:	msub	x10, x21, x8, x9
   4f3d0:	mul	x11, x11, x10
   4f3d4:	ands	x13, x20, #0x7fff
   4f3d8:	mul	x10, x11, x24
   4f3dc:	mov	w12, #0x1                   	// #1
   4f3e0:	b.eq	4f3fc <__gmpn_broot_invm1@@Base+0x128>  // b.none
   4f3e4:	tst	x13, #0x1
   4f3e8:	csinc	x14, x11, xzr, ne  // ne = any
   4f3ec:	lsr	x13, x13, #1
   4f3f0:	mul	x12, x14, x12
   4f3f4:	mul	x11, x11, x11
   4f3f8:	cbnz	x13, 4f3e4 <__gmpn_broot_invm1@@Base+0x110>
   4f3fc:	msub	x11, x12, x8, x9
   4f400:	mul	x11, x10, x11
   4f404:	mul	x10, x11, x24
   4f408:	cbz	x20, 4f45c <__gmpn_broot_invm1@@Base+0x188>
   4f40c:	mov	w12, #0x1                   	// #1
   4f410:	mov	x13, x20
   4f414:	tst	x13, #0x1
   4f418:	csinc	x14, x11, xzr, ne  // ne = any
   4f41c:	lsr	x13, x13, #1
   4f420:	mul	x12, x14, x12
   4f424:	mul	x11, x11, x11
   4f428:	cbnz	x13, 4f414 <__gmpn_broot_invm1@@Base+0x140>
   4f42c:	msub	x11, x12, x8, x9
   4f430:	mul	x10, x11, x10
   4f434:	mov	w11, #0x1                   	// #1
   4f438:	mov	x12, x10
   4f43c:	mov	x13, x20
   4f440:	tst	x13, #0x1
   4f444:	csinc	x14, x12, xzr, ne  // ne = any
   4f448:	lsr	x13, x13, #1
   4f44c:	mul	x11, x14, x11
   4f450:	mul	x12, x12, x12
   4f454:	cbnz	x13, 4f440 <__gmpn_broot_invm1@@Base+0x16c>
   4f458:	b	4f468 <__gmpn_broot_invm1@@Base+0x194>
   4f45c:	sub	x11, x9, x8
   4f460:	mul	x10, x10, x11
   4f464:	mov	w11, #0x1                   	// #1
   4f468:	msub	x8, x11, x8, x9
   4f46c:	ldr	x9, [x19, #24]
   4f470:	mul	x10, x10, x24
   4f474:	mul	x8, x10, x8
   4f478:	cmp	x25, #0x1
   4f47c:	str	x8, [x9]
   4f480:	b.eq	4f5e8 <__gmpn_broot_invm1@@Base+0x314>  // b.none
   4f484:	mov	w1, #0x8                   	// #8
   4f488:	lsr	x8, x20, #1
   4f48c:	bfi	x1, x25, #4, #60
   4f490:	mov	w9, #0x7f00                	// #32512
   4f494:	add	x8, x8, #0x1
   4f498:	cmp	x1, x9
   4f49c:	str	x8, [x19, #40]
   4f4a0:	str	x22, [x19, #8]
   4f4a4:	b.hi	4f5d4 <__gmpn_broot_invm1@@Base+0x300>  // b.pmore
   4f4a8:	add	x9, x1, #0xf
   4f4ac:	mov	x8, sp
   4f4b0:	and	x9, x9, #0xfffffffffffffff0
   4f4b4:	sub	x26, x8, x9
   4f4b8:	mov	sp, x26
   4f4bc:	cmp	x25, #0x2
   4f4c0:	b.lt	4f5e8 <__gmpn_broot_invm1@@Base+0x314>  // b.tstop
   4f4c4:	mov	w9, wzr
   4f4c8:	add	x27, x26, x25, lsl #3
   4f4cc:	add	x8, x19, #0x38
   4f4d0:	add	x10, x25, #0x1
   4f4d4:	add	x11, x25, #0x2
   4f4d8:	cmp	x10, #0x0
   4f4dc:	csinc	x10, x11, x25, lt  // lt = tstop
   4f4e0:	str	x25, [x8, w9, uxtw #3]
   4f4e4:	cmp	x25, #0x2
   4f4e8:	asr	x25, x10, #1
   4f4ec:	add	w9, w9, #0x1
   4f4f0:	b.gt	4f4d0 <__gmpn_broot_invm1@@Base+0x1fc>
   4f4f4:	cbz	w9, 4f5e8 <__gmpn_broot_invm1@@Base+0x314>
   4f4f8:	mov	w22, w9
   4f4fc:	mov	w25, #0x1                   	// #1
   4f500:	b	4f50c <__gmpn_broot_invm1@@Base+0x238>
   4f504:	sub	x22, x22, #0x1
   4f508:	cbz	w21, 4f5e8 <__gmpn_broot_invm1@@Base+0x314>
   4f50c:	ldr	x28, [x19, #24]
   4f510:	mov	x0, x27
   4f514:	mov	x2, x25
   4f518:	mov	x23, x25
   4f51c:	mov	x1, x28
   4f520:	sub	w21, w22, #0x1
   4f524:	bl	c8e0 <__gmpn_sqr@plt>
   4f528:	add	x8, x19, #0x38
   4f52c:	ldr	x25, [x8, w21, uxtw #3]
   4f530:	ldr	x5, [x19, #16]
   4f534:	add	x2, x19, #0x28
   4f538:	mov	w3, #0x1                   	// #1
   4f53c:	mov	x0, x26
   4f540:	mov	x1, x27
   4f544:	mov	x4, x25
   4f548:	bl	c3c0 <__gmpn_powlo@plt>
   4f54c:	ldr	x2, [x19, #8]
   4f550:	mov	x0, x27
   4f554:	mov	x1, x26
   4f558:	mov	x3, x25
   4f55c:	bl	cec0 <__gmpn_mullo_n@plt>
   4f560:	lsl	x8, x23, #3
   4f564:	add	x28, x28, x8
   4f568:	sub	x23, x25, x23
   4f56c:	add	x1, x27, x8
   4f570:	mov	x0, x28
   4f574:	mov	x2, x23
   4f578:	mov	x3, x20
   4f57c:	mov	x4, x24
   4f580:	mov	w5, wzr
   4f584:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4f588:	ldr	x8, [x28]
   4f58c:	cbnz	x8, 4f5a4 <__gmpn_broot_invm1@@Base+0x2d0>
   4f590:	subs	x23, x23, #0x1
   4f594:	str	xzr, [x28]
   4f598:	b.eq	4f504 <__gmpn_broot_invm1@@Base+0x230>  // b.none
   4f59c:	ldr	x8, [x28, #8]!
   4f5a0:	cbz	x8, 4f590 <__gmpn_broot_invm1@@Base+0x2bc>
   4f5a4:	neg	x8, x8
   4f5a8:	subs	x2, x23, #0x1
   4f5ac:	str	x8, [x28]
   4f5b0:	b.eq	4f504 <__gmpn_broot_invm1@@Base+0x230>  // b.none
   4f5b4:	add	x0, x28, #0x8
   4f5b8:	mov	x1, x0
   4f5bc:	bl	c290 <__gmpn_com@plt>
   4f5c0:	b	4f504 <__gmpn_broot_invm1@@Base+0x230>
   4f5c4:	add	x0, x19, #0x20
   4f5c8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4f5cc:	mov	x22, x0
   4f5d0:	b	4f32c <__gmpn_broot_invm1@@Base+0x58>
   4f5d4:	add	x0, x19, #0x20
   4f5d8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4f5dc:	mov	x26, x0
   4f5e0:	cmp	x25, #0x2
   4f5e4:	b.ge	4f4c4 <__gmpn_broot_invm1@@Base+0x1f0>  // b.tcont
   4f5e8:	ldr	x0, [x19, #32]
   4f5ec:	cbnz	x0, 4f610 <__gmpn_broot_invm1@@Base+0x33c>
   4f5f0:	mov	sp, x29
   4f5f4:	ldp	x20, x19, [sp, #80]
   4f5f8:	ldp	x22, x21, [sp, #64]
   4f5fc:	ldp	x24, x23, [sp, #48]
   4f600:	ldp	x26, x25, [sp, #32]
   4f604:	ldp	x28, x27, [sp, #16]
   4f608:	ldp	x29, x30, [sp], #96
   4f60c:	ret
   4f610:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4f614:	b	4f5f0 <__gmpn_broot_invm1@@Base+0x31c>

000000000004f618 <__gmpn_broot@@Base>:
   4f618:	stp	x29, x30, [sp, #-64]!
   4f61c:	stp	x22, x21, [sp, #32]
   4f620:	stp	x20, x19, [sp, #48]
   4f624:	mov	x19, x2
   4f628:	mov	x20, x1
   4f62c:	cmp	x3, #0x1
   4f630:	mov	x21, x0
   4f634:	str	x23, [sp, #16]
   4f638:	mov	x29, sp
   4f63c:	b.ne	4f654 <__gmpn_broot@@Base+0x3c>  // b.any
   4f640:	mov	x0, x21
   4f644:	mov	x1, x20
   4f648:	mov	x2, x19
   4f64c:	bl	ca50 <__gmpn_copyi@plt>
   4f650:	b	4f6b0 <__gmpn_broot@@Base+0x98>
   4f654:	lsl	x1, x19, #3
   4f658:	mov	w8, #0x7f00                	// #32512
   4f65c:	mov	x22, x3
   4f660:	cmp	x1, x8
   4f664:	str	xzr, [x29, #24]
   4f668:	b.hi	4f6c8 <__gmpn_broot@@Base+0xb0>  // b.pmore
   4f66c:	add	x9, x1, #0xf
   4f670:	mov	x8, sp
   4f674:	and	x9, x9, #0xfffffffffffffff0
   4f678:	sub	x23, x8, x9
   4f67c:	mov	sp, x23
   4f680:	mov	x0, x23
   4f684:	mov	x1, x20
   4f688:	mov	x2, x19
   4f68c:	mov	x3, x22
   4f690:	bl	c930 <__gmpn_broot_invm1@plt>
   4f694:	mov	x0, x21
   4f698:	mov	x1, x23
   4f69c:	mov	x2, x20
   4f6a0:	mov	x3, x19
   4f6a4:	bl	cec0 <__gmpn_mullo_n@plt>
   4f6a8:	ldr	x0, [x29, #24]
   4f6ac:	cbnz	x0, 4f6d8 <__gmpn_broot@@Base+0xc0>
   4f6b0:	mov	sp, x29
   4f6b4:	ldp	x20, x19, [sp, #48]
   4f6b8:	ldp	x22, x21, [sp, #32]
   4f6bc:	ldr	x23, [sp, #16]
   4f6c0:	ldp	x29, x30, [sp], #64
   4f6c4:	ret
   4f6c8:	add	x0, x29, #0x18
   4f6cc:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4f6d0:	mov	x23, x0
   4f6d4:	b	4f680 <__gmpn_broot@@Base+0x68>
   4f6d8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4f6dc:	b	4f6b0 <__gmpn_broot@@Base+0x98>

000000000004f6e0 <__gmpn_brootinv@@Base>:
   4f6e0:	stp	x29, x30, [sp, #-96]!
   4f6e4:	stp	x28, x27, [sp, #16]
   4f6e8:	stp	x26, x25, [sp, #32]
   4f6ec:	stp	x24, x23, [sp, #48]
   4f6f0:	stp	x22, x21, [sp, #64]
   4f6f4:	stp	x20, x19, [sp, #80]
   4f6f8:	mov	x29, sp
   4f6fc:	sub	sp, sp, #0x220
   4f700:	adrp	x12, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   4f704:	ldr	x12, [x12, #3952]
   4f708:	add	x8, x2, #0x3
   4f70c:	lsr	x9, x3, #1
   4f710:	ubfx	x10, x3, #1, #7
   4f714:	asr	x11, x8, #1
   4f718:	add	x8, x9, #0x1
   4f71c:	ldrb	w12, [x12, x10]
   4f720:	stur	x8, [x29, #-16]
   4f724:	ldr	x10, [x1]
   4f728:	mov	w14, #0x2                   	// #2
   4f72c:	msub	x13, x12, x3, x14
   4f730:	mul	x12, x13, x12
   4f734:	msub	x13, x12, x3, x14
   4f738:	lsl	w15, w10, #1
   4f73c:	mul	x13, x12, x13
   4f740:	eor	w12, w15, w10, lsl #2
   4f744:	and	w12, w12, w8, lsl #3
   4f748:	and	x12, x12, #0x8
   4f74c:	eor	x12, x12, x10
   4f750:	mov	x19, x4
   4f754:	mov	x20, x3
   4f758:	mov	x22, x0
   4f75c:	lsl	x9, x8, #1
   4f760:	mov	x28, x1
   4f764:	msub	x14, x13, x3, x14
   4f768:	and	x16, x8, #0x3f
   4f76c:	mov	w15, #0x1                   	// #1
   4f770:	mov	x17, x12
   4f774:	mul	x17, x17, x17
   4f778:	tst	x16, #0x1
   4f77c:	csinc	x18, x17, xzr, ne  // ne = any
   4f780:	lsr	x16, x16, #1
   4f784:	mul	x15, x18, x15
   4f788:	cbnz	x16, 4f774 <__gmpn_brootinv@@Base+0x94>
   4f78c:	add	x23, x19, x2, lsl #3
   4f790:	mul	x24, x13, x14
   4f794:	mul	x13, x15, x10
   4f798:	add	x25, x23, x11, lsl #3
   4f79c:	neg	x11, x13
   4f7a0:	madd	x11, x9, x12, x11
   4f7a4:	mul	x11, x11, x24
   4f7a8:	and	x13, x8, #0x3fff
   4f7ac:	mov	w12, #0x1                   	// #1
   4f7b0:	mov	x14, x11
   4f7b4:	mul	x14, x14, x14
   4f7b8:	tst	x13, #0x1
   4f7bc:	csinc	x15, x14, xzr, ne  // ne = any
   4f7c0:	lsr	x13, x13, #1
   4f7c4:	mul	x12, x15, x12
   4f7c8:	cbnz	x13, 4f7b4 <__gmpn_brootinv@@Base+0xd4>
   4f7cc:	mul	x12, x12, x10
   4f7d0:	neg	x12, x12
   4f7d4:	madd	x11, x9, x11, x12
   4f7d8:	mul	x11, x11, x24
   4f7dc:	mov	w12, #0x1                   	// #1
   4f7e0:	mov	x13, x11
   4f7e4:	mov	x14, x8
   4f7e8:	mul	x13, x13, x13
   4f7ec:	tst	x14, #0x1
   4f7f0:	csinc	x15, x13, xzr, ne  // ne = any
   4f7f4:	lsr	x14, x14, #1
   4f7f8:	mul	x12, x15, x12
   4f7fc:	cbnz	x14, 4f7e8 <__gmpn_brootinv@@Base+0x108>
   4f800:	mul	x12, x12, x10
   4f804:	neg	x12, x12
   4f808:	madd	x11, x9, x11, x12
   4f80c:	mul	x11, x11, x24
   4f810:	mov	w12, #0x1                   	// #1
   4f814:	mov	x13, x11
   4f818:	mul	x13, x13, x13
   4f81c:	tst	x8, #0x1
   4f820:	csinc	x14, x13, xzr, ne  // ne = any
   4f824:	lsr	x8, x8, #1
   4f828:	mul	x12, x14, x12
   4f82c:	cbnz	x8, 4f818 <__gmpn_brootinv@@Base+0x138>
   4f830:	mul	x8, x12, x10
   4f834:	neg	x8, x8
   4f838:	madd	x8, x9, x11, x8
   4f83c:	mul	x8, x8, x24
   4f840:	cmp	x2, #0x1
   4f844:	str	x8, [x22]
   4f848:	b.eq	4f98c <__gmpn_brootinv@@Base+0x2ac>  // b.none
   4f84c:	cmp	x2, #0x2
   4f850:	b.ne	4f85c <__gmpn_brootinv@@Base+0x17c>  // b.any
   4f854:	mov	w8, wzr
   4f858:	b	4f87c <__gmpn_brootinv@@Base+0x19c>
   4f85c:	mov	x8, xzr
   4f860:	add	x9, sp, #0x8
   4f864:	add	x10, x2, #0x1
   4f868:	str	x2, [x9, x8, lsl #3]
   4f86c:	asr	x2, x10, #1
   4f870:	cmp	x2, #0x2
   4f874:	add	x8, x8, #0x1
   4f878:	b.ne	4f864 <__gmpn_brootinv@@Base+0x184>  // b.any
   4f87c:	mov	w9, #0x2                   	// #2
   4f880:	add	x10, sp, #0x8
   4f884:	sxtw	x21, w8
   4f888:	mov	w26, #0x1                   	// #1
   4f88c:	str	x9, [x10, w8, uxtw #3]
   4f890:	b	4f8c4 <__gmpn_brootinv@@Base+0x1e4>
   4f894:	mov	x1, x0
   4f898:	bl	c290 <__gmpn_com@plt>
   4f89c:	mov	x0, x22
   4f8a0:	mov	x1, x19
   4f8a4:	mov	x2, x26
   4f8a8:	mov	x3, x20
   4f8ac:	mov	x4, x24
   4f8b0:	mov	w5, wzr
   4f8b4:	bl	c4e0 <__gmpn_pi1_bdiv_q_1@plt>
   4f8b8:	cmp	x21, #0x0
   4f8bc:	sub	x21, x21, #0x1
   4f8c0:	b.le	4f98c <__gmpn_brootinv@@Base+0x2ac>
   4f8c4:	mov	x0, x19
   4f8c8:	mov	x1, x22
   4f8cc:	mov	x2, x26
   4f8d0:	bl	c8e0 <__gmpn_sqr@plt>
   4f8d4:	ldur	x8, [x29, #-16]
   4f8d8:	mov	x0, x23
   4f8dc:	mov	x1, x22
   4f8e0:	mov	x2, x26
   4f8e4:	lsl	x3, x8, #1
   4f8e8:	bl	d490 <__gmpn_mul_1@plt>
   4f8ec:	str	x0, [x23, x26, lsl #3]
   4f8f0:	add	x8, sp, #0x8
   4f8f4:	ldr	x26, [x8, x21, lsl #3]
   4f8f8:	sub	x2, x29, #0x10
   4f8fc:	mov	w3, #0x1                   	// #1
   4f900:	mov	x0, x22
   4f904:	mov	x1, x19
   4f908:	mov	x4, x26
   4f90c:	mov	x5, x25
   4f910:	bl	c3c0 <__gmpn_powlo@plt>
   4f914:	mov	x0, x19
   4f918:	mov	x1, x28
   4f91c:	mov	x2, x22
   4f920:	mov	x3, x26
   4f924:	bl	cec0 <__gmpn_mullo_n@plt>
   4f928:	add	x8, x26, #0x3
   4f92c:	asr	x27, x8, #1
   4f930:	mov	x0, x19
   4f934:	mov	x1, x23
   4f938:	mov	x2, x19
   4f93c:	mov	x3, x27
   4f940:	bl	c2d0 <__gmpn_sub_n@plt>
   4f944:	subs	x2, x26, x27
   4f948:	b.le	4f89c <__gmpn_brootinv@@Base+0x1bc>
   4f94c:	mov	x8, x0
   4f950:	add	x0, x19, x27, lsl #3
   4f954:	cbnz	x8, 4f894 <__gmpn_brootinv@@Base+0x1b4>
   4f958:	ldr	x8, [x0]
   4f95c:	cbnz	x8, 4f974 <__gmpn_brootinv@@Base+0x294>
   4f960:	subs	x2, x2, #0x1
   4f964:	str	xzr, [x0]
   4f968:	b.eq	4f89c <__gmpn_brootinv@@Base+0x1bc>  // b.none
   4f96c:	ldr	x8, [x0, #8]!
   4f970:	cbz	x8, 4f960 <__gmpn_brootinv@@Base+0x280>
   4f974:	neg	x8, x8
   4f978:	subs	x2, x2, #0x1
   4f97c:	str	x8, [x0]
   4f980:	b.eq	4f89c <__gmpn_brootinv@@Base+0x1bc>  // b.none
   4f984:	add	x0, x0, #0x8
   4f988:	b	4f894 <__gmpn_brootinv@@Base+0x1b4>
   4f98c:	add	sp, sp, #0x220
   4f990:	ldp	x20, x19, [sp, #80]
   4f994:	ldp	x22, x21, [sp, #64]
   4f998:	ldp	x24, x23, [sp, #48]
   4f99c:	ldp	x26, x25, [sp, #32]
   4f9a0:	ldp	x28, x27, [sp, #16]
   4f9a4:	ldp	x29, x30, [sp], #96
   4f9a8:	ret

000000000004f9ac <__gmpn_bsqrt@@Base>:
   4f9ac:	stp	x29, x30, [sp, #-48]!
   4f9b0:	stp	x22, x21, [sp, #16]
   4f9b4:	stp	x20, x19, [sp, #32]
   4f9b8:	mov	x19, x3
   4f9bc:	lsr	x22, x2, #6
   4f9c0:	mov	x21, x0
   4f9c4:	add	x3, x3, x22, lsl #3
   4f9c8:	mov	x0, x19
   4f9cc:	mov	x29, sp
   4f9d0:	mov	x20, x1
   4f9d4:	bl	c750 <__gmpn_bsqrtinv@plt>
   4f9d8:	mov	x0, x21
   4f9dc:	mov	x1, x19
   4f9e0:	mov	x2, x20
   4f9e4:	mov	x3, x22
   4f9e8:	ldp	x20, x19, [sp, #32]
   4f9ec:	ldp	x22, x21, [sp, #16]
   4f9f0:	ldp	x29, x30, [sp], #48
   4f9f4:	b	cec0 <__gmpn_mullo_n@plt>

000000000004f9f8 <__gmpn_bsqrtinv@@Base>:
   4f9f8:	stp	x29, x30, [sp, #-80]!
   4f9fc:	stp	x28, x25, [sp, #16]
   4fa00:	stp	x24, x23, [sp, #32]
   4fa04:	stp	x22, x21, [sp, #48]
   4fa08:	stp	x20, x19, [sp, #64]
   4fa0c:	mov	x29, sp
   4fa10:	sub	sp, sp, #0x210
   4fa14:	mov	w8, #0x1                   	// #1
   4fa18:	str	x8, [x0]
   4fa1c:	ldr	x8, [x1]
   4fa20:	cmp	x2, #0x1
   4fa24:	b.ne	4fa38 <__gmpn_bsqrtinv@@Base+0x40>  // b.any
   4fa28:	and	x8, x8, #0x3
   4fa2c:	cmp	x8, #0x1
   4fa30:	b.eq	4fb10 <__gmpn_bsqrtinv@@Base+0x118>  // b.none
   4fa34:	b	4fb18 <__gmpn_bsqrtinv@@Base+0x120>
   4fa38:	and	x8, x8, #0x7
   4fa3c:	cmp	x8, #0x1
   4fa40:	b.ne	4fb18 <__gmpn_bsqrtinv@@Base+0x120>  // b.any
   4fa44:	cmp	x2, #0x2
   4fa48:	b.eq	4fb10 <__gmpn_bsqrtinv@@Base+0x118>  // b.none
   4fa4c:	lsr	x9, x2, #3
   4fa50:	and	x9, x9, #0x1ffffffffffffff8
   4fa54:	add	x9, x9, x3
   4fa58:	mov	x19, x1
   4fa5c:	mov	x20, x0
   4fa60:	mov	x21, x3
   4fa64:	mov	x8, xzr
   4fa68:	add	x22, x9, #0x8
   4fa6c:	add	x9, sp, #0x8
   4fa70:	add	x10, x2, #0x2
   4fa74:	str	x2, [x9, x8, lsl #3]
   4fa78:	lsr	x2, x10, #1
   4fa7c:	cmp	x2, #0x2
   4fa80:	add	x8, x8, #0x1
   4fa84:	b.ne	4fa70 <__gmpn_bsqrtinv@@Base+0x78>  // b.any
   4fa88:	cmp	w8, #0x1
   4fa8c:	b.lt	4fb10 <__gmpn_bsqrtinv@@Base+0x118>  // b.tstop
   4fa90:	and	x24, x8, #0xffffffff
   4fa94:	add	x8, sp, #0x8
   4fa98:	sub	x25, x8, #0x8
   4fa9c:	ldr	x8, [x25, x24, lsl #3]
   4faa0:	mov	x0, x21
   4faa4:	mov	x1, x20
   4faa8:	lsr	x8, x8, #6
   4faac:	add	x23, x8, #0x1
   4fab0:	mov	x2, x23
   4fab4:	bl	c9f0 <__gmpn_sqrlo@plt>
   4fab8:	mov	x0, x22
   4fabc:	mov	x1, x20
   4fac0:	mov	x2, x21
   4fac4:	mov	x3, x23
   4fac8:	bl	cec0 <__gmpn_mullo_n@plt>
   4facc:	mov	w3, #0x3                   	// #3
   4fad0:	mov	x0, x21
   4fad4:	mov	x1, x20
   4fad8:	mov	x2, x23
   4fadc:	bl	d490 <__gmpn_mul_1@plt>
   4fae0:	mov	x0, x20
   4fae4:	mov	x1, x19
   4fae8:	mov	x2, x22
   4faec:	mov	x3, x23
   4faf0:	bl	cec0 <__gmpn_mullo_n@plt>
   4faf4:	mov	x0, x20
   4faf8:	mov	x1, x21
   4fafc:	mov	x2, x20
   4fb00:	mov	x3, x23
   4fb04:	bl	c840 <__gmpn_rsh1sub_n@plt>
   4fb08:	subs	x24, x24, #0x1
   4fb0c:	b.gt	4fa9c <__gmpn_bsqrtinv@@Base+0xa4>
   4fb10:	mov	w0, #0x1                   	// #1
   4fb14:	b	4fb1c <__gmpn_bsqrtinv@@Base+0x124>
   4fb18:	mov	w0, wzr
   4fb1c:	add	sp, sp, #0x210
   4fb20:	ldp	x20, x19, [sp, #64]
   4fb24:	ldp	x22, x21, [sp, #48]
   4fb28:	ldp	x24, x23, [sp, #32]
   4fb2c:	ldp	x28, x25, [sp, #16]
   4fb30:	ldp	x29, x30, [sp], #80
   4fb34:	ret

000000000004fb38 <__gmpn_divexact@@Base>:
   4fb38:	stp	x29, x30, [sp, #-96]!
   4fb3c:	stp	x26, x25, [sp, #32]
   4fb40:	stp	x24, x23, [sp, #48]
   4fb44:	stp	x22, x21, [sp, #64]
   4fb48:	stp	x20, x19, [sp, #80]
   4fb4c:	mov	x23, x3
   4fb50:	ldr	x3, [x3]
   4fb54:	mov	x22, x4
   4fb58:	mov	x20, x1
   4fb5c:	mov	x19, x0
   4fb60:	str	x27, [sp, #16]
   4fb64:	mov	x29, sp
   4fb68:	cbnz	x3, 4fb80 <__gmpn_divexact@@Base+0x48>
   4fb6c:	ldr	x3, [x23, #8]!
   4fb70:	add	x20, x20, #0x8
   4fb74:	sub	x22, x22, #0x1
   4fb78:	sub	x2, x2, #0x1
   4fb7c:	cbz	x3, 4fb6c <__gmpn_divexact@@Base+0x34>
   4fb80:	cmp	x22, #0x1
   4fb84:	b.ne	4fb98 <__gmpn_divexact@@Base+0x60>  // b.any
   4fb88:	mov	x0, x19
   4fb8c:	mov	x1, x20
   4fb90:	bl	c770 <__gmpn_divexact_1@plt>
   4fb94:	b	4fce8 <__gmpn_divexact@@Base+0x1b0>
   4fb98:	sub	x8, x2, x22
   4fb9c:	rbit	x9, x3
   4fba0:	clz	x24, x9
   4fba4:	add	x21, x8, #0x1
   4fba8:	str	xzr, [x29, #24]
   4fbac:	cbz	w24, 4fc34 <__gmpn_divexact@@Base+0xfc>
   4fbb0:	cmp	x22, x21
   4fbb4:	csinc	x27, x22, x21, le
   4fbb8:	lsl	x1, x27, #3
   4fbbc:	mov	w8, #0x7f00                	// #32512
   4fbc0:	cmp	x1, x8
   4fbc4:	add	x25, x21, #0x1
   4fbc8:	b.hi	4fd08 <__gmpn_divexact@@Base+0x1d0>  // b.pmore
   4fbcc:	add	x9, x1, #0xf
   4fbd0:	mov	x8, sp
   4fbd4:	and	x9, x9, #0xfffffffffffffff0
   4fbd8:	sub	x26, x8, x9
   4fbdc:	mov	sp, x26
   4fbe0:	mov	x0, x26
   4fbe4:	mov	x1, x23
   4fbe8:	mov	x2, x27
   4fbec:	mov	w3, w24
   4fbf0:	bl	c1a0 <__gmpn_rshift@plt>
   4fbf4:	lsl	x1, x25, #3
   4fbf8:	mov	w8, #0x7f00                	// #32512
   4fbfc:	cmp	x1, x8
   4fc00:	b.hi	4fd18 <__gmpn_divexact@@Base+0x1e0>  // b.pmore
   4fc04:	add	x9, x1, #0xf
   4fc08:	mov	x8, sp
   4fc0c:	and	x9, x9, #0xfffffffffffffff0
   4fc10:	sub	x27, x8, x9
   4fc14:	mov	sp, x27
   4fc18:	mov	x0, x27
   4fc1c:	mov	x1, x20
   4fc20:	mov	x2, x25
   4fc24:	mov	w3, w24
   4fc28:	bl	c1a0 <__gmpn_rshift@plt>
   4fc2c:	mov	x23, x26
   4fc30:	mov	x20, x27
   4fc34:	cmp	x22, x21
   4fc38:	csel	x22, x21, x22, gt
   4fc3c:	mov	x0, x21
   4fc40:	mov	x1, x22
   4fc44:	bl	d380 <__gmpn_bdiv_q_itch@plt>
   4fc48:	lsl	x1, x0, #3
   4fc4c:	mov	w8, #0x7f00                	// #32512
   4fc50:	cmp	x1, x8
   4fc54:	b.hi	4fcb4 <__gmpn_divexact@@Base+0x17c>  // b.pmore
   4fc58:	add	x9, x1, #0xf
   4fc5c:	mov	x8, sp
   4fc60:	and	x9, x9, #0xfffffffffffffff0
   4fc64:	sub	x5, x8, x9
   4fc68:	mov	sp, x5
   4fc6c:	mov	x0, x19
   4fc70:	mov	x1, x20
   4fc74:	mov	x2, x21
   4fc78:	mov	x3, x23
   4fc7c:	mov	x4, x22
   4fc80:	bl	ca30 <__gmpn_bdiv_q@plt>
   4fc84:	ldr	x0, [x29, #24]
   4fc88:	cbnz	x0, 4fcc4 <__gmpn_divexact@@Base+0x18c>
   4fc8c:	ldr	x8, [x19]
   4fc90:	cbz	x8, 4fcdc <__gmpn_divexact@@Base+0x1a4>
   4fc94:	neg	x8, x8
   4fc98:	subs	x2, x21, #0x1
   4fc9c:	str	x8, [x19]
   4fca0:	b.eq	4fce8 <__gmpn_divexact@@Base+0x1b0>  // b.none
   4fca4:	add	x0, x19, #0x8
   4fca8:	mov	x1, x0
   4fcac:	bl	c290 <__gmpn_com@plt>
   4fcb0:	b	4fce8 <__gmpn_divexact@@Base+0x1b0>
   4fcb4:	add	x0, x29, #0x18
   4fcb8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4fcbc:	mov	x5, x0
   4fcc0:	b	4fc6c <__gmpn_divexact@@Base+0x134>
   4fcc4:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   4fcc8:	ldr	x8, [x19]
   4fccc:	cbnz	x8, 4fc94 <__gmpn_divexact@@Base+0x15c>
   4fcd0:	b	4fcdc <__gmpn_divexact@@Base+0x1a4>
   4fcd4:	ldr	x8, [x19, #8]!
   4fcd8:	cbnz	x8, 4fc94 <__gmpn_divexact@@Base+0x15c>
   4fcdc:	subs	x21, x21, #0x1
   4fce0:	str	xzr, [x19]
   4fce4:	b.ne	4fcd4 <__gmpn_divexact@@Base+0x19c>  // b.any
   4fce8:	mov	sp, x29
   4fcec:	ldp	x20, x19, [sp, #80]
   4fcf0:	ldp	x22, x21, [sp, #64]
   4fcf4:	ldp	x24, x23, [sp, #48]
   4fcf8:	ldp	x26, x25, [sp, #32]
   4fcfc:	ldr	x27, [sp, #16]
   4fd00:	ldp	x29, x30, [sp], #96
   4fd04:	ret
   4fd08:	add	x0, x29, #0x18
   4fd0c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4fd10:	mov	x26, x0
   4fd14:	b	4fbe0 <__gmpn_divexact@@Base+0xa8>
   4fd18:	add	x0, x29, #0x18
   4fd1c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   4fd20:	mov	x27, x0
   4fd24:	b	4fc18 <__gmpn_divexact@@Base+0xe0>
   4fd28:	nop
   4fd2c:	nop

000000000004fd30 <__gmpn_bdiv_dbm1c@@Base>:
   4fd30:	ldr	x5, [x1], #8
   4fd34:	ands	x6, x2, #0x3
   4fd38:	b.eq	4fd58 <__gmpn_bdiv_dbm1c@@Base+0x28>  // b.none
   4fd3c:	cmp	x6, #0x2
   4fd40:	b.cc	4fd68 <__gmpn_bdiv_dbm1c@@Base+0x38>  // b.lo, b.ul, b.last
   4fd44:	b.eq	4fd80 <__gmpn_bdiv_dbm1c@@Base+0x50>  // b.none
   4fd48:	mul	x12, x5, x3
   4fd4c:	umulh	x13, x5, x3
   4fd50:	ldr	x5, [x1], #8
   4fd54:	b	4fdd0 <__gmpn_bdiv_dbm1c@@Base+0xa0>
   4fd58:	mul	x10, x5, x3
   4fd5c:	umulh	x11, x5, x3
   4fd60:	ldr	x5, [x1], #8
   4fd64:	b	4fdb8 <__gmpn_bdiv_dbm1c@@Base+0x88>
   4fd68:	subs	x2, x2, #0x1
   4fd6c:	mul	x12, x5, x3
   4fd70:	umulh	x13, x5, x3
   4fd74:	b.ls	4fe04 <__gmpn_bdiv_dbm1c@@Base+0xd4>  // b.plast
   4fd78:	ldr	x5, [x1], #8
   4fd7c:	b	4fda0 <__gmpn_bdiv_dbm1c@@Base+0x70>
   4fd80:	mul	x10, x5, x3
   4fd84:	umulh	x11, x5, x3
   4fd88:	ldr	x5, [x1], #8
   4fd8c:	b	4fde8 <__gmpn_bdiv_dbm1c@@Base+0xb8>
   4fd90:	ldr	x5, [x1], #8
   4fd94:	subs	x4, x4, x10
   4fd98:	str	x4, [x0], #8
   4fd9c:	sbc	x4, x4, x11
   4fda0:	mul	x10, x5, x3
   4fda4:	umulh	x11, x5, x3
   4fda8:	ldr	x5, [x1], #8
   4fdac:	subs	x4, x4, x12
   4fdb0:	str	x4, [x0], #8
   4fdb4:	sbc	x4, x4, x13
   4fdb8:	mul	x12, x5, x3
   4fdbc:	umulh	x13, x5, x3
   4fdc0:	ldr	x5, [x1], #8
   4fdc4:	subs	x4, x4, x10
   4fdc8:	str	x4, [x0], #8
   4fdcc:	sbc	x4, x4, x11
   4fdd0:	mul	x10, x5, x3
   4fdd4:	umulh	x11, x5, x3
   4fdd8:	ldr	x5, [x1], #8
   4fddc:	subs	x4, x4, x12
   4fde0:	str	x4, [x0], #8
   4fde4:	sbc	x4, x4, x13
   4fde8:	subs	x2, x2, #0x4
   4fdec:	mul	x12, x5, x3
   4fdf0:	umulh	x13, x5, x3
   4fdf4:	b.hi	4fd90 <__gmpn_bdiv_dbm1c@@Base+0x60>  // b.pmore
   4fdf8:	subs	x4, x4, x10
   4fdfc:	str	x4, [x0], #8
   4fe00:	sbc	x4, x4, x11
   4fe04:	subs	x4, x4, x12
   4fe08:	str	x4, [x0]
   4fe0c:	sbc	x0, x4, x13
   4fe10:	ret

000000000004fe14 <__gmpn_redc_1@@Base>:
   4fe14:	stp	x29, x30, [sp, #-64]!
   4fe18:	stp	x22, x21, [sp, #32]
   4fe1c:	stp	x20, x19, [sp, #48]
   4fe20:	mov	x19, x3
   4fe24:	mov	x20, x1
   4fe28:	cmp	x3, #0x1
   4fe2c:	mov	x21, x0
   4fe30:	stp	x24, x23, [sp, #16]
   4fe34:	mov	x29, sp
   4fe38:	b.lt	4fe70 <__gmpn_redc_1@@Base+0x5c>  // b.tstop
   4fe3c:	mov	x22, x4
   4fe40:	mov	x23, x2
   4fe44:	add	x24, x19, #0x1
   4fe48:	ldr	x8, [x20]
   4fe4c:	mov	x0, x20
   4fe50:	mov	x1, x23
   4fe54:	mov	x2, x19
   4fe58:	mul	x3, x8, x22
   4fe5c:	bl	d400 <__gmpn_addmul_1@plt>
   4fe60:	sub	x24, x24, #0x1
   4fe64:	cmp	x24, #0x1
   4fe68:	str	x0, [x20], #8
   4fe6c:	b.gt	4fe48 <__gmpn_redc_1@@Base+0x34>
   4fe70:	sub	x2, x20, x19, lsl #3
   4fe74:	mov	x0, x21
   4fe78:	mov	x1, x20
   4fe7c:	mov	x3, x19
   4fe80:	ldp	x20, x19, [sp, #48]
   4fe84:	ldp	x22, x21, [sp, #32]
   4fe88:	ldp	x24, x23, [sp, #16]
   4fe8c:	ldp	x29, x30, [sp], #64
   4fe90:	b	ca70 <__gmpn_add_n@plt>

000000000004fe94 <__gmpn_redc_2@@Base>:
   4fe94:	stp	x29, x30, [sp, #-96]!
   4fe98:	stp	x24, x23, [sp, #48]
   4fe9c:	stp	x22, x21, [sp, #64]
   4fea0:	stp	x20, x19, [sp, #80]
   4fea4:	mov	x22, x4
   4fea8:	mov	x19, x3
   4feac:	mov	x23, x2
   4feb0:	mov	x20, x1
   4feb4:	mov	x21, x0
   4feb8:	stp	x28, x27, [sp, #16]
   4febc:	stp	x26, x25, [sp, #32]
   4fec0:	mov	x29, sp
   4fec4:	tbz	w19, #0, 4fee8 <__gmpn_redc_2@@Base+0x54>
   4fec8:	ldr	x8, [x20]
   4fecc:	ldr	x9, [x22]
   4fed0:	mov	x0, x20
   4fed4:	mov	x1, x23
   4fed8:	mov	x2, x19
   4fedc:	mul	x3, x9, x8
   4fee0:	bl	d400 <__gmpn_addmul_1@plt>
   4fee4:	str	x0, [x20], #8
   4fee8:	cmp	x19, #0x2
   4feec:	b.lt	4ff64 <__gmpn_redc_2@@Base+0xd0>  // b.tstop
   4fef0:	add	x26, x19, #0x2
   4fef4:	lsl	x27, x19, #3
   4fef8:	mov	x24, x20
   4fefc:	ldp	x8, x11, [x22]
   4ff00:	ldr	x9, [x20]
   4ff04:	ldr	x10, [x24, #8]!
   4ff08:	ldr	x28, [x20, x27]
   4ff0c:	umulh	x12, x8, x9
   4ff10:	mul	x3, x9, x8
   4ff14:	madd	x8, x10, x8, x12
   4ff18:	mov	x0, x20
   4ff1c:	mov	x1, x23
   4ff20:	mov	x2, x19
   4ff24:	madd	x25, x11, x9, x8
   4ff28:	bl	d400 <__gmpn_addmul_1@plt>
   4ff2c:	str	x0, [x20, x27]
   4ff30:	mov	x0, x24
   4ff34:	mov	x1, x23
   4ff38:	mov	x2, x19
   4ff3c:	mov	x3, x25
   4ff40:	bl	d400 <__gmpn_addmul_1@plt>
   4ff44:	str	x0, [x24]
   4ff48:	ldr	x8, [x20, x27]
   4ff4c:	sub	x26, x26, #0x2
   4ff50:	cmp	x26, #0x3
   4ff54:	str	x8, [x20]
   4ff58:	str	x28, [x20, x27]
   4ff5c:	add	x20, x20, #0x10
   4ff60:	b.gt	4fef8 <__gmpn_redc_2@@Base+0x64>
   4ff64:	sub	x2, x20, x19, lsl #3
   4ff68:	mov	x0, x21
   4ff6c:	mov	x1, x20
   4ff70:	mov	x3, x19
   4ff74:	ldp	x20, x19, [sp, #80]
   4ff78:	ldp	x22, x21, [sp, #64]
   4ff7c:	ldp	x24, x23, [sp, #48]
   4ff80:	ldp	x26, x25, [sp, #32]
   4ff84:	ldp	x28, x27, [sp, #16]
   4ff88:	ldp	x29, x30, [sp], #96
   4ff8c:	b	ca70 <__gmpn_add_n@plt>

000000000004ff90 <__gmpn_redc_n@@Base>:
   4ff90:	stp	x29, x30, [sp, #-96]!
   4ff94:	stp	x22, x21, [sp, #64]
   4ff98:	mov	x29, sp
   4ff9c:	mov	x21, x0
   4ffa0:	mov	x0, x3
   4ffa4:	str	x27, [sp, #16]
   4ffa8:	stp	x26, x25, [sp, #32]
   4ffac:	stp	x24, x23, [sp, #48]
   4ffb0:	stp	x20, x19, [sp, #80]
   4ffb4:	mov	x25, x4
   4ffb8:	mov	x19, x3
   4ffbc:	mov	x20, x2
   4ffc0:	mov	x22, x1
   4ffc4:	str	xzr, [x29, #24]
   4ffc8:	bl	c860 <__gmpn_mulmod_bnm1_next_size@plt>
   4ffcc:	cmp	x19, x0, asr #1
   4ffd0:	add	x8, x19, x0, lsl #1
   4ffd4:	csel	x9, x0, xzr, gt
   4ffd8:	add	x8, x8, x9
   4ffdc:	lsl	x8, x8, #3
   4ffe0:	add	x1, x8, #0x20
   4ffe4:	mov	w8, #0x7f00                	// #32512
   4ffe8:	mov	x23, x0
   4ffec:	cmp	x1, x8
   4fff0:	b.hi	500f4 <__gmpn_redc_n@@Base+0x164>  // b.pmore
   4fff4:	add	x9, x1, #0xf
   4fff8:	mov	x8, sp
   4fffc:	and	x9, x9, #0xfffffffffffffff0
   50000:	sub	x24, x8, x9
   50004:	mov	sp, x24
   50008:	mov	x0, x24
   5000c:	mov	x1, x22
   50010:	mov	x2, x25
   50014:	mov	x3, x19
   50018:	bl	cec0 <__gmpn_mullo_n@plt>
   5001c:	add	x25, x24, x19, lsl #3
   50020:	add	x26, x25, x23, lsl #3
   50024:	mov	x0, x25
   50028:	mov	x1, x23
   5002c:	mov	x2, x24
   50030:	mov	x3, x19
   50034:	mov	x4, x20
   50038:	mov	x5, x19
   5003c:	mov	x6, x26
   50040:	bl	c980 <__gmpn_mulmod_bnm1@plt>
   50044:	lsl	x27, x19, #1
   50048:	subs	x3, x27, x23
   5004c:	b.le	5010c <__gmpn_redc_n@@Base+0x17c>
   50050:	mov	x0, x26
   50054:	mov	x1, x25
   50058:	mov	x2, x22
   5005c:	bl	c2d0 <__gmpn_sub_n@plt>
   50060:	add	x8, x25, x27, lsl #3
   50064:	sub	x8, x8, x23, lsl #3
   50068:	ldr	x9, [x8]
   5006c:	subs	x9, x9, x0
   50070:	str	x9, [x8]
   50074:	b.cs	5009c <__gmpn_redc_n@@Base+0x10c>  // b.hs, b.nlast
   50078:	mov	w8, #0x18                  	// #24
   5007c:	mul	x8, x19, x8
   50080:	sub	x8, x8, x23, lsl #3
   50084:	add	x8, x8, x24
   50088:	add	x8, x8, #0x8
   5008c:	ldr	x9, [x8]
   50090:	sub	x10, x9, #0x1
   50094:	str	x10, [x8], #8
   50098:	cbz	x9, 5008c <__gmpn_redc_n@@Base+0xfc>
   5009c:	lsl	x8, x19, #3
   500a0:	add	x1, x22, x8
   500a4:	add	x2, x25, x8
   500a8:	mov	x0, x21
   500ac:	mov	x3, x19
   500b0:	bl	c2d0 <__gmpn_sub_n@plt>
   500b4:	cbz	x0, 500cc <__gmpn_redc_n@@Base+0x13c>
   500b8:	mov	x0, x21
   500bc:	mov	x1, x21
   500c0:	mov	x2, x20
   500c4:	mov	x3, x19
   500c8:	bl	ca70 <__gmpn_add_n@plt>
   500cc:	ldr	x0, [x29, #24]
   500d0:	cbnz	x0, 50104 <__gmpn_redc_n@@Base+0x174>
   500d4:	mov	sp, x29
   500d8:	ldp	x20, x19, [sp, #80]
   500dc:	ldp	x22, x21, [sp, #64]
   500e0:	ldp	x24, x23, [sp, #48]
   500e4:	ldp	x26, x25, [sp, #32]
   500e8:	ldr	x27, [sp, #16]
   500ec:	ldp	x29, x30, [sp], #96
   500f0:	ret
   500f4:	add	x0, x29, #0x18
   500f8:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   500fc:	mov	x24, x0
   50100:	b	50008 <__gmpn_redc_n@@Base+0x78>
   50104:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50108:	b	500d4 <__gmpn_redc_n@@Base+0x144>
   5010c:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   50110:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   50114:	add	x0, x0, #0xb8f
   50118:	add	x2, x2, #0xb98
   5011c:	mov	w1, #0x46                  	// #70
   50120:	bl	c6c0 <__gmp_assert_fail@plt>

0000000000050124 <__gmpn_powm@@Base>:
   50124:	stp	x29, x30, [sp, #-96]!
   50128:	stp	x28, x27, [sp, #16]
   5012c:	stp	x26, x25, [sp, #32]
   50130:	stp	x24, x23, [sp, #48]
   50134:	stp	x22, x21, [sp, #64]
   50138:	stp	x20, x19, [sp, #80]
   5013c:	mov	x29, sp
   50140:	sub	sp, sp, #0x50
   50144:	stur	xzr, [x29, #-24]
   50148:	add	x8, x3, x4, lsl #3
   5014c:	ldur	x8, [x8, #-8]
   50150:	lsl	x9, x4, #6
   50154:	mov	x22, x7
   50158:	mov	x19, x6
   5015c:	clz	x8, x8
   50160:	sub	x26, x9, x8
   50164:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   50168:	mov	x20, x5
   5016c:	mov	x27, x1
   50170:	mov	x21, x0
   50174:	mov	x28, xzr
   50178:	mov	x24, xzr
   5017c:	add	x8, x8, #0xba8
   50180:	mov	x23, x26
   50184:	mov	x9, #0x100000000           	// #4294967296
   50188:	stur	x3, [x29, #-48]
   5018c:	add	x10, x8, x24, lsl #3
   50190:	ldr	x10, [x10, #8]
   50194:	add	x24, x24, #0x1
   50198:	add	x28, x28, x9
   5019c:	cmp	x10, x26
   501a0:	b.cc	5018c <__gmpn_powm@@Base+0x68>  // b.lo, b.ul, b.last
   501a4:	cmp	x19, #0x2a
   501a8:	lsl	x1, x19, #3
   501ac:	stur	x1, [x29, #-64]
   501b0:	stur	x23, [x29, #-40]
   501b4:	b.le	50200 <__gmpn_powm@@Base+0xdc>
   501b8:	mov	w8, #0x7f00                	// #32512
   501bc:	stur	x27, [x29, #-32]
   501c0:	mov	x27, x2
   501c4:	cmp	x1, x8
   501c8:	b.hi	50b94 <__gmpn_powm@@Base+0xa70>  // b.pmore
   501cc:	add	x9, x1, #0xf
   501d0:	mov	x8, sp
   501d4:	and	x9, x9, #0xfffffffffffffff0
   501d8:	sub	x25, x8, x9
   501dc:	mov	sp, x25
   501e0:	mov	x0, x25
   501e4:	mov	x1, x20
   501e8:	mov	x2, x19
   501ec:	mov	x3, x22
   501f0:	bl	cd20 <__gmpn_binvert@plt>
   501f4:	mov	x2, x27
   501f8:	ldur	x27, [x29, #-32]
   501fc:	b	5023c <__gmpn_powm@@Base+0x118>
   50200:	ldr	x8, [x20]
   50204:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   50208:	ldr	x9, [x9, #3952]
   5020c:	sub	x25, x29, #0x10
   50210:	ubfx	x10, x8, #1, #7
   50214:	ldrb	w9, [x9, x10]
   50218:	mov	w10, #0x2                   	// #2
   5021c:	msub	x11, x8, x9, x10
   50220:	mul	x9, x11, x9
   50224:	msub	x10, x9, x8, x10
   50228:	mul	x9, x9, x10
   5022c:	orr	x10, xzr, #0xfffffffffffffffe
   50230:	madd	x8, x9, x8, x10
   50234:	mul	x8, x8, x9
   50238:	stur	x8, [x29, #-16]
   5023c:	sub	x23, x24, #0x1
   50240:	lsl	x8, x19, x23
   50244:	lsl	x1, x8, #3
   50248:	mov	w8, #0x7f00                	// #32512
   5024c:	cmp	x1, x8
   50250:	b.hi	50b78 <__gmpn_powm@@Base+0xa54>  // b.pmore
   50254:	add	x9, x1, #0xf
   50258:	mov	x8, sp
   5025c:	and	x9, x9, #0xfffffffffffffff0
   50260:	sub	x0, x8, x9
   50264:	mov	sp, x0
   50268:	mov	x1, x27
   5026c:	mov	x3, x20
   50270:	mov	x4, x19
   50274:	mov	x27, x0
   50278:	bl	50ba4 <__gmpn_powm@@Base+0xa80>
   5027c:	mov	x0, x22
   50280:	mov	x1, x27
   50284:	mov	x2, x19
   50288:	stur	x27, [x29, #-32]
   5028c:	bl	c8e0 <__gmpn_sqr@plt>
   50290:	cmp	x19, #0x2a
   50294:	b.le	502b4 <__gmpn_powm@@Base+0x190>
   50298:	mov	x0, x21
   5029c:	mov	x1, x22
   502a0:	mov	x2, x20
   502a4:	mov	x3, x19
   502a8:	mov	x4, x25
   502ac:	bl	c7d0 <__gmpn_redc_n@plt>
   502b0:	b	502e4 <__gmpn_powm@@Base+0x1c0>
   502b4:	ldr	x4, [x25]
   502b8:	mov	x0, x21
   502bc:	mov	x1, x22
   502c0:	mov	x2, x20
   502c4:	mov	x3, x19
   502c8:	bl	d0f0 <__gmpn_redc_1@plt>
   502cc:	cbz	x0, 502e4 <__gmpn_powm@@Base+0x1c0>
   502d0:	mov	x0, x21
   502d4:	mov	x1, x21
   502d8:	mov	x2, x20
   502dc:	mov	x3, x19
   502e0:	bl	c2d0 <__gmpn_sub_n@plt>
   502e4:	mov	w8, #0xffffffff            	// #-1
   502e8:	lsl	w8, w8, w23
   502ec:	cmn	w8, #0x2
   502f0:	b.gt	503c8 <__gmpn_powm@@Base+0x2a4>
   502f4:	ldur	x27, [x29, #-32]
   502f8:	mvn	w8, w8
   502fc:	sxtw	x8, w8
   50300:	add	x23, x8, #0x1
   50304:	b	50350 <__gmpn_powm@@Base+0x22c>
   50308:	ldr	x8, [x27]
   5030c:	ldr	x9, [x21]
   50310:	umulh	x10, x8, x9
   50314:	mul	x8, x9, x8
   50318:	stp	x8, x10, [x22]
   5031c:	ldr	x9, [x25]
   50320:	ldr	x11, [x20]
   50324:	cmp	x8, #0x0
   50328:	mul	x9, x9, x8
   5032c:	umulh	x9, x11, x9
   50330:	cinc	x8, x9, ne  // ne = any
   50334:	adds	x8, x8, x10
   50338:	csel	x9, x11, xzr, cs  // cs = hs, nlast
   5033c:	sub	x8, x8, x9
   50340:	str	x8, [x27, #8]!
   50344:	sub	x23, x23, #0x1
   50348:	cmp	x23, #0x1
   5034c:	b.le	503c8 <__gmpn_powm@@Base+0x2a4>
   50350:	cmp	x19, #0x1
   50354:	b.eq	50308 <__gmpn_powm@@Base+0x1e4>  // b.none
   50358:	mov	x0, x22
   5035c:	mov	x1, x27
   50360:	mov	x2, x21
   50364:	mov	x3, x19
   50368:	bl	c990 <__gmpn_mul_n@plt>
   5036c:	cmp	x19, #0x2a
   50370:	add	x27, x27, x19, lsl #3
   50374:	b.le	50394 <__gmpn_powm@@Base+0x270>
   50378:	mov	x0, x27
   5037c:	mov	x1, x22
   50380:	mov	x2, x20
   50384:	mov	x3, x19
   50388:	mov	x4, x25
   5038c:	bl	c7d0 <__gmpn_redc_n@plt>
   50390:	b	50344 <__gmpn_powm@@Base+0x220>
   50394:	ldr	x4, [x25]
   50398:	mov	x0, x27
   5039c:	mov	x1, x22
   503a0:	mov	x2, x20
   503a4:	mov	x3, x19
   503a8:	bl	d0f0 <__gmpn_redc_1@plt>
   503ac:	cbz	x0, 50344 <__gmpn_powm@@Base+0x220>
   503b0:	mov	x0, x27
   503b4:	mov	x1, x27
   503b8:	mov	x2, x20
   503bc:	mov	x3, x19
   503c0:	bl	c2d0 <__gmpn_sub_n@plt>
   503c4:	b	50344 <__gmpn_powm@@Base+0x220>
   503c8:	asr	x13, x28, #32
   503cc:	subs	x8, x26, x13
   503d0:	b.cs	503e0 <__gmpn_powm@@Base+0x2bc>  // b.hs, b.nlast
   503d4:	ldp	x28, x11, [x29, #-48]
   503d8:	ldr	x8, [x28]
   503dc:	b	50418 <__gmpn_powm@@Base+0x2f4>
   503e0:	ldur	x28, [x29, #-48]
   503e4:	lsr	x9, x8, #6
   503e8:	and	x10, x8, #0x3f
   503ec:	mov	w12, #0x40                  	// #64
   503f0:	ldr	x11, [x28, x9, lsl #3]
   503f4:	sub	w10, w12, w10
   503f8:	cmp	w10, w24
   503fc:	lsr	x8, x11, x8
   50400:	b.ge	50414 <__gmpn_powm@@Base+0x2f0>  // b.tcont
   50404:	add	x9, x28, x9, lsl #3
   50408:	ldr	x9, [x9, #8]
   5040c:	lsl	x9, x9, x10
   50410:	add	x8, x9, x8
   50414:	and	x11, x24, #0xffffffff
   50418:	mov	x9, #0xffffffffffffffff    	// #-1
   5041c:	lsl	x9, x9, x11
   50420:	bic	x8, x8, x9
   50424:	subs	x10, x26, x13
   50428:	rbit	x9, x8
   5042c:	csel	x10, xzr, x10, cc  // cc = lo, ul, last
   50430:	clz	x9, x9
   50434:	add	x26, x9, x10
   50438:	lsr	x8, x8, x9
   5043c:	ldur	x9, [x29, #-32]
   50440:	lsr	x8, x8, #1
   50444:	mul	x8, x8, x19
   50448:	mov	x0, x21
   5044c:	add	x1, x9, x8, lsl #3
   50450:	mov	x2, x19
   50454:	stur	x13, [x29, #-40]
   50458:	bl	ca50 <__gmpn_copyi@plt>
   5045c:	subs	x27, x19, #0x1
   50460:	b.ne	505c8 <__gmpn_powm@@Base+0x4a4>  // b.any
   50464:	cbz	x26, 50abc <__gmpn_powm@@Base+0x998>
   50468:	ldur	x0, [x29, #-40]
   5046c:	mov	x8, #0xffffffffffffffff    	// #-1
   50470:	lsl	x9, x8, x24
   50474:	mvn	x9, x9
   50478:	mov	w10, #0x40                  	// #64
   5047c:	b	504c0 <__gmpn_powm@@Base+0x39c>
   50480:	ldr	x12, [x21]
   50484:	umulh	x13, x12, x12
   50488:	mov	x26, x11
   5048c:	mul	x12, x12, x12
   50490:	stp	x12, x13, [x22]
   50494:	ldr	x14, [x25]
   50498:	ldr	x15, [x20]
   5049c:	cmp	x12, #0x0
   504a0:	mul	x14, x14, x12
   504a4:	umulh	x14, x15, x14
   504a8:	cinc	x12, x14, ne  // ne = any
   504ac:	adds	x12, x12, x13
   504b0:	csel	x13, x15, xzr, cs  // cs = hs, nlast
   504b4:	sub	x12, x12, x13
   504b8:	str	x12, [x21]
   504bc:	cbz	x11, 50a84 <__gmpn_powm@@Base+0x960>
   504c0:	sub	x11, x26, #0x1
   504c4:	lsr	x12, x11, #3
   504c8:	and	x12, x12, #0x1ffffffffffffff8
   504cc:	ldr	x12, [x28, x12]
   504d0:	lsr	x12, x12, x11
   504d4:	tbz	w12, #0, 50480 <__gmpn_powm@@Base+0x35c>
   504d8:	subs	x11, x26, x0
   504dc:	b.cs	504f0 <__gmpn_powm@@Base+0x3cc>  // b.hs, b.nlast
   504e0:	ldr	x11, [x28]
   504e4:	lsl	x12, x8, x26
   504e8:	bic	x11, x11, x12
   504ec:	b	50520 <__gmpn_powm@@Base+0x3fc>
   504f0:	lsr	x12, x11, #6
   504f4:	ldr	x14, [x28, x12, lsl #3]
   504f8:	and	x13, x11, #0x3f
   504fc:	sub	w13, w10, w13
   50500:	cmp	w13, w24
   50504:	lsr	x11, x14, x11
   50508:	b.ge	5051c <__gmpn_powm@@Base+0x3f8>  // b.tcont
   5050c:	add	x12, x28, x12, lsl #3
   50510:	ldr	x12, [x12, #8]
   50514:	lsl	x12, x12, x13
   50518:	add	x11, x12, x11
   5051c:	and	x11, x11, x9
   50520:	ldr	x12, [x25]
   50524:	ldr	x15, [x21]
   50528:	subs	x13, x26, x0
   5052c:	rbit	x14, x11
   50530:	csel	w16, w26, w24, cc  // cc = lo, ul, last
   50534:	csel	x17, xzr, x13, cc  // cc = lo, ul, last
   50538:	clz	x13, x14
   5053c:	add	x26, x13, x17
   50540:	sub	w14, w13, w16
   50544:	umulh	x16, x15, x15
   50548:	mul	x15, x15, x15
   5054c:	stp	x15, x16, [x22]
   50550:	ldr	x17, [x20]
   50554:	mul	x18, x12, x15
   50558:	cmp	x15, #0x0
   5055c:	umulh	x15, x17, x18
   50560:	cinc	x15, x15, ne  // ne = any
   50564:	adds	x15, x15, x16
   50568:	csel	x16, x17, xzr, cs  // cs = hs, nlast
   5056c:	sub	x15, x15, x16
   50570:	adds	w14, w14, #0x1
   50574:	str	x15, [x21]
   50578:	b.cc	50544 <__gmpn_powm@@Base+0x420>  // b.lo, b.ul, b.last
   5057c:	lsr	x11, x11, x13
   50580:	ldur	x13, [x29, #-32]
   50584:	lsl	x11, x11, #2
   50588:	and	x11, x11, #0xfffffffffffffff8
   5058c:	ldr	x11, [x13, x11]
   50590:	umulh	x13, x15, x11
   50594:	mul	x11, x11, x15
   50598:	stp	x11, x13, [x22]
   5059c:	ldr	x14, [x20]
   505a0:	mul	x12, x11, x12
   505a4:	cmp	x11, #0x0
   505a8:	umulh	x12, x14, x12
   505ac:	cinc	x11, x12, ne  // ne = any
   505b0:	adds	x11, x11, x13
   505b4:	csel	x12, x14, xzr, cs  // cs = hs, nlast
   505b8:	sub	x11, x11, x12
   505bc:	str	x11, [x21]
   505c0:	cbnz	x26, 504c0 <__gmpn_powm@@Base+0x39c>
   505c4:	b	50a84 <__gmpn_powm@@Base+0x960>
   505c8:	cmp	x19, #0xd
   505cc:	b.le	50730 <__gmpn_powm@@Base+0x60c>
   505d0:	cmp	x19, #0x2a
   505d4:	b.le	508dc <__gmpn_powm@@Base+0x7b8>
   505d8:	cbz	x26, 50abc <__gmpn_powm@@Base+0x998>
   505dc:	ldur	x12, [x29, #-40]
   505e0:	mov	x8, #0xffffffffffffffff    	// #-1
   505e4:	lsl	x8, x8, x24
   505e8:	mvn	x8, x8
   505ec:	stur	x8, [x29, #-72]
   505f0:	b	50628 <__gmpn_powm@@Base+0x504>
   505f4:	mov	x0, x22
   505f8:	mov	x1, x21
   505fc:	mov	x2, x19
   50600:	bl	c8e0 <__gmpn_sqr@plt>
   50604:	mov	x0, x21
   50608:	mov	x1, x22
   5060c:	mov	x2, x20
   50610:	mov	x3, x19
   50614:	mov	x4, x25
   50618:	bl	c7d0 <__gmpn_redc_n@plt>
   5061c:	ldur	x12, [x29, #-40]
   50620:	mov	x26, x23
   50624:	cbz	x23, 50a84 <__gmpn_powm@@Base+0x960>
   50628:	sub	x23, x26, #0x1
   5062c:	lsr	x8, x23, #3
   50630:	and	x8, x8, #0x1ffffffffffffff8
   50634:	ldr	x8, [x28, x8]
   50638:	lsr	x8, x8, x23
   5063c:	tbz	w8, #0, 505f4 <__gmpn_powm@@Base+0x4d0>
   50640:	subs	x8, x26, x12
   50644:	b.cs	5065c <__gmpn_powm@@Base+0x538>  // b.hs, b.nlast
   50648:	ldr	x8, [x28]
   5064c:	mov	x9, #0xffffffffffffffff    	// #-1
   50650:	lsl	x9, x9, x26
   50654:	bic	x9, x8, x9
   50658:	b	50694 <__gmpn_powm@@Base+0x570>
   5065c:	lsr	x9, x8, #6
   50660:	ldr	x11, [x28, x9, lsl #3]
   50664:	and	x10, x8, #0x3f
   50668:	mov	w13, #0x40                  	// #64
   5066c:	sub	w10, w13, w10
   50670:	cmp	w10, w24
   50674:	lsr	x8, x11, x8
   50678:	b.ge	5068c <__gmpn_powm@@Base+0x568>  // b.tcont
   5067c:	add	x9, x28, x9, lsl #3
   50680:	ldr	x9, [x9, #8]
   50684:	lsl	x9, x9, x10
   50688:	add	x8, x9, x8
   5068c:	ldur	x9, [x29, #-72]
   50690:	and	x9, x8, x9
   50694:	subs	x8, x26, x12
   50698:	stur	x9, [x29, #-56]
   5069c:	rbit	x9, x9
   506a0:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   506a4:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   506a8:	clz	x28, x9
   506ac:	add	x26, x28, x8
   506b0:	sub	w23, w28, w10
   506b4:	mov	x0, x22
   506b8:	mov	x1, x21
   506bc:	mov	x2, x19
   506c0:	bl	c8e0 <__gmpn_sqr@plt>
   506c4:	mov	x0, x21
   506c8:	mov	x1, x22
   506cc:	mov	x2, x20
   506d0:	mov	x3, x19
   506d4:	mov	x4, x25
   506d8:	bl	c7d0 <__gmpn_redc_n@plt>
   506dc:	adds	w23, w23, #0x1
   506e0:	b.cc	506b4 <__gmpn_powm@@Base+0x590>  // b.lo, b.ul, b.last
   506e4:	ldur	x8, [x29, #-56]
   506e8:	ldur	x9, [x29, #-32]
   506ec:	mov	x0, x22
   506f0:	mov	x1, x21
   506f4:	lsr	x8, x8, x28
   506f8:	lsr	x8, x8, #1
   506fc:	mul	x8, x8, x19
   50700:	add	x2, x9, x8, lsl #3
   50704:	mov	x3, x19
   50708:	bl	c990 <__gmpn_mul_n@plt>
   5070c:	mov	x0, x21
   50710:	mov	x1, x22
   50714:	mov	x2, x20
   50718:	mov	x3, x19
   5071c:	mov	x4, x25
   50720:	bl	c7d0 <__gmpn_redc_n@plt>
   50724:	ldp	x28, x12, [x29, #-48]
   50728:	cbnz	x26, 50628 <__gmpn_powm@@Base+0x504>
   5072c:	b	50a84 <__gmpn_powm@@Base+0x960>
   50730:	ldur	x12, [x29, #-40]
   50734:	cbz	x26, 50a84 <__gmpn_powm@@Base+0x960>
   50738:	mov	x8, #0xffffffffffffffff    	// #-1
   5073c:	lsl	x8, x8, x24
   50740:	mvn	x8, x8
   50744:	stur	x8, [x29, #-72]
   50748:	b	50758 <__gmpn_powm@@Base+0x634>
   5074c:	ldur	x28, [x29, #-48]
   50750:	ldur	x12, [x29, #-40]
   50754:	cbz	x26, 50a84 <__gmpn_powm@@Base+0x960>
   50758:	sub	x8, x26, #0x1
   5075c:	lsr	x9, x8, #3
   50760:	and	x9, x9, #0x1ffffffffffffff8
   50764:	ldr	x9, [x28, x9]
   50768:	lsr	x9, x9, x8
   5076c:	tbz	w9, #0, 5078c <__gmpn_powm@@Base+0x668>
   50770:	subs	x8, x26, x12
   50774:	b.cs	507d4 <__gmpn_powm@@Base+0x6b0>  // b.hs, b.nlast
   50778:	ldr	x8, [x28]
   5077c:	mov	x9, #0xffffffffffffffff    	// #-1
   50780:	lsl	x9, x9, x26
   50784:	bic	x9, x8, x9
   50788:	b	5080c <__gmpn_powm@@Base+0x6e8>
   5078c:	mov	x0, x22
   50790:	mov	x1, x21
   50794:	mov	x2, x19
   50798:	mov	x26, x8
   5079c:	bl	c190 <__gmpn_sqr_basecase@plt>
   507a0:	ldr	x4, [x25]
   507a4:	mov	x0, x21
   507a8:	mov	x1, x22
   507ac:	mov	x2, x20
   507b0:	mov	x3, x19
   507b4:	bl	d0f0 <__gmpn_redc_1@plt>
   507b8:	cbz	x0, 50750 <__gmpn_powm@@Base+0x62c>
   507bc:	mov	x0, x21
   507c0:	mov	x1, x21
   507c4:	mov	x2, x20
   507c8:	mov	x3, x19
   507cc:	bl	c2d0 <__gmpn_sub_n@plt>
   507d0:	b	50750 <__gmpn_powm@@Base+0x62c>
   507d4:	lsr	x9, x8, #6
   507d8:	ldr	x11, [x28, x9, lsl #3]
   507dc:	and	x10, x8, #0x3f
   507e0:	mov	w13, #0x40                  	// #64
   507e4:	sub	w10, w13, w10
   507e8:	cmp	w10, w24
   507ec:	lsr	x8, x11, x8
   507f0:	b.ge	50804 <__gmpn_powm@@Base+0x6e0>  // b.tcont
   507f4:	add	x9, x28, x9, lsl #3
   507f8:	ldr	x9, [x9, #8]
   507fc:	lsl	x9, x9, x10
   50800:	add	x8, x9, x8
   50804:	ldur	x9, [x29, #-72]
   50808:	and	x9, x8, x9
   5080c:	subs	x8, x26, x12
   50810:	stur	x9, [x29, #-56]
   50814:	rbit	x9, x9
   50818:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   5081c:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   50820:	clz	x28, x9
   50824:	add	x26, x28, x8
   50828:	sub	w23, w28, w10
   5082c:	b	50838 <__gmpn_powm@@Base+0x714>
   50830:	adds	w23, w23, #0x1
   50834:	b.cs	5087c <__gmpn_powm@@Base+0x758>  // b.hs, b.nlast
   50838:	mov	x0, x22
   5083c:	mov	x1, x21
   50840:	mov	x2, x19
   50844:	bl	c190 <__gmpn_sqr_basecase@plt>
   50848:	ldr	x4, [x25]
   5084c:	mov	x0, x21
   50850:	mov	x1, x22
   50854:	mov	x2, x20
   50858:	mov	x3, x19
   5085c:	bl	d0f0 <__gmpn_redc_1@plt>
   50860:	cbz	x0, 50830 <__gmpn_powm@@Base+0x70c>
   50864:	mov	x0, x21
   50868:	mov	x1, x21
   5086c:	mov	x2, x20
   50870:	mov	x3, x19
   50874:	bl	c2d0 <__gmpn_sub_n@plt>
   50878:	b	50830 <__gmpn_powm@@Base+0x70c>
   5087c:	ldur	x8, [x29, #-56]
   50880:	ldur	x9, [x29, #-32]
   50884:	mov	x0, x22
   50888:	mov	x1, x21
   5088c:	lsr	x8, x8, x28
   50890:	lsr	x8, x8, #1
   50894:	mul	x8, x8, x19
   50898:	add	x3, x9, x8, lsl #3
   5089c:	mov	x2, x19
   508a0:	mov	x4, x19
   508a4:	bl	c550 <__gmpn_mul_basecase@plt>
   508a8:	ldr	x4, [x25]
   508ac:	mov	x0, x21
   508b0:	mov	x1, x22
   508b4:	mov	x2, x20
   508b8:	mov	x3, x19
   508bc:	bl	d0f0 <__gmpn_redc_1@plt>
   508c0:	cbz	x0, 5074c <__gmpn_powm@@Base+0x628>
   508c4:	mov	x0, x21
   508c8:	mov	x1, x21
   508cc:	mov	x2, x20
   508d0:	mov	x3, x19
   508d4:	bl	c2d0 <__gmpn_sub_n@plt>
   508d8:	b	5074c <__gmpn_powm@@Base+0x628>
   508dc:	cbz	x26, 50abc <__gmpn_powm@@Base+0x998>
   508e0:	ldur	x12, [x29, #-40]
   508e4:	mov	x8, #0xffffffffffffffff    	// #-1
   508e8:	lsl	x8, x8, x24
   508ec:	mvn	x8, x8
   508f0:	stur	x8, [x29, #-72]
   508f4:	b	50904 <__gmpn_powm@@Base+0x7e0>
   508f8:	ldur	x28, [x29, #-48]
   508fc:	ldur	x12, [x29, #-40]
   50900:	cbz	x26, 50a84 <__gmpn_powm@@Base+0x960>
   50904:	sub	x8, x26, #0x1
   50908:	lsr	x9, x8, #3
   5090c:	and	x9, x9, #0x1ffffffffffffff8
   50910:	ldr	x9, [x28, x9]
   50914:	lsr	x9, x9, x8
   50918:	tbz	w9, #0, 50938 <__gmpn_powm@@Base+0x814>
   5091c:	subs	x8, x26, x12
   50920:	b.cs	50980 <__gmpn_powm@@Base+0x85c>  // b.hs, b.nlast
   50924:	ldr	x8, [x28]
   50928:	mov	x9, #0xffffffffffffffff    	// #-1
   5092c:	lsl	x9, x9, x26
   50930:	bic	x9, x8, x9
   50934:	b	509b8 <__gmpn_powm@@Base+0x894>
   50938:	mov	x0, x22
   5093c:	mov	x1, x21
   50940:	mov	x2, x19
   50944:	mov	x26, x8
   50948:	bl	c8e0 <__gmpn_sqr@plt>
   5094c:	ldr	x4, [x25]
   50950:	mov	x0, x21
   50954:	mov	x1, x22
   50958:	mov	x2, x20
   5095c:	mov	x3, x19
   50960:	bl	d0f0 <__gmpn_redc_1@plt>
   50964:	cbz	x0, 508fc <__gmpn_powm@@Base+0x7d8>
   50968:	mov	x0, x21
   5096c:	mov	x1, x21
   50970:	mov	x2, x20
   50974:	mov	x3, x19
   50978:	bl	c2d0 <__gmpn_sub_n@plt>
   5097c:	b	508fc <__gmpn_powm@@Base+0x7d8>
   50980:	lsr	x9, x8, #6
   50984:	ldr	x11, [x28, x9, lsl #3]
   50988:	and	x10, x8, #0x3f
   5098c:	mov	w13, #0x40                  	// #64
   50990:	sub	w10, w13, w10
   50994:	cmp	w10, w24
   50998:	lsr	x8, x11, x8
   5099c:	b.ge	509b0 <__gmpn_powm@@Base+0x88c>  // b.tcont
   509a0:	add	x9, x28, x9, lsl #3
   509a4:	ldr	x9, [x9, #8]
   509a8:	lsl	x9, x9, x10
   509ac:	add	x8, x9, x8
   509b0:	ldur	x9, [x29, #-72]
   509b4:	and	x9, x8, x9
   509b8:	subs	x8, x26, x12
   509bc:	stur	x9, [x29, #-56]
   509c0:	rbit	x9, x9
   509c4:	csel	w10, w26, w24, cc  // cc = lo, ul, last
   509c8:	csel	x8, xzr, x8, cc  // cc = lo, ul, last
   509cc:	clz	x28, x9
   509d0:	add	x26, x28, x8
   509d4:	sub	w23, w28, w10
   509d8:	b	509e4 <__gmpn_powm@@Base+0x8c0>
   509dc:	adds	w23, w23, #0x1
   509e0:	b.cs	50a28 <__gmpn_powm@@Base+0x904>  // b.hs, b.nlast
   509e4:	mov	x0, x22
   509e8:	mov	x1, x21
   509ec:	mov	x2, x19
   509f0:	bl	c8e0 <__gmpn_sqr@plt>
   509f4:	ldr	x4, [x25]
   509f8:	mov	x0, x21
   509fc:	mov	x1, x22
   50a00:	mov	x2, x20
   50a04:	mov	x3, x19
   50a08:	bl	d0f0 <__gmpn_redc_1@plt>
   50a0c:	cbz	x0, 509dc <__gmpn_powm@@Base+0x8b8>
   50a10:	mov	x0, x21
   50a14:	mov	x1, x21
   50a18:	mov	x2, x20
   50a1c:	mov	x3, x19
   50a20:	bl	c2d0 <__gmpn_sub_n@plt>
   50a24:	b	509dc <__gmpn_powm@@Base+0x8b8>
   50a28:	ldur	x8, [x29, #-56]
   50a2c:	ldur	x9, [x29, #-32]
   50a30:	mov	x0, x22
   50a34:	mov	x1, x21
   50a38:	lsr	x8, x8, x28
   50a3c:	lsr	x8, x8, #1
   50a40:	mul	x8, x8, x19
   50a44:	add	x2, x9, x8, lsl #3
   50a48:	mov	x3, x19
   50a4c:	bl	c990 <__gmpn_mul_n@plt>
   50a50:	ldr	x4, [x25]
   50a54:	mov	x0, x21
   50a58:	mov	x1, x22
   50a5c:	mov	x2, x20
   50a60:	mov	x3, x19
   50a64:	bl	d0f0 <__gmpn_redc_1@plt>
   50a68:	cbz	x0, 508f8 <__gmpn_powm@@Base+0x7d4>
   50a6c:	mov	x0, x21
   50a70:	mov	x1, x21
   50a74:	mov	x2, x20
   50a78:	mov	x3, x19
   50a7c:	bl	c2d0 <__gmpn_sub_n@plt>
   50a80:	b	508f8 <__gmpn_powm@@Base+0x7d4>
   50a84:	mov	x0, x22
   50a88:	mov	x1, x21
   50a8c:	mov	x2, x19
   50a90:	bl	ca50 <__gmpn_copyi@plt>
   50a94:	cbnz	x19, 50acc <__gmpn_powm@@Base+0x9a8>
   50a98:	cmp	x19, #0x2a
   50a9c:	b.le	50ae4 <__gmpn_powm@@Base+0x9c0>
   50aa0:	mov	x0, x21
   50aa4:	mov	x1, x22
   50aa8:	mov	x2, x20
   50aac:	mov	x3, x19
   50ab0:	mov	x4, x25
   50ab4:	bl	c7d0 <__gmpn_redc_n@plt>
   50ab8:	b	50b14 <__gmpn_powm@@Base+0x9f0>
   50abc:	mov	x0, x22
   50ac0:	mov	x1, x21
   50ac4:	mov	x2, x19
   50ac8:	bl	ca50 <__gmpn_copyi@plt>
   50acc:	ldur	x2, [x29, #-64]
   50ad0:	mov	w1, wzr
   50ad4:	add	x0, x22, x2
   50ad8:	bl	c5f0 <memset@plt>
   50adc:	cmp	x19, #0x2a
   50ae0:	b.gt	50aa0 <__gmpn_powm@@Base+0x97c>
   50ae4:	ldr	x4, [x25]
   50ae8:	mov	x0, x21
   50aec:	mov	x1, x22
   50af0:	mov	x2, x20
   50af4:	mov	x3, x19
   50af8:	bl	d0f0 <__gmpn_redc_1@plt>
   50afc:	cbz	x0, 50b14 <__gmpn_powm@@Base+0x9f0>
   50b00:	mov	x0, x21
   50b04:	mov	x1, x21
   50b08:	mov	x2, x20
   50b0c:	mov	x3, x19
   50b10:	bl	c2d0 <__gmpn_sub_n@plt>
   50b14:	add	x8, x27, #0x1
   50b18:	cmp	x8, #0x1
   50b1c:	b.lt	50b3c <__gmpn_powm@@Base+0xa18>  // b.tstop
   50b20:	lsl	x8, x27, #3
   50b24:	ldr	x9, [x21, x8]
   50b28:	ldr	x8, [x20, x8]
   50b2c:	sub	x27, x27, #0x1
   50b30:	cmp	x9, x8
   50b34:	b.eq	50b14 <__gmpn_powm@@Base+0x9f0>  // b.none
   50b38:	b.ls	50b50 <__gmpn_powm@@Base+0xa2c>  // b.plast
   50b3c:	mov	x0, x21
   50b40:	mov	x1, x21
   50b44:	mov	x2, x20
   50b48:	mov	x3, x19
   50b4c:	bl	c2d0 <__gmpn_sub_n@plt>
   50b50:	ldur	x0, [x29, #-24]
   50b54:	cbnz	x0, 50b8c <__gmpn_powm@@Base+0xa68>
   50b58:	mov	sp, x29
   50b5c:	ldp	x20, x19, [sp, #80]
   50b60:	ldp	x22, x21, [sp, #64]
   50b64:	ldp	x24, x23, [sp, #48]
   50b68:	ldp	x26, x25, [sp, #32]
   50b6c:	ldp	x28, x27, [sp, #16]
   50b70:	ldp	x29, x30, [sp], #96
   50b74:	ret
   50b78:	sub	x0, x29, #0x18
   50b7c:	stur	x2, [x29, #-32]
   50b80:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50b84:	ldur	x2, [x29, #-32]
   50b88:	b	50268 <__gmpn_powm@@Base+0x144>
   50b8c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50b90:	b	50b58 <__gmpn_powm@@Base+0xa34>
   50b94:	sub	x0, x29, #0x18
   50b98:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50b9c:	mov	x25, x0
   50ba0:	b	501e0 <__gmpn_powm@@Base+0xbc>
   50ba4:	stp	x29, x30, [sp, #-80]!
   50ba8:	stp	x26, x25, [sp, #16]
   50bac:	stp	x24, x23, [sp, #32]
   50bb0:	stp	x22, x21, [sp, #48]
   50bb4:	stp	x20, x19, [sp, #64]
   50bb8:	mov	x29, sp
   50bbc:	sub	sp, sp, #0x10
   50bc0:	add	x21, x4, x2
   50bc4:	add	x8, x2, x21
   50bc8:	lsl	x8, x8, #3
   50bcc:	mov	x24, x1
   50bd0:	add	x1, x8, #0x8
   50bd4:	mov	w8, #0x7f00                	// #32512
   50bd8:	mov	x19, x4
   50bdc:	mov	x20, x3
   50be0:	mov	x23, x2
   50be4:	mov	x22, x0
   50be8:	cmp	x1, x8
   50bec:	stur	xzr, [x29, #-8]
   50bf0:	b.hi	50c74 <__gmpn_powm@@Base+0xb50>  // b.pmore
   50bf4:	add	x9, x1, #0xf
   50bf8:	mov	x8, sp
   50bfc:	and	x9, x9, #0xfffffffffffffff0
   50c00:	sub	x25, x8, x9
   50c04:	mov	sp, x25
   50c08:	add	x26, x25, x21, lsl #3
   50c0c:	cbz	x19, 50c20 <__gmpn_powm@@Base+0xafc>
   50c10:	lsl	x2, x19, #3
   50c14:	mov	x0, x25
   50c18:	mov	w1, wzr
   50c1c:	bl	c5f0 <memset@plt>
   50c20:	add	x0, x25, x19, lsl #3
   50c24:	mov	x1, x24
   50c28:	mov	x2, x23
   50c2c:	bl	ca50 <__gmpn_copyi@plt>
   50c30:	mov	x0, x26
   50c34:	mov	x1, x22
   50c38:	mov	x2, xzr
   50c3c:	mov	x3, x25
   50c40:	mov	x4, x21
   50c44:	mov	x5, x20
   50c48:	mov	x6, x19
   50c4c:	bl	bf00 <__gmpn_tdiv_qr@plt>
   50c50:	ldur	x0, [x29, #-8]
   50c54:	cbnz	x0, 50c8c <__gmpn_powm@@Base+0xb68>
   50c58:	mov	sp, x29
   50c5c:	ldp	x20, x19, [sp, #64]
   50c60:	ldp	x22, x21, [sp, #48]
   50c64:	ldp	x24, x23, [sp, #32]
   50c68:	ldp	x26, x25, [sp, #16]
   50c6c:	ldp	x29, x30, [sp], #80
   50c70:	ret
   50c74:	sub	x0, x29, #0x8
   50c78:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50c7c:	mov	x25, x0
   50c80:	add	x26, x25, x21, lsl #3
   50c84:	cbnz	x19, 50c10 <__gmpn_powm@@Base+0xaec>
   50c88:	b	50c20 <__gmpn_powm@@Base+0xafc>
   50c8c:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50c90:	b	50c58 <__gmpn_powm@@Base+0xb34>

0000000000050c94 <__gmpn_powlo@@Base>:
   50c94:	stp	x29, x30, [sp, #-96]!
   50c98:	stp	x28, x27, [sp, #16]
   50c9c:	stp	x26, x25, [sp, #32]
   50ca0:	stp	x24, x23, [sp, #48]
   50ca4:	stp	x22, x21, [sp, #64]
   50ca8:	stp	x20, x19, [sp, #80]
   50cac:	mov	x29, sp
   50cb0:	sub	sp, sp, #0x30
   50cb4:	stur	xzr, [x29, #-8]
   50cb8:	add	x8, x2, x3, lsl #3
   50cbc:	ldur	x8, [x8, #-8]
   50cc0:	lsl	x9, x3, #6
   50cc4:	mov	x23, x5
   50cc8:	mov	x19, x4
   50ccc:	clz	x8, x8
   50cd0:	sub	x25, x9, x8
   50cd4:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   50cd8:	mov	x21, x2
   50cdc:	mov	x24, x1
   50ce0:	mov	x20, x0
   50ce4:	mov	w26, #0xffffffff            	// #-1
   50ce8:	add	x8, x8, #0xc00
   50cec:	add	w26, w26, #0x1
   50cf0:	ldr	x9, [x8, w26, uxtw #3]
   50cf4:	cmp	x9, x25
   50cf8:	b.cc	50cec <__gmpn_powlo@@Base+0x58>  // b.lo, b.ul, b.last
   50cfc:	add	w27, w26, #0x1
   50d00:	cmp	w27, #0x2
   50d04:	stur	w27, [x29, #-28]
   50d08:	b.cc	50da8 <__gmpn_powlo@@Base+0x114>  // b.lo, b.ul, b.last
   50d0c:	lsl	x8, x19, x26
   50d10:	lsl	x1, x8, #3
   50d14:	mov	w8, #0x7f00                	// #32512
   50d18:	cmp	x1, x8
   50d1c:	b.hi	50ff0 <__gmpn_powlo@@Base+0x35c>  // b.pmore
   50d20:	add	x9, x1, #0xf
   50d24:	mov	x8, sp
   50d28:	and	x9, x9, #0xfffffffffffffff0
   50d2c:	sub	x28, x8, x9
   50d30:	mov	sp, x28
   50d34:	mov	x0, x28
   50d38:	mov	x1, x24
   50d3c:	mov	x2, x19
   50d40:	bl	ca50 <__gmpn_copyi@plt>
   50d44:	mov	x0, x23
   50d48:	mov	x1, x24
   50d4c:	mov	x2, x19
   50d50:	bl	c9f0 <__gmpn_sqrlo@plt>
   50d54:	mov	w8, #0xffffffff            	// #-1
   50d58:	lsl	w8, w8, w26
   50d5c:	mvn	w8, w8
   50d60:	sxtw	x22, w8
   50d64:	lsl	x26, x19, #3
   50d68:	mov	x1, x28
   50d6c:	add	x24, x1, x26
   50d70:	mov	x0, x24
   50d74:	mov	x2, x23
   50d78:	mov	x3, x19
   50d7c:	bl	cec0 <__gmpn_mullo_n@plt>
   50d80:	subs	x22, x22, #0x1
   50d84:	mov	x1, x24
   50d88:	b.ne	50d6c <__gmpn_powlo@@Base+0xd8>  // b.any
   50d8c:	mov	w26, w27
   50d90:	subs	x8, x25, x26
   50d94:	b.cs	50dd8 <__gmpn_powlo@@Base+0x144>  // b.hs, b.nlast
   50d98:	ldr	x9, [x21]
   50d9c:	mov	x10, #0xffffffffffffffff    	// #-1
   50da0:	lsl	x10, x10, x25
   50da4:	b	50e10 <__gmpn_powlo@@Base+0x17c>
   50da8:	add	x28, x23, x19, lsl #3
   50dac:	mov	x0, x28
   50db0:	mov	x1, x24
   50db4:	mov	x2, x19
   50db8:	bl	ca50 <__gmpn_copyi@plt>
   50dbc:	mov	x0, x20
   50dc0:	mov	x1, x24
   50dc4:	mov	x2, x19
   50dc8:	bl	ca50 <__gmpn_copyi@plt>
   50dcc:	sub	x25, x25, #0x1
   50dd0:	mov	w26, w27
   50dd4:	b	50e3c <__gmpn_powlo@@Base+0x1a8>
   50dd8:	lsr	x10, x8, #6
   50ddc:	ldr	x12, [x21, x10, lsl #3]
   50de0:	and	x9, x8, #0x3f
   50de4:	mov	w11, #0x40                  	// #64
   50de8:	sub	w11, w11, w9
   50dec:	cmp	w11, w27
   50df0:	lsr	x9, x12, x8
   50df4:	b.cs	50e08 <__gmpn_powlo@@Base+0x174>  // b.hs, b.nlast
   50df8:	add	x10, x21, x10, lsl #3
   50dfc:	ldr	x10, [x10, #8]
   50e00:	lsl	x10, x10, x11
   50e04:	add	x9, x10, x9
   50e08:	mov	x10, #0xffffffffffffffff    	// #-1
   50e0c:	lsl	x10, x10, x26
   50e10:	bic	x9, x9, x10
   50e14:	rbit	x10, x9
   50e18:	clz	x10, x10
   50e1c:	add	x25, x8, x10
   50e20:	lsr	x8, x9, x10
   50e24:	lsr	x8, x8, #1
   50e28:	mul	x8, x8, x19
   50e2c:	add	x1, x28, x8, lsl #3
   50e30:	mov	x0, x20
   50e34:	mov	x2, x19
   50e38:	bl	ca50 <__gmpn_copyi@plt>
   50e3c:	mov	x8, #0xffffffffffffffff    	// #-1
   50e40:	lsl	x8, x8, x26
   50e44:	mov	w27, wzr
   50e48:	mvn	x8, x8
   50e4c:	stp	x28, x21, [x29, #-24]
   50e50:	stur	x8, [x29, #-40]
   50e54:	b	50e7c <__gmpn_powlo@@Base+0x1e8>
   50e58:	mov	x0, x20
   50e5c:	mov	x1, x24
   50e60:	mov	x2, x19
   50e64:	bl	c9f0 <__gmpn_sqrlo@plt>
   50e68:	cmp	w27, #0x0
   50e6c:	cset	w27, eq  // eq = none
   50e70:	mov	x25, x22
   50e74:	mov	x23, x24
   50e78:	cbz	x22, 50fac <__gmpn_powlo@@Base+0x318>
   50e7c:	sub	x22, x25, #0x1
   50e80:	lsr	x8, x22, #3
   50e84:	and	x8, x8, #0x1ffffffffffffff8
   50e88:	ldr	x8, [x21, x8]
   50e8c:	mov	x24, x20
   50e90:	mov	x20, x23
   50e94:	lsr	x8, x8, x22
   50e98:	tbz	w8, #0, 50e58 <__gmpn_powlo@@Base+0x1c4>
   50e9c:	subs	x8, x25, x26
   50ea0:	b.cs	50eb8 <__gmpn_powlo@@Base+0x224>  // b.hs, b.nlast
   50ea4:	ldr	x8, [x21]
   50ea8:	mov	x9, #0xffffffffffffffff    	// #-1
   50eac:	lsl	x9, x9, x25
   50eb0:	bic	x23, x8, x9
   50eb4:	b	50ef4 <__gmpn_powlo@@Base+0x260>
   50eb8:	lsr	x9, x8, #6
   50ebc:	and	x10, x8, #0x3f
   50ec0:	mov	w12, #0x40                  	// #64
   50ec4:	ldr	x11, [x21, x9, lsl #3]
   50ec8:	sub	w10, w12, w10
   50ecc:	ldur	w12, [x29, #-28]
   50ed0:	lsr	x8, x11, x8
   50ed4:	cmp	w10, w12
   50ed8:	b.cs	50eec <__gmpn_powlo@@Base+0x258>  // b.hs, b.nlast
   50edc:	add	x9, x21, x9, lsl #3
   50ee0:	ldr	x9, [x9, #8]
   50ee4:	lsl	x9, x9, x10
   50ee8:	add	x8, x9, x8
   50eec:	ldur	x9, [x29, #-40]
   50ef0:	and	x23, x8, x9
   50ef4:	cmp	x25, x26
   50ef8:	rbit	x8, x23
   50efc:	csel	x9, x26, x25, hi  // hi = pmore
   50f00:	clz	x28, x8
   50f04:	sub	w21, w9, w28
   50f08:	cmp	w21, #0x2
   50f0c:	sub	x25, x25, x9
   50f10:	b.cc	50f48 <__gmpn_powlo@@Base+0x2b4>  // b.lo, b.ul, b.last
   50f14:	mov	w22, w21
   50f18:	mov	x0, x20
   50f1c:	mov	x1, x24
   50f20:	mov	x2, x19
   50f24:	bl	c9f0 <__gmpn_sqrlo@plt>
   50f28:	mov	x0, x24
   50f2c:	mov	x1, x20
   50f30:	mov	x2, x19
   50f34:	bl	c9f0 <__gmpn_sqrlo@plt>
   50f38:	sub	w22, w22, #0x2
   50f3c:	cmp	w22, #0x1
   50f40:	b.hi	50f18 <__gmpn_powlo@@Base+0x284>  // b.pmore
   50f44:	and	w21, w21, #0x1
   50f48:	add	x25, x28, x25
   50f4c:	lsr	x28, x23, x28
   50f50:	cbz	w21, 50f70 <__gmpn_powlo@@Base+0x2dc>
   50f54:	mov	x0, x20
   50f58:	mov	x1, x24
   50f5c:	mov	x2, x19
   50f60:	bl	c9f0 <__gmpn_sqrlo@plt>
   50f64:	mov	x23, x20
   50f68:	mov	x20, x24
   50f6c:	b	50f80 <__gmpn_powlo@@Base+0x2ec>
   50f70:	cmp	w27, #0x0
   50f74:	cset	w27, eq  // eq = none
   50f78:	mov	x23, x24
   50f7c:	mov	x24, x20
   50f80:	ldur	x9, [x29, #-24]
   50f84:	lsr	x8, x28, #1
   50f88:	mul	x8, x8, x19
   50f8c:	mov	x0, x24
   50f90:	add	x2, x9, x8, lsl #3
   50f94:	mov	x1, x23
   50f98:	mov	x3, x19
   50f9c:	bl	cec0 <__gmpn_mullo_n@plt>
   50fa0:	ldur	x21, [x29, #-16]
   50fa4:	mov	x24, x23
   50fa8:	cbnz	x25, 50e7c <__gmpn_powlo@@Base+0x1e8>
   50fac:	cbz	w27, 50fc0 <__gmpn_powlo@@Base+0x32c>
   50fb0:	mov	x0, x24
   50fb4:	mov	x1, x20
   50fb8:	mov	x2, x19
   50fbc:	bl	ca50 <__gmpn_copyi@plt>
   50fc0:	ldur	x0, [x29, #-8]
   50fc4:	cbnz	x0, 50fe8 <__gmpn_powlo@@Base+0x354>
   50fc8:	mov	sp, x29
   50fcc:	ldp	x20, x19, [sp, #80]
   50fd0:	ldp	x22, x21, [sp, #64]
   50fd4:	ldp	x24, x23, [sp, #48]
   50fd8:	ldp	x26, x25, [sp, #32]
   50fdc:	ldp	x28, x27, [sp, #16]
   50fe0:	ldp	x29, x30, [sp], #96
   50fe4:	ret
   50fe8:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   50fec:	b	50fc8 <__gmpn_powlo@@Base+0x334>
   50ff0:	sub	x0, x29, #0x8
   50ff4:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   50ff8:	mov	x28, x0
   50ffc:	b	50d34 <__gmpn_powlo@@Base+0xa0>

0000000000051000 <__gmpn_sec_powm@@Base>:
   51000:	sub	sp, sp, #0xb0
   51004:	adrp	x8, 5b000 <__gmpn_bases@@Base+0x2678>
   51008:	stp	x29, x30, [sp, #80]
   5100c:	stp	x22, x21, [sp, #144]
   51010:	stp	x20, x19, [sp, #160]
   51014:	add	x29, sp, #0x50
   51018:	mov	x19, x6
   5101c:	mov	x20, x5
   51020:	mov	x21, x0
   51024:	mov	x12, xzr
   51028:	mov	x22, xzr
   5102c:	add	x8, x8, #0xc70
   51030:	mov	x9, #0x100000000           	// #4294967296
   51034:	stp	x28, x27, [sp, #96]
   51038:	stp	x26, x25, [sp, #112]
   5103c:	stp	x24, x23, [sp, #128]
   51040:	stp	x7, x3, [x29, #-16]
   51044:	stp	x1, x2, [sp, #24]
   51048:	add	x10, x8, x12, lsl #3
   5104c:	ldr	x10, [x10, #8]
   51050:	add	x22, x22, x9
   51054:	add	x12, x12, #0x1
   51058:	cmp	x10, x4
   5105c:	b.cc	51048 <__gmpn_sec_powm@@Base+0x48>  // b.lo, b.ul, b.last
   51060:	str	x4, [sp, #8]
   51064:	ldr	x8, [x20]
   51068:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5106c:	ldr	x9, [x9, #3952]
   51070:	mov	w10, #0x2                   	// #2
   51074:	ubfx	x11, x8, #1, #7
   51078:	lsl	x23, x19, x12
   5107c:	ldrb	w9, [x9, x11]
   51080:	ldur	x11, [x29, #-16]
   51084:	lsl	x27, x19, #3
   51088:	add	x28, x11, x19, lsl #3
   5108c:	mov	w11, #0x1                   	// #1
   51090:	stp	x28, x12, [x29, #-32]
   51094:	str	x11, [x28], #8
   51098:	msub	x11, x8, x9, x10
   5109c:	mul	x9, x11, x9
   510a0:	msub	x10, x9, x8, x10
   510a4:	orr	x11, xzr, #0xfffffffffffffffe
   510a8:	mul	x24, x9, x10
   510ac:	madd	x25, x24, x8, x11
   510b0:	cbz	x19, 510c4 <__gmpn_sec_powm@@Base+0xc4>
   510b4:	mov	x0, x28
   510b8:	mov	w1, wzr
   510bc:	mov	x2, x27
   510c0:	bl	c5f0 <memset@plt>
   510c4:	str	x27, [sp, #40]
   510c8:	ldur	x27, [x29, #-16]
   510cc:	ldr	x8, [sp, #40]
   510d0:	mul	x26, x25, x24
   510d4:	mov	w2, #0x1                   	// #1
   510d8:	add	x24, x27, x23, lsl #3
   510dc:	ldur	x23, [x29, #-32]
   510e0:	add	x25, x28, x8
   510e4:	mov	x0, x25
   510e8:	mov	x1, x23
   510ec:	bl	ca50 <__gmpn_copyi@plt>
   510f0:	add	x1, x19, #0x1
   510f4:	add	x4, x25, #0x8
   510f8:	mov	x0, x28
   510fc:	mov	x2, x20
   51100:	mov	x3, x19
   51104:	bl	c140 <__gmpn_sec_div_r@plt>
   51108:	ldr	x25, [sp, #40]
   5110c:	mov	x0, x27
   51110:	mov	x1, x28
   51114:	mov	x2, x19
   51118:	bl	ca50 <__gmpn_copyi@plt>
   5111c:	add	x28, x23, x25
   51120:	cbz	x19, 51134 <__gmpn_sec_powm@@Base+0x134>
   51124:	mov	x0, x28
   51128:	mov	w1, wzr
   5112c:	mov	x2, x25
   51130:	bl	c5f0 <memset@plt>
   51134:	ldp	x1, x23, [sp, #24]
   51138:	add	x0, x28, x25
   5113c:	mov	x2, x23
   51140:	bl	ca50 <__gmpn_copyi@plt>
   51144:	add	x8, x28, x23, lsl #3
   51148:	add	x1, x19, x23
   5114c:	add	x4, x8, x25
   51150:	mov	x0, x28
   51154:	mov	x2, x20
   51158:	mov	x3, x19
   5115c:	bl	c140 <__gmpn_sec_div_r@plt>
   51160:	ldur	x0, [x29, #-32]
   51164:	mov	x1, x28
   51168:	mov	x2, x19
   5116c:	bl	ca50 <__gmpn_copyi@plt>
   51170:	ldur	x8, [x29, #-24]
   51174:	mov	w9, #0x1                   	// #1
   51178:	lsl	w8, w9, w8
   5117c:	cmp	w8, #0x3
   51180:	str	x8, [sp, #16]
   51184:	b.lt	51278 <__gmpn_sec_powm@@Base+0x278>  // b.tstop
   51188:	add	x9, x19, x19, lsl #1
   5118c:	ldr	x8, [sp, #16]
   51190:	lsl	x9, x9, #3
   51194:	str	x9, [sp, #32]
   51198:	lsl	x9, x19, #4
   5119c:	str	x9, [sp, #24]
   511a0:	ldur	x28, [x29, #-32]
   511a4:	ldur	x23, [x29, #-16]
   511a8:	sub	w8, w8, #0x2
   511ac:	sxtw	x8, w8
   511b0:	add	x27, x8, #0x2
   511b4:	b	5125c <__gmpn_sec_powm@@Base+0x25c>
   511b8:	mov	x3, x28
   511bc:	mov	x4, x19
   511c0:	bl	c550 <__gmpn_mul_basecase@plt>
   511c4:	ldr	x8, [sp, #24]
   511c8:	mov	x1, x24
   511cc:	mov	x2, x20
   511d0:	mov	x3, x19
   511d4:	add	x25, x23, x8
   511d8:	mov	x0, x25
   511dc:	mov	x4, x26
   511e0:	bl	d0f0 <__gmpn_redc_1@plt>
   511e4:	mov	x1, x25
   511e8:	mov	x2, x25
   511ec:	mov	x3, x20
   511f0:	mov	x4, x19
   511f4:	bl	c020 <__gmpn_cnd_sub_n@plt>
   511f8:	ldur	x3, [x29, #-32]
   511fc:	mov	x0, x24
   51200:	mov	x1, x25
   51204:	mov	x2, x19
   51208:	mov	x4, x19
   5120c:	bl	c550 <__gmpn_mul_basecase@plt>
   51210:	ldr	x8, [sp, #32]
   51214:	mov	x1, x24
   51218:	mov	x2, x20
   5121c:	mov	x3, x19
   51220:	add	x23, x23, x8
   51224:	mov	x0, x23
   51228:	mov	x4, x26
   5122c:	bl	d0f0 <__gmpn_redc_1@plt>
   51230:	mov	x1, x23
   51234:	mov	x2, x23
   51238:	mov	x3, x20
   5123c:	mov	x4, x19
   51240:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51244:	ldr	x8, [sp, #40]
   51248:	sub	x27, x27, #0x2
   5124c:	cmp	x27, #0x2
   51250:	mov	x23, x25
   51254:	add	x28, x28, x8
   51258:	b.le	51278 <__gmpn_sec_powm@@Base+0x278>
   5125c:	mov	x0, x24
   51260:	mov	x1, x28
   51264:	mov	x2, x19
   51268:	cmp	x19, #0x11
   5126c:	b.gt	511b8 <__gmpn_sec_powm@@Base+0x1b8>
   51270:	bl	c190 <__gmpn_sqr_basecase@plt>
   51274:	b	511c4 <__gmpn_sec_powm@@Base+0x1c4>
   51278:	ldr	x9, [sp, #8]
   5127c:	asr	x25, x22, #32
   51280:	cmp	x25, x9
   51284:	b.hi	514d8 <__gmpn_sec_powm@@Base+0x4d8>  // b.pmore
   51288:	ldur	x11, [x29, #-8]
   5128c:	sub	x23, x9, x25
   51290:	lsr	x10, x23, #6
   51294:	ldur	x27, [x29, #-24]
   51298:	ldr	x12, [x11, x10, lsl #3]
   5129c:	and	x9, x23, #0x3f
   512a0:	mov	w11, #0x40                  	// #64
   512a4:	sub	w11, w11, w9
   512a8:	and	x8, x27, #0xffffffff
   512ac:	cmp	w11, w27
   512b0:	lsr	x9, x12, x23
   512b4:	b.ge	512cc <__gmpn_sec_powm@@Base+0x2cc>  // b.tcont
   512b8:	ldur	x12, [x29, #-8]
   512bc:	add	x10, x12, x10, lsl #3
   512c0:	ldr	x10, [x10, #8]
   512c4:	lsl	x10, x10, x11
   512c8:	add	x9, x10, x9
   512cc:	mov	x10, #0xffffffffffffffff    	// #-1
   512d0:	lsl	x22, x10, x8
   512d4:	ldr	x8, [sp, #16]
   512d8:	ldur	x1, [x29, #-16]
   512dc:	bic	x4, x9, x22
   512e0:	mov	x0, x21
   512e4:	sxtw	x3, w8
   512e8:	mov	x2, x19
   512ec:	stur	x3, [x29, #-32]
   512f0:	bl	c4c0 <__gmpn_sec_tabselect@plt>
   512f4:	cbz	x23, 5143c <__gmpn_sec_powm@@Base+0x43c>
   512f8:	mvn	x8, x22
   512fc:	add	x28, x24, x19, lsl #4
   51300:	str	x8, [sp, #32]
   51304:	b	51374 <__gmpn_sec_powm@@Base+0x374>
   51308:	ldur	x1, [x29, #-16]
   5130c:	ldur	x3, [x29, #-32]
   51310:	mov	x0, x28
   51314:	mov	x2, x19
   51318:	mov	x4, x22
   5131c:	bl	c4c0 <__gmpn_sec_tabselect@plt>
   51320:	mov	x0, x24
   51324:	mov	x1, x21
   51328:	mov	x2, x19
   5132c:	mov	x3, x28
   51330:	mov	x4, x19
   51334:	bl	c550 <__gmpn_mul_basecase@plt>
   51338:	mov	x0, x21
   5133c:	mov	x1, x24
   51340:	mov	x2, x20
   51344:	mov	x3, x19
   51348:	mov	x4, x26
   5134c:	bl	d0f0 <__gmpn_redc_1@plt>
   51350:	mov	x1, x21
   51354:	mov	x2, x21
   51358:	mov	x3, x20
   5135c:	mov	x4, x19
   51360:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51364:	ldur	x27, [x29, #-24]
   51368:	subs	x8, x23, x25
   5136c:	csel	x23, xzr, x8, cc  // cc = lo, ul, last
   51370:	b.ls	5143c <__gmpn_sec_powm@@Base+0x43c>  // b.plast
   51374:	subs	x8, x23, x25
   51378:	b.cs	51394 <__gmpn_sec_powm@@Base+0x394>  // b.hs, b.nlast
   5137c:	ldur	x8, [x29, #-8]
   51380:	mov	x9, #0xffffffffffffffff    	// #-1
   51384:	lsl	x9, x9, x23
   51388:	ldr	x8, [x8]
   5138c:	bic	x22, x8, x9
   51390:	b	513d4 <__gmpn_sec_powm@@Base+0x3d4>
   51394:	ldur	x10, [x29, #-8]
   51398:	lsr	x9, x8, #6
   5139c:	mov	w12, #0x40                  	// #64
   513a0:	ldr	x11, [x10, x9, lsl #3]
   513a4:	and	x10, x8, #0x3f
   513a8:	sub	w10, w12, w10
   513ac:	cmp	w10, w27
   513b0:	lsr	x8, x11, x8
   513b4:	b.ge	513cc <__gmpn_sec_powm@@Base+0x3cc>  // b.tcont
   513b8:	ldur	x11, [x29, #-8]
   513bc:	add	x9, x11, x9, lsl #3
   513c0:	ldr	x9, [x9, #8]
   513c4:	lsl	x9, x9, x10
   513c8:	add	x8, x9, x8
   513cc:	ldr	x9, [sp, #32]
   513d0:	and	x22, x8, x9
   513d4:	cmp	x23, x25
   513d8:	csel	w27, w23, w27, cc  // cc = lo, ul, last
   513dc:	b	51420 <__gmpn_sec_powm@@Base+0x420>
   513e0:	mov	x3, x21
   513e4:	mov	x4, x19
   513e8:	bl	c550 <__gmpn_mul_basecase@plt>
   513ec:	mov	x0, x21
   513f0:	mov	x1, x24
   513f4:	mov	x2, x20
   513f8:	mov	x3, x19
   513fc:	mov	x4, x26
   51400:	bl	d0f0 <__gmpn_redc_1@plt>
   51404:	mov	x1, x21
   51408:	mov	x2, x21
   5140c:	mov	x3, x20
   51410:	mov	x4, x19
   51414:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51418:	subs	w27, w27, #0x1
   5141c:	b.eq	51308 <__gmpn_sec_powm@@Base+0x308>  // b.none
   51420:	mov	x0, x24
   51424:	mov	x1, x21
   51428:	mov	x2, x19
   5142c:	cmp	x19, #0x11
   51430:	b.gt	513e0 <__gmpn_sec_powm@@Base+0x3e0>
   51434:	bl	c190 <__gmpn_sqr_basecase@plt>
   51438:	b	513ec <__gmpn_sec_powm@@Base+0x3ec>
   5143c:	mov	x0, x24
   51440:	mov	x1, x21
   51444:	mov	x2, x19
   51448:	bl	ca50 <__gmpn_copyi@plt>
   5144c:	cbz	x19, 51460 <__gmpn_sec_powm@@Base+0x460>
   51450:	ldr	x2, [sp, #40]
   51454:	mov	w1, wzr
   51458:	add	x0, x24, x2
   5145c:	bl	c5f0 <memset@plt>
   51460:	mov	x0, x21
   51464:	mov	x1, x24
   51468:	mov	x2, x20
   5146c:	mov	x3, x19
   51470:	mov	x4, x26
   51474:	bl	d0f0 <__gmpn_redc_1@plt>
   51478:	mov	x1, x21
   5147c:	mov	x2, x21
   51480:	mov	x3, x20
   51484:	mov	x4, x19
   51488:	bl	c020 <__gmpn_cnd_sub_n@plt>
   5148c:	mov	x0, x24
   51490:	mov	x1, x21
   51494:	mov	x2, x20
   51498:	mov	x3, x19
   5149c:	bl	c2d0 <__gmpn_sub_n@plt>
   514a0:	mov	x1, x21
   514a4:	mov	x2, x21
   514a8:	mov	x3, x20
   514ac:	mov	x4, x19
   514b0:	ldp	x20, x19, [sp, #160]
   514b4:	ldp	x22, x21, [sp, #144]
   514b8:	ldp	x24, x23, [sp, #128]
   514bc:	ldp	x26, x25, [sp, #112]
   514c0:	ldp	x28, x27, [sp, #96]
   514c4:	ldp	x29, x30, [sp, #80]
   514c8:	cmp	w0, #0x0
   514cc:	cset	w0, eq  // eq = none
   514d0:	add	sp, sp, #0xb0
   514d4:	b	c020 <__gmpn_cnd_sub_n@plt>
   514d8:	adrp	x0, 5b000 <__gmpn_bases@@Base+0x2678>
   514dc:	adrp	x2, 5b000 <__gmpn_bases@@Base+0x2678>
   514e0:	add	x0, x0, #0xc50
   514e4:	add	x2, x2, #0xc5b
   514e8:	mov	w1, #0x12a                 	// #298
   514ec:	bl	c6c0 <__gmp_assert_fail@plt>

00000000000514f0 <__gmpn_sec_powm_itch@@Base>:
   514f0:	adrp	x9, 5b000 <__gmpn_bases@@Base+0x2678>
   514f4:	mov	x8, xzr
   514f8:	add	x9, x9, #0xc70
   514fc:	add	x10, x9, x8, lsl #3
   51500:	ldr	x10, [x10, #8]
   51504:	add	x8, x8, #0x1
   51508:	cmp	x10, x1
   5150c:	b.cc	514fc <__gmpn_sec_powm_itch@@Base+0xc>  // b.lo, b.ul, b.last
   51510:	add	x9, x2, x0
   51514:	add	x9, x9, x2, lsl #1
   51518:	lsl	x8, x2, x8
   5151c:	lsl	x9, x9, #1
   51520:	add	x9, x9, #0x2
   51524:	add	x8, x8, x2, lsl #2
   51528:	cmp	x8, x9
   5152c:	csel	x0, x8, x9, gt
   51530:	ret

0000000000051534 <__gmpn_sec_mul@@Base>:
   51534:	b	c550 <__gmpn_mul_basecase@plt>

0000000000051538 <__gmpn_sec_mul_itch@@Base>:
   51538:	mov	x0, xzr
   5153c:	ret

0000000000051540 <__gmpn_sec_sqr@@Base>:
   51540:	mov	x3, x1
   51544:	mov	x4, x2
   51548:	b	c550 <__gmpn_mul_basecase@plt>

000000000005154c <__gmpn_sec_sqr_itch@@Base>:
   5154c:	mov	x0, xzr
   51550:	ret

0000000000051554 <__gmpn_sec_div_qr_itch@@Base>:
   51554:	add	x8, x0, x0, lsl #1
   51558:	add	x0, x8, #0x4
   5155c:	ret

0000000000051560 <__gmpn_sec_div_qr@@Base>:
   51560:	sub	sp, sp, #0x70
   51564:	stp	x29, x30, [sp, #16]
   51568:	stp	x28, x27, [sp, #32]
   5156c:	stp	x26, x25, [sp, #48]
   51570:	stp	x24, x23, [sp, #64]
   51574:	stp	x22, x21, [sp, #80]
   51578:	stp	x20, x19, [sp, #96]
   5157c:	sub	x26, x4, #0x1
   51580:	ldr	x8, [x3, x26, lsl #3]
   51584:	mov	x21, x5
   51588:	mov	x19, x4
   5158c:	mov	x25, x3
   51590:	mov	x22, x2
   51594:	mov	x20, x1
   51598:	clz	x23, x8
   5159c:	mov	x24, x0
   515a0:	add	x29, sp, #0x10
   515a4:	cbz	w23, 5166c <__gmpn_sec_div_qr@@Base+0x10c>
   515a8:	mov	x0, x21
   515ac:	mov	x1, x25
   515b0:	mov	x2, x19
   515b4:	mov	w3, w23
   515b8:	bl	c180 <__gmpn_lshift@plt>
   515bc:	lsl	x28, x19, #3
   515c0:	add	x25, x21, x28
   515c4:	mov	x0, x25
   515c8:	mov	x1, x20
   515cc:	mov	x2, x22
   515d0:	mov	w3, w23
   515d4:	bl	c180 <__gmpn_lshift@plt>
   515d8:	str	x24, [sp, #8]
   515dc:	lsl	x24, x22, #3
   515e0:	str	x0, [x25, x24]
   515e4:	ldr	x8, [x21, x26, lsl #3]
   515e8:	add	x26, x22, #0x1
   515ec:	cmn	x8, #0x1
   515f0:	cinc	x0, x8, ne  // ne = any
   515f4:	bl	d3f0 <__gmpn_invert_limb@plt>
   515f8:	add	x27, x25, x28
   515fc:	add	x8, x21, x26, lsl #3
   51600:	mov	x5, x0
   51604:	add	x6, x8, x28
   51608:	mov	x0, x27
   5160c:	mov	x1, x25
   51610:	mov	x2, x26
   51614:	mov	x3, x21
   51618:	mov	x4, x19
   5161c:	bl	cef0 <__gmpn_sec_pi1_div_qr@plt>
   51620:	ldr	x0, [sp, #8]
   51624:	sub	x2, x22, x19
   51628:	mov	x1, x27
   5162c:	bl	ca50 <__gmpn_copyi@plt>
   51630:	ldr	x21, [x25, x24]
   51634:	mov	x0, x20
   51638:	mov	x1, x25
   5163c:	mov	x2, x19
   51640:	mov	w3, w23
   51644:	bl	c1a0 <__gmpn_rshift@plt>
   51648:	mov	x0, x21
   5164c:	ldp	x20, x19, [sp, #96]
   51650:	ldp	x22, x21, [sp, #80]
   51654:	ldp	x24, x23, [sp, #64]
   51658:	ldp	x26, x25, [sp, #48]
   5165c:	ldp	x28, x27, [sp, #32]
   51660:	ldp	x29, x30, [sp, #16]
   51664:	add	sp, sp, #0x70
   51668:	ret
   5166c:	cmn	x8, #0x1
   51670:	cinc	x0, x8, ne  // ne = any
   51674:	bl	d3f0 <__gmpn_invert_limb@plt>
   51678:	mov	x5, x0
   5167c:	mov	x0, x24
   51680:	mov	x1, x20
   51684:	mov	x2, x22
   51688:	mov	x3, x25
   5168c:	mov	x4, x19
   51690:	mov	x6, x21
   51694:	ldp	x20, x19, [sp, #96]
   51698:	ldp	x22, x21, [sp, #80]
   5169c:	ldp	x24, x23, [sp, #64]
   516a0:	ldp	x26, x25, [sp, #48]
   516a4:	ldp	x28, x27, [sp, #32]
   516a8:	ldp	x29, x30, [sp, #16]
   516ac:	add	sp, sp, #0x70
   516b0:	b	cef0 <__gmpn_sec_pi1_div_qr@plt>

00000000000516b4 <__gmpn_sec_div_r_itch@@Base>:
   516b4:	add	x8, x0, x1, lsl #1
   516b8:	add	x0, x8, #0x2
   516bc:	ret

00000000000516c0 <__gmpn_sec_div_r@@Base>:
   516c0:	stp	x29, x30, [sp, #-80]!
   516c4:	stp	x26, x25, [sp, #16]
   516c8:	stp	x24, x23, [sp, #32]
   516cc:	stp	x22, x21, [sp, #48]
   516d0:	stp	x20, x19, [sp, #64]
   516d4:	sub	x25, x3, #0x1
   516d8:	ldr	x8, [x2, x25, lsl #3]
   516dc:	mov	x20, x4
   516e0:	mov	x19, x3
   516e4:	mov	x24, x2
   516e8:	mov	x23, x1
   516ec:	clz	x22, x8
   516f0:	mov	x21, x0
   516f4:	mov	x29, sp
   516f8:	cbz	w22, 5178c <__gmpn_sec_div_r@@Base+0xcc>
   516fc:	mov	x0, x20
   51700:	mov	x1, x24
   51704:	mov	x2, x19
   51708:	mov	w3, w22
   5170c:	bl	c180 <__gmpn_lshift@plt>
   51710:	lsl	x26, x19, #3
   51714:	add	x24, x20, x26
   51718:	mov	x0, x24
   5171c:	mov	x1, x21
   51720:	mov	x2, x23
   51724:	mov	w3, w22
   51728:	bl	c180 <__gmpn_lshift@plt>
   5172c:	str	x0, [x24, x23, lsl #3]
   51730:	ldr	x8, [x20, x25, lsl #3]
   51734:	add	x23, x23, #0x1
   51738:	cmn	x8, #0x1
   5173c:	cinc	x0, x8, ne  // ne = any
   51740:	bl	d3f0 <__gmpn_invert_limb@plt>
   51744:	add	x8, x20, x23, lsl #3
   51748:	mov	x4, x0
   5174c:	add	x5, x8, x26
   51750:	mov	x0, x24
   51754:	mov	x1, x23
   51758:	mov	x2, x20
   5175c:	mov	x3, x19
   51760:	bl	c800 <__gmpn_sec_pi1_div_r@plt>
   51764:	mov	x0, x21
   51768:	mov	x1, x24
   5176c:	mov	x2, x19
   51770:	mov	w3, w22
   51774:	ldp	x20, x19, [sp, #64]
   51778:	ldp	x22, x21, [sp, #48]
   5177c:	ldp	x24, x23, [sp, #32]
   51780:	ldp	x26, x25, [sp, #16]
   51784:	ldp	x29, x30, [sp], #80
   51788:	b	c1a0 <__gmpn_rshift@plt>
   5178c:	cmn	x8, #0x1
   51790:	cinc	x0, x8, ne  // ne = any
   51794:	bl	d3f0 <__gmpn_invert_limb@plt>
   51798:	mov	x4, x0
   5179c:	mov	x0, x21
   517a0:	mov	x1, x23
   517a4:	mov	x2, x24
   517a8:	mov	x3, x19
   517ac:	mov	x5, x20
   517b0:	ldp	x20, x19, [sp, #64]
   517b4:	ldp	x22, x21, [sp, #48]
   517b8:	ldp	x24, x23, [sp, #32]
   517bc:	ldp	x26, x25, [sp, #16]
   517c0:	ldp	x29, x30, [sp], #80
   517c4:	b	c800 <__gmpn_sec_pi1_div_r@plt>

00000000000517c8 <__gmpn_sec_pi1_div_qr@@Base>:
   517c8:	sub	sp, sp, #0xa0
   517cc:	stp	x29, x30, [sp, #64]
   517d0:	stp	x28, x27, [sp, #80]
   517d4:	stp	x22, x21, [sp, #128]
   517d8:	stp	x20, x19, [sp, #144]
   517dc:	add	x29, sp, #0x40
   517e0:	mov	x28, x3
   517e4:	mov	x21, x2
   517e8:	mov	x19, x1
   517ec:	subs	x22, x2, x4
   517f0:	stp	x26, x25, [sp, #96]
   517f4:	stp	x24, x23, [sp, #112]
   517f8:	stur	x6, [x29, #-8]
   517fc:	b.ne	51838 <__gmpn_sec_pi1_div_qr@@Base+0x70>  // b.any
   51800:	mov	x0, x19
   51804:	mov	x1, x19
   51808:	mov	x2, x28
   5180c:	mov	x3, x21
   51810:	bl	c2d0 <__gmpn_sub_n@plt>
   51814:	mov	x1, x19
   51818:	mov	x2, x19
   5181c:	mov	x3, x28
   51820:	mov	x4, x21
   51824:	mov	x20, x0
   51828:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   5182c:	mov	w8, #0x1                   	// #1
   51830:	sub	x0, x8, x20
   51834:	b	51a38 <__gmpn_sec_pi1_div_qr@@Base+0x270>
   51838:	mov	x20, x19
   5183c:	ldur	x19, [x29, #-8]
   51840:	mov	x23, x0
   51844:	mov	w3, #0x20                  	// #32
   51848:	mov	x1, x28
   5184c:	mov	x0, x19
   51850:	mov	x2, x4
   51854:	mov	x27, x5
   51858:	mov	x26, x4
   5185c:	bl	c180 <__gmpn_lshift@plt>
   51860:	add	x12, x26, #0x1
   51864:	add	x8, x19, x21, lsl #3
   51868:	cmp	x22, #0x1
   5186c:	add	x24, x19, x12, lsl #3
   51870:	add	x25, x8, #0x8
   51874:	add	x10, x20, x22, lsl #3
   51878:	str	x0, [x19, x26, lsl #3]
   5187c:	b.lt	5195c <__gmpn_sec_pi1_div_qr@@Base+0x194>  // b.tstop
   51880:	ldur	x11, [x29, #-8]
   51884:	lsl	x9, x21, #1
   51888:	mov	x14, x20
   5188c:	lsl	x8, x21, #3
   51890:	sub	x9, x9, x26
   51894:	stp	x25, x24, [sp]
   51898:	stp	x23, x22, [sp, #16]
   5189c:	mov	x23, xzr
   518a0:	mov	x20, xzr
   518a4:	add	x24, x22, #0x1
   518a8:	add	x13, x11, x8
   518ac:	add	x9, x11, x9, lsl #3
   518b0:	add	x8, x14, x8
   518b4:	stp	x9, x13, [x29, #-24]
   518b8:	str	x8, [sp, #32]
   518bc:	ldr	x8, [sp, #32]
   518c0:	mov	x19, x26
   518c4:	mov	x26, x28
   518c8:	add	x9, x10, x23
   518cc:	add	x28, x8, x23
   518d0:	ldur	x8, [x28, #-8]
   518d4:	sub	x21, x9, #0x8
   518d8:	ldur	x1, [x29, #-8]
   518dc:	mov	x0, x21
   518e0:	extr	x8, x20, x8, #32
   518e4:	umulh	x9, x8, x27
   518e8:	add	x3, x8, x9
   518ec:	ldur	x8, [x29, #-24]
   518f0:	mov	x2, x12
   518f4:	mov	x22, x10
   518f8:	mov	x25, x12
   518fc:	str	x3, [x8, x23]
   51900:	bl	c9e0 <__gmpn_submul_1@plt>
   51904:	ldur	x20, [x28, #-8]
   51908:	umulh	x8, x20, x27
   5190c:	mov	x28, x26
   51910:	mov	x0, x21
   51914:	add	x3, x8, x20
   51918:	ldur	x8, [x29, #-16]
   5191c:	mov	x1, x28
   51920:	mov	x2, x19
   51924:	mov	x26, x19
   51928:	str	x3, [x8, x23]
   5192c:	bl	c9e0 <__gmpn_submul_1@plt>
   51930:	sub	x24, x24, #0x1
   51934:	mov	x12, x25
   51938:	mov	x10, x22
   5193c:	sub	x20, x20, x0
   51940:	cmp	x24, #0x1
   51944:	sub	x23, x23, #0x8
   51948:	b.gt	518bc <__gmpn_sec_pi1_div_qr@@Base+0xf4>
   5194c:	add	x10, x10, x23
   51950:	ldp	x23, x22, [sp, #16]
   51954:	ldp	x25, x24, [sp]
   51958:	b	51960 <__gmpn_sec_pi1_div_qr@@Base+0x198>
   5195c:	mov	x20, xzr
   51960:	ldr	x8, [x24]
   51964:	cmp	x20, #0x0
   51968:	cset	w0, ne  // ne = any
   5196c:	mov	x1, x10
   51970:	cinc	x8, x8, ne  // ne = any
   51974:	mov	x2, x10
   51978:	mov	x3, x28
   5197c:	mov	x4, x26
   51980:	str	x8, [x24]
   51984:	mov	x19, x10
   51988:	bl	c020 <__gmpn_cnd_sub_n@plt>
   5198c:	mov	x21, x0
   51990:	mov	x0, x19
   51994:	mov	x1, x19
   51998:	mov	x2, x28
   5199c:	mov	x3, x26
   519a0:	bl	c2d0 <__gmpn_sub_n@plt>
   519a4:	ldr	x8, [x24]
   519a8:	sub	x9, x21, x20
   519ac:	add	x0, x0, x9
   519b0:	mov	x1, x19
   519b4:	sub	x8, x8, x0
   519b8:	add	x8, x8, #0x1
   519bc:	mov	x2, x19
   519c0:	mov	x3, x28
   519c4:	mov	x4, x26
   519c8:	str	x8, [x24]
   519cc:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   519d0:	mov	x0, x19
   519d4:	mov	x1, x19
   519d8:	mov	x2, x28
   519dc:	mov	x3, x26
   519e0:	bl	c2d0 <__gmpn_sub_n@plt>
   519e4:	ldr	x8, [x24]
   519e8:	mov	x1, x19
   519ec:	mov	x2, x19
   519f0:	mov	x3, x28
   519f4:	sub	x8, x8, x0
   519f8:	add	x8, x8, #0x1
   519fc:	mov	x4, x26
   51a00:	str	x8, [x24]
   51a04:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   51a08:	mov	w3, #0x20                  	// #32
   51a0c:	mov	x0, x25
   51a10:	mov	x1, x25
   51a14:	mov	x2, x22
   51a18:	bl	c180 <__gmpn_lshift@plt>
   51a1c:	mov	x19, x0
   51a20:	mov	x0, x23
   51a24:	mov	x1, x25
   51a28:	mov	x2, x24
   51a2c:	mov	x3, x22
   51a30:	bl	ca70 <__gmpn_add_n@plt>
   51a34:	add	x0, x0, x19
   51a38:	ldp	x20, x19, [sp, #144]
   51a3c:	ldp	x22, x21, [sp, #128]
   51a40:	ldp	x24, x23, [sp, #112]
   51a44:	ldp	x26, x25, [sp, #96]
   51a48:	ldp	x28, x27, [sp, #80]
   51a4c:	ldp	x29, x30, [sp, #64]
   51a50:	add	sp, sp, #0xa0
   51a54:	ret

0000000000051a58 <__gmpn_sec_pi1_div_r@@Base>:
   51a58:	stp	x29, x30, [sp, #-96]!
   51a5c:	stp	x28, x27, [sp, #16]
   51a60:	stp	x26, x25, [sp, #32]
   51a64:	stp	x24, x23, [sp, #48]
   51a68:	stp	x20, x19, [sp, #80]
   51a6c:	mov	x19, x2
   51a70:	mov	x24, x1
   51a74:	subs	x28, x1, x3
   51a78:	mov	x25, x0
   51a7c:	stp	x22, x21, [sp, #64]
   51a80:	mov	x29, sp
   51a84:	b.ne	51ab0 <__gmpn_sec_pi1_div_r@@Base+0x58>  // b.any
   51a88:	mov	x0, x25
   51a8c:	mov	x1, x25
   51a90:	mov	x2, x19
   51a94:	mov	x3, x24
   51a98:	bl	c2d0 <__gmpn_sub_n@plt>
   51a9c:	mov	x1, x25
   51aa0:	mov	x2, x25
   51aa4:	mov	x3, x19
   51aa8:	mov	x4, x24
   51aac:	b	51bc4 <__gmpn_sec_pi1_div_r@@Base+0x16c>
   51ab0:	mov	x20, x3
   51ab4:	mov	w3, #0x20                  	// #32
   51ab8:	mov	x0, x5
   51abc:	mov	x1, x19
   51ac0:	mov	x2, x20
   51ac4:	mov	x21, x5
   51ac8:	mov	x22, x4
   51acc:	bl	c180 <__gmpn_lshift@plt>
   51ad0:	cmp	x28, #0x1
   51ad4:	mov	x26, xzr
   51ad8:	str	x0, [x21, x20, lsl #3]
   51adc:	b.lt	51b4c <__gmpn_sec_pi1_div_r@@Base+0xf4>  // b.tstop
   51ae0:	add	x23, x20, #0x1
   51ae4:	add	x25, x25, x24, lsl #3
   51ae8:	neg	x27, x20, lsl #3
   51aec:	add	x28, x28, #0x1
   51af0:	add	x8, x25, x27
   51af4:	ldr	x9, [x25, #-8]!
   51af8:	sub	x24, x8, #0x8
   51afc:	mov	x0, x24
   51b00:	mov	x1, x21
   51b04:	extr	x8, x26, x9, #32
   51b08:	umulh	x9, x8, x22
   51b0c:	add	x3, x8, x9
   51b10:	mov	x2, x23
   51b14:	bl	c9e0 <__gmpn_submul_1@plt>
   51b18:	ldr	x26, [x25]
   51b1c:	umulh	x8, x26, x22
   51b20:	mov	x0, x24
   51b24:	mov	x1, x19
   51b28:	add	x3, x8, x26
   51b2c:	mov	x2, x20
   51b30:	bl	c9e0 <__gmpn_submul_1@plt>
   51b34:	sub	x28, x28, #0x1
   51b38:	cmp	x28, #0x1
   51b3c:	sub	x26, x26, x0
   51b40:	b.gt	51af0 <__gmpn_sec_pi1_div_r@@Base+0x98>
   51b44:	sub	x21, x25, x20, lsl #3
   51b48:	b	51b50 <__gmpn_sec_pi1_div_r@@Base+0xf8>
   51b4c:	add	x21, x25, x28, lsl #3
   51b50:	cmp	x26, #0x0
   51b54:	cset	w0, ne  // ne = any
   51b58:	mov	x1, x21
   51b5c:	mov	x2, x21
   51b60:	mov	x3, x19
   51b64:	mov	x4, x20
   51b68:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51b6c:	mov	x22, x0
   51b70:	mov	x0, x21
   51b74:	mov	x1, x21
   51b78:	mov	x2, x19
   51b7c:	mov	x3, x20
   51b80:	bl	c2d0 <__gmpn_sub_n@plt>
   51b84:	sub	x8, x22, x26
   51b88:	add	x0, x8, x0
   51b8c:	mov	x1, x21
   51b90:	mov	x2, x21
   51b94:	mov	x3, x19
   51b98:	mov	x4, x20
   51b9c:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   51ba0:	mov	x0, x21
   51ba4:	mov	x1, x21
   51ba8:	mov	x2, x19
   51bac:	mov	x3, x20
   51bb0:	bl	c2d0 <__gmpn_sub_n@plt>
   51bb4:	mov	x1, x21
   51bb8:	mov	x2, x21
   51bbc:	mov	x3, x19
   51bc0:	mov	x4, x20
   51bc4:	ldp	x20, x19, [sp, #80]
   51bc8:	ldp	x22, x21, [sp, #64]
   51bcc:	ldp	x24, x23, [sp, #48]
   51bd0:	ldp	x26, x25, [sp, #32]
   51bd4:	ldp	x28, x27, [sp, #16]
   51bd8:	ldp	x29, x30, [sp], #96
   51bdc:	b	d4d0 <__gmpn_cnd_add_n@plt>

0000000000051be0 <__gmpn_sec_add_1_itch@@Base>:
   51be0:	ret

0000000000051be4 <__gmpn_sec_add_1@@Base>:
   51be4:	stp	x29, x30, [sp, #-48]!
   51be8:	stp	x22, x21, [sp, #16]
   51bec:	stp	x20, x19, [sp, #32]
   51bf0:	mov	x19, x4
   51bf4:	mov	x20, x2
   51bf8:	mov	x21, x1
   51bfc:	mov	x22, x0
   51c00:	cmp	x2, #0x1
   51c04:	mov	x29, sp
   51c08:	str	x3, [x4]
   51c0c:	b.eq	51c24 <__gmpn_sec_add_1@@Base+0x40>  // b.none
   51c10:	lsl	x8, x20, #3
   51c14:	add	x0, x19, #0x8
   51c18:	sub	x2, x8, #0x8
   51c1c:	mov	w1, wzr
   51c20:	bl	c5f0 <memset@plt>
   51c24:	mov	x0, x22
   51c28:	mov	x1, x21
   51c2c:	mov	x2, x19
   51c30:	mov	x3, x20
   51c34:	ldp	x20, x19, [sp, #32]
   51c38:	ldp	x22, x21, [sp, #16]
   51c3c:	ldp	x29, x30, [sp], #48
   51c40:	b	ca70 <__gmpn_add_n@plt>

0000000000051c44 <__gmpn_sec_sub_1_itch@@Base>:
   51c44:	ret

0000000000051c48 <__gmpn_sec_sub_1@@Base>:
   51c48:	stp	x29, x30, [sp, #-48]!
   51c4c:	stp	x22, x21, [sp, #16]
   51c50:	stp	x20, x19, [sp, #32]
   51c54:	mov	x19, x4
   51c58:	mov	x20, x2
   51c5c:	mov	x21, x1
   51c60:	mov	x22, x0
   51c64:	cmp	x2, #0x1
   51c68:	mov	x29, sp
   51c6c:	str	x3, [x4]
   51c70:	b.eq	51c88 <__gmpn_sec_sub_1@@Base+0x40>  // b.none
   51c74:	lsl	x8, x20, #3
   51c78:	add	x0, x19, #0x8
   51c7c:	sub	x2, x8, #0x8
   51c80:	mov	w1, wzr
   51c84:	bl	c5f0 <memset@plt>
   51c88:	mov	x0, x22
   51c8c:	mov	x1, x21
   51c90:	mov	x2, x19
   51c94:	mov	x3, x20
   51c98:	ldp	x20, x19, [sp, #32]
   51c9c:	ldp	x22, x21, [sp, #16]
   51ca0:	ldp	x29, x30, [sp], #48
   51ca4:	b	c2d0 <__gmpn_sub_n@plt>

0000000000051ca8 <__gmpn_sec_invert_itch@@Base>:
   51ca8:	lsl	x0, x0, #2
   51cac:	ret

0000000000051cb0 <__gmpn_sec_invert@@Base>:
   51cb0:	sub	sp, sp, #0x70
   51cb4:	stp	x26, x25, [sp, #48]
   51cb8:	add	x26, x5, x3, lsl #4
   51cbc:	stp	x20, x19, [sp, #96]
   51cc0:	mov	x19, x0
   51cc4:	mov	w8, #0x1                   	// #1
   51cc8:	mov	x0, x26
   51ccc:	stp	x29, x30, [sp, #16]
   51cd0:	stp	x28, x27, [sp, #32]
   51cd4:	stp	x24, x23, [sp, #64]
   51cd8:	stp	x22, x21, [sp, #80]
   51cdc:	mov	x24, x1
   51ce0:	str	x8, [x0], #8
   51ce4:	sub	x1, x3, #0x1
   51ce8:	add	x29, sp, #0x10
   51cec:	mov	x25, x5
   51cf0:	mov	x22, x4
   51cf4:	mov	x21, x3
   51cf8:	mov	x28, x2
   51cfc:	str	x1, [sp]
   51d00:	bl	cf30 <__gmpn_zero@plt>
   51d04:	add	x27, x25, x21, lsl #3
   51d08:	mov	x0, x27
   51d0c:	mov	x1, x28
   51d10:	mov	x2, x21
   51d14:	bl	ca50 <__gmpn_copyi@plt>
   51d18:	mov	x0, x19
   51d1c:	mov	x1, x21
   51d20:	str	x19, [sp, #8]
   51d24:	bl	cf30 <__gmpn_zero@plt>
   51d28:	mov	w8, #0x18                  	// #24
   51d2c:	madd	x23, x21, x8, x25
   51d30:	mov	w3, #0x1                   	// #1
   51d34:	mov	x0, x23
   51d38:	mov	x1, x28
   51d3c:	mov	x2, x21
   51d40:	bl	c1a0 <__gmpn_rshift@plt>
   51d44:	mov	w3, #0x1                   	// #1
   51d48:	mov	x0, x23
   51d4c:	mov	x1, x23
   51d50:	mov	x2, x21
   51d54:	mov	x4, x25
   51d58:	bl	c380 <__gmpn_sec_add_1@plt>
   51d5c:	cbz	x22, 51e54 <__gmpn_sec_invert@@Base+0x1a4>
   51d60:	ldr	x8, [x24]
   51d64:	mov	x1, x24
   51d68:	mov	x2, x24
   51d6c:	mov	x3, x27
   51d70:	and	x20, x8, #0x1
   51d74:	mov	x0, x20
   51d78:	mov	x4, x21
   51d7c:	sub	x22, x22, #0x1
   51d80:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51d84:	mov	x1, x27
   51d88:	mov	x2, x27
   51d8c:	mov	x3, x24
   51d90:	mov	x4, x21
   51d94:	mov	x19, x23
   51d98:	mov	x23, x0
   51d9c:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   51da0:	mov	w3, #0x1                   	// #1
   51da4:	mov	x0, x25
   51da8:	mov	x1, x24
   51dac:	mov	x2, x21
   51db0:	bl	c180 <__gmpn_lshift@plt>
   51db4:	sxtw	x0, w23
   51db8:	mov	x1, x24
   51dbc:	mov	x2, x24
   51dc0:	mov	x3, x25
   51dc4:	mov	x4, x21
   51dc8:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51dcc:	mov	x0, x23
   51dd0:	mov	x23, x19
   51dd4:	ldr	x19, [sp, #8]
   51dd8:	mov	x1, x26
   51ddc:	mov	x3, x21
   51de0:	mov	x2, x19
   51de4:	bl	c660 <__gmpn_cnd_swap@plt>
   51de8:	mov	x0, x20
   51dec:	mov	x1, x26
   51df0:	mov	x2, x26
   51df4:	mov	x3, x19
   51df8:	mov	x4, x21
   51dfc:	bl	c020 <__gmpn_cnd_sub_n@plt>
   51e00:	mov	x1, x26
   51e04:	mov	x2, x26
   51e08:	mov	x3, x28
   51e0c:	mov	x4, x21
   51e10:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   51e14:	mov	w3, #0x1                   	// #1
   51e18:	mov	x0, x24
   51e1c:	mov	x1, x24
   51e20:	mov	x2, x21
   51e24:	bl	c1a0 <__gmpn_rshift@plt>
   51e28:	mov	w3, #0x1                   	// #1
   51e2c:	mov	x0, x26
   51e30:	mov	x1, x26
   51e34:	mov	x2, x21
   51e38:	bl	c1a0 <__gmpn_rshift@plt>
   51e3c:	mov	x1, x26
   51e40:	mov	x2, x26
   51e44:	mov	x3, x23
   51e48:	mov	x4, x21
   51e4c:	bl	d4d0 <__gmpn_cnd_add_n@plt>
   51e50:	cbnz	x22, 51d60 <__gmpn_sec_invert@@Base+0xb0>
   51e54:	ldr	x8, [x27]
   51e58:	cmp	x21, #0x2
   51e5c:	eor	x8, x8, #0x1
   51e60:	b.lt	51ec4 <__gmpn_sec_invert@@Base+0x214>  // b.tstop
   51e64:	ldr	x15, [sp]
   51e68:	cmp	x15, #0x2
   51e6c:	b.cc	51eac <__gmpn_sec_invert@@Base+0x1fc>  // b.lo, b.ul, b.last
   51e70:	and	x10, x15, #0xfffffffffffffffe
   51e74:	add	x11, x25, x21, lsl #4
   51e78:	mov	x9, xzr
   51e7c:	sub	x21, x21, x10
   51e80:	sub	x11, x11, #0x8
   51e84:	mov	x12, x10
   51e88:	ldp	x14, x13, [x11, #-8]
   51e8c:	subs	x12, x12, #0x2
   51e90:	sub	x11, x11, #0x10
   51e94:	orr	x8, x13, x8
   51e98:	orr	x9, x14, x9
   51e9c:	b.ne	51e88 <__gmpn_sec_invert@@Base+0x1d8>  // b.any
   51ea0:	cmp	x15, x10
   51ea4:	orr	x8, x9, x8
   51ea8:	b.eq	51ec4 <__gmpn_sec_invert@@Base+0x214>  // b.none
   51eac:	add	x9, x25, x15, lsl #3
   51eb0:	ldr	x10, [x9, x21, lsl #3]
   51eb4:	cmp	x21, #0x2
   51eb8:	sub	x21, x21, #0x1
   51ebc:	orr	x8, x10, x8
   51ec0:	b.gt	51eb0 <__gmpn_sec_invert@@Base+0x200>
   51ec4:	ldp	x20, x19, [sp, #96]
   51ec8:	ldp	x22, x21, [sp, #80]
   51ecc:	ldp	x24, x23, [sp, #64]
   51ed0:	ldp	x26, x25, [sp, #48]
   51ed4:	ldp	x28, x27, [sp, #32]
   51ed8:	ldp	x29, x30, [sp, #16]
   51edc:	cmp	x8, #0x0
   51ee0:	cset	w0, eq  // eq = none
   51ee4:	add	sp, sp, #0x70
   51ee8:	ret

0000000000051eec <__gmpn_trialdiv@@Base>:
   51eec:	stp	x29, x30, [sp, #-96]!
   51ef0:	stp	x26, x25, [sp, #32]
   51ef4:	stp	x24, x23, [sp, #48]
   51ef8:	stp	x22, x21, [sp, #64]
   51efc:	stp	x20, x19, [sp, #80]
   51f00:	ldr	w23, [x3]
   51f04:	str	x27, [sp, #16]
   51f08:	mov	x29, sp
   51f0c:	cmp	w23, #0xc6
   51f10:	b.hi	51fac <__gmpn_trialdiv@@Base+0xc0>  // b.pmore
   51f14:	adrp	x25, 5f000 <__gmp_jacobi_table@@Base+0x3599>
   51f18:	adrp	x26, 5b000 <__gmpn_bases@@Base+0x2678>
   51f1c:	mov	x19, x3
   51f20:	mov	x20, x2
   51f24:	mov	x21, x1
   51f28:	mov	x22, x0
   51f2c:	mov	w24, #0x48                  	// #72
   51f30:	add	x25, x25, #0xb58
   51f34:	add	x26, x26, #0xca8
   51f38:	madd	x27, x23, x24, x25
   51f3c:	ldr	x8, [x27]
   51f40:	ldr	x9, [x27, #16]
   51f44:	add	x3, x27, #0x8
   51f48:	mov	x0, x22
   51f4c:	mov	x1, x21
   51f50:	lsl	x2, x8, x9
   51f54:	bl	d410 <__gmpn_mod_1s_4p@plt>
   51f58:	ldr	w8, [x27, #64]
   51f5c:	lsr	x9, x8, #24
   51f60:	cbz	w9, 51f94 <__gmpn_trialdiv@@Base+0xa8>
   51f64:	and	x8, x8, #0xffffff
   51f68:	add	x8, x26, x8, lsl #4
   51f6c:	mvn	x10, x9
   51f70:	add	x11, x8, #0x8
   51f74:	ldp	x8, x12, [x11, #-8]
   51f78:	mul	x13, x8, x0
   51f7c:	cmp	x13, x12
   51f80:	b.ls	51fb4 <__gmpn_trialdiv@@Base+0xc8>  // b.plast
   51f84:	add	x10, x10, #0x1
   51f88:	cmn	x10, #0x2
   51f8c:	add	x11, x11, #0x10
   51f90:	b.le	51f74 <__gmpn_trialdiv@@Base+0x88>
   51f94:	sub	x20, x20, x9
   51f98:	cmp	x20, #0x1
   51f9c:	b.lt	51fac <__gmpn_trialdiv@@Base+0xc0>  // b.tstop
   51fa0:	cmp	x23, #0xc6
   51fa4:	add	x23, x23, #0x1
   51fa8:	b.cc	51f38 <__gmpn_trialdiv@@Base+0x4c>  // b.lo, b.ul, b.last
   51fac:	mov	x8, xzr
   51fb0:	b	51fb8 <__gmpn_trialdiv@@Base+0xcc>
   51fb4:	str	w23, [x19]
   51fb8:	ldp	x20, x19, [sp, #80]
   51fbc:	ldp	x22, x21, [sp, #64]
   51fc0:	ldp	x24, x23, [sp, #48]
   51fc4:	ldp	x26, x25, [sp, #32]
   51fc8:	ldr	x27, [sp, #16]
   51fcc:	mov	x0, x8
   51fd0:	ldp	x29, x30, [sp], #96
   51fd4:	ret

0000000000051fd8 <__gmpn_remove@@Base>:
   51fd8:	stp	x29, x30, [sp, #-96]!
   51fdc:	stp	x28, x27, [sp, #16]
   51fe0:	stp	x26, x25, [sp, #32]
   51fe4:	stp	x24, x23, [sp, #48]
   51fe8:	stp	x22, x21, [sp, #64]
   51fec:	stp	x20, x19, [sp, #80]
   51ff0:	mov	x29, sp
   51ff4:	sub	sp, sp, #0x370
   51ff8:	add	x22, x3, #0x1
   51ffc:	add	x8, x22, x5
   52000:	cmp	x8, #0x0
   52004:	cinc	x8, x8, lt  // lt = tstop
   52008:	lsr	x8, x8, #1
   5200c:	add	x8, x8, x22, lsl #1
   52010:	mov	x9, x1
   52014:	lsl	x1, x8, #3
   52018:	mov	w8, #0x7f00                	// #32512
   5201c:	mov	x19, sp
   52020:	mov	x20, x5
   52024:	mov	x21, x4
   52028:	mov	x27, x3
   5202c:	mov	x23, x2
   52030:	cmp	x1, x8
   52034:	str	x6, [x19, #48]
   52038:	str	xzr, [x19, #64]
   5203c:	stp	x0, x9, [x19, #8]
   52040:	b.hi	523cc <__gmpn_remove@@Base+0x3f4>  // b.pmore
   52044:	add	x9, x1, #0xf
   52048:	mov	x8, sp
   5204c:	and	x9, x9, #0xfffffffffffffff0
   52050:	sub	x25, x8, x9
   52054:	mov	sp, x25
   52058:	mov	x0, x25
   5205c:	mov	x1, x23
   52060:	mov	x2, x27
   52064:	bl	ca50 <__gmpn_copyi@plt>
   52068:	cmp	x27, x20
   5206c:	b.ge	52078 <__gmpn_remove@@Base+0xa0>  // b.tcont
   52070:	mov	x21, xzr
   52074:	b	52388 <__gmpn_remove@@Base+0x3b0>
   52078:	lsl	x8, x22, #3
   5207c:	add	x10, x25, x8
   52080:	add	x9, x25, x27, lsl #4
   52084:	add	x8, x10, x8
   52088:	mov	x24, xzr
   5208c:	add	x26, x9, #0x8
   52090:	str	x8, [x19, #24]
   52094:	mov	x8, x25
   52098:	stp	x27, x10, [x19, #32]
   5209c:	str	x25, [x19]
   520a0:	ldr	x25, [x19, #40]
   520a4:	ldr	x1, [x19, #24]
   520a8:	mov	x2, x8
   520ac:	add	x3, x27, #0x1
   520b0:	mov	x0, x25
   520b4:	mov	x4, x21
   520b8:	mov	x5, x20
   520bc:	mov	x28, x24
   520c0:	str	xzr, [x8, x27, lsl #3]
   520c4:	str	x8, [x19, #40]
   520c8:	bl	523e4 <__gmpn_remove@@Base+0x40c>
   520cc:	mov	x8, x20
   520d0:	ldr	x9, [x26, x8, lsl #3]
   520d4:	cbnz	x9, 5217c <__gmpn_remove@@Base+0x1a4>
   520d8:	sub	x8, x8, #0x1
   520dc:	cbnz	x8, 520d0 <__gmpn_remove@@Base+0xf8>
   520e0:	ldr	x10, [x25]
   520e4:	sub	x22, x27, x20
   520e8:	add	x8, x22, #0x1
   520ec:	mov	x9, x25
   520f0:	cbnz	x10, 52108 <__gmpn_remove@@Base+0x130>
   520f4:	subs	x8, x8, #0x1
   520f8:	str	xzr, [x9]
   520fc:	b.eq	52124 <__gmpn_remove@@Base+0x14c>  // b.none
   52100:	ldr	x10, [x9, #8]!
   52104:	cbz	x10, 520f4 <__gmpn_remove@@Base+0x11c>
   52108:	neg	x10, x10
   5210c:	subs	x2, x8, #0x1
   52110:	str	x10, [x9]
   52114:	b.eq	52124 <__gmpn_remove@@Base+0x14c>  // b.none
   52118:	add	x0, x9, #0x8
   5211c:	mov	x1, x0
   52120:	bl	c290 <__gmpn_com@plt>
   52124:	ldr	x8, [x25, x22, lsl #3]
   52128:	lsl	x9, x28, #3
   5212c:	mov	w10, #0x4                   	// #4
   52130:	add	x11, x19, #0x1d8
   52134:	cmp	x8, #0x0
   52138:	ldr	x8, [x19, #48]
   5213c:	lsl	x10, x10, x28
   52140:	str	x21, [x11, x9]
   52144:	add	x11, x19, #0x48
   52148:	str	x20, [x11, x9]
   5214c:	sub	x9, x10, #0x1
   52150:	cinc	x27, x22, ne  // ne = any
   52154:	cmp	x9, x8
   52158:	add	x24, x28, #0x1
   5215c:	b.hi	52234 <__gmpn_remove@@Base+0x25c>  // b.pmore
   52160:	lsl	x8, x20, #1
   52164:	sub	x22, x8, #0x1
   52168:	cmp	x22, x27
   5216c:	b.gt	52234 <__gmpn_remove@@Base+0x25c>
   52170:	cbz	x28, 521a8 <__gmpn_remove@@Base+0x1d0>
   52174:	add	x23, x23, x20, lsl #3
   52178:	b	521d0 <__gmpn_remove@@Base+0x1f8>
   5217c:	sub	x8, x21, #0x8
   52180:	mov	x9, x20
   52184:	subs	x10, x9, #0x1
   52188:	b.lt	520e0 <__gmpn_remove@@Base+0x108>  // b.tstop
   5218c:	lsl	x9, x9, #3
   52190:	ldr	x11, [x26, x9]
   52194:	ldr	x9, [x8, x9]
   52198:	cmp	x11, x9
   5219c:	mov	x9, x10
   521a0:	b.eq	52184 <__gmpn_remove@@Base+0x1ac>  // b.none
   521a4:	b	52210 <__gmpn_remove@@Base+0x238>
   521a8:	lsl	x8, x27, #3
   521ac:	add	x1, x8, #0x190
   521b0:	mov	w8, #0x7f00                	// #32512
   521b4:	cmp	x1, x8
   521b8:	b.hi	52200 <__gmpn_remove@@Base+0x228>  // b.pmore
   521bc:	add	x9, x1, #0xf
   521c0:	mov	x8, sp
   521c4:	and	x9, x9, #0xfffffffffffffff0
   521c8:	sub	x23, x8, x9
   521cc:	mov	sp, x23
   521d0:	mov	x0, x23
   521d4:	mov	x1, x21
   521d8:	mov	x2, x20
   521dc:	bl	c8e0 <__gmpn_sqr@plt>
   521e0:	ldr	x8, [x23, x22, lsl #3]
   521e4:	mov	x21, x23
   521e8:	cmp	x8, #0x0
   521ec:	cinc	x20, x22, ne  // ne = any
   521f0:	cmp	x27, x20
   521f4:	mov	x8, x25
   521f8:	b.ge	520a0 <__gmpn_remove@@Base+0xc8>  // b.tcont
   521fc:	b	52234 <__gmpn_remove@@Base+0x25c>
   52200:	add	x0, x19, #0x40
   52204:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   52208:	mov	x23, x0
   5220c:	b	521d0 <__gmpn_remove@@Base+0x1f8>
   52210:	str	x25, [x19, #56]
   52214:	ldr	x25, [x19, #40]
   52218:	mov	x8, #0xffffffffffffffff    	// #-1
   5221c:	lsl	x8, x8, x28
   52220:	mvn	x21, x8
   52224:	mov	x24, x28
   52228:	mov	x8, x25
   5222c:	cbnz	x28, 5224c <__gmpn_remove@@Base+0x274>
   52230:	b	52388 <__gmpn_remove@@Base+0x3b0>
   52234:	ldr	x9, [x19, #40]
   52238:	mov	x8, #0xfffffffffffffffe    	// #-2
   5223c:	lsl	x8, x8, x28
   52240:	mvn	x21, x8
   52244:	mov	x8, x25
   52248:	str	x9, [x19, #56]
   5224c:	ldr	x9, [x19, #32]
   52250:	ldr	x10, [x19]
   52254:	add	x12, x19, #0x48
   52258:	add	x9, x10, x9, lsl #4
   5225c:	add	x28, x9, #0x8
   52260:	b	52280 <__gmpn_remove@@Base+0x2a8>
   52264:	ldr	x8, [x25, x21, lsl #3]
   52268:	cmp	x8, #0x0
   5226c:	cinc	x27, x21, ne  // ne = any
   52270:	ldr	x21, [x19, #40]
   52274:	cmp	x22, #0x1
   52278:	mov	x8, x25
   5227c:	b.le	52388 <__gmpn_remove@@Base+0x3b0>
   52280:	ldr	x25, [x19, #56]
   52284:	mov	x20, x21
   52288:	str	x8, [x19, #56]
   5228c:	add	x8, x27, #0x1
   52290:	str	x8, [x19, #32]
   52294:	b	522a0 <__gmpn_remove@@Base+0x2c8>
   52298:	cmp	x22, #0x1
   5229c:	b.le	52380 <__gmpn_remove@@Base+0x3a8>
   522a0:	mov	x22, x24
   522a4:	sub	x24, x24, #0x1
   522a8:	ldr	x23, [x12, x24, lsl #3]
   522ac:	subs	x21, x27, x23
   522b0:	b.lt	52298 <__gmpn_remove@@Base+0x2c0>  // b.tstop
   522b4:	mov	w8, #0x1                   	// #1
   522b8:	lsl	x8, x8, x24
   522bc:	add	x9, x8, x20
   522c0:	ldr	x8, [x19, #48]
   522c4:	str	x9, [x19, #40]
   522c8:	cmp	x9, x8
   522cc:	b.hi	52298 <__gmpn_remove@@Base+0x2c0>  // b.pmore
   522d0:	add	x8, x19, #0x1d8
   522d4:	ldr	x26, [x8, x24, lsl #3]
   522d8:	ldr	x2, [x19, #56]
   522dc:	ldp	x1, x3, [x19, #24]
   522e0:	mov	x0, x25
   522e4:	mov	x4, x26
   522e8:	mov	x5, x23
   522ec:	str	xzr, [x2, x27, lsl #3]
   522f0:	bl	523e4 <__gmpn_remove@@Base+0x40c>
   522f4:	add	x12, x19, #0x48
   522f8:	mov	x8, x23
   522fc:	ldr	x9, [x28, x8, lsl #3]
   52300:	cbnz	x9, 52310 <__gmpn_remove@@Base+0x338>
   52304:	sub	x8, x8, #0x1
   52308:	cbnz	x8, 522fc <__gmpn_remove@@Base+0x324>
   5230c:	b	52338 <__gmpn_remove@@Base+0x360>
   52310:	sub	x8, x26, #0x8
   52314:	subs	x9, x23, #0x1
   52318:	b.lt	52338 <__gmpn_remove@@Base+0x360>  // b.tstop
   5231c:	lsl	x10, x23, #3
   52320:	ldr	x11, [x28, x10]
   52324:	ldr	x10, [x8, x10]
   52328:	mov	x23, x9
   5232c:	cmp	x11, x10
   52330:	b.eq	52314 <__gmpn_remove@@Base+0x33c>  // b.none
   52334:	b	52298 <__gmpn_remove@@Base+0x2c0>
   52338:	ldr	x10, [x25]
   5233c:	add	x8, x21, #0x1
   52340:	mov	x9, x25
   52344:	cbnz	x10, 5235c <__gmpn_remove@@Base+0x384>
   52348:	subs	x8, x8, #0x1
   5234c:	str	xzr, [x9]
   52350:	b.eq	52264 <__gmpn_remove@@Base+0x28c>  // b.none
   52354:	ldr	x10, [x9, #8]!
   52358:	cbz	x10, 52348 <__gmpn_remove@@Base+0x370>
   5235c:	neg	x10, x10
   52360:	subs	x2, x8, #0x1
   52364:	str	x10, [x9]
   52368:	b.eq	52264 <__gmpn_remove@@Base+0x28c>  // b.none
   5236c:	add	x0, x9, #0x8
   52370:	mov	x1, x0
   52374:	bl	c290 <__gmpn_com@plt>
   52378:	add	x12, x19, #0x48
   5237c:	b	52264 <__gmpn_remove@@Base+0x28c>
   52380:	ldr	x25, [x19, #56]
   52384:	mov	x21, x20
   52388:	ldr	x0, [x19, #8]
   5238c:	mov	x1, x25
   52390:	mov	x2, x27
   52394:	bl	ca50 <__gmpn_copyi@plt>
   52398:	ldr	x8, [x19, #16]
   5239c:	str	x27, [x8]
   523a0:	ldr	x0, [x19, #64]
   523a4:	cbnz	x0, 523dc <__gmpn_remove@@Base+0x404>
   523a8:	mov	x0, x21
   523ac:	mov	sp, x29
   523b0:	ldp	x20, x19, [sp, #80]
   523b4:	ldp	x22, x21, [sp, #64]
   523b8:	ldp	x24, x23, [sp, #48]
   523bc:	ldp	x26, x25, [sp, #32]
   523c0:	ldp	x28, x27, [sp, #16]
   523c4:	ldp	x29, x30, [sp], #96
   523c8:	ret
   523cc:	add	x0, x19, #0x40
   523d0:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   523d4:	mov	x25, x0
   523d8:	b	52058 <__gmpn_remove@@Base+0x80>
   523dc:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   523e0:	b	523a8 <__gmpn_remove@@Base+0x3d0>
   523e4:	stp	x29, x30, [sp, #-64]!
   523e8:	stp	x24, x23, [sp, #16]
   523ec:	stp	x22, x21, [sp, #32]
   523f0:	stp	x20, x19, [sp, #48]
   523f4:	mov	x29, sp
   523f8:	sub	sp, sp, #0x10
   523fc:	mov	x23, x1
   52400:	mov	x24, x0
   52404:	mov	x0, x3
   52408:	mov	x1, x5
   5240c:	mov	x19, x5
   52410:	mov	x20, x4
   52414:	mov	x21, x3
   52418:	mov	x22, x2
   5241c:	stur	xzr, [x29, #-8]
   52420:	bl	c8c0 <__gmpn_bdiv_qr_itch@plt>
   52424:	lsl	x1, x0, #3
   52428:	mov	w8, #0x7f00                	// #32512
   5242c:	cmp	x1, x8
   52430:	b.hi	52484 <__gmpn_remove@@Base+0x4ac>  // b.pmore
   52434:	add	x9, x1, #0xf
   52438:	mov	x8, sp
   5243c:	and	x9, x9, #0xfffffffffffffff0
   52440:	sub	x6, x8, x9
   52444:	mov	sp, x6
   52448:	mov	x0, x24
   5244c:	mov	x1, x23
   52450:	mov	x2, x22
   52454:	mov	x3, x21
   52458:	mov	x4, x20
   5245c:	mov	x5, x19
   52460:	bl	cf50 <__gmpn_bdiv_qr@plt>
   52464:	ldur	x0, [x29, #-8]
   52468:	cbnz	x0, 52494 <__gmpn_remove@@Base+0x4bc>
   5246c:	mov	sp, x29
   52470:	ldp	x20, x19, [sp, #48]
   52474:	ldp	x22, x21, [sp, #32]
   52478:	ldp	x24, x23, [sp, #16]
   5247c:	ldp	x29, x30, [sp], #64
   52480:	ret
   52484:	sub	x0, x29, #0x8
   52488:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   5248c:	mov	x6, x0
   52490:	b	52448 <__gmpn_remove@@Base+0x470>
   52494:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   52498:	b	5246c <__gmpn_remove@@Base+0x494>
   5249c:	nop

00000000000524a0 <__gmpn_and_n@@Base>:
   524a0:	lsr	x18, x3, #2
   524a4:	tbz	w3, #0, 524ec <__gmpn_and_n@@Base+0x4c>
   524a8:	ldr	x7, [x1]
   524ac:	ldr	x11, [x2]
   524b0:	and	x15, x7, x11
   524b4:	str	x15, [x0], #8
   524b8:	tbnz	w3, #1, 524d4 <__gmpn_and_n@@Base+0x34>
   524bc:	cbz	x18, 5254c <__gmpn_and_n@@Base+0xac>
   524c0:	ldp	x4, x5, [x1, #8]
   524c4:	ldp	x8, x9, [x2, #8]
   524c8:	sub	x1, x1, #0x8
   524cc:	sub	x2, x2, #0x8
   524d0:	b	52524 <__gmpn_and_n@@Base+0x84>
   524d4:	ldp	x6, x7, [x1, #8]
   524d8:	ldp	x10, x11, [x2, #8]
   524dc:	add	x1, x1, #0x8
   524e0:	add	x2, x2, #0x8
   524e4:	cbz	x18, 52540 <__gmpn_and_n@@Base+0xa0>
   524e8:	b	52510 <__gmpn_and_n@@Base+0x70>
   524ec:	tbnz	w3, #1, 524fc <__gmpn_and_n@@Base+0x5c>
   524f0:	ldp	x4, x5, [x1], #-16
   524f4:	ldp	x8, x9, [x2], #-16
   524f8:	b	52524 <__gmpn_and_n@@Base+0x84>
   524fc:	ldp	x6, x7, [x1]
   52500:	ldp	x10, x11, [x2]
   52504:	cbz	x18, 52540 <__gmpn_and_n@@Base+0xa0>
   52508:	nop
   5250c:	nop
   52510:	ldp	x4, x5, [x1, #16]
   52514:	ldp	x8, x9, [x2, #16]
   52518:	and	x12, x6, x10
   5251c:	and	x13, x7, x11
   52520:	stp	x12, x13, [x0], #16
   52524:	ldp	x6, x7, [x1, #32]!
   52528:	ldp	x10, x11, [x2, #32]!
   5252c:	and	x12, x4, x8
   52530:	and	x13, x5, x9
   52534:	stp	x12, x13, [x0], #16
   52538:	sub	x18, x18, #0x1
   5253c:	cbnz	x18, 52510 <__gmpn_and_n@@Base+0x70>
   52540:	and	x12, x6, x10
   52544:	and	x13, x7, x11
   52548:	stp	x12, x13, [x0]
   5254c:	ret

0000000000052550 <__gmpn_andn_n@@Base>:
   52550:	lsr	x18, x3, #2
   52554:	tbz	w3, #0, 5259c <__gmpn_andn_n@@Base+0x4c>
   52558:	ldr	x7, [x1]
   5255c:	ldr	x11, [x2]
   52560:	bic	x15, x7, x11
   52564:	str	x15, [x0], #8
   52568:	tbnz	w3, #1, 52584 <__gmpn_andn_n@@Base+0x34>
   5256c:	cbz	x18, 525fc <__gmpn_andn_n@@Base+0xac>
   52570:	ldp	x4, x5, [x1, #8]
   52574:	ldp	x8, x9, [x2, #8]
   52578:	sub	x1, x1, #0x8
   5257c:	sub	x2, x2, #0x8
   52580:	b	525d4 <__gmpn_andn_n@@Base+0x84>
   52584:	ldp	x6, x7, [x1, #8]
   52588:	ldp	x10, x11, [x2, #8]
   5258c:	add	x1, x1, #0x8
   52590:	add	x2, x2, #0x8
   52594:	cbz	x18, 525f0 <__gmpn_andn_n@@Base+0xa0>
   52598:	b	525c0 <__gmpn_andn_n@@Base+0x70>
   5259c:	tbnz	w3, #1, 525ac <__gmpn_andn_n@@Base+0x5c>
   525a0:	ldp	x4, x5, [x1], #-16
   525a4:	ldp	x8, x9, [x2], #-16
   525a8:	b	525d4 <__gmpn_andn_n@@Base+0x84>
   525ac:	ldp	x6, x7, [x1]
   525b0:	ldp	x10, x11, [x2]
   525b4:	cbz	x18, 525f0 <__gmpn_andn_n@@Base+0xa0>
   525b8:	nop
   525bc:	nop
   525c0:	ldp	x4, x5, [x1, #16]
   525c4:	ldp	x8, x9, [x2, #16]
   525c8:	bic	x12, x6, x10
   525cc:	bic	x13, x7, x11
   525d0:	stp	x12, x13, [x0], #16
   525d4:	ldp	x6, x7, [x1, #32]!
   525d8:	ldp	x10, x11, [x2, #32]!
   525dc:	bic	x12, x4, x8
   525e0:	bic	x13, x5, x9
   525e4:	stp	x12, x13, [x0], #16
   525e8:	sub	x18, x18, #0x1
   525ec:	cbnz	x18, 525c0 <__gmpn_andn_n@@Base+0x70>
   525f0:	bic	x12, x6, x10
   525f4:	bic	x13, x7, x11
   525f8:	stp	x12, x13, [x0]
   525fc:	ret

0000000000052600 <__gmpn_nand_n@@Base>:
   52600:	lsr	x18, x3, #2
   52604:	tbz	w3, #0, 52650 <__gmpn_nand_n@@Base+0x50>
   52608:	ldr	x7, [x1]
   5260c:	ldr	x11, [x2]
   52610:	and	x15, x7, x11
   52614:	mvn	x15, x15
   52618:	str	x15, [x0], #8
   5261c:	tbnz	w3, #1, 52638 <__gmpn_nand_n@@Base+0x38>
   52620:	cbz	x18, 526c4 <__gmpn_nand_n@@Base+0xc4>
   52624:	ldp	x4, x5, [x1, #8]
   52628:	ldp	x8, x9, [x2, #8]
   5262c:	sub	x1, x1, #0x8
   52630:	sub	x2, x2, #0x8
   52634:	b	5268c <__gmpn_nand_n@@Base+0x8c>
   52638:	ldp	x6, x7, [x1, #8]
   5263c:	ldp	x10, x11, [x2, #8]
   52640:	add	x1, x1, #0x8
   52644:	add	x2, x2, #0x8
   52648:	cbz	x18, 526b0 <__gmpn_nand_n@@Base+0xb0>
   5264c:	b	52670 <__gmpn_nand_n@@Base+0x70>
   52650:	tbnz	w3, #1, 52660 <__gmpn_nand_n@@Base+0x60>
   52654:	ldp	x4, x5, [x1], #-16
   52658:	ldp	x8, x9, [x2], #-16
   5265c:	b	5268c <__gmpn_nand_n@@Base+0x8c>
   52660:	ldp	x6, x7, [x1]
   52664:	ldp	x10, x11, [x2]
   52668:	cbz	x18, 526b0 <__gmpn_nand_n@@Base+0xb0>
   5266c:	nop
   52670:	ldp	x4, x5, [x1, #16]
   52674:	ldp	x8, x9, [x2, #16]
   52678:	and	x12, x6, x10
   5267c:	and	x13, x7, x11
   52680:	mvn	x12, x12
   52684:	mvn	x13, x13
   52688:	stp	x12, x13, [x0], #16
   5268c:	ldp	x6, x7, [x1, #32]!
   52690:	ldp	x10, x11, [x2, #32]!
   52694:	and	x12, x4, x8
   52698:	and	x13, x5, x9
   5269c:	mvn	x12, x12
   526a0:	mvn	x13, x13
   526a4:	stp	x12, x13, [x0], #16
   526a8:	sub	x18, x18, #0x1
   526ac:	cbnz	x18, 52670 <__gmpn_nand_n@@Base+0x70>
   526b0:	and	x12, x6, x10
   526b4:	and	x13, x7, x11
   526b8:	mvn	x12, x12
   526bc:	mvn	x13, x13
   526c0:	stp	x12, x13, [x0]
   526c4:	ret
   526c8:	nop
   526cc:	nop

00000000000526d0 <__gmpn_ior_n@@Base>:
   526d0:	lsr	x18, x3, #2
   526d4:	tbz	w3, #0, 5271c <__gmpn_ior_n@@Base+0x4c>
   526d8:	ldr	x7, [x1]
   526dc:	ldr	x11, [x2]
   526e0:	orr	x15, x7, x11
   526e4:	str	x15, [x0], #8
   526e8:	tbnz	w3, #1, 52704 <__gmpn_ior_n@@Base+0x34>
   526ec:	cbz	x18, 5277c <__gmpn_ior_n@@Base+0xac>
   526f0:	ldp	x4, x5, [x1, #8]
   526f4:	ldp	x8, x9, [x2, #8]
   526f8:	sub	x1, x1, #0x8
   526fc:	sub	x2, x2, #0x8
   52700:	b	52754 <__gmpn_ior_n@@Base+0x84>
   52704:	ldp	x6, x7, [x1, #8]
   52708:	ldp	x10, x11, [x2, #8]
   5270c:	add	x1, x1, #0x8
   52710:	add	x2, x2, #0x8
   52714:	cbz	x18, 52770 <__gmpn_ior_n@@Base+0xa0>
   52718:	b	52740 <__gmpn_ior_n@@Base+0x70>
   5271c:	tbnz	w3, #1, 5272c <__gmpn_ior_n@@Base+0x5c>
   52720:	ldp	x4, x5, [x1], #-16
   52724:	ldp	x8, x9, [x2], #-16
   52728:	b	52754 <__gmpn_ior_n@@Base+0x84>
   5272c:	ldp	x6, x7, [x1]
   52730:	ldp	x10, x11, [x2]
   52734:	cbz	x18, 52770 <__gmpn_ior_n@@Base+0xa0>
   52738:	nop
   5273c:	nop
   52740:	ldp	x4, x5, [x1, #16]
   52744:	ldp	x8, x9, [x2, #16]
   52748:	orr	x12, x6, x10
   5274c:	orr	x13, x7, x11
   52750:	stp	x12, x13, [x0], #16
   52754:	ldp	x6, x7, [x1, #32]!
   52758:	ldp	x10, x11, [x2, #32]!
   5275c:	orr	x12, x4, x8
   52760:	orr	x13, x5, x9
   52764:	stp	x12, x13, [x0], #16
   52768:	sub	x18, x18, #0x1
   5276c:	cbnz	x18, 52740 <__gmpn_ior_n@@Base+0x70>
   52770:	orr	x12, x6, x10
   52774:	orr	x13, x7, x11
   52778:	stp	x12, x13, [x0]
   5277c:	ret

0000000000052780 <__gmpn_iorn_n@@Base>:
   52780:	lsr	x18, x3, #2
   52784:	tbz	w3, #0, 527cc <__gmpn_iorn_n@@Base+0x4c>
   52788:	ldr	x7, [x1]
   5278c:	ldr	x11, [x2]
   52790:	orn	x15, x7, x11
   52794:	str	x15, [x0], #8
   52798:	tbnz	w3, #1, 527b4 <__gmpn_iorn_n@@Base+0x34>
   5279c:	cbz	x18, 5282c <__gmpn_iorn_n@@Base+0xac>
   527a0:	ldp	x4, x5, [x1, #8]
   527a4:	ldp	x8, x9, [x2, #8]
   527a8:	sub	x1, x1, #0x8
   527ac:	sub	x2, x2, #0x8
   527b0:	b	52804 <__gmpn_iorn_n@@Base+0x84>
   527b4:	ldp	x6, x7, [x1, #8]
   527b8:	ldp	x10, x11, [x2, #8]
   527bc:	add	x1, x1, #0x8
   527c0:	add	x2, x2, #0x8
   527c4:	cbz	x18, 52820 <__gmpn_iorn_n@@Base+0xa0>
   527c8:	b	527f0 <__gmpn_iorn_n@@Base+0x70>
   527cc:	tbnz	w3, #1, 527dc <__gmpn_iorn_n@@Base+0x5c>
   527d0:	ldp	x4, x5, [x1], #-16
   527d4:	ldp	x8, x9, [x2], #-16
   527d8:	b	52804 <__gmpn_iorn_n@@Base+0x84>
   527dc:	ldp	x6, x7, [x1]
   527e0:	ldp	x10, x11, [x2]
   527e4:	cbz	x18, 52820 <__gmpn_iorn_n@@Base+0xa0>
   527e8:	nop
   527ec:	nop
   527f0:	ldp	x4, x5, [x1, #16]
   527f4:	ldp	x8, x9, [x2, #16]
   527f8:	orn	x12, x6, x10
   527fc:	orn	x13, x7, x11
   52800:	stp	x12, x13, [x0], #16
   52804:	ldp	x6, x7, [x1, #32]!
   52808:	ldp	x10, x11, [x2, #32]!
   5280c:	orn	x12, x4, x8
   52810:	orn	x13, x5, x9
   52814:	stp	x12, x13, [x0], #16
   52818:	sub	x18, x18, #0x1
   5281c:	cbnz	x18, 527f0 <__gmpn_iorn_n@@Base+0x70>
   52820:	orn	x12, x6, x10
   52824:	orn	x13, x7, x11
   52828:	stp	x12, x13, [x0]
   5282c:	ret

0000000000052830 <__gmpn_nior_n@@Base>:
   52830:	lsr	x18, x3, #2
   52834:	tbz	w3, #0, 52880 <__gmpn_nior_n@@Base+0x50>
   52838:	ldr	x7, [x1]
   5283c:	ldr	x11, [x2]
   52840:	orr	x15, x7, x11
   52844:	mvn	x15, x15
   52848:	str	x15, [x0], #8
   5284c:	tbnz	w3, #1, 52868 <__gmpn_nior_n@@Base+0x38>
   52850:	cbz	x18, 528f4 <__gmpn_nior_n@@Base+0xc4>
   52854:	ldp	x4, x5, [x1, #8]
   52858:	ldp	x8, x9, [x2, #8]
   5285c:	sub	x1, x1, #0x8
   52860:	sub	x2, x2, #0x8
   52864:	b	528bc <__gmpn_nior_n@@Base+0x8c>
   52868:	ldp	x6, x7, [x1, #8]
   5286c:	ldp	x10, x11, [x2, #8]
   52870:	add	x1, x1, #0x8
   52874:	add	x2, x2, #0x8
   52878:	cbz	x18, 528e0 <__gmpn_nior_n@@Base+0xb0>
   5287c:	b	528a0 <__gmpn_nior_n@@Base+0x70>
   52880:	tbnz	w3, #1, 52890 <__gmpn_nior_n@@Base+0x60>
   52884:	ldp	x4, x5, [x1], #-16
   52888:	ldp	x8, x9, [x2], #-16
   5288c:	b	528bc <__gmpn_nior_n@@Base+0x8c>
   52890:	ldp	x6, x7, [x1]
   52894:	ldp	x10, x11, [x2]
   52898:	cbz	x18, 528e0 <__gmpn_nior_n@@Base+0xb0>
   5289c:	nop
   528a0:	ldp	x4, x5, [x1, #16]
   528a4:	ldp	x8, x9, [x2, #16]
   528a8:	orr	x12, x6, x10
   528ac:	orr	x13, x7, x11
   528b0:	mvn	x12, x12
   528b4:	mvn	x13, x13
   528b8:	stp	x12, x13, [x0], #16
   528bc:	ldp	x6, x7, [x1, #32]!
   528c0:	ldp	x10, x11, [x2, #32]!
   528c4:	orr	x12, x4, x8
   528c8:	orr	x13, x5, x9
   528cc:	mvn	x12, x12
   528d0:	mvn	x13, x13
   528d4:	stp	x12, x13, [x0], #16
   528d8:	sub	x18, x18, #0x1
   528dc:	cbnz	x18, 528a0 <__gmpn_nior_n@@Base+0x70>
   528e0:	orr	x12, x6, x10
   528e4:	orr	x13, x7, x11
   528e8:	mvn	x12, x12
   528ec:	mvn	x13, x13
   528f0:	stp	x12, x13, [x0]
   528f4:	ret
   528f8:	nop
   528fc:	nop

0000000000052900 <__gmpn_xor_n@@Base>:
   52900:	lsr	x18, x3, #2
   52904:	tbz	w3, #0, 5294c <__gmpn_xor_n@@Base+0x4c>
   52908:	ldr	x7, [x1]
   5290c:	ldr	x11, [x2]
   52910:	eor	x15, x7, x11
   52914:	str	x15, [x0], #8
   52918:	tbnz	w3, #1, 52934 <__gmpn_xor_n@@Base+0x34>
   5291c:	cbz	x18, 529ac <__gmpn_xor_n@@Base+0xac>
   52920:	ldp	x4, x5, [x1, #8]
   52924:	ldp	x8, x9, [x2, #8]
   52928:	sub	x1, x1, #0x8
   5292c:	sub	x2, x2, #0x8
   52930:	b	52984 <__gmpn_xor_n@@Base+0x84>
   52934:	ldp	x6, x7, [x1, #8]
   52938:	ldp	x10, x11, [x2, #8]
   5293c:	add	x1, x1, #0x8
   52940:	add	x2, x2, #0x8
   52944:	cbz	x18, 529a0 <__gmpn_xor_n@@Base+0xa0>
   52948:	b	52970 <__gmpn_xor_n@@Base+0x70>
   5294c:	tbnz	w3, #1, 5295c <__gmpn_xor_n@@Base+0x5c>
   52950:	ldp	x4, x5, [x1], #-16
   52954:	ldp	x8, x9, [x2], #-16
   52958:	b	52984 <__gmpn_xor_n@@Base+0x84>
   5295c:	ldp	x6, x7, [x1]
   52960:	ldp	x10, x11, [x2]
   52964:	cbz	x18, 529a0 <__gmpn_xor_n@@Base+0xa0>
   52968:	nop
   5296c:	nop
   52970:	ldp	x4, x5, [x1, #16]
   52974:	ldp	x8, x9, [x2, #16]
   52978:	eor	x12, x6, x10
   5297c:	eor	x13, x7, x11
   52980:	stp	x12, x13, [x0], #16
   52984:	ldp	x6, x7, [x1, #32]!
   52988:	ldp	x10, x11, [x2, #32]!
   5298c:	eor	x12, x4, x8
   52990:	eor	x13, x5, x9
   52994:	stp	x12, x13, [x0], #16
   52998:	sub	x18, x18, #0x1
   5299c:	cbnz	x18, 52970 <__gmpn_xor_n@@Base+0x70>
   529a0:	eor	x12, x6, x10
   529a4:	eor	x13, x7, x11
   529a8:	stp	x12, x13, [x0]
   529ac:	ret

00000000000529b0 <__gmpn_xnor_n@@Base>:
   529b0:	lsr	x18, x3, #2
   529b4:	tbz	w3, #0, 529fc <__gmpn_xnor_n@@Base+0x4c>
   529b8:	ldr	x7, [x1]
   529bc:	ldr	x11, [x2]
   529c0:	eon	x15, x7, x11
   529c4:	str	x15, [x0], #8
   529c8:	tbnz	w3, #1, 529e4 <__gmpn_xnor_n@@Base+0x34>
   529cc:	cbz	x18, 52a5c <__gmpn_xnor_n@@Base+0xac>
   529d0:	ldp	x4, x5, [x1, #8]
   529d4:	ldp	x8, x9, [x2, #8]
   529d8:	sub	x1, x1, #0x8
   529dc:	sub	x2, x2, #0x8
   529e0:	b	52a34 <__gmpn_xnor_n@@Base+0x84>
   529e4:	ldp	x6, x7, [x1, #8]
   529e8:	ldp	x10, x11, [x2, #8]
   529ec:	add	x1, x1, #0x8
   529f0:	add	x2, x2, #0x8
   529f4:	cbz	x18, 52a50 <__gmpn_xnor_n@@Base+0xa0>
   529f8:	b	52a20 <__gmpn_xnor_n@@Base+0x70>
   529fc:	tbnz	w3, #1, 52a0c <__gmpn_xnor_n@@Base+0x5c>
   52a00:	ldp	x4, x5, [x1], #-16
   52a04:	ldp	x8, x9, [x2], #-16
   52a08:	b	52a34 <__gmpn_xnor_n@@Base+0x84>
   52a0c:	ldp	x6, x7, [x1]
   52a10:	ldp	x10, x11, [x2]
   52a14:	cbz	x18, 52a50 <__gmpn_xnor_n@@Base+0xa0>
   52a18:	nop
   52a1c:	nop
   52a20:	ldp	x4, x5, [x1, #16]
   52a24:	ldp	x8, x9, [x2, #16]
   52a28:	eon	x12, x6, x10
   52a2c:	eon	x13, x7, x11
   52a30:	stp	x12, x13, [x0], #16
   52a34:	ldp	x6, x7, [x1, #32]!
   52a38:	ldp	x10, x11, [x2, #32]!
   52a3c:	eon	x12, x4, x8
   52a40:	eon	x13, x5, x9
   52a44:	stp	x12, x13, [x0], #16
   52a48:	sub	x18, x18, #0x1
   52a4c:	cbnz	x18, 52a20 <__gmpn_xnor_n@@Base+0x70>
   52a50:	eon	x12, x6, x10
   52a54:	eon	x13, x7, x11
   52a58:	stp	x12, x13, [x0]
   52a5c:	ret

0000000000052a60 <__gmpn_copyi@@Base>:
   52a60:	cmp	x2, #0x3
   52a64:	b.le	52aac <__gmpn_copyi@@Base+0x4c>
   52a68:	tbz	w0, #3, 52a78 <__gmpn_copyi@@Base+0x18>
   52a6c:	ld1	{v22.1d}, [x1], #8
   52a70:	sub	x2, x2, #0x1
   52a74:	st1	{v22.1d}, [x0], #8
   52a78:	ld1	{v26.2d}, [x1], #16
   52a7c:	sub	x2, x2, #0x6
   52a80:	tbnz	x2, #63, 52aa8 <__gmpn_copyi@@Base+0x48>
   52a84:	nop
   52a88:	nop
   52a8c:	nop
   52a90:	ld1	{v22.2d}, [x1], #16
   52a94:	st1	{v26.2d}, [x0], #16
   52a98:	ld1	{v26.2d}, [x1], #16
   52a9c:	st1	{v22.2d}, [x0], #16
   52aa0:	sub	x2, x2, #0x4
   52aa4:	tbz	x2, #63, 52a90 <__gmpn_copyi@@Base+0x30>
   52aa8:	st1	{v26.2d}, [x0], #16
   52aac:	tbz	w2, #1, 52ab8 <__gmpn_copyi@@Base+0x58>
   52ab0:	ld1	{v22.2d}, [x1], #16
   52ab4:	st1	{v22.2d}, [x0], #16
   52ab8:	tbz	w2, #0, 52ac4 <__gmpn_copyi@@Base+0x64>
   52abc:	ld1	{v22.1d}, [x1]
   52ac0:	st1	{v22.1d}, [x0]
   52ac4:	ret
   52ac8:	nop
   52acc:	nop

0000000000052ad0 <__gmpn_copyd@@Base>:
   52ad0:	add	x0, x0, x2, lsl #3
   52ad4:	add	x1, x1, x2, lsl #3
   52ad8:	cmp	x2, #0x3
   52adc:	b.le	52b40 <__gmpn_copyd@@Base+0x70>
   52ae0:	tbz	w0, #3, 52af8 <__gmpn_copyd@@Base+0x28>
   52ae4:	sub	x1, x1, #0x8
   52ae8:	ld1	{v22.1d}, [x1]
   52aec:	sub	x2, x2, #0x1
   52af0:	sub	x0, x0, #0x8
   52af4:	st1	{v22.1d}, [x0]
   52af8:	sub	x1, x1, #0x10
   52afc:	ld1	{v26.2d}, [x1]
   52b00:	sub	x2, x2, #0x6
   52b04:	sub	x0, x0, #0x10
   52b08:	tbnz	x2, #63, 52b3c <__gmpn_copyd@@Base+0x6c>
   52b0c:	sub	x1, x1, #0x10
   52b10:	mov	x12, #0xfffffffffffffff0    	// #-16
   52b14:	nop
   52b18:	nop
   52b1c:	nop
   52b20:	ld1	{v22.2d}, [x1], x12
   52b24:	st1	{v26.2d}, [x0], x12
   52b28:	ld1	{v26.2d}, [x1], x12
   52b2c:	st1	{v22.2d}, [x0], x12
   52b30:	sub	x2, x2, #0x4
   52b34:	tbz	x2, #63, 52b20 <__gmpn_copyd@@Base+0x50>
   52b38:	add	x1, x1, #0x10
   52b3c:	st1	{v26.2d}, [x0]
   52b40:	tbz	w2, #1, 52b54 <__gmpn_copyd@@Base+0x84>
   52b44:	sub	x1, x1, #0x10
   52b48:	ld1	{v22.2d}, [x1]
   52b4c:	sub	x0, x0, #0x10
   52b50:	st1	{v22.2d}, [x0]
   52b54:	tbz	w2, #0, 52b68 <__gmpn_copyd@@Base+0x98>
   52b58:	sub	x1, x1, #0x8
   52b5c:	ld1	{v22.1d}, [x1]
   52b60:	sub	x0, x0, #0x8
   52b64:	st1	{v22.1d}, [x0]
   52b68:	ret

0000000000052b6c <__gmpn_zero@@Base>:
   52b6c:	cbz	x1, 52b88 <__gmpn_zero@@Base+0x1c>
   52b70:	stp	x29, x30, [sp, #-16]!
   52b74:	lsl	x2, x1, #3
   52b78:	mov	w1, wzr
   52b7c:	mov	x29, sp
   52b80:	bl	c5f0 <memset@plt>
   52b84:	ldp	x29, x30, [sp], #16
   52b88:	ret
   52b8c:	nop

0000000000052b90 <__gmpn_sec_tabselect@@Base>:
   52b90:	dup	v7.2d, x4
   52b94:	mov	x10, #0x1                   	// #1
   52b98:	dup	v6.2d, x10
   52b9c:	subs	x6, x2, #0x4
   52ba0:	b.mi	52bf0 <__gmpn_sec_tabselect@@Base+0x60>  // b.first
   52ba4:	mov	x5, x3
   52ba8:	mov	x12, x1
   52bac:	movi	v5.16b, #0x0
   52bb0:	movi	v2.16b, #0x0
   52bb4:	movi	v3.16b, #0x0
   52bb8:	nop
   52bbc:	nop
   52bc0:	cmeq	v4.2d, v5.2d, v7.2d
   52bc4:	ld1	{v0.2d, v1.2d}, [x1]
   52bc8:	add	v5.2d, v5.2d, v6.2d
   52bcc:	bit	v2.16b, v0.16b, v4.16b
   52bd0:	bit	v3.16b, v1.16b, v4.16b
   52bd4:	add	x1, x1, x2, lsl #3
   52bd8:	sub	x5, x5, #0x1
   52bdc:	cbnz	x5, 52bc0 <__gmpn_sec_tabselect@@Base+0x30>
   52be0:	st1	{v2.2d, v3.2d}, [x0], #32
   52be4:	add	x1, x12, #0x20
   52be8:	subs	x6, x6, #0x4
   52bec:	b.pl	52ba4 <__gmpn_sec_tabselect@@Base+0x14>  // b.nfrst
   52bf0:	tbz	w2, #1, 52c34 <__gmpn_sec_tabselect@@Base+0xa4>
   52bf4:	mov	x5, x3
   52bf8:	mov	x12, x1
   52bfc:	movi	v5.16b, #0x0
   52c00:	movi	v2.16b, #0x0
   52c04:	nop
   52c08:	nop
   52c0c:	nop
   52c10:	cmeq	v4.2d, v5.2d, v7.2d
   52c14:	ld1	{v0.2d}, [x1]
   52c18:	add	v5.2d, v5.2d, v6.2d
   52c1c:	bit	v2.16b, v0.16b, v4.16b
   52c20:	add	x1, x1, x2, lsl #3
   52c24:	sub	x5, x5, #0x1
   52c28:	cbnz	x5, 52c10 <__gmpn_sec_tabselect@@Base+0x80>
   52c2c:	st1	{v2.2d}, [x0], #16
   52c30:	add	x1, x12, #0x10
   52c34:	tbz	w2, #0, 52c74 <__gmpn_sec_tabselect@@Base+0xe4>
   52c38:	mov	x5, x3
   52c3c:	mov	x12, x1
   52c40:	movi	v5.16b, #0x0
   52c44:	movi	v2.16b, #0x0
   52c48:	nop
   52c4c:	nop
   52c50:	cmeq	v4.2d, v5.2d, v7.2d
   52c54:	ld1	{v0.1d}, [x1]
   52c58:	add	v5.2d, v5.2d, v6.2d
   52c5c:	bit	v2.8b, v0.8b, v4.8b
   52c60:	add	x1, x1, x2, lsl #3
   52c64:	sub	x5, x5, #0x1
   52c68:	cbnz	x5, 52c50 <__gmpn_sec_tabselect@@Base+0xc0>
   52c6c:	st1	{v2.1d}, [x0], #8
   52c70:	add	x1, x12, #0x8
   52c74:	ret

0000000000052c78 <__gmpn_invert_limb@@Base>:
   52c78:	lsr	x2, x0, #54
   52c7c:	adrp	x1, 63000 <__gmp_jacobi_table@@Base+0x7599>
   52c80:	and	x2, x2, #0x1fe
   52c84:	add	x1, x1, #0x660
   52c88:	ldrh	w3, [x1, x2]
   52c8c:	lsr	x4, x0, #24
   52c90:	add	x4, x4, #0x1
   52c94:	ubfiz	x2, x3, #11, #16
   52c98:	umull	x3, w3, w3
   52c9c:	mul	x3, x3, x4
   52ca0:	sub	x2, x2, #0x1
   52ca4:	sub	x2, x2, x3, lsr #40
   52ca8:	lsl	x3, x2, #60
   52cac:	mul	x1, x2, x2
   52cb0:	msub	x1, x1, x4, x3
   52cb4:	lsl	x2, x2, #13
   52cb8:	add	x1, x2, x1, lsr #47
   52cbc:	and	x2, x0, #0x1
   52cc0:	neg	x3, x2
   52cc4:	and	x3, x3, x1, lsr #1
   52cc8:	add	x2, x2, x0, lsr #1
   52ccc:	msub	x2, x1, x2, x3
   52cd0:	umulh	x2, x2, x1
   52cd4:	lsl	x1, x1, #31
   52cd8:	add	x1, x1, x2, lsr #1
   52cdc:	mul	x3, x1, x0
   52ce0:	umulh	x2, x1, x0
   52ce4:	adds	x4, x3, x0
   52ce8:	adc	x0, x2, x0
   52cec:	sub	x0, x1, x0
   52cf0:	ret
   52cf4:	nop
   52cf8:	nop
   52cfc:	nop

0000000000052d00 <__gmpn_sqr_diag_addlsh1@@Base>:
   52d00:	ldr	x15, [x2], #8
   52d04:	lsr	x18, x3, #1
   52d08:	tbz	w3, #0, 52d24 <__gmpn_sqr_diag_addlsh1@@Base+0x24>
   52d0c:	adds	x7, xzr, xzr
   52d10:	mul	x12, x15, x15
   52d14:	ldr	x16, [x2], #8
   52d18:	ldp	x4, x5, [x1], #16
   52d1c:	umulh	x11, x15, x15
   52d20:	b	52d64 <__gmpn_sqr_diag_addlsh1@@Base+0x64>
   52d24:	adds	x5, xzr, xzr
   52d28:	mul	x12, x15, x15
   52d2c:	ldr	x17, [x2], #16
   52d30:	ldp	x6, x7, [x1], #32
   52d34:	umulh	x11, x15, x15
   52d38:	sub	x18, x18, #0x1
   52d3c:	cbz	x18, 52d90 <__gmpn_sqr_diag_addlsh1@@Base+0x90>
   52d40:	extr	x9, x6, x5, #63
   52d44:	mul	x10, x17, x17
   52d48:	ldur	x16, [x2, #-8]
   52d4c:	adcs	x13, x9, x11
   52d50:	ldp	x4, x5, [x1, #-16]
   52d54:	umulh	x11, x17, x17
   52d58:	extr	x8, x7, x6, #63
   52d5c:	stp	x12, x13, [x0], #16
   52d60:	adcs	x12, x8, x10
   52d64:	extr	x9, x4, x7, #63
   52d68:	mul	x10, x16, x16
   52d6c:	ldr	x17, [x2], #16
   52d70:	adcs	x13, x9, x11
   52d74:	ldp	x6, x7, [x1], #32
   52d78:	umulh	x11, x16, x16
   52d7c:	extr	x8, x5, x4, #63
   52d80:	stp	x12, x13, [x0], #16
   52d84:	adcs	x12, x8, x10
   52d88:	sub	x18, x18, #0x1
   52d8c:	cbnz	x18, 52d40 <__gmpn_sqr_diag_addlsh1@@Base+0x40>
   52d90:	extr	x9, x6, x5, #63
   52d94:	mul	x10, x17, x17
   52d98:	adcs	x13, x9, x11
   52d9c:	umulh	x11, x17, x17
   52da0:	extr	x8, x7, x6, #63
   52da4:	stp	x12, x13, [x0]
   52da8:	adcs	x12, x8, x10
   52dac:	extr	x9, xzr, x7, #63
   52db0:	adcs	x13, x9, x11
   52db4:	stp	x12, x13, [x0, #16]
   52db8:	ret
   52dbc:	nop

0000000000052dc0 <__gmpn_addlsh1_n@@Base>:
   52dc0:	lsr	x18, x3, #2
   52dc4:	tbz	w3, #0, 52e2c <__gmpn_addlsh1_n@@Base+0x6c>
   52dc8:	ldr	x5, [x1]
   52dcc:	tbnz	w3, #1, 52e0c <__gmpn_addlsh1_n@@Base+0x4c>
   52dd0:	ldr	x11, [x2]
   52dd4:	cbz	x18, 52df4 <__gmpn_addlsh1_n@@Base+0x34>
   52dd8:	ldp	x8, x9, [x2, #8]
   52ddc:	lsl	x13, x11, #1
   52de0:	adds	x15, x13, x5
   52de4:	str	x15, [x0], #8
   52de8:	sub	x1, x1, #0x18
   52dec:	sub	x2, x2, #0x8
   52df0:	b	52e6c <__gmpn_addlsh1_n@@Base+0xac>
   52df4:	lsl	x13, x11, #1
   52df8:	adds	x15, x13, x5
   52dfc:	str	x15, [x0]
   52e00:	lsr	x0, x11, #63
   52e04:	adc	x0, x0, xzr
   52e08:	ret
   52e0c:	ldr	x9, [x2]
   52e10:	ldp	x10, x11, [x2, #8]!
   52e14:	lsl	x13, x9, #1
   52e18:	adds	x17, x13, x5
   52e1c:	str	x17, [x0], #8
   52e20:	sub	x1, x1, #0x8
   52e24:	cbz	x18, 52e90 <__gmpn_addlsh1_n@@Base+0xd0>
   52e28:	b	52e50 <__gmpn_addlsh1_n@@Base+0x90>
   52e2c:	tbnz	w3, #1, 52e40 <__gmpn_addlsh1_n@@Base+0x80>
   52e30:	adds	x11, xzr, xzr
   52e34:	ldp	x8, x9, [x2], #-16
   52e38:	sub	x1, x1, #0x20
   52e3c:	b	52e6c <__gmpn_addlsh1_n@@Base+0xac>
   52e40:	adds	x9, xzr, xzr
   52e44:	ldp	x10, x11, [x2]
   52e48:	sub	x1, x1, #0x10
   52e4c:	cbz	x18, 52e90 <__gmpn_addlsh1_n@@Base+0xd0>
   52e50:	ldp	x4, x5, [x1, #16]
   52e54:	extr	x12, x10, x9, #63
   52e58:	ldp	x8, x9, [x2, #16]
   52e5c:	extr	x13, x11, x10, #63
   52e60:	adcs	x14, x12, x4
   52e64:	adcs	x15, x13, x5
   52e68:	stp	x14, x15, [x0], #16
   52e6c:	ldp	x4, x5, [x1, #32]!
   52e70:	extr	x12, x8, x11, #63
   52e74:	ldp	x10, x11, [x2, #32]!
   52e78:	extr	x13, x9, x8, #63
   52e7c:	adcs	x16, x12, x4
   52e80:	adcs	x17, x13, x5
   52e84:	stp	x16, x17, [x0], #16
   52e88:	sub	x18, x18, #0x1
   52e8c:	cbnz	x18, 52e50 <__gmpn_addlsh1_n@@Base+0x90>
   52e90:	ldp	x4, x5, [x1, #16]
   52e94:	extr	x12, x10, x9, #63
   52e98:	extr	x13, x11, x10, #63
   52e9c:	adcs	x14, x12, x4
   52ea0:	adcs	x15, x13, x5
   52ea4:	stp	x14, x15, [x0]
   52ea8:	lsr	x0, x11, #63
   52eac:	adc	x0, x0, xzr
   52eb0:	ret
   52eb4:	nop
   52eb8:	nop
   52ebc:	nop

0000000000052ec0 <__gmpn_sublsh1_n@@Base>:
   52ec0:	lsr	x18, x3, #2
   52ec4:	tbz	w3, #0, 52f2c <__gmpn_sublsh1_n@@Base+0x6c>
   52ec8:	ldr	x5, [x1]
   52ecc:	tbnz	w3, #1, 52f0c <__gmpn_sublsh1_n@@Base+0x4c>
   52ed0:	ldr	x11, [x2]
   52ed4:	cbz	x18, 52ef4 <__gmpn_sublsh1_n@@Base+0x34>
   52ed8:	ldp	x8, x9, [x2, #8]
   52edc:	lsl	x13, x11, #1
   52ee0:	subs	x15, x5, x13
   52ee4:	str	x15, [x0], #8
   52ee8:	sub	x1, x1, #0x18
   52eec:	sub	x2, x2, #0x8
   52ef0:	b	52f6c <__gmpn_sublsh1_n@@Base+0xac>
   52ef4:	lsl	x13, x11, #1
   52ef8:	subs	x15, x5, x13
   52efc:	str	x15, [x0]
   52f00:	lsr	x0, x11, #63
   52f04:	cinc	x0, x0, cc  // cc = lo, ul, last
   52f08:	ret
   52f0c:	ldr	x9, [x2]
   52f10:	ldp	x10, x11, [x2, #8]!
   52f14:	lsl	x13, x9, #1
   52f18:	subs	x17, x5, x13
   52f1c:	str	x17, [x0], #8
   52f20:	sub	x1, x1, #0x8
   52f24:	cbz	x18, 52f90 <__gmpn_sublsh1_n@@Base+0xd0>
   52f28:	b	52f50 <__gmpn_sublsh1_n@@Base+0x90>
   52f2c:	tbnz	w3, #1, 52f40 <__gmpn_sublsh1_n@@Base+0x80>
   52f30:	negs	x11, xzr
   52f34:	ldp	x8, x9, [x2], #-16
   52f38:	sub	x1, x1, #0x20
   52f3c:	b	52f6c <__gmpn_sublsh1_n@@Base+0xac>
   52f40:	negs	x9, xzr
   52f44:	ldp	x10, x11, [x2]
   52f48:	sub	x1, x1, #0x10
   52f4c:	cbz	x18, 52f90 <__gmpn_sublsh1_n@@Base+0xd0>
   52f50:	ldp	x4, x5, [x1, #16]
   52f54:	extr	x12, x10, x9, #63
   52f58:	ldp	x8, x9, [x2, #16]
   52f5c:	extr	x13, x11, x10, #63
   52f60:	sbcs	x14, x4, x12
   52f64:	sbcs	x15, x5, x13
   52f68:	stp	x14, x15, [x0], #16
   52f6c:	ldp	x4, x5, [x1, #32]!
   52f70:	extr	x12, x8, x11, #63
   52f74:	ldp	x10, x11, [x2, #32]!
   52f78:	extr	x13, x9, x8, #63
   52f7c:	sbcs	x16, x4, x12
   52f80:	sbcs	x17, x5, x13
   52f84:	stp	x16, x17, [x0], #16
   52f88:	sub	x18, x18, #0x1
   52f8c:	cbnz	x18, 52f50 <__gmpn_sublsh1_n@@Base+0x90>
   52f90:	ldp	x4, x5, [x1, #16]
   52f94:	extr	x12, x10, x9, #63
   52f98:	extr	x13, x11, x10, #63
   52f9c:	sbcs	x14, x4, x12
   52fa0:	sbcs	x15, x5, x13
   52fa4:	stp	x14, x15, [x0]
   52fa8:	lsr	x0, x11, #63
   52fac:	cinc	x0, x0, cc  // cc = lo, ul, last
   52fb0:	ret
   52fb4:	nop
   52fb8:	nop
   52fbc:	nop

0000000000052fc0 <__gmpn_rsblsh1_n@@Base>:
   52fc0:	lsr	x18, x3, #2
   52fc4:	tbz	w3, #0, 5302c <__gmpn_rsblsh1_n@@Base+0x6c>
   52fc8:	ldr	x5, [x1]
   52fcc:	tbnz	w3, #1, 5300c <__gmpn_rsblsh1_n@@Base+0x4c>
   52fd0:	ldr	x11, [x2]
   52fd4:	cbz	x18, 52ff4 <__gmpn_rsblsh1_n@@Base+0x34>
   52fd8:	ldp	x8, x9, [x2, #8]
   52fdc:	lsl	x13, x11, #1
   52fe0:	subs	x15, x13, x5
   52fe4:	str	x15, [x0], #8
   52fe8:	sub	x1, x1, #0x18
   52fec:	sub	x2, x2, #0x8
   52ff0:	b	5306c <__gmpn_rsblsh1_n@@Base+0xac>
   52ff4:	lsl	x13, x11, #1
   52ff8:	subs	x15, x13, x5
   52ffc:	str	x15, [x0]
   53000:	lsr	x0, x11, #63
   53004:	sbc	x0, x0, xzr
   53008:	ret
   5300c:	ldr	x9, [x2]
   53010:	ldp	x10, x11, [x2, #8]!
   53014:	lsl	x13, x9, #1
   53018:	subs	x17, x13, x5
   5301c:	str	x17, [x0], #8
   53020:	sub	x1, x1, #0x8
   53024:	cbz	x18, 53090 <__gmpn_rsblsh1_n@@Base+0xd0>
   53028:	b	53050 <__gmpn_rsblsh1_n@@Base+0x90>
   5302c:	tbnz	w3, #1, 53040 <__gmpn_rsblsh1_n@@Base+0x80>
   53030:	negs	x11, xzr
   53034:	ldp	x8, x9, [x2], #-16
   53038:	sub	x1, x1, #0x20
   5303c:	b	5306c <__gmpn_rsblsh1_n@@Base+0xac>
   53040:	negs	x9, xzr
   53044:	ldp	x10, x11, [x2]
   53048:	sub	x1, x1, #0x10
   5304c:	cbz	x18, 53090 <__gmpn_rsblsh1_n@@Base+0xd0>
   53050:	ldp	x4, x5, [x1, #16]
   53054:	extr	x12, x10, x9, #63
   53058:	ldp	x8, x9, [x2, #16]
   5305c:	extr	x13, x11, x10, #63
   53060:	sbcs	x14, x12, x4
   53064:	sbcs	x15, x13, x5
   53068:	stp	x14, x15, [x0], #16
   5306c:	ldp	x4, x5, [x1, #32]!
   53070:	extr	x12, x8, x11, #63
   53074:	ldp	x10, x11, [x2, #32]!
   53078:	extr	x13, x9, x8, #63
   5307c:	sbcs	x16, x12, x4
   53080:	sbcs	x17, x13, x5
   53084:	stp	x16, x17, [x0], #16
   53088:	sub	x18, x18, #0x1
   5308c:	cbnz	x18, 53050 <__gmpn_rsblsh1_n@@Base+0x90>
   53090:	ldp	x4, x5, [x1, #16]
   53094:	extr	x12, x10, x9, #63
   53098:	extr	x13, x11, x10, #63
   5309c:	sbcs	x14, x12, x4
   530a0:	sbcs	x15, x13, x5
   530a4:	stp	x14, x15, [x0]
   530a8:	lsr	x0, x11, #63
   530ac:	sbc	x0, x0, xzr
   530b0:	ret
   530b4:	nop
   530b8:	nop
   530bc:	nop

00000000000530c0 <__gmpn_rsh1add_n@@Base>:
   530c0:	lsr	x18, x3, #2
   530c4:	tbz	w3, #0, 5316c <__gmpn_rsh1add_n@@Base+0xac>
   530c8:	ldr	x5, [x1], #8
   530cc:	ldr	x9, [x2], #8
   530d0:	tbnz	w3, #1, 53128 <__gmpn_rsh1add_n@@Base+0x68>
   530d4:	adds	x13, x5, x9
   530d8:	and	x10, x13, #0x1
   530dc:	cbz	x18, 53114 <__gmpn_rsh1add_n@@Base+0x54>
   530e0:	ldp	x4, x5, [x1], #48
   530e4:	ldp	x8, x9, [x2], #48
   530e8:	adcs	x14, x4, x8
   530ec:	adcs	x15, x5, x9
   530f0:	ldp	x4, x5, [x1, #-32]
   530f4:	ldp	x8, x9, [x2, #-32]
   530f8:	extr	x17, x14, x13, #1
   530fc:	adcs	x12, x4, x8
   53100:	adcs	x13, x5, x9
   53104:	str	x17, [x0], #24
   53108:	sub	x18, x18, #0x1
   5310c:	cbz	x18, 53210 <__gmpn_rsh1add_n@@Base+0x150>
   53110:	b	531d0 <__gmpn_rsh1add_n@@Base+0x110>
   53114:	cset	x14, cs  // cs = hs, nlast
   53118:	extr	x17, x14, x13, #1
   5311c:	str	x17, [x0]
   53120:	mov	x0, x10
   53124:	ret
   53128:	adds	x15, x5, x9
   5312c:	and	x10, x15, #0x1
   53130:	ldp	x4, x5, [x1], #32
   53134:	ldp	x8, x9, [x2], #32
   53138:	adcs	x12, x4, x8
   5313c:	adcs	x13, x5, x9
   53140:	cbz	x18, 53160 <__gmpn_rsh1add_n@@Base+0xa0>
   53144:	ldp	x4, x5, [x1, #-16]
   53148:	ldp	x8, x9, [x2, #-16]
   5314c:	extr	x17, x12, x15, #1
   53150:	adcs	x14, x4, x8
   53154:	adcs	x15, x5, x9
   53158:	str	x17, [x0], #8
   5315c:	b	531ec <__gmpn_rsh1add_n@@Base+0x12c>
   53160:	extr	x17, x12, x15, #1
   53164:	str	x17, [x0], #8
   53168:	b	5321c <__gmpn_rsh1add_n@@Base+0x15c>
   5316c:	tbz	w3, #1, 5319c <__gmpn_rsh1add_n@@Base+0xdc>
   53170:	ldp	x4, x5, [x1], #32
   53174:	ldp	x8, x9, [x2], #32
   53178:	adds	x12, x4, x8
   5317c:	adcs	x13, x5, x9
   53180:	and	x10, x12, #0x1
   53184:	cbz	x18, 5321c <__gmpn_rsh1add_n@@Base+0x15c>
   53188:	ldp	x4, x5, [x1, #-16]
   5318c:	ldp	x8, x9, [x2, #-16]
   53190:	adcs	x14, x4, x8
   53194:	adcs	x15, x5, x9
   53198:	b	531ec <__gmpn_rsh1add_n@@Base+0x12c>
   5319c:	ldp	x4, x5, [x1], #48
   531a0:	ldp	x8, x9, [x2], #48
   531a4:	adds	x14, x4, x8
   531a8:	adcs	x15, x5, x9
   531ac:	and	x10, x14, #0x1
   531b0:	ldp	x4, x5, [x1, #-32]
   531b4:	ldp	x8, x9, [x2, #-32]
   531b8:	adcs	x12, x4, x8
   531bc:	adcs	x13, x5, x9
   531c0:	add	x0, x0, #0x10
   531c4:	sub	x18, x18, #0x1
   531c8:	cbz	x18, 53210 <__gmpn_rsh1add_n@@Base+0x150>
   531cc:	nop
   531d0:	ldp	x4, x5, [x1, #-16]
   531d4:	ldp	x8, x9, [x2, #-16]
   531d8:	extr	x16, x15, x14, #1
   531dc:	extr	x17, x12, x15, #1
   531e0:	adcs	x14, x4, x8
   531e4:	adcs	x15, x5, x9
   531e8:	stp	x16, x17, [x0, #-16]
   531ec:	ldp	x4, x5, [x1], #32
   531f0:	ldp	x8, x9, [x2], #32
   531f4:	extr	x16, x13, x12, #1
   531f8:	extr	x17, x14, x13, #1
   531fc:	adcs	x12, x4, x8
   53200:	adcs	x13, x5, x9
   53204:	stp	x16, x17, [x0], #32
   53208:	sub	x18, x18, #0x1
   5320c:	cbnz	x18, 531d0 <__gmpn_rsh1add_n@@Base+0x110>
   53210:	extr	x16, x15, x14, #1
   53214:	extr	x17, x12, x15, #1
   53218:	stp	x16, x17, [x0, #-16]
   5321c:	cset	x14, cs  // cs = hs, nlast
   53220:	extr	x16, x13, x12, #1
   53224:	extr	x17, x14, x13, #1
   53228:	stp	x16, x17, [x0]
   5322c:	mov	x0, x10
   53230:	ret
   53234:	nop
   53238:	nop
   5323c:	nop

0000000000053240 <__gmpn_rsh1sub_n@@Base>:
   53240:	lsr	x18, x3, #2
   53244:	tbz	w3, #0, 532ec <__gmpn_rsh1sub_n@@Base+0xac>
   53248:	ldr	x5, [x1], #8
   5324c:	ldr	x9, [x2], #8
   53250:	tbnz	w3, #1, 532a8 <__gmpn_rsh1sub_n@@Base+0x68>
   53254:	subs	x13, x5, x9
   53258:	and	x10, x13, #0x1
   5325c:	cbz	x18, 53294 <__gmpn_rsh1sub_n@@Base+0x54>
   53260:	ldp	x4, x5, [x1], #48
   53264:	ldp	x8, x9, [x2], #48
   53268:	sbcs	x14, x4, x8
   5326c:	sbcs	x15, x5, x9
   53270:	ldp	x4, x5, [x1, #-32]
   53274:	ldp	x8, x9, [x2, #-32]
   53278:	extr	x17, x14, x13, #1
   5327c:	sbcs	x12, x4, x8
   53280:	sbcs	x13, x5, x9
   53284:	str	x17, [x0], #24
   53288:	sub	x18, x18, #0x1
   5328c:	cbz	x18, 53390 <__gmpn_rsh1sub_n@@Base+0x150>
   53290:	b	53350 <__gmpn_rsh1sub_n@@Base+0x110>
   53294:	cset	x14, cc  // cc = lo, ul, last
   53298:	extr	x17, x14, x13, #1
   5329c:	str	x17, [x0]
   532a0:	mov	x0, x10
   532a4:	ret
   532a8:	subs	x15, x5, x9
   532ac:	and	x10, x15, #0x1
   532b0:	ldp	x4, x5, [x1], #32
   532b4:	ldp	x8, x9, [x2], #32
   532b8:	sbcs	x12, x4, x8
   532bc:	sbcs	x13, x5, x9
   532c0:	cbz	x18, 532e0 <__gmpn_rsh1sub_n@@Base+0xa0>
   532c4:	ldp	x4, x5, [x1, #-16]
   532c8:	ldp	x8, x9, [x2, #-16]
   532cc:	extr	x17, x12, x15, #1
   532d0:	sbcs	x14, x4, x8
   532d4:	sbcs	x15, x5, x9
   532d8:	str	x17, [x0], #8
   532dc:	b	5336c <__gmpn_rsh1sub_n@@Base+0x12c>
   532e0:	extr	x17, x12, x15, #1
   532e4:	str	x17, [x0], #8
   532e8:	b	5339c <__gmpn_rsh1sub_n@@Base+0x15c>
   532ec:	tbz	w3, #1, 5331c <__gmpn_rsh1sub_n@@Base+0xdc>
   532f0:	ldp	x4, x5, [x1], #32
   532f4:	ldp	x8, x9, [x2], #32
   532f8:	subs	x12, x4, x8
   532fc:	sbcs	x13, x5, x9
   53300:	and	x10, x12, #0x1
   53304:	cbz	x18, 5339c <__gmpn_rsh1sub_n@@Base+0x15c>
   53308:	ldp	x4, x5, [x1, #-16]
   5330c:	ldp	x8, x9, [x2, #-16]
   53310:	sbcs	x14, x4, x8
   53314:	sbcs	x15, x5, x9
   53318:	b	5336c <__gmpn_rsh1sub_n@@Base+0x12c>
   5331c:	ldp	x4, x5, [x1], #48
   53320:	ldp	x8, x9, [x2], #48
   53324:	subs	x14, x4, x8
   53328:	sbcs	x15, x5, x9
   5332c:	and	x10, x14, #0x1
   53330:	ldp	x4, x5, [x1, #-32]
   53334:	ldp	x8, x9, [x2, #-32]
   53338:	sbcs	x12, x4, x8
   5333c:	sbcs	x13, x5, x9
   53340:	add	x0, x0, #0x10
   53344:	sub	x18, x18, #0x1
   53348:	cbz	x18, 53390 <__gmpn_rsh1sub_n@@Base+0x150>
   5334c:	nop
   53350:	ldp	x4, x5, [x1, #-16]
   53354:	ldp	x8, x9, [x2, #-16]
   53358:	extr	x16, x15, x14, #1
   5335c:	extr	x17, x12, x15, #1
   53360:	sbcs	x14, x4, x8
   53364:	sbcs	x15, x5, x9
   53368:	stp	x16, x17, [x0, #-16]
   5336c:	ldp	x4, x5, [x1], #32
   53370:	ldp	x8, x9, [x2], #32
   53374:	extr	x16, x13, x12, #1
   53378:	extr	x17, x14, x13, #1
   5337c:	sbcs	x12, x4, x8
   53380:	sbcs	x13, x5, x9
   53384:	stp	x16, x17, [x0], #32
   53388:	sub	x18, x18, #0x1
   5338c:	cbnz	x18, 53350 <__gmpn_rsh1sub_n@@Base+0x110>
   53390:	extr	x16, x15, x14, #1
   53394:	extr	x17, x12, x15, #1
   53398:	stp	x16, x17, [x0, #-16]
   5339c:	cset	x14, cc  // cc = lo, ul, last
   533a0:	extr	x16, x13, x12, #1
   533a4:	extr	x17, x14, x13, #1
   533a8:	stp	x16, x17, [x0]
   533ac:	mov	x0, x10
   533b0:	ret
   533b4:	nop
   533b8:	nop
   533bc:	nop

00000000000533c0 <__gmpn_addlsh2_n@@Base>:
   533c0:	lsr	x18, x3, #2
   533c4:	tbz	w3, #0, 5342c <__gmpn_addlsh2_n@@Base+0x6c>
   533c8:	ldr	x5, [x1]
   533cc:	tbnz	w3, #1, 5340c <__gmpn_addlsh2_n@@Base+0x4c>
   533d0:	ldr	x11, [x2]
   533d4:	cbz	x18, 533f4 <__gmpn_addlsh2_n@@Base+0x34>
   533d8:	ldp	x8, x9, [x2, #8]
   533dc:	lsl	x13, x11, #2
   533e0:	adds	x15, x13, x5
   533e4:	str	x15, [x0], #8
   533e8:	sub	x1, x1, #0x18
   533ec:	sub	x2, x2, #0x8
   533f0:	b	5346c <__gmpn_addlsh2_n@@Base+0xac>
   533f4:	lsl	x13, x11, #2
   533f8:	adds	x15, x13, x5
   533fc:	str	x15, [x0]
   53400:	lsr	x0, x11, #62
   53404:	adc	x0, x0, xzr
   53408:	ret
   5340c:	ldr	x9, [x2]
   53410:	ldp	x10, x11, [x2, #8]!
   53414:	lsl	x13, x9, #2
   53418:	adds	x17, x13, x5
   5341c:	str	x17, [x0], #8
   53420:	sub	x1, x1, #0x8
   53424:	cbz	x18, 53490 <__gmpn_addlsh2_n@@Base+0xd0>
   53428:	b	53450 <__gmpn_addlsh2_n@@Base+0x90>
   5342c:	tbnz	w3, #1, 53440 <__gmpn_addlsh2_n@@Base+0x80>
   53430:	adds	x11, xzr, xzr
   53434:	ldp	x8, x9, [x2], #-16
   53438:	sub	x1, x1, #0x20
   5343c:	b	5346c <__gmpn_addlsh2_n@@Base+0xac>
   53440:	adds	x9, xzr, xzr
   53444:	ldp	x10, x11, [x2]
   53448:	sub	x1, x1, #0x10
   5344c:	cbz	x18, 53490 <__gmpn_addlsh2_n@@Base+0xd0>
   53450:	ldp	x4, x5, [x1, #16]
   53454:	extr	x12, x10, x9, #62
   53458:	ldp	x8, x9, [x2, #16]
   5345c:	extr	x13, x11, x10, #62
   53460:	adcs	x14, x12, x4
   53464:	adcs	x15, x13, x5
   53468:	stp	x14, x15, [x0], #16
   5346c:	ldp	x4, x5, [x1, #32]!
   53470:	extr	x12, x8, x11, #62
   53474:	ldp	x10, x11, [x2, #32]!
   53478:	extr	x13, x9, x8, #62
   5347c:	adcs	x16, x12, x4
   53480:	adcs	x17, x13, x5
   53484:	stp	x16, x17, [x0], #16
   53488:	sub	x18, x18, #0x1
   5348c:	cbnz	x18, 53450 <__gmpn_addlsh2_n@@Base+0x90>
   53490:	ldp	x4, x5, [x1, #16]
   53494:	extr	x12, x10, x9, #62
   53498:	extr	x13, x11, x10, #62
   5349c:	adcs	x14, x12, x4
   534a0:	adcs	x15, x13, x5
   534a4:	stp	x14, x15, [x0]
   534a8:	lsr	x0, x11, #62
   534ac:	adc	x0, x0, xzr
   534b0:	ret
   534b4:	nop
   534b8:	nop
   534bc:	nop

00000000000534c0 <__gmpn_sublsh2_n@@Base>:
   534c0:	lsr	x18, x3, #2
   534c4:	tbz	w3, #0, 5352c <__gmpn_sublsh2_n@@Base+0x6c>
   534c8:	ldr	x5, [x1]
   534cc:	tbnz	w3, #1, 5350c <__gmpn_sublsh2_n@@Base+0x4c>
   534d0:	ldr	x11, [x2]
   534d4:	cbz	x18, 534f4 <__gmpn_sublsh2_n@@Base+0x34>
   534d8:	ldp	x8, x9, [x2, #8]
   534dc:	lsl	x13, x11, #2
   534e0:	subs	x15, x5, x13
   534e4:	str	x15, [x0], #8
   534e8:	sub	x1, x1, #0x18
   534ec:	sub	x2, x2, #0x8
   534f0:	b	5356c <__gmpn_sublsh2_n@@Base+0xac>
   534f4:	lsl	x13, x11, #2
   534f8:	subs	x15, x5, x13
   534fc:	str	x15, [x0]
   53500:	lsr	x0, x11, #62
   53504:	cinc	x0, x0, cc  // cc = lo, ul, last
   53508:	ret
   5350c:	ldr	x9, [x2]
   53510:	ldp	x10, x11, [x2, #8]!
   53514:	lsl	x13, x9, #2
   53518:	subs	x17, x5, x13
   5351c:	str	x17, [x0], #8
   53520:	sub	x1, x1, #0x8
   53524:	cbz	x18, 53590 <__gmpn_sublsh2_n@@Base+0xd0>
   53528:	b	53550 <__gmpn_sublsh2_n@@Base+0x90>
   5352c:	tbnz	w3, #1, 53540 <__gmpn_sublsh2_n@@Base+0x80>
   53530:	negs	x11, xzr
   53534:	ldp	x8, x9, [x2], #-16
   53538:	sub	x1, x1, #0x20
   5353c:	b	5356c <__gmpn_sublsh2_n@@Base+0xac>
   53540:	negs	x9, xzr
   53544:	ldp	x10, x11, [x2]
   53548:	sub	x1, x1, #0x10
   5354c:	cbz	x18, 53590 <__gmpn_sublsh2_n@@Base+0xd0>
   53550:	ldp	x4, x5, [x1, #16]
   53554:	extr	x12, x10, x9, #62
   53558:	ldp	x8, x9, [x2, #16]
   5355c:	extr	x13, x11, x10, #62
   53560:	sbcs	x14, x4, x12
   53564:	sbcs	x15, x5, x13
   53568:	stp	x14, x15, [x0], #16
   5356c:	ldp	x4, x5, [x1, #32]!
   53570:	extr	x12, x8, x11, #62
   53574:	ldp	x10, x11, [x2, #32]!
   53578:	extr	x13, x9, x8, #62
   5357c:	sbcs	x16, x4, x12
   53580:	sbcs	x17, x5, x13
   53584:	stp	x16, x17, [x0], #16
   53588:	sub	x18, x18, #0x1
   5358c:	cbnz	x18, 53550 <__gmpn_sublsh2_n@@Base+0x90>
   53590:	ldp	x4, x5, [x1, #16]
   53594:	extr	x12, x10, x9, #62
   53598:	extr	x13, x11, x10, #62
   5359c:	sbcs	x14, x4, x12
   535a0:	sbcs	x15, x5, x13
   535a4:	stp	x14, x15, [x0]
   535a8:	lsr	x0, x11, #62
   535ac:	cinc	x0, x0, cc  // cc = lo, ul, last
   535b0:	ret
   535b4:	nop
   535b8:	nop
   535bc:	nop

00000000000535c0 <__gmpn_rsblsh2_n@@Base>:
   535c0:	lsr	x18, x3, #2
   535c4:	tbz	w3, #0, 5362c <__gmpn_rsblsh2_n@@Base+0x6c>
   535c8:	ldr	x5, [x1]
   535cc:	tbnz	w3, #1, 5360c <__gmpn_rsblsh2_n@@Base+0x4c>
   535d0:	ldr	x11, [x2]
   535d4:	cbz	x18, 535f4 <__gmpn_rsblsh2_n@@Base+0x34>
   535d8:	ldp	x8, x9, [x2, #8]
   535dc:	lsl	x13, x11, #2
   535e0:	subs	x15, x13, x5
   535e4:	str	x15, [x0], #8
   535e8:	sub	x1, x1, #0x18
   535ec:	sub	x2, x2, #0x8
   535f0:	b	5366c <__gmpn_rsblsh2_n@@Base+0xac>
   535f4:	lsl	x13, x11, #2
   535f8:	subs	x15, x13, x5
   535fc:	str	x15, [x0]
   53600:	lsr	x0, x11, #62
   53604:	sbc	x0, x0, xzr
   53608:	ret
   5360c:	ldr	x9, [x2]
   53610:	ldp	x10, x11, [x2, #8]!
   53614:	lsl	x13, x9, #2
   53618:	subs	x17, x13, x5
   5361c:	str	x17, [x0], #8
   53620:	sub	x1, x1, #0x8
   53624:	cbz	x18, 53690 <__gmpn_rsblsh2_n@@Base+0xd0>
   53628:	b	53650 <__gmpn_rsblsh2_n@@Base+0x90>
   5362c:	tbnz	w3, #1, 53640 <__gmpn_rsblsh2_n@@Base+0x80>
   53630:	negs	x11, xzr
   53634:	ldp	x8, x9, [x2], #-16
   53638:	sub	x1, x1, #0x20
   5363c:	b	5366c <__gmpn_rsblsh2_n@@Base+0xac>
   53640:	negs	x9, xzr
   53644:	ldp	x10, x11, [x2]
   53648:	sub	x1, x1, #0x10
   5364c:	cbz	x18, 53690 <__gmpn_rsblsh2_n@@Base+0xd0>
   53650:	ldp	x4, x5, [x1, #16]
   53654:	extr	x12, x10, x9, #62
   53658:	ldp	x8, x9, [x2, #16]
   5365c:	extr	x13, x11, x10, #62
   53660:	sbcs	x14, x12, x4
   53664:	sbcs	x15, x13, x5
   53668:	stp	x14, x15, [x0], #16
   5366c:	ldp	x4, x5, [x1, #32]!
   53670:	extr	x12, x8, x11, #62
   53674:	ldp	x10, x11, [x2, #32]!
   53678:	extr	x13, x9, x8, #62
   5367c:	sbcs	x16, x12, x4
   53680:	sbcs	x17, x13, x5
   53684:	stp	x16, x17, [x0], #16
   53688:	sub	x18, x18, #0x1
   5368c:	cbnz	x18, 53650 <__gmpn_rsblsh2_n@@Base+0x90>
   53690:	ldp	x4, x5, [x1, #16]
   53694:	extr	x12, x10, x9, #62
   53698:	extr	x13, x11, x10, #62
   5369c:	sbcs	x14, x12, x4
   536a0:	sbcs	x15, x13, x5
   536a4:	stp	x14, x15, [x0]
   536a8:	lsr	x0, x11, #62
   536ac:	sbc	x0, x0, xzr
   536b0:	ret

00000000000536b4 <__gmpn_add_n_sub_n@@Base>:
   536b4:	stp	x29, x30, [sp, #-96]!
   536b8:	stp	x28, x27, [sp, #16]
   536bc:	stp	x26, x25, [sp, #32]
   536c0:	stp	x24, x23, [sp, #48]
   536c4:	stp	x22, x21, [sp, #64]
   536c8:	stp	x20, x19, [sp, #80]
   536cc:	mov	x29, sp
   536d0:	sub	sp, sp, #0x560
   536d4:	mov	x19, x4
   536d8:	mov	x20, x3
   536dc:	mov	x21, x2
   536e0:	mov	x22, x0
   536e4:	cmp	x0, x2
   536e8:	mov	x23, x1
   536ec:	b.eq	53778 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   536f0:	cmp	x22, x20
   536f4:	b.eq	53778 <__gmpn_add_n_sub_n@@Base+0xc4>  // b.none
   536f8:	cmp	x19, #0x1
   536fc:	b.lt	53898 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   53700:	mov	x27, xzr
   53704:	mov	x25, xzr
   53708:	mov	x24, xzr
   5370c:	mov	x8, x19
   53710:	subs	x28, x8, #0xaa
   53714:	mov	w9, #0xaa                  	// #170
   53718:	csel	x26, x8, x9, lt  // lt = tstop
   5371c:	mov	x0, x22
   53720:	mov	x1, x21
   53724:	mov	x2, x20
   53728:	mov	x3, x26
   5372c:	mov	x4, x24
   53730:	bl	ce90 <__gmpn_add_nc@plt>
   53734:	mov	x24, x0
   53738:	mov	x0, x23
   5373c:	mov	x1, x21
   53740:	mov	x2, x20
   53744:	mov	x3, x26
   53748:	mov	x4, x25
   5374c:	bl	c760 <__gmpn_sub_nc@plt>
   53750:	add	x27, x27, #0xaa
   53754:	mov	x25, x0
   53758:	add	x23, x23, #0x550
   5375c:	add	x20, x20, #0x550
   53760:	add	x21, x21, #0x550
   53764:	cmp	x27, x19
   53768:	add	x22, x22, #0x550
   5376c:	mov	x8, x28
   53770:	b.lt	53710 <__gmpn_add_n_sub_n@@Base+0x5c>  // b.tstop
   53774:	b	538a0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   53778:	cmp	x23, x21
   5377c:	b.eq	53808 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   53780:	cmp	x23, x20
   53784:	b.eq	53808 <__gmpn_add_n_sub_n@@Base+0x154>  // b.none
   53788:	cmp	x19, #0x1
   5378c:	b.lt	53898 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   53790:	mov	x27, xzr
   53794:	mov	x25, xzr
   53798:	mov	x24, xzr
   5379c:	mov	x8, x19
   537a0:	subs	x28, x8, #0xaa
   537a4:	mov	w9, #0xaa                  	// #170
   537a8:	csel	x26, x8, x9, lt  // lt = tstop
   537ac:	mov	x0, x23
   537b0:	mov	x1, x21
   537b4:	mov	x2, x20
   537b8:	mov	x3, x26
   537bc:	mov	x4, x25
   537c0:	bl	c760 <__gmpn_sub_nc@plt>
   537c4:	mov	x25, x0
   537c8:	mov	x0, x22
   537cc:	mov	x1, x21
   537d0:	mov	x2, x20
   537d4:	mov	x3, x26
   537d8:	mov	x4, x24
   537dc:	bl	ce90 <__gmpn_add_nc@plt>
   537e0:	add	x27, x27, #0xaa
   537e4:	mov	x24, x0
   537e8:	add	x22, x22, #0x550
   537ec:	add	x20, x20, #0x550
   537f0:	add	x21, x21, #0x550
   537f4:	cmp	x27, x19
   537f8:	add	x23, x23, #0x550
   537fc:	mov	x8, x28
   53800:	b.lt	537a0 <__gmpn_add_n_sub_n@@Base+0xec>  // b.tstop
   53804:	b	538a0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   53808:	cmp	x19, #0x1
   5380c:	b.lt	53898 <__gmpn_add_n_sub_n@@Base+0x1e4>  // b.tstop
   53810:	mov	x27, xzr
   53814:	mov	x25, xzr
   53818:	mov	x24, xzr
   5381c:	mov	x8, x19
   53820:	subs	x28, x8, #0xaa
   53824:	mov	w9, #0xaa                  	// #170
   53828:	csel	x26, x8, x9, lt  // lt = tstop
   5382c:	add	x0, sp, #0x8
   53830:	mov	x1, x21
   53834:	mov	x2, x20
   53838:	mov	x3, x26
   5383c:	mov	x4, x24
   53840:	bl	ce90 <__gmpn_add_nc@plt>
   53844:	mov	x24, x0
   53848:	mov	x0, x23
   5384c:	mov	x1, x21
   53850:	mov	x2, x20
   53854:	mov	x3, x26
   53858:	mov	x4, x25
   5385c:	bl	c760 <__gmpn_sub_nc@plt>
   53860:	mov	x25, x0
   53864:	add	x1, sp, #0x8
   53868:	mov	x0, x22
   5386c:	mov	x2, x26
   53870:	bl	ca50 <__gmpn_copyi@plt>
   53874:	add	x27, x27, #0xaa
   53878:	add	x22, x22, #0x550
   5387c:	add	x23, x23, #0x550
   53880:	add	x20, x20, #0x550
   53884:	cmp	x27, x19
   53888:	add	x21, x21, #0x550
   5388c:	mov	x8, x28
   53890:	b.lt	53820 <__gmpn_add_n_sub_n@@Base+0x16c>  // b.tstop
   53894:	b	538a0 <__gmpn_add_n_sub_n@@Base+0x1ec>
   53898:	mov	x24, xzr
   5389c:	mov	x25, xzr
   538a0:	add	x0, x25, x24, lsl #1
   538a4:	add	sp, sp, #0x560
   538a8:	ldp	x20, x19, [sp, #80]
   538ac:	ldp	x22, x21, [sp, #64]
   538b0:	ldp	x24, x23, [sp, #48]
   538b4:	ldp	x26, x25, [sp, #32]
   538b8:	ldp	x28, x27, [sp, #16]
   538bc:	ldp	x29, x30, [sp], #96
   538c0:	ret

00000000000538c4 <__gmp_asprintf@@Base>:
   538c4:	sub	sp, sp, #0x100
   538c8:	stp	x29, x30, [sp, #240]
   538cc:	add	x29, sp, #0xf0
   538d0:	mov	x8, #0xffffffffffffffd0    	// #-48
   538d4:	mov	x9, sp
   538d8:	sub	x10, x29, #0x70
   538dc:	movk	x8, #0xff80, lsl #32
   538e0:	add	x11, x29, #0x10
   538e4:	add	x9, x9, #0x80
   538e8:	add	x10, x10, #0x30
   538ec:	stp	x9, x8, [x29, #-16]
   538f0:	stp	x11, x10, [x29, #-32]
   538f4:	stp	x2, x3, [x29, #-112]
   538f8:	stp	x4, x5, [x29, #-96]
   538fc:	stp	x6, x7, [x29, #-80]
   53900:	stp	q1, q2, [sp, #16]
   53904:	str	q0, [sp]
   53908:	ldp	q0, q1, [x29, #-32]
   5390c:	sub	x2, x29, #0x40
   53910:	stp	q3, q4, [sp, #48]
   53914:	stp	q5, q6, [sp, #80]
   53918:	str	q7, [sp, #112]
   5391c:	stp	q0, q1, [x29, #-64]
   53920:	bl	c500 <__gmp_vasprintf@plt>
   53924:	ldp	x29, x30, [sp, #240]
   53928:	add	sp, sp, #0x100
   5392c:	ret

0000000000053930 <__gmp_asprintf_memory@@Base>:
   53930:	stp	x29, x30, [sp, #-48]!
   53934:	str	x21, [sp, #16]
   53938:	stp	x20, x19, [sp, #32]
   5393c:	ldp	x9, x8, [x0, #16]
   53940:	mov	x19, x0
   53944:	mov	x20, x2
   53948:	mov	x21, x1
   5394c:	add	x10, x9, x2
   53950:	cmp	x8, x10
   53954:	mov	x29, sp
   53958:	b.ls	53964 <__gmp_asprintf_memory@@Base+0x34>  // b.plast
   5395c:	ldr	x0, [x19, #8]
   53960:	b	5398c <__gmp_asprintf_memory@@Base+0x5c>
   53964:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   53968:	ldr	x9, [x9, #3792]
   5396c:	lsl	x2, x10, #1
   53970:	str	x2, [x19, #24]
   53974:	ldr	x0, [x19, #8]
   53978:	ldr	x9, [x9]
   5397c:	mov	x1, x8
   53980:	blr	x9
   53984:	ldr	x9, [x19, #16]
   53988:	str	x0, [x19, #8]
   5398c:	add	x0, x0, x9
   53990:	mov	x1, x21
   53994:	mov	x2, x20
   53998:	bl	bed0 <memcpy@plt>
   5399c:	ldr	x8, [x19, #16]
   539a0:	mov	w0, w20
   539a4:	ldr	x21, [sp, #16]
   539a8:	add	x8, x8, x20
   539ac:	str	x8, [x19, #16]
   539b0:	ldp	x20, x19, [sp, #32]
   539b4:	ldp	x29, x30, [sp], #48
   539b8:	ret

00000000000539bc <__gmp_asprintf_reps@@Base>:
   539bc:	stp	x29, x30, [sp, #-48]!
   539c0:	stp	x22, x21, [sp, #16]
   539c4:	stp	x20, x19, [sp, #32]
   539c8:	ldp	x9, x8, [x0, #16]
   539cc:	mov	w20, w2
   539d0:	sxtw	x21, w20
   539d4:	mov	x19, x0
   539d8:	add	x10, x9, x21
   539dc:	cmp	x8, x10
   539e0:	mov	w22, w1
   539e4:	mov	x29, sp
   539e8:	b.ls	539f4 <__gmp_asprintf_reps@@Base+0x38>  // b.plast
   539ec:	ldr	x0, [x19, #8]
   539f0:	b	53a1c <__gmp_asprintf_reps@@Base+0x60>
   539f4:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   539f8:	ldr	x9, [x9, #3792]
   539fc:	lsl	x2, x10, #1
   53a00:	str	x2, [x19, #24]
   53a04:	ldr	x0, [x19, #8]
   53a08:	ldr	x9, [x9]
   53a0c:	mov	x1, x8
   53a10:	blr	x9
   53a14:	ldr	x9, [x19, #16]
   53a18:	str	x0, [x19, #8]
   53a1c:	add	x0, x0, x9
   53a20:	mov	w1, w22
   53a24:	mov	x2, x21
   53a28:	bl	c5f0 <memset@plt>
   53a2c:	ldr	x8, [x19, #16]
   53a30:	mov	w0, w20
   53a34:	add	x8, x8, x21
   53a38:	str	x8, [x19, #16]
   53a3c:	ldp	x20, x19, [sp, #32]
   53a40:	ldp	x22, x21, [sp, #16]
   53a44:	ldp	x29, x30, [sp], #48
   53a48:	ret

0000000000053a4c <__gmp_asprintf_final@@Base>:
   53a4c:	stp	x29, x30, [sp, #-32]!
   53a50:	str	x19, [sp, #16]
   53a54:	mov	x19, x0
   53a58:	ldr	x0, [x0, #8]
   53a5c:	ldr	x8, [x19, #16]
   53a60:	mov	x29, sp
   53a64:	strb	wzr, [x0, x8]
   53a68:	ldp	x8, x1, [x19, #16]
   53a6c:	add	x2, x8, #0x1
   53a70:	cmp	x1, x2
   53a74:	b.eq	53a88 <__gmp_asprintf_final@@Base+0x3c>  // b.none
   53a78:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   53a7c:	ldr	x8, [x8, #3792]
   53a80:	ldr	x8, [x8]
   53a84:	blr	x8
   53a88:	ldr	x8, [x19]
   53a8c:	ldr	x19, [sp, #16]
   53a90:	str	x0, [x8]
   53a94:	mov	w0, wzr
   53a98:	ldp	x29, x30, [sp], #32
   53a9c:	ret

0000000000053aa0 <__gmp_doprnt@@Base>:
   53aa0:	sub	sp, sp, #0x190
   53aa4:	str	d8, [sp, #288]
   53aa8:	stp	x29, x30, [sp, #304]
   53aac:	stp	x28, x27, [sp, #320]
   53ab0:	stp	x26, x25, [sp, #336]
   53ab4:	stp	x24, x23, [sp, #352]
   53ab8:	stp	x22, x21, [sp, #368]
   53abc:	stp	x20, x19, [sp, #384]
   53ac0:	stp	x1, x0, [sp, #16]
   53ac4:	ldp	q1, q0, [x3]
   53ac8:	add	x29, sp, #0x120
   53acc:	mov	x0, x2
   53ad0:	mov	x23, x2
   53ad4:	stp	q1, q0, [x29, #-48]
   53ad8:	bl	bf60 <strlen@plt>
   53adc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   53ae0:	ldr	x8, [x8, #3840]
   53ae4:	add	x19, x0, #0x1
   53ae8:	mov	x0, x19
   53aec:	ldr	x8, [x8]
   53af0:	blr	x8
   53af4:	mov	x1, x23
   53af8:	mov	x2, x19
   53afc:	mov	x20, x0
   53b00:	str	x19, [sp, #8]
   53b04:	bl	bed0 <memcpy@plt>
   53b08:	ldp	q0, q1, [x29, #-48]
   53b0c:	mov	w1, #0x25                  	// #37
   53b10:	mov	x0, x20
   53b14:	stp	q0, q1, [x29, #-112]
   53b18:	bl	cda0 <strchr@plt>
   53b1c:	str	x20, [sp]
   53b20:	cbz	x0, 542c0 <__gmp_doprnt@@Base+0x820>
   53b24:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53b28:	adrp	x10, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53b2c:	ldr	d8, [x9, #2144]
   53b30:	ldr	q0, [x10, #2160]
   53b34:	add	x8, sp, #0x78
   53b38:	adrp	x22, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53b3c:	mov	x26, x0
   53b40:	add	x21, x8, #0x30
   53b44:	add	x8, x8, #0x1c
   53b48:	add	x22, x22, #0x880
   53b4c:	str	xzr, [sp, #32]
   53b50:	str	x8, [x29, #8]
   53b54:	str	q0, [sp, #48]
   53b58:	str	x20, [sp, #40]
   53b5c:	b	53b88 <__gmp_doprnt@@Base+0xe8>
   53b60:	ldur	w8, [x29, #-20]
   53b64:	tbnz	w8, #31, 53f54 <__gmp_doprnt@@Base+0x4b4>
   53b68:	ldur	x8, [x29, #-48]
   53b6c:	add	x8, x8, #0x8
   53b70:	stur	x8, [x29, #-48]
   53b74:	mov	w1, #0x25                  	// #37
   53b78:	mov	x0, x25
   53b7c:	bl	cda0 <strchr@plt>
   53b80:	mov	x26, x0
   53b84:	cbz	x0, 542c4 <__gmp_doprnt@@Base+0x824>
   53b88:	ldp	q0, q1, [x29, #-48]
   53b8c:	adrp	x8, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53b90:	add	x8, x8, #0x985
   53b94:	str	x8, [sp, #128]
   53b98:	stp	q0, q1, [x29, #-80]
   53b9c:	ldr	q0, [sp, #48]
   53ba0:	mov	w8, #0x20                  	// #32
   53ba4:	mov	w20, wzr
   53ba8:	mov	w23, wzr
   53bac:	add	x25, x26, #0x1
   53bb0:	strb	w8, [sp, #140]
   53bb4:	mov	w8, #0x1                   	// #1
   53bb8:	str	d8, [sp, #120]
   53bbc:	str	wzr, [sp, #136]
   53bc0:	stur	q0, [sp, #144]
   53bc4:	str	w8, [sp, #160]
   53bc8:	strb	wzr, [sp, #164]
   53bcc:	str	wzr, [sp, #168]
   53bd0:	mov	x27, x21
   53bd4:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53bd8:	mov	w23, w28
   53bdc:	mov	x8, x25
   53be0:	ldrb	w28, [x25], #1
   53be4:	sub	w9, w28, #0x20
   53be8:	cmp	w9, #0x5a
   53bec:	b.hi	53b74 <__gmp_doprnt@@Base+0xd4>  // b.pmore
   53bf0:	adr	x10, 53b74 <__gmp_doprnt@@Base+0xd4>
   53bf4:	ldrb	w11, [x22, x9]
   53bf8:	add	x10, x10, x11, lsl #2
   53bfc:	br	x10
   53c00:	mov	w19, wzr
   53c04:	mov	x24, x25
   53c08:	mov	w8, #0xa                   	// #10
   53c0c:	madd	w8, w19, w8, w28
   53c10:	ldrb	w28, [x24]
   53c14:	mov	x25, x24
   53c18:	sub	w19, w8, #0x30
   53c1c:	tbnz	w28, #7, 53c34 <__gmp_doprnt@@Base+0x194>
   53c20:	add	x24, x25, #0x1
   53c24:	bl	cae0 <__ctype_b_loc@plt>
   53c28:	ldr	x8, [x0]
   53c2c:	ldrh	w8, [x8, x28, lsl #1]
   53c30:	tbnz	w8, #11, 53c08 <__gmp_doprnt@@Base+0x168>
   53c34:	str	w19, [x27]
   53c38:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53c3c:	strb	w28, [sp, #164]
   53c40:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53c44:	mov	w8, #0x3                   	// #3
   53c48:	str	w8, [sp, #152]
   53c4c:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53c50:	ldursw	x8, [x29, #-24]
   53c54:	tbz	w8, #31, 53cd0 <__gmp_doprnt@@Base+0x230>
   53c58:	add	w9, w8, #0x8
   53c5c:	cmn	w8, #0x8
   53c60:	stur	w9, [x29, #-24]
   53c64:	b.gt	53cd0 <__gmp_doprnt@@Base+0x230>
   53c68:	ldur	x9, [x29, #-40]
   53c6c:	add	x8, x9, x8
   53c70:	b	53cdc <__gmp_doprnt@@Base+0x23c>
   53c74:	mov	w8, #0x1                   	// #1
   53c78:	str	w8, [sp, #144]
   53c7c:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53c80:	ldr	x27, [x29, #8]
   53c84:	mov	w8, #0xffffffff            	// #-1
   53c88:	str	w8, [sp, #148]
   53c8c:	mov	w20, #0x1                   	// #1
   53c90:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53c94:	cmp	x27, x21
   53c98:	b.eq	53cf4 <__gmp_doprnt@@Base+0x254>  // b.none
   53c9c:	str	wzr, [x27]
   53ca0:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53ca4:	mov	w23, #0x6c                  	// #108
   53ca8:	strb	w23, [x8]
   53cac:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53cb0:	cmp	w23, #0x68
   53cb4:	mov	w23, #0x48                  	// #72
   53cb8:	b.eq	53bdc <__gmp_doprnt@@Base+0x13c>  // b.none
   53cbc:	b	53bd8 <__gmp_doprnt@@Base+0x138>
   53cc0:	cmp	w23, #0x6c
   53cc4:	mov	w23, #0x4c                  	// #76
   53cc8:	b.eq	53bdc <__gmp_doprnt@@Base+0x13c>  // b.none
   53ccc:	b	53bd8 <__gmp_doprnt@@Base+0x138>
   53cd0:	ldur	x8, [x29, #-48]
   53cd4:	add	x9, x8, #0x8
   53cd8:	stur	x9, [x29, #-48]
   53cdc:	ldr	w8, [x8]
   53ce0:	cmp	x27, x21
   53ce4:	b.eq	53d18 <__gmp_doprnt@@Base+0x278>  // b.none
   53ce8:	bic	w8, w8, w8, asr #31
   53cec:	str	w8, [sp, #148]
   53cf0:	b	53bdc <__gmp_doprnt@@Base+0x13c>
   53cf4:	ldr	w8, [sp, #144]
   53cf8:	mov	w9, #0x30                  	// #48
   53cfc:	mov	x27, x21
   53d00:	strb	w9, [sp, #140]
   53d04:	cmp	w8, #0x2
   53d08:	b.ne	53bdc <__gmp_doprnt@@Base+0x13c>  // b.any
   53d0c:	mov	w8, #0x3                   	// #3
   53d10:	str	w8, [sp, #144]
   53d14:	b	53bd0 <__gmp_doprnt@@Base+0x130>
   53d18:	tbz	w8, #31, 53d28 <__gmp_doprnt@@Base+0x288>
   53d1c:	mov	w9, #0x1                   	// #1
   53d20:	neg	w8, w8
   53d24:	str	w9, [sp, #144]
   53d28:	str	w8, [sp, #168]
   53d2c:	b	53bd0 <__gmp_doprnt@@Base+0x130>
   53d30:	adrp	x8, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53d34:	add	x8, x8, #0x995
   53d38:	mov	w9, #0xfffffff0            	// #-16
   53d3c:	b	53d94 <__gmp_doprnt@@Base+0x2f4>
   53d40:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53d44:	mov	w8, #0xfffffff6            	// #-10
   53d48:	add	x9, x9, #0x99c
   53d4c:	str	w8, [sp, #120]
   53d50:	str	x9, [sp, #128]
   53d54:	mov	w8, #0x2                   	// #2
   53d58:	b	53dc0 <__gmp_doprnt@@Base+0x320>
   53d5c:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53d60:	mov	w8, #0xfffffff6            	// #-10
   53d64:	add	x9, x9, #0x99c
   53d68:	str	w8, [sp, #120]
   53d6c:	str	x9, [sp, #128]
   53d70:	mov	w8, #0x3                   	// #3
   53d74:	str	w8, [sp, #124]
   53d78:	str	wzr, [sp, #160]
   53d7c:	b	53dc4 <__gmp_doprnt@@Base+0x324>
   53d80:	mov	w8, #0xfffffff0            	// #-16
   53d84:	b	53f00 <__gmp_doprnt@@Base+0x460>
   53d88:	adrp	x8, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53d8c:	add	x8, x8, #0x98e
   53d90:	mov	w9, #0x10                  	// #16
   53d94:	str	x8, [sp, #128]
   53d98:	mov	w10, #0x2                   	// #2
   53d9c:	mov	w8, #0x1                   	// #1
   53da0:	stp	w9, w10, [sp, #120]
   53da4:	str	w8, [sp, #136]
   53da8:	cbnz	w20, 53db4 <__gmp_doprnt@@Base+0x314>
   53dac:	mov	w9, #0xffffffff            	// #-1
   53db0:	str	w9, [sp, #148]
   53db4:	str	w8, [sp, #152]
   53db8:	b	53dd8 <__gmp_doprnt@@Base+0x338>
   53dbc:	mov	w8, #0x1                   	// #1
   53dc0:	str	w8, [sp, #124]
   53dc4:	ldr	w8, [sp, #152]
   53dc8:	cmp	w8, #0x3
   53dcc:	b.ne	53de0 <__gmp_doprnt@@Base+0x340>  // b.any
   53dd0:	mov	w8, #0x1                   	// #1
   53dd4:	str	w8, [sp, #156]
   53dd8:	mov	w8, #0x1                   	// #1
   53ddc:	str	w8, [sp, #160]
   53de0:	and	w8, w23, #0xff
   53de4:	cmp	w8, #0x4c
   53de8:	b.eq	53e58 <__gmp_doprnt@@Base+0x3b8>  // b.none
   53dec:	cmp	w8, #0x46
   53df0:	b.ne	53b60 <__gmp_doprnt@@Base+0xc0>  // b.any
   53df4:	ldp	x19, x1, [sp, #32]
   53df8:	cmp	x26, x1
   53dfc:	b.eq	53e28 <__gmp_doprnt@@Base+0x388>  // b.none
   53e00:	strb	wzr, [x26]
   53e04:	ldp	x0, x8, [sp, #16]
   53e08:	ldp	q0, q1, [x29, #-112]
   53e0c:	add	x2, sp, #0x40
   53e10:	ldr	x8, [x8]
   53e14:	stp	q0, q1, [sp, #64]
   53e18:	blr	x8
   53e1c:	cmn	w0, #0x1
   53e20:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   53e24:	add	w19, w0, w19
   53e28:	mov	w0, #0x10000               	// #65536
   53e2c:	bl	c400 <nl_langinfo@plt>
   53e30:	ldursw	x8, [x29, #-24]
   53e34:	mov	x3, x0
   53e38:	tbz	w8, #31, 53e84 <__gmp_doprnt@@Base+0x3e4>
   53e3c:	add	w9, w8, #0x8
   53e40:	cmn	w8, #0x8
   53e44:	stur	w9, [x29, #-24]
   53e48:	b.gt	53e84 <__gmp_doprnt@@Base+0x3e4>
   53e4c:	ldur	x9, [x29, #-40]
   53e50:	add	x8, x9, x8
   53e54:	b	53e90 <__gmp_doprnt@@Base+0x3f0>
   53e58:	ldur	w8, [x29, #-20]
   53e5c:	tbz	w8, #31, 53e70 <__gmp_doprnt@@Base+0x3d0>
   53e60:	add	w9, w8, #0x10
   53e64:	cmn	w8, #0xf
   53e68:	stur	w9, [x29, #-20]
   53e6c:	b.lt	53b74 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   53e70:	ldur	x8, [x29, #-48]
   53e74:	add	x8, x8, #0xf
   53e78:	and	x8, x8, #0xfffffffffffffff0
   53e7c:	add	x8, x8, #0x10
   53e80:	b	53b70 <__gmp_doprnt@@Base+0xd0>
   53e84:	ldur	x8, [x29, #-48]
   53e88:	add	x9, x8, #0x8
   53e8c:	stur	x9, [x29, #-48]
   53e90:	ldr	x4, [x8]
   53e94:	ldp	x1, x0, [sp, #16]
   53e98:	add	x2, sp, #0x78
   53e9c:	bl	d230 <__gmp_doprnt_mpf2@plt>
   53ea0:	cmn	w0, #0x1
   53ea4:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   53ea8:	ldp	q0, q1, [x29, #-48]
   53eac:	add	w19, w0, w19
   53eb0:	b	5420c <__gmp_doprnt@@Base+0x76c>
   53eb4:	ldr	x1, [sp, #40]
   53eb8:	cmp	x26, x1
   53ebc:	b.eq	54198 <__gmp_doprnt@@Base+0x6f8>  // b.none
   53ec0:	strb	wzr, [x26]
   53ec4:	ldp	x0, x8, [sp, #16]
   53ec8:	ldp	q0, q1, [x29, #-112]
   53ecc:	add	x2, sp, #0x40
   53ed0:	ldr	x8, [x8]
   53ed4:	stp	q0, q1, [sp, #64]
   53ed8:	blr	x8
   53edc:	ldr	x19, [sp, #32]
   53ee0:	cmn	w0, #0x1
   53ee4:	csel	w8, wzr, w0, eq  // eq = none
   53ee8:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   53eec:	add	w19, w8, w19
   53ef0:	b	5419c <__gmp_doprnt@@Base+0x6fc>
   53ef4:	mov	w8, #0x8                   	// #8
   53ef8:	b	53f00 <__gmp_doprnt@@Base+0x460>
   53efc:	mov	w8, #0x10                  	// #16
   53f00:	str	w8, [sp, #120]
   53f04:	cbnz	w20, 53f10 <__gmp_doprnt@@Base+0x470>
   53f08:	mov	w8, #0xffffffff            	// #-1
   53f0c:	str	w8, [sp, #148]
   53f10:	and	w8, w23, #0xff
   53f14:	sub	w8, w8, #0x4c
   53f18:	cmp	w8, #0x2e
   53f1c:	b.hi	53f38 <__gmp_doprnt@@Base+0x498>  // b.pmore
   53f20:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   53f24:	add	x9, x9, #0x956
   53f28:	adr	x10, 53f38 <__gmp_doprnt@@Base+0x498>
   53f2c:	ldrb	w11, [x9, x8]
   53f30:	add	x10, x10, x11, lsl #2
   53f34:	br	x10
   53f38:	ldur	w8, [x29, #-24]
   53f3c:	tbz	w8, #31, 53b68 <__gmp_doprnt@@Base+0xc8>
   53f40:	add	w9, w8, #0x8
   53f44:	cmn	w8, #0x7
   53f48:	stur	w9, [x29, #-24]
   53f4c:	b.lt	53b74 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   53f50:	b	53b68 <__gmp_doprnt@@Base+0xc8>
   53f54:	add	w9, w8, #0x10
   53f58:	cmn	w8, #0xf
   53f5c:	stur	w9, [x29, #-20]
   53f60:	b.lt	53b74 <__gmp_doprnt@@Base+0xd4>  // b.tstop
   53f64:	b	53b68 <__gmp_doprnt@@Base+0xc8>
   53f68:	ldp	x23, x1, [sp, #32]
   53f6c:	cmp	x26, x1
   53f70:	b.eq	53fa0 <__gmp_doprnt@@Base+0x500>  // b.none
   53f74:	strb	wzr, [x26]
   53f78:	ldp	x0, x8, [sp, #16]
   53f7c:	ldp	q0, q1, [x29, #-112]
   53f80:	add	x2, sp, #0x40
   53f84:	ldr	x8, [x8]
   53f88:	stp	q0, q1, [sp, #64]
   53f8c:	blr	x8
   53f90:	cmn	w0, #0x1
   53f94:	csel	w8, wzr, w0, eq  // eq = none
   53f98:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   53f9c:	add	w23, w8, w23
   53fa0:	ldur	w9, [x29, #-24]
   53fa4:	mov	w8, w9
   53fa8:	tbz	w9, #31, 54084 <__gmp_doprnt@@Base+0x5e4>
   53fac:	add	w8, w9, #0x8
   53fb0:	cmn	w9, #0x8
   53fb4:	stur	w8, [x29, #-24]
   53fb8:	b.gt	54084 <__gmp_doprnt@@Base+0x5e4>
   53fbc:	ldur	x10, [x29, #-40]
   53fc0:	sxtw	x9, w9
   53fc4:	add	x9, x10, x9
   53fc8:	b	54090 <__gmp_doprnt@@Base+0x5f0>
   53fcc:	ldp	x23, x1, [sp, #32]
   53fd0:	cmp	x26, x1
   53fd4:	b.eq	54000 <__gmp_doprnt@@Base+0x560>  // b.none
   53fd8:	strb	wzr, [x26]
   53fdc:	ldp	x0, x8, [sp, #16]
   53fe0:	ldp	q0, q1, [x29, #-112]
   53fe4:	add	x2, sp, #0x40
   53fe8:	ldr	x8, [x8]
   53fec:	stp	q0, q1, [sp, #64]
   53ff0:	blr	x8
   53ff4:	cmn	w0, #0x1
   53ff8:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   53ffc:	add	w23, w0, w23
   54000:	ldursw	x8, [x29, #-24]
   54004:	ldr	w1, [sp, #120]
   54008:	tbz	w8, #31, 540b8 <__gmp_doprnt@@Base+0x618>
   5400c:	add	w9, w8, #0x8
   54010:	cmn	w8, #0x8
   54014:	stur	w9, [x29, #-24]
   54018:	b.gt	540b8 <__gmp_doprnt@@Base+0x618>
   5401c:	ldur	x9, [x29, #-40]
   54020:	add	x8, x9, x8
   54024:	b	540c4 <__gmp_doprnt@@Base+0x624>
   54028:	ldp	x23, x1, [sp, #32]
   5402c:	cmp	x26, x1
   54030:	b.eq	5405c <__gmp_doprnt@@Base+0x5bc>  // b.none
   54034:	strb	wzr, [x26]
   54038:	ldp	x0, x8, [sp, #16]
   5403c:	ldp	q0, q1, [x29, #-112]
   54040:	add	x2, sp, #0x40
   54044:	ldr	x8, [x8]
   54048:	stp	q0, q1, [sp, #64]
   5404c:	blr	x8
   54050:	cmn	w0, #0x1
   54054:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   54058:	add	w23, w0, w23
   5405c:	ldursw	x8, [x29, #-24]
   54060:	ldr	w1, [sp, #120]
   54064:	tbz	w8, #31, 540d4 <__gmp_doprnt@@Base+0x634>
   54068:	add	w9, w8, #0x8
   5406c:	cmn	w8, #0x8
   54070:	stur	w9, [x29, #-24]
   54074:	b.gt	540d4 <__gmp_doprnt@@Base+0x634>
   54078:	ldur	x9, [x29, #-40]
   5407c:	add	x8, x9, x8
   54080:	b	540e0 <__gmp_doprnt@@Base+0x640>
   54084:	ldur	x9, [x29, #-48]
   54088:	add	x10, x9, #0x8
   5408c:	stur	x10, [x29, #-48]
   54090:	ldr	x9, [x9]
   54094:	str	x9, [sp, #112]
   54098:	tbz	w8, #31, 540e8 <__gmp_doprnt@@Base+0x648>
   5409c:	add	w10, w8, #0x8
   540a0:	cmn	w8, #0x8
   540a4:	stur	w10, [x29, #-24]
   540a8:	b.gt	540e8 <__gmp_doprnt@@Base+0x648>
   540ac:	ldur	x10, [x29, #-40]
   540b0:	add	x8, x10, w8, sxtw
   540b4:	b	540f4 <__gmp_doprnt@@Base+0x654>
   540b8:	ldur	x8, [x29, #-48]
   540bc:	add	x9, x8, #0x8
   540c0:	stur	x9, [x29, #-48]
   540c4:	ldr	x2, [x8]
   540c8:	mov	x0, xzr
   540cc:	bl	c130 <__gmpq_get_str@plt>
   540d0:	b	54148 <__gmp_doprnt@@Base+0x6a8>
   540d4:	ldur	x8, [x29, #-48]
   540d8:	add	x9, x8, #0x8
   540dc:	stur	x9, [x29, #-48]
   540e0:	ldr	x2, [x8]
   540e4:	b	54140 <__gmp_doprnt@@Base+0x6a0>
   540e8:	ldur	x8, [x29, #-48]
   540ec:	add	x10, x8, #0x8
   540f0:	stur	x10, [x29, #-48]
   540f4:	ldr	x10, [x8]
   540f8:	mov	x11, #0xffffffff00000000    	// #-4294967296
   540fc:	lsl	x8, x10, #32
   54100:	sxtw	x10, w10
   54104:	cmp	x8, x11
   54108:	cneg	x10, x10, le
   5410c:	sub	x11, x9, #0x8
   54110:	mov	x9, x10
   54114:	subs	x10, x10, #0x1
   54118:	b.lt	54124 <__gmp_doprnt@@Base+0x684>  // b.tstop
   5411c:	ldr	x12, [x11, x9, lsl #3]
   54120:	cbz	x12, 54110 <__gmp_doprnt@@Base+0x670>
   54124:	mov	x11, #0xffffffff00000000    	// #-4294967296
   54128:	ldr	w1, [sp, #120]
   5412c:	neg	w10, w9
   54130:	cmp	x8, x11
   54134:	csel	x8, x9, x10, gt
   54138:	str	w8, [sp, #108]
   5413c:	add	x2, sp, #0x68
   54140:	mov	x0, xzr
   54144:	bl	c3a0 <__gmpz_get_str@plt>
   54148:	mov	x19, x0
   5414c:	ldp	x1, x0, [sp, #16]
   54150:	add	x2, sp, #0x78
   54154:	mov	x3, x19
   54158:	bl	cd10 <__gmp_doprnt_integer@plt>
   5415c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54160:	ldr	x8, [x8, #4016]
   54164:	mov	w26, w0
   54168:	mov	x0, x19
   5416c:	ldr	x20, [x8]
   54170:	bl	bf60 <strlen@plt>
   54174:	add	x1, x0, #0x1
   54178:	mov	x0, x19
   5417c:	blr	x20
   54180:	cmn	w26, #0x1
   54184:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   54188:	ldp	q0, q1, [x29, #-48]
   5418c:	add	w23, w26, w23
   54190:	str	x23, [sp, #32]
   54194:	b	54210 <__gmp_doprnt@@Base+0x770>
   54198:	ldr	x19, [sp, #32]
   5419c:	ldur	w9, [x29, #-24]
   541a0:	mov	w8, w9
   541a4:	tbz	w9, #31, 541c8 <__gmp_doprnt@@Base+0x728>
   541a8:	add	w8, w9, #0x8
   541ac:	cmn	w9, #0x8
   541b0:	stur	w8, [x29, #-24]
   541b4:	b.gt	541c8 <__gmp_doprnt@@Base+0x728>
   541b8:	ldur	x10, [x29, #-40]
   541bc:	sxtw	x9, w9
   541c0:	add	x9, x10, x9
   541c4:	b	541d4 <__gmp_doprnt@@Base+0x734>
   541c8:	ldur	x9, [x29, #-48]
   541cc:	add	x10, x9, #0x8
   541d0:	stur	x10, [x29, #-48]
   541d4:	and	w10, w23, #0xff
   541d8:	cmp	w10, #0x7a
   541dc:	b.hi	54208 <__gmp_doprnt@@Base+0x768>  // b.pmore
   541e0:	ldr	x0, [x9]
   541e4:	adrp	x10, 63000 <__gmp_jacobi_table@@Base+0x7599>
   541e8:	and	x9, x23, #0xff
   541ec:	add	x10, x10, #0x8db
   541f0:	adr	x11, 54200 <__gmp_doprnt@@Base+0x760>
   541f4:	ldrb	w12, [x10, x9]
   541f8:	add	x11, x11, x12, lsl #2
   541fc:	br	x11
   54200:	sxtw	x8, w19
   54204:	str	x8, [x0]
   54208:	ldp	q0, q1, [x29, #-48]
   5420c:	str	x19, [sp, #32]
   54210:	stp	q0, q1, [x29, #-112]
   54214:	str	x25, [sp, #40]
   54218:	b	53b74 <__gmp_doprnt@@Base+0xd4>
   5421c:	str	w19, [x0]
   54220:	b	54208 <__gmp_doprnt@@Base+0x768>
   54224:	sxtw	x1, w19
   54228:	bl	c620 <__gmpf_set_si@plt>
   5422c:	b	54208 <__gmp_doprnt@@Base+0x768>
   54230:	strb	w19, [x0]
   54234:	b	54208 <__gmp_doprnt@@Base+0x768>
   54238:	tbz	w8, #31, 5427c <__gmp_doprnt@@Base+0x7dc>
   5423c:	add	w9, w8, #0x8
   54240:	cmn	w8, #0x8
   54244:	stur	w9, [x29, #-24]
   54248:	b.gt	5427c <__gmp_doprnt@@Base+0x7dc>
   5424c:	ldur	x9, [x29, #-40]
   54250:	add	x8, x9, w8, sxtw
   54254:	b	54288 <__gmp_doprnt@@Base+0x7e8>
   54258:	sxtw	x1, w19
   5425c:	mov	w2, #0x1                   	// #1
   54260:	bl	cb70 <__gmpq_set_si@plt>
   54264:	b	54208 <__gmp_doprnt@@Base+0x768>
   54268:	sxtw	x1, w19
   5426c:	bl	d270 <__gmpz_set_si@plt>
   54270:	b	54208 <__gmp_doprnt@@Base+0x768>
   54274:	strh	w19, [x0]
   54278:	b	54208 <__gmp_doprnt@@Base+0x768>
   5427c:	ldur	x8, [x29, #-48]
   54280:	add	x9, x8, #0x8
   54284:	stur	x9, [x29, #-48]
   54288:	ldr	x8, [x8]
   5428c:	cbz	x8, 54208 <__gmp_doprnt@@Base+0x768>
   54290:	cmp	x8, #0x0
   54294:	cneg	x8, x8, mi  // mi = first
   54298:	sxtw	x9, w19
   5429c:	cmp	x8, #0x1
   542a0:	str	x9, [x0]
   542a4:	b.eq	54208 <__gmp_doprnt@@Base+0x768>  // b.none
   542a8:	lsl	x8, x8, #3
   542ac:	add	x0, x0, #0x8
   542b0:	sub	x2, x8, #0x8
   542b4:	mov	w1, wzr
   542b8:	bl	c5f0 <memset@plt>
   542bc:	b	54208 <__gmp_doprnt@@Base+0x768>
   542c0:	stp	xzr, x20, [sp, #32]
   542c4:	ldr	x1, [sp, #40]
   542c8:	ldrb	w8, [x1]
   542cc:	cbz	w8, 542fc <__gmp_doprnt@@Base+0x85c>
   542d0:	ldp	x0, x8, [sp, #16]
   542d4:	ldp	q0, q1, [x29, #-112]
   542d8:	add	x2, sp, #0x40
   542dc:	ldr	x8, [x8]
   542e0:	stp	q0, q1, [sp, #64]
   542e4:	blr	x8
   542e8:	ldr	x19, [sp, #32]
   542ec:	cmn	w0, #0x1
   542f0:	b.eq	5431c <__gmp_doprnt@@Base+0x87c>  // b.none
   542f4:	add	w19, w0, w19
   542f8:	b	54300 <__gmp_doprnt@@Base+0x860>
   542fc:	ldr	x19, [sp, #32]
   54300:	ldr	x8, [sp, #24]
   54304:	ldr	x8, [x8, #24]
   54308:	cbz	x8, 54320 <__gmp_doprnt@@Base+0x880>
   5430c:	ldr	x0, [sp, #16]
   54310:	blr	x8
   54314:	cmn	w0, #0x1
   54318:	b.ne	54320 <__gmp_doprnt@@Base+0x880>  // b.any
   5431c:	mov	w19, #0xffffffff            	// #-1
   54320:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54324:	ldr	x8, [x8, #4016]
   54328:	ldp	x0, x1, [sp]
   5432c:	ldr	x8, [x8]
   54330:	blr	x8
   54334:	mov	w0, w19
   54338:	ldp	x20, x19, [sp, #384]
   5433c:	ldp	x22, x21, [sp, #368]
   54340:	ldp	x24, x23, [sp, #352]
   54344:	ldp	x26, x25, [sp, #336]
   54348:	ldp	x28, x27, [sp, #320]
   5434c:	ldp	x29, x30, [sp, #304]
   54350:	ldr	d8, [sp, #288]
   54354:	add	sp, sp, #0x190
   54358:	ret

000000000005435c <__gmp_doprnt_mpf2@@Base>:
   5435c:	sub	sp, sp, #0x110
   54360:	stp	x29, x30, [sp, #176]
   54364:	stp	x28, x27, [sp, #192]
   54368:	stp	x26, x25, [sp, #208]
   5436c:	stp	x24, x23, [sp, #224]
   54370:	stp	x22, x21, [sp, #240]
   54374:	stp	x20, x19, [sp, #256]
   54378:	stp	x0, x1, [sp, #64]
   5437c:	ldr	w19, [x2, #28]
   54380:	ldr	w8, [x2, #4]
   54384:	mov	x24, x3
   54388:	mov	x20, x2
   5438c:	add	x29, sp, #0xb0
   54390:	tbnz	w19, #31, 543e8 <__gmp_doprnt_mpf2@@Base+0x8c>
   54394:	cmp	w8, #0x2
   54398:	b.eq	5442c <__gmp_doprnt_mpf2@@Base+0xd0>  // b.none
   5439c:	cmp	w8, #0x1
   543a0:	b.ne	54434 <__gmp_doprnt_mpf2@@Base+0xd8>  // b.any
   543a4:	ldr	w8, [x20]
   543a8:	mov	w10, #0x28                  	// #40
   543ac:	ldr	x9, [x4, #8]
   543b0:	cmp	w8, #0x0
   543b4:	cneg	w8, w8, mi  // mi = first
   543b8:	umull	x8, w8, w10
   543bc:	adrp	x10, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   543c0:	ldr	x10, [x10, #3936]
   543c4:	ldr	w8, [x10, x8]
   543c8:	lsr	x10, x9, #63
   543cc:	eor	w10, w10, #0x1
   543d0:	add	w8, w10, w8
   543d4:	madd	w8, w8, w9, w19
   543d8:	add	w8, w8, #0x3
   543dc:	cmp	w8, #0x1
   543e0:	csinc	w8, w8, wzr, gt
   543e4:	b	54444 <__gmp_doprnt_mpf2@@Base+0xe8>
   543e8:	cmp	w8, #0x3
   543ec:	b.ne	54440 <__gmp_doprnt_mpf2@@Base+0xe4>  // b.any
   543f0:	ldr	w11, [x20]
   543f4:	adrp	x12, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   543f8:	ldrsw	x9, [x4]
   543fc:	ldr	x12, [x12, #3936]
   54400:	mov	w10, #0x28                  	// #40
   54404:	cmp	w11, #0x0
   54408:	mov	w8, wzr
   5440c:	madd	x9, x9, x10, x12
   54410:	cneg	w10, w11, mi  // mi = first
   54414:	ldr	x9, [x9, #8]
   54418:	sub	w10, w10, #0x1
   5441c:	sbfiz	x10, x10, #6, #32
   54420:	umulh	x9, x9, x10
   54424:	add	w19, w9, #0x2
   54428:	b	54444 <__gmp_doprnt_mpf2@@Base+0xe8>
   5442c:	add	w8, w19, #0x1
   54430:	b	54444 <__gmp_doprnt_mpf2@@Base+0xe8>
   54434:	cmp	w19, #0x1
   54438:	csinc	w8, w19, wzr, gt
   5443c:	b	54444 <__gmp_doprnt_mpf2@@Base+0xe8>
   54440:	mov	w8, wzr
   54444:	ldr	w2, [x20]
   54448:	mov	w3, w8
   5444c:	sub	x1, x29, #0x10
   54450:	mov	x0, xzr
   54454:	bl	ce20 <__gmpf_get_str@plt>
   54458:	mov	x21, x0
   5445c:	bl	bf60 <strlen@plt>
   54460:	mov	x10, x21
   54464:	ldrb	w9, [x20, #44]
   54468:	ldrb	w11, [x10], #1
   5446c:	ldr	w8, [x20, #4]
   54470:	stp	x0, x21, [sp, #48]
   54474:	cmp	w11, #0x2d
   54478:	cset	w23, eq  // eq = none
   5447c:	csel	x10, x10, x21, eq  // eq = none
   54480:	csel	w21, w11, w9, eq  // eq = none
   54484:	cmp	w8, #0x2
   54488:	sub	w28, w0, w23
   5448c:	str	x10, [sp, #40]
   54490:	b.eq	544b0 <__gmp_doprnt_mpf2@@Base+0x154>  // b.none
   54494:	cmp	w8, #0x1
   54498:	b.ne	544c4 <__gmp_doprnt_mpf2@@Base+0x168>  // b.any
   5449c:	tbnz	w19, #31, 54544 <__gmp_doprnt_mpf2@@Base+0x1e8>
   544a0:	ldur	x22, [x29, #-16]
   544a4:	adds	w26, w19, w22
   544a8:	b.pl	54560 <__gmp_doprnt_mpf2@@Base+0x204>  // b.nfrst
   544ac:	b	546e0 <__gmp_doprnt_mpf2@@Base+0x384>
   544b0:	tbz	w19, #31, 544e0 <__gmp_doprnt_mpf2@@Base+0x184>
   544b4:	sub	w8, w28, #0x1
   544b8:	cmp	w28, #0x0
   544bc:	csel	w19, w8, wzr, gt
   544c0:	b	544e0 <__gmp_doprnt_mpf2@@Base+0x184>
   544c4:	ldur	x22, [x29, #-16]
   544c8:	cmn	x22, #0x3
   544cc:	b.lt	544e0 <__gmp_doprnt_mpf2@@Base+0x184>  // b.tstop
   544d0:	cmp	w19, #0x1
   544d4:	csinc	w8, w19, wzr, gt
   544d8:	cmp	x22, x8
   544dc:	b.le	5467c <__gmp_doprnt_mpf2@@Base+0x320>
   544e0:	ldur	x8, [x29, #-16]
   544e4:	ldr	w9, [x20, #16]
   544e8:	cmp	w28, #0x1
   544ec:	csinc	w27, w28, wzr, lt  // lt = tstop
   544f0:	cmp	w27, #0x0
   544f4:	sub	x8, x8, w27, sxtw
   544f8:	ldr	x2, [x20, #8]
   544fc:	cset	w25, eq  // eq = none
   54500:	cmp	w9, #0x0
   54504:	lsl	x9, x8, #2
   54508:	csel	x8, x8, x9, eq  // eq = none
   5450c:	mov	w10, #0x2d                  	// #45
   54510:	mov	w9, #0x2b                  	// #43
   54514:	cmp	x8, #0x0
   54518:	cneg	x4, x8, mi  // mi = first
   5451c:	csel	w3, w9, w10, ge  // ge = tcont
   54520:	add	x0, sp, #0x54
   54524:	mov	w1, #0x4a                  	// #74
   54528:	sub	w28, w28, w27
   5452c:	bl	c2f0 <snprintf@plt>
   54530:	mov	w23, w0
   54534:	mov	w22, wzr
   54538:	ldr	w8, [x20, #40]
   5453c:	cbnz	w8, 54704 <__gmp_doprnt_mpf2@@Base+0x3a8>
   54540:	b	546a8 <__gmp_doprnt_mpf2@@Base+0x34c>
   54544:	ldur	x22, [x29, #-16]
   54548:	sxtw	x8, w28
   5454c:	sub	x8, x8, x22
   54550:	cmp	x8, #0x0
   54554:	csel	w19, w8, wzr, gt
   54558:	adds	w26, w19, w22
   5455c:	b.mi	546e0 <__gmp_doprnt_mpf2@@Base+0x384>  // b.first
   54560:	cmp	w28, w26
   54564:	b.le	5467c <__gmp_doprnt_mpf2@@Base+0x320>
   54568:	ldr	w8, [x20]
   5456c:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54570:	adrp	x10, 58000 <__gmp_binvert_limb_table@@Base+0x38>
   54574:	add	x9, x9, #0x9a5
   54578:	add	x10, x10, #0xbd
   5457c:	cmp	w8, #0x0
   54580:	mov	x27, x24
   54584:	csel	x24, x10, x9, ge  // ge = tcont
   54588:	cneg	w25, w8, mi  // mi = first
   5458c:	bl	cae0 <__ctype_b_loc@plt>
   54590:	ldr	x8, [sp, #40]
   54594:	ldr	x9, [x0]
   54598:	ldrb	w8, [x8, w26, uxtw]
   5459c:	ldrh	w10, [x9, x8, lsl #1]
   545a0:	tbnz	w10, #11, 545bc <__gmp_doprnt_mpf2@@Base+0x260>
   545a4:	tst	w10, #0x200
   545a8:	mov	w10, #0xffffffa9            	// #-87
   545ac:	mov	w11, #0xffffffc9            	// #-55
   545b0:	csel	w10, w11, w10, eq  // eq = none
   545b4:	add	w8, w10, w8
   545b8:	b	545c0 <__gmp_doprnt_mpf2@@Base+0x264>
   545bc:	sub	w8, w8, #0x30
   545c0:	add	w10, w25, #0x1
   545c4:	cmp	w8, w10, lsr #1
   545c8:	mov	w8, w26
   545cc:	b.ge	54604 <__gmp_doprnt_mpf2@@Base+0x2a8>  // b.tcont
   545d0:	ldr	x9, [sp, #40]
   545d4:	mov	x24, x27
   545d8:	sub	x9, x9, #0x1
   545dc:	subs	x10, x8, #0x1
   545e0:	b.lt	546e0 <__gmp_doprnt_mpf2@@Base+0x384>  // b.tstop
   545e4:	ldrb	w8, [x9, x8]
   545e8:	cmp	w8, #0x30
   545ec:	mov	x8, x10
   545f0:	b.eq	545dc <__gmp_doprnt_mpf2@@Base+0x280>  // b.none
   545f4:	add	w28, w10, #0x1
   545f8:	cmp	x22, #0x0
   545fc:	b.gt	54684 <__gmp_doprnt_mpf2@@Base+0x328>
   54600:	b	546ec <__gmp_doprnt_mpf2@@Base+0x390>
   54604:	ldr	x11, [sp, #40]
   54608:	mov	x10, xzr
   5460c:	mov	w13, #0xffffffc9            	// #-55
   54610:	add	x12, x8, x11
   54614:	mov	w11, #0xffffffa9            	// #-87
   54618:	sub	x12, x12, #0x1
   5461c:	b	54640 <__gmp_doprnt_mpf2@@Base+0x2e4>
   54620:	tst	w15, #0x200
   54624:	csel	w15, w13, w11, eq  // eq = none
   54628:	add	w14, w15, w14
   5462c:	sxtw	x14, w14
   54630:	add	x14, x14, #0x1
   54634:	cmp	w14, w25
   54638:	sub	x10, x10, #0x1
   5463c:	b.ne	546bc <__gmp_doprnt_mpf2@@Base+0x360>  // b.any
   54640:	cmn	x8, x10
   54644:	b.eq	5465c <__gmp_doprnt_mpf2@@Base+0x300>  // b.none
   54648:	ldrb	w14, [x12, x10]
   5464c:	ldrh	w15, [x9, x14, lsl #1]
   54650:	tbz	w15, #11, 54620 <__gmp_doprnt_mpf2@@Base+0x2c4>
   54654:	sub	w14, w14, #0x30
   54658:	b	5462c <__gmp_doprnt_mpf2@@Base+0x2d0>
   5465c:	ldr	x9, [sp, #56]
   54660:	mov	w8, #0x31                  	// #49
   54664:	mov	w28, #0x1                   	// #1
   54668:	mov	x24, x27
   5466c:	strb	w8, [x9, x23]
   54670:	ldur	x8, [x29, #-16]
   54674:	add	x22, x8, #0x1
   54678:	stur	x22, [x29, #-16]
   5467c:	cmp	x22, #0x0
   54680:	b.le	546ec <__gmp_doprnt_mpf2@@Base+0x390>
   54684:	cmp	x22, w28, sxtw
   54688:	mov	w8, wzr
   5468c:	csel	w27, w28, w22, gt
   54690:	mov	w23, wzr
   54694:	sub	w25, w22, w27
   54698:	mov	w22, w8
   5469c:	sub	w28, w28, w27
   546a0:	ldr	w8, [x20, #40]
   546a4:	cbnz	w8, 54704 <__gmp_doprnt_mpf2@@Base+0x3a8>
   546a8:	mov	w10, wzr
   546ac:	add	w8, w22, w28
   546b0:	cmn	w8, w10
   546b4:	b.eq	5472c <__gmp_doprnt_mpf2@@Base+0x3d0>  // b.none
   546b8:	b	54734 <__gmp_doprnt_mpf2@@Base+0x3d8>
   546bc:	ldr	x9, [sp, #40]
   546c0:	ldrb	w11, [x24, x14]
   546c4:	add	w12, w19, w22
   546c8:	mvn	w12, w12
   546cc:	add	x9, x8, x9
   546d0:	cmp	w12, w10
   546d4:	mov	x24, x27
   546d8:	strb	w11, [x9, x10]
   546dc:	b.ne	54a28 <__gmp_doprnt_mpf2@@Base+0x6cc>  // b.any
   546e0:	mov	w28, wzr
   546e4:	mov	x22, xzr
   546e8:	stur	xzr, [x29, #-16]
   546ec:	mov	w27, wzr
   546f0:	mov	w23, wzr
   546f4:	neg	w22, w22
   546f8:	mov	w25, #0x1                   	// #1
   546fc:	ldr	w8, [x20, #40]
   54700:	cbz	w8, 546a8 <__gmp_doprnt_mpf2@@Base+0x34c>
   54704:	ldr	w9, [x20, #4]
   54708:	add	w10, w27, w25
   5470c:	add	w8, w22, w28
   54710:	cmp	w9, #0x3
   54714:	csneg	w9, wzr, w10, ne  // ne = any
   54718:	sub	w10, w19, w8
   5471c:	add	w9, w10, w9
   54720:	bic	w10, w9, w9, asr #31
   54724:	cmn	w8, w10
   54728:	b.ne	54734 <__gmp_doprnt_mpf2@@Base+0x3d8>  // b.any
   5472c:	ldr	w8, [x20, #36]
   54730:	cbz	w8, 54a00 <__gmp_doprnt_mpf2@@Base+0x6a4>
   54734:	mov	x0, x24
   54738:	mov	w19, w10
   5473c:	bl	bf60 <strlen@plt>
   54740:	mov	w10, w19
   54744:	ldr	w8, [x20, #32]
   54748:	and	w19, w21, #0xff
   5474c:	str	x24, [sp, #16]
   54750:	cmp	w8, #0x1
   54754:	b.eq	54768 <__gmp_doprnt_mpf2@@Base+0x40c>  // b.none
   54758:	cmp	w8, #0x3
   5475c:	b.ne	54794 <__gmp_doprnt_mpf2@@Base+0x438>  // b.any
   54760:	orr	w8, w27, w28
   54764:	cbz	w8, 54794 <__gmp_doprnt_mpf2@@Base+0x438>
   54768:	ldr	w8, [x20]
   5476c:	cmn	w8, #0x10
   54770:	b.eq	54a08 <__gmp_doprnt_mpf2@@Base+0x6ac>  // b.none
   54774:	cmp	w8, #0x8
   54778:	b.eq	54a18 <__gmp_doprnt_mpf2@@Base+0x6bc>  // b.none
   5477c:	cmp	w8, #0x10
   54780:	b.ne	54794 <__gmp_doprnt_mpf2@@Base+0x438>  // b.any
   54784:	adrp	x26, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54788:	mov	w21, #0x2                   	// #2
   5478c:	add	x26, x26, #0x9ca
   54790:	b	5479c <__gmp_doprnt_mpf2@@Base+0x440>
   54794:	mov	x26, xzr
   54798:	mov	w21, wzr
   5479c:	cmp	w19, #0x0
   547a0:	csetm	w9, ne  // ne = any
   547a4:	sub	w9, w9, w28
   547a8:	sub	w9, w9, w22
   547ac:	sub	w9, w9, w25
   547b0:	sub	w9, w9, w27
   547b4:	ldr	w8, [x20, #48]
   547b8:	sub	w9, w9, w23
   547bc:	sub	w9, w9, w10
   547c0:	str	w10, [sp, #12]
   547c4:	sub	w9, w9, w0
   547c8:	ldr	w10, [x20, #24]
   547cc:	sub	w9, w9, w21
   547d0:	add	w24, w9, w8
   547d4:	cmp	w24, #0x0
   547d8:	str	x23, [sp]
   547dc:	csel	w23, w10, wzr, gt
   547e0:	str	w22, [sp, #28]
   547e4:	cmp	w23, #0x2
   547e8:	mov	w22, wzr
   547ec:	str	x0, [sp, #32]
   547f0:	b.ne	54814 <__gmp_doprnt_mpf2@@Base+0x4b8>  // b.any
   547f4:	ldp	x8, x0, [sp, #64]
   547f8:	ldrb	w1, [x20, #20]
   547fc:	mov	w2, w24
   54800:	ldr	x8, [x8, #16]
   54804:	blr	x8
   54808:	mov	w22, w0
   5480c:	cmn	w0, #0x1
   54810:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54814:	cbz	w19, 54838 <__gmp_doprnt_mpf2@@Base+0x4dc>
   54818:	ldp	x8, x0, [sp, #64]
   5481c:	mov	w2, #0x1                   	// #1
   54820:	mov	w1, w19
   54824:	ldr	x8, [x8, #16]
   54828:	blr	x8
   5482c:	cmn	w0, #0x1
   54830:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54834:	add	w22, w0, w22
   54838:	cbz	w21, 5485c <__gmp_doprnt_mpf2@@Base+0x500>
   5483c:	ldp	x8, x0, [sp, #64]
   54840:	mov	x1, x26
   54844:	mov	x2, x21
   54848:	ldr	x8, [x8, #8]
   5484c:	blr	x8
   54850:	cmn	w0, #0x1
   54854:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54858:	add	w22, w0, w22
   5485c:	cmp	w23, #0x3
   54860:	b.ne	54884 <__gmp_doprnt_mpf2@@Base+0x528>  // b.any
   54864:	ldp	x8, x0, [sp, #64]
   54868:	ldrb	w1, [x20, #20]
   5486c:	mov	w2, w24
   54870:	ldr	x8, [x8, #16]
   54874:	blr	x8
   54878:	cmn	w0, #0x1
   5487c:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54880:	add	w22, w0, w22
   54884:	ldp	x8, x0, [sp, #64]
   54888:	ldr	x1, [sp, #40]
   5488c:	sxtw	x21, w27
   54890:	mov	x2, x21
   54894:	ldr	x8, [x8, #8]
   54898:	blr	x8
   5489c:	add	w19, w0, w22
   548a0:	cmn	w0, #0x1
   548a4:	csel	w22, w22, w19, eq  // eq = none
   548a8:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   548ac:	cbz	w25, 548d0 <__gmp_doprnt_mpf2@@Base+0x574>
   548b0:	ldp	x8, x0, [sp, #64]
   548b4:	mov	w1, #0x30                  	// #48
   548b8:	mov	w2, w25
   548bc:	ldr	x8, [x8, #16]
   548c0:	blr	x8
   548c4:	cmn	w0, #0x1
   548c8:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   548cc:	add	w19, w0, w22
   548d0:	ldr	x9, [sp, #32]
   548d4:	cbz	w9, 548f8 <__gmp_doprnt_mpf2@@Base+0x59c>
   548d8:	ldp	x8, x0, [sp, #64]
   548dc:	ldr	x1, [sp, #16]
   548e0:	sxtw	x2, w9
   548e4:	ldr	x8, [x8, #8]
   548e8:	blr	x8
   548ec:	cmn	w0, #0x1
   548f0:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   548f4:	add	w19, w0, w19
   548f8:	ldr	w2, [sp, #28]
   548fc:	cbz	w2, 5491c <__gmp_doprnt_mpf2@@Base+0x5c0>
   54900:	ldp	x8, x0, [sp, #64]
   54904:	mov	w1, #0x30                  	// #48
   54908:	ldr	x8, [x8, #16]
   5490c:	blr	x8
   54910:	cmn	w0, #0x1
   54914:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54918:	add	w19, w0, w19
   5491c:	cbz	w28, 54944 <__gmp_doprnt_mpf2@@Base+0x5e8>
   54920:	ldp	x8, x0, [sp, #64]
   54924:	ldr	x9, [sp, #40]
   54928:	sxtw	x2, w28
   5492c:	ldr	x8, [x8, #8]
   54930:	add	x1, x9, x21
   54934:	blr	x8
   54938:	cmn	w0, #0x1
   5493c:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54940:	add	w19, w0, w19
   54944:	ldr	w2, [sp, #12]
   54948:	cbz	w2, 54968 <__gmp_doprnt_mpf2@@Base+0x60c>
   5494c:	ldp	x8, x0, [sp, #64]
   54950:	mov	w1, #0x30                  	// #48
   54954:	ldr	x8, [x8, #16]
   54958:	blr	x8
   5495c:	cmn	w0, #0x1
   54960:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   54964:	add	w19, w0, w19
   54968:	ldr	x9, [sp]
   5496c:	cbz	w9, 54990 <__gmp_doprnt_mpf2@@Base+0x634>
   54970:	ldp	x8, x0, [sp, #64]
   54974:	sxtw	x2, w9
   54978:	add	x1, sp, #0x54
   5497c:	ldr	x8, [x8, #8]
   54980:	blr	x8
   54984:	cmn	w0, #0x1
   54988:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   5498c:	add	w19, w0, w19
   54990:	cmp	w23, #0x1
   54994:	b.ne	549c0 <__gmp_doprnt_mpf2@@Base+0x664>  // b.any
   54998:	ldp	x8, x0, [sp, #64]
   5499c:	ldrb	w1, [x20, #20]
   549a0:	mov	w2, w24
   549a4:	ldr	x8, [x8, #16]
   549a8:	blr	x8
   549ac:	cmn	w0, #0x1
   549b0:	b.eq	549bc <__gmp_doprnt_mpf2@@Base+0x660>  // b.none
   549b4:	add	w19, w0, w19
   549b8:	b	549c0 <__gmp_doprnt_mpf2@@Base+0x664>
   549bc:	mov	w19, #0xffffffff            	// #-1
   549c0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   549c4:	ldp	x9, x0, [sp, #48]
   549c8:	ldr	x8, [x8, #4016]
   549cc:	add	w9, w9, #0x1
   549d0:	ldr	x8, [x8]
   549d4:	sxtw	x1, w9
   549d8:	blr	x8
   549dc:	mov	w0, w19
   549e0:	ldp	x20, x19, [sp, #256]
   549e4:	ldp	x22, x21, [sp, #240]
   549e8:	ldp	x24, x23, [sp, #224]
   549ec:	ldp	x26, x25, [sp, #208]
   549f0:	ldp	x28, x27, [sp, #192]
   549f4:	ldp	x29, x30, [sp, #176]
   549f8:	add	sp, sp, #0x110
   549fc:	ret
   54a00:	mov	x0, xzr
   54a04:	b	54744 <__gmp_doprnt_mpf2@@Base+0x3e8>
   54a08:	adrp	x26, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54a0c:	mov	w21, #0x2                   	// #2
   54a10:	add	x26, x26, #0x9cd
   54a14:	b	5479c <__gmp_doprnt_mpf2@@Base+0x440>
   54a18:	adrp	x26, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54a1c:	mov	w21, #0x1                   	// #1
   54a20:	add	x26, x26, #0xc87
   54a24:	b	5479c <__gmp_doprnt_mpf2@@Base+0x440>
   54a28:	ldur	x22, [x29, #-16]
   54a2c:	add	x8, x8, x10
   54a30:	add	x28, x8, #0x1
   54a34:	cmp	x22, #0x0
   54a38:	b.gt	54684 <__gmp_doprnt_mpf2@@Base+0x328>
   54a3c:	b	546ec <__gmp_doprnt_mpf2@@Base+0x390>

0000000000054a40 <__gmp_doprnt_integer@@Base>:
   54a40:	sub	sp, sp, #0x90
   54a44:	stp	x29, x30, [sp, #48]
   54a48:	add	x29, sp, #0x30
   54a4c:	stp	x28, x27, [sp, #64]
   54a50:	stp	x26, x25, [sp, #80]
   54a54:	stp	x24, x23, [sp, #96]
   54a58:	stp	x22, x21, [sp, #112]
   54a5c:	stp	x20, x19, [sp, #128]
   54a60:	stur	x1, [x29, #-8]
   54a64:	mov	x9, x3
   54a68:	ldrb	w8, [x2, #44]
   54a6c:	ldrb	w10, [x9], #1
   54a70:	mov	x20, x2
   54a74:	stur	x0, [x29, #-16]
   54a78:	cmp	w10, #0x2d
   54a7c:	csel	w19, w10, w8, eq  // eq = none
   54a80:	csel	x8, x3, x9, ne  // ne = any
   54a84:	ldrb	w8, [x8]
   54a88:	csel	x22, x9, x3, eq  // eq = none
   54a8c:	tst	w19, #0xff
   54a90:	cset	w23, ne  // ne = any
   54a94:	cmp	w8, #0x30
   54a98:	b.ne	54aa8 <__gmp_doprnt_integer@@Base+0x68>  // b.any
   54a9c:	ldr	w8, [x20, #28]
   54aa0:	cmp	w8, #0x0
   54aa4:	cinc	x22, x22, eq  // eq = none
   54aa8:	mov	x0, x22
   54aac:	bl	bf60 <strlen@plt>
   54ab0:	mov	x21, x0
   54ab4:	mov	w1, #0x2f                  	// #47
   54ab8:	mov	x0, x22
   54abc:	bl	cda0 <strchr@plt>
   54ac0:	ldr	w8, [x20, #32]
   54ac4:	stur	w23, [x29, #-20]
   54ac8:	cmp	w8, #0x2
   54acc:	b.ne	54adc <__gmp_doprnt_integer@@Base+0x9c>  // b.any
   54ad0:	str	xzr, [sp, #16]
   54ad4:	mov	w27, wzr
   54ad8:	b	54b28 <__gmp_doprnt_integer@@Base+0xe8>
   54adc:	ldr	w9, [x20]
   54ae0:	cmn	w9, #0x10
   54ae4:	b.eq	54b08 <__gmp_doprnt_integer@@Base+0xc8>  // b.none
   54ae8:	cmp	w9, #0x8
   54aec:	b.eq	54b18 <__gmp_doprnt_integer@@Base+0xd8>  // b.none
   54af0:	cmp	w9, #0x10
   54af4:	b.ne	54ad0 <__gmp_doprnt_integer@@Base+0x90>  // b.any
   54af8:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54afc:	mov	w27, #0x2                   	// #2
   54b00:	add	x9, x9, #0x9ca
   54b04:	b	54b24 <__gmp_doprnt_integer@@Base+0xe4>
   54b08:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54b0c:	mov	w27, #0x2                   	// #2
   54b10:	add	x9, x9, #0x9cd
   54b14:	b	54b24 <__gmp_doprnt_integer@@Base+0xe4>
   54b18:	adrp	x9, 63000 <__gmp_jacobi_table@@Base+0x7599>
   54b1c:	mov	w27, #0x1                   	// #1
   54b20:	add	x9, x9, #0xc87
   54b24:	str	x9, [sp, #16]
   54b28:	and	w19, w19, #0xff
   54b2c:	cmp	w8, #0x3
   54b30:	str	x0, [sp, #8]
   54b34:	cbz	x0, 54b4c <__gmp_doprnt_integer@@Base+0x10c>
   54b38:	b.ne	54b64 <__gmp_doprnt_integer@@Base+0x124>  // b.any
   54b3c:	ldrb	w8, [x0, #1]
   54b40:	cmp	w8, #0x30
   54b44:	csel	w28, wzr, w27, eq  // eq = none
   54b48:	b	54b54 <__gmp_doprnt_integer@@Base+0x114>
   54b4c:	mov	w28, wzr
   54b50:	b.ne	54b68 <__gmp_doprnt_integer@@Base+0x128>  // b.any
   54b54:	ldrb	w8, [x22]
   54b58:	cmp	w8, #0x30
   54b5c:	csel	w27, wzr, w27, eq  // eq = none
   54b60:	b	54b68 <__gmp_doprnt_integer@@Base+0x128>
   54b64:	mov	w28, w27
   54b68:	ldp	w10, w9, [x20, #24]
   54b6c:	cmp	w19, #0x0
   54b70:	ldr	w8, [x20, #48]
   54b74:	cinc	w11, w21, ne  // ne = any
   54b78:	add	w11, w28, w11
   54b7c:	add	w11, w11, w27
   54b80:	sub	w23, w9, w21
   54b84:	bic	w24, w23, w23, asr #31
   54b88:	sub	w8, w8, w11
   54b8c:	sub	w26, w8, w24
   54b90:	cmp	w26, #0x0
   54b94:	str	x21, [sp]
   54b98:	csel	w21, w10, wzr, gt
   54b9c:	cmp	w21, #0x2
   54ba0:	mov	w25, wzr
   54ba4:	b.ne	54bc8 <__gmp_doprnt_integer@@Base+0x188>  // b.any
   54ba8:	ldp	x8, x0, [x29, #-16]
   54bac:	ldrb	w1, [x20, #20]
   54bb0:	mov	w2, w26
   54bb4:	ldr	x8, [x8, #16]
   54bb8:	blr	x8
   54bbc:	mov	w25, w0
   54bc0:	cmn	w0, #0x1
   54bc4:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54bc8:	cbz	w19, 54bec <__gmp_doprnt_integer@@Base+0x1ac>
   54bcc:	ldp	x8, x0, [x29, #-16]
   54bd0:	ldur	w2, [x29, #-20]
   54bd4:	mov	w1, w19
   54bd8:	ldr	x8, [x8, #16]
   54bdc:	blr	x8
   54be0:	cmn	w0, #0x1
   54be4:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54be8:	add	w25, w0, w25
   54bec:	cbz	w27, 54c10 <__gmp_doprnt_integer@@Base+0x1d0>
   54bf0:	ldp	x8, x0, [x29, #-16]
   54bf4:	ldr	x1, [sp, #16]
   54bf8:	mov	x2, x27
   54bfc:	ldr	x8, [x8, #8]
   54c00:	blr	x8
   54c04:	cmn	w0, #0x1
   54c08:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54c0c:	add	w25, w0, w25
   54c10:	cmp	w23, #0x1
   54c14:	b.lt	54c38 <__gmp_doprnt_integer@@Base+0x1f8>  // b.tstop
   54c18:	ldp	x8, x0, [x29, #-16]
   54c1c:	mov	w1, #0x30                  	// #48
   54c20:	mov	w2, w24
   54c24:	ldr	x8, [x8, #16]
   54c28:	blr	x8
   54c2c:	cmn	w0, #0x1
   54c30:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54c34:	add	w25, w0, w25
   54c38:	cmp	w21, #0x3
   54c3c:	b.ne	54c60 <__gmp_doprnt_integer@@Base+0x220>  // b.any
   54c40:	ldp	x8, x0, [x29, #-16]
   54c44:	ldrb	w1, [x20, #20]
   54c48:	mov	w2, w26
   54c4c:	ldr	x8, [x8, #16]
   54c50:	blr	x8
   54c54:	cmn	w0, #0x1
   54c58:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54c5c:	add	w25, w0, w25
   54c60:	cbz	w28, 54cc8 <__gmp_doprnt_integer@@Base+0x288>
   54c64:	ldp	x8, x0, [x29, #-16]
   54c68:	ldr	x9, [sp, #8]
   54c6c:	mov	x1, x22
   54c70:	ldr	x8, [x8, #8]
   54c74:	sub	x9, x9, x22
   54c78:	add	x23, x9, #0x1
   54c7c:	sxtw	x19, w23
   54c80:	mov	x2, x19
   54c84:	blr	x8
   54c88:	cmn	w0, #0x1
   54c8c:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54c90:	mov	w24, w0
   54c94:	ldp	x8, x0, [x29, #-16]
   54c98:	ldr	x1, [sp, #16]
   54c9c:	mov	x2, x28
   54ca0:	ldr	x8, [x8, #8]
   54ca4:	blr	x8
   54ca8:	cmn	w0, #0x1
   54cac:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54cb0:	ldr	x9, [sp]
   54cb4:	add	w8, w24, w25
   54cb8:	add	x22, x22, x19
   54cbc:	add	w25, w8, w0
   54cc0:	sub	x9, x9, x23
   54cc4:	b	54ccc <__gmp_doprnt_integer@@Base+0x28c>
   54cc8:	ldr	x9, [sp]
   54ccc:	ldp	x8, x0, [x29, #-16]
   54cd0:	sxtw	x2, w9
   54cd4:	mov	x1, x22
   54cd8:	ldr	x8, [x8, #8]
   54cdc:	blr	x8
   54ce0:	mov	w8, w0
   54ce4:	add	w0, w0, w25
   54ce8:	cmn	w8, #0x1
   54cec:	csel	w19, w25, w0, eq  // eq = none
   54cf0:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54cf4:	cmp	w21, #0x1
   54cf8:	b.ne	54d24 <__gmp_doprnt_integer@@Base+0x2e4>  // b.any
   54cfc:	ldp	x8, x0, [x29, #-16]
   54d00:	ldrb	w1, [x20, #20]
   54d04:	mov	w2, w26
   54d08:	ldr	x8, [x8, #16]
   54d0c:	blr	x8
   54d10:	cmn	w0, #0x1
   54d14:	b.eq	54d20 <__gmp_doprnt_integer@@Base+0x2e0>  // b.none
   54d18:	add	w0, w0, w19
   54d1c:	b	54d24 <__gmp_doprnt_integer@@Base+0x2e4>
   54d20:	mov	w0, #0xffffffff            	// #-1
   54d24:	ldp	x20, x19, [sp, #128]
   54d28:	ldp	x22, x21, [sp, #112]
   54d2c:	ldp	x24, x23, [sp, #96]
   54d30:	ldp	x26, x25, [sp, #80]
   54d34:	ldp	x28, x27, [sp, #64]
   54d38:	ldp	x29, x30, [sp, #48]
   54d3c:	add	sp, sp, #0x90
   54d40:	ret

0000000000054d44 <__gmp_fprintf@@Base>:
   54d44:	sub	sp, sp, #0x100
   54d48:	stp	x29, x30, [sp, #240]
   54d4c:	add	x29, sp, #0xf0
   54d50:	mov	x9, #0xffffffffffffffd0    	// #-48
   54d54:	mov	x10, sp
   54d58:	sub	x11, x29, #0x70
   54d5c:	movk	x9, #0xff80, lsl #32
   54d60:	add	x12, x29, #0x10
   54d64:	add	x10, x10, #0x80
   54d68:	add	x11, x11, #0x30
   54d6c:	stp	x10, x9, [x29, #-16]
   54d70:	stp	x12, x11, [x29, #-32]
   54d74:	stp	x2, x3, [x29, #-112]
   54d78:	stp	x4, x5, [x29, #-96]
   54d7c:	stp	x6, x7, [x29, #-80]
   54d80:	stp	q1, q2, [sp, #16]
   54d84:	str	q0, [sp]
   54d88:	ldp	q0, q1, [x29, #-32]
   54d8c:	mov	x8, x1
   54d90:	mov	x1, x0
   54d94:	stp	q3, q4, [sp, #48]
   54d98:	stp	q5, q6, [sp, #80]
   54d9c:	str	q7, [sp, #112]
   54da0:	stp	q0, q1, [x29, #-64]
   54da4:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54da8:	ldr	x0, [x0, #3816]
   54dac:	sub	x3, x29, #0x40
   54db0:	mov	x2, x8
   54db4:	bl	d030 <__gmp_doprnt@plt>
   54db8:	ldp	x29, x30, [sp, #240]
   54dbc:	add	sp, sp, #0x100
   54dc0:	ret

0000000000054dc4 <__gmp_obstack_printf@@Base>:
   54dc4:	sub	sp, sp, #0x100
   54dc8:	stp	x29, x30, [sp, #240]
   54dcc:	add	x29, sp, #0xf0
   54dd0:	mov	x9, #0xffffffffffffffd0    	// #-48
   54dd4:	mov	x10, sp
   54dd8:	sub	x11, x29, #0x70
   54ddc:	movk	x9, #0xff80, lsl #32
   54de0:	add	x12, x29, #0x10
   54de4:	add	x10, x10, #0x80
   54de8:	add	x11, x11, #0x30
   54dec:	stp	x10, x9, [x29, #-16]
   54df0:	stp	x12, x11, [x29, #-32]
   54df4:	stp	x2, x3, [x29, #-112]
   54df8:	stp	x4, x5, [x29, #-96]
   54dfc:	stp	x6, x7, [x29, #-80]
   54e00:	stp	q1, q2, [sp, #16]
   54e04:	str	q0, [sp]
   54e08:	ldp	q0, q1, [x29, #-32]
   54e0c:	mov	x8, x1
   54e10:	mov	x1, x0
   54e14:	stp	q3, q4, [sp, #48]
   54e18:	stp	q5, q6, [sp, #80]
   54e1c:	str	q7, [sp, #112]
   54e20:	stp	q0, q1, [x29, #-64]
   54e24:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54e28:	ldr	x0, [x0, #4048]
   54e2c:	sub	x3, x29, #0x40
   54e30:	mov	x2, x8
   54e34:	bl	d030 <__gmp_doprnt@plt>
   54e38:	ldp	x29, x30, [sp, #240]
   54e3c:	add	sp, sp, #0x100
   54e40:	ret

0000000000054e44 <__gmp_obstack_vprintf@@Base>:
   54e44:	sub	sp, sp, #0x30
   54e48:	stp	x29, x30, [sp, #32]
   54e4c:	ldp	q1, q0, [x2]
   54e50:	mov	x8, x1
   54e54:	mov	x1, x0
   54e58:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54e5c:	stp	q1, q0, [sp]
   54e60:	ldr	x0, [x0, #4048]
   54e64:	mov	x3, sp
   54e68:	mov	x2, x8
   54e6c:	add	x29, sp, #0x20
   54e70:	bl	d030 <__gmp_doprnt@plt>
   54e74:	ldp	x29, x30, [sp, #32]
   54e78:	add	sp, sp, #0x30
   54e7c:	ret
   54e80:	stp	x29, x30, [sp, #-48]!
   54e84:	stp	x22, x21, [sp, #16]
   54e88:	stp	x20, x19, [sp, #32]
   54e8c:	mov	x19, x0
   54e90:	ldr	x0, [x0, #24]
   54e94:	ldr	x8, [x19, #32]
   54e98:	sxtw	x21, w2
   54e9c:	mov	x20, x2
   54ea0:	add	x9, x0, x21
   54ea4:	cmp	x9, x8
   54ea8:	mov	x22, x1
   54eac:	mov	x29, sp
   54eb0:	b.ls	54ec4 <__gmp_obstack_vprintf@@Base+0x80>  // b.plast
   54eb4:	mov	x0, x19
   54eb8:	mov	w1, w20
   54ebc:	bl	d040 <_obstack_newchunk@plt>
   54ec0:	ldr	x0, [x19, #24]
   54ec4:	mov	x1, x22
   54ec8:	mov	x2, x21
   54ecc:	bl	bed0 <memcpy@plt>
   54ed0:	ldr	x8, [x19, #24]
   54ed4:	mov	w0, w20
   54ed8:	add	x8, x8, x21
   54edc:	str	x8, [x19, #24]
   54ee0:	ldp	x20, x19, [sp, #32]
   54ee4:	ldp	x22, x21, [sp, #16]
   54ee8:	ldp	x29, x30, [sp], #48
   54eec:	ret
   54ef0:	stp	x29, x30, [sp, #-48]!
   54ef4:	stp	x22, x21, [sp, #16]
   54ef8:	stp	x20, x19, [sp, #32]
   54efc:	mov	x20, x0
   54f00:	ldp	x0, x8, [x0, #24]
   54f04:	mov	w19, w2
   54f08:	sxtw	x21, w19
   54f0c:	mov	w22, w1
   54f10:	sub	x8, x8, x0
   54f14:	cmp	x8, x21
   54f18:	mov	x29, sp
   54f1c:	b.ge	54f30 <__gmp_obstack_vprintf@@Base+0xec>  // b.tcont
   54f20:	mov	x0, x20
   54f24:	mov	w1, w19
   54f28:	bl	d040 <_obstack_newchunk@plt>
   54f2c:	ldr	x0, [x20, #24]
   54f30:	add	x8, x0, x21
   54f34:	mov	w1, w22
   54f38:	mov	x2, x21
   54f3c:	str	x8, [x20, #24]
   54f40:	bl	c5f0 <memset@plt>
   54f44:	mov	w0, w19
   54f48:	ldp	x20, x19, [sp, #32]
   54f4c:	ldp	x22, x21, [sp, #16]
   54f50:	ldp	x29, x30, [sp], #48
   54f54:	ret

0000000000054f58 <__gmp_printf@@Base>:
   54f58:	sub	sp, sp, #0x120
   54f5c:	stp	x29, x30, [sp, #256]
   54f60:	add	x29, sp, #0x100
   54f64:	mov	x9, #0xffffffffffffffc8    	// #-56
   54f68:	mov	x10, sp
   54f6c:	sub	x11, x29, #0x78
   54f70:	str	x28, [sp, #272]
   54f74:	stp	x1, x2, [x29, #-120]
   54f78:	stp	x3, x4, [x29, #-104]
   54f7c:	stp	x5, x6, [x29, #-88]
   54f80:	stur	x7, [x29, #-72]
   54f84:	stp	q0, q1, [sp]
   54f88:	stp	q2, q3, [sp, #32]
   54f8c:	stp	q4, q5, [sp, #64]
   54f90:	movk	x9, #0xff80, lsl #32
   54f94:	add	x12, x29, #0x20
   54f98:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54f9c:	add	x10, x10, #0x80
   54fa0:	add	x11, x11, #0x38
   54fa4:	ldr	x13, [x13, #3856]
   54fa8:	stp	x10, x9, [x29, #-16]
   54fac:	stp	x12, x11, [x29, #-32]
   54fb0:	ldp	q0, q1, [x29, #-32]
   54fb4:	mov	x8, x0
   54fb8:	stp	q6, q7, [sp, #96]
   54fbc:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   54fc0:	stp	q0, q1, [x29, #-64]
   54fc4:	ldr	x1, [x13]
   54fc8:	ldr	x0, [x0, #3816]
   54fcc:	sub	x3, x29, #0x40
   54fd0:	mov	x2, x8
   54fd4:	bl	d030 <__gmp_doprnt@plt>
   54fd8:	ldr	x28, [sp, #272]
   54fdc:	ldp	x29, x30, [sp, #256]
   54fe0:	add	sp, sp, #0x120
   54fe4:	ret
   54fe8:	stp	x29, x30, [sp, #-16]!
   54fec:	mov	x8, x1
   54ff0:	mov	x3, x0
   54ff4:	mov	w1, #0x1                   	// #1
   54ff8:	mov	x0, x8
   54ffc:	mov	x29, sp
   55000:	bl	ce30 <fwrite@plt>
   55004:	ldp	x29, x30, [sp], #16
   55008:	ret
   5500c:	sub	sp, sp, #0x140
   55010:	stp	x20, x19, [sp, #304]
   55014:	mov	w19, w2
   55018:	sxtw	x8, w19
   5501c:	stp	x22, x21, [sp, #288]
   55020:	cmp	x8, #0x100
   55024:	mov	w21, #0x100                 	// #256
   55028:	mov	x20, x0
   5502c:	csel	x2, x8, x21, cc  // cc = lo, ul, last
   55030:	mov	x0, sp
   55034:	stp	x29, x30, [sp, #256]
   55038:	stp	x28, x23, [sp, #272]
   5503c:	add	x29, sp, #0x100
   55040:	bl	c5f0 <memset@plt>
   55044:	cmp	w19, #0x1
   55048:	b.lt	55084 <__gmp_printf@@Base+0x12c>  // b.tstop
   5504c:	mov	w22, w19
   55050:	subs	w23, w22, #0x100
   55054:	csel	w2, w22, w21, cc  // cc = lo, ul, last
   55058:	mov	x0, sp
   5505c:	mov	w1, #0x1                   	// #1
   55060:	mov	x3, x20
   55064:	bl	ce30 <fwrite@plt>
   55068:	cmn	w0, #0x1
   5506c:	b.eq	55080 <__gmp_printf@@Base+0x128>  // b.none
   55070:	cmp	w22, #0x101
   55074:	mov	w22, w23
   55078:	b.ge	55050 <__gmp_printf@@Base+0xf8>  // b.tcont
   5507c:	b	55084 <__gmp_printf@@Base+0x12c>
   55080:	mov	w19, #0xffffffff            	// #-1
   55084:	mov	w0, w19
   55088:	ldp	x20, x19, [sp, #304]
   5508c:	ldp	x22, x21, [sp, #288]
   55090:	ldp	x28, x23, [sp, #272]
   55094:	ldp	x29, x30, [sp, #256]
   55098:	add	sp, sp, #0x140
   5509c:	ret

00000000000550a0 <__gmp_snprintf@@Base>:
   550a0:	sub	sp, sp, #0x120
   550a4:	stp	x29, x30, [sp, #256]
   550a8:	add	x29, sp, #0x100
   550ac:	mov	x8, #0xffffffffffffffd8    	// #-40
   550b0:	mov	x9, sp
   550b4:	sub	x10, x29, #0x78
   550b8:	movk	x8, #0xff80, lsl #32
   550bc:	add	x11, x29, #0x20
   550c0:	add	x9, x9, #0x80
   550c4:	add	x10, x10, #0x28
   550c8:	stp	x9, x8, [x29, #-32]
   550cc:	stp	x11, x10, [x29, #-48]
   550d0:	stp	x3, x4, [x29, #-120]
   550d4:	stp	x5, x6, [x29, #-104]
   550d8:	stur	x7, [x29, #-88]
   550dc:	stp	q1, q2, [sp, #16]
   550e0:	str	q0, [sp]
   550e4:	ldp	q0, q1, [x29, #-48]
   550e8:	str	x28, [sp, #272]
   550ec:	stp	q3, q4, [sp, #48]
   550f0:	stp	q5, q6, [sp, #80]
   550f4:	str	q7, [sp, #112]
   550f8:	stp	x0, x1, [x29, #-16]
   550fc:	stp	q0, q1, [x29, #-80]
   55100:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55104:	ldr	x0, [x0, #4024]
   55108:	sub	x1, x29, #0x10
   5510c:	sub	x3, x29, #0x50
   55110:	bl	d030 <__gmp_doprnt@plt>
   55114:	ldr	x28, [sp, #272]
   55118:	ldp	x29, x30, [sp, #256]
   5511c:	add	sp, sp, #0x120
   55120:	ret
   55124:	sub	sp, sp, #0x90
   55128:	stp	x29, x30, [sp, #64]
   5512c:	stp	x24, x23, [sp, #96]
   55130:	stp	x22, x21, [sp, #112]
   55134:	stp	x20, x19, [sp, #128]
   55138:	ldr	x23, [x0, #8]
   5513c:	mov	x19, x2
   55140:	mov	x20, x1
   55144:	str	x25, [sp, #80]
   55148:	cmp	x23, #0x2
   5514c:	add	x29, sp, #0x40
   55150:	b.cc	551bc <__gmp_snprintf@@Base+0x11c>  // b.lo, b.ul, b.last
   55154:	ldp	q1, q0, [x19]
   55158:	mov	x21, x0
   5515c:	mov	x3, sp
   55160:	mov	x1, x23
   55164:	stp	q1, q0, [sp, #32]
   55168:	ldr	x0, [x0]
   5516c:	mov	x2, x20
   55170:	stp	q1, q0, [sp]
   55174:	bl	d110 <vsnprintf@plt>
   55178:	cmn	w0, #0x1
   5517c:	b.eq	55228 <__gmp_snprintf@@Base+0x188>  // b.none
   55180:	mov	w22, w0
   55184:	ldp	x11, x10, [x21]
   55188:	sxtw	x8, w22
   5518c:	sub	x9, x23, #0x1
   55190:	cmp	x9, x8
   55194:	csel	x12, x8, x9, hi  // hi = pmore
   55198:	cmp	x9, x8
   5519c:	sub	x8, x10, x12
   551a0:	add	x9, x11, x12
   551a4:	stp	x9, x8, [x21]
   551a8:	b.ne	5522c <__gmp_snprintf@@Base+0x18c>  // b.any
   551ac:	cmp	w22, #0x80
   551b0:	mov	w8, #0x80                  	// #128
   551b4:	csel	w21, w22, w8, gt
   551b8:	b	551c0 <__gmp_snprintf@@Base+0x120>
   551bc:	mov	w21, #0x80                  	// #128
   551c0:	adrp	x24, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   551c4:	adrp	x25, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   551c8:	ldr	x24, [x24, #3840]
   551cc:	ldr	x25, [x25, #4016]
   551d0:	ldr	x8, [x24]
   551d4:	lsl	x21, x21, #1
   551d8:	mov	x0, x21
   551dc:	blr	x8
   551e0:	ldp	q0, q1, [x19]
   551e4:	mov	x3, sp
   551e8:	mov	x1, x21
   551ec:	mov	x2, x20
   551f0:	stp	q0, q1, [sp, #32]
   551f4:	ldp	q0, q1, [sp, #32]
   551f8:	mov	x23, x0
   551fc:	stp	q0, q1, [sp]
   55200:	bl	d110 <vsnprintf@plt>
   55204:	ldr	x8, [x25]
   55208:	mov	w22, w0
   5520c:	mov	x0, x23
   55210:	mov	x1, x21
   55214:	blr	x8
   55218:	sub	x8, x21, #0x1
   5521c:	cmp	x8, w22, sxtw
   55220:	b.eq	551d0 <__gmp_snprintf@@Base+0x130>  // b.none
   55224:	b	5522c <__gmp_snprintf@@Base+0x18c>
   55228:	mov	w22, #0xffffffff            	// #-1
   5522c:	mov	w0, w22
   55230:	ldp	x20, x19, [sp, #128]
   55234:	ldp	x22, x21, [sp, #112]
   55238:	ldp	x24, x23, [sp, #96]
   5523c:	ldr	x25, [sp, #80]
   55240:	ldp	x29, x30, [sp, #64]
   55244:	add	sp, sp, #0x90
   55248:	ret
   5524c:	stp	x29, x30, [sp, #-48]!
   55250:	stp	x20, x19, [sp, #32]
   55254:	ldr	x8, [x0, #8]
   55258:	mov	x19, x2
   5525c:	str	x21, [sp, #16]
   55260:	mov	x29, sp
   55264:	cmp	x8, #0x2
   55268:	b.cc	55298 <__gmp_snprintf@@Base+0x1f8>  // b.lo, b.ul, b.last
   5526c:	mov	x20, x0
   55270:	ldr	x0, [x0]
   55274:	sub	x8, x8, #0x1
   55278:	cmp	x8, x19
   5527c:	csel	x21, x8, x19, cc  // cc = lo, ul, last
   55280:	mov	x2, x21
   55284:	bl	bed0 <memcpy@plt>
   55288:	ldp	x8, x9, [x20]
   5528c:	add	x8, x8, x21
   55290:	sub	x9, x9, x21
   55294:	stp	x8, x9, [x20]
   55298:	mov	w0, w19
   5529c:	ldp	x20, x19, [sp, #32]
   552a0:	ldr	x21, [sp, #16]
   552a4:	ldp	x29, x30, [sp], #48
   552a8:	ret
   552ac:	stp	x29, x30, [sp, #-48]!
   552b0:	stp	x20, x19, [sp, #32]
   552b4:	ldr	x8, [x0, #8]
   552b8:	mov	w19, w2
   552bc:	str	x21, [sp, #16]
   552c0:	mov	x29, sp
   552c4:	cmp	x8, #0x2
   552c8:	b.cc	552fc <__gmp_snprintf@@Base+0x25c>  // b.lo, b.ul, b.last
   552cc:	mov	x20, x0
   552d0:	sub	x8, x8, #0x1
   552d4:	ldr	x0, [x0]
   552d8:	sxtw	x9, w19
   552dc:	cmp	x8, x9
   552e0:	csel	x21, x8, x9, cc  // cc = lo, ul, last
   552e4:	mov	x2, x21
   552e8:	bl	c5f0 <memset@plt>
   552ec:	ldp	x8, x9, [x20]
   552f0:	add	x8, x8, x21
   552f4:	sub	x9, x9, x21
   552f8:	stp	x8, x9, [x20]
   552fc:	mov	w0, w19
   55300:	ldp	x20, x19, [sp, #32]
   55304:	ldr	x21, [sp, #16]
   55308:	ldp	x29, x30, [sp], #48
   5530c:	ret
   55310:	ldr	x8, [x0, #8]
   55314:	cbz	x8, 55320 <__gmp_snprintf@@Base+0x280>
   55318:	ldr	x8, [x0]
   5531c:	strb	wzr, [x8]
   55320:	mov	w0, wzr
   55324:	ret

0000000000055328 <__gmp_sprintf@@Base>:
   55328:	sub	sp, sp, #0x120
   5532c:	stp	x29, x30, [sp, #256]
   55330:	add	x29, sp, #0x100
   55334:	mov	x10, #0xffffffffffffffd0    	// #-48
   55338:	mov	x11, sp
   5533c:	add	x12, sp, #0x80
   55340:	movk	x10, #0xff80, lsl #32
   55344:	add	x13, x29, #0x20
   55348:	add	x11, x11, #0x80
   5534c:	add	x12, x12, #0x30
   55350:	sub	x9, x29, #0x28
   55354:	stp	x11, x10, [x29, #-24]
   55358:	stp	x13, x12, [x29, #-40]
   5535c:	stp	q1, q2, [sp, #16]
   55360:	str	q0, [sp]
   55364:	ldp	q0, q1, [x9]
   55368:	str	x28, [sp, #272]
   5536c:	stp	x2, x3, [sp, #128]
   55370:	stp	x4, x5, [sp, #144]
   55374:	stp	x6, x7, [sp, #160]
   55378:	stp	q3, q4, [sp, #48]
   5537c:	stp	q5, q6, [sp, #80]
   55380:	str	q7, [sp, #112]
   55384:	stur	x0, [x29, #-8]
   55388:	stp	q0, q1, [x29, #-80]
   5538c:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55390:	ldr	x0, [x0, #3944]
   55394:	mov	x8, x1
   55398:	sub	x1, x29, #0x8
   5539c:	sub	x3, x29, #0x50
   553a0:	mov	x2, x8
   553a4:	bl	d030 <__gmp_doprnt@plt>
   553a8:	ldr	x28, [sp, #272]
   553ac:	ldp	x29, x30, [sp, #256]
   553b0:	add	sp, sp, #0x120
   553b4:	ret
   553b8:	sub	sp, sp, #0x40
   553bc:	stp	x29, x30, [sp, #32]
   553c0:	stp	x20, x19, [sp, #48]
   553c4:	ldr	x20, [x0]
   553c8:	ldp	q1, q0, [x2]
   553cc:	mov	x19, x0
   553d0:	mov	x2, sp
   553d4:	mov	x0, x20
   553d8:	add	x29, sp, #0x20
   553dc:	stp	q1, q0, [sp]
   553e0:	bl	cf10 <vsprintf@plt>
   553e4:	mov	x0, x20
   553e8:	bl	bf60 <strlen@plt>
   553ec:	add	x8, x20, w0, sxtw
   553f0:	str	x8, [x19]
   553f4:	ldp	x20, x19, [sp, #48]
   553f8:	ldp	x29, x30, [sp, #32]
   553fc:	add	sp, sp, #0x40
   55400:	ret
   55404:	stp	x29, x30, [sp, #-32]!
   55408:	ldr	x8, [x0]
   5540c:	str	x19, [sp, #16]
   55410:	mov	x29, sp
   55414:	mov	x19, x2
   55418:	add	x9, x8, x2
   5541c:	str	x9, [x0]
   55420:	mov	x0, x8
   55424:	bl	bed0 <memcpy@plt>
   55428:	mov	w0, w19
   5542c:	ldr	x19, [sp, #16]
   55430:	ldp	x29, x30, [sp], #32
   55434:	ret
   55438:	stp	x29, x30, [sp, #-32]!
   5543c:	ldr	x8, [x0]
   55440:	str	x19, [sp, #16]
   55444:	mov	w19, w2
   55448:	sxtw	x2, w19
   5544c:	add	x9, x8, x2
   55450:	str	x9, [x0]
   55454:	mov	x0, x8
   55458:	mov	x29, sp
   5545c:	bl	c5f0 <memset@plt>
   55460:	mov	w0, w19
   55464:	ldr	x19, [sp, #16]
   55468:	ldp	x29, x30, [sp], #32
   5546c:	ret
   55470:	ldr	x8, [x0]
   55474:	mov	w0, wzr
   55478:	strb	wzr, [x8]
   5547c:	ret
   55480:	sub	sp, sp, #0x80
   55484:	stp	x29, x30, [sp, #64]
   55488:	str	x23, [sp, #80]
   5548c:	stp	x22, x21, [sp, #96]
   55490:	stp	x20, x19, [sp, #112]
   55494:	adrp	x23, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55498:	ldr	x23, [x23, #3792]
   5549c:	mov	x20, x2
   554a0:	mov	x21, x1
   554a4:	mov	x19, x0
   554a8:	mov	w9, #0x100                 	// #256
   554ac:	add	x29, sp, #0x40
   554b0:	ldp	x8, x1, [x19, #16]
   554b4:	add	x9, x8, x9
   554b8:	cmp	x1, x9
   554bc:	b.hi	554dc <__gmp_sprintf@@Base+0x1b4>  // b.pmore
   554c0:	lsl	x2, x9, #1
   554c4:	str	x2, [x19, #24]
   554c8:	ldr	x8, [x23]
   554cc:	ldr	x0, [x19, #8]
   554d0:	blr	x8
   554d4:	ldp	x8, x1, [x19, #16]
   554d8:	str	x0, [x19, #8]
   554dc:	ldp	q1, q0, [x20]
   554e0:	sub	x22, x1, x8
   554e4:	mov	x3, sp
   554e8:	mov	x1, x22
   554ec:	stp	q1, q0, [sp, #32]
   554f0:	ldp	x9, x10, [x19, #8]
   554f4:	mov	x2, x21
   554f8:	stp	q1, q0, [sp]
   554fc:	add	x0, x9, x10
   55500:	bl	d110 <vsnprintf@plt>
   55504:	sub	w8, w22, #0x1
   55508:	cmn	w0, #0x1
   5550c:	csel	w8, w8, w0, eq  // eq = none
   55510:	sxtw	x0, w8
   55514:	sub	x8, x22, #0x1
   55518:	cmp	x8, x0
   5551c:	b.hi	55538 <__gmp_sprintf@@Base+0x210>  // b.pmore
   55520:	add	w10, w0, #0x2
   55524:	lsl	x9, x22, #1
   55528:	sxtw	x10, w10
   5552c:	cmp	x8, x0
   55530:	csel	x9, x9, x10, eq  // eq = none
   55534:	b	554b0 <__gmp_sprintf@@Base+0x188>
   55538:	ldr	x8, [x19, #16]
   5553c:	ldr	x23, [sp, #80]
   55540:	add	x8, x8, x0
   55544:	str	x8, [x19, #16]
   55548:	ldp	x20, x19, [sp, #112]
   5554c:	ldp	x22, x21, [sp, #96]
   55550:	ldp	x29, x30, [sp, #64]
   55554:	add	sp, sp, #0x80
   55558:	ret

000000000005555c <__gmp_vasprintf@@Base>:
   5555c:	sub	sp, sp, #0x60
   55560:	stp	x29, x30, [sp, #64]
   55564:	stp	x20, x19, [sp, #80]
   55568:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5556c:	ldr	x8, [x8, #3840]
   55570:	mov	w9, #0x100                 	// #256
   55574:	str	x0, [sp, #32]
   55578:	mov	w0, #0x100                 	// #256
   5557c:	ldr	x8, [x8]
   55580:	add	x29, sp, #0x40
   55584:	mov	x19, x2
   55588:	mov	x20, x1
   5558c:	str	x9, [sp, #56]
   55590:	blr	x8
   55594:	stp	x0, xzr, [sp, #40]
   55598:	ldp	q1, q0, [x19]
   5559c:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   555a0:	add	x1, sp, #0x20
   555a4:	mov	x3, sp
   555a8:	stp	q1, q0, [sp]
   555ac:	ldr	x0, [x0, #3864]
   555b0:	mov	x2, x20
   555b4:	bl	d030 <__gmp_doprnt@plt>
   555b8:	ldp	x20, x19, [sp, #80]
   555bc:	ldp	x29, x30, [sp, #64]
   555c0:	add	sp, sp, #0x60
   555c4:	ret

00000000000555c8 <__gmp_vfprintf@@Base>:
   555c8:	sub	sp, sp, #0x30
   555cc:	stp	x29, x30, [sp, #32]
   555d0:	ldp	q1, q0, [x2]
   555d4:	mov	x8, x1
   555d8:	mov	x1, x0
   555dc:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   555e0:	stp	q1, q0, [sp]
   555e4:	ldr	x0, [x0, #3816]
   555e8:	mov	x3, sp
   555ec:	mov	x2, x8
   555f0:	add	x29, sp, #0x20
   555f4:	bl	d030 <__gmp_doprnt@plt>
   555f8:	ldp	x29, x30, [sp, #32]
   555fc:	add	sp, sp, #0x30
   55600:	ret

0000000000055604 <__gmp_vprintf@@Base>:
   55604:	sub	sp, sp, #0x30
   55608:	stp	x29, x30, [sp, #32]
   5560c:	ldp	q0, q1, [x1]
   55610:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55614:	ldr	x8, [x8, #3856]
   55618:	mov	x2, x0
   5561c:	stp	q0, q1, [sp]
   55620:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55624:	ldr	x1, [x8]
   55628:	ldr	x0, [x0, #3816]
   5562c:	mov	x3, sp
   55630:	add	x29, sp, #0x20
   55634:	bl	d030 <__gmp_doprnt@plt>
   55638:	ldp	x29, x30, [sp, #32]
   5563c:	add	sp, sp, #0x30
   55640:	ret

0000000000055644 <__gmp_vsnprintf@@Base>:
   55644:	sub	sp, sp, #0x40
   55648:	stp	x29, x30, [sp, #48]
   5564c:	add	x29, sp, #0x30
   55650:	stp	x0, x1, [x29, #-16]
   55654:	ldp	q1, q0, [x3]
   55658:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5565c:	sub	x1, x29, #0x10
   55660:	mov	x3, sp
   55664:	stp	q1, q0, [sp]
   55668:	ldr	x0, [x0, #4024]
   5566c:	bl	d030 <__gmp_doprnt@plt>
   55670:	ldp	x29, x30, [sp, #48]
   55674:	add	sp, sp, #0x40
   55678:	ret

000000000005567c <__gmp_vsprintf@@Base>:
   5567c:	sub	sp, sp, #0x40
   55680:	stp	x29, x30, [sp, #48]
   55684:	add	x29, sp, #0x30
   55688:	stur	x0, [x29, #-8]
   5568c:	ldp	q1, q0, [x2]
   55690:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55694:	mov	x8, x1
   55698:	sub	x1, x29, #0x8
   5569c:	stp	q1, q0, [sp]
   556a0:	ldr	x0, [x0, #3944]
   556a4:	mov	x3, sp
   556a8:	mov	x2, x8
   556ac:	bl	d030 <__gmp_doprnt@plt>
   556b0:	ldp	x29, x30, [sp, #48]
   556b4:	add	sp, sp, #0x40
   556b8:	ret

00000000000556bc <__gmp_doscan@@Base>:
   556bc:	sub	sp, sp, #0x100
   556c0:	stp	x29, x30, [sp, #160]
   556c4:	stp	x28, x27, [sp, #176]
   556c8:	stp	x26, x25, [sp, #192]
   556cc:	stp	x24, x23, [sp, #208]
   556d0:	stp	x22, x21, [sp, #224]
   556d4:	stp	x20, x19, [sp, #240]
   556d8:	str	x1, [sp, #72]
   556dc:	ldp	q1, q0, [x3]
   556e0:	add	x29, sp, #0xa0
   556e4:	mov	x28, x0
   556e8:	mov	x0, x2
   556ec:	mov	x25, x2
   556f0:	stp	q1, q0, [x29, #-48]
   556f4:	bl	bf60 <strlen@plt>
   556f8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   556fc:	ldr	x8, [x8, #3840]
   55700:	add	x0, x0, #0x4
   55704:	str	x0, [sp, #24]
   55708:	ldr	x8, [x8]
   5570c:	blr	x8
   55710:	adrp	x24, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55714:	ldrb	w21, [x25]
   55718:	ldr	x24, [x24, #4016]
   5571c:	mov	x26, x0
   55720:	cbz	w21, 56210 <__gmp_doscan@@Base+0xb54>
   55724:	bl	cae0 <__ctype_b_loc@plt>
   55728:	adrp	x11, 63000 <__gmp_jacobi_table@@Base+0x7599>
   5572c:	mov	w20, wzr
   55730:	mov	w27, wzr
   55734:	add	x11, x11, #0x9d0
   55738:	str	x0, [sp, #80]
   5573c:	str	x26, [sp, #40]
   55740:	stur	x28, [x29, #-72]
   55744:	b	55764 <__gmp_doscan@@Base+0xa8>
   55748:	ldr	w8, [sp, #64]
   5574c:	ldrb	w21, [x25]
   55750:	adrp	x11, 63000 <__gmp_jacobi_table@@Base+0x7599>
   55754:	add	x11, x11, #0x9d0
   55758:	eor	w8, w8, #0x1
   5575c:	add	w27, w27, w8
   55760:	cbz	w21, 56214 <__gmp_doscan@@Base+0xb58>
   55764:	mov	x23, x25
   55768:	sxtw	x19, w20
   5576c:	mov	x25, x23
   55770:	ldr	x8, [sp, #80]
   55774:	and	x9, x21, #0xff
   55778:	add	x23, x25, #0x1
   5577c:	ldr	x8, [x8]
   55780:	ldrh	w9, [x8, x9, lsl #1]
   55784:	tbnz	w9, #13, 5590c <__gmp_doscan@@Base+0x250>
   55788:	and	w9, w21, #0xff
   5578c:	cmp	w9, #0x25
   55790:	b.ne	55940 <__gmp_doscan@@Base+0x284>  // b.any
   55794:	mov	x1, x25
   55798:	mov	w22, wzr
   5579c:	str	xzr, [sp, #64]
   557a0:	mov	x25, x23
   557a4:	add	x9, x25, #0x1
   557a8:	mov	x23, x25
   557ac:	mov	x25, x9
   557b0:	ldurb	w21, [x25, #-1]
   557b4:	cmp	w21, #0x7a
   557b8:	b.hi	55900 <__gmp_doscan@@Base+0x244>  // b.pmore
   557bc:	adr	x9, 557cc <__gmp_doscan@@Base+0x110>
   557c0:	ldrh	w10, [x11, x21, lsl #1]
   557c4:	add	x9, x9, x10, lsl #2
   557c8:	br	x9
   557cc:	add	x25, x25, #0x1
   557d0:	add	x23, x23, #0x1
   557d4:	b	557b0 <__gmp_doscan@@Base+0xf4>
   557d8:	mov	w9, #0x1                   	// #1
   557dc:	str	w9, [sp, #64]
   557e0:	b	557a4 <__gmp_doscan@@Base+0xe8>
   557e4:	ldr	w9, [sp, #68]
   557e8:	and	w9, w9, #0xff
   557ec:	cmp	w9, #0x68
   557f0:	mov	w9, #0x48                  	// #72
   557f4:	str	w9, [sp, #68]
   557f8:	b.eq	557a4 <__gmp_doscan@@Base+0xe8>  // b.none
   557fc:	b	55818 <__gmp_doscan@@Base+0x15c>
   55800:	ldr	w9, [sp, #68]
   55804:	and	w9, w9, #0xff
   55808:	cmp	w9, #0x6c
   5580c:	mov	w9, #0x4c                  	// #76
   55810:	str	w9, [sp, #68]
   55814:	b.eq	557a4 <__gmp_doscan@@Base+0xe8>  // b.none
   55818:	mov	w9, w21
   5581c:	str	w21, [sp, #68]
   55820:	b	557a4 <__gmp_doscan@@Base+0xe8>
   55824:	mov	w22, wzr
   55828:	mov	w12, #0xa                   	// #10
   5582c:	mul	w9, w22, w12
   55830:	add	w9, w9, w21, uxtb
   55834:	ldrb	w21, [x23, #1]!
   55838:	sub	w22, w9, #0x30
   5583c:	ldrh	w10, [x8, x21, lsl #1]
   55840:	tbnz	w10, #11, 5582c <__gmp_doscan@@Base+0x170>
   55844:	b	557a0 <__gmp_doscan@@Base+0xe4>
   55848:	ldr	w8, [sp, #64]
   5584c:	cbnz	w8, 55900 <__gmp_doscan@@Base+0x244>
   55850:	ldursw	x8, [x29, #-24]
   55854:	tbz	w8, #31, 55874 <__gmp_doscan@@Base+0x1b8>
   55858:	add	w9, w8, #0x8
   5585c:	cmn	w8, #0x8
   55860:	stur	w9, [x29, #-24]
   55864:	b.gt	55874 <__gmp_doscan@@Base+0x1b8>
   55868:	ldur	x9, [x29, #-40]
   5586c:	add	x8, x9, x8
   55870:	b	55880 <__gmp_doscan@@Base+0x1c4>
   55874:	ldur	x8, [x29, #-48]
   55878:	add	x9, x8, #0x8
   5587c:	stur	x9, [x29, #-48]
   55880:	ldr	x0, [x8]
   55884:	ldr	w8, [sp, #68]
   55888:	and	w8, w8, #0xff
   5588c:	sub	w9, w8, #0x46
   55890:	cmp	w9, #0x34
   55894:	b.hi	558b8 <__gmp_doscan@@Base+0x1fc>  // b.pmore
   55898:	adrp	x12, 63000 <__gmp_jacobi_table@@Base+0x7599>
   5589c:	add	x12, x12, #0xac6
   558a0:	adr	x8, 558b0 <__gmp_doscan@@Base+0x1f4>
   558a4:	ldrb	w10, [x12, x9]
   558a8:	add	x8, x8, x10, lsl #2
   558ac:	br	x8
   558b0:	str	x19, [x0]
   558b4:	b	55900 <__gmp_doscan@@Base+0x244>
   558b8:	cbnz	w8, 55900 <__gmp_doscan@@Base+0x244>
   558bc:	str	w20, [x0]
   558c0:	b	55900 <__gmp_doscan@@Base+0x244>
   558c4:	mov	x1, x19
   558c8:	bl	c620 <__gmpf_set_si@plt>
   558cc:	b	558f0 <__gmp_doscan@@Base+0x234>
   558d0:	strb	w20, [x0]
   558d4:	b	55900 <__gmp_doscan@@Base+0x244>
   558d8:	mov	w2, #0x1                   	// #1
   558dc:	mov	x1, x19
   558e0:	bl	cb70 <__gmpq_set_si@plt>
   558e4:	b	558f0 <__gmp_doscan@@Base+0x234>
   558e8:	mov	x1, x19
   558ec:	bl	d270 <__gmpz_set_si@plt>
   558f0:	adrp	x11, 63000 <__gmp_jacobi_table@@Base+0x7599>
   558f4:	add	x11, x11, #0x9d0
   558f8:	b	55900 <__gmp_doscan@@Base+0x244>
   558fc:	strh	w20, [x0]
   55900:	ldrb	w21, [x25]
   55904:	cbnz	w21, 55770 <__gmp_doscan@@Base+0xb4>
   55908:	b	56214 <__gmp_doscan@@Base+0xb58>
   5590c:	ldp	x19, x21, [sp, #72]
   55910:	sub	w20, w20, #0x1
   55914:	ldr	x8, [x28, #16]
   55918:	mov	x0, x19
   5591c:	blr	x8
   55920:	ldr	x8, [x21]
   55924:	add	w20, w20, #0x1
   55928:	ldrh	w8, [x8, w0, sxtw #1]
   5592c:	tbnz	w8, #13, 55914 <__gmp_doscan@@Base+0x258>
   55930:	ldr	x8, [x28, #24]
   55934:	mov	x1, x19
   55938:	blr	x8
   5593c:	b	55960 <__gmp_doscan@@Base+0x2a4>
   55940:	mov	x25, x23
   55944:	ldr	x8, [x28, #16]
   55948:	ldr	x0, [sp, #72]
   5594c:	blr	x8
   55950:	cmp	w0, w21, uxtb
   55954:	b.ne	56278 <__gmp_doscan@@Base+0xbbc>  // b.any
   55958:	add	w20, w20, #0x1
   5595c:	mov	x23, x25
   55960:	ldrb	w21, [x23]
   55964:	adrp	x11, 63000 <__gmp_jacobi_table@@Base+0x7599>
   55968:	add	x11, x11, #0x9d0
   5596c:	cbnz	w21, 55768 <__gmp_doscan@@Base+0xac>
   55970:	b	56214 <__gmp_doscan@@Base+0xb58>
   55974:	mov	w23, wzr
   55978:	b	559b4 <__gmp_doscan@@Base+0x2f8>
   5597c:	mov	w23, #0x10                  	// #16
   55980:	b	559b4 <__gmp_doscan@@Base+0x2f8>
   55984:	mov	w23, #0xa                   	// #10
   55988:	b	559b4 <__gmp_doscan@@Base+0x2f8>
   5598c:	mov	x9, x25
   55990:	ldrb	w8, [x9], #1
   55994:	cmp	w8, #0x5e
   55998:	b.ne	55fd8 <__gmp_doscan@@Base+0x91c>  // b.any
   5599c:	ldrb	w8, [x25, #1]
   559a0:	add	x25, x25, #0x2
   559a4:	cmp	w8, #0x5d
   559a8:	b.eq	55ff0 <__gmp_doscan@@Base+0x934>  // b.none
   559ac:	b	55fe4 <__gmp_doscan@@Base+0x928>
   559b0:	mov	w23, #0x8                   	// #8
   559b4:	ldr	w8, [sp, #68]
   559b8:	ldr	x21, [sp, #80]
   559bc:	and	w8, w8, #0xff
   559c0:	sub	w8, w8, #0x46
   559c4:	cmp	w8, #0x14
   559c8:	b.hi	55ffc <__gmp_doscan@@Base+0x940>  // b.pmore
   559cc:	mov	w9, #0x1                   	// #1
   559d0:	lsl	w8, w9, w8
   559d4:	mov	w9, #0x801                 	// #2049
   559d8:	movk	w9, #0x10, lsl #16
   559dc:	tst	w8, w9
   559e0:	b.eq	55ffc <__gmp_doscan@@Base+0x940>  // b.none
   559e4:	sub	w19, w20, #0x2
   559e8:	ldr	x20, [sp, #72]
   559ec:	ldr	x8, [x28, #16]
   559f0:	mov	x0, x20
   559f4:	blr	x8
   559f8:	ldr	x8, [x21]
   559fc:	add	w19, w19, #0x1
   55a00:	ldrh	w8, [x8, w0, sxtw #1]
   55a04:	tbnz	w8, #13, 559ec <__gmp_doscan@@Base+0x330>
   55a08:	ldr	x8, [x28, #24]
   55a0c:	mov	x1, x20
   55a10:	blr	x8
   55a14:	ldr	w8, [sp, #64]
   55a18:	str	w27, [sp, #60]
   55a1c:	cbz	w8, 55a28 <__gmp_doscan@@Base+0x36c>
   55a20:	mov	x21, xzr
   55a24:	b	55a5c <__gmp_doscan@@Base+0x3a0>
   55a28:	ldursw	x8, [x29, #-24]
   55a2c:	tbz	w8, #31, 55a4c <__gmp_doscan@@Base+0x390>
   55a30:	add	w9, w8, #0x8
   55a34:	cmn	w8, #0x8
   55a38:	stur	w9, [x29, #-24]
   55a3c:	b.gt	55a4c <__gmp_doscan@@Base+0x390>
   55a40:	ldur	x9, [x29, #-40]
   55a44:	add	x8, x9, x8
   55a48:	b	55a58 <__gmp_doscan@@Base+0x39c>
   55a4c:	ldur	x8, [x29, #-48]
   55a50:	add	x9, x8, #0x8
   55a54:	stur	x9, [x29, #-48]
   55a58:	ldr	x21, [x8]
   55a5c:	ldr	x8, [x28, #16]
   55a60:	mov	x0, x20
   55a64:	blr	x8
   55a68:	cmn	w0, #0x1
   55a6c:	b.eq	56248 <__gmp_doscan@@Base+0xb8c>  // b.none
   55a70:	str	x21, [sp, #32]
   55a74:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55a78:	ldr	x8, [x8, #3840]
   55a7c:	cmp	w22, #0x0
   55a80:	mov	w9, #0x7ffffffe            	// #2147483646
   55a84:	mov	w28, w0
   55a88:	ldr	x8, [x8]
   55a8c:	csel	w9, w9, w22, eq  // eq = none
   55a90:	mov	w0, #0x200                 	// #512
   55a94:	stur	w9, [x29, #-60]
   55a98:	blr	x8
   55a9c:	ldr	w8, [sp, #68]
   55aa0:	mov	w9, #0x1                   	// #1
   55aa4:	stp	w9, wzr, [sp, #52]
   55aa8:	mov	w9, #0x8                   	// #8
   55aac:	and	w8, w8, #0xff
   55ab0:	cmp	w8, #0x46
   55ab4:	mov	w8, #0xa                   	// #10
   55ab8:	mov	x20, x0
   55abc:	mov	x21, xzr
   55ac0:	mov	w27, #0x200                 	// #512
   55ac4:	csel	w8, w8, w9, eq  // eq = none
   55ac8:	mov	w24, #0x1                   	// #1
   55acc:	str	xzr, [sp]
   55ad0:	str	wzr, [sp, #20]
   55ad4:	stur	w23, [x29, #-56]
   55ad8:	stp	w8, w23, [sp, #12]
   55adc:	cmp	w28, #0x2b
   55ae0:	b.eq	55b2c <__gmp_doscan@@Base+0x470>  // b.none
   55ae4:	cmp	w28, #0x2d
   55ae8:	b.ne	55ce4 <__gmp_doscan@@Base+0x628>  // b.any
   55aec:	cmp	x21, x27
   55af0:	b.cc	55b1c <__gmp_doscan@@Base+0x460>  // b.lo, b.ul, b.last
   55af4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55af8:	ldr	x8, [x8, #3792]
   55afc:	mov	x0, x20
   55b00:	add	x20, x27, #0x200
   55b04:	mov	x1, x27
   55b08:	ldr	x8, [x8]
   55b0c:	mov	x2, x20
   55b10:	blr	x8
   55b14:	mov	x27, x20
   55b18:	mov	x20, x0
   55b1c:	add	x8, x21, #0x1
   55b20:	mov	w9, #0x2d                  	// #45
   55b24:	strb	w9, [x20, x21]
   55b28:	mov	x21, x8
   55b2c:	ldur	w8, [x29, #-60]
   55b30:	add	w23, w24, #0x1
   55b34:	cmp	w24, w8
   55b38:	b.ge	55f68 <__gmp_doscan@@Base+0x8ac>  // b.tcont
   55b3c:	ldur	x8, [x29, #-72]
   55b40:	ldr	x24, [sp, #72]
   55b44:	ldr	x8, [x8, #16]
   55b48:	mov	x0, x24
   55b4c:	blr	x8
   55b50:	mov	w28, w0
   55b54:	ldur	w8, [x29, #-56]
   55b58:	cbz	w8, 55cf4 <__gmp_doscan@@Base+0x638>
   55b5c:	mov	w10, wzr
   55b60:	ldr	x8, [sp, #80]
   55b64:	ldur	w9, [x29, #-56]
   55b68:	ldr	x8, [x8]
   55b6c:	cmp	w9, #0x10
   55b70:	ldrh	w8, [x8, w28, sxtw #1]
   55b74:	b.ne	55b80 <__gmp_doscan@@Base+0x4c4>  // b.any
   55b78:	tbnz	w8, #12, 55b9c <__gmp_doscan@@Base+0x4e0>
   55b7c:	b	55c04 <__gmp_doscan@@Base+0x548>
   55b80:	tbz	w8, #11, 55c04 <__gmp_doscan@@Base+0x548>
   55b84:	ldur	w8, [x29, #-56]
   55b88:	cmp	w8, #0x8
   55b8c:	b.ne	55b9c <__gmp_doscan@@Base+0x4e0>  // b.any
   55b90:	orr	w8, w28, #0x1
   55b94:	cmp	w8, #0x39
   55b98:	b.eq	55c04 <__gmp_doscan@@Base+0x548>  // b.none
   55b9c:	cmp	x21, x27
   55ba0:	b.cc	55bcc <__gmp_doscan@@Base+0x510>  // b.lo, b.ul, b.last
   55ba4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55ba8:	ldr	x8, [x8, #3792]
   55bac:	mov	x0, x20
   55bb0:	add	x20, x27, #0x200
   55bb4:	mov	x1, x27
   55bb8:	ldr	x8, [x8]
   55bbc:	mov	x2, x20
   55bc0:	blr	x8
   55bc4:	mov	x27, x20
   55bc8:	mov	x20, x0
   55bcc:	ldur	w8, [x29, #-60]
   55bd0:	add	x22, x21, #0x1
   55bd4:	strb	w28, [x20, x21]
   55bd8:	cmp	w23, w8
   55bdc:	add	w23, w23, #0x1
   55be0:	b.ge	55f4c <__gmp_doscan@@Base+0x890>  // b.tcont
   55be4:	ldur	x8, [x29, #-72]
   55be8:	mov	x0, x24
   55bec:	ldr	x8, [x8, #16]
   55bf0:	blr	x8
   55bf4:	mov	w28, w0
   55bf8:	mov	w10, #0x1                   	// #1
   55bfc:	mov	x21, x22
   55c00:	b	55b60 <__gmp_doscan@@Base+0x4a4>
   55c04:	ldr	w8, [sp, #52]
   55c08:	cbz	w8, 55f44 <__gmp_doscan@@Base+0x888>
   55c0c:	ldr	w8, [sp, #68]
   55c10:	str	w10, [sp, #48]
   55c14:	and	w8, w8, #0xff
   55c18:	cmp	w8, #0x46
   55c1c:	b.ne	55dcc <__gmp_doscan@@Base+0x710>  // b.any
   55c20:	ldr	w9, [sp, #56]
   55c24:	cbnz	w9, 55dcc <__gmp_doscan@@Base+0x710>
   55c28:	mov	w0, #0x10000               	// #65536
   55c2c:	bl	c400 <nl_langinfo@plt>
   55c30:	ldrb	w8, [x0]
   55c34:	cmp	w28, w8
   55c38:	b.ne	55dd4 <__gmp_doscan@@Base+0x718>  // b.any
   55c3c:	mov	x22, xzr
   55c40:	add	x26, x0, #0x1
   55c44:	add	x8, x21, x22
   55c48:	cmp	x8, x27
   55c4c:	b.cc	55c78 <__gmp_doscan@@Base+0x5bc>  // b.lo, b.ul, b.last
   55c50:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55c54:	ldr	x8, [x8, #3792]
   55c58:	mov	x0, x20
   55c5c:	add	x20, x27, #0x200
   55c60:	mov	x1, x27
   55c64:	ldr	x8, [x8]
   55c68:	mov	x2, x20
   55c6c:	blr	x8
   55c70:	mov	x27, x20
   55c74:	mov	x20, x0
   55c78:	ldur	w9, [x29, #-60]
   55c7c:	add	x8, x20, x21
   55c80:	add	w24, w23, #0x1
   55c84:	strb	w28, [x8, x22]
   55c88:	cmp	w23, w9
   55c8c:	b.ge	55f2c <__gmp_doscan@@Base+0x870>  // b.tcont
   55c90:	ldur	x8, [x29, #-72]
   55c94:	ldr	x0, [sp, #72]
   55c98:	ldr	x8, [x8, #16]
   55c9c:	blr	x8
   55ca0:	ldrb	w8, [x26, x22]
   55ca4:	mov	w28, w0
   55ca8:	cbz	w8, 55cc0 <__gmp_doscan@@Base+0x604>
   55cac:	cmp	w28, w8
   55cb0:	add	x22, x22, #0x1
   55cb4:	mov	w23, w24
   55cb8:	b.eq	55c44 <__gmp_doscan@@Base+0x588>  // b.none
   55cbc:	b	55f60 <__gmp_doscan@@Base+0x8a4>
   55cc0:	ldr	x26, [sp, #40]
   55cc4:	ldr	x24, [sp, #72]
   55cc8:	ldr	w10, [sp, #48]
   55ccc:	add	x8, x21, x22
   55cd0:	add	w23, w23, #0x1
   55cd4:	add	x21, x8, #0x1
   55cd8:	mov	w8, #0x1                   	// #1
   55cdc:	str	w8, [sp, #56]
   55ce0:	b	55b60 <__gmp_doscan@@Base+0x4a4>
   55ce4:	mov	w23, w24
   55ce8:	ldr	x24, [sp, #72]
   55cec:	ldur	w8, [x29, #-56]
   55cf0:	cbnz	w8, 55b5c <__gmp_doscan@@Base+0x4a0>
   55cf4:	cmp	w28, #0x30
   55cf8:	b.ne	55d8c <__gmp_doscan@@Base+0x6d0>  // b.any
   55cfc:	cmp	x21, x27
   55d00:	b.cc	55d2c <__gmp_doscan@@Base+0x670>  // b.lo, b.ul, b.last
   55d04:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55d08:	ldr	x8, [x8, #3792]
   55d0c:	mov	x0, x20
   55d10:	add	x20, x27, #0x200
   55d14:	mov	x1, x27
   55d18:	ldr	x8, [x8]
   55d1c:	mov	x2, x20
   55d20:	blr	x8
   55d24:	mov	x27, x20
   55d28:	mov	x20, x0
   55d2c:	ldur	w8, [x29, #-60]
   55d30:	mov	w28, #0x30                  	// #48
   55d34:	add	x22, x21, #0x1
   55d38:	mov	x26, x20
   55d3c:	cmp	w23, w8
   55d40:	strb	w28, [x20, x21]
   55d44:	add	w20, w23, #0x1
   55d48:	b.ge	560bc <__gmp_doscan@@Base+0xa00>  // b.tcont
   55d4c:	ldur	x8, [x29, #-72]
   55d50:	mov	x0, x24
   55d54:	ldr	x8, [x8, #16]
   55d58:	blr	x8
   55d5c:	orr	w8, w0, #0x20
   55d60:	mov	w28, w0
   55d64:	cmp	w8, #0x78
   55d68:	b.ne	55d9c <__gmp_doscan@@Base+0x6e0>  // b.any
   55d6c:	ldr	w8, [sp, #68]
   55d70:	and	w8, w8, #0xff
   55d74:	cmp	w8, #0x46
   55d78:	b.ne	55dbc <__gmp_doscan@@Base+0x700>  // b.any
   55d7c:	mov	w8, #0x1                   	// #1
   55d80:	str	w8, [sp, #20]
   55d84:	mov	x20, x26
   55d88:	b	55e30 <__gmp_doscan@@Base+0x774>
   55d8c:	mov	w8, #0xa                   	// #10
   55d90:	mov	w10, wzr
   55d94:	stur	w8, [x29, #-56]
   55d98:	b	55b60 <__gmp_doscan@@Base+0x4a4>
   55d9c:	ldr	w8, [sp, #12]
   55da0:	mov	w23, w20
   55da4:	mov	x20, x26
   55da8:	ldr	x26, [sp, #40]
   55dac:	mov	w10, #0x1                   	// #1
   55db0:	mov	x21, x22
   55db4:	stur	w8, [x29, #-56]
   55db8:	b	55b60 <__gmp_doscan@@Base+0x4a4>
   55dbc:	cmp	x22, x27
   55dc0:	b.cs	55dfc <__gmp_doscan@@Base+0x740>  // b.hs, b.nlast
   55dc4:	mov	x20, x26
   55dc8:	b	55e24 <__gmp_doscan@@Base+0x768>
   55dcc:	cmp	w8, #0x46
   55dd0:	b.ne	55e88 <__gmp_doscan@@Base+0x7cc>  // b.any
   55dd4:	ldr	w9, [sp, #20]
   55dd8:	orr	w8, w28, #0x20
   55ddc:	cbz	w9, 55e6c <__gmp_doscan@@Base+0x7b0>
   55de0:	cmp	w8, #0x70
   55de4:	b.ne	55e6c <__gmp_doscan@@Base+0x7b0>  // b.any
   55de8:	ldr	w10, [sp, #48]
   55dec:	mov	w8, #0xa                   	// #10
   55df0:	stur	w8, [x29, #-56]
   55df4:	str	x21, [sp]
   55df8:	b	55e80 <__gmp_doscan@@Base+0x7c4>
   55dfc:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55e00:	ldr	x8, [x8, #3792]
   55e04:	add	x20, x27, #0x200
   55e08:	mov	x0, x26
   55e0c:	mov	x1, x27
   55e10:	ldr	x8, [x8]
   55e14:	mov	x2, x20
   55e18:	blr	x8
   55e1c:	mov	x27, x20
   55e20:	mov	x20, x0
   55e24:	add	x8, x21, #0x2
   55e28:	strb	w28, [x20, x22]
   55e2c:	mov	x22, x8
   55e30:	ldur	w8, [x29, #-60]
   55e34:	ldr	x26, [sp, #40]
   55e38:	add	w23, w23, #0x2
   55e3c:	cmp	w23, w8
   55e40:	b.gt	55f6c <__gmp_doscan@@Base+0x8b0>
   55e44:	ldur	x8, [x29, #-72]
   55e48:	mov	x0, x24
   55e4c:	ldr	x8, [x8, #16]
   55e50:	blr	x8
   55e54:	mov	w8, #0x10                  	// #16
   55e58:	mov	w28, w0
   55e5c:	mov	w10, wzr
   55e60:	stur	w8, [x29, #-56]
   55e64:	mov	x21, x22
   55e68:	b	55b60 <__gmp_doscan@@Base+0x4a4>
   55e6c:	ldr	w10, [sp, #48]
   55e70:	cmp	w8, #0x65
   55e74:	b.ne	55f44 <__gmp_doscan@@Base+0x888>  // b.any
   55e78:	ldr	w8, [sp, #20]
   55e7c:	cbnz	w8, 55f44 <__gmp_doscan@@Base+0x888>
   55e80:	cbnz	w10, 55eb4 <__gmp_doscan@@Base+0x7f8>
   55e84:	b	55f6c <__gmp_doscan@@Base+0x8b0>
   55e88:	ldr	w8, [sp, #68]
   55e8c:	and	w8, w8, #0xff
   55e90:	cmp	w8, #0x51
   55e94:	b.ne	55f40 <__gmp_doscan@@Base+0x884>  // b.any
   55e98:	ldr	w10, [sp, #48]
   55e9c:	cmp	w28, #0x2f
   55ea0:	b.ne	55f44 <__gmp_doscan@@Base+0x888>  // b.any
   55ea4:	cbz	w10, 56204 <__gmp_doscan@@Base+0xb48>
   55ea8:	ldr	w8, [sp, #16]
   55eac:	mov	w10, wzr
   55eb0:	stur	w8, [x29, #-56]
   55eb4:	mov	x0, x20
   55eb8:	cmp	x21, x27
   55ebc:	str	w10, [sp, #48]
   55ec0:	b.cc	55ee4 <__gmp_doscan@@Base+0x828>  // b.lo, b.ul, b.last
   55ec4:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55ec8:	ldr	x8, [x8, #3792]
   55ecc:	add	x20, x27, #0x200
   55ed0:	mov	x1, x27
   55ed4:	mov	x2, x20
   55ed8:	ldr	x8, [x8]
   55edc:	blr	x8
   55ee0:	mov	x27, x20
   55ee4:	ldur	w8, [x29, #-60]
   55ee8:	add	x20, x21, #0x1
   55eec:	mov	x22, x0
   55ef0:	add	w24, w23, #0x1
   55ef4:	cmp	w23, w8
   55ef8:	strb	w28, [x0, x21]
   55efc:	b.ge	561f0 <__gmp_doscan@@Base+0xb34>  // b.tcont
   55f00:	ldur	x8, [x29, #-72]
   55f04:	ldr	x0, [sp, #72]
   55f08:	ldr	x8, [x8, #16]
   55f0c:	blr	x8
   55f10:	mov	w28, w0
   55f14:	str	wzr, [sp, #52]
   55f18:	mov	x21, x20
   55f1c:	mov	x20, x22
   55f20:	cmp	w28, #0x2b
   55f24:	b.ne	55ae4 <__gmp_doscan@@Base+0x428>  // b.any
   55f28:	b	55b2c <__gmp_doscan@@Base+0x470>
   55f2c:	mov	w23, w24
   55f30:	ldr	x26, [sp, #40]
   55f34:	ldr	x24, [sp, #72]
   55f38:	add	x8, x21, x22
   55f3c:	add	x21, x8, #0x1
   55f40:	ldr	w10, [sp, #48]
   55f44:	cbz	w10, 55f6c <__gmp_doscan@@Base+0x8b0>
   55f48:	mov	x22, x21
   55f4c:	ldr	w8, [sp, #64]
   55f50:	cbz	w8, 560d0 <__gmp_doscan@@Base+0xa14>
   55f54:	mov	x21, x20
   55f58:	mov	w20, wzr
   55f5c:	b	55f74 <__gmp_doscan@@Base+0x8b8>
   55f60:	mov	w23, w24
   55f64:	ldr	x26, [sp, #40]
   55f68:	ldr	x24, [sp, #72]
   55f6c:	mov	x21, x20
   55f70:	mov	w20, #0x1                   	// #1
   55f74:	ldur	w8, [x29, #-60]
   55f78:	add	w8, w8, #0x1
   55f7c:	cmp	w23, w8
   55f80:	b.eq	55f98 <__gmp_doscan@@Base+0x8dc>  // b.none
   55f84:	ldur	x8, [x29, #-72]
   55f88:	mov	w0, w28
   55f8c:	mov	x1, x24
   55f90:	ldr	x8, [x8, #24]
   55f94:	blr	x8
   55f98:	adrp	x24, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   55f9c:	ldr	x24, [x24, #4016]
   55fa0:	mov	x0, x21
   55fa4:	mov	x1, x27
   55fa8:	ldr	x8, [x24]
   55fac:	blr	x8
   55fb0:	cbnz	w20, 56268 <__gmp_doscan@@Base+0xbac>
   55fb4:	ldr	w27, [sp, #60]
   55fb8:	ldur	x28, [x29, #-72]
   55fbc:	sub	w8, w23, #0x1
   55fc0:	cmn	w23, #0x1
   55fc4:	stur	w8, [x29, #-52]
   55fc8:	b.eq	5625c <__gmp_doscan@@Base+0xba0>  // b.none
   55fcc:	cbz	w23, 56214 <__gmp_doscan@@Base+0xb58>
   55fd0:	add	w20, w23, w19
   55fd4:	b	55748 <__gmp_doscan@@Base+0x8c>
   55fd8:	mov	x25, x9
   55fdc:	cmp	w8, #0x5d
   55fe0:	b.eq	55ff0 <__gmp_doscan@@Base+0x934>  // b.none
   55fe4:	cmp	w8, #0x5d
   55fe8:	b.eq	55ffc <__gmp_doscan@@Base+0x940>  // b.none
   55fec:	cbz	w8, 56214 <__gmp_doscan@@Base+0xb58>
   55ff0:	ldrb	w8, [x25], #1
   55ff4:	cmp	w8, #0x5d
   55ff8:	b.ne	55fec <__gmp_doscan@@Base+0x930>  // b.any
   55ffc:	sub	x19, x25, x1
   56000:	mov	x0, x26
   56004:	mov	x2, x19
   56008:	bl	bed0 <memcpy@plt>
   5600c:	add	x8, x26, x19
   56010:	mov	w9, #0x6e25                	// #28197
   56014:	strh	w9, [x8]
   56018:	strb	wzr, [x8, #2]
   5601c:	mov	w8, #0xffffffff            	// #-1
   56020:	stur	w8, [x29, #-52]
   56024:	ldr	w8, [sp, #64]
   56028:	cbz	w8, 56048 <__gmp_doscan@@Base+0x98c>
   5602c:	ldr	x8, [x28]
   56030:	ldr	x0, [sp, #72]
   56034:	sub	x2, x29, #0x34
   56038:	mov	x1, x26
   5603c:	mov	x3, xzr
   56040:	blr	x8
   56044:	b	56094 <__gmp_doscan@@Base+0x9d8>
   56048:	ldursw	x8, [x29, #-24]
   5604c:	tbz	w8, #31, 5606c <__gmp_doscan@@Base+0x9b0>
   56050:	add	w9, w8, #0x8
   56054:	cmn	w8, #0x8
   56058:	stur	w9, [x29, #-24]
   5605c:	b.gt	5606c <__gmp_doscan@@Base+0x9b0>
   56060:	ldur	x9, [x29, #-40]
   56064:	add	x8, x9, x8
   56068:	b	56078 <__gmp_doscan@@Base+0x9bc>
   5606c:	ldur	x8, [x29, #-48]
   56070:	add	x9, x8, #0x8
   56074:	stur	x9, [x29, #-48]
   56078:	ldr	x2, [x8]
   5607c:	ldr	x8, [x28]
   56080:	ldr	x0, [sp, #72]
   56084:	sub	x3, x29, #0x34
   56088:	mov	x1, x26
   5608c:	blr	x8
   56090:	cbz	w0, 56214 <__gmp_doscan@@Base+0xb58>
   56094:	cmn	w0, #0x1
   56098:	b.eq	5625c <__gmp_doscan@@Base+0xba0>  // b.none
   5609c:	ldur	w1, [x29, #-52]
   560a0:	cmn	w1, #0x1
   560a4:	b.eq	56214 <__gmp_doscan@@Base+0xb58>  // b.none
   560a8:	ldr	x8, [x28, #8]
   560ac:	ldr	x0, [sp, #72]
   560b0:	add	w20, w1, w20
   560b4:	blr	x8
   560b8:	b	55748 <__gmp_doscan@@Base+0x8c>
   560bc:	mov	w23, w20
   560c0:	mov	x20, x26
   560c4:	ldr	x26, [sp, #40]
   560c8:	ldr	w8, [sp, #64]
   560cc:	cbnz	w8, 55f54 <__gmp_doscan@@Base+0x898>
   560d0:	cmp	x22, x27
   560d4:	b.cc	56100 <__gmp_doscan@@Base+0xa44>  // b.lo, b.ul, b.last
   560d8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   560dc:	ldr	x8, [x8, #3792]
   560e0:	mov	x0, x20
   560e4:	add	x20, x27, #0x200
   560e8:	mov	x1, x27
   560ec:	ldr	x8, [x8]
   560f0:	mov	x2, x20
   560f4:	blr	x8
   560f8:	mov	x27, x20
   560fc:	mov	x20, x0
   56100:	ldr	w8, [sp, #68]
   56104:	strb	wzr, [x20, x22]
   56108:	and	w8, w8, #0xff
   5610c:	cmp	w8, #0x5a
   56110:	b.eq	56184 <__gmp_doscan@@Base+0xac8>  // b.none
   56114:	cmp	w8, #0x51
   56118:	b.eq	5619c <__gmp_doscan@@Base+0xae0>  // b.none
   5611c:	cmp	w8, #0x46
   56120:	b.ne	55f54 <__gmp_doscan@@Base+0x898>  // b.any
   56124:	ldr	x8, [sp]
   56128:	cbz	x8, 561b4 <__gmp_doscan@@Base+0xaf8>
   5612c:	mov	x1, x20
   56130:	add	x20, x20, x8
   56134:	ldr	w8, [sp, #20]
   56138:	ldr	x22, [sp, #32]
   5613c:	mov	w9, #0x10                  	// #16
   56140:	strb	wzr, [x20], #1
   56144:	cmp	w8, #0x0
   56148:	mov	w8, #0xa                   	// #10
   5614c:	csel	w2, w8, w9, eq  // eq = none
   56150:	mov	x0, x22
   56154:	mov	x21, x1
   56158:	bl	c1c0 <__gmpf_set_str@plt>
   5615c:	sub	x1, x29, #0x8
   56160:	mov	w2, #0xa                   	// #10
   56164:	mov	x0, x20
   56168:	bl	cb60 <strtol@plt>
   5616c:	mov	x2, x0
   56170:	tbnz	x0, #63, 561dc <__gmp_doscan@@Base+0xb20>
   56174:	mov	x0, x22
   56178:	mov	x1, x22
   5617c:	bl	cd90 <__gmpf_mul_2exp@plt>
   56180:	b	55f58 <__gmp_doscan@@Base+0x89c>
   56184:	ldr	x0, [sp, #32]
   56188:	ldr	w2, [sp, #16]
   5618c:	mov	x1, x20
   56190:	mov	x21, x20
   56194:	bl	c0d0 <__gmpz_set_str@plt>
   56198:	b	55f58 <__gmp_doscan@@Base+0x89c>
   5619c:	ldr	x0, [sp, #32]
   561a0:	ldr	w2, [sp, #16]
   561a4:	mov	x1, x20
   561a8:	mov	x21, x20
   561ac:	bl	bfe0 <__gmpq_set_str@plt>
   561b0:	b	55f58 <__gmp_doscan@@Base+0x89c>
   561b4:	ldr	w8, [sp, #20]
   561b8:	ldr	x0, [sp, #32]
   561bc:	mov	w9, #0x10                  	// #16
   561c0:	mov	x1, x20
   561c4:	cmp	w8, #0x0
   561c8:	mov	w8, #0xa                   	// #10
   561cc:	csel	w2, w8, w9, eq  // eq = none
   561d0:	mov	x21, x20
   561d4:	bl	c1c0 <__gmpf_set_str@plt>
   561d8:	b	55f58 <__gmp_doscan@@Base+0x89c>
   561dc:	neg	x2, x2
   561e0:	mov	x0, x22
   561e4:	mov	x1, x22
   561e8:	bl	d4c0 <__gmpf_div_2exp@plt>
   561ec:	b	55f58 <__gmp_doscan@@Base+0x89c>
   561f0:	mov	w23, w24
   561f4:	ldr	x24, [sp, #72]
   561f8:	mov	x21, x20
   561fc:	mov	x20, x22
   56200:	b	55f40 <__gmp_doscan@@Base+0x884>
   56204:	mov	x21, x20
   56208:	mov	w28, #0x2f                  	// #47
   5620c:	b	55f70 <__gmp_doscan@@Base+0x8b4>
   56210:	mov	w27, wzr
   56214:	ldr	x8, [x24]
   56218:	ldr	x1, [sp, #24]
   5621c:	mov	x0, x26
   56220:	blr	x8
   56224:	mov	w0, w27
   56228:	ldp	x20, x19, [sp, #240]
   5622c:	ldp	x22, x21, [sp, #224]
   56230:	ldp	x24, x23, [sp, #208]
   56234:	ldp	x26, x25, [sp, #192]
   56238:	ldp	x28, x27, [sp, #176]
   5623c:	ldp	x29, x30, [sp, #160]
   56240:	add	sp, sp, #0x100
   56244:	ret
   56248:	mov	w8, #0xfffffffe            	// #-2
   5624c:	stur	w8, [x29, #-52]
   56250:	adrp	x24, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56254:	ldr	x24, [x24, #4016]
   56258:	ldr	w27, [sp, #60]
   5625c:	cbnz	w27, 56214 <__gmp_doscan@@Base+0xb58>
   56260:	mov	w27, #0xffffffff            	// #-1
   56264:	b	56214 <__gmp_doscan@@Base+0xb58>
   56268:	ldr	w27, [sp, #60]
   5626c:	mov	w8, #0xffffffff            	// #-1
   56270:	stur	w8, [x29, #-52]
   56274:	b	56214 <__gmp_doscan@@Base+0xb58>
   56278:	ldr	x8, [x28, #24]
   5627c:	ldr	x1, [sp, #72]
   56280:	mov	w19, w0
   56284:	blr	x8
   56288:	cbnz	w27, 56214 <__gmp_doscan@@Base+0xb58>
   5628c:	cmn	w19, #0x1
   56290:	b.eq	56260 <__gmp_doscan@@Base+0xba4>  // b.none
   56294:	b	56214 <__gmp_doscan@@Base+0xb58>

0000000000056298 <__gmp_fscanf@@Base>:
   56298:	sub	sp, sp, #0x100
   5629c:	stp	x29, x30, [sp, #240]
   562a0:	add	x29, sp, #0xf0
   562a4:	mov	x9, #0xffffffffffffffd0    	// #-48
   562a8:	mov	x10, sp
   562ac:	sub	x11, x29, #0x70
   562b0:	movk	x9, #0xff80, lsl #32
   562b4:	add	x12, x29, #0x10
   562b8:	add	x10, x10, #0x80
   562bc:	add	x11, x11, #0x30
   562c0:	stp	x10, x9, [x29, #-16]
   562c4:	stp	x12, x11, [x29, #-32]
   562c8:	stp	x2, x3, [x29, #-112]
   562cc:	stp	x4, x5, [x29, #-96]
   562d0:	stp	x6, x7, [x29, #-80]
   562d4:	stp	q1, q2, [sp, #16]
   562d8:	str	q0, [sp]
   562dc:	ldp	q0, q1, [x29, #-32]
   562e0:	mov	x8, x1
   562e4:	mov	x1, x0
   562e8:	stp	q3, q4, [sp, #48]
   562ec:	stp	q5, q6, [sp, #80]
   562f0:	str	q7, [sp, #112]
   562f4:	stp	q0, q1, [x29, #-64]
   562f8:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   562fc:	ldr	x0, [x0, #4064]
   56300:	sub	x3, x29, #0x40
   56304:	mov	x2, x8
   56308:	bl	c0f0 <__gmp_doscan@plt>
   5630c:	ldp	x29, x30, [sp, #240]
   56310:	add	sp, sp, #0x100
   56314:	ret
   56318:	ret

000000000005631c <__gmp_scanf@@Base>:
   5631c:	sub	sp, sp, #0x120
   56320:	stp	x29, x30, [sp, #256]
   56324:	add	x29, sp, #0x100
   56328:	mov	x9, #0xffffffffffffffc8    	// #-56
   5632c:	mov	x10, sp
   56330:	sub	x11, x29, #0x78
   56334:	str	x28, [sp, #272]
   56338:	stp	x1, x2, [x29, #-120]
   5633c:	stp	x3, x4, [x29, #-104]
   56340:	stp	x5, x6, [x29, #-88]
   56344:	stur	x7, [x29, #-72]
   56348:	stp	q0, q1, [sp]
   5634c:	stp	q2, q3, [sp, #32]
   56350:	stp	q4, q5, [sp, #64]
   56354:	movk	x9, #0xff80, lsl #32
   56358:	add	x12, x29, #0x20
   5635c:	adrp	x13, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56360:	add	x10, x10, #0x80
   56364:	add	x11, x11, #0x38
   56368:	ldr	x13, [x13, #3888]
   5636c:	stp	x10, x9, [x29, #-16]
   56370:	stp	x12, x11, [x29, #-32]
   56374:	ldp	q0, q1, [x29, #-32]
   56378:	mov	x8, x0
   5637c:	stp	q6, q7, [sp, #96]
   56380:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56384:	stp	q0, q1, [x29, #-64]
   56388:	ldr	x1, [x13]
   5638c:	ldr	x0, [x0, #4064]
   56390:	sub	x3, x29, #0x40
   56394:	mov	x2, x8
   56398:	bl	c0f0 <__gmp_doscan@plt>
   5639c:	ldr	x28, [sp, #272]
   563a0:	ldp	x29, x30, [sp, #256]
   563a4:	add	sp, sp, #0x120
   563a8:	ret

00000000000563ac <__gmp_sscanf@@Base>:
   563ac:	sub	sp, sp, #0x120
   563b0:	stp	x29, x30, [sp, #256]
   563b4:	add	x29, sp, #0x100
   563b8:	mov	x10, #0xffffffffffffffd0    	// #-48
   563bc:	mov	x11, sp
   563c0:	add	x12, sp, #0x80
   563c4:	movk	x10, #0xff80, lsl #32
   563c8:	add	x13, x29, #0x20
   563cc:	add	x11, x11, #0x80
   563d0:	add	x12, x12, #0x30
   563d4:	sub	x9, x29, #0x28
   563d8:	stp	x11, x10, [x29, #-24]
   563dc:	stp	x13, x12, [x29, #-40]
   563e0:	stp	q1, q2, [sp, #16]
   563e4:	str	q0, [sp]
   563e8:	ldp	q0, q1, [x9]
   563ec:	str	x28, [sp, #272]
   563f0:	stp	x2, x3, [sp, #128]
   563f4:	stp	x4, x5, [sp, #144]
   563f8:	stp	x6, x7, [sp, #160]
   563fc:	stp	q3, q4, [sp, #48]
   56400:	stp	q5, q6, [sp, #80]
   56404:	str	q7, [sp, #112]
   56408:	stur	x0, [x29, #-8]
   5640c:	stp	q0, q1, [x29, #-80]
   56410:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56414:	ldr	x0, [x0, #3984]
   56418:	mov	x8, x1
   5641c:	sub	x1, x29, #0x8
   56420:	sub	x3, x29, #0x50
   56424:	mov	x2, x8
   56428:	bl	c0f0 <__gmp_doscan@plt>
   5642c:	ldr	x28, [sp, #272]
   56430:	ldp	x29, x30, [sp, #256]
   56434:	add	sp, sp, #0x120
   56438:	ret
   5643c:	sub	sp, sp, #0xe0
   56440:	stp	x29, x30, [sp, #208]
   56444:	add	x29, sp, #0xd0
   56448:	mov	x8, #0xffffffffffffffd0    	// #-48
   5644c:	mov	x10, sp
   56450:	movk	x8, #0xff80, lsl #32
   56454:	sub	x11, x29, #0x50
   56458:	add	x10, x10, #0x80
   5645c:	add	x12, x29, #0x10
   56460:	mov	w9, #0xffffffd0            	// #-48
   56464:	add	x11, x11, #0x30
   56468:	stp	x10, x8, [x29, #-16]
   5646c:	mov	w8, #0xffffffd0            	// #-48
   56470:	stp	x2, x3, [x29, #-80]
   56474:	stp	x4, x5, [x29, #-64]
   56478:	stp	x6, x7, [x29, #-48]
   5647c:	stp	q1, q2, [sp, #16]
   56480:	stp	q3, q4, [sp, #48]
   56484:	str	q0, [sp]
   56488:	stp	q5, q6, [sp, #80]
   5648c:	str	q7, [sp, #112]
   56490:	stp	x12, x11, [x29, #-32]
   56494:	tbz	w9, #31, 564c0 <__gmp_sscanf@@Base+0x114>
   56498:	add	w8, w9, #0x8
   5649c:	cmn	w9, #0x8
   564a0:	stur	w8, [x29, #-8]
   564a4:	b.gt	564c0 <__gmp_sscanf@@Base+0x114>
   564a8:	ldur	x9, [x29, #-24]
   564ac:	mov	x10, #0xffffffffffffffd0    	// #-48
   564b0:	add	x9, x9, x10
   564b4:	ldr	x2, [x9]
   564b8:	tbz	w8, #31, 564f0 <__gmp_sscanf@@Base+0x144>
   564bc:	b	564d4 <__gmp_sscanf@@Base+0x128>
   564c0:	ldur	x9, [x29, #-32]
   564c4:	add	x10, x9, #0x8
   564c8:	stur	x10, [x29, #-32]
   564cc:	ldr	x2, [x9]
   564d0:	tbz	w8, #31, 564f0 <__gmp_sscanf@@Base+0x144>
   564d4:	add	w9, w8, #0x8
   564d8:	cmn	w8, #0x8
   564dc:	stur	w9, [x29, #-8]
   564e0:	b.gt	564f0 <__gmp_sscanf@@Base+0x144>
   564e4:	ldur	x9, [x29, #-24]
   564e8:	add	x8, x9, w8, sxtw
   564ec:	b	564fc <__gmp_sscanf@@Base+0x150>
   564f0:	ldur	x8, [x29, #-32]
   564f4:	add	x9, x8, #0x8
   564f8:	stur	x9, [x29, #-32]
   564fc:	ldr	x3, [x8]
   56500:	ldr	x0, [x0]
   56504:	bl	d100 <__isoc99_sscanf@plt>
   56508:	ldp	x29, x30, [sp, #208]
   5650c:	add	sp, sp, #0xe0
   56510:	ret
   56514:	ldr	x8, [x0]
   56518:	add	x8, x8, w1, sxtw
   5651c:	str	x8, [x0]
   56520:	ret
   56524:	ldr	x9, [x0]
   56528:	mov	x8, x0
   5652c:	ldrb	w0, [x9]
   56530:	cbz	w0, 56540 <__gmp_sscanf@@Base+0x194>
   56534:	add	x9, x9, #0x1
   56538:	str	x9, [x8]
   5653c:	ret
   56540:	mov	w0, #0xffffffff            	// #-1
   56544:	ret
   56548:	cmn	w0, #0x1
   5654c:	b.eq	5655c <__gmp_sscanf@@Base+0x1b0>  // b.none
   56550:	ldr	x8, [x1]
   56554:	sub	x8, x8, #0x1
   56558:	str	x8, [x1]
   5655c:	ret

0000000000056560 <__gmp_vfscanf@@Base>:
   56560:	sub	sp, sp, #0x30
   56564:	stp	x29, x30, [sp, #32]
   56568:	ldp	q1, q0, [x2]
   5656c:	mov	x8, x1
   56570:	mov	x1, x0
   56574:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56578:	stp	q1, q0, [sp]
   5657c:	ldr	x0, [x0, #4064]
   56580:	mov	x3, sp
   56584:	mov	x2, x8
   56588:	add	x29, sp, #0x20
   5658c:	bl	c0f0 <__gmp_doscan@plt>
   56590:	ldp	x29, x30, [sp, #32]
   56594:	add	sp, sp, #0x30
   56598:	ret

000000000005659c <__gmp_vscanf@@Base>:
   5659c:	sub	sp, sp, #0x30
   565a0:	stp	x29, x30, [sp, #32]
   565a4:	ldp	q0, q1, [x1]
   565a8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   565ac:	ldr	x8, [x8, #3888]
   565b0:	mov	x2, x0
   565b4:	stp	q0, q1, [sp]
   565b8:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   565bc:	ldr	x1, [x8]
   565c0:	ldr	x0, [x0, #4064]
   565c4:	mov	x3, sp
   565c8:	add	x29, sp, #0x20
   565cc:	bl	c0f0 <__gmp_doscan@plt>
   565d0:	ldp	x29, x30, [sp, #32]
   565d4:	add	sp, sp, #0x30
   565d8:	ret

00000000000565dc <__gmp_vsscanf@@Base>:
   565dc:	sub	sp, sp, #0x40
   565e0:	stp	x29, x30, [sp, #48]
   565e4:	add	x29, sp, #0x30
   565e8:	stur	x0, [x29, #-8]
   565ec:	ldp	q1, q0, [x2]
   565f0:	adrp	x0, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   565f4:	mov	x8, x1
   565f8:	sub	x1, x29, #0x8
   565fc:	stp	q1, q0, [sp]
   56600:	ldr	x0, [x0, #3984]
   56604:	mov	x3, sp
   56608:	mov	x2, x8
   5660c:	bl	c0f0 <__gmp_doscan@plt>
   56610:	ldp	x29, x30, [sp, #48]
   56614:	add	sp, sp, #0x40
   56618:	ret

000000000005661c <__gmp_randinit@@Base>:
   5661c:	sub	sp, sp, #0xe0
   56620:	stp	x29, x30, [sp, #208]
   56624:	add	x29, sp, #0xd0
   56628:	mov	x8, #0xffffffffffffffd0    	// #-48
   5662c:	mov	x9, sp
   56630:	sub	x10, x29, #0x50
   56634:	movk	x8, #0xff80, lsl #32
   56638:	add	x11, x29, #0x10
   5663c:	add	x9, x9, #0x80
   56640:	add	x10, x10, #0x30
   56644:	stp	x2, x3, [x29, #-80]
   56648:	stp	x4, x5, [x29, #-64]
   5664c:	stp	x6, x7, [x29, #-48]
   56650:	stp	q1, q2, [sp, #16]
   56654:	stp	q3, q4, [sp, #48]
   56658:	str	q0, [sp]
   5665c:	stp	q5, q6, [sp, #80]
   56660:	str	q7, [sp, #112]
   56664:	stp	x9, x8, [x29, #-16]
   56668:	stp	x11, x10, [x29, #-32]
   5666c:	cbz	w1, 5668c <__gmp_randinit@@Base+0x70>
   56670:	mov	w8, #0x1                   	// #1
   56674:	adrp	x9, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56678:	ldr	x9, [x9, #3896]
   5667c:	ldr	w10, [x9]
   56680:	orr	w8, w10, w8
   56684:	str	w8, [x9]
   56688:	b	566c8 <__gmp_randinit@@Base+0xac>
   5668c:	ldursw	x8, [x29, #-8]
   56690:	tbz	w8, #31, 566b0 <__gmp_randinit@@Base+0x94>
   56694:	add	w9, w8, #0x8
   56698:	cmn	w8, #0x8
   5669c:	stur	w9, [x29, #-8]
   566a0:	b.gt	566b0 <__gmp_randinit@@Base+0x94>
   566a4:	ldur	x9, [x29, #-24]
   566a8:	add	x8, x9, x8
   566ac:	b	566bc <__gmp_randinit@@Base+0xa0>
   566b0:	ldur	x8, [x29, #-32]
   566b4:	add	x9, x8, #0x8
   566b8:	stur	x9, [x29, #-32]
   566bc:	ldr	x1, [x8]
   566c0:	bl	d190 <__gmp_randinit_lc_2exp_size@plt>
   566c4:	cbz	w0, 566d4 <__gmp_randinit@@Base+0xb8>
   566c8:	ldp	x29, x30, [sp, #208]
   566cc:	add	sp, sp, #0xe0
   566d0:	ret
   566d4:	mov	w8, #0x8                   	// #8
   566d8:	b	56674 <__gmp_randinit@@Base+0x58>

00000000000566dc <__gmp_randclear@@Base>:
   566dc:	ldr	x8, [x0, #24]
   566e0:	ldr	x1, [x8, #16]
   566e4:	br	x1

00000000000566e8 <__gmp_randinit_default@@Base>:
   566e8:	b	cb00 <__gmp_randinit_mt@plt>

00000000000566ec <__gmp_randinit_set@@Base>:
   566ec:	ldr	x8, [x1, #24]
   566f0:	ldr	x2, [x8, #24]
   566f4:	br	x2

00000000000566f8 <__gmp_randinit_lc_2exp_size@@Base>:
   566f8:	sub	sp, sp, #0x30
   566fc:	stp	x20, x19, [sp, #32]
   56700:	cmp	x1, #0x10
   56704:	mov	x19, x0
   56708:	stp	x29, x30, [sp, #16]
   5670c:	add	x29, sp, #0x10
   56710:	b.ls	56728 <__gmp_randinit_lc_2exp_size@@Base+0x30>  // b.plast
   56714:	cmp	x1, #0x11
   56718:	b.ne	56770 <__gmp_randinit_lc_2exp_size@@Base+0x78>  // b.any
   5671c:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56720:	add	x20, x20, #0xb00
   56724:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56728:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5672c:	add	x20, x20, #0xad0
   56730:	ldr	x1, [x20, #8]
   56734:	mov	x0, sp
   56738:	mov	w2, #0x10                  	// #16
   5673c:	bl	d0a0 <__gmpz_init_set_str@plt>
   56740:	ldr	x2, [x20, #16]
   56744:	ldr	x3, [x20]
   56748:	mov	x1, sp
   5674c:	mov	x0, x19
   56750:	bl	cf40 <__gmp_randinit_lc_2exp@plt>
   56754:	mov	x0, sp
   56758:	bl	cb50 <__gmpz_clear@plt>
   5675c:	mov	w0, #0x1                   	// #1
   56760:	ldp	x20, x19, [sp, #32]
   56764:	ldp	x29, x30, [sp, #16]
   56768:	add	sp, sp, #0x30
   5676c:	ret
   56770:	cmp	x1, #0x13
   56774:	b.cc	5678c <__gmp_randinit_lc_2exp_size@@Base+0x94>  // b.lo, b.ul, b.last
   56778:	cmp	x1, #0x13
   5677c:	b.ne	56798 <__gmp_randinit_lc_2exp_size@@Base+0xa0>  // b.any
   56780:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56784:	add	x20, x20, #0xb60
   56788:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   5678c:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56790:	add	x20, x20, #0xb30
   56794:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56798:	cmp	x1, #0x15
   5679c:	b.cc	567e8 <__gmp_randinit_lc_2exp_size@@Base+0xf0>  // b.lo, b.ul, b.last
   567a0:	cmp	x1, #0x1d
   567a4:	b.cc	567f4 <__gmp_randinit_lc_2exp_size@@Base+0xfc>  // b.lo, b.ul, b.last
   567a8:	cmp	x1, #0x21
   567ac:	b.cc	56800 <__gmp_randinit_lc_2exp_size@@Base+0x108>  // b.lo, b.ul, b.last
   567b0:	cmp	x1, #0x33
   567b4:	b.cc	5680c <__gmp_randinit_lc_2exp_size@@Base+0x114>  // b.lo, b.ul, b.last
   567b8:	cmp	x1, #0x41
   567bc:	b.cc	56818 <__gmp_randinit_lc_2exp_size@@Base+0x120>  // b.lo, b.ul, b.last
   567c0:	cmp	x1, #0x4f
   567c4:	b.cc	56824 <__gmp_randinit_lc_2exp_size@@Base+0x12c>  // b.lo, b.ul, b.last
   567c8:	cmp	x1, #0x63
   567cc:	b.cc	56830 <__gmp_randinit_lc_2exp_size@@Base+0x138>  // b.lo, b.ul, b.last
   567d0:	cmp	x1, #0x65
   567d4:	b.cc	5683c <__gmp_randinit_lc_2exp_size@@Base+0x144>  // b.lo, b.ul, b.last
   567d8:	cmp	x1, #0x80
   567dc:	b.ls	56848 <__gmp_randinit_lc_2exp_size@@Base+0x150>  // b.plast
   567e0:	mov	w0, wzr
   567e4:	b	56760 <__gmp_randinit_lc_2exp_size@@Base+0x68>
   567e8:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   567ec:	add	x20, x20, #0xb90
   567f0:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   567f4:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   567f8:	add	x20, x20, #0xba8
   567fc:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56800:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56804:	add	x20, x20, #0xbc0
   56808:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   5680c:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56810:	add	x20, x20, #0xbd8
   56814:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56818:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5681c:	add	x20, x20, #0xbf0
   56820:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56824:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56828:	add	x20, x20, #0xc08
   5682c:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56830:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56834:	add	x20, x20, #0xc20
   56838:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   5683c:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56840:	add	x20, x20, #0xc38
   56844:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>
   56848:	adrp	x20, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5684c:	add	x20, x20, #0xc50
   56850:	b	56730 <__gmp_randinit_lc_2exp_size@@Base+0x38>

0000000000056854 <__gmp_randinit_lc_2exp@@Base>:
   56854:	stp	x29, x30, [sp, #-64]!
   56858:	stp	x24, x23, [sp, #16]
   5685c:	stp	x22, x21, [sp, #32]
   56860:	stp	x20, x19, [sp, #48]
   56864:	mov	x29, sp
   56868:	cbz	x3, 56948 <__gmp_randinit_lc_2exp@@Base+0xf4>
   5686c:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56870:	ldr	x8, [x8, #3840]
   56874:	mov	x23, x0
   56878:	add	x9, x3, #0x3f
   5687c:	mov	w0, #0x38                  	// #56
   56880:	ldr	x8, [x8]
   56884:	mov	x19, x3
   56888:	mov	x20, x2
   5688c:	mov	x22, x1
   56890:	lsr	x24, x9, #6
   56894:	blr	x8
   56898:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   5689c:	add	x8, x8, #0xc80
   568a0:	mov	x1, x19
   568a4:	mov	x21, x0
   568a8:	str	x0, [x23, #8]
   568ac:	str	x8, [x23, #24]
   568b0:	bl	d130 <__gmpz_init2@plt>
   568b4:	cbz	x24, 568c8 <__gmp_randinit_lc_2exp@@Base+0x74>
   568b8:	ldr	x0, [x21, #8]
   568bc:	lsl	x2, x24, #3
   568c0:	mov	w1, wzr
   568c4:	bl	c5f0 <memset@plt>
   568c8:	ldr	x8, [x21, #8]
   568cc:	add	x23, x21, #0x10
   568d0:	str	w24, [x21, #4]
   568d4:	mov	w24, #0x1                   	// #1
   568d8:	mov	x0, x23
   568dc:	str	x24, [x8]
   568e0:	bl	d250 <__gmpz_init@plt>
   568e4:	mov	x0, x23
   568e8:	mov	x1, x22
   568ec:	mov	x2, x19
   568f0:	bl	d0c0 <__gmpz_fdiv_r_2exp@plt>
   568f4:	ldr	w8, [x21, #20]
   568f8:	cbnz	w8, 56914 <__gmp_randinit_lc_2exp@@Base+0xc0>
   568fc:	ldr	w8, [x21, #16]
   56900:	str	w24, [x21, #20]
   56904:	cmp	w8, #0x0
   56908:	b.le	56938 <__gmp_randinit_lc_2exp@@Base+0xe4>
   5690c:	ldr	x0, [x21, #24]
   56910:	str	xzr, [x0]
   56914:	cmp	x20, #0x0
   56918:	cset	w8, ne  // ne = any
   5691c:	stp	x8, x20, [x21, #32]
   56920:	str	x19, [x21, #48]
   56924:	ldp	x20, x19, [sp, #48]
   56928:	ldp	x22, x21, [sp, #32]
   5692c:	ldp	x24, x23, [sp, #16]
   56930:	ldp	x29, x30, [sp], #64
   56934:	ret
   56938:	mov	w1, #0x1                   	// #1
   5693c:	mov	x0, x23
   56940:	bl	c080 <__gmpz_realloc@plt>
   56944:	b	56910 <__gmp_randinit_lc_2exp@@Base+0xbc>
   56948:	adrp	x0, 63000 <__gmp_jacobi_table@@Base+0x7599>
   5694c:	adrp	x2, 63000 <__gmp_jacobi_table@@Base+0x7599>
   56950:	add	x0, x0, #0xc73
   56954:	add	x2, x2, #0xc7e
   56958:	mov	w1, #0x12d                 	// #301
   5695c:	bl	c6c0 <__gmp_assert_fail@plt>
   56960:	stp	x29, x30, [sp, #-32]!
   56964:	stp	x20, x19, [sp, #16]
   56968:	ldr	x19, [x0, #8]
   5696c:	mov	x29, sp
   56970:	ldr	x2, [x19, #48]
   56974:	mov	x0, x19
   56978:	add	x8, x2, #0x3f
   5697c:	lsr	x20, x8, #6
   56980:	bl	d0c0 <__gmpz_fdiv_r_2exp@plt>
   56984:	ldrsw	x8, [x19, #4]
   56988:	subs	x9, x20, x8
   5698c:	b.eq	569a4 <__gmp_randinit_lc_2exp@@Base+0x150>  // b.none
   56990:	ldr	x10, [x19, #8]
   56994:	lsl	x2, x9, #3
   56998:	mov	w1, wzr
   5699c:	add	x0, x10, x8, lsl #3
   569a0:	bl	c5f0 <memset@plt>
   569a4:	str	w20, [x19, #4]
   569a8:	ldp	x20, x19, [sp, #16]
   569ac:	ldp	x29, x30, [sp], #32
   569b0:	ret
   569b4:	stp	x29, x30, [sp, #-96]!
   569b8:	stp	x28, x27, [sp, #16]
   569bc:	stp	x26, x25, [sp, #32]
   569c0:	stp	x24, x23, [sp, #48]
   569c4:	stp	x22, x21, [sp, #64]
   569c8:	stp	x20, x19, [sp, #80]
   569cc:	mov	x29, sp
   569d0:	sub	sp, sp, #0x10
   569d4:	ldr	x8, [x0, #8]
   569d8:	stp	x1, xzr, [x29, #-16]
   569dc:	mov	x19, x2
   569e0:	mov	x21, x0
   569e4:	ldr	x24, [x8, #48]
   569e8:	lsr	x25, x24, #1
   569ec:	add	w8, w25, #0x3f
   569f0:	add	w9, w25, #0x7e
   569f4:	cmp	w8, #0x0
   569f8:	csel	w23, w9, w8, lt  // lt = tstop
   569fc:	asr	w8, w23, #6
   56a00:	sbfiz	x8, x8, #3, #32
   56a04:	mov	w9, #0x7f00                	// #32512
   56a08:	cmp	x8, x9
   56a0c:	b.hi	56b18 <__gmp_randinit_lc_2exp@@Base+0x2c4>  // b.pmore
   56a10:	add	x8, x8, #0xf
   56a14:	mov	x9, sp
   56a18:	and	x8, x8, #0xfffffffffffffff0
   56a1c:	sub	x22, x9, x8
   56a20:	mov	sp, x22
   56a24:	cmp	x19, w25, sxtw
   56a28:	b.cs	56b30 <__gmp_randinit_lc_2exp@@Base+0x2dc>  // b.hs, b.nlast
   56a2c:	mov	x26, xzr
   56a30:	cmp	x26, x19
   56a34:	b.eq	56ad4 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   56a38:	ldur	x11, [x29, #-16]
   56a3c:	sub	w9, w19, w26
   56a40:	lsr	x8, x26, #3
   56a44:	add	w10, w9, #0x3f
   56a48:	and	x8, x8, #0x1ffffffffffffff8
   56a4c:	add	w9, w9, #0x7e
   56a50:	cmp	w10, #0x0
   56a54:	add	x23, x11, x8
   56a58:	csel	w8, w9, w10, lt  // lt = tstop
   56a5c:	mov	x0, x22
   56a60:	mov	x1, x21
   56a64:	sbfx	x24, x8, #6, #26
   56a68:	bl	56c94 <__gmp_randinit_lc_2exp@@Base+0x440>
   56a6c:	ands	x21, x26, #0x3f
   56a70:	b.eq	56afc <__gmp_randinit_lc_2exp@@Base+0x2a8>  // b.none
   56a74:	ldr	x20, [x23]
   56a78:	mov	x0, x23
   56a7c:	mov	x1, x22
   56a80:	mov	x2, x24
   56a84:	mov	w3, w21
   56a88:	bl	c180 <__gmpn_lshift@plt>
   56a8c:	ldr	x8, [x23]
   56a90:	sub	x9, x26, x21
   56a94:	add	x9, x9, x24, lsl #6
   56a98:	cmp	x9, x19
   56a9c:	orr	x8, x8, x20
   56aa0:	str	x8, [x23]
   56aa4:	b.cs	56aac <__gmp_randinit_lc_2exp@@Base+0x258>  // b.hs, b.nlast
   56aa8:	str	x0, [x23, x24, lsl #3]
   56aac:	ands	x8, x19, #0x3f
   56ab0:	b.eq	56ad4 <__gmp_randinit_lc_2exp@@Base+0x280>  // b.none
   56ab4:	ldur	x12, [x29, #-16]
   56ab8:	lsr	x9, x19, #3
   56abc:	and	x9, x9, #0x1ffffffffffffff8
   56ac0:	mov	x11, #0xffffffffffffffff    	// #-1
   56ac4:	ldr	x10, [x12, x9]
   56ac8:	lsl	x8, x11, x8
   56acc:	bic	x8, x10, x8
   56ad0:	str	x8, [x12, x9]
   56ad4:	ldur	x0, [x29, #-8]
   56ad8:	cbnz	x0, 56bd0 <__gmp_randinit_lc_2exp@@Base+0x37c>
   56adc:	mov	sp, x29
   56ae0:	ldp	x20, x19, [sp, #80]
   56ae4:	ldp	x22, x21, [sp, #64]
   56ae8:	ldp	x24, x23, [sp, #48]
   56aec:	ldp	x26, x25, [sp, #32]
   56af0:	ldp	x28, x27, [sp, #16]
   56af4:	ldp	x29, x30, [sp], #96
   56af8:	ret
   56afc:	mov	x0, x23
   56b00:	mov	x1, x22
   56b04:	mov	x2, x24
   56b08:	bl	ca50 <__gmpn_copyi@plt>
   56b0c:	ands	x8, x19, #0x3f
   56b10:	b.ne	56ab4 <__gmp_randinit_lc_2exp@@Base+0x260>  // b.any
   56b14:	b	56ad4 <__gmp_randinit_lc_2exp@@Base+0x280>
   56b18:	sub	x0, x29, #0x8
   56b1c:	mov	x1, x8
   56b20:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   56b24:	mov	x22, x0
   56b28:	cmp	x19, w25, sxtw
   56b2c:	b.cc	56a2c <__gmp_randinit_lc_2exp@@Base+0x1d8>  // b.lo, b.ul, b.last
   56b30:	add	w8, w25, #0x3f
   56b34:	cmp	w25, #0x0
   56b38:	csel	w8, w8, w25, lt  // lt = tstop
   56b3c:	and	w8, w8, #0xffffffc0
   56b40:	sub	w8, w25, w8
   56b44:	mov	x26, xzr
   56b48:	sbfx	x23, x23, #6, #26
   56b4c:	sxtw	x27, w8
   56b50:	sbfx	x28, x24, #1, #32
   56b54:	b	56b74 <__gmp_randinit_lc_2exp@@Base+0x320>
   56b58:	mov	x0, x24
   56b5c:	mov	x1, x21
   56b60:	bl	56c94 <__gmp_randinit_lc_2exp@@Base+0x440>
   56b64:	add	x26, x26, x28
   56b68:	add	x8, x28, x26
   56b6c:	cmp	x8, x19
   56b70:	b.hi	56a30 <__gmp_randinit_lc_2exp@@Base+0x1dc>  // b.pmore
   56b74:	ldur	x9, [x29, #-16]
   56b78:	lsr	x8, x26, #3
   56b7c:	and	x8, x8, #0x1ffffffffffffff8
   56b80:	ands	x25, x26, #0x3f
   56b84:	add	x24, x9, x8
   56b88:	b.eq	56b58 <__gmp_randinit_lc_2exp@@Base+0x304>  // b.none
   56b8c:	mov	x0, x22
   56b90:	mov	x1, x21
   56b94:	bl	56c94 <__gmp_randinit_lc_2exp@@Base+0x440>
   56b98:	ldr	x20, [x24]
   56b9c:	mov	x0, x24
   56ba0:	mov	x1, x22
   56ba4:	mov	x2, x23
   56ba8:	mov	w3, w25
   56bac:	bl	c180 <__gmpn_lshift@plt>
   56bb0:	ldr	x8, [x24]
   56bb4:	add	x9, x25, x27
   56bb8:	cmp	x9, #0x41
   56bbc:	orr	x8, x8, x20
   56bc0:	str	x8, [x24]
   56bc4:	b.cc	56b64 <__gmp_randinit_lc_2exp@@Base+0x310>  // b.lo, b.ul, b.last
   56bc8:	str	x0, [x24, x23, lsl #3]
   56bcc:	b	56b64 <__gmp_randinit_lc_2exp@@Base+0x310>
   56bd0:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   56bd4:	b	56adc <__gmp_randinit_lc_2exp@@Base+0x288>
   56bd8:	stp	x29, x30, [sp, #-32]!
   56bdc:	str	x19, [sp, #16]
   56be0:	ldr	x19, [x0, #8]
   56be4:	mov	x29, sp
   56be8:	mov	x0, x19
   56bec:	bl	cb50 <__gmpz_clear@plt>
   56bf0:	add	x0, x19, #0x10
   56bf4:	bl	cb50 <__gmpz_clear@plt>
   56bf8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56bfc:	ldr	x8, [x8, #4016]
   56c00:	mov	x0, x19
   56c04:	ldr	x19, [sp, #16]
   56c08:	mov	w1, #0x38                  	// #56
   56c0c:	ldr	x2, [x8]
   56c10:	ldp	x29, x30, [sp], #32
   56c14:	br	x2
   56c18:	stp	x29, x30, [sp, #-48]!
   56c1c:	str	x21, [sp, #16]
   56c20:	stp	x20, x19, [sp, #32]
   56c24:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56c28:	ldr	x19, [x1, #8]
   56c2c:	ldr	x8, [x8, #3840]
   56c30:	mov	x20, x0
   56c34:	mov	w0, #0x38                  	// #56
   56c38:	mov	x29, sp
   56c3c:	ldr	x8, [x8]
   56c40:	blr	x8
   56c44:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   56c48:	add	x8, x8, #0xc80
   56c4c:	mov	x1, x19
   56c50:	mov	x21, x0
   56c54:	str	x0, [x20, #8]
   56c58:	str	x8, [x20, #24]
   56c5c:	bl	bf80 <__gmpz_init_set@plt>
   56c60:	add	x0, x21, #0x10
   56c64:	add	x1, x19, #0x10
   56c68:	bl	bf80 <__gmpz_init_set@plt>
   56c6c:	ldr	x8, [x19, #32]
   56c70:	str	x8, [x21, #32]
   56c74:	ldr	x8, [x19, #40]
   56c78:	str	x8, [x21, #40]
   56c7c:	ldr	x8, [x19, #48]
   56c80:	str	x8, [x21, #48]
   56c84:	ldp	x20, x19, [sp, #32]
   56c88:	ldr	x21, [sp, #16]
   56c8c:	ldp	x29, x30, [sp], #48
   56c90:	ret
   56c94:	stp	x29, x30, [sp, #-96]!
   56c98:	stp	x28, x27, [sp, #16]
   56c9c:	stp	x26, x25, [sp, #32]
   56ca0:	stp	x24, x23, [sp, #48]
   56ca4:	stp	x22, x21, [sp, #64]
   56ca8:	stp	x20, x19, [sp, #80]
   56cac:	mov	x29, sp
   56cb0:	sub	sp, sp, #0x10
   56cb4:	ldr	x27, [x1, #8]
   56cb8:	ldr	x26, [x27, #48]
   56cbc:	ldrsw	x22, [x27, #4]
   56cc0:	ldrsw	x23, [x27, #20]
   56cc4:	ldr	x25, [x27, #8]
   56cc8:	ldr	x24, [x27, #24]
   56ccc:	add	x8, x26, #0x3f
   56cd0:	add	x28, x23, x22
   56cd4:	lsr	x20, x8, #6
   56cd8:	cmp	x28, x20
   56cdc:	stp	x0, xzr, [x29, #-16]
   56ce0:	b.ge	56d28 <__gmp_randinit_lc_2exp@@Base+0x4d4>  // b.tcont
   56ce4:	add	x19, x20, #0x1
   56ce8:	lsr	x8, x8, #11
   56cec:	cmp	x8, #0x7e
   56cf0:	lsl	x1, x19, #3
   56cf4:	b.hi	56e40 <__gmp_randinit_lc_2exp@@Base+0x5ec>  // b.pmore
   56cf8:	add	x9, x1, #0xf
   56cfc:	mov	x8, sp
   56d00:	and	x9, x9, #0x7ffffffffffffff0
   56d04:	sub	x21, x8, x9
   56d08:	mov	sp, x21
   56d0c:	subs	x8, x19, x28
   56d10:	b.eq	56d50 <__gmp_randinit_lc_2exp@@Base+0x4fc>  // b.none
   56d14:	add	x0, x21, x28, lsl #3
   56d18:	lsl	x2, x8, #3
   56d1c:	mov	w1, wzr
   56d20:	bl	c5f0 <memset@plt>
   56d24:	b	56d50 <__gmp_randinit_lc_2exp@@Base+0x4fc>
   56d28:	lsl	x8, x28, #3
   56d2c:	add	x1, x8, #0x8
   56d30:	mov	w8, #0x7f00                	// #32512
   56d34:	cmp	x1, x8
   56d38:	b.hi	56e58 <__gmp_randinit_lc_2exp@@Base+0x604>  // b.pmore
   56d3c:	add	x9, x1, #0xf
   56d40:	mov	x8, sp
   56d44:	and	x9, x9, #0xfffffffffffffff0
   56d48:	sub	x21, x8, x9
   56d4c:	mov	sp, x21
   56d50:	mov	x0, x21
   56d54:	mov	x1, x25
   56d58:	mov	x2, x22
   56d5c:	mov	x3, x24
   56d60:	mov	x4, x23
   56d64:	bl	ccd0 <__gmpn_mul@plt>
   56d68:	ldr	x22, [x27, #32]
   56d6c:	cbz	x22, 56da8 <__gmp_randinit_lc_2exp@@Base+0x554>
   56d70:	add	x2, x27, #0x28
   56d74:	mov	x0, x21
   56d78:	mov	x1, x21
   56d7c:	mov	x3, x22
   56d80:	bl	ca70 <__gmpn_add_n@plt>
   56d84:	cbz	x0, 56da8 <__gmp_randinit_lc_2exp@@Base+0x554>
   56d88:	cmp	x22, x20
   56d8c:	b.ge	56da8 <__gmp_randinit_lc_2exp@@Base+0x554>  // b.tcont
   56d90:	lsl	x8, x22, #3
   56d94:	ldr	x9, [x21, x8]
   56d98:	add	x22, x22, #0x1
   56d9c:	adds	x9, x9, #0x1
   56da0:	str	x9, [x21, x8]
   56da4:	b.cs	56d88 <__gmp_randinit_lc_2exp@@Base+0x534>  // b.hs, b.nlast
   56da8:	lsr	x8, x26, #3
   56dac:	and	x8, x8, #0x1ffffffffffffff8
   56db0:	ldr	x9, [x21, x8]
   56db4:	mov	x10, #0xffffffffffffffff    	// #-1
   56db8:	lsl	x10, x10, x26
   56dbc:	mov	x1, x21
   56dc0:	bic	x9, x9, x10
   56dc4:	str	x9, [x21, x8]
   56dc8:	ldr	x0, [x27, #8]
   56dcc:	mov	x2, x20
   56dd0:	bl	ca50 <__gmpn_copyi@plt>
   56dd4:	lsr	x19, x26, #7
   56dd8:	sub	x2, x20, x19
   56ddc:	cmp	x2, #0x1
   56de0:	b.lt	56e10 <__gmp_randinit_lc_2exp@@Base+0x5bc>  // b.tstop
   56de4:	ubfx	w3, w26, #1, #6
   56de8:	add	x1, x21, x19, lsl #3
   56dec:	cbz	w3, 56e08 <__gmp_randinit_lc_2exp@@Base+0x5b4>
   56df0:	mov	x0, x21
   56df4:	bl	c1a0 <__gmpn_rshift@plt>
   56df8:	ldur	x0, [x29, #-16]
   56dfc:	add	x2, x19, #0x1
   56e00:	mov	x1, x21
   56e04:	b	56e0c <__gmp_randinit_lc_2exp@@Base+0x5b8>
   56e08:	ldur	x0, [x29, #-16]
   56e0c:	bl	ca50 <__gmpn_copyi@plt>
   56e10:	ldur	x0, [x29, #-8]
   56e14:	cbnz	x0, 56e38 <__gmp_randinit_lc_2exp@@Base+0x5e4>
   56e18:	mov	sp, x29
   56e1c:	ldp	x20, x19, [sp, #80]
   56e20:	ldp	x22, x21, [sp, #64]
   56e24:	ldp	x24, x23, [sp, #48]
   56e28:	ldp	x26, x25, [sp, #32]
   56e2c:	ldp	x28, x27, [sp, #16]
   56e30:	ldp	x29, x30, [sp], #96
   56e34:	ret
   56e38:	bl	bef0 <__gmp_tmp_reentrant_free@plt>
   56e3c:	b	56e18 <__gmp_randinit_lc_2exp@@Base+0x5c4>
   56e40:	sub	x0, x29, #0x8
   56e44:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   56e48:	mov	x21, x0
   56e4c:	subs	x8, x19, x28
   56e50:	b.ne	56d14 <__gmp_randinit_lc_2exp@@Base+0x4c0>  // b.any
   56e54:	b	56d50 <__gmp_randinit_lc_2exp@@Base+0x4fc>
   56e58:	sub	x0, x29, #0x8
   56e5c:	bl	cbc0 <__gmp_tmp_reentrant_alloc@plt>
   56e60:	mov	x21, x0
   56e64:	b	56d50 <__gmp_randinit_lc_2exp@@Base+0x4fc>

0000000000056e68 <__gmp_mt_recalc_buffer@@Base>:
   56e68:	ld1r	{v0.4s}, [x0]
   56e6c:	mov	w9, #0x7ffffffe            	// #2147483646
   56e70:	dup	v2.4s, w9
   56e74:	mov	w9, #0xb0df                	// #45279
   56e78:	movk	w9, #0x9908, lsl #16
   56e7c:	mov	x8, xzr
   56e80:	movi	v1.4s, #0x80, lsl #24
   56e84:	movi	v3.4s, #0x1
   56e88:	dup	v4.4s, w9
   56e8c:	add	x9, x0, x8
   56e90:	mov	v5.16b, v0.16b
   56e94:	ldur	q0, [x9, #4]
   56e98:	add	x10, x9, #0x634
   56e9c:	ldr	q6, [x10]
   56ea0:	add	x8, x8, #0x10
   56ea4:	ext	v5.16b, v5.16b, v0.16b, #12
   56ea8:	and	v7.16b, v0.16b, v2.16b
   56eac:	and	v5.16b, v5.16b, v1.16b
   56eb0:	orr	v5.16b, v7.16b, v5.16b
   56eb4:	ushr	v5.4s, v5.4s, #1
   56eb8:	eor	v5.16b, v5.16b, v6.16b
   56ebc:	and	v6.16b, v0.16b, v3.16b
   56ec0:	cmeq	v6.4s, v6.4s, #0
   56ec4:	bic	v6.16b, v4.16b, v6.16b
   56ec8:	eor	v5.16b, v5.16b, v6.16b
   56ecc:	cmp	x8, #0x380
   56ed0:	str	q5, [x9]
   56ed4:	b.ne	56e8c <__gmp_mt_recalc_buffer@@Base+0x24>  // b.any
   56ed8:	ldr	w10, [x0, #900]
   56edc:	ldr	w11, [x0, #2484]
   56ee0:	mov	w9, v0.s[3]
   56ee4:	ldr	w12, [x0, #904]
   56ee8:	mov	w16, #0x7ffffffe            	// #2147483646
   56eec:	ldr	w14, [x0, #908]
   56ef0:	and	w9, w9, #0x80000000
   56ef4:	dup	v0.4s, w16
   56ef8:	and	w16, w10, #0x7ffffffe
   56efc:	orr	w9, w16, w9
   56f00:	ldr	w13, [x0, #2488]
   56f04:	ldr	w15, [x0, #2492]
   56f08:	eor	w11, w11, w9, lsr #1
   56f0c:	mov	w9, #0xb0df                	// #45279
   56f10:	and	w17, w10, #0x80000000
   56f14:	and	w18, w12, #0x7ffffffe
   56f18:	movk	w9, #0x9908, lsl #16
   56f1c:	sbfx	w10, w10, #0, #1
   56f20:	and	w16, w12, #0x80000000
   56f24:	orr	w17, w18, w17
   56f28:	and	w18, w14, #0x7ffffffe
   56f2c:	and	w10, w10, w9
   56f30:	orr	w16, w18, w16
   56f34:	eor	w10, w11, w10
   56f38:	sbfx	w11, w12, #0, #1
   56f3c:	sbfx	w12, w14, #0, #1
   56f40:	eor	w13, w13, w17, lsr #1
   56f44:	eor	w15, w15, w16, lsr #1
   56f48:	and	w11, w11, w9
   56f4c:	and	w12, w12, w9
   56f50:	mov	x8, xzr
   56f54:	eor	w11, w13, w11
   56f58:	dup	v4.4s, w14
   56f5c:	eor	w12, w15, w12
   56f60:	movi	v1.4s, #0x80, lsl #24
   56f64:	movi	v2.4s, #0x1
   56f68:	dup	v3.4s, w9
   56f6c:	str	w10, [x0, #896]
   56f70:	str	w11, [x0, #900]
   56f74:	str	w12, [x0, #904]
   56f78:	add	x10, x0, x8
   56f7c:	ldr	q5, [x10, #912]
   56f80:	add	x8, x8, #0x10
   56f84:	cmp	x8, #0x630
   56f88:	ext	v4.16b, v4.16b, v5.16b, #12
   56f8c:	and	v6.16b, v5.16b, v0.16b
   56f90:	and	v4.16b, v4.16b, v1.16b
   56f94:	orr	v4.16b, v6.16b, v4.16b
   56f98:	ldr	q6, [x10]
   56f9c:	ushr	v4.4s, v4.4s, #1
   56fa0:	add	x10, x10, #0x38c
   56fa4:	eor	v4.16b, v4.16b, v6.16b
   56fa8:	and	v6.16b, v5.16b, v2.16b
   56fac:	cmeq	v6.4s, v6.4s, #0
   56fb0:	bic	v6.16b, v3.16b, v6.16b
   56fb4:	eor	v4.16b, v4.16b, v6.16b
   56fb8:	str	q4, [x10]
   56fbc:	mov	v4.16b, v5.16b
   56fc0:	b.ne	56f78 <__gmp_mt_recalc_buffer@@Base+0x110>  // b.any
   56fc4:	ldr	w8, [x0, #2492]
   56fc8:	ldr	w10, [x0]
   56fcc:	ldr	w11, [x0, #1584]
   56fd0:	and	w8, w8, #0x80000000
   56fd4:	and	w12, w10, #0x7ffffffe
   56fd8:	sbfx	w10, w10, #0, #1
   56fdc:	orr	w8, w12, w8
   56fe0:	eor	w8, w11, w8, lsr #1
   56fe4:	and	w9, w10, w9
   56fe8:	eor	w8, w8, w9
   56fec:	str	w8, [x0, #2492]
   56ff0:	ret

0000000000056ff4 <__gmp_randget_mt@@Base>:
   56ff4:	str	x19, [sp, #-16]!
   56ff8:	ldr	x12, [x0, #8]
   56ffc:	mov	w10, #0x5680                	// #22144
   57000:	mov	w13, #0xb0df                	// #45279
   57004:	mov	w9, #0xefc60000            	// #-272236544
   57008:	movk	w10, #0x9d2c, lsl #16
   5700c:	movk	w13, #0x9908, lsl #16
   57010:	lsr	x8, x2, #6
   57014:	and	w11, w2, #0x3f
   57018:	cbz	x8, 57360 <__gmp_randget_mt@@Base+0x36c>
   5701c:	ldr	w16, [x12, #2496]
   57020:	mov	w15, #0x7ffffffe            	// #2147483646
   57024:	mov	x14, xzr
   57028:	movi	v0.4s, #0x80, lsl #24
   5702c:	dup	v1.4s, w15
   57030:	movi	v2.4s, #0x1
   57034:	dup	v3.4s, w13
   57038:	b	57074 <__gmp_randget_mt@@Base+0x80>
   5703c:	add	w16, w17, #0x1
   57040:	str	w16, [x12, #2496]
   57044:	ldr	w17, [x12, w17, sxtw #2]
   57048:	eor	w17, w17, w17, lsr #11
   5704c:	and	w18, w10, w17, lsl #7
   57050:	eor	w17, w18, w17
   57054:	and	w18, w9, w17, lsl #15
   57058:	eor	w17, w18, w17
   5705c:	eor	w17, w17, w17, lsr #18
   57060:	bfi	x15, x17, #32, #32
   57064:	str	x15, [x1, x14, lsl #3]
   57068:	add	x14, x14, #0x1
   5706c:	cmp	x14, x8
   57070:	b.eq	57360 <__gmp_randget_mt@@Base+0x36c>  // b.none
   57074:	cmp	w16, #0x26f
   57078:	b.le	571d4 <__gmp_randget_mt@@Base+0x1e0>
   5707c:	ld1r	{v4.4s}, [x12]
   57080:	mov	x15, xzr
   57084:	add	x16, x12, x15
   57088:	mov	v5.16b, v4.16b
   5708c:	ldur	q4, [x16, #4]
   57090:	add	x17, x16, #0x634
   57094:	ldr	q6, [x17]
   57098:	add	x15, x15, #0x10
   5709c:	ext	v5.16b, v5.16b, v4.16b, #12
   570a0:	and	v7.16b, v4.16b, v1.16b
   570a4:	and	v5.16b, v5.16b, v0.16b
   570a8:	orr	v5.16b, v7.16b, v5.16b
   570ac:	ushr	v5.4s, v5.4s, #1
   570b0:	eor	v5.16b, v5.16b, v6.16b
   570b4:	and	v6.16b, v4.16b, v2.16b
   570b8:	cmeq	v6.4s, v6.4s, #0
   570bc:	bic	v6.16b, v3.16b, v6.16b
   570c0:	eor	v5.16b, v5.16b, v6.16b
   570c4:	cmp	x15, #0x380
   570c8:	str	q5, [x16]
   570cc:	b.ne	57084 <__gmp_randget_mt@@Base+0x90>  // b.any
   570d0:	ldr	w17, [x12, #900]
   570d4:	ldr	w18, [x12, #2484]
   570d8:	ldr	w0, [x12, #904]
   570dc:	mov	w16, v4.s[3]
   570e0:	ldr	w2, [x12, #2488]
   570e4:	ldr	w3, [x12, #908]
   570e8:	and	w16, w16, #0x80000000
   570ec:	and	w5, w17, #0x7ffffffe
   570f0:	and	w6, w17, #0x80000000
   570f4:	orr	w16, w5, w16
   570f8:	sbfx	w17, w17, #0, #1
   570fc:	ldr	w4, [x12, #2492]
   57100:	and	w7, w0, #0x7ffffffe
   57104:	eor	w16, w18, w16, lsr #1
   57108:	and	w17, w17, w13
   5710c:	orr	w6, w7, w6
   57110:	eor	w16, w16, w17
   57114:	sbfx	w17, w0, #0, #1
   57118:	and	w5, w0, #0x80000000
   5711c:	and	w7, w3, #0x7ffffffe
   57120:	eor	w18, w2, w6, lsr #1
   57124:	and	w17, w17, w13
   57128:	orr	w5, w7, w5
   5712c:	eor	w17, w18, w17
   57130:	sbfx	w18, w3, #0, #1
   57134:	eor	w2, w4, w5, lsr #1
   57138:	and	w18, w18, w13
   5713c:	mov	x15, xzr
   57140:	eor	w18, w2, w18
   57144:	dup	v4.4s, w3
   57148:	str	w16, [x12, #896]
   5714c:	str	w17, [x12, #900]
   57150:	str	w18, [x12, #904]
   57154:	add	x16, x12, x15
   57158:	ldr	q5, [x16, #912]
   5715c:	add	x15, x15, #0x10
   57160:	cmp	x15, #0x630
   57164:	ext	v4.16b, v4.16b, v5.16b, #12
   57168:	and	v6.16b, v5.16b, v1.16b
   5716c:	and	v4.16b, v4.16b, v0.16b
   57170:	orr	v4.16b, v6.16b, v4.16b
   57174:	ldr	q6, [x16]
   57178:	ushr	v4.4s, v4.4s, #1
   5717c:	add	x16, x16, #0x38c
   57180:	eor	v4.16b, v4.16b, v6.16b
   57184:	and	v6.16b, v5.16b, v2.16b
   57188:	cmeq	v6.4s, v6.4s, #0
   5718c:	bic	v6.16b, v3.16b, v6.16b
   57190:	eor	v4.16b, v4.16b, v6.16b
   57194:	str	q4, [x16]
   57198:	mov	v4.16b, v5.16b
   5719c:	b.ne	57154 <__gmp_randget_mt@@Base+0x160>  // b.any
   571a0:	ldr	w15, [x12, #2492]
   571a4:	ldr	w17, [x12]
   571a8:	ldr	w18, [x12, #1584]
   571ac:	mov	w16, wzr
   571b0:	and	w15, w15, #0x80000000
   571b4:	and	w0, w17, #0x7ffffffe
   571b8:	sbfx	w17, w17, #0, #1
   571bc:	orr	w15, w0, w15
   571c0:	and	w17, w17, w13
   571c4:	eor	w15, w18, w15, lsr #1
   571c8:	eor	w15, w15, w17
   571cc:	str	w15, [x12, #2492]
   571d0:	str	wzr, [x12, #2496]
   571d4:	add	w17, w16, #0x1
   571d8:	str	w17, [x12, #2496]
   571dc:	ldr	w15, [x12, w16, sxtw #2]
   571e0:	cmp	w16, #0x26f
   571e4:	eor	w15, w15, w15, lsr #11
   571e8:	and	w18, w10, w15, lsl #7
   571ec:	eor	w15, w18, w15
   571f0:	and	w18, w9, w15, lsl #15
   571f4:	eor	w15, w18, w15
   571f8:	eor	w15, w15, w15, lsr #18
   571fc:	str	x15, [x1, x14, lsl #3]
   57200:	b.lt	5703c <__gmp_randget_mt@@Base+0x48>  // b.tstop
   57204:	ld1r	{v4.4s}, [x12]
   57208:	mov	x16, xzr
   5720c:	add	x17, x12, x16
   57210:	mov	v5.16b, v4.16b
   57214:	ldur	q4, [x17, #4]
   57218:	add	x18, x17, #0x634
   5721c:	ldr	q6, [x18]
   57220:	add	x16, x16, #0x10
   57224:	ext	v5.16b, v5.16b, v4.16b, #12
   57228:	and	v7.16b, v4.16b, v1.16b
   5722c:	and	v5.16b, v5.16b, v0.16b
   57230:	orr	v5.16b, v7.16b, v5.16b
   57234:	ushr	v5.4s, v5.4s, #1
   57238:	eor	v5.16b, v5.16b, v6.16b
   5723c:	and	v6.16b, v4.16b, v2.16b
   57240:	cmeq	v6.4s, v6.4s, #0
   57244:	bic	v6.16b, v3.16b, v6.16b
   57248:	eor	v5.16b, v5.16b, v6.16b
   5724c:	cmp	x16, #0x380
   57250:	str	q5, [x17]
   57254:	b.ne	5720c <__gmp_randget_mt@@Base+0x218>  // b.any
   57258:	ldr	w18, [x12, #900]
   5725c:	ldr	w0, [x12, #2484]
   57260:	ldr	w2, [x12, #904]
   57264:	mov	w17, v4.s[3]
   57268:	ldr	w3, [x12, #2488]
   5726c:	ldr	w4, [x12, #908]
   57270:	and	w17, w17, #0x80000000
   57274:	and	w6, w18, #0x7ffffffe
   57278:	and	w7, w18, #0x80000000
   5727c:	orr	w17, w6, w17
   57280:	sbfx	w18, w18, #0, #1
   57284:	ldr	w5, [x12, #2492]
   57288:	and	w19, w2, #0x7ffffffe
   5728c:	eor	w17, w0, w17, lsr #1
   57290:	and	w18, w18, w13
   57294:	orr	w7, w19, w7
   57298:	eor	w17, w17, w18
   5729c:	sbfx	w18, w2, #0, #1
   572a0:	and	w6, w2, #0x80000000
   572a4:	and	w19, w4, #0x7ffffffe
   572a8:	eor	w0, w3, w7, lsr #1
   572ac:	and	w18, w18, w13
   572b0:	orr	w6, w19, w6
   572b4:	eor	w18, w0, w18
   572b8:	sbfx	w0, w4, #0, #1
   572bc:	eor	w3, w5, w6, lsr #1
   572c0:	and	w0, w0, w13
   572c4:	mov	x16, xzr
   572c8:	eor	w0, w3, w0
   572cc:	dup	v4.4s, w4
   572d0:	str	w17, [x12, #896]
   572d4:	str	w18, [x12, #900]
   572d8:	str	w0, [x12, #904]
   572dc:	add	x17, x12, x16
   572e0:	ldr	q5, [x17, #912]
   572e4:	add	x16, x16, #0x10
   572e8:	cmp	x16, #0x630
   572ec:	ext	v4.16b, v4.16b, v5.16b, #12
   572f0:	and	v6.16b, v5.16b, v1.16b
   572f4:	and	v4.16b, v4.16b, v0.16b
   572f8:	orr	v4.16b, v6.16b, v4.16b
   572fc:	ldr	q6, [x17]
   57300:	ushr	v4.4s, v4.4s, #1
   57304:	add	x17, x17, #0x38c
   57308:	eor	v4.16b, v4.16b, v6.16b
   5730c:	and	v6.16b, v5.16b, v2.16b
   57310:	cmeq	v6.4s, v6.4s, #0
   57314:	bic	v6.16b, v3.16b, v6.16b
   57318:	eor	v4.16b, v4.16b, v6.16b
   5731c:	str	q4, [x17]
   57320:	mov	v4.16b, v5.16b
   57324:	b.ne	572dc <__gmp_randget_mt@@Base+0x2e8>  // b.any
   57328:	ldr	w16, [x12, #2492]
   5732c:	ldr	w18, [x12]
   57330:	ldr	w0, [x12, #1584]
   57334:	mov	w17, wzr
   57338:	and	w16, w16, #0x80000000
   5733c:	and	w2, w18, #0x7ffffffe
   57340:	sbfx	w18, w18, #0, #1
   57344:	orr	w16, w2, w16
   57348:	and	w18, w18, w13
   5734c:	eor	w16, w0, w16, lsr #1
   57350:	eor	w16, w16, w18
   57354:	str	w16, [x12, #2492]
   57358:	str	wzr, [x12, #2496]
   5735c:	b	5703c <__gmp_randget_mt@@Base+0x48>
   57360:	cbz	w11, 578e0 <__gmp_randget_mt@@Base+0x8ec>
   57364:	ldr	w15, [x12, #2496]
   57368:	cmp	w11, #0x1f
   5736c:	b.hi	57544 <__gmp_randget_mt@@Base+0x550>  // b.pmore
   57370:	cmp	w15, #0x270
   57374:	b.lt	57508 <__gmp_randget_mt@@Base+0x514>  // b.tstop
   57378:	ld1r	{v0.4s}, [x12]
   5737c:	mov	w15, #0x7ffffffe            	// #2147483646
   57380:	dup	v2.4s, w15
   57384:	mov	w15, #0xb0df                	// #45279
   57388:	movk	w15, #0x9908, lsl #16
   5738c:	mov	x14, xzr
   57390:	movi	v1.4s, #0x80, lsl #24
   57394:	movi	v3.4s, #0x1
   57398:	dup	v4.4s, w15
   5739c:	add	x15, x12, x14
   573a0:	mov	v5.16b, v0.16b
   573a4:	ldur	q0, [x15, #4]
   573a8:	add	x16, x15, #0x634
   573ac:	ldr	q6, [x16]
   573b0:	add	x14, x14, #0x10
   573b4:	ext	v5.16b, v5.16b, v0.16b, #12
   573b8:	and	v7.16b, v0.16b, v2.16b
   573bc:	and	v5.16b, v5.16b, v1.16b
   573c0:	orr	v5.16b, v7.16b, v5.16b
   573c4:	ushr	v5.4s, v5.4s, #1
   573c8:	eor	v5.16b, v5.16b, v6.16b
   573cc:	and	v6.16b, v0.16b, v3.16b
   573d0:	cmeq	v6.4s, v6.4s, #0
   573d4:	bic	v6.16b, v4.16b, v6.16b
   573d8:	eor	v5.16b, v5.16b, v6.16b
   573dc:	cmp	x14, #0x380
   573e0:	str	q5, [x15]
   573e4:	b.ne	5739c <__gmp_randget_mt@@Base+0x3a8>  // b.any
   573e8:	ldr	w16, [x12, #900]
   573ec:	ldr	w17, [x12, #2484]
   573f0:	ldr	w18, [x12, #904]
   573f4:	mov	w15, v0.s[3]
   573f8:	mov	w4, #0x7ffffffe            	// #2147483646
   573fc:	ldr	w0, [x12, #2488]
   57400:	ldr	w2, [x12, #908]
   57404:	and	w15, w15, #0x80000000
   57408:	dup	v0.4s, w4
   5740c:	and	w4, w16, #0x7ffffffe
   57410:	and	w5, w16, #0x80000000
   57414:	orr	w15, w4, w15
   57418:	sbfx	w16, w16, #0, #1
   5741c:	ldr	w3, [x12, #2492]
   57420:	and	w6, w18, #0x7ffffffe
   57424:	eor	w15, w17, w15, lsr #1
   57428:	and	w16, w16, w13
   5742c:	orr	w5, w6, w5
   57430:	eor	w15, w15, w16
   57434:	sbfx	w16, w18, #0, #1
   57438:	and	w4, w18, #0x80000000
   5743c:	and	w6, w2, #0x7ffffffe
   57440:	eor	w17, w0, w5, lsr #1
   57444:	and	w16, w16, w13
   57448:	orr	w4, w6, w4
   5744c:	eor	w16, w17, w16
   57450:	sbfx	w17, w2, #0, #1
   57454:	str	w15, [x12, #896]
   57458:	mov	w15, #0xb0df                	// #45279
   5745c:	eor	w0, w3, w4, lsr #1
   57460:	and	w17, w17, w13
   57464:	movk	w15, #0x9908, lsl #16
   57468:	mov	x14, xzr
   5746c:	dup	v4.4s, w2
   57470:	eor	w17, w0, w17
   57474:	movi	v1.4s, #0x80, lsl #24
   57478:	movi	v2.4s, #0x1
   5747c:	dup	v3.4s, w15
   57480:	str	w16, [x12, #900]
   57484:	str	w17, [x12, #904]
   57488:	add	x15, x12, x14
   5748c:	ldr	q5, [x15, #912]
   57490:	add	x14, x14, #0x10
   57494:	cmp	x14, #0x630
   57498:	ext	v4.16b, v4.16b, v5.16b, #12
   5749c:	and	v6.16b, v5.16b, v0.16b
   574a0:	and	v4.16b, v4.16b, v1.16b
   574a4:	orr	v4.16b, v6.16b, v4.16b
   574a8:	ldr	q6, [x15]
   574ac:	ushr	v4.4s, v4.4s, #1
   574b0:	add	x15, x15, #0x38c
   574b4:	eor	v4.16b, v4.16b, v6.16b
   574b8:	and	v6.16b, v5.16b, v2.16b
   574bc:	cmeq	v6.4s, v6.4s, #0
   574c0:	bic	v6.16b, v3.16b, v6.16b
   574c4:	eor	v4.16b, v4.16b, v6.16b
   574c8:	str	q4, [x15]
   574cc:	mov	v4.16b, v5.16b
   574d0:	b.ne	57488 <__gmp_randget_mt@@Base+0x494>  // b.any
   574d4:	ldr	w14, [x12, #2492]
   574d8:	ldr	w16, [x12]
   574dc:	ldr	w17, [x12, #1584]
   574e0:	mov	w15, wzr
   574e4:	and	w14, w14, #0x80000000
   574e8:	and	w18, w16, #0x7ffffffe
   574ec:	sbfx	w16, w16, #0, #1
   574f0:	orr	w14, w18, w14
   574f4:	and	w13, w16, w13
   574f8:	eor	w14, w17, w14, lsr #1
   574fc:	eor	w13, w14, w13
   57500:	str	w13, [x12, #2492]
   57504:	str	wzr, [x12, #2496]
   57508:	add	w13, w15, #0x1
   5750c:	str	w13, [x12, #2496]
   57510:	ldr	w12, [x12, w15, sxtw #2]
   57514:	mov	x13, #0xffffffffffffffff    	// #-1
   57518:	eor	w12, w12, w12, lsr #11
   5751c:	and	w10, w10, w12, lsl #7
   57520:	eor	w10, w10, w12
   57524:	and	w9, w9, w10, lsl #15
   57528:	eor	w9, w9, w10
   5752c:	eor	w9, w9, w9, lsr #18
   57530:	lsl	x10, x13, x11
   57534:	bic	x9, x9, x10
   57538:	str	x9, [x1, x8, lsl #3]
   5753c:	ldr	x19, [sp], #16
   57540:	ret
   57544:	cmp	w15, #0x270
   57548:	b.lt	576dc <__gmp_randget_mt@@Base+0x6e8>  // b.tstop
   5754c:	ld1r	{v0.4s}, [x12]
   57550:	mov	w15, #0x7ffffffe            	// #2147483646
   57554:	dup	v2.4s, w15
   57558:	mov	w15, #0xb0df                	// #45279
   5755c:	movk	w15, #0x9908, lsl #16
   57560:	mov	x14, xzr
   57564:	movi	v1.4s, #0x80, lsl #24
   57568:	movi	v3.4s, #0x1
   5756c:	dup	v4.4s, w15
   57570:	add	x15, x12, x14
   57574:	mov	v5.16b, v0.16b
   57578:	ldur	q0, [x15, #4]
   5757c:	add	x16, x15, #0x634
   57580:	ldr	q6, [x16]
   57584:	add	x14, x14, #0x10
   57588:	ext	v5.16b, v5.16b, v0.16b, #12
   5758c:	and	v7.16b, v0.16b, v2.16b
   57590:	and	v5.16b, v5.16b, v1.16b
   57594:	orr	v5.16b, v7.16b, v5.16b
   57598:	ushr	v5.4s, v5.4s, #1
   5759c:	eor	v5.16b, v5.16b, v6.16b
   575a0:	and	v6.16b, v0.16b, v3.16b
   575a4:	cmeq	v6.4s, v6.4s, #0
   575a8:	bic	v6.16b, v4.16b, v6.16b
   575ac:	eor	v5.16b, v5.16b, v6.16b
   575b0:	cmp	x14, #0x380
   575b4:	str	q5, [x15]
   575b8:	b.ne	57570 <__gmp_randget_mt@@Base+0x57c>  // b.any
   575bc:	ldr	w16, [x12, #900]
   575c0:	ldr	w17, [x12, #2484]
   575c4:	ldr	w18, [x12, #904]
   575c8:	mov	w15, v0.s[3]
   575cc:	mov	w4, #0x7ffffffe            	// #2147483646
   575d0:	ldr	w0, [x12, #2488]
   575d4:	ldr	w2, [x12, #908]
   575d8:	and	w15, w15, #0x80000000
   575dc:	dup	v0.4s, w4
   575e0:	and	w4, w16, #0x7ffffffe
   575e4:	and	w5, w16, #0x80000000
   575e8:	orr	w15, w4, w15
   575ec:	sbfx	w16, w16, #0, #1
   575f0:	ldr	w3, [x12, #2492]
   575f4:	and	w6, w18, #0x7ffffffe
   575f8:	eor	w15, w17, w15, lsr #1
   575fc:	and	w16, w16, w13
   57600:	orr	w5, w6, w5
   57604:	eor	w15, w15, w16
   57608:	sbfx	w16, w18, #0, #1
   5760c:	and	w4, w18, #0x80000000
   57610:	and	w6, w2, #0x7ffffffe
   57614:	eor	w17, w0, w5, lsr #1
   57618:	and	w16, w16, w13
   5761c:	orr	w4, w6, w4
   57620:	eor	w16, w17, w16
   57624:	sbfx	w17, w2, #0, #1
   57628:	str	w15, [x12, #896]
   5762c:	mov	w15, #0xb0df                	// #45279
   57630:	eor	w0, w3, w4, lsr #1
   57634:	and	w17, w17, w13
   57638:	movk	w15, #0x9908, lsl #16
   5763c:	mov	x14, xzr
   57640:	dup	v4.4s, w2
   57644:	eor	w17, w0, w17
   57648:	movi	v1.4s, #0x80, lsl #24
   5764c:	movi	v2.4s, #0x1
   57650:	dup	v3.4s, w15
   57654:	str	w16, [x12, #900]
   57658:	str	w17, [x12, #904]
   5765c:	add	x15, x12, x14
   57660:	ldr	q5, [x15, #912]
   57664:	add	x14, x14, #0x10
   57668:	cmp	x14, #0x630
   5766c:	ext	v4.16b, v4.16b, v5.16b, #12
   57670:	and	v6.16b, v5.16b, v0.16b
   57674:	and	v4.16b, v4.16b, v1.16b
   57678:	orr	v4.16b, v6.16b, v4.16b
   5767c:	ldr	q6, [x15]
   57680:	ushr	v4.4s, v4.4s, #1
   57684:	add	x15, x15, #0x38c
   57688:	eor	v4.16b, v4.16b, v6.16b
   5768c:	and	v6.16b, v5.16b, v2.16b
   57690:	cmeq	v6.4s, v6.4s, #0
   57694:	bic	v6.16b, v3.16b, v6.16b
   57698:	eor	v4.16b, v4.16b, v6.16b
   5769c:	str	q4, [x15]
   576a0:	mov	v4.16b, v5.16b
   576a4:	b.ne	5765c <__gmp_randget_mt@@Base+0x668>  // b.any
   576a8:	ldr	w14, [x12, #2492]
   576ac:	ldr	w16, [x12]
   576b0:	ldr	w17, [x12, #1584]
   576b4:	mov	w15, wzr
   576b8:	and	w14, w14, #0x80000000
   576bc:	and	w18, w16, #0x7ffffffe
   576c0:	sbfx	w16, w16, #0, #1
   576c4:	orr	w14, w18, w14
   576c8:	and	w16, w16, w13
   576cc:	eor	w14, w17, w14, lsr #1
   576d0:	eor	w14, w14, w16
   576d4:	str	w14, [x12, #2492]
   576d8:	str	wzr, [x12, #2496]
   576dc:	add	w16, w15, #0x1
   576e0:	str	w16, [x12, #2496]
   576e4:	ldr	w14, [x12, w15, sxtw #2]
   576e8:	cmp	w11, #0x21
   576ec:	eor	w14, w14, w14, lsr #11
   576f0:	and	w17, w10, w14, lsl #7
   576f4:	eor	w14, w17, w14
   576f8:	and	w17, w9, w14, lsl #15
   576fc:	eor	w14, w17, w14
   57700:	eor	w14, w14, w14, lsr #18
   57704:	str	x14, [x1, x8, lsl #3]
   57708:	b.cc	578e0 <__gmp_randget_mt@@Base+0x8ec>  // b.lo, b.ul, b.last
   5770c:	cmp	w15, #0x26f
   57710:	b.lt	578a4 <__gmp_randget_mt@@Base+0x8b0>  // b.tstop
   57714:	ld1r	{v0.4s}, [x12]
   57718:	mov	w16, #0x7ffffffe            	// #2147483646
   5771c:	dup	v2.4s, w16
   57720:	mov	w16, #0xb0df                	// #45279
   57724:	movk	w16, #0x9908, lsl #16
   57728:	mov	x15, xzr
   5772c:	movi	v1.4s, #0x80, lsl #24
   57730:	movi	v3.4s, #0x1
   57734:	dup	v4.4s, w16
   57738:	add	x16, x12, x15
   5773c:	mov	v5.16b, v0.16b
   57740:	ldur	q0, [x16, #4]
   57744:	add	x17, x16, #0x634
   57748:	ldr	q6, [x17]
   5774c:	add	x15, x15, #0x10
   57750:	ext	v5.16b, v5.16b, v0.16b, #12
   57754:	and	v7.16b, v0.16b, v2.16b
   57758:	and	v5.16b, v5.16b, v1.16b
   5775c:	orr	v5.16b, v7.16b, v5.16b
   57760:	ushr	v5.4s, v5.4s, #1
   57764:	eor	v5.16b, v5.16b, v6.16b
   57768:	and	v6.16b, v0.16b, v3.16b
   5776c:	cmeq	v6.4s, v6.4s, #0
   57770:	bic	v6.16b, v4.16b, v6.16b
   57774:	eor	v5.16b, v5.16b, v6.16b
   57778:	cmp	x15, #0x380
   5777c:	str	q5, [x16]
   57780:	b.ne	57738 <__gmp_randget_mt@@Base+0x744>  // b.any
   57784:	ldr	w17, [x12, #900]
   57788:	ldr	w18, [x12, #2484]
   5778c:	ldr	w0, [x12, #904]
   57790:	mov	w16, v0.s[3]
   57794:	mov	w5, #0x7ffffffe            	// #2147483646
   57798:	ldr	w2, [x12, #2488]
   5779c:	ldr	w3, [x12, #908]
   577a0:	and	w16, w16, #0x80000000
   577a4:	dup	v0.4s, w5
   577a8:	and	w5, w17, #0x7ffffffe
   577ac:	and	w6, w17, #0x80000000
   577b0:	orr	w16, w5, w16
   577b4:	sbfx	w17, w17, #0, #1
   577b8:	ldr	w4, [x12, #2492]
   577bc:	and	w7, w0, #0x7ffffffe
   577c0:	eor	w16, w18, w16, lsr #1
   577c4:	and	w17, w17, w13
   577c8:	orr	w6, w7, w6
   577cc:	eor	w16, w16, w17
   577d0:	sbfx	w17, w0, #0, #1
   577d4:	and	w5, w0, #0x80000000
   577d8:	and	w7, w3, #0x7ffffffe
   577dc:	eor	w18, w2, w6, lsr #1
   577e0:	and	w17, w17, w13
   577e4:	orr	w5, w7, w5
   577e8:	eor	w17, w18, w17
   577ec:	sbfx	w18, w3, #0, #1
   577f0:	str	w16, [x12, #896]
   577f4:	mov	w16, #0xb0df                	// #45279
   577f8:	eor	w2, w4, w5, lsr #1
   577fc:	and	w18, w18, w13
   57800:	movk	w16, #0x9908, lsl #16
   57804:	mov	x15, xzr
   57808:	dup	v4.4s, w3
   5780c:	eor	w18, w2, w18
   57810:	movi	v1.4s, #0x80, lsl #24
   57814:	movi	v2.4s, #0x1
   57818:	dup	v3.4s, w16
   5781c:	str	w17, [x12, #900]
   57820:	str	w18, [x12, #904]
   57824:	add	x16, x12, x15
   57828:	ldr	q5, [x16, #912]
   5782c:	add	x15, x15, #0x10
   57830:	cmp	x15, #0x630
   57834:	ext	v4.16b, v4.16b, v5.16b, #12
   57838:	and	v6.16b, v5.16b, v0.16b
   5783c:	and	v4.16b, v4.16b, v1.16b
   57840:	orr	v4.16b, v6.16b, v4.16b
   57844:	ldr	q6, [x16]
   57848:	ushr	v4.4s, v4.4s, #1
   5784c:	add	x16, x16, #0x38c
   57850:	eor	v4.16b, v4.16b, v6.16b
   57854:	and	v6.16b, v5.16b, v2.16b
   57858:	cmeq	v6.4s, v6.4s, #0
   5785c:	bic	v6.16b, v3.16b, v6.16b
   57860:	eor	v4.16b, v4.16b, v6.16b
   57864:	str	q4, [x16]
   57868:	mov	v4.16b, v5.16b
   5786c:	b.ne	57824 <__gmp_randget_mt@@Base+0x830>  // b.any
   57870:	ldr	w15, [x12, #2492]
   57874:	ldr	w17, [x12]
   57878:	ldr	w18, [x12, #1584]
   5787c:	mov	w16, wzr
   57880:	and	w15, w15, #0x80000000
   57884:	and	w0, w17, #0x7ffffffe
   57888:	sbfx	w17, w17, #0, #1
   5788c:	orr	w15, w0, w15
   57890:	and	w13, w17, w13
   57894:	eor	w15, w18, w15, lsr #1
   57898:	eor	w13, w15, w13
   5789c:	str	w13, [x12, #2492]
   578a0:	str	wzr, [x12, #2496]
   578a4:	add	w13, w16, #0x1
   578a8:	str	w13, [x12, #2496]
   578ac:	ldr	w12, [x12, w16, sxtw #2]
   578b0:	sub	w11, w11, #0x20
   578b4:	mov	x13, #0xffffffffffffffff    	// #-1
   578b8:	eor	w12, w12, w12, lsr #11
   578bc:	and	w10, w10, w12, lsl #7
   578c0:	eor	w10, w10, w12
   578c4:	and	w9, w9, w10, lsl #15
   578c8:	eor	w9, w9, w10
   578cc:	eor	w9, w9, w9, lsr #18
   578d0:	lsl	x10, x13, x11
   578d4:	bic	w9, w9, w10
   578d8:	bfi	x14, x9, #32, #32
   578dc:	str	x14, [x1, x8, lsl #3]
   578e0:	ldr	x19, [sp], #16
   578e4:	ret

00000000000578e8 <__gmp_randclear_mt@@Base>:
   578e8:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   578ec:	ldr	x8, [x8, #4016]
   578f0:	ldrsw	x9, [x0]
   578f4:	ldr	x0, [x0, #8]
   578f8:	ldr	x2, [x8]
   578fc:	lsl	x1, x9, #3
   57900:	br	x2

0000000000057904 <__gmp_randiset_mt@@Base>:
   57904:	stp	x29, x30, [sp, #-32]!
   57908:	stp	x20, x19, [sp, #16]
   5790c:	ldr	x8, [x1, #24]
   57910:	mov	x20, x0
   57914:	mov	x29, sp
   57918:	mov	x19, x1
   5791c:	str	x8, [x0, #24]
   57920:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   57924:	ldr	x8, [x8, #3840]
   57928:	mov	w0, #0x9c8                 	// #2504
   5792c:	ldr	x8, [x8]
   57930:	blr	x8
   57934:	mov	w8, #0x139                 	// #313
   57938:	str	x0, [x20, #8]
   5793c:	str	w8, [x20]
   57940:	ldr	x8, [x19, #8]
   57944:	add	x9, x8, #0x9c0
   57948:	cmp	x0, x9
   5794c:	b.cs	57978 <__gmp_randiset_mt@@Base+0x74>  // b.hs, b.nlast
   57950:	add	x9, x0, #0x9c0
   57954:	cmp	x9, x8
   57958:	b.ls	57978 <__gmp_randiset_mt@@Base+0x74>  // b.plast
   5795c:	mov	x9, xzr
   57960:	ldr	w10, [x8, x9]
   57964:	str	w10, [x0, x9]
   57968:	add	x9, x9, #0x4
   5796c:	cmp	x9, #0x9c0
   57970:	b.ne	57960 <__gmp_randiset_mt@@Base+0x5c>  // b.any
   57974:	b	57998 <__gmp_randiset_mt@@Base+0x94>
   57978:	mov	x9, xzr
   5797c:	add	x10, x8, x9
   57980:	ldp	q0, q1, [x10]
   57984:	add	x11, x0, x9
   57988:	add	x9, x9, #0x20
   5798c:	cmp	x9, #0x9c0
   57990:	stp	q0, q1, [x11]
   57994:	b.ne	5797c <__gmp_randiset_mt@@Base+0x78>  // b.any
   57998:	ldr	w8, [x8, #2496]
   5799c:	str	w8, [x0, #2496]
   579a0:	ldp	x20, x19, [sp, #16]
   579a4:	ldp	x29, x30, [sp], #32
   579a8:	ret

00000000000579ac <__gmp_randinit_mt_noseed@@Base>:
   579ac:	stp	x29, x30, [sp, #-32]!
   579b0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   579b4:	add	x8, x8, #0xca0
   579b8:	stp	x20, x19, [sp, #16]
   579bc:	str	x8, [x0, #24]
   579c0:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   579c4:	ldr	x8, [x8, #3840]
   579c8:	mov	x19, x0
   579cc:	mov	w0, #0x9c8                 	// #2504
   579d0:	mov	x29, sp
   579d4:	ldr	x8, [x8]
   579d8:	blr	x8
   579dc:	adrp	x1, 63000 <__gmp_jacobi_table@@Base+0x7599>
   579e0:	mov	w8, #0x139                 	// #313
   579e4:	add	x1, x1, #0xc8c
   579e8:	mov	w2, #0x9c0                 	// #2496
   579ec:	mov	x20, x0
   579f0:	str	x0, [x19, #8]
   579f4:	str	w8, [x19]
   579f8:	bl	bed0 <memcpy@plt>
   579fc:	mov	w8, #0x80                  	// #128
   57a00:	str	w8, [x20, #2496]
   57a04:	ldp	x20, x19, [sp, #16]
   57a08:	ldp	x29, x30, [sp], #32
   57a0c:	ret

0000000000057a10 <__gmp_randinit_mt@@Base>:
   57a10:	stp	x29, x30, [sp, #-32]!
   57a14:	str	x19, [sp, #16]
   57a18:	mov	x29, sp
   57a1c:	mov	x19, x0
   57a20:	bl	bf30 <__gmp_randinit_mt_noseed@plt>
   57a24:	adrp	x8, 74000 <__gmp_limbroots_table@@Base+0x109e0>
   57a28:	add	x8, x8, #0xcc0
   57a2c:	str	x8, [x19, #24]
   57a30:	ldr	x19, [sp, #16]
   57a34:	ldp	x29, x30, [sp], #32
   57a38:	ret
   57a3c:	sub	sp, sp, #0x70
   57a40:	stp	x29, x30, [sp, #64]
   57a44:	stp	x20, x19, [sp, #96]
   57a48:	ldr	x19, [x0, #8]
   57a4c:	mov	x20, x1
   57a50:	add	x0, sp, #0x10
   57a54:	mov	w1, #0x4de2                	// #19938
   57a58:	str	x21, [sp, #80]
   57a5c:	add	x29, sp, #0x40
   57a60:	bl	d130 <__gmpz_init2@plt>
   57a64:	mov	x0, sp
   57a68:	mov	w1, #0x4de1                	// #19937
   57a6c:	bl	d130 <__gmpz_init2@plt>
   57a70:	add	x0, sp, #0x10
   57a74:	mov	w1, #0x4de1                	// #19937
   57a78:	bl	c310 <__gmpz_setbit@plt>
   57a7c:	add	x0, sp, #0x10
   57a80:	add	x1, sp, #0x10
   57a84:	mov	w2, #0x4e3b                	// #20027
   57a88:	bl	c120 <__gmpz_sub_ui@plt>
   57a8c:	mov	x0, sp
   57a90:	add	x2, sp, #0x10
   57a94:	mov	x1, x20
   57a98:	bl	cdf0 <__gmpz_mod@plt>
   57a9c:	add	x0, sp, #0x10
   57aa0:	bl	cb50 <__gmpz_clear@plt>
   57aa4:	mov	x0, sp
   57aa8:	mov	x1, sp
   57aac:	mov	w2, #0x2                   	// #2
   57ab0:	bl	c8b0 <__gmpz_add_ui@plt>
   57ab4:	sub	x0, x29, #0x10
   57ab8:	mov	w1, #0x4de1                	// #19937
   57abc:	bl	d130 <__gmpz_init2@plt>
   57ac0:	add	x0, sp, #0x20
   57ac4:	mov	x1, sp
   57ac8:	bl	bf80 <__gmpz_init_set@plt>
   57acc:	mov	w21, #0x8124                	// #33060
   57ad0:	mov	w20, #0x20000000            	// #536870912
   57ad4:	movk	w21, #0x4011, lsl #16
   57ad8:	b	57ae4 <__gmp_randinit_mt@@Base+0xd4>
   57adc:	lsr	x20, x20, #1
   57ae0:	cbz	x20, 57b44 <__gmp_randinit_mt@@Base+0x134>
   57ae4:	mov	x2, sp
   57ae8:	mov	x0, sp
   57aec:	mov	x1, sp
   57af0:	bl	c4b0 <__gmpz_mul@plt>
   57af4:	sub	x0, x29, #0x10
   57af8:	mov	x1, sp
   57afc:	mov	w2, #0x4de1                	// #19937
   57b00:	bl	cc70 <__gmpz_tdiv_q_2exp@plt>
   57b04:	ldur	w8, [x29, #-12]
   57b08:	cbz	w8, 57b30 <__gmp_randinit_mt@@Base+0x120>
   57b0c:	mov	x0, sp
   57b10:	mov	x1, sp
   57b14:	mov	w2, #0x4de1                	// #19937
   57b18:	bl	bee0 <__gmpz_tdiv_r_2exp@plt>
   57b1c:	mov	x0, sp
   57b20:	sub	x1, x29, #0x10
   57b24:	mov	w2, #0x4e37                	// #20023
   57b28:	bl	d320 <__gmpz_addmul_ui@plt>
   57b2c:	b	57af4 <__gmp_randinit_mt@@Base+0xe4>
   57b30:	tst	x21, x20
   57b34:	b.eq	57adc <__gmp_randinit_mt@@Base+0xcc>  // b.none
   57b38:	add	x2, sp, #0x20
   57b3c:	eor	x21, x21, x20
   57b40:	b	57ae8 <__gmp_randinit_mt@@Base+0xd8>
   57b44:	sub	x0, x29, #0x10
   57b48:	bl	cb50 <__gmpz_clear@plt>
   57b4c:	add	x0, sp, #0x20
   57b50:	bl	cb50 <__gmpz_clear@plt>
   57b54:	mov	x0, sp
   57b58:	mov	w1, #0x4de0                	// #19936
   57b5c:	bl	c480 <__gmpz_tstbit@plt>
   57b60:	cmp	w0, #0x0
   57b64:	cset	w8, ne  // ne = any
   57b68:	lsl	w8, w8, #31
   57b6c:	mov	x20, x19
   57b70:	mov	x0, sp
   57b74:	mov	w1, #0x4de0                	// #19936
   57b78:	str	w8, [x20], #4
   57b7c:	bl	c940 <__gmpz_clrbit@plt>
   57b80:	sub	x1, x29, #0x10
   57b84:	mov	x6, sp
   57b88:	mov	w2, #0xffffffff            	// #-1
   57b8c:	mov	w3, #0x4                   	// #4
   57b90:	mov	x0, x20
   57b94:	mov	w4, wzr
   57b98:	mov	x5, xzr
   57b9c:	bl	d220 <__gmpz_export@plt>
   57ba0:	mov	x0, sp
   57ba4:	bl	cb50 <__gmpz_clear@plt>
   57ba8:	ldur	x21, [x29, #-16]
   57bac:	add	x20, x21, #0x1
   57bb0:	cmp	x20, #0x26f
   57bb4:	stur	x20, [x29, #-16]
   57bb8:	b.hi	57c14 <__gmp_randinit_mt@@Base+0x204>  // b.pmore
   57bbc:	mov	w8, #0x9bc                 	// #2492
   57bc0:	add	x0, x19, x20, lsl #2
   57bc4:	sub	x2, x8, x21, lsl #2
   57bc8:	mov	w1, wzr
   57bcc:	bl	c5f0 <memset@plt>
   57bd0:	mov	w8, #0x26f                 	// #623
   57bd4:	sub	x8, x8, x21
   57bd8:	cmp	x8, #0x1
   57bdc:	b.ls	57bfc <__gmp_randinit_mt@@Base+0x1ec>  // b.plast
   57be0:	and	x9, x8, #0xfffffffffffffffe
   57be4:	add	x20, x20, x9
   57be8:	mov	x10, x9
   57bec:	subs	x10, x10, #0x2
   57bf0:	b.ne	57bec <__gmp_randinit_mt@@Base+0x1dc>  // b.any
   57bf4:	cmp	x8, x9
   57bf8:	b.eq	57c10 <__gmp_randinit_mt@@Base+0x200>  // b.none
   57bfc:	mov	x8, x20
   57c00:	add	x20, x8, #0x1
   57c04:	cmp	x8, #0x26f
   57c08:	mov	x8, x20
   57c0c:	b.cc	57c00 <__gmp_randinit_mt@@Base+0x1f0>  // b.lo, b.ul, b.last
   57c10:	stur	x20, [x29, #-16]
   57c14:	mov	x0, x19
   57c18:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   57c1c:	mov	x0, x19
   57c20:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   57c24:	mov	x0, x19
   57c28:	bl	d460 <__gmp_mt_recalc_buffer@plt>
   57c2c:	mov	w8, #0x80                  	// #128
   57c30:	str	w8, [x19, #2496]
   57c34:	ldp	x20, x19, [sp, #96]
   57c38:	ldr	x21, [sp, #80]
   57c3c:	ldp	x29, x30, [sp, #64]
   57c40:	add	sp, sp, #0x70
   57c44:	ret

0000000000057c48 <__gmp_randseed@@Base>:
   57c48:	ldr	x8, [x0, #24]
   57c4c:	ldr	x2, [x8]
   57c50:	br	x2

0000000000057c54 <__gmp_randseed_ui@@Base>:
   57c54:	sub	sp, sp, #0x30
   57c58:	add	x8, sp, #0x8
   57c5c:	cmp	x1, #0x0
   57c60:	str	x1, [sp, #8]
   57c64:	str	x8, [sp, #24]
   57c68:	cset	w8, ne  // ne = any
   57c6c:	add	x1, sp, #0x10
   57c70:	stp	x29, x30, [sp, #32]
   57c74:	add	x29, sp, #0x20
   57c78:	str	w8, [sp, #20]
   57c7c:	bl	cfb0 <__gmp_randseed@plt>
   57c80:	ldp	x29, x30, [sp, #32]
   57c84:	add	sp, sp, #0x30
   57c88:	ret

0000000000057c8c <__gmp_urandomb_ui@@Base>:
   57c8c:	sub	sp, sp, #0x20
   57c90:	stp	x29, x30, [sp, #16]
   57c94:	str	xzr, [sp, #8]
   57c98:	ldr	x8, [x0, #24]
   57c9c:	cmp	x1, #0x40
   57ca0:	mov	w9, #0x40                  	// #64
   57ca4:	csel	x2, x1, x9, cc  // cc = lo, ul, last
   57ca8:	ldr	x8, [x8, #8]
   57cac:	add	x1, sp, #0x8
   57cb0:	add	x29, sp, #0x10
   57cb4:	blr	x8
   57cb8:	ldr	x0, [sp, #8]
   57cbc:	ldp	x29, x30, [sp, #16]
   57cc0:	add	sp, sp, #0x20
   57cc4:	ret

0000000000057cc8 <__gmp_urandomm_ui@@Base>:
   57cc8:	sub	sp, sp, #0x40
   57ccc:	stp	x29, x30, [sp, #16]
   57cd0:	stp	x22, x21, [sp, #32]
   57cd4:	stp	x20, x19, [sp, #48]
   57cd8:	add	x29, sp, #0x10
   57cdc:	cbz	x1, 57d4c <__gmp_urandomm_ui@@Base+0x84>
   57ce0:	sub	x9, x1, #0x1
   57ce4:	tst	x1, x9
   57ce8:	clz	x8, x1
   57cec:	csetm	x9, eq  // eq = none
   57cf0:	sub	x8, x9, x8
   57cf4:	mov	x19, x1
   57cf8:	mov	x20, x0
   57cfc:	str	xzr, [sp, #8]
   57d00:	add	x21, x8, #0x40
   57d04:	mov	w22, #0x50                  	// #80
   57d08:	ldr	x8, [x20, #24]
   57d0c:	add	x1, sp, #0x8
   57d10:	mov	x0, x20
   57d14:	mov	x2, x21
   57d18:	ldr	x8, [x8, #8]
   57d1c:	blr	x8
   57d20:	ldr	x0, [sp, #8]
   57d24:	subs	x8, x0, x19
   57d28:	b.cc	57d38 <__gmp_urandomm_ui@@Base+0x70>  // b.lo, b.ul, b.last
   57d2c:	subs	w22, w22, #0x1
   57d30:	b.ne	57d08 <__gmp_urandomm_ui@@Base+0x40>  // b.any
   57d34:	mov	x0, x8
   57d38:	ldp	x20, x19, [sp, #48]
   57d3c:	ldp	x22, x21, [sp, #32]
   57d40:	ldp	x29, x30, [sp, #16]
   57d44:	add	sp, sp, #0x40
   57d48:	ret
   57d4c:	bl	bfd0 <__gmp_divide_by_zero@plt>

Disassembly of section .fini:

0000000000057d50 <.fini>:
   57d50:	stp	x29, x30, [sp, #-16]!
   57d54:	mov	x29, sp
   57d58:	ldp	x29, x30, [sp], #16
   57d5c:	ret
